copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
information	B
theory	I
inference	B
and	B
learning	B
algorithms	B
david	O
j	O
c	O
mackay	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
information	B
theory	I
inference	B
and	B
learning	B
algorithms	B
david	O
j	O
c	O
mackay	B
mackaymrao	O
cam	O
ac	O
uk	O
university	O
press	O
version	O
printing	O
march	O
please	O
send	O
feedback	B
on	O
this	O
book	O
via	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
version	O
of	O
this	O
book	O
was	O
published	O
by	O
c	O
u	O
p	O
in	O
september	O
it	O
will	O
remain	O
viewable	O
on-screen	O
on	O
the	O
above	O
website	O
in	O
postscript	O
djvu	B
and	O
pdf	O
formats	O
in	O
the	O
second	O
printing	O
minor	O
typos	O
were	O
corrected	O
and	O
the	O
book	O
design	O
was	O
slightly	O
altered	O
to	O
modify	O
the	O
placement	O
of	O
section	B
numbers	O
in	O
the	O
third	O
printing	O
minor	O
typos	O
were	O
corrected	O
and	O
chapter	O
was	O
renamed	O
dependent	O
random	B
variables	O
of	O
correlated	O
in	O
the	O
fourth	O
printing	O
minor	O
typos	O
were	O
corrected	O
replace	O
this	O
page	O
with	O
their	O
own	O
page	O
ii	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
contents	O
preface	O
introduction	O
to	O
information	B
theory	I
probability	B
entropy	B
and	O
inference	B
more	O
about	O
inference	B
i	O
data	O
compression	B
the	O
source	B
coding	B
theorem	I
symbol	O
codes	O
stream	B
codes	I
codes	O
for	B
integers	I
v	O
ii	O
noisy-channel	O
coding	O
dependent	O
random	B
variables	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
noisy-channel	B
coding	B
theorem	I
error-correcting	B
codes	I
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	B
theory	I
hash	O
codes	O
codes	O
for	O
information	B
retrieval	I
binary	O
codes	O
very	B
good	B
linear	B
codes	I
exist	O
further	O
exercises	O
on	O
information	B
theory	I
message	B
passing	I
communication	B
over	O
constrained	B
noiseless	B
channels	O
crosswords	O
and	O
codebreaking	O
why	O
have	O
sex	O
information	B
acquisition	O
and	O
evolution	B
iv	O
probabilities	O
and	O
inference	B
an	O
example	O
inference	B
task	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
maximum	B
likelihood	B
and	O
clustering	B
useful	O
probability	B
distributions	O
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	B
in	O
graphs	O
laplace	B
s	O
method	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
monte	B
carlo	I
methods	I
monte	B
carlo	I
methods	I
exact	O
monte	B
carlo	I
sampling	O
variational	B
methods	I
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	B
random	B
inference	B
topics	O
decision	B
theory	I
bayesian	B
inference	B
and	O
sampling	B
theory	I
v	O
neural	O
networks	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	B
as	O
a	O
capacity	B
of	O
a	O
single	B
neuron	B
learning	B
as	B
inference	B
networks	O
boltzmann	B
machines	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
gaussian	B
processes	I
deconvolution	B
vi	O
sparse	B
graph	B
codes	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
repeataccumulate	O
codes	O
digital	B
fountain	I
codes	O
vii	O
appendices	O
a	O
notation	B
b	O
some	O
physics	B
c	O
some	O
mathematics	O
bibliography	O
index	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
preface	O
this	O
book	O
is	O
aimed	O
at	O
senior	O
undergraduates	O
and	O
graduate	O
students	O
in	O
engineering	O
science	O
mathematics	O
and	O
computing	O
it	O
expects	O
familiarity	O
with	O
calculus	O
probability	B
theory	O
and	O
linear	B
algebra	O
as	O
taught	O
in	O
a	O
or	O
secondyear	O
undergraduate	O
course	O
on	O
mathematics	O
for	O
scientists	B
and	O
engineers	O
conventional	O
courses	O
on	O
information	B
theory	I
cover	B
not	O
only	O
the	O
beautiful	O
theoretical	O
ideas	O
of	O
shannon	B
but	O
also	O
practical	B
solutions	O
to	O
communication	B
problems	O
this	O
book	O
goes	O
further	O
bringing	O
in	O
bayesian	B
data	O
modelling	B
monte	B
carlo	I
methods	I
variational	B
methods	I
clustering	B
algorithms	B
and	O
neural	O
networks	O
why	O
unify	O
information	B
theory	I
and	O
machine	B
learning	B
because	O
they	O
are	O
two	O
sides	O
of	O
the	O
same	O
coin	B
in	O
the	O
a	O
single	O
cybernetics	O
was	O
populated	O
by	O
information	B
theorists	O
computer	B
scientists	B
and	O
neuroscientists	O
all	O
studying	O
common	O
problems	O
information	B
theory	I
and	O
machine	B
learning	B
still	O
belong	O
together	O
brains	O
are	O
the	O
ultimate	O
compression	B
and	B
communication	B
systems	O
and	O
the	O
state-of-the-art	O
algorithms	B
for	O
both	O
data	O
compression	B
and	O
error-correcting	B
codes	I
use	O
the	O
same	O
tools	O
as	O
machine	B
learning	B
how	O
to	O
use	O
this	O
book	O
the	O
essential	O
dependencies	O
between	O
chapters	O
are	O
indicated	O
in	O
the	O
on	O
the	O
next	O
page	O
an	O
arrow	O
from	O
one	O
chapter	O
to	O
another	O
indicates	O
that	O
the	O
second	O
chapter	O
requires	O
some	O
of	O
the	O
within	O
parts	O
i	O
ii	O
iv	O
and	O
v	O
of	O
this	O
book	O
chapters	O
on	O
advanced	O
or	O
optional	O
topics	O
are	O
towards	O
the	O
end	O
all	O
chapters	O
of	O
part	O
iii	O
are	O
optional	O
on	O
a	O
reading	O
except	O
perhaps	O
for	O
chapter	O
passing	O
the	O
same	O
system	O
sometimes	O
applies	O
within	O
a	O
chapter	O
the	O
sections	O
often	O
deal	O
with	O
advanced	O
topics	O
that	O
can	O
be	O
skipped	O
on	O
a	O
reading	O
for	O
example	O
in	O
two	O
key	O
chapters	O
chapter	O
source	B
coding	B
theorem	I
and	O
chapter	O
noisy-channel	B
coding	B
theorem	I
the	O
reader	O
should	O
detour	O
at	O
section	B
and	O
section	B
respectively	O
pages	O
viix	O
show	O
a	O
few	O
ways	O
to	O
use	O
this	O
book	O
first	O
i	O
give	O
the	O
roadmap	O
for	O
a	O
course	O
that	O
i	O
teach	O
in	O
cambridge	O
information	B
theory	I
pattern	B
recognition	B
and	O
neural	O
networks	O
the	O
book	O
is	O
also	O
intended	O
as	O
a	O
textbook	O
for	O
traditional	O
courses	O
in	O
information	B
theory	I
the	O
second	O
roadmap	O
shows	O
the	O
chapters	O
for	O
an	O
introductory	O
information	B
theory	I
course	O
and	O
the	O
third	O
for	O
a	O
course	O
aimed	O
at	O
an	O
understanding	O
of	O
state-of-the-art	O
error-correcting	B
codes	I
the	O
fourth	O
roadmap	O
shows	O
how	O
to	O
use	O
the	O
text	O
in	O
a	O
conventional	O
course	O
on	O
machine	B
learning	B
v	O
vi	O
probability	B
entropy	B
and	O
inference	B
more	O
about	O
inference	B
i	O
data	O
compression	B
the	O
source	B
coding	B
theorem	I
symbol	O
codes	O
stream	B
codes	I
codes	O
for	B
integers	I
dependent	O
random	B
variables	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
noisy-channel	B
coding	B
theorem	I
error-correcting	B
codes	I
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	B
theory	I
hash	O
codes	O
binary	O
codes	O
an	O
example	O
inference	B
task	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
maximum	B
likelihood	B
and	O
clustering	B
useful	O
probability	B
distributions	O
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	B
in	O
graphs	O
laplace	B
s	O
method	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
monte	B
carlo	I
methods	I
ising	O
models	O
exact	O
monte	B
carlo	I
sampling	O
variational	B
methods	I
independent	B
component	I
analysis	I
random	B
inference	B
topics	O
decision	B
theory	I
bayesian	B
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	B
preface	O
ii	O
noisy-channel	O
coding	O
monte	B
carlo	I
methods	I
very	B
good	B
linear	B
codes	I
exist	O
further	O
exercises	O
on	O
information	B
theory	I
v	O
neural	O
networks	O
message	B
passing	I
constrained	B
noiseless	B
channels	O
crosswords	O
and	O
codebreaking	O
why	O
have	O
sex	O
dependencies	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	B
as	O
a	O
capacity	B
of	O
a	O
single	B
neuron	B
learning	B
as	B
inference	B
networks	O
boltzmann	B
machines	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
gaussian	B
processes	I
deconvolution	B
vi	O
sparse	B
graph	B
codes	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
repeataccumulate	O
codes	O
digital	B
fountain	I
codes	O
preface	O
probability	B
entropy	B
and	O
inference	B
probability	B
entropy	B
and	O
inference	B
more	O
about	O
inference	B
more	O
about	O
inference	B
i	O
data	O
compression	B
the	O
source	B
coding	B
theorem	I
the	O
source	B
coding	B
theorem	I
symbol	O
codes	O
symbol	O
codes	O
stream	B
codes	I
stream	B
codes	I
codes	O
for	B
integers	I
dependent	O
random	B
variables	O
dependent	O
random	B
variables	O
communication	B
over	O
a	O
noisy	B
channel	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
noisy-channel	B
coding	B
theorem	I
the	O
noisy-channel	B
coding	B
theorem	I
error-correcting	B
codes	I
and	O
real	O
channels	O
error-correcting	B
codes	I
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	B
theory	I
hash	O
codes	O
binary	O
codes	O
an	O
example	O
inference	B
task	O
clustering	B
an	O
example	O
inference	B
task	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
exact	O
inference	B
by	O
complete	O
enumeration	O
maximum	B
likelihood	B
and	O
clustering	B
maximum	B
likelihood	B
and	O
clustering	B
useful	O
probability	B
distributions	O
exact	O
marginalization	B
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	B
in	O
graphs	O
laplace	B
s	O
method	O
laplace	B
s	O
method	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
monte	B
carlo	I
methods	I
monte	B
carlo	I
methods	I
ising	O
models	O
ising	O
models	O
exact	O
monte	B
carlo	I
sampling	O
exact	O
monte	B
carlo	I
sampling	O
variational	B
methods	I
variational	B
methods	I
independent	B
component	I
analysis	I
random	B
inference	B
topics	O
decision	B
theory	I
bayesian	B
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	B
vii	O
ii	O
noisy-channel	O
coding	O
monte	B
carlo	I
methods	I
monte	B
carlo	I
methods	I
very	B
good	B
linear	B
codes	I
exist	O
further	O
exercises	O
on	O
information	B
theory	I
v	O
neural	O
networks	O
message	B
passing	I
constrained	B
noiseless	B
channels	O
crosswords	O
and	O
codebreaking	O
why	O
have	O
sex	O
my	O
cambridge	O
course	O
on	O
information	B
theory	I
pattern	B
recognition	B
and	O
neural	O
networks	O
introduction	O
to	O
neural	O
networks	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	B
as	O
a	O
the	O
single	B
neuron	B
as	O
a	O
capacity	B
of	O
a	O
single	B
neuron	B
capacity	B
of	O
a	O
single	B
neuron	B
learning	B
as	B
inference	B
learning	B
as	B
inference	B
networks	O
networks	O
boltzmann	B
machines	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
gaussian	B
processes	I
deconvolution	B
vi	O
sparse	B
graph	B
codes	O
low-density	B
parity-check	I
codes	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
repeataccumulate	O
codes	O
digital	B
fountain	I
codes	O
viii	O
probability	B
entropy	B
and	O
inference	B
probability	B
entropy	B
and	O
inference	B
more	O
about	O
inference	B
i	O
data	O
compression	B
the	O
source	B
coding	B
theorem	I
the	O
source	B
coding	B
theorem	I
symbol	O
codes	O
symbol	O
codes	O
stream	B
codes	I
stream	B
codes	I
codes	O
for	B
integers	I
dependent	O
random	B
variables	O
dependent	O
random	B
variables	O
communication	B
over	O
a	O
noisy	B
channel	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
noisy-channel	B
coding	B
theorem	I
the	O
noisy-channel	B
coding	B
theorem	I
error-correcting	B
codes	I
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	B
theory	I
hash	O
codes	O
binary	O
codes	O
an	O
example	O
inference	B
task	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
maximum	B
likelihood	B
and	O
clustering	B
useful	O
probability	B
distributions	O
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	B
in	O
graphs	O
laplace	B
s	O
method	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
monte	B
carlo	I
methods	I
ising	O
models	O
exact	O
monte	B
carlo	I
sampling	O
variational	B
methods	I
independent	B
component	I
analysis	I
random	B
inference	B
topics	O
decision	B
theory	I
bayesian	B
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	B
preface	O
ii	O
noisy-channel	O
coding	O
monte	B
carlo	I
methods	I
very	B
good	B
linear	B
codes	I
exist	O
further	O
exercises	O
on	O
information	B
theory	I
v	O
neural	O
networks	O
message	B
passing	I
constrained	B
noiseless	B
channels	O
crosswords	O
and	O
codebreaking	O
why	O
have	O
sex	O
short	O
course	O
on	O
information	B
theory	I
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	B
as	O
a	O
capacity	B
of	O
a	O
single	B
neuron	B
learning	B
as	B
inference	B
networks	O
boltzmann	B
machines	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
gaussian	B
processes	I
deconvolution	B
vi	O
sparse	B
graph	B
codes	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
repeataccumulate	O
codes	O
digital	B
fountain	I
codes	O
preface	O
probability	B
entropy	B
and	O
inference	B
more	O
about	O
inference	B
i	O
data	O
compression	B
the	O
source	B
coding	B
theorem	I
symbol	O
codes	O
stream	B
codes	I
codes	O
for	B
integers	I
dependent	O
random	B
variables	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
noisy-channel	B
coding	B
theorem	I
error-correcting	B
codes	I
and	O
real	O
channels	O
error-correcting	B
codes	I
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	B
theory	I
hash	O
codes	O
hash	O
codes	O
binary	O
codes	O
binary	O
codes	O
an	O
example	O
inference	B
task	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
maximum	B
likelihood	B
and	O
clustering	B
useful	O
probability	B
distributions	O
exact	O
marginalization	B
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	B
in	O
graphs	O
exact	O
marginalization	B
in	O
graphs	O
laplace	B
s	O
method	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
monte	B
carlo	I
methods	I
ising	O
models	O
exact	O
monte	B
carlo	I
sampling	O
variational	B
methods	I
independent	B
component	I
analysis	I
random	B
inference	B
topics	O
decision	B
theory	I
bayesian	B
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	B
ix	O
ii	O
noisy-channel	O
coding	O
monte	B
carlo	I
methods	I
very	B
good	B
linear	B
codes	I
exist	O
very	B
good	B
linear	B
codes	I
exist	O
further	O
exercises	O
on	O
information	B
theory	I
further	O
exercises	O
on	O
information	B
theory	I
v	O
neural	O
networks	O
message	B
passing	I
message	B
passing	I
constrained	B
noiseless	B
channels	O
constrained	B
noiseless	B
channels	O
crosswords	O
and	O
codebreaking	O
why	O
have	O
sex	O
advanced	O
course	O
on	O
information	B
theory	I
and	O
coding	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	B
as	O
a	O
capacity	B
of	O
a	O
single	B
neuron	B
learning	B
as	B
inference	B
networks	O
boltzmann	B
machines	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
gaussian	B
processes	I
deconvolution	B
vi	O
sparse	B
graph	B
codes	O
low-density	B
parity-check	I
codes	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
convolutional	B
codes	O
and	O
turbo	B
codes	I
repeataccumulate	O
codes	O
repeataccumulate	O
codes	O
digital	B
fountain	I
codes	O
digital	B
fountain	I
codes	O
x	O
probability	B
entropy	B
and	O
inference	B
probability	B
entropy	B
and	O
inference	B
more	O
about	O
inference	B
more	O
about	O
inference	B
i	O
data	O
compression	B
the	O
source	B
coding	B
theorem	I
symbol	O
codes	O
stream	B
codes	I
codes	O
for	B
integers	I
dependent	O
random	B
variables	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
noisy-channel	B
coding	B
theorem	I
error-correcting	B
codes	I
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	B
theory	I
hash	O
codes	O
binary	O
codes	O
an	O
example	O
inference	B
task	O
clustering	B
an	O
example	O
inference	B
task	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
exact	O
inference	B
by	O
complete	O
enumeration	O
maximum	B
likelihood	B
and	O
clustering	B
maximum	B
likelihood	B
and	O
clustering	B
useful	O
probability	B
distributions	O
exact	O
marginalization	B
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	B
in	O
graphs	O
laplace	B
s	O
method	O
laplace	B
s	O
method	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
monte	B
carlo	I
methods	I
monte	B
carlo	I
methods	I
ising	O
models	O
ising	O
models	O
exact	O
monte	B
carlo	I
sampling	O
exact	O
monte	B
carlo	I
sampling	O
variational	B
methods	I
variational	B
methods	I
independent	B
component	I
analysis	I
independent	B
component	I
analysis	I
random	B
inference	B
topics	O
decision	B
theory	I
bayesian	B
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	B
preface	O
ii	O
noisy-channel	O
coding	O
monte	B
carlo	I
methods	I
monte	B
carlo	I
methods	I
very	B
good	B
linear	B
codes	I
exist	O
further	O
exercises	O
on	O
information	B
theory	I
v	O
neural	O
networks	O
message	B
passing	I
constrained	B
noiseless	B
channels	O
crosswords	O
and	O
codebreaking	O
why	O
have	O
sex	O
a	O
course	O
on	O
bayesian	B
inference	B
and	O
machine	B
learning	B
introduction	O
to	O
neural	O
networks	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	B
as	O
a	O
the	O
single	B
neuron	B
as	O
a	O
capacity	B
of	O
a	O
single	B
neuron	B
capacity	B
of	O
a	O
single	B
neuron	B
learning	B
as	B
inference	B
learning	B
as	B
inference	B
networks	O
networks	O
boltzmann	B
machines	O
boltzmann	B
machines	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
gaussian	B
processes	I
gaussian	B
processes	I
deconvolution	B
vi	O
sparse	B
graph	B
codes	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
repeataccumulate	O
codes	O
digital	B
fountain	I
codes	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
xi	O
preface	O
about	O
the	O
exercises	O
you	O
can	O
understand	O
a	O
subject	O
only	O
by	O
creating	O
it	O
for	O
yourself	O
the	O
exercises	O
play	O
an	O
essential	O
role	O
in	O
this	O
book	O
for	O
guidance	O
each	O
has	O
a	O
rating	O
to	O
that	O
used	O
by	O
knuth	O
from	O
to	O
to	O
indicate	O
its	O
in	O
addition	O
exercises	O
that	O
are	O
especially	O
recommended	O
are	O
marked	O
by	O
a	O
marginal	B
encouraging	O
rat	O
some	O
exercises	O
that	O
require	O
the	O
use	O
of	O
a	O
computer	B
are	O
marked	O
with	O
a	O
c	O
answers	O
to	O
many	O
exercises	O
are	O
provided	O
use	O
them	O
wisely	O
where	O
a	O
solution	O
is	O
provided	O
this	O
is	O
indicated	O
by	O
including	O
its	O
page	O
number	O
alongside	O
the	O
rating	O
solutions	O
to	O
many	O
of	O
the	O
other	O
exercises	O
will	O
be	O
supplied	O
to	O
instructors	O
using	O
this	O
book	O
in	O
their	O
teaching	O
please	O
email	B
solutionscambridge	O
org	O
summary	B
of	O
codes	O
for	O
exercises	O
especially	O
recommended	O
c	O
recommended	O
parts	O
require	O
a	O
computer	B
solution	O
provided	O
on	O
page	O
simple	O
minute	O
medium	O
hour	O
moderately	O
hard	O
hard	O
research	O
project	O
internet	B
resources	O
the	O
website	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
contains	O
several	O
resources	O
software	B
teaching	O
software	B
that	O
i	O
use	O
in	O
lectures	O
interactive	O
software	B
and	O
research	O
software	B
written	O
in	O
perl	O
octave	B
tcl	O
c	O
and	O
gnuplot	O
also	O
some	O
animations	O
corrections	O
to	O
the	O
book	O
thank	O
you	O
in	O
advance	O
for	O
emailing	O
these	O
this	O
book	O
the	O
book	O
is	O
provided	O
in	O
postscript	O
pdf	O
and	O
djvu	B
formats	O
for	O
on-screen	O
viewing	O
the	O
same	O
copyright	O
restrictions	O
apply	O
as	O
to	O
a	O
normal	B
book	O
about	O
this	O
edition	O
this	O
is	O
the	O
fourth	O
printing	O
of	O
the	O
edition	O
in	O
the	O
second	O
printing	O
the	O
design	O
of	O
the	O
book	O
was	O
altered	O
slightly	O
page-numbering	O
generally	O
remained	O
unchanged	O
except	O
in	O
chapters	O
and	O
where	O
a	O
few	O
paragraphs	O
and	O
equations	O
moved	O
around	O
all	O
equation	O
section	B
and	O
exercise	O
numbers	O
were	O
unchanged	O
in	O
the	O
third	O
printing	O
chapter	O
was	O
renamed	O
dependent	O
random	B
variables	O
instead	O
of	O
correlated	O
which	O
was	O
sloppy	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
preface	O
xii	O
acknowledgments	O
i	O
am	O
most	O
grateful	O
to	O
the	O
organizations	O
who	O
have	O
supported	O
me	O
while	O
this	O
book	O
gestated	O
the	O
royal	O
society	O
and	O
darwin	O
college	O
who	O
gave	O
me	O
a	O
fantastic	O
research	O
fellowship	O
in	O
the	O
early	O
years	O
the	O
university	O
of	O
cambridge	O
the	O
keck	O
centre	O
at	O
the	O
university	O
of	O
california	O
in	O
san	O
francisco	O
where	O
i	O
spent	O
a	O
productive	O
sabbatical	O
and	O
the	O
gatsby	O
charitable	O
foundation	O
whose	O
support	O
gave	O
me	O
the	O
freedom	O
to	O
break	O
out	O
of	O
the	O
escher	O
staircase	B
that	O
book-writing	O
had	O
become	O
my	O
work	O
has	O
depended	O
on	O
the	O
generosity	O
of	O
free	O
software	B
authors	O
i	O
wrote	O
the	O
book	O
in	O
latex	O
three	O
cheers	O
for	O
donald	O
knuth	O
and	O
leslie	O
lamport	O
our	O
computers	O
run	O
the	O
gnulinux	O
operating	O
system	O
i	O
use	O
emacs	O
perl	O
and	O
gnuplot	O
every	O
day	O
thank	O
you	O
richard	O
stallman	O
thank	O
you	O
linus	O
torvalds	O
thank	O
you	O
everyone	O
many	O
readers	O
too	O
numerous	O
to	O
name	O
here	O
have	O
given	O
feedback	B
on	O
the	O
book	O
and	O
to	O
them	O
all	O
i	O
extend	O
my	O
sincere	O
acknowledgments	O
i	O
especially	O
wish	O
to	O
thank	O
all	O
the	O
students	O
and	O
colleagues	O
at	O
cambridge	O
university	O
who	O
have	O
attended	O
my	O
lectures	O
on	O
information	B
theory	I
and	O
machine	B
learning	B
over	O
the	O
last	O
nine	O
years	O
the	O
members	O
of	O
the	O
inference	B
research	O
group	O
have	O
given	O
immense	O
support	O
and	O
i	O
thank	O
them	O
all	O
for	O
their	O
generosity	O
and	O
patience	O
over	O
the	O
last	O
ten	O
years	O
mark	O
gibbs	B
michelle	O
povinelli	O
simon	O
wilson	B
coryn	O
bailer-jones	O
matthew	O
davey	B
katriona	O
macphee	O
james	O
miskin	O
david	O
ward	O
edward	O
ratzer	O
seb	O
wills	O
john	O
barry	O
john	O
winn	O
phil	O
cowans	O
hanna	O
wallach	O
matthew	O
garrett	O
and	O
especially	O
sanjoy	O
mahajan	O
thank	O
you	O
too	O
to	O
graeme	O
mitchison	O
mike	O
cates	O
and	O
davin	O
yap	O
finally	O
i	O
would	O
like	O
to	O
express	O
my	O
debt	O
to	O
my	O
personal	O
heroes	O
the	O
mentors	O
from	O
whom	O
i	O
have	O
learned	O
so	O
much	O
yaser	O
abu-mostafa	B
andrew	O
blake	O
john	O
bridle	O
peter	O
cheeseman	O
steve	O
gull	B
hinton	B
john	O
steve	O
luttrell	O
robert	O
mackay	B
bob	B
mceliece	O
radford	O
neal	B
roger	O
sewell	O
and	O
john	O
skilling	B
dedication	O
this	O
book	O
is	O
dedicated	O
to	O
the	O
campaign	O
against	O
the	O
arms	O
trade	O
www	O
caat	O
org	O
uk	O
peace	O
cannot	O
be	O
kept	O
by	O
force	O
it	O
can	O
only	O
be	O
achieved	O
through	O
understanding	O
albert	O
einstein	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
in	O
the	O
chapter	O
you	O
will	O
need	O
to	O
be	O
familiar	O
with	O
the	O
binomial	B
distribution	B
and	O
to	O
solve	O
the	O
exercises	O
in	O
the	O
text	O
which	O
i	O
urge	O
you	O
to	O
do	O
you	O
will	O
need	O
to	O
know	O
stirling	B
s	O
approximation	B
for	O
the	O
factorial	B
function	O
x	O
xx	O
and	O
be	O
able	O
to	O
apply	O
it	O
to	O
r	O
these	O
topics	O
are	O
reviewed	O
below	O
n	O
unfamiliar	O
notation	B
see	O
appendix	O
a	O
the	O
binomial	B
distribution	B
example	O
a	O
bent	B
coin	B
has	O
probability	B
f	O
of	O
coming	O
up	O
heads	O
the	O
coin	B
is	O
tossed	O
n	O
times	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
number	O
of	O
heads	O
r	O
what	O
are	O
the	O
mean	B
and	O
variance	B
of	O
r	O
solution	O
the	O
number	O
of	O
heads	O
has	O
a	O
binomial	B
distribution	B
p	O
j	O
f	O
n	O
f	O
the	O
mean	B
er	O
and	O
variance	B
varr	O
of	O
this	O
distribution	B
are	O
by	O
r	O
n	O
p	O
j	O
f	O
n	O
r	O
figure	O
the	O
binomial	B
distribution	B
p	O
j	O
f	O
n	O
er	O
varr	O
ehr	O
n	O
p	O
j	O
f	O
n	O
rather	O
than	O
evaluating	O
the	O
sums	O
over	O
r	O
in	O
and	O
directly	O
it	O
is	O
easiest	O
to	O
obtain	O
the	O
mean	B
and	O
variance	B
by	O
noting	O
that	O
r	O
is	O
the	O
sum	O
of	O
n	O
independent	O
random	B
variables	O
namely	O
the	O
number	O
of	O
heads	O
in	O
the	O
toss	O
is	O
either	O
zero	O
or	O
one	O
the	O
number	O
of	O
heads	O
in	O
the	O
second	O
toss	O
and	O
so	O
forth	O
in	O
general	O
ex	O
y	O
ex	O
ey	O
varx	O
y	O
varx	O
vary	O
for	O
any	O
random	B
variables	O
x	O
and	O
y	O
if	O
x	O
and	O
y	O
are	O
independent	O
so	O
the	O
mean	B
of	O
r	O
is	O
the	O
sum	O
of	O
the	O
means	O
of	O
those	O
random	B
variables	O
and	O
the	O
variance	B
of	O
r	O
is	O
the	O
sum	O
of	O
their	O
variances	O
the	O
mean	B
number	O
of	O
heads	O
in	O
a	O
single	O
toss	O
is	O
f	O
f	O
f	O
and	O
the	O
variance	B
of	O
the	O
number	O
of	O
heads	O
in	O
a	O
single	O
toss	O
is	O
f	O
f	O
f	O
f	O
f	O
f	O
so	O
the	O
mean	B
and	O
variance	B
of	O
r	O
are	O
er	O
n	O
f	O
and	O
varr	O
n	O
f	O
f	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
approximating	O
x	O
and	O
let	O
s	O
derive	O
stirling	B
s	O
approximation	B
by	O
an	O
unconventional	O
route	O
we	O
start	O
from	O
the	O
poisson	B
distribution	B
with	O
mean	B
p	O
j	O
r	O
r	O
for	O
large	O
this	O
distribution	B
is	O
well	O
approximated	O
at	O
least	O
in	O
the	O
vicinity	O
of	O
r	O
by	O
a	O
gaussian	B
distribution	B
with	O
mean	B
and	O
variance	B
r	O
let	O
s	O
plug	O
r	O
into	O
this	O
formula	O
then	O
rearrange	O
it	O
this	O
is	O
stirling	B
s	O
approximation	B
for	O
the	O
factorial	B
function	O
x	O
xx	O
ln	O
x	O
x	O
ln	O
x	O
x	O
ln	O
r	O
figure	O
the	O
poisson	B
distribution	B
p	O
j	O
we	O
have	O
derived	O
not	O
only	O
the	O
leading	O
order	O
behaviour	O
x	O
xx	O
but	O
also	O
at	O
no	O
cost	O
the	O
next-order	O
correction	O
term	O
we	O
now	O
apply	O
stirling	B
s	O
approximation	B
to	O
ln	O
r	O
r	O
r	O
ln	O
n	O
n	O
r	O
r	O
ln	O
n	O
n	O
r	O
since	O
all	O
the	O
terms	O
in	O
this	O
equation	O
are	O
logarithms	B
this	O
result	O
can	O
be	O
rewritten	O
in	O
any	O
base	O
we	O
will	O
denote	O
natural	B
logarithms	B
e	O
by	O
ln	O
and	O
logarithms	B
to	O
base	O
by	O
log	O
if	O
we	O
introduce	O
the	O
binary	B
entropy	B
function	I
recall	O
that	O
x	O
loge	O
x	O
loge	O
loge	O
note	O
that	O
x	O
x	O
x	O
log	O
x	O
log	O
then	O
we	O
can	O
rewrite	O
the	O
approximation	B
as	O
or	O
equivalently	O
n	O
x	O
if	O
we	O
need	O
a	O
more	O
accurate	O
approximation	B
we	O
can	O
include	O
terms	O
of	O
the	O
next	O
order	O
from	O
stirling	B
s	O
approximation	B
figure	O
the	O
binary	B
entropy	B
function	I
n	O
n	O
r	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
the	O
fundamental	O
problem	O
of	O
communication	B
is	O
that	O
of	O
reproducing	O
at	O
one	O
point	O
either	O
exactly	O
or	O
approximately	O
a	O
message	O
selected	O
at	O
another	O
point	O
shannon	B
in	O
the	O
half	O
of	O
this	O
book	O
we	O
study	O
how	B
to	I
measure	I
information	B
content	I
we	O
learn	O
how	O
to	O
compress	B
data	O
and	O
we	O
learn	O
how	O
to	O
communicate	O
perfectly	O
over	O
imperfect	O
communication	B
channels	O
we	O
start	O
by	O
getting	O
a	O
feeling	O
for	O
this	O
last	O
problem	O
how	O
can	O
we	O
achieve	O
perfect	B
communication	B
over	O
an	O
imperfect	O
noisy	B
communication	B
channel	O
some	O
examples	O
of	O
noisy	B
communication	B
channels	O
are	O
an	O
analogue	O
telephone	O
line	O
over	O
which	O
two	O
modems	O
communicate	O
digital	O
information	B
the	O
radio	B
communication	B
link	O
from	O
galileo	O
the	O
jupiter-orbiting	O
space	O
craft	O
to	O
earth	O
reproducing	O
cells	O
in	O
which	O
the	O
daughter	O
cells	O
dna	B
contains	O
information	B
from	O
the	O
parent	B
cells	O
a	O
disk	B
drive	I
the	O
last	O
example	O
shows	O
that	O
communication	B
doesn	O
t	O
have	O
to	O
involve	O
information	B
going	O
from	O
one	O
place	O
to	O
another	O
when	O
we	O
write	O
a	O
on	O
a	O
disk	B
drive	I
we	O
ll	O
read	O
it	O
in	O
the	O
same	O
location	O
but	O
at	O
a	O
later	O
time	O
these	O
channels	O
are	O
noisy	B
a	O
telephone	O
line	O
from	O
cross-talk	O
with	O
other	O
lines	O
the	O
hardware	O
in	O
the	O
line	O
distorts	O
and	O
adds	O
noise	O
to	O
the	O
transmitted	O
signal	O
the	O
deep	O
space	O
network	B
that	O
listens	O
to	O
galileo	O
s	O
puny	O
transmitter	O
receives	O
background	O
radiation	O
from	O
terrestrial	O
and	O
cosmic	O
sources	O
dna	B
is	O
subject	O
to	O
mutations	O
and	O
damage	O
a	O
disk	B
drive	I
which	O
writes	O
a	O
binary	O
digit	O
one	O
or	O
zero	O
also	O
known	O
as	O
a	O
bit	B
by	O
aligning	O
a	O
patch	O
of	O
magnetic	O
material	O
in	O
one	O
of	O
two	O
orientations	O
may	O
later	O
fail	O
to	O
read	O
out	O
the	O
stored	O
binary	O
digit	O
the	O
patch	O
of	O
material	O
might	O
spontaneously	O
magnetization	O
or	O
a	O
glitch	O
of	O
background	O
noise	O
might	O
cause	O
the	O
reading	O
circuit	O
to	O
report	O
the	O
wrong	O
value	O
for	O
the	O
binary	O
digit	O
or	O
the	O
writing	B
head	O
might	O
not	O
induce	O
the	O
magnetization	O
in	O
the	O
place	O
because	O
of	O
interference	O
from	O
neighbouring	O
bits	O
in	O
all	O
these	O
cases	O
if	O
we	O
transmit	O
data	O
e	O
g	O
a	O
string	O
of	O
bits	O
over	O
the	O
channel	O
there	O
is	O
some	O
probability	B
that	O
the	O
received	O
message	O
will	O
not	O
be	O
identical	O
to	O
the	O
modem	O
phone	B
line	O
modem	O
galileo	O
radio	B
waves	O
earth	O
parent	B
cell	O
daughter	O
cell	O
daughter	O
cell	O
computer	B
memory	B
disk	B
drive	I
computer	B
memory	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
transmitted	O
message	O
we	O
would	O
prefer	O
to	O
have	O
a	O
communication	B
channel	O
for	O
which	O
this	O
probability	B
was	O
zero	O
or	O
so	O
close	O
to	O
zero	O
that	O
for	O
practical	B
purposes	O
it	O
is	O
indistinguishable	O
from	O
zero	O
let	O
s	O
consider	O
a	O
noisy	B
disk	B
drive	I
that	O
transmits	O
each	O
bit	B
correctly	O
with	O
probability	B
and	O
incorrectly	O
with	O
probability	B
f	O
this	O
model	B
communication	B
channel	O
is	O
known	O
as	O
the	O
binary	B
symmetric	B
channel	I
x	O
y	O
p	O
x	O
f	O
p	O
x	O
f	O
p	O
x	O
f	O
p	O
x	O
f	O
f	O
f	O
f	O
figure	O
the	O
binary	B
symmetric	B
channel	I
the	O
transmitted	O
symbol	O
is	O
x	O
and	O
the	O
received	O
symbol	O
y	O
the	O
noise	O
level	O
the	O
probability	B
that	O
a	O
bit	B
is	O
is	O
f	O
figure	O
a	O
binary	O
data	O
sequence	B
of	O
length	O
transmitted	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
f	O
image	B
copyright	O
united	O
feature	O
syndicate	O
inc	O
used	O
with	O
permission	O
as	O
an	O
example	O
let	O
s	O
imagine	O
that	O
f	O
that	O
is	O
ten	O
per	O
cent	O
of	O
the	O
bits	O
are	O
a	O
useful	O
disk	B
drive	I
would	O
no	O
bits	O
at	O
all	O
in	O
its	O
entire	O
lifetime	O
if	O
we	O
expect	O
to	O
read	O
and	O
write	O
a	O
gigabyte	O
per	O
day	O
for	O
ten	O
years	O
we	O
require	O
a	O
bit	B
error	B
probability	B
of	O
the	O
order	O
of	O
or	O
smaller	O
there	O
are	O
two	O
approaches	O
to	O
this	O
goal	O
the	O
physical	O
solution	O
the	O
physical	O
solution	O
is	O
to	O
improve	O
the	O
physical	O
characteristics	O
of	O
the	O
communication	B
channel	O
to	O
reduce	O
its	O
error	B
probability	B
we	O
could	O
improve	O
our	O
disk	B
drive	I
by	O
using	O
more	O
reliable	O
components	O
in	O
its	O
circuitry	O
evacuating	O
the	O
air	O
from	O
the	O
disk	O
enclosure	O
so	O
as	O
to	O
eliminate	O
the	O
turbu	O
lence	O
that	O
perturbs	O
the	O
reading	O
head	O
from	O
the	O
track	O
using	O
a	O
larger	O
magnetic	O
patch	O
to	O
represent	O
each	O
bit	B
or	O
using	O
higher-power	O
signals	O
or	O
cooling	O
the	O
circuitry	O
in	O
order	O
to	O
reduce	O
thermal	O
noise	O
these	O
physical	O
typically	O
increase	O
the	O
cost	O
of	O
the	O
communication	B
channel	O
the	O
system	O
solution	O
information	B
theory	I
and	O
coding	B
theory	I
an	O
alternative	O
much	O
more	O
exciting	O
approach	O
we	O
accept	O
the	O
given	O
noisy	B
channel	O
as	O
it	O
is	O
and	O
add	O
communication	B
systems	O
to	O
it	O
so	O
that	O
we	O
can	O
detect	O
and	O
correct	O
the	O
errors	B
introduced	O
by	O
the	O
channel	O
as	O
shown	O
in	O
we	O
add	O
an	O
encoder	B
before	O
the	O
channel	O
and	O
a	O
decoder	B
after	O
it	O
the	O
encoder	B
encodes	O
the	O
source	O
message	O
s	O
into	O
a	O
transmitted	O
message	O
t	O
adding	O
redundancy	B
to	O
the	O
original	O
message	O
in	O
some	O
way	O
the	O
channel	O
adds	O
noise	O
to	O
the	O
transmitted	O
message	O
yielding	O
a	O
received	O
message	O
r	O
the	O
decoder	B
uses	O
the	O
known	O
redundancy	B
introduced	O
by	O
the	O
encoding	O
system	O
to	O
infer	O
both	O
the	O
original	O
signal	O
s	O
and	O
the	O
added	O
noise	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	B
channel	I
source	O
s	O
encoder	B
t	O
decoder	B
r	O
noisy	B
channel	O
figure	O
the	O
system	O
solution	O
for	O
achieving	O
reliable	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
encoding	O
system	O
introduces	O
systematic	B
redundancy	B
into	O
the	O
transmitted	O
vector	O
t	O
the	O
decoding	B
system	O
uses	O
this	O
known	O
redundancy	B
to	O
deduce	O
from	O
the	O
received	O
vector	O
r	O
both	O
the	O
original	O
source	O
vector	O
and	O
the	O
noise	O
introduced	O
by	O
the	O
channel	O
whereas	O
physical	O
solutions	O
give	O
incremental	O
channel	O
improvements	O
only	O
at	O
an	O
ever-increasing	O
cost	O
system	O
solutions	O
can	O
turn	O
noisy	B
channels	O
into	O
reliable	O
communication	B
channels	O
with	O
the	O
only	O
cost	O
being	O
a	O
computational	O
requirement	O
at	O
the	O
encoder	B
and	O
decoder	B
information	B
theory	I
is	O
concerned	O
with	O
the	O
theoretical	O
limitations	O
and	O
po	O
what	O
is	O
the	O
best	O
error-correcting	O
performance	O
we	O
tentials	O
of	O
such	O
systems	O
could	O
achieve	O
coding	B
theory	I
is	O
concerned	O
with	O
the	O
creation	O
of	O
practical	B
encoding	O
and	O
decoding	B
systems	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	B
channel	I
we	O
now	O
consider	O
examples	O
of	O
encoding	O
and	O
decoding	B
systems	O
what	O
is	O
the	O
simplest	O
way	O
to	O
add	O
useful	O
redundancy	B
to	O
a	O
transmission	O
make	O
the	O
rules	B
of	O
the	O
game	O
clear	O
we	O
want	O
to	O
be	O
able	O
to	O
detect	O
and	O
correct	O
errors	B
and	O
retransmission	B
is	O
not	O
an	O
option	O
we	O
get	O
only	O
one	O
chance	O
to	O
encode	O
transmit	O
and	O
decode	O
repetition	B
codes	O
a	O
straightforward	O
idea	O
is	O
to	O
repeat	O
every	O
bit	B
of	O
the	O
message	O
a	O
prearranged	O
number	O
of	O
times	O
for	O
example	O
three	O
times	O
as	O
shown	O
in	O
table	O
we	O
call	O
this	O
repetition	B
code	I
imagine	O
that	O
we	O
transmit	O
the	O
source	O
message	O
s	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
f	O
using	O
this	O
repetition	B
code	I
we	O
can	O
describe	O
the	O
channel	O
as	O
adding	O
a	O
sparse	O
noise	O
vector	O
n	O
to	O
the	O
transmitted	O
vector	O
adding	O
in	O
modulo	O
arithmetic	O
i	O
e	O
the	O
binary	O
algebra	O
in	O
which	O
a	O
possible	O
noise	O
vector	O
n	O
and	O
received	O
vector	O
r	O
t	O
n	O
are	O
shown	O
in	O
source	O
sequence	B
transmitted	O
sequence	B
s	O
t	O
table	O
the	O
repetition	B
code	I
s	O
t	O
n	O
r	O
figure	O
an	O
example	O
transmission	O
using	O
how	O
should	O
we	O
decode	O
this	O
received	O
vector	O
the	O
optimal	B
algorithm	O
looks	O
at	O
the	O
received	O
bits	O
three	O
at	O
a	O
time	O
and	O
takes	O
a	O
majority	B
vote	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
received	O
sequence	B
r	O
likelihood	B
ratio	O
p	O
s	O
p	O
s	O
decoded	O
sequence	B
algorithm	O
majority-vote	O
decoding	B
algorithm	O
for	O
also	O
shown	O
are	O
the	O
likelihood	B
ratios	O
assuming	O
the	O
channel	O
is	O
a	O
binary	B
symmetric	B
channel	I
f	O
at	O
the	O
risk	O
of	O
explaining	O
the	O
obvious	O
let	O
s	O
prove	O
this	O
result	O
the	O
optimal	B
decoding	B
decision	O
in	O
the	O
sense	O
of	O
having	O
the	O
smallest	O
probability	B
of	O
being	O
wrong	O
is	O
to	O
which	O
value	O
of	O
s	O
is	O
most	O
probable	O
given	O
r	O
consider	O
the	O
decoding	B
of	O
a	O
single	O
bit	B
s	O
which	O
was	O
encoded	O
as	O
ts	O
and	O
gave	O
rise	O
to	O
three	O
received	O
bits	O
r	O
by	O
bayes	B
theorem	O
the	O
posterior	B
probability	B
of	O
s	O
is	O
p	O
p	O
j	O
sp	O
p	O
we	O
can	O
spell	B
out	O
the	O
posterior	B
probability	B
of	O
the	O
two	O
alternatives	O
thus	O
p	O
p	O
j	O
s	O
p	O
p	O
p	O
j	O
s	O
p	O
this	O
posterior	B
probability	B
is	O
determined	O
by	O
two	O
factors	O
the	O
prior	B
probability	B
p	O
and	O
the	O
data-dependent	O
term	O
p	O
j	O
s	O
which	O
is	O
called	O
the	O
likelihood	B
of	O
s	O
the	O
normalizing	O
constant	O
p	O
needn	O
t	O
be	O
computed	O
when	O
the	O
optimal	B
decoding	B
decision	O
which	O
is	O
to	O
guess	O
if	O
p	O
r	O
p	O
r	O
and	O
otherwise	O
to	O
p	O
r	O
and	O
p	O
r	O
we	O
must	O
make	O
an	O
assumption	O
about	O
the	O
prior	B
probabilities	O
of	O
the	O
two	O
hypotheses	O
s	O
and	O
s	O
and	O
we	O
must	O
make	O
an	O
assumption	O
about	O
the	O
probability	B
of	O
r	O
given	O
s	O
we	O
assume	O
that	O
the	O
prior	B
probabilities	O
are	O
equal	O
p	O
p	O
then	O
maximizing	O
the	O
posterior	B
probability	B
p	O
r	O
is	O
equivalent	O
to	O
maximizing	O
the	O
likelihood	B
p	O
s	O
and	O
we	O
assume	O
that	O
the	O
channel	O
is	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
f	O
so	O
that	O
the	O
likelihood	B
is	O
p	O
s	O
p	O
ts	O
n	O
p	O
j	O
tns	O
where	O
n	O
is	O
the	O
number	O
of	O
transmitted	O
bits	O
in	O
the	O
block	B
we	O
are	O
considering	O
and	O
p	O
j	O
tn	O
f	O
if	O
if	O
rn	O
tn	O
rn	O
tn	O
thus	O
the	O
likelihood	B
ratio	O
for	O
the	O
two	O
hypotheses	O
is	O
n	O
p	O
s	O
p	O
s	O
p	O
equals	O
each	O
factor	O
p	O
if	O
rn	O
the	O
ratio	O
is	O
greater	O
than	O
since	O
f	O
so	O
the	O
winning	O
hypothesis	O
is	O
the	O
one	O
with	O
the	O
most	O
votes	O
each	O
vote	O
counting	B
for	O
a	O
factor	O
of	O
in	O
the	O
likelihood	B
ratio	O
p	O
j	O
p	O
j	O
if	O
rn	O
and	O
f	O
f	O
f	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	B
channel	I
thus	O
the	O
majority-vote	O
decoder	B
shown	O
in	O
algorithm	O
is	O
the	O
optimal	B
decoder	B
if	O
we	O
assume	O
that	O
the	O
channel	O
is	O
a	O
binary	B
symmetric	B
channel	I
and	O
that	O
the	O
two	O
possible	O
source	O
messages	O
and	O
have	O
equal	O
prior	B
probability	B
we	O
now	O
apply	O
the	O
majority	B
vote	I
decoder	B
to	O
the	O
received	O
vector	O
of	O
the	O
three	O
received	O
bits	O
are	O
all	O
so	O
we	O
decode	O
this	O
triplet	O
as	O
a	O
in	O
the	O
second	O
triplet	O
of	O
there	O
are	O
two	O
and	O
one	O
so	O
we	O
decode	O
this	O
triplet	O
as	O
a	O
which	O
in	O
this	O
case	O
corrects	O
the	O
error	O
not	O
all	O
errors	B
are	O
corrected	O
however	O
if	O
we	O
are	O
unlucky	O
and	O
two	O
errors	B
fall	O
in	O
a	O
single	O
block	B
as	O
in	O
the	O
triplet	O
of	O
then	O
the	O
decoding	B
rule	O
gets	O
the	O
wrong	O
answer	O
as	O
shown	O
in	O
s	O
figure	O
decoding	B
the	O
received	O
vector	O
from	O
t	O
n	O
r	O
corrected	O
errors	B
undetected	O
errors	B
exercise	O
show	O
that	O
the	O
error	B
probability	B
is	O
reduced	O
by	O
the	O
use	O
of	O
by	O
computing	O
the	O
error	B
probability	B
of	O
this	O
code	O
for	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
f	O
the	O
error	B
probability	B
is	O
dominated	O
by	O
the	O
probability	B
that	O
two	O
bits	O
in	O
a	O
block	B
of	O
three	O
are	O
which	O
scales	O
as	O
f	O
in	O
the	O
case	O
of	O
the	O
binary	B
symmetric	B
channel	I
with	O
f	O
the	O
code	O
has	O
a	O
probability	B
of	I
error	I
after	O
decoding	B
of	O
pb	O
per	O
bit	B
figure	O
shows	O
the	O
result	O
of	O
transmitting	O
a	O
binary	O
image	B
over	O
a	O
binary	B
symmetric	B
channel	I
using	O
the	O
repetition	B
code	I
the	O
exercise	O
s	O
rating	O
e	O
g	O
indicates	O
its	O
exercises	O
are	O
the	O
easiest	O
exercises	O
that	O
are	O
accompanied	O
by	O
a	O
marginal	B
rat	O
are	O
especially	O
recommended	O
if	O
a	O
solution	O
or	O
partial	B
solution	O
is	O
provided	O
the	O
page	O
is	O
indicated	O
after	O
the	O
rating	O
for	O
example	O
this	O
exercise	O
s	O
solution	O
is	O
on	O
page	O
s	O
encoder	B
t	O
channel	O
f	O
r	O
decoder	B
figure	O
transmitting	O
source	O
bits	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
f	O
using	O
a	O
repetition	B
code	I
and	O
the	O
majority	B
vote	I
decoding	B
algorithm	O
the	O
probability	B
of	O
decoded	O
bit	B
error	O
has	O
fallen	O
to	O
about	O
the	O
rate	B
has	O
fallen	O
to	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
more	O
useful	O
codes	O
pb	O
figure	O
error	B
probability	B
pb	O
versus	O
rate	B
for	O
repetition	B
codes	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
f	O
the	O
right-hand	O
shows	O
pb	O
on	O
a	O
logarithmic	O
scale	O
we	O
would	O
like	O
the	O
rate	B
to	O
be	O
large	O
and	O
pb	O
to	O
be	O
small	O
more	O
useful	O
codes	O
rate	B
rate	B
the	O
repetition	B
code	I
has	O
therefore	O
reduced	O
the	O
probability	B
of	I
error	I
as	O
desired	O
yet	O
we	O
have	O
lost	O
something	O
our	O
rate	B
of	O
information	B
transfer	O
has	O
fallen	O
by	O
a	O
factor	O
of	O
three	O
so	O
if	O
we	O
use	O
a	O
repetition	B
code	I
to	O
communicate	O
data	O
over	O
a	O
telephone	O
line	O
it	O
will	O
reduce	O
the	O
error	O
frequency	B
but	O
it	O
will	O
also	O
reduce	O
our	O
communication	B
rate	B
we	O
will	O
have	O
to	O
pay	O
three	O
times	O
as	O
much	O
for	O
each	O
phone	B
call	O
similarly	O
we	O
would	O
need	O
three	O
of	O
the	O
original	O
noisy	B
gigabyte	O
disk	O
drives	O
in	O
order	O
to	O
create	O
a	O
one-gigabyte	O
disk	B
drive	I
with	O
pb	O
can	O
we	O
push	O
the	O
error	B
probability	B
lower	O
to	O
the	O
values	O
required	O
for	O
a	O
sellable	O
disk	B
drive	I
we	O
could	O
achieve	O
lower	O
error	O
probabilities	O
by	O
using	O
repetition	B
codes	O
with	O
more	O
repetitions	O
exercise	O
show	O
that	O
the	O
probability	B
of	I
error	I
of	O
rn	O
the	O
repe	O
tition	O
code	O
with	O
n	O
repetitions	O
is	O
pb	O
for	O
odd	O
n	O
n	O
xnn	O
f	O
assuming	O
f	O
which	O
of	O
the	O
terms	O
in	O
this	O
sum	O
is	O
the	O
biggest	O
how	O
much	O
bigger	O
is	O
it	O
than	O
the	O
second-biggest	O
term	O
use	O
stirling	B
s	O
approximation	B
to	O
approximate	O
the	O
largest	O
term	O
and	O
approximately	O
the	O
probability	B
of	I
error	I
of	O
the	O
repetition	B
code	I
with	O
n	O
repetitions	O
in	O
the	O
assuming	O
f	O
how	O
many	O
repetitions	O
are	O
required	O
to	O
get	O
the	O
probability	B
of	I
error	I
down	O
to	O
about	O
so	O
to	O
build	O
a	O
single	O
gigabyte	O
disk	B
drive	I
with	O
the	O
required	O
reliability	O
from	O
noisy	B
gigabyte	O
drives	O
with	O
f	O
we	O
would	O
need	O
sixty	O
of	O
the	O
noisy	B
disk	O
drives	O
the	O
between	O
error	B
probability	B
and	O
rate	B
for	O
repetition	B
codes	O
is	O
shown	O
in	O
block	B
codes	O
the	O
hamming	B
code	I
we	O
would	O
like	O
to	O
communicate	O
with	O
tiny	O
probability	B
of	I
error	I
and	O
at	O
a	O
substantial	O
rate	B
can	O
we	O
improve	O
on	O
repetition	B
codes	O
what	O
if	O
we	O
add	O
redundancy	B
to	O
blocks	O
of	O
data	O
instead	O
of	O
encoding	O
one	O
bit	B
at	O
a	O
time	O
we	O
now	O
study	O
a	O
simple	O
block	B
code	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	B
channel	I
a	O
block	B
code	I
is	O
a	O
rule	O
for	O
converting	O
a	O
sequence	B
of	O
source	O
bits	O
s	O
of	O
length	O
k	O
say	O
into	O
a	O
transmitted	O
sequence	B
t	O
of	O
length	O
n	O
bits	O
to	O
add	O
redundancy	B
we	O
make	O
n	O
greater	O
than	O
k	O
in	O
a	O
linear	B
block	B
code	I
the	O
extra	O
n	O
k	O
bits	O
are	O
linear	B
functions	B
of	O
the	O
original	O
k	O
bits	O
these	O
extra	O
bits	O
are	O
called	O
parity-check	B
bits	I
an	O
example	O
of	O
a	O
linear	B
block	B
code	I
is	O
the	O
hamming	B
code	I
which	O
transmits	O
n	O
bits	O
for	O
every	O
k	O
source	O
bits	O
t	O
s	O
s	O
t	O
s	O
t	O
figure	O
pictorial	O
representation	O
of	O
encoding	O
for	O
the	O
hamming	B
code	I
the	O
encoding	O
operation	O
for	O
the	O
code	O
is	O
shown	O
pictorially	O
in	O
we	O
arrange	O
the	O
seven	O
transmitted	O
bits	O
in	O
three	O
intersecting	O
circles	O
the	O
four	O
transmitted	O
bits	O
are	O
set	B
equal	O
to	O
the	O
four	O
source	O
bits	O
the	O
parity-check	B
bits	I
are	O
set	B
so	O
that	O
the	O
parity	B
within	O
each	O
circle	B
is	O
even	O
the	O
parity-check	O
bit	B
is	O
the	O
parity	B
of	O
the	O
three	O
source	O
bits	O
is	O
it	O
is	O
if	O
the	O
sum	O
of	O
those	O
bits	O
is	O
even	O
and	O
if	O
the	O
sum	O
is	O
odd	O
the	O
second	O
is	O
the	O
parity	B
of	O
the	O
last	O
three	O
and	O
the	O
third	O
parity	B
bit	B
is	O
the	O
parity	B
of	O
source	O
bits	O
one	O
three	O
and	O
four	O
as	O
an	O
example	O
shows	O
the	O
transmitted	O
codeword	B
for	O
the	O
case	O
s	O
table	O
shows	O
the	O
codewords	O
generated	O
by	O
each	O
of	O
the	O
sixteen	O
settings	O
of	O
the	O
four	O
source	O
bits	O
these	O
codewords	O
have	O
the	O
special	O
property	O
that	O
any	O
pair	O
from	O
each	O
other	O
in	O
at	O
least	O
three	O
bits	O
s	O
t	O
s	O
t	O
s	O
t	O
s	O
t	O
table	O
the	O
sixteen	O
codewords	O
ftg	O
of	O
the	O
hamming	B
code	I
any	O
pair	O
of	O
codewords	O
from	O
each	O
other	O
in	O
at	O
least	O
three	O
bits	O
because	O
the	O
hamming	B
code	I
is	O
a	O
linear	B
code	O
it	O
can	O
be	O
written	O
compactly	O
in	O
terms	O
of	O
matrices	B
as	O
follows	O
the	O
transmitted	O
codeword	B
t	O
is	O
obtained	O
from	O
the	O
source	O
sequence	B
s	O
by	O
a	O
linear	B
operation	O
t	O
gts	O
where	O
g	O
is	O
the	O
generator	B
matrix	B
of	O
the	O
code	O
gt	O
and	O
the	O
encoding	O
operation	O
uses	O
arithmetic	O
etc	O
in	O
the	O
encoding	O
operation	O
i	O
have	O
assumed	O
that	O
s	O
and	O
t	O
are	O
column	O
vectors	B
if	O
instead	O
they	O
are	O
row	O
vectors	B
then	O
this	O
equation	O
is	O
replaced	O
by	O
t	O
sg	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
where	O
introduction	O
to	O
information	B
theory	I
g	O
i	O
it	O
easier	O
to	O
relate	O
to	O
the	O
right-multiplication	O
than	O
the	O
left-multiplication	O
many	O
coding	B
theory	I
texts	O
use	O
the	O
left-multiplying	O
conventions	O
however	O
the	O
rows	O
of	O
the	O
generator	B
matrix	B
can	O
be	O
viewed	O
as	O
four	O
basis	O
vectors	B
lying	O
in	O
a	O
seven-dimensional	O
binary	O
space	O
the	O
sixteen	O
codewords	O
are	O
obtained	O
by	O
making	O
all	O
possible	O
linear	B
combinations	O
of	O
these	O
vectors	B
decoding	B
the	O
hamming	B
code	I
when	O
we	O
invent	O
a	O
more	O
complex	B
encoder	B
s	O
t	O
the	O
task	O
of	O
decoding	B
the	O
received	O
vector	O
r	O
becomes	O
less	O
straightforward	O
remember	O
that	O
any	O
of	O
the	O
bits	O
may	O
have	O
been	O
including	O
the	O
parity	B
bits	O
if	O
we	O
assume	O
that	O
the	O
channel	O
is	O
a	O
binary	B
symmetric	B
channel	I
and	O
that	O
all	O
source	O
vectors	B
are	O
equiprobable	O
then	O
the	O
optimal	B
decoder	B
the	O
source	O
vector	O
s	O
whose	O
encoding	O
ts	O
from	O
the	O
received	O
vector	O
r	O
in	O
the	O
fewest	O
bits	O
to	O
the	O
likelihood	B
function	O
to	O
see	O
why	O
this	O
is	O
so	O
we	O
could	O
solve	O
the	O
decoding	B
problem	O
by	O
measuring	O
how	O
far	O
r	O
is	O
from	O
each	O
of	O
the	O
sixteen	O
codewords	O
in	O
table	O
then	O
picking	O
the	O
closest	O
is	O
there	O
a	O
more	O
way	O
of	O
the	O
most	O
probable	O
source	O
vector	O
syndrome	B
decoding	B
for	O
the	O
hamming	B
code	I
for	O
the	O
hamming	B
code	I
there	O
is	O
a	O
pictorial	O
solution	O
to	O
the	O
decoding	B
problem	O
based	O
on	O
the	O
encoding	O
picture	O
as	O
a	O
example	O
let	O
s	O
assume	O
the	O
transmission	O
was	O
t	O
and	O
the	O
noise	O
the	O
second	O
bit	B
so	O
the	O
received	O
vector	O
is	O
r	O
we	O
write	O
the	O
received	O
vector	O
into	O
the	O
three	O
circles	O
as	O
shown	O
in	O
and	O
look	O
at	O
each	O
of	O
the	O
three	O
circles	O
to	O
see	O
whether	O
its	O
parity	B
is	O
even	O
the	O
circles	O
whose	O
parity	B
is	O
not	O
even	O
are	O
shown	O
by	O
dashed	O
lines	O
in	O
the	O
decoding	B
task	O
is	O
to	O
the	O
smallest	O
set	B
of	O
bits	O
that	O
can	O
account	O
for	O
these	O
violations	O
of	O
the	O
parity	B
rules	B
pattern	O
of	O
violations	O
of	O
the	O
parity	B
checks	O
is	O
called	O
the	O
syndrome	B
and	O
can	O
be	O
written	O
as	O
a	O
binary	O
vector	O
for	O
example	O
in	O
the	O
syndrome	B
is	O
z	O
because	O
the	O
two	O
circles	O
are	O
unhappy	O
and	O
the	O
third	O
circle	B
is	O
happy	O
to	O
solve	O
the	O
decoding	B
task	O
we	O
ask	O
the	O
question	O
can	O
we	O
a	O
unique	O
bit	B
that	O
lies	O
inside	O
all	O
the	O
unhappy	O
circles	O
and	O
outside	O
all	O
the	O
happy	O
circles	O
if	O
so	O
the	O
of	O
that	O
bit	B
would	O
account	O
for	O
the	O
observed	O
syndrome	B
in	O
the	O
case	O
shown	O
in	O
the	O
bit	B
lies	O
inside	O
the	O
two	O
unhappy	O
circles	O
and	O
outside	O
the	O
happy	O
circle	B
no	O
other	O
single	O
bit	B
has	O
this	O
property	O
so	O
is	O
the	O
only	O
single	O
bit	B
capable	O
of	O
explaining	O
the	O
syndrome	B
let	O
s	O
work	O
through	O
a	O
couple	O
more	O
examples	O
figure	O
shows	O
what	O
happens	O
if	O
one	O
of	O
the	O
parity	B
bits	O
is	O
by	O
the	O
noise	O
just	O
one	O
of	O
the	O
checks	O
is	O
violated	O
only	O
lies	O
inside	O
this	O
unhappy	O
circle	B
and	O
outside	O
the	O
other	O
two	O
happy	O
circles	O
so	O
is	O
as	O
the	O
only	O
single	O
bit	B
capable	O
of	O
explaining	O
the	O
syndrome	B
if	O
the	O
central	O
bit	B
is	O
received	O
shows	O
that	O
all	O
three	O
checks	O
are	O
violated	O
only	O
lies	O
inside	O
all	O
three	O
circles	O
so	O
is	O
as	O
the	O
suspect	O
bit	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	B
channel	I
figure	O
pictorial	O
representation	O
of	O
decoding	B
of	O
the	O
hamming	B
code	I
the	O
received	O
vector	O
is	O
written	O
into	O
the	O
diagram	O
as	O
shown	O
in	O
in	O
the	O
received	O
vector	O
is	O
shown	O
assuming	O
that	O
the	O
transmitted	O
vector	O
was	O
as	O
in	O
and	O
the	O
bits	O
labelled	O
by	O
were	O
the	O
violated	O
parity	B
checks	O
are	O
highlighted	O
by	O
dashed	O
circles	O
one	O
of	O
the	O
seven	O
bits	O
is	O
the	O
most	O
probable	O
suspect	O
to	O
account	O
for	O
each	O
syndrome	B
i	O
e	O
each	O
pattern	O
of	O
violated	O
and	O
parity	B
checks	O
in	O
examples	O
and	O
the	O
most	O
probable	O
suspect	O
is	O
the	O
one	O
bit	B
that	O
was	O
in	O
example	O
two	O
bits	O
have	O
been	O
and	O
the	O
most	O
probable	O
suspect	O
is	O
marked	O
by	O
a	O
circle	B
in	O
which	O
shows	O
the	O
output	O
of	O
the	O
decoding	B
algorithm	O
algorithm	O
actions	O
taken	O
by	O
the	O
optimal	B
decoder	B
for	O
the	O
hamming	B
code	I
assuming	O
a	O
binary	B
symmetric	B
channel	I
with	O
small	O
noise	O
level	O
f	O
the	O
syndrome	B
vector	O
z	O
lists	O
whether	O
each	O
parity	B
check	O
is	O
violated	O
or	O
going	O
through	O
the	O
checks	O
in	O
the	O
order	O
of	O
the	O
bits	O
and	O
r	O
r	O
r	O
r	O
r	O
r	O
syndrome	B
z	O
this	O
bit	B
none	O
if	O
you	O
try	O
any	O
one	O
of	O
the	O
seven	O
bits	O
you	O
ll	O
that	O
a	O
syndrome	B
is	O
obtained	O
in	O
each	O
case	O
seven	O
non-zero	O
syndromes	O
one	O
for	O
each	O
bit	B
there	O
is	O
only	O
one	O
other	O
syndrome	B
the	O
all-zero	O
syndrome	B
so	O
if	O
the	O
channel	O
is	O
a	O
binary	B
symmetric	B
channel	I
with	O
a	O
small	O
noise	O
level	O
f	O
the	O
optimal	B
decoder	B
at	O
most	O
one	O
bit	B
depending	O
on	O
the	O
syndrome	B
as	O
shown	O
in	O
algorithm	O
each	O
syndrome	B
could	O
have	O
been	O
caused	O
by	O
other	O
noise	O
patterns	O
too	O
but	O
any	O
other	O
noise	O
pattern	O
that	O
has	O
the	O
same	O
syndrome	B
must	O
be	O
less	O
probable	O
because	O
it	O
involves	O
a	O
larger	O
number	O
of	O
noise	O
events	O
what	O
happens	O
if	O
the	O
noise	O
actually	O
more	O
than	O
one	O
bit	B
figure	O
shows	O
the	O
situation	O
when	O
two	O
bits	O
and	O
are	O
received	O
the	O
syndrome	B
makes	O
us	O
suspect	O
the	O
single	O
bit	B
so	O
our	O
optimal	B
decoding	B
algorithm	O
this	O
bit	B
giving	O
a	O
decoded	O
pattern	O
with	O
three	O
errors	B
as	O
shown	O
in	O
if	O
we	O
use	O
the	O
optimal	B
decoding	B
algorithm	O
any	O
two-bit	O
error	O
pattern	O
will	O
lead	O
to	O
a	O
decoded	O
seven-bit	O
vector	O
that	O
contains	O
three	O
errors	B
general	O
view	O
of	O
decoding	B
for	O
linear	B
codes	I
syndrome	B
decoding	B
we	O
can	O
also	O
describe	O
the	O
decoding	B
problem	O
for	O
a	O
linear	B
code	O
in	O
terms	O
of	O
matrices	B
the	O
four	O
received	O
bits	O
purport	O
to	O
be	O
the	O
four	O
source	O
bits	O
and	O
the	O
received	O
bits	O
purport	O
to	O
be	O
the	O
parities	O
of	O
the	O
source	O
bits	O
as	O
by	O
the	O
generator	B
matrix	B
g	O
we	O
evaluate	O
the	O
three	O
parity-check	B
bits	I
for	O
the	O
received	O
bits	O
and	O
see	O
whether	O
they	O
match	O
the	O
three	O
received	O
bits	O
the	O
between	O
these	O
two	O
triplets	O
are	O
called	O
the	O
syndrome	B
of	O
the	O
received	O
vector	O
if	O
the	O
syndrome	B
is	O
zero	O
if	O
all	O
three	O
parity	B
checks	O
are	O
happy	O
then	O
the	O
received	O
vector	O
is	O
a	O
codeword	B
and	O
the	O
most	O
probable	O
decoding	B
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
s	O
encoder	B
t	O
channel	O
f	O
r	O
decoder	B
figure	O
transmitting	O
source	O
bits	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
f	O
using	O
a	O
hamming	B
code	I
the	O
probability	B
of	O
decoded	O
bit	B
error	O
is	O
about	O
parity	B
bits	O
given	O
by	O
reading	O
out	O
its	O
four	O
bits	O
if	O
the	O
syndrome	B
is	O
non-zero	O
then	O
the	O
noise	O
sequence	B
for	O
this	O
block	B
was	O
non-zero	O
and	O
the	O
syndrome	B
is	O
our	O
pointer	B
to	O
the	O
most	O
probable	O
error	O
pattern	O
the	O
computation	O
of	O
the	O
syndrome	B
vector	O
is	O
a	O
linear	B
operation	O
if	O
we	O
the	O
matrix	B
p	O
such	O
that	O
the	O
matrix	B
of	O
equation	O
is	O
p	O
gt	O
where	O
is	O
the	O
identity	B
matrix	B
then	O
the	O
syndrome	B
vector	O
is	O
z	O
hr	O
where	O
the	O
parity-check	B
matrix	B
h	O
is	O
given	O
by	O
h	O
in	O
modulo	O
arithmetic	O
so	O
h	O
p	O
all	O
the	O
codewords	O
t	O
gts	O
of	O
the	O
code	O
satisfy	O
ht	O
exercise	O
prove	O
that	O
this	O
is	O
so	O
by	O
evaluating	O
the	O
matrix	B
hgt	O
since	O
the	O
received	O
vector	O
r	O
is	O
given	O
by	O
r	O
gts	O
n	O
the	O
syndrome-decoding	O
problem	O
is	O
to	O
the	O
most	O
probable	O
noise	O
vector	O
n	O
satisfying	O
the	O
equation	O
hn	O
z	O
a	O
decoding	B
algorithm	O
that	O
solves	O
this	O
problem	O
is	O
called	O
a	O
maximum-likelihood	O
decoder	B
we	O
will	O
discuss	O
decoding	B
problems	O
like	O
this	O
in	O
later	O
chapters	O
summary	B
of	O
the	O
hamming	B
code	I
s	O
properties	O
every	O
possible	O
received	O
vector	O
of	O
length	O
bits	O
is	O
either	O
a	O
codeword	B
or	O
it	O
s	O
one	O
away	O
from	O
a	O
codeword	B
since	O
there	O
are	O
three	O
parity	B
constraints	O
each	O
of	O
which	O
might	O
or	O
might	O
not	O
be	O
violated	O
there	O
are	O
distinct	O
syndromes	O
they	O
can	O
be	O
divided	O
into	O
seven	O
non-zero	O
syndromes	O
one	O
for	O
each	O
of	O
the	O
one-bit	O
error	O
patterns	O
and	O
the	O
all-zero	O
syndrome	B
corresponding	O
to	O
the	O
zero-noise	O
case	O
the	O
optimal	B
decoder	B
takes	O
no	O
action	O
if	O
the	O
syndrome	B
is	O
zero	O
otherwise	O
it	O
uses	O
this	O
mapping	B
of	O
non-zero	O
syndromes	O
onto	O
one-bit	O
error	O
patterns	O
to	O
the	O
suspect	O
bit	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	B
channel	I
there	O
is	O
a	O
decoding	B
error	O
if	O
the	O
four	O
decoded	O
bits	O
do	O
not	O
all	O
match	O
the	O
source	O
bits	O
the	O
probability	B
of	I
block	B
error	I
pb	O
is	O
the	O
probability	B
that	O
one	O
or	O
more	O
of	O
the	O
decoded	O
bits	O
in	O
one	O
block	B
fail	O
to	O
match	O
the	O
corresponding	O
source	O
bits	O
the	O
probability	B
of	O
bit	B
error	O
pb	O
is	O
the	O
average	O
probability	B
that	O
a	O
decoded	O
bit	B
fails	O
to	O
match	O
the	O
corresponding	O
source	O
bit	B
pb	O
p	O
s	O
pb	O
k	O
k	O
p	O
sk	O
in	O
the	O
case	O
of	O
the	O
hamming	B
code	I
a	O
decoding	B
error	O
will	O
occur	O
whenever	O
the	O
noise	O
has	O
more	O
than	O
one	O
bit	B
in	O
a	O
block	B
of	O
seven	O
the	O
probability	B
of	I
block	B
error	I
is	O
thus	O
the	O
probability	B
that	O
two	O
or	O
more	O
bits	O
are	O
in	O
a	O
block	B
this	O
probability	B
scales	O
as	O
of	O
as	O
did	O
the	O
probability	B
of	I
error	I
for	O
the	O
repetition	B
code	I
but	O
notice	O
that	O
the	O
hamming	B
code	I
communicates	O
at	O
a	O
greater	O
rate	B
r	O
figure	O
shows	O
a	O
binary	O
image	B
transmitted	O
over	O
a	O
binary	B
symmetric	B
channel	I
using	O
the	O
hamming	B
code	I
about	O
of	O
the	O
decoded	O
bits	O
are	O
in	O
error	O
notice	O
that	O
the	O
errors	B
are	O
correlated	O
often	O
two	O
or	O
three	O
successive	O
decoded	O
bits	O
are	O
exercise	O
this	O
exercise	O
and	O
the	O
next	O
three	O
refer	O
to	O
the	O
hamming	B
code	I
decode	O
the	O
received	O
strings	O
r	O
r	O
r	O
r	O
exercise	O
calculate	O
the	O
probability	B
of	I
block	B
error	I
pb	O
of	O
the	O
hamming	B
code	I
as	O
a	O
function	O
of	O
the	O
noise	O
level	O
f	O
and	O
show	O
that	O
to	O
leading	O
order	O
it	O
goes	O
as	O
show	O
that	O
to	O
leading	O
order	O
the	O
probability	B
of	O
bit	B
error	O
pb	O
goes	O
as	O
exercise	O
find	O
some	O
noise	O
vectors	B
that	O
give	O
the	O
all-zero	O
syndrome	B
is	O
noise	O
vectors	B
that	O
leave	O
all	O
the	O
parity	B
checks	O
unviolated	O
how	O
many	O
such	O
noise	O
vectors	B
are	O
there	O
exercise	O
i	O
asserted	O
above	O
that	O
a	O
block	B
decoding	B
error	O
will	O
result	O
whenever	O
two	O
or	O
more	O
bits	O
are	O
in	O
a	O
single	O
block	B
show	O
that	O
this	O
is	O
indeed	O
so	O
principle	O
there	O
might	O
be	O
error	O
patterns	O
that	O
after	O
decoding	B
led	O
only	O
to	O
the	O
corruption	O
of	O
the	O
parity	B
bits	O
with	O
no	O
source	O
bits	O
incorrectly	O
decoded	O
summary	B
of	O
codes	O
performances	O
figure	O
shows	O
the	O
performance	O
of	O
repetition	B
codes	O
and	O
the	O
hamming	B
code	I
it	O
also	O
shows	O
the	O
performance	O
of	O
a	O
family	O
of	O
linear	B
block	B
codes	O
that	O
are	O
generalizations	O
of	O
hamming	B
codes	O
called	O
bch	B
codes	I
this	O
shows	O
that	O
we	O
can	O
using	O
linear	B
block	B
codes	O
achieve	O
better	O
performance	O
than	O
repetition	B
codes	O
but	O
the	O
asymptotic	O
situation	O
still	O
looks	O
grim	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
pb	O
more	O
useful	O
codes	O
figure	O
error	B
probability	B
pb	O
versus	O
rate	B
r	O
for	O
repetition	B
codes	O
the	O
hamming	B
code	I
and	O
bch	B
codes	I
with	O
blocklengths	O
up	O
to	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
f	O
the	O
righthand	O
shows	O
pb	O
on	O
a	O
logarithmic	O
scale	O
more	O
useful	O
codes	O
rate	B
rate	B
exercise	O
design	O
an	O
error-correcting	B
code	I
and	O
a	O
decoding	B
algorithm	O
for	O
it	O
estimate	O
its	O
probability	B
of	I
error	I
and	O
add	O
it	O
to	O
t	O
worry	O
if	O
you	O
it	O
to	O
make	O
a	O
code	O
better	O
than	O
the	O
hamming	B
code	I
or	O
if	O
you	O
it	O
to	O
a	O
good	B
decoder	B
for	O
your	O
code	O
that	O
s	O
the	O
point	O
of	O
this	O
exercise	O
exercise	O
a	O
hamming	B
code	I
can	O
correct	O
any	O
one	O
error	O
might	O
there	O
be	O
a	O
code	O
that	O
can	O
correct	O
any	O
two	O
errors	B
optional	O
extra	O
does	O
the	O
answer	O
to	O
this	O
question	O
depend	O
on	O
whether	O
the	O
code	O
is	O
linear	B
or	O
nonlinear	B
exercise	O
design	O
an	O
error-correcting	B
code	I
other	O
than	O
a	O
repetition	B
code	I
that	O
can	O
correct	O
any	O
two	O
errors	B
in	O
a	O
block	B
of	O
size	O
n	O
what	O
performance	O
can	O
the	O
best	O
codes	O
achieve	O
there	O
seems	O
to	O
be	O
a	O
between	O
the	O
decoded	O
bit-error	O
probability	B
pb	O
we	O
would	O
like	O
to	O
reduce	O
and	O
the	O
rate	B
r	O
we	O
would	O
like	O
to	O
keep	O
large	O
how	O
can	O
this	O
be	O
characterized	O
what	O
points	O
in	O
the	O
pb	O
plane	O
are	O
achievable	O
this	O
question	O
was	O
addressed	O
by	O
claude	O
shannon	B
in	O
his	O
pioneering	O
paper	O
of	O
in	O
which	O
he	O
both	O
created	O
the	O
of	O
information	B
theory	I
and	O
solved	O
most	O
of	O
its	O
fundamental	O
problems	O
at	O
that	O
time	O
there	O
was	O
a	O
widespread	O
belief	B
that	O
the	O
boundary	O
between	O
achievable	O
and	O
nonachievable	O
points	O
in	O
the	O
pb	O
plane	O
was	O
a	O
curve	O
passing	O
through	O
the	O
origin	O
pb	O
if	O
this	O
were	O
so	O
then	O
in	O
order	O
to	O
achieve	O
a	O
vanishingly	O
small	O
error	B
probability	B
pb	O
one	O
would	O
have	O
to	O
reduce	O
the	O
rate	B
correspondingly	O
close	O
to	O
zero	O
no	O
pain	O
no	O
gain	B
however	O
shannon	B
proved	O
the	O
remarkable	O
result	O
that	O
the	O
boundary	O
be-	O
tween	O
achievable	O
and	O
nonachievable	O
points	O
meets	O
the	O
r	O
axis	O
at	O
a	O
non-zero	O
value	O
r	O
c	O
as	O
shown	O
in	O
for	O
any	O
channel	O
there	O
exist	O
codes	O
that	O
make	O
it	O
possible	O
to	O
communicate	O
with	O
arbitrarily	O
small	O
probability	B
of	I
error	I
pb	O
at	O
non-zero	O
rates	O
the	O
half	O
of	O
this	O
book	O
iiii	O
will	O
be	O
devoted	O
to	O
understanding	O
this	O
remarkable	O
result	O
which	O
is	O
called	O
the	O
noisy-channel	B
coding	B
theorem	I
example	O
f	O
the	O
maximum	O
rate	B
at	O
which	O
communication	B
is	O
possible	O
with	O
arbitrarily	O
small	O
pb	O
is	O
called	O
the	O
capacity	B
of	O
the	O
channel	O
the	O
formula	O
for	O
the	O
capacity	B
of	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
summary	B
pb	O
achievable	O
not	O
achievable	O
c	O
rate	B
c	O
rate	B
achievable	O
not	O
achievable	O
figure	O
shannon	B
s	O
noisy-channel	B
coding	B
theorem	I
the	O
solid	O
curve	O
shows	O
the	O
shannon	B
limit	O
on	O
achievable	O
values	O
of	O
pb	O
for	O
the	O
binary	B
symmetric	B
channel	I
with	O
f	O
rates	O
up	O
to	O
r	O
c	O
are	O
achievable	O
with	O
arbitrarily	O
small	O
pb	O
the	O
points	O
show	O
the	O
performance	O
of	O
some	O
textbook	O
codes	O
as	O
in	O
the	O
equation	O
the	O
shannon	B
limit	O
solid	O
curve	O
is	O
r	O
where	O
c	O
and	O
are	O
in	O
equation	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
f	O
is	O
cf	O
f	O
f	O
the	O
channel	O
we	O
were	O
discussing	O
earlier	O
with	O
noise	O
level	O
f	O
has	O
capacity	B
c	O
let	O
us	O
consider	O
what	O
this	O
means	O
in	O
terms	O
of	O
noisy	B
disk	O
drives	O
the	O
repetition	B
code	I
could	O
communicate	O
over	O
this	O
channel	O
with	O
pb	O
at	O
a	O
rate	B
r	O
thus	O
we	O
know	O
how	O
to	O
build	O
a	O
single	O
gigabyte	O
disk	B
drive	I
with	O
pb	O
from	O
three	O
noisy	B
gigabyte	O
disk	O
drives	O
we	O
also	O
know	O
how	O
to	O
make	O
a	O
single	O
gigabyte	O
disk	B
drive	I
with	O
pb	O
from	O
sixty	O
noisy	B
one-gigabyte	O
drives	O
and	O
now	O
shannon	B
passes	O
by	O
notices	O
us	O
juggling	B
with	O
disk	O
drives	O
and	O
codes	O
and	O
says	O
what	O
performance	O
are	O
you	O
trying	O
to	O
achieve	O
you	O
don	O
t	O
need	O
sixty	O
disk	O
drives	O
you	O
can	O
get	O
that	O
performance	O
with	O
just	O
two	O
disk	O
drives	O
is	O
less	O
than	O
and	O
if	O
you	O
want	O
pb	O
or	O
or	O
anything	O
you	O
can	O
get	O
there	O
with	O
two	O
disk	O
drives	O
too	O
the	O
above	O
statements	O
might	O
not	O
be	O
quite	O
right	O
since	O
as	O
we	O
shall	O
see	O
shannon	B
proved	O
his	O
noisy-channel	B
coding	B
theorem	I
by	O
studying	O
sequences	O
of	O
block	B
codes	O
with	O
ever-increasing	O
blocklengths	O
and	O
the	O
required	O
blocklength	O
might	O
be	O
bigger	O
than	O
a	O
gigabyte	O
size	O
of	O
our	O
disk	B
drive	I
in	O
which	O
case	O
shannon	B
might	O
say	O
well	O
you	O
can	O
t	O
do	O
it	O
with	O
those	O
tiny	O
disk	O
drives	O
but	O
if	O
you	O
had	O
two	O
noisy	B
terabyte	O
drives	O
you	O
could	O
make	O
a	O
single	O
high-quality	O
terabyte	O
drive	O
from	O
them	O
summary	B
the	O
hamming	B
code	I
by	O
including	O
three	O
parity-check	B
bits	I
in	O
a	O
block	B
of	O
bits	O
it	O
is	O
possible	O
to	O
detect	O
and	O
correct	O
any	O
single	O
bit	B
error	O
in	O
each	O
block	B
shannon	B
s	O
noisy-channel	B
coding	B
theorem	I
information	B
can	O
be	O
communicated	O
over	O
a	O
noisy	B
channel	O
at	O
a	O
non-zero	O
rate	B
with	O
arbitrarily	O
small	O
error	B
probability	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
information	B
theory	I
addresses	O
both	O
the	O
limitations	O
and	O
the	O
possibilities	O
of	O
communication	B
the	O
noisy-channel	B
coding	B
theorem	I
which	O
we	O
will	O
prove	O
in	O
chapter	O
asserts	O
both	O
that	O
reliable	O
communication	B
at	O
any	O
rate	B
beyond	O
the	O
capacity	B
is	O
impossible	O
and	O
that	O
reliable	O
communication	B
at	O
all	O
rates	O
up	O
to	O
capacity	B
is	O
possible	O
the	O
next	O
few	O
chapters	O
lay	O
the	O
foundations	O
for	O
this	O
result	O
by	O
discussing	O
how	B
to	I
measure	I
information	B
content	I
and	O
the	O
intimately	O
related	O
topic	O
of	O
data	O
compression	B
further	O
exercises	O
exercise	O
consider	O
the	O
repetition	B
code	I
one	O
way	O
of	O
viewing	O
this	O
code	O
is	O
as	O
a	O
concatenation	B
of	O
with	O
we	O
encode	O
the	O
source	O
stream	O
with	O
then	O
encode	O
the	O
resulting	O
output	O
with	O
we	O
could	O
call	O
this	O
code	O
this	O
idea	O
motivates	O
an	O
alternative	O
decoding	B
algorithm	O
in	O
which	O
we	O
decode	O
the	O
bits	O
three	O
at	O
a	O
time	O
using	O
the	O
decoder	B
for	O
then	O
decode	O
the	O
decoded	O
bits	O
from	O
that	O
decoder	B
using	O
the	O
decoder	B
for	O
evaluate	O
the	O
probability	B
of	I
error	I
for	O
this	O
decoder	B
and	O
compare	O
it	O
with	O
the	O
probability	B
of	I
error	I
for	O
the	O
optimal	B
decoder	B
for	O
do	O
the	O
concatenated	B
encoder	B
and	O
decoder	B
for	O
those	O
for	O
have	O
advantages	O
over	O
solutions	O
solution	O
to	O
exercise	O
an	O
error	O
is	O
made	O
by	O
if	O
two	O
or	O
more	O
bits	O
are	O
in	O
a	O
block	B
of	O
three	O
so	O
the	O
error	B
probability	B
of	O
is	O
a	O
sum	O
of	O
two	O
terms	O
the	O
probability	B
that	O
all	O
three	O
bits	O
are	O
f	O
and	O
the	O
probability	B
that	O
exactly	O
two	O
bits	O
are	O
f	O
these	O
expressions	O
are	O
not	O
obvious	O
see	O
example	O
the	O
expressions	O
are	O
p	O
f	O
n	O
and	O
p	O
f	O
n	O
pb	O
pb	O
f	O
f	O
this	O
probability	B
is	O
dominated	O
for	O
small	O
f	O
by	O
the	O
term	O
see	O
exercise	O
for	O
further	O
discussion	O
of	O
this	O
problem	O
solution	O
to	O
exercise	O
the	O
probability	B
of	I
error	I
for	O
the	O
repetition	B
code	I
rn	O
is	O
dominated	O
by	O
the	O
probability	B
that	O
bits	O
are	O
which	O
goes	O
odd	O
n	O
as	O
f	O
n	O
can	O
be	O
approximated	O
using	O
the	O
binary	B
entropy	B
function	I
the	O
term	O
where	O
this	O
approximation	B
introduces	O
an	O
error	O
of	O
order	O
pn	O
as	O
shown	O
in	O
equation	O
so	O
notation	B
denotes	O
the	O
smallest	O
integer	O
greater	O
than	O
or	O
equal	O
to	O
n	O
pb	O
pb	O
f	O
f	O
log	O
this	O
answer	O
is	O
a	O
little	O
out	O
because	O
the	O
approximation	B
we	O
used	O
overestimated	O
setting	O
this	O
equal	O
to	O
the	O
required	O
value	O
of	O
we	O
n	O
log	O
and	O
we	O
did	O
not	O
distinguish	O
between	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
a	O
slightly	O
more	O
careful	O
answer	O
of	O
explicit	O
computation	O
goes	O
as	O
follows	O
to	O
the	O
next	O
order	O
we	O
taking	O
the	O
approximation	B
for	O
n	O
this	O
approximation	B
can	O
be	O
proved	O
from	O
an	O
accurate	O
version	O
of	O
stirling	B
s	O
approximation	B
or	O
by	O
considering	O
the	O
binomial	B
distribution	B
with	O
p	O
and	O
noting	O
n	O
where	O
from	O
which	O
equation	O
follows	O
the	O
distinction	O
between	O
and	O
is	O
not	O
important	O
in	O
this	O
term	O
since	O
has	O
a	O
maximum	O
at	O
n	O
k	O
then	O
the	O
probability	B
of	I
error	I
odd	O
n	O
is	O
to	O
leading	O
order	O
pb	O
n	O
f	O
f	O
f	O
the	O
equation	O
pb	O
can	O
be	O
written	O
f	O
f	O
log	O
log	O
f	O
log	O
f	O
in	O
equation	O
the	O
logarithms	B
can	O
be	O
taken	O
to	O
any	O
base	O
as	O
long	O
as	O
it	O
s	O
the	O
same	O
base	O
throughout	O
in	O
equation	O
i	O
use	O
base	O
which	O
may	O
be	O
solved	O
for	O
n	O
iteratively	O
the	O
iteration	O
starting	O
from	O
this	O
answer	O
is	O
found	O
to	O
be	O
stable	O
so	O
n	O
is	O
the	O
blocklength	O
at	O
which	O
pb	O
solution	O
to	O
exercise	O
the	O
probability	B
of	I
block	B
error	I
of	O
the	O
hamming	B
code	I
is	O
a	O
sum	O
of	O
six	B
terms	O
the	O
probabilities	O
that	O
or	O
errors	B
occur	O
in	O
one	O
block	B
pb	O
f	O
to	O
leading	O
order	O
this	O
goes	O
as	O
pb	O
the	O
probability	B
of	O
bit	B
error	O
of	O
the	O
hamming	B
code	I
is	O
smaller	O
than	O
the	O
probability	B
of	I
block	B
error	I
because	O
a	O
block	B
error	O
rarely	O
corrupts	O
all	O
bits	O
in	O
the	O
decoded	O
block	B
the	O
leading-order	O
behaviour	O
is	O
found	O
by	O
considering	O
the	O
outcome	O
in	O
the	O
most	O
probable	O
case	O
where	O
the	O
noise	O
vector	O
has	O
weight	O
two	O
the	O
decoder	B
will	O
erroneously	O
a	O
third	O
bit	B
so	O
that	O
the	O
received	O
vector	O
length	O
in	O
three	O
bits	O
from	O
the	O
transmitted	O
vector	O
that	O
means	O
if	O
we	O
average	O
over	O
all	O
seven	O
bits	O
the	O
probability	B
that	O
a	O
randomly	O
chosen	O
bit	B
is	O
is	O
times	O
the	O
block	B
error	B
probability	B
to	O
leading	O
order	O
now	O
what	O
we	O
really	O
care	O
about	O
is	O
the	O
probability	B
that	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
a	O
source	O
bit	B
is	O
are	O
parity	B
bits	O
or	O
source	O
bits	O
more	O
likely	O
to	O
be	O
among	O
these	O
three	O
bits	O
or	O
are	O
all	O
seven	O
bits	O
equally	O
likely	O
to	O
be	O
corrupted	O
when	O
the	O
noise	O
vector	O
has	O
weight	O
two	O
the	O
hamming	B
code	I
is	O
in	O
fact	O
completely	O
symmetric	B
in	O
the	O
protection	O
it	O
to	O
the	O
seven	O
bits	O
a	O
binary	B
symmetric	B
channel	I
symmetry	O
can	O
be	O
proved	O
by	O
showing	O
that	O
the	O
role	O
of	O
a	O
parity	B
bit	B
can	O
be	O
exchanged	O
with	O
a	O
source	O
bit	B
and	O
the	O
resulting	O
code	O
is	O
still	O
a	O
hamming	B
code	I
see	O
below	O
the	O
probability	B
that	O
any	O
one	O
bit	B
ends	O
up	O
corrupted	O
is	O
the	O
same	O
for	O
all	O
seven	O
bits	O
so	O
the	O
probability	B
of	O
bit	B
error	O
the	O
source	O
bits	O
is	O
simply	O
three	O
sevenths	O
of	O
the	O
probability	B
of	I
block	B
error	I
pb	O
pb	O
symmetry	O
of	O
the	O
hamming	B
code	I
to	O
prove	O
that	O
the	O
code	O
protects	O
all	O
bits	O
equally	O
we	O
start	O
from	O
the	O
paritycheck	O
matrix	B
h	O
h	O
the	O
symmetry	O
among	O
the	O
seven	O
transmitted	O
bits	O
will	O
be	O
easiest	O
to	O
see	O
if	O
we	O
reorder	O
the	O
seven	O
bits	O
using	O
the	O
permutation	B
then	O
we	O
can	O
rewrite	O
h	O
thus	O
now	O
if	O
we	O
take	O
any	O
two	O
parity	B
constraints	O
that	O
t	O
and	O
add	O
them	O
together	O
we	O
get	O
another	O
parity	B
constraint	O
for	O
example	O
row	O
asserts	O
even	O
and	O
row	O
asserts	O
even	O
and	O
the	O
sum	O
of	O
these	O
two	O
constraints	O
is	O
even	O
we	O
can	O
drop	O
the	O
terms	O
and	O
since	O
they	O
are	O
even	O
whatever	O
and	O
are	O
thus	O
we	O
have	O
derived	O
the	O
parity	B
constraint	O
even	O
which	O
we	O
can	O
if	O
we	O
wish	O
add	O
into	O
the	O
parity-check	B
matrix	B
as	O
a	O
fourth	O
row	O
set	B
of	O
vectors	B
satisfying	O
ht	O
will	O
not	O
be	O
changed	O
we	O
thus	O
the	O
fourth	O
row	O
is	O
the	O
sum	O
two	O
of	O
the	O
top	O
two	O
rows	O
notice	O
that	O
the	O
second	O
third	O
and	O
fourth	O
rows	O
are	O
all	O
cyclic	B
shifts	O
of	O
the	O
top	O
row	O
if	O
having	O
added	O
the	O
fourth	O
redundant	O
constraint	O
we	O
drop	O
the	O
constraint	O
we	O
obtain	O
a	O
new	O
parity-check	B
matrix	B
which	O
still	O
for	O
all	O
codewords	O
and	O
which	O
looks	O
just	O
like	O
the	O
starting	O
h	O
in	O
except	O
that	O
all	O
the	O
columns	O
have	O
shifted	O
along	O
one	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
to	O
the	O
right	O
and	O
the	O
rightmost	O
column	O
has	O
reappeared	O
at	O
the	O
left	O
cyclic	B
permutation	B
of	O
the	O
columns	O
this	O
establishes	O
the	O
symmetry	O
among	O
the	O
seven	O
bits	O
iterating	O
the	O
above	O
procedure	O
more	O
times	O
we	O
can	O
make	O
a	O
total	O
of	O
seven	O
h	O
matrices	B
for	O
the	O
same	O
original	O
code	O
each	O
of	O
which	O
assigns	O
each	O
bit	B
to	O
a	O
role	O
we	O
may	O
also	O
construct	O
the	O
super-redundant	O
seven-row	O
parity-check	B
matrix	B
for	O
the	O
code	O
this	O
matrix	B
is	O
redundant	O
in	O
the	O
sense	O
that	O
the	O
space	O
spanned	O
by	O
its	O
rows	O
is	O
only	O
three-dimensional	O
not	O
seven	O
this	O
matrix	B
is	O
also	O
a	O
cyclic	B
matrix	B
every	O
row	O
is	O
a	O
cyclic	B
permutation	B
of	O
the	O
top	O
row	O
cyclic	B
codes	O
if	O
there	O
is	O
an	O
ordering	O
of	O
the	O
bits	O
tn	O
such	O
that	O
a	O
linear	B
code	O
has	O
a	O
cyclic	B
parity-check	B
matrix	B
then	O
the	O
code	O
is	O
called	O
a	O
cyclic	B
code	O
the	O
codewords	O
of	O
such	O
a	O
code	O
also	O
have	O
cyclic	B
properties	O
any	O
cyclic	B
permutation	B
of	O
a	O
codeword	B
is	O
a	O
codeword	B
for	O
example	O
the	O
hamming	B
code	I
with	O
its	O
bits	O
ordered	O
as	O
above	O
consists	O
of	O
all	O
seven	O
cyclic	B
shifts	O
of	O
the	O
codewords	O
and	O
and	O
the	O
codewords	O
and	O
cyclic	B
codes	O
are	O
a	O
cornerstone	O
of	O
the	O
algebraic	O
approach	O
to	O
error-correcting	B
codes	I
we	O
won	O
t	O
use	O
them	O
again	O
in	O
this	O
book	O
however	O
as	O
they	O
have	O
been	O
superceded	O
by	O
sparse-graph	O
codes	O
vi	O
solution	O
to	O
exercise	O
there	O
are	O
non-zero	O
noise	O
vectors	B
which	O
give	O
the	O
all-zero	O
syndrome	B
these	O
are	O
precisely	O
the	O
non-zero	O
codewords	O
of	O
the	O
hamming	B
code	I
notice	O
that	O
because	O
the	O
hamming	B
code	I
is	O
linear	B
the	O
sum	O
of	O
any	O
two	O
codewords	O
is	O
a	O
codeword	B
graphs	O
corresponding	O
to	O
codes	O
solution	O
to	O
exercise	O
when	O
answering	O
this	O
question	O
you	O
will	O
probably	O
that	O
it	O
is	O
easier	O
to	O
invent	O
new	O
codes	O
than	O
to	O
optimal	B
decoders	O
for	O
them	O
there	O
are	O
many	O
ways	O
to	O
design	O
codes	O
and	O
what	O
follows	O
is	O
just	O
one	O
possible	O
train	O
of	O
thought	O
we	O
make	O
a	O
linear	B
block	B
code	I
that	O
is	O
similar	O
to	O
the	O
hamming	B
code	I
but	O
bigger	O
many	O
codes	O
can	O
be	O
conveniently	O
expressed	O
in	O
terms	O
of	O
graphs	O
in	O
we	O
introduced	O
a	O
pictorial	O
representation	O
of	O
the	O
hamming	B
code	I
if	O
we	O
replace	O
that	O
s	O
big	O
circles	O
each	O
of	O
which	O
shows	O
that	O
the	O
parity	B
of	O
four	O
particular	O
bits	O
is	O
even	O
by	O
a	O
parity-check	O
node	O
that	O
is	O
connected	O
to	O
the	O
four	O
bits	O
then	O
we	O
obtain	O
the	O
representation	O
of	O
the	O
hamming	B
code	I
by	O
a	O
bipartite	B
graph	B
as	O
shown	O
in	O
the	O
circles	O
are	O
the	O
transmitted	O
bits	O
the	O
squares	O
are	O
the	O
parity-check	B
nodes	I
to	O
be	O
confused	O
with	O
the	O
parity-check	B
bits	I
which	O
are	O
the	O
three	O
most	O
peripheral	O
circles	O
the	O
graph	B
is	O
a	O
bipartite	B
graph	B
because	O
its	O
nodes	O
fall	O
into	O
two	O
classes	O
bits	O
and	O
checks	O
figure	O
the	O
graph	B
of	O
the	O
hamming	B
code	I
the	O
circles	O
are	O
the	O
bit	B
nodes	O
and	O
the	O
squares	O
are	O
the	O
parity-check	B
nodes	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
information	B
theory	I
and	O
there	O
are	O
edges	O
only	O
between	O
nodes	O
in	O
classes	O
the	O
graph	B
and	O
the	O
code	O
s	O
parity-check	B
matrix	B
are	O
simply	O
related	O
to	O
each	O
other	O
each	O
parity-check	O
node	O
corresponds	O
to	O
a	O
row	O
of	O
h	O
and	O
each	O
bit	B
node	O
corresponds	O
to	O
a	O
column	O
of	O
h	O
for	O
every	O
in	O
h	O
there	O
is	O
an	O
edge	B
between	O
the	O
corresponding	O
pair	O
of	O
nodes	O
having	O
noticed	O
this	O
connection	O
between	O
linear	B
codes	I
and	O
graphs	O
one	O
way	O
to	O
invent	O
linear	B
codes	I
is	O
simply	O
to	O
think	O
of	O
a	O
bipartite	B
graph	B
for	O
example	O
a	O
pretty	O
bipartite	B
graph	B
can	O
be	O
obtained	O
from	O
a	O
dodecahedron	B
by	O
calling	O
the	O
vertices	O
of	O
the	O
dodecahedron	B
the	O
parity-check	B
nodes	I
and	O
putting	O
a	O
transmitted	O
bit	B
on	O
each	O
edge	B
in	O
the	O
dodecahedron	B
this	O
construction	B
a	O
paritycheck	O
matrix	B
in	O
which	O
every	O
column	O
has	O
weight	O
and	O
every	O
row	O
has	O
weight	O
weight	O
of	O
a	O
binary	O
vector	O
is	O
the	O
number	O
of	O
it	O
contains	O
this	O
code	O
has	O
n	O
bits	O
and	O
it	O
appears	O
to	O
have	O
mapparent	O
paritycheck	O
constraints	O
actually	O
there	O
are	O
only	O
m	O
independent	O
constraints	O
the	O
constraint	O
is	O
redundant	O
is	O
if	O
constraints	O
are	O
then	O
the	O
is	O
automatically	O
so	O
the	O
number	O
of	O
source	O
bits	O
is	O
k	O
n	O
m	O
the	O
code	O
is	O
a	O
code	O
it	O
is	O
hard	O
to	O
a	O
decoding	B
algorithm	O
for	O
this	O
code	O
but	O
we	O
can	O
estimate	O
its	O
probability	B
of	I
error	I
by	O
its	O
lowest-weight	O
codewords	O
if	O
we	O
all	O
the	O
bits	O
surrounding	O
one	O
face	O
of	O
the	O
original	O
dodecahedron	B
then	O
all	O
the	O
parity	B
checks	O
will	O
be	O
so	O
the	O
code	O
has	O
codewords	O
of	O
weight	O
one	O
for	O
each	O
face	O
since	O
the	O
lowest-weight	O
codewords	O
have	O
weight	O
we	O
say	O
that	O
the	O
code	O
has	O
distance	B
d	O
the	O
hamming	B
code	I
had	O
distance	B
and	O
could	O
correct	O
all	O
single	O
errors	B
a	O
code	O
with	O
distance	B
can	O
correct	O
all	O
double	O
errors	B
but	O
there	O
are	O
some	O
triple	O
errors	B
that	O
it	O
cannot	O
correct	O
so	O
the	O
error	B
probability	B
of	O
this	O
code	O
assuming	O
a	O
binary	B
symmetric	B
channel	I
will	O
be	O
dominated	O
at	O
least	O
for	O
low	O
noise	O
levels	O
f	O
by	O
a	O
term	O
of	O
order	O
f	O
perhaps	O
something	O
like	O
f	O
of	O
course	O
there	O
is	O
no	O
obligation	O
to	O
make	O
codes	O
whose	O
graphs	O
can	O
be	O
represented	O
on	O
a	O
plane	O
as	O
this	O
one	O
can	O
the	O
best	O
linear	B
codes	I
which	O
have	O
simple	O
graphical	O
descriptions	O
have	O
graphs	O
that	O
are	O
more	O
tangled	O
as	O
illustrated	O
by	O
the	O
tiny	O
code	O
of	O
furthermore	O
there	O
is	O
no	O
reason	O
for	O
sticking	O
to	O
linear	B
codes	I
indeed	O
some	O
nonlinear	B
codes	O
codes	O
whose	O
codewords	O
cannot	O
be	O
by	O
a	O
linear	B
equation	O
like	O
ht	O
have	O
very	B
good	B
properties	O
but	O
the	O
encoding	O
and	O
decoding	B
of	O
a	O
nonlinear	B
code	I
are	O
even	O
trickier	O
tasks	O
solution	O
to	O
exercise	O
code	O
and	O
decoding	B
it	O
with	O
syndrome	B
decoding	B
bits	O
then	O
the	O
number	O
of	O
possible	O
error	O
patterns	O
of	O
weight	O
up	O
to	O
two	O
is	O
first	O
let	O
s	O
assume	O
we	O
are	O
making	O
a	O
linear	B
if	O
there	O
are	O
n	O
transmitted	O
for	O
n	O
that	O
s	O
patterns	O
now	O
every	O
distinguishable	O
error	O
pattern	O
must	O
give	O
rise	O
to	O
a	O
distinct	O
syndrome	B
and	O
the	O
syndrome	B
is	O
a	O
list	O
of	O
m	O
bits	O
so	O
the	O
maximum	O
possible	O
number	O
of	O
syndromes	O
is	O
for	O
a	O
code	O
m	O
so	O
there	O
are	O
at	O
most	O
syndromes	O
the	O
number	O
of	O
possible	O
error	O
patterns	O
of	O
weight	O
up	O
to	O
two	O
is	O
bigger	O
than	O
the	O
number	O
of	O
syndromes	O
so	O
we	O
can	O
immediately	O
rule	O
out	O
the	O
possibility	O
that	O
there	O
is	O
a	O
code	O
that	O
is	O
figure	O
the	O
graph	B
the	O
dodecahedron	B
code	I
the	O
circles	O
are	O
the	O
transmitted	O
bits	O
and	O
the	O
triangles	O
are	O
the	O
parity	B
checks	O
one	O
parity	B
check	O
is	O
redundant	O
figure	O
graph	B
of	O
a	O
low-density	B
parity-check	B
code	I
code	O
with	O
blocklength	O
n	O
and	O
m	O
parity-check	B
constraints	I
each	O
white	B
circle	B
represents	O
a	O
transmitted	O
bit	B
each	O
bit	B
participates	O
in	O
j	O
constraints	O
represented	O
by	O
squares	O
the	O
edges	O
between	O
nodes	O
were	O
placed	O
at	O
random	B
chapter	O
for	O
more	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
the	O
same	O
counting	B
argument	I
works	O
for	O
nonlinear	B
codes	O
too	O
when	O
the	O
decoder	B
receives	O
r	O
t	O
n	O
his	O
aim	O
is	O
to	O
deduce	O
both	O
t	O
and	O
n	O
from	O
r	O
if	O
it	O
is	O
the	O
case	O
that	O
the	O
sender	O
can	O
select	O
any	O
transmission	O
t	O
from	O
a	O
code	O
of	O
size	O
st	O
and	O
the	O
channel	O
can	O
select	O
any	O
noise	O
vector	O
from	O
a	O
set	B
of	O
size	O
sn	O
and	O
those	O
two	O
selections	O
can	O
be	O
recovered	O
from	O
the	O
received	O
bit	B
string	O
r	O
which	O
is	O
one	O
of	O
at	O
most	O
possible	O
strings	O
then	O
it	O
must	O
be	O
the	O
case	O
that	O
so	O
for	O
a	O
k	O
two-error-correcting	O
code	O
whether	O
linear	B
or	O
nonlinear	B
stsn	O
solution	O
to	O
exercise	O
there	O
are	O
various	O
strategies	O
for	O
making	O
codes	O
that	O
can	O
correct	O
multiple	O
errors	B
and	O
i	O
strongly	O
recommend	O
you	O
think	O
out	O
one	O
or	O
two	O
of	O
them	O
for	O
yourself	O
if	O
your	O
approach	O
uses	O
a	O
linear	B
code	O
e	O
g	O
one	O
with	O
a	O
collection	O
of	O
m	O
parity	B
checks	O
it	O
is	O
helpful	O
to	O
bear	O
in	O
mind	O
the	O
counting	B
argument	I
given	O
in	O
the	O
previous	O
exercise	O
in	O
order	O
to	O
anticipate	O
how	O
many	O
parity	B
checks	O
m	O
you	O
might	O
need	O
examples	O
of	O
codes	O
that	O
can	O
correct	O
any	O
two	O
errors	B
are	O
the	O
dodecahedron	B
code	I
on	O
page	O
and	O
the	O
pentagonful	B
code	I
to	O
be	O
introduced	O
on	O
further	O
simple	O
ideas	O
for	O
making	O
codes	O
that	O
can	O
correct	O
multiple	O
errors	B
from	O
codes	O
that	O
can	O
correct	O
only	O
one	O
error	O
are	O
discussed	O
in	O
section	B
solution	O
to	O
exercise	O
the	O
probability	B
of	I
error	I
of	O
order	O
is	O
to	O
leading	O
whereas	O
the	O
probability	B
of	I
error	I
of	O
is	O
dominated	O
by	O
the	O
probability	B
of	O
f	O
the	O
tors	O
of	O
weight	O
four	O
that	O
cause	O
it	O
to	O
make	O
a	O
decoding	B
error	O
decoding	B
procedure	O
is	O
therefore	O
suboptimal	O
since	O
there	O
are	O
noise	O
vec	O
it	O
has	O
the	O
advantage	O
however	O
of	O
requiring	O
smaller	O
computational	O
resources	O
only	O
memorization	O
of	O
three	O
bits	O
and	O
counting	B
up	O
to	O
three	O
rather	O
than	O
counting	B
up	O
to	O
nine	O
this	O
simple	O
code	O
illustrates	O
an	O
important	O
concept	O
concatenated	B
codes	O
are	O
widely	O
used	O
in	O
practice	O
because	O
concatenation	B
allows	O
large	O
codes	O
to	O
be	O
implemented	O
using	O
simple	O
encoding	O
and	O
decoding	B
hardware	O
some	O
of	O
the	O
best	O
known	O
practical	B
codes	O
are	O
concatenated	B
codes	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
this	O
chapter	O
and	O
its	O
sibling	O
chapter	O
devote	O
some	O
time	O
to	O
notation	B
just	O
as	O
the	O
white	B
knight	O
distinguished	O
between	O
the	O
song	O
the	O
name	O
of	O
the	O
song	O
and	O
what	O
the	O
name	O
of	O
the	O
song	O
was	O
called	O
we	O
will	O
sometimes	O
need	O
to	O
be	O
careful	O
to	O
distinguish	O
between	O
a	O
random	B
variable	I
the	O
value	O
of	O
the	O
random	B
variable	I
and	O
the	O
proposition	O
that	O
asserts	O
that	O
the	O
random	B
variable	I
has	O
a	O
particular	O
value	O
in	O
any	O
particular	O
chapter	O
however	O
i	O
will	O
use	O
the	O
most	O
simple	O
and	O
friendly	O
notation	B
possible	O
at	O
the	O
risk	O
of	O
upsetting	O
pure-minded	O
readers	O
for	O
example	O
if	O
something	O
is	O
true	O
with	O
probability	B
i	O
will	O
usually	O
simply	O
say	O
that	O
it	O
is	O
true	O
probabilities	O
and	O
ensembles	O
an	O
ensemble	B
x	O
is	O
a	O
triple	O
where	O
the	O
outcome	O
x	O
is	O
the	O
value	O
of	O
a	O
random	B
variable	I
which	O
takes	O
on	O
one	O
of	O
a	O
set	B
of	O
possible	O
values	O
ax	O
ai	O
aig	O
having	O
probabilities	O
px	O
pig	O
with	O
p	O
ai	O
pi	O
pi	O
and	O
the	O
name	O
a	O
is	O
mnemonic	O
for	O
alphabet	O
one	O
example	O
of	O
an	O
ensemble	B
is	O
a	O
letter	O
that	O
is	O
randomly	O
selected	O
from	O
an	O
english	B
document	O
this	O
ensemble	B
is	O
shown	O
in	O
there	O
are	O
twenty-seven	O
possible	O
letters	O
az	O
and	O
a	O
space	O
character	O
p	O
ai	O
abbreviations	O
briefer	O
notation	B
will	O
sometimes	O
be	O
used	O
for	O
example	O
p	O
ai	O
may	O
be	O
written	O
as	O
p	O
or	O
p	O
probability	B
of	O
a	O
subset	B
if	O
t	O
is	O
a	O
subset	B
of	O
ax	O
then	O
p	O
p	O
t	O
p	O
ai	O
for	O
example	O
fa	O
e	O
i	O
o	O
ug	O
then	O
if	O
we	O
v	O
to	O
be	O
vowels	O
from	O
v	O
p	O
a	O
joint	B
ensemble	B
xy	O
is	O
an	O
ensemble	B
in	O
which	O
each	O
outcome	O
is	O
an	O
ordered	O
pair	O
x	O
y	O
with	O
x	O
ax	O
aig	O
and	O
y	O
ay	O
bjg	O
we	O
call	O
p	O
y	O
the	O
joint	B
probability	B
of	O
x	O
and	O
y	O
commas	O
are	O
optional	O
when	O
writing	B
ordered	O
pairs	O
so	O
xy	O
x	O
y	O
n	O
b	O
in	O
a	O
joint	B
ensemble	B
xy	O
the	O
two	O
variables	O
are	O
not	O
necessarily	O
independent	O
i	O
ai	O
pi	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
figure	O
probability	B
distribution	B
over	O
the	O
outcomes	O
for	O
a	O
randomly	O
selected	O
letter	O
in	O
an	O
english	B
language	O
document	O
from	O
the	O
frequently	O
asked	O
questions	O
manual	O
for	O
linux	O
the	O
picture	O
shows	O
the	O
probabilities	O
by	O
the	O
areas	O
of	O
white	B
squares	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probabilities	O
and	O
ensembles	O
figure	O
the	O
probability	B
distribution	B
over	O
the	O
possible	O
bigrams	O
xy	O
in	O
an	O
english	B
language	O
document	O
the	O
frequently	O
asked	O
questions	O
manual	O
for	O
linux	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
y	O
marginal	B
probability	B
we	O
can	O
obtain	O
the	O
marginal	B
probability	B
p	O
from	O
the	O
joint	B
probability	B
p	O
y	O
by	O
summation	O
p	O
ai	O
p	O
ai	O
y	O
similarly	O
using	O
briefer	O
notation	B
the	O
marginal	B
probability	B
of	O
y	O
is	O
p	O
p	O
y	O
conditional	B
probability	B
p	O
ai	O
j	O
y	O
bj	O
p	O
ai	O
y	O
bj	O
p	O
bj	O
if	O
p	O
bj	O
p	O
bj	O
then	O
p	O
ai	O
j	O
y	O
bj	O
is	O
we	O
pronounce	O
p	O
ai	O
j	O
y	O
bj	O
the	O
probability	B
that	O
x	O
equals	O
ai	O
given	O
y	O
equals	O
bj	O
example	O
an	O
example	O
of	O
a	O
joint	B
ensemble	B
is	O
the	O
ordered	O
pair	O
xy	O
consisting	O
of	O
two	O
successive	O
letters	O
in	O
an	O
english	B
document	O
the	O
possible	O
outcomes	O
are	O
ordered	O
pairs	O
such	O
as	O
aa	O
ab	O
ac	O
and	O
zz	O
of	O
these	O
we	O
might	O
expect	O
ab	O
and	O
ac	O
to	O
be	O
more	O
probable	O
than	O
aa	O
and	O
zz	O
an	O
estimate	O
of	O
the	O
joint	B
probability	B
distribution	B
for	O
two	O
neighbouring	O
characters	O
is	O
shown	O
graphically	O
in	O
this	O
joint	B
ensemble	B
has	O
the	O
special	O
property	O
that	O
its	O
two	O
marginal	B
distributions	O
p	O
and	O
p	O
are	O
identical	O
they	O
are	O
both	O
equal	O
to	O
the	O
monogram	O
distribution	B
shown	O
in	O
from	O
this	O
joint	B
ensemble	B
p	O
y	O
we	O
can	O
obtain	O
conditional	B
distributions	O
p	O
j	O
x	O
and	O
p	O
y	O
by	O
normalizing	O
the	O
rows	O
and	O
columns	O
respectively	O
the	O
probability	B
p	O
j	O
x	O
q	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
second	O
letter	O
given	O
that	O
the	O
letter	O
is	O
a	O
q	O
as	O
you	O
can	O
see	O
in	O
the	O
two	O
most	O
probable	O
values	O
for	O
the	O
second	O
letter	O
y	O
given	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
probability	B
entropy	B
and	O
inference	B
figure	O
conditional	B
probability	B
distributions	O
p	O
j	O
x	O
each	O
row	O
shows	O
the	O
conditional	B
distribution	B
of	O
the	O
second	O
letter	O
y	O
given	O
the	O
letter	O
x	O
in	O
a	O
bigram	O
xy	O
p	O
y	O
each	O
column	O
shows	O
the	O
conditional	B
distribution	B
of	O
the	O
letter	O
x	O
given	O
the	O
second	O
letter	O
y	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
y	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
y	O
p	O
j	O
x	O
p	O
y	O
that	O
the	O
letter	O
x	O
is	O
q	O
are	O
u	O
and	O
space	O
is	O
common	O
after	O
q	O
because	O
the	O
source	O
document	O
makes	O
heavy	O
use	O
of	O
the	O
word	O
faq	O
the	O
probability	B
p	O
y	O
u	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
letter	O
x	O
given	O
that	O
the	O
second	O
letter	O
y	O
is	O
a	O
u	O
as	O
you	O
can	O
see	O
in	O
the	O
two	O
most	O
probable	O
values	O
for	O
x	O
given	O
y	O
u	O
are	O
n	O
and	O
o	O
rather	O
than	O
writing	B
down	O
the	O
joint	B
probability	B
directly	O
we	O
often	O
an	O
ensemble	B
in	O
terms	O
of	O
a	O
collection	O
of	O
conditional	B
probabilities	O
the	O
following	O
rules	B
of	O
probability	B
theory	O
will	O
be	O
useful	O
denotes	O
assumptions	B
on	O
which	O
the	O
probabilities	O
are	O
based	O
product	O
rule	O
obtained	O
from	O
the	O
of	O
conditional	B
probability	B
p	O
y	O
jh	O
p	O
yhp	O
jh	O
p	O
j	O
xhp	O
this	O
rule	O
is	O
also	O
known	O
as	O
the	O
chain	B
rule	I
sum	B
rule	I
a	O
rewriting	O
of	O
the	O
marginal	B
probability	B
p	O
yhp	O
jh	O
p	O
p	O
yhp	O
jh	O
p	O
jh	O
p	O
xy	O
xy	O
p	O
y	O
jh	O
p	O
yhp	O
jh	O
bayes	B
theorem	O
obtained	O
from	O
the	O
product	O
rule	O
p	O
j	O
xh	O
independence	B
two	O
random	B
variables	O
x	O
and	O
y	O
are	O
independent	O
written	O
x	O
y	O
if	O
and	O
only	O
if	O
p	O
y	O
p	O
exercise	O
are	O
the	O
random	B
variables	O
x	O
and	O
y	O
in	O
the	O
joint	B
ensemble	B
of	O
independent	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
meaning	O
of	O
probability	B
i	O
said	O
that	O
we	O
often	O
an	O
ensemble	B
in	O
terms	O
of	O
a	O
collection	O
of	O
condi	O
tional	O
probabilities	O
the	O
following	O
example	O
illustrates	O
this	O
idea	O
example	O
jo	O
has	O
a	O
test	O
for	O
a	O
nasty	O
disease	B
we	O
denote	O
jo	O
s	O
state	O
of	O
health	O
by	O
the	O
variable	O
a	O
and	O
the	O
test	O
result	O
by	O
b	O
a	O
a	O
jo	O
has	O
the	O
disease	B
jo	O
does	O
not	O
have	O
the	O
disease	B
the	O
result	O
of	O
the	O
test	O
is	O
either	O
positive	O
or	O
negative	O
the	O
test	O
is	O
reliable	O
in	O
of	O
cases	O
of	O
people	O
who	O
really	O
have	O
the	O
disease	B
a	O
positive	O
result	O
is	O
returned	O
and	O
in	O
of	O
cases	O
of	O
people	O
who	O
do	O
not	O
have	O
the	O
disease	B
a	O
negative	O
result	O
is	O
obtained	O
the	O
piece	O
of	O
background	O
information	B
is	O
that	O
of	O
people	O
of	O
jo	O
s	O
age	O
and	O
background	O
have	O
the	O
disease	B
ok	O
jo	O
has	O
the	O
test	O
and	O
the	O
result	O
is	O
positive	O
what	O
is	O
the	O
probability	B
that	O
jo	O
has	O
the	O
disease	B
solution	O
we	O
write	O
down	O
all	O
the	O
provided	O
probabilities	O
the	O
test	O
reliability	O
the	O
conditional	B
probability	B
of	O
b	O
given	O
a	O
p	O
a	O
p	O
a	O
p	O
a	O
p	O
a	O
and	O
the	O
disease	B
prevalence	O
tells	O
us	O
about	O
the	O
marginal	B
probability	B
of	O
a	O
p	O
p	O
from	O
the	O
marginal	B
p	O
and	O
the	O
conditional	B
probability	B
p	O
a	O
we	O
can	O
deduce	O
the	O
joint	B
probability	B
p	O
b	O
p	O
a	O
and	O
any	O
other	O
probabilities	O
we	O
are	O
interested	O
in	O
for	O
example	O
by	O
the	O
sum	B
rule	I
the	O
marginal	B
probability	B
of	O
b	O
the	O
probability	B
of	O
getting	O
a	O
positive	O
result	O
is	O
p	O
p	O
a	O
p	O
a	O
jo	O
has	O
received	O
a	O
positive	O
result	O
b	O
and	O
is	O
interested	O
in	O
how	O
plausible	O
it	O
is	O
that	O
she	O
has	O
the	O
disease	B
that	O
a	O
the	O
man	O
in	O
the	O
street	O
might	O
be	O
duped	O
by	O
the	O
statement	O
the	O
test	O
is	O
reliable	O
so	O
jo	O
s	O
positive	O
result	O
implies	O
that	O
there	O
is	O
a	O
chance	O
that	O
jo	O
has	O
the	O
disease	B
but	O
this	O
is	O
incorrect	O
the	O
correct	O
solution	O
to	O
an	O
inference	B
problem	O
is	O
found	O
using	O
bayes	B
theorem	O
p	O
b	O
p	O
a	O
p	O
a	O
p	O
a	O
so	O
in	O
spite	O
of	O
the	O
positive	O
result	O
the	O
probability	B
that	O
jo	O
has	O
the	O
disease	B
is	O
only	O
the	O
meaning	O
of	O
probability	B
probabilities	O
can	O
be	O
used	O
in	O
two	O
ways	O
probabilities	O
can	O
describe	O
frequencies	O
of	O
outcomes	O
in	O
random	B
experiments	O
but	O
giving	O
noncircular	O
of	O
the	O
terms	O
frequency	B
and	O
random	B
is	O
a	O
challenge	O
what	O
does	O
it	O
mean	B
to	O
say	O
that	O
the	O
frequency	B
of	O
a	O
tossed	O
coin	B
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
box	B
the	O
cox	B
axioms	I
if	O
a	O
set	B
of	O
beliefs	O
satisfy	O
these	O
axioms	O
then	O
they	O
can	O
be	O
mapped	O
onto	O
probabilities	O
satisfying	O
p	O
p	O
p	O
and	O
the	O
rules	B
of	O
probability	B
and	O
p	O
p	O
p	O
y	O
p	O
yp	O
notation	B
let	O
the	O
degree	B
of	O
belief	B
in	O
proposition	O
x	O
be	O
denoted	O
by	O
bx	O
the	O
negation	O
of	O
x	O
is	O
written	O
x	O
the	O
degree	B
of	O
belief	B
in	O
a	O
conditional	B
proposition	O
x	O
assuming	O
proposition	O
y	O
to	O
be	O
true	O
is	O
represented	O
by	O
bxj	O
y	O
axiom	O
degrees	B
of	I
belief	B
can	O
be	O
ordered	O
if	O
bx	O
is	O
greater	O
than	O
by	O
and	O
by	O
is	O
greater	O
than	O
bz	O
then	O
bx	O
is	O
greater	O
than	O
bz	O
beliefs	O
can	O
be	O
mapped	O
onto	O
real	O
numbers	O
axiom	O
the	O
degree	B
of	O
belief	B
in	O
a	O
proposition	O
x	O
and	O
its	O
negation	O
x	O
are	O
related	O
there	O
is	O
a	O
function	O
f	O
such	O
that	O
bx	O
f	O
axiom	O
the	O
degree	B
of	O
belief	B
in	O
a	O
conjunction	O
of	O
propositions	O
x	O
y	O
and	O
y	O
is	O
related	O
to	O
the	O
degree	B
of	O
belief	B
in	O
the	O
conditional	B
proposition	O
xj	O
y	O
and	O
the	O
degree	B
of	O
belief	B
in	O
the	O
proposition	O
y	O
there	O
is	O
a	O
function	O
g	O
such	O
that	O
bx	O
y	O
g	O
y	O
by	O
coming	O
up	O
heads	O
is	O
if	O
we	O
say	O
that	O
this	O
frequency	B
is	O
the	O
average	O
fraction	O
of	O
heads	O
in	O
long	O
sequences	O
we	O
have	O
to	O
average	O
and	O
it	O
is	O
hard	O
to	O
average	O
without	O
using	O
a	O
word	O
synonymous	O
to	O
probability	B
i	O
will	O
not	O
attempt	O
to	O
cut	O
this	O
philosophical	O
knot	O
probabilities	O
can	O
also	O
be	O
used	O
more	O
generally	O
to	O
describe	O
degrees	B
of	I
belief	B
in	O
propositions	O
that	O
do	O
not	O
involve	O
random	B
variables	O
for	O
example	O
the	O
probability	B
that	O
mr	O
s	O
was	O
the	O
murderer	O
of	O
mrs	O
s	O
given	O
the	O
evidence	B
either	O
was	O
or	O
wasn	O
t	O
and	O
it	O
s	O
the	O
jury	B
s	O
job	O
to	O
assess	O
how	O
probable	O
it	O
is	O
that	O
he	O
was	O
the	O
probability	B
that	O
thomas	O
had	O
a	O
child	O
by	O
one	O
of	O
his	O
slaves	O
the	O
probability	B
that	O
shakespeare	O
s	O
plays	O
were	O
written	O
by	O
francis	O
bacon	O
or	O
to	O
pick	O
a	O
modern-day	O
example	O
the	O
probability	B
that	O
a	O
particular	O
signature	O
on	O
a	O
particular	O
cheque	O
is	O
genuine	O
the	O
man	O
in	O
the	O
street	O
is	O
happy	O
to	O
use	O
probabilities	O
in	O
both	O
these	O
ways	O
but	O
some	O
books	O
on	O
probability	B
restrict	O
probabilities	O
to	O
refer	O
only	O
to	O
frequencies	O
of	O
outcomes	O
in	O
repeatable	O
random	B
experiments	O
nevertheless	O
degrees	B
of	I
belief	B
can	O
be	O
mapped	O
onto	O
probabilities	O
if	O
they	O
satisfy	O
simple	O
consistency	O
rules	B
known	O
as	O
the	O
cox	B
axioms	I
thus	O
probabilities	O
can	O
be	O
used	O
to	O
describe	O
assumptions	B
and	O
to	O
describe	O
inferences	O
given	O
those	O
assumptions	B
the	O
rules	B
of	O
probability	B
ensure	O
that	O
if	O
two	O
people	O
make	O
the	O
same	O
assumptions	B
and	O
receive	O
the	O
same	O
data	O
then	O
they	O
will	O
draw	O
identical	O
conclusions	O
this	O
more	O
general	O
use	O
of	O
probability	B
to	O
quantify	O
beliefs	O
is	O
known	O
as	O
the	O
bayesian	B
viewpoint	O
it	O
is	O
also	O
known	O
as	O
the	O
subjective	O
interpretation	O
of	O
probability	B
since	O
the	O
probabilities	O
depend	O
on	O
assumptions	B
advocates	O
of	O
a	O
bayesian	B
approach	O
to	O
data	O
modelling	B
and	O
pattern	B
recognition	B
do	O
not	O
view	O
this	O
subjectivity	B
as	O
a	O
defect	O
since	O
in	O
their	O
view	O
you	O
cannot	O
do	O
inference	B
without	O
making	O
assumptions	B
in	O
this	O
book	O
it	O
will	O
from	O
time	O
to	O
time	O
be	O
taken	O
for	O
granted	O
that	O
a	O
bayesian	B
approach	O
makes	O
sense	O
but	O
the	O
reader	O
is	O
warned	O
that	O
this	O
is	O
not	O
yet	O
a	O
globally	O
held	O
view	O
the	O
of	O
statistics	O
was	O
dominated	O
for	O
most	O
of	O
the	O
century	O
by	O
non-bayesian	O
methods	O
in	O
which	O
probabilities	O
are	O
allowed	O
to	O
describe	O
only	O
random	B
variables	O
the	O
big	O
between	O
the	O
two	O
approaches	O
is	O
that	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
forward	O
probabilities	O
and	O
inverse	O
probabilities	O
bayesians	O
also	O
use	O
probabilities	O
to	O
describe	O
inferences	O
forward	O
probabilities	O
and	O
inverse	O
probabilities	O
probability	B
calculations	O
often	O
fall	O
into	O
one	O
of	O
two	O
categories	O
forward	B
probability	B
and	O
inverse	B
probability	B
here	O
is	O
an	O
example	O
of	O
a	O
forward	B
probability	B
problem	O
exercise	O
an	O
urn	B
contains	O
k	O
balls	O
of	O
which	O
b	O
are	O
black	B
and	O
w	O
k	O
b	O
are	O
white	B
fred	O
draws	O
a	O
ball	O
at	O
random	B
from	O
the	O
urn	B
and	O
replaces	O
it	O
n	O
times	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
number	O
of	O
times	O
a	O
black	B
ball	O
is	O
drawn	O
nb	O
what	O
is	O
the	O
expectation	B
of	O
nb	O
what	O
is	O
the	O
variance	B
of	O
nb	O
what	O
is	O
the	O
standard	B
deviation	I
of	O
nb	O
give	O
numerical	O
answers	O
for	O
the	O
cases	O
n	O
and	O
n	O
when	O
b	O
and	O
k	O
forward	B
probability	B
problems	O
involve	O
a	O
generative	B
model	B
that	O
describes	O
a	O
process	O
that	O
is	O
assumed	O
to	O
give	O
rise	O
to	O
some	O
data	O
the	O
task	O
is	O
to	O
compute	O
the	O
probability	B
distribution	B
or	O
expectation	B
of	O
some	O
quantity	O
that	O
depends	O
on	O
the	O
data	O
here	O
is	O
another	O
example	O
of	O
a	O
forward	B
probability	B
problem	O
exercise	O
an	O
urn	B
contains	O
k	O
balls	O
of	O
which	O
b	O
are	O
black	B
and	O
w	O
k	O
b	O
are	O
white	B
we	O
the	O
fraction	O
fb	O
bk	O
fred	O
draws	O
n	O
times	O
from	O
the	O
urn	B
exactly	O
as	O
in	O
exercise	O
obtaining	O
nb	O
blacks	O
and	O
computes	O
the	O
quantity	O
z	O
fbn	O
n	O
fb	O
what	O
is	O
the	O
expectation	B
of	O
z	O
in	O
the	O
case	O
n	O
and	O
fb	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
z	O
what	O
is	O
the	O
probability	B
that	O
z	O
compare	O
z	O
with	O
the	O
quantities	O
computed	O
in	O
the	O
previous	O
exercise	O
like	O
forward	B
probability	B
problems	O
inverse	B
probability	B
problems	O
involve	O
a	O
generative	B
model	B
of	O
a	O
process	O
but	O
instead	O
of	O
computing	O
the	O
probability	B
distribution	B
of	O
some	O
quantity	O
produced	O
by	O
the	O
process	O
we	O
compute	O
the	O
conditional	B
probability	B
of	O
one	O
or	O
more	O
of	O
the	O
unobserved	O
variables	O
in	O
the	O
process	O
given	O
the	O
observed	O
variables	O
this	O
invariably	O
requires	O
the	O
use	O
of	O
bayes	B
theorem	O
example	O
there	O
are	O
eleven	O
urns	O
labelled	O
by	O
u	O
each	O
containing	O
ten	O
balls	O
urn	B
u	O
contains	O
u	O
black	B
balls	O
and	O
u	O
white	B
balls	O
fred	O
selects	O
an	O
urn	B
u	O
at	O
random	B
and	O
draws	O
n	O
times	O
with	O
replacement	O
from	O
that	O
urn	B
obtaining	O
nb	O
blacks	O
and	O
n	O
nb	O
whites	O
fred	O
s	O
friend	O
bill	O
looks	O
on	O
if	O
after	O
n	O
draws	O
nb	O
blacks	O
have	O
been	O
drawn	O
what	O
is	O
the	O
probability	B
that	O
the	O
urn	B
fred	O
is	O
using	O
is	O
urn	B
u	O
from	O
bill	O
s	O
point	O
of	O
view	O
doesn	O
t	O
know	O
the	O
value	O
of	O
u	O
solution	O
the	O
joint	B
probability	B
distribution	B
of	O
the	O
random	B
variables	O
u	O
and	O
nb	O
can	O
be	O
written	O
p	O
nb	O
j	O
n	O
p	O
j	O
u	O
n	O
from	O
the	O
joint	B
probability	B
of	O
u	O
and	O
nb	O
we	O
can	O
obtain	O
the	O
conditional	B
distribution	B
of	O
u	O
given	O
nb	O
p	O
nb	O
n	O
p	O
nb	O
j	O
n	O
p	O
j	O
n	O
p	O
j	O
u	O
n	O
p	O
j	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
figure	O
joint	B
probability	B
of	O
u	O
and	O
nb	O
for	O
bill	O
and	O
fred	O
s	O
urn	B
problem	O
after	O
n	O
draws	O
u	O
u	O
p	O
nb	O
n	O
figure	O
conditional	B
probability	B
of	O
u	O
given	O
nb	O
and	O
n	O
u	O
nb	O
the	O
marginal	B
probability	B
of	O
u	O
is	O
p	O
for	O
all	O
u	O
you	O
wrote	O
down	O
the	O
probability	B
of	O
nb	O
given	O
u	O
and	O
n	O
p	O
j	O
u	O
n	O
when	O
you	O
solved	O
exercise	O
are	O
doing	O
the	O
highly	O
recommended	O
exercises	O
aren	O
t	O
you	O
if	O
we	O
fu	O
then	O
p	O
j	O
u	O
n	O
n	O
nb	O
u	O
what	O
about	O
the	O
denominator	O
p	O
j	O
n	O
this	O
is	O
the	O
marginal	B
probability	B
of	O
nb	O
which	O
we	O
can	O
obtain	O
using	O
the	O
sum	B
rule	I
p	O
j	O
n	O
p	O
nb	O
j	O
n	O
p	O
j	O
u	O
n	O
so	O
the	O
conditional	B
probability	B
of	O
u	O
given	O
nb	O
is	O
p	O
nb	O
n	O
p	O
j	O
u	O
n	O
p	O
j	O
n	O
n	O
p	O
j	O
n	O
nb	O
u	O
this	O
conditional	B
distribution	B
can	O
be	O
found	O
by	O
normalizing	O
column	O
of	O
and	O
is	O
shown	O
in	O
the	O
normalizing	O
constant	O
the	O
marginal	B
probability	B
of	O
nb	O
is	O
p	O
n	O
the	O
posterior	B
probability	B
is	O
correct	O
for	O
all	O
u	O
including	O
the	O
end-points	O
u	O
and	O
u	O
where	O
fu	O
and	O
fu	O
respectively	O
the	O
posterior	B
probability	B
that	O
u	O
given	O
nb	O
is	O
equal	O
to	O
zero	O
because	O
if	O
fred	O
were	O
drawing	O
from	O
urn	B
it	O
would	O
be	O
impossible	O
for	O
any	O
black	B
balls	O
to	O
be	O
drawn	O
the	O
posterior	B
probability	B
that	O
u	O
is	O
also	O
zero	O
because	O
there	O
are	O
no	O
white	B
balls	O
in	O
that	O
urn	B
the	O
other	O
hypotheses	O
u	O
u	O
u	O
all	O
have	O
non-zero	O
posterior	B
probability	B
terminology	O
of	O
inverse	B
probability	B
in	O
inverse	B
probability	B
problems	O
it	O
is	O
convenient	O
to	O
give	O
names	O
to	O
the	O
probabilities	O
appearing	O
in	O
bayes	B
theorem	O
in	O
equation	O
we	O
call	O
the	O
marginal	B
probability	B
p	O
the	O
prior	B
probability	B
of	O
u	O
and	O
p	O
j	O
u	O
n	O
is	O
called	O
the	O
likelihood	B
of	O
u	O
it	O
is	O
important	O
to	O
note	O
that	O
the	O
terms	O
likelihood	B
and	O
probability	B
are	O
not	O
synonyms	O
the	O
quantity	O
p	O
j	O
u	O
n	O
is	O
a	O
function	O
of	O
both	O
nb	O
and	O
u	O
for	O
u	O
p	O
j	O
u	O
n	O
a	O
probability	B
over	O
nb	O
for	O
nb	O
p	O
j	O
u	O
n	O
the	O
likelihood	B
of	O
u	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
forward	O
probabilities	O
and	O
inverse	O
probabilities	O
never	O
say	O
the	O
likelihood	B
of	O
the	O
data	O
always	O
say	O
the	O
likelihood	B
of	O
the	O
parameters	B
the	O
likelihood	B
function	O
is	O
not	O
a	O
probability	B
distribution	B
you	O
want	O
to	O
mention	O
the	O
data	O
that	O
a	O
likelihood	B
function	O
is	O
associated	O
with	O
you	O
may	O
say	O
the	O
likelihood	B
of	O
the	O
parameters	B
given	O
the	O
data	O
the	O
conditional	B
probability	B
p	O
nb	O
n	O
is	O
called	O
the	O
posterior	B
probability	B
of	O
u	O
given	O
nb	O
the	O
normalizing	O
constant	O
p	O
j	O
n	O
has	O
no	O
u-dependence	O
so	O
its	O
value	O
is	O
not	O
important	O
if	O
we	O
simply	O
wish	O
to	O
evaluate	O
the	O
relative	B
probabilities	O
of	O
the	O
alternative	O
hypotheses	O
u	O
however	O
in	O
most	O
data-modelling	O
problems	O
of	O
any	O
complexity	B
this	O
quantity	O
becomes	O
important	O
and	O
it	O
is	O
given	O
various	O
names	O
p	O
j	O
n	O
is	O
known	O
as	O
the	O
evidence	B
or	O
the	O
marginal	B
likelihood	B
if	O
denotes	O
the	O
unknown	O
parameters	B
d	O
denotes	O
the	O
data	O
and	O
h	O
denotes	O
the	O
overall	O
hypothesis	O
space	O
the	O
general	O
equation	O
is	O
written	O
p	O
j	O
dh	O
p	O
j	O
jh	O
p	O
jh	O
posterior	O
likelihood	B
prior	B
evidence	B
inverse	B
probability	B
and	O
prediction	B
example	O
assuming	O
again	O
that	O
bill	O
has	O
observed	O
nb	O
blacks	O
in	O
n	O
draws	O
let	O
fred	O
draw	O
another	O
ball	O
from	O
the	O
same	O
urn	B
what	O
is	O
the	O
probability	B
that	O
the	O
next	O
drawn	O
ball	O
is	O
a	O
black	B
should	O
make	O
use	O
of	O
the	O
posterior	O
probabilities	O
in	O
solution	O
by	O
the	O
sum	B
rule	I
p	O
is	O
black	B
j	O
nb	O
n	O
p	O
is	O
black	B
j	O
u	O
nb	O
n	O
nb	O
n	O
since	O
the	O
balls	O
are	O
drawn	O
with	O
replacement	O
from	O
the	O
chosen	O
urn	B
the	O
probability	B
p	O
is	O
black	B
j	O
u	O
nb	O
n	O
is	O
just	O
fu	O
whatever	O
nb	O
and	O
n	O
are	O
so	O
p	O
is	O
black	B
j	O
nb	O
n	O
fup	O
nb	O
n	O
using	O
the	O
values	O
of	O
p	O
nb	O
n	O
given	O
in	O
we	O
obtain	O
p	O
is	O
black	B
j	O
nb	O
n	O
comment	O
notice	O
the	O
between	O
this	O
prediction	B
obtained	O
using	O
probability	B
theory	O
and	O
the	O
widespread	O
practice	O
in	B
statistics	I
of	O
making	O
predictions	O
by	O
selecting	O
the	O
most	O
plausible	O
hypothesis	O
here	O
would	O
be	O
that	O
the	O
urn	B
is	O
urn	B
u	O
and	O
then	O
making	O
the	O
predictions	O
assuming	O
that	O
hypothesis	O
to	O
be	O
true	O
would	O
give	O
a	O
probability	B
of	O
that	O
the	O
next	O
ball	O
is	O
black	B
the	O
correct	O
prediction	B
is	O
the	O
one	O
that	O
takes	O
into	O
account	O
the	O
uncertainty	O
by	O
marginalizing	O
over	O
the	O
possible	O
values	O
of	O
the	O
hypothesis	O
u	O
marginalization	B
here	O
leads	O
to	O
slightly	O
more	O
moderate	O
less	O
extreme	O
predictions	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
inference	B
as	O
inverse	B
probability	B
now	O
consider	O
the	O
following	O
exercise	O
which	O
has	O
the	O
character	O
of	O
a	O
simple	O
investigation	O
example	O
bill	O
tosses	O
a	O
bent	B
coin	B
n	O
times	O
obtaining	O
a	O
sequence	B
of	O
heads	O
and	O
tails	O
we	O
assume	O
that	O
the	O
coin	B
has	O
a	O
probability	B
fh	O
of	O
coming	O
up	O
heads	O
we	O
do	O
not	O
know	O
fh	O
if	O
nh	O
heads	O
have	O
occurred	O
in	O
n	O
tosses	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
fh	O
example	O
n	O
might	O
be	O
and	O
nh	O
might	O
be	O
or	O
after	O
a	O
lot	O
more	O
tossing	O
we	O
might	O
have	O
n	O
and	O
nh	O
what	O
is	O
the	O
probability	B
that	O
the	O
n	O
outcome	O
will	O
be	O
a	O
head	O
given	O
nh	O
heads	O
in	O
n	O
tosses	O
unlike	O
example	O
this	O
problem	O
has	O
a	O
subjective	O
element	O
given	O
a	O
restricted	O
of	O
probability	B
that	O
says	O
probabilities	O
are	O
the	O
frequencies	O
of	O
random	B
variables	O
this	O
example	O
is	O
from	O
the	O
eleven-urns	O
example	O
whereas	O
the	O
urn	B
u	O
was	O
a	O
random	B
variable	I
the	O
bias	B
fh	O
of	O
the	O
coin	B
would	O
not	O
normally	O
be	O
called	O
a	O
random	B
variable	I
it	O
is	O
just	O
a	O
but	O
unknown	O
parameter	O
that	O
we	O
are	O
interested	O
in	O
yet	O
don	O
t	O
the	O
two	O
examples	O
and	O
seem	O
to	O
have	O
an	O
essential	O
similarity	O
when	O
n	O
and	O
nh	O
to	O
solve	O
example	O
we	O
have	O
to	O
make	O
an	O
assumption	O
about	O
what	O
the	O
bias	B
of	O
the	O
coin	B
fh	O
might	O
be	O
this	O
prior	B
probability	B
distribution	B
over	O
fh	O
p	O
corresponds	O
to	O
the	O
prior	B
over	O
u	O
in	O
the	O
eleven-urns	O
problem	O
in	O
that	O
example	O
the	O
helpful	O
problem	O
p	O
in	O
real	O
life	B
we	O
have	O
to	O
make	O
assumptions	B
in	O
order	O
to	O
assign	O
priors	O
these	O
assumptions	B
will	O
be	O
subjective	O
and	O
our	O
answers	O
will	O
depend	O
on	O
them	O
exactly	O
the	O
same	O
can	O
be	O
said	O
for	O
the	O
other	O
probabilities	O
in	O
our	O
generative	B
model	B
too	O
we	O
are	O
assuming	O
for	O
example	O
that	O
the	O
balls	O
are	O
drawn	O
from	O
an	O
urn	B
independently	O
but	O
could	O
there	O
not	O
be	O
correlations	B
in	O
the	O
sequence	B
because	O
fred	O
s	O
ball-drawing	O
action	O
is	O
not	O
perfectly	O
random	B
indeed	O
there	O
could	O
be	O
so	O
the	O
likelihood	B
function	O
that	O
we	O
use	O
depends	O
on	O
assumptions	B
too	O
in	O
real	O
data	O
modelling	B
problems	O
priors	O
are	O
subjective	O
and	O
so	O
are	O
likelihoods	O
here	O
p	O
denotes	O
a	O
probability	B
density	B
rather	O
than	O
a	O
probability	B
distribution	B
we	O
are	O
now	O
using	O
p	O
to	O
denote	O
probability	B
densities	O
over	O
continuous	B
variables	O
as	O
well	O
as	O
probabilities	O
over	O
discrete	O
variables	O
and	O
probabilities	O
of	O
logical	O
propositions	O
the	O
probability	B
that	O
a	O
continuous	B
variable	O
v	O
lies	O
between	O
values	O
a	O
dv	O
p	O
p	O
is	O
dimensionless	O
the	O
density	B
p	O
is	O
a	O
dimensional	O
quantity	O
having	O
dimensions	B
inverse	O
to	O
the	O
dimensions	B
of	O
v	O
in	O
contrast	O
to	O
discrete	O
probabilities	O
which	O
are	O
dimensionless	O
don	O
t	O
be	O
surprised	O
to	O
see	O
probability	B
densities	O
greater	O
than	O
this	O
is	O
normal	B
a	O
and	O
b	O
b	O
a	O
is	O
to	O
be	O
r	O
b	O
and	O
nothing	O
is	O
wrong	O
as	O
long	O
as	O
r	O
b	O
conditional	B
and	O
joint	B
probability	B
densities	O
are	O
in	O
just	O
the	O
same	O
way	O
as	O
conditional	B
and	O
joint	B
probabilities	O
a	O
dv	O
p	O
for	O
any	O
interval	O
b	O
exercise	O
assuming	O
a	O
uniform	O
prior	B
on	O
fh	O
p	O
solve	O
the	O
problem	O
posed	O
in	O
example	O
sketch	O
the	O
posterior	O
distribution	B
of	O
fh	O
and	O
compute	O
the	O
probability	B
that	O
the	O
n	O
outcome	O
will	O
be	O
a	O
head	O
for	O
n	O
and	O
nh	O
n	O
and	O
nh	O
n	O
and	O
nh	O
n	O
and	O
nh	O
you	O
will	O
the	O
beta	B
integral	B
useful	O
z	O
dpa	O
pfa	O
a	O
pafb	O
fb	O
fa	O
fb	O
fb	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
forward	O
probabilities	O
and	O
inverse	O
probabilities	O
you	O
may	O
also	O
it	O
instructive	O
to	O
look	O
back	O
at	O
example	O
and	O
equation	O
people	O
sometimes	O
confuse	O
assigning	B
a	O
prior	B
distribution	B
to	O
an	O
unknown	O
parameter	O
such	O
as	O
fh	O
with	O
making	O
an	O
initial	O
guess	O
of	O
the	O
value	O
of	O
the	O
parameter	O
but	O
the	O
prior	B
over	O
fh	O
p	O
is	O
not	O
a	O
simple	O
statement	O
like	O
initially	O
i	O
would	O
guess	O
fh	O
the	O
prior	B
is	O
a	O
probability	B
density	B
over	O
fh	O
which	O
the	O
prior	B
degree	B
of	O
belief	B
that	O
fh	O
lies	O
in	O
any	O
interval	O
f	O
it	O
may	O
well	O
be	O
the	O
case	O
that	O
our	O
prior	B
for	O
fh	O
is	O
symmetric	B
about	O
so	O
that	O
the	O
mean	B
of	O
fh	O
under	O
the	O
prior	B
is	O
in	O
this	O
case	O
the	O
predictive	B
distribution	B
for	O
the	O
toss	O
would	O
indeed	O
be	O
p	O
head	O
dfh	O
p	O
headj	O
fh	O
dfh	O
p	O
but	O
the	O
prediction	B
for	O
subsequent	O
tosses	O
will	O
depend	O
on	O
the	O
whole	O
prior	B
distribution	B
not	O
just	O
its	O
mean	B
data	O
compression	B
and	O
inverse	B
probability	B
consider	O
the	O
following	O
task	O
example	O
write	O
a	O
computer	B
program	O
capable	O
of	O
compressing	O
binary	O
like	O
this	O
one	O
the	O
string	O
shown	O
contains	O
and	O
intuitively	O
compression	B
works	O
by	O
taking	O
advantage	O
of	O
the	O
predictability	O
of	O
a	O
in	O
this	O
case	O
the	O
source	O
of	O
the	O
appears	O
more	O
likely	O
to	O
emit	O
than	O
a	O
data	O
compression	B
program	O
that	O
compresses	O
this	O
must	O
implicitly	O
or	O
explicitly	O
be	O
addressing	O
the	O
question	O
what	O
is	O
the	O
probability	B
that	O
the	O
next	O
character	O
in	O
this	O
is	O
a	O
do	O
you	O
think	O
this	O
problem	O
is	O
similar	O
in	O
character	O
to	O
example	O
i	O
do	O
one	O
of	O
the	O
themes	O
of	O
this	O
book	O
is	O
that	O
data	O
compression	B
and	O
data	O
modelling	B
are	O
one	O
and	O
the	O
same	O
and	O
that	O
they	O
should	O
both	O
be	O
addressed	O
like	O
the	O
urn	B
of	O
example	O
using	O
inverse	B
probability	B
example	O
is	O
solved	O
in	O
chapter	O
the	O
likelihood	B
principle	I
please	O
solve	O
the	O
following	O
two	O
exercises	O
a	O
b	O
example	O
urn	B
a	O
contains	O
three	O
balls	O
one	O
black	B
and	O
two	O
white	B
urn	B
b	O
contains	O
three	O
balls	O
two	O
black	B
and	O
one	O
white	B
one	O
of	O
the	O
urns	O
is	O
selected	O
at	O
random	B
and	O
one	O
ball	O
is	O
drawn	O
the	O
ball	O
is	O
black	B
what	O
is	O
the	O
probability	B
that	O
the	O
selected	O
urn	B
is	O
urn	B
a	O
figure	O
urns	O
for	O
example	O
example	O
urn	B
a	O
contains	O
balls	O
one	O
black	B
two	O
white	B
one	O
green	O
and	O
one	O
pink	O
urn	B
b	O
contains	O
hundred	O
balls	O
two	O
hundred	O
black	B
one	O
hundred	O
white	B
yellow	O
cyan	O
sienna	O
green	O
silver	O
gold	O
and	O
purple	O
of	O
a	O
s	O
balls	O
are	O
black	B
of	O
b	O
s	O
are	O
black	B
one	O
of	O
the	O
urns	O
is	O
selected	O
at	O
random	B
and	O
one	O
ball	O
is	O
drawn	O
the	O
ball	O
is	O
black	B
what	O
is	O
the	O
probability	B
that	O
the	O
urn	B
is	O
urn	B
a	O
p	O
g	O
s	O
c	O
g	O
p	O
y	O
figure	O
urns	O
for	O
example	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
ai	O
pi	O
hpi	O
i	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
what	O
do	O
you	O
notice	O
about	O
your	O
solutions	O
does	O
each	O
answer	O
depend	O
on	O
the	O
detailed	O
contents	O
of	O
each	O
urn	B
the	O
details	O
of	O
the	O
other	O
possible	O
outcomes	O
and	O
their	O
probabilities	O
are	O
irrelevant	O
all	O
that	O
matters	O
is	O
the	O
probability	B
of	O
the	O
outcome	O
that	O
actually	O
happened	O
that	O
the	O
ball	O
drawn	O
was	O
black	B
given	O
the	O
hypotheses	O
we	O
need	O
only	O
to	O
know	O
the	O
likelihood	B
i	O
e	O
how	O
the	O
probability	B
of	O
the	O
data	O
that	O
happened	O
varies	O
with	O
the	O
hypothesis	O
this	O
simple	O
rule	O
about	O
inference	B
is	O
known	O
as	O
the	O
likelihood	B
principle	I
the	O
likelihood	B
principle	I
given	O
a	O
generative	B
model	B
for	O
data	O
d	O
given	O
parameters	B
p	O
and	O
having	O
observed	O
a	O
particular	O
outcome	O
all	O
inferences	O
and	O
predictions	O
should	O
depend	O
only	O
on	O
the	O
function	O
p	O
j	O
in	O
spite	O
of	O
the	O
simplicity	O
of	O
this	O
principle	O
many	O
classical	O
statistical	B
methods	O
violate	O
it	O
of	O
entropy	B
and	O
related	O
functions	B
the	O
shannon	B
information	B
content	I
of	O
an	O
outcome	O
x	O
is	O
to	O
be	O
hx	O
p	O
it	O
is	O
measured	O
in	O
bits	O
word	O
bit	B
is	O
also	O
used	O
to	O
denote	O
a	O
variable	O
whose	O
value	O
is	O
or	O
i	O
hope	O
context	O
will	O
always	O
make	O
clear	O
which	O
of	O
the	O
two	O
meanings	O
is	O
intended	O
in	O
the	O
next	O
few	O
chapters	O
we	O
will	O
establish	O
that	O
the	O
shannon	B
information	B
content	I
hai	O
is	O
indeed	O
a	O
natural	B
measure	O
of	O
the	O
information	B
content	I
of	O
the	O
event	O
x	O
ai	O
at	O
that	O
point	O
we	O
will	O
shorten	O
the	O
name	O
of	O
this	O
quantity	O
to	O
the	O
information	B
content	I
the	O
fourth	O
column	O
in	O
table	O
shows	O
the	O
shannon	B
information	B
content	I
of	O
the	O
possible	O
outcomes	O
when	O
a	O
random	B
character	O
is	O
picked	O
from	O
an	O
english	B
document	O
the	O
outcome	O
x	O
z	O
has	O
a	O
shannon	B
information	B
content	I
of	O
bits	O
and	O
x	O
e	O
has	O
an	O
information	B
content	I
of	O
bits	O
the	O
entropy	B
of	O
an	O
ensemble	B
x	O
is	O
to	O
be	O
the	O
average	O
shannon	B
in	O
formation	O
content	O
of	O
an	O
outcome	O
hx	O
p	O
log	O
p	O
convention	O
for	O
p	O
with	O
the	O
log	O
like	O
the	O
information	B
content	I
entropy	B
is	O
measured	O
in	O
bits	O
that	O
log	O
since	O
when	O
it	O
is	O
convenient	O
we	O
may	O
also	O
write	O
hx	O
as	O
hp	O
where	O
p	O
is	O
the	O
vector	O
pi	O
another	O
name	O
for	O
the	O
entropy	B
of	O
x	O
is	O
the	O
uncertainty	O
of	O
x	O
example	O
the	O
entropy	B
of	O
a	O
randomly	O
selected	O
letter	O
in	O
an	O
english	B
document	O
is	O
about	O
bits	O
assuming	O
its	O
probability	B
is	O
as	O
given	O
in	O
table	O
we	O
obtain	O
this	O
number	O
by	O
averaging	O
log	O
in	O
the	O
fourth	O
column	O
under	O
the	O
probability	B
distribution	B
pi	O
in	O
the	O
third	O
column	O
pi	O
pi	O
xi	O
table	O
shannon	B
information	B
contents	O
of	O
the	O
outcomes	O
az	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decomposability	O
of	O
the	O
entropy	B
we	O
now	O
note	O
some	O
properties	O
of	O
the	O
entropy	B
function	O
hx	O
with	O
equality	O
pi	O
for	O
one	O
i	O
means	O
if	O
and	O
only	O
if	O
entropy	B
is	O
maximized	O
if	O
p	O
is	O
uniform	O
hx	O
logjaxj	O
with	O
equality	O
pi	O
for	O
all	O
i	O
notation	B
the	O
vertical	O
bars	O
j	O
j	O
have	O
two	O
meanings	O
if	O
ax	O
is	O
a	O
set	B
jaxj	O
denotes	O
the	O
number	O
of	O
elements	O
in	O
ax	O
if	O
x	O
is	O
a	O
number	O
then	O
jxj	O
is	O
the	O
absolute	B
value	I
of	O
x	O
the	O
redundancy	B
measures	O
the	O
fractional	O
between	O
hx	O
and	O
its	O
maximum	O
possible	O
value	O
logjaxj	O
the	O
redundancy	B
of	O
x	O
is	O
hx	O
log	O
jaxj	O
we	O
won	O
t	O
make	O
use	O
of	O
redundancy	B
in	O
this	O
book	O
so	O
i	O
have	O
not	O
assigned	O
a	O
symbol	O
to	O
it	O
the	O
joint	B
entropy	B
of	O
x	O
y	O
is	O
hx	O
y	O
p	O
y	O
log	O
p	O
y	O
entropy	B
is	O
additive	O
for	O
independent	O
random	B
variables	O
hx	O
y	O
hx	O
hy	O
p	O
y	O
p	O
our	O
for	O
information	B
content	I
so	O
far	O
apply	O
only	O
to	O
discrete	O
probability	B
distributions	O
over	O
sets	O
ax	O
the	O
can	O
be	O
extended	B
to	O
sets	O
though	O
the	O
entropy	B
may	O
then	O
be	O
the	O
case	O
of	O
a	O
probability	B
density	B
over	O
a	O
continuous	B
set	B
is	O
addressed	O
in	O
section	B
further	O
important	O
and	O
exercises	O
to	O
do	O
with	O
entropy	B
will	O
come	O
along	O
in	O
section	B
decomposability	O
of	O
the	O
entropy	B
the	O
entropy	B
function	O
a	O
recursive	O
property	O
that	O
can	O
be	O
very	O
useful	O
when	O
computing	O
entropies	O
for	O
convenience	O
we	O
ll	O
stretch	O
our	O
notation	B
so	O
that	O
we	O
can	O
write	O
hx	O
as	O
hp	O
where	O
p	O
is	O
the	O
probability	B
vector	O
associated	O
with	O
the	O
ensemble	B
x	O
let	O
s	O
illustrate	O
the	O
property	O
by	O
an	O
example	O
imagine	O
that	O
a	O
random	B
variable	I
x	O
is	O
created	O
by	O
a	O
fair	O
coin	B
to	O
determine	O
whether	O
x	O
then	O
if	O
x	O
is	O
not	O
a	O
fair	O
coin	B
a	O
second	O
time	O
to	O
determine	O
whether	O
x	O
is	O
or	O
the	O
probability	B
distribution	B
of	O
x	O
is	O
p	O
p	O
p	O
what	O
is	O
the	O
entropy	B
of	O
x	O
we	O
can	O
either	O
compute	O
it	O
by	O
brute	O
force	O
hx	O
log	O
log	O
log	O
or	O
we	O
can	O
use	O
the	O
following	O
decomposition	O
in	O
which	O
the	O
value	O
of	O
x	O
is	O
revealed	O
gradually	O
imagine	O
learning	B
whether	O
x	O
and	O
then	O
if	O
x	O
is	O
not	O
learning	B
which	O
non-zero	O
value	O
is	O
the	O
case	O
the	O
revelation	O
of	O
whether	O
x	O
or	O
not	O
entails	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
revealing	O
a	O
binary	O
variable	O
whose	O
probability	B
distribution	B
is	O
this	O
revelation	O
has	O
an	O
entropy	B
log	O
bit	B
if	O
x	O
is	O
not	O
we	O
learn	O
the	O
value	O
of	O
the	O
second	O
coin	B
this	O
too	O
is	O
a	O
binary	O
variable	O
whose	O
probability	B
distribution	B
is	O
and	O
whose	O
entropy	B
is	O
bit	B
we	O
only	O
get	O
to	O
experience	O
the	O
second	O
revelation	O
half	O
the	O
time	O
however	O
so	O
the	O
entropy	B
can	O
be	O
written	O
log	O
hx	O
generalizing	O
the	O
observation	O
we	O
are	O
making	O
about	O
the	O
entropy	B
of	O
any	O
probability	B
distribution	B
p	O
pig	O
is	O
that	O
hp	O
pi	O
when	O
it	O
s	O
written	O
as	O
a	O
formula	O
this	O
property	O
looks	O
regrettably	O
ugly	O
nev	O
ertheless	O
it	O
is	O
a	O
simple	O
property	O
and	O
one	O
that	O
you	O
should	O
make	O
use	O
of	O
generalizing	O
further	O
the	O
entropy	B
has	O
the	O
property	O
for	O
any	O
m	O
that	O
hp	O
h	O
pm	O
pi	O
pm	O
pi	O
pm	O
pi	O
example	O
a	O
source	O
produces	O
a	O
character	O
x	O
from	O
the	O
alphabet	O
a	O
a	O
b	O
zg	O
with	O
probability	B
x	O
is	O
a	O
numeral	O
with	O
probability	B
x	O
is	O
a	O
vowel	O
e	O
i	O
o	O
u	O
and	O
with	O
probability	B
it	O
s	O
one	O
of	O
the	O
consonants	O
all	O
numerals	O
are	O
equiprobable	O
and	O
the	O
same	O
goes	O
for	O
vowels	O
and	O
consonants	O
estimate	O
the	O
entropy	B
of	O
x	O
solution	O
log	O
log	O
log	O
log	O
log	O
log	O
bits	O
the	O
ei	O
in	O
leibler	O
is	O
pronounced	O
the	O
same	O
as	O
in	O
heist	O
gibbs	B
inequality	B
the	O
relative	B
entropy	B
or	O
kullbackleibler	O
divergence	B
between	O
two	O
probability	B
distributions	O
p	O
and	O
qx	O
that	O
are	O
over	O
the	O
same	O
alphabet	O
ax	O
is	O
dklpjjq	O
p	O
log	O
p	O
qx	O
the	O
relative	B
entropy	B
gibbs	B
inequality	B
dklpjjq	O
with	O
equality	O
only	O
if	O
p	O
q	O
note	O
that	O
in	O
general	O
the	O
relative	B
entropy	B
is	O
not	O
symmetric	B
under	O
interchange	O
of	O
the	O
distributions	O
p	O
and	O
q	O
in	O
general	O
dklpjjq	O
dklqjjp	O
so	O
dkl	B
although	O
it	O
is	O
sometimes	O
called	O
the	O
kl	B
distance	B
is	O
not	O
strictly	O
a	O
distance	B
the	O
relative	B
entropy	B
is	O
important	O
in	O
pattern	B
recognition	B
and	O
neural	O
networks	O
as	O
well	O
as	O
in	O
information	B
theory	I
gibbs	B
inequality	B
is	O
probably	O
the	O
most	O
important	O
inequality	B
in	O
this	O
book	O
it	O
and	O
many	O
other	O
inequalities	O
can	O
be	O
proved	O
using	O
the	O
concept	O
of	O
convexity	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
jensen	O
s	O
inequality	B
for	O
convex	O
functions	B
jensen	O
s	O
inequality	B
for	O
convex	O
functions	B
the	O
words	O
convex	O
and	O
concave	O
may	O
be	O
pronounced	O
convex-smile	O
and	O
concave-frown	O
this	O
terminology	O
has	O
useful	O
redundancy	B
while	O
one	O
may	O
forget	O
which	O
way	O
up	O
convex	O
and	O
concave	O
are	O
it	O
is	O
harder	O
to	O
confuse	O
a	O
smile	O
with	O
a	O
frown	O
convex	O
functions	B
a	O
function	O
f	O
is	O
convex	O
over	O
b	O
if	O
every	O
chord	O
of	O
the	O
function	O
lies	O
above	O
the	O
function	O
as	O
shown	O
in	O
that	O
is	O
for	O
all	O
b	O
and	O
f	O
a	O
function	O
f	O
is	O
strictly	O
convex	O
if	O
for	O
all	O
b	O
the	O
equality	O
holds	O
only	O
for	O
and	O
similar	O
apply	O
to	O
concave	O
and	O
strictly	O
concave	O
functions	B
f	O
figure	O
of	O
convexity	B
some	O
strictly	O
convex	O
functions	B
are	O
ex	O
and	O
for	O
all	O
x	O
and	O
x	O
log	O
x	O
for	O
x	O
log	O
x	O
x	O
log	O
x	O
figure	O
convex	O
functions	B
centre	B
of	I
gravity	I
jensen	O
s	O
inequality	B
if	O
f	O
is	O
a	O
convex	O
function	O
and	O
x	O
is	O
a	O
random	B
variable	I
then	O
e	O
f	O
where	O
e	O
denotes	O
expectation	B
if	O
f	O
is	O
strictly	O
convex	O
and	O
e	O
f	O
then	O
the	O
random	B
variable	I
x	O
is	O
a	O
constant	O
jensen	O
s	O
inequality	B
can	O
also	O
be	O
rewritten	O
for	O
a	O
concave	O
function	O
with	O
the	O
direction	O
of	O
the	O
inequality	B
reversed	O
a	O
physical	O
version	O
of	O
jensen	O
s	O
inequality	B
runs	O
as	O
follows	O
if	O
a	O
collection	O
of	O
masses	O
pi	O
are	O
placed	O
on	O
a	O
convex	O
curve	O
f	O
at	O
locations	O
f	O
then	O
the	O
centre	B
of	I
gravity	I
of	O
those	O
masses	O
which	O
is	O
at	O
lies	O
above	O
the	O
curve	O
if	O
this	O
fails	O
to	O
convince	O
you	O
then	O
feel	O
free	O
to	O
do	O
the	O
following	O
exercise	O
exercise	O
prove	O
jensen	O
s	O
inequality	B
example	O
three	O
squares	O
have	O
average	O
area	O
the	O
average	O
of	O
the	O
lengths	O
of	O
their	O
sides	O
is	O
m	O
what	O
can	O
be	O
said	O
about	O
the	O
size	O
of	O
the	O
largest	O
of	O
the	O
three	O
squares	O
jensen	O
s	O
inequality	B
solution	O
let	O
x	O
be	O
the	O
length	O
of	O
the	O
side	O
of	O
a	O
square	B
and	O
let	O
the	O
probability	B
of	O
x	O
be	O
over	O
the	O
three	O
lengths	O
then	O
the	O
information	B
that	O
we	O
have	O
is	O
that	O
e	O
and	O
e	O
where	O
f	O
is	O
the	O
function	O
mapping	B
lengths	O
to	O
areas	O
this	O
is	O
a	O
strictly	O
convex	O
function	O
we	O
notice	O
that	O
the	O
equality	O
e	O
f	O
holds	O
therefore	O
x	O
is	O
a	O
constant	O
and	O
the	O
three	O
lengths	O
must	O
all	O
be	O
equal	O
the	O
area	O
of	O
the	O
largest	O
square	B
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
convexity	B
and	O
concavity	O
also	O
relate	O
to	O
maximization	O
if	O
f	O
is	O
concave	O
and	O
there	O
exists	O
a	O
point	O
at	O
which	O
for	O
all	O
k	O
then	O
f	O
has	O
its	O
maximum	O
value	O
at	O
that	O
point	O
the	O
converse	O
does	O
not	O
hold	O
if	O
a	O
concave	O
f	O
is	O
maximized	O
at	O
some	O
x	O
it	O
is	O
not	O
necessarily	O
true	O
that	O
the	O
gradient	O
rf	O
is	O
equal	O
to	O
zero	O
there	O
for	O
example	O
f	O
is	O
maximized	O
at	O
x	O
where	O
its	O
derivative	O
is	O
and	O
f	O
logp	O
for	O
a	O
probability	B
p	O
is	O
maximized	O
on	O
the	O
boundary	O
of	O
the	O
range	O
at	O
p	O
where	O
the	O
gradient	O
df	O
exercises	O
sums	O
of	O
random	B
variables	O
exercise	O
two	O
ordinary	O
dice	O
with	O
faces	O
labelled	O
are	O
thrown	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
sum	O
of	O
the	O
values	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
absolute	O
between	O
the	O
values	O
one	O
hundred	O
ordinary	O
dice	O
are	O
thrown	O
what	O
roughly	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
sum	O
of	O
the	O
values	O
sketch	O
the	O
probability	B
distribution	B
and	O
estimate	O
its	O
mean	B
and	O
standard	B
deviation	I
how	O
can	O
two	O
cubical	O
dice	O
be	O
labelled	O
using	O
the	O
numbers	O
so	O
that	O
when	O
the	O
two	O
dice	O
are	O
thrown	O
the	O
sum	O
has	O
a	O
uniform	O
probability	B
distribution	B
over	O
the	O
integers	O
is	O
there	O
any	O
way	O
that	O
one	O
hundred	O
dice	O
could	O
be	O
labelled	O
with	O
integers	O
such	O
that	O
the	O
probability	B
distribution	B
of	O
the	O
sum	O
is	O
uniform	O
inference	B
problems	O
exercise	O
if	O
q	O
p	O
and	O
a	O
ln	O
pq	O
show	O
that	O
p	O
sketch	O
this	O
function	O
and	O
its	O
relationship	O
to	O
the	O
hyperbolic	O
tangent	O
function	O
tanhu	O
it	O
will	O
be	O
useful	O
to	O
be	O
in	O
logarithms	B
also	O
if	O
b	O
log	O
pq	O
what	O
is	O
p	O
as	O
a	O
function	O
of	O
b	O
exercise	O
let	O
x	O
and	O
y	O
be	O
dependent	O
random	B
variables	O
with	O
x	O
a	O
binary	O
variable	O
taking	O
values	O
in	O
ax	O
use	O
bayes	B
theorem	O
to	O
show	O
that	O
the	O
log	O
posterior	B
probability	B
ratio	O
for	O
x	O
given	O
y	O
is	O
log	O
p	O
y	O
p	O
y	O
log	O
p	O
j	O
x	O
p	O
j	O
x	O
log	O
p	O
p	O
exercise	O
let	O
x	O
and	O
be	O
random	B
variables	O
such	O
that	O
and	O
are	O
conditionally	O
independent	O
given	O
a	O
binary	O
variable	O
x	O
use	O
bayes	B
theorem	O
to	O
show	O
that	O
the	O
posterior	B
probability	B
ratio	O
for	O
x	O
given	O
fdig	O
is	O
p	O
p	O
p	O
j	O
x	O
p	O
j	O
x	O
p	O
j	O
x	O
p	O
j	O
x	O
p	O
p	O
this	O
exercise	O
is	O
intended	O
to	O
help	O
you	O
think	O
about	O
the	O
central-limit	B
theorem	I
which	O
says	O
that	O
if	O
independent	O
random	B
variables	O
xn	O
have	O
means	O
and	O
variances	O
n	O
then	O
in	O
the	O
has	O
a	O
distribution	B
that	O
tends	O
to	O
a	O
normal	B
distribution	B
limit	O
of	O
large	O
n	O
the	O
sum	O
pn	O
xn	O
with	O
meanpn	O
and	O
variance	B
pn	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
life	B
in	O
high-dimensional	O
spaces	O
probability	B
distributions	O
and	O
volumes	O
have	O
some	O
unexpected	O
properties	O
in	O
high-dimensional	O
spaces	O
exercise	O
consider	O
a	O
sphere	O
of	O
radius	O
r	O
in	O
an	O
n	O
real	O
space	O
show	O
that	O
the	O
fraction	O
of	O
the	O
volume	B
of	O
the	O
sphere	O
that	O
is	O
in	O
the	O
surface	O
shell	O
lying	O
at	O
values	O
of	O
the	O
radius	O
between	O
r	O
and	O
r	O
where	O
r	O
is	O
f	O
evaluate	O
f	O
for	O
the	O
cases	O
n	O
n	O
and	O
n	O
with	O
implication	O
points	O
that	O
are	O
uniformly	O
distributed	O
in	O
a	O
sphere	O
in	O
n	O
dimensions	B
where	O
n	O
is	O
large	O
are	O
very	O
likely	O
to	O
be	O
in	O
a	O
thin	B
shell	I
near	O
the	O
surface	O
expectations	O
and	O
entropies	O
you	O
are	O
probably	O
familiar	O
with	O
the	O
idea	O
of	O
computing	O
the	O
expectation	B
of	O
a	O
function	O
of	O
x	O
e	O
hf	O
p	O
maybe	O
you	O
are	O
not	O
so	O
comfortable	O
with	O
computing	O
this	O
expectation	B
in	O
cases	O
where	O
the	O
function	O
f	O
depends	O
on	O
the	O
probability	B
p	O
the	O
next	O
few	O
examples	O
address	B
this	O
concern	O
exercise	O
let	O
pa	O
pb	O
and	O
pc	O
let	O
f	O
f	O
and	O
f	O
what	O
is	O
e	O
what	O
is	O
e	O
exercise	O
for	O
an	O
arbitrary	O
ensemble	B
what	O
is	O
e	O
exercise	O
let	O
pa	O
pb	O
and	O
pc	O
let	O
ga	O
gb	O
and	O
gc	O
what	O
is	O
e	O
exercise	O
let	O
pa	O
pb	O
and	O
pc	O
what	O
is	O
the	O
proba	O
bility	O
that	O
p	O
what	O
is	O
p	O
log	O
exercise	O
prove	O
the	O
assertion	O
that	O
hx	O
logjaxj	O
with	O
equality	O
pi	O
for	O
all	O
i	O
denotes	O
the	O
number	O
of	O
elements	O
in	O
the	O
set	B
ax	O
use	O
jensen	O
s	O
inequality	B
if	O
your	O
attempt	O
to	O
use	O
jensen	O
does	O
not	O
succeed	O
remember	O
that	O
jensen	O
involves	O
both	O
a	O
random	B
variable	I
and	O
a	O
function	O
and	O
you	O
have	O
quite	O
a	O
lot	O
of	O
freedom	O
in	O
choosing	O
these	O
think	O
about	O
whether	O
your	O
chosen	O
function	O
f	O
should	O
be	O
convex	O
or	O
concave	O
exercise	O
prove	O
that	O
the	O
relative	B
entropy	B
dklpjjq	O
inequality	B
with	O
equality	O
only	O
if	O
p	O
q	O
exercise	O
prove	O
that	O
the	O
entropy	B
is	O
indeed	O
decomposable	O
as	O
described	O
in	O
equations	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
g	O
h	O
f	O
exercise	O
a	O
random	B
variable	I
x	O
is	O
selected	O
by	O
a	O
bent	B
coin	B
with	O
bias	B
f	O
to	O
determine	O
whether	O
the	O
outcome	O
is	O
in	O
or	O
then	O
either	O
a	O
second	O
bent	B
coin	B
with	O
bias	B
g	O
or	O
a	O
third	O
bent	B
coin	B
with	O
bias	B
h	O
respectively	O
write	O
down	O
the	O
probability	B
distribution	B
of	O
x	O
use	O
the	O
decomposability	O
of	O
the	O
entropy	B
to	O
the	O
entropy	B
of	O
x	O
how	O
compact	O
an	O
expression	O
is	O
obtained	O
if	O
you	O
make	O
use	O
of	O
the	O
binary	B
entropy	B
function	I
compared	O
with	O
writing	B
out	O
the	O
four-term	O
entropy	B
explicitly	O
find	O
the	O
derivative	O
of	O
hx	O
with	O
respect	O
to	O
f	O
xx	O
exercise	O
an	O
unbiased	O
coin	B
is	O
until	O
one	O
head	O
is	O
thrown	O
what	O
is	O
the	O
entropy	B
of	O
the	O
random	B
variable	I
x	O
the	O
number	O
of	O
repeat	O
the	O
calculation	O
for	O
the	O
case	O
of	O
a	O
biased	O
coin	B
with	O
probability	B
f	O
of	O
coming	O
up	O
heads	O
solve	O
the	O
problem	O
both	O
directly	O
and	O
by	O
using	O
the	O
decomposability	O
of	O
the	O
entropy	B
further	O
exercises	O
forward	B
probability	B
exercise	O
an	O
urn	B
contains	O
w	O
white	B
balls	O
and	O
b	O
black	B
balls	O
two	O
balls	O
are	O
drawn	O
one	O
after	O
the	O
other	O
without	O
replacement	O
prove	O
that	O
the	O
probability	B
that	O
the	O
ball	O
is	O
white	B
is	O
equal	O
to	O
the	O
probability	B
that	O
the	O
second	O
is	O
white	B
exercise	O
a	O
circular	O
coin	B
of	O
diameter	O
a	O
is	O
thrown	O
onto	O
a	O
square	B
grid	O
whose	O
squares	O
are	O
b	O
b	O
b	O
what	O
is	O
the	O
probability	B
that	O
the	O
coin	B
will	O
lie	O
entirely	O
within	O
one	O
square	B
exercise	O
s	O
needle	B
a	O
needle	B
of	O
length	O
a	O
is	O
thrown	O
onto	O
a	O
plane	O
covered	O
with	O
equally	O
spaced	O
parallel	O
lines	O
with	O
separation	B
b	O
what	O
is	O
the	O
probability	B
that	O
the	O
needle	B
will	O
cross	O
a	O
line	O
if	O
a	O
b	O
s	O
noodle	B
on	O
average	O
a	O
random	B
curve	O
of	O
length	O
a	O
is	O
expected	O
to	O
intersect	O
the	O
lines	O
times	O
exercise	O
two	O
points	O
are	O
selected	O
at	O
random	B
on	O
a	O
straight	O
line	O
segment	O
of	O
length	O
what	O
is	O
the	O
probability	B
that	O
a	O
triangle	B
can	O
be	O
constructed	O
out	O
of	O
the	O
three	O
resulting	O
segments	O
exercise	O
an	O
unbiased	O
coin	B
is	O
until	O
one	O
head	O
is	O
thrown	O
what	O
is	O
the	O
expected	O
number	O
of	O
tails	O
and	O
the	O
expected	O
number	O
of	O
heads	O
fred	O
who	O
doesn	O
t	O
know	O
that	O
the	O
coin	B
is	O
unbiased	O
estimates	O
the	O
bias	B
using	O
hh	O
t	O
where	O
h	O
and	O
t	O
are	O
the	O
numbers	O
of	O
heads	O
and	O
tails	O
tossed	O
compute	O
and	O
sketch	O
the	O
probability	B
distribution	B
of	O
n	O
b	O
this	O
is	O
a	O
forward	B
probability	B
problem	O
a	O
sampling	B
theory	I
problem	O
not	O
an	O
inference	B
problem	O
don	O
t	O
use	O
bayes	B
theorem	O
exercise	O
fred	O
rolls	O
an	O
unbiased	O
six-sided	O
die	B
once	O
per	O
second	O
not	O
ing	O
the	O
occasions	O
when	O
the	O
outcome	O
is	O
a	O
six	B
what	O
is	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
one	O
six	B
to	O
the	O
next	O
six	B
between	O
two	O
rolls	O
the	O
clock	O
strikes	O
one	O
what	O
is	O
the	O
mean	B
number	O
of	O
rolls	O
until	O
the	O
next	O
six	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
now	O
think	O
back	O
before	O
the	O
clock	O
struck	O
what	O
is	O
the	O
mean	B
number	O
of	O
rolls	O
going	O
back	O
in	O
time	O
until	O
the	O
most	O
recent	O
six	B
what	O
is	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
the	O
six	B
before	O
the	O
clock	O
struck	O
to	O
the	O
next	O
six	B
is	O
your	O
answer	O
to	O
from	O
your	O
answer	O
to	O
explain	O
another	O
version	O
of	O
this	O
exercise	O
refers	O
to	O
fred	O
waiting	B
for	I
a	I
bus	I
at	O
a	O
bus-stop	B
in	O
poissonville	B
where	O
buses	O
arrive	O
independently	O
at	O
random	B
poisson	B
process	I
with	O
on	O
average	O
one	O
bus	O
every	O
six	B
minutes	O
what	O
is	O
the	O
average	O
wait	O
for	O
a	O
bus	O
after	O
fred	O
arrives	O
at	O
the	O
stop	O
minutes	O
so	O
what	O
is	O
the	O
time	O
between	O
the	O
two	O
buses	O
the	O
one	O
that	O
fred	O
just	O
missed	O
and	O
the	O
one	O
that	O
he	O
catches	O
minutes	O
explain	O
the	O
apparent	O
paradox	B
note	O
the	O
contrast	O
with	O
the	O
situation	O
in	O
clockville	B
where	O
the	O
buses	O
are	O
spaced	O
exactly	O
minutes	O
apart	O
there	O
as	O
you	O
can	O
the	O
mean	B
wait	O
at	O
a	O
bus-stop	B
is	O
minutes	O
and	O
the	O
time	O
between	O
the	O
missed	O
bus	O
and	O
the	O
next	O
one	O
is	O
minutes	O
conditional	B
probability	B
exercise	O
you	O
meet	O
fred	O
fred	O
tells	O
you	O
he	O
has	O
two	O
brothers	O
alf	O
and	O
bob	B
what	O
is	O
the	O
probability	B
that	O
fred	O
is	O
older	O
than	O
bob	B
fred	O
tells	O
you	O
that	O
he	O
is	O
older	O
than	O
alf	O
now	O
what	O
is	O
the	O
probability	B
that	O
fred	O
is	O
older	O
than	O
bob	B
is	O
what	O
is	O
the	O
conditional	B
probability	B
that	O
f	O
b	O
given	O
that	O
f	O
a	O
exercise	O
the	O
inhabitants	O
of	O
an	O
island	O
tell	O
the	O
truth	O
one	O
third	O
of	O
the	O
time	O
they	O
lie	O
with	O
probability	B
on	O
an	O
occasion	O
after	O
one	O
of	O
them	O
made	O
a	O
statement	O
you	O
ask	O
another	O
was	O
that	O
statement	O
true	O
and	O
he	O
says	O
yes	O
what	O
is	O
the	O
probability	B
that	O
the	O
statement	O
was	O
indeed	O
true	O
exercise	O
compare	O
two	O
ways	O
of	O
computing	O
the	O
probability	B
of	I
error	I
of	O
the	O
repetition	B
code	I
assuming	O
a	O
binary	B
symmetric	B
channel	I
did	O
this	O
once	O
for	O
exercise	O
and	O
that	O
they	O
give	O
the	O
same	O
answer	O
binomial	B
distribution	B
method	O
add	O
the	O
probability	B
that	O
all	O
three	O
bits	O
are	O
to	O
the	O
probability	B
that	O
exactly	O
two	O
bits	O
are	O
sum	B
rule	I
method	O
using	O
the	O
sum	B
rule	I
compute	O
the	O
marginal	B
probability	B
that	O
r	O
takes	O
on	O
each	O
of	O
the	O
eight	O
possible	O
values	O
p	O
ps	O
p	O
s	O
then	O
compute	O
the	O
posterior	O
probabil	O
ity	O
of	O
s	O
for	O
each	O
of	O
the	O
eight	O
values	O
of	O
r	O
fact	O
by	O
symmetry	O
only	O
two	O
example	O
cases	O
r	O
and	O
r	O
need	O
be	O
considered	O
notice	O
that	O
some	O
of	O
the	O
inferred	O
bits	O
are	O
better	O
determined	O
than	O
others	O
from	O
the	O
posterior	B
probability	B
p	O
r	O
you	O
can	O
read	O
out	O
the	O
case-by-case	O
error	B
probability	B
the	O
probability	B
that	O
the	O
more	O
probable	O
hypothesis	O
is	O
not	O
correct	O
p	O
r	O
find	O
the	O
average	O
error	B
probability	B
using	O
the	O
sum	B
rule	I
p	O
p	O
j	O
r	O
equation	O
gives	O
the	O
posterior	B
probability	B
of	O
the	O
input	O
s	O
given	O
the	O
received	O
vector	O
r	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
exercise	O
the	O
frequency	B
pn	O
of	O
the	O
nth	O
most	O
frequent	O
word	O
in	O
english	B
is	O
roughly	O
approximated	O
by	O
pn	O
n	O
for	O
n	O
n	O
remarkable	O
law	O
is	O
known	O
as	O
zipf	B
s	O
law	O
and	O
applies	O
to	O
the	O
word	O
frequencies	O
of	O
many	O
languages	O
if	O
we	O
assume	O
that	O
english	B
is	O
generated	O
by	O
picking	O
words	O
at	O
random	B
according	O
to	O
this	O
distribution	B
what	O
is	O
the	O
entropy	B
of	O
english	B
word	O
calculation	O
can	O
be	O
found	O
in	O
prediction	B
and	O
entropy	B
of	O
printed	O
english	B
c	O
e	O
shannon	B
bell	O
syst	O
tech	O
j	O
but	O
inexplicably	O
the	O
great	O
man	O
made	O
numerical	O
errors	B
in	O
it	O
solutions	O
solution	O
to	O
exercise	O
no	O
they	O
are	O
not	O
independent	O
if	O
they	O
were	O
then	O
all	O
the	O
conditional	B
distributions	O
p	O
j	O
x	O
would	O
be	O
identical	O
functions	B
of	O
y	O
regardless	O
of	O
x	O
solution	O
to	O
exercise	O
we	O
the	O
fraction	O
fb	O
bk	O
the	O
number	O
of	O
black	B
balls	O
has	O
a	O
binomial	B
distribution	B
p	O
j	O
fb	O
n	O
n	O
nb	O
b	O
the	O
mean	B
and	O
variance	B
of	O
this	O
distribution	B
are	O
enb	O
n	O
fb	O
varnb	O
n	O
fb	O
these	O
results	O
were	O
derived	O
in	O
example	O
the	O
standard	B
deviation	I
of	O
nb	O
is	O
pvarnb	O
fb	O
when	O
bk	O
and	O
n	O
the	O
expectation	B
and	O
variance	B
of	O
nb	O
are	O
and	O
the	O
standard	B
deviation	I
is	O
when	O
bk	O
and	O
n	O
the	O
expectation	B
and	O
variance	B
of	O
nb	O
are	O
and	O
the	O
standard	B
deviation	I
is	O
solution	O
to	O
exercise	O
the	O
numerator	O
of	O
the	O
quantity	O
z	O
fbn	O
n	O
fb	O
can	O
be	O
recognized	O
as	O
the	O
denominator	O
is	O
equal	O
to	O
the	O
variance	B
of	O
nb	O
which	O
is	O
by	O
the	O
expectation	B
of	O
the	O
numerator	O
so	O
the	O
expectation	B
of	O
z	O
is	O
random	B
variable	I
like	O
z	O
which	O
measures	O
the	O
deviation	O
of	O
data	O
from	O
the	O
expected	O
value	O
is	O
sometimes	O
called	O
in	O
the	O
case	O
n	O
and	O
fb	O
n	O
fb	O
is	O
and	O
varnb	O
is	O
the	O
numerator	O
has	O
possible	O
values	O
only	O
one	O
of	O
which	O
is	O
smaller	O
than	O
fbn	O
has	O
probability	B
p	O
so	O
the	O
probability	B
that	O
z	O
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solution	O
to	O
exercise	O
we	O
wish	O
to	O
prove	O
given	O
the	O
property	O
f	O
that	O
if	O
p	O
pi	O
and	O
pi	O
i	O
pif	O
f	O
i	O
pixi	O
we	O
proceed	O
by	O
recursion	O
working	O
from	O
the	O
right-hand	O
side	O
proof	O
does	O
not	O
handle	O
cases	O
where	O
some	O
pi	O
such	O
details	O
are	O
left	O
to	O
the	O
pedantic	O
reader	O
at	O
the	O
line	O
we	O
use	O
the	O
of	O
convexity	B
with	O
i	O
pi	O
at	O
the	O
second	O
line	O
i	O
pi	O
i	O
pixi	O
f	O
f	O
i	O
i	O
i	O
pixi	O
pi	O
pixi	O
i	O
pif	O
i	O
pi	O
f	O
pi	O
pi	O
pi	O
pi	O
pi	O
and	O
so	O
forth	O
solution	O
to	O
exercise	O
f	O
i	O
pixi	O
i	O
pi	O
for	O
the	O
outcomes	O
the	O
probabilities	O
are	O
p	O
f	O
the	O
value	O
of	O
one	O
die	B
has	O
mean	B
and	O
variance	B
so	O
the	O
sum	O
of	O
one	O
hundred	O
has	O
mean	B
and	O
variance	B
and	O
by	O
the	O
central-limit	B
theorem	I
the	O
probability	B
distribution	B
is	O
roughly	O
gaussian	B
to	O
the	O
integers	O
with	O
this	O
mean	B
and	O
variance	B
in	O
order	O
to	O
obtain	O
a	O
sum	O
that	O
has	O
a	O
uniform	O
distribution	B
we	O
have	O
to	O
start	O
from	O
random	B
variables	O
some	O
of	O
which	O
have	O
a	O
spiky	O
distribution	B
with	O
the	O
probability	B
mass	O
concentrated	O
at	O
the	O
extremes	O
the	O
unique	O
solution	O
is	O
to	O
have	O
one	O
ordinary	O
die	B
and	O
one	O
with	O
faces	O
yes	O
a	O
uniform	O
distribution	B
can	O
be	O
created	O
in	O
several	O
ways	O
for	O
example	O
by	O
labelling	O
the	O
rth	O
die	B
with	O
the	O
numbers	O
to	O
think	O
about	O
does	O
this	O
uniform	O
distribution	B
contradict	O
the	O
central-limit	B
theorem	I
solution	O
to	O
exercise	O
a	O
ln	O
p	O
q	O
p	O
q	O
ea	O
and	O
q	O
p	O
gives	O
p	O
p	O
p	O
the	O
hyperbolic	O
tangent	O
is	O
ea	O
ea	O
ea	O
tanha	O
ea	O
ea	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
so	O
f	O
probability	B
entropy	B
and	O
inference	B
in	O
the	O
case	O
b	O
pq	O
we	O
can	O
repeat	O
steps	O
replacing	O
e	O
by	O
to	O
obtain	O
p	O
solution	O
to	O
exercise	O
p	O
y	O
p	O
j	O
xp	O
p	O
log	O
p	O
y	O
p	O
y	O
p	O
y	O
p	O
y	O
p	O
j	O
x	O
p	O
j	O
x	O
p	O
p	O
log	O
p	O
j	O
x	O
p	O
j	O
x	O
log	O
p	O
p	O
solution	O
to	O
exercise	O
the	O
conditional	B
independence	B
of	O
and	O
given	O
x	O
means	O
p	O
p	O
j	O
xp	O
j	O
x	O
this	O
gives	O
a	O
separation	B
of	O
the	O
posterior	B
probability	B
ratio	O
into	O
a	O
series	O
of	O
factors	O
one	O
for	O
each	O
data	O
point	O
times	O
the	O
prior	B
probability	B
ratio	O
p	O
p	O
p	O
x	O
p	O
x	O
p	O
j	O
x	O
p	O
j	O
x	O
p	O
p	O
p	O
j	O
x	O
p	O
j	O
x	O
p	O
p	O
life	B
in	O
high-dimensional	O
spaces	O
solution	O
to	O
exercise	O
the	O
volume	B
of	O
a	O
hypersphere	B
of	O
radius	O
r	O
in	O
n	O
dimensions	B
is	O
in	O
fact	O
v	O
n	O
rn	O
but	O
you	O
don	O
t	O
need	O
to	O
know	O
this	O
for	O
this	O
question	O
all	O
that	O
we	O
need	O
is	O
the	O
r-dependence	O
v	O
n	O
rn	O
so	O
the	O
fractional	O
volume	B
in	O
r	O
is	O
rn	O
rn	O
the	O
fractional	O
volumes	O
in	O
the	O
shells	O
for	O
the	O
required	O
cases	O
are	O
n	O
notice	O
that	O
no	O
matter	O
how	O
small	O
is	O
for	O
large	O
enough	O
n	O
essentially	O
all	O
the	O
probability	B
mass	O
is	O
in	O
the	O
surface	O
shell	O
of	O
thickness	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solution	O
to	O
exercise	O
f	O
and	O
f	O
pa	O
pb	O
pc	O
f	O
e	O
for	O
each	O
x	O
f	O
so	O
solution	O
to	O
exercise	O
for	O
general	O
x	O
e	O
e	O
p	O
e	O
jaxj	O
solution	O
to	O
exercise	O
pa	O
pb	O
pc	O
ga	O
gb	O
and	O
gc	O
solution	O
to	O
exercise	O
e	O
pb	O
p	O
pb	O
log	O
pa	O
pc	O
p	O
solution	O
to	O
exercise	O
this	O
type	O
of	O
question	O
can	O
be	O
approached	O
in	O
two	O
ways	O
either	O
by	O
the	O
function	O
to	O
be	O
maximized	O
the	O
maximum	O
and	O
proving	O
it	O
is	O
a	O
global	O
maximum	O
this	O
strategy	O
is	O
somewhat	O
risky	O
since	O
it	O
is	O
possible	O
for	O
the	O
maximum	O
of	O
a	O
function	O
to	O
be	O
at	O
the	O
boundary	O
of	O
the	O
space	O
at	O
a	O
place	O
where	O
the	O
derivative	O
is	O
not	O
zero	O
alternatively	O
a	O
carefully	O
chosen	O
inequality	B
can	O
establish	O
the	O
answer	O
the	O
second	O
method	O
is	O
much	O
neater	O
proof	O
by	O
the	O
recommended	O
method	O
since	O
it	O
is	O
slightly	O
easier	O
to	O
ln	O
than	O
we	O
temporarily	O
hx	O
to	O
be	O
measured	O
using	O
natural	B
logarithms	B
thus	O
scaling	B
it	O
down	O
by	O
a	O
factor	O
of	O
log	O
e	O
hx	O
xi	O
pi	O
ln	O
pi	O
ln	O
pi	O
we	O
maximize	O
subject	O
to	O
the	O
constraint	O
pi	O
pi	O
which	O
can	O
be	O
enforced	O
with	O
a	O
lagrange	B
multiplier	I
at	O
a	O
maximum	O
gp	O
hx	O
xi	O
pi	O
ln	O
pi	O
ln	O
pi	O
ln	O
pi	O
so	O
all	O
the	O
pi	O
are	O
equal	O
that	O
this	O
extremum	O
is	O
indeed	O
a	O
maximum	O
is	O
established	O
by	O
the	O
curvature	O
which	O
is	O
negative	O
pi	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
proof	O
using	O
jensen	O
s	O
inequality	B
method	O
the	O
inequality	B
first	O
a	O
reminder	O
of	O
if	O
f	O
is	O
a	O
convex	O
function	O
and	O
x	O
is	O
a	O
random	B
variable	I
then	O
e	O
f	O
if	O
f	O
is	O
strictly	O
convex	O
and	O
e	O
f	O
then	O
the	O
random	B
variable	I
x	O
is	O
a	O
constant	O
probability	B
the	O
secret	B
of	O
a	O
proof	O
using	O
jensen	O
s	O
inequality	B
is	O
to	O
choose	O
the	O
right	O
func	O
tion	O
and	O
the	O
right	O
random	B
variable	I
we	O
could	O
f	O
log	O
u	O
log	O
u	O
is	O
a	O
convex	O
function	O
and	O
think	O
of	O
hx	O
pi	O
log	O
as	O
the	O
mean	B
of	O
f	O
where	O
u	O
p	O
but	O
this	O
would	O
not	O
get	O
us	O
there	O
it	O
would	O
give	O
us	O
an	O
inequality	B
in	O
the	O
wrong	O
direction	O
if	O
instead	O
we	O
pi	O
then	O
we	O
u	O
hx	O
now	O
we	O
know	O
from	O
exercise	O
that	O
jaxj	O
so	O
hx	O
log	O
jaxj	O
equality	O
holds	O
only	O
if	O
the	O
random	B
variable	I
u	O
is	O
a	O
constant	O
which	O
means	O
p	O
is	O
a	O
constant	O
for	O
all	O
x	O
solution	O
to	O
exercise	O
dklpjjq	O
p	O
log	O
p	O
qx	O
we	O
prove	O
gibbs	B
inequality	B
using	O
jensen	O
s	O
inequality	B
let	O
f	O
log	O
and	O
u	O
qx	O
p	O
then	O
dklpjjq	O
ef	O
qx	O
f	O
xx	O
p	O
p	O
with	O
equality	O
only	O
if	O
u	O
qx	O
p	O
is	O
a	O
constant	O
that	O
is	O
if	O
qx	O
p	O
px	O
in	O
the	O
above	O
proof	O
the	O
expectations	O
were	O
with	O
respect	O
to	O
second	O
solution	O
the	O
probability	B
distribution	B
p	O
a	O
second	O
solution	O
method	O
uses	O
jensen	O
s	O
inequality	B
with	O
qx	O
instead	O
we	O
f	O
u	O
log	O
u	O
and	O
let	O
u	O
p	O
qx	O
then	O
qx	O
dklpjjq	O
xx	O
f	O
xx	O
p	O
qx	O
log	O
p	O
qx	O
qx	O
f	O
p	O
qx	O
p	O
with	O
equality	O
only	O
if	O
u	O
p	O
qx	O
is	O
a	O
constant	O
that	O
is	O
if	O
qx	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solution	O
to	O
exercise	O
hx	O
f	O
f	O
solution	O
to	O
exercise	O
the	O
probability	B
that	O
there	O
are	O
tails	O
and	O
then	O
one	O
head	O
we	O
get	O
the	O
head	O
on	O
the	O
xth	O
toss	O
is	O
p	O
f	O
if	O
the	O
toss	O
is	O
a	O
tail	B
the	O
probability	B
distribution	B
for	O
the	O
future	O
looks	O
just	O
like	O
it	O
did	O
before	O
we	O
made	O
the	O
toss	O
thus	O
we	O
have	O
a	O
recursive	O
expression	O
for	O
the	O
entropy	B
rearranging	O
hx	O
f	O
hx	O
solution	O
to	O
exercise	O
the	O
probability	B
of	O
the	O
number	O
of	O
tails	O
t	O
is	O
p	O
for	O
t	O
the	O
expected	O
number	O
of	O
heads	O
is	O
by	O
of	O
the	O
problem	O
the	O
expected	O
number	O
of	O
tails	O
is	O
which	O
may	O
be	O
shown	O
to	O
be	O
in	O
a	O
variety	O
of	O
ways	O
for	O
example	O
since	O
the	O
situation	O
after	O
one	O
tail	B
is	O
thrown	O
is	O
equivalent	O
to	O
the	O
opening	O
situation	O
we	O
can	O
write	O
down	O
the	O
recurrence	O
relation	O
et	O
et	O
et	O
the	O
probability	B
distribution	B
of	O
the	O
estimator	B
t	O
given	O
that	O
f	O
is	O
plotted	O
in	O
the	O
probability	B
of	O
is	O
simply	O
the	O
probability	B
of	O
the	O
corresponding	O
value	O
of	O
t	O
figure	O
the	O
probability	B
distribution	B
of	O
the	O
estimator	B
t	O
given	O
that	O
f	O
solution	O
to	O
exercise	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
one	O
six	B
to	O
the	O
next	O
six	B
is	O
six	B
we	O
start	O
counting	B
rolls	O
after	O
the	O
of	O
the	O
two	O
sixes	O
the	O
probability	B
that	O
the	O
next	O
six	B
occurs	O
on	O
the	O
rth	O
roll	O
is	O
the	O
probability	B
of	O
not	O
getting	O
a	O
six	B
for	O
r	O
rolls	O
multiplied	O
by	O
the	O
probability	B
of	O
then	O
getting	O
a	O
six	B
p	O
r	O
for	O
r	O
this	O
probability	B
distribution	B
of	O
the	O
number	O
of	O
rolls	O
r	O
may	O
be	O
called	O
an	O
exponential	B
distribution	B
since	O
p	O
r	O
where	O
and	O
z	O
is	O
a	O
normalizing	O
constant	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
the	O
clock	O
until	O
the	O
next	O
six	B
is	O
six	B
the	O
mean	B
number	O
of	O
rolls	O
going	O
back	O
in	O
time	O
until	O
the	O
most	O
recent	O
six	B
is	O
six	B
et	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
probability	B
entropy	B
and	O
inference	B
the	O
mean	B
number	O
of	O
rolls	O
from	O
the	O
six	B
before	O
the	O
clock	O
struck	O
to	O
the	O
six	B
after	O
the	O
clock	O
struck	O
is	O
the	O
sum	O
of	O
the	O
answers	O
to	O
and	O
less	O
one	O
that	O
is	O
eleven	O
rather	O
than	O
explaining	O
the	O
between	O
and	O
let	O
me	O
give	O
another	O
hint	O
imagine	O
that	O
the	O
buses	O
in	O
poissonville	B
arrive	O
independently	O
at	O
random	B
poisson	B
process	I
with	O
on	O
average	O
one	O
bus	O
every	O
six	B
minutes	O
imagine	O
that	O
passengers	O
turn	O
up	O
at	O
bus-stops	O
at	O
a	O
uniform	O
rate	B
and	O
are	O
scooped	O
up	O
by	O
the	O
bus	O
without	O
delay	O
so	O
the	O
interval	O
between	O
two	O
buses	O
remains	O
constant	O
buses	O
that	O
follow	O
gaps	O
bigger	O
than	O
six	B
minutes	O
become	O
overcrowded	O
the	O
passengers	O
representative	O
complains	O
that	O
two-thirds	O
of	O
all	O
passengers	O
found	O
themselves	O
on	O
overcrowded	O
buses	O
the	O
bus	O
operator	O
claims	O
no	O
no	O
only	O
one	O
third	O
of	O
our	O
buses	O
are	O
overcrowded	O
can	O
both	O
these	O
claims	O
be	O
true	O
solution	O
to	O
exercise	O
binomial	B
distribution	B
method	O
from	O
the	O
solution	O
to	O
exercise	O
pb	O
f	O
f	O
sum	B
rule	I
method	O
the	O
marginal	B
probabilities	O
of	O
the	O
eight	O
values	O
of	O
r	O
are	O
illustrated	O
by	O
p	O
f	O
p	O
f	O
f	O
f	O
the	O
posterior	O
probabilities	O
are	O
represented	O
by	O
p	O
r	O
f	O
f	O
f	O
figure	O
the	O
probability	B
distribution	B
of	O
the	O
number	O
of	O
rolls	O
from	O
one	O
to	O
the	O
next	O
solid	O
line	O
p	O
r	O
and	O
the	O
probability	B
distribution	B
line	O
of	O
the	O
number	O
of	O
rolls	O
from	O
the	O
before	O
to	O
the	O
next	O
rtot	O
p	O
r	O
r	O
the	O
probability	B
p	O
is	O
about	O
the	O
probability	B
p	O
is	O
about	O
the	O
mean	B
of	O
is	O
and	O
the	O
mean	B
of	O
rtot	O
is	O
and	O
p	O
r	O
f	O
the	O
probabilities	O
of	O
error	O
in	O
these	O
representative	O
cases	O
are	O
thus	O
f	O
f	O
f	O
f	O
f	O
p	O
r	O
f	O
f	O
f	O
and	O
notice	O
that	O
while	O
the	O
average	O
probability	B
of	I
error	I
of	O
is	O
about	O
the	O
probability	B
r	O
that	O
any	O
particular	O
bit	B
is	O
wrong	O
is	O
either	O
about	O
f	O
or	O
f	O
p	O
r	O
f	O
the	O
average	O
error	B
probability	B
using	O
the	O
sum	B
rule	I
is	O
p	O
xr	O
p	O
j	O
r	O
f	O
so	O
f	O
f	O
f	O
f	O
p	O
f	O
f	O
solution	O
to	O
exercise	O
the	O
entropy	B
is	O
bits	O
per	O
word	O
the	O
two	O
terms	O
are	O
for	O
the	O
cases	O
r	O
and	O
the	O
remaining	O
are	O
for	O
the	O
other	O
outcomes	O
which	O
share	O
the	O
same	O
probability	B
of	O
occurring	O
and	O
identical	O
error	B
probability	B
f	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
if	O
you	O
are	O
eager	O
to	O
get	O
on	O
to	O
information	B
theory	I
data	O
compression	B
and	O
noisy	B
channels	O
you	O
can	O
skip	O
to	O
chapter	O
data	O
compression	B
and	O
data	O
modelling	B
are	O
intimately	O
connected	O
however	O
so	O
you	O
ll	O
probably	O
want	O
to	O
come	O
back	O
to	O
this	O
chapter	O
by	O
the	O
time	O
you	O
get	O
to	O
chapter	O
before	O
reading	O
chapter	O
it	O
might	O
be	O
good	B
to	O
look	O
at	O
the	O
following	O
exercises	O
exercise	O
a	O
die	B
is	O
selected	O
at	O
random	B
from	O
two	O
twenty-faced	O
dice	O
on	O
which	O
the	O
symbols	O
are	O
written	O
with	O
nonuniform	O
frequency	B
as	O
follows	O
symbol	O
number	O
of	O
faces	O
of	O
die	B
a	O
number	O
of	O
faces	O
of	O
die	B
b	O
the	O
randomly	O
chosen	O
die	B
is	O
rolled	O
times	O
with	O
the	O
following	O
outcomes	O
what	O
is	O
the	O
probability	B
that	O
the	O
die	B
is	O
die	B
a	O
exercise	O
assume	O
that	O
there	O
is	O
a	O
third	O
twenty-faced	O
die	B
die	B
c	O
on	O
which	O
the	O
symbols	O
are	O
written	O
once	O
each	O
as	O
above	O
one	O
of	O
the	O
three	O
dice	O
is	O
selected	O
at	O
random	B
and	O
rolled	O
times	O
giving	O
the	O
outcomes	O
what	O
is	O
the	O
probability	B
that	O
the	O
die	B
is	O
die	B
a	O
die	B
b	O
die	B
c	O
exercise	O
inferring	O
a	O
decay	O
constant	O
unstable	O
particles	O
are	O
emitted	O
from	O
a	O
source	O
and	O
decay	O
at	O
a	O
distance	B
x	O
a	O
real	O
number	O
that	O
has	O
an	O
exponential	B
probability	B
distribution	B
with	O
characteristic	O
length	O
decay	O
events	O
can	O
be	O
observed	O
only	O
if	O
they	O
occur	O
in	O
a	O
window	B
extending	O
from	O
x	O
cm	O
to	O
x	O
cm	O
n	O
decays	O
are	O
observed	O
at	O
locations	O
xng	O
what	O
is	O
x	O
exercise	O
forensic	B
evidence	B
two	O
people	O
have	O
left	O
traces	O
of	O
their	O
own	O
blood	O
at	O
the	O
scene	O
of	O
a	O
crime	O
a	O
suspect	O
oliver	B
is	O
tested	O
and	O
found	O
to	O
have	O
type	O
o	O
blood	O
the	O
blood	O
groups	O
of	O
the	O
two	O
traces	O
are	O
found	O
to	O
be	O
of	O
type	O
o	O
common	O
type	O
in	O
the	O
local	O
population	O
having	O
frequency	B
and	O
of	O
type	O
ab	O
rare	O
type	O
with	O
frequency	B
do	O
these	O
data	O
o	O
and	O
ab	O
blood	O
were	O
found	O
at	O
scene	O
give	O
evidence	B
in	O
favour	O
of	O
the	O
proposition	O
that	O
oliver	B
was	O
one	O
of	O
the	O
two	O
people	O
present	O
at	O
the	O
crime	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
it	O
is	O
not	O
a	O
controversial	O
statement	O
that	O
bayes	B
theorem	O
provides	O
the	O
correct	O
language	O
for	O
describing	O
the	O
inference	B
of	O
a	O
message	O
communicated	O
over	O
a	O
noisy	B
channel	O
as	O
we	O
used	O
it	O
in	O
chapter	O
but	O
strangely	O
when	O
it	O
comes	O
to	O
other	O
inference	B
problems	O
the	O
use	O
of	O
bayes	B
theorem	O
is	O
not	O
so	O
widespread	O
a	O
inference	B
problem	O
when	O
i	O
was	O
an	O
undergraduate	O
in	O
cambridge	O
i	O
was	O
privileged	O
to	O
receive	O
supervisions	O
from	O
steve	O
gull	B
sitting	O
at	O
his	O
desk	O
in	O
a	O
dishevelled	O
in	O
st	O
john	O
s	O
college	O
i	O
asked	O
him	O
how	O
one	O
ought	O
to	O
answer	O
an	O
old	O
tripos	O
question	O
unstable	O
particles	O
are	O
emitted	O
from	O
a	O
source	O
and	O
decay	O
at	O
a	O
distance	B
x	O
a	O
real	O
number	O
that	O
has	O
an	O
exponential	B
probability	B
distribution	B
with	O
characteristic	O
length	O
decay	O
events	O
can	O
be	O
observed	O
only	O
if	O
they	O
occur	O
in	O
a	O
window	B
extending	O
from	O
x	O
cm	O
to	O
x	O
cm	O
n	O
decays	O
are	O
observed	O
at	O
locations	O
xng	O
what	O
is	O
x	O
i	O
had	O
scratched	O
my	O
head	O
over	O
this	O
for	O
some	O
time	O
my	O
education	O
had	O
provided	O
me	O
with	O
a	O
couple	O
of	O
approaches	O
to	O
solving	O
such	O
inference	B
problems	O
constructing	O
estimators	O
of	O
the	O
unknown	O
parameters	B
or	O
the	O
model	B
to	O
the	O
data	O
or	O
to	O
a	O
processed	O
version	O
of	O
the	O
data	O
since	O
the	O
mean	B
of	O
an	O
unconstrained	O
exponential	B
distribution	B
is	O
it	O
seemed	O
reasonable	O
to	O
examine	O
the	O
sample	B
mean	B
xnn	O
and	O
see	O
if	O
an	O
estimator	B
could	O
be	O
obtained	O
from	O
it	O
it	O
was	O
evident	O
that	O
the	O
estimator	B
would	O
be	O
appropriate	O
for	O
cm	O
but	O
not	O
for	O
cases	O
where	O
the	O
truncation	O
of	O
the	O
distribution	B
at	O
the	O
right-hand	O
side	O
is	O
with	O
a	O
little	O
ingenuity	O
and	O
the	O
introduction	O
of	O
ad	O
hoc	O
bins	O
promising	O
estimators	O
for	O
cm	O
could	O
be	O
constructed	O
but	O
there	O
was	O
no	O
obvious	O
estimator	B
that	O
would	O
work	O
under	O
all	O
conditions	O
to	O
a	O
histogram	O
derived	O
from	O
the	O
data	O
i	O
was	O
stuck	O
nor	O
could	O
i	O
a	O
satisfactory	O
approach	O
based	O
on	O
the	O
density	B
p	O
what	O
is	O
the	O
general	O
solution	O
to	O
this	O
problem	O
and	O
others	O
like	O
it	O
is	O
it	O
always	O
necessary	O
when	O
confronted	O
by	O
a	O
new	O
inference	B
problem	O
to	O
grope	O
in	O
the	O
dark	O
for	O
appropriate	O
estimators	O
and	O
worry	O
about	O
the	O
best	O
estimator	B
that	O
means	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
inference	B
problem	O
figure	O
the	O
probability	B
density	B
p	O
as	O
a	O
function	O
of	O
x	O
x	O
figure	O
the	O
probability	B
density	B
p	O
as	O
a	O
function	O
of	O
for	O
three	O
values	O
of	O
x	O
when	O
plotted	O
this	O
way	O
round	O
the	O
function	O
is	O
known	O
as	O
the	O
likelihood	B
of	O
the	O
marks	O
indicate	O
the	O
three	O
values	O
of	O
that	O
were	O
used	O
in	O
the	O
preceding	O
steve	O
wrote	O
down	O
the	O
probability	B
of	O
one	O
data	O
point	O
given	O
where	O
p	O
dx	O
x	O
otherwise	O
this	O
seemed	O
obvious	O
enough	O
then	O
he	O
wrote	O
bayes	B
theorem	O
p	O
xng	O
p	O
p	O
p	O
suddenly	O
the	O
straightforward	O
distribution	B
p	O
xngj	O
the	O
probability	B
of	O
the	O
data	O
given	O
the	O
hypothesis	O
was	O
being	O
turned	O
on	O
its	O
head	O
so	O
as	O
to	O
the	O
probability	B
of	O
a	O
hypothesis	O
given	O
the	O
data	O
a	O
simple	O
showed	O
the	O
probability	B
of	O
a	O
single	O
data	O
point	O
p	O
as	O
a	O
familiar	O
function	O
of	O
x	O
for	O
values	O
of	O
each	O
curve	O
was	O
an	O
innocent	O
exponential	B
normalized	O
to	O
have	O
area	O
plotting	O
the	O
same	O
function	O
as	O
a	O
function	O
of	O
for	O
a	O
value	O
of	O
x	O
something	O
remarkable	O
happens	O
a	O
peak	O
emerges	O
to	O
help	O
understand	O
these	O
two	O
points	O
of	O
view	O
of	O
the	O
one	O
function	O
shows	O
a	O
surface	O
plot	O
of	O
p	O
as	O
a	O
function	O
of	O
x	O
and	O
the	O
likelihood	B
function	O
p	O
is	O
the	O
product	O
of	O
the	O
n	O
functions	B
of	O
p	O
j	O
for	O
a	O
dataset	O
consisting	O
of	O
several	O
points	O
e	O
g	O
the	O
six	B
points	O
fxgn	O
x	O
figure	O
the	O
probability	B
density	B
p	O
as	O
a	O
function	O
of	O
x	O
and	O
figures	O
and	O
are	O
vertical	O
sections	O
through	O
this	O
surface	O
figure	O
the	O
likelihood	B
function	O
in	O
the	O
case	O
of	O
a	O
six-point	O
dataset	O
p	O
as	O
a	O
function	O
of	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
if	O
you	O
have	O
any	O
understanding	O
this	O
chapter	O
i	O
recommend	O
ensuring	O
you	O
are	O
happy	O
with	O
exercises	O
and	O
then	O
noting	O
their	O
similarity	O
to	O
exercise	O
steve	O
summarized	O
bayes	B
theorem	O
as	O
embodying	O
the	O
fact	O
that	O
what	O
you	O
know	O
about	O
after	O
the	O
data	O
arrive	O
is	O
what	O
you	O
knew	O
before	O
and	O
what	O
the	O
data	O
told	O
you	O
probabilities	O
are	O
used	O
here	O
to	O
quantify	O
degrees	B
of	I
belief	B
to	O
nip	O
possible	O
confusion	O
in	O
the	O
bud	O
it	O
must	O
be	O
emphasized	O
that	O
the	O
hypothesis	O
that	O
correctly	O
describes	O
the	O
situation	O
is	O
not	O
a	O
stochastic	B
variable	O
and	O
the	O
fact	O
that	O
the	O
bayesian	B
uses	O
a	O
probability	B
distribution	B
p	O
does	O
not	O
mean	B
that	O
he	O
thinks	O
of	O
the	O
world	O
as	O
stochastically	O
changing	O
its	O
nature	O
between	O
the	O
states	O
described	O
by	O
the	O
hypotheses	O
he	O
uses	O
the	O
notation	B
of	O
probabilities	O
to	O
represent	O
his	O
beliefs	O
about	O
the	O
mutually	O
exclusive	O
micro-hypotheses	O
values	O
of	O
of	O
which	O
only	O
one	O
is	O
actually	O
true	O
that	O
probabilities	O
can	O
denote	O
degrees	B
of	I
belief	B
given	O
assumptions	B
seemed	O
reasonable	O
to	O
me	O
the	O
posterior	B
probability	B
distribution	B
represents	O
the	O
unique	O
and	O
complete	O
solution	O
to	O
the	O
problem	O
there	O
is	O
no	O
need	O
to	O
invent	O
estimators	O
nor	O
do	O
we	O
need	O
to	O
invent	O
criteria	O
for	O
comparing	O
alternative	O
estimators	O
with	O
each	O
other	O
whereas	O
orthodox	O
statisticians	O
twenty	O
ways	O
of	O
solving	O
a	O
problem	O
and	O
another	O
twenty	O
criteria	O
for	O
deciding	O
which	O
of	O
these	O
solutions	O
is	O
the	O
best	O
bayesian	B
statistics	O
only	O
one	O
answer	O
to	O
a	O
well-posed	O
problem	O
assumptions	B
in	O
inference	B
our	O
inference	B
is	O
conditional	B
on	O
our	O
assumptions	B
example	O
the	O
prior	B
p	O
critics	O
view	O
such	O
priors	O
as	O
a	O
because	O
they	O
are	O
subjective	O
but	O
i	O
don	O
t	O
see	O
how	O
it	O
could	O
be	O
otherwise	O
how	O
can	O
one	O
perform	O
inference	B
without	O
making	O
assumptions	B
i	O
believe	O
that	O
it	O
is	O
of	O
great	O
value	O
that	O
bayesian	B
methods	O
force	O
one	O
to	O
make	O
these	O
tacit	O
assumptions	B
explicit	O
first	O
once	O
assumptions	B
are	O
made	O
the	O
inferences	O
are	O
objective	O
and	O
unique	O
reproducible	O
with	O
complete	O
agreement	O
by	O
anyone	O
who	O
has	O
the	O
same	O
information	B
and	O
makes	O
the	O
same	O
assumptions	B
for	O
example	O
given	O
the	O
assumptions	B
listed	O
above	O
h	O
and	O
the	O
data	O
d	O
everyone	O
will	O
agree	O
about	O
the	O
posterior	B
probability	B
of	O
the	O
decay	O
length	O
p	O
dh	O
p	O
j	O
p	O
jh	O
second	O
when	O
the	O
assumptions	B
are	O
explicit	O
they	O
are	O
easier	O
to	O
criticize	O
and	O
easier	O
to	O
modify	O
indeed	O
we	O
can	O
quantify	O
the	O
sensitivity	O
of	O
our	O
inferences	O
to	O
the	O
details	O
of	O
the	O
assumptions	B
for	O
example	O
we	O
can	O
note	O
from	O
the	O
likelihood	B
curves	O
in	O
that	O
in	O
the	O
case	O
of	O
a	O
single	O
data	O
point	O
at	O
x	O
the	O
likelihood	B
function	O
is	O
less	O
strongly	O
peaked	O
than	O
in	O
the	O
case	O
x	O
the	O
details	O
of	O
the	O
prior	B
p	O
become	O
increasingly	O
important	O
as	O
the	O
sample	B
mean	B
gets	O
closer	O
to	O
the	O
middle	O
of	O
the	O
window	B
in	O
the	O
case	O
x	O
the	O
likelihood	B
function	O
doesn	O
t	O
have	O
a	O
peak	O
at	O
all	O
such	O
data	O
merely	O
rule	O
out	O
small	O
values	O
of	O
and	O
don	O
t	O
give	O
any	O
information	B
about	O
the	O
relative	B
probabilities	O
of	O
large	O
values	O
of	O
so	O
in	O
this	O
case	O
the	O
details	O
of	O
the	O
prior	B
at	O
the	O
end	O
of	O
things	O
are	O
not	O
important	O
but	O
at	O
the	O
end	O
the	O
prior	B
is	O
important	O
third	O
when	O
we	O
are	O
not	O
sure	O
which	O
of	O
various	O
alternative	O
assumptions	B
is	O
the	O
most	O
appropriate	O
for	O
a	O
problem	O
we	O
can	O
treat	O
this	O
question	O
as	O
another	O
inference	B
task	O
thus	O
given	O
data	O
d	O
we	O
can	O
compare	O
alternative	O
assumptions	B
h	O
using	O
bayes	B
theorem	O
p	O
j	O
d	O
i	O
p	O
jh	O
ip	O
j	O
i	O
p	O
j	O
i	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
bent	B
coin	B
where	O
i	O
denotes	O
the	O
highest	O
assumptions	B
which	O
we	O
are	O
not	O
questioning	O
fourth	O
we	O
can	O
take	O
into	O
account	O
our	O
uncertainty	O
regarding	O
such	O
assumptions	B
when	O
we	O
make	O
subsequent	O
predictions	O
rather	O
than	O
choosing	O
one	O
particular	O
assumption	O
and	O
working	O
out	O
our	O
predictions	O
about	O
some	O
quantity	O
t	O
p	O
i	O
we	O
obtain	O
predictions	O
that	O
take	O
into	O
account	O
our	O
uncertainty	O
about	O
h	O
by	O
using	O
the	O
sum	B
rule	I
p	O
d	O
i	O
p	O
dh	O
ip	O
j	O
d	O
i	O
this	O
is	O
another	O
contrast	O
with	O
orthodox	O
statistics	O
in	O
which	O
it	O
is	O
conventional	O
to	O
test	O
a	O
default	O
model	B
and	O
then	O
if	O
the	O
test	O
accepts	O
the	O
model	B
at	O
some	O
level	O
to	O
use	O
exclusively	O
that	O
model	B
to	O
make	O
predictions	O
steve	O
thus	O
persuaded	O
me	O
that	O
probability	B
theory	O
reaches	O
parts	O
that	O
ad	O
hoc	O
methods	O
cannot	O
reach	O
let	O
s	O
look	O
at	O
a	O
few	O
more	O
examples	O
of	O
simple	O
inference	B
problems	O
the	O
bent	B
coin	B
a	O
bent	B
coin	B
is	O
tossed	O
f	O
times	O
we	O
observe	O
a	O
sequence	B
s	O
of	O
heads	O
and	O
tails	O
we	O
ll	O
denote	O
by	O
the	O
symbols	O
a	O
and	O
b	O
we	O
wish	O
to	O
know	O
the	O
bias	B
of	O
the	O
coin	B
and	O
predict	O
the	O
probability	B
that	O
the	O
next	O
toss	O
will	O
result	O
in	O
a	O
head	O
we	O
encountered	O
this	O
task	O
in	O
example	O
and	O
we	O
will	O
encounter	O
it	O
again	O
in	O
chapter	O
when	O
we	O
discuss	O
adaptive	B
data	O
compression	B
it	O
is	O
also	O
the	O
original	O
inference	B
problem	O
studied	O
by	O
thomas	O
bayes	B
in	O
his	O
essay	O
published	O
in	O
as	O
in	O
exercise	O
we	O
will	O
assume	O
a	O
uniform	O
prior	B
distribution	B
and	O
obtain	O
a	O
posterior	O
distribution	B
by	O
multiplying	O
by	O
the	O
likelihood	B
a	O
critic	O
might	O
object	O
where	O
did	O
this	O
prior	B
come	O
from	O
i	O
will	O
not	O
claim	O
that	O
the	O
uniform	O
prior	B
is	O
in	O
any	O
way	O
fundamental	O
indeed	O
we	O
ll	O
give	O
examples	O
of	O
nonuniform	O
priors	O
later	O
the	O
prior	B
is	O
a	O
subjective	O
assumption	O
one	O
of	O
the	O
themes	O
of	O
this	O
book	O
is	O
you	O
can	O
t	O
do	O
inference	B
or	O
data	O
compression	B
without	O
making	O
assumptions	B
we	O
give	O
the	O
name	O
to	O
our	O
assumptions	B
ll	O
be	O
introducing	O
an	O
alternative	O
set	B
of	O
assumptions	B
in	O
a	O
moment	O
the	O
probability	B
given	O
p	O
a	O
that	O
f	O
tosses	O
result	O
in	O
a	O
sequence	B
s	O
that	O
contains	O
ffa	O
fbg	O
counts	O
of	O
the	O
two	O
outcomes	O
is	O
example	O
p	O
aabaj	O
pa	O
f	O
papa	O
our	O
model	B
assumes	O
a	O
uniform	O
prior	B
distribution	B
for	O
pa	O
p	O
pa	O
pfa	O
a	O
pafb	O
p	O
pa	O
and	O
pb	O
pa	O
inferring	O
unknown	O
parameters	B
given	O
a	O
string	O
of	O
length	O
f	O
of	O
which	O
fa	O
are	O
as	O
and	O
fb	O
are	O
bs	O
we	O
are	O
interested	O
in	O
inferring	O
what	O
pa	O
might	O
be	O
predicting	O
whether	O
the	O
next	O
character	O
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
an	O
a	O
or	O
a	O
b	O
are	O
always	O
expressed	O
as	O
probabilities	O
so	O
predicting	O
whether	O
the	O
next	O
character	O
is	O
an	O
a	O
is	O
the	O
same	O
as	O
computing	O
the	O
probability	B
that	O
the	O
next	O
character	O
is	O
an	O
a	O
assuming	O
to	O
be	O
true	O
the	O
posterior	B
probability	B
of	O
pa	O
given	O
a	O
string	O
s	O
of	O
length	O
f	O
that	O
has	O
counts	O
ffa	O
fbg	O
is	O
by	O
bayes	B
theorem	O
p	O
pa	O
p	O
j	O
s	O
p	O
the	O
factor	O
p	O
pa	O
which	O
as	O
a	O
function	O
of	O
pa	O
is	O
known	O
as	O
the	O
likelihood	B
function	O
was	O
given	O
in	O
equation	O
the	O
prior	B
p	O
a	O
was	O
given	O
in	O
equation	O
our	O
inference	B
of	O
pa	O
is	O
thus	O
pfa	O
a	O
pafb	O
p	O
the	O
normalizing	O
constant	O
is	O
given	O
by	O
the	O
beta	B
integral	B
p	O
j	O
s	O
dpa	O
pfa	O
p	O
exercise	O
sketch	O
the	O
posterior	B
probability	B
p	O
j	O
s	O
aba	O
f	O
a	O
pafb	O
what	O
is	O
the	O
most	O
probable	O
value	O
of	O
pa	O
the	O
value	O
that	O
maximizes	O
the	O
posterior	B
probability	B
density	B
what	O
is	O
the	O
mean	B
value	O
of	O
p	O
a	O
under	O
this	O
distribution	B
fb	O
fb	O
fa	O
fb	O
the	O
answer	O
same	O
p	O
j	O
s	O
bbb	O
f	O
from	O
inferences	O
to	O
predictions	O
questions	O
for	O
the	O
posterior	B
probability	B
our	O
prediction	B
about	O
the	O
next	O
toss	O
the	O
probability	B
that	O
the	O
next	O
toss	O
is	O
an	O
a	O
is	O
obtained	O
by	O
integrating	O
over	O
pa	O
this	O
has	O
the	O
of	O
taking	O
into	O
account	O
our	O
uncertainty	O
about	O
pa	O
when	O
making	O
predictions	O
by	O
the	O
sum	B
rule	I
p	O
s	O
f	O
z	O
dpa	O
p	O
pap	O
j	O
s	O
f	O
the	O
probability	B
of	O
an	O
a	O
given	O
pa	O
is	O
simply	O
pa	O
so	O
a	O
pafb	O
pfa	O
p	O
f	O
p	O
s	O
f	O
dpa	O
pa	O
z	O
dpa	O
pafb	O
p	O
f	O
fb	O
fb	O
a	O
which	O
is	O
known	O
as	O
laplace	B
s	O
rule	O
fa	O
fb	O
fb	O
fa	O
fa	O
fb	O
the	O
bent	B
coin	B
and	O
model	B
comparison	I
imagine	O
that	O
a	O
scientist	O
introduces	O
another	O
theory	O
for	O
our	O
data	O
he	O
asserts	O
that	O
the	O
source	O
is	O
not	O
really	O
a	O
bent	B
coin	B
but	O
is	O
really	O
a	O
perfectly	O
formed	O
die	B
with	O
one	O
face	O
painted	O
heads	O
a	O
and	O
the	O
other	O
painted	O
tails	O
b	O
thus	O
the	O
parameter	O
pa	O
which	O
in	O
the	O
original	O
model	B
could	O
take	O
any	O
value	O
between	O
and	O
is	O
according	O
to	O
the	O
new	O
hypothesis	O
not	O
a	O
free	O
parameter	O
at	O
all	O
rather	O
it	O
is	O
equal	O
to	O
hypothesis	O
is	O
termed	O
so	O
that	O
the	O
of	O
each	O
model	B
indicates	O
its	O
number	O
of	O
free	O
parameters	B
how	O
can	O
we	O
compare	O
these	O
two	O
models	O
in	O
the	O
light	O
of	O
data	O
we	O
wish	O
to	O
infer	O
how	O
probable	O
is	O
relative	B
to	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
bent	B
coin	B
and	O
model	B
comparison	I
model	B
comparison	I
as	B
inference	B
in	O
order	O
to	O
perform	O
model	B
comparison	I
we	O
write	O
down	O
bayes	B
theorem	O
again	O
but	O
this	O
time	O
with	O
a	O
argument	O
on	O
the	O
left-hand	O
side	O
we	O
wish	O
to	O
know	O
how	O
probable	O
is	O
given	O
the	O
data	O
by	O
bayes	B
theorem	O
p	O
j	O
s	O
f	O
p	O
p	O
f	O
similarly	O
the	O
posterior	B
probability	B
of	O
is	O
p	O
j	O
s	O
f	O
p	O
p	O
f	O
the	O
normalizing	O
constant	O
in	O
both	O
cases	O
is	O
p	O
f	O
which	O
is	O
the	O
total	O
probability	B
of	O
getting	O
the	O
observed	O
data	O
if	O
and	O
are	O
the	O
only	O
models	O
under	O
consideration	O
this	O
probability	B
is	O
given	O
by	O
the	O
sum	B
rule	I
p	O
f	O
p	O
p	O
to	O
evaluate	O
the	O
posterior	O
probabilities	O
of	O
the	O
hypotheses	O
we	O
need	O
to	O
assign	O
values	O
to	O
the	O
prior	B
probabilities	O
p	O
and	O
p	O
in	O
this	O
case	O
we	O
might	O
set	B
these	O
to	O
each	O
and	O
we	O
need	O
to	O
evaluate	O
the	O
data-dependent	O
terms	O
p	O
and	O
p	O
we	O
can	O
give	O
names	O
to	O
these	O
quantities	O
the	O
quantity	O
p	O
is	O
a	O
measure	O
of	O
how	O
much	O
the	O
data	O
favour	O
and	O
we	O
call	O
it	O
the	O
evidence	B
for	O
model	B
we	O
already	O
encountered	O
this	O
quantity	O
in	O
equation	O
where	O
it	O
appeared	O
as	O
the	O
normalizing	O
constant	O
of	O
the	O
inference	B
we	O
made	O
the	O
inference	B
of	O
pa	O
given	O
the	O
data	O
how	O
model	B
comparison	I
works	O
the	O
evidence	B
for	O
a	O
model	B
is	O
usually	O
the	O
normalizing	O
constant	O
of	O
an	O
earlier	O
bayesian	B
inference	B
we	O
evaluated	O
the	O
normalizing	O
constant	O
for	O
model	B
in	O
the	O
evidence	B
for	O
model	B
is	O
very	O
simple	O
because	O
this	O
model	B
has	O
no	O
parameters	B
to	O
infer	O
to	O
be	O
we	O
have	O
p	O
pfa	O
thus	O
the	O
posterior	B
probability	B
ratio	O
of	O
model	B
to	O
model	B
is	O
p	O
j	O
s	O
f	O
p	O
j	O
s	O
f	O
p	O
p	O
fb	O
pfa	O
fa	O
fb	O
some	O
values	O
of	O
this	O
posterior	B
probability	B
ratio	O
are	O
illustrated	O
in	O
table	O
the	O
lines	O
illustrate	O
that	O
some	O
outcomes	O
favour	O
one	O
model	B
and	O
some	O
favour	O
the	O
other	O
no	O
outcome	O
is	O
completely	O
incompatible	O
with	O
either	O
model	B
with	O
small	O
amounts	O
of	O
data	O
tosses	O
say	O
it	O
is	O
typically	O
not	O
the	O
case	O
that	O
one	O
of	O
the	O
two	O
models	O
is	O
overwhelmingly	O
more	O
probable	O
than	O
the	O
other	O
but	O
with	O
more	O
data	O
the	O
evidence	B
against	O
given	O
by	O
any	O
data	B
set	B
with	O
the	O
ratio	O
fa	O
fb	O
from	O
mounts	O
up	O
you	O
can	O
t	O
predict	O
in	O
advance	O
how	O
much	O
data	O
are	O
needed	O
to	O
be	O
pretty	O
sure	O
which	O
theory	O
is	O
true	O
it	O
depends	O
what	O
p	O
a	O
is	O
the	O
simpler	O
model	B
since	O
it	O
has	O
no	O
adjustable	O
parameters	B
is	O
able	O
to	O
lose	O
out	O
by	O
the	O
biggest	O
margin	O
the	O
odds	B
may	O
be	O
hundreds	O
to	O
one	O
against	O
it	O
the	O
more	O
complex	B
model	B
can	O
never	O
lose	O
out	O
by	O
a	O
large	O
margin	O
there	O
s	O
no	O
data	B
set	B
that	O
is	O
actually	O
unlikely	O
given	O
model	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
table	O
outcome	O
of	O
model	B
comparison	I
between	O
models	O
and	O
for	O
the	O
bent	B
coin	B
model	B
states	O
that	O
pa	O
pb	O
figure	O
typical	B
behaviour	I
of	I
the	O
evidence	B
in	O
favour	O
of	O
as	O
bent	B
coin	B
tosses	O
accumulate	O
under	O
three	O
conditions	O
horizontal	O
axis	O
is	O
the	O
number	O
of	O
tosses	O
f	O
the	O
vertical	O
axis	O
on	O
the	O
left	O
is	O
ln	O
p	O
j	O
f	O
p	O
j	O
f	O
vertical	O
axis	O
shows	O
the	O
values	O
of	O
p	O
j	O
f	O
p	O
j	O
f	O
the	O
three	O
rows	O
show	O
independent	O
simulated	O
experiments	O
also	O
the	O
right-hand	O
f	O
data	O
fb	O
p	O
j	O
s	O
f	O
p	O
j	O
s	O
f	O
is	O
true	O
pa	O
is	O
true	O
pa	O
pa	O
exercise	O
show	O
that	O
after	O
f	O
tosses	O
have	O
taken	O
place	O
the	O
biggest	O
value	O
that	O
the	O
log	O
evidence	B
ratio	O
log	O
p	O
p	O
can	O
have	O
scales	O
linearly	O
with	O
f	O
if	O
is	O
more	O
probable	O
but	O
the	O
log	O
evidence	B
in	O
favour	O
of	O
can	O
grow	O
at	O
most	O
as	O
log	O
f	O
exercise	O
putting	O
your	O
sampling	B
theory	I
hat	B
on	O
assuming	O
fa	O
has	O
not	O
yet	O
been	O
measured	O
compute	O
a	O
plausible	O
range	O
that	O
the	O
log	O
evidence	B
ratio	O
might	O
lie	O
in	O
as	O
a	O
function	O
of	O
f	O
and	O
the	O
true	O
value	O
of	O
p	O
a	O
and	O
sketch	O
it	O
as	O
a	O
function	O
of	O
f	O
for	O
pa	O
pa	O
and	O
pa	O
sketch	O
the	O
log	O
evidence	B
as	O
a	O
function	O
of	O
the	O
random	B
variable	I
f	O
a	O
and	O
work	O
out	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
fa	O
typical	B
behaviour	I
of	I
the	O
evidence	B
figure	O
shows	O
the	O
log	O
evidence	B
ratio	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
tosses	O
f	O
in	O
a	O
number	O
of	O
simulated	O
experiments	O
in	O
the	O
left-hand	O
experiments	O
was	O
true	O
in	O
the	O
right-hand	O
ones	O
was	O
true	O
and	O
the	O
value	O
of	O
pa	O
was	O
either	O
or	O
we	O
will	O
discuss	O
model	B
comparison	I
more	O
in	O
a	O
later	O
chapter	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
an	O
example	O
of	O
legal	O
evidence	B
an	O
example	O
of	O
legal	O
evidence	B
the	O
following	O
example	O
illustrates	O
that	O
there	O
is	O
more	O
to	O
bayesian	B
inference	B
than	O
the	O
priors	O
two	O
people	O
have	O
left	O
traces	O
of	O
their	O
own	O
blood	O
at	O
the	O
scene	O
of	O
a	O
crime	O
a	O
suspect	O
oliver	B
is	O
tested	O
and	O
found	O
to	O
have	O
type	O
o	O
blood	O
the	O
blood	O
groups	O
of	O
the	O
two	O
traces	O
are	O
found	O
to	O
be	O
of	O
type	O
o	O
common	O
type	O
in	O
the	O
local	O
population	O
having	O
frequency	B
and	O
of	O
type	O
ab	O
rare	O
type	O
with	O
frequency	B
do	O
these	O
data	O
o	O
and	O
ab	O
blood	O
were	O
found	O
at	O
scene	O
give	O
evidence	B
in	O
favour	O
of	O
the	O
proposition	O
that	O
oliver	B
was	O
one	O
of	O
the	O
two	O
people	O
present	O
at	O
the	O
crime	O
a	O
careless	O
lawyer	B
might	O
claim	O
that	O
the	O
fact	O
that	O
the	O
suspect	O
s	O
blood	O
type	O
was	O
found	O
at	O
the	O
scene	O
is	O
positive	O
evidence	B
for	O
the	O
theory	O
that	O
he	O
was	O
present	O
but	O
this	O
is	O
not	O
so	O
denote	O
the	O
proposition	O
the	O
suspect	O
and	O
one	O
unknown	O
person	O
were	O
present	O
by	O
s	O
the	O
alternative	O
states	O
two	O
unknown	O
people	O
from	O
the	O
population	O
were	O
present	O
the	O
prior	B
in	O
this	O
problem	O
is	O
the	O
prior	B
probability	B
ratio	O
between	O
the	O
propositions	O
s	O
and	O
this	O
quantity	O
is	O
important	O
to	O
the	O
verdict	O
and	O
would	O
be	O
based	O
on	O
all	O
other	O
available	O
information	B
in	O
the	O
case	O
our	O
task	O
here	O
is	O
just	O
to	O
evaluate	O
the	O
contribution	O
made	O
by	O
the	O
data	O
d	O
that	O
is	O
the	O
likelihood	B
ratio	O
p	O
j	O
shp	O
j	O
in	O
my	O
view	O
a	O
jury	B
s	O
task	O
should	O
generally	O
be	O
to	O
multiply	O
together	O
carefully	O
evaluated	O
likelihood	B
ratios	O
from	O
each	O
independent	O
piece	O
of	O
admissible	O
evidence	B
with	O
an	O
equally	O
carefully	O
reasoned	O
prior	B
probability	B
view	O
is	O
shared	O
by	O
many	O
statisticians	O
but	O
learned	O
british	B
appeal	O
judges	O
recently	O
disagreed	O
and	O
actually	O
overturned	O
the	O
verdict	O
of	O
a	O
trial	O
because	O
the	O
jurors	O
had	O
been	O
taught	O
to	O
use	O
bayes	B
theorem	O
to	O
handle	O
complicated	O
dna	B
evidence	B
the	O
probability	B
of	O
the	O
data	O
given	O
s	O
is	O
the	O
probability	B
that	O
one	O
unknown	O
person	O
drawn	O
from	O
the	O
population	O
has	O
blood	O
type	O
ab	O
p	O
j	O
sh	O
pab	O
given	O
s	O
we	O
already	O
know	O
that	O
one	O
trace	O
will	O
be	O
of	O
type	O
o	O
the	O
probability	B
of	O
the	O
data	O
given	O
is	O
the	O
probability	B
that	O
two	O
unknown	O
people	O
drawn	O
from	O
the	O
population	O
have	O
types	O
o	O
and	O
ab	O
p	O
j	O
po	O
pab	O
in	O
these	O
equations	O
h	O
denotes	O
the	O
assumptions	B
that	O
two	O
people	O
were	O
present	O
and	O
left	O
blood	O
there	O
and	O
that	O
the	O
probability	B
distribution	B
of	O
the	O
blood	O
groups	O
of	O
unknown	O
people	O
in	O
an	O
explanation	O
is	O
the	O
same	O
as	O
the	O
population	O
frequencies	O
dividing	O
we	O
obtain	O
the	O
likelihood	B
ratio	O
p	O
j	O
sh	O
p	O
j	O
thus	O
the	O
data	O
in	O
fact	O
provide	O
weak	O
evidence	B
against	O
the	O
supposition	O
that	O
oliver	B
was	O
present	O
this	O
result	O
may	O
be	O
found	O
surprising	O
so	O
let	O
us	O
examine	O
it	O
from	O
various	O
points	O
of	O
view	O
first	O
consider	O
the	O
case	O
of	O
another	O
suspect	O
alberto	B
who	O
has	O
type	O
ab	O
intuitively	O
the	O
data	O
do	O
provide	O
evidence	B
in	O
favour	O
of	O
the	O
theory	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
that	O
this	O
suspect	O
was	O
present	O
relative	B
to	O
the	O
null	O
hypothesis	O
and	O
indeed	O
the	O
likelihood	B
ratio	O
in	O
this	O
case	O
is	O
p	O
j	O
p	O
j	O
pab	O
now	O
let	O
us	O
change	O
the	O
situation	O
slightly	O
imagine	O
that	O
of	O
people	O
are	O
of	O
blood	O
type	O
o	O
and	O
the	O
rest	O
are	O
of	O
type	O
ab	O
only	O
these	O
two	O
blood	O
types	O
exist	O
in	O
the	O
population	O
the	O
data	O
at	O
the	O
scene	O
are	O
the	O
same	O
as	O
before	O
consider	O
again	O
how	O
these	O
data	O
our	O
beliefs	O
about	O
oliver	B
a	O
suspect	O
of	O
type	O
o	O
and	O
alberto	B
a	O
suspect	O
of	O
type	O
ab	O
intuitively	O
we	O
still	O
believe	O
that	O
the	O
presence	O
of	O
the	O
rare	O
ab	O
blood	O
provides	O
positive	O
evidence	B
that	O
alberto	B
was	O
there	O
but	O
does	O
the	O
fact	O
that	O
type	O
o	O
blood	O
was	O
detected	O
at	O
the	O
scene	O
favour	O
the	O
hypothesis	O
that	O
oliver	B
was	O
present	O
if	O
this	O
were	O
the	O
case	O
that	O
would	O
mean	B
that	O
regardless	O
of	O
who	O
the	O
suspect	O
is	O
the	O
data	O
make	O
it	O
more	O
probable	O
they	O
were	O
present	O
everyone	O
in	O
the	O
population	O
would	O
be	O
under	O
greater	O
suspicion	O
which	O
would	O
be	O
absurd	O
the	O
data	O
may	O
be	O
compatible	O
with	O
any	O
suspect	O
of	O
either	O
blood	O
type	O
being	O
present	O
but	O
if	O
they	O
provide	O
evidence	B
for	O
some	O
theories	O
they	O
must	O
also	O
provide	O
evidence	B
against	O
other	O
theories	O
here	O
is	O
another	O
way	O
of	O
thinking	O
about	O
this	O
imagine	O
that	O
instead	O
of	O
two	O
people	O
s	O
blood	O
stains	O
there	O
are	O
ten	O
and	O
that	O
in	O
the	O
entire	O
local	O
population	O
of	O
one	O
hundred	O
there	O
are	O
ninety	O
type	O
o	O
suspects	O
and	O
ten	O
type	O
ab	O
suspects	O
consider	O
a	O
particular	O
type	O
o	O
suspect	O
oliver	B
without	O
any	O
other	O
information	B
and	O
before	O
the	O
blood	O
test	O
results	O
come	O
in	O
there	O
is	O
a	O
one	O
in	O
chance	O
that	O
he	O
was	O
at	O
the	O
scene	O
since	O
we	O
know	O
that	O
out	O
of	O
the	O
suspects	O
were	O
present	O
we	O
now	O
get	O
the	O
results	O
of	O
blood	O
tests	O
and	O
that	O
nine	O
of	O
the	O
ten	O
stains	O
are	O
of	O
type	O
ab	O
and	O
one	O
of	O
the	O
stains	O
is	O
of	O
type	O
o	O
does	O
this	O
make	O
it	O
more	O
likely	O
that	O
oliver	B
was	O
there	O
no	O
there	O
is	O
now	O
only	O
a	O
one	O
in	O
ninety	O
chance	O
that	O
he	O
was	O
there	O
since	O
we	O
know	O
that	O
only	O
one	O
person	O
present	O
was	O
of	O
type	O
o	O
maybe	O
the	O
intuition	O
is	O
aided	O
by	O
writing	B
down	O
the	O
formulae	O
for	O
the	O
general	O
case	O
where	O
no	O
blood	O
stains	O
of	O
individuals	O
of	O
type	O
o	O
are	O
found	O
and	O
nab	O
of	O
type	O
ab	O
a	O
total	O
of	O
n	O
individuals	O
in	O
all	O
and	O
unknown	O
people	O
come	O
from	O
a	O
large	O
population	O
with	O
fractions	O
po	O
pab	O
may	O
be	O
other	O
blood	O
types	O
too	O
the	O
task	O
is	O
to	O
evaluate	O
the	O
likelihood	B
ratio	O
for	O
the	O
two	O
hypotheses	O
s	O
the	O
type	O
o	O
suspect	O
and	O
unknown	O
others	O
left	O
n	O
stains	O
and	O
n	O
unknowns	O
left	O
n	O
stains	O
the	O
probability	B
of	O
the	O
data	O
under	O
hypothesis	O
is	O
just	O
the	O
probability	B
of	O
getting	O
no	O
nab	O
individuals	O
of	O
the	O
two	O
types	O
when	O
n	O
individuals	O
are	O
drawn	O
at	O
random	B
from	O
the	O
population	O
p	O
nab	O
j	O
n	O
no	O
nab	O
o	O
pnab	O
pno	O
ab	O
in	O
the	O
case	O
of	O
hypothesis	O
s	O
we	O
need	O
the	O
distribution	B
of	O
the	O
n	O
other	O
individuals	O
o	O
pnab	O
ab	O
p	O
nab	O
j	O
s	O
the	O
likelihood	B
ratio	O
is	O
nab	O
p	O
nab	O
j	O
s	O
p	O
nab	O
j	O
non	O
po	O
this	O
is	O
an	O
instructive	O
result	O
the	O
likelihood	B
ratio	O
i	O
e	O
the	O
contribution	O
of	O
these	O
data	O
to	O
the	O
question	O
of	O
whether	O
oliver	B
was	O
present	O
depends	O
simply	O
on	O
a	O
comparison	O
of	O
the	O
frequency	B
of	O
his	O
blood	O
type	O
in	O
the	O
observed	O
data	O
with	O
the	O
background	O
frequency	B
in	O
the	O
population	O
there	O
is	O
no	O
dependence	O
on	O
the	O
counts	O
of	O
the	O
other	O
types	O
found	O
at	O
the	O
scene	O
or	O
their	O
frequencies	O
in	O
the	O
population	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
if	O
there	O
are	O
more	O
type	O
o	O
stains	O
than	O
the	O
average	O
number	O
expected	O
under	O
hypothesis	O
then	O
the	O
data	O
give	O
evidence	B
in	O
favour	O
of	O
the	O
presence	O
of	O
oliver	B
conversely	O
if	O
there	O
are	O
fewer	O
type	O
o	O
stains	O
than	O
the	O
expected	O
number	O
under	O
then	O
the	O
data	O
reduce	O
the	O
probability	B
of	O
the	O
hypothesis	O
that	O
he	O
was	O
there	O
in	O
the	O
special	O
case	O
non	O
po	O
the	O
data	O
contribute	O
no	O
evidence	B
either	O
way	O
regardless	O
of	O
the	O
fact	O
that	O
the	O
data	O
are	O
compatible	O
with	O
the	O
hypothesis	O
s	O
exercises	O
exercise	O
the	O
three	B
doors	B
normal	B
rules	B
on	O
a	O
game	B
show	I
a	O
contestant	O
is	O
told	O
the	O
rules	B
as	O
follows	O
there	O
are	O
three	B
doors	B
labelled	O
a	O
single	O
prize	B
has	O
been	O
hidden	O
behind	O
one	O
of	O
them	O
you	O
get	O
to	O
select	O
one	O
door	O
initially	O
your	O
chosen	O
door	O
will	O
not	O
be	O
opened	O
instead	O
the	O
gameshow	O
host	O
will	O
open	O
one	O
of	O
the	O
other	O
two	O
doors	B
and	O
he	O
will	O
do	O
so	O
in	O
such	O
a	O
way	O
as	O
not	O
to	O
reveal	O
the	O
prize	B
for	O
example	O
if	O
you	O
choose	O
door	O
he	O
will	O
then	O
open	O
one	O
of	O
doors	B
and	O
and	O
it	O
is	O
guaranteed	O
that	O
he	O
will	O
choose	O
which	O
one	O
to	O
open	O
so	O
that	O
the	O
prize	B
will	O
not	O
be	O
revealed	O
at	O
this	O
point	O
you	O
will	O
be	O
given	O
a	O
fresh	O
choice	O
of	O
door	O
you	O
can	O
either	O
stick	O
with	O
your	O
choice	O
or	O
you	O
can	O
switch	O
to	O
the	O
other	O
closed	O
door	O
all	O
the	O
doors	B
will	O
then	O
be	O
opened	O
and	O
you	O
will	O
receive	O
whatever	O
is	O
behind	O
your	O
choice	O
of	O
door	O
imagine	O
that	O
the	O
contestant	O
chooses	O
door	O
then	O
the	O
gameshow	O
host	O
opens	O
door	O
revealing	O
nothing	O
behind	O
the	O
door	O
as	O
promised	O
should	O
the	O
contestant	O
stick	O
with	O
door	O
or	O
switch	O
to	O
door	O
or	O
does	O
it	O
make	O
no	O
exercise	O
the	O
three	B
doors	B
earthquake	B
scenario	O
imagine	O
that	O
the	O
game	O
happens	O
again	O
and	O
just	O
as	O
the	O
gameshow	O
host	O
is	O
about	O
to	O
open	O
one	O
of	O
the	O
doors	B
a	O
violent	O
earthquake	B
rattles	O
the	O
building	O
and	O
one	O
of	O
the	O
three	B
doors	B
open	O
it	O
happens	O
to	O
be	O
door	O
and	O
it	O
happens	O
not	O
to	O
have	O
the	O
prize	B
behind	O
it	O
the	O
contestant	O
had	O
initially	O
chosen	O
door	O
repositioning	O
his	O
the	O
host	O
suggests	O
ok	O
since	O
you	O
chose	O
door	O
initially	O
door	O
is	O
a	O
valid	O
door	O
for	O
me	O
to	O
open	O
according	O
to	O
the	O
rules	B
of	O
the	O
game	O
i	O
ll	O
let	O
door	O
stay	O
open	O
let	O
s	O
carry	O
on	O
as	O
if	O
nothing	O
happened	O
should	O
the	O
contestant	O
stick	O
with	O
door	O
or	O
switch	O
to	O
door	O
or	O
does	O
it	O
make	O
no	O
assume	O
that	O
the	O
prize	B
was	O
placed	O
randomly	O
that	O
the	O
gameshow	O
host	O
does	O
not	O
know	O
where	O
it	O
is	O
and	O
that	O
the	O
door	O
open	O
because	O
its	O
latch	O
was	O
broken	O
by	O
the	O
earthquake	B
similar	O
alternative	O
scenario	O
is	O
a	O
gameshow	O
whose	O
confused	O
host	O
forgets	O
the	O
rules	B
and	O
where	O
the	O
prize	B
is	O
and	O
opens	O
one	O
of	O
the	O
unchosen	O
doors	B
at	O
random	B
he	O
opens	O
door	O
and	O
the	O
prize	B
is	O
not	O
revealed	O
should	O
the	O
contestant	O
choose	O
what	O
s	O
behind	O
door	O
or	O
door	O
does	O
the	O
optimal	B
decision	O
for	O
the	O
contestant	O
depend	O
on	O
the	O
contestant	O
s	O
beliefs	O
about	O
whether	O
the	O
gameshow	O
host	O
is	O
confused	O
or	O
not	O
exercise	O
another	O
example	O
in	O
which	O
the	O
emphasis	O
is	O
not	O
on	O
priors	O
you	O
visit	O
a	O
family	O
whose	O
three	O
children	O
are	O
all	O
at	O
the	O
local	O
school	O
you	O
don	O
t	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
know	O
anything	O
about	O
the	O
sexes	O
of	O
the	O
children	O
while	O
walking	O
clumsily	O
round	O
the	O
home	O
you	O
stumble	O
through	O
one	O
of	O
the	O
three	O
unlabelled	O
bedroom	O
doors	B
that	O
you	O
know	O
belong	O
one	O
each	O
to	O
the	O
three	O
children	O
and	O
that	O
the	O
bedroom	O
contains	O
girlie	O
in	O
quantities	O
to	O
convince	O
you	O
that	O
the	O
child	O
who	O
lives	O
in	O
that	O
bedroom	O
is	O
a	O
girl	O
later	O
you	O
sneak	O
a	O
look	O
at	O
a	O
letter	O
addressed	O
to	O
the	O
parents	O
which	O
reads	O
from	O
the	O
headmaster	O
we	O
are	O
sending	O
this	O
letter	O
to	O
all	O
parents	O
who	O
have	O
male	B
children	O
at	O
the	O
school	O
to	O
inform	O
them	O
about	O
the	O
following	O
boyish	B
matters	I
these	O
two	O
sources	O
of	O
evidence	B
establish	O
that	O
at	O
least	O
one	O
of	O
the	O
three	O
children	O
is	O
a	O
girl	O
and	O
that	O
at	O
least	O
one	O
of	O
the	O
children	O
is	O
a	O
boy	O
what	O
are	O
the	O
probabilities	O
that	O
there	O
are	O
two	O
girls	O
and	O
one	O
boy	O
two	O
boys	O
and	O
one	O
girl	O
exercise	O
mrs	O
s	O
is	O
found	O
stabbed	O
in	O
her	O
family	O
garden	O
mr	O
s	O
behaves	O
strangely	O
after	O
her	O
death	O
and	O
is	O
considered	O
as	O
a	O
suspect	O
on	O
investigation	O
of	O
police	O
and	O
social	O
records	O
it	O
is	O
found	O
that	O
mr	O
s	O
had	O
beaten	O
up	O
his	O
wife	O
on	O
at	O
least	O
nine	O
previous	O
occasions	O
the	O
prosecution	O
advances	O
this	O
data	O
as	O
evidence	B
in	O
favour	O
of	O
the	O
hypothesis	O
that	O
mr	O
s	O
is	O
guilty	O
of	O
the	O
murder	B
ah	O
no	O
says	O
mr	O
s	O
s	O
highly	O
paid	O
lawyer	B
statistically	O
only	O
one	O
in	O
a	O
thousand	O
wife-beaters	O
actually	O
goes	O
on	O
to	O
murder	B
his	O
so	O
the	O
wife-beating	O
is	O
not	O
strong	O
evidence	B
at	O
all	O
in	O
fact	O
given	O
the	O
wife-beating	O
evidence	B
alone	O
it	O
s	O
extremely	O
unlikely	O
that	O
he	O
would	O
be	O
the	O
murderer	O
of	O
his	O
wife	O
only	O
a	O
chance	O
you	O
should	O
therefore	O
him	O
innocent	O
is	O
the	O
lawyer	B
right	O
to	O
imply	O
that	O
the	O
history	O
of	O
wife-beating	O
does	O
not	O
point	O
to	O
mr	O
s	O
s	O
being	O
the	O
murderer	O
or	O
is	O
the	O
lawyer	B
a	O
slimy	O
trickster	O
if	O
the	O
latter	O
what	O
is	O
wrong	O
with	O
his	O
argument	O
received	O
an	O
indignant	O
letter	O
from	O
a	O
lawyer	B
about	O
the	O
preceding	O
paragraph	O
i	O
d	O
like	O
to	O
add	O
an	O
extra	O
inference	B
exercise	O
at	O
this	O
point	O
does	O
my	O
suggestion	O
that	O
mr	O
s	O
s	O
lawyer	B
may	O
have	O
been	O
a	O
slimy	O
trickster	O
imply	O
that	O
i	O
believe	O
all	O
lawyers	O
are	O
slimy	O
tricksters	O
no	O
exercise	O
a	O
bag	O
contains	O
one	O
counter	O
known	O
to	O
be	O
either	O
white	B
or	O
black	B
a	O
white	B
counter	O
is	O
put	O
in	O
the	O
bag	O
is	O
shaken	O
and	O
a	O
counter	O
is	O
drawn	O
out	O
which	O
proves	O
to	O
be	O
white	B
what	O
is	O
now	O
the	O
chance	O
of	O
drawing	O
a	O
white	B
counter	O
that	O
the	O
state	O
of	O
the	O
bag	O
after	O
the	O
operations	O
is	O
exactly	O
identical	O
to	O
its	O
state	O
before	O
exercise	O
you	O
move	O
into	O
a	O
new	O
house	O
the	O
phone	B
is	O
connected	O
and	O
you	O
re	O
pretty	O
sure	O
that	O
the	O
phone	B
number	I
is	O
but	O
not	O
as	O
sure	O
as	O
you	O
would	O
like	O
to	O
be	O
as	O
an	O
experiment	O
you	O
pick	O
up	O
the	O
phone	B
and	O
dial	O
you	O
obtain	O
a	O
busy	O
signal	O
are	O
you	O
now	O
more	O
sure	O
of	O
your	O
phone	B
number	I
if	O
so	O
how	O
much	O
exercise	O
in	O
a	O
game	O
two	O
coins	O
are	O
tossed	O
if	O
either	O
of	O
the	O
coins	O
comes	O
up	O
heads	O
you	O
have	O
won	O
a	O
prize	B
to	O
claim	O
the	O
prize	B
you	O
must	O
point	O
to	O
one	O
of	O
your	O
coins	O
that	O
is	O
a	O
head	O
and	O
say	O
look	O
that	O
coin	B
s	O
a	O
head	O
i	O
ve	O
won	O
you	O
watch	O
fred	O
play	O
the	O
game	O
he	O
tosses	O
the	O
two	O
coins	O
and	O
he	O
the	O
u	O
s	O
a	O
it	O
is	O
estimated	O
that	O
million	O
women	O
are	O
abused	O
each	O
year	O
by	O
their	O
partners	O
in	O
women	O
were	O
victims	O
of	O
homicide	O
of	O
those	O
women	O
were	O
slain	O
by	O
husbands	O
and	O
boyfriends	O
httpwww	O
umn	O
edumincavapapersfactoid	O
htm	O
httpwww	O
gunfree	O
inter	O
netvpcwomenfs	O
htm	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
points	O
to	O
a	O
coin	B
and	O
says	O
look	O
that	O
coin	B
s	O
a	O
head	O
i	O
ve	O
won	O
what	O
is	O
the	O
probability	B
that	O
the	O
other	O
coin	B
is	O
a	O
head	O
exercise	O
a	O
statistical	B
statement	O
appeared	O
in	O
the	O
guardian	O
on	O
friday	O
january	O
when	O
spun	O
on	O
edge	B
times	O
a	O
belgian	O
one-euro	O
coin	B
came	O
up	O
heads	O
times	O
and	O
tails	O
it	O
looks	O
very	O
suspicious	O
to	O
me	O
said	O
barry	O
blight	O
a	O
statistics	O
lecturer	O
at	O
the	O
london	O
school	O
of	O
economics	O
if	O
the	O
coin	B
were	O
unbiased	O
the	O
chance	O
of	O
getting	O
a	O
result	O
as	O
extreme	O
as	O
that	O
would	O
be	O
less	O
than	O
but	O
do	O
these	O
data	O
give	O
evidence	B
that	O
the	O
coin	B
is	O
biased	O
rather	O
than	O
fair	O
see	O
equation	O
solutions	O
solution	O
to	O
exercise	O
probabilities	O
let	O
the	O
data	O
be	O
d	O
assuming	O
equal	O
prior	B
p	O
d	O
p	O
j	O
d	O
and	O
p	O
d	O
solution	O
to	O
exercise	O
the	O
probability	B
of	O
the	O
data	O
given	O
each	O
hypothesis	O
is	O
p	O
j	O
a	O
p	O
j	O
b	O
p	O
j	O
c	O
figure	O
posterior	B
probability	B
for	O
the	O
bias	B
pa	O
of	O
a	O
bent	B
coin	B
given	O
two	O
data	O
sets	O
so	O
p	O
d	O
p	O
j	O
d	O
p	O
j	O
d	O
p	O
j	O
s	O
aba	O
f	O
a	O
pa	O
p	O
j	O
s	O
bbb	O
f	O
solution	O
to	O
exercise	O
p	O
j	O
s	O
aba	O
f	O
pa	O
the	O
most	O
probable	O
value	O
of	O
pa	O
the	O
value	O
that	O
maximizes	O
the	O
posterior	B
probability	B
density	B
is	O
the	O
mean	B
value	O
of	O
pa	O
is	O
see	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
the	O
right-hand	O
figure	O
range	O
of	O
plausible	O
values	O
of	O
the	O
log	O
evidence	B
in	O
favour	O
of	O
as	O
a	O
function	O
of	O
f	O
the	O
vertical	O
axis	O
on	O
the	O
left	O
is	O
log	O
p	O
j	O
p	O
j	O
vertical	O
axis	O
shows	O
the	O
values	O
of	O
p	O
j	O
p	O
j	O
the	O
solid	O
line	O
shows	O
the	O
log	O
evidence	B
if	O
the	O
random	B
variable	I
fa	O
takes	O
on	O
its	O
mean	B
value	O
fa	O
paf	O
the	O
dotted	O
lines	O
show	O
the	O
log	O
evidence	B
if	O
fa	O
is	O
at	O
its	O
or	O
percentile	O
also	O
p	O
j	O
s	O
bbb	O
f	O
the	O
most	O
probable	O
value	O
of	O
pa	O
the	O
value	O
that	O
maximizes	O
the	O
posterior	B
probability	B
density	B
is	O
the	O
mean	B
value	O
of	O
pa	O
is	O
see	O
is	O
true	O
pa	O
is	O
true	O
pa	O
pa	O
solution	O
to	O
exercise	O
the	O
curves	O
in	O
were	O
found	O
by	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
fa	O
then	O
setting	O
fa	O
to	O
the	O
mean	B
two	O
standard	O
deviations	O
to	O
get	O
a	O
plausible	O
range	O
for	O
fa	O
and	O
computing	O
the	O
three	O
corresponding	O
values	O
of	O
the	O
log	O
evidence	B
ratio	O
solution	O
to	O
exercise	O
let	O
hi	O
denote	O
the	O
hypothesis	O
that	O
the	O
prize	B
is	O
behind	O
door	O
i	O
we	O
make	O
the	O
following	O
assumptions	B
the	O
three	O
hypotheses	O
and	O
are	O
equiprobable	O
a	O
priori	O
i	O
e	O
p	O
p	O
p	O
the	O
datum	O
we	O
receive	O
after	O
choosing	O
door	O
is	O
one	O
of	O
d	O
and	O
d	O
door	O
or	O
is	O
opened	O
respectively	O
we	O
assume	O
that	O
these	O
two	O
possible	O
outcomes	O
have	O
the	O
following	O
probabilities	O
if	O
the	O
prize	B
is	O
behind	O
door	O
then	O
the	O
host	O
has	O
a	O
free	O
choice	O
in	O
this	O
case	O
we	O
assume	O
that	O
the	O
host	O
selects	O
at	O
random	B
between	O
d	O
and	O
d	O
otherwise	O
the	O
choice	O
of	O
the	O
host	O
is	O
forced	O
and	O
the	O
probabilities	O
are	O
and	O
p	O
p	O
p	O
p	O
p	O
p	O
now	O
using	O
bayes	B
theorem	O
we	O
evaluate	O
the	O
posterior	O
probabilities	O
of	O
the	O
hypotheses	O
p	O
j	O
d	O
p	O
p	O
p	O
j	O
d	O
p	O
the	O
denominator	O
p	O
is	O
because	O
it	O
is	O
the	O
normalizing	O
constant	O
for	O
this	O
posterior	O
distribution	B
so	O
p	O
j	O
d	O
p	O
p	O
j	O
d	O
p	O
p	O
j	O
d	O
p	O
j	O
d	O
p	O
j	O
d	O
so	O
the	O
contestant	O
should	O
switch	O
to	O
door	O
in	O
order	O
to	O
have	O
the	O
biggest	O
chance	O
of	O
getting	O
the	O
prize	B
many	O
people	O
this	O
outcome	O
surprising	O
there	O
are	O
two	O
ways	O
to	O
make	O
it	O
more	O
intuitive	O
one	O
is	O
to	O
play	O
the	O
game	O
thirty	O
times	O
with	O
a	O
friend	O
and	O
keep	O
track	O
of	O
the	O
frequency	B
with	O
which	O
switching	O
gets	O
the	O
prize	B
alternatively	O
you	O
can	O
perform	O
a	O
thought	O
experiment	O
in	O
which	O
the	O
game	O
is	O
played	O
with	O
a	O
million	O
doors	B
the	O
rules	B
are	O
now	O
that	O
the	O
contestant	O
chooses	O
one	O
door	O
then	O
the	O
game	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
show	O
host	O
opens	O
doors	B
in	O
such	O
a	O
way	O
as	O
not	O
to	O
reveal	O
the	O
prize	B
leaving	O
the	O
contestant	O
s	O
selected	O
door	O
and	O
one	O
other	O
door	O
closed	O
the	O
contestant	O
may	O
now	O
stick	O
or	O
switch	O
imagine	O
the	O
contestant	O
confronted	O
by	O
a	O
million	O
doors	B
of	O
which	O
doors	B
and	O
have	O
not	O
been	O
opened	O
door	O
having	O
been	O
the	O
contestant	O
s	O
initial	O
guess	O
where	O
do	O
you	O
think	O
the	O
prize	B
is	O
solution	O
to	O
exercise	O
if	O
door	O
is	O
opened	O
by	O
an	O
earthquake	B
the	O
inference	B
comes	O
out	O
even	O
though	O
visually	O
the	O
scene	O
looks	O
the	O
same	O
the	O
nature	O
of	O
the	O
data	O
and	O
the	O
probability	B
of	O
the	O
data	O
are	O
both	O
now	O
the	O
possible	O
data	O
outcomes	O
are	O
that	O
any	O
number	O
of	O
the	O
doors	B
might	O
have	O
opened	O
we	O
could	O
label	O
the	O
eight	O
possible	O
outcomes	O
d	O
secondly	O
it	O
might	O
be	O
that	O
the	O
prize	B
is	O
visible	O
after	O
the	O
earthquake	B
has	O
opened	O
one	O
or	O
more	O
doors	B
so	O
the	O
data	O
d	O
consists	O
of	O
the	O
value	O
of	O
d	O
and	O
a	O
statement	O
of	O
whether	O
the	O
prize	B
was	O
revealed	O
it	O
is	O
hard	O
to	O
say	O
what	O
the	O
probabilities	O
of	O
these	O
outcomes	O
are	O
since	O
they	O
depend	O
on	O
our	O
beliefs	O
about	O
the	O
reliability	O
of	O
the	O
door	O
latches	O
and	O
the	O
properties	O
of	O
earthquakes	O
but	O
it	O
is	O
possible	O
to	O
extract	O
the	O
desired	O
posterior	B
probability	B
without	O
naming	O
the	O
values	O
of	O
p	O
for	O
each	O
d	O
all	O
that	O
matters	O
are	O
the	O
relative	B
values	O
of	O
the	O
quantities	O
p	O
p	O
p	O
for	O
the	O
value	O
of	O
d	O
that	O
actually	O
occurred	O
is	O
the	O
likelihood	B
principle	I
which	O
we	O
met	O
in	O
section	B
the	O
value	O
of	O
d	O
that	O
actually	O
occurred	O
is	O
d	O
and	O
no	O
prize	B
visible	O
first	O
it	O
is	O
clear	O
that	O
p	O
since	O
the	O
datum	O
that	O
no	O
prize	B
is	O
visible	O
is	O
incompatible	O
with	O
now	O
assuming	O
that	O
the	O
contestant	O
selected	O
door	O
how	O
does	O
the	O
probability	B
p	O
compare	O
with	O
p	O
assuming	O
that	O
earthquakes	O
are	O
not	O
sensitive	O
to	O
decisions	O
of	O
game	B
show	I
contestants	O
these	O
two	O
quantities	O
have	O
to	O
be	O
equal	O
by	O
symmetry	O
we	O
don	O
t	O
know	O
how	O
likely	O
it	O
is	O
that	O
door	O
falls	O
its	O
hinges	O
but	O
however	O
likely	O
it	O
is	O
it	O
s	O
just	O
as	O
likely	O
to	O
do	O
so	O
whether	O
the	O
prize	B
is	O
behind	O
door	O
or	O
door	O
so	O
if	O
p	O
and	O
p	O
are	O
equal	O
we	O
obtain	O
p	O
p	O
p	O
p	O
p	O
p	O
p	O
p	O
p	O
the	O
two	O
possible	O
hypotheses	O
are	O
now	O
equally	O
likely	O
if	O
we	O
assume	O
that	O
the	O
host	O
knows	O
where	O
the	O
prize	B
is	O
and	O
might	O
be	O
acting	O
deceptively	O
then	O
the	O
answer	O
might	O
be	O
further	O
because	O
we	O
have	O
to	O
view	O
the	O
host	O
s	O
words	O
as	O
part	O
of	O
the	O
data	O
confused	O
it	O
s	O
well	O
worth	O
making	O
sure	O
you	O
understand	O
these	O
two	O
gameshow	O
problems	O
don	O
t	O
worry	O
i	O
slipped	O
up	O
on	O
the	O
second	O
problem	O
the	O
time	O
i	O
met	O
it	O
there	O
is	O
a	O
general	O
rule	O
which	O
helps	O
immensely	O
when	O
you	O
have	O
a	O
confusing	O
probability	B
problem	O
always	O
write	O
down	O
the	O
probability	B
of	O
everything	O
gull	B
from	O
this	O
joint	B
probability	B
any	O
desired	O
inference	B
can	O
be	O
mechanically	O
ob	O
tained	O
solution	O
to	O
exercise	O
the	O
statistic	B
quoted	O
by	O
the	O
lawyer	B
indicates	O
the	O
probability	B
that	O
a	O
randomly	O
selected	O
wife-beater	B
will	O
also	O
murder	B
his	O
wife	O
the	O
probability	B
that	O
the	O
husband	O
was	O
the	O
murderer	O
given	O
that	O
the	O
wife	O
has	O
been	O
murdered	O
is	O
a	O
completely	O
quantity	O
where	O
the	O
prize	B
is	O
door	O
door	O
door	O
none	O
pnone	O
pnone	O
pnone	O
e	O
k	O
a	O
u	O
q	O
h	O
t	O
r	O
a	O
e	O
y	O
b	O
d	O
e	O
n	O
e	O
p	O
o	O
s	O
r	O
o	O
o	O
d	O
h	O
c	O
i	O
h	O
w	O
figure	O
the	O
probability	B
of	O
everything	O
for	O
the	O
second	O
three-door	O
problem	O
assuming	O
an	O
earthquake	B
has	O
just	O
occurred	O
here	O
is	O
the	O
probability	B
that	O
door	O
alone	O
is	O
opened	O
by	O
an	O
earthquake	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
to	O
deduce	O
the	O
latter	O
we	O
need	O
to	O
make	O
further	O
assumptions	B
about	O
the	O
probability	B
that	O
the	O
wife	O
is	O
murdered	O
by	O
someone	O
else	O
if	O
she	O
lives	O
in	O
a	O
neighbourhood	O
with	O
frequent	O
random	B
murders	O
then	O
this	O
probability	B
is	O
large	O
and	O
the	O
posterior	B
probability	B
that	O
the	O
husband	O
did	O
it	O
the	O
absence	O
of	O
other	O
evidence	B
may	O
not	O
be	O
very	O
large	O
but	O
in	O
more	O
peaceful	O
regions	O
it	O
may	O
well	O
be	O
that	O
the	O
most	O
likely	O
person	O
to	O
have	O
murdered	O
you	O
if	O
you	O
are	O
found	O
murdered	O
is	O
one	O
of	O
your	O
closest	O
relatives	O
let	O
s	O
work	O
out	O
some	O
illustrative	O
numbers	O
with	O
the	O
help	O
of	O
the	O
statistics	O
on	O
page	O
let	O
m	O
denote	O
the	O
proposition	O
that	O
a	O
woman	O
has	O
been	O
murdered	O
h	O
the	O
proposition	O
that	O
the	O
husband	O
did	O
it	O
and	O
b	O
the	O
proposition	O
that	O
he	O
beat	O
her	O
in	O
the	O
year	O
preceding	O
the	O
murder	B
the	O
statement	O
someone	O
else	O
did	O
it	O
is	O
denoted	O
by	O
h	O
we	O
need	O
to	O
p	O
m	O
p	O
h	O
m	O
and	O
p	O
h	O
m	O
in	O
order	O
to	O
compute	O
the	O
posterior	B
probability	B
p	O
b	O
m	O
from	O
the	O
statistics	O
we	O
can	O
read	O
out	O
p	O
m	O
and	O
if	O
two	O
million	O
women	O
out	O
of	O
million	O
are	O
beaten	O
then	O
p	O
h	O
m	O
finally	O
we	O
need	O
a	O
value	O
for	O
p	O
h	O
m	O
if	O
a	O
man	O
murders	O
his	O
wife	O
how	O
likely	O
is	O
it	O
that	O
this	O
is	O
the	O
time	O
he	O
laid	O
a	O
on	O
her	O
i	O
expect	O
it	O
s	O
pretty	O
unlikely	O
so	O
maybe	O
p	O
h	O
m	O
is	O
or	O
larger	O
by	O
bayes	B
theorem	O
then	O
p	O
b	O
m	O
one	O
way	O
to	O
make	O
obvious	O
the	O
sliminess	O
of	O
the	O
lawyer	B
on	O
is	O
to	O
construct	O
arguments	O
with	O
the	O
same	O
logical	O
structure	O
as	O
his	O
that	O
are	O
clearly	O
wrong	O
for	O
example	O
the	O
lawyer	B
could	O
say	O
not	O
only	O
was	O
mrs	O
s	O
murdered	O
she	O
was	O
murdered	O
between	O
and	O
statistically	O
only	O
one	O
in	O
a	O
million	O
wife-beaters	O
actually	O
goes	O
on	O
to	O
murder	B
his	O
wife	O
between	O
and	O
so	O
the	O
wife-beating	O
is	O
not	O
strong	O
evidence	B
at	O
all	O
in	O
fact	O
given	O
the	O
wife-beating	O
evidence	B
alone	O
it	O
s	O
extremely	O
unlikely	O
that	O
he	O
would	O
murder	B
his	O
wife	O
in	O
this	O
way	O
only	O
a	O
chance	O
solution	O
to	O
exercise	O
there	O
are	O
two	O
hypotheses	O
your	O
number	O
is	O
it	O
is	O
another	O
number	O
the	O
data	O
d	O
are	O
when	O
i	O
dialed	O
i	O
got	O
a	O
busy	O
signal	O
what	O
is	O
the	O
probability	B
of	O
d	O
given	O
each	O
hypothesis	O
if	O
your	O
number	O
is	O
then	O
we	O
expect	O
a	O
busy	O
signal	O
with	O
certainty	O
p	O
on	O
the	O
other	O
hand	O
if	O
is	O
true	O
then	O
the	O
probability	B
that	O
the	O
number	O
dialled	O
returns	O
a	O
busy	O
signal	O
is	O
smaller	O
than	O
since	O
various	O
other	O
outcomes	O
were	O
also	O
possible	O
ringing	O
tone	O
or	O
a	O
number-unobtainable	O
signal	O
for	O
example	O
the	O
value	O
of	O
this	O
probability	B
p	O
will	O
depend	O
on	O
the	O
probability	B
that	O
a	O
random	B
phone	B
number	I
similar	O
to	O
your	O
own	O
phone	B
number	I
would	O
be	O
a	O
valid	O
phone	B
number	I
and	O
on	O
the	O
probability	B
that	O
you	O
get	O
a	O
busy	O
signal	O
when	O
you	O
dial	O
a	O
valid	O
phone	B
number	I
i	O
estimate	O
from	O
the	O
size	O
of	O
my	O
phone	B
book	O
that	O
cambridge	O
has	O
about	O
valid	O
phone	B
numbers	O
all	O
of	O
length	O
six	B
digits	O
the	O
probability	B
that	O
a	O
random	B
six-digit	O
number	O
is	O
valid	O
is	O
therefore	O
about	O
if	O
we	O
exclude	O
numbers	O
beginning	O
with	O
and	O
from	O
the	O
random	B
choice	O
the	O
probability	B
is	O
about	O
if	O
we	O
assume	O
that	O
telephone	O
numbers	O
are	O
clustered	O
then	O
a	O
misremembered	O
number	O
might	O
be	O
more	O
likely	O
to	O
be	O
valid	O
than	O
a	O
randomly	O
chosen	O
number	O
so	O
the	O
probability	B
that	O
our	O
guessed	O
number	O
would	O
be	O
valid	O
assuming	O
is	O
true	O
might	O
be	O
bigger	O
than	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
anyway	O
must	O
be	O
somewhere	O
between	O
and	O
we	O
can	O
carry	O
forward	O
this	O
uncertainty	O
in	O
the	O
probability	B
and	O
see	O
how	O
much	O
it	O
matters	O
at	O
the	O
end	O
the	O
probability	B
that	O
you	O
get	O
a	O
busy	O
signal	O
when	O
you	O
dial	O
a	O
valid	O
phone	B
number	I
is	O
equal	O
to	O
the	O
fraction	O
of	O
phones	O
you	O
think	O
are	O
in	O
use	O
or	O
when	O
you	O
make	O
your	O
tentative	O
call	O
this	O
fraction	O
varies	O
from	O
town	O
to	O
town	O
and	O
with	O
the	O
time	O
of	O
day	O
in	O
cambridge	O
during	O
the	O
day	O
i	O
would	O
guess	O
that	O
about	O
of	O
phones	O
are	O
in	O
use	O
at	O
maybe	O
or	O
fewer	O
the	O
probability	B
p	O
is	O
the	O
product	O
of	O
and	O
that	O
is	O
about	O
according	O
to	O
our	O
estimates	O
there	O
s	O
about	O
a	O
one-in-a-thousand	O
chance	O
of	O
getting	O
a	O
busy	O
signal	O
when	O
you	O
dial	O
a	O
random	B
number	O
or	O
one-in-ahundred	O
if	O
valid	O
numbers	O
are	O
strongly	O
clustered	O
or	O
if	O
you	O
dial	O
in	O
the	O
wee	O
hours	O
how	O
do	O
the	O
data	O
your	O
beliefs	O
about	O
your	O
phone	B
number	I
the	O
posterior	B
probability	B
ratio	O
is	O
the	O
likelihood	B
ratio	O
times	O
the	O
prior	B
probability	B
ratio	O
p	O
j	O
d	O
p	O
j	O
d	O
p	O
p	O
p	O
p	O
the	O
likelihood	B
ratio	O
is	O
about	O
or	O
so	O
the	O
posterior	B
probability	B
ratio	O
is	O
swung	O
by	O
a	O
factor	O
of	O
or	O
in	O
favour	O
of	O
if	O
the	O
prior	B
probability	B
of	O
was	O
then	O
the	O
posterior	B
probability	B
is	O
p	O
j	O
d	O
p	O
j	O
d	O
p	O
j	O
d	O
or	O
solution	O
to	O
exercise	O
we	O
compare	O
the	O
models	O
the	O
coin	B
is	O
fair	O
and	O
the	O
coin	B
is	O
biased	O
with	O
the	O
prior	B
on	O
its	O
bias	B
set	B
to	O
the	O
uniform	O
distribution	B
p	O
use	O
of	O
a	O
uniform	O
prior	B
seems	O
reasonable	O
to	O
me	O
since	O
i	O
know	O
that	O
some	O
coins	O
such	O
as	O
american	B
pennies	O
have	O
severe	O
biases	O
when	O
spun	O
on	O
edge	B
so	O
the	O
situations	O
p	O
or	O
p	O
or	O
p	O
would	O
not	O
surprise	O
me	O
when	O
i	O
mention	O
the	O
coin	B
is	O
fair	O
a	O
pedant	O
would	O
say	O
how	O
absurd	O
to	O
even	O
consider	O
that	O
the	O
coin	B
is	O
fair	O
any	O
coin	B
is	O
surely	O
biased	O
to	O
some	O
extent	O
and	O
of	O
course	O
i	O
would	O
agree	O
so	O
will	O
pedants	O
kindly	O
understand	O
as	O
meaning	O
the	O
coin	B
is	O
fair	O
to	O
within	O
one	O
part	O
in	O
a	O
thousand	O
i	O
e	O
p	O
the	O
likelihood	B
ratio	O
is	O
p	O
p	O
thus	O
the	O
data	O
give	O
scarcely	O
any	O
evidence	B
either	O
way	O
in	O
fact	O
they	O
give	O
weak	O
evidence	B
to	O
one	O
in	O
favour	O
of	O
no	O
no	O
objects	O
the	O
believer	O
in	O
bias	B
your	O
silly	O
uniform	O
prior	B
doesn	O
t	O
represent	O
my	O
prior	B
beliefs	O
about	O
the	O
bias	B
of	O
biased	O
coins	O
i	O
was	O
expecting	O
only	O
a	O
small	O
bias	B
to	O
be	O
as	O
generous	O
as	O
possible	O
to	O
the	O
let	O
s	O
see	O
how	O
well	O
it	O
could	O
fare	O
if	O
the	O
prior	B
were	O
presciently	O
set	B
let	O
us	O
allow	O
a	O
prior	B
of	O
the	O
form	O
p	O
where	O
figure	O
the	O
probability	B
distribution	B
of	O
the	O
number	O
of	O
heads	O
given	O
the	O
two	O
hypotheses	O
that	O
the	O
coin	B
is	O
fair	O
and	O
that	O
it	O
is	O
biased	O
with	O
the	O
prior	B
distribution	B
of	O
the	O
bias	B
being	O
uniform	O
the	O
outcome	O
heads	O
gives	O
weak	O
evidence	B
in	O
favour	O
of	O
the	O
hypothesis	O
that	O
the	O
coin	B
is	O
fair	O
beta	B
distribution	B
with	O
the	O
original	O
uniform	O
prior	B
reproduced	O
by	O
setting	O
by	O
tweaking	O
the	O
likelihood	B
ratio	O
for	O
over	O
p	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
inference	B
can	O
be	O
increased	O
a	O
little	O
it	O
is	O
shown	O
for	O
several	O
values	O
of	O
in	O
even	O
the	O
most	O
favourable	O
choice	O
of	O
can	O
yield	O
a	O
likelihood	B
ratio	O
of	O
only	O
two	O
to	O
one	O
in	O
favour	O
of	O
in	O
conclusion	O
the	O
data	O
are	O
not	O
very	O
suspicious	O
they	O
can	O
be	O
construed	O
as	O
giving	O
at	O
most	O
two-to-one	O
evidence	B
in	O
favour	O
of	O
one	O
or	O
other	O
of	O
the	O
two	O
hypotheses	O
are	O
these	O
wimpy	O
likelihood	B
ratios	O
the	O
fault	O
of	O
over-restrictive	O
priors	O
is	O
there	O
any	O
way	O
of	O
producing	O
a	O
very	O
suspicious	O
conclusion	O
the	O
prior	B
that	O
is	O
bestmatched	O
to	O
the	O
data	O
in	O
terms	O
of	O
likelihood	B
is	O
the	O
prior	B
that	O
sets	O
p	O
to	O
f	O
with	O
probability	B
one	O
let	O
s	O
call	O
this	O
model	B
the	O
likelihood	B
ratio	O
is	O
p	O
f	O
so	O
the	O
strongest	O
evidence	B
that	O
these	O
data	O
can	O
possibly	O
muster	O
against	O
the	O
hypothesis	O
that	O
there	O
is	O
no	O
bias	B
is	O
six-to-one	O
while	O
we	O
are	O
noticing	O
the	O
absurdly	O
misleading	O
answers	O
that	O
sampling	B
theory	I
statistics	O
produces	O
such	O
as	O
the	O
p-value	B
of	O
in	O
the	O
exercise	O
we	O
just	O
solved	O
let	O
s	O
stick	O
the	O
boot	O
in	O
if	O
we	O
make	O
a	O
tiny	O
change	O
to	O
the	O
data	B
set	B
increasing	O
the	O
number	O
of	O
heads	O
in	O
tosses	O
from	O
to	O
we	O
that	O
the	O
p-value	B
goes	O
below	O
the	O
mystical	O
value	O
of	O
p-value	B
is	O
the	O
sampling	B
theory	I
statistician	O
would	O
happily	O
squeak	O
the	O
probability	B
of	O
getting	O
a	O
result	O
as	O
extreme	O
as	O
heads	O
is	O
smaller	O
than	O
we	O
thus	O
reject	O
the	O
null	O
hypothesis	O
at	O
a	O
level	O
of	O
the	O
correct	O
answer	O
is	O
shown	O
for	O
several	O
values	O
of	O
in	O
the	O
values	O
worth	O
highlighting	O
from	O
this	O
table	O
are	O
the	O
likelihood	B
ratio	O
when	O
uses	O
the	O
standard	O
uniform	O
prior	B
which	O
is	O
in	O
favour	O
of	O
the	O
null	O
hypothesis	O
second	O
the	O
most	O
favourable	O
choice	O
of	O
from	O
the	O
point	O
of	O
view	O
of	O
can	O
only	O
yield	O
a	O
likelihood	B
ratio	O
of	O
about	O
in	O
favour	O
of	O
be	O
warned	O
a	O
p-value	B
of	O
is	O
often	O
interpreted	O
as	O
implying	O
that	O
the	O
odds	B
are	O
stacked	O
about	O
twenty-to-one	O
against	O
the	O
null	O
hypothesis	O
but	O
the	O
truth	O
in	O
this	O
case	O
is	O
that	O
the	O
evidence	B
either	O
slightly	O
favours	O
the	O
null	O
hypothesis	O
or	O
disfavours	O
it	O
by	O
at	O
most	O
to	O
one	O
depending	O
on	O
the	O
choice	O
of	O
prior	B
the	O
p-values	O
and	O
levels	O
of	O
classical	B
statistics	I
should	O
be	O
treated	O
with	O
extreme	O
caution	O
shun	O
them	O
here	O
ends	O
the	O
sermon	O
p	O
p	O
figure	O
likelihood	B
ratio	O
for	O
various	O
choices	O
of	O
the	O
prior	B
distribution	B
s	O
hyperparameter	B
p	O
p	O
figure	O
likelihood	B
ratio	O
for	O
various	O
choices	O
of	O
the	O
prior	B
distribution	B
s	O
hyperparameter	B
when	O
the	O
data	O
are	O
heads	O
in	O
trials	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
part	O
i	O
data	O
compression	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
notation	B
x	O
is	O
a	O
member	O
of	O
the	O
x	O
a	O
set	B
a	O
s	O
is	O
a	O
subset	B
of	O
the	O
s	O
a	O
set	B
a	O
s	O
a	O
s	O
is	O
a	O
subset	B
of	O
or	O
equal	O
to	O
the	O
set	B
a	O
v	O
b	O
a	O
v	O
is	O
the	O
union	B
of	O
the	O
sets	O
b	O
and	O
a	O
v	O
b	O
a	O
v	O
is	O
the	O
intersection	B
of	O
the	O
sets	O
b	O
and	O
a	O
number	O
of	O
elements	O
jaj	O
in	O
set	B
a	O
about	O
chapter	O
in	O
this	O
chapter	O
we	O
discuss	O
how	B
to	I
measure	I
the	O
information	B
content	I
of	O
the	O
outcome	O
of	O
a	O
random	B
experiment	O
this	O
chapter	O
has	O
some	O
tough	O
bits	O
if	O
you	O
the	O
mathematical	O
details	O
hard	O
skim	O
through	O
them	O
and	O
keep	O
going	O
you	O
ll	O
be	O
able	O
to	O
enjoy	O
chapters	O
and	O
without	O
this	O
chapter	O
s	O
tools	O
before	O
reading	O
chapter	O
you	O
should	O
have	O
read	O
chapter	O
and	O
worked	O
on	O
exercises	O
and	O
and	O
exercise	O
below	O
the	O
following	O
exercise	O
is	O
intended	O
to	O
help	O
you	O
think	O
about	O
how	B
to	I
measure	I
information	B
content	I
exercise	O
please	O
work	O
on	O
this	O
problem	O
before	O
reading	O
chapter	O
you	O
are	O
given	O
balls	O
all	O
equal	O
in	O
weight	O
except	O
for	O
one	O
that	O
is	O
either	O
heavier	O
or	O
lighter	O
you	O
are	O
also	O
given	O
a	O
two-pan	O
balance	B
to	O
use	O
in	O
each	O
use	O
of	O
the	O
balance	B
you	O
may	O
put	O
any	O
number	O
of	O
the	O
balls	O
on	O
the	O
left	O
pan	O
and	O
the	O
same	O
number	O
on	O
the	O
right	O
pan	O
and	O
push	O
a	O
button	O
to	O
initiate	O
the	O
weighing	O
there	O
are	O
three	O
possible	O
outcomes	O
either	O
the	O
weights	O
are	O
equal	O
or	O
the	O
balls	O
on	O
the	O
left	O
are	O
heavier	O
or	O
the	O
balls	O
on	O
the	O
left	O
are	O
lighter	O
your	O
task	O
is	O
to	O
design	O
a	O
strategy	O
to	O
determine	O
which	O
is	O
the	O
odd	O
ball	O
and	O
whether	O
it	O
is	O
heavier	O
or	O
lighter	O
than	O
the	O
others	O
in	O
as	O
few	O
uses	O
of	O
the	O
balance	B
as	O
possible	O
while	O
thinking	O
about	O
this	O
problem	O
you	O
may	O
it	O
helpful	O
to	O
consider	O
the	O
following	O
questions	O
how	O
can	O
one	O
measure	O
information	B
when	O
you	O
have	O
the	O
odd	O
ball	O
and	O
whether	O
it	O
is	O
heavy	O
or	O
light	O
how	O
much	O
information	B
have	O
you	O
gained	O
once	O
you	O
have	O
designed	O
a	O
strategy	O
draw	O
a	O
tree	B
showing	O
for	O
each	O
of	O
the	O
possible	O
outcomes	O
of	O
a	O
weighing	O
what	O
weighing	O
you	O
perform	O
next	O
at	O
each	O
node	O
in	O
the	O
tree	B
how	O
much	O
information	B
have	O
the	O
outcomes	O
so	O
far	O
given	O
you	O
and	O
how	O
much	O
information	B
remains	O
to	O
be	O
gained	O
how	O
much	O
information	B
is	O
gained	O
when	O
you	O
learn	O
the	O
state	O
of	O
a	O
coin	B
the	O
states	O
of	O
two	O
coins	O
the	O
outcome	O
when	O
a	O
four-sided	O
die	B
is	O
rolled	O
how	O
much	O
information	B
is	O
gained	O
on	O
the	O
step	O
of	O
the	O
weighing	B
problem	I
if	O
balls	O
are	O
weighed	O
against	O
the	O
other	O
how	O
much	O
is	O
gained	O
if	O
are	O
weighed	O
against	O
on	O
the	O
step	O
leaving	O
out	O
balls	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
how	B
to	I
measure	I
the	O
information	B
content	I
of	O
a	O
random	B
variable	I
in	O
the	O
next	O
few	O
chapters	O
we	O
ll	O
be	O
talking	O
about	O
probability	B
distributions	O
and	O
random	B
variables	O
most	O
of	O
the	O
time	O
we	O
can	O
get	O
by	O
with	O
sloppy	O
notation	B
but	O
occasionally	O
we	O
will	O
need	O
precise	O
notation	B
here	O
is	O
the	O
notation	B
that	O
we	O
established	O
in	O
chapter	O
an	O
ensemble	B
x	O
is	O
a	O
triple	O
where	O
the	O
outcome	O
x	O
is	O
the	O
value	O
of	O
a	O
random	B
variable	I
which	O
takes	O
on	O
one	O
of	O
a	O
set	B
of	O
possible	O
values	O
ax	O
ai	O
aig	O
having	O
probabilities	O
px	O
pig	O
with	O
p	O
ai	O
pi	O
pi	O
and	O
how	O
can	O
we	O
measure	O
the	O
information	B
content	I
of	O
an	O
outcome	O
x	O
ai	O
from	O
such	O
an	O
ensemble	B
in	O
this	O
chapter	O
we	O
examine	O
the	O
assertions	O
p	O
ai	O
that	O
the	O
shannon	B
information	B
content	I
hx	O
ai	O
pi	O
is	O
a	O
sensible	O
measure	O
of	O
the	O
information	B
content	I
of	O
the	O
outcome	O
x	O
ai	O
and	O
that	O
the	O
entropy	B
of	O
the	O
ensemble	B
hx	O
pi	O
pi	O
is	O
a	O
sensible	O
measure	O
of	O
the	O
ensemble	B
s	O
average	O
information	B
content	I
hp	O
p	O
p	O
p	O
hp	O
p	O
figure	O
the	O
shannon	B
information	B
content	I
hp	O
and	O
the	O
binary	B
entropy	B
function	I
hp	O
p	O
p	O
p	O
function	O
of	O
p	O
as	O
a	O
p	O
figure	O
shows	O
the	O
shannon	B
information	B
content	I
of	O
an	O
outcome	O
with	O
probability	B
p	O
as	O
a	O
function	O
of	O
p	O
the	O
less	O
probable	O
an	O
outcome	O
is	O
the	O
greater	O
its	O
shannon	B
information	B
content	I
figure	O
also	O
shows	O
the	O
binary	B
entropy	B
function	I
hp	O
p	O
p	O
p	O
p	O
which	O
is	O
the	O
entropy	B
of	O
the	O
ensemble	B
x	O
whose	O
alphabet	O
and	O
probability	B
distribution	B
are	O
ax	O
fa	O
bgpx	O
fp	O
pg	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
information	B
content	I
of	O
independent	O
random	B
variables	O
why	O
should	O
log	O
have	O
anything	O
to	O
do	O
with	O
the	O
information	B
content	I
why	O
not	O
some	O
other	O
function	O
of	O
pi	O
we	O
ll	O
explore	B
this	O
question	O
in	O
detail	O
shortly	O
but	O
notice	O
a	O
nice	O
property	O
of	O
this	O
particular	O
function	O
hx	O
log	O
imagine	O
learning	B
the	O
value	O
of	O
two	O
independent	O
random	B
variables	O
x	O
and	O
y	O
the	O
of	O
independence	B
is	O
that	O
the	O
probability	B
distribution	B
is	O
separable	O
into	O
a	O
product	O
p	O
y	O
p	O
intuitively	O
we	O
might	O
want	O
any	O
measure	O
of	O
the	O
amount	O
of	O
information	B
gained	O
to	O
have	O
the	O
property	O
of	O
additivity	O
that	O
is	O
for	O
independent	O
random	B
variables	O
x	O
and	O
y	O
the	O
information	B
gained	O
when	O
we	O
learn	O
x	O
and	O
y	O
should	O
equal	O
the	O
sum	O
of	O
the	O
information	B
gained	O
if	O
x	O
alone	O
were	O
learned	O
and	O
the	O
information	B
gained	O
if	O
y	O
alone	O
were	O
learned	O
the	O
shannon	B
information	B
content	I
of	O
the	O
outcome	O
x	O
y	O
is	O
hx	O
y	O
log	O
p	O
y	O
log	O
p	O
log	O
p	O
log	O
p	O
so	O
it	O
does	O
indeed	O
satisfy	O
hx	O
y	O
hx	O
hy	O
if	O
x	O
and	O
y	O
are	O
independent	O
exercise	O
show	O
that	O
if	O
x	O
and	O
y	O
are	O
independent	O
the	O
entropy	B
of	O
the	O
outcome	O
x	O
y	O
hx	O
y	O
hx	O
hy	O
in	O
words	O
entropy	B
is	O
additive	O
for	O
independent	O
variables	O
we	O
now	O
explore	B
these	O
ideas	O
with	O
some	O
examples	O
then	O
in	O
section	B
and	O
in	O
chapters	O
and	O
we	O
prove	O
that	O
the	O
shannon	B
information	B
content	I
and	O
the	O
entropy	B
are	O
related	O
to	O
the	O
number	O
of	O
bits	O
needed	O
to	O
describe	O
the	O
outcome	O
of	O
an	O
experiment	O
the	O
weighing	B
problem	I
designing	O
informative	O
experiments	O
have	O
you	O
solved	O
the	O
weighing	B
problem	I
yet	O
are	O
you	O
sure	O
notice	O
that	O
in	O
three	O
uses	O
of	O
the	O
balance	B
which	O
reads	O
either	O
left	O
heavier	O
right	O
heavier	O
or	O
balanced	O
the	O
number	O
of	O
conceivable	O
outcomes	O
is	O
whereas	O
the	O
number	O
of	O
possible	O
states	O
of	O
the	O
world	O
is	O
the	O
odd	O
ball	O
could	O
be	O
any	O
of	O
twelve	O
balls	O
and	O
it	O
could	O
be	O
heavy	O
or	O
light	O
so	O
in	O
principle	O
the	O
problem	O
might	O
be	O
solvable	O
in	O
three	O
weighings	O
but	O
not	O
in	O
two	O
since	O
if	O
you	O
know	O
how	O
you	O
can	O
determine	O
the	O
odd	O
weight	O
and	O
whether	O
it	O
is	O
heavy	O
or	O
light	O
in	O
three	O
weighings	O
then	O
you	O
may	O
read	O
on	O
if	O
you	O
haven	O
t	O
found	O
a	O
strategy	O
that	O
always	O
gets	O
there	O
in	O
three	O
weighings	O
i	O
encourage	O
you	O
to	O
think	O
about	O
exercise	O
some	O
more	O
why	O
is	O
your	O
strategy	O
optimal	B
what	O
is	O
it	O
about	O
your	O
series	O
of	O
weighings	O
that	O
allows	O
useful	O
information	B
to	O
be	O
gained	O
as	O
quickly	O
as	O
possible	O
the	O
answer	O
is	O
that	O
at	O
each	O
step	O
of	O
an	O
optimal	B
procedure	O
the	O
three	O
outcomes	O
left	O
heavier	O
right	O
heavier	O
and	O
balance	B
are	O
as	O
close	O
as	O
possible	O
to	O
equiprobable	O
an	O
optimal	B
solution	O
is	O
shown	O
in	O
suboptimal	O
strategies	O
such	O
as	O
weighing	O
balls	O
against	O
on	O
the	O
step	O
do	O
not	O
achieve	O
all	O
outcomes	O
with	O
equal	O
probability	B
these	O
two	O
sets	O
of	O
balls	O
can	O
never	O
balance	B
so	O
the	O
only	O
possible	O
outcomes	O
are	O
left	O
heavy	O
and	O
right	O
heavy	O
such	O
a	O
binary	O
outcome	O
rules	B
out	O
only	O
half	O
of	O
the	O
possible	O
hypotheses	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
how	B
to	I
measure	I
the	O
information	B
content	I
of	O
a	O
random	B
variable	I
weigh	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
bbn	O
weigh	O
weigh	O
weigh	O
a	O
a	O
a	O
a	O
au	O
a	O
a	O
a	O
a	O
au	O
a	O
a	O
a	O
a	O
au	O
figure	O
an	O
optimal	B
solution	O
to	O
the	O
weighing	B
problem	I
at	O
each	O
step	O
there	O
are	O
two	O
boxes	O
the	O
left	O
box	B
shows	O
which	O
hypotheses	O
are	O
still	O
possible	O
the	O
right	O
box	B
shows	O
the	O
balls	O
involved	O
in	O
the	O
next	O
weighing	O
the	O
hypotheses	O
are	O
written	O
with	O
e	O
g	O
denoting	O
that	O
is	O
the	O
odd	O
ball	O
and	O
it	O
is	O
heavy	O
weighings	O
are	O
written	O
by	O
listing	O
the	O
names	O
of	O
the	O
balls	O
on	O
the	O
two	O
pans	O
separated	O
by	O
a	O
line	O
for	O
example	O
in	O
the	O
weighing	O
balls	O
and	O
are	O
put	O
on	O
the	O
left-hand	O
side	O
and	O
and	O
on	O
the	O
right	O
in	O
each	O
triplet	O
of	O
arrows	O
the	O
upper	O
arrow	O
leads	O
to	O
the	O
situation	O
when	O
the	O
left	O
side	O
is	O
heavier	O
the	O
middle	O
arrow	O
to	O
the	O
situation	O
when	O
the	O
right	O
side	O
is	O
heavier	O
and	O
the	O
lower	O
arrow	O
to	O
the	O
situation	O
when	O
the	O
outcome	O
is	O
balanced	O
the	O
three	O
points	O
labelled	O
correspond	O
to	O
impossible	O
outcomes	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
so	O
a	O
strategy	O
that	O
uses	O
such	O
outcomes	O
must	O
sometimes	O
take	O
longer	O
to	O
the	O
right	O
answer	O
the	O
insight	O
that	O
the	O
outcomes	O
should	O
be	O
as	O
near	O
as	O
possible	O
to	O
equiprobable	O
makes	O
it	O
easier	O
to	O
search	O
for	O
an	O
optimal	B
strategy	O
the	O
weighing	O
must	O
divide	O
the	O
possible	O
hypotheses	O
into	O
three	O
groups	O
of	O
eight	O
then	O
the	O
second	O
weighing	O
must	O
be	O
chosen	O
so	O
that	O
there	O
is	O
a	O
split	O
of	O
the	O
hypotheses	O
thus	O
we	O
might	O
conclude	O
the	O
outcome	O
of	O
a	O
random	B
experiment	O
is	O
guaranteed	O
to	O
be	O
most	O
informative	O
if	O
the	O
probability	B
distribution	B
over	O
outcomes	O
is	O
uniform	O
this	O
conclusion	O
agrees	O
with	O
the	O
property	O
of	O
the	O
entropy	B
that	O
you	O
proved	O
when	O
you	O
solved	O
exercise	O
the	O
entropy	B
of	O
an	O
ensemble	B
x	O
is	O
biggest	O
if	O
all	O
the	O
outcomes	O
have	O
equal	O
probability	B
pi	O
guessing	B
games	O
in	O
the	O
game	O
of	O
twenty	B
questions	I
one	O
player	O
thinks	O
of	O
an	O
object	O
and	O
the	O
other	O
player	O
attempts	O
to	O
guess	O
what	O
the	O
object	O
is	O
by	O
asking	O
questions	O
that	O
have	O
yesno	O
answers	O
for	O
example	O
is	O
it	O
alive	O
or	O
is	O
it	O
human	B
the	O
aim	O
is	O
to	O
identify	O
the	O
object	O
with	O
as	O
few	O
questions	O
as	O
possible	O
what	O
is	O
the	O
best	O
strategy	O
for	O
playing	O
this	O
game	O
for	O
simplicity	O
imagine	O
that	O
we	O
are	O
playing	O
the	O
rather	O
dull	O
version	O
of	O
twenty	B
questions	I
called	O
sixty-three	B
example	O
the	O
game	O
sixty-three	B
what	O
s	O
the	O
smallest	O
number	O
of	O
yesno	O
questions	O
needed	O
to	O
identify	O
an	O
integer	O
x	O
between	O
and	O
intuitively	O
the	O
best	O
questions	O
successively	O
divide	O
the	O
possibilities	O
into	O
equal	O
sized	O
sets	O
six	B
questions	O
one	O
reasonable	O
strategy	O
asks	O
the	O
following	O
questions	O
is	O
x	O
is	O
x	O
mod	O
is	O
x	O
mod	O
is	O
x	O
mod	O
is	O
x	O
mod	O
is	O
x	O
mod	O
notation	B
x	O
mod	O
pronounced	O
x	O
modulo	O
denotes	O
the	O
remainder	O
when	O
x	O
is	O
divided	O
by	O
for	O
example	O
mod	O
and	O
mod	O
the	O
answers	O
to	O
these	O
questions	O
if	O
translated	O
from	O
fyes	O
nog	O
to	O
give	O
the	O
binary	O
expansion	O
of	O
x	O
for	O
example	O
what	O
are	O
the	O
shannon	B
information	B
contents	O
of	O
the	O
outcomes	O
in	O
this	O
example	O
if	O
we	O
assume	O
that	O
all	O
values	O
of	O
x	O
are	O
equally	O
likely	O
then	O
the	O
answers	O
to	O
the	O
questions	O
are	O
independent	O
and	O
each	O
has	O
shannon	B
information	B
content	I
bit	B
the	O
total	O
shannon	B
information	B
gained	O
is	O
always	O
six	B
bits	O
furthermore	O
the	O
number	O
x	O
that	O
we	O
learn	O
from	O
these	O
questions	O
is	O
a	O
six-bit	O
binary	O
number	O
our	O
questioning	O
strategy	O
a	O
way	O
of	O
encoding	O
the	O
random	B
variable	I
x	O
as	O
a	O
binary	O
so	O
far	O
the	O
shannon	B
information	B
content	I
makes	O
sense	O
it	O
measures	O
the	O
length	O
of	O
a	O
binary	O
that	O
encodes	O
x	O
however	O
we	O
have	O
not	O
yet	O
studied	O
ensembles	O
where	O
the	O
outcomes	O
have	O
unequal	O
probabilities	O
does	O
the	O
shannon	B
information	B
content	I
make	O
sense	O
there	O
too	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
how	B
to	I
measure	I
the	O
information	B
content	I
of	O
a	O
random	B
variable	I
x	O
n	O
x	O
n	O
x	O
y	O
figure	O
a	O
game	O
of	O
submarine	B
the	O
submarine	B
is	O
hit	O
on	O
the	O
attempt	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
x	O
n	O
x	O
n	O
move	O
question	O
outcome	O
p	O
hx	O
total	O
info	O
j	O
the	O
game	O
of	O
submarine	B
how	O
many	O
bits	O
can	O
one	O
bit	B
convey	O
in	O
the	O
game	O
of	O
battleships	B
each	O
player	O
hides	O
a	O
of	O
ships	O
in	O
a	O
sea	O
represented	O
by	O
a	O
square	B
grid	O
on	O
each	O
turn	O
one	O
player	O
attempts	O
to	O
hit	O
the	O
other	O
s	O
ships	O
by	O
at	O
one	O
square	B
in	O
the	O
opponent	O
s	O
sea	O
the	O
response	O
to	O
a	O
selected	O
square	B
such	O
as	O
is	O
either	O
miss	O
hit	O
or	O
hit	O
and	O
destroyed	O
in	O
a	O
boring	O
version	O
of	O
battleships	B
called	O
submarine	B
each	O
player	O
hides	O
just	O
one	O
submarine	B
in	O
one	O
square	B
of	O
an	O
eight-by-eight	O
grid	O
figure	O
shows	O
a	O
few	O
pictures	O
of	O
this	O
game	O
in	O
progress	O
the	O
circle	B
represents	O
the	O
square	B
that	O
is	O
being	O
at	O
and	O
the	O
show	O
squares	O
in	O
which	O
the	O
outcome	O
was	O
a	O
miss	O
x	O
n	O
the	O
submarine	B
is	O
hit	O
x	O
y	O
shown	O
by	O
the	O
symbol	O
s	O
on	O
the	O
attempt	O
each	O
shot	O
made	O
by	O
a	O
player	O
an	O
ensemble	B
the	O
two	O
possible	O
outcomes	O
are	O
fy	O
ng	O
corresponding	O
to	O
a	O
hit	O
and	O
a	O
miss	O
and	O
their	O
probabilities	O
depend	O
on	O
the	O
state	O
of	O
the	O
board	O
at	O
the	O
beginning	O
p	O
and	O
p	O
at	O
the	O
second	O
shot	O
if	O
the	O
shot	O
missed	O
p	O
and	O
p	O
at	O
the	O
third	O
shot	O
if	O
the	O
two	O
shots	O
missed	O
p	O
and	O
p	O
the	O
shannon	B
information	B
gained	O
from	O
an	O
outcome	O
x	O
is	O
hx	O
if	O
we	O
are	O
lucky	O
and	O
hit	O
the	O
submarine	B
on	O
the	O
shot	O
then	O
hx	O
bits	O
now	O
it	O
might	O
seem	O
a	O
little	O
strange	O
that	O
one	O
binary	O
outcome	O
can	O
convey	O
six	B
bits	O
but	O
we	O
have	O
learnt	O
the	O
hiding	O
place	O
which	O
could	O
have	O
been	O
any	O
of	O
squares	O
so	O
we	O
have	O
by	O
one	O
lucky	O
binary	O
question	O
indeed	O
learnt	O
six	B
bits	O
what	O
if	O
the	O
shot	O
misses	O
the	O
shannon	B
information	B
that	O
we	O
gain	B
from	O
this	O
outcome	O
is	O
hx	O
bits	O
does	O
this	O
make	O
sense	O
it	O
is	O
not	O
so	O
obvious	O
let	O
s	O
keep	O
going	O
if	O
our	O
second	O
shot	O
also	O
misses	O
the	O
shannon	B
information	B
content	I
of	O
the	O
second	O
outcome	O
is	O
bits	O
if	O
we	O
miss	O
thirty-two	O
times	O
at	O
a	O
new	O
square	B
each	O
time	O
the	O
total	O
shannon	B
information	B
gained	O
is	O
bits	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
why	O
this	O
round	O
number	O
well	O
what	O
have	O
we	O
learnt	O
we	O
now	O
know	O
that	O
the	O
submarine	B
is	O
not	O
in	O
any	O
of	O
the	O
squares	O
we	O
at	O
learning	B
that	O
fact	O
is	O
just	O
like	O
playing	O
a	O
game	O
of	O
sixty-three	B
asking	O
as	O
our	O
question	O
is	O
x	O
one	O
of	O
the	O
thirty-two	O
numbers	O
corresponding	O
to	O
these	O
squares	O
i	O
at	O
and	O
receiving	O
the	O
answer	O
no	O
this	O
answer	O
rules	B
out	O
half	O
of	O
the	O
hypotheses	O
so	O
it	O
gives	O
us	O
one	O
bit	B
after	O
unsuccessful	O
shots	O
the	O
information	B
gained	O
is	O
bits	O
the	O
unknown	O
location	O
has	O
been	O
narrowed	O
down	O
to	O
one	O
quarter	O
of	O
the	O
original	O
hypothesis	O
space	O
what	O
if	O
we	O
hit	O
the	O
submarine	B
on	O
the	O
shot	O
when	O
there	O
were	O
squares	O
left	O
the	O
shannon	B
information	B
content	I
of	O
this	O
outcome	O
is	O
bits	O
the	O
total	O
shannon	B
information	B
content	I
of	O
all	O
the	O
outcomes	O
is	O
bits	O
so	O
once	O
we	O
know	O
where	O
the	O
submarine	B
is	O
the	O
total	O
shannon	B
information	B
content	I
gained	O
is	O
bits	O
this	O
result	O
holds	O
regardless	O
of	O
when	O
we	O
hit	O
the	O
submarine	B
if	O
we	O
hit	O
it	O
when	O
there	O
are	O
n	O
squares	O
left	O
to	O
choose	O
from	O
n	O
was	O
in	O
equation	O
then	O
the	O
total	O
information	B
gained	O
is	O
n	O
n	O
n	O
n	O
n	O
n	O
bits	O
what	O
have	O
we	O
learned	O
from	O
the	O
examples	O
so	O
far	O
i	O
think	O
the	O
submarine	B
example	O
makes	O
quite	O
a	O
convincing	O
case	O
for	O
the	O
claim	O
that	O
the	O
shannon	B
information	B
content	I
is	O
a	O
sensible	O
measure	O
of	O
information	B
content	I
and	O
the	O
game	O
of	O
sixty-three	B
shows	O
that	O
the	O
shannon	B
information	B
content	I
can	O
be	O
intimately	O
connected	O
to	O
the	O
size	O
of	O
a	O
that	O
encodes	O
the	O
outcomes	O
of	O
a	O
random	B
experiment	O
thus	O
suggesting	O
a	O
possible	O
connection	O
to	O
data	O
compression	B
in	O
case	O
you	O
re	O
not	O
convinced	O
let	O
s	O
look	O
at	O
one	O
more	O
example	O
the	O
wenglish	B
language	O
wenglish	B
is	O
a	O
language	O
similar	O
to	O
english	B
wenglish	B
sentences	O
consist	O
of	O
words	O
drawn	O
at	O
random	B
from	O
the	O
wenglish	B
dictionary	B
which	O
contains	O
words	O
all	O
of	O
length	O
characters	O
each	O
word	O
in	O
the	O
wenglish	B
dictionary	B
was	O
constructed	O
at	O
random	B
by	O
picking	O
letters	O
from	O
the	O
probability	B
distribution	B
over	O
a	O
depicted	O
in	O
some	O
entries	O
from	O
the	O
dictionary	B
are	O
shown	O
in	O
alphabetical	O
order	O
in	O
notice	O
that	O
the	O
number	O
of	O
words	O
in	O
the	O
dictionary	B
is	O
much	O
smaller	O
than	O
the	O
total	O
number	O
of	O
possible	O
words	O
of	O
length	O
letters	O
because	O
the	O
probability	B
of	O
the	O
letter	O
z	O
is	O
about	O
only	O
of	O
the	O
words	O
in	O
the	O
dictionary	B
begin	O
with	O
the	O
letter	O
z	O
in	O
contrast	O
the	O
probability	B
of	O
the	O
letter	O
a	O
is	O
about	O
and	O
of	O
the	O
words	O
begin	O
with	O
the	O
letter	O
a	O
of	O
those	O
words	O
two	O
start	O
az	O
and	O
start	O
aa	O
let	O
s	O
imagine	O
that	O
we	O
are	O
reading	O
a	O
wenglish	B
document	O
and	O
let	O
s	O
discuss	O
the	O
shannon	B
information	B
content	I
of	O
the	O
characters	O
as	O
we	O
acquire	O
them	O
if	O
we	O
aaail	O
aaaiu	O
aaald	O
abati	O
azpan	O
aztdn	O
odrcr	O
zatnt	O
zxast	O
figure	O
the	O
wenglish	B
dictionary	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
data	O
compression	B
are	O
given	O
the	O
text	O
one	O
word	O
at	O
a	O
time	O
the	O
shannon	B
information	B
content	I
of	O
each	O
word	O
is	O
log	O
bits	O
since	O
wenglish	B
uses	O
all	O
its	O
words	O
with	O
equal	O
probability	B
the	O
average	O
information	B
content	I
per	O
character	O
is	O
therefore	O
bits	O
now	O
let	O
s	O
look	O
at	O
the	O
information	B
content	I
if	O
we	O
read	O
the	O
document	O
one	O
character	O
at	O
a	O
time	O
if	O
say	O
the	O
letter	O
of	O
a	O
word	O
is	O
a	O
the	O
shannon	B
information	B
content	I
is	O
log	O
bits	O
if	O
the	O
letter	O
is	O
z	O
the	O
shannon	B
information	B
content	I
is	O
log	O
bits	O
the	O
information	B
content	I
is	O
thus	O
highly	O
variable	O
at	O
the	O
character	O
the	O
total	O
information	B
content	I
of	O
the	O
characters	O
in	O
a	O
word	O
however	O
is	O
exactly	O
bits	O
so	O
the	O
letters	O
that	O
follow	O
an	O
initial	O
z	O
have	O
lower	O
average	O
information	B
content	I
per	O
character	O
than	O
the	O
letters	O
that	O
follow	O
an	O
initial	O
a	O
a	O
rare	O
initial	O
letter	O
such	O
as	O
z	O
indeed	O
conveys	O
more	O
information	B
about	O
what	O
the	O
word	O
is	O
than	O
a	O
common	O
initial	O
letter	O
similarly	O
in	O
english	B
if	O
rare	O
characters	O
occur	O
at	O
the	O
start	O
of	O
the	O
word	O
xyl	O
then	O
often	O
we	O
can	O
identify	O
the	O
whole	O
word	O
immediately	O
whereas	O
words	O
that	O
start	O
with	O
common	O
characters	O
pro	O
require	O
more	O
characters	O
before	O
we	O
can	O
identify	O
them	O
data	O
compression	B
the	O
preceding	O
examples	O
justify	O
the	O
idea	O
that	O
the	O
shannon	B
information	B
content	I
of	O
an	O
outcome	O
is	O
a	O
natural	B
measure	O
of	O
its	O
information	B
content	I
improbable	O
outcomes	O
do	O
convey	O
more	O
information	B
than	O
probable	O
outcomes	O
we	O
now	O
discuss	O
the	O
information	B
content	I
of	O
a	O
source	O
by	O
considering	O
how	O
many	O
bits	O
are	O
needed	O
to	O
describe	O
the	O
outcome	O
of	O
an	O
experiment	O
if	O
we	O
can	O
show	O
that	O
we	O
can	O
compress	B
data	O
from	O
a	O
particular	O
source	O
into	O
a	O
of	O
l	O
bits	O
per	O
source	O
symbol	O
and	O
recover	O
the	O
data	O
reliably	O
then	O
we	O
will	O
say	O
that	O
the	O
average	O
information	B
content	I
of	O
that	O
source	O
is	O
at	O
most	O
l	O
bits	O
per	O
symbol	O
example	O
compression	B
of	O
text	O
a	O
is	O
composed	O
of	O
a	O
sequence	B
of	O
bytes	O
a	O
byte	B
is	O
composed	O
of	O
bits	O
and	O
can	O
have	O
a	O
decimal	O
value	O
between	O
and	O
a	O
typical	B
text	O
is	O
composed	O
of	O
the	O
ascii	O
character	O
set	B
values	O
to	O
this	O
character	O
set	B
uses	O
only	O
seven	O
of	O
the	O
eight	O
bits	O
in	O
a	O
byte	B
here	O
we	O
use	O
the	O
word	O
bit	B
with	O
its	O
meaning	O
a	O
symbol	O
with	O
two	O
values	O
not	O
to	O
be	O
confused	O
with	O
the	O
unit	O
of	O
information	B
content	I
exercise	O
by	O
how	O
much	O
could	O
the	O
size	O
of	O
a	O
be	O
reduced	O
given	O
that	O
it	O
is	O
an	O
ascii	O
how	O
would	O
you	O
achieve	O
this	O
reduction	O
intuitively	O
it	O
seems	O
reasonable	O
to	O
assert	O
that	O
an	O
ascii	O
contains	O
as	O
much	O
information	B
as	O
an	O
arbitrary	O
of	O
the	O
same	O
size	O
since	O
we	O
already	O
know	O
one	O
out	O
of	O
every	O
eight	O
bits	O
before	O
we	O
even	O
look	O
at	O
the	O
this	O
is	O
a	O
simple	O
example	O
of	O
redundancy	B
most	O
sources	O
of	O
data	O
have	O
further	O
redundancy	B
english	B
text	O
use	O
the	O
ascii	O
characters	O
with	O
non-equal	O
frequency	B
certain	O
pairs	O
of	O
letters	O
are	O
more	O
probable	O
than	O
others	O
and	O
entire	O
words	O
can	O
be	O
predicted	O
given	O
the	O
context	O
and	O
a	O
semantic	O
understanding	O
of	O
the	O
text	O
some	O
simple	O
data	O
compression	B
methods	O
that	O
measures	O
of	O
information	B
content	I
one	O
way	O
of	O
measuring	O
the	O
information	B
content	I
of	O
a	O
random	B
variable	I
is	O
simply	O
to	O
count	O
the	O
number	O
of	O
possible	O
outcomes	O
jaxj	O
number	O
of	O
elements	O
in	O
a	O
set	B
a	O
is	O
denoted	O
by	O
jaj	O
if	O
we	O
gave	O
a	O
binary	O
name	O
to	O
each	O
outcome	O
the	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
length	O
of	O
each	O
name	O
would	O
be	O
jaxj	O
bits	O
if	O
jaxj	O
happened	O
to	O
be	O
a	O
power	O
of	O
we	O
thus	O
make	O
the	O
following	O
the	O
raw	O
bit	B
content	O
of	O
x	O
is	O
jaxj	O
is	O
a	O
lower	O
bound	B
for	O
the	O
number	O
of	O
binary	O
questions	O
that	O
are	O
always	O
guaranteed	O
to	O
identify	O
an	O
outcome	O
from	O
the	O
ensemble	B
x	O
it	O
is	O
an	O
additive	O
quantity	O
the	O
raw	O
bit	B
content	O
of	O
an	O
ordered	O
pair	O
x	O
y	O
having	O
jaxjjay	O
j	O
possible	O
outcomes	O
y	O
this	O
measure	O
of	O
information	B
content	I
does	O
not	O
include	O
any	O
probabilistic	O
element	O
and	O
the	O
encoding	O
rule	O
it	O
corresponds	O
to	O
does	O
not	O
compress	B
the	O
source	O
data	O
it	O
simply	O
maps	O
each	O
outcome	O
to	O
a	O
constant-length	O
binary	O
string	O
exercise	O
could	O
there	O
be	O
a	O
compressor	O
that	O
maps	O
an	O
outcome	O
x	O
to	O
a	O
binary	O
code	O
cx	O
and	O
a	O
decompressor	O
that	O
maps	O
c	O
back	O
to	O
x	O
such	O
that	O
every	O
possible	O
outcome	O
is	O
compressed	O
into	O
a	O
binary	O
code	O
of	O
length	O
shorter	O
than	O
bits	O
even	O
though	O
a	O
simple	O
counting	B
argument	I
shows	O
that	O
it	O
is	O
impossible	O
to	O
make	O
a	O
reversible	B
compression	B
program	O
that	O
reduces	O
the	O
size	O
of	O
all	O
amateur	O
compression	B
enthusiasts	O
frequently	O
announce	O
that	O
they	O
have	O
invented	O
a	O
program	O
that	O
can	O
do	O
this	O
indeed	O
that	O
they	O
can	O
further	O
compress	B
compressed	O
by	O
putting	O
them	O
through	O
their	O
compressor	O
several	O
times	O
stranger	O
yet	O
patents	O
have	O
been	O
granted	O
to	O
these	O
modern-day	O
alchemists	B
see	O
the	O
comp	O
compression	B
frequently	O
asked	O
questions	O
for	O
further	O
there	O
are	O
only	O
two	O
ways	O
in	O
which	O
a	O
compressor	O
can	O
actually	O
compress	B
a	O
lossy	B
compressor	O
compresses	O
some	O
but	O
maps	O
some	O
to	O
the	O
same	O
encoding	O
we	O
ll	O
assume	O
that	O
the	O
user	O
requires	O
perfect	B
recovery	O
of	O
the	O
source	O
so	O
the	O
occurrence	O
of	O
one	O
of	O
these	O
confusable	O
leads	O
to	O
a	O
failure	O
in	O
applications	O
such	O
as	O
image	B
compression	B
lossy	B
compression	B
is	O
viewed	O
as	O
satisfactory	O
we	O
ll	O
denote	O
by	O
the	O
probability	B
that	O
the	O
source	O
string	O
is	O
one	O
of	O
the	O
confusable	O
so	O
a	O
lossy	B
compressor	O
has	O
a	O
probability	B
of	O
failure	O
if	O
can	O
be	O
made	O
very	O
small	O
then	O
a	O
lossy	B
compressor	O
may	O
be	O
practically	O
useful	O
a	O
lossless	B
compressor	O
maps	O
all	O
to	O
encodings	O
if	O
it	O
shortens	O
some	O
it	O
necessarily	O
makes	O
others	O
longer	O
we	O
try	O
to	O
design	O
the	O
compressor	O
so	O
that	O
the	O
probability	B
that	O
a	O
is	O
lengthened	O
is	O
very	O
small	O
and	O
the	O
probability	B
that	O
it	O
is	O
shortened	O
is	O
large	O
in	O
this	O
chapter	O
we	O
discuss	O
a	O
simple	O
lossy	B
compressor	O
in	O
subsequent	O
chapters	O
we	O
discuss	O
lossless	B
compression	B
methods	O
information	B
content	I
in	O
terms	O
of	O
lossy	B
compression	B
whichever	O
type	O
of	O
compressor	O
we	O
construct	O
we	O
need	O
somehow	O
to	O
take	O
into	O
account	O
the	O
probabilities	O
of	O
the	O
outcomes	O
imagine	O
comparing	O
the	O
information	B
contents	O
of	O
two	O
text	O
one	O
in	O
which	O
all	O
ascii	O
characters	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
information	B
content	I
in	O
terms	O
of	O
lossy	B
compression	B
are	O
used	O
with	O
equal	O
probability	B
and	O
one	O
in	O
which	O
the	O
characters	O
are	O
used	O
with	O
their	O
frequencies	O
in	O
english	B
text	O
can	O
we	O
a	O
measure	O
of	O
information	B
content	I
that	O
distinguishes	O
between	O
these	O
two	O
intuitively	O
the	O
latter	O
contains	O
less	O
information	B
per	O
character	O
because	O
it	O
is	O
more	O
predictable	O
one	O
simple	O
way	O
to	O
use	O
our	O
knowledge	O
that	O
some	O
symbols	O
have	O
a	O
smaller	O
probability	B
is	O
to	O
imagine	O
recoding	O
the	O
observations	O
into	O
a	O
smaller	O
alphabet	O
thus	O
losing	O
the	O
ability	O
to	O
encode	O
some	O
of	O
the	O
more	O
improbable	O
symbols	O
and	O
then	O
measuring	O
the	O
raw	O
bit	B
content	O
of	O
the	O
new	O
alphabet	O
for	O
example	O
we	O
might	O
take	O
a	O
risk	O
when	O
compressing	O
english	B
text	O
guessing	B
that	O
the	O
most	O
infrequent	O
characters	O
won	O
t	O
occur	O
and	O
make	O
a	O
reduced	O
ascii	O
code	O
that	O
omits	O
the	O
characters	O
f	O
g	O
thereby	O
reducing	O
the	O
size	O
of	O
the	O
alphabet	O
by	O
seventeen	O
the	O
larger	O
the	O
risk	O
we	O
are	O
willing	O
to	O
take	O
the	O
smaller	O
our	O
alphabet	O
becomes	O
we	O
introduce	O
a	O
parameter	O
that	O
describes	O
the	O
risk	O
we	O
are	O
taking	O
when	O
using	O
this	O
compression	B
method	O
is	O
the	O
probability	B
that	O
there	O
will	O
be	O
no	O
name	O
for	O
an	O
outcome	O
x	O
example	O
let	O
ax	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
g	O
g	O
and	O
px	O
the	O
raw	O
bit	B
content	O
of	O
this	O
ensemble	B
is	O
bits	O
corresponding	O
to	O
binary	O
names	O
but	O
notice	O
that	O
p	O
fa	O
b	O
c	O
dg	O
so	O
if	O
we	O
are	O
willing	O
to	O
run	O
a	O
risk	O
of	O
of	O
not	O
having	O
a	O
name	O
for	O
x	O
then	O
we	O
can	O
get	O
by	O
with	O
four	O
names	O
half	O
as	O
many	O
names	O
as	O
are	O
needed	O
if	O
every	O
x	O
ax	O
has	O
a	O
name	O
x	O
cx	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
cx	O
table	O
shows	O
binary	O
names	O
that	O
could	O
be	O
given	O
to	O
the	O
outcomes	O
in	O
the	O
cases	O
and	O
when	O
we	O
need	O
bits	O
to	O
encode	O
the	O
outcome	O
when	O
we	O
need	O
only	O
bits	O
table	O
binary	O
names	O
for	O
the	O
outcomes	O
for	O
two	O
failure	O
probabilities	O
let	O
us	O
now	O
formalize	O
this	O
idea	O
to	O
make	O
a	O
compression	B
strategy	O
with	O
risk	O
we	O
make	O
the	O
smallest	O
possible	O
subset	B
such	O
that	O
the	O
probability	B
that	O
x	O
is	O
not	O
in	O
is	O
less	O
than	O
or	O
equal	O
to	O
i	O
e	O
p	O
for	O
each	O
value	O
of	O
we	O
can	O
then	O
a	O
new	O
measure	O
of	O
information	B
content	I
the	O
log	O
of	O
the	O
size	O
of	O
this	O
smallest	O
subset	B
ensembles	O
in	O
which	O
several	O
elements	O
have	O
the	O
same	O
probability	B
there	O
may	O
be	O
several	O
smallest	O
subsets	O
that	O
contain	O
elements	O
but	O
all	O
that	O
matters	O
is	O
their	O
sizes	O
are	O
equal	O
so	O
we	O
will	O
not	O
dwell	O
on	O
this	O
ambiguity	O
the	O
smallest	O
subset	B
is	O
the	O
smallest	O
subset	B
of	O
ax	O
satisfying	O
p	O
the	O
subset	B
can	O
be	O
constructed	O
by	O
ranking	O
the	O
elements	O
of	O
ax	O
in	O
order	O
of	O
decreasing	O
probability	B
and	O
adding	O
successive	O
elements	O
starting	O
from	O
the	O
most	O
probable	O
elements	O
until	O
the	O
total	O
probability	B
is	O
we	O
can	O
make	O
a	O
data	O
compression	B
code	O
by	O
assigning	B
a	O
binary	O
name	O
to	O
each	O
element	O
of	O
the	O
smallest	O
subset	B
this	O
compression	B
scheme	O
motivates	O
the	O
following	O
measure	O
of	O
information	B
content	I
the	O
essential	O
bit	B
content	O
of	O
x	O
is	O
note	O
that	O
is	O
the	O
special	O
case	O
of	O
with	O
p	O
for	O
all	O
x	O
ax	O
do	O
not	O
confuse	O
and	O
with	O
the	O
function	O
displayed	O
in	O
figure	O
shows	O
for	O
the	O
ensemble	B
of	O
example	O
as	O
a	O
function	O
of	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
efgh	O
p	O
s	O
d	O
abc	O
the	O
source	B
coding	B
theorem	I
figure	O
the	O
outcomes	O
of	O
x	O
example	O
ranked	O
by	O
their	O
probability	B
the	O
essential	O
bit	B
content	O
the	O
labels	O
on	O
the	O
graph	B
show	O
the	O
smallest	O
set	B
as	O
a	O
function	O
of	O
note	O
bits	O
and	O
bits	O
extended	B
ensembles	O
is	O
this	O
compression	B
method	O
any	O
more	O
useful	O
if	O
we	O
compress	B
blocks	O
of	O
symbols	O
from	O
a	O
source	O
we	O
now	O
turn	O
to	O
examples	O
where	O
the	O
outcome	O
x	O
xn	O
is	O
a	O
string	O
of	O
n	O
independent	O
identically	O
distributed	O
random	B
variables	O
from	O
a	O
single	O
ensemble	B
x	O
we	O
will	O
denote	O
by	O
x	O
n	O
the	O
ensemble	B
xn	O
remember	O
that	O
entropy	B
is	O
additive	O
for	O
independent	O
variables	O
so	O
hx	O
n	O
n	O
hx	O
example	O
consider	O
a	O
string	O
of	O
n	O
of	O
a	O
bent	B
coin	B
x	O
xn	O
where	O
xn	O
with	O
probabilities	O
the	O
most	O
probable	O
strings	O
x	O
are	O
those	O
with	O
most	O
if	O
rx	O
is	O
the	O
number	O
of	O
in	O
x	O
then	O
prx	O
p	O
to	O
evaluate	O
n	O
we	O
must	O
the	O
smallest	O
subset	B
this	O
subset	B
will	O
contain	O
all	O
x	O
with	O
rx	O
up	O
to	O
some	O
and	O
some	O
of	O
the	O
x	O
with	O
rx	O
figures	O
and	O
show	O
graphs	O
of	O
n	O
against	O
for	O
the	O
cases	O
n	O
and	O
n	O
the	O
steps	O
are	O
the	O
values	O
of	O
at	O
which	O
changes	O
by	O
and	O
the	O
cusps	O
where	O
the	O
slope	O
of	O
the	O
staircase	B
changes	O
are	O
the	O
points	O
where	O
rmax	O
changes	O
by	O
exercise	O
what	O
are	O
the	O
mathematical	O
shapes	O
of	O
the	O
curves	O
between	O
the	O
cusps	O
for	O
the	O
examples	O
shown	O
in	O
n	O
depends	O
strongly	O
on	O
the	O
value	O
of	O
so	O
it	O
might	O
not	O
seem	O
a	O
fundamental	O
or	O
useful	O
of	O
information	B
content	I
but	O
we	O
will	O
consider	O
what	O
happens	O
as	O
n	O
the	O
number	O
of	O
independent	O
variables	O
in	O
x	O
n	O
increases	O
we	O
will	O
the	O
remarkable	O
result	O
that	O
n	O
becomes	O
almost	O
independent	O
of	O
and	O
for	O
all	O
it	O
is	O
very	O
close	O
to	O
n	O
hx	O
where	O
hx	O
is	O
the	O
entropy	B
of	O
one	O
of	O
the	O
random	B
variables	O
figure	O
illustrates	O
this	O
asymptotic	O
tendency	O
for	O
the	O
binary	O
ensemble	B
of	O
n	O
n	O
becomes	O
an	O
increasingly	O
function	O
example	O
as	O
n	O
increases	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
information	B
content	I
in	O
terms	O
of	O
lossy	B
compression	B
p	O
figure	O
the	O
sixteen	O
outcomes	O
of	O
the	O
ensemble	B
x	O
with	O
ranked	O
by	O
probability	B
the	O
essential	O
bit	B
content	O
the	O
upper	O
schematic	O
diagram	O
indicates	O
the	O
strings	O
probabilities	O
by	O
the	O
vertical	O
lines	O
lengths	O
to	O
scale	O
figure	O
n	O
for	O
n	O
binary	O
variables	O
with	O
n	O
n	O
n	O
n	O
for	O
figure	O
n	O
binary	O
variables	O
with	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
figure	O
the	O
top	O
strings	O
are	O
samples	O
from	O
x	O
where	O
and	O
the	O
bottom	O
two	O
are	O
the	O
most	O
and	O
least	O
probable	O
strings	O
in	O
this	O
ensemble	B
the	O
column	O
shows	O
the	O
log-probabilities	O
of	O
the	O
random	B
strings	O
which	O
may	O
be	O
compared	O
with	O
the	O
entropy	B
hx	O
bits	O
x	O
except	O
for	O
tails	O
close	O
to	O
and	O
as	O
long	O
as	O
we	O
are	O
allowed	O
a	O
tiny	O
probability	B
of	I
error	I
compression	B
down	O
to	O
n	O
h	O
bits	O
is	O
possible	O
even	O
if	O
we	O
are	O
allowed	O
a	O
large	O
probability	B
of	I
error	I
we	O
still	O
can	O
compress	B
only	O
down	O
to	O
n	O
h	O
bits	O
this	O
is	O
the	O
source	B
coding	B
theorem	I
theorem	O
shannon	B
s	O
source	B
coding	B
theorem	I
let	O
x	O
be	O
an	O
ensemble	B
with	O
entropy	B
hx	O
h	O
bits	O
given	O
and	O
there	O
exists	O
a	O
positive	O
integer	O
such	O
that	O
for	O
n	O
n	O
n	O
typicality	B
why	O
does	O
increasing	O
n	O
help	O
let	O
s	O
examine	O
long	O
strings	O
from	O
x	O
n	O
table	O
shows	O
samples	O
from	O
x	O
n	O
for	O
n	O
and	O
the	O
probability	B
of	O
a	O
string	O
x	O
that	O
contains	O
r	O
and	O
is	O
p	O
pr	O
the	O
number	O
of	O
strings	O
that	O
contain	O
r	O
is	O
nr	O
so	O
the	O
number	O
of	O
r	O
has	O
a	O
binomial	B
distribution	B
p	O
these	O
functions	B
are	O
shown	O
in	O
the	O
mean	B
of	O
r	O
is	O
n	O
and	O
its	O
standard	B
deviation	I
is	O
pn	O
if	O
n	O
is	O
then	O
r	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
typicality	B
nr	O
p	O
pr	O
p	O
nrp	O
n	O
n	O
t	O
t	O
r	O
r	O
figure	O
anatomy	O
of	O
the	O
typical	B
set	B
t	O
for	O
and	O
n	O
and	O
n	O
these	O
graphs	O
show	O
nr	O
the	O
number	O
of	O
strings	O
containing	O
r	O
the	O
probability	B
p	O
of	O
a	O
single	O
string	O
that	O
contains	O
r	O
the	O
same	O
probability	B
on	O
a	O
log	O
scale	O
and	O
the	O
total	O
probability	B
nrp	O
of	O
all	O
strings	O
that	O
contain	O
r	O
the	O
number	O
r	O
is	O
on	O
the	O
horizontal	O
axis	O
the	O
plot	O
of	O
p	O
also	O
shows	O
by	O
a	O
dotted	O
line	O
the	O
mean	B
value	O
of	O
p	O
which	O
equals	O
when	O
n	O
and	O
when	O
n	O
the	O
typical	B
set	B
includes	O
only	O
the	O
strings	O
that	O
have	O
p	O
close	O
to	O
this	O
value	O
the	O
range	O
marked	O
t	O
shows	O
the	O
set	B
tn	O
in	O
section	B
for	O
n	O
and	O
and	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
if	O
n	O
then	O
the	O
source	B
coding	B
theorem	I
r	O
notice	O
that	O
as	O
n	O
gets	O
bigger	O
the	O
probability	B
distribution	B
of	O
r	O
becomes	O
more	O
concentrated	O
in	O
the	O
sense	O
that	O
while	O
the	O
range	O
of	O
possible	O
values	O
of	O
r	O
grows	O
as	O
n	O
the	O
standard	B
deviation	I
of	O
r	O
grows	O
only	O
as	O
pn	O
that	O
r	O
is	O
most	O
likely	O
to	O
fall	O
in	O
a	O
small	O
range	O
of	O
values	O
implies	O
that	O
the	O
outcome	O
x	O
is	O
also	O
most	O
likely	O
to	O
fall	O
in	O
a	O
corresponding	O
small	O
subset	B
of	O
outcomes	O
that	O
we	O
will	O
call	O
the	O
typical	B
set	B
of	O
the	O
typical	B
set	B
let	O
us	O
typicality	B
for	O
an	O
arbitrary	O
ensemble	B
x	O
with	O
alphabet	O
ax	O
our	O
of	O
a	O
typical	B
string	O
will	O
involve	O
the	O
string	O
s	O
probability	B
a	O
long	O
string	O
of	O
n	O
symbols	O
will	O
usually	O
contain	O
about	O
occurrences	O
of	O
the	O
symbol	O
occurrences	O
of	O
the	O
second	O
etc	O
hence	O
the	O
probability	B
of	O
this	O
string	O
is	O
roughly	O
p	O
p	O
p	O
ppi	O
n	O
i	O
so	O
that	O
the	O
information	B
content	I
of	O
a	O
typical	B
string	O
is	O
p	O
nxi	O
pi	O
pi	O
n	O
h	O
which	O
is	O
the	O
information	B
content	I
of	O
x	O
is	O
so	O
the	O
random	B
variable	I
very	O
likely	O
to	O
be	O
close	O
in	O
value	O
to	O
n	O
h	O
we	O
build	O
our	O
of	O
typicality	B
on	O
this	O
observation	O
we	O
the	O
typical	B
elements	O
of	O
an	O
x	O
to	O
be	O
those	O
elements	O
that	O
have	O
probability	B
close	O
to	O
h	O
that	O
the	O
typical	B
set	B
unlike	O
the	O
smallest	O
subset	B
does	O
not	O
include	O
the	O
most	O
probable	O
elements	O
of	O
an	O
x	O
but	O
we	O
will	O
show	O
that	O
these	O
most	O
probable	O
elements	O
contribute	O
negligible	O
probability	B
we	O
introduce	O
a	O
parameter	O
that	O
how	O
close	O
the	O
probability	B
has	O
to	O
be	O
to	O
h	O
for	O
an	O
element	O
to	O
be	O
typical	B
we	O
call	O
the	O
set	B
of	O
typical	B
elements	O
the	O
typical	B
set	B
tn	O
x	O
tn	O
an	O
n	O
p	O
we	O
will	O
show	O
that	O
whatever	O
value	O
of	O
we	O
choose	O
the	O
typical	B
set	B
contains	O
almost	O
all	O
the	O
probability	B
as	O
n	O
increases	O
this	O
important	O
result	O
is	O
sometimes	O
called	O
the	O
asymptotic	B
equipartition	B
principle	O
asymptotic	B
equipartition	B
principle	O
for	O
an	O
ensemble	B
of	O
n	O
independent	O
identically	O
distributed	O
random	B
variables	O
x	O
n	O
xn	O
with	O
n	O
large	O
the	O
outcome	O
x	O
xn	O
is	O
almost	O
certain	O
to	O
belong	O
to	O
a	O
subset	B
of	O
an	O
x	O
having	O
only	O
hx	O
members	O
each	O
having	O
probability	B
close	O
to	O
hx	O
notice	O
that	O
if	O
hx	O
then	O
hx	O
is	O
a	O
tiny	O
fraction	O
of	O
the	O
number	O
of	O
possible	O
outcomes	O
jan	O
xj	O
jaxjn	O
the	O
term	O
equipartition	B
is	O
chosen	O
to	O
describe	O
the	O
idea	O
that	O
the	O
members	O
of	O
the	O
typical	B
set	B
have	O
roughly	O
equal	O
probability	B
should	O
not	O
be	O
taken	O
too	O
literally	O
hence	O
my	O
use	O
of	O
quotes	O
around	O
asymptotic	B
equipartition	B
see	O
page	O
a	O
second	O
meaning	O
for	O
equipartition	B
in	O
thermal	O
physics	B
is	O
the	O
idea	O
that	O
each	O
degree	B
of	O
freedom	O
of	O
a	O
classical	O
system	O
has	O
equal	O
average	O
energy	B
kt	O
this	O
second	O
meaning	O
is	O
not	O
intended	O
here	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
proofs	O
p	O
hx	O
tn	O
figure	O
schematic	O
diagram	O
showing	O
all	O
strings	O
in	O
the	O
ensemble	B
x	O
n	O
ranked	O
by	O
their	O
probability	B
and	O
the	O
typical	B
set	B
tn	O
the	O
asymptotic	B
equipartition	B
principle	O
is	O
equivalent	O
to	O
shannon	B
s	O
source	B
coding	B
theorem	I
statement	O
n	O
i	O
i	O
d	O
random	B
variables	O
each	O
with	O
entropy	B
hx	O
can	O
be	O
compressed	O
into	O
more	O
than	O
n	O
hx	O
bits	O
with	O
negligible	O
risk	O
of	O
information	B
loss	O
as	O
n	O
conversely	O
if	O
they	O
are	O
compressed	O
into	O
fewer	O
than	O
n	O
hx	O
bits	O
it	O
is	O
virtually	O
certain	O
that	O
information	B
will	O
be	O
lost	O
these	O
two	O
theorems	O
are	O
equivalent	O
because	O
we	O
can	O
a	O
compression	B
algorithm	O
that	O
gives	O
a	O
distinct	O
name	O
of	O
length	O
n	O
hx	O
bits	O
to	O
each	O
x	O
in	O
the	O
typical	B
set	B
proofs	O
this	O
section	B
may	O
be	O
skipped	O
if	O
found	O
tough	O
going	O
the	O
law	B
of	I
large	I
numbers	I
our	O
proof	O
of	O
the	O
source	B
coding	B
theorem	I
uses	O
the	O
law	B
of	I
large	I
numbers	I
mean	B
and	O
variance	B
of	O
a	O
real	O
random	B
variable	I
are	O
eu	O
pu	O
p	O
u	O
eu	O
p	O
and	O
varu	O
technical	O
note	O
strictly	O
i	O
am	O
assuming	O
here	O
that	O
u	O
is	O
a	O
function	O
ux	O
of	O
a	O
sample	B
x	O
from	O
a	O
discrete	O
ensemble	B
x	O
then	O
the	O
summations	O
pu	O
p	O
should	O
be	O
written	O
px	O
p	O
this	O
means	O
that	O
p	O
is	O
a	O
sum	O
of	O
delta	O
functions	B
this	O
restriction	O
guarantees	O
that	O
the	O
mean	B
and	O
variance	B
of	O
u	O
do	O
exist	O
which	O
is	O
not	O
necessarily	O
the	O
case	O
for	O
general	O
p	O
chebyshev	O
s	O
inequality	B
let	O
t	O
be	O
a	O
non-negative	O
real	O
random	B
variable	I
and	O
let	O
be	O
a	O
positive	O
real	O
number	O
then	O
p	O
proof	O
p	O
p	O
we	O
multiply	O
each	O
term	O
by	O
and	O
obtain	O
p	O
p	O
we	O
add	O
the	O
missing	O
terms	O
and	O
obtain	O
p	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
chebyshev	O
s	O
inequality	B
let	O
x	O
be	O
a	O
random	B
variable	I
and	O
let	O
be	O
a	O
positive	O
real	O
number	O
then	O
proof	O
take	O
t	O
and	O
apply	O
the	O
previous	O
proposition	O
weak	O
law	B
of	I
large	I
numbers	I
take	O
x	O
to	O
be	O
the	O
average	O
of	O
n	O
independent	O
random	B
variables	O
hn	O
having	O
common	O
mean	B
and	O
common	O
variance	B
hn	O
then	O
h	O
x	O
n	O
pn	O
p	O
proof	O
obtained	O
by	O
showing	O
that	O
and	O
that	O
x	O
hn	O
we	O
are	O
interested	O
in	O
x	O
being	O
very	O
close	O
to	O
the	O
mean	B
very	O
small	O
no	O
matter	O
how	O
large	O
h	O
is	O
and	O
no	O
matter	O
how	O
small	O
the	O
required	O
is	O
and	O
no	O
matter	O
how	O
small	O
the	O
desired	O
probability	B
that	O
we	O
can	O
always	O
achieve	O
it	O
by	O
taking	O
n	O
large	O
enough	O
proof	O
of	O
theorem	O
we	O
apply	O
the	O
law	B
of	I
large	I
numbers	I
to	O
the	O
random	B
variable	I
p	O
for	O
x	O
drawn	O
from	O
the	O
ensemble	B
x	O
n	O
this	O
random	B
variable	I
can	O
be	O
written	O
as	O
the	O
average	O
of	O
n	O
information	B
contents	O
hn	O
each	O
of	O
which	O
is	O
a	O
random	B
variable	I
with	O
mean	B
h	O
hx	O
and	O
variance	B
term	O
hn	O
is	O
the	O
shannon	B
information	B
content	I
of	O
the	O
nth	O
outcome	O
n	O
we	O
again	O
the	O
typical	B
set	B
with	O
parameters	B
n	O
and	O
thus	O
tn	O
an	O
x	O
n	O
p	O
for	O
all	O
x	O
tn	O
the	O
probability	B
of	O
x	O
p	O
and	O
by	O
the	O
law	B
of	I
large	I
numbers	I
p	O
tn	O
we	O
have	O
thus	O
proved	O
the	O
asymptotic	B
equipartition	B
principle	O
as	O
n	O
increases	O
the	O
probability	B
that	O
x	O
falls	O
in	O
tn	O
approaches	O
for	O
any	O
how	O
does	O
this	O
result	O
relate	O
to	O
source	O
coding	O
we	O
must	O
relate	O
tn	O
to	O
n	O
we	O
will	O
show	O
that	O
for	O
any	O
given	O
there	O
is	O
a	O
big	O
n	O
such	O
that	O
n	O
n	O
h	O
part	O
n	O
n	O
h	O
the	O
set	B
tn	O
is	O
not	O
the	O
best	O
subset	B
for	B
compression	B
so	O
the	O
size	O
of	O
tn	O
gives	O
an	O
upper	O
bound	B
on	O
we	O
show	O
how	O
small	O
n	O
must	O
be	O
by	O
calculating	O
how	O
big	O
tn	O
could	O
possibly	O
be	O
we	O
are	O
free	O
to	O
set	B
to	O
any	O
convenient	O
value	O
the	O
smallest	O
possible	O
probability	B
that	O
a	O
member	O
of	O
tn	O
can	O
have	O
is	O
and	O
the	O
total	O
probability	B
contained	O
by	O
tn	O
can	O
t	O
be	O
any	O
bigger	O
than	O
so	O
jtn	O
that	O
is	O
the	O
size	O
of	O
the	O
typical	B
set	B
is	O
bounded	O
by	O
jtn	O
if	O
we	O
set	B
and	O
such	O
that	O
tn	O
becomes	O
a	O
witness	O
to	O
the	O
fact	O
that	O
n	O
jtn	O
n	O
then	O
p	O
and	O
the	O
set	B
n	O
n	O
h	O
h	O
h	O
figure	O
schematic	O
illustration	O
of	O
the	O
two	O
parts	O
of	O
the	O
theorem	O
given	O
any	O
and	O
we	O
show	O
that	O
for	O
large	O
enough	O
n	O
n	O
n	O
lies	O
below	O
the	O
line	O
h	O
and	O
above	O
the	O
line	O
h	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
cco	O
c	O
tn	O
tn	O
comments	O
part	O
n	O
n	O
h	O
imagine	O
that	O
someone	O
claims	O
this	O
second	O
part	O
is	O
not	O
so	O
that	O
for	O
any	O
n	O
the	O
smallest	O
subset	B
is	O
smaller	O
than	O
the	O
above	O
inequality	B
would	O
allow	O
we	O
can	O
make	O
use	O
of	O
our	O
typical	B
set	B
to	O
show	O
that	O
they	O
must	O
be	O
mistaken	O
remember	O
that	O
we	O
are	O
free	O
to	O
set	B
to	O
any	O
value	O
we	O
choose	O
we	O
will	O
set	B
so	O
that	O
our	O
task	O
is	O
to	O
prove	O
that	O
a	O
subset	B
having	O
and	O
achieving	O
p	O
cannot	O
exist	O
n	O
greater	O
than	O
an	O
that	O
we	O
will	O
specify	O
so	O
let	O
us	O
consider	O
the	O
probability	B
of	O
falling	O
in	O
this	O
rival	O
smaller	O
subset	B
the	O
probability	B
of	O
the	O
subset	B
is	O
p	O
p	O
p	O
where	O
tn	O
denotes	O
the	O
complement	O
fx	O
tn	O
the	O
maximum	O
value	O
of	O
the	O
term	O
is	O
found	O
if	O
tn	O
contains	O
outcomes	O
all	O
with	O
the	O
maximum	O
probability	B
the	O
maximum	O
value	O
the	O
second	O
term	O
can	O
have	O
is	O
p	O
tn	O
so	O
p	O
tn	O
we	O
can	O
now	O
set	B
and	O
such	O
that	O
p	O
which	O
shows	O
that	O
cannot	O
satisfy	O
the	O
of	O
a	O
subset	B
thus	O
any	O
subset	B
with	O
size	O
has	O
probability	B
less	O
than	O
so	O
by	O
the	O
of	O
n	O
n	O
n	O
n	O
is	O
essentially	O
a	O
constant	O
function	O
of	O
for	O
as	O
illustrated	O
in	O
and	O
thus	O
for	O
large	O
enough	O
n	O
the	O
function	O
comments	O
the	O
source	B
coding	B
theorem	I
has	O
two	O
parts	O
n	O
n	O
h	O
both	O
results	O
are	O
interesting	O
the	O
part	O
tells	O
us	O
that	O
even	O
if	O
the	O
probability	B
of	I
error	I
is	O
extremely	O
small	O
the	O
number	O
of	O
bits	O
per	O
symbol	O
n	O
n	O
needed	O
to	O
specify	O
a	O
long	O
n	O
string	O
x	O
with	O
vanishingly	O
small	O
error	B
probability	B
does	O
not	O
have	O
to	O
exceed	O
h	O
bits	O
we	O
need	O
to	O
have	O
only	O
a	O
tiny	O
tolerance	O
for	O
error	O
and	O
the	O
number	O
of	O
bits	O
required	O
drops	O
from	O
to	O
n	O
n	O
h	O
and	O
what	O
happens	O
if	O
we	O
are	O
yet	O
more	O
tolerant	O
to	O
compression	B
errors	B
part	O
tells	O
us	O
that	O
even	O
if	O
is	O
very	O
close	O
to	O
so	O
that	O
errors	B
are	O
made	O
most	O
of	O
the	O
time	O
the	O
average	O
number	O
of	O
bits	O
per	O
symbol	O
needed	O
to	O
specify	O
x	O
must	O
still	O
be	O
at	O
least	O
h	O
bits	O
these	O
two	O
extremes	O
tell	O
us	O
that	O
regardless	O
of	O
our	O
allowance	O
for	O
error	O
the	O
number	O
of	O
bits	O
per	O
symbol	O
needed	O
to	O
specify	O
x	O
is	O
h	O
bits	O
no	O
more	O
and	O
no	O
less	O
caveat	O
regarding	O
asymptotic	B
equipartition	B
i	O
put	O
the	O
words	O
asymptotic	B
equipartition	B
in	O
quotes	O
because	O
it	O
is	O
important	O
not	O
to	O
think	O
that	O
the	O
elements	O
of	O
the	O
typical	B
set	B
tn	O
really	O
do	O
have	O
roughly	O
the	O
same	O
probability	B
as	O
each	O
other	O
they	O
are	O
similar	O
in	O
probability	B
only	O
in	O
p	O
are	O
within	O
of	O
each	O
other	O
now	O
as	O
the	O
sense	O
that	O
their	O
values	O
of	O
is	O
decreased	O
how	O
does	O
n	O
have	O
to	O
increase	O
if	O
we	O
are	O
to	O
keep	O
our	O
bound	B
on	O
the	O
mass	O
of	O
the	O
typical	B
set	B
p	O
tn	O
constant	O
n	O
must	O
grow	O
as	O
so	O
if	O
we	O
write	O
in	O
terms	O
of	O
n	O
as	O
for	O
some	O
constant	O
then	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
the	O
most	O
probable	O
string	O
in	O
the	O
typical	B
set	B
will	O
be	O
of	O
order	O
times	O
greater	O
than	O
the	O
least	O
probable	O
string	O
in	O
the	O
typical	B
set	B
as	O
decreases	O
n	O
increases	O
and	O
this	O
ratio	O
grows	O
exponentially	O
thus	O
we	O
have	O
equipartition	B
only	O
in	O
a	O
weak	O
sense	O
why	O
did	O
we	O
introduce	O
the	O
typical	B
set	B
the	O
best	O
choice	O
of	O
subset	B
for	O
block	B
compression	B
is	O
not	O
a	O
typical	B
set	B
so	O
why	O
did	O
we	O
bother	O
introducing	O
the	O
typical	B
set	B
the	O
answer	O
is	O
we	O
can	O
count	O
the	O
typical	B
set	B
we	O
know	O
that	O
all	O
its	O
elements	O
have	O
almost	O
identical	O
probability	B
h	O
and	O
we	O
know	O
the	O
whole	O
set	B
has	O
probability	B
almost	O
so	O
the	O
typical	B
set	B
must	O
have	O
roughly	O
h	O
elements	O
without	O
the	O
help	O
of	O
the	O
typical	B
set	B
is	O
very	O
similar	O
to	O
it	O
would	O
have	O
been	O
hard	O
to	O
count	O
how	O
many	O
elements	O
there	O
are	O
in	O
exercises	O
weighing	O
problems	O
exercise	O
while	O
some	O
people	O
when	O
they	O
encounter	O
the	O
weighing	B
problem	I
with	O
balls	O
and	O
the	O
three-outcome	O
balance	B
think	O
that	O
weighing	O
six	B
balls	O
against	O
six	B
balls	O
is	O
a	O
good	B
weighing	O
others	O
say	O
no	O
weighing	O
six	B
against	O
six	B
conveys	O
no	O
information	B
at	O
all	O
explain	O
to	O
the	O
second	O
group	O
why	O
they	O
are	O
both	O
right	O
and	O
wrong	O
compute	O
the	O
information	B
gained	O
about	O
which	O
is	O
the	O
odd	O
ball	O
and	O
the	O
information	B
gained	O
about	O
which	O
is	O
the	O
odd	O
ball	O
and	O
whether	O
it	O
is	O
heavy	O
or	O
light	O
exercise	O
solve	O
the	O
weighing	B
problem	I
for	O
the	O
case	O
where	O
there	O
are	O
balls	O
of	O
which	O
one	O
is	O
known	O
to	O
be	O
odd	O
exercise	O
you	O
are	O
given	O
balls	O
all	O
of	O
which	O
are	O
equal	O
in	O
weight	O
except	O
for	O
one	O
that	O
is	O
either	O
heavier	O
or	O
lighter	O
you	O
are	O
also	O
given	O
a	O
bizarre	O
twopan	O
balance	B
that	O
can	O
report	O
only	O
two	O
outcomes	O
the	O
two	O
sides	O
balance	B
or	O
the	O
two	O
sides	O
do	O
not	O
balance	B
design	O
a	O
strategy	O
to	O
determine	O
which	O
is	O
the	O
odd	O
ball	O
in	O
as	O
few	O
uses	O
of	O
the	O
balance	B
as	O
possible	O
exercise	O
you	O
have	O
a	O
two-pan	O
balance	B
your	O
job	O
is	O
to	O
weigh	O
out	O
bags	O
of	O
with	O
integer	O
weights	O
to	O
pounds	O
inclusive	O
how	O
many	O
weights	O
do	O
you	O
need	O
are	O
allowed	O
to	O
put	O
weights	O
on	O
either	O
pan	O
you	O
re	O
only	O
allowed	O
to	O
put	O
one	O
bag	O
on	O
the	O
balance	B
at	O
a	O
time	O
exercise	O
is	O
it	O
possible	O
to	O
solve	O
exercise	O
weighing	B
problem	I
with	O
balls	O
and	O
the	O
three-outcome	O
balance	B
using	O
a	O
sequence	B
of	O
three	O
weighings	O
such	O
that	O
the	O
balls	O
chosen	O
for	O
the	O
second	O
weighing	O
do	O
not	O
depend	O
on	O
the	O
outcome	O
of	O
the	O
and	O
the	O
third	O
weighing	O
does	O
not	O
depend	O
on	O
the	O
or	O
second	O
find	O
a	O
solution	O
to	O
the	O
general	O
n	O
weighing	B
problem	I
in	O
which	O
exactly	O
one	O
of	O
n	O
balls	O
is	O
odd	O
show	O
that	O
in	O
w	O
weighings	O
an	O
odd	O
ball	O
can	O
be	O
from	O
among	O
n	O
balls	O
exercise	O
you	O
are	O
given	O
balls	O
and	O
the	O
three-outcome	O
balance	B
of	O
exercise	O
this	O
time	O
two	O
of	O
the	O
balls	O
are	O
odd	O
each	O
odd	O
ball	O
may	O
be	O
heavy	O
or	O
light	O
and	O
we	O
don	O
t	O
know	O
which	O
we	O
want	O
to	O
identify	O
the	O
odd	O
balls	O
and	O
in	O
which	O
direction	O
they	O
are	O
odd	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
estimate	O
how	O
many	O
weighings	O
are	O
required	O
by	O
the	O
optimal	B
strategy	O
and	O
what	O
if	O
there	O
are	O
three	O
odd	O
balls	O
how	O
do	O
your	O
answers	O
change	O
if	O
it	O
is	O
known	O
that	O
all	O
the	O
regular	B
balls	O
weigh	O
g	O
that	O
light	O
balls	O
weigh	O
g	O
and	O
heavy	O
ones	O
weigh	O
g	O
source	O
coding	O
with	O
a	O
lossy	B
compressor	O
with	O
loss	O
exercise	O
let	O
px	O
sketch	O
for	O
n	O
and	O
n	O
n	O
as	O
a	O
function	O
of	O
exercise	O
let	O
py	O
sketch	O
n	O
and	O
n	O
n	O
as	O
a	O
function	O
of	O
for	O
exercise	O
physics	B
students	O
discuss	O
the	O
relationship	O
between	O
the	O
proof	O
of	O
the	O
asymptotic	B
equipartition	B
principle	O
and	O
the	O
equivalence	B
large	O
systems	O
of	O
the	O
boltzmann	B
entropy	B
and	O
the	O
gibbs	B
entropy	B
distributions	O
that	O
don	O
t	O
obey	O
the	O
law	B
of	I
large	I
numbers	I
the	O
law	B
of	I
large	I
numbers	I
which	O
we	O
used	O
in	O
this	O
chapter	O
shows	O
that	O
the	O
mean	B
of	O
a	O
set	B
of	O
n	O
i	O
i	O
d	O
random	B
variables	O
has	O
a	O
probability	B
distribution	B
that	O
becomes	O
narrower	O
with	O
width	O
as	O
n	O
increases	O
however	O
we	O
have	O
proved	O
this	O
property	O
only	O
for	O
discrete	O
random	B
variables	O
that	O
is	O
for	O
real	O
numbers	O
taking	O
on	O
a	O
set	B
of	O
possible	O
values	O
while	O
many	O
random	B
variables	O
with	O
continuous	B
probability	B
distributions	O
also	O
satisfy	O
the	O
law	B
of	I
large	I
numbers	I
there	O
are	O
important	O
distributions	O
that	O
do	O
not	O
some	O
continuous	B
distributions	O
do	O
not	O
have	O
a	O
mean	B
or	O
variance	B
exercise	O
sketch	O
the	O
cauchy	B
distribution	B
p	O
z	O
x	O
what	O
is	O
its	O
normalizing	O
constant	O
z	O
can	O
you	O
evaluate	O
its	O
mean	B
or	O
variance	B
consider	O
the	O
sum	O
z	O
where	O
and	O
are	O
independent	O
random	B
variables	O
from	O
a	O
cauchy	B
distribution	B
what	O
is	O
p	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
mean	B
of	O
and	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
mean	B
of	O
n	O
samples	O
from	O
this	O
cauchy	B
distribution	B
other	O
asymptotic	O
properties	O
exercise	O
bound	B
we	O
derived	O
the	O
weak	O
law	B
of	I
large	I
numbers	I
from	O
chebyshev	O
s	O
inequality	B
by	O
letting	O
the	O
random	B
variable	I
t	O
in	O
the	O
inequality	B
p	O
be	O
a	O
function	O
t	O
of	O
the	O
random	B
variable	I
x	O
we	O
were	O
interested	O
in	O
other	O
useful	O
inequalities	O
can	O
be	O
obtained	O
by	O
using	O
other	O
functions	B
the	O
bound	B
which	O
is	O
useful	O
for	O
bounding	O
the	O
tails	O
of	O
a	O
distribution	B
is	O
obtained	O
by	O
letting	O
t	O
expsx	O
show	O
that	O
and	O
p	O
a	O
for	O
any	O
s	O
p	O
a	O
for	O
any	O
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
where	O
gs	O
is	O
the	O
moment-generating	O
function	O
of	O
x	O
gs	O
p	O
esx	O
curious	O
functions	B
related	O
to	O
p	O
log	O
exercise	O
this	O
exercise	O
has	O
no	O
purpose	O
at	O
all	O
it	O
s	O
included	O
for	O
the	O
enjoyment	O
of	O
those	O
who	O
like	O
mathematical	O
curiosities	O
sketch	O
the	O
function	O
f	O
for	O
x	O
hint	O
work	O
out	O
the	O
inverse	O
function	O
to	O
f	O
that	O
is	O
the	O
function	O
gy	O
such	O
that	O
if	O
x	O
gy	O
then	O
y	O
f	O
it	O
s	O
closely	O
related	O
to	O
p	O
log	O
solutions	O
solution	O
to	O
exercise	O
let	O
p	O
y	O
p	O
then	O
hx	O
y	O
xxy	O
xxy	O
xx	O
hx	O
hy	O
p	O
log	O
p	O
p	O
log	O
p	O
p	O
log	O
p	O
p	O
log	O
p	O
p	O
log	O
p	O
solution	O
to	O
exercise	O
an	O
ascii	O
can	O
be	O
reduced	O
in	O
size	O
by	O
a	O
factor	O
of	O
this	O
reduction	O
could	O
be	O
achieved	O
by	O
a	O
block	B
code	I
that	O
maps	O
blocks	O
into	O
blocks	O
by	O
copying	O
the	O
information-carrying	O
bits	O
into	O
bytes	O
and	O
ignoring	O
the	O
last	O
bit	B
of	O
every	O
character	O
solution	O
to	O
exercise	O
the	O
pigeon-hole	B
principle	I
states	O
you	O
can	O
t	O
put	O
pigeons	O
into	O
holes	O
without	O
using	O
one	O
of	O
the	O
holes	O
twice	O
similarly	O
you	O
can	O
t	O
give	O
ax	O
outcomes	O
unique	O
binary	O
names	O
of	O
some	O
length	O
l	O
shorter	O
than	O
jaxj	O
bits	O
because	O
there	O
are	O
only	O
such	O
binary	O
names	O
and	O
l	O
jaxj	O
implies	O
jaxj	O
so	O
at	O
least	O
two	O
inputs	O
to	O
the	O
compressor	O
would	O
compress	B
to	O
the	O
same	O
output	O
solution	O
to	O
exercise	O
between	O
the	O
cusps	O
all	O
the	O
changes	O
in	O
probability	B
are	O
equal	O
and	O
the	O
number	O
of	O
elements	O
in	O
t	O
changes	O
by	O
one	O
at	O
each	O
step	O
so	O
varies	O
logarithmically	O
with	O
solution	O
to	O
exercise	O
this	O
solution	O
was	O
found	O
by	O
dyson	O
and	O
lyness	O
in	O
and	O
presented	O
in	O
the	O
following	O
elegant	O
form	O
by	O
john	O
conway	B
in	O
be	O
warned	O
the	O
symbols	O
a	O
b	O
and	O
c	O
are	O
used	O
to	O
name	O
the	O
balls	O
to	O
name	O
the	O
pans	O
of	O
the	O
balance	B
to	O
name	O
the	O
outcomes	O
and	O
to	O
name	O
the	O
possible	O
states	O
of	O
the	O
odd	O
ball	O
label	O
the	O
balls	O
by	O
the	O
sequences	O
aab	O
aba	O
abb	O
abc	O
bbc	O
bca	O
bcb	O
bcc	O
caa	O
cab	O
cac	O
cca	O
and	O
in	O
the	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
weighings	O
put	O
aab	O
caa	O
cab	O
cac	O
in	O
pan	O
a	O
aba	O
abb	O
abc	O
bbc	O
in	O
pan	O
b	O
aab	O
aba	O
abb	O
abc	O
bbc	O
bca	O
bcb	O
bcc	O
aba	O
bca	O
caa	O
cca	O
aab	O
abb	O
bcb	O
cab	O
now	O
in	O
a	O
given	O
weighing	O
a	O
pan	O
will	O
either	O
end	O
up	O
in	O
the	O
canonical	B
position	O
that	O
it	O
assumes	O
when	O
the	O
pans	O
are	O
balanced	O
or	O
above	O
that	O
position	O
or	O
below	O
it	O
so	O
the	O
three	O
weighings	O
determine	O
for	O
each	O
pan	O
a	O
sequence	B
of	O
three	O
of	O
these	O
letters	O
if	O
both	O
sequences	O
are	O
ccc	O
then	O
there	O
s	O
no	O
odd	O
ball	O
otherwise	O
for	O
just	O
one	O
of	O
the	O
two	O
pans	O
the	O
sequence	B
is	O
among	O
the	O
above	O
and	O
names	O
the	O
odd	O
ball	O
whose	O
weight	O
is	O
above	O
or	O
below	O
the	O
proper	B
one	O
according	O
as	O
the	O
pan	O
is	O
a	O
or	O
b	O
in	O
w	O
weighings	O
the	O
odd	O
ball	O
can	O
be	O
from	O
among	O
n	O
balls	O
in	O
the	O
same	O
way	O
by	O
labelling	O
them	O
with	O
all	O
the	O
non-constant	O
sequences	O
of	O
w	O
letters	O
from	O
a	O
b	O
c	O
whose	O
change	O
is	O
a-to-b	O
or	O
b-to-c	O
or	O
c-to-a	O
and	O
at	O
the	O
wth	O
weighing	O
putting	O
those	O
whose	O
wth	O
letter	O
is	O
a	O
in	O
pan	O
a	O
and	O
those	O
whose	O
wth	O
letter	O
is	O
b	O
in	O
pan	O
b	O
solution	O
to	O
exercise	O
the	O
curves	O
n	O
and	O
are	O
shown	O
in	O
note	O
that	O
bits	O
n	O
n	O
as	O
a	O
function	O
of	O
for	O
n	O
n	O
n	O
n	O
figure	O
n	O
axis	O
against	O
for	O
n	O
binary	O
variables	O
with	O
solution	O
to	O
exercise	O
the	O
gibbs	B
entropy	B
is	O
kbpi	O
pi	O
ln	O
where	O
i	O
runs	O
over	O
all	O
states	O
of	O
the	O
system	O
this	O
entropy	B
is	O
equivalent	O
from	O
the	O
factor	O
of	O
kb	O
to	O
the	O
shannon	B
entropy	B
of	O
the	O
ensemble	B
pi	O
whereas	O
the	O
gibbs	B
entropy	B
can	O
be	O
for	O
any	O
ensemble	B
the	O
boltzmann	B
entropy	B
is	O
only	O
for	O
microcanonical	B
ensembles	O
which	O
have	O
a	O
probability	B
distribution	B
that	O
is	O
uniform	O
over	O
a	O
set	B
of	O
accessible	O
states	O
the	O
boltzmann	B
entropy	B
is	O
to	O
be	O
sb	O
kb	O
ln	O
where	O
is	O
the	O
number	O
of	O
accessible	O
states	O
of	O
the	O
microcanonical	B
ensemble	B
this	O
is	O
equivalent	O
from	O
the	O
factor	O
of	O
kb	O
to	O
the	O
perfect	B
information	B
content	I
of	O
that	O
constrained	B
ensemble	B
the	O
gibbs	B
entropy	B
of	O
a	O
microcanonical	B
ensemble	B
is	O
trivially	O
equal	O
to	O
the	O
boltzmann	B
entropy	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
source	B
coding	B
theorem	I
we	O
now	O
consider	O
a	O
thermal	B
distribution	B
canonical	B
ensemble	B
where	O
the	O
probability	B
of	O
a	O
state	O
x	O
is	O
p	O
z	O
ex	O
kbt	O
with	O
this	O
canonical	B
ensemble	B
we	O
can	O
associate	O
a	O
corresponding	O
microcanonical	B
ensemble	B
an	O
ensemble	B
with	O
total	O
energy	B
to	O
the	O
mean	B
energy	B
of	O
the	O
canonical	B
ensemble	B
to	O
within	O
some	O
precision	B
now	O
the	O
total	O
energy	B
to	O
a	O
precision	B
is	O
equivalent	O
to	O
the	O
value	O
of	O
ln	O
to	O
within	O
our	O
of	O
the	O
typical	B
set	B
tn	O
was	O
precisely	O
that	O
it	O
consisted	O
of	O
all	O
elements	O
that	O
have	O
a	O
value	O
of	O
log	O
p	O
very	O
close	O
to	O
the	O
mean	B
value	O
of	O
log	O
p	O
under	O
the	O
canonical	B
ensemble	B
hx	O
thus	O
the	O
microcanonical	B
ensemble	B
is	O
equivalent	O
to	O
a	O
uniform	O
distribution	B
over	O
the	O
typical	B
set	B
of	O
the	O
canonical	B
ensemble	B
our	O
proof	O
of	O
the	O
asymptotic	B
equipartition	B
principle	O
thus	O
proves	O
for	O
the	O
case	O
of	O
a	O
system	O
whose	O
energy	B
is	O
separable	O
into	O
a	O
sum	O
of	O
independent	O
terms	O
that	O
the	O
boltzmann	B
entropy	B
of	O
the	O
microcanonical	B
ensemble	B
is	O
very	O
close	O
large	O
n	O
to	O
the	O
gibbs	B
entropy	B
of	O
the	O
canonical	B
ensemble	B
if	O
the	O
energy	B
of	O
the	O
microcanonical	B
ensemble	B
is	O
constrained	B
to	O
equal	O
the	O
mean	B
energy	B
of	O
the	O
canonical	B
ensemble	B
solution	O
to	O
exercise	O
the	O
normalizing	O
constant	O
of	O
the	O
cauchy	B
distribution	B
p	O
z	O
is	O
z	O
dx	O
the	O
mean	B
and	O
variance	B
of	O
this	O
distribution	B
are	O
both	O
distribution	B
is	O
symmetrical	O
about	O
zero	O
but	O
this	O
does	O
not	O
imply	O
that	O
its	O
mean	B
is	O
zero	O
the	O
mean	B
is	O
the	O
value	O
of	O
a	O
divergent	O
integral	B
the	O
sum	O
z	O
where	O
and	O
both	O
have	O
cauchy	B
distributions	O
has	O
probability	B
density	B
given	O
by	O
the	O
convolution	B
p	O
z	O
which	O
after	O
a	O
considerable	O
labour	O
using	O
standard	O
methods	O
gives	O
p	O
which	O
we	O
recognize	O
as	O
a	O
cauchy	B
distribution	B
with	O
width	O
parameter	O
the	O
original	O
distribution	B
has	O
width	O
parameter	O
this	O
implies	O
that	O
the	O
mean	B
of	O
the	O
two	O
points	O
has	O
a	O
cauchy	B
distribution	B
with	O
width	O
parameter	O
generalizing	O
the	O
mean	B
of	O
n	O
samples	O
from	O
a	O
cauchy	B
distribution	B
is	O
cauchy-distributed	O
with	O
the	O
same	O
parameters	B
as	O
the	O
individual	O
samples	O
the	O
probability	B
distribution	B
of	O
the	O
mean	B
does	O
not	O
become	O
narrower	O
as	O
the	O
central-limit	B
theorem	I
does	O
not	O
apply	O
to	O
the	O
cauchy	B
distribution	B
be	O
cause	O
it	O
does	O
not	O
have	O
a	O
variance	B
an	O
alternative	O
neat	O
method	O
for	O
getting	O
to	O
equation	O
makes	O
use	O
of	O
the	O
fourier	B
transform	I
of	O
the	O
cauchy	B
distribution	B
which	O
is	O
a	O
biexponential	B
convolution	B
in	O
real	O
space	O
corresponds	O
to	O
multiplication	O
in	O
fourier	O
space	O
so	O
the	O
fourier	B
transform	I
of	O
z	O
is	O
simply	O
reversing	O
the	O
transform	O
we	O
obtain	O
equation	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solution	O
to	O
exercise	O
the	O
function	O
f	O
has	O
inverse	O
function	O
note	O
gy	O
log	O
gy	O
log	O
y	O
i	O
obtained	O
a	O
tentative	O
graph	B
of	O
f	O
by	O
plotting	O
gy	O
with	O
y	O
along	O
the	O
vertical	O
axis	O
and	O
gy	O
along	O
the	O
horizontal	O
axis	O
the	O
resulting	O
graph	B
suggests	O
that	O
f	O
is	O
single	O
valued	O
for	O
x	O
and	O
looks	O
surprisingly	O
well-behaved	O
and	O
ordinary	O
for	O
x	O
f	O
is	O
two-valued	O
f	O
is	O
equal	O
both	O
to	O
and	O
for	O
x	O
is	O
about	O
f	O
is	O
however	O
it	O
might	O
be	O
argued	O
that	O
this	O
approach	O
to	O
sketching	O
f	O
is	O
only	O
partly	O
valid	O
if	O
we	O
f	O
as	O
the	O
limit	O
of	O
the	O
sequence	B
of	O
functions	B
x	O
xx	O
xxx	O
this	O
sequence	B
does	O
not	O
have	O
a	O
limit	O
for	O
x	O
on	O
account	O
of	O
a	O
pitchfork	B
bifurcation	B
at	O
x	O
and	O
for	O
x	O
the	O
sequence	B
s	O
limit	O
is	O
single-valued	O
the	O
lower	O
of	O
the	O
two	O
values	O
sketched	O
in	O
the	O
figure	O
f	O
xxx	O
at	O
three	O
scales	O
x	O
x	O
shown	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
in	O
the	O
last	O
chapter	O
we	O
saw	O
a	O
proof	O
of	O
the	O
fundamental	O
status	O
of	O
the	O
entropy	B
as	O
a	O
measure	O
of	O
average	O
information	B
content	I
we	O
a	O
data	O
compression	B
scheme	O
using	O
length	O
block	B
codes	O
and	O
proved	O
that	O
as	O
n	O
increases	O
it	O
is	O
possible	O
to	O
encode	O
n	O
i	O
i	O
d	O
variables	O
x	O
xn	O
into	O
a	O
block	B
of	O
n	O
bits	O
with	O
vanishing	O
probability	B
of	I
error	I
whereas	O
if	O
we	O
attempt	O
to	O
encode	O
x	O
n	O
into	O
n	O
bits	O
the	O
probability	B
of	I
error	I
is	O
virtually	O
we	O
thus	O
the	O
possibility	O
of	O
data	O
compression	B
but	O
the	O
block	B
coding	O
in	O
the	O
proof	O
did	O
not	O
give	O
a	O
practical	B
algorithm	O
in	O
this	O
chapter	O
and	O
the	O
next	O
we	O
study	O
practical	B
data	O
compression	B
algorithms	B
whereas	O
the	O
last	O
chapter	O
s	O
compression	B
scheme	O
used	O
large	O
blocks	O
of	O
size	O
and	O
was	O
lossy	B
in	O
the	O
next	O
chapter	O
we	O
discuss	O
variable-length	B
compression	B
schemes	O
that	O
are	O
practical	B
for	O
small	O
block	B
sizes	O
and	O
that	O
are	O
not	O
lossy	B
imagine	O
a	O
rubber	O
glove	O
with	O
water	O
if	O
we	O
compress	B
two	O
of	O
the	O
glove	O
some	O
other	O
part	O
of	O
the	O
glove	O
has	O
to	O
expand	O
because	O
the	O
total	O
volume	B
of	O
water	O
is	O
constant	O
is	O
essentially	O
incompressible	O
similarly	O
when	O
we	O
shorten	O
the	O
codewords	O
for	O
some	O
outcomes	O
there	O
must	O
be	O
other	O
codewords	O
that	O
get	O
longer	O
if	O
the	O
scheme	O
is	O
not	O
lossy	B
in	O
this	O
chapter	O
we	O
will	O
discover	O
the	O
information-theoretic	O
equivalent	O
of	O
water	O
volume	B
before	O
reading	O
chapter	O
you	O
should	O
have	O
worked	O
on	O
exercise	O
we	O
will	O
use	O
the	O
following	O
notation	B
for	O
intervals	B
x	O
means	O
that	O
x	O
and	O
x	O
x	O
means	O
that	O
x	O
and	O
x	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
in	O
this	O
chapter	O
we	O
discuss	O
variable-length	B
symbol	O
codes	O
which	O
encode	O
one	O
source	O
symbol	O
at	O
a	O
time	O
instead	O
of	O
encoding	O
huge	O
strings	O
of	O
n	O
source	O
symbols	O
these	O
codes	O
are	O
lossless	B
unlike	O
the	O
last	O
chapter	O
s	O
block	B
codes	O
they	O
are	O
guaranteed	O
to	O
compress	B
and	O
decompress	O
without	O
any	O
errors	B
but	O
there	O
is	O
a	O
chance	O
that	O
the	O
codes	O
may	O
sometimes	O
produce	O
encoded	O
strings	O
longer	O
than	O
the	O
original	O
source	O
string	O
the	O
idea	O
is	O
that	O
we	O
can	O
achieve	O
compression	B
on	O
average	O
by	O
assigning	B
shorter	O
encodings	O
to	O
the	O
more	O
probable	O
outcomes	O
and	O
longer	O
encodings	O
to	O
the	O
less	O
probable	O
the	O
key	O
issues	O
are	O
what	O
are	O
the	O
implications	O
if	O
a	O
symbol	B
code	I
is	O
lossless	B
if	O
some	O
codewords	O
are	O
shortened	O
by	O
how	O
much	O
do	O
other	O
codewords	O
have	O
to	O
be	O
lengthened	O
making	O
compression	B
practical	B
how	O
can	O
we	O
ensure	O
that	O
a	O
symbol	B
code	I
is	O
easy	O
to	O
decode	O
optimal	B
symbol	O
codes	O
how	O
should	O
we	O
assign	O
codelengths	O
to	O
achieve	O
the	O
best	O
compression	B
and	O
what	O
is	O
the	O
best	O
achievable	O
compression	B
we	O
again	O
verify	O
the	O
fundamental	O
status	O
of	O
the	O
shannon	B
information	B
content	I
and	O
the	O
entropy	B
proving	O
source	B
coding	B
theorem	I
codes	O
there	O
exists	O
a	O
variable-length	B
encoding	O
c	O
of	O
an	O
ensemble	B
x	O
such	O
that	O
the	O
average	O
length	O
of	O
an	O
encoded	O
symbol	O
lc	O
x	O
lc	O
x	O
hx	O
the	O
average	O
length	O
is	O
equal	O
to	O
the	O
entropy	B
hx	O
only	O
if	O
the	O
codelength	O
for	O
each	O
outcome	O
is	O
equal	O
to	O
its	O
shannon	B
information	B
content	I
we	O
will	O
also	O
a	O
constructive	O
procedure	O
the	O
coding	O
algorithm	O
that	O
produces	O
optimal	B
symbol	O
codes	O
notation	B
for	O
alphabets	O
an	O
denotes	O
the	O
set	B
of	O
ordered	O
n	O
of	O
elements	O
from	O
the	O
set	B
a	O
i	O
e	O
all	O
strings	O
of	O
length	O
n	O
the	O
symbol	O
a	O
will	O
denote	O
the	O
set	B
of	O
all	O
strings	O
of	O
length	O
composed	O
of	O
elements	O
from	O
the	O
set	B
a	O
example	O
example	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
symbol	O
codes	O
a	O
symbol	B
code	I
c	O
for	O
an	O
ensemble	B
x	O
is	O
a	O
mapping	B
from	O
the	O
range	O
of	O
x	O
ax	O
aig	O
to	O
cx	O
will	O
denote	O
the	O
codeword	B
corresponding	O
to	O
x	O
and	O
lx	O
will	O
denote	O
its	O
length	O
with	O
li	O
lai	O
the	O
extended	B
code	I
c	O
is	O
a	O
mapping	B
from	O
a	O
concatenation	B
without	O
punctuation	O
of	O
the	O
corresponding	O
codewords	O
x	O
to	O
obtained	O
by	O
xn	O
cxn	O
term	O
mapping	B
here	O
is	O
a	O
synonym	O
for	O
function	O
example	O
a	O
symbol	B
code	I
for	O
the	O
ensemble	B
x	O
by	O
ax	O
f	O
a	O
b	O
c	O
d	O
g	O
px	O
f	O
g	O
is	O
shown	O
in	O
the	O
margin	O
using	O
the	O
extended	B
code	I
we	O
may	O
encode	O
acdbac	O
as	O
cacdbac	O
ai	O
cai	O
a	O
b	O
c	O
d	O
li	O
there	O
are	O
basic	O
requirements	O
for	O
a	O
useful	O
symbol	B
code	I
first	O
any	O
encoded	O
string	O
must	O
have	O
a	O
unique	O
decoding	B
second	O
the	O
symbol	B
code	I
must	O
be	O
easy	O
to	O
decode	O
and	O
third	O
the	O
code	O
should	O
achieve	O
as	O
much	O
compression	B
as	O
possible	O
any	O
encoded	O
string	O
must	O
have	O
a	O
unique	O
decoding	B
a	O
code	O
cx	O
is	O
uniquely	B
decodeable	I
if	O
under	O
the	O
extended	B
code	I
c	O
no	O
two	O
distinct	O
strings	O
have	O
the	O
same	O
encoding	O
i	O
e	O
x	O
y	O
a	O
x	O
x	O
y	O
cx	O
cy	O
the	O
code	O
above	O
is	O
an	O
example	O
of	O
a	O
uniquely	B
decodeable	I
code	O
the	O
symbol	B
code	I
must	O
be	O
easy	O
to	O
decode	O
a	O
symbol	B
code	I
is	O
easiest	O
to	O
decode	O
if	O
it	O
is	O
possible	O
to	O
identify	O
the	O
end	O
of	O
a	O
codeword	B
as	O
soon	O
as	O
it	O
arrives	O
which	O
means	O
that	O
no	O
codeword	B
can	O
be	O
a	O
of	O
another	O
codeword	B
word	O
c	O
is	O
a	O
of	O
another	O
word	O
d	O
if	O
there	O
exists	O
a	O
tail	B
string	O
t	O
such	O
that	O
the	O
concatenation	B
ct	O
is	O
identical	O
to	O
d	O
for	O
example	O
is	O
a	O
of	O
and	O
so	O
is	O
we	O
will	O
show	O
later	O
that	O
we	O
don	O
t	O
lose	O
any	O
performance	O
if	O
we	O
constrain	O
our	O
symbol	B
code	I
to	O
be	O
a	O
code	O
a	O
symbol	B
code	I
is	O
called	O
a	O
code	O
if	O
no	O
codeword	B
is	O
a	O
of	O
any	O
other	O
codeword	B
a	O
code	O
is	O
also	O
known	O
as	O
an	O
instantaneous	B
or	O
self-punctuating	B
code	O
because	O
an	O
encoded	O
string	O
can	O
be	O
decoded	O
from	O
left	O
to	O
right	O
without	O
looking	O
ahead	O
to	O
subsequent	O
codewords	O
the	O
end	O
of	O
a	O
codeword	B
is	O
immediately	O
recognizable	O
a	O
code	O
is	O
uniquely	B
decodeable	I
codes	O
are	O
also	O
known	O
as	O
codes	O
or	O
condition	O
codes	O
codes	O
correspond	O
to	O
trees	O
as	O
illustrated	O
in	O
the	O
margin	O
of	O
the	O
next	O
page	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
example	O
the	O
code	O
is	O
a	O
code	O
because	O
is	O
not	O
a	O
of	O
nor	O
is	O
a	O
of	O
example	O
let	O
this	O
code	O
is	O
not	O
a	O
code	O
because	O
is	O
a	O
of	O
example	O
the	O
code	O
is	O
a	O
code	O
example	O
the	O
code	O
is	O
a	O
code	O
exercise	O
is	O
uniquely	B
decodeable	I
example	O
consider	O
exercise	O
and	O
any	O
weighing	O
strategy	O
that	O
the	O
odd	O
ball	O
and	O
whether	O
it	O
is	O
heavy	O
or	O
light	O
can	O
be	O
viewed	O
as	O
assigning	B
a	O
ternary	O
code	O
to	O
each	O
of	O
the	O
possible	O
states	O
this	O
code	O
is	O
a	O
code	O
the	O
code	O
should	O
achieve	O
as	O
much	O
compression	B
as	O
possible	O
the	O
expected	O
length	O
lc	O
x	O
of	O
a	O
symbol	B
code	I
c	O
for	O
ensemble	B
x	O
is	O
lc	O
x	O
p	O
lx	O
we	O
may	O
also	O
write	O
this	O
quantity	O
as	O
where	O
i	O
jaxj	O
example	O
let	O
lc	O
x	O
pili	O
i	O
and	O
ax	O
f	O
a	O
b	O
c	O
d	O
g	O
px	O
f	O
and	O
consider	O
the	O
code	O
the	O
entropy	B
of	O
x	O
is	O
bits	O
and	O
the	O
expected	O
length	O
x	O
of	O
this	O
code	O
is	O
also	O
bits	O
the	O
sequence	B
of	O
symbols	O
x	O
is	O
encoded	O
as	O
cx	O
is	O
a	O
code	O
and	O
is	O
therefore	O
uniquely	B
decodeable	I
notice	O
that	O
the	O
codeword	B
lengths	O
satisfy	O
li	O
or	O
equivalently	O
pi	O
example	O
consider	O
the	O
length	O
code	O
for	O
the	O
same	O
ensemble	B
x	O
the	O
expected	O
length	O
x	O
is	O
bits	O
example	O
consider	O
the	O
expected	O
length	O
x	O
is	O
bits	O
which	O
is	O
less	O
than	O
hx	O
but	O
the	O
code	O
is	O
not	O
uniquely	B
decodeable	I
the	O
sequence	B
x	O
encodes	O
as	O
which	O
can	O
also	O
be	O
decoded	O
as	O
example	O
consider	O
the	O
code	O
the	O
expected	O
length	O
x	O
of	O
this	O
code	O
is	O
bits	O
the	O
sequence	B
of	O
symbols	O
x	O
is	O
encoded	O
as	O
cx	O
is	O
a	O
code	O
it	O
is	O
not	O
because	O
ca	O
is	O
a	O
of	O
both	O
cb	O
and	O
cc	O
codes	O
can	O
be	O
represented	O
on	O
binary	O
trees	O
complete	O
codes	O
correspond	O
to	O
binary	O
trees	O
with	O
no	O
unused	O
branches	O
is	O
an	O
incomplete	O
code	O
pi	O
ai	O
cai	O
a	O
b	O
c	O
d	O
hpi	O
li	O
a	O
b	O
c	O
d	O
pi	O
ai	O
cai	O
a	O
b	O
c	O
d	O
hpi	O
li	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
is	O
uniquely	B
decodeable	I
this	O
is	O
not	O
so	O
obvious	O
if	O
you	O
think	O
that	O
it	O
might	O
not	O
be	O
uniquely	B
decodeable	I
try	O
to	O
prove	O
it	O
so	O
by	O
a	O
pair	O
of	O
strings	O
x	O
and	O
y	O
that	O
have	O
the	O
same	O
encoding	O
of	O
unique	O
decodeability	O
is	O
given	O
in	O
equation	O
certainly	O
isn	O
t	O
easy	O
to	O
decode	O
when	O
we	O
receive	O
it	O
is	O
possible	O
that	O
x	O
could	O
start	O
aa	O
ab	O
or	O
ac	O
once	O
we	O
have	O
received	O
the	O
second	O
symbol	O
is	O
still	O
ambiguous	O
as	O
x	O
could	O
be	O
abd	O
or	O
acd	O
but	O
eventually	O
a	O
unique	O
decoding	B
crystallizes	O
once	O
the	O
next	O
appears	O
in	O
the	O
encoded	O
stream	O
is	O
in	O
fact	O
uniquely	B
decodeable	I
comparing	O
with	O
the	O
code	O
we	O
see	O
that	O
the	O
codewords	O
of	O
are	O
the	O
reverse	B
of	O
s	O
that	O
is	O
uniquely	B
decodeable	I
proves	O
that	O
is	O
too	O
since	O
any	O
string	O
from	O
is	O
identical	O
to	O
a	O
string	O
from	O
read	O
backwards	O
what	O
limit	O
is	O
imposed	O
by	O
unique	O
decodeability	O
we	O
now	O
ask	O
given	O
a	O
list	O
of	O
positive	O
integers	O
flig	O
does	O
there	O
exist	O
a	O
uniquely	B
decodeable	I
code	O
with	O
those	O
integers	O
as	O
its	O
codeword	B
lengths	O
at	O
this	O
stage	O
we	O
ignore	O
the	O
probabilities	O
of	O
the	O
symbols	O
once	O
we	O
understand	O
unique	O
decodeability	O
better	O
we	O
ll	O
reintroduce	O
the	O
probabilities	O
and	O
discuss	O
how	O
to	O
make	O
an	O
optimal	B
uniquely	B
decodeable	I
symbol	B
code	I
in	O
the	O
examples	O
above	O
we	O
have	O
observed	O
that	O
if	O
we	O
take	O
a	O
code	O
such	O
as	O
and	O
shorten	O
one	O
of	O
its	O
codewords	O
for	O
example	O
then	O
we	O
can	O
retain	O
unique	O
decodeability	O
only	O
if	O
we	O
lengthen	O
other	O
codewords	O
thus	O
there	O
seems	O
to	O
be	O
a	O
constrained	B
budget	B
that	O
we	O
can	O
spend	O
on	O
codewords	O
with	O
shorter	O
codewords	O
being	O
more	O
expensive	O
let	O
us	O
explore	B
the	O
nature	O
of	O
this	O
budget	B
if	O
we	O
build	O
a	O
code	O
purely	O
from	O
codewords	O
of	O
length	O
l	O
equal	O
to	O
three	O
how	O
many	O
codewords	O
can	O
we	O
have	O
and	O
retain	O
unique	O
decodeability	O
the	O
answer	O
is	O
once	O
we	O
have	O
chosen	O
all	O
eight	O
of	O
these	O
codewords	O
is	O
there	O
any	O
way	O
we	O
could	O
add	O
to	O
the	O
code	O
another	O
codeword	B
of	O
some	O
other	O
length	O
and	O
retain	O
unique	O
decodeability	O
it	O
would	O
seem	O
not	O
what	O
if	O
we	O
make	O
a	O
code	O
that	O
includes	O
a	O
length-one	O
codeword	B
with	O
the	O
other	O
codewords	O
being	O
of	O
length	O
three	O
how	O
many	O
length-three	O
codewords	O
can	O
we	O
have	O
if	O
we	O
restrict	O
attention	O
to	O
codes	O
then	O
we	O
can	O
have	O
only	O
four	O
codewords	O
of	O
length	O
three	O
namely	O
what	O
about	O
other	O
codes	O
is	O
there	O
any	O
other	O
way	O
of	O
choosing	O
codewords	O
of	O
length	O
that	O
can	O
give	O
more	O
codewords	O
intuitively	O
we	O
think	O
this	O
unlikely	O
a	O
codeword	B
of	O
length	O
appears	O
to	O
have	O
a	O
cost	O
that	O
is	O
times	O
smaller	O
than	O
a	O
codeword	B
of	O
length	O
let	O
s	O
a	O
total	O
budget	B
of	O
size	O
which	O
we	O
can	O
spend	O
on	O
codewords	O
if	O
we	O
set	B
the	O
cost	O
of	O
a	O
codeword	B
whose	O
length	O
is	O
l	O
to	O
then	O
we	O
have	O
a	O
pricing	O
system	O
that	O
the	O
examples	O
discussed	O
above	O
codewords	O
of	O
length	O
cost	O
each	O
codewords	O
of	O
length	O
cost	O
each	O
we	O
can	O
spend	O
our	O
budget	B
on	O
any	O
codewords	O
if	O
we	O
go	O
over	O
our	O
budget	B
then	O
the	O
code	O
will	O
certainly	O
not	O
be	O
uniquely	B
decodeable	I
if	O
on	O
the	O
other	O
hand	O
xi	O
then	O
the	O
code	O
may	O
be	O
uniquely	B
decodeable	I
this	O
inequality	B
is	O
the	O
kraft	B
inequality	B
kraft	B
inequality	B
for	O
any	O
uniquely	B
decodeable	I
code	O
cx	O
over	O
the	O
binary	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
what	O
limit	O
is	O
imposed	O
by	O
unique	O
decodeability	O
alphabet	O
the	O
codeword	B
lengths	O
must	O
satisfy	O
i	O
where	O
i	O
jaxj	O
completeness	O
if	O
a	O
uniquely	B
decodeable	I
code	O
the	O
kraft	B
inequality	B
with	O
equality	O
then	O
it	O
is	O
called	O
a	O
complete	O
code	O
we	O
want	O
codes	O
that	O
are	O
uniquely	B
decodeable	I
codes	O
are	O
uniquely	B
decodeable	I
and	O
are	O
easy	O
to	O
decode	O
so	O
life	B
would	O
be	O
simpler	O
for	O
us	O
if	O
we	O
could	O
restrict	O
attention	O
to	O
codes	O
fortunately	O
for	O
any	O
source	O
there	O
is	O
an	O
optimal	B
symbol	B
code	I
that	O
is	O
also	O
a	O
code	O
kraft	B
inequality	B
and	O
codes	O
given	O
a	O
set	B
of	O
codeword	B
lengths	O
that	O
satisfy	O
the	O
kraft	B
inequality	B
there	O
exists	O
a	O
uniquely	B
decodeable	I
code	O
with	O
these	O
codeword	B
lengths	O
the	O
kraft	B
inequality	B
might	O
be	O
more	O
accurately	O
referred	O
to	O
as	O
the	O
kraft	B
mcmillan	B
inequality	B
kraft	B
proved	O
that	O
if	O
the	O
inequality	B
is	O
then	O
a	O
code	O
exists	O
with	O
the	O
given	O
lengths	O
mcmillan	B
proved	O
the	O
converse	O
that	O
unique	O
decodeability	O
implies	O
that	O
the	O
inequality	B
holds	O
proof	O
of	O
the	O
kraft	B
inequality	B
s	O
consider	O
the	O
quantity	O
lin	O
sn	O
i	O
i	O
i	O
xin	O
the	O
quantity	O
in	O
the	O
exponent	O
lin	O
is	O
the	O
length	O
of	O
the	O
encoding	O
of	O
the	O
string	O
x	O
ain	O
for	O
every	O
string	O
x	O
of	O
length	O
n	O
there	O
is	O
one	O
term	O
in	O
the	O
above	O
sum	O
introduce	O
an	O
array	O
al	O
that	O
counts	O
how	O
many	O
strings	O
x	O
have	O
encoded	O
length	O
l	O
then	O
lmin	O
mini	O
li	O
and	O
lmax	O
maxi	O
li	O
sn	O
n	O
lmax	O
xln	O
lmin	O
now	O
assume	O
c	O
is	O
uniquely	B
decodeable	I
so	O
that	O
for	O
all	O
x	O
y	O
cx	O
cy	O
concentrate	O
on	O
the	O
x	O
that	O
have	O
encoded	O
length	O
l	O
there	O
are	O
a	O
total	O
of	O
distinct	O
bit	B
strings	O
of	O
length	O
l	O
so	O
it	O
must	O
be	O
the	O
case	O
that	O
al	O
so	O
sn	O
n	O
lmax	O
xln	O
lmin	O
n	O
lmax	O
xln	O
lmin	O
n	O
lmax	O
thus	O
sn	O
lmaxn	O
for	O
all	O
n	O
now	O
if	O
s	O
were	O
greater	O
than	O
then	O
as	O
n	O
increases	O
sn	O
would	O
be	O
an	O
exponentially	O
growing	O
function	O
and	O
for	O
large	O
enough	O
n	O
an	O
exponential	B
always	O
exceeds	O
a	O
polynomial	O
such	O
as	O
lmaxn	O
but	O
our	O
result	O
lmaxn	O
is	O
true	O
for	O
any	O
n	O
therefore	O
s	O
exercise	O
prove	O
the	O
result	O
stated	O
above	O
that	O
for	O
any	O
set	B
of	O
codeword	B
lengths	O
flig	O
satisfying	O
the	O
kraft	B
inequality	B
there	O
is	O
a	O
code	O
having	O
those	O
lengths	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
t	O
e	O
g	O
d	O
u	O
b	O
e	O
d	O
o	O
c	O
l	O
o	O
b	O
m	O
y	O
s	O
l	O
a	O
t	O
o	O
t	O
e	O
h	O
t	O
figure	O
the	O
symbol	O
coding	O
budget	B
the	O
cost	O
of	O
each	O
codeword	B
length	O
l	O
is	O
indicated	O
by	O
the	O
size	O
of	O
the	O
box	B
it	O
is	O
written	O
in	O
the	O
total	O
budget	B
available	O
when	O
making	O
a	O
uniquely	B
decodeable	I
code	O
is	O
you	O
can	O
think	O
of	O
this	O
diagram	O
as	O
showing	O
a	O
codeword	B
supermarket	B
with	O
the	O
codewords	O
arranged	O
in	O
aisles	O
by	O
their	O
length	O
and	O
the	O
cost	O
of	O
each	O
codeword	B
indicated	O
by	O
the	O
size	O
of	O
its	O
box	B
on	O
the	O
shelf	O
if	O
the	O
cost	O
of	O
the	O
codewords	O
that	O
you	O
take	O
exceeds	O
the	O
budget	B
then	O
your	O
code	O
will	O
not	O
be	O
uniquely	B
decodeable	I
figure	O
selections	O
of	O
codewords	O
made	O
by	O
codes	O
and	O
from	O
section	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
what	O
s	O
the	O
most	O
compression	B
that	O
we	O
can	O
hope	O
for	O
a	O
pictorial	O
view	O
of	O
the	O
kraft	B
inequality	B
may	O
help	O
you	O
solve	O
this	O
exercise	O
imagine	O
that	O
we	O
are	O
choosing	O
the	O
codewords	O
to	O
make	O
a	O
symbol	B
code	I
we	O
can	O
draw	O
the	O
set	B
of	O
all	O
candidate	O
codewords	O
in	O
a	O
supermarket	B
that	O
displays	O
the	O
cost	O
of	O
the	O
codeword	B
by	O
the	O
area	O
of	O
a	O
box	B
the	O
total	O
budget	B
available	O
the	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
kraft	B
inequality	B
is	O
shown	O
at	O
one	O
side	O
some	O
of	O
the	O
codes	O
discussed	O
in	O
section	B
are	O
illustrated	O
in	O
notice	O
that	O
the	O
codes	O
that	O
are	O
codes	O
and	O
have	O
the	O
property	O
that	O
to	O
the	O
right	O
of	O
any	O
selected	O
codeword	B
there	O
are	O
no	O
other	O
selected	O
codewords	O
because	O
codes	O
correspond	O
to	O
trees	O
notice	O
that	O
a	O
complete	O
code	O
corresponds	O
to	O
a	O
complete	O
tree	B
having	O
no	O
unused	O
branches	O
we	O
are	O
now	O
ready	O
to	O
put	O
back	O
the	O
symbols	O
probabilities	O
fpig	O
given	O
a	O
set	B
of	O
symbol	O
probabilities	O
english	B
language	O
probabilities	O
of	O
for	O
example	O
how	O
do	O
we	O
make	O
the	O
best	O
symbol	B
code	I
one	O
with	O
the	O
smallest	O
possible	O
expected	O
length	O
lc	O
x	O
and	O
what	O
is	O
that	O
smallest	O
possible	O
expected	O
length	O
it	O
s	O
not	O
obvious	O
how	O
to	O
assign	O
the	O
codeword	B
lengths	O
if	O
we	O
give	O
short	O
codewords	O
to	O
the	O
more	O
probable	O
symbols	O
then	O
the	O
expected	O
length	O
might	O
be	O
reduced	O
on	O
the	O
other	O
hand	O
shortening	B
some	O
codewords	O
necessarily	O
causes	O
others	O
to	O
lengthen	O
by	O
the	O
kraft	B
inequality	B
what	O
s	O
the	O
most	O
compression	B
that	O
we	O
can	O
hope	O
for	O
we	O
wish	O
to	O
minimize	O
the	O
expected	O
length	O
of	O
a	O
code	O
lc	O
x	O
xi	O
xi	O
hx	O
pi	O
log	O
log	O
z	O
pili	O
pi	O
log	O
log	O
z	O
lc	O
x	O
xi	O
pili	O
as	O
you	O
might	O
have	O
guessed	O
the	O
entropy	B
appears	O
as	O
the	O
lower	O
bound	B
on	O
the	O
expected	O
length	O
of	O
a	O
code	O
lower	O
bound	B
on	O
expected	O
length	O
the	O
expected	O
length	O
lc	O
x	O
of	O
a	O
uniquely	B
decodeable	I
code	O
is	O
bounded	O
below	O
by	O
hx	O
proof	O
we	O
the	O
implicit	B
probabilities	I
qi	O
where	O
z	O
so	O
that	O
li	O
log	O
log	O
z	O
we	O
then	O
use	O
gibbs	B
inequality	B
pi	O
pi	O
log	O
pi	O
pi	O
log	O
with	O
equality	O
if	O
qi	O
pi	O
and	O
the	O
kraft	B
inequality	B
z	O
the	O
equality	O
lc	O
x	O
hx	O
is	O
achieved	O
only	O
if	O
the	O
kraft	B
equality	O
z	O
is	O
and	O
if	O
the	O
codelengths	O
satisfy	O
li	O
this	O
is	O
an	O
important	O
result	O
so	O
let	O
s	O
say	O
it	O
again	O
optimal	B
source	O
codelengths	O
the	O
expected	O
length	O
is	O
minimized	O
and	O
is	O
equal	O
to	O
hx	O
only	O
if	O
the	O
codelengths	O
are	O
equal	O
to	O
the	O
shannon	B
information	B
contents	O
li	O
implicit	B
probabilities	I
by	O
codelengths	O
conversely	O
any	O
choice	O
of	O
codelengths	O
flig	O
implicitly	O
a	O
probability	B
distribution	B
fqig	O
qi	O
for	O
which	O
those	O
codelengths	O
would	O
be	O
the	O
optimal	B
codelengths	O
if	O
the	O
code	O
is	O
complete	O
then	O
z	O
and	O
the	O
implicit	B
probabilities	I
are	O
given	O
by	O
qi	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
how	O
much	O
can	O
we	O
compress	B
so	O
we	O
can	O
t	O
compress	B
below	O
the	O
entropy	B
how	O
close	O
can	O
we	O
expect	O
to	O
get	O
to	O
the	O
entropy	B
theorem	O
source	B
coding	B
theorem	I
for	O
symbol	O
codes	O
for	O
an	O
ensemble	B
x	O
there	O
exists	O
a	O
code	O
c	O
with	O
expected	O
length	O
satisfying	O
hx	O
lc	O
x	O
hx	O
proof	O
we	O
set	B
the	O
codelengths	O
to	O
integers	O
slightly	O
larger	O
than	O
the	O
optimum	O
lengths	O
li	O
where	O
denotes	O
the	O
smallest	O
integer	O
greater	O
than	O
or	O
equal	O
to	O
are	O
not	O
asserting	O
that	O
the	O
optimal	B
code	O
necessarily	O
uses	O
these	O
lengths	O
we	O
are	O
simply	O
choosing	O
these	O
lengths	O
because	O
we	O
can	O
use	O
them	O
to	O
prove	O
the	O
theorem	O
we	O
check	O
that	O
there	O
is	O
a	O
code	O
with	O
these	O
lengths	O
by	O
that	O
the	O
kraft	B
inequality	B
is	O
xi	O
pi	O
then	O
we	O
lc	O
x	O
hx	O
the	O
cost	O
of	O
using	O
the	O
wrong	O
codelengths	O
if	O
we	O
use	O
a	O
code	O
whose	O
lengths	O
are	O
not	O
equal	O
to	O
the	O
optimal	B
codelengths	O
the	O
average	O
message	O
length	O
will	O
be	O
larger	O
than	O
the	O
entropy	B
if	O
the	O
true	O
probabilities	O
are	O
fpig	O
and	O
we	O
use	O
a	O
complete	O
code	O
with	O
lengths	O
li	O
we	O
can	O
view	O
those	O
lengths	O
as	O
implicit	B
probabilities	I
qi	O
continuing	O
from	O
equation	O
the	O
average	O
length	O
is	O
lc	O
x	O
hx	O
pi	O
log	O
piqi	O
i	O
e	O
it	O
exceeds	O
the	O
entropy	B
by	O
the	O
relative	B
entropy	B
dklpjjq	O
on	O
optimal	B
source	O
coding	O
with	O
symbol	O
codes	O
coding	O
given	O
a	O
set	B
of	O
probabilities	O
p	O
how	O
can	O
we	O
design	O
an	O
optimal	B
code	O
for	O
example	O
what	O
is	O
the	O
best	O
symbol	B
code	I
for	O
the	O
english	B
language	O
ensemble	B
shown	O
in	O
when	O
we	O
say	O
optimal	B
let	O
s	O
assume	O
our	O
aim	O
is	O
to	O
minimize	O
the	O
expected	O
length	O
lc	O
x	O
how	O
not	O
to	O
do	O
it	O
one	O
might	O
try	O
to	O
roughly	O
split	O
the	O
set	B
ax	O
in	O
two	O
and	O
continue	O
bisecting	O
the	O
subsets	O
so	O
as	O
to	O
a	O
binary	O
tree	B
from	O
the	O
root	O
this	O
construction	B
has	O
the	O
right	O
spirit	O
as	O
in	O
the	O
weighing	B
problem	I
but	O
it	O
is	O
not	O
necessarily	O
optimal	B
it	O
achieves	O
lc	O
x	O
hx	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
p	O
figure	O
an	O
ensemble	B
in	O
need	O
of	O
a	O
symbol	B
code	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
optimal	B
source	O
coding	O
with	O
symbol	O
codes	O
coding	O
algorithm	O
coding	O
algorithm	O
ai	O
pi	O
hpi	O
a	O
b	O
c	O
d	O
e	O
li	O
cai	O
table	O
code	O
created	O
by	O
the	O
algorithm	O
the	O
coding	O
algorithm	O
we	O
now	O
present	O
a	O
beautifully	O
simple	O
algorithm	O
for	O
an	O
optimal	B
code	O
the	O
trick	O
is	O
to	O
construct	O
the	O
code	O
backwards	O
starting	O
from	O
the	O
tails	O
of	O
the	O
codewords	O
we	O
build	O
the	O
binary	O
tree	B
from	O
its	O
leaves	O
take	O
the	O
two	O
least	O
probable	O
symbols	O
in	O
the	O
alphabet	O
these	O
two	O
symbols	O
will	O
be	O
given	O
the	O
longest	O
codewords	O
which	O
will	O
have	O
equal	O
length	O
and	O
only	O
in	O
the	O
last	O
digit	O
combine	O
these	O
two	O
symbols	O
into	O
a	O
single	O
symbol	O
and	O
repeat	O
since	O
each	O
step	O
reduces	O
the	O
size	O
of	O
the	O
alphabet	O
by	O
one	O
this	O
algorithm	O
will	O
have	O
assigned	O
strings	O
to	O
all	O
the	O
symbols	O
after	O
jaxj	O
steps	O
example	O
let	O
ax	O
a	O
g	O
and	O
px	O
g	O
c	O
d	O
b	O
e	O
x	O
a	O
b	O
c	O
d	O
e	O
step	O
step	O
step	O
step	O
the	O
codewords	O
are	O
then	O
obtained	O
by	O
concatenating	O
the	O
binary	O
digits	O
in	O
reverse	B
order	O
c	O
the	O
codelengths	O
selected	O
by	O
the	O
algorithm	O
of	O
table	O
are	O
in	O
some	O
cases	O
longer	O
and	O
in	O
some	O
cases	O
shorter	O
than	O
the	O
ideal	O
codelengths	O
the	O
shannon	B
the	O
expected	O
length	O
of	O
the	O
information	B
contents	O
code	O
is	O
l	O
bits	O
whereas	O
the	O
entropy	B
is	O
h	O
bits	O
if	O
at	O
any	O
point	O
there	O
is	O
more	O
than	O
one	O
way	O
of	O
selecting	O
the	O
two	O
least	O
probable	O
symbols	O
then	O
the	O
choice	O
may	O
be	O
made	O
in	O
any	O
manner	O
the	O
expected	O
length	O
of	O
the	O
code	O
will	O
not	O
depend	O
on	O
the	O
choice	O
exercise	O
prove	O
that	O
there	O
is	O
no	O
better	O
symbol	B
code	I
for	O
a	O
source	O
than	O
the	O
code	O
example	O
we	O
can	O
make	O
a	O
code	O
for	O
the	O
probability	B
distribution	B
over	O
the	O
alphabet	O
introduced	O
in	O
the	O
result	O
is	O
shown	O
in	O
this	O
code	O
has	O
an	O
expected	O
length	O
of	O
bits	O
the	O
entropy	B
of	O
the	O
ensemble	B
is	O
bits	O
observe	O
the	O
disparities	O
between	O
the	O
assigned	O
codelengths	O
and	O
the	O
ideal	O
codelengths	O
constructing	O
a	O
binary	O
tree	B
top-down	O
is	O
suboptimal	O
in	O
previous	O
chapters	O
we	O
studied	O
weighing	O
problems	O
in	O
which	O
we	O
built	O
ternary	O
or	O
binary	O
trees	O
we	O
noticed	O
that	O
balanced	O
trees	O
ones	O
in	O
which	O
at	O
every	O
step	O
the	O
two	O
possible	O
outcomes	O
were	O
as	O
close	O
as	O
possible	O
to	O
equiprobable	O
appeared	O
to	O
describe	O
the	O
most	O
experiments	O
this	O
gave	O
an	O
intuitive	O
motivation	O
for	O
entropy	B
as	O
a	O
measure	O
of	O
information	B
content	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
figure	O
code	O
for	O
the	O
english	B
language	O
ensemble	B
statistics	O
ai	O
pi	O
greedy	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
table	O
a	O
greedily-constructed	O
code	O
compared	O
with	O
the	O
code	O
ai	O
pi	O
pi	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
li	O
cai	O
a	O
n	O
s	O
i	O
o	O
e	O
t	O
b	O
g	O
c	O
d	O
h	O
k	O
x	O
y	O
u	O
j	O
z	O
q	O
v	O
w	O
m	O
f	O
p	O
r	O
l	O
it	O
is	O
not	O
the	O
case	O
however	O
that	O
optimal	B
codes	O
can	O
always	O
be	O
constructed	O
by	O
a	O
greedy	O
top-down	O
method	O
in	O
which	O
the	O
alphabet	O
is	O
successively	O
divided	O
into	O
subsets	O
that	O
are	O
as	O
near	O
as	O
possible	O
to	O
equiprobable	O
example	O
find	O
the	O
optimal	B
binary	O
symbol	B
code	I
for	O
the	O
ensemble	B
g	O
g	O
ax	O
f	O
a	O
px	O
f	O
g	O
d	O
b	O
c	O
e	O
f	O
notice	O
that	O
a	O
greedy	O
top-down	O
method	O
can	O
split	O
this	O
set	B
into	O
two	O
subsets	O
fa	O
b	O
c	O
dg	O
and	O
fe	O
f	O
gg	O
which	O
both	O
have	O
probability	B
and	O
that	O
fa	O
b	O
c	O
dg	O
can	O
be	O
divided	O
into	O
subsets	O
fa	O
bg	O
and	O
fc	O
dg	O
which	O
have	O
probability	B
so	O
a	O
greedy	O
top-down	O
method	O
gives	O
the	O
code	O
shown	O
in	O
the	O
third	O
column	O
of	O
table	O
which	O
has	O
expected	O
length	O
the	O
coding	O
algorithm	O
yields	O
the	O
code	O
shown	O
in	O
the	O
fourth	O
column	O
which	O
has	O
expected	O
length	O
disadvantages	B
of	O
the	O
code	O
the	O
algorithm	O
produces	O
an	O
optimal	B
symbol	B
code	I
for	O
an	O
ensemble	B
but	O
this	O
is	O
not	O
the	O
end	O
of	O
the	O
story	O
both	O
the	O
word	O
ensemble	B
and	O
the	O
phrase	O
symbol	B
code	I
need	O
careful	O
attention	O
changing	O
ensemble	B
if	O
we	O
wish	O
to	O
communicate	O
a	O
sequence	B
of	O
outcomes	O
from	O
one	O
unchanging	O
ensemble	B
then	O
a	O
code	O
may	O
be	O
convenient	O
but	O
often	O
the	O
appropriate	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
disadvantages	B
of	O
the	O
code	O
ensemble	B
changes	O
if	O
for	O
example	O
we	O
are	O
compressing	O
text	O
then	O
the	O
symbol	O
frequencies	O
will	O
vary	O
with	O
context	O
in	O
english	B
the	O
letter	O
u	O
is	O
much	O
more	O
probable	O
after	O
a	O
q	O
than	O
after	O
an	O
e	O
and	O
furthermore	O
our	O
knowledge	O
of	O
these	O
context-dependent	O
symbol	O
frequencies	O
will	O
also	O
change	O
as	O
we	O
learn	O
the	O
statistical	B
properties	O
of	O
the	O
text	O
source	O
codes	O
do	O
not	O
handle	O
changing	O
ensemble	B
probabilities	O
with	O
any	O
elegance	O
one	O
brute-force	O
approach	O
would	O
be	O
to	O
recompute	O
the	O
code	O
every	O
time	O
the	O
probability	B
over	O
symbols	O
changes	O
another	O
attitude	O
is	O
to	O
deny	O
the	O
option	O
of	O
adaptation	O
and	O
instead	O
run	O
through	O
the	O
entire	O
in	O
advance	O
and	O
compute	O
a	O
good	B
probability	B
distribution	B
which	O
will	O
then	O
remain	O
throughout	O
transmission	O
the	O
code	O
itself	O
must	O
also	O
be	O
communicated	O
in	O
this	O
scenario	O
such	O
a	O
technique	O
is	O
not	O
only	O
cumbersome	O
and	O
restrictive	O
it	O
is	O
also	O
suboptimal	O
since	O
the	O
initial	O
message	O
specifying	O
the	O
code	O
and	O
the	O
document	O
itself	O
are	O
partially	O
redundant	O
this	O
technique	O
therefore	O
wastes	O
bits	O
the	O
extra	B
bit	B
an	O
equally	O
serious	O
problem	O
with	O
codes	O
is	O
the	O
innocuous-looking	O
extra	B
bit	B
relative	B
to	O
the	O
ideal	O
average	O
length	O
of	O
hx	O
a	O
code	O
achieves	O
a	O
length	O
that	O
hx	O
lc	O
x	O
as	O
proved	O
in	O
theorem	O
a	O
code	O
thus	O
incurs	O
an	O
overhead	O
of	O
between	O
and	O
bits	O
per	O
symbol	O
if	O
hx	O
were	O
large	O
then	O
this	O
overhead	O
would	O
be	O
an	O
unimportant	O
fractional	O
increase	O
but	O
for	O
many	O
applications	O
the	O
entropy	B
may	O
be	O
as	O
low	O
as	O
one	O
bit	B
per	O
symbol	O
or	O
even	O
smaller	O
so	O
the	O
overhead	O
lc	O
x	O
hx	O
may	O
domiin	O
some	O
contexts	O
long	O
nate	O
the	O
encoded	O
length	O
consider	O
english	B
text	O
strings	O
of	O
characters	O
may	O
be	O
highly	O
predictable	O
for	O
example	O
in	O
the	O
context	O
strings	O
of	O
ch	O
one	O
might	O
predict	O
the	O
next	O
nine	O
symbols	O
to	O
be	O
aracters	O
with	O
a	O
probability	B
of	O
each	O
a	O
traditional	O
code	O
would	O
be	O
obliged	O
to	O
use	O
at	O
least	O
one	O
bit	B
per	O
character	O
making	O
a	O
total	O
cost	O
of	O
nine	O
bits	O
where	O
virtually	O
no	O
information	B
is	O
being	O
conveyed	O
bits	O
in	O
total	O
to	O
be	O
precise	O
the	O
entropy	B
of	O
english	B
given	O
a	O
good	B
model	B
is	O
about	O
one	O
bit	B
per	O
character	O
so	O
a	O
code	O
is	O
likely	O
to	O
be	O
highly	O
a	O
traditional	O
patch-up	O
of	O
codes	O
uses	O
them	O
to	O
compress	B
blocks	O
of	O
symbols	O
for	O
example	O
the	O
extended	B
sources	O
x	O
n	O
we	O
discussed	O
in	O
chapter	O
the	O
overhead	O
per	O
block	B
is	O
at	O
most	O
bit	B
so	O
the	O
overhead	O
per	O
symbol	O
is	O
at	O
most	O
bits	O
for	O
large	O
blocks	O
the	O
problem	O
of	O
the	O
extra	B
bit	B
may	O
be	O
removed	O
but	O
only	O
at	O
the	O
expenses	O
of	O
losing	O
the	O
elegant	O
instantaneous	B
decodeability	O
of	O
simple	O
coding	O
and	O
having	O
to	O
compute	O
the	O
probabilities	O
of	O
all	O
relevant	O
strings	O
and	O
build	O
the	O
associated	O
tree	B
one	O
will	O
end	O
up	O
explicitly	O
computing	O
the	O
probabilities	O
and	O
codes	O
for	O
a	O
huge	O
number	O
of	O
strings	O
most	O
of	O
which	O
will	O
never	O
actually	O
occur	O
exercise	O
beyond	O
symbol	O
codes	O
codes	O
therefore	O
although	O
widely	O
trumpeted	O
as	O
optimal	B
have	O
many	O
defects	O
for	O
practical	B
purposes	O
they	O
are	O
optimal	B
symbol	O
codes	O
but	O
for	O
practical	B
purposes	O
we	O
don	O
t	O
want	O
a	O
symbol	B
code	I
the	O
defects	O
of	O
codes	O
are	O
by	O
arithmetic	B
coding	I
which	O
dispenses	O
with	O
the	O
restriction	O
that	O
each	O
symbol	O
must	O
translate	O
into	O
an	O
integer	O
number	O
of	O
bits	O
arithmetic	B
coding	I
is	O
the	O
main	O
topic	O
of	O
the	O
next	O
chapter	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
summary	B
kraft	B
inequality	B
if	O
a	O
code	O
is	O
uniquely	B
decodeable	I
its	O
lengths	O
must	O
satisfy	O
xi	O
for	O
any	O
lengths	O
satisfying	O
the	O
kraft	B
inequality	B
there	O
exists	O
a	O
code	O
with	O
those	O
lengths	O
optimal	B
source	O
codelengths	O
for	O
an	O
ensemble	B
are	O
equal	O
to	O
the	O
shannon	B
information	B
contents	O
li	O
pi	O
and	O
conversely	O
any	O
choice	O
of	O
codelengths	O
implicit	B
probabilities	I
qi	O
z	O
the	O
relative	B
entropy	B
dklpjjq	O
measures	O
how	O
many	O
bits	O
per	O
symbol	O
are	O
wasted	O
by	O
using	O
a	O
code	O
whose	O
implicit	B
probabilities	I
are	O
q	O
when	O
the	O
ensemble	B
s	O
true	O
probability	B
distribution	B
is	O
p	O
source	B
coding	B
theorem	I
for	O
symbol	O
codes	O
for	O
an	O
ensemble	B
x	O
there	O
ex	O
ists	O
a	O
code	O
whose	O
expected	O
length	O
hx	O
lc	O
x	O
hx	O
the	O
coding	O
algorithm	O
generates	O
an	O
optimal	B
symbol	B
code	I
iteratively	O
at	O
each	O
iteration	O
the	O
two	O
least	O
probable	O
symbols	O
are	O
combined	O
exercises	O
exercise	O
is	O
the	O
code	O
uniquely	B
decodeable	I
exercise	O
is	O
the	O
ternary	O
code	O
uniquely	B
decodeable	I
exercise	O
make	O
codes	O
for	O
x	O
x	O
and	O
x	O
where	O
ax	O
and	O
px	O
compute	O
their	O
expected	O
lengths	O
and	O
compare	O
them	O
with	O
the	O
entropies	O
hx	O
hx	O
and	O
hx	O
repeat	O
this	O
exercise	O
for	O
x	O
and	O
x	O
where	O
px	O
exercise	O
find	O
a	O
probability	B
distribution	B
such	O
that	O
there	O
are	O
two	O
optimal	B
codes	O
that	O
assign	O
lengths	O
flig	O
to	O
the	O
four	O
symbols	O
exercise	O
of	O
exercise	O
assume	O
that	O
the	O
four	O
probabilities	O
are	O
ordered	O
such	O
that	O
let	O
q	O
be	O
the	O
set	B
of	O
all	O
probability	B
vectors	B
p	O
such	O
that	O
there	O
are	O
two	O
optimal	B
codes	O
with	O
lengths	O
give	O
a	O
complete	O
description	O
of	O
q	O
find	O
three	O
probability	B
vectors	B
which	O
are	O
the	O
convex	B
hull	I
of	O
q	O
i	O
e	O
such	O
that	O
any	O
p	O
q	O
can	O
be	O
written	O
as	O
p	O
where	O
are	O
positive	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
exercise	O
write	O
a	O
short	O
essay	O
discussing	O
how	O
to	O
play	O
the	O
game	O
of	O
twenty	B
questions	I
optimally	O
twenty	B
questions	I
one	O
player	O
thinks	O
of	O
an	O
object	O
and	O
the	O
other	O
player	O
has	O
to	O
guess	O
the	O
object	O
using	O
as	O
few	O
binary	O
questions	O
as	O
possible	O
preferably	O
fewer	O
than	O
twenty	O
exercise	O
show	O
that	O
if	O
each	O
probability	B
pi	O
is	O
equal	O
to	O
an	O
integer	O
power	O
of	O
then	O
there	O
exists	O
a	O
source	O
code	O
whose	O
expected	O
length	O
equals	O
the	O
entropy	B
exercise	O
make	O
ensembles	O
for	O
which	O
the	O
between	O
the	O
entropy	B
and	O
the	O
expected	O
length	O
of	O
the	O
code	O
is	O
as	O
big	O
as	O
possible	O
exercise	O
a	O
source	O
x	O
has	O
an	O
alphabet	O
of	O
eleven	O
characters	O
fa	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
kg	O
all	O
of	O
which	O
have	O
equal	O
probability	B
find	O
an	O
optimal	B
uniquely	B
decodeable	I
symbol	B
code	I
for	O
this	O
source	O
how	O
much	O
greater	O
is	O
the	O
expected	O
length	O
of	O
this	O
optimal	B
code	O
than	O
the	O
entropy	B
of	O
x	O
exercise	O
consider	O
the	O
optimal	B
symbol	B
code	I
for	O
an	O
ensemble	B
x	O
with	O
alphabet	O
size	O
i	O
from	O
which	O
all	O
symbols	O
have	O
identical	O
probability	B
p	O
i	O
is	O
not	O
a	O
power	O
of	O
show	O
that	O
the	O
fraction	O
f	O
of	O
the	O
i	O
symbols	O
that	O
are	O
assigned	O
codelengths	O
equal	O
to	O
l	O
ie	O
i	O
f	O
and	O
that	O
the	O
expected	O
length	O
of	O
the	O
optimal	B
symbol	B
code	I
is	O
l	O
l	O
f	O
by	O
the	O
excess	O
length	O
l	O
hx	O
with	O
respect	O
to	O
i	O
show	O
that	O
the	O
excess	O
length	O
is	O
bounded	O
by	O
lnln	O
ln	O
ln	O
exercise	O
consider	O
a	O
sparse	O
binary	O
source	O
with	O
px	O
dis	O
cuss	O
how	O
codes	O
could	O
be	O
used	O
to	O
compress	B
this	O
source	O
estimate	O
how	O
many	O
codewords	O
your	O
proposed	O
solutions	O
require	O
exercise	O
american	B
carried	O
the	O
following	O
puzzle	O
in	O
the	O
poisoned	B
glass	I
mathematicians	O
are	O
curious	O
birds	O
the	O
police	O
commissioner	O
said	O
to	O
his	O
wife	O
you	O
see	O
we	O
had	O
all	O
those	O
partly	O
glasses	O
lined	O
up	O
in	O
rows	O
on	O
a	O
table	O
in	O
the	O
hotel	O
kitchen	O
only	O
one	O
contained	O
poison	O
and	O
we	O
wanted	O
to	O
know	O
which	O
one	O
before	O
searching	O
that	O
glass	O
for	O
our	O
lab	O
could	O
test	O
the	O
liquid	O
in	O
each	O
glass	O
but	O
the	O
tests	O
take	O
time	O
and	O
money	O
so	O
we	O
wanted	O
to	O
make	O
as	O
few	O
of	O
them	O
as	O
possible	O
by	O
simultaneously	O
testing	O
mixtures	O
of	O
small	O
samples	O
from	O
groups	O
of	O
glasses	O
the	O
university	O
sent	O
over	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
mathematics	O
professor	O
to	O
help	O
us	O
he	O
counted	O
the	O
glasses	O
smiled	O
and	O
said	O
any	O
glass	O
you	O
want	O
commissioner	O
we	O
ll	O
test	O
it	O
won	O
t	O
that	O
waste	O
a	O
test	O
i	O
asked	O
he	O
said	O
s	O
part	O
of	O
the	O
best	O
procedure	O
we	O
can	O
test	O
one	O
glass	O
it	O
doesn	O
t	O
matter	O
which	O
one	O
how	O
many	O
glasses	O
were	O
there	O
to	O
start	O
with	O
the	O
commissioner	O
s	O
wife	O
asked	O
i	O
don	O
t	O
remember	O
somewhere	O
between	O
and	O
what	O
was	O
the	O
exact	O
number	O
of	O
glasses	O
solve	O
this	O
puzzle	O
and	O
then	O
explain	O
why	O
the	O
professor	O
was	O
in	O
fact	O
wrong	O
and	O
the	O
commissioner	O
was	O
right	O
what	O
is	O
in	O
fact	O
the	O
optimal	B
procedure	O
for	O
identifying	O
the	O
one	O
poisoned	B
glass	I
what	O
is	O
the	O
expected	O
waste	O
relative	B
to	O
this	O
optimum	O
if	O
one	O
followed	O
the	O
professor	O
s	O
strategy	O
explain	O
the	O
relationship	O
to	O
symbol	O
coding	O
exercise	O
assume	O
that	O
a	O
sequence	B
of	O
symbols	O
from	O
the	O
ensemble	B
x	O
introduced	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
is	O
compressed	O
using	O
the	O
code	O
imagine	O
picking	O
one	O
bit	B
at	O
random	B
from	O
the	O
binary	O
encoded	O
sequence	B
c	O
what	O
is	O
the	O
probability	B
that	O
this	O
bit	B
is	O
a	O
exercise	O
how	O
should	O
the	O
binary	O
encoding	O
scheme	O
be	O
to	O
make	O
optimal	B
symbol	O
codes	O
in	O
an	O
encoding	O
alphabet	O
with	O
q	O
symbols	O
known	O
as	O
radix	B
q	O
mixture	O
codes	O
it	O
is	O
a	O
tempting	O
idea	O
to	O
construct	O
a	O
metacode	B
from	O
several	O
symbol	O
codes	O
that	O
assign	O
codewords	O
to	O
the	O
alternative	O
symbols	O
then	O
switch	O
from	O
one	O
code	O
to	O
another	O
choosing	O
whichever	O
assigns	O
the	O
shortest	O
codeword	B
to	O
the	O
current	O
symbol	O
clearly	O
we	O
cannot	O
do	O
this	O
for	O
free	O
if	O
one	O
wishes	O
to	O
choose	O
between	O
two	O
codes	O
then	O
it	O
is	O
necessary	O
to	O
lengthen	O
the	O
message	O
in	O
a	O
way	O
that	O
indicates	O
which	O
of	O
the	O
two	O
codes	O
is	O
being	O
used	O
if	O
we	O
indicate	O
this	O
choice	O
by	O
a	O
single	O
leading	O
bit	B
it	O
will	O
be	O
found	O
that	O
the	O
resulting	O
code	O
is	O
suboptimal	O
because	O
it	O
is	O
incomplete	O
is	O
it	O
fails	O
the	O
kraft	B
equality	O
exercise	O
prove	O
that	O
this	O
metacode	B
is	O
incomplete	O
and	O
explain	O
why	O
this	O
combined	O
code	O
is	O
suboptimal	O
solutions	O
solution	O
to	O
exercise	O
yes	O
is	O
uniquely	B
decodeable	I
even	O
though	O
it	O
is	O
not	O
a	O
code	O
because	O
no	O
two	O
strings	O
can	O
map	O
onto	O
the	O
same	O
string	O
only	O
the	O
codeword	B
contains	O
the	O
symbol	O
solution	O
to	O
exercise	O
we	O
wish	O
to	O
prove	O
that	O
for	O
any	O
set	B
of	O
codeword	B
lengths	O
flig	O
satisfying	O
the	O
kraft	B
inequality	B
there	O
is	O
a	O
code	O
having	O
those	O
lengths	O
this	O
is	O
readily	O
proved	O
by	O
thinking	O
of	O
the	O
codewords	O
illustrated	O
in	O
as	O
being	O
in	O
a	O
codeword	B
supermarket	B
with	O
size	O
indicating	O
cost	O
we	O
imagine	O
purchasing	O
codewords	O
one	O
at	O
a	O
time	O
starting	O
from	O
the	O
shortest	O
codewords	O
the	O
biggest	O
purchases	O
using	O
the	O
budget	B
shown	O
at	O
the	O
right	O
of	O
we	O
start	O
at	O
one	O
side	O
of	O
the	O
codeword	B
supermarket	B
say	O
the	O
pi	O
ai	O
cai	O
a	O
b	O
c	O
d	O
hpi	O
li	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
figure	O
the	O
codeword	B
supermarket	B
and	O
the	O
symbol	O
coding	O
budget	B
the	O
cost	O
of	O
each	O
codeword	B
length	O
l	O
is	O
indicated	O
by	O
the	O
size	O
of	O
the	O
box	B
it	O
is	O
written	O
in	O
the	O
total	O
budget	B
available	O
when	O
making	O
a	O
uniquely	B
decodeable	I
code	O
is	O
figure	O
proof	O
that	O
coding	O
makes	O
an	O
optimal	B
symbol	B
code	I
we	O
assume	O
that	O
the	O
rival	O
code	O
which	O
is	O
said	O
to	O
be	O
optimal	B
assigns	O
unequal	O
length	O
codewords	O
to	O
the	O
two	O
symbols	O
with	O
smallest	O
probability	B
a	O
and	O
b	O
by	O
interchanging	O
codewords	O
a	O
and	O
c	O
of	O
the	O
rival	O
code	O
where	O
c	O
is	O
a	O
symbol	O
with	O
rival	O
codelength	O
as	O
long	O
as	O
b	O
s	O
we	O
can	O
make	O
a	O
code	O
better	O
than	O
the	O
rival	O
code	O
this	O
shows	O
that	O
the	O
rival	O
code	O
was	O
not	O
optimal	B
solutions	O
t	O
e	O
g	O
d	O
u	O
b	O
e	O
d	O
o	O
c	O
l	O
o	O
b	O
m	O
y	O
s	O
l	O
a	O
t	O
o	O
t	O
e	O
h	O
t	O
symbol	O
probability	B
a	O
b	O
c	O
pa	O
pb	O
pc	O
codewords	O
cha	O
chb	O
chc	O
rival	O
code	O
s	O
rival	O
codewords	O
code	O
cra	O
crb	O
crc	O
crc	O
crb	O
cra	O
top	O
and	O
purchase	O
the	O
codeword	B
of	O
the	O
required	O
length	O
we	O
advance	O
down	O
the	O
supermarket	B
a	O
distance	B
and	O
purchase	O
the	O
next	O
codeword	B
of	O
the	O
next	O
required	O
length	O
and	O
so	O
forth	O
because	O
the	O
codeword	B
lengths	O
are	O
getting	O
longer	O
and	O
the	O
corresponding	O
intervals	B
are	O
getting	O
shorter	O
we	O
can	O
always	O
buy	O
an	O
adjacent	O
codeword	B
to	O
the	O
latest	O
purchase	O
so	O
there	O
is	O
no	O
wasting	O
of	O
the	O
budget	B
thus	O
at	O
the	O
ith	O
codeword	B
we	O
have	O
advanced	O
a	O
distance	B
pi	O
down	O
the	O
supermarket	B
ifp	O
we	O
will	O
have	O
purchased	O
all	O
the	O
codewords	O
without	O
running	O
out	O
of	O
budget	B
solution	O
to	O
exercise	O
the	O
proof	O
that	O
coding	O
is	O
optimal	B
depends	O
on	O
proving	O
that	O
the	O
key	O
step	O
in	O
the	O
algorithm	O
the	O
decision	O
to	O
give	O
the	O
two	O
symbols	O
with	O
smallest	O
probability	B
equal	O
encoded	O
lengths	O
cannot	O
lead	O
to	O
a	O
larger	O
expected	O
length	O
than	O
any	O
other	O
code	O
we	O
can	O
prove	O
this	O
by	O
contradiction	O
assume	O
that	O
the	O
two	O
symbols	O
with	O
smallest	O
probability	B
called	O
a	O
and	O
b	O
to	O
which	O
the	O
algorithm	O
would	O
assign	O
equal	O
length	O
codewords	O
do	O
not	O
have	O
equal	O
lengths	O
in	O
any	O
optimal	B
symbol	B
code	I
the	O
optimal	B
symbol	B
code	I
is	O
some	O
other	O
rival	O
code	O
in	O
which	O
these	O
two	O
codewords	O
have	O
unequal	O
lengths	O
la	O
and	O
lb	O
with	O
la	O
lb	O
without	O
loss	O
of	O
generality	O
we	O
can	O
assume	O
that	O
this	O
other	O
code	O
is	O
a	O
complete	O
code	O
because	O
any	O
codelengths	O
of	O
a	O
uniquely	B
decodeable	I
code	O
can	O
be	O
realized	O
by	O
a	O
code	O
in	O
this	O
rival	O
code	O
there	O
must	O
be	O
some	O
other	O
symbol	O
c	O
whose	O
probability	B
pc	O
is	O
greater	O
than	O
pa	O
and	O
whose	O
length	O
in	O
the	O
rival	O
code	O
is	O
greater	O
than	O
or	O
equal	O
to	O
lb	O
because	O
the	O
code	O
for	O
b	O
must	O
have	O
an	O
adjacent	O
codeword	B
of	O
equal	O
or	O
greater	O
length	O
a	O
complete	O
code	O
never	O
has	O
a	O
solo	O
codeword	B
of	O
the	O
maximum	O
length	O
consider	O
exchanging	O
the	O
codewords	O
of	O
a	O
and	O
c	O
so	O
that	O
a	O
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
encoded	O
with	O
the	O
longer	O
codeword	B
that	O
was	O
c	O
s	O
and	O
c	O
which	O
is	O
more	O
probable	O
than	O
a	O
gets	O
the	O
shorter	O
codeword	B
clearly	O
this	O
reduces	O
the	O
expected	O
length	O
of	O
the	O
code	O
the	O
change	O
in	O
expected	O
length	O
is	O
pclc	O
la	O
thus	O
we	O
have	O
contradicted	O
the	O
assumption	O
that	O
the	O
rival	O
code	O
is	O
optimal	B
therefore	O
it	O
is	O
valid	O
to	O
give	O
the	O
two	O
symbols	O
with	O
smallest	O
probability	B
equal	O
encoded	O
lengths	O
coding	O
produces	O
optimal	B
symbol	O
codes	O
solution	O
to	O
exercise	O
a	O
code	O
for	O
x	O
where	O
ax	O
and	O
px	O
is	O
this	O
code	O
has	O
lc	O
x	O
whereas	O
the	O
entropy	B
hx	O
is	O
a	O
code	O
for	O
x	O
is	O
this	O
has	O
expected	O
length	O
lc	O
x	O
whereas	O
the	O
entropy	B
hx	O
is	O
a	O
code	O
for	O
x	O
maps	O
the	O
sixteen	O
source	O
strings	O
to	O
the	O
following	O
codelengths	O
this	O
has	O
expected	O
length	O
lc	O
x	O
whereas	O
the	O
entropy	B
hx	O
is	O
when	O
px	O
the	O
code	O
for	O
x	O
has	O
lengths	O
the	O
expected	O
length	O
is	O
bits	O
and	O
the	O
entropy	B
is	O
bits	O
a	O
code	O
for	O
x	O
is	O
shown	O
in	O
table	O
the	O
expected	O
length	O
is	O
bits	O
and	O
the	O
entropy	B
is	O
bits	O
solution	O
to	O
exercise	O
the	O
set	B
of	O
probabilities	O
gives	O
rise	O
to	O
two	O
optimal	B
sets	O
of	O
codelengths	O
because	O
at	O
the	O
second	O
step	O
of	O
the	O
coding	O
algorithm	O
we	O
can	O
choose	O
any	O
of	O
the	O
three	O
possible	O
pairings	O
we	O
may	O
either	O
put	O
them	O
in	O
a	O
constant	O
length	O
code	O
or	O
the	O
code	O
both	O
codes	O
have	O
expected	O
length	O
another	O
solution	O
is	O
and	O
a	O
third	O
is	O
solution	O
to	O
exercise	O
let	O
pmax	O
be	O
the	O
largest	O
probability	B
in	O
pi	O
the	O
between	O
the	O
expected	O
length	O
l	O
and	O
the	O
entropy	B
h	O
can	O
be	O
no	O
bigger	O
than	O
maxpmax	O
see	O
exercises	O
to	O
understand	O
where	O
the	O
curious	O
comes	O
from	O
solution	O
to	O
exercise	O
length	O
entropy	B
solution	O
to	O
exercise	O
there	O
are	O
two	O
ways	O
to	O
answer	O
this	O
problem	O
correctly	O
and	O
one	O
popular	O
way	O
to	O
answer	O
it	O
incorrectly	O
let	O
s	O
give	O
the	O
incorrect	O
answer	O
erroneous	O
answer	O
can	O
pick	O
a	O
random	B
bit	B
by	O
picking	O
a	O
random	B
source	O
symbol	O
xi	O
with	O
probability	B
pi	O
then	O
picking	O
a	O
random	B
bit	B
from	O
cxi	O
if	O
we	O
fi	O
to	O
be	O
the	O
fraction	O
of	O
the	O
bits	O
of	O
cxi	O
that	O
are	O
we	O
p	O
is	O
xi	O
pifi	O
ai	O
pi	O
li	O
cai	O
table	O
code	O
for	O
x	O
when	O
column	O
shows	O
the	O
assigned	O
codelengths	O
and	O
column	O
the	O
codewords	O
some	O
strings	O
whose	O
probabilities	O
are	O
identical	O
e	O
g	O
the	O
fourth	O
and	O
receive	O
codelengths	O
ai	O
cai	O
a	O
b	O
c	O
d	O
pi	O
li	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
this	O
answer	O
is	O
wrong	O
because	O
it	O
falls	O
for	O
the	O
bus-stop	B
fallacy	O
which	O
was	O
introduced	O
in	O
exercise	O
if	O
buses	O
arrive	O
at	O
random	B
and	O
we	O
are	O
interested	O
in	O
the	O
average	O
time	O
from	O
one	O
bus	O
until	O
the	O
next	O
we	O
must	O
distinguish	O
two	O
possible	O
averages	O
the	O
average	O
time	O
from	O
a	O
randomly	O
chosen	O
bus	O
until	O
the	O
next	O
the	O
average	O
time	O
between	O
the	O
bus	O
you	O
just	O
missed	O
and	O
the	O
next	O
bus	O
the	O
second	O
average	O
is	O
twice	O
as	O
big	O
as	O
the	O
because	O
by	O
waiting	B
for	I
a	I
bus	I
at	O
a	O
random	B
time	O
you	O
bias	B
your	O
selection	O
of	O
a	O
bus	O
in	O
favour	O
of	O
buses	O
that	O
follow	O
a	O
large	O
gap	O
you	O
re	O
unlikely	O
to	O
catch	O
a	O
bus	O
that	O
comes	O
seconds	O
after	O
a	O
preceding	O
bus	O
similarly	O
the	O
symbols	O
c	O
and	O
d	O
get	O
encoded	O
into	O
longer-length	O
binary	O
strings	O
than	O
a	O
so	O
when	O
we	O
pick	O
a	O
bit	B
from	O
the	O
compressed	O
string	O
at	O
random	B
we	O
are	O
more	O
likely	O
to	O
land	O
in	O
a	O
bit	B
belonging	O
to	O
a	O
c	O
or	O
a	O
d	O
than	O
would	O
be	O
given	O
by	O
the	O
probabilities	O
pi	O
in	O
the	O
expectation	B
all	O
the	O
probabilities	O
need	O
to	O
be	O
scaled	O
up	O
by	O
li	O
and	O
renormalized	O
correct	O
answer	O
in	O
the	O
same	O
style	O
every	O
time	O
symbol	O
xi	O
is	O
encoded	O
li	O
bits	O
are	O
added	O
to	O
the	O
binary	O
string	O
of	O
which	O
fili	O
are	O
the	O
expected	O
number	O
of	O
added	O
per	O
symbol	O
is	O
pifili	O
xi	O
and	O
the	O
expected	O
total	O
number	O
of	O
bits	O
added	O
per	O
symbol	O
is	O
pili	O
xi	O
so	O
the	O
fraction	O
of	O
in	O
the	O
transmitted	O
string	O
is	O
p	O
is	O
pi	O
pifili	O
pi	O
pili	O
for	O
a	O
general	O
symbol	B
code	I
and	O
a	O
general	O
ensemble	B
the	O
expectation	B
is	O
the	O
correct	O
answer	O
but	O
in	O
this	O
case	O
we	O
can	O
use	O
a	O
more	O
powerful	O
argument	O
information-theoretic	O
answer	O
the	O
encoded	O
string	O
c	O
is	O
the	O
output	O
of	O
an	O
optimal	B
compressor	O
that	O
compresses	O
samples	O
from	O
x	O
down	O
to	O
an	O
expected	O
length	O
of	O
hx	O
bits	O
we	O
can	O
t	O
expect	O
to	O
compress	B
this	O
data	O
any	O
further	O
but	O
if	O
the	O
probability	B
p	O
is	O
were	O
not	O
equal	O
to	O
then	O
it	O
would	O
be	O
possible	O
to	O
compress	B
the	O
binary	O
string	O
further	O
a	O
block	B
compression	B
code	O
say	O
therefore	O
p	O
is	O
must	O
be	O
equal	O
to	O
indeed	O
the	O
probability	B
of	O
any	O
sequence	B
of	O
l	O
bits	O
in	O
the	O
compressed	O
stream	O
taking	O
on	O
any	O
particular	O
value	O
must	O
be	O
the	O
output	O
of	O
a	O
perfect	B
compressor	O
is	O
always	O
perfectly	O
random	B
bits	O
to	O
put	O
it	O
another	O
way	O
if	O
the	O
probability	B
p	O
is	O
were	O
not	O
equal	O
to	O
then	O
the	O
information	B
content	I
per	O
bit	B
of	O
the	O
compressed	O
string	O
would	O
be	O
at	O
most	O
which	O
would	O
be	O
less	O
than	O
but	O
this	O
contradicts	O
the	O
fact	O
that	O
we	O
can	O
recover	O
the	O
original	O
data	O
from	O
c	O
so	O
the	O
information	B
content	I
per	O
bit	B
of	O
the	O
compressed	O
string	O
must	O
be	O
hxlc	O
x	O
solution	O
to	O
exercise	O
the	O
general	O
coding	O
algorithm	O
for	O
an	O
encoding	O
alphabet	O
with	O
q	O
symbols	O
has	O
one	O
from	O
the	O
binary	O
case	O
the	O
process	O
of	O
combining	O
q	O
symbols	O
into	O
symbol	O
reduces	O
the	O
number	O
of	O
symbols	O
by	O
so	O
if	O
we	O
start	O
with	O
a	O
symbols	O
we	O
ll	O
only	O
end	O
up	O
with	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
symbol	O
codes	O
complete	O
q-ary	O
tree	B
if	O
a	O
mod	O
is	O
equal	O
to	O
otherwise	O
we	O
know	O
that	O
whatever	O
code	O
we	O
make	O
it	O
must	O
be	O
an	O
incomplete	O
tree	B
with	O
a	O
number	O
of	O
missing	O
leaves	O
equal	O
modulo	O
to	O
a	O
mod	O
for	O
example	O
if	O
a	O
ternary	O
tree	B
is	O
built	O
for	O
eight	O
symbols	O
then	O
there	O
will	O
unavoidably	O
be	O
one	O
missing	O
leaf	B
in	O
the	O
tree	B
the	O
optimal	B
q-ary	O
code	O
is	O
made	O
by	O
putting	O
these	O
extra	O
leaves	O
in	O
the	O
longest	O
branch	O
of	O
the	O
tree	B
this	O
can	O
be	O
achieved	O
by	O
adding	O
the	O
appropriate	O
number	O
of	O
symbols	O
to	O
the	O
original	O
source	O
symbol	O
set	B
all	O
of	O
these	O
extra	O
symbols	O
having	O
probability	B
zero	O
the	O
total	O
number	O
of	O
leaves	O
is	O
then	O
equal	O
to	O
for	O
some	O
integer	O
r	O
the	O
symbols	O
are	O
then	O
repeatedly	O
combined	O
by	O
taking	O
the	O
q	O
symbols	O
with	O
smallest	O
probability	B
and	O
replacing	O
them	O
by	O
a	O
single	O
symbol	O
as	O
in	O
the	O
binary	O
coding	O
algorithm	O
solution	O
to	O
exercise	O
we	O
wish	O
to	O
show	O
that	O
a	O
greedy	O
metacode	B
which	O
picks	O
the	O
code	O
which	O
gives	O
the	O
shortest	O
encoding	O
is	O
actually	O
suboptimal	O
because	O
it	O
violates	O
the	O
kraft	B
inequality	B
we	O
ll	O
assume	O
that	O
each	O
symbol	O
x	O
is	O
assigned	O
lengths	O
lkx	O
by	O
each	O
of	O
the	O
candidate	O
codes	O
ck	O
let	O
us	O
assume	O
there	O
are	O
k	O
alternative	O
codes	O
and	O
that	O
we	O
can	O
encode	O
which	O
code	O
is	O
being	O
used	O
with	O
a	O
header	O
of	O
length	O
log	O
k	O
bits	O
then	O
the	O
metacode	B
assigns	O
lengths	O
that	O
are	O
given	O
by	O
k	O
min	O
k	O
lkx	O
we	O
compute	O
the	O
kraft	B
sum	O
mink	O
lkx	O
s	O
k	O
xx	O
let	O
s	O
divide	O
the	O
set	B
ax	O
into	O
non-overlapping	O
subsets	O
fakgk	O
ak	O
contains	O
all	O
the	O
symbols	O
x	O
that	O
the	O
metacode	B
sends	O
via	O
code	O
k	O
then	O
such	O
that	O
subset	B
now	O
if	O
one	O
sub-code	O
k	O
the	O
kraft	B
equality	O
must	O
be	O
the	O
case	O
that	O
then	O
it	O
with	O
equality	O
only	O
if	O
all	O
the	O
symbols	O
x	O
are	O
in	O
ak	O
which	O
would	O
mean	B
that	O
we	O
are	O
only	O
using	O
one	O
of	O
the	O
k	O
codes	O
so	O
s	O
k	O
xk	O
s	O
k	O
k	O
with	O
equality	O
only	O
if	O
equation	O
is	O
an	O
equality	O
for	O
all	O
codes	O
k	O
but	O
it	O
s	O
impossible	O
for	O
all	O
the	O
symbols	O
to	O
be	O
in	O
all	O
the	O
non-overlapping	O
subsets	O
fakgk	O
so	O
we	O
can	O
t	O
have	O
equality	O
holding	O
for	O
all	O
k	O
so	O
s	O
another	O
way	O
of	O
seeing	O
that	O
a	O
mixture	O
code	O
is	O
suboptimal	O
is	O
to	O
consider	O
the	O
binary	O
tree	B
that	O
it	O
think	O
of	O
the	O
special	O
case	O
of	O
two	O
codes	O
the	O
bit	B
we	O
send	O
which	O
code	O
we	O
are	O
using	O
now	O
in	O
a	O
complete	O
code	O
any	O
subsequent	O
binary	O
string	O
is	O
a	O
valid	O
string	O
but	O
once	O
we	O
know	O
that	O
we	O
are	O
using	O
say	O
code	O
a	O
we	O
know	O
that	O
what	O
follows	O
can	O
only	O
be	O
a	O
codeword	B
corresponding	O
to	O
a	O
symbol	O
x	O
whose	O
encoding	O
is	O
shorter	O
under	O
code	O
a	O
than	O
code	O
b	O
so	O
some	O
strings	O
are	O
invalid	O
continuations	O
and	O
the	O
mixture	O
code	O
is	O
incomplete	O
and	O
suboptimal	O
for	O
further	O
discussion	O
of	O
this	O
issue	O
and	O
its	O
relationship	O
to	O
probabilistic	O
modelling	B
read	O
about	O
bits	B
back	I
coding	O
in	O
section	B
and	O
in	O
frey	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
before	O
reading	O
chapter	O
you	O
should	O
have	O
read	O
the	O
previous	O
chapter	O
and	O
worked	O
on	O
most	O
of	O
the	O
exercises	O
in	O
it	O
we	O
ll	O
also	O
make	O
use	O
of	O
some	O
bayesian	B
modelling	B
ideas	O
that	O
arrived	O
in	O
the	O
vicinity	O
of	O
exercise	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
in	O
this	O
chapter	O
we	O
discuss	O
two	O
data	O
compression	B
schemes	O
arithmetic	B
coding	I
is	O
a	O
beautiful	O
method	O
that	O
goes	O
hand	O
in	O
hand	O
with	O
the	O
philosophy	B
that	O
compression	B
of	O
data	O
from	O
a	O
source	O
entails	O
probabilistic	O
modelling	B
of	O
that	O
source	O
as	O
of	O
the	O
best	O
compression	B
methods	O
for	O
text	O
use	O
arithmetic	B
coding	I
and	O
several	O
state-of-the-art	O
image	B
compression	B
systems	O
use	O
it	O
too	O
lempelziv	O
coding	O
is	O
a	O
universal	B
method	O
designed	O
under	O
the	O
philosophy	B
that	O
we	O
would	O
like	O
a	O
single	O
compression	B
algorithm	O
that	O
will	O
do	O
a	O
reasonable	O
job	O
for	O
any	O
source	O
in	O
fact	O
for	O
many	O
real	O
life	B
sources	O
this	O
algorithm	O
s	O
universal	B
properties	O
hold	O
only	O
in	O
the	O
limit	O
of	O
unfeasibly	O
large	O
amounts	O
of	O
data	O
but	O
all	O
the	O
same	O
lempelziv	O
compression	B
is	O
widely	O
used	O
and	O
often	O
the	O
guessing	B
game	I
as	O
a	O
motivation	O
for	O
these	O
two	O
compression	B
methods	O
consider	O
the	O
redundancy	B
in	O
a	O
typical	B
english	B
text	O
such	O
have	O
redundancy	B
at	O
several	O
levels	O
for	O
example	O
they	O
contain	O
the	O
ascii	O
characters	O
with	O
non-equal	O
frequency	B
certain	O
consecutive	O
pairs	O
of	O
letters	O
are	O
more	O
probable	O
than	O
others	O
and	O
entire	O
words	O
can	O
be	O
predicted	O
given	O
the	O
context	O
and	O
a	O
semantic	O
understanding	O
of	O
the	O
text	O
to	O
illustrate	O
the	O
redundancy	B
of	O
english	B
and	O
a	O
curious	O
way	O
in	O
which	O
it	O
could	O
be	O
compressed	O
we	O
can	O
imagine	O
a	O
guessing	B
game	I
in	O
which	O
an	O
english	B
speaker	O
repeatedly	O
attempts	O
to	O
predict	O
the	O
next	O
character	O
in	O
a	O
text	O
for	O
simplicity	O
let	O
us	O
assume	O
that	O
the	O
allowed	O
alphabet	O
consists	O
of	O
the	O
upper	O
case	O
letters	O
abc	O
z	O
and	O
a	O
space	O
the	O
game	O
involves	O
asking	O
the	O
subject	O
to	O
guess	O
the	O
next	O
character	O
repeatedly	O
the	O
only	O
feedback	B
being	O
whether	O
the	O
guess	O
is	O
correct	O
or	O
not	O
until	O
the	O
character	O
is	O
correctly	O
guessed	O
after	O
a	O
correct	O
guess	O
we	O
note	O
the	O
number	O
of	O
guesses	O
that	O
were	O
made	O
when	O
the	O
character	O
was	O
and	O
ask	O
the	O
subject	O
to	O
guess	O
the	O
next	O
character	O
in	O
the	O
same	O
way	O
one	O
sentence	O
gave	O
the	O
following	O
result	O
when	O
a	O
human	B
was	O
asked	O
to	O
guess	O
a	O
sentence	O
the	O
numbers	O
of	O
guesses	O
are	O
listed	O
below	O
each	O
character	O
t	O
h	O
e	O
r	O
e	O
i	O
s	O
n	O
o	O
r	O
e	O
v	O
e	O
r	O
s	O
e	O
o	O
n	O
a	O
m	O
o	O
t	O
o	O
r	O
c	O
y	O
c	O
l	O
e	O
notice	O
that	O
in	O
many	O
cases	O
the	O
next	O
letter	O
is	O
guessed	O
immediately	O
in	O
one	O
guess	O
in	O
other	O
cases	O
particularly	O
at	O
the	O
start	O
of	O
syllables	O
more	O
guesses	O
are	O
needed	O
what	O
do	O
this	O
game	O
and	O
these	O
results	O
us	O
first	O
they	O
demonstrate	O
the	O
redundancy	B
of	O
english	B
from	O
the	O
point	O
of	O
view	O
of	O
an	O
english	B
speaker	O
second	O
this	O
game	O
might	O
be	O
used	O
in	O
a	O
data	O
compression	B
scheme	O
as	O
follows	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
arithmetic	O
codes	O
the	O
string	O
of	O
numbers	O
listed	O
above	O
was	O
obtained	O
by	O
presenting	O
the	O
text	O
to	O
the	O
subject	O
the	O
maximum	O
number	O
of	O
guesses	O
that	O
the	O
subject	O
will	O
make	O
for	O
a	O
given	O
letter	O
is	O
twenty-seven	O
so	O
what	O
the	O
subject	O
is	O
doing	O
for	O
us	O
is	O
performing	O
a	O
time-varying	O
mapping	B
of	O
the	O
twenty-seven	O
letters	O
fa	O
b	O
c	O
onto	O
the	O
twenty-seven	O
numbers	O
which	O
we	O
can	O
view	O
as	O
symbols	O
in	O
a	O
new	O
alphabet	O
the	O
total	O
number	O
of	O
symbols	O
has	O
not	O
been	O
reduced	O
but	O
since	O
he	O
uses	O
some	O
of	O
these	O
symbols	O
much	O
more	O
frequently	O
than	O
others	O
for	O
example	O
and	O
it	O
should	O
be	O
easy	O
to	O
compress	B
this	O
new	O
string	O
of	O
symbols	O
how	O
would	O
the	O
uncompression	B
of	O
the	O
sequence	B
of	O
numbers	O
work	O
at	O
uncompression	B
time	O
we	O
do	O
not	O
have	O
the	O
original	O
string	O
there	O
we	O
have	O
only	O
the	O
encoded	O
sequence	B
imagine	O
that	O
our	O
subject	O
has	O
an	O
absolutely	O
identical	B
twin	B
who	O
also	O
plays	O
the	O
guessing	B
game	I
with	O
us	O
as	O
if	O
we	O
knew	O
the	O
source	O
text	O
if	O
we	O
stop	O
him	O
whenever	O
he	O
has	O
made	O
a	O
number	O
of	O
guesses	O
equal	O
to	O
the	O
given	O
number	O
then	O
he	O
will	O
have	O
just	O
guessed	O
the	O
correct	O
letter	O
and	O
we	O
can	O
then	O
say	O
yes	O
that	O
s	O
right	O
and	O
move	O
to	O
the	O
next	O
character	O
alternatively	O
if	O
the	O
identical	B
twin	B
is	O
not	O
available	O
we	O
could	O
design	O
a	O
compression	B
system	O
with	O
the	O
help	O
of	O
just	O
one	O
human	B
as	O
follows	O
we	O
choose	O
a	O
window	B
length	O
l	O
that	O
is	O
a	O
number	O
of	O
characters	O
of	O
context	O
to	O
show	O
the	O
human	B
for	O
every	O
one	O
of	O
the	O
possible	O
strings	O
of	O
length	O
l	O
we	O
ask	O
them	O
what	O
would	O
you	O
predict	O
is	O
the	O
next	O
character	O
and	O
if	O
that	O
prediction	B
were	O
wrong	O
what	O
would	O
your	O
next	O
guesses	O
be	O
after	O
tabulating	O
their	O
answers	O
to	O
these	O
questions	O
we	O
could	O
use	O
two	O
copies	O
of	O
these	O
enormous	O
tables	O
at	O
the	O
encoder	B
and	O
the	O
decoder	B
in	O
place	O
of	O
the	O
two	O
human	B
twins	O
such	O
a	O
language	B
model	B
is	O
called	O
an	O
lth	O
order	O
markov	O
model	B
these	O
systems	O
are	O
clearly	O
unrealistic	O
for	O
practical	B
compression	B
but	O
they	O
illustrate	O
several	O
principles	O
that	O
we	O
will	O
make	O
use	O
of	O
now	O
arithmetic	O
codes	O
when	O
we	O
discussed	O
variable-length	B
symbol	O
codes	O
and	O
the	O
optimal	B
algorithm	O
for	O
constructing	O
them	O
we	O
concluded	O
by	O
pointing	O
out	O
two	O
practical	B
and	O
theoretical	O
problems	O
with	O
codes	O
these	O
defects	O
are	O
by	O
arithmetic	O
codes	O
which	O
were	O
invented	O
by	O
elias	B
by	O
rissanen	B
and	O
by	O
pasco	O
and	O
subsequently	O
made	O
practical	B
by	O
witten	O
et	O
al	O
in	O
an	O
arithmetic	O
code	O
the	O
probabilistic	O
modelling	B
is	O
clearly	O
separated	O
from	O
the	O
encoding	O
operation	O
the	O
system	O
is	O
rather	O
similar	O
to	O
the	O
guessing	B
game	I
the	O
human	B
predictor	O
is	O
replaced	O
by	O
a	O
probabilistic	B
model	B
of	O
the	O
source	O
as	O
each	O
symbol	O
is	O
produced	O
by	O
the	O
source	O
the	O
probabilistic	B
model	B
supplies	O
a	O
predictive	B
distribution	B
over	O
all	O
possible	O
values	O
of	O
the	O
next	O
symbol	O
that	O
is	O
a	O
list	O
of	O
positive	O
numbers	O
fpig	O
that	O
sum	O
to	O
one	O
if	O
we	O
choose	O
to	O
model	B
the	O
source	O
as	O
producing	O
i	O
i	O
d	O
symbols	O
with	O
some	O
known	O
distribution	B
then	O
the	O
predictive	B
distribution	B
is	O
the	O
same	O
every	O
time	O
but	O
arithmetic	B
coding	I
can	O
with	O
equal	O
ease	O
handle	O
complex	B
adaptive	B
models	I
that	O
produce	O
context-dependent	O
predictive	O
distributions	O
the	O
predictive	O
model	B
is	O
usually	O
implemented	O
in	O
a	O
computer	B
program	O
the	O
encoder	B
makes	O
use	O
of	O
the	O
model	B
s	O
predictions	O
to	O
create	O
a	O
binary	O
string	O
the	O
decoder	B
makes	O
use	O
of	O
an	O
identical	B
twin	B
of	O
the	O
model	B
as	O
in	O
the	O
guessing	B
game	I
to	O
interpret	O
the	O
binary	O
string	O
let	O
the	O
source	O
alphabet	O
be	O
ax	O
aig	O
and	O
let	O
the	O
ith	O
symbol	O
ai	O
have	O
the	O
special	O
meaning	O
end	O
of	O
transmission	O
the	O
source	O
spits	O
out	O
a	O
sequence	B
xn	O
the	O
source	O
does	O
not	O
necessarily	O
produce	O
i	O
i	O
d	O
symbols	O
we	O
will	O
assume	O
that	O
a	O
computer	B
program	O
is	O
provided	O
to	O
the	O
encoder	B
that	O
assigns	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
predictive	O
probability	B
distribution	B
over	O
ai	O
given	O
the	O
sequence	B
that	O
has	O
occurred	O
thus	O
far	O
p	O
ai	O
j	O
the	O
receiver	O
has	O
an	O
identical	O
program	O
that	O
produces	O
the	O
same	O
predictive	O
probability	B
distribution	B
p	O
ai	O
j	O
figure	O
binary	O
strings	O
real	O
intervals	B
within	O
the	O
real	O
line	O
we	O
encountered	O
a	O
picture	O
like	O
this	O
when	O
we	O
discussed	O
the	O
symbol-code	O
supermarket	B
in	O
chapter	O
concepts	O
for	O
understanding	O
arithmetic	B
coding	I
notation	B
for	O
intervals	B
the	O
interval	O
is	O
all	O
numbers	O
between	O
and	O
including	O
but	O
not	O
a	O
binary	O
transmission	O
an	O
interval	O
within	O
the	O
real	O
line	O
from	O
to	O
for	O
example	O
the	O
string	O
is	O
interpreted	O
as	O
a	O
binary	O
real	O
number	O
which	O
corresponds	O
to	O
the	O
interval	O
in	O
binary	O
i	O
e	O
the	O
interval	O
in	O
base	O
ten	O
the	O
longer	O
string	O
corresponds	O
to	O
a	O
smaller	O
interval	O
because	O
has	O
the	O
string	O
as	O
a	O
the	O
new	O
interval	O
is	O
a	O
sub-interval	O
of	O
the	O
interval	O
a	O
one-megabyte	O
binary	O
bits	O
is	O
thus	O
viewed	O
as	O
specifying	O
a	O
number	O
between	O
and	O
to	O
a	O
precision	B
of	O
about	O
two	O
million	O
decimal	O
places	O
two	O
million	O
decimal	O
digits	O
because	O
each	O
byte	B
translates	O
into	O
a	O
little	O
more	O
than	O
two	O
decimal	O
digits	O
now	O
we	O
can	O
also	O
divide	O
the	O
real	O
line	O
into	O
i	O
intervals	B
of	O
lengths	O
equal	O
to	O
the	O
probabilities	O
p	O
ai	O
as	O
shown	O
in	O
p	O
p	O
p	O
p	O
p	O
figure	O
a	O
probabilistic	B
model	B
real	O
intervals	B
within	O
the	O
real	O
line	O
we	O
may	O
then	O
take	O
each	O
interval	O
ai	O
and	O
subdivide	O
it	O
into	O
intervals	B
deis	O
proportional	O
to	O
indeed	O
the	O
length	O
of	O
the	O
interval	O
aiaj	O
will	O
be	O
precisely	O
noted	O
aiai	O
such	O
that	O
the	O
length	O
of	O
aiaj	O
p	O
aj	O
j	O
ai	O
the	O
joint	B
probability	B
p	O
ai	O
aj	O
p	O
aip	O
aj	O
j	O
ai	O
iterating	O
this	O
procedure	O
the	O
interval	O
can	O
be	O
divided	O
into	O
a	O
sequence	B
of	O
intervals	B
corresponding	O
to	O
all	O
possible	O
length	O
strings	O
xn	O
such	O
that	O
the	O
length	O
of	O
an	O
interval	O
is	O
equal	O
to	O
the	O
probability	B
of	O
the	O
string	O
given	O
our	O
model	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
algorithm	O
arithmetic	B
coding	I
iterative	O
procedure	O
to	O
the	O
interval	O
v	O
for	O
the	O
string	O
xn	O
arithmetic	O
codes	O
u	O
v	O
p	O
v	O
u	O
for	O
n	O
to	O
n	O
compute	O
the	O
cumulative	O
probabilities	O
qn	O
and	O
rn	O
v	O
u	O
prnxn	O
j	O
u	O
u	O
pqnxn	O
j	O
p	O
v	O
u	O
formulae	O
describing	O
arithmetic	B
coding	I
the	O
process	O
depicted	O
in	O
can	O
be	O
written	O
explicitly	O
as	O
follows	O
the	O
intervals	B
are	O
in	O
terms	O
of	O
the	O
lower	O
and	O
upper	O
cumulative	O
probabilities	O
qnai	O
j	O
rnai	O
j	O
i	O
p	O
j	O
p	O
j	O
as	O
the	O
nth	O
symbol	O
arrives	O
we	O
subdivide	O
the	O
interval	O
at	O
the	O
points	O
by	O
qn	O
and	O
rn	O
for	O
example	O
starting	O
with	O
the	O
symbol	O
the	O
intervals	B
and	O
ai	O
are	O
p	O
p	O
p	O
and	O
ai	O
p	O
algorithm	O
describes	O
the	O
general	O
procedure	O
to	O
encode	O
a	O
string	O
xn	O
we	O
locate	O
the	O
interval	O
corresponding	O
to	O
xn	O
and	O
send	O
a	O
binary	O
string	O
whose	O
interval	O
lies	O
within	O
that	O
interval	O
this	O
encoding	O
can	O
be	O
performed	O
on	O
the	O
as	O
we	O
now	O
illustrate	O
example	O
compressing	O
the	O
tosses	O
of	O
a	O
bent	B
coin	B
imagine	O
that	O
we	O
watch	O
as	O
a	O
bent	B
coin	B
is	O
tossed	O
some	O
number	O
of	O
times	O
example	O
and	O
section	B
the	O
two	O
outcomes	O
when	O
the	O
coin	B
is	O
tossed	O
are	O
denoted	O
a	O
and	O
b	O
a	O
third	O
possibility	O
is	O
that	O
the	O
experiment	O
is	O
halted	O
an	O
event	O
denoted	O
by	O
the	O
end	O
of	O
symbol	O
because	O
the	O
coin	B
is	O
bent	O
we	O
expect	O
that	O
the	O
probabilities	O
of	O
the	O
outcomes	O
a	O
and	O
b	O
are	O
not	O
equal	O
though	O
beforehand	O
we	O
don	O
t	O
know	O
which	O
is	O
the	O
more	O
probable	O
outcome	O
encoding	O
let	O
the	O
source	O
string	O
be	O
we	O
pass	O
along	O
the	O
string	O
one	O
symbol	O
at	O
a	O
time	O
and	O
use	O
our	O
model	B
to	O
compute	O
the	O
probability	B
distribution	B
of	O
the	O
next	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
symbol	O
given	O
the	O
string	O
thus	O
far	O
let	O
these	O
probabilities	O
be	O
context	O
thus	O
far	O
probability	B
of	O
next	O
symbol	O
b	O
bb	O
bbb	O
bbba	O
p	O
p	O
b	O
p	O
bb	O
p	O
bbb	O
p	O
bbba	O
p	O
p	O
b	O
p	O
bb	O
p	O
bbb	O
p	O
bbba	O
p	O
p	O
b	O
p	O
bb	O
p	O
bbb	O
p	O
bbba	O
figure	O
shows	O
the	O
corresponding	O
intervals	B
the	O
interval	O
b	O
is	O
the	O
middle	O
of	O
the	O
interval	O
bb	O
is	O
the	O
middle	O
of	O
b	O
and	O
so	O
forth	O
b	O
b	O
b	O
b	O
a	O
b	O
ba	O
bba	O
bbba	O
bb	O
bbb	O
bbbb	O
figure	O
illustration	O
of	O
the	O
arithmetic	B
coding	I
process	O
as	O
the	O
sequence	B
is	O
transmitted	O
bbbaa	O
bbba	O
bbbab	O
c	O
c	O
cco	O
when	O
the	O
symbol	O
b	O
is	O
observed	O
the	O
encoder	B
knows	O
that	O
the	O
encoded	O
string	O
will	O
start	O
or	O
but	O
does	O
not	O
know	O
which	O
the	O
encoder	B
writes	O
nothing	O
for	O
the	O
time	O
being	O
and	O
examines	O
the	O
next	O
symbol	O
which	O
is	O
b	O
the	O
interval	O
bb	O
lies	O
wholly	O
within	O
interval	O
so	O
the	O
encoder	B
can	O
write	O
the	O
bit	B
the	O
third	O
symbol	O
b	O
narrows	O
down	O
the	O
interval	O
a	O
little	O
but	O
not	O
quite	O
enough	O
for	O
it	O
to	O
lie	O
wholly	O
within	O
interval	O
only	O
when	O
the	O
next	O
a	O
is	O
read	O
from	O
the	O
source	O
can	O
we	O
transmit	O
some	O
more	O
bits	O
interval	O
bbba	O
lies	O
wholly	O
within	O
the	O
interval	O
so	O
the	O
encoder	B
adds	O
to	O
the	O
it	O
has	O
written	O
finally	O
when	O
the	O
arrives	O
we	O
need	O
a	O
procedure	O
for	O
terminating	O
the	O
encoding	O
magnifying	O
the	O
interval	O
right	O
we	O
note	O
that	O
the	O
marked	O
interval	O
is	O
wholly	O
contained	O
by	O
so	O
the	O
encoding	O
can	O
be	O
completed	O
by	O
appending	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
arithmetic	O
codes	O
exercise	O
show	O
that	O
the	O
overhead	O
required	O
to	O
terminate	O
a	O
message	O
is	O
never	O
more	O
than	O
bits	O
relative	B
to	O
the	O
ideal	O
message	O
length	O
given	O
the	O
probabilistic	B
model	B
h	O
hxjh	O
this	O
is	O
an	O
important	O
result	O
arithmetic	B
coding	I
is	O
very	O
nearly	O
optimal	B
the	O
message	O
length	O
is	O
always	O
within	O
two	O
bits	O
of	O
the	O
shannon	B
information	B
content	I
of	O
the	O
entire	O
source	O
string	O
so	O
the	O
expected	O
message	O
length	O
is	O
within	O
two	O
bits	O
of	O
the	O
entropy	B
of	O
the	O
entire	O
message	O
decoding	B
the	O
decoder	B
receives	O
the	O
string	O
and	O
passes	O
along	O
it	O
one	O
symbol	O
at	O
a	O
time	O
first	O
the	O
probabilities	O
p	O
p	O
p	O
are	O
computed	O
using	O
the	O
identical	O
program	O
that	O
the	O
encoder	B
used	O
and	O
the	O
intervals	B
a	O
b	O
and	O
are	O
deduced	O
once	O
the	O
two	O
bits	O
have	O
been	O
examined	O
it	O
is	O
certain	O
that	O
the	O
original	O
string	O
must	O
have	O
been	O
started	O
with	O
a	O
b	O
since	O
the	O
interval	O
lies	O
wholly	O
within	O
interval	O
b	O
the	O
decoder	B
can	O
then	O
use	O
the	O
model	B
to	O
compute	O
p	O
b	O
p	O
b	O
p	O
b	O
and	O
deduce	O
the	O
boundaries	O
of	O
the	O
intervals	B
ba	O
bb	O
and	O
continuing	O
we	O
decode	O
the	O
second	O
b	O
once	O
we	O
reach	O
the	O
third	O
b	O
once	O
we	O
reach	O
and	O
so	O
forth	O
with	O
the	O
unambiguous	O
of	O
once	O
the	O
whole	O
binary	O
string	O
has	O
been	O
read	O
with	O
the	O
convention	O
that	O
denotes	O
the	O
end	O
of	O
the	O
message	O
the	O
decoder	B
knows	O
to	O
stop	O
decoding	B
transmission	O
of	O
multiple	O
how	O
might	O
one	O
use	O
arithmetic	B
coding	I
to	O
communicate	O
several	O
distinct	O
over	O
the	O
binary	O
channel	O
once	O
the	O
character	O
has	O
been	O
transmitted	O
we	O
imagine	O
that	O
the	O
decoder	B
is	O
reset	O
into	O
its	O
initial	O
state	O
there	O
is	O
no	O
transfer	O
of	O
the	O
learnt	O
statistics	O
of	O
the	O
to	O
the	O
second	O
if	O
however	O
we	O
did	O
believe	O
that	O
there	O
is	O
a	O
relationship	O
among	O
the	O
that	O
we	O
are	O
going	O
to	O
compress	B
we	O
could	O
our	O
alphabet	O
introducing	O
a	O
second	O
character	O
that	O
marks	O
the	O
end	O
of	O
the	O
but	O
instructs	O
the	O
encoder	B
and	O
decoder	B
to	O
continue	O
using	O
the	O
same	O
probabilistic	B
model	B
the	O
big	O
picture	O
notice	O
that	O
to	O
communicate	O
a	O
string	O
of	O
n	O
letters	O
both	O
the	O
encoder	B
and	O
the	O
decoder	B
needed	O
to	O
compute	O
only	O
njaj	O
conditional	B
probabilities	O
the	O
probabilities	O
of	O
each	O
possible	O
letter	O
in	O
each	O
context	O
actually	O
encountered	O
just	O
as	O
in	O
the	O
guessing	B
game	I
this	O
cost	O
can	O
be	O
contrasted	O
with	O
the	O
alternative	O
of	O
using	O
a	O
code	O
with	O
a	O
large	O
block	B
size	O
order	O
to	O
reduce	O
the	O
possible	O
onebit-per-symbol	O
overhead	O
discussed	O
in	O
section	B
where	O
all	O
block	B
sequences	O
that	O
could	O
occur	O
must	O
be	O
considered	O
and	O
their	O
probabilities	O
evaluated	O
notice	O
how	O
arithmetic	B
coding	I
is	O
it	O
can	O
be	O
used	O
with	O
any	O
source	O
alphabet	O
and	O
any	O
encoded	O
alphabet	O
the	O
size	O
of	O
the	O
source	O
alphabet	O
and	O
the	O
encoded	O
alphabet	O
can	O
change	O
with	O
time	O
arithmetic	B
coding	I
can	O
be	O
used	O
with	O
any	O
probability	B
distribution	B
which	O
can	O
change	O
utterly	O
from	O
context	O
to	O
context	O
furthermore	O
if	O
we	O
would	O
like	O
the	O
symbols	O
of	O
the	O
encoding	O
alphabet	O
and	O
to	O
be	O
used	O
with	O
unequal	O
frequency	B
that	O
can	O
easily	O
be	O
arranged	O
by	O
subdividing	O
the	O
right-hand	O
interval	O
in	O
proportion	O
to	O
the	O
required	O
frequencies	O
how	O
the	O
probabilistic	B
model	B
might	O
make	O
its	O
predictions	O
the	O
technique	O
of	O
arithmetic	B
coding	I
does	O
not	O
force	O
one	O
to	O
produce	O
the	O
predictive	O
probability	B
in	O
any	O
particular	O
way	O
but	O
the	O
predictive	O
distributions	O
might	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
figure	O
illustration	O
of	O
the	O
intervals	B
by	O
a	O
simple	O
bayesian	B
probabilistic	B
model	B
the	O
size	O
of	O
an	O
intervals	B
is	O
proportional	O
to	O
the	O
probability	B
of	O
the	O
string	O
this	O
model	B
anticipates	O
that	O
the	O
source	O
is	O
likely	O
to	O
be	O
biased	O
towards	O
one	O
of	O
a	O
and	O
b	O
so	O
sequences	O
having	O
lots	O
of	O
as	O
or	O
lots	O
of	O
bs	O
have	O
larger	O
intervals	B
than	O
sequences	O
of	O
the	O
same	O
length	O
that	O
are	O
as	O
and	O
bs	O
aa	O
ab	O
ba	O
aaa	O
aab	O
aba	O
abb	O
baa	O
bab	O
bba	O
aaaa	O
aaab	O
aaba	O
aabb	O
abaa	O
abab	O
abba	O
abbb	O
baaa	O
baab	O
baba	O
babb	O
bbaa	O
bbab	O
bbba	O
bb	O
bbb	O
bbbb	O
a	O
b	O
naturally	O
be	O
produced	O
by	O
a	O
bayesian	B
model	B
figure	O
was	O
generated	O
using	O
a	O
simple	O
model	B
that	O
always	O
assigns	O
a	O
probability	B
of	O
to	O
and	O
assigns	O
the	O
remaining	O
to	O
a	O
and	O
b	O
divided	O
in	O
proportion	O
to	O
probabilities	O
given	O
by	O
laplace	B
s	O
rule	O
plaj	O
fa	O
fa	O
fb	O
where	O
is	O
the	O
number	O
of	O
times	O
that	O
a	O
has	O
occurred	O
so	O
far	O
and	O
fb	O
is	O
the	O
count	O
of	O
bs	O
these	O
predictions	O
correspond	O
to	O
a	O
simple	O
bayesian	B
model	B
that	O
expects	O
and	O
adapts	O
to	O
a	O
non-equal	O
frequency	B
of	O
use	O
of	O
the	O
source	O
symbols	O
a	O
and	O
b	O
within	O
a	O
figure	O
displays	O
the	O
intervals	B
corresponding	O
to	O
a	O
number	O
of	O
strings	O
of	O
length	O
up	O
to	O
note	O
that	O
if	O
the	O
string	O
so	O
far	O
has	O
contained	O
a	O
large	O
number	O
of	O
bs	O
then	O
the	O
probability	B
of	O
b	O
relative	B
to	O
a	O
is	O
increased	O
and	O
conversely	O
if	O
many	O
as	O
occur	O
then	O
as	O
are	O
made	O
more	O
probable	O
larger	O
intervals	B
remember	O
require	O
fewer	O
bits	O
to	O
encode	O
details	O
of	O
the	O
bayesian	B
model	B
having	O
emphasized	O
that	O
any	O
model	B
could	O
be	O
used	O
arithmetic	B
coding	I
is	O
not	O
wedded	O
to	O
any	O
particular	O
set	B
of	O
probabilities	O
let	O
me	O
explain	O
the	O
simple	O
adaptive	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
arithmetic	O
codes	O
probabilistic	B
model	B
used	O
in	O
the	O
preceding	O
example	O
we	O
encountered	O
this	O
model	B
in	O
exercise	O
assumptions	B
the	O
model	B
will	O
be	O
described	O
using	O
parameters	B
pa	O
and	O
pb	O
below	O
which	O
should	O
not	O
be	O
confused	O
with	O
the	O
predictive	O
probabilities	O
in	O
a	O
particular	O
context	O
for	O
example	O
p	O
s	O
baa	O
a	O
bent	B
coin	B
labelled	O
a	O
and	O
b	O
is	O
tossed	O
some	O
number	O
of	O
times	O
l	O
which	O
we	O
don	O
t	O
know	O
beforehand	O
the	O
coin	B
s	O
probability	B
of	O
coming	O
up	O
a	O
when	O
tossed	O
is	O
pa	O
and	O
pb	O
pa	O
the	O
parameters	B
pa	O
pb	O
are	O
not	O
known	O
beforehand	O
the	O
source	O
string	O
s	O
indicates	O
that	O
l	O
was	O
and	O
the	O
sequence	B
of	O
outcomes	O
was	O
baaba	O
it	O
is	O
assumed	O
that	O
the	O
length	O
of	O
the	O
string	O
l	O
has	O
an	O
exponential	B
probability	B
distribution	B
p	O
this	O
distribution	B
corresponds	O
to	O
assuming	O
a	O
constant	O
probability	B
for	O
the	O
termination	B
symbol	O
at	O
each	O
character	O
it	O
is	O
assumed	O
that	O
the	O
non-terminal	O
characters	O
in	O
the	O
string	O
are	O
selected	O
independently	O
at	O
random	B
from	O
an	O
ensemble	B
with	O
probabilities	O
p	O
fpa	O
pbg	O
the	O
probability	B
pa	O
is	O
throughout	O
the	O
string	O
to	O
some	O
unknown	O
value	O
that	O
could	O
be	O
anywhere	O
between	O
and	O
the	O
probability	B
of	O
an	O
a	O
occurring	O
as	O
the	O
next	O
symbol	O
given	O
pa	O
only	O
we	O
knew	O
it	O
is	O
the	O
probability	B
given	O
pa	O
that	O
an	O
unterminated	O
string	O
of	O
length	O
f	O
is	O
a	O
given	O
string	O
s	O
that	O
contains	O
ffa	O
fbg	O
counts	O
of	O
the	O
two	O
outcomes	O
is	O
the	O
bernoulli	B
distribution	B
p	O
pa	O
f	O
pfa	O
a	O
pafb	O
we	O
assume	O
a	O
uniform	O
prior	B
distribution	B
for	O
pa	O
p	O
pa	O
and	O
pb	O
pa	O
it	O
would	O
be	O
easy	O
to	O
assume	O
other	O
priors	O
on	O
pa	O
with	O
beta	B
distributions	O
being	O
the	O
most	O
convenient	O
to	O
handle	O
this	O
model	B
was	O
studied	O
in	O
section	B
the	O
key	O
result	O
we	O
require	O
is	O
the	O
predictive	B
distribution	B
for	O
the	O
next	O
symbol	O
given	O
the	O
string	O
so	O
far	O
s	O
this	O
probability	B
that	O
the	O
next	O
character	O
is	O
a	O
or	O
b	O
that	O
it	O
is	O
not	O
was	O
derived	O
in	O
equation	O
and	O
is	O
precisely	O
laplace	B
s	O
rule	O
exercise	O
compare	O
the	O
expected	O
message	O
length	O
when	O
an	O
ascii	O
is	O
compressed	O
by	O
the	O
following	O
three	O
methods	O
read	O
the	O
whole	O
the	O
empirical	O
frequency	B
of	O
each	O
symbol	O
construct	O
a	O
code	O
for	O
those	O
frequencies	O
transmit	O
the	O
code	O
by	O
transmitting	O
the	O
lengths	O
of	O
the	O
codewords	O
then	O
transmit	O
the	O
using	O
the	O
code	O
actual	O
codewords	O
don	O
t	O
need	O
to	O
be	O
transmitted	O
since	O
we	O
can	O
use	O
a	O
deterministic	B
method	O
for	O
building	O
the	O
tree	B
given	O
the	O
codelengths	O
arithmetic	O
code	O
using	O
the	O
laplace	B
model	B
plaj	O
fa	O
arithmetic	O
code	O
using	O
a	O
dirichlet	B
model	B
this	O
model	B
s	O
predic	O
tions	O
are	O
pdaj	O
fa	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
where	O
is	O
to	O
a	O
number	O
such	O
as	O
a	O
small	O
value	O
of	O
corresponds	O
to	O
a	O
more	O
responsive	O
version	O
of	O
the	O
laplace	B
model	B
the	O
probability	B
over	O
characters	O
is	O
expected	O
to	O
be	O
more	O
nonuniform	O
reproduces	O
the	O
laplace	B
model	B
take	O
care	O
that	O
the	O
header	O
of	O
your	O
message	O
is	O
self-delimiting	B
special	O
cases	O
worth	O
considering	O
are	O
short	O
with	O
just	O
a	O
few	O
hundred	O
characters	O
large	O
in	O
which	O
some	O
characters	O
are	O
never	O
used	O
further	O
applications	O
of	O
arithmetic	B
coding	I
generation	O
of	O
random	B
samples	O
arithmetic	B
coding	I
not	O
only	O
a	O
way	O
to	O
compress	B
strings	O
believed	O
to	O
come	O
from	O
a	O
given	O
model	B
it	O
also	O
a	O
way	O
to	O
generate	O
random	B
strings	O
from	O
a	O
model	B
imagine	O
sticking	O
a	O
pin	O
into	O
the	O
unit	O
interval	O
at	O
random	B
that	O
line	O
having	O
been	O
divided	O
into	O
subintervals	O
in	O
proportion	O
to	O
probabilities	O
pi	O
the	O
probability	B
that	O
your	O
pin	O
will	O
lie	O
in	O
interval	O
i	O
is	O
pi	O
so	O
to	O
generate	O
a	O
sample	B
from	I
a	O
model	B
all	O
we	O
need	O
to	O
do	O
is	O
feed	O
ordinary	O
random	B
bits	O
into	O
an	O
arithmetic	O
decoder	B
for	O
that	O
model	B
an	O
random	B
bit	B
sequence	B
corresponds	O
to	O
the	O
selection	O
of	O
a	O
point	O
at	O
random	B
from	O
the	O
line	O
so	O
the	O
decoder	B
will	O
then	O
select	O
a	O
string	O
at	O
random	B
from	O
the	O
assumed	O
distribution	B
this	O
arithmetic	O
method	O
is	O
guaranteed	O
to	O
use	O
very	O
nearly	O
the	O
smallest	O
number	O
of	O
random	B
bits	O
possible	O
to	O
make	O
the	O
selection	O
an	O
important	O
point	O
in	O
communities	O
where	O
random	B
numbers	O
are	O
expensive	O
is	O
not	O
a	O
joke	O
large	O
amounts	O
of	O
money	O
are	O
spent	O
on	O
generating	O
random	B
bits	O
in	O
software	B
and	O
hardware	O
random	B
numbers	O
are	O
valuable	O
a	O
simple	O
example	O
of	O
the	O
use	O
of	O
this	O
technique	O
is	O
in	O
the	O
generation	O
of	O
random	B
bits	O
with	O
a	O
nonuniform	O
distribution	B
exercise	O
compare	O
the	O
following	O
two	O
techniques	O
for	O
generating	O
random	B
symbols	O
from	O
a	O
nonuniform	O
distribution	B
the	O
standard	O
method	O
use	O
a	O
standard	O
random	B
number	I
generator	I
to	O
generate	O
an	O
integer	O
between	O
and	O
rescale	O
the	O
integer	O
to	O
test	O
whether	O
this	O
uniformly	O
distributed	O
random	B
variable	I
is	O
less	O
than	O
and	O
emit	O
a	O
or	O
accordingly	O
arithmetic	B
coding	I
using	O
the	O
correct	O
model	B
fed	O
with	O
standard	O
ran	O
dom	O
bits	O
roughly	O
how	O
many	O
random	B
bits	O
will	O
each	O
method	O
use	O
to	O
generate	O
a	O
thousand	O
samples	O
from	O
this	O
sparse	O
distribution	B
data-entry	O
devices	O
when	O
we	O
enter	O
text	O
into	O
a	O
computer	B
we	O
make	O
gestures	O
of	O
some	O
sort	O
maybe	O
we	O
tap	B
a	O
keyboard	B
or	O
scribble	O
with	O
a	O
pointer	B
or	O
click	O
with	O
a	O
mouse	O
an	O
text	B
entry	I
system	O
is	O
one	O
where	O
the	O
number	O
of	O
gestures	O
required	O
to	O
enter	O
a	O
given	O
text	O
string	O
is	O
small	O
writing	B
can	O
be	O
viewed	O
as	O
an	O
inverse	O
process	O
to	O
data	O
compression	B
in	O
data	O
compression	B
the	O
aim	O
is	O
to	O
map	O
a	O
given	O
text	O
string	O
into	O
a	O
small	O
number	O
of	O
bits	O
in	O
text	B
entry	I
we	O
want	O
a	O
small	O
sequence	B
of	O
gestures	O
to	O
produce	O
our	O
intended	O
text	O
by	O
inverting	O
an	O
arithmetic	O
coder	O
we	O
can	O
obtain	O
an	O
text	B
entry	I
device	O
that	O
is	O
driven	O
by	O
continuous	B
pointing	O
gestures	O
et	O
al	O
compression	B
text	O
bits	O
writing	B
text	O
gestures	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
lempelziv	O
coding	O
in	O
this	O
system	O
called	O
dasher	B
the	O
user	O
zooms	O
in	O
on	O
the	O
unit	O
interval	O
to	O
locate	O
the	O
interval	O
corresponding	O
to	O
their	O
intended	O
string	O
in	O
the	O
same	O
style	O
as	O
a	O
language	B
model	B
as	O
used	O
in	O
text	O
compression	B
controls	O
the	O
sizes	O
of	O
the	O
intervals	B
such	O
that	O
probable	O
strings	O
are	O
quick	O
and	O
easy	O
to	O
identify	O
after	O
an	O
hour	O
s	O
practice	O
a	O
novice	O
user	O
can	O
write	O
with	O
one	O
driving	O
dasher	B
at	O
about	O
words	O
per	O
minute	O
that	O
s	O
about	O
half	O
their	O
normal	B
typing	O
speed	O
on	O
a	O
regular	B
keyboard	B
it	O
s	O
even	O
possible	O
to	O
write	O
at	O
words	O
per	O
minute	O
hands-free	O
using	O
gaze	O
direction	O
to	O
drive	O
dasher	B
and	O
mackay	B
dasher	B
is	O
available	O
as	O
free	O
software	B
for	O
various	O
lempelziv	O
coding	O
the	O
lempelziv	O
algorithms	B
which	O
are	O
widely	O
used	O
for	O
data	O
compression	B
the	O
compress	B
and	O
gzip	B
commands	O
are	O
in	O
philosophy	B
to	O
arithmetic	B
coding	I
there	O
is	O
no	O
separation	B
between	O
modelling	B
and	O
coding	O
and	O
no	O
opportunity	O
for	O
explicit	O
modelling	B
basic	O
lempelziv	O
algorithm	O
the	O
method	O
of	O
compression	B
is	O
to	O
replace	O
a	O
substring	B
with	O
a	O
pointer	B
to	O
an	O
earlier	O
occurrence	O
of	O
the	O
same	O
substring	B
for	O
example	O
if	O
the	O
string	O
is	O
we	O
parse	B
it	O
into	O
an	O
ordered	O
dictionary	B
of	O
substrings	O
that	O
have	O
not	O
appeared	O
before	O
as	O
follows	O
we	O
include	O
the	O
empty	O
substring	B
as	O
the	O
substring	B
in	O
the	O
dictionary	B
and	O
order	O
the	O
substrings	O
in	O
the	O
dictionary	B
by	O
the	O
order	O
in	O
which	O
they	O
emerged	O
from	O
the	O
source	O
after	O
every	O
comma	O
we	O
look	O
along	O
the	O
next	O
part	O
of	O
the	O
input	O
sequence	B
until	O
we	O
have	O
read	O
a	O
substring	B
that	O
has	O
not	O
been	O
marked	O
before	O
a	O
moment	O
s	O
will	O
that	O
this	O
substring	B
is	O
longer	O
by	O
one	O
bit	B
than	O
a	O
substring	B
that	O
has	O
occurred	O
earlier	O
in	O
the	O
dictionary	B
this	O
means	O
that	O
we	O
can	O
encode	O
each	O
substring	B
by	O
giving	O
a	O
pointer	B
to	O
the	O
earlier	O
occurrence	O
of	O
that	O
and	O
then	O
sending	O
the	O
extra	B
bit	B
by	O
which	O
the	O
new	O
substring	B
in	O
the	O
dictionary	B
from	O
the	O
earlier	O
substring	B
if	O
at	O
the	O
nth	O
bit	B
we	O
have	O
enumerated	O
sn	O
substrings	O
then	O
we	O
can	O
give	O
the	O
value	O
of	O
the	O
pointer	B
in	O
dlog	O
sne	O
bits	O
the	O
code	O
for	O
the	O
above	O
sequence	B
is	O
then	O
as	O
shown	O
in	O
the	O
fourth	O
line	O
of	O
the	O
following	O
table	O
punctuation	O
included	O
for	O
clarity	O
the	O
upper	O
lines	O
indicating	O
the	O
source	O
string	O
and	O
the	O
value	O
of	O
sn	O
source	O
substrings	O
sn	O
snbinary	O
bit	B
notice	O
that	O
the	O
pointer	B
we	O
send	O
is	O
empty	O
because	O
given	O
that	O
there	O
is	O
only	O
one	O
substring	B
in	O
the	O
dictionary	B
the	O
string	O
no	O
bits	O
are	O
needed	O
to	O
convey	O
the	O
choice	O
of	O
that	O
substring	B
as	O
the	O
the	O
encoded	O
string	O
is	O
the	O
encoding	O
in	O
this	O
simple	O
case	O
is	O
actually	O
a	O
longer	O
string	O
than	O
the	O
source	O
string	O
because	O
there	O
was	O
no	O
obvious	O
redundancy	B
in	O
the	O
source	O
string	O
exercise	O
prove	O
that	O
any	O
uniquely	B
decodeable	I
code	O
from	O
to	O
necessarily	O
makes	O
some	O
strings	O
longer	O
if	O
it	O
makes	O
some	O
strings	O
shorter	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
one	O
reason	O
why	O
the	O
algorithm	O
described	O
above	O
lengthens	O
a	O
lot	O
of	O
strings	O
is	O
because	O
it	O
is	O
it	O
transmits	O
unnecessary	O
bits	O
to	O
put	O
it	O
another	O
way	O
its	O
code	O
is	O
not	O
complete	O
once	O
a	O
substring	B
in	O
the	O
dictionary	B
has	O
been	O
joined	O
there	O
by	O
both	O
of	O
its	O
children	O
then	O
we	O
can	O
be	O
sure	O
that	O
it	O
will	O
not	O
be	O
needed	O
possibly	O
as	O
part	O
of	O
our	O
protocol	B
for	O
terminating	O
a	O
message	O
so	O
at	O
that	O
point	O
we	O
could	O
drop	O
it	O
from	O
our	O
dictionary	B
of	O
substrings	O
and	O
them	O
all	O
along	O
one	O
thereby	O
reducing	O
the	O
length	O
of	O
subsequent	O
pointer	B
messages	O
equivalently	O
we	O
could	O
write	O
the	O
second	O
into	O
the	O
dictionary	B
at	O
the	O
point	O
previously	O
occupied	O
by	O
the	O
parent	B
a	O
second	O
unnecessary	O
overhead	O
is	O
the	O
transmission	O
of	O
the	O
new	O
bit	B
in	O
these	O
cases	O
the	O
second	O
time	O
a	O
is	O
used	O
we	O
can	O
be	O
sure	O
of	O
the	O
identity	O
of	O
the	O
next	O
bit	B
decoding	B
the	O
decoder	B
again	O
involves	O
an	O
identical	B
twin	B
at	O
the	O
decoding	B
end	O
who	O
constructs	O
the	O
dictionary	B
of	O
substrings	O
as	O
the	O
data	O
are	O
decoded	O
exercise	O
encode	O
the	O
string	O
using	O
the	O
basic	O
lempelziv	O
algorithm	O
described	O
above	O
exercise	O
decode	O
the	O
string	O
that	O
was	O
encoded	O
using	O
the	O
basic	O
lempelziv	O
algorithm	O
practicalities	O
in	O
this	O
description	O
i	O
have	O
not	O
discussed	O
the	O
method	O
for	O
terminating	O
a	O
string	O
there	O
are	O
many	O
variations	O
on	O
the	O
lempelziv	O
algorithm	O
all	O
exploiting	O
the	O
same	O
idea	O
but	O
using	O
procedures	O
for	O
dictionary	B
management	O
etc	O
the	O
resulting	O
programs	O
are	O
fast	O
but	O
their	O
performance	O
on	O
compression	B
of	O
english	B
text	O
although	O
useful	O
does	O
not	O
match	O
the	O
standards	O
set	B
in	O
the	O
arithmetic	B
coding	I
literature	O
theoretical	O
properties	O
in	O
contrast	O
to	O
the	O
block	B
code	I
code	O
and	O
arithmetic	B
coding	I
methods	O
we	O
discussed	O
in	O
the	O
last	O
three	O
chapters	O
the	O
lempelziv	O
algorithm	O
is	O
without	O
making	O
any	O
mention	O
of	O
a	O
probabilistic	B
model	B
for	O
the	O
source	O
yet	O
given	O
any	O
ergodic	B
source	O
one	O
that	O
is	O
memoryless	O
on	O
long	O
timescales	O
the	O
lempelziv	O
algorithm	O
can	O
be	O
proven	O
asymptotically	O
to	O
compress	B
down	O
to	O
the	O
entropy	B
of	O
the	O
source	O
this	O
is	O
why	O
it	O
is	O
called	O
a	O
universal	B
compression	B
algorithm	O
for	O
a	O
proof	O
of	O
this	O
property	O
see	O
cover	B
and	O
thomas	O
it	O
achieves	O
its	O
compression	B
however	O
only	O
by	O
memorizing	O
substrings	O
that	O
have	O
happened	O
so	O
that	O
it	O
has	O
a	O
short	O
name	O
for	O
them	O
the	O
next	O
time	O
they	O
occur	O
the	O
asymptotic	O
timescale	O
on	O
which	O
this	O
universal	B
performance	O
is	O
achieved	O
may	O
for	O
many	O
sources	O
be	O
unfeasibly	O
long	O
because	O
the	O
number	O
of	O
typical	B
substrings	O
that	O
need	O
memorizing	O
may	O
be	O
enormous	O
the	O
useful	O
performance	O
of	O
the	O
algorithm	O
in	O
practice	O
is	O
a	O
of	O
the	O
fact	O
that	O
many	O
contain	O
multiple	O
repetitions	O
of	O
particular	O
short	O
sequences	O
of	O
characters	O
a	O
form	O
of	O
redundancy	B
to	O
which	O
the	O
algorithm	O
is	O
well	O
suited	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
demonstration	O
common	O
ground	O
i	O
have	O
emphasized	O
the	O
in	O
philosophy	B
behind	O
arithmetic	B
coding	I
and	O
lempelziv	O
coding	O
there	O
is	O
common	O
ground	O
between	O
them	O
though	O
in	O
principle	O
one	O
can	O
design	O
adaptive	B
probabilistic	O
models	O
and	O
thence	O
arithmetic	O
codes	O
that	O
are	O
universal	B
that	O
is	O
models	O
that	O
will	O
asymptotically	O
compress	B
any	O
source	O
in	O
some	O
class	O
to	O
within	O
some	O
factor	O
of	O
its	O
entropy	B
however	O
for	O
practical	B
purposes	O
i	O
think	O
such	O
universal	B
models	O
can	O
only	O
be	O
constructed	O
if	O
the	O
class	O
of	O
sources	O
is	O
severely	O
restricted	O
a	O
general	O
purpose	O
compressor	O
that	O
can	O
discover	O
the	O
probability	B
distribution	B
of	O
any	O
source	O
would	O
be	O
a	O
general	O
purpose	O
intelligence	O
a	O
general	O
purpose	O
intelligence	O
does	O
not	O
yet	O
exist	O
demonstration	O
an	O
interactive	O
aid	O
for	O
exploring	O
arithmetic	B
coding	I
dasher	B
tcl	O
is	O
a	O
demonstration	O
arithmetic-coding	O
software	B
package	O
written	O
by	O
radford	O
consists	O
of	O
encoding	O
and	O
decoding	B
modules	O
to	O
which	O
the	O
user	O
adds	O
a	O
module	O
the	O
probabilistic	B
model	B
it	O
should	O
be	O
emphasized	O
that	O
there	O
is	O
no	O
single	O
general-purpose	O
arithmetic-coding	O
compressor	O
a	O
new	O
model	B
has	O
to	O
be	O
written	O
for	O
each	O
type	O
of	O
source	O
radford	O
neal	B
s	O
package	O
includes	O
a	O
simple	O
adaptive	B
model	B
similar	O
to	O
the	O
bayesian	B
model	B
demonstrated	O
in	O
section	B
the	O
results	O
using	O
this	O
laplace	B
model	B
should	O
be	O
viewed	O
as	O
a	O
basic	O
benchmark	O
since	O
it	O
is	O
the	O
simplest	O
possible	O
probabilistic	B
model	B
it	O
simply	O
assumes	O
the	O
characters	O
in	O
the	O
come	O
independently	O
from	O
a	O
ensemble	B
the	O
counts	O
ffig	O
of	O
the	O
symbols	O
faig	O
are	O
rescaled	O
and	O
rounded	O
as	O
the	O
is	O
read	O
such	O
that	O
all	O
the	O
counts	O
lie	O
between	O
and	O
a	O
state-of-the-art	O
compressor	O
for	O
documents	O
containing	O
text	O
and	O
images	B
djvu	B
uses	O
arithmetic	O
it	O
uses	O
a	O
carefully	O
designed	O
approximate	O
arithmetic	O
coder	O
for	O
binary	O
alphabets	O
called	O
the	O
z-coder	O
et	O
al	O
which	O
is	O
much	O
faster	O
than	O
the	O
arithmetic	B
coding	I
software	B
described	O
above	O
one	O
of	O
the	O
neat	O
tricks	O
the	O
z-coder	O
uses	O
is	O
this	O
the	O
adaptive	B
model	B
adapts	O
only	O
occasionally	O
save	O
on	O
computer	B
time	O
with	O
the	O
decision	O
about	O
when	O
to	O
adapt	O
being	O
pseudo-randomly	O
controlled	O
by	O
whether	O
the	O
arithmetic	O
encoder	B
emitted	O
a	O
bit	B
the	O
jbig	O
image	B
compression	B
standard	O
for	O
binary	B
images	B
uses	O
arithmetic	B
coding	I
with	O
a	O
context-dependent	O
model	B
which	O
adapts	O
using	O
a	O
rule	O
similar	O
to	O
laplace	B
s	O
rule	O
ppm	O
is	O
a	O
leading	O
method	O
for	O
text	O
compression	B
and	O
it	O
uses	O
arithmetic	B
coding	I
there	O
are	O
many	O
lempelziv-based	O
programs	O
gzip	B
is	O
based	O
on	O
a	O
version	O
of	O
lempelziv	O
called	O
and	O
lempel	O
compress	B
is	O
based	O
on	O
lzw	O
in	O
my	O
experience	O
the	O
best	O
is	O
gzip	B
with	O
compress	B
being	O
inferior	O
on	O
most	O
bzip	O
is	O
a	O
block-sorting	B
compressor	O
which	O
makes	O
use	O
of	O
a	O
neat	O
hack	O
called	O
the	O
burrowswheeler	O
transform	O
and	O
wheeler	O
this	O
method	O
is	O
not	O
based	O
on	O
an	O
explicit	O
probabilistic	B
model	B
and	O
it	O
only	O
works	O
well	O
for	O
larger	O
than	O
several	O
thousand	O
characters	O
but	O
in	O
practice	O
it	O
is	O
a	O
very	O
compressor	O
for	O
in	O
which	O
the	O
context	O
of	O
a	O
character	O
is	O
a	O
good	B
predictor	O
for	O
that	O
is	O
a	O
lot	O
of	O
information	B
about	O
the	O
burrowswheeler	O
transform	O
on	O
the	O
net	O
httpdogma	O
netdatacompressionbwt	O
shtml	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
compression	B
of	O
a	O
text	O
table	O
gives	O
the	O
computer	B
time	O
in	O
seconds	O
taken	O
and	O
the	O
compression	B
achieved	O
when	O
these	O
programs	O
are	O
applied	O
to	O
the	O
latex	O
containing	O
the	O
text	O
of	O
this	O
chapter	O
of	O
size	O
bytes	O
stream	B
codes	I
method	O
compression	B
compressed	O
size	O
uncompression	B
time	O
sec	O
of	O
time	O
sec	O
table	O
comparison	O
of	O
compression	B
algorithms	B
applied	O
to	O
a	O
text	O
laplace	B
model	B
gzip	B
compress	B
bzip	O
ppmz	O
compression	B
of	O
a	O
sparse	O
interestingly	O
gzip	B
does	O
not	O
always	O
do	O
so	O
well	O
table	O
gives	O
the	O
compression	B
achieved	O
when	O
these	O
programs	O
are	O
applied	O
to	O
a	O
text	O
containing	O
characters	O
each	O
of	O
which	O
is	O
either	O
and	O
with	O
probabilities	O
and	O
the	O
laplace	B
model	B
is	O
quite	O
well	O
matched	O
to	O
this	O
source	O
and	O
the	O
benchmark	O
arithmetic	O
coder	O
gives	O
good	B
performance	O
followed	O
closely	O
by	O
compress	B
gzip	B
is	O
worst	O
an	O
ideal	O
model	B
for	O
this	O
source	O
would	O
compress	B
the	O
into	O
about	O
bytes	O
the	O
laplace-model	O
compressor	O
falls	O
short	O
of	O
this	O
performance	O
because	O
it	O
is	O
implemented	O
using	O
only	O
eight-bit	O
precision	B
the	O
ppmz	O
compressor	O
compresses	O
the	O
best	O
of	O
all	O
but	O
takes	O
much	O
more	O
computer	B
time	O
method	O
compression	B
compressed	O
size	O
uncompression	B
time	O
sec	O
bytes	O
time	O
sec	O
laplace	B
model	B
gzip	B
gzip	B
compress	B
bzip	O
ppmz	O
summary	B
in	O
the	O
last	O
three	O
chapters	O
we	O
have	O
studied	O
three	O
classes	O
of	O
data	O
compression	B
codes	O
fixed-length	O
block	B
codes	O
these	O
are	O
mappings	O
from	O
a	O
number	O
of	O
source	O
symbols	O
to	O
a	O
binary	O
message	O
only	O
a	O
tiny	O
fraction	O
of	O
the	O
source	O
strings	O
are	O
given	O
an	O
encoding	O
these	O
codes	O
were	O
fun	O
for	O
identifying	O
the	O
entropy	B
as	O
the	O
measure	O
of	O
compressibility	O
but	O
they	O
are	O
of	O
little	O
practical	B
use	O
table	O
comparison	O
of	O
compression	B
algorithms	B
applied	O
to	O
a	O
random	B
of	O
characters	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
on	O
stream	B
codes	I
symbol	O
codes	O
symbol	O
codes	O
employ	O
a	O
variable-length	B
code	I
for	O
each	O
symbol	O
in	O
the	O
source	O
alphabet	O
the	O
codelengths	O
being	O
integer	O
lengths	O
determined	O
by	O
the	O
probabilities	O
of	O
the	O
symbols	O
s	O
algorithm	O
constructs	O
an	O
optimal	B
symbol	B
code	I
for	O
a	O
given	O
set	B
of	O
symbol	O
probabilities	O
every	O
source	O
string	O
has	O
a	O
uniquely	B
decodeable	I
encoding	O
and	O
if	O
the	O
source	O
symbols	O
come	O
from	O
the	O
assumed	O
distribution	B
then	O
the	O
symbol	B
code	I
will	O
compress	B
to	O
an	O
expected	O
length	O
per	O
character	O
l	O
lying	O
in	O
the	O
interval	O
h	O
statistical	B
in	O
the	O
source	O
may	O
make	O
the	O
actual	O
length	O
longer	O
or	O
shorter	O
than	O
this	O
mean	B
length	O
if	O
the	O
source	O
is	O
not	O
well	O
matched	O
to	O
the	O
assumed	O
distribution	B
then	O
the	O
mean	B
length	O
is	O
increased	O
by	O
the	O
relative	B
entropy	B
dkl	B
between	O
the	O
source	O
distribution	B
and	O
the	O
code	O
s	O
implicit	O
distribution	B
for	O
sources	O
with	O
small	O
entropy	B
the	O
symbol	O
has	O
to	O
emit	O
at	O
least	O
one	O
bit	B
per	O
source	O
symbol	O
compression	B
below	O
one	O
bit	B
per	O
source	O
symbol	O
can	O
be	O
achieved	O
only	O
by	O
the	O
cumbersome	O
procedure	O
of	O
putting	O
the	O
source	O
data	O
into	O
blocks	O
stream	B
codes	I
the	O
distinctive	O
property	O
of	O
stream	B
codes	I
compared	O
with	O
symbol	O
codes	O
is	O
that	O
they	O
are	O
not	O
constrained	B
to	O
emit	O
at	O
least	O
one	O
bit	B
for	O
every	O
symbol	O
read	O
from	O
the	O
source	O
stream	O
so	O
large	O
numbers	O
of	O
source	O
symbols	O
may	O
be	O
coded	O
into	O
a	O
smaller	O
number	O
of	O
bits	O
this	O
property	O
could	O
be	O
obtained	O
using	O
a	O
symbol	B
code	I
only	O
if	O
the	O
source	O
stream	O
were	O
somehow	O
chopped	O
into	O
blocks	O
arithmetic	O
codes	O
combine	O
a	O
probabilistic	B
model	B
with	O
an	O
encoding	O
algorithm	O
that	O
each	O
string	O
with	O
a	O
sub-interval	O
of	O
of	O
size	O
equal	O
to	O
the	O
probability	B
of	O
that	O
string	O
under	O
the	O
model	B
this	O
code	O
is	O
almost	O
optimal	B
in	O
the	O
sense	O
that	O
the	O
compressed	O
length	O
of	O
a	O
string	O
x	O
closely	O
matches	O
the	O
shannon	B
information	B
content	I
of	O
x	O
given	O
the	O
probabilistic	B
model	B
arithmetic	O
codes	O
with	O
the	O
philosophy	B
that	O
good	B
compression	B
requires	O
data	O
modelling	B
in	O
the	O
form	O
of	O
an	O
adaptive	B
bayesian	B
model	B
lempelziv	O
codes	O
are	O
adaptive	B
in	O
the	O
sense	O
that	O
they	O
memorize	O
strings	O
that	O
have	O
already	O
occurred	O
they	O
are	O
built	O
on	O
the	O
philosophy	B
that	O
we	O
don	O
t	O
know	O
anything	O
at	O
all	O
about	O
what	O
the	O
probability	B
distribution	B
of	O
the	O
source	O
will	O
be	O
and	O
we	O
want	O
a	O
compression	B
algorithm	O
that	O
will	O
perform	O
reasonably	O
well	O
whatever	O
that	O
distribution	B
is	O
both	O
arithmetic	O
codes	O
and	O
lempelziv	O
codes	O
will	O
fail	O
to	O
decode	O
correctly	O
if	O
any	O
of	O
the	O
bits	O
of	O
the	O
compressed	O
are	O
altered	O
so	O
if	O
compressed	O
are	O
to	O
be	O
stored	O
or	O
transmitted	O
over	O
noisy	B
media	O
error-correcting	B
codes	I
will	O
be	O
essential	O
reliable	O
communication	B
over	O
unreliable	O
channels	O
is	O
the	O
topic	O
of	O
part	O
ii	O
exercises	O
on	O
stream	B
codes	I
exercise	O
describe	O
an	O
arithmetic	B
coding	I
algorithm	O
to	O
encode	O
random	B
bit	B
strings	O
of	O
length	O
n	O
and	O
weight	O
k	O
k	O
ones	O
and	O
n	O
k	O
zeroes	O
where	O
n	O
and	O
k	O
are	O
given	O
for	O
the	O
case	O
n	O
k	O
show	O
in	O
detail	O
the	O
intervals	B
corresponding	O
to	O
all	O
source	O
substrings	O
of	O
lengths	O
exercise	O
how	O
many	O
bits	O
are	O
needed	O
to	O
specify	O
a	O
selection	O
of	O
k	O
objects	O
from	O
n	O
objects	O
and	O
k	O
are	O
assumed	O
to	O
be	O
known	O
and	O
the	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
selection	O
of	O
k	O
objects	O
is	O
unordered	O
how	O
might	O
such	O
a	O
selection	O
be	O
made	O
at	O
random	B
without	O
being	O
wasteful	O
of	O
random	B
bits	O
exercise	O
a	O
binary	O
source	O
x	O
emits	O
independent	O
identically	O
distributed	O
symbols	O
with	O
probability	B
distribution	B
where	O
find	O
an	O
optimal	B
uniquely-decodeable	O
symbol	B
code	I
for	O
a	O
string	O
x	O
of	O
three	O
successive	O
samples	O
from	O
this	O
source	O
estimate	O
one	O
decimal	O
place	O
the	O
factor	O
by	O
which	O
the	O
expected	O
length	O
of	O
this	O
optimal	B
code	O
is	O
greater	O
than	O
the	O
entropy	B
of	O
the	O
three-bit	O
string	O
x	O
where	O
x	O
x	O
x	O
an	O
arithmetic	O
code	O
is	O
used	O
to	O
compress	B
a	O
string	O
of	O
samples	O
from	O
the	O
source	O
x	O
estimate	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
the	O
length	O
of	O
the	O
compressed	O
exercise	O
describe	O
an	O
arithmetic	B
coding	I
algorithm	O
to	O
generate	O
random	B
bit	B
strings	O
of	O
length	O
n	O
with	O
density	B
f	O
each	O
bit	B
has	O
probability	B
f	O
of	O
being	O
a	O
one	O
where	O
n	O
is	O
given	O
exercise	O
use	O
a	O
lempelziv	O
algorithm	O
in	O
which	O
as	O
discussed	O
on	O
the	O
dictionary	B
of	O
is	O
pruned	O
by	O
writing	B
new	O
into	O
the	O
space	O
occupied	O
by	O
that	O
will	O
not	O
be	O
needed	O
again	O
such	O
can	O
be	O
when	O
both	O
their	O
children	O
have	O
been	O
added	O
to	O
the	O
dictionary	B
of	O
may	O
neglect	O
the	O
issue	O
of	O
termination	B
of	O
encoding	O
use	O
this	O
algorithm	O
to	O
encode	O
the	O
string	O
highlight	O
the	O
bits	O
that	O
follow	O
a	O
on	O
the	O
second	O
occasion	O
that	O
that	O
is	O
used	O
discussed	O
earlier	O
these	O
bits	O
could	O
be	O
omitted	O
exercise	O
show	O
that	O
this	O
lempelziv	O
code	O
is	O
still	O
not	O
complete	O
that	O
is	O
there	O
are	O
binary	O
strings	O
that	O
are	O
not	O
encodings	O
of	O
any	O
string	O
exercise	O
give	O
examples	O
of	O
simple	O
sources	O
that	O
have	O
low	O
entropy	B
but	O
would	O
not	O
be	O
compressed	O
well	O
by	O
the	O
lempelziv	O
algorithm	O
further	O
exercises	O
on	O
data	O
compression	B
the	O
following	O
exercises	O
may	O
be	O
skipped	O
by	O
the	O
reader	O
who	O
is	O
eager	O
to	O
learn	O
about	O
noisy	B
channels	O
exercise	O
consider	O
a	O
gaussian	B
distribution	B
in	O
n	O
dimensions	B
n	O
p	O
estimate	O
the	O
mean	B
the	O
radius	O
of	O
a	O
point	O
x	O
to	O
be	O
r	O
and	O
variance	B
of	O
the	O
square	B
of	O
the	O
radius	O
you	O
may	O
helpful	O
the	O
integral	B
z	O
dx	O
though	O
you	O
should	O
be	O
able	O
to	O
estimate	O
the	O
required	O
quantities	O
without	O
it	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
data	O
compression	B
assuming	O
that	O
n	O
is	O
large	O
show	O
that	O
nearly	O
all	O
the	O
probability	B
of	O
a	O
gaussian	B
is	O
contained	O
in	O
a	O
thin	B
shell	I
of	O
radius	O
pn	O
find	O
the	O
thickness	O
of	O
the	O
shell	O
evaluate	O
the	O
probability	B
density	B
at	O
a	O
point	O
in	O
that	O
thin	B
shell	I
and	O
at	O
the	O
origin	O
x	O
and	O
compare	O
use	O
the	O
case	O
n	O
as	O
an	O
example	O
notice	O
that	O
nearly	O
all	O
the	O
probability	B
mass	O
is	O
located	O
in	O
a	O
part	O
of	O
the	O
space	O
from	O
the	O
region	O
of	O
highest	O
probability	B
density	B
exercise	O
explain	O
what	O
is	O
meant	O
by	O
an	O
optimal	B
binary	O
symbol	B
code	I
find	O
an	O
optimal	B
binary	O
symbol	B
code	I
for	O
the	O
ensemble	B
probability	B
density	B
is	O
maximized	O
here	O
almost	O
all	O
probability	B
mass	O
is	O
here	O
figure	O
schematic	O
representation	O
of	O
the	O
typical	B
set	B
of	O
an	O
n	O
gaussian	B
distribution	B
a	O
fa	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
jg	O
p	O
and	O
compute	O
the	O
expected	O
length	O
of	O
the	O
code	O
exercise	O
a	O
string	O
y	O
consists	O
of	O
two	O
independent	O
samples	O
from	O
an	O
ensemble	B
x	O
ax	O
fa	O
b	O
cgpx	O
what	O
is	O
the	O
entropy	B
of	O
y	O
construct	O
an	O
optimal	B
binary	O
symbol	B
code	I
for	O
the	O
string	O
y	O
and	O
its	O
expected	O
length	O
exercise	O
strings	O
of	O
n	O
independent	O
samples	O
from	O
an	O
ensemble	B
with	O
p	O
are	O
compressed	O
using	O
an	O
arithmetic	O
code	O
that	O
is	O
matched	O
to	O
that	O
ensemble	B
estimate	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
the	O
compressed	O
strings	O
lengths	O
for	O
the	O
case	O
n	O
exercise	O
source	O
coding	O
with	O
variable-length	B
symbols	O
in	O
the	O
chapters	O
on	O
source	O
coding	O
we	O
assumed	O
that	O
we	O
were	O
encoding	O
into	O
a	O
binary	O
alphabet	O
in	O
which	O
both	O
symbols	O
should	O
be	O
used	O
with	O
equal	O
frequency	B
in	O
this	O
question	O
we	O
explore	B
how	O
the	O
encoding	O
alphabet	O
should	O
be	O
used	O
if	O
the	O
symbols	O
take	O
times	O
to	O
transmit	O
a	O
poverty-stricken	O
student	B
communicates	O
for	O
free	O
with	O
a	O
friend	O
using	O
a	O
telephone	O
by	O
selecting	O
an	O
integer	O
n	O
making	O
the	O
friend	O
s	O
phone	B
ring	O
n	O
times	O
then	O
hanging	O
up	O
in	O
the	O
middle	O
of	O
the	O
nth	O
ring	O
this	O
process	O
is	O
repeated	O
so	O
that	O
a	O
string	O
of	O
symbols	O
is	O
received	O
what	O
is	O
the	O
optimal	B
way	O
to	O
communicate	O
if	O
large	O
integers	O
n	O
are	O
selected	O
then	O
the	O
message	O
takes	O
longer	O
to	O
communicate	O
if	O
only	O
small	O
integers	O
n	O
are	O
used	O
then	O
the	O
information	B
content	I
per	O
symbol	O
is	O
small	O
we	O
aim	O
to	O
maximize	O
the	O
rate	B
of	O
information	B
transfer	O
per	O
unit	O
time	O
assume	O
that	O
the	O
time	O
taken	O
to	O
transmit	O
a	O
number	O
of	O
rings	O
n	O
and	O
to	O
redial	O
is	O
ln	O
seconds	O
consider	O
a	O
probability	B
distribution	B
over	O
n	O
fpng	O
the	O
average	O
duration	O
per	O
symbol	O
to	O
be	O
lp	O
pnln	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
and	O
the	O
entropy	B
per	O
symbol	O
to	O
be	O
hp	O
pn	O
pn	O
show	O
that	O
for	O
the	O
average	O
information	B
rate	B
per	O
second	O
to	O
be	O
maximized	O
the	O
symbols	O
must	O
be	O
used	O
with	O
probabilities	O
of	O
the	O
form	O
pn	O
z	O
where	O
z	O
and	O
the	O
implicit	O
equation	O
hp	O
lp	O
that	O
is	O
is	O
the	O
rate	B
of	O
communication	B
show	O
that	O
these	O
two	O
equations	O
imply	O
that	O
must	O
be	O
set	B
such	O
that	O
log	O
z	O
assuming	O
that	O
the	O
channel	O
has	O
the	O
property	O
ln	O
n	O
seconds	O
the	O
optimal	B
distribution	B
p	O
and	O
show	O
that	O
the	O
maximal	O
information	B
rate	B
is	O
bit	B
per	O
second	O
how	O
does	O
this	O
compare	O
with	O
the	O
information	B
rate	B
per	O
second	O
achieved	O
if	O
p	O
is	O
set	B
to	O
that	O
is	O
only	O
the	O
symbols	O
n	O
and	O
n	O
are	O
selected	O
and	O
they	O
have	O
equal	O
probability	B
discuss	O
the	O
relationship	O
between	O
the	O
results	O
derived	O
above	O
and	O
the	O
kraft	B
inequality	B
from	O
source	O
coding	B
theory	I
how	O
might	O
a	O
random	B
binary	O
source	O
be	O
encoded	O
into	O
a	O
sequence	B
of	O
symbols	O
for	O
transmission	O
over	O
the	O
channel	O
in	O
equation	O
exercise	O
how	O
many	O
bits	O
does	O
it	O
take	O
to	O
a	O
pack	O
of	O
cards	O
exercise	O
in	O
the	O
card	B
game	O
bridge	B
the	O
four	O
players	O
receive	O
cards	O
each	O
from	O
the	O
deck	O
of	O
and	O
start	O
each	O
game	O
by	O
looking	O
at	O
their	O
own	O
hand	O
and	O
bidding	O
the	O
legal	O
bids	O
are	O
in	O
ascending	O
order	O
t	O
t	O
and	O
successive	O
bids	O
must	O
follow	O
this	O
order	O
a	O
bid	O
of	O
say	O
may	O
only	O
be	O
followed	O
by	O
higher	O
bids	O
such	O
as	O
or	O
or	O
t	O
us	O
neglect	O
the	O
double	O
bid	O
the	O
players	O
have	O
several	O
aims	O
when	O
bidding	O
one	O
of	O
the	O
aims	O
is	O
for	O
two	O
partners	O
to	O
communicate	O
to	O
each	O
other	O
as	O
much	O
as	O
possible	O
about	O
what	O
cards	O
are	O
in	O
their	O
hands	O
let	O
us	O
concentrate	O
on	O
this	O
task	O
after	O
the	O
cards	O
have	O
been	O
dealt	O
how	O
many	O
bits	O
are	O
needed	O
for	O
north	O
to	O
convey	O
to	O
south	O
what	O
her	O
hand	O
is	O
assuming	O
that	O
e	O
and	O
w	O
do	O
not	O
bid	O
at	O
all	O
what	O
is	O
the	O
maximum	O
total	O
information	B
that	O
n	O
and	O
s	O
can	O
convey	O
to	O
each	O
other	O
while	O
bidding	O
assume	O
that	O
n	O
starts	O
the	O
bidding	O
and	O
that	O
once	O
either	O
n	O
or	O
s	O
stops	O
bidding	O
the	O
bidding	O
stops	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
exercise	O
my	O
old	O
arabic	B
microwave	B
oven	I
had	O
buttons	O
for	O
entering	O
cooking	O
times	O
and	O
my	O
new	O
roman	B
microwave	O
has	O
just	O
the	O
buttons	O
of	O
the	O
roman	B
microwave	O
are	O
labelled	O
minutes	O
minute	O
seconds	O
second	O
and	O
start	O
i	O
ll	O
abbreviate	O
these	O
strings	O
to	O
the	O
symbols	O
m	O
c	O
x	O
i	O
to	O
enter	O
one	O
minute	O
and	O
twenty-three	O
seconds	O
the	O
arabic	B
sequence	B
is	O
arabic	B
roman	B
m	O
c	O
x	O
i	O
figure	O
alternative	O
keypads	O
for	O
microwave	O
ovens	O
and	O
the	O
roman	B
sequence	B
is	O
each	O
of	O
these	O
keypads	O
a	O
code	O
mapping	B
the	O
cooking	O
times	O
from	O
to	O
into	O
a	O
string	O
of	O
symbols	O
which	O
times	O
can	O
be	O
produced	O
with	O
two	O
or	O
three	O
symbols	O
example	O
can	O
be	O
produced	O
by	O
three	O
symbols	O
in	O
either	O
code	O
and	O
are	O
the	O
two	O
codes	O
complete	O
give	O
a	O
detailed	O
answer	O
for	O
each	O
code	O
name	O
a	O
cooking	O
time	O
that	O
it	O
can	O
produce	O
in	O
four	O
symbols	O
that	O
the	O
other	O
code	O
cannot	O
discuss	O
the	O
implicit	O
probability	B
distributions	O
over	O
times	O
to	O
which	O
each	O
of	O
these	O
codes	O
is	O
best	O
matched	O
concoct	O
a	O
plausible	O
probability	B
distribution	B
over	O
times	O
that	O
a	O
real	O
user	O
might	O
use	O
and	O
evaluate	O
roughly	O
the	O
expected	O
number	O
of	O
symbols	O
and	O
maximum	O
number	O
of	O
symbols	O
that	O
each	O
code	O
requires	O
discuss	O
the	O
ways	O
in	O
which	O
each	O
code	O
is	O
or	O
invent	O
a	O
more	O
cooking-time-encoding	O
system	O
for	O
a	O
mi	O
crowave	O
oven	O
exercise	O
is	O
the	O
standard	O
binary	O
representation	O
for	O
positive	O
inte	O
gers	O
a	O
uniquely	B
decodeable	I
code	O
design	O
a	O
binary	O
code	O
for	O
the	O
positive	O
integers	O
i	O
e	O
a	O
mapping	B
from	O
n	O
to	O
cn	O
that	O
is	O
uniquely	B
decodeable	I
try	O
to	O
design	O
codes	O
that	O
are	O
codes	O
and	O
that	O
satisfy	O
the	O
kraft	B
equality	O
pn	O
motivations	O
any	O
data	O
terminated	O
by	O
a	O
special	O
end	O
of	O
character	O
can	O
be	O
mapped	O
onto	O
an	O
integer	O
so	O
a	O
code	O
for	B
integers	I
can	O
be	O
used	O
as	O
a	O
self-delimiting	B
encoding	O
of	O
too	O
large	O
correspond	O
to	O
large	O
integers	O
also	O
one	O
of	O
the	O
building	O
blocks	O
of	O
a	O
universal	B
coding	O
scheme	O
that	O
is	O
a	O
coding	O
scheme	O
that	O
will	O
work	O
ok	O
for	O
a	O
large	O
variety	O
of	O
sources	O
is	O
the	O
ability	O
to	O
encode	O
integers	O
finally	O
in	O
microwave	O
ovens	O
cooking	O
times	O
are	O
positive	O
integers	O
discuss	O
criteria	O
by	O
which	O
one	O
might	O
compare	O
alternative	O
codes	O
for	B
integers	I
equivalently	O
alternative	O
self-delimiting	B
codes	O
for	O
solutions	O
solution	O
to	O
exercise	O
the	O
worst-case	O
situation	O
is	O
when	O
the	O
interval	O
to	O
be	O
represented	O
lies	O
just	O
inside	O
a	O
binary	O
interval	O
in	O
this	O
case	O
we	O
may	O
choose	O
either	O
of	O
two	O
binary	O
intervals	B
as	O
shown	O
in	O
these	O
binary	O
intervals	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
source	O
string	O
s	O
interval	O
binary	O
intervals	B
p	O
figure	O
termination	B
of	O
arithmetic	B
coding	I
in	O
the	O
worst	O
case	O
where	O
there	O
is	O
a	O
two	O
bit	B
overhead	O
either	O
of	O
the	O
two	O
binary	O
intervals	B
marked	O
on	O
the	O
right-hand	O
side	O
may	O
be	O
chosen	O
these	O
binary	O
intervals	B
are	O
no	O
smaller	O
than	O
p	O
are	O
no	O
smaller	O
than	O
p	O
so	O
the	O
binary	O
encoding	O
has	O
a	O
length	O
no	O
greater	O
than	O
which	O
is	O
two	O
bits	O
more	O
than	O
the	O
ideal	O
message	O
length	O
solution	O
to	O
exercise	O
the	O
standard	O
method	O
uses	O
random	B
bits	O
per	O
generated	O
symbol	O
and	O
so	O
requires	O
bits	O
to	O
generate	O
one	O
thousand	O
samples	O
arithmetic	B
coding	I
uses	O
on	O
average	O
about	O
bits	O
per	O
generated	O
symbol	O
and	O
so	O
requires	O
about	O
bits	O
to	O
generate	O
one	O
thousand	O
samples	O
an	O
overhead	O
of	O
roughly	O
two	O
bits	O
associated	O
with	O
termination	B
fluctuations	O
in	O
the	O
number	O
of	O
would	O
produce	O
variations	O
around	O
this	O
mean	B
with	O
standard	B
deviation	I
solution	O
to	O
exercise	O
the	O
encoding	O
is	O
which	O
comes	O
from	O
the	O
parsing	O
which	O
is	O
encoded	O
thus	O
solution	O
to	O
exercise	O
the	O
decoding	B
is	O
solution	O
to	O
exercise	O
this	O
problem	O
is	O
equivalent	O
to	O
exercise	O
bits	O
the	O
selection	O
of	O
k	O
objects	O
from	O
n	O
objects	O
requires	O
dlog	O
n	O
bits	O
this	O
selection	O
could	O
be	O
made	O
using	O
arithmetic	B
coding	I
the	O
selection	O
corresponds	O
to	O
a	O
binary	O
string	O
of	O
length	O
n	O
in	O
which	O
the	O
bits	O
represent	O
which	O
objects	O
are	O
selected	O
initially	O
the	O
probability	B
of	O
a	O
is	O
kn	O
and	O
the	O
probability	B
of	O
a	O
is	O
thereafter	O
given	O
that	O
the	O
emitted	O
string	O
thus	O
far	O
of	O
length	O
n	O
contains	O
k	O
the	O
probability	B
of	O
a	O
is	O
and	O
the	O
probability	B
of	O
a	O
is	O
solution	O
to	O
exercise	O
this	O
lempelziv	O
code	O
is	O
still	O
not	O
complete	O
because	O
for	O
example	O
after	O
have	O
been	O
collected	O
the	O
pointer	B
could	O
be	O
any	O
of	O
the	O
strings	O
but	O
it	O
cannot	O
be	O
or	O
thus	O
there	O
are	O
some	O
binary	O
strings	O
that	O
cannot	O
be	O
produced	O
as	O
encodings	O
solution	O
to	O
exercise	O
sources	O
with	O
low	O
entropy	B
that	O
are	O
not	O
well	O
compressed	O
by	O
lempelziv	O
include	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
sources	O
with	O
some	O
symbols	O
that	O
have	O
long	O
range	O
correlations	B
and	O
intervening	O
random	B
junk	O
an	O
ideal	O
model	B
should	O
capture	O
what	O
s	O
correlated	O
and	O
compress	B
it	O
lempelziv	O
can	O
compress	B
the	O
correlated	O
features	O
only	O
by	O
memorizing	O
all	O
cases	O
of	O
the	O
intervening	O
junk	O
as	O
a	O
simple	O
example	O
consider	O
a	O
telephone	O
book	O
in	O
which	O
every	O
line	O
contains	O
an	O
number	O
new	O
number	O
pair	O
the	O
number	O
of	O
characters	O
per	O
line	O
is	O
drawn	O
from	O
the	O
alphabet	O
the	O
characters	O
and	O
occur	O
in	O
a	O
predictable	O
sequence	B
so	O
the	O
true	O
information	B
content	I
per	O
line	O
assuming	O
all	O
the	O
phone	B
numbers	O
are	O
seven	O
digits	O
long	O
and	O
assuming	O
that	O
they	O
are	O
random	B
sequences	O
is	O
about	O
bans	O
ban	O
is	O
the	O
information	B
content	I
of	O
a	O
random	B
integer	O
between	O
and	O
a	O
state	O
language	B
model	B
could	O
easily	O
capture	O
the	O
regularities	O
in	O
these	O
data	O
a	O
lempelziv	O
algorithm	O
will	O
take	O
a	O
long	O
time	O
before	O
it	O
compresses	O
such	O
a	O
down	O
to	O
bans	O
per	O
line	O
however	O
because	O
in	O
order	O
for	O
it	O
to	O
learn	O
that	O
the	O
string	O
is	O
always	O
followed	O
by	O
for	O
any	O
three	O
digits	O
ddd	O
it	O
will	O
have	O
to	O
see	O
all	O
those	O
strings	O
so	O
near-optimal	O
compression	B
will	O
only	O
be	O
achieved	O
after	O
thousands	O
of	O
lines	O
of	O
the	O
have	O
been	O
read	O
figure	O
a	O
source	O
with	O
low	O
entropy	B
that	O
is	O
not	O
well	O
compressed	O
by	O
lempelziv	O
the	O
bit	B
sequence	B
is	O
read	O
from	O
left	O
to	O
right	O
each	O
line	O
from	O
the	O
line	O
above	O
in	O
f	O
of	O
its	O
bits	O
the	O
image	B
width	O
is	O
pixels	O
sources	O
with	O
long	O
range	O
correlations	B
for	O
example	O
two-dimensional	B
images	B
that	O
are	O
represented	O
by	O
a	O
sequence	B
of	O
pixels	O
row	O
by	O
row	O
so	O
that	O
vertically	O
adjacent	O
pixels	O
are	O
a	O
distance	B
w	O
apart	O
in	O
the	O
source	O
stream	O
where	O
w	O
is	O
the	O
image	B
width	O
consider	O
for	O
example	O
a	O
fax	O
transmission	O
in	O
which	O
each	O
line	O
is	O
very	O
similar	O
to	O
the	O
previous	O
line	O
the	O
true	O
entropy	B
is	O
only	O
per	O
pixel	O
where	O
f	O
is	O
the	O
probability	B
that	O
a	O
pixel	O
from	O
its	O
parent	B
lempelziv	O
algorithms	B
will	O
only	O
compress	B
down	O
to	O
the	O
entropy	B
once	O
all	O
strings	O
of	O
length	O
have	O
occurred	O
and	O
their	O
successors	O
have	O
been	O
memorized	O
there	O
are	O
only	O
about	O
particles	O
in	O
the	O
universe	O
so	O
we	O
can	O
say	O
that	O
lempelziv	O
codes	O
will	O
never	O
capture	O
the	O
redundancy	B
of	O
such	O
an	O
image	B
another	O
highly	O
redundant	O
texture	O
is	O
shown	O
in	O
the	O
image	B
was	O
made	O
by	O
dropping	O
horizontal	O
and	O
vertical	O
pins	O
randomly	O
on	O
the	O
plane	O
it	O
contains	O
both	O
long-range	O
vertical	O
correlations	B
and	O
long-range	O
horizontal	O
correlations	B
there	O
is	O
no	O
practical	B
way	O
that	O
lempelziv	O
fed	O
with	O
a	O
pixel-by-pixel	O
scan	O
of	O
this	O
image	B
could	O
capture	O
both	O
these	O
correlations	B
biological	O
computational	O
systems	O
can	O
readily	O
identify	O
the	O
redundancy	B
in	O
these	O
images	B
and	O
in	B
images	B
that	O
are	O
much	O
more	O
complex	B
thus	O
we	O
might	O
anticipate	O
that	O
the	O
best	O
data	O
compression	B
algorithms	B
will	O
result	O
from	O
the	O
development	O
of	O
intelligence	O
methods	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
stream	B
codes	I
figure	O
a	O
texture	O
consisting	O
of	O
horizontal	O
and	O
vertical	O
pins	O
dropped	O
at	O
random	B
on	O
the	O
plane	O
sources	O
with	O
intricate	O
redundancy	B
such	O
as	O
generated	O
by	O
computers	O
for	O
example	O
a	O
latex	O
followed	O
by	O
its	O
encoding	O
into	O
a	O
postscript	O
the	O
information	B
content	I
of	O
this	O
pair	O
of	O
is	O
roughly	O
equal	O
to	O
the	O
information	B
content	I
of	O
the	O
latex	O
alone	O
a	O
picture	O
of	O
the	O
mandelbrot	B
set	B
the	O
picture	O
has	O
an	O
information	B
content	I
equal	O
to	O
the	O
number	O
of	O
bits	O
required	O
to	O
specify	O
the	O
range	O
of	O
the	O
complex	B
plane	O
studied	O
the	O
pixel	O
sizes	O
and	O
the	O
colouring	O
rule	O
used	O
a	O
picture	O
of	O
a	O
ground	O
state	O
of	O
a	O
frustrated	O
antiferromagnetic	B
ising	B
model	B
which	O
we	O
will	O
discuss	O
in	O
chapter	O
like	O
this	O
binary	O
image	B
has	O
interesting	O
correlations	B
in	O
two	O
directions	O
figure	O
frustrated	O
triangular	O
ising	B
model	B
in	O
one	O
of	O
its	O
ground	O
states	O
cellular	O
automata	O
shows	O
the	O
state	O
history	O
of	O
steps	O
of	O
a	O
cellular	B
automaton	I
with	O
cells	O
the	O
update	O
rule	O
in	O
which	O
each	O
cell	O
s	O
new	O
state	O
depends	O
on	O
the	O
state	O
of	O
preceding	O
cells	O
was	O
selected	O
at	O
random	B
the	O
information	B
content	I
is	O
equal	O
to	O
the	O
information	B
in	O
the	O
boundary	O
bits	O
and	O
the	O
propagation	O
rule	O
which	O
here	O
can	O
be	O
described	O
in	O
bits	O
an	O
optimal	B
compressor	O
will	O
thus	O
give	O
a	O
compressed	O
length	O
which	O
is	O
essentially	O
constant	O
independent	O
of	O
the	O
vertical	O
height	O
of	O
the	O
image	B
lempelziv	O
would	O
only	O
give	O
this	O
zero-cost	O
compression	B
once	O
the	O
cellular	B
automaton	I
has	O
entered	O
a	O
periodic	O
limit	B
cycle	I
which	O
could	O
easily	O
take	O
about	O
iterations	O
in	O
contrast	O
the	O
jbig	O
compression	B
method	O
which	O
models	O
the	O
probability	B
of	O
a	O
pixel	O
given	O
its	O
local	O
context	O
and	O
uses	O
arithmetic	B
coding	I
would	O
do	O
a	O
good	B
job	O
on	O
these	O
images	B
solution	O
to	O
exercise	O
for	O
a	O
one-dimensional	O
gaussian	B
the	O
variance	B
of	O
x	O
is	O
so	O
the	O
mean	B
value	O
of	O
in	O
n	O
dimensions	B
since	O
the	O
components	O
of	O
x	O
are	O
independent	O
random	B
variables	O
is	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
figure	O
the	O
time-history	O
of	O
a	O
cellular	B
automaton	I
with	O
cells	O
the	O
variance	B
of	O
similarly	O
is	O
n	O
times	O
the	O
variance	B
of	O
where	O
x	O
is	O
a	O
one-dimensional	O
gaussian	B
variable	O
dx	O
the	O
integral	B
is	O
found	O
to	O
be	O
so	O
thus	O
the	O
variance	B
of	O
is	O
for	O
large	O
n	O
the	O
central-limit	B
theorem	I
indicates	O
that	O
r	O
has	O
a	O
gaussian	B
distribution	B
with	O
mean	B
n	O
and	O
standard	B
deviation	I
so	O
the	O
probability	B
density	B
of	O
r	O
must	O
similarly	O
be	O
concentrated	O
about	O
r	O
pn	O
the	O
thickness	O
of	O
this	O
shell	O
is	O
given	O
by	O
turning	O
the	O
standard	B
deviation	I
of	O
into	O
a	O
standard	B
deviation	I
on	O
r	O
for	O
small	O
log	O
r	O
log	O
so	O
setting	O
r	O
has	O
standard	B
deviation	I
the	O
probability	B
density	B
of	O
the	O
gaussian	B
at	O
a	O
point	O
xshell	O
where	O
r	O
pn	O
is	O
p	O
n	O
n	O
whereas	O
the	O
probability	B
density	B
at	O
the	O
origin	O
is	O
p	O
thus	O
p	O
exp	O
the	O
probability	B
density	B
at	O
the	O
typical	B
radius	O
is	O
times	O
smaller	O
than	O
the	O
density	B
at	O
the	O
origin	O
if	O
n	O
then	O
the	O
probability	B
density	B
at	O
the	O
origin	O
is	O
times	O
greater	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
codes	O
for	B
integers	I
this	O
chapter	O
is	O
an	O
aside	O
which	O
may	O
safely	O
be	O
skipped	O
solution	O
to	O
exercise	O
to	O
discuss	O
the	O
coding	O
of	O
integers	O
we	O
need	O
some	O
the	O
standard	O
binary	O
representation	O
of	O
a	O
positive	O
integer	O
n	O
will	O
be	O
denoted	O
by	O
cbn	O
e	O
g	O
the	O
standard	O
binary	O
length	O
of	O
a	O
positive	O
integer	O
n	O
lbn	O
length	O
of	O
the	O
string	O
cbn	O
for	O
example	O
is	O
the	O
the	O
standard	O
binary	O
representation	O
cbn	O
is	O
not	O
a	O
uniquely	B
decodeable	I
code	O
for	B
integers	I
since	O
there	O
is	O
no	O
way	O
of	O
knowing	O
when	O
an	O
integer	O
has	O
ended	O
for	O
example	O
is	O
identical	O
to	O
it	O
would	O
be	O
uniquely	B
decodeable	I
if	O
we	O
knew	O
the	O
standard	O
binary	O
length	O
of	O
each	O
integer	O
before	O
it	O
was	O
received	O
noticing	O
that	O
all	O
positive	O
integers	O
have	O
a	O
standard	O
binary	O
representation	O
that	O
starts	O
with	O
a	O
we	O
might	O
another	O
representation	O
the	O
headless	O
binary	O
representation	O
of	O
a	O
positive	O
integer	O
n	O
will	O
be	O
denoted	O
by	O
cbn	O
e	O
g	O
and	O
denotes	O
the	O
null	O
string	O
this	O
representation	O
would	O
be	O
uniquely	B
decodeable	I
if	O
we	O
knew	O
the	O
length	O
lbn	O
of	O
the	O
integer	O
so	O
how	O
can	O
we	O
make	O
a	O
uniquely	B
decodeable	I
code	O
for	B
integers	I
two	O
strate	O
gies	O
can	O
be	O
distinguished	O
self-delimiting	B
codes	O
we	O
communicate	O
somehow	O
the	O
length	O
of	O
the	O
integer	O
lbn	O
which	O
is	O
also	O
a	O
positive	O
integer	O
then	O
communicate	O
the	O
original	O
integer	O
n	O
itself	O
using	O
cbn	O
codes	O
with	O
end	O
of	O
characters	O
we	O
code	O
the	O
integer	O
into	O
blocks	O
of	O
length	O
b	O
bits	O
and	O
reserve	O
one	O
of	O
the	O
symbols	O
to	O
have	O
the	O
special	O
meaning	O
end	O
of	O
the	O
coding	O
of	O
integers	O
into	O
blocks	O
is	O
arranged	O
so	O
that	O
this	O
reserved	O
symbol	O
is	O
not	O
needed	O
for	O
any	O
other	O
purpose	O
the	O
simplest	O
uniquely	B
decodeable	I
code	O
for	B
integers	I
is	O
the	O
unary	O
code	O
which	O
can	O
be	O
viewed	O
as	O
a	O
code	O
with	O
an	O
end	O
of	O
character	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
codes	O
for	B
integers	I
unary	O
code	O
an	O
integer	O
n	O
is	O
encoded	O
by	O
sending	O
a	O
string	O
of	O
followed	O
by	O
a	O
n	O
cun	O
the	O
unary	O
code	O
has	O
length	O
lun	O
n	O
the	O
unary	O
code	O
is	O
the	O
optimal	B
code	O
for	B
integers	I
if	O
the	O
probability	B
distribution	B
over	O
n	O
is	O
pun	O
self-delimiting	B
codes	O
we	O
can	O
use	O
the	O
unary	O
code	O
to	O
encode	O
the	O
length	O
of	O
the	O
binary	O
encoding	O
of	O
n	O
and	O
make	O
a	O
self-delimiting	B
code	O
code	O
we	O
send	O
the	O
unary	O
code	O
for	O
lbn	O
followed	O
by	O
the	O
headless	O
binary	O
representation	O
of	O
n	O
culbncbn	O
table	O
shows	O
the	O
codes	O
for	O
some	O
integers	O
the	O
overlining	O
indicates	O
the	O
division	O
of	O
each	O
string	O
into	O
the	O
parts	O
culbn	O
and	O
cbn	O
we	O
might	O
equivalently	O
view	O
as	O
consisting	O
of	O
a	O
string	O
of	O
zeroes	O
followed	O
by	O
the	O
standard	O
binary	O
representation	O
of	O
n	O
cbn	O
the	O
codeword	B
has	O
length	O
the	O
implicit	O
probability	B
distribution	B
over	O
n	O
for	O
the	O
code	O
is	O
separable	O
into	O
the	O
product	O
of	O
a	O
probability	B
distribution	B
over	O
the	O
length	O
l	O
and	O
a	O
uniform	O
distribution	B
over	O
integers	O
having	O
that	O
length	O
p	O
p	O
l	O
lbn	O
l	O
otherwise	O
now	O
for	O
the	O
above	O
code	O
the	O
header	O
that	O
communicates	O
the	O
length	O
always	O
occupies	O
the	O
same	O
number	O
of	O
bits	O
as	O
the	O
standard	O
binary	O
representation	O
of	O
the	O
integer	O
or	O
take	O
one	O
if	O
we	O
are	O
expecting	O
to	O
encounter	O
large	O
integers	O
then	O
this	O
representation	O
seems	O
suboptimal	O
since	O
it	O
leads	O
to	O
all	O
occupying	O
a	O
size	O
that	O
is	O
double	O
their	O
original	O
uncoded	O
size	O
instead	O
of	O
using	O
the	O
unary	O
code	O
to	O
encode	O
the	O
length	O
lbn	O
we	O
could	O
use	O
code	O
we	O
send	O
the	O
length	O
lbn	O
using	O
followed	O
by	O
the	O
headless	O
binary	O
representation	O
of	O
n	O
iterating	O
this	O
procedure	O
we	O
can	O
a	O
sequence	B
of	O
codes	O
code	O
code	O
n	O
cbn	O
lbn	O
table	O
n	O
table	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
codes	O
for	B
integers	I
n	O
table	O
two	O
codes	O
with	O
symbols	O
and	O
spaces	O
have	O
been	O
included	O
to	O
show	O
the	O
byte	B
boundaries	O
codes	O
with	O
symbols	O
we	O
can	O
also	O
make	O
byte-based	O
representations	O
s	O
use	O
the	O
term	O
byte	B
here	O
to	O
denote	O
any	O
string	O
of	O
bits	O
not	O
just	O
a	O
string	O
of	O
length	O
bits	O
if	O
we	O
encode	O
the	O
number	O
in	O
some	O
base	O
for	O
example	O
decimal	O
then	O
we	O
can	O
represent	O
each	O
digit	O
in	O
a	O
byte	B
in	O
order	O
to	O
represent	O
a	O
digit	O
from	O
to	O
in	O
a	O
byte	B
we	O
need	O
four	O
bits	O
because	O
this	O
leaves	O
extra	O
four-bit	O
symbols	O
that	O
correspond	O
to	O
no	O
decimal	O
digit	O
we	O
can	O
use	O
these	O
as	O
symbols	O
to	O
indicate	O
the	O
end	O
of	O
our	O
positive	O
integer	O
clearly	O
it	O
is	O
redundant	O
to	O
have	O
more	O
than	O
one	O
symbol	O
so	O
a	O
more	O
code	O
would	O
encode	O
the	O
integer	O
into	O
base	O
and	O
use	O
just	O
the	O
sixteenth	O
symbol	O
as	O
the	O
punctuation	O
character	O
generalizing	O
this	O
idea	O
we	O
can	O
make	O
similar	O
byte-based	O
codes	O
for	B
integers	I
in	O
bases	O
and	O
and	O
in	O
any	O
base	O
of	O
the	O
form	O
these	O
codes	O
are	O
almost	O
complete	O
that	O
a	O
code	O
is	O
complete	O
if	O
it	O
the	O
kraft	B
inequality	B
with	O
equality	O
the	O
codes	O
remaining	O
is	O
that	O
they	O
provide	O
the	O
ability	O
to	O
encode	O
the	O
integer	O
zero	O
and	O
the	O
empty	B
string	I
neither	O
of	O
which	O
was	O
required	O
exercise	O
consider	O
the	O
implicit	O
probability	B
distribution	B
over	O
inte	O
gers	O
corresponding	O
to	O
the	O
code	O
with	O
an	O
character	O
if	O
the	O
code	O
has	O
eight-bit	O
blocks	O
the	O
integer	O
is	O
coded	O
in	O
base	O
what	O
is	O
the	O
mean	B
length	O
in	O
bits	O
of	O
the	O
integer	O
under	O
the	O
implicit	O
distribution	B
if	O
one	O
wishes	O
to	O
encode	O
binary	O
of	O
expected	O
size	O
about	O
one	O
hundred	O
kilobytes	O
using	O
a	O
code	O
with	O
an	O
character	O
what	O
is	O
the	O
optimal	B
block	B
size	O
encoding	O
a	O
tiny	O
to	O
illustrate	O
the	O
codes	O
we	O
have	O
discussed	O
we	O
now	O
use	O
each	O
code	O
to	O
encode	O
a	O
small	O
consisting	O
of	O
just	O
characters	O
claude	O
shannon	B
if	O
we	O
map	O
the	O
ascii	O
characters	O
onto	O
seven-bit	O
symbols	O
in	O
decimal	O
c	O
l	O
etc	O
this	O
character	O
corresponds	O
to	O
the	O
integer	O
n	O
the	O
unary	O
code	O
for	O
n	O
consists	O
of	O
this	O
many	O
one	O
zeroes	O
followed	O
by	O
a	O
one	O
if	O
all	O
the	O
oceans	O
were	O
turned	O
into	O
ink	O
and	O
if	O
we	O
wrote	O
a	O
hundred	O
bits	O
with	O
every	O
cubic	O
millimeter	O
there	O
might	O
be	O
enough	O
ink	O
to	O
write	O
cun	O
the	O
standard	O
binary	O
representation	O
of	O
n	O
is	O
this	O
sequence	B
of	O
bits	O
cbn	O
exercise	O
write	O
down	O
or	O
describe	O
the	O
following	O
self-delimiting	B
representations	O
of	O
the	O
above	O
number	O
n	O
and	O
which	O
of	O
these	O
encodings	O
is	O
the	O
shortest	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
algorithm	O
elias	B
s	O
encoder	B
for	O
an	O
integer	O
n	O
codes	O
for	B
integers	I
comparing	O
the	O
codes	O
one	O
could	O
answer	O
the	O
question	O
which	O
of	O
two	O
codes	O
is	O
superior	O
by	O
a	O
sentence	O
of	O
the	O
form	O
for	O
n	O
k	O
code	O
is	O
superior	O
for	O
n	O
k	O
code	O
is	O
superior	O
but	O
i	O
contend	O
that	O
such	O
an	O
answer	O
misses	O
the	O
point	O
any	O
complete	O
code	O
corresponds	O
to	O
a	O
prior	B
for	O
which	O
it	O
is	O
optimal	B
you	O
should	O
not	O
say	O
that	O
any	O
other	O
code	O
is	O
superior	O
to	O
it	O
other	O
codes	O
are	O
optimal	B
for	O
other	O
priors	O
these	O
implicit	O
priors	O
should	O
be	O
thought	O
about	O
so	O
as	O
to	O
achieve	O
the	O
best	O
code	O
for	O
one	O
s	O
application	O
notice	O
that	O
one	O
cannot	O
for	O
free	O
switch	O
from	O
one	O
code	O
to	O
another	O
choosing	O
if	O
one	O
were	O
to	O
do	O
this	O
then	O
it	O
would	O
be	O
necessary	O
to	O
whichever	O
is	O
shorter	O
lengthen	O
the	O
message	O
in	O
some	O
way	O
that	O
indicates	O
which	O
of	O
the	O
two	O
codes	O
is	O
being	O
used	O
if	O
this	O
is	O
done	O
by	O
a	O
single	O
leading	O
bit	B
it	O
will	O
be	O
found	O
that	O
the	O
resulting	O
code	O
is	O
suboptimal	O
because	O
it	O
fails	O
the	O
kraft	B
equality	O
as	O
was	O
discussed	O
in	O
exercise	O
another	O
way	O
to	O
compare	O
codes	O
for	B
integers	I
is	O
to	O
consider	O
a	O
sequence	B
of	O
probability	B
distributions	O
such	O
as	O
monotonic	O
probability	B
distributions	O
over	O
n	O
and	O
rank	O
the	O
codes	O
as	O
to	O
how	O
well	O
they	O
encode	O
any	O
of	O
these	O
distributions	O
a	O
code	O
is	O
called	O
a	O
universal	B
code	O
if	O
for	O
any	O
distribution	B
in	O
a	O
given	O
class	O
it	O
encodes	O
into	O
an	O
average	O
length	O
that	O
is	O
within	O
some	O
factor	O
of	O
the	O
ideal	O
average	O
length	O
let	O
me	O
say	O
this	O
again	O
we	O
are	O
meeting	O
an	O
alternative	O
world	O
view	O
rather	O
than	O
out	O
a	O
good	B
prior	B
over	O
integers	O
as	O
advocated	O
above	O
many	O
theorists	O
have	O
studied	O
the	O
problem	O
of	O
creating	O
codes	O
that	O
are	O
reasonably	O
good	B
codes	O
for	O
any	O
priors	O
in	O
a	O
broad	O
class	O
here	O
the	O
class	O
of	O
priors	O
conventionally	O
considered	O
is	O
the	O
set	B
of	O
priors	O
that	O
assign	O
a	O
monotonically	O
decreasing	O
probability	B
over	O
integers	O
and	O
have	O
entropy	B
several	O
of	O
the	O
codes	O
we	O
have	O
discussed	O
above	O
are	O
universal	B
another	O
code	O
which	O
elegantly	O
transcends	O
the	O
sequence	B
of	O
self-delimiting	B
codes	O
is	O
elias	B
s	O
universal	B
code	O
for	B
integers	I
which	O
chooses	O
from	O
all	O
the	O
codes	O
it	O
works	O
by	O
sending	O
a	O
sequence	B
of	O
messages	O
each	O
of	O
which	O
encodes	O
the	O
length	O
of	O
the	O
next	O
message	O
and	O
indicates	O
by	O
a	O
single	O
bit	B
whether	O
or	O
not	O
that	O
message	O
is	O
the	O
integer	O
its	O
standard	O
binary	O
representation	O
because	O
a	O
length	O
is	O
a	O
positive	O
integer	O
and	O
all	O
positive	O
integers	O
begin	O
with	O
all	O
the	O
leading	O
can	O
be	O
omitted	O
write	O
loop	O
f	O
g	O
if	O
blog	O
nc	O
halt	O
prepend	O
cbn	O
to	O
the	O
written	O
string	O
nblog	O
nc	O
the	O
encoder	B
of	O
c	O
is	O
shown	O
in	O
algorithm	O
the	O
encoding	O
is	O
generated	O
from	O
right	O
to	O
left	O
table	O
shows	O
the	O
resulting	O
codewords	O
exercise	O
show	O
that	O
the	O
elias	B
code	O
is	O
not	O
actually	O
the	O
best	O
code	O
for	O
a	O
prior	B
distribution	B
that	O
expects	O
very	O
large	O
integers	O
this	O
by	O
constructing	O
another	O
code	O
and	O
specifying	O
how	O
large	O
n	O
must	O
be	O
for	O
your	O
code	O
to	O
give	O
a	O
shorter	O
length	O
than	O
elias	B
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
n	O
c	O
n	O
solutions	O
codes	O
for	B
integers	I
n	O
c	O
n	O
n	O
c	O
n	O
n	O
c	O
n	O
table	O
elias	B
s	O
universal	B
code	O
for	B
integers	I
examples	O
from	O
to	O
solution	O
to	O
exercise	O
the	O
use	O
of	O
the	O
symbol	O
in	O
a	O
code	O
that	O
represents	O
the	O
integer	O
in	O
some	O
base	O
q	O
corresponds	O
to	O
a	O
belief	B
that	O
there	O
is	O
a	O
probability	B
of	O
that	O
the	O
current	O
character	O
is	O
the	O
last	O
character	O
of	O
the	O
number	O
thus	O
the	O
prior	B
to	O
which	O
this	O
code	O
is	O
matched	O
puts	O
an	O
exponential	B
prior	B
distribution	B
over	O
the	O
length	O
of	O
the	O
integer	O
the	O
expected	O
number	O
of	O
characters	O
is	O
q	O
so	O
the	O
expected	O
length	O
of	O
the	O
integer	O
is	O
bits	O
we	O
wish	O
to	O
q	O
such	O
that	O
q	O
log	O
q	O
bits	O
a	O
value	O
of	O
q	O
between	O
and	O
this	O
constraint	O
so	O
blocks	O
are	O
roughly	O
the	O
optimal	B
size	O
assuming	O
there	O
is	O
one	O
character	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
part	O
ii	O
noisy-channel	O
coding	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
dependent	O
random	B
variables	O
in	O
the	O
last	O
three	O
chapters	O
on	O
data	O
compression	B
we	O
concentrated	O
on	O
random	B
vectors	B
x	O
coming	O
from	O
an	O
extremely	O
simple	O
probability	B
distribution	B
namely	O
the	O
separable	O
distribution	B
in	O
which	O
each	O
component	O
xn	O
is	O
independent	O
of	O
the	O
others	O
in	O
this	O
chapter	O
we	O
consider	O
joint	B
ensembles	O
in	O
which	O
the	O
random	B
variables	O
are	O
dependent	O
this	O
material	O
has	O
two	O
motivations	O
first	O
data	O
from	O
the	O
real	O
world	O
have	O
interesting	O
correlations	B
so	O
to	O
do	O
data	O
compression	B
well	O
we	O
need	O
to	O
know	O
how	O
to	O
work	O
with	O
models	O
that	O
include	O
dependences	O
second	O
a	O
noisy	B
channel	O
with	O
input	O
x	O
and	O
output	O
y	O
a	O
joint	B
ensemble	B
in	O
which	O
x	O
and	O
y	O
are	O
dependent	O
if	O
they	O
were	O
independent	O
it	O
would	O
be	O
impossible	O
to	O
communicate	O
over	O
the	O
channel	O
so	O
communication	B
over	O
noisy	B
channels	O
topic	O
of	O
chapters	O
is	O
described	O
in	O
terms	O
of	O
the	O
entropy	B
of	O
joint	B
ensembles	O
more	O
about	O
entropy	B
this	O
section	B
gives	O
and	O
exercises	O
to	O
do	O
with	O
entropy	B
carrying	O
on	O
from	O
section	B
the	O
joint	B
entropy	B
of	O
x	O
y	O
is	O
hx	O
y	O
p	O
y	O
log	O
p	O
y	O
entropy	B
is	O
additive	O
for	O
independent	O
random	B
variables	O
hx	O
y	O
hx	O
hy	O
p	O
y	O
p	O
the	O
conditional	B
entropy	B
of	O
x	O
given	O
y	O
bk	O
is	O
the	O
entropy	B
of	O
the	O
proba	O
bility	O
distribution	B
p	O
y	O
bk	O
hx	O
j	O
y	O
bk	O
p	O
y	O
bk	O
log	O
p	O
y	O
bk	O
the	O
conditional	B
entropy	B
of	O
x	O
given	O
y	O
is	O
the	O
average	O
over	O
y	O
of	O
the	O
con	O
ditional	O
entropy	B
of	O
x	O
given	O
y	O
hx	O
j	O
y	O
p	O
p	O
y	O
log	O
p	O
p	O
y	O
log	O
p	O
y	O
this	O
measures	O
the	O
average	O
uncertainty	O
that	O
remains	O
about	O
x	O
when	O
y	O
is	O
known	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
about	O
entropy	B
the	O
marginal	B
entropy	B
of	O
x	O
is	O
another	O
name	O
for	O
the	O
entropy	B
of	O
x	O
hx	O
used	O
to	O
contrast	O
it	O
with	O
the	O
conditional	B
entropies	O
listed	O
above	O
chain	B
rule	I
for	O
information	B
content	I
from	O
the	O
product	O
rule	O
for	O
probabil	O
ities	O
equation	O
we	O
obtain	O
so	O
log	O
p	O
y	O
log	O
p	O
log	O
p	O
j	O
x	O
hx	O
y	O
hx	O
hy	O
j	O
x	O
in	O
words	O
this	O
says	O
that	O
the	O
information	B
content	I
of	O
x	O
and	O
y	O
is	O
the	O
information	B
content	I
of	O
x	O
plus	O
the	O
information	B
content	I
of	O
y	O
given	O
x	O
chain	B
rule	I
for	O
entropy	B
the	O
joint	B
marginal	B
entropy	B
are	O
related	O
by	O
entropy	B
conditional	B
entropy	B
and	O
hx	O
y	O
hx	O
hy	O
j	O
x	O
hy	O
hx	O
j	O
y	O
in	O
words	O
this	O
says	O
that	O
the	O
uncertainty	O
of	O
x	O
and	O
y	O
is	O
the	O
uncertainty	O
of	O
x	O
plus	O
the	O
uncertainty	O
of	O
y	O
given	O
x	O
the	O
mutual	B
information	B
between	O
x	O
and	O
y	O
is	O
ix	O
y	O
hx	O
hx	O
j	O
y	O
and	O
ix	O
y	O
iy	O
x	O
and	O
ix	O
y	O
it	O
measures	O
the	O
average	O
reduction	O
in	O
uncertainty	O
about	O
x	O
that	O
results	O
from	O
learning	B
the	O
value	O
of	O
y	O
or	O
vice	O
versa	O
the	O
average	O
amount	O
of	O
information	B
that	O
x	O
conveys	O
about	O
y	O
the	O
conditional	B
mutual	B
information	B
between	O
x	O
and	O
y	O
given	O
z	O
ck	O
is	O
the	O
mutual	B
information	B
between	O
the	O
random	B
variables	O
x	O
and	O
y	O
in	O
the	O
joint	B
ensemble	B
p	O
y	O
j	O
z	O
ck	O
ix	O
y	O
j	O
z	O
ck	O
hx	O
j	O
z	O
ck	O
hx	O
j	O
y	O
z	O
ck	O
the	O
conditional	B
mutual	B
information	B
between	O
x	O
and	O
y	O
given	O
z	O
is	O
the	O
average	O
over	O
z	O
of	O
the	O
above	O
conditional	B
mutual	B
information	B
ix	O
y	O
j	O
z	O
hx	O
j	O
z	O
hx	O
j	O
y	O
z	O
no	O
other	O
three-term	O
entropies	O
will	O
be	O
for	O
example	O
expressions	O
such	O
as	O
ix	O
y	O
z	O
and	O
ix	O
j	O
y	O
z	O
are	O
illegal	O
but	O
you	O
may	O
put	O
conjunctions	O
of	O
arbitrary	O
numbers	O
of	O
variables	O
in	O
each	O
of	O
the	O
three	O
spots	O
in	O
the	O
expression	O
ix	O
y	O
j	O
z	O
for	O
example	O
ia	O
b	O
c	O
d	O
j	O
e	O
f	O
is	O
it	O
measures	O
how	O
much	O
information	B
on	O
average	O
c	O
and	O
d	O
convey	O
about	O
a	O
and	O
b	O
assuming	O
e	O
and	O
f	O
are	O
known	O
figure	O
shows	O
how	O
the	O
total	O
entropy	B
hx	O
y	O
of	O
a	O
joint	B
ensemble	B
can	O
be	O
broken	O
down	O
this	O
is	O
important	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
dependent	O
random	B
variables	O
hx	O
y	O
hx	O
hy	O
hx	O
j	O
y	O
ix	O
y	O
hy	O
jx	O
figure	O
the	O
relationship	O
between	O
joint	B
information	B
marginal	B
entropy	B
conditional	B
entropy	B
and	O
mutual	O
entropy	B
exercises	O
exercise	O
consider	O
three	O
independent	O
random	B
variables	O
u	O
v	O
w	O
with	O
entropies	O
hu	O
hv	O
hw	O
let	O
x	O
v	O
and	O
y	O
w	O
what	O
is	O
hx	O
y	O
what	O
is	O
hx	O
j	O
y	O
what	O
is	O
ix	O
y	O
exercise	O
referring	O
to	O
the	O
of	O
conditional	B
entropy	B
an	O
example	O
that	O
it	O
is	O
possible	O
for	O
hx	O
j	O
y	O
bk	O
to	O
exceed	O
hx	O
but	O
that	O
the	O
average	O
hx	O
j	O
y	O
is	O
less	O
than	O
hx	O
so	O
data	O
are	O
helpful	O
they	O
do	O
not	O
increase	O
uncertainty	O
on	O
average	O
exercise	O
prove	O
the	O
chain	B
rule	I
for	O
entropy	B
equation	O
y	O
hx	O
hy	O
j	O
x	O
exercise	O
prove	O
that	O
the	O
mutual	B
information	B
ix	O
y	O
hx	O
hx	O
j	O
y	O
ix	O
y	O
iy	O
x	O
and	O
ix	O
y	O
see	O
exercise	O
and	O
note	O
that	O
ix	O
y	O
dklp	O
yjjp	O
exercise	O
the	O
entropy	B
distance	B
between	O
two	O
random	B
variables	O
can	O
be	O
to	O
be	O
the	O
between	O
their	O
joint	B
entropy	B
and	O
their	O
mutual	B
information	B
dh	O
y	O
hx	O
y	O
ix	O
y	O
prove	O
that	O
the	O
entropy	B
distance	B
the	O
axioms	O
for	O
a	O
distance	B
dh	O
y	O
dh	O
x	O
dh	O
y	O
dh	O
x	O
and	O
dh	O
z	O
dh	O
y	O
dh	O
z	O
we	O
are	O
unlikely	O
to	O
see	O
dh	O
y	O
again	O
but	O
it	O
is	O
a	O
good	B
function	O
on	O
which	O
to	O
practise	O
inequality-proving	O
exercise	O
a	O
joint	B
ensemble	B
xy	O
has	O
the	O
following	O
joint	B
distribution	B
p	O
y	O
x	O
y	O
what	O
is	O
the	O
joint	B
entropy	B
hx	O
y	O
what	O
are	O
the	O
marginal	B
entropies	O
hx	O
and	O
hy	O
for	O
each	O
value	O
of	O
y	O
what	O
is	O
the	O
conditional	B
entropy	B
hx	O
j	O
y	O
what	O
is	O
the	O
conditional	B
entropy	B
hx	O
j	O
y	O
what	O
is	O
the	O
conditional	B
entropy	B
of	O
y	O
given	O
x	O
what	O
is	O
the	O
mutual	B
information	B
between	O
x	O
and	O
y	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
exercise	O
consider	O
the	O
ensemble	B
xy	O
z	O
in	O
which	O
ax	O
ay	O
az	O
x	O
and	O
y	O
are	O
independent	O
with	O
px	O
fp	O
pg	O
and	O
py	O
fq	O
and	O
z	O
y	O
mod	O
if	O
q	O
what	O
is	O
pz	O
what	O
is	O
iz	O
x	O
for	O
general	O
p	O
and	O
q	O
what	O
is	O
pz	O
what	O
is	O
iz	O
x	O
notice	O
that	O
this	O
ensemble	B
is	O
related	O
to	O
the	O
binary	B
symmetric	B
channel	I
with	O
x	O
input	O
y	O
noise	O
and	O
z	O
output	O
hy	O
figure	O
a	O
misleading	O
representation	O
of	O
entropies	O
with	O
hxy	O
ixy	O
hyx	O
hxy	O
hx	O
three	O
term	O
entropies	O
exercise	O
many	O
texts	O
draw	O
in	O
the	O
form	O
of	O
a	O
venn	O
diagram	O
discuss	O
why	O
this	O
diagram	O
is	O
a	O
misleading	O
representation	O
of	O
entropies	O
hint	O
consider	O
the	O
three-variable	O
ensemble	B
xy	O
z	O
in	O
which	O
x	O
and	O
y	O
are	O
independent	O
binary	O
variables	O
and	O
z	O
is	O
to	O
be	O
z	O
x	O
y	O
mod	O
further	O
exercises	O
the	O
data-processing	O
theorem	O
the	O
data	O
processing	O
theorem	O
states	O
that	O
data	O
processing	O
can	O
only	O
destroy	O
information	B
exercise	O
prove	O
this	O
theorem	O
by	O
considering	O
an	O
ensemble	B
w	O
dr	O
in	O
which	O
w	O
is	O
the	O
state	O
of	O
the	O
world	O
d	O
is	O
data	O
gathered	O
and	O
r	O
is	O
the	O
processed	O
data	O
so	O
that	O
these	O
three	O
variables	O
form	O
a	O
markov	B
chain	I
that	O
is	O
the	O
probability	B
p	O
d	O
r	O
can	O
be	O
written	O
as	O
w	O
d	O
r	O
p	O
d	O
r	O
p	O
wp	O
j	O
d	O
show	O
that	O
the	O
average	O
information	B
that	O
r	O
conveys	O
about	O
w	O
iw	O
r	O
is	O
less	O
than	O
or	O
equal	O
to	O
the	O
average	O
information	B
that	O
d	O
conveys	O
about	O
w	O
iw	O
d	O
this	O
theorem	O
is	O
as	O
much	O
a	O
caution	O
about	O
our	O
of	O
information	B
as	O
it	O
is	O
a	O
caution	O
about	O
data	O
processing	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
dependent	O
random	B
variables	O
inference	B
and	O
information	B
measures	O
exercise	O
the	O
three	B
cards	I
one	O
card	B
is	O
white	B
on	O
both	O
faces	O
one	O
is	O
black	B
on	O
both	O
faces	O
and	O
one	O
is	O
white	B
on	O
one	O
side	O
and	O
black	B
on	O
the	O
other	O
the	O
three	B
cards	I
are	O
and	O
their	O
orientations	O
randomized	O
one	O
card	B
is	O
drawn	O
and	O
placed	O
on	O
the	O
table	O
the	O
upper	O
face	O
is	O
black	B
what	O
is	O
the	O
colour	O
of	O
its	O
lower	O
face	O
the	O
inference	B
problem	O
does	O
seeing	O
the	O
top	O
face	O
convey	O
information	B
about	O
the	O
colour	O
of	O
the	O
bottom	O
face	O
discuss	O
the	O
information	B
contents	O
and	O
entropies	O
in	O
this	O
situation	O
let	O
the	O
value	O
of	O
the	O
upper	O
face	O
s	O
colour	O
be	O
u	O
and	O
the	O
value	O
of	O
the	O
lower	O
face	O
s	O
colour	O
be	O
l	O
imagine	O
that	O
we	O
draw	O
a	O
random	B
card	B
and	O
learn	O
both	O
u	O
and	O
l	O
what	O
is	O
the	O
entropy	B
of	O
u	O
hu	O
what	O
is	O
the	O
entropy	B
of	O
l	O
hl	O
what	O
is	O
the	O
mutual	B
information	B
between	O
u	O
and	O
l	O
iu	O
l	O
entropies	O
of	O
markov	O
processes	O
exercise	O
in	O
the	O
guessing	B
game	I
we	O
imagined	O
predicting	O
the	O
next	O
letter	O
in	O
a	O
document	O
starting	O
from	O
the	O
beginning	O
and	O
working	O
towards	O
the	O
end	O
consider	O
the	O
task	O
of	O
predicting	O
the	O
reversed	O
text	O
that	O
is	O
predicting	O
the	O
letter	O
that	O
precedes	O
those	O
already	O
known	O
most	O
people	O
this	O
a	O
harder	O
task	O
assuming	O
that	O
we	O
model	B
the	O
language	O
using	O
an	O
n	O
model	B
says	O
the	O
probability	B
of	O
the	O
next	O
character	O
depends	O
only	O
on	O
the	O
n	O
preceding	O
characters	O
is	O
there	O
any	O
between	O
the	O
average	O
information	B
contents	O
of	O
the	O
reversed	O
language	O
and	O
the	O
forward	O
language	O
solutions	O
solution	O
to	O
exercise	O
see	O
exercise	O
for	O
an	O
example	O
where	O
hx	O
j	O
y	O
exceeds	O
hx	O
y	O
we	O
can	O
prove	O
the	O
inequality	B
hx	O
j	O
y	O
hx	O
by	O
turning	O
the	O
expression	O
into	O
a	O
relative	B
entropy	B
bayes	B
theorem	O
and	O
invoking	O
gibbs	B
inequality	B
p	O
p	O
y	O
log	O
p	O
y	O
log	O
p	O
y	O
p	O
hx	O
j	O
y	O
p	O
xxy	O
xx	O
p	O
log	O
p	O
j	O
x	O
log	O
p	O
p	O
j	O
xp	O
p	O
p	O
j	O
x	O
log	O
p	O
p	O
j	O
x	O
the	O
last	O
expression	O
is	O
a	O
sum	O
of	O
relative	B
entropies	O
between	O
the	O
distributions	O
p	O
j	O
x	O
and	O
p	O
so	O
hx	O
j	O
y	O
hx	O
with	O
equality	O
only	O
if	O
p	O
j	O
x	O
p	O
for	O
all	O
x	O
and	O
y	O
is	O
only	O
if	O
x	O
and	O
y	O
are	O
independent	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solution	O
to	O
exercise	O
the	O
chain	B
rule	I
for	O
entropy	B
follows	O
from	O
the	O
decomposition	O
of	O
a	O
joint	B
probability	B
p	O
y	O
p	O
y	O
log	O
hx	O
y	O
xxy	O
xxy	O
xx	O
p	O
hx	O
hy	O
j	O
x	O
p	O
j	O
p	O
log	O
p	O
log	O
p	O
p	O
j	O
p	O
j	O
x	O
log	O
p	O
j	O
x	O
solution	O
to	O
exercise	O
symmetry	O
of	O
mutual	B
information	B
ix	O
y	O
hx	O
hx	O
j	O
y	O
p	O
log	O
p	O
y	O
log	O
p	O
p	O
y	O
p	O
p	O
y	O
log	O
p	O
y	O
p	O
xx	O
xxy	O
xxy	O
p	O
y	O
log	O
p	O
y	O
this	O
expression	O
is	O
symmetric	B
in	O
x	O
and	O
y	O
so	O
ix	O
y	O
hx	O
hx	O
j	O
y	O
hy	O
hy	O
j	O
x	O
we	O
can	O
prove	O
that	O
mutual	B
information	B
is	O
positive	O
in	O
two	O
ways	O
one	O
is	O
to	O
continue	O
from	O
ix	O
y	O
p	O
y	O
log	O
p	O
y	O
p	O
which	O
is	O
a	O
relative	B
entropy	B
and	O
use	O
gibbs	B
inequality	B
on	O
which	O
asserts	O
that	O
this	O
relative	B
entropy	B
is	O
with	O
equality	O
only	O
if	O
p	O
y	O
p	O
that	O
is	O
if	O
x	O
and	O
y	O
are	O
independent	O
the	O
other	O
is	O
to	O
use	O
jensen	O
s	O
inequality	B
on	O
p	O
y	O
log	O
p	O
p	O
y	O
logxxy	O
p	O
y	O
p	O
y	O
p	O
log	O
solution	O
to	O
exercise	O
z	O
x	O
y	O
mod	O
if	O
q	O
pz	O
and	O
iz	O
x	O
hz	O
hz	O
j	O
x	O
for	O
general	O
q	O
and	O
p	O
pz	O
the	O
mutual	B
information	B
is	O
iz	O
x	O
j	O
x	O
three	O
term	O
entropies	O
solution	O
to	O
exercise	O
the	O
depiction	O
of	O
entropies	O
in	O
terms	O
of	O
venn	O
diagrams	O
is	O
misleading	O
for	O
at	O
least	O
two	O
reasons	O
first	O
one	O
is	O
used	O
to	O
thinking	O
of	O
venn	O
diagrams	O
as	O
depicting	O
sets	O
but	O
what	O
are	O
the	O
sets	O
hx	O
and	O
hy	O
depicted	O
in	O
and	O
what	O
are	O
the	O
objects	O
that	O
are	O
members	O
of	O
those	O
sets	O
i	O
think	O
this	O
diagram	O
encourages	O
the	O
novice	O
student	B
to	O
make	O
inappropriate	O
analogies	O
for	O
example	O
some	O
students	O
imagine	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
dependent	O
random	B
variables	O
figure	O
a	O
misleading	O
representation	O
of	O
entropies	O
continued	O
ixy	O
hxyz	O
hx	O
hzx	O
hy	O
hyxz	O
hz	O
ixyz	O
a	O
hzy	O
hxyz	O
hzxy	O
that	O
the	O
random	B
outcome	O
y	O
might	O
correspond	O
to	O
a	O
point	O
in	O
the	O
diagram	O
and	O
thus	O
confuse	O
entropies	O
with	O
probabilities	O
secondly	O
the	O
depiction	O
in	O
terms	O
of	O
venn	O
diagrams	O
encourages	O
one	O
to	O
believe	O
that	O
all	O
the	O
areas	O
correspond	O
to	O
positive	O
quantities	O
in	O
the	O
special	O
case	O
of	O
two	O
random	B
variables	O
it	O
is	O
indeed	O
true	O
that	O
hx	O
j	O
y	O
ix	O
y	O
and	O
hy	O
j	O
x	O
are	O
positive	O
quantities	O
but	O
as	O
soon	O
as	O
we	O
progress	O
to	O
three-variable	O
ensembles	O
we	O
obtain	O
a	O
diagram	O
with	O
positive-looking	O
areas	O
that	O
may	O
actually	O
correspond	O
to	O
negative	O
quantities	O
figure	O
correctly	O
shows	O
relationships	O
such	O
as	O
hx	O
hz	O
j	O
x	O
hy	O
j	O
x	O
z	O
hx	O
y	O
z	O
but	O
it	O
gives	O
the	O
misleading	O
impression	O
that	O
the	O
conditional	B
mutual	B
information	B
ix	O
y	O
j	O
z	O
is	O
less	O
than	O
the	O
mutual	B
information	B
ix	O
y	O
in	O
fact	O
the	O
area	O
labelled	O
a	O
can	O
correspond	O
to	O
a	O
negative	O
quantity	O
consider	O
the	O
joint	B
ensemble	B
y	O
z	O
in	O
which	O
x	O
and	O
y	O
are	O
independent	O
binary	O
variables	O
and	O
z	O
is	O
to	O
be	O
z	O
x	O
y	O
mod	O
then	O
clearly	O
hx	O
hy	O
bit	B
also	O
hz	O
bit	B
and	O
hy	O
j	O
x	O
hy	O
since	O
the	O
two	O
variables	O
are	O
independent	O
so	O
the	O
mutual	B
information	B
between	O
x	O
and	O
y	O
is	O
zero	O
ix	O
y	O
however	O
if	O
z	O
is	O
observed	O
x	O
and	O
y	O
become	O
dependent	O
knowing	O
x	O
given	O
z	O
tells	O
you	O
what	O
y	O
is	O
y	O
z	O
x	O
mod	O
so	O
ix	O
y	O
j	O
z	O
bit	B
thus	O
the	O
area	O
labelled	O
a	O
must	O
correspond	O
to	O
bits	O
for	O
the	O
to	O
give	O
the	O
correct	O
answers	O
the	O
above	O
example	O
is	O
not	O
at	O
all	O
a	O
capricious	O
or	O
exceptional	O
illustration	O
the	O
binary	B
symmetric	B
channel	I
with	O
input	O
x	O
noise	O
y	O
and	O
output	O
z	O
is	O
a	O
situation	O
in	O
which	O
ix	O
y	O
and	O
noise	O
are	O
independent	O
but	O
ix	O
y	O
j	O
z	O
you	O
see	O
the	O
output	O
the	O
unknown	O
input	O
and	O
the	O
unknown	O
noise	O
are	O
intimately	O
related	O
the	O
venn	O
diagram	O
representation	O
is	O
therefore	O
valid	O
only	O
if	O
one	O
is	O
aware	O
that	O
positive	O
areas	O
may	O
represent	O
negative	O
quantities	O
with	O
this	O
proviso	O
kept	O
in	O
mind	O
the	O
interpretation	O
of	O
entropies	O
in	O
terms	O
of	O
sets	O
can	O
be	O
helpful	O
solution	O
to	O
exercise	O
for	O
any	O
joint	B
ensemble	B
xy	O
z	O
the	O
following	O
chain	B
rule	I
for	O
mutual	B
information	B
holds	O
in	O
the	O
case	O
w	O
d	O
r	O
w	O
and	O
r	O
are	O
independent	O
given	O
d	O
so	O
ix	O
y	O
z	O
ix	O
y	O
ix	O
z	O
j	O
y	O
now	O
iw	O
r	O
j	O
d	O
using	O
the	O
chain	B
rule	I
twice	O
we	O
have	O
and	O
so	O
iw	O
d	O
r	O
iw	O
d	O
iw	O
d	O
r	O
iw	O
r	O
iw	O
d	O
j	O
r	O
iw	O
r	O
iw	O
d	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
before	O
reading	O
chapter	O
you	O
should	O
have	O
read	O
chapter	O
and	O
worked	O
on	O
exercise	O
and	O
exercises	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
big	O
picture	O
source	O
coding	O
channel	O
coding	O
source	O
compressor	O
decompressor	O
encoder	B
noisy	B
channel	O
decoder	B
in	O
chapters	O
we	O
discussed	O
source	O
coding	O
with	O
block	B
codes	O
symbol	O
codes	O
and	O
stream	B
codes	I
we	O
implicitly	O
assumed	O
that	O
the	O
channel	O
from	O
the	O
compressor	O
to	O
the	O
decompressor	O
was	O
noise-free	O
real	O
channels	O
are	O
noisy	B
we	O
will	O
now	O
spend	O
two	O
chapters	O
on	O
the	O
subject	O
of	O
noisy-channel	O
coding	O
the	O
fundamental	O
possibilities	O
and	O
limitations	O
of	O
error-free	O
communication	B
through	O
a	O
noisy	B
channel	O
the	O
aim	O
of	O
channel	O
coding	O
is	O
to	O
make	O
the	O
noisy	B
channel	O
behave	O
like	O
a	O
noiseless	B
channel	O
we	O
will	O
assume	O
that	O
the	O
data	O
to	O
be	O
transmitted	O
has	O
been	O
through	O
a	O
good	B
compressor	O
so	O
the	O
bit	B
stream	O
has	O
no	O
obvious	O
redundancy	B
the	O
channel	O
code	O
which	O
makes	O
the	O
transmission	O
will	O
put	O
back	O
redundancy	B
of	O
a	O
special	O
sort	O
designed	O
to	O
make	O
the	O
noisy	B
received	O
signal	O
decodeable	O
suppose	O
we	O
transmit	O
bits	O
per	O
second	O
with	O
over	O
a	O
noisy	B
channel	O
that	O
bits	O
with	O
probability	B
f	O
what	O
is	O
the	O
rate	B
of	O
transmission	O
of	O
information	B
we	O
might	O
guess	O
that	O
the	O
rate	B
is	O
bits	O
per	O
second	O
by	O
subtracting	O
the	O
expected	O
number	O
of	O
errors	B
per	O
second	O
but	O
this	O
is	O
not	O
correct	O
because	O
the	O
recipient	O
does	O
not	O
know	O
where	O
the	O
errors	B
occurred	O
consider	O
the	O
case	O
where	O
the	O
noise	O
is	O
so	O
great	O
that	O
the	O
received	O
symbols	O
are	O
independent	O
of	O
the	O
transmitted	O
symbols	O
this	O
corresponds	O
to	O
a	O
noise	O
level	O
of	O
f	O
since	O
half	O
of	O
the	O
received	O
symbols	O
are	O
correct	O
due	O
to	O
chance	O
alone	O
but	O
when	O
f	O
no	O
information	B
is	O
transmitted	O
at	O
all	O
given	O
what	O
we	O
have	O
learnt	O
about	O
entropy	B
it	O
seems	O
reasonable	O
that	O
a	O
measure	O
of	O
the	O
information	B
transmitted	O
is	O
given	O
by	O
the	O
mutual	B
information	B
between	O
the	O
source	O
and	O
the	O
received	O
signal	O
that	O
is	O
the	O
entropy	B
of	O
the	O
source	O
minus	O
the	O
conditional	B
entropy	B
of	O
the	O
source	O
given	O
the	O
received	O
signal	O
we	O
will	O
now	O
review	O
the	O
of	O
conditional	B
entropy	B
and	O
mutual	B
information	B
then	O
we	O
will	O
examine	O
whether	O
it	O
is	O
possible	O
to	O
use	O
such	O
a	O
noisy	B
channel	O
to	O
communicate	O
reliably	O
we	O
will	O
show	O
that	O
for	O
any	O
channel	O
q	O
there	O
is	O
a	O
non-zero	O
rate	B
the	O
capacity	B
cq	O
up	O
to	O
which	O
information	B
can	O
be	O
sent	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
review	O
of	O
probability	B
and	O
information	B
with	O
arbitrarily	O
small	O
probability	B
of	I
error	I
review	O
of	O
probability	B
and	O
information	B
as	O
an	O
example	O
we	O
take	O
the	O
joint	B
distribution	B
xy	O
from	O
exercise	O
the	O
marginal	B
distributions	O
p	O
and	O
p	O
are	O
shown	O
in	O
the	O
margins	O
p	O
y	O
x	O
p	O
y	O
p	O
the	O
joint	B
entropy	B
is	O
hx	O
y	O
bits	O
the	O
marginal	B
entropies	O
are	O
hx	O
bits	O
and	O
hy	O
bits	O
we	O
can	O
compute	O
the	O
conditional	B
distribution	B
of	O
x	O
for	O
each	O
value	O
of	O
y	O
and	O
the	O
entropy	B
of	O
each	O
of	O
those	O
conditional	B
distributions	O
p	O
y	O
y	O
x	O
hx	O
j	O
ybits	O
hx	O
j	O
y	O
note	O
that	O
whereas	O
hx	O
j	O
y	O
is	O
less	O
than	O
hx	O
hx	O
j	O
y	O
is	O
greater	O
than	O
hx	O
so	O
in	O
some	O
cases	O
learning	B
y	O
can	O
increase	O
our	O
uncertainty	O
about	O
x	O
note	O
also	O
that	O
although	O
p	O
y	O
is	O
a	O
distribution	B
from	O
p	O
the	O
conditional	B
entropy	B
hx	O
j	O
y	O
is	O
equal	O
to	O
hx	O
so	O
learning	B
that	O
y	O
is	O
changes	O
our	O
knowledge	O
about	O
x	O
but	O
does	O
not	O
reduce	O
the	O
uncertainty	O
of	O
x	O
as	O
measured	O
by	O
the	O
entropy	B
on	O
average	O
though	O
learning	B
y	O
does	O
convey	O
information	B
about	O
x	O
since	O
hx	O
j	O
y	O
hx	O
ix	O
y	O
hx	O
hx	O
j	O
y	O
bits	O
one	O
may	O
also	O
evaluate	O
hy	O
jx	O
bits	O
the	O
mutual	B
information	B
is	O
noisy	B
channels	O
a	O
discrete	B
memoryless	I
channel	O
q	O
is	O
characterized	O
by	O
an	O
input	O
alphabet	O
ax	O
an	O
output	O
alphabet	O
ay	O
and	O
a	O
set	B
of	O
conditional	B
probability	B
distributions	O
p	O
j	O
x	O
one	O
for	O
each	O
x	O
ax	O
these	O
transition	B
probabilities	O
may	O
be	O
written	O
in	O
a	O
matrix	B
qjji	O
p	O
bj	O
j	O
x	O
ai	O
i	O
usually	O
orient	O
this	O
matrix	B
with	O
the	O
output	O
variable	O
j	O
indexing	O
the	O
rows	O
and	O
the	O
input	O
variable	O
i	O
indexing	O
the	O
columns	O
so	O
that	O
each	O
column	O
of	O
q	O
is	O
a	O
probability	B
vector	O
with	O
this	O
convention	O
we	O
can	O
obtain	O
the	O
probability	B
of	O
the	O
output	O
py	O
from	O
a	O
probability	B
distribution	B
over	O
the	O
input	O
px	O
by	O
right-multiplication	O
py	O
qpx	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
a	O
noisy	B
channel	O
some	O
useful	O
model	B
channels	O
are	O
binary	B
symmetric	B
channel	I
ax	O
ay	O
p	O
x	O
f	O
p	O
x	O
f	O
binary	B
erasure	B
channel	I
ax	O
ay	O
x	O
y	O
p	O
x	O
f	O
p	O
x	O
f	O
x	O
y	O
p	O
x	O
f	O
p	O
x	O
f	O
p	O
x	O
p	O
x	O
p	O
x	O
f	O
p	O
x	O
f	O
noisy	B
typewriter	I
ax	O
ay	O
the	O
letters	O
fa	O
b	O
z	O
the	O
letters	O
are	O
arranged	O
in	O
a	O
circle	B
and	O
when	O
the	O
typist	O
attempts	O
to	O
type	O
b	O
what	O
comes	O
out	O
is	O
either	O
a	O
b	O
or	O
c	O
with	O
probability	B
each	O
when	O
the	O
input	O
is	O
c	O
the	O
output	O
is	O
b	O
c	O
or	O
d	O
and	O
so	O
forth	O
with	O
the	O
letter	O
adjacent	O
to	O
the	O
letter	O
a	O
c	O
pppq	O
c	O
pppq	O
c	O
pppq	O
c	O
pppq	O
c	O
pppq	O
pppq	O
pppq	O
c	O
c	O
c	O
pppq	O
c	O
pppq	O
cw	O
p	O
fj	O
x	O
g	O
p	O
gj	O
x	O
g	O
p	O
hj	O
x	O
g	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
c	O
c	O
y	O
z	O
y	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
z	B
channel	I
ax	O
ay	O
x	O
y	O
p	O
x	O
p	O
x	O
p	O
x	O
f	O
p	O
x	O
f	O
inferring	O
the	O
input	O
given	O
the	O
output	O
if	O
we	O
assume	O
that	O
the	O
input	O
x	O
to	O
a	O
channel	O
comes	O
from	O
an	O
ensemble	B
x	O
then	O
we	O
obtain	O
a	O
joint	B
ensemble	B
xy	O
in	O
which	O
the	O
random	B
variables	O
x	O
and	O
y	O
have	O
the	O
joint	B
distribution	B
p	O
y	O
p	O
j	O
xp	O
now	O
if	O
we	O
receive	O
a	O
particular	O
symbol	O
y	O
what	O
was	O
the	O
input	O
symbol	O
x	O
we	O
typically	O
won	O
t	O
know	O
for	O
certain	O
we	O
can	O
write	O
down	O
the	O
posterior	O
distribution	B
of	O
the	O
input	O
using	O
bayes	B
theorem	O
p	O
y	O
p	O
j	O
xp	O
p	O
p	O
j	O
xp	O
p	O
j	O
example	O
consider	O
a	O
binary	B
symmetric	B
channel	I
with	O
probability	B
of	I
error	I
f	O
let	O
the	O
input	B
ensemble	B
be	O
px	O
assume	O
we	O
observe	O
y	O
p	O
y	O
p	O
x	O
p	O
j	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
information	B
conveyed	O
by	O
a	O
channel	O
thus	O
x	O
is	O
still	O
less	O
probable	O
than	O
x	O
although	O
it	O
is	O
not	O
as	O
improbable	O
as	O
it	O
was	O
before	O
exercise	O
now	O
assume	O
we	O
observe	O
y	O
compute	O
the	O
probability	B
of	O
x	O
given	O
y	O
example	O
consider	O
a	O
z	B
channel	I
with	O
probability	B
of	I
error	I
f	O
let	O
the	O
input	B
ensemble	B
be	O
px	O
assume	O
we	O
observe	O
y	O
p	O
y	O
so	O
given	O
the	O
output	O
y	O
we	O
become	O
certain	O
of	O
the	O
input	O
exercise	O
alternatively	O
assume	O
we	O
observe	O
y	O
compute	O
p	O
y	O
information	B
conveyed	O
by	O
a	O
channel	O
we	O
now	O
consider	O
how	O
much	O
information	B
can	O
be	O
communicated	O
through	O
a	O
channel	O
in	O
operational	O
terms	O
we	O
are	O
interested	O
in	O
ways	O
of	O
using	O
the	O
channel	O
such	O
that	O
all	O
the	O
bits	O
that	O
are	O
communicated	O
are	O
recovered	O
with	O
negligible	O
probability	B
of	I
error	I
in	O
mathematical	O
terms	O
assuming	O
a	O
particular	O
input	B
ensemble	B
x	O
we	O
can	O
measure	O
how	O
much	O
information	B
the	O
output	O
conveys	O
about	O
the	O
input	O
by	O
the	O
mutual	B
information	B
ix	O
y	O
hx	O
hx	O
j	O
y	O
hy	O
hy	O
jx	O
our	O
aim	O
is	O
to	O
establish	O
the	O
connection	O
between	O
these	O
two	O
ideas	O
let	O
us	O
evaluate	O
ix	O
y	O
for	O
some	O
of	O
the	O
channels	O
above	O
hint	O
for	O
computing	O
mutual	B
information	B
we	O
will	O
tend	O
to	O
think	O
of	O
ix	O
y	O
as	O
hx	O
hx	O
j	O
y	O
i	O
e	O
how	O
much	O
the	O
uncertainty	O
of	O
the	O
input	O
x	O
is	O
reduced	O
when	O
we	O
look	O
at	O
the	O
output	O
y	O
but	O
for	O
computational	O
purposes	O
it	O
is	O
often	O
handy	O
to	O
evaluate	O
hy	O
jx	O
instead	O
hx	O
y	O
hx	O
hy	O
hx	O
j	O
y	O
ix	O
y	O
hy	O
jx	O
figure	O
the	O
relationship	O
between	O
joint	B
information	B
marginal	B
entropy	B
conditional	B
entropy	B
and	O
mutual	O
entropy	B
this	O
is	O
important	O
so	O
i	O
m	O
showing	O
it	O
twice	O
example	O
consider	O
the	O
binary	B
symmetric	B
channel	I
again	O
with	O
f	O
and	O
px	O
we	O
already	O
evaluated	O
the	O
marginal	B
probabilities	O
p	O
implicitly	O
above	O
p	O
p	O
the	O
mutual	B
information	B
is	O
ix	O
y	O
hy	O
hy	O
jx	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
a	O
noisy	B
channel	O
what	O
is	O
hy	O
jx	O
it	O
is	O
to	O
be	O
the	O
weighted	O
sum	O
over	O
x	O
of	O
hy	O
j	O
x	O
but	O
hy	O
j	O
x	O
is	O
the	O
same	O
for	O
each	O
value	O
of	O
x	O
hy	O
j	O
x	O
is	O
and	O
hy	O
j	O
x	O
is	O
so	O
ix	O
y	O
hy	O
hy	O
jx	O
bits	O
throughout	O
this	O
book	O
log	O
means	O
this	O
may	O
be	O
contrasted	O
with	O
the	O
entropy	B
of	O
the	O
source	O
hx	O
bits	O
note	O
here	O
we	O
have	O
used	O
the	O
binary	B
entropy	B
function	I
hp	O
p	O
p	O
log	O
p	O
p	O
log	O
example	O
and	O
now	O
the	O
z	B
channel	I
with	O
px	O
as	O
above	O
p	O
ix	O
y	O
hy	O
hy	O
jx	O
bits	O
the	O
entropy	B
of	O
the	O
source	O
as	O
above	O
is	O
hx	O
bits	O
notice	O
that	O
the	O
mutual	B
information	B
ix	O
y	O
for	O
the	O
z	B
channel	I
is	O
bigger	O
than	O
the	O
mutual	B
information	B
for	O
the	O
binary	B
symmetric	B
channel	I
with	O
the	O
same	O
f	O
the	O
z	B
channel	I
is	O
a	O
more	O
reliable	O
channel	O
exercise	O
compute	O
the	O
mutual	B
information	B
between	O
x	O
and	O
y	O
for	O
the	O
binary	B
symmetric	B
channel	I
with	O
f	O
when	O
the	O
input	O
distribution	B
is	O
px	O
exercise	O
compute	O
the	O
mutual	B
information	B
between	O
x	O
and	O
y	O
for	O
the	O
z	B
channel	I
with	O
f	O
when	O
the	O
input	O
distribution	B
is	O
px	O
maximizing	O
the	O
mutual	B
information	B
we	O
have	O
observed	O
in	O
the	O
above	O
examples	O
that	O
the	O
mutual	B
information	B
between	O
the	O
input	O
and	O
the	O
output	O
depends	O
on	O
the	O
chosen	O
input	B
ensemble	B
let	O
us	O
assume	O
that	O
we	O
wish	O
to	O
maximize	O
the	O
mutual	B
information	B
conveyed	O
by	O
the	O
channel	O
by	O
choosing	O
the	O
best	O
possible	O
input	B
ensemble	B
we	O
the	O
capacity	B
of	O
the	O
channel	O
to	O
be	O
its	O
maximum	O
mutual	B
information	B
the	O
capacity	B
of	O
a	O
channel	O
q	O
is	O
cq	O
max	O
px	O
ix	O
y	O
the	O
distribution	B
px	O
that	O
achieves	O
the	O
maximum	O
is	O
called	O
the	O
optimal	B
input	I
distribution	B
denoted	O
by	O
may	O
be	O
multiple	O
optimal	B
input	O
distributions	O
achieving	O
the	O
same	O
value	O
of	O
ix	O
y	O
in	O
chapter	O
we	O
will	O
show	O
that	O
the	O
capacity	B
does	O
indeed	O
measure	O
the	O
maximum	O
amount	O
of	O
error-free	O
information	B
that	O
can	O
be	O
transmitted	O
over	O
the	O
channel	O
per	O
unit	O
time	O
example	O
consider	O
the	O
binary	B
symmetric	B
channel	I
with	O
f	O
above	O
we	O
considered	O
px	O
and	O
found	O
ix	O
y	O
bits	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
noisy-channel	B
coding	B
theorem	I
how	O
much	O
better	O
can	O
we	O
do	O
by	O
symmetry	O
the	O
optimal	B
input	I
distribution	B
is	O
and	O
the	O
capacity	B
is	O
ix	O
y	O
cqbsc	O
bits	O
we	O
ll	O
justify	O
the	O
symmetry	B
argument	I
later	O
if	O
there	O
s	O
any	O
doubt	O
about	O
the	O
symmetry	B
argument	I
we	O
can	O
always	O
resort	O
to	O
explicit	O
maximization	O
of	O
the	O
mutual	B
information	B
ix	O
y	O
ix	O
y	O
example	O
the	O
noisy	B
typewriter	I
the	O
optimal	B
input	I
distribution	B
is	O
a	O
uni	O
form	O
distribution	B
over	O
x	O
and	O
gives	O
c	O
bits	O
example	O
consider	O
the	O
z	B
channel	I
with	O
f	O
identifying	O
the	O
optimal	B
input	I
distribution	B
is	O
not	O
so	O
straightforward	O
we	O
evaluate	O
ix	O
y	O
explicitly	O
for	O
px	O
first	O
we	O
need	O
to	O
compute	O
p	O
the	O
probability	B
of	O
y	O
is	O
easiest	O
to	O
write	O
down	O
figure	O
the	O
mutual	B
information	B
ix	O
y	O
for	O
a	O
binary	B
symmetric	B
channel	I
with	O
f	O
as	O
a	O
function	O
of	O
the	O
input	O
distribution	B
p	O
f	O
then	O
the	O
mutual	B
information	B
is	O
ix	O
y	O
hy	O
hy	O
jx	O
f	O
f	O
this	O
is	O
a	O
non-trivial	O
function	O
of	O
shown	O
in	O
it	O
is	O
maximized	O
for	O
f	O
by	O
we	O
cqz	O
notice	O
the	O
optimal	B
input	I
distribution	B
is	O
not	O
we	O
can	O
communicate	O
slightly	O
more	O
information	B
by	O
using	O
input	O
symbol	O
more	O
frequently	O
than	O
ix	O
y	O
figure	O
the	O
mutual	B
information	B
ix	O
y	O
for	O
a	O
z	B
channel	I
with	O
f	O
as	O
a	O
function	O
of	O
the	O
input	O
distribution	B
exercise	O
what	O
is	O
the	O
capacity	B
of	O
the	O
binary	B
symmetric	B
channel	I
for	O
general	O
f	O
exercise	O
show	O
that	O
the	O
capacity	B
of	O
the	O
binary	B
erasure	B
channel	I
with	O
f	O
is	O
cbec	O
what	O
is	O
its	O
capacity	B
for	O
general	O
f	O
comment	O
the	O
noisy-channel	B
coding	B
theorem	I
it	O
seems	O
plausible	O
that	O
the	O
capacity	B
we	O
have	O
may	O
be	O
a	O
measure	O
of	O
information	B
conveyed	O
by	O
a	O
channel	O
what	O
is	O
not	O
obvious	O
and	O
what	O
we	O
will	O
prove	O
in	O
the	O
next	O
chapter	O
is	O
that	O
the	O
capacity	B
indeed	O
measures	O
the	O
rate	B
at	O
which	O
blocks	O
of	O
data	O
can	O
be	O
communicated	O
over	O
the	O
channel	O
with	O
arbitrarily	O
small	O
probability	B
of	I
error	I
we	O
make	O
the	O
following	O
an	O
k	O
block	B
code	I
for	O
a	O
channel	O
q	O
is	O
a	O
list	O
of	O
s	O
codewords	O
xs	O
an	O
x	O
length	O
n	O
using	O
this	O
code	O
we	O
can	O
encode	O
a	O
signal	O
s	O
each	O
of	O
as	O
xs	O
number	O
of	O
codewords	O
s	O
is	O
an	O
integer	O
but	O
the	O
number	O
of	O
bits	O
by	O
choosing	O
a	O
codeword	B
k	O
log	O
s	O
is	O
not	O
necessarily	O
an	O
integer	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
a	O
noisy	B
channel	O
the	O
rate	B
of	O
the	O
code	O
is	O
r	O
kn	O
bits	O
per	O
channel	O
use	O
will	O
use	O
this	O
of	O
the	O
rate	B
for	O
any	O
channel	O
not	O
only	O
channels	O
with	O
binary	O
inputs	O
note	O
however	O
that	O
it	O
is	O
sometimes	O
conventional	O
to	O
the	O
rate	B
of	O
a	O
code	O
for	O
a	O
channel	O
with	O
q	O
input	O
symbols	O
to	O
be	O
kn	O
log	O
q	O
a	O
decoder	B
for	O
an	O
k	O
block	B
code	I
is	O
a	O
mapping	B
from	O
the	O
set	B
of	O
length-n	O
y	O
to	O
a	O
codeword	B
label	O
strings	O
of	O
channel	O
outputs	O
an	O
the	O
extra	O
symbol	O
can	O
be	O
used	O
to	O
indicate	O
a	O
failure	O
the	O
probability	B
of	I
block	B
error	I
of	O
a	O
code	O
and	O
decoder	B
for	O
a	O
given	O
channel	O
and	O
for	O
a	O
given	O
probability	B
distribution	B
over	O
the	O
encoded	O
signal	O
p	O
is	O
pb	O
p	O
sin	O
j	O
sin	O
the	O
maximal	O
probability	B
of	I
block	B
error	I
is	O
pbm	O
max	O
sin	O
p	O
sin	O
j	O
sin	O
the	O
optimal	B
decoder	B
for	O
a	O
channel	O
code	O
is	O
the	O
one	O
that	O
minimizes	O
the	O
probability	B
of	I
block	B
error	I
it	O
decodes	O
an	O
output	O
y	O
as	O
the	O
input	O
s	O
that	O
has	O
maximum	O
posterior	B
probability	B
p	O
y	O
p	O
y	O
p	O
j	O
sp	O
p	O
j	O
argmax	O
p	O
y	O
a	O
uniform	O
prior	B
distribution	B
on	O
s	O
is	O
usually	O
assumed	O
in	O
which	O
case	O
the	O
optimal	B
decoder	B
is	O
also	O
the	O
maximum	B
likelihood	B
decoder	B
i	O
e	O
the	O
decoder	B
that	O
maps	O
an	O
output	O
y	O
to	O
the	O
input	O
s	O
that	O
has	O
maximum	B
likelihood	B
p	O
j	O
s	O
the	O
probability	B
of	O
bit	B
error	O
pb	O
is	O
assuming	O
that	O
the	O
codeword	B
number	O
s	O
is	O
represented	O
by	O
a	O
binary	O
vector	O
s	O
of	O
length	O
k	O
bits	O
it	O
is	O
the	O
average	O
probability	B
that	O
a	O
bit	B
of	O
sout	O
is	O
not	O
equal	O
to	O
the	O
corresponding	O
bit	B
of	O
sin	O
over	O
all	O
k	O
bits	O
shannon	B
s	O
noisy-channel	B
coding	B
theorem	I
one	O
associated	O
with	O
each	O
discrete	B
memoryless	I
channel	O
there	O
is	O
a	O
non-negative	O
number	O
c	O
the	O
channel	O
capacity	B
with	O
the	O
following	O
property	O
for	O
any	O
and	O
r	O
c	O
for	O
large	O
enough	O
n	O
there	O
exists	O
a	O
block	B
code	I
of	O
length	O
n	O
and	O
rate	B
r	O
and	O
a	O
decoding	B
algorithm	O
such	O
that	O
the	O
maximal	O
probability	B
of	I
block	B
error	I
is	O
of	O
the	O
theorem	O
for	O
the	O
noisy	B
typewriter	I
channel	O
in	O
the	O
case	O
of	O
the	O
noisy	B
typewriter	I
we	O
can	O
easily	O
the	O
theorem	O
because	O
we	O
can	O
create	O
a	O
completely	O
error-free	O
communication	B
strategy	O
using	O
a	O
block	B
code	I
of	O
length	O
n	O
we	O
use	O
only	O
the	O
letters	O
b	O
e	O
h	O
z	O
i	O
e	O
every	O
third	O
letter	O
these	O
letters	O
form	O
a	O
non-confusable	O
subset	B
of	O
the	O
input	O
alphabet	O
any	O
output	O
can	O
be	O
uniquely	O
decoded	O
the	O
number	O
of	O
inputs	O
in	O
the	O
non-confusable	O
subset	B
is	O
so	O
the	O
error-free	O
information	B
rate	B
of	O
this	O
system	O
is	O
bits	O
which	O
is	O
equal	O
to	O
the	O
capacity	B
c	O
which	O
we	O
evaluated	O
in	O
example	O
pbm	O
achievable	O
r	O
c	O
figure	O
portion	O
of	O
the	O
r	O
pbm	O
plane	O
asserted	O
to	O
be	O
achievable	O
by	O
the	O
part	O
of	O
shannon	B
s	O
noisy	B
channel	O
coding	B
theorem	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
intuitive	O
preview	O
of	O
proof	O
pppq	O
pppq	O
pppq	O
pppq	O
b	O
e	O
h	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
figure	O
a	O
non-confusable	O
subset	B
of	O
inputs	O
for	O
the	O
noisy	B
typewriter	I
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
y	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
figure	O
extended	B
channels	O
obtained	O
from	O
a	O
binary	B
symmetric	B
channel	I
with	O
transition	B
probability	B
n	O
n	O
n	O
how	O
does	O
this	O
translate	O
into	O
the	O
terms	O
of	O
the	O
theorem	O
the	O
following	O
table	O
explains	O
the	O
theorem	O
associated	O
with	O
each	O
discrete	B
memoryless	I
channel	O
there	O
is	O
a	O
non-negative	O
number	O
c	O
for	O
any	O
and	O
r	O
c	O
for	O
large	O
enough	O
n	O
there	O
exists	O
a	O
block	B
code	I
of	O
length	O
n	O
and	O
rate	B
r	O
and	O
a	O
decoding	B
algorithm	O
how	O
it	O
applies	O
to	O
the	O
noisy	B
typewriter	I
the	O
capacity	B
c	O
is	O
no	O
matter	O
what	O
and	O
r	O
are	O
we	O
set	B
the	O
blocklength	O
n	O
to	O
the	O
block	B
code	I
is	O
fb	O
e	O
zg	O
the	O
value	O
of	O
k	O
is	O
given	O
by	O
so	O
k	O
and	O
this	O
code	O
has	O
rate	B
which	O
is	O
greater	O
than	O
the	O
requested	O
value	O
of	O
r	O
the	O
decoding	B
algorithm	O
maps	O
the	O
received	O
letter	O
to	O
the	O
nearest	O
letter	O
in	O
the	O
code	O
such	O
that	O
the	O
maximal	O
probability	B
of	I
block	B
error	I
is	O
the	O
maximal	O
probability	B
of	I
block	B
error	I
is	O
zero	O
which	O
is	O
less	O
than	O
the	O
given	O
intuitive	O
preview	O
of	O
proof	O
extended	B
channels	O
to	O
prove	O
the	O
theorem	O
for	O
any	O
given	O
channel	O
we	O
consider	O
the	O
extended	B
channel	I
corresponding	O
to	O
n	O
uses	O
of	O
the	O
channel	O
the	O
extended	B
channel	I
has	O
jaxjn	O
possible	O
inputs	O
x	O
and	O
jay	O
jn	O
possible	O
outputs	O
extended	B
channels	O
obtained	O
from	O
a	O
binary	B
symmetric	B
channel	I
and	O
from	O
a	O
z	B
channel	I
are	O
shown	O
in	O
and	O
with	O
n	O
and	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
a	O
noisy	B
channel	O
figure	O
extended	B
channels	O
obtained	O
from	O
a	O
z	B
channel	I
with	O
transition	B
probability	B
each	O
column	O
corresponds	O
to	O
an	O
input	O
and	O
each	O
row	O
is	O
a	O
output	O
figure	O
some	O
typical	B
outputs	O
in	O
an	O
y	O
corresponding	O
to	O
typical	B
inputs	O
x	O
a	O
subset	B
of	O
the	O
typical	B
sets	O
shown	O
in	O
that	O
do	O
not	O
overlap	O
each	O
other	O
this	O
picture	O
can	O
be	O
compared	O
with	O
the	O
solution	O
to	O
the	O
noisy	B
typewriter	I
in	O
n	O
n	O
n	O
an	O
typical	B
y	O
y	O
typical	B
y	O
for	O
a	O
given	O
typical	B
x	O
an	O
typical	B
y	O
y	O
exercise	O
find	O
the	O
transition	B
probability	B
matrices	B
q	O
for	O
the	O
extended	B
channel	I
with	O
n	O
derived	O
from	O
the	O
binary	B
erasure	B
channel	I
having	O
erasure	B
probability	B
by	O
selecting	O
two	O
columns	O
of	O
this	O
transition	B
probability	B
matrix	B
we	O
can	O
a	O
code	O
for	O
this	O
channel	O
with	O
blocklength	O
n	O
what	O
is	O
the	O
best	O
choice	O
of	O
two	O
columns	O
what	O
is	O
the	O
decoding	B
algorithm	O
to	O
prove	O
the	O
noisy-channel	B
coding	B
theorem	I
we	O
make	O
use	O
of	O
large	O
blocklengths	O
n	O
the	O
intuitive	O
idea	O
is	O
that	O
if	O
n	O
is	O
large	O
an	O
extended	B
channel	I
looks	O
a	O
lot	O
like	O
the	O
noisy	B
typewriter	I
any	O
particular	O
input	O
x	O
is	O
very	O
likely	O
to	O
produce	O
an	O
output	O
in	O
a	O
small	O
subspace	O
of	O
the	O
output	O
alphabet	O
the	O
typical	B
output	O
set	B
given	O
that	O
input	O
so	O
we	O
can	O
a	O
non-confusable	O
subset	B
of	O
the	O
inputs	O
that	O
produce	O
essentially	O
disjoint	O
output	O
sequences	O
for	O
a	O
given	O
n	O
let	O
us	O
consider	O
a	O
way	O
of	O
generating	O
such	O
a	O
non-confusable	O
subset	B
of	O
the	O
inputs	O
and	O
count	O
up	O
how	O
many	O
distinct	O
inputs	O
it	O
contains	O
imagine	O
making	O
an	O
input	O
sequence	B
x	O
for	O
the	O
extended	B
channel	I
by	O
drawing	O
it	O
from	O
an	O
ensemble	B
x	O
n	O
where	O
x	O
is	O
an	O
arbitrary	O
ensemble	B
over	O
the	O
input	O
alphabet	O
recall	O
the	O
source	B
coding	B
theorem	I
of	O
chapter	O
and	O
consider	O
the	O
number	O
of	O
probable	O
output	O
sequences	O
y	O
the	O
total	O
number	O
of	O
typical	B
output	O
sequences	O
y	O
is	O
hy	O
all	O
having	O
similar	O
probability	B
for	O
any	O
particular	O
typical	B
input	O
sequence	B
x	O
there	O
are	O
about	O
hy	O
jx	O
probable	O
sequences	O
some	O
of	O
these	O
subsets	O
of	O
an	O
we	O
now	O
imagine	O
restricting	O
ourselves	O
to	O
a	O
subset	B
of	O
the	O
typical	B
inputs	O
x	O
such	O
that	O
the	O
corresponding	O
typical	B
output	O
sets	O
do	O
not	O
overlap	O
as	O
shown	O
in	O
we	O
can	O
then	O
bound	B
the	O
number	O
of	O
non-confusable	B
inputs	I
by	O
dividing	O
the	O
size	O
of	O
the	O
typical	B
y	O
set	B
hy	O
by	O
the	O
size	O
of	O
each	O
typical-y	O
y	O
are	O
depicted	O
by	O
circles	O
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
given-typical-x	O
set	B
hy	O
jx	O
so	O
the	O
number	O
of	O
non-confusable	B
inputs	I
if	O
they	O
are	O
selected	O
from	O
the	O
set	B
of	O
typical	B
inputs	O
x	O
x	O
n	O
is	O
hy	O
hy	O
jx	O
ixy	O
the	O
maximum	O
value	O
of	O
this	O
bound	B
is	O
achieved	O
if	O
x	O
is	O
the	O
ensemble	B
that	O
maximizes	O
ix	O
y	O
in	O
which	O
case	O
the	O
number	O
of	O
non-confusable	B
inputs	I
is	O
c	O
thus	O
asymptotically	O
up	O
to	O
c	O
bits	O
per	O
cycle	O
and	O
no	O
more	O
can	O
be	O
communicated	O
with	O
vanishing	O
error	B
probability	B
this	O
sketch	O
has	O
not	O
rigorously	O
proved	O
that	O
reliable	O
communication	B
really	O
is	O
possible	O
that	O
s	O
our	O
task	O
for	O
the	O
next	O
chapter	O
further	O
exercises	O
exercise	O
refer	O
back	O
to	O
the	O
computation	O
of	O
the	O
capacity	B
of	O
the	O
z	B
channel	I
with	O
f	O
why	O
is	O
less	O
than	O
one	O
could	O
argue	O
that	O
it	O
is	O
good	B
to	O
favour	O
the	O
input	O
since	O
it	O
is	O
transmitted	O
without	O
error	O
and	O
also	O
argue	O
that	O
it	O
is	O
good	B
to	O
favour	O
the	O
input	O
since	O
it	O
often	O
gives	O
rise	O
to	O
the	O
highly	O
prized	O
output	O
which	O
allows	O
certain	O
of	O
the	O
input	O
try	O
to	O
make	O
a	O
convincing	O
argument	O
in	O
the	O
case	O
of	O
general	O
f	O
show	O
that	O
the	O
optimal	B
input	I
distribution	B
is	O
f	O
what	O
happens	O
to	O
if	O
the	O
noise	O
level	O
f	O
is	O
very	O
close	O
to	O
exercise	O
sketch	O
graphs	O
of	O
the	O
capacity	B
of	O
the	O
z	B
channel	I
the	O
binary	B
symmetric	B
channel	I
and	O
the	O
binary	B
erasure	B
channel	I
as	O
a	O
function	O
of	O
f	O
exercise	O
what	O
is	O
the	O
capacity	B
of	O
the	O
ten-output	O
channel	O
whose	O
transition	B
probability	B
matrix	B
is	O
exercise	O
consider	O
a	O
gaussian	B
channel	I
with	O
binary	O
input	O
x	O
and	O
real	O
output	O
alphabet	O
ay	O
with	O
transition	B
probability	B
density	B
qy	O
j	O
x	O
where	O
is	O
the	O
signal	O
amplitude	O
compute	O
the	O
posterior	B
probability	B
of	O
x	O
given	O
y	O
assuming	O
that	O
the	O
two	O
inputs	O
are	O
equiprobable	O
put	O
your	O
answer	O
in	O
the	O
form	O
p	O
y	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
a	O
noisy	B
channel	O
sketch	O
the	O
value	O
of	O
p	O
y	O
as	O
a	O
function	O
of	O
y	O
assume	O
that	O
a	O
single	O
bit	B
is	O
to	O
be	O
transmitted	O
what	O
is	O
the	O
optimal	B
decoder	B
and	O
what	O
is	O
its	O
probability	B
of	I
error	I
express	O
your	O
answer	O
in	O
terms	O
of	O
the	O
signal-to-noise	B
ratio	I
and	O
the	O
error	B
function	I
cumulative	B
probability	B
function	I
of	O
the	O
gaussian	B
distribution	B
z	O
dz	O
that	O
this	O
of	O
the	O
error	B
function	I
may	O
not	O
correspond	O
to	O
other	O
people	O
s	O
pattern	B
recognition	B
as	O
a	O
noisy	B
channel	O
we	O
may	O
think	O
of	O
many	O
pattern	B
recognition	B
problems	O
in	O
terms	O
of	O
communication	B
channels	O
consider	O
the	O
case	O
of	O
recognizing	O
handwritten	B
digits	I
as	O
postcodes	O
on	O
envelopes	O
the	O
author	O
of	O
the	O
digit	O
wishes	O
to	O
communicate	O
a	O
message	O
from	O
the	O
set	B
ax	O
this	O
selected	O
message	O
is	O
the	O
input	O
to	O
the	O
channel	O
what	O
comes	O
out	O
of	O
the	O
channel	O
is	O
a	O
pattern	O
of	O
ink	O
on	O
paper	O
if	O
the	O
ink	O
pattern	O
is	O
represented	O
using	O
binary	O
pixels	O
the	O
channel	O
q	O
has	O
as	O
its	O
output	O
a	O
random	B
variable	I
y	O
ay	O
an	O
example	O
of	O
an	O
element	O
from	O
this	O
alphabet	O
is	O
shown	O
in	O
the	O
margin	O
exercise	O
estimate	O
how	O
many	O
patterns	O
in	O
ay	O
are	O
recognizable	O
as	O
the	O
aim	O
of	O
this	O
problem	O
is	O
to	O
try	O
to	O
demonstrate	O
the	O
character	O
existence	O
of	O
as	O
many	O
patterns	O
as	O
possible	O
that	O
are	O
recognizable	O
as	O
discuss	O
how	O
one	O
might	O
model	B
the	O
channel	O
p	O
j	O
x	O
estimate	O
the	O
entropy	B
of	O
the	O
probability	B
distribution	B
p	O
j	O
x	O
one	O
strategy	O
for	O
doing	O
pattern	B
recognition	B
is	O
to	O
create	O
a	O
model	B
for	O
p	O
j	O
x	O
for	O
each	O
value	O
of	O
the	O
input	O
x	O
then	O
use	O
bayes	B
theorem	O
to	O
infer	O
x	O
given	O
y	O
p	O
y	O
p	O
j	O
xp	O
p	O
j	O
this	O
strategy	O
is	O
known	O
as	O
full	O
probabilistic	O
modelling	B
or	O
generative	O
modelling	B
this	O
is	O
essentially	O
how	O
current	O
speech	O
recognition	B
systems	O
work	O
in	O
addition	O
to	O
the	O
channel	O
model	B
p	O
j	O
x	O
one	O
uses	O
a	O
prior	B
probability	B
distribution	B
p	O
which	O
in	O
the	O
case	O
of	O
both	O
character	O
recognition	B
and	O
speech	O
recognition	B
is	O
a	O
language	B
model	B
that	O
the	O
probability	B
of	O
the	O
next	O
characterword	O
given	O
the	O
context	O
and	O
the	O
known	O
grammar	O
and	O
statistics	O
of	O
the	O
language	O
random	B
coding	O
exercise	O
given	O
twenty-four	O
people	O
in	O
a	O
room	O
what	O
is	O
the	O
probability	B
that	O
there	O
are	O
at	O
least	O
two	O
people	O
present	O
who	O
have	O
the	O
same	O
birthday	B
day	O
and	O
month	O
of	O
birth	O
what	O
is	O
the	O
expected	O
number	O
of	O
pairs	O
of	O
people	O
with	O
the	O
same	O
birthday	B
which	O
of	O
these	O
two	O
questions	O
is	O
easiest	O
to	O
solve	O
which	O
answer	O
gives	O
most	O
insight	O
you	O
may	O
it	O
helpful	O
to	O
solve	O
these	O
problems	O
and	O
those	O
that	O
follow	O
using	O
notation	B
such	O
as	O
a	O
number	O
of	O
days	O
in	O
year	O
and	O
s	O
number	O
of	O
people	O
exercise	O
the	O
birthday	B
problem	O
may	O
be	O
related	O
to	O
a	O
coding	O
scheme	O
assume	O
we	O
wish	O
to	O
convey	O
a	O
message	O
to	O
an	O
outsider	O
identifying	O
one	O
of	O
figure	O
some	O
more	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
the	O
twenty-four	O
people	O
we	O
could	O
simply	O
communicate	O
a	O
number	O
s	O
from	O
as	O
having	O
agreed	O
a	O
mapping	B
of	O
people	O
onto	O
numbers	O
alternatively	O
we	O
could	O
convey	O
a	O
number	O
from	O
ax	O
identifying	O
the	O
day	O
of	O
the	O
year	O
that	O
is	O
the	O
selected	O
person	O
s	O
birthday	B
apologies	O
to	O
leapyearians	O
receiver	O
is	O
assumed	O
to	O
know	O
all	O
the	O
people	O
s	O
birthdays	O
what	O
roughly	O
is	O
the	O
probability	B
of	I
error	I
of	O
this	O
communication	B
scheme	O
assuming	O
it	O
is	O
used	O
for	O
a	O
single	O
transmission	O
what	O
is	O
the	O
capacity	B
of	O
the	O
communication	B
channel	O
and	O
what	O
is	O
the	O
rate	B
of	O
communication	B
attempted	O
by	O
this	O
scheme	O
exercise	O
now	O
imagine	O
that	O
there	O
are	O
k	O
rooms	O
in	O
a	O
building	O
each	O
containing	O
q	O
people	O
might	O
think	O
of	O
k	O
and	O
q	O
as	O
an	O
example	O
the	O
aim	O
is	O
to	O
communicate	O
a	O
selection	O
of	O
one	O
person	O
from	O
each	O
room	O
by	O
transmitting	O
an	O
ordered	O
list	O
of	O
k	O
days	O
ax	O
compare	O
the	O
probability	B
of	I
error	I
of	O
the	O
following	O
two	O
schemes	O
as	O
before	O
where	O
each	O
room	O
transmits	O
the	O
birthday	B
of	O
the	O
selected	O
person	O
to	O
each	O
k-tuple	O
of	O
people	O
one	O
drawn	O
from	O
each	O
room	O
an	O
ordered	O
k-tuple	O
of	O
randomly	O
selected	O
days	O
from	O
ax	O
is	O
assigned	O
ktuple	O
has	O
nothing	O
to	O
do	O
with	O
their	O
birthdays	O
this	O
enormous	O
list	O
of	O
s	O
qk	O
strings	O
is	O
known	O
to	O
the	O
receiver	O
when	O
the	O
building	O
has	O
selected	O
a	O
particular	O
person	O
from	O
each	O
room	O
the	O
ordered	O
string	O
of	O
days	O
corresponding	O
to	O
that	O
k-tuple	O
of	O
people	O
is	O
transmitted	O
what	O
is	O
the	O
probability	B
of	I
error	I
when	O
q	O
and	O
k	O
what	O
is	O
the	O
probability	B
of	I
error	I
when	O
q	O
and	O
k	O
is	O
large	O
e	O
g	O
k	O
solutions	O
solution	O
to	O
exercise	O
if	O
we	O
assume	O
we	O
observe	O
y	O
p	O
y	O
p	O
x	O
p	O
j	O
solution	O
to	O
exercise	O
if	O
we	O
observe	O
y	O
p	O
y	O
solution	O
to	O
exercise	O
the	O
probability	B
that	O
y	O
is	O
so	O
the	O
mutual	B
information	B
is	O
ix	O
y	O
hy	O
hy	O
j	O
x	O
bits	O
solution	O
to	O
exercise	O
we	O
again	O
compute	O
the	O
mutual	B
information	B
using	O
ix	O
y	O
hy	O
hy	O
j	O
x	O
the	O
probability	B
that	O
y	O
is	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
a	O
noisy	B
channel	O
hy	O
j	O
x	O
p	O
j	O
x	O
p	O
j	O
x	O
p	O
j	O
x	O
so	O
the	O
mutual	B
information	B
is	O
ix	O
y	O
hy	O
hy	O
j	O
x	O
bits	O
solution	O
to	O
exercise	O
by	O
symmetry	O
the	O
optimal	B
input	I
distribution	B
is	O
then	O
the	O
capacity	B
is	O
c	O
ix	O
y	O
hy	O
hy	O
j	O
x	O
would	O
you	O
like	O
to	O
the	O
optimal	B
input	I
distribution	B
without	O
invoking	O
symmetry	O
we	O
can	O
do	O
this	O
by	O
computing	O
the	O
mutual	B
information	B
in	O
the	O
general	O
case	O
where	O
the	O
input	B
ensemble	B
is	O
ix	O
y	O
hy	O
hy	O
j	O
x	O
f	O
the	O
only	O
p-dependence	O
is	O
in	O
the	O
term	O
f	O
which	O
is	O
maximized	O
by	O
setting	O
the	O
argument	O
to	O
this	O
value	O
is	O
given	O
by	O
setting	O
solution	O
to	O
exercise	O
answer	O
by	O
symmetry	O
the	O
optimal	B
input	I
distribution	B
is	O
the	O
capacity	B
is	O
most	O
easily	O
evaluated	O
by	O
writing	B
the	O
mutual	B
information	B
as	O
ix	O
y	O
hx	O
hx	O
j	O
y	O
the	O
conditional	B
entropy	B
hx	O
j	O
y	O
is	O
py	O
p	O
j	O
y	O
when	O
y	O
is	O
known	O
x	O
is	O
uncertain	O
only	O
if	O
y	O
which	O
occurs	O
with	O
probability	B
f	O
f	O
so	O
the	O
conditional	B
entropy	B
hx	O
j	O
y	O
is	O
f	O
c	O
ix	O
y	O
hx	O
hx	O
j	O
y	O
f	O
f	O
the	O
binary	B
erasure	B
channel	I
fails	O
a	O
fraction	O
f	O
of	O
the	O
time	O
its	O
capacity	B
is	O
precisely	O
f	O
which	O
is	O
the	O
fraction	O
of	O
the	O
time	O
that	O
the	O
channel	O
is	O
reliable	O
this	O
result	O
seems	O
very	O
reasonable	O
but	O
it	O
is	O
far	O
from	O
obvious	O
how	O
to	O
encode	O
information	B
so	O
as	O
to	O
communicate	O
reliably	O
over	O
this	O
channel	O
answer	O
alternatively	O
without	O
invoking	O
the	O
symmetry	O
assumed	O
above	O
we	O
can	O
start	O
from	O
the	O
input	B
ensemble	B
the	O
probability	B
that	O
y	O
is	O
f	O
and	O
when	O
we	O
receive	O
y	O
the	O
posterior	B
probability	B
of	O
x	O
is	O
the	O
same	O
as	O
the	O
prior	B
probability	B
so	O
ix	O
y	O
hx	O
hx	O
j	O
y	O
f	O
f	O
this	O
mutual	B
information	B
achieves	O
its	O
maximum	O
value	O
of	O
when	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
q	O
n	O
n	O
the	O
extended	B
channel	I
solution	O
to	O
exercise	O
is	O
shown	O
in	O
the	O
best	O
code	O
for	O
this	O
channel	O
with	O
n	O
is	O
obtained	O
by	O
choosing	O
two	O
columns	O
that	O
have	O
minimal	O
overlap	O
for	O
example	O
columns	O
and	O
the	O
decoding	B
algorithm	O
returns	O
if	O
the	O
extended	B
channel	I
output	O
is	O
among	O
the	O
top	O
four	O
and	O
if	O
it	O
s	O
among	O
the	O
bottom	O
four	O
and	O
gives	O
up	O
if	O
the	O
output	O
is	O
solution	O
to	O
exercise	O
mutual	B
information	B
between	O
input	O
and	O
output	O
of	O
the	O
z	B
channel	I
is	O
in	O
example	O
we	O
showed	O
that	O
the	O
ix	O
y	O
hy	O
hy	O
j	O
x	O
f	O
we	O
this	O
expression	O
with	O
respect	O
to	O
taking	O
care	O
not	O
to	O
confuse	O
with	O
loge	O
d	O
ix	O
y	O
f	O
f	O
f	O
setting	O
this	O
derivative	O
to	O
zero	O
and	O
rearranging	O
using	O
skills	O
developed	O
in	O
exercise	O
we	O
obtain	O
f	O
so	O
the	O
optimal	B
input	I
distribution	B
is	O
f	O
as	O
the	O
noise	O
level	O
f	O
tends	O
to	O
this	O
expression	O
tends	O
to	O
you	O
can	O
prove	O
using	O
l	O
hopital	O
s	O
rule	O
for	O
all	O
values	O
of	O
f	O
is	O
smaller	O
than	O
a	O
rough	O
intuition	O
for	O
why	O
input	O
is	O
used	O
less	O
than	O
input	O
is	O
that	O
when	O
input	O
is	O
used	O
the	O
noisy	B
channel	O
injects	O
entropy	B
into	O
the	O
received	O
string	O
whereas	O
when	O
input	O
is	O
used	O
the	O
noise	O
has	O
zero	O
entropy	B
solution	O
to	O
exercise	O
the	O
capacities	O
of	O
the	O
three	O
channels	O
are	O
shown	O
in	O
for	O
any	O
f	O
the	O
bec	O
is	O
the	O
channel	O
with	O
highest	O
capacity	B
and	O
the	O
bsc	O
the	O
lowest	O
solution	O
to	O
exercise	O
the	O
logarithm	O
of	O
the	O
posterior	B
probability	B
ratio	O
given	O
y	O
is	O
ay	O
ln	O
p	O
y	O
p	O
y	O
ln	O
qy	O
j	O
x	O
qy	O
j	O
x	O
figure	O
the	O
extended	B
channel	I
obtained	O
from	O
a	O
binary	B
erasure	B
channel	I
with	O
erasure	B
probability	B
a	O
block	B
code	I
consisting	O
of	O
the	O
two	O
codewords	O
and	O
the	O
optimal	B
decoder	B
for	O
this	O
code	O
z	O
bsc	O
bec	O
figure	O
capacities	O
of	O
the	O
z	B
channel	I
binary	B
symmetric	B
channel	I
and	O
binary	B
erasure	B
channel	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
a	O
noisy	B
channel	O
using	O
our	O
skills	O
picked	O
up	O
from	O
exercise	O
we	O
rewrite	O
this	O
in	O
the	O
form	O
p	O
y	O
the	O
optimal	B
decoder	B
selects	O
the	O
most	O
probable	O
hypothesis	O
this	O
can	O
be	O
done	O
simply	O
by	O
looking	O
at	O
the	O
sign	O
of	O
ay	O
if	O
ay	O
then	O
decode	O
as	O
the	O
probability	B
of	I
error	I
is	O
pb	O
dy	O
qy	O
j	O
x	O
dy	O
random	B
coding	O
solution	O
to	O
exercise	O
the	O
probability	B
that	O
s	O
people	O
whose	O
birthdays	O
are	O
drawn	O
at	O
random	B
from	O
a	O
days	O
all	O
have	O
distinct	O
birthdays	O
is	O
aa	O
s	O
as	O
the	O
probability	B
that	O
two	O
more	O
people	O
share	O
a	O
birthday	B
is	O
one	O
minus	O
this	O
quantity	O
which	O
for	O
s	O
and	O
a	O
is	O
about	O
this	O
exact	O
way	O
of	O
answering	O
the	O
question	O
is	O
not	O
very	O
informative	O
since	O
it	O
is	O
not	O
clear	O
for	O
what	O
value	O
of	O
s	O
the	O
probability	B
changes	O
from	O
being	O
close	O
to	O
to	O
being	O
close	O
to	O
the	O
number	O
of	O
pairs	O
is	O
ss	O
and	O
the	O
probability	B
that	O
a	O
particular	O
pair	O
shares	O
a	O
birthday	B
is	O
so	O
the	O
expected	O
number	O
of	O
collisions	O
is	O
ss	O
a	O
this	O
answer	O
is	O
more	O
instructive	O
the	O
expected	O
number	O
of	O
collisions	O
is	O
tiny	O
if	O
s	O
pa	O
and	O
big	O
if	O
s	O
pa	O
we	O
can	O
also	O
approximate	O
the	O
probability	B
that	O
all	O
birthdays	O
are	O
distinct	O
for	O
small	O
s	O
thus	O
aa	O
s	O
as	O
exp	O
i	O
ss	O
a	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
before	O
reading	O
chapter	O
you	O
should	O
have	O
read	O
chapters	O
and	O
exercise	O
is	O
especially	O
recommended	O
cast	O
of	O
characters	O
q	O
c	O
x	O
n	O
c	O
n	O
xs	O
s	O
the	O
noisy	B
channel	O
the	O
capacity	B
of	O
the	O
channel	O
an	O
ensemble	B
used	O
to	O
create	O
a	O
random	B
code	I
a	O
random	B
code	I
the	O
length	O
of	O
the	O
codewords	O
a	O
codeword	B
the	O
sth	O
in	O
the	O
code	O
the	O
number	O
of	O
a	O
chosen	O
codeword	B
selects	O
s	O
the	O
total	O
number	O
of	O
codewords	O
in	O
the	O
code	O
the	O
source	O
s	O
k	O
s	O
the	O
number	O
of	O
bits	O
conveyed	O
by	O
the	O
choice	O
of	O
one	O
codeword	B
s	O
r	O
kn	O
from	O
s	O
assuming	O
it	O
is	O
chosen	O
with	O
uniform	O
probability	B
a	O
binary	O
representation	O
of	O
the	O
number	O
s	O
the	O
rate	B
of	O
the	O
code	O
in	O
bits	O
per	O
channel	O
use	O
called	O
instead	O
the	O
decoder	B
s	O
guess	O
of	O
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
rpb	O
r	O
c	O
figure	O
portion	O
of	O
the	O
r	O
pb	O
plane	O
to	O
be	O
proved	O
achievable	O
and	O
not	O
achievable	O
the	O
noisy-channel	B
coding	B
theorem	I
the	O
theorem	O
the	O
theorem	O
has	O
three	O
parts	O
two	O
positive	O
and	O
one	O
negative	O
the	O
main	O
positive	O
result	O
is	O
the	O
for	O
every	O
discrete	B
memoryless	I
channel	O
the	O
channel	O
capacity	B
pb	O
c	O
max	O
px	O
ix	O
y	O
has	O
the	O
following	O
property	O
for	O
any	O
and	O
r	O
c	O
for	O
large	O
enough	O
n	O
there	O
exists	O
a	O
code	O
of	O
length	O
n	O
and	O
rate	B
r	O
and	O
a	O
decoding	B
algorithm	O
such	O
that	O
the	O
maximal	O
probability	B
of	I
block	B
error	I
is	O
if	O
a	O
probability	B
of	O
bit	B
error	O
pb	O
is	O
acceptable	O
rates	O
up	O
to	O
rpb	O
are	O
achiev	O
able	O
where	O
rpb	O
c	O
for	O
any	O
pb	O
rates	O
greater	O
than	O
rpb	O
are	O
not	O
achievable	O
jointly-typical	O
sequences	O
we	O
formalize	O
the	O
intuitive	O
preview	O
of	O
the	O
last	O
chapter	O
we	O
will	O
codewords	O
xs	O
as	O
coming	O
from	O
an	O
ensemble	B
x	O
n	O
and	O
consider	O
the	O
random	B
selection	O
of	O
one	O
codeword	B
and	O
a	O
corresponding	O
channel	O
output	O
y	O
thus	O
a	O
joint	B
ensemble	B
we	O
will	O
use	O
a	O
typical-set	B
decoder	B
which	O
decodes	O
a	O
received	O
signal	O
y	O
as	O
s	O
if	O
xs	O
and	O
y	O
are	O
jointly	O
typical	B
a	O
term	O
to	O
be	O
shortly	O
the	O
proof	O
will	O
then	O
centre	O
on	O
determining	O
the	O
probabilities	O
that	O
the	O
true	O
input	O
codeword	B
is	O
not	O
jointly	O
typical	B
with	O
the	O
output	O
sequence	B
and	O
that	O
a	O
false	O
input	O
codeword	B
is	O
jointly	O
typical	B
with	O
the	O
output	O
we	O
will	O
show	O
that	O
for	O
large	O
n	O
both	O
probabilities	O
go	O
to	O
zero	O
as	O
long	O
as	O
there	O
are	O
fewer	O
than	O
c	O
codewords	O
and	O
the	O
ensemble	B
x	O
is	O
the	O
optimal	B
input	I
distribution	B
joint	B
typicality	B
a	O
pair	O
of	O
sequences	O
x	O
y	O
of	O
length	O
n	O
are	O
to	O
be	O
jointly	O
typical	B
tolerance	O
with	O
respect	O
to	O
the	O
distribution	B
p	O
y	O
if	O
x	O
is	O
typical	B
of	O
p	O
i	O
e	O
y	O
is	O
typical	B
of	O
p	O
i	O
e	O
and	O
x	O
y	O
is	O
typical	B
of	O
p	O
y	O
i	O
e	O
log	O
log	O
log	O
n	O
n	O
n	O
p	O
p	O
hy	O
p	O
y	O
hx	O
y	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
jointly-typical	O
sequences	O
the	O
jointly-typical	O
set	B
jn	O
is	O
the	O
set	B
of	O
all	O
jointly-typical	O
sequence	B
pairs	O
of	O
length	O
n	O
example	O
here	O
is	O
a	O
jointly-typical	O
pair	O
of	O
length	O
n	O
for	O
the	O
ensemble	B
p	O
y	O
in	O
which	O
p	O
has	O
and	O
p	O
j	O
x	O
corresponds	O
to	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
x	O
y	O
notice	O
that	O
x	O
has	O
and	O
so	O
is	O
typical	B
of	O
the	O
probability	B
p	O
any	O
tolerance	O
and	O
y	O
has	O
so	O
it	O
is	O
typical	B
of	O
p	O
p	O
and	O
x	O
and	O
y	O
in	O
bits	O
which	O
is	O
the	O
typical	B
number	O
of	O
for	O
this	O
channel	O
joint	B
typicality	B
theorem	I
let	O
x	O
y	O
be	O
drawn	O
from	O
the	O
ensemble	B
by	O
then	O
p	O
y	O
p	O
yn	O
n	O
the	O
probability	B
that	O
x	O
y	O
are	O
jointly	O
typical	B
tolerance	O
tends	O
to	O
as	O
n	O
to	O
be	O
precise	O
the	O
number	O
of	O
jointly-typical	O
sequences	O
jjn	O
is	O
close	O
to	O
hxy	O
if	O
x	O
n	O
and	O
y	O
n	O
i	O
e	O
and	O
are	O
independent	O
samples	O
with	O
the	O
same	O
marginal	B
distribution	B
as	O
p	O
y	O
then	O
the	O
probability	B
that	O
lands	O
in	O
the	O
jointly-typical	O
set	B
is	O
about	O
ixy	O
to	O
be	O
precise	O
jjn	O
p	O
jn	O
proof	O
the	O
proof	O
of	O
parts	O
and	O
by	O
the	O
law	B
of	I
large	I
numbers	I
follows	O
that	O
of	O
the	O
source	B
coding	B
theorem	I
in	O
chapter	O
for	O
part	O
let	O
the	O
pair	O
x	O
y	O
play	O
the	O
role	O
of	O
x	O
in	O
the	O
source	B
coding	B
theorem	I
replacing	O
p	O
there	O
by	O
the	O
probability	B
distribution	B
p	O
y	O
for	O
the	O
third	O
part	O
p	O
jn	O
p	O
jjn	O
a	O
cartoon	O
of	O
the	O
jointly-typical	O
set	B
is	O
shown	O
in	O
two	O
independent	O
typical	B
vectors	B
are	O
jointly	O
typical	B
with	O
probability	B
p	O
jn	O
because	O
the	O
total	O
number	O
of	O
independent	O
typical	B
pairs	O
is	O
the	O
area	O
of	O
the	O
dashed	O
rectangle	O
hy	O
and	O
the	O
number	O
of	O
jointly-typical	O
pairs	O
is	O
roughly	O
hxy	O
so	O
the	O
probability	B
of	O
hitting	O
a	O
jointly-typical	O
pair	O
is	O
roughly	O
hxy	O
hxn	O
hy	O
ixy	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
noisy-channel	B
coding	B
theorem	I
y	O
the	O
set	B
of	O
x	O
the	O
set	B
of	O
all	O
input	O
figure	O
the	O
jointly-typical	O
set	B
the	O
horizontal	O
direction	O
represents	O
an	O
strings	O
of	O
length	O
n	O
the	O
vertical	O
direction	O
represents	O
an	O
all	O
output	O
strings	O
of	O
length	O
n	O
the	O
outer	O
box	B
contains	O
all	O
conceivable	O
inputoutput	O
pairs	O
each	O
dot	O
represents	O
a	O
jointly-typical	O
pair	O
of	O
sequences	O
y	O
the	O
total	O
number	O
of	O
jointly-typical	O
sequences	O
is	O
about	O
hxy	O
an	O
x	O
hx	O
hxy	O
dots	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
hy	O
jx	O
hxjy	O
hy	O
an	O
y	O
proof	O
of	O
the	O
noisy-channel	B
coding	B
theorem	I
analogy	O
imagine	O
that	O
we	O
wish	O
to	O
prove	O
that	O
there	O
is	O
a	O
baby	O
in	O
a	O
class	O
of	O
one	O
hundred	O
babies	O
who	O
weighs	O
less	O
than	O
kg	O
individual	O
babies	O
are	O
to	O
catch	O
and	O
weigh	O
shannon	B
s	O
method	O
of	O
solving	O
the	O
task	O
is	O
to	O
scoop	O
up	O
all	O
the	O
babies	O
and	O
weigh	O
them	O
all	O
at	O
once	O
on	O
a	O
big	O
weighing	O
machine	O
if	O
we	O
that	O
their	O
average	O
weight	O
is	O
smaller	O
than	O
kg	O
there	O
must	O
exist	O
at	O
least	O
one	O
baby	O
who	O
weighs	O
less	O
than	O
kg	O
indeed	O
there	O
must	O
be	O
many	O
shannon	B
s	O
method	O
isn	O
t	O
guaranteed	O
to	O
reveal	O
the	O
existence	O
of	O
an	O
underweight	O
child	O
since	O
it	O
relies	O
on	O
there	O
being	O
a	O
tiny	O
number	O
of	O
elephants	O
in	O
the	O
class	O
but	O
if	O
we	O
use	O
his	O
method	O
and	O
get	O
a	O
total	O
weight	O
smaller	O
than	O
kg	O
then	O
our	O
task	O
is	O
solved	O
from	O
skinny	O
children	O
to	O
fantastic	O
codes	O
we	O
wish	O
to	O
show	O
that	O
there	O
exists	O
a	O
code	O
and	O
a	O
decoder	B
having	O
small	O
probability	B
of	I
error	I
evaluating	O
the	O
probability	B
of	I
error	I
of	O
any	O
particular	O
coding	O
and	O
decoding	B
system	O
is	O
not	O
easy	O
shannon	B
s	O
innovation	O
was	O
this	O
instead	O
of	O
constructing	O
a	O
good	B
coding	O
and	O
decoding	B
system	O
and	O
evaluating	O
its	O
error	B
probability	B
shannon	B
calculated	O
the	O
average	O
probability	B
of	I
block	B
error	I
of	O
all	O
codes	O
and	O
proved	O
that	O
this	O
average	O
is	O
small	O
there	O
must	O
then	O
exist	O
individual	O
codes	O
that	O
have	O
small	O
probability	B
of	I
block	B
error	I
random	B
coding	O
and	O
typical-set	O
decoding	B
consider	O
the	O
following	O
encodingdecoding	O
system	O
whose	O
rate	B
is	O
we	O
p	O
and	O
generate	O
the	O
s	O
codewords	O
of	O
a	O
n	O
figure	O
shannon	B
s	O
method	O
for	O
proving	O
one	O
baby	O
weighs	O
less	O
than	O
kg	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
proof	O
of	O
the	O
noisy-channel	B
coding	B
theorem	I
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
ya	O
yb	O
yd	O
yc	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
k	O
code	O
c	O
at	O
random	B
according	O
to	O
p	O
n	O
p	O
figure	O
a	O
random	B
code	I
example	O
decodings	O
by	O
the	O
typical	B
set	B
decoder	B
a	O
sequence	B
that	O
is	O
not	O
jointly	O
typical	B
with	O
any	O
of	O
the	O
codewords	O
such	O
as	O
ya	O
is	O
decoded	O
as	O
a	O
sequence	B
that	O
is	O
jointly	O
typical	B
with	O
codeword	B
alone	O
yb	O
is	O
decoded	O
as	O
similarly	O
yc	O
is	O
decoded	O
as	O
a	O
sequence	B
that	O
is	O
jointly	O
typical	B
with	O
more	O
than	O
one	O
codeword	B
such	O
as	O
yd	O
is	O
decoded	O
as	O
a	O
random	B
code	I
is	O
shown	O
schematically	O
in	O
the	O
code	O
is	O
known	O
to	O
both	O
sender	O
and	O
receiver	O
a	O
message	O
s	O
is	O
chosen	O
from	O
and	O
xs	O
is	O
transmitted	O
the	O
received	O
signal	O
is	O
y	O
with	O
p	O
j	O
xs	O
n	O
p	O
j	O
xs	O
n	O
the	O
signal	O
is	O
decoded	O
by	O
typical-set	O
decoding	B
typical-set	O
decoding	B
decode	O
y	O
as	O
if	O
y	O
are	O
jointly	O
typical	B
and	O
there	O
is	O
no	O
other	O
such	O
that	O
y	O
are	O
jointly	O
typical	B
otherwise	O
declare	O
a	O
failure	O
this	O
is	O
not	O
the	O
optimal	B
decoding	B
algorithm	O
but	O
it	O
will	O
be	O
good	B
enough	O
and	O
easier	O
to	O
analyze	O
the	O
typical-set	B
decoder	B
is	O
illustrated	O
in	O
a	O
decoding	B
error	O
occurs	O
if	O
s	O
there	O
are	O
three	O
probabilities	O
of	O
error	O
that	O
we	O
can	O
distinguish	O
first	O
there	O
is	O
the	O
probability	B
of	I
block	B
error	I
for	O
a	O
particular	O
code	O
c	O
that	O
is	O
pbc	O
p	O
sjc	O
this	O
is	O
a	O
quantity	O
to	O
evaluate	O
for	O
any	O
given	O
code	O
second	O
there	O
is	O
the	O
average	O
over	O
all	O
codes	O
of	O
this	O
block	B
error	B
probability	B
hpbi	O
p	O
sjcp	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
noisy-channel	B
coding	B
theorem	I
hpbi	O
is	O
just	O
the	O
probability	B
that	O
there	O
is	O
a	O
decoding	B
error	O
at	O
step	O
of	O
the	O
process	O
on	O
the	O
previous	O
page	O
fortunately	O
this	O
quantity	O
is	O
much	O
easier	O
to	O
evaluate	O
than	O
the	O
quantity	O
p	O
sjc	O
third	O
the	O
maximal	O
block	B
error	B
probability	B
of	O
a	O
code	O
c	O
pbmc	O
max	O
s	O
p	O
sj	O
sc	O
is	O
the	O
quantity	O
we	O
are	O
most	O
interested	O
in	O
we	O
wish	O
to	O
show	O
that	O
there	O
exists	O
a	O
code	O
c	O
with	O
the	O
required	O
rate	B
whose	O
maximal	O
block	B
error	B
probability	B
is	O
small	O
we	O
will	O
get	O
to	O
this	O
result	O
by	O
the	O
average	O
block	B
error	B
probability	B
hpbi	O
once	O
we	O
have	O
shown	O
that	O
this	O
can	O
be	O
made	O
smaller	O
than	O
a	O
desired	O
small	O
number	O
we	O
immediately	O
deduce	O
that	O
there	O
must	O
exist	O
at	O
least	O
one	O
code	O
c	O
whose	O
block	B
error	B
probability	B
is	O
also	O
less	O
than	O
this	O
small	O
number	O
finally	O
we	O
show	O
that	O
this	O
code	O
whose	O
block	B
error	B
probability	B
is	O
satisfactorily	O
small	O
but	O
whose	O
maximal	O
block	B
error	B
probability	B
is	O
unknown	O
could	O
conceivably	O
be	O
enormous	O
can	O
be	O
to	O
make	O
a	O
code	O
of	O
slightly	O
smaller	O
rate	B
whose	O
maximal	O
block	B
error	B
probability	B
is	O
also	O
guaranteed	O
to	O
be	O
small	O
we	O
modify	O
the	O
code	O
by	O
throwing	O
away	O
the	O
worst	O
of	O
its	O
codewords	O
we	O
therefore	O
now	O
embark	O
on	O
the	O
average	O
probability	B
of	I
block	B
error	B
probability	B
of	I
error	I
of	O
typical-set	B
decoder	B
there	O
are	O
two	O
sources	O
of	O
error	O
when	O
we	O
use	O
typical-set	O
decoding	B
either	O
the	O
output	O
y	O
is	O
not	O
jointly	O
typical	B
with	O
the	O
transmitted	O
codeword	B
xs	O
or	O
there	O
is	O
some	O
other	O
codeword	B
in	O
c	O
that	O
is	O
jointly	O
typical	B
with	O
y	O
by	O
the	O
symmetry	O
of	O
the	O
code	O
construction	B
the	O
average	O
probability	B
of	I
error	I
averaged	O
over	O
all	O
codes	O
does	O
not	O
depend	O
on	O
the	O
selected	O
value	O
of	O
s	O
we	O
can	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
s	O
the	O
probability	B
that	O
the	O
input	O
and	O
the	O
output	O
y	O
are	O
not	O
jointly	O
typical	B
vanishes	O
by	O
the	O
joint	B
typicality	B
theorem	I
s	O
part	O
we	O
give	O
a	O
name	O
to	O
the	O
upper	O
bound	B
on	O
this	O
probability	B
satisfying	O
as	O
n	O
for	O
any	O
desired	O
we	O
can	O
a	O
blocklength	O
n	O
such	O
that	O
the	O
p	O
y	O
jn	O
the	O
probability	B
that	O
and	O
y	O
are	O
jointly	O
typical	B
for	O
a	O
given	O
is	O
by	O
part	O
and	O
there	O
are	O
rival	O
values	O
of	O
to	O
worry	O
about	O
thus	O
the	O
average	O
probability	B
of	I
error	I
hpbi	O
hpbi	O
the	O
inequality	B
that	O
bounds	O
a	O
total	O
probability	B
of	I
error	I
ptot	O
by	O
the	O
sum	O
of	O
the	O
probabilities	O
of	O
all	O
sorts	O
of	O
events	O
each	O
of	O
which	O
is	O
to	O
cause	O
error	O
is	O
called	O
a	O
union	B
bound	B
it	O
is	O
only	O
an	O
equality	O
if	O
the	O
events	O
that	O
cause	O
error	O
never	O
occur	O
at	O
the	O
same	O
time	O
as	O
each	O
other	O
ptot	O
the	O
average	O
probability	B
of	I
error	I
can	O
be	O
made	O
by	O
increasing	O
n	O
if	O
ix	O
y	O
we	O
are	O
almost	O
there	O
we	O
make	O
three	O
we	O
choose	O
p	O
in	O
the	O
proof	O
to	O
be	O
the	O
optimal	B
input	I
distribution	B
of	O
the	O
channel	O
then	O
the	O
condition	O
ix	O
y	O
becomes	O
c	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
errors	B
above	O
capacity	B
figure	O
how	O
expurgation	B
works	O
in	O
a	O
typical	B
random	B
code	I
a	O
small	O
fraction	O
of	O
the	O
codewords	O
are	O
involved	O
in	O
collisions	O
pairs	O
of	O
codewords	O
are	O
close	O
to	O
each	O
other	O
that	O
the	O
probability	B
of	I
error	I
when	O
either	O
codeword	B
is	O
transmitted	O
is	O
not	O
tiny	O
we	O
obtain	O
a	O
new	O
code	O
from	O
a	O
random	B
code	I
by	O
deleting	O
all	O
these	O
confusable	O
codewords	O
the	O
resulting	O
code	O
has	O
slightly	O
fewer	O
codewords	O
so	O
has	O
a	O
slightly	O
lower	O
rate	B
and	O
its	O
maximal	O
probability	B
of	I
error	I
is	O
greatly	O
reduced	O
pb	O
achievable	O
r	O
c	O
figure	O
portion	O
of	O
the	O
r	O
pb	O
plane	O
proved	O
achievable	O
in	O
the	O
part	O
of	O
the	O
theorem	O
ve	O
proved	O
that	O
the	O
maximal	O
probability	B
of	I
block	B
error	I
pbm	O
can	O
be	O
made	O
arbitrarily	O
small	O
so	O
the	O
same	O
goes	O
for	O
the	O
bit	B
error	B
probability	B
pb	O
which	O
must	O
be	O
smaller	O
than	O
pbm	O
a	O
random	B
code	I
expurgated	O
since	O
the	O
average	O
probability	B
of	I
error	I
over	O
all	O
codes	O
is	O
there	O
must	O
exist	O
a	O
code	O
with	O
mean	B
probability	B
of	I
block	B
error	I
pbc	O
to	O
show	O
that	O
not	O
only	O
the	O
average	O
but	O
also	O
the	O
maximal	O
probability	B
of	I
error	I
pbm	O
can	O
be	O
made	O
small	O
we	O
modify	O
this	O
code	O
by	O
throwing	O
away	O
the	O
worst	O
half	O
of	O
the	O
codewords	O
the	O
ones	O
most	O
likely	O
to	O
produce	O
errors	B
those	O
that	O
remain	O
must	O
all	O
have	O
conditional	B
probability	B
of	I
error	I
less	O
than	O
we	O
use	O
these	O
remaining	O
codewords	O
to	O
a	O
new	O
code	O
this	O
new	O
code	O
has	O
codewords	O
i	O
e	O
we	O
have	O
reduced	O
the	O
rate	B
from	O
to	O
negligible	O
reduction	O
if	O
n	O
is	O
large	O
and	O
achieved	O
pbm	O
this	O
trick	O
is	O
called	O
expurgation	B
the	O
resulting	O
code	O
may	O
not	O
be	O
the	O
best	O
code	O
of	O
its	O
rate	B
and	O
length	O
but	O
it	O
is	O
still	O
good	B
enough	O
to	O
prove	O
the	O
noisy-channel	B
coding	B
theorem	I
which	O
is	O
what	O
we	O
are	O
trying	O
to	O
do	O
here	O
in	O
conclusion	O
we	O
can	O
construct	O
a	O
code	O
of	O
rate	B
where	O
c	O
with	O
maximal	O
probability	B
of	I
error	I
we	O
obtain	O
the	O
theorem	O
as	O
stated	O
by	O
setting	O
and	O
n	O
large	O
for	O
the	O
remaining	O
conditions	O
to	O
hold	O
the	O
theorem	O
s	O
part	O
is	O
thus	O
proved	O
communication	B
errors	B
above	O
capacity	B
we	O
have	O
proved	O
for	O
any	O
discrete	B
memoryless	I
channel	O
the	O
achievability	O
of	O
a	O
portion	O
of	O
the	O
r	O
pb	O
plane	O
shown	O
in	O
we	O
have	O
shown	O
that	O
we	O
can	O
turn	O
any	O
noisy	B
channel	O
into	O
an	O
essentially	O
noiseless	B
binary	O
channel	O
with	O
rate	B
up	O
to	O
c	O
bits	O
per	O
cycle	O
we	O
now	O
extend	O
the	O
right-hand	O
boundary	O
of	O
the	O
region	O
of	O
achievability	O
at	O
non-zero	O
error	O
probabilities	O
is	O
called	O
rate-distortion	B
theory	I
we	O
do	O
this	O
with	O
a	O
new	O
trick	O
since	O
we	O
know	O
we	O
can	O
make	O
the	O
noisy	B
channel	O
into	O
a	O
perfect	B
channel	O
with	O
a	O
smaller	O
rate	B
it	O
is	O
to	O
consider	O
communication	B
with	O
errors	B
over	O
a	O
noiseless	B
channel	O
how	O
fast	O
can	O
we	O
communicate	O
over	O
a	O
noiseless	B
channel	O
if	O
we	O
are	O
allowed	O
to	O
make	O
errors	B
consider	O
a	O
noiseless	B
binary	O
channel	O
and	O
assume	O
that	O
we	O
force	O
communication	B
at	O
a	O
rate	B
greater	O
than	O
its	O
capacity	B
of	O
bit	B
for	O
example	O
if	O
we	O
require	O
the	O
sender	O
to	O
attempt	O
to	O
communicate	O
at	O
r	O
bits	O
per	O
cycle	O
then	O
he	O
must	O
throw	O
away	O
half	O
of	O
the	O
information	B
what	O
is	O
the	O
best	O
way	O
to	O
do	O
this	O
if	O
the	O
aim	O
is	O
to	O
achieve	O
the	O
smallest	O
possible	O
probability	B
of	O
bit	B
error	O
one	O
simple	O
strategy	O
is	O
to	O
communicate	O
a	O
fraction	O
of	O
the	O
source	O
bits	O
and	O
ignore	O
the	O
rest	O
the	O
receiver	O
guesses	O
the	O
missing	O
fraction	O
at	O
random	B
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
noisy-channel	B
coding	B
theorem	I
optimum	O
simple	O
pb	O
r	O
figure	O
a	O
simple	O
bound	B
on	O
achievable	O
points	O
pb	O
and	O
shannon	B
s	O
bound	B
the	O
average	O
probability	B
of	O
bit	B
error	O
is	O
pb	O
the	O
curve	O
corresponding	O
to	O
this	O
strategy	O
is	O
shown	O
by	O
the	O
dashed	O
line	O
in	O
the	O
risk	O
of	O
corruption	O
evenly	O
among	O
all	O
the	O
bits	O
pb	O
can	O
this	O
optimum	O
be	O
achieved	O
we	O
can	O
do	O
better	O
than	O
this	O
terms	O
of	O
minimizing	O
pb	O
by	O
spreading	O
out	O
in	O
fact	O
we	O
can	O
achieve	O
which	O
is	O
shown	O
by	O
the	O
solid	O
curve	O
in	O
so	O
how	O
we	O
reuse	O
a	O
tool	O
that	O
we	O
just	O
developed	O
namely	O
the	O
k	O
code	O
for	O
a	O
noisy	B
channel	O
and	O
we	O
turn	O
it	O
on	O
its	O
head	O
using	O
the	O
decoder	B
to	O
a	O
lossy	B
compressor	O
we	O
take	O
an	O
excellent	O
k	O
code	O
for	O
the	O
binary	B
symmetric	B
channel	I
assume	O
that	O
such	O
a	O
code	O
has	O
a	O
rate	B
kn	O
and	O
that	O
it	O
is	O
capable	O
of	O
correcting	O
errors	B
introduced	O
by	O
a	O
binary	B
symmetric	B
channel	I
whose	O
transition	B
probability	B
is	O
q	O
asymptotically	O
codes	O
exist	O
that	O
have	O
recall	O
that	O
if	O
we	O
attach	O
one	O
of	O
these	O
capacity-achieving	O
codes	O
of	O
length	O
n	O
to	O
a	O
binary	B
symmetric	B
channel	I
then	O
the	O
probability	B
distribution	B
over	O
the	O
outputs	O
is	O
close	O
to	O
uniform	O
since	O
the	O
entropy	B
of	O
the	O
output	O
is	O
equal	O
to	O
the	O
entropy	B
of	O
the	O
source	O
plus	O
the	O
entropy	B
of	O
the	O
noise	O
and	O
the	O
optimal	B
decoder	B
of	O
the	O
code	O
in	O
this	O
situation	O
typically	O
maps	O
a	O
received	O
vector	O
of	O
length	O
n	O
to	O
a	O
transmitted	O
vector	O
in	O
qn	O
bits	O
from	O
the	O
received	O
vector	O
we	O
take	O
the	O
signal	O
that	O
we	O
wish	O
to	O
send	O
and	O
chop	O
it	O
into	O
blocks	O
of	O
length	O
n	O
n	O
not	O
k	O
we	O
pass	O
each	O
block	B
through	O
the	O
decoder	B
and	O
obtain	O
a	O
shorter	O
signal	O
of	O
length	O
k	O
bits	O
which	O
we	O
communicate	O
over	O
the	O
noiseless	B
channel	O
to	O
decode	O
the	O
transmission	O
we	O
pass	O
the	O
k	O
bit	B
message	O
to	O
the	O
encoder	B
of	O
the	O
original	O
code	O
the	O
reconstituted	O
message	O
will	O
now	O
from	O
the	O
original	O
message	O
in	O
some	O
of	O
its	O
bits	O
typically	O
qn	O
of	O
them	O
so	O
the	O
probability	B
of	O
bit	B
error	O
will	O
be	O
pb	O
q	O
the	O
rate	B
of	O
this	O
lossy	B
compressor	O
is	O
r	O
nk	O
now	O
attaching	O
this	O
lossy	B
compressor	O
to	O
our	O
capacity-c	O
error-free	O
communicator	O
we	O
have	O
proved	O
the	O
achievability	O
of	O
communication	B
up	O
to	O
the	O
curve	O
r	O
by	O
r	O
c	O
for	O
further	O
reading	O
about	O
rate-distortion	B
theory	I
see	O
gallager	B
p	O
or	O
mceliece	O
p	O
the	O
non-achievable	O
region	O
of	O
the	O
theorem	O
the	O
source	O
encoder	B
noisy	B
channel	O
and	O
decoder	B
a	O
markov	B
chain	I
s	O
x	O
y	O
p	O
x	O
y	O
p	O
sp	O
j	O
xp	O
j	O
y	O
the	O
data	O
processing	O
inequality	B
must	O
apply	O
to	O
this	O
chain	O
is	O
ix	O
y	O
furthermore	O
by	O
the	O
of	O
channel	O
capacity	B
ix	O
y	O
n	O
c	O
so	O
is	O
n	O
c	O
assume	O
that	O
a	O
system	O
achieves	O
a	O
rate	B
r	O
and	O
a	O
bit	B
error	B
probability	B
pb	O
then	O
the	O
mutual	B
information	B
is	O
is	O
n	O
but	O
is	O
n	O
c	O
is	O
not	O
achievable	O
so	O
r	O
is	O
not	O
achievable	O
c	O
exercise	O
fill	O
in	O
the	O
details	O
in	O
the	O
preceding	O
argument	O
if	O
the	O
bit	B
errors	B
between	O
and	O
s	O
are	O
independent	O
then	O
we	O
have	O
is	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
computing	O
capacity	B
sections	O
contain	O
advanced	O
material	O
the	O
reader	O
is	O
encouraged	O
to	O
skip	O
to	O
section	B
what	O
if	O
we	O
have	O
complex	B
correlations	B
among	O
those	O
bit	B
errors	B
why	O
does	O
the	O
inequality	B
is	O
n	O
hold	O
computing	O
capacity	B
we	O
have	O
proved	O
that	O
the	O
capacity	B
of	O
a	O
channel	O
is	O
the	O
maximum	O
rate	B
at	O
which	O
reliable	O
communication	B
can	O
be	O
achieved	O
how	O
can	O
we	O
compute	O
the	O
capacity	B
of	O
a	O
given	O
discrete	B
memoryless	I
channel	O
we	O
need	O
to	O
its	O
optimal	B
input	I
distribution	B
in	O
general	O
we	O
can	O
the	O
optimal	B
input	I
distribution	B
by	O
a	O
computer	B
search	O
making	O
use	O
of	O
the	O
derivative	O
of	O
the	O
mutual	B
information	B
with	O
respect	O
to	O
the	O
input	O
probabilities	O
exercise	O
find	O
the	O
derivative	O
of	O
ix	O
y	O
with	O
respect	O
to	O
the	O
input	O
probability	B
pi	O
y	O
for	O
a	O
channel	O
with	O
conditional	B
probabilities	O
qjji	O
exercise	O
show	O
that	O
ix	O
y	O
is	O
a	O
concave	O
function	O
of	O
the	O
input	O
prob	O
ability	O
vector	O
p	O
since	O
ix	O
y	O
is	O
concave	O
in	O
the	O
input	O
distribution	B
p	O
any	O
probability	B
distribution	B
p	O
at	O
which	O
ix	O
y	O
is	O
stationary	O
must	O
be	O
a	O
global	O
maximum	O
of	O
ix	O
y	O
so	O
it	O
is	O
tempting	O
to	O
put	O
the	O
derivative	O
of	O
ix	O
y	O
into	O
a	O
routine	O
that	O
a	O
local	O
maximum	O
of	O
ix	O
y	O
that	O
is	O
an	O
input	O
distribution	B
p	O
such	O
that	O
y	O
for	O
all	O
i	O
where	O
is	O
a	O
lagrange	B
multiplier	I
associated	O
with	O
the	O
constraint	O
pi	O
pi	O
however	O
this	O
approach	O
may	O
fail	O
to	O
the	O
right	O
answer	O
because	O
ix	O
y	O
might	O
be	O
maximized	O
by	O
a	O
distribution	B
that	O
has	O
pi	O
for	O
some	O
inputs	O
a	O
simple	O
example	O
is	O
given	O
by	O
the	O
ternary	O
confusion	O
channel	O
ternary	O
confusion	O
channel	O
ax	O
ay	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
whenever	O
the	O
input	O
is	O
used	O
the	O
output	O
is	O
random	B
the	O
other	O
inputs	O
are	O
reliable	O
inputs	O
the	O
maximum	O
information	B
rate	B
of	O
bit	B
is	O
achieved	O
by	O
making	O
no	O
use	O
of	O
the	O
input	O
exercise	O
sketch	O
the	O
mutual	B
information	B
for	O
this	O
channel	O
as	O
a	O
function	O
of	O
the	O
input	O
distribution	B
p	O
pick	O
a	O
convenient	O
two-dimensional	B
representation	O
of	O
p	O
the	O
optimization	B
routine	O
must	O
therefore	O
take	O
account	O
of	O
the	O
possibility	O
that	O
as	O
we	O
go	O
up	O
hill	O
on	O
ix	O
y	O
we	O
may	O
run	O
into	O
the	O
inequality	B
constraints	O
pi	O
exercise	O
describe	O
the	O
condition	O
similar	O
to	O
equation	O
that	O
is	O
at	O
a	O
point	O
where	O
ix	O
y	O
is	O
maximized	O
and	O
describe	O
a	O
computer	B
program	O
for	O
the	O
capacity	B
of	O
a	O
channel	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
noisy-channel	B
coding	B
theorem	I
results	O
that	O
may	O
help	O
in	O
the	O
optimal	B
input	I
distribution	B
all	O
outputs	O
must	O
be	O
used	O
ix	O
y	O
is	O
a	O
convex	O
function	O
of	O
the	O
channel	O
parameters	B
there	O
may	O
be	O
several	O
optimal	B
input	O
distributions	O
but	O
they	O
all	O
look	O
the	O
same	O
at	O
the	O
output	O
exercise	O
prove	O
that	O
no	O
output	O
y	O
is	O
unused	O
by	O
an	O
optimal	B
input	O
distri	O
reminder	O
the	O
term	O
convex	O
means	O
convex	O
and	O
the	O
term	O
concave	O
means	O
concave	O
the	O
little	O
smile	O
and	O
frown	O
symbols	O
are	O
included	O
simply	O
to	O
remind	O
you	O
what	O
convex	O
and	O
concave	O
mean	B
bution	O
unless	O
it	O
is	O
unreachable	O
that	O
is	O
has	O
qy	O
j	O
x	O
for	O
all	O
x	O
exercise	O
prove	O
that	O
ix	O
y	O
is	O
a	O
convex	O
function	O
of	O
qy	O
j	O
x	O
exercise	O
prove	O
that	O
all	O
optimal	B
input	O
distributions	O
of	O
a	O
channel	O
have	O
the	O
same	O
output	O
probability	B
distribution	B
p	O
p	O
j	O
x	O
these	O
results	O
along	O
with	O
the	O
fact	O
that	O
ix	O
y	O
is	O
a	O
concave	O
function	O
of	O
the	O
input	O
probability	B
vector	O
p	O
prove	O
the	O
validity	O
of	O
the	O
symmetry	B
argument	I
that	O
we	O
have	O
used	O
when	O
the	O
capacity	B
of	O
symmetric	B
channels	O
if	O
a	O
channel	O
is	O
invariant	O
under	O
a	O
group	O
of	O
symmetry	O
operations	O
for	O
example	O
interchanging	O
the	O
input	O
symbols	O
and	O
interchanging	O
the	O
output	O
symbols	O
then	O
given	O
any	O
optimal	B
input	I
distribution	B
that	O
is	O
not	O
symmetric	B
i	O
e	O
is	O
not	O
invariant	O
under	O
these	O
operations	O
we	O
can	O
create	O
another	O
input	O
distribution	B
by	O
averaging	O
together	O
this	O
optimal	B
input	I
distribution	B
and	O
all	O
its	O
permuted	O
forms	O
that	O
we	O
can	O
make	O
by	O
applying	O
the	O
symmetry	O
operations	O
to	O
the	O
original	O
optimal	B
input	I
distribution	B
the	O
permuted	O
distributions	O
must	O
have	O
the	O
same	O
ix	O
y	O
as	O
the	O
original	O
by	O
symmetry	O
so	O
the	O
new	O
input	O
distribution	B
created	O
by	O
averaging	O
must	O
have	O
ix	O
y	O
bigger	O
than	O
or	O
equal	O
to	O
that	O
of	O
the	O
original	O
distribution	B
because	O
of	O
the	O
concavity	O
of	O
i	O
symmetric	B
channels	O
in	O
order	O
to	O
use	O
symmetry	O
arguments	O
it	O
will	O
help	O
to	O
have	O
a	O
of	O
a	O
symmetric	B
channel	I
i	O
like	O
gallager	B
s	O
a	O
discrete	B
memoryless	I
channel	O
is	O
a	O
symmetric	B
channel	I
if	O
the	O
set	B
of	O
outputs	O
can	O
be	O
partitioned	O
into	O
subsets	O
in	O
such	O
a	O
way	O
that	O
for	O
each	O
subset	B
the	O
matrix	B
of	O
transition	B
probabilities	O
has	O
the	O
property	O
that	O
each	O
row	O
more	O
than	O
is	O
a	O
permutation	B
of	O
each	O
other	O
row	O
and	O
each	O
column	O
is	O
a	O
permutation	B
of	O
each	O
other	O
column	O
example	O
this	O
channel	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
is	O
a	O
symmetric	B
channel	I
because	O
its	O
outputs	O
can	O
be	O
partitioned	O
into	O
and	O
so	O
that	O
the	O
matrix	B
can	O
be	O
rewritten	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
p	O
x	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
other	O
coding	O
theorems	O
symmetry	O
is	O
a	O
useful	O
property	O
because	O
as	O
we	O
will	O
see	O
in	O
a	O
later	O
chapter	O
communication	B
at	O
capacity	B
can	O
be	O
achieved	O
over	O
symmetric	B
channels	O
by	O
linear	B
codes	I
exercise	O
prove	O
that	O
for	O
a	O
symmetric	B
channel	I
with	O
any	O
number	O
of	O
inputs	O
the	O
uniform	O
distribution	B
over	O
the	O
inputs	O
is	O
an	O
optimal	B
input	I
distribution	B
exercise	O
are	O
there	O
channels	O
that	O
are	O
not	O
symmetric	B
whose	O
optimal	B
input	O
distributions	O
are	O
uniform	O
find	O
one	O
or	O
prove	O
there	O
are	O
none	O
other	O
coding	O
theorems	O
the	O
noisy-channel	B
coding	B
theorem	I
that	O
we	O
proved	O
in	O
this	O
chapter	O
is	O
quite	O
general	O
applying	O
to	O
any	O
discrete	B
memoryless	I
channel	O
but	O
it	O
is	O
not	O
very	O
the	O
theorem	O
only	O
says	O
that	O
reliable	O
communication	B
with	O
error	B
probability	B
and	O
rate	B
r	O
can	O
be	O
achieved	O
by	O
using	O
codes	O
with	O
large	O
blocklength	O
n	O
the	O
theorem	O
does	O
not	O
say	O
how	O
large	O
n	O
needs	O
to	O
be	O
to	O
achieve	O
given	O
values	O
of	O
r	O
and	O
presumably	O
the	O
smaller	O
is	O
and	O
the	O
closer	O
r	O
is	O
to	O
c	O
the	O
larger	O
n	O
has	O
to	O
be	O
err	O
c	O
r	O
figure	O
a	O
typical	B
random-coding	B
exponent	I
noisy-channel	B
coding	B
theorem	I
version	O
with	O
explicit	O
n	O
for	O
a	O
discrete	B
memoryless	I
channel	O
a	O
blocklength	O
n	O
and	O
a	O
rate	B
r	O
there	O
exist	O
block	B
codes	O
of	O
length	O
n	O
whose	O
average	O
probability	B
of	I
error	I
pb	O
exp	O
err	O
where	O
err	O
is	O
the	O
random-coding	B
exponent	I
of	O
the	O
channel	O
a	O
convex	O
decreasing	O
positive	O
function	O
of	O
r	O
for	O
r	O
c	O
the	O
random-coding	B
exponent	I
is	O
also	O
known	O
as	O
the	O
reliability	B
function	I
an	O
expurgation	B
argument	O
it	O
can	O
also	O
be	O
shown	O
that	O
there	O
exist	O
block	B
codes	O
for	O
which	O
the	O
maximal	O
probability	B
of	I
error	I
pbm	O
is	O
also	O
exponentially	O
small	O
in	O
n	O
the	O
of	O
err	O
is	O
given	O
in	O
gallager	B
p	O
err	O
approaches	O
zero	O
as	O
r	O
c	O
the	O
typical	B
behaviour	I
of	I
this	O
function	O
is	O
illustrated	O
in	O
the	O
computation	O
of	O
the	O
random-coding	B
exponent	I
for	O
interesting	O
channels	O
is	O
a	O
challenging	O
task	O
on	O
which	O
much	O
has	O
been	O
expended	O
even	O
for	O
simple	O
channels	O
like	O
the	O
binary	B
symmetric	B
channel	I
there	O
is	O
no	O
simple	O
expression	O
for	O
err	O
lower	O
bounds	O
on	O
the	O
error	B
probability	B
as	O
a	O
function	O
of	O
blocklength	O
the	O
theorem	O
stated	O
above	O
asserts	O
that	O
there	O
are	O
codes	O
with	O
pb	O
smaller	O
than	O
exp	O
err	O
but	O
how	O
small	O
can	O
the	O
error	B
probability	B
be	O
could	O
it	O
be	O
much	O
smaller	O
for	O
any	O
code	O
with	O
blocklength	O
n	O
on	O
a	O
discrete	B
memoryless	I
channel	O
the	O
probability	B
of	I
error	I
assuming	O
all	O
source	O
messages	O
are	O
used	O
with	O
equal	O
probability	B
pb	O
espr	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
noisy-channel	B
coding	B
theorem	I
where	O
the	O
function	O
espr	O
the	O
sphere-packing	B
exponent	I
of	O
the	O
channel	O
is	O
a	O
convex	O
decreasing	O
positive	O
function	O
of	O
r	O
for	O
r	O
c	O
for	O
a	O
precise	O
statement	O
of	O
this	O
result	O
and	O
further	O
references	O
see	O
gallager	B
p	O
noisy-channel	O
coding	O
theorems	O
and	O
coding	O
practice	O
imagine	O
a	O
customer	O
who	O
wants	O
to	O
buy	O
an	O
error-correcting	B
code	I
and	O
decoder	B
for	O
a	O
noisy	B
channel	O
the	O
results	O
described	O
above	O
allow	O
us	O
to	O
the	O
following	O
service	O
if	O
he	O
tells	O
us	O
the	O
properties	O
of	O
his	O
channel	O
the	O
desired	O
rate	B
r	O
and	O
the	O
desired	O
error	B
probability	B
pb	O
we	O
can	O
after	O
working	O
out	O
the	O
relevant	O
functions	B
c	O
err	O
and	O
espr	O
advise	O
him	O
that	O
there	O
exists	O
a	O
solution	O
to	O
his	O
problem	O
using	O
a	O
particular	O
blocklength	O
n	O
indeed	O
that	O
almost	O
any	O
randomly	O
chosen	O
code	O
with	O
that	O
blocklength	O
should	O
do	O
the	O
job	O
unfortunately	O
we	O
have	O
not	O
found	O
out	O
how	O
to	O
implement	O
these	O
encoders	O
and	O
decoders	O
in	O
practice	O
the	O
cost	O
of	O
implementing	O
the	O
encoder	B
and	O
decoder	B
for	O
a	O
random	B
code	I
with	O
large	O
n	O
would	O
be	O
exponentially	O
large	O
in	O
n	O
furthermore	O
for	O
practical	B
purposes	O
the	O
customer	O
is	O
unlikely	O
to	O
know	O
exactly	O
what	O
channel	O
he	O
is	O
dealing	O
with	O
so	O
berlekamp	B
suggests	O
that	O
the	O
sensible	O
way	O
to	O
approach	O
error-correction	B
is	O
to	O
design	O
encoding-decoding	O
systems	O
and	O
plot	O
their	O
performance	O
on	O
a	O
variety	O
of	O
idealized	O
channels	O
as	O
a	O
function	O
of	O
the	O
channel	O
s	O
noise	O
level	O
these	O
charts	O
of	O
which	O
is	O
illustrated	O
on	O
page	O
can	O
then	O
be	O
shown	O
to	O
the	O
customer	O
who	O
can	O
choose	O
among	O
the	O
systems	O
on	O
without	O
having	O
to	O
specify	O
what	O
he	O
really	O
thinks	O
his	O
channel	O
is	O
like	O
with	O
this	O
attitude	O
to	O
the	O
practical	B
problem	O
the	O
importance	O
of	O
the	O
functions	B
err	O
and	O
espr	O
is	O
diminished	O
further	O
exercises	O
exercise	O
a	O
binary	B
erasure	B
channel	I
with	O
input	O
x	O
and	O
output	O
y	O
has	O
transition	B
probability	B
matrix	B
q	O
q	O
q	O
q	O
q	O
find	O
the	O
mutual	B
information	B
ix	O
y	O
between	O
the	O
input	O
and	O
output	O
for	O
general	O
input	O
distribution	B
and	O
show	O
that	O
the	O
capacity	B
of	O
this	O
channel	O
is	O
c	O
q	O
bits	O
a	O
z	B
channel	I
has	O
transition	B
probability	B
matrix	B
q	O
q	O
q	O
show	O
that	O
using	O
a	O
code	O
two	O
uses	O
of	O
a	O
z	B
channel	I
can	O
be	O
made	O
to	O
emulate	O
one	O
use	O
of	O
an	O
erasure	B
channel	I
and	O
state	O
the	O
erasure	B
probability	B
of	O
that	O
erasure	B
channel	I
hence	O
show	O
that	O
the	O
capacity	B
of	O
the	O
z	B
channel	I
cz	O
cz	O
q	O
bits	O
explain	O
why	O
the	O
result	O
cz	O
equality	O
q	O
is	O
an	O
inequality	B
rather	O
than	O
an	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
exercise	O
a	O
transatlantic	B
cable	I
contains	O
n	O
indistinguishable	O
electrical	O
wires	O
you	O
have	O
the	O
job	O
of	O
out	O
which	O
wire	O
is	O
which	O
that	O
is	O
to	O
create	O
a	O
consistent	O
labelling	O
of	O
the	O
wires	O
at	O
each	O
end	O
your	O
only	O
tools	O
are	O
the	O
ability	O
to	O
connect	O
wires	O
to	O
each	O
other	O
in	O
groups	O
of	O
two	O
or	O
more	O
and	O
to	O
test	O
for	O
connectedness	O
with	O
a	O
continuity	O
tester	O
what	O
is	O
the	O
smallest	O
number	O
of	O
transatlantic	B
trips	O
you	O
need	O
to	O
make	O
and	O
how	O
do	O
you	O
do	O
it	O
how	O
would	O
you	O
solve	O
the	O
problem	O
for	O
larger	O
n	O
such	O
as	O
n	O
as	O
an	O
illustration	O
if	O
n	O
were	O
then	O
the	O
task	O
can	O
be	O
solved	O
in	O
two	O
steps	O
by	O
labelling	O
one	O
wire	O
at	O
one	O
end	O
a	O
connecting	O
the	O
other	O
two	O
together	O
crossing	O
the	O
atlantic	B
measuring	O
which	O
two	O
wires	O
are	O
connected	O
labelling	O
them	O
b	O
and	O
c	O
and	O
the	O
unconnected	O
one	O
a	O
then	O
connecting	O
b	O
to	O
a	O
and	O
returning	O
across	O
the	O
atlantic	B
whereupon	O
on	O
disconnecting	O
b	O
from	O
c	O
the	O
identities	O
of	O
b	O
and	O
c	O
can	O
be	O
deduced	O
this	O
problem	O
can	O
be	O
solved	O
by	O
persistent	O
search	O
but	O
the	O
reason	O
it	O
is	O
posed	O
in	O
this	O
chapter	O
is	O
that	O
it	O
can	O
also	O
be	O
solved	O
by	O
a	O
greedy	O
approach	O
based	O
on	O
maximizing	O
the	O
acquired	O
information	B
let	O
the	O
unknown	O
permutation	B
of	O
wires	O
be	O
x	O
having	O
chosen	O
a	O
set	B
of	O
connections	O
of	O
wires	O
c	O
at	O
one	O
end	O
you	O
can	O
then	O
make	O
measurements	O
at	O
the	O
other	O
end	O
and	O
these	O
measurements	O
y	O
convey	O
information	B
about	O
x	O
how	O
much	O
and	O
for	O
what	O
set	B
of	O
connections	O
is	O
the	O
information	B
that	O
y	O
conveys	O
about	O
x	O
maximized	O
solutions	O
solution	O
to	O
exercise	O
the	O
mutual	B
information	B
is	O
if	O
the	O
input	O
distribution	B
is	O
p	O
p	O
ix	O
y	O
hy	O
hy	O
jx	O
p	O
we	O
can	O
build	O
a	O
good	B
sketch	O
of	O
this	O
function	O
in	O
two	O
ways	O
by	O
careful	O
inspection	O
of	O
the	O
function	O
or	O
by	O
looking	O
at	O
special	O
cases	O
for	O
the	O
plots	O
the	O
two-dimensional	B
representation	O
of	O
p	O
i	O
will	O
use	O
has	O
and	O
as	O
the	O
independent	O
variables	O
so	O
that	O
p	O
p	O
if	O
we	O
use	O
the	O
quantities	O
and	O
p	O
as	O
our	O
two	O
by	O
inspection	O
degrees	B
of	I
freedom	I
the	O
mutual	B
information	B
becomes	O
very	O
simple	O
ix	O
y	O
p	O
converting	O
back	O
to	O
and	O
we	O
obtain	O
the	O
sketch	O
shown	O
at	O
the	O
left	O
below	O
this	O
function	O
is	O
like	O
a	O
tunnel	O
rising	O
up	O
the	O
direction	O
of	O
increasing	O
and	O
to	O
obtain	O
the	O
required	O
plot	O
of	O
ix	O
y	O
we	O
have	O
to	O
strip	O
away	O
the	O
parts	O
of	O
this	O
tunnel	O
that	O
live	O
outside	O
the	O
feasible	O
simplex	B
of	O
probabilities	O
we	O
do	O
this	O
by	O
redrawing	O
the	O
surface	O
showing	O
only	O
the	O
parts	O
where	O
and	O
a	O
full	O
plot	O
of	O
the	O
function	O
is	O
shown	O
at	O
the	O
right	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
noisy-channel	B
coding	B
theorem	I
figure	O
skeleton	O
of	O
the	O
mutual	B
information	B
for	O
the	O
ternary	O
confusion	O
channel	O
special	O
cases	O
channel	O
and	O
ix	O
y	O
in	O
the	O
special	O
case	O
p	O
the	O
channel	O
is	O
a	O
noiseless	B
binary	O
in	O
the	O
special	O
case	O
the	O
term	O
is	O
equal	O
to	O
so	O
ix	O
y	O
p	O
we	O
know	O
how	O
to	O
sketch	O
that	O
from	O
the	O
previous	O
chapter	O
in	O
the	O
special	O
case	O
the	O
channel	O
is	O
a	O
z	B
channel	I
with	O
error	B
probability	B
these	O
special	O
cases	O
allow	O
us	O
to	O
construct	O
the	O
skeleton	O
shown	O
in	O
solution	O
to	O
exercise	O
necessary	O
and	O
conditions	O
for	O
p	O
to	O
maximize	O
ix	O
y	O
are	O
and	O
pi	O
and	O
pi	O
for	O
all	O
i	O
where	O
is	O
a	O
constant	O
related	O
to	O
the	O
capacity	B
by	O
c	O
e	O
this	O
result	O
can	O
be	O
used	O
in	O
a	O
computer	B
program	O
that	O
evaluates	O
the	O
derivatives	O
and	O
increments	O
and	O
decrements	O
the	O
probabilities	O
pi	O
in	O
proportion	O
to	O
the	O
between	O
those	O
derivatives	O
this	O
result	O
is	O
also	O
useful	O
for	O
lazy	O
human	B
who	O
are	O
good	B
guessers	O
having	O
guessed	O
the	O
optimal	B
input	I
distribution	B
one	O
can	O
simply	O
that	O
equation	O
holds	O
solution	O
to	O
exercise	O
we	O
certainly	O
expect	O
nonsymmetric	O
channels	O
with	O
uniform	O
optimal	B
input	O
distributions	O
to	O
exist	O
since	O
when	O
inventing	O
a	O
channel	O
we	O
have	O
ij	O
degrees	B
of	I
freedom	I
whereas	O
the	O
optimal	B
input	I
distribution	B
is	O
just	O
so	O
in	O
the	O
ij	O
space	O
of	O
perturbations	O
around	O
a	O
symmetric	B
channel	I
we	O
expect	O
there	O
to	O
be	O
a	O
subspace	O
of	O
perturbations	O
of	O
dimension	O
ij	O
ij	O
that	O
leave	O
the	O
optimal	B
input	I
distribution	B
unchanged	O
here	O
is	O
an	O
explicit	O
example	O
a	O
bit	B
like	O
a	O
z	B
channel	I
q	O
solution	O
to	O
exercise	O
the	O
labelling	O
problem	O
can	O
be	O
solved	O
for	O
any	O
n	O
with	O
just	O
two	O
trips	O
one	O
each	O
way	O
across	O
the	O
atlantic	B
the	O
key	O
step	O
in	O
the	O
information-theoretic	O
approach	O
to	O
this	O
problem	O
is	O
to	O
write	O
down	O
the	O
information	B
content	I
of	O
one	O
partition	B
the	O
combinatorial	O
object	O
that	O
is	O
the	O
connecting	O
together	O
of	O
subsets	O
of	O
wires	O
if	O
n	O
wires	O
are	O
grouped	O
together	O
into	O
subsets	O
of	O
size	O
subsets	O
of	O
size	O
then	O
the	O
number	O
of	O
such	O
partitions	O
is	O
n	O
gr	O
yr	O
and	O
the	O
information	B
content	I
of	O
one	O
such	O
partition	B
is	O
the	O
log	O
of	O
this	O
quantity	O
in	O
a	O
greedy	O
strategy	O
we	O
choose	O
the	O
partition	B
to	O
maximize	O
this	O
information	B
content	I
one	O
game	O
we	O
can	O
play	O
is	O
to	O
maximize	O
this	O
information	B
content	I
with	O
respect	O
to	O
the	O
quantities	O
gr	O
treated	O
as	O
real	O
numbers	O
subject	O
to	O
the	O
constraint	O
introducing	O
a	O
lagrange	B
multiplier	I
for	O
the	O
constraint	O
the	O
pr	O
grr	O
n	O
derivative	O
is	O
log	O
grr	O
log	O
r	O
log	O
gr	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
which	O
when	O
set	B
to	O
zero	O
leads	O
to	O
the	O
rather	O
nice	O
expression	O
gr	O
r	O
the	O
optimal	B
gr	O
is	O
proportional	O
to	O
a	O
poisson	B
distribution	B
we	O
can	O
solve	O
for	O
the	O
n	O
gives	O
the	O
implicit	O
equation	O
lagrange	B
multiplier	I
by	O
plugging	O
gr	O
into	O
the	O
constraint	O
pr	O
grr	O
n	O
which	O
where	O
is	O
a	O
convenient	O
reparameterization	O
of	O
the	O
lagrange	B
multiplier	I
figure	O
shows	O
a	O
graph	B
of	O
shows	O
the	O
deduced	O
noninteger	O
assignments	O
gr	O
when	O
and	O
nearby	O
integers	O
gr	O
that	O
motivate	O
setting	O
the	O
partition	B
to	O
this	O
partition	B
produces	O
a	O
random	B
partition	B
at	O
the	O
other	O
end	O
which	O
has	O
an	O
information	B
content	I
of	O
log	O
bits	O
which	O
is	O
a	O
lot	O
more	O
than	O
half	O
the	O
total	O
information	B
content	I
we	O
need	O
to	O
acquire	O
to	O
infer	O
the	O
transatlantic	B
permutation	B
log	O
bits	O
contrast	O
if	O
all	O
the	O
wires	O
are	O
joined	O
together	O
in	O
pairs	O
the	O
information	B
content	I
generated	O
is	O
only	O
about	O
bits	O
how	O
to	O
choose	O
the	O
second	O
partition	B
is	O
left	O
to	O
the	O
reader	O
a	O
shannonesque	O
approach	O
is	O
appropriate	O
picking	O
a	O
random	B
partition	B
at	O
the	O
other	O
end	O
using	O
the	O
same	O
fgrg	O
you	O
need	O
to	O
ensure	O
the	O
two	O
partitions	O
are	O
as	O
unlike	O
each	O
other	O
as	O
possible	O
if	O
n	O
or	O
then	O
the	O
labelling	O
problem	O
has	O
solutions	O
that	O
are	O
particularly	O
simple	O
to	O
implement	O
called	O
knowltongraham	O
partitions	O
partition	B
ng	O
into	O
disjoint	O
sets	O
in	O
two	O
ways	O
a	O
and	O
b	O
subject	O
to	O
the	O
condition	O
that	O
at	O
most	O
one	O
element	O
appears	O
both	O
in	O
an	O
a	O
set	B
of	O
cardinality	O
j	O
and	O
in	O
a	O
b	O
set	B
of	O
cardinality	O
k	O
for	O
each	O
j	O
and	O
k	O
graham	B
and	O
knowlton	O
figure	O
approximate	O
solution	O
of	O
the	O
cable-labelling	O
problem	O
using	O
lagrange	O
multipliers	O
the	O
parameter	O
as	O
a	O
function	O
of	O
n	O
the	O
value	O
is	O
highlighted	O
non-integer	O
values	O
of	O
the	O
function	O
gr	O
are	O
shown	O
by	O
lines	O
and	O
integer	O
values	O
of	O
gr	O
motivated	O
by	O
those	O
non-integer	O
values	O
are	O
shown	O
by	O
crosses	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
before	O
reading	O
chapter	O
you	O
should	O
have	O
read	O
chapters	O
and	O
you	O
will	O
also	O
need	O
to	O
be	O
familiar	O
with	O
the	O
gaussian	B
distribution	B
one-dimensional	O
gaussian	B
distribution	B
if	O
a	O
random	B
variable	I
y	O
is	O
gaus	O
sian	O
and	O
has	O
mean	B
and	O
variance	B
which	O
we	O
write	O
y	O
or	O
p	O
normaly	O
then	O
the	O
distribution	B
of	O
y	O
is	O
p	O
j	O
use	O
the	O
symbol	O
p	O
for	O
both	O
probability	B
densities	O
and	O
probabilities	O
the	O
inverse-variance	O
is	O
sometimes	O
called	O
the	O
precision	B
of	O
the	O
gaussian	B
distribution	B
multi-dimensional	B
gaussian	B
distribution	B
if	O
y	O
yn	O
has	O
a	O
multivariate	B
gaussian	B
distribution	B
then	O
p	O
j	O
x	O
a	O
za	O
xtay	O
where	O
x	O
is	O
the	O
mean	B
of	O
the	O
distribution	B
a	O
is	O
the	O
inverse	O
of	O
the	O
variancecovariance	O
matrix	B
and	O
the	O
normalizing	O
constant	O
is	O
za	O
this	O
distribution	B
has	O
the	O
property	O
that	O
the	O
variance	B
of	O
yi	O
and	O
the	O
covariance	B
of	O
yi	O
and	O
yj	O
are	O
given	O
by	O
e	O
ij	O
where	O
is	O
the	O
inverse	O
of	O
the	O
matrix	B
a	O
the	O
marginal	B
distribution	B
p	O
of	O
one	O
component	O
yi	O
is	O
gaussian	B
the	O
joint	B
marginal	B
distribution	B
of	O
any	O
subset	B
of	O
the	O
components	O
is	O
multivariate-gaussian	O
and	O
the	O
conditional	B
density	B
of	O
any	O
subset	B
given	O
the	O
values	O
of	O
another	O
subset	B
for	O
example	O
p	O
j	O
yj	O
is	O
also	O
gaussian	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
real	O
channels	O
the	O
noisy-channel	B
coding	B
theorem	I
that	O
we	O
have	O
proved	O
shows	O
that	O
there	O
exist	O
reliable	O
error-correcting	B
codes	I
for	O
any	O
noisy	B
channel	O
in	O
this	O
chapter	O
we	O
address	B
two	O
questions	O
first	O
many	O
practical	B
channels	O
have	O
real	O
rather	O
than	O
discrete	O
inputs	O
and	O
outputs	O
what	O
can	O
shannon	B
tell	O
us	O
about	O
these	O
continuous	B
channels	O
and	O
how	O
should	O
digital	O
signals	O
be	O
mapped	O
into	O
analogue	O
waveforms	O
and	O
vice	O
versa	O
second	O
how	O
are	O
practical	B
error-correcting	B
codes	I
made	O
and	O
what	O
is	O
achieved	O
in	O
practice	O
relative	B
to	O
the	O
possibilities	O
proved	O
by	O
shannon	B
the	O
gaussian	B
channel	I
the	O
most	O
popular	O
model	B
of	O
a	O
real-input	O
real-output	O
channel	O
is	O
the	O
gaussian	B
channel	I
the	O
gaussian	B
channel	I
has	O
a	O
real	O
input	O
x	O
and	O
a	O
real	O
output	O
y	O
the	O
condi	O
tional	O
distribution	B
of	O
y	O
given	O
x	O
is	O
a	O
gaussian	B
distribution	B
p	O
j	O
x	O
this	O
channel	O
has	O
a	O
continuous	B
input	O
and	O
output	O
but	O
is	O
discrete	O
in	O
time	O
we	O
will	O
show	O
below	O
that	O
certain	O
continuous-time	O
channels	O
are	O
equivalent	O
to	O
the	O
discrete-time	O
gaussian	B
channel	I
this	O
channel	O
channel	O
is	O
sometimes	O
called	O
the	O
additive	O
white	B
gaussian	B
noise	O
as	O
with	O
discrete	O
channels	O
we	O
will	O
discuss	O
what	O
rate	B
of	O
error-free	O
information	B
communication	B
can	O
be	O
achieved	O
over	O
this	O
channel	O
motivation	O
in	O
terms	O
of	O
a	O
continuous-time	O
channel	O
consider	O
a	O
physical	O
say	O
channel	O
with	O
inputs	O
and	O
outputs	O
that	O
are	O
continuous	B
in	O
time	O
we	O
put	O
in	O
xt	O
and	O
out	O
comes	O
yt	O
xt	O
nt	O
our	O
transmission	O
has	O
a	O
power	B
cost	I
the	O
average	O
power	O
of	O
a	O
transmission	O
of	O
length	O
t	O
may	O
be	O
constrained	B
thus	O
z	O
t	O
dt	O
p	O
the	O
received	O
signal	O
is	O
assumed	O
to	O
from	O
xt	O
by	O
additive	O
noise	O
nt	O
example	O
johnson	B
noise	I
which	O
we	O
will	O
model	B
as	O
white	B
gaussian	B
noise	O
the	O
magnitude	O
of	O
this	O
noise	O
is	O
by	O
the	O
noise	O
spectral	B
density	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
and	O
real	O
channels	O
how	O
could	O
such	O
a	O
channel	O
be	O
used	O
to	O
communicate	O
information	B
consider	O
in	O
a	O
signal	O
of	O
duration	O
t	O
made	O
transmitting	O
a	O
set	B
of	O
n	O
real	O
numbers	O
fxngn	O
up	O
of	O
a	O
weighted	O
combination	B
of	O
orthonormal	O
basis	O
functions	B
xt	O
n	O
where	O
r	O
t	O
dt	O
the	O
receiver	O
can	O
then	O
compute	O
the	O
scalars	O
yn	O
z	O
t	O
dt	O
xn	O
t	O
xn	O
nn	O
dt	O
for	O
n	O
n	O
if	O
there	O
were	O
no	O
noise	O
then	O
yn	O
would	O
equal	O
xn	O
the	O
white	B
gaussian	B
noise	O
nt	O
adds	O
scalar	O
noise	O
nn	O
to	O
the	O
estimate	O
yn	O
this	O
noise	O
is	O
gaussian	B
nn	O
where	O
is	O
the	O
spectral	B
density	B
introduced	O
above	O
thus	O
a	O
continuous	B
channel	I
used	O
in	O
this	O
way	O
is	O
equivalent	O
to	O
the	O
gaussian	B
channel	I
at	O
dt	O
p	O
t	O
a	O
constraint	O
on	O
tion	O
the	O
power	O
constraint	O
r	O
t	O
the	O
signal	O
amplitudes	O
xn	O
xt	O
figure	O
three	O
basis	O
functions	B
and	O
a	O
weighted	O
combination	B
of	O
them	O
xt	O
with	O
and	O
n	O
p	O
t	O
xn	O
n	O
p	O
t	O
n	O
before	O
returning	O
to	O
the	O
gaussian	B
channel	I
we	O
the	O
bandwidth	B
sured	O
in	O
hertz	B
of	O
the	O
continuous	B
channel	I
to	O
be	O
w	O
n	O
max	O
where	O
n	O
max	O
is	O
the	O
maximum	O
number	O
of	O
orthonormal	O
functions	B
that	O
can	O
be	O
produced	O
in	O
an	O
interval	O
of	O
length	O
t	O
this	O
can	O
be	O
motivated	O
by	O
imagining	O
creating	O
a	O
band-limited	B
signal	I
of	O
duration	O
t	O
from	O
orthonormal	O
cosine	O
and	O
sine	O
curves	O
of	O
maximum	O
frequency	B
w	O
the	O
number	O
of	O
orthonormal	O
functions	B
is	O
n	O
max	O
t	O
this	O
relates	O
to	O
the	O
nyquist	B
sampling	I
theorem	I
if	O
the	O
highest	O
frequency	B
present	O
in	O
a	O
signal	O
is	O
w	O
then	O
the	O
signal	O
can	O
be	O
fully	O
determined	O
from	O
its	O
values	O
at	O
a	O
series	O
of	O
discrete	O
sample	B
points	O
separated	O
by	O
the	O
nyquist	O
interval	O
seconds	O
so	O
the	O
use	O
of	O
a	O
real	O
continuous	B
channel	I
with	O
bandwidth	B
w	O
noise	O
spectral	B
density	B
and	O
power	O
p	O
is	O
equivalent	O
to	O
nt	O
uses	O
per	O
second	O
of	O
a	O
gaussian	B
channel	I
with	O
noise	O
level	O
and	O
subject	O
to	O
the	O
signal	O
power	O
constraint	O
n	O
of	O
imagine	O
that	O
the	O
gaussian	B
channel	I
yn	O
xn	O
nn	O
is	O
used	O
with	O
an	O
encoding	O
system	O
to	O
transmit	O
binary	O
source	O
bits	O
at	O
a	O
rate	B
of	O
r	O
bits	O
per	O
channel	O
use	O
how	O
can	O
we	O
compare	O
two	O
encoding	O
systems	O
that	O
have	O
rates	O
of	O
communication	B
r	O
and	O
that	O
use	O
powers	O
n	O
transmitting	O
at	O
a	O
large	O
rate	B
r	O
is	O
good	B
using	O
small	O
power	O
is	O
good	B
too	O
it	O
is	O
conventional	O
to	O
measure	O
the	O
rate-compensated	O
signal-to-noise	B
ratio	I
by	O
nr	O
to	O
the	O
noise	O
spectral	B
density	B
the	O
ratio	O
of	O
the	O
power	O
per	O
source	O
bit	B
eb	O
n	O
is	O
one	O
of	O
the	O
measures	O
used	O
to	O
compare	O
coding	O
schemes	O
for	O
gaussian	B
channels	O
is	O
dimensionless	O
but	O
it	O
is	O
usually	O
reported	O
in	O
the	O
units	B
of	O
decibels	O
the	O
value	O
given	O
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
inferring	O
the	O
input	O
to	O
a	O
real	O
channel	O
inferring	O
the	O
input	O
to	O
a	O
real	O
channel	O
the	O
best	O
detection	O
of	O
pulses	O
in	O
shannon	B
wrote	O
a	O
memorandum	O
on	O
the	O
problem	O
of	O
best	O
between	O
two	O
types	O
of	O
pulses	O
of	O
known	O
shape	O
represented	O
by	O
vectors	B
and	O
given	O
that	O
one	O
of	O
them	O
has	O
been	O
transmitted	O
over	O
a	O
noisy	B
channel	O
this	O
is	O
a	O
pattern	B
recognition	B
problem	O
it	O
is	O
assumed	O
that	O
the	O
noise	O
is	O
gaussian	B
with	O
probability	B
density	B
p	O
a	O
where	O
a	O
is	O
the	O
inverse	O
of	O
the	O
variancecovariance	O
matrix	B
of	O
the	O
noise	O
a	O
symmetric	B
and	O
matrix	B
a	O
is	O
a	O
multiple	O
of	O
the	O
identity	B
matrix	B
then	O
the	O
noise	O
is	O
white	B
for	O
more	O
general	O
a	O
the	O
noise	O
is	O
coloured	B
the	O
probability	B
of	O
the	O
received	O
vector	O
y	O
given	O
that	O
the	O
source	O
signal	O
was	O
s	O
zero	O
or	O
one	O
is	O
then	O
p	O
j	O
s	O
a	O
xstay	O
the	O
optimal	B
detector	O
is	O
based	O
on	O
the	O
posterior	B
probability	B
ratio	O
p	O
p	O
p	O
y	O
p	O
y	O
exp	O
p	O
j	O
s	O
p	O
j	O
s	O
ln	O
p	O
p	O
where	O
is	O
a	O
constant	O
independent	O
of	O
the	O
received	O
vector	O
y	O
xt	O
xt	O
ln	O
p	O
p	O
if	O
the	O
detector	O
is	O
forced	O
to	O
make	O
a	O
decision	O
guess	O
either	O
s	O
or	O
s	O
then	O
the	O
decision	O
that	O
minimizes	O
the	O
probability	B
of	I
error	I
is	O
to	O
guess	O
the	O
most	O
probable	O
hypothesis	O
we	O
can	O
write	O
the	O
optimal	B
decision	O
in	O
terms	O
of	O
a	O
discriminant	B
function	I
with	O
the	O
decisions	O
ay	O
ay	O
guess	O
s	O
ay	O
guess	O
s	O
ay	O
guess	O
either	O
notice	O
that	O
ay	O
is	O
a	O
linear	B
function	O
of	O
the	O
received	O
vector	O
ay	O
wty	O
where	O
w	O
capacity	B
of	O
gaussian	B
channel	I
until	O
now	O
we	O
have	O
measured	O
the	O
joint	B
marginal	B
and	O
conditional	B
entropy	B
of	O
discrete	O
variables	O
only	O
in	O
order	O
to	O
the	O
information	B
conveyed	O
by	O
continuous	B
variables	O
there	O
are	O
two	O
issues	O
we	O
must	O
address	B
the	O
length	O
of	O
the	O
real	O
line	O
and	O
the	O
precision	B
of	O
real	O
numbers	O
y	O
figure	O
two	O
pulses	O
and	O
represented	O
as	O
vectors	B
and	O
a	O
noisy	B
version	O
of	O
one	O
of	O
them	O
y	O
w	O
figure	O
the	O
weight	O
vector	O
w	O
that	O
is	O
used	O
to	O
discriminate	O
between	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
inputs	O
error-correcting	B
codes	I
and	O
real	O
channels	O
figure	O
a	O
probability	B
density	B
p	O
question	O
can	O
we	O
the	O
entropy	B
of	O
this	O
density	B
we	O
could	O
evaluate	O
the	O
entropies	O
of	O
a	O
sequence	B
of	O
probability	B
distributions	O
with	O
decreasing	O
grain-size	O
g	O
but	O
these	O
entropies	O
tend	O
to	O
z	O
p	O
log	O
p	O
independent	O
of	O
g	O
the	O
entropy	B
goes	O
up	O
by	O
one	O
bit	B
for	O
every	O
halving	O
of	O
g	O
dx	O
which	O
is	O
not	O
z	O
p	O
log	O
integral	B
p	O
dx	O
is	O
an	O
illegal	O
how	O
much	O
information	B
can	O
we	O
convey	O
in	O
one	O
use	O
of	O
a	O
gaussian	B
channel	I
if	O
we	O
are	O
allowed	O
to	O
put	O
any	O
real	O
number	O
x	O
into	O
the	O
gaussian	B
channel	I
we	O
could	O
communicate	O
an	O
enormous	O
string	O
of	O
n	O
digits	O
dn	O
by	O
setting	O
x	O
dn	O
the	O
amount	O
of	O
error-free	O
information	B
conveyed	O
in	O
just	O
a	O
single	O
transmission	O
could	O
be	O
made	O
arbitrarily	O
large	O
by	O
increasing	O
n	O
and	O
the	O
communication	B
could	O
be	O
made	O
arbitrarily	O
reliable	O
by	O
increasing	O
the	O
number	O
of	O
zeroes	O
at	O
the	O
end	O
of	O
x	O
there	O
is	O
usually	O
some	O
power	B
cost	I
associated	O
with	O
large	O
inputs	O
however	O
not	O
to	O
mention	O
practical	B
limits	O
in	O
the	O
dynamic	O
range	O
acceptable	O
to	O
a	O
receiver	O
it	O
is	O
therefore	O
conventional	O
to	O
introduce	O
a	O
cost	B
function	I
vx	O
for	O
every	O
input	O
x	O
and	O
constrain	O
codes	O
to	O
have	O
an	O
average	O
cost	O
less	O
than	O
or	O
equal	O
to	O
some	O
maximum	O
value	O
a	O
generalized	B
channel	O
coding	B
theorem	I
including	O
a	O
cost	B
function	I
for	O
the	O
inputs	O
can	O
be	O
proved	O
see	O
mceliece	O
the	O
result	O
is	O
a	O
channel	O
capacity	B
that	O
is	O
a	O
function	O
of	O
the	O
permitted	O
cost	O
for	O
the	O
gaussian	B
channel	I
we	O
will	O
assume	O
a	O
cost	O
vx	O
such	O
that	O
the	O
average	O
power	O
of	O
the	O
input	O
is	O
constrained	B
we	O
motivated	O
this	O
cost	B
function	I
above	O
in	O
the	O
case	O
of	O
real	O
electrical	O
channels	O
in	O
which	O
the	O
physical	O
power	O
consumption	O
is	O
indeed	O
quadratic	O
in	O
x	O
the	O
constraint	O
makes	O
it	O
impossible	O
to	O
communicate	O
information	B
in	O
one	O
use	O
of	O
the	O
gaussian	B
channel	I
precision	B
it	O
is	O
tempting	O
to	O
joint	B
marginal	B
and	O
conditional	B
entropies	O
for	O
real	O
variables	O
simply	O
by	O
replacing	O
summations	O
by	O
integrals	O
but	O
this	O
is	O
not	O
a	O
well	O
operation	O
as	O
we	O
discretize	O
an	O
interval	O
into	O
smaller	O
and	O
smaller	O
divisions	O
the	O
entropy	B
of	O
the	O
discrete	O
distribution	B
diverges	O
the	O
logarithm	O
of	O
the	O
granularity	O
also	O
it	O
is	O
not	O
permissible	O
to	O
take	O
the	O
logarithm	O
of	O
a	O
dimensional	O
quantity	O
such	O
as	O
a	O
probability	B
density	B
p	O
dimensions	B
are	O
there	O
is	O
one	O
information	B
measure	O
however	O
that	O
has	O
a	O
well-behaved	O
limit	O
namely	O
the	O
mutual	B
information	B
and	O
this	O
is	O
the	O
one	O
that	O
really	O
matters	O
since	O
it	O
measures	O
how	O
much	O
information	B
one	O
variable	O
conveys	O
about	O
another	O
in	O
the	O
discrete	O
case	O
ix	O
y	O
p	O
y	O
log	O
p	O
y	O
p	O
now	O
because	O
the	O
argument	O
of	O
the	O
log	O
is	O
a	O
ratio	O
of	O
two	O
probabilities	O
over	O
the	O
same	O
space	O
it	O
is	O
ok	O
to	O
have	O
p	O
y	O
p	O
and	O
p	O
be	O
probability	B
densities	O
and	O
replace	O
the	O
sum	O
by	O
an	O
integral	B
ix	O
y	O
z	O
dx	O
dy	O
p	O
y	O
log	O
p	O
y	O
p	O
z	O
dx	O
dy	O
p	O
j	O
x	O
log	O
p	O
j	O
x	O
p	O
we	O
can	O
now	O
ask	O
these	O
questions	O
for	O
the	O
gaussian	B
channel	I
what	O
probability	B
distribution	B
p	O
maximizes	O
the	O
mutual	B
information	B
to	O
the	O
constraint	O
v	O
and	O
does	O
the	O
maximal	O
mutual	B
information	B
still	O
measure	O
the	O
maximum	O
error-free	O
communication	B
rate	B
of	O
this	O
real	O
channel	O
as	O
it	O
did	O
for	O
the	O
discrete	O
channel	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
capacity	B
of	O
gaussian	B
channel	I
exercise	O
prove	O
that	O
the	O
probability	B
distribution	B
p	O
that	O
maximizes	O
the	O
mutual	B
information	B
to	O
the	O
constraint	O
v	O
is	O
a	O
gaussian	B
distribution	B
of	O
mean	B
zero	O
and	O
variance	B
v	O
exercise	O
show	O
that	O
the	O
mutual	B
information	B
ix	O
y	O
in	O
the	O
case	O
of	O
this	O
optimized	O
distribution	B
is	O
c	O
v	O
this	O
is	O
an	O
important	O
result	O
we	O
see	O
that	O
the	O
capacity	B
of	O
the	O
gaussian	B
channel	I
is	O
a	O
function	O
of	O
the	O
signal-to-noise	B
ratio	I
inferences	O
given	O
a	O
gaussian	B
input	O
distribution	B
if	O
p	O
normalx	O
v	O
and	O
p	O
j	O
x	O
normaly	O
x	O
then	O
the	O
marginal	B
distribution	B
of	O
y	O
is	O
p	O
normaly	O
and	O
the	O
posterior	O
distribution	B
of	O
the	O
input	O
given	O
that	O
the	O
output	O
is	O
y	O
is	O
p	O
y	O
p	O
j	O
xp	O
normal	B
x	O
v	O
y	O
v	O
v	O
step	O
from	O
to	O
is	O
made	O
by	O
completing	O
the	O
square	B
in	O
the	O
exponent	O
this	O
formula	O
deserves	O
careful	O
study	O
the	O
mean	B
of	O
the	O
posterior	O
distribution	B
y	O
can	O
be	O
viewed	O
as	O
a	O
weighted	O
combination	B
of	O
the	O
value	O
that	O
best	O
the	O
output	O
x	O
y	O
and	O
the	O
value	O
that	O
best	O
the	O
prior	B
x	O
v	O
v	O
v	O
y	O
y	O
the	O
weights	O
and	O
are	O
the	O
precisions	O
of	O
the	O
two	O
gaussians	O
that	O
we	O
multiplied	O
together	O
in	O
equation	O
the	O
prior	B
and	O
the	O
likelihood	B
the	O
precision	B
of	O
the	O
posterior	O
distribution	B
is	O
the	O
sum	O
of	O
these	O
two	O
precisions	O
this	O
is	O
a	O
general	O
property	O
whenever	O
two	O
independent	O
sources	O
contribute	O
information	B
via	O
gaussian	B
distributions	O
about	O
an	O
unknown	O
variable	O
the	O
precisions	B
add	I
is	O
the	O
dual	B
to	O
the	O
better-known	O
relationship	O
when	O
independent	O
variables	O
are	O
added	O
their	O
variances	B
add	I
noisy-channel	B
coding	B
theorem	I
for	O
the	O
gaussian	B
channel	I
we	O
have	O
evaluated	O
a	O
maximal	O
mutual	B
information	B
does	O
it	O
correspond	O
to	O
a	O
maximum	O
possible	O
rate	B
of	O
error-free	O
information	B
transmission	O
one	O
way	O
of	O
proving	O
that	O
this	O
is	O
so	O
is	O
to	O
a	O
sequence	B
of	O
discrete	O
channels	O
all	O
derived	O
from	O
the	O
gaussian	B
channel	I
with	O
increasing	O
numbers	O
of	O
inputs	O
and	O
outputs	O
and	O
prove	O
that	O
the	O
maximum	O
mutual	B
information	B
of	O
these	O
channels	O
tends	O
to	O
the	O
asserted	O
c	O
the	O
noisy-channel	B
coding	B
theorem	I
for	O
discrete	O
channels	O
applies	O
to	O
each	O
of	O
these	O
derived	O
channels	O
thus	O
we	O
obtain	O
a	O
coding	B
theorem	I
for	O
the	O
continuous	B
channel	I
alternatively	O
we	O
can	O
make	O
an	O
intuitive	O
argument	O
for	O
the	O
coding	B
theorem	I
for	O
the	O
gaussian	B
channel	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
and	O
real	O
channels	O
geometrical	O
view	O
of	O
the	O
noisy-channel	B
coding	B
theorem	I
sphere	B
packing	I
consider	O
a	O
sequence	B
x	O
xn	O
of	O
inputs	O
and	O
the	O
corresponding	O
output	O
y	O
as	O
two	O
points	O
in	O
an	O
n	O
dimensional	O
space	O
for	O
large	O
n	O
the	O
noise	O
power	O
is	O
very	O
likely	O
to	O
be	O
close	O
to	O
n	O
the	O
output	O
y	O
is	O
therefore	O
very	O
likely	O
to	O
be	O
close	O
to	O
the	O
surface	O
of	O
a	O
sphere	O
of	O
radius	O
pn	O
centred	O
on	O
x	O
similarly	O
if	O
the	O
original	O
signal	O
x	O
is	O
generated	O
at	O
random	B
subject	O
to	O
an	O
average	O
power	O
constraint	O
v	O
then	O
x	O
is	O
likely	O
to	O
lie	O
close	O
to	O
a	O
sphere	O
centred	O
on	O
the	O
origin	O
of	O
radius	O
pn	O
v	O
and	O
because	O
the	O
total	O
average	O
power	O
of	O
y	O
is	O
v	O
the	O
received	O
signal	O
y	O
is	O
likely	O
to	O
lie	O
on	O
the	O
surface	O
of	O
a	O
sphere	O
of	O
radius	O
pn	O
centred	O
on	O
the	O
origin	O
the	O
volume	B
of	O
an	O
n	O
sphere	O
of	O
radius	O
r	O
is	O
v	O
n	O
rn	O
now	O
consider	O
making	O
a	O
communication	B
system	O
based	O
on	O
non-confusable	B
inputs	I
x	O
that	O
is	O
inputs	O
whose	O
spheres	O
do	O
not	O
overlap	O
the	O
maximum	O
number	O
s	O
of	O
non-confusable	B
inputs	I
is	O
given	O
by	O
dividing	O
the	O
volume	B
of	O
the	O
sphere	O
of	O
probable	O
ys	O
by	O
the	O
volume	B
of	O
the	O
sphere	O
for	O
y	O
given	O
x	O
s	O
pn	O
pn	O
thus	O
the	O
capacity	B
is	O
bounded	O
by	O
c	O
n	O
log	O
m	O
v	O
a	O
more	O
detailed	O
argument	O
like	O
the	O
one	O
used	O
in	O
the	O
previous	O
chapter	O
can	O
establish	O
equality	O
back	O
to	O
the	O
continuous	B
channel	I
recall	O
that	O
the	O
use	O
of	O
a	O
real	O
continuous	B
channel	I
with	O
bandwidth	B
w	O
noise	O
spectral	B
density	B
and	O
power	O
p	O
is	O
equivalent	O
to	O
nt	O
uses	O
per	O
second	O
of	O
a	O
gaussian	B
channel	I
with	O
and	O
subject	O
to	O
the	O
constraint	O
n	O
substituting	O
the	O
result	O
for	O
the	O
capacity	B
of	O
the	O
gaussian	B
channel	I
we	O
the	O
capacity	B
of	O
the	O
continuous	B
channel	I
to	O
be	O
c	O
w	O
p	O
bits	O
per	O
second	O
this	O
formula	O
gives	O
insight	O
into	O
the	O
of	O
practical	B
communication	B
imagine	O
that	O
we	O
have	O
a	O
power	O
constraint	O
what	O
is	O
the	O
best	O
bandwidth	B
to	O
make	O
use	O
of	O
that	O
power	O
introducing	O
i	O
e	O
the	O
bandwidth	B
for	O
which	O
the	O
signal-to-noise	B
ratio	I
is	O
shows	O
as	O
a	O
function	O
of	O
the	O
capacity	B
increases	O
to	O
an	O
asymptote	O
of	O
log	O
e	O
it	O
is	O
dramatically	O
better	O
terms	O
of	O
capacity	B
for	O
power	O
to	O
transmit	O
at	O
a	O
low	O
signal-to-noise	B
ratio	I
over	O
a	O
large	O
bandwidth	B
than	O
with	O
high	O
signal-to-noise	O
in	O
a	O
narrow	O
bandwidth	B
this	O
is	O
one	O
motivation	O
for	O
wideband	O
communication	B
methods	O
such	O
as	O
the	O
direct	O
sequence	B
spread-spectrum	O
approach	O
used	O
in	O
mobile	O
phones	O
of	O
course	O
you	O
are	O
not	O
alone	O
and	O
your	O
electromagnetic	O
neighbours	O
may	O
not	O
be	O
pleased	O
if	O
you	O
use	O
a	O
large	O
bandwidth	B
so	O
for	O
social	O
reasons	O
engineers	O
often	O
have	O
to	O
make	O
do	O
with	O
higher-power	O
narrow-bandwidth	O
transmitters	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
bandwidth	B
figure	O
capacity	B
versus	O
bandwidth	B
for	O
a	O
real	O
channel	O
log	O
as	O
a	O
function	O
of	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
what	O
are	O
the	O
capabilities	O
of	O
practical	B
error-correcting	B
codes	I
what	O
are	O
the	O
capabilities	O
of	O
practical	B
error-correcting	B
codes	I
nearly	O
all	O
codes	O
are	O
good	B
but	O
nearly	O
all	O
codes	O
require	O
exponential	B
look-up	O
tables	O
for	O
practical	B
implementation	O
of	O
the	O
encoder	B
and	O
decoder	B
exponential	B
in	O
the	O
blocklength	O
n	O
and	O
the	O
coding	B
theorem	I
required	O
n	O
to	O
be	O
large	O
by	O
a	O
practical	B
error-correcting	B
code	I
we	O
mean	B
one	O
that	O
can	O
be	O
encoded	O
and	O
decoded	O
in	O
a	O
reasonable	O
amount	O
of	O
time	O
for	O
example	O
a	O
time	O
that	O
scales	O
as	O
a	O
polynomial	O
function	O
of	O
the	O
blocklength	O
n	O
preferably	O
linearly	O
the	O
shannon	B
limit	O
is	O
not	O
achieved	O
in	O
practice	O
the	O
non-constructive	O
proof	O
of	O
the	O
noisy-channel	B
coding	B
theorem	I
showed	O
that	O
good	B
block	B
codes	O
exist	O
for	O
any	O
noisy	B
channel	O
and	O
indeed	O
that	O
nearly	O
all	O
block	B
codes	O
are	O
good	B
but	O
writing	B
down	O
an	O
explicit	O
and	O
practical	B
encoder	B
and	O
decoder	B
that	O
are	O
as	O
good	B
as	O
promised	O
by	O
shannon	B
is	O
still	O
an	O
unsolved	O
problem	O
very	B
good	B
codes	O
given	O
a	O
channel	O
a	O
family	O
of	O
block	B
codes	O
that	O
achieve	O
arbitrarily	O
small	O
probability	B
of	I
error	I
at	O
any	O
communication	B
rate	B
up	O
to	O
the	O
capacity	B
of	O
the	O
channel	O
are	O
called	O
very	B
good	B
codes	O
for	O
that	O
channel	O
good	B
codes	O
are	O
code	O
families	O
that	O
achieve	O
arbitrarily	O
small	O
probability	B
of	I
error	I
at	O
non-zero	O
communication	B
rates	O
up	O
to	O
some	O
maximum	O
rate	B
that	O
may	O
be	O
less	O
than	O
the	O
capacity	B
of	O
the	O
given	O
channel	O
bad	B
codes	O
are	O
code	O
families	O
that	O
cannot	O
achieve	O
arbitrarily	O
small	O
probability	B
of	I
error	I
or	O
that	O
can	O
achieve	O
arbitrarily	O
small	O
probability	B
of	I
error	I
only	O
by	O
decreasing	O
the	O
information	B
rate	B
to	O
zero	O
repetition	B
codes	O
are	O
an	O
example	O
of	O
a	O
bad	B
code	O
family	O
codes	O
are	O
not	O
necessarily	O
useless	O
for	O
practical	B
purposes	O
practical	B
codes	O
are	O
code	O
families	O
that	O
can	O
be	O
encoded	O
and	O
decoded	O
in	O
time	O
and	O
space	O
polynomial	O
in	O
the	O
blocklength	O
most	O
established	O
codes	O
are	O
linear	B
codes	I
let	O
us	O
review	O
the	O
of	O
a	O
block	B
code	I
and	O
then	O
add	O
the	O
of	O
a	O
linear	B
block	B
code	I
an	O
k	O
block	B
code	I
for	O
a	O
channel	O
q	O
is	O
a	O
list	O
of	O
s	O
codewords	O
each	O
of	O
length	O
n	O
xs	O
an	O
x	O
the	O
signal	O
to	O
be	O
encoded	O
s	O
which	O
comes	O
from	O
an	O
alphabet	O
of	O
size	O
is	O
encoded	O
as	O
xs	O
a	O
linear	B
k	O
block	B
code	I
is	O
a	O
block	B
code	I
in	O
which	O
the	O
codewords	O
fxsg	O
make	O
up	O
a	O
k-dimensional	O
subspace	O
of	O
an	O
x	O
the	O
encoding	O
operation	O
can	O
be	O
represented	O
by	O
an	O
n	O
k	O
binary	O
matrix	B
gt	O
such	O
that	O
if	O
the	O
signal	O
to	O
be	O
encoded	O
in	O
binary	O
notation	B
is	O
s	O
vector	O
of	O
length	O
k	O
bits	O
then	O
the	O
encoded	O
signal	O
is	O
t	O
gts	O
modulo	O
the	O
codewords	O
ftg	O
can	O
be	O
as	O
the	O
set	B
of	O
vectors	B
satisfying	O
ht	O
mod	O
where	O
h	O
is	O
the	O
parity-check	B
matrix	B
of	O
the	O
code	O
for	O
example	O
the	O
hamming	B
code	I
of	O
section	B
takes	O
k	O
signal	O
bits	O
s	O
and	O
transmits	O
them	O
followed	O
by	O
three	O
parity-check	B
bits	I
the	O
n	O
transmitted	O
symbols	O
are	O
given	O
by	O
gts	O
mod	O
coding	B
theory	I
was	O
born	O
with	O
the	O
work	O
of	O
hamming	B
who	O
invented	O
a	O
family	O
of	O
practical	B
error-correcting	B
codes	I
each	O
able	O
to	O
correct	O
one	O
error	O
in	O
a	O
block	B
of	O
length	O
n	O
of	O
which	O
the	O
repetition	B
code	I
and	O
the	O
code	O
are	O
gt	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
and	O
real	O
channels	O
the	O
simplest	O
since	O
then	O
most	O
established	O
codes	O
have	O
been	O
generalizations	O
of	O
hamming	B
s	O
codes	O
bosechaudhuryhocquenhem	O
codes	O
codes	O
reedsolomon	O
codes	O
and	O
goppa	O
codes	O
to	O
name	O
a	O
few	O
convolutional	B
codes	O
another	O
family	O
of	O
linear	B
codes	I
are	O
convolutional	B
codes	O
which	O
do	O
not	O
divide	O
the	O
source	O
stream	O
into	O
blocks	O
but	O
instead	O
read	O
and	O
transmit	O
bits	O
continuously	O
the	O
transmitted	O
bits	O
are	O
a	O
linear	B
function	O
of	O
the	O
past	O
source	O
bits	O
usually	O
the	O
rule	O
for	O
generating	O
the	O
transmitted	O
bits	O
involves	O
feeding	O
the	O
present	O
source	O
bit	B
into	O
a	O
linear-feedback	B
shift-register	I
of	O
length	O
k	O
and	O
transmitting	O
one	O
or	O
more	O
linear	B
functions	B
of	O
the	O
state	O
of	O
the	O
shift	O
register	O
at	O
each	O
iteration	O
the	O
resulting	O
transmitted	O
bit	B
stream	O
is	O
the	O
convolution	B
of	O
the	O
source	O
stream	O
with	O
a	O
linear	B
the	O
impulse-response	O
function	O
of	O
this	O
may	O
have	O
or	O
duration	O
depending	O
on	O
the	O
choice	O
of	O
feedback	B
shift-register	O
we	O
will	O
discuss	O
convolutional	B
codes	O
in	O
chapter	O
are	O
linear	B
codes	I
good	B
one	O
might	O
ask	O
is	O
the	O
reason	O
that	O
the	O
shannon	B
limit	O
is	O
not	O
achieved	O
in	O
practice	O
because	O
linear	B
codes	I
are	O
inherently	O
not	O
as	O
good	B
as	O
random	B
codes	O
the	O
answer	O
is	O
no	O
the	O
noisy-channel	B
coding	B
theorem	I
can	O
still	O
be	O
proved	O
for	O
linear	B
codes	I
at	O
least	O
for	O
some	O
channels	O
chapter	O
though	O
the	O
proofs	O
like	O
shannon	B
s	O
proof	O
for	O
random	B
codes	O
are	O
non-constructive	O
linear	B
codes	I
are	O
easy	O
to	O
implement	O
at	O
the	O
encoding	O
end	O
is	O
decoding	B
a	O
linear	B
code	O
also	O
easy	O
not	O
necessarily	O
the	O
general	O
decoding	B
problem	O
the	O
maximum	B
likelihood	B
s	O
in	O
the	O
equation	O
gts	O
n	O
r	O
is	O
in	O
fact	O
np-complete	B
et	O
al	O
problems	O
are	O
computational	O
problems	O
that	O
are	O
all	O
equally	O
and	O
which	O
are	O
widely	O
believed	O
to	O
require	O
exponential	B
computer	B
time	O
to	O
solve	O
in	O
general	O
so	O
attention	O
focuses	O
on	O
families	O
of	O
codes	O
for	O
which	O
there	O
is	O
a	O
fast	O
decoding	B
algorithm	O
concatenation	B
one	O
trick	O
for	O
building	O
codes	O
with	O
practical	B
decoders	O
is	O
the	O
idea	O
of	O
concatenation	B
an	O
encoderchanneldecoder	O
system	O
c	O
q	O
d	O
can	O
be	O
viewed	O
as	O
a	O
super-channel	B
with	O
a	O
smaller	O
probability	B
of	I
error	I
and	O
with	O
complex	B
correlations	B
among	O
its	O
errors	B
we	O
can	O
create	O
an	O
encoder	B
and	O
decoder	B
for	O
this	O
super-channel	B
the	O
code	O
consisting	O
of	O
the	O
outer	B
code	I
followed	O
by	O
the	O
inner	B
code	I
c	O
is	O
known	O
as	O
a	O
concatenated	B
code	O
some	O
concatenated	B
codes	O
make	O
use	O
of	O
the	O
idea	O
of	O
interleaving	B
we	O
read	O
the	O
data	O
in	O
blocks	O
the	O
size	O
of	O
each	O
block	B
being	O
larger	O
than	O
the	O
blocklengths	O
of	O
the	O
constituent	O
codes	O
c	O
and	O
after	O
encoding	O
the	O
data	O
of	O
one	O
block	B
using	O
code	O
the	O
bits	O
are	O
reordered	O
within	O
the	O
block	B
in	O
such	O
a	O
way	O
that	O
nearby	O
bits	O
are	O
separated	O
from	O
each	O
other	O
once	O
the	O
block	B
is	O
fed	O
to	O
the	O
second	O
code	O
c	O
a	O
simple	O
example	O
of	O
an	O
interleaver	O
is	O
a	O
rectangular	B
code	I
or	O
product	B
code	I
in	O
which	O
the	O
data	O
are	O
arranged	O
in	O
a	O
block	B
and	O
encoded	O
horizontally	O
using	O
an	O
linear	B
code	O
then	O
vertically	O
using	O
a	O
linear	B
code	O
exercise	O
show	O
that	O
either	O
of	O
the	O
two	O
codes	O
can	O
be	O
viewed	O
as	O
the	O
inner	B
code	I
or	O
the	O
outer	B
code	I
as	O
an	O
example	O
shows	O
a	O
product	B
code	I
in	O
which	O
we	O
encode	O
with	O
the	O
repetition	B
code	I
known	O
as	O
the	O
hamming	B
code	I
c	O
q	O
d	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
what	O
are	O
the	O
capabilities	O
of	O
practical	B
error-correcting	B
codes	I
figure	O
a	O
product	B
code	I
a	O
string	O
encoded	O
using	O
a	O
concatenated	B
code	O
consisting	O
of	O
two	O
hamming	B
codes	O
and	O
a	O
noise	O
pattern	O
that	O
bits	O
the	O
received	O
vector	O
after	O
decoding	B
using	O
the	O
horizontal	O
decoder	B
and	O
after	O
subsequently	O
using	O
the	O
vertical	O
decoder	B
the	O
decoded	O
vector	O
matches	O
the	O
original	O
after	O
decoding	B
in	O
the	O
other	O
order	O
three	O
errors	B
still	O
remain	O
horizontally	O
then	O
with	O
vertically	O
the	O
blocklength	O
of	O
the	O
concatenated	B
code	O
is	O
the	O
number	O
of	O
source	O
bits	O
per	O
codeword	B
is	O
four	O
shown	O
by	O
the	O
small	O
rectangle	O
we	O
can	O
decode	O
conveniently	O
not	O
optimally	O
by	O
using	O
the	O
individual	O
decoders	O
for	O
each	O
of	O
the	O
subcodes	O
in	O
some	O
sequence	B
it	O
makes	O
most	O
sense	O
to	O
decode	O
the	O
code	O
which	O
has	O
the	O
lowest	O
rate	B
and	O
hence	O
the	O
greatest	O
errorcorrecting	O
ability	O
figure	O
shows	O
what	O
happens	O
if	O
we	O
receive	O
the	O
codeword	B
of	O
with	O
some	O
errors	B
bits	O
as	O
shown	O
and	O
apply	O
the	O
decoder	B
for	O
and	O
then	O
the	O
decoder	B
for	O
the	O
decoder	B
corrects	O
three	O
of	O
the	O
errors	B
but	O
erroneously	O
the	O
third	O
bit	B
in	O
the	O
second	O
row	O
where	O
there	O
are	O
two	O
bit	B
errors	B
the	O
decoder	B
can	O
then	O
correct	O
all	O
three	O
of	O
these	O
errors	B
figure	O
shows	O
what	O
happens	O
if	O
we	O
decode	O
the	O
two	O
codes	O
in	O
the	O
other	O
order	O
in	O
columns	O
one	O
and	O
two	O
there	O
are	O
two	O
errors	B
so	O
the	O
decoder	B
introduces	O
two	O
extra	O
errors	B
it	O
corrects	O
the	O
one	O
error	O
in	O
column	O
the	O
decoder	B
then	O
cleans	O
up	O
four	O
of	O
the	O
errors	B
but	O
erroneously	O
infers	O
the	O
second	O
bit	B
interleaving	B
the	O
motivation	O
for	O
interleaving	B
is	O
that	O
by	O
spreading	O
out	O
bits	O
that	O
are	O
nearby	O
in	O
one	O
code	O
we	O
make	O
it	O
possible	O
to	O
ignore	O
the	O
complex	B
correlations	B
among	O
the	O
errors	B
that	O
are	O
produced	O
by	O
the	O
inner	B
code	I
maybe	O
the	O
inner	B
code	I
will	O
mess	O
up	O
an	O
entire	O
codeword	B
but	O
that	O
codeword	B
is	O
spread	O
out	O
one	O
bit	B
at	O
a	O
time	O
over	O
several	O
codewords	O
of	O
the	O
outer	B
code	I
so	O
we	O
can	O
treat	O
the	O
errors	B
introduced	O
by	O
the	O
inner	B
code	I
as	O
if	O
they	O
are	O
independent	O
other	O
channel	O
models	O
in	O
addition	O
to	O
the	O
binary	B
symmetric	B
channel	I
and	O
the	O
gaussian	B
channel	I
coding	O
theorists	O
keep	O
more	O
complex	B
channels	O
in	O
mind	O
also	O
burst-error	O
channels	O
are	O
important	O
models	O
in	O
practice	O
reedsolomon	O
codes	O
use	O
galois	O
appendix	O
with	O
large	O
numbers	O
of	O
elements	O
as	O
their	O
input	O
alphabets	O
and	O
thereby	O
automatically	O
achieve	O
a	O
degree	B
of	O
burst-error	O
tolerance	O
in	O
that	O
even	O
if	O
successive	O
bits	O
are	O
corrupted	O
only	O
successive	O
symbols	O
in	O
the	O
galois	O
representation	O
are	O
corrupted	O
concatenation	B
and	O
interleaving	B
can	O
give	O
further	O
protection	O
against	O
burst	B
errors	B
the	O
concatenated	B
reedsolomon	O
codes	O
used	O
on	O
digital	O
compact	O
discs	O
are	O
able	O
to	O
correct	O
bursts	O
of	O
errors	B
of	O
length	O
bits	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
and	O
real	O
channels	O
exercise	O
the	O
technique	O
of	O
interleaving	B
which	O
allows	O
bursts	O
of	O
errors	B
to	O
be	O
treated	O
as	O
independent	O
is	O
widely	O
used	O
but	O
is	O
theoretically	O
a	O
poor	O
way	O
to	O
protect	O
data	O
against	O
burst	B
errors	B
in	O
terms	O
of	O
the	O
amount	O
of	O
redundancy	B
required	O
explain	O
why	O
interleaving	B
is	O
a	O
poor	O
method	O
using	O
the	O
following	O
burst-error	O
channel	O
as	O
an	O
example	O
time	O
is	O
divided	O
into	O
chunks	O
of	O
length	O
n	O
clock	O
cycles	O
during	O
each	O
chunk	O
there	O
is	O
a	O
burst	O
with	O
probability	B
b	O
during	O
a	O
burst	O
the	O
channel	O
is	O
a	O
binary	B
symmetric	B
channel	I
with	O
f	O
if	O
there	O
is	O
no	O
burst	O
the	O
channel	O
is	O
an	O
error-free	O
binary	O
channel	O
compute	O
the	O
capacity	B
of	O
this	O
channel	O
and	O
compare	O
it	O
with	O
the	O
maximum	O
communication	B
rate	B
that	O
could	O
conceivably	O
be	O
achieved	O
if	O
one	O
used	O
interleaving	B
and	O
treated	O
the	O
errors	B
as	O
independent	O
fading	B
channels	O
are	O
real	O
channels	O
like	O
gaussian	B
channels	O
except	O
that	O
the	O
received	O
power	O
is	O
assumed	O
to	O
vary	O
with	O
time	O
a	O
moving	O
mobile	B
phone	B
is	O
an	O
important	O
example	O
the	O
incoming	O
radio	B
signal	O
is	O
nearby	O
objects	O
so	O
that	O
there	O
are	O
interference	O
patterns	O
and	O
the	O
intensity	O
of	O
the	O
signal	O
received	O
by	O
the	O
phone	B
varies	O
with	O
its	O
location	O
the	O
received	O
power	O
can	O
easily	O
vary	O
by	O
decibels	O
factor	O
of	O
ten	O
as	O
the	O
phone	B
s	O
antenna	O
moves	O
through	O
a	O
distance	B
similar	O
to	O
the	O
wavelength	O
of	O
the	O
radio	B
signal	O
few	O
centimetres	O
the	O
state	O
of	O
the	O
art	O
what	O
are	O
the	O
best	O
known	O
codes	O
for	O
communicating	O
over	O
gaussian	B
channels	O
all	O
the	O
practical	B
codes	O
are	O
linear	B
codes	I
and	O
are	O
either	O
based	O
on	O
convolutional	B
codes	O
or	O
block	B
codes	O
convolutional	B
codes	O
and	O
codes	O
based	O
on	O
them	O
textbook	O
convolutional	B
codes	O
the	O
de	O
facto	O
standard	O
error-correcting	B
code	I
for	O
satellite	B
communications	I
is	O
a	O
convolutional	B
code	I
with	O
constraint	O
length	O
convolutional	B
codes	O
are	O
discussed	O
in	O
chapter	O
concatenated	B
convolutional	B
codes	O
the	O
above	O
convolutional	B
code	I
can	O
be	O
used	O
as	O
the	O
inner	B
code	I
of	O
a	O
concatenated	B
code	O
whose	O
outer	B
code	I
is	O
a	O
reed	O
solomon	O
code	O
with	O
eight-bit	O
symbols	O
this	O
code	O
was	O
used	O
in	O
deep	O
space	O
communication	B
systems	O
such	O
as	O
the	O
voyager	O
spacecraft	O
for	O
further	O
reading	O
about	O
reedsolomon	O
codes	O
see	O
lin	O
and	O
costello	O
the	O
code	O
for	O
galileo	O
a	O
code	O
using	O
the	O
same	O
format	O
but	O
using	O
a	O
longer	O
constraint	O
length	O
for	O
its	O
convolutional	B
code	I
and	O
a	O
larger	O
reed	O
solomon	O
code	O
was	O
developed	O
by	O
the	O
jet	B
propulsion	I
laboratory	I
the	O
details	O
of	O
this	O
code	O
are	O
unpublished	O
outside	O
jpl	O
and	O
the	O
decoding	B
is	O
only	O
possible	O
using	O
a	O
room	O
full	O
of	O
special-purpose	O
hardware	O
in	O
this	O
was	O
the	O
best	O
code	O
known	O
of	O
rate	B
turbo	B
codes	I
in	O
berrou	B
glavieux	B
and	O
thitimajshima	B
reported	O
work	O
on	O
turbo	B
codes	I
the	O
encoder	B
of	O
a	O
turbo	B
code	I
is	O
based	O
on	O
the	O
encoders	O
of	O
two	O
convolutional	B
codes	O
the	O
source	O
bits	O
are	O
fed	O
into	O
each	O
encoder	B
the	O
order	O
of	O
the	O
source	O
bits	O
being	O
permuted	O
in	O
a	O
random	B
way	O
and	O
the	O
resulting	O
parity	B
bits	O
from	O
each	O
constituent	O
code	O
are	O
transmitted	O
the	O
decoding	B
algorithm	O
involves	O
iteratively	O
decoding	B
each	O
constituent	O
code	O
using	O
its	O
standard	O
decoding	B
algorithm	O
then	O
using	O
the	O
output	O
of	O
the	O
decoder	B
as	O
the	O
input	O
to	O
the	O
other	O
decoder	B
this	O
decoding	B
algorithm	O
figure	O
the	O
encoder	B
of	O
a	O
turbo	B
code	I
each	O
box	B
contains	O
a	O
convolutional	B
code	I
the	O
source	O
bits	O
are	O
reordered	O
using	O
a	O
permutation	B
before	O
they	O
are	O
fed	O
to	O
the	O
transmitted	O
codeword	B
is	O
obtained	O
by	O
concatenating	O
or	O
interleaving	B
the	O
outputs	O
of	O
the	O
two	O
convolutional	B
codes	O
the	O
random	B
permutation	B
is	O
chosen	O
when	O
the	O
code	O
is	O
designed	O
and	O
thereafter	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
summary	B
is	O
an	O
instance	O
of	O
a	O
message-passing	B
algorithm	O
called	O
the	O
sumproduct	O
algorithm	O
turbo	B
codes	I
are	O
discussed	O
in	O
chapter	O
and	O
message	B
passing	I
in	O
chapters	O
and	O
h	O
block	B
codes	O
gallager	B
s	O
low-density	B
parity-check	I
codes	O
the	O
best	O
block	B
codes	O
known	O
for	O
gaussian	B
channels	O
were	O
invented	O
by	O
gallager	B
in	O
but	O
were	O
promptly	O
forgotten	O
by	O
most	O
of	O
the	O
coding	B
theory	I
community	O
they	O
were	O
rediscovered	O
in	O
and	O
shown	O
to	O
have	O
outstanding	O
theoretical	O
and	O
practical	B
properties	O
like	O
turbo	B
codes	I
they	O
are	O
decoded	O
by	O
message-passing	B
algorithms	B
we	O
will	O
discuss	O
these	O
beautifully	O
simple	O
codes	O
in	O
chapter	O
the	O
performances	O
of	O
the	O
above	O
codes	O
are	O
compared	O
for	O
gaussian	B
channels	O
in	O
summary	B
random	B
codes	O
are	O
good	B
but	O
they	O
require	O
exponential	B
resources	O
to	O
encode	O
and	O
decode	O
them	O
non-random	O
codes	O
tend	O
for	O
the	O
most	O
part	O
not	O
to	O
be	O
as	O
good	B
as	O
random	B
codes	O
for	O
a	O
non-random	O
code	O
encoding	O
may	O
be	O
easy	O
but	O
even	O
for	O
linear	B
codes	I
the	O
decoding	B
problem	O
remains	O
very	O
the	O
best	O
practical	B
codes	O
employ	O
very	O
large	O
block	B
sizes	O
are	O
based	O
on	O
semi-random	O
code	O
constructions	O
and	O
make	O
use	O
of	O
probabilitybased	O
decoding	B
algorithms	B
nonlinear	B
codes	O
figure	O
a	O
low-density	B
parity-check	B
matrix	B
and	O
the	O
corresponding	O
graph	B
of	O
a	O
low-density	B
parity-check	B
code	I
with	O
blocklength	O
n	O
and	O
m	O
constraints	O
each	O
white	B
circle	B
represents	O
a	O
transmitted	O
bit	B
each	O
bit	B
participates	O
in	O
j	O
constraints	O
represented	O
by	O
squares	O
each	O
constraint	O
forces	O
the	O
sum	O
of	O
the	O
k	O
bits	O
to	O
which	O
it	O
is	O
connected	O
to	O
be	O
even	O
this	O
code	O
is	O
a	O
code	O
outstanding	O
performance	O
is	O
obtained	O
when	O
the	O
blocklength	O
is	O
increased	O
to	O
n	O
most	O
practically	O
used	O
codes	O
are	O
linear	B
but	O
not	O
all	O
digital	O
soundtracks	O
are	O
encoded	O
onto	O
cinema	B
as	O
a	O
binary	O
pattern	O
the	O
likely	O
errors	B
the	O
involve	O
dirt	O
and	O
scratches	O
which	O
produce	O
large	O
numbers	O
of	O
and	O
respectively	O
we	O
want	O
none	O
of	O
the	O
codewords	O
to	O
look	O
like	O
or	O
so	O
that	O
it	O
will	O
be	O
easy	O
to	O
detect	O
errors	B
caused	O
by	O
dirt	O
and	O
scratches	O
one	O
of	O
the	O
codes	O
used	O
in	O
digital	B
cinema	B
sound	B
systems	O
is	O
a	O
nonlinear	B
code	I
consisting	O
of	O
of	O
the	O
binary	O
patterns	O
of	O
weight	O
errors	B
other	O
than	O
noise	O
another	O
source	O
of	O
uncertainty	O
for	O
the	O
receiver	O
is	O
uncertainty	O
about	O
the	O
timing	B
of	O
the	O
transmitted	O
signal	O
xt	O
in	O
ordinary	O
coding	B
theory	I
and	O
information	B
theory	I
the	O
transmitter	O
s	O
time	O
t	O
and	O
the	O
receiver	O
s	O
time	O
u	O
are	O
assumed	O
to	O
be	O
perfectly	O
synchronized	O
but	O
if	O
the	O
receiver	O
receives	O
a	O
signal	O
yu	O
where	O
the	O
receiver	O
s	O
time	O
u	O
is	O
an	O
imperfectly	O
known	O
function	O
ut	O
of	O
the	O
transmitter	O
s	O
time	O
t	O
then	O
the	O
capacity	B
of	O
this	O
channel	O
for	O
communication	B
is	O
reduced	O
the	O
theory	O
of	O
such	O
channels	O
is	O
incomplete	O
compared	O
with	O
the	O
synchronized	O
channels	O
we	O
have	O
discussed	O
thus	O
far	O
not	O
even	O
the	O
capacity	B
of	O
channels	O
with	O
synchronization	B
errors	B
is	O
known	O
ferreira	O
et	O
al	O
codes	O
for	O
reliable	O
communication	B
over	O
channels	O
with	O
synchronization	B
errors	B
remain	O
an	O
active	O
research	O
area	O
and	O
mackay	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
reading	O
error-correcting	B
codes	I
and	O
real	O
channels	O
for	O
a	O
review	O
of	O
the	O
history	O
of	O
spread-spectrum	O
methods	O
see	O
scholtz	O
exercises	O
the	O
gaussian	B
channel	I
exercise	O
consider	O
a	O
gaussian	B
channel	I
with	O
a	O
real	O
input	O
x	O
and	O
signal	O
to	O
noise	O
ratio	O
what	O
is	O
its	O
capacity	B
c	O
if	O
the	O
input	O
is	O
constrained	B
to	O
be	O
binary	O
x	O
what	O
is	O
the	O
capacity	B
of	O
this	O
constrained	B
channel	I
if	O
in	O
addition	O
the	O
output	O
of	O
the	O
channel	O
is	O
thresholded	O
using	O
the	O
mapping	B
y	O
y	O
y	O
what	O
is	O
the	O
capacity	B
of	O
the	O
resulting	O
channel	O
plot	O
the	O
three	O
capacities	O
above	O
as	O
a	O
function	O
of	O
from	O
to	O
ll	O
need	O
to	O
do	O
a	O
numerical	O
integral	B
to	O
evaluate	O
exercise	O
for	O
large	O
integers	O
k	O
and	O
n	O
what	O
fraction	O
of	O
all	O
binary	O
errorcorrecting	O
codes	O
of	O
length	O
n	O
and	O
rate	B
r	O
kn	O
are	O
linear	B
codes	I
answer	O
will	O
depend	O
on	O
whether	O
you	O
choose	O
to	O
the	O
code	O
to	O
be	O
an	O
ordered	O
list	O
of	O
codewords	O
that	O
is	O
a	O
mapping	B
from	O
s	O
to	O
xs	O
or	O
to	O
the	O
code	O
to	O
be	O
an	O
unordered	O
list	O
so	O
that	O
two	O
codes	O
consisting	O
of	O
the	O
same	O
codewords	O
are	O
identical	O
use	O
the	O
latter	O
a	O
code	O
is	O
a	O
set	B
of	O
codewords	O
how	O
the	O
encoder	B
operates	O
is	O
not	O
part	O
of	O
the	O
of	O
the	O
code	O
erasure	B
channels	O
exercise	O
design	O
a	O
code	O
for	O
the	O
binary	B
erasure	B
channel	I
and	O
a	O
decoding	B
algorithm	O
and	O
evaluate	O
their	O
probability	B
of	I
error	I
design	O
of	O
good	B
codes	O
for	O
erasure	B
channels	O
is	O
an	O
active	O
research	O
area	O
byers	O
et	O
al	O
see	O
also	O
chapter	O
exercise	O
design	O
a	O
code	O
for	O
the	O
q-ary	O
erasure	B
channel	I
whose	O
input	O
x	O
is	O
drawn	O
from	O
and	O
whose	O
output	O
y	O
is	O
equal	O
to	O
x	O
with	O
probability	B
f	O
and	O
equal	O
to	O
otherwise	O
erasure	B
channel	I
is	O
a	O
good	B
model	B
for	O
packets	O
transmitted	O
over	O
the	O
internet	B
which	O
are	O
either	O
received	O
reliably	O
or	O
are	O
lost	O
exercise	O
how	O
do	O
redundant	O
arrays	O
of	O
independent	O
disks	O
work	O
these	O
are	O
information	B
storage	O
systems	O
consisting	O
of	O
about	O
ten	O
disk	O
drives	O
of	O
which	O
any	O
two	O
or	O
three	O
can	O
be	O
disabled	O
and	O
the	O
others	O
are	O
able	O
to	O
still	O
able	O
to	O
reconstruct	O
any	O
requested	O
what	O
codes	O
are	O
used	O
and	O
how	O
far	O
are	O
these	O
systems	O
from	O
the	O
shannon	B
limit	O
for	O
the	O
problem	O
they	O
are	O
solving	O
how	O
would	O
you	O
design	O
a	O
better	O
raid	B
system	O
some	O
information	B
is	O
provided	O
in	O
the	O
solution	O
section	B
see	O
httpwww	O
acnc	O
see	O
also	O
chapter	O
people	O
say	O
raid	B
stands	O
for	O
redundant	O
array	O
of	O
inexpensive	O
disks	O
but	O
i	O
think	O
that	O
s	O
silly	O
raid	B
would	O
still	O
be	O
a	O
good	B
idea	O
even	O
if	O
the	O
disks	O
were	O
expensive	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solutions	O
solution	O
to	O
exercise	O
introduce	O
a	O
lagrange	B
multiplier	I
for	O
the	O
power	O
constraint	O
and	O
another	O
for	O
the	O
constraint	O
of	O
normalization	O
of	O
p	O
f	O
ix	O
y	O
dx	O
p	O
dx	O
p	O
z	O
dx	O
p	O
dy	O
p	O
j	O
x	O
ln	O
p	O
p	O
j	O
x	O
make	O
the	O
functional	O
derivative	O
with	O
respect	O
to	O
p	O
z	O
dy	O
p	O
j	O
ln	O
p	O
j	O
p	O
dx	O
p	O
dy	O
p	O
j	O
x	O
p	O
the	O
factor	O
is	O
found	O
using	O
p	O
dx	O
p	O
j	O
x	O
to	O
be	O
p	O
j	O
and	O
the	O
whole	O
of	O
the	O
last	O
term	O
collapses	O
in	O
a	O
of	O
smoke	O
to	O
which	O
can	O
be	O
absorbed	O
into	O
the	O
term	O
substitute	O
p	O
j	O
x	O
and	O
set	B
the	O
derivative	O
to	O
zero	O
p	O
j	O
x	O
p	O
ln	O
this	O
condition	O
must	O
be	O
by	O
lnp	O
for	O
all	O
x	O
dy	O
writing	B
a	O
taylor	O
expansion	O
of	O
lnp	O
abycy	O
only	O
a	O
quadratic	O
function	O
lnp	O
a	O
would	O
satisfy	O
the	O
constraint	O
higher	O
order	O
terms	O
yp	O
p	O
would	O
produce	O
terms	O
in	O
xp	O
that	O
are	O
not	O
present	O
on	O
the	O
right-hand	O
side	O
therefore	O
p	O
is	O
gaussian	B
we	O
can	O
obtain	O
this	O
optimal	B
output	O
distribution	B
by	O
using	O
a	O
gaussian	B
input	O
distribution	B
p	O
z	O
dy	O
p	O
j	O
x	O
ln	O
solution	O
to	O
exercise	O
given	O
a	O
gaussian	B
input	O
distribution	B
of	O
variance	B
v	O
the	O
output	O
distribution	B
is	O
v	O
since	O
x	O
and	O
the	O
noise	O
are	O
independent	O
random	B
variables	O
and	O
variances	B
add	I
for	O
independent	O
random	B
variables	O
the	O
mutual	B
information	B
is	O
ix	O
y	O
z	O
dx	O
dy	O
p	O
j	O
x	O
log	O
p	O
j	O
x	O
dy	O
p	O
log	O
p	O
log	O
v	O
v	O
log	O
solution	O
to	O
exercise	O
the	O
capacity	B
of	O
the	O
channel	O
is	O
one	O
minus	O
the	O
information	B
content	I
of	O
the	O
noise	O
that	O
it	O
adds	O
that	O
information	B
content	I
is	O
per	O
chunk	O
the	O
entropy	B
of	O
the	O
selection	O
of	O
whether	O
the	O
chunk	O
is	O
bursty	B
plus	O
with	O
probability	B
b	O
the	O
entropy	B
of	O
the	O
bits	O
n	O
which	O
adds	O
up	O
to	O
n	O
b	O
per	O
chunk	O
accurate	O
if	O
n	O
is	O
large	O
so	O
per	O
bit	B
the	O
capacity	B
is	O
for	O
n	O
c	O
n	O
in	O
contrast	O
interleaving	B
which	O
treats	O
bursts	O
of	O
errors	B
as	O
independent	O
causes	O
the	O
channel	O
to	O
be	O
treated	O
as	O
a	O
binary	B
symmetric	B
channel	I
with	O
f	O
whose	O
capacity	B
is	O
about	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
error-correcting	B
codes	I
and	O
real	O
channels	O
interleaving	B
throws	O
away	O
the	O
useful	O
information	B
about	O
the	O
correlatedness	O
of	O
the	O
errors	B
theoretically	O
we	O
should	O
be	O
able	O
to	O
communicate	O
about	O
times	O
faster	O
using	O
a	O
code	O
and	O
decoder	B
that	O
explicitly	O
treat	O
bursts	O
as	O
bursts	O
solution	O
to	O
exercise	O
figure	O
capacities	O
top	O
to	O
bottom	O
in	O
each	O
graph	B
c	O
c	O
and	O
c	O
versus	O
the	O
signal-to-noise	B
ratio	I
the	O
lower	O
graph	B
is	O
a	O
loglog	O
plot	O
c	O
v	O
putting	O
together	O
the	O
results	O
of	O
exercises	O
and	O
we	O
deduce	O
that	O
a	O
gaussian	B
channel	I
with	O
real	O
input	O
x	O
and	O
signal	O
to	O
noise	O
ratio	O
has	O
capacity	B
if	O
the	O
input	O
is	O
constrained	B
to	O
be	O
binary	O
x	O
the	O
capacity	B
is	O
achieved	O
by	O
using	O
these	O
two	O
inputs	O
with	O
equal	O
probability	B
the	O
capacity	B
is	O
reduced	O
to	O
a	O
somewhat	O
messy	O
integral	B
dy	O
n	O
log	O
n	O
dy	O
p	O
log	O
p	O
where	O
n	O
x	O
expy	O
x	O
and	O
p	O
x	O
n	O
this	O
capacity	B
is	O
smaller	O
than	O
the	O
unconstrained	O
capacity	B
but	O
for	O
small	O
signal-to-noise	B
ratio	I
the	O
two	O
capacities	O
are	O
close	O
in	O
value	O
if	O
the	O
output	O
is	O
thresholded	O
then	O
the	O
gaussian	B
channel	I
is	O
turned	O
into	O
a	O
binary	B
symmetric	B
channel	I
whose	O
transition	B
probability	B
is	O
given	O
by	O
the	O
error	B
function	I
on	O
page	O
the	O
capacity	B
is	O
where	O
f	O
solution	O
to	O
exercise	O
there	O
are	O
several	O
raid	B
systems	O
one	O
of	O
the	O
easiest	O
to	O
understand	O
consists	O
of	O
disk	O
drives	O
which	O
store	O
data	O
at	O
rate	B
using	O
a	O
hamming	B
code	I
each	O
successive	O
four	O
bits	O
are	O
encoded	O
with	O
the	O
code	O
and	O
the	O
seven	O
codeword	B
bits	O
are	O
written	O
one	O
to	O
each	O
disk	O
two	O
or	O
perhaps	O
three	O
disk	O
drives	O
can	O
go	O
down	O
and	O
the	O
others	O
can	O
recover	O
the	O
data	O
the	O
channel	O
model	B
here	O
is	O
a	O
binary	B
erasure	B
channel	I
because	O
it	O
is	O
assumed	O
that	O
we	O
can	O
tell	O
when	O
a	O
disk	O
is	O
dead	O
it	O
is	O
not	O
possible	O
to	O
recover	O
the	O
data	O
for	O
some	O
choices	O
of	O
the	O
three	O
dead	O
disk	O
drives	O
can	O
you	O
see	O
why	O
exercise	O
give	O
an	O
example	O
of	O
three	O
disk	O
drives	O
that	O
if	O
lost	O
lead	O
to	O
failure	O
of	O
the	O
above	O
raid	B
system	O
and	O
three	O
that	O
can	O
be	O
lost	O
without	O
failure	O
solution	O
to	O
exercise	O
the	O
hamming	B
code	I
has	O
codewords	O
of	O
weight	O
if	O
any	O
set	B
of	O
three	O
disk	O
drives	O
corresponding	O
to	O
one	O
of	O
those	O
codewords	O
is	O
lost	O
then	O
the	O
other	O
four	O
disks	O
can	O
recover	O
only	O
bits	O
of	O
information	B
about	O
the	O
four	O
source	O
bits	O
a	O
fourth	O
bit	B
is	O
lost	O
exercise	O
with	O
q	O
there	O
are	O
no	O
binary	O
mds	B
codes	O
this	O
is	O
discussed	O
further	O
in	O
section	B
any	O
other	O
set	B
of	O
three	O
disk	O
drives	O
can	O
be	O
lost	O
without	O
problems	O
because	O
the	O
corresponding	O
four	O
by	O
four	O
submatrix	O
of	O
the	O
generator	B
matrix	B
is	O
invertible	O
a	O
better	O
code	O
would	O
be	O
a	O
digital	B
fountain	I
see	O
chapter	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
part	O
iii	O
further	O
topics	O
in	O
information	B
theory	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
in	O
chapters	O
we	O
concentrated	O
on	O
two	O
aspects	O
of	O
information	B
theory	I
and	O
coding	B
theory	I
source	O
coding	O
the	O
compression	B
of	O
information	B
so	O
as	O
to	O
make	O
use	O
of	O
data	O
transmission	O
and	O
storage	O
channels	O
and	O
channel	O
coding	O
the	O
redundant	O
encoding	O
of	O
information	B
so	O
as	O
to	O
be	O
able	O
to	O
detect	O
and	O
correct	O
communication	B
errors	B
in	O
both	O
these	O
areas	O
we	O
started	O
by	O
ignoring	O
practical	B
considerations	O
concentrating	O
on	O
the	O
question	O
of	O
the	O
theoretical	O
limitations	O
and	O
possibilities	O
of	O
coding	O
we	O
then	O
discussed	O
practical	B
source-coding	O
and	O
channel-coding	O
schemes	O
shifting	O
the	O
emphasis	O
towards	O
computational	O
feasibility	O
but	O
the	O
prime	O
criterion	O
for	O
comparing	O
encoding	O
schemes	O
remained	O
the	O
of	O
the	O
code	O
in	O
terms	O
of	O
the	O
channel	O
resources	O
it	O
required	O
the	O
best	O
source	O
codes	O
were	O
those	O
that	O
achieved	O
the	O
greatest	O
compression	B
the	O
best	O
channel	O
codes	O
were	O
those	O
that	O
communicated	O
at	O
the	O
highest	O
rate	B
with	O
a	O
given	O
probability	B
of	I
error	I
in	O
this	O
chapter	O
we	O
now	O
shift	O
our	O
viewpoint	O
a	O
little	O
thinking	O
of	O
ease	O
of	O
information	B
retrieval	I
as	O
a	O
primary	O
goal	O
it	O
turns	O
out	O
that	O
the	O
random	B
codes	O
which	O
were	O
theoretically	O
useful	O
in	O
our	O
study	O
of	O
channel	O
coding	O
are	O
also	O
useful	O
for	O
rapid	O
information	B
retrieval	I
information	B
retrieval	I
is	O
one	O
of	O
the	O
problems	O
that	O
brains	O
seem	O
to	O
solve	O
and	O
content-addressable	B
memory	B
is	O
one	O
of	O
the	O
topics	O
we	O
will	O
study	O
when	O
we	O
look	O
at	O
neural	O
networks	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
string	O
length	O
number	O
of	O
strings	O
number	O
of	O
possible	O
strings	O
n	O
s	O
figure	O
cast	O
of	O
characters	O
hash	O
codes	O
codes	O
for	O
information	B
retrieval	I
the	O
information-retrieval	O
problem	O
a	O
simple	O
example	O
of	O
an	O
information-retrieval	O
problem	O
is	O
the	O
task	O
of	O
implementing	O
a	O
phone	B
directory	B
service	O
which	O
in	O
response	O
to	O
a	O
person	O
s	O
name	O
returns	O
a	O
that	O
that	O
person	O
is	O
listed	O
in	O
the	O
directory	B
and	O
the	O
person	O
s	O
phone	B
number	I
and	O
other	O
details	O
we	O
could	O
formalize	O
this	O
problem	O
as	O
follows	O
with	O
s	O
being	O
the	O
number	O
of	O
names	O
that	O
must	O
be	O
stored	O
in	O
the	O
directory	B
you	O
are	O
given	O
a	O
list	O
of	O
s	O
binary	O
strings	O
of	O
length	O
n	O
bits	O
xsg	O
where	O
s	O
is	O
considerably	O
smaller	O
than	O
the	O
total	O
number	O
of	O
possible	O
strings	O
we	O
will	O
call	O
the	O
superscript	O
s	O
in	O
xs	O
the	O
record	O
number	O
of	O
the	O
string	O
the	O
idea	O
is	O
that	O
s	O
runs	O
over	O
customers	O
in	O
the	O
order	O
in	O
which	O
they	O
are	O
added	O
to	O
the	O
directory	B
and	O
xs	O
is	O
the	O
name	O
of	O
customer	O
s	O
we	O
assume	O
for	O
simplicity	O
that	O
all	O
people	O
have	O
names	O
of	O
the	O
same	O
length	O
the	O
name	O
length	O
might	O
be	O
say	O
n	O
bits	O
and	O
we	O
might	O
want	O
to	O
store	O
the	O
details	O
of	O
ten	O
million	O
customers	O
so	O
s	O
we	O
will	O
ignore	O
the	O
possibility	O
that	O
two	O
customers	O
have	O
identical	O
names	O
the	O
task	O
is	O
to	O
construct	O
the	O
inverse	O
of	O
the	O
mapping	B
from	O
s	O
to	O
xs	O
i	O
e	O
to	O
make	O
a	O
system	O
that	O
given	O
a	O
string	O
x	O
returns	O
the	O
value	O
of	O
s	O
such	O
that	O
x	O
xs	O
if	O
one	O
exists	O
and	O
otherwise	O
reports	O
that	O
no	O
such	O
s	O
exists	O
we	O
have	O
the	O
record	O
number	O
we	O
can	O
go	O
and	O
look	O
in	O
memory	B
location	O
s	O
in	O
a	O
separate	O
memory	B
full	O
of	O
phone	B
numbers	O
to	O
the	O
required	O
number	O
the	O
aim	O
when	O
solving	O
this	O
task	O
is	O
to	O
use	O
minimal	O
computational	O
resources	O
in	O
terms	O
of	O
the	O
amount	O
of	O
memory	B
used	O
to	O
store	O
the	O
inverse	O
mapping	B
from	O
x	O
to	O
s	O
and	O
the	O
amount	O
of	O
time	O
to	O
compute	O
the	O
inverse	O
mapping	B
and	O
preferably	O
the	O
inverse	O
mapping	B
should	O
be	O
implemented	O
in	O
such	O
a	O
way	O
that	O
further	O
new	O
strings	O
can	O
be	O
added	O
to	O
the	O
directory	B
in	O
a	O
small	O
amount	O
of	O
computer	B
time	O
too	O
some	O
standard	O
solutions	O
the	O
simplest	O
and	O
dumbest	O
solutions	O
to	O
the	O
information-retrieval	O
problem	O
are	O
a	O
look-up	O
table	O
and	O
a	O
raw	O
list	O
the	O
look-up	O
table	O
is	O
a	O
piece	O
of	O
memory	B
of	O
size	O
s	O
s	O
being	O
the	O
amount	O
of	O
memory	B
required	O
to	O
store	O
an	O
integer	O
between	O
and	O
s	O
in	O
each	O
of	O
the	O
locations	O
we	O
put	O
a	O
zero	O
except	O
for	O
the	O
locations	O
x	O
that	O
correspond	O
to	O
strings	O
xs	O
into	O
which	O
we	O
write	O
the	O
value	O
of	O
s	O
the	O
look-up	O
table	O
is	O
a	O
simple	O
and	O
quick	O
solution	O
but	O
only	O
if	O
there	O
is	O
memory	B
for	O
the	O
table	O
and	O
if	O
the	O
cost	O
of	O
looking	O
up	O
entries	O
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
hash	O
codes	O
codes	O
for	O
information	B
retrieval	I
memory	B
is	O
independent	O
of	O
the	O
memory	B
size	O
but	O
in	O
our	O
of	O
the	O
task	O
we	O
assumed	O
that	O
n	O
is	O
about	O
bits	O
or	O
more	O
so	O
the	O
amount	O
of	O
memory	B
required	O
would	O
be	O
of	O
size	O
this	O
solution	O
is	O
completely	O
out	O
of	O
the	O
question	O
bear	O
in	O
mind	O
that	O
the	O
number	O
of	O
particles	O
in	O
the	O
solar	B
system	I
is	O
only	O
about	O
the	O
raw	O
list	O
is	O
a	O
simple	O
list	O
of	O
ordered	O
pairs	O
xs	O
ordered	O
by	O
the	O
value	O
of	O
s	O
the	O
mapping	B
from	O
x	O
to	O
s	O
is	O
achieved	O
by	O
searching	O
through	O
the	O
list	O
of	O
strings	O
starting	O
from	O
the	O
top	O
and	O
comparing	O
the	O
incoming	O
string	O
x	O
with	O
each	O
record	O
xs	O
until	O
a	O
match	O
is	O
found	O
this	O
system	O
is	O
very	O
easy	O
to	O
maintain	O
and	O
uses	O
a	O
small	O
amount	O
of	O
memory	B
about	O
sn	O
bits	O
but	O
is	O
rather	O
slow	O
to	O
use	O
since	O
on	O
average	O
million	O
pairwise	O
comparisons	O
will	O
be	O
made	O
exercise	O
show	O
that	O
the	O
average	O
time	O
taken	O
to	O
the	O
required	O
string	O
in	O
a	O
raw	O
list	O
assuming	O
that	O
the	O
original	O
names	O
were	O
chosen	O
at	O
random	B
is	O
about	O
s	O
n	O
binary	O
comparisons	O
that	O
you	O
don	O
t	O
have	O
to	O
compare	O
the	O
whole	O
string	O
of	O
length	O
n	O
since	O
a	O
comparison	O
can	O
be	O
terminated	O
as	O
soon	O
as	O
a	O
mismatch	O
occurs	O
show	O
that	O
you	O
need	O
on	O
average	O
two	O
binary	O
comparisons	O
per	O
incorrect	O
string	O
match	O
compare	O
this	O
with	O
the	O
worst-case	O
search	O
time	O
assuming	O
that	O
the	O
devil	O
chooses	O
the	O
set	B
of	O
strings	O
and	O
the	O
search	O
key	O
the	O
standard	O
way	O
in	O
which	O
phone	B
directories	O
are	O
made	O
improves	O
on	O
the	O
look-up	O
table	O
and	O
the	O
raw	O
list	O
by	O
using	O
an	O
alphabetically-ordered	O
list	O
alphabetical	O
list	O
the	O
strings	O
fxsg	O
are	O
sorted	O
into	O
alphabetical	O
order	O
searching	O
for	O
an	O
entry	O
now	O
usually	O
takes	O
less	O
time	O
than	O
was	O
needed	O
for	O
the	O
raw	O
list	O
because	O
we	O
can	O
take	O
advantage	O
of	O
the	O
sortedness	O
for	O
example	O
we	O
can	O
open	O
the	O
phonebook	O
at	O
its	O
middle	O
page	O
and	O
compare	O
the	O
name	O
we	O
there	O
with	O
the	O
target	O
string	O
if	O
the	O
target	O
is	O
greater	O
than	O
the	O
middle	O
string	O
then	O
we	O
know	O
that	O
the	O
required	O
string	O
if	O
it	O
exists	O
will	O
be	O
found	O
in	O
the	O
second	O
half	O
of	O
the	O
alphabetical	O
directory	B
otherwise	O
we	O
look	O
in	O
the	O
half	O
by	O
iterating	O
this	O
splitting-in-the-middle	O
procedure	O
we	O
can	O
identify	O
the	O
target	O
string	O
or	O
establish	O
that	O
the	O
string	O
is	O
not	O
listed	O
in	O
se	O
string	O
comparisons	O
the	O
expected	O
number	O
of	O
binary	O
comparisons	O
per	O
string	O
comparison	O
will	O
tend	O
to	O
increase	O
as	O
the	O
search	O
progresses	O
but	O
the	O
total	O
number	O
of	O
binary	O
comparisons	O
required	O
will	O
be	O
no	O
greater	O
than	O
sen	O
the	O
amount	O
of	O
memory	B
required	O
is	O
the	O
same	O
as	O
that	O
required	O
for	O
the	O
raw	O
list	O
adding	O
new	O
strings	O
to	O
the	O
database	O
requires	O
that	O
we	O
insert	O
them	O
in	O
the	O
correct	O
location	O
in	O
the	O
list	O
to	O
that	O
location	O
takes	O
about	O
dlog	O
se	O
binary	O
comparisons	O
can	O
we	O
improve	O
on	O
the	O
well-established	O
alphabetized	O
list	O
let	O
us	O
consider	O
our	O
task	O
from	O
some	O
new	O
viewpoints	O
the	O
task	O
is	O
to	O
construct	O
a	O
mapping	B
x	O
s	O
from	O
n	O
bits	O
to	O
log	O
s	O
bits	O
this	O
is	O
a	O
pseudo-invertible	O
mapping	B
since	O
for	O
any	O
x	O
that	O
maps	O
to	O
a	O
non-zero	O
s	O
the	O
customer	O
database	O
contains	O
the	O
pair	O
xs	O
that	O
takes	O
us	O
back	O
where	O
have	O
we	O
come	O
across	O
the	O
idea	O
of	O
mapping	B
from	O
n	O
bits	O
to	O
m	O
bits	O
before	O
we	O
encountered	O
this	O
idea	O
twice	O
in	O
source	O
coding	O
we	O
studied	O
block	B
codes	O
which	O
were	O
mappings	O
from	O
strings	O
of	O
n	O
symbols	O
to	O
a	O
selection	O
of	O
one	O
label	O
in	O
a	O
list	O
the	O
task	O
of	O
information	B
retrieval	I
is	O
similar	O
to	O
the	O
task	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
hash	O
codes	O
string	O
length	O
number	O
of	O
strings	O
size	O
of	O
hash	B
function	I
m	O
bits	O
size	O
of	O
hash	O
table	O
n	O
s	O
t	O
figure	O
revised	O
cast	O
of	O
characters	O
we	O
never	O
actually	O
solved	O
of	O
making	O
an	O
encoder	B
for	O
a	O
typical-set	O
compression	B
code	O
the	O
second	O
time	O
that	O
we	O
mapped	O
bit	B
strings	O
to	O
bit	B
strings	O
of	O
another	O
dimensionality	O
was	O
when	O
we	O
studied	O
channel	O
codes	O
there	O
we	O
considered	O
codes	O
that	O
mapped	O
from	O
k	O
bits	O
to	O
n	O
bits	O
with	O
n	O
greater	O
than	O
k	O
and	O
we	O
made	O
theoretical	O
progress	O
using	O
random	B
codes	O
in	O
hash	O
codes	O
we	O
put	O
together	O
these	O
two	O
notions	O
we	O
will	O
study	O
random	B
codes	O
that	O
map	O
from	O
n	O
bits	O
to	O
m	O
bits	O
where	O
m	O
is	O
smaller	O
than	O
n	O
the	O
idea	O
is	O
that	O
we	O
will	O
map	O
the	O
original	O
high-dimensional	O
space	O
down	O
into	O
a	O
lower-dimensional	O
space	O
one	O
in	O
which	O
it	O
is	O
feasible	O
to	O
implement	O
the	O
dumb	O
look-up	O
table	O
method	O
which	O
we	O
rejected	O
a	O
moment	O
ago	O
hash	O
codes	O
first	O
we	O
will	O
describe	O
how	O
a	O
hash	B
code	I
works	O
then	O
we	O
will	O
study	O
the	O
properties	O
of	O
idealized	O
hash	O
codes	O
a	O
hash	B
code	I
implements	O
a	O
solution	O
to	O
the	O
informationretrieval	O
problem	O
that	O
is	O
a	O
mapping	B
from	O
x	O
to	O
s	O
with	O
the	O
help	O
of	O
a	O
pseudorandom	O
function	O
called	O
a	O
hash	B
function	I
which	O
maps	O
the	O
n	O
string	O
x	O
to	O
an	O
m	O
string	O
hx	O
where	O
m	O
is	O
smaller	O
than	O
n	O
m	O
is	O
typically	O
chosen	O
such	O
that	O
the	O
table	O
size	O
t	O
is	O
a	O
little	O
bigger	O
than	O
s	O
say	O
ten	O
times	O
bigger	O
for	O
example	O
if	O
we	O
were	O
expecting	O
s	O
to	O
be	O
about	O
a	O
million	O
we	O
might	O
map	O
x	O
into	O
a	O
hash	O
h	O
of	O
the	O
size	O
n	O
of	O
each	O
item	O
x	O
the	O
hash	B
function	I
is	O
some	O
deterministic	B
function	O
which	O
should	O
ideally	O
be	O
indistinguishable	O
from	O
a	O
random	B
code	I
for	O
practical	B
purposes	O
the	O
hash	B
function	I
must	O
be	O
quick	O
to	O
compute	O
two	O
simple	O
examples	O
of	O
hash	O
functions	B
are	O
division	O
method	O
the	O
table	O
size	O
t	O
is	O
a	O
prime	O
number	O
preferably	O
one	O
that	O
is	O
not	O
close	O
to	O
a	O
power	O
of	O
the	O
hash	O
value	O
is	O
the	O
remainder	O
when	O
the	O
integer	O
x	O
is	O
divided	O
by	O
t	O
variable	O
string	O
addition	O
method	O
this	O
method	O
assumes	O
that	O
x	O
is	O
a	O
string	O
of	O
bytes	O
and	O
that	O
the	O
table	O
size	O
t	O
is	O
the	O
characters	O
of	O
x	O
are	O
added	O
modulo	O
this	O
hash	B
function	I
has	O
the	O
defect	O
that	O
it	O
maps	O
strings	O
that	O
are	O
anagrams	O
of	O
each	O
other	O
onto	O
the	O
same	O
hash	O
it	O
may	O
be	O
improved	O
by	O
putting	O
the	O
running	O
total	O
through	O
a	O
pseudorandom	O
permutation	B
after	O
each	O
character	O
is	O
added	O
in	O
the	O
variable	O
string	O
exclusive-or	O
method	O
with	O
table	O
size	O
the	O
string	O
is	O
hashed	O
twice	O
in	O
this	O
way	O
with	O
the	O
initial	O
running	O
total	O
being	O
set	B
to	O
and	O
respectively	O
the	O
result	O
is	O
a	O
hash	O
having	O
picked	O
a	O
hash	B
function	I
hx	O
we	O
implement	O
an	O
information	B
retriever	O
as	O
follows	O
encoding	O
a	O
piece	O
of	O
memory	B
called	O
the	O
hash	O
table	O
is	O
created	O
of	O
size	O
b	O
memory	B
units	B
where	O
b	O
is	O
the	O
amount	O
of	O
memory	B
needed	O
to	O
represent	O
an	O
integer	O
between	O
and	O
s	O
this	O
table	O
is	O
initially	O
set	B
to	O
zero	O
throughout	O
each	O
memory	B
xs	O
is	O
put	O
through	O
the	O
hash	B
function	I
and	O
at	O
the	O
location	O
in	O
the	O
hash	O
table	O
corresponding	O
to	O
the	O
resulting	O
vector	O
hs	O
hxs	O
the	O
integer	O
s	O
is	O
written	O
unless	O
that	O
entry	O
in	O
the	O
hash	O
table	O
is	O
already	O
occupied	O
in	O
which	O
case	O
we	O
have	O
a	O
collision	B
between	O
xs	O
and	O
some	O
earlier	O
which	O
both	O
happen	O
to	O
have	O
the	O
same	O
hash	B
code	I
collisions	O
can	O
be	O
handled	O
in	O
various	O
ways	O
we	O
will	O
discuss	O
some	O
in	O
a	O
moment	O
but	O
let	O
us	O
complete	O
the	O
basic	O
picture	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
hash	O
codes	O
codes	O
for	O
information	B
retrieval	I
algorithm	O
c	O
code	O
implementing	O
the	O
variable	O
string	O
exclusive-or	O
method	O
to	O
create	O
a	O
hash	O
h	O
in	O
the	O
range	O
from	O
a	O
string	O
x	O
author	O
thomas	O
niemann	O
unsigned	O
char	O
this	O
array	O
contains	O
a	O
random	B
int	O
hashchar	O
int	O
h	O
unsigned	O
char	O
permutation	B
from	O
to	O
x	O
is	O
a	O
pointer	B
to	O
the	O
first	O
char	O
is	O
the	O
first	O
character	O
if	O
return	O
x	O
while	O
special	O
handling	O
of	O
empty	B
string	I
initialize	O
two	O
hashes	O
proceed	O
to	O
the	O
next	O
character	O
exclusive-or	O
with	O
the	O
two	O
hashes	O
x	O
and	O
put	O
through	O
the	O
randomizer	O
h	O
end	O
of	O
string	O
is	O
reached	O
when	O
shift	O
left	O
bits	O
and	O
add	O
return	O
h	O
hash	O
is	O
concatenation	B
of	O
and	O
figure	O
use	O
of	O
hash	O
functions	B
for	O
information	B
retrieval	I
for	O
each	O
string	O
xs	O
the	O
hash	O
h	O
hxs	O
is	O
computed	O
and	O
the	O
value	O
of	O
s	O
is	O
written	O
into	O
the	O
hth	O
row	O
of	O
the	O
hash	O
table	O
blank	O
rows	O
in	O
the	O
hash	O
table	O
contain	O
the	O
value	O
zero	O
the	O
table	O
size	O
is	O
t	O
strings	O
hash	B
function	I
hashes	O
hash	O
table	O
bits	O
n	O
bits	O
xs	O
s	O
a	O
a	O
a	O
a	O
a	O
au	O
hxs	O
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
collision	B
resolution	O
decoding	B
to	O
retrieve	O
a	O
piece	O
of	O
information	B
corresponding	O
to	O
a	O
target	O
vector	O
x	O
we	O
compute	O
the	O
hash	O
h	O
of	O
x	O
and	O
look	O
at	O
the	O
corresponding	O
location	O
in	O
the	O
hash	O
table	O
if	O
there	O
is	O
a	O
zero	O
then	O
we	O
know	O
immediately	O
that	O
the	O
string	O
x	O
is	O
not	O
in	O
the	O
database	O
the	O
cost	O
of	O
this	O
answer	O
is	O
the	O
cost	O
of	O
one	O
hash-function	O
evaluation	O
and	O
one	O
look-up	O
in	O
the	O
table	O
of	O
size	O
if	O
on	O
the	O
other	O
hand	O
there	O
is	O
a	O
non-zero	O
entry	O
s	O
in	O
the	O
table	O
there	O
are	O
two	O
possibilities	O
either	O
the	O
vector	O
x	O
is	O
indeed	O
equal	O
to	O
xs	O
or	O
the	O
vector	O
xs	O
is	O
another	O
vector	O
that	O
happens	O
to	O
have	O
the	O
same	O
hash	B
code	I
as	O
the	O
target	O
x	O
third	O
possibility	O
is	O
that	O
this	O
non-zero	O
entry	O
might	O
have	O
something	O
to	O
do	O
with	O
our	O
yet-to-be-discussed	O
collision-resolution	O
system	O
to	O
check	O
whether	O
x	O
is	O
indeed	O
equal	O
to	O
xs	O
we	O
take	O
the	O
tentative	O
answer	O
s	O
look	O
up	O
xs	O
in	O
the	O
original	O
forward	O
database	O
and	O
compare	O
it	O
bit	B
by	O
bit	B
with	O
x	O
if	O
it	O
matches	O
then	O
we	O
report	O
s	O
as	O
the	O
desired	O
answer	O
this	O
successful	O
retrieval	O
has	O
an	O
overall	O
cost	O
of	O
one	O
hash-function	O
evaluation	O
one	O
look-up	O
in	O
the	O
table	O
of	O
size	O
another	O
look-up	O
in	O
a	O
table	O
of	O
size	O
s	O
and	O
n	O
binary	O
comparisons	O
which	O
may	O
be	O
much	O
cheaper	O
than	O
the	O
simple	O
solutions	O
presented	O
in	O
section	B
exercise	O
if	O
we	O
have	O
checked	O
the	O
few	O
bits	O
of	O
xs	O
with	O
x	O
and	O
found	O
them	O
to	O
be	O
equal	O
what	O
is	O
the	O
probability	B
that	O
the	O
correct	O
entry	O
has	O
been	O
retrieved	O
if	O
the	O
alternative	O
hypothesis	O
is	O
that	O
x	O
is	O
actually	O
not	O
in	O
the	O
database	O
assume	O
that	O
the	O
original	O
source	O
strings	O
are	O
random	B
and	O
the	O
hash	B
function	I
is	O
a	O
random	B
hash	B
function	I
how	O
many	O
binary	O
evaluations	O
are	O
needed	O
to	O
be	O
sure	O
with	O
odds	B
of	O
a	O
billion	O
to	O
one	O
that	O
the	O
correct	O
entry	O
has	O
been	O
retrieved	O
the	O
hashing	O
method	O
of	O
information	B
retrieval	I
can	O
be	O
used	O
for	O
strings	O
x	O
of	O
arbitrary	O
length	O
if	O
the	O
hash	B
function	I
hx	O
can	O
be	O
applied	O
to	O
strings	O
of	O
any	O
length	O
collision	B
resolution	O
we	O
will	O
study	O
two	O
ways	O
of	O
resolving	O
collisions	O
appending	O
in	O
the	O
table	O
and	O
storing	O
elsewhere	O
appending	O
in	O
table	O
when	O
encoding	O
if	O
a	O
collision	B
occurs	O
we	O
continue	O
down	O
the	O
hash	O
table	O
and	O
write	O
the	O
value	O
of	O
s	O
into	O
the	O
next	O
available	O
location	O
in	O
memory	B
that	O
currently	O
contains	O
a	O
zero	O
if	O
we	O
reach	O
the	O
bottom	O
of	O
the	O
table	O
before	O
encountering	O
a	O
zero	O
we	O
continue	O
from	O
the	O
top	O
when	O
decoding	B
if	O
we	O
compute	O
the	O
hash	B
code	I
for	O
x	O
and	O
that	O
the	O
s	O
contained	O
in	O
the	O
table	O
doesn	O
t	O
point	O
to	O
an	O
xs	O
that	O
matches	O
the	O
cue	O
x	O
we	O
continue	O
down	O
the	O
hash	O
table	O
until	O
we	O
either	O
an	O
s	O
whose	O
xs	O
does	O
match	O
the	O
cue	O
x	O
in	O
which	O
case	O
we	O
are	O
done	O
or	O
else	O
encounter	O
a	O
zero	O
in	O
which	O
case	O
we	O
know	O
that	O
the	O
cue	O
x	O
is	O
not	O
in	O
the	O
database	O
for	O
this	O
method	O
it	O
is	O
essential	O
that	O
the	O
table	O
be	O
substantially	O
bigger	O
in	O
size	O
than	O
s	O
if	O
s	O
then	O
the	O
encoding	O
rule	O
will	O
become	O
stuck	O
with	O
nowhere	O
to	O
put	O
the	O
last	O
strings	O
storing	O
elsewhere	O
a	O
more	O
robust	O
and	O
method	O
is	O
to	O
use	O
pointers	O
to	O
additional	O
pieces	O
of	O
memory	B
in	O
which	O
collided	O
strings	O
are	O
stored	O
there	O
are	O
many	O
ways	O
of	O
doing	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
hash	O
codes	O
codes	O
for	O
information	B
retrieval	I
this	O
as	O
an	O
example	O
we	O
could	O
store	O
in	O
location	O
h	O
in	O
the	O
hash	O
table	O
a	O
pointer	B
must	O
be	O
distinguishable	O
from	O
a	O
valid	O
record	O
number	O
s	O
to	O
a	O
bucket	O
where	O
all	O
the	O
strings	O
that	O
have	O
hash	B
code	I
h	O
are	O
stored	O
in	O
a	O
sorted	O
list	O
the	O
encoder	B
sorts	O
the	O
strings	O
in	O
each	O
bucket	O
alphabetically	O
as	O
the	O
hash	O
table	O
and	O
buckets	O
are	O
created	O
the	O
decoder	B
simply	O
has	O
to	O
go	O
and	O
look	O
in	O
the	O
relevant	O
bucket	O
and	O
then	O
check	O
the	O
short	O
list	O
of	O
strings	O
that	O
are	O
there	O
by	O
a	O
brief	O
alphabetical	O
search	O
this	O
method	O
of	O
storing	O
the	O
strings	O
in	O
buckets	O
allows	O
the	O
option	O
of	O
making	O
the	O
hash	O
table	O
quite	O
small	O
which	O
may	O
have	O
practical	B
we	O
may	O
make	O
it	O
so	O
small	O
that	O
almost	O
all	O
strings	O
are	O
involved	O
in	O
collisions	O
so	O
all	O
buckets	O
contain	O
a	O
small	O
number	O
of	O
strings	O
it	O
only	O
takes	O
a	O
small	O
number	O
of	O
binary	O
comparisons	O
to	O
identify	O
which	O
of	O
the	O
strings	O
in	O
the	O
bucket	O
matches	O
the	O
cue	O
x	O
planning	O
for	O
collisions	O
a	O
birthday	B
problem	O
exercise	O
if	O
we	O
wish	O
to	O
store	O
s	O
entries	O
using	O
a	O
hash	B
function	I
whose	O
output	O
has	O
m	O
bits	O
how	O
many	O
collisions	O
should	O
we	O
expect	O
to	O
happen	O
assuming	O
that	O
our	O
hash	B
function	I
is	O
an	O
ideal	O
random	B
function	O
what	O
size	O
m	O
of	O
hash	O
table	O
is	O
needed	O
if	O
we	O
would	O
like	O
the	O
expected	O
number	O
of	O
collisions	O
to	O
be	O
smaller	O
than	O
what	O
size	O
m	O
of	O
hash	O
table	O
is	O
needed	O
if	O
we	O
would	O
like	O
the	O
expected	O
number	O
of	O
collisions	O
to	O
be	O
a	O
small	O
fraction	O
say	O
of	O
s	O
the	O
similarity	O
of	O
this	O
problem	O
to	O
exercise	O
other	O
roles	O
for	O
hash	O
codes	O
checking	O
arithmetic	O
if	O
you	O
wish	O
to	O
check	O
an	O
addition	O
that	O
was	O
done	O
by	O
hand	O
you	O
may	O
useful	O
the	O
method	O
of	O
casting	B
out	I
nines	B
in	O
casting	B
out	I
nines	B
one	O
the	O
sum	O
modulo	O
nine	O
of	O
all	O
the	O
digits	O
of	O
the	O
numbers	O
to	O
be	O
summed	O
and	O
compares	O
it	O
with	O
the	O
sum	O
modulo	O
nine	O
of	O
the	O
digits	O
of	O
the	O
putative	O
answer	O
a	O
little	O
practice	O
these	O
sums	O
can	O
be	O
computed	O
much	O
more	O
rapidly	O
than	O
the	O
full	O
original	O
addition	O
example	O
in	O
the	O
calculation	O
shown	O
in	O
the	O
margin	O
the	O
sum	O
modulo	O
nine	O
of	O
the	O
digits	O
in	O
is	O
and	O
the	O
sum	O
modulo	O
nine	O
of	O
is	O
the	O
calculation	O
thus	O
passes	O
the	O
casting-out-nines	O
test	O
casting	B
out	I
nines	B
gives	O
a	O
simple	O
example	O
of	O
a	O
hash	B
function	I
for	O
any	O
addition	O
expression	O
of	O
the	O
form	O
a	O
b	O
c	O
where	O
a	O
b	O
c	O
are	O
decimal	O
numbers	O
we	O
h	O
by	O
ha	O
b	O
c	O
sum	O
modulo	O
nine	O
of	O
all	O
digits	O
in	O
a	O
b	O
c	O
then	O
it	O
is	O
nice	O
property	O
of	O
decimal	O
arithmetic	O
that	O
if	O
a	O
b	O
c	O
m	O
n	O
o	O
then	O
the	O
hashes	O
ha	O
b	O
c	O
and	O
hm	O
n	O
o	O
are	O
equal	O
exercise	O
what	O
evidence	B
does	O
a	O
correct	O
casting-out-nines	O
match	O
give	O
in	O
favour	O
of	O
the	O
hypothesis	O
that	O
the	O
addition	O
has	O
been	O
done	O
correctly	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
other	O
roles	O
for	O
hash	O
codes	O
error	B
detection	I
among	O
friends	O
are	O
two	O
the	O
same	O
if	O
the	O
are	O
on	O
the	O
same	O
computer	B
we	O
could	O
just	O
compare	O
them	O
bit	B
by	O
bit	B
but	O
if	O
the	O
two	O
are	O
on	O
separate	O
machines	O
it	O
would	O
be	O
nice	O
to	O
have	O
a	O
way	O
of	O
that	O
two	O
are	O
identical	O
without	O
having	O
to	O
transfer	O
one	O
of	O
the	O
from	O
a	O
to	O
b	O
even	O
if	O
we	O
did	O
transfer	O
one	O
of	O
the	O
we	O
would	O
still	O
like	O
a	O
way	O
to	O
whether	O
it	O
has	O
been	O
received	O
without	O
this	O
problem	O
can	O
be	O
solved	O
using	O
hash	O
codes	O
let	O
alice	B
and	O
bob	B
be	O
the	O
holders	O
of	O
the	O
two	O
alice	B
sent	O
the	O
to	O
bob	B
and	O
they	O
wish	O
to	O
it	O
has	O
been	O
received	O
without	O
error	O
if	O
alice	B
computes	O
the	O
hash	O
of	O
her	O
and	O
sends	O
it	O
to	O
bob	B
and	O
bob	B
computes	O
the	O
hash	O
of	O
his	O
using	O
the	O
same	O
m	O
hash	B
function	I
and	O
the	O
two	O
hashes	O
match	O
then	O
bob	B
can	O
deduce	O
that	O
the	O
two	O
are	O
almost	O
surely	O
the	O
same	O
example	O
what	O
is	O
the	O
probability	B
of	O
a	O
false	O
negative	O
i	O
e	O
the	O
probability	B
given	O
that	O
the	O
two	O
do	O
that	O
the	O
two	O
hashes	O
are	O
nevertheless	O
identical	O
if	O
we	O
assume	O
that	O
the	O
hash	B
function	I
is	O
random	B
and	O
that	O
the	O
process	O
that	O
causes	O
the	O
to	O
knows	O
nothing	O
about	O
the	O
hash	B
function	I
then	O
the	O
probability	B
of	O
a	O
false	O
negative	O
is	O
a	O
hash	O
gives	O
a	O
probability	B
of	O
false	O
negative	O
of	O
about	O
it	O
is	O
common	O
practice	O
to	O
use	O
a	O
linear	B
hash	B
function	I
called	O
a	O
cyclic	B
redundancy	B
check	O
to	O
detect	O
errors	B
in	O
cyclic	B
redundancy	B
check	O
is	O
a	O
set	B
of	O
paritycheck	O
bits	O
similar	O
to	O
the	O
parity-check	B
bits	I
of	O
the	O
hamming	B
code	I
to	O
have	O
a	O
false-negative	O
rate	B
smaller	O
than	O
one	O
in	O
a	O
billion	O
m	O
bits	O
is	O
plenty	O
if	O
the	O
errors	B
are	O
produced	O
by	O
noise	O
exercise	O
such	O
a	O
simple	O
parity-check	B
code	I
only	O
detects	O
errors	B
it	O
doesn	O
t	O
help	O
correct	O
them	O
since	O
error-correcting	B
codes	I
exist	O
why	O
not	O
use	O
one	O
of	O
them	O
to	O
get	O
some	O
error-correcting	O
capability	O
too	O
tamper	B
detection	I
what	O
if	O
the	O
between	O
the	O
two	O
are	O
not	O
simply	O
noise	O
but	O
are	O
introduced	O
by	O
an	O
adversary	O
a	O
clever	O
forger	O
called	O
fiona	O
who	O
the	O
original	O
to	O
make	O
a	O
forgery	B
that	O
purports	O
to	O
be	O
alice	B
s	O
how	O
can	O
alice	B
make	O
a	O
digital	B
signature	I
for	O
the	O
so	O
that	O
bob	B
can	O
that	O
no-one	O
has	O
tampered	O
with	O
the	O
and	O
how	O
can	O
we	O
prevent	O
fiona	O
from	O
listening	O
in	O
on	O
alice	B
s	O
signature	O
and	O
attaching	O
it	O
to	O
other	O
let	O
s	O
assume	O
that	O
alice	B
computes	O
a	O
hash	B
function	I
for	O
the	O
and	O
sends	O
it	O
securely	O
to	O
bob	B
if	O
alice	B
computes	O
a	O
simple	O
hash	B
function	I
for	O
the	O
like	O
the	O
linear	B
cyclic	B
redundancy	B
check	O
and	O
fiona	O
knows	O
that	O
this	O
is	O
the	O
method	O
of	O
verifying	O
the	O
s	O
integrity	O
fiona	O
can	O
make	O
her	O
chosen	O
to	O
the	O
and	O
then	O
easily	O
identify	O
linear	B
algebra	O
a	O
further	O
single	O
bits	O
that	O
when	O
restore	O
the	O
hash	B
function	I
of	O
the	O
to	O
its	O
original	O
value	O
linear	B
hash	O
functions	B
give	O
no	O
security	B
against	O
forgers	O
we	O
must	O
therefore	O
require	O
that	O
the	O
hash	B
function	I
be	O
hard	O
to	O
invert	O
so	O
that	O
no-one	O
can	O
construct	O
a	O
tampering	O
that	O
leaves	O
the	O
hash	B
function	I
we	O
would	O
still	O
like	O
the	O
hash	B
function	I
to	O
be	O
easy	O
to	O
compute	O
however	O
so	O
that	O
bob	B
doesn	O
t	O
have	O
to	O
do	O
hours	O
of	O
work	O
to	O
verify	O
every	O
he	O
received	O
such	O
a	O
hash	B
function	I
easy	O
to	O
compute	O
but	O
hard	O
to	O
invert	O
is	O
called	O
a	O
one-way	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
hash	O
codes	O
codes	O
for	O
information	B
retrieval	I
hash	B
function	I
finding	O
such	O
functions	B
is	O
one	O
of	O
the	O
active	O
research	O
areas	O
of	O
cryptography	B
a	O
hash	B
function	I
that	O
is	O
widely	O
used	O
in	O
the	O
free	O
software	B
community	O
to	O
that	O
two	O
do	O
not	O
is	O
which	O
produces	O
a	O
hash	O
the	O
details	O
of	O
how	O
it	O
works	O
are	O
quite	O
complicated	O
involving	O
convoluted	O
exclusiveor-ing	O
and	O
if-ing	O
and	O
even	O
with	O
a	O
good	B
one-way	B
hash	B
function	I
the	O
digital	B
signatures	I
described	O
above	O
are	O
still	O
vulnerable	O
to	O
attack	O
if	O
fiona	O
has	O
access	O
to	O
the	O
hash	B
function	I
fiona	O
could	O
take	O
the	O
tampered	O
and	O
hunt	O
for	O
a	O
further	O
tiny	O
to	O
it	O
such	O
that	O
its	O
hash	O
matches	O
the	O
original	O
hash	O
of	O
alice	B
s	O
this	O
would	O
take	O
some	O
time	O
on	O
average	O
about	O
attempts	O
if	O
the	O
hash	B
function	I
has	O
bits	O
but	O
eventually	O
fiona	O
would	O
a	O
tampered	O
that	O
matches	O
the	O
given	O
hash	O
to	O
be	O
secure	O
against	O
forgery	B
digital	B
signatures	I
must	O
either	O
have	O
enough	O
bits	O
for	O
such	O
a	O
random	B
search	O
to	O
take	O
too	O
long	O
or	O
the	O
hash	B
function	I
itself	O
must	O
be	O
kept	O
secret	B
fiona	O
has	O
to	O
hash	O
to	O
cheat	B
is	O
not	O
very	O
many	O
so	O
a	O
hash	B
function	I
is	O
not	O
large	O
enough	O
for	O
forgery	B
prevention	O
another	O
person	O
who	O
might	O
have	O
a	O
motivation	O
for	O
forgery	B
is	O
alice	B
herself	O
for	O
example	O
she	O
might	O
be	O
making	O
a	O
bet	B
on	O
the	O
outcome	O
of	O
a	O
race	B
without	O
wishing	O
to	O
broadcast	B
her	O
prediction	B
publicly	O
a	O
method	O
for	O
placing	O
bets	O
would	O
be	O
for	O
her	O
to	O
send	O
to	O
bob	B
the	O
bookie	O
the	O
hash	O
of	O
her	O
bet	B
later	O
on	O
she	O
could	O
send	O
bob	B
the	O
details	O
of	O
her	O
bet	B
everyone	O
can	O
that	O
her	O
bet	B
is	O
consistent	O
with	O
the	O
previously	O
publicized	O
hash	O
method	O
of	O
secret	B
publication	O
was	O
used	O
by	O
isaac	O
newton	B
and	O
robert	O
hooke	B
when	O
they	O
wished	O
to	O
establish	O
priority	O
for	O
ideas	O
without	O
revealing	O
them	O
hooke	B
s	O
hash	B
function	I
was	O
alphabetization	O
as	O
illustrated	O
by	O
the	O
conversion	O
of	O
ut	O
tensio	O
sic	O
vis	O
into	O
the	O
anagram	B
ceiiinosssttuv	O
such	O
a	O
protocol	B
relies	O
on	O
the	O
assumption	O
that	O
alice	B
cannot	O
change	O
her	O
bet	B
after	O
the	O
event	O
without	O
the	O
hash	O
coming	O
out	O
wrong	O
how	O
big	O
a	O
hash	B
function	I
do	O
we	O
need	O
to	O
use	O
to	O
ensure	O
that	O
alice	B
cannot	O
cheat	B
the	O
answer	O
is	O
from	O
the	O
size	O
of	O
the	O
hash	O
we	O
needed	O
in	O
order	O
to	O
defeat	O
fiona	O
above	O
because	O
alice	B
is	O
the	O
author	O
of	O
both	O
alice	B
could	O
cheat	B
by	O
searching	O
for	O
two	O
that	O
have	O
identical	O
hashes	O
to	O
each	O
other	O
for	O
example	O
if	O
she	O
d	O
like	O
to	O
cheat	B
by	O
placing	O
two	O
bets	O
for	O
the	O
price	O
of	O
one	O
she	O
could	O
make	O
a	O
large	O
number	O
of	O
versions	O
of	O
bet	B
one	O
from	O
each	O
other	O
in	O
minor	O
details	O
only	O
and	O
a	O
large	O
number	O
of	O
versions	O
of	O
bet	B
two	O
and	O
hash	O
them	O
all	O
if	O
there	O
s	O
a	O
collision	B
between	O
the	O
hashes	O
of	O
two	O
bets	O
of	O
types	O
then	O
she	O
can	O
submit	O
the	O
common	O
hash	O
and	O
thus	O
buy	O
herself	O
the	O
option	O
of	O
placing	O
either	O
bet	B
example	O
if	O
the	O
hash	O
has	O
m	O
bits	O
how	O
big	O
do	O
and	O
need	O
to	O
be	O
for	O
alice	B
to	O
have	O
a	O
good	B
chance	O
of	O
two	O
bets	O
with	O
the	O
same	O
hash	O
this	O
is	O
a	O
birthday	B
problem	O
like	O
exercise	O
if	O
there	O
are	O
montagues	O
and	O
capulets	O
at	O
a	O
party	O
and	O
each	O
is	O
assigned	O
a	O
birthday	B
of	O
m	O
bits	O
the	O
expected	O
number	O
of	O
collisions	O
between	O
a	O
montague	O
and	O
a	O
capulet	O
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
so	O
to	O
minimize	O
the	O
number	O
of	O
hashed	O
alice	B
should	O
make	O
and	O
equal	O
and	O
will	O
need	O
to	O
hash	O
about	O
until	O
she	O
two	O
that	O
match	O
alice	B
has	O
to	O
hash	O
to	O
cheat	B
is	O
the	O
square	B
root	O
of	O
the	O
number	O
of	O
hashes	O
fiona	O
had	O
to	O
make	O
if	O
alice	B
has	O
the	O
use	O
of	O
c	O
computers	O
for	O
t	O
years	O
each	O
computer	B
taking	O
t	O
ns	O
to	O
evaluate	O
a	O
hash	O
the	O
bet-communication	O
system	O
is	O
secure	O
against	O
alice	B
s	O
dishonesty	O
only	O
if	O
m	O
ct	O
bits	O
further	O
reading	O
the	O
bible	O
for	O
hash	O
codes	O
is	O
volume	B
of	O
knuth	O
i	O
highly	O
recommend	O
the	O
story	O
of	O
doug	O
mcilroy	O
s	O
spell	B
program	O
as	O
told	O
in	O
section	B
of	O
programming	O
pearls	O
this	O
astonishing	O
piece	O
of	O
software	B
makes	O
use	O
of	O
a	O
data	O
structure	O
to	O
store	O
the	O
spellings	O
of	O
all	O
the	O
words	O
of	O
dictionary	B
further	O
exercises	O
exercise	O
what	O
is	O
the	O
shortest	O
the	O
address	B
on	O
a	O
typical	B
international	O
letter	O
could	O
be	O
if	O
it	O
is	O
to	O
get	O
to	O
a	O
unique	O
human	B
recipient	O
the	O
permitted	O
characters	O
are	O
how	O
long	O
are	O
typical	B
email	B
addresses	O
exercise	O
how	O
long	O
does	O
a	O
piece	O
of	O
text	O
need	O
to	O
be	O
for	O
you	O
to	O
be	O
pretty	O
sure	O
that	O
no	O
human	B
has	O
written	O
that	O
string	O
of	O
characters	O
before	O
how	O
many	O
notes	O
are	O
there	O
in	O
a	O
new	O
melody	B
that	O
has	O
not	O
been	O
composed	O
before	O
exercise	O
pattern	B
recognition	B
by	O
molecules	B
some	O
proteins	O
produced	O
in	O
a	O
cell	O
have	O
a	O
regulatory	B
role	O
a	O
regulatory	B
protein	B
controls	O
the	O
transcription	O
of	O
genes	B
in	O
the	O
genome	B
this	O
control	O
often	O
involves	O
the	O
protein	B
s	O
binding	O
to	O
a	O
particular	O
dna	B
sequence	B
in	O
the	O
vicinity	O
of	O
the	O
regulated	O
gene	O
the	O
presence	O
of	O
the	O
bound	B
protein	B
either	O
promotes	O
or	O
inhibits	O
transcription	O
of	O
the	O
gene	O
use	O
information-theoretic	O
arguments	O
to	O
obtain	O
a	O
lower	O
bound	B
on	O
the	O
size	O
of	O
a	O
typical	B
protein	B
that	O
acts	O
as	O
a	O
regulator	O
to	O
one	O
gene	O
in	O
the	O
whole	O
human	B
genome	B
assume	O
that	O
the	O
genome	B
is	O
a	O
sequence	B
of	O
nucleotides	O
drawn	O
from	O
a	O
four	O
letter	O
alphabet	O
fa	O
c	O
g	O
tg	O
a	O
protein	B
is	O
a	O
sequence	B
of	O
amino	O
acids	O
drawn	O
from	O
a	O
twenty	O
letter	O
alphabet	O
establish	O
how	O
long	O
the	O
recognized	O
dna	B
sequence	B
has	O
to	O
be	O
in	O
order	O
for	O
that	O
sequence	B
to	O
be	O
unique	O
to	O
the	O
vicinity	O
of	O
one	O
gene	O
treating	O
the	O
rest	O
of	O
the	O
genome	B
as	O
a	O
random	B
sequence	B
then	O
discuss	O
how	O
big	O
the	O
protein	B
must	O
be	O
to	O
recognize	O
a	O
sequence	B
of	O
that	O
length	O
uniquely	O
some	O
of	O
the	O
sequences	O
recognized	O
by	O
dna-binding	O
regulatory	B
proteins	O
consist	O
of	O
a	O
subsequence	O
that	O
is	O
repeated	O
twice	O
or	O
more	O
for	O
example	O
the	O
sequence	B
gccccccacccctgccccc	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
hash	O
codes	O
codes	O
for	O
information	B
retrieval	I
is	O
a	O
binding	O
site	O
found	O
upstream	O
of	O
the	O
alpha-actin	O
gene	O
in	O
humans	O
does	O
the	O
fact	O
that	O
some	O
binding	O
sites	O
consist	O
of	O
a	O
repeated	O
subsequence	O
your	O
answer	O
to	O
part	O
solutions	O
solution	O
to	O
exercise	O
first	O
imagine	O
comparing	O
the	O
string	O
x	O
with	O
another	O
random	B
string	O
xs	O
the	O
probability	B
that	O
the	O
bits	O
of	O
the	O
two	O
strings	O
match	O
is	O
the	O
probability	B
that	O
the	O
second	O
bits	O
match	O
is	O
assuming	O
we	O
stop	O
comparing	O
once	O
we	O
hit	O
the	O
mismatch	O
the	O
expected	O
number	O
of	O
matches	O
is	O
so	O
the	O
expected	O
number	O
of	O
comparisons	O
is	O
assuming	O
the	O
correct	O
string	O
is	O
located	O
at	O
random	B
in	O
the	O
raw	O
list	O
we	O
will	O
have	O
to	O
compare	O
with	O
an	O
average	O
of	O
strings	O
before	O
we	O
it	O
which	O
costs	O
binary	O
comparisons	O
and	O
comparing	O
the	O
correct	O
strings	O
takes	O
n	O
binary	O
comparisons	O
giving	O
a	O
total	O
expectation	B
of	O
s	O
n	O
binary	O
comparisons	O
if	O
the	O
strings	O
are	O
chosen	O
at	O
random	B
in	O
the	O
worst	O
case	O
may	O
indeed	O
happen	O
in	O
practice	O
the	O
other	O
strings	O
are	O
very	O
similar	O
to	O
the	O
search	O
key	O
so	O
that	O
a	O
lengthy	O
sequence	B
of	O
comparisons	O
is	O
needed	O
to	O
each	O
mismatch	O
the	O
worst	O
case	O
is	O
when	O
the	O
correct	O
string	O
is	O
last	O
in	O
the	O
list	O
and	O
all	O
the	O
other	O
strings	O
in	O
the	O
last	O
bit	B
only	O
giving	O
a	O
requirement	O
of	O
sn	O
binary	O
comparisons	O
solution	O
to	O
exercise	O
the	O
likelihood	B
ratio	O
for	O
the	O
two	O
hypotheses	O
xs	O
x	O
and	O
xs	O
x	O
contributed	O
by	O
the	O
datum	O
the	O
bits	O
of	O
xs	O
and	O
x	O
are	O
equal	O
is	O
p	O
p	O
if	O
the	O
r	O
bits	O
all	O
match	O
the	O
likelihood	B
ratio	O
is	O
to	O
one	O
on	O
that	O
bits	O
match	O
the	O
odds	B
are	O
a	O
billion	O
to	O
one	O
in	O
favour	O
of	O
assuming	O
we	O
start	O
from	O
even	O
odds	B
a	O
complete	O
answer	O
we	O
should	O
compute	O
the	O
evidence	B
given	O
by	O
the	O
prior	B
information	B
that	O
the	O
hash	O
entry	O
s	O
has	O
been	O
found	O
in	O
the	O
table	O
at	O
hx	O
this	O
fact	O
gives	O
further	O
evidence	B
in	O
favour	O
of	O
solution	O
to	O
exercise	O
let	O
the	O
hash	B
function	I
have	O
an	O
output	O
alphabet	O
of	O
size	O
t	O
if	O
m	O
were	O
equal	O
to	O
s	O
then	O
we	O
would	O
have	O
exactly	O
enough	O
bits	O
for	O
each	O
entry	O
to	O
have	O
its	O
own	O
unique	O
hash	O
the	O
probability	B
that	O
one	O
particular	O
pair	O
of	O
entries	O
collide	O
under	O
a	O
random	B
hash	B
function	I
is	O
the	O
number	O
of	O
pairs	O
is	O
ss	O
so	O
the	O
expected	O
number	O
of	O
collisions	O
between	O
pairs	O
is	O
exactly	O
ss	O
if	O
we	O
would	O
like	O
this	O
to	O
be	O
smaller	O
than	O
then	O
we	O
need	O
t	O
ss	O
so	O
m	O
s	O
we	O
need	O
twice	O
as	O
many	O
bits	O
as	O
the	O
number	O
of	O
bits	O
log	O
s	O
that	O
would	O
be	O
to	O
give	O
each	O
entry	O
a	O
unique	O
name	O
if	O
we	O
are	O
happy	O
to	O
have	O
occasional	O
collisions	O
involving	O
a	O
fraction	O
f	O
of	O
the	O
names	O
s	O
then	O
we	O
need	O
t	O
sf	O
the	O
probability	B
that	O
one	O
particular	O
name	O
is	O
collided-with	O
is	O
f	O
st	O
so	O
m	O
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
which	O
means	O
for	O
f	O
that	O
we	O
need	O
an	O
extra	O
bits	O
above	O
s	O
the	O
important	O
point	O
to	O
note	O
is	O
the	O
scaling	B
of	O
t	O
with	O
s	O
in	O
the	O
two	O
cases	O
if	O
we	O
want	O
the	O
hash	B
function	I
to	O
be	O
collision-free	O
then	O
we	O
must	O
have	O
t	O
greater	O
than	O
if	O
we	O
are	O
happy	O
to	O
have	O
a	O
small	O
frequency	B
of	O
collisions	O
then	O
t	O
needs	O
to	O
be	O
of	O
order	O
s	O
only	O
solution	O
to	O
exercise	O
the	O
posterior	B
probability	B
ratio	O
for	O
the	O
two	O
hypotheses	O
h	O
calculation	O
correct	O
and	O
calculation	O
incorrect	O
is	O
the	O
product	O
of	O
the	O
prior	B
probability	B
ratio	O
p	O
and	O
the	O
likelihood	B
ratio	O
p	O
jhp	O
this	O
second	O
factor	O
is	O
the	O
answer	O
to	O
the	O
question	O
the	O
numerator	O
p	O
jh	O
is	O
equal	O
to	O
the	O
denominator	O
s	O
value	O
depends	O
on	O
our	O
model	B
of	O
errors	B
if	O
we	O
know	O
that	O
the	O
human	B
calculator	B
is	O
prone	O
to	O
errors	B
involving	O
multiplication	O
of	O
the	O
answer	O
by	O
or	O
to	O
transposition	O
of	O
adjacent	O
digits	O
neither	O
of	O
which	O
the	O
hash	O
value	O
then	O
p	O
could	O
be	O
equal	O
to	O
also	O
so	O
that	O
the	O
correct	O
match	O
gives	O
no	O
evidence	B
in	O
favour	O
of	O
h	O
but	O
if	O
we	O
assume	O
that	O
errors	B
are	O
random	B
from	O
the	O
point	O
of	O
view	O
of	O
the	O
hash	B
function	I
then	O
the	O
probability	B
of	O
a	O
false	O
positive	O
is	O
p	O
and	O
the	O
correct	O
match	O
gives	O
evidence	B
in	O
favour	O
of	O
h	O
solution	O
to	O
exercise	O
if	O
you	O
add	O
a	O
tiny	O
m	O
extra	O
bits	O
of	O
hash	O
to	O
a	O
huge	O
n	O
you	O
get	O
pretty	O
good	B
error	B
detection	I
the	O
probability	B
that	O
an	O
error	O
is	O
undetected	O
is	O
less	O
than	O
one	O
in	O
a	O
billion	O
to	O
do	O
error	O
correction	O
requires	O
far	O
more	O
check	O
bits	O
the	O
number	O
depending	O
on	O
the	O
expected	O
types	O
of	O
corruption	O
and	O
on	O
the	O
size	O
for	O
example	O
if	O
just	O
eight	O
random	B
bits	O
in	O
a	O
megabyte	O
are	O
corrupted	O
it	O
would	O
take	O
about	O
bits	O
to	O
specify	O
which	O
are	O
the	O
corrupted	O
bits	O
and	O
the	O
number	O
of	O
parity-check	B
bits	I
used	O
by	O
a	O
successful	O
error-correcting	B
code	I
would	O
have	O
to	O
be	O
at	O
least	O
this	O
number	O
by	O
the	O
counting	B
argument	I
of	O
exercise	O
solution	O
to	O
exercise	O
we	O
want	O
to	O
know	O
the	O
length	O
l	O
of	O
a	O
string	O
such	O
that	O
it	O
is	O
very	O
improbable	O
that	O
that	O
string	O
matches	O
any	O
part	O
of	O
the	O
entire	O
writings	O
of	O
humanity	O
let	O
s	O
estimate	O
that	O
these	O
writings	O
total	O
about	O
one	O
book	O
for	O
each	O
person	O
living	O
and	O
that	O
each	O
book	O
contains	O
two	O
million	O
characters	O
pages	O
with	O
characters	O
per	O
page	O
that	O
s	O
characters	O
drawn	O
from	O
an	O
alphabet	O
of	O
say	O
characters	O
the	O
probability	B
that	O
a	O
randomly	O
chosen	O
string	O
of	O
length	O
l	O
matches	O
at	O
one	O
point	O
in	O
the	O
collected	O
works	O
of	O
humanity	O
is	O
so	O
the	O
expected	O
number	O
of	O
matches	O
is	O
which	O
is	O
vanishingly	O
small	O
if	O
l	O
log	O
because	O
of	O
the	O
redundancy	B
and	O
repetition	B
of	O
humanity	O
s	O
writings	O
it	O
is	O
possible	O
that	O
l	O
is	O
an	O
overestimate	O
so	O
if	O
you	O
want	O
to	O
write	O
something	O
unique	O
sit	O
down	O
and	O
compose	O
a	O
string	O
of	O
ten	O
characters	O
but	O
don	O
t	O
write	O
gidnebinzz	O
because	O
i	O
already	O
thought	O
of	O
that	O
string	O
as	O
for	O
a	O
new	O
melody	B
if	O
we	O
focus	B
on	O
the	O
sequence	B
of	O
notes	O
ignoring	O
duration	O
and	O
stress	O
and	O
allow	O
leaps	O
of	O
up	O
to	O
an	O
octave	B
at	O
each	O
note	O
then	O
the	O
number	O
of	O
choices	O
per	O
note	O
is	O
the	O
pitch	O
of	O
the	O
note	O
is	O
arbitrary	O
the	O
number	O
of	O
melodies	O
of	O
length	O
r	O
notes	O
in	O
this	O
rather	O
ugly	O
ensemble	B
of	O
tunes	O
is	O
for	O
example	O
there	O
are	O
of	O
length	O
r	O
restricting	O
the	O
permitted	O
intervals	B
will	O
reduce	O
this	O
including	O
duration	O
and	O
stress	O
will	O
increase	O
it	O
again	O
we	O
restrict	O
the	O
permitted	O
intervals	B
to	O
repetitions	O
and	O
tones	O
or	O
semitones	O
the	O
reduction	O
is	O
particularly	O
severe	O
is	O
this	O
why	O
the	O
melody	B
of	O
ode	B
to	I
joy	I
sounds	O
so	O
boring	O
the	O
number	O
of	O
recorded	O
compositions	O
is	O
probably	O
less	O
than	O
a	O
million	O
if	O
you	O
learn	O
new	O
melodies	O
per	O
week	O
for	O
every	O
week	O
of	O
your	O
life	B
then	O
you	O
will	O
have	O
learned	O
melodies	O
at	O
age	O
based	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
hash	O
codes	O
codes	O
for	O
information	B
retrieval	I
in	O
guess	B
that	I
tune	I
one	O
player	O
chooses	O
a	O
melody	B
and	O
sings	O
a	O
gradually-increasing	O
number	O
of	O
its	O
notes	O
while	O
the	O
other	O
participants	O
try	O
to	O
guess	O
the	O
whole	O
melody	B
the	O
parsons	B
code	I
is	O
a	O
related	O
hash	B
function	I
for	O
melodies	O
each	O
pair	O
of	O
consecutive	O
notes	O
is	O
coded	O
as	O
u	O
up	O
if	O
the	O
second	O
note	O
is	O
higher	O
than	O
the	O
r	O
repeat	O
if	O
the	O
pitches	O
are	O
equal	O
and	O
d	O
down	O
otherwise	O
you	O
can	O
out	O
how	O
well	O
this	O
hash	B
function	I
works	O
at	O
httpmusipedia	O
org	O
on	O
empirical	O
experience	O
of	O
playing	O
the	O
game	O
guess	B
that	I
tune	I
it	O
seems	O
to	O
me	O
that	O
whereas	O
many	O
four-note	O
sequences	O
are	O
shared	O
in	O
common	O
between	O
melodies	O
the	O
number	O
of	O
collisions	O
between	O
sequences	O
is	O
rather	O
smaller	O
most	O
famous	O
sequences	O
are	O
unique	O
solution	O
to	O
exercise	O
let	O
the	O
dna-binding	O
protein	B
recognize	O
a	O
sequence	B
of	O
length	O
l	O
nucleotides	O
that	O
is	O
it	O
binds	O
preferentially	O
to	O
that	O
dna	B
sequence	B
and	O
not	O
to	O
any	O
other	O
pieces	O
of	O
dna	B
in	O
the	O
whole	O
genome	B
reality	O
the	O
recognized	O
sequence	B
may	O
contain	O
some	O
wildcard	O
characters	O
e	O
g	O
the	O
in	O
tataaa	O
which	O
denotes	O
any	O
of	O
a	O
c	O
g	O
and	O
t	O
so	O
to	O
be	O
precise	O
we	O
are	O
assuming	O
that	O
the	O
recognized	O
sequence	B
contains	O
l	O
non-wildcard	O
characters	O
assuming	O
the	O
rest	O
of	O
the	O
genome	B
is	O
random	B
i	O
e	O
that	O
the	O
sequence	B
consists	O
of	O
random	B
nucleotides	O
a	O
c	O
g	O
and	O
t	O
with	O
equal	O
probability	B
which	O
is	O
obviously	O
untrue	O
but	O
it	O
shouldn	O
t	O
make	O
too	O
much	O
to	O
our	O
calculation	O
the	O
chance	O
that	O
there	O
is	O
no	O
other	O
occurrence	O
of	O
the	O
target	O
sequence	B
in	O
the	O
whole	O
genome	B
of	O
length	O
n	O
nucleotides	O
is	O
roughly	O
which	O
is	O
close	O
to	O
one	O
only	O
if	O
that	O
is	O
n	O
using	O
n	O
we	O
require	O
the	O
recognized	O
sequence	B
to	O
be	O
longer	O
than	O
lmin	O
nucleotides	O
l	O
log	O
n	O
log	O
what	O
size	O
of	O
protein	B
does	O
this	O
imply	O
a	O
weak	O
lower	O
bound	B
can	O
be	O
obtained	O
by	O
assuming	O
that	O
the	O
information	B
content	I
of	O
the	O
protein	B
sequence	B
itself	O
is	O
greater	O
than	O
the	O
information	B
content	I
of	O
the	O
nucleotide	B
sequence	B
the	O
protein	B
prefers	O
to	O
bind	O
to	O
we	O
have	O
argued	O
above	O
must	O
be	O
at	O
least	O
bits	O
this	O
gives	O
a	O
minimum	O
protein	B
length	O
of	O
amino	O
acids	O
thinking	O
realistically	O
the	O
recognition	B
of	O
the	O
dna	B
sequence	B
by	O
the	O
protein	B
presumably	O
involves	O
the	O
protein	B
coming	O
into	O
contact	O
with	O
all	O
sixteen	O
nucleotides	O
in	O
the	O
target	O
sequence	B
if	O
the	O
protein	B
is	O
a	O
monomer	O
it	O
must	O
be	O
big	O
enough	O
that	O
it	O
can	O
simultaneously	O
make	O
contact	O
with	O
sixteen	O
nucleotides	O
of	O
dna	B
one	O
helical	O
turn	O
of	O
dna	B
containing	O
ten	O
nucleotides	O
has	O
a	O
length	O
of	O
nm	O
so	O
a	O
contiguous	O
sequence	B
of	O
sixteen	O
nucleotides	O
has	O
a	O
length	O
of	O
nm	O
the	O
diameter	O
of	O
the	O
protein	B
must	O
therefore	O
be	O
about	O
nm	O
or	O
greater	O
egg-white	O
lysozyme	O
is	O
a	O
small	O
globular	O
protein	B
with	O
a	O
length	O
of	O
amino	O
acids	O
and	O
a	O
diameter	O
of	O
about	O
nm	O
assuming	O
that	O
volume	B
is	O
proportional	O
to	O
sequence	B
length	O
and	O
that	O
volume	B
scales	O
as	O
the	O
cube	O
of	O
the	O
diameter	O
a	O
protein	B
of	O
diameter	O
nm	O
must	O
have	O
a	O
sequence	B
of	O
length	O
amino	O
acids	O
if	O
however	O
a	O
target	O
sequence	B
consists	O
of	O
a	O
twice-repeated	O
sub-sequence	O
we	O
can	O
get	O
by	O
with	O
a	O
much	O
smaller	O
protein	B
that	O
recognizes	O
only	O
the	O
sub-sequence	O
and	O
that	O
binds	O
to	O
the	O
dna	B
strongly	O
only	O
if	O
it	O
can	O
form	O
a	O
dimer	B
both	O
halves	O
of	O
which	O
are	O
bound	B
to	O
the	O
recognized	O
sequence	B
halving	O
the	O
diameter	O
of	O
the	O
protein	B
we	O
now	O
only	O
need	O
a	O
protein	B
whose	O
length	O
is	O
greater	O
than	O
amino	O
acids	O
a	O
protein	B
of	O
length	O
smaller	O
than	O
this	O
cannot	O
by	O
itself	O
serve	O
as	O
a	O
regulatory	B
protein	B
to	O
one	O
gene	O
because	O
it	O
s	O
simply	O
too	O
small	O
to	O
be	O
able	O
to	O
make	O
a	O
match	O
its	O
available	O
surface	O
does	O
not	O
have	O
enough	O
information	B
content	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
in	O
chapters	O
we	O
established	O
shannon	B
s	O
noisy-channel	B
coding	B
theorem	I
for	O
a	O
general	O
channel	O
with	O
any	O
input	O
and	O
output	O
alphabets	O
a	O
great	O
deal	O
of	O
attention	O
in	O
coding	B
theory	I
focuses	O
on	O
the	O
special	O
case	O
of	O
channels	O
with	O
binary	O
inputs	O
constraining	O
ourselves	O
to	O
these	O
channels	O
matters	O
and	O
leads	O
us	O
into	O
an	O
exceptionally	O
rich	O
world	O
which	O
we	O
will	O
only	O
taste	O
in	O
this	O
book	O
one	O
of	O
the	O
aims	O
of	O
this	O
chapter	O
is	O
to	O
point	O
out	O
a	O
contrast	O
between	O
shannon	B
s	O
aim	O
of	O
achieving	O
reliable	O
communication	B
over	O
a	O
noisy	B
channel	O
and	O
the	O
apparent	O
aim	O
of	O
many	O
in	O
the	O
world	O
of	O
coding	B
theory	I
many	O
coding	O
theorists	O
take	O
as	O
their	O
fundamental	O
problem	O
the	O
task	O
of	O
packing	O
as	O
many	O
spheres	O
as	O
possible	O
with	O
radius	O
as	O
large	O
as	O
possible	O
into	O
an	O
n	O
space	O
with	O
no	O
spheres	O
overlapping	O
prizes	O
are	O
awarded	O
to	O
people	O
who	O
packings	O
that	O
squeeze	O
in	O
an	O
extra	O
few	O
spheres	O
while	O
this	O
is	O
a	O
fascinating	O
mathematical	O
topic	O
we	O
shall	O
see	O
that	O
the	O
aim	O
of	O
maximizing	O
the	O
distance	B
between	O
codewords	O
in	O
a	O
code	O
has	O
only	O
a	O
tenuous	O
relationship	O
to	O
shannon	B
s	O
aim	O
of	O
reliable	O
communication	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
we	O
ve	O
established	O
shannon	B
s	O
noisy-channel	B
coding	B
theorem	I
for	O
a	O
general	O
channel	O
with	O
any	O
input	O
and	O
output	O
alphabets	O
a	O
great	O
deal	O
of	O
attention	O
in	O
coding	B
theory	I
focuses	O
on	O
the	O
special	O
case	O
of	O
channels	O
with	O
binary	O
inputs	O
the	O
implicit	O
choice	O
being	O
the	O
binary	B
symmetric	B
channel	I
the	O
optimal	B
decoder	B
for	O
a	O
code	O
given	O
a	O
binary	B
symmetric	B
channel	I
the	O
codeword	B
that	O
is	O
closest	O
to	O
the	O
received	O
vector	O
closest	O
in	O
hamming	B
distance	B
the	O
hamming	B
distance	B
between	O
two	O
binary	O
vectors	B
is	O
the	O
number	O
of	O
coordinates	O
in	O
which	O
the	O
two	O
vectors	B
decoding	B
errors	B
will	O
occur	O
if	O
the	O
noise	O
takes	O
us	O
from	O
the	O
transmitted	O
codeword	B
t	O
to	O
a	O
received	O
vector	O
r	O
that	O
is	O
closer	O
to	O
some	O
other	O
codeword	B
the	O
distances	O
between	O
codewords	O
are	O
thus	O
relevant	O
to	O
the	O
probability	B
of	O
a	O
decoding	B
error	O
distance	B
properties	O
of	O
a	O
code	O
the	O
distance	B
of	O
a	O
code	O
is	O
the	O
smallest	O
separation	B
between	O
two	O
of	O
its	O
codewords	O
example	O
the	O
hamming	B
code	I
has	O
distance	B
d	O
all	O
pairs	O
of	O
its	O
codewords	O
in	O
at	O
least	O
bits	O
the	O
maximum	O
number	O
of	O
errors	B
it	O
can	O
correct	O
is	O
t	O
in	O
general	O
a	O
code	O
with	O
distance	B
d	O
is	O
a	O
more	O
precise	O
term	O
for	O
distance	B
is	O
the	O
minimum	O
distance	B
of	O
the	O
code	O
the	O
distance	B
of	O
a	O
code	O
is	O
often	O
denoted	O
by	O
d	O
or	O
dmin	O
we	O
ll	O
now	O
constrain	O
our	O
attention	O
to	O
linear	B
codes	I
in	O
a	O
linear	B
code	O
all	O
codewords	O
have	O
identical	O
distance	B
properties	O
so	O
we	O
can	O
summarize	O
all	O
the	O
distances	O
between	O
the	O
code	O
s	O
codewords	O
by	O
counting	B
the	O
distances	O
from	O
the	O
all-zero	O
codeword	B
the	O
weight	B
enumerator	I
function	O
of	O
a	O
code	O
aw	O
is	O
to	O
be	O
the	O
number	O
of	O
codewords	O
in	O
the	O
code	O
that	O
have	O
weight	O
w	O
the	O
weight	B
enumerator	I
function	O
is	O
also	O
known	O
as	O
the	O
distance	B
distribution	B
of	O
the	O
code	O
example	O
the	O
weight	B
enumerator	I
functions	B
of	O
the	O
hamming	B
code	I
and	O
the	O
dodecahedron	B
code	I
are	O
shown	O
in	O
and	O
obsession	O
with	O
distance	B
since	O
the	O
maximum	O
number	O
of	O
errors	B
that	O
a	O
code	O
can	O
guarantee	O
to	O
correct	O
t	O
is	O
related	O
to	O
its	O
distance	B
d	O
by	O
t	O
many	O
coding	O
theorists	O
focus	B
on	O
the	O
distance	B
of	O
a	O
code	O
searching	O
for	O
codes	O
of	O
a	O
given	O
size	O
that	O
have	O
the	O
biggest	O
possible	O
distance	B
much	O
of	O
practical	B
coding	B
theory	I
has	O
focused	O
on	O
decoders	O
that	O
give	O
the	O
optimal	B
decoding	B
for	O
all	O
error	O
patterns	O
of	O
weight	O
up	O
to	O
the	O
half-distance	O
t	O
of	O
their	O
codes	O
example	O
the	O
hamming	B
distance	B
between	O
and	O
is	O
w	O
aw	O
total	O
figure	O
the	O
graph	B
of	O
the	O
hamming	B
code	I
and	O
its	O
weight	B
enumerator	I
function	O
d	O
if	O
d	O
is	O
odd	O
and	O
d	O
if	O
d	O
is	O
even	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
obsession	O
with	O
distance	B
w	O
aw	O
total	O
figure	O
the	O
graph	B
the	O
dodecahedron	B
code	I
circles	O
are	O
the	O
transmitted	O
bits	O
and	O
the	O
triangles	O
are	O
the	O
parity	B
checks	O
one	O
of	O
which	O
is	O
redundant	O
and	O
the	O
weight	B
enumerator	I
function	O
lines	O
the	O
dotted	O
lines	O
show	O
the	O
average	O
weight	B
enumerator	I
function	O
of	O
all	O
random	B
linear	B
codes	I
with	O
the	O
same	O
size	O
of	O
generator	B
matrix	B
which	O
will	O
be	O
computed	O
shortly	O
the	O
lower	O
shows	O
the	O
same	O
functions	B
on	O
a	O
log	O
scale	O
a	O
bounded-distance	B
decoder	B
is	O
a	O
decoder	B
that	O
returns	O
the	O
closest	O
codeword	B
to	O
a	O
received	O
binary	O
vector	O
r	O
if	O
the	O
distance	B
from	O
r	O
to	O
that	O
codeword	B
is	O
less	O
than	O
or	O
equal	O
to	O
t	O
otherwise	O
it	O
returns	O
a	O
failure	O
message	O
the	O
rationale	O
for	O
not	O
trying	O
to	O
decode	O
when	O
more	O
than	O
t	O
errors	B
have	O
occurred	O
might	O
be	O
we	O
can	O
t	O
guarantee	O
that	O
we	O
can	O
correct	O
more	O
than	O
t	O
errors	B
so	O
we	O
won	O
t	O
bother	O
trying	O
who	O
would	O
be	O
interested	O
in	O
a	O
decoder	B
that	O
corrects	O
some	O
error	O
patterns	O
of	O
weight	O
greater	O
than	O
t	O
but	O
not	O
others	O
this	O
defeatist	O
attitude	O
is	O
an	O
example	O
of	O
worst-case-ism	B
a	O
widespread	O
mental	O
ailment	O
which	O
this	O
book	O
is	O
intended	O
to	O
cure	O
the	O
fact	O
is	O
that	O
bounded-distance	B
decoders	O
cannot	O
reach	O
the	O
shannon	B
limit	O
of	O
the	O
binary	B
symmetric	B
channel	I
only	O
a	O
decoder	B
that	O
often	O
corrects	O
more	O
than	O
t	O
errors	B
can	O
do	O
this	O
the	O
state	O
of	O
the	O
art	O
in	O
error-correcting	B
codes	I
have	O
decoders	O
that	O
work	O
way	O
beyond	O
the	O
minimum	O
distance	B
of	O
the	O
code	O
of	O
good	B
and	O
bad	B
distance	B
properties	O
given	O
a	O
family	O
of	O
codes	O
of	O
increasing	O
blocklength	O
n	O
and	O
with	O
rates	O
approaching	O
a	O
limit	O
r	O
we	O
may	O
be	O
able	O
to	O
put	O
that	O
family	O
in	O
one	O
of	O
the	O
following	O
categories	O
which	O
have	O
some	O
similarities	O
to	O
the	O
categories	O
of	O
good	B
and	O
bad	B
codes	O
earlier	O
a	O
sequence	B
of	O
codes	O
has	O
good	B
distance	B
if	O
dn	O
tends	O
to	O
a	O
constant	O
greater	O
than	O
zero	O
a	O
sequence	B
of	O
codes	O
has	O
bad	B
distance	B
if	O
dn	O
tends	O
to	O
zero	O
a	O
sequence	B
of	O
codes	O
has	O
very	B
bad	B
distance	B
if	O
d	O
tends	O
to	O
a	O
constant	O
example	O
a	O
low-density	O
generator-matrix	O
code	O
is	O
a	O
linear	B
code	O
whose	O
n	O
generator	B
matrix	B
g	O
has	O
a	O
small	O
number	O
of	O
per	O
row	O
regardless	O
of	O
how	O
big	O
n	O
is	O
the	O
minimum	O
distance	B
of	O
such	O
a	O
code	O
is	O
at	O
most	O
so	O
low-density	O
generator-matrix	O
codes	O
have	O
very	B
bad	B
distance	B
while	O
having	O
large	O
distance	B
is	O
no	O
bad	B
thing	O
we	O
ll	O
see	O
later	O
on	O
why	O
an	O
emphasis	O
on	O
distance	B
can	O
be	O
unhealthy	O
figure	O
the	O
graph	B
of	O
a	O
low-density	O
generator-matrix	O
code	O
the	O
rightmost	O
m	O
of	O
the	O
transmitted	O
bits	O
are	O
each	O
connected	O
to	O
a	O
single	O
distinct	O
parity	B
constraint	O
the	O
leftmost	O
k	O
transmitted	O
bits	O
are	O
each	O
connected	O
to	O
a	O
small	O
number	O
of	O
parity	B
constraints	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
figure	O
schematic	O
picture	O
of	O
part	O
of	O
hamming	B
space	O
perfectly	O
by	O
t-spheres	O
centred	O
on	O
the	O
codewords	O
of	O
a	O
perfect	B
code	I
t	O
t	O
t	O
perfect	B
codes	O
a	O
t-sphere	O
a	O
sphere	O
of	O
radius	O
t	O
in	O
hamming	B
space	O
centred	O
on	O
a	O
point	O
x	O
is	O
the	O
set	B
of	O
points	O
whose	O
hamming	B
distance	B
from	O
x	O
is	O
less	O
than	O
or	O
equal	O
to	O
t	O
the	O
hamming	B
code	I
has	O
the	O
beautiful	O
property	O
that	O
if	O
we	O
place	O
about	O
each	O
of	O
its	O
codewords	O
those	O
spheres	O
perfectly	O
hamming	B
space	O
without	O
overlapping	O
as	O
we	O
saw	O
in	O
chapter	O
every	O
binary	O
vector	O
of	O
length	O
is	O
within	O
a	O
distance	B
of	O
t	O
of	O
exactly	O
one	O
codeword	B
of	O
the	O
hamming	B
code	I
a	O
code	O
is	O
a	O
perfect	B
t-error-correcting	O
code	O
if	O
the	O
set	B
of	O
t-spheres	O
centred	O
on	O
the	O
codewords	O
of	O
the	O
code	O
the	O
hamming	B
space	O
without	O
overlapping	O
let	O
s	O
recap	O
our	O
cast	O
of	O
characters	O
the	O
number	O
of	O
codewords	O
is	O
s	O
the	O
number	O
of	O
points	O
in	O
the	O
entire	O
hamming	B
space	O
is	O
the	O
number	O
of	O
points	O
in	O
a	O
hamming	B
sphere	O
of	O
radius	O
t	O
is	O
t	O
for	O
a	O
code	O
to	O
be	O
perfect	B
with	O
these	O
parameters	B
we	O
require	O
s	O
times	O
the	O
number	O
of	O
points	O
in	O
the	O
t-sphere	O
to	O
equal	O
for	O
a	O
perfect	B
code	I
or	O
equivalently	O
t	O
t	O
for	O
a	O
perfect	B
code	I
the	O
number	O
of	O
noise	O
vectors	B
in	O
one	O
sphere	O
must	O
equal	O
the	O
number	O
of	O
possible	O
syndromes	O
the	O
hamming	B
code	I
this	O
numerological	O
condition	O
because	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
perfect	B
codes	O
t	O
t	O
t	O
t	O
figure	O
schematic	O
picture	O
of	O
hamming	B
space	O
not	O
perfectly	O
by	O
t-spheres	O
centred	O
on	O
the	O
codewords	O
of	O
a	O
code	O
the	O
grey	O
regions	O
show	O
points	O
that	O
are	O
at	O
a	O
hamming	B
distance	B
of	O
more	O
than	O
t	O
from	O
any	O
codeword	B
this	O
is	O
a	O
misleading	O
picture	O
as	O
for	O
any	O
code	O
with	O
large	O
t	O
in	O
high	B
dimensions	B
the	O
grey	O
space	O
between	O
the	O
spheres	O
takes	O
up	O
almost	O
all	O
of	O
hamming	B
space	O
how	O
happy	O
we	O
would	O
be	O
to	O
use	O
perfect	B
codes	O
if	O
there	O
were	O
large	O
numbers	O
of	O
perfect	B
codes	O
to	O
choose	O
from	O
with	O
a	O
wide	O
range	O
of	O
blocklengths	O
and	O
rates	O
then	O
these	O
would	O
be	O
the	O
perfect	B
solution	O
to	O
shannon	B
s	O
problem	O
we	O
could	O
communicate	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
f	O
for	O
example	O
by	O
picking	O
a	O
perfect	B
t-error-correcting	O
code	O
with	O
blocklength	O
n	O
and	O
t	O
f	O
where	O
f	O
and	O
n	O
and	O
are	O
chosen	O
such	O
that	O
the	O
probability	B
that	O
the	O
noise	O
more	O
than	O
t	O
bits	O
is	O
satisfactorily	O
small	O
however	O
there	O
are	O
almost	O
no	O
perfect	B
codes	O
the	O
only	O
nontrivial	O
perfect	B
binary	O
codes	O
are	O
the	O
hamming	B
codes	O
which	O
are	O
perfect	B
codes	O
with	O
t	O
and	O
blocklength	O
n	O
below	O
the	O
rate	B
of	O
a	O
hamming	B
code	I
approaches	O
as	O
its	O
blocklength	O
n	O
increases	O
the	O
repetition	B
codes	O
of	O
odd	O
blocklength	O
n	O
which	O
are	O
perfect	B
codes	O
with	O
t	O
the	O
rate	B
of	O
repetition	B
codes	O
goes	O
to	O
zero	O
as	O
and	O
one	O
remarkable	O
code	O
with	O
codewords	O
of	O
blocklength	O
n	O
known	O
as	O
the	O
binary	O
golay	B
code	I
second	O
golay	B
code	I
of	O
length	O
n	O
over	O
a	O
ternary	O
alphabet	O
was	O
discovered	O
by	O
a	O
finnish	O
football-pool	O
enthusiast	O
called	O
juhani	O
virtakallio	B
in	O
there	O
are	O
no	O
other	O
binary	O
perfect	B
codes	O
why	O
this	O
shortage	O
of	O
perfect	B
codes	O
is	O
it	O
because	O
precise	O
numerological	O
coincidences	O
like	O
those	O
by	O
the	O
parameters	B
of	O
the	O
hamming	B
code	I
and	O
the	O
golay	B
code	I
are	O
rare	O
are	O
there	O
plenty	O
of	O
almost-perfect	O
codes	O
for	O
which	O
the	O
t-spheres	O
almost	O
the	O
whole	O
space	O
no	O
in	O
fact	O
the	O
picture	O
of	O
hamming	B
spheres	O
centred	O
on	O
the	O
codewords	O
almost	O
hamming	B
space	O
is	O
a	O
misleading	O
one	O
for	O
most	O
codes	O
whether	O
they	O
are	O
good	B
codes	O
or	O
bad	B
codes	O
almost	O
all	O
the	O
hamming	B
space	O
is	O
taken	O
up	O
by	O
the	O
space	O
between	O
t-spheres	O
is	O
shown	O
in	O
grey	O
in	O
having	O
established	O
this	O
gloomy	O
picture	O
we	O
spend	O
a	O
moment	O
in	O
the	O
properties	O
of	O
the	O
perfect	B
codes	O
mentioned	O
above	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
figure	O
three	O
codewords	O
wn	O
xn	O
un	O
vn	O
n	O
the	O
hamming	B
codes	O
the	O
hamming	B
code	I
can	O
be	O
as	O
the	O
linear	B
code	O
whose	O
paritycheck	O
matrix	B
contains	O
as	O
its	O
columns	O
all	O
the	O
non-zero	O
vectors	B
of	O
length	O
since	O
these	O
vectors	B
are	O
all	O
any	O
single	O
produces	O
a	O
distinct	O
syndrome	B
so	O
all	O
single-bit	O
errors	B
can	O
be	O
detected	O
and	O
corrected	O
we	O
can	O
generalize	O
this	O
code	O
with	O
m	O
parity	B
constraints	O
as	O
follows	O
the	O
hamming	B
codes	O
are	O
single-error-correcting	O
codes	O
by	O
picking	O
a	O
number	O
of	O
parity-check	B
constraints	I
m	O
the	O
blocklength	O
n	O
is	O
n	O
the	O
paritycheck	O
matrix	B
contains	O
as	O
its	O
columns	O
all	O
the	O
n	O
non-zero	O
vectors	B
of	O
length	O
m	O
bits	O
the	O
few	O
hamming	B
codes	O
have	O
the	O
following	O
rates	O
checks	O
m	O
k	O
r	O
kn	O
repetition	B
code	I
hamming	B
code	I
exercise	O
what	O
is	O
the	O
probability	B
of	I
block	B
error	I
of	O
the	O
k	O
hamming	B
code	I
to	O
leading	O
order	O
when	O
the	O
code	O
is	O
used	O
for	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
density	B
f	O
perfectness	O
is	O
unattainable	O
proof	O
we	O
will	O
show	O
in	O
several	O
ways	O
that	O
useful	O
perfect	B
codes	O
do	O
not	O
exist	O
useful	O
means	O
having	O
large	O
blocklength	O
n	O
and	O
rate	B
close	O
neither	O
to	O
nor	O
shannon	B
proved	O
that	O
given	O
a	O
binary	B
symmetric	B
channel	I
with	O
any	O
noise	O
level	O
f	O
there	O
exist	O
codes	O
with	O
large	O
blocklength	O
n	O
and	O
rate	B
as	O
close	O
as	O
you	O
like	O
to	O
cf	O
that	O
enable	O
communication	B
with	O
arbitrarily	O
small	O
error	B
probability	B
for	O
large	O
n	O
the	O
number	O
of	O
errors	B
per	O
block	B
will	O
typically	O
be	O
about	O
fn	O
so	O
these	O
codes	O
of	O
shannon	B
are	O
almost-certainly-fn	O
codes	O
let	O
s	O
pick	O
the	O
special	O
case	O
of	O
a	O
noisy	B
channel	O
with	O
f	O
can	O
we	O
a	O
large	O
perfect	B
code	I
that	O
is	O
fn	O
well	O
let	O
s	O
suppose	O
that	O
such	O
a	O
code	O
has	O
been	O
found	O
and	O
examine	O
just	O
three	O
of	O
its	O
codewords	O
that	O
the	O
code	O
ought	O
to	O
have	O
rate	B
r	O
so	O
it	O
should	O
have	O
an	O
enormous	O
number	O
r	O
of	O
codewords	O
without	O
loss	O
of	O
generality	O
we	O
choose	O
one	O
of	O
the	O
codewords	O
to	O
be	O
the	O
all-zero	O
codeword	B
and	O
the	O
other	O
two	O
to	O
have	O
overlaps	O
with	O
it	O
as	O
shown	O
in	O
the	O
second	O
codeword	B
from	O
the	O
in	O
a	O
fraction	O
u	O
v	O
of	O
its	O
coordinates	O
the	O
third	O
codeword	B
from	O
the	O
in	O
a	O
fraction	O
v	O
w	O
and	O
from	O
the	O
second	O
in	O
a	O
fraction	O
u	O
w	O
a	O
fraction	O
x	O
of	O
the	O
coordinates	O
have	O
value	O
zero	O
in	O
all	O
three	O
codewords	O
now	O
if	O
the	O
code	O
is	O
fn	O
its	O
minimum	O
distance	B
must	O
be	O
greater	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
weight	B
enumerator	I
function	O
of	O
random	B
linear	B
codes	I
than	O
so	O
u	O
v	O
v	O
w	O
and	O
u	O
w	O
summing	O
these	O
three	O
inequalities	O
and	O
dividing	O
by	O
two	O
we	O
have	O
u	O
v	O
w	O
so	O
if	O
f	O
we	O
can	O
deduce	O
u	O
v	O
w	O
so	O
that	O
x	O
which	O
is	O
impossible	O
such	O
a	O
code	O
cannot	O
exist	O
so	O
the	O
code	O
cannot	O
have	O
three	O
codewords	O
let	O
alone	O
r	O
we	O
conclude	O
that	O
whereas	O
shannon	B
proved	O
there	O
are	O
plenty	O
of	O
codes	O
for	O
communicating	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
f	O
there	O
are	O
no	O
perfect	B
codes	O
that	O
can	O
do	O
this	O
we	O
now	O
study	O
a	O
more	O
general	O
argument	O
that	O
indicates	O
that	O
there	O
are	O
no	O
large	O
perfect	B
linear	B
codes	I
for	O
general	O
rates	O
than	O
and	O
we	O
do	O
this	O
by	O
the	O
typical	B
distance	B
of	O
a	O
random	B
linear	B
code	O
weight	B
enumerator	I
function	O
of	O
random	B
linear	B
codes	I
imagine	O
making	O
a	O
code	O
by	O
picking	O
the	O
binary	O
entries	O
in	O
the	O
parity-check	B
matrix	B
h	O
at	O
random	B
what	O
weight	B
enumerator	I
function	O
should	O
we	O
expect	O
the	O
weight	B
enumerator	I
of	O
one	O
particular	O
code	O
with	O
parity-check	B
matrix	B
h	O
awh	O
is	O
the	O
number	O
of	O
codewords	O
of	O
weight	O
w	O
which	O
can	O
be	O
written	O
awh	O
xxjxjw	O
where	O
the	O
sum	O
is	O
over	O
all	O
vectors	B
x	O
whose	O
weight	O
is	O
w	O
and	O
the	O
truth	B
function	I
equals	O
one	O
if	O
hx	O
and	O
zero	O
otherwise	O
we	O
can	O
the	O
expected	O
value	O
of	O
aw	O
n	O
m	O
figure	O
a	O
random	B
binary	O
parity-check	B
matrix	B
hawi	O
xh	O
p	O
xxjxjwxh	O
p	O
by	O
evaluating	O
the	O
probability	B
that	O
a	O
particular	O
word	O
of	O
weight	O
w	O
is	O
a	O
codeword	B
of	O
the	O
code	O
over	O
all	O
binary	O
linear	B
codes	I
in	O
our	O
ensemble	B
by	O
symmetry	O
this	O
probability	B
depends	O
only	O
on	O
the	O
weight	O
w	O
of	O
the	O
word	O
not	O
on	O
the	O
details	O
of	O
the	O
word	O
the	O
probability	B
that	O
the	O
entire	O
syndrome	B
hx	O
is	O
zero	O
can	O
be	O
found	O
by	O
multiplying	O
together	O
the	O
probabilities	O
that	O
each	O
of	O
the	O
m	O
bits	O
in	O
the	O
syndrome	B
is	O
zero	O
each	O
bit	B
zm	O
of	O
the	O
syndrome	B
is	O
a	O
sum	O
of	O
w	O
random	B
bits	O
so	O
the	O
probability	B
that	O
zm	O
is	O
the	O
probability	B
that	O
hx	O
is	O
thus	O
p	O
xh	O
independent	O
of	O
w	O
the	O
expected	O
number	O
of	O
words	O
of	O
weight	O
w	O
is	O
given	O
by	O
summing	O
over	O
all	O
words	O
of	O
weight	O
w	O
the	O
probability	B
that	O
each	O
word	O
is	O
a	O
codeword	B
the	O
number	O
of	O
words	O
of	O
weight	O
w	O
is	O
hawi	O
so	O
for	O
any	O
w	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
figure	O
the	O
expected	O
weight	B
enumerator	I
function	O
hawi	O
of	O
a	O
random	B
linear	B
code	O
with	O
n	O
and	O
m	O
lower	O
shows	O
hawi	O
on	O
a	O
logarithmic	O
scale	O
capacity	B
r	O
gv	O
f	O
figure	O
contrast	O
between	O
shannon	B
s	O
channel	O
capacity	B
c	O
and	O
the	O
gilbert	O
rate	B
rgv	O
the	O
maximum	O
communication	B
rate	B
achievable	O
using	O
a	O
bounded-distance	B
decoder	B
as	O
a	O
function	O
of	O
noise	O
level	O
f	O
for	O
any	O
given	O
rate	B
r	O
the	O
maximum	O
tolerable	O
noise	O
level	O
for	O
shannon	B
is	O
twice	O
as	O
big	O
as	O
the	O
maximum	O
tolerable	O
noise	O
level	O
for	O
a	O
worst-case-ist	O
who	O
uses	O
a	O
bounded-distance	B
decoder	B
for	O
large	O
n	O
we	O
can	O
use	O
n	O
and	O
r	O
mn	O
to	O
write	O
n	O
m	O
n	O
r	O
for	O
any	O
w	O
as	O
a	O
concrete	O
example	O
shows	O
the	O
expected	O
weight	B
enumerator	I
function	O
of	O
a	O
random	B
linear	B
code	O
with	O
n	O
and	O
m	O
gilbertvarshamov	O
distance	B
for	O
weights	O
w	O
such	O
that	O
r	O
the	O
expectation	B
of	O
aw	O
is	O
smaller	O
than	O
for	O
weights	O
such	O
that	O
r	O
the	O
expectation	B
is	O
greater	O
than	O
we	O
thus	O
expect	O
for	O
large	O
n	O
that	O
the	O
minimum	O
distance	B
of	O
a	O
random	B
linear	B
code	O
will	O
be	O
close	O
to	O
the	O
distance	B
dgv	O
by	O
r	O
this	O
distance	B
dgv	O
n	O
distance	B
for	O
rate	B
r	O
and	O
blocklength	O
n	O
r	O
is	O
the	O
gilbertvarshamov	O
the	O
gilbertvarshamov	O
conjecture	O
widely	O
believed	O
asserts	O
that	O
large	O
n	O
it	O
is	O
not	O
possible	O
to	O
create	O
binary	O
codes	O
with	O
minimum	O
distance	B
greater	O
than	O
dgv	O
the	O
gilbertvarshamov	O
rate	B
rgv	O
is	O
the	O
maximum	O
rate	B
at	O
which	O
you	O
can	O
reliably	O
communicate	O
with	O
a	O
bounded-distance	B
decoder	B
on	O
assuming	O
that	O
the	O
gilbertvarshamov	O
conjecture	O
is	O
true	O
why	O
sphere-packing	B
is	O
a	O
bad	B
perspective	O
and	O
an	O
obsession	O
with	O
distance	B
is	O
inappropriate	O
if	O
one	O
uses	O
a	O
bounded-distance	B
decoder	B
the	O
maximum	O
tolerable	O
noise	O
level	O
will	O
a	O
fraction	O
fbd	O
dminn	O
of	O
the	O
bits	O
so	O
assuming	O
dmin	O
is	O
equal	O
to	O
the	O
gilbert	O
distance	B
dgv	O
we	O
have	O
rgv	O
rgv	O
now	O
here	O
s	O
the	O
crunch	O
what	O
did	O
shannon	B
say	O
is	O
achievable	O
he	O
said	O
the	O
maximum	O
possible	O
rate	B
of	O
communication	B
is	O
the	O
capacity	B
c	O
so	O
for	O
a	O
given	O
rate	B
r	O
the	O
maximum	O
tolerable	O
noise	O
level	O
according	O
to	O
shannon	B
is	O
given	O
by	O
r	O
our	O
conclusion	O
imagine	O
a	O
good	B
code	O
of	O
rate	B
r	O
has	O
been	O
chosen	O
equations	O
and	O
respectively	O
the	O
maximum	O
noise	O
levels	O
tolerable	O
by	O
a	O
bounded-distance	B
decoder	B
fbd	O
and	O
by	O
shannon	B
s	O
decoder	B
f	O
fbd	O
f	O
bounded-distance	B
decoders	O
can	O
only	O
ever	O
cope	O
with	O
half	O
the	O
noise-level	O
that	O
shannon	B
proved	O
is	O
tolerable	O
how	O
does	O
this	O
relate	O
to	O
perfect	B
codes	O
a	O
code	O
is	O
perfect	B
if	O
there	O
are	O
tspheres	O
around	O
its	O
codewords	O
that	O
hamming	B
space	O
without	O
overlapping	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
berlekamp	B
s	O
bats	O
figure	O
two	O
overlapping	O
spheres	O
whose	O
radius	O
is	O
almost	O
as	O
big	O
as	O
the	O
distance	B
between	O
their	O
centres	O
but	O
when	O
a	O
typical	B
random	B
linear	B
code	O
is	O
used	O
to	O
communicate	O
over	O
a	O
binary	B
symmetric	B
channel	I
near	O
to	O
the	O
shannon	B
limit	O
the	O
typical	B
number	O
of	O
bits	O
is	O
fn	O
and	O
the	O
minimum	O
distance	B
between	O
codewords	O
is	O
also	O
fn	O
or	O
a	O
little	O
bigger	O
if	O
we	O
are	O
a	O
little	O
below	O
the	O
shannon	B
limit	O
so	O
the	O
fn	O
around	O
the	O
codewords	O
overlap	O
with	O
each	O
other	O
that	O
each	O
sphere	O
almost	O
contains	O
the	O
centre	O
of	O
its	O
nearest	O
neighbour	O
the	O
reason	O
why	O
this	O
overlap	O
is	O
not	O
disastrous	O
is	O
because	O
in	O
high	B
dimensions	B
the	O
volume	B
associated	O
with	O
the	O
overlap	O
shown	O
shaded	O
in	O
is	O
a	O
tiny	O
fraction	O
of	O
either	O
sphere	O
so	O
the	O
probability	B
of	O
landing	O
in	O
it	O
is	O
extremely	O
small	O
the	O
moral	O
of	O
the	O
story	O
is	O
that	O
worst-case-ism	B
can	O
be	O
bad	B
for	O
you	O
halving	O
your	O
ability	O
to	O
tolerate	O
noise	O
you	O
have	O
to	O
be	O
able	O
to	O
decode	O
way	O
beyond	O
the	O
minimum	O
distance	B
of	O
a	O
code	O
to	O
get	O
to	O
the	O
shannon	B
limit	O
nevertheless	O
the	O
minimum	O
distance	B
of	O
a	O
code	O
is	O
of	O
interest	O
in	O
practice	O
because	O
under	O
some	O
conditions	O
the	O
minimum	O
distance	B
dominates	O
the	O
errors	B
made	O
by	O
a	O
code	O
berlekamp	B
s	O
bats	O
a	O
blind	O
bat	B
lives	O
in	O
a	O
cave	B
it	O
about	O
the	O
centre	O
of	O
the	O
cave	B
which	O
corresponds	O
to	O
one	O
codeword	B
with	O
its	O
typical	B
distance	B
from	O
the	O
centre	O
controlled	O
by	O
a	O
friskiness	O
parameter	O
f	O
displacement	O
of	O
the	O
bat	B
from	O
the	O
centre	O
corresponds	O
to	O
the	O
noise	O
vector	O
the	O
boundaries	O
of	O
the	O
cave	B
are	O
made	O
up	O
of	O
stalactites	O
that	O
point	O
in	O
towards	O
the	O
centre	O
of	O
the	O
cave	B
each	O
stalactite	B
is	O
analogous	O
to	O
the	O
boundary	O
between	O
the	O
home	O
codeword	B
and	O
another	O
codeword	B
the	O
stalactite	B
is	O
like	O
the	O
shaded	O
region	O
in	O
but	O
reshaped	O
to	O
convey	O
the	O
idea	O
that	O
it	O
is	O
a	O
region	O
of	O
very	O
small	O
volume	B
decoding	B
errors	B
correspond	O
to	O
the	O
bat	B
s	O
intended	O
trajectory	O
passing	O
inside	O
a	O
stalactite	B
collisions	O
with	O
stalactites	O
at	O
various	O
distances	O
from	O
the	O
centre	O
are	O
possible	O
if	O
the	O
friskiness	O
is	O
very	O
small	O
the	O
bat	B
is	O
usually	O
very	O
close	O
to	O
the	O
centre	O
of	O
the	O
cave	B
collisions	O
will	O
be	O
rare	O
and	O
when	O
they	O
do	O
occur	O
they	O
will	O
usually	O
involve	O
the	O
stalactites	O
whose	O
tips	O
are	O
closest	O
to	O
the	O
centre	O
point	O
similarly	O
under	O
low-noise	O
conditions	O
decoding	B
errors	B
will	O
be	O
rare	O
and	O
they	O
will	O
typically	O
involve	O
low-weight	O
codewords	O
under	O
low-noise	O
conditions	O
the	O
minimum	O
distance	B
of	O
a	O
code	O
is	O
relevant	O
to	O
the	O
small	O
probability	B
of	I
error	I
figure	O
berlekamp	B
s	O
schematic	O
picture	O
of	O
hamming	B
space	O
in	O
the	O
vicinity	O
of	O
a	O
codeword	B
the	O
jagged	O
solid	O
line	O
encloses	O
all	O
points	O
to	O
which	O
this	O
codeword	B
is	O
the	O
closest	O
the	O
t-sphere	O
around	O
the	O
codeword	B
takes	O
up	O
a	O
small	O
fraction	O
of	O
this	O
space	O
t	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
if	O
the	O
friskiness	O
is	O
higher	O
the	O
bat	B
may	O
often	O
make	O
excursions	O
beyond	O
the	O
safe	O
distance	B
t	O
where	O
the	O
longest	O
stalactites	O
start	O
but	O
it	O
will	O
collide	O
most	O
frequently	O
with	O
more	O
distant	O
stalactites	O
owing	O
to	O
their	O
greater	O
number	O
there	O
s	O
only	O
a	O
tiny	O
number	O
of	O
stalactites	O
at	O
the	O
minimum	O
distance	B
so	O
they	O
are	O
relatively	O
unlikely	O
to	O
cause	O
the	O
errors	B
similarly	O
errors	B
in	O
a	O
real	O
error-correcting	B
code	I
depend	O
on	O
the	O
properties	O
of	O
the	O
weight	B
enumerator	I
function	O
at	O
very	O
high	O
friskiness	O
the	O
bat	B
is	O
always	O
a	O
long	O
way	O
from	O
the	O
centre	O
of	O
the	O
cave	B
and	O
almost	O
all	O
its	O
collisions	O
involve	O
contact	O
with	O
distant	O
stalactites	O
under	O
these	O
conditions	O
the	O
bat	B
s	O
collision	B
frequency	B
has	O
nothing	O
to	O
do	O
with	O
the	O
distance	B
from	O
the	O
centre	O
to	O
the	O
closest	O
stalactite	B
concatenation	B
of	O
hamming	B
codes	O
it	O
is	O
instructive	O
to	O
play	O
some	O
more	O
with	O
the	O
concatenation	B
of	O
hamming	B
codes	O
a	O
concept	O
we	O
visited	O
in	O
because	O
we	O
will	O
get	O
insights	O
into	O
the	O
notion	O
of	O
good	B
codes	O
and	O
the	O
relevance	O
or	O
otherwise	O
of	O
the	O
minimum	O
distance	B
of	O
a	O
code	O
we	O
can	O
create	O
a	O
concatenated	B
code	O
for	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
density	B
f	O
by	O
encoding	O
with	O
several	O
hamming	B
codes	O
in	O
succession	O
the	O
table	O
recaps	O
the	O
key	O
properties	O
of	O
the	O
hamming	B
codes	O
indexed	O
by	O
number	O
of	O
constraints	O
m	O
all	O
the	O
hamming	B
codes	O
have	O
minimum	O
distance	B
d	O
and	O
can	O
correct	O
one	O
error	O
in	O
n	O
blocklength	O
n	O
k	O
n	O
m	O
number	O
of	O
source	O
bits	O
pb	O
probability	B
of	I
block	B
error	I
to	O
leading	O
order	O
if	O
we	O
make	O
a	O
product	B
code	I
by	O
concatenating	O
a	O
sequence	B
of	O
c	O
hamming	B
in	O
such	O
a	O
codes	O
with	O
increasing	O
m	O
we	O
can	O
choose	O
those	O
parameters	B
fmcgc	O
way	O
that	O
the	O
rate	B
of	O
the	O
product	B
code	I
rc	O
nc	O
mc	O
nc	O
c	O
tends	O
to	O
a	O
non-zero	O
limit	O
as	O
c	O
increases	O
for	O
example	O
if	O
we	O
set	B
etc	O
then	O
the	O
asymptotic	O
rate	B
is	O
the	O
blocklength	O
n	O
is	O
a	O
rapidly-growing	O
function	O
of	O
c	O
so	O
these	O
codes	O
are	O
somewhat	O
impractical	O
a	O
further	O
weakness	B
of	I
these	O
codes	O
is	O
that	O
their	O
minimum	O
distance	B
is	O
not	O
very	B
good	B
every	O
one	O
of	O
the	O
constituent	O
hamming	B
codes	O
has	O
minimum	O
distance	B
so	O
the	O
minimum	O
distance	B
of	O
the	O
cth	O
product	O
is	O
the	O
blocklength	O
n	O
grows	O
faster	O
than	O
so	O
the	O
ratio	O
dn	O
tends	O
to	O
zero	O
as	O
c	O
increases	O
in	O
contrast	O
for	O
typical	B
random	B
codes	O
the	O
ratio	O
dn	O
tends	O
to	O
a	O
constant	O
such	O
that	O
r	O
concatenated	B
hamming	B
codes	O
thus	O
have	O
bad	B
distance	B
nevertheless	O
it	O
turns	O
out	O
that	O
this	O
simple	O
sequence	B
of	O
codes	O
yields	O
good	B
codes	O
for	O
some	O
channels	O
but	O
not	O
very	B
good	B
codes	O
section	B
to	O
recall	O
the	O
of	O
the	O
terms	O
good	B
and	O
very	B
good	B
rather	O
than	O
prove	O
this	O
result	O
we	O
will	O
simply	O
explore	B
it	O
numerically	O
figure	O
shows	O
the	O
bit	B
error	B
probability	B
pb	O
of	O
the	O
concatenated	B
codes	O
assuming	O
that	O
the	O
constituent	O
codes	O
are	O
decoded	O
in	O
sequence	B
as	O
described	O
in	O
section	B
one-code-at-a-time	O
decoding	B
is	O
suboptimal	O
as	O
we	O
saw	O
there	O
the	O
horizontal	O
axis	O
shows	O
the	O
rates	O
of	O
the	O
codes	O
as	O
the	O
number	O
of	O
concatenations	O
increases	O
the	O
rate	B
drops	O
to	O
and	O
the	O
error	B
probability	B
drops	O
towards	O
zero	O
the	O
channel	O
assumed	O
in	O
the	O
is	O
the	O
binary	B
symmetric	B
r	O
c	O
figure	O
the	O
rate	B
r	O
of	O
the	O
concatenated	B
hamming	B
code	I
as	O
a	O
function	O
of	O
the	O
number	O
of	O
concatenations	O
c	O
c	O
figure	O
the	O
blocklength	O
nc	O
curve	O
and	O
minimum	O
distance	B
dc	O
curve	O
of	O
the	O
concatenated	B
hamming	B
code	I
as	O
a	O
function	O
of	O
the	O
number	O
of	O
concatenations	O
c	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
distance	B
isn	O
t	O
everything	O
channel	O
with	O
f	O
this	O
is	O
the	O
highest	O
noise	O
level	O
that	O
can	O
be	O
tolerated	O
using	O
this	O
concatenated	B
code	O
the	O
take-home	O
message	O
from	O
this	O
story	O
is	O
distance	B
isn	O
t	O
everything	O
the	O
minimum	O
distance	B
of	O
a	O
code	O
although	O
widely	O
worshipped	O
by	O
coding	O
theorists	O
is	O
not	O
of	O
fundamental	O
importance	O
to	O
shannon	B
s	O
mission	O
of	O
achieving	O
reliable	O
communication	B
over	O
noisy	B
channels	O
pb	O
r	O
figure	O
the	O
bit	B
error	O
probabilities	O
versus	O
the	O
rates	O
r	O
of	O
the	O
concatenated	B
hamming	B
codes	O
for	O
the	O
binary	B
symmetric	B
channel	I
with	O
f	O
labels	O
alongside	O
the	O
points	O
show	O
the	O
blocklengths	O
n	O
the	O
solid	O
line	O
shows	O
the	O
shannon	B
limit	O
for	O
this	O
channel	O
the	O
bit	B
error	B
probability	B
drops	O
to	O
zero	O
while	O
the	O
rate	B
tends	O
to	O
so	O
the	O
concatenated	B
hamming	B
codes	O
are	O
a	O
good	B
code	O
family	O
figure	O
the	O
error	B
probability	B
associated	O
with	O
a	O
single	O
codeword	B
of	O
weight	O
d	O
d	O
f	O
as	O
a	O
function	O
of	O
f	O
exercise	O
prove	O
that	O
there	O
exist	O
families	O
of	O
codes	O
with	O
bad	B
distance	B
that	O
are	O
very	B
good	B
codes	O
distance	B
isn	O
t	O
everything	O
let	O
s	O
get	O
a	O
quantitative	O
feeling	O
for	O
the	O
of	O
the	O
minimum	O
distance	B
of	O
a	O
code	O
for	O
the	O
special	O
case	O
of	O
a	O
binary	B
symmetric	B
channel	I
the	O
error	B
probability	B
associated	O
with	O
one	O
low-weight	O
codeword	B
let	O
a	O
binary	O
code	O
have	O
blocklength	O
n	O
and	O
just	O
two	O
codewords	O
which	O
in	O
d	O
places	O
for	O
simplicity	O
let	O
s	O
assume	O
d	O
is	O
even	O
what	O
is	O
the	O
error	B
probability	B
if	O
this	O
code	O
is	O
used	O
on	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
f	O
bit	B
matter	O
only	O
in	O
places	O
where	O
the	O
two	O
codewords	O
the	O
error	B
probability	B
is	O
dominated	O
by	O
the	O
probability	B
that	O
of	O
these	O
bits	O
are	O
what	O
happens	O
to	O
the	O
other	O
bits	O
is	O
irrelevant	O
since	O
the	O
optimal	B
decoder	B
ignores	O
them	O
p	O
error	O
d	O
f	O
this	O
error	B
probability	B
associated	O
with	O
a	O
single	O
codeword	B
of	O
weight	O
d	O
is	O
plotted	O
in	O
using	O
the	O
approximation	B
for	O
the	O
binomial	B
we	O
can	O
further	O
approximate	O
p	O
error	O
f	O
where	O
f	O
is	O
called	O
the	O
bhattacharyya	B
parameter	I
of	O
the	O
channel	O
now	O
consider	O
a	O
general	O
linear	B
code	O
with	O
distance	B
d	O
its	O
block	B
error	B
probability	B
must	O
be	O
at	O
least	O
d	O
f	O
independent	O
of	O
the	O
blocklength	O
n	O
of	O
the	O
code	O
for	O
this	O
reason	O
a	O
sequence	B
of	O
codes	O
of	O
increasing	O
blocklength	O
n	O
and	O
constant	O
distance	B
d	O
very	B
bad	B
distance	B
cannot	O
have	O
a	O
block	B
error	B
probability	B
that	O
tends	O
to	O
zero	O
on	O
any	O
binary	B
symmetric	B
channel	I
if	O
we	O
are	O
interested	O
in	O
making	O
superb	O
error-correcting	B
codes	I
with	O
tiny	O
tiny	O
error	B
probability	B
we	O
might	O
therefore	O
shun	O
codes	O
with	O
bad	B
distance	B
however	O
being	O
pragmatic	O
we	O
should	O
look	O
more	O
carefully	O
at	O
in	O
chapter	O
we	O
argued	O
that	O
codes	O
for	O
disk	O
drives	O
need	O
an	O
error	B
probability	B
smaller	O
than	O
about	O
if	O
the	O
raw	O
error	B
probability	B
in	O
the	O
disk	B
drive	I
is	O
about	O
the	O
error	B
probability	B
associated	O
with	O
one	O
codeword	B
at	O
distance	B
d	O
is	O
smaller	O
than	O
if	O
the	O
raw	O
error	B
probability	B
in	O
the	O
disk	B
drive	I
is	O
about	O
the	O
error	B
probability	B
associated	O
with	O
one	O
codeword	B
at	O
distance	B
d	O
is	O
smaller	O
than	O
for	O
practical	B
purposes	O
therefore	O
it	O
is	O
not	O
essential	O
for	O
a	O
code	O
to	O
have	O
good	B
distance	B
for	O
example	O
codes	O
of	O
blocklength	O
known	O
to	O
have	O
many	O
codewords	O
of	O
weight	O
can	O
nevertheless	O
correct	O
errors	B
of	O
weight	O
with	O
tiny	O
error	B
probability	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
i	O
wouldn	O
t	O
want	O
you	O
to	O
think	O
i	O
am	O
recommending	O
the	O
use	O
of	O
codes	O
with	O
bad	B
distance	B
in	O
chapter	O
we	O
will	O
discuss	O
low-density	B
parity-check	I
codes	O
my	O
favourite	O
codes	O
which	O
have	O
both	O
excellent	O
performance	O
and	O
good	B
distance	B
the	O
union	B
bound	B
the	O
error	B
probability	B
of	O
a	O
code	O
on	O
the	O
binary	B
symmetric	B
channel	I
can	O
be	O
bounded	O
in	O
terms	O
of	O
its	O
weight	B
enumerator	I
function	O
by	O
adding	O
up	O
appropriate	O
multiples	O
of	O
the	O
error	B
probability	B
associated	O
with	O
a	O
single	O
codeword	B
p	O
error	O
this	O
inequality	B
which	O
is	O
an	O
example	O
of	O
a	O
union	B
bound	B
is	O
accurate	O
for	O
low	O
noise	O
levels	O
f	O
but	O
inaccurate	O
for	O
high	O
noise	O
levels	O
because	O
it	O
overcounts	O
the	O
contribution	O
of	O
errors	B
that	O
cause	O
confusion	O
with	O
more	O
than	O
one	O
codeword	B
at	O
a	O
time	O
exercise	O
poor	O
man	O
s	O
noisy-channel	B
coding	B
theorem	I
pretending	O
that	O
the	O
union	B
bound	B
is	O
accurate	O
and	O
using	O
the	O
average	O
weight	B
enumerator	I
function	O
of	O
a	O
random	B
linear	B
code	O
as	O
aw	O
estimate	O
the	O
maximum	O
rate	B
rubf	O
at	O
which	O
one	O
can	O
communicate	O
over	O
a	O
binary	B
symmetric	B
channel	I
or	O
to	O
look	O
at	O
it	O
more	O
positively	O
using	O
the	O
union	B
bound	B
as	O
an	O
inequality	B
show	O
that	O
communication	B
at	O
rates	O
up	O
to	O
rubf	O
is	O
possible	O
over	O
the	O
binary	B
symmetric	B
channel	I
in	O
the	O
following	O
chapter	O
by	O
analysing	O
the	O
probability	B
of	I
error	I
of	O
syndrome	B
decoding	B
for	O
a	O
binary	O
linear	B
code	O
and	O
using	O
a	O
union	B
bound	B
we	O
will	O
prove	O
shannon	B
s	O
noisy-channel	B
coding	B
theorem	I
symmetric	B
binary	O
channels	O
and	O
thus	O
show	O
that	O
very	B
good	B
linear	B
codes	I
exist	O
dual	B
codes	O
a	O
concept	O
that	O
has	O
some	O
importance	O
in	O
coding	B
theory	I
though	O
we	O
will	O
have	O
no	O
immediate	O
use	O
for	O
it	O
in	O
this	O
book	O
is	O
the	O
idea	O
of	O
the	O
dual	B
of	O
a	O
linear	B
errorcorrecting	O
code	O
an	O
k	O
linear	B
error-correcting	B
code	I
can	O
be	O
thought	O
of	O
as	O
a	O
set	B
of	O
codewords	O
generated	O
by	O
adding	O
together	O
all	O
combinations	O
of	O
k	O
independent	O
basis	O
codewords	O
the	O
generator	B
matrix	B
of	O
the	O
code	O
consists	O
of	O
those	O
k	O
basis	O
codewords	O
conventionally	O
written	O
as	O
row	O
vectors	B
for	O
example	O
the	O
hamming	B
code	I
s	O
generator	B
matrix	B
is	O
g	O
and	O
its	O
sixteen	O
codewords	O
were	O
displayed	O
in	O
table	O
the	O
codewords	O
of	O
this	O
code	O
are	O
linear	B
combinations	O
of	O
the	O
four	O
vectors	B
and	O
an	O
k	O
code	O
may	O
also	O
be	O
described	O
in	O
terms	O
of	O
an	O
m	O
n	O
parity-check	B
matrix	B
m	O
n	O
k	O
as	O
the	O
set	B
of	O
vectors	B
ftg	O
that	O
satisfy	O
ht	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
dual	B
codes	O
one	O
way	O
of	O
thinking	O
of	O
this	O
equation	O
is	O
that	O
each	O
row	O
of	O
h	O
a	O
vector	O
to	O
which	O
t	O
must	O
be	O
orthogonal	O
if	O
it	O
is	O
a	O
codeword	B
the	O
generator	B
matrix	B
k	O
vectors	B
from	O
which	O
all	O
codewords	O
can	O
be	O
built	O
and	O
the	O
parity-check	B
matrix	B
a	O
set	B
of	O
m	O
vectors	B
to	O
which	O
all	O
codewords	O
are	O
orthogonal	O
the	O
dual	B
of	O
a	O
code	O
is	O
obtained	O
by	O
exchanging	O
the	O
generator	B
matrix	B
and	O
the	O
parity-check	B
matrix	B
the	O
set	B
of	O
all	O
vectors	B
of	O
length	O
n	O
that	O
are	O
orthogonal	O
to	O
all	O
codewords	O
in	O
a	O
code	O
c	O
is	O
called	O
the	O
dual	B
of	O
the	O
code	O
c	O
if	O
t	O
is	O
orthogonal	O
to	O
and	O
then	O
it	O
is	O
also	O
orthogonal	O
to	O
so	O
all	O
codewords	O
are	O
orthogonal	O
to	O
any	O
linear	B
combination	B
of	O
the	O
m	O
rows	O
of	O
h	O
so	O
the	O
set	B
of	O
all	O
linear	B
combinations	O
of	O
the	O
rows	O
of	O
the	O
parity-check	B
matrix	B
is	O
the	O
dual	B
code	O
for	O
our	O
hamming	B
code	I
the	O
parity-check	B
matrix	B
is	O
h	O
p	O
the	O
dual	B
of	O
the	O
hamming	B
code	I
is	O
the	O
code	O
shown	O
in	O
table	O
a	O
possibly	O
unexpected	O
property	O
of	O
this	O
pair	O
of	O
codes	O
is	O
that	O
the	O
dual	B
is	O
contained	O
within	O
the	O
code	O
itself	O
every	O
word	O
in	O
the	O
dual	B
code	O
is	O
a	O
codeword	B
of	O
the	O
original	O
hamming	B
code	I
this	O
relationship	O
can	O
be	O
written	O
using	O
set	B
notation	B
the	O
possibility	O
that	O
the	O
set	B
of	O
dual	B
vectors	B
can	O
overlap	O
the	O
set	B
of	O
codeword	B
vectors	B
is	O
counterintuitive	O
if	O
we	O
think	O
of	O
the	O
vectors	B
as	O
real	O
vectors	B
how	O
can	O
a	O
vector	O
be	O
orthogonal	O
to	O
itself	O
but	O
when	O
we	O
work	O
in	O
modulo-two	O
arithmetic	O
many	O
non-zero	O
vectors	B
are	O
indeed	O
orthogonal	O
to	O
themselves	O
exercise	O
give	O
a	O
simple	O
rule	O
that	O
distinguishes	O
whether	O
a	O
binary	O
vector	O
is	O
orthogonal	O
to	O
itself	O
as	O
is	O
each	O
of	O
the	O
three	O
vectors	B
and	O
some	O
more	O
duals	O
in	O
general	O
if	O
a	O
code	O
has	O
a	O
systematic	B
generator	B
matrix	B
g	O
where	O
p	O
is	O
a	O
k	O
m	O
matrix	B
then	O
its	O
parity-check	B
matrix	B
is	O
h	O
table	O
the	O
eight	O
codewords	O
of	O
the	O
dual	B
of	O
the	O
hamming	B
code	I
with	O
table	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
example	O
the	O
repetition	B
code	I
has	O
generator	B
matrix	B
its	O
parity-check	B
matrix	B
is	O
g	O
h	O
the	O
two	O
codewords	O
are	O
and	O
the	O
dual	B
code	O
has	O
generator	B
matrix	B
g	O
h	O
or	O
equivalently	O
modifying	O
g	O
into	O
systematic	B
form	O
by	O
row	O
additions	O
g	O
we	O
call	O
this	O
dual	B
code	O
the	O
simple	B
parity	B
code	O
it	O
is	O
the	O
code	O
with	O
one	O
parity-check	O
bit	B
which	O
is	O
equal	O
to	O
the	O
sum	O
of	O
the	O
two	O
source	O
bits	O
the	O
dual	B
code	O
s	O
four	O
codewords	O
are	O
and	O
in	O
this	O
case	O
the	O
only	O
vector	O
common	O
to	O
the	O
code	O
and	O
the	O
dual	B
is	O
the	O
all-zero	O
codeword	B
goodness	O
of	O
duals	O
if	O
a	O
sequence	B
of	O
codes	O
is	O
good	B
are	O
their	O
duals	O
good	B
too	O
examples	O
can	O
be	O
constructed	O
of	O
all	O
cases	O
good	B
codes	O
with	O
good	B
duals	O
linear	B
codes	I
bad	B
codes	O
with	O
bad	B
duals	O
and	O
good	B
codes	O
with	O
bad	B
duals	O
the	O
last	O
category	O
is	O
especially	O
important	O
many	O
state-of-the-art	O
codes	O
have	O
the	O
property	O
that	O
their	O
duals	O
are	O
bad	B
the	O
classic	O
example	O
is	O
the	O
low-density	B
parity-check	B
code	I
whose	O
dual	B
is	O
a	O
low-density	O
generator-matrix	O
code	O
exercise	O
show	O
that	O
low-density	O
generator-matrix	O
codes	O
are	O
bad	B
a	O
family	O
of	O
low-density	O
generator-matrix	O
codes	O
is	O
by	O
two	O
parameters	B
j	O
k	O
which	O
are	O
the	O
column	O
weight	O
and	O
row	O
weight	O
of	O
all	O
rows	O
and	O
columns	O
respectively	O
of	O
g	O
these	O
weights	O
are	O
independent	O
of	O
n	O
for	O
example	O
k	O
show	O
that	O
the	O
code	O
has	O
low-weight	O
codewords	O
then	O
use	O
the	O
argument	O
from	O
exercise	O
show	O
that	O
low-density	B
parity-check	I
codes	O
are	O
good	B
and	O
have	O
good	B
distance	B
solutions	O
see	O
gallager	B
and	O
mackay	B
self-dual	B
codes	O
the	O
hamming	B
code	I
had	O
the	O
property	O
that	O
the	O
dual	B
was	O
contained	O
in	O
the	O
code	O
itself	O
a	O
code	O
is	O
self-orthogonal	B
if	O
it	O
is	O
contained	O
in	O
its	O
dual	B
for	O
example	O
the	O
dual	B
of	O
the	O
hamming	B
code	I
is	O
a	O
self-orthogonal	B
code	O
one	O
way	O
of	O
seeing	O
this	O
is	O
that	O
the	O
overlap	O
between	O
any	O
pair	O
of	O
rows	O
of	O
h	O
is	O
even	O
codes	O
that	O
contain	O
their	O
duals	O
are	O
important	O
in	O
quantum	B
error-correction	B
and	O
shor	O
it	O
is	O
intriguing	O
though	O
not	O
necessarily	O
useful	O
to	O
look	O
at	O
codes	O
that	O
are	O
self-dual	B
a	O
code	O
c	O
is	O
self-dual	B
if	O
the	O
dual	B
of	O
the	O
code	O
is	O
identical	O
to	O
the	O
code	O
c	O
c	O
some	O
properties	O
of	O
self-dual	B
codes	O
can	O
be	O
deduced	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
generalizing	O
perfectness	O
to	O
other	O
channels	O
if	O
a	O
code	O
is	O
self-dual	B
then	O
its	O
generator	B
matrix	B
is	O
also	O
a	O
parity-check	B
matrix	B
for	O
the	O
code	O
self-dual	B
codes	O
have	O
rate	B
i	O
e	O
m	O
k	O
all	O
codewords	O
have	O
even	O
weight	O
exercise	O
what	O
property	O
must	O
the	O
matrix	B
p	O
satisfy	O
if	O
the	O
code	O
with	O
generator	B
matrix	B
g	O
is	O
self-dual	B
examples	O
of	O
self-dual	B
codes	O
the	O
repetition	B
code	I
is	O
a	O
simple	O
example	O
of	O
a	O
self-dual	B
code	O
the	O
smallest	O
non-trivial	O
self-dual	B
code	O
is	O
the	O
following	O
code	O
g	O
h	O
g	O
pt	O
exercise	O
find	O
the	O
relationship	O
of	O
the	O
above	O
code	O
to	O
the	O
hamming	B
code	I
duals	O
and	O
graphs	O
let	O
a	O
code	O
be	O
represented	O
by	O
a	O
graph	B
in	O
which	O
there	O
are	O
nodes	O
of	O
two	O
types	O
parity-check	B
constraints	I
and	O
equality	O
constraints	O
joined	O
by	O
edges	O
which	O
represent	O
the	O
bits	O
of	O
the	O
code	O
all	O
of	O
which	O
need	O
be	O
transmitted	O
the	O
dual	B
code	O
s	O
graph	B
is	O
obtained	O
by	O
replacing	O
all	O
parity-check	B
nodes	I
by	O
equality	O
nodes	O
and	O
vice	O
versa	O
this	O
type	O
of	O
graph	B
is	O
called	O
a	O
normal	B
graph	B
by	O
forney	O
further	O
reading	O
duals	O
are	O
important	O
in	O
coding	B
theory	I
because	O
functions	B
involving	O
a	O
code	O
as	O
the	O
posterior	O
distribution	B
over	O
codewords	O
can	O
be	O
transformed	O
by	O
a	O
fourier	B
transform	I
into	O
functions	B
over	O
the	O
dual	B
code	O
for	O
an	O
accessible	O
introduction	O
to	O
fourier	O
analysis	O
on	O
groups	O
see	O
terras	O
see	O
also	O
macwilliams	O
and	O
sloane	O
generalizing	O
perfectness	O
to	O
other	O
channels	O
having	O
given	O
up	O
on	O
the	O
search	O
for	O
perfect	B
codes	O
for	O
the	O
binary	B
symmetric	B
channel	I
we	O
could	O
console	O
ourselves	O
by	O
changing	O
channel	O
we	O
could	O
call	O
a	O
code	O
a	O
perfect	B
u-error-correcting	O
code	O
for	O
the	O
binary	B
erasure	B
channel	I
if	O
it	O
can	O
restore	O
any	O
u	O
erased	O
bits	O
and	O
never	O
more	O
than	O
u	O
rather	O
than	O
using	O
the	O
word	O
perfect	B
however	O
the	O
conventional	O
term	O
for	O
such	O
a	O
code	O
is	O
a	O
maximum	B
distance	B
separable	I
code	O
or	O
mds	B
code	O
as	O
we	O
already	O
noted	O
in	O
exercise	O
the	O
hamming	B
code	I
is	O
not	O
an	O
mds	B
code	O
it	O
can	O
recover	O
some	O
sets	O
of	O
erased	O
bits	O
but	O
not	O
all	O
if	O
any	O
bits	O
corresponding	O
to	O
a	O
codeword	B
of	O
weight	O
are	O
erased	O
then	O
one	O
bit	B
of	O
information	B
is	O
unrecoverable	O
this	O
is	O
why	O
the	O
code	O
is	O
a	O
poor	O
choice	O
for	O
a	O
raid	B
system	O
in	O
a	O
perfect	B
u-error-correcting	O
code	O
for	O
the	O
binary	B
erasure	B
channel	I
the	O
number	O
of	O
redundant	O
bits	O
must	O
be	O
n	O
k	O
u	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
a	O
tiny	O
example	O
of	O
a	O
maximum	B
distance	B
separable	I
code	O
is	O
the	O
simple	O
paritycheck	O
code	O
whose	O
parity-check	B
matrix	B
is	O
h	O
this	O
code	O
has	O
codewords	O
all	O
of	O
which	O
have	O
even	O
parity	B
all	O
codewords	O
are	O
separated	O
by	O
a	O
distance	B
of	O
any	O
single	O
erased	O
bit	B
can	O
be	O
restored	O
by	O
setting	O
it	O
to	O
the	O
parity	B
of	O
the	O
other	O
two	O
bits	O
the	O
repetition	B
codes	O
are	O
also	O
maximum	B
distance	B
separable	I
codes	O
exercise	O
can	O
you	O
make	O
an	O
k	O
code	O
with	O
m	O
n	O
k	O
parity	B
symbols	O
for	O
a	O
q-ary	O
erasure	B
channel	I
such	O
that	O
the	O
decoder	B
can	O
recover	O
the	O
codeword	B
when	O
any	O
m	O
symbols	O
are	O
erased	O
in	O
a	O
block	B
of	O
n	O
for	O
the	O
channel	O
with	O
q	O
symbols	O
there	O
is	O
an	O
k	O
code	O
which	O
can	O
correct	O
any	O
m	O
erasures	O
for	O
the	O
q-ary	O
erasure	B
channel	I
with	O
q	O
there	O
are	O
large	O
numbers	O
of	O
mds	B
codes	O
of	O
which	O
the	O
reedsolomon	O
codes	O
are	O
the	O
most	O
famous	O
and	O
most	O
widely	O
used	O
as	O
long	O
as	O
the	O
size	O
q	O
is	O
bigger	O
than	O
the	O
blocklength	O
n	O
mds	B
block	B
codes	O
of	O
any	O
rate	B
can	O
be	O
found	O
further	O
reading	O
see	O
lin	O
and	O
costello	O
summary	B
shannon	B
s	O
codes	O
for	O
the	O
binary	B
symmetric	B
channel	I
can	O
almost	O
always	O
correct	O
fn	O
errors	B
but	O
they	O
are	O
not	O
fn	O
codes	O
reasons	O
why	O
the	O
distance	B
of	O
a	O
code	O
has	O
little	O
relevance	O
the	O
shannon	B
limit	O
shows	O
that	O
the	O
best	O
codes	O
must	O
be	O
able	O
to	O
cope	O
with	O
a	O
noise	O
level	O
twice	O
as	O
big	O
as	O
the	O
maximum	O
noise	O
level	O
for	O
a	O
boundeddistance	O
decoder	B
when	O
the	O
binary	B
symmetric	B
channel	I
has	O
f	O
no	O
code	O
with	O
a	O
bounded-distance	B
decoder	B
can	O
communicate	O
at	O
all	O
but	O
shannon	B
says	O
good	B
codes	O
exist	O
for	O
such	O
channels	O
concatenation	B
shows	O
that	O
we	O
can	O
get	O
good	B
performance	O
even	O
if	O
the	O
dis	O
tance	O
is	O
bad	B
the	O
whole	O
weight	B
enumerator	I
function	O
is	O
relevant	O
to	O
the	O
question	O
of	O
whether	O
a	O
code	O
is	O
a	O
good	B
code	O
the	O
relationship	O
between	O
good	B
codes	O
and	B
distance	B
properties	O
is	O
discussed	O
further	O
in	O
exercise	O
further	O
exercises	O
exercise	O
a	O
codeword	B
t	O
is	O
selected	O
from	O
a	O
linear	B
k	O
code	O
c	O
and	O
it	O
is	O
transmitted	O
over	O
a	O
noisy	B
channel	O
the	O
received	O
signal	O
is	O
y	O
we	O
assume	O
that	O
the	O
channel	O
is	O
a	O
memoryless	O
channel	O
such	O
as	O
a	O
gaussian	B
channel	I
given	O
an	O
assumed	O
channel	O
model	B
p	O
j	O
t	O
there	O
are	O
two	O
decoding	B
problems	O
the	O
codeword	B
decoding	B
problem	O
is	O
the	O
task	O
of	O
inferring	O
which	O
codeword	B
t	O
was	O
transmitted	O
given	O
the	O
received	O
signal	O
the	O
bitwise	B
decoding	B
problem	O
is	O
the	O
task	O
of	O
inferring	O
for	O
each	O
transmitted	O
bit	B
tn	O
how	O
likely	O
it	O
is	O
that	O
that	O
bit	B
was	O
a	O
one	O
rather	O
than	O
a	O
zero	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
consider	O
optimal	B
decoders	O
for	O
these	O
two	O
decoding	B
problems	O
prove	O
that	O
the	O
probability	B
of	I
error	I
of	O
the	O
optimal	B
bitwise-decoder	O
is	O
closely	O
related	O
to	O
the	O
probability	B
of	I
error	I
of	O
the	O
optimal	B
codeword-decoder	O
by	O
proving	O
the	O
following	O
theorem	O
theorem	O
if	O
a	O
binary	O
linear	B
code	O
has	O
minimum	O
distance	B
dmin	O
then	O
for	O
any	O
given	O
channel	O
the	O
codeword	B
bit	B
error	B
probability	B
of	O
the	O
optimal	B
bitwise	B
decoder	B
pb	O
and	O
the	O
block	B
error	B
probability	B
of	O
the	O
maximum	B
likelihood	B
decoder	B
pb	O
are	O
related	O
by	O
pb	O
pb	O
dmin	O
n	O
pb	O
exercise	O
what	O
are	O
the	O
minimum	O
distances	O
of	O
the	O
hamming	B
code	I
and	O
the	O
hamming	B
code	I
exercise	O
let	O
aw	O
be	O
the	O
average	O
weight	B
enumerator	I
function	O
of	O
a	O
random	B
linear	B
code	O
with	O
n	O
and	O
m	O
estimate	O
from	O
principles	O
the	O
value	O
of	O
aw	O
at	O
w	O
exercise	O
a	O
code	O
with	O
minimum	O
distance	B
greater	O
than	O
dgv	O
a	O
rather	O
nice	O
code	O
is	O
generated	O
by	O
this	O
generator	B
matrix	B
which	O
is	O
based	O
on	O
measuring	O
the	O
parities	O
of	O
all	O
the	O
g	O
triplets	O
of	O
source	O
bits	O
find	O
the	O
minimum	O
distance	B
and	O
weight	B
enumerator	I
function	O
of	O
this	O
code	O
exercise	O
find	O
the	O
minimum	O
distance	B
of	O
the	O
pentagonful	B
low	O
density	B
parity-check	B
code	I
whose	O
parity-check	B
matrix	B
is	O
h	O
show	O
that	O
nine	O
of	O
the	O
ten	O
rows	O
are	O
independent	O
so	O
the	O
code	O
has	O
parameters	B
n	O
k	O
using	O
a	O
computer	B
its	O
weight	B
enumerator	I
function	O
exercise	O
replicate	O
the	O
calculations	O
used	O
to	O
produce	O
check	O
the	O
assertion	O
that	O
the	O
highest	O
noise	O
level	O
that	O
s	O
correctable	O
is	O
explore	B
alternative	O
concatenated	B
sequences	O
of	O
codes	O
can	O
you	O
a	O
better	O
sequence	B
of	O
concatenated	B
codes	O
better	O
in	O
the	O
sense	O
that	O
it	O
has	O
either	O
higher	O
asymptotic	O
rate	B
r	O
or	O
can	O
tolerate	O
a	O
higher	O
noise	O
level	O
f	O
figure	O
the	O
graph	B
of	O
the	O
pentagonful	B
low-density	B
parity-check	B
code	I
with	O
bit	B
nodes	O
and	O
parity-check	B
nodes	I
graph	B
is	O
known	O
as	O
the	O
petersen	B
graph	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
exercise	O
investigate	O
the	O
possibility	O
of	O
achieving	O
the	O
shannon	B
limit	O
with	O
linear	B
block	B
codes	O
using	O
the	O
following	O
counting	B
argument	I
assume	O
a	O
linear	B
code	O
of	O
large	O
blocklength	O
n	O
and	O
rate	B
r	O
kn	O
the	O
code	O
s	O
parity-check	B
matrix	B
h	O
has	O
m	O
n	O
k	O
rows	O
assume	O
that	O
the	O
code	O
s	O
optimal	B
decoder	B
which	O
solves	O
the	O
syndrome	B
decoding	B
problem	O
hn	O
z	O
allows	O
reliable	O
communication	B
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
probability	B
f	O
how	O
many	O
typical	B
noise	O
vectors	B
n	O
are	O
there	O
roughly	O
how	O
many	O
distinct	O
syndromes	O
z	O
are	O
there	O
since	O
n	O
is	O
reliably	O
deduced	O
from	O
z	O
by	O
the	O
optimal	B
decoder	B
the	O
number	O
of	O
syndromes	O
must	O
be	O
greater	O
than	O
or	O
equal	O
to	O
the	O
number	O
of	O
typical	B
noise	O
vectors	B
what	O
does	O
this	O
tell	O
you	O
about	O
the	O
largest	O
possible	O
value	O
of	O
rate	B
r	O
for	O
a	O
given	O
f	O
exercise	O
linear	B
binary	O
codes	O
use	O
the	O
input	O
symbols	O
and	O
with	O
equal	O
probability	B
implicitly	O
treating	O
the	O
channel	O
as	O
a	O
symmetric	B
channel	I
investigate	O
how	O
much	O
loss	O
in	O
communication	B
rate	B
is	O
caused	O
by	O
this	O
assumption	O
if	O
in	O
fact	O
the	O
channel	O
is	O
a	O
highly	O
asymmetric	O
channel	O
take	O
as	O
an	O
example	O
a	O
z-channel	O
how	O
much	O
smaller	O
is	O
the	O
maximum	O
possible	O
rate	B
of	O
communication	B
using	O
symmetric	B
inputs	O
than	O
the	O
capacity	B
of	O
the	O
channel	O
about	O
exercise	O
show	O
that	O
codes	O
with	O
very	B
bad	B
distance	B
are	O
bad	B
codes	O
as	O
in	O
section	B
exercise	O
one	O
linear	B
code	O
can	O
be	O
obtained	O
from	O
another	O
by	O
puncturing	B
puncturing	B
means	O
taking	O
each	O
codeword	B
and	O
deleting	O
a	O
set	B
of	O
bits	O
puncturing	B
turns	O
an	O
k	O
code	O
into	O
an	O
k	O
code	O
where	O
n	O
another	O
way	O
to	O
make	O
new	O
linear	B
codes	I
from	O
old	O
is	O
shortening	B
shortening	B
means	O
constraining	O
a	O
set	B
of	O
bits	O
to	O
be	O
zero	O
and	O
then	O
deleting	O
them	O
from	O
the	O
codewords	O
typically	O
if	O
we	O
shorten	O
by	O
one	O
bit	B
half	O
of	O
the	O
code	O
s	O
codewords	O
are	O
lost	O
shortening	B
typically	O
turns	O
an	O
k	O
code	O
into	O
an	O
code	O
where	O
n	O
k	O
another	O
way	O
to	O
make	O
a	O
new	O
linear	B
code	O
from	O
two	O
old	O
ones	O
is	O
to	O
make	O
the	O
intersection	B
of	O
the	O
two	O
codes	O
a	O
codeword	B
is	O
only	O
retained	O
in	O
the	O
new	O
code	O
if	O
it	O
is	O
present	O
in	O
both	O
of	O
the	O
two	O
old	O
codes	O
discuss	O
the	O
on	O
a	O
code	O
s	O
distance-properties	O
of	O
puncturing	B
shortening	B
and	O
intersection	B
is	O
it	O
possible	O
to	O
turn	O
a	O
code	O
family	O
with	O
bad	B
distance	B
into	O
a	O
code	O
family	O
with	O
good	B
distance	B
or	O
vice	O
versa	O
by	O
each	O
of	O
these	O
three	O
manipulations	O
exercise	O
todd	O
ebert	B
s	O
hat	B
puzzle	I
three	O
players	O
enter	O
a	O
room	O
and	O
a	O
red	O
or	O
blue	O
hat	B
is	O
placed	O
on	O
each	O
person	O
s	O
head	O
the	O
colour	O
of	O
each	O
hat	B
is	O
determined	O
by	O
a	O
coin	B
toss	O
with	O
the	O
outcome	O
of	O
one	O
coin	B
toss	O
having	O
no	O
on	O
the	O
others	O
each	O
person	O
can	O
see	O
the	O
other	O
players	O
hats	O
but	O
not	O
his	O
own	O
no	O
communication	B
of	O
any	O
sort	O
is	O
allowed	O
except	O
for	O
an	O
initial	O
strategy	O
session	O
before	O
the	O
group	O
enters	O
the	O
room	O
once	O
they	O
have	O
had	O
a	O
chance	O
to	O
look	O
at	O
the	O
other	O
hats	O
the	O
players	O
must	O
simultaneously	O
guess	O
their	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
if	O
you	O
already	O
know	O
the	O
hat	B
puzzle	I
you	O
could	O
try	O
the	O
scottish	O
version	O
of	O
the	O
rules	B
in	O
which	O
the	O
prize	B
is	O
only	O
awarded	O
to	O
the	O
group	O
if	O
they	O
all	O
guess	O
correctly	O
in	O
the	O
reformed	O
scottish	O
version	O
all	O
the	O
players	O
must	O
guess	O
correctly	O
and	O
there	O
are	O
two	O
rounds	O
of	O
guessing	B
those	O
players	O
who	O
guess	O
during	O
round	O
one	O
leave	O
the	O
room	O
the	O
remaining	O
players	O
must	O
guess	O
in	O
round	O
two	O
what	O
strategy	O
should	O
the	O
team	O
adopt	O
to	O
maximize	O
their	O
chance	O
of	O
winning	O
own	O
hat	B
s	O
colour	O
or	O
pass	O
the	O
group	O
shares	O
a	O
million	O
prize	B
if	O
at	O
least	O
one	O
player	O
guesses	O
correctly	O
and	O
no	O
players	O
guess	O
incorrectly	O
the	O
same	O
game	O
can	O
be	O
played	O
with	O
any	O
number	O
of	O
players	O
the	O
general	O
problem	O
is	O
to	O
a	O
strategy	O
for	O
the	O
group	O
that	O
maximizes	O
its	O
chances	O
of	O
winning	O
the	O
prize	B
find	O
the	O
best	O
strategies	O
for	O
groups	O
of	O
size	O
three	O
and	O
seven	O
when	O
you	O
ve	O
done	O
three	O
and	O
seven	O
you	O
might	O
be	O
able	O
to	O
solve	O
exercise	O
estimate	O
how	O
many	O
binary	O
low-density	B
parity-check	I
codes	O
have	O
self-orthogonal	B
duals	O
that	O
we	O
don	O
t	O
expect	O
a	O
huge	O
number	O
since	O
almost	O
all	O
low-density	B
parity-check	I
codes	O
are	O
good	B
but	O
a	O
lowdensity	O
parity-check	B
code	I
that	O
contains	O
its	O
dual	B
must	O
be	O
bad	B
exercise	O
in	O
we	O
plotted	O
the	O
error	B
probability	B
associated	O
with	O
a	O
single	O
codeword	B
of	O
weight	O
d	O
as	O
a	O
function	O
of	O
the	O
noise	O
level	O
f	O
of	O
a	O
binary	B
symmetric	B
channel	I
make	O
an	O
equivalent	O
plot	O
for	O
the	O
case	O
of	O
the	O
gaussian	B
channel	I
showing	O
the	O
error	B
probability	B
associated	O
with	O
a	O
single	O
codeword	B
of	O
weight	O
d	O
as	O
a	O
function	O
of	O
the	O
rate-compensated	O
signal-tonoise	O
ratio	O
because	O
depends	O
on	O
the	O
rate	B
you	O
have	O
to	O
choose	O
a	O
code	O
rate	B
choose	O
r	O
or	O
solutions	O
solution	O
to	O
exercise	O
the	O
probability	B
of	I
block	B
error	I
to	O
leading	O
order	O
is	O
pb	O
solution	O
to	O
exercise	O
a	O
binary	O
vector	O
is	O
perpendicular	O
to	O
itself	O
if	O
it	O
has	O
even	O
weight	O
i	O
e	O
an	O
even	O
number	O
of	O
solution	O
to	O
exercise	O
the	O
self-dual	B
code	O
has	O
two	O
equivalent	O
parity-check	O
matrices	B
g	O
and	O
these	O
must	O
be	O
equivalent	O
to	O
each	O
other	O
through	O
row	O
additions	O
that	O
is	O
there	O
is	O
a	O
matrix	B
u	O
such	O
that	O
so	O
from	O
the	O
right-hand	O
sides	O
of	O
this	O
equation	O
we	O
have	O
u	O
pt	O
so	O
the	O
left-hand	O
sides	O
become	O
thus	O
if	O
a	O
code	O
with	O
generator	B
matrix	B
g	O
is	O
self-dual	B
then	O
p	O
is	O
an	O
orthogonal	O
matrix	B
modulo	O
and	O
vice	O
versa	O
ptp	O
ik	O
solution	O
to	O
exercise	O
the	O
and	O
codes	O
are	O
intimately	O
related	O
the	O
code	O
whose	O
parity-check	B
matrix	B
is	O
h	O
p	O
is	O
obtained	O
by	O
appending	O
an	O
extra	O
parity-check	O
bit	B
which	O
can	O
be	O
thought	O
of	O
as	O
the	O
parity	B
of	O
all	O
seven	O
bits	O
of	O
the	O
hamming	B
code	I
and	O
reordering	O
the	O
four	O
bits	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
if	O
an	O
k	O
code	O
with	O
m	O
n	O
k	O
parity	B
solution	O
to	O
exercise	O
symbols	O
has	O
the	O
property	O
that	O
the	O
decoder	B
can	O
recover	O
the	O
codeword	B
when	O
any	O
m	O
symbols	O
are	O
erased	O
in	O
a	O
block	B
of	O
n	O
then	O
the	O
code	O
is	O
said	O
to	O
be	O
maximum	B
distance	B
separable	I
no	O
mds	B
binary	O
codes	O
exist	O
apart	O
from	O
the	O
repetition	B
codes	O
and	O
simple	B
parity	B
codes	O
for	O
q	O
some	O
mds	B
codes	O
can	O
be	O
found	O
as	O
a	O
simple	O
example	O
here	O
is	O
a	O
code	O
for	O
the	O
erasure	B
channel	I
the	O
code	O
is	O
in	O
terms	O
of	O
the	O
multiplication	O
and	O
addition	O
rules	B
of	O
gf	O
which	O
are	O
given	O
in	O
appendix	O
the	O
elements	O
of	O
the	O
input	O
alphabet	O
are	O
a	O
b	O
c	O
d	O
e	O
fg	O
and	O
the	O
generator	B
matrix	B
of	O
the	O
code	O
is	O
g	O
a	O
b	O
c	O
d	O
e	O
f	O
the	O
resulting	O
codewords	O
are	O
solution	O
to	O
exercise	O
quick	O
rough	O
proof	O
of	O
the	O
theorem	O
let	O
x	O
denote	O
the	O
between	O
the	O
reconstructed	O
codeword	B
and	O
the	O
transmitted	O
codeword	B
for	O
any	O
given	O
channel	O
output	O
r	O
there	O
is	O
a	O
posterior	O
distribution	B
over	O
x	O
this	O
posterior	O
distribution	B
is	O
positive	O
only	O
on	O
vectors	B
x	O
belonging	O
to	O
the	O
code	O
the	O
sums	O
that	O
follow	O
are	O
over	O
codewords	O
x	O
the	O
block	B
error	B
probability	B
is	O
pb	O
p	O
r	O
the	O
average	O
bit	B
error	B
probability	B
averaging	O
over	O
all	O
bits	O
in	O
the	O
codeword	B
is	O
pb	O
p	O
r	O
wx	O
n	O
where	O
wx	O
is	O
the	O
weight	O
of	O
codeword	B
x	O
now	O
the	O
weights	O
of	O
the	O
non-zero	O
codewords	O
satisfy	O
wx	O
n	O
dmin	O
n	O
substituting	O
the	O
inequalities	O
into	O
the	O
we	O
obtain	O
pb	O
pb	O
dmin	O
n	O
pb	O
which	O
is	O
a	O
factor	O
of	O
two	O
stronger	O
on	O
the	O
right	O
than	O
the	O
stated	O
result	O
in	O
making	O
the	O
proof	O
watertight	O
i	O
have	O
weakened	O
the	O
result	O
a	O
little	O
careful	O
proof	O
the	O
theorem	O
relates	O
the	O
performance	O
of	O
the	O
optimal	B
block	B
decoding	B
algorithm	O
and	O
the	O
optimal	B
bitwise	B
decoding	B
algorithm	O
we	O
introduce	O
another	O
pair	O
of	O
decoding	B
algorithms	B
called	O
the	O
blockguessing	O
decoder	B
and	O
the	O
bit-guessing	O
decoder	B
the	O
idea	O
is	O
that	O
these	O
two	O
algorithms	B
are	O
similar	O
to	O
the	O
optimal	B
block	B
decoder	B
and	O
the	O
optimal	B
bitwise	B
decoder	B
but	O
lend	O
themselves	O
more	O
easily	O
to	O
analysis	O
we	O
now	O
these	O
decoders	O
let	O
x	O
denote	O
the	O
inferred	O
codeword	B
for	O
any	O
given	O
code	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
the	O
optimal	B
block	B
decoder	B
returns	O
the	O
codeword	B
x	O
that	O
maximizes	O
the	O
posterior	B
probability	B
p	O
r	O
which	O
is	O
proportional	O
to	O
the	O
likelihood	B
p	O
x	O
the	O
probability	B
of	I
error	I
of	O
this	O
decoder	B
is	O
called	O
pb	O
the	O
optimal	B
bit	B
decoder	B
returns	O
the	O
value	O
of	O
a	O
that	O
maximizes	O
the	O
posterior	B
probability	B
p	O
aj	O
r	O
the	O
n	O
bits	O
xn	O
for	O
each	O
of	O
px	O
p	O
r	O
a	O
the	O
probability	B
of	I
error	I
of	O
this	O
decoder	B
is	O
called	O
pb	O
the	O
block-guessing	O
decoder	B
returns	O
a	O
random	B
codeword	B
x	O
with	O
probabil	O
ity	O
distribution	B
given	O
by	O
the	O
posterior	B
probability	B
p	O
r	O
the	O
probability	B
of	I
error	I
of	O
this	O
decoder	B
is	O
called	O
pg	O
b	O
the	O
bit-guessing	O
decoder	B
returns	O
for	O
each	O
of	O
the	O
n	O
bits	O
xn	O
a	O
random	B
bit	B
from	O
the	O
probability	B
distribution	B
p	O
aj	O
r	O
the	O
probability	B
of	I
error	I
of	O
this	O
decoder	B
is	O
called	O
pg	O
b	O
the	O
theorem	O
states	O
that	O
the	O
optimal	B
bit	B
error	B
probability	B
pb	O
is	O
bounded	O
above	O
by	O
pb	O
and	O
below	O
by	O
a	O
given	O
multiple	O
of	O
pb	O
the	O
left-hand	O
inequality	B
in	O
is	O
trivially	O
true	O
if	O
a	O
block	B
is	O
correct	O
all	O
its	O
constituent	O
bits	O
are	O
correct	O
so	O
if	O
the	O
optimal	B
block	B
decoder	B
outperformed	O
the	O
optimal	B
bit	B
decoder	B
we	O
could	O
make	O
a	O
better	O
bit	B
decoder	B
from	O
the	O
block	B
decoder	B
we	O
prove	O
the	O
right-hand	O
inequality	B
by	O
establishing	O
that	O
the	O
bit-guessing	O
decoder	B
is	O
nearly	O
as	O
good	B
as	O
the	O
optimal	B
bit	B
decoder	B
pg	O
b	O
the	O
bit-guessing	O
decoder	B
s	O
error	B
probability	B
is	O
related	O
to	O
the	O
block	B
guessing	B
decoder	B
s	O
by	O
then	O
since	O
pg	O
b	O
pb	O
we	O
have	O
pb	O
pg	O
b	O
pg	O
b	O
dmin	O
n	O
pg	O
b	O
dmin	O
n	O
pg	O
b	O
dmin	O
n	O
pb	O
we	O
now	O
prove	O
the	O
two	O
lemmas	O
near-optimality	O
of	O
guessing	B
consider	O
the	O
case	O
of	O
a	O
single	O
bit	B
with	O
posterior	B
probability	B
the	O
optimal	B
bit	B
decoder	B
has	O
probability	B
of	I
error	I
p	O
optimal	B
the	O
guessing	B
decoder	B
picks	O
from	O
and	O
the	O
truth	O
is	O
also	O
distributed	O
with	O
the	O
same	O
probability	B
the	O
probability	B
that	O
the	O
guesser	O
and	O
the	O
truth	O
match	O
is	O
the	O
probability	B
that	O
they	O
mismatch	O
is	O
the	O
guessing	B
error	B
probability	B
p	O
guess	O
optimal	B
since	O
pg	O
b	O
is	O
the	O
average	O
of	O
many	O
such	O
error	O
probabilities	O
p	O
guess	O
and	O
pb	O
is	O
the	O
average	O
of	O
the	O
corresponding	O
optimal	B
error	O
probabilities	O
p	O
optimal	B
we	O
obtain	O
the	O
desired	O
relationship	O
between	O
pg	O
b	O
and	O
pb	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
binary	O
codes	O
relationship	O
between	O
bit	B
error	B
probability	B
and	O
block	B
error	B
probability	B
the	O
bitguessing	O
and	O
block-guessing	O
decoders	O
can	O
be	O
combined	O
in	O
a	O
single	O
system	O
we	O
can	O
draw	O
a	O
sample	B
xn	O
from	O
the	O
marginal	B
distribution	B
p	O
j	O
r	O
by	O
drawing	O
a	O
sample	B
x	O
from	O
the	O
joint	B
distribution	B
p	O
xj	O
r	O
then	O
discarding	O
the	O
value	O
of	O
x	O
we	O
can	O
distinguish	O
between	O
two	O
cases	O
the	O
discarded	O
value	O
of	O
x	O
is	O
the	O
correct	O
codeword	B
or	O
not	O
the	O
probability	B
of	O
bit	B
error	O
for	O
the	O
bit-guessing	O
decoder	B
can	O
then	O
be	O
written	O
as	O
a	O
sum	O
of	O
two	O
terms	O
pg	O
b	O
p	O
correctp	O
errorj	O
x	O
correct	O
p	O
incorrectp	O
errorj	O
x	O
incorrect	O
pg	O
bp	O
errorj	O
x	O
incorrect	O
now	O
whenever	O
the	O
guessed	O
x	O
is	O
incorrect	O
the	O
true	O
x	O
must	O
from	O
it	O
in	O
at	O
least	O
d	O
bits	O
so	O
the	O
probability	B
of	O
bit	B
error	O
in	O
these	O
cases	O
is	O
at	O
least	O
dn	O
so	O
pg	O
b	O
d	O
n	O
pg	O
b	O
qed	O
solution	O
to	O
exercise	O
the	O
number	O
of	O
typical	B
noise	O
vectors	B
n	O
is	O
roughly	O
the	O
number	O
of	O
distinct	O
syndromes	O
z	O
is	O
so	O
reliable	O
communication	B
implies	O
or	O
in	O
terms	O
of	O
the	O
rate	B
r	O
mn	O
m	O
n	O
r	O
a	O
bound	B
which	O
agrees	O
precisely	O
with	O
the	O
capacity	B
of	O
the	O
channel	O
this	O
argument	O
is	O
turned	O
into	O
a	O
proof	O
in	O
the	O
following	O
chapter	O
solution	O
to	O
exercise	O
the	O
group	O
to	O
win	O
three-quarters	O
of	O
the	O
time	O
in	O
the	O
three-player	O
case	O
it	O
is	O
possible	O
for	O
three-quarters	O
of	O
the	O
time	O
two	O
of	O
the	O
players	O
will	O
have	O
hats	O
of	O
the	O
same	O
colour	O
and	O
the	O
third	O
player	O
s	O
hat	B
will	O
be	O
the	O
opposite	O
colour	O
the	O
group	O
can	O
win	O
every	O
time	O
this	O
happens	O
by	O
using	O
the	O
following	O
strategy	O
each	O
player	O
looks	O
at	O
the	O
other	O
two	O
players	O
hats	O
if	O
the	O
two	O
hats	O
are	O
colours	O
he	O
passes	O
if	O
they	O
are	O
the	O
same	O
colour	O
the	O
player	O
guesses	O
his	O
own	O
hat	B
is	O
the	O
opposite	O
colour	O
this	O
way	O
every	O
time	O
the	O
hat	B
colours	O
are	O
distributed	O
two	O
and	O
one	O
one	O
player	O
will	O
guess	O
correctly	O
and	O
the	O
others	O
will	O
pass	O
and	O
the	O
group	O
will	O
win	O
the	O
game	O
when	O
all	O
the	O
hats	O
are	O
the	O
same	O
colour	O
however	O
all	O
three	O
players	O
will	O
guess	O
incorrectly	O
and	O
the	O
group	O
will	O
lose	O
when	O
any	O
particular	O
player	O
guesses	O
a	O
colour	O
it	O
is	O
true	O
that	O
there	O
is	O
only	O
a	O
chance	O
that	O
their	O
guess	O
is	O
right	O
the	O
reason	O
that	O
the	O
group	O
wins	O
of	O
the	O
time	O
is	O
that	O
their	O
strategy	O
ensures	O
that	O
when	O
players	O
are	O
guessing	B
wrong	O
a	O
great	O
many	O
are	O
guessing	B
wrong	O
for	O
larger	O
numbers	O
of	O
players	O
the	O
aim	O
is	O
to	O
ensure	O
that	O
most	O
of	O
the	O
time	O
no	O
one	O
is	O
wrong	O
and	O
occasionally	O
everyone	O
is	O
wrong	O
at	O
once	O
in	O
the	O
game	O
with	O
players	O
there	O
is	O
a	O
strategy	O
for	O
which	O
the	O
group	O
wins	O
out	O
of	O
every	O
times	O
they	O
play	O
in	O
the	O
game	O
with	O
players	O
the	O
group	O
can	O
win	O
out	O
of	O
times	O
if	O
you	O
have	O
not	O
out	O
these	O
winning	O
strategies	O
for	O
teams	O
of	O
and	O
i	O
recommend	O
thinking	O
about	O
the	O
solution	O
to	O
the	O
three-player	O
game	O
in	O
terms	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
of	O
the	O
locations	O
of	O
the	O
winning	O
and	O
losing	O
states	O
on	O
the	O
three-dimensional	O
hypercube	O
then	O
thinking	O
laterally	O
if	O
the	O
number	O
of	O
players	O
n	O
is	O
the	O
optimal	B
strategy	O
can	O
be	O
using	O
a	O
hamming	B
code	I
of	O
length	O
n	O
and	O
the	O
probability	B
of	O
winning	O
the	O
prize	B
is	O
nn	O
each	O
player	O
is	O
with	O
a	O
number	O
n	O
n	O
the	O
two	O
colours	O
are	O
mapped	O
onto	O
and	O
any	O
state	O
of	O
their	O
hats	O
can	O
be	O
viewed	O
as	O
a	O
received	O
vector	O
out	O
of	O
a	O
binary	O
channel	O
a	O
random	B
binary	O
vector	O
of	O
length	O
n	O
is	O
either	O
a	O
codeword	B
of	O
the	O
hamming	B
code	I
with	O
probability	B
or	O
it	O
in	O
exactly	O
one	O
bit	B
from	O
a	O
codeword	B
each	O
player	O
looks	O
at	O
all	O
the	O
other	O
bits	O
and	O
considers	O
whether	O
his	O
bit	B
can	O
be	O
set	B
to	O
a	O
colour	O
such	O
that	O
the	O
state	O
is	O
a	O
codeword	B
can	O
be	O
deduced	O
using	O
the	O
decoder	B
of	O
the	O
hamming	B
code	I
if	O
it	O
can	O
then	O
the	O
player	O
guesses	O
that	O
his	O
hat	B
is	O
the	O
other	O
colour	O
if	O
the	O
state	O
is	O
actually	O
a	O
codeword	B
all	O
players	O
will	O
guess	O
and	O
will	O
guess	O
wrong	O
if	O
the	O
state	O
is	O
a	O
non-codeword	O
only	O
one	O
player	O
will	O
guess	O
and	O
his	O
guess	O
will	O
be	O
correct	O
it	O
s	O
quite	O
easy	O
to	O
train	O
seven	O
players	O
to	O
follow	O
the	O
optimal	B
strategy	O
if	O
the	O
cyclic	B
representation	O
of	O
the	O
hamming	B
code	I
is	O
used	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
in	O
this	O
chapter	O
we	O
will	O
draw	O
together	O
several	O
ideas	O
that	O
we	O
ve	O
encountered	O
so	O
far	O
in	O
one	O
nice	O
short	O
proof	O
we	O
will	O
simultaneously	O
prove	O
both	O
shannon	B
s	O
noisy-channel	B
coding	B
theorem	I
symmetric	B
binary	O
channels	O
and	O
his	O
source	B
coding	B
theorem	I
binary	O
sources	O
while	O
this	O
proof	O
has	O
connections	O
to	O
many	O
preceding	O
chapters	O
in	O
the	O
book	O
it	O
s	O
not	O
essential	O
to	O
have	O
read	O
them	O
all	O
on	O
the	O
noisy-channel	O
coding	O
side	O
our	O
proof	O
will	O
be	O
more	O
constructive	O
than	O
the	O
proof	O
given	O
in	O
chapter	O
there	O
we	O
proved	O
that	O
almost	O
any	O
random	B
code	I
is	O
very	B
good	B
here	O
we	O
will	O
show	O
that	O
almost	O
any	O
linear	B
code	O
is	O
very	B
good	B
we	O
will	O
make	O
use	O
of	O
the	O
idea	O
of	O
typical	B
sets	O
and	O
and	O
we	O
ll	O
borrow	O
from	O
the	O
previous	O
chapter	O
s	O
calculation	O
of	O
the	O
weight	B
enumerator	I
function	O
of	O
random	B
linear	B
codes	I
on	O
the	O
source	O
coding	O
side	O
our	O
proof	O
will	O
show	O
that	O
random	B
linear	B
hash	O
functions	B
can	O
be	O
used	O
for	B
compression	B
of	O
compressible	O
binary	O
sources	O
thus	O
giving	O
a	O
link	O
to	O
chapter	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
very	B
good	B
linear	B
codes	I
exist	O
in	O
this	O
chapter	O
we	O
ll	O
use	O
a	O
single	O
calculation	O
to	O
prove	O
simultaneously	O
the	O
source	B
coding	B
theorem	I
and	O
the	O
noisy-channel	B
coding	B
theorem	I
for	O
the	O
binary	B
symmetric	B
channel	I
incidentally	O
this	O
proof	O
works	O
for	O
much	O
more	O
general	O
channel	O
models	O
not	O
only	O
the	O
binary	B
symmetric	B
channel	I
for	O
example	O
the	O
proof	O
can	O
be	O
reworked	O
for	O
channels	O
with	O
non-binary	O
outputs	O
for	O
time-varying	O
channels	O
and	O
for	O
channels	O
with	B
memory	B
as	O
long	O
as	O
they	O
have	O
binary	O
inputs	O
satisfying	O
a	O
symmetry	O
property	O
cf	O
section	B
a	O
simultaneous	O
proof	O
of	O
the	O
source	O
coding	O
and	O
noisy-channel	O
coding	O
theorems	O
we	O
consider	O
a	O
linear	B
error-correcting	B
code	I
with	O
binary	O
parity-check	B
matrix	B
h	O
the	O
matrix	B
has	O
m	O
rows	O
and	O
n	O
columns	O
later	O
in	O
the	O
proof	O
we	O
will	O
increase	O
n	O
and	O
m	O
keeping	O
m	O
n	O
the	O
rate	B
of	O
the	O
code	O
r	O
m	O
n	O
if	O
all	O
the	O
rows	O
of	O
h	O
are	O
independent	O
then	O
this	O
is	O
an	O
equality	O
r	O
mn	O
in	O
what	O
follows	O
we	O
ll	O
assume	O
the	O
equality	O
holds	O
eager	O
readers	O
may	O
work	O
out	O
the	O
expected	O
rank	O
of	O
a	O
random	B
binary	O
matrix	B
h	O
s	O
very	O
close	O
to	O
m	O
and	O
pursue	O
the	O
that	O
the	O
rank	O
has	O
on	O
the	O
rest	O
of	O
this	O
proof	O
s	O
negligible	O
a	O
codeword	B
t	O
is	O
selected	O
satisfying	O
ht	O
mod	O
and	O
a	O
binary	B
symmetric	B
channel	I
adds	O
noise	O
x	O
giving	O
the	O
received	O
signal	O
r	O
t	O
x	O
mod	O
the	O
receiver	O
aims	O
to	O
infer	O
both	O
t	O
and	O
x	O
from	O
r	O
using	O
a	O
syndrome-decoding	O
approach	O
syndrome	B
decoding	B
was	O
introduced	O
in	O
section	B
and	O
the	O
receiver	O
computes	O
the	O
syndrome	B
z	O
hr	O
mod	O
ht	O
hx	O
mod	O
hx	O
mod	O
the	O
syndrome	B
only	O
depends	O
on	O
the	O
noise	O
x	O
and	O
the	O
decoding	B
problem	O
is	O
to	O
the	O
most	O
probable	O
x	O
that	O
hx	O
z	O
mod	O
in	O
this	O
chapter	O
x	O
denotes	O
the	O
noise	O
added	O
by	O
the	O
channel	O
not	O
the	O
input	O
to	O
the	O
channel	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
very	B
good	B
linear	B
codes	I
exist	O
this	O
best	O
estimate	O
for	O
the	O
noise	O
vector	O
is	O
then	O
subtracted	O
from	O
r	O
to	O
give	O
the	O
best	O
guess	O
for	O
t	O
our	O
aim	O
is	O
to	O
show	O
that	O
as	O
long	O
as	O
r	O
where	O
f	O
is	O
the	O
probability	B
of	O
the	O
binary	B
symmetric	B
channel	I
the	O
optimal	B
decoder	B
for	O
this	O
syndrome-decoding	O
problem	O
has	O
vanishing	O
probability	B
of	I
error	I
as	O
n	O
increases	O
for	O
random	B
h	O
we	O
prove	O
this	O
result	O
by	O
studying	O
a	O
sub-optimal	O
strategy	O
for	O
solving	O
the	O
decoding	B
problem	O
neither	O
the	O
optimal	B
decoder	B
nor	O
this	O
typical-set	B
decoder	B
would	O
be	O
easy	O
to	O
implement	O
but	O
the	O
typical-set	B
decoder	B
is	O
easier	O
to	O
analyze	O
the	O
typical-set	B
decoder	B
examines	O
the	O
typical	B
set	B
t	O
of	O
noise	O
vectors	B
the	O
set	B
of	O
noise	O
vectors	B
that	O
satisfy	O
log	O
n	O
hx	O
checking	O
to	O
see	O
if	O
any	O
of	O
we	O
ll	O
leave	O
out	O
the	O
and	O
that	O
those	O
typical	B
vectors	B
the	O
observed	O
syndrome	B
make	O
a	O
typical-set	O
rigorous	O
enthusiasts	O
are	O
encouraged	O
to	O
revisit	O
section	B
and	O
put	O
these	O
details	O
into	O
this	O
proof	O
z	O
if	O
exactly	O
one	O
typical	B
vector	O
does	O
so	O
the	O
typical	B
set	B
decoder	B
reports	O
that	O
vector	O
as	O
the	O
hypothesized	O
noise	O
vector	O
if	O
no	O
typical	B
vector	O
matches	O
the	O
observed	O
syndrome	B
or	O
more	O
than	O
one	O
does	O
then	O
the	O
typical	B
set	B
decoder	B
reports	O
an	O
error	O
the	O
probability	B
of	I
error	I
of	O
the	O
typical-set	B
decoder	B
for	O
a	O
given	O
matrix	B
h	O
can	O
be	O
written	O
as	O
a	O
sum	O
of	O
two	O
terms	O
ptsjh	O
p	O
p	O
tsjh	O
where	O
p	O
is	O
the	O
probability	B
that	O
the	O
true	O
noise	O
vector	O
x	O
is	O
itself	O
not	O
typical	B
and	O
p	O
tsjh	O
is	O
the	O
probability	B
that	O
the	O
true	O
x	O
is	O
typical	B
and	O
at	O
least	O
one	O
other	O
typical	B
vector	O
clashes	O
with	O
it	O
the	O
probability	B
vanishes	O
as	O
n	O
increases	O
as	O
we	O
proved	O
when	O
we	O
studied	O
typical	B
sets	O
we	O
concentrate	O
on	O
the	O
second	O
probability	B
to	O
recap	O
we	O
re	O
imagining	O
a	O
true	O
noise	O
vector	O
x	O
and	O
if	O
any	O
of	O
the	O
typical	B
noise	O
vectors	B
from	O
x	O
x	O
then	O
we	O
have	O
an	O
error	O
we	O
use	O
the	O
truth	B
function	I
x	O
whose	O
value	O
is	O
one	O
if	O
the	O
statement	O
x	O
is	O
true	O
and	O
zero	O
otherwise	O
we	O
can	O
bound	B
the	O
number	O
of	O
type	O
ii	O
errors	B
made	O
when	O
the	O
noise	O
is	O
x	O
thus	O
of	O
errors	B
given	O
x	O
and	O
h	O
t	O
x	O
x	O
the	O
number	O
of	O
errors	B
is	O
either	O
zero	O
or	O
one	O
the	O
sum	O
on	O
the	O
right-hand	O
side	O
may	O
exceed	O
one	O
in	O
cases	O
where	O
several	O
typical	B
noise	O
vectors	B
have	O
the	O
same	O
syndrome	B
we	O
can	O
now	O
write	O
down	O
the	O
probability	B
of	O
a	O
type-ii	O
error	O
by	O
averaging	O
over	O
equation	O
is	O
a	O
union	B
bound	B
x	O
p	O
tsjh	O
p	O
t	O
x	O
x	O
now	O
we	O
will	O
the	O
average	O
of	O
this	O
probability	B
of	O
type-ii	O
error	O
over	O
all	O
linear	B
codes	I
by	O
averaging	O
over	O
h	O
by	O
showing	O
that	O
the	O
average	O
probability	B
of	O
type-ii	O
error	O
vanishes	O
we	O
will	O
thus	O
show	O
that	O
there	O
exist	O
linear	B
codes	I
with	O
vanishing	O
error	B
probability	B
indeed	O
that	O
almost	O
all	O
linear	B
codes	I
are	O
very	B
good	B
we	O
denote	O
averaging	O
over	O
all	O
binary	O
matrices	B
h	O
by	O
h	O
the	O
average	O
probability	B
of	O
type-ii	O
error	O
is	O
ts	O
xh	O
p	O
tsjh	O
dp	O
tsjheh	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
data	O
compression	B
by	O
linear	B
hash	O
codes	O
p	O
t	O
p	O
t	O
x	O
x	O
x	O
h	O
now	O
the	O
quantity	O
h	O
x	O
already	O
cropped	O
up	O
when	O
we	O
were	O
calculating	O
the	O
expected	O
weight	B
enumerator	I
function	O
of	O
random	B
linear	B
codes	I
for	O
any	O
non-zero	O
binary	O
vector	O
v	O
the	O
probability	B
that	O
hv	O
averaging	O
over	O
all	O
matrices	B
h	O
is	O
so	O
ts	O
p	O
where	O
jtj	O
denotes	O
the	O
size	O
of	O
the	O
typical	B
set	B
as	O
you	O
will	O
recall	O
from	O
chapter	O
there	O
are	O
roughly	O
hx	O
noise	O
vectors	B
in	O
the	O
typical	B
set	B
so	O
jtj	O
ts	O
this	O
bound	B
on	O
the	O
probability	B
of	I
error	I
either	O
vanishes	O
or	O
grows	O
exponentially	O
as	O
n	O
increases	O
that	O
we	O
are	O
keeping	O
m	O
proportional	O
to	O
n	O
as	O
n	O
increases	O
it	O
vanishes	O
if	O
substituting	O
r	O
mn	O
we	O
have	O
thus	O
established	O
the	O
noisy-channel	B
coding	B
theorem	I
for	O
the	O
binary	B
symmetric	B
channel	I
very	B
good	B
linear	B
codes	I
exist	O
for	O
any	O
rate	B
r	O
satisfying	O
hx	O
mn	O
where	O
hx	O
is	O
the	O
entropy	B
of	O
the	O
channel	O
noise	O
per	O
bit	B
r	O
hx	O
exercise	O
redo	O
the	O
proof	O
for	O
a	O
more	O
general	O
channel	O
data	O
compression	B
by	O
linear	B
hash	O
codes	O
the	O
decoding	B
game	O
we	O
have	O
just	O
played	O
can	O
also	O
be	O
viewed	O
as	O
an	O
uncompression	B
game	O
the	O
world	O
produces	O
a	O
binary	O
noise	O
vector	O
x	O
from	O
a	O
source	O
p	O
the	O
noise	O
has	O
redundancy	B
the	O
probability	B
is	O
not	O
we	O
compress	B
it	O
with	O
a	O
linear	B
compressor	O
that	O
maps	O
the	O
n	O
input	O
x	O
noise	O
to	O
the	O
m	O
output	O
z	O
syndrome	B
our	O
uncompression	B
task	O
is	O
to	O
recover	O
the	O
input	O
x	O
from	O
the	O
output	O
z	O
the	O
rate	B
of	O
the	O
compressor	O
is	O
rcompressor	O
mn	O
don	O
t	O
care	O
about	O
the	O
possibility	O
of	O
linear	B
redundancies	O
in	O
our	O
of	O
the	O
rate	B
here	O
the	O
result	O
that	O
we	O
just	O
found	O
that	O
the	O
decoding	B
problem	O
can	O
be	O
solved	O
for	O
almost	O
any	O
h	O
with	O
vanishing	O
error	B
probability	B
as	O
long	O
as	O
hx	O
mn	O
thus	O
instantly	O
proves	O
a	O
source	B
coding	B
theorem	I
given	O
a	O
binary	O
source	O
x	O
of	O
entropy	B
hx	O
and	O
a	O
required	O
compressed	O
rate	B
r	O
hx	O
there	O
exists	O
a	O
linear	B
compressor	O
x	O
z	O
hx	O
mod	O
having	O
rate	B
mn	O
equal	O
to	O
that	O
required	O
rate	B
r	O
and	O
an	O
associated	O
uncompressor	O
that	O
is	O
virtually	O
lossless	B
this	O
theorem	O
is	O
true	O
not	O
only	O
for	O
a	O
source	O
of	O
independent	O
identically	O
distributed	O
symbols	O
but	O
also	O
for	O
any	O
source	O
for	O
which	O
a	O
typical	B
set	B
can	O
be	O
sources	O
with	B
memory	B
and	O
time-varying	O
sources	O
for	O
example	O
all	O
that	O
s	O
required	O
is	O
that	O
the	O
source	O
be	O
ergodic	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
notes	O
very	B
good	B
linear	B
codes	I
exist	O
this	O
method	O
for	O
proving	O
that	O
codes	O
are	O
good	B
can	O
be	O
applied	O
to	O
other	O
linear	B
codes	I
such	O
as	O
low-density	B
parity-check	I
codes	O
aji	O
et	O
al	O
for	O
each	O
code	O
we	O
need	O
an	O
approximation	B
of	O
its	O
expected	O
weight	B
enumerator	I
function	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
information	B
theory	I
the	O
most	O
exciting	O
exercises	O
which	O
will	O
introduce	O
you	O
to	O
further	O
ideas	O
in	O
information	B
theory	I
are	O
towards	O
the	O
end	O
of	O
this	O
chapter	O
refresher	O
exercises	O
on	O
source	O
coding	O
and	O
noisy	B
channels	O
exercise	O
let	O
x	O
be	O
an	O
ensemble	B
with	O
ax	O
and	O
px	O
consider	O
source	O
coding	O
using	O
the	O
block	B
coding	O
of	O
x	O
where	O
every	O
x	O
x	O
containing	O
or	O
fewer	O
is	O
assigned	O
a	O
distinct	O
codeword	B
while	O
the	O
other	O
xs	O
are	O
ignored	O
if	O
the	O
assigned	O
codewords	O
are	O
all	O
of	O
the	O
same	O
length	O
the	O
minimum	O
length	O
required	O
to	O
provide	O
the	O
above	O
set	B
with	O
distinct	O
codewords	O
calculate	O
the	O
probability	B
of	O
getting	O
an	O
x	O
that	O
will	O
be	O
ignored	O
exercise	O
let	O
x	O
be	O
an	O
ensemble	B
with	O
px	O
the	O
ensemble	B
is	O
encoded	O
using	O
the	O
symbol	B
code	I
c	O
consider	O
the	O
codeword	B
corresponding	O
to	O
x	O
x	O
n	O
where	O
n	O
is	O
large	O
compute	O
the	O
entropy	B
of	O
the	O
fourth	O
bit	B
of	O
transmission	O
compute	O
the	O
conditional	B
entropy	B
of	O
the	O
fourth	O
bit	B
given	O
the	O
third	O
bit	B
estimate	O
the	O
entropy	B
of	O
the	O
hundredth	O
bit	B
estimate	O
the	O
conditional	B
entropy	B
of	O
the	O
hundredth	O
bit	B
given	O
the	O
ninety-ninth	O
bit	B
exercise	O
two	O
fair	O
dice	O
are	O
rolled	O
by	O
alice	B
and	O
the	O
sum	O
is	O
recorded	O
bob	B
s	O
task	O
is	O
to	O
ask	O
a	O
sequence	B
of	O
questions	O
with	O
yesno	O
answers	O
to	O
out	O
this	O
number	O
devise	O
in	O
detail	O
a	O
strategy	O
that	O
achieves	O
the	O
minimum	O
possible	O
average	O
number	O
of	O
questions	O
exercise	O
how	O
can	O
you	O
use	O
a	O
coin	B
to	O
draw	B
straws	B
among	O
people	O
exercise	O
in	O
a	O
magic	B
trick	I
there	O
are	O
three	O
participants	O
the	O
magician	B
an	O
assistant	O
and	O
a	O
volunteer	O
the	O
assistant	O
who	O
claims	O
to	O
have	O
paranormal	B
abilities	O
is	O
in	O
a	O
soundproof	O
room	O
the	O
magician	B
gives	O
the	O
volunteer	O
six	B
blank	O
cards	O
white	B
and	O
one	O
blue	O
the	O
volunteer	O
writes	O
a	O
different	O
integer	O
from	O
to	O
on	O
each	O
card	B
as	O
the	O
magician	B
is	O
watching	O
the	O
volunteer	O
keeps	O
the	O
blue	O
card	B
the	O
magician	B
arranges	O
the	O
white	B
cards	O
in	O
some	O
order	O
and	O
passes	O
them	O
to	O
the	O
assistant	O
the	O
assistant	O
then	O
announces	O
the	O
number	O
on	O
the	O
blue	O
card	B
how	O
does	O
the	O
trick	O
work	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
information	B
theory	I
exercise	O
how	O
does	O
this	O
trick	O
work	O
here	O
s	O
an	O
ordinary	O
pack	O
of	O
cards	O
into	O
random	B
order	O
please	O
choose	O
cards	O
from	O
the	O
pack	O
any	O
that	O
you	O
wish	O
don	O
t	O
let	O
me	O
see	O
their	O
faces	O
no	O
don	O
t	O
give	O
them	O
to	O
me	O
pass	O
them	O
to	O
my	O
assistant	O
esmerelda	O
she	O
can	O
look	O
at	O
them	O
now	O
esmerelda	O
show	O
me	O
four	O
of	O
the	O
cards	O
hmm	O
nine	O
of	O
spades	O
six	B
of	O
clubs	O
four	O
of	O
hearts	O
ten	O
of	O
diamonds	O
the	O
hidden	O
card	B
then	O
must	O
be	O
the	O
queen	O
of	O
spades	O
the	O
trick	O
can	O
be	O
performed	O
as	O
described	O
above	O
for	O
a	O
pack	O
of	O
cards	O
use	O
information	B
theory	I
to	O
give	O
an	O
upper	O
bound	B
on	O
the	O
number	O
of	O
cards	O
for	O
which	O
the	O
trick	O
can	O
be	O
performed	O
exercise	O
find	O
a	O
probability	B
sequence	B
p	O
such	O
that	O
hp	O
exercise	O
consider	O
a	O
discrete	B
memoryless	I
source	O
with	O
ax	O
fa	O
b	O
c	O
dg	O
and	O
px	O
there	O
are	O
eight-letter	O
words	O
that	O
can	O
be	O
formed	O
from	O
the	O
four	O
letters	O
find	O
the	O
total	O
number	O
of	O
such	O
words	O
that	O
are	O
in	O
the	O
typical	B
set	B
tn	O
where	O
n	O
and	O
exercise	O
consider	O
and	O
the	O
channel	O
whose	O
transition	B
probability	B
matrix	B
is	O
the	O
source	O
as	O
q	O
fa	O
b	O
c	O
d	O
eg	O
ps	O
note	O
that	O
the	O
source	O
alphabet	O
has	O
symbols	O
but	O
the	O
channel	O
alphabet	O
ax	O
ay	O
has	O
only	O
four	O
assume	O
that	O
the	O
source	O
produces	O
symbols	O
at	O
exactly	O
the	O
rate	B
that	O
the	O
channel	O
accepts	O
channel	O
symbols	O
for	O
a	O
given	O
explain	O
how	O
you	O
would	O
design	O
a	O
system	O
for	O
communicating	O
the	O
source	O
s	O
output	O
over	O
the	O
channel	O
with	O
an	O
average	O
error	B
probability	B
per	O
source	O
symbol	O
less	O
than	O
be	O
as	O
explicit	O
as	O
possible	O
in	O
particular	O
do	O
not	O
invoke	O
shannon	B
s	O
noisy-channel	B
coding	B
theorem	I
exercise	O
consider	O
a	O
binary	B
symmetric	B
channel	I
and	O
a	O
code	O
c	O
assume	O
that	O
the	O
four	O
codewords	O
are	O
used	O
with	O
probabilities	O
what	O
is	O
the	O
decoding	B
rule	O
that	O
minimizes	O
the	O
probability	B
of	O
decoding	B
error	O
optimal	B
decoding	B
rule	O
depends	O
on	O
the	O
noise	O
level	O
f	O
of	O
the	O
binary	B
symmetric	B
channel	I
give	O
the	O
decoding	B
rule	O
for	O
each	O
range	O
of	O
values	O
of	O
f	O
for	O
f	O
between	O
and	O
exercise	O
find	O
the	O
capacity	B
and	O
optimal	B
input	I
distribution	B
for	O
the	O
three-input	O
three-output	O
channel	O
whose	O
transition	B
probabilities	O
are	O
q	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
information	B
theory	I
exercise	O
the	O
input	O
to	O
a	O
channel	O
q	O
is	O
a	O
word	O
of	O
bits	O
the	O
output	O
is	O
also	O
a	O
word	O
of	O
bits	O
each	O
time	O
it	O
is	O
used	O
the	O
channel	O
exactly	O
one	O
of	O
the	O
transmitted	O
bits	O
but	O
the	O
receiver	O
does	O
not	O
know	O
which	O
one	O
the	O
other	O
seven	O
bits	O
are	O
received	O
without	O
error	O
all	O
bits	O
are	O
equally	O
likely	O
to	O
be	O
the	O
one	O
that	O
is	O
derive	O
the	O
capacity	B
of	O
this	O
channel	O
show	O
by	O
describing	O
an	O
explicit	O
encoder	B
and	O
decoder	B
that	O
it	O
is	O
possible	O
reliably	O
is	O
with	O
zero	O
error	B
probability	B
to	O
communicate	O
bits	O
per	O
cycle	O
over	O
this	O
channel	O
exercise	O
a	O
channel	O
with	O
input	O
x	O
fa	O
b	O
cg	O
and	O
output	O
y	O
fr	O
s	O
t	O
ug	O
has	O
conditional	B
probability	B
matrix	B
q	O
what	O
is	O
its	O
capacity	B
hhj	O
hhj	O
hhj	O
a	O
b	O
c	O
r	O
s	O
t	O
u	O
table	O
some	O
valid	O
isbns	O
hyphens	O
are	O
included	O
for	O
legibility	O
exercise	O
the	O
ten-digit	O
number	O
on	O
the	O
cover	B
of	O
a	O
book	O
known	O
as	O
the	O
isbn	B
incorporates	O
an	O
error-detecting	O
code	O
the	O
number	O
consists	O
of	O
nine	O
source	O
digits	O
satisfying	O
xn	O
and	O
a	O
tenth	O
check	O
digit	O
whose	O
value	O
is	O
given	O
by	O
nxn	O
mod	O
here	O
if	O
then	O
the	O
tenth	O
digit	O
is	O
shown	O
using	O
the	O
roman	B
numeral	O
x	O
show	O
that	O
a	O
valid	O
isbn	B
nxn	O
mod	O
imagine	O
that	O
an	O
isbn	B
is	O
communicated	O
over	O
an	O
unreliable	O
human	B
channel	O
which	O
sometimes	O
digits	O
and	O
sometimes	O
reorders	O
digits	O
show	O
that	O
this	O
code	O
can	O
be	O
used	O
to	O
detect	O
not	O
correct	O
all	O
errors	B
in	O
which	O
any	O
one	O
of	O
the	O
ten	O
digits	O
is	O
example	O
show	O
that	O
this	O
code	O
can	O
be	O
used	O
to	O
detect	O
all	O
errors	B
in	O
which	O
any	O
two	O
adjacent	O
digits	O
are	O
transposed	O
example	O
what	O
other	O
transpositions	O
of	O
pairs	O
of	O
non-adjacent	O
digits	O
can	O
be	O
detected	O
if	O
the	O
tenth	O
digit	O
were	O
to	O
be	O
nxn	O
mod	O
why	O
would	O
the	O
code	O
not	O
work	O
so	O
well	O
the	O
detection	O
of	O
both	O
of	O
single	O
digits	O
and	O
transpositions	O
of	O
digits	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
information	B
theory	I
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
remember	O
d	O
dp	O
p	O
exercise	O
a	O
channel	O
with	O
input	O
x	O
and	O
output	O
y	O
has	O
transition	B
proba	O
bility	O
matrix	B
f	O
f	O
g	O
g	O
g	O
g	O
f	O
f	O
q	O
px	O
p	O
assuming	O
an	O
input	O
distribution	B
of	O
the	O
form	O
p	O
p	O
p	O
write	O
down	O
the	O
entropy	B
of	O
the	O
output	O
hy	O
and	O
the	O
conditional	B
entropy	B
of	O
the	O
output	O
given	O
the	O
input	O
hy	O
jx	O
show	O
that	O
the	O
optimal	B
input	I
distribution	B
is	O
given	O
by	O
p	O
where	O
f	O
write	O
down	O
the	O
optimal	B
input	I
distribution	B
and	O
the	O
capacity	B
of	O
the	O
channel	O
in	O
the	O
case	O
f	O
g	O
and	O
comment	O
on	O
your	O
answer	O
f	O
f	O
exercise	O
what	O
are	O
the	O
in	O
the	O
redundancies	O
needed	O
in	O
an	O
error-detecting	O
code	O
can	O
reliably	O
detect	O
that	O
a	O
block	B
of	O
data	O
has	O
been	O
corrupted	O
and	O
an	O
error-correcting	B
code	I
can	O
detect	O
and	O
correct	O
errors	B
further	O
tales	O
from	O
information	B
theory	I
the	O
following	O
exercises	O
give	O
you	O
the	O
chance	O
to	O
discover	O
for	O
yourself	O
the	O
answers	O
to	O
some	O
more	O
surprising	O
results	O
of	O
information	B
theory	I
exercise	O
communication	B
of	O
information	B
from	O
correlated	B
sources	I
imagine	O
that	O
we	O
want	O
to	O
communicate	O
data	O
from	O
two	O
data	O
sources	O
x	O
and	O
x	O
to	O
a	O
central	O
location	O
c	O
via	O
noise-free	O
one-way	B
communication	B
channels	O
the	O
signals	O
xa	O
and	O
xb	O
are	O
strongly	O
dependent	O
so	O
their	O
joint	B
information	B
content	I
is	O
only	O
a	O
little	O
greater	O
than	O
the	O
marginal	B
information	B
content	I
of	O
either	O
of	O
them	O
for	O
example	O
c	O
is	O
a	O
weather	B
collator	I
who	O
wishes	O
to	O
receive	O
a	O
string	O
of	O
reports	O
saying	O
whether	O
it	O
is	O
raining	O
in	O
allerton	O
and	O
whether	O
it	O
is	O
raining	O
in	O
bognor	O
the	O
joint	B
probability	B
of	O
xa	O
and	O
xb	O
might	O
be	O
p	O
xb	O
xa	O
xb	O
the	O
weather	B
collator	I
would	O
like	O
to	O
know	O
n	O
successive	O
values	O
of	O
xa	O
and	O
xb	O
exactly	O
but	O
since	O
he	O
has	O
to	O
pay	O
for	O
every	O
bit	B
of	O
information	B
he	O
receives	O
he	O
is	O
interested	O
in	O
the	O
possibility	O
of	O
avoiding	O
buying	O
n	O
bits	O
from	O
source	O
a	O
and	O
n	O
bits	O
from	O
source	O
b	O
assuming	O
that	O
variables	O
xa	O
and	O
xb	O
are	O
generated	O
repeatedly	O
from	O
this	O
distribution	B
can	O
they	O
be	O
encoded	O
at	O
rates	O
ra	O
and	O
rb	O
in	O
such	O
a	O
way	O
that	O
c	O
can	O
reconstruct	O
all	O
the	O
variables	O
with	O
the	O
sum	O
of	O
information	B
transmission	O
rates	O
on	O
the	O
two	O
lines	O
being	O
less	O
than	O
two	O
bits	O
per	O
cycle	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
information	B
theory	I
xa	O
encode	O
ra	O
ta	O
xb	O
encode	O
rb	O
tb	O
hhhj	O
c	O
rb	O
hx	O
x	O
hx	O
hx	O
j	O
x	O
achievable	O
hx	O
j	O
x	O
hx	O
ra	O
the	O
answer	O
which	O
you	O
should	O
demonstrate	O
is	O
indicated	O
in	O
in	O
the	O
general	O
case	O
of	O
two	O
dependent	B
sources	I
x	O
and	O
x	O
there	O
exist	O
codes	O
for	O
the	O
two	O
transmitters	O
that	O
can	O
achieve	O
reliable	O
communication	B
of	O
both	O
x	O
and	O
x	O
to	O
c	O
as	O
long	O
as	O
the	O
information	B
rate	B
from	O
x	O
ra	O
exceeds	O
hx	O
j	O
x	O
the	O
information	B
rate	B
from	O
x	O
rb	O
exceeds	O
hx	O
j	O
x	O
and	O
the	O
total	O
information	B
rate	B
ra	O
rb	O
exceeds	O
the	O
joint	B
entropy	B
hx	O
x	O
and	O
wolf	B
so	O
in	O
the	O
case	O
of	O
xa	O
and	O
xb	O
above	O
each	O
transmitter	O
must	O
transmit	O
at	O
a	O
rate	B
greater	O
than	O
bits	O
and	O
the	O
total	O
rate	B
ra	O
rb	O
must	O
be	O
greater	O
than	O
bits	O
for	O
example	O
ra	O
rb	O
there	O
exist	O
codes	O
that	O
can	O
achieve	O
these	O
rates	O
your	O
task	O
is	O
to	O
out	O
why	O
this	O
is	O
so	O
try	O
to	O
an	O
explicit	O
solution	O
in	O
which	O
one	O
of	O
the	O
sources	O
is	O
sent	O
as	O
plain	O
text	O
tb	O
xb	O
and	O
the	O
other	O
is	O
encoded	O
exercise	O
multiple	B
access	I
channels	O
consider	O
a	O
channel	O
with	O
two	O
sets	O
of	O
inputs	O
and	O
one	O
output	O
for	O
example	O
a	O
shared	O
telephone	O
line	O
a	O
simple	O
model	B
system	O
has	O
two	O
binary	O
inputs	O
xa	O
and	O
xb	O
and	O
a	O
ternary	O
output	O
y	O
equal	O
to	O
the	O
arithmetic	O
sum	O
of	O
the	O
two	O
inputs	O
that	O
s	O
or	O
there	O
is	O
no	O
noise	O
users	O
a	O
and	O
b	O
cannot	O
communicate	O
with	O
each	O
other	O
and	O
they	O
cannot	O
hear	O
the	O
output	O
of	O
the	O
channel	O
if	O
the	O
output	O
is	O
a	O
the	O
receiver	O
can	O
be	O
certain	O
that	O
both	O
inputs	O
were	O
set	B
to	O
and	O
if	O
the	O
output	O
is	O
a	O
the	O
receiver	O
can	O
be	O
certain	O
that	O
both	O
inputs	O
were	O
set	B
to	O
but	O
if	O
the	O
output	O
is	O
then	O
it	O
could	O
be	O
that	O
the	O
input	O
state	O
was	O
or	O
how	O
should	O
users	O
a	O
and	O
b	O
use	O
this	O
channel	O
so	O
that	O
their	O
messages	O
can	O
be	O
deduced	O
from	O
the	O
received	O
signals	O
how	O
fast	O
can	O
a	O
and	O
b	O
communicate	O
clearly	O
the	O
total	O
information	B
rate	B
from	O
a	O
and	O
b	O
to	O
the	O
receiver	O
cannot	O
be	O
two	O
bits	O
on	O
the	O
other	O
hand	O
it	O
is	O
easy	O
to	O
achieve	O
a	O
total	O
information	B
rate	B
rarb	O
of	O
one	O
bit	B
can	O
reliable	O
communication	B
be	O
achieved	O
at	O
rates	O
rb	O
such	O
that	O
ra	O
rb	O
the	O
answer	O
is	O
indicated	O
in	O
some	O
practical	B
codes	O
for	O
multi-user	O
channels	O
are	O
presented	O
in	O
ratzer	O
and	O
mackay	B
exercise	O
broadcast	B
channels	O
a	O
broadcast	B
channel	I
consists	O
of	O
a	O
single	O
transmitter	O
and	O
two	O
or	O
more	O
receivers	O
the	O
properties	O
of	O
the	O
channel	O
are	O
by	O
a	O
conditional	B
distribution	B
qya	O
yb	O
j	O
x	O
ll	O
assume	O
the	O
channel	O
is	O
memoryless	O
the	O
task	O
is	O
to	O
add	O
an	O
encoder	B
and	O
two	O
decoders	O
to	O
enable	O
reliable	O
communication	B
of	O
a	O
common	O
message	O
at	O
rate	B
to	O
both	O
receivers	O
an	O
individual	O
message	O
at	O
rate	B
ra	O
to	O
receiver	O
a	O
and	O
an	O
individual	O
message	O
at	O
rate	B
rb	O
to	O
receiver	O
b	O
the	O
capacity	B
region	O
of	O
the	O
broadcast	B
channel	I
is	O
the	O
convex	B
hull	I
of	O
the	O
set	B
of	O
achievable	O
rate	B
triplets	O
ra	O
rb	O
a	O
simple	O
benchmark	O
for	O
such	O
a	O
channel	O
is	O
given	O
by	O
time-sharing	O
signaling	O
if	O
the	O
capacities	O
of	O
the	O
two	O
channels	O
considered	O
separately	O
figure	O
communication	B
of	O
information	B
from	O
dependent	B
sources	I
xa	O
and	O
xb	O
are	O
dependent	B
sources	I
dependence	O
is	O
represented	O
by	O
the	O
dotted	O
arrow	O
strings	O
of	O
values	O
of	O
each	O
variable	O
are	O
encoded	O
using	O
codes	O
of	O
rate	B
ra	O
and	O
rb	O
into	O
transmissions	O
ta	O
and	O
tb	O
which	O
are	O
communicated	O
over	O
noise-free	O
channels	O
to	O
a	O
receiver	O
c	O
the	O
achievable	O
rate	B
region	O
both	O
strings	O
can	O
be	O
conveyed	O
without	O
error	O
even	O
though	O
ra	O
hx	O
and	O
rb	O
hx	O
x	O
hhj	O
ya	O
yb	O
figure	O
the	O
broadcast	B
channel	I
x	O
is	O
the	O
channel	O
input	O
ya	O
and	O
yb	O
are	O
the	O
outputs	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
information	B
theory	I
xa	O
xb	O
p	O
xb	O
y	O
rb	O
y	O
xb	O
xa	O
achievable	O
ra	O
are	O
c	O
and	O
c	O
then	O
by	O
devoting	O
a	O
fraction	O
of	O
the	O
transmission	O
time	O
to	O
channel	O
a	O
and	O
to	O
channel	O
b	O
we	O
can	O
achieve	O
ra	O
rb	O
we	O
can	O
do	O
better	O
than	O
this	O
however	O
as	O
an	O
analogy	O
imagine	O
speaking	O
simultaneously	O
to	O
an	O
american	B
and	O
a	O
belarusian	B
you	O
are	O
in	O
american	B
and	O
in	O
belarusian	B
but	O
neither	O
of	O
your	O
two	O
receivers	O
understands	O
the	O
other	O
s	O
language	O
if	O
each	O
receiver	O
can	O
distinguish	O
whether	O
a	O
word	O
is	O
in	O
their	O
own	O
language	O
or	O
not	O
then	O
an	O
extra	O
binary	O
can	O
be	O
conveyed	O
to	O
both	O
recipients	O
by	O
using	O
its	O
bits	O
to	O
decide	O
whether	O
the	O
next	O
transmitted	O
word	O
should	O
be	O
from	O
the	O
american	B
source	O
text	O
or	O
from	O
the	O
belarusian	B
source	O
text	O
each	O
recipient	O
can	O
concatenate	O
the	O
words	O
that	O
they	O
understand	O
in	O
order	O
to	O
receive	O
their	O
personal	O
message	O
and	O
can	O
also	O
recover	O
the	O
binary	O
string	O
an	O
example	O
of	O
a	O
broadcast	B
channel	I
consists	O
of	O
two	O
binary	B
symmetric	B
channels	O
with	O
a	O
common	O
input	O
the	O
two	O
halves	O
of	O
the	O
channel	O
have	O
probabilities	O
fa	O
and	O
fb	O
we	O
ll	O
assume	O
that	O
a	O
has	O
the	O
better	O
half-channel	O
i	O
e	O
fa	O
fb	O
closely	O
related	O
channel	O
is	O
a	O
degraded	O
broadcast	B
channel	I
in	O
which	O
the	O
conditional	B
probabilities	O
are	O
such	O
that	O
the	O
random	B
variables	O
have	O
the	O
structure	O
of	O
a	O
markov	B
chain	I
x	O
ya	O
yb	O
i	O
e	O
yb	O
is	O
a	O
further	O
degraded	O
version	O
of	O
ya	O
in	O
this	O
special	O
case	O
it	O
turns	O
out	O
that	O
whatever	O
information	B
is	O
getting	O
through	O
to	O
receiver	O
b	O
can	O
also	O
be	O
recovered	O
by	O
receiver	O
a	O
so	O
there	O
is	O
no	O
point	O
distinguishing	O
between	O
and	O
rb	O
the	O
task	O
is	O
to	O
the	O
capacity	B
region	O
for	O
the	O
rate	B
pair	O
ra	O
where	O
is	O
the	O
rate	B
of	O
information	B
reaching	O
both	O
a	O
and	O
b	O
and	O
ra	O
is	O
the	O
rate	B
of	O
the	O
extra	O
information	B
reaching	O
a	O
the	O
following	O
exercise	O
is	O
equivalent	O
to	O
this	O
one	O
and	O
a	O
solution	O
to	O
it	O
is	O
illustrated	O
in	O
exercise	O
variable-rate	O
error-correcting	B
codes	I
for	O
channels	O
with	O
unknown	B
noise	I
level	I
in	O
real	O
life	B
channels	O
may	O
sometimes	O
not	O
be	O
well	O
characterized	O
before	O
the	O
encoder	B
is	O
installed	O
as	O
a	O
model	B
of	O
this	O
situation	O
imagine	O
that	O
a	O
channel	O
is	O
known	O
to	O
be	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
either	O
fa	O
or	O
fb	O
let	O
fb	O
fa	O
and	O
let	O
the	O
two	O
capacities	O
be	O
ca	O
and	O
cb	O
those	O
who	O
like	O
to	O
live	O
dangerously	O
might	O
install	O
a	O
system	O
designed	O
for	O
noise	O
level	O
fa	O
with	O
rate	B
ra	O
ca	O
in	O
the	O
event	O
that	O
the	O
noise	O
level	O
turns	O
out	O
to	O
be	O
fb	O
our	O
experience	O
of	O
shannon	B
s	O
theories	O
would	O
lead	O
us	O
to	O
expect	O
that	O
there	O
figure	O
multiple	B
access	I
channels	O
a	O
general	O
multiple	B
access	I
channel	I
with	O
two	O
transmitters	O
and	O
one	O
receiver	O
a	O
binary	O
multiple	B
access	I
channel	I
with	O
output	O
equal	O
to	O
the	O
sum	O
of	O
two	O
inputs	O
the	O
achievable	O
region	O
rb	O
c	O
c	O
ra	O
figure	O
rates	O
achievable	O
by	O
simple	O
timesharing	O
r	O
c	O
a	O
bc	O
f	O
a	O
f	O
b	O
f	O
figure	O
rate	B
of	O
reliable	O
communication	B
r	O
as	O
a	O
function	O
of	O
noise	O
level	O
f	O
for	O
shannonesque	O
codes	O
designed	O
to	O
operate	O
at	O
noise	O
levels	O
fa	O
line	O
and	O
fb	O
line	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
information	B
theory	I
r	O
c	O
a	O
bc	O
f	O
a	O
f	O
b	O
f	O
figure	O
rate	B
of	O
reliable	O
communication	B
r	O
as	O
a	O
function	O
of	O
noise	O
level	O
f	O
for	O
a	O
desired	O
variable-rate	O
code	O
figure	O
an	O
achievable	O
region	O
for	O
the	O
channel	O
with	O
unknown	B
noise	I
level	I
assuming	O
the	O
two	O
possible	O
noise	O
levels	O
are	O
fa	O
and	O
fb	O
the	O
dashed	O
lines	O
show	O
the	O
rates	O
ra	O
rb	O
that	O
are	O
achievable	O
using	O
a	O
simple	O
time-sharing	O
approach	O
and	O
the	O
solid	O
line	O
shows	O
rates	O
achievable	O
using	O
a	O
more	O
cunning	O
approach	O
would	O
be	O
a	O
catastrophic	O
failure	O
to	O
communicate	O
information	B
reliably	O
line	O
in	O
a	O
conservative	O
approach	O
would	O
design	O
the	O
encoding	O
system	O
for	O
the	O
worstcase	O
scenario	O
installing	O
a	O
code	O
with	O
rate	B
rb	O
cb	O
line	O
in	O
in	O
the	O
event	O
that	O
the	O
lower	O
noise	O
level	O
fa	O
holds	O
true	O
the	O
managers	O
would	O
have	O
a	O
feeling	O
of	O
regret	B
because	O
of	O
the	O
wasted	O
capacity	B
ca	O
rb	O
is	O
it	O
possible	O
to	O
create	O
a	O
system	O
that	O
not	O
only	O
transmits	O
reliably	O
at	O
some	O
rate	B
whatever	O
the	O
noise	O
level	O
but	O
also	O
communicates	O
some	O
extra	O
lowerpriority	O
bits	O
if	O
the	O
noise	O
level	O
is	O
low	O
as	O
shown	O
in	O
this	O
code	O
communicates	O
the	O
high-priority	O
bits	O
reliably	O
at	O
all	O
noise	O
levels	O
between	O
fa	O
and	O
fb	O
and	O
communicates	O
the	O
low-priority	O
bits	O
also	O
if	O
the	O
noise	O
level	O
is	O
fa	O
or	O
below	O
this	O
problem	O
is	O
mathematically	O
equivalent	O
to	O
the	O
previous	O
problem	O
the	O
degraded	O
broadcast	B
channel	I
the	O
lower	O
rate	B
of	O
communication	B
was	O
there	O
called	O
and	O
the	O
rate	B
at	O
which	O
the	O
low-priority	O
bits	O
are	O
communicated	O
if	O
the	O
noise	O
level	O
is	O
low	O
was	O
called	O
ra	O
an	O
illustrative	O
answer	O
is	O
shown	O
in	O
for	O
the	O
case	O
fa	O
and	O
fb	O
also	O
shows	O
the	O
achievable	O
region	O
for	O
a	O
broadcast	B
channel	I
whose	O
two	O
half-channels	O
have	O
noise	O
levels	O
fa	O
and	O
fb	O
i	O
admit	O
i	O
the	O
gap	O
between	O
the	O
simple	O
time-sharing	O
solution	O
and	O
the	O
cunning	O
solution	O
disappointingly	O
small	O
in	O
chapter	O
we	O
will	O
discuss	O
codes	O
for	O
a	O
special	O
class	O
of	O
broadcast	B
channels	O
namely	O
erasure	B
channels	O
where	O
every	O
symbol	O
is	O
either	O
received	O
without	O
error	O
or	O
erased	O
these	O
codes	O
have	O
the	O
nice	O
property	O
that	O
they	O
are	O
rateless	B
the	O
number	O
of	O
symbols	O
transmitted	O
is	O
determined	O
on	O
the	O
such	O
that	O
reliable	O
comunication	O
is	O
achieved	O
whatever	O
the	O
erasure	B
statistics	O
of	O
the	O
channel	O
exercise	O
multiterminal	B
information	B
networks	O
are	O
both	O
important	O
practically	O
and	O
intriguing	O
theoretically	O
consider	O
the	O
following	O
example	O
of	O
a	O
two-way	O
binary	O
channel	O
two	O
people	O
both	O
wish	O
to	O
talk	O
over	O
the	O
channel	O
and	O
they	O
both	O
want	O
to	O
hear	O
what	O
the	O
other	O
person	O
is	O
saying	O
but	O
you	O
can	O
hear	O
the	O
signal	O
transmitted	O
by	O
the	O
other	O
person	O
only	O
if	O
you	O
are	O
transmitting	O
a	O
zero	O
what	O
simultaneous	O
information	B
rates	O
from	O
a	O
to	O
b	O
and	O
from	O
b	O
to	O
a	O
can	O
be	O
achieved	O
and	O
how	O
everyday	O
examples	O
of	O
such	O
networks	O
include	O
the	O
vhf	O
channels	O
used	O
by	O
ships	O
and	O
computer	B
ethernet	O
networks	O
which	O
all	O
the	O
devices	O
are	O
unable	O
to	O
hear	O
anything	O
if	O
two	O
or	O
more	O
devices	O
are	O
broadcasting	O
simultaneously	O
obviously	O
we	O
can	O
achieve	O
rates	O
of	O
in	O
both	O
directions	O
by	O
simple	O
timesharing	O
but	O
can	O
the	O
two	O
information	B
rates	O
be	O
made	O
larger	O
finding	O
the	O
capacity	B
of	O
a	O
general	O
two-way	O
channel	O
is	O
still	O
an	O
open	O
problem	O
however	O
we	O
can	O
obtain	O
interesting	O
results	O
concerning	O
achievable	O
points	O
for	O
the	O
simple	O
binary	O
channel	O
discussed	O
above	O
as	O
indicated	O
in	O
there	O
exist	O
codes	O
that	O
can	O
achieve	O
rates	O
up	O
to	O
the	O
boundary	O
shown	O
there	O
may	O
exist	O
better	O
codes	O
too	O
solutions	O
solution	O
to	O
exercise	O
cq	O
bits	O
hint	O
for	O
the	O
last	O
part	O
a	O
solution	O
exists	O
that	O
involves	O
a	O
simple	O
code	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
on	O
information	B
theory	I
xb	O
xa	O
figure	O
a	O
general	O
two-way	O
channel	O
the	O
rules	B
for	O
a	O
binary	O
two-way	O
channel	O
the	O
two	O
tables	O
show	O
the	O
outputs	O
ya	O
and	O
yb	O
that	O
result	O
for	O
each	O
state	O
of	O
the	O
inputs	O
achievable	O
region	O
for	O
the	O
two-way	O
binary	O
channel	O
rates	O
below	O
the	O
solid	O
line	O
are	O
achievable	O
the	O
dotted	O
line	O
shows	O
the	O
obviously	O
achievable	O
region	O
which	O
can	O
be	O
attained	O
by	O
simple	O
time-sharing	O
xa	O
ya	O
p	O
ybjxa	O
xb	O
yb	O
ya	O
xb	O
xa	O
yb	O
xb	O
b	O
r	O
achievable	O
ra	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
message	B
passing	I
one	O
of	O
the	O
themes	O
of	O
this	O
book	O
is	O
the	O
idea	O
of	O
doing	O
complicated	O
calculations	O
using	O
simple	O
distributed	O
hardware	O
it	O
turns	O
out	O
that	O
quite	O
a	O
few	O
interesting	O
problems	O
can	O
be	O
solved	O
by	O
message-passing	B
algorithms	B
in	O
which	O
simple	O
messages	O
are	O
passed	O
locally	O
among	O
simple	O
processors	O
whose	O
operations	O
lead	O
after	O
some	O
time	O
to	O
the	O
solution	O
of	O
a	O
global	O
problem	O
counting	B
as	O
an	O
example	O
consider	O
a	O
line	O
of	O
soldiers	O
walking	O
in	O
the	O
mist	O
the	O
commander	B
wishes	O
to	O
perform	O
the	O
complex	B
calculation	O
of	O
counting	B
the	O
number	O
of	O
soldiers	O
in	O
the	O
line	O
this	O
problem	O
could	O
be	O
solved	O
in	O
two	O
ways	O
first	O
there	O
is	O
a	O
solution	O
that	O
uses	O
expensive	O
hardware	O
the	O
loud	O
booming	O
voices	O
of	O
the	O
commander	B
and	O
his	O
men	O
the	O
commander	B
could	O
shout	O
all	O
soldiers	O
report	O
back	O
to	O
me	O
within	O
one	O
minute	O
then	O
he	O
could	O
listen	O
carefully	O
as	O
the	O
men	O
respond	O
molesworth	B
here	O
sir	O
fotheringtonthomas	O
here	O
sir	O
and	O
so	O
on	O
this	O
solution	O
relies	O
on	O
several	O
expensive	O
pieces	O
of	O
hardware	O
there	O
must	O
be	O
a	O
reliable	O
communication	B
channel	O
to	O
and	O
from	O
every	O
soldier	B
the	O
commander	B
must	O
be	O
able	O
to	O
listen	O
to	O
all	O
the	O
incoming	O
messages	O
even	O
when	O
there	O
are	O
hundreds	O
of	O
soldiers	O
and	O
must	O
be	O
able	O
to	O
count	O
and	O
all	O
the	O
soldiers	O
must	O
be	O
well-fed	O
if	O
they	O
are	O
to	O
be	O
able	O
to	O
shout	O
back	O
across	O
the	O
possibly-large	O
distance	B
separating	O
them	O
from	O
the	O
commander	B
the	O
second	O
way	O
of	O
this	O
global	O
function	O
the	O
number	O
of	O
soldiers	O
does	O
not	O
require	O
global	O
communication	B
hardware	O
high	O
iq	O
or	O
good	B
food	O
we	O
simply	O
require	O
that	O
each	O
soldier	B
can	O
communicate	O
single	O
integers	O
with	O
the	O
two	O
adjacent	O
soldiers	O
in	O
the	O
line	O
and	O
that	O
the	O
soldiers	O
are	O
capable	O
of	O
adding	O
one	O
to	O
a	O
number	O
each	O
soldier	B
follows	O
these	O
rules	B
if	O
you	O
are	O
the	O
front	O
soldier	B
in	O
the	O
line	O
say	O
the	O
number	O
one	O
to	O
the	O
soldier	B
behind	O
you	O
algorithm	O
message-passing	B
rule-set	O
a	O
if	O
you	O
are	O
the	O
rearmost	O
soldier	B
in	O
the	O
line	O
say	O
the	O
number	O
one	O
to	O
the	O
soldier	B
in	O
front	O
of	O
you	O
if	O
a	O
soldier	B
ahead	O
of	O
or	O
behind	O
you	O
says	O
a	O
number	O
to	O
you	O
add	O
one	O
to	O
it	O
and	O
say	O
the	O
new	O
number	O
to	O
the	O
soldier	B
on	O
the	O
other	O
side	O
if	O
the	O
clever	O
commander	B
can	O
not	O
only	O
add	O
one	O
to	O
a	O
number	O
but	O
also	O
add	O
two	O
numbers	O
together	O
then	O
he	O
can	O
the	O
global	O
number	O
of	O
soldiers	O
by	O
simply	O
adding	O
together	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
message	B
passing	I
figure	O
a	O
line	O
of	O
soldiers	O
counting	B
themselves	O
using	O
message-passing	B
rule-set	O
a	O
the	O
commander	B
can	O
add	O
from	O
the	O
soldier	B
in	O
front	O
from	O
the	O
soldier	B
behind	O
and	O
for	O
himself	O
and	O
deduce	O
that	O
there	O
are	O
soldiers	O
in	O
total	O
figure	O
a	O
swarm	O
of	O
guerillas	O
the	O
number	O
said	O
to	O
him	O
by	O
the	O
soldier	B
in	O
front	O
of	O
him	O
the	O
number	O
said	O
to	O
the	O
commander	B
by	O
the	O
soldier	B
behind	O
him	O
one	O
equals	O
the	O
total	O
number	O
of	O
soldiers	O
in	O
front	O
is	O
the	O
number	O
behind	O
count	O
the	O
commander	B
himself	O
this	O
solution	O
requires	O
only	O
local	O
communication	B
hardware	O
and	O
simple	O
computations	O
and	O
addition	O
of	O
integers	O
commander	B
separation	B
this	O
clever	O
trick	O
makes	O
use	O
of	O
a	O
profound	O
property	O
of	O
the	O
total	O
number	O
of	O
soldiers	O
that	O
it	O
can	O
be	O
written	O
as	O
the	O
sum	O
of	O
the	O
number	O
of	O
soldiers	O
in	O
front	O
of	O
a	O
point	O
and	O
the	O
number	O
behind	O
that	O
point	O
two	O
quantities	O
which	O
can	O
be	O
computed	O
separately	O
because	O
the	O
two	O
groups	O
are	O
separated	O
by	O
the	O
commander	B
if	O
the	O
soldiers	O
were	O
not	O
arranged	O
in	O
a	O
line	O
but	O
were	O
travelling	O
in	O
a	O
swarm	O
then	O
it	O
would	O
not	O
be	O
easy	O
to	O
separate	O
them	O
into	O
two	O
groups	O
in	O
this	O
way	O
the	O
commander	B
jim	O
guerillas	O
in	O
could	O
not	O
be	O
counted	O
using	O
the	O
above	O
message-passing	B
rule-set	O
a	O
because	O
while	O
the	O
guerillas	O
do	O
have	O
neighbours	O
by	O
lines	O
it	O
is	O
not	O
clear	O
who	O
is	O
in	O
front	O
and	O
who	O
is	O
behind	O
furthermore	O
since	O
the	O
graph	B
of	O
connections	O
between	O
the	O
guerillas	O
contains	O
cycles	O
it	O
is	O
not	O
possible	O
for	O
a	O
guerilla	B
in	O
a	O
cycle	O
as	O
jim	O
to	O
separate	O
the	O
group	O
into	O
two	O
groups	O
those	O
in	O
front	O
and	O
those	O
behind	O
a	O
swarm	O
of	O
guerillas	O
can	O
be	O
counted	O
by	O
a	O
message-passing	B
algo	O
rithm	O
if	O
they	O
are	O
arranged	O
in	O
a	O
graph	B
that	O
contains	O
no	O
cycles	O
rule-set	O
b	O
is	O
a	O
message-passing	B
algorithm	O
for	O
counting	B
a	O
swarm	O
of	O
guerillas	O
whose	O
connections	O
form	O
a	O
cycle-free	O
graph	B
also	O
known	O
as	O
a	O
tree	B
as	O
illustrated	O
in	O
any	O
guerilla	B
can	O
deduce	O
the	O
total	O
in	O
the	O
tree	B
from	O
the	O
messages	O
that	O
they	O
receive	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
counting	B
figure	O
a	O
swarm	O
of	O
guerillas	O
whose	O
connections	O
form	O
a	O
tree	B
commander	B
jim	O
algorithm	O
message-passing	B
rule-set	O
b	O
count	O
your	O
number	O
of	O
neighbours	O
n	O
keep	O
count	O
of	O
the	O
number	O
of	O
messages	O
you	O
have	O
received	O
from	O
your	O
neighbours	O
m	O
and	O
of	O
the	O
values	O
vn	O
of	O
each	O
of	O
those	O
messages	O
let	O
v	O
be	O
the	O
running	O
total	O
of	O
the	O
messages	O
you	O
have	O
received	O
if	O
the	O
number	O
of	O
messages	O
you	O
have	O
received	O
m	O
is	O
equal	O
to	O
n	O
then	O
identify	O
the	O
neighbour	O
who	O
has	O
not	O
sent	O
you	O
a	O
message	O
and	O
tell	O
them	O
the	O
number	O
v	O
if	O
the	O
number	O
of	O
messages	O
you	O
have	O
received	O
is	O
equal	O
to	O
n	O
then	O
the	O
number	O
v	O
is	O
the	O
required	O
total	O
for	O
each	O
neighbour	O
n	O
f	O
say	O
to	O
neighbour	O
n	O
the	O
number	O
v	O
vn	O
g	O
a	O
figure	O
a	O
triangular	O
grid	O
how	O
many	O
paths	O
are	O
there	O
from	O
a	O
to	O
b	O
one	O
path	O
is	O
shown	O
b	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
path-counting	B
a	O
more	O
profound	O
task	O
than	O
counting	B
squaddies	O
is	O
the	O
task	O
of	O
counting	B
the	O
number	O
of	O
paths	O
through	O
a	O
grid	O
and	O
how	O
many	O
paths	O
pass	O
through	O
any	O
given	O
point	O
in	O
the	O
grid	O
figure	O
shows	O
a	O
rectangular	B
grid	O
and	O
a	O
path	O
through	O
the	O
grid	O
connecting	O
points	O
a	O
and	O
b	O
a	O
valid	O
path	O
is	O
one	O
that	O
starts	O
from	O
a	O
and	O
proceeds	O
to	O
b	O
by	O
rightward	O
and	O
downward	O
moves	O
our	O
questions	O
are	O
how	O
many	O
such	O
paths	O
are	O
there	O
from	O
a	O
to	O
b	O
if	O
a	O
random	B
path	O
from	O
a	O
to	O
b	O
is	O
selected	O
what	O
is	O
the	O
probability	B
that	O
it	O
passes	O
through	O
a	O
particular	O
node	O
in	O
the	O
grid	O
we	O
say	O
random	B
we	O
mean	B
that	O
all	O
paths	O
have	O
exactly	O
the	O
same	O
probability	B
of	O
being	O
selected	O
how	O
can	O
a	O
random	B
path	O
from	O
a	O
to	O
b	O
be	O
selected	O
counting	B
all	O
the	O
paths	O
from	O
a	O
to	O
b	O
doesn	O
t	O
seem	O
straightforward	O
the	O
number	O
of	O
paths	O
is	O
expected	O
to	O
be	O
pretty	O
big	O
even	O
if	O
the	O
permitted	O
grid	O
were	O
a	O
diagonal	O
strip	O
only	O
three	O
nodes	O
wide	O
there	O
would	O
still	O
be	O
about	O
possible	O
paths	O
the	O
computational	O
breakthrough	O
is	O
to	O
realize	O
that	O
to	O
the	O
number	O
of	O
paths	O
we	O
do	O
not	O
have	O
to	O
enumerate	O
all	O
the	O
paths	O
explicitly	O
pick	O
a	O
point	O
p	O
in	O
the	O
grid	O
and	O
consider	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
p	O
every	O
path	O
from	O
a	O
to	O
p	O
must	O
come	O
in	O
to	O
p	O
through	O
one	O
of	O
its	O
upstream	O
neighbours	O
upstream	O
meaning	O
above	O
or	O
to	O
the	O
left	O
so	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
p	O
can	O
be	O
found	O
by	O
adding	O
up	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
each	O
of	O
those	O
neighbours	O
this	O
message-passing	B
algorithm	O
is	O
illustrated	O
in	O
for	O
a	O
simple	O
grid	O
with	O
ten	O
vertices	O
connected	O
by	O
twelve	O
directed	O
edges	O
we	O
start	O
by	O
sending	O
the	O
message	O
from	O
a	O
when	O
any	O
node	O
has	O
received	O
messages	O
from	O
all	O
its	O
upstream	O
neighbours	O
it	O
sends	O
the	O
sum	O
of	O
them	O
on	O
to	O
its	O
downstream	O
neighbours	O
at	O
b	O
the	O
number	O
emerges	O
we	O
have	O
counted	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
b	O
without	O
enumerating	O
them	O
all	O
as	O
a	O
sanity-check	O
shows	O
the	O
distinct	O
paths	O
from	O
a	O
to	O
b	O
having	O
counted	O
all	O
paths	O
we	O
can	O
now	O
move	O
on	O
to	O
more	O
challenging	O
problems	O
computing	O
the	O
probability	B
that	O
a	O
random	B
path	O
goes	O
through	O
a	O
given	O
vertex	O
and	O
creating	O
a	O
random	B
path	O
probability	B
of	O
passing	O
through	O
a	O
node	O
by	O
making	O
a	O
backward	B
pass	I
as	O
well	O
as	O
the	O
forward	B
pass	I
we	O
can	O
deduce	O
how	O
many	O
of	O
the	O
paths	O
go	O
through	O
each	O
node	O
and	O
if	O
we	O
divide	O
that	O
by	O
the	O
total	O
number	O
of	O
paths	O
we	O
obtain	O
the	O
probability	B
that	O
a	O
randomly	O
selected	O
path	O
passes	O
through	O
that	O
node	O
figure	O
shows	O
the	O
backward-passing	O
messages	O
in	O
the	O
lower-right	O
corners	O
of	O
the	O
tables	O
and	O
the	O
original	O
forward-passing	O
messages	O
in	O
the	O
upper-left	O
corners	O
by	O
multiplying	O
these	O
two	O
numbers	O
at	O
a	O
given	O
vertex	O
we	O
the	O
total	O
number	O
of	O
paths	O
passing	O
through	O
that	O
vertex	O
for	O
example	O
four	O
paths	O
pass	O
through	O
the	O
central	O
vertex	O
figure	O
shows	O
the	O
result	O
of	O
this	O
computation	O
for	O
the	O
triangular	O
grid	O
the	O
area	O
of	O
each	O
blob	O
is	O
proportional	O
to	O
the	O
probability	B
of	O
passing	O
through	O
the	O
corresponding	O
node	O
random	B
path	O
sampling	O
exercise	O
if	O
one	O
creates	O
a	O
random	B
path	O
from	O
a	O
to	O
b	O
by	O
a	O
fair	O
coin	B
at	O
every	O
junction	O
where	O
there	O
is	O
a	O
choice	O
of	O
two	O
directions	O
is	O
message	B
passing	I
a	O
n	O
p	O
m	O
b	O
figure	O
every	O
path	O
from	O
a	O
to	O
p	O
enters	O
p	O
through	O
an	O
upstream	O
neighbour	O
of	O
p	O
either	O
m	O
or	O
n	O
so	O
we	O
can	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
p	O
by	O
adding	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
m	O
to	O
the	O
number	O
from	O
a	O
to	O
n	O
a	O
b	O
figure	O
messages	O
sent	O
in	O
the	O
forward	B
pass	I
a	O
b	O
figure	O
the	O
paths	O
a	O
b	O
figure	O
messages	O
sent	O
in	O
the	O
forward	O
and	O
backward	O
passes	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
finding	O
the	O
lowest-cost	O
path	O
the	O
resulting	O
path	O
a	O
uniform	O
random	B
sample	B
from	I
the	O
set	B
of	O
all	O
paths	O
imagine	O
trying	O
it	O
for	O
the	O
grid	O
of	O
there	O
is	O
a	O
neat	O
insight	O
to	O
be	O
had	O
here	O
and	O
i	O
d	O
like	O
you	O
to	O
have	O
the	O
satisfaction	O
of	O
it	O
out	O
exercise	O
having	O
run	O
the	O
forward	O
and	O
backward	O
algorithms	B
between	O
points	O
a	O
and	O
b	O
on	O
a	O
grid	O
how	O
can	O
one	O
draw	O
one	O
path	O
from	O
a	O
to	O
b	O
uniformly	O
at	O
random	B
a	O
figure	O
the	O
probability	B
of	O
passing	O
through	O
each	O
node	O
and	O
a	O
randomly	O
chosen	O
path	O
b	O
the	O
message-passing	B
algorithm	O
we	O
used	O
to	O
count	O
the	O
paths	O
to	O
b	O
is	O
an	O
example	O
of	O
the	O
sumproduct	O
algorithm	O
the	O
sum	O
takes	O
place	O
at	O
each	O
node	O
when	O
it	O
adds	O
together	O
the	O
messages	O
coming	O
from	O
its	O
predecessors	O
the	O
product	O
was	O
not	O
mentioned	O
but	O
you	O
can	O
think	O
of	O
the	O
sum	O
as	O
a	O
weighted	O
sum	O
in	O
which	O
all	O
the	O
summed	O
terms	O
happened	O
to	O
have	O
weight	O
finding	O
the	O
lowest-cost	O
path	O
imagine	O
you	O
wish	O
to	O
travel	O
as	O
quickly	O
as	O
possible	O
from	O
ambridge	O
to	O
bognor	O
the	O
various	O
possible	O
routes	O
are	O
shown	O
in	O
along	O
with	O
the	O
cost	O
in	O
hours	O
of	O
traversing	O
each	O
edge	B
in	O
the	O
graph	B
for	O
example	O
the	O
route	O
ail	O
nb	O
has	O
a	O
cost	O
of	O
hours	O
we	O
would	O
like	O
to	O
the	O
lowest-cost	O
path	O
without	O
explicitly	O
evaluating	O
the	O
cost	O
of	O
all	O
paths	O
we	O
can	O
do	O
this	O
by	O
for	O
each	O
node	O
what	O
the	O
cost	O
of	O
the	O
lowest-cost	O
path	O
to	O
that	O
node	O
from	O
a	O
is	O
these	O
quantities	O
can	O
be	O
computed	O
by	O
message-passing	B
starting	O
from	O
node	O
a	O
the	O
message-passing	B
algorithm	O
is	O
called	O
the	O
minsum	O
algorithm	O
or	O
viterbi	B
algorithm	I
for	O
brevity	O
we	O
ll	O
call	O
the	O
cost	O
of	O
the	O
lowest-cost	O
path	O
from	O
node	O
a	O
to	O
node	O
x	O
the	O
cost	O
of	O
x	O
each	O
node	O
can	O
broadcast	B
its	O
cost	O
to	O
its	O
descendants	O
once	O
it	O
knows	O
the	O
costs	O
of	O
all	O
its	O
possible	O
predecessors	O
let	O
s	O
step	O
through	O
the	O
algorithm	O
by	O
hand	O
the	O
cost	O
of	O
a	O
is	O
zero	O
we	O
pass	O
this	O
news	O
on	O
to	O
h	O
and	O
i	O
as	O
the	O
message	O
passes	O
along	O
each	O
edge	B
in	O
the	O
graph	B
the	O
cost	O
of	O
that	O
edge	B
is	O
added	O
we	O
the	O
costs	O
of	O
h	O
and	O
i	O
are	O
and	O
respectively	O
similarly	O
then	O
the	O
costs	O
of	O
j	O
and	O
l	O
are	O
found	O
to	O
be	O
and	O
respectively	O
but	O
what	O
about	O
k	O
out	O
of	O
the	O
edge	B
hk	O
comes	O
the	O
message	O
that	O
a	O
path	O
of	O
cost	O
exists	O
from	O
a	O
to	O
k	O
via	O
h	O
and	O
from	O
edge	B
ik	O
we	O
learn	O
of	O
an	O
alternative	O
path	O
of	O
cost	O
the	O
minsum	O
algorithm	O
sets	O
the	O
cost	O
of	O
k	O
equal	O
to	O
the	O
minimum	O
of	O
these	O
min	O
and	O
records	O
which	O
was	O
the	O
smallest-cost	O
route	O
into	O
k	O
by	O
retaining	O
only	O
the	O
edge	B
ik	O
and	O
pruning	O
away	O
the	O
other	O
edges	O
leading	O
to	O
k	O
figures	O
and	O
e	O
show	O
the	O
remaining	O
two	O
iterations	O
of	O
the	O
algorithm	O
which	O
reveal	O
that	O
there	O
is	O
a	O
path	O
from	O
a	O
to	O
b	O
with	O
cost	O
the	O
minsum	O
algorithm	O
encounters	O
a	O
tie	O
where	O
the	O
minimum-cost	O
a	O
hhhj	O
h	O
i	O
hhhj	O
hhhj	O
j	O
k	O
l	O
hhhj	O
hhhj	O
m	O
n	O
hhhj	O
b	O
figure	O
route	O
diagram	O
from	O
ambridge	O
to	O
bognor	O
showing	O
the	O
costs	O
associated	O
with	O
the	O
edges	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
message	B
passing	I
path	O
to	O
a	O
node	O
is	O
achieved	O
by	O
more	O
than	O
one	O
route	O
to	O
it	O
then	O
the	O
algorithm	O
can	O
pick	O
any	O
of	O
those	O
routes	O
at	O
random	B
we	O
can	O
recover	O
this	O
lowest-cost	O
path	O
by	O
backtracking	O
from	O
b	O
following	O
the	O
trail	O
of	O
surviving	O
edges	O
back	O
to	O
a	O
we	O
deduce	O
that	O
the	O
lowest-cost	O
path	O
is	O
aikmb	O
other	O
applications	O
of	O
the	O
minsum	O
algorithm	O
imagine	O
that	O
you	O
manage	O
the	O
production	O
of	O
a	O
product	O
from	O
raw	O
materials	O
via	O
a	O
large	O
set	B
of	O
operations	O
you	O
wish	O
to	O
identify	O
the	O
critical	B
path	I
in	O
your	O
process	O
that	O
is	O
the	O
subset	B
of	O
operations	O
that	O
are	O
holding	O
up	O
production	O
if	O
any	O
operations	O
on	O
the	O
critical	B
path	I
were	O
carried	O
out	O
a	O
little	O
faster	O
then	O
the	O
time	O
to	O
get	O
from	O
raw	O
materials	O
to	O
product	O
would	O
be	O
reduced	O
the	O
critical	B
path	I
of	O
a	O
set	B
of	O
operations	O
can	O
be	O
found	O
using	O
the	O
minsum	O
algorithm	O
in	O
chapter	O
the	O
minsum	O
algorithm	O
will	O
be	O
used	O
in	O
the	O
decoding	B
of	O
error-correcting	B
codes	I
summary	B
and	O
related	O
ideas	O
some	O
global	O
functions	B
have	O
a	O
separability	O
property	O
for	O
example	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
p	O
separates	O
into	O
the	O
sum	O
of	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
m	O
point	O
to	O
p	O
s	O
left	O
and	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
n	O
point	O
above	O
p	O
such	O
functions	B
can	O
be	O
computed	O
by	O
message-passing	B
other	O
functions	B
do	O
not	O
have	O
such	O
separability	O
properties	O
for	O
example	O
the	O
number	O
of	O
pairs	O
of	O
soldiers	O
in	O
a	O
troop	O
who	O
share	O
the	O
same	O
birthday	B
the	O
size	O
of	O
the	O
largest	O
group	O
of	O
soldiers	O
who	O
share	O
a	O
common	O
height	O
to	O
the	O
nearest	O
centimetre	O
the	O
length	O
of	O
the	O
shortest	O
tour	O
that	O
a	O
travelling	O
salesman	O
could	O
take	O
that	O
visits	O
every	O
soldier	B
in	O
a	O
troop	O
one	O
of	O
the	O
challenges	B
of	O
machine	B
learning	B
is	O
to	O
low-cost	O
solutions	O
to	O
problems	O
like	O
these	O
the	O
problem	O
of	O
a	O
large	O
subset	B
of	O
variables	O
that	O
are	O
approximately	O
equal	O
can	O
be	O
solved	O
with	O
a	O
neural	B
network	B
approach	O
and	O
brody	B
and	O
brody	B
a	O
neural	O
approach	O
to	O
the	O
travelling	B
salesman	I
problem	I
will	O
be	O
discussed	O
in	O
section	B
a	O
a	O
a	O
a	O
a	O
hhhj	O
h	O
i	O
hhhj	O
hhhj	O
hhhj	O
hhhj	O
hhhj	O
hhhj	O
h	O
i	O
h	O
i	O
h	O
i	O
h	O
i	O
hhj	O
hhhj	O
hhhj	O
hhhj	O
hhhj	O
j	O
k	O
l	O
j	O
k	O
l	O
j	O
k	O
l	O
j	O
k	O
l	O
j	O
k	O
l	O
m	O
n	O
hhhj	O
b	O
m	O
n	O
hhhj	O
b	O
m	O
n	O
hhhj	O
b	O
hhhj	O
hhhj	O
hhhj	O
hhhj	O
hhhj	O
hhhj	O
m	O
hhhj	O
hhhj	O
n	O
b	O
hhhj	O
m	O
hhhj	O
n	O
b	O
further	O
exercises	O
exercise	O
describe	O
the	O
asymptotic	O
properties	O
of	O
the	O
probabilities	O
de	O
picted	O
in	O
for	O
a	O
grid	O
in	O
a	O
triangle	B
of	O
width	O
and	O
height	O
n	O
exercise	O
in	O
image	B
processing	I
the	O
integral	B
image	B
ix	O
y	O
obtained	O
from	O
an	O
image	B
f	O
y	O
x	O
and	O
y	O
are	O
pixel	O
coordinates	O
is	O
by	O
figure	O
minsum	O
message-passing	B
algorithm	O
to	O
the	O
cost	O
of	O
getting	O
to	O
each	O
node	O
and	O
thence	O
the	O
lowest	O
cost	O
route	O
from	O
a	O
to	O
b	O
ix	O
y	O
x	O
y	O
f	O
v	O
show	O
that	O
the	O
integral	B
image	B
ix	O
y	O
can	O
be	O
computed	O
by	O
message	B
passing	I
show	O
that	O
from	O
the	O
integral	B
image	B
some	O
simple	O
functions	B
of	O
the	O
image	B
can	O
be	O
obtained	O
for	O
example	O
give	O
an	O
expression	O
for	O
the	O
sum	O
of	O
the	O
image	B
intensities	O
f	O
y	O
for	O
all	O
y	O
in	O
a	O
rectangular	B
region	O
extending	O
from	O
to	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solutions	O
solution	O
to	O
exercise	O
since	O
there	O
are	O
paths	O
through	O
the	O
grid	O
of	O
they	O
must	O
all	O
have	O
probability	B
but	O
a	O
strategy	O
based	O
on	O
fair	O
will	O
produce	O
paths	O
whose	O
probabilities	O
are	O
powers	O
of	O
solution	O
to	O
exercise	O
to	O
make	O
a	O
uniform	O
random	B
walk	I
each	O
forward	O
step	O
of	O
the	O
walk	O
should	O
be	O
chosen	O
using	O
a	O
biased	O
coin	B
at	O
each	O
junction	O
with	O
the	O
biases	O
chosen	O
in	O
proportion	O
to	O
the	O
backward	O
messages	O
emanating	O
from	O
the	O
two	O
options	O
for	O
example	O
at	O
the	O
choice	O
after	O
leaving	O
a	O
there	O
is	O
a	O
message	O
coming	O
from	O
the	O
east	O
and	O
a	O
coming	O
from	O
south	O
so	O
one	O
should	O
go	O
east	O
with	O
probability	B
and	O
south	O
with	O
probability	B
this	O
is	O
how	O
the	O
path	O
in	O
was	O
generated	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
constrained	B
noiseless	B
channels	O
in	O
this	O
chapter	O
we	O
study	O
the	O
task	O
of	O
communicating	O
over	O
a	O
constrained	B
noiseless	B
channel	O
a	O
constrained	B
channel	I
over	O
which	O
not	O
all	O
strings	O
from	O
the	O
input	O
alphabet	O
may	O
be	O
transmitted	O
we	O
make	O
use	O
of	O
the	O
idea	O
introduced	O
in	O
chapter	O
that	O
global	O
properties	O
of	O
graphs	O
can	O
be	O
computed	O
by	O
a	O
local	O
message-passing	B
algorithm	O
three	O
examples	O
of	O
constrained	B
binary	O
channels	O
a	O
constrained	B
channel	I
can	O
be	O
by	O
rules	B
that	O
which	O
strings	O
are	O
permitted	O
example	O
in	O
channel	O
a	O
every	O
must	O
be	O
followed	O
by	O
at	O
least	O
one	O
a	O
valid	O
string	O
for	O
this	O
channel	O
is	O
channel	O
a	O
the	O
substring	B
is	O
forbidden	O
as	O
a	O
motivation	O
for	O
this	O
model	B
consider	O
a	O
channel	O
in	O
which	O
are	O
represented	O
by	O
pulses	O
of	O
electromagnetic	O
energy	B
and	O
the	O
device	O
that	O
produces	O
those	O
pulses	O
requires	O
a	O
recovery	O
time	O
of	O
one	O
clock	O
cycle	O
after	O
generating	O
a	O
pulse	O
before	O
it	O
can	O
generate	O
another	O
example	O
channel	O
b	O
has	O
the	O
rule	O
that	O
all	O
must	O
come	O
in	O
groups	O
of	O
two	O
or	O
more	O
and	O
all	O
must	O
come	O
in	O
groups	O
of	O
two	O
or	O
more	O
a	O
valid	O
string	O
for	O
this	O
channel	O
is	O
channel	O
b	O
and	O
are	O
forbidden	O
as	O
a	O
motivation	O
for	O
this	O
model	B
consider	O
a	O
disk	B
drive	I
in	O
which	O
successive	O
bits	O
are	O
written	O
onto	O
neighbouring	O
points	O
in	O
a	O
track	O
along	O
the	O
disk	O
surface	O
the	O
values	O
and	O
are	O
represented	O
by	O
two	O
opposite	O
magnetic	O
orientations	O
the	O
strings	O
and	O
are	O
forbidden	O
because	O
a	O
single	O
isolated	O
magnetic	O
domain	O
surrounded	O
by	O
domains	O
having	O
the	O
opposite	O
orientation	O
is	O
unstable	O
so	O
that	O
might	O
turn	O
into	O
for	O
example	O
example	O
channel	O
c	O
has	O
the	O
rule	O
that	O
the	O
largest	O
permitted	O
runlength	B
is	O
two	O
that	O
is	O
each	O
symbol	O
can	O
be	O
repeated	O
at	O
most	O
once	O
a	O
valid	O
string	O
for	O
this	O
channel	O
is	O
channel	O
c	O
and	O
are	O
forbidden	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
three	O
examples	O
of	O
constrained	B
binary	O
channels	O
a	O
physical	O
motivation	O
for	O
this	O
model	B
is	O
a	O
disk	B
drive	I
in	O
which	O
the	O
rate	B
of	O
rotation	O
of	O
the	O
disk	O
is	O
not	O
known	O
accurately	O
so	O
it	O
is	O
to	O
distinguish	O
between	O
a	O
string	O
of	O
two	O
and	O
a	O
string	O
of	O
three	O
which	O
are	O
represented	O
by	O
oriented	O
magnetizations	O
of	O
duration	O
and	O
respectively	O
where	O
is	O
the	O
known	O
time	O
taken	O
for	O
one	O
bit	B
to	O
pass	O
by	O
to	O
avoid	O
the	O
possibility	O
of	O
confusion	O
and	O
the	O
resulting	O
loss	O
of	O
synchronization	B
of	O
sender	O
and	O
receiver	O
we	O
forbid	O
the	O
string	O
of	O
three	O
and	O
the	O
string	O
of	O
three	O
all	O
three	O
of	O
these	O
channels	O
are	O
examples	O
of	O
runlength-limited	O
channels	O
the	O
rules	B
constrain	O
the	O
minimum	O
and	O
maximum	O
numbers	O
of	O
successive	O
and	O
channel	O
runlength	B
of	O
runlength	B
of	O
minimum	O
maximum	O
minimum	O
maximum	O
unconstrained	O
a	O
b	O
c	O
in	O
channel	O
a	O
runs	O
of	O
may	O
be	O
of	O
any	O
length	O
but	O
runs	O
of	O
are	O
restricted	O
to	O
length	O
one	O
in	O
channel	O
b	O
all	O
runs	O
must	O
be	O
of	O
length	O
two	O
or	O
more	O
in	O
channel	O
c	O
all	O
runs	O
must	O
be	O
of	O
length	O
one	O
or	O
two	O
the	O
capacity	B
of	O
the	O
unconstrained	O
binary	O
channel	O
is	O
one	O
bit	B
per	O
channel	O
use	O
what	O
are	O
the	O
capacities	O
of	O
the	O
three	O
constrained	B
channels	O
be	O
fair	O
we	O
haven	O
t	O
the	O
capacity	B
of	O
such	O
channels	O
yet	O
please	O
understand	O
capacity	B
as	O
meaning	O
how	O
many	O
bits	O
can	O
be	O
conveyed	O
reliably	O
per	O
channel-use	O
some	O
codes	O
for	O
a	O
constrained	B
channel	I
let	O
us	O
concentrate	O
for	O
a	O
moment	O
on	O
channel	O
a	O
in	O
which	O
runs	O
of	O
may	O
be	O
of	O
any	O
length	O
but	O
runs	O
of	O
are	O
restricted	O
to	O
length	O
one	O
we	O
would	O
like	O
to	O
communicate	O
a	O
random	B
binary	O
over	O
this	O
channel	O
as	O
as	O
possible	O
a	O
simple	O
starting	O
point	O
is	O
a	O
code	O
that	O
maps	O
each	O
source	O
bit	B
into	O
two	O
transmitted	O
bits	O
this	O
is	O
a	O
code	O
and	O
it	O
respects	O
the	O
constraints	O
of	O
channel	O
a	O
so	O
the	O
capacity	B
of	O
channel	O
a	O
is	O
at	O
least	O
can	O
we	O
do	O
better	O
is	O
redundant	O
because	O
if	O
the	O
of	O
two	O
received	O
bits	O
is	O
a	O
zero	O
we	O
know	O
that	O
the	O
second	O
bit	B
will	O
also	O
be	O
a	O
zero	O
we	O
can	O
achieve	O
a	O
smaller	O
average	O
transmitted	O
length	O
using	O
a	O
code	O
that	O
omits	O
the	O
redundant	O
zeroes	O
in	O
is	O
such	O
a	O
variable-length	B
code	I
if	O
the	O
source	O
symbols	O
are	O
used	O
with	O
equal	O
frequency	B
then	O
the	O
average	O
transmitted	O
length	O
per	O
source	O
bit	B
is	O
l	O
so	O
the	O
average	O
communication	B
rate	B
is	O
r	O
and	O
the	O
capacity	B
of	O
channel	O
a	O
must	O
be	O
at	O
least	O
can	O
we	O
do	O
better	O
than	O
there	O
are	O
two	O
ways	O
to	O
argue	O
that	O
the	O
infor	O
mation	O
rate	B
could	O
be	O
increased	O
above	O
r	O
the	O
argument	O
assumes	O
we	O
are	O
comfortable	O
with	O
the	O
entropy	B
as	O
a	O
measure	O
of	O
information	B
content	I
the	O
idea	O
is	O
that	O
starting	O
from	O
code	O
we	O
can	O
reduce	O
the	O
average	O
message	O
length	O
without	O
greatly	O
reducing	O
the	O
entropy	B
code	O
s	O
t	O
code	O
s	O
t	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
constrained	B
noiseless	B
channels	O
rf	O
figure	O
top	O
the	O
information	B
content	I
per	O
source	O
symbol	O
and	O
mean	B
transmitted	O
length	O
per	O
source	O
symbol	O
as	O
a	O
function	O
of	O
the	O
source	O
density	B
bottom	O
the	O
information	B
content	I
per	O
transmitted	O
symbol	O
in	O
bits	O
as	O
a	O
function	O
of	O
f	O
of	O
the	O
message	O
we	O
send	O
by	O
decreasing	O
the	O
fraction	O
of	O
that	O
we	O
transmit	O
imagine	O
feeding	O
into	O
a	O
stream	O
of	O
bits	O
in	O
which	O
the	O
frequency	B
of	O
is	O
f	O
a	O
stream	O
could	O
be	O
obtained	O
from	O
an	O
arbitrary	O
binary	O
by	O
passing	O
the	O
source	O
into	O
the	O
decoder	B
of	O
an	O
arithmetic	O
code	O
that	O
is	O
optimal	B
for	O
compressing	O
binary	O
strings	O
of	O
density	B
f	O
the	O
information	B
rate	B
r	O
achieved	O
is	O
the	O
entropy	B
of	O
the	O
source	O
divided	O
by	O
the	O
mean	B
transmitted	O
length	O
thus	O
lf	O
f	O
f	O
rf	O
lf	O
f	O
the	O
original	O
code	O
without	O
preprocessor	O
corresponds	O
to	O
f	O
what	O
happens	O
if	O
we	O
perturb	O
f	O
a	O
little	O
towards	O
smaller	O
f	O
setting	O
f	O
for	O
small	O
negative	O
in	O
the	O
vicinity	O
of	O
f	O
the	O
denominator	O
lf	O
varies	O
linearly	O
with	O
in	O
contrast	O
the	O
numerator	O
only	O
has	O
a	O
second-order	O
dependence	O
on	O
exercise	O
find	O
to	O
order	O
the	O
taylor	O
expansion	O
of	O
as	O
a	O
function	O
of	O
to	O
order	O
rf	O
increases	O
linearly	O
with	O
decreasing	O
it	O
must	O
be	O
possible	O
to	O
increase	O
r	O
by	O
decreasing	O
f	O
figure	O
shows	O
these	O
functions	B
rf	O
does	O
indeed	O
increase	O
as	O
f	O
decreases	O
and	O
has	O
a	O
maximum	O
of	O
about	O
bits	O
per	O
channel	O
use	O
at	O
f	O
maxf	O
rf	O
by	O
this	O
argument	O
we	O
have	O
shown	O
that	O
the	O
capacity	B
of	O
channel	O
a	O
is	O
at	O
least	O
exercise	O
if	O
a	O
containing	O
a	O
fraction	O
f	O
is	O
transmitted	O
by	O
what	O
fraction	O
of	O
the	O
transmitted	O
stream	O
is	O
what	O
fraction	O
of	O
the	O
transmitted	O
bits	O
is	O
if	O
we	O
drive	O
code	O
with	O
a	O
sparse	O
source	O
of	O
density	B
f	O
a	O
second	O
more	O
fundamental	O
approach	O
counts	O
how	O
many	O
valid	O
sequences	O
of	O
length	O
n	O
there	O
are	O
sn	O
we	O
can	O
communicate	O
log	O
sn	O
bits	O
in	O
n	O
channel	O
cycles	O
by	O
giving	O
one	O
name	O
to	O
each	O
of	O
these	O
valid	O
sequences	O
the	O
capacity	B
of	O
a	O
constrained	B
noiseless	B
channel	O
we	O
the	O
capacity	B
of	O
a	O
noisy	B
channel	O
in	O
terms	O
of	O
the	O
mutual	B
information	B
between	O
its	O
input	O
and	O
its	O
output	O
then	O
we	O
proved	O
that	O
this	O
number	O
the	O
capacity	B
was	O
related	O
to	O
the	O
number	O
of	O
distinguishable	O
messages	O
sn	O
that	O
could	O
be	O
reliably	O
conveyed	O
over	O
the	O
channel	O
in	O
n	O
uses	O
of	O
the	O
channel	O
by	O
c	O
lim	O
n	O
log	O
sn	O
in	O
the	O
case	O
of	O
the	O
constrained	B
noiseless	B
channel	O
we	O
can	O
adopt	O
this	O
identity	O
as	O
our	O
of	O
the	O
channel	O
s	O
capacity	B
however	O
the	O
name	O
s	O
which	O
when	O
we	O
were	O
making	O
codes	O
for	O
noisy	B
channels	O
ran	O
over	O
messages	O
s	O
s	O
is	O
about	O
to	O
take	O
on	O
a	O
new	O
role	O
labelling	O
the	O
states	O
of	O
our	O
channel	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
counting	B
the	O
number	O
of	O
possible	O
messages	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
f	O
sn	O
j	O
j	O
b	O
sn	O
j	O
j	O
m	O
m	O
m	O
m	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
aau	O
m	O
m	O
m	O
m	O
figure	O
state	B
diagram	I
for	O
channel	O
a	O
trellis	B
section	B
trellis	B
connection	B
matrix	B
figure	O
state	O
diagrams	O
trellis	B
sections	O
and	O
connection	O
matrices	B
for	O
channels	O
b	O
and	O
c	O
a	O
c	O
a	O
a	O
a	O
a	O
a	O
aau	O
sn	O
a	O
n	O
n	O
n	O
n	O
a	O
n	O
n	O
n	O
n	O
so	O
in	O
this	O
chapter	O
we	O
will	O
denote	O
the	O
number	O
of	O
distinguishable	O
messages	O
of	O
length	O
n	O
by	O
mn	O
and	O
the	O
capacity	B
to	O
be	O
c	O
lim	O
n	O
log	O
mn	O
once	O
we	O
have	O
out	O
the	O
capacity	B
of	O
a	O
channel	O
we	O
will	O
return	O
to	O
the	O
task	O
of	O
making	O
a	O
practical	B
code	O
for	O
that	O
channel	O
counting	B
the	O
number	O
of	O
possible	O
messages	O
first	O
let	O
us	O
introduce	O
some	O
representations	O
of	O
constrained	B
channels	O
in	O
a	O
state	B
diagram	I
states	O
of	O
the	O
transmitter	O
are	O
represented	O
by	O
circles	O
labelled	O
with	O
the	O
name	O
of	O
the	O
state	O
directed	O
edges	O
from	O
one	O
state	O
to	O
another	O
indicate	O
that	O
the	O
transmitter	O
is	O
permitted	O
to	O
move	O
from	O
the	O
state	O
to	O
the	O
second	O
and	O
a	O
label	O
on	O
that	O
edge	B
indicates	O
the	O
symbol	O
emitted	O
when	O
that	O
transition	B
is	O
made	O
figure	O
shows	O
the	O
state	B
diagram	I
for	O
channel	O
a	O
it	O
has	O
two	O
states	O
and	O
when	O
transitions	O
to	O
state	O
are	O
made	O
a	O
is	O
transmitted	O
when	O
transitions	O
to	O
state	O
are	O
made	O
a	O
is	O
transmitted	O
transitions	O
from	O
state	O
to	O
state	O
are	O
not	O
possible	O
we	O
can	O
also	O
represent	O
the	O
state	B
diagram	I
by	O
a	O
trellis	B
section	B
which	O
shows	O
two	O
successive	O
states	O
in	O
time	O
at	O
two	O
successive	O
horizontal	O
locations	O
the	O
state	O
of	O
the	O
transmitter	O
at	O
time	O
n	O
is	O
called	O
sn	O
the	O
set	B
of	O
possible	O
state	O
sequences	O
can	O
be	O
represented	O
by	O
a	O
trellis	B
as	O
shown	O
in	O
a	O
valid	O
sequence	B
corresponds	O
to	O
a	O
path	O
through	O
the	O
trellis	B
and	O
the	O
number	O
of	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
constrained	B
noiseless	B
channels	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
figure	O
counting	B
the	O
number	O
of	O
paths	O
in	O
the	O
trellis	B
of	O
channel	O
a	O
the	O
counts	O
next	O
to	O
the	O
nodes	O
are	O
accumulated	O
by	O
passing	O
from	O
left	O
to	O
right	O
across	O
the	O
trellises	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
channel	O
a	O
channel	O
b	O
channel	O
c	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
a	O
a	O
a	O
a	O
a	O
aau	O
a	O
aau	O
a	O
a	O
h	O
h	O
h	O
h	O
a	O
a	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
aau	O
a	O
a	O
a	O
aau	O
a	O
a	O
h	O
h	O
h	O
h	O
a	O
a	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
a	O
a	O
a	O
a	O
a	O
aau	O
h	O
h	O
h	O
h	O
figure	O
counting	B
the	O
number	O
of	O
paths	O
in	O
the	O
trellises	O
of	O
channels	O
a	O
b	O
and	O
c	O
we	O
assume	O
that	O
at	O
the	O
start	O
the	O
bit	B
is	O
preceded	O
by	O
so	O
that	O
for	O
channels	O
a	O
and	O
b	O
any	O
initial	O
character	O
is	O
permitted	O
but	O
for	O
channel	O
c	O
the	O
character	O
must	O
be	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
counting	B
the	O
number	O
of	O
possible	O
messages	O
n	O
mn	O
mn	O
n	O
mn	O
figure	O
counting	B
the	O
number	O
of	O
paths	O
in	O
the	O
trellis	B
of	O
channel	O
a	O
valid	O
sequences	O
is	O
the	O
number	O
of	O
paths	O
for	O
the	O
purpose	O
of	O
counting	B
how	O
many	O
paths	O
there	O
are	O
through	O
the	O
trellis	B
we	O
can	O
ignore	O
the	O
labels	O
on	O
the	O
edges	O
and	O
summarize	O
the	O
trellis	B
section	B
by	O
the	O
connection	B
matrix	B
a	O
in	O
which	O
if	O
there	O
is	O
an	O
edge	B
from	O
state	O
s	O
to	O
and	O
otherwise	O
figure	O
shows	O
the	O
state	O
diagrams	O
trellis	B
sections	O
and	O
connection	O
matrices	B
for	O
channels	O
b	O
and	O
c	O
let	O
s	O
count	O
the	O
number	O
of	O
paths	O
for	O
channel	O
a	O
by	O
message-passing	B
in	O
its	O
trellis	B
figure	O
shows	O
the	O
few	O
steps	O
of	O
this	O
counting	B
process	O
and	O
shows	O
the	O
number	O
of	O
paths	O
ending	O
in	O
each	O
state	O
after	O
n	O
steps	O
for	O
n	O
the	O
total	O
number	O
of	O
paths	O
of	O
length	O
n	O
mn	O
is	O
shown	O
along	O
the	O
top	O
we	O
recognize	O
mn	O
as	O
the	O
fibonacci	B
series	O
exercise	O
show	O
that	O
the	O
ratio	O
of	O
successive	O
terms	O
in	O
the	O
fibonacci	B
series	O
tends	O
to	O
the	O
golden	B
ratio	I
thus	O
to	O
within	O
a	O
constant	O
factor	O
mn	O
scales	O
as	O
mn	O
as	O
n	O
so	O
the	O
capacity	B
of	O
channel	O
a	O
is	O
c	O
lim	O
n	O
how	O
can	O
we	O
describe	O
what	O
we	O
just	O
did	O
the	O
count	O
of	O
the	O
number	O
of	O
paths	O
is	O
a	O
vector	O
cn	O
we	O
can	O
obtain	O
from	O
cn	O
using	O
acn	O
so	O
cn	O
an	O
where	O
is	O
the	O
state	O
count	O
before	O
any	O
symbols	O
are	O
transmitted	O
in	O
we	O
assumed	O
i	O
e	O
that	O
either	O
of	O
the	O
two	O
symbols	O
is	O
permitted	O
at	O
s	O
cn	O
n	O
in	O
the	O
limit	O
the	O
outset	O
the	O
total	O
number	O
of	O
paths	O
is	O
mn	O
cn	O
cn	O
becomes	O
dominated	O
by	O
the	O
principal	O
right-eigenvector	O
of	O
a	O
cn	O
constant	O
r	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
constrained	B
noiseless	B
channels	O
here	O
is	O
the	O
principal	O
eigenvalue	B
of	O
a	O
so	O
to	O
the	O
capacity	B
of	O
any	O
constrained	B
channel	I
all	O
we	O
need	O
to	O
do	O
is	O
the	O
principal	O
eigenvalue	B
of	O
its	O
connection	B
matrix	B
then	O
c	O
back	O
to	O
our	O
model	B
channels	O
comparing	O
and	O
and	O
c	O
it	O
looks	O
as	O
if	O
channels	O
b	O
and	O
c	O
have	O
the	O
same	O
capacity	B
as	O
channel	O
a	O
the	O
principal	O
eigenvalues	O
of	O
the	O
three	O
trellises	O
are	O
the	O
same	O
eigenvectors	O
for	O
channels	O
a	O
and	O
b	O
are	O
given	O
at	O
the	O
bottom	O
of	O
table	O
and	O
indeed	O
the	O
channels	O
are	O
intimately	O
related	O
t	O
hd	O
s	O
hd	O
t	O
s	O
figure	O
an	O
accumulator	B
and	O
a	O
equivalence	B
of	O
channels	O
a	O
and	O
b	O
if	O
we	O
take	O
any	O
valid	O
string	O
s	O
for	O
channel	O
a	O
and	O
pass	O
it	O
through	O
an	O
accumulator	B
obtaining	O
t	O
by	O
tn	O
sn	O
mod	O
for	O
n	O
then	O
the	O
resulting	O
string	O
is	O
a	O
valid	O
string	O
for	O
channel	O
b	O
because	O
there	O
are	O
no	O
in	O
s	O
so	O
there	O
are	O
no	O
isolated	O
digits	O
in	O
t	O
the	O
accumulator	B
is	O
an	O
invertible	O
operator	O
so	O
similarly	O
any	O
valid	O
string	O
t	O
for	O
channel	O
b	O
can	O
be	O
mapped	O
onto	O
a	O
valid	O
string	O
s	O
for	O
channel	O
a	O
through	O
the	O
binary	O
sn	O
tn	O
mod	O
for	O
n	O
because	O
and	O
are	O
equivalent	O
in	O
modulo	O
arithmetic	O
the	O
is	O
also	O
a	O
blurrer	O
convolving	O
the	O
source	O
stream	O
with	O
the	O
channel	O
c	O
is	O
also	O
intimately	O
related	O
to	O
channels	O
a	O
and	O
b	O
exercise	O
what	O
is	O
the	O
relationship	O
of	O
channel	O
c	O
to	O
channels	O
a	O
and	O
b	O
practical	B
communication	B
over	O
constrained	B
channels	O
ok	O
how	O
to	O
do	O
it	O
in	O
practice	O
since	O
all	O
three	O
channels	O
are	O
equivalent	O
we	O
can	O
concentrate	O
on	O
channel	O
a	O
fixed-length	O
solutions	O
we	O
start	O
with	O
explicitly-enumerated	O
codes	O
the	O
code	O
in	O
the	O
table	O
achieves	O
a	O
rate	B
of	O
exercise	O
similarly	O
enumerate	O
all	O
strings	O
of	O
length	O
that	O
end	O
in	O
the	O
zero	O
state	O
are	O
of	O
them	O
hence	O
show	O
that	O
we	O
can	O
map	O
bits	O
source	O
strings	O
to	O
transmitted	O
bits	O
and	O
achieve	O
rate	B
what	O
rate	B
can	O
be	O
achieved	O
by	O
mapping	B
an	O
integer	O
number	O
of	O
source	O
bits	O
to	O
n	O
transmitted	O
bits	O
s	O
cs	O
table	O
a	O
runlength-limited	O
code	O
for	O
channel	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
practical	B
communication	B
over	O
constrained	B
channels	O
optimal	B
variable-length	B
solution	O
the	O
optimal	B
way	O
to	O
convey	O
information	B
over	O
the	O
constrained	B
channel	I
is	O
to	O
the	O
optimal	B
transition	B
probabilities	O
for	O
all	O
points	O
in	O
the	O
trellis	B
and	O
make	O
transitions	O
with	O
these	O
probabilities	O
when	O
discussing	O
channel	O
a	O
we	O
showed	O
that	O
a	O
sparse	O
source	O
with	O
density	B
f	O
driving	O
code	O
would	O
achieve	O
capacity	B
and	O
we	O
know	O
how	O
to	O
make	O
we	O
design	O
an	O
arithmetic	O
code	O
that	O
is	O
optimal	B
for	O
compressing	O
a	O
sparse	O
source	O
then	O
its	O
associated	O
decoder	B
gives	O
an	O
optimal	B
mapping	B
from	O
dense	O
random	B
binary	O
strings	O
to	O
sparse	O
strings	O
the	O
task	O
of	O
the	O
optimal	B
probabilities	O
is	O
given	O
as	O
an	O
exercise	O
exercise	O
show	O
that	O
the	O
optimal	B
transition	B
probabilities	O
q	O
can	O
be	O
found	O
as	O
follows	O
find	O
the	O
principal	O
right-	O
and	O
left-eigenvectors	O
of	O
a	O
that	O
is	O
the	O
solutions	O
of	O
aer	O
and	O
elt	O
with	O
largest	O
eigenvalue	B
then	O
construct	O
a	O
matrix	B
q	O
whose	O
invariant	B
distribution	B
is	O
proportional	O
to	O
er	O
i	O
a	O
namely	O
el	O
i	O
el	O
s	O
exercise	O
might	O
give	O
helpful	O
cross-fertilization	O
here	O
exercise	O
show	O
that	O
when	O
sequences	O
are	O
generated	O
using	O
the	O
optimal	B
transition	B
probability	B
matrix	B
the	O
entropy	B
of	O
the	O
resulting	O
sequence	B
is	O
asymptotically	O
per	O
symbol	O
consider	O
the	O
conditional	B
entropy	B
of	O
just	O
one	O
symbol	O
given	O
the	O
previous	O
one	O
assuming	O
the	O
previous	O
one	O
s	O
distribution	B
is	O
the	O
invariant	B
distribution	B
in	O
practice	O
we	O
would	O
probably	O
use	O
approximations	O
to	O
the	O
optimal	B
variable-length	B
solution	O
one	O
might	O
dislike	O
variable-length	B
solutions	O
because	O
of	O
the	O
resulting	O
unpredictability	O
of	O
the	O
actual	O
encoded	O
length	O
in	O
any	O
particular	O
case	O
perhaps	O
in	O
some	O
applications	O
we	O
would	O
like	O
a	O
guarantee	O
that	O
the	O
encoded	O
length	O
of	O
a	O
source	O
of	O
size	O
n	O
bits	O
will	O
be	O
less	O
than	O
a	O
given	O
length	O
such	O
as	O
nc	O
for	O
example	O
a	O
disk	B
drive	I
is	O
easier	O
to	O
control	O
if	O
all	O
blocks	O
of	O
bytes	O
are	O
known	O
to	O
take	O
exactly	O
the	O
same	O
amount	O
of	O
disk	O
real-estate	O
for	O
some	O
constrained	B
channels	O
we	O
can	O
make	O
a	O
simple	O
to	O
our	O
variable-length	B
encoding	O
and	O
such	O
a	O
guarantee	O
as	O
follows	O
we	O
two	O
codes	O
two	O
mappings	O
of	O
binary	O
strings	O
to	O
variable-length	B
encodings	O
having	O
the	O
property	O
that	O
for	O
any	O
source	O
string	O
x	O
if	O
the	O
encoding	O
of	O
x	O
under	O
the	O
code	O
is	O
shorter	O
than	O
average	O
then	O
the	O
encoding	O
of	O
x	O
under	O
the	O
second	O
code	O
is	O
longer	O
than	O
average	O
and	O
vice	O
versa	O
then	O
to	O
transmit	O
a	O
string	O
x	O
we	O
encode	O
the	O
whole	O
string	O
with	O
both	O
codes	O
and	O
send	O
whichever	O
encoding	O
has	O
the	O
shortest	O
length	O
prepended	O
by	O
a	O
suitably	O
encoded	O
single	O
bit	B
to	O
convey	O
which	O
of	O
the	O
two	O
codes	O
is	O
being	O
used	O
exercise	O
how	O
many	O
valid	O
sequences	O
of	O
length	O
starting	O
with	O
a	O
are	O
there	O
for	O
the	O
run-length-limited	O
channels	O
shown	O
in	O
what	O
are	O
the	O
capacities	O
of	O
these	O
channels	O
using	O
a	O
computer	B
the	O
matrices	B
q	O
for	O
generating	O
a	O
random	B
path	O
through	O
the	O
trellises	O
of	O
the	O
channel	O
a	O
and	O
the	O
two	O
run-length-limited	O
channels	O
shown	O
in	O
figure	O
state	O
diagrams	O
and	O
connection	O
matrices	B
for	O
channels	O
with	O
maximum	O
runlengths	O
for	O
equal	O
to	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
constrained	B
noiseless	B
channels	O
exercise	O
consider	O
the	O
run-length-limited	O
channel	O
in	O
which	O
any	O
length	O
of	O
run	O
of	O
is	O
permitted	O
and	O
the	O
maximum	O
run	O
length	O
of	O
is	O
a	O
large	O
number	O
l	O
such	O
as	O
nine	O
or	O
ninety	O
estimate	O
the	O
capacity	B
of	O
this	O
channel	O
the	O
two	O
terms	O
in	O
a	O
series	O
expansion	O
involving	O
l	O
what	O
roughly	O
is	O
the	O
form	O
of	O
the	O
optimal	B
matrix	B
q	O
for	O
generating	O
a	O
random	B
path	O
through	O
the	O
trellis	B
of	O
this	O
channel	O
focus	B
on	O
the	O
values	O
of	O
the	O
elements	O
the	O
probability	B
of	O
generating	O
a	O
given	O
a	O
preceding	O
and	O
the	O
probability	B
of	O
generating	O
a	O
given	O
a	O
preceding	O
run	O
of	O
check	O
your	O
answer	O
by	O
explicit	O
computation	O
for	O
the	O
channel	O
in	O
which	O
the	O
maximum	O
runlength	B
of	O
is	O
nine	O
variable	B
symbol	I
durations	I
we	O
can	O
add	O
a	O
further	O
frill	O
to	O
the	O
task	O
of	O
communicating	O
over	O
constrained	B
channels	O
by	O
assuming	O
that	O
the	O
symbols	O
we	O
send	O
have	O
durations	O
and	O
that	O
our	O
aim	O
is	O
to	O
communicate	O
at	O
the	O
maximum	O
possible	O
rate	B
per	O
unit	O
time	O
such	O
channels	O
can	O
come	O
in	O
two	O
unconstrained	O
and	O
constrained	B
unconstrained	O
channels	O
with	O
variable	B
symbol	I
durations	I
we	O
encountered	O
an	O
unconstrained	O
noiseless	B
channel	O
with	O
variable	B
symbol	I
durations	I
in	O
exercise	O
solve	O
that	O
problem	O
and	O
you	O
ve	O
done	O
this	O
topic	O
the	O
task	O
is	O
to	O
determine	O
the	O
optimal	B
frequencies	O
with	O
which	O
the	O
symbols	O
should	O
be	O
used	O
given	O
their	O
durations	O
there	O
is	O
a	O
nice	O
analogy	O
between	O
this	O
task	O
and	O
the	O
task	O
of	O
designing	O
an	O
optimal	B
symbol	B
code	I
when	O
we	O
make	O
an	O
binary	O
symbol	B
code	I
for	O
a	O
source	O
with	O
unequal	O
probabilities	O
pi	O
the	O
optimal	B
message	O
lengths	O
are	O
so	O
pi	O
i	O
similarly	O
when	O
we	O
have	O
a	O
channel	O
whose	O
symbols	O
have	O
durations	O
li	O
some	O
units	B
of	O
time	O
the	O
optimal	B
probability	B
with	O
which	O
those	O
symbols	O
should	O
be	O
used	O
is	O
where	O
is	O
the	O
capacity	B
of	O
the	O
channel	O
in	O
bits	O
per	O
unit	O
time	O
constrained	B
channels	O
with	O
variable	B
symbol	I
durations	I
once	O
you	O
have	O
grasped	O
the	O
preceding	O
topics	O
in	O
this	O
chapter	O
you	O
should	O
be	O
able	O
to	O
out	O
how	O
to	O
and	O
the	O
capacity	B
of	O
these	O
the	O
trickiest	O
constrained	B
channels	O
exercise	O
a	O
classic	O
example	O
of	O
a	O
constrained	B
channel	I
with	O
variable	B
symbol	I
durations	I
is	O
the	O
morse	B
channel	O
whose	O
symbols	O
are	O
the	O
dot	O
the	O
dash	O
the	O
short	O
space	O
between	O
letters	O
in	O
morse	B
code	O
the	O
long	O
space	O
between	O
words	O
d	O
d	O
s	O
and	O
s	O
the	O
constraints	O
are	O
that	O
spaces	O
may	O
only	O
be	O
followed	O
by	O
dots	O
and	O
dashes	O
find	O
the	O
capacity	B
of	O
this	O
channel	O
in	O
bits	O
per	O
unit	O
time	O
assuming	O
that	O
all	O
four	O
symbols	O
have	O
equal	O
durations	O
or	O
that	O
the	O
symbol	O
durations	O
are	O
and	O
time	O
units	B
respectively	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
exercise	O
how	O
well-designed	O
is	O
morse	B
code	O
for	O
english	B
say	O
the	O
probability	B
distribution	B
of	O
exercise	O
how	O
is	O
it	O
to	O
get	O
dna	B
into	O
a	O
narrow	O
tube	B
to	O
an	O
information	B
theorist	O
the	O
entropy	B
associated	O
with	O
a	O
constrained	B
channel	I
reveals	O
how	O
much	O
information	B
can	O
be	O
conveyed	O
over	O
it	O
in	O
statistical	B
physics	B
the	O
same	O
calculations	O
are	O
done	O
for	O
a	O
reason	O
to	O
predict	O
the	O
thermodynamics	B
of	O
polymers	O
for	O
example	O
as	O
a	O
toy	O
example	O
consider	O
a	O
polymer	B
of	O
length	O
n	O
that	O
can	O
either	O
sit	O
in	O
a	O
constraining	O
tube	B
of	O
width	O
l	O
or	O
in	O
the	O
open	O
where	O
there	O
are	O
no	O
constraints	O
in	O
the	O
open	O
the	O
polymer	B
adopts	O
a	O
state	O
drawn	O
at	O
random	B
from	O
the	O
set	B
of	O
one	O
dimensional	O
random	B
walks	O
with	O
say	O
possible	O
directions	O
per	O
step	O
the	O
entropy	B
of	O
this	O
walk	O
is	O
log	O
per	O
step	O
i	O
e	O
a	O
free	O
energy	B
of	O
the	O
polymer	B
is	O
to	O
be	O
total	O
of	O
n	O
log	O
times	O
this	O
where	O
t	O
is	O
the	O
temperature	B
in	O
the	O
tube	B
the	O
polymer	B
s	O
onedimensional	O
walk	O
can	O
go	O
in	O
directions	O
unless	O
the	O
wall	O
is	O
in	O
the	O
way	O
so	O
the	O
connection	B
matrix	B
is	O
for	O
example	O
l	O
now	O
what	O
is	O
the	O
entropy	B
of	O
the	O
polymer	B
what	O
is	O
the	O
change	O
in	O
entropy	B
associated	O
with	O
the	O
polymer	B
entering	O
the	O
tube	B
if	O
possible	O
obtain	O
an	O
expression	O
as	O
a	O
function	O
of	O
l	O
use	O
a	O
computer	B
to	O
the	O
entropy	B
of	O
the	O
walk	O
for	O
a	O
particular	O
value	O
of	O
l	O
e	O
g	O
and	O
plot	O
the	O
probability	B
density	B
of	O
the	O
polymer	B
s	O
transverse	O
location	O
in	O
the	O
tube	B
notice	O
the	O
in	O
capacity	B
between	O
two	O
channels	O
one	O
constrained	B
and	O
one	O
unconstrained	O
is	O
directly	O
proportional	O
to	O
the	O
force	O
required	O
to	O
pull	O
the	O
dna	B
into	O
the	O
tube	B
solutions	O
solution	O
to	O
exercise	O
a	O
transmitted	O
by	O
contains	O
on	O
average	O
one-third	O
and	O
two-thirds	O
if	O
f	O
the	O
fraction	O
of	O
is	O
f	O
f	O
solution	O
to	O
exercise	O
a	O
valid	O
string	O
for	O
channel	O
c	O
can	O
be	O
obtained	O
from	O
a	O
valid	O
string	O
for	O
channel	O
a	O
by	O
inverting	O
it	O
then	O
passing	O
it	O
through	O
an	O
accumulator	B
these	O
operations	O
are	O
invertible	O
so	O
any	O
valid	O
string	O
for	O
c	O
can	O
also	O
be	O
mapped	O
onto	O
a	O
valid	O
string	O
for	O
a	O
the	O
only	O
proviso	O
here	O
comes	O
from	O
the	O
edge	B
if	O
we	O
assume	O
that	O
the	O
character	O
transmitted	O
over	O
channel	O
c	O
is	O
preceded	O
by	O
a	O
string	O
of	O
zeroes	O
so	O
that	O
the	O
character	O
is	O
forced	O
to	O
be	O
a	O
then	O
the	O
two	O
channels	O
are	O
exactly	O
equivalent	O
only	O
if	O
we	O
assume	O
that	O
channel	O
a	O
s	O
character	O
must	O
be	O
a	O
zero	O
solution	O
to	O
exercise	O
with	O
n	O
transmitted	O
bits	O
the	O
largest	O
integer	O
number	O
of	O
source	O
bits	O
that	O
can	O
be	O
encoded	O
is	O
so	O
the	O
maximum	O
rate	B
of	O
a	O
length	O
code	O
with	O
n	O
is	O
figure	O
model	B
of	O
dna	B
squashed	O
in	O
a	O
narrow	O
tube	B
the	O
dna	B
will	O
have	O
a	O
tendency	O
to	O
pop	O
out	O
of	O
the	O
tube	B
because	O
outside	O
the	O
tube	B
its	O
random	B
walk	I
has	O
greater	O
entropy	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
communication	B
over	O
constrained	B
noiseless	B
channels	O
solution	O
to	O
exercise	O
let	O
the	O
invariant	B
distribution	B
be	O
p	O
s	O
er	O
s	O
where	O
is	O
a	O
normalization	O
constant	O
the	O
entropy	B
of	O
st	O
given	O
assuming	O
comes	O
from	O
the	O
invariant	B
distribution	B
is	O
hlog	O
el	O
el	O
er	O
s	O
p	O
log	O
p	O
s	O
er	O
s	O
el	O
s	O
log	O
el	O
s	O
s	O
i	O
log	O
log	O
log	O
el	O
now	O
is	O
either	O
or	O
so	O
the	O
contributions	O
from	O
the	O
terms	O
proportional	O
to	O
log	O
are	O
all	O
zero	O
so	O
here	O
as	O
in	O
chapter	O
st	O
denotes	O
the	O
ensemble	B
whose	O
random	B
variable	I
is	O
the	O
state	O
st	O
log	O
log	O
el	O
s	O
el	O
xs	O
er	O
el	O
s	O
log	O
el	O
s	O
el	O
log	O
log	O
log	O
el	O
s	O
er	O
s	O
log	O
el	O
s	O
solution	O
to	O
exercise	O
the	O
principal	O
eigenvalues	O
of	O
the	O
connection	O
matrices	B
of	O
the	O
two	O
channels	O
are	O
and	O
the	O
capacities	O
are	O
and	O
bits	O
solution	O
to	O
exercise	O
the	O
channel	O
is	O
similar	O
to	O
the	O
unconstrained	O
binary	O
channel	O
runs	O
of	O
length	O
greater	O
than	O
l	O
are	O
rare	O
if	O
l	O
is	O
large	O
so	O
we	O
only	O
expect	O
weak	O
from	O
this	O
channel	O
these	O
will	O
show	O
up	O
in	O
contexts	O
where	O
the	O
run	O
length	O
is	O
close	O
to	O
l	O
the	O
capacity	B
of	O
the	O
channel	O
is	O
very	O
close	O
to	O
one	O
bit	B
a	O
lower	O
bound	B
on	O
the	O
capacity	B
is	O
obtained	O
by	O
considering	O
the	O
simple	O
variable-length	B
code	I
for	O
this	O
channel	O
which	O
replaces	O
occurrences	O
of	O
the	O
maximum	O
runlength	B
string	O
by	O
and	O
otherwise	O
leaves	O
the	O
source	O
unchanged	O
the	O
average	O
rate	B
of	O
this	O
code	O
is	O
because	O
the	O
invariant	B
distribution	B
will	O
hit	O
the	O
add	O
an	O
extra	O
zero	O
state	O
a	O
fraction	O
of	O
the	O
time	O
we	O
can	O
reuse	O
the	O
solution	O
for	O
the	O
variable-length	B
channel	O
in	O
exercise	O
the	O
capacity	B
is	O
the	O
value	O
of	O
such	O
that	O
the	O
equation	O
is	O
the	O
terms	O
in	O
the	O
sum	O
correspond	O
to	O
the	O
possible	O
strings	O
that	O
can	O
be	O
emitted	O
the	O
sum	O
is	O
exactly	O
given	O
by	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
l	O
true	O
capacity	B
n	O
arn	O
we	O
used	O
we	O
anticipate	O
that	O
should	O
be	O
a	O
little	O
less	O
than	O
in	O
order	O
for	O
to	O
equal	O
rearranging	O
and	O
solving	O
approximately	O
for	O
using	O
x	O
x	O
arn	O
r	O
ln	O
we	O
evaluated	O
the	O
true	O
capacities	O
for	O
l	O
and	O
l	O
in	O
an	O
earlier	O
exercise	O
the	O
table	O
compares	O
the	O
approximate	O
capacity	B
with	O
the	O
true	O
capacity	B
for	O
a	O
selection	O
of	O
values	O
of	O
l	O
the	O
element	O
will	O
be	O
close	O
to	O
a	O
tiny	O
bit	B
larger	O
since	O
in	O
the	O
unconstrained	O
binary	O
channel	O
when	O
a	O
run	O
of	O
length	O
l	O
has	O
occurred	O
we	O
have	O
a	O
choice	O
of	O
printing	O
or	O
let	O
the	O
probability	B
of	O
selecting	O
be	O
f	O
let	O
us	O
estimate	O
the	O
entropy	B
of	O
the	O
remaining	O
n	O
characters	O
in	O
the	O
stream	O
as	O
a	O
function	O
of	O
f	O
assuming	O
the	O
rest	O
of	O
the	O
matrix	B
q	O
to	O
have	O
been	O
set	B
to	O
its	O
optimal	B
value	O
the	O
entropy	B
of	O
the	O
next	O
n	O
characters	O
in	O
the	O
stream	O
is	O
the	O
entropy	B
of	O
the	O
bit	B
plus	O
the	O
entropy	B
of	O
the	O
remaining	O
characters	O
which	O
is	O
roughly	O
bits	O
if	O
we	O
select	O
as	O
the	O
bit	B
and	O
bits	O
if	O
is	O
selected	O
more	O
precisely	O
if	O
c	O
is	O
the	O
capacity	B
of	O
the	O
channel	O
is	O
roughly	O
hthe	O
next	O
n	O
chars	O
f	O
c	O
n	O
c	O
f	O
c	O
n	O
f	O
and	O
setting	O
to	O
zero	O
to	O
the	O
optimal	B
f	O
we	O
obtain	O
f	O
f	O
f	O
f	O
f	O
the	O
probability	B
of	O
emitting	O
a	O
thus	O
decreases	O
from	O
about	O
to	O
about	O
as	O
the	O
number	O
of	O
emitted	O
increases	O
here	O
is	O
the	O
optimal	B
matrix	B
our	O
rough	O
theory	O
works	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
crosswords	O
and	O
codebreaking	O
in	O
this	O
chapter	O
we	O
make	O
a	O
random	B
walk	I
through	O
a	O
few	O
topics	O
related	O
to	O
language	O
modelling	B
crosswords	O
the	O
rules	B
of	O
crossword-making	O
may	O
be	O
thought	O
of	O
as	O
a	O
constrained	B
channel	I
the	O
fact	O
that	O
many	O
valid	O
crosswords	O
can	O
be	O
made	O
demonstrates	O
that	O
this	O
constrained	B
channel	I
has	O
a	O
capacity	B
greater	O
than	O
zero	O
there	O
are	O
two	O
archetypal	O
crossword	B
formats	O
in	O
a	O
type	O
a	O
american	B
crossword	B
every	O
row	O
and	O
column	O
consists	O
of	O
a	O
succession	O
of	O
words	O
of	O
length	O
or	O
more	O
separated	O
by	O
one	O
or	O
more	O
spaces	O
in	O
a	O
type	O
b	O
british	B
crossword	B
each	O
row	O
and	O
column	O
consists	O
of	O
a	O
mixture	O
of	O
words	O
and	O
single	O
characters	O
separated	O
by	O
one	O
or	O
more	O
spaces	O
and	O
every	O
character	O
lies	O
in	O
at	O
least	O
one	O
word	O
or	O
vertical	O
whereas	O
in	O
a	O
type	O
a	O
crossword	B
every	O
letter	O
lies	O
in	O
a	O
horizontal	O
word	O
and	O
a	O
vertical	O
word	O
in	O
a	O
typical	B
type	O
b	O
crossword	B
only	O
about	O
half	O
of	O
the	O
letters	O
do	O
so	O
the	O
other	O
half	O
lie	O
in	O
one	O
word	O
only	O
type	O
a	O
crosswords	O
are	O
harder	O
to	O
create	O
than	O
type	O
b	O
because	O
of	O
the	O
constraint	O
that	O
no	O
single	O
characters	O
are	O
permitted	O
type	O
b	O
crosswords	O
are	O
generally	O
harder	O
to	O
solve	O
because	O
there	O
are	O
fewer	O
constraints	O
per	O
character	O
why	O
are	O
crosswords	O
possible	O
if	O
a	O
language	O
has	O
no	O
redundancy	B
then	O
any	O
letters	O
written	O
on	O
a	O
grid	O
form	O
a	O
valid	O
crossword	B
in	O
a	O
language	O
with	O
high	O
redundancy	B
on	O
the	O
other	O
hand	O
it	O
is	O
hard	O
to	O
make	O
crosswords	O
perhaps	O
a	O
small	O
number	O
of	O
trivial	O
ones	O
the	O
possibility	O
of	O
making	O
crosswords	O
in	O
a	O
language	O
thus	O
demonstrates	O
a	O
bound	B
on	O
the	O
redundancy	B
of	O
that	O
language	O
crosswords	O
are	O
not	O
normally	O
written	O
in	O
genuine	O
english	B
they	O
are	O
written	O
in	O
word-english	B
the	O
language	O
consisting	O
of	O
strings	O
of	O
words	O
from	O
a	O
dictionary	B
separated	O
by	O
spaces	O
d	O
u	O
f	O
f	O
s	O
t	O
u	O
d	O
g	O
i	O
l	O
d	O
s	O
b	O
p	O
v	O
j	O
d	O
p	O
b	O
a	O
f	O
a	O
r	O
t	O
i	O
t	O
o	O
a	O
d	O
i	O
e	O
u	O
a	O
v	O
a	O
l	O
a	O
n	O
c	O
h	O
e	O
u	O
s	O
h	O
e	O
r	O
t	O
o	O
t	O
o	O
o	O
l	O
a	O
v	O
r	O
i	O
d	O
e	O
r	O
n	O
r	O
l	O
a	O
n	O
e	O
i	O
i	O
a	O
s	O
h	O
m	O
o	O
t	O
h	O
e	O
r	O
g	O
o	O
o	O
s	O
e	O
g	O
a	O
l	O
l	O
e	O
o	O
n	O
n	O
e	O
t	O
t	O
l	O
e	O
s	O
e	O
v	O
i	O
l	O
s	O
c	O
u	O
l	O
t	O
e	O
i	O
n	O
o	O
i	O
w	O
t	O
s	O
t	O
r	O
e	O
s	O
s	O
s	O
o	O
l	O
e	O
b	O
a	O
s	O
r	O
o	O
a	O
s	O
t	O
b	O
e	O
e	O
f	O
n	O
o	O
b	O
e	O
l	O
c	O
i	O
t	O
e	O
s	O
u	O
t	O
t	O
e	O
r	O
r	O
o	O
t	O
m	O
i	O
e	O
u	O
a	O
e	O
h	O
e	O
i	O
r	O
s	O
n	O
e	O
e	O
r	O
c	O
o	O
r	O
e	O
b	O
r	O
e	O
m	O
n	O
e	O
r	O
r	O
o	O
t	O
a	O
t	O
e	O
s	O
m	O
u	O
m	O
a	O
t	O
l	O
a	O
s	O
m	O
a	O
t	O
t	O
e	O
a	O
n	O
e	O
h	O
c	O
t	O
o	O
p	O
e	O
p	O
a	O
u	O
l	O
m	O
i	O
s	O
h	O
a	O
p	O
k	O
i	O
t	O
e	O
s	O
a	O
u	O
s	O
t	O
r	O
a	O
l	O
i	O
a	O
e	O
p	O
i	O
c	O
c	O
a	O
r	O
t	O
e	O
e	O
l	O
p	O
t	O
a	O
e	O
u	O
s	O
i	O
s	O
t	O
e	O
r	O
k	O
e	O
n	O
n	O
y	O
r	O
a	O
h	O
r	O
o	O
c	O
k	O
e	O
t	O
s	O
e	O
x	O
c	O
u	O
s	O
e	O
s	O
a	O
l	O
o	O
h	O
a	O
i	O
r	O
o	O
n	O
t	O
r	O
e	O
e	O
i	O
a	O
t	O
o	O
p	O
k	O
t	O
t	O
s	O
i	O
r	O
e	O
s	O
l	O
a	O
t	O
e	O
e	O
a	O
r	O
l	O
e	O
l	O
t	O
o	O
n	O
d	O
e	O
s	O
p	O
e	O
r	O
a	O
t	O
e	O
s	O
a	O
b	O
r	O
e	O
y	O
s	O
e	O
r	O
a	O
t	O
o	O
m	O
s	O
s	O
a	O
y	O
r	O
r	O
n	O
exercise	O
estimate	O
the	O
capacity	B
of	O
word-english	B
in	O
bits	O
per	O
character	O
think	O
of	O
word-english	B
as	O
a	O
constrained	B
channel	I
and	O
see	O
exercise	O
figure	O
crosswords	O
of	O
types	O
a	O
and	O
b	O
the	O
fact	O
that	O
many	O
crosswords	O
can	O
be	O
made	O
leads	O
to	O
a	O
lower	O
bound	B
on	O
the	O
entropy	B
of	O
word-english	B
for	O
simplicity	O
we	O
now	O
model	B
word-english	B
by	O
wenglish	B
the	O
language	O
introduced	O
in	O
section	B
which	O
consists	O
of	O
w	O
words	O
all	O
of	O
length	O
l	O
the	O
entropy	B
of	O
such	O
a	O
language	O
per	O
character	O
including	O
inter-word	O
spaces	O
is	O
hw	O
w	O
l	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
crosswords	O
we	O
ll	O
that	O
the	O
conclusions	O
we	O
come	O
to	O
depend	O
on	O
the	O
value	O
of	O
hw	O
and	O
are	O
not	O
terribly	O
sensitive	O
to	O
the	O
value	O
of	O
l	O
consider	O
a	O
large	O
crossword	B
of	O
size	O
s	O
squares	O
in	O
area	O
let	O
the	O
number	O
of	O
words	O
be	O
fws	O
and	O
let	O
the	O
number	O
of	O
letter-occupied	O
squares	O
be	O
for	O
typical	B
crosswords	O
of	O
types	O
a	O
and	O
b	O
made	O
of	O
words	O
of	O
length	O
l	O
the	O
two	O
fractions	O
fw	O
and	O
have	O
roughly	O
the	O
values	O
in	O
table	O
we	O
now	O
estimate	O
how	O
many	O
crosswords	O
there	O
are	O
of	O
size	O
s	O
using	O
our	O
simple	O
model	B
of	O
wenglish	B
we	O
assume	O
that	O
wenglish	B
is	O
created	O
at	O
random	B
by	O
generating	O
w	O
strings	O
from	O
a	O
monogram	O
memoryless	O
source	O
with	O
entropy	B
if	O
for	O
example	O
the	O
source	O
used	O
all	O
a	O
characters	O
with	O
equal	O
probability	B
then	O
a	O
bits	O
if	O
instead	O
we	O
use	O
chapter	O
s	O
distribution	B
then	O
the	O
entropy	B
is	O
the	O
redundancy	B
of	O
wenglish	B
stems	O
from	O
two	O
sources	O
it	O
tends	O
to	O
use	O
some	O
letters	O
more	O
than	O
others	O
and	O
there	O
are	O
only	O
w	O
words	O
in	O
the	O
dictionary	B
let	O
s	O
now	O
count	O
how	O
many	O
crosswords	O
there	O
are	O
by	O
imagining	O
in	O
the	O
squares	O
of	O
a	O
crossword	B
at	O
random	B
using	O
the	O
same	O
distribution	B
that	O
produced	O
the	O
wenglish	B
dictionary	B
and	O
evaluating	O
the	O
probability	B
that	O
this	O
random	B
scribbling	O
produces	O
valid	O
words	O
in	O
all	O
rows	O
and	O
columns	O
the	O
total	O
number	O
of	O
typical	B
of	O
the	O
squares	O
in	O
the	O
crossword	B
that	O
can	O
be	O
made	O
is	O
the	O
probability	B
that	O
one	O
word	O
of	O
length	O
l	O
is	O
validly	O
is	O
jtj	O
w	O
and	O
the	O
probability	B
that	O
the	O
whole	O
crossword	B
made	O
of	O
fws	O
words	O
is	O
validly	O
by	O
a	O
single	O
typical	B
is	O
approximately	O
so	O
the	O
log	O
of	O
the	O
number	O
of	O
valid	O
crosswords	O
of	O
size	O
s	O
is	O
estimated	O
to	O
be	O
log	O
s	O
fw	O
log	O
w	O
s	O
fwl	O
which	O
is	O
an	O
increasing	O
function	O
of	O
s	O
only	O
if	O
fwl	O
so	O
arbitrarily	O
many	O
crosswords	O
can	O
be	O
made	O
only	O
if	O
there	O
s	O
enough	O
words	O
in	O
the	O
wenglish	B
dictionary	B
that	O
hw	O
fwl	O
plugging	O
in	O
the	O
values	O
of	O
and	O
fw	O
from	O
table	O
we	O
the	O
following	O
a	O
b	O
l	O
l	O
l	O
l	O
l	O
l	O
fw	O
table	O
factors	O
fw	O
and	O
by	O
which	O
the	O
number	O
of	O
words	O
and	O
number	O
of	O
letter-squares	O
respectively	O
are	O
smaller	O
than	O
the	O
total	O
number	O
of	O
squares	O
this	O
calculation	O
underestimates	O
the	O
number	O
of	O
valid	O
wenglish	B
crosswords	O
by	O
counting	B
only	O
crosswords	O
with	O
typical	B
strings	O
if	O
the	O
monogram	O
distribution	B
is	O
non-uniform	O
then	O
the	O
true	O
count	O
is	O
dominated	O
by	O
atypical	O
in	O
which	O
crossword-friendly	O
words	O
appear	O
more	O
often	O
crossword	B
type	O
a	O
condition	O
for	O
crosswords	O
hw	O
b	O
hw	O
l	O
l	O
if	O
we	O
set	B
bits	O
and	O
assume	O
there	O
are	O
w	O
words	O
in	O
a	O
normal	B
english-speaker	O
s	O
dictionary	B
all	O
with	O
length	O
l	O
then	O
we	O
that	O
the	O
condition	O
for	O
crosswords	O
of	O
type	O
b	O
is	O
but	O
the	O
condition	O
for	O
crosswords	O
of	O
type	O
a	O
is	O
only	O
just	O
this	O
with	O
my	O
experience	O
that	O
crosswords	O
of	O
type	O
a	O
usually	O
contain	O
more	O
obscure	O
words	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
reading	O
crosswords	O
and	O
codebreaking	O
figure	O
a	O
binary	O
pattern	O
in	O
which	O
every	O
pixel	O
is	O
adjacent	O
to	O
four	O
black	B
and	O
four	O
white	B
pixels	O
these	O
observations	O
about	O
crosswords	O
were	O
made	O
by	O
shannon	B
i	O
learned	O
about	O
them	O
from	O
wolf	B
and	O
siegel	B
the	O
topic	O
is	O
closely	O
related	O
to	O
the	O
capacity	B
of	O
two-dimensional	B
constrained	B
channels	O
an	O
example	O
of	O
a	O
two-dimensional	B
constrained	B
channel	I
is	O
a	O
two-dimensional	B
bar-code	B
as	O
seen	O
on	O
parcels	O
exercise	O
a	O
two-dimensional	B
channel	O
is	O
by	O
the	O
constraint	O
that	O
of	O
the	O
eight	O
neighbours	O
of	O
every	O
interior	O
pixel	O
in	O
an	O
n	O
n	O
rectangular	B
grid	O
four	O
must	O
be	O
black	B
and	O
four	O
white	B
counts	O
of	O
black	B
and	O
white	B
pixels	O
around	O
boundary	O
pixels	O
are	O
not	O
constrained	B
a	O
binary	O
pattern	O
satisfying	O
this	O
constraint	O
is	O
shown	O
in	O
what	O
is	O
the	O
capacity	B
of	O
this	O
channel	O
in	O
bits	O
per	O
pixel	O
for	O
large	O
n	O
simple	O
language	O
models	O
the	O
zipfmandelbrot	O
distribution	B
the	O
crudest	O
model	B
for	O
a	O
language	O
is	O
the	O
monogram	O
model	B
which	O
asserts	O
that	O
each	O
successive	O
word	O
is	O
drawn	O
independently	O
from	O
a	O
distribution	B
over	O
words	O
what	O
is	O
the	O
nature	O
of	O
this	O
distribution	B
over	O
words	O
zipf	B
s	O
law	O
asserts	O
that	O
the	O
probability	B
of	O
the	O
rth	O
most	O
probable	O
word	O
in	O
a	O
language	O
is	O
approximately	O
p	O
where	O
the	O
exponent	O
has	O
a	O
value	O
close	O
to	O
and	O
is	O
a	O
constant	O
according	O
to	O
zipf	B
a	O
loglog	O
plot	O
of	O
frequency	B
versus	O
word-rank	O
should	O
show	O
a	O
straight	O
line	O
with	O
slope	O
eter	O
v	O
asserting	O
that	O
the	O
probabilities	O
are	O
given	O
by	O
mandelbrot	B
s	O
of	O
zipf	B
s	O
law	O
introduces	O
a	O
third	O
param	O
p	O
for	O
some	O
documents	O
such	O
as	O
jane	O
austen	O
s	O
emma	O
the	O
zipfmandelbrot	O
distribution	B
well	O
other	O
documents	O
give	O
distributions	O
that	O
are	O
not	O
so	O
well	O
by	O
a	O
zipf	B
mandelbrot	B
distribution	B
figure	O
shows	O
a	O
plot	O
of	O
frequency	B
versus	O
rank	O
for	O
the	O
latex	O
source	O
of	O
this	O
book	O
qualitatively	O
the	O
graph	B
is	O
similar	O
to	O
a	O
straight	O
line	O
but	O
a	O
curve	O
is	O
noticeable	O
to	O
be	O
fair	O
this	O
source	O
is	O
not	O
written	O
in	O
pure	O
english	B
it	O
is	O
a	O
mix	O
of	O
english	B
maths	O
symbols	O
such	O
as	O
x	O
and	O
latex	O
commands	O
to	O
theand	O
of	O
i	O
is	O
harriet	O
information	B
probability	B
figure	O
fit	O
of	O
the	O
zipfmandelbrot	O
distribution	B
to	O
the	O
empirical	O
frequencies	O
of	O
words	O
in	O
jane	O
austen	O
s	O
emma	O
the	O
parameters	B
are	O
v	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
simple	O
language	O
models	O
the	O
of	O
a	O
is	O
x	O
probability	B
information	B
shannon	B
bayes	B
book	O
figure	O
loglog	O
plot	O
of	O
frequency	B
versus	O
rank	O
for	O
the	O
words	O
in	O
the	O
latex	O
of	O
this	O
book	O
figure	O
zipf	B
plots	O
for	O
four	O
languages	O
randomly	O
generated	O
from	O
dirichlet	B
processes	O
with	O
parameter	O
ranging	O
from	O
to	O
also	O
shown	O
is	O
the	O
zipf	B
plot	I
for	O
this	O
book	O
the	O
dirichlet	B
process	O
assuming	O
we	O
are	O
interested	O
in	O
monogram	O
models	O
for	O
languages	O
what	O
model	B
should	O
we	O
use	O
one	O
in	O
modelling	B
a	O
language	O
is	O
the	O
unboundedness	O
of	O
vocabulary	O
the	O
greater	O
the	O
sample	B
of	O
language	O
the	O
greater	O
the	O
number	O
of	O
words	O
encountered	O
a	O
generative	B
model	B
for	O
a	O
language	O
should	O
emulate	O
this	O
property	O
if	O
asked	O
what	O
is	O
the	O
next	O
word	O
in	O
a	O
newly-discovered	O
work	O
of	O
shakespeare	O
our	O
probability	B
distribution	B
over	O
words	O
must	O
surely	O
include	O
some	O
non-zero	O
probability	B
for	O
words	O
that	O
shakespeare	O
never	O
used	O
before	O
our	O
generative	O
monogram	O
model	B
for	O
language	O
should	O
also	O
satisfy	O
a	O
consistency	O
rule	O
called	O
exchangeability	B
if	O
we	O
imagine	O
generating	O
a	O
new	O
language	O
from	O
our	O
generative	B
model	B
producing	O
an	O
ever-growing	O
corpus	O
of	O
text	O
all	O
statistical	B
properties	O
of	O
the	O
text	O
should	O
be	O
homogeneous	B
the	O
probability	B
of	O
a	O
particular	O
word	O
at	O
a	O
given	O
location	O
in	O
the	O
stream	O
of	O
text	O
should	O
be	O
the	O
same	O
everywhere	O
in	O
the	O
stream	O
the	O
dirichlet	B
process	O
model	B
is	O
a	O
model	B
for	O
a	O
stream	O
of	O
symbols	O
we	O
think	O
of	O
as	O
words	O
that	O
the	O
exchangeability	B
rule	O
and	O
that	O
allows	O
the	O
vocabulary	O
of	O
symbols	O
to	O
grow	O
without	O
limit	O
the	O
model	B
has	O
one	O
parameter	O
as	O
the	O
stream	O
of	O
symbols	O
is	O
produced	O
we	O
identify	O
each	O
new	O
symbol	O
by	O
a	O
unique	O
integer	O
w	O
when	O
we	O
have	O
seen	O
a	O
stream	O
of	O
length	O
f	O
symbols	O
we	O
the	O
probability	B
of	O
the	O
next	O
symbol	O
in	O
terms	O
of	O
the	O
counts	O
ffwg	O
of	O
the	O
symbols	O
seen	O
so	O
far	O
thus	O
the	O
probability	B
that	O
the	O
next	O
symbol	O
is	O
a	O
new	O
symbol	O
never	O
seen	O
before	O
is	O
f	O
the	O
probability	B
that	O
the	O
next	O
symbol	O
is	O
symbol	O
w	O
is	O
fw	O
f	O
figure	O
shows	O
zipf	B
plots	O
plots	O
of	O
symbol	O
frequency	B
versus	O
rank	O
for	O
million-symbol	O
documents	O
generated	O
by	O
dirichlet	B
process	O
priors	O
with	O
values	O
of	O
ranging	O
from	O
to	O
it	O
is	O
evident	O
that	O
a	O
dirichlet	B
process	O
is	O
not	O
an	O
adequate	O
model	B
for	O
observed	O
distributions	O
that	O
roughly	O
obey	O
zipf	B
s	O
law	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
crosswords	O
and	O
codebreaking	O
figure	O
zipf	B
plots	O
for	O
the	O
words	O
of	O
two	O
languages	O
generated	O
by	O
creating	O
successive	O
characters	O
from	O
a	O
dirichlet	B
process	O
with	O
and	O
declaring	O
one	O
character	O
to	O
be	O
the	O
space	O
character	O
the	O
two	O
curves	O
result	O
from	O
two	O
choices	O
of	O
the	O
space	O
character	O
with	O
a	O
small	O
tweak	O
however	O
dirichlet	B
processes	O
can	O
produce	O
rather	O
nice	O
zipf	B
plots	O
imagine	O
generating	O
a	O
language	O
composed	O
of	O
elementary	O
symbols	O
using	O
a	O
dirichlet	B
process	O
with	O
a	O
rather	O
small	O
value	O
of	O
the	O
parameter	O
so	O
that	O
the	O
number	O
of	O
reasonably	O
frequent	O
symbols	O
is	O
about	O
if	O
we	O
then	O
declare	O
one	O
of	O
those	O
symbols	O
called	O
characters	O
rather	O
than	O
words	O
to	O
be	O
a	O
space	O
character	O
then	O
we	O
can	O
identify	O
the	O
strings	O
between	O
the	O
space	O
characters	O
as	O
words	O
if	O
we	O
generate	O
a	O
language	O
in	O
this	O
way	O
then	O
the	O
frequencies	O
of	O
words	O
often	O
come	O
out	O
as	O
very	O
nice	O
zipf	B
plots	O
as	O
shown	O
in	O
which	O
character	O
is	O
selected	O
as	O
the	O
space	O
character	O
determines	O
the	O
slope	O
of	O
the	O
zipf	B
plot	I
a	O
less	O
probable	O
space	O
character	O
gives	O
rise	O
to	O
a	O
richer	O
language	O
with	O
a	O
shallower	O
slope	O
units	B
of	O
information	B
content	I
the	O
information	B
content	I
of	O
an	O
outcome	O
x	O
whose	O
probability	B
is	O
p	O
is	O
to	O
be	O
hx	O
log	O
p	O
the	O
entropy	B
of	O
an	O
ensemble	B
is	O
an	O
average	O
information	B
content	I
hx	O
p	O
log	O
p	O
when	O
we	O
compare	O
hypotheses	O
with	O
each	O
other	O
in	O
the	O
light	O
of	O
data	O
it	O
is	O
often	O
convenient	O
to	O
compare	O
the	O
log	O
of	O
the	O
probability	B
of	O
the	O
data	O
under	O
the	O
alternative	O
hypotheses	O
log	O
evidence	B
for	O
hi	O
log	O
p	O
jhi	O
or	O
in	O
the	O
case	O
where	O
just	O
two	O
hypotheses	O
are	O
being	O
compared	O
we	O
evaluate	O
the	O
log	O
odds	B
log	O
p	O
p	O
which	O
has	O
also	O
been	O
called	O
the	O
weight	O
of	O
evidence	B
in	O
favour	O
of	O
the	O
log	O
evidence	B
for	O
a	O
hypothesis	O
log	O
p	O
jhi	O
is	O
the	O
negative	O
of	O
the	O
information	B
content	I
of	O
the	O
data	O
d	O
if	O
the	O
data	O
have	O
large	O
information	B
content	I
given	O
a	O
hypothesis	O
then	O
they	O
are	O
surprising	O
to	O
that	O
hypothesis	O
if	O
some	O
other	O
hypothesis	O
is	O
not	O
so	O
surprised	O
by	O
the	O
data	O
then	O
that	O
hypothesis	O
becomes	O
more	O
probable	O
information	B
content	I
surprise	B
value	I
and	O
log	O
likelihood	B
or	O
log	O
evidence	B
are	O
the	O
same	O
thing	O
all	O
these	O
quantities	O
are	O
logarithms	B
of	O
probabilities	O
or	O
weighted	O
sums	O
of	O
logarithms	B
of	O
probabilities	O
so	O
they	O
can	O
all	O
be	O
measured	O
in	O
the	O
same	O
units	B
the	O
units	B
depend	O
on	O
the	O
choice	O
of	O
the	O
base	O
of	O
the	O
logarithm	O
the	O
names	O
that	O
have	O
been	O
given	O
to	O
these	O
units	B
are	O
shown	O
in	O
table	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
taste	O
of	O
banburismus	B
unit	O
bit	B
nat	O
ban	O
deciban	O
expression	O
that	O
has	O
those	O
units	B
table	O
units	B
of	O
measurement	O
of	O
information	B
content	I
p	O
loge	O
p	O
p	O
p	O
the	O
bit	B
is	O
the	O
unit	O
that	O
we	O
use	O
most	O
in	O
this	O
book	O
because	O
the	O
word	O
bit	B
has	O
other	O
meanings	O
a	O
backup	O
name	O
for	O
this	O
unit	O
is	O
the	O
shannon	B
a	O
byte	B
is	O
bits	O
a	O
megabyte	O
is	O
bytes	O
if	O
one	O
works	O
in	O
natural	B
logarithms	B
information	B
contents	O
and	O
weights	O
of	O
evidence	B
are	O
measured	O
in	O
nats	O
the	O
most	O
interesting	O
units	B
are	O
the	O
ban	O
and	O
the	O
deciban	O
the	O
history	O
of	O
the	O
ban	O
let	O
me	O
tell	O
you	O
why	O
a	O
factor	O
of	O
ten	O
in	O
probability	B
is	O
called	O
a	O
ban	O
when	O
alan	O
turing	B
and	O
the	O
other	O
codebreakers	B
at	O
bletchley	B
park	I
were	O
breaking	O
each	O
new	O
day	O
s	O
enigma	B
code	O
their	O
task	O
was	O
a	O
huge	O
inference	B
problem	O
to	O
infer	O
given	O
the	O
day	O
s	O
cyphertext	O
which	O
three	O
wheels	O
were	O
in	O
the	O
enigma	B
machines	O
that	O
day	O
what	O
their	O
starting	O
positions	O
were	O
what	O
further	O
letter	O
substitutions	O
were	O
in	O
use	O
on	O
the	O
steckerboard	O
and	O
not	O
least	O
what	O
the	O
original	O
german	O
messages	O
were	O
these	O
inferences	O
were	O
conducted	O
using	O
bayesian	B
methods	O
course	O
and	O
the	O
chosen	O
units	B
were	O
decibans	O
or	O
half-decibans	O
the	O
deciban	O
being	O
judged	O
the	O
smallest	O
weight	O
of	O
evidence	B
discernible	O
to	O
a	O
human	B
the	O
evidence	B
in	O
favour	O
of	O
particular	O
hypotheses	O
was	O
tallied	O
using	O
sheets	O
of	O
paper	O
that	O
were	O
specially	O
printed	O
in	O
banbury	O
a	O
town	O
about	O
miles	O
from	O
bletchley	O
the	O
inference	B
task	O
was	O
known	O
as	O
banburismus	B
and	O
the	O
units	B
in	O
which	O
banburismus	B
was	O
played	O
were	O
called	O
bans	O
after	O
that	O
town	O
a	O
taste	O
of	O
banburismus	B
the	O
details	O
of	O
the	O
code-breaking	O
methods	O
of	O
bletchley	B
park	I
were	O
kept	O
secret	B
for	O
a	O
long	O
time	O
but	O
some	O
aspects	O
of	O
banburismus	B
can	O
be	O
pieced	O
together	O
i	O
hope	O
the	O
following	O
description	O
of	O
a	O
small	O
part	O
of	O
banburismus	B
is	O
not	O
too	O
how	O
much	O
information	B
was	O
needed	O
the	O
number	O
of	O
possible	O
settings	O
of	O
the	O
enigma	B
machine	O
was	O
about	O
to	O
deduce	O
the	O
state	O
of	O
the	O
machine	O
it	O
was	O
therefore	O
necessary	O
to	O
about	O
decibans	O
from	O
somewhere	O
as	O
good	B
puts	O
it	O
banburismus	B
was	O
aimed	O
not	O
at	O
deducing	O
the	O
entire	O
state	O
of	O
the	O
machine	O
but	O
only	O
at	O
out	O
which	O
wheels	O
were	O
in	O
use	O
the	O
logic-based	O
bombes	B
fed	O
with	O
guesses	O
of	O
the	O
plaintext	B
were	O
then	O
used	O
to	O
crack	O
what	O
the	O
settings	O
of	O
the	O
wheels	O
were	O
the	O
enigma	B
machine	O
once	O
its	O
wheels	O
and	O
plugs	O
were	O
put	O
in	O
place	O
implemented	O
a	O
continually-changing	O
permutation	B
cypher	O
that	O
wandered	O
deterministically	O
through	O
a	O
state	O
space	O
of	O
permutations	O
because	O
an	O
enormous	O
number	O
of	O
messages	O
were	O
sent	O
each	O
day	O
there	O
was	O
a	O
good	B
chance	O
that	O
whatever	O
state	O
one	O
machine	O
was	O
in	O
when	O
sending	O
one	O
character	O
of	O
a	O
message	O
there	O
would	O
be	O
another	O
machine	O
in	O
the	O
same	O
state	O
while	O
sending	O
a	O
particular	O
character	O
in	O
another	O
message	O
because	O
the	O
evolution	B
of	O
the	O
machine	O
s	O
state	O
was	O
deterministic	B
the	O
two	O
machines	O
would	O
remain	O
in	O
the	O
same	O
state	O
as	O
each	O
other	O
ve	O
been	O
most	O
helped	O
by	O
descriptions	O
codesandciphers	O
org	O
uklectures	O
and	O
by	O
jack	O
good	B
who	O
worked	O
with	O
turing	B
at	O
bletchley	O
given	O
by	O
tony	O
sale	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
crosswords	O
and	O
codebreaking	O
for	O
the	O
rest	O
of	O
the	O
transmission	O
the	O
resulting	O
correlations	B
between	O
the	O
outputs	O
of	O
such	O
pairs	O
of	O
machines	O
provided	O
a	O
dribble	O
of	O
information-content	O
from	O
which	O
turing	B
and	O
his	O
co-workers	O
extracted	O
their	O
daily	O
decibans	O
how	O
to	O
detect	O
that	O
two	O
messages	O
came	O
from	O
machines	O
with	O
a	O
common	O
state	O
sequence	B
the	O
hypotheses	O
are	O
the	O
null	O
hypothesis	O
which	O
states	O
that	O
the	O
machines	O
are	O
in	O
states	O
and	O
that	O
the	O
two	O
plain	O
messages	O
are	O
unrelated	O
and	O
the	O
match	O
hypothesis	O
which	O
says	O
that	O
the	O
machines	O
are	O
in	O
the	O
same	O
state	O
and	O
that	O
the	O
two	O
plain	O
messages	O
are	O
unrelated	O
no	O
attempt	O
is	O
being	O
made	O
here	O
to	O
infer	O
what	O
the	O
state	O
of	O
either	O
machine	O
is	O
the	O
data	O
provided	O
are	O
the	O
two	O
cyphertexts	O
x	O
and	O
y	O
let	O
s	O
assume	O
they	O
both	O
have	O
length	O
t	O
and	O
that	O
the	O
alphabet	O
size	O
is	O
a	O
in	O
enigma	B
what	O
is	O
the	O
probability	B
of	O
the	O
data	O
given	O
the	O
two	O
hypotheses	O
first	O
the	O
null	O
hypothesis	O
this	O
hypothesis	O
asserts	O
that	O
the	O
two	O
cyphertexts	O
are	O
given	O
by	O
and	O
x	O
y	O
where	O
the	O
codes	O
ct	O
and	O
are	O
two	O
unrelated	O
time-varying	O
permutations	O
of	O
the	O
alphabet	O
and	O
and	O
are	O
the	O
plaintext	B
messages	O
an	O
exact	O
computation	O
of	O
the	O
probability	B
of	O
the	O
data	O
y	O
would	O
depend	O
on	O
a	O
language	B
model	B
of	O
the	O
plain	O
text	O
and	O
a	O
model	B
of	O
the	O
enigma	B
machine	O
s	O
guts	O
but	O
if	O
we	O
assume	O
that	O
each	O
enigma	B
machine	O
is	O
an	O
ideal	O
random	B
time-varying	O
permutation	B
then	O
the	O
probability	B
distribution	B
of	O
the	O
two	O
cyphertexts	O
is	O
uniform	O
all	O
cyphertexts	O
are	O
equally	O
likely	O
p	O
y	O
for	O
all	O
x	O
y	O
of	O
length	O
t	O
what	O
about	O
this	O
hypothesis	O
asserts	O
that	O
a	O
single	O
time-varying	O
permutation	B
ct	O
underlies	O
both	O
x	O
and	O
y	O
what	O
is	O
the	O
probability	B
of	O
the	O
data	O
y	O
we	O
have	O
to	O
make	O
some	O
assumptions	B
about	O
the	O
plaintext	B
language	O
if	O
it	O
were	O
the	O
case	O
that	O
the	O
plaintext	B
language	O
was	O
completely	O
random	B
then	O
the	O
probability	B
of	O
and	O
would	O
be	O
uniform	O
and	O
so	O
would	O
that	O
of	O
x	O
and	O
y	O
so	O
the	O
probability	B
p	O
y	O
would	O
be	O
equal	O
to	O
p	O
y	O
and	O
the	O
two	O
hypotheses	O
and	O
would	O
be	O
indistinguishable	O
we	O
make	O
progress	O
by	O
assuming	O
that	O
the	O
plaintext	B
is	O
not	O
completely	O
random	B
both	O
plaintexts	O
are	O
written	O
in	O
a	O
language	O
and	O
that	O
language	O
has	O
redundancies	O
assume	O
for	O
example	O
that	O
particular	O
plaintext	B
letters	O
are	O
used	O
more	O
often	O
than	O
others	O
so	O
even	O
though	O
the	O
two	O
plaintext	B
messages	O
are	O
unrelated	O
they	O
are	O
slightly	O
more	O
likely	O
to	O
use	O
the	O
same	O
letters	O
as	O
each	O
other	O
if	O
is	O
true	O
two	O
synchronized	O
letters	O
from	O
the	O
two	O
cyphertexts	O
are	O
slightly	O
more	O
likely	O
to	O
be	O
identical	O
similarly	O
if	O
a	O
language	O
uses	O
particular	O
bigrams	O
and	O
trigrams	O
frequently	O
then	O
the	O
two	O
plaintext	B
messages	O
will	O
occasionally	O
contain	O
the	O
same	O
bigrams	O
and	O
trigrams	O
at	O
the	O
same	O
time	O
as	O
each	O
other	O
giving	O
rise	O
if	O
is	O
true	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
taste	O
of	O
banburismus	B
u	O
little-jack-horner-sat-in-the-corner-eating-a-christmas-pie--he-put-in-h	O
v	O
ride-a-cock-horse-to-banbury-cross-to-see-a-fine-lady-upon-a-white-horse	O
matches	O
table	O
two	O
aligned	O
pieces	O
of	O
english	B
plaintext	B
u	O
and	O
v	O
with	O
matches	O
marked	O
by	O
notice	O
that	O
there	O
are	O
twelve	O
matches	O
including	O
a	O
run	O
of	O
six	B
whereas	O
the	O
expected	O
number	O
of	O
matches	O
in	O
two	O
completely	O
random	B
strings	O
of	O
length	O
t	O
would	O
be	O
about	O
the	O
two	O
corresponding	O
cyphertexts	O
from	O
two	O
machines	O
in	O
identical	O
states	O
would	O
also	O
have	O
twelve	O
matches	O
to	O
a	O
little	O
burst	O
of	O
or	O
identical	O
letters	O
table	O
shows	O
such	O
a	O
coincidence	B
in	O
two	O
plaintext	B
messages	O
that	O
are	O
unrelated	O
except	O
that	O
they	O
are	O
both	O
written	O
in	O
english	B
the	O
codebreakers	B
hunted	O
among	O
pairs	O
of	O
messages	O
for	O
pairs	O
that	O
were	O
suspiciously	O
similar	O
to	O
each	O
other	O
counting	B
up	O
the	O
numbers	O
of	O
matching	O
monograms	O
bigrams	O
trigrams	O
etc	O
this	O
method	O
was	O
used	O
by	O
the	O
polish	O
codebreaker	O
rejewski	O
let	O
s	O
look	O
at	O
the	O
simple	O
case	O
of	O
a	O
monogram	O
language	B
model	B
and	O
estimate	O
how	O
long	O
a	O
message	O
is	O
needed	O
to	O
be	O
able	O
to	O
decide	O
whether	O
two	O
machines	O
are	O
in	O
the	O
same	O
state	O
i	O
ll	O
assume	O
the	O
source	O
language	O
is	O
monogram-english	O
the	O
language	O
in	O
which	O
successive	O
letters	O
are	O
drawn	O
i	O
i	O
d	O
from	O
the	O
probability	B
distribution	B
fpig	O
of	O
the	O
probability	B
of	O
x	O
and	O
y	O
is	O
nonuniform	O
consider	O
two	O
single	O
characters	O
xt	O
ctut	O
and	O
yt	O
ctvt	O
the	O
probability	B
that	O
they	O
are	O
identical	O
is	O
xutvt	O
p	O
vt	O
xi	O
i	O
m	O
we	O
give	O
this	O
quantity	O
the	O
name	O
m	O
for	O
match	O
probability	B
for	O
both	O
english	B
and	O
german	O
m	O
is	O
about	O
rather	O
than	O
value	O
that	O
would	O
hold	O
for	O
a	O
completely	O
random	B
language	O
assuming	O
that	O
ct	O
is	O
an	O
ideal	O
random	B
permutation	B
the	O
probability	B
of	O
xt	O
and	O
yt	O
is	O
by	O
symmetry	O
p	O
yt	O
m	O
a	O
if	O
xt	O
yt	O
for	O
xt	O
yt	O
given	O
a	O
pair	O
of	O
cyphertexts	O
x	O
and	O
y	O
of	O
length	O
t	O
that	O
match	O
in	O
m	O
places	O
and	O
do	O
not	O
match	O
in	O
n	O
places	O
the	O
log	O
evidence	B
in	O
favour	O
of	O
is	O
then	O
log	O
p	O
y	O
p	O
y	O
m	O
log	O
ma	O
n	O
log	O
m	O
log	O
ma	O
n	O
log	O
ma	O
a	O
every	O
match	O
contributes	O
log	O
ma	O
in	O
favour	O
of	O
every	O
non-match	O
contributes	O
log	O
db	O
ma	O
db	O
in	O
favour	O
of	O
match	O
probability	B
for	O
monogram-english	O
coincidental	O
match	O
probability	B
log-evidence	O
for	O
per	O
match	O
log-evidence	O
for	O
per	O
non-match	O
if	O
there	O
were	O
m	O
matches	O
and	O
n	O
non-matches	O
in	O
a	O
pair	O
of	O
length	O
t	O
for	O
example	O
the	O
weight	O
of	O
evidence	B
in	O
favour	O
of	O
would	O
be	O
decibans	O
or	O
a	O
likelihood	B
ratio	O
of	O
to	O
in	O
favour	O
the	O
expected	O
weight	O
of	O
evidence	B
from	O
a	O
line	O
of	O
text	O
of	O
length	O
t	O
characters	O
is	O
the	O
expectation	B
of	O
which	O
depends	O
on	O
whether	O
or	O
is	O
true	O
if	O
is	O
true	O
then	O
matches	O
are	O
expected	O
to	O
turn	O
up	O
at	O
rate	B
m	O
and	O
the	O
expected	O
weight	O
of	O
evidence	B
is	O
decibans	O
per	O
characters	O
if	O
is	O
true	O
m	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
crosswords	O
and	O
codebreaking	O
then	O
spurious	O
matches	O
are	O
expected	O
to	O
turn	O
up	O
at	O
rate	B
and	O
the	O
expected	O
weight	O
of	O
evidence	B
is	O
decibans	O
per	O
characters	O
typically	O
roughly	O
characters	O
need	O
to	O
be	O
inspected	O
in	O
order	O
to	O
have	O
a	O
weight	O
of	O
evidence	B
greater	O
than	O
a	O
hundred	O
to	O
one	O
decibans	O
in	O
favour	O
of	O
one	O
hypothesis	O
or	O
the	O
other	O
so	O
two	O
english	B
plaintexts	O
have	O
more	O
matches	O
than	O
two	O
random	B
strings	O
furthermore	O
because	O
consecutive	O
characters	O
in	O
english	B
are	O
not	O
independent	O
the	O
bigram	O
and	O
trigram	O
statistics	O
of	O
english	B
are	O
nonuniform	O
and	O
the	O
matches	O
tend	O
to	O
occur	O
in	O
bursts	O
of	O
consecutive	O
matches	O
same	O
observations	O
also	O
apply	O
to	O
german	O
using	O
better	O
language	O
models	O
the	O
evidence	B
contributed	O
by	O
runs	O
of	O
matches	O
was	O
more	O
accurately	O
computed	O
such	O
a	O
scoring	O
system	O
was	O
worked	O
out	O
by	O
turing	B
and	O
by	O
good	B
positive	O
results	O
were	O
passed	O
on	O
to	O
automated	O
and	O
human-powered	O
codebreakers	B
according	O
to	O
good	B
the	O
longest	O
false-positive	O
that	O
arose	O
in	O
this	O
work	O
was	O
a	O
string	O
of	O
consecutive	O
matches	O
between	O
two	O
machines	O
that	O
were	O
actually	O
in	O
unrelated	O
states	O
further	O
reading	O
for	O
further	O
reading	O
about	O
turing	B
and	O
bletchley	B
park	I
see	O
hodges	O
and	O
good	B
for	O
an	O
in-depth	O
read	O
about	O
cryptography	B
schneier	O
s	O
book	O
is	O
highly	O
recommended	O
it	O
is	O
readable	O
clear	O
and	O
entertaining	O
exercises	O
exercise	O
another	O
weakness	O
in	O
the	O
design	O
of	O
the	O
enigma	B
machine	O
which	O
was	O
intended	O
to	O
emulate	O
a	O
perfectly	O
random	B
time-varying	O
permutation	B
is	O
that	O
it	O
never	O
mapped	O
a	O
letter	O
to	O
itself	O
when	O
you	O
press	O
q	O
what	O
comes	O
out	O
is	O
always	O
a	O
letter	O
from	O
q	O
how	O
much	O
information	B
per	O
character	O
is	O
leaked	O
by	O
this	O
design	O
how	O
long	O
a	O
crib	B
would	O
be	O
needed	O
to	O
be	O
that	O
the	O
crib	B
is	O
correctly	O
aligned	O
with	O
the	O
cyphertext	O
and	O
how	O
long	O
a	O
crib	B
would	O
be	O
needed	O
to	O
be	O
able	O
to	O
identify	O
the	O
correct	O
key	O
crib	B
is	O
a	O
guess	O
for	O
what	O
the	O
plaintext	B
was	O
imagine	O
that	O
the	O
brits	O
know	O
that	O
a	O
very	O
important	O
german	O
is	O
travelling	O
from	O
berlin	O
to	O
aachen	O
and	O
they	O
intercept	O
enigma-encoded	O
messages	O
sent	O
to	O
aachen	O
it	O
is	O
a	O
good	B
bet	B
that	O
one	O
or	O
more	O
of	O
the	O
original	O
plaintext	B
messages	O
contains	O
the	O
string	O
obersturmbannfuehrerxgrafxheinrichxvonxweizsaecker	O
the	O
name	O
of	O
the	O
important	O
chap	O
a	O
crib	B
could	O
be	O
used	O
in	O
a	O
brute-force	O
approach	O
to	O
the	O
correct	O
enigma	B
key	O
the	O
received	O
messages	O
through	O
all	O
possible	O
engima	O
machines	O
and	O
see	O
if	O
any	O
of	O
the	O
putative	O
decoded	O
texts	O
match	O
the	O
above	O
plaintext	B
this	O
question	O
centres	O
on	O
the	O
idea	O
that	O
the	O
crib	B
can	O
also	O
be	O
used	O
in	O
a	O
much	O
less	O
expensive	O
manner	O
slide	O
the	O
plaintext	B
crib	B
along	O
all	O
the	O
encoded	O
messages	O
until	O
a	O
perfect	B
mismatch	O
of	O
the	O
crib	B
and	O
the	O
encoded	O
message	O
is	O
found	O
if	O
correct	O
this	O
alignment	O
then	O
tells	O
you	O
a	O
lot	O
about	O
the	O
key	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
why	O
have	O
sex	O
information	B
acquisition	O
and	O
evolution	B
evolution	B
has	O
been	O
happening	O
on	O
earth	O
for	O
about	O
the	O
last	O
years	O
undeniably	O
information	B
has	O
been	O
acquired	O
during	O
this	O
process	O
thanks	O
to	O
the	O
tireless	O
work	O
of	O
the	O
blind	B
watchmaker	I
some	O
cells	O
now	O
carry	O
within	O
them	O
all	O
the	O
information	B
required	O
to	O
be	O
outstanding	O
spiders	O
other	O
cells	O
carry	O
all	O
the	O
information	B
required	O
to	O
make	O
excellent	O
octopuses	O
where	O
did	O
this	O
information	B
come	O
from	O
the	O
entire	O
blueprint	O
of	O
all	O
organisms	O
on	O
the	O
planet	O
has	O
emerged	O
in	O
a	O
teaching	O
process	O
in	O
which	O
the	O
teacher	O
is	O
natural	B
selection	I
individuals	O
have	O
more	O
progeny	O
the	O
being	O
by	O
the	O
local	O
environment	O
the	O
other	O
organisms	O
the	O
teaching	O
signal	O
is	O
only	O
a	O
few	O
bits	O
per	O
individual	O
an	O
individual	O
simply	O
has	O
a	O
smaller	O
or	O
larger	O
number	O
of	O
grandchildren	O
depending	O
on	O
the	O
individual	O
s	O
fitness	O
is	O
a	O
broad	O
term	O
that	O
could	O
cover	B
the	O
ability	O
of	O
an	O
antelope	O
to	O
run	O
faster	O
than	O
other	O
antelopes	O
and	O
hence	O
avoid	O
being	O
eaten	O
by	O
a	O
lion	O
the	O
ability	O
of	O
a	O
lion	O
to	O
be	O
well-enough	O
and	O
run	O
fast	O
enough	O
to	O
catch	O
one	O
antelope	O
per	O
day	O
the	O
ability	O
of	O
a	O
peacock	O
to	O
attract	O
a	O
peahen	O
to	O
mate	O
with	O
it	O
the	O
ability	O
of	O
a	O
peahen	O
to	O
rear	O
many	O
young	O
simultaneously	O
the	O
of	O
an	O
organism	O
is	O
largely	O
determined	O
by	O
its	O
dna	B
both	O
the	O
coding	O
regions	O
or	O
genes	B
and	O
the	O
non-coding	O
regions	O
play	O
an	O
important	O
role	O
in	O
regulating	O
the	O
transcription	O
of	O
genes	B
we	O
ll	O
think	O
of	O
as	O
a	O
function	O
of	O
the	O
dna	B
sequence	B
and	O
the	O
environment	O
how	O
does	O
the	O
dna	B
determine	O
and	O
how	O
does	O
information	B
get	O
from	O
natural	B
selection	I
into	O
the	O
genome	B
well	O
if	O
the	O
gene	O
that	O
codes	O
for	O
one	O
of	O
an	O
antelope	O
s	O
proteins	O
is	O
defective	O
that	O
antelope	O
might	O
get	O
eaten	O
by	O
a	O
lion	O
early	O
in	O
life	B
and	O
have	O
only	O
two	O
grandchildren	O
rather	O
than	O
forty	O
the	O
information	B
content	I
of	O
natural	B
selection	I
is	O
fully	O
contained	O
in	O
a	O
of	O
which	O
survived	O
to	O
have	O
children	O
an	O
information	B
content	I
of	O
at	O
most	O
one	O
bit	B
per	O
the	O
teaching	O
signal	O
does	O
not	O
communicate	O
to	O
the	O
ecosystem	O
any	O
description	O
of	O
the	O
imperfections	O
in	O
the	O
organism	O
that	O
caused	O
it	O
to	O
have	O
fewer	O
children	O
the	O
bits	O
of	O
the	O
teaching	O
signal	O
are	O
highly	O
redundant	O
because	O
throughout	O
a	O
species	B
individuals	O
who	O
are	O
similar	O
to	O
each	O
other	O
will	O
be	O
failing	O
to	O
have	O
for	O
similar	O
reasons	O
so	O
how	O
many	O
bits	O
per	O
generation	O
are	O
acquired	O
by	O
the	O
species	B
as	O
a	O
whole	O
by	O
natural	B
selection	I
how	O
many	O
bits	O
has	O
natural	B
selection	I
succeeded	O
in	O
conveying	O
to	O
the	O
human	B
branch	O
of	O
the	O
tree	B
of	O
life	B
since	O
the	O
divergence	B
between	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
why	O
have	O
sex	O
information	B
acquisition	O
and	O
evolution	B
australopithecines	O
and	O
apes	O
years	O
ago	O
assuming	O
a	O
generation	O
time	O
of	O
years	O
for	O
reproduction	O
there	O
have	O
been	O
about	O
generations	O
of	O
human	B
precursors	O
since	O
the	O
divergence	B
from	O
apes	O
assuming	O
a	O
population	O
of	O
individuals	O
each	O
receiving	O
a	O
couple	O
of	O
bits	O
of	O
information	B
from	O
natural	B
selection	I
the	O
total	O
number	O
of	O
bits	O
of	O
information	B
responsible	O
for	O
modifying	O
the	O
genomes	O
of	O
million	O
b	O
c	O
into	O
today	O
s	O
human	B
genome	B
is	O
about	O
bits	O
however	O
as	O
we	O
noted	O
natural	B
selection	I
is	O
not	O
smart	O
at	O
collating	O
the	O
information	B
that	O
it	O
dishes	O
out	O
to	O
the	O
population	O
and	O
there	O
is	O
a	O
great	O
deal	O
of	O
redundancy	B
in	O
that	O
information	B
if	O
the	O
population	O
size	O
were	O
twice	O
as	O
great	O
would	O
it	O
evolve	O
twice	O
as	O
fast	O
no	O
because	O
natural	B
selection	I
will	O
simply	O
be	O
correcting	O
the	O
same	O
defects	O
twice	O
as	O
often	O
john	O
maynard	O
smith	O
has	O
suggested	O
that	O
the	O
rate	B
of	O
information	B
acquisition	O
by	O
a	O
species	B
is	O
independent	O
of	O
the	O
population	O
size	O
and	O
is	O
of	O
order	O
bit	B
per	O
generation	O
this	O
would	O
allow	O
for	O
only	O
bits	O
of	O
between	O
apes	O
and	O
humans	O
a	O
number	O
that	O
is	O
much	O
smaller	O
than	O
the	O
total	O
size	O
of	O
the	O
human	B
genome	B
bits	O
human	B
genome	B
contains	O
about	O
nucleotides	O
it	O
is	O
certainly	O
the	O
case	O
that	O
the	O
genomic	O
overlap	O
between	O
apes	O
and	O
humans	O
is	O
huge	O
but	O
is	O
the	O
that	O
small	O
in	O
this	O
chapter	O
we	O
ll	O
develop	O
a	O
crude	O
model	B
of	O
the	O
process	O
of	O
information	B
acquisition	O
through	O
evolution	B
based	O
on	O
the	O
assumption	O
that	O
a	O
gene	O
with	O
two	O
defects	O
is	O
typically	O
likely	O
to	O
be	O
more	O
defective	O
than	O
a	O
gene	O
with	O
one	O
defect	O
and	O
an	O
organism	O
with	O
two	O
defective	O
genes	B
is	O
likely	O
to	O
be	O
less	O
than	O
an	O
organism	O
with	O
one	O
defective	O
gene	O
undeniably	O
this	O
is	O
a	O
crude	O
model	B
since	O
real	O
biological	O
systems	O
are	O
baroque	O
constructions	O
with	O
complex	B
interactions	O
nevertheless	O
we	O
persist	O
with	O
a	O
simple	O
model	B
because	O
it	O
readily	O
yields	O
striking	O
results	O
what	O
we	O
from	O
this	O
simple	O
model	B
is	O
that	O
john	O
maynard	O
smith	O
s	O
of	O
bit	B
per	O
generation	O
is	O
correct	O
for	O
an	O
asexually-reproducing	O
population	O
in	O
contrast	O
if	O
the	O
species	B
reproduces	O
sexually	O
the	O
rate	B
of	O
information	B
acquisition	O
can	O
be	O
as	O
large	O
as	O
pg	O
bits	O
per	O
generation	O
where	O
g	O
is	O
the	O
size	O
of	O
the	O
genome	B
we	O
ll	O
also	O
interesting	O
results	O
concerning	O
the	O
maximum	O
mutation	B
rate	B
that	O
a	O
species	B
can	O
withstand	O
the	O
model	B
we	O
study	O
a	O
simple	O
model	B
of	O
a	O
reproducing	O
population	O
of	O
n	O
individuals	O
with	O
a	O
genome	B
of	O
size	O
g	O
bits	O
variation	O
is	O
produced	O
by	O
mutation	O
or	O
by	O
recombination	O
sex	O
and	O
truncation	O
selection	O
selects	O
the	O
n	O
children	O
at	O
each	O
generation	O
to	O
be	O
the	O
parents	O
of	O
the	O
next	O
we	O
striking	O
between	O
populations	O
that	O
have	O
recombination	O
and	O
populations	O
that	O
do	O
not	O
the	O
genotype	O
of	O
each	O
individual	O
is	O
a	O
vector	O
x	O
of	O
g	O
bits	O
each	O
having	O
a	O
good	B
state	O
xg	O
and	O
a	O
bad	B
state	O
xg	O
the	O
f	O
of	O
an	O
individual	O
is	O
simply	O
the	O
sum	O
of	O
her	O
bits	O
the	O
bits	O
in	O
the	O
genome	B
could	O
be	O
considered	O
to	O
correspond	O
either	O
to	O
genes	B
that	O
have	O
good	B
alleles	O
and	O
bad	B
alleles	O
or	O
to	O
the	O
nucleotides	O
of	O
a	O
genome	B
we	O
will	O
concentrate	O
on	O
the	O
latter	O
interpretation	O
the	O
essential	O
property	O
of	O
that	O
we	O
are	O
assuming	O
is	O
that	O
it	O
is	O
locally	O
a	O
roughly	O
linear	B
function	O
of	O
the	O
genome	B
that	O
is	O
that	O
there	O
are	O
many	O
possible	O
changes	O
one	O
f	O
xg	O
g	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
rate	B
of	O
increase	O
of	O
could	O
make	O
to	O
the	O
genome	B
each	O
of	O
which	O
has	O
a	O
small	O
on	O
and	O
that	O
these	O
combine	O
approximately	O
linearly	O
we	O
the	O
normalized	O
f	O
f	O
we	O
consider	O
evolution	B
by	O
natural	B
selection	I
under	O
two	O
models	O
of	O
variation	O
variation	O
by	O
mutation	O
the	O
model	B
assumes	O
discrete	O
generations	O
at	O
each	O
generation	O
t	O
every	O
individual	O
produces	O
two	O
children	O
the	O
children	O
s	O
genotypes	O
from	O
the	O
parent	B
s	O
by	O
random	B
mutations	O
natural	B
selection	I
selects	O
the	O
n	O
progeny	O
in	O
the	O
child	O
population	O
to	O
reproduce	O
and	O
a	O
new	O
generation	O
starts	O
selection	O
of	O
the	O
n	O
individuals	O
at	O
each	O
generation	O
is	O
known	O
as	O
truncation	O
selection	O
the	O
simplest	O
model	B
of	O
mutations	O
is	O
that	O
the	O
child	O
s	O
bits	O
fxgg	O
are	O
independent	O
each	O
bit	B
has	O
a	O
small	O
probability	B
of	O
being	O
which	O
thinking	O
of	O
the	O
bits	O
as	O
corresponding	O
roughly	O
to	O
nucleotides	O
is	O
taken	O
to	O
be	O
a	O
constant	O
m	O
independent	O
of	O
xg	O
alternatively	O
we	O
thought	O
of	O
the	O
bits	O
as	O
corresponding	O
to	O
genes	B
then	O
we	O
would	O
model	B
the	O
probability	B
of	O
the	O
discovery	O
of	O
a	O
good	B
gene	O
p	O
xg	O
as	O
being	O
a	O
smaller	O
number	O
than	O
the	O
probability	B
of	O
a	O
deleterious	O
mutation	O
in	O
a	O
good	B
gene	O
p	O
xg	O
variation	O
by	O
recombination	O
crossover	B
or	O
sex	O
our	O
organisms	O
are	O
haploid	O
not	O
diploid	O
they	O
enjoy	O
sex	O
by	O
recombination	O
the	O
n	O
individuals	O
in	O
the	O
population	O
are	O
married	O
into	O
m	O
couples	O
at	O
random	B
and	O
each	O
couple	O
has	O
c	O
children	O
with	O
c	O
children	O
being	O
our	O
standard	O
assumption	O
so	O
as	O
to	O
have	O
the	O
population	O
double	O
and	O
halve	O
every	O
generation	O
as	O
before	O
the	O
c	O
children	O
s	O
genotypes	O
are	O
independent	O
given	O
the	O
parents	O
each	O
child	O
obtains	O
its	O
genotype	O
z	O
by	O
random	B
crossover	B
of	O
its	O
parents	O
genotypes	O
x	O
and	O
y	O
the	O
simplest	O
model	B
of	O
recombination	O
has	O
no	O
linkage	O
so	O
that	O
zg	O
xg	O
with	O
probability	B
yg	O
with	O
probability	B
once	O
the	O
m	O
c	O
progeny	O
have	O
been	O
born	O
the	O
parents	O
pass	O
away	O
the	O
n	O
progeny	O
are	O
selected	O
by	O
natural	B
selection	I
and	O
a	O
new	O
generation	O
starts	O
we	O
now	O
study	O
these	O
two	O
models	O
of	O
variation	O
in	O
detail	O
rate	B
of	O
increase	O
of	O
theory	O
of	O
mutations	O
we	O
assume	O
that	O
the	O
genotype	O
of	O
an	O
individual	O
with	O
normalized	O
f	O
fg	O
is	O
subjected	O
to	O
mutations	O
that	O
bits	O
with	O
probability	B
m	O
we	O
show	O
that	O
if	O
the	O
average	O
normalized	O
f	O
of	O
the	O
population	O
is	O
greater	O
than	O
then	O
the	O
optimal	B
mutation	B
rate	B
is	O
small	O
and	O
the	O
rate	B
of	O
acquisition	O
of	O
information	B
is	O
at	O
most	O
of	O
order	O
one	O
bit	B
per	O
generation	O
since	O
it	O
is	O
easy	O
to	O
achieve	O
a	O
normalized	O
of	O
f	O
by	O
simple	O
mutation	O
we	O
ll	O
assume	O
f	O
and	O
work	O
in	O
terms	O
of	O
the	O
excess	O
normalized	O
f	O
if	O
an	O
individual	O
with	O
excess	O
normalized	O
has	O
a	O
child	O
and	O
the	O
mutation	B
rate	B
m	O
is	O
small	O
the	O
probability	B
distribution	B
of	O
the	O
excess	O
normalized	O
of	O
the	O
child	O
has	O
mean	B
child	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
and	O
variance	B
why	O
have	O
sex	O
information	B
acquisition	O
and	O
evolution	B
m	O
g	O
m	O
g	O
if	O
the	O
population	O
of	O
parents	O
has	O
mean	B
and	O
variance	B
then	O
the	O
child	O
population	O
before	O
selection	O
will	O
have	O
mean	B
and	O
variance	B
natural	B
selection	I
chooses	O
the	O
upper	O
half	O
of	O
this	O
distribution	B
so	O
the	O
mean	B
and	O
variance	B
of	O
at	O
the	O
next	O
generation	O
are	O
given	O
by	O
m	O
g	O
m	O
g	O
where	O
is	O
the	O
mean	B
deviation	O
from	O
the	O
mean	B
measured	O
in	O
standard	O
deviations	O
and	O
is	O
the	O
factor	O
by	O
which	O
the	O
child	O
distribution	B
s	O
variance	B
is	O
reduced	O
by	O
selection	O
the	O
numbers	O
and	O
are	O
of	O
order	O
for	O
the	O
case	O
of	O
a	O
gaussian	B
distribution	B
and	O
if	O
we	O
assume	O
that	O
the	O
variance	B
is	O
in	O
dynamic	O
equilibrium	O
i	O
e	O
then	O
so	O
and	O
the	O
factor	O
in	O
equation	O
is	O
equal	O
to	O
if	O
we	O
take	O
the	O
results	O
for	O
the	O
gaussian	B
distribution	B
an	O
approximation	B
that	O
becomes	O
poorest	O
when	O
the	O
discreteness	O
of	O
becomes	O
important	O
i	O
e	O
for	O
small	O
m	O
the	O
rate	B
of	O
increase	O
of	O
normalized	O
is	O
thus	O
dt	O
m	O
which	O
assuming	O
is	O
maximized	O
for	O
df	O
g	O
at	O
which	O
point	O
mopt	O
df	O
so	O
the	O
rate	B
of	O
increase	O
of	O
f	O
f	O
g	O
is	O
at	O
most	O
df	O
dt	O
per	O
generation	O
for	O
a	O
population	O
with	O
low	O
the	O
rate	B
of	O
increase	O
of	O
the	O
rate	B
of	O
increase	O
if	O
may	O
exceed	O
unit	O
per	O
generation	O
indeed	O
if	O
m	O
is	O
of	O
order	O
pg	O
this	O
initial	O
spurt	O
can	O
last	O
only	O
of	O
order	O
pg	O
generations	O
for	O
the	O
rate	B
of	O
increase	O
of	O
is	O
smaller	O
than	O
one	O
per	O
generation	O
as	O
the	O
approaches	O
g	O
the	O
optimal	B
mutation	B
rate	B
tends	O
to	O
m	O
so	O
that	O
an	O
average	O
of	O
bits	O
are	O
per	O
genotype	O
and	O
the	O
rate	B
of	O
increase	O
of	O
is	O
also	O
equal	O
to	O
information	B
is	O
gained	O
at	O
a	O
rate	B
of	O
about	O
bits	O
per	O
generation	O
it	O
takes	O
about	O
generations	O
for	O
the	O
genotypes	O
of	O
all	O
individuals	O
in	O
the	O
population	O
to	O
attain	O
perfection	O
for	O
m	O
the	O
is	O
given	O
by	O
c	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
rate	B
of	O
increase	O
of	O
no	O
sex	O
sex	O
histogram	O
of	O
parents	O
histogram	O
of	O
children	O
s	O
selected	O
children	O
s	O
figure	O
why	O
sex	O
is	O
better	O
than	O
sex-free	O
reproduction	O
if	O
mutations	O
are	O
used	O
to	O
create	O
variation	O
among	O
children	O
then	O
it	O
is	O
unavoidable	O
that	O
the	O
average	O
of	O
the	O
children	O
is	O
lower	O
than	O
the	O
parents	O
the	O
greater	O
the	O
variation	O
the	O
greater	O
the	O
average	O
selection	O
bumps	O
up	O
the	O
mean	B
again	O
in	O
contrast	O
recombination	O
produces	O
variation	O
without	O
a	O
decrease	O
in	O
average	O
the	O
typical	B
amount	O
of	O
variation	O
scales	O
as	O
pg	O
where	O
g	O
is	O
the	O
genome	B
size	O
so	O
after	O
selection	O
the	O
average	O
rises	O
by	O
opg	O
subject	O
to	O
the	O
constraint	O
where	O
c	O
is	O
a	O
constant	O
of	O
integration	O
equal	O
to	O
if	O
f	O
if	O
the	O
mean	B
number	O
of	O
bits	O
per	O
genotype	O
mg	O
exceeds	O
then	O
the	O
f	O
approaches	O
an	O
equilibrium	O
value	O
feqm	O
this	O
theory	O
is	O
somewhat	O
inaccurate	O
in	O
that	O
the	O
true	O
probability	B
distribution	B
of	O
is	O
non-gaussian	O
asymmetrical	O
and	O
quantized	O
to	O
integer	O
values	O
all	O
the	O
same	O
the	O
predictions	O
of	O
the	O
theory	O
are	O
not	O
grossly	O
at	O
variance	B
with	O
the	O
results	O
of	O
simulations	O
described	O
below	O
theory	O
of	O
sex	O
the	O
analysis	O
of	O
the	O
sexual	O
population	O
becomes	O
tractable	O
with	O
two	O
approximations	O
we	O
assume	O
that	O
the	O
gene-pool	O
mixes	O
rapidly	O
that	O
correlations	B
between	O
genes	B
can	O
be	O
neglected	O
second	O
we	O
assume	O
homogeneity	O
i	O
e	O
that	O
the	O
fraction	O
fg	O
of	O
bits	O
g	O
that	O
are	O
in	O
the	O
good	B
state	O
is	O
the	O
same	O
f	O
for	O
all	O
g	O
given	O
these	O
assumptions	B
if	O
two	O
parents	O
of	O
f	O
f	O
g	O
mate	O
the	O
probability	B
distribution	B
of	O
their	O
children	O
s	O
has	O
mean	B
equal	O
to	O
the	O
parents	O
f	O
the	O
variation	O
produced	O
by	O
sex	O
does	O
not	O
reduce	O
the	O
average	O
the	O
standard	B
deviation	I
of	O
the	O
of	O
the	O
children	O
scales	O
as	O
pgf	O
f	O
since	O
after	O
selection	O
the	O
increase	O
in	O
is	O
proportional	O
to	O
this	O
standard	B
deviation	I
the	O
increase	O
per	O
generation	O
scales	O
as	O
the	O
square	B
root	O
of	O
the	O
size	O
of	O
the	O
genome	B
pg	O
as	O
shown	O
in	O
box	B
the	O
mean	B
f	O
g	O
evolves	O
in	O
accordance	O
with	O
the	O
equation	O
d	O
dt	O
f	O
where	O
the	O
solution	O
of	O
this	O
equation	O
is	O
for	O
t	O
c	O
where	O
c	O
is	O
a	O
constant	O
of	O
integration	O
c	O
so	O
this	O
idealized	O
system	O
reaches	O
a	O
state	O
of	O
eugenic	O
perfection	O
within	O
a	O
time	O
generations	O
pg	O
f	O
simulations	O
figure	O
shows	O
the	O
of	O
a	O
sexual	O
population	O
of	O
n	O
individuals	O
with	O
a	O
genome	B
size	O
of	O
g	O
starting	O
from	O
a	O
random	B
initial	O
state	O
with	O
normalized	O
it	O
also	O
shows	O
the	O
theoretical	O
curve	O
f	O
from	O
equation	O
which	O
remarkably	O
well	O
in	O
contrast	O
and	O
show	O
the	O
evolving	O
when	O
variation	O
is	O
produced	O
by	O
mutation	O
at	O
rates	O
m	O
and	O
m	O
respectively	O
note	O
the	O
in	O
the	O
horizontal	O
scales	O
from	O
panel	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
why	O
have	O
sex	O
information	B
acquisition	O
and	O
evolution	B
how	O
does	O
f	O
depend	O
on	O
f	O
let	O
s	O
assume	O
the	O
two	O
parents	O
of	O
a	O
child	O
both	O
have	O
exactly	O
f	O
good	B
bits	O
and	O
by	O
our	O
homogeneity	O
assumption	O
that	O
those	O
bits	O
are	O
independent	O
random	B
subsets	O
of	O
the	O
g	O
bits	O
the	O
number	O
of	O
bits	O
that	O
are	O
good	B
in	O
both	O
parents	O
is	O
roughly	O
f	O
and	O
the	O
number	O
that	O
are	O
good	B
in	O
one	O
parent	B
only	O
is	O
roughly	O
so	O
the	O
of	O
the	O
child	O
will	O
be	O
f	O
plus	O
the	O
sum	O
of	O
fair	O
coin	B
which	O
has	O
a	O
binomial	B
distribution	B
of	O
mean	B
f	O
f	O
and	O
variance	B
f	O
f	O
the	O
of	O
a	O
child	O
is	O
thus	O
roughly	O
distributed	O
as	O
fchild	O
normal	B
mean	B
f	O
variance	B
f	O
f	O
box	B
details	O
of	O
the	O
theory	O
of	O
sex	O
the	O
important	O
property	O
of	O
this	O
distribution	B
contrasted	O
with	O
the	O
distribution	B
under	O
mutation	O
is	O
that	O
the	O
mean	B
is	O
equal	O
to	O
the	O
parents	O
the	O
variation	O
produced	O
by	O
sex	O
does	O
not	O
reduce	O
the	O
average	O
if	O
we	O
include	O
the	O
parental	O
population	O
s	O
variance	B
which	O
we	O
will	O
write	O
as	O
f	O
f	O
the	O
children	O
s	O
are	O
distributed	O
as	O
fchild	O
normal	B
mean	B
f	O
variance	B
f	O
f	O
natural	B
selection	I
selects	O
the	O
children	O
on	O
the	O
upper	O
side	O
of	O
this	O
distribution	B
the	O
mean	B
increase	O
in	O
will	O
be	O
and	O
the	O
variance	B
of	O
the	O
surviving	O
children	O
will	O
be	O
f	O
f	O
f	O
f	O
where	O
then	O
the	O
factor	O
in	O
is	O
and	O
if	O
there	O
is	O
dynamic	O
equilibrium	O
we	O
conclude	O
that	O
under	O
sex	O
and	O
natural	B
selection	I
the	O
mean	B
of	O
the	O
population	O
increases	O
at	O
a	O
rate	B
proportional	O
to	O
the	O
square	B
root	O
of	O
the	O
size	O
of	O
the	O
genome	B
this	O
constant	O
to	O
be	O
d	O
dt	O
f	O
f	O
bits	O
per	O
generation	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
maximal	O
tolerable	O
mutation	B
rate	B
figure	O
fitness	O
as	O
a	O
function	O
of	O
time	O
the	O
genome	B
size	O
is	O
g	O
the	O
dots	O
show	O
the	O
of	O
six	B
randomly	O
selected	O
individuals	O
from	O
the	O
birth	O
population	O
at	O
each	O
generation	O
the	O
initial	O
population	O
of	O
n	O
had	O
randomly	O
generated	O
genomes	O
with	O
f	O
variation	O
produced	O
by	O
sex	O
alone	O
line	O
shows	O
theoretical	O
curve	O
for	O
homogeneous	B
population	O
variation	O
produced	O
by	O
mutation	O
with	O
and	O
without	O
sex	O
when	O
the	O
mutation	B
rate	B
is	O
mg	O
or	O
bits	O
per	O
genome	B
the	O
dashed	O
line	O
shows	O
the	O
curve	O
figure	O
maximal	O
tolerable	O
mutation	B
rate	B
shown	O
as	O
number	O
of	O
errors	B
per	O
genome	B
versus	O
normalized	O
f	O
fg	O
left	O
panel	O
genome	B
size	O
g	O
right	O
g	O
independent	O
of	O
genome	B
size	O
a	O
parthenogenetic	O
species	B
sex	O
can	O
tolerate	O
only	O
of	O
order	O
error	O
per	O
genome	B
per	O
generation	O
a	O
species	B
that	O
uses	O
recombination	O
can	O
tolerate	O
far	O
greater	O
mutation	O
rates	O
sex	O
no	O
sex	O
sex	O
no	O
sex	O
g	O
g	O
mg	O
with	O
sex	O
without	O
sex	O
with	O
sex	O
without	O
sex	O
f	O
f	O
exercise	O
dependence	O
on	O
population	O
size	O
how	O
do	O
the	O
results	O
for	O
a	O
sexual	O
population	O
depend	O
on	O
the	O
population	O
size	O
we	O
anticipate	O
that	O
there	O
is	O
a	O
minimum	O
population	O
size	O
above	O
which	O
the	O
theory	O
of	O
sex	O
is	O
accurate	O
how	O
is	O
that	O
minimum	O
population	O
size	O
related	O
to	O
g	O
exercise	O
dependence	O
on	O
crossover	B
mechanism	O
in	O
the	O
simple	O
model	B
of	O
sex	O
each	O
bit	B
is	O
taken	O
at	O
random	B
from	O
one	O
of	O
the	O
two	O
parents	O
that	O
is	O
we	O
allow	O
crossovers	O
to	O
occur	O
with	O
probability	B
between	O
any	O
two	O
adjacent	O
nucleotides	O
how	O
is	O
the	O
model	B
if	O
the	O
crossover	B
probability	B
is	O
smaller	O
if	O
crossovers	O
occur	O
exclusively	O
at	O
hot-spots	O
located	O
every	O
d	O
bits	O
along	O
the	O
genome	B
the	O
maximal	O
tolerable	O
mutation	B
rate	B
what	O
if	O
we	O
combine	O
the	O
two	O
models	O
of	O
variation	O
what	O
is	O
the	O
maximum	O
mutation	B
rate	B
that	O
can	O
be	O
tolerated	O
by	O
a	O
species	B
that	O
has	O
sex	O
the	O
rate	B
of	O
increase	O
of	O
is	O
given	O
by	O
df	O
dt	O
m	O
f	O
f	O
g	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
why	O
have	O
sex	O
information	B
acquisition	O
and	O
evolution	B
which	O
is	O
positive	O
if	O
the	O
mutation	B
rate	B
m	O
f	O
f	O
g	O
let	O
us	O
compare	O
this	O
rate	B
with	O
the	O
result	O
in	O
the	O
absence	O
of	O
sex	O
which	O
from	O
equation	O
is	O
that	O
the	O
maximum	O
tolerable	O
mutation	B
rate	B
is	O
the	O
tolerable	O
mutation	B
rate	B
with	O
sex	O
is	O
of	O
order	O
pg	O
times	O
greater	O
than	O
that	O
without	O
sex	O
m	O
g	O
a	O
parthenogenetic	O
species	B
could	O
try	O
to	O
wriggle	O
out	O
of	O
this	O
bound	B
on	O
its	O
mutation	B
rate	B
by	O
increasing	O
its	O
litter	O
sizes	O
but	O
if	O
mutation	O
on	O
average	O
mg	O
bits	O
the	O
probability	B
that	O
no	O
bits	O
are	O
in	O
one	O
genome	B
is	O
roughly	O
so	O
a	O
mother	O
needs	O
to	O
have	O
roughly	O
emg	O
in	O
order	O
to	O
have	O
a	O
good	B
chance	O
of	O
having	O
one	O
child	O
with	O
the	O
same	O
as	O
her	O
the	O
litter	O
size	O
of	O
a	O
non-sexual	O
species	B
thus	O
has	O
to	O
be	O
exponential	B
in	O
mg	O
mg	O
is	O
bigger	O
than	O
if	O
the	O
species	B
is	O
to	O
persist	O
so	O
the	O
maximum	O
tolerable	O
mutation	B
rate	B
is	O
pinned	O
close	O
to	O
for	O
a	O
nonsexual	O
species	B
whereas	O
it	O
is	O
a	O
larger	O
number	O
of	O
order	O
for	O
a	O
species	B
with	O
recombination	O
turning	O
these	O
results	O
around	O
we	O
can	O
predict	O
the	O
largest	O
possible	O
genome	B
size	O
for	O
a	O
given	O
mutation	B
rate	B
m	O
for	O
a	O
parthenogenetic	O
species	B
the	O
largest	O
genome	B
size	O
is	O
of	O
order	O
and	O
for	O
a	O
sexual	O
species	B
taking	O
the	O
m	O
as	O
the	O
mutation	B
rate	B
per	O
nucleotide	B
per	O
generation	O
and	O
keightley	O
and	O
allowing	O
for	O
a	O
maximum	O
brood	O
size	O
of	O
is	O
mg	O
we	O
predict	O
that	O
all	O
species	B
with	O
more	O
than	O
g	O
coding	O
nucleotides	O
make	O
at	O
least	O
occasional	O
use	O
of	O
recombination	O
if	O
the	O
brood	O
size	O
is	O
then	O
this	O
number	O
falls	O
to	O
g	O
fitness	O
increase	O
and	O
information	B
acquisition	O
for	O
this	O
simple	O
model	B
it	O
is	O
possible	O
to	O
relate	O
increasing	O
to	O
information	B
acquisition	O
if	O
the	O
bits	O
are	O
set	B
at	O
random	B
the	O
is	O
roughly	O
f	O
if	O
evolution	B
leads	O
to	O
a	O
population	O
in	O
which	O
all	O
individuals	O
have	O
the	O
maximum	O
f	O
g	O
then	O
g	O
bits	O
of	O
information	B
have	O
been	O
acquired	O
by	O
the	O
species	B
namely	O
for	O
each	O
bit	B
xg	O
the	O
species	B
has	O
out	O
which	O
of	O
the	O
two	O
states	O
is	O
the	O
better	O
we	O
the	O
information	B
acquired	O
at	O
an	O
intermediate	O
to	O
be	O
the	O
amount	O
of	O
selection	O
in	O
bits	O
required	O
to	O
select	O
the	O
perfect	B
state	O
from	O
the	O
gene	O
pool	O
let	O
a	O
fraction	O
fg	O
of	O
the	O
population	O
have	O
xg	O
because	O
is	O
the	O
information	B
required	O
to	O
a	O
black	B
ball	O
in	O
an	O
urn	B
containing	O
black	B
and	O
white	B
balls	O
in	O
the	O
ratio	O
f	O
we	O
the	O
information	B
acquired	O
to	O
be	O
bits	O
fg	O
i	O
if	O
all	O
the	O
fractions	O
fg	O
are	O
equal	O
to	O
fg	O
then	O
g	O
i	O
g	O
which	O
is	O
well	O
approximated	O
by	O
the	O
rate	B
of	O
information	B
acquisition	O
is	O
thus	O
roughly	O
two	O
times	O
the	O
rate	B
of	O
increase	O
of	O
in	O
the	O
population	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
discussion	O
discussion	O
these	O
results	O
quantify	O
the	O
well	O
known	O
argument	O
for	O
why	O
species	B
reproduce	O
by	O
sex	O
with	O
recombination	O
namely	O
that	O
recombination	O
allows	O
useful	O
mutations	O
to	O
spread	O
more	O
rapidly	O
through	O
the	O
species	B
and	O
allows	O
deleterious	O
mutations	O
to	O
be	O
more	O
rapidly	O
cleared	O
from	O
the	O
population	O
smith	O
felsenstein	O
maynard	O
smith	O
maynard	O
smith	O
and	O
a	O
population	O
that	O
reproduces	O
by	O
recombination	O
can	O
acquire	O
information	B
from	O
natural	B
selection	I
at	O
a	O
rate	B
of	O
order	O
pg	O
times	O
faster	O
than	O
a	O
parthenogenetic	O
population	O
and	O
it	O
can	O
tolerate	O
a	O
mutation	B
rate	B
that	O
is	O
of	O
order	O
pg	O
times	O
greater	O
for	O
genomes	O
of	O
size	O
g	O
coding	O
nucleotides	O
this	O
factor	O
of	O
pg	O
is	O
substantial	O
this	O
enormous	O
advantage	O
conferred	O
by	O
sex	O
has	O
been	O
noted	O
before	O
by	O
kondrashov	O
but	O
this	O
meme	O
which	O
kondrashov	O
calls	O
the	O
deterministic	B
mutation	O
hypothesis	O
does	O
not	O
seem	O
to	O
have	O
throughout	O
the	O
evolutionary	O
research	O
community	O
as	O
there	O
are	O
still	O
numerous	O
papers	O
in	O
which	O
the	O
prevalence	O
of	O
sex	O
is	O
viewed	O
as	O
a	O
mystery	O
to	O
be	O
explained	O
by	O
elaborate	O
mechanisms	O
the	O
cost	B
of	I
males	I
stability	O
of	O
a	O
gene	O
for	O
sex	O
or	O
parthenogenesis	B
why	O
do	O
people	O
declare	O
sex	O
to	O
be	O
a	O
mystery	O
the	O
main	O
motivation	O
for	O
being	O
is	O
an	O
idea	O
called	O
the	O
cost	B
of	I
males	I
sexual	O
reproduction	O
is	O
disadvantageous	O
compared	O
with	O
asexual	O
reproduction	O
it	O
s	O
argued	O
because	O
of	O
every	O
two	O
produced	O
by	O
sex	O
one	O
average	O
is	O
a	O
useless	O
male	B
incapable	O
of	O
child-bearing	O
and	O
only	O
one	O
is	O
a	O
productive	O
female	B
in	O
the	O
same	O
time	O
a	O
parthenogenetic	O
mother	O
could	O
give	O
birth	O
to	O
two	O
female	B
clones	O
to	O
put	O
it	O
another	O
way	O
the	O
big	O
advantage	O
of	O
parthenogenesis	B
from	O
the	O
point	O
of	O
view	O
of	O
the	O
individual	O
is	O
that	O
one	O
is	O
able	O
to	O
pass	O
on	O
of	O
one	O
s	O
genome	B
to	O
one	O
s	O
children	O
instead	O
of	O
only	O
thus	O
if	O
there	O
were	O
two	O
versions	O
of	O
a	O
species	B
one	O
reproducing	O
with	O
and	O
one	O
without	O
sex	O
the	O
single	O
mothers	O
would	O
be	O
expected	O
to	O
outstrip	O
their	O
sexual	O
cousins	O
the	O
simple	O
model	B
presented	O
thus	O
far	O
did	O
not	O
include	O
either	O
genders	O
or	O
the	O
ability	O
to	O
convert	O
from	O
sexual	O
reproduction	O
to	O
asexual	O
but	O
we	O
can	O
easily	O
modify	O
the	O
model	B
we	O
modify	O
the	O
model	B
so	O
that	O
one	O
of	O
the	O
g	O
bits	O
in	O
the	O
genome	B
determines	O
whether	O
an	O
individual	O
prefers	O
to	O
reproduce	O
parthenogenetically	O
or	O
sexually	O
the	O
results	O
depend	O
on	O
the	O
number	O
of	O
children	O
had	O
by	O
a	O
single	O
parthenogenetic	O
mother	O
kp	O
and	O
the	O
number	O
of	O
children	O
born	O
by	O
a	O
sexual	O
couple	O
ks	O
both	O
ks	O
and	O
ks	O
are	O
reasonable	O
models	O
the	O
former	O
ks	O
would	O
seem	O
most	O
appropriate	O
in	O
the	O
case	O
of	O
unicellular	O
organisms	O
where	O
the	O
cytoplasm	O
of	O
both	O
parents	O
goes	O
into	O
the	O
children	O
the	O
latter	O
ks	O
is	O
appropriate	O
if	O
the	O
children	O
are	O
solely	O
nurtured	O
by	O
one	O
of	O
the	O
parents	O
so	O
single	O
mothers	O
have	O
just	O
as	O
many	O
as	O
a	O
sexual	O
pair	O
i	O
concentrate	O
on	O
the	O
latter	O
model	B
since	O
it	O
gives	O
the	O
greatest	O
advantage	O
to	O
the	O
parthenogens	O
who	O
are	O
supposedly	O
expected	O
to	O
outbreed	O
the	O
sexual	O
community	O
because	O
parthenogens	O
have	O
four	O
children	O
per	O
generation	O
the	O
maximum	O
tolerable	O
mutation	B
rate	B
for	O
them	O
is	O
twice	O
the	O
expression	O
derived	O
before	O
for	O
kp	O
if	O
the	O
is	O
large	O
the	O
maximum	O
tolerable	O
rate	B
is	O
mg	O
initially	O
the	O
genomes	O
are	O
set	B
randomly	O
with	O
f	O
with	O
half	O
of	O
the	O
population	O
having	O
the	O
gene	O
for	O
parthenogenesis	B
figure	O
shows	O
the	O
outcome	O
during	O
the	O
learning	B
phase	O
of	O
evolution	B
in	O
which	O
the	O
is	O
increasing	O
rapidly	O
pockets	O
of	O
parthenogens	O
appear	O
but	O
then	O
disappear	O
within	O
a	O
couple	O
of	O
generations	O
as	O
their	O
sexual	O
cousins	O
overtake	O
them	O
in	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
why	O
have	O
sex	O
information	B
acquisition	O
and	O
evolution	B
figure	O
results	O
when	O
there	O
is	O
a	O
gene	O
for	O
parthenogenesis	B
and	O
no	O
interbreeding	O
and	O
single	O
mothers	O
produce	O
as	O
many	O
children	O
as	O
sexual	O
couples	O
g	O
n	O
mg	O
mg	O
vertical	O
axes	O
show	O
the	O
of	O
the	O
two	O
sub-populations	O
and	O
the	O
percentage	O
of	O
the	O
population	O
that	O
is	O
parthenogenetic	O
mg	O
mg	O
s	O
e	O
s	O
s	O
e	O
n	O
t	O
i	O
f	O
e	O
g	O
a	O
t	O
n	O
e	O
c	O
r	O
e	O
p	O
sexual	O
fitness	O
parthen	O
fitness	O
sexual	O
fitness	O
parthen	O
fitness	O
in	O
the	O
presence	O
of	O
a	O
higher	O
mutation	B
rate	B
however	O
leave	O
them	O
behind	O
once	O
the	O
population	O
reaches	O
its	O
top	O
however	O
the	O
parthenogens	O
can	O
take	O
over	O
if	O
the	O
mutation	B
rate	B
is	O
low	O
the	O
parthenogens	O
never	O
take	O
over	O
the	O
breadth	O
of	O
the	O
sexual	O
population	O
s	O
is	O
of	O
order	O
pg	O
so	O
a	O
mutant	O
parthenogenetic	O
colony	O
arising	O
with	O
slightly	O
above-average	O
will	O
last	O
for	O
about	O
pgmg	O
generations	O
before	O
its	O
falls	O
below	O
that	O
of	O
its	O
sexual	O
cousins	O
as	O
long	O
as	O
the	O
population	O
size	O
is	O
large	O
for	O
some	O
sexual	O
individuals	O
to	O
survive	O
for	O
this	O
time	O
sex	O
will	O
not	O
die	B
out	O
in	O
a	O
unstable	O
environment	O
where	O
the	O
function	O
is	O
continually	O
changing	O
the	O
parthenogens	O
will	O
always	O
lag	O
behind	O
the	O
sexual	O
community	O
these	O
results	O
are	O
consistent	O
with	O
the	O
argument	O
of	O
haldane	B
and	O
hamilton	B
that	O
sex	O
is	O
helpful	O
in	O
an	O
arms	B
race	B
with	O
parasites	O
the	O
parasites	O
an	O
function	O
which	O
changes	O
with	O
time	O
and	O
a	O
sexual	O
population	O
will	O
always	O
ascend	O
the	O
current	O
function	O
more	O
rapidly	O
additive	O
function	O
of	O
course	O
our	O
results	O
depend	O
on	O
the	O
function	O
that	O
we	O
assume	O
and	O
on	O
our	O
model	B
of	O
selection	O
is	O
it	O
reasonable	O
to	O
model	B
to	O
order	O
as	O
a	O
sum	O
of	O
independent	O
terms	O
maynard	O
smith	O
argues	O
that	O
it	O
is	O
the	O
more	O
good	B
genes	B
you	O
have	O
the	O
higher	O
you	O
come	O
in	O
the	O
pecking	O
order	O
for	O
example	O
the	O
directional	O
selection	O
model	B
has	O
been	O
used	O
extensively	O
in	O
theoretical	O
population	O
genetic	B
studies	O
we	O
might	O
expect	O
real	O
functions	B
to	O
involve	O
interactions	O
in	O
which	O
case	O
crossover	B
might	O
reduce	O
the	O
average	O
however	O
since	O
recombination	O
gives	O
the	O
biggest	O
advantage	O
to	O
species	B
whose	O
functions	B
are	O
additive	O
we	O
might	O
predict	O
that	O
evolution	B
will	O
have	O
favoured	O
species	B
that	O
used	O
a	O
representation	O
of	O
the	O
genome	B
that	O
corresponds	O
to	O
a	O
function	O
that	O
has	O
only	O
weak	O
interactions	O
and	O
even	O
if	O
there	O
are	O
interactions	O
it	O
seems	O
plausible	O
that	O
the	O
would	O
still	O
involve	O
a	O
sum	O
of	O
such	O
interacting	O
terms	O
with	O
the	O
number	O
of	O
terms	O
being	O
some	O
fraction	O
of	O
the	O
genome	B
size	O
g	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
exercise	O
investigate	O
how	O
fast	O
sexual	O
and	O
asexual	O
species	B
evolve	O
if	O
they	O
have	O
a	O
function	O
with	O
interactions	O
for	O
example	O
let	O
the	O
be	O
a	O
sum	O
of	O
exclusive-ors	O
of	O
pairs	O
of	O
bits	O
compare	O
the	O
evolving	O
with	O
those	O
of	O
the	O
sexual	O
and	O
asexual	O
species	B
with	O
a	O
simple	O
additive	O
function	O
furthermore	O
if	O
the	O
function	O
were	O
a	O
highly	O
nonlinear	B
function	O
of	O
the	O
genotype	O
it	O
could	O
be	O
made	O
more	O
smooth	O
and	O
locally	O
linear	B
by	O
the	O
baldwin	O
the	O
baldwin	O
hinton	B
and	O
nowlan	O
has	O
been	O
widely	O
studied	O
as	O
a	O
mechanism	O
whereby	O
learning	B
guides	O
evolution	B
and	O
it	O
could	O
also	O
act	O
at	O
the	O
level	O
of	O
transcription	O
and	O
translation	O
consider	O
the	O
evolution	B
of	O
a	O
peptide	O
sequence	B
for	O
a	O
new	O
purpose	O
assume	O
the	O
of	O
the	O
peptide	O
is	O
a	O
highly	O
nonlinear	B
function	O
of	O
the	O
sequence	B
perhaps	O
having	O
a	O
small	O
island	O
of	O
good	B
sequences	O
surrounded	O
by	O
an	O
ocean	O
of	O
equally	O
bad	B
sequences	O
in	O
an	O
organism	O
whose	O
transcription	O
and	O
translation	O
machinery	O
is	O
the	O
will	O
be	O
an	O
equally	O
nonlinear	B
function	O
of	O
the	O
dna	B
sequence	B
and	O
evolution	B
will	O
wander	O
around	O
the	O
ocean	O
making	O
progress	O
towards	O
the	O
island	O
only	O
by	O
a	O
random	B
walk	I
in	O
contrast	O
an	O
organism	O
having	O
the	O
same	O
dna	B
sequence	B
but	O
whose	O
dna-to-rna	O
transcription	O
or	O
rna-to-protein	O
translation	O
is	O
faulty	O
will	O
occasionally	O
by	O
mistranslation	O
or	O
mistranscription	O
accidentally	O
produce	O
a	O
working	O
enzyme	O
and	O
it	O
will	O
do	O
so	O
with	O
greater	O
probability	B
if	O
its	O
dna	B
sequence	B
is	O
close	O
to	O
a	O
good	B
sequence	B
one	O
cell	O
might	O
produce	O
proteins	O
from	O
the	O
one	O
mrna	O
sequence	B
of	O
which	O
have	O
no	O
enzymatic	O
and	O
one	O
does	O
the	O
one	O
working	O
catalyst	O
will	O
be	O
enough	O
for	O
that	O
cell	O
to	O
have	O
an	O
increased	O
relative	B
to	O
rivals	O
whose	O
dna	B
sequence	B
is	O
further	O
from	O
the	O
island	O
of	O
good	B
sequences	O
for	O
this	O
reason	O
i	O
conjecture	O
that	O
at	O
least	O
early	O
in	B
evolution	B
and	O
perhaps	O
still	O
now	O
the	O
genetic	B
code	I
was	O
not	O
implemented	O
perfectly	O
but	O
was	O
implemented	O
noisily	O
with	O
some	O
codons	O
coding	O
for	O
a	O
distribution	B
of	O
possible	O
amino	O
acids	O
this	O
noisy	B
code	O
could	O
even	O
be	O
switched	O
on	O
and	O
from	O
cell	O
to	O
cell	O
in	O
an	O
organism	O
by	O
having	O
multiple	O
aminoacyl-trna	O
synthetases	O
some	O
more	O
reliable	O
than	O
others	O
whilst	O
our	O
model	B
assumed	O
that	O
the	O
bits	O
of	O
the	O
genome	B
do	O
not	O
interact	O
ignored	O
the	O
fact	O
that	O
the	O
information	B
is	O
represented	O
redundantly	O
assumed	O
that	O
there	O
is	O
a	O
direct	O
relationship	O
between	O
phenotypic	O
and	O
the	O
genotype	O
and	O
assumed	O
that	O
the	O
crossover	B
probability	B
in	O
recombination	O
is	O
high	O
i	O
believe	O
these	O
qualitative	O
results	O
would	O
still	O
hold	O
if	O
more	O
complex	B
models	O
of	O
and	O
crossover	B
were	O
used	O
the	O
relative	B
of	O
sex	O
will	O
still	O
scale	O
as	O
pg	O
only	O
in	O
small	O
in-bred	O
populations	O
are	O
the	O
of	O
sex	O
expected	O
to	O
be	O
diminished	O
in	O
summary	B
why	O
have	O
sex	O
because	O
sex	O
is	O
good	B
for	O
your	O
bits	O
further	O
reading	O
how	O
did	O
a	O
high-information-content	O
self-replicating	O
system	O
ever	O
emerge	O
in	O
the	O
place	O
in	O
the	O
general	O
area	O
of	O
the	O
origins	O
of	O
life	B
and	O
other	O
tricky	O
questions	O
about	O
evolution	B
i	O
highly	O
recommend	O
maynard	O
smith	O
and	O
maynard	O
smith	O
and	O
kondrashov	O
maynard	O
smith	O
ridley	O
dyson	O
cairns-smith	O
and	O
further	O
exercises	O
exercise	O
how	O
good	B
must	O
the	O
error-correcting	O
machinery	O
in	B
dna	B
replication	B
be	O
given	O
that	O
mammals	O
have	O
not	O
all	O
died	O
out	O
long	O
ago	O
estimate	O
the	O
probability	B
of	O
nucleotide	B
substitution	O
per	O
cell	O
division	O
appendix	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
why	O
have	O
sex	O
information	B
acquisition	O
and	O
evolution	B
exercise	O
given	O
that	O
dna	B
replication	B
is	O
achieved	O
by	O
bumbling	O
brownian	B
motion	I
and	O
ordinary	O
thermodynamics	B
in	O
a	O
biochemical	O
porridge	B
at	O
a	O
temperature	B
of	O
c	O
it	O
s	O
astonishing	O
that	O
the	O
error-rate	O
of	O
dna	B
replication	B
is	O
about	O
per	O
replicated	O
nucleotide	B
how	O
can	O
this	O
reliability	O
be	O
achieved	O
given	O
that	O
the	O
energetic	O
between	O
a	O
correct	O
base-pairing	B
and	O
an	O
incorrect	O
one	O
is	O
only	O
one	O
or	O
two	O
hydrogen	O
bonds	O
and	O
the	O
thermal	O
energy	B
kt	O
is	O
only	O
about	O
a	O
factor	O
of	O
four	O
smaller	O
than	O
the	O
free	O
energy	B
associated	O
with	O
a	O
hydrogen	B
bond	I
if	O
ordinary	O
thermodynamics	B
is	O
what	O
favours	O
correct	O
base-pairing	B
surely	O
the	O
frequency	B
of	O
incorrect	O
base-pairing	B
should	O
be	O
about	O
f	O
where	O
is	O
the	O
free	O
energy	B
i	O
e	O
an	O
error	O
frequency	B
of	O
f	O
how	O
has	O
dna	B
replication	B
cheated	O
thermodynamics	B
the	O
situation	O
is	O
equally	O
perplexing	O
in	O
the	O
case	O
of	O
protein	B
synthesis	B
which	O
translates	O
an	O
mrna	O
sequence	B
into	O
a	O
polypeptide	O
in	O
accordance	O
with	O
the	O
genetic	B
code	I
two	O
chemical	O
reactions	O
are	O
protected	O
against	O
errors	B
the	O
binding	O
of	O
trna	O
molecules	B
to	O
amino	O
acids	O
and	O
the	O
production	O
of	O
the	O
polypeptide	O
in	O
the	O
ribosome	O
which	O
involves	O
base-pairing	B
again	O
the	O
is	O
high	O
error	O
rate	B
of	O
about	O
and	O
this	O
can	O
t	O
be	O
caused	O
by	O
the	O
energy	B
of	O
the	O
correct	O
state	O
being	O
especially	O
low	O
the	O
correct	O
polypeptide	O
sequence	B
is	O
not	O
expected	O
to	O
be	O
lower	O
in	O
energy	B
than	O
any	O
other	O
sequence	B
how	O
do	O
cells	O
perform	O
error	O
correction	O
like	O
dna	B
replication	B
exercise	O
while	O
the	O
genome	B
acquires	O
information	B
through	O
natural	B
selection	I
at	O
a	O
rate	B
of	O
a	O
few	O
bits	O
per	O
generation	O
your	O
brain	B
acquires	O
information	B
at	O
a	O
greater	O
rate	B
estimate	O
at	O
what	O
rate	B
new	O
information	B
can	O
be	O
stored	O
in	O
long	O
term	O
memory	B
by	O
your	O
brain	B
think	O
of	O
learning	B
the	O
words	O
of	O
a	O
new	O
language	O
for	O
example	O
solutions	O
solution	O
to	O
exercise	O
for	O
small	O
enough	O
n	O
whilst	O
the	O
average	O
of	O
the	O
population	O
increases	O
some	O
unlucky	O
bits	O
become	O
frozen	O
into	O
the	O
bad	B
state	O
bad	B
genes	B
are	O
sometimes	O
known	O
as	O
hitchhikers	O
the	O
homogeneity	O
assumption	O
breaks	O
down	O
eventually	O
all	O
individuals	O
have	O
identical	O
genotypes	O
that	O
are	O
mainly	O
but	O
contain	O
some	O
too	O
the	O
smaller	O
the	O
population	O
the	O
greater	O
the	O
number	O
of	O
frozen	O
is	O
expected	O
to	O
be	O
how	O
small	O
can	O
the	O
population	O
size	O
n	O
be	O
if	O
the	O
theory	O
of	O
sex	O
is	O
accurate	O
we	O
experimentally	O
that	O
the	O
theory	O
based	O
on	O
assuming	O
homogeneity	O
poorly	O
only	O
if	O
the	O
population	O
size	O
n	O
is	O
smaller	O
than	O
if	O
n	O
is	O
smaller	O
than	O
pg	O
information	B
cannot	O
possibly	O
be	O
acquired	O
at	O
a	O
rate	B
as	O
big	O
as	O
pg	O
since	O
the	O
information	B
content	I
of	O
the	O
blind	B
watchmaker	I
s	O
decisions	O
cannot	O
be	O
any	O
greater	O
than	O
bits	O
per	O
generation	O
this	O
being	O
the	O
number	O
of	O
bits	O
required	O
to	O
specify	O
which	O
of	O
the	O
children	O
get	O
to	O
reproduce	O
baum	O
et	O
al	O
analyzing	O
a	O
similar	O
model	B
show	O
that	O
the	O
population	O
size	O
n	O
should	O
be	O
about	O
pglog	O
to	O
make	O
hitchhikers	O
unlikely	O
to	O
arise	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
part	O
iv	O
probabilities	O
and	O
inference	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
part	O
iv	O
the	O
number	O
of	O
inference	B
problems	O
that	O
can	O
perhaps	O
should	O
be	O
tackled	O
by	O
bayesian	B
inference	B
methods	O
is	O
enormous	O
in	O
this	O
book	O
for	O
example	O
we	O
discuss	O
the	O
decoding	B
problem	O
for	O
error-correcting	B
codes	I
the	O
task	O
of	O
inferring	O
clusters	O
from	O
data	O
the	O
task	O
of	O
interpolation	O
through	O
noisy	B
data	O
and	O
the	O
task	O
of	O
classifying	O
patterns	O
given	O
labelled	O
examples	O
most	O
techniques	O
for	O
solving	O
these	O
problems	O
can	O
be	O
categorized	O
as	O
follows	O
exact	O
methods	O
compute	O
the	O
required	O
quantities	O
directly	O
only	O
a	O
few	O
interesting	O
problems	O
have	O
a	O
direct	O
solution	O
but	O
exact	O
methods	O
are	O
important	O
as	O
tools	O
for	O
solving	O
subtasks	O
within	O
larger	O
problems	O
methods	O
for	O
the	O
exact	O
solution	O
of	O
inference	B
problems	O
are	O
the	O
subject	O
of	O
chapters	O
and	O
approximate	O
methods	O
can	O
be	O
subdivided	O
into	O
deterministic	B
approximations	O
which	O
include	O
maximum	B
likelihood	B
laplace	B
s	O
method	O
and	O
and	O
variational	B
methods	I
and	O
monte	B
carlo	I
methods	I
techniques	O
in	O
which	O
random	B
numbers	O
play	O
an	O
integral	B
part	O
which	O
will	O
be	O
discussed	O
in	O
chapters	O
and	O
this	O
part	O
of	O
the	O
book	O
does	O
not	O
form	O
a	O
one-dimensional	O
story	O
rather	O
the	O
ideas	O
make	O
up	O
a	O
web	O
of	O
interrelated	O
threads	O
which	O
will	O
recombine	O
in	O
subsequent	O
chapters	O
chapter	O
which	O
is	O
an	O
honorary	O
member	O
of	O
this	O
part	O
discussed	O
a	O
range	O
of	O
simple	O
examples	O
of	O
inference	B
problems	O
and	O
their	O
bayesian	B
solutions	O
to	O
give	O
further	O
motivation	O
for	O
the	O
toolbox	O
of	O
inference	B
methods	O
discussed	O
in	O
this	O
part	O
chapter	O
discusses	O
the	O
problem	O
of	O
clustering	B
subsequent	O
chapters	O
discuss	O
the	O
probabilistic	O
interpretation	O
of	O
clustering	B
as	O
mixture	B
modelling	B
chapter	O
discusses	O
the	O
option	O
of	O
dealing	O
with	O
probability	B
distributions	O
by	O
completely	O
enumerating	O
all	O
hypotheses	O
chapter	O
introduces	O
the	O
idea	O
of	O
maximization	O
methods	O
as	O
a	O
way	O
of	O
avoiding	O
the	O
large	O
cost	O
associated	O
with	O
complete	O
enumeration	O
and	O
points	O
out	O
reasons	O
why	O
maximum	B
likelihood	B
is	O
not	O
good	B
enough	O
chapter	O
reviews	O
the	O
probability	B
distributions	O
that	O
arise	O
most	O
often	O
in	O
bayesian	B
inference	B
chapters	O
and	O
discuss	O
another	O
way	O
of	O
avoiding	O
the	O
cost	O
of	O
complete	O
enumeration	O
marginalization	B
chapter	O
discusses	O
message-passing	B
methods	O
appropriate	O
for	O
graphical	O
models	O
using	O
the	O
decoding	B
of	O
error-correcting	B
codes	I
as	O
an	O
example	O
chapter	O
combines	O
these	O
ideas	O
with	O
message-passing	B
concepts	O
from	O
chapters	O
and	O
these	O
chapters	O
are	O
a	O
prerequisite	O
for	O
the	O
understanding	O
of	O
advanced	O
error-correcting	B
codes	I
chapter	O
discusses	O
deterministic	B
approximations	O
including	O
laplace	B
s	O
method	O
this	O
chapter	O
is	O
a	O
prerequisite	O
for	O
understanding	O
the	O
topic	O
of	O
complexity	B
control	I
in	O
learning	B
algorithms	B
an	O
idea	O
that	O
is	O
discussed	O
in	O
general	O
terms	O
in	O
chapter	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
part	O
iv	O
chapter	O
discusses	O
monte	B
carlo	I
methods	I
chapter	O
gives	O
details	O
of	O
state-of-the-art	O
monte	B
carlo	I
techniques	O
chapter	O
introduces	O
the	O
ising	B
model	B
as	O
a	O
test-bed	O
for	O
probabilistic	O
methods	O
an	O
exact	O
message-passing	B
method	O
and	O
a	O
monte	B
carlo	I
method	O
are	O
demonstrated	O
a	O
motivation	O
for	O
studying	O
the	O
ising	B
model	B
is	O
that	O
it	O
is	O
intimately	O
related	O
to	O
several	O
neural	B
network	B
models	O
chapter	O
describes	O
exact	O
monte	B
carlo	I
methods	I
and	O
demonstrates	O
their	O
application	O
to	O
the	O
ising	B
model	B
chapter	O
discusses	O
variational	B
methods	I
and	O
their	O
application	O
to	O
ising	O
models	O
and	O
to	O
simple	O
statistical	B
inference	B
problems	O
including	O
clustering	B
this	O
chapter	O
will	O
help	O
the	O
reader	O
understand	O
the	O
network	B
and	O
the	O
em	B
algorithm	I
which	O
is	O
an	O
important	O
method	O
in	O
latent-variable	O
modelling	B
chapter	O
discusses	O
a	O
particularly	O
simple	O
latent	B
variable	B
model	B
called	O
independent	B
component	I
analysis	I
chapter	O
discusses	O
a	O
ragbag	O
of	O
assorted	O
inference	B
topics	O
chapter	O
discusses	O
a	O
simple	O
example	O
of	O
decision	B
theory	I
chapter	O
discusses	O
between	O
sampling	B
theory	I
and	O
bayesian	B
methods	O
a	O
theme	O
what	O
inference	B
is	O
about	O
a	O
widespread	O
misconception	O
is	O
that	O
the	O
aim	O
of	O
inference	B
is	O
to	O
the	O
most	O
probable	O
explanation	O
for	O
some	O
data	O
while	O
this	O
most	O
probable	O
hypothesis	O
may	O
be	O
of	O
interest	O
and	O
some	O
inference	B
methods	O
do	O
locate	O
it	O
this	O
hypothesis	O
is	O
just	O
the	O
peak	O
of	O
a	O
probability	B
distribution	B
and	O
it	O
is	O
the	O
whole	O
distribution	B
that	O
is	O
of	O
interest	O
as	O
we	O
saw	O
in	O
chapter	O
the	O
most	O
probable	O
outcome	O
from	O
a	O
source	O
is	O
often	O
not	O
a	O
typical	B
outcome	O
from	O
that	O
source	O
similarly	O
the	O
most	O
probable	O
hypothesis	O
given	O
some	O
data	O
may	O
be	O
atypical	O
of	O
the	O
whole	O
set	B
of	O
reasonablyplausible	O
hypotheses	O
about	O
chapter	O
before	O
reading	O
the	O
next	O
chapter	O
exercise	O
and	O
section	B
the	O
input	O
to	O
a	O
gaussian	B
channel	I
are	O
recommended	O
reading	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
an	O
example	O
inference	B
task	O
clustering	B
human	B
brains	O
are	O
good	B
at	O
regularities	O
in	O
data	O
one	O
way	O
of	O
expressing	O
regularity	O
is	O
to	O
put	O
a	O
set	B
of	O
objects	O
into	O
groups	O
that	O
are	O
similar	O
to	O
each	O
other	O
for	O
example	O
biologists	O
have	O
found	O
that	O
most	O
objects	O
in	O
the	O
natural	B
world	O
fall	O
into	O
one	O
of	O
two	O
categories	O
things	O
that	O
are	O
brown	O
and	O
run	O
away	O
and	O
things	O
that	O
are	O
green	O
and	O
don	O
t	O
run	O
away	O
the	O
group	O
they	O
call	O
animals	O
and	O
the	O
second	O
plants	O
we	O
ll	O
call	O
this	O
operation	O
of	O
grouping	O
things	O
together	O
clustering	B
if	O
the	O
biologist	O
further	O
sub-divides	O
the	O
cluster	O
of	O
plants	O
into	O
subclusters	O
we	O
would	O
call	O
this	O
hierarchical	B
clustering	B
but	O
we	O
won	O
t	O
be	O
talking	O
about	O
hierarchical	B
clustering	B
yet	O
in	O
this	O
chapter	O
we	O
ll	O
just	O
discuss	O
ways	O
to	O
take	O
a	O
set	B
of	O
n	O
objects	O
and	O
group	O
them	O
into	O
k	O
clusters	O
there	O
are	O
several	O
motivations	O
for	O
clustering	B
first	O
a	O
good	B
clustering	B
has	O
predictive	O
power	O
when	O
an	O
early	O
biologist	O
encounters	O
a	O
new	O
green	O
thing	O
he	O
has	O
not	O
seen	O
before	O
his	O
internal	O
model	B
of	O
plants	O
and	O
animals	O
in	O
predictions	O
for	O
attributes	O
of	O
the	O
green	O
thing	O
it	O
s	O
unlikely	O
to	O
jump	O
on	O
him	O
and	O
eat	O
him	O
if	O
he	O
touches	O
it	O
he	O
might	O
get	O
grazed	O
or	O
stung	O
if	O
he	O
eats	O
it	O
he	O
might	O
feel	O
sick	O
all	O
of	O
these	O
predictions	O
while	O
uncertain	O
are	O
useful	O
because	O
they	O
help	O
the	O
biologist	O
invest	O
his	O
resources	O
example	O
the	O
time	O
spent	O
watching	O
for	O
predators	O
well	O
thus	O
we	O
perform	O
clustering	B
because	O
we	O
believe	O
the	O
underlying	O
cluster	O
labels	O
are	O
meaningful	O
will	O
lead	O
to	O
a	O
more	O
description	O
of	O
our	O
data	O
and	O
will	O
help	O
us	O
choose	O
better	O
actions	O
this	O
type	O
of	O
clustering	B
is	O
sometimes	O
called	O
mixture	O
density	B
modelling	B
and	O
the	O
objective	B
function	I
that	O
measures	O
how	O
well	O
the	O
predictive	O
model	B
is	O
working	O
is	O
the	O
information	B
content	I
of	O
the	O
data	O
log	O
second	O
clusters	O
can	O
be	O
a	O
useful	O
aid	O
to	O
communication	B
because	O
they	O
allow	O
lossy	B
compression	B
the	O
biologist	O
can	O
give	O
directions	O
to	O
a	O
friend	O
such	O
as	O
go	O
to	O
the	O
third	O
tree	B
on	O
the	O
right	O
then	O
take	O
a	O
right	O
turn	O
than	O
go	O
past	O
the	O
large	O
green	O
thing	O
with	O
red	O
berries	O
then	O
past	O
the	O
large	O
green	O
thing	O
with	O
thorns	O
then	O
the	O
brief	O
category	O
name	O
tree	B
is	O
helpful	O
because	O
it	O
is	O
to	O
identify	O
an	O
object	O
similarly	O
in	O
lossy	B
image	B
compression	B
the	O
aim	O
is	O
to	O
convey	O
in	O
as	O
few	O
bits	O
as	O
possible	O
a	O
reasonable	O
reproduction	O
of	O
a	O
picture	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
divide	O
the	O
image	B
into	O
n	O
small	O
patches	O
and	O
a	O
close	O
match	O
to	O
each	O
patch	O
in	O
an	O
alphabet	O
of	O
k	O
image-templates	O
then	O
we	O
send	O
a	O
close	O
to	O
the	O
image	B
by	O
sending	O
the	O
list	O
of	O
labels	O
kn	O
of	O
the	O
matching	O
templates	O
the	O
task	O
of	O
creating	O
a	O
good	B
library	O
of	O
image-templates	O
is	O
equivalent	O
to	O
a	O
set	B
of	O
cluster	O
centres	O
this	O
type	O
of	O
clustering	B
is	O
sometimes	O
called	O
vector	B
quantization	I
we	O
can	O
formalize	O
a	O
vector	O
quantizer	O
in	O
terms	O
of	O
an	O
assignment	O
rule	O
x	O
kx	O
for	O
assigning	B
datapoints	O
x	O
to	O
one	O
of	O
k	O
codenames	O
and	O
a	O
reconstruction	O
rule	O
k	O
mk	O
the	O
aim	O
being	O
to	O
choose	O
the	O
functions	B
kx	O
and	O
mk	O
so	O
as	O
to	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
k-means	B
clustering	B
minimize	O
the	O
expected	O
distortion	O
which	O
might	O
be	O
to	O
be	O
d	O
p	O
ideal	O
objective	B
function	I
would	O
be	O
to	O
minimize	O
the	O
psychologically	O
perceived	O
distortion	O
of	O
the	O
image	B
since	O
it	O
is	O
hard	O
to	O
quantify	O
the	O
distortion	O
perceived	O
by	O
a	O
human	B
vector	B
quantization	I
and	O
lossy	B
compression	B
are	O
not	O
so	O
crisply	O
problems	O
as	O
data	O
modelling	B
and	O
lossless	B
compression	B
in	O
vector	B
quantization	I
we	O
don	O
t	O
necessarily	O
believe	O
that	O
the	O
templates	O
fmkg	O
have	O
any	O
natural	B
meaning	O
they	O
are	O
simply	O
tools	O
to	O
do	O
a	O
job	O
we	O
note	O
in	O
passing	O
the	O
similarity	O
of	O
the	O
assignment	O
rule	O
the	O
encoder	B
of	O
vector	B
quantization	I
to	O
the	O
decoding	B
problem	O
when	O
decoding	B
an	O
error-correcting	B
code	I
a	O
third	O
reason	O
for	O
making	O
a	O
cluster	O
model	B
is	O
that	O
failures	O
of	O
the	O
cluster	O
model	B
may	O
highlight	O
interesting	O
objects	O
that	O
deserve	O
special	O
attention	O
if	O
we	O
have	O
trained	O
a	O
vector	O
quantizer	O
to	O
do	O
a	O
good	B
job	O
of	O
compressing	O
satellite	O
pictures	O
of	O
ocean	O
surfaces	O
then	O
maybe	O
patches	O
of	O
image	B
that	O
are	O
not	O
well	O
compressed	O
by	O
the	O
vector	O
quantizer	O
are	O
the	O
patches	O
that	O
contain	O
ships	O
if	O
the	O
biologist	O
encounters	O
a	O
green	O
thing	O
and	O
sees	O
it	O
run	O
slither	O
away	O
this	O
with	O
his	O
cluster	O
model	B
says	O
green	O
things	O
don	O
t	O
run	O
away	O
cues	O
him	O
to	O
pay	O
special	O
attention	O
one	O
can	O
t	O
spend	O
all	O
one	O
s	O
time	O
being	O
fascinated	O
by	O
things	O
the	O
cluster	O
model	B
can	O
help	O
sift	O
out	O
from	O
the	O
multitude	O
of	O
objects	O
in	O
one	O
s	O
world	O
the	O
ones	O
that	O
really	O
deserve	O
attention	O
a	O
fourth	O
reason	O
for	O
liking	O
clustering	B
algorithms	B
is	O
that	O
they	O
may	O
serve	O
as	O
models	O
of	O
learning	B
processes	O
in	O
neural	O
systems	O
the	O
clustering	B
algorithm	O
that	O
we	O
now	O
discuss	O
the	O
k-means	O
algorithm	O
is	O
an	O
example	O
of	O
a	O
competitive	B
learning	B
algorithm	O
the	O
algorithm	O
works	O
by	O
having	O
the	O
k	O
clusters	O
compete	O
with	O
each	O
other	O
for	O
the	O
right	O
to	O
own	O
the	O
data	O
points	O
k-means	B
clustering	B
figure	O
n	O
data	O
points	O
the	O
k-means	O
algorithm	O
is	O
an	O
algorithm	O
for	O
putting	O
n	O
data	O
points	O
in	O
an	O
idimensional	O
space	O
into	O
k	O
clusters	O
each	O
cluster	O
is	O
parameterized	O
by	O
a	O
vector	O
mk	O
called	O
its	O
mean	B
the	O
data	O
points	O
will	O
be	O
denoted	O
by	O
fxng	O
where	O
the	O
superscript	O
n	O
runs	O
from	O
to	O
the	O
number	O
of	O
data	O
points	O
n	O
each	O
vector	O
x	O
has	O
i	O
components	O
xi	O
we	O
will	O
assume	O
that	O
the	O
space	O
that	O
x	O
lives	O
in	O
is	O
a	O
real	O
space	O
and	O
that	O
we	O
have	O
a	O
metric	B
that	O
distances	O
between	O
points	O
for	O
example	O
about	O
the	O
name	O
as	O
far	O
as	O
i	O
know	O
the	O
k	O
in	O
k-means	B
clustering	B
simply	O
refers	O
to	O
the	O
chosen	O
number	O
of	O
clusters	O
if	O
newton	B
had	O
followed	O
the	O
same	O
naming	O
policy	O
maybe	O
we	O
would	O
learn	O
at	O
school	O
about	O
calculus	O
for	O
the	O
variable	O
x	O
it	O
s	O
a	O
silly	O
name	O
but	O
we	O
are	O
stuck	O
with	O
it	O
dx	O
y	O
to	O
start	O
the	O
k-means	O
algorithm	O
the	O
k	O
means	O
fmkg	O
are	O
initialized	O
in	O
some	O
way	O
for	O
example	O
to	O
random	B
values	O
k-means	O
is	O
then	O
an	O
iterative	O
two-step	O
algorithm	O
in	O
the	O
assignment	O
step	O
each	O
data	O
point	O
n	O
is	O
assigned	O
to	O
the	O
nearest	O
mean	B
in	O
the	O
update	O
step	O
the	O
means	O
are	O
adjusted	O
to	O
match	O
the	O
sample	B
means	O
of	O
the	O
data	O
points	O
that	O
they	O
are	O
responsible	O
for	O
the	O
k-means	O
algorithm	O
is	O
demonstrated	O
for	O
a	O
toy	O
two-dimensional	B
data	B
set	B
in	O
where	O
means	O
are	O
used	O
the	O
assignments	O
of	O
the	O
points	O
to	O
the	O
two	O
clusters	O
are	O
indicated	O
by	O
two	O
point	O
styles	O
and	O
the	O
two	O
means	O
are	O
shown	O
by	O
the	O
circles	O
the	O
algorithm	O
converges	O
after	O
three	O
iterations	O
at	O
which	O
point	O
the	O
assignments	O
are	O
unchanged	O
so	O
the	O
means	O
remain	O
unmoved	O
when	O
updated	O
the	O
k-means	O
algorithm	O
always	O
converges	O
to	O
a	O
point	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
an	O
example	O
inference	B
task	O
clustering	B
algorithm	O
the	O
k-means	B
clustering	B
algorithm	O
initialization	O
set	B
k	O
means	O
fmkg	O
to	O
random	B
values	O
assignment	O
step	O
each	O
data	O
point	O
n	O
is	O
assigned	O
to	O
the	O
nearest	O
mean	B
we	O
denote	O
our	O
guess	O
for	O
the	O
cluster	O
kn	O
that	O
the	O
point	O
xn	O
belongs	O
to	O
by	O
fdmk	O
xng	O
argmin	O
k	O
an	O
alternative	O
equivalent	O
representation	O
of	O
this	O
assignment	O
of	O
points	O
to	O
clusters	O
is	O
given	O
by	O
responsibilities	O
which	O
are	O
indicator	O
variables	O
rn	O
to	O
one	O
if	O
mean	B
k	O
is	O
the	O
closest	O
mean	B
to	O
datapoint	O
xn	O
otherwise	O
rn	O
k	O
in	O
the	O
assignment	O
step	O
we	O
set	B
rn	O
is	O
zero	O
k	O
k	O
rn	O
k	O
if	O
if	O
k	O
k	O
what	O
about	O
ties	O
we	O
don	O
t	O
expect	O
two	O
means	O
to	O
be	O
exactly	O
the	O
same	O
distance	B
from	O
a	O
data	O
point	O
but	O
if	O
a	O
tie	O
does	O
happen	O
is	O
set	B
to	O
the	O
smallest	O
of	O
the	O
winning	O
fkg	O
update	O
step	O
the	O
model	B
parameters	B
the	O
means	O
are	O
adjusted	O
to	O
match	O
the	O
sample	B
means	O
of	O
the	O
data	O
points	O
that	O
they	O
are	O
responsible	O
for	O
mk	O
xn	O
rn	O
k	O
xn	O
rk	O
where	O
rk	O
is	O
the	O
total	O
responsibility	B
of	O
mean	B
k	O
rk	O
rn	O
k	O
what	O
about	O
means	O
with	O
no	O
responsibilities	O
if	O
rk	O
then	O
we	O
leave	O
the	O
mean	B
mk	O
where	O
it	O
is	O
repeat	O
the	O
assignment	O
step	O
and	O
update	O
step	O
until	O
the	O
assign	O
ments	O
do	O
not	O
change	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
k-means	B
clustering	B
data	O
figure	O
k-means	O
algorithm	O
applied	O
to	O
a	O
data	B
set	B
of	O
points	O
k	O
means	O
evolve	O
to	O
stable	O
locations	O
after	O
three	O
iterations	O
assignment	O
update	O
assignment	O
update	O
assignment	O
update	O
run	O
run	O
figure	O
k-means	O
algorithm	O
applied	O
to	O
a	O
data	B
set	B
of	O
points	O
two	O
separate	O
runs	O
both	O
with	O
k	O
means	O
reach	O
solutions	O
each	O
frame	O
shows	O
a	O
successive	O
assignment	O
step	O
exercise	O
see	O
if	O
you	O
can	O
prove	O
that	O
k-means	O
always	O
converges	O
a	O
physical	O
analogy	O
and	O
an	O
associated	O
lyapunov	B
function	I
lyapunov	B
function	I
is	O
a	O
function	O
of	O
the	O
state	O
of	O
the	O
algorithm	O
that	O
decreases	O
whenever	O
the	O
state	O
changes	O
and	O
that	O
is	O
bounded	O
below	O
if	O
a	O
system	O
has	O
a	O
lyapunov	B
function	I
then	O
its	O
dynamics	O
converge	O
the	O
k-means	O
algorithm	O
with	O
a	O
larger	O
number	O
of	O
means	O
is	O
demonstrated	O
in	O
the	O
outcome	O
of	O
the	O
algorithm	O
depends	O
on	O
the	O
initial	O
condition	O
in	O
the	O
case	O
after	O
iterations	O
a	O
steady	O
state	O
is	O
found	O
in	O
which	O
the	O
data	O
points	O
are	O
fairly	O
evenly	O
split	O
between	O
the	O
four	O
clusters	O
in	O
the	O
second	O
case	O
after	O
six	B
iterations	O
half	O
the	O
data	O
points	O
are	O
in	O
one	O
cluster	O
and	O
the	O
others	O
are	O
shared	O
among	O
the	O
other	O
three	O
clusters	O
questions	O
about	O
this	O
algorithm	O
the	O
k-means	O
algorithm	O
has	O
several	O
ad	O
hoc	O
features	O
why	O
does	O
the	O
update	O
step	O
set	B
the	O
mean	B
to	O
the	O
mean	B
of	O
the	O
assigned	O
points	O
where	O
did	O
the	O
distance	B
d	O
come	O
from	O
what	O
if	O
we	O
used	O
a	O
measure	O
of	O
distance	B
between	O
x	O
and	O
m	O
how	O
can	O
we	O
choose	O
the	O
best	O
distance	B
vector	B
quantization	I
the	O
distance	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
an	O
example	O
inference	B
task	O
clustering	B
figure	O
k-means	O
algorithm	O
for	O
a	O
case	O
with	O
two	O
dissimilar	O
clusters	O
the	O
n	O
large	O
data	O
a	O
stable	O
set	B
of	O
assignments	O
and	O
means	O
note	O
that	O
four	O
points	O
belonging	O
to	O
the	O
broad	O
cluster	O
have	O
been	O
incorrectly	O
assigned	O
to	O
the	O
narrower	O
cluster	O
assigned	O
to	O
the	O
right-hand	O
cluster	O
are	O
shown	O
by	O
plus	O
signs	O
figure	O
two	O
elongated	O
clusters	O
and	O
the	O
stable	O
solution	O
found	O
by	O
the	O
k-means	O
algorithm	O
function	O
is	O
provided	O
as	O
part	O
of	O
the	O
problem	O
but	O
i	O
m	O
assuming	O
we	O
are	O
interested	O
in	O
data-modelling	O
rather	O
than	O
vector	B
quantization	I
how	O
do	O
we	O
choose	O
k	O
having	O
found	O
multiple	O
alternative	O
clusterings	O
for	O
a	O
given	O
k	O
how	O
can	O
we	O
choose	O
among	O
them	O
cases	O
where	O
k-means	O
might	O
be	O
viewed	O
as	O
failing	O
further	O
questions	O
arise	O
when	O
we	O
look	O
for	O
cases	O
where	O
the	O
algorithm	O
behaves	O
badly	O
with	O
what	O
the	O
man	O
in	O
the	O
street	O
would	O
call	O
clustering	B
figure	O
shows	O
a	O
set	B
of	O
data	O
points	O
generated	O
from	O
a	O
mixture	O
of	O
two	O
gaussians	O
the	O
right-hand	O
gaussian	B
has	O
less	O
weight	O
one	O
of	O
the	O
data	O
points	O
and	O
it	O
is	O
a	O
less	O
broad	O
cluster	O
figure	O
shows	O
the	O
outcome	O
of	O
using	O
k-means	B
clustering	B
with	O
k	O
means	O
four	O
of	O
the	O
big	O
cluster	O
s	O
data	O
points	O
have	O
been	O
assigned	O
to	O
the	O
small	O
cluster	O
and	O
both	O
means	O
end	O
up	O
displaced	O
to	O
the	O
left	O
of	O
the	O
true	O
centres	O
of	O
the	O
clusters	O
the	O
k-means	O
algorithm	O
takes	O
account	O
only	O
of	O
the	O
distance	B
between	O
the	O
means	O
and	O
the	O
data	O
points	O
it	O
has	O
no	O
representation	O
of	O
the	O
weight	O
or	O
breadth	O
of	O
each	O
cluster	O
consequently	O
data	O
points	O
that	O
actually	O
belong	O
to	O
the	O
broad	O
cluster	O
are	O
incorrectly	O
assigned	O
to	O
the	O
narrow	O
cluster	O
figure	O
shows	O
another	O
case	O
of	O
k-means	O
behaving	O
badly	O
the	O
data	O
evidently	O
fall	O
into	O
two	O
elongated	O
clusters	O
but	O
the	O
only	O
stable	O
state	O
of	O
the	O
k-means	O
algorithm	O
is	O
that	O
shown	O
in	O
the	O
two	O
clusters	O
have	O
been	O
sliced	O
in	O
half	O
these	O
two	O
examples	O
show	O
that	O
there	O
is	O
something	O
wrong	O
with	O
the	O
distance	B
d	O
in	O
the	O
k-means	O
algorithm	O
the	O
k-means	O
algorithm	O
has	O
no	O
way	O
of	O
representing	O
the	O
size	O
or	O
shape	O
of	O
a	O
cluster	O
a	O
criticism	O
of	O
k-means	O
is	O
that	O
it	O
is	O
a	O
hard	O
rather	O
than	O
a	O
soft	B
algorithm	O
points	O
are	O
assigned	O
to	O
exactly	O
one	O
cluster	O
and	O
all	O
points	O
assigned	O
to	O
a	O
cluster	O
are	O
equals	O
in	O
that	O
cluster	O
points	O
located	O
near	O
the	O
border	O
between	O
two	O
or	O
more	O
clusters	O
should	O
arguably	O
play	O
a	O
partial	B
role	O
in	O
determining	O
the	O
locations	O
of	O
all	O
the	O
clusters	O
that	O
they	O
could	O
plausibly	O
be	O
assigned	O
to	O
but	O
in	O
the	O
k-means	O
algorithm	O
each	O
borderline	O
point	O
is	O
dumped	O
in	O
one	O
cluster	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
soft	B
k-means	B
clustering	B
has	O
an	O
equal	O
vote	O
with	O
all	O
the	O
other	O
points	O
in	O
that	O
cluster	O
and	O
no	O
vote	O
in	O
any	O
other	O
clusters	O
soft	B
k-means	B
clustering	B
these	O
criticisms	B
of	O
k-means	O
motivate	O
the	O
soft	B
k-means	O
algorithm	O
algorithm	O
the	O
algorithm	O
has	O
one	O
parameter	O
which	O
we	O
could	O
term	O
the	O
assignment	O
step	O
each	O
data	O
point	O
xn	O
is	O
given	O
a	O
soft	B
degree	B
of	O
assignment	O
to	O
each	O
of	O
the	O
means	O
we	O
call	O
the	O
degree	B
to	O
which	O
xn	O
is	O
assigned	O
to	O
cluster	O
k	O
the	O
responsibility	B
rn	O
responsibility	B
of	O
cluster	O
k	O
for	O
point	O
n	O
k	O
algorithm	O
soft	B
k-means	O
algorithm	O
version	O
rn	O
k	O
dmk	O
the	O
sum	O
of	O
the	O
k	O
responsibilities	O
for	O
the	O
nth	O
point	O
is	O
update	O
step	O
the	O
model	B
parameters	B
the	O
means	O
are	O
adjusted	O
to	O
match	O
the	O
sample	B
means	O
of	O
the	O
data	O
points	O
that	O
they	O
are	O
responsible	O
for	O
mk	O
xn	O
rn	O
k	O
xn	O
rk	O
where	O
rk	O
is	O
the	O
total	O
responsibility	B
of	O
mean	B
k	O
rk	O
rn	O
k	O
notice	O
the	O
similarity	O
of	O
this	O
soft	B
k-means	O
algorithm	O
to	O
the	O
hard	O
k-means	O
algorithm	O
the	O
update	O
step	O
is	O
identical	O
the	O
only	O
is	O
that	O
the	O
responsibilities	O
rn	O
can	O
take	O
on	O
values	O
between	O
and	O
whereas	O
the	O
assignment	O
in	O
the	O
k-means	O
algorithm	O
involved	O
a	O
min	O
over	O
the	O
distances	O
the	O
rule	O
for	O
assigning	B
the	O
responsibilities	O
is	O
a	O
soft-min	O
k	O
exercise	O
show	O
that	O
as	O
the	O
goes	O
to	O
the	O
soft	B
k-means	O
algo	O
rithm	O
becomes	O
identical	O
to	O
the	O
original	O
hard	O
k-means	O
algorithm	O
except	O
for	O
the	O
way	O
in	O
which	O
means	O
with	O
no	O
assigned	O
points	O
behave	O
describe	O
what	O
those	O
means	O
do	O
instead	O
of	O
sitting	O
still	O
dimensionally	O
the	O
is	O
an	O
inverse-length-squared	O
so	O
we	O
can	O
as	O
sociate	O
a	O
lengthscale	O
with	O
it	O
the	O
soft	B
k-means	O
algorithm	O
is	O
demonstrated	O
in	O
the	O
lengthscale	O
is	O
shown	O
by	O
the	O
radius	O
of	O
the	O
circles	O
surrounding	O
the	O
four	O
means	O
each	O
panel	O
shows	O
the	O
point	O
reached	O
for	O
a	O
value	O
of	O
the	O
lengthscale	O
conclusion	O
at	O
this	O
point	O
we	O
may	O
have	O
some	O
of	O
the	O
problems	O
with	O
the	O
original	O
kmeans	O
algorithm	O
by	O
introducing	O
an	O
extra	O
complexity-control	O
parameter	O
but	O
how	O
should	O
we	O
set	B
and	O
what	O
about	O
the	O
problem	O
of	O
the	O
elongated	O
clusters	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
large	O
an	O
example	O
inference	B
task	O
clustering	B
figure	O
soft	B
k-means	O
algorithm	O
version	O
applied	O
to	O
a	O
data	B
set	B
of	O
points	O
k	O
implicit	O
lengthscale	O
parameter	O
varied	O
from	O
a	O
large	O
to	O
a	O
small	O
value	O
each	O
picture	O
shows	O
the	O
state	O
of	O
all	O
four	O
means	O
with	O
the	O
implicit	O
lengthscale	O
shown	O
by	O
the	O
radius	O
of	O
the	O
four	O
circles	O
after	O
running	O
the	O
algorithm	O
for	O
several	O
tens	O
of	O
iterations	O
at	O
the	O
largest	O
lengthscale	O
all	O
four	O
means	O
converge	O
exactly	O
to	O
the	O
data	O
mean	B
then	O
the	O
four	O
means	O
separate	O
into	O
two	O
groups	O
of	O
two	O
at	O
shorter	O
lengthscales	O
each	O
of	O
these	O
pairs	O
itself	O
bifurcates	O
into	O
subgroups	O
small	O
and	O
the	O
clusters	O
of	O
unequal	O
weight	O
and	O
width	O
adding	O
one	O
parameter	O
is	O
not	O
going	O
to	O
make	O
all	O
these	O
problems	O
go	O
away	O
we	O
ll	O
come	O
back	O
to	O
these	O
questions	O
in	O
a	O
later	O
chapter	O
as	O
we	O
develop	O
the	O
mixture-density-modelling	O
view	O
of	O
clustering	B
further	O
reading	O
for	O
a	O
vector-quantization	O
approach	O
to	O
clustering	B
see	O
luttrell	O
exercises	O
exercise	O
explore	B
the	O
properties	O
of	O
the	O
soft	B
k-means	O
algorithm	O
version	O
assuming	O
that	O
the	O
datapoints	O
fxg	O
come	O
from	O
a	O
single	O
separable	O
two-dimensional	B
gaussian	B
distribution	B
with	O
mean	B
zero	O
and	O
variances	O
set	B
k	O
assume	O
n	O
is	O
large	O
and	O
investigate	O
the	O
points	O
of	O
the	O
algorithm	O
as	O
is	O
varied	O
assume	O
that	O
and	O
with	O
exercise	O
consider	O
the	O
soft	B
k-means	O
algorithm	O
applied	O
to	O
a	O
large	O
amount	O
of	O
one-dimensional	O
data	O
that	O
comes	O
from	O
a	O
mixture	O
of	O
two	O
equalweight	O
gaussians	O
with	O
true	O
means	O
and	O
standard	B
deviation	I
for	O
example	O
show	O
that	O
the	O
hard	O
k-means	O
algorithm	O
with	O
k	O
leads	O
to	O
a	O
solution	O
in	O
which	O
the	O
two	O
means	O
are	O
further	O
apart	O
than	O
the	O
two	O
true	O
means	O
discuss	O
what	O
happens	O
for	O
other	O
values	O
of	O
and	O
the	O
value	O
of	O
such	O
that	O
the	O
soft	B
algorithm	O
puts	O
the	O
two	O
means	O
in	O
the	O
correct	O
places	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solutions	O
solution	O
to	O
exercise	O
we	O
can	O
associate	O
an	O
energy	B
with	O
the	O
state	O
of	O
the	O
k-means	O
algorithm	O
by	O
connecting	O
a	O
spring	B
between	O
each	O
point	O
xn	O
and	O
the	O
mean	B
that	O
is	O
responsible	O
for	O
it	O
the	O
energy	B
of	O
one	O
spring	B
is	O
proportional	O
to	O
its	O
squared	O
length	O
namely	O
mk	O
where	O
is	O
the	O
of	O
the	O
spring	B
the	O
total	O
energy	B
of	O
all	O
the	O
springs	O
is	O
a	O
lyapunov	B
function	I
for	O
the	O
algorithm	O
because	O
the	O
assignment	O
step	O
can	O
only	O
decrease	O
the	O
energy	B
a	O
point	O
only	O
changes	O
its	O
allegiance	O
if	O
the	O
length	O
of	O
its	O
spring	B
would	O
be	O
reduced	O
the	O
update	O
step	O
can	O
only	O
decrease	O
the	O
energy	B
moving	O
mk	O
to	O
the	O
mean	B
is	O
the	O
way	O
to	O
minimize	O
the	O
energy	B
of	O
its	O
springs	O
and	O
the	O
energy	B
is	O
bounded	O
below	O
which	O
is	O
the	O
second	O
condition	O
for	O
a	O
lyapunov	B
function	I
since	O
the	O
algorithm	O
has	O
a	O
lyapunov	B
function	I
it	O
converges	O
if	O
the	O
means	O
are	O
initialized	O
to	O
solution	O
to	O
exercise	O
and	O
the	O
assignment	O
step	O
for	O
a	O
point	O
at	O
location	O
gives	O
and	O
the	O
updated	O
m	O
is	O
r	O
p	O
r	O
p	O
p	O
figure	O
schematic	O
diagram	O
of	O
the	O
bifurcation	B
as	O
the	O
largest	O
data	O
variance	B
increases	O
from	O
below	O
to	O
above	O
the	O
data	O
variance	B
is	O
indicated	O
by	O
the	O
ellipse	O
now	O
m	O
is	O
a	O
point	O
but	O
the	O
question	O
is	O
is	O
it	O
stable	O
or	O
unstable	O
for	O
tiny	O
m	O
is	O
we	O
can	O
taylor-expand	O
so	O
z	O
p	O
for	O
small	O
m	O
m	O
either	O
grows	O
or	O
decays	O
exponentially	O
under	O
this	O
mapping	B
depending	O
on	O
whether	O
is	O
greater	O
than	O
or	O
less	O
than	O
the	O
point	O
m	O
is	O
stable	O
if	O
if	O
and	O
unstable	O
otherwise	O
this	O
derivation	B
shows	O
that	O
this	O
result	O
is	O
general	O
holding	O
for	O
any	O
true	O
probability	B
distribution	B
p	O
having	O
variance	B
not	O
just	O
the	O
gaussian	B
then	O
there	O
is	O
a	O
bifurcation	B
and	O
there	O
are	O
two	O
stable	O
points	O
surrounding	O
the	O
unstable	O
point	O
at	O
m	O
to	O
illustrate	O
this	O
bifurcation	B
shows	O
the	O
outcome	O
of	O
running	O
the	O
soft	B
k-means	O
algorithm	O
with	O
on	O
one-dimensional	O
data	O
with	O
standard	B
deviation	I
for	O
various	O
values	O
of	O
figure	O
shows	O
this	O
pitchfork	B
bifurcation	B
from	O
the	O
other	O
point	O
of	O
view	O
where	O
the	O
data	O
s	O
standard	B
deviation	I
is	O
and	O
the	O
algorithm	O
s	O
lengthscale	O
is	O
varied	O
on	O
the	O
horizontal	O
axis	O
data	O
density	B
mean	B
locations	O
figure	O
the	O
stable	O
mean	B
locations	O
as	O
a	O
function	O
of	O
for	O
constant	O
found	O
numerically	O
lines	O
and	O
the	O
approximation	B
lines	O
data	O
density	B
mean	B
locns	O
figure	O
the	O
stable	O
mean	B
locations	O
as	O
a	O
function	O
of	O
for	O
constant	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
an	O
example	O
inference	B
task	O
clustering	B
here	O
is	O
a	O
cheap	O
theory	O
to	O
model	B
how	O
the	O
parameters	B
behave	O
beyond	O
the	O
bifurcation	B
based	O
on	O
continuing	O
the	O
series	O
expansion	O
this	O
continuation	O
of	O
the	O
series	O
is	O
rather	O
suspect	O
since	O
the	O
series	O
isn	O
t	O
necessarily	O
expected	O
to	O
converge	O
beyond	O
the	O
bifurcation	B
point	O
but	O
the	O
theory	O
well	O
anyway	O
we	O
take	O
our	O
analytic	O
approach	O
one	O
term	O
further	O
in	O
the	O
expansion	O
then	O
we	O
can	O
solve	O
for	O
the	O
shape	O
of	O
the	O
bifurcation	B
to	O
leading	O
order	O
which	O
depends	O
on	O
the	O
fourth	O
moment	O
of	O
the	O
distribution	B
z	O
p	O
we	O
use	O
the	O
fact	O
that	O
p	O
is	O
gaussian	B
to	O
the	O
fourth	O
moment	O
this	O
map	O
has	O
a	O
point	O
at	O
m	O
such	O
that	O
i	O
e	O
m	O
the	O
thin	O
line	O
in	O
shows	O
this	O
theoretical	O
approximation	B
figure	O
shows	O
the	O
bifurcation	B
as	O
a	O
function	O
of	O
for	O
shows	O
the	O
bifurcation	B
as	O
a	O
function	O
of	O
for	O
exercise	O
why	O
does	O
the	O
pitchfork	O
in	O
tend	O
to	O
the	O
values	O
as	O
give	O
an	O
analytic	O
expression	O
for	O
this	O
asymptote	O
solution	O
to	O
exercise	O
the	O
asymptote	O
is	O
the	O
mean	B
of	O
the	O
gaussian	B
r	O
normalx	O
dx	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
inference	B
by	O
complete	O
enumeration	O
we	O
open	O
our	O
toolbox	O
of	O
methods	O
for	O
handling	O
probabilities	O
by	O
discussing	O
a	O
brute-force	O
inference	B
method	O
complete	O
enumeration	O
of	O
all	O
hypotheses	O
and	O
evaluation	O
of	O
their	O
probabilities	O
this	O
approach	O
is	O
an	O
exact	O
method	O
and	O
the	O
of	O
carrying	O
it	O
out	O
will	O
motivate	O
the	O
smarter	O
exact	O
and	O
approximate	O
methods	O
introduced	O
in	O
the	O
following	O
chapters	O
the	O
burglar	O
alarm	O
bayesian	B
probability	B
theory	O
is	O
sometimes	O
called	O
common	O
sense	O
when	O
thinking	O
about	O
the	O
following	O
questions	O
please	O
ask	O
your	O
common	O
sense	O
what	O
it	O
thinks	O
the	O
answers	O
are	O
we	O
will	O
then	O
see	O
how	O
bayesian	B
methods	O
your	O
everyday	O
intuition	O
example	O
fred	O
lives	O
in	O
los	O
angeles	O
and	O
commutes	O
miles	O
to	O
work	O
whilst	O
at	O
work	O
he	O
receives	O
a	O
phone-call	O
from	O
his	O
neighbour	O
saying	O
that	O
fred	O
s	O
burglar	O
alarm	O
is	O
ringing	O
what	O
is	O
the	O
probability	B
that	O
there	O
was	O
a	O
burglar	O
in	O
his	O
house	O
today	O
while	O
driving	O
home	O
to	O
investigate	O
fred	O
hears	O
on	O
the	O
radio	B
that	O
there	O
was	O
a	O
small	O
earthquake	B
that	O
day	O
near	O
his	O
home	O
oh	O
he	O
says	O
feeling	O
relieved	O
it	O
was	O
probably	O
the	O
earthquake	B
that	O
set	B
the	O
alarm	O
what	O
is	O
the	O
probability	B
that	O
there	O
was	O
a	O
burglar	O
in	O
his	O
house	O
pearl	O
let	O
s	O
introduce	O
variables	O
b	O
burglar	O
was	O
present	O
in	O
fred	O
s	O
house	O
today	O
a	O
alarm	O
is	O
ringing	O
p	O
receives	O
a	O
phonecall	O
from	O
the	O
neighbour	O
reporting	O
the	O
alarm	O
e	O
small	O
earthquake	B
took	O
place	O
today	O
near	O
fred	O
s	O
house	O
and	O
r	O
radio	B
report	O
of	O
earthquake	B
is	O
heard	O
by	O
fred	O
the	O
probability	B
of	O
all	O
these	O
variables	O
might	O
factorize	O
as	O
follows	O
p	O
e	O
a	O
p	O
r	O
p	O
b	O
ep	O
ap	O
j	O
e	O
and	O
plausible	O
values	O
for	O
the	O
probabilities	O
are	O
burglar	O
probability	B
p	O
p	O
e	O
g	O
gives	O
a	O
mean	B
burglary	O
rate	B
of	O
once	O
every	O
three	O
years	O
earthquake	B
probability	B
p	O
p	O
jearthquake	O
jburglar	O
jphonecall	O
figure	O
belief	B
network	B
for	O
the	O
burglar	O
alarm	O
problem	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
inference	B
by	O
complete	O
enumeration	O
with	O
e	O
g	O
our	O
assertion	O
that	O
the	O
earthquakes	O
are	O
independent	O
of	O
burglars	O
i	O
e	O
the	O
prior	B
probability	B
of	O
b	O
and	O
e	O
is	O
p	O
e	O
p	O
seems	O
reasonable	O
unless	O
we	O
take	O
into	O
account	O
opportunistic	O
burglars	O
who	O
strike	O
immediately	O
after	O
earthquakes	O
alarm	O
ringing	O
probability	B
we	O
assume	O
the	O
alarm	O
will	O
ring	O
if	O
any	O
of	O
the	O
following	O
three	O
events	O
happens	O
a	O
burglar	O
enters	O
the	O
house	O
and	O
triggers	O
the	O
alarm	O
s	O
assume	O
the	O
alarm	O
has	O
a	O
reliability	O
of	O
i	O
e	O
of	O
burglars	O
trigger	O
the	O
alarm	O
an	O
earthquake	B
takes	O
place	O
and	O
triggers	O
the	O
alarm	O
of	O
alarms	O
are	O
triggered	O
by	O
earthquakes	O
or	O
some	O
other	O
event	O
causes	O
a	O
false	O
alarm	O
let	O
s	O
assume	O
the	O
false	O
alarm	O
rate	B
f	O
is	O
so	O
fred	O
has	O
false	O
alarms	O
from	O
non-earthquake	O
causes	O
once	O
every	O
three	O
years	O
type	O
of	O
dependence	O
of	O
a	O
on	O
b	O
and	O
e	O
is	O
known	O
as	O
a	O
noisy-or	B
the	O
probabilities	O
of	O
a	O
given	O
b	O
and	O
e	O
are	O
then	O
p	O
b	O
e	O
f	O
p	O
b	O
e	O
f	O
p	O
b	O
e	O
f	O
p	O
b	O
e	O
f	O
p	O
b	O
e	O
f	O
p	O
b	O
e	O
f	O
p	O
b	O
e	O
f	O
p	O
b	O
e	O
f	O
or	O
in	O
numbers	O
p	O
b	O
e	O
p	O
b	O
e	O
p	O
b	O
e	O
p	O
b	O
e	O
p	O
b	O
e	O
p	O
b	O
e	O
p	O
b	O
e	O
p	O
b	O
e	O
we	O
assume	O
the	O
neighbour	O
would	O
never	O
phone	B
if	O
the	O
alarm	O
is	O
not	O
ringing	O
a	O
and	O
that	O
the	O
radio	B
is	O
a	O
trustworthy	O
reporter	O
too	O
e	O
we	O
won	O
t	O
need	O
to	O
specify	O
the	O
probabilities	O
p	O
a	O
or	O
p	O
e	O
in	O
order	O
to	O
answer	O
the	O
questions	O
above	O
since	O
the	O
outcomes	O
p	O
and	O
r	O
give	O
us	O
certainty	O
respectively	O
that	O
a	O
and	O
e	O
we	O
can	O
answer	O
the	O
two	O
questions	O
about	O
the	O
burglar	O
by	O
computing	O
the	O
posterior	O
probabilities	O
of	O
all	O
hypotheses	O
given	O
the	O
available	O
information	B
let	O
s	O
start	O
by	O
reminding	O
ourselves	O
that	O
the	O
probability	B
that	O
there	O
is	O
a	O
burglar	O
before	O
either	O
p	O
or	O
r	O
is	O
observed	O
is	O
p	O
and	O
the	O
probability	B
that	O
an	O
earthquake	B
took	O
place	O
is	O
p	O
and	O
these	O
two	O
propositions	O
are	O
independent	O
first	O
when	O
p	O
we	O
know	O
that	O
the	O
alarm	O
is	O
ringing	O
a	O
the	O
posterior	B
probability	B
of	O
b	O
and	O
e	O
becomes	O
p	O
ej	O
a	O
p	O
b	O
ep	O
p	O
the	O
numerator	O
s	O
four	O
possible	O
values	O
are	O
p	O
b	O
e	O
p	O
p	O
p	O
b	O
e	O
p	O
p	O
p	O
b	O
e	O
p	O
p	O
p	O
b	O
e	O
p	O
p	O
the	O
normalizing	O
constant	O
is	O
the	O
sum	O
of	O
these	O
four	O
numbers	O
p	O
and	O
the	O
posterior	O
probabilities	O
are	O
p	O
e	O
a	O
p	O
e	O
a	O
p	O
e	O
a	O
p	O
e	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
inference	B
for	O
continuous	B
hypothesis	O
spaces	O
to	O
answer	O
the	O
question	O
what	O
s	O
the	O
probability	B
a	O
burglar	O
was	O
there	O
we	O
marginalize	O
over	O
the	O
earthquake	B
variable	O
e	O
p	O
a	O
p	O
e	O
a	O
p	O
e	O
a	O
p	O
a	O
p	O
e	O
a	O
p	O
e	O
a	O
so	O
there	O
is	O
nearly	O
a	O
chance	O
that	O
there	O
was	O
a	O
burglar	O
present	O
it	O
is	O
important	O
to	O
note	O
that	O
the	O
variables	O
b	O
and	O
e	O
which	O
were	O
independent	O
a	O
priori	O
are	O
now	O
dependent	O
the	O
posterior	O
distribution	B
is	O
not	O
a	O
separable	O
function	O
of	O
b	O
and	O
e	O
this	O
fact	O
is	O
illustrated	O
most	O
simply	O
by	O
studying	O
the	O
of	O
learning	B
that	O
e	O
when	O
we	O
learn	O
e	O
the	O
posterior	B
probability	B
of	O
b	O
is	O
given	O
by	O
p	O
e	O
a	O
p	O
e	O
a	O
a	O
i	O
e	O
by	O
dividing	O
the	O
bottom	O
two	O
rows	O
of	O
by	O
their	O
sum	O
p	O
a	O
the	O
posterior	B
probability	B
of	O
b	O
is	O
p	O
e	O
a	O
p	O
e	O
a	O
there	O
is	O
thus	O
now	O
an	O
chance	O
that	O
a	O
burglar	O
was	O
in	O
fred	O
s	O
house	O
it	O
is	O
in	O
accordance	O
with	O
everyday	O
intuition	O
that	O
the	O
probability	B
that	O
b	O
possible	O
cause	O
of	O
the	O
alarm	O
reduces	O
when	O
fred	O
learns	O
that	O
an	O
earthquake	B
an	O
alternative	O
explanation	O
of	O
the	O
alarm	O
has	O
happened	O
explaining	B
away	I
this	O
phenomenon	O
that	O
one	O
of	O
the	O
possible	O
causes	O
of	O
some	O
data	O
data	O
in	O
this	O
case	O
being	O
a	O
becomes	O
less	O
probable	O
when	O
another	O
of	O
the	O
causes	O
becomes	O
more	O
probable	O
even	O
though	O
those	O
two	O
causes	O
were	O
independent	O
variables	O
a	O
priori	O
is	O
known	O
as	O
explaining	B
away	I
explaining	B
away	I
is	O
an	O
important	O
feature	O
of	O
correct	O
inferences	O
and	O
one	O
that	O
any	O
intelligence	O
should	O
replicate	O
if	O
we	O
believe	O
that	O
the	O
neighbour	O
and	O
the	O
radio	B
service	O
are	O
unreliable	O
or	O
capricious	O
so	O
that	O
we	O
are	O
not	O
certain	O
that	O
the	O
alarm	O
really	O
is	O
ringing	O
or	O
that	O
an	O
earthquake	B
really	O
has	O
happened	O
the	O
calculations	O
become	O
more	O
complex	B
but	O
the	O
explaining-away	O
persists	O
the	O
arrival	O
of	O
the	O
earthquake	B
report	O
r	O
simultaneously	O
makes	O
it	O
more	O
probable	O
that	O
the	O
alarm	O
truly	O
is	O
ringing	O
and	O
less	O
probable	O
that	O
the	O
burglar	O
was	O
present	O
in	O
summary	B
we	O
solved	O
the	O
inference	B
questions	O
about	O
the	O
burglar	O
by	O
enumerating	O
all	O
four	O
hypotheses	O
about	O
the	O
variables	O
e	O
their	O
posterior	O
probabilities	O
and	O
marginalizing	O
to	O
obtain	O
the	O
required	O
inferences	O
about	O
b	O
exercise	O
after	O
fred	O
receives	O
the	O
phone-call	O
about	O
the	O
burglar	O
alarm	O
but	O
before	O
he	O
hears	O
the	O
radio	B
report	O
what	O
from	O
his	O
point	O
of	O
view	O
is	O
the	O
probability	B
that	O
there	O
was	O
a	O
small	O
earthquake	B
today	O
exact	O
inference	B
for	O
continuous	B
hypothesis	O
spaces	O
many	O
of	O
the	O
hypothesis	O
spaces	O
we	O
will	O
consider	O
are	O
naturally	O
thought	O
of	O
as	O
continuous	B
for	O
example	O
the	O
unknown	O
decay	O
length	O
of	O
section	B
lives	O
in	O
a	O
continuous	B
one-dimensional	O
space	O
and	O
the	O
unknown	O
mean	B
and	O
standard	B
deviation	I
of	O
a	O
gaussian	B
live	O
in	O
a	O
continuous	B
two-dimensional	B
space	O
in	O
any	O
practical	B
computer	B
implementation	O
such	O
continuous	B
spaces	O
will	O
necessarily	O
be	O
discretized	O
however	O
and	O
so	O
can	O
in	O
principle	O
be	O
enumerated	O
at	O
a	O
grid	O
of	O
parameter	O
values	O
for	O
example	O
in	O
we	O
plotted	O
the	O
likelihood	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
inference	B
by	O
complete	O
enumeration	O
figure	O
enumeration	O
of	O
an	O
entire	O
hypothesis	O
space	O
for	O
one	O
gaussian	B
with	O
parameters	B
axis	O
and	O
function	O
for	O
the	O
decay	O
length	O
as	O
a	O
function	O
of	O
by	O
evaluating	O
the	O
likelihood	B
at	O
a	O
series	O
of	O
points	O
a	O
two-parameter	O
model	B
let	O
s	O
look	O
at	O
the	O
gaussian	B
distribution	B
as	O
an	O
example	O
of	O
a	O
model	B
with	O
a	O
twodimensional	O
hypothesis	O
space	O
the	O
one-dimensional	O
gaussian	B
distribution	B
is	O
parameterized	O
by	O
a	O
mean	B
and	O
a	O
standard	B
deviation	I
p	O
normalx	O
figure	O
shows	O
an	O
enumeration	O
of	O
one	O
hundred	O
hypotheses	O
about	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
a	O
one-dimensional	O
gaussian	B
distribution	B
these	O
hypotheses	O
are	O
evenly	O
spaced	O
in	O
a	O
ten	O
by	O
ten	O
square	B
grid	O
covering	O
ten	O
values	O
of	O
and	O
ten	O
values	O
of	O
each	O
hypothesis	O
is	O
represented	O
by	O
a	O
picture	O
showing	O
the	O
probability	B
density	B
that	O
it	O
puts	O
on	O
x	O
we	O
now	O
examine	O
the	O
inference	B
of	O
and	O
given	O
data	O
points	O
xn	O
n	O
n	O
assumed	O
to	O
be	O
drawn	O
independently	O
from	O
this	O
density	B
imagine	O
that	O
we	O
acquire	O
data	O
for	O
example	O
the	O
points	O
shown	O
in	O
we	O
can	O
now	O
evaluate	O
the	O
posterior	B
probability	B
of	O
each	O
of	O
the	O
one	O
hundred	O
subhypotheses	O
by	O
evaluating	O
the	O
likelihood	B
of	O
each	O
that	O
is	O
the	O
value	O
of	O
p	O
j	O
the	O
likelihood	B
values	O
are	O
shown	O
diagrammatically	O
in	O
using	O
the	O
line	O
thickness	O
to	O
encode	O
the	O
value	O
of	O
the	O
likelihood	B
subhypotheses	O
with	O
likelihood	B
smaller	O
than	O
times	O
the	O
maximum	B
likelihood	B
have	O
been	O
deleted	O
using	O
a	O
grid	O
we	O
can	O
represent	O
the	O
same	O
information	B
by	O
plotting	O
the	O
likelihood	B
as	O
a	O
surface	O
plot	O
or	O
contour	O
plot	O
as	O
a	O
function	O
of	O
and	O
a	O
mixture	O
model	B
eyeballing	O
the	O
data	O
you	O
might	O
agree	O
that	O
it	O
seems	O
more	O
plausible	O
that	O
they	O
come	O
not	O
from	O
a	O
single	O
gaussian	B
but	O
from	O
a	O
mixture	O
of	O
two	O
gaussians	O
by	O
two	O
means	O
two	O
standard	O
deviations	O
and	O
two	O
mixing	O
the	O
horizontal	O
figure	O
five	O
datapoints	O
coordinate	O
is	O
the	O
value	O
of	O
the	O
datum	O
xn	O
the	O
vertical	O
coordinate	O
has	O
no	O
meaning	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
inference	B
for	O
continuous	B
hypothesis	O
spaces	O
figure	O
likelihood	B
function	O
given	O
the	O
data	O
of	O
represented	O
by	O
line	O
thickness	O
subhypotheses	O
having	O
likelihood	B
smaller	O
than	O
times	O
the	O
maximum	B
likelihood	B
are	O
not	O
shown	O
sigma	O
mean	B
mean	B
sigma	O
figure	O
the	O
likelihood	B
function	O
for	O
the	O
parameters	B
of	O
a	O
gaussian	B
distribution	B
surface	O
plot	O
and	O
contour	O
plot	O
of	O
the	O
log	O
likelihood	B
as	O
a	O
function	O
of	O
and	O
the	O
data	B
set	B
of	O
n	O
points	O
had	O
mean	B
and	O
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
inference	B
by	O
complete	O
enumeration	O
figure	O
enumeration	O
of	O
the	O
entire	O
hypothesis	O
space	O
for	O
a	O
mixture	O
of	O
two	O
gaussians	O
weight	O
of	O
the	O
mixture	O
components	O
is	O
in	O
the	O
top	O
half	O
and	O
in	O
the	O
bottom	O
half	O
means	O
and	O
vary	O
horizontally	O
and	O
standard	O
deviations	O
and	O
vary	O
vertically	O
and	O
satisfying	O
p	O
let	O
s	O
enumerate	O
the	O
subhypotheses	O
for	O
this	O
alternative	O
model	B
the	O
parameter	O
space	O
is	O
so	O
it	O
becomes	O
challenging	O
to	O
represent	O
it	O
on	O
a	O
single	O
page	O
figure	O
enumerates	O
subhypotheses	O
with	O
values	O
of	O
the	O
parameters	B
the	O
means	O
are	O
varied	O
between	O
values	O
each	O
in	O
the	O
horizontal	O
directions	O
the	O
standard	O
deviations	O
take	O
on	O
four	O
values	O
each	O
vertically	O
and	O
takes	O
on	O
two	O
values	O
vertically	O
we	O
can	O
represent	O
the	O
inference	B
about	O
these	O
parameters	B
in	O
the	O
light	O
of	O
the	O
datapoints	O
as	O
shown	O
in	O
if	O
we	O
wish	O
to	O
compare	O
the	O
one-gaussian	O
model	B
with	O
the	O
mixture-of-two	O
model	B
we	O
can	O
the	O
models	O
posterior	O
probabilities	O
by	O
evaluating	O
the	O
marginal	B
likelihood	B
or	O
evidence	B
for	O
each	O
model	B
h	O
p	O
the	O
evidence	B
is	O
given	O
by	O
integrating	O
over	O
the	O
parameters	B
the	O
integration	O
can	O
be	O
implemented	O
numerically	O
by	O
summing	O
over	O
the	O
alternative	O
enumerated	O
values	O
of	O
p	O
p	O
where	O
p	O
is	O
the	O
prior	B
distribution	B
over	O
the	O
grid	O
of	O
parameter	O
values	O
which	O
i	O
take	O
to	O
be	O
uniform	O
for	O
the	O
mixture	O
of	O
two	O
gaussians	O
this	O
integral	B
is	O
a	O
integral	B
if	O
it	O
is	O
to	O
be	O
performed	O
at	O
all	O
accurately	O
the	O
grid	O
of	O
points	O
will	O
need	O
to	O
be	O
much	O
than	O
the	O
grids	O
shown	O
in	O
the	O
if	O
the	O
uncertainty	O
about	O
each	O
of	O
k	O
parameters	B
has	O
been	O
reduced	O
by	O
say	O
a	O
factor	O
of	O
ten	O
by	O
observing	O
the	O
data	O
then	O
brute-force	O
integration	O
requires	O
a	O
grid	O
of	O
at	O
least	O
points	O
this	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
inference	B
for	O
continuous	B
hypothesis	O
spaces	O
figure	O
inferring	O
a	O
mixture	O
of	O
two	O
gaussians	O
likelihood	B
function	O
given	O
the	O
data	O
of	O
represented	O
by	O
line	O
thickness	O
the	O
hypothesis	O
space	O
is	O
identical	O
to	O
that	O
shown	O
in	O
subhypotheses	O
having	O
likelihood	B
smaller	O
than	O
times	O
the	O
maximum	B
likelihood	B
are	O
not	O
shown	O
hence	O
the	O
blank	O
regions	O
which	O
correspond	O
to	O
hypotheses	O
that	O
the	O
data	O
have	O
ruled	O
out	O
exponential	B
growth	O
of	O
computation	O
with	O
model	B
size	O
is	O
the	O
reason	O
why	O
complete	O
enumeration	O
is	O
rarely	O
a	O
feasible	O
computational	O
strategy	O
exercise	O
imagine	O
a	O
mixture	O
of	O
ten	O
gaussians	O
to	O
data	O
in	O
a	O
twenty-dimensional	O
space	O
estimate	O
the	O
computational	O
cost	O
of	O
implementing	O
inferences	O
for	O
this	O
model	B
by	O
enumeration	O
of	O
a	O
grid	O
of	O
parameter	O
values	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
maximum	B
likelihood	B
and	O
clustering	B
rather	O
than	O
enumerate	O
all	O
hypotheses	O
which	O
may	O
be	O
exponential	B
in	O
number	O
we	O
can	O
save	O
a	O
lot	O
of	O
time	O
by	O
homing	O
in	O
on	O
one	O
good	B
hypothesis	O
that	O
the	O
data	O
well	O
this	O
is	O
the	O
philosophy	B
behind	O
the	O
maximum	B
likelihood	B
method	O
which	O
the	O
setting	O
of	O
the	O
parameter	O
vector	O
that	O
maximizes	O
the	O
likelihood	B
p	O
j	O
for	O
some	O
models	O
the	O
maximum	B
likelihood	B
parameters	B
can	O
be	O
instantly	O
from	O
the	O
data	O
for	O
more	O
complex	B
models	O
the	O
maximum	B
likelihood	B
parameters	B
may	O
require	O
an	O
iterative	O
algorithm	O
for	O
any	O
model	B
it	O
is	O
usually	O
easiest	O
to	O
work	O
with	O
the	O
logarithm	O
of	O
the	O
likelihood	B
rather	O
than	O
the	O
likelihood	B
since	O
likelihoods	O
being	O
products	O
of	O
the	O
probabilities	O
of	O
many	O
data	O
points	O
tend	O
to	O
be	O
very	O
small	O
likelihoods	O
multiply	O
log	O
likelihoods	O
add	O
maximum	B
likelihood	B
for	O
one	O
gaussian	B
we	O
return	O
to	O
the	O
gaussian	B
for	O
our	O
examples	O
assume	O
we	O
have	O
data	O
fxngn	O
the	O
log	O
likelihood	B
is	O
ln	O
p	O
j	O
the	O
likelihood	B
can	O
be	O
expressed	O
in	O
terms	O
of	O
two	O
functions	B
of	O
the	O
data	O
the	O
sample	B
mean	B
and	O
the	O
sum	O
of	O
square	B
deviations	O
n	O
xnn	O
s	O
ln	O
p	O
j	O
because	O
the	O
likelihood	B
depends	O
on	O
the	O
data	O
only	O
through	O
and	O
s	O
these	O
two	O
quantities	O
are	O
known	O
as	O
statistics	O
example	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
and	O
show	O
that	O
if	O
the	O
standard	B
deviation	I
is	O
known	O
to	O
be	O
the	O
maximum	B
likelihood	B
mean	B
of	O
a	O
gaussian	B
is	O
equal	O
to	O
the	O
sample	B
mean	B
for	O
any	O
value	O
of	O
solution	O
ln	O
p	O
n	O
when	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
maximum	B
likelihood	B
for	O
one	O
gaussian	B
figure	O
the	O
likelihood	B
function	O
for	O
the	O
parameters	B
of	O
a	O
gaussian	B
distribution	B
surface	O
plot	O
and	O
contour	O
plot	O
of	O
the	O
log	O
likelihood	B
as	O
a	O
function	O
of	O
and	O
the	O
data	B
set	B
of	O
n	O
points	O
had	O
mean	B
and	O
s	O
the	O
posterior	B
probability	B
of	O
for	O
various	O
values	O
of	O
the	O
posterior	B
probability	B
of	O
for	O
various	O
values	O
of	O
as	O
a	O
density	B
over	O
ln	O
sigma	O
sigma	O
mean	B
mean	B
r	O
o	O
i	O
r	O
e	O
t	O
s	O
o	O
p	O
mean	B
if	O
we	O
taylor-expand	O
the	O
log	O
likelihood	B
about	O
the	O
maximum	O
we	O
can	O
approximate	O
error	B
bars	I
on	O
the	O
maximum	B
likelihood	B
parameter	O
we	O
use	O
a	O
quadratic	O
approximation	B
to	O
estimate	O
how	O
far	O
from	O
the	O
maximum-likelihood	O
parameter	O
setting	O
we	O
can	O
go	O
before	O
the	O
likelihood	B
falls	O
by	O
some	O
standard	O
factor	O
for	O
example	O
or	O
in	O
the	O
special	O
case	O
of	O
a	O
likelihood	B
that	O
is	O
a	O
gaussian	B
function	O
of	O
the	O
parameters	B
the	O
quadratic	O
approximation	B
is	O
exact	O
example	O
find	O
the	O
second	O
derivative	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
and	O
the	O
error	B
bars	I
on	O
given	O
the	O
data	O
and	O
solution	O
ln	O
p	O
n	O
comparing	O
this	O
curvature	O
with	O
the	O
curvature	O
of	O
the	O
log	O
of	O
a	O
gaussian	B
distribution	B
over	O
of	O
standard	B
deviation	I
we	O
can	O
deduce	O
that	O
the	O
error	B
bars	I
on	O
from	O
the	O
likelihood	B
function	O
are	O
which	O
is	O
pn	O
the	O
error	B
bars	I
have	O
this	O
property	O
at	O
the	O
two	O
points	O
the	O
likelihood	B
is	O
smaller	O
than	O
its	O
maximum	O
value	O
by	O
a	O
factor	O
of	O
example	O
find	O
the	O
maximum	B
likelihood	B
standard	B
deviation	I
of	O
a	O
gaussian	B
whose	O
mean	B
is	O
known	O
to	O
be	O
in	O
the	O
light	O
of	O
data	O
fxngn	O
find	O
the	O
second	O
derivative	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
ln	O
and	O
error	B
bars	I
on	O
ln	O
solution	O
the	O
likelihood	B
s	O
dependence	O
on	O
is	O
ln	O
p	O
j	O
stot	O
where	O
stot	O
pnxn	O
to	O
the	O
maximum	O
of	O
the	O
likelihood	B
we	O
can	O
with	O
respect	O
to	O
ln	O
s	O
often	O
most	O
hygienic	O
to	O
with	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
maximum	B
likelihood	B
and	O
clustering	B
respect	O
to	O
ln	O
u	O
rather	O
than	O
u	O
when	O
u	O
is	O
a	O
scale	O
variable	O
we	O
use	O
dundln	O
u	O
nun	O
ln	O
p	O
ln	O
j	O
stot	O
this	O
derivative	O
is	O
zero	O
when	O
stot	O
n	O
i	O
e	O
the	O
second	O
derivative	O
is	O
n	O
ln	O
p	O
j	O
stot	O
and	O
at	O
the	O
maximum-likelihood	O
value	O
of	O
this	O
equals	O
so	O
error	B
bars	I
on	O
ln	O
are	O
exercise	O
show	O
that	O
the	O
values	O
of	O
and	O
ln	O
that	O
jointly	O
maximize	O
the	O
likelihood	B
are	O
where	O
n	O
maximum	B
likelihood	B
for	O
a	O
mixture	B
of	I
gaussians	I
we	O
now	O
derive	O
an	O
algorithm	O
for	O
a	O
mixture	B
of	I
gaussians	I
to	O
onedimensional	O
data	O
in	O
fact	O
this	O
algorithm	O
is	O
so	O
important	O
to	O
understand	O
that	O
you	O
gentle	O
reader	O
get	O
to	O
derive	O
the	O
algorithm	O
please	O
work	O
through	O
the	O
following	O
exercise	O
exercise	O
a	O
random	B
variable	I
x	O
is	O
assumed	O
to	O
have	O
a	O
probability	B
distribution	B
that	O
is	O
a	O
mixture	O
of	O
two	O
gaussians	O
p	O
pk	O
where	O
the	O
two	O
gaussians	O
are	O
given	O
the	O
labels	O
k	O
and	O
k	O
the	O
prior	B
probability	B
of	O
the	O
class	O
label	O
k	O
is	O
are	O
the	O
means	O
of	O
the	O
two	O
gaussians	O
and	O
both	O
have	O
standard	B
deviation	I
for	O
brevity	O
we	O
denote	O
these	O
parameters	B
by	O
a	O
data	B
set	B
consists	O
of	O
n	O
points	O
fxngn	O
which	O
are	O
assumed	O
to	O
be	O
independent	O
samples	O
from	O
this	O
distribution	B
let	O
kn	O
denote	O
the	O
unknown	O
class	O
label	O
of	O
the	O
nth	O
point	O
assuming	O
that	O
and	O
are	O
known	O
show	O
that	O
the	O
posterior	B
probability	B
of	O
the	O
class	O
label	O
kn	O
of	O
the	O
nth	O
point	O
can	O
be	O
written	O
as	O
p	O
xn	O
p	O
xn	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
enhancements	O
to	O
soft	B
k-means	O
and	O
give	O
expressions	O
for	O
and	O
assume	O
now	O
that	O
the	O
means	O
are	O
not	O
known	O
and	O
that	O
we	O
wish	O
to	O
infer	O
them	O
from	O
the	O
data	O
fxngn	O
standard	B
deviation	I
is	O
known	O
in	O
the	O
remainder	O
of	O
this	O
question	O
we	O
will	O
derive	O
an	O
iterative	O
algorithm	O
for	O
values	O
for	O
that	O
maximize	O
the	O
likelihood	B
p	O
p	O
let	O
l	O
denote	O
the	O
natural	B
log	O
of	O
the	O
likelihood	B
show	O
that	O
the	O
derivative	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
is	O
given	O
by	O
l	O
pkjn	O
where	O
pkjn	O
p	O
k	O
j	O
xn	O
appeared	O
above	O
at	O
equation	O
show	O
neglecting	O
terms	O
in	O
is	O
approximately	O
given	O
by	O
p	O
k	O
j	O
xn	O
that	O
the	O
second	O
derivative	O
k	O
l	O
pkjn	O
hence	O
show	O
that	O
from	O
an	O
initial	O
state	O
an	O
approximate	O
newtonraphson	O
step	O
updates	O
these	O
parameters	B
to	O
where	O
pn	O
pkjnxn	O
pn	O
pkjn	O
newtonraphson	O
method	O
for	O
maximizing	O
updates	O
to	O
h	O
assuming	O
that	O
sketch	O
a	O
contour	O
plot	O
of	O
the	O
likelihood	B
function	O
as	O
a	O
function	O
of	O
and	O
for	O
the	O
data	B
set	B
shown	O
above	O
the	O
data	B
set	B
consists	O
of	O
points	O
describe	O
the	O
peaks	O
in	O
your	O
sketch	O
and	O
indicate	O
their	O
widths	O
notice	O
that	O
the	O
algorithm	O
you	O
have	O
derived	O
for	O
maximizing	O
the	O
likelihood	B
is	O
identical	O
to	O
the	O
soft	B
k-means	O
algorithm	O
of	O
section	B
now	O
that	O
it	O
is	O
clear	O
that	O
clustering	B
can	O
be	O
viewed	O
as	O
mixture-density-modelling	O
we	O
are	O
able	O
to	O
derive	O
enhancements	O
to	O
the	O
k-means	O
algorithm	O
which	O
rectify	O
the	O
problems	O
we	O
noted	O
earlier	O
enhancements	O
to	O
soft	B
k-means	O
algorithm	O
shows	O
a	O
version	O
of	O
the	O
soft-k-means	O
algorithm	O
corresponding	O
to	O
a	O
modelling	B
assumption	O
that	O
each	O
cluster	O
is	O
a	O
spherical	O
gaussian	B
having	O
its	O
own	O
width	O
cluster	O
has	O
its	O
own	O
k	O
the	O
algorithm	O
updates	O
the	O
lengthscales	O
for	O
itself	O
the	O
algorithm	O
also	O
includes	O
cluster	O
weight	O
parameters	B
which	O
also	O
update	O
themselves	O
allowing	O
accurate	O
modelling	B
of	O
data	O
from	O
clusters	O
of	O
unequal	O
weights	O
this	O
algorithm	O
is	O
demonstrated	O
in	O
for	O
two	O
data	O
sets	O
that	O
we	O
ve	O
seen	O
before	O
the	O
second	O
example	O
shows	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
maximum	B
likelihood	B
and	O
clustering	B
assignment	O
step	O
the	O
responsibilities	O
are	O
algorithm	O
the	O
soft	B
k-means	O
algorithm	O
version	O
rn	O
k	O
k	O
dmk	O
where	O
i	O
is	O
the	O
dimensionality	O
of	O
x	O
update	O
step	O
each	O
cluster	O
s	O
parameters	B
mk	O
and	O
k	O
are	O
adjusted	O
to	O
match	O
the	O
data	O
points	O
that	O
it	O
is	O
responsible	O
for	O
rn	O
k	O
xn	O
rk	O
mk	O
xn	O
k	O
xn	O
rn	O
k	O
where	O
rk	O
is	O
the	O
total	O
responsibility	B
of	O
mean	B
k	O
irk	O
rk	O
pk	O
rk	O
rk	O
rn	O
k	O
t	O
t	O
t	O
t	O
t	O
figure	O
soft	B
k-means	O
algorithm	O
with	O
k	O
applied	O
to	O
the	O
data	B
set	B
of	O
to	O
the	O
little	O
n	O
large	O
data	B
set	B
of	O
t	O
t	O
t	O
t	O
t	O
t	O
rn	O
k	O
i	O
exp	O
i	O
xn	O
qi	O
with	O
in	O
place	O
of	O
k	O
i	O
i	O
i	O
xn	O
rn	O
k	O
i	O
i	O
mk	O
rk	O
i	O
algorithm	O
the	O
soft	B
k-means	O
algorithm	O
version	O
which	O
corresponds	O
to	O
a	O
model	B
of	O
axis-aligned	O
gaussians	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
fatal	O
of	O
maximum	B
likelihood	B
t	O
t	O
t	O
t	O
figure	O
soft	B
k-means	O
algorithm	O
version	O
applied	O
to	O
the	O
data	O
consisting	O
of	O
two	O
cigar-shaped	O
clusters	O
k	O
t	O
t	O
t	O
t	O
t	O
figure	O
soft	B
k-means	O
algorithm	O
version	O
applied	O
to	O
the	O
little	O
n	O
large	O
data	B
set	B
k	O
that	O
convergence	O
can	O
take	O
a	O
long	O
time	O
but	O
eventually	O
the	O
algorithm	O
the	O
small	O
cluster	O
and	O
the	O
large	O
cluster	O
soft	B
k-means	O
version	O
is	O
a	O
maximum-likelihood	O
algorithm	O
for	O
a	O
mixture	O
of	O
spherical	O
gaussians	O
to	O
data	O
spherical	O
meaning	O
that	O
the	O
variance	B
of	O
the	O
gaussian	B
is	O
the	O
same	O
in	O
all	O
directions	O
this	O
algorithm	O
is	O
still	O
no	O
good	B
at	O
modelling	B
the	O
cigar-shaped	O
clusters	O
of	O
if	O
we	O
wish	O
to	O
model	B
the	O
clusters	O
by	O
axis-aligned	O
gaussians	O
with	O
possibly-unequal	O
variances	O
we	O
replace	O
the	O
assignment	O
rule	O
and	O
the	O
variance	B
update	O
rule	O
by	O
the	O
rules	B
and	O
displayed	O
in	O
algorithm	O
this	O
third	O
version	O
of	O
soft	B
k-means	O
is	O
demonstrated	O
in	O
on	O
the	O
two	O
cigars	O
data	B
set	B
of	O
after	O
iterations	O
the	O
algorithm	O
correctly	O
locates	O
the	O
two	O
clusters	O
figure	O
shows	O
the	O
same	O
algorithm	O
applied	O
to	O
the	O
little	O
n	O
large	O
data	B
set	B
again	O
the	O
correct	O
cluster	O
locations	O
are	O
found	O
a	O
fatal	O
of	O
maximum	B
likelihood	B
finally	O
sounds	O
a	O
cautionary	O
note	O
when	O
we	O
k	O
means	O
to	O
our	O
toy	O
data	B
set	B
we	O
sometimes	O
that	O
very	O
small	O
clusters	O
form	O
covering	O
just	O
one	O
or	O
two	O
data	O
points	O
this	O
is	O
a	O
pathological	O
property	O
of	O
soft	B
k-means	B
clustering	B
versions	O
and	O
exercise	O
investigate	O
what	O
happens	O
if	O
one	O
mean	B
mk	O
sits	O
exactly	O
on	O
k	O
is	O
small	O
top	O
of	O
one	O
data	O
point	O
show	O
that	O
if	O
the	O
variance	B
then	O
no	O
return	O
is	O
possible	O
k	O
becomes	O
ever	O
smaller	O
t	O
t	O
t	O
t	O
a	O
proof	O
that	O
the	O
algorithm	O
does	O
indeed	O
maximize	O
the	O
likelihood	B
is	O
deferred	O
to	O
section	B
figure	O
soft	B
k-means	O
algorithm	O
applied	O
to	O
a	O
data	B
set	B
of	O
points	O
k	O
notice	O
that	O
at	O
convergence	O
one	O
very	O
small	O
cluster	O
has	O
formed	O
between	O
two	O
data	O
points	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
kaboom	B
maximum	B
likelihood	B
and	O
clustering	B
soft	B
k-means	O
can	O
blow	B
up	I
put	O
one	O
cluster	O
exactly	O
on	O
one	O
data	O
point	O
and	O
let	O
its	O
variance	B
go	O
to	O
zero	O
you	O
can	O
obtain	O
an	O
arbitrarily	O
large	O
likelihood	B
maximum	B
likelihood	B
methods	O
can	O
break	O
down	O
by	O
highly	O
tuned	O
models	O
that	O
part	O
of	O
the	O
data	O
perfectly	O
this	O
phenomenon	O
is	O
known	O
as	O
the	O
reason	O
we	O
are	O
not	O
interested	O
in	O
these	O
solutions	O
with	O
enormous	O
likelihood	B
is	O
this	O
sure	O
these	O
parameter-settings	O
may	O
have	O
enormous	O
posterior	B
probability	B
density	B
but	O
the	O
density	B
is	O
large	O
over	O
only	O
a	O
very	O
small	O
volume	B
of	O
parameter	O
space	O
so	O
the	O
probability	B
mass	O
associated	O
with	O
these	O
likelihood	B
spikes	O
is	O
usually	O
tiny	O
we	O
conclude	O
that	O
maximum	B
likelihood	B
methods	O
are	O
not	O
a	O
satisfactory	O
general	O
solution	O
to	O
data-modelling	O
problems	O
the	O
likelihood	B
may	O
be	O
large	O
at	O
certain	O
parameter	O
settings	O
even	O
if	O
the	O
likelihood	B
does	O
not	O
have	O
spikes	O
the	O
maximum	O
of	O
the	O
likelihood	B
is	O
often	O
unrepresentative	O
in	O
highdimensional	O
problems	O
even	O
in	O
low-dimensional	O
problems	O
maximum	B
likelihood	B
solutions	O
can	O
be	O
unrepresentative	O
as	O
you	O
may	O
know	O
from	O
basic	O
statistics	O
the	O
maximum	B
likelihood	B
estimator	B
for	O
a	O
gaussian	B
s	O
standard	B
deviation	I
n	O
is	O
a	O
biased	O
estimator	B
a	O
topic	O
that	O
we	O
ll	O
take	O
up	O
in	O
chapter	O
the	O
maximum	B
a	I
posteriori	I
method	O
a	O
popular	O
replacement	O
for	O
maximizing	O
the	O
likelihood	B
is	O
maximizing	O
the	O
bayesian	B
posterior	B
probability	B
density	B
of	O
the	O
parameters	B
instead	O
however	O
multiplying	O
the	O
likelihood	B
by	O
a	O
prior	B
and	O
maximizing	O
the	O
posterior	O
does	O
not	O
make	O
the	O
above	O
problems	O
go	O
away	O
the	O
posterior	O
density	B
often	O
also	O
has	O
spikes	O
and	O
the	O
maximum	O
of	O
the	O
posterior	B
probability	B
density	B
is	O
often	O
unrepresentative	O
of	O
the	O
whole	O
posterior	O
distribution	B
think	O
back	O
to	O
the	O
concept	O
of	O
typicality	B
which	O
we	O
encountered	O
in	O
chapter	O
in	O
high	B
dimensions	B
most	O
of	O
the	O
probability	B
mass	O
is	O
in	O
a	O
typical	B
set	B
whose	O
properties	O
are	O
quite	O
from	O
the	O
points	O
that	O
have	O
the	O
maximum	O
probability	B
density	B
maxima	O
are	O
atypical	O
a	O
further	O
reason	O
for	O
disliking	O
the	O
maximum	B
a	I
posteriori	I
is	O
that	O
it	O
is	O
basisdependent	O
if	O
we	O
make	O
a	O
nonlinear	B
change	O
of	O
basis	O
from	O
the	O
parameter	O
to	O
the	O
parameter	O
u	O
f	O
then	O
the	O
probability	B
density	B
of	O
is	O
transformed	O
to	O
p	O
p	O
the	O
maximum	O
of	O
the	O
density	B
p	O
will	O
usually	O
not	O
coincide	O
with	O
the	O
maximum	O
of	O
the	O
density	B
p	O
illustrating	O
such	O
nonlinear	B
changes	O
of	O
basis	O
see	O
the	O
next	O
chapter	O
it	O
seems	O
undesirable	O
to	O
use	O
a	O
method	O
whose	O
answers	O
change	O
when	O
we	O
change	O
representation	O
further	O
reading	O
the	O
soft	B
k-means	O
algorithm	O
is	O
at	O
the	O
heart	O
of	O
the	O
automatic	O
package	O
autoclass	B
et	O
al	O
hanson	O
et	O
al	O
further	O
exercises	O
exercises	O
where	O
maximum	B
likelihood	B
may	O
be	O
useful	O
exercise	O
make	O
a	O
version	O
of	O
the	O
k-means	O
algorithm	O
that	O
models	O
the	O
data	O
as	O
a	O
mixture	O
of	O
k	O
arbitrary	O
gaussians	O
i	O
e	O
gaussians	O
that	O
are	O
not	O
constrained	B
to	O
be	O
axis-aligned	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
exercise	O
a	O
photon	B
counter	I
is	O
pointed	O
at	O
a	O
remote	O
star	O
for	O
one	O
minute	O
in	O
order	O
to	O
infer	O
the	O
brightness	O
i	O
e	O
the	O
rate	B
of	O
photons	O
arriving	O
at	O
the	O
counter	O
per	O
minute	O
assuming	O
the	O
number	O
of	O
photons	O
collected	O
r	O
has	O
a	O
poisson	B
distribution	B
with	O
mean	B
p	O
j	O
r	O
what	O
is	O
the	O
maximum	B
likelihood	B
estimate	O
for	O
given	O
r	O
find	O
error	B
bars	I
on	O
ln	O
same	O
situation	O
but	O
now	O
we	O
assume	O
that	O
the	O
counter	O
detects	O
not	O
only	O
photons	O
from	O
the	O
star	O
but	O
also	O
background	O
photons	O
the	O
background	B
rate	B
of	O
photons	O
is	O
known	O
to	O
be	O
b	O
photons	O
per	O
minute	O
we	O
assume	O
the	O
number	O
of	O
photons	O
collected	O
r	O
has	O
a	O
poisson	B
distribution	B
with	O
mean	B
now	O
given	O
r	O
detected	O
photons	O
what	O
is	O
the	O
maximum	B
likelihood	B
estimate	O
for	O
comment	O
on	O
this	O
answer	O
discussing	O
also	O
the	O
bayesian	B
posterior	O
distribution	B
and	O
the	O
unbiased	B
estimator	B
of	O
sampling	B
theory	I
r	O
b	O
exercise	O
a	O
bent	B
coin	B
is	O
tossed	O
n	O
times	O
giving	O
na	O
heads	O
and	O
nb	O
tails	O
assume	O
a	O
beta	B
distribution	B
prior	B
for	O
the	O
probability	B
of	O
heads	O
p	O
for	O
example	O
the	O
uniform	O
distribution	B
find	O
the	O
maximum	B
likelihood	B
and	O
maximum	B
a	I
posteriori	I
values	O
of	O
p	O
then	O
the	O
maximum	B
likelihood	B
and	O
maximum	B
a	I
posteriori	I
values	O
of	O
the	O
logit	B
a	O
compare	O
with	O
the	O
predictive	B
distribution	B
i	O
e	O
the	O
probability	B
that	O
the	O
next	O
toss	O
will	O
come	O
up	O
heads	O
exercise	O
two	O
men	O
looked	O
through	O
prison	O
bars	O
one	O
saw	O
stars	B
the	O
other	O
tried	O
to	O
infer	O
where	O
the	O
window	B
frame	O
was	O
from	O
the	O
other	O
side	O
of	O
a	O
room	O
you	O
look	O
through	O
a	O
window	B
and	O
see	O
stars	B
at	O
locations	O
fxn	O
yng	O
you	O
can	O
t	O
see	O
the	O
window	B
edges	O
because	O
it	O
is	O
totally	O
dark	O
apart	O
from	O
the	O
stars	B
assuming	O
the	O
window	B
is	O
rectangular	B
and	O
that	O
the	O
visible	O
stars	B
locations	O
are	O
independently	O
randomly	O
distributed	O
what	O
are	O
the	O
inferred	O
values	O
of	O
ymin	O
xmax	O
ymax	O
according	O
to	O
maximum	B
likelihood	B
sketch	O
the	O
likelihood	B
as	O
a	O
function	O
of	O
xmax	O
for	O
xmin	O
ymin	O
and	O
ymax	O
exercise	O
a	O
sailor	B
infers	O
his	O
location	O
y	O
by	O
measuring	O
the	O
bearings	O
of	O
three	O
buoys	O
whose	O
locations	O
yn	O
are	O
given	O
on	O
his	O
chart	O
let	O
the	O
true	O
bearings	O
of	O
the	O
buoys	O
be	O
assuming	O
that	O
his	O
measurement	O
of	O
each	O
bearing	B
is	O
subject	O
to	O
gaussian	B
noise	O
of	O
small	O
standard	B
deviation	I
what	O
is	O
his	O
inferred	O
location	O
by	O
maximum	B
likelihood	B
the	O
sailor	B
s	O
rule	B
of	I
thumb	I
says	O
that	O
the	O
boat	O
s	O
position	O
can	O
be	O
taken	O
to	O
be	O
the	O
centre	O
of	O
the	O
cocked	B
hat	B
the	O
triangle	B
produced	O
by	O
the	O
intersection	B
of	O
the	O
three	O
measured	O
bearings	O
can	O
you	O
persuade	O
him	O
that	O
the	O
maximum	B
likelihood	B
answer	O
is	O
better	O
exercise	O
maximum	B
likelihood	B
of	O
an	O
exponential-family	B
model	B
assume	O
that	O
a	O
variable	O
x	O
comes	O
from	O
a	O
probability	B
distribution	B
of	O
the	O
form	O
p	O
w	O
zw	O
exp	O
xk	O
wkfkx	O
ymax	O
ymin	O
b	O
q	O
q	O
b	O
a	O
q	O
a	O
q	O
a	O
qq	O
a	O
a	O
a	O
a	O
a	O
a	O
b	O
figure	O
the	O
standard	O
way	O
of	O
drawing	O
three	O
slightly	O
inconsistent	O
bearings	O
on	O
a	O
chart	O
produces	O
a	O
triangle	B
called	O
a	O
cocked	B
hat	B
where	O
is	O
the	O
sailor	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
maximum	B
likelihood	B
and	O
clustering	B
where	O
the	O
functions	B
fkx	O
are	O
given	O
and	O
the	O
parameters	B
w	O
fwkg	O
are	O
not	O
known	O
a	O
data	B
set	B
fxng	O
of	O
n	O
points	O
is	O
supplied	O
show	O
by	O
the	O
log	O
likelihood	B
that	O
the	O
maximum-likelihood	O
parameters	B
wml	O
satisfy	O
p	O
wmlfkx	O
xx	O
n	O
xn	O
fkxn	O
where	O
the	O
left-hand	O
sum	O
is	O
over	O
all	O
x	O
and	O
the	O
right-hand	O
sum	O
is	O
over	O
the	O
data	O
points	O
a	O
shorthand	O
for	O
this	O
result	O
is	O
that	O
each	O
function-average	O
under	O
the	O
model	B
must	O
equal	O
the	O
function-average	O
found	O
in	O
the	O
data	O
hfkip	O
j	O
wml	O
hfkidata	O
exercise	O
maximum	B
entropy	B
of	O
models	O
to	O
constraints	O
when	O
confronted	O
by	O
a	O
probability	B
distribution	B
p	O
about	O
which	O
only	O
a	O
few	O
facts	O
are	O
known	O
the	O
maximum	B
entropy	B
principle	O
a	O
rule	O
for	O
choosing	O
a	O
distribution	B
that	O
those	O
constraints	O
according	O
to	O
maxent	O
you	O
should	O
select	O
the	O
p	O
that	O
maximizes	O
the	O
entropy	B
h	O
p	O
log	O
subject	O
to	O
the	O
constraints	O
assuming	O
the	O
constraints	O
assert	O
that	O
the	O
averages	O
of	O
certain	O
functions	B
fkx	O
are	O
known	O
i	O
e	O
hfkip	O
fk	O
show	O
by	O
introducing	O
lagrange	O
multipliers	O
for	O
each	O
constraint	O
including	O
normalization	O
that	O
the	O
maximum-entropy	O
distribution	B
has	O
the	O
form	O
exp	O
xk	O
wkfkx	O
p	O
z	O
where	O
the	O
parameters	B
z	O
and	O
fwkg	O
are	O
set	B
such	O
that	O
the	O
constraints	O
are	O
and	O
hence	O
the	O
maximum	B
entropy	B
method	O
gives	O
identical	O
results	O
to	O
maximum	B
likelihood	B
of	O
an	O
exponential-family	B
model	B
exercise	O
the	O
maximum	B
entropy	B
method	O
has	O
sometimes	O
been	O
recommended	O
as	O
a	O
method	O
for	O
assigning	B
prior	B
distributions	O
in	O
bayesian	B
modelling	B
while	O
the	O
outcomes	O
of	O
the	O
maximum	B
entropy	B
method	O
are	O
sometimes	O
interesting	O
and	O
thought-provoking	O
i	O
do	O
not	O
advocate	O
maxent	O
as	O
the	O
approach	O
to	O
assigning	B
priors	O
maximum	B
entropy	B
is	O
also	O
sometimes	O
proposed	O
as	O
a	O
method	O
for	O
solving	O
inference	B
problems	O
for	O
example	O
given	O
that	O
the	O
mean	B
score	O
of	O
this	O
unfair	O
six-sided	O
die	B
is	O
what	O
is	O
its	O
probability	B
distribution	B
i	O
think	O
it	O
is	O
a	O
bad	B
idea	O
to	O
use	O
maximum	B
entropy	B
in	O
this	O
way	O
it	O
can	O
give	O
silly	O
answers	O
the	O
correct	O
way	O
to	O
solve	O
inference	B
problems	O
is	O
to	O
use	O
bayes	B
theorem	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
exercises	O
where	O
maximum	B
likelihood	B
and	O
map	O
have	O
a	O
b	O
c	O
d-g	O
scientist	O
xn	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
figure	O
seven	O
measurements	O
fxng	O
of	O
a	O
parameter	O
by	O
seven	O
scientists	B
each	O
having	O
his	O
own	O
noise-level	O
exercise	O
this	O
exercise	O
explores	O
the	O
idea	O
that	O
maximizing	O
a	O
probability	B
density	B
is	O
a	O
poor	O
way	O
to	O
a	O
point	O
that	O
is	O
representative	O
of	O
the	O
density	B
consider	O
a	O
gaussian	B
distribution	B
in	O
a	O
k-dimensional	O
space	O
p	O
w	O
show	O
that	O
nearly	O
all	O
of	O
the	O
probability	B
mass	O
of	O
a	O
gaussian	B
is	O
in	O
a	O
thin	B
shell	I
of	O
radius	O
r	O
and	O
of	O
thickness	O
proportional	O
to	O
rpk	O
for	O
example	O
in	O
dimensions	B
of	O
the	O
mass	O
of	O
a	O
gaussian	B
with	O
is	O
in	O
a	O
shell	O
of	O
radius	O
and	O
thickness	O
however	O
the	O
probability	B
density	B
at	O
the	O
origin	O
is	O
times	O
bigger	O
than	O
the	O
density	B
at	O
this	O
shell	O
where	O
most	O
of	O
the	O
probability	B
mass	O
is	O
i	O
now	O
consider	O
two	O
gaussian	B
densities	O
in	O
dimensions	B
that	O
in	O
radius	O
by	O
just	O
and	O
that	O
contain	O
equal	O
total	O
probability	B
mass	O
show	O
that	O
the	O
maximum	O
probability	B
density	B
is	O
greater	O
at	O
the	O
centre	O
of	O
the	O
gaussian	B
with	O
smaller	O
by	O
a	O
factor	O
of	O
in	O
ill-posed	O
problems	O
a	O
typical	B
posterior	O
distribution	B
is	O
often	O
a	O
weighted	O
superposition	O
of	O
gaussians	O
with	O
varying	O
means	O
and	O
standard	O
deviations	O
so	O
the	O
true	O
posterior	O
has	O
a	O
skew	O
peak	O
with	O
the	O
maximum	O
of	O
the	O
probability	B
density	B
located	O
near	O
the	O
mean	B
of	O
the	O
gaussian	B
distribution	B
that	O
has	O
the	O
smallest	O
standard	B
deviation	I
not	O
the	O
gaussian	B
with	O
the	O
greatest	O
weight	O
exercise	O
the	O
seven	O
scientists	B
n	O
datapoints	O
fxng	O
are	O
drawn	O
from	O
n	O
distributions	O
all	O
of	O
which	O
are	O
gaussian	B
with	O
a	O
common	O
mean	B
but	O
with	O
unknown	O
standard	O
deviations	O
what	O
are	O
the	O
maximum	B
likelihood	B
parameters	B
given	O
the	O
data	O
for	O
example	O
seven	O
scientists	B
b	O
c	O
d	O
e	O
f	O
g	O
with	O
experimental	O
skills	O
measure	O
you	O
expect	O
some	O
of	O
them	O
to	O
do	O
accurate	O
work	O
to	O
have	O
small	O
and	O
some	O
of	O
them	O
to	O
turn	O
in	O
wildly	O
inaccurate	O
answers	O
to	O
have	O
enormous	O
figure	O
shows	O
their	O
seven	O
results	O
what	O
is	O
and	O
how	O
reliable	O
is	O
each	O
scientist	O
i	O
hope	O
you	O
agree	O
that	O
intuitively	O
it	O
looks	O
pretty	O
certain	O
that	O
a	O
and	O
b	O
are	O
both	O
inept	O
measurers	O
that	O
dg	O
are	O
better	O
and	O
that	O
the	O
true	O
value	O
of	O
is	O
somewhere	O
close	O
to	O
but	O
what	O
does	O
maximizing	O
the	O
likelihood	B
tell	O
you	O
exercise	O
problems	O
with	O
map	B
method	I
a	O
collection	O
of	O
widgets	O
i	O
k	O
have	O
a	O
property	O
called	O
wodge	B
wi	O
which	O
we	O
measure	O
widget	B
by	O
widget	B
in	O
noisy	B
experiments	O
with	O
a	O
known	O
noise	O
level	O
our	O
model	B
for	O
these	O
quantities	O
is	O
that	O
they	O
come	O
from	O
a	O
gaussian	B
prior	B
p	O
j	O
where	O
w	O
is	O
not	O
known	O
our	O
prior	B
for	O
this	O
variance	B
is	O
over	O
log	O
from	O
to	O
scenario	O
suppose	O
four	O
widgets	O
have	O
been	O
measured	O
and	O
give	O
the	O
following	O
data	O
we	O
are	O
interested	O
in	O
inferring	O
the	O
wodges	O
of	O
these	O
four	O
widgets	O
find	O
the	O
values	O
of	O
w	O
and	O
that	O
maximize	O
the	O
posterior	B
probability	B
p	O
log	O
d	O
marginalize	O
over	O
and	O
the	O
posterior	B
probability	B
density	B
of	O
w	O
given	O
the	O
data	O
skills	O
required	O
see	O
mackay	B
for	O
solution	O
find	O
maxima	O
of	O
p	O
j	O
d	O
two	O
maxima	O
one	O
at	O
wmp	O
with	O
error	B
bars	I
on	O
all	O
four	O
parameters	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
maximum	B
likelihood	B
and	O
clustering	B
from	B
gaussian	B
approximation	B
to	O
the	O
posterior	O
and	O
one	O
at	O
with	O
error	B
bars	I
scenario	O
suppose	O
in	O
addition	O
to	O
the	O
four	O
measurements	O
above	O
we	O
are	O
now	O
informed	O
that	O
there	O
are	O
four	O
more	O
widgets	O
that	O
have	O
been	O
measured	O
with	O
a	O
much	O
less	O
accurate	O
instrument	O
having	O
thus	O
we	O
now	O
have	O
both	O
well-determined	O
and	O
ill-determined	O
parameters	B
as	O
in	O
a	O
typical	B
ill-posed	B
problem	I
the	O
data	O
from	O
these	O
measurements	O
were	O
a	O
string	O
of	O
uninformative	O
values	O
we	O
are	O
again	O
asked	O
to	O
infer	O
the	O
wodges	O
of	O
the	O
widgets	O
intuitively	O
our	O
inferences	O
about	O
the	O
well-measured	O
widgets	O
should	O
be	O
negligibly	O
by	O
this	O
vacuous	O
information	B
about	O
the	O
poorly-measured	O
widgets	O
but	O
what	O
happens	O
to	O
the	O
map	B
method	I
find	O
the	O
values	O
of	O
w	O
and	O
that	O
maximize	O
the	O
posterior	B
probability	B
p	O
log	O
d	O
find	O
maxima	O
of	O
p	O
j	O
d	O
only	O
one	O
maximum	O
wmp	O
with	O
error	B
bars	I
on	O
all	O
eight	O
parameters	B
solutions	O
solution	O
to	O
exercise	O
figure	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
likelihood	B
function	O
for	O
the	O
data	O
points	O
the	O
peaks	O
are	O
pretty-near	O
centred	O
on	O
the	O
points	O
and	O
and	O
are	O
pretty-near	O
circular	O
in	O
their	O
contours	O
the	O
width	O
of	O
each	O
of	O
the	O
peaks	O
is	O
a	O
standard	B
deviation	I
of	O
the	O
peaks	O
are	O
roughly	O
gaussian	B
in	O
shape	O
solution	O
to	O
exercise	O
the	O
log	O
likelihood	B
is	O
ln	O
p	O
w	O
ln	O
zw	O
xk	O
wkfkxn	O
figure	O
the	O
likelihood	B
as	O
a	O
function	O
of	O
and	O
ln	O
p	O
w	O
ln	O
zw	O
fkx	O
now	O
the	O
fun	O
part	O
is	O
what	O
happens	O
when	O
we	O
the	O
log	O
of	O
the	O
normalizing	O
constant	O
exp	O
zwxx	O
fkx	O
xx	O
p	O
wfkx	O
ln	O
zw	O
so	O
exp	O
zwxx	O
ln	O
p	O
w	O
p	O
wfkx	O
fkx	O
and	O
at	O
the	O
maximum	O
of	O
the	O
likelihood	B
xx	O
p	O
wmlfkx	O
n	O
xn	O
fkxn	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
useful	O
probability	B
distributions	O
in	O
bayesian	B
data	O
modelling	B
there	O
s	O
a	O
small	O
collection	O
of	O
probability	B
distributions	O
that	O
come	O
up	O
again	O
and	O
again	O
the	O
purpose	O
of	O
this	O
chapter	O
is	O
to	O
introduce	O
these	O
distributions	O
so	O
that	O
they	O
won	O
t	O
be	O
intimidating	O
when	O
encountered	O
in	O
combat	O
situations	O
there	O
is	O
no	O
need	O
to	O
memorize	O
any	O
of	O
them	O
except	O
perhaps	O
the	O
gaussian	B
if	O
a	O
distribution	B
is	O
important	O
enough	O
it	O
will	O
memorize	O
itself	O
and	O
otherwise	O
it	O
can	O
easily	O
be	O
looked	O
up	O
distributions	O
over	O
integers	O
binomial	B
poisson	B
exponential	B
we	O
already	O
encountered	O
the	O
binomial	B
distribution	B
and	O
the	O
poisson	B
distribution	B
on	O
page	O
the	O
binomial	B
distribution	B
for	O
an	O
integer	O
r	O
with	O
parameters	B
f	O
bias	B
f	O
and	O
n	O
number	O
of	O
trials	O
is	O
f	O
p	O
j	O
f	O
n	O
r	O
ng	O
the	O
binomial	B
distribution	B
arises	O
for	O
example	O
when	O
we	O
a	O
bent	B
coin	B
with	O
bias	B
f	O
n	O
times	O
and	O
observe	O
the	O
number	O
of	O
heads	O
r	O
the	O
poisson	B
distribution	B
with	O
parameter	O
is	O
p	O
j	O
r	O
r	O
the	O
poisson	B
distribution	B
arises	O
for	O
example	O
when	O
we	O
count	O
the	O
number	O
of	O
photons	O
r	O
that	O
arrive	O
in	O
a	O
pixel	O
during	O
a	O
interval	O
given	O
that	O
the	O
mean	B
intensity	O
on	O
the	O
pixel	O
corresponds	O
to	O
an	O
average	O
number	O
of	O
photons	O
the	O
exponential	B
distribution	B
on	B
integers	I
p	O
j	O
f	O
f	O
f	O
r	O
arises	O
in	O
waiting	O
problems	O
how	O
long	O
will	O
you	O
have	O
to	O
wait	O
until	O
a	O
six	B
is	O
rolled	O
if	O
a	O
fair	O
six-sided	O
dice	O
is	O
rolled	O
answer	O
the	O
probability	B
distribution	B
of	O
the	O
number	O
of	O
rolls	O
r	O
is	O
exponential	B
over	O
integers	O
with	O
parameter	O
f	O
the	O
distribution	B
may	O
also	O
be	O
written	O
p	O
j	O
f	O
f	O
r	O
where	O
r	O
figure	O
the	O
binomial	B
distribution	B
p	O
j	O
f	O
n	O
on	O
a	O
linear	B
scale	O
and	O
a	O
logarithmic	O
scale	O
r	O
figure	O
the	O
poisson	B
distribution	B
p	O
j	O
on	O
a	O
linear	B
scale	O
and	O
a	O
logarithmic	O
scale	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
useful	O
probability	B
distributions	O
distributions	O
over	O
unbounded	O
real	O
numbers	O
gaussian	B
student	B
cauchy	B
biexponential	B
inverse-cosh	B
the	O
gaussian	B
distribution	B
or	O
normal	B
distribution	B
with	O
mean	B
and	O
standard	B
deviation	I
is	O
p	O
z	O
where	O
x	O
z	O
it	O
is	O
sometimes	O
useful	O
to	O
work	O
with	O
the	O
quantity	O
which	O
is	O
called	O
the	O
precision	B
parameter	O
of	O
the	O
gaussian	B
a	O
sample	B
z	O
from	O
a	O
standard	O
univariate	O
gaussian	B
can	O
be	O
generated	O
by	O
computing	O
z	O
where	O
and	O
are	O
uniformly	O
distributed	O
in	O
a	O
second	O
sample	B
independent	O
of	O
the	O
can	O
then	O
be	O
obtained	O
for	O
free	O
the	O
gaussian	B
distribution	B
is	O
widely	O
used	O
and	O
often	O
asserted	O
to	O
be	O
a	O
very	O
common	O
distribution	B
in	O
the	O
real	O
world	O
but	O
i	O
am	O
sceptical	O
about	O
this	O
assertion	O
yes	O
unimodal	O
distributions	O
may	O
be	O
common	O
but	O
a	O
gaussian	B
is	O
a	O
special	O
rather	O
extreme	O
unimodal	O
distribution	B
it	O
has	O
very	O
light	O
tails	O
the	O
logprobability-density	O
decreases	O
quadratically	O
the	O
typical	B
deviation	O
of	O
x	O
from	O
is	O
but	O
the	O
respective	O
probabilities	O
that	O
x	O
deviates	O
from	O
by	O
more	O
than	O
and	O
are	O
and	O
in	O
my	O
experience	O
deviations	O
from	O
a	O
mean	B
four	O
or	O
times	O
greater	O
than	O
the	O
typical	B
deviation	O
may	O
be	O
rare	O
but	O
not	O
as	O
rare	O
as	O
i	O
therefore	O
urge	O
caution	O
in	O
the	O
use	O
of	O
gaussian	B
distributions	O
if	O
a	O
variable	O
that	O
is	O
modelled	O
with	O
a	O
gaussian	B
actually	O
has	O
a	O
heavier-tailed	O
distribution	B
the	O
rest	O
of	O
the	O
model	B
will	O
contort	O
itself	O
to	O
reduce	O
the	O
deviations	O
of	O
the	O
outliers	O
like	O
a	O
sheet	O
of	O
paper	O
being	O
crushed	O
by	O
a	O
rubber	O
band	O
exercise	O
pick	O
a	O
variable	O
that	O
is	O
supposedly	O
bell-shaped	O
in	O
probability	B
distribution	B
gather	O
data	O
and	O
make	O
a	O
plot	O
of	O
the	O
variable	O
s	O
empirical	O
distribution	B
show	O
the	O
distribution	B
as	O
a	O
histogram	O
on	O
a	O
log	O
scale	O
and	O
investigate	O
whether	O
the	O
tails	O
are	O
well-modelled	O
by	O
a	O
gaussian	B
distribution	B
example	O
of	O
a	O
variable	O
to	O
study	O
is	O
the	O
amplitude	O
of	O
an	O
audio	O
signal	O
one	O
distribution	B
with	O
heavier	O
tails	O
than	O
a	O
gaussian	B
is	O
a	O
mixture	B
of	I
gaussians	I
a	O
mixture	O
of	O
two	O
gaussians	O
for	O
example	O
is	O
by	O
two	O
means	O
two	O
standard	O
deviations	O
and	O
two	O
mixing	O
and	O
satisfying	O
p	O
if	O
we	O
take	O
an	O
appropriately	O
weighted	O
mixture	O
of	O
an	O
number	O
of	O
gaussians	O
all	O
having	O
mean	B
we	O
obtain	O
a	O
student-t	B
distribution	B
where	O
p	O
s	O
n	O
z	O
z	O
figure	O
three	O
unimodal	O
distributions	O
two	O
student	B
distributions	O
with	O
parameters	B
s	O
line	O
cauchy	B
distribution	B
and	O
line	O
and	O
a	O
gaussian	B
distribution	B
with	O
mean	B
and	O
standard	B
deviation	I
line	O
shown	O
on	O
linear	B
vertical	O
scales	O
and	O
logarithmic	O
vertical	O
scales	O
notice	O
that	O
the	O
heavy	O
tails	O
of	O
the	O
cauchy	B
distribution	B
are	O
scarcely	O
evident	O
in	O
the	O
upper	O
bell-shaped	O
curve	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
distributions	O
over	O
positive	O
real	O
numbers	O
and	O
n	O
is	O
called	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
and	O
is	O
the	O
gamma	B
function	I
if	O
n	O
then	O
the	O
student	B
distribution	B
has	O
a	O
mean	B
and	O
that	O
mean	B
is	O
if	O
n	O
the	O
distribution	B
also	O
has	O
a	O
variance	B
as	O
n	O
the	O
student	B
distribution	B
approaches	O
the	O
normal	B
distribution	B
with	O
mean	B
and	O
standard	B
deviation	I
s	O
the	O
student	B
distribution	B
arises	O
both	O
in	O
classical	B
statistics	I
the	O
sampling-theoretic	O
distribution	B
of	O
certain	O
statistics	O
and	O
in	O
bayesian	B
inference	B
the	O
probability	B
distribution	B
of	O
a	O
variable	O
coming	O
from	O
a	O
gaussian	B
distribution	B
whose	O
standard	B
deviation	I
we	O
aren	O
t	O
sure	O
of	O
in	O
the	O
special	O
case	O
n	O
the	O
student	B
distribution	B
is	O
called	O
the	O
cauchy	B
distribution	B
a	O
distribution	B
whose	O
tails	O
are	O
intermediate	O
in	O
heaviness	O
between	O
student	B
and	O
gaussian	B
is	O
the	O
biexponential	B
distribution	B
p	O
s	O
z	O
s	O
x	O
where	O
z	O
the	O
inverse-cosh	B
distribution	B
p	O
is	O
a	O
popular	O
model	B
in	O
independent	B
component	I
analysis	I
in	O
the	O
limit	O
of	O
large	O
the	O
probability	B
distribution	B
p	O
becomes	O
a	O
biexponential	B
distribution	B
in	O
the	O
limit	O
p	O
approaches	O
a	O
gaussian	B
with	O
mean	B
zero	O
and	O
variance	B
distributions	O
over	O
positive	O
real	O
numbers	O
exponential	B
gamma	B
inverse-gamma	O
and	O
log-normal	B
the	O
exponential	B
distribution	B
where	O
p	O
s	O
z	O
x	O
x	O
z	O
s	O
arises	O
in	O
waiting	O
problems	O
how	O
long	O
will	O
you	O
have	O
to	O
wait	O
for	O
a	O
bus	O
in	O
poissonville	B
given	O
that	O
buses	O
arrive	O
independently	O
at	O
random	B
with	O
one	O
every	O
s	O
minutes	O
on	O
average	O
answer	O
the	O
probability	B
distribution	B
of	O
your	O
wait	O
x	O
is	O
exponential	B
with	O
mean	B
s	O
the	O
gamma	B
distribution	B
is	O
like	O
a	O
gaussian	B
distribution	B
except	O
whereas	O
the	O
gaussian	B
goes	O
from	O
to	O
gamma	B
distributions	O
go	O
from	O
to	O
just	O
as	O
the	O
gaussian	B
distribution	B
has	O
two	O
parameters	B
and	O
which	O
control	O
the	O
mean	B
and	O
width	O
of	O
the	O
distribution	B
the	O
gamma	B
distribution	B
has	O
two	O
parameters	B
it	O
is	O
the	O
product	O
of	O
the	O
one-parameter	O
exponential	B
distribution	B
with	O
a	O
polynomial	O
the	O
exponent	O
c	O
in	O
the	O
polynomial	O
is	O
the	O
second	O
parameter	O
p	O
s	O
c	O
s	O
c	O
where	O
z	O
x	O
x	O
x	O
z	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
useful	O
probability	B
distributions	O
x	O
l	O
ln	O
x	O
figure	O
two	O
gamma	B
distributions	O
with	O
parameters	B
c	O
lines	O
and	O
lines	O
shown	O
on	O
linear	B
vertical	O
scales	O
and	O
logarithmic	O
vertical	O
scales	O
and	O
shown	O
as	O
a	O
function	O
of	O
x	O
on	O
the	O
left	O
and	O
l	O
ln	O
x	O
on	O
the	O
right	O
this	O
is	O
a	O
simple	O
peaked	O
distribution	B
with	O
mean	B
sc	O
and	O
variance	B
it	O
is	O
often	O
natural	B
to	O
represent	O
a	O
positive	O
real	O
variable	O
x	O
in	O
terms	O
of	O
its	O
logarithm	O
l	O
ln	O
x	O
the	O
probability	B
density	B
of	O
l	O
is	O
p	O
p	O
zl	O
xl	O
s	O
xl	O
s	O
p	O
where	O
zl	O
gamma	B
distribution	B
is	O
named	O
after	O
its	O
normalizing	O
constant	O
an	O
odd	O
convention	O
it	O
seems	O
to	O
me	O
figure	O
shows	O
a	O
couple	O
of	O
gamma	B
distributions	O
as	O
a	O
function	O
of	O
x	O
and	O
of	O
l	O
notice	O
that	O
where	O
the	O
original	O
gamma	B
distribution	B
may	O
have	O
a	O
spike	O
at	O
x	O
the	O
distribution	B
over	O
l	O
never	O
has	O
such	O
a	O
spike	O
the	O
spike	O
is	O
an	O
artefact	O
of	O
a	O
bad	B
choice	O
of	O
basis	O
in	O
the	O
limit	O
sc	O
c	O
we	O
obtain	O
the	O
noninformative	B
prior	B
for	O
a	O
scale	O
parameter	O
the	O
prior	B
this	O
improper	B
prior	B
is	O
called	O
noninformative	B
because	O
it	O
has	O
no	O
associated	O
length	O
scale	O
no	O
characteristic	O
value	O
of	O
x	O
so	O
it	O
prefers	O
all	O
values	O
of	O
x	O
equally	O
it	O
is	O
invariant	O
under	O
the	O
reparameterization	O
x	O
mx	O
if	O
we	O
transform	O
the	O
probability	B
density	B
into	O
a	O
density	B
over	O
l	O
ln	O
x	O
we	O
the	O
latter	O
density	B
is	O
uniform	O
exercise	O
imagine	O
that	O
we	O
reparameterize	O
a	O
positive	O
variable	O
x	O
in	O
terms	O
of	O
its	O
cube	O
root	O
u	O
if	O
the	O
probability	B
density	B
of	O
x	O
is	O
the	O
improper	B
distribution	B
what	O
is	O
the	O
probability	B
density	B
of	O
u	O
the	O
gamma	B
distribution	B
is	O
always	O
a	O
unimodal	O
density	B
over	O
l	O
ln	O
x	O
and	O
as	O
can	O
be	O
seen	O
in	O
the	O
it	O
is	O
asymmetric	O
if	O
x	O
has	O
a	O
gamma	B
distribution	B
and	O
we	O
decide	O
to	O
work	O
in	O
terms	O
of	O
the	O
inverse	O
of	O
x	O
v	O
we	O
obtain	O
a	O
new	O
distribution	B
in	O
which	O
the	O
density	B
over	O
l	O
is	O
left-for-right	O
the	O
probability	B
density	B
of	O
v	O
is	O
called	O
an	O
inverse-gamma	B
distribution	B
p	O
j	O
s	O
c	O
where	O
zv	O
v	O
zv	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
distributions	O
over	B
periodic	I
variables	I
v	O
ln	O
v	O
figure	O
two	O
inverse	O
gamma	B
distributions	O
with	O
parameters	B
c	O
lines	O
and	O
lines	O
shown	O
on	O
linear	B
vertical	O
scales	O
and	O
logarithmic	O
vertical	O
scales	O
and	O
shown	O
as	O
a	O
function	O
of	O
x	O
on	O
the	O
left	O
and	O
l	O
ln	O
x	O
on	O
the	O
right	O
gamma	B
and	O
inverse	O
gamma	B
distributions	O
crop	O
up	O
in	O
many	O
inference	B
problems	O
in	O
which	O
a	O
positive	O
quantity	O
is	O
inferred	O
from	O
data	O
examples	O
include	O
inferring	O
the	O
variance	B
of	O
gaussian	B
noise	O
from	O
some	O
noise	O
samples	O
and	O
inferring	O
the	O
rate	B
parameter	O
of	O
a	O
poisson	B
distribution	B
from	O
the	O
count	O
gamma	B
distributions	O
also	O
arise	O
naturally	O
in	O
the	O
distributions	O
of	O
waiting	O
times	O
between	O
poisson-distributed	O
events	O
given	O
a	O
poisson	B
process	I
with	O
rate	B
the	O
probability	B
density	B
of	O
the	O
arrival	O
time	O
x	O
of	O
the	O
mth	O
event	O
is	O
log-normal	B
distribution	B
another	O
distribution	B
over	O
a	O
positive	O
real	O
number	O
x	O
is	O
the	O
log-normal	B
distribution	B
which	O
is	O
the	O
distribution	B
that	O
results	O
when	O
l	O
ln	O
x	O
has	O
a	O
normal	B
distribution	B
we	O
m	O
to	O
be	O
the	O
median	O
value	O
of	O
x	O
and	O
s	O
to	O
be	O
the	O
standard	B
deviation	I
of	O
ln	O
x	O
p	O
j	O
m	O
s	O
z	O
ln	O
l	O
z	O
where	O
implies	O
p	O
m	O
s	O
x	O
z	O
x	O
ln	O
x	O
figure	O
two	O
log-normal	B
distributions	O
with	O
parameters	B
s	O
line	O
and	O
line	O
shown	O
on	O
linear	B
vertical	O
scales	O
and	O
logarithmic	O
vertical	O
scales	O
they	O
really	O
do	O
have	O
the	O
same	O
value	O
of	O
the	O
median	O
m	O
distributions	O
over	B
periodic	I
variables	I
a	O
periodic	B
variable	I
is	O
a	O
real	O
number	O
having	O
the	O
property	O
that	O
and	O
are	O
equivalent	O
a	O
distribution	B
that	O
plays	O
for	O
periodic	O
variables	O
the	O
role	O
played	O
by	O
the	O
gaus	O
sian	O
distribution	B
for	O
real	O
variables	O
is	O
the	O
von	B
mises	I
distribution	B
p	O
j	O
z	O
exp	O
the	O
normalizing	O
constant	O
is	O
z	O
where	O
is	O
a	O
bessel	O
function	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
useful	O
probability	B
distributions	O
a	O
distribution	B
that	O
arises	O
from	O
brownian	O
around	O
the	O
circle	B
is	O
the	O
wrapped	O
gaussian	B
distribution	B
p	O
j	O
distributions	O
over	O
probabilities	O
beta	B
distribution	B
dirichlet	B
distribution	B
entropic	B
distribution	B
the	O
beta	B
distribution	B
is	O
a	O
probability	B
density	B
over	O
a	O
variable	O
p	O
that	O
is	O
a	O
probability	B
p	O
p	O
the	O
parameters	B
may	O
take	O
any	O
positive	O
value	O
the	O
normalizing	O
constant	O
is	O
the	O
beta	B
function	I
special	O
cases	O
include	O
the	O
uniform	O
distribution	B
the	O
prior	B
and	O
the	O
improper	B
laplace	B
prior	B
if	O
we	O
transform	O
the	O
beta	B
distribution	B
to	O
the	O
corresponding	O
density	B
over	O
the	O
logit	B
l	O
ln	O
p	O
p	O
we	O
it	O
is	O
always	O
a	O
pleasant	O
bell-shaped	O
density	B
over	O
l	O
while	O
the	O
density	B
over	O
p	O
may	O
have	O
singularities	O
at	O
p	O
and	O
p	O
more	O
dimensions	B
figure	O
three	O
beta	B
distributions	O
with	O
and	O
the	O
upper	O
shows	O
p	O
as	O
a	O
function	O
of	O
p	O
the	O
lower	O
shows	O
the	O
corresponding	O
density	B
over	O
the	O
logit	B
ln	O
p	O
p	O
notice	O
how	O
well-behaved	O
the	O
densities	O
are	O
as	O
a	O
function	O
of	O
the	O
logit	B
the	O
dirichlet	B
distribution	B
is	O
a	O
density	B
over	O
an	O
i-dimensional	O
vector	O
p	O
whose	O
i	O
components	O
are	O
positive	O
and	O
sum	O
to	O
the	O
beta	B
distribution	B
is	O
a	O
special	O
case	O
of	O
a	O
dirichlet	B
distribution	B
with	O
i	O
the	O
dirichlet	B
distribution	B
is	O
parameterized	O
by	O
a	O
measure	O
u	O
vector	O
with	O
all	O
ui	O
which	O
i	O
will	O
write	O
here	O
as	O
u	O
where	O
m	O
is	O
a	O
normalized	O
measure	O
over	O
the	O
i	O
components	O
mi	O
and	O
is	O
positive	O
p	O
i	O
pi	O
dirichletipj	O
to	O
the	O
simplex	B
such	O
that	O
p	O
is	O
normalized	O
i	O
e	O
pi	O
pi	O
the	O
normalizing	O
the	O
function	O
is	O
the	O
dirac	O
delta	B
function	I
which	O
restricts	O
the	O
distribution	B
constant	O
of	O
the	O
dirichlet	B
distribution	B
is	O
i	O
the	O
vector	O
m	O
is	O
the	O
mean	B
of	O
the	O
probability	B
distribution	B
z	O
dirichletipj	O
p	O
dip	O
m	O
when	O
working	O
with	O
a	O
probability	B
vector	O
p	O
it	O
is	O
often	O
helpful	O
to	O
work	O
in	O
the	O
softmax	B
basis	O
in	O
which	O
for	O
example	O
a	O
three-dimensional	O
probability	B
p	O
is	O
represented	O
by	O
three	O
numbers	O
satisfying	O
and	O
pi	O
z	O
eai	O
where	O
z	O
eai	O
this	O
nonlinear	B
transformation	O
is	O
analogous	O
to	O
the	O
ln	O
transformation	O
for	O
a	O
scale	O
variable	O
and	O
the	O
logit	B
transformation	O
for	O
a	O
single	O
probability	B
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
distributions	O
over	O
probabilities	O
u	O
u	O
u	O
ln	O
p	O
dirichlet	B
distribution	B
disappear	O
and	O
the	O
density	B
is	O
given	O
by	O
in	O
the	O
softmax	B
basis	O
the	O
ugly	O
minus-ones	O
in	O
the	O
exponents	O
in	O
the	O
p	O
i	O
i	O
ai	O
the	O
role	O
of	O
the	O
parameter	O
can	O
be	O
characterized	O
in	O
two	O
ways	O
first	O
measures	O
the	O
sharpness	O
of	O
the	O
distribution	B
it	O
measures	O
how	O
we	O
expect	O
typical	B
samples	O
p	O
from	O
the	O
distribution	B
to	O
be	O
from	O
the	O
mean	B
m	O
just	O
as	O
the	O
precision	B
of	O
a	O
gaussian	B
measures	O
how	O
far	O
samples	O
stray	O
from	O
its	O
mean	B
a	O
large	O
value	O
of	O
produces	O
a	O
distribution	B
over	O
p	O
that	O
is	O
sharply	O
peaked	O
around	O
m	O
the	O
of	O
in	O
higher-dimensional	O
situations	O
can	O
be	O
visualized	O
by	O
drawing	O
a	O
typical	B
sample	B
from	I
the	O
distribution	B
dirichletipj	O
with	O
m	O
set	B
to	O
the	O
uniform	O
vector	O
mi	O
and	O
making	O
a	O
zipf	B
plot	I
that	O
is	O
a	O
ranked	O
plot	O
of	O
the	O
values	O
of	O
the	O
components	O
pi	O
it	O
is	O
traditional	O
to	O
plot	O
both	O
pi	O
axis	O
and	O
the	O
rank	O
axis	O
on	O
logarithmic	O
scales	O
so	O
that	O
power	B
law	I
relationships	O
appear	O
as	O
straight	O
lines	O
figure	O
shows	O
these	O
plots	O
for	O
a	O
single	O
sample	B
from	I
ensembles	O
with	O
i	O
and	O
i	O
and	O
with	O
from	O
to	O
for	O
large	O
the	O
plot	O
is	O
shallow	O
with	O
many	O
components	O
having	O
similar	O
values	O
for	O
small	O
typically	O
one	O
component	O
pi	O
receives	O
an	O
overwhelming	O
share	O
of	O
the	O
probability	B
and	O
of	O
the	O
small	O
probability	B
that	O
remains	O
to	O
be	O
shared	O
among	O
the	O
other	O
components	O
another	O
component	O
receives	O
a	O
similarly	O
large	O
share	O
in	O
the	O
limit	O
as	O
goes	O
to	O
zero	O
the	O
plot	O
tends	O
to	O
an	O
increasingly	O
steep	O
power	B
law	I
second	O
we	O
can	O
characterize	O
the	O
role	O
of	O
in	O
terms	O
of	O
the	O
predictive	B
distribution	B
that	O
results	O
when	O
we	O
observe	O
samples	O
from	O
p	O
and	O
obtain	O
counts	O
f	O
fi	O
of	O
the	O
possible	O
outcomes	O
the	O
value	O
of	O
the	O
number	O
of	O
samples	O
from	O
p	O
that	O
are	O
required	O
in	O
order	O
that	O
the	O
data	O
dominate	O
over	O
the	O
prior	B
in	O
predictions	O
exercise	O
the	O
dirichlet	B
distribution	B
a	O
nice	O
additivity	O
property	O
imagine	O
that	O
a	O
biased	O
six-sided	O
die	B
has	O
two	O
red	O
faces	O
and	O
four	O
blue	O
faces	O
the	O
die	B
is	O
rolled	O
n	O
times	O
and	O
two	O
bayesians	O
examine	O
the	O
outcomes	O
in	O
order	O
to	O
infer	O
the	O
bias	B
of	O
the	O
die	B
and	O
make	O
predictions	O
one	O
bayesian	B
has	O
access	O
to	O
the	O
redblue	O
colour	O
outcomes	O
only	O
and	O
he	O
infers	O
a	O
twocomponent	O
probability	B
vector	O
pb	O
the	O
other	O
bayesian	B
has	O
access	O
to	O
each	O
full	O
outcome	O
he	O
can	O
see	O
which	O
of	O
the	O
six	B
faces	O
came	O
up	O
and	O
he	O
infers	O
a	O
six-component	O
probability	B
vector	O
where	O
figure	O
three	O
dirichlet	B
distributions	O
over	O
a	O
three-dimensional	O
probability	B
vector	O
the	O
upper	O
show	O
random	B
draws	O
from	O
each	O
distribution	B
showing	O
the	O
values	O
of	O
and	O
on	O
the	O
two	O
axes	O
the	O
triangle	B
in	O
the	O
is	O
the	O
simplex	B
of	O
legal	O
probability	B
distributions	O
the	O
lower	O
show	O
the	O
same	O
points	O
in	O
the	O
softmax	B
basis	O
the	O
two	O
axes	O
show	O
and	O
i	O
i	O
figure	O
zipf	B
plots	O
for	O
random	B
samples	O
from	O
dirichlet	B
distributions	O
with	O
various	O
values	O
of	O
for	O
each	O
value	O
of	O
i	O
or	O
and	O
each	O
one	O
sample	B
p	O
from	O
the	O
dirichlet	B
distribution	B
was	O
generated	O
the	O
zipf	B
plot	I
shows	O
the	O
probabilities	O
pi	O
ranked	O
by	O
magnitude	O
versus	O
their	O
rank	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
useful	O
probability	B
distributions	O
pr	O
and	O
pb	O
assuming	O
that	O
the	O
second	O
bayesian	B
assigns	O
a	O
dirichlet	B
distribution	B
to	O
with	O
hyperparameters	O
show	O
that	O
in	O
order	O
for	O
the	O
bayesian	B
s	O
inferences	O
to	O
be	O
consistent	O
with	O
those	O
of	O
the	O
second	O
bayesian	B
the	O
bayesian	B
s	O
prior	B
should	O
be	O
a	O
dirichlet	B
distribution	B
with	O
hyperparameters	O
hint	O
a	O
brute-force	O
approach	O
is	O
to	O
compute	O
the	O
integral	B
p	O
pb	O
r	O
p	O
u	O
a	O
cheaper	O
approach	O
is	O
to	O
compute	O
the	O
predictive	O
distributions	O
given	O
arbitrary	O
data	O
and	O
the	O
condition	O
for	O
the	O
two	O
predictive	O
distributions	O
to	O
match	O
for	O
all	O
data	O
the	O
entropic	B
distribution	B
for	O
a	O
probability	B
vector	O
p	O
is	O
sometimes	O
used	O
in	O
the	O
maximum	B
entropy	B
image	B
reconstruction	I
community	O
p	O
m	O
m	O
pi	O
where	O
m	O
the	O
measure	O
is	O
a	O
positive	O
vector	O
and	O
dklpjjm	O
pi	O
log	O
pimi	O
further	O
reading	O
see	O
and	O
peto	O
for	O
fun	O
with	O
dirichlets	O
further	O
exercises	O
exercise	O
n	O
datapoints	O
fxng	O
are	O
drawn	O
from	O
a	O
gamma	B
distribution	B
p	O
s	O
c	O
s	O
c	O
with	O
unknown	O
parameters	B
s	O
and	O
c	O
what	O
are	O
the	O
maximum	B
likelihood	B
parameters	B
s	O
and	O
c	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
how	O
can	O
we	O
avoid	O
the	O
exponentially	O
large	O
cost	O
of	O
complete	O
enumeration	O
of	O
all	O
hypotheses	O
before	O
we	O
stoop	O
to	O
approximate	O
methods	O
we	O
explore	B
two	O
approaches	O
to	O
exact	O
marginalization	B
marginalization	B
over	O
continuous	B
variables	O
known	O
as	O
nuisance	B
parameters	B
by	O
doing	O
integrals	O
and	O
second	O
summation	O
over	O
discrete	O
variables	O
by	O
message-passing	B
exact	O
marginalization	B
over	O
continuous	B
parameters	B
is	O
a	O
macho	B
activity	O
enjoyed	O
by	O
those	O
who	O
are	O
in	O
integration	O
this	O
chapter	O
uses	O
gamma	B
distributions	O
as	O
was	O
explained	O
in	O
the	O
previous	O
chapter	O
gamma	B
distributions	O
are	O
a	O
lot	O
like	O
gaussian	B
distributions	O
except	O
that	O
whereas	O
the	O
gaussian	B
goes	O
from	O
to	O
gamma	B
distributions	O
go	O
from	O
to	O
inferring	O
the	O
mean	B
and	O
variance	B
of	O
a	O
gaussian	B
distribution	B
we	O
discuss	O
again	O
the	O
one-dimensional	O
gaussian	B
distribution	B
parameterized	O
by	O
a	O
mean	B
and	O
a	O
standard	B
deviation	I
p	O
normalx	O
when	O
inferring	O
these	O
parameters	B
we	O
must	O
specify	O
their	O
prior	B
distribution	B
the	O
prior	B
gives	O
us	O
the	O
opportunity	O
to	O
include	O
knowledge	O
that	O
we	O
have	O
about	O
and	O
independent	O
experiments	O
or	O
on	O
theoretical	O
grounds	O
for	O
example	O
if	O
we	O
have	O
no	O
such	O
knowledge	O
then	O
we	O
can	O
construct	O
an	O
appropriate	O
prior	B
that	O
embodies	O
our	O
supposed	O
ignorance	B
in	O
section	B
we	O
assumed	O
a	O
uniform	O
prior	B
over	O
the	O
range	O
of	O
parameters	B
plotted	O
if	O
we	O
wish	O
to	O
be	O
able	O
to	O
perform	O
exact	O
marginalizations	O
it	O
may	O
be	O
useful	O
to	O
consider	O
conjugate	O
priors	O
these	O
are	O
priors	O
whose	O
functional	O
form	O
combines	O
naturally	O
with	O
the	O
likelihood	B
such	O
that	O
the	O
inferences	O
have	O
a	O
convenient	O
form	O
conjugate	O
priors	O
for	O
and	O
the	O
conjugate	B
prior	B
for	O
a	O
mean	B
is	O
a	O
gaussian	B
we	O
introduce	O
two	O
hyperparameters	O
and	O
which	O
parameterize	O
the	O
prior	B
on	O
and	O
write	O
p	O
in	O
the	O
limit	O
we	O
obtain	O
the	O
noninformative	B
prior	B
for	O
a	O
location	O
parameter	O
the	O
prior	B
this	O
is	O
noninformative	B
because	O
it	O
is	O
invariant	O
under	O
the	O
natural	B
reparameterization	O
c	O
the	O
prior	B
p	O
const	O
is	O
also	O
an	O
improper	B
prior	B
that	O
is	O
it	O
is	O
not	O
normalizable	O
the	O
conjugate	B
prior	B
for	O
a	O
standard	B
deviation	I
is	O
a	O
gamma	B
distribution	B
which	O
has	O
two	O
parameters	B
and	O
it	O
is	O
most	O
convenient	O
to	O
the	O
prior	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
density	B
of	O
the	O
inverse	O
variance	B
precision	B
parameter	O
p	O
b	O
this	O
is	O
a	O
simple	O
peaked	O
distribution	B
with	O
mean	B
and	O
variance	B
in	O
the	O
limit	O
we	O
obtain	O
the	O
noninformative	B
prior	B
for	O
a	O
scale	O
parameter	O
the	O
prior	B
this	O
is	O
noninformative	B
because	O
it	O
is	O
invariant	O
under	O
the	O
reparameterization	O
the	O
prior	B
is	O
less	O
strange-looking	O
if	O
we	O
examine	O
the	O
resulting	O
density	B
over	O
ln	O
or	O
ln	O
which	O
is	O
this	O
is	O
the	O
prior	B
that	O
expresses	O
ignorance	B
about	O
by	O
saying	O
well	O
it	O
could	O
be	O
or	O
it	O
could	O
be	O
or	O
it	O
could	O
be	O
scale	O
variables	O
such	O
as	O
are	O
usually	O
best	O
represented	O
in	O
terms	O
of	O
their	O
logarithm	O
again	O
this	O
noninformative	B
prior	B
is	O
improper	B
in	O
the	O
following	O
examples	O
i	O
will	O
use	O
the	O
improper	B
noninformative	B
priors	O
for	O
and	O
using	O
improper	B
priors	O
is	O
viewed	O
as	O
distasteful	O
in	O
some	O
circles	O
so	O
let	O
me	O
excuse	O
myself	O
by	O
saying	O
it	O
s	O
for	O
the	O
sake	O
of	O
readability	O
if	O
i	O
included	O
proper	B
priors	O
the	O
calculations	O
could	O
still	O
be	O
done	O
but	O
the	O
key	O
points	O
would	O
be	O
obscured	O
by	O
the	O
of	O
extra	O
parameters	B
maximum	B
likelihood	B
and	O
marginalization	B
and	O
the	O
task	O
of	O
inferring	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
a	O
gaussian	B
distribution	B
from	O
n	O
samples	O
is	O
a	O
familiar	O
one	O
though	O
maybe	O
not	O
everyone	O
understands	O
the	O
between	O
the	O
and	O
buttons	O
on	O
their	O
calculator	B
let	O
us	O
recap	O
the	O
formulae	O
then	O
derive	O
them	O
given	O
data	O
d	O
fxngn	O
an	O
estimator	B
of	O
is	O
reminder	O
when	O
we	O
change	O
variables	O
from	O
to	O
a	O
one-to-one	O
function	O
of	O
the	O
probability	B
density	B
transforms	O
from	O
to	O
here	O
the	O
jacobian	B
is	O
pll	O
ln	O
and	O
two	O
estimators	O
of	O
are	O
n	O
xnn	O
and	O
n	O
there	O
are	O
two	O
principal	O
paradigms	O
for	O
statistics	O
sampling	B
theory	I
and	O
bayesian	B
inference	B
in	O
sampling	B
theory	I
known	O
as	O
frequentist	O
or	O
orthodox	O
statistics	O
one	O
invents	O
estimators	O
of	O
quantities	O
of	O
interest	O
and	O
then	O
chooses	O
between	O
those	O
estimators	O
using	O
some	O
criterion	O
measuring	O
their	O
sampling	O
properties	O
there	O
is	O
no	O
clear	O
principle	O
for	O
deciding	O
which	O
criterion	O
to	O
use	O
to	O
measure	O
the	O
performance	O
of	O
an	O
estimator	B
nor	O
for	O
most	O
criteria	O
is	O
there	O
any	O
systematic	B
procedure	O
for	O
the	O
construction	B
of	O
optimal	B
estimators	O
in	O
bayesian	B
inference	B
in	O
contrast	O
once	O
we	O
have	O
made	O
explicit	O
all	O
our	O
assumptions	B
about	O
the	O
model	B
and	O
the	O
data	O
our	O
inferences	O
are	O
mechanical	O
whatever	O
question	O
we	O
wish	O
to	O
pose	O
the	O
rules	B
of	O
probability	B
theory	O
give	O
a	O
unique	O
answer	O
which	O
consistently	O
takes	O
into	O
account	O
all	O
the	O
given	O
information	B
human-designed	O
estimators	O
and	O
intervals	B
have	O
no	O
role	O
in	O
bayesian	B
inference	B
human	B
input	O
only	O
enters	O
into	O
the	O
important	O
tasks	O
of	O
designing	O
the	O
hypothesis	O
space	O
is	O
the	O
of	O
the	O
model	B
and	O
all	O
its	O
probability	B
distributions	O
and	O
out	O
how	O
to	O
do	O
the	O
computations	O
that	O
implement	O
inference	B
in	O
that	O
space	O
the	O
answers	O
to	O
our	O
questions	O
are	O
probability	B
distributions	O
over	O
the	O
quantities	O
of	O
interest	O
we	O
often	O
that	O
the	O
estimators	O
of	O
sampling	B
theory	I
emerge	O
automatically	O
as	O
modes	O
or	O
means	O
of	O
these	O
posterior	O
distributions	O
when	O
we	O
choose	O
a	O
simple	O
hypothesis	O
space	O
and	O
turn	O
the	O
handle	O
of	O
bayesian	B
inference	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
inferring	O
the	O
mean	B
and	O
variance	B
of	O
a	O
gaussian	B
distribution	B
figure	O
the	O
likelihood	B
function	O
for	O
the	O
parameters	B
of	O
a	O
gaussian	B
distribution	B
repeated	O
from	O
surface	O
plot	O
and	O
contour	O
plot	O
of	O
the	O
log	O
likelihood	B
as	O
a	O
function	O
of	O
and	O
the	O
data	B
set	B
of	O
n	O
points	O
had	O
mean	B
and	O
s	O
notice	O
that	O
the	O
maximum	O
is	O
skew	O
in	O
the	O
two	O
estimators	O
of	O
standard	B
deviation	I
have	O
values	O
and	O
the	O
posterior	B
probability	B
of	O
for	O
various	O
values	O
of	O
as	O
a	O
density	B
over	O
ln	O
the	O
posterior	B
probability	B
of	O
p	O
j	O
d	O
assuming	O
a	O
prior	B
on	O
obtained	O
by	O
projecting	O
the	O
probability	B
mass	O
in	O
onto	O
the	O
axis	O
the	O
maximum	O
of	O
p	O
j	O
d	O
is	O
at	O
by	O
contrast	O
the	O
maximum	O
of	O
p	O
j	O
d	O
is	O
at	O
probabilities	O
are	O
shows	O
as	O
densities	O
over	O
ln	O
sigma	O
sigma	O
mean	B
mean	B
psigmad	O
in	O
sampling	B
theory	I
the	O
estimators	O
above	O
can	O
be	O
motivated	O
as	O
follows	O
is	O
an	O
unbiased	B
estimator	B
of	O
which	O
out	O
of	O
all	O
the	O
possible	O
unbiased	O
estimators	O
of	O
has	O
smallest	O
variance	B
this	O
variance	B
is	O
computed	O
by	O
averaging	O
over	O
an	O
ensemble	B
of	O
imaginary	O
experiments	O
in	O
which	O
the	O
data	O
samples	O
are	O
assumed	O
to	O
come	O
from	O
an	O
unknown	O
gaussian	B
distribution	B
the	O
estimator	B
n	O
is	O
the	O
maximum	B
likelihood	B
estimator	B
for	O
the	O
estimator	B
is	O
biased	O
however	O
the	O
expectation	B
of	O
given	O
averaging	O
over	O
many	O
imagined	O
experiments	O
is	O
not	O
exercise	O
give	O
an	O
intuitive	O
explanation	O
why	O
the	O
estimator	B
is	O
biased	O
this	O
bias	B
motivates	O
the	O
invention	O
in	O
sampling	B
theory	I
of	O
which	O
can	O
be	O
shown	O
to	O
be	O
an	O
unbiased	B
estimator	B
or	O
to	O
be	O
precise	O
it	O
is	O
that	O
is	O
an	O
unbiased	B
estimator	B
of	O
we	O
now	O
look	O
at	O
some	O
bayesian	B
inferences	O
for	O
this	O
problem	O
assuming	O
noninformative	B
priors	O
for	O
and	O
the	O
emphasis	O
is	O
thus	O
not	O
on	O
the	O
priors	O
but	O
rather	O
on	O
the	O
likelihood	B
function	O
and	O
the	O
concept	O
of	O
marginalization	B
the	O
joint	B
posterior	B
probability	B
of	O
and	O
is	O
proportional	O
to	O
the	O
likelihood	B
function	O
illustrated	O
by	O
a	O
contour	O
plot	O
in	O
the	O
log	O
likelihood	B
is	O
ln	O
p	O
j	O
where	O
s	O
pnxn	O
given	O
the	O
gaussian	B
model	B
the	O
likelihood	B
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
two	O
functions	B
of	O
the	O
data	O
and	O
s	O
so	O
these	O
two	O
quantities	O
are	O
known	O
as	O
statistics	O
the	O
posterior	B
probability	B
of	O
and	O
is	O
using	O
the	O
improper	B
priors	O
p	O
jfxngn	O
p	O
j	O
p	O
n	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
this	O
function	O
describes	O
the	O
answer	O
to	O
the	O
question	O
given	O
the	O
data	O
and	O
the	O
noninformative	B
priors	O
what	O
might	O
and	O
be	O
it	O
may	O
be	O
of	O
interest	O
to	O
the	O
parameter	O
values	O
that	O
maximize	O
the	O
posterior	B
probability	B
though	O
it	O
should	O
be	O
emphasized	O
that	O
posterior	B
probability	B
maxima	O
have	O
no	O
fundamental	O
status	O
in	O
bayesian	B
inference	B
since	O
their	O
location	O
depends	O
on	O
the	O
choice	O
of	O
basis	O
here	O
we	O
choose	O
the	O
basis	O
ln	O
in	O
which	O
our	O
prior	B
is	O
so	O
that	O
the	O
posterior	B
probability	B
maximum	O
coincides	O
with	O
the	O
maximum	O
of	O
the	O
likelihood	B
as	O
we	O
saw	O
in	O
exercise	O
the	O
maximum	B
likelihood	B
solution	O
for	O
and	O
ln	O
is	O
there	O
is	O
more	O
to	O
the	O
posterior	O
distribution	B
than	O
just	O
its	O
mode	O
as	O
can	O
be	O
seen	O
in	O
the	O
likelihood	B
has	O
a	O
skew	O
peak	O
as	O
we	O
increase	O
the	O
width	O
of	O
the	O
conditional	B
distribution	B
of	O
increases	O
and	O
if	O
we	O
to	O
a	O
sequence	B
of	O
values	O
moving	O
away	O
from	O
the	O
sample	B
mean	B
we	O
obtain	O
a	O
sequence	B
of	O
conditional	B
distributions	O
over	O
whose	O
maxima	O
move	O
to	O
increasing	O
values	O
of	O
the	O
posterior	B
probability	B
of	O
given	O
is	O
p	O
p	O
j	O
p	O
j	O
we	O
note	O
the	O
familiar	O
scaling	B
of	O
the	O
error	B
bars	I
on	O
let	O
us	O
now	O
ask	O
the	O
question	O
given	O
the	O
data	O
and	O
the	O
noninformative	B
priors	O
what	O
might	O
be	O
this	O
question	O
from	O
the	O
one	O
we	O
asked	O
in	O
that	O
we	O
are	O
now	O
not	O
interested	O
in	O
this	O
parameter	O
must	O
therefore	O
be	O
marginalized	O
over	O
the	O
posterior	B
probability	B
of	O
is	O
p	O
jfxngn	O
p	O
j	O
p	O
the	O
data-dependent	O
term	O
p	O
j	O
appeared	O
earlier	O
as	O
the	O
normalizing	O
constant	O
in	O
equation	O
one	O
name	O
for	O
this	O
quantity	O
is	O
the	O
evidence	B
or	O
marginal	B
likelihood	B
for	O
we	O
obtain	O
the	O
evidence	B
for	O
by	O
integrating	O
out	O
a	O
noninformative	B
prior	B
p	O
constant	O
is	O
assumed	O
we	O
call	O
this	O
constant	O
so	O
that	O
we	O
can	O
think	O
of	O
the	O
prior	B
as	O
a	O
top-hat	O
prior	B
of	O
width	O
the	O
gaussian	B
integral	B
p	O
j	O
yields	O
j	O
p	O
ln	O
p	O
j	O
s	O
ln	O
the	O
two	O
terms	O
are	O
the	O
log	O
likelihood	B
the	O
log	O
likelihood	B
with	O
the	O
last	O
term	O
is	O
the	O
log	O
of	O
the	O
occam	B
factor	I
which	O
penalizes	O
smaller	O
values	O
of	O
will	O
discuss	O
occam	O
factors	O
more	O
in	O
chapter	O
when	O
we	O
the	O
log	O
evidence	B
with	O
respect	O
to	O
ln	O
to	O
the	O
most	O
probable	O
the	O
additional	O
volume	B
factor	O
shifts	O
the	O
maximum	O
from	O
to	O
intuitively	O
the	O
denominator	O
counts	O
the	O
number	O
of	O
noise	O
measurements	O
contained	O
in	O
the	O
quantity	O
s	O
the	O
sum	O
contains	O
n	O
residuals	O
squared	O
but	O
there	O
are	O
only	O
noise	O
measurements	O
because	O
the	O
determination	O
of	O
one	O
parameter	O
from	O
the	O
data	O
causes	O
one	O
dimension	O
of	O
noise	O
to	O
be	O
gobbled	O
up	O
in	O
unavoidable	O
in	O
the	O
terminology	O
of	O
classical	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
statistics	O
the	O
bayesian	B
s	O
best	O
guess	O
for	O
sets	O
measure	O
of	O
deviance	O
by	O
equal	O
to	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
n	O
figure	O
shows	O
the	O
posterior	B
probability	B
of	O
which	O
is	O
proportional	O
to	O
the	O
marginal	B
likelihood	B
this	O
may	O
be	O
contrasted	O
with	O
the	O
posterior	B
probability	B
of	O
with	O
to	O
its	O
most	O
probable	O
value	O
which	O
is	O
shown	O
in	O
and	O
d	O
the	O
inference	B
we	O
might	O
wish	O
to	O
make	O
is	O
given	O
the	O
data	O
what	O
is	O
exercise	O
marginalize	O
over	O
and	O
obtain	O
the	O
posterior	O
marginal	B
distri	O
bution	O
of	O
which	O
is	O
a	O
student-t	B
distribution	B
p	O
d	O
further	O
reading	O
a	O
bible	O
of	O
exact	O
marginalization	B
is	O
bretthorst	O
s	O
book	O
on	O
bayesian	B
spectrum	O
analysis	O
and	O
parameter	O
estimation	O
exercises	O
exercise	O
exercise	O
requires	O
macho	B
integration	O
capabilities	O
give	O
a	O
bayesian	B
solution	O
to	O
exercise	O
where	O
seven	O
scientists	B
of	O
varying	O
capabilities	O
have	O
measured	O
with	O
personal	O
noise	O
levels	O
and	O
we	O
are	O
interested	O
in	O
inferring	O
let	O
the	O
prior	B
on	O
each	O
be	O
a	O
broad	O
prior	B
for	O
example	O
a	O
gamma	B
distribution	B
with	O
parameters	B
c	O
find	O
the	O
posterior	O
distribution	B
of	O
plot	O
it	O
and	O
explore	B
its	O
properties	O
for	O
a	O
variety	O
of	O
data	O
sets	O
such	O
as	O
the	O
one	O
given	O
and	O
the	O
data	B
set	B
fxng	O
the	O
posterior	O
distribution	B
of	O
given	O
and	O
xn	O
p	O
j	O
xn	O
note	O
that	O
the	O
normalizing	O
constant	O
for	O
this	O
inference	B
is	O
p	O
j	O
marginalize	O
over	O
to	O
this	O
normalizing	O
constant	O
then	O
use	O
bayes	B
theorem	O
a	O
second	O
time	O
to	O
p	O
solutions	O
solution	O
to	O
exercise	O
the	O
data	O
points	O
are	O
distributed	O
with	O
mean	B
squared	O
deviation	O
about	O
the	O
true	O
mean	B
the	O
sample	B
mean	B
is	O
unlikely	O
to	O
exactly	O
equal	O
the	O
true	O
mean	B
the	O
sample	B
mean	B
is	O
the	O
value	O
of	O
that	O
minimizes	O
the	O
sum	O
squared	O
deviation	O
of	O
the	O
data	O
points	O
from	O
any	O
other	O
value	O
of	O
particular	O
the	O
true	O
value	O
of	O
will	O
have	O
a	O
larger	O
value	O
of	O
the	O
sum-squared	O
deviation	O
that	O
so	O
the	O
expected	O
mean	B
squared	O
deviation	O
from	O
the	O
sample	B
mean	B
is	O
neces	O
sarily	O
smaller	O
than	O
the	O
mean	B
squared	O
deviation	O
about	O
the	O
true	O
mean	B
a	O
b	O
c	O
d-g	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
trellises	O
in	O
this	O
chapter	O
we	O
will	O
discuss	O
a	O
few	O
exact	O
methods	O
that	O
are	O
used	O
in	O
probabilistic	O
modelling	B
as	O
an	O
example	O
we	O
will	O
discuss	O
the	O
task	O
of	O
decoding	B
a	O
linear	B
error-correcting	B
code	I
we	O
will	O
see	O
that	O
inferences	O
can	O
be	O
conducted	O
most	O
by	O
message-passing	B
algorithms	B
which	O
take	O
advantage	O
of	O
the	O
graphical	O
structure	O
of	O
the	O
problem	O
to	O
avoid	O
unnecessary	O
duplication	O
of	O
computations	O
chapter	O
decoding	B
problems	O
a	O
codeword	B
t	O
is	O
selected	O
from	O
a	O
linear	B
k	O
code	O
c	O
and	O
it	O
is	O
transmitted	O
over	O
a	O
noisy	B
channel	O
the	O
received	O
signal	O
is	O
y	O
in	O
this	O
chapter	O
we	O
will	O
assume	O
that	O
the	O
channel	O
is	O
a	O
memoryless	O
channel	O
such	O
as	O
a	O
gaussian	B
channel	I
given	O
an	O
assumed	O
channel	O
model	B
p	O
j	O
t	O
there	O
are	O
two	O
decoding	B
problems	O
the	O
codeword	B
decoding	B
problem	O
is	O
the	O
task	O
of	O
inferring	O
which	O
codeword	B
t	O
was	O
transmitted	O
given	O
the	O
received	O
signal	O
the	O
bitwise	B
decoding	B
problem	O
is	O
the	O
task	O
of	O
inferring	O
for	O
each	O
transmit	O
ted	O
bit	B
tn	O
how	O
likely	O
it	O
is	O
that	O
that	O
bit	B
was	O
a	O
one	O
rather	O
than	O
a	O
zero	O
as	O
a	O
concrete	O
example	O
take	O
the	O
hamming	B
code	I
in	O
chapter	O
we	O
discussed	O
the	O
codeword	B
decoding	B
problem	O
for	O
that	O
code	O
assuming	O
a	O
binary	B
symmetric	B
channel	I
we	O
didn	O
t	O
discuss	O
the	O
bitwise	B
decoding	B
problem	O
and	O
we	O
didn	O
t	O
discuss	O
how	O
to	O
handle	O
more	O
general	O
channel	O
models	O
such	O
as	O
a	O
gaussian	B
channel	I
solving	O
the	O
codeword	B
decoding	B
problem	O
by	O
bayes	B
theorem	O
the	O
posterior	B
probability	B
of	O
the	O
codeword	B
t	O
is	O
p	O
y	O
p	O
j	O
tp	O
p	O
likelihood	B
function	O
the	O
factor	O
in	O
the	O
numerator	O
p	O
j	O
t	O
is	O
the	O
likelihood	B
of	O
the	O
codeword	B
which	O
for	O
any	O
memoryless	O
channel	O
is	O
a	O
separable	O
function	O
p	O
j	O
t	O
n	O
p	O
j	O
tn	O
for	O
example	O
if	O
the	O
channel	O
is	O
a	O
gaussian	B
channel	I
with	O
transmissions	O
and	O
additive	O
noise	O
of	O
standard	B
deviation	I
then	O
the	O
probability	B
density	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decoding	B
problems	O
of	O
the	O
received	O
signal	O
yn	O
in	O
the	O
two	O
cases	O
tn	O
is	O
p	O
j	O
tn	O
p	O
j	O
tn	O
from	O
the	O
point	O
of	O
view	O
of	O
decoding	B
all	O
that	O
matters	O
is	O
the	O
likelihood	B
ratio	O
which	O
for	O
the	O
case	O
of	O
the	O
gaussian	B
channel	I
is	O
p	O
j	O
tn	O
p	O
j	O
tn	O
exercise	O
show	O
that	O
from	O
the	O
point	O
of	O
view	O
of	O
decoding	B
a	O
gaussian	B
channel	I
is	O
equivalent	O
to	O
a	O
time-varying	O
binary	B
symmetric	B
channel	I
with	O
a	O
known	O
noise	O
level	O
fn	O
which	O
depends	O
on	O
n	O
prior	B
the	O
second	O
factor	O
in	O
the	O
numerator	O
is	O
the	O
prior	B
probability	B
of	O
the	O
codeword	B
p	O
which	O
is	O
usually	O
assumed	O
to	O
be	O
uniform	O
over	O
all	O
valid	O
codewords	O
the	O
denominator	O
in	O
is	O
the	O
normalizing	O
constant	O
p	O
p	O
j	O
tp	O
the	O
complete	O
solution	O
to	O
the	O
codeword	B
decoding	B
problem	O
is	O
a	O
list	O
of	O
all	O
codewords	O
and	O
their	O
probabilities	O
as	O
given	O
by	O
equation	O
since	O
the	O
number	O
of	O
codewords	O
in	O
a	O
linear	B
code	O
is	O
often	O
very	O
large	O
and	O
since	O
we	O
are	O
not	O
interested	O
in	O
knowing	O
the	O
detailed	O
probabilities	O
of	O
all	O
the	O
codewords	O
we	O
often	O
restrict	O
attention	O
to	O
a	O
version	O
of	O
the	O
codeword	B
decoding	B
problem	O
the	O
map	O
codeword	B
decoding	B
problem	O
is	O
the	O
task	O
of	O
identifying	O
the	O
most	O
probable	O
codeword	B
t	O
given	O
the	O
received	O
signal	O
if	O
the	O
prior	B
probability	B
over	O
codewords	O
is	O
uniform	O
then	O
this	O
task	O
is	O
identical	O
to	O
the	O
problem	O
of	O
maximum	B
likelihood	B
decoding	B
that	O
is	O
identifying	O
the	O
codeword	B
that	O
maximizes	O
p	O
j	O
t	O
example	O
in	O
chapter	O
for	O
the	O
hamming	B
code	I
and	O
a	O
binary	B
symmetric	B
channel	I
we	O
discussed	O
a	O
method	O
for	O
deducing	O
the	O
most	O
probable	O
codeword	B
from	O
the	O
syndrome	B
of	O
the	O
received	O
signal	O
thus	O
solving	O
the	O
map	O
codeword	B
decoding	B
problem	O
for	O
that	O
case	O
we	O
would	O
like	O
a	O
more	O
general	O
solution	O
the	O
map	O
codeword	B
decoding	B
problem	O
can	O
be	O
solved	O
in	O
exponential	B
time	O
order	O
by	O
searching	O
through	O
all	O
codewords	O
for	O
the	O
one	O
that	O
maximizes	O
p	O
j	O
tp	O
but	O
we	O
are	O
interested	O
in	O
methods	O
that	O
are	O
more	O
than	O
this	O
in	O
section	B
we	O
will	O
discuss	O
an	O
exact	O
method	O
known	O
as	O
the	O
minsum	O
algorithm	O
which	O
may	O
be	O
able	O
to	O
solve	O
the	O
codeword	B
decoding	B
problem	O
more	O
how	O
much	O
more	O
depends	O
on	O
the	O
properties	O
of	O
the	O
code	O
it	O
is	O
worth	O
emphasizing	O
that	O
map	O
codeword	B
decoding	B
for	O
a	O
general	O
linear	B
code	O
is	O
known	O
to	O
be	O
np-complete	B
means	O
in	O
layman	O
s	O
terms	O
that	O
map	O
codeword	B
decoding	B
has	O
a	O
complexity	B
that	O
scales	O
exponentially	O
with	O
the	O
blocklength	O
unless	O
there	O
is	O
a	O
revolution	O
in	O
computer	B
science	O
so	O
restricting	O
attention	O
to	O
the	O
map	O
decoding	B
problem	O
hasn	O
t	O
necessarily	O
made	O
the	O
task	O
much	O
less	O
challenging	O
it	O
simply	O
makes	O
the	O
answer	O
briefer	O
to	O
report	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
trellises	O
solving	O
the	O
bitwise	B
decoding	B
problem	O
formally	O
the	O
exact	O
solution	O
of	O
the	O
bitwise	B
decoding	B
problem	O
is	O
obtained	O
from	O
equation	O
by	O
marginalizing	O
over	O
the	O
other	O
bits	O
p	O
j	O
y	O
p	O
y	O
we	O
can	O
also	O
write	O
this	O
marginal	B
with	O
the	O
aid	O
of	O
a	O
truth	B
function	I
that	O
is	O
one	O
if	O
the	O
proposition	O
s	O
is	O
true	O
and	O
zero	O
otherwise	O
p	O
y	O
xt	O
p	O
y	O
xt	O
p	O
y	O
p	O
y	O
computing	O
these	O
marginal	B
probabilities	O
by	O
an	O
explicit	O
sum	O
over	O
all	O
codewords	O
t	O
takes	O
exponential	B
time	O
but	O
for	O
certain	O
codes	O
the	O
bitwise	B
decoding	B
problem	O
can	O
be	O
solved	O
much	O
more	O
using	O
the	O
forwardbackward	O
algorithm	O
we	O
will	O
describe	O
this	O
algorithm	O
which	O
is	O
an	O
example	O
of	O
the	O
sumproduct	O
algorithm	O
in	O
a	O
moment	O
both	O
the	O
minsum	O
algorithm	O
and	O
the	O
sumproduct	O
algorithm	O
have	O
widespread	O
importance	O
and	O
have	O
been	O
invented	O
many	O
times	O
in	O
many	O
codes	O
and	O
trellises	O
in	O
chapters	O
and	O
we	O
represented	O
linear	B
k	O
codes	O
in	O
terms	O
of	O
their	O
generator	O
matrices	B
and	O
their	O
parity-check	O
matrices	B
in	O
the	O
case	O
of	O
a	O
systematic	B
block	B
code	I
the	O
k	O
transmitted	O
bits	O
in	O
each	O
block	B
of	O
size	O
n	O
are	O
the	O
source	O
bits	O
and	O
the	O
remaining	O
m	O
n	O
bits	O
are	O
the	O
parity-check	B
bits	I
this	O
means	O
that	O
the	O
generator	B
matrix	B
of	O
the	O
code	O
can	O
be	O
written	O
repetition	B
code	I
simple	B
parity	B
code	O
p	O
gt	O
ik	O
and	O
the	O
parity-check	B
matrix	B
can	O
be	O
written	O
h	O
p	O
im	O
where	O
p	O
is	O
an	O
m	O
k	O
matrix	B
in	O
this	O
section	B
we	O
will	O
study	O
another	O
representation	O
of	O
a	O
linear	B
code	O
called	O
a	O
trellis	B
the	O
codes	O
that	O
these	O
trellises	O
represent	O
will	O
not	O
in	O
general	O
be	O
systematic	B
codes	O
but	O
they	O
can	O
be	O
mapped	O
onto	O
systematic	B
codes	O
if	O
desired	O
by	O
a	O
reordering	O
of	O
the	O
bits	O
in	O
a	O
block	B
hamming	B
code	I
figure	O
examples	O
of	O
trellises	O
each	O
edge	B
in	O
a	O
trellis	B
is	O
labelled	O
by	O
a	O
zero	O
by	O
a	O
square	B
or	O
a	O
one	O
by	O
a	O
cross	O
of	O
a	O
trellis	B
our	O
will	O
be	O
quite	O
narrow	O
for	O
a	O
more	O
comprehensive	O
view	O
of	O
trellises	O
the	O
reader	O
should	O
consult	O
kschischang	O
and	O
sorokine	O
a	O
trellis	B
is	O
a	O
graph	B
consisting	O
of	O
nodes	O
known	O
as	O
states	O
or	O
vertices	O
and	O
edges	O
the	O
nodes	O
are	O
grouped	O
into	O
vertical	O
slices	O
called	O
times	O
and	O
the	O
times	O
are	O
ordered	O
such	O
that	O
each	O
edge	B
connects	O
a	O
node	O
in	O
one	O
time	O
to	O
a	O
node	O
in	O
a	O
neighbouring	O
time	O
every	O
edge	B
is	O
labelled	O
with	O
a	O
symbol	O
the	O
leftmost	O
and	O
rightmost	O
states	O
contain	O
only	O
one	O
node	O
apart	O
from	O
these	O
two	O
extreme	O
nodes	O
all	O
nodes	O
in	O
the	O
trellis	B
have	O
at	O
least	O
one	O
edge	B
connecting	O
leftwards	O
and	O
at	O
least	O
one	O
connecting	O
rightwards	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solving	O
the	O
decoding	B
problems	O
on	O
a	O
trellis	B
a	O
trellis	B
with	O
n	O
times	O
a	O
code	O
of	O
blocklength	O
n	O
as	O
follows	O
a	O
codeword	B
is	O
obtained	O
by	O
taking	O
a	O
path	O
that	O
crosses	O
the	O
trellis	B
from	O
left	O
to	O
right	O
and	O
reading	O
out	O
the	O
symbols	O
on	O
the	O
edges	O
that	O
are	O
traversed	O
each	O
valid	O
path	O
through	O
the	O
trellis	B
a	O
codeword	B
we	O
will	O
number	O
the	O
leftmost	O
time	O
time	O
and	O
the	O
rightmost	O
time	O
n	O
we	O
will	O
number	O
the	O
leftmost	O
state	O
state	O
and	O
the	O
rightmost	O
state	O
i	O
where	O
i	O
is	O
the	O
total	O
number	O
of	O
states	O
in	O
the	O
trellis	B
the	O
nth	O
bit	B
of	O
the	O
codeword	B
is	O
emitted	O
as	O
we	O
move	O
from	O
time	O
to	O
time	O
n	O
the	O
width	O
of	O
the	O
trellis	B
at	O
a	O
given	O
time	O
is	O
the	O
number	O
of	O
nodes	O
in	O
that	O
time	O
the	O
maximal	O
width	O
of	O
a	O
trellis	B
is	O
what	O
it	O
sounds	O
like	O
a	O
trellis	B
is	O
called	O
a	O
linear	B
trellis	B
if	O
the	O
code	O
it	O
is	O
a	O
linear	B
code	O
we	O
will	O
solely	O
be	O
concerned	O
with	O
linear	B
trellises	O
from	O
now	O
on	O
as	O
nonlinear	B
trellises	O
are	O
much	O
more	O
complex	B
beasts	O
for	O
brevity	O
we	O
will	O
only	O
discuss	O
binary	O
trellises	O
that	O
is	O
trellises	O
whose	O
edges	O
are	O
labelled	O
with	O
zeroes	O
and	O
ones	O
it	O
is	O
not	O
hard	O
to	O
generalize	O
the	O
methods	O
that	O
follow	O
to	O
q-ary	O
trellises	O
figures	O
show	O
the	O
trellises	O
corresponding	O
to	O
the	O
repetition	B
code	I
which	O
has	O
k	O
the	O
parity	B
code	O
with	O
k	O
and	O
the	O
hamming	B
code	I
exercise	O
that	O
the	O
sixteen	O
codewords	O
listed	O
in	O
table	O
are	O
generated	O
by	O
the	O
trellis	B
shown	O
in	O
observations	O
about	O
linear	B
trellises	O
for	O
any	O
linear	B
code	O
the	O
minimal	O
trellis	B
is	O
the	O
one	O
that	O
has	O
the	O
smallest	O
number	O
of	O
nodes	O
in	O
a	O
minimal	O
trellis	B
each	O
node	O
has	O
at	O
most	O
two	O
edges	O
entering	O
it	O
and	O
at	O
most	O
two	O
edges	O
leaving	O
it	O
all	O
nodes	O
in	O
a	O
time	O
have	O
the	O
same	O
left	O
degree	B
as	O
each	O
other	O
and	O
they	O
have	O
the	O
same	O
right	O
degree	B
as	O
each	O
other	O
the	O
width	O
is	O
always	O
a	O
power	O
of	O
two	O
a	O
minimal	O
trellis	B
for	O
a	O
linear	B
k	O
code	O
cannot	O
have	O
a	O
width	O
greater	O
than	O
since	O
every	O
node	O
has	O
at	O
least	O
one	O
valid	O
codeword	B
through	O
it	O
and	O
there	O
are	O
only	O
codewords	O
furthermore	O
if	O
we	O
m	O
n	O
k	O
the	O
minimal	O
trellis	B
s	O
width	O
is	O
everywhere	O
less	O
than	O
this	O
will	O
be	O
proved	O
in	O
section	B
notice	O
that	O
for	O
the	O
linear	B
trellises	O
in	O
all	O
of	O
which	O
are	O
minimal	O
trellises	O
k	O
is	O
the	O
number	O
of	O
times	O
a	O
binary	O
branch	O
point	O
is	O
encountered	O
as	O
the	O
trellis	B
is	O
traversed	O
from	O
left	O
to	O
right	O
or	O
from	O
right	O
to	O
left	O
we	O
will	O
discuss	O
the	O
construction	B
of	O
trellises	O
more	O
in	O
section	B
but	O
we	O
now	O
know	O
enough	O
to	O
discuss	O
the	O
decoding	B
problem	O
solving	O
the	O
decoding	B
problems	O
on	O
a	O
trellis	B
we	O
can	O
view	O
the	O
trellis	B
of	O
a	O
linear	B
code	O
as	O
giving	O
a	O
causal	O
description	O
of	O
the	O
probabilistic	O
process	O
that	O
gives	O
rise	O
to	O
a	O
codeword	B
with	O
time	O
from	O
left	O
to	O
right	O
each	O
time	O
a	O
divergence	B
is	O
encountered	O
a	O
random	B
source	O
source	O
of	O
information	B
bits	O
for	O
communication	B
determines	O
which	O
way	O
we	O
go	O
at	O
the	O
receiving	O
end	O
we	O
receive	O
a	O
noisy	B
version	O
of	O
the	O
sequence	B
of	O
edgelabels	O
and	O
wish	O
to	O
infer	O
which	O
path	O
was	O
taken	O
or	O
to	O
be	O
precise	O
we	O
want	O
to	O
identify	O
the	O
most	O
probable	O
path	O
in	O
order	O
to	O
solve	O
the	O
codeword	B
decoding	B
problem	O
and	O
we	O
want	O
to	O
the	O
probability	B
that	O
the	O
transmitted	O
symbol	O
at	O
time	O
n	O
was	O
a	O
zero	O
or	O
a	O
one	O
to	O
solve	O
the	O
bitwise	B
decoding	B
problem	O
example	O
consider	O
the	O
case	O
of	O
a	O
single	O
transmission	O
from	O
the	O
hamming	B
trellis	B
shown	O
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
trellises	O
figure	O
posterior	O
probabilities	O
over	O
the	O
sixteen	O
codewords	O
when	O
the	O
received	O
vector	O
y	O
has	O
normalized	O
likelihoods	O
t	O
likelihood	B
posterior	B
probability	B
let	O
the	O
normalized	O
likelihoods	O
be	O
that	O
is	O
the	O
ratios	O
of	O
the	O
likelihoods	O
are	O
p	O
j	O
p	O
j	O
p	O
j	O
p	O
j	O
etc	O
how	O
should	O
this	O
received	O
signal	O
be	O
decoded	O
if	O
we	O
threshold	B
the	O
likelihoods	O
at	O
to	O
turn	O
the	O
signal	O
into	O
a	O
binary	O
received	O
vector	O
we	O
have	O
r	O
which	O
decodes	O
using	O
the	O
decoder	B
for	O
the	O
binary	B
symmetric	B
channel	I
into	O
this	O
is	O
not	O
the	O
optimal	B
decoding	B
procedure	O
optimal	B
inferences	O
are	O
always	O
obtained	O
by	O
using	O
bayes	B
theorem	O
we	O
can	O
the	O
posterior	B
probability	B
over	O
codewords	O
by	O
explicit	O
enumeration	O
of	O
all	O
sixteen	O
codewords	O
this	O
posterior	O
distribution	B
is	O
shown	O
in	O
of	O
course	O
we	O
aren	O
t	O
really	O
interested	O
in	O
such	O
brute-force	O
solutions	O
and	O
the	O
aim	O
of	O
this	O
chapter	O
is	O
to	O
understand	O
algorithms	B
for	O
getting	O
the	O
same	O
information	B
out	O
in	O
less	O
than	O
computer	B
time	O
examining	O
the	O
posterior	O
probabilities	O
we	O
notice	O
that	O
the	O
most	O
probable	O
codeword	B
is	O
actually	O
the	O
string	O
t	O
this	O
is	O
more	O
than	O
twice	O
as	O
probable	O
as	O
the	O
answer	O
found	O
by	O
thresholding	O
using	O
the	O
posterior	O
probabilities	O
shown	O
in	O
we	O
can	O
also	O
compute	O
the	O
posterior	O
marginal	B
distributions	O
of	O
each	O
of	O
the	O
bits	O
the	O
result	O
is	O
shown	O
in	O
notice	O
that	O
bits	O
and	O
are	O
all	O
quite	O
inferred	O
to	O
be	O
zero	O
the	O
strengths	O
of	O
the	O
posterior	O
probabilities	O
for	O
bits	O
and	O
are	O
not	O
so	O
great	O
in	O
the	O
above	O
example	O
the	O
map	O
codeword	B
is	O
in	O
agreement	O
with	O
the	O
bitwise	B
decoding	B
that	O
is	O
obtained	O
by	O
selecting	O
the	O
most	O
probable	O
state	O
for	O
each	O
bit	B
using	O
the	O
posterior	O
marginal	B
distributions	O
but	O
this	O
is	O
not	O
always	O
the	O
case	O
as	O
the	O
following	O
exercise	O
shows	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solving	O
the	O
decoding	B
problems	O
on	O
a	O
trellis	B
likelihood	B
posterior	O
marginals	O
p	O
j	O
tn	O
p	O
j	O
tn	O
p	O
y	O
p	O
y	O
figure	O
marginal	B
posterior	O
probabilities	O
for	O
the	O
bits	O
under	O
the	O
posterior	O
distribution	B
of	O
n	O
exercise	O
find	O
the	O
most	O
probable	O
codeword	B
in	O
the	O
case	O
where	O
the	O
normalized	O
likelihood	B
is	O
also	O
or	O
estimate	O
the	O
marginal	B
posterior	B
probability	B
for	O
each	O
of	O
the	O
seven	O
bits	O
and	O
give	O
the	O
bit-by-bit	O
decoding	B
concentrate	O
on	O
the	O
few	O
codewords	O
that	O
have	O
the	O
largest	O
probability	B
we	O
now	O
discuss	O
how	O
to	O
use	O
message	B
passing	I
on	O
a	O
code	O
s	O
trellis	B
to	O
solve	O
the	O
decoding	B
problems	O
the	O
minsum	O
algorithm	O
the	O
map	O
codeword	B
decoding	B
problem	O
can	O
be	O
solved	O
using	O
the	O
minsum	O
algorithm	O
that	O
was	O
introduced	O
in	O
section	B
each	O
codeword	B
of	O
the	O
code	O
corresponds	O
to	O
a	O
path	O
across	O
the	O
trellis	B
just	O
as	O
the	O
cost	O
of	O
a	O
journey	O
is	O
the	O
sum	O
of	O
the	O
costs	O
of	O
its	O
constituent	O
steps	O
the	O
log	O
likelihood	B
of	O
a	O
codeword	B
is	O
the	O
sum	O
of	O
the	O
bitwise	B
log	O
likelihoods	O
by	O
convention	O
we	O
the	O
sign	O
of	O
the	O
log	O
likelihood	B
we	O
would	O
like	O
to	O
maximize	O
and	O
talk	O
in	O
terms	O
of	O
a	O
cost	O
which	O
we	O
would	O
like	O
to	O
minimize	O
we	O
associate	O
with	O
each	O
edge	B
a	O
cost	O
p	O
j	O
tn	O
where	O
tn	O
is	O
the	O
transmitted	O
bit	B
associated	O
with	O
that	O
edge	B
and	O
yn	O
is	O
the	O
received	O
symbol	O
the	O
minsum	O
algorithm	O
presented	O
in	O
section	B
can	O
then	O
identify	O
the	O
most	O
probable	O
codeword	B
in	O
a	O
number	O
of	O
computer	B
operations	O
equal	O
to	O
the	O
number	O
of	O
edges	O
in	O
the	O
trellis	B
this	O
algorithm	O
is	O
also	O
known	O
as	O
the	O
viterbi	B
algorithm	I
the	O
sumproduct	O
algorithm	O
to	O
solve	O
the	O
bitwise	B
decoding	B
problem	O
we	O
can	O
make	O
a	O
small	O
to	O
the	O
minsum	O
algorithm	O
so	O
that	O
the	O
messages	O
passed	O
through	O
the	O
trellis	B
the	O
probability	B
of	O
the	O
data	O
up	O
to	O
the	O
current	O
point	O
instead	O
of	O
the	O
cost	O
of	O
the	O
best	O
route	O
to	O
this	O
point	O
we	O
replace	O
the	O
costs	O
on	O
the	O
edges	O
p	O
j	O
tn	O
by	O
the	O
likelihoods	O
themselves	O
p	O
j	O
tn	O
we	O
replace	O
the	O
min	O
and	O
sum	O
operations	O
of	O
the	O
minsum	O
algorithm	O
by	O
a	O
sum	O
and	O
product	O
respectively	O
let	O
i	O
run	O
over	O
nodesstates	O
i	O
be	O
the	O
label	O
for	O
the	O
start	O
state	O
pi	O
denote	O
the	O
set	B
of	O
states	O
that	O
are	O
parents	O
of	O
state	O
i	O
and	O
wij	O
be	O
the	O
likelihood	B
associated	O
with	O
the	O
edge	B
from	O
node	O
j	O
to	O
node	O
i	O
we	O
the	O
forward-pass	O
messages	O
by	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
trellises	O
these	O
messages	O
can	O
be	O
computed	O
sequentially	O
from	O
left	O
to	O
right	O
exercise	O
show	O
that	O
for	O
a	O
node	O
i	O
whose	O
time-coordinate	O
is	O
n	O
is	O
proportional	O
to	O
the	O
joint	B
probability	B
that	O
the	O
codeword	B
s	O
path	O
passed	O
through	O
node	O
i	O
and	O
that	O
the	O
n	O
received	O
symbols	O
were	O
yn	O
the	O
message	O
computed	O
at	O
the	O
end	O
node	O
of	O
the	O
trellis	B
is	O
proportional	O
to	O
the	O
marginal	B
probability	B
of	O
the	O
data	O
exercise	O
what	O
is	O
the	O
constant	O
of	O
proportionality	O
we	O
a	O
second	O
set	B
of	O
backward-pass	O
messages	O
in	O
a	O
similar	O
manner	O
let	O
node	O
i	O
be	O
the	O
end	O
node	O
these	O
messages	O
can	O
be	O
computed	O
sequentially	O
in	O
a	O
backward	B
pass	I
from	O
right	O
to	O
left	O
exercise	O
show	O
that	O
for	O
a	O
node	O
i	O
whose	O
time-coordinate	O
is	O
n	O
is	O
proportional	O
to	O
the	O
conditional	B
probability	B
given	O
that	O
the	O
codeword	B
s	O
path	O
passed	O
through	O
node	O
i	O
that	O
the	O
subsequent	O
received	O
symbols	O
were	O
yn	O
finally	O
to	O
the	O
probability	B
that	O
the	O
nth	O
bit	B
was	O
a	O
or	O
we	O
do	O
two	O
summations	O
of	O
products	O
of	O
the	O
forward	O
and	O
backward	O
messages	O
let	O
i	O
run	O
over	O
nodes	O
at	O
time	O
n	O
and	O
j	O
run	O
over	O
nodes	O
at	O
time	O
n	O
and	O
let	O
tij	O
be	O
the	O
value	O
of	O
tn	O
associated	O
with	O
the	O
trellis	B
edge	B
from	O
node	O
j	O
to	O
node	O
i	O
for	O
each	O
value	O
of	O
t	O
we	O
compute	O
rt	O
n	O
xij	O
tij	O
then	O
the	O
posterior	B
probability	B
that	O
tn	O
was	O
t	O
is	O
p	O
tj	O
y	O
z	O
n	O
where	O
the	O
normalizing	O
constant	O
z	O
forward	O
message	O
that	O
was	O
computed	O
earlier	O
exercise	O
that	O
the	O
above	O
sumproduct	O
algorithm	O
does	O
com	O
n	O
should	O
be	O
identical	O
to	O
the	O
rt	O
n	O
pute	O
p	O
tj	O
y	O
other	O
names	O
for	O
the	O
sumproduct	O
algorithm	O
presented	O
here	O
are	O
the	O
forward	O
backward	O
algorithm	O
the	O
bcjr	B
algorithm	I
and	O
belief	B
propagation	I
exercise	O
a	O
codeword	B
of	O
the	O
simple	B
parity	B
code	O
is	O
transmitted	O
and	O
the	O
received	O
signal	O
y	O
has	O
associated	O
likelihoods	O
shown	O
in	O
table	O
use	O
the	O
minsum	O
algorithm	O
and	O
the	O
sumproduct	O
algorithm	O
in	O
the	O
trellis	B
to	O
solve	O
the	O
map	O
codeword	B
decoding	B
problem	O
and	O
the	O
bitwise	B
decoding	B
problem	O
your	O
answers	O
by	O
enumeration	O
of	O
all	O
codewords	O
use	O
logs	O
to	O
base	O
and	O
do	O
the	O
minsum	O
computations	O
by	O
hand	O
when	O
working	O
the	O
sumproduct	O
algorithm	O
by	O
hand	O
you	O
may	O
it	O
helpful	O
to	O
use	O
three	O
colours	O
of	O
pen	O
one	O
for	O
the	O
one	O
for	O
the	O
ws	O
and	O
one	O
for	O
the	O
n	O
p	O
j	O
tn	O
tn	O
tn	O
table	O
bitwise	B
likelihoods	O
for	O
a	O
codeword	B
of	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
more	O
on	O
trellises	O
more	O
on	O
trellises	O
we	O
now	O
discuss	O
various	O
ways	O
of	O
making	O
the	O
trellis	B
of	O
a	O
code	O
you	O
may	O
safely	O
jump	O
over	O
this	O
section	B
the	O
span	B
of	O
a	O
codeword	B
is	O
the	O
set	B
of	O
bits	O
contained	O
between	O
the	O
bit	B
in	O
the	O
codeword	B
that	O
is	O
non-zero	O
and	O
the	O
last	O
bit	B
that	O
is	O
non-zero	O
inclusive	O
we	O
can	O
indicate	O
the	O
span	B
of	O
a	O
codeword	B
by	O
a	O
binary	O
vector	O
as	O
shown	O
in	O
table	O
codeword	B
span	B
table	O
some	O
codewords	O
and	O
their	O
spans	O
a	O
generator	B
matrix	B
is	O
in	O
trellis-oriented	O
form	O
if	O
the	O
spans	O
of	O
the	O
rows	O
of	O
the	O
generator	B
matrix	B
all	O
start	O
in	O
columns	O
and	O
the	O
spans	O
all	O
end	O
in	O
columns	O
how	O
to	O
make	O
a	O
trellis	B
from	O
a	O
generator	B
matrix	B
first	O
put	O
the	O
generator	B
matrix	B
into	O
trellis-oriented	O
form	O
by	O
row-manipulations	O
similar	O
to	O
gaussian	B
elimination	O
for	O
example	O
our	O
hamming	B
code	I
can	O
be	O
generated	O
by	O
g	O
but	O
this	O
matrix	B
is	O
not	O
in	O
trellis-oriented	O
form	O
for	O
example	O
rows	O
and	O
all	O
have	O
spans	O
that	O
end	O
in	O
the	O
same	O
column	O
by	O
subtracting	O
lower	O
rows	O
from	O
upper	O
rows	O
we	O
can	O
obtain	O
an	O
equivalent	O
generator	B
matrix	B
is	O
one	O
that	O
generates	O
the	O
same	O
set	B
of	O
codewords	O
as	O
follows	O
g	O
now	O
each	O
row	O
of	O
the	O
generator	B
matrix	B
can	O
be	O
thought	O
of	O
as	O
an	O
subcode	O
of	O
the	O
k	O
code	O
that	O
is	O
in	O
this	O
case	O
a	O
code	O
with	O
two	O
codewords	O
of	O
length	O
n	O
for	O
the	O
row	O
the	O
code	O
consists	O
of	O
the	O
two	O
codewords	O
and	O
the	O
subcode	O
by	O
the	O
second	O
row	O
consists	O
of	O
and	O
it	O
is	O
easy	O
to	O
construct	O
the	O
minimal	O
trellises	O
of	O
these	O
subcodes	O
they	O
are	O
shown	O
in	O
the	O
left	O
column	O
of	O
we	O
build	O
the	O
trellis	B
incrementally	O
as	O
shown	O
in	O
we	O
start	O
with	O
the	O
trellis	B
corresponding	O
to	O
the	O
subcode	O
given	O
by	O
the	O
row	O
of	O
the	O
generator	B
matrix	B
then	O
we	O
add	O
in	O
one	O
subcode	O
at	O
a	O
time	O
the	O
vertices	O
within	O
the	O
span	B
of	O
the	O
new	O
subcode	O
are	O
all	O
duplicated	O
the	O
edge	B
symbols	O
in	O
the	O
original	O
trellis	B
are	O
left	O
unchanged	O
and	O
the	O
edge	B
symbols	O
in	O
the	O
second	O
part	O
of	O
the	O
trellis	B
are	O
wherever	O
the	O
new	O
subcode	O
has	O
a	O
and	O
otherwise	O
left	O
alone	O
another	O
hamming	B
code	I
can	O
be	O
generated	O
by	O
g	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
trellises	O
figure	O
trellises	O
for	O
four	O
subcodes	O
of	O
the	O
hamming	B
code	I
column	O
and	O
the	O
sequence	B
of	O
trellises	O
that	O
are	O
made	O
when	O
constructing	O
the	O
trellis	B
for	O
the	O
hamming	B
code	I
column	O
each	O
edge	B
in	O
a	O
trellis	B
is	O
labelled	O
by	O
a	O
zero	O
by	O
a	O
square	B
or	O
a	O
one	O
by	O
a	O
cross	O
the	O
hamming	B
code	I
generated	O
by	O
this	O
matrix	B
by	O
a	O
permutation	B
of	O
its	O
bits	O
from	O
the	O
code	O
generated	O
by	O
the	O
systematic	B
matrix	B
used	O
in	O
chapter	O
and	O
above	O
the	O
parity-check	B
matrix	B
corresponding	O
to	O
this	O
permutation	B
is	O
h	O
the	O
trellis	B
obtained	O
from	O
the	O
permuted	O
matrix	B
g	O
given	O
in	O
equation	O
is	O
shown	O
in	O
notice	O
that	O
the	O
number	O
of	O
nodes	O
in	O
this	O
trellis	B
is	O
smaller	O
than	O
the	O
number	O
of	O
nodes	O
in	O
the	O
previous	O
trellis	B
for	O
the	O
hamming	B
code	I
in	O
we	O
thus	O
observe	O
that	O
rearranging	O
the	O
order	O
of	O
the	O
codeword	B
bits	O
can	O
sometimes	O
lead	O
to	O
smaller	O
simpler	O
trellises	O
trellises	O
from	O
parity-check	O
matrices	B
another	O
way	O
of	O
viewing	O
the	O
trellis	B
is	O
in	O
terms	O
of	O
the	O
syndrome	B
the	O
syndrome	B
of	O
a	O
vector	O
r	O
is	O
to	O
be	O
hr	O
where	O
h	O
is	O
the	O
parity-check	B
matrix	B
a	O
vector	O
is	O
only	O
a	O
codeword	B
if	O
its	O
syndrome	B
is	O
zero	O
as	O
we	O
generate	O
a	O
codeword	B
we	O
can	O
describe	O
the	O
current	O
state	O
by	O
the	O
partial	B
syndrome	B
that	O
is	O
the	O
product	O
of	O
h	O
with	O
the	O
codeword	B
bits	O
thus	O
far	O
generated	O
each	O
state	O
in	O
the	O
trellis	B
is	O
a	O
partial	B
syndrome	B
at	O
one	O
time	O
coordinate	O
the	O
starting	O
and	O
ending	O
states	O
are	O
both	O
constrained	B
to	O
be	O
the	O
zero	O
syndrome	B
each	O
node	O
in	O
a	O
state	O
represents	O
a	O
possible	O
value	O
for	O
the	O
partial	B
syndrome	B
since	O
h	O
is	O
an	O
m	O
n	O
matrix	B
where	O
m	O
n	O
k	O
the	O
syndrome	B
is	O
at	O
most	O
an	O
m	O
vector	O
so	O
we	O
need	O
at	O
most	O
nodes	O
in	O
each	O
state	O
we	O
can	O
construct	O
the	O
trellis	B
of	O
a	O
code	O
from	O
its	O
parity-check	B
matrix	B
by	O
walking	O
from	O
each	O
end	O
generating	O
two	O
trees	O
of	O
possible	O
syndrome	B
sequences	O
the	O
intersection	B
of	O
these	O
two	O
trees	O
the	O
trellis	B
of	O
the	O
code	O
in	O
the	O
pictures	O
we	O
obtain	O
from	O
this	O
construction	B
we	O
can	O
let	O
the	O
vertical	O
coordinate	O
represent	O
the	O
syndrome	B
then	O
any	O
horizontal	O
edge	B
is	O
necessarily	O
associated	O
with	O
a	O
zero	O
bit	B
only	O
a	O
non-zero	O
bit	B
changes	O
the	O
syndrome	B
figure	O
trellises	O
for	O
the	O
permuted	O
hamming	B
code	I
generated	O
from	O
the	O
generator	B
matrix	B
by	O
the	O
method	O
of	O
the	O
parity-check	B
matrix	B
by	O
the	O
method	O
on	O
page	O
each	O
edge	B
in	O
a	O
trellis	B
is	O
labelled	O
by	O
a	O
zero	O
by	O
a	O
square	B
or	O
a	O
one	O
by	O
a	O
cross	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
and	O
any	O
non-horizontal	O
edge	B
is	O
associated	O
with	O
a	O
one	O
bit	B
in	O
this	O
representation	O
we	O
no	O
longer	O
need	O
to	O
label	O
the	O
edges	O
in	O
the	O
trellis	B
figure	O
shows	O
the	O
trellis	B
corresponding	O
to	O
the	O
parity-check	B
matrix	B
of	O
equation	O
solutions	O
t	O
likelihood	B
posterior	B
probability	B
table	O
the	O
posterior	B
probability	B
over	O
codewords	O
for	O
exercise	O
solution	O
to	O
exercise	O
the	O
posterior	B
probability	B
over	O
codewords	O
is	O
shown	O
in	O
table	O
the	O
most	O
probable	O
codeword	B
is	O
the	O
marginal	B
posterior	O
probabilities	O
of	O
all	O
seven	O
bits	O
are	O
n	O
likelihood	B
posterior	O
marginals	O
p	O
j	O
tn	O
p	O
j	O
tn	O
p	O
y	O
p	O
y	O
so	O
the	O
bitwise	B
decoding	B
is	O
which	O
is	O
not	O
actually	O
a	O
codeword	B
solution	O
to	O
exercise	O
the	O
map	O
codeword	B
is	O
and	O
its	O
likelihood	B
is	O
the	O
normalizing	O
constant	O
of	O
the	O
sumproduct	O
algorithm	O
is	O
z	O
the	O
intermediate	O
are	O
left	O
to	O
right	O
the	O
intermediate	O
are	O
right	O
to	O
left	O
the	O
bitwise	B
decoding	B
is	O
p	O
y	O
p	O
y	O
p	O
y	O
the	O
codewords	O
probabilities	O
are	O
for	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
graphs	O
we	O
now	O
take	O
a	O
more	O
general	O
view	O
of	O
the	O
tasks	O
of	O
inference	B
and	O
marginalization	B
before	O
reading	O
this	O
chapter	O
you	O
should	O
read	O
about	O
message	B
passing	I
in	O
chapter	O
the	O
general	O
problem	O
assume	O
that	O
a	O
function	O
p	O
of	O
a	O
set	B
of	O
n	O
variables	O
x	O
fxngn	O
a	O
product	O
of	O
m	O
factors	O
as	O
follows	O
is	O
as	O
p	O
m	O
fmxm	O
each	O
of	O
the	O
factors	O
fmxm	O
is	O
a	O
function	O
of	O
a	O
subset	B
xm	O
of	O
the	O
variables	O
that	O
make	O
up	O
x	O
if	O
p	O
is	O
a	O
positive	O
function	O
then	O
we	O
may	O
be	O
interested	O
in	O
a	O
second	O
normalized	O
function	O
p	O
z	O
p	O
z	O
m	O
fmxm	O
where	O
the	O
normalizing	O
constant	O
z	O
is	O
by	O
z	O
m	O
fmxm	O
as	O
an	O
example	O
of	O
the	O
notation	B
we	O
ve	O
just	O
introduced	O
here	O
s	O
a	O
function	O
of	O
three	O
binary	O
variables	O
by	O
the	O
factors	O
or	O
or	O
or	O
or	O
p	O
p	O
z	O
the	O
subsets	O
of	O
denoted	O
by	O
xm	O
in	O
the	O
general	O
function	O
are	O
here	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
general	O
problem	O
g	O
g	O
g	O
figure	O
the	O
factor	B
graph	B
associated	O
with	O
the	O
function	O
p	O
the	O
function	O
p	O
by	O
the	O
way	O
may	O
be	O
recognized	O
as	O
the	O
posterior	B
probability	B
distribution	B
of	O
the	O
three	O
transmitted	O
bits	O
in	O
a	O
repetition	B
code	I
when	O
the	O
received	O
signal	O
is	O
r	O
and	O
the	O
channel	O
is	O
a	O
binary	B
symmetric	B
channel	I
with	O
probability	B
the	O
factors	O
and	O
respectively	O
enforce	O
the	O
constraints	O
that	O
and	O
must	O
be	O
identical	O
and	O
that	O
and	O
must	O
be	O
identical	O
the	O
factors	O
are	O
the	O
likelihood	B
functions	B
contributed	O
by	O
each	O
component	O
of	O
r	O
a	O
function	O
of	O
the	O
factored	O
form	O
can	O
be	O
depicted	O
by	O
a	O
factor	B
graph	B
in	O
which	O
the	O
variables	O
are	O
depicted	O
by	O
circular	O
nodes	O
and	O
the	O
factors	O
are	O
depicted	O
by	O
square	B
nodes	O
an	O
edge	B
is	O
put	O
between	O
variable	O
node	O
n	O
and	O
factor	O
node	O
m	O
if	O
the	O
function	O
fmxm	O
has	O
any	O
dependence	O
on	O
variable	O
xn	O
the	O
factor	B
graph	B
for	O
the	O
example	O
function	O
is	O
shown	O
in	O
the	O
normalization	O
problem	O
the	O
task	O
to	O
be	O
solved	O
is	O
to	O
compute	O
the	O
normalizing	O
constant	O
z	O
the	O
marginalization	B
problems	O
the	O
second	O
task	O
to	O
be	O
solved	O
is	O
to	O
compute	O
the	O
marginal	B
function	O
of	O
any	O
variable	O
xn	O
by	O
znxn	O
p	O
for	O
example	O
if	O
f	O
is	O
a	O
function	O
of	O
three	O
variables	O
then	O
the	O
marginal	B
for	O
n	O
is	O
by	O
f	O
this	O
type	O
of	O
summation	O
over	O
all	O
the	O
except	O
for	O
xn	O
is	O
so	O
important	O
that	O
it	O
can	O
be	O
useful	O
to	O
have	O
a	O
special	O
notation	B
for	O
it	O
the	O
not-sum	B
or	O
summary	B
the	O
third	O
task	O
to	O
be	O
solved	O
is	O
to	O
compute	O
the	O
normalized	O
marginal	B
of	O
any	O
variable	O
xn	O
by	O
pnxn	O
p	O
include	O
the	O
n	O
in	O
pnxn	O
departing	O
from	O
our	O
normal	B
practice	O
in	O
the	O
rest	O
of	O
the	O
book	O
where	O
we	O
would	O
omit	O
it	O
exercise	O
show	O
that	O
the	O
normalized	O
marginal	B
is	O
related	O
to	O
the	O
marginal	B
znxn	O
by	O
pnxn	O
znxn	O
z	O
we	O
might	O
also	O
be	O
interested	O
in	O
marginals	O
over	O
a	O
subset	B
of	O
the	O
variables	O
such	O
as	O
p	O
all	O
these	O
tasks	O
are	O
intractable	O
in	O
general	O
even	O
if	O
every	O
factor	O
is	O
a	O
function	O
of	O
only	O
three	O
variables	O
the	O
cost	O
of	O
computing	O
exact	O
solutions	O
for	O
z	O
and	O
for	O
the	O
marginals	O
is	O
believed	O
in	O
general	O
to	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
variables	O
n	O
for	O
certain	O
functions	B
p	O
however	O
the	O
marginals	O
can	O
be	O
computed	O
by	O
exploiting	O
the	O
factorization	O
of	O
p	O
the	O
idea	O
of	O
how	O
this	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
graphs	O
arises	O
is	O
well	O
illustrated	O
by	O
the	O
message-passing	B
examples	O
of	O
chapter	O
the	O
sumproduct	O
algorithm	O
that	O
we	O
now	O
review	O
is	O
a	O
generalization	B
of	O
messagepassing	O
rule-set	O
b	O
as	O
was	O
the	O
case	O
there	O
the	O
sumproduct	O
algorithm	O
is	O
only	O
valid	O
if	O
the	O
graph	B
is	O
tree-like	O
the	O
sumproduct	O
algorithm	O
notation	B
we	O
identify	O
the	O
set	B
of	O
variables	O
that	O
the	O
mth	O
factor	O
depends	O
on	O
xm	O
by	O
the	O
set	B
of	O
their	O
indices	O
n	O
for	O
our	O
example	O
function	O
the	O
sets	O
are	O
n	O
is	O
a	O
function	O
of	O
alone	O
n	O
n	O
n	O
and	O
n	O
similarly	O
we	O
the	O
set	B
of	O
factors	O
in	O
which	O
variable	O
n	O
participates	O
by	O
mn	O
we	O
denote	O
a	O
set	B
n	O
with	O
variable	O
n	O
excluded	O
by	O
n	O
we	O
introduce	O
the	O
shorthand	O
xmnn	O
or	O
xmnn	O
to	O
denote	O
the	O
set	B
of	O
variables	O
in	O
xm	O
with	O
xn	O
excluded	O
i	O
e	O
xmnn	O
n	O
the	O
sumproduct	O
algorithm	O
will	O
involve	O
messages	O
of	O
two	O
types	O
passing	O
along	O
the	O
edges	O
in	O
the	O
factor	B
graph	B
messages	O
qn	O
m	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
and	O
messages	O
rm	O
n	O
from	O
factor	O
nodes	O
to	O
variable	O
nodes	O
a	O
message	O
either	O
type	O
q	O
or	O
r	O
that	O
is	O
sent	O
along	O
an	O
edge	B
connecting	O
factor	O
fm	O
to	O
variable	O
xn	O
is	O
always	O
a	O
function	O
of	O
the	O
variable	O
xn	O
here	O
are	O
the	O
two	O
rules	B
for	O
the	O
updating	O
of	O
the	O
two	O
sets	O
of	O
messages	O
from	O
variable	O
to	O
factor	O
qn	O
mxn	O
from	O
factor	O
to	O
variable	O
rm	O
nxn	O
xxmnn	O
a	O
xn	O
rm	O
nxn	O
fmxn	O
fm	O
how	O
these	O
rules	B
apply	O
to	O
leaves	O
in	O
the	O
factor	B
graph	B
a	O
node	O
that	O
has	O
only	O
one	O
edge	B
connecting	O
it	O
to	O
another	O
node	O
is	O
called	O
a	O
leaf	B
node	O
some	O
factor	O
nodes	O
in	O
the	O
graph	B
may	O
be	O
connected	O
to	O
only	O
one	O
variable	O
node	O
in	O
which	O
case	O
the	O
set	B
n	O
of	O
variables	O
appearing	O
in	O
the	O
factor	O
message	O
update	O
is	O
an	O
empty	O
set	B
and	O
the	O
product	O
of	O
functions	B
is	O
the	O
empty	O
product	O
whose	O
value	O
is	O
such	O
a	O
fac	O
tor	O
node	O
therefore	O
always	O
broadcasts	O
to	O
its	O
one	O
neighbour	O
xn	O
the	O
message	O
rm	O
nxn	O
fmxn	O
similarly	O
there	O
may	O
be	O
variable	O
nodes	O
that	O
are	O
connected	O
to	O
only	O
one	O
factor	O
node	O
so	O
the	O
set	B
mnnm	O
in	O
is	O
empty	O
these	O
nodes	O
perpetually	O
broadcast	B
the	O
message	O
qn	O
mxn	O
starting	O
and	O
method	O
the	O
algorithm	O
can	O
be	O
initialized	O
in	O
two	O
ways	O
if	O
the	O
graph	B
is	O
tree-like	O
then	O
it	O
must	O
have	O
nodes	O
that	O
are	O
leaves	O
these	O
leaf	B
nodes	O
can	O
broadcast	B
their	O
figure	O
a	O
factor	O
node	O
that	O
is	O
a	O
leaf	B
node	O
perpetually	O
sends	O
the	O
message	O
rm	O
nxn	O
fmxn	O
to	O
its	O
one	O
neighbour	O
xn	O
xn	O
qn	O
mxn	O
fm	O
figure	O
a	O
variable	O
node	O
that	O
is	O
a	O
leaf	B
node	O
perpetually	O
sends	O
the	O
message	O
qn	O
mxn	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
sumproduct	O
algorithm	O
messages	O
to	O
their	O
respective	O
neighbours	O
from	O
the	O
start	O
for	O
all	O
leaf	B
variable	O
nodes	O
n	O
for	O
all	O
leaf	B
factor	O
nodes	O
m	O
qn	O
mxn	O
rm	O
nxn	O
fmxn	O
we	O
can	O
then	O
adopt	O
the	O
procedure	O
used	O
in	O
chapter	O
s	O
message-passing	B
ruleset	O
b	O
a	O
message	O
is	O
created	O
in	O
accordance	O
with	O
the	O
rules	B
only	O
if	O
all	O
the	O
messages	O
on	O
which	O
it	O
depends	O
are	O
present	O
for	O
example	O
in	O
the	O
message	O
from	O
to	O
will	O
be	O
sent	O
only	O
when	O
the	O
message	O
from	O
to	O
has	O
been	O
received	O
and	O
the	O
message	O
from	O
to	O
can	O
be	O
sent	O
only	O
when	O
the	O
messages	O
and	O
have	O
both	O
been	O
received	O
messages	O
will	O
thus	O
through	O
the	O
tree	B
one	O
in	O
each	O
direction	O
along	O
every	O
edge	B
and	O
after	O
a	O
number	O
of	O
steps	O
equal	O
to	O
the	O
diameter	O
of	O
the	O
graph	B
every	O
message	O
will	O
have	O
been	O
created	O
the	O
answers	O
we	O
require	O
can	O
then	O
be	O
read	O
out	O
the	O
marginal	B
function	O
of	O
xn	O
is	O
obtained	O
by	O
multiplying	O
all	O
the	O
incoming	O
messages	O
at	O
that	O
node	O
znxn	O
rm	O
nxn	O
g	O
g	O
g	O
figure	O
our	O
model	B
factor	B
graph	B
for	O
the	O
function	O
p	O
the	O
normalizing	O
constant	O
z	O
can	O
be	O
obtained	O
by	O
summing	O
any	O
marginal	B
function	O
z	O
znxn	O
and	O
the	O
normalized	O
marginals	O
obtained	O
from	O
znxn	O
z	O
pnxn	O
exercise	O
apply	O
the	O
sumproduct	O
algorithm	O
to	O
the	O
function	O
in	O
equation	O
and	O
check	O
that	O
the	O
normalized	O
marginals	O
are	O
consistent	O
with	O
what	O
you	O
know	O
about	O
the	O
repetition	B
code	I
exercise	O
prove	O
that	O
the	O
sumproduct	O
algorithm	O
correctly	O
computes	O
the	O
marginal	B
functions	B
znxn	O
if	O
the	O
graph	B
is	O
tree-like	O
exercise	O
describe	O
how	O
to	O
use	O
the	O
messages	O
computed	O
by	O
the	O
sum	O
product	O
algorithm	O
to	O
obtain	O
more	O
complicated	O
marginal	B
functions	B
in	O
a	O
tree-like	O
graph	B
for	O
example	O
for	O
two	O
variables	O
and	O
that	O
are	O
connected	O
to	O
one	O
common	O
factor	O
node	O
starting	O
and	O
method	O
alternatively	O
the	O
algorithm	O
can	O
be	O
initialized	O
by	O
setting	O
all	O
the	O
initial	O
messages	O
from	O
variables	O
to	O
for	O
all	O
n	O
m	O
qn	O
mxn	O
then	O
proceeding	O
with	O
the	O
factor	O
message	O
update	O
rule	O
alternating	O
with	O
the	O
variable	O
message	O
update	O
rule	O
compared	O
with	O
method	O
this	O
lazy	O
initialization	O
method	O
leads	O
to	O
a	O
load	O
of	O
wasted	O
computations	O
whose	O
results	O
are	O
gradually	O
out	O
by	O
the	O
correct	O
answers	O
computed	O
by	O
method	O
after	O
a	O
number	O
of	O
iterations	O
equal	O
to	O
the	O
diameter	O
of	O
the	O
factor	B
graph	B
the	O
algorithm	O
will	O
converge	O
to	O
a	O
set	B
of	O
messages	O
satisfying	O
the	O
sumproduct	O
relationships	O
exercise	O
apply	O
this	O
second	O
version	O
of	O
the	O
sumproduct	O
algorithm	O
to	O
the	O
function	O
in	O
equation	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
graphs	O
the	O
reason	O
for	O
introducing	O
this	O
lazy	O
method	O
is	O
that	O
method	O
it	O
can	O
be	O
applied	O
to	O
graphs	O
that	O
are	O
not	O
tree-like	O
when	O
the	O
sumproduct	O
algorithm	O
is	O
run	O
on	O
a	O
graph	B
with	O
cycles	O
the	O
algorithm	O
does	O
not	O
necessarily	O
converge	O
and	O
certainly	O
does	O
not	O
in	O
general	O
compute	O
the	O
correct	O
marginal	B
functions	B
but	O
it	O
is	O
nevertheless	O
an	O
algorithm	O
of	O
great	O
practical	B
importance	O
especially	O
in	O
the	O
decoding	B
of	O
sparse-graph	O
codes	O
sumproduct	O
algorithm	O
with	O
normalization	O
if	O
we	O
are	O
interested	O
in	O
only	O
the	O
normalized	O
marginals	O
then	O
another	O
version	O
of	O
the	O
sumproduct	O
algorithm	O
may	O
be	O
useful	O
the	O
factor-to-variable	O
messages	O
rm	O
n	O
are	O
computed	O
in	O
just	O
the	O
same	O
way	O
but	O
the	O
variable-to-factor	O
messages	O
are	O
normalized	O
thus	O
qn	O
mxn	O
where	O
is	O
a	O
scalar	O
chosen	O
such	O
that	O
qn	O
mxn	O
xxn	O
exercise	O
apply	O
this	O
normalized	O
version	O
of	O
the	O
sumproduct	O
algorithm	O
to	O
the	O
function	O
in	O
equation	O
and	O
a	O
factorization	O
view	O
of	O
the	O
sumproduct	O
algorithm	O
one	O
way	O
to	O
view	O
the	O
sumproduct	O
algorithm	O
is	O
that	O
it	O
reexpresses	O
the	O
original	O
fmxm	O
as	O
another	O
factored	O
function	O
the	O
product	O
of	O
m	O
factors	O
p	O
factored	O
function	O
which	O
is	O
the	O
product	O
of	O
m	O
n	O
factors	O
p	O
m	O
n	O
nxn	O
each	O
factor	O
is	O
associated	O
with	O
a	O
factor	O
node	O
m	O
and	O
each	O
factor	O
nxn	O
is	O
associated	O
with	O
a	O
variable	O
node	O
initially	O
fmxm	O
and	O
nxn	O
each	O
time	O
a	O
factor-to-variable	O
message	O
rm	O
nxn	O
is	O
sent	O
the	O
factorization	O
is	O
updated	O
thus	O
nxn	O
rm	O
nxn	O
and	O
each	O
message	O
can	O
be	O
computed	O
in	O
terms	O
of	O
and	O
using	O
f	O
rm	O
nxn	O
a	O
rm	O
nxn	O
xxmnn	O
which	O
from	O
the	O
assignment	O
in	O
that	O
the	O
product	O
is	O
over	O
all	O
n	O
exercise	O
that	O
the	O
update	O
rules	B
are	O
equivalent	O
to	O
the	O
sumproduct	O
rules	B
so	O
nxn	O
eventually	O
becomes	O
the	O
marginal	B
znxn	O
this	O
factorization	O
viewpoint	O
applies	O
whether	O
or	O
not	O
the	O
graph	B
is	O
tree-like	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
minsum	O
algorithm	O
computational	O
tricks	O
normalization	O
is	O
a	O
good	B
idea	O
from	O
a	O
computational	O
point	O
of	O
view	O
because	O
if	O
p	O
is	O
a	O
product	O
of	O
many	O
factors	O
its	O
values	O
are	O
likely	O
to	O
be	O
very	O
large	O
or	O
very	O
small	O
another	O
useful	O
computational	O
trick	O
involves	O
passing	O
the	O
logarithms	B
of	O
the	O
messages	O
q	O
and	O
r	O
instead	O
of	O
q	O
and	O
r	O
themselves	O
the	O
computations	O
of	O
the	O
products	O
in	O
the	O
algorithm	O
are	O
then	O
replaced	O
by	O
simpler	O
additions	O
the	O
summations	O
in	O
of	O
course	O
become	O
more	O
to	O
carry	O
them	O
out	O
and	O
return	O
the	O
logarithm	O
we	O
need	O
to	O
compute	O
softmax	B
functions	B
like	O
l	O
but	O
this	O
computation	O
can	O
be	O
done	O
using	O
look-up	O
tables	O
along	O
with	O
the	O
observation	O
that	O
the	O
value	O
of	O
the	O
answer	O
l	O
is	O
typically	O
just	O
a	O
little	O
larger	O
than	O
maxi	O
li	O
if	O
we	O
store	O
in	O
look-up	O
tables	O
values	O
of	O
the	O
function	O
negative	O
then	O
l	O
can	O
be	O
computed	O
exactly	O
in	O
a	O
number	O
of	O
look-ups	O
and	O
additions	O
scaling	B
as	O
the	O
number	O
of	O
terms	O
in	O
the	O
sum	O
if	O
look-ups	O
and	O
sorting	O
operations	O
are	O
cheaper	O
than	O
exp	O
then	O
this	O
approach	O
costs	O
less	O
than	O
the	O
direct	O
evaluation	O
the	O
number	O
of	O
operations	O
can	O
be	O
further	O
reduced	O
by	O
omitting	O
negligible	O
contributions	O
from	O
the	O
smallest	O
of	O
the	O
flig	O
a	O
third	O
computational	O
trick	O
applicable	O
to	O
certain	O
error-correcting	B
codes	I
is	O
to	O
pass	O
not	O
the	O
messages	O
but	O
the	O
fourier	O
transforms	O
of	O
the	O
messages	O
this	O
again	O
makes	O
the	O
computations	O
of	O
the	O
factor-to-variable	O
messages	O
quicker	O
a	O
simple	O
example	O
of	O
this	O
fourier	B
transform	I
trick	O
is	O
given	O
in	O
chapter	O
at	O
equation	O
the	O
minsum	O
algorithm	O
the	O
sumproduct	O
algorithm	O
solves	O
the	O
problem	O
of	O
the	O
marginal	B
function	O
of	O
a	O
given	O
product	O
p	O
this	O
is	O
analogous	O
to	O
solving	O
the	O
bitwise	B
decoding	B
problem	O
of	O
section	B
and	O
just	O
as	O
there	O
were	O
other	O
decoding	B
problems	O
example	O
the	O
codeword	B
decoding	B
problem	O
we	O
can	O
other	O
tasks	O
involving	O
p	O
that	O
can	O
be	O
solved	O
by	O
of	O
the	O
sumproduct	O
algorithm	O
for	O
example	O
consider	O
this	O
task	O
analogous	O
to	O
the	O
codeword	B
decoding	B
problem	O
the	O
maximization	O
problem	O
find	O
the	O
setting	O
of	O
x	O
that	O
maximizes	O
the	O
product	O
p	O
this	O
problem	O
can	O
be	O
solved	O
by	O
replacing	O
the	O
two	O
operations	O
add	O
and	O
multiply	O
everywhere	O
they	O
appear	O
in	O
the	O
sumproduct	O
algorithm	O
by	O
another	O
pair	O
of	O
operations	O
that	O
satisfy	O
the	O
distributive	O
law	O
namely	O
max	O
and	O
multiply	O
if	O
we	O
replace	O
summation	O
p	O
by	O
maximization	O
we	O
notice	O
that	O
the	O
quantity	O
formerly	O
known	O
as	O
the	O
normalizing	O
constant	O
p	O
z	O
becomes	O
maxx	O
p	O
thus	O
the	O
sumproduct	O
algorithm	O
can	O
be	O
turned	O
into	O
a	O
maxproduct	O
algorithm	O
that	O
computes	O
maxx	O
p	O
and	O
from	O
which	O
the	O
solution	O
of	O
the	O
maximization	O
problem	O
can	O
be	O
deduced	O
each	O
marginal	B
znxn	O
then	O
lists	O
the	O
maximum	O
value	O
that	O
p	O
can	O
attain	O
for	O
each	O
value	O
of	O
xn	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
marginalization	B
in	O
graphs	O
in	O
practice	O
the	O
maxproduct	O
algorithm	O
is	O
most	O
often	O
carried	O
out	O
in	O
the	O
negative	O
log	O
likelihood	B
domain	O
where	O
max	O
and	O
product	O
become	O
min	O
and	O
sum	O
the	O
minsum	O
algorithm	O
is	O
also	O
known	O
as	O
the	O
viterbi	B
algorithm	I
the	O
junction	B
tree	B
algorithm	I
what	O
should	O
one	O
do	O
when	O
the	O
factor	B
graph	B
one	O
is	O
interested	O
in	O
is	O
not	O
a	O
tree	B
there	O
are	O
several	O
options	O
and	O
they	O
divide	O
into	O
exact	O
methods	O
and	O
approximate	O
methods	O
the	O
most	O
widely	O
used	O
exact	O
method	O
for	O
handling	O
marginalization	B
on	O
graphs	O
with	O
cycles	O
is	O
called	O
the	O
junction	B
tree	B
algorithm	I
this	O
algorithm	O
works	O
by	O
agglomerating	O
variables	O
together	O
until	O
the	O
agglomerated	O
graph	B
has	O
no	O
cycles	O
you	O
can	O
probably	O
out	O
the	O
details	O
for	O
yourself	O
the	O
complexity	B
of	O
the	O
marginalization	B
grows	O
exponentially	O
with	O
the	O
number	O
of	O
agglomerated	O
variables	O
read	O
more	O
about	O
the	O
junction	B
tree	B
algorithm	I
in	O
jordan	B
there	O
are	O
many	O
approximate	O
methods	O
and	O
we	O
ll	O
visit	O
some	O
of	O
them	O
over	O
the	O
next	O
few	O
chapters	O
monte	B
carlo	I
methods	I
and	O
variational	B
methods	I
to	O
name	O
a	O
couple	O
however	O
the	O
most	O
amusing	O
way	O
of	O
handling	O
factor	O
graphs	O
to	O
which	O
the	O
sumproduct	O
algorithm	O
may	O
not	O
be	O
applied	O
is	O
as	O
we	O
already	O
mentioned	O
to	O
apply	O
the	O
sumproduct	O
algorithm	O
we	O
simply	O
compute	O
the	O
messages	O
for	O
each	O
node	O
in	O
the	O
graph	B
as	O
if	O
the	O
graph	B
were	O
a	O
tree	B
iterate	O
and	O
cross	O
our	O
this	O
so-called	O
loopy	B
message	B
passing	I
has	O
great	O
importance	O
in	O
the	O
decoding	B
of	O
error-correcting	B
codes	I
and	O
we	O
ll	O
come	O
back	O
to	O
it	O
in	O
section	B
and	O
part	O
vi	O
further	O
reading	O
for	O
further	O
reading	O
about	O
factor	O
graphs	O
and	O
the	O
sumproduct	O
algorithm	O
see	O
kschischang	O
et	O
al	O
yedidia	B
et	O
al	O
yedidia	B
et	O
al	O
yedidia	B
et	O
al	O
wainwright	B
et	O
al	O
and	O
forney	O
see	O
also	O
pearl	O
a	O
good	B
reference	O
for	O
the	O
fundamental	O
theory	O
of	O
graphical	O
models	O
is	O
lauritzen	O
a	O
readable	O
introduction	O
to	O
bayesian	B
networks	O
is	O
given	O
by	O
jensen	O
interesting	O
message-passing	B
algorithms	B
that	O
have	O
capabilities	O
from	O
the	O
sumproduct	O
algorithm	O
include	O
expectation	B
propagation	I
and	O
survey	B
propagation	I
et	O
al	O
see	O
also	O
section	B
exercises	O
exercise	O
express	O
the	O
joint	B
probability	B
distribution	B
from	O
the	O
burglar	B
alarm	I
and	I
earthquake	B
problem	O
as	O
a	O
factor	B
graph	B
and	O
the	O
marginal	B
probabilities	O
of	O
all	O
the	O
variables	O
as	O
each	O
piece	O
of	O
information	B
comes	O
to	O
fred	O
s	O
attention	O
using	O
the	O
sumproduct	O
algorithm	O
with	O
normalization	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
laplace	B
s	O
method	O
the	O
idea	O
behind	O
the	O
laplace	B
approximation	B
is	O
simple	O
we	O
assume	O
that	O
an	O
unnormalized	O
probability	B
density	B
p	O
whose	O
normalizing	O
constant	O
zp	O
p	O
dx	O
is	O
of	O
interest	O
has	O
a	O
peak	O
at	O
a	O
point	O
we	O
taylor-expand	O
the	O
logarithm	O
of	O
p	O
around	O
this	O
peak	O
ln	O
p	O
ln	O
p	O
c	O
where	O
c	O
ln	O
p	O
we	O
then	O
approximate	O
p	O
by	O
an	O
unnormalized	O
gaussian	B
p	O
c	O
p	O
ln	O
p	O
ln	O
p	O
ln	O
p	O
and	O
we	O
approximate	O
the	O
normalizing	O
constant	O
zp	O
by	O
the	O
normalizing	O
constant	O
of	O
this	O
gaussian	B
zq	O
p	O
c	O
we	O
can	O
generalize	O
this	O
integral	B
to	O
approximate	O
zp	O
for	O
a	O
density	B
p	O
over	O
a	O
k-dimensional	O
space	O
x	O
if	O
the	O
matrix	B
of	O
second	O
derivatives	O
of	O
ln	O
p	O
at	O
the	O
maximum	O
is	O
a	O
by	O
aij	O
ln	O
p	O
so	O
that	O
the	O
expansion	O
is	O
generalized	B
to	O
ln	O
p	O
ln	O
p	O
then	O
the	O
normalizing	O
constant	O
can	O
be	O
approximated	O
by	O
zp	O
zq	O
p	O
p	O
det	O
a	O
predictions	O
can	O
be	O
made	O
using	O
the	O
approximation	B
q	O
physicists	O
also	O
call	O
this	O
widely-used	O
approximation	B
the	O
saddle-point	B
approximation	B
qdet	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
laplace	B
s	O
method	O
the	O
fact	O
that	O
the	O
normalizing	O
constant	O
of	O
a	O
gaussian	B
is	O
given	O
by	O
z	O
dkx	O
det	O
a	O
can	O
be	O
proved	O
by	O
making	O
an	O
orthogonal	O
transformation	O
into	O
the	O
basis	O
u	O
in	O
which	O
a	O
is	O
transformed	O
into	O
a	O
diagonal	O
matrix	B
the	O
integral	B
then	O
separates	O
into	O
a	O
product	O
of	O
one-dimensional	O
integrals	O
each	O
of	O
the	O
form	O
z	O
dui	O
the	O
product	O
of	O
the	O
eigenvalues	O
is	O
the	O
determinant	O
of	O
a	O
the	O
laplace	B
approximation	B
is	O
basis-dependent	O
if	O
x	O
is	O
transformed	O
to	O
a	O
nonlinear	B
function	O
ux	O
and	O
the	O
density	B
is	O
transformed	O
to	O
p	O
p	O
then	O
in	O
general	O
the	O
approximate	O
normalizing	O
constants	O
zq	O
will	O
be	O
this	O
can	O
be	O
viewed	O
as	O
a	O
defect	O
since	O
the	O
true	O
value	O
zp	O
is	O
basis-independent	O
or	O
an	O
opportunity	O
because	O
we	O
can	O
hunt	O
for	O
a	O
choice	O
of	O
basis	O
in	O
which	O
the	O
laplace	B
approximation	B
is	O
most	O
accurate	O
exercises	O
exercise	O
also	O
exercise	O
a	O
photon	B
counter	I
is	O
pointed	O
at	O
a	O
remote	O
star	O
for	O
one	O
minute	O
in	O
order	O
to	O
infer	O
the	O
rate	B
of	O
photons	O
arriving	O
at	O
the	O
counter	O
per	O
minute	O
assuming	O
the	O
number	O
of	O
photons	O
collected	O
r	O
has	O
a	O
poisson	B
distribution	B
with	O
mean	B
p	O
j	O
r	O
and	O
assuming	O
the	O
improper	B
prior	B
p	O
make	O
laplace	B
approximations	O
to	O
the	O
posterior	O
distribution	B
over	O
over	O
log	O
constant	O
the	O
improper	B
prior	B
transforms	O
to	O
p	O
exercise	O
use	O
laplace	B
s	O
method	O
to	O
approximate	O
the	O
integral	B
da	O
f	O
f	O
where	O
f	O
and	O
are	O
positive	O
check	O
the	O
accuracy	O
of	O
the	O
approximation	B
against	O
the	O
exact	O
answer	O
for	O
and	O
measure	O
the	O
error	O
zp	O
log	O
zq	O
in	O
bits	O
exercise	O
linear	B
regression	B
n	O
datapoints	O
fxn	O
tng	O
are	O
generated	O
by	O
the	O
experimenter	O
choosing	O
each	O
xn	O
then	O
the	O
world	O
delivering	O
a	O
noisy	B
version	O
of	O
the	O
linear	B
function	O
yx	O
tn	O
normalyxn	O
assuming	O
gaussian	B
priors	O
on	O
and	O
make	O
the	O
laplace	B
approximation	B
to	O
the	O
posterior	O
distribution	B
of	O
and	O
is	O
exact	O
in	O
fact	O
and	O
obtain	O
the	O
predictive	B
distribution	B
for	O
the	O
next	O
datapoint	O
given	O
mackay	B
for	O
further	O
reading	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
occam	O
s	O
razor	O
how	O
many	O
boxes	O
are	O
in	O
the	O
picture	O
in	O
particular	O
how	O
many	O
boxes	O
are	O
in	O
the	O
vicinity	O
of	O
the	O
tree	B
if	O
we	O
looked	O
with	O
x-ray	O
spectacles	O
would	O
we	O
see	O
one	O
or	O
two	O
boxes	O
behind	O
the	O
trunk	O
even	O
more	O
occam	O
s	O
razor	O
is	O
the	O
principle	O
that	O
states	O
a	O
preference	O
for	O
simple	O
theories	O
accept	O
the	O
simplest	O
explanation	O
that	O
the	O
data	O
thus	O
according	O
to	O
occam	O
s	O
razor	O
we	O
should	O
deduce	O
that	O
there	O
is	O
only	O
one	O
box	B
behind	O
the	O
tree	B
is	O
this	O
an	O
ad	O
hoc	O
rule	B
of	I
thumb	I
or	O
is	O
there	O
a	O
convincing	O
reason	O
for	O
believing	O
there	O
is	O
most	O
likely	O
one	O
box	B
perhaps	O
your	O
intuition	O
likes	O
the	O
argument	O
well	O
it	O
would	O
be	O
a	O
remarkable	O
coincidence	B
for	O
the	O
two	O
boxes	O
to	O
be	O
just	O
the	O
same	O
height	O
and	O
colour	O
as	O
each	O
other	O
if	O
we	O
wish	O
to	O
make	O
intelligences	O
that	O
interpret	O
data	O
correctly	O
we	O
must	O
translate	O
this	O
intuitive	O
feeling	O
into	O
a	O
concrete	O
theory	O
motivations	O
for	O
occam	O
s	O
razor	O
if	O
several	O
explanations	O
are	O
compatible	O
with	O
a	O
set	B
of	O
observations	O
occam	O
s	O
razor	O
advises	O
us	O
to	O
buy	O
the	O
simplest	O
this	O
principle	O
is	O
often	O
advocated	O
for	O
one	O
of	O
two	O
reasons	O
the	O
is	O
aesthetic	O
a	O
theory	O
with	O
mathematical	O
beauty	O
is	O
more	O
likely	O
to	O
be	O
correct	O
than	O
an	O
ugly	O
one	O
that	O
some	O
experimental	O
data	O
figure	O
a	O
picture	O
to	O
be	O
interpreted	O
it	O
contains	O
a	O
tree	B
and	O
some	O
boxes	O
or	O
figure	O
how	O
many	O
boxes	O
are	O
behind	O
the	O
tree	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
figure	O
why	O
bayesian	B
inference	B
embodies	O
occam	O
s	O
razor	O
this	O
gives	O
the	O
basic	O
intuition	O
for	O
why	O
complex	B
models	O
can	O
turn	O
out	O
to	O
be	O
less	O
probable	O
the	O
horizontal	O
axis	O
represents	O
the	O
space	O
of	O
possible	O
data	O
sets	O
d	O
bayes	B
theorem	O
rewards	O
models	O
in	O
proportion	O
to	O
how	O
much	O
they	O
predicted	O
the	O
data	O
that	O
occurred	O
these	O
predictions	O
are	O
by	O
a	O
normalized	O
probability	B
distribution	B
on	O
d	O
this	O
probability	B
of	O
the	O
data	O
given	O
model	B
hi	O
p	O
jhi	O
is	O
called	O
the	O
evidence	B
for	O
hi	O
a	O
simple	O
model	B
makes	O
only	O
a	O
limited	O
range	O
of	O
predictions	O
shown	O
by	O
p	O
a	O
more	O
powerful	O
model	B
that	O
has	O
for	O
example	O
more	O
free	O
parameters	B
than	O
is	O
able	O
to	O
predict	O
a	O
greater	O
variety	O
of	O
data	O
sets	O
this	O
means	O
however	O
that	O
does	O
not	O
predict	O
the	O
data	O
sets	O
in	O
region	O
as	O
strongly	O
as	O
suppose	O
that	O
equal	O
prior	B
probabilities	O
have	O
been	O
assigned	O
to	O
the	O
two	O
models	O
then	O
if	O
the	O
data	B
set	B
falls	O
in	O
region	O
the	O
less	O
powerful	O
model	B
will	O
be	O
the	O
more	O
probable	O
model	B
evidence	B
pdh	O
pdh	O
c	O
d	O
dirac	O
the	O
second	O
reason	O
is	O
the	O
past	O
empirical	O
success	O
of	O
occam	O
s	O
razor	O
however	O
there	O
is	O
a	O
for	O
occam	O
s	O
razor	O
namely	O
coherent	O
inference	B
embodied	O
by	O
bayesian	B
probability	B
automatically	O
embodies	O
occam	O
s	O
razor	O
quantitatively	O
it	O
is	O
indeed	O
more	O
probable	O
that	O
there	O
s	O
one	O
box	B
behind	O
the	O
tree	B
and	O
we	O
can	O
compute	O
how	O
much	O
more	O
probable	O
one	O
is	O
than	O
two	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
we	O
evaluate	O
the	O
plausibility	O
of	O
two	O
alternative	O
theories	O
and	O
in	O
the	O
light	O
of	O
data	O
d	O
as	O
follows	O
using	O
bayes	B
theorem	O
we	O
relate	O
the	O
plausibility	O
of	O
model	B
given	O
the	O
data	O
p	O
j	O
d	O
to	O
the	O
predictions	O
made	O
by	O
the	O
model	B
about	O
the	O
data	O
p	O
and	O
the	O
prior	B
plausibility	O
of	O
p	O
this	O
gives	O
the	O
following	O
probability	B
ratio	O
between	O
theory	O
and	O
theory	O
p	O
j	O
d	O
p	O
j	O
d	O
p	O
p	O
p	O
p	O
the	O
ratio	O
on	O
the	O
right-hand	O
side	O
measures	O
how	O
much	O
our	O
initial	O
beliefs	O
favoured	O
over	O
the	O
second	O
ratio	O
expresses	O
how	O
well	O
the	O
observed	O
data	O
were	O
predicted	O
by	O
compared	O
to	O
how	O
does	O
this	O
relate	O
to	O
occam	O
s	O
razor	O
when	O
is	O
a	O
simpler	O
model	B
than	O
the	O
ratio	O
gives	O
us	O
the	O
opportunity	O
if	O
we	O
wish	O
to	O
insert	O
a	O
prior	B
bias	B
in	O
favour	O
of	O
on	O
aesthetic	O
grounds	O
or	O
on	O
the	O
basis	O
of	O
experience	O
this	O
would	O
correspond	O
to	O
the	O
aesthetic	O
and	O
empirical	O
motivations	O
for	O
occam	O
s	O
razor	O
mentioned	O
earlier	O
but	O
such	O
a	O
prior	B
bias	B
is	O
not	O
necessary	O
the	O
second	O
ratio	O
the	O
data-dependent	O
factor	O
embodies	O
occam	O
s	O
razor	O
automatically	O
simple	O
models	O
tend	O
to	O
make	O
precise	O
predictions	O
complex	B
models	O
by	O
their	O
nature	O
are	O
capable	O
of	O
making	O
a	O
greater	O
variety	O
of	O
predictions	O
so	O
if	O
is	O
a	O
more	O
complex	B
model	B
it	O
must	O
spread	O
its	O
predictive	O
probability	B
p	O
more	O
thinly	O
over	O
the	O
data	O
space	O
than	O
thus	O
in	O
the	O
case	O
where	O
the	O
data	O
are	O
compatible	O
with	O
both	O
theories	O
the	O
simpler	O
will	O
turn	O
out	O
more	O
probable	O
than	O
without	O
our	O
having	O
to	O
express	O
any	O
subjective	O
dislike	O
for	O
complex	B
models	O
our	O
subjective	O
prior	B
just	O
needs	O
to	O
assign	O
equal	O
prior	B
probabilities	O
to	O
the	O
possibilities	O
of	O
simplicity	O
and	O
complexity	B
probability	B
theory	O
then	O
allows	O
the	O
observed	O
data	O
to	O
express	O
their	O
opinion	O
let	O
us	O
turn	O
to	O
a	O
simple	O
example	O
here	O
is	O
a	O
sequence	B
of	O
numbers	O
the	O
task	O
is	O
to	O
predict	O
the	O
next	O
two	O
numbers	O
and	O
infer	O
the	O
underlying	O
process	O
that	O
gave	O
rise	O
to	O
this	O
sequence	B
a	O
popular	O
answer	O
to	O
this	O
question	O
is	O
the	O
prediction	B
with	O
the	O
explanation	O
add	O
to	O
the	O
previous	O
number	O
what	O
about	O
the	O
alternative	O
answer	O
with	O
the	O
underlying	O
rule	O
being	O
get	O
the	O
next	O
number	O
from	O
the	O
previous	O
number	O
x	O
by	O
evaluating	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
occam	O
s	O
razor	O
i	O
assume	O
that	O
this	O
prediction	B
seems	O
rather	O
less	O
plausible	O
but	O
the	O
second	O
rule	O
the	O
data	O
just	O
as	O
well	O
as	O
the	O
rule	O
add	O
so	O
why	O
should	O
we	O
it	O
less	O
plausible	O
let	O
us	O
give	O
labels	O
to	O
the	O
two	O
general	O
theories	O
ha	O
the	O
sequence	B
is	O
an	O
arithmetic	B
progression	I
add	O
n	O
where	O
n	O
is	O
an	O
integer	O
hc	O
the	O
sequence	B
is	O
generated	O
by	O
a	O
cubic	O
function	O
of	O
the	O
form	O
x	O
e	O
where	O
c	O
d	O
and	O
e	O
are	O
fractions	O
one	O
reason	O
for	O
the	O
second	O
explanation	O
hc	O
less	O
plausible	O
might	O
be	O
that	O
arithmetic	O
progressions	O
are	O
more	O
frequently	O
encountered	O
than	O
cubic	O
functions	B
this	O
would	O
put	O
a	O
bias	B
in	O
the	O
prior	B
probability	B
ratio	O
p	O
in	O
equation	O
but	O
let	O
us	O
give	O
the	O
two	O
theories	O
equal	O
prior	B
probabilities	O
and	O
concentrate	O
on	O
what	O
the	O
data	O
have	O
to	O
say	O
how	O
well	O
did	O
each	O
theory	O
predict	O
the	O
data	O
to	O
obtain	O
p	O
jha	O
we	O
must	O
specify	O
the	O
probability	B
distribution	B
that	O
each	O
model	B
assigns	O
to	O
its	O
parameters	B
first	O
ha	O
depends	O
on	O
the	O
added	O
integer	O
n	O
and	O
the	O
number	O
in	O
the	O
sequence	B
let	O
us	O
say	O
that	O
these	O
numbers	O
could	O
each	O
have	O
been	O
anywhere	O
between	O
and	O
then	O
since	O
only	O
the	O
pair	O
of	O
values	O
fn	O
number	O
give	O
rise	O
to	O
the	O
observed	O
data	O
d	O
the	O
probability	B
of	O
the	O
data	O
given	O
ha	O
is	O
p	O
jha	O
to	O
evaluate	O
p	O
jhc	O
we	O
must	O
similarly	O
say	O
what	O
values	O
the	O
fractions	O
c	O
d	O
and	O
e	O
might	O
take	O
on	O
choose	O
to	O
represent	O
these	O
numbers	O
as	O
fractions	O
rather	O
than	O
real	O
numbers	O
because	O
if	O
we	O
used	O
real	O
numbers	O
the	O
model	B
would	O
assign	O
relative	B
to	O
ha	O
an	O
probability	B
to	O
d	O
real	O
parameters	B
are	O
the	O
norm	O
however	O
and	O
are	O
assumed	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
a	O
reasonable	O
prior	B
might	O
state	O
that	O
for	O
each	O
fraction	O
the	O
numerator	O
could	O
be	O
any	O
number	O
between	O
and	O
and	O
the	O
denominator	O
is	O
any	O
number	O
between	O
and	O
as	O
for	O
the	O
initial	O
value	O
in	O
the	O
sequence	B
let	O
us	O
leave	O
its	O
probability	B
distribution	B
the	O
same	O
as	O
in	O
ha	O
there	O
are	O
four	O
ways	O
of	O
expressing	O
the	O
fraction	O
c	O
under	O
this	O
prior	B
and	O
similarly	O
there	O
are	O
four	O
and	O
two	O
possible	O
solutions	O
for	O
d	O
and	O
e	O
respectively	O
so	O
the	O
probability	B
of	O
the	O
observed	O
data	O
given	O
hc	O
is	O
found	O
to	O
be	O
p	O
jhc	O
thus	O
comparing	O
p	O
jhc	O
with	O
p	O
jha	O
even	O
if	O
our	O
prior	B
probabilities	O
for	O
ha	O
and	O
hc	O
are	O
equal	O
the	O
odds	B
p	O
jha	O
p	O
jhc	O
in	O
favour	O
of	O
ha	O
over	O
hc	O
given	O
the	O
sequence	B
d	O
are	O
about	O
forty	O
million	O
to	O
one	O
this	O
answer	O
depends	O
on	O
several	O
subjective	O
assumptions	B
in	O
particular	O
the	O
probability	B
assigned	O
to	O
the	O
free	O
parameters	B
n	O
c	O
d	O
e	O
of	O
the	O
theories	O
bayesians	O
make	O
no	O
apologies	O
for	O
this	O
there	O
is	O
no	O
such	O
thing	O
as	B
inference	B
or	O
prediction	B
without	O
assumptions	B
however	O
the	O
quantitative	O
details	O
of	O
the	O
prior	B
probabilities	O
have	O
no	O
on	O
the	O
qualitative	O
occam	O
s	O
razor	O
the	O
complex	B
theory	O
hc	O
always	O
an	O
occam	B
factor	I
because	O
it	O
has	O
more	O
parameters	B
and	O
so	O
can	O
predict	O
a	O
greater	O
variety	O
of	O
data	O
sets	O
this	O
was	O
only	O
a	O
small	O
example	O
and	O
there	O
were	O
only	O
four	O
data	O
points	O
as	O
we	O
move	O
to	O
larger	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
figure	O
where	O
bayesian	B
inference	B
into	O
the	O
data	O
modelling	B
process	O
this	O
illustrates	O
an	O
abstraction	O
of	O
the	O
part	O
of	O
the	O
process	O
in	O
which	O
data	O
are	O
collected	O
and	O
modelled	O
in	O
particular	O
this	O
applies	O
to	O
pattern	O
learning	B
interpolation	O
etc	O
the	O
two	O
double-framed	O
boxes	O
denote	O
the	O
two	O
steps	O
which	O
involve	O
inference	B
it	O
is	O
only	O
in	O
those	O
two	O
steps	O
that	O
bayes	B
theorem	O
can	O
be	O
used	O
bayes	B
does	O
not	O
tell	O
you	O
how	O
to	O
invent	O
models	O
for	O
example	O
the	O
box	B
each	O
model	B
to	O
the	O
data	O
is	O
the	O
task	O
of	O
inferring	O
what	O
the	O
model	B
parameters	B
might	O
be	O
given	O
the	O
model	B
and	O
the	O
data	O
bayesian	B
methods	O
may	O
be	O
used	O
to	O
the	O
most	O
probable	O
parameter	O
values	O
and	O
error	B
bars	I
on	O
those	O
parameters	B
the	O
result	O
of	O
applying	O
bayesian	B
methods	O
to	O
this	O
problem	O
is	O
often	O
little	O
from	O
the	O
answers	O
given	O
by	O
orthodox	O
statistics	O
the	O
second	O
inference	B
task	O
model	B
comparison	I
in	O
the	O
light	O
of	O
the	O
data	O
is	O
where	O
bayesian	B
methods	O
are	O
in	O
a	O
class	O
of	O
their	O
own	O
this	O
second	O
inference	B
problem	O
requires	O
a	O
quantitative	O
occam	O
s	O
razor	O
to	O
penalize	O
over-complex	O
models	O
bayesian	B
methods	O
can	O
assign	O
objective	O
preferences	O
to	O
the	O
alternative	O
models	O
in	O
a	O
way	O
that	O
automatically	O
embodies	O
occam	O
s	O
razor	O
gather	O
data	O
create	O
alternative	O
models	O
fit	O
each	O
model	B
to	O
the	O
data	O
gather	O
more	O
data	O
choose	O
what	O
data	O
to	O
gather	O
next	O
assign	O
preferences	O
to	O
the	O
alternative	O
models	O
choose	O
future	O
actions	O
create	O
new	O
models	O
decide	O
whether	O
to	O
create	O
new	O
models	O
and	O
more	O
sophisticated	O
problems	O
the	O
magnitude	O
of	O
the	O
occam	O
factors	O
typically	O
increases	O
and	O
the	O
degree	B
to	O
which	O
our	O
inferences	O
are	O
by	O
the	O
quantitative	O
details	O
of	O
our	O
subjective	O
assumptions	B
becomes	O
smaller	O
bayesian	B
methods	O
and	O
data	O
analysis	O
let	O
us	O
now	O
relate	O
the	O
discussion	O
above	O
to	O
real	O
problems	O
in	O
data	O
analysis	O
there	O
are	O
countless	O
problems	O
in	O
science	O
statistics	O
and	O
technology	O
which	O
require	O
that	O
given	O
a	O
limited	O
data	B
set	B
preferences	O
be	O
assigned	O
to	O
alternative	O
models	O
of	O
complexities	O
for	O
example	O
two	O
alternative	O
hypotheses	O
accounting	O
for	O
planetary	O
motion	O
are	O
mr	O
inquisition	B
s	O
geocentric	O
model	B
based	O
on	O
epicycles	B
and	O
mr	O
copernicus	B
s	O
simpler	O
model	B
of	O
the	O
solar	B
system	I
with	O
the	O
sun	O
at	O
the	O
centre	O
the	O
epicyclic	O
model	B
data	O
on	O
planetary	O
motion	O
at	O
least	O
as	O
well	O
as	O
the	O
copernican	O
model	B
but	O
does	O
so	O
using	O
more	O
parameters	B
coincidentally	O
for	O
mr	O
inquisition	B
two	O
of	O
the	O
extra	O
epicyclic	O
parameters	B
for	O
every	O
planet	O
are	O
found	O
to	O
be	O
identical	O
to	O
the	O
period	O
and	O
radius	O
of	O
the	O
sun	O
s	O
cycle	O
around	O
the	O
earth	O
intuitively	O
we	O
mr	O
copernicus	B
s	O
theory	O
more	O
probable	O
the	O
mechanism	O
of	O
the	O
bayesian	B
razor	O
the	O
evidence	B
and	O
the	O
occam	B
factor	I
two	O
levels	O
of	O
inference	B
can	O
often	O
be	O
distinguished	O
in	O
the	O
process	O
of	O
data	O
modelling	B
at	O
the	O
level	O
of	O
inference	B
we	O
assume	O
that	O
a	O
particular	O
model	B
is	O
true	O
and	O
we	O
that	O
model	B
to	O
the	O
data	O
i	O
e	O
we	O
infer	O
what	O
values	O
its	O
free	O
parameters	B
should	O
plausibly	O
take	O
given	O
the	O
data	O
the	O
results	O
of	O
this	O
inference	B
are	O
often	O
summarized	O
by	O
the	O
most	O
probable	O
parameter	O
values	O
and	O
error	B
bars	I
on	O
those	O
parameters	B
this	O
analysis	O
is	O
repeated	O
for	O
each	O
model	B
the	O
second	O
level	O
of	O
inference	B
is	O
the	O
task	O
of	O
model	B
comparison	I
here	O
we	O
wish	O
to	O
compare	O
the	O
models	O
in	O
the	O
light	O
of	O
the	O
data	O
and	O
assign	O
some	O
sort	O
of	O
preference	O
or	O
ranking	O
to	O
the	O
alternatives	O
note	O
that	O
both	O
levels	O
of	O
inference	B
are	O
distinct	O
from	O
decision	B
theory	I
the	O
goal	O
of	O
inference	B
is	O
given	O
a	O
hypothesis	O
space	O
and	O
a	O
particular	O
data	B
set	B
to	O
assign	O
probabilities	O
to	O
hypotheses	O
decision	B
theory	I
typically	O
chooses	O
between	O
alternative	O
actions	O
on	O
the	O
basis	O
of	O
these	O
probabilities	O
so	O
as	O
to	O
minimize	O
the	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
occam	O
s	O
razor	O
expectation	B
of	O
a	O
loss	B
function	I
this	O
chapter	O
concerns	O
inference	B
alone	O
and	O
no	O
loss	O
functions	B
are	O
involved	O
when	O
we	O
discuss	O
model	B
comparison	I
this	O
should	O
not	O
be	O
construed	O
as	O
implying	O
model	B
choice	O
ideal	O
bayesian	B
predictions	O
do	O
not	O
involve	O
choice	O
between	O
models	O
rather	O
predictions	O
are	O
made	O
by	O
summing	O
over	O
all	O
the	O
alternative	O
models	O
weighted	O
by	O
their	O
probabilities	O
bayesian	B
methods	O
are	O
able	O
consistently	O
and	O
quantitatively	O
to	O
solve	O
both	O
the	O
inference	B
tasks	O
there	O
is	O
a	O
popular	O
myth	B
that	O
states	O
that	O
bayesian	B
methods	O
from	O
orthodox	O
statistical	B
methods	O
only	O
by	O
the	O
inclusion	O
of	O
subjective	O
priors	O
which	O
are	O
to	O
assign	O
and	O
which	O
usually	O
don	O
t	O
make	O
much	O
difference	O
to	O
the	O
conclusions	O
it	O
is	O
true	O
that	O
at	O
the	O
level	O
of	O
inference	B
a	O
bayesian	B
s	O
results	O
will	O
often	O
little	O
from	O
the	O
outcome	O
of	O
an	O
orthodox	O
attack	O
what	O
is	O
not	O
widely	O
appreciated	O
is	O
how	O
a	O
bayesian	B
performs	O
the	O
second	O
level	O
of	O
inference	B
this	O
chapter	O
will	O
therefore	O
focus	B
on	O
bayesian	B
model	B
comparison	I
model	B
comparison	I
is	O
a	O
task	O
because	O
it	O
is	O
not	O
possible	O
simply	O
to	O
choose	O
the	O
model	B
that	O
the	O
data	O
best	O
more	O
complex	B
models	O
can	O
always	O
the	O
data	O
better	O
so	O
the	O
maximum	B
likelihood	B
model	B
choice	O
would	O
lead	O
us	O
inevitably	O
to	O
implausible	O
over-parameterized	O
models	O
which	O
generalize	O
poorly	O
occam	O
s	O
razor	O
is	O
needed	O
let	O
us	O
write	O
down	O
bayes	B
theorem	O
for	O
the	O
two	O
levels	O
of	O
inference	B
described	O
above	O
so	O
as	O
to	O
see	O
explicitly	O
how	O
bayesian	B
model	B
comparison	I
works	O
each	O
model	B
hi	O
is	O
assumed	O
to	O
have	O
a	O
vector	O
of	O
parameters	B
w	O
a	O
model	B
is	O
by	O
a	O
collection	O
of	O
probability	B
distributions	O
a	O
prior	B
distribution	B
p	O
jhi	O
which	O
states	O
what	O
values	O
the	O
model	B
s	O
parameters	B
might	O
be	O
expected	O
to	O
take	O
and	O
a	O
set	B
of	O
conditional	B
distributions	O
one	O
for	O
each	O
value	O
of	O
w	O
the	O
predictions	O
p	O
j	O
whi	O
that	O
the	O
model	B
makes	O
about	O
the	O
data	O
d	O
model	B
at	O
the	O
level	O
of	O
inference	B
we	O
assume	O
that	O
one	O
model	B
the	O
ith	O
say	O
is	O
true	O
and	O
we	O
infer	O
what	O
the	O
model	B
s	O
parameters	B
w	O
might	O
be	O
given	O
the	O
data	O
d	O
using	O
bayes	B
theorem	O
the	O
posterior	B
probability	B
of	O
the	O
parameters	B
w	O
is	O
p	O
j	O
dhi	O
p	O
j	O
whip	O
jhi	O
p	O
jhi	O
that	O
is	O
posterior	O
likelihood	B
prior	B
evidence	B
the	O
normalizing	O
constant	O
p	O
jhi	O
is	O
commonly	O
ignored	O
since	O
it	O
is	O
irrelevant	O
to	O
the	O
level	O
of	O
inference	B
i	O
e	O
the	O
inference	B
of	O
w	O
but	O
it	O
becomes	O
important	O
in	O
the	O
second	O
level	O
of	O
inference	B
and	O
we	O
name	O
it	O
the	O
evidence	B
for	O
hi	O
it	O
is	O
common	O
practice	O
to	O
use	O
gradient-based	O
methods	O
to	O
the	O
maximum	O
of	O
the	O
posterior	O
which	O
the	O
most	O
probable	O
value	O
for	O
the	O
parameters	B
wmp	O
it	O
is	O
then	O
usual	O
to	O
summarize	O
the	O
posterior	O
distribution	B
by	O
the	O
value	O
of	O
wmp	O
and	O
error	B
bars	I
or	O
intervals	B
on	O
these	O
parameters	B
error	B
bars	I
can	O
be	O
obtained	O
from	O
the	O
curvature	O
of	O
the	O
posterior	O
evaluating	O
the	O
hessian	B
at	O
wmp	O
a	O
ln	O
p	O
j	O
dhijwmp	O
and	O
taylor-expanding	O
the	O
log	O
posterior	B
probability	B
with	O
p	O
j	O
dhi	O
p	O
j	O
dhi	O
we	O
see	O
that	O
the	O
posterior	O
can	O
be	O
locally	O
approximated	O
as	O
a	O
gaussian	B
with	O
covariance	B
matrix	B
to	O
error	B
bars	I
this	O
approximation	B
is	O
good	B
or	O
not	O
will	O
depend	O
on	O
the	O
problem	O
we	O
are	O
solving	O
indeed	O
the	O
maximum	O
and	O
mean	B
of	O
the	O
posterior	O
distribution	B
have	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
figure	O
the	O
occam	B
factor	I
this	O
shows	O
the	O
quantities	O
that	O
determine	O
the	O
occam	B
factor	I
for	O
a	O
hypothesis	O
hi	O
having	O
a	O
single	O
parameter	O
w	O
the	O
prior	B
distribution	B
line	O
for	O
the	O
parameter	O
has	O
width	O
the	O
posterior	O
distribution	B
line	O
has	O
a	O
single	O
peak	O
at	O
wmp	O
with	O
characteristic	O
width	O
the	O
occam	B
factor	I
is	O
jhi	O
p	O
j	O
dhi	O
p	O
jhi	O
wmp	O
w	O
no	O
fundamental	O
status	O
in	O
bayesian	B
inference	B
they	O
both	O
change	O
under	O
nonlinear	B
reparameterizations	O
maximization	O
of	O
a	O
posterior	B
probability	B
is	O
useful	O
only	O
if	O
an	O
approximation	B
like	O
equation	O
gives	O
a	O
good	B
summary	B
of	O
the	O
distribution	B
model	B
comparison	I
at	O
the	O
second	O
level	O
of	O
inference	B
we	O
wish	O
to	O
infer	O
which	O
model	B
is	O
most	O
plausible	O
given	O
the	O
data	O
the	O
posterior	B
probability	B
of	O
each	O
model	B
is	O
p	O
j	O
d	O
p	O
jhip	O
notice	O
that	O
the	O
data-dependent	O
term	O
p	O
jhi	O
is	O
the	O
evidence	B
for	O
hi	O
which	O
appeared	O
as	O
the	O
normalizing	O
constant	O
in	O
the	O
second	O
term	O
p	O
is	O
the	O
subjective	O
prior	B
over	O
our	O
hypothesis	O
space	O
which	O
expresses	O
how	O
plausible	O
we	O
thought	O
the	O
alternative	O
models	O
were	O
before	O
the	O
data	O
arrived	O
assuming	O
that	O
we	O
choose	O
to	O
assign	O
equal	O
priors	O
p	O
to	O
the	O
alternative	O
models	O
models	O
hi	O
are	O
ranked	O
by	O
evaluating	O
the	O
evidence	B
the	O
normalizing	O
constant	O
p	O
p	O
jhip	O
has	O
been	O
omitted	O
from	O
equation	O
because	O
in	O
the	O
data-modelling	O
process	O
we	O
may	O
develop	O
new	O
models	O
after	O
the	O
data	O
have	O
arrived	O
when	O
an	O
inadequacy	O
of	O
the	O
models	O
is	O
detected	O
for	O
example	O
inference	B
is	O
open	O
ended	O
we	O
continually	O
seek	O
more	O
probable	O
models	O
to	O
account	O
for	O
the	O
data	O
we	O
gather	O
to	O
repeat	O
the	O
key	O
idea	O
to	O
rank	O
alternative	O
models	O
hi	O
a	O
bayesian	B
evaluates	O
the	O
evidence	B
p	O
jhi	O
this	O
concept	O
is	O
very	O
general	O
the	O
evidence	B
can	O
be	O
evaluated	O
for	O
parametric	O
and	O
non-parametric	O
models	O
alike	O
whatever	O
our	O
data-modelling	O
task	O
a	O
regression	B
problem	O
a	O
problem	O
or	O
a	O
density	B
estimation	O
problem	O
the	O
evidence	B
is	O
a	O
transportable	O
quantity	O
for	O
comparing	O
alternative	O
models	O
in	O
all	O
these	O
cases	O
the	O
evidence	B
naturally	O
embodies	O
occam	O
s	O
razor	O
evaluating	O
the	O
evidence	B
let	O
us	O
now	O
study	O
the	O
evidence	B
more	O
closely	O
to	O
gain	B
insight	O
into	O
how	O
the	O
bayesian	B
occam	O
s	O
razor	O
works	O
the	O
evidence	B
is	O
the	O
normalizing	O
constant	O
for	O
equation	O
p	O
jhi	O
p	O
j	O
whip	O
jhi	O
dw	O
for	O
many	O
problems	O
the	O
posterior	O
p	O
j	O
dhi	O
p	O
j	O
whip	O
jhi	O
has	O
a	O
strong	O
peak	O
at	O
the	O
most	O
probable	O
parameters	B
wmp	O
then	O
taking	O
for	O
simplicity	O
the	O
one-dimensional	O
case	O
the	O
evidence	B
can	O
be	O
approximated	O
using	O
laplace	B
s	O
method	O
by	O
the	O
height	O
of	O
the	O
peak	O
of	O
the	O
integrand	O
p	O
j	O
whip	O
jhi	O
times	O
its	O
width	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
occam	O
s	O
razor	O
p	O
jhi	O
evidence	B
best	O
likelihood	B
occam	B
factor	I
p	O
jhi	O
p	O
j	O
wmphi	O
thus	O
the	O
evidence	B
is	O
found	O
by	O
taking	O
the	O
likelihood	B
that	O
the	O
model	B
can	O
achieve	O
and	O
multiplying	O
it	O
by	O
an	O
occam	B
factor	I
which	O
is	O
a	O
term	O
with	O
magnitude	O
less	O
than	O
one	O
that	O
penalizes	O
hi	O
for	O
having	O
the	O
parameter	O
w	O
interpretation	O
of	O
the	O
occam	B
factor	I
the	O
quantity	O
is	O
the	O
posterior	O
uncertainty	O
in	O
w	O
suppose	O
for	O
simplicity	O
that	O
the	O
prior	B
p	O
jhi	O
is	O
uniform	O
on	O
some	O
large	O
interval	O
representing	O
the	O
range	O
of	O
values	O
of	O
w	O
that	O
were	O
possible	O
a	O
priori	O
according	O
to	O
hi	O
then	O
p	O
jhi	O
and	O
occam	B
factor	I
i	O
e	O
the	O
occam	B
factor	I
is	O
equal	O
to	O
the	O
ratio	O
of	O
the	O
posterior	O
accessible	O
volume	B
of	O
hi	O
s	O
parameter	O
space	O
to	O
the	O
prior	B
accessible	O
volume	B
or	O
the	O
factor	O
by	O
which	O
hi	O
s	O
hypothesis	O
space	O
collapses	O
when	O
the	O
data	O
arrive	O
the	O
model	B
hi	O
can	O
be	O
viewed	O
as	O
consisting	O
of	O
a	O
certain	O
number	O
of	O
exclusive	O
submodels	O
of	O
which	O
only	O
one	O
survives	O
when	O
the	O
data	O
arrive	O
the	O
occam	B
factor	I
is	O
the	O
inverse	O
of	O
that	O
number	O
the	O
logarithm	O
of	O
the	O
occam	B
factor	I
is	O
a	O
measure	O
of	O
the	O
amount	O
of	O
information	B
we	O
gain	B
about	O
the	O
model	B
s	O
parameters	B
when	O
the	O
data	O
arrive	O
a	O
complex	B
model	B
having	O
many	O
parameters	B
each	O
of	O
which	O
is	O
free	O
to	O
vary	O
over	O
a	O
large	O
range	O
will	O
typically	O
be	O
penalized	O
by	O
a	O
stronger	O
occam	B
factor	I
than	O
a	O
simpler	O
model	B
the	O
occam	B
factor	I
also	O
penalizes	O
models	O
that	O
have	O
to	O
be	O
tuned	O
to	O
the	O
data	O
favouring	O
models	O
for	O
which	O
the	O
required	O
precision	B
of	O
the	O
parameters	B
is	O
coarse	O
the	O
magnitude	O
of	O
the	O
occam	B
factor	I
is	O
thus	O
a	O
measure	O
of	O
complexity	B
of	O
the	O
model	B
it	O
relates	O
to	O
the	O
complexity	B
of	O
the	O
predictions	O
that	O
the	O
model	B
makes	O
in	O
data	O
space	O
this	O
depends	O
not	O
only	O
on	O
the	O
number	O
of	O
parameters	B
in	O
the	O
model	B
but	O
also	O
on	O
the	O
prior	B
probability	B
that	O
the	O
model	B
assigns	O
to	O
them	O
which	O
model	B
achieves	O
the	O
greatest	O
evidence	B
is	O
determined	O
by	O
a	O
between	O
minimizing	O
this	O
natural	B
complexity	B
measure	O
and	O
minimizing	O
the	O
data	O
in	O
contrast	O
to	O
alternative	O
measures	O
of	B
model	B
complexity	B
the	O
occam	B
factor	I
for	O
a	O
model	B
is	O
straightforward	O
to	O
evaluate	O
it	O
simply	O
depends	O
on	O
the	O
error	B
bars	I
on	O
the	O
parameters	B
which	O
we	O
already	O
evaluated	O
when	O
the	O
model	B
to	O
the	O
data	O
figure	O
displays	O
an	O
entire	O
hypothesis	O
space	O
so	O
as	O
to	O
illustrate	O
the	O
various	O
probabilities	O
in	O
the	O
analysis	O
there	O
are	O
three	O
models	O
which	O
have	O
equal	O
prior	B
probabilities	O
each	O
model	B
has	O
one	O
parameter	O
w	O
shown	O
on	O
a	O
horizontal	O
axis	O
but	O
assigns	O
a	O
prior	B
range	O
to	O
that	O
parameter	O
is	O
the	O
most	O
or	O
complex	B
model	B
assigning	B
the	O
broadest	O
prior	B
range	O
a	O
one-dimensional	O
data	O
space	O
is	O
shown	O
by	O
the	O
vertical	O
axis	O
each	O
model	B
assigns	O
a	O
joint	B
probability	B
distribution	B
p	O
w	O
jhi	O
to	O
the	O
data	O
and	O
the	O
parameters	B
illustrated	O
by	O
a	O
cloud	O
of	O
dots	O
these	O
dots	O
represent	O
random	B
samples	O
from	O
the	O
full	O
probability	B
distribution	B
the	O
total	O
number	O
of	O
dots	O
in	O
each	O
of	O
the	O
three	O
model	B
subspaces	O
is	O
the	O
same	O
because	O
we	O
assigned	O
equal	O
prior	B
probabilities	O
to	O
the	O
models	O
when	O
a	O
particular	O
data	B
set	B
d	O
is	O
received	O
line	O
we	O
infer	O
the	O
posterior	O
distribution	B
of	O
w	O
for	O
a	O
model	B
say	O
by	O
reading	O
out	O
the	O
density	B
along	O
that	O
horizontal	O
line	O
and	O
normalizing	O
the	O
posterior	B
probability	B
p	O
j	O
is	O
shown	O
by	O
the	O
dotted	O
curve	O
at	O
the	O
bottom	O
also	O
shown	O
is	O
the	O
prior	B
distribution	B
p	O
the	O
case	O
of	O
model	B
which	O
is	O
very	O
poorly	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
figure	O
a	O
hypothesis	O
space	O
consisting	O
of	O
three	O
exclusive	O
models	O
each	O
having	O
one	O
parameter	O
w	O
and	O
a	O
one-dimensional	O
data	B
set	B
d	O
the	O
data	B
set	B
is	O
a	O
single	O
measured	O
value	O
which	O
from	O
the	O
parameter	O
w	O
by	O
a	O
small	O
amount	O
of	O
additive	O
noise	O
typical	B
samples	O
from	O
the	O
joint	B
distribution	B
p	O
wh	O
are	O
shown	O
by	O
dots	O
these	O
are	O
not	O
data	O
points	O
the	O
observed	O
data	B
set	B
is	O
a	O
single	O
particular	O
value	O
for	O
d	O
shown	O
by	O
the	O
dashed	O
horizontal	O
line	O
the	O
dashed	O
curves	O
below	O
show	O
the	O
posterior	B
probability	B
of	O
w	O
for	O
each	O
model	B
given	O
this	O
data	B
set	B
the	O
evidence	B
for	O
the	O
models	O
is	O
obtained	O
by	O
marginalizing	O
onto	O
the	O
d	O
axis	O
at	O
the	O
left-hand	O
side	O
d	O
d	O
p	O
p	O
p	O
p	O
j	O
p	O
j	O
p	O
p	O
p	O
j	O
p	O
w	O
w	O
w	O
matched	O
to	O
the	O
data	O
the	O
shape	O
of	O
the	O
posterior	O
distribution	B
will	O
depend	O
on	O
the	O
details	O
of	O
the	O
tails	O
of	O
the	O
prior	B
p	O
and	O
the	O
likelihood	B
p	O
j	O
the	O
curve	O
shown	O
is	O
for	O
the	O
case	O
where	O
the	O
prior	B
falls	O
more	O
strongly	O
we	O
obtain	O
by	O
marginalizing	O
the	O
joint	B
distributions	O
p	O
w	O
jhi	O
onto	O
the	O
d	O
axis	O
at	O
the	O
left-hand	O
side	O
for	O
the	O
data	B
set	B
d	O
shown	O
by	O
the	O
dotted	O
horizontal	O
line	O
the	O
evidence	B
p	O
for	O
the	O
more	O
model	B
has	O
a	O
smaller	O
value	O
than	O
the	O
evidence	B
for	O
this	O
is	O
because	O
placed	O
less	O
predictive	O
probability	B
dots	O
on	O
that	O
line	O
in	O
terms	O
of	O
the	O
distributions	O
over	O
w	O
model	B
has	O
smaller	O
evidence	B
because	O
the	O
occam	B
factor	I
is	O
smaller	O
for	O
than	O
for	O
the	O
simplest	O
model	B
has	O
the	O
smallest	O
evidence	B
of	O
all	O
because	O
the	O
best	O
that	O
it	O
can	O
achieve	O
to	O
the	O
data	O
d	O
is	O
very	O
poor	O
given	O
this	O
data	B
set	B
the	O
most	O
probable	O
model	B
is	O
occam	B
factor	I
for	O
several	O
parameters	B
if	O
the	O
posterior	O
is	O
well	O
approximated	O
by	O
a	O
gaussian	B
then	O
the	O
occam	B
factor	I
is	O
obtained	O
from	O
the	O
determinant	O
of	O
the	O
corresponding	O
covariance	B
matrix	B
equation	O
and	O
chapter	O
p	O
jhi	O
p	O
j	O
wmp	O
hi	O
evidence	B
best	O
likelihood	B
p	O
jhi	O
occam	B
factor	I
where	O
a	O
ln	O
p	O
j	O
dhi	O
the	O
hessian	B
which	O
we	O
evaluated	O
when	O
we	O
calculated	O
the	O
error	B
bars	I
on	O
wmp	O
and	O
chapter	O
as	O
the	O
amount	O
of	O
data	O
collected	O
increases	O
this	O
gaussian	B
approximation	B
is	O
expected	O
to	O
become	O
increasingly	O
accurate	O
in	O
summary	B
bayesian	B
model	B
comparison	I
is	O
a	O
simple	O
extension	O
of	O
maximum	B
likelihood	B
model	B
selection	O
the	O
evidence	B
is	O
obtained	O
by	O
multiplying	O
the	O
likelihood	B
by	O
the	O
occam	B
factor	I
to	O
evaluate	O
the	O
occam	B
factor	I
we	O
need	O
only	O
the	O
hessian	B
a	O
if	O
the	O
gaussian	B
approximation	B
is	O
good	B
thus	O
the	O
bayesian	B
method	O
of	O
model	B
comparison	I
by	O
evaluating	O
the	O
evidence	B
is	O
no	O
more	O
computationally	O
demanding	O
than	O
the	O
task	O
of	O
for	O
each	O
model	B
the	O
parameters	B
and	O
their	O
error	B
bars	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
or	O
figure	O
how	O
many	O
boxes	O
are	O
behind	O
the	O
tree	B
example	O
example	O
let	O
s	O
return	O
to	O
the	O
example	O
that	O
opened	O
this	O
chapter	O
are	O
there	O
one	O
or	O
two	O
boxes	O
behind	O
the	O
tree	B
in	O
why	O
do	O
coincidences	O
make	O
us	O
suspicious	O
let	O
s	O
assume	O
the	O
image	B
of	O
the	O
area	O
round	O
the	O
trunk	O
and	O
box	B
has	O
a	O
size	O
of	O
pixels	O
that	O
the	O
trunk	O
is	O
pixels	O
wide	O
and	O
that	O
colours	O
of	O
boxes	O
can	O
be	O
distinguished	O
the	O
theory	O
that	O
says	O
there	O
is	O
one	O
box	B
near	O
the	O
trunk	O
has	O
four	O
free	O
parameters	B
three	O
coordinates	O
the	O
top	O
three	O
edges	O
of	O
the	O
box	B
and	O
one	O
parameter	O
giving	O
the	O
box	B
s	O
colour	O
boxes	O
could	O
levitate	O
there	O
would	O
be	O
free	O
parameters	B
the	O
theory	O
that	O
says	O
there	O
are	O
two	O
boxes	O
near	O
the	O
trunk	O
has	O
eight	O
free	O
parameters	B
four	O
plus	O
a	O
ninth	O
a	O
binary	O
variable	O
that	O
indicates	O
which	O
of	O
the	O
two	O
boxes	O
is	O
the	O
closest	O
to	O
the	O
viewer	O
what	O
is	O
the	O
evidence	B
for	O
each	O
model	B
we	O
ll	O
do	O
we	O
need	O
a	O
prior	B
on	O
the	O
parameters	B
to	O
evaluate	O
the	O
evidence	B
for	O
convenience	O
let	O
s	O
work	O
in	O
pixels	O
let	O
s	O
assign	O
a	O
separable	O
prior	B
to	O
the	O
horizontal	O
location	O
of	O
the	O
box	B
its	O
width	O
its	O
height	O
and	O
its	O
colour	O
the	O
height	O
could	O
have	O
any	O
of	O
say	O
distinguishable	O
values	O
so	O
could	O
the	O
width	O
and	O
so	O
could	O
the	O
location	O
the	O
colour	O
could	O
have	O
any	O
of	O
values	O
we	O
ll	O
put	O
uniform	O
priors	O
over	O
these	O
variables	O
we	O
ll	O
ignore	O
all	O
the	O
parameters	B
associated	O
with	O
other	O
objects	O
in	O
the	O
image	B
since	O
they	O
don	O
t	O
come	O
into	O
the	O
model	B
comparison	I
between	O
and	O
the	O
evidence	B
is	O
p	O
since	O
only	O
one	O
setting	O
of	O
the	O
parameters	B
the	O
data	O
and	O
it	O
predicts	O
the	O
data	O
perfectly	O
as	O
for	O
model	B
six	B
of	O
its	O
nine	O
parameters	B
are	O
well-determined	O
and	O
three	O
of	O
them	O
are	O
partly-constrained	O
by	O
the	O
data	O
if	O
the	O
left-hand	O
box	B
is	O
furthest	O
away	O
for	O
example	O
then	O
its	O
width	O
is	O
at	O
least	O
pixels	O
and	O
at	O
most	O
if	O
it	O
s	O
the	O
closer	O
of	O
the	O
two	O
boxes	O
then	O
its	O
width	O
is	O
between	O
and	O
pixels	O
m	O
assuming	O
here	O
that	O
the	O
visible	O
portion	O
of	O
the	O
left-hand	O
box	B
is	O
about	O
pixels	O
wide	O
to	O
get	O
the	O
evidence	B
we	O
need	O
to	O
sum	O
up	O
the	O
prior	B
probabilities	O
of	O
all	O
viable	O
hypotheses	O
to	O
do	O
an	O
exact	O
calculation	O
we	O
need	O
to	O
be	O
more	O
about	O
the	O
data	O
and	O
the	O
priors	O
but	O
let	O
s	O
just	O
get	O
the	O
ballpark	O
answer	O
assuming	O
that	O
the	O
two	O
unconstrained	O
real	O
variables	O
have	O
half	O
their	O
values	O
available	O
and	O
that	O
the	O
binary	O
variable	O
is	O
completely	O
undetermined	O
an	O
exercise	O
you	O
can	O
make	O
an	O
explicit	O
model	B
and	O
work	O
out	O
the	O
exact	O
answer	O
p	O
thus	O
the	O
posterior	B
probability	B
ratio	O
is	O
equal	O
prior	B
probability	B
p	O
p	O
so	O
the	O
data	O
are	O
roughly	O
to	O
in	O
favour	O
of	O
the	O
simpler	O
hypothesis	O
the	O
four	O
factors	O
in	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
occam	O
factors	O
the	O
more	O
complex	B
model	B
has	O
four	O
extra	O
parameters	B
for	O
sizes	O
and	O
colours	O
three	O
for	O
sizes	O
and	O
one	O
for	O
colour	O
it	O
has	O
to	O
pay	O
two	O
big	O
occam	O
factors	O
and	O
for	O
the	O
highly	O
suspicious	B
coincidences	I
that	O
the	O
two	O
box	B
heights	O
match	O
exactly	O
and	O
the	O
two	O
colours	O
match	O
exactly	O
and	O
it	O
also	O
pays	O
two	O
lesser	O
occam	O
factors	O
for	O
the	O
two	O
lesser	O
coincidences	O
that	O
both	O
boxes	O
happened	O
to	O
have	O
one	O
of	O
their	O
edges	O
conveniently	O
hidden	O
behind	O
a	O
tree	B
or	O
behind	O
each	O
other	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
figure	O
a	O
popular	O
view	O
of	O
model	B
comparison	I
by	O
minimum	B
description	I
length	I
each	O
model	B
hi	O
communicates	O
the	O
data	O
d	O
by	O
sending	O
the	O
identity	O
of	O
the	O
model	B
sending	O
the	O
parameters	B
of	O
the	O
model	B
then	O
sending	O
the	O
data	O
relative	B
to	O
those	O
parameters	B
as	O
we	O
proceed	O
to	O
more	O
complex	B
models	O
the	O
length	O
of	O
the	O
parameter	O
message	O
increases	O
on	O
the	O
other	O
hand	O
the	O
length	O
of	O
the	O
data	O
message	O
decreases	O
because	O
a	O
complex	B
model	B
is	O
able	O
to	O
the	O
data	O
better	O
making	O
the	O
residuals	O
smaller	O
in	O
this	O
example	O
the	O
intermediate	O
model	B
achieves	O
the	O
optimum	O
between	O
these	O
two	O
trends	O
ld	O
j	O
ld	O
j	O
ld	O
j	O
minimum	B
description	I
length	I
a	O
complementary	O
view	O
of	O
bayesian	B
model	B
comparison	I
is	O
obtained	O
by	O
replacing	O
probabilities	O
of	O
events	O
by	O
the	O
lengths	O
in	O
bits	O
of	O
messages	O
that	O
communicate	O
the	O
events	O
without	O
loss	O
to	O
a	O
receiver	O
message	O
lengths	O
lx	O
correspond	O
to	O
a	O
probabilistic	B
model	B
over	O
events	O
x	O
via	O
the	O
relations	O
p	O
lx	O
p	O
the	O
mdl	O
principle	O
and	O
boulton	O
states	O
that	O
one	O
should	O
prefer	O
models	O
that	O
can	O
communicate	O
the	O
data	O
in	O
the	O
smallest	O
number	O
of	O
bits	O
consider	O
a	O
two-part	O
message	O
that	O
states	O
which	O
model	B
h	O
is	O
to	O
be	O
used	O
and	O
then	O
communicates	O
the	O
data	O
d	O
within	O
that	O
model	B
to	O
some	O
pre-arranged	O
precision	B
this	O
produces	O
a	O
message	O
of	O
length	O
ldh	O
lh	O
ld	O
jh	O
the	O
lengths	O
lh	O
for	O
h	O
an	O
implicit	O
prior	B
p	O
over	O
the	O
alternative	O
models	O
similarly	O
ld	O
jh	O
corresponds	O
to	O
a	O
density	B
p	O
jh	O
thus	O
a	O
procedure	O
for	O
assigning	B
message	O
lengths	O
can	O
be	O
mapped	O
onto	O
posterior	O
probabilities	O
ldh	O
log	O
p	O
log	O
log	O
p	O
j	O
d	O
const	O
in	O
principle	O
then	O
mdl	O
can	O
always	O
be	O
interpreted	O
as	O
bayesian	B
model	B
comparison	I
and	O
vice	O
versa	O
however	O
this	O
simple	O
discussion	O
has	O
not	O
addressed	O
how	O
one	O
would	O
actually	O
evaluate	O
the	O
key	O
data-dependent	O
term	O
ld	O
jh	O
which	O
corresponds	O
to	O
the	O
evidence	B
for	O
h	O
often	O
this	O
message	O
is	O
imagined	O
as	O
being	O
subdivided	O
into	O
a	O
parameter	O
block	B
and	O
a	O
data	O
block	B
models	O
with	O
a	O
small	O
number	O
of	O
parameters	B
have	O
only	O
a	O
short	O
parameter	O
block	B
but	O
do	O
not	O
the	O
data	O
well	O
and	O
so	O
the	O
data	O
message	O
list	O
of	O
large	O
residuals	O
is	O
long	O
as	O
the	O
number	O
of	O
parameters	B
increases	O
the	O
parameter	O
block	B
lengthens	O
and	O
the	O
data	O
message	O
becomes	O
shorter	O
there	O
is	O
an	O
optimum	O
model	B
complexity	B
in	O
the	O
for	O
which	O
the	O
sum	O
is	O
minimized	O
this	O
picture	O
glosses	O
over	O
some	O
subtle	O
issues	O
we	O
have	O
not	O
the	O
precision	B
to	O
which	O
the	O
parameters	B
w	O
should	O
be	O
sent	O
this	O
precision	B
has	O
an	O
important	O
the	O
precision	B
to	O
which	O
real-valued	O
data	O
d	O
are	O
sent	O
which	O
assuming	O
is	O
small	O
relative	B
to	O
the	O
noise	O
level	O
just	O
introduces	O
an	O
additive	O
constant	O
as	O
we	O
decrease	O
the	O
precision	B
to	O
which	O
w	O
is	O
sent	O
the	O
parameter	O
message	O
shortens	O
but	O
the	O
data	O
message	O
typically	O
lengthens	O
because	O
the	O
truncated	O
parameters	B
do	O
not	O
match	O
the	O
data	O
so	O
well	O
there	O
is	O
a	O
non-trivial	O
optimal	B
precision	B
in	O
simple	O
gaussian	B
cases	O
it	O
is	O
possible	O
to	O
solve	O
for	O
this	O
optimal	B
precision	B
and	O
freeman	O
and	O
it	O
is	O
closely	O
related	O
to	O
the	O
posterior	O
error	B
bars	I
on	O
the	O
parameters	B
where	O
a	O
ln	O
p	O
j	O
dh	O
it	O
turns	O
out	O
that	O
the	O
optimal	B
parameter	O
message	O
length	O
is	O
virtually	O
identical	O
to	O
the	O
log	O
of	O
the	O
occam	B
factor	I
in	O
equation	O
random	B
element	O
involved	O
in	O
parameter	O
truncation	O
means	O
that	O
the	O
encoding	O
is	O
slightly	O
sub-optimal	O
with	O
care	O
therefore	O
one	O
can	O
replicate	O
bayesian	B
results	O
in	O
mdl	O
terms	O
although	O
some	O
of	O
the	O
earliest	O
work	O
on	O
complex	B
model	B
comparison	I
involved	O
the	O
mdl	O
framework	O
and	O
wallace	O
mdl	O
has	O
no	O
apparent	O
advantages	O
over	O
the	O
direct	O
probabilistic	O
approach	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
minimum	B
description	I
length	I
mdl	O
does	O
have	O
its	O
uses	O
as	O
a	O
pedagogical	O
tool	O
the	O
description	O
length	O
concept	O
is	O
useful	O
for	O
motivating	O
prior	B
probability	B
distributions	O
also	O
ways	O
of	O
breaking	O
down	O
the	O
task	O
of	O
communicating	O
data	O
using	O
a	O
model	B
can	O
give	O
helpful	O
insights	O
into	O
the	O
modelling	B
process	O
as	O
will	O
now	O
be	O
illustrated	O
on-line	O
learning	B
and	O
cross-validation	B
in	O
cases	O
where	O
the	O
data	O
consist	O
of	O
a	O
sequence	B
of	O
points	O
d	O
tn	O
the	O
log	O
evidence	B
can	O
be	O
decomposed	O
as	O
a	O
sum	O
of	O
on-line	O
predictive	O
performances	O
log	O
p	O
jh	O
log	O
p	O
jh	O
log	O
p	O
j	O
log	O
p	O
j	O
log	O
p	O
j	O
this	O
decomposition	O
can	O
be	O
used	O
to	O
explain	O
the	O
between	O
the	O
evidence	B
and	O
leave-one-out	O
cross-validation	B
as	O
measures	O
of	O
predictive	O
ability	O
cross-validation	B
examines	O
the	O
average	O
value	O
of	O
just	O
the	O
last	O
term	O
log	O
p	O
j	O
under	O
random	B
re-orderings	O
of	O
the	O
data	O
the	O
evidence	B
on	O
the	O
other	O
hand	O
sums	O
up	O
how	O
well	O
the	O
model	B
predicted	O
all	O
the	O
data	O
starting	O
from	O
scratch	O
the	O
bits-back	O
encoding	O
method	O
another	O
mdl	O
thought	O
experiment	O
and	O
van	O
camp	O
involves	O
incorporating	O
random	B
bits	O
into	O
our	O
message	O
the	O
data	O
are	O
communicated	O
using	O
a	O
parameter	O
block	B
and	O
a	O
data	O
block	B
the	O
parameter	O
vector	O
sent	O
is	O
a	O
random	B
sample	B
from	I
the	O
posterior	O
p	O
j	O
dh	O
p	O
j	O
whp	O
jhp	O
jh	O
this	O
sample	B
w	O
is	O
sent	O
to	O
an	O
arbitrary	O
small	O
granularity	O
using	O
a	O
message	O
length	O
lw	O
jh	O
logp	O
the	O
data	O
are	O
encoded	O
relative	B
to	O
w	O
with	O
a	O
message	O
of	O
length	O
ld	O
j	O
wh	O
logp	O
j	O
once	O
the	O
data	O
message	O
has	O
been	O
received	O
the	O
random	B
bits	O
used	O
to	O
generate	O
the	O
sample	B
w	O
from	O
the	O
posterior	O
can	O
be	O
deduced	O
by	O
the	O
receiver	O
the	O
number	O
of	O
bits	O
so	O
recovered	O
is	O
j	O
these	O
recovered	O
bits	O
need	O
not	O
count	O
towards	O
the	O
message	O
length	O
since	O
we	O
might	O
use	O
some	O
other	O
optimally-encoded	O
message	O
as	O
a	O
random	B
bit	B
string	O
thereby	O
communicating	O
that	O
message	O
at	O
the	O
same	O
time	O
the	O
net	O
description	O
cost	O
is	O
therefore	O
lw	O
jh	O
ld	O
j	O
wh	O
bits	B
back	I
log	O
log	O
p	O
jh	O
log	O
thus	O
this	O
thought	O
experiment	O
has	O
yielded	O
the	O
optimal	B
description	O
length	O
bitsback	O
encoding	O
has	O
been	O
turned	O
into	O
a	O
practical	B
compression	B
method	O
for	O
data	O
modelled	O
with	O
latent	B
variable	I
models	I
by	O
frey	B
p	O
jh	O
p	O
j	O
wh	O
p	O
j	O
dh	O
further	O
reading	O
bayesian	B
methods	O
are	O
introduced	O
and	O
contrasted	O
with	O
sampling-theory	O
statistics	O
in	O
gull	B
loredo	O
the	O
bayesian	B
occam	O
s	O
razor	O
is	O
demonstrated	O
on	O
model	B
problems	O
in	O
mackay	B
useful	O
textbooks	O
are	O
and	O
tiao	O
berger	O
one	O
debate	O
worth	O
understanding	O
is	O
the	O
question	O
of	O
whether	O
it	O
s	O
permissible	O
to	O
use	O
improper	B
priors	O
in	O
bayesian	B
inference	B
et	O
al	O
if	O
we	O
want	O
to	O
do	O
model	B
comparison	I
discussed	O
in	O
this	O
chapter	O
it	O
is	O
essential	O
to	O
use	O
proper	B
priors	O
otherwise	O
the	O
evidences	O
and	O
the	O
occam	O
factors	O
are	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
model	B
comparison	I
and	O
occam	O
s	O
razor	O
meaningless	O
only	O
when	O
one	O
has	O
no	O
intention	O
to	O
do	O
model	B
comparison	I
may	O
it	O
be	O
safe	O
to	O
use	O
improper	B
priors	O
and	O
even	O
in	O
such	O
cases	O
there	O
are	O
pitfalls	O
as	O
dawid	O
et	O
al	O
explain	O
i	O
would	O
agree	O
with	O
their	O
advice	O
to	O
always	O
use	O
proper	B
priors	O
tempered	O
by	O
an	O
encouragement	O
to	O
be	O
smart	O
when	O
making	O
calculations	O
recognizing	O
opportunities	O
for	O
approximation	B
exercises	O
p	O
x	O
p	O
m	O
x	O
y	O
x	O
exercise	O
random	B
variables	O
x	O
come	O
independently	O
from	O
a	O
probability	B
distribution	B
p	O
according	O
to	O
model	B
p	O
is	O
a	O
uniform	O
distribution	B
p	O
x	O
according	O
to	O
model	B
p	O
is	O
a	O
nonuniform	O
distribution	B
with	O
an	O
unknown	O
parameter	O
m	O
p	O
x	O
mx	O
given	O
the	O
data	O
d	O
what	O
is	O
the	O
evidence	B
for	O
and	O
exercise	O
datapoints	O
t	O
are	O
believed	O
to	O
come	O
from	O
a	O
straight	O
line	O
the	O
experimenter	O
chooses	O
x	O
and	O
t	O
is	O
gaussian-distributed	O
about	O
y	O
with	O
variance	B
according	O
to	O
model	B
the	O
straight	O
line	O
is	O
horizontal	O
so	O
according	O
to	O
model	B
is	O
a	O
parameter	O
with	O
prior	B
distribution	B
both	O
models	O
assign	O
a	O
prior	B
distribution	B
to	O
given	O
the	O
data	B
set	B
d	O
and	O
assuming	O
the	O
noise	O
level	O
is	O
what	O
is	O
the	O
evidence	B
for	O
each	O
model	B
exercise	O
a	O
six-sided	O
die	B
is	O
rolled	O
times	O
and	O
the	O
numbers	O
of	O
times	O
each	O
face	O
came	O
up	O
were	O
f	O
what	O
is	O
the	O
probability	B
that	O
the	O
die	B
is	O
a	O
perfectly	O
fair	O
die	B
assuming	O
the	O
alternative	O
hypothesis	O
says	O
that	O
the	O
die	B
has	O
a	O
biased	O
distribution	B
p	O
and	O
the	O
prior	B
density	B
for	O
p	O
is	O
uniform	O
over	O
the	O
simplex	B
pi	O
pi	O
pi	O
solve	O
this	O
problem	O
two	O
ways	O
exactly	O
using	O
the	O
helpful	O
dirichlet	B
formulae	O
and	O
approximately	O
using	O
laplace	B
s	O
method	O
notice	O
that	O
your	O
choice	O
of	O
basis	O
for	O
the	O
laplace	B
approximation	B
is	O
important	O
see	O
mackay	B
for	O
discussion	O
of	O
this	O
exercise	O
exercise	O
the	O
of	O
race	B
on	O
the	O
imposition	O
of	O
the	O
death	B
penalty	I
for	O
murder	B
in	O
america	B
has	O
been	O
much	O
studied	O
the	O
following	O
three-way	O
table	O
cases	O
in	O
which	O
the	O
defendant	O
was	O
convicted	O
of	O
murder	B
the	O
three	O
variables	O
are	O
the	O
defendant	O
s	O
race	B
the	O
victim	O
s	O
race	B
and	O
whether	O
the	O
defendant	O
was	O
sentenced	O
to	O
death	O
from	O
m	O
radelet	O
racial	O
characteristics	O
and	O
imposition	O
of	O
the	O
death	B
penalty	I
american	B
sociological	O
review	O
pp	O
white	B
defendant	O
black	B
defendant	O
death	B
penalty	I
yes	O
no	O
death	B
penalty	I
yes	O
no	O
white	B
victim	O
black	B
victim	O
white	B
victim	O
black	B
victim	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
it	O
seems	O
that	O
the	O
death	B
penalty	I
was	O
applied	O
much	O
more	O
often	O
when	O
the	O
victim	O
was	O
white	B
then	O
when	O
the	O
victim	O
was	O
black	B
when	O
the	O
victim	O
was	O
white	B
of	O
defendants	O
got	O
the	O
death	B
penalty	I
but	O
when	O
the	O
victim	O
was	O
black	B
of	O
defendants	O
got	O
the	O
death	B
penalty	I
these	O
data	O
provide	O
an	O
example	O
of	O
a	O
phenomenon	O
known	O
as	O
simpson	O
s	O
paradox	B
a	O
higher	O
fraction	O
of	O
white	B
defendants	O
are	O
sentenced	O
to	O
death	O
overall	O
but	O
in	O
cases	O
involving	O
black	B
victims	O
a	O
higher	O
fraction	O
of	O
black	B
defendants	O
are	O
sentenced	O
to	O
death	O
and	O
in	O
cases	O
involving	O
white	B
victims	O
a	O
higher	O
fraction	O
of	O
black	B
defendants	O
are	O
sentenced	O
to	O
death	O
quantify	O
the	O
evidence	B
for	O
the	O
four	O
alternative	O
hypotheses	O
shown	O
in	O
i	O
should	O
mention	O
that	O
i	O
don	O
t	O
believe	O
any	O
of	O
these	O
models	O
is	O
adequate	O
several	O
additional	O
variables	O
are	O
important	O
in	O
murder	B
cases	O
such	O
as	O
whether	O
the	O
victim	O
and	O
murderer	O
knew	O
each	O
other	O
whether	O
the	O
murder	B
was	O
premeditated	O
and	O
whether	O
the	O
defendant	O
had	O
a	O
prior	B
criminal	O
record	O
none	O
of	O
these	O
variables	O
is	O
included	O
in	O
the	O
table	O
so	O
this	O
is	O
an	O
academic	O
exercise	O
in	O
model	B
comparison	I
rather	O
than	O
a	O
serious	O
study	O
of	O
racial	O
bias	B
in	O
the	O
state	O
of	O
florida	B
the	O
hypotheses	O
are	O
shown	O
as	O
graphical	O
models	O
with	O
arrows	O
showing	O
dependencies	O
between	O
the	O
variables	O
v	O
race	B
m	O
race	B
and	O
d	O
death	B
penalty	I
given	O
model	B
has	O
only	O
one	O
free	O
parameter	O
the	O
probability	B
of	O
receiving	O
the	O
death	B
penalty	I
model	B
has	O
four	O
such	O
parameters	B
one	O
for	O
each	O
state	O
of	O
the	O
variables	O
v	O
and	O
m	O
assign	O
uniform	O
priors	O
to	O
these	O
variables	O
how	O
sensitive	O
are	O
the	O
conclusions	O
to	O
the	O
choice	O
of	O
prior	B
v	O
v	O
m	O
m	O
v	O
v	O
m	O
m	O
d	O
d	O
d	O
d	O
v	O
v	O
m	O
m	O
v	O
v	O
m	O
m	O
d	O
d	O
d	O
d	O
figure	O
four	O
hypotheses	O
concerning	O
the	O
dependence	O
of	O
the	O
imposition	O
of	O
the	O
death	B
penalty	I
d	O
on	O
the	O
race	B
of	O
the	O
victim	O
v	O
and	O
the	O
race	B
of	O
the	O
convicted	O
murderer	O
m	O
for	O
example	O
asserts	O
that	O
the	O
probability	B
of	O
receiving	O
the	O
death	B
penalty	I
does	O
depend	O
on	O
the	O
murderer	O
s	O
race	B
but	O
not	O
on	O
the	O
victim	O
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
the	O
last	O
couple	O
of	O
chapters	O
have	O
assumed	O
that	O
a	O
gaussian	B
approximation	B
to	O
the	O
probability	B
distribution	B
we	O
are	O
interested	O
in	O
is	O
adequate	O
what	O
if	O
it	O
is	O
not	O
we	O
have	O
already	O
seen	O
an	O
example	O
clustering	B
where	O
the	O
likelihood	B
function	O
is	O
multimodal	O
and	O
has	O
nasty	O
unboundedly-high	O
spikes	O
in	O
certain	O
locations	O
in	O
the	O
parameter	O
space	O
so	O
maximizing	O
the	O
posterior	B
probability	B
and	O
a	O
gaussian	B
is	O
not	O
always	O
going	O
to	O
work	O
this	O
with	O
laplace	B
s	O
method	O
is	O
one	O
motivation	O
for	O
being	O
interested	O
in	O
monte	B
carlo	I
methods	I
in	O
fact	O
monte	B
carlo	I
methods	I
provide	O
a	O
general-purpose	O
set	B
of	O
tools	O
with	O
applications	O
in	O
bayesian	B
data	O
modelling	B
and	O
many	O
other	O
this	O
chapter	O
describes	O
a	O
sequence	B
of	O
methods	O
importance	B
sampling	I
rejection	B
sampling	I
the	O
metropolis	B
method	I
gibbs	B
sampling	I
and	O
slice	B
sampling	I
for	O
each	O
method	O
we	O
discuss	O
whether	O
the	O
method	O
is	O
expected	O
to	O
be	O
useful	O
for	O
high-dimensional	O
problems	O
such	O
as	O
arise	O
in	O
inference	B
with	O
graphical	O
models	O
graphical	O
model	B
is	O
a	O
probabilistic	B
model	B
in	O
which	O
dependencies	O
and	O
independencies	O
of	O
variables	O
are	O
represented	O
by	O
edges	O
in	O
a	O
graph	B
whose	O
nodes	O
are	O
the	O
variables	O
along	O
the	O
way	O
the	O
terminology	O
of	O
markov	B
chain	I
monte	B
carlo	I
methods	I
is	O
presented	O
the	O
subsequent	O
chapter	O
discusses	O
advanced	O
methods	O
for	O
reducing	O
random	B
walk	I
behaviour	O
for	O
details	O
of	O
monte	B
carlo	I
methods	I
theorems	O
and	O
proofs	O
and	O
a	O
full	O
list	O
of	O
references	O
the	O
reader	O
is	O
directed	O
to	O
neal	B
gilks	B
et	O
al	O
and	O
tanner	B
in	O
this	O
chapter	O
i	O
will	O
use	O
the	O
word	O
sample	B
in	O
the	O
following	O
sense	O
a	O
sample	B
from	I
a	O
distribution	B
p	O
is	O
a	O
single	O
realization	O
x	O
whose	O
probability	B
distribution	B
is	O
p	O
this	O
contrasts	O
with	O
the	O
alternative	O
usage	O
in	B
statistics	I
where	O
sample	B
refers	O
to	O
a	O
collection	O
of	O
realizations	O
fxg	O
cation	O
convention	O
i	O
like	O
my	O
matrices	B
to	O
act	O
to	O
the	O
right	O
preferring	O
when	O
we	O
discuss	O
transition	B
probability	B
matrices	B
i	O
will	O
use	O
a	O
right-multipli	O
u	O
mv	O
to	O
ut	O
vtmt	O
a	O
transition	B
probability	B
matrix	B
tij	O
or	O
tijj	O
the	O
probability	B
given	O
the	O
current	O
state	O
is	O
j	O
of	O
making	O
the	O
transition	B
from	O
j	O
to	O
i	O
the	O
columns	O
of	O
t	O
are	O
probability	B
vectors	B
if	O
we	O
write	O
down	O
a	O
transition	B
probability	B
density	B
we	O
use	O
the	O
same	O
convention	O
for	O
the	O
order	O
of	O
its	O
arguments	O
t	O
x	O
is	O
a	O
transition	B
probability	B
density	B
from	O
x	O
to	O
this	O
unfortunately	O
means	O
that	O
you	O
have	O
to	O
get	O
used	O
to	O
reading	O
from	O
right	O
to	O
left	O
the	O
sequence	B
xyz	O
has	O
probability	B
t	O
yt	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
the	O
problems	O
to	O
be	O
solved	O
monte	B
carlo	I
methods	I
are	O
computational	O
techniques	O
that	O
make	O
use	O
of	O
random	B
numbers	O
the	O
aims	O
of	O
monte	B
carlo	I
methods	I
are	O
to	O
solve	O
one	O
or	O
both	O
of	O
the	O
following	O
problems	O
problem	O
to	O
generate	O
samples	O
fxrgr	O
from	O
a	O
given	O
probability	B
distribu	O
tion	O
p	O
problem	O
to	O
estimate	O
expectations	O
of	O
functions	B
under	O
this	O
distribution	B
for	O
example	O
dn	O
x	O
p	O
the	O
probability	B
distribution	B
p	O
which	O
we	O
call	O
the	O
target	O
density	B
might	O
be	O
a	O
distribution	B
from	O
statistical	B
physics	B
or	O
a	O
conditional	B
distribution	B
arising	O
in	O
data	O
modelling	B
for	O
example	O
the	O
posterior	B
probability	B
of	O
a	O
model	B
s	O
parameters	B
given	O
some	O
observed	O
data	O
we	O
will	O
generally	O
assume	O
that	O
x	O
is	O
an	O
n	O
vector	O
with	O
real	O
components	O
xn	O
but	O
we	O
will	O
sometimes	O
consider	O
discrete	O
spaces	O
also	O
simple	O
examples	O
of	O
functions	B
whose	O
expectations	O
we	O
might	O
be	O
interested	O
in	O
include	O
the	O
and	O
second	O
moments	O
of	O
quantities	O
that	O
we	O
wish	O
to	O
predict	O
from	O
which	O
we	O
can	O
compute	O
means	O
and	O
variances	O
for	O
example	O
if	O
some	O
quantity	O
t	O
depends	O
on	O
x	O
we	O
can	O
the	O
mean	B
and	O
variance	B
of	O
t	O
under	O
p	O
by	O
the	O
expectations	O
of	O
the	O
functions	B
tx	O
and	O
then	O
using	O
and	O
and	O
vart	O
it	O
is	O
assumed	O
that	O
p	O
is	O
complex	B
that	O
we	O
cannot	O
evaluate	O
these	O
expectations	O
by	O
exact	O
methods	O
so	O
we	O
are	O
interested	O
in	O
monte	B
carlo	I
methods	I
we	O
will	O
concentrate	O
on	O
the	O
problem	O
because	O
if	O
we	O
have	O
solved	O
it	O
then	O
we	O
can	O
solve	O
the	O
second	O
problem	O
by	O
using	O
the	O
random	B
samples	O
fxrgr	O
to	O
give	O
the	O
estimator	B
rxr	O
are	O
generated	O
from	O
p	O
then	O
the	O
expectation	B
of	O
is	O
if	O
the	O
vectors	B
fxrgr	O
also	O
as	O
the	O
number	O
of	O
samples	O
r	O
increases	O
the	O
variance	B
of	O
will	O
decrease	O
as	O
where	O
is	O
the	O
variance	B
of	O
dn	O
x	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
figure	O
the	O
function	O
p	O
how	O
to	O
draw	O
samples	O
from	O
this	O
density	B
the	O
function	O
p	O
evaluated	O
at	O
a	O
discrete	O
set	B
of	O
uniformly	O
spaced	O
points	O
fxig	O
how	O
to	O
draw	O
samples	O
from	O
this	O
discrete	O
distribution	B
px	O
px	O
this	O
is	O
one	O
of	O
the	O
important	O
properties	O
of	O
monte	B
carlo	I
methods	I
the	O
accuracy	O
of	O
the	O
monte	B
carlo	I
estimate	O
depends	O
only	O
on	O
the	O
variance	B
of	O
not	O
on	O
the	O
dimensionality	O
of	O
the	O
space	O
sampled	O
to	O
be	O
precise	O
the	O
variance	B
of	O
goes	O
as	O
so	O
regardless	O
of	O
the	O
dimensionality	O
of	O
x	O
it	O
may	O
be	O
that	O
as	O
few	O
as	O
a	O
dozen	O
independent	O
samples	O
fxrg	O
to	O
estimate	O
satisfactorily	O
we	O
will	O
later	O
however	O
that	O
high	O
dimensionality	O
can	O
cause	O
other	O
for	O
monte	B
carlo	I
methods	I
obtaining	O
independent	O
samples	O
from	O
a	O
given	O
distribution	B
p	O
is	O
often	O
not	O
easy	O
why	O
is	O
sampling	O
from	O
p	O
hard	O
we	O
will	O
assume	O
that	O
the	O
density	B
from	O
which	O
we	O
wish	O
to	O
draw	O
samples	O
p	O
can	O
be	O
evaluated	O
at	O
least	O
to	O
within	O
a	O
multiplicative	O
constant	O
that	O
is	O
we	O
can	O
evaluate	O
a	O
function	O
p	O
such	O
that	O
p	O
p	O
if	O
we	O
can	O
evaluate	O
p	O
why	O
can	O
we	O
not	O
easily	O
solve	O
problem	O
why	O
is	O
it	O
in	O
general	O
to	O
obtain	O
samples	O
from	O
p	O
there	O
are	O
two	O
the	O
is	O
that	O
we	O
typically	O
do	O
not	O
know	O
the	O
normalizing	O
constant	O
z	O
dn	O
x	O
p	O
the	O
second	O
is	O
that	O
even	O
if	O
we	O
did	O
know	O
z	O
the	O
problem	O
of	O
drawing	O
samples	O
from	O
p	O
is	O
still	O
a	O
challenging	O
one	O
especially	O
in	O
high-dimensional	O
spaces	O
because	O
there	O
is	O
no	O
obvious	O
way	O
to	O
sample	B
from	I
p	O
without	O
enumerating	O
most	O
or	O
all	O
of	O
the	O
possible	O
states	O
correct	O
samples	O
from	O
p	O
will	O
by	O
tend	O
to	O
come	O
from	O
places	O
in	O
x-space	O
where	O
p	O
is	O
big	O
how	O
can	O
we	O
identify	O
those	O
places	O
where	O
p	O
is	O
big	O
without	O
evaluating	O
p	O
everywhere	O
there	O
are	O
only	O
a	O
few	O
high-dimensional	O
densities	O
from	O
which	O
it	O
is	O
easy	O
to	O
draw	O
samples	O
for	O
example	O
the	O
gaussian	B
distribution	B
let	O
us	O
start	O
with	O
a	O
simple	O
one-dimensional	O
example	O
imagine	O
that	O
we	O
wish	O
to	O
draw	O
samples	O
from	O
the	O
density	B
p	O
p	O
where	O
p	O
x	O
we	O
can	O
plot	O
this	O
function	O
but	O
that	O
does	O
not	O
mean	B
we	O
can	O
draw	O
samples	O
from	O
it	O
to	O
start	O
with	O
we	O
don	O
t	O
know	O
the	O
normalizing	O
constant	O
z	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
problems	O
to	O
be	O
solved	O
to	O
give	O
ourselves	O
a	O
simpler	O
problem	O
we	O
could	O
discretize	O
the	O
variable	O
x	O
and	O
ask	O
for	O
samples	O
from	O
the	O
discrete	O
probability	B
distribution	B
over	O
a	O
set	B
of	O
uniformly	O
spaced	O
points	O
fxig	O
how	O
could	O
we	O
solve	O
this	O
problem	O
if	O
we	O
evaluate	O
p	O
at	O
each	O
point	O
xi	O
we	O
can	O
compute	O
z	O
and	O
pi	O
and	O
we	O
can	O
then	O
sample	B
from	I
the	O
probability	B
distribution	B
fpig	O
using	O
various	O
methods	O
based	O
on	O
a	O
source	O
of	O
random	B
bits	O
section	B
but	O
what	O
is	O
the	O
cost	O
of	O
this	O
procedure	O
and	O
how	O
does	O
it	O
scale	O
with	O
the	O
dimensionality	O
of	O
the	O
space	O
n	O
let	O
us	O
concentrate	O
on	O
the	O
initial	O
cost	O
of	O
evaluating	O
z	O
to	O
compute	O
z	O
we	O
have	O
to	O
visit	O
every	O
point	O
in	O
the	O
space	O
in	O
there	O
are	O
uniformly	O
spaced	O
points	O
in	O
one	O
dimension	O
if	O
our	O
system	O
had	O
n	O
dimensions	B
n	O
say	O
then	O
the	O
corresponding	O
number	O
of	O
points	O
would	O
be	O
an	O
unimaginable	O
number	O
of	O
evaluations	O
of	O
p	O
even	O
if	O
each	O
component	O
xn	O
took	O
only	O
two	O
discrete	O
values	O
the	O
number	O
of	O
evaluations	O
of	O
p	O
would	O
be	O
a	O
number	O
that	O
is	O
still	O
horribly	O
huge	O
if	O
every	O
electron	O
in	O
the	O
universe	O
are	O
about	O
of	O
them	O
were	O
a	O
gigahertz	O
computer	B
that	O
could	O
evaluate	O
p	O
for	O
a	O
trillion	O
states	O
every	O
second	O
and	O
if	O
we	O
ran	O
those	O
computers	O
for	O
a	O
time	O
equal	O
to	O
the	O
age	O
of	O
the	O
universe	O
seconds	O
they	O
would	O
still	O
only	O
visit	O
states	O
we	O
d	O
have	O
to	O
wait	O
for	O
more	O
than	O
universe	O
ages	O
to	O
elapse	O
before	O
all	O
states	O
had	O
been	O
visited	O
systems	O
with	O
states	O
are	O
two	O
a	O
penny	O
one	O
example	O
is	O
a	O
collection	O
of	O
spins	O
such	O
as	O
a	O
fragment	O
of	O
an	O
ising	B
model	B
whose	O
probability	B
distribution	B
is	O
proportional	O
to	O
p	O
where	O
xn	O
and	O
ex	O
jmnxmxn	O
hnxn	O
the	O
energy	B
function	O
ex	O
is	O
readily	O
evaluated	O
for	O
any	O
x	O
but	O
if	O
we	O
wish	O
to	O
evaluate	O
this	O
function	O
at	O
all	O
states	O
x	O
the	O
computer	B
time	O
required	O
would	O
be	O
function	O
evaluations	O
the	O
ising	B
model	B
is	O
a	O
simple	O
model	B
which	O
has	O
been	O
around	O
for	O
a	O
long	O
time	O
but	O
the	O
task	O
of	O
generating	O
samples	O
from	O
the	O
distribution	B
p	O
p	O
is	O
still	O
an	O
active	O
research	O
area	O
the	O
exact	O
samples	O
from	O
this	O
distribution	B
were	O
created	O
in	O
the	O
pioneering	O
work	O
of	O
propp	B
and	O
wilson	B
as	O
we	O
ll	O
describe	O
in	O
chapter	O
a	O
useful	O
analogy	O
imagine	O
the	O
tasks	O
of	O
drawing	O
random	B
water	O
samples	O
from	O
a	O
lake	B
and	O
the	O
average	O
plankton	B
concentration	O
the	O
depth	O
of	O
the	O
lake	B
at	O
x	O
y	O
is	O
p	O
and	O
we	O
assert	O
order	O
to	O
make	O
the	O
analogy	O
work	O
that	O
the	O
plankton	B
concentration	O
is	O
a	O
function	O
of	O
x	O
the	O
required	O
average	O
concentration	O
is	O
an	O
integral	B
like	O
namely	O
z	O
z	O
dn	O
x	O
p	O
translation	O
for	O
american	B
readers	O
such	O
systems	O
are	O
a	O
dime	O
a	O
dozen	O
incidentally	O
this	O
equivalence	B
shows	O
that	O
the	O
correct	O
exchange	B
rate	B
between	O
our	O
currencies	O
is	O
p	O
figure	O
a	O
lake	B
whose	O
depth	O
at	O
x	O
y	O
is	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
where	O
z	O
r	O
dx	O
dy	O
p	O
is	O
the	O
volume	B
of	O
the	O
lake	B
you	O
are	O
provided	O
with	O
a	O
boat	O
a	O
satellite	O
navigation	B
system	O
and	O
a	O
plumbline	O
using	O
the	O
navigator	O
you	O
can	O
take	O
your	O
boat	O
to	O
any	O
desired	O
location	O
x	O
on	O
the	O
map	O
using	O
the	O
plumbline	O
you	O
can	O
measure	O
p	O
at	O
that	O
point	O
you	O
can	O
also	O
measure	O
the	O
plankton	B
concentration	O
there	O
problem	O
is	O
to	O
draw	O
water	O
samples	O
at	O
random	B
from	O
the	O
lake	B
in	O
such	O
a	O
way	O
that	O
each	O
sample	B
is	O
equally	O
likely	O
to	O
come	O
from	O
any	O
point	O
within	O
the	O
lake	B
problem	O
is	O
to	O
the	O
average	O
plankton	B
concentration	O
these	O
are	O
problems	O
to	O
solve	O
because	O
at	O
the	O
outset	O
we	O
know	O
nothing	O
about	O
the	O
depth	O
p	O
perhaps	O
much	O
of	O
the	O
volume	B
of	O
the	O
lake	B
is	O
contained	O
in	O
narrow	O
deep	O
underwater	O
canyons	O
in	O
which	O
case	O
to	O
correctly	O
sample	B
from	I
the	O
lake	B
and	O
correctly	O
estimate	O
our	O
method	O
must	O
implicitly	O
discover	O
the	O
canyons	O
and	O
their	O
volume	B
relative	B
to	O
the	O
rest	O
of	O
the	O
lake	B
problems	O
yes	O
nevertheless	O
we	O
ll	O
see	O
that	O
clever	O
monte	B
carlo	I
methods	I
can	O
solve	O
them	O
uniform	O
sampling	O
having	O
accepted	O
that	O
we	O
cannot	O
exhaustively	O
visit	O
every	O
location	O
x	O
in	O
the	O
state	O
space	O
we	O
might	O
consider	O
trying	O
to	O
solve	O
the	O
second	O
problem	O
the	O
expectation	B
of	O
a	O
function	O
by	O
drawing	O
random	B
samples	O
fxrgr	O
uniformly	O
from	O
the	O
state	O
space	O
and	O
evaluating	O
p	O
at	O
those	O
points	O
then	O
we	O
could	O
introduce	O
a	O
normalizing	O
constant	O
zr	O
by	O
figure	O
a	O
slice	O
through	O
a	O
lake	B
that	O
includes	O
some	O
canyons	O
p	O
r	O
zr	O
and	O
estimate	O
dn	O
x	O
by	O
r	O
p	O
zr	O
is	O
anything	O
wrong	O
with	O
this	O
strategy	O
well	O
it	O
depends	O
on	O
the	O
functions	B
and	O
p	O
let	O
us	O
assume	O
that	O
is	O
a	O
benign	O
smoothly	O
varying	O
function	O
and	O
concentrate	O
on	O
the	O
nature	O
of	O
p	O
as	O
we	O
learnt	O
in	O
chapter	O
a	O
highdimensional	O
distribution	B
is	O
often	O
concentrated	O
in	O
a	O
small	O
region	O
of	O
the	O
state	O
space	O
known	O
as	O
its	O
typical	B
set	B
t	O
whose	O
volume	B
is	O
given	O
by	O
jtj	O
where	O
hx	O
is	O
the	O
entropy	B
of	O
the	O
probability	B
distribution	B
p	O
if	O
almost	O
all	O
the	O
probability	B
mass	O
is	O
located	O
in	O
the	O
typical	B
set	B
and	O
is	O
a	O
benign	O
function	O
the	O
value	O
of	O
dn	O
x	O
will	O
be	O
principally	O
determined	O
by	O
the	O
values	O
that	O
takes	O
on	O
in	O
the	O
typical	B
set	B
so	O
uniform	O
sampling	O
will	O
only	O
stand	O
a	O
chance	O
of	O
giving	O
a	O
good	B
estimate	O
of	O
if	O
we	O
make	O
the	O
number	O
of	O
samples	O
r	O
large	O
that	O
we	O
are	O
likely	O
to	O
hit	O
the	O
typical	B
set	B
at	O
least	O
once	O
or	O
twice	O
so	O
how	O
many	O
samples	O
are	O
required	O
let	O
us	O
take	O
the	O
case	O
of	O
the	O
ising	B
model	B
again	O
the	O
ising	B
model	B
may	O
not	O
be	O
a	O
good	B
example	O
since	O
it	O
doesn	O
t	O
necessarily	O
have	O
a	O
typical	B
set	B
as	O
in	O
chapter	O
the	O
of	O
a	O
typical	B
set	B
was	O
that	O
all	O
states	O
had	O
log	O
probability	B
close	O
to	O
the	O
entropy	B
which	O
for	O
an	O
ising	B
model	B
would	O
mean	B
that	O
the	O
energy	B
is	O
very	O
close	O
to	O
the	O
mean	B
energy	B
but	O
in	O
the	O
vicinity	O
of	O
phase	O
transitions	O
the	O
variance	B
of	O
energy	B
also	O
known	O
as	O
the	O
heat	B
capacity	B
may	O
diverge	O
which	O
means	O
that	O
the	O
energy	B
of	O
a	O
random	B
state	O
is	O
not	O
necessarily	O
expected	O
to	O
be	O
very	O
close	O
to	O
the	O
mean	B
energy	B
the	O
total	O
size	O
of	O
the	O
state	O
space	O
is	O
states	O
and	O
the	O
typical	B
set	B
has	O
size	O
so	O
each	O
sample	B
has	O
a	O
chance	O
of	O
of	O
falling	O
in	O
the	O
typical	B
set	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
importance	B
sampling	I
y	O
p	O
o	O
r	O
t	O
n	O
e	O
figure	O
entropy	B
of	O
a	O
ising	B
model	B
as	O
a	O
function	O
of	O
temperature	B
one	O
state	O
of	O
a	O
ising	B
model	B
temperature	B
the	O
number	O
of	O
samples	O
required	O
to	O
hit	O
the	O
typical	B
set	B
once	O
is	O
thus	O
of	O
order	O
rmin	O
so	O
what	O
is	O
h	O
at	O
high	O
temperatures	O
the	O
probability	B
distribution	B
of	O
an	O
ising	B
model	B
tends	O
to	O
a	O
uniform	O
distribution	B
and	O
the	O
entropy	B
tends	O
to	O
hmax	O
n	O
bits	O
which	O
means	O
rmin	O
is	O
of	O
order	O
under	O
these	O
conditions	O
uniform	O
sampling	O
may	O
well	O
be	O
a	O
satisfactory	O
technique	O
for	O
estimating	O
but	O
high	O
temperatures	O
are	O
not	O
of	O
great	O
interest	O
considerably	O
more	O
interesting	O
are	O
intermediate	O
temperatures	O
such	O
as	O
the	O
critical	O
temperature	B
at	O
which	O
the	O
ising	B
model	B
melts	O
from	O
an	O
ordered	O
phase	O
to	O
a	O
disordered	O
phase	O
the	O
critical	O
temperature	B
of	O
an	O
ising	B
model	B
at	O
which	O
it	O
melts	O
is	O
at	O
this	O
temperature	B
the	O
entropy	B
of	O
an	O
ising	B
model	B
is	O
roughly	O
bits	O
for	O
this	O
probability	B
distribution	B
the	O
number	O
of	O
samples	O
required	O
simply	O
to	O
hit	O
the	O
typical	B
set	B
once	O
is	O
of	O
order	O
rmin	O
which	O
for	O
n	O
is	O
about	O
this	O
is	O
roughly	O
the	O
square	B
of	O
the	O
number	O
of	O
particles	O
in	O
the	O
universe	O
thus	O
uniform	O
sampling	O
is	O
utterly	O
useless	O
for	O
the	O
study	O
of	O
ising	O
models	O
of	O
modest	O
size	O
and	O
in	O
most	O
high-dimensional	O
problems	O
if	O
the	O
distribution	B
p	O
is	O
not	O
actually	O
uniform	O
uniform	O
sampling	O
is	O
unlikely	O
to	O
be	O
useful	O
overview	O
having	O
established	O
that	O
drawing	O
samples	O
from	O
a	O
high-dimensional	O
distribution	B
p	O
p	O
is	O
even	O
if	O
p	O
is	O
easy	O
to	O
evaluate	O
we	O
will	O
now	O
study	O
a	O
sequence	B
of	O
more	O
sophisticated	O
monte	B
carlo	I
methods	I
importance	B
sampling	I
rejection	B
sampling	I
the	O
metropolis	B
method	I
gibbs	B
sampling	I
and	O
slice	B
sampling	I
importance	B
sampling	I
importance	B
sampling	I
is	O
not	O
a	O
method	O
for	O
generating	O
samples	O
from	O
p	O
it	O
is	O
just	O
a	O
method	O
for	O
estimating	O
the	O
expectation	B
of	O
a	O
function	O
it	O
can	O
be	O
viewed	O
as	O
a	O
generalization	B
of	O
the	O
uniform	O
sampling	O
method	O
for	O
illustrative	O
purposes	O
let	O
us	O
imagine	O
that	O
the	O
target	O
distribution	B
is	O
a	O
one-dimensional	O
density	B
p	O
let	O
us	O
assume	O
that	O
we	O
are	O
able	O
to	O
evaluate	O
this	O
density	B
at	O
any	O
chosen	O
point	O
x	O
at	O
least	O
to	O
within	O
a	O
multiplicative	O
constant	O
thus	O
we	O
can	O
evaluate	O
a	O
function	O
p	O
such	O
that	O
p	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
but	O
p	O
is	O
too	O
complicated	O
a	O
function	O
for	O
us	O
to	O
be	O
able	O
to	O
sample	B
from	I
it	O
directly	O
we	O
now	O
assume	O
that	O
we	O
have	O
a	O
simpler	O
density	B
qx	O
from	O
which	O
we	O
can	O
generate	O
samples	O
and	O
which	O
we	O
can	O
evaluate	O
to	O
within	O
a	O
multiplicative	O
constant	O
is	O
we	O
can	O
evaluate	O
where	O
qx	O
an	O
example	O
of	O
the	O
functions	B
p	O
and	O
is	O
shown	O
in	O
we	O
call	O
q	O
the	O
sampler	B
density	B
in	O
importance	B
sampling	I
we	O
generate	O
r	O
samples	O
fxrgr	O
from	O
qx	O
if	O
these	O
points	O
were	O
samples	O
from	O
p	O
then	O
we	O
could	O
estimate	O
by	O
equation	O
but	O
when	O
we	O
generate	O
samples	O
from	O
q	O
values	O
of	O
x	O
where	O
qx	O
is	O
greater	O
than	O
p	O
will	O
be	O
over-represented	O
in	O
this	O
estimator	B
and	O
points	O
where	O
qx	O
is	O
less	O
than	O
p	O
will	O
be	O
under-represented	O
to	O
take	O
into	O
account	O
the	O
fact	O
that	O
we	O
have	O
sampled	O
from	O
the	O
wrong	O
distribution	B
we	O
introduce	O
weights	O
wr	O
p	O
which	O
we	O
use	O
to	O
adjust	O
the	O
importance	O
of	O
each	O
point	O
in	O
our	O
estimator	B
thus	O
pr	O
pr	O
wr	O
exercise	O
prove	O
that	O
if	O
qx	O
is	O
non-zero	O
for	O
all	O
x	O
where	O
p	O
is	O
non-zero	O
the	O
estimator	B
converges	O
to	O
the	O
mean	B
value	O
of	O
as	O
r	O
increases	O
what	O
is	O
the	O
variance	B
of	O
this	O
estimator	B
asymptotically	O
hint	O
consider	O
the	O
statistics	O
of	O
the	O
numerator	O
and	O
the	O
denominator	O
separately	O
is	O
the	O
estimator	B
an	O
unbiased	B
estimator	B
for	O
small	O
r	O
a	O
practical	B
with	O
importance	B
sampling	I
is	O
that	O
it	O
is	O
hard	O
to	O
estimate	O
how	O
reliable	O
the	O
estimator	B
is	O
the	O
variance	B
of	O
the	O
estimator	B
is	O
unknown	O
beforehand	O
because	O
it	O
depends	O
on	O
an	O
integral	B
over	O
x	O
of	O
a	O
function	O
involving	O
p	O
and	O
the	O
variance	B
of	O
is	O
hard	O
to	O
estimate	O
because	O
the	O
empirical	O
variances	O
of	O
the	O
quantities	O
wr	O
and	O
are	O
not	O
necessarily	O
a	O
good	B
guide	O
to	O
the	O
true	O
variances	O
of	O
the	O
numerator	O
and	O
denominator	O
in	O
equation	O
if	O
the	O
proposal	B
density	B
qx	O
is	O
small	O
in	O
a	O
region	O
where	O
is	O
large	O
then	O
it	O
is	O
quite	O
possible	O
even	O
after	O
many	O
points	O
xr	O
have	O
been	O
generated	O
that	O
none	O
of	O
them	O
will	O
have	O
fallen	O
in	O
that	O
region	O
in	O
this	O
case	O
the	O
estimate	O
of	O
would	O
be	O
drastically	O
wrong	O
and	O
there	O
would	O
be	O
no	O
indication	O
in	O
the	O
empirical	O
variance	B
that	O
the	O
true	O
variance	B
of	O
the	O
estimator	B
is	O
large	O
cautionary	O
illustration	O
of	O
importance	B
sampling	I
in	O
a	O
toy	O
problem	O
related	O
to	O
the	O
modelling	B
of	O
amino	B
acid	I
probability	B
distributions	O
with	O
a	O
one-dimensional	O
variable	O
x	O
i	O
evaluated	O
a	O
quantity	O
of	O
interest	O
using	O
importance	B
sampling	I
the	O
results	O
using	O
a	O
gaussian	B
sampler	O
and	O
a	O
cauchy	B
sampler	O
are	O
shown	O
in	O
the	O
horizontal	O
axis	O
shows	O
the	O
number	O
of	O
p	O
x	O
figure	O
functions	B
involved	O
in	O
importance	B
sampling	I
we	O
wish	O
to	O
estimate	O
the	O
expectation	B
of	O
under	O
p	O
p	O
we	O
can	O
generate	O
samples	O
from	O
the	O
simpler	O
distribution	B
qx	O
we	O
can	O
evaluate	O
and	O
p	O
at	O
any	O
point	O
figure	O
importance	B
sampling	I
in	O
action	O
using	O
a	O
gaussian	B
sampler	B
density	B
using	O
a	O
cauchy	B
sampler	B
density	B
vertical	O
axis	O
shows	O
the	O
estimate	O
the	O
horizontal	O
line	O
indicates	O
the	O
true	O
value	O
of	O
horizontal	O
axis	O
shows	O
number	O
of	O
samples	O
on	O
a	O
log	O
scale	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
px	O
qx	O
phix	O
figure	O
a	O
multimodal	O
distribution	B
p	O
and	O
a	O
unimodal	O
sampler	O
qx	O
importance	B
sampling	I
samples	O
on	O
a	O
log	O
scale	O
in	O
the	O
case	O
of	O
the	O
gaussian	B
sampler	O
after	O
about	O
samples	O
had	O
been	O
evaluated	O
one	O
might	O
be	O
tempted	O
to	O
call	O
a	O
halt	O
but	O
evidently	O
there	O
are	O
infrequent	O
samples	O
that	O
make	O
a	O
huge	O
contribution	O
to	O
and	O
the	O
value	O
of	O
the	O
estimate	O
at	O
samples	O
is	O
wrong	O
even	O
after	O
a	O
million	O
samples	O
have	O
been	O
taken	O
the	O
estimate	O
has	O
still	O
not	O
settled	O
down	O
close	O
to	O
the	O
true	O
value	O
in	O
contrast	O
the	O
cauchy	B
sampler	O
does	O
not	O
from	O
glitches	O
it	O
converges	O
the	O
scale	O
shown	O
here	O
after	O
about	O
samples	O
this	O
example	O
illustrates	O
the	O
fact	O
that	O
an	O
importance	O
sampler	O
should	O
have	O
heavy	O
tails	O
exercise	O
consider	O
the	O
situation	O
where	O
p	O
is	O
multimodal	O
consisting	O
of	O
several	O
widely-separated	O
peaks	O
distributions	O
like	O
this	O
arise	O
frequently	O
in	O
statistical	B
data	O
modelling	B
discuss	O
whether	O
it	O
is	O
a	O
wise	O
strategy	O
to	O
do	O
importance	B
sampling	I
using	O
a	O
sampler	O
qx	O
that	O
is	O
a	O
unimodal	O
distribution	B
to	O
one	O
of	O
these	O
peaks	O
assume	O
that	O
the	O
function	O
whose	O
mean	B
is	O
to	O
be	O
estimated	O
is	O
a	O
smoothly	O
varying	O
function	O
of	O
x	O
such	O
as	O
mx	O
c	O
describe	O
the	O
typical	B
evolution	B
of	O
the	O
estimator	B
as	O
a	O
function	O
of	O
the	O
number	O
of	O
samples	O
r	O
importance	B
sampling	I
in	O
many	O
dimensions	B
we	O
have	O
already	O
observed	O
that	O
care	O
is	O
needed	O
in	O
one-dimensional	O
importance	B
sampling	I
problems	O
is	O
importance	B
sampling	I
a	O
useful	O
technique	O
in	O
spaces	O
of	O
higher	O
dimensionality	O
say	O
n	O
consider	O
a	O
simple	O
case-study	O
where	O
the	O
target	O
density	B
p	O
is	O
a	O
uniform	O
distribution	B
inside	O
a	O
sphere	O
where	O
the	O
origin	O
p	O
rp	O
rp	O
i	O
and	O
the	O
proposal	B
density	B
is	O
a	O
gaussian	B
centred	O
on	O
qx	O
normalxi	O
an	O
importance-sampling	O
method	O
will	O
be	O
in	O
trouble	O
if	O
the	O
estimator	B
is	O
dominated	O
by	O
a	O
few	O
large	O
weights	O
wr	O
what	O
will	O
be	O
the	O
typical	B
range	O
of	O
values	O
of	O
the	O
weights	O
wr	O
we	O
know	O
from	O
our	O
discussions	O
of	O
typical	B
sequences	O
in	O
part	O
i	O
see	O
exercise	O
for	O
example	O
that	O
if	O
is	O
the	O
distance	B
from	O
the	O
origin	O
of	O
a	O
sample	B
from	I
q	O
the	O
quantity	O
has	O
a	O
roughly	O
gaussian	B
distribution	B
with	O
mean	B
and	O
standard	B
deviation	I
n	O
thus	O
almost	O
all	O
samples	O
from	O
q	O
lie	O
in	O
a	O
typical	B
set	B
with	O
distance	B
from	O
the	O
origin	O
very	O
close	O
to	O
pn	O
let	O
us	O
assume	O
that	O
is	O
chosen	O
such	O
that	O
the	O
typical	B
set	B
of	O
q	O
lies	O
inside	O
the	O
sphere	O
of	O
radius	O
rp	O
it	O
does	O
not	O
then	O
the	O
law	B
of	I
large	I
numbers	I
implies	O
that	O
almost	O
all	O
the	O
samples	O
generated	O
from	O
q	O
will	O
fall	O
outside	O
rp	O
and	O
will	O
have	O
weight	O
zero	O
then	O
we	O
know	O
that	O
most	O
samples	O
from	O
q	O
will	O
have	O
a	O
value	O
of	O
q	O
that	O
lies	O
in	O
the	O
range	O
exp	O
n	O
thus	O
the	O
weights	O
wr	O
p	O
will	O
typically	O
have	O
values	O
in	O
the	O
range	O
exp	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
p	O
p	O
u	O
x	O
x	O
x	O
so	O
if	O
we	O
draw	O
a	O
hundred	O
samples	O
what	O
will	O
the	O
typical	B
range	O
of	O
weights	O
be	O
we	O
can	O
roughly	O
estimate	O
the	O
ratio	O
of	O
the	O
largest	O
weight	O
to	O
the	O
median	O
weight	O
by	O
doubling	O
the	O
standard	B
deviation	I
in	O
equation	O
the	O
largest	O
weight	O
and	O
the	O
median	O
weight	O
will	O
typically	O
be	O
in	O
the	O
ratio	O
monte	B
carlo	I
methods	I
figure	O
rejection	B
sampling	I
the	O
functions	B
involved	O
in	O
rejection	B
sampling	I
we	O
desire	O
samples	O
from	O
p	O
p	O
we	O
are	O
able	O
to	O
draw	O
samples	O
from	O
qx	O
and	O
we	O
know	O
a	O
value	O
c	O
such	O
that	O
c	O
p	O
for	O
all	O
x	O
a	O
point	O
u	O
is	O
generated	O
at	O
random	B
in	O
the	O
lightly	O
shaded	O
area	O
under	O
the	O
curve	O
c	O
if	O
this	O
point	O
also	O
lies	O
below	O
p	O
then	O
it	O
is	O
accepted	O
r	O
wmax	O
wmed	O
r	O
in	O
n	O
dimensions	B
therefore	O
the	O
largest	O
weight	O
after	O
one	O
hundred	O
samples	O
is	O
likely	O
to	O
be	O
roughly	O
times	O
greater	O
than	O
the	O
median	O
weight	O
thus	O
an	O
importance	B
sampling	I
estimate	O
for	O
a	O
high-dimensional	O
problem	O
will	O
very	O
likely	O
be	O
utterly	O
dominated	O
by	O
a	O
few	O
samples	O
with	O
huge	O
weights	O
in	O
conclusion	O
importance	B
sampling	I
in	O
high	B
dimensions	B
often	O
from	O
two	O
first	O
we	O
need	O
to	O
obtain	O
samples	O
that	O
lie	O
in	O
the	O
typical	B
set	B
of	O
p	O
and	O
this	O
may	O
take	O
a	O
long	O
time	O
unless	O
q	O
is	O
a	O
good	B
approximation	B
to	O
p	O
second	O
even	O
if	O
we	O
obtain	O
samples	O
in	O
the	O
typical	B
set	B
the	O
weights	O
associated	O
with	O
those	O
samples	O
are	O
likely	O
to	O
vary	O
by	O
large	O
factors	O
because	O
the	O
probabilities	O
of	O
points	O
in	O
a	O
typical	B
set	B
although	O
similar	O
to	O
each	O
other	O
still	O
by	O
factors	O
of	O
order	O
exppn	O
so	O
the	O
weights	O
will	O
too	O
unless	O
q	O
is	O
a	O
near-perfect	O
approximation	B
to	O
p	O
rejection	B
sampling	I
we	O
assume	O
again	O
a	O
one-dimensional	O
density	B
p	O
p	O
that	O
is	O
too	O
complicated	O
a	O
function	O
for	O
us	O
to	O
be	O
able	O
to	O
sample	B
from	I
it	O
directly	O
we	O
assume	O
that	O
we	O
have	O
a	O
simpler	O
proposal	B
density	B
qx	O
which	O
we	O
can	O
evaluate	O
a	O
multiplicative	O
factor	O
zq	O
as	O
before	O
and	O
from	O
which	O
we	O
can	O
generate	O
samples	O
we	O
further	O
assume	O
that	O
we	O
know	O
the	O
value	O
of	O
a	O
constant	O
c	O
such	O
that	O
c	O
p	O
for	O
all	O
x	O
a	O
schematic	O
picture	O
of	O
the	O
two	O
functions	B
is	O
shown	O
in	O
we	O
generate	O
two	O
random	B
numbers	O
the	O
x	O
is	O
generated	O
from	O
the	O
proposal	B
density	B
qx	O
we	O
then	O
evaluate	O
c	O
and	O
generate	O
a	O
uniformly	O
distributed	O
random	B
variable	I
u	O
from	O
the	O
interval	O
c	O
these	O
two	O
random	B
numbers	O
can	O
be	O
viewed	O
as	O
selecting	O
a	O
point	O
in	O
the	O
two-dimensional	B
plane	O
as	O
shown	O
in	O
we	O
now	O
evaluate	O
p	O
and	O
accept	O
or	O
reject	O
the	O
sample	B
x	O
by	O
comparing	O
the	O
value	O
of	O
u	O
with	O
the	O
value	O
of	O
p	O
if	O
u	O
p	O
then	O
x	O
is	O
rejected	O
otherwise	O
it	O
is	O
accepted	O
which	O
means	O
that	O
we	O
add	O
x	O
to	O
our	O
set	B
of	O
samples	O
fxrg	O
the	O
value	O
of	O
u	O
is	O
discarded	O
why	O
does	O
this	O
procedure	O
generate	O
samples	O
from	O
p	O
the	O
proposed	O
point	O
u	O
comes	O
with	O
uniform	O
probability	B
from	O
the	O
lightly	O
shaded	O
area	O
underneath	O
the	O
curve	O
c	O
as	O
shown	O
in	O
the	O
rejection	B
rule	O
rejects	O
all	O
the	O
points	O
that	O
lie	O
above	O
the	O
curve	O
p	O
so	O
the	O
points	O
u	O
that	O
are	O
accepted	O
are	O
uniformly	O
distributed	O
in	O
the	O
heavily	O
shaded	O
area	O
under	O
p	O
this	O
implies	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
metropolishastings	O
method	O
that	O
the	O
probability	B
density	B
of	O
the	O
x-coordinates	O
of	O
the	O
accepted	O
points	O
must	O
be	O
proportional	O
to	O
p	O
so	O
the	O
samples	O
must	O
be	O
independent	O
samples	O
from	O
p	O
rejection	B
sampling	I
will	O
work	O
best	O
if	O
q	O
is	O
a	O
good	B
approximation	B
to	O
p	O
if	O
q	O
is	O
very	O
from	O
p	O
then	O
for	O
c	O
q	O
to	O
exceed	O
p	O
everywhere	O
c	O
will	O
necessarily	O
have	O
to	O
be	O
large	O
and	O
the	O
frequency	B
of	O
rejection	B
will	O
be	O
large	O
rejection	B
sampling	I
in	O
many	O
dimensions	B
in	O
a	O
high-dimensional	O
problem	O
it	O
is	O
very	O
likely	O
that	O
the	O
requirement	O
that	O
c	O
be	O
an	O
upper	O
bound	B
for	O
p	O
will	O
force	O
c	O
to	O
be	O
so	O
huge	O
that	O
acceptances	O
will	O
be	O
very	O
rare	O
indeed	O
finding	O
such	O
a	O
value	O
of	O
c	O
may	O
be	O
too	O
since	O
in	O
many	O
problems	O
we	O
know	O
neither	O
where	O
the	O
modes	O
of	O
p	O
are	O
located	O
nor	O
how	O
high	O
they	O
are	O
as	O
a	O
case	O
study	O
consider	O
a	O
pair	O
of	O
n	O
gaussian	B
distributions	O
with	O
mean	B
zero	O
imagine	O
generating	O
samples	O
from	O
one	O
with	O
standard	B
deviation	I
and	O
using	O
rejection	B
sampling	I
to	O
obtain	O
samples	O
from	O
the	O
other	O
whose	O
standard	B
deviation	I
is	O
let	O
us	O
assume	O
that	O
these	O
two	O
standard	O
deviations	O
are	O
close	O
in	O
value	O
say	O
is	O
larger	O
than	O
must	O
be	O
larger	O
than	O
because	O
if	O
this	O
is	O
not	O
the	O
case	O
there	O
is	O
no	O
c	O
such	O
that	O
c	O
q	O
exceeds	O
p	O
for	O
all	O
x	O
so	O
what	O
value	O
of	O
c	O
is	O
required	O
if	O
the	O
dimensionality	O
is	O
n	O
the	O
density	B
of	O
qx	O
at	O
the	O
origin	O
is	O
so	O
for	O
c	O
q	O
to	O
exceed	O
p	O
we	O
need	O
to	O
set	B
c	O
p	O
ln	O
with	O
n	O
and	O
we	O
c	O
what	O
will	O
the	O
acceptance	B
rate	B
be	O
for	O
this	O
value	O
of	O
c	O
the	O
answer	O
is	O
immediate	O
since	O
the	O
acceptance	B
rate	B
is	O
the	O
ratio	O
of	O
the	O
volume	B
under	O
the	O
curve	O
p	O
to	O
the	O
volume	B
under	O
c	O
qx	O
the	O
fact	O
that	O
p	O
and	O
q	O
are	O
both	O
normalized	O
here	O
implies	O
that	O
the	O
acceptance	B
rate	B
will	O
be	O
for	O
example	O
in	O
general	O
c	O
grows	O
exponentially	O
with	O
the	O
dimensionality	O
n	O
so	O
the	O
acceptance	B
rate	B
is	O
expected	O
to	O
be	O
exponentially	O
small	O
in	O
n	O
rejection	B
sampling	I
therefore	O
whilst	O
a	O
useful	O
method	O
for	O
one-dimensional	O
problems	O
is	O
not	O
expected	O
to	O
be	O
a	O
practical	B
technique	O
for	O
generating	O
samples	O
from	O
high-dimensional	O
distributions	O
p	O
the	O
metropolishastings	O
method	O
importance	B
sampling	I
and	O
rejection	B
sampling	I
work	O
well	O
only	O
if	O
the	O
proposal	B
density	B
qx	O
is	O
similar	O
to	O
p	O
in	O
large	O
and	O
complex	B
problems	O
it	O
is	O
to	O
create	O
a	O
single	O
density	B
qx	O
that	O
has	O
this	O
property	O
the	O
metropolishastings	O
algorithm	O
instead	O
makes	O
use	O
of	O
a	O
proposal	B
density	B
q	O
which	O
depends	O
on	O
the	O
current	O
state	O
xt	O
the	O
density	B
xt	O
might	O
be	O
a	O
simple	O
distribution	B
such	O
as	O
a	O
gaussian	B
centred	O
on	O
the	O
current	O
xt	O
the	O
proposal	B
density	B
x	O
can	O
be	O
any	O
density	B
from	O
which	O
we	O
can	O
draw	O
samples	O
in	O
contrast	O
to	O
importance	B
sampling	I
and	O
rejection	B
sampling	I
it	O
is	O
not	O
necessary	O
that	O
xt	O
look	O
at	O
all	O
similar	O
to	O
p	O
in	O
order	O
for	O
the	O
algorithm	O
to	O
be	O
practically	O
useful	O
an	O
example	O
of	O
a	O
proposal	B
density	B
is	O
shown	O
in	O
this	O
shows	O
the	O
density	B
xt	O
for	O
two	O
states	O
and	O
as	O
before	O
we	O
assume	O
that	O
we	O
can	O
evaluate	O
p	O
for	O
any	O
x	O
a	O
tentative	O
new	O
state	O
is	O
generated	O
from	O
the	O
proposal	B
density	B
xt	O
to	O
decide	O
px	O
cqx	O
figure	O
a	O
gaussian	B
p	O
and	O
a	O
slightly	O
broader	O
gaussian	B
qx	O
scaled	O
up	O
by	O
a	O
factor	O
c	O
such	O
that	O
c	O
qx	O
p	O
qx	O
p	O
p	O
x	O
qx	O
x	O
figure	O
metropolishastings	O
method	O
in	O
one	O
dimension	O
the	O
proposal	O
distribution	B
x	O
is	O
here	O
shown	O
as	O
having	O
a	O
shape	O
that	O
changes	O
as	O
x	O
changes	O
though	O
this	O
is	O
not	O
typical	B
of	O
the	O
proposal	O
densities	O
used	O
in	O
practice	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
whether	O
to	O
accept	O
the	O
new	O
state	O
we	O
compute	O
the	O
quantity	O
a	O
p	O
p	O
qxt	O
xt	O
if	O
a	O
then	O
the	O
new	O
state	O
is	O
accepted	O
otherwise	O
the	O
new	O
state	O
is	O
accepted	O
with	O
probability	B
a	O
if	O
the	O
step	O
is	O
accepted	O
we	O
set	B
if	O
the	O
step	O
is	O
rejected	O
then	O
we	O
set	B
xt	O
note	O
the	O
from	O
rejection	B
sampling	I
in	O
rejection	B
sampling	I
rejected	O
points	O
are	O
discarded	O
and	O
have	O
no	O
on	O
the	O
list	O
of	O
samples	O
fxrg	O
that	O
we	O
collected	O
here	O
a	O
rejection	B
causes	O
the	O
current	O
state	O
to	O
be	O
written	O
again	O
onto	O
the	O
list	O
notation	B
i	O
have	O
used	O
the	O
superscript	O
r	O
r	O
to	O
label	O
points	O
that	O
are	O
independent	O
samples	O
from	O
a	O
distribution	B
and	O
the	O
superscript	O
t	O
t	O
to	O
label	O
the	O
sequence	B
of	O
states	O
in	O
a	O
markov	B
chain	I
it	O
is	O
important	O
to	O
note	O
that	O
a	O
metropolishastings	O
simulation	O
of	O
t	O
iterations	O
does	O
not	O
produce	O
t	O
independent	O
samples	O
from	O
the	O
target	O
distribution	B
p	O
the	O
samples	O
are	O
dependent	O
to	O
compute	O
the	O
acceptance	O
probability	B
we	O
need	O
to	O
be	O
able	O
to	O
compute	O
the	O
probability	B
ratios	O
p	O
and	O
qxt	O
xt	O
if	O
the	O
proposal	B
density	B
is	O
a	O
simple	O
symmetrical	O
density	B
such	O
as	O
a	O
gaussian	B
centred	O
on	O
the	O
current	O
point	O
then	O
the	O
latter	O
factor	O
is	O
unity	O
and	O
the	O
metropolishastings	O
method	O
simply	O
involves	O
comparing	O
the	O
value	O
of	O
the	O
target	O
density	B
at	O
the	O
two	O
points	O
this	O
special	O
case	O
is	O
sometimes	O
called	O
the	O
metropolis	B
method	I
however	O
with	O
apologies	O
to	O
hastings	O
i	O
will	O
call	O
the	O
general	O
metropolishastings	O
algorithm	O
for	O
asymmetric	O
q	O
the	O
metropolis	B
method	I
since	O
i	O
believe	O
important	O
ideas	O
deserve	O
short	O
names	O
convergence	O
of	O
the	O
metropolis	B
method	I
to	O
the	O
target	O
density	B
it	O
can	O
be	O
shown	O
that	O
for	O
any	O
positive	O
q	O
is	O
any	O
q	O
such	O
that	O
x	O
for	O
all	O
x	O
as	O
t	O
the	O
probability	B
distribution	B
of	O
xt	O
tends	O
to	O
p	O
p	O
statement	O
should	O
not	O
be	O
seen	O
as	O
implying	O
that	O
q	O
has	O
to	O
assign	O
positive	O
probability	B
to	O
every	O
point	O
we	O
will	O
discuss	O
examples	O
later	O
where	O
x	O
for	O
some	O
x	O
notice	O
also	O
that	O
we	O
have	O
said	O
nothing	O
about	O
how	O
rapidly	O
the	O
convergence	O
to	O
p	O
takes	O
place	O
the	O
metropolis	B
method	I
is	O
an	O
example	O
of	O
a	O
markov	B
chain	I
monte	B
carlo	I
method	O
mcmc	O
in	O
contrast	O
to	O
rejection	B
sampling	I
where	O
the	O
accepted	O
points	O
fxrg	O
are	O
independent	O
samples	O
from	O
the	O
desired	O
distribution	B
markov	B
chain	I
monte	B
carlo	I
methods	I
involve	O
a	O
markov	O
process	O
in	O
which	O
a	O
sequence	B
of	O
states	O
fxtg	O
is	O
generated	O
each	O
sample	B
xt	O
having	O
a	O
probability	B
distribution	B
that	O
depends	O
on	O
the	O
previous	O
value	O
since	O
successive	O
samples	O
are	O
dependent	O
the	O
markov	B
chain	I
may	O
have	O
to	O
be	O
run	O
for	O
a	O
considerable	O
time	O
in	O
order	O
to	O
generate	O
samples	O
that	O
are	O
independent	O
samples	O
from	O
p	O
just	O
as	O
it	O
was	O
to	O
estimate	O
the	O
variance	B
of	O
an	O
importance	B
sampling	I
estimator	B
so	O
it	O
is	O
to	O
assess	O
whether	O
a	O
markov	B
chain	I
monte	B
carlo	I
method	O
has	O
converged	O
and	O
to	O
quantify	O
how	O
long	O
one	O
has	O
to	O
wait	O
to	O
obtain	O
samples	O
that	O
are	O
independent	O
samples	O
from	O
p	O
demonstration	O
of	O
the	O
metropolis	B
method	I
the	O
metropolis	B
method	I
is	O
widely	O
used	O
for	O
high-dimensional	O
problems	O
many	O
implementations	O
of	O
the	O
metropolis	B
method	I
employ	O
a	O
proposal	O
distribution	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
metropolishastings	O
method	O
qx	O
p	O
l	O
figure	O
metropolis	B
method	I
in	O
two	O
dimensions	B
showing	O
a	O
traditional	O
proposal	B
density	B
that	O
has	O
a	O
small	O
step	O
size	O
that	O
the	O
acceptance	O
frequency	B
will	O
be	O
about	O
with	O
a	O
length	O
scale	O
that	O
is	O
short	O
relative	B
to	O
the	O
longest	O
length	O
scale	O
l	O
of	O
the	O
probable	O
region	O
a	O
reason	O
for	O
choosing	O
a	O
small	O
length	O
scale	O
is	O
that	O
for	O
most	O
high-dimensional	O
problems	O
a	O
large	O
random	B
step	O
from	O
a	O
typical	B
point	O
is	O
a	O
sample	B
from	I
p	O
is	O
very	O
likely	O
to	O
end	O
in	O
a	O
state	O
that	O
has	O
very	O
low	O
probability	B
such	O
steps	O
are	O
unlikely	O
to	O
be	O
accepted	O
if	O
is	O
large	O
movement	O
around	O
the	O
state	O
space	O
will	O
only	O
occur	O
when	O
such	O
a	O
transition	B
to	O
a	O
low-probability	O
state	O
is	O
actually	O
accepted	O
or	O
when	O
a	O
large	O
random	B
step	O
chances	O
to	O
land	O
in	O
another	O
probable	O
state	O
so	O
the	O
rate	B
of	O
progress	O
will	O
be	O
slow	O
if	O
large	O
steps	O
are	O
used	O
the	O
disadvantage	O
of	O
small	O
steps	O
on	O
the	O
other	O
hand	O
is	O
that	O
the	O
metropolis	B
method	I
will	O
explore	B
the	O
probability	B
distribution	B
by	O
a	O
random	B
walk	I
and	O
a	O
random	B
walk	I
takes	O
a	O
long	O
time	O
to	O
get	O
anywhere	O
especially	O
if	O
the	O
walk	O
is	O
made	O
of	O
small	O
steps	O
exercise	O
consider	O
a	O
one-dimensional	O
random	B
walk	I
on	O
each	O
step	O
of	O
which	O
the	O
state	O
moves	O
randomly	O
to	O
the	O
left	O
or	O
to	O
the	O
right	O
with	O
equal	O
probability	B
show	O
that	O
after	O
t	O
steps	O
of	O
size	O
the	O
state	O
is	O
likely	O
to	O
have	O
moved	O
only	O
a	O
distance	B
about	O
pt	O
the	O
root	O
mean	B
square	B
distance	B
travelled	O
recall	O
that	O
the	O
aim	O
of	O
monte	B
carlo	I
sampling	O
is	O
to	O
generate	O
a	O
number	O
of	O
independent	O
samples	O
from	O
the	O
given	O
distribution	B
dozen	O
say	O
if	O
the	O
largest	O
length	O
scale	O
of	O
the	O
state	O
space	O
is	O
l	O
then	O
we	O
have	O
to	O
simulate	O
a	O
random-walk	B
metropolis	B
method	I
for	O
a	O
time	O
t	O
before	O
we	O
can	O
expect	O
to	O
get	O
a	O
sample	B
that	O
is	O
roughly	O
independent	O
of	O
the	O
initial	O
condition	O
and	O
that	O
s	O
assuming	O
that	O
every	O
step	O
is	O
accepted	O
if	O
only	O
a	O
fraction	O
f	O
of	O
the	O
steps	O
are	O
accepted	O
on	O
average	O
then	O
this	O
time	O
is	O
increased	O
by	O
a	O
factor	O
rule	B
of	I
thumb	I
lower	O
bound	B
on	O
number	O
of	O
iterations	O
of	O
a	O
metropolis	B
method	I
if	O
the	O
largest	O
length	O
scale	O
of	O
the	O
space	O
of	O
probable	O
states	O
is	O
l	O
a	O
metropolis	B
method	I
whose	O
proposal	O
distribution	B
generates	O
a	O
random	B
walk	I
with	O
step	O
size	O
must	O
be	O
run	O
for	O
at	O
least	O
t	O
iterations	O
to	O
obtain	O
an	O
independent	O
sample	B
this	O
rule	B
of	I
thumb	I
gives	O
only	O
a	O
lower	O
bound	B
the	O
situation	O
may	O
be	O
much	O
worse	O
if	O
for	O
example	O
the	O
probability	B
distribution	B
consists	O
of	O
several	O
islands	O
of	O
high	O
probability	B
separated	O
by	O
regions	O
of	O
low	O
probability	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
figure	O
metropolis	B
method	I
for	O
a	O
toy	O
problem	O
the	O
state	O
sequence	B
for	O
t	O
horizontal	O
direction	O
states	O
from	O
to	O
vertical	O
direction	O
time	O
from	O
to	O
the	O
cross	O
bars	O
mark	O
time	O
intervals	B
of	O
duration	O
histogram	O
of	O
occupancy	O
of	O
the	O
states	O
after	O
and	O
iterations	O
for	O
comparison	O
histograms	O
resulting	O
when	O
successive	O
points	O
are	O
drawn	O
independently	O
from	O
the	O
target	O
distribution	B
metropolis	O
independent	O
sampling	O
iterations	O
iterations	O
iterations	O
iterations	O
iterations	O
iterations	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
metropolishastings	O
method	O
to	O
illustrate	O
how	O
slowly	O
a	O
random	B
walk	I
explores	O
a	O
state	O
space	O
shows	O
a	O
simulation	O
of	O
a	O
metropolis	O
algorithm	O
for	O
generating	O
samples	O
from	O
the	O
distribution	B
p	O
x	O
otherwise	O
the	O
proposal	O
distribution	B
is	O
otherwise	O
x	O
x	O
because	O
the	O
target	O
distribution	B
p	O
is	O
uniform	O
rejections	O
occur	O
only	O
when	O
the	O
proposal	O
takes	O
the	O
state	O
to	O
or	O
the	O
simulation	O
was	O
started	O
in	O
the	O
state	O
and	O
its	O
evolution	B
is	O
shown	O
in	O
how	O
long	O
does	O
it	O
take	O
to	O
reach	O
one	O
of	O
the	O
end	O
states	O
x	O
and	O
x	O
since	O
the	O
distance	B
is	O
steps	O
the	O
rule	B
of	I
thumb	I
predicts	O
that	O
it	O
will	O
typically	O
take	O
a	O
time	O
t	O
iterations	O
to	O
reach	O
an	O
end	O
state	O
this	O
is	O
in	O
the	O
present	O
example	O
the	O
step	O
into	O
an	O
end	O
state	O
occurs	O
on	O
the	O
iteration	O
how	O
long	O
does	O
it	O
take	O
to	O
visit	O
both	O
end	O
states	O
the	O
rule	B
of	I
thumb	I
predicts	O
about	O
iterations	O
are	O
required	O
to	O
traverse	O
the	O
whole	O
state	O
space	O
and	O
indeed	O
the	O
encounter	O
with	O
the	O
other	O
end	O
state	O
takes	O
place	O
on	O
the	O
iteration	O
thus	O
samples	O
are	O
generated	O
only	O
by	O
simulating	O
for	O
about	O
four	O
hundred	O
iterations	O
per	O
independent	O
sample	B
this	O
simple	O
example	O
shows	O
that	O
it	O
is	O
important	O
to	O
try	O
to	O
abolish	O
random	B
walk	I
behaviour	O
in	O
monte	B
carlo	I
methods	I
a	O
systematic	B
exploration	O
of	O
the	O
toy	O
state	O
space	O
could	O
get	O
around	O
it	O
using	O
the	O
same	O
step	O
sizes	O
in	O
about	O
twenty	O
steps	O
instead	O
of	O
four	O
hundred	O
methods	O
for	O
reducing	O
random	B
walk	I
behaviour	O
are	O
discussed	O
in	O
the	O
next	O
chapter	O
metropolis	B
method	I
in	O
high	B
dimensions	B
the	O
rule	B
of	I
thumb	I
which	O
gives	O
a	O
lower	O
bound	B
on	O
the	O
number	O
of	O
iterations	O
of	O
a	O
random	B
walk	I
metropolis	B
method	I
also	O
applies	O
to	O
higher-dimensional	O
problems	O
consider	O
the	O
simple	O
case	O
of	O
a	O
target	O
distribution	B
that	O
is	O
an	O
n	O
dimensional	O
gaussian	B
and	O
a	O
proposal	O
distribution	B
that	O
is	O
a	O
spherical	O
gaussian	B
of	O
standard	B
deviation	I
in	O
each	O
direction	O
without	O
loss	O
of	O
generality	O
we	O
can	O
assume	O
that	O
the	O
target	O
distribution	B
is	O
a	O
separable	O
distribution	B
aligned	O
with	O
the	O
axes	O
fxng	O
and	O
that	O
it	O
has	O
standard	B
deviation	I
in	O
direction	O
n	O
let	O
and	O
be	O
the	O
largest	O
and	O
smallest	O
of	O
these	O
standard	O
deviations	O
let	O
us	O
assume	O
that	O
is	O
adjusted	O
such	O
that	O
the	O
acceptance	O
frequency	B
is	O
close	O
to	O
under	O
this	O
assumption	O
each	O
variable	O
xn	O
evolves	O
independently	O
of	O
all	O
the	O
others	O
executing	O
a	O
random	B
walk	I
with	O
step	O
size	O
about	O
the	O
time	O
taken	O
to	O
generate	O
independent	O
samples	O
from	O
the	O
target	O
distribution	B
will	O
be	O
controlled	O
by	O
the	O
largest	O
lengthscale	O
just	O
as	O
in	O
the	O
previous	O
section	B
where	O
we	O
needed	O
at	O
least	O
t	O
iterations	O
to	O
obtain	O
an	O
independent	O
sample	B
here	O
we	O
need	O
t	O
now	O
how	O
big	O
can	O
be	O
the	O
bigger	O
it	O
is	O
the	O
smaller	O
this	O
number	O
t	O
becomes	O
but	O
if	O
is	O
too	O
big	O
bigger	O
than	O
then	O
the	O
acceptance	B
rate	B
will	O
fall	O
sharply	O
it	O
seems	O
plausible	O
that	O
the	O
optimal	B
must	O
be	O
similar	O
to	O
min	O
strictly	O
this	O
may	O
not	O
be	O
true	O
in	O
special	O
cases	O
where	O
the	O
second	O
smallest	O
is	O
greater	O
than	O
the	O
optimal	B
may	O
be	O
closer	O
to	O
that	O
second	O
smallest	O
but	O
our	O
rough	O
conclusion	O
is	O
this	O
where	O
simple	O
spherical	O
proposal	O
distributions	O
are	O
used	O
we	O
will	O
need	O
at	O
least	O
t	O
iterations	O
to	O
obtain	O
an	O
independent	O
sample	B
where	O
and	O
are	O
the	O
longest	O
and	O
shortest	O
lengthscales	O
of	O
the	O
target	O
distribution	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
figure	O
gibbs	B
sampling	I
the	O
joint	B
density	B
p	O
from	O
which	O
samples	O
are	O
required	O
starting	O
from	O
a	O
state	O
xt	O
is	O
sampled	O
from	O
the	O
conditional	B
density	B
p	O
j	O
xt	O
a	O
sample	B
is	O
then	O
made	O
from	O
the	O
conditional	B
density	B
p	O
j	O
a	O
couple	O
of	O
iterations	O
of	O
gibbs	B
sampling	I
p	O
p	O
j	O
xt	O
xt	O
p	O
j	O
xt	O
this	O
is	O
good	B
news	O
and	O
bad	B
news	O
it	O
is	O
good	B
news	O
because	O
unlike	O
the	O
cases	O
of	O
rejection	B
sampling	I
and	O
importance	B
sampling	I
there	O
is	O
no	O
catastrophic	O
dependence	O
on	O
the	O
dimensionality	O
n	O
our	O
computer	B
will	O
give	O
useful	O
answers	O
in	O
a	O
time	O
shorter	O
than	O
the	O
age	O
of	O
the	O
universe	O
but	O
it	O
is	O
bad	B
news	O
all	O
the	O
same	O
because	O
this	O
quadratic	O
dependence	O
on	O
the	O
lengthscale-ratio	O
may	O
still	O
force	O
us	O
to	O
make	O
very	O
lengthy	O
simulations	O
fortunately	O
there	O
are	O
methods	O
for	O
suppressing	O
random	B
walks	O
in	O
monte	B
carlo	I
simulations	O
which	O
we	O
will	O
discuss	O
in	O
the	O
next	O
chapter	O
gibbs	B
sampling	I
we	O
introduced	O
importance	B
sampling	I
rejection	B
sampling	I
and	O
the	O
metropolis	B
method	I
using	O
one-dimensional	O
examples	O
gibbs	B
sampling	I
also	O
known	O
as	O
the	O
heat	B
bath	I
method	O
or	O
glauber	B
dynamics	I
is	O
a	O
method	O
for	O
sampling	O
from	O
distributions	O
over	O
at	O
least	O
two	O
dimensions	B
gibbs	B
sampling	I
can	O
be	O
viewed	O
as	O
a	O
metropolis	B
method	I
in	O
which	O
a	O
sequence	B
of	O
proposal	O
distributions	O
q	O
are	O
in	O
terms	O
of	O
the	O
conditional	B
distributions	O
of	O
the	O
joint	B
distribution	B
p	O
it	O
is	O
assumed	O
that	O
whilst	O
p	O
is	O
too	O
complex	B
to	O
draw	O
samples	O
from	O
directly	O
its	O
conditional	B
distributions	O
p	O
are	O
tractable	O
to	O
work	O
with	O
for	O
many	O
graphical	O
models	O
not	O
all	O
these	O
one-dimensional	O
conditional	B
distributions	O
are	O
straightforward	O
to	O
sample	B
from	I
for	O
example	O
if	O
a	O
gaussian	B
distribution	B
for	O
some	O
variables	O
d	O
has	O
an	O
unknown	O
mean	B
m	O
and	O
the	O
prior	B
distribution	B
of	O
m	O
is	O
gaussian	B
then	O
the	O
conditional	B
distribution	B
of	O
m	O
given	O
d	O
is	O
also	O
gaussian	B
conditional	B
distributions	O
that	O
are	O
not	O
of	O
standard	O
form	O
may	O
still	O
be	O
sampled	O
from	O
by	O
adaptive	B
rejection	B
sampling	I
if	O
the	O
conditional	B
distribution	B
certain	O
convexity	B
properties	O
and	O
wild	O
gibbs	B
sampling	I
is	O
illustrated	O
for	O
a	O
case	O
with	O
two	O
variables	O
x	O
in	O
on	O
each	O
iteration	O
we	O
start	O
from	O
the	O
current	O
state	O
xt	O
and	O
is	O
sampled	O
from	O
the	O
conditional	B
density	B
p	O
j	O
with	O
to	O
xt	O
a	O
sample	B
is	O
then	O
made	O
from	O
the	O
conditional	B
density	B
p	O
j	O
using	O
the	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gibbs	B
sampling	I
new	O
value	O
of	O
this	O
brings	O
us	O
to	O
the	O
new	O
state	O
and	O
completes	O
the	O
iteration	O
in	O
the	O
general	O
case	O
of	O
a	O
system	O
with	O
k	O
variables	O
a	O
single	O
iteration	O
involves	O
sampling	O
one	O
parameter	O
at	O
a	O
time	O
p	O
j	O
xt	O
xt	O
p	O
j	O
p	O
j	O
xt	O
k	O
xt	O
xt	O
k	O
xt	O
k	O
etc	O
convergence	O
of	O
gibbs	B
sampling	I
to	O
the	O
target	O
density	B
exercise	O
show	O
that	O
a	O
single	O
variable-update	O
of	O
gibbs	B
sampling	I
can	O
be	O
viewed	O
as	O
a	O
metropolis	B
method	I
with	O
target	O
density	B
p	O
and	O
that	O
this	O
metropolis	B
method	I
has	O
the	O
property	O
that	O
every	O
proposal	O
is	O
always	O
accepted	O
because	O
gibbs	B
sampling	I
is	O
a	O
metropolis	B
method	I
the	O
probability	B
distribution	B
of	O
xt	O
tends	O
to	O
p	O
as	O
t	O
as	O
long	O
as	O
p	O
does	O
not	O
have	O
pathological	O
properties	O
exercise	O
discuss	O
whether	O
the	O
syndrome	B
decoding	B
problem	O
for	O
a	O
hamming	B
code	I
can	O
be	O
solved	O
using	O
gibbs	B
sampling	I
the	O
syndrome	B
decoding	B
problem	O
if	O
we	O
are	O
to	O
solve	O
it	O
with	O
a	O
monte	B
carlo	I
approach	O
is	O
to	O
draw	O
samples	O
from	O
the	O
posterior	O
distribution	B
of	O
the	O
noise	O
vector	O
n	O
nn	O
nn	O
p	O
f	O
z	O
z	O
n	O
f	O
nn	O
n	O
z	O
where	O
fn	O
is	O
the	O
normalized	O
likelihood	B
for	O
the	O
nth	O
transmitted	O
bit	B
and	O
z	O
is	O
the	O
observed	O
syndrome	B
the	O
factor	O
z	O
is	O
if	O
n	O
has	O
the	O
correct	O
syndrome	B
z	O
and	O
otherwise	O
what	O
about	O
the	O
syndrome	B
decoding	B
problem	O
for	O
any	O
linear	B
error-correcting	B
code	I
gibbs	B
sampling	I
in	O
high	B
dimensions	B
gibbs	B
sampling	I
from	O
the	O
same	O
defect	O
as	O
simple	O
metropolis	O
algorithms	B
the	O
state	O
space	O
is	O
explored	O
by	O
a	O
slow	O
random	B
walk	I
unless	O
a	O
fortuitous	O
parameterization	O
has	O
been	O
chosen	O
that	O
makes	O
the	O
probability	B
distribution	B
p	O
separable	O
if	O
say	O
two	O
variables	O
and	O
are	O
strongly	O
correlated	O
having	O
marginal	B
densities	O
of	O
width	O
l	O
and	O
conditional	B
densities	O
of	O
width	O
then	O
it	O
will	O
take	O
at	O
least	O
about	O
iterations	O
to	O
generate	O
an	O
independent	O
sample	B
from	I
the	O
target	O
density	B
figure	O
illustrates	O
the	O
slow	O
progress	O
made	O
by	O
gibbs	B
sampling	I
when	O
l	O
however	O
gibbs	B
sampling	I
involves	O
no	O
adjustable	O
parameters	B
so	O
it	O
is	O
an	O
attractive	O
strategy	O
when	O
one	O
wants	O
to	O
get	O
a	O
model	B
running	O
quickly	O
an	O
excellent	O
software	B
package	O
bugs	B
makes	O
it	O
easy	O
to	O
set	B
up	O
almost	O
arbitrary	O
probabilistic	O
models	O
and	O
simulate	O
them	O
by	O
gibbs	B
sampling	I
et	O
al	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
terminology	O
for	O
markov	B
chain	I
monte	B
carlo	I
methods	I
we	O
now	O
spend	O
a	O
few	O
moments	O
sketching	O
the	O
theory	O
on	O
which	O
the	O
metropolis	B
method	I
and	O
gibbs	B
sampling	I
are	O
based	O
we	O
denote	O
by	O
ptx	O
the	O
probability	B
distribution	B
of	O
the	O
state	O
of	O
a	O
markov	B
chain	I
simulator	O
visualize	O
this	O
distribution	B
imagine	O
running	O
an	O
collection	O
of	O
identical	O
simulators	O
in	O
parallel	O
our	O
aim	O
is	O
to	O
a	O
markov	B
chain	I
such	O
that	O
as	O
t	O
ptx	O
tends	O
to	O
the	O
desired	O
distribution	B
p	O
a	O
markov	B
chain	I
can	O
be	O
by	O
an	O
initial	O
probability	B
distribution	B
and	O
a	O
transition	B
probability	B
t	O
x	O
the	O
probability	B
distribution	B
of	O
the	O
state	O
at	O
the	O
iteration	O
of	O
the	O
markov	B
chain	I
is	O
given	O
by	O
dn	O
x	O
t	O
xptx	O
example	O
an	O
example	O
of	O
a	O
markov	B
chain	I
is	O
given	O
by	O
the	O
metropolis	O
demonstration	O
of	O
section	B
for	O
which	O
the	O
transition	B
probability	B
is	O
t	O
and	O
the	O
initial	O
distribution	B
was	O
the	O
probability	B
distribution	B
ptx	O
of	O
the	O
state	O
at	O
the	O
tth	O
iteration	O
is	O
shown	O
for	O
t	O
in	O
an	O
equivalent	O
sequence	B
of	O
distributions	O
is	O
shown	O
in	O
for	O
the	O
chain	O
that	O
begins	O
in	O
initial	O
state	O
both	O
chains	O
converge	O
to	O
the	O
target	O
density	B
the	O
uniform	O
density	B
as	O
t	O
required	O
properties	O
when	O
designing	O
a	O
markov	B
chain	I
monte	B
carlo	I
method	O
we	O
construct	O
a	O
chain	O
with	O
the	O
following	O
properties	O
the	O
desired	O
distribution	B
p	O
is	O
an	O
invariant	B
distribution	B
of	O
the	O
chain	O
figure	O
the	O
probability	B
distribution	B
of	O
the	O
state	O
of	O
the	O
markov	B
chain	I
of	O
example	O
a	O
distribution	B
is	O
an	O
invariant	B
distribution	B
of	O
the	O
transition	B
probability	B
t	O
x	O
if	O
an	O
invariant	B
distribution	B
is	O
an	O
eigenvector	O
of	O
the	O
transition	B
probability	B
matrix	B
that	O
has	O
eigenvalue	B
dn	O
x	O
t	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
terminology	O
for	O
markov	B
chain	I
monte	B
carlo	I
methods	I
the	O
chain	O
must	O
also	O
be	O
ergodic	B
that	O
is	O
ptx	O
as	O
t	O
for	O
any	O
a	O
couple	O
of	O
reasons	O
why	O
a	O
chain	O
might	O
not	O
be	O
ergodic	B
are	O
figure	O
the	O
probability	B
distribution	B
of	O
the	O
state	O
of	O
the	O
markov	B
chain	I
for	O
initial	O
condition	O
its	O
matrix	B
might	O
be	O
reducible	B
which	O
means	O
that	O
the	O
state	O
space	O
contains	O
two	O
or	O
more	O
subsets	O
of	O
states	O
that	O
can	O
never	O
be	O
reached	O
from	O
each	O
other	O
such	O
a	O
chain	O
has	O
many	O
invariant	O
distributions	O
which	O
one	O
ptx	O
would	O
tend	O
to	O
as	O
t	O
would	O
depend	O
on	O
the	O
initial	O
condition	O
the	O
transition	B
probability	B
matrix	B
of	O
such	O
a	O
chain	O
has	O
more	O
than	O
one	O
eigenvalue	B
equal	O
to	O
the	O
chain	O
might	O
have	O
a	O
periodic	O
set	B
which	O
means	O
that	O
for	O
some	O
initial	O
conditions	O
ptx	O
doesn	O
t	O
tend	O
to	O
an	O
invariant	B
distribution	B
but	O
instead	O
tends	O
to	O
a	O
periodic	O
limit-cycle	O
a	O
simple	O
markov	B
chain	I
with	O
this	O
property	O
is	O
the	O
random	B
walk	I
on	O
the	O
n	O
hypercube	O
the	O
chain	O
t	O
takes	O
the	O
state	O
from	O
one	O
corner	O
to	O
a	O
randomly	O
chosen	O
adjacent	O
corner	O
the	O
unique	O
invariant	B
distribution	B
of	O
this	O
chain	O
is	O
the	O
uniform	O
distribution	B
over	O
all	O
states	O
but	O
the	O
chain	O
is	O
not	O
ergodic	B
it	O
is	O
periodic	O
with	O
period	O
two	O
if	O
we	O
divide	O
the	O
states	O
into	O
states	O
with	O
odd	O
parity	B
and	O
states	O
with	O
even	O
parity	B
we	O
notice	O
that	O
every	O
odd	O
state	O
is	O
surrounded	O
by	O
even	O
states	O
and	O
vice	O
versa	O
so	O
if	O
the	O
initial	O
condition	O
at	O
time	O
t	O
is	O
a	O
state	O
with	O
even	O
parity	B
then	O
at	O
time	O
t	O
and	O
at	O
all	O
odd	O
times	O
the	O
state	O
must	O
have	O
odd	O
parity	B
and	O
at	O
all	O
even	O
times	O
the	O
state	O
will	O
be	O
of	O
even	O
parity	B
the	O
transition	B
probability	B
matrix	B
of	O
such	O
a	O
chain	O
has	O
more	O
than	O
one	O
eigenvalue	B
with	O
magnitude	O
equal	O
to	O
the	O
random	B
walk	I
on	O
the	O
hypercube	O
for	O
example	O
has	O
eigenvalues	O
equal	O
to	O
and	O
methods	O
of	O
construction	B
of	O
markov	O
chains	O
it	O
is	O
often	O
convenient	O
to	O
construct	O
t	O
by	O
mixing	O
or	O
concatenating	O
simple	O
base	B
transitions	I
b	O
all	O
of	O
which	O
satisfy	O
p	O
dn	O
x	O
xp	O
for	O
the	O
desired	O
density	B
p	O
i	O
e	O
they	O
all	O
have	O
the	O
desired	O
density	B
as	O
an	O
invariant	B
distribution	B
these	O
base	B
transitions	I
need	O
not	O
individually	O
be	O
ergodic	B
t	O
is	O
a	O
mixture	O
of	O
several	O
base	B
transitions	I
x	O
if	O
we	O
make	O
the	O
transition	B
by	O
picking	O
one	O
of	O
the	O
base	B
transitions	I
at	O
random	B
and	O
allowing	O
it	O
to	O
determine	O
the	O
transition	B
i	O
e	O
x	O
t	O
x	O
where	O
fpbg	O
is	O
a	O
probability	B
distribution	B
over	O
the	O
base	B
transitions	I
t	O
is	O
a	O
concatenation	B
of	O
two	O
base	B
transitions	I
x	O
and	O
x	O
if	O
we	O
make	O
a	O
transition	B
to	O
an	O
intermediate	O
state	O
using	O
and	O
then	O
make	O
a	O
transition	B
from	O
state	O
to	O
using	O
t	O
x	O
dn	O
x	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
detailed	B
balance	B
many	O
useful	O
transition	B
probabilities	O
satisfy	O
the	O
detailed	B
balance	B
property	O
t	O
xbp	O
t	O
xap	O
for	O
all	O
xb	O
and	O
xa	O
this	O
equation	O
says	O
that	O
if	O
we	O
pick	O
magic	O
a	O
state	O
from	O
the	O
target	O
density	B
p	O
and	O
make	O
a	O
transition	B
under	O
t	O
to	O
another	O
state	O
it	O
is	O
just	O
as	O
likely	O
that	O
we	O
will	O
pick	O
xb	O
and	O
go	O
from	O
xb	O
to	O
xa	O
as	O
it	O
is	O
that	O
we	O
will	O
pick	O
xa	O
and	O
go	O
from	O
xa	O
to	O
xb	O
markov	O
chains	O
that	O
satisfy	O
detailed	B
balance	B
are	O
also	O
called	O
reversible	B
markov	O
chains	O
the	O
reason	O
why	O
the	O
detailed-balance	O
property	O
is	O
of	O
interest	O
is	O
that	O
detailed	B
balance	B
implies	O
invariance	B
of	O
the	O
distribution	B
p	O
under	O
the	O
markov	B
chain	I
t	O
which	O
is	O
a	O
necessary	O
condition	O
for	O
the	O
key	O
property	O
that	O
we	O
want	O
from	O
our	O
mcmc	O
simulation	O
that	O
the	O
probability	B
distribution	B
of	O
the	O
chain	O
should	O
converge	O
to	O
p	O
exercise	O
prove	O
that	O
detailed	B
balance	B
implies	O
invariance	B
of	O
the	O
distri	O
bution	O
p	O
under	O
the	O
markov	B
chain	I
t	O
proving	O
that	O
detailed	B
balance	B
holds	O
is	O
often	O
a	O
key	O
step	O
when	O
proving	O
that	O
a	O
markov	B
chain	I
monte	B
carlo	I
simulation	O
will	O
converge	O
to	O
the	O
desired	O
distribution	B
the	O
metropolis	B
method	I
detailed	B
balance	B
for	O
example	O
detailed	B
balance	B
is	O
not	O
an	O
essential	O
condition	O
however	O
and	O
we	O
will	O
see	O
later	O
that	O
irreversible	O
markov	O
chains	O
can	O
be	O
useful	O
in	O
practice	O
because	O
they	O
may	O
have	O
random	B
walk	I
properties	O
exercise	O
show	O
that	O
if	O
we	O
concatenate	O
two	O
base	B
transitions	I
and	O
that	O
satisfy	O
detailed	B
balance	B
it	O
is	O
not	O
necessarily	O
the	O
case	O
that	O
the	O
t	O
thus	O
detailed	B
balance	B
exercise	O
does	O
gibbs	B
sampling	I
with	O
several	O
variables	O
all	O
updated	O
in	O
a	O
deterministic	B
sequence	B
satisfy	O
detailed	B
balance	B
slice	B
sampling	I
slice	B
sampling	I
neal	B
is	O
a	O
markov	B
chain	I
monte	B
carlo	I
method	O
that	O
has	O
similarities	O
to	O
rejection	B
sampling	I
gibbs	B
sampling	I
and	O
the	O
metropolis	B
method	I
it	O
can	O
be	O
applied	O
wherever	O
the	O
metropolis	B
method	I
can	O
be	O
applied	O
that	O
is	O
to	O
any	O
system	O
for	O
which	O
the	O
target	O
density	B
p	O
can	O
be	O
evaluated	O
at	O
any	O
point	O
x	O
it	O
has	O
the	O
advantage	O
over	O
simple	O
metropolis	O
methods	O
that	O
it	O
is	O
more	O
robust	O
to	O
the	O
choice	O
of	O
parameters	B
like	O
step	O
sizes	O
the	O
simplest	O
version	O
of	O
slice	B
sampling	I
is	O
similar	O
to	O
gibbs	B
sampling	I
in	O
that	O
it	O
consists	O
of	O
one-dimensional	O
transitions	O
in	O
the	O
state	O
space	O
however	O
there	O
is	O
no	O
requirement	O
that	O
the	O
one-dimensional	O
conditional	B
distributions	O
be	O
easy	O
to	O
sample	B
from	I
nor	O
that	O
they	O
have	O
any	O
convexity	B
properties	O
such	O
as	O
are	O
required	O
for	O
adaptive	B
rejection	B
sampling	I
and	O
slice	B
sampling	I
is	O
similar	O
to	O
rejection	B
sampling	I
in	O
that	O
it	O
is	O
a	O
method	O
that	O
asymptotically	O
draws	O
samples	O
from	O
the	O
volume	B
under	O
the	O
curve	O
described	O
by	O
p	O
but	O
there	O
is	O
no	O
requirement	O
for	O
an	O
upper-bounding	O
function	O
i	O
will	O
describe	O
slice	B
sampling	I
by	O
giving	O
a	O
sketch	O
of	O
a	O
one-dimensional	O
sampling	O
algorithm	O
then	O
giving	O
a	O
pictorial	O
description	O
that	O
includes	O
the	O
details	O
that	O
make	O
the	O
method	O
valid	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
slice	B
sampling	I
the	O
skeleton	O
of	O
slice	B
sampling	I
let	O
us	O
assume	O
that	O
we	O
want	O
to	O
draw	O
samples	O
from	O
p	O
p	O
where	O
x	O
is	O
a	O
real	O
number	O
a	O
one-dimensional	O
slice	B
sampling	I
algorithm	O
is	O
a	O
method	O
for	O
making	O
transitions	O
from	O
a	O
two-dimensional	B
point	O
u	O
lying	O
under	O
the	O
curve	O
p	O
to	O
another	O
point	O
lying	O
under	O
the	O
same	O
curve	O
such	O
that	O
the	O
probability	B
distribution	B
of	O
u	O
tends	O
to	O
a	O
uniform	O
distribution	B
over	O
the	O
area	O
under	O
the	O
curve	O
p	O
whatever	O
initial	O
point	O
we	O
start	O
from	O
like	O
the	O
uniform	O
distribution	B
under	O
the	O
curve	O
p	O
produced	O
by	O
rejection	B
sampling	I
a	O
single	O
transition	B
u	O
of	O
a	O
one-dimensional	O
slice	B
sampling	I
algorithm	O
has	O
the	O
following	O
steps	O
of	O
which	O
steps	O
and	O
will	O
require	O
further	O
elaboration	O
evaluate	O
p	O
draw	O
a	O
vertical	O
coordinate	O
p	O
create	O
a	O
horizontal	O
interval	O
xr	O
enclosing	O
x	O
loop	O
f	O
g	O
draw	O
uniformxl	O
xr	O
evaluate	O
p	O
if	O
p	O
break	O
out	O
of	O
loop	O
else	O
modify	O
the	O
interval	O
xr	O
there	O
are	O
several	O
methods	O
for	O
creating	O
the	O
interval	O
xr	O
in	O
step	O
and	O
several	O
methods	O
for	O
modifying	O
it	O
at	O
step	O
the	O
important	O
point	O
is	O
that	O
the	O
overall	O
method	O
must	O
satisfy	O
detailed	B
balance	B
so	O
that	O
the	O
uniform	O
distribution	B
for	O
u	O
under	O
the	O
curve	O
p	O
is	O
invariant	O
the	O
stepping	O
out	O
method	O
for	O
step	O
in	O
the	O
stepping	O
out	O
method	O
for	O
creating	O
an	O
interval	O
xr	O
enclosing	O
x	O
we	O
step	O
out	O
in	O
steps	O
of	O
length	O
w	O
until	O
we	O
endpoints	O
xl	O
and	O
xr	O
at	O
which	O
p	O
is	O
smaller	O
than	O
u	O
the	O
algorithm	O
is	O
shown	O
in	O
draw	O
r	O
xl	O
x	O
rw	O
xr	O
x	O
rw	O
while	O
f	O
xl	O
xl	O
w	O
g	O
while	O
f	O
xr	O
xr	O
w	O
g	O
the	O
shrinking	O
method	O
for	O
step	O
whenever	O
a	O
point	O
is	O
drawn	O
such	O
that	O
lies	O
above	O
the	O
curve	O
p	O
we	O
shrink	O
the	O
interval	O
so	O
that	O
one	O
of	O
the	O
end	O
points	O
is	O
and	O
such	O
that	O
the	O
original	O
point	O
x	O
is	O
still	O
enclosed	O
in	O
the	O
interval	O
if	O
x	O
f	O
xr	O
g	O
else	O
f	O
xl	O
g	O
properties	O
of	O
slice	B
sampling	I
like	O
a	O
standard	O
metropolis	B
method	I
slice	B
sampling	I
gets	O
around	O
by	O
a	O
random	B
walk	I
but	O
whereas	O
in	O
the	O
metropolis	B
method	I
the	O
choice	O
of	O
the	O
step	O
size	O
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
figure	O
slice	B
sampling	I
each	O
panel	O
is	O
labelled	O
by	O
the	O
steps	O
of	O
the	O
algorithm	O
that	O
are	O
executed	O
in	O
it	O
at	O
step	O
p	O
is	O
evaluated	O
at	O
the	O
current	O
point	O
x	O
at	O
step	O
a	O
vertical	O
coordinate	O
is	O
selected	O
giving	O
the	O
point	O
shown	O
by	O
the	O
box	B
at	O
steps	O
an	O
interval	O
of	O
size	O
w	O
containing	O
is	O
created	O
at	O
random	B
at	O
step	O
p	O
is	O
evaluated	O
at	O
the	O
left	O
end	O
of	O
the	O
interval	O
and	O
is	O
found	O
to	O
be	O
larger	O
than	O
so	O
a	O
step	O
to	O
the	O
left	O
of	O
size	O
w	O
is	O
made	O
at	O
step	O
p	O
is	O
evaluated	O
at	O
the	O
right	O
end	O
of	O
the	O
interval	O
and	O
is	O
found	O
to	O
be	O
smaller	O
than	O
so	O
no	O
stepping	O
out	O
to	O
the	O
right	O
is	O
needed	O
when	O
step	O
is	O
repeated	O
p	O
is	O
found	O
to	O
be	O
smaller	O
than	O
so	O
the	O
stepping	O
out	O
halts	O
at	O
step	O
a	O
point	O
is	O
drawn	O
from	O
the	O
interval	O
shown	O
by	O
a	O
step	O
establishes	O
that	O
this	O
point	O
is	O
above	O
p	O
and	O
step	O
shrinks	O
the	O
interval	O
to	O
the	O
rejected	O
point	O
in	O
such	O
a	O
way	O
that	O
the	O
original	O
point	O
x	O
is	O
still	O
in	O
the	O
interval	O
when	O
step	O
is	O
repeated	O
the	O
new	O
coordinate	O
is	O
to	O
the	O
right-hand	O
side	O
of	O
the	O
interval	O
gives	O
a	O
value	O
of	O
p	O
greater	O
than	O
so	O
this	O
point	O
is	O
the	O
outcome	O
at	O
step	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
slice	B
sampling	I
figure	O
p	O
critical	O
to	O
the	O
rate	B
of	O
progress	O
in	O
slice	B
sampling	I
the	O
step	O
size	O
is	O
self-tuning	O
if	O
the	O
initial	O
interval	O
size	O
w	O
is	O
too	O
small	O
by	O
a	O
factor	O
f	O
compared	O
with	O
the	O
width	O
of	O
the	O
probable	O
region	O
then	O
the	O
stepping-out	O
procedure	O
expands	O
the	O
interval	O
size	O
the	O
cost	O
of	O
this	O
stepping-out	O
is	O
only	O
linear	B
in	O
f	O
whereas	O
in	O
the	O
metropolis	B
method	I
the	O
computer-time	O
scales	O
as	O
the	O
square	B
of	O
f	O
if	O
the	O
step	O
size	O
is	O
too	O
small	O
if	O
the	O
chosen	O
value	O
of	O
w	O
is	O
too	O
large	O
by	O
a	O
factor	O
f	O
then	O
the	O
algorithm	O
spends	O
a	O
time	O
proportional	O
to	O
the	O
logarithm	O
of	O
f	O
shrinking	O
the	O
interval	O
down	O
to	O
the	O
right	O
size	O
since	O
the	O
interval	O
typically	O
shrinks	O
by	O
a	O
factor	O
in	O
the	O
ballpark	O
of	O
each	O
time	O
a	O
point	O
is	O
rejected	O
in	O
contrast	O
the	O
metropolis	O
algorithm	O
responds	O
to	O
a	O
too-large	O
step	O
size	O
by	O
rejecting	O
almost	O
all	O
proposals	O
so	O
the	O
rate	B
of	O
progress	O
is	O
exponentially	O
bad	B
in	O
f	O
there	O
are	O
no	O
rejections	O
in	O
slice	B
sampling	I
the	O
probability	B
of	O
staying	O
in	O
exactly	O
the	O
same	O
place	O
is	O
very	O
small	O
exercise	O
investigate	O
the	O
properties	O
of	O
slice	B
sampling	I
applied	O
to	O
the	O
density	B
shown	O
in	O
x	O
is	O
a	O
real	O
variable	O
between	O
and	O
how	O
long	O
does	O
it	O
take	O
typically	O
for	O
slice	B
sampling	I
to	O
get	O
from	O
an	O
x	O
in	O
the	O
peak	O
region	O
x	O
to	O
an	O
x	O
in	O
the	O
tail	B
region	O
x	O
and	O
vice	O
versa	O
that	O
the	O
probabilities	O
of	O
these	O
transitions	O
do	O
yield	O
an	O
asymptotic	O
probability	B
density	B
that	O
is	O
correct	O
how	O
slice	B
sampling	I
is	O
used	O
in	O
real	O
problems	O
an	O
n	O
density	B
p	O
p	O
may	O
be	O
sampled	O
with	O
the	O
help	O
of	O
the	O
one-dimensional	O
slice	B
sampling	I
method	O
presented	O
above	O
by	O
picking	O
a	O
sequence	B
of	O
directions	O
and	O
x	O
xt	O
xyt	O
the	O
function	O
p	O
above	O
is	O
replaced	O
by	O
p	O
p	O
xyt	O
the	O
directions	O
may	O
be	O
chosen	O
in	O
various	O
ways	O
for	O
example	O
as	O
in	O
gibbs	B
sampling	I
the	O
directions	O
could	O
be	O
the	O
coordinate	O
axes	O
alternatively	O
the	O
directions	O
yt	O
may	O
be	O
selected	O
at	O
random	B
in	O
any	O
manner	O
such	O
that	O
the	O
overall	O
procedure	O
detailed	B
balance	B
computer-friendly	O
slice	B
sampling	I
the	O
real	O
variables	O
of	O
a	O
probabilistic	B
model	B
will	O
always	O
be	O
represented	O
in	O
a	O
computer	B
using	O
a	O
number	O
of	O
bits	O
in	O
the	O
following	O
implementation	O
of	O
slice	B
sampling	I
due	O
to	O
skilling	B
the	O
stepping-out	O
randomization	O
and	O
shrinking	O
operations	O
described	O
above	O
in	O
terms	O
of	O
operations	O
are	O
replaced	O
by	O
binary	O
and	O
integer	O
operations	O
we	O
assume	O
that	O
the	O
variable	O
x	O
that	O
is	O
being	O
slice-sampled	O
is	O
represented	O
by	O
a	O
b-bit	O
integer	O
x	O
taking	O
on	O
one	O
of	O
b	O
values	O
many	O
or	O
all	O
of	O
which	O
correspond	O
to	O
valid	O
values	O
of	O
x	O
using	O
an	O
integer	O
grid	O
eliminates	O
any	O
errors	B
in	O
detailed	B
balance	B
that	O
might	O
ensue	O
from	O
variable-precision	O
rounding	O
of	O
numbers	O
the	O
mapping	B
from	O
x	O
to	O
x	O
need	O
not	O
be	O
linear	B
if	O
it	O
is	O
nonlinear	B
we	O
assume	O
that	O
the	O
function	O
p	O
is	O
replaced	O
by	O
an	O
appropriately	O
transformed	O
function	O
for	O
example	O
p	O
p	O
we	O
assume	O
the	O
following	O
operators	O
on	O
b-bit	O
integers	O
are	O
available	O
x	O
n	O
x	O
n	O
x	O
n	O
n	O
randbitsl	O
arithmetic	O
sum	O
modulo	O
b	O
of	O
x	O
and	O
n	O
modulo	O
b	O
of	O
x	O
and	O
n	O
bitwise	B
exclusive-or	O
of	O
x	O
and	O
n	O
sets	O
n	O
to	O
a	O
random	B
l-bit	O
integer	O
a	O
slice-sampling	O
procedure	O
for	B
integers	I
is	O
then	O
as	O
follows	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
given	O
a	O
current	O
point	O
x	O
and	O
a	O
height	O
y	O
p	O
p	O
u	O
randbitsb	O
set	B
l	O
to	O
a	O
value	O
l	O
b	O
do	O
f	O
n	O
randbitsl	O
u	O
n	O
u	O
l	O
l	O
g	O
until	O
x	O
or	O
y	O
a	O
random	B
translation	O
u	O
of	O
the	O
binary	O
coordinate	O
system	O
set	B
initial	O
l-bit	O
sampling	O
range	O
a	O
random	B
move	O
within	O
the	O
current	O
interval	O
of	O
width	O
randomize	O
the	O
lowest	O
l	O
bits	O
of	O
x	O
the	O
translated	O
coordinate	O
system	O
if	O
is	O
not	O
acceptable	O
decrease	O
l	O
and	O
try	O
again	O
with	O
a	O
smaller	O
perturbation	O
of	O
x	O
termination	B
at	O
or	O
before	O
l	O
is	O
assured	O
x	O
b	O
figure	O
the	O
sequence	B
of	O
intervals	B
from	O
which	O
the	O
new	O
candidate	O
points	O
are	O
drawn	O
the	O
translation	O
u	O
is	O
introduced	O
to	O
avoid	O
permanent	O
sharp	O
edges	O
where	O
for	O
example	O
the	O
adjacent	O
binary	O
integers	O
and	O
would	O
otherwise	O
be	O
permanently	O
in	O
sectors	O
making	O
it	O
for	O
x	O
to	O
move	O
from	O
one	O
to	O
the	O
other	O
the	O
sequence	B
of	O
intervals	B
from	O
which	O
the	O
new	O
candidate	O
points	O
are	O
drawn	O
is	O
illustrated	O
in	O
first	O
a	O
point	O
is	O
drawn	O
from	O
the	O
entire	O
interval	O
shown	O
by	O
the	O
top	O
horizontal	O
line	O
at	O
each	O
subsequent	O
draw	O
the	O
interval	O
is	O
halved	O
in	O
such	O
a	O
way	O
as	O
to	O
contain	O
the	O
previous	O
point	O
x	O
if	O
preliminary	O
stepping-out	O
from	O
the	O
initial	O
range	O
is	O
required	O
step	O
above	O
can	O
be	O
replaced	O
by	O
the	O
following	O
similar	O
procedure	O
l	O
sets	O
the	O
initial	O
width	O
set	B
l	O
to	O
a	O
value	O
l	O
b	O
do	O
f	O
n	O
randbitsl	O
u	O
n	O
u	O
l	O
l	O
g	O
until	O
b	O
or	O
y	O
these	O
shrinking	O
and	O
stepping	O
out	O
methods	O
shrink	O
and	O
expand	O
by	O
a	O
factor	O
of	O
two	O
per	O
evaluation	O
a	O
variant	O
is	O
to	O
shrink	O
or	O
expand	O
by	O
more	O
than	O
one	O
bit	B
each	O
time	O
setting	O
l	O
l	O
with	O
taking	O
at	O
each	O
step	O
from	O
any	O
pre-assigned	O
distribution	B
may	O
include	O
allows	O
extra	O
exercise	O
in	O
the	O
shrinking	O
phase	O
after	O
an	O
unacceptable	O
has	O
been	O
produced	O
the	O
choice	O
of	O
is	O
allowed	O
to	O
depend	O
on	O
the	O
between	O
the	O
slice	O
s	O
height	O
y	O
and	O
the	O
value	O
of	O
p	O
without	O
spoiling	O
the	O
algorithm	O
s	O
validity	O
this	O
it	O
might	O
be	O
a	O
good	B
idea	O
to	O
choose	O
a	O
larger	O
value	O
of	O
when	O
y	O
p	O
is	O
large	O
investigate	O
this	O
idea	O
theoretically	O
or	O
empirically	O
a	O
feature	O
of	O
using	O
the	O
integer	O
representation	O
is	O
that	O
with	O
a	O
suitably	O
extended	B
number	O
of	O
bits	O
the	O
single	O
integer	O
x	O
can	O
represent	O
two	O
or	O
more	O
real	O
parameters	B
for	O
example	O
by	O
mapping	B
x	O
to	O
through	O
a	O
curve	O
such	O
as	O
a	O
peano	O
curve	O
thus	O
multi-dimensional	B
slice	B
sampling	I
can	O
be	O
performed	O
using	O
the	O
same	O
software	B
as	O
for	O
one	O
dimension	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
practicalities	O
practicalities	O
can	O
we	O
predict	O
how	O
long	O
a	O
markov	B
chain	I
monte	B
carlo	I
simulation	O
will	O
take	O
to	O
equilibrate	O
by	O
considering	O
the	O
random	B
walks	O
involved	O
in	O
a	O
markov	B
chain	I
monte	B
carlo	I
simulation	O
we	O
can	O
obtain	O
simple	O
lower	O
bounds	O
on	O
the	O
time	O
required	O
for	O
convergence	O
but	O
predicting	O
this	O
time	O
more	O
precisely	O
is	O
a	O
problem	O
and	O
most	O
of	O
the	O
theoretical	O
results	O
giving	O
upper	O
bounds	O
on	O
the	O
convergence	O
time	O
are	O
of	O
little	O
practical	B
use	O
the	O
exact	B
sampling	I
methods	O
of	O
chapter	O
a	O
solution	O
to	O
this	O
problem	O
for	O
certain	O
markov	O
chains	O
can	O
we	O
diagnose	O
or	O
detect	O
convergence	O
in	O
a	O
running	O
simulation	O
this	O
is	O
also	O
a	O
problem	O
there	O
are	O
a	O
few	O
practical	B
tools	O
available	O
but	O
none	O
of	O
them	O
is	O
perfect	B
and	O
carlin	O
can	O
we	O
speed	O
up	O
the	O
convergence	O
time	O
and	O
time	O
between	O
independent	O
samples	O
of	O
a	O
markov	B
chain	I
monte	B
carlo	I
method	O
here	O
there	O
is	O
good	B
news	O
as	O
described	O
in	O
the	O
next	O
chapter	O
which	O
describes	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
overrelaxation	B
and	O
simulated	B
annealing	B
further	O
practical	B
issues	O
can	O
the	O
normalizing	O
constant	O
be	O
evaluated	O
if	O
the	O
target	O
density	B
p	O
is	O
given	O
in	O
the	O
form	O
of	O
an	O
unnormalized	O
density	B
p	O
with	O
p	O
z	O
p	O
the	O
value	O
of	O
z	O
may	O
well	O
be	O
of	O
interest	O
monte	B
carlo	I
methods	I
do	O
not	O
readily	O
yield	O
an	O
estimate	O
of	O
this	O
quantity	O
and	O
it	O
is	O
an	O
area	O
of	O
active	O
research	O
to	O
ways	O
of	O
evaluating	O
it	O
techniques	O
for	O
evaluating	O
z	O
include	O
importance	B
sampling	I
by	O
neal	B
and	O
annealed	O
impor	O
tance	O
sampling	O
thermodynamic	B
integration	I
during	O
simulated	B
annealing	B
the	O
acceptance	B
ratio	I
method	I
and	O
umbrella	B
sampling	I
by	O
neal	B
reversible	B
jump	I
markov	B
chain	I
monte	B
carlo	I
one	O
way	O
of	O
dealing	O
with	O
z	O
however	O
may	O
be	O
to	O
a	O
solution	O
to	O
one	O
s	O
task	O
that	O
does	O
not	O
require	O
that	O
z	O
be	O
evaluated	O
in	O
bayesian	B
data	O
modelling	B
one	O
might	O
be	O
able	O
to	O
avoid	O
the	O
need	O
to	O
evaluate	O
z	O
which	O
would	O
be	O
important	O
for	O
model	B
comparison	I
by	O
not	O
having	O
more	O
than	O
one	O
model	B
instead	O
of	O
using	O
several	O
models	O
in	O
complexity	B
for	O
example	O
and	O
evaluating	O
their	O
relative	B
posterior	O
probabilities	O
one	O
can	O
make	O
a	O
single	O
hierarchical	B
model	B
having	O
for	O
example	O
various	O
continuous	B
hyperparameters	O
which	O
play	O
a	O
role	O
similar	O
to	O
that	O
played	O
by	O
the	O
distinct	O
models	O
in	O
noting	O
the	O
possibility	O
of	O
not	O
computing	O
z	O
i	O
am	O
not	O
endorsing	O
this	O
approach	O
the	O
normalizing	O
constant	O
z	O
is	O
often	O
the	O
single	O
most	O
important	O
number	O
in	O
the	O
problem	O
and	O
i	O
think	O
every	O
should	O
be	O
devoted	O
to	O
calculating	O
it	O
the	O
metropolis	B
method	I
for	O
big	O
models	O
our	O
original	O
description	O
of	O
the	O
metropolis	B
method	I
involved	O
a	O
joint	B
updating	O
of	O
all	O
the	O
variables	O
using	O
a	O
proposal	B
density	B
x	O
for	O
big	O
problems	O
it	O
may	O
be	O
more	O
to	O
use	O
several	O
proposal	O
distributions	O
x	O
each	O
of	O
which	O
updates	O
only	O
some	O
of	O
the	O
components	O
of	O
x	O
each	O
proposal	O
is	O
individually	O
accepted	O
or	O
rejected	O
and	O
the	O
proposal	O
distributions	O
are	O
repeatedly	O
run	O
through	O
in	O
sequence	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
exercise	O
explain	O
why	O
the	O
rate	B
of	O
movement	O
through	O
the	O
state	O
space	O
will	O
be	O
greater	O
when	O
b	O
proposals	O
qb	O
are	O
considered	O
individually	O
in	O
sequence	B
compared	O
with	O
the	O
case	O
of	O
a	O
single	O
proposal	O
by	O
the	O
concatenation	B
of	O
qb	O
assume	O
that	O
each	O
proposal	O
distribution	B
x	O
has	O
an	O
acceptance	B
rate	B
f	O
in	O
the	O
metropolis	B
method	I
the	O
proposal	B
density	B
x	O
typically	O
has	O
a	O
number	O
of	O
parameters	B
that	O
control	O
for	O
example	O
its	O
width	O
these	O
parameters	B
are	O
usually	O
set	B
by	O
trial	O
and	O
error	O
with	O
the	O
rule	B
of	I
thumb	I
being	O
to	O
aim	O
for	O
a	O
rejection	B
frequency	B
of	O
about	O
it	O
is	O
not	O
valid	O
to	O
have	O
the	O
width	O
parameters	B
be	O
dynamically	O
updated	O
during	O
the	O
simulation	O
in	O
a	O
way	O
that	O
depends	O
on	O
the	O
history	O
of	O
the	O
simulation	O
such	O
a	O
of	O
the	O
proposal	B
density	B
would	O
violate	O
the	O
detailed-balance	O
condition	O
that	O
guarantees	O
that	O
the	O
markov	B
chain	I
has	O
the	O
correct	O
invariant	B
distribution	B
gibbs	B
sampling	I
in	O
big	O
models	O
our	O
description	O
of	O
gibbs	B
sampling	I
involved	O
sampling	O
one	O
parameter	O
at	O
a	O
time	O
as	O
described	O
in	O
equations	O
for	O
big	O
problems	O
it	O
may	O
be	O
more	O
to	O
sample	B
groups	O
of	O
variables	O
jointly	O
that	O
is	O
to	O
use	O
several	O
proposal	O
distributions	O
a	O
b	O
p	O
xa	O
j	O
xt	O
p	O
xb	O
j	O
xt	O
k	O
a	O
xt	O
xt	O
k	O
etc	O
how	O
many	O
samples	O
are	O
needed	O
at	O
the	O
start	O
of	O
this	O
chapter	O
we	O
observed	O
that	O
the	O
variance	B
of	O
an	O
estimator	B
depends	O
only	O
on	O
the	O
number	O
of	O
independent	O
samples	O
r	O
and	O
the	O
value	O
of	O
dn	O
x	O
p	O
we	O
have	O
now	O
discussed	O
a	O
variety	O
of	O
methods	O
for	O
generating	O
samples	O
from	O
p	O
how	O
many	O
independent	O
samples	O
r	O
should	O
we	O
aim	O
for	O
in	O
many	O
problems	O
we	O
really	O
only	O
need	O
about	O
twelve	O
independent	O
samples	O
from	O
p	O
imagine	O
that	O
x	O
is	O
an	O
unknown	O
vector	O
such	O
as	O
the	O
amount	O
of	O
corrosion	O
present	O
in	O
each	O
of	O
underground	O
pipelines	O
around	O
cambridge	O
and	O
is	O
the	O
total	O
cost	O
of	O
repairing	O
those	O
pipelines	O
the	O
distribution	B
p	O
describes	O
the	O
probability	B
of	O
a	O
state	O
x	O
given	O
the	O
tests	O
that	O
have	O
been	O
carried	O
out	O
on	O
some	O
pipelines	O
and	O
the	O
assumptions	B
about	O
the	O
physics	B
of	O
corrosion	O
the	O
quantity	O
is	O
the	O
expected	O
cost	O
of	O
the	O
repairs	O
the	O
quantity	O
is	O
the	O
variance	B
of	O
the	O
cost	O
measures	O
by	O
how	O
much	O
we	O
should	O
expect	O
the	O
actual	O
cost	O
to	O
from	O
the	O
expectation	B
now	O
how	O
accurately	O
would	O
a	O
manager	O
like	O
to	O
know	O
i	O
would	O
suggest	O
there	O
is	O
little	O
point	O
in	O
knowing	O
to	O
a	O
precision	B
than	O
about	O
after	O
all	O
the	O
true	O
cost	O
is	O
likely	O
to	O
by	O
from	O
if	O
we	O
obtain	O
r	O
independent	O
samples	O
from	O
p	O
we	O
can	O
estimate	O
to	O
a	O
precision	B
of	O
which	O
is	O
smaller	O
than	O
so	O
twelve	O
samples	O
allocation	O
of	O
resources	O
assuming	O
we	O
have	O
decided	O
how	O
many	O
independent	O
samples	O
r	O
are	O
required	O
an	O
important	O
question	O
is	O
how	O
one	O
should	O
make	O
use	O
of	O
one	O
s	O
limited	O
computer	B
resources	O
to	O
obtain	O
these	O
samples	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
figure	O
three	O
possible	O
markov	B
chain	I
monte	B
carlo	I
strategies	O
for	O
obtaining	O
twelve	O
samples	O
in	O
a	O
amount	O
of	O
computer	B
time	O
time	O
is	O
represented	O
by	O
horizontal	O
lines	O
samples	O
by	O
white	B
circles	O
a	O
single	O
run	O
consisting	O
of	O
one	O
long	O
burn	O
in	O
period	O
followed	O
by	O
a	O
sampling	O
period	O
four	O
medium-length	O
runs	O
with	O
initial	O
conditions	O
and	O
a	O
medium-length	O
burn	O
in	O
period	O
twelve	O
short	O
runs	O
summary	B
a	O
typical	B
markov	B
chain	I
monte	B
carlo	I
experiment	O
involves	O
an	O
initial	O
period	O
in	O
which	O
control	O
parameters	B
of	O
the	O
simulation	O
such	O
as	O
step	O
sizes	O
may	O
be	O
adjusted	O
this	O
is	O
followed	O
by	O
a	O
burn	O
in	O
period	O
during	O
which	O
we	O
hope	O
the	O
simulation	O
converges	O
to	O
the	O
desired	O
distribution	B
finally	O
as	O
the	O
simulation	O
continues	O
we	O
record	O
the	O
state	O
vector	O
occasionally	O
so	O
as	O
to	O
create	O
a	O
list	O
of	O
states	O
fxrgr	O
that	O
we	O
hope	O
are	O
roughly	O
independent	O
samples	O
from	O
p	O
there	O
are	O
several	O
possible	O
strategies	O
make	O
one	O
long	O
run	O
obtaining	O
all	O
r	O
samples	O
from	O
it	O
make	O
a	O
few	O
medium-length	O
runs	O
with	O
initial	O
conditions	O
obtain	O
ing	O
some	O
samples	O
from	O
each	O
make	O
r	O
short	O
runs	O
each	O
starting	O
from	O
a	O
random	B
initial	O
condition	O
with	O
the	O
only	O
state	O
that	O
is	O
recorded	O
being	O
the	O
state	O
of	O
each	O
simulation	O
the	O
strategy	O
has	O
the	O
best	O
chance	O
of	O
attaining	O
convergence	O
the	O
last	O
strategy	O
may	O
have	O
the	O
advantage	O
that	O
the	O
correlations	B
between	O
the	O
recorded	O
samples	O
are	O
smaller	O
the	O
middle	O
path	O
is	O
popular	O
with	O
markov	B
chain	I
monte	B
carlo	I
experts	O
et	O
al	O
because	O
it	O
avoids	O
the	O
of	O
discarding	O
burn-in	O
iterations	O
in	O
many	O
runs	O
while	O
still	O
allowing	O
one	O
to	O
detect	O
problems	O
with	O
lack	O
of	O
convergence	O
that	O
would	O
not	O
be	O
apparent	O
from	O
a	O
single	O
run	O
finally	O
i	O
should	O
emphasize	O
that	O
there	O
is	O
no	O
need	O
to	O
make	O
the	O
points	O
in	O
the	O
estimate	O
nearly-independent	O
averaging	O
over	O
dependent	O
points	O
is	O
it	O
won	O
t	O
lead	O
to	O
any	O
bias	B
in	O
the	O
estimates	O
for	O
example	O
when	O
you	O
use	O
strategy	O
or	O
you	O
may	O
if	O
you	O
wish	O
include	O
all	O
the	O
points	O
between	O
the	O
and	O
last	O
sample	B
in	O
each	O
run	O
of	O
course	O
estimating	O
the	O
accuracy	O
of	O
the	O
estimate	O
is	O
harder	O
when	O
the	O
points	O
are	O
dependent	O
summary	B
monte	B
carlo	I
methods	I
are	O
a	O
powerful	O
tool	O
that	O
allow	O
one	O
to	O
sample	B
from	I
any	O
probability	B
distribution	B
that	O
can	O
be	O
expressed	O
in	O
the	O
form	O
p	O
z	O
p	O
monte	B
carlo	I
methods	I
can	O
answer	O
virtually	O
any	O
query	O
related	O
to	O
p	O
by	O
putting	O
the	O
query	O
in	O
the	O
form	O
z	O
rxr	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
in	O
high-dimensional	O
problems	O
the	O
only	O
satisfactory	O
methods	O
are	O
those	O
based	O
on	O
markov	O
chains	O
such	O
as	O
the	O
metropolis	B
method	I
gibbs	B
sampling	I
and	O
slice	B
sampling	I
gibbs	B
sampling	I
is	O
an	O
attractive	O
method	O
because	O
it	O
has	O
no	O
adjustable	O
parameters	B
but	O
its	O
use	O
is	O
restricted	O
to	O
cases	O
where	O
samples	O
can	O
be	O
generated	O
from	O
the	O
conditional	B
distributions	O
slice	B
sampling	I
is	O
attractive	O
because	O
whilst	O
it	O
has	O
step-length	O
parameters	B
its	O
performance	O
is	O
not	O
very	O
sensitive	O
to	O
their	O
values	O
simple	O
metropolis	O
algorithms	B
and	O
gibbs	B
sampling	I
algorithms	B
although	O
widely	O
used	O
perform	O
poorly	O
because	O
they	O
explore	B
the	O
space	O
by	O
a	O
slow	O
random	B
walk	I
the	O
next	O
chapter	O
will	O
discuss	O
methods	O
for	O
speeding	O
up	O
markov	B
chain	I
monte	B
carlo	I
simulations	O
slice	B
sampling	I
does	O
not	O
avoid	O
random	B
walk	I
behaviour	O
but	O
it	O
automatically	O
chooses	O
the	O
largest	O
appropriate	O
step	O
size	O
thus	O
reducing	O
the	O
bad	B
of	O
the	O
random	B
walk	I
compared	O
with	O
say	O
a	O
metropolis	B
method	I
with	O
a	O
tiny	O
step	O
size	O
exercises	O
exercise	O
a	O
study	O
of	O
importance	B
sampling	I
we	O
already	O
established	O
in	O
section	B
that	O
importance	B
sampling	I
is	O
likely	O
to	O
be	O
useless	O
in	O
high-dimensional	O
problems	O
this	O
exercise	O
explores	O
a	O
further	O
cautionary	O
tale	O
showing	O
that	O
importance	B
sampling	I
can	O
fail	O
even	O
in	O
one	O
dimension	O
even	O
with	O
friendly	O
gaussian	B
distributions	O
imagine	O
that	O
we	O
want	O
to	O
know	O
the	O
expectation	B
of	O
a	O
function	O
under	O
a	O
distribution	B
p	O
dx	O
p	O
and	O
that	O
this	O
expectation	B
is	O
estimated	O
by	O
importance	B
sampling	I
with	O
a	O
distribution	B
qx	O
alternatively	O
perhaps	O
we	O
wish	O
to	O
estimate	O
the	O
normalizing	O
constant	O
z	O
in	O
p	O
p	O
using	O
z	O
dx	O
p	O
dx	O
qx	O
p	O
qx	O
p	O
now	O
let	O
p	O
and	O
qx	O
be	O
gaussian	B
distributions	O
with	O
mean	B
zero	O
and	O
standard	O
deviations	O
and	O
each	O
point	O
x	O
drawn	O
from	O
q	O
will	O
have	O
an	O
associated	O
weight	O
p	O
what	O
is	O
the	O
variance	B
of	O
the	O
weights	O
that	O
p	O
p	O
so	O
p	O
is	O
actually	O
normalized	O
and	O
z	O
though	O
we	O
can	O
pretend	O
that	O
we	O
didn	O
t	O
know	O
that	O
what	O
happens	O
to	O
the	O
variance	B
of	O
the	O
weights	O
as	O
check	O
your	O
theory	O
by	O
simulating	O
this	O
importance-sampling	O
problem	O
on	O
a	O
computer	B
q	O
exercise	O
consider	O
the	O
metropolis	O
algorithm	O
for	O
the	O
one-dimensional	O
toy	O
problem	O
of	O
section	B
sampling	O
from	O
whenever	O
the	O
current	O
state	O
is	O
one	O
of	O
the	O
end	O
states	O
the	O
proposal	B
density	B
given	O
in	O
equation	O
will	O
propose	O
with	O
probability	B
a	O
state	O
that	O
will	O
be	O
rejected	O
to	O
reduce	O
this	O
waste	O
fred	O
the	O
software	B
responsible	O
for	O
generating	O
samples	O
from	O
q	O
so	O
that	O
when	O
x	O
the	O
proposal	B
density	B
is	O
on	O
and	O
similarly	O
when	O
x	O
is	O
always	O
proposed	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
fred	O
sets	O
the	O
software	B
that	O
implements	O
the	O
acceptance	O
rule	O
so	O
that	O
the	O
software	B
accepts	O
all	O
proposed	O
moves	O
what	O
probability	B
p	O
will	O
fred	O
s	O
software	B
generate	O
samples	O
from	O
what	O
is	O
the	O
correct	O
acceptance	O
rule	O
for	O
fred	O
s	O
proposal	B
density	B
in	O
order	O
to	O
obtain	O
samples	O
from	O
p	O
exercise	O
implement	O
gibbs	B
sampling	I
for	O
the	O
inference	B
of	O
a	O
single	O
one-dimensional	O
gaussian	B
which	O
we	O
studied	O
using	O
maximum	B
likelihood	B
in	O
section	B
assign	O
a	O
broad	O
gaussian	B
prior	B
to	O
and	O
a	O
broad	O
gamma	B
prior	B
to	O
the	O
precision	B
parameter	O
each	O
update	O
of	O
will	O
involve	O
a	O
sample	B
from	I
a	O
gaussian	B
distribution	B
and	O
each	O
update	O
of	O
requires	O
a	O
sample	B
from	I
a	O
gamma	B
distribution	B
exercise	O
gibbs	B
sampling	I
for	O
clustering	B
implement	O
gibbs	B
sampling	I
for	O
the	O
inference	B
of	O
a	O
mixture	O
of	O
k	O
one-dimensional	O
gaussians	O
which	O
we	O
studied	O
using	O
maximum	B
likelihood	B
in	O
section	B
allow	O
the	O
clusters	O
to	O
have	O
standard	O
deviations	O
assign	O
priors	O
to	O
the	O
means	O
and	O
standard	O
deviations	O
in	O
the	O
same	O
way	O
as	O
the	O
previous	O
exercise	O
either	O
the	O
prior	B
probabilities	O
of	O
the	O
classes	O
to	O
be	O
equal	O
or	O
put	O
a	O
uniform	O
prior	B
over	O
the	O
parameters	B
and	O
include	O
them	O
in	O
the	O
gibbs	B
sampling	I
notice	O
the	O
similarity	O
of	O
gibbs	B
sampling	I
to	O
the	O
soft	B
k-means	B
clustering	B
algorithm	O
we	O
can	O
alternately	O
assign	O
the	O
class	O
labels	O
fkng	O
given	O
the	O
parameters	B
then	O
update	O
the	O
parameters	B
given	O
the	O
class	O
labels	O
the	O
assignment	O
step	O
involves	O
sampling	O
from	O
the	O
probability	B
distributions	O
by	O
the	O
responsibilities	O
and	O
the	O
update	O
step	O
updates	O
the	O
means	O
and	O
variances	O
using	O
probability	B
distributions	O
centred	O
on	O
the	O
k-means	O
algorithm	O
s	O
values	O
do	O
your	O
experiments	O
that	O
monte	B
carlo	I
methods	I
bypass	O
the	O
of	O
maximum	B
likelihood	B
discussed	O
in	O
section	B
a	O
solution	O
to	O
this	O
exercise	O
and	O
the	O
previous	O
one	O
written	O
in	O
octave	B
is	O
exercise	O
implement	O
gibbs	B
sampling	I
for	O
the	O
seven	O
scientists	B
inference	B
problem	O
which	O
we	O
encountered	O
in	O
exercise	O
and	O
which	O
you	O
may	O
have	O
solved	O
by	O
exact	O
marginalization	B
s	O
not	O
essential	O
to	O
have	O
done	O
the	O
latter	O
exercise	O
a	O
metropolis	B
method	I
is	O
used	O
to	O
explore	B
a	O
distribution	B
p	O
that	O
is	O
actually	O
a	O
spherical	O
gaussian	B
distribution	B
of	O
standard	B
deviation	I
in	O
all	O
dimensions	B
the	O
proposal	B
density	B
q	O
is	O
a	O
spherical	O
gaussian	B
distribution	B
of	O
standard	B
deviation	I
roughly	O
what	O
is	O
the	O
step	O
size	O
if	O
the	O
acceptance	B
rate	B
is	O
assuming	O
this	O
value	O
of	O
roughly	O
how	O
long	O
would	O
the	O
method	O
take	O
to	O
traverse	O
the	O
distribution	B
and	O
generate	O
a	O
sample	B
independent	O
of	O
the	O
initial	O
condition	O
by	O
how	O
much	O
does	O
ln	O
p	O
change	O
in	O
a	O
typical	B
step	O
by	O
how	O
much	O
should	O
ln	O
p	O
vary	O
when	O
x	O
is	O
drawn	O
from	O
p	O
what	O
happens	O
if	O
rather	O
than	O
using	O
a	O
metropolis	B
method	I
that	O
tries	O
to	O
change	O
all	O
components	O
at	O
once	O
one	O
instead	O
uses	O
a	O
concatenation	B
of	O
metropolis	O
updates	O
changing	O
one	O
component	O
at	O
a	O
time	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
exercise	O
when	O
discussing	O
the	O
time	O
taken	O
by	O
the	O
metropolis	O
algorithm	O
to	O
generate	O
independent	O
samples	O
we	O
considered	O
a	O
distribution	B
with	O
longest	O
spatial	O
length	O
scale	O
l	O
being	O
explored	O
using	O
a	O
proposal	O
distribution	B
with	O
step	O
size	O
another	O
dimension	O
that	O
a	O
mcmc	O
method	O
must	O
explore	B
is	O
the	O
range	O
of	O
possible	O
values	O
of	O
the	O
log	O
probability	B
ln	O
p	O
assuming	O
that	O
the	O
state	O
x	O
contains	O
a	O
number	O
of	O
independent	O
random	B
variables	O
proportional	O
to	O
n	O
when	O
samples	O
are	O
drawn	O
from	O
p	O
the	O
asymptotic	B
equipartition	B
principle	O
tell	O
us	O
that	O
the	O
value	O
of	O
ln	O
p	O
is	O
likely	O
to	O
be	O
close	O
to	O
the	O
entropy	B
of	O
x	O
varying	O
either	O
side	O
with	O
a	O
standard	B
deviation	I
that	O
scales	O
as	O
pn	O
consider	O
a	O
metropolis	B
method	I
with	O
a	O
symmetrical	O
proposal	B
density	B
that	O
is	O
one	O
that	O
qx	O
x	O
assuming	O
that	O
accepted	O
jumps	O
either	O
increase	O
ln	O
p	O
by	O
some	O
amount	O
or	O
decrease	O
it	O
by	O
a	O
small	O
amount	O
e	O
g	O
ln	O
e	O
this	O
a	O
reasonable	O
assumption	O
discuss	O
how	O
long	O
it	O
must	O
take	O
to	O
generate	O
roughly	O
independent	O
samples	O
from	O
p	O
discuss	O
whether	O
gibbs	B
sampling	I
has	O
similar	O
properties	O
exercise	O
markov	B
chain	I
monte	B
carlo	I
methods	I
do	O
not	O
compute	O
partition	B
functions	B
z	O
yet	O
they	O
allow	O
ratios	O
of	O
quantities	O
like	O
z	O
to	O
be	O
estimated	O
for	O
example	O
consider	O
a	O
random-walk	B
metropolis	I
algorithm	O
in	O
a	O
state	O
space	O
where	O
the	O
energy	B
is	O
zero	O
in	O
a	O
connected	O
accessible	O
region	O
and	O
large	O
everywhere	O
else	O
and	O
imagine	O
that	O
the	O
accessible	O
space	O
can	O
be	O
chopped	O
into	O
two	O
regions	O
connected	O
by	O
one	O
or	O
more	O
corridor	O
states	O
the	O
fraction	O
of	O
times	O
spent	O
in	O
each	O
region	O
at	O
equilibrium	O
is	O
proportional	O
to	O
the	O
volume	B
of	O
the	O
region	O
how	O
does	O
the	O
monte	B
carlo	I
method	O
manage	O
to	O
do	O
this	O
without	O
measuring	O
the	O
volumes	O
exercise	O
philosophy	B
one	O
curious	O
defect	O
of	O
these	O
monte	B
carlo	I
methods	I
which	O
are	O
widely	O
used	O
by	O
bayesian	B
statisticians	O
is	O
that	O
they	O
are	O
all	O
non-bayesian	O
hagan	O
they	O
involve	O
computer	B
experiments	O
from	O
which	O
estimators	O
of	O
quantities	O
of	O
interest	O
are	O
derived	O
these	O
estimators	O
depend	O
on	O
the	O
proposal	O
distributions	O
that	O
were	O
used	O
to	O
generate	O
the	O
samples	O
and	O
on	O
the	O
random	B
numbers	O
that	O
happened	O
to	O
come	O
out	O
of	O
our	O
random	B
number	I
generator	I
in	O
contrast	O
an	O
alternative	O
bayesian	B
approach	O
to	O
the	O
problem	O
would	O
use	O
the	O
results	O
of	O
our	O
computer	B
experiments	O
to	O
infer	O
the	O
properties	O
of	O
the	O
target	O
function	O
p	O
and	O
generate	O
predictive	O
distributions	O
for	O
quantities	O
of	O
interest	O
such	O
as	O
this	O
approach	O
would	O
give	O
answers	O
that	O
would	O
depend	O
only	O
on	O
the	O
computed	O
values	O
of	O
p	O
at	O
the	O
points	O
fxrg	O
the	O
answers	O
would	O
not	O
depend	O
on	O
how	O
those	O
points	O
were	O
chosen	O
can	O
you	O
make	O
a	O
bayesian	B
monte	B
carlo	I
method	O
rasmussen	O
and	O
ghahramani	O
for	O
a	O
practical	B
attempt	O
solutions	O
solution	O
to	O
exercise	O
we	O
wish	O
to	O
show	O
that	O
pr	O
pr	O
wr	O
converges	O
to	O
the	O
expectation	B
of	O
under	O
p	O
we	O
consider	O
the	O
numerator	O
and	O
the	O
denominator	O
separately	O
first	O
the	O
denominator	O
consider	O
a	O
single	O
importance	O
weight	O
wr	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
what	O
is	O
its	O
expectation	B
averaged	O
under	O
the	O
distribution	B
q	O
of	O
the	O
point	O
xr	O
hwri	O
dx	O
qx	O
p	O
dx	O
zq	O
p	O
zp	O
zq	O
so	O
the	O
expectation	B
of	O
the	O
denominator	O
is	O
wr	O
r	O
zp	O
zq	O
as	O
long	O
as	O
the	O
variance	B
of	O
wr	O
is	O
the	O
denominator	O
divided	O
by	O
r	O
will	O
converge	O
to	O
zp	O
as	O
r	O
increases	O
fact	O
the	O
estimate	O
converges	O
to	O
the	O
right	O
answer	O
even	O
if	O
this	O
variance	B
is	O
as	O
long	O
as	O
the	O
expectation	B
is	O
similarly	O
the	O
expectation	B
of	O
one	O
term	O
in	O
the	O
numerator	O
is	O
dx	O
qx	O
p	O
dx	O
zq	O
p	O
zp	O
zq	O
where	O
is	O
the	O
expectation	B
of	O
under	O
p	O
so	O
the	O
numerator	O
divided	O
by	O
r	O
converges	O
to	O
zp	O
zq	O
with	O
increasing	O
r	O
thus	O
converges	O
to	O
the	O
numerator	O
and	O
the	O
denominator	O
are	O
unbiased	O
estimators	O
of	O
rzp	O
and	O
rzp	O
respectively	O
but	O
their	O
ratio	O
is	O
not	O
necessarily	O
an	O
unbiased	B
estimator	B
for	O
r	O
solution	O
to	O
exercise	O
when	O
the	O
true	O
density	B
p	O
is	O
multimodal	O
it	O
is	O
unwise	O
to	O
use	O
importance	B
sampling	I
with	O
a	O
sampler	B
density	B
to	O
one	O
mode	O
because	O
on	O
the	O
rare	O
occasions	O
that	O
a	O
point	O
is	O
produced	O
that	O
lands	O
in	O
one	O
of	O
the	O
other	O
modes	O
the	O
weight	O
associated	O
with	O
that	O
point	O
will	O
be	O
enormous	O
the	O
estimates	O
will	O
have	O
enormous	O
variance	B
but	O
this	O
enormous	O
variance	B
may	O
not	O
be	O
evident	O
to	O
the	O
user	O
if	O
no	O
points	O
in	O
the	O
other	O
modes	O
have	O
been	O
seen	O
solution	O
to	O
exercise	O
the	O
posterior	O
distribution	B
for	O
the	O
syndrome	B
decoding	B
problem	O
is	O
a	O
pathological	O
distribution	B
from	O
the	O
point	O
of	O
view	O
of	O
gibbs	B
sampling	I
the	O
factor	O
z	O
is	O
only	O
on	O
a	O
small	O
fraction	O
of	O
the	O
space	O
of	O
possible	O
vectors	B
n	O
namely	O
the	O
points	O
that	O
correspond	O
to	O
the	O
valid	O
codewords	O
no	O
two	O
codewords	O
are	O
adjacent	O
so	O
similarly	O
any	O
single	O
bit	B
from	O
a	O
viable	O
state	O
n	O
will	O
take	O
us	O
to	O
a	O
state	O
with	O
zero	O
probability	B
and	O
so	O
the	O
state	O
will	O
never	O
move	O
in	O
gibbs	B
sampling	I
a	O
general	O
code	O
has	O
exactly	O
the	O
same	O
problem	O
the	O
points	O
corresponding	O
to	O
valid	O
codewords	O
are	O
relatively	O
few	O
in	O
number	O
and	O
they	O
are	O
not	O
adjacent	O
least	O
for	O
any	O
useful	O
code	O
so	O
gibbs	B
sampling	I
is	O
no	O
use	O
for	O
syndrome	B
decoding	B
for	O
two	O
reasons	O
first	O
any	O
reasonably	O
good	B
hypothesis	O
is	O
and	O
as	O
long	O
as	O
the	O
state	O
is	O
not	O
near	O
a	O
valid	O
codeword	B
gibbs	B
sampling	I
cannot	O
help	O
since	O
none	O
of	O
the	O
conditional	B
distributions	O
is	O
and	O
second	O
once	O
we	O
are	O
in	O
a	O
valid	O
hypothesis	O
gibbs	B
sampling	I
will	O
never	O
take	O
us	O
out	O
of	O
it	O
one	O
could	O
attempt	O
to	O
perform	O
gibbs	B
sampling	I
using	O
the	O
bits	O
of	O
the	O
original	O
message	O
s	O
as	O
the	O
variables	O
this	O
approach	O
would	O
not	O
get	O
locked	O
up	O
in	O
the	O
way	O
just	O
described	O
but	O
for	O
a	O
good	B
code	O
any	O
single	O
bit	B
would	O
substantially	O
alter	O
the	O
reconstructed	O
codeword	B
so	O
if	O
one	O
had	O
found	O
a	O
state	O
with	O
reasonably	O
large	O
likelihood	B
gibbs	B
sampling	I
would	O
take	O
an	O
impractically	O
large	O
time	O
to	O
escape	O
from	O
it	O
solution	O
to	O
exercise	O
each	O
metropolis	O
proposal	O
will	O
take	O
the	O
energy	B
of	O
the	O
state	O
up	O
or	O
down	O
by	O
some	O
amount	O
the	O
total	O
change	O
in	O
energy	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
when	O
b	O
proposals	O
are	O
concatenated	B
will	O
be	O
the	O
end-point	O
of	O
a	O
random	B
walk	I
with	O
b	O
steps	O
in	O
it	O
this	O
walk	O
might	O
have	O
mean	B
zero	O
or	O
it	O
might	O
have	O
a	O
tendency	O
to	O
drift	O
upwards	O
most	O
moves	O
increase	O
the	O
energy	B
and	O
only	O
a	O
few	O
decrease	O
it	O
in	O
general	O
the	O
latter	O
will	O
hold	O
if	O
the	O
acceptance	B
rate	B
f	O
is	O
small	O
the	O
mean	B
change	O
in	O
energy	B
from	O
any	O
one	O
move	O
will	O
be	O
some	O
and	O
so	O
the	O
acceptance	O
probability	B
for	O
the	O
concatenation	B
of	O
b	O
moves	O
will	O
be	O
of	O
order	O
which	O
scales	O
roughly	O
as	O
f	O
b	O
the	O
mean-square-distance	O
moved	O
will	O
be	O
of	O
order	O
f	O
where	O
is	O
the	O
typical	B
step	O
size	O
in	O
contrast	O
the	O
mean-square-distance	O
moved	O
when	O
the	O
moves	O
are	O
considered	O
individually	O
will	O
be	O
of	O
order	O
f	O
theory	O
theory	O
figure	O
importance	B
sampling	I
in	O
one	O
dimension	O
for	O
r	O
and	O
the	O
normalizing	O
constant	O
of	O
a	O
gaussian	B
distribution	B
in	O
fact	O
to	O
be	O
was	O
estimated	O
using	O
importance	B
sampling	I
with	O
a	O
sampler	B
density	B
of	O
standard	B
deviation	I
axis	O
the	O
same	O
random	B
number	O
seed	O
was	O
used	O
for	O
all	O
runs	O
the	O
three	O
plots	O
show	O
the	O
estimated	O
normalizing	O
constant	O
the	O
empirical	O
standard	B
deviation	I
of	O
the	O
r	O
weights	O
of	O
the	O
weights	O
solution	O
to	O
exercise	O
the	O
weights	O
are	O
w	O
p	O
and	O
x	O
is	O
drawn	O
from	O
q	O
the	O
mean	B
weight	O
is	O
z	O
dx	O
qx	O
dx	O
p	O
assuming	O
the	O
integral	B
converges	O
the	O
variance	B
is	O
varw	O
z	O
dx	O
p	O
qx	O
z	O
dx	O
dx	O
p	O
p	O
qx	O
qx	O
zq	O
p	O
z	O
p	O
where	O
zqz	O
of	O
in	O
the	O
exponent	O
is	O
positive	O
i	O
e	O
if	O
p	O
the	O
integral	B
in	O
is	O
only	O
if	O
the	O
if	O
this	O
condition	O
is	O
the	O
variance	B
is	O
q	O
p	O
varw	O
p	O
p	O
q	O
q	O
as	O
approaches	O
the	O
critical	O
value	O
about	O
the	O
variance	B
becomes	O
figure	O
illustrates	O
these	O
phenomena	O
for	O
with	O
varying	O
from	O
to	O
the	O
same	O
random	B
number	O
seed	O
was	O
used	O
for	O
all	O
runs	O
so	O
the	O
weights	O
and	O
estimates	O
follow	O
smooth	O
curves	O
notice	O
that	O
the	O
empirical	O
standard	B
deviation	I
of	O
the	O
r	O
weights	O
can	O
look	O
quite	O
small	O
and	O
well-behaved	O
at	O
when	O
the	O
true	O
standard	B
deviation	I
is	O
nevertheless	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
this	O
chapter	O
discusses	O
several	O
methods	O
for	O
reducing	O
random	B
walk	I
behaviour	O
in	O
metropolis	O
methods	O
the	O
aim	O
is	O
to	O
reduce	O
the	O
time	O
required	O
to	O
obtain	O
independent	O
samples	O
for	O
brevity	O
we	O
will	O
say	O
independent	O
samples	O
when	O
we	O
mean	B
independent	O
samples	O
hamiltonian	B
monte	B
carlo	I
the	O
hamiltonian	B
monte	B
carlo	I
method	O
is	O
a	O
metropolis	B
method	I
applicable	O
to	O
continuous	B
state	O
spaces	O
that	O
makes	O
use	O
of	O
gradient	O
information	B
to	O
reduce	O
random	B
walk	I
behaviour	O
hamiltonian	B
monte	B
carlo	I
method	O
was	O
originally	O
called	O
hybrid	O
monte	B
carlo	I
for	O
historical	O
reasons	O
for	O
many	O
systems	O
whose	O
probability	B
p	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
z	O
not	O
only	O
ex	O
but	O
also	O
its	O
gradient	O
with	O
respect	O
to	O
x	O
can	O
be	O
readily	O
evaluated	O
it	O
seems	O
wasteful	O
to	O
use	O
a	O
simple	O
random-walk	B
metropolis	B
method	I
when	O
this	O
gradient	O
is	O
available	O
the	O
gradient	O
indicates	O
which	O
direction	O
one	O
should	O
go	O
in	O
to	O
states	O
that	O
have	O
higher	O
probability	B
overview	O
of	O
hamiltonian	B
monte	B
carlo	I
in	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
the	O
state	O
space	O
x	O
is	O
augmented	O
by	O
momentum	B
variables	O
p	O
and	O
there	O
is	O
an	O
alternation	O
of	O
two	O
types	O
of	O
proposal	O
the	O
proposal	O
randomizes	O
the	O
momentum	B
variable	O
leaving	O
the	O
state	O
x	O
unchanged	O
the	O
second	O
proposal	O
changes	O
both	O
x	O
and	O
p	O
using	O
simulated	O
hamiltonian	O
dynamics	O
as	O
by	O
the	O
hamiltonian	O
hx	O
p	O
ex	O
kp	O
where	O
kp	O
is	O
a	O
kinetic	O
energy	B
such	O
as	O
kp	O
these	O
two	O
proposals	O
are	O
used	O
to	O
create	O
samples	O
from	O
the	O
joint	B
density	B
ph	O
p	O
zh	O
p	O
zh	O
this	O
density	B
is	O
separable	O
so	O
the	O
marginal	B
distribution	B
of	O
x	O
is	O
the	O
desired	O
distribution	B
so	O
simply	O
discarding	O
the	O
momentum	B
variables	O
we	O
obtain	O
a	O
sequence	B
of	O
samples	O
fxtg	O
that	O
asymptotically	O
come	O
from	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
algorithm	O
octave	B
source	O
code	O
for	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
g	O
grade	O
x	O
e	O
finde	O
x	O
set	B
gradient	O
using	O
initial	O
x	O
set	B
objective	B
function	I
too	O
for	O
l	O
p	O
randn	O
sizex	O
h	O
p	O
p	O
e	O
loop	O
l	O
times	O
initial	O
momentum	B
is	O
evaluate	O
hxp	O
xnew	O
x	O
for	O
tau	O
gnew	O
g	O
make	O
tau	O
leapfrog	B
steps	O
p	O
p	O
epsilon	O
gnew	O
make	O
half-step	O
in	O
p	O
xnew	O
xnew	O
epsilon	O
p	O
gnew	O
grade	O
xnew	O
p	O
p	O
epsilon	O
gnew	O
make	O
half-step	O
in	O
p	O
make	O
step	O
in	O
x	O
find	O
new	O
gradient	O
endfor	O
enew	O
finde	O
xnew	O
hnew	O
p	O
p	O
enew	O
dh	O
hnew	O
h	O
find	O
new	O
value	O
of	O
h	O
decide	O
whether	O
to	O
accept	O
if	O
dh	O
accept	O
elseif	O
rand	O
exp-dh	O
accept	O
else	O
accept	O
endif	O
if	O
accept	O
g	O
gnew	O
endif	O
endfor	O
x	O
xnew	O
e	O
enew	O
hamiltonian	B
monte	B
carlo	I
simple	O
metropolis	O
figure	O
hamiltonian	B
monte	B
carlo	I
used	O
to	O
generate	O
samples	O
from	O
a	O
bivariate	B
gaussian	B
with	O
correlation	O
for	O
comparison	O
a	O
simple	O
random-walk	B
metropolis	B
method	I
given	O
equal	O
computer	B
time	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
hamiltonian	B
monte	B
carlo	I
details	O
of	O
hamiltonian	B
monte	B
carlo	I
the	O
proposal	O
which	O
can	O
be	O
viewed	O
as	O
a	O
gibbs	B
sampling	I
update	O
draws	O
a	O
new	O
momentum	B
from	O
the	O
gaussian	B
density	B
this	O
proposal	O
is	O
always	O
accepted	O
during	O
the	O
second	O
dynamical	O
proposal	O
the	O
momentum	B
variable	O
determines	O
where	O
the	O
state	O
x	O
goes	O
and	O
the	O
gradient	O
of	O
ex	O
determines	O
how	O
the	O
momentum	B
p	O
changes	O
in	O
accordance	O
with	O
the	O
equations	O
p	O
because	O
of	O
the	O
persistent	O
motion	O
of	O
x	O
in	O
the	O
direction	O
of	O
the	O
momentum	B
p	O
during	O
each	O
dynamical	O
proposal	O
the	O
state	O
of	O
the	O
system	O
tends	O
to	O
move	O
a	O
distance	B
that	O
goes	O
linearly	O
with	O
the	O
computer	B
time	O
rather	O
than	O
as	O
the	O
square	B
root	O
the	O
second	O
proposal	O
is	O
accepted	O
in	O
accordance	O
with	O
the	O
metropolis	O
rule	O
if	O
the	O
simulation	O
of	O
the	O
hamiltonian	O
dynamics	O
is	O
numerically	O
perfect	B
then	O
the	O
proposals	O
are	O
accepted	O
every	O
time	O
because	O
the	O
total	O
energy	B
hx	O
p	O
is	O
a	O
constant	O
of	O
the	O
motion	O
and	O
so	O
a	O
in	O
equation	O
is	O
equal	O
to	O
one	O
if	O
the	O
simulation	O
is	O
imperfect	O
because	O
of	O
step	O
sizes	O
for	O
example	O
then	O
some	O
of	O
the	O
dynamical	O
proposals	O
will	O
be	O
rejected	O
the	O
rejection	B
rule	O
makes	O
use	O
of	O
the	O
change	O
in	O
hx	O
p	O
which	O
is	O
zero	O
if	O
the	O
simulation	O
is	O
perfect	B
the	O
occasional	O
rejections	O
ensure	O
that	O
asymptotically	O
we	O
obtain	O
samples	O
pt	O
from	O
the	O
required	O
joint	B
density	B
ph	O
p	O
the	O
source	O
code	O
in	O
describes	O
a	O
hamiltonian	B
monte	B
carlo	I
method	O
that	O
uses	O
the	O
leapfrog	B
algorithm	I
to	O
simulate	O
the	O
dynamics	O
on	O
the	O
function	O
findex	O
whose	O
gradient	O
is	O
found	O
by	O
the	O
function	O
gradex	O
figure	O
shows	O
this	O
algorithm	O
generating	O
samples	O
from	O
a	O
bivariate	B
gaussian	B
whose	O
energy	B
function	O
is	O
ex	O
xtax	O
with	O
a	O
corresponding	O
to	O
a	O
variancecovariance	O
matrix	B
of	O
in	O
starting	O
from	O
the	O
state	O
marked	O
by	O
the	O
arrow	O
the	O
solid	O
line	O
represents	O
two	O
successive	O
trajectories	O
generated	O
by	O
the	O
hamiltonian	O
dynamics	O
the	O
squares	O
show	O
the	O
endpoints	O
of	O
these	O
two	O
trajectories	O
each	O
trajectory	O
consists	O
of	O
tau	O
leapfrog	B
steps	O
with	O
epsilon	O
these	O
steps	O
are	O
indicated	O
by	O
the	O
crosses	O
on	O
the	O
trajectory	O
in	O
the	O
inset	O
after	O
each	O
trajectory	O
the	O
momentum	B
is	O
randomized	O
here	O
both	O
trajectories	O
are	O
accepted	O
the	O
errors	B
in	O
the	O
hamiltonian	O
were	O
only	O
and	O
respectively	O
figure	O
shows	O
how	O
a	O
sequence	B
of	O
four	O
trajectories	O
converges	O
from	O
an	O
initial	O
condition	O
indicated	O
by	O
the	O
arrow	O
that	O
is	O
not	O
close	O
to	O
the	O
typical	B
set	B
of	O
the	O
target	O
distribution	B
the	O
trajectory	O
parameters	B
tau	O
and	O
epsilon	O
were	O
randomized	O
for	O
each	O
trajectory	O
using	O
uniform	O
distributions	O
with	O
means	O
and	O
respectively	O
the	O
trajectory	O
takes	O
us	O
to	O
a	O
new	O
state	O
similar	O
in	O
energy	B
to	O
the	O
state	O
the	O
second	O
trajectory	O
happens	O
to	O
end	O
in	O
a	O
state	O
nearer	O
the	O
bottom	O
of	O
the	O
energy	B
landscape	O
here	O
since	O
the	O
potential	O
energy	B
e	O
is	O
smaller	O
the	O
kinetic	O
energy	B
k	O
is	O
necessarily	O
larger	O
than	O
it	O
was	O
at	O
the	O
start	O
of	O
the	O
trajectory	O
when	O
the	O
momentum	B
is	O
randomized	O
before	O
the	O
third	O
trajectory	O
its	O
kinetic	O
energy	B
becomes	O
much	O
smaller	O
after	O
the	O
fourth	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gibbs	B
sampling	I
overrelaxation	B
monte	B
carlo	I
methods	I
figure	O
overrelaxation	B
contrasted	O
with	O
gibbs	B
sampling	I
for	O
a	O
bivariate	B
gaussian	B
with	O
correlation	O
the	O
state	O
sequence	B
for	O
iterations	O
each	O
iteration	O
involving	O
one	O
update	O
of	O
both	O
variables	O
the	O
overrelaxation	B
method	O
had	O
excessively	O
large	O
value	O
is	O
chosen	O
to	O
make	O
it	O
easy	O
to	O
see	O
how	O
the	O
overrelaxation	B
method	O
reduces	O
random	B
walk	I
behaviour	O
the	O
dotted	O
line	O
shows	O
the	O
contour	O
detail	O
of	O
showing	O
the	O
two	O
steps	O
making	O
up	O
each	O
iteration	O
time-course	O
of	O
the	O
variable	O
during	O
iterations	O
of	O
the	O
two	O
methods	O
the	O
overrelaxation	B
method	O
had	O
neal	B
gibbs	B
sampling	I
overrelaxation	B
trajectory	O
has	O
been	O
simulated	O
the	O
state	O
appears	O
to	O
have	O
become	O
typical	B
of	O
the	O
target	O
density	B
figures	O
and	O
show	O
a	O
random-walk	B
metropolis	B
method	I
using	O
a	O
gaussian	B
proposal	B
density	B
to	O
sample	B
from	I
the	O
same	O
gaussian	B
distribution	B
starting	O
from	O
the	O
initial	O
conditions	O
of	O
and	O
respectively	O
in	O
the	O
step	O
size	O
was	O
adjusted	O
such	O
that	O
the	O
acceptance	B
rate	B
was	O
the	O
number	O
of	O
proposals	O
was	O
so	O
the	O
total	O
amount	O
of	O
computer	B
time	O
used	O
was	O
similar	O
to	O
that	O
in	O
the	O
distance	B
moved	O
is	O
small	O
because	O
of	O
random	B
walk	I
behaviour	O
in	O
the	O
random-walk	B
metropolis	B
method	I
was	O
used	O
and	O
started	O
from	O
the	O
same	O
initial	O
condition	O
as	O
and	O
given	O
a	O
similar	O
amount	O
of	O
computer	B
time	O
overrelaxation	B
the	O
method	O
of	O
overrelaxation	B
is	O
a	O
method	O
for	O
reducing	O
random	B
walk	I
behaviour	O
in	O
gibbs	B
sampling	I
overrelaxation	B
was	O
originally	O
introduced	O
for	O
systems	O
in	O
which	O
all	O
the	O
conditional	B
distributions	O
are	O
gaussian	B
an	O
example	O
of	O
a	O
joint	B
distribution	B
that	O
is	O
not	O
gaussian	B
but	O
whose	O
conditional	B
distributions	O
are	O
all	O
gaussian	B
is	O
p	O
y	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
overrelaxation	B
overrelaxation	B
for	O
gaussian	B
conditional	B
distributions	O
in	O
ordinary	O
gibbs	B
sampling	I
one	O
draws	O
the	O
new	O
value	O
of	O
the	O
current	O
variable	O
xi	O
from	O
its	O
conditional	B
distribution	B
ignoring	O
the	O
old	O
value	O
xt	O
the	O
i	O
state	O
makes	O
lengthy	O
random	B
walks	O
in	O
cases	O
where	O
the	O
variables	O
are	O
strongly	O
correlated	O
as	O
illustrated	O
in	O
the	O
left-hand	O
panel	O
of	O
this	O
uses	O
a	O
correlated	O
gaussian	B
distribution	B
as	O
the	O
target	O
density	B
i	O
in	O
adler	O
s	O
overrelaxation	B
method	O
one	O
instead	O
samples	O
from	O
a	O
gaussian	B
that	O
is	O
biased	O
to	O
the	O
opposite	O
side	O
of	O
the	O
conditional	B
distribution	B
if	O
the	O
conditional	B
distribution	B
of	O
xi	O
is	O
and	O
the	O
current	O
value	O
of	O
xi	O
is	O
xt	O
i	O
then	O
adler	O
s	O
method	O
sets	O
xi	O
to	O
i	O
i	O
where	O
and	O
is	O
a	O
parameter	O
between	O
and	O
usually	O
set	B
to	O
a	O
negative	O
value	O
is	O
positive	O
then	O
the	O
method	O
is	O
called	O
under-relaxation	O
exercise	O
show	O
that	O
this	O
individual	O
transition	B
leaves	O
invariant	O
the	O
con	O
i	O
ditional	O
distribution	B
xi	O
a	O
single	O
iteration	O
of	O
adler	O
s	O
overrelaxation	B
like	O
one	O
of	O
gibbs	B
sampling	I
updates	O
each	O
variable	O
in	O
turn	O
as	O
indicated	O
in	O
equation	O
the	O
transition	B
matrix	B
t	O
x	O
by	O
a	O
complete	O
update	O
of	O
all	O
variables	O
in	O
some	O
order	O
does	O
not	O
satisfy	O
detailed	B
balance	B
each	O
individual	O
transition	B
for	O
one	O
coordinate	O
just	O
described	O
does	O
satisfy	O
detailed	B
balance	B
so	O
the	O
overall	O
chain	O
gives	O
a	O
valid	O
sampling	O
strategy	O
which	O
converges	O
to	O
the	O
target	O
density	B
p	O
but	O
when	O
we	O
form	O
a	O
chain	O
by	O
applying	O
the	O
individual	O
transitions	O
in	O
a	O
sequence	B
the	O
overall	O
chain	O
is	O
not	O
reversible	B
this	O
temporal	O
asymmetry	O
is	O
the	O
key	O
to	O
why	O
overrelaxation	B
can	O
be	O
if	O
say	O
two	O
variables	O
are	O
positively	O
correlated	O
then	O
they	O
will	O
a	O
short	O
timescale	O
evolve	O
in	O
a	O
directed	O
manner	O
instead	O
of	O
by	O
random	B
walk	I
as	O
shown	O
in	O
this	O
may	O
reduce	O
the	O
time	O
required	O
to	O
obtain	O
independent	O
samples	O
exercise	O
the	O
transition	B
matrix	B
t	O
x	O
by	O
a	O
complete	O
update	O
of	O
all	O
variables	O
in	O
some	O
order	O
does	O
not	O
satisfy	O
detailed	B
balance	B
if	O
the	O
updates	O
were	O
in	O
a	O
random	B
order	O
then	O
t	O
would	O
be	O
symmetric	B
investigate	O
for	O
the	O
toy	O
two-dimensional	B
gaussian	B
distribution	B
the	O
assertion	O
that	O
the	O
advantages	O
of	O
overrelaxation	B
are	O
lost	O
if	O
the	O
overrelaxed	O
updates	O
are	O
made	O
in	O
a	O
random	B
order	O
ordered	B
overrelaxation	B
the	O
overrelaxation	B
method	O
has	O
been	O
generalized	B
by	O
neal	B
whose	O
ordered	B
overrelaxation	B
method	O
is	O
applicable	O
to	O
any	O
system	O
where	O
gibbs	B
sampling	I
is	O
used	O
in	O
ordered	B
overrelaxation	B
instead	O
of	O
taking	O
one	O
sample	B
from	I
the	O
conditional	B
distribution	B
p	O
we	O
create	O
k	O
such	O
samples	O
xk	O
where	O
k	O
might	O
be	O
set	B
to	O
twenty	O
or	O
so	O
often	O
generating	O
k	O
extra	O
samples	O
adds	O
a	O
negligible	O
computational	O
cost	O
to	O
the	O
initial	O
computations	O
required	O
for	O
making	O
the	O
sample	B
the	O
points	O
fxk	O
i	O
g	O
are	O
then	O
sorted	O
numerically	O
and	O
the	O
current	O
value	O
of	O
xi	O
is	O
inserted	O
into	O
the	O
sorted	O
list	O
giving	O
a	O
list	O
of	O
k	O
points	O
we	O
give	O
them	O
ranks	O
k	O
let	O
be	O
the	O
rank	O
of	O
the	O
current	O
value	O
of	O
xi	O
in	O
the	O
list	O
we	O
set	B
to	O
the	O
value	O
that	O
is	O
an	O
equal	O
distance	B
from	O
the	O
other	O
end	O
of	O
the	O
list	O
that	O
is	O
the	O
value	O
with	O
rank	O
k	O
the	O
role	O
played	O
by	O
adler	O
s	O
parameter	O
is	O
here	O
played	O
by	O
the	O
parameter	O
k	O
when	O
k	O
we	O
obtain	O
ordinary	O
gibbs	B
sampling	I
for	O
practical	B
purposes	O
neal	B
estimates	O
that	O
ordered	B
overrelaxation	B
may	O
speed	O
up	O
a	O
simulation	O
by	O
a	O
factor	O
of	O
ten	O
or	O
twenty	O
i	O
i	O
i	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
simulated	B
annealing	B
a	O
third	O
technique	O
for	O
speeding	O
convergence	O
is	O
simulated	B
annealing	B
in	O
simulated	B
annealing	B
a	O
temperature	B
parameter	O
is	O
introduced	O
which	O
when	O
large	O
allows	O
the	O
system	O
to	O
make	O
transitions	O
that	O
would	O
be	O
improbable	O
at	O
temperature	B
the	O
temperature	B
is	O
set	B
to	O
a	O
large	O
value	O
and	O
gradually	O
reduced	O
to	O
this	O
procedure	O
is	O
supposed	O
to	O
reduce	O
the	O
chance	O
that	O
the	O
simulation	O
gets	O
stuck	O
in	O
an	O
unrepresentative	O
probability	B
island	O
we	O
asssume	O
that	O
we	O
wish	O
to	O
sample	B
from	I
a	O
distribution	B
of	O
the	O
form	O
p	O
z	O
where	O
ex	O
can	O
be	O
evaluated	O
in	O
the	O
simplest	O
simulated	B
annealing	B
method	O
we	O
instead	O
sample	B
from	I
the	O
distribution	B
pt	O
zt	O
ex	O
t	O
and	O
decrease	O
t	O
gradually	O
to	O
often	O
the	O
energy	B
function	O
can	O
be	O
separated	O
into	O
two	O
terms	O
ex	O
of	O
which	O
the	O
term	O
is	O
nice	O
example	O
a	O
separable	O
function	O
of	O
x	O
and	O
the	O
second	O
is	O
nasty	O
in	O
these	O
cases	O
a	O
better	O
simulated	B
annealing	B
method	O
might	O
make	O
use	O
of	O
the	O
distribution	B
p	O
z	O
with	O
t	O
gradually	O
decreasing	O
to	O
in	O
this	O
way	O
the	O
distribution	B
at	O
high	O
temperatures	O
reverts	O
to	O
a	O
well-behaved	O
distribution	B
by	O
simulated	B
annealing	B
is	O
often	O
used	O
as	O
an	O
optimization	B
method	O
where	O
the	O
aim	O
is	O
to	O
an	O
x	O
that	O
minimizes	O
ex	O
in	O
which	O
case	O
the	O
temperature	B
is	O
decreased	O
to	O
zero	O
rather	O
than	O
to	O
as	O
a	O
monte	B
carlo	I
method	O
simulated	B
annealing	B
as	O
described	O
above	O
doesn	O
t	O
sample	B
exactly	O
from	O
the	O
right	O
distribution	B
because	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
probability	B
of	O
falling	O
into	O
one	O
basin	O
of	O
the	O
energy	B
is	O
equal	O
to	O
the	O
total	O
probability	B
of	O
all	O
the	O
states	O
in	O
that	O
basin	O
the	O
closely	O
related	O
simulated	O
tempering	O
method	O
and	O
parisi	O
corrects	O
the	O
biases	O
introduced	O
by	O
the	O
annealing	B
process	O
by	O
making	O
the	O
temperature	B
itself	O
a	O
random	B
variable	I
that	O
is	O
updated	O
in	O
metropolis	O
fashion	O
during	O
the	O
simulation	O
neal	B
s	O
annealed	O
importance	B
sampling	I
method	O
removes	O
the	O
biases	O
introduced	O
by	O
annealing	B
by	O
computing	O
importance	O
weights	O
for	O
each	O
generated	O
point	O
skilling	B
s	O
multi-state	B
leapfrog	B
method	O
a	O
fourth	O
method	O
for	O
speeding	O
up	O
monte	B
carlo	I
simulations	O
due	O
to	O
john	O
skilling	B
has	O
a	O
similar	O
spirit	O
to	O
overrelaxation	B
but	O
works	O
in	O
more	O
dimensions	B
this	O
method	O
is	O
applicable	O
to	O
sampling	O
from	O
a	O
distribution	B
over	O
a	O
continuous	B
state	O
space	O
and	O
the	O
sole	O
requirement	O
is	O
that	O
the	O
energy	B
ex	O
should	O
be	O
easy	O
to	O
evaluate	O
the	O
gradient	O
is	O
not	O
used	O
this	O
leapfrog	B
method	O
is	O
not	O
intended	O
to	O
be	O
used	O
on	O
its	O
own	O
but	O
rather	O
in	O
sequence	B
with	O
other	O
monte	B
carlo	I
operators	O
instead	O
of	O
moving	O
just	O
one	O
state	O
vector	O
x	O
around	O
the	O
state	O
space	O
as	O
was	O
the	O
case	O
for	O
all	O
the	O
monte	B
carlo	I
methods	I
discussed	O
thus	O
far	O
skilling	B
s	O
leapfrog	B
method	O
simultaneously	O
maintains	O
a	O
set	B
of	O
s	O
state	O
vectors	B
fxsg	O
where	O
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
skilling	B
s	O
multi-state	B
leapfrog	B
method	O
might	O
be	O
six	B
or	O
twelve	O
the	O
aim	O
is	O
that	O
all	O
s	O
of	O
these	O
vectors	B
will	O
represent	O
independent	O
samples	O
from	O
the	O
same	O
distribution	B
p	O
skilling	B
s	O
leapfrog	B
makes	O
a	O
proposal	O
for	O
the	O
new	O
state	O
which	O
is	O
accepted	O
or	O
rejected	O
in	O
accordance	O
with	O
the	O
metropolis	B
method	I
by	O
leapfrogging	O
the	O
current	O
state	O
xs	O
over	O
another	O
state	O
vector	O
xt	O
xt	O
xs	O
xs	O
all	O
the	O
other	O
state	O
vectors	B
are	O
left	O
where	O
they	O
are	O
so	O
the	O
acceptance	O
probability	B
depends	O
only	O
on	O
the	O
change	O
in	O
energy	B
of	O
xs	O
which	O
vector	O
t	O
is	O
the	O
partner	O
for	O
the	O
leapfrog	B
event	O
can	O
be	O
chosen	O
in	O
various	O
ways	O
the	O
simplest	O
method	O
is	O
to	O
select	O
the	O
partner	O
at	O
random	B
from	O
the	O
other	O
vectors	B
it	O
might	O
be	O
better	O
to	O
choose	O
t	O
by	O
selecting	O
one	O
of	O
the	O
nearest	O
neighbours	O
xs	O
nearest	O
by	O
any	O
chosen	O
distance	B
function	O
as	O
long	O
as	O
one	O
then	O
uses	O
an	O
acceptance	O
rule	O
that	O
ensures	O
detailed	B
balance	B
by	O
checking	O
whether	O
point	O
t	O
is	O
still	O
among	O
the	O
nearest	O
neighbours	O
of	O
the	O
new	O
point	O
xs	O
xt	O
why	O
the	O
leapfrog	B
is	O
a	O
good	B
idea	O
imagine	O
that	O
the	O
target	O
density	B
p	O
has	O
strong	O
correlations	B
for	O
example	O
the	O
density	B
might	O
be	O
a	O
needle-like	O
gaussian	B
with	O
width	O
and	O
length	O
where	O
l	O
as	O
we	O
have	O
emphasized	O
motion	O
around	O
such	O
a	O
density	B
by	O
standard	O
methods	O
proceeds	O
by	O
a	O
slow	O
random	B
walk	I
imagine	O
now	O
that	O
our	O
set	B
of	O
s	O
points	O
is	O
lurking	O
initially	O
in	O
a	O
location	O
that	O
is	O
probable	O
under	O
the	O
density	B
but	O
in	O
an	O
inappropriately	O
small	O
ball	O
of	O
size	O
now	O
under	O
skilling	B
s	O
leapfrog	B
method	O
a	O
typical	B
move	O
will	O
take	O
the	O
point	O
a	O
little	O
outside	O
the	O
current	O
ball	O
perhaps	O
doubling	O
its	O
distance	B
from	O
the	O
centre	O
of	O
the	O
ball	O
after	O
all	O
the	O
points	O
have	O
had	O
a	O
chance	O
to	O
move	O
the	O
ball	O
will	O
have	O
increased	O
in	O
size	O
if	O
all	O
the	O
moves	O
are	O
accepted	O
the	O
ball	O
will	O
be	O
bigger	O
by	O
a	O
factor	O
of	O
two	O
or	O
so	O
in	O
all	O
dimensions	B
the	O
rejection	B
of	O
some	O
moves	O
will	O
mean	B
that	O
the	O
ball	O
containing	O
the	O
points	O
will	O
probably	O
have	O
elongated	O
in	O
the	O
needle	B
s	O
long	O
direction	O
by	O
a	O
factor	O
of	O
say	O
two	O
after	O
another	O
cycle	O
through	O
the	O
points	O
the	O
ball	O
will	O
have	O
grown	O
in	O
the	O
long	O
direction	O
by	O
another	O
factor	O
of	O
two	O
so	O
the	O
typical	B
distance	B
travelled	O
in	O
the	O
long	O
dimension	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
iterations	O
now	O
maybe	O
a	O
factor	O
of	O
two	O
growth	O
per	O
iteration	O
is	O
on	O
the	O
optimistic	O
side	O
but	O
even	O
if	O
the	O
ball	O
only	O
grows	O
by	O
a	O
factor	O
of	O
let	O
s	O
say	O
per	O
iteration	O
the	O
growth	O
is	O
nevertheless	O
exponential	B
it	O
will	O
only	O
take	O
a	O
number	O
of	O
iterations	O
proportional	O
to	O
log	O
l	O
for	O
the	O
long	O
dimension	O
to	O
be	O
explored	O
exercise	O
discuss	O
how	O
the	O
of	O
skilling	B
s	O
method	O
scales	O
with	O
dimensionality	O
using	O
a	O
correlated	O
n	O
gaussian	B
distribution	B
as	O
an	O
example	O
find	O
an	O
expression	O
for	O
the	O
rejection	B
probability	B
assuming	O
the	O
markov	B
chain	I
is	O
at	O
equilibrium	O
also	O
discuss	O
how	O
it	O
scales	O
with	O
the	O
strength	O
of	O
correlation	O
among	O
the	O
gaussian	B
variables	O
skilling	B
s	O
method	O
is	O
invariant	O
under	O
transformations	O
so	O
the	O
rejection	B
probability	B
at	O
equilibrium	O
can	O
be	O
found	O
by	O
looking	O
at	O
the	O
case	O
of	O
a	O
separable	O
gaussian	B
this	O
method	O
has	O
some	O
similarity	O
to	O
the	O
adaptive	B
direction	I
sampling	I
method	O
of	O
gilks	B
et	O
al	O
but	O
the	O
leapfrog	B
method	O
is	O
simpler	O
and	O
can	O
be	O
applied	O
to	O
a	O
greater	O
variety	O
of	O
distributions	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
monte	B
carlo	I
algorithms	B
as	B
communication	B
channels	O
it	O
may	O
be	O
a	O
helpful	O
perspective	O
when	O
thinking	O
about	O
speeding	O
up	O
monte	B
carlo	I
methods	I
to	O
think	O
about	O
the	O
information	B
that	O
is	O
being	O
communicated	O
two	O
communications	O
take	O
place	O
when	O
a	O
sample	B
from	I
p	O
is	O
being	O
generated	O
first	O
the	O
selection	O
of	O
a	O
particular	O
x	O
from	O
p	O
necessarily	O
requires	O
that	O
at	O
least	O
log	O
random	B
bits	O
be	O
consumed	O
the	O
use	O
of	O
inverse	O
arithmetic	B
coding	I
as	O
a	O
method	O
for	O
generating	O
samples	O
from	O
given	O
distributions	O
second	O
the	O
generation	O
of	O
a	O
sample	B
conveys	O
information	B
about	O
p	O
from	O
the	O
subroutine	O
that	O
is	O
able	O
to	O
evaluate	O
p	O
from	O
any	O
other	O
subroutines	O
that	O
have	O
access	O
to	O
properties	O
of	O
p	O
consider	O
a	O
dumb	B
metropolis	B
method	I
for	O
example	O
in	O
a	O
dumb	B
metropolis	B
method	I
the	O
proposals	O
x	O
have	O
nothing	O
to	O
do	O
with	O
p	O
properties	O
of	O
p	O
are	O
only	O
involved	O
in	O
the	O
algorithm	O
at	O
the	O
acceptance	O
step	O
when	O
the	O
ratio	O
p	O
is	O
computed	O
the	O
channel	O
from	O
the	O
true	O
distribution	B
p	O
to	O
the	O
user	O
who	O
is	O
interested	O
in	O
computing	O
properties	O
of	O
p	O
thus	O
passes	O
through	O
a	O
bottleneck	O
all	O
the	O
information	B
about	O
p	O
is	O
conveyed	O
by	O
the	O
string	O
of	O
acceptances	O
and	O
rejections	O
if	O
p	O
were	O
replaced	O
by	O
a	O
distribution	B
the	O
only	O
way	O
in	O
which	O
this	O
change	O
would	O
have	O
an	O
is	O
that	O
the	O
string	O
of	O
acceptances	O
and	O
rejections	O
would	O
be	O
changed	O
i	O
am	O
not	O
aware	O
of	O
much	O
use	O
being	O
made	O
of	O
this	O
information-theoretic	O
view	O
of	O
monte	B
carlo	I
algorithms	B
but	O
i	O
think	O
it	O
is	O
an	O
instructive	O
viewpoint	O
if	O
the	O
aim	O
is	O
to	O
obtain	O
information	B
about	O
properties	O
of	O
p	O
then	O
presumably	O
it	O
is	O
helpful	O
to	O
identify	O
the	O
channel	O
through	O
which	O
this	O
information	B
and	O
maximize	O
the	O
rate	B
of	O
information	B
transfer	O
example	O
the	O
information-theoretic	O
viewpoint	O
a	O
simple	O
for	O
the	O
widely-adopted	O
rule	B
of	I
thumb	I
which	O
states	O
that	O
the	O
parameters	B
of	O
a	O
dumb	B
metropolis	B
method	I
should	O
be	O
adjusted	O
such	O
that	O
the	O
acceptance	B
rate	B
is	O
about	O
one	O
half	O
let	O
s	O
call	O
the	O
acceptance	O
history	O
that	O
is	O
the	O
binary	O
string	O
of	O
accept	O
or	O
reject	O
decisions	O
a	O
the	O
information	B
learned	O
about	O
p	O
after	O
the	O
algorithm	O
has	O
run	O
for	O
t	O
steps	O
is	O
less	O
than	O
or	O
equal	O
to	O
the	O
information	B
content	I
of	O
a	O
since	O
all	O
information	B
about	O
p	O
is	O
mediated	O
by	O
a	O
and	O
the	O
information	B
content	I
of	O
a	O
is	O
upper-bounded	O
by	O
t	O
where	O
f	O
is	O
the	O
acceptance	B
rate	B
this	O
bound	B
on	O
information	B
acquired	O
about	O
p	O
is	O
maximized	O
by	O
setting	O
f	O
another	O
helpful	O
analogy	O
for	O
a	O
dumb	B
metropolis	B
method	I
is	O
an	O
evolutionary	O
one	O
each	O
proposal	O
generates	O
a	O
progeny	O
from	O
the	O
current	O
state	O
x	O
these	O
two	O
individuals	O
then	O
compete	O
with	O
each	O
other	O
and	O
the	O
metropolis	B
method	I
uses	O
a	O
noisy	B
rule	O
if	O
the	O
progeny	O
is	O
than	O
the	O
parent	B
p	O
p	O
assuming	O
the	O
qq	O
factor	O
is	O
unity	O
then	O
the	O
progeny	O
replaces	O
the	O
parent	B
the	O
survival	O
rule	O
also	O
allows	O
progeny	O
to	O
replace	O
the	O
parent	B
sometimes	O
insights	O
about	O
the	O
rate	B
of	O
evolution	B
can	O
thus	O
be	O
applied	O
to	O
monte	B
carlo	I
methods	I
exercise	O
let	O
x	O
and	O
let	O
p	O
be	O
a	O
separable	O
distribution	B
p	O
pxg	O
with	O
and	O
for	O
example	O
let	O
the	O
proposal	B
density	B
of	O
a	O
dumb	B
metropolis	I
algorithm	O
q	O
involve	O
a	O
fraction	O
m	O
of	O
the	O
g	O
bits	O
in	O
the	O
state	O
x	O
analyze	O
how	O
long	O
it	O
takes	O
for	O
the	O
chain	O
to	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
multi-state	B
methods	O
converge	O
to	O
the	O
target	O
density	B
as	O
a	O
function	O
of	O
m	O
find	O
the	O
optimal	B
m	O
and	O
deduce	O
how	O
long	O
the	O
metropolis	B
method	I
must	O
run	O
for	O
compare	O
the	O
result	O
with	O
the	O
results	O
for	O
an	O
evolving	O
population	O
under	O
natural	B
selection	I
found	O
in	O
chapter	O
the	O
insight	O
that	O
the	O
fastest	O
progress	O
that	O
a	O
standard	O
metropolis	B
method	I
can	O
make	O
in	O
information	B
terms	O
is	O
about	O
one	O
bit	B
per	O
iteration	O
gives	O
a	O
strong	O
motivation	O
for	O
speeding	O
up	O
the	O
algorithm	O
this	O
chapter	O
has	O
already	O
reviewed	O
several	O
methods	O
for	O
reducing	O
random-walk	O
behaviour	O
do	O
these	O
methods	O
also	O
speed	O
up	O
the	O
rate	B
at	O
which	O
information	B
is	O
acquired	O
exercise	O
does	O
gibbs	B
sampling	I
which	O
is	O
a	O
smart	O
metropolis	B
method	I
whose	O
proposal	O
distributions	O
do	O
depend	O
on	O
p	O
allow	O
information	B
about	O
p	O
to	O
leak	O
out	O
at	O
a	O
rate	B
faster	O
than	O
one	O
bit	B
per	O
iteration	O
find	O
toy	O
examples	O
in	O
which	O
this	O
question	O
can	O
be	O
precisely	O
investigated	O
exercise	O
hamiltonian	B
monte	B
carlo	I
is	O
another	O
smart	O
metropolis	B
method	I
in	O
which	O
the	O
proposal	O
distributions	O
depend	O
on	O
p	O
can	O
hamiltonian	B
monte	B
carlo	I
extract	O
information	B
about	O
p	O
at	O
a	O
rate	B
faster	O
than	O
one	O
bit	B
per	O
iteration	O
exercise	O
in	O
importance	B
sampling	I
the	O
weight	O
wr	O
p	O
a	O
number	O
is	O
computed	O
and	O
retained	O
until	O
the	O
end	O
of	O
the	O
computation	O
in	O
contrast	O
in	O
the	O
dumb	B
metropolis	B
method	I
the	O
ratio	O
a	O
p	O
is	O
reduced	O
to	O
a	O
single	O
bit	B
is	O
a	O
bigger	O
than	O
or	O
smaller	O
than	O
the	O
random	B
number	O
u	O
thus	O
in	O
principle	O
importance	B
sampling	I
preserves	O
more	O
information	B
about	O
p	O
than	O
does	O
dumb	B
metropolis	I
can	O
you	O
a	O
toy	O
example	O
in	O
which	O
this	O
extra	O
information	B
does	O
indeed	O
lead	O
to	O
faster	O
convergence	O
of	O
importance	B
sampling	I
than	O
metropolis	O
can	O
you	O
design	O
a	O
markov	B
chain	I
monte	B
carlo	I
algorithm	O
that	O
moves	O
around	O
adaptively	O
like	O
a	O
metropolis	B
method	I
and	O
that	O
retains	O
more	O
useful	O
information	B
about	O
the	O
value	O
of	O
p	O
like	O
importance	B
sampling	I
in	O
chapter	O
we	O
noticed	O
that	O
an	O
evolving	O
population	O
of	O
n	O
individuals	O
can	O
make	O
faster	O
evolutionary	O
progress	O
if	O
the	O
individuals	O
engage	O
in	O
sexual	O
reproduction	O
this	O
observation	O
motivates	O
looking	O
at	O
monte	B
carlo	I
algorithms	B
in	O
which	O
multiple	O
parameter	O
vectors	B
x	O
are	O
evolved	O
and	O
interact	O
multi-state	B
methods	O
in	O
a	O
multi-state	B
method	O
multiple	O
parameter	O
vectors	B
x	O
are	O
maintained	O
they	O
evolve	O
individually	O
under	O
moves	O
such	O
as	O
metropolis	O
and	O
gibbs	B
there	O
are	O
also	O
interactions	O
among	O
the	O
vectors	B
the	O
intention	O
is	O
either	O
that	O
eventually	O
all	O
the	O
vectors	B
x	O
should	O
be	O
samples	O
from	O
p	O
illustrated	O
by	O
skilling	B
s	O
leapfrog	B
method	O
or	O
that	O
information	B
associated	O
with	O
the	O
vectors	B
x	O
should	O
allow	O
us	O
to	O
approximate	O
expectations	O
under	O
p	O
as	O
in	O
importance	B
sampling	I
genetic	B
methods	O
genetic	B
algorithms	B
are	O
not	O
often	O
described	O
by	O
their	O
proponents	O
as	O
monte	B
carlo	I
algorithms	B
but	O
i	O
think	O
this	O
is	O
the	O
correct	O
categorization	O
and	O
an	O
ideal	O
genetic	B
algorithm	I
would	O
be	O
one	O
that	O
can	O
be	O
proved	O
to	O
be	O
a	O
valid	O
monte	B
carlo	I
algorithm	O
that	O
converges	O
to	O
a	O
density	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
i	O
ll	O
use	O
r	O
to	O
denote	O
the	O
number	O
of	O
vectors	B
in	O
the	O
population	O
we	O
aim	O
to	O
have	O
p	O
three	O
types	O
p	O
a	O
genetic	B
algorithm	I
involves	O
moves	O
of	O
two	O
or	O
first	O
individual	O
moves	O
in	O
which	O
one	O
state	O
vector	O
is	O
perturbed	O
xr	O
which	O
could	O
be	O
performed	O
using	O
any	O
of	O
the	O
monte	B
carlo	I
methods	I
we	O
have	O
mentioned	O
so	O
far	O
second	O
we	O
allow	O
crossover	B
moves	O
of	O
the	O
form	O
x	O
y	O
in	O
a	O
typical	B
crossover	B
move	O
the	O
progeny	O
receives	O
half	O
his	O
state	O
vector	O
from	O
one	O
parent	B
x	O
and	O
half	O
from	O
the	O
other	O
y	O
the	O
secret	B
of	O
success	O
in	O
a	O
genetic	B
algorithm	I
is	O
that	O
the	O
parameter	O
x	O
must	O
be	O
encoded	O
in	O
such	O
a	O
way	O
that	O
the	O
crossover	B
of	O
two	O
independent	O
states	O
x	O
and	O
y	O
both	O
of	O
which	O
have	O
good	B
p	O
should	O
have	O
a	O
reasonably	O
good	B
chance	O
of	O
producing	O
progeny	O
who	O
are	O
equally	O
this	O
constraint	O
is	O
a	O
hard	O
one	O
to	O
satisfy	O
in	O
many	O
problems	O
which	O
is	O
why	O
genetic	B
algorithms	B
are	O
mainly	O
talked	O
about	O
and	O
hyped	O
up	O
and	O
rarely	O
used	O
by	O
serious	O
experts	O
having	O
introduced	O
a	O
crossover	B
move	O
x	O
y	O
we	O
need	O
to	O
choose	O
an	O
acceptance	O
rule	O
one	O
easy	O
way	O
to	O
obtain	O
a	O
valid	O
algorithm	O
is	O
to	O
accept	O
or	O
reject	O
the	O
crossover	B
proposal	O
using	O
the	O
metropolis	O
rule	O
with	O
p	O
as	O
the	O
target	O
density	B
this	O
involves	O
comparing	O
the	O
before	O
and	O
after	O
the	O
crossover	B
using	O
the	O
ratio	O
p	O
p	O
if	O
the	O
crossover	B
operator	O
is	O
reversible	B
then	O
we	O
have	O
an	O
easy	O
proof	O
that	O
this	O
procedure	O
detailed	B
balance	B
and	O
so	O
is	O
a	O
valid	O
component	O
in	O
a	O
chain	O
converging	O
to	O
p	O
exercise	O
discuss	O
whether	O
the	O
above	O
two	O
operators	O
individual	O
variation	O
and	O
crossover	B
with	O
the	O
metropolis	O
acceptance	O
rule	O
will	O
give	O
a	O
more	O
monte	B
carlo	I
method	O
than	O
a	O
standard	O
method	O
with	O
only	O
one	O
state	O
vector	O
and	O
no	O
crossover	B
the	O
reason	O
why	O
the	O
sexual	O
community	O
could	O
acquire	O
information	B
faster	O
than	O
the	O
asexual	O
community	O
in	O
chapter	O
was	O
because	O
the	O
crossover	B
operation	O
produced	O
diversity	O
with	O
standard	B
deviation	I
pg	O
then	O
the	O
blind	B
watchmaker	I
was	O
able	O
to	O
convey	O
lots	O
of	O
information	B
about	O
the	O
function	O
by	O
killing	O
the	O
less	O
the	O
above	O
two	O
operators	O
do	O
not	O
a	O
speed-up	O
of	O
pg	O
compared	O
with	O
standard	O
monte	B
carlo	I
methods	I
because	O
there	O
is	O
no	O
killing	O
what	O
s	O
required	O
in	O
order	O
to	O
obtain	O
a	O
speed-up	O
is	O
two	O
things	O
multiplication	O
and	O
death	O
and	O
at	O
least	O
one	O
of	O
these	O
must	O
operate	O
selectively	O
either	O
we	O
must	O
kill	O
the	O
state	O
vectors	B
or	O
we	O
must	O
allow	O
the	O
state	O
vectors	B
to	O
give	O
rise	O
to	O
more	O
while	O
it	O
s	O
easy	O
to	O
sketch	O
these	O
ideas	O
it	O
is	O
hard	O
to	O
a	O
valid	O
method	O
for	O
doing	O
it	O
exercise	O
design	O
a	O
birth	O
rule	O
and	O
a	O
death	O
rule	O
such	O
that	O
the	O
chain	O
converges	O
to	O
p	O
i	O
believe	O
this	O
is	O
still	O
an	O
open	O
research	O
problem	O
particle	O
particle	O
which	O
are	O
particularly	O
popular	O
in	O
inference	B
problems	O
involving	O
temporal	O
tracking	O
are	O
multistate	O
methods	O
that	O
mix	O
the	O
ideas	O
of	O
importance	B
sampling	I
and	O
markov	B
chain	I
monte	B
carlo	I
see	O
isard	O
and	O
blake	O
isard	O
and	O
blake	O
berzuini	O
et	O
al	O
berzuini	O
and	O
gilks	B
doucet	O
et	O
al	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
methods	O
that	O
do	O
not	O
necessarily	O
help	O
methods	O
that	O
do	O
not	O
necessarily	O
help	O
it	O
is	O
common	O
practice	O
to	O
use	O
many	O
initial	O
conditions	O
for	O
a	O
particular	O
markov	B
chain	I
if	O
you	O
are	O
worried	O
about	O
sampling	O
well	O
from	O
a	O
complicated	O
density	B
p	O
can	O
you	O
ensure	O
the	O
states	O
produced	O
by	O
the	O
simulations	O
are	O
well	O
distributed	O
about	O
the	O
typical	B
set	B
of	O
p	O
by	O
ensuring	O
that	O
the	O
initial	O
points	O
are	O
well	O
distributed	O
about	O
the	O
whole	O
state	O
space	O
the	O
answer	O
is	O
unfortunately	O
no	O
in	O
hierarchical	O
bayesian	B
models	O
for	O
example	O
a	O
large	O
number	O
of	O
parameters	B
fxng	O
may	O
be	O
coupled	O
together	O
via	O
another	O
parameter	O
as	O
a	O
hyperparameter	B
for	O
example	O
the	O
quantities	O
fxng	O
might	O
be	O
independent	O
noise	O
signals	O
and	O
might	O
be	O
the	O
inverse-variance	O
of	O
the	O
noise	O
source	O
the	O
joint	B
distribution	B
of	O
and	O
fxng	O
might	O
be	O
p	O
p	O
p	O
p	O
j	O
n	O
n	O
where	O
and	O
p	O
is	O
a	O
broad	O
distribution	B
describing	O
our	O
ignorance	B
about	O
the	O
noise	O
level	O
for	O
simplicity	O
let	O
s	O
leave	O
out	O
all	O
the	O
other	O
variables	O
data	O
and	O
such	O
that	O
might	O
be	O
involved	O
in	O
a	O
realistic	O
problem	O
let	O
s	O
imagine	O
that	O
we	O
want	O
to	O
sample	B
from	I
p	O
by	O
gibbs	B
sampling	I
alternately	O
sampling	O
from	O
the	O
conditional	B
distribution	B
p	O
j	O
xn	O
then	O
sampling	O
all	O
the	O
xn	O
from	O
their	O
conditional	B
distributions	O
p	O
j	O
resulting	O
marginal	B
distribution	B
of	O
should	O
asymptotically	O
be	O
the	O
broad	O
distribution	B
p	O
if	O
n	O
is	O
large	O
then	O
the	O
conditional	B
distribution	B
of	O
given	O
any	O
particular	O
setting	O
of	O
fxng	O
will	O
be	O
tightly	O
concentrated	O
on	O
a	O
particular	O
most-probable	O
value	O
of	O
with	O
width	O
proportional	O
to	O
progress	O
up	O
and	O
down	O
the	O
will	O
therefore	O
take	O
place	O
by	O
a	O
slow	O
random	B
walk	I
with	O
steps	O
of	O
size	O
so	O
to	O
the	O
initialization	O
strategy	O
can	O
we	O
our	O
slow	O
convergence	O
problem	O
by	O
using	O
initial	O
conditions	O
located	O
all	O
over	O
the	O
state	O
space	O
sadly	O
if	O
we	O
distribute	O
the	O
points	O
fxng	O
widely	O
what	O
we	O
are	O
actually	O
doing	O
is	O
no	O
favouring	O
an	O
initial	O
value	O
of	O
the	O
noise	O
level	O
that	O
is	O
large	O
the	O
random	B
walk	I
of	O
the	O
parameter	O
will	O
thus	O
tend	O
after	O
the	O
drawing	O
of	O
from	O
p	O
j	O
xn	O
always	O
to	O
start	O
from	O
one	O
end	O
of	O
the	O
further	O
reading	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
et	O
al	O
is	O
reviewed	O
in	O
neal	B
this	O
excellent	O
tome	O
also	O
reviews	O
a	O
huge	O
range	O
of	O
other	O
monte	B
carlo	I
methods	I
including	O
the	O
related	O
topics	O
of	O
simulated	B
annealing	B
and	O
free	O
energy	B
estimation	O
further	O
exercises	O
exercise	O
an	O
important	O
detail	O
of	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
is	O
that	O
the	O
simulation	O
of	O
the	O
hamiltonian	O
dynamics	O
while	O
it	O
may	O
be	O
inaccurate	O
must	O
be	O
perfectly	O
reversible	B
in	O
the	O
sense	O
that	O
if	O
the	O
initial	O
condition	O
p	O
goes	O
to	O
then	O
the	O
same	O
simulator	O
must	O
take	O
to	O
and	O
the	O
inaccurate	O
dynamics	O
must	O
conserve	O
state-space	O
volume	B
leapfrog	B
method	O
in	O
algorithm	O
these	O
rules	B
explain	O
why	O
these	O
rules	B
must	O
be	O
and	O
create	O
an	O
example	O
illustrating	O
the	O
problems	O
that	O
arise	O
if	O
they	O
are	O
not	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
methods	I
exercise	O
a	O
multi-state	B
idea	O
for	O
slice	B
sampling	I
investigate	O
the	O
following	O
multi-state	B
method	O
for	O
slice	B
sampling	I
as	O
in	O
skilling	B
s	O
multi-state	B
leapfrog	B
method	O
maintain	O
a	O
set	B
of	O
s	O
state	O
vectors	B
fxsg	O
update	O
one	O
state	O
vector	O
xs	O
by	O
one-dimensional	O
slice	B
sampling	I
in	O
a	O
direction	O
y	O
determined	O
by	O
picking	O
two	O
other	O
state	O
vectors	B
xv	O
and	O
xw	O
at	O
random	B
and	O
setting	O
y	O
xv	O
xw	O
investigate	O
this	O
method	O
on	O
toy	O
problems	O
such	O
as	O
a	O
highly-correlated	O
multivariate	B
gaussian	B
distribution	B
bear	O
in	O
mind	O
that	O
if	O
s	O
is	O
smaller	O
than	O
the	O
number	O
of	O
dimensions	B
n	O
then	O
this	O
method	O
will	O
not	O
be	O
ergodic	B
by	O
itself	O
so	O
it	O
may	O
need	O
to	O
be	O
mixed	O
with	O
other	O
methods	O
are	O
there	O
classes	O
of	O
problems	O
that	O
are	O
better	O
solved	O
by	O
this	O
slice-sampling	O
method	O
than	O
by	O
the	O
standard	O
methods	O
for	O
picking	O
y	O
such	O
as	O
cycling	O
through	O
the	O
coordinate	O
axes	O
or	O
picking	O
u	O
at	O
random	B
from	O
a	O
gaussian	B
distribution	B
solutions	O
xv	O
xs	O
xw	O
solution	O
to	O
exercise	O
consider	O
the	O
spherical	O
gaussian	B
distribution	B
where	O
all	O
components	O
have	O
mean	B
zero	O
and	O
variance	B
in	O
one	O
dimension	O
the	O
nth	O
if	O
n	O
we	O
obtain	O
the	O
proposed	O
coordinate	O
n	O
leapfrogs	O
over	O
n	O
n	O
n	O
n	O
and	O
assuming	O
that	O
in	O
energy	B
contributed	O
by	O
this	O
one	O
dimension	O
will	O
be	O
n	O
are	O
gaussian	B
random	B
variables	O
from	O
n	O
is	O
gaussian	B
from	O
where	O
the	O
change	O
n	O
n	O
n	O
n	O
n	O
n	O
so	O
the	O
typical	B
change	O
in	O
energy	B
is	O
n	O
this	O
positive	O
change	O
is	O
bad	B
news	O
in	O
n	O
dimensions	B
the	O
typical	B
change	O
in	O
energy	B
when	O
a	O
leapfrog	B
move	O
is	O
made	O
at	O
equilibrium	O
is	O
thus	O
the	O
probability	B
of	O
acceptance	O
of	O
the	O
move	O
scales	O
as	O
this	O
implies	O
that	O
skilling	B
s	O
method	O
as	O
described	O
is	O
not	O
in	O
very	O
highdimensional	O
problems	O
at	O
least	O
not	O
once	O
convergence	O
has	O
occurred	O
nevertheless	O
it	O
has	O
the	O
impressive	O
advantage	O
that	O
its	O
convergence	O
properties	O
are	O
independent	O
of	O
the	O
strength	O
of	O
correlations	B
between	O
the	O
variables	O
a	O
property	O
that	O
not	O
even	O
the	O
hamiltonian	B
monte	B
carlo	I
and	O
overrelaxation	B
methods	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
some	O
of	O
the	O
neural	B
network	B
models	O
that	O
we	O
will	O
encounter	O
are	O
related	O
to	O
ising	O
models	O
which	O
are	O
idealized	O
magnetic	O
systems	O
it	O
is	O
not	O
essential	O
to	O
understand	O
the	O
statistical	B
physics	B
of	O
ising	O
models	O
to	O
understand	O
these	O
neural	O
networks	O
but	O
i	O
hope	O
you	O
ll	O
them	O
helpful	O
ising	O
models	O
are	O
also	O
related	O
to	O
several	O
other	O
topics	O
in	O
this	O
book	O
we	O
will	O
use	O
exact	O
tree-based	O
computation	O
methods	O
like	O
those	O
introduced	O
in	O
chapter	O
to	O
evaluate	O
properties	O
of	O
interest	O
in	O
ising	O
models	O
ising	O
models	O
crude	O
models	O
for	O
binary	B
images	B
and	O
ising	O
models	O
relate	O
to	O
two-dimensional	B
constrained	B
channels	O
chapter	O
a	O
two-dimensional	B
bar-code	B
in	O
which	O
a	O
black	B
dot	O
may	O
not	O
be	O
completely	O
surrounded	O
by	O
black	B
dots	O
and	O
a	O
white	B
dot	O
may	O
not	O
be	O
completely	O
surrounded	O
by	O
white	B
dots	O
is	O
similar	O
to	O
an	O
antiferromagnetic	B
ising	B
model	B
at	O
low	O
temperature	B
evaluating	O
the	O
entropy	B
of	O
this	O
ising	B
model	B
is	O
equivalent	O
to	O
evaluating	O
the	O
capacity	B
of	O
the	O
constrained	B
channel	I
for	O
conveying	O
bits	O
if	O
you	O
would	O
like	O
to	O
jog	O
your	O
memory	B
on	O
statistical	B
physics	B
and	O
thermodynamics	B
you	O
might	O
appendix	O
b	O
helpful	O
i	O
also	O
recommend	O
the	O
book	O
by	O
reif	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
an	O
ising	B
model	B
is	O
an	O
array	O
of	O
spins	O
atoms	O
that	O
can	O
take	O
states	O
that	O
are	O
magnetically	O
coupled	O
to	O
each	O
other	O
if	O
one	O
spin	O
is	O
say	O
in	O
the	O
state	O
then	O
it	O
is	O
energetically	O
favourable	O
for	O
its	O
immediate	O
neighbours	O
to	O
be	O
in	O
the	O
same	O
state	O
in	O
the	O
case	O
of	O
a	O
ferromagnetic	B
model	B
and	O
in	O
the	O
opposite	O
state	O
in	O
the	O
case	O
of	O
an	O
antiferromagnet	O
in	O
this	O
chapter	O
we	O
discuss	O
two	O
computational	O
techniques	O
for	O
studying	O
ising	O
models	O
let	O
the	O
state	O
x	O
of	O
an	O
ising	B
model	B
with	O
n	O
spins	O
be	O
a	O
vector	O
in	O
which	O
each	O
component	O
xn	O
takes	O
values	O
or	O
if	O
two	O
spins	O
m	O
and	O
n	O
are	O
neighbours	O
we	O
write	O
n	O
n	O
the	O
coupling	O
between	O
neighbouring	O
spins	O
is	O
j	O
we	O
jmn	O
j	O
if	O
m	O
and	O
n	O
are	O
neighbours	O
and	O
jmn	O
otherwise	O
the	O
energy	B
of	O
a	O
state	O
x	O
is	O
jmnxmxn	O
hxn	O
ex	O
j	O
h	O
where	O
h	O
is	O
the	O
applied	O
if	O
j	O
then	O
the	O
model	B
is	O
ferromagnetic	B
and	O
if	O
j	O
it	O
is	O
antiferromagnetic	B
we	O
ve	O
included	O
the	O
factor	O
of	O
because	O
each	O
pair	O
is	O
counted	O
twice	O
in	O
the	O
sum	O
once	O
as	O
n	O
and	O
once	O
as	O
m	O
at	O
equilibrium	O
at	O
temperature	B
t	O
the	O
probability	B
that	O
the	O
state	O
is	O
x	O
is	O
p	O
j	O
h	O
j	O
h	O
j	O
h	O
where	O
kb	O
is	O
boltzmann	B
s	O
constant	O
and	O
j	O
h	O
j	O
h	O
relevance	O
of	O
ising	O
models	O
ising	O
models	O
are	O
relevant	O
for	O
three	O
reasons	O
ising	O
models	O
are	O
important	O
as	O
models	O
of	O
magnetic	O
systems	O
that	O
have	O
a	O
phase	B
transition	B
the	O
theory	O
of	O
universality	B
in	O
statistical	B
physics	B
shows	O
that	O
all	O
systems	O
with	O
the	O
same	O
dimension	O
two	O
and	O
the	O
same	O
symmetries	O
have	O
equivalent	O
critical	O
properties	O
i	O
e	O
the	O
scaling	B
laws	O
shown	O
by	O
their	O
phase	O
transitions	O
are	O
identical	O
so	O
by	O
studying	O
ising	O
models	O
we	O
can	O
out	O
not	O
only	O
about	O
magnetic	O
phase	O
transitions	O
but	O
also	O
about	O
phase	O
transitions	O
in	O
many	O
other	O
systems	O
second	O
if	O
we	O
generalize	O
the	O
energy	B
function	O
to	O
ex	O
j	O
h	O
jmnxmxn	O
hnxn	O
where	O
the	O
couplings	O
jmn	O
and	O
applied	O
hn	O
are	O
not	O
constant	O
we	O
obtain	O
a	O
family	O
of	O
models	O
known	O
as	O
spin	O
glasses	O
to	O
physicists	O
and	O
as	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
networks	O
or	O
boltzmann	B
machines	O
to	O
the	O
neural	B
network	B
community	O
in	O
some	O
of	O
these	O
models	O
all	O
spins	O
are	O
declared	O
to	O
be	O
neighbours	O
of	O
each	O
other	O
in	O
which	O
case	O
physicists	O
call	O
the	O
system	O
an	O
spin	O
glass	O
and	O
networkers	O
call	O
it	O
a	O
fully	O
connected	O
network	B
third	O
the	O
ising	B
model	B
is	O
also	O
useful	O
as	O
a	O
statistical	B
model	B
in	O
its	O
own	O
right	O
in	O
this	O
chapter	O
we	O
will	O
study	O
ising	O
models	O
using	O
two	O
computational	O
techniques	O
some	O
remarkable	O
relationships	O
in	O
statistical	B
physics	B
we	O
would	O
like	O
to	O
get	O
as	O
much	O
information	B
as	O
possible	O
out	O
of	O
our	O
computations	O
consider	O
for	O
example	O
the	O
heat	B
capacity	B
of	O
a	O
system	O
which	O
is	O
to	O
be	O
c	O
z	O
xx	O
where	O
ex	O
to	O
work	O
out	O
the	O
heat	B
capacity	B
of	O
a	O
system	O
we	O
might	O
naively	O
guess	O
that	O
we	O
have	O
to	O
increase	O
the	O
temperature	B
and	O
measure	O
the	O
energy	B
change	O
heat	B
capacity	B
however	O
is	O
intimately	O
related	O
to	O
energy	B
at	O
constant	O
temperature	B
let	O
s	O
start	O
from	O
the	O
partition	B
function	I
z	O
the	O
mean	B
energy	B
is	O
obtained	O
by	O
with	O
respect	O
to	O
ln	O
z	O
z	O
xx	O
a	O
further	O
spits	O
out	O
the	O
variance	B
of	O
the	O
energy	B
ln	O
z	O
z	O
xx	O
vare	O
but	O
the	O
heat	B
capacity	B
is	O
also	O
the	O
derivative	O
of	O
with	O
respect	O
to	O
temperature	B
ln	O
z	O
ln	O
z	O
so	O
for	O
any	O
system	O
at	O
temperature	B
t	O
c	O
vare	O
kbt	O
vare	O
thus	O
if	O
we	O
can	O
observe	O
the	O
variance	B
of	O
the	O
energy	B
of	O
a	O
system	O
at	O
equilibrium	O
we	O
can	O
estimate	O
its	O
heat	B
capacity	B
i	O
this	O
an	O
almost	O
paradoxical	O
relationship	O
consider	O
a	O
system	O
with	O
a	O
set	B
of	O
states	O
and	O
imagine	O
heating	O
it	O
up	O
at	O
high	O
temperature	B
all	O
states	O
will	O
be	O
equiprobable	O
so	O
the	O
mean	B
energy	B
will	O
be	O
essentially	O
constant	O
and	O
the	O
heat	B
capacity	B
will	O
be	O
essentially	O
zero	O
but	O
on	O
the	O
other	O
hand	O
with	O
all	O
states	O
being	O
equiprobable	O
there	O
will	O
certainly	O
be	O
in	O
energy	B
so	O
how	O
can	O
the	O
heat	B
capacity	B
be	O
related	O
to	O
the	O
the	O
answer	O
is	O
in	O
the	O
words	O
essentially	O
zero	O
above	O
the	O
heat	B
capacity	B
is	O
not	O
quite	O
zero	O
at	O
high	O
temperature	B
it	O
just	O
tends	O
to	O
zero	O
and	O
it	O
tends	O
to	O
zero	O
as	O
vare	O
kbt	O
with	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
the	O
quantity	O
vare	O
tending	O
to	O
a	O
constant	O
at	O
high	O
temperatures	O
this	O
behaviour	O
of	O
the	O
heat	B
capacity	B
of	O
systems	O
at	O
high	O
temperatures	O
is	O
thus	O
very	O
general	O
the	O
factor	O
can	O
be	O
viewed	O
as	O
an	O
accident	O
of	O
history	O
perature	O
scales	O
had	O
been	O
using	O
capacity	B
would	O
be	O
if	O
only	O
temkbt	O
then	O
the	O
of	O
heat	O
c	O
vare	O
and	O
heat	B
capacity	B
and	O
would	O
be	O
identical	O
quantities	O
exercise	O
will	O
call	O
the	O
entropy	B
of	O
a	O
physical	O
system	O
s	O
rather	O
than	O
h	O
while	O
we	O
are	O
in	O
a	O
statistical	B
physics	B
chapter	O
we	O
set	B
kb	O
the	O
entropy	B
of	O
a	O
system	O
whose	O
states	O
are	O
x	O
at	O
temperature	B
t	O
is	O
where	O
show	O
that	O
s	O
pxln	O
px	O
s	O
ln	O
where	O
is	O
the	O
mean	B
energy	B
of	O
the	O
system	O
show	O
that	O
s	O
where	O
the	O
free	O
energy	B
f	O
ln	O
z	O
and	O
kt	O
ising	O
models	O
monte	B
carlo	I
simulation	O
in	O
this	O
section	B
we	O
study	O
two-dimensional	B
planar	O
ising	O
models	O
using	O
a	O
simple	O
gibbs-sampling	O
method	O
starting	O
from	O
some	O
initial	O
state	O
a	O
spin	O
n	O
is	O
selected	O
at	O
random	B
and	O
the	O
probability	B
that	O
it	O
should	O
be	O
given	O
the	O
state	O
of	O
the	O
other	O
spins	O
and	O
the	O
temperature	B
is	O
computed	O
p	O
bn	O
where	O
and	O
bn	O
is	O
the	O
local	O
bn	O
jxm	O
h	O
factor	O
of	O
appears	O
in	O
equation	O
because	O
the	O
two	O
spin	O
states	O
are	O
rather	O
than	O
spin	O
n	O
is	O
set	B
to	O
with	O
that	O
probability	B
and	O
otherwise	O
to	O
then	O
the	O
next	O
spin	O
to	O
update	O
is	O
selected	O
at	O
random	B
after	O
many	O
iterations	O
this	O
procedure	O
converges	O
to	O
the	O
equilibrium	O
distribution	B
an	O
alternative	O
to	O
the	O
gibbs	B
sampling	I
formula	O
is	O
the	O
metropolis	O
algorithm	O
in	O
which	O
we	O
consider	O
the	O
change	O
in	O
energy	B
that	O
results	O
from	O
the	O
chosen	O
spin	O
from	O
its	O
current	O
state	O
xn	O
and	O
adopt	O
this	O
change	O
in	O
with	O
probability	B
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
figure	O
rectangular	B
ising	B
model	B
t	O
figure	O
sample	B
states	O
of	O
rectangular	B
ising	O
models	O
with	O
j	O
at	O
a	O
sequence	B
of	O
temperatures	O
t	O
ising	O
models	O
monte	B
carlo	I
simulation	O
this	O
procedure	O
has	O
roughly	O
double	O
the	O
probability	B
of	O
accepting	O
energetically	O
unfavourable	O
moves	O
so	O
may	O
be	O
a	O
more	O
sampler	O
but	O
at	O
very	O
low	O
temperatures	O
the	O
relative	B
merits	O
of	O
gibbs	B
sampling	I
and	O
the	O
metropolis	O
algorithm	O
may	O
be	O
subtle	O
rectangular	B
geometry	O
i	O
simulated	O
an	O
ising	B
model	B
with	O
the	O
rectangular	B
geometry	O
shown	O
in	O
and	O
with	O
periodic	O
boundary	O
conditions	O
a	O
line	O
between	O
two	O
spins	O
indicates	O
that	O
they	O
are	O
neighbours	O
i	O
set	B
the	O
external	O
h	O
and	O
considered	O
the	O
two	O
cases	O
j	O
which	O
are	O
a	O
ferromagnet	O
and	O
antiferromagnet	O
respectively	O
i	O
started	O
at	O
a	O
large	O
temperature	B
and	O
changed	O
the	O
temperature	B
every	O
i	O
iterations	O
decreasing	O
it	O
gradually	O
to	O
t	O
then	O
increasing	O
it	O
gradually	O
back	O
to	O
a	O
large	O
temperature	B
again	O
this	O
procedure	O
gives	O
a	O
crude	O
check	O
on	O
whether	O
equilibrium	O
has	O
been	O
reached	O
at	O
each	O
temperature	B
if	O
not	O
we	O
d	O
expect	O
to	O
see	O
some	O
hysteresis	O
in	O
the	O
graphs	O
we	O
plot	O
it	O
also	O
gives	O
an	O
idea	O
of	O
the	O
reproducibility	O
of	O
the	O
results	O
if	O
we	O
assume	O
that	O
the	O
two	O
runs	O
with	O
decreasing	O
and	O
increasing	O
temperature	B
are	O
independent	O
of	O
each	O
other	O
at	O
each	O
temperature	B
i	O
recorded	O
the	O
mean	B
energy	B
per	O
spin	O
and	O
the	O
standard	B
deviation	I
of	O
the	O
energy	B
and	O
the	O
mean	B
square	B
value	O
of	O
the	O
magnetization	O
m	O
m	O
n	O
xn	O
xn	O
one	O
tricky	O
decision	O
that	O
has	O
to	O
be	O
made	O
is	O
how	O
soon	O
to	O
start	O
taking	O
these	O
measurements	O
after	O
a	O
new	O
temperature	B
has	O
been	O
established	O
it	O
is	O
to	O
detect	O
equilibrium	O
or	O
even	O
to	O
give	O
a	O
clear	O
of	O
a	O
system	O
s	O
being	O
at	O
equilibrium	O
in	O
chapter	O
we	O
will	O
see	O
a	O
solution	O
to	O
this	O
problem	O
my	O
crude	O
strategy	O
was	O
to	O
let	O
the	O
number	O
of	O
iterations	O
at	O
each	O
temperature	B
i	O
be	O
a	O
few	O
hundred	O
times	O
the	O
number	O
of	O
spins	O
n	O
and	O
to	O
discard	O
the	O
of	O
those	O
iterations	O
with	O
n	O
i	O
found	O
i	O
needed	O
more	O
than	O
iterations	O
to	O
reach	O
equilibrium	O
at	O
any	O
given	O
temperature	B
results	O
for	O
small	O
n	O
with	O
j	O
i	O
simulated	O
an	O
l	O
l	O
grid	O
for	O
l	O
let	O
s	O
have	O
a	O
quick	O
think	O
about	O
what	O
results	O
we	O
expect	O
at	O
low	O
temperatures	O
the	O
system	O
is	O
expected	O
to	O
be	O
in	O
a	O
ground	O
state	O
the	O
rectangular	B
ising	B
model	B
with	O
j	O
has	O
two	O
ground	O
states	O
the	O
all	O
state	O
and	O
the	O
all	O
state	O
the	O
energy	B
per	O
spin	O
of	O
either	O
ground	O
state	O
is	O
at	O
high	O
temperatures	O
the	O
spins	O
are	O
independent	O
all	O
states	O
are	O
equally	O
probable	O
and	O
the	O
energy	B
is	O
expected	O
to	O
around	O
a	O
mean	B
of	O
with	O
a	O
standard	B
deviation	I
proportional	O
to	O
let	O
s	O
look	O
at	O
some	O
results	O
in	O
all	O
temperature	B
t	O
is	O
shown	O
with	O
kb	O
the	O
basic	O
picture	O
emerges	O
with	O
as	O
few	O
as	O
spins	O
top	O
the	O
energy	B
rises	O
monotonically	O
as	O
we	O
increase	O
the	O
number	O
of	O
spins	O
to	O
bottom	O
some	O
new	O
details	O
emerge	O
first	O
as	O
expected	O
the	O
at	O
large	O
temperature	B
decrease	O
as	O
second	O
the	O
at	O
intermediate	O
temperature	B
become	O
relatively	O
bigger	O
this	O
is	O
the	O
signature	O
of	O
a	O
collective	B
phenomenon	O
in	O
this	O
case	O
a	O
phase	B
transition	B
only	O
systems	O
with	O
n	O
show	O
true	O
phase	O
transitions	O
but	O
with	O
n	O
we	O
are	O
getting	O
a	O
hint	O
of	O
the	O
critical	O
figure	O
shows	O
details	O
of	O
the	O
graphs	O
for	O
n	O
and	O
n	O
figure	O
shows	O
a	O
sequence	B
of	O
typical	B
states	O
from	O
the	O
simulation	O
of	O
n	O
spins	O
at	O
a	O
sequence	B
of	O
decreasing	O
temperatures	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
figure	O
monte	B
carlo	I
simulations	O
of	O
rectangular	B
ising	O
models	O
with	O
j	O
mean	B
energy	B
and	O
in	O
energy	B
as	O
a	O
function	O
of	O
temperature	B
mean	B
square	B
magnetization	O
as	O
a	O
function	O
of	O
temperature	B
in	O
the	O
top	O
row	O
n	O
and	O
the	O
bottom	O
n	O
for	O
even	O
larger	O
n	O
see	O
later	O
t	O
figure	O
schematic	O
diagram	O
to	O
explain	O
the	O
meaning	O
of	O
a	O
schottky	B
anomaly	I
the	O
curve	O
shows	O
the	O
heat	B
capacity	B
of	O
two	O
gases	O
as	O
a	O
function	O
of	O
temperature	B
the	O
lower	O
curve	O
shows	O
a	O
normal	B
gas	O
whose	O
heat	B
capacity	B
is	O
an	O
increasing	O
function	O
of	O
temperature	B
the	O
upper	O
curve	O
has	O
a	O
small	O
peak	O
in	O
the	O
heat	B
capacity	B
which	O
is	O
known	O
as	O
a	O
schottky	B
anomaly	I
least	O
in	O
cambridge	O
the	O
peak	O
is	O
produced	O
by	O
the	O
gas	O
having	O
magnetic	O
degrees	B
of	I
freedom	I
with	O
a	O
number	O
of	O
accessible	O
states	O
n	O
mean	B
energy	B
and	O
mean	B
square	B
magnetization	O
y	O
g	O
r	O
e	O
n	O
e	O
y	O
g	O
r	O
e	O
n	O
e	O
temperature	B
temperature	B
n	O
o	O
i	O
t	O
a	O
z	O
i	O
t	O
e	O
n	O
g	O
a	O
m	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
n	O
o	O
i	O
t	O
a	O
z	O
i	O
t	O
e	O
n	O
g	O
a	O
m	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
temperature	B
temperature	B
contrast	O
with	O
schottky	B
anomaly	I
a	O
peak	O
in	O
the	O
heat	B
capacity	B
as	O
a	O
function	O
of	O
temperature	B
occurs	O
in	O
any	O
system	O
that	O
has	O
a	O
number	O
of	O
energy	B
levels	O
a	O
peak	O
is	O
not	O
in	O
itself	O
evidence	B
of	O
a	O
phase	B
transition	B
such	O
peaks	O
were	O
viewed	O
as	O
anomalies	O
in	O
classical	O
thermodynamics	B
since	O
normal	B
systems	O
with	O
numbers	O
of	O
energy	B
levels	O
as	O
a	O
particle	O
in	O
a	O
box	B
have	O
heat	O
capacities	O
that	O
are	O
either	O
constant	O
or	O
increasing	O
functions	B
of	O
temperature	B
in	O
contrast	O
systems	O
with	O
a	O
number	O
of	O
levels	O
produced	O
small	O
blips	O
in	O
the	O
heat	B
capacity	B
graph	B
let	O
us	O
refresh	O
our	O
memory	B
of	O
the	O
simplest	O
such	O
system	O
a	O
two-level	O
system	O
with	O
states	O
x	O
and	O
x	O
the	O
mean	B
energy	B
is	O
and	O
the	O
derivative	O
with	O
respect	O
to	O
is	O
so	O
the	O
heat	B
capacity	B
is	O
c	O
dedt	O
de	O
kbt	O
kbt	O
and	O
the	O
in	O
energy	B
are	O
given	O
by	O
vare	O
ckbt	O
which	O
was	O
evaluated	O
in	O
the	O
heat	B
capacity	B
and	O
are	O
plotted	O
in	O
the	O
take-home	O
message	O
at	O
this	O
point	O
is	O
that	O
whilst	O
schottky	O
anomalies	O
do	O
have	O
a	O
peak	O
in	O
the	O
heat	B
capacity	B
there	O
is	O
no	O
peak	O
in	O
their	O
the	O
variance	B
of	O
the	O
energy	B
simply	O
increases	O
monotonically	O
with	O
temperature	B
to	O
a	O
value	O
proportional	O
to	O
the	O
number	O
of	O
independent	O
spins	O
thus	O
it	O
is	O
a	O
peak	O
in	O
the	O
that	O
is	O
interesting	O
rather	O
than	O
a	O
peak	O
in	O
the	O
heat	B
capacity	B
the	O
ising	B
model	B
has	O
such	O
a	O
peak	O
in	O
its	O
as	O
can	O
be	O
seen	O
in	O
the	O
second	O
row	O
of	O
rectangular	B
ising	B
model	B
with	O
j	O
what	O
do	O
we	O
expect	O
to	O
happen	O
in	O
the	O
case	O
j	O
the	O
ground	O
states	O
of	O
an	O
system	O
are	O
the	O
two	O
checkerboard	B
patterns	O
and	O
they	O
have	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
monte	B
carlo	I
simulation	O
y	O
g	O
r	O
e	O
n	O
e	O
y	O
g	O
r	O
e	O
n	O
e	O
f	O
o	O
d	O
s	O
n	O
o	O
i	O
t	O
a	O
z	O
i	O
t	O
e	O
n	O
g	O
a	O
m	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
t	O
a	O
e	O
h	O
figure	O
detail	O
of	O
monte	B
carlo	I
simulations	O
of	O
rectangular	B
ising	O
models	O
with	O
j	O
mean	B
energy	B
and	O
in	O
energy	B
as	O
a	O
function	O
of	O
temperature	B
fluctuations	O
in	O
energy	B
deviation	O
mean	B
square	B
magnetization	O
heat	B
capacity	B
n	O
n	O
heat	B
capacity	B
vare	O
figure	O
schottky	B
anomaly	I
heat	B
capacity	B
and	O
in	O
energy	B
as	O
a	O
function	O
of	O
temperature	B
for	O
a	O
two-level	O
system	O
with	O
separation	B
and	O
kb	O
temperature	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
energy	B
per	O
spin	O
like	O
the	O
ground	O
states	O
of	O
the	O
j	O
model	B
can	O
this	O
analogy	O
be	O
pressed	O
further	O
a	O
moment	O
s	O
will	O
that	O
the	O
two	O
systems	O
are	O
equivalent	O
to	O
each	O
other	O
under	O
a	O
checkerboard	B
symmetry	O
operation	O
if	O
you	O
take	O
an	O
j	O
system	O
in	O
some	O
state	O
and	O
all	O
the	O
spins	O
that	O
lie	O
on	O
the	O
black	B
squares	O
of	O
an	O
checkerboard	B
and	O
set	B
j	O
then	O
the	O
energy	B
is	O
unchanged	O
magnetization	O
changes	O
of	O
course	O
so	O
all	O
thermodynamic	O
properties	O
of	O
the	O
two	O
systems	O
are	O
expected	O
to	O
be	O
identical	O
in	O
the	O
case	O
of	O
zero	O
applied	O
but	O
there	O
is	O
a	O
subtlety	O
lurking	O
here	O
have	O
you	O
spotted	O
it	O
we	O
are	O
simulating	O
grids	O
with	O
periodic	O
boundary	O
conditions	O
if	O
the	O
size	O
of	O
the	O
grid	O
in	O
any	O
direction	O
is	O
odd	O
then	O
the	O
checkerboard	B
operation	O
is	O
no	O
longer	O
a	O
symmetry	O
operation	O
relating	O
j	O
to	O
j	O
because	O
the	O
checkerboard	B
doesn	O
t	O
match	O
up	O
at	O
the	O
boundaries	O
this	O
means	O
that	O
for	O
systems	O
of	O
odd	O
size	O
the	O
ground	O
state	O
of	O
a	O
system	O
with	O
j	O
will	O
have	O
degeneracy	O
greater	O
than	O
and	O
the	O
energy	B
of	O
those	O
ground	O
states	O
will	O
not	O
be	O
as	O
low	O
as	O
per	O
spin	O
so	O
we	O
expect	O
qualitative	O
between	O
the	O
cases	O
j	O
in	O
odd-sized	O
systems	O
these	O
are	O
expected	O
to	O
be	O
most	O
prominent	O
for	O
small	O
systems	O
the	O
frustrations	O
are	O
introduced	O
by	O
the	O
boundaries	O
and	O
the	O
length	O
of	O
the	O
boundary	O
grows	O
as	O
the	O
square	B
root	O
of	O
the	O
system	O
size	O
so	O
the	O
fractional	O
of	O
this	O
boundary-related	O
frustration	B
on	O
the	O
energy	B
and	O
entropy	B
of	O
the	O
system	O
will	O
decrease	O
as	O
figure	O
compares	O
the	O
energies	O
of	O
the	O
ferromagnetic	B
and	O
antiferromagnetic	B
models	O
with	O
n	O
here	O
the	O
is	O
striking	O
figure	O
the	O
two	O
ground	O
states	O
of	O
a	O
rectangular	B
ising	B
model	B
with	O
j	O
j	O
j	O
figure	O
two	O
states	O
of	O
rectangular	B
ising	O
models	O
with	O
j	O
that	O
have	O
identical	O
energy	B
j	O
y	O
g	O
r	O
e	O
n	O
e	O
j	O
figure	O
monte	B
carlo	I
simulations	O
of	O
rectangular	B
ising	O
models	O
with	O
j	O
and	O
n	O
mean	B
energy	B
and	O
in	O
energy	B
as	O
a	O
function	O
of	O
temperature	B
temperature	B
temperature	B
triangular	O
ising	B
model	B
we	O
can	O
repeat	O
these	O
computations	O
for	O
a	O
triangular	O
ising	B
model	B
do	O
we	O
expect	O
the	O
triangular	O
ising	B
model	B
with	O
j	O
to	O
show	O
physical	O
properties	O
from	O
the	O
rectangular	B
ising	B
model	B
presumably	O
the	O
j	O
model	B
will	O
have	O
broadly	O
similar	O
properties	O
to	O
its	O
rectangular	B
counterpart	O
but	O
the	O
case	O
j	O
is	O
radically	O
from	O
what	O
s	O
gone	O
before	O
think	O
about	O
it	O
there	O
is	O
no	O
unfrustrated	O
ground	O
state	O
in	O
any	O
state	O
there	O
must	O
be	O
frustrations	O
pairs	O
of	O
neighbours	O
who	O
have	O
the	O
same	O
sign	O
as	O
each	O
other	O
unlike	O
the	O
case	O
of	O
the	O
rectangular	B
model	B
with	O
odd	O
size	O
the	O
frustrations	O
are	O
not	O
introduced	O
by	O
the	O
periodic	O
boundary	O
conditions	O
every	O
set	B
of	O
three	O
mutually	O
neighbouring	O
spins	O
must	O
be	O
in	O
a	O
state	O
of	O
frustration	B
as	O
shown	O
in	O
lines	O
show	O
happy	O
couplings	O
which	O
contribute	O
to	O
the	O
energy	B
dashed	O
lines	O
show	O
unhappy	O
couplings	O
which	O
contribute	O
jjj	O
thus	O
we	O
certainly	O
expect	O
behaviour	O
at	O
low	O
temperatures	O
in	O
fact	O
we	O
might	O
expect	O
this	O
system	O
to	O
have	O
a	O
non-zero	O
entropy	B
at	O
absolute	O
zero	O
triangular	O
model	B
violates	O
third	B
law	I
of	I
thermodynamics	B
let	O
s	O
look	O
at	O
some	O
results	O
sample	B
states	O
are	O
shown	O
in	O
and	O
shows	O
the	O
energy	B
and	O
heat	B
capacity	B
for	O
n	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
figure	O
in	O
an	O
antiferromagnetic	B
triangular	O
ising	B
model	B
any	O
three	O
neighbouring	O
spins	O
are	O
frustrated	O
of	O
the	O
eight	O
possible	O
of	O
three	O
spins	O
six	B
have	O
energy	B
and	O
two	O
have	O
energy	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
direct	O
computation	O
of	O
partition	B
function	I
of	O
ising	O
models	O
note	O
how	O
the	O
results	O
for	O
j	O
are	O
there	O
is	O
no	O
peak	O
at	O
all	O
in	O
the	O
standard	B
deviation	I
of	O
the	O
energy	B
in	O
the	O
case	O
j	O
this	O
indicates	O
that	O
the	O
antiferromagnetic	B
system	O
does	O
not	O
have	O
a	O
phase	B
transition	B
to	O
a	O
state	O
with	O
long-range	O
order	O
y	O
g	O
r	O
e	O
n	O
e	O
y	O
g	O
r	O
e	O
n	O
e	O
f	O
o	O
d	O
s	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
t	O
a	O
e	O
h	O
j	O
temperature	B
temperature	B
temperature	B
y	O
g	O
r	O
e	O
n	O
e	O
y	O
g	O
r	O
e	O
n	O
e	O
f	O
o	O
d	O
s	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
t	O
a	O
e	O
h	O
j	O
temperature	B
temperature	B
temperature	B
direct	O
computation	O
of	O
partition	B
function	I
of	O
ising	O
models	O
we	O
now	O
examine	O
a	O
completely	O
approach	O
to	O
ising	O
models	O
the	O
transfer	B
matrix	B
method	I
is	O
an	O
exact	O
and	O
abstract	O
approach	O
that	O
obtains	O
physical	O
properties	O
of	O
the	O
model	B
from	O
the	O
partition	B
function	I
j	O
b	O
j	O
b	O
usual	O
let	O
kb	O
the	O
free	O
energy	B
is	O
given	O
by	O
f	O
where	O
the	O
summation	O
is	O
over	O
all	O
states	O
x	O
and	O
the	O
inverse	O
temperature	B
is	O
ln	O
z	O
the	O
number	O
of	O
states	O
is	O
so	O
direct	O
computation	O
of	O
the	O
partition	B
function	I
is	O
not	O
possible	O
for	O
large	O
n	O
to	O
avoid	O
enumerating	O
all	O
global	O
states	O
explicitly	O
we	O
can	O
use	O
a	O
trick	O
similar	O
to	O
the	O
sumproduct	O
algorithm	O
discussed	O
in	O
chapter	O
we	O
concentrate	O
on	O
models	O
that	O
have	O
the	O
form	O
of	O
a	O
long	B
thin	I
strip	I
of	O
width	O
w	O
with	O
periodic	O
boundary	O
conditions	O
in	O
both	O
directions	O
and	O
we	O
iterate	O
along	O
the	O
length	O
of	O
our	O
model	B
working	O
out	O
a	O
set	B
of	O
partial	B
partition	B
functions	B
at	O
one	O
location	O
l	O
in	O
terms	O
of	O
partial	B
partition	B
functions	B
at	O
the	O
previous	O
location	O
l	O
each	O
iteration	O
involves	O
a	O
summation	O
over	O
all	O
the	O
states	O
at	O
the	O
boundary	O
this	O
operation	O
is	O
exponential	B
in	O
the	O
width	O
of	O
the	O
strip	O
w	O
the	O
clever	O
trick	O
figure	O
monte	B
carlo	I
simulations	O
of	O
triangular	O
ising	O
models	O
with	O
j	O
and	O
n	O
j	O
j	O
d	O
mean	B
energy	B
and	O
in	O
energy	B
as	O
a	O
function	O
of	O
temperature	B
e	O
fluctuations	O
in	O
energy	B
deviation	O
f	O
heat	B
capacity	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
t	O
j	O
t	O
j	O
figure	O
sample	B
states	O
of	O
triangular	O
ising	O
models	O
with	O
j	O
and	O
j	O
high	O
temperatures	O
at	O
the	O
top	O
low	O
at	O
the	O
bottom	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
direct	O
computation	O
of	O
partition	B
function	I
of	O
ising	O
models	O
is	O
to	O
note	O
that	O
if	O
the	O
system	O
is	O
translation-invariant	B
along	O
its	O
length	O
then	O
we	O
need	O
to	O
do	O
only	O
one	O
iteration	O
in	O
order	O
to	O
the	O
properties	O
of	O
a	O
system	O
of	O
any	O
length	O
the	O
computational	O
task	O
becomes	O
the	O
evaluation	O
of	O
an	O
s	O
s	O
matrix	B
where	O
s	O
is	O
the	O
number	O
of	O
microstates	O
that	O
need	O
to	O
be	O
considered	O
at	O
the	O
boundary	O
and	O
the	O
computation	O
of	O
its	O
eigenvalues	O
the	O
eigenvalue	B
of	O
largest	O
magnitude	O
gives	O
the	O
partition	B
function	I
for	O
an	O
thin	O
strip	O
here	O
is	O
a	O
more	O
detailed	O
explanation	O
label	O
the	O
states	O
of	O
the	O
c	O
columns	O
of	O
the	O
thin	O
strip	O
sc	O
with	O
each	O
s	O
an	O
integer	O
from	O
to	O
the	O
rth	O
bit	B
of	O
sc	O
indicates	O
whether	O
the	O
spin	O
in	O
row	O
r	O
column	O
c	O
is	O
up	O
or	O
down	O
the	O
partition	B
function	I
is	O
z	O
xx	O
exp	O
c	O
esc	O
where	O
esc	O
is	O
an	O
appropriately	O
energy	B
and	O
if	O
we	O
want	O
periodic	O
boundary	O
conditions	O
is	O
to	O
be	O
one	O
for	O
e	O
is	O
j	O
xmxn	O
j	O
xmxn	O
j	O
xmxn	O
esc	O
this	O
of	O
the	O
energy	B
has	O
the	O
nice	O
property	O
that	O
the	O
rectangular	B
ising	B
model	B
it	O
a	O
matrix	B
that	O
is	O
symmetric	B
in	O
its	O
two	O
indices	O
sc	O
the	O
factors	O
of	O
are	O
needed	O
because	O
vertical	O
links	O
are	O
counted	O
four	O
times	O
let	O
us	O
figure	O
illustration	O
to	O
help	O
explain	O
the	O
counts	O
all	O
the	O
contributions	O
to	O
the	O
energy	B
in	O
the	O
rectangle	O
the	O
total	O
energy	B
is	O
given	O
by	O
stepping	O
the	O
rectangle	O
along	O
each	O
horizontal	O
bond	O
inside	O
the	O
rectangle	O
is	O
counted	O
once	O
each	O
vertical	O
bond	O
is	O
half-inside	O
the	O
rectangle	O
will	O
be	O
half-inside	O
an	O
adjacent	O
rectangle	O
so	O
half	O
its	O
energy	B
is	O
included	O
in	O
the	O
factor	O
of	O
appears	O
in	O
the	O
second	O
term	O
because	O
m	O
and	O
n	O
both	O
run	O
over	O
all	O
nodes	O
in	O
column	O
c	O
so	O
each	O
bond	O
is	O
visited	O
twice	O
for	O
the	O
state	O
shown	O
here	O
the	O
horizontal	O
bonds	O
contribute	O
to	O
and	O
the	O
vertical	O
bonds	O
contribute	O
on	O
the	O
left	O
and	O
on	O
the	O
right	O
assuming	O
periodic	O
boundary	O
conditions	O
between	O
top	O
and	O
bottom	O
so	O
then	O
continuing	O
from	O
equation	O
c	O
z	O
trace	O
xa	O
a	O
where	O
z	O
becomes	O
dominated	O
by	O
the	O
largest	O
eigenvalue	B
are	O
the	O
eigenvalues	O
of	O
m	O
as	O
the	O
length	O
of	O
the	O
strip	O
c	O
increases	O
z	O
max	O
so	O
the	O
free	O
energy	B
per	O
spin	O
in	O
the	O
limit	O
of	O
an	O
thin	O
strip	O
is	O
given	O
by	O
f	O
ln	O
zw	O
c	O
c	O
ln	O
c	O
ln	O
it	O
s	O
really	O
neat	O
that	O
all	O
the	O
thermodynamic	O
properties	O
of	O
a	O
long	B
thin	I
strip	I
can	O
be	O
obtained	O
from	O
just	O
the	O
largest	O
eigenvalue	B
of	O
this	O
matrix	B
m	O
computations	O
i	O
computed	O
the	O
partition	B
functions	B
of	O
long-thin-strip	O
ising	O
models	O
with	O
the	O
geometries	O
shown	O
in	O
as	O
in	O
the	O
last	O
section	B
i	O
set	B
the	O
applied	O
h	O
to	O
zero	O
and	O
considered	O
the	O
two	O
cases	O
j	O
which	O
are	O
a	O
ferromagnet	O
and	O
antiferromagnet	O
respectively	O
i	O
computed	O
the	O
free	O
energy	B
per	O
spin	O
f	O
j	O
h	O
fn	O
for	O
widths	O
from	O
w	O
to	O
as	O
a	O
function	O
of	O
for	O
h	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
w	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
computational	O
ideas	O
triangular	O
bbbb	O
bbbb	O
bbbb	O
w	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
ising	O
models	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
hh	O
hh	O
hh	O
hh	O
bbbb	O
bbbb	O
rectangular	B
bbbb	O
bbbb	O
bbbb	O
figure	O
two	O
long-thin-strip	O
ising	O
models	O
a	O
line	O
between	O
two	O
spins	O
indicates	O
that	O
they	O
are	O
neighbours	O
the	O
strips	O
have	O
width	O
w	O
and	O
length	O
only	O
the	O
largest	O
eigenvalue	B
is	O
needed	O
there	O
are	O
several	O
ways	O
of	O
getting	O
this	O
quantity	O
for	O
example	O
iterative	O
multiplication	O
of	O
the	O
matrix	B
by	O
an	O
initial	O
vector	O
because	O
the	O
matrix	B
is	O
all	O
positive	O
we	O
know	O
that	O
the	O
principal	O
eigenvector	O
is	O
all	O
positive	O
too	O
theorem	O
so	O
a	O
reasonable	O
initial	O
vector	O
is	O
this	O
iterative	O
procedure	O
may	O
be	O
faster	O
than	O
explicit	O
computation	O
of	O
all	O
eigenvalues	O
i	O
computed	O
them	O
all	O
anyway	O
which	O
has	O
the	O
advantage	O
that	O
we	O
can	O
the	O
free	O
energy	B
of	O
length	O
strips	O
using	O
equation	O
as	O
well	O
as	O
ones	O
ferromagnets	O
of	O
width	O
triangular	O
rectangular	B
y	O
g	O
r	O
e	O
n	O
e	O
e	O
e	O
r	O
f	O
temperature	B
antiferromagnets	O
of	O
width	O
triangular	O
rectangular	B
figure	O
free	O
energy	B
per	O
spin	O
of	O
long-thin-strip	O
ising	O
models	O
note	O
the	O
non-zero	O
gradient	O
at	O
t	O
in	O
the	O
case	O
of	O
the	O
triangular	O
antiferromagnet	O
temperature	B
comments	O
on	O
graphs	O
for	O
large	O
temperatures	O
all	O
ising	O
models	O
should	O
show	O
the	O
same	O
behaviour	O
the	O
free	O
energy	B
is	O
entropy-dominated	O
and	O
the	O
entropy	B
per	O
spin	O
is	O
the	O
mean	B
energy	B
per	O
spin	O
goes	O
to	O
zero	O
the	O
free	O
energy	B
per	O
spin	O
should	O
tend	O
to	O
the	O
free	O
energies	O
are	O
shown	O
in	O
one	O
of	O
the	O
interesting	O
properties	O
we	O
can	O
obtain	O
from	O
the	O
free	O
energy	B
is	O
the	O
degeneracy	O
of	O
the	O
ground	O
state	O
as	O
the	O
temperature	B
goes	O
to	O
zero	O
the	O
boltzmann	B
distribution	B
becomes	O
concentrated	O
in	O
the	O
ground	O
state	O
if	O
the	O
ground	O
state	O
is	O
degenerate	O
there	O
are	O
multiple	O
ground	O
states	O
with	O
identical	O
y	O
p	O
o	O
r	O
t	O
n	O
e	O
triangular-	O
rectangular	B
triangular	O
temperature	B
figure	O
entropies	O
nats	O
of	O
width	O
ising	O
systems	O
as	O
a	O
function	O
of	O
temperature	B
obtained	O
by	O
the	O
free	O
energy	B
curves	O
in	O
the	O
rectangular	B
ferromagnet	O
and	O
antiferromagnet	O
have	O
identical	O
thermal	O
properties	O
for	O
the	O
triangular	O
systems	O
the	O
upper	O
curve	O
denotes	O
the	O
antiferromagnet	O
and	O
the	O
lower	O
curve	O
the	O
ferromagnet	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
direct	O
computation	O
of	O
partition	B
function	I
of	O
ising	O
models	O
rectangular	B
ferromagnet	O
width	O
width	O
triangular-	O
rectangular-	O
triangular	O
triangular	O
ising	O
models	O
width	O
width	O
width	O
width	O
temperature	B
temperature	B
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
t	O
a	O
e	O
h	O
energy	B
then	O
the	O
entropy	B
as	O
t	O
is	O
non-zero	O
we	O
can	O
the	O
entropy	B
from	O
the	O
free	O
energy	B
using	O
s	O
the	O
entropy	B
of	O
the	O
triangular	O
antiferromagnet	O
at	O
absolute	O
zero	O
appears	O
to	O
be	O
about	O
that	O
is	O
about	O
half	O
its	O
high	O
temperature	B
value	O
the	O
mean	B
energy	B
as	O
a	O
function	O
of	O
temperature	B
is	O
plotted	O
in	O
it	O
is	O
evaluated	O
using	O
the	O
identity	O
hei	O
ln	O
figure	O
shows	O
the	O
estimated	O
heat	B
capacity	B
raw	O
derivatives	O
of	O
the	O
mean	B
energy	B
as	O
a	O
function	O
of	O
temperature	B
for	O
the	O
triangular	O
models	O
with	O
widths	O
and	O
figure	O
shows	O
the	O
in	O
energy	B
as	O
a	O
function	O
of	O
temperature	B
all	O
of	O
these	O
should	O
show	O
smooth	O
graphs	O
the	O
roughness	O
of	O
the	O
curves	O
is	O
due	O
to	O
inaccurate	O
numerics	O
the	O
nature	O
of	O
any	O
phase	B
transition	B
is	O
not	O
obvious	O
but	O
the	O
graphs	O
seem	O
compatible	O
with	O
the	O
assertion	O
that	O
the	O
ferromagnet	O
shows	O
and	O
the	O
antiferromagnet	O
does	O
not	O
show	O
a	O
phase	B
transition	B
the	O
pictures	O
of	O
the	O
free	O
energy	B
in	O
give	O
some	O
insight	O
into	O
how	O
we	O
could	O
predict	O
the	O
transition	B
temperature	B
we	O
can	O
see	O
how	O
the	O
two	O
phases	O
of	O
the	O
ferromagnetic	B
systems	O
each	O
have	O
simple	O
free	O
energies	O
a	O
straight	O
sloping	O
line	O
through	O
f	O
t	O
for	O
the	O
high	O
temperature	B
phase	O
and	O
a	O
horizontal	O
line	O
for	O
the	O
low	O
temperature	B
phase	O
slope	O
of	O
each	O
line	O
shows	O
what	O
the	O
entropy	B
per	O
spin	O
of	O
that	O
phase	O
is	O
the	O
phase	B
transition	B
occurs	O
roughly	O
at	O
the	O
intersection	B
of	O
these	O
lines	O
so	O
we	O
predict	O
the	O
transition	B
temperature	B
to	O
be	O
linearly	O
related	O
to	O
the	O
ground	O
state	O
energy	B
e	O
r	O
a	O
v	O
rectangular	B
ferromagnet	O
width	O
width	O
temperature	B
triangular	O
ising	O
models	O
width	O
width	O
width	O
width	O
temperature	B
figure	O
mean	B
energy	B
versus	O
temperature	B
of	O
long	B
thin	I
strip	I
ising	O
models	O
with	O
width	O
compare	O
with	O
figure	O
heat	O
capacities	O
of	O
rectangular	B
model	B
triangular	O
models	O
with	O
widths	O
and	O
denoting	O
ferromagnet	O
and	O
antiferromagnet	O
compare	O
with	O
figure	O
energy	B
variances	O
per	O
spin	O
of	O
rectangular	B
model	B
triangular	O
models	O
with	O
widths	O
and	O
denoting	O
ferromagnet	O
and	O
antiferromagnet	O
compare	O
with	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
ising	O
models	O
comparison	O
with	O
the	O
monte	B
carlo	I
results	O
the	O
agreement	O
between	O
the	O
results	O
of	O
the	O
two	O
experiments	O
seems	O
very	B
good	B
the	O
two	O
systems	O
simulated	O
long	B
thin	I
strip	I
and	O
the	O
periodic	O
square	B
are	O
not	O
quite	O
identical	O
one	O
could	O
a	O
more	O
accurate	O
comparison	O
by	O
all	O
eigenvalues	O
for	O
the	O
strip	O
of	O
width	O
w	O
and	O
computingp	O
to	O
get	O
the	O
partition	B
function	I
of	O
a	O
w	O
w	O
patch	O
exercises	O
exercise	O
what	O
would	O
be	O
the	O
best	O
way	O
to	O
extract	O
the	O
entropy	B
from	O
the	O
monte	B
carlo	I
simulations	O
what	O
would	O
be	O
the	O
best	O
way	O
to	O
obtain	O
the	O
entropy	B
and	O
the	O
heat	B
capacity	B
from	O
the	O
partition	B
function	I
computation	O
exercise	O
an	O
ising	B
model	B
may	O
be	O
generalized	B
to	O
have	O
a	O
coupling	O
jmn	O
between	O
any	O
spins	O
m	O
and	O
n	O
and	O
the	O
value	O
of	O
jmn	O
could	O
be	O
for	O
each	O
m	O
and	O
n	O
in	O
the	O
special	O
case	O
where	O
all	O
the	O
couplings	O
are	O
positive	O
we	O
know	O
that	O
the	O
system	O
has	O
two	O
ground	O
states	O
the	O
all-up	O
and	O
all-down	O
states	O
for	O
a	O
more	O
general	O
setting	O
of	O
jmn	O
it	O
is	O
conceivable	O
that	O
there	O
could	O
be	O
many	O
ground	O
states	O
imagine	O
that	O
it	O
is	O
required	O
to	O
make	O
a	O
spin	B
system	I
whose	O
local	O
minima	O
are	O
a	O
given	O
list	O
of	O
states	O
xs	O
can	O
you	O
think	O
of	O
a	O
way	O
of	O
setting	O
j	O
such	O
that	O
the	O
chosen	O
states	O
are	O
low	O
energy	B
states	O
you	O
are	O
allowed	O
to	O
adjust	O
all	O
the	O
fjmng	O
to	O
whatever	O
values	O
you	O
wish	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
monte	B
carlo	I
sampling	O
the	O
problem	O
with	O
monte	B
carlo	I
methods	I
for	O
high-dimensional	O
problems	O
the	O
most	O
widely	O
used	O
random	B
sampling	O
methods	O
are	O
markov	B
chain	I
monte	B
carlo	I
methods	I
like	O
the	O
metropolis	B
method	I
gibbs	B
sampling	I
and	O
slice	B
sampling	I
the	O
problem	O
with	O
all	O
these	O
methods	O
is	O
this	O
yes	O
a	O
given	O
algorithm	O
can	O
be	O
guaranteed	O
to	O
produce	O
samples	O
from	O
the	O
target	O
density	B
p	O
asymptotically	O
once	O
the	O
chain	O
has	O
converged	O
to	O
the	O
equilibrium	O
distribution	B
but	O
if	O
one	O
runs	O
the	O
chain	O
for	O
too	O
short	O
a	O
time	O
t	O
then	O
the	O
samples	O
will	O
come	O
from	O
some	O
other	O
distribution	B
p	O
for	O
how	O
long	O
must	O
the	O
markov	B
chain	I
be	O
run	O
before	O
it	O
has	O
converged	O
as	O
was	O
mentioned	O
in	O
chapter	O
this	O
question	O
is	O
usually	O
very	O
hard	O
to	O
answer	O
however	O
the	O
pioneering	O
work	O
of	O
propp	B
and	O
wilson	B
allows	O
one	O
for	O
certain	O
chains	O
to	O
answer	O
this	O
very	O
question	O
furthermore	O
propp	B
and	O
wilson	B
show	O
how	O
to	O
obtain	O
exact	O
samples	O
from	O
the	O
target	O
density	B
exact	B
sampling	I
concepts	O
propp	B
and	O
wilson	B
s	O
exact	B
sampling	I
method	O
known	O
as	O
perfect	B
simulation	I
or	O
coupling	B
from	I
the	I
past	I
depends	O
on	O
three	O
ideas	O
coalescence	B
of	O
coupled	O
markov	O
chains	O
first	O
if	O
several	O
markov	O
chains	O
starting	O
from	O
initial	O
conditions	O
share	O
a	O
single	O
random-number	O
generator	O
then	O
their	O
trajectories	O
in	O
state	O
space	O
may	O
coalesce	O
and	O
having	O
coalesced	O
will	O
not	O
separate	O
again	O
if	O
all	O
initial	O
conditions	O
lead	O
to	O
trajectories	O
that	O
coalesce	O
into	O
a	O
single	O
trajectory	O
then	O
we	O
can	O
be	O
sure	O
that	O
the	O
markov	B
chain	I
has	O
forgotten	O
its	O
initial	O
condition	O
figure	O
shows	O
twenty-one	O
markov	O
chains	O
identical	O
to	O
the	O
one	O
described	O
in	O
section	B
which	O
samples	O
from	O
using	O
the	O
metropolis	O
algorithm	O
each	O
of	O
the	O
chains	O
has	O
a	O
initial	O
condition	O
but	O
they	O
are	O
all	O
driven	O
by	O
a	O
single	O
random	B
number	I
generator	I
the	O
chains	O
coalesce	O
after	O
about	O
steps	O
figure	O
shows	O
the	O
same	O
markov	O
chains	O
with	O
a	O
random	B
number	O
seed	O
in	O
this	O
case	O
coalescence	B
does	O
not	O
occur	O
until	O
steps	O
have	O
elapsed	O
shown	O
figure	O
shows	O
similar	O
markov	O
chains	O
each	O
of	O
which	O
has	O
identical	O
proposal	B
density	B
to	O
those	O
in	O
section	B
and	O
but	O
in	O
the	O
proposed	O
move	O
at	O
each	O
step	O
left	O
or	O
right	O
is	O
obtained	O
in	O
the	O
same	O
way	O
by	O
all	O
the	O
chains	O
at	O
any	O
timestep	O
independent	O
of	O
the	O
current	O
state	O
this	O
coupling	O
of	O
the	O
chains	O
changes	O
the	O
statistics	O
of	O
coalescence	B
because	O
two	O
neighbouring	O
paths	O
merge	O
only	O
when	O
a	O
rejection	B
occurs	O
and	O
rejections	O
occur	O
only	O
at	O
the	O
walls	O
this	O
particular	O
markov	B
chain	I
coalescence	B
will	O
occur	O
only	O
when	O
the	O
chains	O
are	O
all	O
in	O
the	O
leftmost	O
state	O
or	O
all	O
in	O
the	O
rightmost	O
state	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
monte	B
carlo	I
sampling	O
figure	O
coalescence	B
the	O
idea	O
behind	O
the	O
exact	B
sampling	I
method	O
time	O
runs	O
from	O
bottom	O
to	O
top	O
in	O
the	O
leftmost	O
panel	O
coalescence	B
occurred	O
within	O
steps	O
coalescence	B
properties	O
are	O
obtained	O
depending	O
on	O
the	O
way	O
each	O
state	O
uses	O
the	O
random	B
numbers	O
it	O
is	O
supplied	O
with	O
two	O
runs	O
of	O
a	O
metropolis	O
simulator	O
in	O
which	O
the	O
random	B
bits	O
that	O
determine	O
the	O
proposed	O
step	O
depend	O
on	O
the	O
current	O
state	O
a	O
random	B
number	O
seed	O
was	O
used	O
in	O
each	O
case	O
in	O
this	O
simulator	O
the	O
random	B
proposal	O
left	O
or	O
right	O
is	O
the	O
same	O
for	O
all	O
states	O
in	O
each	O
panel	O
one	O
of	O
the	O
paths	O
the	O
one	O
starting	O
at	O
location	O
x	O
has	O
been	O
highlighted	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	B
sampling	I
concepts	O
coupling	B
from	I
the	I
past	I
how	O
can	O
we	O
use	O
the	O
coalescence	B
property	O
to	O
an	O
exact	O
sample	B
from	I
the	O
equilibrium	O
distribution	B
of	O
the	O
chain	O
the	O
state	O
of	O
the	O
system	O
at	O
the	O
moment	O
when	O
complete	O
coalescence	B
occurs	O
is	O
not	O
a	O
valid	O
sample	B
from	I
the	O
equilibrium	O
distribution	B
for	O
example	O
in	O
coalescence	B
always	O
occurs	O
when	O
the	O
state	O
is	O
against	O
one	O
of	O
the	O
two	O
walls	O
because	O
trajectories	O
merge	O
only	O
at	O
the	O
walls	O
so	O
sampling	O
forward	O
in	O
time	O
until	O
coalescence	B
occurs	O
is	O
not	O
a	O
valid	O
method	O
the	O
second	O
key	O
idea	O
of	O
exact	B
sampling	I
is	O
that	O
we	O
can	O
obtain	O
exact	O
samples	O
by	O
sampling	O
from	O
a	O
time	O
in	O
the	O
past	O
up	O
to	O
the	O
present	O
if	O
coalescence	B
has	O
occurred	O
the	O
present	O
sample	B
is	O
an	O
unbiased	O
sample	B
from	I
the	O
equilibrium	O
distribution	B
if	O
not	O
we	O
restart	O
the	O
simulation	O
from	O
a	O
time	O
further	O
into	O
the	O
past	O
reusing	O
the	O
same	O
random	B
numbers	O
the	O
simulation	O
is	O
repeated	O
at	O
a	O
sequence	B
of	O
ever	O
more	O
distant	O
times	O
with	O
a	O
doubling	O
of	O
from	O
one	O
run	O
to	O
the	O
next	O
being	O
a	O
convenient	O
choice	O
when	O
coalescence	B
occurs	O
at	O
a	O
time	O
before	O
the	O
present	O
we	O
can	O
record	O
as	O
an	O
exact	O
sample	B
from	I
the	O
equilibrium	O
distribution	B
of	O
the	O
markov	B
chain	I
figure	O
shows	O
two	O
exact	O
samples	O
produced	O
in	O
this	O
way	O
in	O
the	O
leftmost	O
panel	O
of	O
we	O
start	O
twenty-one	O
chains	O
in	O
all	O
possible	O
initial	O
conditions	O
at	O
and	O
run	O
them	O
forward	O
in	O
time	O
coalescence	B
does	O
not	O
occur	O
we	O
restart	O
the	O
simulation	O
from	O
all	O
possible	O
initial	O
conditions	O
at	O
and	O
reset	O
the	O
random	B
number	I
generator	I
in	O
such	O
a	O
way	O
that	O
the	O
random	B
numbers	O
generated	O
at	O
each	O
time	O
t	O
particular	O
from	O
t	O
to	O
t	O
will	O
be	O
identical	O
to	O
what	O
they	O
were	O
in	O
the	O
run	O
notice	O
that	O
the	O
trajectories	O
produced	O
from	O
t	O
to	O
t	O
by	O
these	O
runs	O
that	O
started	O
from	O
are	O
identical	O
to	O
a	O
subset	B
of	O
the	O
trajectories	O
in	O
the	O
simulation	O
with	O
coalescence	B
still	O
does	O
not	O
occur	O
so	O
we	O
double	O
again	O
to	O
this	O
time	O
all	O
the	O
trajectories	O
coalesce	O
and	O
we	O
obtain	O
an	O
exact	O
sample	B
shown	O
by	O
the	O
arrow	O
if	O
we	O
pick	O
an	O
earlier	O
time	O
such	O
as	O
all	O
the	O
trajectories	O
must	O
still	O
end	O
in	O
the	O
same	O
point	O
at	O
t	O
since	O
every	O
trajectory	O
must	O
pass	O
through	O
some	O
state	O
at	O
t	O
and	O
all	O
those	O
states	O
lead	O
to	O
the	O
same	O
point	O
so	O
if	O
we	O
ran	O
the	O
markov	B
chain	I
for	O
an	O
time	O
in	O
the	O
past	O
from	O
any	O
initial	O
condition	O
it	O
would	O
end	O
in	O
the	O
same	O
state	O
figure	O
shows	O
an	O
exact	O
sample	B
produced	O
in	O
the	O
same	O
way	O
with	O
the	O
markov	O
chains	O
of	O
this	O
method	O
called	O
coupling	B
from	I
the	I
past	I
is	O
important	O
because	O
it	O
allows	O
us	O
to	O
obtain	O
exact	O
samples	O
from	O
the	O
equilibrium	O
distribution	B
but	O
as	O
described	O
here	O
it	O
is	O
of	O
little	O
practical	B
use	O
since	O
we	O
are	O
obliged	O
to	O
simulate	O
chains	O
starting	O
in	O
all	O
initial	O
states	O
in	O
the	O
examples	O
shown	O
there	O
are	O
only	O
twenty-one	O
states	O
but	O
in	O
any	O
realistic	O
sampling	O
problem	O
there	O
will	O
be	O
an	O
utterly	O
enormous	O
number	O
of	O
states	O
think	O
of	O
the	O
states	O
of	O
a	O
system	O
of	O
binary	O
spins	O
for	O
example	O
the	O
whole	O
point	O
of	O
introducing	O
monte	B
carlo	I
methods	I
was	O
to	O
try	O
to	O
avoid	O
having	O
to	O
visit	O
all	O
the	O
states	O
of	O
such	O
a	O
system	O
monotonicity	O
having	O
established	O
that	O
we	O
can	O
obtain	O
valid	O
samples	O
by	O
simulating	O
forward	O
from	O
times	O
in	O
the	O
past	O
starting	O
in	O
all	O
possible	O
states	O
at	O
those	O
times	O
the	O
third	O
trick	O
of	O
propp	B
and	O
wilson	B
which	O
makes	O
the	O
exact	B
sampling	I
method	O
useful	O
in	O
practice	O
is	O
the	O
idea	O
that	O
for	O
some	O
markov	O
chains	O
it	O
may	O
be	O
possible	O
to	O
detect	O
coalescence	B
of	O
all	O
trajectories	O
without	O
simulating	O
all	O
those	O
trajectories	O
this	O
property	O
holds	O
for	O
example	O
in	O
the	O
chain	O
of	O
which	O
has	O
the	O
property	O
that	O
two	O
trajectories	O
never	O
cross	O
so	O
if	O
we	O
simply	O
track	O
the	O
two	O
trajectories	O
starting	O
from	O
the	O
leftmost	O
and	O
rightmost	O
states	O
we	O
will	O
know	O
that	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
monte	B
carlo	I
sampling	O
figure	O
coupling	B
from	I
the	I
past	I
the	O
second	O
idea	O
behind	O
the	O
exact	B
sampling	I
method	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	B
sampling	I
concepts	O
figure	O
ordering	O
of	O
states	O
the	O
third	O
idea	O
behind	O
the	O
exact	B
sampling	I
method	O
the	O
trajectories	O
shown	O
here	O
are	O
the	O
left-most	O
and	O
right-most	O
trajectories	O
of	O
in	O
order	O
to	O
establish	O
what	O
the	O
state	O
at	O
time	O
zero	O
is	O
we	O
only	O
need	O
to	O
run	O
simulations	O
from	O
and	O
after	O
which	O
point	O
coalescence	B
occurs	O
two	O
more	O
exact	O
samples	O
from	O
the	O
target	O
density	B
generated	O
by	O
this	O
method	O
and	O
random	B
number	O
seeds	O
the	O
initial	O
times	O
required	O
were	O
and	O
respectively	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
monte	B
carlo	I
sampling	O
compute	O
ai	O
jijxj	O
draw	O
u	O
from	O
if	O
u	O
xi	O
else	O
xi	O
algorithm	O
gibbs	B
sampling	I
coupling	O
method	O
the	O
markov	O
chains	O
are	O
coupled	O
together	O
by	O
having	O
all	O
chains	O
update	O
the	O
same	O
spin	O
i	O
at	O
each	O
time	O
step	O
and	O
having	O
all	O
chains	O
share	O
a	O
common	O
sequence	B
of	O
random	B
numbers	O
u	O
figure	O
an	O
exact	O
sample	B
from	I
the	O
ising	B
model	B
at	O
its	O
critical	O
temperature	B
produced	O
by	O
d	O
b	O
wilson	B
such	O
samples	O
can	O
be	O
produced	O
within	O
seconds	O
on	O
an	O
ordinary	O
computer	B
by	O
exact	B
sampling	I
coalescence	B
of	O
all	O
trajectories	O
has	O
occurred	O
when	O
those	O
two	O
trajectories	O
coalesce	O
figure	O
illustrates	O
this	O
idea	O
by	O
showing	O
only	O
the	O
left-most	O
and	O
right-most	O
trajectories	O
of	O
figure	O
shows	O
two	O
more	O
exact	O
samples	O
from	O
the	O
same	O
equilibrium	O
distribution	B
generated	O
by	O
running	O
the	O
coupling	B
from	I
the	I
past	I
method	O
starting	O
from	O
the	O
two	O
end-states	O
alone	O
in	O
two	O
runs	O
coalesced	O
starting	O
from	O
in	O
it	O
was	O
necessary	O
to	O
try	O
times	O
up	O
to	O
to	O
achieve	O
coalescence	B
exact	B
sampling	I
from	O
interesting	O
distributions	O
in	O
the	O
toy	O
problem	O
we	O
studied	O
the	O
states	O
could	O
be	O
put	O
in	O
a	O
one-dimensional	O
order	O
such	O
that	O
no	O
two	O
trajectories	O
crossed	O
the	O
states	O
of	O
many	O
interesting	O
state	O
spaces	O
can	O
also	O
be	O
put	O
into	O
a	O
partial	B
order	I
and	O
coupled	O
markov	O
chains	O
can	O
be	O
found	O
that	O
respect	O
this	O
partial	B
order	I
example	O
of	O
a	O
partial	B
order	I
on	O
the	O
four	O
possible	O
states	O
of	O
two	O
spins	O
is	O
this	O
and	O
and	O
the	O
states	O
and	O
are	O
not	O
ordered	O
for	O
such	O
systems	O
we	O
can	O
show	O
that	O
coalescence	B
has	O
occurred	O
merely	O
by	O
verifying	O
that	O
coalescence	B
has	O
occurred	O
for	O
all	O
the	O
histories	O
whose	O
initial	O
states	O
were	O
maximal	O
and	O
minimal	O
states	O
of	O
the	O
state	O
space	O
as	O
an	O
example	O
consider	O
the	O
gibbs	B
sampling	I
method	O
applied	O
to	O
a	O
ferromagnetic	B
ising	O
spin	B
system	I
with	O
the	O
partial	B
ordering	O
of	O
states	O
being	O
thus	O
state	O
x	O
is	O
greater	O
than	O
or	O
equal	O
to	O
state	O
y	O
if	O
xi	O
yi	O
for	O
all	O
spins	O
i	O
the	O
maximal	O
and	O
minimal	O
states	O
are	O
the	O
the	O
all-up	O
and	O
all-down	O
states	O
the	O
markov	O
chains	O
are	O
coupled	O
together	O
as	O
shown	O
in	O
algorithm	O
propp	B
and	O
wilson	B
show	O
that	O
exact	O
samples	O
can	O
be	O
generated	O
for	O
this	O
system	O
although	O
the	O
time	O
to	O
exact	O
samples	O
is	O
large	O
if	O
the	O
ising	B
model	B
is	O
below	O
its	O
critical	O
temperature	B
since	O
the	O
gibbs	B
sampling	I
method	O
itself	O
is	O
slowly-mixing	O
under	O
these	O
conditions	O
propp	B
and	O
wilson	B
have	O
improved	O
on	O
this	O
method	O
for	O
the	O
ising	B
model	B
by	O
using	O
a	O
markov	B
chain	I
called	O
the	O
single-bond	O
heat	B
bath	I
algorithm	O
to	O
sample	B
from	I
a	O
related	O
model	B
called	O
the	O
random	B
cluster	I
model	B
they	O
show	O
that	O
exact	O
samples	O
from	O
the	O
random	B
cluster	I
model	B
can	O
be	O
obtained	O
rapidly	O
and	O
can	O
be	O
converted	O
into	O
exact	O
samples	O
from	O
the	O
ising	B
model	B
their	O
ground-breaking	O
paper	O
includes	O
an	O
exact	O
sample	B
from	I
a	O
ising	B
model	B
at	O
its	O
critical	O
temperature	B
a	O
sample	B
for	O
a	O
smaller	O
ising	B
model	B
is	O
shown	O
in	O
a	O
generalization	B
of	O
the	O
exact	B
sampling	I
method	O
for	O
non-attractive	O
distributions	O
the	O
method	O
of	O
propp	B
and	O
wilson	B
for	O
the	O
ising	B
model	B
sketched	O
above	O
can	O
be	O
applied	O
only	O
to	O
probability	B
distributions	O
that	O
are	O
as	O
they	O
call	O
them	O
attractive	O
rather	O
than	O
this	O
term	O
let	O
s	O
say	O
what	O
it	O
means	O
for	O
practical	B
purposes	O
the	O
method	O
can	O
be	O
applied	O
to	O
spin	O
systems	O
in	O
which	O
all	O
the	O
couplings	O
are	O
positive	O
the	O
ferromagnet	O
and	O
to	O
a	O
few	O
special	O
spin	O
systems	O
with	O
negative	O
couplings	O
as	O
we	O
already	O
observed	O
in	O
chapter	O
the	O
rectangular	B
ferromagnet	O
and	O
antiferromagnet	O
are	O
equivalent	O
but	O
it	O
cannot	O
be	O
applied	O
to	O
general	O
spin	O
systems	O
in	O
which	O
some	O
couplings	O
are	O
negative	O
because	O
in	O
such	O
systems	O
the	O
trajectories	O
followed	O
by	O
the	O
all-up	O
and	O
all-down	O
states	O
are	O
not	O
guaranteed	O
to	O
be	O
upper	O
and	O
lower	O
bounds	O
for	O
the	O
set	B
of	O
all	O
trajectories	O
fortunately	O
however	O
we	O
do	O
not	O
need	O
to	O
be	O
so	O
strict	O
it	O
is	O
possible	O
to	O
re-express	O
the	O
propp	B
and	O
wilson	B
algorithm	O
in	O
a	O
way	O
that	O
generalizes	O
to	O
the	O
case	O
of	O
spin	O
systems	O
with	O
negative	O
couplings	O
the	O
idea	O
of	O
the	O
summary	B
state	I
version	O
of	O
exact	B
sampling	I
is	O
still	O
that	O
we	O
keep	O
track	O
of	O
bounds	O
on	O
the	O
set	B
of	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	B
sampling	I
from	O
interesting	O
distributions	O
all	O
trajectories	O
and	O
detect	O
when	O
these	O
bounds	O
are	O
equal	O
so	O
as	O
to	O
exact	O
samples	O
but	O
the	O
bounds	O
will	O
not	O
themselves	O
be	O
actual	O
trajectories	O
and	O
they	O
will	O
not	O
necessarily	O
be	O
tight	O
bounds	O
instead	O
of	O
simulating	O
two	O
trajectories	O
each	O
of	O
which	O
moves	O
in	O
a	O
state	O
space	O
we	O
simulate	O
one	O
trajectory	O
envelope	O
in	O
an	O
augmented	O
state	O
space	O
where	O
the	O
symbol	O
denotes	O
either	O
or	O
we	O
call	O
the	O
state	O
of	O
this	O
augmented	O
system	O
the	O
summary	B
state	I
an	O
example	O
summary	B
state	I
of	O
a	O
six-spin	O
system	O
is	O
this	O
summary	B
state	I
is	O
shorthand	O
for	O
the	O
set	B
of	O
states	O
the	O
update	O
rule	O
at	O
each	O
step	O
of	O
the	O
markov	B
chain	I
takes	O
a	O
single	O
spin	O
enumerates	O
all	O
possible	O
states	O
of	O
the	O
neighbouring	O
spins	O
that	O
are	O
compatible	O
with	O
the	O
current	O
summary	B
state	I
and	O
for	O
each	O
of	O
these	O
local	O
scenarios	O
computes	O
the	O
new	O
value	O
or	O
of	O
the	O
spin	O
using	O
gibbs	B
sampling	I
to	O
a	O
random	B
number	O
u	O
as	O
in	O
algorithm	O
if	O
all	O
these	O
new	O
values	O
agree	O
then	O
the	O
new	O
value	O
of	O
the	O
updated	O
spin	O
in	O
the	O
summary	B
state	I
is	O
set	B
to	O
the	O
unanimous	O
value	O
or	O
otherwise	O
the	O
new	O
value	O
of	O
the	O
spin	O
in	O
the	O
summary	B
state	I
is	O
the	O
initial	O
condition	O
at	O
time	O
is	O
given	O
by	O
setting	O
all	O
the	O
spins	O
in	O
the	O
summary	B
state	I
to	O
which	O
corresponds	O
to	O
considering	O
all	O
possible	O
start	O
in	O
the	O
case	O
of	O
a	O
spin	B
system	I
with	O
positive	O
couplings	O
this	O
summary	B
state	I
simulation	O
will	O
be	O
identical	O
to	O
the	O
simulation	O
of	O
the	O
uppermost	O
state	O
and	O
lowermost	O
states	O
in	O
the	O
style	O
of	O
propp	B
and	O
wilson	B
with	O
coalescence	B
occuring	O
when	O
all	O
the	O
symbols	O
have	O
disappeared	O
the	O
summary	B
state	I
method	O
can	O
be	O
applied	O
to	O
general	O
spin	O
systems	O
with	O
any	O
couplings	O
the	O
only	O
shortcoming	O
of	O
this	O
method	O
is	O
that	O
the	O
envelope	O
may	O
describe	O
an	O
unnecessarily	O
large	O
set	B
of	O
states	O
so	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
summary	B
state	I
algorithm	O
will	O
converge	O
the	O
time	O
for	O
coalescence	B
to	O
be	O
detected	O
may	O
be	O
considerably	O
larger	O
than	O
the	O
actual	O
time	O
taken	O
for	O
the	O
underlying	O
markov	B
chain	I
to	O
coalesce	O
the	O
summary	B
state	I
scheme	O
has	O
been	O
applied	O
to	O
exact	B
sampling	I
in	O
belief	B
networks	O
by	O
harvey	O
and	O
neal	B
and	O
to	O
the	O
triangular	O
antiferromagnetic	B
ising	B
model	B
by	O
childs	O
et	O
al	O
summary	B
state	I
methods	O
were	O
introduced	O
by	O
huber	O
they	O
also	O
go	O
by	O
the	O
names	O
sandwiching	O
methods	O
and	O
bounding	O
chains	O
further	O
reading	O
for	O
further	O
reading	O
impressive	O
pictures	O
of	O
exact	O
samples	O
from	O
other	O
distributions	O
and	O
generalizations	O
of	O
the	O
exact	B
sampling	I
method	O
browse	O
the	O
perfectlyrandom	O
sampling	O
for	O
beautiful	O
exact-sampling	O
demonstrations	O
running	O
live	O
in	O
your	O
web	O
browser	O
see	O
jim	O
propp	B
s	O
other	O
uses	O
for	O
coupling	O
the	O
idea	O
of	O
coupling	O
together	O
markov	O
chains	O
by	O
having	O
them	O
share	O
a	O
random	B
number	I
generator	I
has	O
other	O
applications	O
beyond	O
exact	B
sampling	I
pinto	O
and	O
neal	B
have	O
shown	O
that	O
the	O
accuracy	O
of	O
estimates	O
obtained	O
from	O
a	O
markov	B
chain	I
monte	B
carlo	I
simulation	O
second	O
problem	O
discussed	O
in	O
section	B
using	O
the	O
estimator	B
t	O
xt	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exact	O
monte	B
carlo	I
sampling	O
figure	O
a	O
perfectly	O
random	B
tiling	B
of	O
a	O
hexagon	O
by	O
lozenges	O
provided	O
by	O
j	O
g	O
propp	B
and	O
d	O
b	O
wilson	B
can	O
be	O
improved	O
by	O
coupling	O
the	O
chain	O
of	O
interest	O
which	O
converges	O
to	O
p	O
to	O
a	O
second	O
chain	O
which	O
generates	O
samples	O
from	O
a	O
second	O
simpler	O
distribution	B
q	O
the	O
coupling	O
must	O
be	O
set	B
up	O
in	O
such	O
a	O
way	O
that	O
the	O
states	O
of	O
the	O
two	O
chains	O
are	O
strongly	O
correlated	O
the	O
idea	O
is	O
that	O
we	O
estimate	O
the	O
expectations	O
of	O
a	O
function	O
of	O
interest	O
under	O
p	O
and	O
under	O
q	O
in	O
the	O
normal	B
way	O
and	O
compare	O
the	O
estimate	O
under	O
q	O
with	O
the	O
true	O
value	O
of	O
the	O
expectation	B
under	O
q	O
which	O
we	O
assume	O
can	O
be	O
evaluated	O
exactly	O
if	O
is	O
an	O
overestimate	O
then	O
it	O
is	O
likely	O
that	O
will	O
be	O
an	O
overestimate	O
too	O
the	O
can	O
thus	O
be	O
used	O
to	O
correct	O
exercises	O
exercise	O
is	O
there	O
any	O
relationship	O
between	O
the	O
probability	B
distribution	B
of	O
the	O
time	O
taken	O
for	O
all	O
trajectories	O
to	O
coalesce	O
and	O
the	O
equilibration	O
time	O
of	O
a	O
markov	B
chain	I
prove	O
that	O
there	O
is	O
a	O
relationship	O
or	O
a	O
single	O
chain	O
that	O
can	O
be	O
realized	O
in	O
two	O
ways	O
that	O
have	O
coalescence	B
times	O
exercise	O
imagine	O
that	O
fred	O
ignores	O
the	O
requirement	O
that	O
the	O
random	B
bits	O
used	O
at	O
some	O
time	O
t	O
in	O
every	O
run	O
from	O
increasingly	O
distant	O
times	O
must	O
be	O
identical	O
and	O
makes	O
a	O
coupled-markov-chain	O
simulator	O
that	O
uses	O
fresh	O
random	B
numbers	O
every	O
time	O
is	O
changed	O
describe	O
what	O
happens	O
if	O
fred	O
applies	O
his	O
method	O
to	O
the	O
markov	B
chain	I
that	O
is	O
intended	O
to	O
sample	B
from	I
the	O
uniform	O
distribution	B
over	O
the	O
states	O
and	O
using	O
the	O
metropolis	B
method	I
driven	O
by	O
a	O
random	B
bit	B
source	O
as	O
in	O
exercise	O
investigate	O
the	O
application	O
of	O
perfect	B
sampling	O
to	O
linear	B
regression	B
in	O
holmes	O
and	O
mallick	O
or	O
holmes	O
and	O
denison	O
and	O
try	O
to	O
generalize	O
it	O
exercise	O
the	O
concept	O
of	O
coalescence	B
has	O
many	O
applications	O
some	O
surnames	O
are	O
more	O
frequent	O
than	O
others	O
and	O
some	O
die	B
out	O
altogether	O
make	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
a	O
model	B
of	O
this	O
process	O
how	O
long	O
will	O
it	O
take	O
until	O
everyone	O
has	O
the	O
same	O
surname	O
similarly	O
variability	O
in	O
any	O
particular	O
portion	O
of	O
the	O
human	B
genome	B
forms	O
the	O
basis	O
of	O
forensic	B
dna	B
is	O
inherited	O
like	O
a	O
surname	O
a	O
dna	B
is	O
like	O
a	O
string	O
of	O
surnames	O
should	O
the	O
fact	O
that	O
these	O
surnames	O
are	O
subject	O
to	O
coalescences	O
so	O
that	O
some	O
surnames	O
are	O
by	O
chance	O
more	O
prevalent	O
than	O
others	O
the	O
way	O
in	O
which	O
dna	B
evidence	B
is	O
used	O
in	O
court	O
exercise	O
how	O
can	O
you	O
use	O
a	O
coin	B
to	O
create	O
a	O
random	B
ranking	O
of	O
people	O
construct	O
a	O
solution	O
that	O
uses	O
exact	B
sampling	I
for	O
example	O
you	O
could	O
apply	O
exact	B
sampling	I
to	O
a	O
markov	B
chain	I
in	O
which	O
the	O
coin	B
is	O
repeatedly	O
used	O
alternately	O
to	O
decide	O
whether	O
to	O
switch	O
and	O
second	O
then	O
whether	O
to	O
switch	O
second	O
and	O
third	O
exercise	O
finding	O
the	O
partition	B
function	I
z	O
of	O
a	O
probability	B
distribution	B
is	O
a	O
problem	O
many	O
markov	B
chain	I
monte	B
carlo	I
methods	I
produce	O
valid	O
samples	O
from	O
a	O
distribution	B
without	O
ever	O
out	O
what	O
z	O
is	O
is	O
there	O
any	O
probability	B
distribution	B
and	O
markov	B
chain	I
such	O
that	O
either	O
the	O
time	O
taken	O
to	O
produce	O
a	O
perfect	B
sample	B
or	O
the	O
number	O
of	O
random	B
bits	O
used	O
to	O
create	O
a	O
perfect	B
sample	B
are	O
related	O
to	O
the	O
value	O
of	O
z	O
are	O
there	O
some	O
situations	O
in	O
which	O
the	O
time	O
to	O
coalescence	B
conveys	O
information	B
about	O
z	O
solutions	O
solution	O
to	O
exercise	O
it	O
is	O
perhaps	O
surprising	O
that	O
there	O
is	O
no	O
direct	O
relationship	O
between	O
the	O
equilibration	O
time	O
and	O
the	O
time	O
to	O
coalescence	B
we	O
can	O
prove	O
this	O
using	O
the	O
example	O
of	O
the	O
uniform	O
distribution	B
over	O
the	O
integers	O
a	O
a	O
markov	B
chain	I
that	O
converges	O
to	O
this	O
distribution	B
in	O
exactly	O
one	O
iteration	O
is	O
the	O
chain	O
for	O
which	O
the	O
probability	B
of	O
state	O
given	O
st	O
is	O
the	O
uniform	O
distribution	B
for	O
all	O
st	O
such	O
a	O
chain	O
can	O
be	O
coupled	O
to	O
a	O
random	B
number	I
generator	I
in	O
two	O
ways	O
we	O
could	O
draw	O
a	O
random	B
integer	O
u	O
a	O
and	O
set	B
equal	O
to	O
u	O
regardless	O
of	O
st	O
or	O
we	O
could	O
draw	O
a	O
random	B
integer	O
u	O
a	O
and	O
set	B
equal	O
to	O
u	O
mod	O
method	O
would	O
produce	O
a	O
cohort	O
of	O
trajectories	O
locked	O
together	O
similar	O
to	O
the	O
trajectories	O
in	O
except	O
that	O
no	O
coalescence	B
ever	O
occurs	O
thus	O
while	O
the	O
equilibration	O
times	O
of	O
methods	O
and	O
are	O
both	O
one	O
the	O
coalescence	B
times	O
are	O
respectively	O
one	O
and	O
it	O
seems	O
plausible	O
on	O
the	O
other	O
hand	O
that	O
coalescence	B
time	O
provides	O
some	O
sort	O
of	O
upper	O
bound	B
on	O
equilibration	O
time	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gibbs	B
inequality	B
appeared	O
in	O
equation	O
see	O
also	O
exercise	O
variational	B
methods	I
variational	B
methods	I
are	O
an	O
important	O
technique	O
for	O
the	O
approximation	B
of	O
complicated	O
probability	B
distributions	O
having	O
applications	O
in	O
statistical	B
physics	B
data	O
modelling	B
and	O
neural	O
networks	O
variational	B
free	I
energy	B
minimization	B
one	O
method	O
for	O
approximating	O
a	O
complex	B
distribution	B
in	O
a	O
physical	O
system	O
is	O
mean	B
theory	O
mean	B
theory	O
is	O
a	O
special	O
case	O
of	O
a	O
general	O
variational	B
free	I
energy	B
approach	O
of	O
feynman	B
and	O
bogoliubov	O
which	O
we	O
will	O
now	O
study	O
the	O
key	O
piece	O
of	O
mathematics	O
needed	O
to	O
understand	O
this	O
method	O
is	O
gibbs	B
inequality	B
which	O
we	O
repeat	O
here	O
the	O
relative	B
entropy	B
between	O
two	O
probability	B
distributions	O
qx	O
and	O
p	O
that	O
are	O
over	O
the	O
same	O
alphabet	O
ax	O
is	O
dklqjjp	O
qx	O
log	O
qx	O
p	O
the	O
relative	B
entropy	B
dklqjjp	O
inequality	B
with	O
equality	O
only	O
if	O
q	O
p	O
in	O
general	O
dklqjjp	O
dklpjjq	O
in	O
this	O
chapter	O
we	O
will	O
replace	O
the	O
log	O
by	O
ln	O
and	O
measure	O
the	O
divergence	B
in	O
nats	O
probability	B
distributions	O
in	O
statistical	B
physics	B
in	O
statistical	B
physics	B
one	O
often	O
encounters	O
probability	B
distributions	O
of	O
the	O
form	O
p	O
j	O
j	O
j	O
where	O
for	O
example	O
the	O
state	O
vector	O
is	O
x	O
and	O
ex	O
j	O
is	O
some	O
energy	B
function	O
such	O
as	O
ex	O
j	O
jmnxmxn	O
hnxn	O
the	O
partition	B
function	I
constant	O
is	O
j	O
j	O
the	O
probability	B
distribution	B
of	O
equation	O
is	O
complex	B
not	O
unbearably	O
complex	B
we	O
can	O
after	O
all	O
evaluate	O
ex	O
j	O
for	O
any	O
particular	O
x	O
in	O
a	O
time	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
free	I
energy	B
minimization	B
polynomial	O
in	O
the	O
number	O
of	O
spins	O
but	O
evaluating	O
the	O
normalizing	O
constant	O
j	O
is	O
as	O
we	O
saw	O
in	O
chapter	O
and	O
describing	O
the	O
properties	O
of	O
the	O
probability	B
distribution	B
is	O
also	O
hard	O
knowing	O
the	O
value	O
of	O
ex	O
j	O
at	O
a	O
few	O
arbitrary	O
points	O
x	O
for	O
example	O
gives	O
no	O
useful	O
information	B
about	O
what	O
the	O
average	O
properties	O
of	O
the	O
system	O
are	O
an	O
evaluation	O
of	O
j	O
would	O
be	O
particularly	O
desirable	O
because	O
from	O
z	O
we	O
can	O
derive	O
all	O
the	O
thermodynamic	O
properties	O
of	O
the	O
system	O
variational	B
free	I
energy	B
minimization	B
is	O
a	O
method	O
for	O
approximating	O
the	O
complex	B
distribution	B
p	O
by	O
a	O
simpler	O
ensemble	B
qx	O
that	O
is	O
parameterized	O
by	O
adjustable	O
parameters	B
we	O
adjust	O
these	O
parameters	B
so	O
as	O
to	O
get	O
q	O
to	O
best	O
approximate	O
p	O
in	O
some	O
sense	O
a	O
by-product	O
of	O
this	O
approximation	B
is	O
a	O
lower	O
bound	B
on	O
j	O
the	O
variational	B
free	I
energy	B
the	O
objective	B
function	I
chosen	O
to	O
measure	O
the	O
quality	O
of	O
the	O
approximation	B
is	O
the	O
variational	B
free	I
energy	B
qx	O
ln	O
qx	O
j	O
this	O
expression	O
can	O
be	O
manipulated	O
into	O
a	O
couple	O
of	O
interesting	O
forms	O
qx	O
j	O
hex	O
jiq	O
sq	O
qx	O
ln	O
qx	O
where	O
hex	O
jiq	O
is	O
the	O
average	O
of	O
the	O
energy	B
function	O
under	O
the	O
distribution	B
qx	O
and	O
sq	O
is	O
the	O
entropy	B
of	O
the	O
distribution	B
qx	O
set	B
kb	O
to	O
one	O
in	O
the	O
of	O
s	O
so	O
that	O
it	O
is	O
identical	O
to	O
the	O
of	O
the	O
entropy	B
h	O
in	O
part	O
i	O
second	O
we	O
can	O
use	O
the	O
of	O
p	O
j	O
to	O
write	O
xx	O
qx	O
ln	O
qx	O
p	O
j	O
ln	O
j	O
dklqjjp	O
where	O
f	O
is	O
the	O
true	O
free	O
energy	B
by	O
ln	O
j	O
and	O
dklqjjp	O
is	O
the	O
relative	B
entropy	B
between	O
the	O
approximating	O
distribution	B
qx	O
and	O
the	O
true	O
distribution	B
p	O
j	O
thus	O
by	O
gibbs	B
inequality	B
the	O
variational	B
free	I
energy	B
is	O
bounded	O
below	O
by	O
f	O
and	O
attains	O
this	O
value	O
only	O
for	O
qx	O
p	O
j	O
our	O
strategy	O
is	O
thus	O
to	O
vary	O
in	O
such	O
a	O
way	O
that	O
is	O
minimized	O
the	O
approximating	O
distribution	B
then	O
gives	O
a	O
approximation	B
to	O
the	O
true	O
distribution	B
that	O
may	O
be	O
useful	O
and	O
the	O
value	O
of	O
will	O
be	O
an	O
upper	O
bound	B
for	O
equivalently	O
can	O
the	O
objective	B
function	I
be	O
evaluated	O
is	O
a	O
lower	O
bound	B
for	O
z	O
we	O
have	O
already	O
agreed	O
that	O
the	O
evaluation	O
of	O
various	O
interesting	O
sums	O
over	O
x	O
is	O
intractable	O
for	O
example	O
the	O
partition	B
function	I
z	O
j	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
methods	I
the	O
energy	B
and	O
the	O
entropy	B
heip	O
ex	O
j	O
j	O
z	O
xx	O
s	O
p	O
j	O
ln	O
p	O
j	O
are	O
all	O
presumed	O
to	O
be	O
impossible	O
to	O
evaluate	O
so	O
why	O
should	O
we	O
suppose	O
that	O
this	O
objective	B
function	I
which	O
is	O
also	O
in	O
terms	O
of	O
a	O
sum	O
over	O
all	O
x	O
should	O
be	O
a	O
convenient	O
quantity	O
to	O
deal	O
with	O
well	O
for	O
a	O
range	O
of	O
interesting	O
energy	B
functions	B
and	O
for	O
simple	O
approximating	O
distributions	O
the	O
variational	B
free	I
energy	B
can	O
be	O
evaluated	O
variational	B
free	I
energy	B
minimization	B
for	O
spin	O
systems	O
an	O
example	O
of	O
a	O
tractable	O
variational	B
free	I
energy	B
is	O
given	O
by	O
the	O
spin	B
system	I
whose	O
energy	B
function	O
was	O
given	O
in	O
equation	O
which	O
we	O
can	O
approximate	O
with	O
a	O
separable	O
approximating	O
distribution	B
qx	O
a	O
zq	O
exp	O
xn	O
anxn	O
the	O
variational	B
parameters	B
of	O
the	O
variational	B
free	I
energy	B
are	O
the	O
components	O
of	O
the	O
vector	O
a	O
to	O
evaluate	O
the	O
variational	B
free	I
energy	B
we	O
need	O
the	O
entropy	B
of	O
this	O
distribution	B
sq	O
qx	O
a	O
ln	O
qx	O
a	O
and	O
the	O
mean	B
of	O
the	O
energy	B
hex	O
jiq	O
qx	O
aex	O
j	O
the	O
entropy	B
of	O
the	O
separable	O
approximating	O
distribution	B
is	O
simply	O
the	O
sum	O
of	O
the	O
entropies	O
of	O
the	O
individual	O
spins	O
sq	O
h	O
where	O
qn	O
is	O
the	O
probability	B
that	O
spin	O
n	O
is	O
qn	O
ean	O
ean	O
and	O
h	O
q	O
ln	O
q	O
q	O
ln	O
q	O
the	O
mean	B
energy	B
under	O
q	O
is	O
easy	O
to	O
obtain	O
because	O
pmn	O
jmnxmxn	O
is	O
a	O
sum	O
of	O
terms	O
each	O
involving	O
the	O
product	O
of	O
two	O
independent	O
random	B
variables	O
are	O
no	O
self-couplings	O
so	O
jmn	O
when	O
m	O
n	O
if	O
we	O
the	O
mean	B
value	O
of	O
xn	O
to	O
be	O
which	O
is	O
given	O
by	O
ean	O
ean	O
tanhan	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
free	I
energy	B
minimization	B
for	O
spin	O
systems	O
we	O
obtain	O
hex	O
jiq	O
xx	O
qx	O
jmn	O
jmnxmxn	O
hnxn	O
hn	O
so	O
the	O
variational	B
free	I
energy	B
is	O
given	O
by	O
hex	O
jmn	O
hn	O
h	O
we	O
now	O
consider	O
minimizing	O
this	O
function	O
with	O
respect	O
to	O
the	O
variational	B
parameters	B
a	O
if	O
q	O
the	O
derivative	O
of	O
the	O
entropy	B
is	O
h	O
e	O
ln	O
q	O
q	O
so	O
we	O
obtain	O
jmn	O
qm	O
qm	O
jmn	O
hm	O
am	O
xn	O
this	O
derivative	O
is	O
equal	O
to	O
zero	O
when	O
am	O
xn	O
jmn	O
hm	O
so	O
is	O
extremized	O
at	O
any	O
point	O
that	O
equation	O
and	O
tanhan	O
the	O
variational	B
free	I
energy	B
may	O
be	O
a	O
multimodal	O
function	O
in	O
which	O
case	O
each	O
stationary	O
point	O
minimum	O
or	O
saddle	O
will	O
satisfy	O
equations	O
and	O
one	O
way	O
of	O
using	O
these	O
equations	O
in	O
the	O
case	O
of	O
a	O
system	O
with	O
an	O
arbitrary	O
coupling	O
matrix	B
j	O
is	O
to	O
update	O
each	O
parameter	O
am	O
and	O
the	O
corresponding	O
value	O
of	O
using	O
equation	O
one	O
at	O
a	O
time	O
this	O
asynchronous	O
updating	O
of	O
the	O
parameters	B
is	O
guaranteed	O
to	O
decrease	O
equations	O
and	O
may	O
be	O
recognized	O
as	O
the	O
mean	B
equations	O
for	O
a	O
spin	B
system	I
the	O
variational	B
parameter	O
an	O
may	O
be	O
thought	O
of	O
as	O
the	O
strength	O
of	O
a	O
applied	O
to	O
an	O
isolated	O
spin	O
n	O
equation	O
describes	O
the	O
mean	B
response	O
of	O
spin	O
n	O
and	O
equation	O
describes	O
how	O
the	O
am	O
is	O
set	B
in	O
response	O
to	O
the	O
mean	B
state	O
of	O
all	O
the	O
other	O
spins	O
the	O
variational	B
free	I
energy	B
derivation	B
is	O
a	O
helpful	O
viewpoint	O
for	O
mean	B
theory	O
for	O
two	O
reasons	O
this	O
approach	O
associates	O
an	O
objective	B
function	I
with	O
the	O
mean	B
equations	O
such	O
an	O
objective	B
function	I
is	O
useful	O
because	O
it	O
can	O
help	O
identify	O
alternative	O
dynamical	O
systems	O
that	O
minimize	O
the	O
same	O
function	O
figure	O
the	O
variational	B
free	I
energy	B
of	O
the	O
two-spin	O
system	O
whose	O
energy	B
is	O
ex	O
as	O
a	O
function	O
of	O
the	O
two	O
variational	B
parameters	B
and	O
the	O
inverse-temperature	O
is	O
the	O
function	O
plotted	O
is	O
where	O
notice	O
that	O
for	O
the	O
function	O
is	O
convex	O
with	O
respect	O
to	O
and	O
for	O
it	O
is	O
convex	O
with	O
respect	O
to	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
h	O
h	O
h	O
variational	B
methods	I
figure	O
solutions	O
of	O
the	O
variational	B
free	I
energy	B
extremization	O
problem	O
for	O
the	O
ising	B
model	B
for	O
three	O
applied	O
h	O
horizontal	O
axis	O
temperature	B
t	O
vertical	O
axis	O
magnetization	O
the	O
critical	O
temperature	B
found	O
by	O
mean	B
theory	O
is	O
t	O
mft	O
c	O
the	O
theory	O
is	O
readily	O
generalized	B
to	O
other	O
approximating	O
distributions	O
we	O
can	O
imagine	O
introducing	O
a	O
more	O
complex	B
approximation	B
qx	O
that	O
might	O
for	O
example	O
capture	O
correlations	B
among	O
the	O
spins	O
instead	O
of	O
modelling	B
the	O
spins	O
as	O
independent	O
one	O
could	O
then	O
evaluate	O
the	O
variational	B
free	I
energy	B
and	O
optimize	O
the	O
parameters	B
of	O
this	O
more	O
complex	B
approximation	B
the	O
more	O
degrees	B
of	I
freedom	I
the	O
approximating	O
distribution	B
has	O
the	O
tighter	O
the	O
bound	B
on	O
the	O
free	O
energy	B
becomes	O
however	O
if	O
the	O
complexity	B
of	O
an	O
approximation	B
is	O
increased	O
the	O
evaluation	O
of	O
either	O
the	O
mean	B
energy	B
or	O
the	O
entropy	B
typically	O
becomes	O
more	O
challenging	O
example	O
mean	B
theory	O
for	O
the	O
ferromagnetic	B
ising	B
model	B
in	O
the	O
simple	O
ising	B
model	B
studied	O
in	O
chapter	O
every	O
coupling	O
jmn	O
is	O
equal	O
to	O
j	O
if	O
m	O
and	O
n	O
are	O
neighbours	O
and	O
zero	O
otherwise	O
there	O
is	O
an	O
applied	O
hn	O
h	O
that	O
is	O
the	O
same	O
for	O
all	O
spins	O
a	O
very	O
simple	O
approximating	O
distribution	B
is	O
one	O
with	O
just	O
a	O
single	O
variational	B
parameter	O
a	O
which	O
a	O
separable	O
distribution	B
qx	O
a	O
zq	O
exp	O
xn	O
axn	O
in	O
which	O
all	O
spins	O
are	O
independent	O
and	O
have	O
the	O
same	O
probability	B
qn	O
of	O
being	O
up	O
the	O
mean	B
magnetization	O
is	O
tanha	O
and	O
the	O
equation	O
which	O
the	O
minimum	O
of	O
the	O
variational	B
free	I
energy	B
becomes	O
a	O
h	O
where	O
c	O
is	O
the	O
number	O
of	O
couplings	O
that	O
a	O
spin	O
is	O
involved	O
in	O
c	O
in	O
the	O
case	O
of	O
a	O
rectangular	B
two-dimensional	B
ising	B
model	B
we	O
can	O
solve	O
equations	O
and	O
for	O
numerically	O
in	O
fact	O
it	O
is	O
easiest	O
to	O
vary	O
and	O
solve	O
for	O
and	O
obtain	O
graphs	O
of	O
the	O
free	O
energy	B
minima	O
and	O
maxima	O
as	O
a	O
function	O
of	O
temperature	B
as	O
shown	O
in	O
the	O
solid	O
line	O
shows	O
versus	O
t	O
for	O
the	O
case	O
c	O
j	O
when	O
h	O
there	O
is	O
a	O
pitchfork	B
bifurcation	B
at	O
a	O
critical	O
temperature	B
t	O
mft	O
pitchfork	B
bifurcation	B
is	O
a	O
transition	B
like	O
the	O
one	O
shown	O
by	O
the	O
solid	O
lines	O
in	O
c	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
methods	I
in	O
inference	B
and	O
data	O
modelling	B
from	O
a	O
system	O
with	O
one	O
minimum	O
as	O
a	O
function	O
of	O
a	O
the	O
right	O
to	O
a	O
system	O
the	O
left	O
with	O
two	O
minima	O
and	O
one	O
maximum	O
the	O
maximum	O
is	O
the	O
middle	O
one	O
of	O
the	O
three	O
lines	O
the	O
solid	O
lines	O
look	O
like	O
a	O
pitchfork	O
above	O
this	O
temperature	B
there	O
is	O
only	O
one	O
minimum	O
in	O
the	O
variational	B
free	I
energy	B
at	O
a	O
and	O
this	O
minimum	O
corresponds	O
to	O
an	O
approximating	O
distribution	B
that	O
is	O
uniform	O
over	O
all	O
states	O
below	O
the	O
critical	O
temperature	B
there	O
are	O
two	O
minima	O
corresponding	O
to	O
approximating	O
distributions	O
that	O
are	O
symmetry-broken	O
with	O
all	O
spins	O
more	O
likely	O
to	O
be	O
up	O
or	O
all	O
spins	O
more	O
likely	O
to	O
be	O
down	O
the	O
state	O
persists	O
as	O
a	O
stationary	O
point	O
of	O
the	O
variational	B
free	I
energy	B
but	O
now	O
it	O
is	O
a	O
local	O
maximum	O
of	O
the	O
variational	B
free	I
energy	B
when	O
h	O
there	O
is	O
a	O
global	O
variational	B
free	I
energy	B
minimum	O
at	O
any	O
temperature	B
for	O
a	O
positive	O
value	O
of	O
shown	O
by	O
the	O
upper	O
dotted	O
curves	O
in	O
as	O
long	O
as	O
h	O
jc	O
there	O
is	O
also	O
a	O
second	O
local	O
minimum	O
in	O
the	O
free	O
energy	B
if	O
the	O
temperature	B
is	O
small	O
this	O
second	O
minimum	O
corresponds	O
to	O
a	O
self-preserving	O
state	O
of	O
magnetization	O
in	O
the	O
opposite	O
direction	O
to	O
the	O
applied	O
the	O
temperature	B
at	O
which	O
the	O
second	O
minimum	O
appears	O
is	O
smaller	O
than	O
t	O
mft	O
and	O
when	O
it	O
appears	O
it	O
is	O
accompanied	O
by	O
a	O
saddle	O
point	O
located	O
between	O
the	O
two	O
minima	O
a	O
name	O
given	O
to	O
this	O
type	O
of	O
bifurcation	B
is	O
a	O
saddle-node	O
bifurcation	B
c	O
the	O
variational	B
free	I
energy	B
per	O
spin	O
is	O
given	O
by	O
c	O
j	O
h	O
exercise	O
sketch	O
the	O
variational	B
free	I
energy	B
as	O
a	O
function	O
of	O
its	O
one	O
parameter	O
for	O
a	O
variety	O
of	O
values	O
of	O
the	O
temperature	B
t	O
and	O
the	O
applied	O
h	O
figure	O
reproduces	O
the	O
key	O
properties	O
of	O
the	O
real	O
ising	O
system	O
that	O
for	O
h	O
there	O
is	O
a	O
critical	O
temperature	B
below	O
which	O
the	O
system	O
has	O
longrange	O
order	O
and	O
that	O
it	O
can	O
adopt	O
one	O
of	O
two	O
macroscopic	O
states	O
however	O
by	O
probing	O
a	O
little	O
more	O
we	O
can	O
reveal	O
some	O
inadequacies	O
of	O
the	O
variational	B
approximation	B
to	O
start	O
with	O
the	O
critical	O
temperature	B
t	O
mft	O
is	O
which	O
is	O
nearly	O
a	O
factor	O
of	O
greater	O
than	O
the	O
true	O
critical	O
temperature	B
tc	O
also	O
the	O
variational	B
model	B
has	O
equivalent	O
properties	O
in	O
any	O
number	O
of	O
dimensions	B
including	O
d	O
where	O
the	O
true	O
system	O
does	O
not	O
have	O
a	O
phase	B
transition	B
so	O
the	O
bifurcation	B
at	O
t	O
mft	O
should	O
not	O
be	O
described	O
as	O
a	O
phase	B
transition	B
c	O
c	O
for	O
the	O
case	O
h	O
we	O
can	O
follow	O
the	O
trajectory	O
of	O
the	O
global	O
minimum	O
as	O
a	O
function	O
of	O
and	O
the	O
entropy	B
heat	B
capacity	B
and	O
of	O
the	O
approximating	O
distribution	B
and	O
compare	O
them	O
with	O
those	O
of	O
a	O
real	O
fragment	O
using	O
the	O
matrix	B
method	O
of	O
chapter	O
as	O
shown	O
in	O
one	O
of	O
the	O
biggest	O
is	O
in	O
the	O
in	O
energy	B
the	O
real	O
system	O
has	O
large	O
near	O
the	O
critical	O
temperature	B
whereas	O
the	O
approximating	O
distribution	B
has	O
no	O
correlations	B
among	O
its	O
spins	O
and	O
thus	O
has	O
an	O
energy-variance	O
which	O
scales	O
simply	O
linearly	O
with	O
the	O
number	O
of	O
spins	O
variational	B
methods	I
in	O
inference	B
and	O
data	O
modelling	B
in	O
statistical	B
data	O
modelling	B
we	O
are	O
interested	O
in	O
the	O
posterior	B
probability	B
distribution	B
of	O
a	O
parameter	O
vector	O
w	O
given	O
data	O
d	O
and	O
model	B
assumptions	B
h	O
p	O
j	O
dh	O
p	O
j	O
dh	O
p	O
j	O
whp	O
jh	O
p	O
jh	O
in	O
traditional	O
approaches	O
to	O
model	B
a	O
single	O
parameter	O
vector	O
w	O
is	O
optimized	O
to	O
the	O
mode	O
of	O
this	O
distribution	B
what	O
is	O
really	O
of	O
interest	O
is	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
methods	I
free	O
energy	B
energy	B
figure	O
comparison	O
of	O
approximating	O
distribution	B
s	O
properties	O
with	O
those	O
of	O
a	O
real	O
fragment	O
notice	O
that	O
the	O
variational	B
free	I
energy	B
of	O
the	O
approximating	O
distribution	B
is	O
indeed	O
an	O
upper	O
bound	B
on	O
the	O
free	O
energy	B
of	O
the	O
real	O
system	O
all	O
quantities	O
are	O
shown	O
per	O
spin	O
mean	B
field	O
theory	O
real	O
system	O
mean	B
field	O
theory	O
real	O
system	O
entropy	B
mean	B
field	O
theory	O
real	O
system	O
fluctuations	O
vare	O
mean	B
field	O
theory	O
real	O
system	O
heat	B
capacity	B
dedt	O
mean	B
field	O
theory	O
real	O
system	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
case	O
of	O
an	O
unknown	O
gaussian	B
the	O
whole	O
distribution	B
we	O
may	O
also	O
be	O
interested	O
in	O
its	O
normalizing	O
constant	O
p	O
jh	O
if	O
we	O
wish	O
to	O
do	O
model	B
comparison	I
the	O
probability	B
distribution	B
p	O
j	O
dh	O
is	O
often	O
a	O
complex	B
distribution	B
in	O
a	O
variational	B
approach	O
to	O
inference	B
we	O
introduce	O
an	O
approximating	O
probability	B
distribution	B
over	O
the	O
parameters	B
qw	O
and	O
optimize	O
this	O
distribution	B
varying	O
its	O
own	O
parameters	B
so	O
that	O
it	O
approximates	O
the	O
posterior	O
distribution	B
of	O
the	O
parameters	B
p	O
j	O
dh	O
well	O
proximation	O
is	O
the	O
variational	B
free	I
energy	B
one	O
objective	B
function	I
we	O
may	O
choose	O
to	O
measure	O
the	O
quality	O
of	O
the	O
ap	O
dkw	O
qw	O
ln	O
qw	O
p	O
j	O
whp	O
jh	O
the	O
denominator	O
p	O
j	O
whp	O
jh	O
is	O
within	O
a	O
multiplicative	O
constant	O
the	O
posterior	B
probability	B
p	O
j	O
dh	O
p	O
j	O
whp	O
jhp	O
jh	O
so	O
the	O
variational	B
free	I
energy	B
can	O
be	O
viewed	O
as	O
the	O
sum	O
of	O
ln	O
p	O
jh	O
and	O
the	O
relative	B
entropy	B
between	O
qw	O
and	O
p	O
j	O
dh	O
is	O
bounded	O
below	O
by	O
ln	O
p	O
jh	O
and	O
only	O
attains	O
this	O
value	O
for	O
qw	O
p	O
j	O
dh	O
for	O
certain	O
models	O
and	O
certain	O
approximating	O
distributions	O
this	O
free	O
energy	B
and	O
its	O
derivatives	O
with	O
respect	O
to	O
the	O
approximating	O
distribution	B
s	O
parameters	B
can	O
be	O
evaluated	O
the	O
approximation	B
of	O
posterior	B
probability	B
distributions	O
using	O
variational	B
free	I
energy	B
minimization	B
provides	O
a	O
useful	O
approach	O
to	O
approximating	O
bayesian	B
inference	B
in	O
a	O
number	O
of	O
ranging	O
from	O
neural	O
networks	O
to	O
the	O
decoding	B
of	O
error-correcting	B
codes	I
and	O
van	O
camp	O
hinton	B
and	O
zemel	O
dayan	O
et	O
al	O
neal	B
and	O
hinton	B
mackay	B
the	O
method	O
is	O
sometimes	O
called	O
ensemble	B
learning	B
to	O
contrast	O
it	O
with	O
traditional	O
learning	B
processes	O
in	O
which	O
a	O
single	O
parameter	O
vector	O
is	O
optimized	O
another	O
name	O
for	O
it	O
is	O
variational	B
bayes	B
let	O
us	O
examine	O
how	O
ensemble	B
learning	B
works	O
in	O
the	O
simple	O
case	O
of	O
a	O
gaussian	B
distribution	B
the	O
case	O
of	O
an	O
unknown	O
gaussian	B
approximating	O
the	O
posterior	O
distribution	B
of	O
and	O
we	O
will	O
an	O
approximating	O
ensemble	B
to	O
the	O
posterior	O
distribution	B
that	O
we	O
studied	O
in	O
chapter	O
p	O
jfxngn	O
p	O
j	O
p	O
n	O
p	O
we	O
make	O
the	O
single	O
assumption	O
that	O
the	O
approximating	O
ensemble	B
is	O
separable	O
in	O
the	O
form	O
no	O
restrictions	O
on	O
the	O
functional	O
form	O
of	O
and	O
are	O
made	O
we	O
write	O
down	O
a	O
variational	B
free	I
energy	B
ln	O
p	O
j	O
we	O
can	O
the	O
optimal	B
separable	O
distribution	B
q	O
by	O
considering	O
separately	O
the	O
optimization	B
of	O
over	O
for	O
and	O
then	O
the	O
optimization	B
of	O
for	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
methods	I
figure	O
optimization	B
of	O
an	O
approximating	O
distribution	B
the	O
posterior	O
distribution	B
p	O
jfxng	O
which	O
is	O
the	O
same	O
as	O
that	O
in	O
is	O
shown	O
by	O
solid	O
contours	O
initial	O
condition	O
the	O
approximating	O
distribution	B
contours	O
is	O
an	O
arbitrary	O
separable	O
distribution	B
has	O
been	O
updated	O
using	O
equation	O
has	O
been	O
updated	O
using	O
equation	O
updated	O
again	O
updated	O
again	O
converged	O
approximation	B
iterations	O
the	O
arrows	O
point	O
to	O
the	O
peaks	O
of	O
the	O
two	O
distributions	O
which	O
are	O
at	O
p	O
and	O
q	O
optimization	B
of	O
as	O
a	O
functional	O
of	O
is	O
ln	O
p	O
j	O
lnp	O
z	O
ln	O
where	O
and	O
denote	O
constants	O
that	O
do	O
not	O
depend	O
on	O
the	O
dependence	O
on	O
thus	O
collapses	O
down	O
to	O
a	O
simple	O
dependence	O
on	O
the	O
mean	B
now	O
we	O
can	O
recognize	O
the	O
function	O
as	O
the	O
logarithm	O
of	O
a	O
gaussian	B
identical	O
to	O
the	O
posterior	O
distribution	B
for	O
a	O
particular	O
value	O
of	O
since	O
a	O
relative	B
entropy	B
r	O
q	O
lnqp	O
is	O
minimized	O
by	O
setting	O
q	O
p	O
we	O
can	O
immediately	O
write	O
down	O
the	O
distribution	B
qopt	O
that	O
minimizes	O
for	O
p	O
d	O
qopt	O
where	O
optimization	B
of	O
we	O
represent	O
using	O
the	O
density	B
over	O
as	O
a	O
functional	O
of	O
is	O
additive	O
constants	O
ln	O
p	O
j	O
lnp	O
z	O
ln	O
ln	O
n	O
the	O
prior	B
p	O
transforms	O
to	O
p	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
interlude	O
where	O
the	O
integral	B
over	O
is	O
performed	O
assuming	O
qopt	O
here	O
the	O
expression	O
in	O
square	B
brackets	O
can	O
be	O
recognized	O
as	O
the	O
logarithm	O
of	O
a	O
gamma	B
distribution	B
over	O
see	O
equation	O
giving	O
as	O
the	O
distribution	B
that	O
minimizes	O
for	O
with	O
qopt	O
s	O
and	O
n	O
in	O
these	O
two	O
update	O
rules	B
are	O
applied	O
alternately	O
starting	O
from	O
an	O
arbitrary	O
initial	O
condition	O
the	O
algorithm	O
converges	O
to	O
the	O
optimal	B
approximating	O
ensemble	B
in	O
a	O
few	O
iterations	O
direct	O
solution	O
for	O
the	O
joint	B
optimum	O
in	O
this	O
problem	O
we	O
do	O
not	O
need	O
to	O
resort	O
to	O
iterative	O
computation	O
to	O
the	O
optimal	B
approximating	O
ensemble	B
equations	O
and	O
the	O
optimum	O
implicitly	O
we	O
must	O
simultaneously	O
have	O
the	O
solution	O
is	O
and	O
sn	O
this	O
is	O
similar	O
to	O
the	O
true	O
posterior	O
distribution	B
of	O
which	O
is	O
a	O
gamma	B
distribution	B
with	O
and	O
equation	O
this	O
true	O
posterior	O
also	O
has	O
a	O
mean	B
value	O
of	O
satisfying	O
sn	O
the	O
only	O
is	O
that	O
the	O
approximating	O
distribution	B
s	O
parameter	O
is	O
too	O
large	O
by	O
the	O
approximations	O
given	O
by	O
variational	B
free	I
energy	B
minimization	B
always	O
tend	O
to	O
be	O
more	O
compact	O
than	O
the	O
true	O
distribution	B
in	O
conclusion	O
ensemble	B
learning	B
gives	O
an	O
approximation	B
to	O
the	O
posterior	O
that	O
agrees	O
nicely	O
with	O
the	O
conventional	O
estimators	O
the	O
approximate	O
posterior	O
distribution	B
over	O
is	O
a	O
gamma	B
distribution	B
with	O
mean	B
corresponding	O
to	O
a	O
variance	B
of	O
sn	O
and	O
the	O
approximate	O
posterior	O
distribution	B
over	O
is	O
a	O
gaussian	B
with	O
mean	B
and	O
standard	B
deviation	I
the	O
variational	B
free	I
energy	B
minimization	B
approach	O
has	O
the	O
nice	O
property	O
that	O
it	O
is	O
parameterization-independent	O
it	O
avoids	O
the	O
problem	O
of	O
basisdependence	O
from	O
which	O
map	O
methods	O
and	O
laplace	B
s	O
method	O
a	O
convenient	O
software	B
package	O
for	O
automatic	O
implementation	O
of	O
variational	B
inference	B
in	O
graphical	O
models	O
is	O
vibes	B
et	O
al	O
it	O
plays	O
the	O
same	O
role	O
for	O
variational	B
inference	B
as	O
bugs	B
plays	O
for	O
monte	B
carlo	I
inference	B
interlude	O
one	O
of	O
my	O
students	O
asked	O
how	O
do	O
you	O
ever	O
come	O
up	O
with	O
a	O
useful	O
approximating	O
distribution	B
given	O
that	O
the	O
true	O
distribution	B
is	O
so	O
complex	B
you	O
can	O
t	O
compute	O
it	O
directly	O
let	O
s	O
answer	O
this	O
question	O
in	O
the	O
context	O
of	O
bayesian	B
data	O
modelling	B
let	O
the	O
true	O
distribution	B
of	O
interest	O
be	O
the	O
posterior	B
probability	B
distribution	B
over	O
a	O
set	B
of	O
parameters	B
x	O
p	O
d	O
a	O
standard	O
data	O
modelling	B
practice	O
is	O
to	O
a	O
single	O
setting	O
of	O
the	O
parameters	B
for	O
example	O
by	O
the	O
maximum	O
of	O
the	O
likelihood	B
function	O
p	O
j	O
x	O
or	O
of	O
the	O
posterior	O
distribution	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
methods	I
one	O
interpretation	O
of	O
this	O
standard	O
practice	O
is	O
that	O
the	O
full	O
description	O
of	O
our	O
knowledge	O
about	O
x	O
p	O
d	O
is	O
being	O
approximated	O
by	O
a	O
delta-function	O
a	O
probability	B
distribution	B
concentrated	O
on	O
from	O
this	O
perspective	O
any	O
approximating	O
distribution	B
qx	O
no	O
matter	O
how	O
crummy	O
it	O
is	O
has	O
to	O
be	O
an	O
improvement	O
on	O
the	O
spike	O
produced	O
by	O
the	O
standard	O
method	O
so	O
even	O
if	O
we	O
use	O
only	O
a	O
simple	O
gaussian	B
approximation	B
we	O
are	O
doing	O
well	O
we	O
now	O
study	O
an	O
application	O
of	O
the	O
variational	B
approach	O
to	O
a	O
realistic	O
example	O
data	O
clustering	B
k-means	B
clustering	B
and	O
the	O
expectationmaximization	O
algo	O
rithm	O
as	O
a	O
variational	B
method	O
in	O
chapter	O
we	O
introduced	O
the	O
soft	B
k-means	B
clustering	B
algorithm	O
version	O
in	O
chapter	O
we	O
introduced	O
versions	O
and	O
of	O
this	O
algorithm	O
and	O
motivated	O
the	O
algorithm	O
as	O
a	O
maximum	B
likelihood	B
algorithm	O
k-means	B
clustering	B
is	O
an	O
example	O
of	O
an	O
expectationmaximization	O
algorithm	O
with	O
the	O
two	O
steps	O
which	O
we	O
called	O
assignment	O
and	O
update	O
being	O
known	O
as	O
the	O
e-step	O
and	O
the	O
m-step	O
respectively	O
we	O
now	O
give	O
a	O
more	O
general	O
view	O
of	O
k-means	B
clustering	B
due	O
to	O
neal	B
and	O
hinton	B
in	O
which	O
the	O
algorithm	O
is	O
shown	O
to	O
optimize	O
a	O
variational	B
objective	B
function	I
neal	B
and	O
hinton	B
s	O
derivation	B
applies	O
to	O
any	O
em	B
algorithm	I
the	O
probability	B
of	O
everything	O
let	O
the	O
parameters	B
of	O
the	O
mixture	O
model	B
the	O
means	O
standard	O
deviations	O
and	O
weights	O
be	O
denoted	O
by	O
for	O
each	O
data	O
point	O
there	O
is	O
a	O
missing	O
variable	O
known	O
as	O
a	O
latent	B
variable	I
the	O
class	O
label	O
kn	O
for	O
that	O
point	O
the	O
probability	B
of	O
everything	O
given	O
our	O
assumed	O
model	B
h	O
is	O
p	O
kngn	O
jh	O
p	O
jh	O
n	O
j	O
kn	O
j	O
the	O
posterior	B
probability	B
of	O
everything	O
given	O
the	O
data	O
is	O
proportional	O
to	O
the	O
probability	B
of	O
everything	O
p	O
jfxngn	O
p	O
kngn	O
p	O
jh	O
jh	O
we	O
now	O
approximate	O
this	O
posterior	O
distribution	B
by	O
a	O
separable	O
distribution	B
and	O
a	O
variational	B
free	I
energy	B
in	O
the	O
usual	O
way	O
qkfkngn	O
q	O
q	O
ln	O
q	O
qkfkngn	O
p	O
kngn	O
jh	O
q	O
xfkngz	O
qkfkngn	O
is	O
bounded	O
below	O
by	O
minus	O
the	O
evidence	B
ln	O
p	O
jh	O
we	O
can	O
now	O
make	O
an	O
iterative	O
algorithm	O
with	O
an	O
assignment	O
step	O
and	O
an	O
update	O
step	O
in	O
the	O
assignment	O
step	O
qkfkngn	O
in	O
the	O
update	O
step	O
q	O
is	O
adjusted	O
to	O
reduce	O
for	O
q	O
is	O
adjusted	O
to	O
reduce	O
for	O
qk	O
if	O
we	O
wish	O
to	O
obtain	O
exactly	O
the	O
soft	B
k-means	O
algorithm	O
we	O
impose	O
a	O
is	O
constrained	B
to	O
be	O
further	O
constraint	O
on	O
our	O
approximating	O
distribution	B
q	O
a	O
delta	B
function	I
centred	O
on	O
a	O
point	B
estimate	I
of	O
q	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
methods	I
other	O
than	O
free	O
energy	B
minimization	B
upper	O
bound	B
lower	O
bound	B
h	O
e	O
where	O
figure	O
illustration	O
of	O
the	O
jaakkolajordan	O
variational	B
method	O
upper	O
and	O
lower	O
bounds	O
on	O
the	O
logistic	O
function	O
line	O
ga	O
these	O
upper	O
and	O
lower	O
bounds	O
are	O
exponential	B
or	O
gaussian	B
functions	B
of	O
a	O
and	O
so	O
easier	O
to	O
integrate	O
over	O
the	O
graph	B
shows	O
the	O
sigmoid	B
function	O
and	O
upper	O
and	O
lower	O
bounds	O
with	O
and	O
large	O
integral	B
r	O
q	O
unfortunately	O
this	O
distribution	B
contributes	O
to	O
the	O
variational	B
free	I
energy	B
an	O
so	O
we	O
d	O
better	O
leave	O
that	O
term	O
out	O
of	O
treating	O
it	O
as	O
an	O
additive	O
constant	O
a	O
delta	B
function	I
q	O
is	O
not	O
a	O
good	B
idea	O
if	O
our	O
aim	O
is	O
to	O
minimize	O
moving	O
on	O
our	O
aim	O
is	O
to	O
derive	O
the	O
soft	B
k-means	O
algorithm	O
ln	O
q	O
exercise	O
show	O
that	O
given	O
q	O
the	O
optimal	B
qk	O
in	O
the	O
sense	O
of	O
minimizing	O
is	O
a	O
separable	O
distribution	B
in	O
which	O
the	O
probability	B
that	O
kn	O
k	O
is	O
given	O
by	O
the	O
responsibility	B
rn	O
k	O
exercise	O
show	O
that	O
given	O
a	O
separable	O
qk	O
as	O
described	O
above	O
the	O
optimal	B
in	O
the	O
sense	O
of	O
minimizing	O
is	O
obtained	O
by	O
the	O
update	O
step	O
of	O
the	O
soft	B
k-means	O
algorithm	O
a	O
uniform	O
prior	B
on	O
exercise	O
we	O
can	O
instantly	O
improve	O
on	O
the	O
large	O
value	O
of	O
achieved	O
by	O
soft	B
k-means	B
clustering	B
by	O
allowing	O
q	O
to	O
be	O
a	O
more	O
general	O
distribution	B
than	O
a	O
delta-function	O
derive	O
an	O
update	O
step	O
in	O
which	O
q	O
is	O
allowed	O
to	O
be	O
a	O
separable	O
distribution	B
a	O
product	O
of	O
and	O
discuss	O
whether	O
this	O
generalized	B
algorithm	O
still	O
from	O
soft	B
k-means	O
s	O
kaboom	B
problem	O
where	O
the	O
algorithm	O
glues	O
an	O
evershrinking	O
gaussian	B
to	O
one	O
data	O
point	O
sadly	O
while	O
it	O
sounds	O
like	O
a	O
promising	O
generalization	B
of	O
the	O
algorithm	O
to	O
be	O
a	O
non-delta-function	O
and	O
the	O
kaboom	B
problem	O
goes	O
to	O
allow	O
q	O
away	O
other	O
artefacts	O
can	O
arise	O
in	O
this	O
approximate	O
inference	B
method	O
involving	O
local	O
minima	O
of	O
for	O
further	O
reading	O
see	O
mackay	B
variational	B
methods	I
other	O
than	O
free	O
energy	B
minimization	B
there	O
are	O
other	O
strategies	O
for	O
approximating	O
a	O
complicated	O
distribution	B
p	O
in	O
addition	O
to	O
those	O
based	O
on	O
minimizing	O
the	O
relative	B
entropy	B
between	O
an	O
approximating	O
distribution	B
q	O
and	O
p	O
one	O
approach	O
pioneered	O
by	O
jaakkola	B
and	O
jordan	B
is	O
to	O
create	O
adjustable	O
upper	O
and	O
lower	O
bounds	O
qu	O
and	O
ql	O
to	O
p	O
as	O
illustrated	O
in	O
these	O
bounds	O
are	O
unnormalized	O
densities	O
are	O
parameterized	O
by	O
variational	B
parameters	B
which	O
are	O
adjusted	O
in	O
order	O
to	O
obtain	O
the	O
tightest	O
possible	O
the	O
lower	O
bound	B
can	O
be	O
adjusted	O
to	O
maximize	O
qlx	O
xx	O
and	O
the	O
upper	O
bound	B
can	O
be	O
adjusted	O
to	O
minimize	O
qu	O
xx	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
variational	B
methods	I
using	O
the	O
normalized	O
versions	O
of	O
the	O
optimized	O
bounds	O
we	O
then	O
compute	O
approximations	O
to	O
the	O
predictive	O
distributions	O
further	O
reading	O
on	O
such	O
methods	O
can	O
be	O
found	O
in	O
the	O
references	O
and	O
jordan	B
jaakkola	B
and	O
jordan	B
jaakkola	B
and	O
jordan	B
gibbs	B
and	O
mackay	B
further	O
reading	O
the	O
bethe	O
and	O
kikuchi	O
free	O
energies	O
in	O
chapter	O
we	O
discussed	O
the	O
sumproduct	O
algorithm	O
for	O
functions	B
of	O
the	O
factor-graph	O
form	O
if	O
the	O
factor	B
graph	B
is	O
tree-like	O
the	O
sumproduct	O
algorithm	O
converges	O
and	O
correctly	O
computes	O
the	O
marginal	B
function	O
of	O
any	O
variable	O
xn	O
and	O
can	O
also	O
yield	O
the	O
joint	B
marginal	B
function	O
of	O
subsets	O
of	O
variables	O
that	O
appear	O
in	O
a	O
common	O
factor	O
such	O
as	O
xm	O
the	O
sumproduct	O
algorithm	O
may	O
also	O
be	O
applied	O
to	O
factor	O
graphs	O
that	O
are	O
not	O
tree-like	O
if	O
the	O
algorithm	O
converges	O
to	O
a	O
point	O
it	O
has	O
been	O
shown	O
that	O
that	O
point	O
is	O
a	O
stationary	O
point	O
a	O
minimum	O
of	O
a	O
function	O
of	O
the	O
messages	O
called	O
the	O
kikuchi	B
free	I
energy	B
in	O
the	O
special	O
case	O
where	O
all	O
factors	O
in	O
factor	B
graph	B
are	O
functions	B
of	O
one	O
or	O
two	O
variables	O
the	O
kikuchi	B
free	I
energy	B
is	O
called	O
the	O
bethe	B
free	I
energy	B
for	O
articles	O
on	O
this	O
idea	O
and	O
new	O
approximate	O
inference	B
algorithms	B
motivated	O
by	O
it	O
see	O
yedidia	B
yedidia	B
et	O
al	O
welling	O
and	O
teh	O
yuille	O
yedidia	B
et	O
al	O
yedidia	B
et	O
al	O
further	O
exercises	O
exercise	O
this	O
exercise	O
explores	O
the	O
assertion	O
made	O
above	O
that	O
the	O
approximations	O
given	O
by	O
variational	B
free	I
energy	B
minimization	B
always	O
tend	O
to	O
be	O
more	O
compact	O
than	O
the	O
true	O
distribution	B
consider	O
a	O
two	O
dimensional	O
gaussian	B
distribution	B
p	O
with	O
axes	O
aligned	O
with	O
the	O
directions	O
and	O
let	O
the	O
variances	O
in	O
these	O
two	O
directions	O
be	O
what	O
is	O
the	O
optimal	B
variance	B
if	O
this	O
distribution	B
is	O
approximated	O
by	O
a	O
spherical	O
gaussian	B
with	O
variance	B
q	O
optimized	O
by	O
variational	B
free	I
energy	B
minimization	B
if	O
we	O
instead	O
optimized	O
the	O
objective	B
function	I
and	O
p	O
qx	O
g	O
dx	O
p	O
ln	O
what	O
would	O
be	O
the	O
optimal	B
value	O
of	O
sketch	O
a	O
contour	O
of	O
the	O
true	O
distribution	B
p	O
and	O
the	O
two	O
approximating	O
distributions	O
in	O
the	O
case	O
that	O
in	O
general	O
it	O
is	O
not	O
possible	O
to	O
evaluate	O
the	O
objective	B
function	I
g	O
because	O
integrals	O
under	O
the	O
true	O
distribution	B
p	O
are	O
usually	O
intractable	O
exercise	O
what	O
do	O
you	O
think	O
of	O
the	O
idea	O
of	O
using	O
a	O
variational	B
method	O
to	O
optimize	O
an	O
approximating	O
distribution	B
q	O
which	O
we	O
then	O
use	O
as	O
a	O
proposal	B
density	B
for	O
importance	B
sampling	I
exercise	O
the	O
relative	B
entropy	B
or	O
kullbackleibler	O
divergence	B
between	O
two	O
probability	B
distributions	O
p	O
and	O
q	O
and	O
state	O
gibbs	B
inequality	B
consider	O
the	O
problem	O
of	O
approximating	O
a	O
joint	B
distribution	B
p	O
y	O
by	O
a	O
separable	O
distribution	B
qx	O
y	O
qxxqy	O
show	O
that	O
if	O
the	O
objec	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
tive	O
function	O
for	O
this	O
approximation	B
is	O
gqx	O
qy	O
p	O
y	O
p	O
y	O
qx	O
that	O
the	O
minimal	O
value	O
of	O
g	O
is	O
achieved	O
when	O
qx	O
and	O
qy	O
are	O
equal	O
to	O
the	O
marginal	B
distributions	O
over	O
x	O
and	O
y	O
now	O
consider	O
the	O
alternative	O
objective	B
function	I
f	O
qy	O
qxxqy	O
qxxqy	O
p	O
y	O
the	O
probability	B
distribution	B
p	O
y	O
shown	O
in	O
the	O
margin	O
is	O
to	O
be	O
approximated	O
by	O
a	O
separable	O
distribution	B
qx	O
y	O
qxxqy	O
state	O
the	O
value	O
of	O
f	O
qy	O
if	O
qx	O
and	O
qy	O
are	O
set	B
to	O
the	O
marginal	B
distributions	O
over	O
x	O
and	O
y	O
show	O
that	O
f	O
qy	O
has	O
three	O
distinct	O
minima	O
identify	O
those	O
minima	O
and	O
evaluate	O
f	O
at	O
each	O
of	O
them	O
solutions	O
solution	O
to	O
exercise	O
we	O
need	O
to	O
know	O
the	O
relative	B
entropy	B
between	O
two	O
one-dimensional	O
gaussian	B
distributions	O
p	O
y	O
x	O
y	O
z	O
dx	O
normalx	O
ln	O
z	O
dx	O
normalx	O
p	O
ln	O
p	O
q	O
q	O
normalx	O
normalx	O
q	O
p	O
so	O
if	O
we	O
approximate	O
p	O
whose	O
variances	O
are	O
are	O
both	O
q	O
we	O
and	O
by	O
q	O
whose	O
variances	O
f	O
q	O
ln	O
q	O
q	O
ln	O
q	O
q	O
which	O
is	O
zero	O
when	O
q	O
d	O
d	O
q	O
f	O
q	O
q	O
thus	O
we	O
set	B
the	O
approximating	O
distribution	B
s	O
inverse	O
variance	B
to	O
the	O
mean	B
inverse	O
variance	B
of	O
the	O
target	O
distribution	B
p	O
in	O
the	O
case	O
and	O
we	O
obtain	O
which	O
is	O
just	O
a	O
factor	O
of	O
larger	O
than	O
pretty	O
much	O
independent	O
of	O
the	O
value	O
of	O
the	O
larger	O
standard	B
deviation	I
variational	B
free	I
energy	B
minimization	B
typically	O
leads	O
to	O
approximating	O
distributions	O
whose	O
length	O
scales	O
match	O
the	O
shortest	O
length	O
scale	O
of	O
the	O
target	O
distribution	B
the	O
approximating	O
distribution	B
might	O
be	O
viewed	O
as	O
too	O
compact	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
in	O
contrast	O
if	O
we	O
use	O
the	O
objective	B
function	I
g	O
then	O
we	O
q	O
ln	O
q	O
q	O
ln	O
q	O
q	O
constant	O
where	O
the	O
constant	O
depends	O
on	O
and	O
only	O
d	O
d	O
ln	O
q	O
g	O
q	O
q	O
variational	B
methods	I
figure	O
two	O
separable	O
gaussian	B
approximations	O
lines	O
to	O
a	O
bivariate	B
gaussian	B
distribution	B
line	O
the	O
approximation	B
that	O
minimizes	O
the	O
variational	B
free	I
energy	B
the	O
approximation	B
that	O
minimizes	O
the	O
objective	B
function	I
g	O
in	O
each	O
the	O
lines	O
show	O
the	O
contours	O
at	O
which	O
xtax	O
where	O
a	O
is	O
the	O
inverse	O
covariance	B
matrix	B
of	O
the	O
gaussian	B
which	O
is	O
zero	O
when	O
q	O
thus	O
we	O
set	B
the	O
approximating	O
distribution	B
s	O
variance	B
to	O
the	O
mean	B
variance	B
of	O
the	O
target	O
distribution	B
p	O
factor	O
of	O
smaller	O
than	O
independent	O
of	O
the	O
value	O
of	O
the	O
two	O
approximations	O
are	O
shown	O
to	O
scale	O
in	O
in	O
the	O
case	O
and	O
we	O
obtain	O
which	O
is	O
just	O
a	O
solution	O
to	O
exercise	O
the	O
best	O
possible	O
variational	B
approximation	B
is	O
of	O
course	O
the	O
target	O
distribution	B
p	O
assuming	O
that	O
this	O
is	O
not	O
possible	O
a	O
good	B
variational	B
approximation	B
is	O
more	O
compact	O
than	O
the	O
true	O
distribution	B
in	O
contrast	O
a	O
good	B
sampler	O
is	O
more	O
heavy	O
tailed	O
than	O
the	O
true	O
distribution	B
an	O
over-compact	O
distribution	B
would	O
be	O
a	O
lousy	O
sampler	O
with	O
a	O
large	O
variance	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
sk	O
g	O
yn	O
figure	O
error-correcting	B
codes	I
as	O
latent	B
variable	I
models	I
the	O
k	O
latent	O
variables	O
are	O
the	O
independent	O
source	O
bits	O
sk	O
these	O
give	O
rise	O
to	O
the	O
observables	O
via	O
the	O
generator	B
matrix	B
g	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	B
latent	B
variable	I
models	I
many	O
statistical	B
models	O
are	O
generative	O
models	O
is	O
models	O
that	O
specify	O
a	O
full	O
probability	B
density	B
over	O
all	O
variables	O
in	O
the	O
situation	O
that	O
make	O
use	O
of	O
latent	O
variables	O
to	O
describe	O
a	O
probability	B
distribution	B
over	O
observables	O
examples	O
of	O
latent	B
variable	I
models	I
include	O
chapter	O
s	O
mixture	O
models	O
which	O
model	B
the	O
observables	O
as	O
coming	O
from	O
a	O
superposed	O
mixture	O
of	O
simple	O
probability	B
distributions	O
latent	O
variables	O
are	O
the	O
unknown	O
class	O
labels	O
of	O
the	O
examples	O
hidden	O
markov	O
models	O
and	O
juang	O
durbin	O
et	O
al	O
and	O
factor	B
analysis	I
the	O
decoding	B
problem	O
for	O
error-correcting	B
codes	I
can	O
also	O
be	O
viewed	O
in	O
in	O
that	O
case	O
the	O
encoding	O
terms	O
of	O
a	O
latent	B
variable	B
model	B
matrix	B
g	O
is	O
normally	O
known	O
in	O
advance	O
in	O
latent	B
variable	I
modelling	B
the	O
parameters	B
equivalent	O
to	O
g	O
are	O
usually	O
not	O
known	O
and	O
must	O
be	O
inferred	O
from	O
the	O
data	O
along	O
with	O
the	O
latent	O
variables	O
s	O
usually	O
the	O
latent	O
variables	O
have	O
a	O
simple	O
distribution	B
often	O
a	O
separable	O
distribution	B
thus	O
when	O
we	O
a	O
latent	B
variable	B
model	B
we	O
are	O
a	O
description	O
of	O
the	O
data	O
in	O
terms	O
of	O
independent	O
components	O
the	O
independent	B
component	I
analysis	I
algorithm	O
corresponds	O
to	O
perhaps	O
the	O
simplest	O
possible	O
latent	B
variable	B
model	B
with	O
continuous	B
latent	O
variables	O
the	O
generative	B
model	B
for	O
independent	B
component	I
analysis	I
a	O
set	B
of	O
n	O
observations	O
d	O
fxngn	O
are	O
assumed	O
to	O
be	O
generated	O
as	O
follows	O
each	O
j-dimensional	O
vector	O
x	O
is	O
a	O
linear	B
mixture	O
of	O
i	O
underlying	O
source	O
signals	O
s	O
x	O
gs	O
where	O
the	O
matrix	B
of	O
mixing	O
g	O
is	O
not	O
known	O
the	O
simplest	O
algorithm	O
results	O
if	O
we	O
assume	O
that	O
the	O
number	O
of	O
sources	O
is	O
equal	O
to	O
the	O
number	O
of	O
observations	O
i	O
e	O
i	O
j	O
our	O
aim	O
is	O
to	O
recover	O
the	O
source	O
variables	O
s	O
some	O
multiplicative	O
factors	O
and	O
possibly	O
permuted	O
to	O
put	O
it	O
another	O
way	O
we	O
aim	O
to	O
create	O
the	O
inverse	O
of	O
g	O
a	O
post-multiplicative	O
factor	O
given	O
only	O
a	O
set	B
of	O
examples	O
fxg	O
we	O
assume	O
that	O
the	O
latent	O
variables	O
are	O
independently	O
distributed	O
with	O
marginal	B
distributions	O
p	O
jh	O
pisi	O
here	O
h	O
denotes	O
the	O
assumed	O
form	O
of	O
this	O
model	B
and	O
the	O
assumed	O
probability	B
distributions	O
pi	O
of	O
the	O
latent	O
variables	O
the	O
probability	B
of	O
the	O
observables	O
and	O
the	O
hidden	O
variables	O
given	O
g	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	B
h	O
is	O
p	O
sngn	O
j	O
gh	O
n	O
j	O
sn	O
ghp	O
jhi	O
i	O
a	O
yi	O
j	O
gjisn	O
pisn	O
i	O
n	O
we	O
assume	O
that	O
the	O
vector	O
x	O
is	O
generated	O
without	O
noise	O
this	O
assumption	O
is	O
not	O
usually	O
made	O
in	O
latent	B
variable	I
modelling	B
since	O
noise-free	O
data	O
are	O
rare	O
but	O
it	O
makes	O
the	O
inference	B
problem	O
far	O
simpler	O
to	O
solve	O
the	O
likelihood	B
function	O
for	O
learning	B
about	O
g	O
from	O
the	O
data	O
d	O
the	O
relevant	O
quantity	O
is	O
the	O
likelihood	B
function	O
p	O
j	O
gh	O
p	O
j	O
gh	O
which	O
is	O
a	O
product	O
of	O
factors	O
each	O
of	O
which	O
is	O
obtained	O
by	O
marginalizing	O
over	O
the	O
latent	O
variables	O
when	O
we	O
marginalize	O
over	O
delta	O
functions	B
remember	O
v	O
f	O
we	O
adopt	O
summation	B
convention	I
at	O
this	O
a	O
single	O
factor	O
in	O
the	O
that	O
r	O
ds	O
vsf	O
point	O
such	O
that	O
for	O
example	O
gjisn	O
likelihood	B
is	O
given	O
by	O
i	O
pi	O
gjisn	O
i	O
p	O
j	O
gh	O
z	O
disn	O
p	O
j	O
sn	O
ghp	O
jh	O
z	O
disn	O
yj	O
jdet	O
gjyi	O
j	O
gjisn	O
i	O
pisn	O
i	O
ij	O
xj	O
ln	O
p	O
j	O
gh	O
lnjdet	O
gj	O
ln	O
ij	O
xj	O
to	O
obtain	O
a	O
maximum	B
likelihood	B
algorithm	O
we	O
the	O
gradient	O
of	O
the	O
log	O
if	O
we	O
introduce	O
w	O
the	O
log	O
likelihood	B
contributed	O
by	O
a	O
likelihood	B
single	O
example	O
may	O
be	O
written	O
ln	O
p	O
j	O
gh	O
lnjdet	O
wj	O
ln	O
piwijxj	O
we	O
ll	O
assume	O
from	O
now	O
on	O
that	O
det	O
w	O
is	O
positive	O
so	O
that	O
we	O
can	O
omit	O
the	O
absolute	B
value	I
sign	O
we	O
will	O
need	O
the	O
following	O
identities	O
ln	O
det	O
g	O
ij	O
wij	O
lj	O
lm	O
f	O
im	O
gli	O
let	O
us	O
ai	O
wijxj	O
d	O
ln	O
piaidai	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
generative	B
model	B
for	O
independent	B
component	I
analysis	I
algorithm	O
independent	B
component	I
analysis	I
online	O
steepest	O
ascents	O
version	O
see	O
also	O
algorithm	O
which	O
is	O
to	O
be	O
preferred	O
repeat	O
for	O
each	O
datapoint	O
x	O
put	O
x	O
through	O
a	O
linear	B
mapping	B
a	O
wx	O
put	O
a	O
through	O
a	O
nonlinear	B
map	O
zi	O
where	O
a	O
popular	O
choice	O
for	O
is	O
tanhai	O
adjust	O
the	O
weights	O
in	O
accordance	O
with	O
zxt	O
and	O
zi	O
which	O
indicates	O
in	O
which	O
direction	O
ai	O
needs	O
to	O
change	O
to	O
make	O
the	O
probability	B
of	O
the	O
data	O
greater	O
we	O
may	O
then	O
obtain	O
the	O
gradient	O
with	O
respect	O
to	O
gji	O
using	O
equations	O
and	O
ln	O
p	O
j	O
gh	O
or	O
alternatively	O
the	O
derivative	O
with	O
respect	O
to	O
wij	O
ln	O
p	O
j	O
gh	O
gji	O
xjzi	O
if	O
we	O
choose	O
to	O
change	O
w	O
so	O
as	O
to	O
ascend	O
this	O
gradient	O
we	O
obtain	O
the	O
learning	B
rule	I
zxt	O
the	O
algorithm	O
so	O
far	O
is	O
summarized	O
in	O
algorithm	O
choices	O
of	O
the	O
choice	O
of	O
the	O
function	O
the	O
assumed	O
prior	B
distribution	B
of	O
the	O
latent	B
variable	I
s	O
let	O
s	O
consider	O
the	O
linear	B
choice	O
which	O
implicitly	O
equation	O
assumes	O
a	O
gaussian	B
distribution	B
on	O
the	O
latent	O
variables	O
the	O
gaussian	B
distribution	B
on	O
the	O
latent	O
variables	O
is	O
invariant	O
under	O
rotation	O
of	O
the	O
latent	O
variables	O
so	O
there	O
can	O
be	O
no	O
evidence	B
favouring	O
any	O
particular	O
alignment	O
of	O
the	O
latent	B
variable	I
space	O
the	O
linear	B
algorithm	O
is	O
thus	O
uninteresting	O
in	O
that	O
it	O
will	O
never	O
recover	O
the	O
matrix	B
g	O
or	O
the	O
original	O
sources	O
our	O
only	O
hope	O
is	O
thus	O
that	O
the	O
sources	O
are	O
non-gaussian	O
thankfully	O
most	O
real	O
sources	O
have	O
non-gaussian	O
distributions	O
often	O
they	O
have	O
heavier	O
tails	O
than	O
gaussians	O
we	O
thus	O
move	O
on	O
to	O
the	O
popular	O
tanh	O
nonlinearity	O
if	O
then	O
implicitly	O
we	O
are	O
assuming	O
tanhai	O
pisi	O
coshsi	O
esi	O
this	O
is	O
a	O
heavier-tailed	O
distribution	B
for	O
the	O
latent	O
variables	O
than	O
the	O
gaussian	B
distribution	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	B
figure	O
illustration	O
of	O
the	O
generative	O
models	O
implicit	O
in	O
the	O
learning	B
algorithm	O
distributions	O
over	O
two	O
observables	O
generated	O
by	O
cosh	O
distributions	O
on	O
the	O
latent	O
distribution	B
and	O
variables	O
for	O
g	O
g	O
distribution	B
contours	O
of	O
the	O
generative	O
distributions	O
when	O
the	O
latent	O
variables	O
have	O
cauchy	B
distributions	O
the	O
learning	B
algorithm	O
this	O
amoeboid	O
object	O
to	O
the	O
empirical	O
data	O
in	O
such	O
a	O
way	O
as	O
to	O
maximize	O
the	O
likelihood	B
the	O
contour	O
plot	O
in	O
does	O
not	O
adequately	O
represent	O
this	O
heavy-tailed	O
distribution	B
part	O
of	O
the	O
tails	O
of	O
the	O
cauchy	B
distribution	B
giving	O
the	O
contours	O
times	O
the	O
density	B
at	O
the	O
origin	O
some	O
data	O
from	O
one	O
of	O
the	O
generative	O
distributions	O
illustrated	O
in	O
and	O
can	O
you	O
tell	O
which	O
samples	O
were	O
created	O
of	O
which	O
fell	O
in	O
the	O
plotted	O
region	O
we	O
could	O
also	O
use	O
a	O
tanh	O
nonlinearity	O
with	O
gain	B
that	O
is	O
whose	O
implicit	O
probabilistic	B
model	B
is	O
pisi	O
in	O
the	O
limit	O
of	O
large	O
the	O
nonlinearity	O
becomes	O
a	O
step	O
function	O
and	O
the	O
probability	B
distribution	B
pisi	O
becomes	O
a	O
biexponential	B
distribution	B
pisi	O
in	O
the	O
limit	O
pisi	O
approaches	O
a	O
gaussian	B
with	O
mean	B
zero	O
and	O
variance	B
heavier-tailed	O
distributions	O
than	O
these	O
may	O
also	O
be	O
used	O
the	O
student	B
and	O
cauchy	B
distributions	O
spring	B
to	O
mind	O
example	O
distributions	O
figures	O
illustrate	O
typical	B
distributions	O
generated	O
by	O
the	O
independent	O
components	O
model	B
when	O
the	O
components	O
have	O
cosh	O
and	O
cauchy	B
distributions	O
figure	O
shows	O
some	O
samples	O
from	O
the	O
cauchy	B
model	B
the	O
cauchy	B
distribution	B
being	O
the	O
more	O
heavy-tailed	O
gives	O
the	O
clearest	O
picture	O
of	O
how	O
the	O
predictive	B
distribution	B
depends	O
on	O
the	O
assumed	O
generative	O
parameters	B
g	O
a	O
covariant	B
simpler	O
and	O
faster	O
learning	B
algorithm	O
we	O
have	O
thus	O
derived	O
a	O
learning	B
algorithm	O
that	O
performs	O
steepest	B
descents	I
on	O
the	O
likelihood	B
function	O
the	O
algorithm	O
does	O
not	O
work	O
very	O
quickly	O
even	O
on	O
toy	O
data	O
the	O
algorithm	O
is	O
ill-conditioned	O
and	O
illustrates	O
nicely	O
the	O
general	O
advice	O
that	O
while	O
the	O
gradient	O
of	O
an	O
objective	B
function	I
is	O
a	O
splendid	O
idea	O
ascending	O
the	O
gradient	O
directly	O
may	O
not	O
be	O
the	O
fact	O
that	O
the	O
algorithm	O
is	O
ill-conditioned	O
can	O
be	O
seen	O
in	O
the	O
fact	O
that	O
it	O
involves	O
a	O
matrix	B
inverse	O
which	O
can	O
be	O
arbitrarily	O
large	O
or	O
even	O
covariant	B
optimization	B
in	O
general	O
the	O
principle	O
of	O
covariance	B
says	O
that	O
a	O
consistent	O
algorithm	O
should	O
give	O
the	O
same	O
results	O
independent	O
of	O
the	O
units	B
in	O
which	O
quantities	O
are	O
measured	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
covariant	B
simpler	O
and	O
faster	O
learning	B
algorithm	O
here	O
n	O
is	O
the	O
number	O
of	O
iterations	O
a	O
prime	O
example	O
of	O
a	O
non-covariant	O
algorithm	O
is	O
the	O
popular	O
steepest	B
descents	I
rule	O
a	O
dimensionless	O
objective	B
function	I
lw	O
is	O
its	O
derivative	O
with	O
respect	O
to	O
some	O
parameters	B
w	O
is	O
computed	O
and	O
then	O
w	O
is	O
changed	O
by	O
the	O
rule	O
this	O
popular	O
equation	O
is	O
dimensionally	O
inconsistent	O
the	O
left-hand	O
side	O
of	O
this	O
equation	O
has	O
dimensions	B
of	O
and	O
the	O
right-hand	O
side	O
has	O
dimensions	B
the	O
behaviour	O
of	O
the	O
learning	B
algorithm	O
is	O
not	O
covariant	B
with	O
respect	O
to	O
linear	B
rescaling	O
of	O
the	O
vector	O
w	O
dimensional	O
inconsistency	O
is	O
not	O
the	O
end	O
of	O
the	O
world	O
as	O
the	O
success	O
of	O
numerous	O
gradient	B
descent	I
algorithms	B
has	O
demonstrated	O
and	O
indeed	O
if	O
decreases	O
with	O
n	O
on-line	O
learning	B
as	O
then	O
the	O
munrorobbins	O
theorem	O
p	O
shows	O
that	O
the	O
parameters	B
will	O
asymptotically	O
converge	O
to	O
the	O
maximum	B
likelihood	B
parameters	B
but	O
the	O
non-covariant	O
algorithm	O
may	O
take	O
a	O
very	O
large	O
number	O
of	O
iterations	O
to	O
achieve	O
this	O
convergence	O
indeed	O
many	O
former	O
users	O
of	O
steepest	B
descents	I
algorithms	B
prefer	O
to	O
use	O
algorithms	B
such	O
as	O
conjugate	O
gradients	O
that	O
adaptively	O
out	O
the	O
curvature	O
of	O
the	O
objective	B
function	I
the	O
defense	O
of	O
equation	O
that	O
points	O
out	O
could	O
be	O
a	O
dimensional	O
constant	O
is	O
untenable	O
if	O
not	O
all	O
the	O
parameters	B
wi	O
have	O
the	O
same	O
dimensions	B
the	O
algorithm	O
would	O
be	O
covariant	B
if	O
it	O
had	O
the	O
form	O
where	O
m	O
is	O
a	O
matrix	B
whose	O
i	O
element	O
has	O
dimensions	B
from	O
where	O
can	O
we	O
obtain	O
such	O
a	O
matrix	B
two	O
sources	O
of	O
such	O
matrices	B
are	O
metrics	O
and	O
curvatures	O
metrics	O
and	O
curvatures	O
if	O
there	O
is	O
a	O
natural	B
metric	B
that	O
distances	O
in	O
our	O
parameter	O
space	O
w	O
then	O
a	O
matrix	B
m	O
can	O
be	O
obtained	O
from	O
the	O
metric	B
there	O
is	O
often	O
a	O
natural	B
choice	O
in	O
the	O
special	O
case	O
where	O
there	O
is	O
a	O
known	O
quadratic	O
metric	B
the	O
length	O
of	O
a	O
vector	O
w	O
then	O
the	O
matrix	B
can	O
be	O
obtained	O
from	O
the	O
quadratic	O
form	O
for	O
example	O
if	O
the	O
length	O
is	O
then	O
the	O
natural	B
matrix	B
is	O
m	O
i	O
and	O
steepest	B
descents	I
is	O
appropriate	O
another	O
way	O
of	O
a	O
metric	B
is	O
to	O
look	O
at	O
the	O
curvature	O
of	O
the	O
objective	B
function	I
a	O
r	O
then	O
the	O
matrix	B
m	O
will	O
give	O
a	O
covariant	B
algorithm	I
what	O
is	O
more	O
this	O
algorithm	O
is	O
the	O
newton	B
algorithm	I
so	O
we	O
recognize	O
that	O
it	O
will	O
alleviate	O
one	O
of	O
the	O
principal	O
with	O
steepest	B
descents	I
namely	O
its	O
slow	O
convergence	O
to	O
a	O
minimum	O
when	O
the	O
objective	B
function	I
is	O
at	O
all	O
ill-conditioned	O
the	O
newton	B
algorithm	I
converges	O
to	O
the	O
minimum	O
in	O
a	O
single	O
step	O
if	O
l	O
is	O
quadratic	O
in	O
some	O
problems	O
it	O
may	O
be	O
that	O
the	O
curvature	O
a	O
consists	O
of	O
both	O
datadependent	O
terms	O
and	O
data-independent	O
terms	O
in	O
this	O
case	O
one	O
might	O
choose	O
to	O
the	O
metric	B
using	O
the	O
data-independent	O
terms	O
only	O
the	O
resulting	O
algorithm	O
will	O
still	O
be	O
covariant	B
but	O
it	O
will	O
not	O
implement	O
an	O
exact	O
newton	B
step	O
obviously	O
there	O
are	O
many	O
covariant	B
algorithms	B
there	O
is	O
no	O
unique	O
choice	O
but	O
covariant	B
algorithms	B
are	O
a	O
small	O
subset	B
of	O
the	O
set	B
of	O
all	O
algorithms	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	B
back	O
to	O
independent	B
component	I
analysis	I
for	O
the	O
present	O
maximum	B
likelihood	B
problem	O
we	O
have	O
evaluated	O
the	O
gradient	O
with	O
respect	O
to	O
g	O
and	O
the	O
gradient	O
with	O
respect	O
to	O
w	O
steepest	O
ascents	O
in	O
w	O
is	O
not	O
covariant	B
let	O
us	O
construct	O
an	O
alternative	O
covariant	B
algorithm	I
with	O
the	O
help	O
of	O
the	O
curvature	O
of	O
the	O
log	O
likelihood	B
taking	O
the	O
second	O
derivative	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
w	O
we	O
obtain	O
two	O
terms	O
the	O
of	O
which	O
is	O
data-independent	O
and	O
the	O
second	O
of	O
which	O
is	O
data-dependent	O
sum	O
over	O
i	O
where	O
is	O
the	O
derivative	O
of	O
z	O
it	O
is	O
tempting	O
to	O
drop	O
the	O
data-dependent	O
term	O
and	O
the	O
matrix	B
m	O
by	O
however	O
this	O
matrix	B
is	O
not	O
positive	O
has	O
at	O
least	O
one	O
non-positive	O
eigenvalue	B
so	O
it	O
is	O
a	O
poor	O
approximation	B
to	O
the	O
curvature	O
of	O
the	O
log	O
likelihood	B
which	O
must	O
be	O
positive	O
in	O
the	O
neighbourhood	O
of	O
a	O
maximum	B
likelihood	B
solution	O
we	O
must	O
therefore	O
consult	O
the	O
data-dependent	O
term	O
for	O
inspiration	O
the	O
aim	O
is	O
to	O
a	O
convenient	O
approximation	B
to	O
the	O
curvature	O
and	O
to	O
obtain	O
a	O
covariant	B
algorithm	I
not	O
necessarily	O
to	O
implement	O
an	O
exact	O
newton	B
step	O
what	O
is	O
the	O
average	O
value	O
of	O
if	O
the	O
true	O
value	O
of	O
g	O
is	O
then	O
we	O
now	O
make	O
several	O
severe	O
approximations	O
we	O
replace	O
by	O
the	O
present	O
value	O
of	O
g	O
and	O
replace	O
the	O
correlated	O
average	O
by	O
here	O
is	O
the	O
variancecovariance	O
matrix	B
of	O
the	O
latent	O
variables	O
is	O
assumed	O
to	O
exist	O
and	O
di	O
is	O
the	O
typical	B
value	O
of	O
the	O
curvature	O
ln	O
given	O
that	O
the	O
sources	O
are	O
assumed	O
to	O
be	O
independent	O
and	O
d	O
are	O
both	O
diagonal	O
matrices	B
these	O
approximations	O
motivate	O
the	O
matrix	B
m	O
given	O
by	O
that	O
is	O
mijkl	O
i	O
for	O
simplicity	O
we	O
further	O
assume	O
that	O
the	O
sources	O
are	O
similar	O
to	O
each	O
other	O
so	O
that	O
and	O
d	O
are	O
both	O
homogeneous	B
and	O
that	O
this	O
will	O
lead	O
us	O
to	O
an	O
algorithm	O
that	O
is	O
covariant	B
with	O
respect	O
to	O
linear	B
rescaling	O
of	O
the	O
data	O
x	O
but	O
not	O
with	O
respect	O
to	O
linear	B
rescaling	O
of	O
the	O
latent	O
variables	O
we	O
thus	O
use	O
mijkl	O
multiplying	O
this	O
matrix	B
by	O
the	O
gradient	O
in	O
equation	O
we	O
obtain	O
the	O
following	O
covariant	B
learning	B
algorithm	O
notice	O
that	O
this	O
expression	O
does	O
not	O
require	O
any	O
inversion	O
of	O
the	O
matrix	B
w	O
the	O
only	O
additional	O
computation	O
once	O
z	O
has	O
been	O
computed	O
is	O
a	O
single	O
backward	B
pass	I
through	O
the	O
weights	O
to	O
compute	O
the	O
quantity	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
covariant	B
simpler	O
and	O
faster	O
learning	B
algorithm	O
repeat	O
for	O
each	O
datapoint	O
x	O
put	O
x	O
through	O
a	O
linear	B
mapping	B
a	O
wx	O
put	O
a	O
through	O
a	O
nonlinear	B
map	O
zi	O
where	O
a	O
popular	O
choice	O
for	O
is	O
tanhai	O
put	O
a	O
back	O
through	O
w	O
wta	O
adjust	O
the	O
weights	O
in	O
accordance	O
with	O
w	O
in	O
terms	O
of	O
which	O
the	O
covariant	B
algorithm	I
reads	O
algorithm	O
independent	B
component	I
analysis	I
covariant	B
version	O
the	O
quantity	O
on	O
the	O
right-hand	O
side	O
is	O
sometimes	O
called	O
the	O
natural	B
gradient	I
the	O
covariant	B
independent	B
component	I
analysis	I
algorithm	O
is	O
summarized	O
in	O
algorithm	O
further	O
reading	O
ica	O
was	O
originally	O
derived	O
using	O
an	O
information	B
maximization	I
approach	O
and	O
sejnowski	B
another	O
view	O
of	O
ica	O
in	O
terms	O
of	O
energy	B
functions	B
which	O
motivates	O
more	O
general	O
models	O
is	O
given	O
by	O
hinton	B
et	O
al	O
another	O
generalization	B
of	O
ica	O
can	O
be	O
found	O
in	O
pearlmutter	O
and	O
parra	O
there	O
is	O
now	O
an	O
enormous	O
literature	O
on	O
applications	O
of	O
ica	O
a	O
variational	B
free	I
energy	B
minimization	B
approach	O
to	O
ica-like	O
models	O
is	O
given	O
in	O
miskin	O
and	O
mackay	B
miskin	O
and	O
mackay	B
further	O
reading	O
on	O
blind	O
separation	B
including	O
non-ica	O
algorithms	B
can	O
be	O
found	O
in	O
and	O
herault	O
comon	O
et	O
al	O
hendin	O
et	O
al	O
amari	O
et	O
al	O
hojen-sorensen	O
et	O
al	O
models	O
while	O
latent	B
variable	I
models	I
with	O
a	O
number	O
of	O
latent	O
variables	O
are	O
widely	O
used	O
it	O
is	O
often	O
the	O
case	O
that	O
our	O
beliefs	O
about	O
the	O
situation	O
would	O
be	O
most	O
accurately	O
captured	O
by	O
a	O
very	O
large	O
number	O
of	O
latent	O
variables	O
consider	O
clustering	B
for	O
example	O
if	O
we	O
attack	O
speech	O
recognition	B
by	O
modelling	B
words	O
using	O
a	O
cluster	O
model	B
how	O
many	O
clusters	O
should	O
we	O
use	O
the	O
number	O
of	O
possible	O
words	O
is	O
unbounded	O
so	O
we	O
would	O
really	O
like	O
to	O
use	O
a	O
model	B
in	O
which	O
it	O
s	O
always	O
possible	O
for	O
new	O
clusters	O
to	O
arise	O
furthermore	O
if	O
we	O
do	O
a	O
careful	O
job	O
of	O
modelling	B
the	O
cluster	O
corresponding	O
to	O
just	O
one	O
english	B
word	O
we	O
will	O
probably	O
that	O
the	O
cluster	O
for	O
one	O
word	O
should	O
itself	O
be	O
modelled	O
as	O
composed	O
of	O
clusters	O
indeed	O
a	O
hierarchy	O
of	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	B
clusters	O
within	O
clusters	O
the	O
levels	O
of	O
the	O
hierarchy	O
would	O
divide	O
male	B
speakers	O
from	O
female	B
and	O
would	O
separate	O
speakers	O
from	O
regions	O
india	O
britain	O
europe	O
and	O
so	O
forth	O
within	O
each	O
of	O
those	O
clusters	O
would	O
be	O
subclusters	O
for	O
the	O
accents	O
within	O
each	O
region	O
the	O
subclusters	O
could	O
have	O
subsubclusters	O
right	O
down	O
to	O
the	O
level	O
of	O
villages	O
streets	O
or	O
families	O
thus	O
we	O
would	O
often	O
like	O
to	O
have	O
numbers	O
of	O
clusters	O
in	O
some	O
cases	O
the	O
clusters	O
would	O
have	O
a	O
hierarchical	O
structure	O
and	O
in	O
other	O
cases	O
the	O
hierarchy	O
would	O
be	O
so	O
how	O
should	O
such	O
models	O
be	O
implemented	O
in	O
computers	O
and	O
how	O
should	O
we	O
set	B
up	O
our	O
bayesian	B
models	O
so	O
as	O
to	O
avoid	O
getting	O
silly	O
answers	O
mixture	O
models	O
for	O
categorical	O
data	O
are	O
presented	O
in	O
neal	B
along	O
with	O
a	O
monte	B
carlo	I
method	O
for	O
simulating	O
inferences	O
and	O
predictions	O
gaussian	B
mixture	O
models	O
with	O
a	O
hierarchical	O
structure	O
are	O
presented	O
in	O
rasmussen	O
neal	B
shows	O
how	O
to	O
use	O
dirichlet	B
trees	O
to	O
models	O
of	O
hierarchical	O
clusters	O
most	O
of	O
these	O
ideas	O
build	O
on	O
the	O
dirichlet	B
process	O
this	O
remains	O
an	O
active	O
research	O
area	O
and	O
ghahramani	O
beal	O
et	O
al	O
exercises	O
exercise	O
repeat	O
the	O
derivation	B
of	O
the	O
algorithm	O
but	O
assume	O
a	O
small	O
amount	O
of	O
noise	O
in	O
x	O
x	O
gs	O
n	O
so	O
the	O
term	O
in	O
the	O
joint	B
probability	B
is	O
replaced	O
by	O
a	O
probability	B
distribution	B
over	O
xn	O
show	O
that	O
if	O
this	O
noise	O
distribution	B
has	O
small	O
standard	B
deviation	I
the	O
identical	O
algorithm	O
results	O
i	O
j	O
gjisn	O
j	O
with	O
meanpi	O
gjisn	O
i	O
exercise	O
implement	O
the	O
covariant	B
ica	O
algorithm	O
and	O
apply	O
it	O
to	O
toy	O
data	O
exercise	O
create	O
algorithms	B
appropriate	O
for	O
the	O
situations	O
x	O
includes	O
substantial	O
gaussian	B
noise	O
more	O
measurements	O
than	O
latent	O
variables	O
i	O
fewer	O
measurements	O
than	O
latent	O
variables	O
i	O
factor	B
analysis	I
assumes	O
that	O
the	O
observations	O
x	O
can	O
be	O
described	O
in	O
terms	O
of	O
independent	O
latent	O
variables	O
fskg	O
and	O
independent	O
additive	O
noise	O
thus	O
the	O
observable	O
x	O
is	O
given	O
by	O
x	O
gs	O
n	O
where	O
n	O
is	O
a	O
noise	O
vector	O
whose	O
components	O
have	O
a	O
separable	O
probability	B
distribution	B
in	O
factor	B
analysis	I
it	O
is	O
often	O
assumed	O
that	O
the	O
probability	B
distributions	O
of	O
fskg	O
and	O
fnig	O
are	O
zero-mean	O
gaussians	O
the	O
noise	O
terms	O
may	O
have	O
variances	O
i	O
exercise	O
make	O
a	O
maximum	B
likelihood	B
algorithm	O
for	O
inferring	O
g	O
from	O
data	O
assuming	O
the	O
generative	B
model	B
x	O
gs	O
n	O
is	O
correct	O
and	O
that	O
s	O
and	O
n	O
have	O
independent	O
gaussian	B
distributions	O
include	O
parameters	B
j	O
to	O
describe	O
the	O
variance	B
of	O
each	O
nj	O
and	O
maximize	O
the	O
likelihood	B
with	O
respect	O
to	O
them	O
too	O
let	O
the	O
variance	B
of	O
each	O
si	O
be	O
exercise	O
implement	O
the	O
gaussian	B
mixture	O
model	B
of	O
rasmussen	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
random	B
inference	B
topics	O
what	O
do	O
you	O
know	O
if	O
you	O
are	O
ignorant	O
example	O
a	O
real	O
variable	O
x	O
is	O
measured	O
in	O
an	O
accurate	O
experiment	O
for	O
example	O
x	O
might	O
be	O
the	O
half-life	O
of	O
the	O
neutron	O
the	O
wavelength	O
of	O
light	O
emitted	O
by	O
a	O
the	O
depth	B
of	I
lake	B
vostok	O
or	O
the	O
mass	O
of	O
jupiter	O
s	O
moon	O
io	O
what	O
is	O
the	O
probability	B
that	O
the	O
value	O
of	O
x	O
starts	O
with	O
a	O
like	O
the	O
charge	O
of	O
the	O
electron	O
s	O
i	O
units	B
e	O
c	O
and	O
the	O
boltzmann	B
constant	O
k	O
j	O
and	O
what	O
is	O
the	O
probability	B
that	O
it	O
starts	O
with	O
a	O
like	O
the	O
faraday	O
constant	O
f	O
c	O
what	O
about	O
the	O
second	O
digit	O
what	O
is	O
the	O
probability	B
that	O
the	O
mantissa	O
of	O
x	O
starts	O
and	O
what	O
is	O
the	O
probability	B
that	O
x	O
starts	O
solution	O
an	O
expert	O
on	O
neutrons	O
antarctica	O
or	O
jove	O
might	O
be	O
able	O
to	O
predict	O
the	O
value	O
of	O
x	O
and	O
thus	O
predict	O
the	O
digit	O
with	O
some	O
but	O
what	O
about	O
someone	O
with	O
no	O
knowledge	O
of	O
the	O
topic	O
what	O
is	O
the	O
probability	B
distribution	B
corresponding	O
to	O
knowing	O
nothing	O
one	O
way	O
to	O
attack	O
this	O
question	O
is	O
to	O
notice	O
that	O
the	O
units	B
of	O
x	O
have	O
not	O
been	O
if	O
the	O
half-life	O
of	O
the	O
neutron	O
were	O
measured	O
in	O
fortnights	O
instead	O
of	O
seconds	O
the	O
number	O
x	O
would	O
be	O
divided	O
by	O
if	O
it	O
were	O
measured	O
in	O
years	O
it	O
would	O
be	O
divided	O
by	O
now	O
is	O
our	O
knowledge	O
about	O
x	O
and	O
in	O
particular	O
our	O
knowledge	O
of	O
its	O
digit	O
by	O
the	O
change	O
in	O
units	B
for	O
the	O
expert	O
the	O
answer	O
is	O
yes	O
but	O
let	O
us	O
take	O
someone	O
truly	O
ignorant	O
for	O
whom	O
the	O
answer	O
is	O
no	O
their	O
predictions	O
about	O
the	O
digit	O
of	O
x	O
are	O
independent	O
of	O
the	O
units	B
the	O
arbitrariness	O
of	O
the	O
units	B
corresponds	O
to	O
invariance	B
of	O
the	O
probability	B
distribution	B
when	O
x	O
is	O
multiplied	O
by	O
any	O
number	O
metres	O
feet	O
inches	O
figure	O
when	O
viewed	O
on	O
a	O
logarithmic	O
scale	O
scales	O
using	O
units	B
are	O
translated	O
relative	B
to	O
each	O
other	O
if	O
you	O
don	O
t	O
know	O
the	O
units	B
that	O
a	O
quantity	O
is	O
measured	O
in	O
the	O
probability	B
of	O
the	O
digit	O
must	O
be	O
proportional	O
to	O
the	O
length	O
of	O
the	O
corresponding	O
piece	O
of	O
logarithmic	O
scale	O
the	O
probability	B
that	O
the	O
digit	O
of	O
a	O
number	O
is	O
is	O
thus	O
log	O
log	O
log	O
log	O
log	O
log	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
random	B
inference	B
topics	O
p	O
p	O
now	O
so	O
without	O
needing	O
a	O
calculator	B
we	O
have	O
log	O
log	O
and	O
more	O
generally	O
the	O
probability	B
that	O
the	O
digit	O
is	O
d	O
is	O
logdlog	O
log	O
this	O
observation	O
about	O
initial	O
digits	O
is	O
known	O
as	O
benford	O
s	O
law	O
does	O
not	O
correspond	O
to	O
a	O
uniform	O
probability	B
distribution	B
over	O
d	O
ignorance	B
exercise	O
a	O
pin	O
is	O
thrown	O
tumbling	O
in	O
the	O
air	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
angle	O
between	O
the	O
pin	O
and	O
the	O
vertical	O
at	O
a	O
moment	O
while	O
it	O
is	O
in	O
the	O
air	O
the	O
tumbling	O
pin	O
is	O
photographed	O
what	O
is	O
the	O
probability	B
distribution	B
of	O
the	O
angle	O
between	O
the	O
pin	O
and	O
the	O
vertical	O
as	O
imaged	O
in	O
the	O
photograph	O
exercise	O
record	B
breaking	I
consider	O
keeping	O
track	O
of	O
the	O
world	B
record	I
for	O
some	O
quantity	O
x	O
say	O
earthquake	B
magnitude	O
or	O
longjump	O
distances	O
jumped	O
at	O
world	O
championships	O
if	O
we	O
assume	O
that	O
attempts	O
to	O
break	O
the	O
record	O
take	O
place	O
at	O
a	O
steady	O
rate	B
and	O
if	O
we	O
assume	O
that	O
the	O
underlying	O
probability	B
distribution	B
of	O
the	O
outcome	O
x	O
p	O
is	O
not	O
changing	O
an	O
assumption	O
that	O
i	O
think	O
is	O
unlikely	O
to	O
be	O
true	O
in	O
the	O
case	O
of	O
sports	O
endeavours	O
but	O
an	O
interesting	O
assumption	O
to	O
consider	O
nonetheless	O
and	O
assuming	O
no	O
knowledge	O
at	O
all	O
about	O
p	O
what	O
can	O
be	O
predicted	O
about	O
successive	O
intervals	B
between	O
the	O
dates	O
when	O
records	O
are	O
broken	O
the	O
distribution	B
exercise	O
in	O
their	O
landmark	O
paper	O
demonstrating	O
that	O
bacteria	O
could	O
mutate	O
from	O
virus	O
sensitivity	O
to	O
virus	O
resistance	O
luria	B
and	O
wanted	O
to	O
estimate	O
the	O
mutation	B
rate	B
in	O
an	O
exponentially-growing	O
population	O
from	O
the	O
total	O
number	O
of	O
mutants	O
found	O
at	O
the	O
end	O
of	O
the	O
experiment	O
this	O
problem	O
is	O
because	O
the	O
quantity	O
measured	O
number	O
of	O
mutated	O
bacteria	O
has	O
a	O
heavy-tailed	O
probability	B
distribution	B
a	O
mutation	O
occuring	O
early	O
in	O
the	O
experiment	O
can	O
give	O
rise	O
to	O
a	O
huge	O
number	O
of	O
mutants	O
unfortunately	O
luria	B
and	O
didn	O
t	O
know	O
bayes	B
theorem	O
and	O
their	O
way	O
of	O
coping	O
with	O
the	O
heavy-tailed	O
distribution	B
involves	O
arbitrary	O
hacks	O
leading	O
to	O
two	O
estimators	O
of	O
the	O
mutation	B
rate	B
one	O
of	O
these	O
estimators	O
on	O
the	O
mean	B
number	O
of	O
mutated	O
bacteria	O
averaging	O
over	O
several	O
experiments	O
has	O
appallingly	O
large	O
variance	B
yet	O
sampling	O
theorists	O
continue	O
to	O
use	O
it	O
and	O
base	O
intervals	B
around	O
it	O
and	O
oprea	O
in	O
this	O
exercise	O
you	O
ll	O
do	O
the	O
inference	B
right	O
in	O
each	O
culture	O
a	O
single	O
bacterium	O
that	O
is	O
not	O
resistant	O
gives	O
rise	O
after	O
g	O
generations	O
to	O
n	O
descendants	O
all	O
clones	O
except	O
for	O
arising	O
from	O
mutations	O
the	O
culture	O
is	O
then	O
exposed	O
to	O
a	O
virus	O
and	O
the	O
number	O
of	O
resistant	O
bacteria	O
n	O
is	O
measured	O
according	O
to	O
the	O
now	O
accepted	O
mutation	O
hypothesis	O
these	O
resistant	O
bacteria	O
got	O
their	O
resistance	O
from	O
random	B
mutations	O
that	O
took	O
place	O
during	O
the	O
growth	O
of	O
the	O
colony	O
the	O
mutation	B
rate	B
cell	O
per	O
generation	O
a	O
is	O
about	O
one	O
in	O
a	O
hundred	O
million	O
the	O
total	O
number	O
of	O
n	O
if	O
a	O
bacterium	O
mutates	O
at	O
the	O
ith	O
generation	O
its	O
descendants	O
all	O
inherit	O
the	O
mutation	O
and	O
the	O
number	O
of	O
resistant	O
bacteria	O
contributed	O
by	O
that	O
one	O
ancestor	O
is	O
opportunities	O
to	O
mutate	O
is	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
inferring	O
causation	O
given	O
m	O
separate	O
experiments	O
in	O
each	O
of	O
which	O
a	O
colony	O
of	O
size	O
n	O
is	O
created	O
and	O
where	O
the	O
measured	O
numbers	O
of	O
resistant	O
bacteria	O
are	O
fnmgm	O
what	O
can	O
we	O
infer	O
about	O
the	O
mutation	B
rate	B
a	O
make	O
the	O
inference	B
given	O
the	O
following	O
dataset	O
from	O
luria	B
and	O
for	O
n	O
fnmg	O
small	O
amount	O
of	O
computation	O
is	O
required	O
to	O
solve	O
this	O
problem	O
inferring	O
causation	O
exercise	O
in	O
the	O
bayesian	B
graphical	O
model	B
community	O
the	O
task	O
of	O
inferring	O
which	O
way	O
the	O
arrows	O
point	O
that	O
is	O
which	O
nodes	O
are	O
parents	O
and	O
which	O
children	O
is	O
one	O
on	O
which	O
much	O
has	O
been	O
written	O
inferring	O
causation	O
is	O
tricky	O
because	O
of	O
likelihood	B
equivalence	B
two	O
graphical	O
models	O
are	O
likelihood-equivalent	O
if	O
for	O
any	O
setting	O
of	O
the	O
parameters	B
of	O
either	O
there	O
exists	O
a	O
setting	O
of	O
the	O
parameters	B
of	O
the	O
other	O
such	O
that	O
the	O
two	O
joint	B
probability	B
distributions	O
of	O
all	O
observables	O
are	O
identical	O
an	O
example	O
of	O
a	O
pair	O
of	O
likelihood-equivalent	O
models	O
are	O
a	O
b	O
and	O
b	O
a	O
the	O
model	B
a	O
b	O
asserts	O
that	O
a	O
is	O
the	O
parent	B
of	O
b	O
or	O
in	O
very	O
sloppy	O
terminology	O
a	O
causes	O
b	O
an	O
example	O
of	O
a	O
situation	O
where	O
b	O
a	O
is	O
true	O
is	O
the	O
case	O
where	O
b	O
is	O
the	O
variable	O
burglar	O
in	O
house	O
and	O
a	O
is	O
the	O
variable	O
alarm	O
is	O
ringing	O
here	O
it	O
is	O
literally	O
true	O
that	O
b	O
causes	O
a	O
but	O
this	O
choice	O
of	O
words	O
is	O
confusing	O
if	O
applied	O
to	O
another	O
example	O
r	O
d	O
where	O
r	O
denotes	O
it	O
rained	O
this	O
morning	O
and	O
d	O
denotes	O
the	O
pavement	O
is	O
dry	O
r	O
causes	O
d	O
is	O
confusing	O
i	O
ll	O
therefore	O
use	O
the	O
words	O
b	O
is	O
a	O
parent	B
of	O
a	O
to	O
denote	O
causation	O
some	O
statistical	B
methods	O
that	O
use	O
the	O
likelihood	B
alone	O
are	O
unable	O
to	O
use	O
data	O
to	O
distinguish	O
between	O
likelihood-equivalent	O
models	O
in	O
a	O
bayesian	B
approach	O
on	O
the	O
other	O
hand	O
two	O
likelihood-equivalent	O
models	O
may	O
nevertheless	O
be	O
somewhat	O
distinguished	O
in	O
the	O
light	O
of	O
data	O
since	O
likelihood-equivalence	O
does	O
not	O
force	O
a	O
bayesian	B
to	O
use	O
priors	O
that	O
assign	O
equivalent	O
densities	O
over	O
the	O
two	O
parameter	O
spaces	O
of	O
the	O
models	O
however	O
many	O
bayesian	B
graphical	O
modelling	B
folks	O
perhaps	O
out	O
of	O
sympathy	O
for	O
their	O
non-bayesian	O
colleagues	O
or	O
from	O
a	O
latent	O
urge	O
not	O
to	O
appear	O
from	O
them	O
deliberately	O
discard	O
this	O
potential	O
advantage	O
of	O
bayesian	B
methods	O
the	O
ability	O
to	O
infer	O
causation	O
from	O
data	O
by	O
skewing	O
their	O
models	O
so	O
that	O
the	O
ability	O
goes	O
away	O
a	O
widespread	O
orthodoxy	O
holds	O
that	O
one	O
should	O
identify	O
the	O
choices	O
of	O
prior	B
for	O
which	O
prior	B
equivalence	B
holds	O
i	O
e	O
the	O
priors	O
such	O
that	O
models	O
that	O
are	O
likelihood-equivalent	O
also	O
have	O
identical	O
posterior	O
probabilities	O
and	O
then	O
one	O
should	O
use	O
one	O
of	O
those	O
priors	O
in	O
inference	B
and	O
prediction	B
this	O
argument	O
motivates	O
the	O
use	O
as	O
the	O
prior	B
over	O
all	O
probability	B
vectors	B
of	O
specially-constructed	O
dirichlet	B
distributions	O
in	O
my	O
view	O
it	O
is	O
a	O
philosophical	O
error	O
to	O
use	O
only	O
those	O
priors	O
such	O
that	O
causation	O
cannot	O
be	O
inferred	O
priors	O
should	O
be	O
set	B
to	O
describe	O
one	O
s	O
assumptions	B
when	O
this	O
is	O
done	O
it	O
s	O
likely	O
that	O
interesting	O
inferences	O
about	O
causation	O
can	O
be	O
made	O
from	O
data	O
in	O
this	O
exercise	O
you	O
ll	O
make	O
an	O
example	O
of	O
such	O
an	O
inference	B
consider	O
the	O
toy	O
problem	O
where	O
a	O
and	O
b	O
are	O
binary	O
variables	O
the	O
two	O
models	O
are	O
ha	O
b	O
and	O
hb	O
a	O
ha	O
b	O
asserts	O
that	O
the	O
marginal	B
probability	B
of	O
a	O
comes	O
from	O
a	O
beta	B
distribution	B
with	O
parameters	B
i	O
e	O
the	O
uniform	O
distribution	B
and	O
that	O
the	O
two	O
conditional	B
distributions	O
p	O
a	O
and	O
p	O
a	O
also	O
come	O
independently	O
from	O
beta	B
distributions	O
with	O
parameters	B
the	O
other	O
model	B
assigns	O
similar	O
priors	O
to	O
the	O
marginal	B
probability	B
of	O
b	O
and	O
the	O
conditional	B
distributions	O
of	O
a	O
given	O
b	O
data	O
are	O
gathered	O
and	O
the	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
random	B
inference	B
topics	O
counts	O
given	O
f	O
outcomes	O
are	O
b	O
b	O
a	O
a	O
what	O
are	O
the	O
posterior	O
probabilities	O
of	O
the	O
two	O
hypotheses	O
it	O
s	O
a	O
good	B
idea	O
to	O
work	O
this	O
exercise	O
out	O
symbolically	O
in	O
order	O
to	O
spot	O
hint	O
all	O
the	O
that	O
emerge	O
d	O
dx	O
ln	O
lnx	O
the	O
topic	O
of	O
inferring	O
causation	O
is	O
a	O
complex	B
one	O
the	O
fact	O
that	O
bayesian	B
inference	B
can	O
sensibly	O
be	O
used	O
to	O
infer	O
the	O
directions	O
of	O
arrows	O
in	O
graphs	O
seems	O
to	O
be	O
a	O
neglected	O
view	O
but	O
it	O
is	O
certainly	O
not	O
the	O
whole	O
story	O
see	O
pearl	O
for	O
discussion	O
of	O
many	O
other	O
aspects	O
of	O
causality	O
further	O
exercises	O
exercise	O
photons	O
arriving	O
at	O
a	O
photon	O
detector	O
are	O
believed	O
to	O
be	O
emit	O
ted	O
as	O
a	O
poisson	B
process	I
with	O
a	O
time-varying	O
rate	B
expa	O
b	O
sin	O
t	O
where	O
the	O
parameters	B
a	O
b	O
and	O
are	O
known	O
data	O
are	O
collected	O
during	O
the	O
time	O
t	O
t	O
given	O
that	O
n	O
photons	O
arrived	O
at	O
times	O
ftngn	O
reading	O
gregory	O
and	O
discuss	O
the	O
inference	B
of	O
a	O
b	O
and	O
loredo	O
exercise	O
a	O
data	O
consisting	O
of	O
two	O
columns	O
of	O
numbers	O
has	O
been	O
printed	O
in	O
such	O
a	O
way	O
that	O
the	O
boundaries	O
between	O
the	O
columns	O
are	O
unclear	O
here	O
are	O
the	O
resulting	O
strings	O
discuss	O
how	O
probable	O
it	O
is	O
given	O
these	O
data	O
that	O
the	O
correct	O
parsing	O
of	O
each	O
item	O
is	O
etc	O
etc	O
parsing	O
of	O
a	O
string	O
is	O
a	O
grammatical	O
interpretation	O
of	O
the	O
string	O
for	O
example	O
punch	B
bores	O
could	O
be	O
parsed	O
as	O
punch	B
bores	O
or	O
punch	B
verb	O
bores	O
noun	O
exercise	O
in	O
an	O
experiment	O
the	O
measured	O
quantities	O
fxng	O
come	O
inde	O
pendently	O
from	O
a	O
biexponential	B
distribution	B
with	O
mean	B
p	O
z	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
where	O
z	O
is	O
the	O
normalizing	O
constant	O
z	O
the	O
mean	B
is	O
not	O
known	O
an	O
example	O
of	O
this	O
distribution	B
with	O
is	O
shown	O
in	O
assuming	O
the	O
four	O
datapoints	O
are	O
figure	O
the	O
biexponential	B
distribution	B
p	O
fxng	O
what	O
do	O
these	O
data	O
tell	O
us	O
about	O
include	O
detailed	O
sketches	O
in	O
your	O
answer	O
give	O
a	O
range	O
of	O
plausible	O
values	O
of	O
solutions	O
solution	O
to	O
exercise	O
a	O
population	O
of	O
size	O
n	O
has	O
n	O
opportunities	O
to	O
mutate	O
the	O
probability	B
of	O
the	O
number	O
of	O
mutations	O
that	O
occurred	O
r	O
is	O
roughly	O
poisson	B
p	O
j	O
a	O
n	O
r	O
is	O
slightly	O
inaccurate	O
because	O
the	O
descendants	O
of	O
a	O
mutant	O
cannot	O
themselves	O
undergo	O
the	O
same	O
mutation	O
each	O
mutation	O
gives	O
rise	O
to	O
a	O
number	O
of	O
mutant	O
cells	O
ni	O
that	O
depends	O
on	O
the	O
generation	O
time	O
of	O
the	O
mutation	O
if	O
multiplication	O
went	O
like	O
clockwork	O
then	O
the	O
probability	B
of	O
ni	O
being	O
would	O
be	O
the	O
probability	B
of	O
would	O
be	O
the	O
probability	B
of	O
would	O
be	O
and	O
p	O
for	O
all	O
ni	O
that	O
are	O
powers	O
of	O
two	O
but	O
we	O
don	O
t	O
expect	O
the	O
mutant	O
progeny	O
to	O
divide	O
in	O
exact	O
synchrony	O
and	O
we	O
don	O
t	O
know	O
the	O
precise	O
timing	B
of	O
the	O
end	O
of	O
the	O
experiment	O
compared	O
to	O
the	O
division	O
times	O
a	O
smoothed	O
version	O
of	O
this	O
distribution	B
that	O
permits	O
all	O
integers	O
to	O
occur	O
is	O
p	O
z	O
i	O
where	O
z	O
distribution	B
s	O
moments	O
are	O
all	O
wrong	O
since	O
ni	O
can	O
never	O
exceed	O
n	O
but	O
who	O
cares	O
about	O
moments	O
only	O
sampling	B
theory	I
statisticians	O
who	O
are	O
barking	O
up	O
the	O
wrong	O
tree	B
constructing	O
unbiased	O
estimators	O
such	O
as	O
log	O
n	O
the	O
error	O
that	O
we	O
introduce	O
in	O
the	O
likelihood	B
function	O
by	O
using	O
the	O
approximation	B
to	O
p	O
is	O
negligible	O
the	O
observed	O
number	O
of	O
mutants	O
n	O
is	O
the	O
sum	O
n	O
ni	O
r	O
the	O
probability	B
distribution	B
of	O
n	O
given	O
r	O
is	O
the	O
convolution	B
of	O
r	O
identical	O
distributions	O
of	O
the	O
form	O
for	O
example	O
p	O
r	O
z	O
for	O
n	O
the	O
probability	B
distribution	B
of	O
n	O
given	O
a	O
which	O
is	O
what	O
we	O
need	O
for	O
the	O
bayesian	B
inference	B
is	O
given	O
by	O
summing	O
over	O
r	O
p	O
a	O
n	O
p	O
rp	O
j	O
a	O
n	O
this	O
quantity	O
can	O
t	O
be	O
evaluated	O
analytically	O
but	O
for	O
small	O
a	O
it	O
s	O
easy	O
to	O
evaluate	O
to	O
any	O
desired	O
numerical	O
precision	B
by	O
explicitly	O
summing	O
over	O
r	O
from	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
random	B
inference	B
topics	O
r	O
to	O
some	O
rmax	O
with	O
p	O
r	O
also	O
being	O
found	O
for	O
each	O
r	O
by	O
rmax	O
explicit	O
convolutions	O
for	O
all	O
required	O
values	O
of	O
n	O
if	O
rmax	O
nmax	O
the	O
largest	O
value	O
of	O
n	O
encountered	O
in	O
the	O
data	O
then	O
p	O
a	O
is	O
computed	O
exactly	O
but	O
for	O
this	O
question	O
s	O
data	O
rmax	O
is	O
plenty	O
for	O
an	O
accurate	O
result	O
i	O
used	O
rmax	O
to	O
make	O
the	O
graphs	O
in	O
octave	B
source	O
code	O
is	O
incidentally	O
for	O
data	O
sets	O
like	O
the	O
one	O
in	O
this	O
exercise	O
which	O
have	O
a	O
substantial	O
number	O
of	O
zero	O
counts	O
very	O
little	O
is	O
lost	O
by	O
making	O
luria	B
and	O
delbruck	O
s	O
second	O
approximation	B
which	O
is	O
to	O
retain	O
only	O
the	O
count	O
of	O
how	O
many	O
n	O
were	O
equal	O
to	O
zero	O
and	O
how	O
many	O
were	O
non-zero	O
the	O
likelihood	B
function	O
found	O
using	O
this	O
weakened	O
data	B
set	B
la	O
is	O
scarcely	O
distinguishable	O
from	O
the	O
likelihood	B
computed	O
using	O
full	O
information	B
solution	O
to	O
exercise	O
from	O
the	O
six	B
terms	O
of	O
the	O
form	O
p	O
qi	O
fi	O
most	O
factors	O
cancel	O
and	O
all	O
that	O
remains	O
is	O
qi	O
p	O
j	O
data	O
p	O
j	O
data	O
there	O
is	O
modest	O
evidence	B
in	O
favour	O
of	O
ha	O
b	O
because	O
the	O
three	O
probabilities	O
inferred	O
for	O
that	O
hypothesis	O
and	O
are	O
more	O
typical	B
of	O
the	O
prior	B
than	O
are	O
the	O
three	O
probabilities	O
inferred	O
for	O
the	O
other	O
and	O
this	O
statement	O
sounds	O
absurd	O
if	O
we	O
think	O
of	O
the	O
priors	O
as	O
uniform	O
over	O
the	O
three	O
probabilities	O
surely	O
under	O
a	O
uniform	O
prior	B
any	O
settings	O
of	O
the	O
probabilities	O
are	O
equally	O
probable	O
but	O
in	O
the	O
natural	B
basis	O
the	O
logit	B
basis	O
the	O
prior	B
is	O
proportional	O
to	O
p	O
and	O
the	O
posterior	B
probability	B
ratio	O
can	O
be	O
estimated	O
by	O
which	O
is	O
not	O
exactly	O
right	O
but	O
it	O
does	O
illustrate	O
where	O
the	O
preference	O
for	O
a	O
b	O
is	O
coming	O
from	O
figure	O
likelihood	B
of	O
the	O
mutation	B
rate	B
a	O
on	O
a	O
linear	B
scale	O
and	O
log	O
scale	O
given	O
luria	B
and	O
delbruck	O
s	O
data	O
vertical	O
axis	O
horizontal	O
axis	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decision	B
theory	I
decision	B
theory	I
is	O
trivial	O
apart	O
from	O
computational	O
details	O
like	O
playing	O
chess	B
you	O
have	O
a	O
choice	O
of	O
various	O
actions	O
a	O
the	O
world	O
may	O
be	O
in	O
one	O
of	O
many	O
states	O
x	O
which	O
one	O
occurs	O
may	O
be	O
by	O
your	O
action	O
the	O
world	O
s	O
state	O
has	O
a	O
probability	B
distribution	B
p	O
a	O
finally	O
there	O
is	O
a	O
utility	B
function	O
u	O
a	O
which	O
the	O
you	O
receive	O
when	O
the	O
world	O
is	O
in	O
state	O
x	O
and	O
you	O
chose	O
action	O
a	O
the	O
task	O
of	O
decision	B
theory	I
is	O
to	O
select	O
the	O
action	O
that	O
maximizes	O
the	O
expected	O
utility	B
eu	O
j	O
a	O
dkx	O
u	O
ap	O
a	O
that	O
s	O
all	O
the	O
computational	O
problem	O
is	O
to	O
maximize	O
eu	O
j	O
a	O
over	O
a	O
may	O
prefer	O
to	O
a	O
loss	B
function	I
l	O
instead	O
of	O
a	O
utility	B
function	O
u	O
and	O
minimize	O
the	O
expected	O
loss	O
is	O
there	O
anything	O
more	O
to	O
be	O
said	O
about	O
decision	B
theory	I
well	O
in	O
a	O
real	O
problem	O
the	O
choice	O
of	O
an	O
appropriate	O
utility	B
function	O
may	O
be	O
quite	O
furthermore	O
when	O
a	O
sequence	B
of	O
actions	O
is	O
to	O
be	O
taken	O
with	O
each	O
action	O
providing	O
information	B
about	O
x	O
we	O
have	O
to	O
take	O
into	O
account	O
the	O
that	O
this	O
anticipated	O
information	B
may	O
have	O
on	O
our	O
subsequent	O
actions	O
the	O
resulting	O
mixture	O
of	O
forward	B
probability	B
and	O
inverse	B
probability	B
computations	O
in	O
a	O
decision	O
problem	O
is	O
distinctive	O
in	O
a	O
realistic	O
problem	O
such	O
as	O
playing	O
a	O
board	O
game	O
the	O
tree	B
of	O
possible	O
cogitations	O
and	O
actions	O
that	O
must	O
be	O
considered	O
becomes	O
enormous	O
and	O
doing	O
the	O
right	O
thing	O
is	O
not	O
simple	O
because	O
the	O
expected	O
utility	B
of	O
an	O
action	O
cannot	O
be	O
computed	O
exactly	O
and	O
wefald	O
baum	O
and	O
smith	O
baum	O
and	O
smith	O
let	O
s	O
explore	B
an	O
example	O
rational	O
prospecting	B
suppose	O
you	O
have	O
the	O
task	O
of	O
choosing	O
the	O
site	O
for	O
a	O
tanzanite	B
mine	O
your	O
action	O
will	O
be	O
to	O
select	O
the	O
site	O
from	O
a	O
list	O
of	O
n	O
sites	O
the	O
nth	O
site	O
has	O
a	O
net	O
value	O
called	O
the	O
return	O
xn	O
which	O
is	O
initially	O
unknown	O
and	O
will	O
be	O
found	O
out	O
exactly	O
only	O
after	O
site	O
n	O
has	O
been	O
chosen	O
equals	O
the	O
revenue	O
earned	O
from	O
selling	O
the	O
tanzanite	B
from	O
that	O
site	O
minus	O
the	O
costs	O
of	O
buying	O
the	O
site	O
paying	O
the	O
and	O
so	O
forth	O
at	O
the	O
outset	O
the	O
return	O
xn	O
has	O
a	O
probability	B
distribution	B
p	O
based	O
on	O
the	O
information	B
already	O
available	O
before	O
you	O
take	O
your	O
action	O
you	O
have	O
the	O
opportunity	O
to	O
do	O
some	O
prospecting	B
prospecting	B
at	O
the	O
nth	O
site	O
has	O
a	O
cost	O
cn	O
and	O
yields	O
data	O
dn	O
which	O
reduce	O
the	O
uncertainty	O
about	O
xn	O
ll	O
assume	O
that	O
the	O
returns	O
of	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decision	B
theory	I
the	O
n	O
sites	O
are	O
unrelated	O
to	O
each	O
other	O
and	O
that	O
prospecting	B
at	O
one	O
site	O
only	O
yields	O
information	B
about	O
that	O
site	O
and	O
doesn	O
t	O
the	O
return	O
from	O
that	O
site	O
your	O
decision	O
problem	O
is	O
given	O
the	O
initial	O
probability	B
distributions	O
p	O
p	O
p	O
decide	O
whether	O
to	O
prospect	O
and	O
at	O
which	O
sites	O
then	O
in	O
the	O
light	O
of	O
your	O
prospecting	B
results	O
choose	O
which	O
site	O
to	O
mine	O
for	O
simplicity	O
let	O
s	O
make	O
everything	O
in	O
the	O
problem	O
gaussian	B
and	O
focus	B
on	O
the	O
question	O
of	O
whether	O
to	O
prospect	O
once	O
or	O
not	O
we	O
ll	O
assume	O
our	O
utility	B
function	O
is	O
linear	B
in	O
xn	O
we	O
wish	O
to	O
maximize	O
our	O
expected	O
return	O
the	O
utility	B
function	O
is	O
the	O
notation	B
p	O
normaly	O
indicates	O
that	O
y	O
has	O
gaussian	B
distribution	B
with	O
mean	B
and	O
variance	B
if	O
no	O
prospecting	B
is	O
done	O
where	O
na	O
is	O
the	O
chosen	O
action	O
site	O
and	O
if	O
prospecting	B
is	O
done	O
the	O
utility	B
is	O
u	O
xna	O
where	O
np	O
is	O
the	O
site	O
at	O
which	O
prospecting	B
took	O
place	O
u	O
xna	O
the	O
prior	B
distribution	B
of	O
the	O
return	O
of	O
site	O
n	O
is	O
p	O
normalxn	O
n	O
if	O
you	O
prospect	O
at	O
site	O
n	O
the	O
datum	O
dn	O
is	O
a	O
noisy	B
version	O
of	O
xn	O
p	O
j	O
xn	O
normaldn	O
xn	O
exercise	O
given	O
these	O
assumptions	B
show	O
that	O
the	O
prior	B
probability	B
dis	O
tribution	O
of	O
dn	O
is	O
p	O
normaldn	O
n	O
when	O
independent	O
variables	O
add	O
variances	B
add	I
and	O
that	O
the	O
posterior	O
distribution	B
of	O
xn	O
given	O
dn	O
is	O
where	O
p	O
j	O
dn	O
when	O
gaussians	O
multiply	O
precisions	B
add	I
n	O
n	O
and	O
n	O
to	O
start	O
with	O
let	O
s	O
evaluate	O
the	O
expected	O
utility	B
if	O
we	O
do	O
no	O
prospecting	B
choose	O
the	O
site	O
immediately	O
then	O
we	O
ll	O
evaluate	O
the	O
expected	O
utility	B
if	O
we	O
prospect	O
at	O
one	O
site	O
and	O
then	O
make	O
our	O
choice	O
from	O
these	O
two	O
results	O
we	O
will	O
be	O
able	O
to	O
decide	O
whether	O
to	O
prospect	O
once	O
or	O
zero	O
times	O
and	O
if	O
we	O
prospect	O
once	O
at	O
which	O
site	O
so	O
we	O
consider	O
the	O
expected	O
utility	B
without	O
any	O
prospecting	B
exercise	O
show	O
that	O
the	O
optimal	B
action	O
assuming	O
no	O
prospecting	B
is	O
to	O
select	O
the	O
site	O
with	O
biggest	O
mean	B
and	O
the	O
expected	O
utility	B
of	O
this	O
action	O
is	O
na	O
argmax	O
n	O
eu	O
j	O
optimal	B
n	O
max	O
n	O
your	O
intuition	O
says	O
surely	O
the	O
optimal	B
decision	O
should	O
take	O
into	O
account	O
the	O
uncertainties	O
too	O
the	O
answer	O
to	O
this	O
question	O
is	O
reasonable	O
if	O
so	O
then	O
the	O
utility	B
function	O
should	O
be	O
nonlinear	B
in	O
x	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
reading	O
now	O
the	O
exciting	O
bit	B
should	O
we	O
prospect	O
once	O
we	O
have	O
prospected	O
at	O
site	O
np	O
we	O
will	O
choose	O
the	O
site	O
using	O
the	O
decision	O
rule	O
with	O
the	O
value	O
of	O
mean	B
replaced	O
by	O
the	O
updated	O
value	O
given	O
by	O
what	O
makes	O
the	O
problem	O
exciting	O
is	O
that	O
we	O
don	O
t	O
yet	O
know	O
the	O
value	O
of	O
dn	O
so	O
we	O
don	O
t	O
know	O
what	O
our	O
action	O
na	O
will	O
be	O
indeed	O
the	O
whole	O
value	O
of	O
doing	O
the	O
prospecting	B
comes	O
from	O
the	O
fact	O
that	O
the	O
outcome	O
dn	O
may	O
alter	O
the	O
action	O
from	O
the	O
one	O
that	O
we	O
would	O
have	O
taken	O
in	O
the	O
absence	O
of	O
the	O
experimental	O
information	B
from	O
the	O
expression	O
for	O
the	O
new	O
mean	B
in	O
terms	O
of	O
dn	O
and	O
the	O
known	O
variance	B
of	O
dn	O
we	O
can	O
compute	O
the	O
probability	B
distribution	B
of	O
the	O
key	O
quantity	O
and	O
can	O
work	O
out	O
the	O
expected	O
utility	B
by	O
integrating	O
over	O
all	O
possible	O
outcomes	O
and	O
their	O
associated	O
actions	O
exercise	O
show	O
that	O
the	O
probability	B
distribution	B
of	O
the	O
new	O
mean	B
is	O
gaussian	B
with	O
mean	B
and	O
variance	B
n	O
n	O
n	O
consider	O
prospecting	B
at	O
site	O
n	O
let	O
the	O
biggest	O
mean	B
of	O
the	O
other	O
sites	O
be	O
when	O
we	O
obtain	O
the	O
new	O
value	O
of	O
the	O
mean	B
we	O
will	O
choose	O
site	O
n	O
and	O
get	O
an	O
expected	O
return	O
of	O
if	O
and	O
we	O
will	O
choose	O
site	O
and	O
get	O
an	O
expected	O
return	O
of	O
if	O
so	O
the	O
expected	O
utility	B
of	O
prospecting	B
at	O
site	O
n	O
then	O
picking	O
the	O
best	O
site	O
is	O
eu	O
j	O
prospect	O
at	O
n	O
p	O
the	O
in	O
utility	B
between	O
prospecting	B
and	O
not	O
prospecting	B
is	O
the	O
quantity	O
of	O
interest	O
and	O
it	O
depends	O
on	O
what	O
we	O
would	O
have	O
done	O
without	O
prospecting	B
and	O
that	O
depends	O
on	O
whether	O
is	O
bigger	O
than	O
eu	O
j	O
no	O
prospecting	B
if	O
if	O
so	O
eu	O
j	O
prospect	O
at	O
n	O
eu	O
j	O
no	O
prospecting	B
if	O
if	O
we	O
can	O
plot	O
the	O
change	O
in	O
expected	O
utility	B
due	O
to	O
prospecting	B
cn	O
as	O
a	O
function	O
of	O
the	O
axis	O
and	O
the	O
initial	O
standard	B
deviation	I
axis	O
in	O
the	O
the	O
noise	O
variance	B
is	O
further	O
reading	O
if	O
the	O
world	O
in	O
which	O
we	O
act	O
is	O
a	O
little	O
more	O
complicated	O
than	O
the	O
prospecting	B
problem	O
for	O
example	O
if	O
multiple	O
iterations	O
of	O
prospecting	B
are	O
possible	O
and	O
the	O
cost	O
of	O
prospecting	B
is	O
uncertain	O
then	O
the	O
optimal	B
balance	B
between	O
exploration	O
and	O
exploitation	O
becomes	O
a	O
much	O
harder	O
computational	O
problem	O
reinforcement	B
learning	B
addresses	O
approximate	O
methods	O
for	O
this	O
problem	O
and	O
barto	O
figure	O
contour	O
plot	O
of	O
the	O
gain	B
in	O
expected	O
utility	B
due	O
to	O
prospecting	B
the	O
contours	O
are	O
equally	O
spaced	O
from	O
to	O
in	O
steps	O
of	O
to	O
decide	O
whether	O
it	O
is	O
worth	O
prospecting	B
at	O
site	O
n	O
the	O
contour	O
equal	O
to	O
cn	O
cost	O
of	O
prospecting	B
all	O
points	O
above	O
that	O
contour	O
are	O
worthwhile	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decision	B
theory	I
further	O
exercises	O
exercise	O
the	O
four	O
doors	B
problem	O
a	O
new	O
game	B
show	I
uses	O
rules	B
similar	O
to	O
those	O
of	O
the	O
three	B
doors	B
but	O
there	O
are	O
four	O
doors	B
and	O
the	O
host	O
explains	O
first	O
you	O
will	O
point	O
to	O
one	O
of	O
the	O
doors	B
and	O
then	O
i	O
will	O
open	O
one	O
of	O
the	O
other	O
doors	B
guaranteeing	O
to	O
choose	O
a	O
non-winner	O
then	O
you	O
decide	O
whether	O
to	O
stick	O
with	O
your	O
original	O
pick	O
or	O
switch	O
to	O
one	O
of	O
the	O
remaining	O
doors	B
then	O
i	O
will	O
open	O
another	O
non-winner	O
never	O
the	O
current	O
pick	O
you	O
will	O
then	O
make	O
your	O
decision	O
by	O
sticking	O
with	O
the	O
door	O
picked	O
on	O
the	O
previous	O
decision	O
or	O
by	O
switching	O
to	O
the	O
only	O
other	O
remaining	O
door	O
what	O
is	O
the	O
optimal	B
strategy	O
should	O
you	O
switch	O
on	O
the	O
opportunity	O
should	O
you	O
switch	O
on	O
the	O
second	O
opportunity	O
exercise	O
one	O
of	O
the	O
challenges	B
of	O
decision	B
theory	I
is	O
out	O
exactly	O
what	O
the	O
utility	B
function	O
is	O
the	O
utility	B
of	O
money	O
for	O
example	O
is	O
notoriously	O
nonlinear	B
for	O
most	O
people	O
in	O
fact	O
the	O
behaviour	O
of	O
many	O
people	O
cannot	O
be	O
captured	O
by	O
a	O
coherent	O
utility	B
function	O
as	O
illustrated	O
by	O
the	O
allais	B
paradox	B
which	O
runs	O
as	O
follows	O
which	O
of	O
these	O
choices	O
do	O
you	O
most	O
attractive	O
a	O
million	O
guaranteed	O
b	O
chance	O
of	O
million	O
chance	O
of	O
million	O
chance	O
of	O
nothing	O
now	O
consider	O
these	O
choices	O
c	O
d	O
chance	O
of	O
nothing	O
chance	O
of	O
million	O
chance	O
of	O
nothing	O
chance	O
of	O
million	O
many	O
people	O
prefer	O
a	O
to	O
b	O
and	O
at	O
the	O
same	O
time	O
d	O
to	O
c	O
prove	O
that	O
these	O
preferences	O
are	O
inconsistent	O
with	O
any	O
utility	B
function	O
u	O
for	O
money	O
exercise	O
optimal	B
stopping	I
a	O
large	O
queue	B
of	O
n	O
potential	O
partners	O
is	O
waiting	O
at	O
your	O
door	O
all	O
asking	O
to	O
marry	O
you	O
they	O
have	O
arrived	O
in	O
random	B
order	O
as	O
you	O
meet	O
each	O
partner	O
you	O
have	O
to	O
decide	O
on	O
the	O
spot	O
based	O
on	O
the	O
information	B
so	O
far	O
whether	O
to	O
marry	O
them	O
or	O
say	O
no	O
each	O
potential	O
partner	O
has	O
a	O
desirability	O
dn	O
which	O
you	O
out	O
if	O
and	O
when	O
you	O
meet	O
them	O
you	O
must	O
marry	O
one	O
of	O
them	O
but	O
you	O
are	O
not	O
allowed	O
to	O
go	O
back	O
to	O
anyone	O
you	O
have	O
said	O
no	O
to	O
there	O
are	O
several	O
ways	O
to	O
the	O
precise	O
problem	O
assuming	O
your	O
aim	O
is	O
to	O
maximize	O
the	O
desirability	O
dn	O
i	O
e	O
your	O
utility	B
function	O
is	O
dn	O
where	O
is	O
the	O
partner	O
selected	O
what	O
strategy	O
should	O
you	O
use	O
assuming	O
you	O
wish	O
very	O
much	O
to	O
marry	O
the	O
most	O
desirable	O
person	O
your	O
utility	B
function	O
is	O
if	O
you	O
achieve	O
that	O
and	O
zero	O
otherwise	O
what	O
strategy	O
should	O
you	O
use	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
action	O
buy	O
don	O
t	O
buy	O
outcome	O
no	O
win	O
wins	O
table	O
utility	B
in	O
the	O
lottery	O
ticket	O
problem	O
action	O
buy	O
don	O
t	O
buy	O
outcome	O
no	O
win	O
wins	O
table	O
regret	B
in	O
the	O
lottery	O
ticket	O
problem	O
assuming	O
you	O
wish	O
very	O
much	O
to	O
marry	O
the	O
most	O
desirable	O
person	O
and	O
that	O
your	O
strategy	O
will	O
be	O
strategy	O
m	O
strategy	O
m	O
meet	O
the	O
m	O
partners	O
and	O
say	O
no	O
to	O
all	O
of	O
them	O
memorize	O
the	O
maximum	O
desirability	O
dmax	O
among	O
them	O
then	O
meet	O
the	O
others	O
in	O
sequence	B
waiting	O
until	O
a	O
partner	O
with	O
dn	O
dmax	O
comes	O
along	O
and	O
marry	O
them	O
if	O
none	O
more	O
desirable	O
comes	O
along	O
marry	O
the	O
n	O
th	O
partner	O
feel	O
miserable	O
what	O
is	O
the	O
optimal	B
value	O
of	O
m	O
exercise	O
regret	B
as	O
an	O
objective	B
function	I
the	O
preceding	O
exercise	O
b	O
and	O
c	O
involved	O
a	O
utility	B
function	O
based	O
on	O
regret	B
if	O
one	O
married	O
the	O
tenth	O
most	O
desirable	O
candidate	O
the	O
utility	B
function	O
asserts	O
that	O
one	O
would	O
feel	O
regret	B
for	O
having	O
not	O
chosen	O
the	O
most	O
desirable	O
many	O
people	O
working	O
in	O
learning	B
theory	O
and	O
decision	B
theory	I
use	O
minimizing	O
the	O
maximal	O
possible	O
regret	B
as	O
an	O
objective	B
function	I
but	O
does	O
this	O
make	O
sense	O
imagine	O
that	O
fred	O
has	O
bought	O
a	O
lottery	O
ticket	O
and	O
to	O
sell	O
it	O
to	O
you	O
before	O
it	O
s	O
known	O
whether	O
the	O
ticket	O
is	O
a	O
winner	O
for	O
simplicity	O
say	O
the	O
probability	B
that	O
the	O
ticket	O
is	O
a	O
winner	O
is	O
and	O
if	O
it	O
is	O
a	O
winner	O
it	O
is	O
worth	O
fred	O
to	O
sell	O
you	O
the	O
ticket	O
for	O
do	O
you	O
buy	O
it	O
the	O
possible	O
actions	O
are	O
buy	O
and	O
don	O
t	O
buy	O
the	O
utilities	O
of	O
the	O
four	O
possible	O
actionoutcome	O
pairs	O
are	O
shown	O
in	O
table	O
i	O
have	O
assumed	O
that	O
the	O
utility	B
of	O
small	O
amounts	O
of	O
money	O
for	O
you	O
is	O
linear	B
if	O
you	O
don	O
t	O
buy	O
the	O
ticket	O
then	O
the	O
utility	B
is	O
zero	O
regardless	O
of	O
whether	O
the	O
ticket	O
proves	O
to	O
be	O
a	O
winner	O
if	O
you	O
do	O
buy	O
the	O
ticket	O
you	O
end	O
up	O
either	O
losing	O
one	O
pound	O
probability	B
or	O
gaining	O
nine	O
probability	B
in	O
the	O
minimax	B
regret	B
community	O
actions	O
are	O
chosen	O
to	O
minimize	O
the	O
maximum	O
possible	O
regret	B
the	O
four	O
possible	O
regret	B
outcomes	O
are	O
shown	O
in	O
table	O
if	O
you	O
buy	O
the	O
ticket	O
and	O
it	O
doesn	O
t	O
win	O
you	O
have	O
a	O
regret	B
of	O
because	O
if	O
you	O
had	O
not	O
bought	O
it	O
you	O
would	O
have	O
been	O
better	O
if	O
you	O
do	O
not	O
buy	O
the	O
ticket	O
and	O
it	O
wins	O
you	O
have	O
a	O
regret	B
of	O
because	O
if	O
you	O
had	O
bought	O
it	O
you	O
would	O
have	O
been	O
better	O
the	O
action	O
that	O
minimizes	O
the	O
maximum	O
possible	O
regret	B
is	O
thus	O
to	O
buy	O
the	O
ticket	O
discuss	O
whether	O
this	O
use	O
of	O
regret	B
to	O
choose	O
actions	O
can	O
be	O
philosophically	O
the	O
above	O
problem	O
can	O
be	O
turned	O
into	O
an	O
investment	B
portfolio	B
decision	O
problem	O
by	O
imagining	O
that	O
you	O
have	O
been	O
given	O
one	O
pound	O
to	O
invest	O
in	O
two	O
possible	O
funds	O
for	O
one	O
day	O
fred	O
s	O
lottery	O
fund	O
and	O
the	O
cash	O
fund	O
if	O
you	O
put	O
into	O
fred	O
s	O
lottery	O
fund	O
fred	O
promises	O
to	O
return	O
to	O
you	O
if	O
the	O
lottery	O
ticket	O
is	O
a	O
winner	O
and	O
otherwise	O
nothing	O
the	O
remaining	O
is	O
kept	O
as	O
cash	O
what	O
is	O
the	O
best	O
investment	O
show	O
that	O
the	O
minimax	B
regret	B
community	O
will	O
invest	O
of	O
their	O
money	O
in	O
the	O
high	O
risk	O
high	O
return	O
lottery	O
fund	O
and	O
only	O
in	O
cash	O
can	O
this	O
investment	O
method	O
be	O
exercise	O
gambling	B
oddities	O
cover	B
and	O
thomas	O
a	O
horse	B
race	B
involving	O
i	O
horses	O
occurs	O
repeatedly	O
and	O
you	O
are	O
obliged	O
to	O
bet	B
all	O
your	O
money	O
each	O
time	O
your	O
bet	B
at	O
time	O
t	O
can	O
be	O
represented	O
by	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decision	B
theory	I
a	O
normalized	O
probability	B
vector	O
b	O
multiplied	O
by	O
your	O
money	O
mt	O
the	O
odds	B
by	O
the	O
bookies	B
are	O
such	O
that	O
if	O
horse	O
i	O
wins	O
then	O
your	O
return	O
is	O
bioimt	O
assuming	O
the	O
bookies	B
odds	B
are	O
fair	O
that	O
is	O
oi	O
xi	O
and	O
assuming	O
that	O
the	O
probability	B
that	O
horse	O
i	O
wins	O
is	O
pi	O
work	O
out	O
the	O
optimal	B
betting	O
strategy	O
if	O
your	O
aim	O
is	O
cover	B
s	O
aim	O
namely	O
to	O
maximize	O
the	O
expected	O
value	O
of	O
log	O
mt	O
show	O
that	O
the	O
optimal	B
strategy	O
sets	O
b	O
equal	O
to	O
p	O
independent	O
of	O
the	O
bookies	B
odds	B
o	O
show	O
that	O
when	O
this	O
strategy	O
is	O
used	O
the	O
money	O
is	O
expected	O
to	O
grow	O
exponentially	O
as	O
where	O
w	O
pi	O
log	O
bioi	O
if	O
you	O
only	O
bet	B
once	O
is	O
the	O
optimal	B
strategy	O
any	O
do	O
you	O
think	O
this	O
optimal	B
strategy	O
makes	O
sense	O
do	O
you	O
think	O
that	O
it	O
s	O
optimal	B
in	O
common	O
language	O
to	O
ignore	O
the	O
bookies	B
odds	B
what	O
can	O
you	O
conclude	O
about	O
cover	B
s	O
aim	O
exercise	O
two	O
ordinary	O
dice	O
are	O
thrown	O
repeatedly	O
the	O
outcome	O
of	O
each	O
throw	O
is	O
the	O
sum	O
of	O
the	O
two	O
numbers	O
joe	O
shark	O
who	O
says	O
that	O
and	O
are	O
his	O
lucky	O
numbers	O
bets	O
even	O
money	O
that	O
a	O
will	O
be	O
thrown	O
before	O
the	O
is	O
thrown	O
if	O
you	O
were	O
a	O
gambler	O
would	O
you	O
take	O
the	O
bet	B
what	O
is	O
your	O
probability	B
of	O
winning	O
joe	O
then	O
bets	O
even	O
money	O
that	O
an	O
will	O
be	O
thrown	O
before	O
the	O
is	O
thrown	O
would	O
you	O
take	O
the	O
bet	B
having	O
gained	O
your	O
joe	O
suggests	O
combining	O
the	O
two	O
bets	O
into	O
a	O
single	O
bet	B
he	O
bets	O
a	O
larger	O
sum	O
still	O
at	O
even	O
odds	B
that	O
an	O
and	O
a	O
will	O
be	O
thrown	O
before	O
two	O
have	O
been	O
thrown	O
would	O
you	O
take	O
the	O
bet	B
what	O
is	O
your	O
probability	B
of	O
winning	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bayesian	B
inference	B
and	O
sampling	B
theory	I
there	O
are	O
two	O
schools	O
of	O
statistics	O
sampling	O
theorists	O
concentrate	O
on	O
having	O
methods	O
guaranteed	O
to	O
work	O
most	O
of	O
the	O
time	O
given	O
minimal	O
assumptions	B
bayesians	O
try	O
to	O
make	O
inferences	O
that	O
take	O
into	O
account	O
all	O
available	O
information	B
and	O
answer	O
the	O
question	O
of	O
interest	O
given	O
the	O
particular	O
data	B
set	B
as	O
you	O
have	O
probably	O
gathered	O
i	O
strongly	O
recommend	O
the	O
use	O
of	O
bayesian	B
methods	O
sampling	B
theory	I
is	O
the	O
widely	O
used	O
approach	O
to	O
statistics	O
and	O
most	O
papers	O
in	O
most	O
journals	O
report	O
their	O
experiments	O
using	O
quantities	O
like	O
intervals	B
levels	O
and	O
p-values	O
a	O
p-value	B
p	O
is	O
the	O
probability	B
given	O
a	O
null	O
hypothesis	O
for	O
the	O
probability	B
distribution	B
of	O
the	O
data	O
that	O
the	O
outcome	O
would	O
be	O
as	O
extreme	O
as	O
or	O
more	O
extreme	O
than	O
the	O
observed	O
outcome	O
untrained	O
readers	O
and	O
perhaps	O
more	O
worryingly	O
the	O
authors	O
of	O
many	O
papers	O
usually	O
interpret	O
such	O
a	O
p-value	B
as	O
if	O
it	O
is	O
a	O
bayesian	B
probability	B
example	O
the	O
posterior	B
probability	B
of	O
the	O
null	O
hypothesis	O
an	O
interpretation	O
that	O
both	O
sampling	O
theorists	O
and	O
bayesians	O
would	O
agree	O
is	O
incorrect	O
in	O
this	O
chapter	O
we	O
study	O
a	O
couple	O
of	O
simple	O
inference	B
problems	O
in	O
order	O
to	O
compare	O
these	O
two	O
approaches	O
to	O
statistics	O
while	O
in	O
some	O
cases	O
the	O
answers	O
from	O
a	O
bayesian	B
approach	O
and	O
from	O
sampling	B
theory	I
are	O
very	O
similar	O
we	O
can	O
also	O
cases	O
where	O
there	O
are	O
we	O
have	O
already	O
seen	O
such	O
an	O
example	O
in	O
exercise	O
where	O
a	O
sampling	O
theorist	O
got	O
a	O
p-value	B
smaller	O
than	O
and	O
viewed	O
this	O
as	O
strong	O
evidence	B
against	O
the	O
null	O
hypothesis	O
whereas	O
the	O
data	O
actually	O
favoured	O
the	O
null	O
hypothesis	O
over	O
the	O
simplest	O
alternative	O
on	O
another	O
example	O
was	O
given	O
where	O
the	O
p-value	B
was	O
smaller	O
than	O
the	O
mystical	O
value	O
of	O
yet	O
the	O
data	O
again	O
favoured	O
the	O
null	O
hypothesis	O
thus	O
in	O
some	O
cases	O
sampling	B
theory	I
can	O
be	O
trigger-happy	O
declaring	O
results	O
to	O
be	O
improbable	O
that	O
the	O
null	O
hypothesis	O
should	O
be	O
rejected	O
when	O
those	O
results	O
actually	O
weakly	O
support	O
the	O
null	O
hypothesis	O
as	O
we	O
will	O
now	O
see	O
there	O
are	O
also	O
inference	B
problems	O
where	O
sampling	B
theory	I
fails	O
to	O
detect	O
evidence	B
where	O
a	O
bayesian	B
approach	O
and	O
everyday	O
intuition	O
agree	O
that	O
the	O
evidence	B
is	O
strong	O
most	O
telling	O
of	O
all	O
are	O
the	O
inference	B
problems	O
where	O
the	O
assigned	O
by	O
sampling	B
theory	I
changes	O
depending	O
on	O
irrelevant	O
factors	O
concerned	O
with	O
the	O
design	O
of	O
the	O
experiment	O
this	O
chapter	O
is	O
only	O
provided	O
for	O
those	O
readers	O
who	O
are	O
curious	O
about	O
the	O
sampling	B
theory	I
bayesian	B
methods	O
debate	O
if	O
you	O
any	O
of	O
this	O
chapter	O
tough	O
to	O
understand	O
please	O
skip	O
it	O
there	O
is	O
no	O
point	O
trying	O
to	O
understand	O
the	O
debate	O
just	O
use	O
bayesian	B
methods	O
they	O
are	O
much	O
easier	O
to	O
understand	O
than	O
the	O
debate	O
itself	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bayesian	B
inference	B
and	O
sampling	B
theory	I
a	O
medical	O
example	O
we	O
are	O
trying	O
to	O
reduce	O
the	O
incidence	O
of	O
an	O
unpleasant	O
disease	B
called	O
microsoftus	B
two	O
vaccinations	O
a	O
and	O
b	O
are	O
tested	O
on	O
a	O
group	O
of	O
volunteers	O
vaccination	B
b	O
is	O
a	O
control	B
treatment	I
a	O
placebo	O
treatment	O
with	O
no	O
active	O
ingredients	O
of	O
the	O
subjects	O
are	O
randomly	O
assigned	O
to	O
have	O
treatment	O
a	O
and	O
the	O
other	O
are	O
given	O
the	O
control	B
treatment	I
b	O
we	O
observe	O
the	O
subjects	O
for	O
one	O
year	O
after	O
their	O
vaccinations	O
of	O
the	O
in	O
group	O
a	O
one	O
contracts	O
microsoftus	B
of	O
the	O
in	O
group	O
b	O
three	O
contract	O
microsoftus	B
is	O
treatment	O
a	O
better	O
than	O
treatment	O
b	O
sampling	B
theory	I
has	O
a	O
go	O
the	O
standard	O
sampling	B
theory	I
approach	O
to	O
the	O
question	O
is	O
a	O
better	O
than	O
b	O
is	O
to	O
construct	O
a	O
statistical	B
test	I
the	O
test	O
usually	O
compares	O
a	O
hypothesis	O
such	O
as	O
a	O
and	O
b	O
have	O
with	O
a	O
null	O
hypothesis	O
such	O
as	O
a	O
and	O
b	O
have	O
exactly	O
the	O
same	O
as	O
each	O
other	O
a	O
novice	O
might	O
object	O
no	O
no	O
i	O
want	O
to	O
compare	O
the	O
hypothesis	O
is	O
better	O
than	O
b	O
with	O
the	O
alternative	O
is	O
better	O
than	O
a	O
but	O
such	O
objections	O
are	O
not	O
welcome	O
in	O
sampling	B
theory	I
once	O
the	O
two	O
hypotheses	O
have	O
been	O
the	O
hypothesis	O
is	O
scarcely	O
mentioned	O
again	O
attention	O
focuses	O
solely	O
on	O
the	O
null	O
hypothesis	O
it	O
makes	O
me	O
laugh	O
to	O
write	O
this	O
but	O
it	O
s	O
true	O
the	O
null	O
hypothesis	O
is	O
accepted	O
or	O
rejected	O
purely	O
on	O
the	O
basis	O
of	O
how	O
unexpected	O
the	O
data	O
were	O
to	O
not	O
on	O
how	O
much	O
better	O
predicted	O
the	O
data	O
one	O
chooses	O
a	O
statistic	B
which	O
measures	O
how	O
much	O
a	O
data	B
set	B
deviates	O
from	O
the	O
null	O
hypothesis	O
in	O
the	O
example	O
here	O
the	O
standard	O
statistic	B
to	O
use	O
would	O
be	O
one	O
called	O
to	O
compute	O
we	O
take	O
the	O
between	O
each	O
data	O
measurement	O
and	O
its	O
expected	O
value	O
assuming	O
the	O
null	O
hypothesis	O
to	O
be	O
true	O
and	O
divide	O
the	O
square	B
of	O
that	O
by	O
the	O
variance	B
of	O
the	O
measurement	O
assuming	O
the	O
null	O
hypothesis	O
to	O
be	O
true	O
in	O
the	O
present	O
problem	O
the	O
four	O
data	O
measurements	O
are	O
the	O
integers	O
fa	O
fb	O
and	O
that	O
is	O
the	O
number	O
of	O
subjects	O
given	O
treatment	O
a	O
who	O
contracted	O
microsoftus	B
the	O
number	O
of	O
subjects	O
given	O
treatment	O
a	O
who	O
didn	O
t	O
and	O
so	O
forth	O
the	O
of	O
is	O
hfii	O
actually	O
in	O
my	O
elementary	O
statistics	O
book	O
i	O
yates	O
s	O
correction	O
is	O
recommended	O
hfiij	O
hfii	O
in	O
this	O
case	O
given	O
the	O
null	O
hypothesis	O
that	O
treatments	O
a	O
and	O
b	O
are	O
equally	O
and	O
have	O
rates	O
f	O
and	O
for	O
the	O
two	O
outcomes	O
the	O
expected	O
counts	O
are	O
hfaifna	O
hfbifnb	O
if	O
you	O
want	O
to	O
know	O
about	O
yates	O
s	O
correction	O
read	O
a	O
sampling	B
theory	I
textbook	O
the	O
point	O
of	O
this	O
chapter	O
is	O
not	O
to	O
teach	O
sampling	B
theory	I
i	O
merely	O
mention	O
yates	O
s	O
correction	O
because	O
it	O
is	O
what	O
a	O
professional	O
sampling	O
theorist	O
might	O
use	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
medical	O
example	O
the	O
sampling	B
distribution	B
of	O
a	O
statistic	B
is	O
the	O
probability	B
distribution	B
of	O
its	O
value	O
under	O
repetitions	O
of	O
the	O
experiment	O
assuming	O
that	O
the	O
null	O
hypothesis	O
is	O
true	O
the	O
test	O
accepts	O
or	O
rejects	O
the	O
null	O
hypothesis	O
on	O
the	O
basis	O
of	O
how	O
big	O
is	O
to	O
make	O
this	O
test	O
precise	O
and	O
give	O
it	O
a	O
level	O
we	O
have	O
to	O
work	O
out	O
what	O
the	O
sampling	B
distribution	B
of	O
is	O
taking	O
into	O
account	O
the	O
fact	O
that	O
the	O
four	O
data	O
points	O
are	O
not	O
independent	O
satisfy	O
the	O
two	O
constraints	O
fa	O
na	O
and	O
fb	O
nb	O
and	O
the	O
fact	O
that	O
the	O
parameters	B
are	O
not	O
known	O
these	O
three	O
constraints	O
reduce	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
data	O
from	O
four	O
to	O
one	O
you	O
want	O
to	O
learn	O
more	O
about	O
computing	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
read	O
a	O
sampling	B
theory	I
book	O
in	O
bayesian	B
methods	O
we	O
don	O
t	O
need	O
to	O
know	O
all	O
that	O
and	O
quantities	O
equivalent	O
to	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
pop	O
straight	O
out	O
of	O
a	O
bayesian	B
analysis	O
when	O
they	O
are	O
appropriate	O
these	O
sampling	O
distributions	O
are	O
tabulated	O
by	O
sampling	B
theory	I
gnomes	O
and	O
come	O
accompanied	O
by	O
warnings	O
about	O
the	O
conditions	O
under	O
which	O
they	O
are	O
accurate	O
for	O
example	O
standard	O
tabulated	O
distributions	O
for	O
are	O
only	O
accurate	O
if	O
the	O
expected	O
numbers	O
fi	O
are	O
about	O
or	O
more	O
once	O
the	O
data	O
arrive	O
sampling	O
theorists	O
estimate	O
the	O
unknown	O
parameters	B
of	O
the	O
null	O
hypothesis	O
from	O
the	O
data	O
fa	O
fb	O
na	O
nb	O
na	O
nb	O
and	O
evaluate	O
at	O
this	O
point	O
the	O
sampling	B
theory	I
school	O
divides	O
itself	O
into	O
two	O
camps	O
one	O
camp	O
uses	O
the	O
following	O
protocol	B
before	O
looking	O
at	O
the	O
data	O
pick	O
the	O
level	O
of	O
the	O
test	O
and	O
determine	O
the	O
critical	O
value	O
of	O
above	O
which	O
the	O
null	O
hypothesis	O
will	O
be	O
rejected	O
level	O
is	O
the	O
fraction	O
of	O
times	O
that	O
the	O
statistic	B
would	O
exceed	O
the	O
critical	O
value	O
if	O
the	O
null	O
hypothesis	O
were	O
true	O
then	O
evaluate	O
compare	O
with	O
the	O
critical	O
value	O
and	O
declare	O
the	O
outcome	O
of	O
the	O
test	O
and	O
its	O
level	O
was	O
beforehand	O
the	O
second	O
camp	O
looks	O
at	O
the	O
data	O
then	O
looks	O
in	O
the	O
table	O
of	O
for	O
the	O
level	O
p	O
for	O
which	O
the	O
observed	O
value	O
of	O
would	O
be	O
the	O
critical	O
value	O
the	O
result	O
of	O
the	O
test	O
is	O
then	O
reported	O
by	O
giving	O
this	O
value	O
of	O
p	O
which	O
is	O
the	O
fraction	O
of	O
times	O
that	O
a	O
result	O
as	O
extreme	O
as	O
the	O
one	O
observed	O
or	O
more	O
extreme	O
would	O
be	O
expected	O
to	O
arise	O
if	O
the	O
null	O
hypothesis	O
were	O
true	O
let	O
s	O
apply	O
these	O
two	O
methods	O
first	O
camp	O
cance	O
level	O
the	O
critical	O
value	O
for	O
with	O
one	O
degree	B
of	O
freedom	O
is	O
the	O
estimated	O
values	O
of	O
are	O
let	O
s	O
pick	O
as	O
our	O
the	O
expected	O
values	O
of	O
the	O
four	O
measurements	O
are	O
f	O
hfai	O
hfbi	O
and	O
in	O
equation	O
is	O
since	O
this	O
value	O
exceeds	O
we	O
reject	O
the	O
null	O
hypothesis	O
that	O
the	O
two	O
treatments	O
are	O
equivalent	O
at	O
the	O
level	O
however	O
if	O
we	O
use	O
yates	O
s	O
correction	O
we	O
and	O
therefore	O
accept	O
the	O
null	O
hypothesis	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bayesian	B
inference	B
and	O
sampling	B
theory	I
camp	O
two	O
runs	O
a	O
across	O
the	O
table	O
found	O
at	O
the	O
back	O
of	O
any	O
good	B
and	O
interpolating	O
between	O
sampling	B
theory	I
book	O
and	O
camp	O
two	O
reports	O
the	O
p-value	B
is	O
p	O
notice	O
that	O
this	O
answer	O
does	O
not	O
say	O
how	O
much	O
more	O
a	O
is	O
than	O
b	O
it	O
simply	O
says	O
that	O
a	O
is	O
from	O
b	O
and	O
here	O
means	O
only	O
statistically	O
not	O
practically	O
the	O
man	O
in	O
the	O
street	O
reading	O
the	O
statement	O
that	O
the	O
treatment	O
was	O
from	O
the	O
control	O
might	O
come	O
to	O
the	O
conclusion	O
that	O
there	O
is	O
a	O
chance	O
that	O
the	O
treatments	O
in	O
but	O
what	O
p	O
actually	O
means	O
is	O
if	O
you	O
did	O
this	O
experiment	O
many	O
times	O
and	O
the	O
two	O
treatments	O
had	O
equal	O
then	O
of	O
the	O
time	O
you	O
would	O
a	O
value	O
of	O
more	O
extreme	O
than	O
the	O
one	O
that	O
happened	O
here	O
this	O
has	O
almost	O
nothing	O
to	O
do	O
with	O
what	O
we	O
want	O
to	O
know	O
which	O
is	O
how	O
likely	O
it	O
is	O
that	O
treatment	O
a	O
is	O
better	O
than	O
b	O
let	O
me	O
through	O
i	O
m	O
a	O
bayesian	B
ok	O
now	O
let	O
s	O
infer	O
what	O
we	O
really	O
want	O
to	O
know	O
we	O
scrap	O
the	O
hypothesis	O
that	O
the	O
two	O
treatments	O
have	O
exactly	O
equal	O
since	O
we	O
do	O
not	O
believe	O
it	O
there	O
are	O
two	O
unknown	O
parameters	B
pa	O
and	O
pb	O
which	O
are	O
the	O
probabilities	O
that	O
people	O
given	O
treatments	O
a	O
and	O
b	O
respectively	O
contract	O
the	O
disease	B
given	O
the	O
data	O
we	O
can	O
infer	O
these	O
two	O
probabilities	O
and	O
we	O
can	O
answer	O
questions	O
of	O
interest	O
by	O
examining	O
the	O
posterior	O
distribution	B
the	O
posterior	O
distribution	B
is	O
p	O
pb	O
jffig	O
p	O
pa	O
pbp	O
pb	O
p	O
the	O
likelihood	B
function	O
is	O
p	O
pa	O
pb	O
na	O
nb	O
a	O
b	O
what	O
prior	B
distribution	B
should	O
we	O
use	O
the	O
prior	B
distribution	B
gives	O
us	O
the	O
opportunity	O
to	O
include	O
knowledge	O
from	O
other	O
experiments	O
or	O
a	O
prior	B
belief	B
that	O
the	O
two	O
parameters	B
pa	O
and	O
pb	O
while	O
from	O
each	O
other	O
are	O
expected	O
to	O
have	O
similar	O
values	O
here	O
we	O
will	O
use	O
the	O
simplest	O
vanilla	O
prior	B
distribution	B
a	O
uniform	O
distri	O
bution	O
over	O
each	O
parameter	O
p	O
pb	O
we	O
can	O
now	O
plot	O
the	O
posterior	O
distribution	B
given	O
the	O
assumption	O
of	O
a	O
separable	O
prior	B
on	O
pa	O
and	O
pb	O
the	O
posterior	O
distribution	B
is	O
also	O
separable	O
p	O
pb	O
jffig	O
p	O
j	O
fa	O
j	O
fb	O
the	O
two	O
posterior	O
distributions	O
are	O
shown	O
in	O
the	O
graphs	O
are	O
not	O
normalized	O
and	O
the	O
joint	B
posterior	B
probability	B
is	O
shown	O
in	O
if	O
we	O
want	O
to	O
know	O
the	O
answer	O
to	O
the	O
question	O
how	O
probable	O
is	O
it	O
that	O
pa	O
is	O
smaller	O
than	O
pb	O
we	O
can	O
answer	O
exactly	O
that	O
question	O
by	O
computing	O
the	O
posterior	B
probability	B
p	O
pb	O
j	O
data	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
medical	O
example	O
figure	O
posterior	O
probabilities	O
of	O
the	O
two	O
treatment	O
a	O
solid	O
line	O
b	O
dotted	O
line	O
figure	O
joint	B
posterior	B
probability	B
of	O
the	O
two	O
contour	O
plot	O
and	O
surface	O
plot	O
pb	O
pa	O
which	O
is	O
the	O
integral	B
of	O
the	O
joint	B
posterior	B
probability	B
p	O
pb	O
j	O
data	O
shown	O
in	O
over	O
the	O
region	O
in	O
which	O
pa	O
pb	O
i	O
e	O
the	O
shaded	O
triangle	B
in	O
the	O
value	O
of	O
this	O
integral	B
by	O
a	O
straightforward	O
numerical	O
integration	O
of	O
the	O
likelihood	B
function	O
over	O
the	O
relevant	O
region	O
is	O
p	O
pb	O
j	O
data	O
thus	O
there	O
is	O
a	O
chance	O
given	O
the	O
data	O
and	O
our	O
prior	B
assumptions	B
that	O
treatment	O
a	O
is	O
superior	O
to	O
treatment	O
b	O
in	O
conclusion	O
according	O
to	O
our	O
bayesian	B
model	B
the	O
data	O
out	O
of	O
contracted	O
the	O
disease	B
after	O
vaccination	B
a	O
and	O
out	O
of	O
contracted	O
the	O
disease	B
after	O
vaccination	B
b	O
give	O
very	O
strong	O
evidence	B
about	O
to	O
one	O
that	O
treatment	O
a	O
is	O
superior	O
to	O
treatment	O
b	O
in	O
the	O
bayesian	B
approach	O
it	O
is	O
also	O
easy	O
to	O
answer	O
other	O
relevant	O
questions	O
for	O
example	O
if	O
we	O
want	O
to	O
know	O
how	O
likely	O
is	O
it	O
that	O
treatment	O
a	O
is	O
ten	O
times	O
more	O
than	O
treatment	O
b	O
we	O
can	O
integrate	O
the	O
joint	B
posterior	B
probability	B
p	O
pb	O
j	O
data	O
over	O
the	O
region	O
in	O
which	O
pa	O
pb	O
model	B
comparison	I
pb	O
pa	O
figure	O
the	O
proposition	O
pa	O
pb	O
is	O
true	O
for	O
all	O
points	O
in	O
the	O
shaded	O
triangle	B
to	O
the	O
probability	B
of	O
this	O
proposition	O
we	O
integrate	O
the	O
joint	B
posterior	B
probability	B
p	O
pb	O
j	O
data	O
over	O
this	O
region	O
pb	O
if	O
there	O
were	O
a	O
situation	O
in	O
which	O
we	O
really	O
did	O
want	O
to	O
compare	O
the	O
two	O
hypotheses	O
pa	O
pb	O
and	O
pa	O
pb	O
we	O
can	O
of	O
course	O
do	O
this	O
directly	O
with	O
bayesian	B
methods	O
also	O
as	O
an	O
example	O
consider	O
the	O
data	B
set	B
d	O
one	O
subject	O
given	O
treatment	O
a	O
subsequently	O
contracted	O
microsoftus	B
one	O
subject	O
given	O
treatment	O
b	O
did	O
not	O
pa	O
figure	O
the	O
proposition	O
pa	O
pb	O
is	O
true	O
for	O
all	O
points	O
in	O
the	O
shaded	O
triangle	B
treatment	O
a	O
b	O
got	O
disease	B
did	O
not	O
total	O
treated	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bayesian	B
inference	B
and	O
sampling	B
theory	I
how	O
strongly	O
does	O
this	O
data	B
set	B
favour	O
over	O
we	O
answer	O
this	O
question	O
by	O
computing	O
the	O
evidence	B
for	O
each	O
hypothesis	O
let	O
s	O
assume	O
uniform	O
priors	O
over	O
the	O
unknown	O
parameters	B
of	O
the	O
models	O
the	O
hypothesis	O
pa	O
pb	O
has	O
just	O
one	O
unknown	O
parameter	O
let	O
s	O
call	O
it	O
p	O
we	O
ll	O
use	O
the	O
uniform	O
prior	B
over	O
the	O
two	O
parameters	B
of	O
model	B
that	O
we	O
used	O
before	O
p	O
p	O
p	O
pb	O
pa	O
pb	O
now	O
the	O
probability	B
of	O
the	O
data	O
d	O
under	O
model	B
is	O
the	O
normalizing	O
constant	O
from	O
the	O
inference	B
of	O
p	O
given	O
d	O
p	O
z	O
dp	O
p	O
j	O
pp	O
z	O
dp	O
p	O
the	O
probability	B
of	O
the	O
data	O
d	O
under	O
model	B
is	O
given	O
by	O
a	O
simple	O
twodimensional	O
integral	B
p	O
z	O
z	O
dpa	O
dpb	O
p	O
j	O
pa	O
pbp	O
pb	O
z	O
dpa	O
pa	O
z	O
dpb	O
pb	O
thus	O
the	O
evidence	B
ratio	O
in	O
favour	O
of	O
model	B
which	O
asserts	O
that	O
the	O
two	O
are	O
unequal	O
is	O
p	O
p	O
so	O
if	O
the	O
prior	B
probability	B
over	O
the	O
two	O
hypotheses	O
was	O
the	O
posterior	B
probability	B
is	O
in	O
favour	O
of	O
is	O
it	O
not	O
easy	O
to	O
get	O
sensible	O
answers	O
to	O
well-posed	O
questions	O
using	O
bayesian	B
methods	O
sampling	B
theory	I
answer	O
to	O
this	O
question	O
would	O
involve	O
the	O
identical	O
test	O
that	O
was	O
used	O
in	O
the	O
preceding	O
problem	O
that	O
test	O
would	O
yield	O
a	O
not	O
result	O
i	O
think	O
it	O
is	O
greatly	O
preferable	O
to	O
acknowledge	O
what	O
is	O
obvious	O
to	O
the	O
intuition	O
namely	O
that	O
the	O
data	O
d	O
do	O
give	O
weak	O
evidence	B
in	O
favour	O
of	O
bayesian	B
methods	O
quantify	O
how	O
weak	O
the	O
evidence	B
is	O
dependence	O
of	O
p-values	O
on	O
irrelevant	O
information	B
in	O
an	O
expensive	O
laboratory	O
dr	O
bloggs	O
tosses	O
a	O
coin	B
labelled	O
a	O
and	O
b	O
twelve	O
times	O
and	O
the	O
outcome	O
is	O
the	O
string	O
aaabaaaabaab	O
which	O
contains	O
three	O
bs	O
and	O
nine	O
as	O
what	O
evidence	B
do	O
these	O
data	O
give	O
that	O
the	O
coin	B
is	O
biased	O
in	O
favour	O
of	O
a	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
dependence	O
of	O
p-values	O
on	O
irrelevant	O
information	B
dr	O
bloggs	O
consults	O
his	O
sampling	B
theory	I
friend	O
who	O
says	O
let	O
r	O
be	O
the	O
number	O
of	O
bs	O
and	O
n	O
be	O
the	O
total	O
number	O
of	O
tosses	O
i	O
view	O
r	O
as	O
the	O
random	B
variable	I
and	O
the	O
probability	B
of	O
r	O
taking	O
on	O
the	O
value	O
r	O
or	O
a	O
more	O
extreme	B
value	I
assuming	O
the	O
null	O
hypothesis	O
pa	O
to	O
be	O
true	O
he	O
thus	O
computes	O
p	O
n	O
n	O
and	O
reports	O
at	O
the	O
level	O
of	O
there	O
is	O
not	O
evidence	B
of	O
bias	B
in	O
favour	O
of	O
a	O
or	O
if	O
the	O
friend	O
prefers	O
to	O
report	O
p-values	O
rather	O
than	O
simply	O
compare	O
p	O
with	O
he	O
would	O
report	O
the	O
p-value	B
is	O
which	O
is	O
not	O
conventionally	O
viewed	O
as	O
small	O
if	O
a	O
two-tailed	O
test	O
seemed	O
more	O
appropriate	O
he	O
might	O
compute	O
the	O
two-tailed	O
area	O
which	O
is	O
twice	O
the	O
above	O
probability	B
and	O
report	O
the	O
p-value	B
is	O
which	O
is	O
not	O
small	O
we	O
won	O
t	O
focus	B
on	O
the	O
issue	O
of	O
the	O
choice	O
between	O
the	O
one-tailed	O
and	O
two-tailed	O
tests	O
as	O
we	O
have	O
bigger	O
to	O
catch	O
dr	O
bloggs	O
pays	O
careful	O
attention	O
to	O
the	O
calculation	O
and	O
responds	O
no	O
no	O
the	O
random	B
variable	I
in	O
the	O
experiment	O
was	O
not	O
r	O
i	O
decided	O
before	O
running	O
the	O
experiment	O
that	O
i	O
would	O
keep	O
tossing	O
the	O
coin	B
until	O
i	O
saw	O
three	O
bs	O
the	O
random	B
variable	I
is	O
thus	O
n	O
such	O
experimental	O
designs	O
are	O
not	O
unusual	O
in	O
my	O
experiments	O
on	O
errorcorrecting	O
codes	O
i	O
often	O
simulate	O
the	O
decoding	B
of	O
a	O
code	O
until	O
a	O
chosen	O
number	O
r	O
of	O
block	B
errors	B
has	O
occurred	O
since	O
the	O
error	O
on	O
the	O
inferred	O
value	O
of	O
log	O
pb	O
goes	O
roughly	O
as	O
pr	O
independent	O
of	O
n	O
exercise	O
find	O
the	O
bayesian	B
inference	B
about	O
the	O
bias	B
pa	O
of	O
the	O
coin	B
given	O
the	O
data	O
and	O
determine	O
whether	O
a	O
bayesian	B
s	O
inferences	O
depend	O
on	O
what	O
stopping	B
rule	I
was	O
in	O
force	O
according	O
to	O
sampling	B
theory	I
a	O
calculation	O
is	O
required	O
in	O
order	O
to	O
assess	O
the	O
of	O
the	O
result	O
n	O
the	O
probability	B
distribution	B
of	O
n	O
given	O
is	O
the	O
probability	B
that	O
the	O
tosses	O
contain	O
exactly	O
bs	O
and	O
then	O
the	O
nth	O
toss	O
is	O
a	O
b	O
p	O
r	O
n	O
the	O
sampling	O
theorist	O
thus	O
computes	O
p	O
r	O
he	O
reports	O
back	O
to	O
dr	O
bloggs	O
the	O
p-value	B
is	O
there	O
is	O
evidence	B
of	O
bias	B
after	O
all	O
what	O
do	O
you	O
think	O
dr	O
bloggs	O
should	O
do	O
should	O
he	O
publish	O
the	O
result	O
with	O
this	O
marvellous	O
p-value	B
in	O
one	O
of	O
the	O
journals	O
that	O
insists	O
that	O
all	O
experimental	O
results	O
have	O
their	O
assessed	O
using	O
sampling	B
theory	I
or	O
should	O
he	O
boot	O
the	O
sampling	O
theorist	O
out	O
of	O
the	O
door	O
and	O
seek	O
a	O
coherent	O
method	O
of	O
assessing	O
one	O
that	O
does	O
not	O
depend	O
on	O
the	O
stopping	B
rule	I
at	O
this	O
point	O
the	O
audience	O
divides	O
in	O
two	O
half	O
the	O
audience	O
intuitively	O
feel	O
that	O
the	O
stopping	B
rule	I
is	O
irrelevant	O
and	O
don	O
t	O
need	O
any	O
convincing	O
that	O
the	O
answer	O
to	O
exercise	O
is	O
the	O
inferences	O
about	O
pa	O
do	O
not	O
depend	O
on	O
the	O
stopping	B
rule	I
the	O
other	O
half	O
perhaps	O
on	O
account	O
of	O
a	O
thorough	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bayesian	B
inference	B
and	O
sampling	B
theory	I
training	O
in	O
sampling	B
theory	I
intuitively	O
feel	O
that	O
dr	O
bloggs	O
s	O
stopping	B
rule	I
which	O
stopped	O
tossing	O
the	O
moment	O
the	O
third	O
b	O
appeared	O
may	O
have	O
biased	O
the	O
experiment	O
somehow	O
if	O
you	O
are	O
in	O
the	O
second	O
group	O
i	O
encourage	O
you	O
to	O
on	O
the	O
situation	O
and	O
hope	O
you	O
ll	O
eventually	O
come	O
round	O
to	O
the	O
view	O
that	O
is	O
consistent	O
with	O
the	O
likelihood	B
principle	I
which	O
is	O
that	O
the	O
stopping	B
rule	I
is	O
not	O
relevant	O
to	O
what	O
we	O
have	O
learned	O
about	O
pa	O
as	O
a	O
thought	O
experiment	O
consider	O
some	O
onlookers	O
who	O
order	O
to	O
save	O
money	O
are	O
spying	O
on	O
dr	O
bloggs	O
s	O
experiments	O
each	O
time	O
he	O
tosses	O
the	O
coin	B
the	O
spies	O
update	O
the	O
values	O
of	O
r	O
and	O
n	O
the	O
spies	O
are	O
eager	O
to	O
make	O
inferences	O
from	O
the	O
data	O
as	O
soon	O
as	O
each	O
new	O
result	O
occurs	O
should	O
the	O
spies	O
beliefs	O
about	O
the	O
bias	B
of	O
the	O
coin	B
depend	O
on	O
dr	O
bloggs	O
s	O
intentions	O
regarding	O
the	O
continuation	O
of	O
the	O
experiment	O
the	O
fact	O
that	O
the	O
p-values	O
of	O
sampling	B
theory	I
do	O
depend	O
on	O
the	O
stopping	B
rule	I
whole	O
volumes	O
of	O
the	O
sampling	B
theory	I
literature	O
are	O
concerned	O
with	O
the	O
task	O
of	O
assessing	O
when	O
a	O
complicated	O
stopping	B
rule	I
is	O
required	O
sequential	O
probability	B
ratio	O
tests	O
for	O
example	O
seems	O
to	O
me	O
a	O
compelling	O
argument	O
for	O
having	O
nothing	O
to	O
do	O
with	O
p-values	O
at	O
all	O
a	O
bayesian	B
solution	O
to	O
this	O
inference	B
problem	O
was	O
given	O
in	O
sections	O
and	O
and	O
exercise	O
would	O
it	O
help	O
clarify	O
this	O
issue	O
if	O
i	O
added	O
one	O
more	O
scene	O
to	O
the	O
story	O
the	O
janitor	B
who	O
s	O
been	O
eavesdropping	O
on	O
dr	O
bloggs	O
s	O
conversation	O
comes	O
in	O
and	O
says	O
i	O
happened	O
to	O
notice	O
that	O
just	O
after	O
you	O
stopped	O
doing	O
the	O
experiments	O
on	O
the	O
coin	B
the	O
for	O
whimsical	O
departmental	O
rules	B
ordered	O
the	O
immediate	O
destruction	O
of	O
all	O
such	O
coins	O
your	O
coin	B
was	O
therefore	O
destroyed	O
by	O
the	O
departmental	O
safety	O
there	O
is	O
no	O
way	O
you	O
could	O
have	O
continued	O
the	O
experiment	O
much	O
beyond	O
n	O
tosses	O
seems	O
to	O
me	O
you	O
need	O
to	O
recompute	O
your	O
p-value	B
intervals	B
in	O
an	O
experiment	O
in	O
which	O
data	O
d	O
are	O
obtained	O
from	O
a	O
system	O
with	O
an	O
unknown	O
parameter	O
a	O
standard	O
concept	O
in	O
sampling	B
theory	I
is	O
the	O
idea	O
of	O
a	O
interval	O
for	O
such	O
an	O
interval	O
has	O
associated	O
with	O
it	O
a	O
level	O
such	O
as	O
which	O
is	O
informally	O
interpreted	O
as	O
the	O
probability	B
that	O
lies	O
in	O
the	O
interval	O
let	O
s	O
make	O
precise	O
what	O
the	O
level	O
really	O
means	O
then	O
give	O
an	O
example	O
a	O
interval	O
is	O
a	O
function	O
of	O
the	O
data	B
set	B
d	O
the	O
level	O
of	O
the	O
interval	O
is	O
a	O
property	O
that	O
we	O
can	O
compute	O
before	O
the	O
data	O
arrive	O
we	O
imagine	O
generating	O
many	O
data	O
sets	O
from	O
a	O
particular	O
true	O
value	O
of	O
and	O
calculating	O
the	O
interval	O
and	O
then	O
checking	O
whether	O
the	O
true	O
value	O
of	O
lies	O
in	O
that	O
interval	O
if	O
averaging	O
over	O
all	O
these	O
imagined	O
repetitions	O
of	O
the	O
experiment	O
the	O
true	O
value	O
of	O
lies	O
in	O
the	O
interval	O
a	O
fraction	O
f	O
of	O
the	O
time	O
and	O
this	O
property	O
holds	O
for	O
all	O
true	O
values	O
of	O
then	O
the	O
level	O
of	O
the	O
interval	O
is	O
f	O
for	O
example	O
if	O
is	O
the	O
mean	B
of	O
a	O
gaussian	B
distribution	B
which	O
is	O
known	O
to	O
have	O
standard	B
deviation	I
and	O
d	O
is	O
a	O
sample	B
from	I
that	O
gaussian	B
then	O
is	O
a	O
interval	O
for	O
let	O
us	O
now	O
look	O
at	O
a	O
simple	O
example	O
where	O
the	O
meaning	O
of	O
the	O
level	O
becomes	O
clearer	O
let	O
the	O
parameter	O
be	O
an	O
integer	O
and	O
let	O
the	O
data	O
be	O
a	O
pair	O
of	O
points	O
drawn	O
independently	O
from	O
the	O
following	O
distribution	B
p	O
x	O
x	O
for	O
other	O
values	O
of	O
x	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
some	O
compromise	O
positions	O
for	O
example	O
if	O
were	O
then	O
we	O
could	O
expect	O
the	O
following	O
data	O
sets	O
d	O
with	O
probability	B
with	O
probability	B
with	O
probability	B
with	O
probability	B
we	O
now	O
consider	O
the	O
following	O
interval	O
for	O
example	O
if	O
then	O
the	O
interval	O
for	O
would	O
be	O
let	O
s	O
think	O
about	O
this	O
interval	O
what	O
is	O
its	O
level	O
by	O
considering	O
the	O
four	O
possibilities	O
shown	O
in	O
we	O
can	O
see	O
that	O
there	O
is	O
a	O
chance	O
that	O
the	O
interval	O
will	O
contain	O
the	O
true	O
value	O
the	O
interval	O
therefore	O
has	O
a	O
level	O
of	O
by	O
now	O
what	O
if	O
the	O
data	O
we	O
acquire	O
are	O
well	O
we	O
can	O
compute	O
the	O
interval	O
and	O
it	O
is	O
so	O
shall	O
we	O
report	O
this	O
interval	O
and	O
its	O
associated	O
level	O
this	O
would	O
be	O
correct	O
by	O
the	O
rules	B
of	O
sampling	B
theory	I
but	O
does	O
this	O
make	O
sense	O
what	O
do	O
we	O
actually	O
know	O
in	O
this	O
case	O
intuitively	O
or	O
by	O
bayes	B
theorem	O
it	O
is	O
clear	O
that	O
could	O
either	O
be	O
or	O
and	O
both	O
possibilities	O
are	O
equally	O
likely	O
the	O
prior	B
probabilities	O
of	O
and	O
were	O
equal	O
the	O
posterior	B
probability	B
of	O
is	O
on	O
and	O
on	O
what	O
if	O
the	O
data	O
are	O
in	O
this	O
case	O
the	O
interval	O
is	O
still	O
and	O
its	O
associated	O
level	O
is	O
but	O
in	O
this	O
case	O
by	O
bayes	B
theorem	O
or	O
common	O
sense	O
we	O
are	O
sure	O
that	O
is	O
in	O
neither	O
case	O
is	O
the	O
probability	B
that	O
lies	O
in	O
the	O
interval	O
equal	O
to	O
thus	O
the	O
way	O
in	O
which	O
many	O
people	O
interpret	O
the	O
levels	O
of	O
sampling	B
theory	I
is	O
incorrect	O
given	O
some	O
data	O
what	O
people	O
usually	O
want	O
to	O
know	O
they	O
know	O
it	O
or	O
not	O
is	O
a	O
bayesian	B
posterior	B
probability	B
distribution	B
are	O
all	O
these	O
examples	O
contrived	O
am	O
i	O
making	O
a	O
fuss	O
about	O
nothing	O
if	O
you	O
are	O
sceptical	O
about	O
the	O
dogmatic	O
views	O
i	O
have	O
expressed	O
i	O
encourage	O
you	O
to	O
look	O
at	O
a	O
case	O
study	O
look	O
in	O
depth	O
at	O
exercise	O
and	O
the	O
reference	O
and	O
oprea	O
in	O
which	O
sampling	B
theory	I
estimates	O
and	O
intervals	B
for	O
a	O
mutation	B
rate	B
are	O
constructed	O
try	O
both	O
methods	O
on	O
simulated	O
data	O
the	O
bayesian	B
approach	O
based	O
on	O
simply	O
computing	O
the	O
likelihood	B
function	O
and	O
the	O
interval	O
from	O
sampling	B
theory	I
and	O
let	O
me	O
know	O
if	O
you	O
don	O
t	O
that	O
the	O
bayesian	B
answer	O
is	O
always	O
better	O
than	O
the	O
sampling	B
theory	I
answer	O
and	O
often	O
much	O
much	O
better	O
this	O
suboptimality	O
of	O
sampling	B
theory	I
achieved	O
with	O
great	O
is	O
why	O
i	O
am	O
passionate	O
about	O
bayesian	B
methods	O
bayesian	B
methods	O
are	O
straightforward	O
and	O
they	O
optimally	O
use	O
all	O
the	O
information	B
in	O
the	O
data	O
some	O
compromise	O
positions	O
let	O
s	O
end	O
on	O
a	O
conciliatory	O
note	O
many	O
sampling	O
theorists	O
are	O
pragmatic	O
they	O
are	O
happy	O
to	O
choose	O
from	O
a	O
selection	O
of	O
statistical	B
methods	O
choosing	O
whichever	O
has	O
the	O
best	O
long-run	O
properties	O
in	O
contrast	O
i	O
have	O
no	O
problem	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bayesian	B
inference	B
and	O
sampling	B
theory	I
with	O
the	O
idea	O
that	O
there	O
is	O
only	O
one	O
answer	O
to	O
a	O
well-posed	O
problem	O
but	O
it	O
s	O
not	O
essential	O
to	O
convert	O
sampling	O
theorists	O
to	O
this	O
viewpoint	O
instead	O
we	O
can	O
them	O
bayesian	B
estimators	O
and	O
bayesian	B
intervals	B
and	O
request	O
that	O
the	O
sampling	O
theoretical	O
properties	O
of	O
these	O
methods	O
be	O
evaluated	O
we	O
don	O
t	O
need	O
to	O
mention	O
that	O
the	O
methods	O
are	O
derived	O
from	O
a	O
bayesian	B
perspective	O
if	O
the	O
sampling	O
properties	O
are	O
good	B
then	O
the	O
pragmatic	O
sampling	O
theorist	O
will	O
choose	O
to	O
use	O
the	O
bayesian	B
methods	O
it	O
is	O
indeed	O
the	O
case	O
that	O
many	O
bayesian	B
methods	O
have	O
good	B
sampling-theoretical	O
properties	O
perhaps	O
it	O
s	O
not	O
surprising	O
that	O
a	O
method	O
that	O
gives	O
the	O
optimal	B
answer	O
for	O
each	O
individual	O
case	O
should	O
also	O
be	O
good	B
in	O
the	O
long	O
run	O
another	O
piece	O
of	O
common	O
ground	O
can	O
be	O
conceded	O
while	O
i	O
believe	O
that	O
most	O
well-posed	O
inference	B
problems	O
have	O
a	O
unique	O
correct	O
answer	O
which	O
can	O
be	O
found	O
by	O
bayesian	B
methods	O
not	O
all	O
problems	O
are	O
well-posed	O
a	O
common	O
question	O
arising	O
in	O
data	O
modelling	B
is	O
am	O
i	O
using	O
an	O
appropriate	O
model	B
model	B
criticism	O
that	O
is	O
hunting	O
for	O
defects	O
in	O
a	O
current	O
model	B
is	O
a	O
task	O
that	O
may	O
be	O
aided	O
by	O
sampling	B
theory	I
tests	O
in	O
which	O
the	O
null	O
hypothesis	O
the	O
current	O
model	B
is	O
correct	O
is	O
well	O
but	O
the	O
alternative	O
model	B
is	O
not	O
one	O
could	O
use	O
sampling	B
theory	I
measures	O
such	O
as	O
p-values	O
to	O
guide	O
one	O
s	O
search	O
for	O
the	O
aspects	O
of	O
the	O
model	B
most	O
in	O
need	O
of	O
scrutiny	O
further	O
reading	O
my	O
favourite	O
reading	O
on	O
this	O
topic	O
includes	O
gull	B
loredo	O
berger	O
jaynes	O
treatises	O
on	O
bayesian	B
statistics	O
from	O
the	O
statistics	O
community	O
include	O
and	O
tiao	O
o	O
hagan	O
further	O
exercises	O
exercise	O
a	O
survey	O
records	O
on	O
two	O
successive	O
days	O
on	O
friday	O
morning	O
there	O
are	O
vehicles	O
in	O
one	O
hour	O
on	O
saturday	O
morning	O
there	O
are	O
vehicles	O
in	O
half	O
an	O
hour	O
assuming	O
that	O
the	O
vehicles	O
are	O
poisson	B
distributed	O
with	O
rates	O
and	O
vehicles	O
per	O
hour	O
respectively	O
is	O
greater	O
than	O
by	O
what	O
factor	O
is	O
bigger	O
or	O
smaller	O
than	O
exercise	O
write	O
a	O
program	O
to	O
compare	O
treatments	O
a	O
and	O
b	O
given	O
data	O
fa	O
fb	O
as	O
described	O
in	O
section	B
the	O
outputs	O
of	O
the	O
program	O
should	O
be	O
the	O
probability	B
that	O
treatment	O
a	O
is	O
more	O
than	O
treatment	O
b	O
the	O
probability	B
that	O
pa	O
pb	O
the	O
probability	B
that	O
pb	O
pa	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
part	O
v	O
neural	O
networks	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
neural	O
networks	O
in	O
the	O
of	O
neural	O
networks	O
we	O
study	O
the	O
properties	O
of	O
networks	O
of	O
idealized	O
neurons	O
three	O
motivations	O
underlie	O
work	O
in	O
this	O
broad	O
and	O
interdisciplinary	O
biology	O
the	O
task	O
of	O
understanding	O
how	O
the	O
brain	B
works	O
is	O
one	O
of	O
the	O
outstanding	O
unsolved	O
problems	O
in	O
science	O
some	O
neural	B
network	B
models	O
are	O
intended	O
to	O
shed	O
light	O
on	O
the	O
way	O
in	O
which	O
computation	O
and	O
memory	B
are	O
performed	O
by	O
brains	O
engineering	O
many	O
researchers	O
would	O
like	O
to	O
create	O
machines	O
that	O
can	O
learn	O
perform	O
pattern	B
recognition	B
or	O
discover	O
patterns	O
in	O
data	O
complex	B
systems	O
a	O
third	O
motivation	O
for	O
being	O
interested	O
in	O
neural	O
networks	O
is	O
that	O
they	O
are	O
complex	B
adaptive	B
systems	O
whose	O
properties	O
are	O
interesting	O
in	O
their	O
own	O
right	O
i	O
should	O
emphasize	O
several	O
points	O
at	O
the	O
outset	O
this	O
book	O
gives	O
only	O
a	O
taste	O
of	O
this	O
there	O
are	O
many	O
interesting	O
neural	B
network	B
models	O
which	O
we	O
will	O
not	O
have	O
time	O
to	O
touch	O
on	O
the	O
models	O
that	O
we	O
discuss	O
are	O
not	O
intended	O
to	O
be	O
faithful	O
models	O
of	O
biological	O
systems	O
if	O
they	O
are	O
at	O
all	O
relevant	O
to	O
biology	O
their	O
relevance	O
is	O
on	O
an	O
abstract	O
level	O
i	O
will	O
describe	O
some	O
neural	B
network	B
methods	O
that	O
are	O
widely	O
used	O
in	O
nonlinear	B
data	O
modelling	B
but	O
i	O
will	O
not	O
be	O
able	O
to	O
give	O
a	O
full	O
description	O
of	O
the	O
state	O
of	O
the	O
art	O
if	O
you	O
wish	O
to	O
solve	O
real	O
problems	O
with	O
neural	O
networks	O
please	O
read	O
the	O
relevant	O
papers	O
memories	O
in	O
the	O
next	O
few	O
chapters	O
we	O
will	O
meet	O
several	O
neural	B
network	B
models	O
which	O
come	O
with	O
simple	O
learning	B
algorithms	B
which	O
make	O
them	O
function	O
as	O
memories	O
perhaps	O
we	O
should	O
dwell	O
for	O
a	O
moment	O
on	O
the	O
conventional	O
idea	O
of	O
memory	B
in	O
digital	O
computation	O
a	O
memory	B
string	O
of	O
bits	O
describing	O
the	O
name	O
of	O
a	O
person	O
and	O
an	O
image	B
of	O
their	O
face	O
say	O
is	O
stored	O
in	O
a	O
digital	O
computer	B
at	O
an	O
address	B
to	O
retrieve	O
the	O
memory	B
you	O
need	O
to	O
know	O
the	O
address	B
the	O
address	B
has	O
nothing	O
to	O
do	O
with	O
the	O
memory	B
itself	O
notice	O
the	O
properties	O
that	O
this	O
scheme	O
does	O
not	O
have	O
address-based	B
memory	B
is	O
not	O
associative	B
imagine	O
you	O
know	O
half	O
of	O
a	O
memory	B
say	O
someone	O
s	O
face	O
and	O
you	O
would	O
like	O
to	O
recall	O
the	O
rest	O
of	O
the	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
memories	O
memory	B
their	O
name	O
if	O
your	O
memory	B
is	O
address-based	B
then	O
you	O
can	O
t	O
get	O
at	O
a	O
memory	B
without	O
knowing	O
the	O
address	B
scientists	B
have	O
devoted	O
to	O
wrapping	O
traditional	O
address-based	B
memories	O
inside	O
cunning	O
software	B
to	O
produce	O
content-addressable	B
memories	O
but	O
contentaddressability	O
does	O
not	O
come	O
naturally	O
it	O
has	O
to	O
be	O
added	O
on	O
address-based	B
memory	B
is	O
not	O
robust	O
or	O
fault-tolerant	O
if	O
a	O
one-bit	O
mistake	O
is	O
made	O
in	O
specifying	O
the	O
address	B
then	O
a	O
completely	O
memory	B
will	O
be	O
retrieved	O
if	O
one	O
bit	B
of	O
a	O
memory	B
is	O
then	O
whenever	O
that	O
memory	B
is	O
retrieved	O
the	O
error	O
will	O
be	O
present	O
of	O
course	O
in	O
all	O
modern	O
computers	O
error-correcting	B
codes	I
are	O
used	O
in	O
the	O
memory	B
so	O
that	O
small	O
numbers	O
of	O
errors	B
can	O
be	O
detected	O
and	O
corrected	O
but	O
this	O
errortolerance	O
is	O
not	O
an	O
intrinsic	O
property	O
of	O
the	O
memory	B
system	O
if	O
minor	O
damage	O
occurs	O
to	O
certain	O
hardware	O
that	O
implements	O
memory	B
retrieval	O
it	O
is	O
likely	O
that	O
all	O
functionality	O
will	O
be	O
catastrophically	O
lost	O
address-based	B
memory	B
is	O
not	O
distributed	O
in	O
a	O
serial	O
computer	B
that	O
is	O
accessing	O
a	O
particular	O
memory	B
only	O
a	O
tiny	O
fraction	O
of	O
the	O
devices	O
participate	O
in	O
the	O
memory	B
recall	O
the	O
cpu	O
and	O
the	O
circuits	O
that	O
are	O
storing	O
the	O
required	O
byte	B
all	O
the	O
other	O
millions	O
of	O
devices	O
in	O
the	O
machine	O
are	O
sitting	O
idle	O
are	O
there	O
models	O
of	O
truly	O
parallel	O
computation	O
in	O
which	O
multiple	O
devices	O
participate	O
in	O
all	O
computations	O
parallel	O
computers	O
scarcely	O
from	O
serial	O
computers	O
from	O
this	O
point	O
of	O
view	O
memory	B
retrieval	O
works	O
in	O
just	O
the	O
same	O
way	O
and	O
control	O
of	O
the	O
computation	O
process	O
resides	O
in	O
cpus	O
there	O
are	O
simply	O
a	O
few	O
more	O
cpus	O
most	O
of	O
the	O
devices	O
sit	O
idle	O
most	O
of	O
the	O
time	O
biological	O
memory	B
systems	O
are	O
completely	O
biological	O
memory	B
is	O
associative	B
memory	B
recall	O
is	O
content-addressable	B
given	O
a	O
person	O
s	O
name	O
we	O
can	O
often	O
recall	O
their	O
face	O
and	O
vice	O
versa	O
memories	O
are	O
apparently	O
recalled	O
spontaneously	O
not	O
just	O
at	O
the	O
request	O
of	O
some	O
cpu	O
biological	O
memory	B
recall	O
is	O
error-tolerant	O
and	O
robust	O
errors	B
in	O
the	O
cues	O
for	O
memory	B
recall	O
can	O
be	O
corrected	O
an	O
example	O
asks	O
you	O
to	O
recall	O
an	O
american	B
politician	O
who	O
was	O
very	O
intelligent	O
and	O
whose	O
politician	O
father	O
did	O
not	O
like	O
broccoli	O
many	O
people	O
think	O
of	O
president	O
bush	O
even	O
though	O
one	O
of	O
the	O
cues	O
contains	O
an	O
error	O
hardware	O
faults	O
can	O
also	O
be	O
tolerated	O
our	O
brains	O
are	O
noisy	B
lumps	O
of	O
meat	O
that	O
are	O
in	O
a	O
continual	O
state	O
of	O
change	O
with	O
cells	O
being	O
damaged	O
by	O
natural	B
processes	O
alcohol	O
and	O
boxing	O
while	O
the	O
cells	O
in	O
our	O
brains	O
and	O
the	O
proteins	O
in	O
our	O
cells	O
are	O
continually	O
changing	O
many	O
of	O
our	O
memories	O
persist	O
biological	O
memory	B
is	O
parallel	O
and	O
distributed	O
not	O
completely	O
distributed	O
throughout	O
the	O
whole	O
brain	B
there	O
does	O
appear	O
to	O
be	O
some	O
functional	O
specialization	O
but	O
in	O
the	O
parts	O
of	O
the	O
brain	B
where	O
memories	O
are	O
stored	O
it	O
seems	O
that	O
many	O
neurons	O
participate	O
in	O
the	O
storage	O
of	O
multiple	O
memories	O
these	O
properties	O
of	O
biological	O
memory	B
systems	O
motivate	O
the	O
study	O
of	O
neural	O
networks	O
parallel	O
distributed	O
computational	O
systems	O
consisting	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
introduction	O
to	O
neural	O
networks	O
of	O
many	O
interacting	O
simple	O
elements	O
the	O
hope	O
is	O
that	O
these	O
model	B
systems	O
might	O
give	O
some	O
hints	O
as	O
to	O
how	O
neural	O
computation	O
is	O
achieved	O
in	O
real	O
biological	O
neural	O
networks	O
terminology	O
each	O
time	O
we	O
describe	O
a	O
neural	B
network	B
algorithm	O
we	O
will	O
typically	O
specify	O
three	O
things	O
any	O
of	O
this	O
terminology	O
is	O
hard	O
to	O
understand	O
it	O
s	O
probably	O
best	O
to	O
dive	O
straight	O
into	O
the	O
next	O
chapter	O
architecture	B
the	O
architecture	B
what	O
variables	O
are	O
involved	O
in	O
the	O
network	B
and	O
their	O
topological	O
relationships	O
for	O
example	O
the	O
variables	O
involved	O
in	O
a	O
neural	O
net	O
might	O
be	O
the	O
weights	O
of	O
the	O
connections	O
between	O
the	O
neurons	O
along	O
with	O
the	O
activities	O
of	O
the	O
neurons	O
activity	B
rule	I
most	O
neural	B
network	B
models	O
have	O
short	O
time-scale	O
dynamics	O
local	O
rules	B
how	O
the	O
activities	O
of	O
the	O
neurons	O
change	O
in	O
response	O
to	O
each	O
other	O
typically	O
the	O
activity	B
rule	I
depends	O
on	O
the	O
weights	O
parameters	B
in	O
the	O
network	B
learning	B
rule	I
the	O
learning	B
rule	I
the	O
way	O
in	O
which	O
the	O
neural	B
network	B
s	O
weights	O
change	O
with	O
time	O
this	O
learning	B
is	O
usually	O
viewed	O
as	O
taking	O
place	O
on	O
a	O
longer	O
time	O
scale	O
than	O
the	O
time	O
scale	O
of	O
the	O
dynamics	O
under	O
the	O
activity	B
rule	I
usually	O
the	O
learning	B
rule	I
will	O
depend	O
on	O
the	O
activities	O
of	O
the	O
neurons	O
it	O
may	O
also	O
depend	O
on	O
the	O
values	O
of	O
target	O
values	O
supplied	O
by	O
a	O
teacher	O
and	O
on	O
the	O
current	O
value	O
of	O
the	O
weights	O
where	O
do	O
these	O
rules	B
come	O
from	O
often	O
activity	O
rules	B
and	B
learning	B
rules	B
are	O
invented	O
by	O
imaginative	O
researchers	O
alternatively	O
activity	O
rules	B
and	B
learning	B
rules	B
may	O
be	O
derived	O
from	O
carefully	O
chosen	O
objective	O
functions	B
neural	B
network	B
algorithms	B
can	O
be	O
roughly	O
divided	O
into	O
two	O
classes	O
supervised	O
neural	O
networks	O
are	O
given	O
data	O
in	O
the	O
form	O
of	O
inputs	O
and	O
targets	O
the	O
targets	O
being	O
a	O
teacher	O
s	O
of	O
what	O
the	O
neural	B
network	B
s	O
response	O
to	O
the	O
input	O
should	O
be	O
unsupervised	O
neural	O
networks	O
are	O
given	O
data	O
in	O
an	O
undivided	O
form	O
simply	O
a	O
set	B
of	O
examples	O
fxg	O
some	O
learning	B
algorithms	B
are	O
intended	O
simply	O
to	O
memorize	O
these	O
data	O
in	O
such	O
a	O
way	O
that	O
the	O
examples	O
can	O
be	O
recalled	O
in	O
the	O
future	O
other	O
algorithms	B
are	O
intended	O
to	O
generalize	O
to	O
discover	O
patterns	O
in	O
the	O
data	O
or	O
extract	O
the	O
underlying	O
features	O
from	O
them	O
some	O
unsupervised	O
algorithms	B
are	O
able	O
to	O
make	O
predictions	O
for	O
example	O
some	O
algorithms	B
can	O
in	O
missing	O
variables	O
in	O
an	O
example	O
x	O
and	O
so	O
can	O
also	O
be	O
viewed	O
as	O
supervised	O
networks	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
single	B
neuron	B
as	O
a	O
the	O
single	B
neuron	B
we	O
will	O
study	O
a	O
single	B
neuron	B
for	O
two	O
reasons	O
first	O
many	O
neural	B
network	B
models	O
are	O
built	O
out	O
of	O
single	O
neurons	O
so	O
it	O
is	O
good	B
to	O
understand	O
them	O
in	O
detail	O
and	O
second	O
a	O
single	B
neuron	B
is	O
itself	O
capable	O
of	O
learning	B
indeed	O
various	O
standard	O
statistical	B
methods	O
can	O
be	O
viewed	O
in	O
terms	O
of	O
single	O
neurons	O
so	O
this	O
model	B
will	O
serve	O
as	O
a	O
example	O
of	O
a	O
supervised	O
neural	B
network	B
of	O
a	O
single	B
neuron	B
we	O
will	O
start	O
by	O
the	O
architecture	B
and	O
the	O
activity	B
rule	I
of	O
a	O
single	B
neuron	B
and	O
we	O
will	O
then	O
derive	O
a	O
learning	B
rule	I
architecture	B
a	O
single	B
neuron	B
has	O
a	O
number	O
i	O
of	O
inputs	O
xi	O
and	O
one	O
output	O
which	O
we	O
will	O
here	O
call	O
y	O
associated	O
with	O
each	O
input	O
is	O
a	O
weight	O
wi	O
i	O
there	O
may	O
be	O
an	O
additional	O
parameter	O
of	O
the	O
neuron	B
called	O
a	O
bias	B
which	O
we	O
may	O
view	O
as	O
being	O
the	O
weight	O
associated	O
with	O
an	O
input	O
that	O
is	O
permanently	O
set	B
to	O
the	O
single	B
neuron	B
is	O
a	O
feedforward	O
device	O
the	O
connections	O
are	O
directed	O
from	O
the	O
inputs	O
to	O
the	O
output	O
of	O
the	O
neuron	B
activity	B
rule	I
the	O
activity	B
rule	I
has	O
two	O
steps	O
first	O
in	O
response	O
to	O
the	O
imposed	O
inputs	O
x	O
we	O
compute	O
the	O
activa	O
tion	O
of	O
the	O
neuron	B
a	O
wixi	O
where	O
the	O
sum	O
is	O
over	O
i	O
i	O
if	O
there	O
is	O
a	O
bias	B
and	O
i	O
i	O
otherwise	O
second	O
the	O
output	O
y	O
is	O
set	B
as	O
a	O
function	O
f	O
of	O
the	O
activation	O
the	O
output	O
is	O
also	O
called	O
the	O
activity	O
of	O
the	O
neuron	B
not	O
to	O
be	O
confused	O
with	O
the	O
activation	O
a	O
there	O
are	O
several	O
possible	O
activation	O
functions	B
here	O
are	O
the	O
most	O
popular	O
deterministic	B
activation	O
functions	B
i	O
linear	B
ii	O
sigmoid	B
function	O
ya	O
a	O
ya	O
b	O
b	O
b	O
b	O
a	O
wi	O
a	O
xi	O
aa	O
ee	O
e	O
a	O
e	O
e	O
y	O
figure	O
a	O
single	B
neuron	B
activation	O
activity	O
a	O
ya	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
single	B
neuron	B
as	O
a	O
iii	O
sigmoid	B
ya	O
tanha	O
iv	O
threshold	B
function	O
ya	O
a	O
a	O
stochastic	B
activation	O
functions	B
y	O
is	O
stochastically	O
selected	O
from	O
i	O
heat	B
bath	I
ya	O
with	O
probability	B
otherwise	O
ii	O
the	O
metropolis	O
rule	O
produces	O
the	O
output	O
in	O
a	O
way	O
that	O
depends	O
on	O
the	O
previous	O
output	O
state	O
y	O
compute	O
ay	O
if	O
y	O
to	O
the	O
other	O
state	O
else	O
y	O
to	O
the	O
other	O
state	O
with	O
probability	B
basic	O
neural	B
network	B
concepts	O
a	O
neural	B
network	B
implements	O
a	O
function	O
yx	O
w	O
the	O
output	O
of	O
the	O
network	B
y	O
is	O
a	O
nonlinear	B
function	O
of	O
the	O
inputs	O
x	O
this	O
function	O
is	O
parameterized	O
by	O
weights	O
w	O
we	O
will	O
study	O
a	O
single	B
neuron	B
which	O
produces	O
an	O
output	O
between	O
and	O
as	O
the	O
following	O
function	O
of	O
x	O
yx	O
w	O
exercise	O
in	O
what	O
contexts	O
have	O
we	O
encountered	O
the	O
function	O
yx	O
w	O
already	O
motivations	O
for	O
the	O
linear	B
logistic	O
function	O
in	O
section	B
we	O
studied	O
the	O
best	O
detection	O
of	O
pulses	O
assuming	O
that	O
one	O
of	O
two	O
signals	O
and	O
had	O
been	O
transmitted	O
over	O
a	O
gaussian	B
channel	I
with	O
variancecovariance	O
matrix	B
we	O
found	O
that	O
the	O
probability	B
that	O
the	O
source	O
signal	O
was	O
s	O
rather	O
than	O
s	O
given	O
the	O
received	O
signal	O
y	O
was	O
p	O
y	O
where	O
ay	O
was	O
a	O
linear	B
function	O
of	O
the	O
received	O
vector	O
ay	O
wty	O
with	O
w	O
the	O
exercises	O
the	O
linear	B
logistic	O
function	O
can	O
be	O
motivated	O
in	O
several	O
other	O
ways	O
see	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
basic	O
neural	B
network	B
concepts	O
figure	O
output	O
of	O
a	O
simple	O
neural	B
network	B
as	O
a	O
function	O
of	O
its	O
input	O
w	O
input	O
space	O
and	O
weight	B
space	I
for	O
convenience	O
let	O
us	O
study	O
the	O
case	O
where	O
the	O
input	O
vector	O
x	O
and	O
the	O
parameter	O
vector	O
w	O
are	O
both	O
two-dimensional	B
x	O
w	O
then	O
we	O
can	O
spell	B
out	O
the	O
function	O
performed	O
by	O
the	O
neuron	B
thus	O
yx	O
w	O
figure	O
shows	O
the	O
output	O
of	O
the	O
neuron	B
as	O
a	O
function	O
of	O
the	O
input	O
vector	O
for	O
w	O
the	O
two	O
horizontal	O
axes	O
of	O
this	O
are	O
the	O
inputs	O
and	O
with	O
the	O
output	O
y	O
on	O
the	O
vertical	O
axis	O
notice	O
that	O
on	O
any	O
line	O
perpendicular	O
to	O
w	O
the	O
output	O
is	O
constant	O
and	O
along	O
a	O
line	O
in	O
the	O
direction	O
of	O
w	O
the	O
output	O
is	O
a	O
sigmoid	B
function	O
we	O
now	O
introduce	O
the	O
idea	O
of	O
weight	B
space	I
that	O
is	O
the	O
parameter	O
space	O
of	O
the	O
network	B
in	O
this	O
case	O
there	O
are	O
two	O
parameters	B
and	O
so	O
the	O
weight	B
space	I
is	O
two	O
dimensional	O
this	O
weight	B
space	I
is	O
shown	O
in	O
for	O
a	O
selection	O
of	O
values	O
of	O
the	O
parameter	O
vector	O
w	O
smaller	O
inset	O
show	O
the	O
function	O
of	O
x	O
performed	O
by	O
the	O
network	B
when	O
w	O
is	O
set	B
to	O
those	O
values	O
each	O
of	O
these	O
smaller	O
is	O
equivalent	O
to	O
thus	O
each	O
point	O
in	O
w	O
space	O
corresponds	O
to	O
a	O
function	O
of	O
x	O
notice	O
that	O
the	O
gain	B
of	O
the	O
sigmoid	B
function	O
gradient	O
of	O
the	O
ramp	O
increases	O
as	O
the	O
magnitude	O
of	O
w	O
increases	O
now	O
the	O
central	O
idea	O
of	O
supervised	O
neural	O
networks	O
is	O
this	O
given	O
examples	O
of	O
a	O
relationship	O
between	O
an	O
input	O
vector	O
x	O
and	O
a	O
target	O
t	O
we	O
hope	O
to	O
make	O
the	O
neural	B
network	B
learn	O
a	O
model	B
of	O
the	O
relationship	O
between	O
x	O
and	O
t	O
a	O
successfully	O
trained	O
network	B
will	O
for	O
any	O
given	O
x	O
give	O
an	O
output	O
y	O
that	O
is	O
close	O
some	O
sense	O
to	O
the	O
target	O
value	O
t	O
training	O
the	O
network	B
involves	O
searching	O
in	O
the	O
weight	B
space	I
of	O
the	O
network	B
for	O
a	O
value	O
of	O
w	O
that	O
produces	O
a	O
function	O
that	O
the	O
provided	O
training	B
data	I
well	O
typically	O
an	O
objective	B
function	I
or	O
error	B
function	I
is	O
as	O
a	O
function	O
of	O
w	O
to	O
measure	O
how	O
well	O
the	O
network	B
with	O
weights	O
set	B
to	O
w	O
solves	O
the	O
task	O
the	O
objective	B
function	I
is	O
a	O
sum	O
of	O
terms	O
one	O
for	O
each	O
inputtarget	O
pair	O
fx	O
tg	O
measuring	O
how	O
close	O
the	O
output	O
yx	O
w	O
is	O
to	O
the	O
target	O
t	O
the	O
training	O
process	O
is	O
an	O
exercise	O
in	O
function	B
minimization	B
i	O
e	O
adjusting	O
w	O
in	O
such	O
a	O
way	O
as	O
to	O
a	O
w	O
that	O
minimizes	O
the	O
objective	B
function	I
many	O
function-minimization	O
algorithms	B
make	O
use	O
not	O
only	O
of	O
the	O
objective	B
function	I
but	O
also	O
its	O
gradient	O
with	O
respect	O
to	O
the	O
parameters	B
w	O
for	O
general	O
feedforward	O
neural	O
networks	O
the	O
backpropagation	B
algorithm	O
evaluates	O
the	O
gradient	O
of	O
the	O
output	O
y	O
with	O
respect	O
to	O
the	O
parameters	B
w	O
and	O
thence	O
the	O
gradient	O
of	O
the	O
objective	B
function	I
with	O
respect	O
to	O
w	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
single	B
neuron	B
as	O
a	O
w	O
w	O
w	O
w	O
w	O
w	O
w	O
w	O
w	O
w	O
figure	O
weight	B
space	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
training	O
the	O
single	B
neuron	B
as	O
a	O
binary	O
training	O
the	O
single	B
neuron	B
as	O
a	O
binary	O
we	O
assume	O
we	O
have	O
a	O
data	B
set	B
of	O
inputs	O
fxngn	O
and	O
a	O
neuron	B
whose	O
output	O
yx	O
w	O
is	O
bounded	O
between	O
and	O
we	O
can	O
then	O
write	O
down	O
the	O
following	O
error	B
function	I
with	O
binary	O
labels	O
ftngn	O
gw	O
htn	O
ln	O
yxn	O
w	O
tn	O
yxn	O
wi	O
each	O
term	O
in	O
this	O
objective	B
function	I
may	O
be	O
recognized	O
as	O
the	O
information	B
content	I
of	O
one	O
outcome	O
it	O
may	O
also	O
be	O
described	O
as	O
the	O
relative	B
entropy	B
between	O
the	O
empirical	O
probability	B
distribution	B
tn	O
and	O
the	O
probability	B
distribution	B
implied	O
by	O
the	O
output	O
of	O
the	O
neuron	B
the	O
objective	B
function	I
is	O
bounded	O
below	O
by	O
zero	O
and	O
only	O
attains	O
this	O
value	O
if	O
yxn	O
w	O
tn	O
for	O
all	O
n	O
we	O
now	O
this	O
objective	B
function	I
with	O
respect	O
to	O
w	O
exercise	O
the	O
backpropagation	B
algorithm	O
show	O
that	O
the	O
derivative	O
g	O
is	O
given	O
by	O
gj	O
n	O
ynxn	O
j	O
notice	O
that	O
the	O
quantity	O
en	O
tn	O
yn	O
is	O
the	O
error	O
on	O
example	O
n	O
the	O
between	O
the	O
target	O
and	O
the	O
output	O
the	O
simplest	O
thing	O
to	O
do	O
with	O
a	O
gradient	O
of	O
an	O
error	B
function	I
is	O
to	O
descend	O
it	O
though	O
this	O
is	O
often	O
dimensionally	O
incorrect	O
since	O
a	O
gradient	O
has	O
dimensions	B
whereas	O
a	O
change	O
in	O
a	O
parameter	O
has	O
dimensions	B
since	O
the	O
derivative	O
is	O
a	O
sum	O
of	O
terms	O
gn	O
by	O
gn	O
j	O
ynxn	O
j	O
for	O
n	O
n	O
we	O
can	O
obtain	O
a	O
simple	O
on-line	O
algorithm	O
by	O
putting	O
each	O
input	O
through	O
the	O
network	B
one	O
at	O
a	O
time	O
and	O
adjusting	O
w	O
a	O
little	O
in	O
a	O
direction	O
opposite	O
to	O
gn	O
we	O
summarize	O
the	O
whole	O
learning	B
algorithm	O
the	O
on-line	O
gradient-descent	O
learning	B
algorithm	O
architecture	B
a	O
single	B
neuron	B
has	O
a	O
number	O
i	O
of	O
inputs	O
xi	O
and	O
one	O
output	O
y	O
associated	O
with	O
each	O
input	O
is	O
a	O
weight	O
wi	O
i	O
activity	B
rule	I
first	O
in	O
response	O
to	O
the	O
received	O
inputs	O
x	O
may	O
be	O
arbitrary	O
real	O
numbers	O
we	O
compute	O
the	O
activation	O
of	O
the	O
neuron	B
a	O
wixi	O
where	O
the	O
sum	O
is	O
over	O
i	O
i	O
if	O
there	O
is	O
a	O
bias	B
and	O
i	O
i	O
otherwise	O
second	O
the	O
output	O
y	O
is	O
set	B
as	O
a	O
sigmoid	B
function	O
of	O
the	O
activation	O
ya	O
this	O
output	O
might	O
be	O
viewed	O
as	O
stating	O
the	O
probability	B
according	O
to	O
the	O
neuron	B
that	O
the	O
given	O
input	O
is	O
in	O
class	O
rather	O
than	O
class	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
single	B
neuron	B
as	O
a	O
learning	B
rule	I
the	O
teacher	O
supplies	O
a	O
target	O
value	O
t	O
which	O
says	O
what	O
the	O
correct	O
answer	O
is	O
for	O
the	O
given	O
input	O
we	O
compute	O
the	O
error	O
signal	O
e	O
t	O
y	O
then	O
adjust	O
the	O
weights	O
w	O
in	O
a	O
direction	O
that	O
would	O
reduce	O
the	O
magnitude	O
of	O
this	O
error	O
where	O
is	O
the	O
learning	B
rate	B
commonly	O
is	O
set	B
by	O
trial	O
and	O
error	O
to	O
a	O
constant	O
value	O
or	O
to	O
a	O
decreasing	O
function	O
of	O
simulation	O
time	O
such	O
as	O
the	O
activity	B
rule	I
and	B
learning	B
rule	I
are	O
repeated	O
for	O
each	O
inputtarget	O
pair	O
t	O
that	O
is	O
presented	O
if	O
there	O
is	O
a	O
data	B
set	B
of	O
size	O
n	O
we	O
can	O
cycle	O
through	O
the	O
data	O
multiple	O
times	O
batch	O
learning	B
versus	O
on-line	O
learning	B
here	O
we	O
have	O
described	O
the	O
on-line	O
learning	B
algorithm	O
in	O
which	O
a	O
change	O
in	O
the	O
weights	O
is	O
made	O
after	O
every	O
example	O
is	O
presented	O
an	O
alternative	O
paradigm	O
is	O
to	O
go	O
through	O
a	O
batch	O
of	O
examples	O
computing	O
the	O
outputs	O
and	O
errors	B
and	O
accumulating	O
the	O
changes	O
in	O
equation	O
which	O
are	O
then	O
made	O
at	O
the	O
end	O
of	O
the	O
batch	O
batch	O
learning	B
for	O
the	O
single	B
neuron	B
for	O
each	O
inputtarget	O
pair	O
tn	O
n	O
compute	O
yn	O
yxn	O
w	O
where	O
yx	O
w	O
en	O
tn	O
yn	O
and	O
compute	O
for	O
each	O
weight	O
wi	O
wixi	O
gn	O
i	O
i	O
then	O
let	O
gn	O
i	O
this	O
batch	O
learning	B
algorithm	O
is	O
a	O
gradient	B
descent	I
algorithm	O
whereas	O
the	O
on-line	O
algorithm	O
is	O
a	O
stochastic	B
gradient	B
descent	I
algorithm	O
source	O
code	O
implementing	O
batch	O
learning	B
is	O
given	O
in	O
algorithm	O
this	O
algorithm	O
is	O
demonstrated	O
in	O
for	O
a	O
neuron	B
with	O
two	O
inputs	O
with	O
weights	O
and	O
and	O
a	O
bias	B
performing	O
the	O
function	O
yx	O
w	O
the	O
bias	B
is	O
included	O
in	O
contrast	O
to	O
where	O
it	O
was	O
omitted	O
the	O
neuron	B
is	O
trained	O
on	O
a	O
data	B
set	B
of	O
ten	O
labelled	O
examples	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
training	O
the	O
single	B
neuron	B
as	O
a	O
binary	O
gw	O
e	O
ww	O
figure	O
a	O
single	B
neuron	B
learning	B
to	O
classify	O
by	O
gradient	B
descent	I
the	O
neuron	B
has	O
two	O
weights	O
and	O
and	O
a	O
bias	B
the	O
learning	B
rate	B
was	O
set	B
to	O
and	O
batch-mode	O
gradient	B
descent	I
was	O
performed	O
using	O
the	O
code	O
displayed	O
in	O
algorithm	O
the	O
training	B
data	I
evolution	B
of	O
weights	O
and	O
as	O
a	O
function	O
of	O
number	O
of	O
iterations	O
log	O
scale	O
evolution	B
of	O
weights	O
and	O
in	O
weight	B
space	I
the	O
objective	B
function	I
gw	O
as	O
a	O
function	O
of	O
number	O
of	O
iterations	O
the	O
magnitude	O
of	O
the	O
weights	O
ew	O
as	O
a	O
function	O
of	O
time	O
the	O
function	O
performed	O
by	O
the	O
neuron	B
by	O
three	O
of	O
its	O
contours	O
after	O
and	O
iterations	O
the	O
contours	O
shown	O
are	O
those	O
corresponding	O
to	O
a	O
namely	O
y	O
and	O
also	O
shown	O
is	O
a	O
vector	O
proportional	O
to	O
the	O
larger	O
the	O
weights	O
are	O
the	O
bigger	O
this	O
vector	O
becomes	O
and	O
the	O
closer	O
together	O
are	O
the	O
contours	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
single	B
neuron	B
as	O
a	O
algorithm	O
octave	B
source	O
code	O
for	O
a	O
gradient	B
descent	I
optimizer	O
of	O
a	O
single	B
neuron	B
batch	O
learning	B
with	O
optional	O
weight	B
decay	I
alpha	O
octave	B
notation	B
the	O
instruction	O
a	O
x	O
w	O
causes	O
the	O
i	O
matrix	B
x	O
consisting	O
of	O
all	O
the	O
input	O
vectors	B
to	O
be	O
multiplied	O
by	O
the	O
weight	O
vector	O
w	O
giving	O
the	O
vector	O
a	O
listing	O
the	O
activations	O
for	O
all	O
n	O
input	O
vectors	B
x	O
means	O
x-transpose	O
the	O
single	O
command	O
y	O
sigmoida	O
computes	O
the	O
sigmoid	B
function	O
of	O
all	O
elements	O
of	O
the	O
vector	O
a	O
figure	O
the	O
of	O
weight	B
decay	I
on	O
a	O
single	B
neuron	B
s	O
learning	B
the	O
objective	B
function	I
is	O
m	O
gw	O
the	O
learning	B
method	O
was	O
as	O
in	B
evolution	B
of	O
weights	O
and	O
evolution	B
of	O
weights	O
and	O
in	O
weight	B
space	I
shown	O
by	O
points	O
contrasted	O
with	O
the	O
trajectory	O
followed	O
in	O
the	O
case	O
of	O
zero	O
weight	B
decay	I
shown	O
by	O
a	O
thin	O
line	O
notice	O
that	O
for	O
this	O
problem	O
weight	B
decay	I
has	O
an	O
very	O
similar	O
to	O
early	O
stopping	O
the	O
objective	B
function	I
m	O
and	O
the	O
error	B
function	I
gw	O
as	O
a	O
function	O
of	O
number	O
of	O
iterations	O
the	O
function	O
performed	O
by	O
the	O
neuron	B
after	O
iterations	O
global	O
x	O
global	O
t	O
x	O
is	O
an	O
n	O
i	O
matrix	B
containing	O
all	O
the	O
input	O
vectors	B
t	O
is	O
a	O
vector	O
of	O
length	O
n	O
containing	O
all	O
the	O
targets	O
for	O
l	O
loop	O
l	O
times	O
a	O
x	O
w	O
y	O
sigmoida	O
e	O
t	O
y	O
g	O
x	O
e	O
w	O
w	O
eta	O
g	O
alpha	O
w	O
compute	O
all	O
activations	O
compute	O
outputs	O
compute	O
errors	B
compute	O
the	O
gradient	O
vector	O
make	O
step	O
using	O
learning	B
rate	B
eta	O
and	O
weight	B
decay	I
alpha	O
endfor	O
function	O
f	O
sigmoid	B
v	O
f	O
exp	O
v	O
endfunction	O
gw	O
mw	O
gw	O
mw	O
gw	O
mw	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
beyond	O
descent	O
on	O
the	O
error	B
function	I
regularization	B
beyond	O
descent	O
on	O
the	O
error	B
function	I
regularization	B
if	O
the	O
parameter	O
is	O
set	B
to	O
an	O
appropriate	O
value	O
this	O
algorithm	O
works	O
the	O
algorithm	O
a	O
setting	O
of	O
w	O
that	O
correctly	O
as	O
many	O
of	O
the	O
examples	O
as	O
possible	O
if	O
the	O
examples	O
are	O
in	O
fact	O
linearly	O
separable	O
then	O
the	O
neuron	B
this	O
linear	B
separation	B
and	O
its	O
weights	O
diverge	O
to	O
ever-larger	O
values	O
as	O
the	O
simulation	O
continues	O
this	O
can	O
be	O
seen	O
happening	O
in	O
this	O
is	O
an	O
example	O
of	O
where	O
a	O
model	B
the	O
data	O
so	O
well	O
that	O
its	O
generalization	B
performance	O
is	O
likely	O
to	O
be	O
adversely	O
this	O
behaviour	O
may	O
be	O
viewed	O
as	O
undesirable	O
how	O
can	O
it	O
be	O
an	O
ad	O
hoc	O
solution	O
to	O
is	O
to	O
use	O
early	O
stopping	O
that	O
is	O
use	O
an	O
algorithm	O
originally	O
intended	O
to	O
minimize	O
the	O
error	B
function	I
gw	O
then	O
prevent	O
it	O
from	O
doing	O
so	O
by	O
halting	O
the	O
algorithm	O
at	O
some	O
point	O
a	O
more	O
principled	O
solution	O
to	O
makes	O
use	O
of	O
regularization	B
regularization	B
involves	O
modifying	O
the	O
objective	B
function	I
in	O
such	O
a	O
way	O
as	O
to	O
incorporate	O
a	O
bias	B
against	O
the	O
sorts	O
of	O
solution	O
w	O
which	O
we	O
dislike	O
in	O
the	O
above	O
example	O
what	O
we	O
dislike	O
is	O
the	O
development	O
of	O
a	O
very	O
sharp	O
decision	O
boundary	O
in	O
this	O
sharp	O
boundary	O
is	O
associated	O
with	O
large	O
weight	O
values	O
so	O
we	O
use	O
a	O
regularizer	O
that	O
penalizes	O
large	O
weight	O
values	O
we	O
modify	O
the	O
objective	B
function	I
to	O
m	O
gw	O
where	O
the	O
simplest	O
choice	O
of	O
regularizer	O
is	O
the	O
weight	B
decay	I
regularizer	O
ew	O
i	O
the	O
regularization	B
constant	I
is	O
called	O
the	O
weight	B
decay	I
rate	B
this	O
additional	O
term	O
favours	O
small	O
values	O
of	O
w	O
and	O
decreases	O
the	O
tendency	O
of	O
a	O
model	B
to	O
details	O
of	O
the	O
training	B
data	I
the	O
quantity	O
is	O
known	O
as	O
a	O
hyperparameter	B
hyperparameters	O
play	O
a	O
role	O
in	O
the	O
learning	B
algorithm	O
but	O
play	O
no	O
role	O
in	O
the	O
activity	B
rule	I
of	O
the	O
network	B
exercise	O
compute	O
the	O
derivative	O
of	O
m	O
with	O
respect	O
to	O
wi	O
why	O
is	O
the	O
above	O
regularizer	O
known	O
as	O
the	O
weight	B
decay	I
regularizer	O
the	O
gradient	B
descent	I
source	O
code	O
of	O
algorithm	O
implements	O
weight	B
decay	I
this	O
gradient	B
descent	I
algorithm	O
is	O
demonstrated	O
in	O
using	O
weight	B
decay	I
rates	O
and	O
as	O
the	O
weight	B
decay	I
rate	B
is	O
increased	O
the	O
solution	O
becomes	O
biased	O
towards	O
broader	O
sigmoid	B
functions	B
with	O
decision	O
boundaries	O
that	O
are	O
closer	O
to	O
the	O
origin	O
note	O
gradient	B
descent	I
with	O
a	O
step	O
size	O
is	O
in	O
general	O
not	O
the	O
most	O
way	O
to	O
minimize	O
a	O
function	O
a	O
of	O
gradient	B
descent	I
known	O
as	O
momentum	B
while	O
improving	O
convergence	O
is	O
also	O
not	O
recommended	O
most	O
neural	B
network	B
experts	O
use	O
more	O
advanced	O
optimizers	O
such	O
as	O
conjugate	B
gradient	I
algorithms	B
do	O
not	O
confuse	O
momentum	B
which	O
is	O
sometimes	O
given	O
the	O
symbol	O
with	O
weight	B
decay	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
single	B
neuron	B
as	O
a	O
further	O
exercises	O
more	O
motivations	O
for	O
the	O
linear	B
neuron	B
exercise	O
consider	O
the	O
task	O
of	O
recognizing	O
which	O
of	O
two	O
gaussian	B
distributions	O
a	O
vector	O
z	O
comes	O
from	O
unlike	O
the	O
case	O
studied	O
in	O
section	B
where	O
the	O
distributions	O
had	O
means	O
but	O
a	O
common	O
variance	B
covariance	B
matrix	B
we	O
will	O
assume	O
that	O
the	O
two	O
distributions	O
have	O
exactly	O
the	O
same	O
mean	B
but	O
variances	O
let	O
the	O
probability	B
of	O
z	O
given	O
s	O
be	O
p	O
s	O
i	O
normalzi	O
si	O
si	O
is	O
the	O
variance	B
of	O
zi	O
when	O
the	O
source	O
symbol	O
is	O
s	O
show	O
that	O
where	O
p	O
z	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
z	O
where	O
xi	O
is	O
an	O
appropriate	O
function	O
of	O
zi	O
xi	O
gzi	O
exercise	O
the	O
noisy	B
led	O
consider	O
an	O
led	O
display	O
with	O
elements	O
numbered	O
as	O
shown	O
above	O
the	O
state	O
of	O
the	O
display	O
is	O
a	O
vector	O
x	O
when	O
the	O
controller	O
wants	O
the	O
display	O
to	O
show	O
character	O
number	O
s	O
e	O
g	O
s	O
each	O
element	O
xj	O
either	O
adopts	O
its	O
intended	O
state	O
cjs	O
with	O
probability	B
or	O
is	O
with	O
probability	B
f	O
let	O
s	O
call	O
the	O
two	O
states	O
of	O
x	O
and	O
assuming	O
that	O
the	O
intended	O
character	O
s	O
is	O
actually	O
a	O
or	O
a	O
what	O
is	O
the	O
probability	B
of	O
s	O
given	O
the	O
state	O
x	O
show	O
that	O
p	O
x	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
x	O
and	O
compute	O
the	O
values	O
of	O
the	O
weights	O
w	O
in	O
the	O
case	O
f	O
assuming	O
that	O
s	O
is	O
one	O
of	O
with	O
prior	B
probabilities	O
ps	O
what	O
is	O
the	O
probability	B
of	O
s	O
given	O
the	O
state	O
x	O
put	O
your	O
answer	O
in	O
the	O
form	O
p	O
x	O
eas	O
where	O
fasg	O
are	O
functions	B
of	O
fcjsg	O
and	O
x	O
could	O
you	O
make	O
a	O
better	O
alphabet	O
of	O
characters	O
for	O
a	O
noisy	B
led	O
i	O
e	O
an	O
alphabet	O
less	O
susceptible	O
to	O
confusion	O
exercise	O
a	O
error-correcting	B
code	I
consists	O
of	O
the	O
two	O
codewords	O
and	O
a	O
source	O
bit	B
s	O
having	O
probability	B
distribution	B
is	O
used	O
to	O
select	O
one	O
of	O
the	O
two	O
codewords	O
for	O
transmission	O
over	O
a	O
binary	B
symmetric	B
channel	I
with	O
noise	O
level	O
f	O
the	O
table	O
an	O
alternative	O
alphabet	O
for	O
the	O
led	O
display	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
received	O
vector	O
is	O
r	O
show	O
that	O
the	O
posterior	B
probability	B
of	O
s	O
given	O
r	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
r	O
and	O
give	O
expressions	O
for	O
the	O
describe	O
with	O
a	O
diagram	O
how	O
this	O
optimal	B
decoder	B
can	O
be	O
expressed	O
in	O
terms	O
of	O
a	O
neuron	B
and	O
the	O
bias	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
problems	O
to	O
look	O
at	O
before	O
chapter	O
exercise	O
what	O
is	O
pn	O
symbol	O
means	O
the	O
combination	B
n	O
exercise	O
if	O
the	O
top	O
row	O
of	O
pascal	O
s	O
triangle	B
contains	O
the	O
single	O
number	O
is	O
denoted	O
row	O
zero	O
what	O
is	O
the	O
sum	O
of	O
all	O
the	O
numbers	O
in	O
the	O
triangle	B
above	O
row	O
n	O
exercise	O
points	O
are	O
selected	O
at	O
random	B
on	O
the	O
surface	O
of	O
a	O
sphere	O
what	O
is	O
the	O
probability	B
that	O
all	O
of	O
them	O
lie	O
on	O
a	O
single	O
hemisphere	O
this	O
chapter	O
s	O
material	O
is	O
originally	O
due	O
to	O
polya	O
and	O
cover	B
and	O
the	O
exposition	O
that	O
follows	O
is	O
yaser	O
abu-mostafa	B
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
capacity	B
of	O
a	O
single	B
neuron	B
ftngn	O
learning	B
algorithm	O
w	O
fxngn	O
w	O
ftngn	O
figure	O
neural	B
network	B
learning	B
viewed	O
as	B
communication	B
fxngn	O
neural	B
network	B
learning	B
as	B
communication	B
at	O
given	O
locations	O
fxngn	O
many	O
neural	B
network	B
models	O
involve	O
the	O
adaptation	O
of	O
a	O
set	B
of	O
weights	O
w	O
in	O
response	O
to	O
a	O
set	B
of	O
data	O
points	O
for	O
example	O
a	O
set	B
of	O
n	O
target	O
values	O
dn	O
ftngn	O
the	O
adapted	O
weights	O
are	O
then	O
used	O
to	O
process	O
subsequent	O
input	O
data	O
this	O
process	O
can	O
be	O
viewed	O
as	O
a	O
communication	B
process	O
in	O
which	O
the	O
sender	O
examines	O
the	O
data	O
dn	O
and	O
creates	O
a	O
message	O
w	O
that	O
depends	O
on	O
those	O
data	O
the	O
receiver	O
then	O
uses	O
w	O
for	O
example	O
the	O
receiver	O
might	O
use	O
the	O
weights	O
to	O
try	O
to	O
reconstruct	O
what	O
the	O
data	O
dn	O
was	O
neural	B
network	B
parlance	O
this	O
is	O
using	O
the	O
neuron	B
for	O
memory	B
rather	O
than	O
for	O
generalization	B
generalizing	O
means	O
extrapolating	O
from	O
the	O
observed	O
data	O
to	O
the	O
value	O
of	O
tn	O
at	O
some	O
new	O
location	O
xn	O
just	O
as	O
a	O
disk	B
drive	I
is	O
a	O
communication	B
channel	O
the	O
adapted	O
network	B
weights	O
w	O
therefore	O
play	O
the	O
role	O
of	O
a	O
communication	B
channel	O
conveying	O
information	B
about	O
the	O
training	B
data	I
to	O
a	O
future	O
user	O
of	O
that	O
neural	O
net	O
the	O
question	O
we	O
now	O
address	B
is	O
what	O
is	O
the	O
capacity	B
of	O
this	O
channel	O
that	O
is	O
how	O
much	O
information	B
can	O
be	O
stored	O
by	O
training	O
a	O
neural	B
network	B
if	O
we	O
had	O
a	O
learning	B
algorithm	O
that	O
either	O
produces	O
a	O
network	B
whose	O
response	O
to	O
all	O
inputs	O
is	O
or	O
a	O
network	B
whose	O
response	O
to	O
all	O
inputs	O
is	O
depending	O
on	O
the	O
training	B
data	I
then	O
the	O
weights	O
allow	O
us	O
to	O
distinguish	O
between	O
just	O
two	O
sorts	O
of	O
data	B
set	B
the	O
maximum	O
information	B
such	O
a	O
learning	B
algorithm	O
could	O
convey	O
about	O
the	O
data	O
is	O
therefore	O
bit	B
this	O
information	B
content	I
being	O
achieved	O
if	O
the	O
two	O
sorts	O
of	O
data	B
set	B
are	O
equiprobable	O
how	O
much	O
more	O
information	B
can	O
be	O
conveyed	O
if	O
we	O
make	O
full	O
use	O
of	O
a	O
neural	B
network	B
s	O
ability	O
to	O
represent	O
other	O
functions	B
the	O
capacity	B
of	O
a	O
single	B
neuron	B
we	O
will	O
look	O
at	O
the	O
simplest	O
case	O
that	O
of	O
a	O
single	O
binary	O
threshold	B
neuron	B
we	O
will	O
that	O
the	O
capacity	B
of	O
such	O
a	O
neuron	B
is	O
two	O
bits	O
per	O
weight	O
a	O
neuron	B
with	O
k	O
inputs	O
can	O
store	O
bits	O
of	O
information	B
to	O
obtain	O
this	O
interesting	O
result	O
we	O
lay	O
down	O
some	O
rules	B
to	O
exclude	O
less	O
interesting	O
answers	O
such	O
as	O
the	O
capacity	B
of	O
a	O
neuron	B
is	O
because	O
each	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
capacity	B
of	O
a	O
single	B
neuron	B
of	O
its	O
weights	O
is	O
a	O
real	O
number	O
and	O
so	O
can	O
convey	O
an	O
number	O
of	O
bits	O
we	O
exclude	O
this	O
answer	O
by	O
saying	O
that	O
the	O
receiver	O
is	O
not	O
able	O
to	O
examine	O
the	O
weights	O
directly	O
nor	O
is	O
the	O
receiver	O
allowed	O
to	O
probe	O
the	O
weights	O
by	O
observing	O
the	O
output	O
of	O
the	O
neuron	B
for	O
arbitrarily	O
chosen	O
inputs	O
we	O
constrain	O
the	O
receiver	O
to	O
observe	O
the	O
output	O
of	O
the	O
neuron	B
at	O
the	O
same	O
set	B
of	O
n	O
points	O
fxng	O
that	O
were	O
in	O
the	O
training	O
set	B
what	O
matters	O
now	O
is	O
how	O
many	O
distinguishable	O
functions	B
our	O
neuron	B
can	O
produce	O
given	O
that	O
we	O
can	O
observe	O
the	O
function	O
only	O
at	O
these	O
n	O
points	O
how	O
many	O
binary	O
labellings	O
of	O
n	O
points	O
can	O
a	O
linear	B
threshold	B
function	O
produce	O
and	O
how	O
does	O
this	O
number	O
compare	O
with	O
the	O
maximum	O
possible	O
number	O
of	O
binary	O
labellings	O
if	O
nearly	O
all	O
of	O
the	O
labellings	O
can	O
be	O
realized	O
by	O
our	O
neuron	B
then	O
it	O
is	O
a	O
communication	B
channel	O
that	O
can	O
convey	O
all	O
n	O
bits	O
target	O
values	O
ftng	O
with	O
small	O
probability	B
of	I
error	I
we	O
will	O
identify	O
the	O
capacity	B
of	O
the	O
neuron	B
as	O
the	O
maximum	O
value	O
that	O
n	O
can	O
have	O
such	O
that	O
the	O
probability	B
of	I
error	I
is	O
very	O
small	O
are	O
departing	O
a	O
little	O
from	O
the	O
of	O
capacity	B
in	O
chapter	O
we	O
thus	O
examine	O
the	O
following	O
scenario	O
the	O
sender	O
is	O
given	O
a	O
neuron	B
with	O
k	O
inputs	O
and	O
a	O
data	B
set	B
dn	O
which	O
is	O
a	O
labelling	O
of	O
n	O
points	O
the	O
sender	O
uses	O
an	O
adaptive	B
algorithm	O
to	O
try	O
to	O
a	O
w	O
that	O
can	O
reproduce	O
this	O
labelling	O
exactly	O
we	O
will	O
assume	O
the	O
algorithm	O
such	O
a	O
w	O
if	O
it	O
exists	O
the	O
receiver	O
then	O
evaluates	O
the	O
threshold	B
function	O
on	O
the	O
n	O
input	O
values	O
what	O
is	O
the	O
probability	B
that	O
all	O
n	O
bits	O
are	O
correctly	O
reproduced	O
how	O
large	O
can	O
n	O
become	O
for	O
a	O
given	O
k	O
without	O
this	O
probability	B
becoming	O
substantially	O
less	O
than	O
one	O
general	B
position	I
one	O
technical	O
detail	O
needs	O
to	O
be	O
pinned	O
down	O
what	O
set	B
of	O
inputs	O
fxng	O
are	O
we	O
considering	O
our	O
answer	O
might	O
depend	O
on	O
this	O
choice	O
we	O
will	O
assume	O
that	O
the	O
points	O
are	O
in	O
general	B
position	I
a	O
set	B
of	O
points	O
fxng	O
in	O
k-dimensional	O
space	O
are	O
in	O
general	B
position	I
if	O
any	O
subset	B
of	O
size	O
k	O
is	O
linearly	O
independent	O
and	O
no	O
k	O
of	O
them	O
lie	O
in	O
a	O
plane	O
in	O
k	O
dimensions	B
for	O
example	O
a	O
set	B
of	O
points	O
are	O
in	O
general	B
position	I
if	O
no	O
three	O
points	O
are	O
colinear	O
and	O
no	O
four	O
points	O
are	O
coplanar	O
the	O
intuitive	O
idea	O
is	O
that	O
points	O
in	O
general	B
position	I
are	O
like	O
random	B
points	O
in	O
the	O
space	O
in	O
terms	O
of	O
the	O
linear	B
dependences	O
between	O
points	O
you	O
don	O
t	O
expect	O
three	O
random	B
points	O
in	O
three	O
dimensions	B
to	O
lie	O
on	O
a	O
straight	O
line	O
the	O
linear	B
threshold	B
function	O
the	O
neuron	B
we	O
will	O
consider	O
performs	O
the	O
function	O
where	O
y	O
f	O
k	O
f	O
wkxk	O
a	O
a	O
we	O
will	O
not	O
have	O
a	O
bias	B
the	O
capacity	B
for	O
a	O
neuron	B
with	O
a	O
bias	B
can	O
be	O
obtained	O
by	O
replacing	O
k	O
by	O
k	O
in	O
the	O
result	O
below	O
i	O
e	O
considering	O
one	O
of	O
the	O
inputs	O
to	O
be	O
to	O
input	O
points	O
would	O
not	O
then	O
be	O
in	O
general	B
position	I
the	O
derivation	B
still	O
works	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
counting	B
threshold	B
functions	B
figure	O
one	O
data	O
point	O
in	O
a	O
two-dimensional	B
input	O
space	O
and	O
the	O
two	O
regions	O
of	O
weight	B
space	I
that	O
give	O
the	O
two	O
alternative	O
labellings	O
of	O
that	O
point	O
counting	B
threshold	B
functions	B
let	O
us	O
denote	O
by	O
t	O
k	O
the	O
number	O
of	O
distinct	O
threshold	B
functions	B
on	O
n	O
points	O
in	O
general	B
position	I
in	O
k	O
dimensions	B
we	O
will	O
derive	O
a	O
formula	O
for	O
t	O
k	O
to	O
start	O
with	O
let	O
us	O
work	O
out	O
a	O
few	O
cases	O
by	O
hand	O
in	O
k	O
dimension	O
for	O
any	O
n	O
the	O
n	O
points	O
lie	O
on	O
a	O
line	O
by	O
changing	O
the	O
sign	O
of	O
the	O
one	O
weight	O
we	O
can	O
label	O
all	O
points	O
on	O
the	O
right	O
side	O
of	O
the	O
origin	O
and	O
the	O
others	O
or	O
vice	O
versa	O
thus	O
there	O
are	O
two	O
distinct	O
threshold	B
functions	B
t	O
with	O
n	O
point	O
for	O
any	O
k	O
if	O
there	O
is	O
just	O
one	O
point	O
then	O
we	O
can	O
realize	O
both	O
possible	O
labellings	O
by	O
setting	O
w	O
thus	O
t	O
k	O
in	O
k	O
dimensions	B
in	O
two	O
dimensions	B
with	O
n	O
points	O
we	O
are	O
free	O
to	O
spin	O
the	O
separating	O
line	O
around	O
the	O
origin	O
each	O
time	O
the	O
line	O
passes	O
over	O
a	O
point	O
we	O
obtain	O
a	O
new	O
function	O
once	O
we	O
have	O
spun	O
the	O
line	O
through	O
degrees	O
we	O
reproduce	O
the	O
function	O
we	O
started	O
from	O
because	O
the	O
points	O
are	O
in	O
general	B
position	I
the	O
separating	O
plane	O
crosses	O
only	O
one	O
point	O
at	O
a	O
time	O
in	O
one	O
revolution	O
every	O
point	O
is	O
passed	O
over	O
twice	O
there	O
are	O
therefore	O
distinct	O
threshold	B
functions	B
t	O
comparing	O
with	O
the	O
total	O
number	O
of	O
binary	O
functions	B
we	O
may	O
note	O
that	O
for	O
n	O
not	O
all	O
binary	O
functions	B
can	O
be	O
realized	O
by	O
a	O
linear	B
threshold	B
function	O
one	O
famous	O
example	O
of	O
an	O
unrealizable	O
function	O
with	O
n	O
and	O
k	O
is	O
the	O
exclusive-or	O
function	O
on	O
the	O
points	O
x	O
points	O
are	O
not	O
in	O
general	B
position	I
but	O
you	O
may	O
that	O
the	O
function	O
remains	O
unrealizable	O
even	O
if	O
the	O
points	O
are	O
perturbed	O
into	O
general	B
position	I
in	O
k	O
dimensions	B
from	O
the	O
point	O
of	O
view	O
of	O
weight	B
space	I
there	O
is	O
another	O
way	O
of	O
visualizing	O
this	O
problem	O
instead	O
of	O
visualizing	O
a	O
plane	O
separating	O
points	O
in	O
the	O
two-dimensional	B
input	O
space	O
we	O
can	O
consider	O
the	O
two-dimensional	B
weight	B
space	I
colouring	O
regions	O
in	O
weight	B
space	I
colours	O
if	O
they	O
label	O
the	O
given	O
datapoints	O
we	O
can	O
then	O
count	O
the	O
number	O
of	O
threshold	B
functions	B
by	O
counting	B
how	O
many	O
distinguishable	O
regions	O
there	O
are	O
in	O
weight	B
space	I
consider	O
the	O
set	B
of	O
weight	O
vectors	B
in	O
weight	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
capacity	B
of	O
a	O
single	B
neuron	B
figure	O
two	O
data	O
points	O
in	O
a	O
two-dimensional	B
input	O
space	O
and	O
the	O
four	O
regions	O
of	O
weight	B
space	I
that	O
give	O
the	O
four	O
alternative	O
labellings	O
figure	O
three	O
data	O
points	O
in	O
a	O
two-dimensional	B
input	O
space	O
and	O
the	O
six	B
regions	O
of	O
weight	B
space	I
that	O
give	O
alternative	O
labellings	O
of	O
those	O
points	O
in	O
this	O
case	O
the	O
labellings	O
and	O
cannot	O
be	O
realized	O
for	O
any	O
three	O
points	O
in	O
general	B
position	I
there	O
are	O
always	O
two	O
labellings	O
that	O
cannot	O
be	O
realized	O
space	O
that	O
classify	O
a	O
particular	O
example	O
xn	O
as	O
a	O
for	O
example	O
shows	O
a	O
single	O
point	O
in	O
our	O
two-dimensional	B
x-space	O
and	O
shows	O
the	O
two	O
corresponding	O
sets	O
of	O
points	O
in	O
w-space	O
one	O
set	B
of	O
weight	O
vectors	B
occupy	O
the	O
half	O
space	O
and	O
the	O
others	O
occupy	O
in	O
we	O
have	O
added	O
a	O
second	O
point	O
in	O
the	O
input	O
space	O
there	O
are	O
now	O
possible	O
labellings	O
and	O
figure	O
shows	O
the	O
two	O
hyperplanes	O
and	O
which	O
separate	O
the	O
sets	O
of	O
weight	O
vectors	B
that	O
produce	O
each	O
of	O
these	O
labellings	O
when	O
n	O
weight	B
space	I
is	O
divided	O
by	O
three	O
hyperplanes	O
into	O
six	B
regions	O
not	O
all	O
of	O
the	O
eight	O
conceivable	O
labellings	O
can	O
be	O
realized	O
thus	O
t	O
in	O
k	O
dimensions	B
we	O
now	O
use	O
this	O
weight	B
space	I
visualization	B
to	O
study	O
the	O
three	O
dimensional	O
case	O
let	O
us	O
imagine	O
adding	O
one	O
point	O
at	O
a	O
time	O
and	O
count	O
the	O
number	O
of	O
threshold	B
functions	B
as	O
we	O
do	O
so	O
when	O
n	O
weight	B
space	I
is	O
divided	O
by	O
two	O
hyperplanes	O
and	O
into	O
four	O
regions	O
in	O
any	O
one	O
region	O
all	O
vectors	B
w	O
produce	O
the	O
same	O
function	O
on	O
the	O
input	O
vectors	B
thus	O
t	O
adding	O
a	O
third	O
point	O
in	O
general	B
position	I
produces	O
a	O
third	O
plane	O
in	O
w	O
space	O
so	O
that	O
there	O
are	O
distinguishable	O
regions	O
t	O
the	O
three	O
bisecting	O
planes	O
are	O
shown	O
in	O
at	O
this	O
point	O
matters	O
become	O
slightly	O
more	O
tricky	O
as	O
illustrates	O
the	O
fourth	O
plane	O
in	O
the	O
three-dimensional	O
w	O
space	O
cannot	O
transect	O
all	O
eight	O
of	O
the	O
sets	O
created	O
by	O
the	O
three	O
planes	O
six	B
of	O
the	O
existing	O
regions	O
are	O
cut	O
in	O
two	O
and	O
the	O
remaining	O
two	O
are	O
so	O
t	O
two	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
counting	B
threshold	B
functions	B
k	O
n	O
figure	O
weight	B
space	I
illustrations	O
for	O
t	O
and	O
t	O
t	O
three	O
hyperplanes	O
to	O
three	O
points	O
in	O
general	B
position	I
divide	O
into	O
regions	O
shown	O
here	O
by	O
colouring	O
the	O
relevant	O
part	O
of	O
the	O
surface	O
of	O
a	O
hollow	O
semi-transparent	O
cube	O
centred	O
on	O
the	O
origin	O
t	O
four	O
hyperplanes	O
divide	O
into	O
regions	O
of	O
which	O
this	O
shows	O
region	O
is	O
out	O
of	O
view	O
on	O
the	O
right-hand	O
face	O
compare	O
with	O
all	O
of	O
the	O
regions	O
that	O
are	O
not	O
coloured	B
white	B
have	O
been	O
cut	O
into	O
two	O
table	O
values	O
of	O
t	O
k	O
deduced	O
by	O
hand	O
figure	O
illustration	O
of	O
the	O
cutting	O
process	O
going	O
from	O
t	O
to	O
t	O
the	O
eight	O
regions	O
of	O
with	O
one	O
added	O
hyperplane	O
all	O
of	O
the	O
regions	O
that	O
are	O
not	O
coloured	B
white	B
have	O
been	O
cut	O
into	O
two	O
here	O
the	O
hollow	O
cube	O
has	O
been	O
made	O
solid	O
so	O
we	O
can	O
see	O
which	O
regions	O
are	O
cut	O
by	O
the	O
fourth	O
plane	O
the	O
front	O
half	O
of	O
the	O
cube	O
has	O
been	O
cut	O
away	O
this	O
shows	O
the	O
new	O
two	O
dimensional	O
hyperplane	O
which	O
is	O
divided	O
into	O
six	B
regions	O
by	O
the	O
three	O
one-dimensional	O
hyperplanes	O
which	O
cross	O
it	O
each	O
of	O
these	O
regions	O
corresponds	O
to	O
one	O
of	O
the	O
three-dimensional	O
regions	O
in	O
which	O
is	O
cut	O
into	O
two	O
by	O
this	O
new	O
hyperplane	O
this	O
shows	O
that	O
t	O
t	O
figure	O
should	O
be	O
compared	O
with	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
capacity	B
of	O
a	O
single	B
neuron	B
of	O
the	O
binary	O
functions	B
on	O
points	O
in	O
dimensions	B
cannot	O
be	O
realized	O
by	O
a	O
linear	B
threshold	B
function	O
we	O
have	O
now	O
in	O
the	O
values	O
of	O
t	O
k	O
shown	O
in	O
table	O
can	O
we	O
obtain	O
any	O
insights	O
into	O
our	O
derivation	B
of	O
t	O
in	O
order	O
to	O
in	O
the	O
rest	O
of	O
the	O
table	O
for	O
t	O
k	O
why	O
was	O
t	O
greater	O
than	O
t	O
by	O
six	B
six	B
is	O
the	O
number	O
of	O
regions	O
that	O
the	O
new	O
hyperplane	O
bisected	O
in	O
w-space	O
b	O
equivalently	O
if	O
we	O
look	O
in	O
the	O
k	O
dimensional	O
subspace	O
that	O
is	O
the	O
n	O
th	O
hyperplane	O
that	O
subspace	O
is	O
divided	O
into	O
six	B
regions	O
by	O
the	O
previous	O
hyperplanes	O
now	O
this	O
is	O
a	O
concept	O
we	O
have	O
met	O
before	O
compare	O
with	O
how	O
many	O
regions	O
are	O
created	O
by	O
n	O
hyperplanes	O
in	O
a	O
dimensional	O
space	O
why	O
t	O
of	O
course	O
in	O
the	O
present	O
case	O
n	O
k	O
we	O
can	O
look	O
up	O
t	O
in	O
the	O
previous	O
section	B
so	O
t	O
t	O
t	O
recurrence	O
relation	O
for	O
any	O
n	O
k	O
generalizing	O
this	O
picture	O
we	O
see	O
that	O
when	O
we	O
add	O
an	O
n	O
th	O
hyperplane	O
in	O
k	O
dimensions	B
it	O
will	O
bisect	O
t	O
of	O
the	O
t	O
k	O
regions	O
that	O
were	O
created	O
by	O
the	O
previous	O
n	O
hyperplanes	O
therefore	O
the	O
total	O
number	O
of	O
regions	O
obtained	O
after	O
adding	O
the	O
n	O
th	O
hyperplane	O
is	O
t	O
out	O
of	O
t	O
k	O
regions	O
are	O
split	O
in	O
two	O
plus	O
the	O
remaining	O
t	O
k	O
t	O
regions	O
not	O
split	O
by	O
the	O
n	O
th	O
hyperplane	O
which	O
gives	O
the	O
following	O
equation	O
for	O
t	O
k	O
t	O
k	O
t	O
k	O
t	O
now	O
all	O
that	O
remains	O
is	O
to	O
solve	O
this	O
recurrence	O
relation	O
given	O
the	O
boundary	O
conditions	O
t	O
and	O
t	O
k	O
does	O
the	O
recurrence	O
relation	O
look	O
familiar	O
maybe	O
you	O
remember	O
building	O
pascal	O
s	O
triangle	B
by	O
adding	O
together	O
two	O
adjacent	O
numbers	O
in	O
one	O
row	O
to	O
get	O
the	O
number	O
below	O
the	O
n	O
k	O
element	O
of	O
pascal	O
s	O
triangle	B
is	O
equal	O
to	O
cn	O
k	O
n	O
k	O
k	O
table	O
pascal	O
s	O
triangle	B
n	O
k	O
satisfy	O
the	O
equation	O
combinations	O
cn	O
k	O
k	O
we	O
are	O
adopting	O
the	O
convention	O
that	O
if	O
k	O
n	O
or	O
k	O
the	O
required	O
recurrence	O
relation	O
this	O
doesn	O
t	O
mean	B
so	O
since	O
many	O
functions	B
can	O
satisfy	O
one	O
recurrence	O
relation	O
t	O
k	O
for	O
all	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
counting	B
threshold	B
functions	B
n	O
k	O
nk	O
k	O
nk	O
n	O
log	O
tnk	O
log	O
nk	O
nk	O
figure	O
the	O
fraction	O
of	O
functions	B
on	O
n	O
points	O
in	O
k	O
dimensions	B
that	O
are	O
linear	B
threshold	B
functions	B
t	O
shown	O
from	O
various	O
viewpoints	O
in	O
we	O
see	O
the	O
dependence	O
on	O
k	O
which	O
is	O
approximately	O
an	O
error	B
function	I
passing	O
through	O
at	O
k	O
the	O
fraction	O
reaches	O
at	O
k	O
n	O
in	O
we	O
see	O
the	O
dependence	O
on	O
n	O
which	O
is	O
up	O
to	O
n	O
k	O
and	O
drops	O
sharply	O
at	O
n	O
panel	O
shows	O
the	O
dependence	O
on	O
nk	O
for	O
k	O
there	O
is	O
a	O
sudden	O
drop	O
in	O
the	O
fraction	O
of	O
realizable	O
labellings	O
when	O
n	O
panel	O
shows	O
the	O
values	O
of	O
t	O
k	O
and	O
as	O
a	O
function	O
of	O
n	O
for	O
k	O
these	O
were	O
plotted	O
using	O
the	O
approximation	B
of	O
t	O
by	O
the	O
error	B
function	I
but	O
perhaps	O
we	O
can	O
express	O
t	O
k	O
as	O
a	O
linear	B
superposition	O
of	O
combination	B
functions	B
of	O
the	O
form	O
k	O
we	O
can	O
see	O
how	O
to	O
satisfy	O
the	O
boundary	O
conditions	O
we	O
simply	O
need	O
to	O
translate	O
pascal	O
s	O
triangle	B
to	O
the	O
right	O
by	O
superpose	O
add	O
multiply	O
by	O
two	O
and	O
drop	O
the	O
whole	O
table	O
by	O
one	O
line	O
thus	O
by	O
comparing	O
tables	O
and	O
t	O
k	O
k	O
using	O
the	O
fact	O
that	O
the	O
n	O
th	O
row	O
of	O
pascal	O
s	O
triangle	B
sums	O
to	O
that	O
is	O
k	O
we	O
can	O
simplify	O
the	O
cases	O
where	O
t	O
k	O
k	O
n	O
k	O
k	O
n	O
interpretation	O
it	O
is	O
natural	B
to	O
compare	O
t	O
k	O
with	O
the	O
total	O
number	O
of	O
binary	O
functions	B
on	O
n	O
points	O
the	O
ratio	O
t	O
tells	O
us	O
the	O
probability	B
that	O
an	O
arbitrary	O
labelling	O
ftngn	O
can	O
be	O
memorized	O
by	O
our	O
neuron	B
the	O
two	O
functions	B
are	O
equal	O
for	O
all	O
n	O
k	O
the	O
line	O
n	O
k	O
is	O
thus	O
a	O
special	O
line	O
the	O
maximum	O
number	O
of	O
points	O
on	O
which	O
any	O
arbitrary	O
labelling	O
can	O
be	O
realized	O
this	O
number	O
of	O
points	O
is	O
referred	O
to	O
as	O
the	O
vapnikchervonenkis	O
dimension	O
dimension	O
of	O
the	O
class	O
of	O
functions	B
the	O
vc	B
dimension	I
of	O
a	O
binary	O
threshold	B
function	O
on	O
k	O
dimensions	B
is	O
thus	O
k	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
capacity	B
of	O
a	O
single	B
neuron	B
what	O
is	O
interesting	O
is	O
large	O
k	O
the	O
number	O
of	O
points	O
n	O
such	O
that	O
almost	O
any	O
labelling	O
can	O
be	O
realized	O
the	O
ratio	O
t	O
is	O
for	O
n	O
still	O
greater	O
than	O
and	O
for	O
large	O
k	O
the	O
ratio	O
is	O
very	O
close	O
to	O
for	O
our	O
purposes	O
the	O
sum	O
in	O
equation	O
is	O
well	O
approximated	O
by	O
the	O
error	B
function	I
k	O
pn	O
k	O
figure	O
shows	O
the	O
realizable	O
fraction	O
where	O
z	O
t	O
as	O
a	O
function	O
of	O
n	O
and	O
k	O
the	O
take-home	O
message	O
is	O
shown	O
in	O
although	O
the	O
fraction	O
t	O
is	O
less	O
than	O
for	O
n	O
k	O
it	O
is	O
only	O
negligibly	O
less	O
than	O
up	O
to	O
n	O
there	O
there	O
is	O
a	O
catastrophic	O
drop	O
to	O
zero	O
so	O
that	O
for	O
n	O
only	O
a	O
tiny	O
fraction	O
of	O
the	O
binary	O
labellings	O
can	O
be	O
realized	O
by	O
the	O
threshold	B
function	O
conclusion	O
the	O
capacity	B
of	O
a	O
linear	B
threshold	B
neuron	B
for	O
large	O
k	O
is	O
bits	O
per	O
weight	O
a	O
single	B
neuron	B
can	O
almost	O
certainly	O
memorize	O
up	O
to	O
n	O
random	B
binary	O
labels	O
perfectly	O
but	O
will	O
almost	O
certainly	O
fail	O
to	O
memorize	O
more	O
further	O
exercises	O
exercise	O
can	O
a	O
set	B
of	O
distinct	O
points	O
in	O
a	O
two-dimensional	B
space	O
be	O
split	O
in	O
half	O
by	O
a	O
straight	O
line	O
if	O
the	O
points	O
are	O
in	O
general	B
position	I
if	O
the	O
points	O
are	O
not	O
in	O
general	B
position	I
can	O
points	O
in	O
a	O
k	O
dimensional	O
space	O
be	O
split	O
in	O
half	O
by	O
a	O
k	O
dimensional	O
hyperplane	O
exercise	O
four	O
points	O
are	O
selected	O
at	O
random	B
on	O
the	O
surface	O
of	O
a	O
sphere	O
what	O
is	O
the	O
probability	B
that	O
all	O
of	O
them	O
lie	O
on	O
a	O
single	O
hemisphere	O
how	O
does	O
this	O
question	O
relate	O
to	O
t	O
k	O
exercise	O
consider	O
the	O
binary	O
threshold	B
neuron	B
in	O
k	O
dimensions	B
and	O
the	O
set	B
of	O
points	O
fxg	O
find	O
a	O
parameter	O
vector	O
w	O
such	O
that	O
the	O
neuron	B
memorizes	O
the	O
labels	O
ftg	O
ftg	O
find	O
an	O
unrealizable	O
labelling	O
ftg	O
exercise	O
in	O
this	O
chapter	O
we	O
constrained	B
all	O
our	O
hyperplanes	O
to	O
go	O
through	O
the	O
origin	O
in	O
this	O
exercise	O
we	O
remove	O
this	O
constraint	O
how	O
many	O
regions	O
in	O
a	O
plane	O
are	O
created	O
by	O
n	O
lines	O
in	O
general	B
position	I
exercise	O
estimate	O
in	O
bits	O
the	O
total	O
sensory	O
experience	O
that	O
you	O
have	O
had	O
in	O
your	O
life	B
visual	O
information	B
auditory	O
information	B
etc	O
estimate	O
how	O
much	O
information	B
you	O
have	O
memorized	O
estimate	O
the	O
information	B
content	I
of	O
the	O
works	O
of	O
shakespeare	O
compare	O
these	O
with	O
the	O
capacity	B
of	O
your	O
brain	B
assuming	O
you	O
have	O
neurons	O
each	O
making	O
synaptic	O
connections	O
and	O
that	O
the	O
capacity	B
result	O
for	O
one	O
neuron	B
bits	O
per	O
connection	O
applies	O
is	O
your	O
brain	B
full	O
yet	O
figure	O
three	O
lines	O
in	O
a	O
plane	O
create	O
seven	O
regions	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
exercise	O
what	O
is	O
the	O
capacity	B
of	O
the	O
axon	O
of	O
a	O
spiking	O
neuron	B
viewed	O
in	O
bits	O
per	O
second	O
mackay	B
and	O
as	O
a	O
communication	B
channel	O
mcculloch	O
for	O
an	O
early	O
publication	O
on	O
this	O
topic	O
multiply	O
by	O
the	O
number	O
of	O
axons	O
in	O
the	O
optic	B
nerve	I
or	O
cochlear	O
nerve	O
per	O
ear	O
to	O
estimate	O
again	O
the	O
rate	B
of	O
acquisition	O
sensory	O
experience	O
solutions	O
solution	O
to	O
exercise	O
the	O
probability	B
that	O
all	O
four	O
points	O
lie	O
on	O
a	O
single	O
hemisphere	O
is	O
t	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
learning	B
as	B
inference	B
neural	B
network	B
learning	B
as	B
inference	B
in	O
chapter	O
we	O
trained	O
a	O
simple	O
neural	B
network	B
as	O
a	O
by	O
minimizing	O
an	O
objective	B
function	I
m	O
gw	O
made	O
up	O
of	O
an	O
error	B
function	I
gw	O
htn	O
ln	O
yxn	O
w	O
tn	O
yxn	O
wi	O
and	O
a	O
regularizer	O
ew	O
i	O
this	O
neural	B
network	B
learning	B
process	O
can	O
be	O
given	O
the	O
following	O
probabilistic	O
interpretation	O
we	O
interpret	O
the	O
output	O
yx	O
w	O
of	O
the	O
neuron	B
literally	O
as	O
its	O
parameters	B
w	O
are	O
the	O
probability	B
that	O
an	O
input	O
x	O
belongs	O
to	O
class	O
t	O
rather	O
than	O
the	O
alternative	O
t	O
thus	O
yx	O
w	O
p	O
x	O
w	O
then	O
each	O
value	O
of	O
w	O
a	O
hypothesis	O
about	O
the	O
probability	B
of	O
class	O
relative	B
to	O
class	O
as	O
a	O
function	O
of	O
x	O
we	O
the	O
observed	O
data	O
d	O
to	O
be	O
the	O
targets	O
ftg	O
the	O
inputs	O
fxg	O
are	O
assumed	O
to	O
be	O
given	O
and	O
not	O
to	O
be	O
modelled	O
to	O
infer	O
w	O
given	O
the	O
data	O
we	O
require	O
a	O
likelihood	B
function	O
and	O
a	O
prior	B
probability	B
over	O
w	O
the	O
likelihood	B
function	O
measures	O
how	O
well	O
the	O
parameters	B
w	O
predict	O
the	O
observed	O
data	O
it	O
is	O
the	O
probability	B
assigned	O
to	O
the	O
observed	O
t	O
values	O
by	O
the	O
model	B
with	O
parameters	B
set	B
to	O
w	O
now	O
the	O
two	O
equations	O
p	O
w	O
x	O
y	O
p	O
w	O
x	O
y	O
can	O
be	O
rewritten	O
as	O
the	O
single	O
equation	O
p	O
w	O
x	O
expt	O
ln	O
y	O
t	O
y	O
so	O
the	O
error	B
function	I
g	O
can	O
be	O
interpreted	O
as	O
minus	O
the	O
log	O
likelihood	B
p	O
j	O
w	O
similarly	O
the	O
regularizer	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
a	O
log	O
prior	B
proba	O
bility	O
distribution	B
over	O
the	O
parameters	B
p	O
j	O
zw	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
illustration	O
for	O
a	O
neuron	B
with	O
two	O
weights	O
if	O
ew	O
is	O
quadratic	O
as	O
above	O
then	O
the	O
corresponding	O
prior	B
distribution	B
w	O
and	O
is	O
equal	O
to	O
is	O
a	O
gaussian	B
with	O
variance	B
where	O
k	O
is	O
the	O
number	O
of	O
parameters	B
in	O
the	O
vector	O
w	O
the	O
objective	B
function	I
m	O
then	O
corresponds	O
to	O
the	O
inference	B
of	O
the	O
parameters	B
w	O
given	O
the	O
data	O
p	O
j	O
d	O
p	O
j	O
wp	O
j	O
p	O
j	O
p	O
j	O
zm	O
so	O
the	O
w	O
found	O
by	O
minimizing	O
m	O
can	O
be	O
interpreted	O
as	O
the	O
most	O
probable	O
parameter	O
vector	O
from	O
now	O
on	O
we	O
will	O
refer	O
to	O
as	O
wmp	O
why	O
is	O
it	O
natural	B
to	O
interpret	O
the	O
error	O
functions	B
as	O
log	O
probabilities	O
error	O
functions	B
are	O
usually	O
additive	O
for	O
example	O
g	O
is	O
a	O
sum	O
of	O
information	B
contents	O
and	O
ew	O
is	O
a	O
sum	O
of	O
squared	O
weights	O
probabilities	O
on	O
the	O
other	O
hand	O
are	O
multiplicative	O
for	O
independent	O
events	O
x	O
and	O
y	O
the	O
joint	B
probability	B
is	O
p	O
y	O
p	O
the	O
logarithmic	O
mapping	B
maintains	O
this	O
correspondence	O
the	O
interpretation	O
of	O
m	O
as	O
a	O
log	O
probability	B
has	O
numerous	O
some	O
of	O
which	O
we	O
will	O
discuss	O
in	O
a	O
moment	O
illustration	O
for	O
a	O
neuron	B
with	O
two	O
weights	O
in	O
the	O
case	O
of	O
a	O
neuron	B
with	O
just	O
two	O
inputs	O
and	O
no	O
bias	B
yx	O
w	O
we	O
can	O
plot	O
the	O
posterior	B
probability	B
of	O
w	O
p	O
j	O
d	O
imagine	O
that	O
we	O
receive	O
some	O
data	O
as	O
shown	O
in	O
the	O
left	O
column	O
of	O
each	O
data	O
point	O
consists	O
of	O
a	O
two-dimensional	B
input	O
vector	O
x	O
and	O
a	O
t	O
value	O
indicated	O
by	O
or	O
the	O
likelihood	B
function	O
is	O
shown	O
as	O
a	O
function	O
of	O
w	O
in	O
the	O
second	O
column	O
it	O
is	O
a	O
product	O
of	O
functions	B
of	O
the	O
form	O
the	O
product	O
of	O
traditional	O
learning	B
is	O
a	O
point	O
in	O
w-space	O
the	O
estimator	B
which	O
maximizes	O
the	O
posterior	B
probability	B
density	B
in	O
contrast	O
in	O
the	O
bayesian	B
view	O
the	O
product	O
of	O
learning	B
is	O
an	O
ensemble	B
of	O
plausible	O
parameter	O
values	O
right	O
of	O
we	O
do	O
not	O
choose	O
one	O
particular	O
hypothesis	O
w	O
rather	O
we	O
evaluate	O
their	O
posterior	O
probabilities	O
the	O
posterior	O
distribution	B
is	O
obtained	O
by	O
multiplying	O
the	O
likelihood	B
by	O
a	O
prior	B
distribution	B
over	O
w	O
space	O
as	O
a	O
broad	O
gaussian	B
at	O
the	O
upper	O
right	O
of	O
the	O
posterior	O
ensemble	B
a	O
multiplicative	O
constant	O
is	O
shown	O
in	O
the	O
third	O
column	O
of	O
and	O
as	O
a	O
contour	O
plot	O
in	O
the	O
fourth	O
column	O
as	O
the	O
amount	O
of	O
data	O
increases	O
top	O
to	O
bottom	O
the	O
posterior	O
ensemble	B
becomes	O
increasingly	O
concentrated	O
around	O
the	O
most	O
probable	O
value	O
beyond	O
optimization	B
making	O
predictions	O
let	O
us	O
consider	O
the	O
task	O
of	O
making	O
predictions	O
with	O
the	O
neuron	B
which	O
we	O
trained	O
as	O
a	O
in	O
section	B
this	O
was	O
a	O
neuron	B
with	O
two	O
inputs	O
and	O
a	O
bias	B
yx	O
w	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
data	B
set	B
likelihood	B
probability	B
of	O
parameters	B
learning	B
as	B
inference	B
x	O
n	O
n	O
n	O
x	O
n	O
x	O
figure	O
the	O
bayesian	B
interpretation	O
and	O
generalization	B
of	O
traditional	O
neural	B
network	B
learning	B
evolution	B
of	O
the	O
probability	B
distribution	B
over	O
parameters	B
as	O
data	O
arrive	O
a	O
b	O
samples	O
from	O
pwdh	O
wmp	O
w	O
a	O
b	O
figure	O
making	O
predictions	O
the	O
function	O
performed	O
by	O
an	O
optimized	O
neuron	B
wmp	O
by	O
three	O
of	O
its	O
contours	O
trained	O
with	O
weight	B
decay	I
the	O
contours	O
shown	O
are	O
those	O
corresponding	O
to	O
a	O
namely	O
y	O
and	O
are	O
these	O
predictions	O
more	O
reasonable	O
shown	O
are	O
for	O
y	O
and	O
the	O
posterior	B
probability	B
of	O
w	O
the	O
bayesian	B
predictions	O
shown	O
in	O
were	O
obtained	O
by	O
averaging	O
together	O
the	O
predictions	O
made	O
by	O
each	O
possible	O
value	O
of	O
the	O
weights	O
w	O
with	O
each	O
value	O
of	O
w	O
receiving	O
a	O
vote	O
proportional	O
to	O
its	O
probability	B
under	O
the	O
posterior	O
ensemble	B
the	O
method	O
used	O
to	O
create	O
is	O
described	O
in	O
section	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
beyond	O
optimization	B
making	O
predictions	O
when	O
we	O
last	O
played	O
with	O
it	O
we	O
trained	O
it	O
by	O
minimizing	O
the	O
objective	B
function	I
m	O
gw	O
the	O
resulting	O
optimized	O
function	O
for	O
the	O
case	O
is	O
reproduced	O
in	O
we	O
now	O
consider	O
the	O
task	O
of	O
predicting	O
the	O
class	O
corresponding	O
to	O
a	O
new	O
input	O
it	O
is	O
common	O
practice	O
when	O
making	O
predictions	O
simply	O
to	O
use	O
a	O
neural	B
network	B
with	O
its	O
weights	O
to	O
their	O
optimized	O
value	O
wmp	O
but	O
this	O
is	O
not	O
optimal	B
as	O
can	O
be	O
seen	O
intuitively	O
by	O
considering	O
the	O
predictions	O
shown	O
in	O
are	O
these	O
reasonable	O
predictions	O
consider	O
new	O
data	O
arriving	O
at	O
points	O
a	O
and	O
b	O
the	O
model	B
assigns	O
both	O
of	O
these	O
examples	O
probability	B
of	O
being	O
in	O
class	O
because	O
they	O
have	O
the	O
same	O
value	O
of	O
w	O
if	O
we	O
really	O
knew	O
that	O
w	O
was	O
equal	O
to	O
wmp	O
then	O
these	O
predictions	O
would	O
be	O
correct	O
but	O
we	O
do	O
not	O
know	O
w	O
the	O
parameters	B
are	O
uncertain	O
intuitively	O
we	O
might	O
be	O
inclined	O
to	O
assign	O
a	O
less	O
probability	B
to	O
at	O
b	O
than	O
at	O
a	O
as	O
shown	O
in	O
since	O
point	O
b	O
is	O
far	O
from	O
the	O
training	B
data	I
the	O
parameters	B
wmp	O
often	O
give	O
predictions	O
a	O
non-bayesian	O
approach	O
to	O
this	O
problem	O
is	O
to	O
downweight	O
all	O
predictions	O
uniformly	O
by	O
an	O
empirically	O
determined	O
factor	O
this	O
is	O
not	O
ideal	O
since	O
intuition	O
suggests	O
the	O
strength	O
of	O
the	O
predictions	O
at	O
b	O
should	O
be	O
downweighted	O
more	O
than	O
those	O
at	O
a	O
a	O
bayesian	B
viewpoint	O
helps	O
us	O
to	O
understand	O
the	O
cause	O
of	O
the	O
problem	O
and	O
provides	O
a	O
straightforward	O
solution	O
in	O
a	O
nutshell	O
we	O
obtain	O
bayesian	B
predictions	O
by	O
taking	O
into	O
account	O
the	O
whole	O
posterior	O
ensemble	B
shown	O
schematically	O
in	O
the	O
bayesian	B
prediction	B
of	O
a	O
new	O
datum	O
involves	O
marginalizing	O
over	O
the	O
parameters	B
over	O
anything	O
else	O
about	O
which	O
we	O
are	O
uncertain	O
for	O
simplicity	O
let	O
us	O
assume	O
that	O
the	O
weights	O
w	O
are	O
the	O
only	O
uncertain	O
quantities	O
the	O
weight	B
decay	I
rate	B
and	O
the	O
model	B
h	O
itself	O
are	O
assumed	O
to	O
be	O
then	O
by	O
the	O
sum	B
rule	I
the	O
predictive	O
probability	B
of	O
a	O
new	O
target	O
at	O
a	O
location	O
is	O
p	O
j	O
d	O
dkw	O
p	O
j	O
w	O
j	O
d	O
where	O
k	O
is	O
the	O
dimensionality	O
of	O
w	O
three	O
in	O
the	O
toy	O
problem	O
thus	O
the	O
predictions	O
are	O
obtained	O
by	O
weighting	O
the	O
prediction	B
for	O
each	O
possible	O
w	O
p	O
w	O
w	O
p	O
w	O
w	O
with	O
a	O
weight	O
given	O
by	O
the	O
posterior	B
probability	B
of	O
w	O
p	O
j	O
d	O
which	O
we	O
most	O
recently	O
wrote	O
down	O
in	O
equation	O
this	O
posterior	B
probability	B
is	O
zm	O
p	O
j	O
d	O
zm	O
dkw	O
where	O
in	O
summary	B
we	O
can	O
get	O
the	O
bayesian	B
predictions	O
if	O
we	O
can	O
a	O
way	O
of	O
computing	O
the	O
integral	B
p	O
d	O
dkw	O
w	O
zm	O
which	O
is	O
the	O
average	O
of	O
the	O
output	O
of	O
the	O
neuron	B
at	O
under	O
the	O
posterior	O
distribution	B
of	O
w	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
learning	B
as	B
inference	B
figure	O
one	O
step	O
of	O
the	O
langevin	B
method	I
in	O
two	O
dimensions	B
contrasted	O
with	O
a	O
traditional	O
dumb	B
metropolis	B
method	I
and	O
with	O
gradient	B
descent	I
the	O
proposal	B
density	B
of	O
the	O
langevin	B
method	I
is	O
given	O
by	O
gradient	B
descent	I
with	O
noise	O
p	O
qx	O
dumb	B
metropolis	I
gradient	B
descent	I
langevin	O
implementation	O
how	O
shall	O
we	O
compute	O
the	O
integral	B
for	O
our	O
toy	O
problem	O
the	O
weight	B
space	I
is	O
three	O
dimensional	O
for	O
a	O
realistic	O
neural	B
network	B
the	O
dimensionality	O
k	O
might	O
be	O
in	O
the	O
thousands	O
bayesian	B
inference	B
for	O
general	O
data	O
modelling	B
problems	O
may	O
be	O
implemented	O
by	O
exact	O
methods	O
by	O
monte	B
carlo	I
sampling	O
or	O
by	O
deterministic	B
approximate	O
methods	O
for	O
example	O
methods	O
that	O
make	O
gaussian	B
approximations	O
to	O
p	O
j	O
d	O
using	O
laplace	B
s	O
method	O
or	O
variational	B
methods	I
for	O
neural	O
networks	O
there	O
are	O
few	O
exact	O
methods	O
the	O
two	O
main	O
approaches	O
to	O
implementing	O
bayesian	B
inference	B
for	O
neural	O
networks	O
are	O
the	O
monte	B
carlo	I
methods	I
developed	O
by	O
neal	B
and	O
the	O
gaussian	B
approximation	B
methods	O
developed	O
by	O
mackay	B
monte	B
carlo	I
implementation	O
of	O
a	O
single	B
neuron	B
first	O
we	O
will	O
use	O
a	O
monte	B
carlo	I
approach	O
in	O
which	O
the	O
task	O
of	O
evaluating	O
the	O
integral	B
is	O
solved	O
by	O
treating	O
w	O
as	O
a	O
function	O
f	O
of	O
w	O
whose	O
mean	B
we	O
compute	O
using	O
hf	O
f	O
rxr	O
zm	O
where	O
fwrg	O
are	O
samples	O
from	O
the	O
posterior	O
distribution	B
equation	O
we	O
obtain	O
the	O
samples	O
using	O
a	O
metropolis	B
method	I
as	O
an	O
aside	O
a	O
possible	O
disadvantage	O
of	O
this	O
monte	B
carlo	I
approach	O
is	O
that	O
it	O
is	O
a	O
poor	O
way	O
of	O
estimating	O
the	O
probability	B
of	O
an	O
improbable	O
event	O
i	O
e	O
a	O
p	O
dh	O
that	O
is	O
very	O
close	O
to	O
zero	O
if	O
the	O
improbable	O
event	O
is	O
most	O
likely	O
to	O
occur	O
in	O
conjunction	O
with	O
improbable	O
parameter	O
values	O
how	O
to	O
generate	O
the	O
samples	O
fwrg	O
radford	O
neal	B
introduced	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
to	O
neural	O
networks	O
we	O
met	O
this	O
sophisticated	O
metropolis	B
method	I
which	O
makes	O
use	O
of	O
gradient	O
information	B
in	O
chapter	O
the	O
method	O
we	O
now	O
demonstrate	O
is	O
a	O
simple	O
version	O
of	O
hamiltonian	B
monte	B
carlo	I
called	O
the	O
langevin	B
monte	B
carlo	I
method	O
the	O
langevin	B
monte	B
carlo	I
method	O
the	O
langevin	B
method	I
may	O
be	O
summarized	O
as	O
gradient	B
descent	I
with	O
added	O
noise	O
as	O
shown	O
pictorially	O
in	O
a	O
noise	O
vector	O
p	O
is	O
generated	O
from	O
a	O
gaussian	B
with	O
unit	O
variance	B
the	O
gradient	O
g	O
is	O
computed	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
implementation	O
of	O
a	O
single	B
neuron	B
g	O
gradm	O
w	O
m	O
findm	O
w	O
set	B
gradient	O
using	O
initial	O
w	O
set	B
objective	B
function	I
too	O
for	O
l	O
p	O
randn	O
sizew	O
h	O
p	O
p	O
m	O
loop	O
l	O
times	O
initial	O
momentum	B
is	O
evaluate	O
hwp	O
p	O
p	O
epsilon	O
g	O
wnew	O
w	O
epsilon	O
p	O
gnew	O
gradm	O
wnew	O
p	O
p	O
epsilon	O
gnew	O
make	O
half-step	O
in	O
p	O
make	O
step	O
in	O
w	O
find	O
new	O
gradient	O
make	O
half-step	O
in	O
p	O
algorithm	O
octave	B
source	O
code	O
for	O
the	O
langevin	B
monte	B
carlo	I
method	O
to	O
obtain	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
we	O
repeat	O
the	O
four	O
lines	O
marked	O
multiple	O
times	O
mnew	O
findm	O
wnew	O
hnew	O
p	O
p	O
mnew	O
dh	O
hnew	O
h	O
if	O
dh	O
accept	O
elseif	O
rand	O
exp-dh	O
accept	O
else	O
accept	O
endif	O
if	O
accept	O
g	O
gnew	O
w	O
wnew	O
find	O
new	O
objective	B
function	I
evaluate	O
new	O
value	O
of	O
h	O
decide	O
whether	O
to	O
accept	O
compare	O
with	O
a	O
uniform	O
variate	O
m	O
mnew	O
endif	O
endfor	O
function	O
gm	O
gradm	O
w	O
a	O
x	O
w	O
y	O
sigmoida	O
e	O
t	O
y	O
g	O
x	O
e	O
gm	O
alpha	O
w	O
g	O
gradient	O
of	O
objective	B
function	I
compute	O
activations	O
compute	O
outputs	O
compute	O
errors	B
compute	O
the	O
gradient	O
of	O
gw	O
endfunction	O
function	O
m	O
findm	O
w	O
objective	B
function	I
g	O
logy	O
log	O
ew	O
w	O
w	O
m	O
g	O
alpha	O
ew	O
endfunction	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gw	O
langevin	O
gw	O
optimizer	O
learning	B
as	B
inference	B
mw	O
langevin	O
mw	O
optimizer	O
figure	O
a	O
single	B
neuron	B
learning	B
under	O
the	O
langevin	B
monte	B
carlo	I
method	O
evolution	B
of	O
weights	O
and	O
as	O
a	O
function	O
of	O
number	O
of	O
iterations	O
evolution	B
of	O
weights	O
and	O
in	O
weight	B
space	I
also	O
shown	O
by	O
a	O
line	O
is	O
the	O
evolution	B
of	O
the	O
weights	O
using	O
the	O
optimizer	O
of	O
the	O
error	B
function	I
gw	O
as	O
a	O
function	O
of	O
number	O
of	O
iterations	O
also	O
shown	O
is	O
the	O
error	B
function	I
during	O
the	O
optimization	B
of	O
the	O
objective	B
function	I
m	O
as	O
a	O
function	O
of	O
number	O
of	O
iterations	O
see	O
also	O
and	O
and	O
a	O
step	O
in	O
w	O
is	O
made	O
given	O
by	O
notice	O
that	O
if	O
the	O
term	O
were	O
omitted	O
this	O
would	O
simply	O
be	O
gradient	B
descent	I
with	O
learning	B
rate	B
this	O
step	O
in	O
w	O
is	O
accepted	O
or	O
rejected	O
depending	O
on	O
the	O
change	O
in	O
the	O
value	O
of	O
the	O
objective	B
function	I
m	O
and	O
on	O
the	O
change	O
in	O
gradient	O
with	O
a	O
probability	B
of	O
acceptance	O
such	O
that	O
detailed	B
balance	B
holds	O
the	O
langevin	B
method	I
has	O
one	O
free	O
parameter	O
which	O
controls	O
the	O
typical	B
step	O
size	O
if	O
is	O
set	B
to	O
too	O
large	O
a	O
value	O
moves	O
may	O
be	O
rejected	O
if	O
it	O
is	O
set	B
to	O
a	O
very	O
small	O
value	O
progress	O
around	O
the	O
state	O
space	O
will	O
be	O
slow	O
demonstration	O
of	O
langevin	B
method	I
the	O
langevin	B
method	I
is	O
demonstrated	O
in	O
and	O
here	O
the	O
objective	B
function	I
is	O
m	O
gw	O
with	O
these	O
include	O
for	O
comparison	O
the	O
results	O
of	O
the	O
previous	O
optimization	B
method	O
using	O
gradient	B
descent	I
on	O
the	O
same	O
objective	B
function	I
it	O
can	O
be	O
seen	O
that	O
the	O
mean	B
evolution	B
of	O
w	O
is	O
similar	O
to	O
the	O
evolution	B
of	O
the	O
parameters	B
under	O
gradient	B
descent	I
the	O
monte	B
carlo	I
method	O
appears	O
to	O
have	O
converged	O
to	O
the	O
posterior	O
distribution	B
after	O
about	O
iterations	O
the	O
average	O
acceptance	B
rate	B
during	O
this	O
simulation	O
was	O
only	O
of	O
the	O
proposed	O
moves	O
were	O
rejected	O
probably	O
faster	O
progress	O
around	O
the	O
state	O
space	O
would	O
have	O
been	O
made	O
if	O
a	O
larger	O
step	O
size	O
had	O
been	O
used	O
but	O
the	O
value	O
was	O
chosen	O
so	O
that	O
the	O
descent	O
rate	B
matched	O
the	O
step	O
size	O
of	O
the	O
earlier	O
simulations	O
making	O
bayesian	B
predictions	O
from	O
iteration	O
to	O
the	O
weights	O
were	O
sampled	O
every	O
iterations	O
and	O
the	O
corresponding	O
functions	B
of	O
x	O
are	O
plotted	O
in	O
there	O
is	O
a	O
considerable	O
variety	O
of	O
plausible	O
functions	B
we	O
obtain	O
a	O
monte	B
carlo	I
approximation	B
to	O
the	O
bayesian	B
predictions	O
by	O
averaging	O
these	O
thirty	O
functions	B
of	O
x	O
together	O
the	O
result	O
is	O
shown	O
in	O
and	O
contrasted	O
with	O
the	O
predictions	O
given	O
by	O
the	O
optimized	O
parameters	B
the	O
bayesian	B
predictions	O
become	O
satisfyingly	O
moderate	O
as	O
we	O
move	O
away	O
from	O
the	O
region	O
of	O
highest	O
data	O
density	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
monte	B
carlo	I
implementation	O
of	O
a	O
single	B
neuron	B
figure	O
samples	O
obtained	O
by	O
the	O
langevin	B
monte	B
carlo	I
method	O
the	O
learning	B
rate	B
was	O
set	B
to	O
and	O
the	O
weight	B
decay	I
rate	B
to	O
the	O
step	O
size	O
is	O
given	O
by	O
the	O
function	O
performed	O
by	O
the	O
neuron	B
is	O
shown	O
by	O
three	O
of	O
its	O
contours	O
every	O
iterations	O
from	O
iteration	O
to	O
the	O
contours	O
shown	O
are	O
those	O
corresponding	O
to	O
a	O
namely	O
y	O
and	O
also	O
shown	O
is	O
a	O
vector	O
proportional	O
to	O
figure	O
bayesian	B
predictions	O
found	O
by	O
the	O
langevin	B
monte	B
carlo	I
method	O
compared	O
with	O
the	O
predictions	O
using	O
the	O
optimized	O
parameters	B
the	O
predictive	O
function	O
obtained	O
by	O
averaging	O
the	O
predictions	O
for	O
samples	O
uniformly	O
spaced	O
between	O
iterations	O
and	O
shown	O
in	O
the	O
contours	O
shown	O
are	O
those	O
corresponding	O
to	O
a	O
namely	O
y	O
and	O
for	O
contrast	O
the	O
predictions	O
given	O
by	O
the	O
most	O
probable	O
setting	O
of	O
the	O
neuron	B
s	O
parameters	B
as	O
given	O
by	O
optimization	B
of	O
m	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
learning	B
as	B
inference	B
algorithm	O
octave	B
source	O
code	O
for	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
the	O
algorithm	O
is	O
identical	O
to	O
the	O
langevin	B
method	I
in	O
algorithm	O
except	O
for	O
the	O
replacement	O
of	O
the	O
four	O
lines	O
marked	O
in	O
that	O
algorithm	O
by	O
the	O
fragment	O
shown	O
here	O
figure	O
comparison	O
of	O
sampling	O
properties	O
of	O
the	O
langevin	B
monte	B
carlo	I
method	O
and	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
the	O
horizontal	O
axis	O
is	O
the	O
number	O
of	O
gradient	O
evaluations	O
made	O
each	O
shows	O
the	O
weights	O
during	O
the	O
iterations	O
the	O
rejection	B
rate	B
during	O
this	O
hamiltonian	B
monte	B
carlo	I
simulation	O
was	O
wnew	O
w	O
gnew	O
g	O
for	O
tau	O
p	O
p	O
epsilon	O
gnew	O
wnew	O
wnew	O
epsilon	O
p	O
make	O
half-step	O
in	O
p	O
make	O
step	O
in	O
w	O
gnew	O
gradm	O
wnew	O
p	O
p	O
epsilon	O
gnew	O
find	O
new	O
gradient	O
make	O
half-step	O
in	O
p	O
endfor	O
langevin	O
hmc	O
the	O
bayesian	B
is	O
better	O
able	O
to	O
identify	O
the	O
points	O
where	O
the	O
is	O
uncertain	O
this	O
pleasing	O
behaviour	O
results	O
simply	O
from	O
a	O
mechanical	O
application	O
of	O
the	O
rules	B
of	O
probability	B
optimization	B
and	O
typicality	B
a	O
observation	O
concerns	O
the	O
behaviour	O
of	O
the	O
functions	B
gw	O
and	O
m	O
during	O
the	O
monte	B
carlo	I
sampling	O
process	O
compared	O
with	O
the	O
values	O
of	O
g	O
and	O
m	O
at	O
the	O
optimum	O
wmp	O
the	O
function	O
gw	O
around	O
the	O
value	O
of	O
gwmp	O
though	O
not	O
in	O
a	O
symmetrical	O
way	O
the	O
function	O
m	O
also	O
but	O
it	O
does	O
not	O
around	O
m	O
obviously	O
it	O
cannot	O
because	O
m	O
is	O
minimized	O
at	O
wmp	O
so	O
m	O
could	O
not	O
go	O
any	O
smaller	O
furthermore	O
m	O
only	O
rarely	O
drops	O
close	O
to	O
m	O
in	O
the	O
language	O
of	O
information	B
theory	I
the	O
typical	B
set	B
of	O
w	O
has	O
properties	O
from	O
the	O
most	O
probable	O
state	O
w	O
mp	O
a	O
general	O
message	O
therefore	O
emerges	O
applicable	O
to	O
all	O
data	O
models	O
not	O
just	O
neural	O
networks	O
one	O
should	O
be	O
cautious	O
about	O
making	O
use	O
of	O
optimized	O
parameters	B
as	O
the	O
properties	O
of	O
optimized	O
parameters	B
may	O
be	O
unrepresentative	O
of	O
the	O
properties	O
of	O
typical	B
plausible	O
parameters	B
and	O
the	O
predictions	O
obtained	O
using	O
optimized	O
parameters	B
alone	O
will	O
often	O
be	O
unreasonably	O
reducing	O
random	B
walk	I
behaviour	O
using	O
hamiltonian	B
monte	B
carlo	I
as	O
a	O
study	O
of	O
monte	B
carlo	I
methods	I
we	O
now	O
compare	O
the	O
langevin	B
monte	B
carlo	I
method	O
with	O
its	O
big	O
brother	O
the	O
hamiltonian	B
monte	B
carlo	I
method	O
the	O
change	O
to	O
hamiltonian	B
monte	B
carlo	I
is	O
simple	O
to	O
implement	O
as	O
shown	O
in	O
algorithm	O
each	O
single	O
proposal	O
makes	O
use	O
of	O
multiple	O
gradient	O
evaluations	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
implementing	O
inference	B
with	O
gaussian	B
approximations	O
along	O
a	O
dynamical	O
trajectory	O
in	O
w	O
p	O
space	O
where	O
p	O
are	O
the	O
extra	O
momentum	B
variables	O
of	O
the	O
langevin	O
and	O
hamiltonian	B
monte	B
carlo	I
methods	I
the	O
number	O
of	O
steps	O
tau	O
was	O
set	B
at	O
random	B
to	O
a	O
number	O
between	O
and	O
for	O
each	O
trajectory	O
the	O
step	O
size	O
was	O
kept	O
so	O
as	O
to	O
retain	O
comparability	O
with	O
the	O
simulations	O
that	O
have	O
gone	O
before	O
it	O
is	O
recommended	O
that	O
one	O
randomize	O
the	O
step	O
size	O
in	O
practical	B
applications	O
however	O
figure	O
compares	O
the	O
sampling	O
properties	O
of	O
the	O
langevin	O
and	O
hamiltonian	B
monte	B
carlo	I
methods	I
the	O
autocorrelation	O
of	O
the	O
state	O
of	O
the	O
hamiltonian	B
monte	B
carlo	I
simulation	O
falls	O
much	O
more	O
rapidly	O
with	O
simulation	O
time	O
than	O
that	O
of	O
the	O
langevin	B
method	I
for	O
this	O
toy	O
problem	O
hamiltonian	B
monte	B
carlo	I
is	O
at	O
least	O
ten	O
times	O
more	O
in	O
its	O
use	O
of	O
computer	B
time	O
implementing	O
inference	B
with	O
gaussian	B
approximations	O
physicists	O
love	O
to	O
take	O
nonlinearities	O
and	O
locally	O
linearize	O
them	O
and	O
they	O
love	O
to	O
approximate	O
probability	B
distributions	O
by	O
gaussians	O
such	O
approximations	O
an	O
alternative	O
strategy	O
for	O
dealing	O
with	O
the	O
integral	B
p	O
d	O
dkw	O
w	O
zm	O
which	O
we	O
just	O
evaluated	O
using	O
monte	B
carlo	I
methods	I
we	O
start	O
by	O
making	O
a	O
gaussian	B
approximation	B
to	O
the	O
posterior	B
probability	B
we	O
go	O
to	O
the	O
minimum	O
of	O
m	O
a	O
gradient-based	O
optimizer	O
and	O
taylorexpand	O
m	O
there	O
m	O
m	O
wmptaw	O
wmp	O
where	O
a	O
is	O
the	O
matrix	B
of	O
second	O
derivatives	O
also	O
known	O
as	O
the	O
hessian	B
by	O
aij	O
m	O
wmptaw	O
we	O
thus	O
our	O
gaussian	B
approximation	B
qw	O
wmp	O
a	O
we	O
can	O
think	O
of	O
the	O
matrix	B
a	O
as	O
error	B
bars	I
on	O
w	O
to	O
be	O
precise	O
q	O
is	O
a	O
normal	B
distribution	B
whose	O
variancecovariance	O
matrix	B
is	O
exercise	O
show	O
that	O
the	O
second	O
derivative	O
of	O
m	O
with	O
respect	O
to	O
w	O
is	O
given	O
by	O
m	O
n	O
i	O
xn	O
j	O
where	O
is	O
the	O
derivative	O
of	O
f	O
which	O
is	O
and	O
d	O
da	O
f	O
f	O
f	O
an	O
wjxn	O
j	O
having	O
computed	O
the	O
hessian	B
our	O
task	O
is	O
then	O
to	O
perform	O
the	O
integral	B
using	O
our	O
gaussian	B
approximation	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
learning	B
as	B
inference	B
figure	O
the	O
marginalized	O
probability	B
and	O
an	O
approximation	B
to	O
it	O
the	O
function	O
evaluated	O
numerically	O
in	O
the	O
functions	B
and	O
in	O
the	O
text	O
are	O
shown	O
as	O
a	O
function	O
of	O
a	O
for	O
from	O
mackay	B
figure	O
the	O
gaussian	B
approximation	B
in	O
weight	B
space	I
and	O
its	O
approximate	O
predictions	O
in	O
input	O
space	O
a	O
projection	O
of	O
the	O
gaussian	B
approximation	B
onto	O
the	O
plane	O
of	O
weight	B
space	I
the	O
one-	O
and	O
two-standard-deviation	O
contours	O
are	O
shown	O
also	O
shown	O
are	O
the	O
trajectory	O
of	O
the	O
optimizer	O
and	O
the	O
monte	B
carlo	I
method	O
s	O
samples	O
the	O
predictive	O
function	O
obtained	O
from	O
the	O
gaussian	B
approximation	B
and	O
equation	O
a	O
b	O
calculating	O
the	O
marginalized	O
probability	B
the	O
output	O
yx	O
w	O
depends	O
on	O
w	O
only	O
through	O
the	O
scalar	O
ax	O
w	O
so	O
we	O
can	O
reduce	O
the	O
dimensionality	O
of	O
the	O
integral	B
by	O
the	O
probability	B
density	B
of	O
a	O
we	O
are	O
assuming	O
a	O
locally	O
gaussian	B
posterior	B
probability	B
distribution	B
over	O
w	O
wmp	O
p	O
j	O
d	O
for	O
our	O
single	B
neuron	B
the	O
activation	O
ax	O
w	O
is	O
a	O
linear	B
function	O
of	O
w	O
with	O
x	O
so	O
for	O
any	O
x	O
the	O
activation	O
a	O
is	O
gaussian-distributed	O
exercise	O
assuming	O
w	O
is	O
gaussian-distributed	O
with	O
mean	B
wmp	O
and	O
variancecovariance	O
matrix	B
show	O
that	O
the	O
probability	B
distribution	B
of	O
ax	O
is	O
p	O
x	O
d	O
normalamp	O
where	O
amp	O
ax	O
wmp	O
and	O
this	O
means	O
that	O
the	O
marginalized	O
output	O
is	O
p	O
x	O
d	O
da	O
f	O
normalamp	O
this	O
is	O
to	O
be	O
contrasted	O
with	O
yx	O
wmp	O
f	O
the	O
output	O
of	O
the	O
most	O
probable	O
network	B
the	O
integral	B
of	O
a	O
sigmoid	B
times	O
a	O
gaussian	B
can	O
be	O
approximated	O
by	O
f	O
with	O
demonstration	O
figure	O
shows	O
the	O
result	O
of	O
a	O
gaussian	B
approximation	B
at	O
the	O
optimum	O
wmp	O
and	O
the	O
results	O
of	O
using	O
that	O
gaussian	B
approximation	B
and	O
equa	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
implementing	O
inference	B
with	O
gaussian	B
approximations	O
tion	O
to	O
make	O
predictions	O
comparing	O
these	O
predictions	O
with	O
those	O
of	O
the	O
langevin	B
monte	B
carlo	I
method	O
we	O
observe	O
that	O
whilst	O
qualitatively	O
the	O
same	O
the	O
two	O
are	O
clearly	O
numerically	O
so	O
at	O
least	O
one	O
of	O
the	O
two	O
methods	O
is	O
not	O
completely	O
accurate	O
exercise	O
is	O
the	O
gaussian	B
approximation	B
to	O
p	O
j	O
d	O
too	O
heavy-tailed	O
or	O
too	O
light-tailed	O
or	O
both	O
it	O
may	O
help	O
to	O
consider	O
p	O
j	O
d	O
as	O
a	O
function	O
of	O
one	O
parameter	O
wi	O
and	O
to	O
think	O
of	O
the	O
two	O
distributions	O
on	O
a	O
logarithmic	O
scale	O
discuss	O
the	O
conditions	O
under	O
which	O
the	O
gaussian	B
approximation	B
is	O
most	O
accurate	O
why	O
marginalize	O
if	O
the	O
output	O
is	O
immediately	O
used	O
to	O
make	O
a	O
decision	O
and	O
the	O
costs	O
associated	O
with	O
error	O
are	O
symmetrical	O
then	O
the	O
use	O
of	O
marginalized	O
outputs	O
under	O
this	O
gaussian	B
approximation	B
will	O
make	O
no	O
to	O
the	O
performance	O
of	O
the	O
compared	O
with	O
using	O
the	O
outputs	O
given	O
by	O
the	O
most	O
probable	O
parameters	B
since	O
both	O
functions	B
pass	O
through	O
at	O
amp	O
but	O
these	O
bayesian	B
outputs	O
will	O
make	O
a	O
if	O
for	O
example	O
there	O
is	O
an	O
option	O
of	O
saying	O
i	O
don	O
t	O
know	O
in	O
addition	O
to	O
saying	O
i	O
guess	O
and	O
i	O
guess	O
and	O
even	O
if	O
there	O
are	O
just	O
the	O
two	O
choices	O
and	O
if	O
the	O
costs	O
associated	O
with	O
error	O
are	O
unequal	O
then	O
the	O
decision	O
boundary	O
will	O
be	O
some	O
contour	O
other	O
than	O
the	O
contour	O
and	O
the	O
boundary	O
will	O
be	O
by	O
marginalization	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
postscript	O
on	O
supervised	O
neural	O
networks	O
one	O
of	O
my	O
students	O
robert	O
asked	O
maybe	O
i	O
m	O
missing	O
something	O
fundamental	O
but	O
supervised	O
neural	O
networks	O
seem	O
equivalent	O
to	O
a	O
function	O
to	O
some	O
given	O
data	O
then	O
extrapolating	O
what	O
s	O
the	O
i	O
agree	O
with	O
robert	O
the	O
supervised	O
neural	O
networks	O
we	O
have	O
studied	O
so	O
far	O
are	O
simply	O
parameterized	O
nonlinear	B
functions	B
which	O
can	O
be	O
to	O
data	O
hopefully	O
you	O
will	O
agree	O
with	O
another	O
comment	O
that	O
robert	O
made	O
unsupervised	O
networks	O
seem	O
much	O
more	O
interesting	O
than	O
their	O
supervised	O
counterparts	O
i	O
m	O
amazed	O
that	O
it	O
works	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
networks	O
we	O
have	O
now	O
spent	O
three	O
chapters	O
studying	O
the	O
single	B
neuron	B
the	O
time	O
has	O
come	O
to	O
connect	O
multiple	O
neurons	O
together	O
making	O
the	O
output	O
of	O
one	O
neuron	B
be	O
the	O
input	O
to	O
another	O
so	O
as	O
to	O
make	O
neural	O
networks	O
neural	O
networks	O
can	O
be	O
divided	O
into	O
two	O
classes	O
on	O
the	O
basis	O
of	O
their	O
con	O
nectivity	O
figure	O
a	O
feedforward	O
network	B
a	O
feedback	B
network	B
feedforward	O
networks	O
in	O
a	O
feedforward	O
network	B
all	O
the	O
connections	O
are	O
directed	O
such	O
that	O
the	O
network	B
forms	O
a	O
directed	O
acyclic	O
graph	B
feedback	B
networks	O
any	O
network	B
that	O
is	O
not	O
a	O
feedforward	O
network	B
will	O
be	O
called	O
a	O
feedback	B
network	B
in	O
this	O
chapter	O
we	O
will	O
discuss	O
a	O
fully	O
connected	O
feedback	B
network	B
called	O
the	O
network	B
the	O
weights	O
in	O
the	O
network	B
are	O
constrained	B
to	O
be	O
symmetric	B
i	O
e	O
the	O
weight	O
from	O
neuron	B
i	O
to	O
neuron	B
j	O
is	O
equal	O
to	O
the	O
weight	O
from	O
neuron	B
j	O
to	O
neuron	B
i	O
networks	O
have	O
two	O
applications	O
first	O
they	O
can	O
act	O
as	O
associative	B
memories	O
second	O
they	O
can	O
be	O
used	O
to	O
solve	O
optimization	B
problems	O
we	O
will	O
discuss	O
the	O
idea	O
of	O
associative	B
memory	B
also	O
known	O
as	O
content-addressable	B
memory	B
hebbian	B
learning	B
in	O
chapter	O
we	O
discussed	O
the	O
contrast	O
between	O
traditional	O
digital	O
memories	O
and	O
biological	O
memories	O
perhaps	O
the	O
most	O
striking	O
is	O
the	O
associative	B
nature	O
of	O
biological	O
memory	B
a	O
simple	O
model	B
due	O
to	O
donald	O
hebb	B
captures	O
the	O
idea	O
of	O
associative	B
memory	B
imagine	O
that	O
the	O
weights	O
between	O
neurons	O
whose	O
activities	O
are	O
positively	O
correlated	O
are	O
increased	O
dwij	O
dt	O
correlationxi	O
xj	O
now	O
imagine	O
that	O
when	O
stimulus	O
m	O
is	O
present	O
example	O
the	O
smell	O
of	O
a	O
banana	O
the	O
activity	O
of	O
neuron	B
m	O
increases	O
and	O
that	O
neuron	B
n	O
is	O
associated	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
networks	O
with	O
another	O
stimulus	O
n	O
example	O
the	O
sight	O
of	O
a	O
yellow	O
object	O
if	O
these	O
two	O
stimuli	O
a	O
yellow	O
sight	O
and	O
a	O
banana	O
smell	O
co-occur	O
in	O
the	O
environment	O
then	O
the	O
hebbian	B
learning	B
rule	I
will	O
increase	O
the	O
weights	O
wnm	O
and	O
wmn	O
this	O
means	O
that	O
when	O
on	O
a	O
later	O
occasion	O
stimulus	O
n	O
occurs	O
in	O
isolation	O
making	O
the	O
activity	O
xn	O
large	O
the	O
positive	O
weight	O
from	O
n	O
to	O
m	O
will	O
cause	O
neuron	B
m	O
also	O
to	O
be	O
activated	O
thus	O
the	O
response	O
to	O
the	O
sight	O
of	O
a	O
yellow	O
object	O
is	O
an	O
automatic	O
association	O
with	O
the	O
smell	O
of	O
a	O
banana	O
we	O
could	O
call	O
this	O
pattern	O
completion	O
no	O
teacher	O
is	O
required	O
for	O
this	O
associative	B
memory	B
to	O
work	O
no	O
signal	O
is	O
needed	O
to	O
indicate	O
that	O
a	O
correlation	O
has	O
been	O
detected	O
or	O
that	O
an	O
association	O
should	O
be	O
made	O
the	O
unsupervised	O
local	O
learning	B
algorithm	O
and	O
the	O
unsupervised	O
local	O
activity	B
rule	I
spontaneously	O
produce	O
associative	B
memory	B
this	O
idea	O
seems	O
so	O
simple	O
and	O
so	O
that	O
it	O
must	O
be	O
relevant	O
to	O
how	O
memories	O
work	O
in	O
the	O
brain	B
of	O
the	O
binary	O
network	B
convention	O
for	O
weights	O
our	O
convention	O
in	O
general	O
will	O
be	O
that	O
wij	O
denotes	O
the	O
connection	O
from	O
neuron	B
j	O
to	O
neuron	B
i	O
architecture	B
a	O
network	B
consists	O
of	O
i	O
neurons	O
they	O
are	O
fully	O
connected	O
through	O
symmetric	B
bidirectional	O
connections	O
with	O
weights	O
wij	O
wji	O
there	O
are	O
no	O
self-connections	O
so	O
wii	O
for	O
all	O
i	O
biases	O
may	O
be	O
included	O
may	O
be	O
viewed	O
as	O
weights	O
from	O
a	O
neuron	B
whose	O
activity	O
is	O
permanently	O
we	O
will	O
denote	O
the	O
activity	O
of	O
neuron	B
i	O
output	O
by	O
xi	O
activity	B
rule	I
roughly	O
a	O
network	B
s	O
activity	B
rule	I
is	O
for	O
each	O
neuron	B
to	O
update	O
its	O
state	O
as	O
if	O
it	O
were	O
a	O
single	B
neuron	B
with	O
the	O
threshold	B
activation	B
function	I
xa	O
a	O
a	O
since	O
there	O
is	O
feedback	B
in	O
a	O
network	B
neuron	B
s	O
output	O
is	O
an	O
input	O
to	O
all	O
the	O
other	O
neurons	O
we	O
will	O
have	O
to	O
specify	O
an	O
order	O
for	O
the	O
updates	O
to	O
occur	O
the	O
updates	O
may	O
be	O
synchronous	O
or	O
asynchronous	O
synchronous	O
updates	O
all	O
neurons	O
compute	O
their	O
activations	O
ai	O
wijxj	O
then	O
update	O
their	O
states	O
simultaneously	O
to	O
xi	O
asynchronous	O
updates	O
one	O
neuron	B
at	O
a	O
time	O
computes	O
its	O
activation	O
and	O
updates	O
its	O
state	O
the	O
sequence	B
of	O
selected	O
neurons	O
may	O
be	O
a	O
sequence	B
or	O
a	O
random	B
sequence	B
the	O
properties	O
of	O
a	O
network	B
may	O
be	O
sensitive	O
to	O
the	O
above	O
choices	O
learning	B
rule	I
the	O
learning	B
rule	I
is	O
intended	O
to	O
make	O
a	O
set	B
of	O
desired	O
memories	O
fxng	O
be	O
stable	O
states	O
of	O
the	O
network	B
s	O
activity	B
rule	I
each	O
memory	B
is	O
a	O
binary	O
pattern	O
with	O
xi	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
of	O
the	O
continuous	B
network	B
moscow------russia	O
lima----------peru	O
london-----england	O
tokyo--------japan	O
edinburgh-scotland	O
ottawa------canada	O
oslo--------norway	O
stockholm---sweden	O
paris-------france	O
moscow---	O
moscow------russia	O
ottawa------canada	O
otowa-------canada	O
ottawa------canada	O
egindurrh-sxotland	O
edinburgh-scotland	O
the	O
weights	O
are	O
set	B
using	O
the	O
sum	O
of	O
outer	O
products	O
or	O
hebb	B
rule	O
wij	O
xn	O
i	O
xn	O
j	O
where	O
is	O
an	O
unimportant	O
constant	O
to	O
prevent	O
the	O
largest	O
possible	O
weight	O
from	O
growing	O
with	O
n	O
we	O
might	O
choose	O
to	O
set	B
exercise	O
explain	O
why	O
the	O
value	O
of	O
is	O
not	O
important	O
for	O
the	O
network	B
above	O
of	O
the	O
continuous	B
network	B
figure	O
associative	B
memory	B
a	O
list	O
of	O
desired	O
memories	O
the	O
purpose	O
of	O
an	O
associative	B
memory	B
is	O
pattern	O
completion	O
given	O
a	O
partial	B
pattern	O
the	O
second	O
purpose	O
of	O
a	O
memory	B
is	O
error	O
correction	O
ai	O
wijxj	O
using	O
the	O
identical	O
architecture	B
and	B
learning	B
rule	I
we	O
can	O
a	O
network	B
whose	O
activities	O
are	O
real	O
numbers	O
between	O
and	O
activity	B
rule	I
a	O
network	B
s	O
activity	B
rule	I
is	O
for	O
each	O
neuron	B
to	O
update	O
its	O
state	O
as	O
if	O
it	O
were	O
a	O
single	B
neuron	B
with	O
a	O
sigmoid	B
activation	B
function	I
the	O
updates	O
may	O
be	O
synchronous	O
or	O
asynchronous	O
and	O
involve	O
the	O
equations	O
and	O
xi	O
tanhai	O
the	O
learning	B
rule	I
is	O
the	O
same	O
as	O
in	O
the	O
binary	O
network	B
but	O
the	O
value	O
of	O
becomes	O
relevant	O
alternatively	O
we	O
may	O
and	O
introduce	O
a	O
gain	B
into	O
the	O
activation	B
function	I
xi	O
exercise	O
where	O
have	O
we	O
encountered	O
equations	O
and	O
before	O
convergence	O
of	O
the	O
network	B
the	O
hope	O
is	O
that	O
the	O
networks	O
we	O
have	O
will	O
perform	O
associative	B
memory	B
recall	O
as	O
shown	O
schematically	O
in	O
we	O
hope	O
that	O
the	O
activity	B
rule	I
of	O
a	O
network	B
will	O
take	O
a	O
partial	B
memory	B
or	O
a	O
corrupted	O
memory	B
and	O
perform	O
pattern	O
completion	O
or	O
error	O
correction	O
to	O
restore	O
the	O
original	O
memory	B
but	O
why	O
should	O
we	O
expect	O
any	O
pattern	O
to	O
be	O
stable	O
under	O
the	O
activity	B
rule	I
let	O
alone	O
the	O
desired	O
memories	O
we	O
address	B
the	O
continuous	B
network	B
since	O
the	O
binary	O
network	B
is	O
a	O
special	O
case	O
of	O
it	O
we	O
have	O
already	O
encountered	O
the	O
activity	B
rule	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
networks	O
when	O
we	O
discussed	O
variational	B
methods	I
when	O
we	O
approximated	O
the	O
spin	B
system	I
whose	O
energy	B
function	O
was	O
with	O
a	O
separable	O
distribution	B
ex	O
j	O
hnxn	O
zq	O
qx	O
a	O
jmnxmxn	O
anxn	O
exp	O
xn	O
qx	O
aex	O
j	O
am	O
xn	O
jmn	O
hm	O
we	O
found	O
that	O
the	O
pair	O
of	O
iterative	O
equations	O
and	O
optimized	O
the	O
latter	O
so	O
as	O
to	O
minimize	O
the	O
variational	B
free	I
energy	B
qx	O
a	O
ln	O
qx	O
a	O
and	O
were	O
guaranteed	O
to	O
decrease	O
the	O
variational	B
free	I
energy	B
tanhan	O
jmn	O
hn	O
h	O
if	O
we	O
simply	O
replace	O
j	O
by	O
w	O
by	O
x	O
and	O
hn	O
by	O
we	O
see	O
that	O
the	O
equations	O
of	O
the	O
network	B
are	O
identical	O
to	O
a	O
set	B
of	O
equations	O
that	O
minimize	O
xtwx	O
h	O
there	O
is	O
a	O
general	O
name	O
for	O
a	O
function	O
that	O
decreases	O
under	O
the	O
dynamical	O
evolution	B
of	O
a	O
system	O
and	O
that	O
is	O
bounded	O
below	O
such	O
a	O
function	O
is	O
a	O
lyapunov	B
function	I
for	O
the	O
system	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
prove	O
the	O
existence	O
of	O
lyapunov	O
functions	B
if	O
a	O
system	O
has	O
a	O
lyapunov	B
function	I
then	O
its	O
dynamics	O
are	O
bound	B
to	O
settle	O
down	O
to	O
a	O
point	O
which	O
is	O
a	O
local	O
minimum	O
of	O
the	O
lyapunov	B
function	I
or	O
a	O
limit	B
cycle	I
along	O
which	O
the	O
lyapunov	B
function	I
is	O
a	O
constant	O
chaotic	O
behaviour	O
is	O
not	O
possible	O
for	O
a	O
system	O
with	O
a	O
lyapunov	B
function	I
if	O
a	O
system	O
has	O
a	O
lyapunov	B
function	I
then	O
its	O
state	O
space	O
can	O
be	O
divided	O
into	O
basins	O
of	O
attraction	O
one	O
basin	O
associated	O
with	O
each	O
attractor	O
so	O
the	O
continuous	B
network	B
s	O
activity	O
rules	B
implemented	O
asynchronously	O
have	O
a	O
lyapunov	B
function	I
this	O
lyapunov	B
function	I
is	O
a	O
convex	O
function	O
of	O
each	O
parameter	O
ai	O
so	O
a	O
network	B
s	O
dynamics	O
will	O
always	O
converge	O
to	O
a	O
stable	O
point	O
this	O
convergence	O
proof	O
depends	O
crucially	O
on	O
the	O
fact	O
that	O
the	O
network	B
s	O
connections	O
are	O
symmetric	B
it	O
also	O
depends	O
on	O
the	O
updates	O
being	O
made	O
asynchronously	O
exercise	O
show	O
by	O
constructing	O
an	O
example	O
that	O
if	O
a	O
feedback	B
network	B
does	O
not	O
have	O
symmetric	B
connections	O
then	O
its	O
dynamics	O
may	O
fail	O
to	O
converge	O
to	O
a	O
point	O
exercise	O
show	O
by	O
constructing	O
an	O
example	O
that	O
if	O
a	O
network	B
is	O
updated	O
synchronously	O
that	O
from	O
some	O
initial	O
conditions	O
it	O
may	O
fail	O
to	O
converge	O
to	O
a	O
point	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
convergence	O
of	O
the	O
network	B
figure	O
binary	O
network	B
storing	O
four	O
memories	O
the	O
four	O
memories	O
and	O
the	O
weight	O
matrix	B
initial	O
states	O
that	O
by	O
one	O
two	O
three	O
four	O
or	O
even	O
bits	O
from	O
a	O
desired	O
memory	B
are	O
restored	O
to	O
that	O
memory	B
in	O
one	O
or	O
two	O
iterations	O
some	O
initial	O
conditions	O
that	O
are	O
far	O
from	O
the	O
memories	O
lead	O
to	O
stable	O
states	O
other	O
than	O
the	O
four	O
memories	O
in	O
the	O
stable	O
state	O
looks	O
like	O
a	O
mixture	O
of	O
two	O
memories	O
d	O
and	O
j	O
stable	O
state	O
is	O
like	O
a	O
mixture	O
of	O
j	O
and	O
c	O
in	O
we	O
a	O
corrupted	O
version	O
of	O
the	O
m	O
memory	B
bits	O
distant	O
in	O
a	O
corrupted	O
version	O
of	O
j	O
bits	O
distant	O
and	O
in	O
a	O
state	O
which	O
looks	O
spurious	O
until	O
we	O
recognize	O
that	O
it	O
is	O
the	O
inverse	O
of	O
the	O
stable	O
state	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
networks	O
the	O
associative	B
memory	B
in	O
action	O
figure	O
shows	O
the	O
dynamics	O
of	O
a	O
binary	O
network	B
that	O
has	O
learnt	O
four	O
patterns	O
by	O
hebbian	B
learning	B
the	O
four	O
patterns	O
are	O
displayed	O
as	O
by	O
binary	B
images	B
in	O
for	O
twelve	O
initial	O
conditions	O
panels	O
show	O
the	O
state	O
of	O
the	O
network	B
iteration	O
by	O
iteration	O
all	O
units	B
being	O
updated	O
asynchronously	O
in	O
each	O
iteration	O
for	O
an	O
initial	O
condition	O
randomly	O
perturbed	O
from	O
a	O
memory	B
it	O
often	O
only	O
takes	O
one	O
iteration	O
for	O
all	O
the	O
errors	B
to	O
be	O
corrected	O
the	O
network	B
has	O
more	O
stable	O
states	O
in	O
addition	O
to	O
the	O
four	O
desired	O
memories	O
the	O
inverse	O
of	O
any	O
stable	O
state	O
is	O
also	O
a	O
stable	O
state	O
and	O
there	O
are	O
several	O
stable	O
states	O
that	O
can	O
be	O
interpreted	O
as	O
mixtures	O
of	O
the	O
memories	O
brain	B
damage	O
the	O
network	B
can	O
be	O
severely	O
damaged	O
and	O
still	O
work	O
as	O
an	O
associative	B
memory	B
if	O
we	O
take	O
the	O
weights	O
of	O
the	O
network	B
shown	O
in	O
and	O
randomly	O
set	B
or	O
of	O
them	O
to	O
zero	O
we	O
still	O
that	O
the	O
desired	O
memories	O
are	O
attracting	O
stable	O
states	O
imagine	O
a	O
digital	O
computer	B
that	O
still	O
works	O
even	O
when	O
of	O
its	O
components	O
are	O
destroyed	O
exercise	O
implement	O
a	O
network	B
and	O
this	O
amazing	O
robust	O
error-correcting	O
capability	O
more	O
memories	O
we	O
can	O
squash	O
more	O
memories	O
into	O
the	O
network	B
too	O
figure	O
shows	O
a	O
set	B
of	O
memories	O
when	O
we	O
train	O
the	O
network	B
with	O
hebbian	B
learning	B
all	O
memories	O
are	O
stable	O
states	O
even	O
when	O
of	O
the	O
weights	O
are	O
randomly	O
deleted	O
shown	O
by	O
the	O
x	O
s	O
in	O
the	O
weight	O
matrix	B
however	O
the	O
basins	O
of	O
attraction	O
are	O
smaller	O
than	O
before	O
show	O
the	O
dynamics	O
resulting	O
from	O
randomly	O
chosen	O
starting	O
states	O
close	O
to	O
each	O
of	O
the	O
memories	O
bits	O
only	O
three	O
of	O
the	O
memories	O
are	O
recovered	O
correctly	O
if	O
we	O
try	O
to	O
store	O
too	O
many	O
patterns	O
the	O
associative	B
memory	B
fails	O
catastrophically	O
when	O
we	O
add	O
a	O
sixth	O
pattern	O
as	O
shown	O
in	O
only	O
one	O
of	O
the	O
patterns	O
is	O
stable	O
the	O
others	O
all	O
into	O
one	O
of	O
two	O
spurious	O
stable	O
states	O
the	O
continuous-time	O
continuous	B
network	B
the	O
fact	O
that	O
the	O
network	B
s	O
properties	O
are	O
not	O
robust	O
to	O
the	O
minor	O
change	O
from	O
asynchronous	O
to	O
synchronous	O
updates	O
might	O
be	O
a	O
cause	O
for	O
concern	O
can	O
this	O
model	B
be	O
a	O
useful	O
model	B
of	O
biological	O
networks	O
it	O
turns	O
out	O
that	O
once	O
we	O
move	O
to	O
a	O
continuous-time	O
version	O
of	O
the	O
networks	O
this	O
issue	O
melts	O
away	O
we	O
assume	O
that	O
each	O
neuron	B
s	O
activity	O
xi	O
is	O
a	O
continuous	B
function	O
of	O
time	O
xit	O
and	O
that	O
the	O
activations	O
ait	O
are	O
computed	O
instantaneously	O
in	O
accordance	O
with	O
wijxjt	O
ait	O
the	O
neuron	B
s	O
response	O
to	O
its	O
activation	O
is	O
assumed	O
to	O
be	O
mediated	O
by	O
the	O
equation	O
d	O
dt	O
xit	O
f	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
continuous-time	O
continuous	B
network	B
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
x	O
figure	O
network	B
storing	O
memories	O
and	O
deletion	O
of	O
of	O
its	O
weights	O
the	O
memories	O
and	O
the	O
weights	O
of	O
the	O
network	B
with	O
deleted	O
weights	O
shown	O
by	O
x	O
initial	O
states	O
that	O
by	O
three	O
random	B
bits	O
from	O
a	O
memory	B
some	O
are	O
restored	O
but	O
some	O
converge	O
to	O
other	O
states	O
desired	O
memories	O
figure	O
an	O
overloaded	O
network	B
trained	O
on	O
six	B
memories	O
most	O
of	O
which	O
are	O
not	O
stable	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
desired	O
memories	O
moscow------russia	O
lima----------peru	O
london-----england	O
tokyo--------japan	O
edinburgh-scotland	O
ottawa------canada	O
oslo--------norway	O
stockholm---sweden	O
paris-------france	O
w	O
attracting	O
stable	O
states	O
moscow------russia	O
lima----------peru	O
londog-----englard	O
tonco--------japan	O
edinburgh-scotland	O
oslo--------norway	O
stockholm---sweden	O
paris-------france	O
wzkmhewn--xqwqwpoq	O
paris-------sweden	O
ecnarf-------sirap	O
where	O
f	O
is	O
the	O
activation	B
function	I
for	O
example	O
f	O
tanha	O
for	O
a	O
steady	O
activation	O
ai	O
the	O
activity	O
xit	O
relaxes	O
exponentially	O
to	O
f	O
with	O
time-constant	O
now	O
here	O
is	O
the	O
nice	O
result	O
as	O
long	O
as	O
the	O
weight	O
matrix	B
is	O
symmetric	B
this	O
system	O
has	O
the	O
variational	B
free	I
energy	B
as	O
its	O
lyapunov	B
function	I
exercise	O
by	O
computing	O
d	O
dt	O
prove	O
that	O
the	O
variational	B
free	I
energy	B
is	O
a	O
lyapunov	B
function	I
for	O
the	O
continuous-time	O
network	B
networks	O
figure	O
failure	O
modes	O
of	O
a	O
network	B
schematic	O
a	O
list	O
of	O
desired	O
memories	O
and	O
the	O
resulting	O
list	O
of	O
attracting	O
stable	O
states	O
notice	O
some	O
memories	O
that	O
are	O
retained	O
with	O
a	O
small	O
number	O
of	O
errors	B
desired	O
memories	O
that	O
are	O
completely	O
lost	O
is	O
no	O
attracting	O
stable	O
state	O
at	O
the	O
desired	O
memory	B
or	O
near	O
it	O
spurious	O
stable	O
states	O
unrelated	O
to	O
the	O
original	O
list	O
spurious	O
stable	O
states	O
that	O
are	O
confabulations	O
of	O
desired	O
memories	O
it	O
is	O
particularly	O
easy	O
to	O
prove	O
that	O
a	O
function	O
l	O
is	O
a	O
lyapunov	B
function	I
if	O
the	O
system	O
s	O
dynamics	O
perform	O
steepest	O
descent	O
on	O
l	O
with	O
d	O
l	O
in	O
the	O
case	O
of	O
the	O
continuous-time	O
continuous	B
network	B
it	O
is	O
not	O
quite	O
so	O
simple	O
but	O
every	O
component	O
of	O
d	O
which	O
means	O
that	O
with	O
an	O
appropriately	O
metric	B
the	O
network	B
dynamics	O
do	O
perform	O
steepest	B
descents	I
on	O
dt	O
xit	O
dt	O
xit	O
does	O
have	O
the	O
same	O
sign	O
as	O
the	O
capacity	B
of	O
the	O
network	B
one	O
way	O
in	O
which	O
we	O
viewed	O
learning	B
in	O
the	O
single	B
neuron	B
was	O
as	B
communication	B
communication	B
of	O
the	O
labels	O
of	O
the	O
training	B
data	B
set	B
from	O
one	O
point	O
in	O
time	O
to	O
a	O
later	O
point	O
in	O
time	O
we	O
found	O
that	O
the	O
capacity	B
of	O
a	O
linear	B
threshold	B
neuron	B
was	O
bits	O
per	O
weight	O
similarly	O
we	O
might	O
view	O
the	O
associative	B
memory	B
as	O
a	O
communication	B
channel	O
a	O
list	O
of	O
desired	O
memories	O
is	O
encoded	O
into	O
a	O
set	B
of	O
weights	O
w	O
using	O
the	O
hebb	B
rule	O
of	O
equation	O
or	O
perhaps	O
some	O
other	O
learning	B
rule	I
the	O
receiver	O
receiving	O
the	O
weights	O
w	O
only	O
the	O
stable	O
states	O
of	O
the	O
network	B
which	O
he	O
interprets	O
as	O
the	O
original	O
memories	O
this	O
communication	B
system	O
can	O
fail	O
in	O
various	O
ways	O
as	O
illustrated	O
in	O
the	O
individual	O
bits	O
in	O
some	O
memories	O
might	O
be	O
corrupted	O
that	O
is	O
a	O
stable	O
state	O
of	O
the	O
network	B
is	O
displaced	O
a	O
little	O
from	O
the	O
desired	O
memory	B
entire	O
memories	O
might	O
be	O
absent	O
from	O
the	O
list	O
of	O
attractors	O
of	O
the	O
network	B
or	O
a	O
stable	O
state	O
might	O
be	O
present	O
but	O
have	O
such	O
a	O
small	O
basin	O
of	O
attraction	O
that	O
it	O
is	O
of	O
no	O
use	O
for	O
pattern	O
completion	O
and	O
error	O
correction	O
spurious	O
additional	O
memories	O
unrelated	O
to	O
the	O
desired	O
memories	O
might	O
be	O
present	O
spurious	O
additional	O
memories	O
derived	O
from	O
the	O
desired	O
memories	O
by	O
op	O
erations	O
such	O
as	O
mixing	O
and	O
inversion	O
may	O
also	O
be	O
present	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
capacity	B
of	O
the	O
network	B
of	O
these	O
failure	O
modes	O
modes	O
and	O
are	O
clearly	O
undesirable	O
mode	O
especially	O
so	O
mode	O
might	O
not	O
matter	O
so	O
much	O
as	O
long	O
as	O
each	O
of	O
the	O
desired	O
memories	O
has	O
a	O
large	O
basin	O
of	O
attraction	O
the	O
fourth	O
failure	O
mode	O
might	O
in	O
some	O
contexts	O
actually	O
be	O
viewed	O
as	O
for	O
example	O
if	O
a	O
network	B
is	O
required	O
to	O
memorize	O
examples	O
of	O
valid	O
sentences	O
such	O
as	O
john	O
loves	O
mary	O
and	O
john	O
gets	O
cake	O
we	O
might	O
be	O
happy	O
to	O
that	O
john	O
loves	O
cake	O
was	O
also	O
a	O
stable	O
state	O
of	O
the	O
network	B
we	O
might	O
call	O
this	O
behaviour	O
generalization	B
the	O
capacity	B
of	O
a	O
network	B
with	O
i	O
neurons	O
might	O
be	O
to	O
be	O
the	O
number	O
of	O
random	B
patterns	O
n	O
that	O
can	O
be	O
stored	O
without	O
failure-mode	O
having	O
substantial	O
probability	B
if	O
we	O
also	O
require	O
failure-mode	O
to	O
have	O
tiny	O
probability	B
then	O
the	O
resulting	O
capacity	B
is	O
much	O
smaller	O
we	O
now	O
study	O
these	O
alternative	O
of	O
the	O
capacity	B
the	O
capacity	B
of	O
the	O
network	B
stringent	O
we	O
will	O
explore	B
the	O
information	B
storage	O
capabilities	O
of	O
a	O
binary	O
network	B
that	O
learns	O
using	O
the	O
hebb	B
rule	O
by	O
considering	O
the	O
stability	O
of	O
just	O
one	O
bit	B
of	O
one	O
of	O
the	O
desired	O
patterns	O
assuming	O
that	O
the	O
state	O
of	O
the	O
network	B
is	O
set	B
to	O
that	O
desired	O
pattern	O
xn	O
we	O
will	O
assume	O
that	O
the	O
patterns	O
to	O
be	O
stored	O
are	O
randomly	O
selected	O
binary	O
patterns	O
the	O
activation	O
of	O
a	O
particular	O
neuron	B
i	O
is	O
wijxn	O
j	O
ai	O
where	O
the	O
weights	O
are	O
for	O
i	O
j	O
wij	O
xn	O
i	O
xn	O
j	O
xm	O
i	O
xm	O
j	O
here	O
we	O
have	O
split	O
w	O
into	O
two	O
terms	O
the	O
of	O
which	O
will	O
contribute	O
signal	O
reinforcing	O
the	O
desired	O
memory	B
and	O
the	O
second	O
noise	O
substituting	O
for	O
wij	O
the	O
activation	O
is	O
ai	O
xn	O
i	O
xn	O
j	O
xn	O
j	O
xm	O
i	O
xm	O
j	O
xn	O
j	O
xm	O
i	O
xm	O
j	O
xn	O
j	O
i	O
the	O
term	O
is	O
times	O
the	O
desired	O
state	O
xn	O
second	O
term	O
is	O
a	O
sum	O
of	O
random	B
quantities	O
xm	O
if	O
this	O
were	O
the	O
only	O
term	O
it	O
would	O
keep	O
the	O
neuron	B
clamped	O
in	O
the	O
desired	O
state	O
the	O
a	O
moment	O
s	O
that	O
these	O
quantities	O
are	O
independent	O
random	B
binary	O
variables	O
with	O
mean	B
and	O
variance	B
i	O
xm	O
j	O
xn	O
j	O
i	O
thus	O
considering	O
the	O
statistics	O
of	O
ai	O
under	O
the	O
ensemble	B
of	O
random	B
patterns	O
we	O
conclude	O
that	O
ai	O
has	O
mean	B
and	O
variance	B
for	O
brevity	O
we	O
will	O
now	O
assume	O
i	O
and	O
n	O
are	O
large	O
enough	O
that	O
we	O
can	O
neglect	O
the	O
distinction	O
between	O
i	O
and	O
i	O
and	O
between	O
n	O
and	O
n	O
then	O
we	O
can	O
restate	O
our	O
conclusion	O
ai	O
is	O
gaussian-distributed	O
with	O
mean	B
ixn	O
and	O
variance	B
in	O
i	O
i	O
what	O
then	O
is	O
the	O
probability	B
that	O
the	O
selected	O
bit	B
is	O
stable	O
if	O
we	O
put	O
the	O
network	B
into	O
the	O
state	O
xn	O
the	O
probability	B
that	O
bit	B
i	O
will	O
on	O
the	O
iteration	O
of	O
the	O
network	B
s	O
dynamics	O
is	O
p	O
unstable	O
i	O
pni	O
pin	O
i	O
ai	O
figure	O
the	O
probability	B
density	B
of	O
the	O
activation	O
ai	O
in	O
the	O
case	O
xn	O
i	O
the	O
probability	B
that	O
bit	B
i	O
becomes	O
is	O
the	O
area	O
of	O
the	O
tail	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
networks	O
figure	O
overlap	O
between	O
a	O
desired	O
memory	B
and	O
the	O
stable	O
state	O
nearest	O
to	O
it	O
as	O
a	O
function	O
of	O
the	O
loading	O
fraction	O
ni	O
the	O
overlap	O
is	O
to	O
be	O
the	O
scaled	O
i	O
which	O
is	O
when	O
recall	O
is	O
perfect	B
and	O
zero	O
when	O
the	O
stable	O
state	O
has	O
of	O
the	O
bits	O
there	O
is	O
an	O
abrupt	O
transition	B
at	O
ni	O
where	O
the	O
overlap	O
drops	O
from	O
to	O
zero	O
inner	O
productpi	O
xixn	O
where	O
z	O
dz	O
the	O
important	O
quantity	O
ni	O
is	O
the	O
ratio	O
of	O
the	O
number	O
of	O
patterns	O
stored	O
to	O
the	O
number	O
of	O
neurons	O
if	O
for	O
example	O
we	O
try	O
to	O
store	O
n	O
patterns	O
in	O
the	O
network	B
then	O
there	O
is	O
a	O
chance	O
of	O
that	O
a	O
bit	B
in	O
a	O
pattern	O
will	O
be	O
unstable	O
on	O
the	O
iteration	O
we	O
are	O
now	O
in	O
a	O
position	O
to	O
derive	O
our	O
capacity	B
result	O
for	O
the	O
case	O
where	O
no	O
corruption	O
of	O
the	O
desired	O
memories	O
is	O
permitted	O
exercise	O
assume	O
that	O
we	O
wish	O
all	O
the	O
desired	O
patterns	O
to	O
be	O
completely	O
stable	O
we	O
don	O
t	O
want	O
any	O
of	O
the	O
bits	O
to	O
when	O
the	O
network	B
is	O
put	O
into	O
any	O
desired	O
pattern	O
state	O
and	O
the	O
total	O
probability	B
of	O
any	O
error	O
at	O
all	O
is	O
required	O
to	O
be	O
less	O
than	O
a	O
small	O
number	O
using	O
the	O
approximation	B
to	O
the	O
error	B
function	I
for	O
large	O
z	O
z	O
show	O
that	O
the	O
maximum	O
number	O
of	O
patterns	O
that	O
can	O
be	O
stored	O
nmax	O
is	O
nmax	O
ln	O
i	O
i	O
if	O
however	O
we	O
allow	O
a	O
small	O
amount	O
of	O
corruption	O
of	O
memories	O
to	O
occur	O
the	O
number	O
of	O
patterns	O
that	O
can	O
be	O
stored	O
increases	O
the	O
statistical	B
physicists	O
capacity	B
the	O
analysis	O
that	O
led	O
to	O
equation	O
tells	O
us	O
that	O
if	O
we	O
try	O
to	O
store	O
n	O
patterns	O
in	O
the	O
network	B
then	O
starting	O
from	O
a	O
desired	O
memory	B
about	O
of	O
the	O
bits	O
will	O
be	O
unstable	O
on	O
the	O
iteration	O
our	O
analysis	O
does	O
not	O
shed	O
light	O
on	O
what	O
is	O
expected	O
to	O
happen	O
on	O
subsequent	O
iterations	O
the	O
of	O
these	O
bits	O
might	O
make	O
some	O
of	O
the	O
other	O
bits	O
unstable	O
too	O
causing	O
an	O
increasing	O
number	O
of	O
bits	O
to	O
be	O
this	O
process	O
might	O
lead	O
to	O
an	O
avalanche	O
in	O
which	O
the	O
network	B
s	O
state	O
ends	O
up	O
a	O
long	O
way	O
from	O
the	O
desired	O
memory	B
in	O
fact	O
when	O
ni	O
is	O
large	O
such	O
avalanches	O
do	O
happen	O
when	O
ni	O
is	O
small	O
they	O
tend	O
not	O
to	O
there	O
is	O
a	O
stable	O
state	O
near	O
to	O
each	O
desired	O
memory	B
for	O
the	O
limit	O
of	O
large	O
i	O
amit	O
et	O
al	O
have	O
used	O
methods	O
from	O
statistical	B
physics	B
to	O
numerically	O
the	O
transition	B
between	O
these	O
two	O
behaviours	O
there	O
is	O
a	O
sharp	O
discontinuity	O
at	O
ncrit	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
improving	O
on	O
the	O
capacity	B
of	O
the	O
hebb	B
rule	O
below	O
this	O
critical	O
value	O
there	O
is	O
likely	O
to	O
be	O
a	O
stable	O
state	O
near	O
every	O
desired	O
memory	B
in	O
which	O
a	O
small	O
fraction	O
of	O
the	O
bits	O
are	O
when	O
ni	O
exceeds	O
the	O
system	O
has	O
only	O
spurious	O
stable	O
states	O
known	O
as	O
spin	O
glass	O
states	O
none	O
of	O
which	O
is	O
correlated	O
with	O
any	O
of	O
the	O
desired	O
memories	O
just	O
below	O
the	O
critical	O
value	O
the	O
fraction	O
of	O
bits	O
that	O
are	O
when	O
a	O
desired	O
memory	B
has	O
evolved	O
to	O
its	O
associated	O
stable	O
state	O
is	O
figure	O
shows	O
the	O
overlap	O
between	O
the	O
desired	O
memory	B
and	O
the	O
nearest	O
stable	O
state	O
as	O
a	O
function	O
of	O
ni	O
some	O
other	O
transitions	O
in	O
properties	O
of	O
the	O
model	B
occur	O
at	O
some	O
additional	O
values	O
of	O
ni	O
as	O
summarized	O
below	O
for	O
all	O
ni	O
stable	O
spin	O
glass	O
states	O
exist	O
uncorrelated	O
with	O
the	O
desired	O
memories	O
for	O
ni	O
these	O
spin	O
glass	O
states	O
are	O
the	O
only	O
stable	O
states	O
for	O
ni	O
there	O
are	O
stable	O
states	O
close	O
to	O
the	O
desired	O
memories	O
for	O
ni	O
the	O
stable	O
states	O
associated	O
with	O
the	O
desired	O
memories	O
have	O
lower	O
energy	B
than	O
the	O
spurious	O
spin	O
glass	O
states	O
for	O
ni	O
the	O
spin	O
glass	O
states	O
dominate	O
there	O
are	O
spin	O
glass	O
states	O
that	O
have	O
lower	O
energy	B
than	O
the	O
stable	O
states	O
associated	O
with	O
the	O
desired	O
memories	O
for	O
ni	O
there	O
are	O
additional	O
mixture	O
states	O
which	O
are	O
combinations	O
of	O
several	O
desired	O
memories	O
these	O
stable	O
states	O
do	O
not	O
have	O
as	O
low	O
energy	B
as	O
the	O
stable	O
states	O
associated	O
with	O
the	O
desired	O
memories	O
in	O
conclusion	O
the	O
capacity	B
of	O
the	O
network	B
with	O
i	O
neurons	O
if	O
we	O
the	O
capacity	B
in	O
terms	O
of	O
the	O
abrupt	O
discontinuity	O
discussed	O
above	O
is	O
random	B
binary	O
patterns	O
each	O
of	O
length	O
i	O
each	O
of	O
which	O
is	O
received	O
with	O
of	O
its	O
bits	O
in	O
bits	O
this	O
capacity	B
is	O
i	O
bits	O
since	O
there	O
are	O
i	O
weights	O
in	O
the	O
network	B
we	O
can	O
also	O
express	O
the	O
capacity	B
as	O
bits	O
per	O
weight	O
improving	O
on	O
the	O
capacity	B
of	O
the	O
hebb	B
rule	O
the	O
capacities	O
discussed	O
in	O
the	O
previous	O
section	B
are	O
the	O
capacities	O
of	O
the	O
network	B
whose	O
weights	O
are	O
set	B
using	O
the	O
hebbian	B
learning	B
rule	I
we	O
can	O
do	O
better	O
than	O
the	O
hebb	B
rule	O
by	O
an	O
objective	B
function	I
that	O
measures	O
how	O
well	O
the	O
network	B
stores	O
all	O
the	O
memories	O
and	O
minimizing	O
it	O
for	O
an	O
associative	B
memory	B
to	O
be	O
useful	O
it	O
must	O
be	O
able	O
to	O
correct	O
at	O
least	O
one	O
bit	B
let	O
s	O
make	O
an	O
objective	B
function	I
that	O
measures	O
whether	O
bits	O
tend	O
to	O
be	O
restored	O
correctly	O
our	O
intention	O
is	O
that	O
for	O
every	O
neuron	B
i	O
in	O
the	O
network	B
the	O
weights	O
to	O
that	O
neuron	B
should	O
satisfy	O
this	O
rule	O
this	O
expression	O
for	O
the	O
capacity	B
omits	O
a	O
smaller	O
negative	O
term	O
of	O
order	O
n	O
n	O
bits	O
associated	O
with	O
the	O
arbitrary	O
order	O
of	O
the	O
memories	O
for	O
every	O
pattern	O
xn	O
if	O
the	O
neurons	O
other	O
than	O
i	O
are	O
set	B
correctly	O
to	O
xj	O
xn	O
then	O
the	O
activation	O
of	O
neuron	B
i	O
should	O
be	O
such	O
that	O
its	O
preferred	O
output	O
is	O
xi	O
xn	O
j	O
i	O
is	O
this	O
rule	O
a	O
familiar	O
idea	O
yes	O
it	O
is	O
precisely	O
what	O
we	O
wanted	O
the	O
single	B
neuron	B
of	O
chapter	O
to	O
do	O
each	O
pattern	O
xn	O
an	O
input	O
target	O
pair	O
for	O
the	O
single	B
neuron	B
i	O
and	O
it	O
an	O
input	O
target	O
pair	O
for	O
all	O
the	O
other	O
neurons	O
too	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
networks	O
algorithm	O
octave	B
source	O
code	O
for	O
optimizing	O
the	O
weights	O
of	O
a	O
network	B
so	O
that	O
it	O
works	O
as	O
an	O
associative	B
memory	B
cf	O
algorithm	O
the	O
data	O
matrix	B
x	O
has	O
i	O
columns	O
and	O
n	O
rows	O
the	O
matrix	B
t	O
is	O
identical	O
to	O
x	O
except	O
that	O
are	O
replaced	O
by	O
w	O
x	O
x	O
initialize	O
the	O
weights	O
using	O
hebb	B
rule	O
for	O
l	O
loop	O
l	O
times	O
for	O
wii	O
end	O
ensure	O
the	O
self-weights	O
are	O
zero	O
a	O
y	O
e	O
gw	O
x	O
e	O
gw	O
gw	O
gw	O
x	O
w	O
sigmoida	O
t	O
y	O
compute	O
all	O
activations	O
compute	O
all	O
outputs	O
compute	O
all	O
errors	B
compute	O
the	O
gradients	O
symmetrize	O
gradients	O
w	O
w	O
eta	O
gw	O
alpha	O
w	O
make	O
step	O
endfor	O
so	O
just	O
as	O
we	O
an	O
objective	B
function	I
for	O
the	O
training	O
of	O
a	O
single	B
neuron	B
as	O
a	O
we	O
can	O
gw	O
xn	O
tn	O
i	O
ln	O
yn	O
i	O
tn	O
i	O
yn	O
i	O
where	O
and	O
tn	O
i	O
xn	O
i	O
xn	O
i	O
yn	O
i	O
i	O
where	O
an	O
i	O
wijxn	O
j	O
we	O
can	O
then	O
steal	O
the	O
algorithm	O
which	O
we	O
wrote	O
for	O
the	O
single	B
neuron	B
to	O
write	O
an	O
algorithm	O
for	O
optimizing	O
a	O
network	B
algorithm	O
the	O
convenient	O
syntax	O
of	O
octave	B
requires	O
very	O
few	O
changes	O
the	O
extra	O
lines	O
enforce	O
the	O
constraints	O
that	O
the	O
self-weights	O
wii	O
should	O
all	O
be	O
zero	O
and	O
that	O
the	O
weight	O
matrix	B
should	O
be	O
symmetrical	O
wji	O
as	O
expected	O
this	O
learning	B
algorithm	O
does	O
a	O
better	O
job	O
than	O
the	O
one-shot	O
hebbian	B
learning	B
rule	I
when	O
the	O
six	B
patterns	O
of	O
which	O
cannot	O
be	O
memorized	O
by	O
the	O
hebb	B
rule	O
are	O
learned	O
using	O
algorithm	O
all	O
six	B
patterns	O
become	O
stable	O
states	O
exercise	O
implement	O
this	O
learning	B
rule	I
and	O
investigate	O
empirically	O
its	O
capacity	B
for	O
memorizing	O
random	B
patterns	O
also	O
compare	O
its	O
avalanche	O
properties	O
with	O
those	O
of	O
the	O
hebb	B
rule	O
networks	O
for	O
optimization	B
problems	O
since	O
a	O
network	B
s	O
dynamics	O
minimize	O
an	O
energy	B
function	O
it	O
is	O
natural	B
to	O
ask	O
whether	O
we	O
can	O
map	O
interesting	O
optimization	B
problems	O
onto	O
networks	O
biological	O
data	O
processing	O
problems	O
often	O
involve	O
an	O
element	O
of	O
constraint	B
satisfaction	I
in	O
scene	O
interpretation	O
for	O
example	O
one	O
might	O
wish	O
to	O
infer	O
the	O
spatial	O
location	O
orientation	O
brightness	O
and	O
texture	O
of	O
each	O
visible	O
element	O
and	O
which	O
visible	O
elements	O
are	O
connected	O
together	O
in	O
objects	O
these	O
inferences	O
are	O
constrained	B
by	O
the	O
given	O
data	O
and	O
by	O
prior	B
knowledge	O
about	O
continuity	O
of	O
objects	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
networks	O
for	O
optimization	B
problems	O
place	O
in	O
tour	O
place	O
in	O
tour	O
city	O
a	O
b	O
c	O
d	O
a	O
c	O
city	O
a	O
b	O
c	O
d	O
a	O
c	O
b	O
d	O
b	O
d	O
figure	O
network	B
for	O
solving	O
a	O
travelling	B
salesman	I
problem	I
with	O
k	O
cities	O
two	O
solution	O
states	O
of	O
the	O
network	B
with	O
activites	O
represented	O
by	O
black	B
white	B
and	O
the	O
tours	O
corresponding	O
to	O
these	O
network	B
states	O
the	O
negative	O
weights	O
between	O
node	O
and	O
other	O
nodes	O
these	O
weights	O
enforce	O
validity	O
of	O
a	O
tour	O
the	O
negative	O
weights	O
that	O
embody	O
the	O
distance	B
objective	B
function	I
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
and	O
tank	B
suggested	O
that	O
one	O
might	O
take	O
an	O
interesting	O
constraint	B
satisfaction	I
problem	O
and	O
design	O
the	O
weights	O
of	O
a	O
binary	O
or	O
continuous	B
network	B
such	O
that	O
the	O
settling	O
process	O
of	O
the	O
network	B
would	O
minimize	O
the	O
objective	B
function	I
of	O
the	O
problem	O
the	O
travelling	B
salesman	I
problem	I
a	O
classic	O
constraint	B
satisfaction	I
problem	O
to	O
which	O
networks	O
have	O
been	O
applied	O
is	O
the	O
travelling	B
salesman	I
problem	I
a	O
set	B
of	O
k	O
cities	O
is	O
given	O
and	O
a	O
matrix	B
of	O
the	O
distances	O
between	O
those	O
cities	O
the	O
task	O
is	O
to	O
a	O
closed	O
tour	O
of	O
the	O
cities	O
visiting	O
each	O
city	O
once	O
that	O
has	O
the	O
smallest	O
total	O
distance	B
the	O
travelling	B
salesman	I
problem	I
is	O
equivalent	O
in	O
to	O
an	O
np-complete	B
problem	O
the	O
method	O
suggested	O
by	O
and	O
tank	B
is	O
to	O
represent	O
a	O
tentative	O
solution	O
to	O
the	O
problem	O
by	O
the	O
state	O
of	O
a	O
network	B
with	O
i	O
k	O
neurons	O
arranged	O
in	O
a	O
square	B
with	O
each	O
neuron	B
representing	O
the	O
hypothesis	O
that	O
a	O
particular	O
city	O
comes	O
at	O
a	O
particular	O
point	O
in	O
the	O
tour	O
it	O
will	O
be	O
convenient	O
to	O
consider	O
the	O
states	O
of	O
the	O
neurons	O
as	O
being	O
between	O
and	O
rather	O
than	O
and	O
two	O
solution	O
states	O
for	O
a	O
four-city	O
travelling	B
salesman	I
problem	I
are	O
shown	O
in	O
the	O
weights	O
in	O
the	O
network	B
play	O
two	O
roles	O
first	O
they	O
must	O
an	O
energy	B
function	O
which	O
is	O
minimized	O
only	O
when	O
the	O
state	O
of	O
the	O
network	B
represents	O
a	O
valid	O
tour	O
a	O
valid	O
state	O
is	O
one	O
that	O
looks	O
like	O
a	O
permutation	B
matrix	B
having	O
exactly	O
one	O
in	O
every	O
row	O
and	O
one	O
in	O
every	O
column	O
this	O
rule	O
can	O
be	O
enforced	O
by	O
putting	O
large	O
negative	O
weights	O
between	O
any	O
pair	O
of	O
neurons	O
that	O
are	O
in	O
the	O
same	O
row	O
or	O
the	O
same	O
column	O
and	O
setting	O
a	O
positive	O
bias	B
for	O
all	O
neurons	O
to	O
ensure	O
that	O
k	O
neurons	O
do	O
turn	O
on	O
figure	O
shows	O
the	O
negative	O
weights	O
that	O
are	O
connected	O
to	O
one	O
neuron	B
which	O
represents	O
the	O
statement	O
city	O
b	O
comes	O
second	O
in	O
the	O
tour	O
second	O
the	O
weights	O
must	O
encode	O
the	O
objective	B
function	I
that	O
we	O
want	O
to	O
minimize	O
the	O
total	O
distance	B
this	O
can	O
be	O
done	O
by	O
putting	O
negative	O
weights	O
proportional	O
to	O
the	O
appropriate	O
distances	O
between	O
the	O
nodes	O
in	O
adjacent	O
columns	O
for	O
example	O
between	O
the	O
b	O
and	O
d	O
nodes	O
in	O
adjacent	O
columns	O
the	O
weight	O
would	O
be	O
the	O
negative	O
weights	O
that	O
are	O
connected	O
to	O
neuron	B
are	O
shown	O
in	O
the	O
result	O
is	O
that	O
when	O
the	O
network	B
is	O
in	O
a	O
valid	O
state	O
its	O
total	O
energy	B
will	O
be	O
the	O
total	O
distance	B
of	O
the	O
corresponding	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
networks	O
figure	O
evolution	B
of	O
the	O
state	O
of	O
a	O
continuous	B
network	B
solving	O
a	O
travelling	B
salesman	I
problem	I
using	O
aiyer	B
s	O
graduated	B
non-convexity	I
method	O
the	O
state	O
of	O
the	O
network	B
is	O
projected	O
into	O
the	O
two-dimensional	B
space	O
in	O
which	O
the	O
cities	O
are	O
located	O
by	O
the	O
centre	O
of	O
mass	O
for	O
each	O
point	O
in	O
the	O
tour	O
using	O
the	O
neuron	B
activities	O
as	O
the	O
mass	O
function	O
the	O
travelling	O
scholar	O
problem	O
the	O
shortest	O
tour	O
linking	O
the	O
cambridge	O
colleges	O
the	O
engineering	O
department	O
the	O
university	O
library	O
and	O
sree	O
aiyer	B
s	O
house	O
from	O
aiyer	B
tour	O
plus	O
a	O
constant	O
given	O
by	O
the	O
energy	B
associated	O
with	O
the	O
biases	O
now	O
since	O
a	O
network	B
minimizes	O
its	O
energy	B
it	O
is	O
hoped	O
that	O
the	O
binary	O
or	O
continuous	B
network	B
s	O
dynamics	O
will	O
take	O
the	O
state	O
to	O
a	O
minimum	O
that	O
is	O
a	O
valid	O
tour	O
and	O
which	O
might	O
be	O
an	O
optimal	B
tour	O
this	O
hope	O
is	O
not	O
for	O
large	O
travelling	O
salesman	O
problems	O
however	O
without	O
some	O
careful	O
we	O
have	O
not	O
the	O
size	O
of	O
the	O
weights	O
that	O
enforce	O
the	O
tour	O
s	O
validity	O
relative	B
to	O
the	O
size	O
of	O
the	O
distance	B
weights	O
and	O
setting	O
this	O
scale	O
factor	O
poses	O
if	O
large	O
validity-enforcing	O
weights	O
are	O
used	O
the	O
network	B
s	O
dynamics	O
will	O
rattle	O
into	O
a	O
valid	O
state	O
with	O
little	O
regard	O
for	O
the	O
distances	O
if	O
small	O
validity-enforcing	O
weights	O
are	O
used	O
it	O
is	O
possible	O
that	O
the	O
distance	B
weights	O
will	O
cause	O
the	O
network	B
to	O
adopt	O
an	O
invalid	O
state	O
that	O
has	O
lower	O
energy	B
than	O
any	O
valid	O
state	O
our	O
original	O
formulation	O
of	O
the	O
energy	B
function	O
puts	O
the	O
objective	B
function	I
and	O
the	O
solution	O
s	O
validity	O
in	O
potential	O
with	O
each	O
other	O
this	O
has	O
been	O
resolved	O
by	O
the	O
work	O
of	O
sree	O
aiyer	B
who	O
showed	O
how	O
to	O
modify	O
the	O
distance	B
weights	O
so	O
that	O
they	O
would	O
not	O
interfere	O
with	O
the	O
solution	O
s	O
validity	O
and	O
how	O
to	O
a	O
continuous	B
network	B
whose	O
dynamics	O
are	O
at	O
all	O
times	O
to	O
a	O
valid	O
subspace	O
aiyer	B
used	O
a	O
graduated	B
non-convexity	I
or	O
deterministic	B
annealing	B
approach	O
to	O
good	B
solutions	O
using	O
these	O
networks	O
the	O
deterministic	B
annealing	B
approach	O
involves	O
gradually	O
increasing	O
the	O
gain	B
of	O
the	O
neurons	O
in	O
the	O
network	B
from	O
to	O
at	O
which	O
point	O
the	O
state	O
of	O
the	O
network	B
corresponds	O
to	O
a	O
valid	O
tour	O
a	O
sequence	B
of	O
trajectories	O
generated	O
by	O
applying	O
this	O
method	O
to	O
a	O
thirtycity	O
travelling	B
salesman	I
problem	I
is	O
shown	O
in	O
a	O
solution	O
to	O
the	O
travelling	O
scholar	O
problem	O
found	O
by	O
aiyer	B
using	O
a	O
con	O
tinuous	O
network	B
is	O
shown	O
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
further	O
exercises	O
exercise	O
storing	O
two	O
memories	O
two	O
binary	O
memories	O
m	O
and	O
n	O
ni	O
are	O
stored	O
by	O
hebbian	B
learning	B
in	O
a	O
network	B
using	O
wij	O
mimj	O
ninj	O
for	O
i	O
j	O
for	O
i	O
j	O
the	O
biases	O
bi	O
are	O
set	B
to	O
zero	O
the	O
network	B
is	O
put	O
in	O
the	O
state	O
x	O
m	O
evaluate	O
the	O
activation	O
ai	O
of	O
neuron	B
i	O
and	O
show	O
that	O
in	O
can	O
be	O
written	O
in	O
the	O
form	O
ai	O
by	O
comparing	O
the	O
signal	O
strength	O
with	O
the	O
magnitude	O
of	O
the	O
noise	O
strength	O
show	O
that	O
x	O
m	O
is	O
a	O
stable	O
state	O
of	O
the	O
dynamics	O
of	O
the	O
network	B
the	O
network	B
is	O
put	O
in	O
a	O
state	O
x	O
in	O
d	O
places	O
from	O
m	O
x	O
m	O
where	O
the	O
perturbation	O
d	O
di	O
d	O
is	O
the	O
number	O
of	O
components	O
of	O
d	O
that	O
are	O
non-zero	O
and	O
for	O
each	O
di	O
that	O
is	O
non-zero	O
di	O
the	O
overlap	O
between	O
m	O
and	O
n	O
to	O
be	O
omn	O
mini	O
i	O
evaluate	O
the	O
activation	O
ai	O
of	O
neuron	B
i	O
again	O
and	O
show	O
that	O
the	O
dynamics	O
of	O
the	O
network	B
will	O
restore	O
x	O
to	O
m	O
if	O
the	O
number	O
of	O
bits	O
d	O
jomnj	O
how	O
does	O
this	O
number	O
compare	O
with	O
the	O
maximum	O
number	O
of	O
bits	O
that	O
can	O
be	O
corrected	O
by	O
the	O
optimal	B
decoder	B
assuming	O
the	O
vector	O
x	O
is	O
either	O
a	O
noisy	B
version	O
of	O
m	O
or	O
of	O
n	O
exercise	O
network	B
as	O
a	O
collection	O
of	O
binary	O
this	O
exercise	O
explores	O
the	O
link	O
between	O
unsupervised	O
networks	O
and	O
supervised	O
networks	O
if	O
a	O
network	B
s	O
desired	O
memories	O
are	O
all	O
attracting	O
stable	O
states	O
then	O
every	O
neuron	B
in	O
the	O
network	B
has	O
weights	O
going	O
to	O
it	O
that	O
solve	O
a	O
problem	O
personal	O
to	O
that	O
neuron	B
take	O
the	O
set	B
of	O
memories	O
and	O
write	O
them	O
in	O
the	O
form	O
xn	O
where	O
denotes	O
all	O
the	O
components	O
for	O
all	O
i	O
and	O
let	O
denote	O
the	O
vector	O
of	O
weights	O
for	O
i	O
using	O
what	O
we	O
know	O
about	O
the	O
capacity	B
of	O
the	O
single	B
neuron	B
show	O
that	O
it	O
is	O
almost	O
certainly	O
impossible	O
to	O
store	O
more	O
than	O
random	B
memories	O
in	O
a	O
network	B
of	O
i	O
neurons	O
i	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
lyapunov	O
functions	B
exercise	O
erik	O
s	O
puzzle	O
in	O
a	O
stripped-down	O
version	O
of	O
conway	B
s	O
game	O
of	O
life	B
cells	O
are	O
arranged	O
on	O
a	O
square	B
grid	O
each	O
cell	O
is	O
either	O
alive	O
or	O
dead	O
live	O
cells	O
do	O
not	O
die	B
dead	O
cells	O
become	O
alive	O
if	O
two	O
or	O
more	O
of	O
their	O
immediate	O
neighbours	O
are	O
alive	O
to	O
north	O
south	O
east	O
and	O
west	O
what	O
is	O
the	O
smallest	O
number	O
of	O
live	O
cells	O
needed	O
in	O
order	O
that	O
these	O
rules	B
lead	O
to	O
an	O
entire	O
n	O
n	O
square	B
being	O
alive	O
in	O
a	O
d-dimensional	O
version	O
of	O
the	O
same	O
game	O
the	O
rule	O
is	O
that	O
if	O
d	O
neighbours	O
are	O
alive	O
then	O
you	O
come	O
to	O
life	B
what	O
is	O
the	O
smallest	O
number	O
of	O
live	O
cells	O
needed	O
in	O
order	O
that	O
an	O
entire	O
n	O
n	O
n	O
hypercube	O
becomes	O
alive	O
how	O
should	O
those	O
live	O
cells	O
be	O
arranged	O
networks	O
figure	O
erik	O
s	O
dynamics	O
the	O
southeast	B
puzzle	I
u	O
u	O
u	O
u	O
u	O
u	O
uu	O
u	O
eee	O
ee	O
e	O
eeee	O
the	O
southeast	B
puzzle	I
is	O
played	O
on	O
a	O
chess	B
board	I
starting	O
at	O
its	O
northwest	O
left	O
corner	O
there	O
are	O
three	O
rules	B
figure	O
the	O
southeast	B
puzzle	I
in	O
the	O
starting	O
position	O
one	O
piece	O
is	O
placed	O
in	O
the	O
northwest-most	O
square	B
it	O
is	O
not	O
permitted	O
for	O
more	O
than	O
one	O
piece	O
to	O
be	O
on	O
any	O
given	O
square	B
at	O
each	O
step	O
you	O
remove	O
one	O
piece	O
from	O
the	O
board	O
and	O
replace	O
it	O
with	O
two	O
pieces	O
one	O
in	O
the	O
square	B
immediately	O
to	O
the	O
east	O
and	O
one	O
in	O
the	O
the	O
square	B
immediately	O
to	O
the	O
south	O
as	O
illustrated	O
in	O
every	O
such	O
step	O
increases	O
the	O
number	O
of	O
pieces	O
on	O
the	O
board	O
by	O
one	O
after	O
move	O
has	O
been	O
made	O
either	O
piece	O
may	O
be	O
selected	O
for	O
the	O
next	O
move	O
figure	O
shows	O
the	O
outcome	O
of	O
moving	O
the	O
lower	O
piece	O
at	O
the	O
next	O
move	O
either	O
the	O
lowest	O
piece	O
or	O
the	O
middle	O
piece	O
of	O
the	O
three	O
may	O
be	O
selected	O
the	O
uppermost	O
piece	O
may	O
not	O
be	O
selected	O
since	O
that	O
would	O
violate	O
rule	O
at	O
move	O
we	O
have	O
selected	O
the	O
middle	O
piece	O
now	O
any	O
of	O
the	O
pieces	O
may	O
be	O
moved	O
except	O
for	O
the	O
leftmost	O
piece	O
now	O
here	O
is	O
the	O
puzzle	O
exercise	O
is	O
it	O
possible	O
to	O
obtain	O
a	O
position	O
in	O
which	O
all	O
the	O
ten	O
squares	O
closest	O
to	O
the	O
northwest	O
corner	O
marked	O
in	O
are	O
empty	O
this	O
puzzle	O
has	O
a	O
connection	O
to	O
data	O
compression	B
solutions	O
solution	O
to	O
exercise	O
take	O
a	O
binary	O
feedback	B
network	B
with	O
neurons	O
and	O
let	O
and	O
then	O
whenever	O
neuron	B
is	O
updated	O
it	O
will	O
match	O
neuron	B
and	O
whenever	O
neuron	B
is	O
updated	O
it	O
will	O
to	O
the	O
opposite	O
state	O
from	O
neuron	B
there	O
is	O
no	O
stable	O
state	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
solution	O
to	O
exercise	O
take	O
a	O
binary	O
network	B
with	O
neurons	O
and	O
let	O
and	O
let	O
the	O
initial	O
condition	O
be	O
then	O
if	O
the	O
dynamics	O
are	O
synchronous	O
on	O
every	O
iteration	O
both	O
neurons	O
will	O
their	O
state	O
the	O
dynamics	O
do	O
not	O
converge	O
to	O
a	O
point	O
solution	O
to	O
exercise	O
the	O
key	O
to	O
this	O
problem	O
is	O
to	O
notice	O
its	O
similarity	O
to	O
the	O
construction	B
of	O
a	O
binary	O
symbol	B
code	I
starting	O
from	O
the	O
empty	B
string	I
we	O
can	O
build	O
a	O
binary	O
tree	B
by	O
repeatedly	O
splitting	O
a	O
codeword	B
into	O
two	O
every	O
codeword	B
has	O
an	O
implicit	O
probability	B
where	O
l	O
is	O
the	O
depth	O
of	O
the	O
codeword	B
in	O
the	O
binary	O
tree	B
whenever	O
we	O
split	O
a	O
codeword	B
in	O
two	O
and	O
create	O
two	O
new	O
codewords	O
whose	O
length	O
is	O
increased	O
by	O
one	O
the	O
two	O
new	O
codewords	O
each	O
have	O
implicit	O
probability	B
equal	O
to	O
half	O
that	O
of	O
the	O
old	O
codeword	B
for	O
a	O
complete	O
binary	O
code	O
the	O
kraft	B
equality	O
that	O
the	O
sum	O
of	O
these	O
implicit	B
probabilities	I
is	O
similarly	O
in	O
southeast	B
we	O
can	O
associate	O
a	O
weight	O
with	O
each	O
piece	O
on	O
the	O
board	O
if	O
we	O
assign	O
a	O
weight	O
of	O
to	O
any	O
piece	O
sitting	O
on	O
the	O
top	O
left	O
square	B
a	O
weight	O
of	O
to	O
any	O
piece	O
on	O
a	O
square	B
whose	O
distance	B
from	O
the	O
top	O
left	O
is	O
one	O
a	O
weight	O
of	O
to	O
any	O
piece	O
whose	O
distance	B
from	O
the	O
top	O
left	O
is	O
two	O
and	O
so	O
forth	O
with	O
distance	B
being	O
the	O
city-block	O
distance	B
then	O
every	O
legal	O
move	O
in	O
southeast	B
leaves	O
unchanged	O
the	O
total	O
weight	O
of	O
all	O
pieces	O
on	O
the	O
board	O
lyapunov	O
functions	B
come	O
in	O
two	O
the	O
function	O
may	O
be	O
a	O
function	O
of	O
state	O
whose	O
value	O
is	O
known	O
to	O
stay	O
constant	O
or	O
it	O
may	O
be	O
a	O
function	O
of	O
state	O
that	O
is	O
bounded	O
below	O
and	O
whose	O
value	O
always	O
decreases	O
or	O
stays	O
constant	O
the	O
total	O
weight	O
is	O
a	O
lyapunov	B
function	I
of	O
the	O
second	O
type	O
the	O
starting	O
weight	O
is	O
so	O
now	O
we	O
have	O
a	O
powerful	O
tool	O
a	O
conserved	O
function	O
of	O
the	O
state	O
is	O
it	O
possible	O
to	O
a	O
position	O
in	O
which	O
the	O
ten	O
highestweight	O
squares	O
are	O
vacant	O
and	O
the	O
total	O
weight	O
is	O
what	O
is	O
the	O
total	O
weight	O
if	O
all	O
the	O
other	O
squares	O
on	O
the	O
board	O
are	O
occupied	O
the	O
total	O
weight	O
would	O
be	O
which	O
is	O
equal	O
to	O
so	O
it	O
is	O
impossible	O
to	O
empty	O
all	O
ten	O
of	O
those	O
squares	O
uuuuu	O
uu	O
uu	O
uuuuuu	O
figure	O
a	O
possible	O
position	O
for	O
the	O
southeast	B
puzzle	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
boltzmann	B
machines	O
from	O
networks	O
to	O
boltzmann	B
machines	O
we	O
have	O
noticed	O
that	O
the	O
binary	O
network	B
minimizes	O
an	O
energy	B
function	O
xtwx	O
ex	O
and	O
that	O
the	O
continuous	B
network	B
with	O
activation	B
function	I
xn	O
tanhan	O
can	O
be	O
viewed	O
as	O
approximating	O
the	O
probability	B
distribution	B
associated	O
with	O
that	O
energy	B
function	O
p	O
w	O
zw	O
zw	O
these	O
observations	O
motivate	O
the	O
idea	O
of	O
working	O
with	O
a	O
neural	B
network	B
model	B
that	O
actually	O
implements	O
the	O
above	O
probability	B
distribution	B
the	O
stochastic	B
network	B
or	O
boltzmann	B
machine	I
and	O
se	O
jnowski	O
has	O
the	O
following	O
activity	B
rule	I
activity	B
rule	I
of	O
boltzmann	B
machine	I
after	O
computing	O
the	O
activa	O
tion	O
ai	O
set	B
xi	O
with	O
probability	B
else	O
set	B
xi	O
this	O
rule	O
implements	O
gibbs	B
sampling	I
for	O
the	O
probability	B
distribution	B
boltzmann	B
machine	B
learning	B
given	O
a	O
set	B
of	O
examples	O
fxngn	O
in	O
adjusting	O
the	O
weights	O
w	O
such	O
that	O
the	O
generative	B
model	B
from	O
the	O
real	O
world	O
we	O
might	O
be	O
interested	O
p	O
w	O
zw	O
is	O
well	O
matched	O
to	O
those	O
examples	O
we	O
can	O
derive	O
a	O
learning	B
algorithm	O
by	O
writing	B
down	O
bayes	B
theorem	O
to	O
obtain	O
the	O
posterior	B
probability	B
of	O
the	O
weights	O
given	O
the	O
data	O
p	O
jfxngn	O
g	O
n	O
p	O
j	O
w	O
p	O
p	O
g	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
from	O
networks	O
to	O
boltzmann	B
machines	O
we	O
concentrate	O
on	O
the	O
term	O
in	O
the	O
numerator	O
the	O
likelihood	B
and	O
derive	O
a	O
maximum	B
likelihood	B
algorithm	O
there	O
might	O
be	O
advantages	O
in	O
pursuing	O
a	O
full	O
bayesian	B
approach	O
as	O
we	O
did	O
in	O
the	O
case	O
of	O
the	O
single	B
neuron	B
we	O
the	O
logarithm	O
of	O
the	O
likelihood	B
ln	O
n	O
p	O
j	O
w	O
n	O
xnt	O
wxn	O
ln	O
with	O
respect	O
to	O
wij	O
bearing	B
in	O
mind	O
that	O
w	O
is	O
to	O
be	O
symmetric	B
with	O
wji	O
wij	O
exercise	O
show	O
that	O
the	O
derivative	O
of	O
ln	O
zw	O
with	O
respect	O
to	O
wij	O
is	O
ln	O
zw	O
xixjp	O
w	O
hxixjip	O
w	O
exercise	O
is	O
similar	O
to	O
exercise	O
the	O
derivative	O
of	O
the	O
log	O
likelihood	B
is	O
therefore	O
ln	O
p	O
gj	O
w	O
n	O
i	O
xn	O
j	O
hxixjip	O
j	O
wi	O
nhhxixjidata	O
hxixjip	O
wi	O
this	O
gradient	O
is	O
proportional	O
to	O
the	O
of	O
two	O
terms	O
the	O
term	O
is	O
the	O
empirical	O
correlation	O
between	O
xi	O
and	O
xj	O
hxixjidata	O
n	O
n	O
i	O
xn	O
j	O
i	O
and	O
the	O
second	O
term	O
is	O
the	O
correlation	O
between	O
xi	O
and	O
xj	O
under	O
the	O
current	O
model	B
hxixjip	O
j	O
w	O
xixjp	O
w	O
the	O
correlation	O
hxixjidata	O
is	O
readily	O
evaluated	O
it	O
is	O
just	O
the	O
empirical	O
correlation	O
between	O
the	O
activities	O
in	O
the	O
real	O
world	O
the	O
second	O
correlation	O
hxixjip	O
j	O
w	O
is	O
not	O
so	O
easy	O
to	O
evaluate	O
but	O
it	O
can	O
be	O
estimated	O
by	O
monte	B
carlo	I
methods	I
that	O
is	O
by	O
observing	O
the	O
average	O
value	O
of	O
xixj	O
while	O
the	O
activity	B
rule	I
of	O
the	O
boltzmann	B
machine	I
equation	O
is	O
iterated	O
in	O
the	O
special	O
case	O
w	O
we	O
can	O
evaluate	O
the	O
gradient	O
exactly	O
because	O
by	O
symmetry	O
the	O
correlation	O
hxixjip	O
j	O
w	O
must	O
be	O
zero	O
if	O
the	O
weights	O
are	O
adjusted	O
by	O
gradient	B
descent	I
with	O
learning	B
rate	B
then	O
after	O
one	O
iteration	O
the	O
weights	O
will	O
be	O
wij	O
n	O
i	O
xn	O
j	O
i	O
precisely	O
the	O
value	O
of	O
the	O
weights	O
given	O
by	O
the	O
hebb	B
rule	O
equation	O
with	O
which	O
we	O
trained	O
the	O
network	B
interpretation	O
of	O
boltzmann	B
machine	B
learning	B
one	O
way	O
of	O
viewing	O
the	O
two	O
terms	O
in	O
the	O
gradient	O
is	O
as	O
waking	O
and	O
sleeping	O
rules	B
while	O
the	O
network	B
is	O
awake	O
it	O
measures	O
the	O
correlation	O
between	O
xi	O
and	O
xj	O
in	O
the	O
real	O
world	O
and	O
weights	O
are	O
increased	O
in	O
proportion	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
boltzmann	B
machines	O
figure	O
the	O
shifter	O
ensembles	O
four	O
samples	O
from	O
the	O
plain	O
shifter	B
ensemble	B
four	O
corresponding	O
samples	O
from	O
the	O
labelled	O
shifter	B
ensemble	B
while	O
the	O
network	B
is	O
asleep	O
it	O
dreams	O
about	O
the	O
world	O
using	O
the	O
generative	B
model	B
and	O
measures	O
the	O
correlations	B
between	O
xi	O
and	O
xj	O
in	O
the	O
model	B
world	O
these	O
correlations	B
determine	O
a	O
proportional	O
decrease	O
in	O
the	O
weights	O
if	O
the	O
second-order	O
correlations	B
in	O
the	O
dream	B
world	O
match	O
the	O
correlations	B
in	O
the	O
real	O
world	O
then	O
the	O
two	O
terms	O
balance	B
and	O
the	O
weights	O
do	O
not	O
change	O
criticism	O
of	O
networks	O
and	O
simple	O
boltzmann	B
machines	O
up	O
to	O
this	O
point	O
we	O
have	O
discussed	O
networks	O
and	O
boltzmann	B
machines	O
in	O
which	O
all	O
of	O
the	O
neurons	O
correspond	O
to	O
visible	O
variables	O
xi	O
the	O
result	O
is	O
a	O
probabilistic	B
model	B
that	O
when	O
optimized	O
can	O
capture	O
the	O
second-order	O
statistics	O
of	O
the	O
environment	O
second-order	O
statistics	O
of	O
an	O
ensemble	B
p	O
are	O
the	O
expected	O
values	O
hxixji	O
of	O
all	O
the	O
pairwise	O
products	O
xixj	O
the	O
real	O
world	O
however	O
often	O
has	O
higher-order	O
correlations	B
that	O
must	O
be	O
included	O
if	O
our	O
description	O
of	O
it	O
is	O
to	O
be	O
often	O
the	O
second-order	O
correlations	B
in	O
themselves	O
may	O
carry	O
little	O
or	O
no	O
useful	O
information	B
consider	O
for	O
example	O
the	O
ensemble	B
of	O
binary	B
images	B
of	O
chairs	O
we	O
can	O
imagine	O
images	B
of	O
chairs	O
with	O
various	O
designs	O
four-legged	O
chairs	O
comfy	O
chairs	O
chairs	O
with	O
legs	O
and	O
wheels	O
wooden	O
chairs	O
cushioned	O
chairs	O
chairs	O
with	O
rockers	O
instead	O
of	O
legs	O
a	O
child	O
can	O
easily	O
learn	O
to	O
distinguish	O
these	O
images	B
from	O
images	B
of	O
carrots	O
and	O
parrots	O
but	O
i	O
expect	O
the	O
second-order	O
statistics	O
of	O
the	O
raw	O
data	O
are	O
useless	O
for	O
describing	O
the	O
ensemble	B
second-order	O
statistics	O
only	O
capture	O
whether	O
two	O
pixels	O
are	O
likely	O
to	O
be	O
in	O
the	O
same	O
state	O
as	O
each	O
other	O
higher-order	O
concepts	O
are	O
needed	O
to	O
make	O
a	O
good	B
generative	B
model	B
of	O
images	B
of	O
chairs	O
a	O
simpler	O
ensemble	B
of	O
images	B
in	O
which	O
high-order	B
statistics	O
are	O
important	O
is	O
the	O
shifter	B
ensemble	B
which	O
comes	O
in	O
two	O
figure	O
shows	O
a	O
few	O
samples	O
from	O
the	O
plain	O
shifter	B
ensemble	B
in	O
each	O
image	B
the	O
bottom	O
eight	O
pixels	O
are	O
a	O
copy	O
of	O
the	O
top	O
eight	O
pixels	O
either	O
shifted	O
one	O
pixel	O
to	O
the	O
left	O
or	O
unshifted	O
or	O
shifted	O
one	O
pixel	O
to	O
the	O
right	O
top	O
eight	O
pixels	O
are	O
set	B
at	O
random	B
this	O
ensemble	B
is	O
a	O
simple	O
model	B
of	O
the	O
visual	O
signals	O
from	O
the	O
two	O
eyes	O
arriving	O
at	O
early	O
levels	O
of	O
the	O
brain	B
the	O
signals	O
from	O
the	O
two	O
eyes	O
are	O
similar	O
to	O
each	O
other	O
but	O
may	O
by	O
small	O
translations	O
because	O
of	O
the	O
varying	O
depth	O
of	O
the	O
visual	O
world	O
this	O
ensemble	B
is	O
simple	O
to	O
describe	O
but	O
its	O
second-order	O
statistics	O
convey	O
no	O
useful	O
information	B
the	O
correlation	O
between	O
one	O
pixel	O
and	O
any	O
of	O
the	O
three	O
pixels	O
above	O
it	O
is	O
the	O
correlation	O
between	O
any	O
other	O
two	O
pixels	O
is	O
zero	O
figure	O
shows	O
a	O
few	O
samples	O
from	O
the	O
labelled	O
shifter	B
ensemble	B
here	O
the	O
problem	O
has	O
been	O
made	O
easier	O
by	O
including	O
an	O
extra	O
three	O
neurons	O
that	O
label	O
the	O
visual	O
image	B
as	O
being	O
an	O
instance	O
of	O
either	O
the	O
shift	O
left	O
no	O
shift	O
or	O
shift	O
right	O
sub-ensemble	O
but	O
with	O
this	O
extra	O
information	B
the	O
ensemble	B
is	O
still	O
not	O
learnable	O
using	O
second-order	O
statistics	O
alone	O
the	O
secondorder	O
correlation	O
between	O
any	O
label	O
neuron	B
and	O
any	O
image	B
neuron	B
is	O
zero	O
we	O
need	O
models	O
that	O
can	O
capture	O
higher-order	O
statistics	O
of	O
an	O
environment	O
so	O
how	O
can	O
we	O
develop	O
such	O
models	O
one	O
idea	O
might	O
be	O
to	O
create	O
models	O
that	O
directly	O
capture	O
higher-order	O
correlations	B
such	O
as	O
p	O
w	O
v	O
wijxixj	O
vijkxixjxk	O
a	O
such	O
higher-order	O
boltzmann	B
machines	O
are	O
equally	O
easy	O
to	O
simulate	O
using	O
stochastic	B
updates	O
and	O
the	O
learning	B
rule	I
for	O
the	O
higher-order	O
parameters	B
vijk	O
is	O
equivalent	O
to	O
the	O
learning	B
rule	I
for	O
wij	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
boltzmann	B
machine	I
with	O
hidden	O
units	B
exercise	O
derive	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
vijk	O
it	O
is	O
possible	O
that	O
the	O
spines	B
found	O
on	O
biological	O
neurons	O
are	O
responsible	O
for	O
detecting	O
correlations	B
between	O
small	O
numbers	O
of	O
incoming	O
signals	O
however	O
to	O
capture	O
statistics	O
of	O
high	O
enough	O
order	O
to	O
describe	O
the	O
ensemble	B
of	O
images	B
of	O
chairs	O
well	O
would	O
require	O
an	O
unimaginable	O
number	O
of	O
terms	O
to	O
capture	O
merely	O
the	O
fourth-order	O
statistics	O
in	O
a	O
pixel	O
image	B
we	O
need	O
more	O
than	O
parameters	B
so	O
measuring	O
moments	O
of	O
images	B
is	O
not	O
a	O
good	B
way	O
to	O
describe	O
their	O
underlying	O
structure	O
perhaps	O
what	O
we	O
need	O
instead	O
or	O
in	O
addition	O
are	O
hidden	O
variables	O
also	O
known	O
to	O
statisticians	O
as	O
latent	O
variables	O
this	O
is	O
the	O
important	O
innovation	O
introduced	O
by	O
hinton	B
and	O
sejnowski	B
the	O
idea	O
is	O
that	O
the	O
high-order	B
correlations	B
among	O
the	O
visible	O
variables	O
are	O
described	O
by	O
including	O
extra	O
hidden	O
variables	O
and	O
sticking	O
to	O
a	O
model	B
that	O
has	O
only	O
second-order	O
interactions	O
between	O
its	O
variables	O
the	O
hidden	O
variables	O
induce	O
higher-order	O
correlations	B
between	O
the	O
visible	O
variables	O
boltzmann	B
machine	I
with	O
hidden	O
units	B
we	O
now	O
add	O
hidden	B
neurons	I
to	O
our	O
stochastic	B
model	B
these	O
are	O
neurons	O
that	O
do	O
not	O
correspond	O
to	O
observed	O
variables	O
they	O
are	O
free	O
to	O
play	O
any	O
role	O
in	O
the	O
probabilistic	B
model	B
by	O
equation	O
they	O
might	O
actually	O
take	O
on	O
interpretable	O
roles	O
performing	O
feature	O
extraction	O
learning	B
in	O
boltzmann	B
machines	O
with	O
hidden	O
units	B
the	O
activity	B
rule	I
of	O
a	O
boltzmann	B
machine	I
with	O
hidden	O
units	B
is	O
identical	O
to	O
that	O
of	O
the	O
original	O
boltzmann	B
machine	I
the	O
learning	B
rule	I
can	O
again	O
be	O
derived	O
by	O
maximum	B
likelihood	B
but	O
now	O
we	O
need	O
to	O
take	O
into	O
account	O
the	O
fact	O
that	O
the	O
states	O
of	O
the	O
hidden	O
units	B
are	O
unknown	O
we	O
will	O
denote	O
the	O
states	O
of	O
the	O
visible	O
units	B
by	O
x	O
the	O
states	O
of	O
the	O
hidden	O
units	B
by	O
h	O
and	O
the	O
generic	O
state	O
of	O
a	O
neuron	B
visible	O
or	O
hidden	O
by	O
yi	O
with	O
y	O
h	O
the	O
state	O
of	O
the	O
network	B
when	O
the	O
visible	O
neurons	O
are	O
clamped	O
in	O
state	O
xn	O
is	O
yn	O
h	O
the	O
likelihood	B
of	O
w	O
given	O
a	O
single	O
data	O
example	O
xn	O
is	O
p	O
j	O
w	O
where	O
p	O
hj	O
w	O
zw	O
equation	O
may	O
also	O
be	O
written	O
zw	O
where	O
p	O
j	O
w	O
zxnw	O
zxn	O
zw	O
the	O
likelihood	B
as	O
before	O
we	O
that	O
the	O
derivative	O
with	O
respect	O
to	O
any	O
weight	O
wij	O
is	O
again	O
the	O
between	O
a	O
waking	O
term	O
and	O
a	O
sleeping	O
term	O
ln	O
p	O
j	O
w	O
nhyiyjip	O
j	O
xnw	O
hyiyjip	O
wo	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
boltzmann	B
machines	O
the	O
term	O
hyiyjip	O
j	O
xnw	O
is	O
the	O
correlation	O
between	O
yi	O
and	O
yj	O
if	O
the	O
boltzmann	B
machine	I
is	O
simulated	O
with	O
the	O
visible	O
variables	O
clamped	O
to	O
xn	O
and	O
the	O
hidden	O
variables	O
freely	O
sampling	O
from	O
their	O
conditional	B
distribution	B
the	O
second	O
term	O
hyiyjip	O
w	O
is	O
the	O
correlation	O
between	O
yi	O
and	O
yj	O
when	O
hinton	B
and	O
sejnowski	B
demonstrated	O
that	O
non-trivial	O
ensembles	O
such	O
as	O
the	O
labelled	O
shifter	B
ensemble	B
can	O
be	O
learned	O
using	O
a	O
boltzmann	B
machine	I
with	O
hidden	O
units	B
the	O
hidden	O
units	B
take	O
on	O
the	O
role	O
of	O
feature	O
detectors	O
that	O
spot	O
patterns	O
likely	O
to	O
be	O
associated	O
with	O
one	O
of	O
the	O
three	O
shifts	O
the	O
boltzmann	B
machine	I
generates	O
samples	O
from	O
its	O
model	B
distribution	B
the	O
boltzmann	B
machine	I
is	O
time-consuming	O
to	O
simulate	O
because	O
the	O
computation	O
of	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
depends	O
on	O
taking	O
the	O
of	O
two	O
gradients	O
both	O
found	O
by	O
monte	B
carlo	I
methods	I
so	O
boltzmann	B
machines	O
are	O
not	O
in	O
widespread	O
use	O
it	O
is	O
an	O
area	O
of	O
active	O
research	O
to	O
create	O
models	O
that	O
embody	O
the	O
same	O
capabilities	O
using	O
more	O
computations	O
et	O
al	O
dayan	O
et	O
al	O
hinton	B
and	O
ghahramani	O
hinton	B
hinton	B
and	O
teh	O
exercise	O
exercise	O
can	O
the	O
bars	O
and	O
stripes	O
ensemble	B
be	O
learned	O
by	O
a	O
boltzmann	B
machine	I
with	O
no	O
hidden	O
units	B
may	O
be	O
surprised	O
figure	O
four	O
samples	O
from	O
the	O
bars	O
and	O
stripes	O
ensemble	B
each	O
sample	B
is	O
generated	O
by	O
picking	O
an	O
orientation	O
horizontal	O
or	O
vertical	O
then	O
for	O
each	O
row	O
of	O
spins	O
in	O
that	O
orientation	O
bar	O
or	O
stripe	O
respectively	O
switching	O
all	O
spins	O
on	O
with	O
probability	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
multilayer	O
perceptrons	O
no	O
course	O
on	O
neural	O
networks	O
could	O
be	O
complete	O
without	O
a	O
discussion	O
of	O
supervised	O
multilayer	O
networks	O
also	O
known	O
as	O
backpropagation	B
networks	O
the	O
multilayer	B
perceptron	I
is	O
a	O
feedforward	O
network	B
it	O
has	O
input	O
neurons	O
hidden	B
neurons	I
and	O
output	O
neurons	O
the	O
hidden	B
neurons	I
may	O
be	O
arranged	O
in	O
a	O
sequence	B
of	O
layers	O
the	O
most	O
common	O
multilayer	O
perceptrons	O
have	O
a	O
single	O
hidden	O
layer	O
and	O
are	O
known	O
as	O
two-layer	O
networks	O
the	O
number	O
two	O
counting	B
the	O
number	O
of	O
layers	O
of	O
neurons	O
not	O
including	O
the	O
inputs	O
such	O
a	O
feedforward	O
network	B
a	O
nonlinear	B
parameterized	O
mapping	B
from	O
an	O
input	O
x	O
to	O
an	O
output	O
y	O
yx	O
wa	O
the	O
output	O
is	O
a	O
continuous	B
function	O
of	O
the	O
input	O
and	O
of	O
the	O
parameters	B
w	O
the	O
architecture	B
of	O
the	O
net	O
i	O
e	O
the	O
functional	O
form	O
of	O
the	O
mapping	B
is	O
denoted	O
by	O
a	O
feedforward	O
networks	O
can	O
be	O
trained	O
to	O
perform	O
regression	B
and	O
tasks	O
regression	B
networks	O
in	O
the	O
case	O
of	O
a	O
regression	B
problem	O
the	O
mapping	B
for	O
a	O
network	B
with	O
one	O
hidden	O
layer	O
may	O
have	O
the	O
form	O
hidden	O
layer	O
output	O
layer	O
j	O
i	O
jl	O
xl	O
j	O
hj	O
f	O
j	O
ij	O
hj	O
i	O
yi	O
f	O
i	O
where	O
for	O
example	O
f	O
tanha	O
and	O
f	O
a	O
here	O
l	O
runs	O
over	O
the	O
inputs	O
xl	O
j	O
runs	O
over	O
the	O
hidden	O
units	B
and	O
i	O
runs	O
over	O
the	O
outputs	O
the	O
weights	O
w	O
and	O
biases	O
together	O
make	O
up	O
the	O
parameter	O
vector	O
w	O
the	O
nonlinear	B
sigmoid	B
function	O
f	O
at	O
the	O
hidden	O
layer	O
gives	O
the	O
neural	B
network	B
greater	O
computational	O
than	O
a	O
standard	O
linear	B
regression	B
model	B
graphically	O
we	O
can	O
represent	O
the	O
neural	B
network	B
as	O
a	O
set	B
of	O
layers	O
of	O
connected	O
neurons	O
what	O
sorts	O
of	O
functions	B
can	O
these	O
networks	O
implement	O
just	O
as	O
we	O
explored	O
the	O
weight	B
space	I
of	O
the	O
single	B
neuron	B
in	O
chapter	O
examining	O
the	O
functions	B
it	O
could	O
produce	O
let	O
us	O
explore	B
the	O
weight	B
space	I
of	O
a	O
multilayer	O
network	B
in	O
and	O
i	O
take	O
a	O
network	B
with	O
one	O
input	O
and	O
one	O
output	O
and	O
a	O
large	O
number	O
h	O
of	O
hidden	O
units	B
set	B
the	O
biases	O
outputs	O
hiddens	O
inputs	O
figure	O
a	O
typical	B
two-layer	O
network	B
with	O
six	B
inputs	O
seven	O
hidden	O
units	B
and	O
three	O
outputs	O
each	O
line	O
represents	O
one	O
weight	O
figure	O
samples	O
from	O
the	O
prior	B
over	O
functions	B
of	O
a	O
one-input	O
network	B
for	O
each	O
of	O
a	O
sequence	B
of	O
values	O
of	O
and	O
is	O
shown	O
the	O
other	O
hyperparameters	O
of	O
the	O
network	B
were	O
h	O
bias	B
one	O
random	B
function	O
out	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
output	O
ty	O
ttttt	O
t	O
tx	O
hidden	O
layer	O
input	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
t	O
u	O
p	O
t	O
u	O
o	O
input	O
figure	O
properties	O
of	O
a	O
function	O
produced	O
by	O
a	O
random	B
network	B
the	O
vertical	O
scale	O
of	O
a	O
typical	B
function	O
produced	O
by	O
the	O
network	B
with	O
random	B
weights	O
is	O
of	O
order	O
the	O
horizontal	O
range	O
in	O
which	O
the	O
function	O
varies	O
is	O
of	O
order	O
and	O
the	O
shortest	O
horizontal	O
length	O
scale	O
is	O
of	O
order	O
the	O
function	O
shown	O
was	O
produced	O
by	O
making	O
a	O
random	B
network	B
with	O
h	O
hidden	O
units	B
and	O
gaussian	B
weights	O
with	O
and	O
i	O
j	O
jl	O
and	O
ij	O
to	O
random	B
values	O
and	O
plot	O
the	O
resulting	O
i	O
set	B
the	O
hidden	O
units	B
biases	O
and	O
weights	O
j	O
function	O
yx	O
to	O
random	B
values	O
from	O
a	O
gaussian	B
with	O
zero	O
mean	B
and	O
standard	B
deviation	I
the	O
input-to-hidden	O
weights	O
to	O
random	B
values	O
with	O
standard	B
deviation	I
and	O
the	O
bias	B
and	O
jl	O
output	O
weights	O
to	O
random	B
values	O
with	O
standard	B
deviation	I
the	O
sort	O
of	O
functions	B
that	O
we	O
obtain	O
depend	O
on	O
the	O
values	O
of	O
and	O
as	O
the	O
weights	O
and	O
biases	O
are	O
made	O
bigger	O
we	O
obtain	O
more	O
complex	B
functions	B
with	O
more	O
features	O
and	O
a	O
greater	O
sensitivity	O
to	O
the	O
input	O
variable	O
the	O
vertical	O
scale	O
of	O
a	O
typical	B
function	O
produced	O
by	O
the	O
network	B
with	O
random	B
weights	O
is	O
of	O
order	O
the	O
horizontal	O
range	O
in	O
which	O
the	O
function	O
varies	O
is	O
of	O
order	O
and	O
the	O
shortest	O
horizontal	O
length	O
scale	O
is	O
of	O
order	O
and	O
ij	O
i	O
radford	O
neal	B
has	O
also	O
shown	O
that	O
in	O
the	O
limit	O
as	O
h	O
the	O
statistical	B
properties	O
of	O
the	O
functions	B
generated	O
by	O
randomizing	O
the	O
weights	O
are	O
independent	O
of	O
the	O
number	O
of	O
hidden	O
units	B
so	O
interestingly	O
the	O
complexity	B
of	O
the	O
functions	B
becomes	O
independent	O
of	O
the	O
number	O
of	O
parameters	B
in	O
the	O
model	B
what	O
determines	O
the	O
complexity	B
of	O
the	O
typical	B
functions	B
is	O
the	O
characteristic	O
magnitude	O
of	O
the	O
weights	O
thus	O
we	O
anticipate	O
that	O
when	O
we	O
these	O
models	O
to	O
real	O
data	O
an	O
important	O
way	O
of	O
controlling	O
the	O
complexity	B
of	O
the	O
function	O
will	O
be	O
to	O
control	O
the	O
characteristic	O
magnitude	O
of	O
the	O
weights	O
figure	O
shows	O
one	O
typical	B
function	O
produced	O
by	O
a	O
network	B
with	O
two	O
inputs	O
and	O
one	O
output	O
this	O
should	O
be	O
contrasted	O
with	O
the	O
function	O
produced	O
by	O
a	O
traditional	O
linear	B
regression	B
model	B
which	O
is	O
a	O
plane	O
neural	O
networks	O
can	O
create	O
functions	B
with	O
more	O
complexity	B
than	O
a	O
linear	B
regression	B
how	O
a	O
regression	B
network	B
is	O
traditionally	O
trained	O
this	O
network	B
is	O
trained	O
using	O
a	O
data	B
set	B
d	O
fxn	O
tng	O
by	O
adjusting	O
w	O
so	O
as	O
to	O
minimize	O
an	O
error	B
function	I
e	O
g	O
edw	O
xi	O
i	O
yixn	O
this	O
objective	B
function	I
is	O
a	O
sum	O
of	O
terms	O
one	O
for	O
each	O
inputtarget	O
pair	O
fx	O
tg	O
measuring	O
how	O
close	O
the	O
output	O
yx	O
w	O
is	O
to	O
the	O
target	O
t	O
this	O
minimization	B
is	O
based	O
on	O
repeated	O
evaluation	O
of	O
the	O
gradient	O
of	O
ed	O
this	O
gradient	O
can	O
be	O
computed	O
using	O
the	O
backpropagation	B
algorithm	O
et	O
al	O
which	O
uses	O
the	O
chain	B
rule	I
to	O
the	O
derivatives	O
figure	O
one	O
sample	B
from	I
the	O
prior	B
of	O
a	O
two-input	O
network	B
with	O
fh	O
outg	O
bias	B
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
neural	B
network	B
learning	B
as	B
inference	B
often	O
regularization	B
known	O
as	O
weight	B
decay	I
is	O
included	O
modifying	O
the	O
objective	B
function	I
to	O
m	O
where	O
for	O
example	O
ew	O
i	O
this	O
additional	O
term	O
favours	O
small	O
values	O
of	O
w	O
and	O
decreases	O
the	O
tendency	O
of	O
a	O
model	B
to	O
noise	O
in	O
the	O
training	B
data	I
rumelhart	O
et	O
al	O
showed	O
that	O
multilayer	O
perceptrons	O
can	O
be	O
trained	O
by	O
gradient	B
descent	I
on	O
m	O
to	O
discover	O
solutions	O
to	O
non-trivial	O
problems	O
such	O
as	O
deciding	O
whether	O
an	O
image	B
is	O
symmetric	B
or	O
not	O
these	O
networks	O
have	O
been	O
successfully	O
applied	O
to	O
real-world	O
tasks	O
as	O
varied	O
as	O
pronouncing	O
english	B
text	O
and	O
rosenberg	O
and	O
focussing	O
multiple-mirror	O
telescopes	O
et	O
al	O
neural	B
network	B
learning	B
as	B
inference	B
the	O
neural	B
network	B
learning	B
process	O
above	O
can	O
be	O
given	O
the	O
following	O
probabilistic	O
interpretation	O
we	O
repeat	O
and	O
generalize	O
the	O
discussion	O
of	O
chapter	O
the	O
error	B
function	I
is	O
interpreted	O
as	O
a	O
noise	O
model	B
is	O
the	O
negative	O
log	O
likelihood	B
p	O
j	O
w	O
thus	O
the	O
use	O
of	O
the	O
sum-squared	O
error	O
ed	O
corresponds	O
to	O
an	O
assumption	O
of	O
gaussian	B
noise	O
on	O
the	O
target	O
variables	O
and	O
the	O
parameter	O
a	O
noise	O
level	O
similarly	O
the	O
regularizer	O
is	O
interpreted	O
in	O
terms	O
of	O
a	O
log	O
prior	B
probability	B
distribution	B
over	O
the	O
parameters	B
p	O
j	O
zw	O
if	O
ew	O
is	O
quadratic	O
as	O
above	O
then	O
the	O
corresponding	O
prior	B
distribution	B
is	O
a	O
gaussian	B
with	O
variance	B
w	O
the	O
probabilistic	B
model	B
h	O
the	O
architecture	B
a	O
of	O
the	O
network	B
the	O
likelihood	B
and	O
the	O
prior	B
the	O
objective	B
function	I
m	O
then	O
corresponds	O
to	O
the	O
inference	B
of	O
the	O
parameters	B
w	O
given	O
the	O
data	O
p	O
j	O
d	O
p	O
j	O
w	O
j	O
p	O
j	O
zm	O
the	O
w	O
found	O
by	O
minimizing	O
m	O
is	O
then	O
interpreted	O
as	O
the	O
most	O
probable	O
parameter	O
vector	O
wmp	O
the	O
interpretation	O
of	O
m	O
as	O
a	O
log	O
probability	B
adds	O
little	O
new	O
at	O
this	O
stage	O
but	O
new	O
tools	O
will	O
emerge	O
when	O
we	O
proceed	O
to	O
other	O
inferences	O
first	O
though	O
let	O
us	O
establish	O
the	O
probabilistic	O
interpretation	O
of	O
networks	O
to	O
which	O
the	O
same	O
tools	O
apply	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
binary	O
networks	O
if	O
the	O
targets	O
t	O
in	O
a	O
data	B
set	B
are	O
binary	O
labels	O
it	O
is	O
natural	B
to	O
use	O
a	O
neural	B
network	B
whose	O
output	O
yx	O
wa	O
is	O
bounded	O
between	O
and	O
and	O
is	O
interpreted	O
as	O
a	O
probability	B
p	O
j	O
x	O
wa	O
for	O
example	O
a	O
network	B
with	O
one	O
hidden	O
layer	O
could	O
be	O
described	O
by	O
the	O
feedforward	O
equations	O
and	O
with	O
f	O
the	O
error	B
function	I
is	O
replaced	O
by	O
the	O
negative	O
log	O
likelihood	B
gw	O
tn	O
ln	O
yxn	O
w	O
tn	O
yxn	O
w	O
the	O
total	O
objective	B
function	I
is	O
then	O
m	O
g	O
note	O
that	O
this	O
includes	O
no	O
parameter	O
there	O
is	O
no	O
gaussian	B
noise	O
multi-class	O
networks	O
for	O
a	O
multi-class	O
problem	O
we	O
can	O
represent	O
the	O
targets	O
by	O
a	O
vector	O
t	O
in	O
which	O
a	O
single	O
element	O
is	O
set	B
to	O
indicating	O
the	O
correct	O
class	O
and	O
all	O
other	O
elements	O
are	O
set	B
to	O
in	O
this	O
case	O
it	O
is	O
appropriate	O
to	O
use	O
a	O
softmax	B
network	B
having	O
coupled	O
outputs	O
which	O
sum	O
to	O
one	O
and	O
are	O
interpreted	O
as	O
class	O
probabilities	O
yi	O
p	O
j	O
x	O
wa	O
the	O
last	O
part	O
of	O
equation	O
is	O
replaced	O
by	O
eai	O
yi	O
the	O
negative	O
log	O
likelihood	B
in	O
this	O
case	O
is	O
gw	O
xi	O
tn	O
i	O
ln	O
yixn	O
w	O
as	O
in	O
the	O
case	O
of	O
the	O
regression	B
network	B
the	O
minimization	B
of	O
the	O
objective	B
function	I
m	O
g	O
corresponds	O
to	O
an	O
inference	B
of	O
the	O
form	O
a	O
variety	O
of	O
useful	O
results	O
can	O
be	O
built	O
on	O
this	O
interpretation	O
of	O
the	O
bayesian	B
approach	O
to	O
supervised	O
feedforward	O
neural	O
networks	O
from	O
the	O
statistical	B
perspective	O
supervised	O
neural	O
networks	O
are	O
nothing	O
more	O
than	O
nonlinear	B
devices	O
curve	O
is	O
not	O
a	O
trivial	O
task	O
however	O
the	O
complexity	B
of	O
an	O
interpolating	O
model	B
is	O
of	O
crucial	O
importance	O
as	O
illustrated	O
in	O
consider	O
a	O
control	O
parameter	O
that	O
the	O
complexity	B
of	O
a	O
model	B
for	O
example	O
a	O
regularization	B
constant	I
decay	O
parameter	O
as	O
the	O
control	O
parameter	O
is	O
varied	O
to	O
increase	O
the	O
complexity	B
of	O
the	O
model	B
from	O
and	O
going	O
from	O
left	O
to	O
right	O
across	O
the	O
best	O
to	O
the	O
training	B
data	I
that	O
the	O
model	B
can	O
achieve	O
becomes	O
increasingly	O
good	B
however	O
the	O
empirical	O
performance	O
of	O
the	O
model	B
the	O
test	O
error	O
decreases	O
then	O
increases	O
again	O
an	O
over-complex	O
model	B
the	O
data	O
and	O
generalizes	O
poorly	O
this	O
problem	O
may	O
also	O
complicate	O
the	O
choice	O
of	O
architecture	B
in	O
a	O
multilayer	B
perceptron	I
the	O
radius	O
of	O
the	O
basis	O
functions	B
in	O
a	O
radial	B
basis	I
function	I
network	B
and	O
the	O
choice	O
of	O
the	O
input	O
variables	O
themselves	O
in	O
any	O
multidimensional	O
regression	B
problem	O
finding	O
values	O
for	O
model	B
control	O
parameters	B
that	O
are	O
appropriate	O
for	O
the	O
data	O
is	O
therefore	O
an	O
important	O
and	O
non-trivial	O
problem	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
of	O
the	O
bayesian	B
approach	O
to	O
supervised	O
feedforward	O
neural	O
networks	O
figure	O
optimization	B
of	B
model	B
complexity	B
panels	O
show	O
a	O
radial	B
basis	I
function	I
model	B
interpolating	O
a	O
simple	O
data	B
set	B
with	O
one	O
input	O
variable	O
and	O
one	O
output	O
variable	O
as	O
the	O
regularization	B
constant	I
is	O
varied	O
to	O
increase	O
the	O
complexity	B
of	O
the	O
model	B
to	O
the	O
interpolant	O
is	O
able	O
to	O
the	O
training	B
data	I
increasingly	O
well	O
but	O
beyond	O
a	O
certain	O
point	O
the	O
generalization	B
ability	O
error	O
of	O
the	O
model	B
deteriorates	O
probability	B
theory	O
allows	O
us	O
to	O
optimize	O
the	O
control	O
parameters	B
without	O
needing	O
a	O
test	O
set	B
test	O
error	O
training	O
error	O
model	B
control	O
parameters	B
log	O
probabilitytraining	O
data	O
control	O
parameters	B
model	B
control	O
parameters	B
the	O
problem	O
can	O
be	O
solved	O
by	O
using	O
a	O
bayesian	B
approach	O
to	O
control	O
model	B
complexity	B
if	O
we	O
give	O
a	O
probabilistic	O
interpretation	O
to	O
the	O
model	B
then	O
we	O
can	O
evaluate	O
the	O
evidence	B
for	O
alternative	O
values	O
of	O
the	O
control	O
parameters	B
as	O
was	O
explained	O
in	O
chapter	O
over-complex	O
models	O
turn	O
out	O
to	O
be	O
less	O
probable	O
and	O
the	O
evidence	B
p	O
j	O
control	O
parameters	B
can	O
be	O
used	O
as	O
an	O
objective	B
function	I
for	O
optimization	B
of	O
model	B
control	O
parameters	B
the	O
setting	O
of	O
that	O
maximizes	O
the	O
evidence	B
is	O
displayed	O
in	O
bayesian	B
optimization	B
of	O
model	B
control	O
parameters	B
has	O
four	O
important	O
advantages	O
no	O
test	O
set	B
or	O
validation	O
set	B
is	O
involved	O
so	O
all	O
available	O
training	B
data	I
can	O
be	O
devoted	O
to	O
both	O
model	B
and	O
model	B
comparison	I
regularization	B
constants	O
can	O
be	O
optimized	O
on-line	O
i	O
e	O
simultaneously	O
with	O
the	O
optimization	B
of	O
ordinary	O
model	B
parameters	B
the	O
bayesian	B
objective	B
function	I
is	O
not	O
noisy	B
in	O
contrast	O
to	O
a	O
cross-validation	B
measure	O
the	O
gradient	O
of	O
the	O
evidence	B
with	O
respect	O
to	O
the	O
control	O
parameters	B
can	O
be	O
evaluated	O
making	O
it	O
possible	O
to	O
simultaneously	O
optimize	O
a	O
large	O
number	O
of	O
control	O
parameters	B
probabilistic	O
modelling	B
also	O
handles	O
uncertainty	O
in	O
a	O
natural	B
manner	O
it	O
a	O
unique	O
prescription	O
marginalization	B
for	O
incorporating	O
uncertainty	O
about	O
parameters	B
into	O
predictions	O
this	O
procedure	O
yields	O
better	O
predictions	O
as	O
we	O
saw	O
in	O
chapter	O
figure	O
shows	O
error	B
bars	I
on	O
the	O
predictions	O
of	O
a	O
trained	O
neural	B
network	B
implementation	O
of	O
bayesian	B
inference	B
as	O
was	O
mentioned	O
in	O
chapter	O
bayesian	B
inference	B
for	O
multilayer	O
networks	O
may	O
be	O
implemented	O
by	O
monte	B
carlo	I
sampling	O
or	O
by	O
deterministic	B
methods	O
employing	O
gaussian	B
approximations	O
mackay	B
figure	O
error	B
bars	I
on	O
the	O
predictions	O
of	O
a	O
trained	O
regression	B
network	B
the	O
solid	O
line	O
gives	O
the	O
predictions	O
of	O
the	O
parameters	B
of	O
a	O
multilayer	B
perceptron	I
trained	O
on	O
the	O
data	O
points	O
the	O
error	B
bars	I
lines	O
are	O
those	O
produced	O
by	O
the	O
uncertainty	O
of	O
the	O
parameters	B
w	O
notice	O
that	O
the	O
error	B
bars	I
become	O
larger	O
where	O
the	O
data	O
are	O
sparse	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
supervised	O
learning	B
in	O
multilayer	O
networks	O
within	O
the	O
bayesian	B
framework	O
for	O
data	O
modelling	B
it	O
is	O
easy	O
to	O
improve	O
our	O
probabilistic	O
models	O
for	O
example	O
if	O
we	O
believe	O
that	O
some	O
input	O
variables	O
in	O
a	O
problem	O
may	O
be	O
irrelevant	O
to	O
the	O
predicted	O
quantity	O
but	O
we	O
don	O
t	O
know	O
which	O
we	O
can	O
a	O
new	O
model	B
with	O
multiple	O
hyperparameters	O
that	O
captures	O
the	O
idea	O
of	O
uncertain	O
input	O
variable	O
relevance	O
neal	B
mackay	B
these	O
models	O
then	O
infer	O
automatically	O
from	O
the	O
data	O
which	O
are	O
the	O
relevant	O
input	O
variables	O
for	O
a	O
problem	O
exercises	O
exercise	O
how	B
to	I
measure	I
a	O
s	O
quality	O
you	O
ve	O
just	O
written	O
a	O
new	O
algorithm	O
and	O
want	O
to	O
measure	O
how	O
well	O
it	O
performs	O
on	O
a	O
test	O
set	B
and	O
compare	O
it	O
with	O
other	O
what	O
performance	O
measure	O
should	O
you	O
use	O
there	O
are	O
several	O
standard	O
answers	O
let	O
s	O
assume	O
the	O
gives	O
an	O
output	O
yx	O
where	O
x	O
is	O
the	O
input	O
which	O
we	O
won	O
t	O
discuss	O
further	O
and	O
that	O
the	O
true	O
target	O
value	O
is	O
t	O
in	O
the	O
simplest	O
discussions	O
of	O
both	O
y	O
and	O
t	O
are	O
binary	O
variables	O
but	O
you	O
might	O
care	O
to	O
consider	O
cases	O
where	O
y	O
and	O
t	O
are	O
more	O
general	O
objects	O
also	O
the	O
most	O
widely	O
used	O
measure	O
of	O
performance	O
on	O
a	O
test	O
set	B
is	O
the	O
error	O
rate	B
the	O
fraction	O
of	O
made	O
by	O
the	O
this	O
measure	O
forces	O
the	O
to	O
give	O
a	O
output	O
and	O
ignores	O
any	O
additional	O
information	B
that	O
the	O
might	O
be	O
able	O
to	O
for	O
example	O
an	O
indication	O
of	O
the	O
of	O
a	O
prediction	B
unfortunately	O
the	O
error	O
rate	B
does	O
not	O
necessarily	O
measure	O
how	O
informative	O
a	O
s	O
output	O
is	O
consider	O
frequency	B
tables	O
showing	O
the	O
joint	B
frequency	B
of	O
the	O
output	O
of	O
a	O
axis	O
and	O
the	O
true	O
variable	O
axis	O
the	O
numbers	O
that	O
we	O
ll	O
show	O
are	O
percentages	O
the	O
error	O
rate	B
e	O
is	O
the	O
sum	O
of	O
the	O
two	O
numbers	O
which	O
we	O
could	O
call	O
the	O
false	O
positive	O
rate	B
e	O
and	O
the	O
false	O
negative	O
rate	B
of	O
the	O
following	O
three	O
a	O
and	O
b	O
have	O
the	O
same	O
error	O
rate	B
of	O
and	O
c	O
has	O
a	O
greater	O
error	O
rate	B
of	O
a	O
y	O
t	O
b	O
y	O
c	O
y	O
t	O
t	O
but	O
clearly	O
a	O
which	O
simply	O
guesses	O
that	O
the	O
outcome	O
is	O
for	O
all	O
cases	O
is	O
conveying	O
no	O
information	B
at	O
all	O
about	O
t	O
whereas	O
b	O
has	O
an	O
informative	O
output	O
if	O
y	O
then	O
we	O
are	O
sure	O
that	O
t	O
really	O
is	O
zero	O
and	O
if	O
y	O
then	O
there	O
is	O
a	O
chance	O
that	O
t	O
as	O
compared	O
to	O
the	O
prior	B
probability	B
p	O
c	O
is	O
slightly	O
less	O
informative	O
than	O
b	O
but	O
it	O
is	O
still	O
much	O
more	O
useful	O
than	O
the	O
information-free	O
a	O
one	O
way	O
to	O
improve	O
on	O
the	O
error	O
rate	B
as	O
a	O
performance	O
measure	O
is	O
to	O
report	O
the	O
pair	O
the	O
false	O
positive	O
error	O
rate	B
and	O
the	O
false	O
negative	O
error	O
rate	B
which	O
are	O
and	O
for	O
a	O
and	O
b	O
it	O
is	O
especially	O
important	O
to	O
distinguish	O
between	O
these	O
two	O
error	O
probabilities	O
in	O
applications	O
where	O
the	O
two	O
sorts	O
of	O
error	O
have	O
associated	O
costs	O
however	O
there	O
are	O
a	O
couple	O
of	O
problems	O
with	O
the	O
error	O
rate	B
pair	O
first	O
if	O
i	O
simply	O
told	O
you	O
that	O
a	O
has	O
error	O
rates	O
and	O
b	O
has	O
error	O
rates	O
it	O
would	O
not	O
be	O
immediately	O
evident	O
that	O
a	O
is	O
actually	O
utterly	O
worthless	O
surely	O
we	O
should	O
have	O
a	O
performance	O
measure	O
that	O
gives	O
the	O
worst	O
possible	O
score	O
to	O
a	O
how	O
common	O
sense	O
ranks	O
the	O
b	O
c	O
a	O
how	O
error	O
rate	B
ranks	O
the	O
a	O
b	O
c	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
exercises	O
second	O
if	O
we	O
turn	O
to	O
a	O
multiple-class	O
problem	O
such	O
as	O
digit	O
recognition	B
then	O
the	O
number	O
of	O
types	O
of	O
error	O
increases	O
from	O
two	O
to	O
one	O
for	O
each	O
possible	O
confusion	O
of	O
class	O
t	O
with	O
it	O
would	O
be	O
nice	O
to	O
have	O
some	O
sensible	O
way	O
of	O
collapsing	O
these	O
numbers	O
into	O
a	O
single	O
rankable	O
number	O
that	O
makes	O
more	O
sense	O
than	O
the	O
error	O
rate	B
another	O
reason	O
for	O
not	O
liking	O
the	O
error	O
rate	B
is	O
that	O
it	O
doesn	O
t	O
give	O
a	O
credit	O
for	O
accurately	O
specifying	O
its	O
uncertainty	O
consider	O
that	O
have	O
three	O
outputs	O
available	O
and	O
a	O
rejection	B
class	O
which	O
indicates	O
that	O
the	O
is	O
not	O
sure	O
consider	O
d	O
and	O
e	O
with	O
the	O
following	O
frequency	B
tables	O
in	O
percentages	O
d	O
y	O
t	O
e	O
y	O
t	O
both	O
of	O
these	O
have	O
r	O
but	O
are	O
they	O
equally	O
good	B
compare	O
e	O
with	O
c	O
the	O
two	O
are	O
equivalent	O
e	O
is	O
just	O
c	O
in	O
disguise	O
we	O
could	O
make	O
e	O
by	O
taking	O
the	O
output	O
of	O
c	O
and	O
tossing	O
a	O
coin	B
when	O
c	O
says	O
in	O
order	O
to	O
decide	O
whether	O
to	O
give	O
output	O
or	O
so	O
e	O
is	O
equal	O
to	O
c	O
and	O
thus	O
inferior	O
to	O
b	O
now	O
compare	O
d	O
with	O
b	O
can	O
you	O
justify	O
the	O
suggestion	O
that	O
d	O
is	O
a	O
more	O
informative	O
than	O
b	O
and	O
thus	O
is	O
superior	O
to	O
e	O
yet	O
d	O
and	O
e	O
have	O
the	O
same	O
r	O
scores	O
people	O
often	O
plot	O
error-reject	B
curves	I
known	O
as	O
roc	B
curves	O
roc	B
stands	O
for	O
receiver	B
operating	I
characteristic	I
which	O
show	O
the	O
total	O
e	O
versus	O
r	O
as	O
r	O
is	O
allowed	O
to	O
vary	O
from	O
to	O
and	O
use	O
these	O
curves	O
to	O
compare	O
the	O
special	O
case	O
of	O
binary	O
problems	O
e	O
may	O
be	O
plotted	O
versus	O
instead	O
but	O
as	O
we	O
have	O
seen	O
error	O
rates	O
can	O
be	O
undiscerning	O
performance	O
measures	O
does	O
plotting	O
one	O
error	O
rate	B
as	O
a	O
function	O
of	O
another	O
make	O
this	O
weakness	B
of	I
error	O
rates	O
go	O
away	O
for	O
this	O
exercise	O
either	O
construct	O
an	O
explicit	O
example	O
demonstrating	O
that	O
the	O
error-reject	O
curve	O
and	O
the	O
area	O
under	O
it	O
are	O
not	O
necessarily	O
good	B
ways	O
to	O
compare	O
or	O
prove	O
that	O
they	O
are	O
as	O
a	O
suggested	O
alternative	O
method	O
for	O
comparing	O
consider	O
the	O
mutual	B
information	B
between	O
the	O
output	O
and	O
the	O
target	O
it	O
y	O
ht	O
ht	O
j	O
y	O
p	O
y	O
log	O
p	O
p	O
y	O
which	O
measures	O
how	O
many	O
bits	O
the	O
s	O
output	O
conveys	O
about	O
the	O
target	O
evaluate	O
the	O
mutual	B
information	B
for	O
ae	O
above	O
investigate	O
this	O
performance	O
measure	O
and	O
discuss	O
whether	O
it	O
is	O
a	O
useful	O
one	O
does	O
it	O
have	O
practical	B
drawbacks	O
error	O
rate	B
rejection	B
rate	B
figure	O
an	O
error-reject	O
curve	O
some	O
people	O
use	O
the	O
area	O
under	O
this	O
curve	O
as	O
a	O
measure	O
of	O
quality	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
feedforward	O
neural	O
networks	O
such	O
as	O
multilayer	O
perceptrons	O
are	O
popular	O
tools	O
for	O
nonlinear	B
regression	B
and	O
problems	O
from	O
a	O
bayesian	B
perspective	O
a	O
choice	O
of	O
a	O
neural	B
network	B
model	B
can	O
be	O
viewed	O
as	O
a	O
prior	B
probability	B
distribution	B
over	O
nonlinear	B
functions	B
and	O
the	O
neural	B
network	B
s	O
learning	B
process	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
the	O
posterior	B
probability	B
distribution	B
over	O
the	O
unknown	O
function	O
learning	B
algorithms	B
search	O
for	O
the	O
function	O
with	O
maximum	O
posterior	B
probability	B
and	O
other	O
monte	B
carlo	I
methods	I
draw	O
samples	O
from	O
this	O
posterior	B
probability	B
in	O
the	O
limit	O
of	O
large	O
but	O
otherwise	O
standard	O
networks	O
neal	B
has	O
shown	O
that	O
the	O
prior	B
distribution	B
over	O
nonlinear	B
functions	B
implied	O
by	O
the	O
bayesian	B
neural	B
network	B
falls	O
in	O
a	O
class	O
of	O
probability	B
distributions	O
known	O
as	O
gaussian	B
processes	I
the	O
hyperparameters	O
of	O
the	O
neural	B
network	B
model	B
determine	O
the	O
characteristic	O
lengthscales	O
of	O
the	O
gaussian	B
process	O
neal	B
s	O
observation	O
motivates	O
the	O
idea	O
of	O
discarding	O
parameterized	O
networks	O
and	O
working	O
directly	O
with	O
gaussian	B
processes	I
computations	O
in	O
which	O
the	O
parameters	B
of	O
the	O
network	B
are	O
optimized	O
are	O
then	O
replaced	O
by	O
simple	O
matrix	B
operations	O
using	O
the	O
covariance	B
matrix	B
of	O
the	O
gaussian	B
process	O
in	O
this	O
chapter	O
i	O
will	O
review	O
work	O
on	O
this	O
idea	O
by	O
williams	O
and	O
rasmussen	O
neal	B
barber	O
and	O
williams	O
and	O
gibbs	B
and	O
mackay	B
and	O
will	O
assess	O
whether	O
for	O
supervised	O
regression	B
and	O
tasks	O
the	O
feedforward	O
network	B
has	O
been	O
superceded	O
exercise	O
i	O
regret	B
that	O
this	O
chapter	O
is	O
rather	O
dry	O
there	O
s	O
no	O
simple	O
explanatory	O
examples	O
in	O
it	O
and	O
few	O
pictures	O
this	O
exercise	O
asks	O
you	O
to	O
create	O
interesting	O
pictures	O
to	O
explain	O
to	O
yourself	O
this	O
chapter	O
s	O
ideas	O
source	O
code	O
for	O
computer	B
demonstrations	O
written	O
in	O
the	O
free	O
language	O
octave	B
is	O
available	O
at	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitprnnsoftware	O
html	O
radford	O
neal	B
s	O
software	B
for	O
gaussian	B
processes	I
is	O
available	O
at	O
httpwww	O
cs	O
toronto	O
eduradford	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gaussian	B
processes	I
after	O
the	O
publication	O
of	O
rumelhart	O
hinton	B
and	O
williams	O
s	O
paper	O
on	O
supervised	O
learning	B
in	O
neural	O
networks	O
there	O
was	O
a	O
surge	O
of	O
interest	O
in	O
the	O
empirical	O
modelling	B
of	O
relationships	O
in	O
high-dimensional	O
data	O
using	O
nonlinear	B
parametric	O
models	O
such	O
as	O
multilayer	O
perceptrons	O
and	O
radial	O
basis	O
functions	B
in	O
the	O
bayesian	B
interpretation	O
of	O
these	O
modelling	B
methods	O
a	O
nonlinear	B
function	O
yx	O
parameterized	O
by	O
parameters	B
w	O
is	O
assumed	O
to	O
underlie	O
the	O
data	O
fxn	O
tngn	O
and	O
the	O
adaptation	O
of	O
the	O
model	B
to	O
the	O
data	O
corresponds	O
to	O
an	O
inference	B
of	O
the	O
function	O
given	O
the	O
data	O
we	O
will	O
denote	O
the	O
set	B
of	O
input	O
vectors	B
by	O
xn	O
fxngn	O
and	O
the	O
set	B
of	O
corresponding	O
target	O
values	O
by	O
the	O
vector	O
tn	O
ftngn	O
the	O
inference	B
of	O
yx	O
is	O
described	O
by	O
the	O
posterior	B
probability	B
distribution	B
p	O
tn	O
xn	O
p	O
j	O
yx	O
xn	O
p	O
j	O
xn	O
of	O
the	O
two	O
terms	O
on	O
the	O
right-hand	O
side	O
the	O
p	O
j	O
yx	O
xn	O
is	O
the	O
probability	B
of	O
the	O
target	O
values	O
given	O
the	O
function	O
yx	O
which	O
in	O
the	O
case	O
of	O
regression	B
problems	O
is	O
often	O
assumed	O
to	O
be	O
a	O
separable	O
gaussian	B
distribution	B
and	O
the	O
second	O
term	O
p	O
is	O
the	O
prior	B
distribution	B
on	O
functions	B
assumed	O
by	O
the	O
model	B
this	O
prior	B
is	O
implicit	O
in	O
the	O
choice	O
of	O
parametric	O
model	B
and	O
the	O
choice	O
of	O
regularizers	O
used	O
during	O
the	O
model	B
the	O
prior	B
typically	O
that	O
the	O
function	O
yx	O
is	O
expected	O
to	O
be	O
continuous	B
and	O
smooth	O
and	O
has	O
less	O
high	O
frequency	B
power	O
than	O
low	O
frequency	B
power	O
but	O
the	O
precise	O
meaning	O
of	O
the	O
prior	B
is	O
somewhat	O
obscured	O
by	O
the	O
use	O
of	O
the	O
parametric	O
model	B
now	O
for	O
the	O
prediction	B
of	O
future	O
values	O
of	O
t	O
all	O
that	O
matters	O
is	O
the	O
assumed	O
prior	B
p	O
and	O
the	O
assumed	O
noise	O
model	B
p	O
j	O
yx	O
xn	O
the	O
parameterization	O
of	O
the	O
function	O
yx	O
w	O
is	O
irrelevant	O
the	O
idea	O
of	O
gaussian	B
process	O
modelling	B
is	O
to	O
place	O
a	O
prior	B
p	O
directly	O
on	O
the	O
space	O
of	O
functions	B
without	O
parameterizing	O
yx	O
the	O
simplest	O
type	O
of	O
prior	B
over	O
functions	B
is	O
called	O
a	O
gaussian	B
process	O
it	O
can	O
be	O
thought	O
of	O
as	O
the	O
generalization	B
of	O
a	O
gaussian	B
distribution	B
over	O
a	O
vector	O
space	O
to	O
a	O
function	O
space	O
of	O
dimension	O
just	O
as	O
a	O
gaussian	B
distribution	B
is	O
fully	O
by	O
its	O
mean	B
and	O
covariance	B
matrix	B
a	O
gaussian	B
process	O
is	O
by	O
a	O
mean	B
and	O
a	O
covariance	B
function	I
here	O
the	O
mean	B
is	O
a	O
function	O
of	O
x	O
we	O
will	O
often	O
take	O
to	O
be	O
the	O
zero	O
function	O
and	O
the	O
covariance	B
is	O
a	O
function	O
cx	O
that	O
expresses	O
the	O
expected	O
covariance	B
between	O
the	O
values	O
of	O
the	O
function	O
y	O
at	O
the	O
points	O
x	O
and	O
the	O
function	O
yx	O
in	O
any	O
one	O
data	O
modelling	B
problem	O
is	O
assumed	O
to	O
be	O
a	O
single	O
sample	B
from	I
this	O
gaussian	B
distribution	B
gaussian	B
processes	I
are	O
already	O
well	O
established	O
models	O
for	O
various	O
spatial	O
and	O
temporal	O
problems	O
for	O
example	O
brownian	B
motion	I
langevin	O
processes	O
and	O
wiener	B
processes	O
are	O
all	O
examples	O
of	O
gaussian	B
processes	I
kalman	O
widely	O
used	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gaussian	B
processes	I
to	O
model	B
speech	O
waveforms	O
also	O
correspond	O
to	O
gaussian	B
process	O
models	O
the	O
method	O
of	O
kriging	B
in	O
geostatistics	B
is	O
a	O
gaussian	B
process	O
regression	B
method	O
reservations	O
about	O
gaussian	B
processes	I
it	O
might	O
be	O
thought	O
that	O
it	O
is	O
not	O
possible	O
to	O
reproduce	O
the	O
interesting	O
properties	O
of	O
neural	B
network	B
interpolation	O
methods	O
with	O
something	O
so	O
simple	O
as	O
a	O
gaussian	B
distribution	B
but	O
as	O
we	O
shall	O
now	O
see	O
many	O
popular	O
nonlinear	B
interpolation	O
methods	O
are	O
equivalent	O
to	O
particular	O
gaussian	B
processes	I
use	O
the	O
term	O
interpolation	O
to	O
cover	B
both	O
the	O
problem	O
of	O
regression	B
a	O
curve	O
through	O
noisy	B
data	O
and	O
the	O
task	O
of	O
an	O
interpolant	O
that	O
passes	O
exactly	O
through	O
the	O
given	O
data	O
points	O
it	O
might	O
also	O
be	O
thought	O
that	O
the	O
computational	O
complexity	B
of	O
inference	B
when	O
we	O
work	O
with	O
priors	O
over	O
function	O
spaces	O
might	O
be	O
large	O
but	O
by	O
concentrating	O
on	O
the	O
joint	B
probability	B
distribution	B
of	O
the	O
observed	O
data	O
and	O
the	O
quantities	O
we	O
wish	O
to	O
predict	O
it	O
is	O
possible	O
to	O
make	O
predictions	O
with	O
resources	O
that	O
scale	O
as	O
polynomial	O
functions	B
of	O
n	O
the	O
number	O
of	O
data	O
points	O
standard	O
methods	O
for	O
nonlinear	B
regression	B
the	O
problem	O
we	O
are	O
given	O
n	O
data	O
points	O
xn	O
tn	O
fxn	O
tngn	O
the	O
inputs	O
x	O
are	O
vectors	B
of	O
some	O
input	O
dimension	O
i	O
the	O
targets	O
t	O
are	O
either	O
real	O
numbers	O
in	O
which	O
case	O
the	O
task	O
will	O
be	O
a	O
regression	B
or	O
interpolation	O
task	O
or	O
they	O
are	O
categorical	O
variables	O
for	O
example	O
t	O
in	O
which	O
case	O
the	O
task	O
is	O
a	O
task	O
we	O
will	O
concentrate	O
on	O
the	O
case	O
of	O
regression	B
for	O
the	O
time	O
being	O
assuming	O
that	O
a	O
function	O
yx	O
underlies	O
the	O
observed	O
data	O
the	O
task	O
is	O
to	O
infer	O
the	O
function	O
from	O
the	O
given	O
data	O
and	O
predict	O
the	O
function	O
s	O
value	O
or	O
the	O
value	O
of	O
the	O
observation	O
tn	O
at	O
a	O
new	O
point	O
xn	O
parametric	O
approaches	O
to	O
the	O
problem	O
in	O
a	O
parametric	O
approach	O
to	O
regression	B
we	O
express	O
the	O
unknown	O
function	O
yx	O
in	O
terms	O
of	O
a	O
nonlinear	B
function	O
yx	O
w	O
parameterized	O
by	O
parameters	B
w	O
example	O
fixed	O
basis	O
functions	B
using	O
a	O
set	B
of	O
basis	O
functions	B
we	O
can	O
write	O
h	O
yx	O
w	O
if	O
the	O
basis	O
functions	B
are	O
nonlinear	B
functions	B
of	O
x	O
such	O
as	O
radial	O
basis	O
functions	B
centred	O
at	O
points	O
fchgh	O
then	O
yx	O
w	O
is	O
a	O
nonlinear	B
function	O
of	O
x	O
however	O
since	O
the	O
dependence	O
of	O
y	O
on	O
the	O
parameters	B
w	O
is	O
linear	B
we	O
might	O
sometimes	O
refer	O
to	O
this	O
as	O
a	O
linear	B
model	B
in	O
neural	B
network	B
terms	O
this	O
model	B
is	O
like	O
a	O
multilayer	O
network	B
whose	O
connections	O
from	O
the	O
input	O
layer	O
to	O
the	O
nonlinear	B
hidden	O
layer	O
are	O
only	O
the	O
output	O
weights	O
w	O
are	O
adaptive	B
other	O
possible	O
sets	O
of	O
basis	O
functions	B
include	O
polynomials	O
such	O
as	O
xp	O
j	O
where	O
p	O
and	O
q	O
are	O
integer	O
powers	O
that	O
depend	O
on	O
h	O
i	O
xq	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
standard	O
methods	O
for	O
nonlinear	B
regression	B
example	O
adaptive	B
basis	O
functions	B
alternatively	O
we	O
might	O
make	O
a	O
function	O
yx	O
from	O
basis	O
functions	B
that	O
depend	O
on	O
additional	O
parameters	B
included	O
in	O
the	O
vector	O
w	O
in	O
a	O
two-layer	O
feedforward	O
neural	B
network	B
with	O
nonlinear	B
hidden	O
units	B
and	O
a	O
linear	B
output	O
the	O
function	O
can	O
be	O
written	O
yx	O
w	O
h	O
h	O
tanh	O
i	O
hi	O
xi	O
where	O
i	O
is	O
the	O
dimensionality	O
of	O
the	O
input	O
space	O
and	O
the	O
weight	O
vector	O
hi	O
g	O
the	O
hidden	O
unit	O
biases	O
g	O
in	O
this	O
model	B
the	O
w	O
consists	O
of	O
the	O
input	O
weights	O
the	O
output	O
weights	O
h	O
g	O
and	O
the	O
output	O
bias	B
dependence	O
of	O
y	O
on	O
w	O
is	O
nonlinear	B
having	O
chosen	O
the	O
parameterization	O
we	O
then	O
infer	O
the	O
function	O
yx	O
w	O
by	O
inferring	O
the	O
parameters	B
w	O
the	O
posterior	B
probability	B
of	O
the	O
parameters	B
is	O
p	O
j	O
tn	O
xn	O
p	O
j	O
w	O
xn	O
p	O
j	O
xn	O
the	O
factor	O
p	O
j	O
w	O
xn	O
states	O
the	O
probability	B
of	O
the	O
observed	O
data	O
points	O
when	O
the	O
parameters	B
w	O
hence	O
the	O
function	O
y	O
are	O
known	O
this	O
probability	B
distribution	B
is	O
often	O
taken	O
to	O
be	O
a	O
separable	O
gaussian	B
each	O
data	O
point	O
tn	O
from	O
the	O
underlying	O
value	O
yxn	O
w	O
by	O
additive	O
noise	O
the	O
factor	O
p	O
the	O
prior	B
probability	B
distribution	B
of	O
the	O
parameters	B
this	O
too	O
is	O
often	O
taken	O
to	O
be	O
a	O
separable	O
gaussian	B
distribution	B
if	O
the	O
dependence	O
of	O
y	O
on	O
w	O
is	O
nonlinear	B
the	O
posterior	O
distribution	B
p	O
j	O
tn	O
xn	O
is	O
in	O
general	O
not	O
a	O
gaussian	B
distribution	B
the	O
inference	B
can	O
be	O
implemented	O
in	O
various	O
ways	O
in	O
the	O
laplace	B
method	O
we	O
minimize	O
an	O
objective	B
function	I
m	O
ln	O
j	O
w	O
xn	O
with	O
respect	O
to	O
w	O
locating	O
the	O
locally	O
most	O
probable	O
parameters	B
then	O
use	O
the	O
curvature	O
of	O
m	O
to	O
error	B
bars	I
on	O
w	O
alternatively	O
we	O
can	O
use	O
more	O
general	O
markov	B
chain	I
monte	B
carlo	I
techniques	O
to	O
create	O
samples	O
from	O
the	O
posterior	O
distribution	B
p	O
j	O
tn	O
xn	O
having	O
obtained	O
one	O
of	O
these	O
representations	O
of	O
the	O
inference	B
of	O
w	O
given	O
the	O
data	O
predictions	O
are	O
then	O
made	O
by	O
marginalizing	O
over	O
the	O
parameters	B
p	O
j	O
tn	O
xn	O
dhw	O
p	O
j	O
w	O
xn	O
j	O
tn	O
xn	O
if	O
we	O
have	O
a	O
gaussian	B
representation	O
of	O
the	O
posterior	O
p	O
j	O
tn	O
xn	O
then	O
this	O
integral	B
can	O
typically	O
be	O
evaluated	O
directly	O
in	O
the	O
alternative	O
monte	B
carlo	I
approach	O
which	O
generates	O
r	O
samples	O
wr	O
that	O
are	O
intended	O
to	O
be	O
samples	O
from	O
the	O
posterior	O
distribution	B
p	O
j	O
tn	O
xn	O
we	O
approximate	O
the	O
predictive	B
distribution	B
by	O
p	O
j	O
tn	O
xn	O
r	O
r	O
p	O
j	O
wr	O
xn	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gaussian	B
processes	I
nonparametric	B
approaches	O
in	O
nonparametric	B
methods	O
predictions	O
are	O
obtained	O
without	O
explicitly	O
parameterizing	O
the	O
unknown	O
function	O
yx	O
yx	O
lives	O
in	O
the	O
space	O
of	O
all	O
continuous	B
functions	B
of	O
x	O
one	O
well	O
known	O
nonparametric	B
approach	O
to	O
the	O
regression	B
problem	O
is	O
the	O
spline	B
smoothing	O
method	O
and	O
wahba	O
a	O
spline	B
solution	O
to	O
a	O
one-dimensional	O
regression	B
problem	O
can	O
be	O
described	O
as	O
follows	O
we	O
the	O
estimator	B
of	O
yx	O
to	O
be	O
the	O
function	O
that	O
minimizes	O
the	O
functional	O
m	O
n	O
dx	O
where	O
yp	O
is	O
the	O
pth	O
derivative	O
of	O
y	O
and	O
p	O
is	O
a	O
positive	O
number	O
if	O
p	O
is	O
set	B
to	O
then	O
the	O
resulting	O
function	O
is	O
a	O
cubic	O
spline	B
that	O
is	O
a	O
piecewise	O
cubic	O
function	O
that	O
has	O
knots	O
discontinuities	O
in	O
its	O
second	O
derivative	O
at	O
the	O
data	O
points	O
fxng	O
tifying	O
the	O
prior	B
for	O
the	O
function	O
yx	O
as	O
this	O
estimation	O
method	O
can	O
be	O
interpreted	O
as	O
a	O
bayesian	B
method	O
by	O
iden	O
ln	O
p	O
dx	O
const	O
and	O
the	O
probability	B
of	O
the	O
data	O
measurements	O
tn	O
ftngn	O
pendent	O
gaussian	B
noise	O
as	O
assuming	O
inde	O
ln	O
p	O
tn	O
j	O
yx	O
n	O
const	O
constants	O
in	O
equations	O
and	O
are	O
functions	B
of	O
and	O
respectively	O
strictly	O
the	O
prior	B
is	O
improper	B
since	O
addition	O
of	O
an	O
arbitrary	O
polynomial	O
of	O
degree	B
to	O
yx	O
is	O
not	O
constrained	B
this	O
impropriety	O
is	O
easily	O
by	O
the	O
addition	O
of	O
appropriate	O
terms	O
to	O
given	O
this	O
interpretation	O
of	O
the	O
functions	B
in	O
equation	O
m	O
is	O
equal	O
to	O
minus	O
the	O
log	O
of	O
the	O
posterior	B
probability	B
p	O
tn	O
within	O
an	O
additive	O
constant	O
and	O
the	O
splines	O
estimation	O
procedure	O
can	O
be	O
interpreted	O
as	O
yielding	O
a	O
bayesian	B
map	O
estimate	O
the	O
bayesian	B
perspective	O
allows	O
us	O
additionally	O
to	O
put	O
error	B
bars	I
on	O
the	O
splines	O
estimate	O
and	O
to	O
draw	O
typical	B
samples	O
from	O
the	O
posterior	O
distribution	B
and	O
it	O
gives	O
an	O
automatic	O
method	O
for	O
inferring	O
the	O
hyperparameters	O
and	O
comments	O
splines	O
priors	O
are	O
gaussian	B
processes	I
the	O
prior	B
distribution	B
in	O
equation	O
is	O
our	O
example	O
of	O
a	O
gaussian	B
process	O
throwing	O
mathematical	O
precision	B
to	O
the	O
winds	O
a	O
gaussian	B
process	O
can	O
be	O
as	O
a	O
probability	B
distribution	B
on	O
a	O
space	O
of	O
functions	B
yx	O
that	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
a	O
z	O
where	O
is	O
the	O
mean	B
function	O
and	O
a	O
is	O
a	O
linear	B
operator	O
and	O
where	O
the	O
inner	O
product	O
of	O
two	O
functions	B
yxtzx	O
is	O
by	O
for	O
example	O
r	O
dx	O
yxzx	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
standard	O
methods	O
for	O
nonlinear	B
regression	B
here	O
if	O
we	O
denote	O
by	O
d	O
the	O
linear	B
operator	O
that	O
maps	O
yx	O
to	O
the	O
derivative	O
of	O
yx	O
we	O
can	O
write	O
equation	O
as	O
ln	O
p	O
dx	O
const	O
yxtayx	O
const	O
which	O
has	O
the	O
same	O
form	O
as	O
equation	O
with	O
and	O
a	O
ptdp	O
in	O
order	O
for	O
the	O
prior	B
in	O
equation	O
to	O
be	O
a	O
proper	B
prior	B
a	O
must	O
be	O
a	O
positive	O
operator	O
i	O
e	O
one	O
satisfying	O
yxtayx	O
for	O
all	O
functions	B
yx	O
other	O
than	O
yx	O
splines	O
can	O
be	O
written	O
as	O
parametric	O
models	O
splines	O
may	O
be	O
written	O
in	O
terms	O
of	O
an	O
set	B
of	O
basis	O
functions	B
as	O
in	O
equation	O
as	O
follows	O
first	O
rescale	O
the	O
x	O
axis	O
so	O
that	O
the	O
interval	O
is	O
much	O
wider	O
than	O
the	O
range	O
of	O
x	O
values	O
of	O
interest	O
let	O
the	O
basis	O
functions	B
be	O
a	O
fourier	O
set	B
fcos	O
hx	O
sin	O
hx	O
h	O
so	O
the	O
function	O
is	O
whcos	O
coshx	O
whsin	O
sinhx	O
yx	O
use	O
the	O
regularizer	O
ew	O
h	O
to	O
a	O
gaussian	B
prior	B
on	O
w	O
p	O
hcos	O
h	O
p	O
hsin	O
p	O
j	O
zw	O
if	O
p	O
then	O
we	O
have	O
the	O
cubic	O
splines	O
regularizer	O
ew	O
dx	O
as	O
if	O
p	O
we	O
have	O
the	O
regularizer	O
ew	O
dx	O
in	O
equation	O
make	O
the	O
prior	B
proper	B
we	O
must	O
add	O
an	O
extra	O
regularizer	O
on	O
the	O
etc	O
term	O
thus	O
in	O
terms	O
of	O
the	O
prior	B
p	O
there	O
is	O
no	O
fundamental	O
between	O
the	O
nonparametric	B
splines	O
approach	O
and	O
other	O
parametric	O
approaches	O
representation	O
is	O
irrelevant	O
for	O
prediction	B
from	O
the	O
point	O
of	O
view	O
of	O
prediction	B
at	O
least	O
there	O
are	O
two	O
objects	O
of	O
interest	O
the	O
is	O
the	O
conditional	B
distribution	B
p	O
j	O
tn	O
xn	O
in	O
equation	O
the	O
other	O
object	O
of	O
interest	O
should	O
we	O
wish	O
to	O
compare	O
one	O
model	B
with	O
others	O
is	O
the	O
joint	B
probability	B
of	O
all	O
the	O
observed	O
data	O
given	O
the	O
model	B
the	O
evidence	B
p	O
j	O
xn	O
which	O
appeared	O
as	O
the	O
normalizing	O
constant	O
in	O
equation	O
neither	O
of	O
these	O
quantities	O
makes	O
any	O
reference	O
to	O
the	O
representation	O
of	O
the	O
unknown	O
function	O
yx	O
so	O
at	O
the	O
end	O
of	O
the	O
day	O
our	O
choice	O
of	O
representation	O
is	O
irrelevant	O
the	O
question	O
we	O
now	O
address	B
is	O
in	O
the	O
case	O
of	O
popular	O
parametric	O
models	O
what	O
form	O
do	O
these	O
two	O
quantities	O
take	O
we	O
will	O
see	O
that	O
for	O
standard	O
models	O
with	O
basis	O
functions	B
and	O
gaussian	B
distributions	O
on	O
the	O
unknown	O
parameters	B
the	O
joint	B
probability	B
of	O
all	O
the	O
observed	O
data	O
given	O
the	O
model	B
p	O
j	O
xn	O
is	O
a	O
multivariate	B
gaussian	B
distribution	B
with	O
mean	B
zero	O
and	O
with	O
a	O
covariance	B
matrix	B
determined	O
by	O
the	O
basis	O
functions	B
this	O
implies	O
that	O
the	O
conditional	B
distribution	B
p	O
j	O
tn	O
xn	O
is	O
also	O
a	O
gaussian	B
distribution	B
whose	O
mean	B
depends	O
linearly	O
on	O
the	O
values	O
of	O
the	O
targets	O
tn	O
standard	O
parametric	O
models	O
are	O
simple	O
examples	O
of	O
gaussian	B
processes	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gaussian	B
processes	I
from	O
parametric	O
models	O
to	O
gaussian	B
processes	I
linear	B
models	O
let	O
us	O
consider	O
a	O
regression	B
problem	O
using	O
h	O
basis	O
functions	B
for	O
example	O
one-dimensional	O
radial	O
basis	O
functions	B
as	O
in	O
equation	O
let	O
us	O
assume	O
that	O
a	O
list	O
of	O
n	O
input	O
points	O
fxng	O
has	O
been	O
and	O
the	O
n	O
h	O
matrix	B
r	O
to	O
be	O
the	O
matrix	B
of	O
values	O
of	O
the	O
basis	O
functions	B
at	O
the	O
points	O
fxng	O
rnh	O
we	O
the	O
vector	O
yn	O
to	O
be	O
the	O
vector	O
of	O
values	O
of	O
yx	O
at	O
the	O
n	O
points	O
yn	O
rnhwh	O
if	O
the	O
prior	B
distribution	B
of	O
w	O
is	O
gaussian	B
with	O
zero	O
mean	B
p	O
normalw	O
wi	O
then	O
y	O
being	O
a	O
linear	B
function	O
of	O
w	O
is	O
also	O
gaussian	B
distributed	O
with	O
mean	B
zero	O
the	O
covariance	B
matrix	B
of	O
y	O
is	O
q	O
hyyti	O
hrwwtrti	O
rhwwti	O
rt	O
wrrt	O
so	O
the	O
prior	B
distribution	B
of	O
y	O
is	O
p	O
normaly	O
q	O
normaly	O
wrrt	O
this	O
result	O
that	O
the	O
vector	O
of	O
n	O
function	O
values	O
y	O
has	O
a	O
gaussian	B
distribution	B
is	O
true	O
for	O
any	O
selected	O
points	O
xn	O
this	O
is	O
the	O
property	O
of	O
a	O
gaussian	B
process	O
the	O
probability	B
distribution	B
of	O
a	O
function	O
yx	O
is	O
a	O
gaussian	B
process	O
if	O
for	O
any	O
selection	O
of	O
points	O
xn	O
the	O
density	B
p	O
yxn	O
is	O
a	O
gaussian	B
now	O
if	O
the	O
number	O
of	O
basis	O
functions	B
h	O
is	O
smaller	O
than	O
the	O
number	O
of	O
data	O
points	O
n	O
then	O
the	O
matrix	B
q	O
will	O
not	O
have	O
full	O
rank	O
in	O
this	O
case	O
the	O
probability	B
distribution	B
of	O
y	O
might	O
be	O
thought	O
of	O
as	O
a	O
elliptical	O
pancake	O
to	O
an	O
h-dimensional	O
subspace	O
in	O
the	O
n	O
space	O
in	O
which	O
y	O
lives	O
what	O
about	O
the	O
target	O
values	O
if	O
each	O
target	O
tn	O
is	O
assumed	O
to	O
by	O
from	O
the	O
corresponding	O
function	O
value	O
additive	O
gaussian	B
noise	O
of	O
variance	B
yn	O
then	O
t	O
also	O
has	O
a	O
gaussian	B
prior	B
distribution	B
p	O
normalt	O
q	O
we	O
will	O
denote	O
the	O
covariance	B
matrix	B
of	O
t	O
by	O
c	O
c	O
q	O
wrrt	O
whether	O
or	O
not	O
q	O
has	O
full	O
rank	O
the	O
covariance	B
matrix	B
c	O
has	O
full	O
rank	O
since	O
is	O
full	O
rank	O
what	O
does	O
the	O
covariance	B
matrix	B
q	O
look	O
like	O
in	O
general	O
the	O
entry	O
of	O
q	O
is	O
wxh	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
from	O
parametric	O
models	O
to	O
gaussian	B
processes	I
and	O
the	O
entry	O
of	O
c	O
is	O
wxh	O
where	O
if	O
n	O
and	O
otherwise	O
example	O
let	O
s	O
take	O
as	O
an	O
example	O
a	O
one-dimensional	O
case	O
with	O
radial	O
basis	O
functions	B
the	O
expression	O
for	O
becomes	O
simplest	O
if	O
we	O
assume	O
we	O
have	O
uniformly-spaced	O
basis	O
functions	B
with	O
the	O
basis	O
function	O
labelled	O
h	O
centred	O
on	O
the	O
point	O
x	O
h	O
and	O
take	O
the	O
limit	O
h	O
so	O
that	O
the	O
sum	O
over	O
h	O
becomes	O
an	O
integral	B
to	O
avoid	O
having	O
a	O
covariance	B
that	O
diverges	O
with	O
h	O
we	O
had	O
better	O
make	O
w	O
scale	O
as	O
where	O
is	O
the	O
number	O
of	O
basis	O
functions	B
per	O
unit	O
length	O
of	O
the	O
x-axis	O
and	O
s	O
is	O
a	O
constant	O
then	O
sz	O
hmax	O
sz	O
hmax	O
hmin	O
hmin	O
if	O
we	O
let	O
the	O
limits	O
of	O
integration	O
be	O
we	O
can	O
solve	O
this	O
integral	B
dh	O
dh	O
s	O
we	O
are	O
arriving	O
at	O
a	O
new	O
perspective	O
on	O
the	O
interpolation	O
problem	O
instead	O
of	O
specifying	O
the	O
prior	B
distribution	B
on	O
functions	B
in	O
terms	O
of	O
basis	O
functions	B
and	O
priors	O
on	O
parameters	B
the	O
prior	B
can	O
be	O
summarized	O
simply	O
by	O
a	O
covariance	B
function	I
cxn	O
where	O
we	O
have	O
given	O
a	O
new	O
name	O
to	O
the	O
constant	O
out	O
front	O
generalizing	O
from	O
this	O
particular	O
case	O
a	O
vista	O
of	O
interpolation	O
methods	O
opens	O
up	O
given	O
any	O
valid	O
covariance	B
function	I
cx	O
we	O
ll	O
discuss	O
in	O
a	O
moment	O
what	O
valid	O
means	O
we	O
can	O
the	O
covariance	B
matrix	B
for	O
n	O
function	O
values	O
at	O
locations	O
xn	O
to	O
be	O
the	O
matrix	B
q	O
given	O
by	O
cxn	O
and	O
the	O
covariance	B
matrix	B
for	O
n	O
corresponding	O
target	O
values	O
assuming	O
gaussian	B
noise	O
to	O
be	O
the	O
matrix	B
c	O
given	O
by	O
cxn	O
in	O
conclusion	O
the	O
prior	B
probability	B
of	O
the	O
n	O
target	O
values	O
t	O
in	O
the	O
data	B
set	B
is	O
p	O
normalt	O
c	O
z	O
samples	O
from	O
this	O
gaussian	B
process	O
and	O
a	O
few	O
other	O
simple	O
gaussian	B
processes	I
are	O
displayed	O
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
t	O
t	O
x	O
t	O
t	O
x	O
gaussian	B
processes	I
figure	O
samples	O
drawn	O
from	B
gaussian	B
process	O
priors	O
each	O
panel	O
shows	O
two	O
functions	B
drawn	O
from	O
a	O
gaussian	B
process	O
prior	B
the	O
four	O
corresponding	O
covariance	B
functions	B
are	O
given	O
below	O
each	O
plot	O
the	O
decrease	O
in	O
lengthscale	O
from	O
to	O
produces	O
more	O
rapidly	O
functions	B
the	O
periodic	O
properties	O
of	O
the	O
covariance	B
function	I
in	O
can	O
be	O
seen	O
the	O
covariance	B
function	I
in	O
contains	O
the	O
non-stationary	O
term	O
corresponding	O
to	O
the	O
covariance	B
of	O
a	O
straight	O
line	O
so	O
that	O
typical	B
functions	B
include	O
linear	B
trends	O
from	O
gibbs	B
x	O
x	O
multilayer	O
neural	O
networks	O
and	O
gaussian	B
processes	I
figures	O
and	O
show	O
some	O
random	B
samples	O
from	O
the	O
prior	B
distribution	B
over	O
functions	B
by	O
a	O
selection	O
of	O
standard	O
multilayer	O
perceptrons	O
with	O
large	O
numbers	O
of	O
hidden	O
units	B
those	O
samples	O
don	O
t	O
seem	O
a	O
million	O
miles	O
away	O
from	O
the	O
gaussian	B
process	O
samples	O
of	O
and	O
indeed	O
neal	B
showed	O
that	O
the	O
properties	O
of	O
a	O
neural	B
network	B
with	O
one	O
hidden	O
layer	O
in	O
equation	O
converge	O
to	O
those	O
of	O
a	O
gaussian	B
process	O
as	O
the	O
number	O
of	O
hidden	B
neurons	I
tends	O
to	O
if	O
standard	O
weight	B
decay	I
priors	O
are	O
assumed	O
the	O
covariance	B
function	I
of	O
this	O
gaussian	B
process	O
depends	O
on	O
the	O
details	O
of	O
the	O
priors	O
assumed	O
for	O
the	O
weights	O
in	O
the	O
network	B
and	O
the	O
activation	O
functions	B
of	O
the	O
hidden	O
units	B
using	O
a	O
given	O
gaussian	B
process	O
model	B
in	O
regression	B
we	O
have	O
spent	O
some	O
time	O
talking	O
about	O
priors	O
we	O
now	O
return	O
to	O
our	O
data	O
and	O
the	O
problem	O
of	O
prediction	B
how	O
do	O
we	O
make	O
predictions	O
with	O
a	O
gaussian	B
process	O
having	O
formed	O
the	O
covariance	B
matrix	B
c	O
in	O
equation	O
our	O
task	O
is	O
to	O
infer	O
tn	O
given	O
the	O
observed	O
vector	O
tn	O
the	O
joint	B
density	B
p	O
tn	O
is	O
a	O
gaussian	B
so	O
the	O
conditional	B
distribution	B
p	O
j	O
tn	O
p	O
tn	O
p	O
is	O
also	O
a	O
gaussian	B
we	O
now	O
distinguish	O
between	O
sizes	O
of	O
covariance	B
matrix	B
c	O
with	O
a	O
subscript	O
such	O
that	O
cn	O
is	O
the	O
covariance	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
examples	O
of	O
covariance	B
functions	B
matrix	B
for	O
the	O
vector	O
tn	O
tn	O
we	O
submatrices	O
of	O
cn	O
as	O
follows	O
cn	O
cn	O
kt	O
tn	O
n	O
tn	O
tn	O
p	O
j	O
tn	O
the	O
posterior	O
distribution	B
is	O
given	O
by	O
we	O
can	O
evaluate	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
the	O
posterior	O
distribution	B
of	O
tn	O
by	O
brute-force	O
inversion	O
of	O
cn	O
there	O
is	O
a	O
more	O
elegant	O
expression	O
for	O
the	O
predictive	B
distribution	B
however	O
which	O
is	O
useful	O
whenever	O
predictions	O
are	O
to	O
be	O
made	O
at	O
a	O
number	O
of	O
new	O
points	O
on	O
the	O
basis	O
of	O
the	O
data	B
set	B
of	O
size	O
n	O
we	O
can	O
write	O
n	O
using	O
the	O
partitioned	B
inverse	I
equations	O
n	O
in	O
terms	O
of	O
cn	O
and	O
n	O
m	O
m	O
mt	O
m	O
where	O
m	O
kt	O
m	O
n	O
k	O
m	O
n	O
m	O
n	O
mmt	O
when	O
we	O
substitute	O
this	O
matrix	B
into	O
equation	O
we	O
p	O
j	O
tn	O
z	O
where	O
kt	O
n	O
tn	O
kt	O
n	O
k	O
the	O
predictive	O
mean	B
at	O
the	O
new	O
point	O
is	O
given	O
by	O
and	O
the	O
error	B
bars	I
on	O
this	O
prediction	B
notice	O
that	O
we	O
do	O
not	O
need	O
to	O
invert	O
cn	O
in	O
order	O
to	O
make	O
predictions	O
at	O
xn	O
only	O
cn	O
needs	O
to	O
be	O
inverted	O
thus	O
gaussian	B
processes	I
allow	O
one	O
to	O
implement	O
a	O
model	B
with	O
a	O
number	O
of	O
basis	O
functions	B
h	O
much	O
larger	O
than	O
the	O
number	O
of	O
data	O
points	O
n	O
with	O
the	O
computational	O
requirement	O
being	O
of	O
order	O
n	O
independent	O
of	O
h	O
ll	O
discuss	O
ways	O
of	O
reducing	O
this	O
cost	O
later	O
the	O
predictions	O
produced	O
by	O
a	O
gaussian	B
process	O
depend	O
entirely	O
on	O
the	O
covariance	B
matrix	B
c	O
we	O
now	O
discuss	O
the	O
sorts	O
of	O
covariance	B
functions	B
one	O
might	O
choose	O
to	O
c	O
and	O
how	O
we	O
can	O
automate	O
the	O
selection	O
of	O
the	O
covariance	B
function	I
in	O
response	O
to	O
data	O
examples	O
of	O
covariance	B
functions	B
the	O
only	O
constraint	O
on	O
our	O
choice	O
of	O
covariance	B
function	I
is	O
that	O
it	O
must	O
generate	O
a	O
covariance	B
matrix	B
for	O
any	O
set	B
of	O
points	O
fxngn	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gaussian	B
processes	I
we	O
will	O
denote	O
the	O
parameters	B
of	O
a	O
covariance	B
function	I
by	O
the	O
covariance	B
matrix	B
of	O
t	O
has	O
entries	O
given	O
by	O
cmn	O
cxm	O
xn	O
where	O
c	O
is	O
the	O
covariance	B
function	I
and	O
n	O
is	O
a	O
noise	O
model	B
which	O
might	O
be	O
stationary	O
or	O
spatially	O
varying	O
for	O
example	O
n	O
for	O
input-dependent	O
noise	O
the	O
continuity	O
properties	O
of	O
c	O
determine	O
the	O
continuity	O
properties	O
of	O
typical	B
samples	O
from	O
the	O
gaussian	B
process	O
prior	B
an	O
encyclopaedic	O
paper	O
on	O
gaussian	B
processes	I
giving	O
many	O
valid	O
covariance	B
functions	B
has	O
been	O
written	O
by	O
abrahamsen	O
for	O
input-independent	O
noise	O
stationary	O
covariance	B
functions	B
a	O
stationary	O
covariance	B
function	I
is	O
one	O
that	O
is	O
translation	O
invariant	O
in	O
that	O
it	O
cx	O
dx	O
for	O
some	O
function	O
d	O
i	O
e	O
the	O
covariance	B
is	O
a	O
function	O
of	O
separation	B
only	O
also	O
known	O
as	O
the	O
autocovariance	O
function	O
if	O
additionally	O
c	O
depends	O
only	O
on	O
the	O
magnitude	O
of	O
the	O
distance	B
between	O
x	O
and	O
then	O
the	O
covariance	B
function	I
is	O
said	O
to	O
be	O
homogeneous	B
stationary	O
covariance	B
functions	B
may	O
also	O
be	O
described	O
in	O
terms	O
of	O
the	O
fourier	B
transform	I
of	O
the	O
function	O
d	O
which	O
is	O
known	O
as	O
the	O
power	O
spectrum	O
of	O
the	O
gaussian	B
process	O
this	O
fourier	B
transform	I
is	O
necessarily	O
a	O
positive	O
function	O
of	O
frequency	B
one	O
way	O
of	O
constructing	O
a	O
valid	O
stationary	O
covariance	B
function	I
is	O
to	O
invent	O
a	O
positive	O
function	O
of	O
frequency	B
and	O
d	O
to	O
be	O
its	O
inverse	O
fourier	B
transform	I
example	O
let	O
the	O
power	O
spectrum	O
be	O
a	O
gaussian	B
function	O
of	O
frequency	B
since	O
the	O
fourier	B
transform	I
of	O
a	O
gaussian	B
is	O
a	O
gaussian	B
the	O
autocovariance	O
function	O
corresponding	O
to	O
this	O
power	O
spectrum	O
is	O
a	O
gaussian	B
function	O
of	O
separation	B
this	O
argument	O
rederives	O
the	O
covariance	B
function	I
we	O
derived	O
at	O
equation	O
generalizing	O
slightly	O
a	O
popular	O
form	O
for	O
c	O
with	O
hyperparameters	O
is	O
cx	O
i	O
i	O
x	O
is	O
an	O
i-dimensional	O
vector	O
and	O
ri	O
is	O
a	O
lengthscale	O
associated	O
with	O
input	O
xi	O
the	O
lengthscale	O
in	O
direction	O
i	O
on	O
which	O
y	O
is	O
expected	O
to	O
vary	O
a	O
very	O
large	O
lengthscale	O
means	O
that	O
y	O
is	O
expected	O
to	O
be	O
essentially	O
a	O
constant	O
function	O
of	O
that	O
input	O
such	O
an	O
input	O
could	O
be	O
said	O
to	O
be	O
irrelevant	O
as	O
in	O
the	O
automatic	O
relevance	O
determination	O
method	O
for	O
neural	O
networks	O
neal	B
the	O
hyperparameter	B
the	O
vertical	O
scale	O
of	O
variations	O
of	O
a	O
typical	B
function	O
the	O
hyperparameter	B
allows	O
the	O
whole	O
function	O
to	O
be	O
away	O
from	O
zero	O
by	O
some	O
unknown	O
constant	O
to	O
understand	O
this	O
term	O
examine	O
equation	O
and	O
consider	O
the	O
basis	O
function	O
another	O
stationary	O
covariance	B
function	I
is	O
cx	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
adaptation	O
of	O
gaussian	B
process	O
models	O
figure	O
multimodal	O
likelihood	B
functions	B
for	O
gaussian	B
processes	I
a	O
data	B
set	B
of	O
points	O
is	O
modelled	O
with	O
the	O
simple	O
covariance	B
function	I
with	O
one	O
hyperparameter	B
controlling	O
the	O
noise	O
variance	B
panels	O
a	O
and	O
b	O
show	O
the	O
most	O
probable	O
interpolant	O
and	O
its	O
error	B
bars	I
when	O
the	O
hyperparameters	O
are	O
set	B
to	O
two	O
values	O
that	O
maximize	O
the	O
likelihood	B
p	O
j	O
xn	O
panel	O
c	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
likelihood	B
as	O
a	O
function	O
of	O
and	O
with	O
the	O
two	O
maxima	O
shown	O
by	O
crosses	O
from	O
gibbs	B
for	O
this	O
is	O
a	O
special	O
case	O
of	O
the	O
previous	O
covariance	B
function	I
for	O
the	O
typical	B
functions	B
from	O
this	O
prior	B
are	O
smooth	O
but	O
not	O
analytic	O
functions	B
for	O
typical	B
functions	B
are	O
continuous	B
but	O
not	O
smooth	O
a	O
covariance	B
function	I
that	O
models	O
a	O
function	O
that	O
is	O
periodic	O
with	O
known	O
period	O
in	O
the	O
ith	O
input	O
direction	O
is	O
cx	O
ri	O
a	O
figure	O
shows	O
some	O
random	B
samples	O
drawn	O
from	B
gaussian	B
processes	I
with	O
a	O
variety	O
of	O
covariance	B
functions	B
nonstationary	O
covariance	B
functions	B
the	O
simplest	O
nonstationary	O
covariance	B
function	I
is	O
the	O
one	O
corresponding	O
to	O
a	O
linear	B
trend	O
consider	O
the	O
plane	O
yx	O
pi	O
wixi	O
c	O
if	O
the	O
fwig	O
and	O
c	O
have	O
gaussian	B
distributions	O
with	O
zero	O
mean	B
and	O
variances	O
then	O
the	O
plane	O
has	O
a	O
covariance	B
function	I
w	O
and	O
c	O
respectively	O
clinx	O
i	O
c	O
an	O
example	O
of	O
random	B
sample	B
functions	B
incorporating	O
the	O
linear	B
term	O
can	O
be	O
seen	O
in	O
adaptation	O
of	O
gaussian	B
process	O
models	O
let	O
us	O
assume	O
that	O
a	O
form	O
of	O
covariance	B
function	I
has	O
been	O
chosen	O
but	O
that	O
it	O
depends	O
on	O
undetermined	O
hyperparameters	O
we	O
would	O
like	O
to	O
learn	O
these	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gaussian	B
processes	I
hyperparameters	O
from	O
the	O
data	O
this	O
learning	B
process	O
is	O
equivalent	O
to	O
the	O
inference	B
of	O
the	O
hyperparameters	O
of	O
a	O
neural	B
network	B
for	O
example	O
weight	B
decay	I
hyperparameters	O
it	O
is	O
a	O
complexity-control	O
problem	O
one	O
that	O
is	O
solved	O
nicely	O
by	O
the	O
bayesian	B
occam	O
s	O
razor	O
ideally	O
we	O
would	O
like	O
to	O
a	O
prior	B
distribution	B
on	O
the	O
hyperparameters	O
and	O
integrate	O
over	O
them	O
in	O
order	O
to	O
make	O
our	O
predictions	O
i	O
e	O
we	O
would	O
like	O
to	O
p	O
j	O
xn	O
p	O
j	O
xn	O
jd	O
but	O
this	O
integral	B
is	O
usually	O
intractable	O
there	O
are	O
two	O
approaches	O
we	O
can	O
take	O
we	O
can	O
approximate	O
the	O
integral	B
by	O
using	O
the	O
most	O
probable	O
values	O
of	O
hyperparameters	O
p	O
j	O
xn	O
p	O
j	O
xn	O
mp	O
or	O
we	O
can	O
perform	O
the	O
integration	O
over	O
numerically	O
using	O
monte	B
carlo	I
methods	I
and	O
rasmussen	O
neal	B
either	O
of	O
these	O
approaches	O
is	O
implemented	O
most	O
if	O
the	O
gradient	O
of	O
the	O
posterior	B
probability	B
of	O
can	O
be	O
evaluated	O
gradient	O
the	O
posterior	B
probability	B
of	O
is	O
p	O
jd	O
p	O
j	O
xn	O
the	O
log	O
of	O
the	O
term	O
evidence	B
for	O
the	O
hyperparameters	O
is	O
ln	O
p	O
j	O
xn	O
n	O
tn	O
and	O
its	O
derivative	O
with	O
respect	O
to	O
a	O
hyperparameter	B
is	O
ln	O
det	O
cn	O
n	O
tt	O
n	O
ln	O
ln	O
p	O
j	O
xn	O
trace	O
n	O
n	O
tt	O
n	O
n	O
tn	O
comments	O
assuming	O
that	O
the	O
derivatives	O
of	O
the	O
priors	O
is	O
straightforward	O
we	O
can	O
now	O
search	O
for	O
mp	O
however	O
there	O
are	O
two	O
problems	O
that	O
we	O
need	O
to	O
be	O
aware	O
of	O
firstly	O
as	O
illustrated	O
in	O
the	O
evidence	B
may	O
be	O
multimodal	O
suitable	O
priors	O
and	O
sensible	O
optimization	B
strategies	O
often	O
eliminate	O
poor	O
optima	O
secondly	O
and	O
perhaps	O
most	O
importantly	O
the	O
evaluation	O
of	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
requires	O
the	O
evaluation	O
of	O
n	O
any	O
exact	O
inversion	O
method	O
as	O
cholesky	B
decomposition	I
lu	O
decomposition	O
or	O
gaussjordan	O
elimination	O
has	O
an	O
associated	O
computational	O
cost	O
that	O
is	O
of	O
order	O
n	O
and	O
so	O
calculating	O
gradients	O
becomes	O
time	O
consuming	O
for	O
large	O
training	B
data	I
sets	O
approximate	O
methods	O
for	O
implementing	O
the	O
predictions	O
and	O
and	O
gradient	O
computation	O
are	O
an	O
active	O
research	O
area	O
one	O
approach	O
based	O
on	O
the	O
ideas	O
of	O
skilling	B
makes	O
approximations	O
to	O
and	O
trace	O
using	O
iterative	O
methods	O
with	O
cost	O
on	O
and	O
mackay	B
gibbs	B
further	O
references	O
on	O
this	O
topic	O
are	O
given	O
at	O
the	O
end	O
of	O
the	O
chapter	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gaussian	B
processes	I
can	O
be	O
integrated	O
into	O
modelling	B
once	O
we	O
identify	O
a	O
variable	O
that	O
can	O
sensibly	O
be	O
given	O
a	O
gaussian	B
process	O
prior	B
in	O
a	O
binary	O
problem	O
we	O
can	O
a	O
quantity	O
an	O
axn	O
such	O
that	O
the	O
probability	B
that	O
the	O
class	O
is	O
rather	O
than	O
is	O
p	O
an	O
large	O
positive	O
values	O
of	O
a	O
correspond	O
to	O
probabilities	O
close	O
to	O
one	O
large	O
negative	O
values	O
of	O
a	O
probabilities	O
that	O
are	O
close	O
to	O
zero	O
in	O
a	O
problem	O
we	O
typically	O
intend	O
that	O
the	O
probability	B
p	O
should	O
be	O
a	O
smoothly	O
varying	O
function	O
of	O
x	O
we	O
can	O
embody	O
this	O
prior	B
belief	B
by	O
ax	O
to	O
have	O
a	O
gaussian	B
process	O
prior	B
implementation	O
it	O
is	O
not	O
so	O
easy	O
to	O
perform	O
inferences	O
and	O
adapt	O
the	O
gaussian	B
process	O
model	B
to	O
data	O
in	O
a	O
model	B
as	O
in	O
regression	B
problems	O
because	O
the	O
likelihood	B
function	O
is	O
not	O
a	O
gaussian	B
function	O
of	O
an	O
so	O
the	O
posterior	O
distribution	B
of	O
a	O
given	O
some	O
observations	O
t	O
is	O
not	O
gaussian	B
and	O
the	O
normalization	O
constant	O
p	O
j	O
xn	O
cannot	O
be	O
written	O
down	O
analytically	O
barber	O
and	O
williams	O
have	O
implemented	O
based	O
on	O
gaussian	B
process	O
priors	O
using	O
laplace	B
approximations	O
neal	B
has	O
implemented	O
a	O
monte	B
carlo	I
approach	O
to	O
implementing	O
a	O
gaussian	B
process	O
gibbs	B
and	O
mackay	B
have	O
implemented	O
another	O
cheap	O
and	O
cheerful	O
approach	O
based	O
on	O
the	O
methods	O
of	O
jaakkola	B
and	O
jordan	B
in	O
this	O
variational	B
gaussian	B
process	I
we	O
obtain	O
tractable	O
upper	O
and	O
lower	O
bounds	O
for	O
the	O
unnormalized	O
posterior	O
density	B
over	O
a	O
p	O
j	O
ap	O
these	O
bounds	O
are	O
parameterized	O
by	O
variational	B
parameters	B
which	O
are	O
adjusted	O
in	O
order	O
to	O
obtain	O
the	O
tightest	O
possible	O
using	O
normalized	O
versions	O
of	O
the	O
optimized	O
bounds	O
we	O
then	O
compute	O
approximations	O
to	O
the	O
predictive	O
distributions	O
multi-class	O
problems	O
can	O
also	O
be	O
solved	O
with	O
monte	B
carlo	I
methods	I
and	O
variational	B
methods	I
discussion	O
gaussian	B
processes	I
are	O
moderately	O
simple	O
to	O
implement	O
and	O
use	O
because	O
very	O
few	O
parameters	B
of	O
the	O
model	B
need	O
to	O
be	O
determined	O
by	O
hand	O
only	O
the	O
priors	O
on	O
the	O
hyperparameters	O
gaussian	B
processes	I
are	O
useful	O
tools	O
for	O
automated	O
tasks	O
where	O
tuning	O
for	O
each	O
problem	O
is	O
not	O
possible	O
we	O
do	O
not	O
appear	O
to	O
any	O
performance	O
for	O
this	O
simplicity	O
it	O
is	O
easy	O
to	O
construct	O
gaussian	B
processes	I
that	O
have	O
particular	O
desired	O
properties	O
for	O
example	O
we	O
can	O
make	O
a	O
straightforward	O
automatic	O
relevance	O
determination	O
model	B
one	O
obvious	O
problem	O
with	O
gaussian	B
processes	I
is	O
the	O
computational	O
cost	O
associated	O
with	O
inverting	O
an	O
n	O
n	O
matrix	B
the	O
cost	O
of	O
direct	O
methods	O
of	O
inversion	O
becomes	O
prohibitive	O
when	O
the	O
number	O
of	O
data	O
points	O
n	O
is	O
greater	O
than	O
about	O
have	O
we	O
thrown	O
the	O
baby	O
out	O
with	O
the	O
bath	O
water	O
according	O
to	O
the	O
hype	O
of	O
neural	O
networks	O
were	O
meant	O
to	O
be	O
intelligent	O
models	O
that	O
discovered	O
features	O
and	O
patterns	O
in	O
data	O
gaussian	B
processes	I
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gaussian	B
processes	I
contrast	O
are	O
simply	O
smoothing	O
devices	O
how	O
can	O
gaussian	B
processes	I
possibly	O
replace	O
neural	O
networks	O
were	O
neural	O
networks	O
over-hyped	O
or	O
have	O
we	O
underestimated	O
the	O
power	O
of	O
smoothing	O
methods	O
i	O
think	O
both	O
these	O
propositions	O
are	O
true	O
the	O
success	O
of	O
gaussian	B
processes	I
shows	O
that	O
many	O
real-world	O
data	O
modelling	B
problems	O
are	O
perfectly	O
well	O
solved	O
by	O
sensible	O
smoothing	O
methods	O
the	O
most	O
interesting	O
problems	O
the	O
task	O
of	O
feature	O
discovery	O
for	O
example	O
are	O
not	O
ones	O
that	O
gaussian	B
processes	I
will	O
solve	O
but	O
maybe	O
multilayer	O
perceptrons	O
can	O
t	O
solve	O
them	O
either	O
perhaps	O
a	O
fresh	O
start	O
is	O
needed	O
approaching	O
the	O
problem	O
of	O
machine	B
learning	B
from	O
a	O
paradigm	O
from	O
the	O
supervised	O
feedforward	O
mapping	B
further	O
reading	O
the	O
study	O
of	O
gaussian	B
processes	I
for	O
regression	B
is	O
far	O
from	O
new	O
time	O
series	O
analysis	O
was	O
being	O
performed	O
by	O
the	O
astronomer	O
t	O
n	O
thiele	B
using	O
gaussian	B
processes	I
in	O
in	O
the	O
wienerkolmogorov	O
prediction	B
theory	O
was	O
introduced	O
for	O
prediction	B
of	O
trajectories	O
of	O
military	O
targets	O
within	O
the	O
geostatistics	B
matheron	O
proposed	O
a	O
framework	O
for	O
regression	B
using	O
optimal	B
linear	B
estimators	O
which	O
he	O
called	O
kriging	B
after	O
d	O
g	O
krige	O
a	O
south	O
african	O
mining	O
engineer	O
this	O
framework	O
is	O
identical	O
to	O
the	O
gaussian	B
process	O
approach	O
to	O
regression	B
kriging	B
has	O
been	O
developed	O
considerably	O
in	O
the	O
last	O
thirty	O
years	O
cressie	O
for	O
a	O
review	O
including	O
several	O
bayesian	B
treatments	O
kitanidis	O
however	O
the	O
geostatistics	B
approach	O
to	O
the	O
gaussian	B
process	O
model	B
has	O
concentrated	O
mainly	O
on	O
low-dimensional	O
problems	O
and	O
has	O
largely	O
ignored	O
any	O
probabilistic	O
interpretation	O
of	O
the	O
model	B
kalman	O
are	O
widely	O
used	O
to	O
implement	O
inferences	O
for	O
stationary	O
one-dimensional	O
gaussian	B
processes	I
and	O
are	O
popular	O
models	O
for	O
speech	O
and	O
music	B
modelling	B
and	O
fortmann	O
generalized	B
radial	O
basis	O
functions	B
and	O
girosi	O
arma	O
models	O
and	O
variable	O
metric	B
kernel	B
methods	O
are	O
all	O
closely	O
related	O
to	O
gaussian	B
processes	I
see	O
also	O
o	O
hagan	O
the	O
idea	O
of	O
replacing	O
supervised	O
neural	O
networks	O
by	B
gaussian	B
processes	I
was	O
explored	O
by	O
williams	O
and	O
rasmussen	O
and	O
neal	B
a	O
thorough	O
comparison	O
of	O
gaussian	B
processes	I
with	O
other	O
methods	O
such	O
as	O
neural	O
networks	O
and	O
mars	O
was	O
made	O
by	O
rasmussen	O
methods	O
for	O
reducing	O
the	O
complexity	B
of	O
data	O
modelling	B
with	O
gaussian	B
processes	I
remain	O
an	O
active	O
research	O
area	O
and	O
girosi	O
luo	O
and	O
wahba	O
tresp	O
williams	O
and	O
seeger	O
smola	O
and	O
bartlett	O
rasmussen	O
seeger	O
et	O
al	O
opper	O
and	O
winther	O
a	O
longer	O
review	O
of	O
gaussian	B
processes	I
is	O
in	O
a	O
review	O
paper	O
on	O
regression	B
with	O
complexity	B
control	I
using	O
hierarchical	O
bayesian	B
models	O
is	O
gaussian	B
processes	I
and	O
support	B
vector	I
learning	B
machines	O
et	O
al	O
vapnik	O
have	O
a	O
lot	O
in	O
common	O
both	O
are	O
kernel-based	O
predictors	O
the	O
kernel	B
being	O
another	O
name	O
for	O
the	O
covariance	B
function	I
a	O
bayesian	B
version	O
of	O
support	O
vectors	B
exploiting	O
this	O
connection	O
can	O
be	O
found	O
in	O
et	O
al	O
chu	O
et	O
al	O
chu	O
et	O
al	O
chu	O
et	O
al	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
deconvolution	B
traditional	O
image	B
reconstruction	I
methods	O
optimal	B
linear	B
in	O
many	O
imaging	O
problems	O
the	O
data	O
measurements	O
fdng	O
are	O
linearly	O
related	O
to	O
the	O
underlying	O
image	B
f	O
dn	O
rnkfk	O
nn	O
the	O
vector	O
n	O
denotes	O
the	O
inevitable	O
noise	O
that	O
corrupts	O
real	O
data	O
in	O
the	O
case	O
of	O
a	O
camera	B
which	O
produces	O
a	O
blurred	O
picture	O
the	O
vector	O
f	O
denotes	O
the	O
true	O
image	B
d	O
denotes	O
the	O
blurred	O
and	O
noisy	B
picture	O
and	O
the	O
linear	B
operator	O
r	O
is	O
a	O
convolution	B
by	O
the	O
point	B
spread	I
function	I
of	O
the	O
camera	B
in	O
this	O
special	O
case	O
the	O
true	O
image	B
and	O
the	O
data	O
vector	O
reside	O
in	O
the	O
same	O
space	O
but	O
it	O
is	O
important	O
to	O
maintain	O
a	O
distinction	O
between	O
them	O
we	O
will	O
use	O
the	O
subscript	O
n	O
n	O
to	O
run	O
over	O
data	O
measurements	O
and	O
the	O
subscripts	O
k	O
k	O
to	O
run	O
over	O
image	B
pixels	O
one	O
might	O
speculate	O
that	O
since	O
the	O
blur	B
was	O
created	O
by	O
a	O
linear	B
operation	O
then	O
perhaps	O
it	O
might	O
be	O
deblurred	O
by	O
another	O
linear	B
operation	O
we	O
can	O
derive	O
the	O
optimal	B
linear	B
in	O
two	O
ways	O
bayesian	B
derivation	B
we	O
assume	O
that	O
the	O
linear	B
operator	O
r	O
is	O
known	O
and	O
that	O
the	O
noise	O
n	O
is	O
gaussian	B
and	O
independent	O
with	O
a	O
known	O
standard	B
deviation	I
p	O
f	O
exp	O
we	O
assume	O
that	O
the	O
prior	B
probability	B
of	O
the	O
image	B
is	O
also	O
gaussian	B
with	O
a	O
scale	O
parameter	O
p	O
j	O
c	O
f	O
f	O
a	O
if	O
we	O
assume	O
no	O
correlations	B
among	O
the	O
pixels	O
then	O
the	O
symmetric	B
full	O
rank	O
matrix	B
c	O
is	O
equal	O
to	O
the	O
identity	B
matrix	B
i	O
the	O
more	O
sophisticated	O
intrinsic	B
correlation	I
function	I
model	B
uses	O
c	O
where	O
g	O
is	O
a	O
convolution	B
that	O
takes	O
us	O
from	O
an	O
imaginary	O
hidden	O
image	B
which	O
is	O
uncorrelated	O
to	O
the	O
real	O
correlated	O
image	B
the	O
intrinsic	B
correlation	I
function	I
should	O
not	O
be	O
confused	O
with	O
the	O
point	B
spread	I
function	I
r	O
which	O
the	O
image-to-data	O
mapping	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
deconvolution	B
a	O
zero-mean	O
gaussian	B
prior	B
is	O
clearly	O
a	O
poor	O
assumption	O
if	O
it	O
is	O
known	O
that	O
all	O
elements	O
of	O
the	O
image	B
f	O
are	O
positive	O
but	O
let	O
us	O
proceed	O
we	O
can	O
now	O
write	O
down	O
the	O
posterior	B
probability	B
of	O
an	O
image	B
f	O
given	O
the	O
data	O
d	O
in	O
words	O
p	O
j	O
d	O
p	O
f	O
j	O
p	O
posterior	O
likelihood	B
prior	B
evidence	B
the	O
evidence	B
p	O
is	O
the	O
normalizing	O
constant	O
for	O
this	O
posterior	O
distribution	B
here	O
it	O
is	O
unimportant	O
but	O
it	O
is	O
used	O
in	O
a	O
more	O
sophisticated	O
analysis	O
to	O
compare	O
for	O
example	O
values	O
of	O
and	O
or	O
point	O
spread	O
functions	B
r	O
since	O
the	O
posterior	O
distribution	B
is	O
the	O
product	O
of	O
two	O
gaussian	B
functions	B
of	O
f	O
it	O
is	O
also	O
a	O
gaussian	B
and	O
can	O
therefore	O
be	O
summarized	O
by	O
its	O
mean	B
which	O
is	O
also	O
the	O
most	O
probable	O
image	B
fmp	O
and	O
its	O
covariance	B
matrix	B
log	O
p	O
j	O
d	O
f	O
fmp	O
rt	O
is	O
called	O
the	O
optimal	B
linear	B
when	O
which	O
the	O
joint	B
error	B
bars	I
on	O
f	O
in	O
this	O
equation	O
the	O
symbol	O
r	O
denotes	O
with	O
respect	O
to	O
the	O
image	B
parameters	B
f	O
we	O
can	O
fmp	O
by	O
the	O
log	O
of	O
the	O
posterior	O
and	O
solving	O
for	O
the	O
derivative	O
being	O
zero	O
we	O
obtain	O
rtd	O
the	O
operator	O
the	O
term	O
f	O
rt	O
the	O
term	O
f	O
f	O
c	O
can	O
be	O
neglected	O
the	O
optimal	B
linear	B
is	O
the	O
pseudoinverse	B
c	O
regularizes	O
this	O
ill-conditioned	O
inverse	O
the	O
optimal	B
linear	B
can	O
also	O
be	O
manipulated	O
into	O
the	O
form	O
optimal	B
linear	B
f	O
minimum	O
square	B
error	O
derivation	B
the	O
non-bayesian	O
derivation	B
of	O
the	O
optimal	B
linear	B
starts	O
by	O
assuming	O
that	O
we	O
will	O
estimate	O
the	O
true	O
image	B
f	O
by	O
a	O
linear	B
function	O
of	O
the	O
data	O
wd	O
the	O
linear	B
operator	O
w	O
is	O
then	O
optimized	O
by	O
minimizing	O
the	O
expected	O
sumsquared	O
error	O
between	O
and	O
the	O
unknown	O
true	O
image	B
in	O
the	O
following	O
equations	O
summations	O
over	O
repeated	O
indices	O
k	O
n	O
are	O
implicit	O
the	O
expectation	B
is	O
over	O
both	O
the	O
statistics	O
of	O
the	O
random	B
variables	O
fnng	O
and	O
the	O
ensemble	B
of	O
images	B
f	O
which	O
we	O
expect	O
to	O
bump	O
into	O
we	O
assume	O
that	O
the	O
noise	O
is	O
zero	O
mean	B
and	O
uncorrelated	O
to	O
second	O
order	O
with	O
itself	O
and	O
everything	O
else	O
with	O
hei	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
traditional	O
image	B
reconstruction	I
methods	O
with	O
respect	O
to	O
w	O
and	O
introducing	O
f	O
the	O
bayesian	B
derivation	B
above	O
we	O
that	O
the	O
optimal	B
linear	B
is	O
f	O
in	O
wopt	O
f	O
we	O
obtain	O
the	O
optimal	B
linear	B
of	O
the	O
if	O
we	O
identify	O
f	O
bayesian	B
derivation	B
the	O
ad	O
hoc	O
assumptions	B
made	O
in	O
this	O
derivation	B
were	O
the	O
choice	O
of	O
a	O
quadratic	O
error	O
measure	O
and	O
the	O
decision	O
to	O
use	O
a	O
linear	B
estimator	B
it	O
is	O
interesting	O
that	O
without	O
explicit	O
assumptions	B
of	O
gaussian	B
distributions	O
this	O
derivation	B
has	O
reproduced	O
the	O
same	O
estimator	B
as	O
the	O
bayesian	B
posterior	O
mode	O
fmp	O
the	O
advantage	O
of	O
a	O
bayesian	B
approach	O
is	O
that	O
we	O
can	O
criticize	O
these	O
as	O
sumptions	O
and	O
modify	O
them	O
in	O
order	O
to	O
make	O
better	O
reconstructions	O
other	O
image	B
models	I
the	O
better	O
matched	O
our	O
model	B
of	O
images	B
p	O
jh	O
is	O
to	O
the	O
real	O
world	O
the	O
better	O
our	O
image	B
reconstructions	O
will	O
be	O
and	O
the	O
less	O
data	O
we	O
will	O
need	O
to	O
answer	O
any	O
given	O
question	O
the	O
gaussian	B
models	O
which	O
lead	O
to	O
the	O
optimal	B
linear	B
are	O
spectacularly	O
poorly	O
matched	O
to	O
the	O
real	O
world	O
for	O
example	O
the	O
gaussian	B
prior	B
fails	O
to	O
specify	O
that	O
all	O
pixel	O
intensities	O
in	O
an	O
image	B
are	O
positive	O
this	O
omission	O
leads	O
to	O
the	O
most	O
pronounced	O
artefacts	O
where	O
the	O
image	B
under	O
observation	O
has	O
high	O
contrast	O
or	O
large	O
black	B
patches	O
optimal	B
linear	B
applied	O
to	O
astronomical	O
data	O
give	O
reconstructions	O
with	O
negative	O
areas	O
in	O
them	O
corresponding	O
to	O
patches	O
of	O
sky	O
that	O
suck	O
energy	B
out	O
of	O
telescopes	O
the	O
maximum	B
entropy	B
model	B
for	O
image	B
deconvolution	B
and	O
daniell	O
was	O
a	O
great	O
success	O
principally	O
because	O
this	O
model	B
forced	O
the	O
reconstructed	O
image	B
to	O
be	O
positive	O
the	O
spurious	O
negative	O
areas	O
and	O
complementary	O
spurious	O
positive	O
areas	O
are	O
eliminated	O
and	O
the	O
quality	O
of	O
the	O
reconstruction	O
is	O
greatly	O
enhanced	O
the	O
classic	O
maximum	B
entropy	B
model	B
assigns	O
an	O
entropic	O
prior	B
p	O
j	O
mhclassic	O
mz	O
sf	O
m	O
lnmifi	O
fi	O
mi	O
where	O
this	O
model	B
enforces	O
positivity	B
the	O
parameter	O
a	O
characteristic	O
dynamic	O
range	O
by	O
which	O
the	O
pixel	O
values	O
are	O
expected	O
to	O
from	O
the	O
default	O
image	B
m	O
the	O
intrinsic-correlation-function	O
maximum-entropy	O
model	B
introduces	O
an	O
expectation	B
of	O
spatial	O
correlations	B
into	O
the	O
prior	B
on	O
f	O
by	O
writing	B
f	O
gh	O
where	O
g	O
is	O
a	O
convolution	B
with	O
an	O
intrinsic	B
correlation	I
function	I
and	O
putting	O
a	O
classic	O
maxent	O
prior	B
on	O
the	O
underlying	O
hidden	O
image	B
h	O
probabilistic	O
movies	O
having	O
found	O
not	O
only	O
the	O
most	O
probable	O
image	B
fmp	O
but	O
also	O
error	B
bars	I
on	O
it	O
one	O
task	O
is	O
to	O
visualize	O
those	O
error	B
bars	I
whether	O
or	O
not	O
we	O
use	O
monte	B
carlo	I
methods	I
to	O
infer	O
f	O
a	O
correlated	O
random	B
walk	I
around	O
the	O
posterior	O
distribution	B
can	O
be	O
used	O
to	O
visualize	O
the	O
uncertainties	O
and	O
correlations	B
for	O
a	O
gaussian	B
posterior	O
distribution	B
we	O
can	O
create	O
a	O
correlated	O
sequence	B
of	O
unit	O
normal	B
random	B
vectors	B
n	O
using	O
cnt	O
sz	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
deconvolution	B
where	O
z	O
is	O
a	O
unit	O
normal	B
random	B
vector	O
and	O
controls	O
how	O
persistent	O
the	O
memory	B
of	O
the	O
sequence	B
is	O
we	O
then	O
render	O
the	O
image	B
sequence	B
by	O
f	O
fmp	O
fjd	O
nt	O
fjd	O
is	O
the	O
cholesky	B
decomposition	I
of	O
where	O
supervised	O
neural	O
networks	O
for	O
image	B
deconvolution	B
neural	B
network	B
researchers	O
often	O
exploit	B
the	O
following	O
strategy	O
given	O
a	O
problem	O
currently	O
solved	O
with	O
a	O
standard	O
algorithm	O
interpret	O
the	O
computations	O
performed	O
by	O
the	O
algorithm	O
as	O
a	O
parameterized	O
mapping	B
from	O
an	O
input	O
to	O
an	O
output	O
and	O
call	O
this	O
mapping	B
a	O
neural	B
network	B
then	O
adapt	O
the	O
parameters	B
to	O
data	O
so	O
as	O
to	O
produce	O
another	O
mapping	B
that	O
solves	O
the	O
task	O
better	O
by	O
construction	B
the	O
neural	B
network	B
can	O
reproduce	O
the	O
standard	O
algorithm	O
so	O
this	O
data-driven	O
adaptation	O
can	O
only	O
make	O
the	O
performance	O
better	O
there	O
are	O
several	O
reasons	O
why	O
standard	O
algorithms	B
can	O
be	O
bettered	O
in	O
this	O
way	O
algorithms	B
are	O
often	O
not	O
designed	O
to	O
optimize	O
the	O
real	O
objective	B
function	I
for	O
example	O
in	O
speech	O
recognition	B
a	O
hidden	B
markov	I
model	B
is	O
designed	O
to	O
model	B
the	O
speech	O
signal	O
and	O
is	O
so	O
as	O
to	O
to	O
maximize	O
the	O
generative	O
probability	B
given	O
the	O
known	O
string	O
of	O
words	O
in	O
the	O
training	B
data	I
but	O
the	O
real	O
objective	O
is	O
to	O
discriminate	O
between	O
words	O
if	O
an	O
inadequate	O
model	B
is	O
being	O
used	O
the	O
neural-net-style	O
training	O
of	O
the	O
model	B
will	O
focus	B
the	O
limited	O
resources	O
of	O
the	O
model	B
on	O
the	O
aspects	O
relevant	O
to	O
the	O
discrimination	O
task	O
discriminative	B
training	I
of	O
hidden	O
markov	O
models	O
for	O
speech	O
recognition	B
does	O
improve	O
their	O
performance	O
the	O
neural	B
network	B
can	O
be	O
more	O
than	O
the	O
standard	O
model	B
some	O
of	O
the	O
adaptive	B
parameters	B
might	O
have	O
been	O
viewed	O
as	O
features	O
by	O
the	O
original	O
designers	O
a	O
network	B
can	O
properties	O
in	O
the	O
data	O
that	O
were	O
not	O
included	O
in	O
the	O
original	O
model	B
deconvolution	B
in	O
humans	O
a	O
huge	O
fraction	O
of	O
our	O
brain	B
is	O
devoted	O
to	O
vision	B
one	O
of	O
the	O
neglected	O
features	O
of	O
our	O
visual	O
system	O
is	O
that	O
the	O
raw	O
image	B
falling	O
on	O
the	O
retina	O
is	O
severely	O
blurred	O
while	O
most	O
people	O
can	O
see	O
with	O
a	O
resolution	O
of	O
about	O
arcminute	O
sixtieth	O
of	O
a	O
degree	B
under	O
any	O
daylight	O
conditions	O
bright	O
or	O
dim	O
the	O
image	B
on	O
our	O
retina	O
is	O
blurred	O
through	O
a	O
point	B
spread	I
function	I
of	O
width	O
as	O
large	O
as	O
arcminutes	O
and	O
howarth	O
and	O
bradley	O
it	O
is	O
amazing	O
that	O
we	O
are	O
able	O
to	O
resolve	O
pixels	O
that	O
are	O
times	O
smaller	O
in	O
area	O
than	O
the	O
blob	O
produced	O
on	O
our	O
retina	O
by	O
any	O
point	O
source	O
isaac	O
newton	B
was	O
aware	O
of	O
this	O
conundrum	O
it	O
s	O
hard	O
to	O
make	O
a	O
lens	O
that	O
does	O
not	O
have	O
chromatic	B
aberration	I
and	O
our	O
cornea	O
and	O
lens	O
like	O
a	O
lens	O
made	O
of	O
ordinary	O
glass	O
refract	O
blue	O
light	O
more	O
strongly	O
than	O
red	O
typically	O
our	O
eyes	O
focus	B
correctly	O
for	O
the	O
middle	O
of	O
the	O
visible	O
spectrum	O
so	O
if	O
we	O
look	O
at	O
a	O
single	O
white	B
dot	O
made	O
of	O
red	O
green	O
and	O
blue	O
light	O
the	O
image	B
on	O
our	O
retina	O
consists	O
of	O
a	O
sharply	O
focussed	O
green	O
dot	O
surrounded	O
by	O
a	O
broader	O
red	O
blob	O
superposed	O
on	O
an	O
even	O
broader	O
blue	O
blob	O
the	O
width	O
of	O
the	O
red	O
and	O
blue	O
blobs	O
is	O
proportional	O
to	O
the	O
diameter	O
of	O
the	O
pupil	B
which	O
is	O
largest	O
under	O
dim	O
lighting	O
conditions	O
blobs	O
are	O
roughly	O
concentric	O
though	O
most	O
people	O
have	O
a	O
slight	O
bias	B
such	O
that	O
in	O
one	O
eye	O
the	O
red	O
blob	O
is	O
centred	O
a	O
tiny	O
distance	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
deconvolution	B
in	O
humans	O
to	O
the	O
left	O
and	O
the	O
blue	O
is	O
centred	O
a	O
tiny	O
distance	B
to	O
the	O
right	O
and	O
in	O
the	O
other	O
eye	O
it	O
s	O
the	O
other	O
way	O
round	O
this	O
slight	O
bias	B
explains	O
why	O
when	O
we	O
look	O
at	O
blue	O
and	O
red	O
writing	B
on	O
a	O
dark	O
background	O
most	O
people	O
perceive	O
the	O
blue	O
writing	B
to	O
be	O
at	O
a	O
slightly	O
greater	O
depth	O
than	O
the	O
red	O
in	O
a	O
minority	O
of	O
people	O
this	O
small	O
bias	B
is	O
the	O
other	O
way	O
round	O
and	O
the	O
redblue	O
depth	O
perception	O
is	O
reversed	O
but	O
this	O
many	O
people	O
are	O
aware	O
of	O
having	O
noticed	O
it	O
in	O
cinemas	O
for	O
example	O
is	O
tiny	O
compared	O
with	O
the	O
chromatic	B
aberration	I
we	O
are	O
discussing	O
you	O
can	O
vividly	O
demonstrate	O
to	O
yourself	O
how	O
enormous	O
the	O
chromatic	B
aberration	I
in	O
your	O
eye	O
is	O
with	O
the	O
help	O
of	O
a	O
sheet	O
of	O
card	B
and	O
a	O
colour	O
computer	B
screen	O
for	O
the	O
most	O
impressive	O
results	O
i	O
guarantee	O
you	O
will	O
be	O
amazed	O
use	O
a	O
dim	O
room	O
with	O
no	O
light	O
apart	O
from	O
the	O
computer	B
screen	O
a	O
pretty	O
strong	O
will	O
still	O
be	O
seen	O
even	O
if	O
the	O
room	O
has	O
daylight	O
coming	O
into	O
it	O
as	O
long	O
as	O
it	O
is	O
not	O
bright	O
sunshine	O
cut	O
a	O
slit	O
about	O
mm	O
wide	O
in	O
the	O
card	B
on	O
the	O
screen	O
display	O
a	O
few	O
small	O
coloured	B
objects	O
on	O
a	O
black	B
background	O
i	O
especially	O
recommend	O
thin	O
vertical	O
objects	O
coloured	B
pure	O
red	O
pure	O
blue	O
magenta	O
red	O
plus	O
blue	O
and	O
white	B
plus	O
blue	O
plus	O
include	O
a	O
little	O
blackand-white	O
text	O
on	O
the	O
screen	O
too	O
stand	O
or	O
sit	O
far	O
away	O
that	O
you	O
can	O
only	O
just	O
read	O
the	O
text	O
perhaps	O
a	O
distance	B
of	O
four	O
metres	O
or	O
so	O
if	O
you	O
have	O
normal	B
vision	B
now	O
hold	O
the	O
slit	O
vertically	O
in	O
front	O
of	O
one	O
of	O
your	O
eyes	O
and	O
close	O
the	O
other	O
eye	O
hold	O
the	O
slit	O
near	O
to	O
your	O
eye	O
brushing	O
your	O
eyelashes	O
and	O
look	O
through	O
it	O
waggle	O
the	O
slit	O
slowly	O
to	O
the	O
left	O
and	O
to	O
the	O
right	O
so	O
that	O
the	O
slit	O
is	O
alternately	O
in	O
front	O
of	O
the	O
left	O
and	O
right	O
sides	O
of	O
your	O
pupil	B
what	O
do	O
you	O
see	O
i	O
see	O
the	O
red	O
objects	O
waggling	O
to	O
and	O
fro	O
and	O
the	O
blue	O
objects	O
waggling	O
to	O
and	O
fro	O
through	O
huge	O
distances	O
and	O
in	O
opposite	O
directions	O
while	O
white	B
objects	O
appear	O
to	O
stay	O
still	O
and	O
are	O
negligibly	O
distorted	O
thin	O
magenta	O
objects	O
can	O
be	O
seen	O
splitting	O
into	O
their	O
constituent	O
red	O
and	O
blue	O
parts	O
measure	O
how	O
large	O
the	O
motion	O
of	O
the	O
red	O
and	O
blue	O
objects	O
is	O
it	O
s	O
more	O
than	O
minutes	O
of	O
arc	O
for	O
me	O
in	O
a	O
dim	O
room	O
then	O
check	O
how	O
sharply	O
you	O
can	O
see	O
under	O
these	O
conditions	O
look	O
at	O
the	O
text	O
on	O
the	O
screen	O
for	O
example	O
is	O
it	O
not	O
the	O
case	O
that	O
you	O
can	O
see	O
your	O
whole	O
pupil	B
features	O
far	O
smaller	O
than	O
the	O
distance	B
through	O
which	O
the	O
red	O
and	O
blue	O
components	O
were	O
waggling	O
yet	O
when	O
you	O
are	O
using	O
the	O
whole	O
pupil	B
what	O
is	O
falling	O
on	O
your	O
retina	O
must	O
be	O
an	O
image	B
blurred	O
with	O
a	O
blurring	O
diameter	O
equal	O
to	O
the	O
waggling	O
amplitude	O
one	O
of	O
the	O
main	O
functions	B
of	O
early	O
visual	O
processing	O
must	O
be	O
to	O
deconvolve	O
this	O
chromatic	B
aberration	I
neuroscientists	O
sometimes	O
conjecture	O
that	O
the	O
reason	O
why	O
retinal	O
ganglion	B
cells	I
and	O
cells	O
in	O
the	O
lateral	O
geniculate	O
nucleus	O
main	O
brain	B
area	O
to	O
which	O
retinal	O
ganglion	B
cells	I
project	O
have	O
centre-surround	O
receptive	O
with	O
colour	O
opponency	O
wavelength	O
in	O
the	O
centre	O
and	O
medium	O
wavelength	O
in	O
the	O
surround	O
for	O
example	O
is	O
in	O
order	O
to	O
perform	O
feature	O
extraction	O
or	O
edge	B
detection	O
but	O
i	O
think	O
this	O
view	O
is	O
mistaken	O
the	O
reason	O
we	O
have	O
centre-surround	O
at	O
the	O
stage	O
of	O
visual	O
processing	O
the	O
fovea	B
at	O
least	O
is	O
for	O
the	O
huge	O
task	O
of	O
deconvolution	B
of	O
chromatic	B
aberration	I
i	O
speculate	O
that	O
the	O
mccollough	O
an	O
extremely	O
long-lasting	O
association	O
of	O
colours	O
with	O
orientation	O
mackay	B
and	O
mackay	B
is	O
produced	O
by	O
the	O
adaptation	O
mechanism	O
that	O
tunes	O
our	O
chromaticaberration-deconvolution	O
circuits	O
our	O
deconvolution	B
circuits	O
need	O
to	O
be	O
rapidly	O
tuneable	O
because	O
the	O
point	B
spread	I
function	I
of	O
our	O
eye	O
changes	O
with	O
our	O
pupil	B
diameter	O
which	O
can	O
change	O
within	O
seconds	O
and	O
indeed	O
the	O
mccollough	O
can	O
be	O
induced	O
within	O
seconds	O
at	O
the	O
same	O
time	O
the	O
is	O
long-lasting	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
deconvolution	B
when	O
an	O
eye	O
is	O
covered	O
because	O
it	O
s	O
in	O
our	O
interests	O
that	O
our	O
deconvolution	B
circuits	O
should	O
stay	O
well-tuned	O
while	O
we	O
sleep	B
so	O
that	O
we	O
can	O
see	O
sharply	O
the	O
instant	O
we	O
wake	O
up	O
i	O
also	O
wonder	O
whether	O
the	O
main	O
reason	O
that	O
we	O
evolved	O
colour	B
vision	B
was	O
not	O
in	O
order	O
to	O
see	O
fruit	O
better	O
but	O
so	O
as	O
to	O
be	O
able	O
to	O
see	O
black	B
and	O
white	B
sharper	O
deconvolving	O
chromatic	B
aberration	I
is	O
easier	O
even	O
in	O
an	O
entirely	O
black	B
and	O
white	B
world	O
if	O
one	O
has	O
access	O
to	O
chromatic	O
information	B
in	O
the	O
image	B
and	O
a	O
speculation	O
why	O
do	O
our	O
eyes	O
make	O
micro-saccades	B
when	O
we	O
look	O
at	O
things	O
these	O
miniature	O
eye-movements	O
are	O
of	O
an	O
angular	O
size	O
bigger	O
than	O
the	O
spacing	O
between	O
the	O
cones	B
in	O
the	O
fovea	B
are	O
spaced	O
at	O
roughly	O
minute	O
of	O
arc	O
the	O
perceived	O
resolution	O
of	O
the	O
eye	O
the	O
typical	B
size	O
of	O
a	O
microsaccade	O
is	O
minutes	O
of	O
arc	O
and	O
riggs	O
is	O
it	O
a	O
coincidence	B
that	O
this	O
is	O
the	O
same	O
as	O
the	O
size	O
of	O
chromatic	B
aberration	I
surely	O
micro-saccades	B
must	O
play	O
an	O
essential	O
role	O
in	O
the	O
deconvolution	B
mechanism	O
that	O
delivers	O
our	O
high-resolution	O
vision	B
exercises	O
exercise	O
blur	B
an	O
image	B
with	O
a	O
circular	O
hat	B
point	B
spread	I
function	I
and	O
add	O
noise	O
then	O
deconvolve	O
the	O
blurry	O
noisy	B
image	B
using	O
the	O
optimal	B
linear	B
find	O
error	B
bars	I
and	O
visualize	O
them	O
by	O
making	O
a	O
probabilistic	B
movie	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
part	O
vi	O
sparse	B
graph	B
codes	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
part	O
vi	O
the	O
central	O
problem	O
of	O
communication	B
theory	O
is	O
to	O
construct	O
an	O
encoding	O
and	O
a	O
decoding	B
system	O
that	O
make	O
it	O
possible	O
to	O
communicate	O
reliably	O
over	O
a	O
noisy	B
channel	O
during	O
the	O
remarkable	O
progress	O
was	O
made	O
towards	O
the	O
shannon	B
limit	O
using	O
codes	O
that	O
are	O
in	O
terms	O
of	O
sparse	O
random	B
graphs	O
and	O
which	O
are	O
decoded	O
by	O
a	O
simple	O
probability-based	O
message-passing	B
algorithm	O
in	O
a	O
sparse-graph	B
code	I
the	O
nodes	O
in	O
the	O
graph	B
represent	O
the	O
transmitted	O
bits	O
and	O
the	O
constraints	O
they	O
satisfy	O
for	O
a	O
linear	B
code	O
with	O
a	O
codeword	B
length	O
n	O
and	O
rate	B
r	O
kn	O
the	O
number	O
of	O
constraints	O
is	O
of	O
order	O
m	O
n	O
k	O
any	O
linear	B
code	O
can	O
be	O
described	O
by	O
a	O
graph	B
but	O
what	O
makes	O
a	O
sparse-graph	B
code	I
special	O
is	O
that	O
each	O
constraint	O
involves	O
only	O
a	O
small	O
number	O
of	O
variables	O
in	O
the	O
graph	B
so	O
the	O
number	O
of	O
edges	O
in	O
the	O
graph	B
scales	O
roughly	O
linearly	O
with	O
n	O
rather	O
than	O
quadratically	O
in	O
the	O
following	O
four	O
chapters	O
we	O
will	O
look	O
at	O
four	O
families	O
of	O
sparse-graph	O
codes	O
three	O
families	O
that	O
are	O
excellent	O
for	O
error-correction	B
low-density	O
paritycheck	O
codes	O
turbo	B
codes	I
and	O
repeataccumulate	O
codes	O
and	O
the	O
family	O
of	O
digital	B
fountain	I
codes	O
which	O
are	O
outstanding	O
for	O
erasure-correction	O
all	O
these	O
codes	O
can	O
be	O
decoded	O
by	O
a	O
local	O
message-passing	B
algorithm	O
on	O
the	O
graph	B
the	O
sumproduct	O
algorithm	O
and	O
while	O
this	O
algorithm	O
is	O
not	O
a	O
perfect	B
maximum	B
likelihood	B
decoder	B
the	O
empirical	O
results	O
are	O
record-breaking	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
h	O
figure	O
a	O
low-density	B
parity-check	B
matrix	B
and	O
the	O
corresponding	O
graph	B
of	O
a	O
low-density	B
parity-check	B
code	I
with	O
blocklength	O
n	O
and	O
m	O
constraints	O
each	O
white	B
circle	B
represents	O
a	O
transmitted	O
bit	B
each	O
bit	B
participates	O
in	O
j	O
constraints	O
represented	O
by	O
squares	O
each	O
constraint	O
forces	O
the	O
sum	O
of	O
the	O
k	O
bits	O
to	O
which	O
it	O
is	O
connected	O
to	O
be	O
even	O
a	O
low-density	B
parity-check	B
code	I
gallager	B
code	I
is	O
a	O
block	B
code	I
that	O
has	O
a	O
parity-check	B
matrix	B
h	O
every	O
row	O
and	O
column	O
of	O
which	O
is	O
sparse	O
a	O
regular	B
gallager	B
code	I
is	O
a	O
low-density	B
parity-check	B
code	I
in	O
which	O
every	O
column	O
of	O
h	O
has	O
the	O
same	O
weight	O
j	O
and	O
every	O
row	O
has	O
the	O
same	O
weight	O
k	O
regular	B
gallager	B
codes	O
are	O
constructed	O
at	O
random	B
subject	O
to	O
these	O
constraints	O
a	O
low-density	B
parity-check	B
code	I
with	O
j	O
and	O
k	O
is	O
illustrated	O
in	O
theoretical	O
properties	O
low-density	B
parity-check	I
codes	O
lend	O
themselves	O
to	O
theoretical	O
study	O
the	O
following	O
results	O
are	O
proved	O
in	O
gallager	B
and	O
mackay	B
low-density	B
parity-check	I
codes	O
in	O
spite	O
of	O
their	O
simple	O
construction	B
are	O
good	B
codes	O
given	O
an	O
optimal	B
decoder	B
codes	O
in	O
the	O
sense	O
of	O
section	B
furthermore	O
they	O
have	O
good	B
distance	B
the	O
sense	O
of	O
section	B
these	O
two	O
results	O
hold	O
for	O
any	O
column	O
weight	O
j	O
furthermore	O
there	O
are	O
sequences	O
of	O
low-density	B
parity-check	I
codes	O
in	O
which	O
j	O
increases	O
gradually	O
with	O
n	O
in	O
such	O
a	O
way	O
that	O
the	O
ratio	O
jn	O
still	O
goes	O
to	O
zero	O
that	O
are	O
very	B
good	B
and	O
that	O
have	O
very	B
good	B
distance	B
however	O
we	O
don	O
t	O
have	O
an	O
optimal	B
decoder	B
and	O
decoding	B
low-density	B
parity-check	I
codes	O
is	O
an	O
np-complete	B
problem	O
so	O
what	O
can	O
we	O
do	O
in	O
practice	O
practical	B
decoding	B
given	O
a	O
channel	O
output	O
r	O
we	O
wish	O
to	O
the	O
codeword	B
t	O
whose	O
likelihood	B
p	O
t	O
is	O
biggest	O
all	O
the	O
decoding	B
strategies	O
for	O
low-density	O
paritycheck	O
codes	O
are	O
message-passing	B
algorithms	B
the	O
best	O
algorithm	O
known	O
is	O
the	O
sumproduct	O
algorithm	O
also	O
known	O
as	O
iterative	B
probabilistic	I
decoding	B
or	O
belief	B
propagation	I
we	O
ll	O
assume	O
that	O
the	O
channel	O
is	O
a	O
memoryless	O
channel	O
more	O
complex	B
channels	O
can	O
easily	O
be	O
handled	O
by	O
running	O
the	O
sumproduct	O
algorithm	O
on	O
a	O
more	O
complex	B
graph	B
that	O
represents	O
the	O
expected	O
correlations	B
among	O
the	O
errors	B
and	O
stark	O
for	O
any	O
memoryless	O
channel	O
there	O
are	O
two	O
approaches	O
to	O
the	O
decoding	B
problem	O
both	O
of	O
which	O
lead	O
to	O
the	O
generic	O
problem	O
the	O
x	O
that	O
maximizes	O
p	O
p	O
z	O
where	O
p	O
is	O
a	O
separable	O
distribution	B
on	O
a	O
binary	O
vector	O
x	O
and	O
z	O
is	O
another	O
binary	O
vector	O
each	O
of	O
these	O
two	O
approaches	O
represents	O
the	O
decoding	B
problem	O
in	O
terms	O
of	O
a	O
factor	B
graph	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
tn	O
the	O
prior	B
distribution	B
over	O
codewords	O
p	O
p	O
j	O
tn	O
tn	O
p	O
nn	O
zm	O
the	O
variable	O
nodes	O
are	O
the	O
transmitted	O
bits	O
ftng	O
each	O
node	O
represents	O
the	O
factor	O
tn	O
mod	O
the	O
posterior	O
distribution	B
over	O
codewords	O
p	O
r	O
p	O
t	O
each	O
upper	O
function	O
node	O
represents	O
a	O
likelihood	B
factor	O
p	O
j	O
tn	O
the	O
joint	B
probability	B
of	O
the	O
noise	O
n	O
and	O
syndrome	B
z	O
p	O
z	O
p	O
hn	O
the	O
top	O
variable	O
nodes	O
are	O
now	O
the	O
noise	O
bits	O
fnng	O
the	O
added	O
variable	O
nodes	O
at	O
the	O
base	O
are	O
the	O
syndrome	B
values	O
fzmg	O
each	O
zm	O
hmnnn	O
mod	O
is	O
enforced	O
by	O
a	O
factor	O
figure	O
factor	O
graphs	O
associated	O
with	O
a	O
low-density	B
parity-check	B
code	I
the	O
codeword	B
decoding	B
viewpoint	O
first	O
we	O
note	O
that	O
the	O
prior	B
distribution	B
over	O
codewords	O
p	O
mod	O
can	O
be	O
represented	O
by	O
a	O
factor	B
graph	B
with	O
the	O
factorization	O
being	O
tn	O
mod	O
p	O
ym	O
ll	O
omit	O
the	O
mod	O
s	O
from	O
now	O
on	O
the	O
posterior	O
distribution	B
over	O
codewords	O
is	O
given	O
by	O
multiplying	O
this	O
prior	B
by	O
the	O
likelihood	B
which	O
introduces	O
another	O
n	O
factors	O
one	O
for	O
each	O
received	O
bit	B
p	O
r	O
p	O
t	O
ym	O
tn	O
yn	O
p	O
j	O
tn	O
the	O
factor	B
graph	B
corresponding	O
to	O
this	O
function	O
is	O
shown	O
in	O
it	O
is	O
the	O
same	O
as	O
the	O
graph	B
for	O
the	O
prior	B
except	O
for	O
the	O
addition	O
of	O
likelihood	B
dongles	O
to	O
the	O
transmitted	O
bits	O
in	O
this	O
viewpoint	O
the	O
received	O
signal	O
rn	O
can	O
live	O
in	O
any	O
alphabet	O
all	O
that	O
matters	O
are	O
the	O
values	O
of	O
p	O
j	O
tn	O
the	O
syndrome	B
decoding	B
viewpoint	O
alternatively	O
we	O
can	O
view	O
the	O
channel	O
output	O
in	O
terms	O
of	O
a	O
binary	O
received	O
vector	O
r	O
and	O
a	O
noise	O
vector	O
n	O
with	O
a	O
probability	B
distribution	B
p	O
that	O
can	O
be	O
derived	O
from	O
the	O
channel	O
properties	O
and	O
whatever	O
additional	O
information	B
is	O
available	O
at	O
the	O
channel	O
outputs	O
for	O
example	O
with	O
a	O
binary	B
symmetric	B
channel	I
we	O
the	O
noise	O
by	O
r	O
t	O
n	O
the	O
syndrome	B
z	O
hr	O
and	O
noise	O
model	B
p	O
f	O
for	O
other	O
channels	O
such	O
as	O
the	O
gaussian	B
channel	I
with	O
output	O
y	O
we	O
may	O
a	O
received	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decoding	B
with	O
the	O
sumproduct	O
algorithm	O
binary	O
vector	O
r	O
however	O
we	O
wish	O
and	O
obtain	O
an	O
binary	O
noise	O
model	B
p	O
from	O
y	O
and	O
the	O
joint	B
probability	B
of	O
the	O
noise	O
n	O
and	O
syndrome	B
z	O
hn	O
can	O
be	O
factored	O
as	O
p	O
z	O
p	O
hn	O
p	O
ym	O
yn	O
nn	O
the	O
factor	B
graph	B
of	O
this	O
function	O
is	O
shown	O
in	O
the	O
variables	O
n	O
and	O
z	O
can	O
also	O
be	O
drawn	O
in	O
a	O
belief	B
network	B
known	O
as	O
a	O
bayesian	B
network	B
causal	O
network	B
or	O
diagram	O
similar	O
to	O
but	O
with	O
arrows	O
on	O
the	O
edges	O
from	O
the	O
upper	O
circular	O
nodes	O
represent	O
the	O
variables	O
n	O
to	O
the	O
lower	O
square	B
nodes	O
now	O
represent	O
the	O
variables	O
z	O
we	O
can	O
say	O
that	O
every	O
bit	B
xn	O
is	O
the	O
parent	B
of	O
j	O
checks	O
zm	O
and	O
each	O
check	O
zm	O
is	O
the	O
child	O
of	O
k	O
bits	O
both	O
decoding	B
viewpoints	O
involve	O
essentially	O
the	O
same	O
graph	B
either	O
version	O
of	O
the	O
decoding	B
problem	O
can	O
be	O
expressed	O
as	O
the	O
generic	O
decoding	B
problem	O
the	O
x	O
that	O
maximizes	O
p	O
p	O
z	O
in	O
the	O
codeword	B
decoding	B
viewpoint	O
x	O
is	O
the	O
codeword	B
t	O
and	O
z	O
is	O
in	O
the	O
syndrome	B
decoding	B
viewpoint	O
x	O
is	O
the	O
noise	O
n	O
and	O
z	O
is	O
the	O
syndrome	B
it	O
doesn	O
t	O
matter	O
which	O
viewpoint	O
we	O
take	O
when	O
we	O
apply	O
the	O
sumproduct	O
algorithm	O
the	O
two	O
decoding	B
algorithms	B
are	O
isomorphic	O
and	O
will	O
give	O
equivalent	O
outcomes	O
numerical	O
errors	B
intervene	O
i	O
tend	O
to	O
use	O
the	O
syndrome	B
decoding	B
viewpoint	O
because	O
it	O
has	O
one	O
advantage	O
one	O
does	O
not	O
need	O
to	O
implement	O
an	O
encoder	B
for	O
a	O
code	O
in	O
order	O
to	O
be	O
able	O
to	O
simulate	O
a	O
decoding	B
problem	O
realistically	O
we	O
ll	O
now	O
talk	O
in	O
terms	O
of	O
the	O
generic	O
decoding	B
problem	O
decoding	B
with	O
the	O
sumproduct	O
algorithm	O
we	O
aim	O
given	O
the	O
observed	O
checks	O
to	O
compute	O
the	O
marginal	B
posterior	O
probabilities	O
p	O
z	O
h	O
for	O
each	O
n	O
it	O
is	O
hard	O
to	O
compute	O
these	O
exactly	O
because	O
the	O
graph	B
contains	O
many	O
cycles	O
however	O
it	O
is	O
interesting	O
to	O
implement	O
the	O
decoding	B
algorithm	O
that	O
would	O
be	O
appropriate	O
if	O
there	O
were	O
no	O
cycles	O
on	O
the	O
assumption	O
that	O
the	O
errors	B
introduced	O
might	O
be	O
relatively	O
small	O
this	O
approach	O
of	O
ignoring	O
cycles	O
has	O
been	O
used	O
in	O
the	O
intelligence	O
literature	O
but	O
is	O
now	O
frowned	O
upon	O
because	O
it	O
produces	O
inaccurate	O
probabilities	O
however	O
if	O
we	O
are	O
decoding	B
a	O
good	B
error-correcting	B
code	I
we	O
don	O
t	O
care	O
about	O
accurate	O
marginal	B
probabilities	O
we	O
just	O
want	O
the	O
correct	O
codeword	B
also	O
the	O
posterior	B
probability	B
in	O
the	O
case	O
of	O
a	O
good	B
code	O
communicating	O
at	O
an	O
achievable	O
rate	B
is	O
expected	O
typically	O
to	O
be	O
hugely	O
concentrated	O
on	O
the	O
most	O
probable	O
decoding	B
so	O
we	O
are	O
dealing	O
with	O
a	O
distinctive	O
probability	B
distribution	B
to	O
which	O
experience	O
gained	O
in	O
other	O
may	O
not	O
apply	O
the	O
sumproduct	O
algorithm	O
was	O
presented	O
in	O
chapter	O
we	O
now	O
write	O
out	O
explicitly	O
how	O
it	O
works	O
for	O
solving	O
the	O
decoding	B
problem	O
hx	O
z	O
for	O
brevity	O
we	O
reabsorb	O
the	O
dongles	O
hanging	O
the	O
x	O
and	O
z	O
nodes	O
in	O
and	O
modify	O
the	O
sumproduct	O
algorithm	O
accordingly	O
the	O
graph	B
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
which	O
x	O
and	O
z	O
live	O
is	O
then	O
the	O
original	O
graph	B
whose	O
edges	O
are	O
by	O
the	O
in	O
h	O
the	O
graph	B
contains	O
nodes	O
of	O
two	O
types	O
which	O
we	O
ll	O
call	O
checks	O
and	O
bits	O
the	O
graph	B
connecting	O
the	O
checks	O
and	O
bits	O
is	O
a	O
bipartite	B
graph	B
bits	O
connect	O
only	O
to	O
checks	O
and	O
vice	O
versa	O
on	O
each	O
iteration	O
a	O
probability	B
ratio	O
is	O
propagated	O
along	O
each	O
edge	B
in	O
the	O
graph	B
and	O
each	O
bit	B
node	O
xn	O
updates	O
its	O
probability	B
that	O
it	O
should	O
be	O
in	O
state	O
we	O
denote	O
the	O
set	B
of	O
bits	O
n	O
that	O
participate	O
in	O
check	O
m	O
by	O
n	O
fn	O
hmn	O
similarly	O
we	O
the	O
set	B
of	O
checks	O
in	O
which	O
bit	B
n	O
participates	O
mn	O
fm	O
hmn	O
we	O
denote	O
a	O
set	B
n	O
with	O
bit	B
n	O
excluded	O
by	O
n	O
the	O
algorithm	O
has	O
two	O
alternating	O
parts	O
in	O
which	O
quantities	O
qmn	O
and	O
rmn	O
associated	O
with	O
each	O
edge	B
in	O
the	O
graph	B
are	O
iteratively	O
updated	O
the	O
quantity	O
qx	O
mn	O
is	O
meant	O
to	O
be	O
the	O
probability	B
that	O
bit	B
n	O
of	O
x	O
has	O
the	O
value	O
x	O
given	O
the	O
information	B
obtained	O
via	O
checks	O
other	O
than	O
check	O
m	O
the	O
quantity	O
rx	O
mn	O
is	O
meant	O
to	O
be	O
the	O
probability	B
of	O
check	O
m	O
being	O
if	O
bit	B
n	O
of	O
x	O
is	O
considered	O
at	O
x	O
and	O
the	O
other	O
bits	O
have	O
a	O
separable	O
distribution	B
given	O
by	O
the	O
probabilities	O
n	O
the	O
algorithm	O
would	O
produce	O
the	O
exact	O
posterior	O
probabilities	O
of	O
all	O
the	O
bits	O
after	O
a	O
number	O
of	O
iterations	O
if	O
the	O
bipartite	B
graph	B
by	O
the	O
matrix	B
h	O
contained	O
no	O
cycles	O
n	O
p	O
initialization	O
let	O
n	O
p	O
prior	B
probability	B
that	O
bit	B
xn	O
is	O
and	O
let	O
n	O
if	O
we	O
are	O
taking	O
the	O
syndrome	B
decoding	B
viewpoint	O
and	O
the	O
channel	O
is	O
a	O
binary	B
symmetric	B
channel	I
then	O
n	O
will	O
equal	O
f	O
if	O
the	O
noise	O
level	O
varies	O
in	O
a	O
known	O
way	O
example	O
if	O
the	O
channel	O
is	O
a	O
binary-input	O
gaussian	B
channel	I
with	O
a	O
real	O
output	O
then	O
n	O
is	O
initialized	O
to	O
the	O
appropriate	O
normalized	O
likelihood	B
for	O
every	O
m	O
such	O
that	O
hmn	O
the	O
variables	O
mn	O
are	O
initialized	O
to	O
the	O
values	O
n	O
respectively	O
mn	O
and	O
n	O
and	O
horizontal	O
step	O
in	O
the	O
horizontal	O
step	O
of	O
the	O
algorithm	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
matrix	B
h	O
we	O
run	O
through	O
the	O
checks	O
m	O
and	O
compute	O
for	O
each	O
n	O
n	O
two	O
probabilities	O
mn	O
the	O
probability	B
of	O
the	O
observed	O
value	O
of	O
zm	O
arising	O
when	O
xn	O
given	O
that	O
the	O
other	O
bits	O
ng	O
have	O
a	O
separable	O
distribution	B
given	O
by	O
the	O
probabilities	O
fq	O
mn	O
j	O
xn	O
n	O
mn	O
the	O
probability	B
of	O
the	O
observed	O
value	O
of	O
zm	O
arising	O
when	O
by	O
and	O
second	O
xn	O
by	O
mn	O
j	O
xn	O
n	O
the	O
conditional	B
probabilities	O
in	O
these	O
summations	O
are	O
either	O
zero	O
or	O
one	O
depending	O
on	O
whether	O
the	O
observed	O
zm	O
matches	O
the	O
hypothesized	O
values	O
for	O
xn	O
and	O
the	O
these	O
probabilities	O
can	O
be	O
computed	O
in	O
various	O
obvious	O
ways	O
based	O
on	O
equation	O
and	O
the	O
computations	O
may	O
be	O
done	O
most	O
jn	O
is	O
large	O
by	O
regarding	O
zm	O
as	O
the	O
state	O
of	O
a	O
markov	B
chain	I
with	O
states	O
and	O
this	O
chain	O
being	O
started	O
in	O
state	O
and	O
undergoing	O
transitions	O
corresponding	O
to	O
additions	O
of	O
the	O
various	O
with	O
transition	B
probabilities	O
given	O
by	O
the	O
corresponding	O
the	O
probabilities	O
for	O
zm	O
having	O
its	O
observed	O
value	O
given	O
either	O
xn	O
or	O
xn	O
can	O
then	O
be	O
found	O
by	O
use	O
of	O
the	O
forwardbackward	O
algorithm	O
and	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decoding	B
with	O
the	O
sumproduct	O
algorithm	O
a	O
particularly	O
convenient	O
implementation	O
of	O
this	O
method	O
uses	O
forward	O
and	O
mn	O
are	O
backward	O
passes	O
in	O
which	O
products	O
of	O
the	O
computed	O
we	O
obtain	O
mn	O
from	O
the	O
identity	O
mn	O
mn	O
this	O
identity	O
is	O
derived	O
by	O
iterating	O
the	O
following	O
observation	O
mod	O
and	O
and	O
have	O
probabilities	O
then	O
p	O
p	O
we	O
recover	O
and	O
p	O
mn	O
and	O
and	O
mn	O
using	O
if	O
of	O
being	O
and	O
thus	O
p	O
mn	O
mn	O
the	O
transformations	O
into	O
and	O
back	O
from	O
to	O
frg	O
may	O
be	O
viewed	O
as	O
a	O
fourier	B
transform	I
and	O
an	O
inverse	O
fourier	O
transformation	O
vertical	O
step	O
the	O
vertical	O
step	O
takes	O
the	O
computed	O
values	O
of	O
r	O
and	O
updates	O
the	O
values	O
of	O
the	O
probabilities	O
compute	O
mn	O
and	O
mn	O
mn	O
for	O
each	O
n	O
we	O
mn	O
and	O
mn	O
mn	O
n	O
n	O
where	O
is	O
chosen	O
such	O
that	O
computed	O
in	O
a	O
downward	O
pass	O
and	O
an	O
upward	O
pass	O
mn	O
these	O
products	O
can	O
be	O
we	O
can	O
also	O
compute	O
the	O
pseudoposterior	O
probabilities	O
q	O
n	O
and	O
n	O
at	O
this	O
iteration	O
given	O
by	O
n	O
n	O
n	O
n	O
mn	O
mn	O
these	O
quantities	O
are	O
used	O
to	O
create	O
a	O
tentative	O
decoding	B
the	O
consistency	O
of	O
which	O
is	O
used	O
to	O
decide	O
whether	O
the	O
decoding	B
algorithm	O
can	O
halt	O
if	O
hx	O
z	O
at	O
this	O
point	O
the	O
algorithm	O
repeats	O
from	O
the	O
horizontal	O
step	O
the	O
stop-when-it	O
s-done	O
decoding	B
method	O
the	O
recommended	O
decoding	B
procedure	O
is	O
to	O
set	B
to	O
if	O
n	O
and	O
see	O
if	O
the	O
checks	O
hx	O
z	O
mod	O
are	O
all	O
halting	O
when	O
they	O
are	O
and	O
declaring	O
a	O
failure	O
if	O
some	O
maximum	O
number	O
of	O
iterations	O
or	O
occurs	O
without	O
successful	O
decoding	B
in	O
the	O
event	O
of	O
a	O
failure	O
we	O
may	O
still	O
report	O
but	O
we	O
the	O
whole	O
block	B
as	O
a	O
failure	O
we	O
note	O
in	O
passing	O
the	O
between	O
this	O
decoding	B
procedure	O
and	O
the	O
widespread	O
practice	O
in	O
the	O
turbo	B
code	I
community	O
where	O
the	O
decoding	B
algorithm	O
is	O
run	O
for	O
a	O
number	O
of	O
iterations	O
of	O
whether	O
the	O
decoder	B
a	O
consistent	O
state	O
at	O
some	O
earlier	O
time	O
this	O
practice	O
is	O
wasteful	O
of	O
computer	B
time	O
and	O
it	O
blurs	O
the	O
distinction	O
between	O
undetected	O
and	O
detected	O
errors	B
in	O
our	O
procedure	O
undetected	O
errors	B
occur	O
if	O
the	O
decoder	B
an	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
parity	B
bits	O
figure	O
demonstration	O
of	O
encoding	O
with	O
a	O
gallager	B
code	I
the	O
encoder	B
is	O
derived	O
from	O
a	O
very	O
sparse	O
parity-check	B
matrix	B
with	O
three	O
per	O
column	O
the	O
code	O
creates	O
transmitted	O
vectors	B
consisting	O
of	O
source	O
bits	O
and	O
paritycheck	O
bits	O
here	O
the	O
source	O
sequence	B
has	O
been	O
altered	O
by	O
changing	O
the	O
bit	B
notice	O
that	O
many	O
of	O
the	O
parity-check	B
bits	I
are	O
changed	O
each	O
parity	B
bit	B
depends	O
on	O
about	O
half	O
of	O
the	O
source	O
bits	O
the	O
transmission	O
for	O
the	O
case	O
s	O
this	O
vector	O
is	O
the	O
between	O
transmissions	O
and	O
image	B
copyright	O
united	O
feature	O
syndicate	O
inc	O
used	O
with	O
permission	O
satisfying	O
hx	O
z	O
mod	O
that	O
is	O
not	O
equal	O
to	O
the	O
true	O
x	O
detected	O
errors	B
occur	O
if	O
the	O
algorithm	O
runs	O
for	O
the	O
maximum	O
number	O
of	O
iterations	O
without	O
a	O
valid	O
decoding	B
undetected	O
errors	B
are	O
of	O
interest	O
because	O
they	O
reveal	O
distance	B
properties	O
of	O
a	O
code	O
and	O
in	O
engineering	O
practice	O
it	O
would	O
seem	O
preferable	O
for	O
the	O
blocks	O
that	O
are	O
known	O
to	O
contain	O
detected	O
errors	B
to	O
be	O
so	O
labelled	O
if	O
practically	O
possible	O
cost	O
in	O
a	O
brute-force	O
approach	O
the	O
time	O
to	O
create	O
the	O
generator	B
matrix	B
scales	O
as	O
n	O
where	O
n	O
is	O
the	O
block	B
size	O
the	O
encoding	O
time	O
scales	O
as	O
n	O
but	O
encoding	O
involves	O
only	O
binary	O
arithmetic	O
so	O
for	O
the	O
block	B
lengths	O
studied	O
here	O
it	O
takes	O
considerably	O
less	O
time	O
than	O
the	O
simulation	O
of	O
the	O
gaussian	B
channel	I
decoding	B
involves	O
approximately	O
j	O
multiplies	O
per	O
iteration	O
so	O
the	O
total	O
number	O
of	O
operations	O
per	O
decoded	O
bit	B
iterations	O
is	O
about	O
independent	O
of	O
blocklength	O
for	O
the	O
codes	O
presented	O
in	O
the	O
next	O
section	B
this	O
is	O
about	O
operations	O
the	O
encoding	O
complexity	B
can	O
be	O
reduced	O
by	O
clever	O
encoding	O
tricks	O
invented	O
by	O
richardson	B
and	O
urbanke	B
or	O
by	O
specially	O
constructing	O
the	O
paritycheck	O
matrix	B
et	O
al	O
the	O
decoding	B
complexity	B
can	O
be	O
reduced	O
with	O
only	O
a	O
small	O
loss	O
in	O
performance	O
by	O
passing	O
low-precision	O
messages	O
in	O
place	O
of	O
real	O
numbers	O
and	O
urbanke	B
pictorial	O
demonstration	O
of	O
gallager	B
codes	O
figures	O
illustrate	O
visually	O
the	O
conditions	O
under	O
which	O
low-density	B
parity-check	I
codes	O
can	O
give	O
reliable	O
communication	B
over	O
binary	B
symmetric	B
channels	O
and	O
gaussian	B
channels	O
these	O
demonstrations	O
may	O
be	O
viewed	O
as	O
animations	O
on	O
the	O
world	O
wide	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
pictorial	O
demonstration	O
of	O
gallager	B
codes	O
h	O
figure	O
a	O
low-density	B
parity-check	B
matrix	B
with	O
n	O
columns	O
of	O
weight	O
j	O
and	O
m	O
rows	O
of	O
weight	O
k	O
encoding	O
figure	O
illustrates	O
the	O
encoding	O
operation	O
for	O
the	O
case	O
of	O
a	O
gallager	B
code	I
whose	O
parity-check	B
matrix	B
is	O
a	O
matrix	B
with	O
three	O
per	O
column	O
the	O
high	O
density	B
of	O
the	O
generator	B
matrix	B
is	O
illustrated	O
in	O
and	O
c	O
by	O
showing	O
the	O
change	O
in	O
the	O
transmitted	O
vector	O
when	O
one	O
of	O
the	O
source	O
bits	O
is	O
altered	O
of	O
course	O
the	O
source	O
images	B
shown	O
here	O
are	O
highly	O
redundant	O
and	O
such	O
images	B
should	O
really	O
be	O
compressed	O
before	O
encoding	O
redundant	O
images	B
are	O
chosen	O
in	O
these	O
demonstrations	O
to	O
make	O
it	O
easier	O
to	O
see	O
the	O
correction	O
process	O
during	O
the	O
iterative	O
decoding	B
the	O
decoding	B
algorithm	O
does	O
not	O
take	O
advantage	O
of	O
the	O
redundancy	B
of	O
the	O
source	O
vector	O
and	O
it	O
would	O
work	O
in	O
exactly	O
the	O
same	O
way	O
irrespective	O
of	O
the	O
choice	O
of	O
source	O
vector	O
iterative	O
decoding	B
the	O
transmission	O
is	O
sent	O
over	O
a	O
channel	O
with	O
noise	O
level	O
f	O
and	O
the	O
received	O
vector	O
is	O
shown	O
in	O
the	O
upper	O
left	O
of	O
the	O
subsequent	O
pictures	O
in	O
show	O
the	O
iterative	B
probabilistic	I
decoding	B
process	O
the	O
sequence	B
of	O
shows	O
the	O
best	O
guess	O
bit	B
by	O
bit	B
given	O
by	O
the	O
iterative	O
decoder	B
after	O
and	O
iterations	O
the	O
decoder	B
halts	O
after	O
the	O
iteration	O
when	O
the	O
best	O
guess	O
violates	O
no	O
parity	B
checks	O
this	O
decoding	B
is	O
error	O
free	O
in	O
the	O
case	O
of	O
an	O
unusually	O
noisy	B
transmission	O
the	O
decoding	B
algorithm	O
fails	O
to	O
a	O
valid	O
decoding	B
for	O
this	O
code	O
and	O
a	O
channel	O
with	O
f	O
such	O
failures	O
happen	O
about	O
once	O
in	O
every	O
transmissions	O
figure	O
shows	O
this	O
error	O
rate	B
compared	O
with	O
the	O
block	B
error	O
rates	O
of	O
classical	O
error-correcting	B
codes	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
received	O
decoded	O
figure	O
iterative	B
probabilistic	I
decoding	B
of	O
a	O
low-density	B
parity-check	B
code	I
for	O
a	O
transmission	O
received	O
over	O
a	O
channel	O
with	O
noise	O
level	O
f	O
the	O
sequence	B
of	O
shows	O
the	O
best	O
guess	O
bit	B
by	O
bit	B
given	O
by	O
the	O
iterative	O
decoder	B
after	O
and	O
iterations	O
the	O
decoder	B
halts	O
after	O
the	O
iteration	O
when	O
the	O
best	O
guess	O
violates	O
no	O
parity	B
checks	O
this	O
decoding	B
is	O
error	O
free	O
r	O
o	O
r	O
r	O
e	O
r	O
e	O
d	O
o	O
c	O
e	O
d	O
f	O
o	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
low-density	B
parity-check	B
code	I
shannon	B
limit	O
gv	O
c	O
rate	B
figure	O
error	B
probability	B
of	O
the	O
low-density	B
parity-check	B
code	I
error	B
bars	I
for	O
binary	B
symmetric	B
channel	I
with	O
f	O
compared	O
with	O
algebraic	O
codes	O
squares	O
repetition	B
codes	O
and	O
hamming	B
code	I
other	O
points	O
reedmuller	O
and	O
bch	B
codes	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
pictorial	O
demonstration	O
of	O
gallager	B
codes	O
figure	O
demonstration	O
of	O
a	O
gallager	B
code	I
for	O
a	O
gaussian	B
channel	I
the	O
received	O
vector	O
after	O
transmission	O
over	O
a	O
gaussian	B
channel	I
with	O
db	O
the	O
greyscale	O
represents	O
the	O
value	O
of	O
the	O
normalized	O
likelihood	B
this	O
transmission	O
can	O
be	O
perfectly	O
decoded	O
by	O
the	O
sumproduct	O
decoder	B
the	O
empirical	O
probability	B
of	O
decoding	B
failure	O
is	O
about	O
the	O
probability	B
distribution	B
of	O
the	O
output	O
y	O
of	O
the	O
channel	O
with	O
for	O
each	O
of	O
the	O
two	O
possible	O
inputs	O
the	O
received	O
transmission	O
over	O
a	O
gaussian	B
channel	I
with	O
which	O
corresponds	O
to	O
the	O
shannon	B
limit	O
the	O
probability	B
distribution	B
of	O
the	O
output	O
y	O
of	O
the	O
channel	O
with	O
for	O
each	O
of	O
the	O
two	O
possible	O
inputs	O
figure	O
performance	O
of	O
gallager	B
codes	O
on	O
the	O
gaussian	B
channel	I
vertical	O
axis	O
block	B
error	B
probability	B
horizontal	O
axis	O
signal-to-noise	B
ratio	I
dependence	O
on	O
blocklength	O
n	O
for	O
k	O
codes	O
from	O
left	O
to	O
right	O
n	O
n	O
n	O
n	O
the	O
dashed	O
lines	O
show	O
the	O
frequency	B
of	O
undetected	O
errors	B
which	O
is	O
measurable	O
only	O
when	O
the	O
blocklength	O
is	O
as	O
small	O
as	O
n	O
or	O
n	O
dependence	O
on	O
column	O
weight	O
j	O
for	O
codes	O
of	O
blocklength	O
n	O
py	O
py	O
gaussian	B
channel	I
py	O
py	O
in	O
the	O
left	O
picture	O
shows	O
the	O
received	O
vector	O
after	O
transmission	O
over	O
a	O
gaussian	B
channel	I
with	O
the	O
greyscale	O
represents	O
the	O
value	O
this	O
signal-to-noise	B
ratio	I
of	O
the	O
normalized	O
likelihood	B
is	O
a	O
noise	O
level	O
at	O
which	O
this	O
gallager	B
code	I
communicates	O
reliably	O
probability	B
of	I
error	I
is	O
to	O
show	O
how	O
close	O
we	O
are	O
to	O
the	O
shannon	B
limit	O
the	O
right	O
panel	O
shows	O
the	O
received	O
vector	O
when	O
the	O
signal-tonoise	O
ratio	O
is	O
reduced	O
to	O
which	O
corresponds	O
to	O
the	O
shannon	B
limit	O
for	O
codes	O
of	O
rate	B
p	O
j	O
t	O
j	O
t	O
p	O
j	O
t	O
variation	O
of	O
performance	O
with	O
code	O
parameters	B
figure	O
shows	O
how	O
the	O
parameters	B
n	O
and	O
j	O
the	O
performance	O
of	O
low-density	B
parity-check	I
codes	O
as	O
shannon	B
would	O
predict	O
increasing	O
the	O
blocklength	O
leads	O
to	O
improved	O
performance	O
the	O
dependence	O
on	O
j	O
follows	O
a	O
pattern	O
given	O
an	O
optimal	B
decoder	B
the	O
best	O
performance	O
would	O
be	O
obtained	O
for	O
the	O
codes	O
closest	O
to	O
random	B
codes	O
that	O
is	O
the	O
codes	O
with	O
largest	O
j	O
however	O
the	O
sumproduct	O
decoder	B
makes	O
poor	O
progress	O
in	O
dense	O
graphs	O
so	O
the	O
best	O
performance	O
is	O
obtained	O
for	O
a	O
small	O
value	O
of	O
j	O
among	O
the	O
values	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
f	O
f	O
figure	O
schematic	O
illustration	O
of	O
constructions	O
of	O
a	O
completely	O
regular	B
gallager	B
code	I
with	O
j	O
k	O
and	O
r	O
of	O
a	O
nearly-regular	O
gallager	B
code	I
with	O
rate	B
notation	B
an	O
integer	O
represents	O
a	O
number	O
of	O
permutation	B
matrices	B
superposed	O
on	O
the	O
surrounding	O
square	B
a	O
diagonal	O
line	O
represents	O
an	O
identity	B
matrix	B
figure	O
monte	B
carlo	I
simulation	O
of	B
density	B
evolution	B
following	O
the	O
decoding	B
process	O
for	O
j	O
k	O
each	O
curve	O
shows	O
the	O
average	O
entropy	B
of	O
a	O
bit	B
as	O
a	O
function	O
of	O
number	O
of	O
iterations	O
as	O
estimated	O
by	O
a	O
monte	B
carlo	I
algorithm	O
using	O
samples	O
per	O
iteration	O
the	O
noise	O
level	O
of	O
the	O
binary	B
symmetric	B
channel	I
f	O
increases	O
by	O
steps	O
of	O
from	O
bottom	O
graph	B
to	O
top	O
graph	B
there	O
is	O
evidently	O
a	O
threshold	B
at	O
about	O
f	O
above	O
which	O
the	O
algorithm	O
cannot	O
determine	O
x	O
from	O
mackay	B
of	O
j	O
shown	O
in	O
the	O
j	O
is	O
the	O
best	O
for	O
a	O
blocklength	O
of	O
down	O
to	O
a	O
block	B
error	B
probability	B
of	O
this	O
observation	O
motivates	O
construction	B
of	O
gallager	B
codes	O
with	O
some	O
columns	O
of	O
weight	O
a	O
construction	B
with	O
columns	O
of	O
weight	O
is	O
shown	O
in	O
too	O
many	O
columns	O
of	O
weight	O
and	O
the	O
code	O
becomes	O
a	O
much	O
poorer	O
code	O
as	O
we	O
ll	O
discuss	O
later	O
we	O
can	O
do	O
even	O
better	O
by	O
making	O
the	O
code	O
even	O
more	O
irregular	B
density	B
evolution	B
one	O
way	O
to	O
study	O
the	O
decoding	B
algorithm	O
is	O
to	O
imagine	O
it	O
running	O
on	O
an	O
tree-like	O
graph	B
with	O
the	O
same	O
local	O
topology	O
as	O
the	O
gallager	B
code	I
s	O
graph	B
the	O
larger	O
the	O
matrix	B
h	O
the	O
closer	O
its	O
decoding	B
properties	O
should	O
approach	O
those	O
of	O
the	O
graph	B
imagine	O
an	O
belief	B
network	B
with	O
no	O
loops	O
in	O
which	O
every	O
bit	B
xn	O
connects	O
to	O
j	O
checks	O
and	O
every	O
check	O
zm	O
connects	O
to	O
k	O
bits	O
we	O
consider	O
the	O
iterative	O
of	O
information	B
in	O
this	O
network	B
and	O
examine	O
the	O
average	O
entropy	B
of	O
one	O
bit	B
as	O
a	O
function	O
of	O
number	O
of	O
iterations	O
at	O
each	O
iteration	O
a	O
bit	B
has	O
accumulated	O
information	B
from	O
its	O
local	O
network	B
out	O
to	O
a	O
radius	O
equal	O
to	O
the	O
number	O
of	O
iterations	O
successful	O
decoding	B
will	O
occur	O
only	O
if	O
the	O
average	O
entropy	B
of	O
a	O
bit	B
decreases	O
to	O
zero	O
as	O
the	O
number	O
of	O
iterations	O
increases	O
the	O
iterations	O
of	O
an	O
belief	B
network	B
can	O
be	O
simulated	O
by	O
monte	B
carlo	I
methods	I
a	O
technique	O
used	O
by	O
gallager	B
imagine	O
a	O
network	B
of	O
radius	O
i	O
total	O
number	O
of	O
iterations	O
centred	O
on	O
one	O
bit	B
our	O
aim	O
is	O
to	O
compute	O
the	O
conditional	B
entropy	B
of	O
the	O
central	O
bit	B
x	O
given	O
the	O
state	O
z	O
of	O
all	O
checks	O
out	O
to	O
radius	O
i	O
to	O
evaluate	O
the	O
probability	B
that	O
the	O
central	O
bit	B
is	O
given	O
a	O
particular	O
syndrome	B
z	O
involves	O
an	O
i-step	O
propagation	O
from	O
the	O
outside	O
of	O
the	O
network	B
into	O
the	O
centre	O
at	O
the	O
ith	O
iteration	O
probabilities	O
r	O
at	O
figure	O
local	O
topology	O
of	O
the	O
graph	B
of	O
a	O
gallager	B
code	I
with	O
column	O
weight	O
j	O
and	O
row	O
weight	O
k	O
white	B
nodes	O
represent	O
bits	O
xl	O
black	B
nodes	O
represent	O
checks	O
zm	O
each	O
edge	B
corresponds	O
to	O
a	O
in	O
h	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
improving	O
gallager	B
codes	O
radius	O
i	O
i	O
are	O
transformed	O
into	O
qs	O
and	O
then	O
into	O
rs	O
at	O
radius	O
i	O
i	O
in	O
a	O
way	O
that	O
depends	O
on	O
the	O
states	O
x	O
of	O
the	O
unknown	O
bits	O
at	O
radius	O
i	O
i	O
in	O
the	O
monte	B
carlo	I
method	O
rather	O
than	O
simulating	O
this	O
network	B
exactly	O
which	O
would	O
take	O
a	O
time	O
that	O
grows	O
exponentially	O
with	O
i	O
we	O
create	O
for	O
each	O
iteration	O
a	O
representative	O
sample	B
size	O
say	O
of	O
the	O
values	O
of	O
fr	O
xg	O
in	O
the	O
case	O
of	O
a	O
regular	B
network	B
with	O
parameters	B
j	O
k	O
each	O
new	O
pair	O
fr	O
xg	O
in	O
the	O
list	O
at	O
the	O
ith	O
iteration	O
is	O
created	O
by	O
drawing	O
the	O
new	O
x	O
from	O
its	O
distribution	B
and	O
drawing	O
at	O
random	B
with	O
replacement	O
pairs	O
fr	O
xg	O
from	O
the	O
list	O
at	O
the	O
iteration	O
these	O
are	O
assembled	O
into	O
a	O
tree	B
fragment	O
and	O
the	O
sumproduct	O
algorithm	O
is	O
run	O
from	O
top	O
to	O
bottom	O
to	O
the	O
new	O
r	O
value	O
associated	O
with	O
the	O
new	O
node	O
as	O
an	O
example	O
the	O
results	O
of	O
runs	O
with	O
j	O
k	O
and	O
noise	O
densities	O
f	O
between	O
and	O
using	O
samples	O
at	O
each	O
iteration	O
are	O
shown	O
in	O
runs	O
with	O
low	O
enough	O
noise	O
level	O
show	O
a	O
collapse	O
to	O
zero	O
entropy	B
after	O
a	O
small	O
number	O
of	O
iterations	O
and	O
those	O
with	O
high	O
noise	O
level	O
decrease	O
to	O
a	O
non-zero	O
entropy	B
corresponding	O
to	O
a	O
failure	O
to	O
decode	O
the	O
boundary	O
between	O
these	O
two	O
behaviours	O
is	O
called	O
the	O
threshold	B
of	O
the	O
decoding	B
algorithm	O
for	O
the	O
binary	B
symmetric	B
channel	I
figure	O
shows	O
by	O
monte	B
carlo	I
simulation	O
that	O
the	O
threshold	B
for	O
regular	B
k	O
codes	O
is	O
about	O
richardson	B
and	O
urbanke	B
have	O
derived	O
thresholds	O
for	O
regular	B
codes	O
by	O
a	O
tour	O
de	O
force	O
of	O
direct	O
analytic	O
methods	O
some	O
of	O
these	O
thresholds	O
are	O
shown	O
in	O
table	O
approximate	O
density	B
evolution	B
for	O
practical	B
purposes	O
the	O
computational	O
cost	O
of	B
density	B
evolution	B
can	O
be	O
reduced	O
by	O
making	O
gaussian	B
approximations	O
to	O
the	O
probability	B
distributions	O
over	O
the	O
messages	O
in	O
density	B
evolution	B
and	O
updating	O
only	O
the	O
parameters	B
of	O
these	O
approximations	O
for	O
further	O
information	B
about	O
these	O
techniques	O
which	O
produce	O
diagrams	O
known	O
as	O
exit	O
charts	O
see	O
brink	O
chung	O
et	O
al	O
ten	O
brink	O
et	O
al	O
improving	O
gallager	B
codes	O
since	O
the	O
rediscovery	O
of	O
gallager	B
codes	O
two	O
methods	O
have	O
been	O
found	O
for	O
enhancing	O
their	O
performance	O
clump	O
bits	O
and	O
checks	O
together	O
first	O
we	O
can	O
make	O
gallager	B
codes	O
in	O
which	O
the	O
variable	O
nodes	O
are	O
grouped	O
together	O
into	O
metavariables	O
consisting	O
of	O
say	O
binary	O
variables	O
and	O
the	O
check	O
nodes	O
are	O
similarly	O
grouped	O
together	O
into	O
metachecks	O
as	O
before	O
a	O
sparse	B
graph	B
can	O
be	O
constructed	O
connecting	O
metavariables	O
to	O
metachecks	O
with	O
a	O
lot	O
of	O
freedom	O
about	O
the	O
details	O
of	O
how	O
the	O
variables	O
and	O
checks	O
within	O
are	O
wired	O
up	O
one	O
way	O
to	O
set	B
the	O
wiring	O
is	O
to	O
work	O
in	O
a	O
gf	O
such	O
as	O
gf	O
or	O
gf	O
low-density	B
parity-check	I
matrices	B
using	O
elements	O
of	O
gf	O
and	O
translate	O
our	O
binary	O
messages	O
into	O
gf	O
using	O
a	O
mapping	B
such	O
as	O
the	O
one	O
for	O
gf	O
given	O
in	O
table	O
now	O
when	O
messages	O
are	O
passed	O
during	O
decoding	B
those	O
messages	O
are	O
probabilities	O
and	O
likelihoods	O
over	O
conjunctions	O
of	O
binary	O
variables	O
for	O
example	O
if	O
each	O
clump	O
contains	O
three	O
binary	O
variables	O
then	O
the	O
likelihoods	O
will	O
describe	O
the	O
likelihoods	O
of	O
the	O
eight	O
alternative	O
states	O
of	O
those	O
bits	O
with	O
carefully	O
optimized	O
constructions	O
the	O
resulting	O
codes	O
over	O
gf	O
rf	O
f	O
f	O
x	O
iteration	O
f	O
f	O
f	O
x	O
r	O
iteration	O
i	O
figure	O
a	O
tree-fragment	O
constructed	O
during	O
monte	B
carlo	I
simulation	O
of	B
density	B
evolution	B
this	O
fragment	O
is	O
appropriate	O
for	O
a	O
regular	B
j	O
k	O
gallager	B
code	I
k	O
fmax	O
table	O
thresholds	O
fmax	O
for	O
regular	B
low-density	B
parity-check	I
codes	O
assuming	O
sumproduct	O
decoding	B
algorithm	O
from	O
richardson	B
and	O
urbanke	B
the	O
shannon	B
limit	O
for	O
codes	O
is	O
fmax	O
gf	O
binary	O
a	O
b	O
table	O
translation	O
between	O
gf	O
and	O
binary	O
for	O
message	O
symbols	O
gf	O
binary	O
a	O
b	O
table	O
translation	O
between	O
gf	O
and	O
binary	O
for	O
matrix	B
entries	O
an	O
m	O
n	O
parity-check	B
matrix	B
over	O
gf	O
can	O
be	O
turned	O
into	O
a	O
binary	O
parity-check	B
matrix	B
in	O
this	O
way	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
f	O
f	O
a	O
f	O
b	O
f	O
f	O
a	O
f	O
b	O
f	O
a	O
f	O
a	O
f	O
b	O
f	O
b	O
f	O
a	O
f	O
b	O
luby	B
reg	O
irreg	O
irreg	O
reg	O
gallileo	O
turbo	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
r	O
o	O
r	O
r	O
e	O
t	O
i	O
b	O
l	O
a	O
c	O
i	O
r	O
i	O
p	O
m	O
e	O
algorithm	O
the	O
fourier	B
transform	I
over	O
gf	O
the	O
fourier	B
transform	I
f	O
of	O
a	O
function	O
f	O
over	O
gf	O
is	O
given	O
by	O
f	O
f	O
f	O
f	O
f	O
f	O
transforms	O
over	O
gf	O
can	O
be	O
viewed	O
as	O
a	O
sequence	B
of	O
binary	O
transforms	O
in	O
each	O
of	O
k	O
dimensions	B
the	O
inverse	O
transform	O
is	O
identical	O
to	O
the	O
fourier	B
transform	I
except	O
that	O
we	O
also	O
divide	O
by	O
signal	O
to	O
noise	O
ratio	O
figure	O
comparison	O
of	O
regular	B
binary	O
gallager	B
codes	O
with	O
irregular	B
codes	O
codes	O
over	O
gf	O
and	O
other	O
outstanding	O
codes	O
of	O
rate	B
from	O
left	O
performance	O
to	O
right	O
irregular	B
low-density	B
parity-check	B
code	I
over	O
gf	O
blocklength	O
bits	O
jpl	O
turbo	B
code	I
blocklength	O
regular	B
low-density	B
parity-check	I
over	O
gf	O
blocklength	O
bits	O
and	O
mackay	B
irregular	B
binary	O
low-density	O
paritycheck	O
code	O
blocklength	O
bits	O
luby	B
et	O
al	O
irregular	B
binary	O
lowdensity	O
parity-check	B
code	I
blocklength	O
bits	O
jpl	O
code	O
for	O
galileo	O
this	O
was	O
the	O
best	O
known	O
code	O
of	O
rate	B
regular	B
binary	O
low-density	B
parity-check	B
code	I
blocklength	O
bits	O
the	O
shannon	B
limit	O
is	O
at	O
about	O
db	O
as	O
of	O
even	O
better	O
sparse-graph	O
codes	O
have	O
been	O
constructed	O
gf	O
and	O
gf	O
perform	O
nearly	O
one	O
decibel	B
better	O
than	O
comparable	O
binary	O
gallager	B
codes	O
the	O
computational	O
cost	O
for	O
decoding	B
in	O
gf	O
scales	O
as	O
q	O
log	O
q	O
if	O
the	O
appropriate	O
fourier	B
transform	I
is	O
used	O
in	O
the	O
check	O
nodes	O
the	O
update	O
rule	O
for	O
the	O
check-to-variable	O
message	O
ra	O
mn	O
xxxna	O
qxj	O
mj	O
is	O
a	O
convolution	B
of	O
the	O
quantities	O
qa	O
mj	O
so	O
the	O
summation	O
can	O
be	O
replaced	O
by	O
a	O
product	O
of	O
the	O
fourier	O
transforms	O
of	O
qa	O
mj	O
for	O
j	O
n	O
followed	O
by	O
an	O
inverse	O
fourier	B
transform	I
the	O
fourier	B
transform	I
for	O
gf	O
is	O
shown	O
in	O
algorithm	O
make	O
the	O
graph	B
irregular	B
the	O
second	O
way	O
of	O
improving	O
gallager	B
codes	O
introduced	O
by	O
luby	B
et	O
al	O
is	O
to	O
make	O
their	O
graphs	O
irregular	B
instead	O
of	O
giving	O
all	O
variable	O
nodes	O
the	O
same	O
degree	B
j	O
we	O
can	O
have	O
some	O
variable	O
nodes	O
with	O
degree	B
some	O
some	O
and	O
a	O
few	O
with	O
degree	B
check	O
nodes	O
can	O
also	O
be	O
given	O
unequal	O
degrees	O
this	O
helps	O
improve	O
performance	O
on	O
erasure	B
channels	O
but	O
it	O
turns	O
out	O
that	O
for	O
the	O
gaussian	B
channel	I
the	O
best	O
graphs	O
have	O
regular	B
check	O
degrees	O
figure	O
illustrates	O
the	O
by	O
these	O
two	O
methods	O
for	O
improving	O
gallager	B
codes	O
focussing	O
on	O
codes	O
of	O
rate	B
making	O
the	O
binary	O
code	O
irregular	B
gives	O
a	O
win	O
of	O
about	O
db	O
switching	O
from	O
gf	O
to	O
gf	O
gives	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
fast	B
encoding	I
of	O
low-density	B
parity-check	I
codes	O
figure	O
an	O
algebraically	O
constructed	O
low-density	B
parity-check	B
code	I
satisfying	O
many	O
redundant	O
constraints	O
outperforms	O
an	O
equivalent	O
random	B
gallager	B
code	I
the	O
table	O
shows	O
the	O
n	O
m	O
k	O
distance	B
d	O
and	O
row	O
weight	O
k	O
of	O
some	O
cyclic	B
codes	O
highlighting	O
the	O
codes	O
that	O
have	O
large	O
dn	O
small	O
k	O
and	O
large	O
nm	O
in	O
the	O
comparison	O
the	O
gallager	B
code	I
had	O
k	O
and	O
rate	B
identical	O
to	O
the	O
n	O
cyclic	B
code	O
vertical	O
axis	O
block	B
error	B
probability	B
horizontal	O
axis	O
signal-to-noise	B
ratio	I
difference	O
set	B
cyclic	B
codes	O
n	O
m	O
k	O
d	O
k	O
about	O
db	O
and	O
matthew	O
davey	B
s	O
code	O
that	O
combines	O
both	O
these	O
features	O
it	O
s	O
irregular	B
over	O
gf	O
gives	O
a	O
win	O
of	O
about	O
db	O
over	O
the	O
regular	B
binary	O
gallager	B
code	I
methods	O
for	O
optimizing	O
the	O
of	O
a	O
gallager	B
code	I
is	O
its	O
number	O
of	O
rows	O
and	O
columns	O
of	O
each	O
degree	B
have	O
been	O
developed	O
by	O
richardson	B
et	O
al	O
and	O
have	O
led	O
to	O
low-density	B
parity-check	I
codes	O
whose	O
performance	O
when	O
decoded	O
by	O
the	O
sumproduct	O
algorithm	O
is	O
within	O
a	O
hair	O
s	O
breadth	O
of	O
the	O
shannon	B
limit	O
algebraic	O
constructions	O
of	O
gallager	B
codes	O
the	O
performance	O
of	O
regular	B
gallager	B
codes	O
can	O
be	O
enhanced	O
in	O
a	O
third	O
manner	O
by	O
designing	O
the	O
code	O
to	O
have	O
redundant	O
sparse	O
constraints	O
there	O
is	O
a	O
cyclic	B
code	O
for	O
example	O
that	O
has	O
n	O
and	O
k	O
but	O
the	O
code	O
not	O
m	O
but	O
n	O
i	O
e	O
low-weight	O
constraints	O
it	O
is	O
impossible	O
to	O
make	O
random	B
gallager	B
codes	O
that	O
have	O
anywhere	O
near	O
this	O
much	O
redundancy	B
among	O
their	O
checks	O
the	O
cyclic	B
code	O
performs	O
about	O
db	O
better	O
than	O
an	O
equivalent	O
random	B
gallager	B
code	I
an	O
open	O
problem	O
is	O
to	O
discover	O
codes	O
sharing	O
the	O
remarkable	O
properties	O
of	O
the	O
cyclic	B
codes	O
but	O
with	O
blocklengths	O
and	O
rates	O
i	O
call	O
this	O
task	O
the	O
tanner	B
challenge	I
fast	B
encoding	I
of	O
low-density	B
parity-check	I
codes	O
we	O
now	O
discuss	O
methods	O
for	O
fast	B
encoding	I
of	O
low-density	B
parity-check	I
codes	O
faster	O
than	O
the	O
standard	O
method	O
in	O
which	O
a	O
generator	B
matrix	B
g	O
is	O
found	O
by	B
gaussian	B
elimination	O
a	O
cost	O
of	O
order	O
m	O
and	O
then	O
each	O
block	B
is	O
encoded	O
by	O
multiplying	O
it	O
by	O
g	O
a	O
cost	O
of	O
order	O
m	O
staircase	B
codes	O
certain	O
low-density	B
parity-check	I
matrices	B
with	O
m	O
columns	O
of	O
weight	O
or	O
less	O
can	O
be	O
encoded	O
easily	O
in	O
linear	B
time	O
for	O
example	O
if	O
the	O
matrix	B
has	O
a	O
staircase	B
structure	O
as	O
illustrated	O
by	O
the	O
right-hand	O
side	O
of	O
h	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
and	O
if	O
the	O
data	O
s	O
are	O
loaded	O
into	O
the	O
k	O
bits	O
then	O
the	O
m	O
parity	B
bits	O
p	O
can	O
be	O
computed	O
from	O
left	O
to	O
right	O
in	O
linear	B
time	O
pk	O
pk	O
pk	O
pm	O
pk	O
hm	O
nsn	O
if	O
we	O
call	O
two	O
parts	O
of	O
the	O
h	O
matrix	B
we	O
can	O
describe	O
the	O
encoding	O
operation	O
in	O
two	O
steps	O
compute	O
an	O
intermediate	O
parity	B
vector	O
v	O
hss	O
then	O
pass	O
v	O
through	O
an	O
accumulator	B
to	O
create	O
p	O
figure	O
the	O
parity-check	B
matrix	B
in	O
approximate	O
lower-triangular	O
form	O
the	O
cost	O
of	O
this	O
encoding	O
method	O
is	O
linear	B
if	O
the	O
sparsity	O
of	O
h	O
is	O
exploited	O
when	O
computing	O
the	O
sums	O
in	O
fast	B
encoding	I
of	O
general	O
low-density	B
parity-check	I
codes	O
richardson	B
and	O
urbanke	B
demonstrated	O
an	O
elegant	O
method	O
by	O
which	O
the	O
encoding	O
cost	O
of	O
any	O
low-density	B
parity-check	B
code	I
can	O
be	O
reduced	O
from	O
the	O
straightforward	O
method	O
s	O
m	O
to	O
a	O
cost	O
of	O
n	O
where	O
g	O
the	O
gap	O
is	O
hopefully	O
a	O
small	O
constant	O
and	O
in	O
the	O
worst	O
cases	O
scales	O
as	O
a	O
small	O
fraction	O
of	O
n	O
m	O
a	O
c	O
b	O
d	O
n	O
t	O
e	O
m	O
g	O
in	O
the	O
step	O
the	O
parity-check	B
matrix	B
is	O
rearranged	O
by	O
row-interchange	O
and	O
column-interchange	O
into	O
the	O
approximate	O
lower-triangular	O
form	O
shown	O
in	O
the	O
original	O
matrix	B
h	O
was	O
very	O
sparse	O
so	O
the	O
six	B
matrices	B
a	O
b	O
t	O
c	O
d	O
and	O
e	O
are	O
also	O
very	O
sparse	O
the	O
matrix	B
t	O
is	O
lower	O
triangular	O
and	O
has	O
everywhere	O
on	O
the	O
diagonal	O
h	O
a	O
b	O
t	O
c	O
d	O
e	O
the	O
source	O
vector	O
s	O
of	O
length	O
k	O
n	O
m	O
is	O
encoded	O
into	O
a	O
transmission	O
t	O
as	O
follows	O
compute	O
the	O
upper	O
syndrome	B
of	O
the	O
source	O
vector	O
za	O
as	O
this	O
can	O
be	O
done	O
in	O
linear	B
time	O
find	O
a	O
setting	O
of	O
the	O
second	O
parity	B
bits	O
pa	O
such	O
that	O
the	O
upper	O
syn	O
drome	O
is	O
zero	O
pa	O
this	O
vector	O
can	O
be	O
found	O
in	O
linear	B
time	O
by	O
back-substitution	O
i	O
e	O
computing	O
the	O
bit	B
of	O
pa	O
then	O
the	O
second	O
then	O
the	O
third	O
and	O
so	O
forth	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
reading	O
compute	O
the	O
lower	O
syndrome	B
of	O
the	O
vector	O
pa	O
zb	O
cs	O
epa	O
this	O
can	O
be	O
done	O
in	O
linear	B
time	O
now	O
we	O
get	O
to	O
the	O
clever	O
bit	B
the	O
matrix	B
f	O
d	O
and	O
its	O
inverse	O
this	O
computation	O
needs	O
to	O
be	O
done	O
once	O
only	O
and	O
its	O
cost	O
is	O
of	O
order	O
this	O
inverse	O
is	O
a	O
dense	O
g	O
matrix	B
f	O
is	O
not	O
invertible	O
then	O
either	O
h	O
is	O
not	O
of	O
full	O
rank	O
or	O
else	O
further	O
column	O
permutations	O
of	O
h	O
can	O
produce	O
an	O
f	O
that	O
is	O
invertible	O
set	B
the	O
parity	B
bits	O
to	O
this	O
operation	O
has	O
a	O
cost	O
of	O
order	O
claim	O
at	O
this	O
point	O
we	O
have	O
found	O
the	O
correct	O
setting	O
of	O
the	O
parity	B
bits	O
discard	O
the	O
tentative	O
parity	B
bits	O
pa	O
and	O
the	O
new	O
upper	O
syndrome	B
zc	O
za	O
this	O
can	O
be	O
done	O
in	O
linear	B
time	O
find	O
a	O
setting	O
of	O
the	O
second	O
parity	B
bits	O
such	O
that	O
the	O
upper	O
syndrome	B
is	O
zero	O
this	O
vector	O
can	O
be	O
found	O
in	O
linear	B
time	O
by	O
back-substitution	O
further	O
reading	O
low-density	B
parity-check	I
codes	O
codes	O
were	O
studied	O
in	O
by	O
gallager	B
then	O
were	O
generally	O
forgotten	O
by	O
the	O
coding	B
theory	I
community	O
tanner	B
generalized	B
gallager	B
s	O
work	O
by	O
introducing	O
more	O
general	O
constraint	O
nodes	O
the	O
codes	O
that	O
are	O
now	O
called	O
turbo	O
product	O
codes	O
should	O
in	O
fact	O
be	O
called	O
tanner	B
product	O
codes	O
since	O
tanner	B
proposed	O
them	O
and	O
his	O
colleagues	O
and	O
krit	O
implemented	O
them	O
in	O
hardware	O
publications	O
on	O
gallager	B
codes	O
contributing	O
to	O
their	O
rebirth	O
include	O
et	O
al	O
mackay	B
and	O
neal	B
mackay	B
and	O
neal	B
wiberg	B
mackay	B
spielman	B
sipser	O
and	O
spielman	B
low-precision	O
decoding	B
algorithms	B
and	O
fast	B
encoding	I
algorithms	B
for	O
gallager	B
codes	O
are	O
discussed	O
in	O
and	O
urbanke	B
richardson	B
and	O
urbanke	B
mackay	B
and	O
davey	B
showed	O
that	O
low-density	B
parity-check	I
codes	O
can	O
outperform	O
reedsolomon	O
codes	O
even	O
on	O
the	O
reedsolomon	O
codes	O
home	O
turf	O
high	O
rate	B
and	O
short	O
blocklengths	O
other	O
important	O
papers	O
include	O
et	O
al	O
luby	B
et	O
al	O
luby	B
et	O
al	O
davey	B
and	O
mackay	B
richardson	B
et	O
al	O
chung	O
et	O
al	O
useful	O
tools	O
for	O
the	O
design	O
of	O
irregular	B
low-density	O
paritycheck	O
codes	O
include	O
et	O
al	O
urbanke	B
see	O
frey	B
mceliece	O
et	O
al	O
for	O
further	O
discussion	O
of	O
the	O
sumproduct	O
algorithm	O
for	O
a	O
view	O
of	O
low-density	B
parity-check	B
code	I
decoding	B
in	O
terms	O
of	O
group	O
theory	O
and	O
coding	B
theory	I
see	O
and	O
soljanin	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
low-density	B
parity-check	I
codes	O
and	O
soljanin	O
and	O
for	O
background	O
reading	O
on	O
this	O
topic	O
see	O
and	O
rudolph	O
terras	O
there	O
is	O
a	O
growing	O
literature	O
on	O
the	O
practical	B
design	O
of	O
low-density	B
parity-check	I
codes	O
and	O
banihashemi	O
mao	O
and	O
banihashemi	O
ten	O
brink	O
et	O
al	O
they	O
are	O
now	O
being	O
adopted	O
for	O
applications	O
from	O
hard	O
drives	O
to	O
satellite	B
communications	I
for	O
low-density	B
parity-check	I
codes	O
applicable	O
to	O
quantum	B
error-correction	B
see	O
mackay	B
et	O
al	O
exercises	O
exercise	O
the	O
hyperbolic	O
tangent	O
version	O
of	O
the	O
decoding	B
algorithm	O
in	O
section	B
the	O
sumproduct	O
decoding	B
algorithm	O
for	O
low-density	O
paritycheck	O
codes	O
was	O
presented	O
in	O
terms	O
of	O
quantities	O
q	O
mn	O
then	O
in	O
terms	O
of	O
quantities	O
and	O
there	O
is	O
a	O
third	O
description	O
in	O
which	O
the	O
fqg	O
are	O
replaced	O
by	O
log	O
probability-ratios	O
mn	O
and	O
lmn	O
ln	O
mn	O
mn	O
show	O
that	O
mn	O
mn	O
derive	O
the	O
update	O
rules	B
for	O
frg	O
and	O
flg	O
exercise	O
i	O
am	O
sometimes	O
asked	O
why	O
not	O
decode	O
other	O
linear	B
codes	I
for	O
example	O
algebraic	O
codes	O
by	O
transforming	O
their	O
parity-check	O
matrices	B
so	O
that	O
they	O
are	O
low-density	O
and	O
applying	O
the	O
sumproduct	O
algorithm	O
that	O
any	O
linear	B
combination	B
of	O
rows	O
of	O
h	O
ph	O
is	O
a	O
valid	O
parity-check	B
matrix	B
for	O
a	O
code	O
as	O
long	O
as	O
the	O
matrix	B
p	O
is	O
invertible	O
so	O
there	O
are	O
many	O
parity	B
check	O
matrices	B
for	O
any	O
one	O
code	O
explain	O
why	O
a	O
random	B
linear	B
code	O
does	O
not	O
have	O
a	O
low-density	O
paritycheck	O
matrix	B
low-density	O
means	O
having	O
row-weight	O
at	O
most	O
k	O
where	O
k	O
is	O
some	O
small	O
constant	O
n	O
exercise	O
show	O
that	O
if	O
a	O
low-density	B
parity-check	B
code	I
has	O
more	O
than	O
m	O
columns	O
of	O
weight	O
say	O
columns	O
where	O
then	O
the	O
code	O
will	O
have	O
words	O
with	O
weight	O
of	O
order	O
log	O
m	O
exercise	O
in	O
section	B
we	O
found	O
the	O
expected	O
value	O
of	O
the	O
weight	B
enumerator	I
function	O
aw	O
averaging	O
over	O
the	O
ensemble	B
of	O
all	O
random	B
linear	B
codes	I
this	O
calculation	O
can	O
also	O
be	O
carried	O
out	O
for	O
the	O
ensemble	B
of	O
low-density	B
parity-check	I
codes	O
mackay	B
litsyn	B
and	O
shevelev	O
it	O
is	O
plausible	O
however	O
that	O
the	O
mean	B
value	O
of	O
aw	O
is	O
not	O
always	O
a	O
good	B
indicator	O
of	O
the	O
typical	B
value	O
of	O
aw	O
in	O
the	O
ensemble	B
for	O
example	O
if	O
at	O
a	O
particular	O
value	O
of	O
w	O
of	O
codes	O
have	O
aw	O
and	O
have	O
aw	O
then	O
while	O
we	O
might	O
say	O
the	O
typical	B
value	O
of	O
aw	O
is	O
zero	O
the	O
mean	B
is	O
found	O
to	O
be	O
find	O
the	O
typical	B
weight	B
enumerator	I
function	O
of	O
low-density	B
parity-check	I
codes	O
solutions	O
solution	O
to	O
exercise	O
consider	O
codes	O
of	O
rate	B
r	O
and	O
blocklength	O
n	O
having	O
k	O
rn	O
source	O
bits	O
and	O
m	O
parity-check	B
bits	I
let	O
all	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
solutions	O
the	O
codes	O
have	O
their	O
bits	O
ordered	O
so	O
that	O
the	O
k	O
bits	O
are	O
independent	O
so	O
that	O
we	O
could	O
if	O
we	O
wish	O
put	O
the	O
code	O
in	O
systematic	B
form	O
g	O
h	O
the	O
number	O
of	O
distinct	O
linear	B
codes	I
is	O
the	O
number	O
of	O
matrices	B
p	O
which	O
is	O
k	O
can	O
these	O
all	O
be	O
expressed	O
as	O
distinct	O
low-density	B
parity-check	I
codes	O
the	O
number	O
of	O
low-density	B
parity-check	I
matrices	B
with	O
row-weight	O
k	O
is	O
log	O
n	O
r	O
and	O
the	O
number	O
of	O
distinct	O
codes	O
that	O
they	O
is	O
at	O
most	O
m	O
which	O
is	O
much	O
smaller	O
than	O
so	O
by	O
the	O
pigeon-hole	B
principle	I
it	O
is	O
not	O
possible	O
for	O
every	O
random	B
linear	B
code	O
to	O
map	O
on	O
to	O
a	O
low-density	O
h	O
log	O
n	O
k	O
log	O
n	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
this	O
chapter	O
follows	O
tightly	O
on	O
from	O
chapter	O
it	O
makes	O
use	O
of	O
the	O
ideas	O
of	O
codes	O
and	O
trellises	O
and	O
the	O
forwardbackward	O
algorithm	O
introduction	O
to	O
convolutional	B
codes	O
when	O
we	O
studied	O
linear	B
block	B
codes	O
we	O
described	O
them	O
in	O
three	O
ways	O
the	O
generator	B
matrix	B
describes	O
how	O
to	O
turn	O
a	O
string	O
of	O
k	O
arbitrary	O
source	O
bits	O
into	O
a	O
transmission	O
of	O
n	O
bits	O
the	O
parity-check	B
matrix	B
the	O
m	O
n	O
k	O
parity-check	O
con	O
straints	O
that	O
a	O
valid	O
codeword	B
the	O
trellis	B
of	O
the	O
code	O
describes	O
its	O
valid	O
codewords	O
in	O
terms	O
of	O
paths	O
through	O
a	O
trellis	B
with	O
labelled	O
edges	O
a	O
fourth	O
way	O
of	O
describing	O
some	O
block	B
codes	O
the	O
algebraic	O
approach	O
is	O
not	O
covered	O
in	O
this	O
book	O
because	O
it	O
has	O
been	O
well	O
covered	O
by	O
numerous	O
other	O
books	O
in	O
coding	B
theory	I
because	O
as	O
this	O
part	O
of	O
the	O
book	O
discusses	O
the	O
state-of-the-art	O
in	O
error-correcting	B
codes	I
makes	O
little	O
use	O
of	O
algebraic	B
coding	B
theory	I
and	O
because	O
i	O
am	O
not	O
competent	O
to	O
teach	O
this	O
subject	O
we	O
will	O
now	O
describe	O
convolutional	B
codes	O
in	O
two	O
ways	O
in	O
terms	O
of	O
mechanisms	O
for	O
generating	O
transmissions	O
t	O
from	O
source	O
bits	O
s	O
and	O
second	O
in	O
terms	O
of	O
trellises	O
that	O
describe	O
the	O
constraints	O
by	O
valid	O
transmissions	O
linear-feedback	O
shift-registers	O
we	O
generate	O
a	O
transmission	O
with	O
a	O
convolutional	B
code	I
by	O
putting	O
a	O
source	O
stream	O
through	O
a	O
linear	B
this	O
makes	O
use	O
of	O
a	O
shift	O
register	O
linear	B
output	O
functions	B
and	O
possibly	O
linear	B
feedback	B
i	O
will	O
draw	O
the	O
shift-register	O
in	O
a	O
right-to-left	O
orientation	O
bits	O
roll	O
from	O
right	O
to	O
left	O
as	O
time	O
goes	O
on	O
figure	O
shows	O
three	O
linear-feedback	O
shift-registers	O
which	O
could	O
be	O
used	O
to	O
convolutional	B
codes	O
the	O
rectangular	B
box	B
surrounding	O
the	O
bits	O
indicates	O
the	O
memory	B
of	O
the	O
also	O
known	O
as	O
its	O
state	O
all	O
three	O
have	O
one	O
input	O
and	O
two	O
outputs	O
on	O
each	O
clock	O
cycle	O
the	O
source	O
supplies	O
one	O
bit	B
and	O
the	O
outputs	O
two	O
bits	O
ta	O
and	O
tb	O
by	O
concatenating	O
together	O
these	O
bits	O
we	O
can	O
obtain	O
from	O
our	O
source	O
stream	O
a	O
transmission	O
stream	O
ta	O
because	O
there	O
are	O
two	O
transmitted	O
bits	O
for	O
every	O
source	O
bit	B
the	O
codes	O
shown	O
in	O
have	O
rate	B
because	O
tb	O
ta	O
tb	O
ta	O
tb	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
linear-feedback	O
shift-registers	O
figure	O
linear-feedback	O
shift-registers	O
for	O
generating	O
convolutional	B
codes	O
with	O
rate	B
the	O
symbol	O
hd	O
indicates	O
a	O
copying	O
with	O
a	O
delay	O
of	O
one	O
clock	O
cycle	O
the	O
symbol	O
denotes	O
linear	B
addition	O
modulo	O
with	O
no	O
delay	O
the	O
are	O
systematic	B
and	O
nonrecursive	B
nonsystematic	O
and	O
nonrecursive	B
systematic	B
and	O
recursive	O
table	O
how	O
taps	O
in	O
the	O
delay	B
line	I
are	O
converted	O
to	O
octal	B
hd	O
octal	B
name	O
ta	O
s	O
tb	O
tb	O
hd	O
s	O
ta	O
hd	O
tb	O
ta	O
s	O
these	O
require	O
k	O
bits	O
of	O
memory	B
the	O
codes	O
they	O
are	O
known	O
as	O
a	O
constraint-length	O
codes	O
convolutional	B
codes	O
come	O
in	O
three	O
corresponding	O
to	O
the	O
three	O
types	O
of	O
in	O
systematic	B
nonrecursive	B
the	O
shown	O
in	O
has	O
no	O
feedback	B
it	O
also	O
has	O
the	O
property	O
that	O
one	O
of	O
the	O
output	O
bits	O
ta	O
is	O
identical	O
to	O
the	O
source	O
bit	B
s	O
this	O
encoder	B
is	O
thus	O
called	O
systematic	B
because	O
the	O
source	O
bits	O
are	O
reproduced	O
transparently	O
in	O
the	O
transmitted	O
stream	O
and	O
nonrecursive	B
because	O
it	O
has	O
no	O
feedback	B
the	O
other	O
transmitted	O
bit	B
tb	O
is	O
a	O
linear	B
function	O
of	O
the	O
state	O
of	O
the	O
one	O
way	O
of	O
describing	O
that	O
function	O
is	O
as	O
a	O
dot	O
product	O
between	O
two	O
binary	O
vectors	B
of	O
length	O
k	O
a	O
binary	O
vector	O
gb	O
and	O
the	O
state	O
vector	O
z	O
we	O
include	O
in	O
the	O
state	O
vector	O
the	O
bit	B
that	O
will	O
be	O
put	O
into	O
the	O
bit	B
of	O
the	O
memory	B
on	O
the	O
next	O
cycle	O
the	O
vector	O
gb	O
has	O
gb	O
for	O
every	O
where	O
there	O
is	O
a	O
tap	B
downward	O
pointing	O
arrow	O
from	O
state	O
bit	B
into	O
the	O
transmitted	O
bit	B
tb	O
a	O
convenient	O
way	O
to	O
describe	O
these	O
binary	O
tap	B
vectors	B
is	O
in	O
octal	B
thus	O
i	O
have	O
drawn	O
the	O
delay	O
lines	O
from	O
this	O
makes	O
use	O
of	O
the	O
tap	B
vector	O
right	O
to	O
left	O
to	O
make	O
it	O
easy	O
to	O
relate	O
the	O
diagrams	O
to	O
these	O
octal	B
numbers	O
nonsystematic	O
nonrecursive	B
the	O
shown	O
in	O
also	O
has	O
no	O
feedback	B
but	O
it	O
is	O
not	O
systematic	B
it	O
makes	O
use	O
of	O
two	O
tap	B
vectors	B
ga	O
and	O
gb	O
to	O
create	O
its	O
two	O
transmitted	O
bits	O
this	O
encoder	B
is	O
thus	O
nonsystematic	O
and	O
nonrecursive	B
because	O
of	O
their	O
added	O
complexity	B
nonsystematic	O
codes	O
can	O
have	O
error-correcting	O
abilities	O
superior	O
to	O
those	O
of	O
systematic	B
nonrecursive	B
codes	O
with	O
the	O
same	O
constraint	O
length	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
systematic	B
recursive	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
the	O
shown	O
in	O
is	O
similar	O
to	O
the	O
nonsystematic	O
nonrecursive	B
shown	O
in	O
but	O
it	O
uses	O
the	O
taps	O
that	O
formerly	O
made	O
up	O
ga	O
to	O
make	O
a	O
linear	B
signal	O
that	O
is	O
fed	O
back	O
into	O
the	O
shift	O
register	O
along	O
with	O
the	O
source	O
bit	B
the	O
output	O
tb	O
is	O
a	O
linear	B
function	O
of	O
the	O
state	O
vector	O
as	O
before	O
the	O
other	O
output	O
is	O
ta	O
s	O
so	O
this	O
is	O
systematic	B
a	O
recursive	O
code	O
is	O
conventionally	O
by	O
an	O
octal	B
ratio	O
e	O
g	O
ure	O
s	O
code	O
is	O
denoted	O
by	O
equivalence	B
of	O
systematic	B
recursive	O
and	O
nonsystematic	O
nonrecursive	B
codes	O
the	O
two	O
in	O
are	O
code-equivalent	B
in	O
that	O
the	O
sets	O
of	O
codewords	O
that	O
they	O
are	O
identical	O
for	O
every	O
codeword	B
of	O
the	O
nonsystematic	O
nonrecursive	B
code	O
we	O
can	O
choose	O
a	O
source	O
stream	O
for	O
the	O
other	O
encoder	B
such	O
that	O
its	O
output	O
is	O
identical	O
vice	O
versa	O
ga	O
to	O
prove	O
this	O
we	O
denote	O
by	O
p	O
the	O
quantity	O
pk	O
as	O
shown	O
in	O
and	O
b	O
which	O
shows	O
a	O
pair	O
of	O
smaller	O
but	O
otherwise	O
equivalent	O
if	O
the	O
two	O
transmissions	O
are	O
to	O
be	O
equivalent	O
that	O
is	O
the	O
tas	O
are	O
equal	O
in	O
both	O
and	O
so	O
are	O
the	O
tbs	O
then	O
on	O
every	O
cycle	O
the	O
source	O
bit	B
in	O
the	O
systematic	B
code	O
must	O
be	O
s	O
ta	O
so	O
now	O
we	O
must	O
simply	O
that	O
for	O
this	O
choice	O
of	O
s	O
the	O
systematic	B
code	O
s	O
shift	O
register	O
will	O
follow	O
the	O
same	O
state	O
sequence	B
as	O
that	O
of	O
the	O
nonsystematic	O
code	O
assuming	O
that	O
the	O
states	O
match	O
initially	O
in	O
we	O
have	O
tb	O
hd	O
s	O
ta	O
tb	O
hd	O
ta	O
s	O
figure	O
two	O
convolutional	B
codes	O
with	O
constraint	O
length	O
k	O
non-recursive	O
recursive	O
the	O
two	O
codes	O
are	O
equivalent	O
ta	O
p	O
znonrecursive	O
whereas	O
in	O
we	O
have	O
zrecursive	O
ta	O
p	O
substituting	O
for	O
ta	O
and	O
using	O
p	O
p	O
we	O
immediately	O
zrecursive	O
znonrecursive	O
thus	O
any	O
codeword	B
of	O
a	O
nonsystematic	O
nonrecursive	B
code	O
is	O
a	O
codeword	B
of	O
a	O
systematic	B
recursive	O
code	O
with	O
the	O
same	O
taps	O
the	O
same	O
taps	O
in	O
the	O
sense	O
that	O
there	O
are	O
vertical	O
arrows	O
in	O
all	O
the	O
same	O
places	O
in	O
and	O
though	O
one	O
of	O
the	O
arrows	O
points	O
up	O
instead	O
of	O
down	O
in	O
now	O
while	O
these	O
two	O
codes	O
are	O
equivalent	O
the	O
two	O
encoders	O
behave	O
differently	O
the	O
nonrecursive	B
encoder	B
has	O
a	O
impulse	O
response	O
that	O
is	O
if	O
one	O
puts	O
in	O
a	O
string	O
that	O
is	O
all	O
zeroes	O
except	O
for	O
a	O
single	O
one	O
the	O
resulting	O
output	O
stream	O
contains	O
a	O
number	O
of	O
ones	O
once	O
the	O
one	O
bit	B
has	O
passed	O
through	O
all	O
the	O
states	O
of	O
the	O
memory	B
the	O
delay	B
line	I
returns	O
to	O
the	O
all-zero	O
state	O
figure	O
shows	O
the	O
state	O
sequence	B
resulting	O
from	O
the	O
source	O
string	O
s	O
figure	O
shows	O
the	O
trellis	B
of	O
the	O
recursive	O
code	O
of	O
and	O
the	O
response	O
of	O
this	O
to	O
the	O
same	O
source	O
string	O
s	O
the	O
has	O
an	O
impulse	O
response	O
the	O
response	O
settles	O
into	O
a	O
periodic	O
state	O
with	O
period	O
equal	O
to	O
three	O
clock	O
cycles	O
exercise	O
what	O
is	O
the	O
input	O
to	O
the	O
recursive	O
such	O
that	O
its	O
state	O
sequence	B
and	O
the	O
transmission	O
are	O
the	O
same	O
as	O
those	O
of	O
the	O
nonrecursive	B
see	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
linear-feedback	O
shift-registers	O
transmit	O
source	O
transmit	O
source	O
transmit	O
source	O
figure	O
trellises	O
of	O
the	O
convolutional	B
codes	O
of	O
it	O
is	O
assumed	O
that	O
the	O
initial	O
state	O
of	O
the	O
is	O
time	O
is	O
on	O
the	O
horizontal	O
axis	O
and	O
the	O
state	O
of	O
the	O
at	O
each	O
time	O
step	O
is	O
the	O
vertical	O
coordinate	O
on	O
the	O
line	O
segments	O
are	O
shown	O
the	O
emitted	O
symbols	O
ta	O
and	O
tb	O
with	O
stars	B
for	O
and	O
boxes	O
for	O
the	O
paths	O
taken	O
through	O
the	O
trellises	O
when	O
the	O
source	O
sequence	B
is	O
are	O
highlighted	O
with	O
a	O
solid	O
line	O
the	O
light	O
dotted	O
lines	O
show	O
the	O
state	O
trajectories	O
that	O
are	O
possible	O
for	O
other	O
source	O
sequences	O
figure	O
the	O
source	O
sequence	B
for	O
the	O
systematic	B
recursive	O
code	O
produces	O
the	O
same	O
path	O
through	O
the	O
trellis	B
as	O
does	O
in	O
the	O
nonsystematic	O
nonrecursive	B
case	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
hd	O
tb	O
ta	O
s	O
figure	O
the	O
trellis	B
for	O
a	O
k	O
code	O
painted	O
with	O
the	O
likelihood	B
function	O
when	O
the	O
received	O
vector	O
is	O
equal	O
to	O
a	O
codeword	B
with	O
just	O
one	O
bit	B
there	O
are	O
three	O
line	O
styles	O
depending	O
on	O
the	O
value	O
of	O
the	O
likelihood	B
thick	O
solid	O
lines	O
show	O
the	O
edges	O
in	O
the	O
trellis	B
that	O
match	O
the	O
corresponding	O
two	O
bits	O
of	O
the	O
received	O
string	O
exactly	O
thick	O
dotted	O
lines	O
show	O
edges	O
that	O
match	O
one	O
bit	B
but	O
mismatch	O
the	O
other	O
and	O
thin	O
dotted	O
lines	O
show	O
the	O
edges	O
that	O
mismatch	O
both	O
bits	O
received	O
in	O
general	O
a	O
linear-feedback	B
shift-register	I
with	O
k	O
bits	O
of	O
memory	B
has	O
an	O
impulse	O
response	O
that	O
is	O
periodic	O
with	O
a	O
period	O
that	O
is	O
at	O
most	O
corresponding	O
to	O
the	O
visiting	O
every	O
non-zero	O
state	O
in	O
its	O
state	O
space	O
incidentally	O
cheap	O
pseudorandom	O
number	O
generators	O
and	O
cheap	O
cryptographic	O
products	O
make	O
use	O
of	O
exactly	O
these	O
periodic	O
sequences	O
though	O
with	O
larger	O
values	O
of	O
k	O
than	O
the	O
random	B
number	O
seed	O
or	O
cryptographic	O
key	O
selects	O
the	O
initial	O
state	O
of	O
the	O
memory	B
there	O
is	O
thus	O
a	O
close	O
connection	O
between	O
certain	O
cryptanalysis	B
problems	O
and	O
the	O
decoding	B
of	O
convolutional	B
codes	O
decoding	B
convolutional	B
codes	O
the	O
receiver	O
receives	O
a	O
bit	B
stream	O
and	O
wishes	O
to	O
infer	O
the	O
state	O
sequence	B
and	O
thence	O
the	O
source	O
stream	O
the	O
posterior	B
probability	B
of	O
each	O
bit	B
can	O
be	O
found	O
by	O
the	O
sumproduct	O
algorithm	O
known	O
as	O
the	O
forwardbackward	O
or	O
bcjr	B
algorithm	I
which	O
was	O
introduced	O
in	O
section	B
the	O
most	O
probable	O
state	O
sequence	B
can	O
be	O
found	O
using	O
the	O
minsum	O
algorithm	O
of	O
section	B
known	O
as	O
the	O
viterbi	B
algorithm	I
the	O
nature	O
of	O
this	O
task	O
is	O
illustrated	O
in	O
which	O
shows	O
the	O
cost	O
associated	O
with	O
each	O
edge	B
in	O
the	O
trellis	B
for	O
the	O
case	O
of	O
a	O
sixteen-state	O
code	O
the	O
channel	O
is	O
assumed	O
to	O
be	O
a	O
binary	B
symmetric	B
channel	I
and	O
the	O
received	O
vector	O
is	O
equal	O
to	O
a	O
codeword	B
except	O
that	O
one	O
bit	B
has	O
been	O
there	O
are	O
three	O
line	O
styles	O
depending	O
on	O
the	O
value	O
of	O
the	O
likelihood	B
thick	O
solid	O
lines	O
show	O
the	O
edges	O
in	O
the	O
trellis	B
that	O
match	O
the	O
corresponding	O
two	O
bits	O
of	O
the	O
received	O
string	O
exactly	O
thick	O
dotted	O
lines	O
show	O
edges	O
that	O
match	O
one	O
bit	B
but	O
mismatch	O
the	O
other	O
and	O
thin	O
dotted	O
lines	O
show	O
the	O
edges	O
that	O
mismatch	O
both	O
bits	O
the	O
minsum	O
algorithm	O
seeks	O
the	O
path	O
through	O
the	O
trellis	B
that	O
uses	O
as	O
many	O
solid	O
lines	O
as	O
possible	O
more	O
precisely	O
it	O
minimizes	O
the	O
cost	O
of	O
the	O
path	O
where	O
the	O
cost	O
is	O
zero	O
for	O
a	O
solid	O
line	O
one	O
for	O
a	O
thick	O
dotted	O
line	O
and	O
two	O
for	O
a	O
thin	O
dotted	O
line	O
exercise	O
can	O
you	O
spot	O
the	O
most	O
probable	O
path	O
and	O
the	O
bit	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
figure	O
two	O
paths	O
that	O
in	O
two	O
transmitted	O
bits	O
only	O
figure	O
a	O
terminated	O
trellis	B
when	O
any	O
codeword	B
is	O
completed	O
the	O
state	O
is	O
turbo	B
codes	I
transmit	O
source	O
transmit	O
source	O
unequal	O
protection	O
a	O
defect	O
of	O
the	O
convolutional	B
codes	O
presented	O
thus	O
far	O
is	O
that	O
they	O
unequal	O
protection	O
to	O
the	O
source	O
bits	O
figure	O
shows	O
two	O
paths	O
through	O
the	O
trellis	B
that	O
in	O
only	O
two	O
transmitted	O
bits	O
the	O
last	O
source	O
bit	B
is	O
less	O
well	O
protected	O
than	O
the	O
other	O
source	O
bits	O
this	O
unequal	O
protection	O
of	O
bits	O
motivates	O
the	O
termination	B
of	O
the	O
trellis	B
a	O
terminated	O
trellis	B
is	O
shown	O
in	O
termination	B
slightly	O
reduces	O
the	O
number	O
of	O
source	O
bits	O
used	O
per	O
codeword	B
here	O
four	O
source	O
bits	O
are	O
turned	O
into	O
parity	B
bits	O
because	O
the	O
k	O
memory	B
bits	O
must	O
be	O
returned	O
to	O
zero	O
turbo	B
codes	I
an	O
k	O
turbo	B
code	I
is	O
by	O
a	O
number	O
of	O
constituent	O
convolutional	B
encoders	O
two	O
and	O
an	O
equal	O
number	O
of	O
interleavers	O
which	O
are	O
k	O
k	O
permutation	B
matrices	B
without	O
loss	O
of	O
generality	O
we	O
take	O
the	O
interleaver	O
to	O
be	O
the	O
identity	B
matrix	B
a	O
string	O
of	O
k	O
source	O
bits	O
is	O
encoded	O
by	O
feeding	O
them	O
into	O
each	O
constituent	O
encoder	B
in	O
the	O
order	O
by	O
the	O
associated	O
interleaver	O
and	O
transmitting	O
the	O
bits	O
that	O
come	O
out	O
of	O
each	O
constituent	O
encoder	B
often	O
the	O
constituent	O
encoder	B
is	O
chosen	O
to	O
be	O
a	O
systematic	B
encoder	B
just	O
like	O
the	O
recursive	O
shown	O
in	O
and	O
the	O
second	O
is	O
a	O
non-systematic	O
one	O
of	O
rate	B
that	O
emits	O
parity	B
bits	O
only	O
the	O
transmitted	O
codeword	B
then	O
consists	O
of	O
figure	O
the	O
encoder	B
of	O
a	O
turbo	B
code	I
each	O
box	B
contains	O
a	O
convolutional	B
code	I
the	O
source	O
bits	O
are	O
reordered	O
using	O
a	O
permutation	B
before	O
they	O
are	O
fed	O
to	O
the	O
transmitted	O
codeword	B
is	O
obtained	O
by	O
concatenating	O
or	O
interleaving	B
the	O
outputs	O
of	O
the	O
two	O
convolutional	B
codes	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
figure	O
and	O
turbo	B
codes	I
represented	O
as	O
factor	O
graphs	O
the	O
circles	O
represent	O
the	O
codeword	B
bits	O
the	O
two	O
rectangles	O
represent	O
trellises	O
of	O
convolutional	B
codes	O
with	O
the	O
systematic	B
bits	O
occupying	O
the	O
left	O
half	O
of	O
the	O
rectangle	O
and	O
the	O
parity	B
bits	O
occupying	O
the	O
right	O
half	O
the	O
puncturing	B
of	O
these	O
constituent	O
codes	O
in	O
the	O
turbo	B
code	I
is	O
represented	O
by	O
the	O
lack	O
of	O
connections	O
to	O
half	O
of	O
the	O
parity	B
bits	O
in	O
each	O
trellis	B
k	O
source	O
bits	O
followed	O
by	O
parity	B
bits	O
generated	O
by	O
the	O
convolutional	B
code	I
and	O
parity	B
bits	O
from	O
the	O
second	O
the	O
resulting	O
turbo	B
code	I
has	O
rate	B
the	O
turbo	B
code	I
can	O
be	O
represented	O
by	O
a	O
factor	B
graph	B
in	O
which	O
the	O
two	O
trellises	O
are	O
represented	O
by	O
two	O
large	O
rectangular	B
nodes	O
the	O
k	O
source	O
bits	O
and	O
the	O
parity	B
bits	O
participate	O
in	O
the	O
trellis	B
and	O
the	O
k	O
source	O
bits	O
and	O
the	O
last	O
parity	B
bits	O
participate	O
in	O
the	O
second	O
trellis	B
each	O
codeword	B
bit	B
participates	O
in	O
either	O
one	O
or	O
two	O
trellises	O
depending	O
on	O
whether	O
it	O
is	O
a	O
parity	B
bit	B
or	O
a	O
source	O
bit	B
each	O
trellis	B
node	O
contains	O
a	O
trellis	B
exactly	O
like	O
the	O
terminated	O
trellis	B
shown	O
in	O
except	O
one	O
thousand	O
times	O
as	O
long	O
are	O
other	O
factor	B
graph	B
representations	O
for	O
turbo	B
codes	I
that	O
make	O
use	O
of	O
more	O
elementary	O
nodes	O
but	O
the	O
factor	B
graph	B
given	O
here	O
yields	O
the	O
standard	O
version	O
of	O
the	O
sumproduct	O
algorithm	O
used	O
for	O
turbo	B
codes	I
if	O
a	O
turbo	B
code	I
of	O
smaller	O
rate	B
such	O
as	O
is	O
required	O
a	O
standard	O
to	O
the	O
code	O
is	O
to	O
puncture	O
some	O
of	O
the	O
parity	B
bits	O
turbo	B
codes	I
are	O
decoded	O
using	O
the	O
sumproduct	O
algorithm	O
described	O
in	O
chapter	O
on	O
the	O
iteration	O
each	O
trellis	B
receives	O
the	O
channel	O
likelihoods	O
and	O
runs	O
the	O
forwardbackward	O
algorithm	O
to	O
compute	O
for	O
each	O
bit	B
the	O
relative	B
likelihood	B
of	O
its	O
being	O
or	O
given	O
the	O
information	B
about	O
the	O
other	O
bits	O
these	O
likelihoods	O
are	O
then	O
passed	O
across	O
from	O
each	O
trellis	B
to	O
the	O
other	O
and	O
multiplied	O
by	O
the	O
channel	O
likelihoods	O
on	O
the	O
way	O
we	O
are	O
then	O
ready	O
for	O
the	O
second	O
iteration	O
the	O
forwardbackward	O
algorithm	O
is	O
run	O
again	O
in	O
each	O
trellis	B
using	O
the	O
updated	O
probabilities	O
after	O
about	O
ten	O
or	O
twenty	O
such	O
iterations	O
it	O
s	O
hoped	O
that	O
the	O
correct	O
decoding	B
will	O
be	O
found	O
it	O
is	O
common	O
practice	O
to	O
stop	O
after	O
some	O
number	O
of	O
iterations	O
but	O
we	O
can	O
do	O
better	O
as	O
a	O
stopping	O
criterion	O
the	O
following	O
procedure	O
can	O
be	O
used	O
at	O
every	O
iteration	O
for	O
each	O
time-step	O
in	O
each	O
trellis	B
we	O
identify	O
the	O
most	O
probable	O
edge	B
according	O
to	O
the	O
local	O
messages	O
if	O
these	O
most	O
probable	O
edges	O
join	O
up	O
into	O
two	O
valid	O
paths	O
one	O
in	O
each	O
trellis	B
and	O
if	O
these	O
two	O
paths	O
are	O
consistent	O
with	O
each	O
other	O
it	O
is	O
reasonable	O
to	O
stop	O
as	O
subsequent	O
iterations	O
are	O
unlikely	O
to	O
take	O
the	O
decoder	B
away	O
from	O
this	O
codeword	B
if	O
a	O
maximum	O
number	O
of	O
iterations	O
is	O
reached	O
without	O
this	O
stopping	O
criterion	O
being	O
a	O
decoding	B
error	O
can	O
be	O
reported	O
this	O
stopping	O
procedure	O
is	O
recommended	O
for	O
several	O
reasons	O
it	O
allows	O
a	O
big	O
saving	O
in	O
decoding	B
time	O
with	O
no	O
loss	O
in	O
error	B
probability	B
it	O
allows	O
decoding	B
failures	O
that	O
are	O
detected	O
by	O
the	O
decoder	B
to	O
be	O
so	O
knowing	O
that	O
a	O
particular	O
block	B
is	O
corrupted	O
is	O
surely	O
useful	O
information	B
for	O
the	O
receiver	O
and	O
when	O
we	O
distinguish	O
between	O
detected	O
and	O
undetected	O
errors	B
the	O
undetected	O
errors	B
give	O
helpful	O
insights	O
into	O
the	O
low-weight	O
codewords	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
parity-check	O
matrices	B
of	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
of	O
the	O
code	O
which	O
may	O
improve	O
the	O
process	O
of	B
code	I
design	O
turbo	B
codes	I
as	O
described	O
here	O
have	O
excellent	O
performance	O
down	O
to	O
decoded	O
error	O
probabilities	O
of	O
about	O
but	O
randomly-constructed	O
turbo	B
codes	I
tend	O
to	O
have	O
an	O
error	O
starting	O
at	O
that	O
level	O
this	O
error	O
is	O
caused	O
by	O
lowweight	O
codewords	O
to	O
reduce	O
the	O
height	O
of	O
the	O
error	O
one	O
can	O
attempt	O
to	O
modify	O
the	O
random	B
construction	B
to	O
increase	O
the	O
weight	O
of	O
these	O
low-weight	O
codewords	O
the	O
tweaking	O
of	O
turbo	B
codes	I
is	O
a	O
black	B
art	O
and	O
it	O
never	O
succeeds	O
in	O
totalling	O
eliminating	O
low-weight	O
codewords	O
more	O
precisely	O
the	O
low-weight	O
codewords	O
can	O
be	O
eliminated	O
only	O
by	O
the	O
turbo	B
code	I
s	O
excellent	O
performance	O
in	O
contrast	O
low-density	B
parity-check	I
codes	O
rarely	O
have	O
error	O
as	O
long	O
as	O
their	O
number	O
of	O
columns	O
is	O
not	O
too	O
large	O
exercise	O
figure	O
schematic	O
pictures	O
of	O
the	O
parity-check	O
matrices	B
of	O
a	O
convolutional	B
code	I
rate	B
and	O
a	O
turbo	B
code	I
rate	B
notation	B
a	O
diagonal	O
line	O
represents	O
an	O
identity	B
matrix	B
a	O
band	O
of	O
diagonal	O
lines	O
represent	O
a	O
band	O
of	O
diagonal	O
a	O
circle	B
inside	O
a	O
square	B
represents	O
the	O
random	B
permutation	B
of	O
all	O
the	O
columns	O
in	O
that	O
square	B
a	O
number	O
inside	O
a	O
square	B
represents	O
the	O
number	O
of	O
random	B
permutation	B
matrices	B
superposed	O
in	O
that	O
square	B
horizontal	O
and	O
vertical	O
lines	O
indicate	O
the	O
boundaries	O
of	O
the	O
blocks	O
within	O
the	O
matrix	B
parity-check	O
matrices	B
of	O
convolutional	B
codes	O
and	O
turbo	B
codes	I
we	O
close	O
by	O
discussing	O
the	O
parity-check	B
matrix	B
of	O
a	O
convolutional	B
code	I
viewed	O
as	O
a	O
linear	B
block	B
code	I
we	O
adopt	O
the	O
convention	O
that	O
the	O
n	O
bits	O
of	O
one	O
block	B
are	O
made	O
up	O
of	O
the	O
bits	O
ta	O
followed	O
by	O
the	O
bits	O
tb	O
exercise	O
prove	O
that	O
a	O
convolutional	B
code	I
has	O
a	O
low-density	O
parity	B
check	O
matrix	B
as	O
shown	O
schematically	O
in	O
hint	O
it	O
s	O
easiest	O
to	O
out	O
the	O
parity	B
constraints	O
by	O
a	O
convolutional	B
code	I
by	O
thinking	O
about	O
the	O
nonsystematic	O
nonrecursive	B
encoder	B
consider	O
putting	O
through	O
a	O
a	O
stream	O
that	O
s	O
been	O
through	O
convolutional	B
b	O
and	O
vice	O
versa	O
compare	O
the	O
two	O
resulting	O
streams	O
ignore	O
termination	B
of	O
the	O
trellises	O
the	O
parity-check	B
matrix	B
of	O
a	O
turbo	B
code	I
can	O
be	O
written	O
down	O
by	O
listing	O
the	O
constraints	O
by	O
the	O
two	O
constituent	O
trellises	O
so	O
turbo	B
codes	I
are	O
also	O
special	O
cases	O
of	O
low-density	B
parity-check	I
codes	O
if	O
a	O
turbo	B
code	I
is	O
punctured	O
it	O
no	O
longer	O
necessarily	O
has	O
a	O
low-density	B
parity-check	B
matrix	B
but	O
it	O
always	O
has	O
a	O
generalized	B
parity-check	B
matrix	B
that	O
is	O
sparse	O
as	O
explained	O
in	O
the	O
next	O
chapter	O
further	O
reading	O
for	O
further	O
reading	O
about	O
convolutional	B
codes	O
johannesson	O
and	O
zigangirov	O
is	O
highly	O
recommended	O
one	O
topic	O
i	O
would	O
have	O
liked	O
to	O
include	O
is	O
sequential	B
decoding	B
sequential	B
decoding	B
explores	O
only	O
the	O
most	O
promising	O
paths	O
in	O
the	O
trellis	B
and	O
backtracks	O
when	O
evidence	B
accumulates	O
that	O
a	O
wrong	O
turning	O
has	O
been	O
taken	O
sequential	B
decoding	B
is	O
used	O
when	O
the	O
trellis	B
is	O
too	O
big	O
for	O
us	O
to	O
be	O
able	O
to	O
apply	O
the	O
maximum	B
likelihood	B
algorithm	O
the	O
min	O
sum	O
algorithm	O
you	O
can	O
read	O
about	O
sequential	B
decoding	B
in	O
johannesson	O
and	O
zigangirov	O
for	O
further	O
information	B
about	O
the	O
use	O
of	O
the	O
sumproduct	O
algorithm	O
in	O
turbo	B
codes	I
and	O
the	O
rarely-used	O
but	O
highly	O
recommended	O
stopping	O
criteria	O
for	O
halting	O
their	O
decoding	B
frey	B
is	O
essential	O
reading	O
there	O
s	O
lots	O
more	O
good	B
in	O
the	O
same	O
book	O
solutions	O
solution	O
to	O
exercise	O
the	O
bit	B
was	O
the	O
most	O
probable	O
path	O
is	O
the	O
upper	O
one	O
in	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
repeataccumulate	O
codes	O
in	O
chapter	O
we	O
discussed	O
a	O
very	O
simple	O
and	O
not	O
very	O
method	O
for	O
communicating	O
over	O
a	O
noisy	B
channel	O
the	O
repetition	B
code	I
we	O
now	O
discuss	O
a	O
code	O
that	O
is	O
almost	O
as	O
simple	O
and	O
whose	O
performance	O
is	O
outstandingly	O
good	B
repeataccumulate	O
codes	O
were	O
studied	O
by	O
divsalar	O
et	O
al	O
for	O
theoretical	O
purposes	O
as	O
simple	O
turbo-like	O
codes	O
that	O
might	O
be	O
more	O
amenable	O
to	O
analysis	O
than	O
messy	O
turbo	B
codes	I
their	O
practical	B
performance	O
turned	O
out	O
to	O
be	O
just	O
as	O
good	B
as	O
other	O
sparse-graph	O
codes	O
the	O
encoder	B
take	O
k	O
source	O
bits	O
sk	O
repeat	O
each	O
bit	B
three	O
times	O
giving	O
n	O
bits	O
sksksk	O
permute	O
these	O
n	O
bits	O
using	O
a	O
random	B
permutation	B
random	B
permutation	B
the	O
same	O
one	O
for	O
every	O
codeword	B
call	O
the	O
permuted	O
string	O
u	O
un	O
transmit	O
the	O
accumulated	O
sum	O
tn	O
un	O
tn	O
un	O
that	O
s	O
it	O
graph	B
figure	O
shows	O
the	O
graph	B
of	O
a	O
repeataccumulate	O
code	O
using	O
four	O
types	O
of	O
node	O
equality	O
constraints	O
intermediate	O
binary	O
variables	O
circles	O
parity	B
constraints	O
and	O
the	O
transmitted	O
bits	O
circles	O
the	O
source	O
sets	O
the	O
values	O
of	O
the	O
black	B
bits	O
at	O
the	O
bottom	O
three	O
at	O
a	O
time	O
and	O
the	O
accumulator	B
computes	O
the	O
transmitted	O
bits	O
along	O
the	O
top	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
decoding	B
total	O
undetected	O
figure	O
factor	O
graphs	O
for	O
a	O
repeataccumulate	O
code	O
with	O
rate	B
using	O
elementary	O
nodes	O
each	O
white	B
circle	B
represents	O
a	O
transmitted	O
bit	B
each	O
constraint	O
forces	O
the	O
sum	O
of	O
the	O
bits	O
to	O
which	O
it	O
is	O
connected	O
to	O
be	O
even	O
each	O
black	B
circle	B
represents	O
an	O
intermediate	O
binary	O
variable	O
each	O
variables	O
to	O
which	O
it	O
is	O
connected	O
to	O
be	O
equal	O
factor	B
graph	B
normally	O
used	O
for	O
decoding	B
the	O
top	O
rectangle	O
represents	O
the	O
trellis	B
of	O
the	O
accumulator	B
shown	O
in	O
the	O
inset	O
constraint	O
forces	O
the	O
three	O
figure	O
performance	O
of	O
six	B
repeataccumulate	O
codes	O
on	O
the	O
gaussian	B
channel	I
the	O
blocklengths	O
range	O
from	O
n	O
to	O
n	O
vertical	O
axis	O
block	B
error	B
probability	B
horizontal	O
axis	O
the	O
dotted	O
lines	O
show	O
the	O
frequency	B
of	O
undetected	O
errors	B
this	O
graph	B
is	O
a	O
factor	B
graph	B
for	O
the	O
prior	B
probability	B
over	O
codewords	O
with	O
the	O
circles	O
being	O
binary	O
variable	O
nodes	O
and	O
the	O
squares	O
representing	O
contributes	O
a	O
factor	O
of	O
the	O
form	O
two	O
types	O
of	O
factor	O
nodes	O
as	O
usual	O
each	O
contributes	O
a	O
factor	O
of	O
the	O
form	O
x	O
mod	O
each	O
decoding	B
the	O
repeataccumulate	O
code	O
is	O
normally	O
decoded	O
using	O
the	O
sumproduct	O
algorithm	O
on	O
the	O
factor	B
graph	B
depicted	O
in	O
the	O
top	O
box	B
represents	O
the	O
trellis	B
of	O
the	O
accumulator	B
including	O
the	O
channel	O
likelihoods	O
in	O
the	O
half	O
of	O
each	O
iteration	O
the	O
top	O
trellis	B
receives	O
likelihoods	O
for	O
every	O
transition	B
in	O
the	O
trellis	B
and	O
runs	O
the	O
forwardbackward	O
algorithm	O
so	O
as	O
to	O
produce	O
likelihoods	O
for	O
each	O
variable	O
node	O
in	O
the	O
second	O
half	O
of	O
the	O
iteration	O
these	O
likelihoods	O
nodes	O
to	O
produce	O
new	O
likelihood	B
messages	O
to	O
are	O
multiplied	O
together	O
at	O
the	O
send	O
back	O
to	O
the	O
trellis	B
as	O
with	O
gallager	B
codes	O
and	O
turbo	B
codes	I
the	O
stop-when-it	O
s-done	O
decoding	B
method	O
can	O
be	O
applied	O
so	O
it	O
is	O
possible	O
to	O
distinguish	O
between	O
undetected	O
errors	B
are	O
caused	O
by	O
low-weight	O
codewords	O
in	O
the	O
code	O
and	O
detected	O
errors	B
the	O
decoder	B
gets	O
stuck	O
and	O
knows	O
that	O
it	O
has	O
failed	O
to	O
a	O
valid	O
answer	O
figure	O
shows	O
the	O
performance	O
of	O
six	B
randomly-constructed	O
repeat	O
accumulate	O
codes	O
on	O
the	O
gaussian	B
channel	I
if	O
one	O
does	O
not	O
mind	O
the	O
error	O
which	O
kicks	O
in	O
at	O
about	O
a	O
block	B
error	B
probability	B
of	O
the	O
performance	O
is	O
staggeringly	O
good	B
for	O
such	O
a	O
simple	O
code	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
total	O
detected	O
undetected	O
repeataccumulate	O
codes	O
db	O
db	O
empirical	O
distribution	B
of	O
decoding	B
times	O
it	O
is	O
interesting	O
to	O
study	O
the	O
number	O
of	O
iterations	O
of	O
the	O
sumproduct	O
algorithm	O
required	O
to	O
decode	O
a	O
sparse-graph	B
code	I
given	O
one	O
code	O
and	O
a	O
set	B
of	O
channel	O
conditions	O
the	O
decoding	B
time	O
varies	O
randomly	O
from	O
trial	O
to	O
trial	O
we	O
that	O
the	O
histogram	O
of	O
decoding	B
times	O
follows	O
a	O
power	B
law	I
p	O
for	O
large	O
the	O
power	O
p	O
depends	O
on	O
the	O
signal-to-noise	B
ratio	I
and	O
becomes	O
smaller	O
that	O
the	O
distribution	B
is	O
more	O
heavy-tailed	O
as	O
the	O
signal-to-noise	B
ratio	I
decreases	O
we	O
have	O
observed	O
power	O
laws	O
in	O
repeataccumulate	O
codes	O
and	O
in	O
irregular	B
and	O
regular	B
gallager	B
codes	O
figures	O
and	O
show	O
the	O
distribution	B
of	O
decoding	B
times	O
of	O
a	O
repeataccumulate	O
code	O
at	O
two	O
signal-to-noise	O
ratios	O
the	O
power	O
laws	O
extend	O
over	O
several	O
orders	O
of	O
magnitude	O
exercise	O
investigate	O
these	O
power	O
laws	O
does	O
density	B
evolution	B
predict	O
them	O
can	O
the	O
design	O
of	O
a	O
code	O
be	O
used	O
to	O
manipulate	O
the	O
power	B
law	I
in	O
a	O
useful	O
way	O
generalized	B
parity-check	O
matrices	B
figure	O
histograms	O
of	O
number	O
of	O
iterations	O
to	O
a	O
valid	O
decoding	B
for	O
a	O
repeataccumulate	O
code	O
with	O
source	O
block	B
length	O
k	O
and	O
transmitted	O
blocklength	O
n	O
block	B
error	B
probability	B
versus	O
signal-to-noise	B
ratio	I
for	O
the	O
ra	O
code	O
histogram	O
for	O
db	O
db	O
iii	O
c	O
fits	O
of	O
power	O
laws	O
to	O
and	O
and	O
node	O
to	O
a	O
i	O
that	O
it	O
is	O
helpful	O
when	O
relating	O
sparse-graph	O
codes	O
to	O
each	O
other	O
to	O
use	O
a	O
common	O
representation	O
for	O
them	O
all	O
forney	O
introduced	O
the	O
idea	O
of	O
and	O
all	O
variable	O
nodes	O
a	O
normal	B
graph	B
in	O
which	O
the	O
only	O
nodes	O
are	O
have	O
degree	B
one	O
or	O
two	O
variable	O
nodes	O
with	O
degree	B
two	O
can	O
be	O
represented	O
on	O
node	O
the	O
generalized	B
parity-check	B
matrix	B
edges	O
that	O
connect	O
a	O
is	O
a	O
graphical	O
way	O
of	O
representing	O
normal	B
graphs	O
in	O
a	O
parity-check	B
matrix	B
the	O
columns	O
are	O
transmitted	O
bits	O
and	O
the	O
rows	O
are	O
linear	B
constraints	O
in	O
a	O
generalized	B
parity-check	B
matrix	B
additional	O
columns	O
may	O
be	O
included	O
which	O
represent	O
state	O
variables	O
that	O
are	O
not	O
transmitted	O
one	O
way	O
of	O
thinking	O
of	O
these	O
state	O
variables	O
is	O
that	O
they	O
are	O
punctured	O
from	O
the	O
code	O
before	O
transmission	O
state	O
variables	O
are	O
indicated	O
by	O
a	O
horizontal	O
line	O
above	O
the	O
corresponding	O
columns	O
the	O
other	O
pieces	O
of	O
diagrammatic	O
notation	B
for	O
generalized	B
parity	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
generalized	B
parity-check	O
matrices	B
gt	O
h	O
figure	O
the	O
generator	B
matrix	B
parity-check	B
matrix	B
and	O
a	O
generalized	B
parity-check	B
matrix	B
of	O
a	O
repetition	B
code	I
with	O
rate	B
fa	O
pg	O
check	O
matrices	B
are	O
as	O
in	O
mackay	B
et	O
al	O
a	O
diagonal	O
line	O
in	O
a	O
square	B
indicates	O
that	O
that	O
part	O
of	O
the	O
matrix	B
contains	O
an	O
identity	B
matrix	B
two	O
or	O
more	O
parallel	O
diagonal	O
lines	O
indicate	O
a	O
band-diagonal	O
matrix	B
with	O
a	O
corresponding	O
number	O
of	O
per	O
row	O
a	O
horizontal	O
ellipse	O
with	O
an	O
arrow	O
on	O
it	O
indicates	O
that	O
the	O
corresponding	O
columns	O
in	O
a	O
block	B
are	O
randomly	O
permuted	O
a	O
vertical	O
ellipse	O
with	O
an	O
arrow	O
on	O
it	O
indicates	O
that	O
the	O
corresponding	O
rows	O
in	O
a	O
block	B
are	O
randomly	O
permuted	O
an	O
integer	O
surrounded	O
by	O
a	O
circle	B
represents	O
that	O
number	O
of	O
superposed	O
random	B
permutation	B
matrices	B
a	O
generalized	B
parity-check	B
matrix	B
is	O
a	O
pair	O
fa	O
pg	O
where	O
a	O
is	O
a	O
binary	O
matrix	B
and	O
p	O
is	O
a	O
list	O
of	O
the	O
punctured	O
bits	O
the	O
matrix	B
a	O
set	B
of	O
valid	O
vectors	B
x	O
satisfying	O
ax	O
for	O
each	O
valid	O
vector	O
there	O
is	O
a	O
codeword	B
tx	O
that	O
is	O
obtained	O
by	O
puncturing	B
from	O
x	O
the	O
bits	O
indicated	O
by	O
p	O
for	O
any	O
one	O
code	O
there	O
are	O
many	O
generalized	B
parity-check	O
matrices	B
the	O
rate	B
of	O
a	O
code	O
with	O
generalized	B
parity-check	B
matrix	B
fa	O
pg	O
can	O
be	O
estimated	O
as	O
follows	O
if	O
a	O
is	O
l	O
m	O
and	O
p	O
punctures	O
s	O
bits	O
and	O
selects	O
n	O
bits	O
for	O
transmission	O
n	O
s	O
then	O
the	O
number	O
of	O
constraints	O
on	O
the	O
codeword	B
m	O
is	O
the	O
number	O
of	O
source	O
bits	O
is	O
m	O
s	O
k	O
n	O
m	O
l	O
and	O
the	O
rate	B
is	O
greater	O
than	O
or	O
equal	O
to	O
r	O
m	O
n	O
s	O
l	O
s	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
gt	O
gt	O
h	O
a	O
p	O
repeataccumulate	O
codes	O
figure	O
the	O
generator	B
matrix	B
and	O
parity-check	B
matrix	B
of	O
a	O
systematic	B
low-density	O
generator-matrix	O
code	O
the	O
code	O
has	O
rate	B
figure	O
the	O
generator	B
matrix	B
and	O
generalized	B
parity-check	B
matrix	B
of	O
a	O
non-systematic	O
low-density	O
generator-matrix	O
code	O
the	O
code	O
has	O
rate	B
examples	O
repetition	B
code	I
the	O
generator	B
matrix	B
parity-check	B
matrix	B
and	O
generalized	B
parity-check	B
matrix	B
of	O
a	O
simple	O
repetition	B
code	I
are	O
shown	O
in	O
in	O
an	O
k	O
systematic	B
lowsystematic	O
low-density	O
generator-matrix	O
code	O
density	B
generator-matrix	O
code	O
there	O
are	O
no	O
state	O
variables	O
a	O
transmitted	O
codeword	B
t	O
of	O
length	O
n	O
is	O
given	O
by	O
where	O
t	O
gts	O
gt	O
ik	O
p	O
with	O
ik	O
denoting	O
the	O
identity	B
matrix	B
and	O
p	O
being	O
a	O
very	O
sparse	O
m	O
matrix	B
where	O
m	O
n	O
k	O
the	O
parity-check	B
matrix	B
of	O
this	O
code	O
is	O
h	O
in	O
the	O
case	O
of	O
a	O
code	O
this	O
parity-check	B
matrix	B
might	O
be	O
represented	O
as	O
shown	O
in	O
non-systematic	O
low-density	O
generator-matrix	O
code	O
in	O
an	O
k	O
non-systematic	O
low-density	O
generator-matrix	O
code	O
a	O
transmitted	O
codeword	B
t	O
of	O
length	O
n	O
is	O
given	O
by	O
where	O
gt	O
is	O
a	O
very	O
sparse	O
n	O
k	O
matrix	B
the	O
generalized	B
parity-check	B
matrix	B
of	O
this	O
code	O
is	O
t	O
gts	O
and	O
the	O
corresponding	O
generalized	B
parity-check	O
equation	O
is	O
a	O
ax	O
where	O
x	O
s	O
t	O
whereas	O
the	O
parity-check	B
matrix	B
of	O
this	O
simple	O
code	O
is	O
typically	O
a	O
complex	B
dense	O
matrix	B
the	O
generalized	B
parity-check	B
matrix	B
retains	O
the	O
underlying	O
simplicity	O
of	O
the	O
code	O
in	O
the	O
case	O
of	O
a	O
code	O
this	O
generalized	B
parity-check	B
matrix	B
might	O
be	O
represented	O
as	O
shown	O
in	O
low-density	B
parity-check	I
codes	O
and	O
linear	B
mn	O
codes	O
the	O
parity-check	B
matrix	B
of	O
a	O
low-density	B
parity-check	B
code	I
is	O
shown	O
in	O
figure	O
the	O
generalized	B
parity-check	O
matrices	B
of	O
a	O
gallager	B
code	I
with	O
columns	O
of	O
weight	O
a	O
linear	B
mn	O
code	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
generalized	B
parity-check	O
matrices	B
figure	O
the	O
generalized	B
parity-check	O
matrices	B
of	O
a	O
convolutional	B
code	I
with	O
rate	B
a	O
turbo	B
code	I
built	O
by	O
parallel	O
concatenation	B
of	O
two	O
convolutional	B
codes	O
a	O
linear	B
mn	O
code	O
is	O
a	O
non-systematic	O
low-density	B
parity-check	B
code	I
the	O
k	O
state	O
bits	O
of	O
an	O
mn	O
code	O
are	O
the	O
source	O
bits	O
figure	O
shows	O
the	O
generalized	B
parity-check	B
matrix	B
of	O
a	O
linear	B
mn	O
code	O
in	O
a	O
non-systematic	O
non-recursive	O
convolutional	B
code	I
convolutional	B
codes	O
the	O
source	O
bits	O
which	O
play	O
the	O
role	O
of	O
state	O
bits	O
are	O
fed	O
into	O
a	O
delay-line	O
and	O
two	O
linear	B
functions	B
of	O
the	O
delay-line	O
are	O
transmitted	O
in	O
these	O
two	O
parity	B
streams	O
are	O
shown	O
as	O
two	O
successive	O
vectors	B
of	O
length	O
k	O
is	O
common	O
to	O
interleave	O
these	O
two	O
parity	B
streams	O
a	O
bit-reordering	O
that	O
is	O
not	O
relevant	O
here	O
and	O
is	O
not	O
illustrated	O
concatenation	B
parallel	O
concatenation	B
of	O
two	O
codes	O
is	O
represented	O
in	O
one	O
of	O
these	O
diagrams	O
by	O
aligning	O
the	O
matrices	B
of	O
two	O
codes	O
in	O
such	O
a	O
way	O
that	O
the	O
source	O
bits	O
line	O
up	O
and	O
by	O
adding	O
blocks	O
of	O
zero-entries	O
to	O
the	O
matrix	B
such	O
that	O
the	O
state	O
bits	O
and	O
parity	B
bits	O
of	O
the	O
two	O
codes	O
occupy	O
separate	O
columns	O
an	O
example	O
is	O
given	O
by	O
the	O
turbo	B
code	I
that	O
follows	O
in	O
serial	O
concatenation	B
the	O
columns	O
corresponding	O
to	O
the	O
transmitted	O
bits	O
of	O
the	O
code	O
are	O
aligned	O
with	O
the	O
columns	O
corresponding	O
to	O
the	O
source	O
bits	O
of	O
the	O
second	O
code	O
turbo	B
codes	I
a	O
turbo	B
code	I
is	O
the	O
parallel	O
concatenation	B
of	O
two	O
convolutional	B
codes	O
the	O
generalized	B
parity-check	B
matrix	B
of	O
a	O
turbo	B
code	I
is	O
shown	O
in	O
repeataccumulate	O
codes	O
the	O
generalized	B
parity-check	O
matrices	B
of	O
a	O
repeataccumulate	O
code	O
is	O
shown	O
in	O
repeat-accumulate	O
codes	O
are	O
equivalent	O
to	O
staircase	B
codes	O
intersection	B
the	O
generalized	B
parity-check	B
matrix	B
of	O
the	O
intersection	B
of	O
two	O
codes	O
is	O
made	O
by	O
stacking	O
their	O
generalized	B
parity-check	O
matrices	B
on	O
top	O
of	O
each	O
other	O
in	O
such	O
a	O
way	O
that	O
all	O
the	O
transmitted	O
bits	O
columns	O
are	O
correctly	O
aligned	O
and	O
any	O
punctured	O
bits	O
associated	O
with	O
the	O
two	O
component	O
codes	O
occupy	O
separate	O
columns	O
figure	O
the	O
generalized	B
parity-check	B
matrix	B
of	O
a	O
repeataccumulate	O
code	O
with	O
rate	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
chapter	O
the	O
following	O
exercise	O
provides	O
a	O
helpful	O
background	O
for	O
digital	B
fountain	I
codes	O
exercise	O
an	O
author	O
proofreads	O
his	O
k	O
book	O
by	O
inspecting	O
random	B
pages	O
he	O
makes	O
n	O
page-inspections	O
and	O
does	O
not	O
take	O
any	O
precautions	O
to	O
avoid	O
inspecting	O
the	O
same	O
page	O
twice	O
after	O
n	O
k	O
page-inspections	O
what	O
fraction	O
of	O
pages	O
do	O
you	O
expect	O
have	O
never	O
been	O
inspected	O
after	O
n	O
k	O
page-inspections	O
what	O
is	O
the	O
probability	B
that	O
one	O
or	O
more	O
pages	O
have	O
never	O
been	O
inspected	O
show	O
that	O
in	O
order	O
for	O
the	O
probability	B
that	O
all	O
k	O
pages	O
have	O
been	O
inspected	O
to	O
be	O
we	O
require	O
n	O
k	O
page-inspections	O
problem	O
is	O
commonly	O
presented	O
in	O
terms	O
of	O
throwing	O
n	O
balls	O
at	O
random	B
into	O
k	O
bins	O
what	O
s	O
the	O
probability	B
that	O
every	O
bin	O
gets	O
at	O
least	O
one	O
ball	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
digital	B
fountain	I
codes	O
digital	B
fountain	I
codes	O
are	O
record-breaking	O
sparse-graph	O
codes	O
for	O
channels	O
with	O
erasures	O
channels	O
with	O
erasures	O
are	O
of	O
great	O
importance	O
for	O
example	O
sent	O
over	O
the	O
internet	B
are	O
chopped	O
into	O
packets	O
and	O
each	O
packet	B
is	O
either	O
received	O
without	O
error	O
or	O
not	O
received	O
a	O
simple	O
channel	O
model	B
describing	O
this	O
situation	O
is	O
a	O
q-ary	O
erasure	B
channel	I
which	O
has	O
all	O
inputs	O
in	O
the	O
input	O
alphabet	O
a	O
probability	B
f	O
of	O
transmitting	O
the	O
input	O
without	O
error	O
and	O
probability	B
f	O
of	O
delivering	O
the	O
output	O
the	O
alphabet	O
size	O
q	O
is	O
where	O
l	O
is	O
the	O
number	O
of	O
bits	O
in	O
a	O
packet	B
common	O
methods	O
for	O
communicating	O
over	O
such	O
channels	O
employ	O
a	O
feedback	B
channel	O
from	O
receiver	O
to	O
sender	O
that	O
is	O
used	O
to	O
control	O
the	O
retransmission	B
of	O
erased	O
packets	O
for	O
example	O
the	O
receiver	O
might	O
send	O
back	O
messages	O
that	O
identify	O
the	O
missing	O
packets	O
which	O
are	O
then	O
retransmitted	O
alternatively	O
the	O
receiver	O
might	O
send	O
back	O
messages	O
that	O
acknowledge	O
each	O
received	O
packet	B
the	O
sender	O
keeps	O
track	O
of	O
which	O
packets	O
have	O
been	O
acknowledged	O
and	O
retransmits	O
the	O
others	O
until	O
all	O
packets	O
have	O
been	O
acknowledged	O
these	O
simple	O
retransmission	B
protocols	O
have	O
the	O
advantage	O
that	O
they	O
will	O
work	O
regardless	O
of	O
the	O
erasure	B
probability	B
f	O
but	O
purists	O
who	O
have	O
learned	O
their	O
shannon	B
theory	O
will	O
feel	O
that	O
these	O
retransmission	B
protocols	O
are	O
wasteful	O
if	O
the	O
erasure	B
probability	B
f	O
is	O
large	O
the	O
number	O
of	O
feedback	B
messages	O
sent	O
by	O
the	O
protocol	B
will	O
be	O
large	O
under	O
the	O
second	O
protocol	B
it	O
s	O
likely	O
that	O
the	O
receiver	O
will	O
end	O
up	O
receiving	O
multiple	O
redundant	O
copies	O
of	O
some	O
packets	O
and	O
heavy	O
use	O
is	O
made	O
of	O
the	O
feedback	B
channel	O
according	O
to	O
shannon	B
there	O
is	O
no	O
need	O
for	O
the	O
feedback	B
channel	O
the	O
capacity	B
of	O
the	O
forward	O
channel	O
is	O
f	O
bits	O
whether	O
or	O
not	O
we	O
have	O
feedback	B
the	O
wastefulness	O
of	O
the	O
simple	O
retransmission	B
protocols	O
is	O
especially	O
evident	O
in	O
the	O
case	O
of	O
a	O
broadcast	B
channel	I
with	O
erasures	O
channels	O
where	O
one	O
sender	O
broadcasts	O
to	O
many	O
receivers	O
and	O
each	O
receiver	O
receives	O
a	O
random	B
fraction	O
f	O
of	O
the	O
packets	O
if	O
every	O
packet	B
that	O
is	O
missed	O
by	O
one	O
or	O
more	O
receivers	O
has	O
to	O
be	O
retransmitted	O
those	O
retransmissions	O
will	O
be	O
terribly	O
redundant	O
every	O
receiver	O
will	O
have	O
already	O
received	O
most	O
of	O
the	O
retransmitted	O
packets	O
so	O
we	O
would	O
like	O
to	O
make	O
erasure-correcting	O
codes	O
that	O
require	O
no	O
feedback	B
or	O
almost	O
no	O
feedback	B
the	O
classic	O
block	B
codes	O
for	O
erasure	B
correction	I
are	O
called	O
reedsolomon	O
codes	O
an	O
k	O
reedsolomon	O
code	O
an	O
alphabet	O
of	O
size	O
q	O
has	O
the	O
ideal	O
property	O
that	O
if	O
any	O
k	O
of	O
the	O
n	O
transmitted	O
symbols	O
are	O
received	O
then	O
the	O
original	O
k	O
source	O
symbols	O
can	O
be	O
recovered	O
berlekamp	B
or	O
lin	O
and	O
costello	O
for	O
further	O
information	B
reedsolomon	O
codes	O
exist	O
for	O
n	O
q	O
but	O
reedsolomon	O
codes	O
have	O
the	O
disadvantage	O
that	O
they	O
are	O
practical	B
only	O
for	O
small	O
k	O
n	O
and	O
q	O
standard	O
im	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
digital	B
fountain	I
codes	O
lt	O
stands	O
for	O
luby	B
transform	O
plementations	O
of	O
encoding	O
and	O
decoding	B
have	O
a	O
cost	O
of	O
order	O
log	O
n	O
packet	B
operations	O
furthermore	O
with	O
a	O
reedsolomon	O
code	O
as	O
with	O
any	O
block	B
code	I
one	O
must	O
estimate	O
the	O
erasure	B
probability	B
f	O
and	O
choose	O
the	O
code	O
rate	B
r	O
kn	O
before	O
transmission	O
if	O
we	O
are	O
unlucky	O
and	O
f	O
is	O
larger	O
than	O
expected	O
and	O
the	O
receiver	O
receives	O
fewer	O
than	O
k	O
symbols	O
what	O
are	O
we	O
to	O
do	O
we	O
d	O
like	O
a	O
simple	O
way	O
to	O
extend	O
the	O
code	O
on	O
the	O
to	O
create	O
a	O
lower-rate	O
k	O
code	O
for	O
reedsolomon	O
codes	O
no	O
such	O
method	O
exists	O
there	O
is	O
a	O
better	O
way	O
pioneered	O
by	O
michael	O
luby	B
at	O
his	O
company	O
digital	B
fountain	I
the	O
company	O
whose	O
business	O
is	O
based	O
on	O
sparse-graph	O
codes	O
the	O
digital	B
fountain	I
codes	O
i	O
describe	O
here	O
lt	O
codes	O
were	O
invented	O
by	O
luby	B
in	O
the	O
idea	O
of	O
a	O
digital	B
fountain	B
code	I
is	O
as	O
follows	O
the	O
encoder	B
is	O
a	O
fountain	O
that	O
produces	O
an	O
endless	O
supply	O
of	O
water	O
drops	O
packets	O
let	O
s	O
say	O
the	O
original	O
source	O
has	O
a	O
size	O
of	O
kl	O
bits	O
and	O
each	O
drop	O
contains	O
l	O
encoded	O
bits	O
now	O
anyone	O
who	O
wishes	O
to	O
receive	O
the	O
encoded	O
holds	O
a	O
bucket	O
under	O
the	O
fountain	O
and	O
collects	O
drops	O
until	O
the	O
number	O
of	O
drops	O
in	O
the	O
bucket	O
is	O
a	O
little	O
larger	O
than	O
k	O
they	O
can	O
then	O
recover	O
the	O
original	O
digital	B
fountain	I
codes	O
are	O
rateless	B
in	O
the	O
sense	O
that	O
the	O
number	O
of	O
encoded	O
packets	O
that	O
can	O
be	O
generated	O
from	O
the	O
source	O
message	O
is	O
potentially	O
limitless	O
and	O
the	O
number	O
of	O
encoded	O
packets	O
generated	O
can	O
be	O
determined	O
on	O
the	O
regardless	O
of	O
the	O
statistics	O
of	O
the	O
erasure	B
events	O
on	O
the	O
channel	O
we	O
can	O
send	O
as	O
many	O
encoded	O
packets	O
as	O
are	O
needed	O
in	O
order	O
for	O
the	O
decoder	B
to	O
recover	O
the	O
source	O
data	O
the	O
source	O
data	O
can	O
be	O
decoded	O
from	O
any	O
set	B
of	O
k	O
encoded	O
packets	O
for	O
slightly	O
larger	O
than	O
k	O
practice	O
about	O
larger	O
digital	B
fountain	I
codes	O
also	O
have	O
fantastically	O
small	O
encoding	O
and	O
decoding	B
complexities	O
with	O
probability	B
k	O
packets	O
can	O
be	O
communicated	O
with	O
average	O
encoding	O
and	O
decoding	B
costs	O
both	O
of	O
order	O
k	O
packet	B
operations	O
luby	B
calls	O
these	O
codes	O
universal	B
because	O
they	O
are	O
simultaneously	O
nearoptimal	O
for	O
every	O
erasure	B
channel	I
and	O
they	O
are	O
very	O
as	O
the	O
length	O
k	O
grows	O
the	O
overhead	O
k	O
is	O
of	O
order	O
a	O
digital	B
fountain	I
s	O
encoder	B
each	O
encoded	O
packet	B
tn	O
is	O
produced	O
from	O
the	O
source	O
sk	O
as	O
follows	O
randomly	O
choose	O
the	O
degree	B
dn	O
of	O
the	O
packet	B
from	O
a	O
degree	B
distribution	B
the	O
appropriate	O
choice	O
of	O
depends	O
on	O
the	O
source	O
size	O
k	O
as	O
we	O
ll	O
discuss	O
later	O
choose	O
uniformly	O
at	O
random	B
dn	O
distinct	O
input	O
packets	O
and	O
set	B
tn	O
equal	O
to	O
the	O
bitwise	B
sum	O
modulo	O
of	O
those	O
dn	O
packets	O
this	O
sum	O
can	O
be	O
done	O
by	O
successively	O
exclusive-or-ing	O
the	O
packets	O
together	O
this	O
encoding	O
operation	O
a	O
graph	B
connecting	O
encoded	O
packets	O
to	O
source	O
packets	O
if	O
the	O
mean	B
degree	B
is	O
smaller	O
than	O
k	O
then	O
the	O
graph	B
is	O
sparse	O
we	O
can	O
think	O
of	O
the	O
resulting	O
code	O
as	O
an	O
irregular	B
low-density	O
generator-matrix	O
code	O
the	O
decoder	B
needs	O
to	O
know	O
the	O
degree	B
of	O
each	O
packet	B
that	O
is	O
received	O
and	O
which	O
source	O
packets	O
it	O
is	O
connected	O
to	O
in	O
the	O
graph	B
this	O
information	B
can	O
be	O
communicated	O
to	O
the	O
decoder	B
in	O
various	O
ways	O
for	O
example	O
if	O
the	O
sender	O
and	O
receiver	O
have	O
synchronized	O
clocks	O
they	O
could	O
use	O
identical	O
pseudo-random	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
the	O
decoder	B
a	O
b	O
c	O
d	O
e	O
f	O
figure	O
example	O
decoding	B
for	O
a	O
digital	B
fountain	B
code	I
with	O
k	O
source	O
bits	O
and	O
n	O
encoded	O
bits	O
number	O
generators	O
seeded	O
by	O
the	O
clock	O
to	O
choose	O
each	O
random	B
degree	B
and	O
each	O
set	B
of	O
connections	O
alternatively	O
the	O
sender	O
could	O
pick	O
a	O
random	B
key	O
given	O
which	O
the	O
degree	B
and	O
the	O
connections	O
are	O
determined	O
by	O
a	O
pseudorandom	O
process	O
and	O
send	O
that	O
key	O
in	O
the	O
header	O
of	O
the	O
packet	B
as	O
long	O
as	O
the	O
packet	B
size	O
l	O
is	O
much	O
bigger	O
than	O
the	O
key	O
size	O
need	O
only	O
be	O
bits	O
or	O
so	O
this	O
key	O
introduces	O
only	O
a	O
small	O
overhead	O
cost	O
the	O
decoder	B
decoding	B
a	O
sparse-graph	B
code	I
is	O
especially	O
easy	O
in	O
the	O
case	O
of	O
an	O
erasure	B
channel	I
the	O
decoder	B
s	O
task	O
is	O
to	O
recover	O
s	O
from	O
t	O
gs	O
where	O
g	O
is	O
the	O
matrix	B
associated	O
with	O
the	O
graph	B
the	O
simple	O
way	O
to	O
attempt	O
to	O
solve	O
this	O
problem	O
is	O
by	O
message-passing	B
we	O
can	O
think	O
of	O
the	O
decoding	B
algorithm	O
as	O
the	O
sumproduct	O
algorithm	O
if	O
we	O
wish	O
but	O
all	O
messages	O
are	O
either	O
completely	O
uncertain	O
messages	O
or	O
completely	O
certain	O
messages	O
uncertain	O
messages	O
assert	O
that	O
a	O
message	O
packet	B
sk	O
could	O
have	O
any	O
value	O
with	O
equal	O
probability	B
certain	O
messages	O
assert	O
that	O
sk	O
has	O
a	O
particular	O
value	O
with	O
probability	B
one	O
this	O
simplicity	O
of	O
the	O
messages	O
allows	O
a	O
simple	O
description	O
of	O
the	O
decoding	B
process	O
we	O
ll	O
call	O
the	O
encoded	O
packets	O
ftng	O
check	O
nodes	O
find	O
a	O
check	O
node	O
tn	O
that	O
is	O
connected	O
to	O
only	O
one	O
source	O
packet	B
sk	O
there	O
is	O
no	O
such	O
check	O
node	O
this	O
decoding	B
algorithm	O
halts	O
at	O
this	O
point	O
and	O
fails	O
to	O
recover	O
all	O
the	O
source	O
packets	O
set	B
sk	O
tn	O
add	O
sk	O
to	O
all	O
checks	O
that	O
are	O
connected	O
to	O
sk	O
sk	O
for	O
all	O
such	O
that	O
remove	O
all	O
the	O
edges	O
connected	O
to	O
the	O
source	O
packet	B
sk	O
repeat	O
until	O
all	O
fskg	O
are	O
determined	O
this	O
decoding	B
process	O
is	O
illustrated	O
in	O
for	O
a	O
toy	O
case	O
where	O
each	O
packet	B
is	O
just	O
one	O
bit	B
there	O
are	O
three	O
source	O
packets	O
by	O
the	O
upper	O
circles	O
and	O
four	O
received	O
packets	O
by	O
the	O
lower	O
check	O
symbols	O
which	O
have	O
the	O
values	O
at	O
the	O
start	O
of	O
the	O
algorithm	O
at	O
the	O
iteration	O
the	O
only	O
check	O
node	O
that	O
is	O
connected	O
to	O
a	O
sole	O
source	O
bit	B
is	O
the	O
check	O
node	O
a	O
we	O
set	B
that	O
source	O
bit	B
accordingly	O
b	O
discard	O
the	O
check	O
node	O
then	O
add	O
the	O
value	O
of	O
to	O
the	O
checks	O
to	O
which	O
it	O
is	O
connected	O
c	O
disconnecting	O
from	O
the	O
graph	B
at	O
the	O
start	O
of	O
the	O
second	O
iteration	O
c	O
the	O
fourth	O
check	O
node	O
is	O
connected	O
to	O
a	O
sole	O
source	O
bit	B
we	O
set	B
to	O
in	O
panel	O
d	O
and	O
add	O
to	O
the	O
two	O
checks	O
it	O
is	O
connected	O
to	O
e	O
finally	O
we	O
that	O
two	O
check	O
nodes	O
are	O
both	O
connected	O
to	O
and	O
they	O
agree	O
about	O
the	O
value	O
of	O
we	O
would	O
hope	O
which	O
is	O
restored	O
in	O
panel	O
f	O
designing	O
the	O
degree	B
distribution	B
the	O
probability	B
distribution	B
of	O
the	O
degree	B
is	O
a	O
critical	O
part	O
of	O
the	O
design	O
occasional	O
encoded	O
packets	O
must	O
have	O
high	O
degree	B
d	O
similar	O
to	O
k	O
in	O
order	O
to	O
ensure	O
that	O
there	O
are	O
not	O
some	O
source	O
packets	O
that	O
are	O
connected	O
to	O
no-one	O
many	O
packets	O
must	O
have	O
low	O
degree	B
so	O
that	O
the	O
decoding	B
process	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
digital	B
fountain	I
codes	O
can	O
get	O
started	O
and	O
keep	O
going	O
and	O
so	O
that	O
the	O
total	O
number	O
of	O
addition	O
operations	O
involved	O
in	O
the	O
encoding	O
and	O
decoding	B
is	O
kept	O
small	O
for	O
a	O
given	O
degree	B
distribution	B
the	O
statistics	O
of	O
the	O
decoding	B
process	O
can	O
be	O
predicted	O
by	O
an	O
appropriate	O
version	O
of	B
density	B
evolution	B
ideally	O
to	O
avoid	O
redundancy	B
we	O
d	O
like	O
the	O
received	O
graph	B
to	O
have	O
the	O
property	O
that	O
just	O
one	O
check	O
node	O
has	O
degree	B
one	O
at	O
each	O
iteration	O
at	O
each	O
iteration	O
when	O
this	O
check	O
node	O
is	O
processed	O
the	O
degrees	O
in	O
the	O
graph	B
are	O
reduced	O
in	O
such	O
a	O
way	O
that	O
one	O
new	O
degree-one	O
check	O
node	O
appears	O
in	O
expectation	B
this	O
ideal	O
behaviour	O
is	O
achieved	O
by	O
the	O
ideal	O
soliton	B
distribution	B
for	O
d	O
k	O
the	O
expected	O
degree	B
under	O
this	O
distribution	B
is	O
roughly	O
ln	O
k	O
exercise	O
derive	O
the	O
ideal	O
soliton	B
distribution	B
at	O
the	O
iteration	O
let	O
the	O
number	O
of	O
packets	O
of	O
degree	B
d	O
be	O
show	O
that	O
d	O
the	O
expected	O
number	O
of	O
packets	O
of	O
degree	B
d	O
that	O
have	O
their	O
degree	B
reduced	O
to	O
d	O
is	O
and	O
at	O
the	O
tth	O
iteration	O
when	O
t	O
of	O
the	O
k	O
packets	O
have	O
been	O
recovered	O
and	O
the	O
number	O
of	O
packets	O
of	O
degree	B
d	O
is	O
htd	O
the	O
expected	O
number	O
of	O
packets	O
of	O
degree	B
d	O
that	O
have	O
their	O
degree	B
reduced	O
to	O
d	O
is	O
htddk	O
t	O
hence	O
show	O
that	O
in	O
order	O
to	O
have	O
the	O
expected	O
number	O
of	O
packets	O
of	O
degree	B
satisfy	O
for	O
all	O
t	O
k	O
we	O
must	O
to	O
start	O
with	O
have	O
and	O
and	O
more	O
generally	O
then	O
by	O
recursion	O
solve	O
for	O
for	O
d	O
upwards	O
this	O
degree	B
distribution	B
works	O
poorly	O
in	O
practice	O
because	O
around	O
the	O
expected	O
behaviour	O
make	O
it	O
very	O
likely	O
that	O
at	O
some	O
point	O
in	O
the	O
decoding	B
process	O
there	O
will	O
be	O
no	O
degree-one	O
check	O
nodes	O
and	O
furthermore	O
a	O
few	O
source	O
nodes	O
will	O
receive	O
no	O
connections	O
at	O
all	O
a	O
small	O
these	O
problems	O
the	O
robust	O
soliton	B
distribution	B
has	O
two	O
extra	O
parameters	B
c	O
and	O
it	O
is	O
designed	O
to	O
ensure	O
that	O
the	O
expected	O
number	O
of	O
degree-one	O
checks	O
is	O
about	O
s	O
c	O
rather	O
than	O
throughout	O
the	O
decoding	B
process	O
the	O
parameter	O
is	O
a	O
bound	B
on	O
the	O
probability	B
that	O
the	O
decoding	B
fails	O
to	O
run	O
to	O
completion	O
after	O
a	O
certain	O
number	O
of	O
packets	O
have	O
been	O
received	O
the	O
parameter	O
c	O
is	O
a	O
constant	O
of	O
order	O
if	O
our	O
aim	O
is	O
to	O
prove	O
luby	B
s	O
main	O
theorem	O
about	O
lt	O
codes	O
in	O
practice	O
however	O
it	O
can	O
be	O
viewed	O
as	O
a	O
free	O
parameter	O
with	O
a	O
value	O
somewhat	O
smaller	O
than	O
giving	O
good	B
results	O
we	O
a	O
positive	O
function	O
d	O
s	O
k	O
s	O
k	O
for	O
d	O
for	O
d	O
ks	O
for	O
d	O
ks	O
and	O
exercise	O
then	O
add	O
the	O
ideal	O
soliton	B
distribution	B
to	O
and	O
normalize	O
to	O
obtain	O
the	O
robust	O
soliton	B
distribution	B
z	O
where	O
z	O
pd	O
the	O
number	O
of	O
encoded	O
packets	O
required	O
at	O
the	O
receiving	O
end	O
to	O
ensure	O
that	O
the	O
decoding	B
can	O
run	O
to	O
completion	O
with	O
probability	B
at	O
least	O
is	O
kz	O
rho	O
tau	O
figure	O
the	O
distributions	O
and	O
for	O
the	O
case	O
k	O
c	O
which	O
gives	O
s	O
ks	O
and	O
z	O
the	O
distribution	B
is	O
largest	O
at	O
d	O
and	O
d	O
ks	O
c	O
figure	O
the	O
number	O
of	O
degree-one	O
checks	O
s	O
and	O
the	O
quantity	O
k	O
as	O
a	O
function	O
of	O
the	O
two	O
parameters	B
c	O
and	O
for	O
k	O
luby	B
s	O
main	O
theorem	O
proves	O
that	O
there	O
exists	O
a	O
value	O
of	O
c	O
such	O
that	O
given	O
k	O
received	O
packets	O
the	O
decoding	B
algorithm	O
will	O
recover	O
the	O
k	O
source	O
packets	O
with	O
probability	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
applications	O
figure	O
histograms	O
of	O
the	O
actual	O
number	O
of	O
packets	O
n	O
required	O
in	O
order	O
to	O
recover	O
a	O
of	O
size	O
k	O
packets	O
the	O
parameters	B
were	O
as	O
follows	O
top	O
histogram	O
c	O
ks	O
and	O
z	O
middle	O
c	O
ks	O
and	O
z	O
bottom	O
c	O
ks	O
and	O
z	O
luby	B
s	O
analysis	O
explains	O
how	O
the	O
small-d	O
end	O
of	O
has	O
the	O
role	O
of	O
ensuring	O
that	O
the	O
decoding	B
process	O
gets	O
started	O
and	O
the	O
spike	O
in	O
at	O
d	O
ks	O
is	O
included	O
to	O
ensure	O
that	O
every	O
source	O
packet	B
is	O
likely	O
to	O
be	O
connected	O
to	O
a	O
check	O
at	O
least	O
once	O
luby	B
s	O
key	O
result	O
is	O
that	O
an	O
appropriate	O
value	O
of	O
the	O
constant	O
c	O
receiving	O
k	O
checks	O
ensures	O
that	O
all	O
packets	O
can	O
be	O
recovered	O
with	O
probability	B
at	O
least	O
in	O
the	O
illustrative	O
i	O
have	O
set	B
the	O
allowable	O
decoder	B
failure	O
probability	B
quite	O
large	O
because	O
the	O
actual	O
failure	O
probability	B
is	O
much	O
smaller	O
than	O
is	O
suggested	O
by	O
luby	B
s	O
conservative	O
analysis	O
in	O
practice	O
lt	O
codes	O
can	O
be	O
tuned	O
so	O
that	O
a	O
of	O
original	O
size	O
k	O
packets	O
is	O
recovered	O
with	O
an	O
overhead	O
of	O
about	O
figure	O
shows	O
histograms	O
of	O
the	O
actual	O
number	O
of	O
packets	O
required	O
for	O
a	O
couple	O
of	O
settings	O
of	O
the	O
parameters	B
achieving	O
mean	B
overheads	O
smaller	O
than	O
and	O
respectively	O
applications	O
digital	B
fountain	I
codes	O
are	O
an	O
excellent	O
solution	O
in	O
a	O
wide	O
variety	O
of	O
situations	O
let	O
s	O
mention	O
two	O
storage	O
you	O
wish	O
to	O
make	O
a	O
backup	O
of	O
a	O
large	O
but	O
you	O
are	O
aware	O
that	O
your	O
magnetic	O
tapes	O
and	O
hard	O
drives	O
are	O
all	O
unreliable	O
in	O
the	O
sense	O
that	O
catastrophic	O
failures	O
in	O
which	O
some	O
stored	O
packets	O
are	O
permanently	O
lost	O
within	O
one	O
device	O
occur	O
at	O
a	O
rate	B
of	O
something	O
like	O
per	O
day	O
how	O
should	O
you	O
store	O
your	O
a	O
digital	B
fountain	I
can	O
be	O
used	O
to	O
spray	O
encoded	O
packets	O
all	O
over	O
the	O
place	O
on	O
every	O
storage	O
device	O
available	O
then	O
to	O
recover	O
the	O
backup	O
whose	O
size	O
was	O
k	O
packets	O
one	O
simply	O
needs	O
to	O
k	O
packets	O
from	O
anywhere	O
corrupted	O
packets	O
do	O
not	O
matter	O
we	O
simply	O
skip	O
over	O
them	O
and	O
more	O
packets	O
elsewhere	O
this	O
method	O
of	O
storage	O
also	O
has	O
advantages	O
in	O
terms	O
of	O
speed	O
of	O
recovery	O
in	O
a	O
hard	B
drive	I
it	O
is	O
standard	O
practice	O
to	O
store	O
a	O
in	O
successive	O
sectors	O
of	O
a	O
hard	B
drive	I
to	O
allow	O
rapid	O
reading	O
of	O
the	O
but	O
if	O
as	O
occasionally	O
happens	O
a	O
packet	B
is	O
lost	O
to	O
the	O
reading	O
head	O
being	O
track	O
for	O
a	O
moment	O
giving	O
a	O
burst	O
of	O
errors	B
that	O
cannot	O
be	O
corrected	O
by	O
the	O
packet	B
s	O
error-correcting	B
code	I
a	O
whole	O
revolution	O
of	O
the	O
drive	O
must	O
be	O
performed	O
to	O
bring	O
back	O
the	O
packet	B
to	O
the	O
head	O
for	O
a	O
second	O
read	O
the	O
time	O
taken	O
for	O
one	O
revolution	O
produces	O
an	O
undesirable	O
delay	O
in	O
the	O
system	O
if	O
were	O
instead	O
stored	O
using	O
the	O
digital	B
fountain	I
principle	O
with	O
the	O
digital	O
drops	O
stored	O
in	O
one	O
or	O
more	O
consecutive	O
sectors	O
on	O
the	O
drive	O
then	O
one	O
would	O
never	O
need	O
to	O
endure	O
the	O
delay	O
of	O
re-reading	O
a	O
packet	B
packet	B
loss	O
would	O
become	O
less	O
important	O
and	O
the	O
hard	B
drive	I
could	O
consequently	O
be	O
operated	O
faster	O
with	O
higher	O
noise	O
level	O
and	O
with	O
fewer	O
resources	O
devoted	O
to	O
noisychannel	O
coding	O
exercise	O
compare	O
the	O
digital	B
fountain	I
method	O
of	O
robust	O
storage	O
on	O
multiple	O
hard	O
drives	O
with	O
raid	B
redundant	O
array	O
of	O
independent	O
disks	O
broadcast	B
imagine	O
that	O
ten	O
thousand	O
subscribers	O
in	O
an	O
area	O
wish	O
to	O
receive	O
a	O
digital	O
movie	B
from	O
a	O
broadcaster	O
the	O
broadcaster	O
can	O
send	O
the	O
movie	B
in	O
packets	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
digital	B
fountain	I
codes	O
over	O
a	O
broadcast	B
network	B
for	O
example	O
by	O
a	O
wide-bandwidth	O
phone	B
line	O
or	O
by	O
satellite	O
imagine	O
that	O
not	O
all	O
packets	O
are	O
received	O
at	O
all	O
the	O
houses	O
let	O
s	O
say	O
f	O
of	O
them	O
are	O
lost	O
at	O
each	O
house	O
in	O
a	O
standard	O
approach	O
in	O
which	O
the	O
is	O
transmitted	O
as	O
a	O
plain	O
sequence	B
of	O
packets	O
with	O
no	O
encoding	O
each	O
house	O
would	O
have	O
to	O
notify	O
the	O
broadcaster	O
of	O
the	O
f	O
k	O
missing	O
packets	O
and	O
request	O
that	O
they	O
be	O
retransmitted	O
and	O
with	O
ten	O
thousand	O
subscribers	O
all	O
requesting	O
such	O
retransmissions	O
there	O
would	O
be	O
a	O
retransmission	B
request	O
for	O
almost	O
every	O
packet	B
thus	O
the	O
broadcaster	O
would	O
have	O
to	O
repeat	O
the	O
entire	O
broadcast	B
twice	O
in	O
order	O
to	O
ensure	O
that	O
most	O
subscribers	O
have	O
received	O
the	O
whole	O
movie	B
and	O
most	O
users	O
would	O
have	O
to	O
wait	O
roughly	O
twice	O
as	O
long	O
as	O
the	O
ideal	O
time	O
before	O
the	O
download	O
was	O
complete	O
if	O
the	O
broadcaster	O
uses	O
a	O
digital	B
fountain	I
to	O
encode	O
the	O
movie	B
each	O
subscriber	B
can	O
recover	O
the	O
movie	B
from	O
any	O
k	O
packets	O
so	O
the	O
broadcast	B
needs	O
to	O
last	O
for	O
only	O
say	O
packets	O
and	O
every	O
house	O
is	O
very	O
likely	O
to	O
have	O
successfully	O
recovered	O
the	O
whole	O
another	O
application	O
is	O
broadcasting	O
data	O
to	O
cars	O
imagine	O
that	O
we	O
want	O
to	O
send	O
updates	O
to	O
in-car	B
navigation	B
databases	O
by	O
satellite	O
there	O
are	O
hundreds	O
of	O
thousands	O
of	O
vehicles	O
and	O
they	O
can	O
receive	O
data	O
only	O
when	O
they	O
are	O
out	O
on	O
the	O
open	O
road	O
there	O
are	O
no	O
feedback	B
channels	O
a	O
standard	O
method	O
for	O
sending	O
the	O
data	O
is	O
to	O
put	O
it	O
in	O
a	O
carousel	O
broadcasting	O
the	O
packets	O
in	O
a	O
periodic	O
sequence	B
yes	O
a	O
car	O
may	O
go	O
through	O
a	O
tunnel	O
and	O
miss	O
out	O
on	O
a	O
few	O
hundred	O
packets	O
but	O
it	O
will	O
be	O
able	O
to	O
collect	O
those	O
missed	O
packets	O
an	O
hour	O
later	O
when	O
the	O
carousel	O
has	O
gone	O
through	O
a	O
full	O
revolution	O
hope	O
or	O
maybe	O
the	O
following	O
day	O
if	O
instead	O
the	O
satellite	O
uses	O
a	O
digital	B
fountain	I
each	O
car	O
needs	O
to	O
receive	O
only	O
an	O
amount	O
of	O
data	O
equal	O
to	O
the	O
original	O
size	O
further	O
reading	O
the	O
encoders	O
and	O
decoders	O
sold	O
by	O
digital	B
fountain	I
have	O
even	O
higher	O
than	O
the	O
lt	O
codes	O
described	O
here	O
and	O
they	O
work	O
well	O
for	O
all	O
blocklengths	O
not	O
only	O
large	O
lengths	O
such	O
as	O
k	O
shokrollahi	B
presents	O
raptor	B
codes	I
which	O
are	O
an	O
extension	O
of	O
lt	O
codes	O
with	O
linear-time	O
encoding	O
and	O
decoding	B
further	O
exercises	O
exercise	O
understanding	O
the	O
robust	O
soliton	B
distribution	B
repeat	O
the	O
analysis	O
of	O
exercise	O
but	O
now	O
aim	O
to	O
have	O
the	O
expected	O
number	O
of	O
packets	O
of	O
degree	B
be	O
s	O
for	O
all	O
t	O
instead	O
of	O
show	O
that	O
the	O
initial	O
required	O
number	O
of	O
packets	O
is	O
k	O
dd	O
s	O
d	O
for	O
d	O
the	O
reason	O
for	O
truncating	O
the	O
second	O
term	O
beyond	O
d	O
ks	O
and	O
replacing	O
it	O
by	O
the	O
spike	O
at	O
d	O
ks	O
equation	O
is	O
to	O
ensure	O
that	O
the	O
decoding	B
complexity	B
does	O
not	O
grow	O
larger	O
than	O
ok	O
ln	O
k	O
estimate	O
the	O
expected	O
number	O
of	O
packets	O
pd	O
and	O
the	O
expected	O
number	O
of	O
edges	O
in	O
the	O
sparse	B
graph	B
pd	O
determines	O
the	O
decoding	B
complexity	B
if	O
the	O
histogram	O
of	O
packets	O
is	O
as	O
given	O
in	O
compare	O
with	O
the	O
expected	O
numbers	O
of	O
packets	O
and	O
edges	O
when	O
the	O
robust	O
soliton	B
distribution	B
is	O
used	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
further	O
exercises	O
exercise	O
show	O
that	O
the	O
spike	O
at	O
d	O
ks	O
is	O
an	O
ade	O
quate	O
replacement	O
for	O
the	O
tail	B
of	O
high-weight	O
packets	O
in	O
exercise	O
investigate	O
experimentally	O
how	O
necessary	O
the	O
spike	O
at	O
d	O
ks	O
is	O
for	O
successful	O
decoding	B
investigate	O
also	O
whether	O
the	O
tail	B
of	O
beyond	O
d	O
ks	O
is	O
necessary	O
what	O
happens	O
if	O
all	O
highweight	O
degrees	O
are	O
removed	O
both	O
the	O
spike	O
at	O
d	O
ks	O
and	O
the	O
tail	B
of	O
beyond	O
d	O
ks	O
exercise	O
fill	O
in	O
the	O
details	O
in	O
the	O
proof	O
of	O
luby	B
s	O
main	O
theorem	O
that	O
receiving	O
k	O
checks	O
ensures	O
that	O
all	O
the	O
source	O
packets	O
can	O
be	O
recovered	O
with	O
probability	B
at	O
least	O
exercise	O
optimize	O
the	O
degree	B
distribution	B
of	O
a	O
digital	B
fountain	B
code	I
for	O
a	O
of	O
k	O
packets	O
pick	O
a	O
sensible	O
objective	B
function	I
for	O
your	O
optimization	B
such	O
as	O
minimizing	O
the	O
mean	B
of	O
n	O
the	O
number	O
of	O
packets	O
required	O
for	O
complete	O
decoding	B
or	O
the	O
percentile	O
of	O
the	O
histogram	O
of	O
n	O
exercise	O
make	O
a	O
model	B
of	O
the	O
situation	O
where	O
a	O
data	O
stream	O
is	O
broadcast	B
to	O
cars	O
and	O
quantify	O
the	O
advantage	O
that	O
the	O
digital	B
fountain	I
has	O
over	O
the	O
carousel	O
method	O
exercise	O
construct	O
a	O
simple	O
example	O
to	O
illustrate	O
the	O
fact	O
that	O
the	O
digital	B
fountain	I
decoder	B
of	O
section	B
is	O
suboptimal	O
it	O
sometimes	O
gives	O
up	O
even	O
though	O
the	O
information	B
available	O
is	O
to	O
decode	O
the	O
whole	O
how	O
does	O
the	O
cost	O
of	O
the	O
optimal	B
decoder	B
compare	O
exercise	O
if	O
every	O
transmitted	O
packet	B
were	O
created	O
by	O
adding	O
together	O
source	O
packets	O
at	O
random	B
with	O
probability	B
of	O
each	O
source	O
packet	B
s	O
being	O
included	O
show	O
that	O
the	O
probability	B
that	O
k	O
received	O
packets	O
for	O
the	O
optimal	B
decoder	B
to	O
be	O
able	O
to	O
recover	O
the	O
k	O
source	O
packets	O
is	O
just	O
a	O
little	O
below	O
put	O
it	O
another	O
way	O
what	O
is	O
the	O
probability	B
that	O
a	O
random	B
k	O
k	O
matrix	B
has	O
full	O
rank	O
show	O
that	O
if	O
k	O
packets	O
are	O
received	O
the	O
probability	B
that	O
they	O
will	O
not	O
for	O
the	O
optimal	B
decoder	B
is	O
roughly	O
exercise	O
implement	O
an	O
optimal	B
digital	B
fountain	I
decoder	B
that	O
uses	O
the	O
method	O
of	O
richardson	B
and	O
urbanke	B
derived	O
for	O
fast	B
encoding	I
of	O
sparse-graph	O
codes	O
to	O
handle	O
the	O
matrix	B
inversion	O
required	O
for	O
optimal	B
decoding	B
now	O
that	O
you	O
have	O
changed	O
the	O
decoder	B
you	O
can	O
reoptimize	O
the	O
degree	B
distribution	B
using	O
higher-weight	O
packets	O
by	O
how	O
much	O
can	O
you	O
reduce	O
the	O
overhead	O
the	O
assertion	O
that	O
this	O
approach	O
makes	O
digital	B
fountain	I
codes	O
viable	O
as	O
erasure-correcting	O
codes	O
for	O
all	O
blocklengths	O
not	O
just	O
the	O
large	O
blocklengths	O
for	O
which	O
lt	O
codes	O
are	O
excellent	O
exercise	O
digital	B
fountain	I
codes	O
are	O
excellent	O
rateless	B
codes	O
for	O
erasure	B
channels	O
make	O
a	O
rateless	B
code	I
for	O
a	O
channel	O
that	O
has	O
both	O
erasures	O
and	O
noise	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
digital	B
fountain	I
codes	O
summary	B
of	O
sparse-graph	O
codes	O
a	O
simple	O
method	O
for	O
designing	O
error-correcting	B
codes	I
for	O
noisy	B
channels	O
pioneered	O
by	O
gallager	B
has	O
recently	O
been	O
rediscovered	O
and	O
generalized	B
and	B
communication	B
theory	O
has	O
been	O
transformed	O
the	O
practical	B
performance	O
of	O
gallager	B
s	O
low-density	B
parity-check	I
codes	O
and	O
their	O
modern	O
cousins	O
is	O
vastly	O
better	O
than	O
the	O
performance	O
of	O
the	O
codes	O
with	O
which	O
textbooks	O
have	O
been	O
in	O
the	O
intervening	O
years	O
which	O
sparse-graph	B
code	I
is	O
best	O
for	O
a	O
noisy	B
channel	O
depends	O
on	O
the	O
chosen	O
rate	B
and	O
blocklength	O
the	O
permitted	O
encoding	O
and	O
decoding	B
complexity	B
and	O
the	O
question	O
of	O
whether	O
occasional	O
undetected	O
errors	B
are	O
acceptable	O
lowdensity	O
parity-check	O
codes	O
are	O
the	O
most	O
versatile	O
it	O
s	O
easy	O
to	O
make	O
a	O
competitive	O
low-density	B
parity-check	B
code	I
with	O
almost	O
any	O
rate	B
and	O
blocklength	O
and	O
low-density	B
parity-check	I
codes	O
virtually	O
never	O
make	O
undetected	O
errors	B
for	O
the	O
special	O
case	O
of	O
the	O
erasure	B
channel	I
the	O
sparse-graph	O
codes	O
that	O
are	O
best	O
are	O
digital	B
fountain	I
codes	O
conclusion	O
the	O
best	O
solution	O
to	O
the	O
communication	B
problem	O
is	O
combine	O
a	O
simple	O
pseudo-random	O
code	O
with	O
a	O
message-passing	B
decoder	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
part	O
vii	O
appendices	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
notation	B
what	O
does	O
p	O
b	O
c	O
mean	B
p	O
b	O
c	O
is	O
pronounced	O
the	O
probability	B
that	O
a	O
is	O
true	O
given	O
that	O
b	O
is	O
true	O
and	O
c	O
is	O
true	O
or	O
more	O
the	O
probability	B
of	O
a	O
given	O
b	O
and	O
c	O
chapter	O
what	O
do	O
log	O
and	O
ln	O
mean	B
in	O
this	O
book	O
log	O
x	O
means	O
the	O
base-two	O
loga	O
rithm	O
x	O
ln	O
x	O
means	O
the	O
natural	B
logarithm	O
loge	O
x	O
what	O
does	O
mean	B
usually	O
a	O
hat	B
over	O
a	O
variable	O
denotes	O
a	O
guess	O
or	O
es	O
timator	O
so	O
is	O
a	O
guess	O
at	O
the	O
value	O
of	O
s	O
integrals	O
there	O
is	O
no	O
between	O
r	O
f	O
du	O
andr	O
du	O
f	O
the	O
inte	O
grand	O
is	O
f	O
in	O
both	O
cases	O
what	O
does	O
but	O
it	O
denotes	O
a	O
product	O
it	O
s	O
pronounced	O
product	O
over	O
n	O
from	O
to	O
n	O
so	O
for	O
example	O
n	O
n	O
mean	B
this	O
is	O
like	O
the	O
summation	O
pn	O
n	O
n	O
n	O
exp	O
n	O
ln	O
n	O
i	O
like	O
to	O
choose	O
the	O
name	O
of	O
the	O
free	O
variable	O
in	O
a	O
sum	O
or	O
a	O
product	O
here	O
n	O
to	O
be	O
the	O
lower	O
case	O
version	O
of	O
the	O
range	O
of	O
the	O
sum	O
so	O
n	O
usually	O
runs	O
from	O
to	O
n	O
and	O
m	O
usually	O
runs	O
from	O
to	O
m	O
this	O
is	O
a	O
habit	O
i	O
learnt	O
from	O
yaser	O
abu-mostafa	B
and	O
i	O
think	O
it	O
makes	O
formulae	O
easier	O
to	O
understand	O
what	O
does	O
mean	B
this	O
is	O
pronounced	O
n	O
choose	O
n	O
and	O
it	O
is	O
the	O
number	O
of	O
ways	O
of	O
selecting	O
an	O
unordered	O
set	B
of	O
n	O
objects	O
from	O
a	O
set	B
of	O
size	O
n	O
n	O
n	O
n	O
this	O
function	O
is	O
known	O
as	O
the	O
combination	B
function	O
what	O
is	O
the	O
gamma	B
function	I
is	O
by	O
r	O
du	O
for	O
x	O
the	O
gamma	B
function	I
is	O
an	O
extension	O
of	O
the	O
factorial	B
function	O
to	O
real	O
number	O
arguments	O
in	O
general	O
and	O
for	O
integer	O
arguments	O
x	O
the	O
digamma	B
function	I
is	O
by	O
d	O
dx	O
ln	O
for	O
large	O
x	O
practical	B
purposes	O
x	O
ln	O
lnx	O
x	O
ln	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
notation	B
and	O
for	O
small	O
x	O
practical	B
purposes	O
x	O
ln	O
ln	O
where	O
is	O
euler	O
s	O
constant	O
x	O
what	O
does	O
rc	O
mean	B
just	O
as	O
denotes	O
the	O
inverse	O
function	O
to	O
s	O
sinx	O
so	O
there	O
is	O
potential	O
confusion	O
when	O
people	O
use	O
x	O
to	O
denote	O
since	O
then	O
we	O
might	O
expect	O
s	O
to	O
denote	O
sins	O
i	O
therefore	O
like	O
to	O
avoid	O
using	O
the	O
notation	B
x	O
is	O
the	O
inverse	O
function	O
to	O
h	O
what	O
does	O
mean	B
the	O
answer	O
depends	O
on	O
the	O
context	O
often	O
a	O
prime	O
is	O
used	O
to	O
denote	O
d	O
dx	O
f	O
similarly	O
a	O
dot	O
denotes	O
with	O
respect	O
to	O
time	O
t	O
d	O
dt	O
x	O
however	O
the	O
prime	O
is	O
also	O
a	O
useful	O
indicator	O
for	O
another	O
variable	O
for	O
example	O
a	O
new	O
value	O
for	O
a	O
variable	O
so	O
for	O
example	O
might	O
denote	O
the	O
new	O
value	O
of	O
x	O
also	O
if	O
there	O
are	O
two	O
integers	O
that	O
both	O
range	O
from	O
to	O
n	O
i	O
will	O
often	O
name	O
those	O
integers	O
n	O
and	O
so	O
my	O
rule	O
is	O
if	O
a	O
prime	O
occurs	O
in	O
an	O
expression	O
that	O
could	O
be	O
a	O
function	O
such	O
as	O
or	O
then	O
it	O
denotes	O
otherwise	O
it	O
indicates	O
another	O
variable	O
what	O
is	O
the	O
error	B
function	I
of	O
this	O
function	O
vary	O
i	O
it	O
to	O
be	O
the	O
cumulative	O
probability	B
of	O
a	O
standard	O
normal	B
distribution	B
z	O
dz	O
what	O
does	O
er	O
mean	B
er	O
is	O
pronounced	O
the	O
expected	O
value	O
of	O
r	O
or	O
the	O
expectation	B
of	O
r	O
and	O
it	O
is	O
the	O
mean	B
value	O
of	O
r	O
another	O
symbol	O
for	O
expected	O
value	O
is	O
the	O
pair	O
of	O
angle-brackets	O
hri	O
what	O
does	O
jxj	O
mean	B
the	O
vertical	O
bars	O
j	O
j	O
have	O
two	O
meanings	O
if	O
a	O
is	O
a	O
set	B
then	O
jaj	O
denotes	O
the	O
number	O
of	O
elements	O
in	O
the	O
set	B
if	O
x	O
is	O
a	O
number	O
then	O
jxj	O
is	O
the	O
absolute	B
value	I
of	O
x	O
what	O
does	O
mean	B
here	O
a	O
and	O
p	O
are	O
matrices	B
with	O
the	O
same	O
number	O
of	O
rows	O
denotes	O
the	O
double-width	O
matrix	B
obtained	O
by	O
putting	O
a	O
alongside	O
p	O
the	O
vertical	O
bar	O
is	O
used	O
to	O
avoid	O
confusion	O
with	O
the	O
product	O
ap	O
what	O
does	O
xt	O
mean	B
the	O
superscript	O
t	O
is	O
pronounced	O
transpose	O
trans	O
posing	O
a	O
row-vector	O
turns	O
it	O
into	O
a	O
column	O
vector	O
a	O
and	O
vice	O
versa	O
are	O
column	O
vectors	B
my	O
vectors	B
indicated	O
by	O
bold	O
face	O
type	O
similarly	O
matrices	B
can	O
be	O
transposed	O
if	O
mij	O
is	O
the	O
entry	O
in	O
row	O
i	O
and	O
column	O
j	O
of	O
matrix	B
m	O
and	O
n	O
mt	O
then	O
nji	O
mij	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
a	O
notation	B
what	O
are	O
trace	O
m	O
and	O
det	O
m	O
the	O
trace	O
of	O
a	O
matrix	B
is	O
the	O
sum	O
of	O
its	O
di	O
agonal	O
elements	O
trace	O
m	O
mii	O
the	O
determinant	O
of	O
m	O
is	O
denoted	O
det	O
m	O
what	O
does	O
mean	B
the	O
matrix	B
is	O
the	O
identity	B
matrix	B
if	O
m	O
n	O
if	O
m	O
n	O
another	O
name	O
for	O
the	O
identity	B
matrix	B
is	O
i	O
or	O
sometimes	O
i	O
include	O
a	O
subscript	O
on	O
this	O
symbol	O
which	O
indicates	O
the	O
size	O
of	O
the	O
matrix	B
k	O
what	O
does	O
mean	B
the	O
delta	B
function	I
has	O
the	O
property	O
z	O
dx	O
f	O
f	O
another	O
possible	O
meaning	O
for	O
is	O
the	O
truth	B
function	I
which	O
is	O
if	O
the	O
proposition	O
s	O
is	O
true	O
but	O
i	O
have	O
adopted	O
another	O
notation	B
for	O
that	O
after	O
all	O
the	O
symbol	O
is	O
quite	O
busy	O
already	O
with	O
the	O
two	O
roles	O
mentioned	O
above	O
in	O
addition	O
to	O
its	O
role	O
as	O
a	O
small	O
real	O
number	O
and	O
an	O
increment	O
operator	O
in	O
what	O
does	O
mean	B
is	O
the	O
truth	B
function	I
which	O
is	O
if	O
the	O
proposition	O
s	O
is	O
true	O
and	O
otherwise	O
for	O
example	O
the	O
number	O
of	O
positive	O
numbers	O
in	O
the	O
set	B
t	O
can	O
be	O
written	O
what	O
is	O
the	O
between	O
and	O
in	O
an	O
algorithm	O
x	O
y	O
means	O
that	O
the	O
variable	O
x	O
is	O
updated	O
by	O
assigning	B
it	O
the	O
value	O
of	O
y	O
in	O
contrast	O
x	O
y	O
is	O
a	O
proposition	O
a	O
statement	O
that	O
x	O
is	O
equal	O
to	O
y	O
see	O
chapters	O
and	O
for	O
further	O
and	O
notation	B
relating	O
to	O
probability	B
distributions	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
b	O
some	O
physics	B
about	O
phase	O
transitions	O
a	O
system	O
with	O
states	O
x	O
in	O
contact	O
with	O
a	O
heat	B
bath	I
at	O
temperature	B
t	O
has	O
probability	B
distribution	B
p	O
the	O
partition	B
function	I
is	O
the	O
inverse	O
temperature	B
can	O
be	O
interpreted	O
as	O
an	O
exchange	B
rate	B
between	O
entropy	B
and	O
energy	B
is	O
the	O
amount	O
of	O
energy	B
that	O
must	O
be	O
given	O
to	O
a	O
heat	B
bath	I
to	O
increase	O
its	O
entropy	B
by	O
one	O
nat	O
often	O
the	O
system	O
will	O
be	O
by	O
some	O
other	O
parameters	B
such	O
as	O
the	O
volume	B
of	O
the	O
box	B
it	O
is	O
in	O
v	O
in	O
which	O
case	O
z	O
is	O
a	O
function	O
of	O
v	O
too	O
v	O
for	O
any	O
system	O
with	O
a	O
number	O
of	O
states	O
the	O
function	O
is	O
evidently	O
a	O
continuous	B
function	O
of	O
since	O
it	O
is	O
simply	O
a	O
sum	O
of	O
exponentials	O
moreover	O
all	O
the	O
derivatives	O
of	O
with	O
respect	O
to	O
are	O
continuous	B
too	O
what	O
phase	O
transitions	O
are	O
all	O
about	O
however	O
is	O
this	O
phase	O
transitions	O
correspond	O
to	O
values	O
of	O
and	O
v	O
critical	O
points	O
at	O
which	O
the	O
derivatives	O
of	O
z	O
have	O
discontinuities	O
or	O
divergences	O
immediately	O
we	O
can	O
deduce	O
only	O
systems	O
with	O
an	O
number	O
of	O
states	O
can	O
show	O
phase	O
transitions	O
often	O
we	O
include	O
a	O
parameter	O
n	O
describing	O
the	O
size	O
of	O
the	O
system	O
phase	O
transitions	O
may	O
appear	O
in	O
the	O
limit	O
n	O
real	O
systems	O
may	O
have	O
a	O
value	O
of	O
n	O
like	O
if	O
we	O
make	O
the	O
system	O
large	O
by	O
simply	O
grouping	O
together	O
n	O
independent	O
systems	O
whose	O
partition	B
function	I
is	O
then	O
nothing	O
interesting	O
happens	O
the	O
partition	B
function	I
for	O
n	O
independent	O
identical	O
systems	O
is	O
simply	O
zn	O
now	O
while	O
this	O
function	O
zn	O
may	O
be	O
a	O
very	O
rapidly	O
varying	O
function	O
of	O
that	O
doesn	O
t	O
mean	B
it	O
is	O
showing	O
phase	O
transitions	O
the	O
natural	B
way	O
to	O
look	O
at	O
the	O
partition	B
function	I
is	O
in	O
the	O
logarithm	O
ln	O
zn	O
n	O
ln	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
b	O
some	O
physics	B
duplicating	O
the	O
original	O
system	O
n	O
times	O
simply	O
scales	O
up	O
all	O
properties	O
like	O
the	O
energy	B
and	O
heat	B
capacity	B
of	O
the	O
system	O
by	O
a	O
factor	O
of	O
n	O
so	O
if	O
the	O
original	O
system	O
showed	O
no	O
phase	O
transitions	O
then	O
the	O
scaled	O
up	O
system	O
won	O
t	O
have	O
any	O
either	O
only	O
systems	O
with	O
long-range	O
correlations	B
show	O
phase	O
transitions	O
long-range	O
correlations	B
do	O
not	O
require	O
long-range	O
energetic	O
couplings	O
for	O
example	O
a	O
magnet	B
has	O
only	O
short-range	O
couplings	O
adjacent	O
spins	O
but	O
these	O
are	O
to	O
create	O
long-range	O
order	O
why	O
are	O
points	O
at	O
which	O
derivatives	O
diverge	O
interesting	O
the	O
derivatives	O
of	O
ln	O
z	O
describe	O
properties	O
like	O
the	O
heat	B
capacity	B
of	O
the	O
system	O
s	O
the	O
second	O
derivative	O
or	O
its	O
in	O
energy	B
if	O
the	O
second	O
derivative	O
of	O
ln	O
z	O
diverges	O
at	O
a	O
temperature	B
then	O
the	O
heat	B
capacity	B
of	O
the	O
system	O
diverges	O
there	O
which	O
means	O
it	O
can	O
absorb	O
or	O
release	O
energy	B
without	O
changing	O
temperature	B
of	O
ice	O
melting	O
in	O
ice	O
water	O
when	O
the	O
system	O
is	O
at	O
equilibrium	O
at	O
that	O
temperature	B
its	O
energy	B
a	O
lot	O
in	O
contrast	O
to	O
the	O
normal	B
law-of-large-numbers	O
behaviour	O
where	O
the	O
energy	B
only	O
varies	O
by	O
one	O
part	O
in	O
pn	O
a	O
toy	O
system	O
that	O
shows	O
a	O
phase	B
transition	B
imagine	O
a	O
collection	O
of	O
n	O
coupled	O
spins	O
that	O
have	O
the	O
following	O
energy	B
as	O
a	O
function	O
of	O
their	O
state	O
x	O
ex	O
x	O
otherwise	O
this	O
energy	B
function	O
describes	O
a	O
ground	O
state	O
in	O
which	O
all	O
the	O
spins	O
are	O
aligned	O
in	O
the	O
zero	O
direction	O
the	O
energy	B
per	O
spin	O
in	O
this	O
state	O
is	O
if	O
any	O
spin	O
changes	O
state	O
then	O
the	O
energy	B
is	O
zero	O
this	O
model	B
is	O
like	O
an	O
extreme	O
version	O
of	O
a	O
magnetic	O
interaction	O
which	O
encourages	O
pairs	O
of	O
spins	O
to	O
be	O
aligned	O
we	O
can	O
contrast	O
it	O
with	O
an	O
ordinary	O
system	O
of	O
n	O
independent	O
spins	O
whose	O
energy	B
is	O
like	O
the	O
system	O
the	O
system	O
of	O
independent	O
spins	O
has	O
a	O
single	O
ground	O
state	O
with	O
energy	B
and	O
it	O
has	O
roughly	O
states	O
with	O
energy	B
very	O
close	O
to	O
so	O
the	O
low-temperature	O
and	O
high-temperature	O
properties	O
of	O
the	O
independent-spin	O
system	O
and	O
the	O
coupled-spin	O
system	O
are	O
virtually	O
identical	O
the	O
partition	B
function	I
of	O
the	O
coupled-spin	O
system	O
is	O
the	O
function	O
is	O
sketched	O
in	O
along	O
with	O
its	O
low	O
temperature	B
behaviour	O
ln	O
ln	O
n	O
and	O
its	O
high	O
temperature	B
behaviour	O
ln	O
n	O
ln	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
about	O
phase	O
transitions	O
log	O
z	O
n	O
beta	B
epsilon	O
n	O
log	O
vare	O
vare	O
beta	B
beta	B
figure	O
partition	B
function	I
of	O
toy	O
system	O
which	O
shows	O
a	O
phase	B
transition	B
for	O
large	O
n	O
the	O
arrow	O
marks	O
the	O
point	O
log	O
the	O
same	O
for	O
larger	O
n	O
the	O
variance	B
of	O
the	O
energy	B
of	O
the	O
system	O
as	O
a	O
function	O
of	O
for	O
two	O
system	O
sizes	O
as	O
n	O
increases	O
the	O
variance	B
has	O
an	O
increasingly	O
sharp	O
peak	O
at	O
the	O
critical	O
point	O
contrast	O
with	O
figure	O
the	O
partition	B
function	I
and	O
energy-variance	O
of	O
a	O
system	O
consisting	O
of	O
n	O
independent	O
spins	O
the	O
partition	B
function	I
changes	O
gradually	O
from	O
one	O
asymptote	O
to	O
the	O
other	O
regardless	O
of	O
how	O
large	O
n	O
is	O
the	O
variance	B
of	O
the	O
energy	B
does	O
not	O
have	O
a	O
peak	O
the	O
are	O
largest	O
at	O
high	O
temperature	B
and	O
scale	O
linearly	O
with	O
system	O
size	O
n	O
log	O
z	O
n	O
beta	B
epsilon	O
n	O
log	O
beta	B
log	O
z	O
n	O
beta	B
epsilon	O
n	O
log	O
vare	O
vare	O
beta	B
beta	B
the	O
arrow	O
marks	O
the	O
point	O
ln	O
at	O
which	O
these	O
two	O
asymptotes	O
intersect	O
in	O
the	O
limit	O
n	O
the	O
graph	B
of	O
ln	O
becomes	O
more	O
and	O
more	O
sharply	O
bent	O
at	O
this	O
point	O
the	O
second	O
derivative	O
of	O
ln	O
z	O
which	O
describes	O
the	O
variance	B
of	O
the	O
energy	B
of	O
the	O
system	O
has	O
a	O
peak	O
value	O
at	O
ln	O
roughly	O
equal	O
to	O
n	O
which	O
corresponds	O
to	O
the	O
system	O
spending	O
half	O
of	O
its	O
time	O
in	O
the	O
ground	O
state	O
and	O
half	O
its	O
time	O
in	O
the	O
other	O
states	O
at	O
this	O
critical	O
point	O
the	O
heat	B
capacity	B
of	O
this	O
system	O
is	O
thus	O
proportional	O
to	O
n	O
the	O
heat	B
capacity	B
per	O
spin	O
is	O
proportional	O
to	O
n	O
which	O
for	O
n	O
is	O
in	O
contrast	O
to	O
the	O
behaviour	O
of	O
systems	O
away	O
from	O
phase	O
transitions	O
whose	O
capacity	B
per	O
atom	O
is	O
a	O
number	O
for	O
comparison	O
shows	O
the	O
partition	B
function	I
and	O
energy-variance	O
of	O
the	O
ordinary	O
independent-spin	O
system	O
more	O
generally	O
phase	O
transitions	O
can	O
be	O
categorized	O
into	O
and	O
continuous	B
transitions	O
in	O
a	O
phase	B
transition	B
there	O
is	O
a	O
discontinuous	O
change	O
of	O
one	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
b	O
some	O
physics	B
or	O
more	O
order-parameters	O
in	O
a	O
continuous	B
transition	B
all	O
order-parameters	O
change	O
continuously	O
s	O
an	O
order-parameter	O
a	O
scalar	O
function	O
of	O
the	O
state	O
of	O
the	O
system	O
or	O
to	O
be	O
precise	O
the	O
expectation	B
of	O
such	O
a	O
function	O
in	O
the	O
vicinity	O
of	O
a	O
critical	O
point	O
the	O
concept	O
of	O
typicality	B
in	O
chapter	O
does	O
not	O
hold	O
for	O
example	O
our	O
toy	O
system	O
at	O
its	O
critical	O
point	O
has	O
a	O
chance	O
of	O
being	O
in	O
a	O
state	O
with	O
energy	B
and	O
roughly	O
a	O
chance	O
of	O
being	O
in	O
each	O
of	O
the	O
other	O
states	O
that	O
have	O
energy	B
zero	O
it	O
is	O
thus	O
not	O
the	O
case	O
that	O
ln	O
is	O
very	O
likely	O
to	O
be	O
close	O
to	O
the	O
entropy	B
of	O
the	O
system	O
at	O
this	O
point	O
unlike	O
a	O
system	O
with	O
n	O
i	O
i	O
d	O
components	O
remember	O
that	O
information	B
content	I
and	O
energy	B
are	O
very	O
closely	O
related	O
if	O
typicality	B
holds	O
then	O
the	O
system	O
s	O
energy	B
has	O
negligible	O
and	O
vice	O
versa	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
c	O
some	O
mathematics	O
finite	O
theory	O
most	O
linear	B
codes	I
are	O
expressed	O
in	O
the	O
language	O
of	O
galois	O
theory	O
why	O
are	O
galois	O
an	O
appropriate	O
language	O
for	O
linear	B
codes	I
first	O
a	O
and	O
some	O
examples	O
a	O
f	O
is	O
a	O
set	B
f	O
f	O
such	O
that	O
f	O
forms	O
an	O
abelian	O
group	O
under	O
an	O
addition	O
operation	O
with	O
being	O
the	O
identity	O
means	O
all	O
elements	O
commute	O
i	O
e	O
satisfy	O
a	O
b	O
b	O
a	O
f	O
forms	O
an	O
abelian	O
group	O
under	O
a	O
multiplication	O
operation	O
multiplication	O
of	O
any	O
element	O
by	O
yields	O
these	O
operations	O
satisfy	O
the	O
distributive	O
rule	O
c	O
c	O
c	O
for	O
example	O
the	O
real	O
numbers	O
form	O
a	O
with	O
and	O
denoting	O
ordinary	O
addition	O
and	O
multiplication	O
a	O
galois	O
gf	O
is	O
a	O
with	O
a	O
number	O
of	O
elements	O
q	O
a	O
unique	O
galois	O
exists	O
for	O
any	O
q	O
pm	O
where	O
p	O
is	O
a	O
prime	O
number	O
and	O
m	O
is	O
a	O
positive	O
integer	O
there	O
are	O
no	O
other	O
gf	O
the	O
addition	O
and	O
multiplication	O
tables	O
for	O
gf	O
are	O
shown	O
in	O
ta	O
ble	O
these	O
are	O
the	O
rules	B
of	O
addition	O
and	O
multiplication	O
modulo	O
gf	O
for	O
any	O
prime	O
number	O
p	O
the	O
addition	O
and	O
multiplication	O
rules	B
are	O
those	O
for	O
ordinary	O
addition	O
and	O
multiplication	O
modulo	O
p	O
gf	O
the	O
rules	B
for	O
gf	O
with	O
m	O
are	O
not	O
those	O
of	O
ordinary	O
addition	O
and	O
multiplication	O
for	O
example	O
the	O
tables	O
for	O
gf	O
are	O
not	O
the	O
rules	B
of	O
addition	O
and	O
multiplication	O
modulo	O
notice	O
that	O
for	O
example	O
so	O
how	O
can	O
gf	O
be	O
described	O
it	O
turns	O
out	O
that	O
the	O
elements	O
can	O
be	O
related	O
to	O
polynomials	O
consider	O
polynomial	O
functions	B
of	O
x	O
of	O
degree	B
and	O
with	O
that	O
are	O
elements	O
of	O
gf	O
the	O
polynomials	O
shown	O
in	O
table	O
obey	O
the	O
addition	O
and	O
multiplication	O
rules	B
of	O
gf	O
if	O
addition	O
and	O
multiplication	O
are	O
modulo	O
the	O
polynomial	O
x	O
and	O
the	O
of	O
the	O
polynomials	O
are	O
from	O
gf	O
for	O
example	O
b	O
b	O
x	O
a	O
each	O
element	O
may	O
also	O
be	O
represented	O
as	O
a	O
bit	B
pattern	O
as	O
shown	O
in	O
table	O
with	O
addition	O
being	O
bitwise	B
modulo	O
and	O
multiplication	O
with	O
an	O
appropriate	O
carry	O
operation	O
table	O
addition	O
and	O
multiplication	O
tables	O
for	O
gf	O
a	O
b	O
a	O
b	O
b	O
a	O
a	O
a	O
b	O
b	O
b	O
a	O
a	O
b	O
a	O
b	O
a	O
a	O
b	O
b	O
b	O
a	O
table	O
addition	O
and	O
multiplication	O
tables	O
for	O
gf	O
element	O
polynomial	O
bit	B
pattern	O
a	O
b	O
x	O
x	O
table	O
representations	O
of	O
the	O
elements	O
of	O
gf	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
c	O
some	O
mathematics	O
gf	O
we	O
can	O
denote	O
the	O
elements	O
of	O
gf	O
by	O
a	O
b	O
c	O
d	O
e	O
fg	O
each	O
element	O
can	O
be	O
mapped	O
onto	O
a	O
polynomial	O
over	O
gf	O
the	O
multiplication	O
and	O
addition	O
operations	O
are	O
given	O
by	O
multiplication	O
and	O
addition	O
of	O
the	O
polynomials	O
modulo	O
x	O
the	O
multiplication	O
table	O
is	O
given	O
below	O
element	O
polynomial	O
binary	O
representation	O
a	O
b	O
c	O
d	O
e	O
f	O
x	O
x	O
x	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
a	O
b	O
c	O
d	O
e	O
f	O
a	O
a	O
c	O
e	O
b	O
f	O
d	O
b	O
b	O
e	O
d	O
f	O
c	O
a	O
c	O
c	O
b	O
f	O
e	O
a	O
d	O
d	O
d	O
c	O
a	O
f	O
b	O
e	O
e	O
e	O
f	O
d	O
b	O
a	O
c	O
f	O
f	O
d	O
a	O
e	O
c	O
b	O
why	O
are	O
galois	O
relevant	O
to	O
linear	B
codes	I
imagine	O
generalizing	O
a	O
binary	O
generator	B
matrix	B
g	O
and	O
binary	O
vector	O
s	O
to	O
a	O
matrix	B
and	O
vector	O
with	O
elements	O
from	O
a	O
larger	O
set	B
and	O
generalizing	O
the	O
addition	O
and	O
multiplication	O
operations	O
that	O
the	O
product	O
gs	O
in	O
order	O
to	O
produce	O
an	O
appropriate	O
input	O
for	O
a	O
symmetric	B
channel	I
it	O
would	O
be	O
convenient	O
if	O
for	O
random	B
s	O
the	O
product	O
gs	O
produced	O
all	O
elements	O
in	O
the	O
enlarged	O
set	B
with	O
equal	O
probability	B
this	O
uniform	O
distribution	B
is	O
easiest	O
to	O
guarantee	O
if	O
these	O
elements	O
form	O
a	O
group	O
under	O
both	O
addition	O
and	O
multiplication	O
because	O
then	O
these	O
operations	O
do	O
not	O
break	O
the	O
symmetry	O
among	O
the	O
elements	O
when	O
two	O
random	B
elements	O
of	O
a	O
multiplicative	O
group	O
are	O
multiplied	O
together	O
all	O
elements	O
are	O
produced	O
with	O
equal	O
probability	B
this	O
is	O
not	O
true	O
of	O
other	O
sets	O
such	O
as	O
the	O
integers	O
for	O
which	O
the	O
multiplication	O
operation	O
is	O
more	O
likely	O
to	O
give	O
rise	O
to	O
some	O
elements	O
composite	O
numbers	O
than	O
others	O
galois	O
by	O
their	O
avoid	O
such	O
symmetry-breaking	O
eigenvectors	O
and	O
eigenvalues	O
a	O
right-eigenvector	O
of	O
a	O
square	B
matrix	B
a	O
is	O
a	O
non-zero	O
vector	O
er	O
that	O
aer	O
where	O
is	O
the	O
eigenvalue	B
associated	O
with	O
that	O
eigenvector	O
the	O
eigenvalue	B
may	O
be	O
a	O
real	O
number	O
or	O
complex	B
number	O
and	O
it	O
may	O
be	O
zero	O
eigenvectors	O
may	O
be	O
real	O
or	O
complex	B
a	O
left-eigenvector	O
of	O
a	O
matrix	B
a	O
is	O
a	O
vector	O
el	O
that	O
la	O
et	O
l	O
the	O
following	O
statements	O
for	O
right-eigenvectors	O
also	O
apply	O
to	O
left-eigenvectors	O
if	O
a	O
matrix	B
has	O
two	O
or	O
more	O
linearly	O
independent	O
right-eigenvectors	O
with	O
the	O
same	O
eigenvalue	B
then	O
that	O
eigenvalue	B
is	O
called	O
a	O
degenerate	O
eigenvalue	B
of	O
the	O
matrix	B
or	O
a	O
repeated	O
eigenvalue	B
any	O
linear	B
combination	B
of	O
those	O
eigenvectors	O
is	O
another	O
right-eigenvector	O
with	O
the	O
same	O
eigenvalue	B
the	O
principal	O
right-eigenvector	O
of	O
a	O
matrix	B
is	O
by	O
the	O
right	O
eigenvector	O
with	O
the	O
largest	O
associated	O
eigenvalue	B
if	O
a	O
real	O
matrix	B
has	O
a	O
right-eigenvector	O
with	O
complex	B
eigenvalue	B
x	O
yi	O
then	O
it	O
also	O
has	O
a	O
right-eigenvector	O
with	O
the	O
conjugate	O
eigenvalue	B
x	O
yi	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
eigenvectors	O
and	O
eigenvalues	O
symmetric	B
matrices	B
if	O
a	O
is	O
a	O
real	O
symmetric	B
n	O
n	O
matrix	B
then	O
all	O
the	O
eigenvalues	O
and	O
eigenvectors	O
of	O
a	O
are	O
real	O
every	O
left-eigenvector	O
of	O
a	O
is	O
also	O
a	O
right-eigenvector	O
of	O
a	O
with	O
the	O
same	O
eigenvalue	B
and	O
vice	O
versa	O
a	O
set	B
of	O
n	O
eigenvectors	O
and	O
eigenvalues	O
fea	O
are	O
orthonormal	O
that	O
is	O
can	O
be	O
found	O
that	O
the	O
matrix	B
can	O
be	O
expressed	O
as	O
a	O
weighted	O
sum	O
of	O
outer	O
products	O
of	O
the	O
eigenvectors	O
a	O
n	O
i	O
often	O
use	O
i	O
and	O
n	O
as	O
indices	O
for	O
sets	O
of	O
size	O
i	O
and	O
n	O
i	O
will	O
use	O
the	O
indices	O
a	O
and	O
b	O
to	O
run	O
over	O
eigenvectors	O
even	O
if	O
there	O
are	O
n	O
of	O
them	O
this	O
is	O
to	O
avoid	O
confusion	O
with	O
the	O
components	O
of	O
the	O
eigenvectors	O
which	O
are	O
indexed	O
by	O
n	O
e	O
g	O
ea	O
n	O
general	O
square	B
matrices	B
an	O
n	O
n	O
matrix	B
can	O
have	O
up	O
to	O
n	O
distinct	O
eigenvalues	O
generically	O
there	O
are	O
n	O
eigenvalues	O
all	O
distinct	O
and	O
each	O
has	O
one	O
left-eigenvector	O
and	O
one	O
righteigenvector	O
in	O
cases	O
where	O
two	O
or	O
more	O
eigenvalues	O
coincide	O
for	O
each	O
distinct	O
eigenvalue	B
that	O
is	O
non-zero	O
there	O
is	O
at	O
least	O
one	O
left-eigenvector	O
and	O
one	O
righteigenvector	O
left-	O
and	O
right-eigenvectors	O
that	O
have	O
eigenvalue	B
are	O
orthogonal	O
that	O
is	O
if	O
then	O
ea	O
l	O
r	O
non-negative	O
matrices	B
if	O
all	O
the	O
elements	O
of	O
a	O
non-zero	O
matrix	B
c	O
satisfy	O
cmn	O
then	O
c	O
is	O
a	O
non-negative	O
matrix	B
similarly	O
if	O
all	O
the	O
elements	O
of	O
a	O
non-zero	O
vector	O
c	O
satisfy	O
cn	O
then	O
c	O
is	O
a	O
non-negative	O
vector	O
properties	O
a	O
non-negative	O
matrix	B
has	O
a	O
principal	O
eigenvector	O
that	O
is	O
nonnegative	O
it	O
may	O
also	O
have	O
other	O
eigenvectors	O
with	O
the	O
same	O
eigenvalue	B
that	O
are	O
not	O
non-negative	O
but	O
if	O
the	O
principal	O
eigenvalue	B
of	O
a	O
non-negative	O
matrix	B
is	O
not	O
degenerate	O
then	O
the	O
matrix	B
has	O
only	O
one	O
principal	O
eigenvector	O
and	O
it	O
is	O
non-negative	O
generically	O
all	O
the	O
other	O
eigenvalues	O
are	O
smaller	O
in	O
absolute	O
magnitude	O
can	O
be	O
several	O
eigenvalues	O
of	O
identical	O
magnitude	O
in	O
special	O
cases	O
transition	B
probability	B
matrices	B
an	O
important	O
example	O
of	O
a	O
non-negative	O
matrix	B
is	O
a	O
transition	B
probability	B
matrix	B
q	O
a	O
transition	B
probability	B
matrix	B
q	O
has	O
columns	O
that	O
are	O
probability	B
vectors	B
that	O
is	O
it	O
q	O
and	O
qij	O
for	O
all	O
j	O
xi	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
matrix	B
eigenvalues	O
and	O
eigenvectors	O
el	O
er	O
c	O
some	O
mathematics	O
table	O
some	O
matrices	B
and	O
their	O
eigenvectors	O
matrix	B
eigenvalues	O
and	O
eigenvectors	O
el	O
er	O
table	O
transition	B
probability	B
matrices	B
for	O
generating	O
random	B
paths	O
through	O
trellises	O
this	O
property	O
can	O
be	O
rewritten	O
in	O
terms	O
of	O
the	O
all-ones	O
vector	O
n	O
so	O
n	O
is	O
the	O
principal	O
left-eigenvector	O
of	O
q	O
with	O
eigenvalue	B
ntq	O
nt	O
l	O
n	O
because	O
it	O
is	O
a	O
non-negative	O
matrix	B
q	O
has	O
a	O
principal	O
right-eigenvector	O
that	O
is	O
non-negative	O
r	O
generically	O
for	O
markov	O
processes	O
that	O
are	O
ergodic	B
this	O
eigenvector	O
is	O
the	O
only	O
right-eigenvector	O
with	O
eigenvalue	B
of	O
magnitude	O
table	O
for	O
illustrative	O
exceptions	O
this	O
vector	O
if	O
we	O
normalize	O
it	O
such	O
that	O
r	O
is	O
called	O
the	O
invariant	B
distribution	B
of	O
the	O
transition	B
probability	B
matrix	B
it	O
is	O
the	O
probability	B
density	B
that	O
is	O
left	O
unchanged	O
under	O
q	O
unlike	O
the	O
principal	O
left-eigenvector	O
which	O
we	O
explicitly	O
above	O
we	O
can	O
t	O
usually	O
identify	O
the	O
principal	O
right-eigenvector	O
without	O
computation	O
the	O
matrix	B
may	O
have	O
up	O
to	O
n	O
other	O
right-eigenvectors	O
all	O
of	O
which	O
are	O
orthogonal	O
to	O
the	O
left-eigenvector	O
n	O
that	O
is	O
they	O
are	O
zero-sum	O
vectors	B
perturbation	O
theory	O
perturbation	O
theory	O
is	O
not	O
used	O
in	O
this	O
book	O
but	O
it	O
is	O
useful	O
in	O
this	O
book	O
s	O
in	O
this	O
section	B
we	O
derive	O
perturbation	O
theory	O
for	O
the	O
eigenvectors	O
and	O
eigenvalues	O
of	O
square	B
not	O
necessarily	O
symmetric	B
matrices	B
most	O
presentations	O
of	O
perturbation	O
theory	O
focus	B
on	O
symmetric	B
matrices	B
but	O
nonsymmetric	O
matrices	B
as	O
transition	B
matrices	B
also	O
deserve	O
to	O
be	O
perturbed	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
perturbation	O
theory	O
matrix	B
eigenvalues	O
and	O
eigenvectors	O
el	O
er	O
table	O
illustrative	O
transition	B
probability	B
matrices	B
and	O
their	O
eigenvectors	O
showing	O
the	O
two	O
ways	O
of	O
being	O
non-ergodic	O
more	O
than	O
one	O
principal	O
eigenvector	O
with	O
eigenvalue	B
because	O
the	O
state	O
space	O
falls	O
into	O
two	O
unconnected	O
pieces	O
a	O
small	O
perturbation	O
breaks	O
the	O
degeneracy	O
of	O
the	O
principal	O
eigenvectors	O
under	O
this	O
chain	O
the	O
density	B
may	O
oscillate	O
between	O
two	O
parts	O
of	O
the	O
state	O
space	O
in	O
addition	O
to	O
the	O
invariant	B
distribution	B
there	O
is	O
another	O
right-eigenvector	O
with	O
eigenvalue	B
in	O
general	O
such	O
circulating	O
densities	O
correspond	O
to	O
complex	B
eigenvalues	O
with	O
magnitude	O
we	O
assume	O
that	O
we	O
have	O
an	O
n	O
n	O
matrix	B
h	O
that	O
is	O
a	O
function	O
of	O
a	O
real	O
parameter	O
with	O
being	O
our	O
starting	O
point	O
we	O
assume	O
that	O
a	O
taylor	O
expansion	O
of	O
is	O
appropriate	O
where	O
v	O
we	O
assume	O
that	O
for	O
all	O
of	O
interest	O
has	O
a	O
complete	O
set	B
of	O
n	O
righteigenvectors	O
and	O
left-eigenvectors	O
and	O
that	O
these	O
eigenvectors	O
and	O
their	O
eigenvalues	O
are	O
continuous	B
functions	B
of	O
this	O
last	O
assumption	O
is	O
not	O
necessarily	O
a	O
good	B
one	O
if	O
has	O
degenerate	O
eigenvalues	O
then	O
it	O
is	O
possible	O
for	O
the	O
eigenvectors	O
to	O
be	O
discontinuous	O
in	O
in	O
such	O
cases	O
degenerate	O
perturbation	O
theory	O
is	O
needed	O
that	O
s	O
a	O
fun	O
topic	O
but	O
let	O
s	O
stick	O
with	O
the	O
non-degenerate	O
case	O
here	O
we	O
write	O
the	O
eigenvectors	O
and	O
eigenvalues	O
as	O
follows	O
r	O
r	O
and	O
we	O
taylor-expand	O
with	O
and	O
ea	O
r	O
ea	O
r	O
r	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
with	O
c	O
some	O
mathematics	O
r	O
f	O
r	O
l	O
and	O
f	O
and	O
similar	O
for	O
ea	O
l	O
we	O
these	O
left-vectors	O
to	O
be	O
row	O
vectors	B
so	O
that	O
the	O
transpose	O
operation	O
is	O
not	O
needed	O
and	O
can	O
be	O
banished	O
we	O
are	O
free	O
to	O
constrain	O
the	O
magnitudes	O
of	O
the	O
eigenvectors	O
in	O
whatever	O
way	O
we	O
please	O
each	O
left-eigenvector	O
and	O
each	O
right-eigenvector	O
has	O
an	O
arbitrary	O
magnitude	O
the	O
natural	B
constraints	O
to	O
use	O
are	O
as	O
follows	O
first	O
we	O
constrain	O
the	O
inner	O
products	O
with	O
ea	O
l	O
r	O
for	O
all	O
a	O
expanding	O
the	O
eigenvectors	O
in	O
equation	O
implies	O
l	O
l	O
r	O
r	O
from	O
which	O
we	O
can	O
extract	O
the	O
terms	O
in	O
which	O
say	O
ea	O
l	O
r	O
f	O
l	O
ea	O
r	O
we	O
are	O
now	O
free	O
to	O
choose	O
the	O
two	O
constraints	O
l	O
ea	O
r	O
l	O
ea	O
f	O
r	O
which	O
in	O
the	O
special	O
case	O
of	O
a	O
symmetric	B
matrix	B
correspond	O
to	O
constraining	O
the	O
eigenvectors	O
to	O
be	O
of	O
constant	O
length	O
as	O
by	O
the	O
euclidean	O
norm	O
ok	O
now	O
that	O
we	O
have	O
our	O
cast	O
of	O
characters	O
what	O
do	O
the	O
equations	O
and	O
tell	O
us	O
about	O
our	O
taylor	O
expansions	O
and	O
we	O
expand	O
equation	O
in	O
r	O
r	O
r	O
r	O
identifying	O
the	O
terms	O
of	O
order	O
we	O
have	O
r	O
vea	O
r	O
r	O
r	O
we	O
can	O
extract	O
interesting	O
results	O
from	O
this	O
equation	O
by	O
hitting	O
it	O
with	O
eb	O
l	O
l	O
eb	O
r	O
eb	O
l	O
r	O
eb	O
l	O
r	O
l	O
r	O
l	O
r	O
eb	O
l	O
r	O
l	O
r	O
setting	O
b	O
a	O
we	O
obtain	O
ea	O
l	O
r	O
alternatively	O
choosing	O
b	O
a	O
we	O
obtain	O
eb	O
l	O
r	O
eb	O
l	O
r	O
eb	O
l	O
r	O
l	O
r	O
eb	O
now	O
assuming	O
that	O
the	O
right-eigenvectors	O
feb	O
we	O
must	O
be	O
able	O
to	O
write	O
r	O
form	O
a	O
complete	O
basis	O
f	O
r	O
wbeb	O
r	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
perturbation	O
theory	O
where	O
wb	O
eb	O
l	O
r	O
so	O
comparing	O
and	O
we	O
have	O
f	O
r	O
eb	O
l	O
r	O
eb	O
r	O
equations	O
and	O
are	O
the	O
solution	O
to	O
the	O
perturbation	O
theory	O
problem	O
giving	O
respectively	O
the	O
derivative	O
of	O
the	O
eigenvalue	B
and	O
the	O
eigenvectors	O
second-order	O
perturbation	O
theory	O
if	O
we	O
expand	O
the	O
eigenvector	O
equation	O
to	O
second	O
order	O
in	O
and	O
assume	O
that	O
the	O
equation	O
is	O
exact	O
that	O
is	O
h	O
is	O
a	O
purely	O
linear	B
function	O
of	O
then	O
we	O
have	O
r	O
r	O
r	O
r	O
r	O
r	O
and	O
are	O
the	O
second	O
derivatives	O
of	O
the	O
eigenvector	O
and	O
eigenvalue	B
where	O
ga	O
equating	O
the	O
second-order	O
terms	O
in	O
in	O
equation	O
r	O
vf	O
r	O
r	O
r	O
r	O
r	O
hitting	O
this	O
equation	O
on	O
the	O
left	O
with	O
ea	O
l	O
we	O
obtain	O
l	O
ea	O
r	O
l	O
r	O
the	O
term	O
ea	O
r	O
l	O
l	O
r	O
l	O
r	O
l	O
r	O
is	O
equal	O
to	O
zero	O
because	O
of	O
our	O
constraints	O
so	O
so	O
the	O
second	O
derivative	O
of	O
the	O
eigenvalue	B
with	O
respect	O
to	O
is	O
given	O
by	O
ea	O
l	O
r	O
ea	O
l	O
eb	O
l	O
r	O
r	O
eb	O
r	O
l	O
l	O
r	O
this	O
is	O
as	O
far	O
as	O
we	O
will	O
take	O
the	O
perturbation	O
expansion	O
summary	B
if	O
we	O
introduce	O
the	O
abbreviation	O
vba	O
for	O
eb	O
eigenvectors	O
of	O
to	O
order	O
as	O
l	O
r	O
we	O
can	O
write	O
the	O
ea	O
r	O
ea	O
r	O
vba	O
eb	O
r	O
and	O
the	O
eigenvalues	O
to	O
second	O
order	O
as	O
vbavab	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
some	O
numbers	O
c	O
some	O
mathematics	O
number	O
of	O
distinct	O
number	O
of	O
states	O
of	O
a	O
ising	B
model	B
with	O
spins	O
number	O
of	O
binary	O
strings	O
of	O
length	O
number	O
of	O
binary	O
strings	O
of	O
length	O
having	O
and	O
number	O
of	O
electrons	O
in	O
universe	O
number	O
of	O
electrons	O
in	O
solar	B
system	I
number	O
of	O
electrons	O
in	O
the	O
earth	O
age	O
of	O
universepicoseconds	O
age	O
of	O
universeseconds	O
number	O
of	O
neurons	O
in	O
human	B
brain	B
number	O
of	O
bits	O
stored	O
on	O
a	O
dvd	O
number	O
of	O
bits	O
in	O
the	O
wheat	O
genome	B
number	O
of	O
bits	O
in	O
the	O
human	B
genome	B
population	O
of	O
earth	O
number	O
of	O
bits	O
in	O
c	O
elegans	O
worm	O
genome	B
number	O
of	O
bits	O
in	O
arabidopsis	O
thaliana	O
plant	O
related	O
to	O
broccoli	O
genome	B
one	O
yearseconds	O
number	O
of	O
bits	O
in	O
the	O
compressed	O
postscript	O
that	O
is	O
this	O
book	O
number	O
of	O
bits	O
in	O
unix	O
kernel	B
number	O
of	O
bits	O
in	O
the	O
e	O
coli	O
genome	B
or	O
in	O
a	O
disk	O
number	O
of	O
years	O
since	O
humanchimpanzee	O
divergence	B
number	O
of	O
in	O
the	O
corpus	O
callosum	O
number	O
of	O
base	O
pairs	O
in	O
a	O
gene	O
number	O
of	O
generations	O
since	O
humanchimpanzee	O
divergence	B
number	O
of	O
genes	B
in	O
human	B
genome	B
number	O
of	O
genes	B
in	O
arabidopsis	O
thaliana	O
genome	B
lifetime	O
probability	B
of	O
dying	O
from	O
smoking	O
one	O
pack	O
of	O
cigarettes	O
per	O
day	O
lifetime	O
probability	B
of	O
dying	O
in	O
a	O
motor	O
vehicle	O
accident	O
lifetime	O
probability	B
of	O
developing	O
cancer	O
because	O
of	O
drinking	O
litres	O
per	O
day	O
of	O
water	O
containing	O
p	O
p	O
b	O
benzene	O
probability	B
of	I
error	I
in	O
transmission	O
of	O
coding	O
dna	B
per	O
nucleotide	B
per	O
generation	O
probability	B
of	O
undetected	O
error	O
in	O
a	O
hard	O
disk	B
drive	I
after	O
error	O
correction	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bibliography	O
abrahamsen	O
p	O
a	O
review	O
of	O
gaussian	B
random	B
and	O
correlation	O
functions	B
technical	O
report	O
norwegian	O
computing	O
center	O
blindern	O
oslo	O
norway	O
edition	O
abramson	O
n	O
information	B
theory	I
and	O
coding	O
mcgraw	O
hill	O
adler	O
s	O
l	O
over-relaxation	O
method	O
for	O
the	O
monte-carlo	O
evaluation	O
of	O
the	O
partition	B
function	I
for	O
multiquadratic	O
actions	O
physical	O
review	O
d	O
particles	O
and	O
fields	O
aiyer	B
s	O
v	O
b	O
solving	O
combinatorial	O
optimization	B
problems	O
using	O
neural	O
networks	O
cambridge	O
univ	O
engineering	O
dept	O
phd	O
dissertation	O
cuedf-infengtr	O
aji	O
s	O
jin	O
h	O
khandekar	O
a	O
mceliece	O
r	O
j	O
and	O
mackay	B
d	O
j	O
c	O
bsc	O
thresholds	O
for	O
code	O
ensembles	O
based	O
on	O
typical	B
pairs	O
decoding	B
in	O
codes	O
systems	O
and	O
graphical	O
models	O
ed	O
by	O
b	O
marcus	O
and	O
j	O
rosenthal	O
volume	B
of	O
ima	O
volumes	O
in	O
mathematics	O
and	O
its	O
applications	O
pp	O
springer	O
amari	O
s	O
cichocki	O
a	O
and	O
yang	O
h	O
h	O
a	O
new	O
learning	B
algorithm	O
for	O
blind	O
signal	O
separation	B
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
d	O
s	O
touretzky	O
m	O
c	O
mozer	O
and	O
m	O
e	O
hasselmo	O
volume	B
pp	O
mit	O
press	O
storing	O
numbers	O
of	O
patterns	O
in	O
a	O
spin	O
glass	O
model	B
of	O
neural	O
networks	O
phys	O
rev	O
lett	O
amit	O
d	O
j	O
gutfreund	O
h	O
and	O
sompolinsky	O
h	O
angel	O
j	O
r	O
p	O
wizinowich	O
p	O
lloyd-hart	O
m	O
and	O
sandler	O
d	O
adaptive	B
optics	O
for	O
array	O
telescopes	O
using	O
neural-network	O
techniques	O
nature	O
bahl	O
l	O
r	O
cocke	O
j	O
jelinek	O
f	O
and	O
raviv	O
j	O
optimal	B
decoding	B
of	O
linear	B
codes	I
for	O
minimizing	O
symbol	O
error	O
rate	B
ieee	O
trans	O
info	O
theory	O
baldwin	O
j	O
a	O
new	O
factor	O
in	B
evolution	B
american	B
natu	O
ralist	O
bar-shalom	O
y	O
and	O
fortmann	O
t	O
tracking	O
and	O
data	O
association	O
academic	O
press	O
barber	O
d	O
and	O
williams	O
c	O
k	O
i	O
gaussian	B
processes	I
for	O
bayesian	B
via	O
hybrid	O
monte	B
carlo	I
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
m	O
c	O
mozer	O
m	O
i	O
jordan	B
and	O
t	O
petsche	O
pp	O
mit	O
press	O
barnett	O
s	O
matrix	B
methods	O
for	O
engineers	O
and	O
scientists	B
mcgraw-hill	O
battail	O
g	O
we	O
can	O
think	O
of	O
good	B
codes	O
and	O
even	O
decode	O
them	O
in	O
eurocode	O
udine	O
italy	O
october	O
ed	O
by	O
p	O
camion	O
p	O
charpin	O
and	O
s	O
harari	O
number	O
in	O
cism	O
courses	O
and	O
lectures	O
pp	O
springer	O
baum	O
e	O
boneh	O
d	O
and	O
garrett	O
c	O
on	O
genetic	B
in	O
proc	O
eighth	O
annual	O
conf	O
on	O
computational	O
algorithms	B
learning	B
theory	O
pp	O
acm	O
baum	O
e	O
b	O
and	O
smith	O
w	O
d	O
best	O
play	O
for	O
imperfect	O
players	O
and	O
game	O
tree	B
search	O
technical	O
report	O
nec	O
princeton	O
nj	O
baum	O
e	O
b	O
and	O
smith	O
w	O
d	O
a	O
bayesian	B
approach	O
to	O
relevance	O
in	O
game	O
playing	O
intelligence	O
baum	O
l	O
e	O
and	O
petrie	O
t	O
inference	B
for	O
probabilistic	O
functions	B
of	O
markov	O
chains	O
ann	O
math	O
stat	O
statistical	B
beal	O
m	O
j	O
ghahramani	O
z	O
and	O
rasmussen	O
c	O
e	O
the	O
hidden	B
markov	I
model	B
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
mit	O
press	O
bell	O
a	O
j	O
and	O
sejnowski	B
t	O
j	O
an	O
information	B
maximization	I
approach	O
to	O
blind	O
separation	B
and	O
blind	O
deconvolution	B
neural	O
computation	O
bentley	O
j	O
programming	O
pearls	O
addison-wesley	O
sec	O
ond	O
edition	O
berger	O
j	O
statistical	B
decision	B
theory	I
and	O
bayesian	B
anal	O
ysis	O
springer	O
berlekamp	B
e	O
r	O
algebraic	B
coding	B
theory	I
mcgraw	O
hill	O
berlekamp	B
e	O
r	O
the	O
technology	O
of	O
error-correcting	B
codes	I
ieee	O
trans	O
info	O
theory	O
berlekamp	B
e	O
r	O
mceliece	O
r	O
j	O
and	O
van	O
tilborg	O
h	O
c	O
a	O
on	O
the	O
intractability	O
of	O
certain	O
coding	O
problems	O
ieee	O
trans	O
info	O
theory	O
berrou	B
c	O
and	O
glavieux	B
a	O
near	O
optimum	O
error	O
corieee	O
trans	O
on	O
recting	O
coding	O
and	O
decoding	B
turbo-codes	O
communications	O
berrou	B
c	O
glavieux	B
a	O
and	O
thitimajshima	B
p	O
near	O
shannon	B
limit	O
error-correcting	O
coding	O
and	O
decoding	B
turbocodes	O
in	O
proc	O
ieee	O
international	O
conf	O
on	O
communications	O
geneva	O
switzerland	O
pp	O
berzuini	O
c	O
best	O
n	O
g	O
gilks	B
w	O
r	O
and	O
larizza	O
c	O
dynamic	O
conditional	B
independence	B
models	O
and	O
markov	B
chain	I
monte	B
carlo	I
methods	I
j	O
american	B
statistical	B
assoc	O
berzuini	O
c	O
and	O
gilks	B
w	O
r	O
following	O
a	O
moving	O
target	O
monte	B
carlo	I
inference	B
for	O
dynamic	O
bayesian	B
models	O
j	O
royal	O
statistical	B
society	O
series	O
b	O
statistical	B
methodology	O
bhattacharyya	O
a	O
on	O
a	O
measure	O
of	O
divergence	B
between	O
two	O
statistical	B
populations	O
by	O
their	O
probability	B
distributions	O
bull	O
calcutta	O
math	O
soc	O
bishop	O
c	O
m	O
exact	O
calculation	O
of	O
the	O
hessian	B
matrix	B
for	O
the	O
multilayer	B
perceptron	I
neural	O
computation	O
bishop	O
c	O
m	O
neural	O
networks	O
for	O
pattern	B
recognition	B
oxford	O
univ	O
press	O
bishop	O
c	O
m	O
winn	O
j	O
m	O
and	O
spiegelhalter	O
d	O
vibes	B
a	O
variational	B
inference	B
engine	O
for	O
bayesian	B
networks	O
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
xv	O
ed	O
by	O
s	O
becker	O
s	O
thrun	O
and	O
k	O
obermayer	O
blahut	O
r	O
e	O
principles	O
and	O
practice	O
of	O
information	B
theory	I
addison-wesley	O
bottou	B
l	O
howard	O
p	O
g	O
and	O
bengio	O
y	O
the	O
zcoder	O
adaptive	B
binary	O
coder	O
in	O
proc	O
data	O
compression	B
conf	O
snowbird	O
utah	O
march	O
pp	O
box	B
g	O
e	O
p	O
and	O
tiao	O
g	O
c	O
bayesian	B
inference	B
in	O
statistical	B
analysis	O
addisonwesley	O
braunstein	O
a	O
m	O
and	O
zecchina	O
r	O
survey	B
propagation	I
an	O
algorithm	O
for	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bibliography	O
bretthorst	O
g	O
bayesian	B
spectrum	O
analysis	O
and	O
parameter	O
estimation	O
springer	O
also	O
available	O
at	O
bayes	B
wustl	O
edu	O
bridle	O
j	O
s	O
probabilistic	O
interpretation	O
of	O
feedforward	O
network	B
outputs	O
with	O
relationships	O
to	O
statistical	B
pattern	B
recognition	B
in	O
neuro-computing	O
algorithms	B
architectures	O
and	O
applications	O
ed	O
by	O
f	O
fougelman-soulie	O
and	O
j	O
springerverlag	O
bulmer	O
m	O
the	O
mathematical	O
theory	O
of	O
quantitative	O
genetics	O
oxford	O
univ	O
press	O
burrows	O
m	O
and	O
wheeler	O
d	O
j	O
a	O
block-sorting	B
lossless	B
data	O
compression	B
algorithm	O
technical	O
report	O
digital	O
src	O
byers	O
j	O
luby	B
m	O
mitzenmacher	B
m	O
and	O
rege	O
a	O
a	O
digital	B
fountain	I
approach	O
to	O
reliable	O
distribution	B
of	O
bulk	O
data	O
in	O
proc	O
acm	O
sigcomm	O
september	O
cairns-smith	O
a	O
g	O
seven	O
clues	O
to	O
the	O
origin	O
of	O
life	B
cambridge	O
univ	O
press	O
calderbank	O
a	O
r	O
and	O
shor	O
p	O
w	O
good	B
quantum	B
error-correcting	B
codes	I
exist	O
phys	O
rev	O
a	O
quant-ph	O
carroll	O
l	O
alice	B
s	O
adventures	O
in	O
wonderland	O
and	O
and	O
what	O
alice	B
found	O
there	O
through	O
the	O
looking-glass	O
macmillan	O
children	O
s	O
books	O
childs	O
a	O
m	O
patterson	O
r	O
b	O
and	O
mackay	B
d	O
j	O
c	O
exact	B
sampling	I
from	O
non-attractive	O
distributions	O
using	O
summary	B
states	O
physical	O
review	O
e	O
chu	O
w	O
keerthi	O
s	O
s	O
and	O
ong	O
c	O
j	O
a	O
loss	B
function	I
in	O
bayesian	B
framework	O
for	O
support	B
vector	I
regression	B
in	O
proc	O
international	O
conf	O
on	O
machine	B
learning	B
pp	O
chu	O
w	O
keerthi	O
s	O
s	O
and	O
ong	O
c	O
j	O
a	O
new	O
bayesian	B
design	O
method	O
for	O
support	B
vector	I
in	O
special	O
section	B
on	O
support	B
vector	I
machines	O
of	O
the	O
international	O
conf	O
on	O
neural	O
information	B
processing	O
chu	O
w	O
keerthi	O
s	O
s	O
and	O
ong	O
c	O
j	O
bayesian	B
support	B
vector	I
regression	B
using	O
a	O
loss	B
function	I
ieee	O
trans	O
on	O
neural	O
networks	O
submitted	O
chu	O
w	O
keerthi	O
s	O
s	O
and	O
ong	O
c	O
j	O
bayesian	B
trigonometric	O
support	B
vector	I
neural	O
computation	O
chung	O
s	O
-y	O
richardson	B
t	O
j	O
and	O
urbanke	B
r	O
l	O
analysis	O
of	O
sum-product	O
decoding	B
of	O
low-density	B
parity-check	I
codes	O
using	O
a	O
gaussian	B
approximation	B
ieee	O
trans	O
info	O
theory	O
chung	O
s	O
-y	O
urbanke	B
r	O
l	O
and	O
richardson	B
t	O
j	O
ldpc	O
code	O
design	O
applet	O
lids	O
mit	O
edusychung	O
gaopt	O
html	O
comon	O
p	O
jutten	O
c	O
and	O
herault	O
j	O
blind	O
separation	B
of	O
sources	O
problems	O
statement	O
signal	O
processing	O
copas	O
j	O
b	O
regression	B
prediction	B
and	O
shrinkage	O
discussion	O
j	O
r	O
statist	O
soc	O
b	O
cover	B
t	O
m	O
geometrical	O
and	O
statistical	B
properties	O
of	O
systems	O
of	O
linear	B
inequalities	O
with	O
applications	O
in	O
pattern	B
recognition	B
ieee	O
trans	O
on	O
electronic	O
computers	O
cover	B
t	O
m	O
and	O
thomas	O
j	O
a	O
elements	O
of	O
informa	O
tion	O
theory	O
wiley	O
cowles	O
m	O
k	O
and	O
carlin	O
b	O
p	O
markov-chain	O
montecarlo	O
convergence	O
diagnostics	O
a	O
comparative	O
review	O
j	O
american	B
statistical	B
assoc	O
cox	O
r	O
probability	B
frequency	B
and	O
reasonable	O
expecta	O
tion	O
am	O
j	O
physics	B
cressie	O
n	O
statistics	O
for	O
spatial	O
data	O
wiley	O
davey	B
m	O
c	O
error-correction	B
using	O
low-density	O
parity	B
check	O
codes	O
univ	O
of	O
cambridge	O
phd	O
dissertation	O
davey	B
m	O
c	O
and	O
mackay	B
d	O
j	O
c	O
low	O
density	B
parity	B
check	O
codes	O
over	O
gfq	O
ieee	O
communications	O
letters	O
davey	B
m	O
c	O
and	O
mackay	B
d	O
j	O
c	O
watermark	O
codes	O
reliable	O
communication	B
over	O
insertiondeletion	O
channels	O
in	O
proc	O
ieee	O
international	O
symposium	O
on	O
info	O
theory	O
p	O
davey	B
m	O
c	O
and	O
mackay	B
d	O
j	O
c	O
reliable	O
communication	B
over	O
channels	O
with	O
insertions	B
deletions	B
and	O
substitutions	O
ieee	O
trans	O
info	O
theory	O
dawid	O
a	O
stone	O
m	O
and	O
zidek	O
j	O
critique	O
of	O
e	O
t	O
jaynes	O
s	O
paradoxes	O
of	O
probability	B
theory	O
technical	O
report	O
dept	O
of	O
statistical	B
science	O
univ	O
college	O
london	O
dayan	O
p	O
hinton	B
g	O
e	O
neal	B
r	O
m	O
and	O
zemel	O
r	O
s	O
the	O
helmholtz	O
machine	O
neural	O
computation	O
divsalar	O
d	O
jin	O
h	O
and	O
mceliece	O
r	O
j	O
coding	O
theorems	O
for	O
turbo-like	O
codes	O
in	O
proc	O
allerton	O
conf	O
on	O
communication	B
control	O
and	O
computing	O
sept	O
pp	O
allerton	O
house	O
doucet	O
a	O
de	O
freitas	O
j	O
and	O
gordon	O
n	O
eds	O
se	O
quential	O
monte	B
carlo	I
methods	I
in	O
practice	O
springer	O
duane	O
s	O
kennedy	O
a	O
d	O
pendleton	O
b	O
j	O
and	O
roweth	O
d	O
hybrid	O
monte	B
carlo	I
physics	B
letters	O
b	O
durbin	O
r	O
eddy	O
s	O
r	O
krogh	O
a	O
and	O
mitchison	O
g	O
biological	O
sequence	B
analysis	O
probabilistic	O
models	O
of	O
proteins	O
and	O
nucleic	O
acids	O
cambridge	O
univ	O
press	O
dyson	O
f	O
j	O
origins	O
of	O
life	B
cambridge	O
univ	O
press	O
elias	B
p	O
universal	B
codeword	B
sets	O
and	O
representations	O
of	O
the	O
integers	O
ieee	O
trans	O
info	O
theory	O
felsenstein	O
j	O
eyre-walker	O
a	O
and	O
keightley	O
p	O
high	O
genomic	O
deleterious	O
mutation	O
rates	O
in	O
hominids	O
nature	O
is	O
maynard	O
smith	O
necessary	O
in	B
evolution	B
essays	O
in	O
honour	O
of	O
john	O
maynard	O
smith	O
ed	O
by	O
p	O
j	O
greenwood	O
p	O
h	O
harvey	O
and	O
m	O
slatkin	O
pp	O
cambridge	O
univ	O
press	O
recombination	O
and	O
sex	O
ferreira	O
h	O
clarke	O
w	O
helberg	O
a	O
abdel-ghaffar	O
k	O
s	O
and	O
vinck	O
a	O
h	O
insertiondeletion	O
correction	O
with	O
spectral	O
nulls	O
ieee	O
trans	O
info	O
theory	O
feynman	B
r	O
p	O
statistical	B
mechanics	O
addisonwesley	O
forney	O
jr	O
g	O
d	O
concatenated	B
codes	O
mit	O
press	O
forney	O
jr	O
g	O
d	O
codes	O
on	O
graphs	O
normal	B
realiza	O
tions	O
ieee	O
trans	O
info	O
theory	O
frey	B
b	O
j	O
graphical	O
models	O
for	O
machine	B
learning	B
and	O
digital	O
communication	B
mit	O
press	O
gallager	B
r	O
g	O
low	O
density	B
parity	B
check	O
codes	O
ire	O
trans	O
info	O
theory	O
gallager	B
r	O
g	O
low	O
density	B
parity	B
check	O
codes	O
number	O
in	O
mit	O
research	O
monograph	O
series	O
mit	O
press	O
available	O
from	O
www	O
inference	B
phy	O
cam	O
ac	O
ukmackaygallager	O
papers	O
gallager	B
r	O
g	O
information	B
theory	I
and	O
reliable	O
com	O
munication	O
wiley	O
gallager	B
r	O
g	O
variations	O
on	O
a	O
theme	O
by	O
ieee	O
trans	O
info	O
theory	O
gibbs	B
m	O
n	O
bayesian	B
gaussian	B
processes	I
for	O
regression	B
and	O
cambridge	O
univ	O
phd	O
dissertation	O
gibbs	B
m	O
n	O
and	O
mackay	B
d	O
j	O
c	O
effor	O
interpowww	O
inference	B
phy	O
cam	O
ac	O
ukmackayabstracts	O
implementation	O
of	O
gaussian	B
processes	I
lation	O
gpros	O
html	O
gibbs	B
m	O
n	O
and	O
mackay	B
d	O
j	O
c	O
variational	B
gaussian	B
process	I
ieee	O
trans	O
on	O
neural	O
networks	O
gilks	B
w	O
roberts	O
g	O
and	O
george	O
e	O
adaptive	B
direction	I
sampling	I
statistician	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bibliography	O
gilks	B
w	O
and	O
wild	O
p	O
adaptive	B
rejection	B
sampling	I
for	O
gibbs	B
sampling	I
applied	O
statistics	O
gilks	B
w	O
r	O
richardson	B
s	O
and	O
spiegelhalter	O
d	O
j	O
markov	B
chain	I
monte	B
carlo	I
in	O
practice	O
chapman	O
and	O
hall	O
goldie	O
c	O
m	O
and	O
pinch	O
r	O
g	O
e	O
communication	B
theory	O
cambridge	O
univ	O
press	O
golomb	O
s	O
w	O
peile	O
r	O
e	O
and	O
scholtz	O
r	O
a	O
basic	O
concepts	O
in	O
information	B
theory	I
and	O
coding	O
the	O
adventures	O
of	O
secret	B
agent	O
plenum	O
press	O
good	B
i	O
j	O
studies	O
in	O
the	O
history	O
of	O
probability	B
and	O
statistics	O
xxxvii	O
a	O
m	O
turing	B
s	O
statistical	B
work	O
in	O
world	O
war	O
ii	O
biometrika	O
graham	B
r	O
l	O
on	O
partitions	O
of	O
a	O
set	B
journal	O
of	O
combinatorial	O
theory	O
graham	B
r	O
l	O
and	O
knowlton	O
k	O
c	O
method	O
of	O
identifying	O
conductors	O
in	O
a	O
cable	O
by	O
establishing	O
conductor	O
connection	O
groupings	O
at	O
both	O
ends	O
of	O
the	O
cable	O
u	O
s	O
patent	O
green	O
p	O
j	O
reversible	B
jump	I
markov	B
chain	I
monte	B
carlo	I
computation	O
and	O
bayesian	B
model	B
determination	O
biometrika	O
gregory	O
p	O
c	O
and	O
loredo	O
t	O
j	O
a	O
new	O
method	O
for	O
the	O
detection	O
of	O
a	O
periodic	O
signal	O
of	O
unknown	O
shape	O
and	O
period	O
in	O
maximum	B
entropy	B
and	O
bayesian	B
methods	O
ed	O
by	O
g	O
erickson	O
and	O
c	O
smith	O
kluwer	O
also	O
in	O
astrophysical	O
journal	O
pp	O
oct	O
gull	B
s	O
f	O
bayesian	B
inductive	O
inference	B
and	O
maximum	B
entropy	B
in	O
maximum	B
entropy	B
and	O
bayesian	B
methods	O
in	O
science	O
and	O
engineering	O
vol	O
foundations	O
ed	O
by	O
g	O
erickson	O
and	O
c	O
smith	O
pp	O
kluwer	O
gull	B
s	O
f	O
developments	O
in	O
maximum	B
entropy	B
data	O
analysis	O
in	O
maximum	B
entropy	B
and	O
bayesian	B
methods	O
cambridge	O
ed	O
by	O
j	O
skilling	B
pp	O
kluwer	O
gull	B
s	O
f	O
and	O
daniell	O
g	O
image	B
reconstruction	I
from	O
incomplete	O
and	O
noisy	B
data	O
nature	O
hamilton	B
w	O
d	O
narrow	O
roads	O
of	O
gene	O
land	O
volume	B
evolution	B
of	O
sex	O
oxford	O
univ	O
press	O
hanson	O
r	O
stutz	O
j	O
and	O
cheeseman	O
p	O
bayesian	B
theory	O
technical	O
report	O
nasa	O
ames	O
hanson	O
r	O
stutz	O
j	O
and	O
cheeseman	O
p	O
bayesian	B
with	O
correlation	O
and	O
inheritance	O
in	O
proc	O
intern	O
joint	B
conf	O
on	O
intelligence	O
sydney	O
australia	O
volume	B
pp	O
morgan	O
kaufmann	O
hartmann	O
c	O
r	O
p	O
and	O
rudolph	O
l	O
d	O
an	O
optimum	O
symbol	O
by	O
symbol	O
decoding	B
rule	O
for	O
linear	B
codes	I
ieee	O
trans	O
info	O
theory	O
harvey	O
m	O
and	O
neal	B
r	O
m	O
inference	B
for	O
belief	B
networks	O
using	O
coupling	B
from	I
the	I
past	I
in	O
uncertainty	O
in	O
intelligence	O
proc	O
sixteenth	O
conf	O
pp	O
hebb	B
d	O
o	O
the	O
organization	O
of	O
behavior	O
wiley	O
hendin	O
o	O
horn	O
d	O
and	O
hopfield	O
j	O
j	O
decomposition	O
of	O
a	O
mixture	O
of	O
signals	O
in	O
a	O
model	B
of	O
the	O
olfactory	O
bulb	O
proc	O
natl	O
acad	O
sci	O
usa	O
hertz	B
j	O
krogh	O
a	O
and	O
palmer	O
r	O
g	O
introduction	O
to	O
the	O
theory	O
of	O
neural	O
computation	O
addison-wesley	O
hinton	B
g	O
training	O
products	O
of	O
experts	O
by	O
minimizing	O
contrastive	O
divergence	B
technical	O
report	O
gatsby	O
computational	O
neuroscience	O
unit	O
univ	O
college	O
london	O
hinton	B
g	O
and	O
nowlan	O
s	O
how	O
learning	B
can	O
guide	O
evolution	B
complex	B
systems	O
hinton	B
g	O
e	O
dayan	O
p	O
frey	B
b	O
j	O
and	O
neal	B
r	O
m	O
the	O
wake-sleep	O
algorithm	O
for	O
unsupervised	O
neural	O
networks	O
science	O
hinton	B
g	O
e	O
and	O
sejnowski	B
t	O
j	O
learning	B
and	O
relearning	O
in	O
boltzmann	B
machines	O
in	O
parallel	O
distributed	O
processing	O
ed	O
by	O
d	O
e	O
rumelhart	O
and	O
j	O
e	O
mcclelland	O
pp	O
mit	O
press	O
hinton	B
g	O
e	O
and	O
teh	O
y	O
w	O
discovering	O
multiple	O
constraints	O
that	O
are	O
frequently	O
approximately	O
in	O
uncertainty	O
in	O
intelligence	O
proc	O
seventeenth	O
conf	O
pp	O
morgan	O
kaufmann	O
hinton	B
g	O
e	O
and	O
van	O
camp	O
d	O
keeping	O
neural	O
networks	O
simple	O
by	O
minimizing	O
the	O
description	O
length	O
of	O
the	O
weights	O
in	O
proc	O
annual	O
workshop	O
on	O
comput	O
learning	B
theory	O
pp	O
acm	O
press	O
new	O
york	O
ny	O
hinton	B
g	O
e	O
welling	O
m	O
teh	O
y	O
w	O
and	O
osindero	O
s	O
a	O
new	O
view	O
of	O
ica	O
in	O
proc	O
international	O
conf	O
on	O
independent	B
component	I
analysis	I
and	O
blind	O
signal	O
separation	B
volume	B
hinton	B
g	O
e	O
and	O
zemel	O
r	O
s	O
autoencoders	O
minimum	B
description	I
length	I
and	O
helmholtz	O
free	O
energy	B
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
j	O
d	O
cowan	O
g	O
tesauro	O
and	O
j	O
alspector	O
morgan	O
kaufmann	O
hodges	O
a	O
alan	O
turing	B
the	O
enigma	B
simon	O
and	O
schus	O
ter	O
hojen-sorensen	O
p	O
a	O
winther	O
o	O
and	O
hansen	O
l	O
k	O
mean	B
approaches	O
to	O
independent	B
component	I
analysis	I
neural	O
computation	O
holmes	O
c	O
and	O
denison	O
d	O
perfect	B
sampling	O
for	O
wavelet	O
reconstruction	O
of	O
signals	O
ieee	O
trans	O
signal	O
processing	O
holmes	O
c	O
and	O
mallick	O
b	O
perfect	B
simulation	I
for	O
orthogonal	O
model	B
mixing	O
technical	O
report	O
imperial	O
college	O
london	O
hopfield	O
j	O
j	O
kinetic	O
proofreading	O
a	O
new	O
mechanism	O
for	O
reducing	O
errors	B
in	O
biosynthetic	O
processes	O
requiring	O
high	O
proc	O
natl	O
acad	O
sci	O
usa	O
hopfield	O
j	O
j	O
origin	O
of	B
the	I
genetic	B
code	I
a	O
testable	O
hypothesis	O
based	O
on	O
trna	O
structure	O
sequence	B
and	O
kinetic	O
proofreading	O
proc	O
natl	O
acad	O
sci	O
usa	O
hopfield	O
j	O
j	O
the	O
energy	B
relay	O
a	O
proofreading	O
scheme	O
based	O
on	O
dynamic	O
cooperativity	O
and	O
lacking	O
all	O
characteristic	O
symptoms	O
of	O
kinetic	O
proofreading	O
in	B
dna	B
replication	B
and	O
protein	B
synthesis	B
proc	O
natl	O
acad	O
sci	O
usa	O
hopfield	O
j	O
j	O
neural	O
networks	O
and	O
physical	O
systems	O
with	O
emergent	O
collective	B
computational	O
abilities	O
proc	O
natl	O
acad	O
sci	O
usa	O
hopfield	O
j	O
j	O
neurons	O
with	O
graded	O
response	O
properties	O
have	O
collective	B
computational	O
properties	O
like	O
those	O
of	O
two-state	O
neurons	O
proc	O
natl	O
acad	O
sci	O
usa	O
hopfield	O
j	O
j	O
learning	B
algorithms	B
and	O
probability	B
distributions	O
in	O
feed-forward	O
and	O
feed-back	O
networks	O
proc	O
natl	O
acad	O
sci	O
usa	O
hopfield	O
j	O
j	O
and	O
brody	B
c	O
d	O
what	O
is	O
a	O
moment	O
sensory	O
integration	O
over	O
a	O
brief	O
interval	O
proc	O
natl	O
acad	O
sci	O
hopfield	O
j	O
j	O
and	O
brody	B
c	O
d	O
what	O
is	O
a	O
moment	O
transient	O
synchrony	O
as	O
a	O
collective	B
mechanism	O
for	O
spatiotemporal	O
integration	O
proc	O
natl	O
acad	O
sci	O
hopfield	O
j	O
j	O
and	O
tank	B
d	O
w	O
neural	O
computation	O
of	O
decisions	O
in	O
optimization	B
problems	O
biol	O
cybernetics	O
howarth	O
p	O
and	O
bradley	O
a	O
the	O
longitudinal	O
aberration	O
of	O
the	O
human	B
eye	O
and	O
its	O
correction	O
vision	B
res	O
hinton	B
g	O
e	O
and	O
ghahramani	O
z	O
generative	O
models	O
for	O
discovering	O
sparse	O
distributed	O
representations	O
philosophical	O
trans	O
royal	O
society	O
b	O
huber	O
m	O
exact	B
sampling	I
and	O
approximate	O
counting	B
techniques	O
in	O
proc	O
acm	O
symposium	O
on	O
the	O
theory	O
of	O
computing	O
pp	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bibliography	O
huffman	O
d	O
a	O
method	O
for	O
construction	B
of	O
minimum	O
redundancy	B
codes	O
proc	O
of	O
ire	O
ichikawa	O
k	O
bhadeshia	O
h	O
k	O
d	O
h	O
and	O
mackay	B
d	O
j	O
c	O
model	B
for	O
hot	O
cracking	O
in	O
low-alloy	O
steel	O
weld	O
metals	O
science	O
and	O
technology	O
of	O
welding	O
and	O
joining	O
levenshtein	O
v	O
i	O
binary	O
codes	O
capable	O
of	O
correcting	O
deletions	B
insertions	B
and	O
reversals	O
soviet	O
physics	B
doklady	O
lin	O
s	O
and	O
costello	O
jr	O
d	O
j	O
error	O
control	O
coding	O
fundamentals	O
and	O
applications	O
prentice-hall	O
isard	O
m	O
and	O
blake	O
a	O
visual	O
tracking	O
by	O
stochastic	B
propagation	O
of	O
conditional	B
density	B
in	O
proc	O
fourth	O
european	O
conf	O
computer	B
vision	B
pp	O
litsyn	B
s	O
and	O
shevelev	O
v	O
on	O
ensembles	O
of	O
lowdensity	O
parity-check	O
codes	O
asymptotic	O
distance	B
distributions	O
ieee	O
trans	O
info	O
theory	O
isard	O
m	O
and	O
blake	O
a	O
condensation	O
conditional	B
density	B
propagation	O
for	O
visual	O
tracking	O
international	O
journal	O
of	O
computer	B
vision	B
jaakkola	B
t	O
s	O
and	O
jordan	B
m	O
i	O
computing	O
upper	O
and	O
lower	O
bounds	O
on	O
likelihoods	O
in	O
intractable	O
networks	O
in	O
proc	O
twelfth	O
conf	O
on	O
uncertainty	O
in	O
ai	O
morgan	O
kaufman	O
jaakkola	B
t	O
s	O
and	O
jordan	B
m	O
i	O
bayesian	B
logistic	O
regression	B
a	O
variational	B
approach	O
statistics	O
and	O
computing	O
jaakkola	B
t	O
s	O
and	O
jordan	B
m	O
i	O
bayesian	B
parameter	O
estimation	O
via	O
variational	B
methods	I
statistics	O
and	O
computing	O
jaynes	O
e	O
t	O
bayesian	B
intervals	B
versus	O
intervals	B
in	O
e	O
t	O
jaynes	O
papers	O
on	O
probability	B
statistics	O
and	O
statistical	B
physics	B
ed	O
by	O
r	O
d	O
rosenkrantz	O
p	O
kluwer	O
jaynes	O
e	O
t	O
probability	B
theory	O
the	O
logic	O
of	O
science	O
cambridge	O
univ	O
press	O
edited	O
by	O
g	O
larry	O
bretthorst	O
jensen	O
f	O
v	O
an	O
introduction	O
to	O
bayesian	B
networks	O
ucl	O
press	O
johannesson	O
r	O
and	O
zigangirov	O
k	O
s	O
fundamentals	O
of	O
convolutional	B
coding	O
ieee	O
press	O
jordan	B
m	O
i	O
ed	O
learning	B
in	O
graphical	O
models	O
nato	O
science	O
series	O
kluwer	O
academic	O
publishers	O
jpl	O
turbo	B
codes	I
performance	O
available	O
from	O
jutten	O
c	O
and	O
herault	O
j	O
blind	O
separation	B
of	O
sources	O
an	O
adaptive	B
algorithm	O
based	O
on	O
neuromimetic	O
architecture	B
signal	O
processing	O
karplus	O
k	O
and	O
krit	O
h	O
a	O
semi-systolic	O
decoder	B
for	O
the	O
error-correcting	B
code	I
discrete	O
applied	O
mathematics	O
kepler	O
t	O
and	O
oprea	O
m	O
improved	O
inference	B
of	O
mutation	O
rates	O
i	O
an	O
integral	B
representation	O
of	O
the	O
distribution	B
theoretical	O
population	O
biology	O
kimeldorf	O
g	O
s	O
and	O
wahba	O
g	O
a	O
correspondence	O
between	O
bayesian	B
estimation	O
of	O
stochastic	B
processes	O
and	O
smoothing	O
by	O
splines	O
annals	O
of	O
math	O
statistics	O
kitanidis	O
p	O
k	O
parameter	O
uncertainty	O
in	O
estimation	O
of	O
spatial	O
functions	B
bayesian	B
analysis	O
water	O
resources	O
research	O
loredo	O
t	O
j	O
from	O
laplace	B
to	O
supernova	O
sn	O
bayesian	B
inference	B
in	O
astrophysics	O
in	O
maximum	B
entropy	B
and	O
bayesian	B
methods	O
dartmouth	O
u	O
s	O
a	O
ed	O
by	O
p	O
fougere	O
pp	O
kluwer	O
lowe	O
d	O
g	O
similarity	O
metric	B
learning	B
for	O
a	O
variable	O
kernel	B
neural	O
computation	O
luby	B
m	O
lt	O
codes	O
in	O
proc	O
the	O
annual	O
ieee	O
symposium	O
on	O
foundations	O
of	O
computer	B
science	O
november	O
pp	O
luby	B
m	O
g	O
mitzenmacher	B
m	O
shokrollahi	B
m	O
a	O
and	O
spielman	B
d	O
a	O
improved	O
low-density	B
parity-check	I
codes	O
using	O
irregular	B
graphs	O
and	O
belief	B
propagation	I
in	O
proc	O
ieee	O
international	O
symposium	O
on	O
info	O
theory	O
p	O
luby	B
m	O
g	O
mitzenmacher	B
m	O
shokrollahi	B
m	O
a	O
and	O
spielman	B
d	O
a	O
erasure	B
correcting	O
codes	O
ieee	O
trans	O
info	O
theory	O
luby	B
m	O
g	O
mitzenmacher	B
m	O
shokrollahi	B
m	O
a	O
and	O
spielman	B
d	O
a	O
improved	O
low-density	B
parity-check	I
codes	O
using	O
irregular	B
graphs	O
and	O
belief	B
propagation	I
ieee	O
trans	O
info	O
theory	O
luby	B
m	O
g	O
mitzenmacher	B
m	O
shokrollahi	B
m	O
a	O
spielman	B
d	O
a	O
and	O
stemann	O
v	O
practical	B
loss-resilient	O
codes	O
in	O
proc	O
twenty-ninth	O
annual	O
acm	O
symposium	O
on	O
theory	O
of	O
computing	O
luo	O
z	O
and	O
wahba	O
g	O
hybrid	O
adaptive	B
splines	O
j	O
amer	O
statist	O
assoc	O
luria	B
s	O
e	O
and	O
m	O
mutations	O
of	O
bacteria	O
from	O
virus	O
sensitivity	O
to	O
virus	O
resistance	O
genetics	O
reprinted	O
in	O
microbiology	O
a	O
centenary	O
perspective	O
wolfgang	O
k	O
joklik	O
ed	O
asm	O
press	O
and	O
available	O
from	O
www	O
esp	O
org	O
luttrell	O
s	O
p	O
hierarchical	O
vector	O
quantisation	O
proc	O
iee	O
part	O
i	O
luttrell	O
s	O
p	O
derivation	B
of	O
a	O
class	O
of	O
training	O
algo	O
rithms	O
ieee	O
trans	O
on	O
neural	O
networks	O
mackay	B
d	O
j	O
c	O
bayesian	B
methods	O
for	O
adaptive	B
models	I
california	O
institute	O
of	O
technology	O
phd	O
dissertation	O
mackay	B
d	O
j	O
c	O
bayesian	B
interpolation	O
neural	O
com	O
putation	O
mackay	B
d	O
j	O
c	O
the	O
evidence	B
framework	O
applied	O
to	O
knuth	O
d	O
e	O
the	O
art	O
of	O
computer	B
programming	O
ad	O
networks	O
neural	O
computation	O
dison	O
wesley	O
kondrashov	O
a	O
s	O
deleterious	O
mutations	O
and	O
the	O
evo	O
lution	O
of	O
sexual	O
reproduction	O
nature	O
kschischang	O
f	O
r	O
frey	B
b	O
j	O
and	O
loeliger	O
h	O
-a	O
ieee	O
trans	O
factor	O
graphs	O
and	O
the	O
sum-product	O
algorithm	O
info	O
theory	O
kschischang	O
f	O
r	O
and	O
sorokine	O
v	O
on	O
the	O
trelieee	O
trans	O
info	O
theory	O
lis	O
structure	O
of	O
block	B
codes	O
lauritzen	O
s	O
l	O
time	O
series	O
analysis	O
in	O
a	O
discussion	O
of	O
contributions	O
made	O
by	O
t	O
n	O
thiele	B
isi	O
review	O
lauritzen	O
s	O
l	O
graphical	O
models	O
number	O
in	O
oxford	O
statistical	B
science	O
series	O
clarendon	O
press	O
mackay	B
d	O
j	O
c	O
a	O
practical	B
bayesian	B
framework	O
for	O
backpropagation	B
networks	O
neural	O
computation	O
bayesian	B
methods	O
for	O
backpropmackay	O
d	O
j	O
c	O
in	O
models	O
of	O
neural	O
networks	O
iii	O
ed	O
by	O
agation	O
networks	O
e	O
domany	O
j	O
l	O
van	O
hemmen	O
and	O
k	O
schulten	O
chapter	O
pp	O
springer	O
mackay	B
d	O
j	O
c	O
bayesian	B
non-linear	O
modelling	B
for	O
the	O
prediction	B
competition	O
in	O
ashrae	O
trans	O
pp	O
american	B
society	O
of	O
heating	O
refrigeration	O
and	O
air-conditioning	O
engineers	O
mackay	B
d	O
j	O
c	O
free	O
energy	B
minimization	B
algorithm	O
for	O
decoding	B
and	O
cryptanalysis	B
electronics	O
letters	O
lauritzen	O
s	O
l	O
and	O
spiegelhalter	O
d	O
j	O
local	O
computations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
and	O
their	O
application	O
to	O
expert	O
systems	O
j	O
royal	O
statistical	B
society	O
b	O
mackay	B
d	O
j	O
c	O
probable	O
networks	O
and	O
plausible	O
predictions	O
a	O
review	O
of	O
practical	B
bayesian	B
methods	O
for	O
supervised	O
neural	O
networks	O
network	B
computation	O
in	O
neural	O
systems	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bibliography	O
mackay	B
d	O
j	O
c	O
ensemble	B
learning	B
for	O
hidden	O
markov	O
www	O
inference	B
phy	O
cam	O
ac	O
ukmackayabstracts	O
models	O
ensemblepaper	O
html	O
mackay	B
d	O
j	O
c	O
iterative	B
probabilistic	I
decoding	B
of	O
low	O
density	B
parity	B
check	O
codes	O
animations	O
available	O
on	O
world	O
wide	O
web	O
www	O
inference	B
phy	O
cam	O
ac	O
ukmackaycodesgifs	O
mackay	B
d	O
j	O
c	O
choice	O
of	O
basis	O
for	O
laplace	B
approxi	O
matheron	O
g	O
principles	O
of	O
geostatistics	B
economic	O
ge	O
ology	O
maynard	O
smith	O
j	O
haldane	B
s	O
dilemma	O
and	O
the	O
rate	B
of	O
evolution	B
nature	O
maynard	O
smith	O
j	O
the	O
evolution	B
of	O
sex	O
cambridge	O
univ	O
press	O
maynard	O
smith	O
j	O
games	O
sex	O
and	O
evolution	B
mation	O
machine	B
learning	B
harvesterwheatsheaf	O
mackay	B
d	O
j	O
c	O
introduction	O
to	O
gaussian	B
processes	I
in	O
neural	O
networks	O
and	O
machine	B
learning	B
ed	O
by	O
c	O
m	O
bishop	O
nato	O
asi	O
series	O
pp	O
kluwer	O
mackay	B
d	O
j	O
c	O
comparison	O
of	O
approximate	O
methods	O
for	O
handling	O
hyperparameters	O
neural	O
computation	O
mackay	B
d	O
j	O
c	O
good	B
error	O
correcting	O
codes	O
based	O
on	O
very	O
sparse	O
matrices	B
ieee	O
trans	O
info	O
theory	O
mackay	B
d	O
j	O
c	O
an	O
alternative	O
to	O
runlength-limiting	O
codes	O
turn	O
timing	B
errors	B
into	O
substitution	O
errors	B
available	O
from	O
www	O
inference	B
phy	O
cam	O
ac	O
ukmackay	O
mackay	B
d	O
j	O
c	O
a	O
problem	O
with	O
variational	B
free	I
energy	B
minimization	B
www	O
inference	B
phy	O
cam	O
ac	O
ukmackay	O
abstractsminima	O
html	O
mackay	B
d	O
j	O
c	O
and	O
davey	B
m	O
c	O
evaluation	O
of	O
gallager	B
codes	O
for	O
short	O
block	B
length	O
and	O
high	O
rate	B
applications	O
in	O
codes	O
systems	O
and	O
graphical	O
models	O
ed	O
by	O
b	O
marcus	O
and	O
j	O
rosenthal	O
volume	B
of	O
ima	O
volumes	O
in	O
mathematics	O
and	O
its	O
applications	O
pp	O
springer	O
mackay	B
d	O
j	O
c	O
mitchison	O
g	O
j	O
and	O
mcfadden	O
p	O
l	O
sparse-graph	O
codes	O
for	O
quantum	B
error-correction	B
ieee	O
trans	O
info	O
theory	O
mackay	B
d	O
j	O
c	O
and	O
neal	B
r	O
m	O
good	B
codes	O
based	O
on	O
very	O
sparse	O
matrices	B
in	O
cryptography	B
and	O
coding	O
ima	O
conf	O
lncs	O
ed	O
by	O
c	O
boyd	O
pp	O
springer	O
mackay	B
d	O
j	O
c	O
and	O
neal	B
r	O
m	O
near	O
shannon	B
limit	O
performance	O
of	O
low	O
density	B
parity	B
check	O
codes	O
electronics	O
letters	O
reprinted	O
electronics	O
letters	O
march	O
mackay	B
d	O
j	O
c	O
and	O
peto	O
l	O
a	O
hierarchical	O
dirichlet	B
language	B
model	B
natural	B
language	O
engineering	O
mackay	B
d	O
j	O
c	O
wilson	B
s	O
t	O
and	O
davey	B
m	O
c	O
comparison	O
of	O
constructions	O
of	O
irregular	B
gallager	B
codes	O
in	O
proc	O
allerton	O
conf	O
on	O
communication	B
control	O
and	O
computing	O
sept	O
pp	O
allerton	O
house	O
mackay	B
d	O
j	O
c	O
wilson	B
s	O
t	O
and	O
davey	B
m	O
c	O
comparison	O
of	O
constructions	O
of	O
irregular	B
gallager	B
codes	O
ieee	O
trans	O
on	O
communications	O
mackay	B
d	O
m	O
and	O
mackay	B
v	O
the	O
time	O
course	O
of	O
the	O
mccollough	O
and	O
its	O
physiological	O
implications	O
j	O
physiol	O
mackay	B
d	O
m	O
and	O
mcculloch	O
w	O
s	O
the	O
limiting	O
information	B
capacity	B
of	O
a	O
neuronal	O
link	O
bull	O
math	O
biophys	O
macwilliams	O
f	O
j	O
and	O
sloane	O
n	O
j	O
a	O
the	O
theory	O
of	O
error-correcting	B
codes	I
north-holland	O
mandelbrot	B
b	O
the	O
fractal	O
geometry	O
of	O
nature	O
w	O
h	O
freeman	O
mao	O
y	O
and	O
banihashemi	O
a	O
design	O
of	O
good	B
ldpc	O
codes	O
using	O
girth	O
distribution	B
in	O
ieee	O
international	O
symposium	O
on	O
info	O
theory	O
italy	O
june	O
mao	O
y	O
and	O
banihashemi	O
a	O
a	O
heuristic	O
search	O
for	O
in	O
ieee	O
interna	O
good	B
ldpc	O
codes	O
at	O
short	O
block	B
lengths	O
tional	O
conf	O
on	O
communications	O
marinari	O
e	O
and	O
parisi	O
g	O
simulated	O
tempering	O
a	O
new	O
monte-carlo	O
scheme	O
europhysics	O
letters	O
maynard	O
smith	O
j	O
and	O
e	O
the	O
major	O
transitions	O
in	B
evolution	B
freeman	O
maynard	O
smith	O
j	O
and	O
e	O
the	O
origins	O
of	O
life	B
oxford	O
univ	O
press	O
mccollough	O
c	O
color	O
adaptation	O
of	O
edge-detectors	O
in	O
the	O
human	B
visual	O
system	O
science	O
mceliece	O
r	O
j	O
the	O
theory	O
of	O
information	B
and	O
coding	O
cambridge	O
univ	O
press	O
second	O
edition	O
mceliece	O
r	O
j	O
mackay	B
d	O
j	O
c	O
and	O
cheng	O
j	O
-f	O
turbo	O
decoding	B
as	O
an	O
instance	O
of	O
pearl	O
s	O
belief	B
propagation	I
algorithm	O
ieee	O
journal	O
on	O
selected	O
areas	O
in	O
communications	O
mcmillan	B
b	O
two	O
inequalities	O
implied	O
by	O
unique	O
deci	O
pherability	O
ire	O
trans	O
inform	O
theory	O
minka	B
t	O
a	O
family	O
of	O
algorithms	B
for	O
approximate	O
bayesian	B
inference	B
mit	O
phd	O
dissertation	O
miskin	O
j	O
w	O
ensemble	B
learning	B
for	O
independent	B
component	I
analysis	I
dept	O
of	O
physics	B
univ	O
of	O
cambridge	O
phd	O
dissertation	O
miskin	O
j	O
w	O
and	O
mackay	B
d	O
j	O
c	O
ensemble	B
learning	B
for	O
blind	O
image	B
separation	B
and	O
deconvolution	B
in	O
advances	O
in	O
independent	B
component	I
analysis	I
ed	O
by	O
m	O
girolami	O
springer	O
miskin	O
j	O
w	O
and	O
mackay	B
d	O
j	O
c	O
ensemble	B
learning	B
for	O
blind	O
source	O
separation	B
in	O
ica	O
principles	O
and	O
practice	O
ed	O
by	O
s	O
roberts	O
and	O
r	O
everson	O
cambridge	O
univ	O
press	O
mosteller	O
f	O
and	O
wallace	O
d	O
l	O
applied	O
bayesian	B
and	O
classical	O
inference	B
the	O
case	O
of	O
the	O
federalist	O
papers	O
springer	O
neal	B
r	O
m	O
bayesian	B
mixture	B
modelling	B
by	O
monte	B
carlo	I
simulation	O
technical	O
report	O
computer	B
science	O
univ	O
of	O
toronto	O
neal	B
r	O
m	O
bayesian	B
learning	B
via	O
stochastic	B
dynamics	O
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
c	O
l	O
giles	O
s	O
j	O
hanson	O
and	O
j	O
d	O
cowan	O
pp	O
morgan	O
kaufmann	O
neal	B
r	O
m	O
probabilistic	O
inference	B
using	O
markov	B
chain	I
monte	B
carlo	I
methods	I
technical	O
report	O
dept	O
of	O
computer	B
science	O
univ	O
of	O
toronto	O
neal	B
r	O
m	O
suppressing	O
random	B
walks	O
in	O
markov	B
chain	I
monte	B
carlo	I
using	O
ordered	B
overrelaxation	B
technical	O
report	O
dept	O
of	O
statistics	O
univ	O
of	O
toronto	O
neal	B
r	O
m	O
bayesian	B
learning	B
for	O
neural	O
networks	O
springer	O
neal	B
r	O
m	O
markov	B
chain	I
monte	B
carlo	I
methods	I
based	O
on	O
slicing	O
the	O
density	B
function	O
technical	O
report	O
dept	O
of	O
statistics	O
univ	O
of	O
toronto	O
neal	B
r	O
m	O
monte	B
carlo	I
implementation	O
of	O
gaussian	B
process	O
models	O
for	O
bayesian	B
regression	B
and	O
technical	O
report	O
dept	O
of	O
computer	B
science	O
univ	O
of	O
toronto	O
neal	B
r	O
m	O
annealed	O
importance	B
sampling	I
technical	O
report	O
dept	O
of	O
statistics	O
univ	O
of	O
toronto	O
neal	B
r	O
m	O
priors	O
for	O
distributions	O
using	O
dirichlet	B
trees	O
technical	O
report	O
dept	O
of	O
statistics	O
univ	O
of	O
toronto	O
neal	B
r	O
m	O
slice	B
sampling	I
annals	O
of	O
statistics	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bibliography	O
neal	B
r	O
m	O
and	O
hinton	B
g	O
e	O
a	O
new	O
view	O
of	O
the	O
em	B
algorithm	I
that	O
incremental	O
sparse	O
and	O
other	O
variants	O
in	O
learning	B
in	O
graphical	O
models	O
ed	O
by	O
m	O
i	O
jordan	B
nato	O
science	O
series	O
pp	O
kluwer	O
nielsen	O
m	O
and	O
chuang	O
i	O
quantum	B
computation	O
and	O
quantum	B
information	B
cambridge	O
univ	O
press	O
offer	O
e	O
and	O
soljanin	O
e	O
an	O
algebraic	O
description	O
of	O
iterative	O
decoding	B
schemes	O
in	O
codes	O
systems	O
and	O
graphical	O
models	O
ed	O
by	O
b	O
marcus	O
and	O
j	O
rosenthal	O
volume	B
of	O
ima	O
volumes	O
in	O
mathematics	O
and	O
its	O
applications	O
pp	O
springer	O
offer	O
e	O
and	O
soljanin	O
e	O
ldpc	O
codes	O
a	O
group	O
algebra	O
formulation	O
in	O
proc	O
internat	O
workshop	O
on	O
coding	O
and	O
cryptography	B
wcc	O
jan	O
paris	O
o	O
hagan	O
a	O
on	O
curve	O
and	O
optimal	B
design	O
for	O
regression	B
j	O
royal	O
statistical	B
society	O
b	O
o	O
hagan	O
a	O
monte	B
carlo	I
is	O
fundamentally	O
unsound	O
the	O
statistician	O
o	O
hagan	O
a	O
bayesian	B
inference	B
volume	B
of	O
kendall	O
s	O
advanced	O
theory	O
of	O
statistics	O
edward	O
arnold	O
omre	O
h	O
bayesian	B
kriging	B
merging	O
observations	O
and	O
guesses	O
in	O
kriging	B
mathematical	O
geology	O
gaussian	B
processes	I
for	O
algorithms	B
neural	O
computation	O
opper	O
m	O
and	O
winther	O
o	O
rasmussen	O
c	O
e	O
and	O
ghahramani	O
z	O
bayesian	B
monte	B
carlo	I
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
xv	O
ed	O
by	O
s	O
becker	O
s	O
thrun	O
and	O
k	O
obermayer	O
ratliff	O
f	O
and	O
riggs	O
l	O
a	O
involuntary	O
motions	O
of	O
the	O
eye	O
during	O
monocular	O
j	O
exptl	O
psychol	O
ratzer	O
e	O
a	O
and	O
mackay	B
d	O
j	O
c	O
sparse	O
low-density	B
parity-check	I
codes	O
for	O
channels	O
with	O
cross-talk	O
in	O
proc	O
ieee	O
info	O
theory	O
workshop	O
paris	O
reif	O
f	O
fundamentals	O
of	O
statistical	B
and	O
thermal	O
physics	B
mcgrawhill	O
richardson	B
t	O
shokrollahi	B
m	O
a	O
and	O
urbanke	B
r	O
design	O
of	O
capacity-approaching	O
irregular	B
low-density	O
parity	B
check	O
codes	O
ieee	O
trans	O
info	O
theory	O
the	O
capacity	B
of	O
low-density	O
parity	B
check	O
codes	O
under	O
message-passing	B
decoding	B
ieee	O
trans	O
info	O
theory	O
richardson	B
t	O
and	O
urbanke	B
r	O
richardson	B
t	O
and	O
urbanke	B
r	O
encoding	O
of	O
low-density	B
parity-check	I
codes	O
ieee	O
trans	O
info	O
theory	O
ridley	O
m	O
mendel	O
s	O
demon	O
gene	O
justice	O
and	O
the	O
com	O
plexity	O
of	O
life	B
phoenix	O
ripley	O
b	O
d	O
statistical	B
inference	B
for	O
spatial	O
processes	O
cambridge	O
univ	O
press	O
ripley	O
b	O
d	O
pattern	B
recognition	B
and	O
neural	O
networks	O
cambridge	O
univ	O
press	O
patrick	O
j	O
d	O
and	O
wallace	O
c	O
s	O
stone	O
circle	B
geometries	O
an	O
information	B
theory	I
approach	O
in	O
archaeoastronomy	O
in	O
the	O
old	O
world	O
ed	O
by	O
d	O
c	O
heggie	O
pp	O
cambridge	O
univ	O
press	O
rumelhart	O
d	O
e	O
hinton	B
g	O
e	O
and	O
williams	O
r	O
j	O
learning	B
representations	O
by	O
back-propagating	O
errors	B
nature	O
russell	O
s	O
and	O
wefald	O
e	O
do	B
the	I
right	I
thing	I
studies	O
pearl	O
j	O
probabilistic	O
reasoning	O
in	O
intelligent	O
systems	O
in	O
limited	O
rationality	O
mit	O
press	O
networks	O
of	O
plausible	O
inference	B
morgan	O
kaufmann	O
pearl	O
j	O
causality	O
cambridge	O
univ	O
press	O
pearlmutter	O
b	O
a	O
and	O
parra	O
l	O
c	O
a	O
contextsensitive	O
generalization	B
of	O
ica	O
in	O
international	O
conf	O
on	O
neural	O
information	B
processing	O
hong	O
kong	O
pp	O
pearlmutter	O
b	O
a	O
and	O
parra	O
l	O
c	O
maximum	B
likelihood	B
blind	O
source	O
separation	B
a	O
context-sensitive	O
generalization	B
of	O
ica	O
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
m	O
c	O
mozer	O
m	O
i	O
jordan	B
and	O
t	O
petsche	O
volume	B
p	O
mit	O
press	O
pinto	O
r	O
l	O
and	O
neal	B
r	O
m	O
improving	O
markov	B
chain	I
monte	B
carlo	I
estimators	O
by	O
coupling	O
to	O
an	O
approximating	O
chain	O
technical	O
report	O
dept	O
of	O
statistics	O
univ	O
of	O
toronto	O
poggio	O
t	O
and	O
girosi	O
f	O
a	O
theory	O
of	O
networks	O
for	O
approximation	B
and	B
learning	B
technical	O
report	O
a	O
i	O
mit	O
poggio	O
t	O
and	O
girosi	O
f	O
networks	O
for	O
approximation	B
and	B
learning	B
proc	O
ieee	O
polya	O
g	O
induction	O
and	O
analogy	O
in	O
mathematics	O
prince	O
ton	O
univ	O
press	O
schneier	O
b	O
applied	O
cryptography	B
wiley	O
scholkopf	O
b	O
burges	O
c	O
and	O
vapnik	O
v	O
extracting	O
support	O
data	O
for	O
a	O
given	O
task	O
in	O
proc	O
first	O
international	O
conf	O
on	O
knowledge	O
discovery	O
and	O
data	O
mining	O
ed	O
by	O
u	O
m	O
fayyad	O
and	O
r	O
uthurusamy	O
aaai	O
press	O
scholtz	O
r	O
a	O
the	O
origins	O
of	O
spread-spectrum	O
commu	O
nications	O
ieee	O
trans	O
on	O
communications	O
seeger	O
m	O
williams	O
c	O
k	O
i	O
and	O
lawrence	O
n	O
fast	O
forward	O
selection	O
to	O
speed	O
up	O
sparse	O
gaussian	B
process	O
regression	B
in	O
proc	O
ninth	O
international	O
workshop	O
on	O
intelligence	O
and	O
statistics	O
ed	O
by	O
c	O
bishop	O
and	O
b	O
j	O
frey	B
society	O
for	O
intelligence	O
and	O
statistics	O
sejnowski	B
t	O
j	O
higher	O
order	O
boltzmann	B
machines	O
in	O
neural	O
networks	O
for	O
computing	O
ed	O
by	O
j	O
denker	O
pp	O
american	B
institute	O
of	O
physics	B
sejnowski	B
t	O
j	O
and	O
rosenberg	O
c	O
r	O
parallel	O
networks	O
that	O
learn	O
to	O
pronounce	O
english	B
text	O
journal	O
of	O
complex	B
systems	O
shannon	B
c	O
e	O
a	O
mathematical	O
theory	O
of	O
communica	O
propp	B
j	O
g	O
and	O
wilson	B
d	O
b	O
exact	B
sampling	I
with	O
coupled	O
markov	O
chains	O
and	O
applications	O
to	O
statistical	B
mechanics	O
random	B
structures	O
and	O
algorithms	B
rabiner	O
l	O
r	O
and	O
juang	O
b	O
h	O
an	O
introduction	O
to	O
tion	O
bell	O
sys	O
tech	O
j	O
shannon	B
c	O
e	O
the	O
best	O
detection	O
of	O
pulses	O
in	O
collected	O
papers	O
of	O
claude	O
shannon	B
ed	O
by	O
n	O
j	O
a	O
sloane	O
and	O
a	O
d	O
wyner	O
pp	O
ieee	O
press	O
hidden	O
markov	O
models	O
ieee	O
assp	O
magazine	O
pp	O
shannon	B
c	O
e	O
and	O
weaver	O
w	O
the	O
mathematical	O
rasmussen	O
c	O
e	O
evaluation	O
of	O
gaussian	B
processes	I
and	O
other	O
methods	O
for	O
non-linear	O
regression	B
univ	O
of	O
toronto	O
phd	O
dissertation	O
rasmussen	O
c	O
e	O
the	O
gaussian	B
mixture	O
model	B
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
s	O
solla	O
t	O
leen	O
and	O
k	O
-r	O
pp	O
mit	O
press	O
rasmussen	O
c	O
e	O
reduced	O
rank	O
gaussian	B
process	O
learn	O
ing	O
unpublished	O
manuscript	O
rasmussen	O
c	O
e	O
and	O
ghahramani	O
z	O
mixtures	O
of	O
gaussian	B
process	O
experts	O
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
t	O
g	O
diettrich	O
s	O
becker	O
and	O
z	O
ghahramani	O
mit	O
press	O
theory	O
of	O
communication	B
univ	O
of	O
illinois	O
press	O
shokrollahi	B
a	O
raptor	B
codes	I
technical	O
report	O
laboratoire	O
d	O
algorithmique	O
polytechnique	O
de	O
lausanne	O
lausanne	O
switzerland	O
available	O
from	O
algo	O
epfl	O
ch	O
sipser	O
m	O
and	O
spielman	B
d	O
a	O
expander	O
codes	O
ieee	O
trans	O
info	O
theory	O
skilling	B
j	O
classic	O
maximum	B
entropy	B
in	O
maximum	B
entropy	B
and	O
bayesian	B
methods	O
cambridge	O
ed	O
by	O
j	O
skilling	B
kluwer	O
skilling	B
j	O
bayesian	B
numerical	O
analysis	O
in	O
physics	B
and	O
probability	B
ed	O
by	O
w	O
t	O
grandy	O
jr	O
and	O
p	O
milonni	O
cambridge	O
univ	O
press	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
bibliography	O
skilling	B
j	O
and	O
mackay	B
d	O
j	O
c	O
slice	B
sampling	I
a	O
binary	O
implementation	O
annals	O
of	O
statistics	O
discussion	O
of	O
slice	B
sampling	I
by	O
radford	O
m	O
neal	B
slepian	O
d	O
and	O
wolf	B
j	O
noiseless	B
coding	O
of	O
correlated	O
information	B
sources	O
ieee	O
trans	O
info	O
theory	O
smola	O
a	O
j	O
and	O
bartlett	O
p	O
sparse	O
greedy	O
gaussian	B
process	O
regression	B
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
t	O
k	O
leen	O
t	O
g	O
diettrich	O
and	O
v	O
tresp	O
pp	O
mit	O
press	O
spiegel	O
m	O
r	O
statistics	O
schaum	O
s	O
outline	O
series	O
mcgraw-hill	O
edition	O
spielman	B
d	O
a	O
able	O
error-correcting	B
codes	I
linear-time	O
encodable	O
and	O
decodieee	O
trans	O
info	O
theory	O
sutton	O
r	O
s	O
and	O
barto	O
a	O
g	O
reinforcement	O
learn	O
ing	O
an	O
introduction	O
mit	O
press	O
swanson	O
l	O
a	O
new	O
code	O
for	O
galileo	O
in	O
proc	O
ieee	O
international	O
symposium	O
info	O
theory	O
pp	O
tanner	B
m	O
a	O
tools	O
for	O
statistical	B
inference	B
methods	O
for	O
the	O
exploration	O
of	O
posterior	O
distributions	O
and	O
likelihood	B
functions	B
springer	O
series	O
in	B
statistics	I
springer	O
edition	O
tanner	B
r	O
m	O
a	O
recursive	O
approach	O
to	O
low	O
complexity	B
codes	O
ieee	O
trans	O
info	O
theory	O
teahan	O
w	O
j	O
probability	B
estimation	O
for	O
ppm	O
in	O
proc	O
nzcsrsc	O
available	O
from	O
citeseer	O
nj	O
nec	O
com	O
ten	O
brink	O
s	O
convergence	O
of	O
iterative	O
decoding	B
elec	O
tronics	O
letters	O
ten	O
brink	O
s	O
kramer	O
g	O
and	O
ashikhmin	O
a	O
design	O
of	O
low-density	B
parity-check	I
codes	O
for	O
multi-antenna	O
modulation	O
and	O
detection	O
submitted	O
to	O
ieee	O
trans	O
on	O
communications	O
terras	O
a	O
fourier	O
analysis	O
on	O
finite	O
groups	O
and	O
ap	O
plications	O
cambridge	O
univ	O
press	O
thomas	O
a	O
spiegelhalter	O
d	O
j	O
and	O
gilks	B
w	O
r	O
bugs	B
a	O
program	O
to	O
perform	O
bayesian	B
inference	B
using	O
gibbs	B
sampling	I
in	O
bayesian	B
statistics	O
ed	O
by	O
j	O
m	O
bernardo	O
j	O
o	O
berger	O
a	O
p	O
dawid	O
and	O
a	O
f	O
m	O
smith	O
pp	O
clarendon	O
press	O
tresp	O
v	O
a	O
bayesian	B
committee	O
machine	O
neural	O
com	O
putation	O
urbanke	B
r	O
ldpcopt	O
a	O
fast	O
and	O
accurate	O
degree	B
distribution	B
optimizer	O
for	O
ldpc	O
code	O
ensembles	O
lthcwww	O
epfl	O
ch	O
researchldpcopt	O
vapnik	O
v	O
the	O
nature	O
of	O
statistical	B
learning	B
theory	O
springer	O
viterbi	B
a	O
j	O
error	O
bounds	O
for	O
convolutional	B
codes	O
and	O
an	O
asymptotically	O
optimum	O
decoding	B
algorithm	O
ieee	O
trans	O
info	O
theory	O
wahba	O
g	O
spline	B
models	O
for	O
observational	O
data	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
cbms-nsf	O
regional	O
conf	O
series	O
in	O
applied	O
mathematics	O
wainwright	B
m	O
j	O
jaakkola	B
t	O
and	O
willsky	O
a	O
s	O
tree-based	O
reparameterization	O
framework	O
for	O
analysis	O
of	O
sumproduct	O
and	O
related	O
algorithms	B
ieee	O
trans	O
info	O
theory	O
wald	O
g	O
and	O
griffin	O
d	O
the	O
change	O
in	O
refractive	O
power	O
of	O
the	O
eye	O
in	O
bright	O
and	O
dim	O
light	O
j	O
opt	O
soc	O
am	O
wallace	O
c	O
and	O
boulton	O
d	O
an	O
information	B
measure	O
for	O
comput	O
j	O
wallace	O
c	O
s	O
and	O
freeman	O
p	O
r	O
estimation	O
and	O
inference	B
by	O
compact	O
coding	O
j	O
r	O
statist	O
soc	O
b	O
ward	O
d	O
j	O
blackwell	O
a	O
f	O
and	O
mackay	B
d	O
j	O
c	O
dasher	B
a	O
data	B
entry	I
interface	O
using	O
continuous	B
gestures	O
and	O
language	O
models	O
in	O
proc	O
user	O
interface	O
software	B
and	O
technology	O
pp	O
ward	O
d	O
j	O
and	O
mackay	B
d	O
j	O
c	O
fast	O
hands-free	O
writing	B
by	O
gaze	O
direction	O
nature	O
welch	O
t	O
a	O
a	O
technique	O
for	O
high-performance	O
data	O
compression	B
ieee	O
computer	B
welling	O
m	O
and	O
teh	O
y	O
w	O
belief	B
optimization	B
for	O
binary	O
networks	O
a	O
stable	O
alternative	O
to	O
loopy	B
belief	B
propagation	I
in	O
uncertainty	O
in	O
intelligence	O
proc	O
seventeenth	O
conf	O
pp	O
morgan	O
kaufmann	O
wiberg	B
n	O
codes	O
and	O
decoding	B
on	O
general	O
graphs	O
dept	O
of	O
elec	O
eng	O
sweden	O
phd	O
dissertation	O
studies	O
in	O
science	O
and	O
technology	O
no	O
wiberg	B
n	O
loeliger	O
h	O
-a	O
and	O
r	O
codes	O
and	O
iterative	O
decoding	B
on	O
general	O
graphs	O
european	O
trans	O
on	O
telecommunications	O
wiener	B
n	O
cybernetics	O
wiley	O
williams	O
c	O
k	O
i	O
and	O
rasmussen	O
c	O
e	O
gaussian	B
processes	I
for	O
regression	B
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
d	O
s	O
touretzky	O
m	O
c	O
mozer	O
and	O
m	O
e	O
hasselmo	O
mit	O
press	O
williams	O
c	O
k	O
i	O
and	O
seeger	O
m	O
using	O
the	O
method	O
to	O
speed	O
up	O
kernel	B
machines	O
in	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
ed	O
by	O
t	O
k	O
leen	O
t	O
g	O
diettrich	O
and	O
v	O
tresp	O
pp	O
mit	O
press	O
witten	O
i	O
h	O
neal	B
r	O
m	O
and	O
cleary	O
j	O
g	O
arithmetic	B
coding	I
for	O
data	O
compression	B
communications	O
of	O
the	O
acm	O
wolf	B
j	O
k	O
and	O
siegel	B
p	O
on	O
two-dimensional	B
arrays	O
and	O
crossword	B
puzzles	O
in	O
proc	O
allerton	O
conf	O
on	O
communication	B
control	O
and	O
computing	O
sept	O
pp	O
allerton	O
house	O
worthen	O
a	O
p	O
and	O
stark	O
w	O
e	O
low-density	O
parity	B
check	O
codes	O
for	O
fading	B
channels	O
with	B
memory	B
in	O
proc	O
allerton	O
conf	O
on	O
communication	B
control	O
and	O
computing	O
sept	O
pp	O
yedidia	B
j	O
s	O
an	O
idiosyncratic	O
journey	O
beyond	O
mean	B
theory	O
technical	O
report	O
mitsubishi	O
electric	O
res	O
labs	O
yedidia	B
j	O
s	O
freeman	O
w	O
t	O
and	O
weiss	O
y	O
generalized	B
belief	B
propagation	I
technical	O
report	O
mitsubishi	O
electric	O
res	O
labs	O
yedidia	B
j	O
s	O
freeman	O
w	O
t	O
and	O
weiss	O
y	O
bethe	B
free	I
energy	B
kikuchi	O
approximations	O
and	O
belief	B
propagation	I
algorithms	B
technical	O
report	O
mitsubishi	O
electric	O
res	O
labs	O
yedidia	B
j	O
s	O
freeman	O
w	O
t	O
and	O
weiss	O
y	O
characterization	O
of	O
belief	B
propagation	I
and	O
its	O
generalizations	O
technical	O
report	O
mitsubishi	O
electric	O
res	O
labs	O
yedidia	B
j	O
s	O
freeman	O
w	O
t	O
and	O
weiss	O
y	O
constructing	O
free	O
energy	B
approximations	O
and	O
generalized	B
belief	B
propagation	I
algorithms	B
technical	O
report	O
mitsubishi	O
electric	O
res	O
labs	O
yeung	O
r	O
w	O
a	O
new	O
outlook	O
on	O
shannon-information	O
measures	O
ieee	O
trans	O
info	O
theory	O
yuille	O
a	O
l	O
a	O
double-loop	O
algorithm	O
to	O
minimize	O
the	O
bethe	O
and	O
kikuchi	O
free	O
energies	O
in	O
energy	B
minimization	B
methods	O
in	O
computer	B
vision	B
and	O
pattern	B
recognition	B
ed	O
by	O
m	O
figueiredo	O
j	O
zerubia	O
and	O
a	O
jain	O
number	O
in	O
lncs	O
pp	O
springer	O
zipf	B
g	O
k	O
human	B
behavior	O
and	O
the	O
principle	O
of	O
least	O
addison-wesley	O
ziv	O
j	O
and	O
lempel	O
a	O
a	O
universal	B
algorithm	O
for	O
sequential	O
data	O
compression	B
ieee	O
trans	O
info	O
theory	O
ziv	O
j	O
and	O
lempel	O
a	O
compression	B
of	O
individual	O
sequences	O
via	O
variable-rate	O
coding	O
ieee	O
trans	O
info	O
theory	O
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
and	O
abu-mostafa	B
yaser	O
acceptance	B
rate	B
acceptance	B
ratio	I
method	I
accumulator	B
activation	B
function	I
activity	B
rule	I
adaptive	B
direction	I
sampling	I
adaptive	B
models	I
adaptive	B
rejection	B
sampling	I
address	B
aiyer	B
sree	O
alberto	B
alchemists	B
algebraic	B
coding	B
theory	I
algorithm	O
see	O
learning	B
algorithms	B
bcjr	B
belief	B
propagation	I
covariant	B
em	B
exact	B
sampling	I
expectationmaximization	O
function	B
minimization	B
genetic	B
hamiltonian	B
monte	B
carlo	I
independent	B
component	I
analysis	I
langevin	B
monte	B
carlo	I
leapfrog	B
maxproduct	O
message-passing	B
minsum	O
monte	B
carlo	I
see	O
monte	B
carlo	I
methods	I
newtonraphson	O
perfect	B
simulation	I
sumproduct	O
viterbi	B
alice	B
allais	B
paradox	B
alphabetical	B
ordering	I
america	B
american	B
amino	B
acid	I
anagram	B
index	O
annealing	B
deterministic	B
importance	B
sampling	I
antiferromagnetic	B
ape	B
approximation	B
by	B
gaussian	B
laplace	B
of	B
complex	B
distribution	B
of	B
density	B
evolution	B
saddle-point	B
stirling	B
variational	B
arabic	B
architecture	B
arithmetic	B
coding	I
decoder	B
software	B
uses	B
beyond	I
compression	B
arithmetic	B
progression	I
arms	B
race	B
intelligence	O
associative	B
memory	B
assumptions	B
astronomy	B
asymptotic	B
equipartition	B
why	B
it	I
is	I
a	I
misleading	I
term	I
atlantic	B
autoclass	B
automatic	O
relevance	O
determination	O
automobile	B
data	I
reception	I
average	O
see	O
expectation	B
awgn	B
background	B
rate	B
backpropagation	B
backward	B
pass	I
bad	B
see	O
error-correcting	B
code	I
balakrishnan	B
sree	O
balance	B
baldwin	O
ban	O
banburismus	B
band-limited	B
signal	I
bandwidth	B
bar-code	B
base	B
transitions	I
base-pairing	B
basis	B
dependence	I
bat	B
battleships	B
bayes	B
theorem	O
bayes	B
rev	O
thomas	O
bayesian	B
belief	B
networks	I
bayesian	B
inference	B
bch	B
codes	I
bcjr	B
algorithm	I
bearing	B
belarusian	B
belief	B
belief	B
propagation	I
see	O
message	B
passing	I
and	O
sumproduct	O
algorithm	O
benford	O
s	O
law	O
bent	B
coin	B
berlekamp	B
elwyn	O
bernoulli	B
distribution	B
berrou	B
c	O
bet	B
beta	B
distribution	B
beta	B
function	I
beta	B
integral	B
bethe	B
free	I
energy	B
bhattacharyya	B
parameter	I
bias	B
in	B
neural	I
net	I
in	B
statistics	I
biexponential	B
bifurcation	B
binary	B
entropy	B
function	I
binary	B
erasure	B
channel	I
binary	B
images	B
binary	B
representations	I
binary	B
symmetric	B
channel	I
binding	B
dna	B
binomial	B
distribution	B
bipartite	B
graph	B
birthday	B
bit	B
bit	B
bits	B
back	I
bivariate	B
gaussian	B
black	B
bletchley	B
park	I
blind	B
watchmaker	I
block	B
code	I
see	O
source	O
code	O
or	O
error-correcting	B
code	I
block-sorting	B
blood	B
group	I
blow	B
up	I
blur	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
index	O
bob	B
boltzmann	B
entropy	B
boltzmann	B
machine	I
bombes	B
book	B
isbn	B
bookies	B
bottou	B
leon	O
bound	B
union	B
bounded-distance	B
decoder	B
bounding	B
chain	I
box	B
boyish	B
matters	I
brain	B
bridge	B
british	B
broadcast	B
channel	I
brody	B
carlos	O
brownian	B
motion	I
bsc	O
see	O
channel	O
binary	B
symmetric	B
budget	B
s	O
needle	B
bugs	B
buoy	B
burglar	B
alarm	I
and	I
earthquake	B
burrowswheeler	O
transform	O
burst	B
errors	B
bus-stop	B
paradox	B
byte	B
cable	B
labelling	I
calculator	B
camera	B
canonical	B
capacity	B
channel	O
with	O
synchronization	B
errors	B
constrained	B
channel	I
gaussian	B
channel	I
network	B
neural	B
network	B
neuron	B
symmetry	B
argument	I
car	B
data	I
reception	I
card	B
casting	B
out	I
nines	B
cauchy	B
distribution	B
caution	O
see	O
sermon	O
equipartition	B
gaussian	B
distribution	B
importance	B
sampling	I
sampling	B
theory	I
cave	B
caveat	O
see	O
caution	O
and	O
sermon	O
cellphone	O
see	O
mobile	B
phone	B
cellular	B
automaton	I
central-limit	B
theorem	I
see	O
law	B
of	I
large	I
numbers	I
centre	B
of	I
gravity	I
chain	B
rule	I
challenges	B
tanner	B
channel	O
awgn	B
binary	B
erasure	B
binary	B
symmetric	B
broadcast	B
bursty	B
capacity	B
connection	B
with	I
physics	B
coding	B
theorem	I
see	O
noisy-channel	B
coding	B
theorem	I
complex	B
constrained	B
continuous	B
discrete	B
memoryless	I
erasure	B
extended	B
fading	B
gaussian	B
input	B
ensemble	B
multiple	B
access	I
multiterminal	B
noiseless	B
noisy	B
noisy	B
typewriter	I
symmetric	B
two-dimensional	B
unknown	B
noise	I
level	I
variable	B
symbol	I
durations	I
with	B
dependent	B
sources	I
with	B
memory	B
z	B
channel	I
cheat	B
chebyshev	B
inequality	B
checkerboard	B
bound	B
chess	B
chess	B
board	I
chi-squared	B
cholesky	B
decomposition	I
chromatic	B
aberration	I
cinema	B
circle	B
classical	B
statistics	I
criticisms	B
clockville	B
clustering	B
coalescence	B
cocked	B
hat	B
code	O
see	O
error-correcting	B
code	I
source	O
code	O
data	O
compression	B
symbol	B
code	I
arithmetic	B
coding	I
linear	B
code	O
random	B
code	I
or	O
hash	B
code	I
dual	B
see	O
error-correcting	B
code	I
dual	B
for	B
constrained	B
channel	I
variable-length	B
code-equivalent	B
codebreakers	B
codeword	B
see	O
source	O
code	O
symbol	B
code	I
or	O
error-correcting	B
code	I
coding	B
theory	I
coin	B
coincidence	B
collective	B
collision	B
coloured	B
noise	I
combination	B
commander	B
communication	B
v	O
broadcast	B
of	B
dependent	I
information	B
over	B
noiseless	B
channels	I
perspective	B
on	I
learning	B
competitive	B
learning	B
complexity	B
complexity	B
control	I
compress	B
compression	B
see	O
source	O
code	O
future	B
methods	I
lossless	B
lossy	B
of	O
already-compressed	O
of	O
any	O
universal	B
computer	B
concatenation	B
error-correcting	B
codes	I
in	B
compression	B
in	B
markov	I
chains	I
concave	O
conditional	B
entropy	B
cones	B
interval	O
level	O
confused	B
gameshow	I
host	I
conjugate	B
gradient	I
conjugate	B
prior	B
conjuror	B
connection	O
between	O
channel	B
capacity	B
and	I
physics	B
error	O
correcting	O
code	O
and	O
latent	B
variable	B
model	B
pattern	B
recognition	B
and	O
error-correction	B
supervised	O
and	O
unsupervised	O
learning	B
vector	B
quantization	I
and	O
error-correction	B
connection	B
matrix	B
constrained	B
channel	I
variable-length	B
code	I
constraint	B
satisfaction	I
content-addressable	B
memory	B
continuous	B
channel	I
control	B
treatment	I
conventions	O
see	O
notation	B
convex	B
hull	I
convex	O
convexity	B
convolution	B
convolutional	B
code	I
equivalence	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
conway	B
john	O
h	O
copernicus	B
correlated	B
sources	I
correlations	B
among	B
errors	B
and	B
phase	I
transitions	I
high-order	B
in	B
images	B
cost	B
function	I
cost	B
of	I
males	I
counting	B
counting	B
argument	I
coupling	B
from	I
the	I
past	I
covariance	B
covariance	B
function	I
covariance	B
matrix	B
covariant	B
algorithm	I
cover	B
thomas	O
cox	B
axioms	I
crib	B
critical	O
critical	B
path	I
cross-validation	B
crossover	B
crossword	B
cryptanalysis	B
cryptography	B
digital	B
signatures	I
tamper	B
detection	I
cumulative	B
probability	B
function	I
cycles	B
in	I
graphs	I
cyclic	B
dasher	B
data	O
compression	B
see	O
source	O
code	O
and	O
compression	B
data	B
entry	I
data	O
modelling	B
see	O
modelling	B
data	B
set	B
davey	B
matthew	O
c	O
death	B
penalty	I
deciban	O
decibel	B
decision	B
theory	I
decoder	B
bitwise	B
bounded-distance	B
codeword	B
maximum	B
a	I
posteriori	I
probability	B
of	I
error	I
deconvolution	B
degree	B
degree	B
sequence	B
see	O
degrees	B
of	I
belief	B
degrees	B
of	I
freedom	I
vu	O
delay	B
line	I
max	O
deletions	B
delta	B
function	I
density	B
evolution	B
density	B
modelling	B
dependent	B
sources	I
depth	B
of	I
lake	B
design	B
theory	I
detailed	B
balance	B
detection	B
of	I
forgery	B
deterministic	B
annealing	B
dictionary	B
die	B
rolling	O
cyclic	B
code	O
digamma	B
function	I
digital	B
cinema	B
digital	B
fountain	I
digital	B
signature	I
digital	B
video	I
broadcast	B
dimensions	B
dimer	B
directory	B
dirichlet	B
distribution	B
dirichlet	B
model	B
discriminant	B
function	I
discriminative	B
training	I
disease	B
disk	B
drive	I
distance	B
dkl	B
bad	B
distance	B
distribution	B
entropy	B
distance	B
gilbertvarshamov	O
good	B
hamming	B
isn	O
t	O
everything	O
of	B
code	I
goodbad	O
of	B
concatenated	B
code	I
of	B
product	B
code	I
relative	B
entropy	B
very	B
bad	B
distribution	B
beta	B
biexponential	B
binomial	B
cauchy	B
dirichlet	B
exponential	B
gamma	B
gaussian	B
sample	B
from	I
inverse-cosh	B
log-normal	B
normal	B
over	B
periodic	I
variables	I
poisson	B
student-t	B
von	B
mises	I
divergence	B
djvu	B
dna	B
replication	B
do	B
the	I
right	I
thing	I
dodecahedron	B
code	I
dongle	B
doors	B
on	O
game	B
show	I
dr	O
bloggs	O
draw	B
straws	B
dream	B
index	O
dsc	O
see	O
cyclic	B
code	O
dual	B
dumb	B
metropolis	I
earthquake	B
and	I
burglar	I
alarm	I
earthquake	B
during	O
game	B
show	I
ebert	B
todd	O
edge	B
eigenvalue	B
elias	B
peter	O
em	B
algorithm	I
email	B
empty	B
string	I
encoder	B
energy	B
english	B
enigma	B
ensemble	B
extended	B
ensemble	B
learning	B
entropic	B
distribution	B
entropy	B
boltzmann	B
conditional	B
gibbs	B
joint	B
marginal	B
mutual	B
information	B
of	B
continuous	B
variable	I
relative	B
entropy	B
distance	B
epicycles	B
equipartition	B
erasure	B
channel	I
erasure	B
correction	I
erf	O
see	O
error	B
function	I
ergodic	B
error	B
bars	I
error	O
correction	O
see	O
error-correcting	B
code	I
in	B
dna	B
replication	B
in	B
protein	B
synthesis	B
error	B
detection	I
error	O
error	B
function	I
error	B
probability	B
and	B
distance	B
block	B
in	B
compression	B
error-correcting	B
code	I
bad	B
block	B
code	I
concatenated	B
convolutional	B
cyclic	B
decoding	B
density	B
evolution	B
cyclic	B
distance	B
see	O
distance	B
dodecahedron	B
dual	B
equivalence	B
erasure	B
channel	I
error	B
probability	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
index	O
fountain	B
code	I
gallager	B
golay	B
good	B
hamming	B
in	B
dna	B
replication	B
in	B
protein	B
synthesis	B
interleaving	B
linear	B
coding	B
theorem	I
low-density	O
generator-matrix	O
low-density	B
parity-check	I
fast	B
encoding	I
staircase	B
lt	B
code	I
maximum	B
distance	B
separable	I
nonlinear	B
parity-check	B
code	I
pentagonful	B
perfect	B
practical	B
product	B
code	I
quantum	B
random	B
random	B
linear	B
raptor	B
code	I
rate	B
rateless	B
rectangular	B
reedsolomon	O
code	O
repeataccumulate	O
repetition	B
simple	B
parity	B
sparse	B
graph	B
density	B
evolution	B
syndrome	B
decoding	B
variable	B
rate	B
very	B
bad	B
very	B
good	B
weight	B
enumerator	I
with	O
varying	O
level	O
of	O
protection	O
error-reject	B
curves	I
errors	B
see	O
channel	O
estimator	B
eugenics	B
euro	B
evidence	B
typical	B
behaviour	I
of	I
evolution	B
as	B
learning	B
baldwin	O
colour	B
vision	B
of	B
the	I
genetic	B
code	I
evolutionary	B
computing	I
exact	B
sampling	I
exchange	B
rate	B
exchangeability	B
exclusive	B
or	I
exit	B
chart	I
expectation	B
expectation	B
propagation	I
expectationmaximization	O
algorithm	O
experimental	B
design	I
experimental	B
skill	I
explaining	B
away	I
exploit	B
explore	B
exponential	B
distribution	B
on	B
integers	I
exponential-family	B
expurgation	B
extended	B
channel	I
extended	B
code	I
extended	B
ensemble	B
extra	B
bit	B
extreme	B
value	I
eye	B
movements	I
factor	B
analysis	I
factor	B
graph	B
factorial	B
fading	B
channel	I
feedback	B
female	B
ferromagnetic	B
feynman	B
richard	O
fibonacci	B
see	O
galois	O
storage	O
theory	O
see	O
galois	O
point	O
florida	B
analysis	O
focus	B
football	B
pools	I
forensic	B
forgery	B
forward	B
pass	I
forward	B
probability	B
forwardbackward	O
algorithm	O
fotheringtonthomas	O
fountain	B
code	I
fourier	B
transform	I
fovea	B
free	O
energy	B
see	O
partition	B
function	B
minimization	B
variational	B
frequency	B
frequentist	O
see	O
sampling	B
theory	I
frey	B
brendan	O
j	O
frobeniusperron	O
theorem	O
frustration	B
full	B
probabilistic	B
model	B
function	B
minimization	B
functions	B
gain	B
galileo	B
code	I
gallager	B
code	I
gallager	B
robert	O
g	O
galois	O
gambling	B
game	O
see	O
puzzle	O
bridge	B
chess	B
guess	B
that	I
tune	I
guessing	B
life	B
sixty-three	B
submarine	B
three	B
doors	B
twenty	B
questions	I
game	B
show	I
game-playing	B
gamma	B
distribution	B
gamma	B
function	I
ganglion	B
cells	I
gaussian	B
channel	I
gaussian	B
distribution	B
n	O
approximation	B
parameters	B
sample	B
from	B
gaussian	B
processes	I
variational	B
general	B
position	I
generalization	B
generalized	B
parity-check	B
matrix	B
generating	B
function	I
generative	B
model	B
generator	B
matrix	B
genes	B
genetic	B
algorithm	I
genetic	B
code	I
genome	B
geometric	B
progression	I
geostatistics	B
gfq	O
see	O
galois	O
gibbs	B
entropy	B
gibbs	B
sampling	I
see	O
monte	B
carlo	I
methods	I
gibbs	B
inequality	B
gilbertvarshamov	O
conjecture	O
gilbertvarshamov	O
distance	B
gilbertvarshamov	O
rate	B
gilks	B
wally	O
r	O
girlie	O
glauber	B
dynamics	I
glavieux	B
a	O
golay	B
code	I
golden	B
ratio	I
good	B
see	O
error-correcting	B
code	I
good	B
jack	O
gradient	B
descent	I
natural	B
graduated	B
non-convexity	I
graham	B
ronald	O
l	O
grain	B
size	I
graph	B
factor	B
graph	B
of	B
code	I
graphs	B
and	I
cycles	I
guerilla	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
guessing	B
decoder	B
guessing	B
game	I
gull	B
steve	O
gzip	B
haldane	B
j	O
b	O
s	O
hamilton	B
william	O
d	O
hamiltonian	B
monte	B
carlo	I
hamming	B
code	I
graph	B
hamming	B
distance	B
handwritten	B
digits	I
hard	B
drive	I
hash	B
code	I
hash	B
function	I
linear	B
one-way	B
hat	B
puzzle	I
heat	B
bath	I
heat	B
capacity	B
hebb	B
donald	O
hebbian	B
learning	B
hertz	B
hessian	B
hidden	B
markov	I
model	B
hidden	B
neurons	I
hierarchical	B
clustering	B
hierarchical	B
model	B
high	B
dimensions	B
life	B
in	O
hint	O
for	O
computing	O
mutual	B
information	B
hinton	B
e	O
hitchhiker	B
homogeneous	B
hooke	B
robert	O
network	B
capacity	B
john	O
j	O
horse	B
race	B
hot-spot	B
code	O
optimality	O
disadvantages	B
general	B
alphabet	I
human	B
humanmachine	O
interfaces	O
hybrid	O
monte	B
carlo	I
see	O
hamiltonian	B
monte	B
carlo	I
hydrogen	B
bond	I
hyperparameter	B
hypersphere	B
hypothesis	O
testing	O
see	O
model	B
comparison	I
sampling	B
theory	I
i	O
i	O
d	O
ica	O
see	O
independent	B
component	I
analysis	I
icf	O
correlation	O
function	O
identical	B
twin	B
identity	B
matrix	B
ignorance	B
ill-posed	B
problem	I
image	B
integral	B
image	B
analysis	I
image	B
compression	B
image	B
models	I
image	B
processing	I
image	B
reconstruction	I
implicit	B
assumptions	B
implicit	B
probabilities	I
importance	B
sampling	I
weakness	B
of	I
improper	B
in-car	B
navigation	B
independence	B
independent	B
component	I
analysis	I
indicator	B
function	I
inequality	B
inference	B
and	B
learning	B
information	B
content	I
how	B
to	I
measure	I
shannon	B
information	B
maximization	I
information	B
retrieval	I
information	B
theory	I
inner	B
code	I
inquisition	B
insertions	B
instantaneous	B
integral	B
image	B
interleaving	B
internet	B
intersection	B
intrinsic	B
correlation	I
function	I
invariance	B
invariant	B
distribution	B
inverse	B
probability	B
inverse-arithmetic-coder	B
inverse-cosh	B
distribution	B
inverse-gamma	B
distribution	B
inversion	B
of	I
hash	B
function	I
investment	B
portfolio	B
irregular	B
isbn	B
ising	B
model	B
iterative	B
probabilistic	I
decoding	B
jaakkola	B
tommi	O
s	O
jacobian	B
janitor	B
prior	B
jensen	O
s	O
inequality	B
jet	B
propulsion	I
laboratory	I
johnson	B
noise	I
joint	B
ensemble	B
joint	B
entropy	B
joint	B
typicality	B
joint	B
typicality	B
theorem	I
jordan	B
michael	O
i	O
journal	B
publication	I
policy	I
judge	B
juggling	B
junction	B
tree	B
algorithm	I
jury	B
index	O
k-means	B
clustering	B
derivation	B
soft	B
kaboom	B
kalman	O
kernel	B
key	O
points	O
communication	B
how	B
much	I
data	I
needed	I
likelihood	B
principle	I
model	B
comparison	I
monte	B
carlo	I
solving	B
probability	B
problems	I
keyboard	B
kikuchi	B
free	I
energy	B
kl	B
distance	B
knowltongraham	O
partitions	O
knuth	O
donald	O
xii	O
kolmogorov	B
andrei	O
nikolaevich	O
kraft	B
inequality	B
kraft	B
l	O
g	O
kriging	B
kullbackleibler	O
divergence	B
see	O
relative	B
entropy	B
lagrange	B
multiplier	I
lake	B
langevin	B
method	I
langevin	B
process	I
language	B
model	B
laplace	B
approximation	B
see	O
laplace	B
s	O
method	O
laplace	B
model	B
laplace	B
prior	B
laplace	B
s	O
method	O
laplace	B
s	O
rule	O
latent	B
variable	I
latent	B
variable	B
model	B
compression	B
law	B
of	I
large	I
numbers	I
lawyer	B
le	B
cun	I
yann	O
leaf	B
leapfrog	B
algorithm	I
learning	B
as	B
communication	B
as	B
inference	B
hebbian	B
in	B
evolution	B
learning	B
algorithms	B
see	O
algorithm	O
backpropagation	B
boltzmann	B
machine	I
competitive	B
learning	B
network	B
k-means	B
clustering	B
multilayer	B
perceptron	I
single	B
neuron	B
learning	B
rule	I
lempelziv	O
coding	O
criticisms	B
life	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
index	O
life	B
in	I
high	B
dimensions	B
likelihood	B
contrasted	B
with	I
probability	B
subjectivity	B
likelihood	B
equivalence	B
likelihood	B
principle	I
limit	B
cycle	I
linear	B
block	B
code	I
coding	B
theorem	I
decoding	B
linear	B
regression	B
linear-feedback	B
shift-register	I
litsyn	B
simon	O
little	O
n	O
large	O
data	B
set	B
log-normal	B
logarithms	B
logit	B
long	B
thin	I
strip	I
loopy	B
belief	B
propagation	I
loopy	B
message-passing	B
loss	B
function	I
lossy	B
compression	B
low-density	O
generator-matrix	O
code	O
low-density	B
parity-check	B
code	I
see	O
error-correcting	B
code	I
lt	B
code	I
luby	B
michael	O
g	O
luria	B
salvador	O
lyapunov	B
function	I
machine	B
learning	B
macho	B
mackay	B
david	O
j	O
c	O
magician	B
magnet	B
magnetic	B
recording	I
majority	B
vote	I
male	B
mandelbrot	B
benoit	O
map	O
see	O
maximum	B
a	I
posteriori	I
mapping	B
marginal	B
entropy	B
marginal	B
likelihood	B
see	O
evidence	B
marginal	B
probability	B
marginalization	B
markov	B
chain	I
construction	B
markov	B
chain	I
monte	B
carlo	I
see	O
monte	B
carlo	I
methods	I
markov	O
model	B
see	O
markov	B
chain	I
marriage	B
matrix	B
matrix	B
identities	I
maxproduct	O
maxent	O
see	O
maximum	B
entropy	B
maximum	B
distance	B
separable	I
maximum	B
entropy	B
maximum	B
likelihood	B
maximum	B
a	I
posteriori	I
mccollough	O
mcmc	O
chain	O
monte	B
carlo	I
see	O
monte	B
carlo	I
methods	I
mcmillan	B
b	O
mdl	O
see	O
minimum	B
description	I
length	I
mds	B
mean	B
mean	B
theory	O
melody	B
memory	B
address-based	B
associative	B
content-addressable	B
memsys	B
message	B
passing	I
bcjr	B
belief	B
propagation	I
forwardbackward	O
in	B
graphs	I
with	I
cycles	I
loopy	B
sumproduct	O
algorithm	O
viterbi	B
metacode	B
metric	B
metropolis	B
method	I
see	O
monte	B
carlo	I
methods	I
marc	O
micro-saccades	B
microcanonical	B
microsoftus	B
microwave	B
oven	I
minsum	O
algorithm	O
mine	O
in	O
ground	O
minimax	B
minimization	B
see	O
optimization	B
minimum	B
description	I
length	I
minimum	O
distance	B
see	O
distance	B
minka	B
thomas	O
mirror	B
mitzenmacher	B
michael	O
mixing	O
mixture	B
modelling	B
mixture	B
of	I
gaussians	I
mixtures	B
in	B
markov	I
chains	I
ml	O
see	O
maximum	B
likelihood	B
mlp	O
see	O
multilayer	B
perceptron	I
mml	O
see	O
minimum	B
description	I
length	I
mobile	B
phone	B
model	B
model	B
comparison	I
typical	B
evidence	B
modelling	B
density	B
modelling	B
images	B
latent	B
variable	I
models	I
nonparametric	B
moderation	O
see	O
marginalization	B
molecules	B
molesworth	B
momentum	B
monte	B
carlo	I
methods	I
acceptance	B
rate	B
acceptance	B
ratio	I
method	I
and	B
communication	B
annealed	O
importance	B
sampling	I
coalescence	B
dependence	B
on	I
dimension	I
exact	B
sampling	I
for	B
visualization	B
gibbs	B
sampling	I
hamiltonian	B
monte	B
carlo	I
hybrid	O
monte	B
carlo	I
see	O
hamiltonian	B
monte	B
carlo	I
importance	B
sampling	I
weakness	B
of	I
langevin	B
method	I
markov	B
chain	I
monte	B
carlo	I
metropolis	B
method	I
dumb	B
metropolis	I
metropolishastings	O
multi-state	B
overrelaxation	B
perfect	B
simulation	I
random	B
walk	I
suppression	B
random-walk	B
metropolis	I
rejection	B
sampling	I
adaptive	B
reversible	B
jump	I
simulated	B
annealing	B
slice	B
sampling	I
thermodynamic	B
integration	I
umbrella	B
sampling	I
monty	B
hall	I
problem	I
morse	B
motorcycle	B
movie	B
multilayer	B
perceptron	I
multiple	B
access	I
channel	I
multiterminal	B
networks	I
multivariate	B
gaussian	B
munrorobbins	O
theorem	O
murder	B
music	B
mutation	B
rate	B
mutual	B
information	B
how	B
to	I
compute	I
myth	B
compression	B
nat	O
natural	B
gradient	I
natural	B
selection	I
navigation	B
neal	B
radford	O
m	O
needle	B
s	O
network	B
neural	B
network	B
capacity	B
learning	B
as	B
communication	B
learning	B
as	B
inference	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
neuron	B
capacity	B
newton	B
isaac	O
newtonraphson	O
method	O
nines	B
noise	O
see	O
channel	O
coloured	B
spectral	B
density	B
white	B
noisy	B
channel	O
see	O
channel	O
noisy	B
typewriter	I
noisy-channel	B
coding	B
theorem	I
gaussian	B
channel	I
linear	B
codes	I
poor	O
man	O
s	O
version	O
noisy-or	B
non-confusable	B
inputs	I
noninformative	B
nonlinear	B
nonlinear	B
code	I
nonparametric	B
data	I
modelling	B
nonrecursive	B
noodle	B
s	O
normal	B
see	O
gaussian	B
normal	B
graph	B
normalizing	O
constant	O
see	O
partition	B
function	I
not-sum	B
notation	B
absolute	B
value	I
conventions	B
of	I
this	I
book	I
convexconcave	O
entropy	B
error	B
function	I
expectation	B
intervals	B
logarithms	B
matrices	B
probability	B
set	B
size	I
transition	B
probability	B
vectors	B
np-complete	B
nucleotide	B
nuisance	B
parameters	B
numerology	B
nyquist	B
sampling	I
theorem	I
objective	B
function	I
occam	B
factor	I
occam	O
s	O
razor	O
octal	B
octave	B
odds	B
ode	B
to	I
joy	I
for	O
whimsical	O
departmental	O
rules	B
oliver	B
one-way	B
hash	B
function	I
optic	B
nerve	I
optimal	B
decoder	B
optimal	B
input	I
distribution	B
optimal	B
linear	B
optimal	B
stopping	I
optimization	B
gradient	B
descent	I
newton	B
algorithm	I
of	B
model	B
complexity	B
order	B
parameter	I
ordered	B
overrelaxation	B
orthodox	O
statistics	O
see	O
sampling	B
theory	I
outer	B
code	I
overrelaxation	B
p-value	B
packet	B
paradox	B
allais	B
bus-stop	B
heat	B
capacity	B
simpson	O
s	O
waiting	B
for	I
a	I
six	B
paranormal	B
parasite	B
parent	B
parity	B
parity-check	B
bits	I
parity-check	B
code	I
parity-check	B
constraints	I
parity-check	B
matrix	B
generalized	B
parity-check	B
nodes	I
parse	B
parsons	B
code	I
parthenogenesis	B
partial	B
order	I
partial	B
partition	B
functions	B
particle	O
partition	B
partition	B
function	I
analogy	B
with	I
lake	B
partial	B
partitioned	B
inverse	I
path-counting	B
pattern	B
recognition	B
pentagonful	B
code	I
perfect	B
code	I
perfect	B
simulation	I
periodic	B
variable	I
permutation	B
petersen	B
graph	B
phase	B
transition	B
philosophy	B
phone	B
cellular	O
see	O
mobile	B
phone	B
phone	B
directory	B
phone	B
number	I
photon	B
counter	I
physics	B
pigeon-hole	B
principle	I
pitchfork	B
bifurcation	B
plaintext	B
plankton	B
point	B
estimate	I
index	O
point	B
spread	I
function	I
pointer	B
poisoned	B
glass	I
poisson	B
distribution	B
poisson	B
process	I
poissonville	B
polymer	B
poor	O
man	O
s	O
coding	B
theorem	I
porridge	B
portfolio	B
positive	O
positivity	B
posterior	B
probability	B
power	B
cost	I
power	B
law	I
practical	B
see	O
error-correcting	B
code	I
precision	B
precisions	B
add	I
prediction	B
predictive	B
distribution	B
code	O
prior	B
assigning	B
improper	B
subjectivity	B
prior	B
equivalence	B
priority	B
of	I
bits	I
in	I
a	I
message	I
prize	B
on	O
game	B
show	I
probabilistic	B
model	B
probabilistic	B
movie	B
probability	B
bayesian	B
contrasted	B
with	I
likelihood	B
density	B
probability	B
distributions	O
see	O
distribution	B
probability	B
of	I
block	B
error	B
probability	B
propagation	O
see	O
sumproduct	O
algorithm	O
product	B
code	I
of	O
random	B
graph	B
pronunciation	B
proper	B
proposal	B
density	B
propp	B
jim	O
g	O
prosecutor	O
s	O
fallacy	O
prospecting	B
protein	B
regulatory	B
synthesis	B
protocol	B
pseudoinverse	B
punch	B
puncturing	B
pupil	B
puzzle	O
see	O
game	O
cable	B
labelling	I
chessboard	B
of	O
dna	B
replication	B
hat	B
life	B
magic	B
trick	I
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
index	O
poisoned	B
glass	I
secretary	B
southeast	B
transatlantic	B
cable	I
weighing	O
balls	O
quantum	B
error-correction	B
queue	B
qwerty	B
see	O
repetition	B
code	I
race	B
radial	B
basis	I
function	I
radio	B
radix	B
raid	B
random	B
random	B
cluster	I
model	B
random	B
code	I
for	B
compression	B
random	B
number	I
generator	I
random	B
variable	I
random	B
walk	I
suppression	B
random-coding	B
exponent	I
random-walk	B
metropolis	B
method	I
rant	O
see	O
sermon	O
raptor	B
codes	I
rate	B
rate-distortion	B
theory	I
rateless	B
code	I
reading	B
aloud	I
receiver	B
operating	I
characteristic	I
recognition	B
record	B
breaking	I
rectangular	B
code	I
reducible	B
redundancy	B
in	B
channel	I
code	I
redundant	O
array	O
of	O
independent	O
disks	O
redundant	B
constraints	I
in	I
code	I
reedsolomon	O
code	O
regression	B
regret	B
regular	B
regularization	B
regularization	B
constant	I
reinforcement	B
learning	B
rejection	B
rejection	B
sampling	I
adaptive	B
relative	B
entropy	B
reliability	B
function	I
repeataccumulate	O
code	O
repetition	B
code	I
responsibility	B
retransmission	B
reverse	B
reversible	B
reversible	B
jump	I
richardson	B
thomas	O
j	O
rissanen	B
jorma	O
roc	B
rolling	B
die	B
roman	B
rule	B
of	I
thumb	I
runlength	B
runlength-limited	B
channel	I
saccades	B
saddle-point	B
approximation	B
sailor	B
sample	B
from	B
gaussian	B
sampler	B
density	B
sampling	B
distribution	B
sampling	B
theory	I
criticisms	B
sandwiching	B
method	I
satellite	B
communications	I
scaling	B
schottky	B
anomaly	I
scientists	B
secret	B
secretary	B
problem	I
security	B
seek	B
time	I
sejnowski	B
terry	O
j	O
self-delimiting	B
self-dual	B
self-orthogonal	B
self-punctuating	B
separation	B
sequence	B
sequential	B
decoding	B
sequential	B
probability	B
ratio	I
test	I
sermon	O
see	O
caution	O
classical	B
statistics	I
level	O
dimensions	B
gradient	B
descent	I
illegal	B
integral	B
importance	B
sampling	I
interleaving	B
map	B
method	I
maximum	B
entropy	B
maximum	B
likelihood	B
most	B
probable	I
is	I
atypical	I
p-value	B
sampling	B
theory	I
sphere-packing	B
stopping	B
rule	I
turbo	B
codes	I
unbiased	B
estimator	B
worst-case-ism	B
set	B
shannon	B
shannon	B
claude	O
see	O
noisy-channel	B
coding	B
theorem	I
source	B
coding	B
theorem	I
information	B
content	I
shattering	B
shifter	B
ensemble	B
shokrollahi	B
m	O
amin	O
shortening	B
siegel	B
paul	O
sigmoid	B
signal-to-noise	B
ratio	I
level	O
simplex	B
simpson	O
s	O
paradox	B
simpson	O
o	O
j	O
see	O
wife-beaters	O
simulated	B
annealing	B
see	O
annealing	B
six	B
waiting	O
for	O
skilling	B
john	O
sleep	B
slepianwolf	O
see	O
dependent	B
sources	I
slice	B
sampling	I
multi-dimensional	B
soft	B
k-means	B
clustering	B
softmax	B
softmin	O
software	B
xi	O
arithmetic	B
coding	I
bugs	B
dasher	B
free	O
xii	O
gaussian	B
processes	I
hash	B
function	I
vibes	B
solar	B
system	I
soldier	B
soliton	B
distribution	B
sound	B
source	O
code	O
see	O
compression	B
symbol	B
code	I
arithmetic	B
coding	I
lempelziv	O
algorithms	B
block	B
code	I
block-sorting	B
compression	B
burrowswheeler	O
transform	O
for	B
complex	B
sources	I
for	B
constrained	B
channel	I
for	B
integers	I
see	O
code	O
implicit	B
probabilities	I
optimal	B
lengths	I
code	O
software	B
stream	B
codes	I
supermarket	B
symbol	B
code	I
uniquely	B
decodeable	I
variable	B
symbol	I
durations	I
source	B
coding	B
theorem	I
southeast	B
puzzle	I
span	B
sparse-graph	B
code	I
density	B
evolution	B
species	B
spell	B
sphere	B
packing	I
sphere-packing	B
exponent	I
spielman	B
daniel	O
a	O
spin	B
system	I
spines	B
spline	B
copyright	O
cambridge	O
university	O
press	O
on-screen	O
viewing	O
permitted	O
printing	O
not	O
permitted	O
you	O
can	O
buy	O
this	O
book	O
for	O
pounds	O
or	O
see	O
httpwww	O
inference	B
phy	O
cam	O
ac	O
ukmackayitila	O
for	O
links	O
spread	B
spectrum	I
spring	B
spy	B
square	B
staircase	B
stalactite	B
standard	B
deviation	I
stars	B
state	B
diagram	I
statistic	B
statistical	B
physics	B
see	O
physics	B
statistical	B
test	I
steepest	B
descents	I
stereoscopic	B
vision	B
stirling	B
s	O
approximation	B
stochastic	B
stochastic	B
dynamics	O
see	O
hamiltonian	B
monte	B
carlo	I
stochastic	B
gradient	I
stop-when-it	O
s-done	O
stopping	B
rule	I
straws	B
drawing	O
stream	B
codes	I
student	B
student-t	B
distribution	B
subjective	B
probability	B
submarine	B
subscriber	B
subset	B
substring	B
statistics	O
sum	B
rule	I
sumproduct	O
algorithm	O
summary	B
summary	B
state	I
summation	B
convention	I
super-channel	B
supermarket	B
support	B
vector	I
surprise	B
value	I
survey	B
propagation	I
suspicious	B
coincidences	I
symbol	B
code	I
budget	B
codeword	B
disadvantages	B
optimal	B
self-delimiting	B
supermarket	B
symmetric	B
channel	I
symmetry	B
argument	I
synchronization	B
synchronization	B
errors	B
syndrome	B
syndrome	B
decoding	B
systematic	B
t-distribution	O
see	O
student-t	B
tail	B
tamper	B
detection	I
tank	B
david	O
w	O
tanner	B
challenge	I
tanner	B
product	B
code	I
tanner	B
michael	O
tanzanite	B
tap	B
telephone	O
see	O
phone	B
telescope	B
temperature	B
termination	B
terminology	O
see	O
notation	B
monte	B
carlo	I
methods	I
test	O
statistical	B
text	B
entry	I
thermal	B
distribution	B
thermodynamic	B
integration	I
thermodynamics	B
third	B
law	I
thiele	B
t	O
n	O
thin	B
shell	I
third	B
law	I
of	I
thermodynamics	B
thitimajshima	B
p	O
three	B
cards	I
three	B
doors	B
threshold	B
tiling	B
time-division	B
timing	B
training	B
data	I
transatlantic	B
transfer	B
matrix	B
method	I
transition	B
transition	B
probability	B
translation-invariant	B
travelling	B
salesman	I
problem	I
tree	B
trellis	B
section	B
termination	B
triangle	B
truth	B
function	I
tube	B
turbo	B
code	I
turbo	B
product	B
code	I
turing	B
alan	O
twenty	B
questions	I
twin	B
twos	B
typical	B
evidence	B
typical	B
set	B
for	B
compression	B
for	B
noisy	B
channel	I
typical-set	B
decoder	B
typicality	B
umbrella	B
sampling	I
unbiased	B
estimator	B
uncompression	B
union	B
union	B
bound	B
uniquely	B
decodeable	I
units	B
universal	B
universality	B
in	O
physics	B
urbanke	B
urn	B
user	B
interfaces	I
utility	B
index	O
vaccination	B
vapnikchervonenkis	O
dimension	O
variable-length	B
code	I
variable-rate	O
error-correcting	B
codes	I
variance	B
variancecovariance	O
matrix	B
variances	B
add	I
variational	B
bayes	B
variational	B
free	I
energy	B
variational	B
methods	I
typical	B
properties	I
variational	B
gaussian	B
process	I
vc	B
dimension	I
vector	B
quantization	I
very	B
good	B
see	O
error-correcting	B
code	I
vibes	B
virtakallio	B
juhani	O
vision	B
visualization	B
viterbi	B
algorithm	I
volume	B
von	B
mises	I
distribution	B
wainwright	B
martin	O
waiting	B
for	I
a	I
bus	I
warning	O
see	O
caution	O
and	O
sermon	O
watsoncrick	O
base	O
pairing	O
weather	B
collator	I
weighing	B
babies	I
weighing	B
problem	I
weight	O
importance	B
sampling	I
in	B
neural	I
net	I
of	B
binary	I
vector	I
weight	B
decay	I
weight	B
enumerator	I
typical	B
weight	B
space	I
wenglish	B
what	O
number	O
comes	O
next	O
white	B
white	B
noise	I
wiberg	B
niclas	O
widget	B
wiener	B
process	I
wiener	B
norbert	O
wife-beater	B
wilson	B
david	O
b	O
window	B
winfree	B
erik	O
wodge	B
wolf	B
jack	O
word-english	B
world	B
record	I
worst-case-ism	B
writing	B
yedidia	B
jonathan	O
z	B
channel	I
zipf	B
plot	I
zipf	B
s	O
law	O
zipf	B
george	O
k	O
