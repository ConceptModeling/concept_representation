springer	B
texts	I
in	I
statistics	I
series	O
editors	O
g	O
casella	O
s	O
fienberg	O
i	O
olkin	O
for	O
further	O
volumes	O
gareth	O
james	O
daniela	O
witten	O
trevor	O
hastie	O
robert	O
tibshirani	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
gareth	O
james	O
department	O
of	O
data	B
sciences	O
and	O
operations	O
university	O
of	O
southern	O
california	O
los	O
angeles	O
ca	O
usa	O
trevor	O
hastie	O
department	O
of	O
statistics	O
stanford	O
university	O
stanford	O
ca	O
usa	O
daniela	O
witten	O
department	O
of	O
biostatistics	O
university	O
of	O
washington	O
seattle	O
wa	O
usa	O
robert	O
tibshirani	O
department	O
of	O
statistics	O
stanford	O
university	O
stanford	O
ca	O
usa	O
issn	O
isbn	O
doi	O
springer	O
new	O
york	O
heidelberg	O
dordrecht	O
london	O
isbn	O
library	O
of	O
congress	O
control	O
number	O
th	O
printing	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
at	O
this	O
work	O
is	O
subject	O
to	O
copyright	O
all	O
rights	O
are	O
reserved	O
by	O
the	O
publisher	O
whether	O
the	O
whole	O
or	O
part	O
of	O
the	O
material	O
is	O
concerned	O
specifically	O
the	O
rights	O
of	O
translation	O
reprinting	O
reuse	O
of	O
illustrations	O
recitation	O
broadcasting	O
reproduction	O
on	O
microfilms	O
or	O
in	O
any	O
other	O
physical	O
way	O
and	O
transmission	O
or	O
information	O
storage	O
and	O
retrieval	O
electronic	O
adaptation	O
computer	O
software	O
or	O
by	O
similar	O
or	O
dissimilar	O
methodology	O
now	O
known	O
or	O
hereafter	O
developed	O
exempted	O
from	O
this	O
legal	O
reservation	O
are	O
brief	O
excerpts	O
in	O
connection	O
with	O
reviews	O
or	O
scholarly	O
analysis	B
or	O
material	O
supplied	O
specifically	O
for	O
the	O
purpose	O
of	O
being	O
entered	O
and	O
executed	O
on	O
a	O
computer	O
system	O
for	O
exclusive	O
use	O
by	O
the	O
purchaser	O
of	O
the	O
work	O
duplication	O
of	O
this	O
publication	O
or	O
parts	O
thereof	O
is	O
permitted	O
only	O
under	O
the	O
provisions	O
of	O
the	O
copyright	O
law	O
of	O
the	O
publisher	O
s	O
location	O
in	O
its	O
current	O
version	O
and	O
permission	O
for	O
use	O
must	O
always	O
be	O
obtained	O
from	O
springer	O
permissions	O
for	O
use	O
may	O
be	O
obtained	O
through	O
rightslink	O
at	O
the	O
copyright	O
clearance	O
center	O
violations	O
are	O
liable	O
to	O
prosecution	O
under	O
the	O
respective	O
copyright	O
law	O
the	O
use	O
of	O
general	O
descriptive	O
names	O
registered	O
names	O
trademarks	O
service	O
marks	O
etc	O
in	O
this	O
publication	O
does	O
not	O
imply	O
even	O
in	O
the	O
absence	O
of	O
a	O
specific	O
statement	O
that	O
such	O
names	O
are	O
exempt	O
from	O
the	O
relevant	O
protective	O
laws	O
and	O
regulations	O
and	O
therefore	O
free	O
for	O
general	O
use	O
while	O
the	O
advice	O
and	O
information	O
in	O
this	O
book	O
are	O
believed	O
to	O
be	O
true	O
and	O
accurate	O
at	O
the	O
date	O
of	O
publication	O
neither	O
the	O
authors	O
nor	O
the	O
editors	O
nor	O
the	O
publisher	O
can	O
accept	O
any	O
legal	O
responsibility	O
for	O
any	O
errors	O
or	O
omissions	O
that	O
may	O
be	O
made	O
the	O
publisher	O
makes	O
no	O
warranty	O
express	O
or	O
implied	O
with	O
respect	O
to	O
the	O
material	O
contained	O
herein	O
printed	O
on	O
acid-free	O
paper	O
springer	O
is	O
part	O
of	O
springer	O
sciencebusiness	O
media	O
to	O
our	O
parents	O
alison	O
and	O
michael	O
james	O
chiara	O
nappi	O
and	O
edward	O
witten	O
valerie	O
and	O
patrick	O
hastie	O
vera	O
and	O
sami	O
tibshirani	O
and	O
to	O
our	O
families	O
michael	O
daniel	O
and	O
catherine	O
tessa	O
theo	O
and	O
ari	O
samantha	O
timothy	O
and	O
lynda	O
charlie	O
ryan	O
julie	O
and	O
cheryl	O
preface	O
statistical	O
learning	O
refers	O
to	O
a	O
set	B
of	O
tools	O
for	O
modeling	O
and	O
understanding	O
complex	O
datasets	O
it	O
is	O
a	O
recently	O
developed	O
area	O
in	O
statistics	O
and	O
blends	O
with	O
parallel	O
developments	O
in	O
computer	O
science	O
and	O
in	O
particular	O
machine	B
learning	O
the	O
field	O
encompasses	O
many	O
methods	O
such	O
as	O
the	O
lasso	B
and	O
sparse	B
regression	B
classification	B
and	O
regression	B
trees	O
and	O
boosting	B
and	O
support	B
vector	B
machines	O
with	O
the	O
explosion	O
of	O
big	O
data	B
problems	O
statistical	O
learning	O
has	O
become	O
a	O
very	O
hot	O
field	O
in	O
many	O
scientific	O
areas	O
as	O
well	O
as	O
marketing	O
finance	O
and	O
other	O
business	O
disciplines	O
people	O
with	O
statistical	O
learning	O
skills	O
are	O
in	O
high	O
demand	O
one	O
of	O
the	O
first	O
books	O
in	O
this	O
area	O
the	O
elements	O
of	O
statistical	O
learning	O
tibshirani	O
and	O
friedman	O
was	O
published	O
in	O
with	O
a	O
second	O
edition	O
in	O
esl	O
has	O
become	O
a	O
popular	O
text	O
not	O
only	O
in	O
statistics	O
but	O
also	O
in	O
related	O
fields	O
one	O
of	O
the	O
reasons	O
for	O
esl	O
s	O
popularity	O
is	O
its	O
relatively	O
accessible	O
style	O
but	O
esl	O
is	O
intended	O
for	O
individuals	O
with	O
advanced	O
training	O
in	O
the	O
mathematical	O
sciences	O
an	O
introduction	O
to	O
statistical	O
learning	O
arose	O
from	O
the	O
perceived	O
need	O
for	O
a	O
broader	O
and	O
less	O
technical	O
treatment	O
of	O
these	O
topics	O
in	O
this	O
new	O
book	O
we	O
cover	O
many	O
of	O
the	O
same	O
topics	O
as	O
esl	O
but	O
we	O
concentrate	O
more	O
on	O
the	O
applications	O
of	O
the	O
methods	O
and	O
less	O
on	O
the	O
mathematical	O
details	O
we	O
have	O
created	O
labs	O
illustrating	O
how	O
to	O
implement	O
each	O
of	O
the	O
statistical	O
learning	O
methods	O
using	O
the	O
popular	O
statistical	O
software	O
package	O
r	O
these	O
labs	O
provide	O
the	O
reader	O
with	O
valuable	O
hands-on	O
experience	O
this	O
book	O
is	O
appropriate	O
for	O
advanced	O
undergraduates	O
or	O
master	O
s	O
students	O
in	O
statistics	O
or	O
related	O
quantitative	B
fields	O
or	O
for	O
individuals	O
in	O
other	O
vii	O
viii	O
preface	O
disciplines	O
who	O
wish	O
to	O
use	O
statistical	O
learning	O
tools	O
to	O
analyze	O
their	O
data	B
it	O
can	O
be	O
used	O
as	O
a	O
textbook	O
for	O
a	O
course	O
spanning	O
one	O
or	O
two	O
semesters	O
we	O
would	O
like	O
to	O
thank	O
several	O
readers	O
for	O
valuable	O
comments	O
on	O
preliminary	O
drafts	O
of	O
this	O
book	O
pallavi	O
basu	O
alexandra	O
chouldechova	O
patrick	O
danaher	O
will	O
fithian	O
luella	O
fu	O
sam	O
gross	O
max	O
grazier	O
g	O
sell	O
courtney	O
paulson	O
xinghao	O
qiao	O
elisa	O
sheng	O
noah	O
simon	O
kean	O
ming	O
tan	O
and	O
xin	O
lu	O
tan	O
it	O
s	O
tough	O
to	O
make	O
predictions	O
especially	O
about	O
the	O
future	O
los	O
angeles	O
usa	O
seattle	O
usa	O
palo	O
alto	O
usa	O
palo	O
alto	O
usa	O
berra	O
gareth	O
james	O
daniela	O
witten	O
trevor	O
hastie	O
robert	O
tibshirani	O
contents	O
preface	O
introduction	O
statistical	O
learning	O
what	O
is	O
statistical	O
learning	O
why	O
estimate	O
f	O
how	O
do	O
we	O
estimate	O
f	O
the	O
trade-off	B
between	O
prediction	B
accuracy	O
and	O
model	B
interpretability	B
supervised	O
versus	O
unsupervised	B
learning	I
regression	B
versus	O
classification	B
problems	O
assessing	O
model	B
accuracy	O
measuring	O
the	O
quality	O
of	O
fit	O
the	O
bias-variance	O
trade-off	B
the	O
classification	B
setting	O
lab	O
introduction	O
to	O
r	O
basic	O
commands	O
graphics	O
indexing	O
data	B
loading	O
data	B
additional	O
graphical	O
and	O
numerical	O
summaries	O
exercises	O
vii	O
ix	O
x	O
contents	O
linear	B
regression	B
simple	B
linear	B
regression	B
estimating	O
the	O
coefficients	O
assessing	O
the	O
accuracy	O
of	O
the	O
coefficient	O
some	O
important	O
questions	O
estimates	O
assessing	O
the	O
accuracy	O
of	O
the	O
model	B
multiple	B
linear	B
regression	B
estimating	O
the	O
regression	B
coefficients	O
other	O
considerations	O
in	O
the	O
regression	B
model	B
qualitative	B
predictors	O
extensions	O
of	O
the	O
linear	B
model	B
potential	O
problems	O
the	O
marketing	O
plan	O
comparison	O
of	O
linear	B
regression	B
with	O
k-nearest	O
neighbors	O
lab	O
linear	B
regression	B
libraries	O
simple	B
linear	B
regression	B
multiple	B
linear	B
regression	B
non-linear	B
transformations	O
of	O
the	O
predictors	O
qualitative	B
predictors	O
writing	O
functions	O
interaction	B
terms	O
exercises	O
classification	B
an	O
overview	O
of	O
classification	B
why	O
not	O
linear	B
regression	B
logistic	B
regression	B
the	O
logistic	O
model	B
estimating	O
the	O
regression	B
coefficients	O
making	O
predictions	O
multiple	B
logistic	B
regression	B
logistic	B
regression	B
for	O
response	B
classes	O
linear	B
discriminant	I
analysis	B
using	O
bayes	O
theorem	O
for	O
classification	B
linear	B
discriminant	I
analysis	B
for	O
p	O
linear	B
discriminant	I
analysis	B
for	O
p	O
quadratic	B
discriminant	O
analysis	B
a	O
comparison	O
of	O
classification	B
methods	O
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
the	O
stock	O
market	O
data	B
logistic	B
regression	B
linear	B
discriminant	I
analysis	B
contents	O
xi	O
quadratic	B
discriminant	O
analysis	B
k-nearest	O
neighbors	O
an	O
application	O
to	O
caravan	B
insurance	O
data	B
exercises	O
resampling	B
methods	O
cross-validation	B
the	O
validation	B
set	B
approach	B
leave-one-out	B
cross-validation	B
k-fold	B
cross-validation	B
bias-variance	O
trade-off	B
for	O
k-fold	B
cross-validation	B
cross-validation	B
on	O
classification	B
problems	O
the	O
bootstrap	B
lab	O
cross-validation	B
and	O
the	O
bootstrap	B
the	O
validation	B
set	B
approach	B
leave-one-out	B
cross-validation	B
k-fold	B
cross-validation	B
the	O
bootstrap	B
exercises	O
linear	B
model	B
selection	B
and	O
regularization	B
dimension	B
reduction	I
methods	O
subset	B
selection	B
best	B
subset	B
selection	B
stepwise	O
selection	B
choosing	O
the	O
optimal	O
model	B
shrinkage	B
methods	O
ridge	B
regression	B
the	O
lasso	B
selecting	O
the	O
tuning	B
parameter	B
principal	B
components	I
regression	B
partial	B
least	B
squares	I
considerations	O
in	O
high	O
dimensions	O
high-dimensional	B
data	B
what	O
goes	O
wrong	O
in	O
high	O
dimensions	O
regression	B
in	O
high	O
dimensions	O
interpreting	O
results	O
in	O
high	O
dimensions	O
lab	O
subset	B
selection	B
methods	O
best	B
subset	B
selection	B
forward	O
and	O
backward	B
stepwise	I
selection	B
choosing	O
among	O
models	O
using	O
the	O
validation	B
set	B
approach	B
and	O
cross-validation	B
xii	O
contents	O
lab	O
ridge	B
regression	B
and	O
the	O
lasso	B
ridge	B
regression	B
the	O
lasso	B
lab	O
pcr	O
and	O
pls	O
regression	B
principal	B
components	I
regression	B
partial	B
least	B
squares	I
exercises	O
moving	O
beyond	O
linearity	O
polynomial	B
regression	B
step	O
functions	O
basis	B
functions	O
regression	B
splines	O
piecewise	O
polynomials	O
constraints	O
and	O
splines	O
the	O
spline	B
basis	B
representation	O
choosing	O
the	O
number	O
and	O
locations	O
of	O
the	O
knots	O
smoothing	B
splines	O
comparison	O
to	O
polynomial	B
regression	B
an	O
overview	O
of	O
smoothing	B
splines	O
choosing	O
the	O
smoothing	B
parameter	B
local	B
regression	B
generalized	O
additive	B
models	O
gams	O
for	O
regression	B
problems	O
gams	O
for	O
classification	B
problems	O
lab	O
non-linear	B
modeling	O
polynomial	B
regression	B
and	O
step	O
functions	O
splines	O
gams	O
exercises	O
tree-based	O
methods	O
the	O
basics	O
of	O
decision	O
trees	O
regression	B
trees	O
classification	B
trees	O
trees	O
versus	O
linear	B
models	O
advantages	O
and	O
disadvantages	O
of	O
trees	O
bagging	B
random	O
forests	O
boosting	B
bagging	B
random	O
forests	O
boosting	B
lab	O
decision	O
trees	O
fitting	O
classification	B
trees	O
fitting	O
regression	B
trees	O
bagging	B
and	O
random	O
forests	O
boosting	B
exercises	O
contents	O
xiii	O
support	B
vector	B
machines	O
maximal	O
margin	B
classifier	B
what	O
is	O
a	O
hyperplane	B
classification	B
using	O
a	O
separating	B
hyperplane	B
the	O
maximal	O
margin	B
classifier	B
construction	O
of	O
the	O
maximal	O
margin	B
classifier	B
the	O
non-separable	O
case	O
support	B
vector	B
classifiers	O
overview	O
of	O
the	O
support	B
vector	B
classifier	B
details	O
of	O
the	O
support	B
vector	B
classifier	B
support	B
vector	B
machines	O
classification	B
with	O
non-linear	B
decision	O
boundaries	O
the	O
support	B
vector	B
machine	B
an	O
application	O
to	O
the	O
heart	B
disease	O
data	B
svms	O
with	O
more	O
than	O
two	O
classes	O
one-versus-one	B
classification	B
one-versus-all	B
classification	B
relationship	O
to	O
logistic	B
regression	B
lab	O
support	B
vector	B
machines	O
support	B
vector	B
classifier	B
support	B
vector	B
machine	B
roc	O
curves	O
svm	O
with	O
multiple	B
classes	O
application	O
to	O
gene	O
expression	O
data	B
exercises	O
unsupervised	B
learning	I
the	O
challenge	O
of	O
unsupervised	B
learning	I
principal	B
components	I
analysis	B
what	O
are	O
principal	B
components	I
another	O
interpretation	O
of	O
principal	B
components	I
more	O
on	O
pca	O
other	O
uses	O
for	O
principal	B
components	I
clustering	B
methods	O
k-means	B
clustering	B
hierarchical	B
clustering	B
practical	O
issues	O
in	O
clustering	B
lab	O
principal	B
components	I
analysis	B
xiv	O
contents	O
lab	O
clustering	B
k-means	B
clustering	B
hierarchical	B
clustering	B
pca	O
on	O
the	O
data	B
clustering	B
the	O
observations	B
of	O
the	O
data	B
lab	O
data	B
example	O
exercises	O
index	O
introduction	O
an	O
overview	O
of	O
statistical	O
learning	O
statistical	O
learning	O
refers	O
to	O
a	O
vast	O
set	B
of	O
tools	O
for	O
understanding	O
data	B
these	O
tools	O
can	O
be	O
classified	O
as	O
supervised	O
or	O
unsupervised	O
broadly	O
speaking	O
supervised	O
statistical	O
learning	O
involves	O
building	O
a	O
statistical	B
model	B
for	O
predicting	O
or	O
estimating	O
an	O
output	B
based	O
on	O
one	O
or	O
more	O
inputs	O
problems	O
of	O
this	O
nature	O
occur	O
in	O
fields	O
as	O
diverse	O
as	O
business	O
medicine	O
astrophysics	O
and	O
public	O
policy	O
with	O
unsupervised	O
statistical	O
learning	O
there	O
are	O
inputs	O
but	O
no	O
supervising	O
output	B
nevertheless	O
we	O
can	O
learn	O
relationships	O
and	O
structure	O
from	O
such	O
data	B
to	O
provide	O
an	O
illustration	O
of	O
some	O
applications	O
of	O
statistical	O
learning	O
we	O
briefly	O
discuss	O
three	O
real-world	O
data	B
sets	O
that	O
are	O
considered	O
in	O
this	O
book	O
wage	B
data	B
in	O
this	O
application	O
we	O
refer	O
to	O
as	O
the	O
wage	B
data	B
set	B
throughout	O
this	O
book	O
we	O
examine	O
a	O
number	O
of	O
factors	O
that	O
relate	O
to	O
wages	O
for	O
a	O
group	O
of	O
males	O
from	O
the	O
atlantic	O
region	O
of	O
the	O
united	O
states	O
in	O
particular	O
we	O
wish	O
to	O
understand	O
the	O
association	O
between	O
an	O
employee	O
s	O
age	O
and	O
education	O
as	O
well	O
as	O
the	O
calendar	O
year	O
on	O
his	O
wage	B
consider	O
for	O
example	O
the	O
left-hand	O
panel	O
of	O
figure	O
which	O
displays	O
wage	B
versus	O
age	O
for	O
each	O
of	O
the	O
individuals	O
in	O
the	O
data	B
set	B
there	O
is	O
evidence	O
that	O
wage	B
increases	O
with	O
age	O
but	O
then	O
decreases	O
again	O
after	O
approximately	O
age	O
the	O
blue	O
line	B
which	O
provides	O
an	O
estimate	O
of	O
the	O
average	B
wage	B
for	O
a	O
given	O
age	O
makes	O
this	O
trend	O
clearer	O
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
introduction	O
e	O
g	O
a	O
w	O
e	O
g	O
a	O
w	O
e	O
g	O
a	O
w	O
age	O
year	O
education	O
level	B
figure	O
wage	B
data	B
which	O
contains	O
income	B
survey	O
information	O
for	O
males	O
from	O
the	O
central	O
atlantic	O
region	O
of	O
the	O
united	O
states	O
left	O
wage	B
as	O
a	O
function	B
of	O
age	O
on	O
average	B
wage	B
increases	O
with	O
age	O
until	O
about	O
years	O
of	O
age	O
at	O
which	O
point	O
it	O
begins	O
to	O
decline	O
center	O
wage	B
as	O
a	O
function	B
of	O
year	O
there	O
is	O
a	O
slow	O
but	O
steady	O
increase	O
of	O
approximately	O
in	O
the	O
average	B
wage	B
between	O
and	O
right	O
boxplots	O
displaying	O
wage	B
as	O
a	O
function	B
of	O
education	O
with	O
indicating	O
the	O
lowest	O
level	B
high	O
school	O
diploma	O
and	O
the	O
highest	O
level	B
advanced	O
graduate	O
degree	O
on	O
average	B
wage	B
increases	O
with	O
the	O
level	B
of	O
education	O
given	O
an	O
employee	O
s	O
age	O
we	O
can	O
use	O
this	O
curve	O
to	O
predict	O
his	O
wage	B
however	O
it	O
is	O
also	O
clear	O
from	O
figure	O
that	O
there	O
is	O
a	O
significant	O
amount	O
of	O
variability	O
associated	O
with	O
this	O
average	B
value	O
and	O
so	O
age	O
alone	O
is	O
unlikely	O
to	O
provide	O
an	O
accurate	O
prediction	B
of	O
a	O
particular	O
man	O
s	O
wage	B
we	O
also	O
have	O
information	O
regarding	O
each	O
employee	O
s	O
education	O
level	B
and	O
the	O
year	O
in	O
which	O
the	O
wage	B
was	O
earned	O
the	O
center	O
and	O
right-hand	O
panels	O
of	O
figure	O
which	O
display	O
wage	B
as	O
a	O
function	B
of	O
both	O
year	O
and	O
education	O
indicate	O
that	O
both	O
of	O
these	O
factors	O
are	O
associated	O
with	O
wage	B
wages	O
increase	O
by	O
approximately	O
in	O
a	O
roughly	O
linear	B
straight-line	O
fashion	O
between	O
and	O
though	O
this	O
rise	O
is	O
very	O
slight	O
relative	O
to	O
the	O
variability	O
in	O
the	O
data	B
wages	O
are	O
also	O
typically	O
greater	O
for	O
individuals	O
with	O
higher	O
education	O
levels	O
men	O
with	O
the	O
lowest	O
education	O
level	B
tend	O
to	O
have	O
substantially	O
lower	O
wages	O
than	O
those	O
with	O
the	O
highest	O
education	O
level	B
clearly	O
the	O
most	O
accurate	O
prediction	B
of	O
a	O
given	O
man	O
s	O
wage	B
will	O
be	O
obtained	O
by	O
combining	O
his	O
age	O
his	O
education	O
and	O
the	O
year	O
in	O
chapter	O
we	O
discuss	O
linear	B
regression	B
which	O
can	O
be	O
used	O
to	O
predict	O
wage	B
from	O
this	O
data	B
set	B
ideally	O
we	O
should	O
predict	O
wage	B
in	O
a	O
way	O
that	O
accounts	O
for	O
the	O
non-linear	B
relationship	O
between	O
wage	B
and	O
age	O
in	O
chapter	O
we	O
discuss	O
a	O
class	O
of	O
approaches	O
for	O
addressing	O
this	O
problem	O
stock	O
market	O
data	B
the	O
wage	B
data	B
involves	O
predicting	O
a	O
continuous	B
or	O
quantitative	B
output	B
value	O
this	O
is	O
often	O
referred	O
to	O
as	O
a	O
regression	B
problem	O
however	O
in	O
certain	O
cases	O
we	O
may	O
instead	O
wish	O
to	O
predict	O
a	O
non-numerical	O
value	O
that	O
is	O
a	O
categorical	B
yesterday	O
two	O
days	O
previous	O
three	O
days	O
previous	O
introduction	O
p	O
s	O
n	O
i	O
e	O
g	O
n	O
a	O
h	O
c	O
e	O
g	O
a	O
n	O
e	O
c	O
r	O
e	O
p	O
t	O
p	O
s	O
n	O
i	O
e	O
g	O
n	O
a	O
h	O
c	O
e	O
g	O
a	O
t	O
n	O
e	O
c	O
r	O
e	O
p	O
p	O
s	O
n	O
i	O
e	O
g	O
n	O
a	O
h	O
c	O
e	O
g	O
a	O
n	O
e	O
c	O
r	O
e	O
p	O
t	O
down	O
up	O
today	O
s	O
direction	O
down	O
up	O
today	O
s	O
direction	O
down	O
up	O
today	O
s	O
direction	O
figure	O
left	O
boxplots	O
of	O
the	O
previous	O
day	O
s	O
percentage	O
change	O
in	O
the	O
sp	O
index	O
for	O
the	O
days	O
for	O
which	O
the	O
market	O
increased	O
or	O
decreased	O
obtained	O
from	O
the	O
smarket	B
data	B
center	O
and	O
right	O
same	O
as	O
left	O
panel	O
but	O
the	O
percentage	O
changes	O
for	O
and	O
days	O
previous	O
are	O
shown	O
or	O
qualitative	B
output	B
for	O
example	O
in	O
chapter	O
we	O
examine	O
a	O
stock	O
market	O
data	B
set	B
that	O
contains	O
the	O
daily	O
movements	O
in	O
the	O
standard	O
poor	O
s	O
stock	O
index	O
over	O
a	O
period	O
between	O
and	O
we	O
refer	O
to	O
this	O
as	O
the	O
smarket	B
data	B
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
index	O
will	O
increase	O
or	O
decrease	O
on	O
a	O
given	O
day	O
using	O
the	O
past	O
days	O
percentage	O
changes	O
in	O
the	O
index	O
here	O
the	O
statistical	O
learning	O
problem	O
does	O
not	O
involve	O
predicting	O
a	O
numerical	O
value	O
instead	O
it	O
involves	O
predicting	O
whether	O
a	O
given	O
day	O
s	O
stock	O
market	O
performance	O
will	O
fall	O
into	O
the	O
up	O
bucket	O
or	O
the	O
down	O
bucket	O
this	O
is	O
known	O
as	O
a	O
classification	B
problem	O
a	O
model	B
that	O
could	O
accurately	O
predict	O
the	O
direction	O
in	O
which	O
the	O
market	O
will	O
move	O
would	O
be	O
very	O
useful	O
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
two	O
boxplots	O
of	O
the	O
previous	O
day	O
s	O
percentage	O
changes	O
in	O
the	O
stock	O
index	O
one	O
for	O
the	O
days	O
for	O
which	O
the	O
market	O
increased	O
on	O
the	O
subsequent	O
day	O
and	O
one	O
for	O
the	O
days	O
for	O
which	O
the	O
market	O
decreased	O
the	O
two	O
plots	O
look	O
almost	O
identical	O
suggesting	O
that	O
there	O
is	O
no	O
simple	B
strategy	O
for	O
using	O
yesterday	O
s	O
movement	O
in	O
the	O
sp	O
to	O
predict	O
today	O
s	O
returns	O
the	O
remaining	O
panels	O
which	O
display	O
boxplots	O
for	O
the	O
percentage	O
changes	O
and	O
days	O
previous	O
to	O
today	O
similarly	O
indicate	O
little	O
association	O
between	O
past	O
and	O
present	O
returns	O
of	O
course	O
this	O
lack	O
of	O
pattern	O
is	O
to	O
be	O
expected	O
in	O
the	O
presence	O
of	O
strong	O
correlations	O
between	O
successive	O
days	O
returns	O
one	O
could	O
adopt	O
a	O
simple	B
trading	O
strategy	O
to	O
generate	O
profits	O
from	O
the	O
market	O
nevertheless	O
in	O
chapter	O
we	O
explore	O
these	O
data	B
using	O
several	O
different	O
statistical	O
learning	O
methods	O
interestingly	O
there	O
are	O
hints	O
of	O
some	O
weak	O
trends	O
in	O
the	O
data	B
that	O
suggest	O
that	O
at	O
least	O
for	O
this	O
period	O
it	O
is	O
possible	O
to	O
correctly	O
predict	O
the	O
direction	O
of	O
movement	O
in	O
the	O
market	O
approximately	O
of	O
the	O
time	O
introduction	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
d	O
e	O
i	O
t	O
c	O
d	O
e	O
r	O
p	O
down	O
up	O
today	O
s	O
direction	O
figure	O
we	O
fit	O
a	O
quadratic	B
discriminant	O
analysis	B
model	B
to	O
the	O
subset	O
of	O
the	O
smarket	B
data	B
corresponding	O
to	O
the	O
time	O
period	O
and	O
predicted	O
the	O
probability	B
of	O
a	O
stock	O
market	O
decrease	O
using	O
the	O
data	B
on	O
average	B
the	O
predicted	O
probability	B
of	O
decrease	O
is	O
higher	O
for	O
the	O
days	O
in	O
which	O
the	O
market	O
does	O
decrease	O
based	O
on	O
these	O
results	O
we	O
are	O
able	O
to	O
correctly	O
predict	O
the	O
direction	O
of	O
movement	O
in	O
the	O
market	O
of	O
the	O
time	O
gene	O
expression	O
data	B
the	O
previous	O
two	O
applications	O
illustrate	O
data	B
sets	O
with	O
both	O
input	B
and	O
output	B
variables	O
however	O
another	O
important	O
class	O
of	O
problems	O
involves	O
situations	O
in	O
which	O
we	O
only	O
observe	O
input	B
variables	O
with	O
no	O
corresponding	O
output	B
for	O
example	O
in	O
a	O
marketing	O
setting	O
we	O
might	O
have	O
demographic	O
information	O
for	O
a	O
number	O
of	O
current	O
or	O
potential	O
customers	O
we	O
may	O
wish	O
to	O
understand	O
which	O
types	O
of	O
customers	O
are	O
similar	O
to	O
each	O
other	O
by	O
grouping	O
individuals	O
according	O
to	O
their	O
observed	O
characteristics	O
this	O
is	O
known	O
as	O
a	O
clustering	B
problem	O
unlike	O
in	O
the	O
previous	O
examples	O
here	O
we	O
are	O
not	O
trying	O
to	O
predict	O
an	O
output	B
variable	B
we	O
devote	O
chapter	O
to	O
a	O
discussion	O
of	O
statistical	O
learning	O
methods	O
for	O
problems	O
in	O
which	O
no	O
natural	B
output	B
variable	B
is	O
available	O
we	O
consider	O
the	O
data	B
set	B
which	O
consists	O
of	O
gene	O
expression	O
measurements	O
for	O
each	O
of	O
cancer	O
cell	O
lines	O
instead	O
of	O
predicting	O
a	O
particular	O
output	B
variable	B
we	O
are	O
interested	O
in	O
determining	O
whether	O
there	O
are	O
groups	O
or	O
clusters	O
among	O
the	O
cell	O
lines	O
based	O
on	O
their	O
gene	O
expression	O
measurements	O
this	O
is	O
a	O
difficult	O
question	O
to	O
address	O
in	O
part	O
because	O
there	O
are	O
thousands	O
of	O
gene	O
expression	O
measurements	O
per	O
cell	O
line	B
making	O
it	O
hard	O
to	O
visualize	O
the	O
data	B
the	O
left-hand	O
panel	O
of	O
figure	O
addresses	O
this	O
problem	O
by	O
representing	O
each	O
of	O
the	O
cell	O
lines	O
using	O
just	O
two	O
numbers	O
and	O
these	O
are	O
the	O
first	O
two	O
principal	B
components	I
of	O
the	O
data	B
which	O
summarize	O
the	O
expression	O
measurements	O
for	O
each	O
cell	O
line	B
down	O
to	O
two	O
numbers	O
or	O
dimensions	O
while	O
it	O
is	O
likely	O
that	O
this	O
dimension	B
reduction	I
has	O
resulted	O
in	O
introduction	O
z	O
z	O
figure	O
left	O
representation	O
of	O
the	O
gene	O
expression	O
data	B
set	B
in	O
a	O
two-dimensional	O
space	O
and	O
each	O
point	O
corresponds	O
to	O
one	O
of	O
the	O
cell	O
lines	O
there	O
appear	O
to	O
be	O
four	O
groups	O
of	O
cell	O
lines	O
which	O
we	O
have	O
represented	O
using	O
different	O
colors	O
right	O
same	O
as	O
left	O
panel	O
except	O
that	O
we	O
have	O
represented	O
each	O
of	O
the	O
different	O
types	O
of	O
cancer	O
using	O
a	O
different	O
colored	O
symbol	O
cell	O
lines	O
corresponding	O
to	O
the	O
same	O
cancer	O
type	O
tend	O
to	O
be	O
nearby	O
in	O
the	O
two-dimensional	O
space	O
some	O
loss	O
of	O
information	O
it	O
is	O
now	O
possible	O
to	O
visually	O
examine	O
the	O
data	B
for	O
evidence	O
of	O
clustering	B
deciding	O
on	O
the	O
number	O
of	O
clusters	O
is	O
often	O
a	O
difficult	O
problem	O
but	O
the	O
left-hand	O
panel	O
of	O
figure	O
suggests	O
at	O
least	O
four	O
groups	O
of	O
cell	O
lines	O
which	O
we	O
have	O
represented	O
using	O
separate	O
colors	O
we	O
can	O
now	O
examine	O
the	O
cell	O
lines	O
within	O
each	O
cluster	O
for	O
similarities	O
in	O
their	O
types	O
of	O
cancer	O
in	O
order	O
to	O
better	O
understand	O
the	O
relationship	O
between	O
gene	O
expression	O
levels	O
and	O
cancer	O
in	O
this	O
particular	O
data	B
set	B
it	O
turns	O
out	O
that	O
the	O
cell	O
lines	O
correspond	O
to	O
different	O
types	O
of	O
cancer	O
this	O
information	O
was	O
not	O
used	O
to	O
create	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
right-hand	O
panel	O
of	O
figure	O
is	O
identical	O
to	O
the	O
left-hand	O
panel	O
except	O
that	O
the	O
cancer	O
types	O
are	O
shown	O
using	O
distinct	O
colored	O
symbols	O
there	O
is	O
clear	O
evidence	O
that	O
cell	O
lines	O
with	O
the	O
same	O
cancer	O
type	O
tend	O
to	O
be	O
located	O
near	O
each	O
other	O
in	O
this	O
two-dimensional	O
representation	O
in	O
addition	O
even	O
though	O
the	O
cancer	O
information	O
was	O
not	O
used	O
to	O
produce	O
the	O
left-hand	O
panel	O
the	O
clustering	B
obtained	O
does	O
bear	O
some	O
resemblance	O
to	O
some	O
of	O
the	O
actual	O
cancer	O
types	O
observed	O
in	O
the	O
right-hand	O
panel	O
this	O
provides	O
some	O
independent	B
verification	O
of	O
the	O
accuracy	O
of	O
our	O
clustering	B
analysis	B
a	O
brief	O
history	O
of	O
statistical	O
learning	O
though	O
the	O
term	B
statistical	O
learning	O
is	O
fairly	O
new	O
many	O
of	O
the	O
concepts	O
that	O
underlie	O
the	O
field	O
were	O
developed	O
long	O
ago	O
at	O
the	O
beginning	O
of	O
the	O
nineteenth	O
century	O
legendre	O
and	O
gauss	O
published	O
papers	O
on	O
the	O
method	O
introduction	O
of	O
least	B
squares	I
which	O
implemented	O
the	O
earliest	O
form	O
of	O
what	O
is	O
now	O
known	O
as	O
linear	B
regression	B
the	O
approach	B
was	O
first	O
successfully	O
applied	O
to	O
problems	O
in	O
astronomy	O
linear	B
regression	B
is	O
used	O
for	O
predicting	O
quantitative	B
values	O
such	O
as	O
an	O
individual	O
s	O
salary	O
in	O
order	O
to	O
predict	O
qualitative	B
values	O
such	O
as	O
whether	O
a	O
patient	O
survives	O
or	O
dies	O
or	O
whether	O
the	O
stock	O
market	O
increases	O
or	O
decreases	O
fisher	O
proposed	O
linear	B
discriminant	I
analysis	B
in	O
in	O
the	O
various	O
authors	O
put	O
forth	O
an	O
alternative	O
approach	B
logistic	B
regression	B
in	O
the	O
early	O
nelder	O
and	O
wedderburn	O
coined	O
the	O
term	B
generalized	O
linear	B
models	O
for	O
an	O
entire	O
class	O
of	O
statistical	O
learning	O
methods	O
that	O
include	O
both	O
linear	B
and	O
logistic	B
regression	B
as	O
special	O
cases	O
by	O
the	O
end	O
of	O
the	O
many	O
more	O
techniques	O
for	O
learning	O
from	O
data	B
were	O
available	O
however	O
they	O
were	O
almost	O
exclusively	O
linear	B
methods	O
because	O
fitting	O
non-linear	B
relationships	O
was	O
computationally	O
infeasible	O
at	O
the	O
time	O
by	O
the	O
computing	O
technology	O
had	O
finally	O
improved	O
sufficiently	O
that	O
non-linear	B
methods	O
were	O
no	O
longer	O
computationally	O
prohibitive	O
in	O
mid	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
introduced	O
classification	B
and	O
regression	B
trees	O
and	O
were	O
among	O
the	O
first	O
to	O
demonstrate	O
the	O
power	B
of	O
a	O
detailed	O
practical	O
implementation	O
of	O
a	O
method	O
including	O
cross-validation	B
for	O
model	B
selection	B
hastie	O
and	O
tibshirani	O
coined	O
the	O
term	B
generalized	O
additive	B
models	O
in	O
for	O
a	O
class	O
of	O
non-linear	B
extensions	O
to	O
generalized	O
linear	B
models	O
and	O
also	O
provided	O
a	O
practical	O
software	O
implementation	O
since	O
that	O
time	O
inspired	O
by	O
the	O
advent	O
of	O
machine	B
learning	O
and	O
other	O
disciplines	O
statistical	O
learning	O
has	O
emerged	O
as	O
a	O
new	O
subfield	O
in	O
statistics	O
focused	O
on	O
supervised	O
and	O
unsupervised	O
modeling	O
and	O
prediction	B
in	O
recent	O
years	O
progress	O
in	O
statistical	O
learning	O
has	O
been	O
marked	O
by	O
the	O
increasing	O
availability	O
of	O
powerful	O
and	O
relatively	O
user-friendly	O
software	O
such	O
as	O
the	O
popular	O
and	O
freely	O
available	O
r	O
system	O
this	O
has	O
the	O
potential	O
to	O
continue	O
the	O
transformation	O
of	O
the	O
field	O
from	O
a	O
set	B
of	O
techniques	O
used	O
and	O
developed	O
by	O
statisticians	O
and	O
computer	O
scientists	O
to	O
an	O
essential	O
toolkit	O
for	O
a	O
much	O
broader	O
community	O
this	O
book	O
the	O
elements	O
of	O
statistical	O
learning	O
by	O
hastie	O
tibshirani	O
and	O
friedman	O
was	O
first	O
published	O
in	O
since	O
that	O
time	O
it	O
has	O
become	O
an	O
important	O
reference	O
on	O
the	O
fundamentals	O
of	O
statistical	O
machine	B
learning	O
its	O
success	O
derives	O
from	O
its	O
comprehensive	O
and	O
detailed	O
treatment	O
of	O
many	O
important	O
topics	O
in	O
statistical	O
learning	O
as	O
well	O
as	O
the	O
fact	O
that	O
to	O
many	O
upper-level	O
statistics	O
textbooks	O
it	O
is	O
accessible	O
to	O
a	O
wide	O
audience	O
however	O
the	O
greatest	O
factor	B
behind	O
the	O
success	O
of	O
esl	O
has	O
been	O
its	O
topical	O
nature	O
at	O
the	O
time	O
of	O
its	O
publication	O
interest	O
in	O
the	O
field	O
of	O
statistical	O
introduction	O
learning	O
was	O
starting	O
to	O
explode	O
esl	O
provided	O
one	O
of	O
the	O
first	O
accessible	O
and	O
comprehensive	O
introductions	O
to	O
the	O
topic	O
since	O
esl	O
was	O
first	O
published	O
the	O
field	O
of	O
statistical	O
learning	O
has	O
continued	O
to	O
flourish	O
the	O
field	O
s	O
expansion	O
has	O
taken	O
two	O
forms	O
the	O
most	O
obvious	O
growth	O
has	O
involved	O
the	O
development	O
of	O
new	O
and	O
improved	O
statistical	O
learning	O
approaches	O
aimed	O
at	O
answering	O
a	O
range	O
of	O
scientific	O
questions	O
across	O
a	O
number	O
of	O
fields	O
however	O
the	O
field	O
of	O
statistical	O
learning	O
has	O
also	O
expanded	O
its	O
audience	O
in	O
the	O
increases	O
in	O
computational	O
power	B
generated	O
a	O
surge	O
of	O
interest	O
in	O
the	O
field	O
from	O
non-statisticians	O
who	O
were	O
eager	O
to	O
use	O
cutting-edge	O
statistical	O
tools	O
to	O
analyze	O
their	O
data	B
unfortunately	O
the	O
highly	O
technical	O
nature	O
of	O
these	O
approaches	O
meant	O
that	O
the	O
user	O
community	O
remained	O
primarily	O
restricted	O
to	O
experts	O
in	O
statistics	O
computer	O
science	O
and	O
related	O
fields	O
with	O
the	O
training	O
time	O
to	O
understand	O
and	O
implement	O
them	O
in	O
recent	O
years	O
new	O
and	O
improved	O
software	O
packages	O
have	O
significantly	O
eased	O
the	O
implementation	O
burden	O
for	O
many	O
statistical	O
learning	O
methods	O
at	O
the	O
same	O
time	O
there	O
has	O
been	O
growing	O
recognition	O
across	O
a	O
number	O
of	O
fields	O
from	O
business	O
to	O
health	O
care	O
to	O
genetics	O
to	O
the	O
social	O
sciences	O
and	O
beyond	O
that	O
statistical	O
learning	O
is	O
a	O
powerful	O
tool	O
with	O
important	O
practical	O
applications	O
as	O
a	O
result	O
the	O
field	O
has	O
moved	O
from	O
one	O
of	O
primarily	O
academic	O
interest	O
to	O
a	O
mainstream	O
discipline	O
with	O
an	O
enormous	O
potential	O
audience	O
this	O
trend	O
will	O
surely	O
continue	O
with	O
the	O
increasing	O
availability	O
of	O
enormous	O
quantities	O
of	O
data	B
and	O
the	O
software	O
to	O
analyze	O
it	O
the	O
purpose	O
of	O
an	O
introduction	O
to	O
statistical	O
learning	O
is	O
to	O
facilitate	O
the	O
transition	O
of	O
statistical	O
learning	O
from	O
an	O
academic	O
to	O
a	O
mainstream	O
field	O
isl	O
is	O
not	O
intended	O
to	O
replace	O
esl	O
which	O
is	O
a	O
far	O
more	O
comprehensive	O
text	O
both	O
in	O
terms	O
of	O
the	O
number	O
of	O
approaches	O
considered	O
and	O
the	O
depth	O
to	O
which	O
they	O
are	O
explored	O
we	O
consider	O
esl	O
to	O
be	O
an	O
important	O
companion	O
for	O
professionals	O
graduate	O
degrees	O
in	O
statistics	O
machine	B
learning	O
or	O
related	O
fields	O
who	O
need	O
to	O
understand	O
the	O
technical	O
details	O
behind	O
statistical	O
learning	O
approaches	O
however	O
the	O
community	O
of	O
users	O
of	O
statistical	O
learning	O
techniques	O
has	O
expanded	O
to	O
include	O
individuals	O
with	O
a	O
wider	O
range	O
of	O
interests	O
and	O
backgrounds	O
therefore	O
we	O
believe	O
that	O
there	O
is	O
now	O
a	O
place	O
for	O
a	O
less	O
technical	O
and	O
more	O
accessible	O
version	O
of	O
esl	O
in	O
teaching	O
these	O
topics	O
over	O
the	O
years	O
we	O
have	O
discovered	O
that	O
they	O
are	O
of	O
interest	O
to	O
master	O
s	O
and	O
phd	O
students	O
in	O
fields	O
as	O
disparate	O
as	O
business	O
administration	O
biology	O
and	O
computer	O
science	O
as	O
well	O
as	O
to	O
quantitativelyoriented	O
upper-division	O
undergraduates	O
it	O
is	O
important	O
for	O
this	O
diverse	O
group	O
to	O
be	O
able	O
to	O
understand	O
the	O
models	O
intuitions	O
and	O
strengths	O
and	O
weaknesses	O
of	O
the	O
various	O
approaches	O
but	O
for	O
this	O
audience	O
many	O
of	O
the	O
technical	O
details	O
behind	O
statistical	O
learning	O
methods	O
such	O
as	O
optimization	O
algorithms	O
and	O
theoretical	O
properties	O
are	O
not	O
of	O
primary	O
interest	O
we	O
believe	O
that	O
these	O
students	O
do	O
not	O
need	O
a	O
deep	O
understanding	O
of	O
these	O
aspects	O
in	O
order	O
to	O
become	O
informed	O
users	O
of	O
the	O
various	O
methodologies	O
and	O
introduction	O
in	O
order	O
to	O
contribute	O
to	O
their	O
chosen	O
fields	O
through	O
the	O
use	O
of	O
statistical	O
learning	O
tools	O
islr	O
is	O
based	O
on	O
the	O
following	O
four	O
premises	O
many	O
statistical	O
learning	O
methods	O
are	O
relevant	O
and	O
useful	O
in	O
a	O
wide	O
range	O
of	O
academic	O
and	O
non-academic	O
disciplines	O
beyond	O
just	O
the	O
statistical	O
sciences	O
we	O
believe	O
that	O
many	O
contemporary	O
statistical	O
learning	O
procedures	O
should	O
and	O
will	O
become	O
as	O
widely	O
available	O
and	O
used	O
as	O
is	O
currently	O
the	O
case	O
for	O
classical	O
methods	O
such	O
as	O
linear	B
regression	B
as	O
a	O
result	O
rather	O
than	O
attempting	O
to	O
consider	O
every	O
possible	O
approach	B
impossible	O
task	O
we	O
have	O
concentrated	O
on	O
presenting	O
the	O
methods	O
that	O
we	O
believe	O
are	O
most	O
widely	O
applicable	O
statistical	O
learning	O
should	O
not	O
be	O
viewed	O
as	O
a	O
series	O
of	O
black	O
boxes	O
no	O
single	B
approach	B
will	O
perform	O
well	O
in	O
all	O
possible	O
applications	O
without	O
understanding	O
all	O
of	O
the	O
cogs	O
inside	O
the	O
box	O
or	O
the	O
interaction	B
between	O
those	O
cogs	O
it	O
is	O
impossible	O
to	O
select	O
the	O
best	O
box	O
hence	O
we	O
have	O
attempted	O
to	O
carefully	O
describe	O
the	O
model	B
intuition	O
assumptions	O
and	O
trade-offs	O
behind	O
each	O
of	O
the	O
methods	O
that	O
we	O
consider	O
while	O
it	O
is	O
important	O
to	O
know	O
what	O
job	O
is	O
performed	O
by	O
each	O
cog	O
it	O
is	O
not	O
necessary	O
to	O
have	O
the	O
skills	O
to	O
construct	O
the	O
machine	B
inside	O
the	O
box	O
thus	O
we	O
have	O
minimized	O
discussion	O
of	O
technical	O
details	O
related	O
to	O
fitting	O
procedures	O
and	O
theoretical	O
properties	O
we	O
assume	O
that	O
the	O
reader	O
is	O
comfortable	O
with	O
basic	O
mathematical	O
concepts	O
but	O
we	O
do	O
not	O
assume	O
a	O
graduate	O
degree	O
in	O
the	O
mathematical	O
sciences	O
for	O
instance	O
we	O
have	O
almost	O
completely	O
avoided	O
the	O
use	O
of	O
matrix	O
algebra	O
and	O
it	O
is	O
possible	O
to	O
understand	O
the	O
entire	O
book	O
without	O
a	O
detailed	O
knowledge	O
of	O
matrices	O
and	O
vectors	O
we	O
presume	O
that	O
the	O
reader	O
is	O
interested	O
in	O
applying	O
statistical	O
learning	O
methods	O
to	O
real-world	O
problems	O
in	O
order	O
to	O
facilitate	O
this	O
as	O
well	O
as	O
to	O
motivate	O
the	O
techniques	O
discussed	O
we	O
have	O
devoted	O
a	O
section	O
within	O
each	O
chapter	O
to	O
r	O
computer	O
labs	O
in	O
each	O
lab	O
we	O
walk	O
the	O
reader	O
through	O
a	O
realistic	O
application	O
of	O
the	O
methods	O
considered	O
in	O
that	O
chapter	O
when	O
we	O
have	O
taught	O
this	O
material	O
in	O
our	O
courses	O
we	O
have	O
allocated	O
roughly	O
one-third	O
of	O
classroom	O
time	O
to	O
working	O
through	O
the	O
labs	O
and	O
we	O
have	O
found	O
them	O
to	O
be	O
extremely	O
useful	O
many	O
of	O
the	O
less	O
computationally-oriented	O
students	O
who	O
were	O
initially	O
intimidated	O
by	O
r	O
s	O
command	O
level	B
interface	O
got	O
the	O
hang	O
of	O
things	O
over	O
the	O
course	O
of	O
the	O
quarter	O
or	O
semester	O
we	O
have	O
used	O
r	O
because	O
it	O
is	O
freely	O
available	O
and	O
is	O
powerful	O
enough	O
to	O
implement	O
all	O
of	O
the	O
methods	O
discussed	O
in	O
the	O
book	O
it	O
also	O
has	O
optional	O
packages	O
that	O
can	O
be	O
downloaded	O
to	O
implement	O
literally	O
thousands	O
of	O
additional	O
methods	O
most	O
importantly	O
r	O
is	O
the	O
language	O
of	O
choice	O
for	O
academic	O
statisticians	O
and	O
new	O
approaches	O
often	O
become	O
available	O
in	O
introduction	O
r	O
years	O
before	O
they	O
are	O
implemented	O
in	O
commercial	O
packages	O
however	O
the	O
labs	O
in	O
isl	O
are	O
self-contained	O
and	O
can	O
be	O
skipped	O
if	O
the	O
reader	O
wishes	O
to	O
use	O
a	O
different	O
software	O
package	O
or	O
does	O
not	O
wish	O
to	O
apply	O
the	O
methods	O
discussed	O
to	O
real-world	O
problems	O
who	O
should	O
read	O
this	O
book	O
this	O
book	O
is	O
intended	O
for	O
anyone	O
who	O
is	O
interested	O
in	O
using	O
modern	O
statistical	O
methods	O
for	O
modeling	O
and	O
prediction	B
from	O
data	B
this	O
group	O
includes	O
scientists	O
engineers	O
data	B
analysts	O
or	O
quants	O
but	O
also	O
less	O
technical	O
individuals	O
with	O
degrees	O
in	O
non-quantitative	O
fields	O
such	O
as	O
the	O
social	O
sciences	O
or	O
business	O
we	O
expect	O
that	O
the	O
reader	O
will	O
have	O
had	O
at	O
least	O
one	O
elementary	O
course	O
in	O
statistics	O
background	O
in	O
linear	B
regression	B
is	O
also	O
useful	O
though	O
not	O
required	O
since	O
we	O
review	O
the	O
key	O
concepts	O
behind	O
linear	B
regression	B
in	O
chapter	O
the	O
mathematical	O
level	B
of	O
this	O
book	O
is	O
modest	O
and	O
a	O
detailed	O
knowledge	O
of	O
matrix	O
operations	O
is	O
not	O
required	O
this	O
book	O
provides	O
an	O
introduction	O
to	O
the	O
statistical	O
programming	O
language	O
r	O
previous	O
exposure	O
to	O
a	O
programming	O
language	O
such	O
as	O
matlab	O
or	O
python	O
is	O
useful	O
but	O
not	O
required	O
we	O
have	O
successfully	O
taught	O
material	O
at	O
this	O
level	B
to	O
master	O
s	O
and	O
phd	O
students	O
in	O
business	O
computer	O
science	O
biology	O
earth	O
sciences	O
psychology	O
and	O
many	O
other	O
areas	O
of	O
the	O
physical	O
and	O
social	O
sciences	O
this	O
book	O
could	O
also	O
be	O
appropriate	O
for	O
advanced	O
undergraduates	O
who	O
have	O
already	O
taken	O
a	O
course	O
on	O
linear	B
regression	B
in	O
the	O
context	O
of	O
a	O
more	O
mathematically	O
rigorous	O
course	O
in	O
which	O
esl	O
serves	O
as	O
the	O
primary	O
textbook	O
isl	O
could	O
be	O
used	O
as	O
a	O
supplementary	O
text	O
for	O
teaching	O
computational	O
aspects	O
of	O
the	O
various	O
approaches	O
notation	O
and	O
simple	B
matrix	O
algebra	O
choosing	O
notation	O
for	O
a	O
textbook	O
is	O
always	O
a	O
difficult	O
task	O
for	O
the	O
most	O
part	O
we	O
adopt	O
the	O
same	O
notational	O
conventions	O
as	O
esl	O
we	O
will	O
use	O
n	O
to	O
represent	O
the	O
number	O
of	O
distinct	O
data	B
points	O
or	O
observations	B
in	O
our	O
sample	O
we	O
will	O
let	O
p	O
denote	O
the	O
number	O
of	O
variables	O
that	O
are	O
available	O
for	O
use	O
in	O
making	O
predictions	O
for	O
example	O
the	O
wage	B
data	B
set	B
consists	O
of	O
variables	O
for	O
people	O
so	O
we	O
have	O
n	O
observations	B
and	O
p	O
variables	O
as	O
year	O
age	O
and	O
more	O
note	O
that	O
throughout	O
this	O
book	O
we	O
indicate	O
variable	B
names	O
using	O
colored	O
font	O
variable	B
name	O
sex	O
in	O
some	O
examples	O
p	O
might	O
be	O
quite	O
large	O
such	O
as	O
on	O
the	O
order	O
of	O
thousands	O
or	O
even	O
millions	O
this	O
situation	O
arises	O
quite	O
often	O
for	O
example	O
in	O
the	O
analysis	B
of	O
modern	O
biological	O
data	B
or	O
web-based	O
advertising	B
data	B
introduction	O
in	O
general	O
we	O
will	O
let	O
xij	O
represent	O
the	O
value	O
of	O
the	O
jth	O
variable	B
for	O
the	O
ith	O
observation	O
where	O
i	O
n	O
and	O
j	O
p	O
throughout	O
this	O
book	O
i	O
will	O
be	O
used	O
to	O
index	O
the	O
samples	O
or	O
observations	B
to	O
n	O
and	O
j	O
will	O
be	O
used	O
to	O
index	O
the	O
variables	O
to	O
p	O
we	O
let	O
x	O
denote	O
a	O
n	O
p	O
matrix	O
whose	O
jth	O
element	O
is	O
xij	O
that	O
is	O
x	O
xnp	O
for	O
readers	O
who	O
are	O
unfamiliar	O
with	O
matrices	O
it	O
is	O
useful	O
to	O
visualize	O
x	O
as	O
a	O
spreadsheet	O
of	O
numbers	O
with	O
n	O
rows	O
and	O
p	O
columns	O
at	O
times	O
we	O
will	O
be	O
interested	O
in	O
the	O
rows	O
of	O
x	O
which	O
we	O
write	O
as	O
xn	O
here	O
xi	O
is	O
a	O
vector	B
of	O
length	O
p	O
containing	O
the	O
p	O
variable	B
measurements	O
for	O
the	O
ith	O
observation	O
that	O
is	O
xip	O
xi	O
are	O
by	O
default	B
represented	O
as	O
columns	O
for	O
example	O
for	O
the	O
wage	B
data	B
xi	O
is	O
a	O
vector	B
of	O
length	O
consisting	O
of	O
year	O
age	O
and	O
other	O
values	O
for	O
the	O
ith	O
individual	O
at	O
other	O
times	O
we	O
will	O
instead	O
be	O
interested	O
in	O
the	O
columns	O
of	O
x	O
which	O
we	O
write	O
as	O
xp	O
each	O
is	O
a	O
vector	B
of	O
length	O
n	O
that	O
is	O
sex	O
xnj	O
xj	O
for	O
example	O
for	O
the	O
wage	B
data	B
contains	O
the	O
n	O
values	O
for	O
year	O
using	O
this	O
notation	O
the	O
matrix	O
x	O
can	O
be	O
written	O
as	O
x	O
or	O
xp	O
xt	O
xt	O
xt	O
n	O
x	O
introduction	O
the	O
t	O
notation	O
denotes	O
the	O
transpose	O
of	O
a	O
matrix	O
or	O
vector	B
so	O
for	O
example	O
xt	O
xnp	O
while	O
xip	O
xt	O
i	O
we	O
use	O
yi	O
to	O
denote	O
the	O
ith	O
observation	O
of	O
the	O
variable	B
on	O
which	O
we	O
wish	O
to	O
make	O
predictions	O
such	O
as	O
wage	B
hence	O
we	O
write	O
the	O
set	B
of	O
all	O
n	O
observations	B
in	O
vector	B
form	O
as	O
yn	O
y	O
then	O
our	O
observed	O
data	B
consists	O
of	O
yn	O
where	O
each	O
xi	O
is	O
a	O
vector	B
of	O
length	O
p	O
p	O
then	O
xi	O
is	O
simply	O
a	O
scalar	O
in	O
this	O
text	O
a	O
vector	B
of	O
length	O
n	O
will	O
always	O
be	O
denoted	O
in	O
lower	O
case	O
bold	O
e	O
g	O
an	O
a	O
however	O
vectors	O
that	O
are	O
not	O
of	O
length	O
n	O
as	O
feature	B
vectors	O
of	O
length	O
p	O
as	O
in	O
will	O
be	O
denoted	O
in	O
lower	O
case	O
normal	O
font	O
e	O
g	O
a	O
scalars	O
will	O
also	O
be	O
denoted	O
in	O
lower	O
case	O
normal	O
font	O
e	O
g	O
a	O
in	O
the	O
rare	O
cases	O
in	O
which	O
these	O
two	O
uses	O
for	O
lower	O
case	O
normal	O
font	O
lead	O
to	O
ambiguity	O
we	O
will	O
clarify	O
which	O
use	O
is	O
intended	O
matrices	O
will	O
be	O
denoted	O
using	O
bold	O
capitals	O
such	O
as	O
a	O
random	O
variables	O
will	O
be	O
denoted	O
using	O
capital	O
normal	O
font	O
e	O
g	O
a	O
regardless	O
of	O
their	O
dimensions	O
occasionally	O
we	O
will	O
want	O
to	O
indicate	O
the	O
dimension	O
of	O
a	O
particular	O
object	O
to	O
indicate	O
that	O
an	O
object	O
is	O
a	O
scalar	O
we	O
will	O
use	O
the	O
notation	O
a	O
r	O
to	O
indicate	O
that	O
it	O
is	O
a	O
vector	B
of	O
length	O
k	O
we	O
will	O
use	O
a	O
rk	O
a	O
rn	O
if	O
it	O
is	O
of	O
length	O
n	O
we	O
will	O
indicate	O
that	O
an	O
object	O
is	O
a	O
r	O
s	O
matrix	O
using	O
a	O
rr	O
s	O
we	O
have	O
avoided	O
using	O
matrix	O
algebra	O
whenever	O
possible	O
however	O
in	O
a	O
few	O
instances	O
it	O
becomes	O
too	O
cumbersome	O
to	O
avoid	O
it	O
entirely	O
in	O
these	O
rare	O
instances	O
it	O
is	O
important	O
to	O
understand	O
the	O
concept	O
of	O
multiplying	O
two	O
matrices	O
suppose	O
that	O
a	O
rr	O
d	O
and	O
b	O
rd	O
s	O
then	O
the	O
product	O
introduction	O
of	O
a	O
and	O
b	O
is	O
denoted	O
ab	O
the	O
jth	O
element	O
of	O
ab	O
is	O
computed	O
by	O
multiplying	O
each	O
element	O
of	O
the	O
ith	O
row	O
of	O
a	O
by	O
the	O
corresponding	O
element	O
d	O
of	O
the	O
jth	O
column	O
of	O
b	O
that	O
is	O
aikbkj	O
as	O
an	O
example	O
consider	O
and	O
b	O
a	O
then	O
ab	O
note	O
that	O
this	O
operation	O
produces	O
an	O
r	O
s	O
matrix	O
it	O
is	O
only	O
possible	O
to	O
compute	O
ab	O
if	O
the	O
number	O
of	O
columns	O
of	O
a	O
is	O
the	O
same	O
as	O
the	O
number	O
of	O
rows	O
of	O
b	O
organization	O
of	O
this	O
book	O
chapter	O
introduces	O
the	O
basic	O
terminology	O
and	O
concepts	O
behind	O
statistical	O
learning	O
this	O
chapter	O
also	O
presents	O
the	O
k-nearest	O
neighbor	O
classifier	B
a	O
very	O
simple	B
method	O
that	O
works	O
surprisingly	O
well	O
on	O
many	O
problems	O
chapters	O
and	O
cover	O
classical	O
linear	B
methods	O
for	O
regression	B
and	O
classification	B
in	O
particular	O
chapter	O
reviews	O
linear	B
regression	B
the	O
fundamental	O
starting	O
point	O
for	O
all	O
regression	B
methods	O
in	O
chapter	O
we	O
discuss	O
two	O
of	O
the	O
most	O
important	O
classical	O
classification	B
methods	O
logistic	B
regression	B
and	O
linear	B
discriminant	I
analysis	B
a	O
central	O
problem	O
in	O
all	O
statistical	O
learning	O
situations	O
involves	O
choosing	O
the	O
best	O
method	O
for	O
a	O
given	O
application	O
hence	O
in	O
chapter	O
we	O
introduce	O
cross-validation	B
and	O
the	O
bootstrap	B
which	O
can	O
be	O
used	O
to	O
estimate	O
the	O
accuracy	O
of	O
a	O
number	O
of	O
different	O
methods	O
in	O
order	O
to	O
choose	O
the	O
best	O
one	O
much	O
of	O
the	O
recent	O
research	O
in	O
statistical	O
learning	O
has	O
concentrated	O
on	O
non-linear	B
methods	O
however	O
linear	B
methods	O
often	O
have	O
advantages	O
over	O
their	O
non-linear	B
competitors	O
in	O
terms	O
of	O
interpretability	B
and	O
sometimes	O
also	O
accuracy	O
hence	O
in	O
chapter	O
we	O
consider	O
a	O
host	O
of	O
linear	B
methods	O
both	O
classical	O
and	O
more	O
modern	O
which	O
offer	O
potential	O
improvements	O
over	O
standard	O
linear	B
regression	B
these	O
include	O
stepwise	O
selection	B
ridge	B
regression	B
principal	B
components	I
regression	B
partial	B
least	B
squares	I
and	O
the	O
lasso	B
the	O
remaining	O
chapters	O
move	O
into	O
the	O
world	O
of	O
non-linear	B
statistical	O
learning	O
we	O
first	O
introduce	O
in	O
chapter	O
a	O
number	O
of	O
non-linear	B
methods	O
that	O
work	O
well	O
for	O
problems	O
with	O
a	O
single	B
input	B
variable	B
we	O
then	O
show	O
how	O
these	O
methods	O
can	O
be	O
used	O
to	O
fit	O
non-linear	B
additive	B
models	O
for	O
which	O
there	O
is	O
more	O
than	O
one	O
input	B
in	O
chapter	O
we	O
investigate	O
tree-based	O
methods	O
including	O
bagging	B
boosting	B
and	O
random	O
forests	O
support	B
vector	B
machines	O
a	O
set	B
of	O
approaches	O
for	O
performing	O
both	O
linear	B
and	O
non-linear	B
classification	B
introduction	O
are	O
discussed	O
in	O
chapter	O
finally	O
in	O
chapter	O
we	O
consider	O
a	O
setting	O
in	O
which	O
we	O
have	O
input	B
variables	O
but	O
no	O
output	B
variable	B
in	O
particular	O
we	O
present	O
principal	B
components	I
analysis	B
k-means	B
clustering	B
and	O
hierarchical	B
clustering	B
at	O
the	O
end	O
of	O
each	O
chapter	O
we	O
present	O
one	O
or	O
more	O
r	O
lab	O
sections	O
in	O
which	O
we	O
systematically	O
work	O
through	O
applications	O
of	O
the	O
various	O
methods	O
discussed	O
in	O
that	O
chapter	O
these	O
labs	O
demonstrate	O
the	O
strengths	O
and	O
weaknesses	O
of	O
the	O
various	O
approaches	O
and	O
also	O
provide	O
a	O
useful	O
reference	O
for	O
the	O
syntax	O
required	O
to	O
implement	O
the	O
various	O
methods	O
the	O
reader	O
may	O
choose	O
to	O
work	O
through	O
the	O
labs	O
at	O
his	O
or	O
her	O
own	O
pace	O
or	O
the	O
labs	O
may	O
be	O
the	O
focus	O
of	O
group	O
sessions	O
as	O
part	O
of	O
a	O
classroom	O
environment	O
within	O
each	O
r	O
lab	O
we	O
present	O
the	O
results	O
that	O
we	O
obtained	O
when	O
we	O
performed	O
the	O
lab	O
at	O
the	O
time	O
of	O
writing	O
this	O
book	O
however	O
new	O
versions	O
of	O
r	O
are	O
continuously	O
released	O
and	O
over	O
time	O
the	O
packages	O
called	O
in	O
the	O
labs	O
will	O
be	O
updated	O
therefore	O
in	O
the	O
future	O
it	O
is	O
possible	O
that	O
the	O
results	O
shown	O
in	O
the	O
lab	O
sections	O
may	O
no	O
longer	O
correspond	O
precisely	O
to	O
the	O
results	O
obtained	O
by	O
the	O
reader	O
who	O
performs	O
the	O
labs	O
as	O
necessary	O
we	O
will	O
post	O
updates	O
to	O
the	O
labs	O
on	O
the	O
book	O
website	O
we	O
use	O
the	O
symbol	O
to	O
denote	O
sections	O
or	O
exercises	O
that	O
contain	O
more	O
challenging	O
concepts	O
these	O
can	O
be	O
easily	O
skipped	O
by	O
readers	O
who	O
do	O
not	O
wish	O
to	O
delve	O
as	O
deeply	O
into	O
the	O
material	O
or	O
who	O
lack	O
the	O
mathematical	O
background	O
data	B
sets	O
used	O
in	O
labs	O
and	O
exercises	O
in	O
this	O
textbook	O
we	O
illustrate	O
statistical	O
learning	O
methods	O
using	O
applications	O
from	O
marketing	O
finance	O
biology	O
and	O
other	O
areas	O
the	O
islr	O
package	O
available	O
on	O
the	O
book	O
website	O
contains	O
a	O
number	O
of	O
data	B
sets	O
that	O
are	O
required	O
in	O
order	O
to	O
perform	O
the	O
labs	O
and	O
exercises	O
associated	O
with	O
this	O
book	O
one	O
other	O
data	B
set	B
is	O
contained	O
in	O
the	O
mass	O
library	O
and	O
yet	O
another	O
is	O
part	O
of	O
the	O
base	O
r	O
distribution	B
table	O
contains	O
a	O
summary	O
of	O
the	O
data	B
sets	O
required	O
to	O
perform	O
the	O
labs	O
and	O
exercises	O
a	O
couple	O
of	O
these	O
data	B
sets	O
are	O
also	O
available	O
as	O
text	O
files	O
on	O
the	O
book	O
website	O
for	O
use	O
in	O
chapter	O
book	O
website	O
the	O
website	O
for	O
this	O
book	O
is	O
located	O
at	O
www	O
statlearning	O
com	O
introduction	O
description	O
gas	O
mileage	O
horsepower	O
and	O
other	O
information	O
for	O
cars	O
housing	O
values	O
and	O
other	O
information	O
about	O
boston	B
suburbs	O
information	O
about	O
individuals	O
offered	O
caravan	B
insurance	O
information	O
about	O
car	O
seat	O
sales	O
in	O
stores	O
demographic	O
characteristics	O
tuition	O
and	O
more	O
for	O
usa	O
colleges	O
customer	O
default	B
records	O
for	O
a	O
credit	B
card	O
company	O
records	O
and	O
salaries	O
for	O
baseball	O
players	O
gene	O
expression	O
measurements	O
for	O
four	O
cancer	O
types	O
gene	O
expression	O
measurements	O
for	O
cancer	O
cell	O
lines	O
sales	O
information	O
for	O
citrus	O
hill	O
and	O
minute	O
maid	O
orange	O
juice	O
name	O
auto	B
boston	B
caravan	B
carseats	B
college	B
default	B
hitters	B
khan	B
oj	B
portfolio	B
past	O
values	O
of	O
financial	O
assets	O
for	O
use	O
in	O
portfolio	B
allocation	O
daily	O
percentage	O
returns	O
for	O
sp	O
over	O
a	O
period	O
smarket	B
usarrests	B
crime	O
statistics	O
per	O
residents	O
in	O
states	O
of	O
usa	O
wage	B
weekly	B
income	B
survey	O
data	B
for	O
males	O
in	O
central	O
atlantic	O
region	O
of	O
usa	O
weekly	B
stock	O
market	O
returns	O
for	O
years	O
table	O
a	O
list	O
of	O
data	B
sets	O
needed	O
to	O
perform	O
the	O
labs	O
and	O
exercises	O
in	O
this	O
textbook	O
all	O
data	B
sets	O
are	O
available	O
in	O
the	O
islr	O
library	O
with	O
the	O
exception	O
of	O
boston	B
of	O
mass	O
and	O
usarrests	B
of	O
the	O
base	O
r	O
distribution	B
it	O
contains	O
a	O
number	O
of	O
resources	O
including	O
the	O
r	O
package	O
associated	O
with	O
this	O
book	O
and	O
some	O
additional	O
data	B
sets	O
acknowledgements	O
a	O
few	O
of	O
the	O
plots	O
in	O
this	O
book	O
were	O
taken	O
from	O
esl	O
figures	O
and	O
all	O
other	O
plots	O
are	O
new	O
to	O
this	O
book	O
statistical	O
learning	O
what	O
is	O
statistical	O
learning	O
in	O
order	O
to	O
motivate	O
our	O
study	O
of	O
statistical	O
learning	O
we	O
begin	O
with	O
a	O
simple	B
example	O
suppose	O
that	O
we	O
are	O
statistical	O
consultants	O
hired	O
by	O
a	O
client	O
to	O
provide	O
advice	O
on	O
how	O
to	O
improve	O
sales	O
of	O
a	O
particular	O
product	O
the	O
advertising	B
data	B
set	B
consists	O
of	O
the	O
sales	O
of	O
that	O
product	O
in	O
different	O
markets	O
along	O
with	O
advertising	B
budgets	O
for	O
the	O
product	O
in	O
each	O
of	O
those	O
markets	O
for	O
three	O
different	O
media	O
tv	O
radio	O
and	O
newspaper	O
the	O
data	B
are	O
displayed	O
in	O
figure	O
it	O
is	O
not	O
possible	O
for	O
our	O
client	O
to	O
directly	O
increase	O
sales	O
of	O
the	O
product	O
on	O
the	O
other	O
hand	O
they	O
can	O
control	O
the	O
advertising	B
expenditure	O
in	O
each	O
of	O
the	O
three	O
media	O
therefore	O
if	O
we	O
determine	O
that	O
there	O
is	O
an	O
association	O
between	O
advertising	B
and	O
sales	O
then	O
we	O
can	O
instruct	O
our	O
client	O
to	O
adjust	O
advertising	B
budgets	O
thereby	O
indirectly	O
increasing	O
sales	O
in	O
other	O
words	O
our	O
goal	O
is	O
to	O
develop	O
an	O
accurate	O
model	B
that	O
can	O
be	O
used	O
to	O
predict	O
sales	O
on	O
the	O
basis	B
of	O
the	O
three	O
media	O
budgets	O
in	O
this	O
setting	O
the	O
advertising	B
budgets	O
are	O
input	B
variables	O
while	O
sales	O
is	O
an	O
output	B
variable	B
the	O
input	B
variables	O
are	O
typically	O
denoted	O
using	O
the	O
symbol	O
x	O
with	O
a	O
subscript	O
to	O
distinguish	O
them	O
so	O
might	O
be	O
the	O
tv	O
budget	O
the	O
radio	O
budget	O
and	O
the	O
newspaper	O
budget	O
the	O
inputs	O
go	O
by	O
different	O
names	O
such	O
as	O
predictors	O
independent	B
variables	O
features	O
or	O
sometimes	O
just	O
variables	O
the	O
output	B
variable	B
in	O
this	O
case	O
sales	O
is	O
often	O
called	O
the	O
response	B
or	O
dependent	B
variable	B
and	O
is	O
typically	O
denoted	O
using	O
the	O
symbol	O
y	O
throughout	O
this	O
book	O
we	O
will	O
use	O
all	O
of	O
these	O
terms	O
interchangeably	O
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
input	B
variable	B
output	B
variable	B
predictor	B
independent	B
variable	B
feature	B
variable	B
response	B
dependent	B
variable	B
statistical	O
learning	O
l	O
s	O
e	O
a	O
s	O
l	O
s	O
e	O
a	O
s	O
l	O
s	O
e	O
a	O
s	O
tv	O
radio	O
newspaper	O
figure	O
the	O
advertising	B
data	B
set	B
the	O
plot	B
displays	O
sales	O
in	O
thousands	O
of	O
units	O
as	O
a	O
function	B
of	O
tv	O
radio	O
and	O
newspaper	O
budgets	O
in	O
thousands	O
of	O
dollars	O
for	O
different	O
markets	O
in	O
each	O
plot	B
we	O
show	O
the	O
simple	B
least	B
squares	I
fit	O
of	O
sales	O
to	O
that	O
variable	B
as	O
described	O
in	O
chapter	O
in	O
other	O
words	O
each	O
blue	O
line	B
represents	O
a	O
simple	B
model	B
that	O
can	O
be	O
used	O
to	O
predict	O
sales	O
using	O
tv	O
radio	O
and	O
newspaper	O
respectively	O
more	O
generally	O
suppose	O
that	O
we	O
observe	O
a	O
quantitative	B
response	B
y	O
and	O
p	O
different	O
predictors	O
xp	O
we	O
assume	O
that	O
there	O
is	O
some	O
relationship	O
between	O
y	O
and	O
x	O
xp	O
which	O
can	O
be	O
written	O
in	O
the	O
very	O
general	O
form	O
y	O
f	O
here	O
f	O
is	O
some	O
fixed	O
but	O
unknown	O
function	B
of	O
xp	O
and	O
is	O
a	O
random	O
error	B
term	B
which	O
is	O
independent	B
of	O
x	O
and	O
has	O
mean	O
zero	O
in	O
this	O
formulation	O
f	O
represents	O
the	O
systematic	B
information	O
that	O
x	O
provides	O
about	O
y	O
as	O
another	O
example	O
consider	O
the	O
left-hand	O
panel	O
of	O
figure	O
a	O
plot	B
of	O
income	B
versus	O
years	O
of	O
education	O
for	O
individuals	O
in	O
the	O
income	B
data	B
set	B
the	O
plot	B
suggests	O
that	O
one	O
might	O
be	O
able	O
to	O
predict	O
income	B
using	O
years	O
of	O
education	O
however	O
the	O
function	B
f	O
that	O
connects	O
the	O
input	B
variable	B
to	O
the	O
output	B
variable	B
is	O
in	O
general	O
unknown	O
in	O
this	O
situation	O
one	O
must	O
estimate	O
f	O
based	O
on	O
the	O
observed	O
points	O
since	O
income	B
is	O
a	O
simulated	O
data	B
set	B
f	O
is	O
known	O
and	O
is	O
shown	O
by	O
the	O
blue	O
curve	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
vertical	O
lines	O
represent	O
the	O
error	B
terms	O
we	O
note	O
that	O
some	O
of	O
the	O
observations	B
lie	O
above	O
the	O
blue	O
curve	O
and	O
some	O
lie	O
below	O
it	O
overall	O
the	O
errors	O
have	O
approximately	O
mean	O
zero	O
in	O
general	O
the	O
function	B
f	O
may	O
involve	O
more	O
than	O
one	O
input	B
variable	B
in	O
figure	O
we	O
plot	B
income	B
as	O
a	O
function	B
of	O
years	O
of	O
education	O
and	O
seniority	O
here	O
f	O
is	O
a	O
two-dimensional	O
surface	O
that	O
must	O
be	O
estimated	O
based	O
on	O
the	O
observed	O
data	B
error	B
term	B
systematic	B
what	O
is	O
statistical	O
learning	O
e	O
m	O
o	O
c	O
n	O
i	O
e	O
m	O
o	O
c	O
n	O
i	O
years	O
of	O
education	O
years	O
of	O
education	O
figure	O
the	O
income	B
data	B
set	B
left	O
the	O
red	O
dots	O
are	O
the	O
observed	O
values	O
of	O
income	B
tens	O
of	O
thousands	O
of	O
dollars	O
and	O
years	O
of	O
education	O
for	O
individuals	O
right	O
the	O
blue	O
curve	O
represents	O
the	O
true	O
underlying	O
relationship	O
between	O
income	B
and	O
years	O
of	O
education	O
which	O
is	O
generally	O
unknown	O
is	O
known	O
in	O
this	O
case	O
because	O
the	O
data	B
were	O
simulated	O
the	O
black	O
lines	O
represent	O
the	O
error	B
associated	O
with	O
each	O
observation	O
note	O
that	O
some	O
errors	O
are	O
positive	O
an	O
observation	O
lies	O
above	O
the	O
blue	O
curve	O
and	O
some	O
are	O
negative	O
an	O
observation	O
lies	O
below	O
the	O
curve	O
overall	O
these	O
errors	O
have	O
approximately	O
mean	O
zero	O
in	O
essence	O
statistical	O
learning	O
refers	O
to	O
a	O
set	B
of	O
approaches	O
for	O
estimating	O
f	O
in	O
this	O
chapter	O
we	O
outline	O
some	O
of	O
the	O
key	O
theoretical	O
concepts	O
that	O
arise	O
in	O
estimating	O
f	O
as	O
well	O
as	O
tools	O
for	O
evaluating	O
the	O
estimates	O
obtained	O
why	O
estimate	O
f	O
there	O
are	O
two	O
main	O
reasons	O
that	O
we	O
may	O
wish	O
to	O
estimate	O
f	O
prediction	B
and	O
inference	B
we	O
discuss	O
each	O
in	O
turn	O
prediction	B
in	O
many	O
situations	O
a	O
set	B
of	O
inputs	O
x	O
are	O
readily	O
available	O
but	O
the	O
output	B
y	O
cannot	O
be	O
easily	O
obtained	O
in	O
this	O
setting	O
since	O
the	O
error	B
term	B
averages	O
to	O
zero	O
we	O
can	O
predict	O
y	O
using	O
y	O
f	O
where	O
f	O
represents	O
our	O
estimate	O
for	O
f	O
and	O
y	O
represents	O
the	O
resulting	O
prediction	B
for	O
y	O
in	O
this	O
setting	O
f	O
is	O
often	O
treated	O
as	O
a	O
black	O
box	O
in	O
the	O
sense	O
that	O
one	O
is	O
not	O
typically	O
concerned	O
with	O
the	O
exact	O
form	O
of	O
f	O
provided	O
that	O
it	O
yields	O
accurate	O
predictions	O
for	O
y	O
statistical	O
learning	O
i	O
n	O
c	O
o	O
m	O
e	O
ye	O
ars	O
of	O
e	O
d	O
ucatio	O
n	O
seniority	O
figure	O
the	O
plot	B
displays	O
income	B
as	O
a	O
function	B
of	O
years	O
of	O
education	O
and	O
seniority	O
in	O
the	O
income	B
data	B
set	B
the	O
blue	O
surface	O
represents	O
the	O
true	O
underlying	O
relationship	O
between	O
income	B
and	O
years	O
of	O
education	O
and	O
seniority	O
which	O
is	O
known	O
since	O
the	O
data	B
are	O
simulated	O
the	O
red	O
dots	O
indicate	O
the	O
observed	O
values	O
of	O
these	O
quantities	O
for	O
individuals	O
as	O
an	O
example	O
suppose	O
that	O
xp	O
are	O
characteristics	O
of	O
a	O
patient	O
s	O
blood	O
sample	O
that	O
can	O
be	O
easily	O
measured	O
in	O
a	O
lab	O
and	O
y	O
is	O
a	O
variable	B
encoding	O
the	O
patient	O
s	O
risk	O
for	O
a	O
severe	O
adverse	O
reaction	O
to	O
a	O
particular	O
drug	O
it	O
is	O
natural	B
to	O
seek	O
to	O
predict	O
y	O
using	O
x	O
since	O
we	O
can	O
then	O
avoid	O
giving	O
the	O
drug	O
in	O
question	O
to	O
patients	O
who	O
are	O
at	O
high	O
risk	O
of	O
an	O
adverse	O
reaction	O
that	O
is	O
patients	O
for	O
whom	O
the	O
estimate	O
of	O
y	O
is	O
high	O
the	O
accuracy	O
of	O
y	O
as	O
a	O
prediction	B
for	O
y	O
depends	O
on	O
two	O
quantities	O
which	O
we	O
will	O
call	O
the	O
reducible	B
error	B
and	O
the	O
irreducible	B
error	B
in	O
general	O
f	O
will	O
not	O
be	O
a	O
perfect	O
estimate	O
for	O
f	O
and	O
this	O
inaccuracy	O
will	O
introduce	O
some	O
error	B
this	O
error	B
is	O
reducible	B
because	O
we	O
can	O
potentially	O
improve	O
the	O
accuracy	O
of	O
f	O
by	O
using	O
the	O
most	O
appropriate	O
statistical	O
learning	O
technique	O
to	O
estimate	O
f	O
however	O
even	O
if	O
it	O
were	O
possible	O
to	O
form	O
a	O
perfect	O
estimate	O
for	O
f	O
so	O
that	O
our	O
estimated	O
response	B
took	O
the	O
form	O
y	O
f	O
our	O
prediction	B
would	O
still	O
have	O
some	O
error	B
in	O
it	O
this	O
is	O
because	O
y	O
is	O
also	O
a	O
function	B
of	O
which	O
by	O
definition	O
cannot	O
be	O
predicted	O
using	O
x	O
therefore	O
variability	O
associated	O
with	O
also	O
affects	O
the	O
accuracy	O
of	O
our	O
predictions	O
this	O
is	O
known	O
as	O
the	O
irreducible	B
error	B
because	O
no	O
matter	O
how	O
well	O
we	O
estimate	O
f	O
we	O
cannot	O
reduce	O
the	O
error	B
introduced	O
by	O
why	O
is	O
the	O
irreducible	B
error	B
larger	O
than	O
zero	O
the	O
quantity	O
may	O
contain	O
unmeasured	O
variables	O
that	O
are	O
useful	O
in	O
predicting	O
y	O
since	O
we	O
don	O
t	O
measure	O
them	O
f	O
cannot	O
use	O
them	O
for	O
its	O
prediction	B
the	O
quantity	O
may	O
also	O
contain	O
unmeasurable	O
variation	O
for	O
example	O
the	O
risk	O
of	O
an	O
adverse	O
reaction	O
might	O
vary	O
for	O
a	O
given	O
patient	O
on	O
a	O
given	O
day	O
depending	O
on	O
reducible	B
error	B
irreducible	B
error	B
what	O
is	O
statistical	O
learning	O
manufacturing	O
variation	O
in	O
the	O
drug	O
itself	O
or	O
the	O
patient	O
s	O
general	O
feeling	O
of	O
well-being	O
on	O
that	O
day	O
consider	O
a	O
given	O
estimate	O
f	O
and	O
a	O
set	B
of	O
predictors	O
x	O
which	O
yields	O
the	O
prediction	B
y	O
f	O
assume	O
for	O
a	O
moment	O
that	O
both	O
f	O
and	O
x	O
are	O
fixed	O
then	O
it	O
is	O
easy	O
to	O
show	O
that	O
ey	O
y	O
ef	O
f	O
f	O
var	O
expected	B
value	I
variance	B
reducible	B
irreducible	B
where	O
ey	O
y	O
represents	O
the	O
average	B
or	O
expected	B
value	I
of	O
the	O
squared	O
difference	O
between	O
the	O
predicted	O
and	O
actual	O
value	O
of	O
y	O
and	O
var	O
represents	O
the	O
variance	B
associated	O
with	O
the	O
error	B
term	B
the	O
focus	O
of	O
this	O
book	O
is	O
on	O
techniques	O
for	O
estimating	O
f	O
with	O
the	O
aim	O
of	O
minimizing	O
the	O
reducible	B
error	B
it	O
is	O
important	O
to	O
keep	O
in	O
mind	O
that	O
the	O
irreducible	B
error	B
will	O
always	O
provide	O
an	O
upper	O
bound	O
on	O
the	O
accuracy	O
of	O
our	O
prediction	B
for	O
y	O
this	O
bound	O
is	O
almost	O
always	O
unknown	O
in	O
practice	O
inference	B
we	O
are	O
often	O
interested	O
in	O
understanding	O
the	O
way	O
that	O
y	O
is	O
affected	O
as	O
xp	O
change	O
in	O
this	O
situation	O
we	O
wish	O
to	O
estimate	O
f	O
but	O
our	O
goal	O
is	O
not	O
necessarily	O
to	O
make	O
predictions	O
for	O
y	O
we	O
instead	O
want	O
to	O
understand	O
the	O
relationship	O
between	O
x	O
and	O
y	O
or	O
more	O
specifically	O
to	O
understand	O
how	O
y	O
changes	O
as	O
a	O
function	B
of	O
xp	O
now	O
f	O
cannot	O
be	O
treated	O
as	O
a	O
black	O
box	O
because	O
we	O
need	O
to	O
know	O
its	O
exact	O
form	O
in	O
this	O
setting	O
one	O
may	O
be	O
interested	O
in	O
answering	O
the	O
following	O
questions	O
which	O
predictors	O
are	O
associated	O
with	O
the	O
response	B
it	O
is	O
often	O
the	O
case	O
that	O
only	O
a	O
small	O
fraction	O
of	O
the	O
available	O
predictors	O
are	O
substantially	O
associated	O
with	O
y	O
identifying	O
the	O
few	O
important	O
predictors	O
among	O
a	O
large	O
set	B
of	O
possible	O
variables	O
can	O
be	O
extremely	O
useful	O
depending	O
on	O
the	O
application	O
what	O
is	O
the	O
relationship	O
between	O
the	O
response	B
and	O
each	O
predictor	B
some	O
predictors	O
may	O
have	O
a	O
positive	O
relationship	O
with	O
y	O
in	O
the	O
sense	O
that	O
increasing	O
the	O
predictor	B
is	O
associated	O
with	O
increasing	O
values	O
of	O
y	O
other	O
predictors	O
may	O
have	O
the	O
opposite	O
relationship	O
depending	O
on	O
the	O
complexity	O
of	O
f	O
the	O
relationship	O
between	O
the	O
response	B
and	O
a	O
given	O
predictor	B
may	O
also	O
depend	O
on	O
the	O
values	O
of	O
the	O
other	O
predictors	O
can	O
the	O
relationship	O
between	O
y	O
and	O
each	O
predictor	B
be	O
adequately	O
summarized	O
using	O
a	O
linear	B
equation	O
or	O
is	O
the	O
relationship	O
more	O
complicated	O
historically	O
most	O
methods	O
for	O
estimating	O
f	O
have	O
taken	O
a	O
linear	B
form	O
in	O
some	O
situations	O
such	O
an	O
assumption	O
is	O
reasonable	O
or	O
even	O
desirable	O
but	O
often	O
the	O
true	O
relationship	O
is	O
more	O
complicated	O
in	O
which	O
case	O
a	O
linear	B
model	B
may	O
not	O
provide	O
an	O
accurate	O
representation	O
of	O
the	O
relationship	O
between	O
the	O
input	B
and	O
output	B
variables	O
statistical	O
learning	O
in	O
this	O
book	O
we	O
will	O
see	O
a	O
number	O
of	O
examples	O
that	O
fall	O
into	O
the	O
prediction	B
setting	O
the	O
inference	B
setting	O
or	O
a	O
combination	O
of	O
the	O
two	O
for	O
instance	O
consider	O
a	O
company	O
that	O
is	O
interested	O
in	O
conducting	O
a	O
direct-marketing	O
campaign	O
the	O
goal	O
is	O
to	O
identify	O
individuals	O
who	O
will	O
respond	O
positively	O
to	O
a	O
mailing	O
based	O
on	O
observations	B
of	O
demographic	O
variables	O
measured	O
on	O
each	O
individual	O
in	O
this	O
case	O
the	O
demographic	O
variables	O
serve	O
as	O
predictors	O
and	O
response	B
to	O
the	O
marketing	O
campaign	O
positive	O
or	O
negative	O
serves	O
as	O
the	O
outcome	O
the	O
company	O
is	O
not	O
interested	O
in	O
obtaining	O
a	O
deep	O
understanding	O
of	O
the	O
relationships	O
between	O
each	O
individual	O
predictor	B
and	O
the	O
response	B
instead	O
the	O
company	O
simply	O
wants	O
an	O
accurate	O
model	B
to	O
predict	O
the	O
response	B
using	O
the	O
predictors	O
this	O
is	O
an	O
example	O
of	O
modeling	O
for	O
prediction	B
in	O
contrast	B
consider	O
the	O
advertising	B
data	B
illustrated	O
in	O
figure	O
one	O
may	O
be	O
interested	O
in	O
answering	O
questions	O
such	O
as	O
which	O
media	O
contribute	O
to	O
sales	O
which	O
media	O
generate	O
the	O
biggest	O
boost	O
in	O
sales	O
or	O
how	O
much	O
increase	O
in	O
sales	O
is	O
associated	O
with	O
a	O
given	O
increase	O
in	O
tv	O
advertising	B
this	O
situation	O
falls	O
into	O
the	O
inference	B
paradigm	O
another	O
example	O
involves	O
modeling	O
the	O
brand	O
of	O
a	O
product	O
that	O
a	O
customer	O
might	O
purchase	O
based	O
on	O
variables	O
such	O
as	O
price	O
store	O
location	O
discount	O
levels	O
competition	O
price	O
and	O
so	O
forth	O
in	O
this	O
situation	O
one	O
might	O
really	O
be	O
most	O
interested	O
in	O
how	O
each	O
of	O
the	O
individual	O
variables	O
affects	O
the	O
probability	B
of	O
purchase	O
for	O
instance	O
what	O
effect	O
will	O
changing	O
the	O
price	O
of	O
a	O
product	O
have	O
on	O
sales	O
this	O
is	O
an	O
example	O
of	O
modeling	O
for	O
inference	B
finally	O
some	O
modeling	O
could	O
be	O
conducted	O
both	O
for	O
prediction	B
and	O
inference	B
for	O
example	O
in	O
a	O
real	O
estate	O
setting	O
one	O
may	O
seek	O
to	O
relate	O
values	O
of	O
homes	O
to	O
inputs	O
such	O
as	O
crime	O
rate	B
zoning	O
distance	O
from	O
a	O
river	O
air	O
quality	O
schools	O
income	B
level	B
of	O
community	O
size	O
of	O
houses	O
and	O
so	O
forth	O
in	O
this	O
case	O
one	O
might	O
be	O
interested	O
in	O
how	O
the	O
individual	O
input	B
variables	O
affect	O
the	O
prices	O
that	O
is	O
how	O
much	O
extra	O
will	O
a	O
house	O
be	O
worth	O
if	O
it	O
has	O
a	O
view	O
of	O
the	O
river	O
this	O
is	O
an	O
inference	B
problem	O
alternatively	O
one	O
may	O
simply	O
be	O
interested	O
in	O
predicting	O
the	O
value	O
of	O
a	O
home	O
given	O
its	O
characteristics	O
is	O
this	O
house	O
under-	O
or	O
over-valued	O
this	O
is	O
a	O
prediction	B
problem	O
depending	O
on	O
whether	O
our	O
ultimate	O
goal	O
is	O
prediction	B
inference	B
or	O
a	O
combination	O
of	O
the	O
two	O
different	O
methods	O
for	O
estimating	O
f	O
may	O
be	O
appropriate	O
for	O
example	O
linear	B
models	O
allow	O
for	O
relatively	O
simple	B
and	O
interpretable	O
inference	B
but	O
may	O
not	O
yield	O
as	O
accurate	O
predictions	O
as	O
some	O
other	O
approaches	O
in	O
contrast	B
some	O
of	O
the	O
highly	O
non-linear	B
approaches	O
that	O
we	O
discuss	O
in	O
the	O
later	O
chapters	O
of	O
this	O
book	O
can	O
potentially	O
provide	O
quite	O
accurate	O
predictions	O
for	O
y	O
but	O
this	O
comes	O
at	O
the	O
expense	O
of	O
a	O
less	O
interpretable	O
model	B
for	O
which	O
inference	B
is	O
more	O
challenging	O
linear	B
model	B
what	O
is	O
statistical	O
learning	O
how	O
do	O
we	O
estimate	O
f	O
throughout	O
this	O
book	O
we	O
explore	O
many	O
linear	B
and	O
non-linear	B
approaches	O
for	O
estimating	O
f	O
however	O
these	O
methods	O
generally	O
share	O
certain	O
characteristics	O
we	O
provide	O
an	O
overview	O
of	O
these	O
shared	O
characteristics	O
in	O
this	O
section	O
we	O
will	O
always	O
assume	O
that	O
we	O
have	O
observed	O
a	O
set	B
of	O
n	O
different	O
data	B
points	O
for	O
example	O
in	O
figure	O
we	O
observed	O
n	O
data	B
points	O
these	O
observations	B
are	O
called	O
the	O
training	O
data	B
because	O
we	O
will	O
use	O
these	O
observations	B
to	O
train	B
or	O
teach	O
our	O
method	O
how	O
to	O
estimate	O
f	O
let	O
xij	O
represent	O
the	O
value	O
of	O
the	O
jth	O
predictor	B
or	O
input	B
for	O
observation	O
i	O
where	O
i	O
n	O
and	O
j	O
p	O
correspondingly	O
let	O
yi	O
represent	O
the	O
response	B
variable	B
for	O
the	O
ith	O
observation	O
then	O
our	O
training	O
data	B
consist	O
of	O
yn	O
where	O
xi	O
xipt	O
our	O
goal	O
is	O
to	O
apply	O
a	O
statistical	O
learning	O
method	O
to	O
the	O
training	O
data	B
in	O
order	O
to	O
estimate	O
the	O
unknown	O
function	B
f	O
in	O
other	O
words	O
we	O
want	O
to	O
find	O
a	O
function	B
f	O
such	O
that	O
y	O
f	O
for	O
any	O
observation	O
y	O
broadly	O
speaking	O
most	O
statistical	O
learning	O
methods	O
for	O
this	O
task	O
can	O
be	O
characterized	O
as	O
either	O
parametric	B
or	O
non-parametric	B
we	O
now	O
briefly	O
discuss	O
these	O
two	O
types	O
of	O
approaches	O
parametric	B
methods	O
parametric	B
methods	O
involve	O
a	O
two-step	O
model-based	O
approach	B
first	O
we	O
make	O
an	O
assumption	O
about	O
the	O
functional	O
form	O
or	O
shape	O
of	O
f	O
for	O
example	O
one	O
very	O
simple	B
assumption	O
is	O
that	O
f	O
is	O
linear	B
in	O
x	O
f	O
pxp	O
this	O
is	O
a	O
linear	B
model	B
which	O
will	O
be	O
discussed	O
extensively	O
in	O
chapter	O
once	O
we	O
have	O
assumed	O
that	O
f	O
is	O
linear	B
the	O
problem	O
of	O
estimating	O
f	O
is	O
greatly	O
simplified	O
instead	O
of	O
having	O
to	O
estimate	O
an	O
entirely	O
arbitrary	O
p-dimensional	O
function	B
f	O
one	O
only	O
needs	O
to	O
estimate	O
the	O
p	O
coefficients	O
p	O
training	O
data	B
parametric	B
nonparametric	O
after	O
a	O
model	B
has	O
been	O
selected	O
we	O
need	O
a	O
procedure	O
that	O
uses	O
the	O
training	O
data	B
to	O
fit	O
or	O
train	B
the	O
model	B
in	O
the	O
case	O
of	O
the	O
linear	B
model	B
we	O
need	O
to	O
estimate	O
the	O
parameters	O
p	O
that	O
is	O
we	O
want	O
to	O
find	O
values	O
of	O
these	O
parameters	O
such	O
that	O
y	O
pxp	O
fit	O
train	B
the	O
most	O
common	O
approach	B
to	O
fitting	O
the	O
model	B
is	O
referred	O
to	O
as	O
least	B
squares	I
which	O
we	O
discuss	O
in	O
chapter	O
however	O
least	B
squares	I
is	O
one	O
of	O
many	O
possible	O
ways	O
to	O
fit	O
the	O
linear	B
model	B
in	O
chapter	O
we	O
discuss	O
other	O
approaches	O
for	O
estimating	O
the	O
parameters	O
in	O
least	B
squares	I
the	O
model-based	O
approach	B
just	O
described	O
is	O
referred	O
to	O
as	O
parametric	B
it	O
reduces	O
the	O
problem	O
of	O
estimating	O
f	O
down	O
to	O
one	O
of	O
estimating	O
a	O
set	B
of	O
statistical	O
learning	O
i	O
n	O
c	O
o	O
m	O
e	O
ye	O
ars	O
of	O
e	O
d	O
ucatio	O
n	O
seniority	O
figure	O
a	O
linear	B
model	B
fit	O
by	O
least	B
squares	I
to	O
the	O
income	B
data	B
from	O
figure	O
the	O
observations	B
are	O
shown	O
in	O
red	O
and	O
the	O
yellow	O
plane	O
indicates	O
the	O
least	B
squares	I
fit	O
to	O
the	O
data	B
parameters	O
assuming	O
a	O
parametric	B
form	O
for	O
f	O
simplifies	O
the	O
problem	O
of	O
estimating	O
f	O
because	O
it	O
is	O
generally	O
much	O
easier	O
to	O
estimate	O
a	O
set	B
of	O
parameters	O
such	O
as	O
p	O
in	O
the	O
linear	B
model	B
than	O
it	O
is	O
to	O
fit	O
an	O
entirely	O
arbitrary	O
function	B
f	O
the	O
potential	O
disadvantage	O
of	O
a	O
parametric	B
approach	B
is	O
that	O
the	O
model	B
we	O
choose	O
will	O
usually	O
not	O
match	O
the	O
true	O
unknown	O
form	O
of	O
f	O
if	O
the	O
chosen	O
model	B
is	O
too	O
far	O
from	O
the	O
true	O
f	O
then	O
our	O
estimate	O
will	O
be	O
poor	O
we	O
can	O
try	O
to	O
address	O
this	O
problem	O
by	O
choosing	O
flexible	O
models	O
that	O
can	O
fit	O
many	O
different	O
possible	O
functional	O
forms	O
for	O
f	O
but	O
in	O
general	O
fitting	O
a	O
more	O
flexible	O
model	B
requires	O
estimating	O
a	O
greater	O
number	O
of	O
parameters	O
these	O
more	O
complex	O
models	O
can	O
lead	O
to	O
a	O
phenomenon	O
known	O
as	O
overfitting	B
the	O
data	B
which	O
essentially	O
means	O
they	O
follow	O
the	O
errors	O
or	O
noise	B
too	O
closely	O
these	O
issues	O
are	O
discussed	O
throughout	O
this	O
book	O
figure	O
shows	O
an	O
example	O
of	O
the	O
parametric	B
approach	B
applied	O
to	O
the	O
income	B
data	B
from	O
figure	O
we	O
have	O
fit	O
a	O
linear	B
model	B
of	O
the	O
form	O
income	B
education	O
seniority	O
since	O
we	O
have	O
assumed	O
a	O
linear	B
relationship	O
between	O
the	O
response	B
and	O
the	O
two	O
predictors	O
the	O
entire	O
fitting	O
problem	O
reduces	O
to	O
estimating	O
and	O
which	O
we	O
do	O
using	O
least	B
squares	I
linear	B
regression	B
comparing	O
figure	O
to	O
figure	O
we	O
can	O
see	O
that	O
the	O
linear	B
fit	O
given	O
in	O
figure	O
is	O
not	O
quite	O
right	O
the	O
true	O
f	O
has	O
some	O
curvature	O
that	O
is	O
not	O
captured	O
in	O
the	O
linear	B
fit	O
however	O
the	O
linear	B
fit	O
still	O
appears	O
to	O
do	O
a	O
reasonable	O
job	O
of	O
capturing	O
the	O
positive	O
relationship	O
between	O
years	O
of	O
education	O
and	O
income	B
as	O
well	O
as	O
the	O
flexible	O
overfitting	B
noise	B
what	O
is	O
statistical	O
learning	O
seniority	O
i	O
n	O
c	O
o	O
m	O
e	O
ars	O
of	O
e	O
d	O
ye	O
ucatio	O
n	O
figure	O
a	O
smooth	O
thin-plate	B
spline	B
fit	O
to	O
the	O
income	B
data	B
from	O
figure	O
is	O
shown	O
in	O
yellow	O
the	O
observations	B
are	O
displayed	O
in	O
red	O
splines	O
are	O
discussed	O
in	O
chapter	O
slightly	O
less	O
positive	O
relationship	O
between	O
seniority	O
and	O
income	B
it	O
may	O
be	O
that	O
with	O
such	O
a	O
small	O
number	O
of	O
observations	B
this	O
is	O
the	O
best	O
we	O
can	O
do	O
non-parametric	B
methods	O
non-parametric	B
methods	O
do	O
not	O
make	O
explicit	O
assumptions	O
about	O
the	O
functional	O
form	O
of	O
f	O
instead	O
they	O
seek	O
an	O
estimate	O
of	O
f	O
that	O
gets	O
as	O
close	O
to	O
the	O
data	B
points	O
as	O
possible	O
without	O
being	O
too	O
rough	O
or	O
wiggly	O
such	O
approaches	O
can	O
have	O
a	O
major	O
advantage	O
over	O
parametric	B
approaches	O
by	O
avoiding	O
the	O
assumption	O
of	O
a	O
particular	O
functional	O
form	O
for	O
f	O
they	O
have	O
the	O
potential	O
to	O
accurately	O
fit	O
a	O
wider	O
range	O
of	O
possible	O
shapes	O
for	O
f	O
any	O
parametric	B
approach	B
brings	O
with	O
it	O
the	O
possibility	O
that	O
the	O
functional	O
form	O
used	O
to	O
estimate	O
f	O
is	O
very	O
different	O
from	O
the	O
true	O
f	O
in	O
which	O
case	O
the	O
resulting	O
model	B
will	O
not	O
fit	O
the	O
data	B
well	O
in	O
contrast	B
non-parametric	B
approaches	O
completely	O
avoid	O
this	O
danger	O
since	O
essentially	O
no	O
assumption	O
about	O
the	O
form	O
of	O
f	O
is	O
made	O
but	O
non-parametric	B
approaches	O
do	O
suffer	O
from	O
a	O
major	O
disadvantage	O
since	O
they	O
do	O
not	O
reduce	O
the	O
problem	O
of	O
estimating	O
f	O
to	O
a	O
small	O
number	O
of	O
parameters	O
a	O
very	O
large	O
number	O
of	O
observations	B
more	O
than	O
is	O
typically	O
needed	O
for	O
a	O
parametric	B
approach	B
is	O
required	O
in	O
order	O
to	O
obtain	O
an	O
accurate	O
estimate	O
for	O
f	O
an	O
example	O
of	O
a	O
non-parametric	B
approach	B
to	O
fitting	O
the	O
income	B
data	B
is	O
shown	O
in	O
figure	O
a	O
thin-plate	B
spline	B
is	O
used	O
to	O
estimate	O
f	O
this	O
approach	B
does	O
not	O
impose	O
any	O
pre-specified	O
model	B
on	O
f	O
it	O
instead	O
attempts	O
to	O
produce	O
an	O
estimate	O
for	O
f	O
that	O
is	O
as	O
close	O
as	O
possible	O
to	O
the	O
observed	O
data	B
subject	O
to	O
the	O
fit	O
that	O
is	O
the	O
yellow	O
surface	O
in	O
figure	O
being	O
thin-plate	B
spline	B
statistical	O
learning	O
i	O
n	O
c	O
o	O
m	O
e	O
ye	O
ars	O
of	O
e	O
d	O
ucatio	O
n	O
seniority	O
figure	O
a	O
rough	O
thin-plate	B
spline	B
fit	O
to	O
the	O
income	B
data	B
from	O
figure	O
this	O
fit	O
makes	O
zero	O
errors	O
on	O
the	O
training	O
data	B
smooth	O
in	O
this	O
case	O
the	O
non-parametric	B
fit	O
has	O
produced	O
a	O
remarkably	O
accurate	O
estimate	O
of	O
the	O
true	O
f	O
shown	O
in	O
figure	O
in	O
order	O
to	O
fit	O
a	O
thin-plate	B
spline	B
the	O
data	B
analyst	O
must	O
select	O
a	O
level	B
of	O
smoothness	O
figure	O
shows	O
the	O
same	O
thin-plate	B
spline	B
fit	O
using	O
a	O
lower	O
level	B
of	O
smoothness	O
allowing	O
for	O
a	O
rougher	O
fit	O
the	O
resulting	O
estimate	O
fits	O
the	O
observed	O
data	B
perfectly	O
however	O
the	O
spline	B
fit	O
shown	O
in	O
figure	O
is	O
far	O
more	O
variable	B
than	O
the	O
true	O
function	B
f	O
from	O
figure	O
this	O
is	O
an	O
example	O
of	O
overfitting	B
the	O
data	B
which	O
we	O
discussed	O
previously	O
it	O
is	O
an	O
undesirable	O
situation	O
because	O
the	O
fit	O
obtained	O
will	O
not	O
yield	O
accurate	O
estimates	O
of	O
the	O
response	B
on	O
new	O
observations	B
that	O
were	O
not	O
part	O
of	O
the	O
original	O
training	O
data	B
set	B
we	O
discuss	O
methods	O
for	O
choosing	O
the	O
correct	O
amount	O
of	O
smoothness	O
in	O
chapter	O
splines	O
are	O
discussed	O
in	O
chapter	O
as	O
we	O
have	O
seen	O
there	O
are	O
advantages	O
and	O
disadvantages	O
to	O
parametric	B
and	O
non-parametric	B
methods	O
for	O
statistical	O
learning	O
we	O
explore	O
both	O
types	O
of	O
methods	O
throughout	O
this	O
book	O
the	O
trade-off	B
between	O
prediction	B
accuracy	O
and	O
model	B
interpretability	B
of	O
the	O
many	O
methods	O
that	O
we	O
examine	O
in	O
this	O
book	O
some	O
are	O
less	O
flexible	O
or	O
more	O
restrictive	O
in	O
the	O
sense	O
that	O
they	O
can	O
produce	O
just	O
a	O
relatively	O
small	O
range	O
of	O
shapes	O
to	O
estimate	O
f	O
for	O
example	O
linear	B
regression	B
is	O
a	O
relatively	O
inflexible	O
approach	B
because	O
it	O
can	O
only	O
generate	O
linear	B
functions	O
such	O
as	O
the	O
lines	O
shown	O
in	O
figure	O
or	O
the	O
plane	O
shown	O
in	O
figure	O
what	O
is	O
statistical	O
learning	O
h	O
g	O
h	O
i	O
subset	B
selection	B
lasso	B
y	O
t	O
i	O
l	O
i	O
b	O
a	O
t	O
e	O
r	O
p	O
r	O
e	O
t	O
n	O
i	O
w	O
o	O
l	O
low	O
least	B
squares	I
generalized	O
additive	B
models	O
trees	O
bagging	B
boosting	B
support	B
vector	B
machines	O
flexibility	O
high	O
figure	O
a	O
representation	O
of	O
the	O
tradeoff	O
between	O
flexibility	O
and	O
interpretability	B
using	O
different	O
statistical	O
learning	O
methods	O
in	O
general	O
as	O
the	O
flexibility	O
of	O
a	O
method	O
increases	O
its	O
interpretability	B
decreases	O
other	O
methods	O
such	O
as	O
the	O
thin	O
plate	O
splines	O
shown	O
in	O
figures	O
and	O
are	O
considerably	O
more	O
flexible	O
because	O
they	O
can	O
generate	O
a	O
much	O
wider	O
range	O
of	O
possible	O
shapes	O
to	O
estimate	O
f	O
one	O
might	O
reasonably	O
ask	O
the	O
following	O
question	O
why	O
would	O
we	O
ever	O
choose	O
to	O
use	O
a	O
more	O
restrictive	O
method	O
instead	O
of	O
a	O
very	O
flexible	O
approach	B
there	O
are	O
several	O
reasons	O
that	O
we	O
might	O
prefer	O
a	O
more	O
restrictive	O
model	B
if	O
we	O
are	O
mainly	O
interested	O
in	O
inference	B
then	O
restrictive	O
models	O
are	O
much	O
more	O
interpretable	O
for	O
instance	O
when	O
inference	B
is	O
the	O
goal	O
the	O
linear	B
model	B
may	O
be	O
a	O
good	O
choice	O
since	O
it	O
will	O
be	O
quite	O
easy	O
to	O
understand	O
the	O
relationship	O
between	O
y	O
and	O
xp	O
in	O
contrast	B
very	O
flexible	O
approaches	O
such	O
as	O
the	O
splines	O
discussed	O
in	O
chapter	O
and	O
displayed	O
in	O
figures	O
and	O
and	O
the	O
boosting	B
methods	O
discussed	O
in	O
chapter	O
can	O
lead	O
to	O
such	O
complicated	O
estimates	O
of	O
f	O
that	O
it	O
is	O
difficult	O
to	O
understand	O
how	O
any	O
individual	O
predictor	B
is	O
associated	O
with	O
the	O
response	B
figure	O
provides	O
an	O
illustration	O
of	O
the	O
trade-off	B
between	O
flexibility	O
and	O
interpretability	B
for	O
some	O
of	O
the	O
methods	O
that	O
we	O
cover	O
in	O
this	O
book	O
least	B
squares	I
linear	B
regression	B
discussed	O
in	O
chapter	O
is	O
relatively	O
inflexible	O
but	O
is	O
quite	O
interpretable	O
the	O
lasso	B
discussed	O
in	O
chapter	O
relies	O
upon	O
the	O
linear	B
model	B
but	O
uses	O
an	O
alternative	O
fitting	O
procedure	O
for	O
estimating	O
the	O
coefficients	O
p	O
the	O
new	O
procedure	O
is	O
more	O
restrictive	O
in	O
estimating	O
the	O
coefficients	O
and	O
sets	O
a	O
number	O
of	O
them	O
to	O
exactly	O
zero	O
hence	O
in	O
this	O
sense	O
the	O
lasso	B
is	O
a	O
less	O
flexible	O
approach	B
than	O
linear	B
regression	B
it	O
is	O
also	O
more	O
interpretable	O
than	O
linear	B
regression	B
because	O
in	O
the	O
final	O
model	B
the	O
response	B
variable	B
will	O
only	O
be	O
related	O
to	O
a	O
small	O
subset	O
of	O
the	O
predictors	O
namely	O
those	O
with	O
nonzero	O
coefficient	O
estimates	O
generalized	O
lasso	B
generalized	B
additive	B
model	B
bagging	B
boosting	B
support	B
vector	B
machine	B
statistical	O
learning	O
additive	B
models	O
discussed	O
in	O
chapter	O
instead	O
extend	O
the	O
linear	B
model	B
to	O
allow	O
for	O
certain	O
non-linear	B
relationships	O
consequently	O
gams	O
are	O
more	O
flexible	O
than	O
linear	B
regression	B
they	O
are	O
also	O
somewhat	O
less	O
interpretable	O
than	O
linear	B
regression	B
because	O
the	O
relationship	O
between	O
each	O
predictor	B
and	O
the	O
response	B
is	O
now	O
modeled	O
using	O
a	O
curve	O
finally	O
fully	O
non-linear	B
methods	O
such	O
as	O
bagging	B
boosting	B
and	O
support	B
vector	B
machines	O
with	O
non-linear	B
kernels	O
discussed	O
in	O
chapters	O
and	O
are	O
highly	O
flexible	O
approaches	O
that	O
are	O
harder	O
to	O
interpret	O
we	O
have	O
established	O
that	O
when	O
inference	B
is	O
the	O
goal	O
there	O
are	O
clear	O
advantages	O
to	O
using	O
simple	B
and	O
relatively	O
inflexible	O
statistical	O
learning	O
methods	O
in	O
some	O
settings	O
however	O
we	O
are	O
only	O
interested	O
in	O
prediction	B
and	O
the	O
interpretability	B
of	O
the	O
predictive	O
model	B
is	O
simply	O
not	O
of	O
interest	O
for	O
instance	O
if	O
we	O
seek	O
to	O
develop	O
an	O
algorithm	O
to	O
predict	O
the	O
price	O
of	O
a	O
stock	O
our	O
sole	O
requirement	O
for	O
the	O
algorithm	O
is	O
that	O
it	O
predict	O
accurately	O
interpretability	B
is	O
not	O
a	O
concern	O
in	O
this	O
setting	O
we	O
might	O
expect	O
that	O
it	O
will	O
be	O
best	O
to	O
use	O
the	O
most	O
flexible	O
model	B
available	O
surprisingly	O
this	O
is	O
not	O
always	O
the	O
case	O
we	O
will	O
often	O
obtain	O
more	O
accurate	O
predictions	O
using	O
a	O
less	O
flexible	O
method	O
this	O
phenomenon	O
which	O
may	O
seem	O
counterintuitive	O
at	O
first	O
glance	O
has	O
to	O
do	O
with	O
the	O
potential	O
for	O
overfitting	B
in	O
highly	O
flexible	O
methods	O
we	O
saw	O
an	O
example	O
of	O
overfitting	B
in	O
figure	O
we	O
will	O
discuss	O
this	O
very	O
important	O
concept	O
further	O
in	O
section	O
and	O
throughout	O
this	O
book	O
supervised	O
versus	O
unsupervised	B
learning	I
most	O
statistical	O
learning	O
problems	O
fall	O
into	O
one	O
of	O
two	O
categories	O
supervised	O
or	O
unsupervised	O
the	O
examples	O
that	O
we	O
have	O
discussed	O
so	O
far	O
in	O
this	O
chapter	O
all	O
fall	O
into	O
the	O
supervised	B
learning	I
domain	O
for	O
each	O
observation	O
of	O
the	O
predictor	B
measurements	O
xi	O
i	O
n	O
there	O
is	O
an	O
associated	O
response	B
measurement	O
yi	O
we	O
wish	O
to	O
fit	O
a	O
model	B
that	O
relates	O
the	O
response	B
to	O
the	O
predictors	O
with	O
the	O
aim	O
of	O
accurately	O
predicting	O
the	O
response	B
for	O
future	O
observations	B
or	O
better	O
understanding	O
the	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictors	O
many	O
classical	O
statistical	O
learning	O
methods	O
such	O
as	O
linear	B
regression	B
and	O
logistic	B
regression	B
as	O
well	O
as	O
more	O
modern	O
approaches	O
such	O
as	O
gam	O
boosting	B
and	O
support	B
vector	B
machines	O
operate	O
in	O
the	O
supervised	B
learning	I
domain	O
the	O
vast	O
majority	O
of	O
this	O
book	O
is	O
devoted	O
to	O
this	O
setting	O
in	O
contrast	B
unsupervised	B
learning	I
describes	O
the	O
somewhat	O
more	O
challenging	O
situation	O
in	O
which	O
for	O
every	O
observation	O
i	O
n	O
we	O
observe	O
a	O
vector	B
of	O
measurements	O
xi	O
but	O
no	O
associated	O
response	B
yi	O
it	O
is	O
not	O
possible	O
to	O
fit	O
a	O
linear	B
regression	B
model	B
since	O
there	O
is	O
no	O
response	B
variable	B
to	O
predict	O
in	O
this	O
setting	O
we	O
are	O
in	O
some	O
sense	O
working	O
blind	O
the	O
situation	O
is	O
referred	O
to	O
as	O
unsupervised	O
because	O
we	O
lack	O
a	O
response	B
variable	B
that	O
can	O
supervise	O
our	O
analysis	B
what	O
sort	O
of	O
statistical	O
analysis	B
is	O
supervised	O
unsupervised	O
logistic	B
regression	B
what	O
is	O
statistical	O
learning	O
figure	O
a	O
clustering	B
data	B
set	B
involving	O
three	O
groups	O
each	O
group	O
is	O
shown	O
using	O
a	O
different	O
colored	O
symbol	O
left	O
the	O
three	O
groups	O
are	O
well-separated	O
in	O
this	O
setting	O
a	O
clustering	B
approach	B
should	O
successfully	O
identify	O
the	O
three	O
groups	O
right	O
there	O
is	O
some	O
overlap	O
among	O
the	O
groups	O
now	O
the	O
clustering	B
task	O
is	O
more	O
challenging	O
possible	O
we	O
can	O
seek	O
to	O
understand	O
the	O
relationships	O
between	O
the	O
variables	O
or	O
between	O
the	O
observations	B
one	O
statistical	O
learning	O
tool	O
that	O
we	O
may	O
use	O
in	O
this	O
setting	O
is	O
cluster	B
analysis	B
or	O
clustering	B
the	O
goal	O
of	O
cluster	B
analysis	B
is	O
to	O
ascertain	O
on	O
the	O
basis	B
of	O
xn	O
whether	O
the	O
observations	B
fall	O
into	O
relatively	O
distinct	O
groups	O
for	O
example	O
in	O
a	O
market	O
segmentation	O
study	O
we	O
might	O
observe	O
multiple	B
characteristics	O
for	O
potential	O
customers	O
such	O
as	O
zip	O
code	O
family	O
income	B
and	O
shopping	O
habits	O
we	O
might	O
believe	O
that	O
the	O
customers	O
fall	O
into	O
different	O
groups	O
such	O
as	O
big	O
spenders	O
versus	O
low	O
spenders	O
if	O
the	O
information	O
about	O
each	O
customer	O
s	O
spending	O
patterns	O
were	O
available	O
then	O
a	O
supervised	O
analysis	B
would	O
be	O
possible	O
however	O
this	O
information	O
is	O
not	O
available	O
that	O
is	O
we	O
do	O
not	O
know	O
whether	O
each	O
potential	O
customer	O
is	O
a	O
big	O
spender	O
or	O
not	O
in	O
this	O
setting	O
we	O
can	O
try	O
to	O
cluster	O
the	O
customers	O
on	O
the	O
basis	B
of	O
the	O
variables	O
measured	O
in	O
order	O
to	O
identify	O
distinct	O
groups	O
of	O
potential	O
customers	O
identifying	O
such	O
groups	O
can	O
be	O
of	O
interest	O
because	O
it	O
might	O
be	O
that	O
the	O
groups	O
differ	O
with	O
respect	O
to	O
some	O
property	O
of	O
interest	O
such	O
as	O
spending	O
habits	O
figure	O
provides	O
a	O
simple	B
illustration	O
of	O
the	O
clustering	B
problem	O
we	O
have	O
plotted	O
observations	B
with	O
measurements	O
on	O
two	O
variables	O
and	O
each	O
observation	O
corresponds	O
to	O
one	O
of	O
three	O
distinct	O
groups	O
for	O
illustrative	O
purposes	O
we	O
have	O
plotted	O
the	O
members	O
of	O
each	O
group	O
using	O
different	O
colors	O
and	O
symbols	O
however	O
in	O
practice	O
the	O
group	O
memberships	O
are	O
unknown	O
and	O
the	O
goal	O
is	O
to	O
determine	O
the	O
group	O
to	O
which	O
each	O
observation	O
belongs	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
this	O
is	O
a	O
relatively	O
easy	O
task	O
because	O
the	O
groups	O
are	O
well-separated	O
in	O
contrast	B
the	O
right-hand	O
panel	O
illustrates	O
a	O
more	O
challenging	O
problem	O
in	O
which	O
there	O
is	O
some	O
overlap	O
cluster	B
analysis	B
statistical	O
learning	O
between	O
the	O
groups	O
a	O
clustering	B
method	O
could	O
not	O
be	O
expected	O
to	O
assign	O
all	O
of	O
the	O
overlapping	O
points	O
to	O
their	O
correct	O
group	O
green	O
or	O
orange	O
in	O
the	O
examples	O
shown	O
in	O
figure	O
there	O
are	O
only	O
two	O
variables	O
and	O
so	O
one	O
can	O
simply	O
visually	O
inspect	O
the	O
scatterplots	O
of	O
the	O
observations	B
in	O
order	O
to	O
identify	O
clusters	O
however	O
in	O
practice	O
we	O
often	O
encounter	O
data	B
sets	O
that	O
contain	O
many	O
more	O
than	O
two	O
variables	O
in	O
this	O
case	O
we	O
cannot	O
easily	O
plot	B
the	O
observations	B
for	O
instance	O
if	O
there	O
are	O
p	O
variables	O
in	O
our	O
data	B
set	B
then	O
pp	O
distinct	O
scatterplots	O
can	O
be	O
made	O
and	O
visual	O
inspection	O
is	O
simply	O
not	O
a	O
viable	O
way	O
to	O
identify	O
clusters	O
for	O
this	O
reason	O
automated	O
clustering	B
methods	O
are	O
important	O
we	O
discuss	O
clustering	B
and	O
other	O
unsupervised	B
learning	I
approaches	O
in	O
chapter	O
many	O
problems	O
fall	O
naturally	O
into	O
the	O
supervised	O
or	O
unsupervised	B
learning	I
paradigms	O
however	O
sometimes	O
the	O
question	O
of	O
whether	O
an	O
analysis	B
should	O
be	O
considered	O
supervised	O
or	O
unsupervised	O
is	O
less	O
clear-cut	O
for	O
instance	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
n	O
observations	B
for	O
m	O
of	O
the	O
observations	B
where	O
m	O
n	O
we	O
have	O
both	O
predictor	B
measurements	O
and	O
a	O
response	B
measurement	O
for	O
the	O
remaining	O
n	O
m	O
observations	B
we	O
have	O
predictor	B
measurements	O
but	O
no	O
response	B
measurement	O
such	O
a	O
scenario	O
can	O
arise	O
if	O
the	O
predictors	O
can	O
be	O
measured	O
relatively	O
cheaply	O
but	O
the	O
corresponding	O
responses	O
are	O
much	O
more	O
expensive	O
to	O
collect	O
we	O
refer	O
to	O
this	O
setting	O
as	O
a	O
semi-supervised	B
learning	I
problem	O
in	O
this	O
setting	O
we	O
wish	O
to	O
use	O
a	O
statistical	O
learning	O
method	O
that	O
can	O
incorporate	O
the	O
m	O
observations	B
for	O
which	O
response	B
measurements	O
are	O
available	O
as	O
well	O
as	O
the	O
n	O
m	O
observations	B
for	O
which	O
they	O
are	O
not	O
although	O
this	O
is	O
an	O
interesting	O
topic	O
it	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
semisupervised	O
learning	O
regression	B
versus	O
classification	B
problems	O
variables	O
can	O
be	O
characterized	O
as	O
either	O
quantitative	B
or	O
qualitative	B
known	O
as	O
categorical	B
quantitative	B
variables	O
take	O
on	O
numerical	O
values	O
examples	O
include	O
a	O
person	O
s	O
age	O
height	O
or	O
income	B
the	O
value	O
of	O
a	O
house	O
and	O
the	O
price	O
of	O
a	O
stock	O
in	O
contrast	B
qualitative	B
variables	O
take	O
on	O
values	O
in	O
one	O
of	O
k	O
different	O
classes	O
or	O
categories	O
examples	O
of	O
qualitative	B
variables	O
include	O
a	O
person	O
s	O
gender	O
or	O
female	O
the	O
brand	O
of	O
product	O
purchased	O
a	O
b	O
or	O
c	O
whether	O
a	O
person	O
defaults	O
on	O
a	O
debt	O
or	O
no	O
or	O
a	O
cancer	O
diagnosis	O
myelogenous	O
leukemia	O
acute	O
lymphoblastic	O
leukemia	O
or	O
no	O
leukemia	O
we	O
tend	O
to	O
refer	O
to	O
problems	O
with	O
a	O
quantitative	B
response	B
as	O
regression	B
problems	O
while	O
those	O
involving	O
a	O
qualitative	B
response	B
are	O
often	O
referred	O
to	O
as	O
classification	B
problems	O
however	O
the	O
distinction	O
is	O
not	O
always	O
that	O
crisp	O
least	B
squares	I
linear	B
regression	B
is	O
used	O
with	O
a	O
quantitative	B
response	B
whereas	O
logistic	B
regression	B
is	O
typically	O
used	O
with	O
a	O
qualitative	B
or	O
binary	B
response	B
as	O
such	O
it	O
is	O
often	O
used	O
as	O
a	O
classification	B
method	O
but	O
since	O
it	O
estimates	O
class	O
probabilities	O
it	O
can	O
be	O
thought	O
of	O
as	O
a	O
regression	B
quantitative	B
qualitative	B
categorical	B
class	O
regression	B
classification	B
binary	B
assessing	O
model	B
accuracy	O
method	O
as	O
well	O
some	O
statistical	O
methods	O
such	O
as	O
k-nearest	O
neighbors	O
and	O
and	O
boosting	B
can	O
be	O
used	O
in	O
the	O
case	O
of	O
either	O
quantitative	B
or	O
qualitative	B
responses	O
we	O
tend	O
to	O
select	O
statistical	O
learning	O
methods	O
on	O
the	O
basis	B
of	O
whether	O
the	O
response	B
is	O
quantitative	B
or	O
qualitative	B
i	O
e	O
we	O
might	O
use	O
linear	B
regression	B
when	O
quantitative	B
and	O
logistic	B
regression	B
when	O
qualitative	B
however	O
whether	O
the	O
predictors	O
are	O
qualitative	B
or	O
quantitative	B
is	O
generally	O
considered	O
less	O
important	O
most	O
of	O
the	O
statistical	O
learning	O
methods	O
discussed	O
in	O
this	O
book	O
can	O
be	O
applied	O
regardless	O
of	O
the	O
predictor	B
variable	B
type	O
provided	O
that	O
any	O
qualitative	B
predictors	O
are	O
properly	O
coded	O
before	O
the	O
analysis	B
is	O
performed	O
this	O
is	O
discussed	O
in	O
chapter	O
assessing	O
model	B
accuracy	O
one	O
of	O
the	O
key	O
aims	O
of	O
this	O
book	O
is	O
to	O
introduce	O
the	O
reader	O
to	O
a	O
wide	O
range	O
of	O
statistical	O
learning	O
methods	O
that	O
extend	O
far	O
beyond	O
the	O
standard	O
linear	B
regression	B
approach	B
why	O
is	O
it	O
necessary	O
to	O
introduce	O
so	O
many	O
different	O
statistical	O
learning	O
approaches	O
rather	O
than	O
just	O
a	O
single	B
best	O
method	O
there	O
is	O
no	O
free	O
lunch	O
in	O
statistics	O
no	O
one	O
method	O
dominates	O
all	O
others	O
over	O
all	O
possible	O
data	B
sets	O
on	O
a	O
particular	O
data	B
set	B
one	O
specific	O
method	O
may	O
work	O
best	O
but	O
some	O
other	O
method	O
may	O
work	O
better	O
on	O
a	O
similar	O
but	O
different	O
data	B
set	B
hence	O
it	O
is	O
an	O
important	O
task	O
to	O
decide	O
for	O
any	O
given	O
set	B
of	O
data	B
which	O
method	O
produces	O
the	O
best	O
results	O
selecting	O
the	O
best	O
approach	B
can	O
be	O
one	O
of	O
the	O
most	O
challenging	O
parts	O
of	O
performing	O
statistical	O
learning	O
in	O
practice	O
in	O
this	O
section	O
we	O
discuss	O
some	O
of	O
the	O
most	O
important	O
concepts	O
that	O
arise	O
in	O
selecting	O
a	O
statistical	O
learning	O
procedure	O
for	O
a	O
specific	O
data	B
set	B
as	O
the	O
book	O
progresses	O
we	O
will	O
explain	O
how	O
the	O
concepts	O
presented	O
here	O
can	O
be	O
applied	O
in	O
practice	O
measuring	O
the	O
quality	O
of	O
fit	O
in	O
order	O
to	O
evaluate	O
the	O
performance	O
of	O
a	O
statistical	O
learning	O
method	O
on	O
a	O
given	O
data	B
set	B
we	O
need	O
some	O
way	O
to	O
measure	O
how	O
well	O
its	O
predictions	O
actually	O
match	O
the	O
observed	O
data	B
that	O
is	O
we	O
need	O
to	O
quantify	O
the	O
extent	O
to	O
which	O
the	O
predicted	O
response	B
value	O
for	O
a	O
given	O
observation	O
is	O
close	O
to	O
the	O
true	O
response	B
value	O
for	O
that	O
observation	O
in	O
the	O
regression	B
setting	O
the	O
most	O
commonly-used	O
measure	O
is	O
the	O
mean	B
squared	I
error	B
given	O
by	O
mean	O
m	O
se	O
n	O
f	O
squared	O
error	B
statistical	O
learning	O
where	O
f	O
is	O
the	O
prediction	B
that	O
f	O
gives	O
for	O
the	O
ith	O
observation	O
the	O
mse	B
will	O
be	O
small	O
if	O
the	O
predicted	O
responses	O
are	O
very	O
close	O
to	O
the	O
true	O
responses	O
and	O
will	O
be	O
large	O
if	O
for	O
some	O
of	O
the	O
observations	B
the	O
predicted	O
and	O
true	O
responses	O
differ	O
substantially	O
the	O
mse	B
in	O
is	O
computed	O
using	O
the	O
training	O
data	B
that	O
was	O
used	O
to	O
fit	O
the	O
model	B
and	O
so	O
should	O
more	O
accurately	O
be	O
referred	O
to	O
as	O
the	O
training	O
mse	B
but	O
in	O
general	O
we	O
do	O
not	O
really	O
care	O
how	O
well	O
the	O
method	O
works	O
on	O
the	O
training	O
data	B
rather	O
we	O
are	O
interested	O
in	O
the	O
accuracy	O
of	O
the	O
predictions	O
that	O
we	O
obtain	O
when	O
we	O
apply	O
our	O
method	O
to	O
previously	O
unseen	O
test	O
data	B
why	O
is	O
this	O
what	O
we	O
care	O
about	O
suppose	O
that	O
we	O
are	O
interested	O
in	O
developing	O
an	O
algorithm	O
to	O
predict	O
a	O
stock	O
s	O
price	O
based	O
on	O
previous	O
stock	O
returns	O
we	O
can	O
train	B
the	O
method	O
using	O
stock	O
returns	O
from	O
the	O
past	O
months	O
but	O
we	O
don	O
t	O
really	O
care	O
how	O
well	O
our	O
method	O
predicts	O
last	O
week	O
s	O
stock	O
price	O
we	O
instead	O
care	O
about	O
how	O
well	O
it	O
will	O
predict	O
tomorrow	O
s	O
price	O
or	O
next	O
month	O
s	O
price	O
on	O
a	O
similar	O
note	O
suppose	O
that	O
we	O
have	O
clinical	O
measurements	O
weight	O
blood	O
pressure	O
height	O
age	O
family	O
history	O
of	O
disease	O
for	O
a	O
number	O
of	O
patients	O
as	O
well	O
as	O
information	O
about	O
whether	O
each	O
patient	O
has	O
diabetes	O
we	O
can	O
use	O
these	O
patients	O
to	O
train	B
a	O
statistical	O
learning	O
method	O
to	O
predict	O
risk	O
of	O
diabetes	O
based	O
on	O
clinical	O
measurements	O
in	O
practice	O
we	O
want	O
this	O
method	O
to	O
accurately	O
predict	O
diabetes	O
risk	O
for	O
future	O
patients	O
based	O
on	O
their	O
clinical	O
measurements	O
we	O
are	O
not	O
very	O
interested	O
in	O
whether	O
or	O
not	O
the	O
method	O
accurately	O
predicts	O
diabetes	O
risk	O
for	O
patients	O
used	O
to	O
train	B
the	O
model	B
since	O
we	O
already	O
know	O
which	O
of	O
those	O
patients	O
have	O
diabetes	O
to	O
state	O
it	O
more	O
mathematically	O
suppose	O
that	O
we	O
fit	O
our	O
statistical	O
learn	O
ing	O
method	O
on	O
our	O
training	O
observations	B
yn	O
and	O
we	O
obtain	O
the	O
estimate	O
f	O
we	O
can	O
then	O
compute	O
f	O
f	O
f	O
if	O
these	O
are	O
approximately	O
equal	O
to	O
yn	O
then	O
the	O
training	O
mse	B
given	O
by	O
is	O
small	O
however	O
we	O
are	O
really	O
not	O
interested	O
in	O
whether	O
f	O
yi	O
instead	O
we	O
want	O
to	O
know	O
whether	O
f	O
is	O
approximately	O
equal	O
to	O
where	O
is	O
a	O
previously	O
unseen	O
test	O
observation	O
not	O
used	O
to	O
train	B
the	O
statistical	O
learning	O
method	O
we	O
want	O
to	O
choose	O
the	O
method	O
that	O
gives	O
the	O
lowest	O
test	O
mse	B
as	O
opposed	O
to	O
the	O
lowest	O
training	O
mse	B
in	O
other	O
words	O
if	O
we	O
had	O
a	O
large	O
number	O
of	O
test	O
observations	B
we	O
could	O
compute	O
f	O
the	O
average	B
squared	O
prediction	B
error	B
for	O
these	O
test	O
observations	B
we	O
d	O
like	O
to	O
select	O
the	O
model	B
for	O
which	O
the	O
average	B
of	O
this	O
quantity	O
the	O
test	O
mse	B
is	O
as	O
small	O
as	O
possible	O
how	O
can	O
we	O
go	O
about	O
trying	O
to	O
select	O
a	O
method	O
that	O
minimizes	O
the	O
test	O
mse	B
in	O
some	O
settings	O
we	O
may	O
have	O
a	O
test	O
data	B
set	B
available	O
that	O
is	O
we	O
may	O
have	O
access	O
to	O
a	O
set	B
of	O
observations	B
that	O
were	O
not	O
used	O
to	O
train	B
the	O
statistical	O
learning	O
method	O
we	O
can	O
then	O
simply	O
evaluate	O
on	O
the	O
test	O
observations	B
and	O
select	O
the	O
learning	O
method	O
for	O
which	O
the	O
test	O
mse	B
is	O
training	O
mse	B
test	O
data	B
test	O
mse	B
assessing	O
model	B
accuracy	O
y	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
x	O
flexibility	O
figure	O
left	O
data	B
simulated	O
from	O
f	O
shown	O
in	O
black	O
three	O
estimates	O
of	O
f	O
are	O
shown	O
the	O
linear	B
regression	B
line	B
curve	O
and	O
two	O
smoothing	B
spline	B
fits	O
and	O
green	O
curves	O
right	O
training	O
mse	B
curve	O
test	O
mse	B
curve	O
and	O
minimum	O
possible	O
test	O
mse	B
over	O
all	O
methods	O
line	B
squares	O
represent	O
the	O
training	O
and	O
test	O
mses	O
for	O
the	O
three	O
fits	O
shown	O
in	O
the	O
left-hand	O
panel	O
smallest	O
but	O
what	O
if	O
no	O
test	O
observations	B
are	O
available	O
in	O
that	O
case	O
one	O
might	O
imagine	O
simply	O
selecting	O
a	O
statistical	O
learning	O
method	O
that	O
minimizes	O
the	O
training	O
mse	B
this	O
seems	O
like	O
it	O
might	O
be	O
a	O
sensible	O
approach	B
since	O
the	O
training	O
mse	B
and	O
the	O
test	O
mse	B
appear	O
to	O
be	O
closely	O
related	O
unfortunately	O
there	O
is	O
a	O
fundamental	O
problem	O
with	O
this	O
strategy	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
method	O
with	O
the	O
lowest	O
training	O
mse	B
will	O
also	O
have	O
the	O
lowest	O
test	O
mse	B
roughly	O
speaking	O
the	O
problem	O
is	O
that	O
many	O
statistical	O
methods	O
specifically	O
estimate	O
coefficients	O
so	O
as	O
to	O
minimize	O
the	O
training	O
set	B
mse	B
for	O
these	O
methods	O
the	O
training	O
set	B
mse	B
can	O
be	O
quite	O
small	O
but	O
the	O
test	O
mse	B
is	O
often	O
much	O
larger	O
figure	O
illustrates	O
this	O
phenomenon	O
on	O
a	O
simple	B
example	O
in	O
the	O
lefthand	O
panel	O
of	O
figure	O
we	O
have	O
generated	O
observations	B
from	O
with	O
the	O
true	O
f	O
given	O
by	O
the	O
black	O
curve	O
the	O
orange	O
blue	O
and	O
green	O
curves	O
illustrate	O
three	O
possible	O
estimates	O
for	O
f	O
obtained	O
using	O
methods	O
with	O
increasing	O
levels	O
of	O
flexibility	O
the	O
orange	O
line	B
is	O
the	O
linear	B
regression	B
fit	O
which	O
is	O
relatively	O
inflexible	O
the	O
blue	O
and	O
green	O
curves	O
were	O
produced	O
using	O
smoothing	B
splines	O
discussed	O
in	O
chapter	O
with	O
different	O
levels	O
of	O
smoothness	O
it	O
is	O
clear	O
that	O
as	O
the	O
level	B
of	O
flexibility	O
increases	O
the	O
curves	O
fit	O
the	O
observed	O
data	B
more	O
closely	O
the	O
green	O
curve	O
is	O
the	O
most	O
flexible	O
and	O
matches	O
the	O
data	B
very	O
well	O
however	O
we	O
observe	O
that	O
it	O
fits	O
the	O
true	O
f	O
in	O
black	O
poorly	O
because	O
it	O
is	O
too	O
wiggly	O
by	O
adjusting	O
the	O
level	B
of	O
flexibility	O
of	O
the	O
smoothing	B
spline	B
fit	O
we	O
can	O
produce	O
many	O
different	O
fits	O
to	O
this	O
data	B
smoothing	B
spline	B
degrees	B
of	I
freedom	I
statistical	O
learning	O
we	O
now	O
move	O
on	O
to	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
grey	O
curve	O
displays	O
the	O
average	B
training	O
mse	B
as	O
a	O
function	B
of	O
flexibility	O
or	O
more	O
formally	O
the	O
degrees	B
of	I
freedom	I
for	O
a	O
number	O
of	O
smoothing	B
splines	O
the	O
degrees	B
of	I
freedom	I
is	O
a	O
quantity	O
that	O
summarizes	O
the	O
flexibility	O
of	O
a	O
curve	O
it	O
is	O
discussed	O
more	O
fully	O
in	O
chapter	O
the	O
orange	O
blue	O
and	O
green	O
squares	O
indicate	O
the	O
mses	O
associated	O
with	O
the	O
corresponding	O
curves	O
in	O
the	O
lefthand	O
panel	O
a	O
more	O
restricted	O
and	O
hence	O
smoother	B
curve	O
has	O
fewer	O
degrees	B
of	I
freedom	I
than	O
a	O
wiggly	O
curve	O
note	O
that	O
in	O
figure	O
linear	B
regression	B
is	O
at	O
the	O
most	O
restrictive	O
end	O
with	O
two	O
degrees	B
of	I
freedom	I
the	O
training	O
mse	B
declines	O
monotonically	O
as	O
flexibility	O
increases	O
in	O
this	O
example	O
the	O
true	O
f	O
is	O
non-linear	B
and	O
so	O
the	O
orange	O
linear	B
fit	O
is	O
not	O
flexible	O
enough	O
to	O
estimate	O
f	O
well	O
the	O
green	O
curve	O
has	O
the	O
lowest	O
training	O
mse	B
of	O
all	O
three	O
methods	O
since	O
it	O
corresponds	O
to	O
the	O
most	O
flexible	O
of	O
the	O
three	O
curves	O
fit	O
in	O
the	O
left-hand	O
panel	O
in	O
this	O
example	O
we	O
know	O
the	O
true	O
function	B
f	O
and	O
so	O
we	O
can	O
also	O
compute	O
the	O
test	O
mse	B
over	O
a	O
very	O
large	O
test	O
set	B
as	O
a	O
function	B
of	O
flexibility	O
course	O
in	O
general	O
f	O
is	O
unknown	O
so	O
this	O
will	O
not	O
be	O
possible	O
the	O
test	O
mse	B
is	O
displayed	O
using	O
the	O
red	O
curve	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
as	O
with	O
the	O
training	O
mse	B
the	O
test	O
mse	B
initially	O
declines	O
as	O
the	O
level	B
of	O
flexibility	O
increases	O
however	O
at	O
some	O
point	O
the	O
test	O
mse	B
levels	O
off	O
and	O
then	O
starts	O
to	O
increase	O
again	O
consequently	O
the	O
orange	O
and	O
green	O
curves	O
both	O
have	O
high	O
test	O
mse	B
the	O
blue	O
curve	O
minimizes	O
the	O
test	O
mse	B
which	O
should	O
not	O
be	O
surprising	O
given	O
that	O
visually	O
it	O
appears	O
to	O
estimate	O
f	O
the	O
best	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
horizontal	O
dashed	O
line	B
indicates	O
var	O
the	O
irreducible	B
error	B
in	O
which	O
corresponds	O
to	O
the	O
lowest	O
achievable	O
test	O
mse	B
among	O
all	O
possible	O
methods	O
hence	O
the	O
smoothing	B
spline	B
represented	O
by	O
the	O
blue	O
curve	O
is	O
close	O
to	O
optimal	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
as	O
the	O
flexibility	O
of	O
the	O
statistical	O
learning	O
method	O
increases	O
we	O
observe	O
a	O
monotone	O
decrease	O
in	O
the	O
training	O
mse	B
and	O
a	O
u-shape	O
in	O
the	O
test	O
mse	B
this	O
is	O
a	O
fundamental	O
property	O
of	O
statistical	O
learning	O
that	O
holds	O
regardless	O
of	O
the	O
particular	O
data	B
set	B
at	O
hand	O
and	O
regardless	O
of	O
the	O
statistical	O
method	O
being	O
used	O
as	O
model	B
flexibility	O
increases	O
training	O
mse	B
will	O
decrease	O
but	O
the	O
test	O
mse	B
may	O
not	O
when	O
a	O
given	O
method	O
yields	O
a	O
small	O
training	O
mse	B
but	O
a	O
large	O
test	O
mse	B
we	O
are	O
said	O
to	O
be	O
overfitting	B
the	O
data	B
this	O
happens	O
because	O
our	O
statistical	O
learning	O
procedure	O
is	O
working	O
too	O
hard	O
to	O
find	O
patterns	O
in	O
the	O
training	O
data	B
and	O
may	O
be	O
picking	O
up	O
some	O
patterns	O
that	O
are	O
just	O
caused	O
by	O
random	O
chance	O
rather	O
than	O
by	O
true	O
properties	O
of	O
the	O
unknown	O
function	B
f	O
when	O
we	O
overfit	O
the	O
training	O
data	B
the	O
test	O
mse	B
will	O
be	O
very	O
large	O
because	O
the	O
supposed	O
patterns	O
that	O
the	O
method	O
found	O
in	O
the	O
training	O
data	B
simply	O
don	O
t	O
exist	O
in	O
the	O
test	O
data	B
note	O
that	O
regardless	O
of	O
whether	O
or	O
not	O
overfitting	B
has	O
occurred	O
we	O
almost	O
always	O
expect	O
the	O
training	O
mse	B
to	O
be	O
smaller	O
than	O
the	O
test	O
mse	B
because	O
most	O
statistical	O
learning	O
methods	O
either	O
directly	O
or	O
indirectly	O
seek	O
to	O
minimize	O
the	O
training	O
mse	B
overfitting	B
refers	O
specifically	O
to	O
the	O
case	O
in	O
which	O
a	O
less	O
flexible	O
model	B
would	O
have	O
yielded	O
a	O
smaller	O
test	O
mse	B
assessing	O
model	B
accuracy	O
y	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
x	O
flexibility	O
figure	O
details	O
are	O
as	O
in	O
figure	O
using	O
a	O
different	O
true	O
f	O
that	O
is	O
much	O
closer	O
to	O
linear	B
in	O
this	O
setting	O
linear	B
regression	B
provides	O
a	O
very	O
good	O
fit	O
to	O
the	O
data	B
figure	O
provides	O
another	O
example	O
in	O
which	O
the	O
true	O
f	O
is	O
approximately	O
linear	B
again	O
we	O
observe	O
that	O
the	O
training	O
mse	B
decreases	O
monotonically	O
as	O
the	O
model	B
flexibility	O
increases	O
and	O
that	O
there	O
is	O
a	O
u-shape	O
in	O
the	O
test	O
mse	B
however	O
because	O
the	O
truth	O
is	O
close	O
to	O
linear	B
the	O
test	O
mse	B
only	O
decreases	O
slightly	O
before	O
increasing	O
again	O
so	O
that	O
the	O
orange	O
least	B
squares	I
fit	O
is	O
substantially	O
better	O
than	O
the	O
highly	O
flexible	O
green	O
curve	O
finally	O
figure	O
displays	O
an	O
example	O
in	O
which	O
f	O
is	O
highly	O
non-linear	B
the	O
training	O
and	O
test	O
mse	B
curves	O
still	O
exhibit	O
the	O
same	O
general	O
patterns	O
but	O
now	O
there	O
is	O
a	O
rapid	O
decrease	O
in	O
both	O
curves	O
before	O
the	O
test	O
mse	B
starts	O
to	O
increase	O
slowly	O
in	O
practice	O
one	O
can	O
usually	O
compute	O
the	O
training	O
mse	B
with	O
relative	O
ease	O
but	O
estimating	O
test	O
mse	B
is	O
considerably	O
more	O
difficult	O
because	O
usually	O
no	O
test	O
data	B
are	O
available	O
as	O
the	O
previous	O
three	O
examples	O
illustrate	O
the	O
flexibility	O
level	B
corresponding	O
to	O
the	O
model	B
with	O
the	O
minimal	O
test	O
mse	B
can	O
vary	O
considerably	O
among	O
data	B
sets	O
throughout	O
this	O
book	O
we	O
discuss	O
a	O
variety	O
of	O
approaches	O
that	O
can	O
be	O
used	O
in	O
practice	O
to	O
estimate	O
this	O
minimum	O
point	O
one	O
important	O
method	O
is	O
cross-validation	B
which	O
is	O
a	O
crossmethod	O
for	O
estimating	O
test	O
mse	B
using	O
the	O
training	O
data	B
validation	O
the	O
bias-variance	O
trade-off	B
the	O
u-shape	O
observed	O
in	O
the	O
test	O
mse	B
curves	O
turns	O
out	O
to	O
be	O
the	O
result	O
of	O
two	O
competing	O
properties	O
of	O
statistical	O
learning	O
methods	O
though	O
the	O
mathematical	O
proof	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
it	O
is	O
possible	O
to	O
show	O
that	O
the	O
expected	O
test	O
mse	B
for	O
a	O
given	O
value	O
can	O
statistical	O
learning	O
y	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
x	O
flexibility	O
figure	O
details	O
are	O
as	O
in	O
figure	O
using	O
a	O
different	O
f	O
that	O
is	O
far	O
from	O
linear	B
in	O
this	O
setting	O
linear	B
regression	B
provides	O
a	O
very	O
poor	O
fit	O
to	O
the	O
data	B
always	O
be	O
decomposed	O
into	O
the	O
sum	O
of	O
three	O
fundamental	O
quantities	O
the	O
variance	B
of	O
f	O
the	O
squared	O
bias	B
of	O
f	O
and	O
the	O
variance	B
of	O
the	O
error	B
terms	O
that	O
is	O
var	O
f	O
f	O
var	O
e	O
f	O
f	O
here	O
the	O
notation	O
e	O
defines	O
the	O
expected	O
test	O
mse	B
and	O
refers	O
to	O
the	O
average	B
test	O
mse	B
that	O
we	O
would	O
obtain	O
if	O
we	O
repeatedly	O
estimated	O
f	O
using	O
a	O
large	O
number	O
of	O
training	O
sets	O
and	O
tested	O
each	O
at	O
the	O
overall	O
f	O
over	O
all	O
expected	O
test	O
mse	B
can	O
be	O
computed	O
by	O
averaging	O
e	O
possible	O
values	O
of	O
in	O
the	O
test	O
set	B
equation	O
tells	O
us	O
that	O
in	O
order	O
to	O
minimize	O
the	O
expected	O
test	O
error	B
we	O
need	O
to	O
select	O
a	O
statistical	O
learning	O
method	O
that	O
simultaneously	O
achieves	O
low	O
variance	B
and	O
low	O
bias	B
note	O
that	O
variance	B
is	O
inherently	O
a	O
nonnegative	O
quantity	O
and	O
squared	O
bias	B
is	O
also	O
nonnegative	O
hence	O
we	O
see	O
that	O
the	O
expected	O
test	O
mse	B
can	O
never	O
lie	O
below	O
var	O
the	O
irreducible	B
error	B
from	O
what	O
do	O
we	O
mean	O
by	O
the	O
variance	B
and	O
bias	B
of	O
a	O
statistical	O
learning	O
method	O
variance	B
refers	O
to	O
the	O
amount	O
by	O
which	O
f	O
would	O
change	O
if	O
we	O
estimated	O
it	O
using	O
a	O
different	O
training	O
data	B
set	B
since	O
the	O
training	O
data	B
are	O
used	O
to	O
fit	O
the	O
statistical	O
learning	O
method	O
different	O
training	O
data	B
sets	O
will	O
result	O
in	O
a	O
different	O
f	O
but	O
ideally	O
the	O
estimate	O
for	O
f	O
should	O
not	O
vary	O
too	O
much	O
between	O
training	O
sets	O
however	O
if	O
a	O
method	O
has	O
high	O
variance	B
then	O
small	O
changes	O
in	O
the	O
training	O
data	B
can	O
result	O
in	O
large	O
changes	O
in	O
f	O
in	O
general	O
more	O
flexible	O
statistical	O
methods	O
have	O
higher	O
variance	B
consider	O
the	O
variance	B
bias	B
expected	O
test	O
mse	B
assessing	O
model	B
accuracy	O
green	O
and	O
orange	O
curves	O
in	O
figure	O
the	O
flexible	O
green	O
curve	O
is	O
following	O
the	O
observations	B
very	O
closely	O
it	O
has	O
high	O
variance	B
because	O
changing	O
any	O
one	O
of	O
these	O
data	B
points	O
may	O
cause	O
the	O
estimate	O
f	O
to	O
change	O
considerably	O
in	O
contrast	B
the	O
orange	O
least	B
squares	I
line	B
is	O
relatively	O
inflexible	O
and	O
has	O
low	O
variance	B
because	O
moving	O
any	O
single	B
observation	O
will	O
likely	O
cause	O
only	O
a	O
small	O
shift	O
in	O
the	O
position	O
of	O
the	O
line	B
on	O
the	O
other	O
hand	O
bias	B
refers	O
to	O
the	O
error	B
that	O
is	O
introduced	O
by	O
approximating	O
a	O
real-life	O
problem	O
which	O
may	O
be	O
extremely	O
complicated	O
by	O
a	O
much	O
simpler	O
model	B
for	O
example	O
linear	B
regression	B
assumes	O
that	O
there	O
is	O
a	O
linear	B
relationship	O
between	O
y	O
and	O
xp	O
it	O
is	O
unlikely	O
that	O
any	O
real-life	O
problem	O
truly	O
has	O
such	O
a	O
simple	B
linear	B
relationship	O
and	O
so	O
performing	O
linear	B
regression	B
will	O
undoubtedly	O
result	O
in	O
some	O
bias	B
in	O
the	O
estimate	O
of	O
f	O
in	O
figure	O
the	O
true	O
f	O
is	O
substantially	O
non-linear	B
so	O
no	O
matter	O
how	O
many	O
training	O
observations	B
we	O
are	O
given	O
it	O
will	O
not	O
be	O
possible	O
to	O
produce	O
an	O
accurate	O
estimate	O
using	O
linear	B
regression	B
in	O
other	O
words	O
linear	B
regression	B
results	O
in	O
high	O
bias	B
in	O
this	O
example	O
however	O
in	O
figure	O
the	O
true	O
f	O
is	O
very	O
close	O
to	O
linear	B
and	O
so	O
given	O
enough	O
data	B
it	O
should	O
be	O
possible	O
for	O
linear	B
regression	B
to	O
produce	O
an	O
accurate	O
estimate	O
generally	O
more	O
flexible	O
methods	O
result	O
in	O
less	O
bias	B
as	O
a	O
general	O
rule	O
as	O
we	O
use	O
more	O
flexible	O
methods	O
the	O
variance	B
will	O
increase	O
and	O
the	O
bias	B
will	O
decrease	O
the	O
relative	O
rate	B
of	O
change	O
of	O
these	O
two	O
quantities	O
determines	O
whether	O
the	O
test	O
mse	B
increases	O
or	O
decreases	O
as	O
we	O
increase	O
the	O
flexibility	O
of	O
a	O
class	O
of	O
methods	O
the	O
bias	B
tends	O
to	O
initially	O
decrease	O
faster	O
than	O
the	O
variance	B
increases	O
consequently	O
the	O
expected	O
test	O
mse	B
declines	O
however	O
at	O
some	O
point	O
increasing	O
flexibility	O
has	O
little	O
impact	O
on	O
the	O
bias	B
but	O
starts	O
to	O
significantly	O
increase	O
the	O
variance	B
when	O
this	O
happens	O
the	O
test	O
mse	B
increases	O
note	O
that	O
we	O
observed	O
this	O
pattern	O
of	O
decreasing	O
test	O
mse	B
followed	O
by	O
increasing	O
test	O
mse	B
in	O
the	O
right-hand	O
panels	O
of	O
figures	O
the	O
three	O
plots	O
in	O
figure	O
illustrate	O
equation	O
for	O
the	O
examples	O
in	O
figures	O
in	O
each	O
case	O
the	O
blue	O
solid	O
curve	O
represents	O
the	O
squared	O
bias	B
for	O
different	O
levels	O
of	O
flexibility	O
while	O
the	O
orange	O
curve	O
corresponds	O
to	O
the	O
variance	B
the	O
horizontal	O
dashed	O
line	B
represents	O
var	O
the	O
irreducible	B
error	B
finally	O
the	O
red	O
curve	O
corresponding	O
to	O
the	O
test	O
set	B
mse	B
is	O
the	O
sum	O
of	O
these	O
three	O
quantities	O
in	O
all	O
three	O
cases	O
the	O
variance	B
increases	O
and	O
the	O
bias	B
decreases	O
as	O
the	O
method	O
s	O
flexibility	O
increases	O
however	O
the	O
flexibility	O
level	B
corresponding	O
to	O
the	O
optimal	O
test	O
mse	B
differs	O
considerably	O
among	O
the	O
three	O
data	B
sets	O
because	O
the	O
squared	O
bias	B
and	O
variance	B
change	O
at	O
different	O
rates	O
in	O
each	O
of	O
the	O
data	B
sets	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
bias	B
initially	O
decreases	O
rapidly	O
resulting	O
in	O
an	O
initial	O
sharp	O
decrease	O
in	O
the	O
expected	O
test	O
mse	B
on	O
the	O
other	O
hand	O
in	O
the	O
center	O
panel	O
of	O
figure	O
the	O
true	O
f	O
is	O
close	O
to	O
linear	B
so	O
there	O
is	O
only	O
a	O
small	O
decrease	O
in	O
bias	B
as	O
flexibility	O
increases	O
and	O
the	O
test	O
mse	B
only	O
declines	O
slightly	O
before	O
increasing	O
rapidly	O
as	O
the	O
variance	B
increases	O
finally	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
as	O
flexibility	O
increases	O
there	O
is	O
a	O
dramatic	O
decline	O
in	O
bias	B
because	O
statistical	O
learning	O
mse	B
bias	B
var	O
flexibility	O
flexibility	O
flexibility	O
figure	O
squared	O
bias	B
curve	O
variance	B
curve	O
var	O
line	B
and	O
test	O
mse	B
curve	O
for	O
the	O
three	O
data	B
sets	O
in	O
figures	O
the	O
vertical	O
dotted	O
line	B
indicates	O
the	O
flexibility	O
level	B
corresponding	O
to	O
the	O
smallest	O
test	O
mse	B
the	O
true	O
f	O
is	O
very	O
non-linear	B
there	O
is	O
also	O
very	O
little	O
increase	O
in	O
variance	B
as	O
flexibility	O
increases	O
consequently	O
the	O
test	O
mse	B
declines	O
substantially	O
before	O
experiencing	O
a	O
small	O
increase	O
as	O
model	B
flexibility	O
increases	O
the	O
relationship	O
between	O
bias	B
variance	B
and	O
test	O
set	B
mse	B
given	O
in	O
equation	O
and	O
displayed	O
in	O
figure	O
is	O
referred	O
to	O
as	O
the	O
bias-variance	O
trade-off	B
good	O
test	O
set	B
performance	O
of	O
a	O
statistical	O
learning	O
method	O
requires	O
low	O
variance	B
as	O
well	O
as	O
low	O
squared	O
bias	B
this	O
is	O
referred	O
to	O
as	O
a	O
trade-off	B
because	O
it	O
is	O
easy	O
to	O
obtain	O
a	O
method	O
with	O
extremely	O
low	O
bias	B
but	O
high	O
variance	B
instance	O
by	O
drawing	O
a	O
curve	O
that	O
passes	O
through	O
every	O
single	B
training	O
observation	O
or	O
a	O
method	O
with	O
very	O
low	O
variance	B
but	O
high	O
bias	B
fitting	O
a	O
horizontal	O
line	B
to	O
the	O
data	B
the	O
challenge	O
lies	O
in	O
finding	O
a	O
method	O
for	O
which	O
both	O
the	O
variance	B
and	O
the	O
squared	O
bias	B
are	O
low	O
this	O
trade-off	B
is	O
one	O
of	O
the	O
most	O
important	O
recurring	O
themes	O
in	O
this	O
book	O
in	O
a	O
real-life	O
situation	O
in	O
which	O
f	O
is	O
unobserved	O
it	O
is	O
generally	O
not	O
possible	O
to	O
explicitly	O
compute	O
the	O
test	O
mse	B
bias	B
or	O
variance	B
for	O
a	O
statistical	O
learning	O
method	O
nevertheless	O
one	O
should	O
always	O
keep	O
the	O
bias-variance	O
trade-off	B
in	O
mind	O
in	O
this	O
book	O
we	O
explore	O
methods	O
that	O
are	O
extremely	O
flexible	O
and	O
hence	O
can	O
essentially	O
eliminate	O
bias	B
however	O
this	O
does	O
not	O
guarantee	O
that	O
they	O
will	O
outperform	O
a	O
much	O
simpler	O
method	O
such	O
as	O
linear	B
regression	B
to	O
take	O
an	O
extreme	O
example	O
suppose	O
that	O
the	O
true	O
f	O
is	O
linear	B
in	O
this	O
situation	O
linear	B
regression	B
will	O
have	O
no	O
bias	B
making	O
it	O
very	O
hard	O
for	O
a	O
more	O
flexible	O
method	O
to	O
compete	O
in	O
contrast	B
if	O
the	O
true	O
f	O
is	O
highly	O
non-linear	B
and	O
we	O
have	O
an	O
ample	O
number	O
of	O
training	O
observations	B
then	O
we	O
may	O
do	O
better	O
using	O
a	O
highly	O
flexible	O
approach	B
as	O
in	O
figure	O
in	O
chapter	O
we	O
discuss	O
cross-validation	B
which	O
is	O
a	O
way	O
to	O
estimate	O
the	O
test	O
mse	B
using	O
the	O
training	O
data	B
bias-variance	O
trade-off	B
assessing	O
model	B
accuracy	O
the	O
classification	B
setting	O
thus	O
far	O
our	O
discussion	O
of	O
model	B
accuracy	O
has	O
been	O
focused	O
on	O
the	O
regression	B
setting	O
but	O
many	O
of	O
the	O
concepts	O
that	O
we	O
have	O
encountered	O
such	O
as	O
the	O
bias-variance	O
trade-off	B
transfer	O
over	O
to	O
the	O
classification	B
setting	O
with	O
only	O
some	O
modifications	O
due	O
to	O
the	O
fact	O
that	O
yi	O
is	O
no	O
longer	O
numerical	O
suppose	O
that	O
we	O
seek	O
to	O
estimate	O
f	O
on	O
the	O
basis	B
of	O
training	O
observations	B
yn	O
where	O
now	O
yn	O
are	O
qualitative	B
the	O
most	O
common	O
approach	B
for	O
quantifying	O
the	O
accuracy	O
of	O
our	O
estimate	O
f	O
is	O
the	O
training	O
error	B
rate	B
the	O
proportion	O
of	O
mistakes	O
that	O
are	O
made	O
if	O
we	O
apply	O
our	O
estimate	O
f	O
to	O
the	O
training	O
observations	B
n	O
iyi	O
yi	O
here	O
yi	O
is	O
the	O
predicted	O
class	O
label	O
for	O
the	O
ith	O
observation	O
using	O
f	O
and	O
iyi	O
yi	O
is	O
an	O
indicator	B
variable	B
that	O
equals	O
if	O
yi	O
yi	O
and	O
zero	O
if	O
yi	O
yi	O
if	O
iyi	O
yi	O
then	O
the	O
ith	O
observation	O
was	O
classified	O
correctly	O
by	O
our	O
classification	B
method	O
otherwise	O
it	O
was	O
misclassified	O
hence	O
equation	O
computes	O
the	O
fraction	O
of	O
incorrect	O
classifications	O
equation	O
is	O
referred	O
to	O
as	O
the	O
training	O
error	B
rate	B
because	O
it	O
is	O
computed	O
based	O
on	O
the	O
data	B
that	O
was	O
used	O
to	O
train	B
our	O
classifier	B
as	O
in	O
the	O
regression	B
setting	O
we	O
are	O
most	O
interested	O
in	O
the	O
error	B
rates	O
that	O
result	O
from	O
applying	O
our	O
classifier	B
to	O
test	O
observations	B
that	O
were	O
not	O
used	O
in	O
training	O
the	O
test	O
error	B
rate	B
associated	O
with	O
a	O
set	B
of	O
test	O
observations	B
of	O
the	O
form	O
is	O
given	O
by	O
ave	O
error	B
rate	B
indicator	B
variable	B
training	O
error	B
test	O
error	B
where	O
is	O
the	O
predicted	O
class	O
label	O
that	O
results	O
from	O
applying	O
the	O
classifier	B
to	O
the	O
test	O
observation	O
with	O
predictor	B
a	O
good	O
classifier	B
is	O
one	O
for	O
which	O
the	O
test	O
error	B
is	O
smallest	O
the	O
bayes	O
classifier	B
it	O
is	O
possible	O
to	O
show	O
the	O
proof	O
is	O
outside	O
of	O
the	O
scope	O
of	O
this	O
book	O
that	O
the	O
test	O
error	B
rate	B
given	O
in	O
is	O
minimized	O
on	O
average	B
by	O
a	O
very	O
simple	B
classifier	B
that	O
assigns	O
each	O
observation	O
to	O
the	O
most	O
likely	O
class	O
given	O
its	O
predictor	B
values	O
in	O
other	O
words	O
we	O
should	O
simply	O
assign	O
a	O
test	O
observation	O
with	O
predictor	B
vector	B
to	O
the	O
class	O
j	O
for	O
which	O
pry	O
jx	O
is	O
largest	O
note	O
that	O
is	O
a	O
conditional	B
probability	B
it	O
is	O
the	O
probability	B
that	O
y	O
j	O
given	O
the	O
observed	O
predictor	B
vector	B
this	O
very	O
simple	B
classifier	B
is	O
called	O
the	O
bayes	O
classifier	B
in	O
a	O
two-class	O
problem	O
where	O
there	O
are	O
only	O
two	O
possible	O
response	B
values	O
say	O
class	O
or	O
class	O
the	O
bayes	O
classifier	B
conditional	B
probability	B
bayes	O
classifier	B
statistical	O
learning	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
a	O
simulated	O
data	B
set	B
consisting	O
of	O
observations	B
in	O
each	O
of	O
two	O
groups	O
indicated	O
in	O
blue	O
and	O
in	O
orange	O
the	O
purple	O
dashed	O
line	B
represents	O
the	O
bayes	O
decision	B
boundary	I
the	O
orange	O
background	O
grid	O
indicates	O
the	O
region	O
in	O
which	O
a	O
test	O
observation	O
will	O
be	O
assigned	O
to	O
the	O
orange	O
class	O
and	O
the	O
blue	O
background	O
grid	O
indicates	O
the	O
region	O
in	O
which	O
a	O
test	O
observation	O
will	O
be	O
assigned	O
to	O
the	O
blue	O
class	O
corresponds	O
to	O
predicting	O
class	O
one	O
if	O
pry	O
and	O
class	O
two	O
otherwise	O
figure	O
provides	O
an	O
example	O
using	O
a	O
simulated	O
data	B
set	B
in	O
a	O
twodimensional	O
space	O
consisting	O
of	O
predictors	O
and	O
the	O
orange	O
and	O
blue	O
circles	O
correspond	O
to	O
training	O
observations	B
that	O
belong	O
to	O
two	O
different	O
classes	O
for	O
each	O
value	O
of	O
and	O
there	O
is	O
a	O
different	O
probability	B
of	O
the	O
response	B
being	O
orange	O
or	O
blue	O
since	O
this	O
is	O
simulated	O
data	B
we	O
know	O
how	O
the	O
data	B
were	O
generated	O
and	O
we	O
can	O
calculate	O
the	O
conditional	O
probabilities	O
for	O
each	O
value	O
of	O
and	O
the	O
orange	O
shaded	O
region	O
reflects	O
the	O
set	B
of	O
points	O
for	O
which	O
pry	O
orangex	O
is	O
greater	O
than	O
while	O
the	O
blue	O
shaded	O
region	O
indicates	O
the	O
set	B
of	O
points	O
for	O
which	O
the	O
probability	B
is	O
below	O
the	O
purple	O
dashed	O
line	B
represents	O
the	O
points	O
where	O
the	O
probability	B
is	O
exactly	O
this	O
is	O
called	O
the	O
bayes	O
decision	B
boundary	I
the	O
bayes	O
classifier	B
s	O
prediction	B
is	O
determined	O
by	O
the	O
bayes	O
decision	B
boundary	I
an	O
observation	O
that	O
falls	O
on	O
the	O
orange	O
side	O
of	O
the	O
boundary	O
will	O
be	O
assigned	O
to	O
the	O
orange	O
class	O
and	O
similarly	O
an	O
observation	O
on	O
the	O
blue	O
side	O
of	O
the	O
boundary	O
will	O
be	O
assigned	O
to	O
the	O
blue	O
class	O
the	O
bayes	O
classifier	B
produces	O
the	O
lowest	O
possible	O
test	O
error	B
rate	B
called	O
the	O
bayes	O
error	B
rate	B
since	O
the	O
bayes	O
classifier	B
will	O
always	O
choose	O
the	O
class	O
for	O
which	O
is	O
largest	O
the	O
error	B
rate	B
at	O
x	O
will	O
be	O
maxj	O
pry	O
jx	O
in	O
general	O
the	O
overall	O
bayes	O
error	B
rate	B
is	O
given	O
by	O
max	O
pry	O
jx	O
e	O
j	O
bayes	O
decision	B
boundary	I
bayes	O
error	B
rate	B
k-nearest	O
neighbors	O
assessing	O
model	B
accuracy	O
where	O
the	O
expectation	O
averages	O
the	O
probability	B
over	O
all	O
possible	O
values	O
of	O
x	O
for	O
our	O
simulated	O
data	B
the	O
bayes	O
error	B
rate	B
is	O
it	O
is	O
greater	O
than	O
zero	O
because	O
the	O
classes	O
overlap	O
in	O
the	O
true	O
population	O
so	O
maxj	O
pry	O
jx	O
for	O
some	O
values	O
of	O
the	O
bayes	O
error	B
rate	B
is	O
analogous	O
to	O
the	O
irreducible	B
error	B
discussed	O
earlier	O
k-nearest	O
neighbors	O
in	O
theory	O
we	O
would	O
always	O
like	O
to	O
predict	O
qualitative	B
responses	O
using	O
the	O
bayes	O
classifier	B
but	O
for	O
real	O
data	B
we	O
do	O
not	O
know	O
the	O
conditional	O
distribution	B
of	O
y	O
given	O
x	O
and	O
so	O
computing	O
the	O
bayes	O
classifier	B
is	O
impossible	O
therefore	O
the	O
bayes	O
classifier	B
serves	O
as	O
an	O
unattainable	O
gold	O
standard	O
against	O
which	O
to	O
compare	O
other	O
methods	O
many	O
approaches	O
attempt	O
to	O
estimate	O
the	O
conditional	O
distribution	B
of	O
y	O
given	O
x	O
and	O
then	O
classify	O
a	O
given	O
observation	O
to	O
the	O
class	O
with	O
highest	O
estimated	O
probability	B
one	O
such	O
method	O
is	O
the	O
k-nearest	O
neighbors	O
classifier	B
given	O
a	O
positive	O
integer	O
k	O
and	O
a	O
test	O
observation	O
the	O
knn	O
classifier	B
first	O
identifies	O
the	O
k	O
points	O
in	O
the	O
training	O
data	B
that	O
are	O
closest	O
to	O
represented	O
by	O
points	O
in	O
whose	O
response	B
values	O
equal	O
j	O
it	O
then	O
estimates	O
the	O
conditional	B
probability	B
for	O
class	O
j	O
as	O
the	O
fraction	O
of	O
pry	O
jx	O
i	O
k	O
iyi	O
j	O
finally	O
knn	O
applies	O
bayes	O
rule	O
and	O
classifies	O
the	O
test	O
observation	O
to	O
the	O
class	O
with	O
the	O
largest	O
probability	B
figure	O
provides	O
an	O
illustrative	O
example	O
of	O
the	O
knn	O
approach	B
in	O
the	O
left-hand	O
panel	O
we	O
have	O
plotted	O
a	O
small	O
training	O
data	B
set	B
consisting	O
of	O
six	O
blue	O
and	O
six	O
orange	O
observations	B
our	O
goal	O
is	O
to	O
make	O
a	O
prediction	B
for	O
the	O
point	O
labeled	O
by	O
the	O
black	O
cross	O
suppose	O
that	O
we	O
choose	O
k	O
then	O
knn	O
will	O
first	O
identify	O
the	O
three	O
observations	B
that	O
are	O
closest	O
to	O
the	O
cross	O
this	O
neighborhood	O
is	O
shown	O
as	O
a	O
circle	O
it	O
consists	O
of	O
two	O
blue	O
points	O
and	O
one	O
orange	O
point	O
resulting	O
in	O
estimated	O
probabilities	O
of	O
for	O
the	O
blue	O
class	O
and	O
for	O
the	O
orange	O
class	O
hence	O
knn	O
will	O
predict	O
that	O
the	O
black	O
cross	O
belongs	O
to	O
the	O
blue	O
class	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
we	O
have	O
applied	O
the	O
knn	O
approach	B
with	O
k	O
at	O
all	O
of	O
the	O
possible	O
values	O
for	O
and	O
and	O
have	O
drawn	O
in	O
the	O
corresponding	O
knn	O
decision	B
boundary	I
despite	O
the	O
fact	O
that	O
it	O
is	O
a	O
very	O
simple	B
approach	B
knn	O
can	O
often	O
produce	O
classifiers	O
that	O
are	O
surprisingly	O
close	O
to	O
the	O
optimal	O
bayes	O
classifier	B
figure	O
displays	O
the	O
knn	O
decision	B
boundary	I
using	O
k	O
when	O
applied	O
to	O
the	O
larger	O
simulated	O
data	B
set	B
from	O
figure	O
notice	O
that	O
even	O
though	O
the	O
true	O
distribution	B
is	O
not	O
known	O
by	O
the	O
knn	O
classifier	B
the	O
knn	O
decision	B
boundary	I
is	O
very	O
close	O
to	O
that	O
of	O
the	O
bayes	O
classifier	B
the	O
test	O
error	B
rate	B
using	O
knn	O
is	O
which	O
is	O
close	O
to	O
the	O
bayes	O
error	B
rate	B
of	O
statistical	O
learning	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
the	O
knn	O
approach	B
using	O
k	O
is	O
illustrated	O
in	O
a	O
simple	B
situation	O
with	O
six	O
blue	O
observations	B
and	O
six	O
orange	O
observations	B
left	O
a	O
test	O
observation	O
at	O
which	O
a	O
predicted	O
class	O
label	O
is	O
desired	O
is	O
shown	O
as	O
a	O
black	O
cross	O
the	O
three	O
closest	O
points	O
to	O
the	O
test	O
observation	O
are	O
identified	O
and	O
it	O
is	O
predicted	O
that	O
the	O
test	O
observation	O
belongs	O
to	O
the	O
most	O
commonly-occurring	O
class	O
in	O
this	O
case	O
blue	O
right	O
the	O
knn	O
decision	B
boundary	I
for	O
this	O
example	O
is	O
shown	O
in	O
black	O
the	O
blue	O
grid	O
indicates	O
the	O
region	O
in	O
which	O
a	O
test	O
observation	O
will	O
be	O
assigned	O
to	O
the	O
blue	O
class	O
and	O
the	O
orange	O
grid	O
indicates	O
the	O
region	O
in	O
which	O
it	O
will	O
be	O
assigned	O
to	O
the	O
orange	O
class	O
the	O
choice	O
of	O
k	O
has	O
a	O
drastic	O
effect	O
on	O
the	O
knn	O
classifier	B
obtained	O
figure	O
displays	O
two	O
knn	O
fits	O
to	O
the	O
simulated	O
data	B
from	O
figure	O
using	O
k	O
and	O
k	O
when	O
k	O
the	O
decision	B
boundary	I
is	O
overly	O
flexible	O
and	O
finds	O
patterns	O
in	O
the	O
data	B
that	O
don	O
t	O
correspond	O
to	O
the	O
bayes	O
decision	B
boundary	I
this	O
corresponds	O
to	O
a	O
classifier	B
that	O
has	O
low	O
bias	B
but	O
very	O
high	O
variance	B
as	O
k	O
grows	O
the	O
method	O
becomes	O
less	O
flexible	O
and	O
produces	O
a	O
decision	B
boundary	I
that	O
is	O
close	O
to	O
linear	B
this	O
corresponds	O
to	O
a	O
low-variance	O
but	O
high-bias	O
classifier	B
on	O
this	O
simulated	O
data	B
set	B
neither	O
k	O
nor	O
k	O
give	O
good	O
predictions	O
they	O
have	O
test	O
error	B
rates	O
of	O
and	O
respectively	O
just	O
as	O
in	O
the	O
regression	B
setting	O
there	O
is	O
not	O
a	O
strong	O
relationship	O
between	O
the	O
training	O
error	B
rate	B
and	O
the	O
test	O
error	B
rate	B
with	O
k	O
the	O
knn	O
training	O
error	B
rate	B
is	O
but	O
the	O
test	O
error	B
rate	B
may	O
be	O
quite	O
high	O
in	O
general	O
as	O
we	O
use	O
more	O
flexible	O
classification	B
methods	O
the	O
training	O
error	B
rate	B
will	O
decline	O
but	O
the	O
test	O
error	B
rate	B
may	O
not	O
in	O
figure	O
we	O
have	O
plotted	O
the	O
knn	O
test	O
and	O
training	O
errors	O
as	O
a	O
function	B
of	O
as	O
increases	O
the	O
method	O
becomes	O
more	O
flexible	O
as	O
in	O
the	O
regression	B
setting	O
the	O
training	O
error	B
rate	B
consistently	O
declines	O
as	O
the	O
flexibility	O
increases	O
however	O
the	O
test	O
error	B
exhibits	O
a	O
characteristic	O
u-shape	O
declining	O
at	O
first	O
a	O
minimum	O
at	O
approximately	O
k	O
before	O
increasing	O
again	O
when	O
the	O
method	O
becomes	O
excessively	O
flexible	O
and	O
overfits	O
assessing	O
model	B
accuracy	O
knn	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
the	O
black	O
curve	O
indicates	O
the	O
knn	O
decision	B
boundary	I
on	O
the	O
data	B
from	O
figure	O
using	O
k	O
the	O
bayes	O
decision	B
boundary	I
is	O
shown	O
as	O
a	O
purple	O
dashed	O
line	B
the	O
knn	O
and	O
bayes	O
decision	O
boundaries	O
are	O
very	O
similar	O
knn	O
knn	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
a	O
comparison	O
of	O
the	O
knn	O
decision	O
boundaries	O
black	O
curves	O
obtained	O
using	O
k	O
and	O
k	O
on	O
the	O
data	B
from	O
figure	O
with	O
k	O
the	O
decision	B
boundary	I
is	O
overly	O
flexible	O
while	O
with	O
k	O
it	O
is	O
not	O
sufficiently	O
flexible	O
the	O
bayes	O
decision	B
boundary	I
is	O
shown	O
as	O
a	O
purple	O
dashed	O
line	B
statistical	O
learning	O
e	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
training	O
errors	O
test	O
errors	O
figure	O
the	O
knn	O
training	O
error	B
rate	B
observations	B
and	O
test	O
error	B
rate	B
observations	B
on	O
the	O
data	B
from	O
figure	O
as	O
the	O
level	B
of	O
flexibility	O
using	O
increases	O
or	O
equivalently	O
as	O
the	O
number	O
of	O
neighbors	O
k	O
decreases	O
the	O
black	O
dashed	O
line	B
indicates	O
the	O
bayes	O
error	B
rate	B
the	O
jumpiness	O
of	O
the	O
curves	O
is	O
due	O
to	O
the	O
small	O
size	O
of	O
the	O
training	O
data	B
set	B
in	O
both	O
the	O
regression	B
and	O
classification	B
settings	O
choosing	O
the	O
correct	O
level	B
of	O
flexibility	O
is	O
critical	O
to	O
the	O
success	O
of	O
any	O
statistical	O
learning	O
method	O
the	O
bias-variance	O
tradeoff	O
and	O
the	O
resulting	O
u-shape	O
in	O
the	O
test	O
error	B
can	O
make	O
this	O
a	O
difficult	O
task	O
in	O
chapter	O
we	O
return	O
to	O
this	O
topic	O
and	O
discuss	O
various	O
methods	O
for	O
estimating	O
test	O
error	B
rates	O
and	O
thereby	O
choosing	O
the	O
optimal	O
level	B
of	O
flexibility	O
for	O
a	O
given	O
statistical	O
learning	O
method	O
lab	O
introduction	O
to	O
r	O
in	O
this	O
lab	O
we	O
will	O
introduce	O
some	O
simple	B
r	O
commands	O
the	O
best	O
way	O
to	O
learn	O
a	O
new	O
language	O
is	O
to	O
try	O
out	O
the	O
commands	O
r	O
can	O
be	O
downloaded	O
from	O
httpcran	O
r-project	O
org	O
basic	O
commands	O
r	O
uses	O
functions	O
to	O
perform	O
operations	O
to	O
run	O
a	O
function	B
called	O
funcname	O
we	O
type	O
where	O
the	O
inputs	O
arguments	O
function	B
argument	B
lab	O
introduction	O
to	O
r	O
and	O
tell	O
r	O
how	O
to	O
run	O
the	O
function	B
a	O
function	B
can	O
have	O
any	O
number	O
of	O
inputs	O
for	O
example	O
to	O
create	O
a	O
vector	B
of	O
numbers	O
we	O
use	O
the	O
function	B
c	O
concatenate	O
any	O
numbers	O
inside	O
the	O
parentheses	O
are	O
joined	O
together	O
the	O
following	O
command	O
instructs	O
r	O
to	O
join	O
together	O
the	O
numbers	O
and	O
and	O
to	O
save	O
them	O
as	O
a	O
vector	B
named	O
x	O
when	O
we	O
type	O
x	O
it	O
gives	O
us	O
back	O
the	O
vector	B
c	O
vector	B
x	O
c	O
x	O
note	O
that	O
the	O
is	O
not	O
part	O
of	O
the	O
command	O
rather	O
it	O
is	O
printed	O
by	O
r	O
to	O
indicate	O
that	O
it	O
is	O
ready	O
for	O
another	O
command	O
to	O
be	O
entered	O
we	O
can	O
also	O
save	O
things	O
using	O
rather	O
than	O
x	O
c	O
x	O
y	O
c	O
hitting	O
the	O
up	O
arrow	O
multiple	B
times	O
will	O
display	O
the	O
previous	O
commands	O
which	O
can	O
then	O
be	O
edited	O
this	O
is	O
useful	O
since	O
one	O
often	O
wishes	O
to	O
repeat	O
a	O
similar	O
command	O
in	O
addition	O
typing	O
will	O
always	O
cause	O
r	O
to	O
open	O
a	O
new	O
help	O
file	O
window	O
with	O
additional	O
information	O
about	O
the	O
function	B
funcname	O
we	O
can	O
tell	O
r	O
to	O
add	O
two	O
sets	O
of	O
numbers	O
together	O
it	O
will	O
then	O
add	O
the	O
first	O
number	O
from	O
x	O
to	O
the	O
first	O
number	O
from	O
y	O
and	O
so	O
on	O
however	O
x	O
and	O
y	O
should	O
be	O
the	O
same	O
length	O
we	O
can	O
check	O
their	O
length	O
using	O
the	O
length	O
function	B
length	O
length	O
x	O
length	O
y	O
x	O
y	O
the	O
ls	O
function	B
allows	O
us	O
to	O
look	O
at	O
a	O
list	O
of	O
all	O
of	O
the	O
objects	O
such	O
as	O
data	B
and	O
functions	O
that	O
we	O
have	O
saved	O
so	O
far	O
the	O
rm	O
function	B
can	O
be	O
used	O
to	O
delete	O
any	O
that	O
we	O
don	O
t	O
want	O
ls	O
rm	O
ls	O
x	O
y	O
rm	O
y	O
ls	O
character	O
it	O
s	O
also	O
possible	O
to	O
remove	O
all	O
objects	O
at	O
once	O
rm	O
list	O
ls	O
statistical	O
learning	O
the	O
matrix	O
function	B
can	O
be	O
used	O
to	O
create	O
a	O
matrix	O
of	O
numbers	O
before	O
we	O
use	O
the	O
matrix	O
function	B
we	O
can	O
learn	O
more	O
about	O
it	O
matrix	O
matrix	O
the	O
help	O
file	O
reveals	O
that	O
the	O
matrix	O
function	B
takes	O
a	O
number	O
of	O
inputs	O
but	O
for	O
now	O
we	O
focus	O
on	O
the	O
first	O
three	O
the	O
data	B
entries	O
in	O
the	O
matrix	O
the	O
number	O
of	O
rows	O
and	O
the	O
number	O
of	O
columns	O
first	O
we	O
create	O
a	O
simple	B
matrix	O
x	O
matrix	O
data	B
c	O
nrow	O
ncol	O
x	O
note	O
that	O
we	O
could	O
just	O
as	O
well	O
omit	O
typing	O
data	B
nrow	O
and	O
ncol	O
in	O
the	O
matrix	O
command	O
above	O
that	O
is	O
we	O
could	O
just	O
type	O
x	O
matrix	O
c	O
and	O
this	O
would	O
have	O
the	O
same	O
effect	O
however	O
it	O
can	O
sometimes	O
be	O
useful	O
to	O
specify	O
the	O
names	O
of	O
the	O
arguments	O
passed	O
in	O
since	O
otherwise	O
r	O
will	O
assume	O
that	O
the	O
function	B
arguments	O
are	O
passed	O
into	O
the	O
function	B
in	O
the	O
same	O
order	O
that	O
is	O
given	O
in	O
the	O
function	B
s	O
help	O
file	O
as	O
this	O
example	O
illustrates	O
by	O
default	B
r	O
creates	O
matrices	O
by	O
successively	O
filling	O
in	O
columns	O
alternatively	O
the	O
byrowtrue	O
option	O
can	O
be	O
used	O
to	O
populate	O
the	O
matrix	O
in	O
order	O
of	O
the	O
rows	O
matrix	O
c	O
byrow	O
true	O
notice	O
that	O
in	O
the	O
above	O
command	O
we	O
did	O
not	O
assign	O
the	O
matrix	O
to	O
a	O
value	O
such	O
as	O
x	O
in	O
this	O
case	O
the	O
matrix	O
is	O
printed	O
to	O
the	O
screen	O
but	O
is	O
not	O
saved	O
for	O
future	O
calculations	O
the	O
sqrt	O
function	B
returns	O
the	O
square	O
root	O
of	O
each	O
element	O
of	O
a	O
vector	B
or	O
matrix	O
the	O
command	O
raises	O
each	O
element	O
of	O
x	O
to	O
the	O
power	B
any	O
powers	O
are	O
possible	O
including	O
fractional	O
or	O
negative	O
powers	O
sqrt	O
sqrt	O
x	O
x	O
the	O
rnorm	O
function	B
generates	O
a	O
vector	B
of	O
random	O
normal	O
variables	O
with	O
first	O
argument	B
n	O
the	O
sample	O
size	O
each	O
time	O
we	O
call	O
this	O
function	B
we	O
will	O
get	O
a	O
different	O
answer	O
here	O
we	O
create	O
two	O
correlated	O
sets	O
of	O
numbers	O
x	O
and	O
y	O
and	O
use	O
the	O
cor	O
function	B
to	O
compute	O
the	O
correlation	B
between	O
them	O
rnorm	O
cor	O
lab	O
introduction	O
to	O
r	O
x	O
rnorm	O
y	O
x	O
rnorm	O
mean	O
sd	O
cor	O
x	O
y	O
by	O
default	B
rnorm	O
creates	O
standard	O
normal	O
random	O
variables	O
with	O
a	O
mean	O
of	O
and	O
a	O
standard	O
deviation	O
of	O
however	O
the	O
mean	O
and	O
standard	O
deviation	O
can	O
be	O
altered	O
using	O
the	O
mean	O
and	O
sd	O
arguments	O
as	O
illustrated	O
above	O
sometimes	O
we	O
want	O
our	O
code	O
to	O
reproduce	O
the	O
exact	O
same	O
set	B
of	O
random	O
numbers	O
we	O
can	O
use	O
the	O
set	B
seed	B
function	B
to	O
do	O
this	O
the	O
set	B
seed	B
function	B
takes	O
an	O
integer	O
argument	B
set	B
seed	B
set	B
seed	B
rnorm	O
we	O
use	O
set	B
seed	B
throughout	O
the	O
labs	O
whenever	O
we	O
perform	O
calculations	O
involving	O
random	O
quantities	O
in	O
general	O
this	O
should	O
allow	O
the	O
user	O
to	O
reproduce	O
our	O
results	O
however	O
it	O
should	O
be	O
noted	O
that	O
as	O
new	O
versions	O
of	O
r	O
become	O
available	O
it	O
is	O
possible	O
that	O
some	O
small	O
discrepancies	O
may	O
form	O
between	O
the	O
book	O
and	O
the	O
output	B
from	O
r	O
the	O
mean	O
and	O
var	O
functions	O
can	O
be	O
used	O
to	O
compute	O
the	O
mean	O
and	O
variance	B
of	O
a	O
vector	B
of	O
numbers	O
applying	O
sqrt	O
to	O
the	O
output	B
of	O
var	O
will	O
give	O
the	O
standard	O
deviation	O
or	O
we	O
can	O
simply	O
use	O
the	O
sd	O
function	B
mean	O
var	O
sd	O
set	B
seed	B
y	O
rnorm	O
mean	O
y	O
var	O
y	O
sqrt	O
var	O
y	O
sd	O
y	O
graphics	O
the	O
plot	B
function	B
is	O
the	O
primary	O
way	O
to	O
plot	B
data	B
in	O
r	O
for	O
instance	O
plotxy	O
produces	O
a	O
scatterplot	B
of	O
the	O
numbers	O
in	O
x	O
versus	O
the	O
numbers	O
in	O
y	O
there	O
are	O
many	O
additional	O
options	O
that	O
can	O
be	O
passed	O
in	O
to	O
the	O
plot	B
function	B
for	O
example	O
passing	O
in	O
the	O
argument	B
xlab	O
will	O
result	O
in	O
a	O
label	O
on	O
the	O
x-axis	O
to	O
find	O
out	O
more	O
information	O
about	O
the	O
plot	B
function	B
type	O
plot	B
x	O
rnorm	O
y	O
rnorm	O
plot	B
y	O
plot	B
xlab	O
this	O
is	O
the	O
x	O
axis	O
ylab	O
this	O
is	O
the	O
y	O
axis	O
main	O
plot	B
of	O
x	O
vs	O
y	O
statistical	O
learning	O
we	O
will	O
often	O
want	O
to	O
save	O
the	O
output	B
of	O
an	O
r	O
plot	B
the	O
command	O
that	O
we	O
use	O
to	O
do	O
this	O
will	O
depend	O
on	O
the	O
file	O
type	O
that	O
we	O
would	O
like	O
to	O
create	O
for	O
instance	O
to	O
create	O
a	O
pdf	O
we	O
use	O
the	O
pdf	O
function	B
and	O
to	O
create	O
a	O
jpeg	O
we	O
use	O
the	O
jpeg	O
function	B
pdf	O
jpeg	O
pdf	O
figure	O
pdf	O
plot	B
col	O
green	O
dev	O
off	O
null	B
device	O
the	O
function	B
dev	O
off	O
indicates	O
to	O
r	O
that	O
we	O
are	O
done	O
creating	O
the	O
plot	B
alternatively	O
we	O
can	O
simply	O
copy	O
the	O
plot	B
window	O
and	O
paste	O
it	O
into	O
an	O
appropriate	O
file	O
type	O
such	O
as	O
a	O
word	O
document	O
the	O
function	B
seq	O
can	O
be	O
used	O
to	O
create	O
a	O
sequence	O
of	O
numbers	O
for	O
instance	O
seqab	O
makes	O
a	O
vector	B
of	O
integers	O
between	O
a	O
and	O
b	O
there	O
are	O
many	O
other	O
options	O
for	O
instance	O
makes	O
a	O
sequence	O
of	O
numbers	O
that	O
are	O
equally	O
spaced	O
between	O
and	O
typing	O
is	O
a	O
shorthand	O
for	O
for	O
integer	O
arguments	O
dev	O
off	O
seq	O
x	O
seq	O
x	O
x	O
x	O
x	O
seq	O
pi	O
pi	O
length	O
we	O
will	O
now	O
create	O
some	O
more	O
sophisticated	O
plots	O
the	O
contour	O
function	B
produces	O
a	O
contour	B
plot	B
in	O
order	O
to	O
represent	O
three-dimensional	O
data	B
it	O
is	O
like	O
a	O
topographical	O
map	O
it	O
takes	O
three	O
arguments	O
contour	O
contour	B
plot	B
a	O
vector	B
of	O
the	O
x	O
values	O
first	O
dimension	O
a	O
vector	B
of	O
the	O
y	O
values	O
second	O
dimension	O
and	O
a	O
matrix	O
whose	O
elements	O
correspond	O
to	O
the	O
z	O
value	O
third	O
dimen	O
sion	O
for	O
each	O
pair	O
of	O
coordinates	O
as	O
with	O
the	O
plot	B
function	B
there	O
are	O
many	O
other	O
inputs	O
that	O
can	O
be	O
used	O
to	O
fine-tune	O
the	O
output	B
of	O
the	O
contour	O
function	B
to	O
learn	O
more	O
about	O
these	O
take	O
a	O
look	O
at	O
the	O
help	O
file	O
by	O
typing	O
y	O
x	O
f	O
outer	O
function	B
y	O
cos	O
y	O
x	O
contour	O
f	O
contour	O
nlevels	O
add	O
t	O
fa	O
f	O
t	O
f	O
contour	O
fa	O
nlevels	O
the	O
image	O
function	B
works	O
the	O
same	O
way	O
as	O
contour	O
except	O
that	O
it	O
produces	O
a	O
color-coded	O
plot	B
whose	O
colors	O
depend	O
on	O
the	O
z	O
value	O
this	O
is	O
image	O
lab	O
introduction	O
to	O
r	O
known	O
as	O
a	O
heatmap	B
and	O
is	O
sometimes	O
used	O
to	O
plot	B
temperature	O
in	O
weather	O
forecasts	O
alternatively	O
persp	O
can	O
be	O
used	O
to	O
produce	O
a	O
three-dimensional	O
plot	B
the	O
arguments	O
theta	O
and	O
phi	O
control	O
the	O
angles	O
at	O
which	O
the	O
plot	B
is	O
viewed	O
heatmap	B
persp	O
image	O
fa	O
persp	O
fa	O
persp	O
fa	O
theta	O
persp	O
fa	O
theta	O
phi	O
persp	O
fa	O
theta	O
phi	O
persp	O
fa	O
theta	O
phi	O
indexing	O
data	B
we	O
often	O
wish	O
to	O
examine	O
part	O
of	O
a	O
set	B
of	O
data	B
suppose	O
that	O
our	O
data	B
is	O
stored	O
in	O
the	O
matrix	O
a	O
a	O
matrix	O
a	O
then	O
typing	O
a	O
will	O
select	O
the	O
element	O
corresponding	O
to	O
the	O
second	O
row	O
and	O
the	O
third	O
column	O
the	O
first	O
number	O
after	O
the	O
open-bracket	O
symbol	O
always	O
refers	O
to	O
the	O
row	O
and	O
the	O
second	O
number	O
always	O
refers	O
to	O
the	O
column	O
we	O
can	O
also	O
select	O
multiple	B
rows	O
and	O
columns	O
at	O
a	O
time	O
by	O
providing	O
vectors	O
as	O
the	O
indices	O
a	O
c	O
c	O
a	O
a	O
a	O
statistical	O
learning	O
the	O
last	O
two	O
examples	O
include	O
either	O
no	O
index	O
for	O
the	O
columns	O
or	O
no	O
index	O
for	O
the	O
rows	O
these	O
indicate	O
that	O
r	O
should	O
include	O
all	O
columns	O
or	O
all	O
rows	O
respectively	O
r	O
treats	O
a	O
single	B
row	O
or	O
column	O
of	O
a	O
matrix	O
as	O
a	O
vector	B
a	O
the	O
use	O
of	O
a	O
negative	O
sign	O
in	O
the	O
index	O
tells	O
r	O
to	O
keep	O
all	O
rows	O
or	O
columns	O
except	O
those	O
indicated	O
in	O
the	O
index	O
a	O
c	O
a	O
c	O
the	O
dim	O
function	B
outputs	O
the	O
number	O
of	O
rows	O
followed	O
by	O
the	O
number	O
of	O
columns	O
of	O
a	O
given	O
matrix	O
dim	O
dim	O
a	O
loading	O
data	B
for	O
most	O
analyses	O
the	O
first	O
step	O
involves	O
importing	O
a	O
data	B
set	B
into	O
r	O
the	O
read	O
table	O
function	B
is	O
one	O
of	O
the	O
primary	O
ways	O
to	O
do	O
this	O
the	O
help	O
file	O
contains	O
details	O
about	O
how	O
to	O
use	O
this	O
function	B
we	O
can	O
use	O
the	O
function	B
write	O
table	O
to	O
export	O
data	B
before	O
attempting	O
to	O
load	O
a	O
data	B
set	B
we	O
must	O
make	O
sure	O
that	O
r	O
knows	O
to	O
search	O
for	O
the	O
data	B
in	O
the	O
proper	O
directory	O
for	O
example	O
on	O
a	O
windows	O
system	O
one	O
could	O
select	O
the	O
directory	O
using	O
the	O
change	O
dir	O
option	O
under	O
the	O
file	O
menu	O
however	O
the	O
details	O
of	O
how	O
to	O
do	O
this	O
depend	O
on	O
the	O
operating	O
system	O
windows	O
mac	O
unix	O
that	O
is	O
being	O
used	O
and	O
so	O
we	O
do	O
not	O
give	O
further	O
details	O
here	O
we	O
begin	O
by	O
loading	O
in	O
the	O
auto	B
data	B
set	B
this	O
data	B
is	O
part	O
of	O
the	O
islr	O
library	O
discuss	O
libraries	O
in	O
chapter	O
but	O
to	O
illustrate	O
the	O
read	O
table	O
function	B
we	O
load	O
it	O
now	O
from	O
a	O
text	O
file	O
the	O
following	O
command	O
will	O
load	O
the	O
auto	B
data	B
file	O
into	O
r	O
and	O
store	O
it	O
as	O
an	O
object	O
called	O
auto	B
in	O
a	O
format	O
referred	O
to	O
as	O
a	O
data	B
frame	I
text	O
file	O
can	O
be	O
obtained	O
from	O
this	O
book	O
s	O
website	O
once	O
the	O
data	B
has	O
been	O
loaded	O
the	O
fix	O
function	B
can	O
be	O
used	O
to	O
view	O
it	O
in	O
a	O
spreadsheet	O
like	O
window	O
however	O
the	O
window	O
must	O
be	O
closed	O
before	O
further	O
r	O
commands	O
can	O
be	O
entered	O
read	O
table	O
write	O
table	O
data	B
frame	I
auto	B
read	O
table	O
auto	B
data	B
fix	O
auto	B
lab	O
introduction	O
to	O
r	O
note	O
that	O
auto	B
data	B
is	O
simply	O
a	O
text	O
file	O
which	O
you	O
could	O
alternatively	O
open	O
on	O
your	O
computer	O
using	O
a	O
standard	O
text	O
editor	O
it	O
is	O
often	O
a	O
good	O
idea	O
to	O
view	O
a	O
data	B
set	B
using	O
a	O
text	O
editor	O
or	O
other	O
software	O
such	O
as	O
excel	O
before	O
loading	O
it	O
into	O
r	O
this	O
particular	O
data	B
set	B
has	O
not	O
been	O
loaded	O
correctly	O
because	O
r	O
has	O
assumed	O
that	O
the	O
variable	B
names	O
are	O
part	O
of	O
the	O
data	B
and	O
so	O
has	O
included	O
them	O
in	O
the	O
first	O
row	O
the	O
data	B
set	B
also	O
includes	O
a	O
number	O
of	O
missing	O
observations	B
indicated	O
by	O
a	O
question	O
mark	O
missing	O
values	O
are	O
a	O
common	O
occurrence	O
in	O
real	O
data	B
sets	O
using	O
the	O
option	O
headert	O
headertrue	O
in	O
the	O
read	O
table	O
function	B
tells	O
r	O
that	O
the	O
first	O
line	B
of	O
the	O
file	O
contains	O
the	O
variable	B
names	O
and	O
using	O
the	O
option	O
na	O
strings	O
tells	O
r	O
that	O
any	O
time	O
it	O
sees	O
a	O
particular	O
character	O
or	O
set	B
of	O
characters	O
as	O
a	O
question	O
mark	O
it	O
should	O
be	O
treated	O
as	O
a	O
missing	O
element	O
of	O
the	O
data	B
matrix	O
auto	B
read	O
table	O
auto	B
data	B
header	O
na	O
strings	O
fix	O
auto	B
excel	O
is	O
a	O
common-format	O
data	B
storage	O
program	O
an	O
easy	O
way	O
to	O
load	O
such	O
data	B
into	O
r	O
is	O
to	O
save	O
it	O
as	O
a	O
csv	O
separated	O
value	O
file	O
and	O
then	O
use	O
the	O
read	O
csv	O
function	B
to	O
load	O
it	O
in	O
auto	B
read	O
csv	O
auto	B
csv	O
header	O
na	O
strings	O
fix	O
auto	B
dim	O
auto	B
auto	B
the	O
dim	O
function	B
tells	O
us	O
that	O
the	O
data	B
has	O
observations	B
or	O
rows	O
and	O
nine	O
variables	O
or	O
columns	O
there	O
are	O
various	O
ways	O
to	O
deal	O
with	O
the	O
missing	B
data	B
in	O
this	O
case	O
only	O
five	O
of	O
the	O
rows	O
contain	O
missing	O
observations	B
and	O
so	O
we	O
choose	O
to	O
use	O
the	O
na	O
omit	O
function	B
to	O
simply	O
remove	O
these	O
rows	O
auto	B
na	O
omit	O
auto	B
dim	O
auto	B
once	O
the	O
data	B
are	O
loaded	O
correctly	O
we	O
can	O
use	O
names	O
to	O
check	O
the	O
variable	B
names	O
names	O
auto	B
mpg	O
weight	O
name	O
cylinders	O
a	O
c	O
c	O
e	O
l	O
e	O
r	O
a	O
t	O
i	O
o	O
n	O
year	O
d	O
i	O
s	O
p	O
l	O
a	O
c	O
e	O
m	O
e	O
n	O
t	O
horsepower	O
origin	O
dim	O
na	O
omit	O
names	O
additional	O
graphical	O
and	O
numerical	O
summaries	O
we	O
can	O
use	O
the	O
plot	B
function	B
to	O
produce	O
scatterplots	O
of	O
the	O
quantitative	B
variables	O
however	O
simply	O
typing	O
the	O
variable	B
names	O
will	O
produce	O
an	O
error	B
message	O
because	O
r	O
does	O
not	O
know	O
to	O
look	O
in	O
the	O
auto	B
data	B
set	B
for	O
those	O
variables	O
scatterplot	B
statistical	O
learning	O
plot	B
cylinders	O
mpg	O
error	B
in	O
plot	B
cylinders	O
mpg	O
object	O
cylinders	O
not	O
found	O
to	O
refer	O
to	O
a	O
variable	B
we	O
must	O
type	O
the	O
data	B
set	B
and	O
the	O
variable	B
name	O
joined	O
with	O
a	O
symbol	O
alternatively	O
we	O
can	O
use	O
the	O
attach	O
function	B
in	O
order	O
to	O
tell	O
r	O
to	O
make	O
the	O
variables	O
in	O
this	O
data	B
frame	I
available	O
by	O
name	O
attach	O
plot	B
autocylinders	O
autompg	O
attach	O
auto	B
plot	B
cylinders	O
mpg	O
the	O
cylinders	O
variable	B
is	O
stored	O
as	O
a	O
numeric	O
vector	B
so	O
r	O
has	O
treated	O
it	O
as	O
quantitative	B
however	O
since	O
there	O
are	O
only	O
a	O
small	O
number	O
of	O
possible	O
values	O
for	O
cylinders	O
one	O
may	O
prefer	O
to	O
treat	O
it	O
as	O
a	O
qualitative	B
variable	B
the	O
as	O
factor	B
function	B
converts	O
quantitative	B
variables	O
into	O
qualitative	B
variables	O
as	O
factor	B
cylinders	O
as	O
factor	B
cylinders	O
if	O
the	O
variable	B
plotted	O
on	O
the	O
x-axis	O
is	O
categorial	O
then	O
boxplots	O
will	O
automatically	O
be	O
produced	O
by	O
the	O
plot	B
function	B
as	O
usual	O
a	O
number	O
of	O
options	O
can	O
be	O
specified	O
in	O
order	O
to	O
customize	O
the	O
plots	O
boxplot	B
plot	B
cylinders	O
mpg	O
plot	B
cylinders	O
mpg	O
col	O
red	O
plot	B
cylinders	O
mpg	O
col	O
red	O
varwidth	O
t	O
plot	B
cylinders	O
mpg	O
col	O
red	O
varwidth	O
horizontal	O
t	O
plot	B
cylinders	O
mpg	O
col	O
red	O
varwidth	O
xlab	O
cylinders	O
ylab	O
mpg	O
the	O
hist	O
function	B
can	O
be	O
used	O
to	O
plot	B
a	O
histogram	B
note	O
that	O
has	O
the	O
same	O
effect	O
as	O
colred	O
hist	O
mpg	O
hist	O
mpg	O
col	O
hist	O
mpg	O
col	O
breaks	O
hist	O
histogram	B
the	O
pairs	O
function	B
creates	O
a	O
scatterplot	B
matrix	I
i	O
e	O
a	O
scatterplot	B
for	O
every	O
pair	O
of	O
variables	O
for	O
any	O
given	O
data	B
set	B
we	O
can	O
also	O
produce	O
scatterplots	O
for	O
just	O
a	O
subset	O
of	O
the	O
variables	O
scatterplot	B
matrix	I
pairs	O
auto	B
pairs	O
mpg	O
d	O
i	O
s	O
p	O
l	O
a	O
c	O
e	O
m	O
e	O
n	O
t	O
horsepowe	O
r	O
weight	O
acceleration	O
auto	B
in	O
conjunction	O
with	O
the	O
plot	B
function	B
identify	O
provides	O
a	O
useful	O
interactive	O
method	O
for	O
identifying	O
the	O
value	O
for	O
a	O
particular	O
variable	B
for	O
points	O
on	O
a	O
plot	B
we	O
pass	O
in	O
three	O
arguments	O
to	O
identify	O
the	O
x-axis	O
variable	B
the	O
y-axis	O
variable	B
and	O
the	O
variable	B
whose	O
values	O
we	O
would	O
like	O
to	O
see	O
printed	O
for	O
each	O
point	O
then	O
clicking	O
on	O
a	O
given	O
point	O
in	O
the	O
plot	B
will	O
cause	O
r	O
to	O
print	O
the	O
value	O
of	O
the	O
variable	B
of	O
interest	O
right-clicking	O
on	O
the	O
plot	B
will	O
exit	O
the	O
identify	O
function	B
on	O
a	O
mac	O
the	O
numbers	O
printed	O
under	O
the	O
identify	O
function	B
correspond	O
to	O
the	O
rows	O
for	O
the	O
selected	O
points	O
identify	O
plot	B
horsepower	O
mpg	O
identify	O
horsepower	O
mpg	O
name	O
lab	O
introduction	O
to	O
r	O
the	O
summary	O
function	B
produces	O
a	O
numerical	O
summary	O
of	O
each	O
variable	B
in	O
a	O
particular	O
data	B
set	B
summary	O
summary	O
auto	B
mpg	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
horsepower	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
cylinders	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
d	O
i	O
s	O
p	O
l	O
a	O
c	O
e	O
m	O
e	O
n	O
t	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
weight	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
a	O
c	O
c	O
e	O
l	O
e	O
r	O
a	O
t	O
i	O
o	O
n	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
year	O
origin	O
name	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
amc	O
matador	O
ford	O
pinto	O
toyota	O
corolla	O
amc	O
gremlin	O
amc	O
hornet	O
chevrolet	O
chevette	O
other	O
for	O
qualitative	B
variables	O
such	O
as	O
name	O
r	O
will	O
list	O
the	O
number	O
of	O
observations	B
that	O
fall	O
in	O
each	O
category	O
we	O
can	O
also	O
produce	O
a	O
summary	O
of	O
just	O
a	O
single	B
variable	B
summary	O
mpg	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
once	O
we	O
have	O
finished	O
using	O
r	O
we	O
type	O
q	O
in	O
order	O
to	O
shut	O
it	O
down	O
or	O
quit	O
when	O
exiting	O
r	O
we	O
have	O
the	O
option	O
to	O
save	O
the	O
current	O
workspace	B
so	O
that	O
all	O
objects	O
as	O
data	B
sets	O
that	O
we	O
have	O
created	O
in	O
this	O
r	O
session	O
will	O
be	O
available	O
next	O
time	O
before	O
exiting	O
r	O
we	O
may	O
want	O
to	O
save	O
a	O
record	O
of	O
all	O
of	O
the	O
commands	O
that	O
we	O
typed	O
in	O
the	O
most	O
recent	O
session	O
this	O
can	O
be	O
accomplished	O
using	O
the	O
savehistory	O
function	B
next	O
time	O
we	O
enter	O
r	O
we	O
can	O
load	O
that	O
history	O
using	O
the	O
loadhistory	O
function	B
q	O
workspace	B
savehistory	O
loadhistory	O
statistical	O
learning	O
exercises	O
conceptual	O
for	O
each	O
of	O
parts	O
through	O
indicate	O
whether	O
we	O
would	O
generally	O
expect	O
the	O
performance	O
of	O
a	O
flexible	O
statistical	O
learning	O
method	O
to	O
be	O
better	O
or	O
worse	O
than	O
an	O
inflexible	O
method	O
justify	O
your	O
answer	O
the	O
sample	O
size	O
n	O
is	O
extremely	O
large	O
and	O
the	O
number	O
of	O
predic	O
tors	O
p	O
is	O
small	O
the	O
number	O
of	O
predictors	O
p	O
is	O
extremely	O
large	O
and	O
the	O
number	O
of	O
observations	B
n	O
is	O
small	O
the	O
relationship	O
between	O
the	O
predictors	O
and	O
response	B
is	O
highly	O
non-linear	B
the	O
variance	B
of	O
the	O
error	B
terms	O
i	O
e	O
var	O
is	O
extremely	O
high	O
explain	O
whether	O
each	O
scenario	O
is	O
a	O
classification	B
or	O
regression	B
problem	O
and	O
indicate	O
whether	O
we	O
are	O
most	O
interested	O
in	O
inference	B
or	O
prediction	B
finally	O
provide	O
n	O
and	O
p	O
we	O
collect	O
a	O
set	B
of	O
data	B
on	O
the	O
top	O
firms	O
in	O
the	O
us	O
for	O
each	O
firm	O
we	O
record	O
profit	O
number	O
of	O
employees	O
industry	O
and	O
the	O
ceo	O
salary	O
we	O
are	O
interested	O
in	O
understanding	O
which	O
factors	O
affect	O
ceo	O
salary	O
we	O
are	O
considering	O
launching	O
a	O
new	O
product	O
and	O
wish	O
to	O
know	O
whether	O
it	O
will	O
be	O
a	O
success	O
or	O
a	O
failure	O
we	O
collect	O
data	B
on	O
similar	O
products	O
that	O
were	O
previously	O
launched	O
for	O
each	O
product	O
we	O
have	O
recorded	O
whether	O
it	O
was	O
a	O
success	O
or	O
failure	O
price	O
charged	O
for	O
the	O
product	O
marketing	O
budget	O
competition	O
price	O
and	O
ten	O
other	O
variables	O
we	O
are	O
interest	O
ed	O
in	O
predicting	O
the	O
change	O
in	O
the	O
usdeuro	O
exchange	O
rate	B
in	O
relation	O
to	O
the	O
weekly	B
changes	O
in	O
the	O
world	O
stock	O
markets	O
hence	O
we	O
collect	O
weekly	B
data	B
for	O
all	O
of	O
for	O
each	O
week	O
we	O
record	O
the	O
change	O
in	O
the	O
usdeuro	O
the	O
change	O
in	O
the	O
us	O
market	O
the	O
change	O
in	O
the	O
british	O
market	O
and	O
the	O
change	O
in	O
the	O
german	O
market	O
we	O
now	O
revisit	O
the	O
bias-variance	O
decomposition	B
provide	O
a	O
sketch	O
of	O
typical	O
bias	B
variance	B
training	O
error	B
test	O
error	B
and	O
bayes	O
irreducible	B
error	B
curves	O
on	O
a	O
single	B
plot	B
as	O
we	O
go	O
from	O
less	O
flexible	O
statistical	O
learning	O
methods	O
towards	O
more	O
flexible	O
approaches	O
the	O
x-axis	O
should	O
represent	O
exercises	O
the	O
amount	O
of	O
flexibility	O
in	O
the	O
method	O
and	O
the	O
y-axis	O
should	O
represent	O
the	O
values	O
for	O
each	O
curve	O
there	O
should	O
be	O
five	O
curves	O
make	O
sure	O
to	O
label	O
each	O
one	O
explain	O
why	O
each	O
of	O
the	O
five	O
curves	O
has	O
the	O
shape	O
displayed	O
in	O
part	O
you	O
will	O
now	O
think	O
of	O
some	O
real-life	O
applications	O
for	O
statistical	O
learn	O
ing	O
describe	O
three	O
real-life	O
applications	O
in	O
which	O
classification	B
might	O
be	O
useful	O
describe	O
the	O
response	B
as	O
well	O
as	O
the	O
predictors	O
is	O
the	O
goal	O
of	O
each	O
application	O
inference	B
or	O
prediction	B
explain	O
your	O
answer	O
describe	O
three	O
real-life	O
applications	O
in	O
which	O
regression	B
might	O
be	O
useful	O
describe	O
the	O
response	B
as	O
well	O
as	O
the	O
predictors	O
is	O
the	O
goal	O
of	O
each	O
application	O
inference	B
or	O
prediction	B
explain	O
your	O
answer	O
describe	O
three	O
real-life	O
applications	O
in	O
which	O
cluster	B
analysis	B
might	O
be	O
useful	O
what	O
are	O
the	O
advantages	O
and	O
disadvantages	O
of	O
a	O
very	O
flexible	O
a	O
less	O
flexible	O
approach	B
for	O
regression	B
or	O
classification	B
under	O
what	O
circumstances	O
might	O
a	O
more	O
flexible	O
approach	B
be	O
preferred	O
to	O
a	O
less	O
flexible	O
approach	B
when	O
might	O
a	O
less	O
flexible	O
approach	B
be	O
preferred	O
describe	O
the	O
differences	O
between	O
a	O
parametric	B
and	O
a	O
non-parametric	B
statistical	O
learning	O
approach	B
what	O
are	O
the	O
advantages	O
of	O
a	O
parametric	B
approach	B
to	O
regression	B
or	O
classification	B
opposed	O
to	O
a	O
nonparametric	O
approach	B
what	O
are	O
its	O
disadvantages	O
the	O
table	O
below	O
provides	O
a	O
training	O
data	B
set	B
containing	O
six	O
observa	O
tions	O
three	O
predictors	O
and	O
one	O
qualitative	B
response	B
variable	B
obs	O
y	O
red	O
red	O
red	O
green	O
green	O
red	O
suppose	O
we	O
wish	O
to	O
use	O
this	O
data	B
set	B
to	O
make	O
a	O
prediction	B
for	O
y	O
when	O
using	O
k-nearest	O
neighbors	O
compute	O
the	O
euclidean	B
distance	I
between	O
each	O
observation	O
and	O
the	O
test	O
point	O
statistical	O
learning	O
what	O
is	O
our	O
prediction	B
with	O
k	O
why	O
what	O
is	O
our	O
prediction	B
with	O
k	O
why	O
if	O
the	O
bayes	O
decision	B
boundary	I
in	O
this	O
problem	O
is	O
highly	O
nonlinear	O
then	O
would	O
we	O
expect	O
the	O
best	O
value	O
for	O
k	O
to	O
be	O
large	O
or	O
small	O
why	O
applied	O
this	O
exercise	O
relates	O
to	O
the	O
college	B
data	B
set	B
which	O
can	O
be	O
found	O
in	O
the	O
file	O
college	B
csv	O
it	O
contains	O
a	O
number	O
of	O
variables	O
for	O
different	O
universities	O
and	O
colleges	O
in	O
the	O
us	O
the	O
variables	O
are	O
private	O
publicprivate	O
indicator	B
apps	O
number	O
of	O
applications	O
received	O
accept	O
number	O
of	O
applicants	O
accepted	O
enroll	O
number	O
of	O
new	O
students	O
enrolled	O
new	O
students	O
from	O
top	O
of	O
high	O
school	O
class	O
new	O
students	O
from	O
top	O
of	O
high	O
school	O
class	O
f	O
undergrad	O
number	O
of	O
full-time	O
undergraduates	O
p	O
undergrad	O
number	O
of	O
part-time	O
undergraduates	O
outstate	O
out-of-state	O
tuition	O
room	O
board	O
room	O
and	O
board	O
costs	O
books	O
estimated	O
book	O
costs	O
personal	O
estimated	O
personal	O
spending	O
phd	O
percent	O
of	O
faculty	O
with	O
ph	O
d	O
s	O
terminal	B
percent	O
of	O
faculty	O
with	O
terminal	B
degree	O
s	O
f	O
ratio	O
studentfaculty	O
ratio	O
perc	O
alumni	O
percent	O
of	O
alumni	O
who	O
donate	O
expend	O
instructional	O
expenditure	O
per	O
student	O
grad	O
rate	B
graduation	O
rate	B
before	O
reading	O
the	O
data	B
into	O
r	O
it	O
can	O
be	O
viewed	O
in	O
excel	O
or	O
a	O
text	O
editor	O
use	O
the	O
read	O
csv	O
function	B
to	O
read	O
the	O
data	B
into	O
r	O
call	O
the	O
loaded	O
data	B
college	B
make	O
sure	O
that	O
you	O
have	O
the	O
directory	O
set	B
to	O
the	O
correct	O
location	O
for	O
the	O
data	B
look	O
at	O
the	O
data	B
using	O
the	O
fix	O
function	B
you	O
should	O
notice	O
that	O
the	O
first	O
column	O
is	O
just	O
the	O
name	O
of	O
each	O
university	O
we	O
don	O
t	O
really	O
want	O
r	O
to	O
treat	O
this	O
as	O
data	B
however	O
it	O
may	O
be	O
handy	O
to	O
have	O
these	O
names	O
for	O
later	O
try	O
the	O
following	O
commands	O
exercises	O
rownames	O
college	B
college	B
fix	O
college	B
you	O
should	O
see	O
that	O
there	O
is	O
now	O
a	O
row	O
names	O
column	O
with	O
the	O
name	O
of	O
each	O
university	O
recorded	O
this	O
means	O
that	O
r	O
has	O
given	O
each	O
row	O
a	O
name	O
corresponding	O
to	O
the	O
appropriate	O
university	O
r	O
will	O
not	O
try	O
to	O
perform	O
calculations	O
on	O
the	O
row	O
names	O
however	O
we	O
still	O
need	O
to	O
eliminate	O
the	O
first	O
column	O
in	O
the	O
data	B
where	O
the	O
names	O
are	O
stored	O
try	O
college	B
college	B
fix	O
college	B
now	O
you	O
should	O
see	O
that	O
the	O
first	O
data	B
column	O
is	O
private	O
note	O
that	O
another	O
column	O
labeled	O
row	O
names	O
now	O
appears	O
before	O
the	O
private	O
column	O
however	O
this	O
is	O
not	O
a	O
data	B
column	O
but	O
rather	O
the	O
name	O
that	O
r	O
is	O
giving	O
to	O
each	O
row	O
i	O
use	O
the	O
summary	O
function	B
to	O
produce	O
a	O
numerical	O
summary	O
of	O
the	O
variables	O
in	O
the	O
data	B
set	B
ii	O
use	O
the	O
pairs	O
function	B
to	O
produce	O
a	O
scatterplot	B
matrix	I
of	O
the	O
first	O
ten	O
columns	O
or	O
variables	O
of	O
the	O
data	B
recall	B
that	O
you	O
can	O
reference	O
the	O
first	O
ten	O
columns	O
of	O
a	O
matrix	O
a	O
using	O
iii	O
use	O
the	O
plot	B
function	B
to	O
produce	O
side-by-side	O
boxplots	O
of	O
outstate	O
versus	O
private	O
iv	O
create	O
a	O
new	O
qualitative	B
variable	B
called	O
elite	O
by	O
binning	O
the	O
variable	B
we	O
are	O
going	O
to	O
divide	O
universities	O
into	O
two	O
groups	O
based	O
on	O
whether	O
or	O
not	O
the	O
proportion	O
of	O
students	O
coming	O
from	O
the	O
top	O
of	O
their	O
high	O
school	O
classes	O
exceeds	O
elite	O
rep	O
no	O
nrow	O
college	B
elite	O
pe	O
rc	O
yes	O
elite	O
as	O
factor	B
elite	O
college	B
data	B
frame	I
college	B
elite	O
use	O
the	O
summary	O
function	B
to	O
see	O
how	O
many	O
elite	O
universities	O
there	O
are	O
now	O
use	O
the	O
plot	B
function	B
to	O
produce	O
side-by-side	O
boxplots	O
of	O
outstate	O
versus	O
elite	O
v	O
use	O
the	O
hist	O
function	B
to	O
produce	O
some	O
histograms	O
with	O
differing	O
numbers	O
of	O
bins	O
for	O
a	O
few	O
of	O
the	O
quantitative	B
variables	O
you	O
may	O
find	O
the	O
command	O
useful	O
it	O
will	O
divide	O
the	O
print	O
window	O
into	O
four	O
regions	O
so	O
that	O
four	O
plots	O
can	O
be	O
made	O
simultaneously	O
modifying	O
the	O
arguments	O
to	O
this	O
function	B
will	O
divide	O
the	O
screen	O
in	O
other	O
ways	O
vi	O
continue	O
exploring	O
the	O
data	B
and	O
provide	O
a	O
brief	O
summary	O
of	O
what	O
you	O
discover	O
statistical	O
learning	O
this	O
exercise	O
involves	O
the	O
auto	B
data	B
set	B
studied	O
in	O
the	O
lab	O
make	O
sure	O
that	O
the	O
missing	O
values	O
have	O
been	O
removed	O
from	O
the	O
data	B
which	O
of	O
the	O
predictors	O
are	O
quantitative	B
and	O
which	O
are	O
quali	O
tative	O
what	O
is	O
the	O
range	O
of	O
each	O
quantitative	B
predictor	B
you	O
can	O
an	O
swer	O
this	O
using	O
the	O
range	O
function	B
what	O
is	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
each	O
quantitative	B
range	O
predictor	B
now	O
remove	O
the	O
through	O
observations	B
what	O
is	O
the	O
range	O
mean	O
and	O
standard	O
deviation	O
of	O
each	O
predictor	B
in	O
the	O
subset	O
of	O
the	O
data	B
that	O
remains	O
using	O
the	O
full	O
data	B
set	B
investigate	O
the	O
predictors	O
graphically	O
using	O
scatterplots	O
or	O
other	O
tools	O
of	O
your	O
choice	O
create	O
some	O
plots	O
highlighting	O
the	O
relationships	O
among	O
the	O
predictors	O
comment	O
on	O
your	O
findings	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
gas	O
mileage	O
on	O
the	O
basis	B
of	O
the	O
other	O
variables	O
do	O
your	O
plots	O
suggest	O
that	O
any	O
of	O
the	O
other	O
variables	O
might	O
be	O
useful	O
in	O
predicting	O
mpg	O
justify	O
your	O
answer	O
this	O
exercise	O
involves	O
the	O
boston	B
housing	O
data	B
set	B
to	O
begin	O
load	O
in	O
the	O
boston	B
data	B
set	B
the	O
boston	B
data	B
set	B
is	O
part	O
of	O
the	O
mass	O
library	O
in	O
r	O
library	O
mass	O
now	O
the	O
data	B
set	B
is	O
contained	O
in	O
the	O
object	O
boston	B
boston	B
read	O
about	O
the	O
data	B
set	B
boston	B
how	O
many	O
rows	O
are	O
in	O
this	O
data	B
set	B
how	O
many	O
columns	O
what	O
do	O
the	O
rows	O
and	O
columns	O
represent	O
make	O
some	O
pairwise	O
scatterplots	O
of	O
the	O
predictors	O
in	O
this	O
data	B
set	B
describe	O
your	O
findings	O
are	O
any	O
of	O
the	O
predictors	O
associated	O
with	O
per	O
capita	O
crime	O
rate	B
if	O
so	O
explain	O
the	O
relationship	O
do	O
any	O
of	O
the	O
suburbs	O
of	O
boston	B
appear	O
to	O
have	O
particularly	O
high	O
crime	O
rates	O
tax	O
rates	O
pupil-teacher	O
ratios	O
comment	O
on	O
the	O
range	O
of	O
each	O
predictor	B
how	O
many	O
of	O
the	O
suburbs	O
in	O
this	O
data	B
set	B
bound	O
the	O
charles	O
river	O
exercises	O
what	O
is	O
the	O
median	O
pupil-teacher	O
ratio	O
among	O
the	O
towns	O
in	O
this	O
data	B
set	B
which	O
suburb	O
of	O
boston	B
has	O
lowest	O
median	O
value	O
of	O
owneroccupied	O
homes	O
what	O
are	O
the	O
values	O
of	O
the	O
other	O
predictors	O
for	O
that	O
suburb	O
and	O
how	O
do	O
those	O
values	O
compare	O
to	O
the	O
overall	O
ranges	O
for	O
those	O
predictors	O
comment	O
on	O
your	O
findings	O
in	O
this	O
data	B
set	B
how	O
many	O
of	O
the	O
suburbs	O
average	B
more	O
than	O
seven	O
rooms	O
per	O
dwelling	O
more	O
than	O
eight	O
rooms	O
per	O
dwelling	O
comment	O
on	O
the	O
suburbs	O
that	O
average	B
more	O
than	O
eight	O
rooms	O
per	O
dwelling	O
linear	B
regression	B
this	O
chapter	O
is	O
about	O
linear	B
regression	B
a	O
very	O
simple	B
approach	B
for	O
supervised	B
learning	I
in	O
particular	O
linear	B
regression	B
is	O
a	O
useful	O
tool	O
for	O
predicting	O
a	O
quantitative	B
response	B
linear	B
regression	B
has	O
been	O
around	O
for	O
a	O
long	O
time	O
and	O
is	O
the	O
topic	O
of	O
innumerable	O
textbooks	O
though	O
it	O
may	O
seem	O
somewhat	O
dull	O
compared	O
to	O
some	O
of	O
the	O
more	O
modern	O
statistical	O
learning	O
approaches	O
described	O
in	O
later	O
chapters	O
of	O
this	O
book	O
linear	B
regression	B
is	O
still	O
a	O
useful	O
and	O
widely	O
used	O
statistical	O
learning	O
method	O
moreover	O
it	O
serves	O
as	O
a	O
good	O
jumping-off	O
point	O
for	O
newer	O
approaches	O
as	O
we	O
will	O
see	O
in	O
later	O
chapters	O
many	O
fancy	O
statistical	O
learning	O
approaches	O
can	O
be	O
seen	O
as	O
generalizations	O
or	O
extensions	O
of	O
linear	B
regression	B
consequently	O
the	O
importance	B
of	O
having	O
a	O
good	O
understanding	O
of	O
linear	B
regression	B
before	O
studying	O
more	O
complex	O
learning	O
methods	O
cannot	O
be	O
overstated	O
in	O
this	O
chapter	O
we	O
review	O
some	O
of	O
the	O
key	O
ideas	O
underlying	O
the	O
linear	B
regression	B
model	B
as	O
well	O
as	O
the	O
least	B
squares	I
approach	B
that	O
is	O
most	O
commonly	O
used	O
to	O
fit	O
this	O
model	B
recall	B
the	O
advertising	B
data	B
from	O
chapter	O
figure	O
displays	O
sales	O
thousands	O
of	O
units	O
for	O
a	O
particular	O
product	O
as	O
a	O
function	B
of	O
advertising	B
budgets	O
thousands	O
of	O
dollars	O
for	O
tv	O
radio	O
and	O
newspaper	O
media	O
suppose	O
that	O
in	O
our	O
role	O
as	O
statistical	O
consultants	O
we	O
are	O
asked	O
to	O
suggest	O
on	O
the	O
basis	B
of	O
this	O
data	B
a	O
marketing	O
plan	O
for	O
next	O
year	O
that	O
will	O
result	O
in	O
high	O
product	O
sales	O
what	O
information	O
would	O
be	O
useful	O
in	O
order	O
to	O
provide	O
such	O
a	O
recommendation	O
here	O
are	O
a	O
few	O
important	O
questions	O
that	O
we	O
might	O
seek	O
to	O
address	O
is	O
there	O
a	O
relationship	O
between	O
advertising	B
budget	O
and	O
sales	O
our	O
first	O
goal	O
should	O
be	O
to	O
determine	O
whether	O
the	O
data	B
provide	O
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
linear	B
regression	B
evidence	O
of	O
an	O
association	O
between	O
advertising	B
expenditure	O
and	O
sales	O
if	O
the	O
evidence	O
is	O
weak	O
then	O
one	O
might	O
argue	O
that	O
no	O
money	O
should	O
be	O
spent	O
on	O
advertising	B
how	O
strong	O
is	O
the	O
relationship	O
between	O
advertising	B
budget	O
and	O
sales	O
assuming	O
that	O
there	O
is	O
a	O
relationship	O
between	O
advertising	B
and	O
sales	O
we	O
would	O
like	O
to	O
know	O
the	O
strength	O
of	O
this	O
relationship	O
in	O
other	O
words	O
given	O
a	O
certain	O
advertising	B
budget	O
can	O
we	O
predict	O
sales	O
with	O
a	O
high	O
level	B
of	O
accuracy	O
this	O
would	O
be	O
a	O
strong	O
relationship	O
or	O
is	O
a	O
prediction	B
of	O
sales	O
based	O
on	O
advertising	B
expenditure	O
only	O
slightly	O
better	O
than	O
a	O
random	O
guess	O
this	O
would	O
be	O
a	O
weak	O
relationship	O
which	O
media	O
contribute	O
to	O
sales	O
do	O
all	O
three	O
media	O
tv	O
radio	O
and	O
newspaper	O
contribute	O
to	O
sales	O
or	O
do	O
just	O
one	O
or	O
two	O
of	O
the	O
media	O
contribute	O
to	O
answer	O
this	O
question	O
we	O
must	O
find	O
a	O
way	O
to	O
separate	O
out	O
the	O
individual	O
effects	O
of	O
each	O
medium	O
when	O
we	O
have	O
spent	O
money	O
on	O
all	O
three	O
media	O
how	O
accurately	O
can	O
we	O
estimate	O
the	O
effect	O
of	O
each	O
medium	O
on	O
sales	O
for	O
every	O
dollar	O
spent	O
on	O
advertising	B
in	O
a	O
particular	O
medium	O
by	O
what	O
amount	O
will	O
sales	O
increase	O
how	O
accurately	O
can	O
we	O
predict	O
this	O
amount	O
of	O
increase	O
how	O
accurately	O
can	O
we	O
predict	O
future	O
sales	O
for	O
any	O
given	O
level	B
of	O
television	O
radio	O
or	O
newspaper	O
advertising	B
what	O
is	O
our	O
prediction	B
for	O
sales	O
and	O
what	O
is	O
the	O
accuracy	O
of	O
this	O
prediction	B
is	O
the	O
relationship	O
linear	B
if	O
there	O
is	O
approximately	O
a	O
straight-line	O
relationship	O
between	O
advertising	B
expenditure	O
in	O
the	O
various	O
media	O
and	O
sales	O
then	O
linear	B
regression	B
is	O
an	O
appropriate	O
tool	O
if	O
not	O
then	O
it	O
may	O
still	O
be	O
possible	O
to	O
transform	O
the	O
predictor	B
or	O
the	O
response	B
so	O
that	O
linear	B
regression	B
can	O
be	O
used	O
is	O
there	O
synergy	B
among	O
the	O
advertising	B
media	O
perhaps	O
spending	O
on	O
television	O
advertising	B
and	O
on	O
radio	O
advertising	B
results	O
in	O
more	O
sales	O
than	O
allocating	O
to	O
either	O
television	O
or	O
radio	O
individually	O
in	O
marketing	O
this	O
is	O
known	O
as	O
a	O
synergy	B
effect	O
while	O
in	O
statistics	O
it	O
is	O
called	O
an	O
interaction	B
effect	O
it	O
turns	O
out	O
that	O
linear	B
regression	B
can	O
be	O
used	O
to	O
answer	O
each	O
of	O
these	O
questions	O
we	O
will	O
first	O
discuss	O
all	O
of	O
these	O
questions	O
in	O
a	O
general	O
context	O
and	O
then	O
return	O
to	O
them	O
in	O
this	O
specific	O
context	O
in	O
section	O
synergy	B
interaction	B
simple	B
linear	B
regression	B
simple	B
linear	B
regression	B
simple	B
linear	B
regression	B
lives	O
up	O
to	O
its	O
name	O
it	O
is	O
a	O
very	O
straightforward	O
approach	B
for	O
predicting	O
a	O
quantitative	B
response	B
y	O
on	O
the	O
basis	B
of	O
a	O
single	B
predictor	B
variable	B
x	O
it	O
assumes	O
that	O
there	O
is	O
approximately	O
a	O
linear	B
relationship	O
between	O
x	O
and	O
y	O
mathematically	O
we	O
can	O
write	O
this	O
linear	B
relationship	O
as	O
you	O
might	O
read	O
as	O
is	O
approximately	O
modeled	O
as	O
we	O
will	O
sometimes	O
describe	O
by	O
saying	O
that	O
we	O
are	O
regressing	O
y	O
on	O
x	O
y	O
onto	O
x	O
for	O
example	O
x	O
may	O
represent	O
tv	O
advertising	B
and	O
y	O
may	O
represent	O
sales	O
then	O
we	O
can	O
regress	O
sales	O
onto	O
tv	O
by	O
fitting	O
the	O
model	B
y	O
sales	O
tv	O
simple	B
linear	B
regression	B
in	O
equation	O
and	O
are	O
two	O
unknown	O
constants	O
that	O
represent	O
the	O
intercept	B
and	O
slope	B
terms	O
in	O
the	O
linear	B
model	B
together	O
and	O
are	O
known	O
as	O
the	O
model	B
coefficients	O
or	O
parameters	O
once	O
we	O
have	O
used	O
our	O
training	O
data	B
to	O
produce	O
estimates	O
and	O
for	O
the	O
model	B
coefficients	O
we	O
can	O
predict	O
future	O
sales	O
on	O
the	O
basis	B
of	O
a	O
particular	O
value	O
of	O
tv	O
advertising	B
by	O
computing	O
intercept	B
slope	B
coefficient	O
parameter	B
y	O
where	O
y	O
indicates	O
a	O
prediction	B
of	O
y	O
on	O
the	O
basis	B
of	O
x	O
x	O
here	O
we	O
use	O
a	O
hat	O
symbol	O
to	O
denote	O
the	O
estimated	O
value	O
for	O
an	O
unknown	O
parameter	B
or	O
coefficient	O
or	O
to	O
denote	O
the	O
predicted	O
value	O
of	O
the	O
response	B
estimating	O
the	O
coefficients	O
in	O
practice	O
and	O
are	O
unknown	O
so	O
before	O
we	O
can	O
use	O
to	O
make	O
predictions	O
we	O
must	O
use	O
data	B
to	O
estimate	O
the	O
coefficients	O
let	O
yn	O
represent	O
n	O
observation	O
pairs	O
each	O
of	O
which	O
consists	O
of	O
a	O
measurement	O
of	O
x	O
and	O
a	O
measurement	O
of	O
y	O
in	O
the	O
advertising	B
example	O
this	O
data	B
set	B
consists	O
of	O
the	O
tv	O
advertising	B
budget	O
and	O
product	O
sales	O
in	O
n	O
different	O
markets	O
that	O
the	O
data	B
are	O
displayed	O
in	O
figure	O
our	O
goal	O
is	O
to	O
obtain	O
coefficient	O
estimates	O
and	O
such	O
that	O
the	O
linear	B
model	B
fits	O
the	O
available	O
data	B
well	O
that	O
is	O
so	O
that	O
yi	O
for	O
i	O
n	O
in	O
other	O
words	O
we	O
want	O
to	O
find	O
an	O
intercept	B
and	O
a	O
slope	B
such	O
that	O
the	O
resulting	O
line	B
is	O
as	O
close	O
as	O
possible	O
to	O
the	O
n	O
data	B
points	O
there	O
are	O
a	O
number	O
of	O
ways	O
of	O
measuring	O
closeness	O
however	O
by	O
far	O
the	O
most	O
common	O
approach	B
involves	O
minimizing	O
the	O
least	B
squares	I
criterion	O
and	O
we	O
take	O
that	O
approach	B
in	O
this	O
chapter	O
alternative	O
approaches	O
will	O
be	O
considered	O
in	O
chapter	O
least	B
squares	I
linear	B
regression	B
l	O
s	O
e	O
a	O
s	O
tv	O
figure	O
for	O
the	O
advertising	B
data	B
the	O
least	B
squares	I
fit	O
for	O
the	O
regression	B
of	O
sales	O
onto	O
tv	O
is	O
shown	O
the	O
fit	O
is	O
found	O
by	O
minimizing	O
the	O
sum	O
of	O
squared	O
errors	O
each	O
grey	O
line	B
segment	O
represents	O
an	O
error	B
and	O
the	O
fit	O
makes	O
a	O
compromise	O
by	O
averaging	O
their	O
squares	O
in	O
this	O
case	O
a	O
linear	B
fit	O
captures	O
the	O
essence	O
of	O
the	O
relationship	O
although	O
it	O
is	O
somewhat	O
deficient	O
in	O
the	O
left	O
of	O
the	O
plot	B
let	O
yi	O
be	O
the	O
prediction	B
for	O
y	O
based	O
on	O
the	O
ith	O
value	O
of	O
x	O
then	O
ei	O
yi	O
yi	O
represents	O
the	O
ith	O
residual	B
this	O
is	O
the	O
difference	O
between	O
the	O
ith	O
observed	O
response	B
value	O
and	O
the	O
ith	O
response	B
value	O
that	O
is	O
predicted	O
by	O
our	O
linear	B
model	B
we	O
define	O
the	O
residual	B
sum	B
of	I
squares	I
as	O
rss	O
n	O
or	O
equivalently	O
as	O
residual	B
residual	B
sum	B
of	I
squares	I
rss	O
the	O
least	B
squares	I
approach	B
chooses	O
and	O
to	O
minimize	O
the	O
rss	O
using	O
some	O
calculus	O
one	O
can	O
show	O
that	O
the	O
minimizers	O
are	O
where	O
y	O
n	O
xi	O
are	O
the	O
sample	O
means	O
in	O
other	O
words	O
defines	O
the	O
least	B
squares	I
coefficient	O
estimates	O
for	O
simple	B
linear	B
regression	B
yi	O
and	O
x	O
n	O
n	O
n	O
figure	O
displays	O
the	O
simple	B
linear	B
regression	B
fit	O
to	O
the	O
advertising	B
data	B
where	O
and	O
in	O
other	O
words	O
according	O
to	O
n	O
xyi	O
y	O
y	O
x	O
n	O
simple	B
linear	B
regression	B
r	O
s	O
s	O
figure	O
contour	O
and	O
three-dimensional	O
plots	O
of	O
the	O
rss	O
on	O
the	O
advertising	B
data	B
using	O
sales	O
as	O
the	O
response	B
and	O
tv	O
as	O
the	O
predictor	B
the	O
red	O
dots	O
correspond	O
to	O
the	O
least	B
squares	I
estimates	O
and	O
given	O
by	O
this	O
approximation	O
an	O
additional	O
spent	O
on	O
tv	O
advertising	B
is	O
associated	O
with	O
selling	O
approximately	O
additional	O
units	O
of	O
the	O
product	O
in	O
figure	O
we	O
have	O
computed	O
rss	O
for	O
a	O
number	O
of	O
values	O
of	O
and	O
using	O
the	O
advertising	B
data	B
with	O
sales	O
as	O
the	O
response	B
and	O
tv	O
as	O
the	O
predictor	B
in	O
each	O
plot	B
the	O
red	O
dot	O
represents	O
the	O
pair	O
of	O
least	B
squares	I
estimates	O
given	O
by	O
these	O
values	O
clearly	O
minimize	O
the	O
rss	O
assessing	O
the	O
accuracy	O
of	O
the	O
coefficient	O
estimates	O
recall	B
from	O
that	O
we	O
assume	O
that	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
takes	O
the	O
form	O
y	O
f	O
for	O
some	O
unknown	O
function	B
f	O
where	O
is	O
a	O
mean-zero	O
random	O
error	B
term	B
if	O
f	O
is	O
to	O
be	O
approximated	O
by	O
a	O
linear	B
function	B
then	O
we	O
can	O
write	O
this	O
relationship	O
as	O
y	O
here	O
is	O
the	O
intercept	B
term	B
that	O
is	O
the	O
expected	B
value	I
of	O
y	O
when	O
x	O
and	O
is	O
the	O
slope	B
the	O
average	B
increase	O
in	O
y	O
associated	O
with	O
a	O
one-unit	O
increase	O
in	O
x	O
the	O
error	B
term	B
is	O
a	O
catch-all	O
for	O
what	O
we	O
miss	O
with	O
this	O
simple	B
model	B
the	O
true	O
relationship	O
is	O
probably	O
not	O
linear	B
there	O
may	O
be	O
other	O
variables	O
that	O
cause	O
variation	O
in	O
y	O
and	O
there	O
may	O
be	O
measurement	O
error	B
we	O
typically	O
assume	O
that	O
the	O
error	B
term	B
is	O
independent	B
of	O
x	O
the	O
model	B
given	O
by	O
defines	O
the	O
population	B
regression	B
line	B
which	O
is	O
the	O
best	O
linear	B
approximation	O
to	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
the	O
least	B
squares	I
regression	B
coefficient	O
estimates	O
characterize	O
the	O
least	B
squares	I
line	B
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
these	O
assumption	O
of	O
linearity	O
is	O
often	O
a	O
useful	O
working	O
model	B
however	O
despite	O
what	O
many	O
textbooks	O
might	O
tell	O
us	O
we	O
seldom	O
believe	O
that	O
the	O
true	O
relationship	O
is	O
linear	B
population	B
regression	B
line	B
least	B
squares	I
line	B
linear	B
regression	B
y	O
y	O
x	O
x	O
figure	O
a	O
simulated	O
data	B
set	B
left	O
the	O
red	O
line	B
represents	O
the	O
true	O
relationship	O
f	O
which	O
is	O
known	O
as	O
the	O
population	B
regression	B
line	B
the	O
blue	O
line	B
is	O
the	O
least	B
squares	I
line	B
it	O
is	O
the	O
least	B
squares	I
estimate	O
for	O
f	O
based	O
on	O
the	O
observed	O
data	B
shown	O
in	O
black	O
right	O
the	O
population	B
regression	B
line	B
is	O
again	O
shown	O
in	O
red	O
and	O
the	O
least	B
squares	I
line	B
in	O
dark	O
blue	O
in	O
light	O
blue	O
ten	O
least	B
squares	I
lines	O
are	O
shown	O
each	O
computed	O
on	O
the	O
basis	B
of	O
a	O
separate	O
random	O
set	B
of	O
observations	B
each	O
least	B
squares	I
line	B
is	O
different	O
but	O
on	O
average	B
the	O
least	B
squares	I
lines	O
are	O
quite	O
close	O
to	O
the	O
population	B
regression	B
line	B
two	O
lines	O
in	O
a	O
simple	B
simulated	O
example	O
we	O
created	O
random	O
xs	O
and	O
generated	O
corresponding	O
y	O
s	O
from	O
the	O
model	B
y	O
where	O
was	O
generated	O
from	O
a	O
normal	O
distribution	B
with	O
mean	O
zero	O
the	O
red	O
line	B
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
the	O
true	O
relationship	O
f	O
while	O
the	O
blue	O
line	B
is	O
the	O
least	B
squares	I
estimate	O
based	O
on	O
the	O
observed	O
data	B
the	O
true	O
relationship	O
is	O
generally	O
not	O
known	O
for	O
real	O
data	B
but	O
the	O
least	B
squares	I
line	B
can	O
always	O
be	O
computed	O
using	O
the	O
coefficient	O
estimates	O
given	O
in	O
in	O
other	O
words	O
in	O
real	O
applications	O
we	O
have	O
access	O
to	O
a	O
set	B
of	O
observations	B
from	O
which	O
we	O
can	O
compute	O
the	O
least	B
squares	I
line	B
however	O
the	O
population	B
regression	B
line	B
is	O
unobserved	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
we	O
have	O
generated	O
ten	O
different	O
data	B
sets	O
from	O
the	O
model	B
given	O
by	O
and	O
plotted	O
the	O
corresponding	O
ten	O
least	B
squares	I
lines	O
notice	O
that	O
different	O
data	B
sets	O
generated	O
from	O
the	O
same	O
true	O
model	B
result	O
in	O
slightly	O
different	O
least	B
squares	I
lines	O
but	O
the	O
unobserved	O
population	B
regression	B
line	B
does	O
not	O
change	O
at	O
first	O
glance	O
the	O
difference	O
between	O
the	O
population	B
regression	B
line	B
and	O
the	O
least	B
squares	I
line	B
may	O
seem	O
subtle	O
and	O
confusing	O
we	O
only	O
have	O
one	O
data	B
set	B
and	O
so	O
what	O
does	O
it	O
mean	O
that	O
two	O
different	O
lines	O
describe	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
fundamentally	O
the	O
simple	B
linear	B
regression	B
concept	O
of	O
these	O
two	O
lines	O
is	O
a	O
natural	B
extension	O
of	O
the	O
standard	O
statistical	O
approach	B
of	O
using	O
information	O
from	O
a	O
sample	O
to	O
estimate	O
characteristics	O
of	O
a	O
large	O
population	O
for	O
example	O
suppose	O
that	O
we	O
are	O
interested	O
in	O
knowing	O
the	O
population	O
mean	O
of	O
some	O
random	O
variable	B
y	O
unfortunately	O
is	O
unknown	O
but	O
we	O
do	O
have	O
access	O
to	O
n	O
observations	B
from	O
y	O
which	O
we	O
can	O
write	O
as	O
yn	O
and	O
which	O
we	O
can	O
use	O
to	O
estimate	O
a	O
reasonable	O
n	O
estimate	O
is	O
y	O
where	O
y	O
yi	O
is	O
the	O
sample	O
mean	O
the	O
sample	O
n	O
mean	O
and	O
the	O
population	O
mean	O
are	O
different	O
but	O
in	O
general	O
the	O
sample	O
mean	O
will	O
provide	O
a	O
good	O
estimate	O
of	O
the	O
population	O
mean	O
in	O
the	O
same	O
way	O
the	O
unknown	O
coefficients	O
and	O
in	O
linear	B
regression	B
define	O
the	O
population	B
regression	B
line	B
we	O
seek	O
to	O
estimate	O
these	O
unknown	O
coefficients	O
using	O
and	O
given	O
in	O
these	O
coefficient	O
estimates	O
define	O
the	O
least	B
squares	I
line	B
the	O
analogy	O
between	O
linear	B
regression	B
and	O
estimation	O
of	O
the	O
mean	O
of	O
a	O
random	O
variable	B
is	O
an	O
apt	O
one	O
based	O
on	O
the	O
concept	O
of	O
bias	B
if	O
we	O
use	O
the	O
sample	O
mean	O
to	O
estimate	O
this	O
estimate	O
is	O
unbiased	O
in	O
the	O
sense	O
that	O
on	O
average	B
we	O
expect	O
to	O
equal	O
what	O
exactly	O
does	O
this	O
mean	O
it	O
means	O
that	O
on	O
the	O
basis	B
of	O
one	O
particular	O
set	B
of	O
observations	B
yn	O
might	O
overestimate	O
and	O
on	O
the	O
basis	B
of	O
another	O
set	B
of	O
observations	B
might	O
underestimate	O
but	O
if	O
we	O
could	O
average	B
a	O
huge	O
number	O
of	O
estimates	O
of	O
obtained	O
from	O
a	O
huge	O
number	O
of	O
sets	O
of	O
observations	B
then	O
this	O
average	B
would	O
exactly	O
equal	O
hence	O
an	O
unbiased	O
estimator	O
does	O
not	O
systematically	O
over-	O
or	O
under-estimate	O
the	O
true	O
parameter	B
the	O
property	O
of	O
unbiasedness	O
holds	O
for	O
the	O
least	B
squares	I
coefficient	O
estimates	O
given	O
by	O
as	O
well	O
if	O
we	O
estimate	O
and	O
on	O
the	O
basis	B
of	O
a	O
particular	O
data	B
set	B
then	O
our	O
estimates	O
won	O
t	O
be	O
exactly	O
equal	O
to	O
and	O
but	O
if	O
we	O
could	O
average	B
the	O
estimates	O
obtained	O
over	O
a	O
huge	O
number	O
of	O
data	B
sets	O
then	O
the	O
average	B
of	O
these	O
estimates	O
would	O
be	O
spot	O
on	O
in	O
fact	O
we	O
can	O
see	O
from	O
the	O
righthand	O
panel	O
of	O
figure	O
that	O
the	O
average	B
of	O
many	O
least	B
squares	I
lines	O
each	O
estimated	O
from	O
a	O
separate	O
data	B
set	B
is	O
pretty	O
close	O
to	O
the	O
true	O
population	B
regression	B
line	B
we	O
continue	O
the	O
analogy	O
with	O
the	O
estimation	O
of	O
the	O
population	O
mean	O
of	O
a	O
random	O
variable	B
y	O
a	O
natural	B
question	O
is	O
as	O
follows	O
how	O
accurate	O
is	O
the	O
sample	O
mean	O
as	O
an	O
estimate	O
of	O
we	O
have	O
established	O
that	O
the	O
average	B
of	O
s	O
over	O
many	O
data	B
sets	O
will	O
be	O
very	O
close	O
to	O
but	O
that	O
a	O
single	B
estimate	O
may	O
be	O
a	O
substantial	O
underestimate	O
or	O
overestimate	O
of	O
how	O
far	O
off	O
will	O
that	O
single	B
estimate	O
of	O
be	O
in	O
general	O
we	O
answer	O
this	O
question	O
by	O
computing	O
the	O
standard	B
error	B
of	O
written	O
as	O
se	O
we	O
have	O
the	O
well-known	O
formula	O
bias	B
unbiased	O
standard	B
error	B
var	O
se	O
n	O
linear	B
regression	B
n	O
where	O
is	O
the	O
standard	O
deviation	O
of	O
each	O
of	O
the	O
realizations	O
yi	O
of	O
y	O
roughly	O
speaking	O
the	O
standard	B
error	B
tells	O
us	O
the	O
average	B
amount	O
that	O
this	O
estimate	O
differs	O
from	O
the	O
actual	O
value	O
of	O
equation	O
also	O
tells	O
us	O
how	O
this	O
deviation	O
shrinks	O
with	O
n	O
the	O
more	O
observations	B
we	O
have	O
the	O
smaller	O
the	O
standard	B
error	B
of	O
in	O
a	O
similar	O
vein	O
we	O
can	O
wonder	O
how	O
close	O
and	O
are	O
to	O
the	O
true	O
values	O
and	O
to	O
compute	O
the	O
standard	O
errors	O
associated	O
with	O
and	O
we	O
use	O
the	O
following	O
formulas	O
n	O
n	O
se	O
se	O
where	O
var	O
for	O
these	O
formulas	O
to	O
be	O
strictly	O
valid	O
we	O
need	O
to	O
assume	O
that	O
the	O
errors	O
for	O
each	O
observation	O
are	O
uncorrelated	O
with	O
common	O
variance	B
this	O
is	O
clearly	O
not	O
true	O
in	O
figure	O
but	O
the	O
formula	O
still	O
turns	O
out	O
to	O
be	O
a	O
good	O
approximation	O
notice	O
in	O
the	O
formula	O
that	O
se	O
is	O
smaller	O
when	O
the	O
xi	O
are	O
more	O
spread	O
out	O
intuitively	O
we	O
have	O
more	O
leverage	B
to	O
estimate	O
a	O
slope	B
when	O
this	O
is	O
the	O
case	O
we	O
also	O
see	O
that	O
se	O
would	O
be	O
the	O
same	O
as	O
se	O
if	O
x	O
were	O
zero	O
which	O
case	O
would	O
be	O
equal	O
to	O
y	O
in	O
general	O
is	O
not	O
known	O
but	O
can	O
be	O
estimated	O
from	O
the	O
data	B
the	O
estimate	O
of	O
is	O
known	O
as	O
the	O
residual	B
standard	B
error	B
and	O
is	O
given	O
by	O
the	O
formula	O
rssn	O
strictly	O
speaking	O
when	O
is	O
estimated	O
from	O
the	O
rse	O
data	B
we	O
should	O
write	O
to	O
indicate	O
that	O
an	O
estimate	O
has	O
been	O
made	O
but	O
for	O
simplicity	O
of	O
notation	O
we	O
will	O
drop	O
this	O
extra	O
hat	O
standard	O
errors	O
can	O
be	O
used	O
to	O
compute	O
confidence	O
intervals	O
a	O
confidence	B
interval	B
is	O
defined	O
as	O
a	O
range	O
of	O
values	O
such	O
that	O
with	O
probability	B
the	O
range	O
will	O
contain	O
the	O
true	O
unknown	O
value	O
of	O
the	O
parameter	B
the	O
range	O
is	O
defined	O
in	O
terms	O
of	O
lower	O
and	O
upper	O
limits	O
computed	O
from	O
the	O
sample	O
of	O
data	B
for	O
linear	B
regression	B
the	O
confidence	B
interval	B
for	O
approximately	O
takes	O
the	O
form	O
that	O
is	O
there	O
is	O
approximately	O
a	O
chance	O
that	O
the	O
interval	B
se	O
se	O
will	O
contain	O
the	O
true	O
value	O
of	O
similarly	O
a	O
confidence	B
interval	B
for	O
approximately	O
takes	O
the	O
form	O
se	O
se	O
residual	B
standard	B
error	B
confidence	B
interval	B
formula	O
holds	O
provided	O
that	O
the	O
n	O
observations	B
are	O
uncorrelated	O
for	O
several	O
reasons	O
equation	O
relies	O
on	O
the	O
assumption	O
that	O
the	O
errors	O
are	O
gaussian	O
also	O
the	O
factor	B
of	O
in	O
front	O
of	O
the	O
se	O
term	B
will	O
vary	O
slightly	O
depending	O
on	O
the	O
number	O
of	O
observations	B
n	O
in	O
the	O
linear	B
regression	B
to	O
be	O
precise	O
rather	O
than	O
the	O
number	O
should	O
contain	O
the	O
quantile	O
of	O
a	O
t-distribution	B
with	O
n	O
degrees	B
of	I
freedom	I
details	O
of	O
how	O
to	O
compute	O
the	O
confidence	B
interval	B
precisely	O
in	O
r	O
will	O
be	O
provided	O
later	O
in	O
this	O
chapter	O
simple	B
linear	B
regression	B
in	O
the	O
case	O
of	O
the	O
advertising	B
data	B
the	O
confidence	B
interval	B
for	O
is	O
and	O
the	O
confidence	B
interval	B
for	O
is	O
therefore	O
we	O
can	O
conclude	O
that	O
in	O
the	O
absence	O
of	O
any	O
advertising	B
sales	O
will	O
on	O
average	B
fall	O
somewhere	O
between	O
and	O
units	O
furthermore	O
for	O
each	O
increase	O
in	O
television	O
advertising	B
there	O
will	O
be	O
an	O
average	B
increase	O
in	O
sales	O
of	O
between	O
and	O
units	O
standard	O
errors	O
can	O
also	O
be	O
used	O
to	O
perform	O
hypothesis	B
tests	O
on	O
the	O
coefficients	O
the	O
most	O
common	O
hypothesis	B
test	I
involves	O
testing	O
the	O
null	B
hypothesis	B
of	O
there	O
is	O
no	O
relationship	O
between	O
x	O
and	O
y	O
versus	O
the	O
alternative	B
hypothesis	B
ha	O
there	O
is	O
some	O
relationship	O
between	O
x	O
and	O
y	O
hypothesis	B
test	I
null	B
hypothesis	B
alternative	B
hypothesis	B
mathematically	O
this	O
corresponds	O
to	O
testing	O
versus	O
ha	O
since	O
if	O
then	O
the	O
model	B
reduces	O
to	O
y	O
and	O
x	O
is	O
not	O
associated	O
with	O
y	O
to	O
test	O
the	O
null	B
hypothesis	B
we	O
need	O
to	O
determine	O
whether	O
our	O
estimate	O
for	O
is	O
sufficiently	O
far	O
from	O
zero	O
that	O
we	O
can	O
be	O
confident	O
that	O
is	O
non-zero	O
how	O
far	O
is	O
far	O
enough	O
this	O
of	O
course	O
depends	O
on	O
the	O
accuracy	O
of	O
that	O
is	O
it	O
depends	O
on	O
se	O
if	O
se	O
is	O
small	O
then	O
even	O
relatively	O
small	O
values	O
of	O
may	O
provide	O
strong	O
evidence	O
that	O
and	O
hence	O
that	O
there	O
is	O
a	O
relationship	O
between	O
x	O
and	O
y	O
in	O
contrast	B
if	O
se	O
is	O
large	O
then	O
must	O
be	O
large	O
in	O
absolute	O
value	O
in	O
order	O
for	O
us	O
to	O
reject	O
the	O
null	B
hypothesis	B
in	O
practice	O
we	O
compute	O
a	O
t-statistic	B
given	O
by	O
t-statistic	B
se	O
t	O
which	O
measures	O
the	O
number	O
of	O
standard	O
deviations	O
that	O
is	O
away	O
from	O
if	O
there	O
really	O
is	O
no	O
relationship	O
between	O
x	O
and	O
y	O
then	O
we	O
expect	O
that	O
will	O
have	O
a	O
t-distribution	B
with	O
n	O
degrees	B
of	I
freedom	I
the	O
tdistribution	O
has	O
a	O
bell	O
shape	O
and	O
for	O
values	O
of	O
n	O
greater	O
than	O
approximately	O
it	O
is	O
quite	O
similar	O
to	O
the	O
normal	O
distribution	B
consequently	O
it	O
is	O
a	O
simple	B
matter	O
to	O
compute	O
the	O
probability	B
of	O
observing	O
any	O
number	O
equal	O
to	O
or	O
larger	O
in	O
absolute	O
value	O
assuming	O
we	O
call	O
this	O
probability	B
the	O
p-value	B
roughly	O
speaking	O
we	O
interpret	O
the	O
p-value	B
as	O
follows	O
a	O
small	O
p-value	B
indicates	O
that	O
it	O
is	O
unlikely	O
to	O
observe	O
such	O
a	O
substant	O
ial	O
association	O
between	O
the	O
predictor	B
and	O
the	O
response	B
due	O
to	O
chance	O
in	O
the	O
absence	O
of	O
any	O
real	O
association	O
between	O
the	O
predictor	B
and	O
the	O
response	B
hence	O
if	O
we	O
see	O
a	O
small	O
p-value	B
p-value	B
linear	B
regression	B
then	O
we	O
can	O
infer	O
that	O
there	O
is	O
an	O
association	O
between	O
the	O
predictor	B
and	O
the	O
response	B
we	O
reject	O
the	O
null	B
hypothesis	B
that	O
is	O
we	O
declare	O
a	O
relationship	O
to	O
exist	O
between	O
x	O
and	O
y	O
if	O
the	O
p-value	B
is	O
small	O
enough	O
typical	O
p-value	B
cutoffs	O
for	O
rejecting	O
the	O
null	B
hypothesis	B
are	O
or	O
when	O
n	O
these	O
correspond	O
to	O
t-statistics	O
of	O
around	O
and	O
respectively	O
intercept	B
tv	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
table	O
for	O
the	O
advertising	B
data	B
coefficients	O
of	O
the	O
least	B
squares	I
model	B
for	O
the	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
advertising	B
budget	O
an	O
increase	O
of	O
in	O
the	O
tv	O
advertising	B
budget	O
is	O
associated	O
with	O
an	O
increase	O
in	O
sales	O
by	O
around	O
units	O
that	O
the	O
sales	O
variable	B
is	O
in	O
thousands	O
of	O
units	O
and	O
the	O
tv	O
variable	B
is	O
in	O
thousands	O
of	O
dollars	O
table	O
provides	O
details	O
of	O
the	O
least	B
squares	I
model	B
for	O
the	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
advertising	B
budget	O
for	O
the	O
advertising	B
data	B
notice	O
that	O
the	O
coefficients	O
for	O
and	O
are	O
very	O
large	O
relative	O
to	O
their	O
standard	O
errors	O
so	O
the	O
t-statistics	O
are	O
also	O
large	O
the	O
probabilities	O
of	O
seeing	O
such	O
values	O
if	O
is	O
true	O
are	O
virtually	O
zero	O
hence	O
we	O
can	O
conclude	O
that	O
and	O
assessing	O
the	O
accuracy	O
of	O
the	O
model	B
once	O
we	O
have	O
rejected	O
the	O
null	B
hypothesis	B
in	O
favor	O
of	O
the	O
alternative	B
hypothesis	B
it	O
is	O
natural	B
to	O
want	O
to	O
quantify	O
the	O
extent	O
to	O
which	O
the	O
model	B
fits	O
the	O
data	B
the	O
quality	O
of	O
a	O
linear	B
regression	B
fit	O
is	O
typically	O
assessed	O
using	O
two	O
related	O
quantities	O
the	O
residual	B
standard	B
error	B
and	O
the	O
statistic	O
table	O
displays	O
the	O
rse	O
the	O
statistic	O
and	O
the	O
f-statistic	B
be	O
described	O
in	O
section	O
for	O
the	O
linear	B
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
advertising	B
budget	O
residual	B
standard	B
error	B
recall	B
from	O
the	O
model	B
that	O
associated	O
with	O
each	O
observation	O
is	O
an	O
error	B
term	B
due	O
to	O
the	O
presence	O
of	O
these	O
error	B
terms	O
even	O
if	O
we	O
knew	O
the	O
true	O
regression	B
line	B
even	O
if	O
and	O
were	O
known	O
we	O
would	O
not	O
be	O
able	O
to	O
perfectly	O
predict	O
y	O
from	O
x	O
the	O
rse	O
is	O
an	O
estimate	O
of	O
the	O
standard	O
table	O
a	O
small	O
p-value	B
for	O
the	O
intercept	B
indicates	O
that	O
we	O
can	O
reject	O
the	O
null	B
hypothesis	B
that	O
and	O
a	O
small	O
p-value	B
for	O
tv	O
indicates	O
that	O
we	O
can	O
reject	O
the	O
null	B
hypothesis	B
that	O
rejecting	O
the	O
latter	O
null	B
hypothesis	B
allows	O
us	O
to	O
conclude	O
that	O
there	O
is	O
a	O
relationship	O
between	O
tv	O
and	O
sales	O
rejecting	O
the	O
former	O
allows	O
us	O
to	O
conclude	O
that	O
in	O
the	O
absence	O
of	O
tv	O
expenditure	O
sales	O
are	O
non-zero	O
simple	B
linear	B
regression	B
quantity	O
residual	B
standard	B
error	B
f-statistic	B
value	O
table	O
for	O
the	O
advertising	B
data	B
more	O
information	O
about	O
the	O
least	B
squares	I
model	B
for	O
the	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
advertising	B
budget	O
deviation	O
of	O
roughly	O
speaking	O
it	O
is	O
the	O
average	B
amount	O
that	O
the	O
response	B
will	O
deviate	O
from	O
the	O
true	O
regression	B
line	B
it	O
is	O
computed	O
using	O
the	O
formula	O
rse	O
n	O
rss	O
n	O
note	O
that	O
rss	O
was	O
defined	O
in	O
section	O
and	O
is	O
given	O
by	O
the	O
formula	O
rss	O
in	O
the	O
case	O
of	O
the	O
advertising	B
data	B
we	O
see	O
from	O
the	O
linear	B
regression	B
output	B
in	O
table	O
that	O
the	O
rse	O
is	O
in	O
other	O
words	O
actual	O
sales	O
in	O
each	O
market	O
deviate	O
from	O
the	O
true	O
regression	B
line	B
by	O
approximately	O
units	O
on	O
average	B
another	O
way	O
to	O
think	O
about	O
this	O
is	O
that	O
even	O
if	O
the	O
model	B
were	O
correct	O
and	O
the	O
true	O
values	O
of	O
the	O
unknown	O
coefficients	O
and	O
were	O
known	O
exactly	O
any	O
prediction	B
of	O
sales	O
on	O
the	O
basis	B
of	O
tv	O
advertising	B
would	O
still	O
be	O
off	O
by	O
about	O
units	O
on	O
average	B
of	O
course	O
whether	O
or	O
not	O
units	O
is	O
an	O
acceptable	O
prediction	B
error	B
depends	O
on	O
the	O
problem	O
context	O
in	O
the	O
advertising	B
data	B
set	B
the	O
mean	O
value	O
of	O
sales	O
over	O
all	O
markets	O
is	O
approximately	O
units	O
and	O
so	O
the	O
percentage	O
error	B
is	O
the	O
rse	O
is	O
considered	O
a	O
measure	O
of	O
the	O
lack	O
of	O
fit	O
of	O
the	O
model	B
to	O
the	O
data	B
if	O
the	O
predictions	O
obtained	O
using	O
the	O
model	B
are	O
very	O
close	O
to	O
the	O
true	O
outcome	O
values	O
that	O
is	O
if	O
yi	O
yi	O
for	O
i	O
n	O
then	O
will	O
be	O
small	O
and	O
we	O
can	O
conclude	O
that	O
the	O
model	B
fits	O
the	O
data	B
very	O
well	O
on	O
the	O
other	O
hand	O
if	O
yi	O
is	O
very	O
far	O
from	O
yi	O
for	O
one	O
or	O
more	O
observations	B
then	O
the	O
rse	O
may	O
be	O
quite	O
large	O
indicating	O
that	O
the	O
model	B
doesn	O
t	O
fit	O
the	O
data	B
well	O
statistic	O
the	O
rse	O
provides	O
an	O
absolute	O
measure	O
of	O
lack	O
of	O
fit	O
of	O
the	O
model	B
to	O
the	O
data	B
but	O
since	O
it	O
is	O
measured	O
in	O
the	O
units	O
of	O
y	O
it	O
is	O
not	O
always	O
clear	O
what	O
constitutes	O
a	O
good	O
rse	O
the	O
statistic	O
provides	O
an	O
alternative	O
measure	O
of	O
fit	O
it	O
takes	O
the	O
form	O
of	O
a	O
proportion	O
the	O
proportion	O
of	O
variance	B
explained	B
and	O
so	O
it	O
always	O
takes	O
on	O
a	O
value	O
between	O
and	O
and	O
is	O
independent	B
of	O
the	O
scale	O
of	O
y	O
linear	B
regression	B
to	O
calculate	O
we	O
use	O
the	O
formula	O
tss	O
rss	O
tss	O
rss	O
tss	O
is	O
the	O
total	B
sum	B
of	I
squares	I
and	O
rss	O
is	O
defined	O
where	O
tss	O
in	O
tss	O
measures	O
the	O
total	O
variance	B
in	O
the	O
response	B
y	O
and	O
can	O
be	O
thought	O
of	O
as	O
the	O
amount	O
of	O
variability	O
inherent	O
in	O
the	O
response	B
before	O
the	O
regression	B
is	O
performed	O
in	O
contrast	B
rss	O
measures	O
the	O
amount	O
of	O
variability	O
that	O
is	O
left	O
unexplained	O
after	O
performing	O
the	O
regression	B
hence	O
tss	O
rss	O
measures	O
the	O
amount	O
of	O
variability	O
in	O
the	O
response	B
that	O
is	O
explained	B
removed	O
by	O
performing	O
the	O
regression	B
and	O
measures	O
the	O
proportion	O
of	O
variability	O
in	O
y	O
that	O
can	O
be	O
explained	B
using	O
x	O
an	O
statistic	O
that	O
is	O
close	O
to	O
indicates	O
that	O
a	O
large	O
proportion	O
of	O
the	O
variability	O
in	O
the	O
response	B
has	O
been	O
explained	B
by	O
the	O
regression	B
a	O
number	O
near	O
indicates	O
that	O
the	O
regression	B
did	O
not	O
explain	O
much	O
of	O
the	O
variability	O
in	O
the	O
response	B
this	O
might	O
occur	O
because	O
the	O
linear	B
model	B
is	O
wrong	O
or	O
the	O
inherent	O
error	B
is	O
high	O
or	O
both	O
in	O
table	O
the	O
was	O
and	O
so	O
just	O
under	O
two-thirds	O
of	O
the	O
variability	O
in	O
sales	O
is	O
explained	B
by	O
a	O
linear	B
regression	B
on	O
tv	O
total	B
sum	B
of	I
squares	I
the	O
statistic	O
has	O
an	O
interpretational	O
advantage	O
over	O
the	O
rse	O
since	O
unlike	O
the	O
rse	O
it	O
always	O
lies	O
between	O
and	O
however	O
it	O
can	O
still	O
be	O
challenging	O
to	O
determine	O
what	O
is	O
a	O
good	O
value	O
and	O
in	O
general	O
this	O
will	O
depend	O
on	O
the	O
application	O
for	O
instance	O
in	O
certain	O
problems	O
in	O
physics	O
we	O
may	O
know	O
that	O
the	O
data	B
truly	O
comes	O
from	O
a	O
linear	B
model	B
with	O
a	O
small	O
residual	B
error	B
in	O
this	O
case	O
we	O
would	O
expect	O
to	O
see	O
an	O
value	O
that	O
is	O
extremely	O
close	O
to	O
and	O
a	O
substantially	O
smaller	O
value	O
might	O
indicate	O
a	O
serious	O
problem	O
with	O
the	O
experiment	O
in	O
which	O
the	O
data	B
were	O
generated	O
on	O
the	O
other	O
hand	O
in	O
typical	O
applications	O
in	O
biology	O
psychology	O
marketing	O
and	O
other	O
domains	O
the	O
linear	B
model	B
is	O
at	O
best	O
an	O
extremely	O
rough	O
approximation	O
to	O
the	O
data	B
and	O
residual	B
errors	O
due	O
to	O
other	O
unmeasured	O
factors	O
are	O
often	O
very	O
large	O
in	O
this	O
setting	O
we	O
would	O
expect	O
only	O
a	O
very	O
small	O
proportion	O
of	O
the	O
variance	B
in	O
the	O
response	B
to	O
be	O
explained	B
by	O
the	O
predictor	B
and	O
an	O
value	O
well	O
below	O
might	O
be	O
more	O
realistic	O
the	O
statistic	O
is	O
a	O
measure	O
of	O
the	O
linear	B
relationship	O
between	O
x	O
and	O
y	O
recall	B
that	O
correlation	B
defined	O
as	O
corx	O
y	O
n	O
n	O
xyi	O
y	O
n	O
correlation	B
is	O
also	O
a	O
measure	O
of	O
the	O
linear	B
relationship	O
between	O
x	O
and	O
y	O
this	O
suggests	O
that	O
we	O
might	O
be	O
able	O
to	O
use	O
r	O
corx	O
y	O
instead	O
of	O
in	O
order	O
to	O
assess	O
the	O
fit	O
of	O
the	O
linear	B
model	B
in	O
fact	O
it	O
can	O
be	O
shown	O
that	O
in	O
the	O
simple	B
linear	B
regression	B
setting	O
in	O
other	O
words	O
the	O
squared	O
correlation	B
note	O
that	O
in	O
fact	O
the	O
right-hand	O
side	O
of	O
is	O
the	O
sample	O
correlation	B
thus	O
it	O
would	O
be	O
more	O
correct	O
to	O
write	O
y	O
however	O
we	O
omit	O
the	O
hat	O
for	O
ease	O
of	O
notation	O
multiple	B
linear	B
regression	B
and	O
the	O
statistic	O
are	O
identical	O
however	O
in	O
the	O
next	O
section	O
we	O
will	O
discuss	O
the	O
multiple	B
linear	B
regression	B
problem	O
in	O
which	O
we	O
use	O
several	O
predictors	O
simultaneously	O
to	O
predict	O
the	O
response	B
the	O
concept	O
of	O
correlation	B
between	O
the	O
predictors	O
and	O
the	O
response	B
does	O
not	O
extend	O
automatically	O
to	O
this	O
setting	O
since	O
correlation	B
quantifies	O
the	O
association	O
between	O
a	O
single	B
pair	O
of	O
variables	O
rather	O
than	O
between	O
a	O
larger	O
number	O
of	O
variables	O
we	O
will	O
see	O
that	O
fills	O
this	O
role	O
multiple	B
linear	B
regression	B
simple	B
linear	B
regression	B
is	O
a	O
useful	O
approach	B
for	O
predicting	O
a	O
response	B
on	O
the	O
basis	B
of	O
a	O
single	B
predictor	B
variable	B
however	O
in	O
practice	O
we	O
often	O
have	O
more	O
than	O
one	O
predictor	B
for	O
example	O
in	O
the	O
advertising	B
data	B
we	O
have	O
examined	O
the	O
relationship	O
between	O
sales	O
and	O
tv	O
advertising	B
we	O
also	O
have	O
data	B
for	O
the	O
amount	O
of	O
money	O
spent	O
advertising	B
on	O
the	O
radio	O
and	O
in	O
newspapers	O
and	O
we	O
may	O
want	O
to	O
know	O
whether	O
either	O
of	O
these	O
two	O
media	O
is	O
associated	O
with	O
sales	O
how	O
can	O
we	O
extend	O
our	O
analysis	B
of	O
the	O
advertising	B
data	B
in	O
order	O
to	O
accommodate	O
these	O
two	O
additional	O
predictors	O
one	O
option	O
is	O
to	O
run	O
three	O
separate	O
simple	B
linear	B
regressions	O
each	O
of	O
which	O
uses	O
a	O
different	O
advertising	B
medium	O
as	O
a	O
predictor	B
for	O
instance	O
we	O
can	O
fit	O
a	O
simple	B
linear	B
regression	B
to	O
predict	O
sales	O
on	O
the	O
basis	B
of	O
the	O
amount	O
spent	O
on	O
radio	O
advertisements	O
results	O
are	O
shown	O
in	O
table	O
table	O
we	O
find	O
that	O
a	O
increase	O
in	O
spending	O
on	O
radio	O
advertising	B
is	O
associated	O
with	O
an	O
increase	O
in	O
sales	O
by	O
around	O
units	O
table	O
table	O
contains	O
the	O
least	B
squares	I
coefficients	O
for	O
a	O
simple	B
linear	B
regression	B
of	O
sales	O
onto	O
newspaper	O
advertising	B
budget	O
a	O
increase	O
in	O
newspaper	O
advertising	B
budget	O
is	O
associated	O
with	O
an	O
increase	O
in	O
sales	O
by	O
approximately	O
units	O
however	O
the	O
approach	B
of	O
fitting	O
a	O
separate	O
simple	B
linear	B
regression	B
model	B
for	O
each	O
predictor	B
is	O
not	O
entirely	O
satisfactory	O
first	O
of	O
all	O
it	O
is	O
unclear	O
how	O
to	O
make	O
a	O
single	B
prediction	B
of	O
sales	O
given	O
levels	O
of	O
the	O
three	O
advertising	B
media	O
budgets	O
since	O
each	O
of	O
the	O
budgets	O
is	O
associated	O
with	O
a	O
separate	O
regression	B
equation	O
second	O
each	O
of	O
the	O
three	O
regression	B
equations	O
ignores	O
the	O
other	O
two	O
media	O
in	O
forming	O
estimates	O
for	O
the	O
regression	B
coefficients	O
we	O
will	O
see	O
shortly	O
that	O
if	O
the	O
media	O
budgets	O
are	O
correlated	O
with	O
each	O
other	O
in	O
the	O
markets	O
that	O
constitute	O
our	O
data	B
set	B
then	O
this	O
can	O
lead	O
to	O
very	O
misleading	O
estimates	O
of	O
the	O
individual	O
media	O
effects	O
on	O
sales	O
instead	O
of	O
fitting	O
a	O
separate	O
simple	B
linear	B
regression	B
model	B
for	O
each	O
predictor	B
a	O
better	O
approach	B
is	O
to	O
extend	O
the	O
simple	B
linear	B
regression	B
model	B
so	O
that	O
it	O
can	O
directly	O
accommodate	O
multiple	B
predictors	O
we	O
can	O
do	O
this	O
by	O
giving	O
each	O
predictor	B
a	O
separate	O
slope	B
coefficient	O
in	O
a	O
single	B
model	B
in	O
general	O
suppose	O
that	O
we	O
have	O
p	O
distinct	O
predictors	O
then	O
the	O
multiple	B
linear	B
regression	B
model	B
takes	O
the	O
form	O
y	O
pxp	O
linear	B
regression	B
simple	B
regression	B
of	O
sales	O
on	O
radio	O
intercept	B
radio	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
simple	B
regression	B
of	O
sales	O
on	O
newspaper	O
intercept	B
newspaper	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
table	O
more	O
simple	B
linear	B
regression	B
models	O
for	O
the	O
advertising	B
data	B
coefficients	O
of	O
the	O
simple	B
linear	B
regression	B
model	B
for	O
number	O
of	O
units	O
sold	O
on	O
top	O
radio	O
advertising	B
budget	O
and	O
bottom	O
newspaper	O
advertising	B
budget	O
a	O
increase	O
in	O
spending	O
on	O
radio	O
advertising	B
is	O
associated	O
with	O
an	O
average	B
increase	O
in	O
sales	O
by	O
around	O
units	O
while	O
the	O
same	O
increase	O
in	O
spending	O
on	O
newspaper	O
advertising	B
is	O
associated	O
with	O
an	O
average	B
increase	O
in	O
sales	O
by	O
around	O
units	O
that	O
the	O
sales	O
variable	B
is	O
in	O
thousands	O
of	O
units	O
and	O
the	O
radio	O
and	O
newspaper	O
variables	O
are	O
in	O
thousands	O
of	O
dollars	O
where	O
xj	O
represents	O
the	O
jth	O
predictor	B
and	O
j	O
quantifies	O
the	O
association	O
between	O
that	O
variable	B
and	O
the	O
response	B
we	O
interpret	O
j	O
as	O
the	O
average	B
effect	O
on	O
y	O
of	O
a	O
one	O
unit	O
increase	O
in	O
xj	O
holding	O
all	O
other	O
predictors	O
fixed	O
in	O
the	O
advertising	B
example	O
becomes	O
sales	O
tv	O
radio	O
newspaper	O
estimating	O
the	O
regression	B
coefficients	O
as	O
was	O
the	O
case	O
in	O
the	O
simple	B
linear	B
regression	B
setting	O
the	O
regression	B
coefficients	O
p	O
in	O
are	O
unknown	O
and	O
must	O
be	O
estimated	O
given	O
estimates	O
p	O
we	O
can	O
make	O
predictions	O
using	O
the	O
formula	O
y	O
pxp	O
the	O
parameters	O
are	O
estimated	O
using	O
the	O
same	O
least	B
squares	I
approach	B
that	O
we	O
saw	O
in	O
the	O
context	O
of	O
simple	B
linear	B
regression	B
we	O
choose	O
p	O
to	O
minimize	O
the	O
sum	O
of	O
squared	O
residuals	B
rss	O
multiple	B
linear	B
regression	B
y	O
figure	O
in	O
a	O
three-dimensional	O
setting	O
with	O
two	O
predictors	O
and	O
one	O
response	B
the	O
least	B
squares	I
regression	B
line	B
becomes	O
a	O
plane	O
the	O
plane	O
is	O
chosen	O
to	O
minimize	O
the	O
sum	O
of	O
the	O
squared	O
vertical	O
distances	O
between	O
each	O
observation	O
in	O
red	O
and	O
the	O
plane	O
the	O
values	O
p	O
that	O
minimize	O
are	O
the	O
multiple	B
least	B
squares	I
regression	B
coefficient	O
estimates	O
unlike	O
the	O
simple	B
linear	B
regression	B
estimates	O
given	O
in	O
the	O
multiple	B
regression	B
coefficient	O
estimates	O
have	O
somewhat	O
complicated	O
forms	O
that	O
are	O
most	O
easily	O
represented	O
using	O
matrix	O
algebra	O
for	O
this	O
reason	O
we	O
do	O
not	O
provide	O
them	O
here	O
any	O
statistical	O
software	O
package	O
can	O
be	O
used	O
to	O
compute	O
these	O
coefficient	O
estimates	O
and	O
later	O
in	O
this	O
chapter	O
we	O
will	O
show	O
how	O
this	O
can	O
be	O
done	O
in	O
r	O
figure	O
illustrates	O
an	O
example	O
of	O
the	O
least	B
squares	I
fit	O
to	O
a	O
toy	O
data	B
set	B
with	O
p	O
predictors	O
table	O
displays	O
the	O
multiple	B
regression	B
coefficient	O
estimates	O
when	O
tv	O
radio	O
and	O
newspaper	O
advertising	B
budgets	O
are	O
used	O
to	O
predict	O
product	O
sales	O
using	O
the	O
advertising	B
data	B
we	O
interpret	O
these	O
results	O
as	O
follows	O
for	O
a	O
given	O
amount	O
of	O
tv	O
and	O
newspaper	O
advertising	B
spending	O
an	O
additional	O
on	O
radio	O
advertising	B
leads	O
to	O
an	O
increase	O
in	O
sales	O
by	O
approximately	O
units	O
comparing	O
these	O
coefficient	O
estimates	O
to	O
those	O
displayed	O
in	O
tables	O
and	O
we	O
notice	O
that	O
the	O
multiple	B
regression	B
coefficient	O
estimates	O
for	O
tv	O
and	O
radio	O
are	O
pretty	O
similar	O
to	O
the	O
simple	B
linear	B
regression	B
coefficient	O
estimates	O
however	O
while	O
the	O
newspaper	O
regression	B
coefficient	O
estimate	O
in	O
table	O
was	O
significantly	O
non-zero	O
the	O
coefficient	O
estimate	O
for	O
newspaper	O
in	O
the	O
multiple	B
regression	B
model	B
is	O
close	O
to	O
zero	O
and	O
the	O
corresponding	O
p-value	B
is	O
no	O
longer	O
significant	O
with	O
a	O
value	O
around	O
this	O
illustrates	O
linear	B
regression	B
intercept	B
tv	O
radio	O
newspaper	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
table	O
for	O
the	O
advertising	B
data	B
least	B
squares	I
coefficient	O
estimates	O
of	O
the	O
multiple	B
linear	B
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
radio	O
tv	O
and	O
newspaper	O
advertising	B
budgets	O
that	O
the	O
simple	B
and	O
multiple	B
regression	B
coefficients	O
can	O
be	O
quite	O
different	O
this	O
difference	O
stems	O
from	O
the	O
fact	O
that	O
in	O
the	O
simple	B
regression	B
case	O
the	O
slope	B
term	B
represents	O
the	O
average	B
effect	O
of	O
a	O
increase	O
in	O
newspaper	O
advertising	B
ignoring	O
other	O
predictors	O
such	O
as	O
tv	O
and	O
radio	O
in	O
contrast	B
in	O
the	O
multiple	B
regression	B
setting	O
the	O
coefficient	O
for	O
newspaper	O
represents	O
the	O
average	B
effect	O
of	O
increasing	O
newspaper	O
spending	O
by	O
while	O
holding	O
tv	O
and	O
radio	O
fixed	O
does	O
it	O
make	O
sense	O
for	O
the	O
multiple	B
regression	B
to	O
suggest	O
no	O
relationship	O
between	O
sales	O
and	O
newspaper	O
while	O
the	O
simple	B
linear	B
regression	B
implies	O
the	O
opposite	O
in	O
fact	O
it	O
does	O
consider	O
the	O
correlation	B
matrix	O
for	O
the	O
three	O
predictor	B
variables	O
and	O
response	B
variable	B
displayed	O
in	O
table	O
notice	O
that	O
the	O
correlation	B
between	O
radio	O
and	O
newspaper	O
is	O
this	O
reveals	O
a	O
tendency	O
to	O
spend	O
more	O
on	O
newspaper	O
advertising	B
in	O
markets	O
where	O
more	O
is	O
spent	O
on	O
radio	O
advertising	B
now	O
suppose	O
that	O
the	O
multiple	B
regression	B
is	O
correct	O
and	O
newspaper	O
advertising	B
has	O
no	O
direct	O
impact	O
on	O
sales	O
but	O
radio	O
advertising	B
does	O
increase	O
sales	O
then	O
in	O
markets	O
where	O
we	O
spend	O
more	O
on	O
radio	O
our	O
sales	O
will	O
tend	O
to	O
be	O
higher	O
and	O
as	O
our	O
correlation	B
matrix	O
shows	O
we	O
also	O
tend	O
to	O
spend	O
more	O
on	O
newspaper	O
advertising	B
in	O
those	O
same	O
markets	O
hence	O
in	O
a	O
simple	B
linear	B
regression	B
which	O
only	O
examines	O
sales	O
versus	O
newspaper	O
we	O
will	O
observe	O
that	O
higher	O
values	O
of	O
newspaper	O
tend	O
to	O
be	O
associated	O
with	O
higher	O
values	O
of	O
sales	O
even	O
though	O
newspaper	O
advertising	B
does	O
not	O
actually	O
affect	O
sales	O
so	O
newspaper	O
sales	O
are	O
a	O
surrogate	O
for	O
radio	O
advertising	B
newspaper	O
gets	O
credit	B
for	O
the	O
effect	O
of	O
radio	O
on	O
sales	O
this	O
slightly	O
counterintuitive	O
result	O
is	O
very	O
common	O
in	O
many	O
real	O
life	O
situations	O
consider	O
an	O
absurd	O
example	O
to	O
illustrate	O
the	O
point	O
running	O
a	O
regression	B
of	O
shark	O
attacks	O
versus	O
ice	O
cream	O
sales	O
for	O
data	B
collected	O
at	O
a	O
given	O
beach	O
community	O
over	O
a	O
period	O
of	O
time	O
would	O
show	O
a	O
positive	O
relationship	O
similar	O
to	O
that	O
seen	O
between	O
sales	O
and	O
newspaper	O
of	O
course	O
no	O
one	O
has	O
suggested	O
that	O
ice	O
creams	O
should	O
be	O
banned	O
at	O
beaches	O
to	O
reduce	O
shark	O
attacks	O
in	O
reality	O
higher	O
temperatures	O
cause	O
more	O
people	O
to	O
visit	O
the	O
beach	O
which	O
in	O
turn	O
results	O
in	O
more	O
ice	O
cream	O
sales	O
and	O
more	O
shark	O
attacks	O
a	O
multiple	B
regression	B
of	O
attacks	O
versus	O
ice	O
cream	O
sales	O
and	O
temperature	O
reveals	O
that	O
as	O
intuition	O
implies	O
the	O
former	O
predictor	B
is	O
no	O
longer	O
significant	O
after	O
adjusting	O
for	O
temperature	O
multiple	B
linear	B
regression	B
tv	O
radio	O
newspaper	O
sales	O
tv	O
radio	O
newspaper	O
sales	O
table	O
correlation	B
matrix	O
for	O
tv	O
radio	O
newspaper	O
and	O
sales	O
for	O
the	O
advertising	B
data	B
some	O
important	O
questions	O
when	O
we	O
perform	O
multiple	B
linear	B
regression	B
we	O
usually	O
are	O
interested	O
in	O
answering	O
a	O
few	O
important	O
questions	O
is	O
at	O
least	O
one	O
of	O
the	O
predictors	O
xp	O
useful	O
in	O
predicting	O
the	O
response	B
do	O
all	O
the	O
predictors	O
help	O
to	O
explain	O
y	O
or	O
is	O
only	O
a	O
subset	O
of	O
the	O
predictors	O
useful	O
how	O
well	O
does	O
the	O
model	B
fit	O
the	O
data	B
given	O
a	O
set	B
of	O
predictor	B
values	O
what	O
response	B
value	O
should	O
we	O
predict	O
and	O
how	O
accurate	O
is	O
our	O
prediction	B
we	O
now	O
address	O
each	O
of	O
these	O
questions	O
in	O
turn	O
one	O
is	O
there	O
a	O
relationship	O
between	O
the	O
response	B
and	O
predictors	O
recall	B
that	O
in	O
the	O
simple	B
linear	B
regression	B
setting	O
in	O
order	O
to	O
determine	O
whether	O
there	O
is	O
a	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictor	B
we	O
can	O
simply	O
check	O
whether	O
in	O
the	O
multiple	B
regression	B
setting	O
with	O
p	O
predictors	O
we	O
need	O
to	O
ask	O
whether	O
all	O
of	O
the	O
regression	B
coefficients	O
are	O
zero	O
i	O
e	O
whether	O
p	O
as	O
in	O
the	O
simple	B
linear	B
regression	B
setting	O
we	O
use	O
a	O
hypothesis	B
test	I
to	O
answer	O
this	O
question	O
we	O
test	O
the	O
null	B
hypothesis	B
p	O
versus	O
the	O
alternative	O
ha	O
at	O
least	O
one	O
j	O
is	O
non-zero	O
this	O
hypothesis	B
test	I
is	O
performed	O
by	O
computing	O
the	O
f-statistic	B
rssp	O
rssn	O
p	O
f	O
f-statistic	B
linear	B
regression	B
quantity	O
residual	B
standard	B
error	B
f-statistic	B
value	O
table	O
more	O
information	O
about	O
the	O
least	B
squares	I
model	B
for	O
the	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
newspaper	O
and	O
radio	O
advertising	B
budgets	O
in	O
the	O
advertising	B
data	B
other	O
information	O
about	O
this	O
model	B
was	O
displayed	O
in	O
table	O
where	O
as	O
with	O
simple	B
linear	B
regression	B
tss	O
and	O
rss	O
if	O
the	O
linear	B
model	B
assumptions	O
are	O
correct	O
one	O
can	O
show	O
that	O
erssn	O
p	O
and	O
that	O
provided	O
is	O
true	O
etss	O
rssp	O
hence	O
when	O
there	O
is	O
no	O
relationship	O
between	O
the	O
response	B
and	O
predictors	O
one	O
would	O
expect	O
the	O
f-statistic	B
to	O
take	O
on	O
a	O
value	O
close	O
to	O
on	O
the	O
other	O
hand	O
if	O
ha	O
is	O
true	O
then	O
etss	O
rssp	O
so	O
we	O
expect	O
f	O
to	O
be	O
greater	O
than	O
the	O
f-statistic	B
for	O
the	O
multiple	B
linear	B
regression	B
model	B
obtained	O
by	O
regressing	O
sales	O
onto	O
radio	O
tv	O
and	O
newspaper	O
is	O
shown	O
in	O
table	O
in	O
this	O
example	O
the	O
f-statistic	B
is	O
since	O
this	O
is	O
far	O
larger	O
than	O
it	O
provides	O
compelling	O
evidence	O
against	O
the	O
null	B
hypothesis	B
in	O
other	O
words	O
the	O
large	O
f-statistic	B
suggests	O
that	O
at	O
least	O
one	O
of	O
the	O
advertising	B
media	O
must	O
be	O
related	O
to	O
sales	O
however	O
what	O
if	O
the	O
f-statistic	B
had	O
been	O
closer	O
to	O
how	O
large	O
does	O
the	O
f-statistic	B
need	O
to	O
be	O
before	O
we	O
can	O
reject	O
and	O
conclude	O
that	O
there	O
is	O
a	O
relationship	O
it	O
turns	O
out	O
that	O
the	O
answer	O
depends	O
on	O
the	O
values	O
of	O
n	O
and	O
p	O
when	O
n	O
is	O
large	O
an	O
f-statistic	B
that	O
is	O
just	O
a	O
little	O
larger	O
than	O
might	O
still	O
provide	O
evidence	O
against	O
in	O
contrast	B
a	O
larger	O
f-statistic	B
is	O
needed	O
to	O
reject	O
if	O
n	O
is	O
small	O
when	O
is	O
true	O
and	O
the	O
errors	O
have	O
a	O
normal	O
distribution	B
the	O
f-statistic	B
follows	O
an	O
for	O
any	O
given	O
value	O
of	O
n	O
and	O
p	O
any	O
statistical	O
software	O
package	O
can	O
be	O
used	O
to	O
compute	O
the	O
p-value	B
associated	O
with	O
the	O
f-statistic	B
using	O
this	O
distribution	B
based	O
on	O
this	O
p-value	B
we	O
can	O
determine	O
whether	O
or	O
not	O
to	O
reject	O
for	O
the	O
advertising	B
data	B
the	O
p-value	B
associated	O
with	O
the	O
f-statistic	B
in	O
table	O
is	O
essentially	O
zero	O
so	O
we	O
have	O
extremely	O
strong	O
evidence	O
that	O
at	O
least	O
one	O
of	O
the	O
media	O
is	O
associated	O
with	O
increased	O
sales	O
in	O
we	O
are	O
testing	O
that	O
all	O
the	O
coefficients	O
are	O
zero	O
sometimes	O
we	O
want	O
to	O
test	O
that	O
a	O
particular	O
subset	O
of	O
q	O
of	O
the	O
coefficients	O
are	O
zero	O
this	O
corresponds	O
to	O
a	O
null	B
hypothesis	B
p	O
p	O
p	O
if	O
the	O
errors	O
are	O
not	O
normally-distributed	O
the	O
f-statistic	B
approximately	O
follows	O
an	O
f-distribution	O
provided	O
that	O
the	O
sample	O
size	O
n	O
is	O
large	O
multiple	B
linear	B
regression	B
where	O
for	O
convenience	O
we	O
have	O
put	O
the	O
variables	O
chosen	O
for	O
omission	O
at	O
the	O
end	O
of	O
the	O
list	O
in	O
this	O
case	O
we	O
fit	O
a	O
second	O
model	B
that	O
uses	O
all	O
the	O
variables	O
except	O
those	O
last	O
q	O
suppose	O
that	O
the	O
residual	B
sum	B
of	I
squares	I
for	O
that	O
model	B
is	O
then	O
the	O
appropriate	O
f-statistic	B
is	O
rssq	O
rssn	O
p	O
f	O
notice	O
that	O
in	O
table	O
for	O
each	O
individual	O
predictor	B
a	O
t-statistic	B
and	O
a	O
p-value	B
were	O
reported	O
these	O
provide	O
information	O
about	O
whether	O
each	O
individual	O
predictor	B
is	O
related	O
to	O
the	O
response	B
after	O
adjusting	O
for	O
the	O
other	O
predictors	O
it	O
turns	O
out	O
that	O
each	O
of	O
these	O
are	O
exactly	O
to	O
the	O
f-test	O
that	O
omits	O
that	O
single	B
variable	B
from	O
the	O
model	B
leaving	O
all	O
the	O
others	O
in	O
i	O
e	O
in	O
so	O
it	O
reports	O
the	O
partial	O
effect	O
of	O
adding	O
that	O
variable	B
to	O
the	O
model	B
for	O
instance	O
as	O
we	O
discussed	O
earlier	O
these	O
p-values	O
indicate	O
that	O
tv	O
and	O
radio	O
are	O
related	O
to	O
sales	O
but	O
that	O
there	O
is	O
no	O
evidence	O
that	O
newspaper	O
is	O
associated	O
with	O
sales	O
in	O
the	O
presence	O
of	O
these	O
two	O
given	O
these	O
individual	O
p-values	O
for	O
each	O
variable	B
why	O
do	O
we	O
need	O
to	O
look	O
at	O
the	O
overall	O
f-statistic	B
after	O
all	O
it	O
seems	O
likely	O
that	O
if	O
any	O
one	O
of	O
the	O
p-values	O
for	O
the	O
individual	O
variables	O
is	O
very	O
small	O
then	O
at	O
least	O
one	O
of	O
the	O
predictors	O
is	O
related	O
to	O
the	O
response	B
however	O
this	O
logic	O
is	O
flawed	O
especially	O
when	O
the	O
number	O
of	O
predictors	O
p	O
is	O
large	O
for	O
instance	O
consider	O
an	O
example	O
in	O
which	O
p	O
and	O
p	O
is	O
true	O
so	O
no	O
variable	B
is	O
truly	O
associated	O
with	O
the	O
response	B
in	O
this	O
situation	O
about	O
of	O
the	O
p-values	O
associated	O
with	O
each	O
variable	B
the	O
type	O
shown	O
in	O
table	O
will	O
be	O
below	O
by	O
chance	O
in	O
other	O
words	O
we	O
expect	O
to	O
see	O
approximately	O
five	O
small	O
p-values	O
even	O
in	O
the	O
absence	O
of	O
any	O
true	O
association	O
between	O
the	O
predictors	O
and	O
the	O
response	B
in	O
fact	O
we	O
are	O
almost	O
guaranteed	O
that	O
we	O
will	O
observe	O
at	O
least	O
one	O
p-value	B
below	O
by	O
chance	O
hence	O
if	O
we	O
use	O
the	O
individual	O
t-statistics	O
and	O
associated	O
pvalues	O
in	O
order	O
to	O
decide	O
whether	O
or	O
not	O
there	O
is	O
any	O
association	O
between	O
the	O
variables	O
and	O
the	O
response	B
there	O
is	O
a	O
very	O
high	O
chance	O
that	O
we	O
will	O
incorrectly	O
conclude	O
that	O
there	O
is	O
a	O
relationship	O
however	O
the	O
f-statistic	B
does	O
not	O
suffer	O
from	O
this	O
problem	O
because	O
it	O
adjusts	O
for	O
the	O
number	O
of	O
predictors	O
hence	O
if	O
is	O
true	O
there	O
is	O
only	O
a	O
chance	O
that	O
the	O
fstatistic	O
will	O
result	O
in	O
a	O
p-value	B
below	O
regardless	O
of	O
the	O
number	O
of	O
predictors	O
or	O
the	O
number	O
of	O
observations	B
the	O
approach	B
of	O
using	O
an	O
f-statistic	B
to	O
test	O
for	O
any	O
association	O
between	O
the	O
predictors	O
and	O
the	O
response	B
works	O
when	O
p	O
is	O
relatively	O
small	O
and	O
certainly	O
small	O
compared	O
to	O
n	O
however	O
sometimes	O
we	O
have	O
a	O
very	O
large	O
number	O
of	O
variables	O
if	O
p	O
n	O
then	O
there	O
are	O
more	O
coefficients	O
j	O
to	O
estimate	O
than	O
observations	B
from	O
which	O
to	O
estimate	O
them	O
in	O
this	O
case	O
we	O
cannot	O
even	O
fit	O
the	O
multiple	B
linear	B
regression	B
model	B
using	O
least	B
squares	I
so	O
the	O
square	O
of	O
each	O
t-statistic	B
is	O
the	O
corresponding	O
f-statistic	B
linear	B
regression	B
f-statistic	B
cannot	O
be	O
used	O
and	O
neither	O
can	O
most	O
of	O
the	O
other	O
concepts	O
that	O
we	O
have	O
seen	O
so	O
far	O
in	O
this	O
chapter	O
when	O
p	O
is	O
large	O
some	O
of	O
the	O
approaches	O
discussed	O
in	O
the	O
next	O
section	O
such	O
as	O
forward	O
selection	B
can	O
be	O
used	O
this	O
high-dimensional	B
setting	O
is	O
discussed	O
in	O
greater	O
detail	O
in	O
chapter	O
highdimensional	O
two	O
deciding	O
on	O
important	O
variables	O
as	O
discussed	O
in	O
the	O
previous	O
section	O
the	O
first	O
step	O
in	O
a	O
multiple	B
regression	B
analysis	B
is	O
to	O
compute	O
the	O
f-statistic	B
and	O
to	O
examine	O
the	O
associated	O
pvalue	O
if	O
we	O
conclude	O
on	O
the	O
basis	B
of	O
that	O
p-value	B
that	O
at	O
least	O
one	O
of	O
the	O
predictors	O
is	O
related	O
to	O
the	O
response	B
then	O
it	O
is	O
natural	B
to	O
wonder	O
which	O
are	O
the	O
guilty	O
ones	O
we	O
could	O
look	O
at	O
the	O
individual	O
p-values	O
as	O
in	O
table	O
but	O
as	O
discussed	O
if	O
p	O
is	O
large	O
we	O
are	O
likely	O
to	O
make	O
some	O
false	O
discoveries	O
it	O
is	O
possible	O
that	O
all	O
of	O
the	O
predictors	O
are	O
associated	O
with	O
the	O
response	B
but	O
it	O
is	O
more	O
often	O
the	O
case	O
that	O
the	O
response	B
is	O
only	O
related	O
to	O
a	O
subset	O
of	O
the	O
predictors	O
the	O
task	O
of	O
determining	O
which	O
predictors	O
are	O
associated	O
with	O
the	O
response	B
in	O
order	O
to	O
fit	O
a	O
single	B
model	B
involving	O
only	O
those	O
predictors	O
is	O
referred	O
to	O
as	O
variable	B
selection	B
the	O
variable	B
selection	B
problem	O
is	O
studied	O
extensively	O
in	O
chapter	O
and	O
so	O
here	O
we	O
will	O
provide	O
only	O
a	O
brief	O
outline	O
of	O
some	O
classical	O
approaches	O
ideally	O
we	O
would	O
like	O
to	O
perform	O
variable	B
selection	B
by	O
trying	O
out	O
a	O
lot	O
of	O
different	O
models	O
each	O
containing	O
a	O
different	O
subset	O
of	O
the	O
predictors	O
for	O
instance	O
if	O
p	O
then	O
we	O
can	O
consider	O
four	O
models	O
a	O
model	B
containing	O
no	O
variables	O
a	O
model	B
containing	O
only	O
a	O
model	B
containing	O
only	O
and	O
a	O
model	B
containing	O
both	O
and	O
we	O
can	O
then	O
select	O
the	O
best	O
model	B
out	O
of	O
all	O
of	O
the	O
models	O
that	O
we	O
have	O
considered	O
how	O
do	O
we	O
determine	O
which	O
model	B
is	O
best	O
various	O
statistics	O
can	O
be	O
used	O
to	O
judge	O
the	O
quality	O
of	O
a	O
model	B
these	O
include	O
mallow	O
s	O
cp	B
akaike	B
information	I
criterion	I
bayesian	B
information	O
criterion	O
and	O
adjusted	O
these	O
are	O
discussed	O
in	O
more	O
detail	O
in	O
chapter	O
we	O
can	O
also	O
determine	O
which	O
model	B
is	O
best	O
by	O
plotting	O
various	O
model	B
outputs	O
such	O
as	O
the	O
residuals	B
in	O
order	O
to	O
search	O
for	O
patterns	O
unfortunately	O
there	O
are	O
a	O
total	O
of	O
models	O
that	O
contain	O
subsets	O
of	O
p	O
variables	O
this	O
means	O
that	O
even	O
for	O
moderate	O
p	O
trying	O
out	O
every	O
possible	O
subset	O
of	O
the	O
predictors	O
is	O
infeasible	O
for	O
instance	O
we	O
saw	O
that	O
if	O
p	O
then	O
there	O
are	O
models	O
to	O
consider	O
but	O
if	O
p	O
then	O
we	O
must	O
consider	O
models	O
this	O
is	O
not	O
practical	O
therefore	O
unless	O
p	O
is	O
very	O
small	O
we	O
cannot	O
consider	O
all	O
models	O
and	O
instead	O
we	O
need	O
an	O
automated	O
and	O
efficient	O
approach	B
to	O
choose	O
a	O
smaller	O
set	B
of	O
models	O
to	O
consider	O
there	O
are	O
three	O
classical	O
approaches	O
for	O
this	O
task	O
variable	B
selection	B
mallow	O
s	O
cp	B
akaike	B
information	I
criterion	I
bayesian	B
information	O
criterion	O
adjusted	O
forward	O
selection	B
we	O
begin	O
with	O
the	O
null	B
model	B
a	O
model	B
that	O
contains	O
an	O
intercept	B
but	O
no	O
predictors	O
we	O
then	O
fit	O
p	O
simple	B
linear	B
regressions	O
and	O
add	O
to	O
the	O
null	B
model	B
the	O
variable	B
that	O
results	O
in	O
the	O
lowest	O
rss	O
we	O
then	O
add	O
to	O
that	O
model	B
the	O
variable	B
that	O
results	O
forward	O
selection	B
null	B
model	B
multiple	B
linear	B
regression	B
in	O
the	O
lowest	O
rss	O
for	O
the	O
new	O
two-variable	O
model	B
this	O
approach	B
is	O
continued	O
until	O
some	O
stopping	O
rule	O
is	O
satisfied	O
backward	O
selection	B
we	O
start	O
with	O
all	O
variables	O
in	O
the	O
model	B
and	O
remove	O
the	O
variable	B
with	O
the	O
largest	O
p-value	B
that	O
is	O
the	O
variable	B
that	O
is	O
the	O
least	O
statistically	O
significant	O
the	O
new	O
model	B
is	O
fit	O
and	O
the	O
variable	B
with	O
the	O
largest	O
p-value	B
is	O
removed	O
this	O
procedure	O
continues	O
until	O
a	O
stopping	O
rule	O
is	O
reached	O
for	O
instance	O
we	O
may	O
stop	O
when	O
all	O
remaining	O
variables	O
have	O
a	O
p-value	B
below	O
some	O
threshold	O
mixed	B
selection	B
this	O
is	O
a	O
combination	O
of	O
forward	O
and	O
backward	O
selection	B
we	O
start	O
with	O
no	O
variables	O
in	O
the	O
model	B
and	O
as	O
with	O
forward	O
selection	B
we	O
add	O
the	O
variable	B
that	O
provides	O
the	O
best	O
fit	O
we	O
continue	O
to	O
add	O
variables	O
one-by-one	O
of	O
course	O
as	O
we	O
noted	O
with	O
the	O
advertising	B
example	O
the	O
p-values	O
for	O
variables	O
can	O
become	O
larger	O
as	O
new	O
predictors	O
are	O
added	O
to	O
the	O
model	B
hence	O
if	O
at	O
any	O
point	O
the	O
p-value	B
for	O
one	O
of	O
the	O
variables	O
in	O
the	O
model	B
rises	O
above	O
a	O
certain	O
threshold	O
then	O
we	O
remove	O
that	O
variable	B
from	O
the	O
model	B
we	O
continue	O
to	O
perform	O
these	O
forward	O
and	O
backward	O
steps	O
until	O
all	O
variables	O
in	O
the	O
model	B
have	O
a	O
sufficiently	O
low	O
p-value	B
and	O
all	O
variables	O
outside	O
the	O
model	B
would	O
have	O
a	O
large	O
p-value	B
if	O
added	O
to	O
the	O
model	B
backward	O
selection	B
mixed	B
selection	B
backward	O
selection	B
cannot	O
be	O
used	O
if	O
p	O
n	O
while	O
forward	O
selection	B
can	O
always	O
be	O
used	O
forward	O
selection	B
is	O
a	O
greedy	O
approach	B
and	O
might	O
include	O
variables	O
early	O
that	O
later	O
become	O
redundant	O
mixed	B
selection	B
can	O
remedy	O
this	O
three	O
model	B
fit	O
two	O
of	O
the	O
most	O
common	O
numerical	O
measures	O
of	O
model	B
fit	O
are	O
the	O
rse	O
and	O
the	O
fraction	O
of	O
variance	B
explained	B
these	O
quantities	O
are	O
computed	O
and	O
interpreted	O
in	O
the	O
same	O
fashion	O
as	O
for	O
simple	B
linear	B
regression	B
recall	B
that	O
in	O
simple	B
regression	B
is	O
the	O
square	O
of	O
the	O
correlation	B
of	O
the	O
response	B
and	O
the	O
variable	B
in	O
multiple	B
linear	B
regression	B
it	O
turns	O
out	O
that	O
it	O
equals	O
cory	O
y	O
the	O
square	O
of	O
the	O
correlation	B
between	O
the	O
response	B
and	O
the	O
fitted	O
linear	B
model	B
in	O
fact	O
one	O
property	O
of	O
the	O
fitted	O
linear	B
model	B
is	O
that	O
it	O
maximizes	O
this	O
correlation	B
among	O
all	O
possible	O
linear	B
models	O
an	O
value	O
close	O
to	O
indicates	O
that	O
the	O
model	B
explains	O
a	O
large	O
portion	O
of	O
the	O
variance	B
in	O
the	O
response	B
variable	B
as	O
an	O
example	O
we	O
saw	O
in	O
table	O
that	O
for	O
the	O
advertising	B
data	B
the	O
model	B
that	O
uses	O
all	O
three	O
advertising	B
media	O
to	O
predict	O
sales	O
has	O
an	O
of	O
on	O
the	O
other	O
hand	O
the	O
model	B
that	O
uses	O
only	O
tv	O
and	O
radio	O
to	O
predict	O
sales	O
has	O
an	O
value	O
of	O
in	O
other	O
words	O
there	O
is	O
a	O
small	O
increase	O
in	O
if	O
we	O
include	O
newspaper	O
advertising	B
in	O
the	O
model	B
that	O
already	O
contains	O
tv	O
and	O
radio	O
advertising	B
even	O
though	O
we	O
saw	O
earlier	O
that	O
the	O
p-value	B
for	O
newspaper	O
advertising	B
in	O
table	O
is	O
not	O
significant	O
it	O
turns	O
out	O
that	O
will	O
always	O
increase	O
when	O
more	O
variables	O
linear	B
regression	B
are	O
added	O
to	O
the	O
model	B
even	O
if	O
those	O
variables	O
are	O
only	O
weakly	O
associated	O
with	O
the	O
response	B
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
adding	O
another	O
variable	B
to	O
the	O
least	B
squares	I
equations	O
must	O
allow	O
us	O
to	O
fit	O
the	O
training	O
data	B
not	O
necessarily	O
the	O
testing	O
data	B
more	O
accurately	O
thus	O
the	O
statistic	O
which	O
is	O
also	O
computed	O
on	O
the	O
training	O
data	B
must	O
increase	O
the	O
fact	O
that	O
adding	O
newspaper	O
advertising	B
to	O
the	O
model	B
containing	O
only	O
tv	O
and	O
radio	O
advertising	B
leads	O
to	O
just	O
a	O
tiny	O
increase	O
in	O
provides	O
additional	O
evidence	O
that	O
newspaper	O
can	O
be	O
dropped	O
from	O
the	O
model	B
essentially	O
newspaper	O
provides	O
no	O
real	O
improvement	O
in	O
the	O
model	B
fit	O
to	O
the	O
training	O
samples	O
and	O
its	O
inclusion	O
will	O
likely	O
lead	O
to	O
poor	O
results	O
on	O
independent	B
test	O
samples	O
due	O
to	O
overfitting	B
in	O
contrast	B
the	O
model	B
containing	O
only	O
tv	O
as	O
a	O
predictor	B
had	O
an	O
of	O
adding	O
radio	O
to	O
the	O
model	B
leads	O
to	O
a	O
substantial	O
improvement	O
in	O
this	O
implies	O
that	O
a	O
model	B
that	O
uses	O
tv	O
and	O
radio	O
expenditures	O
to	O
predict	O
sales	O
is	O
substantially	O
better	O
than	O
one	O
that	O
uses	O
only	O
tv	O
advertising	B
we	O
could	O
further	O
quantify	O
this	O
improvement	O
by	O
looking	O
at	O
the	O
p-value	B
for	O
the	O
radio	O
coefficient	O
in	O
a	O
model	B
that	O
contains	O
only	O
tv	O
and	O
radio	O
as	O
predictors	O
the	O
model	B
that	O
contains	O
only	O
tv	O
and	O
radio	O
as	O
predictors	O
has	O
an	O
rse	O
of	O
and	O
the	O
model	B
that	O
also	O
contains	O
newspaper	O
as	O
a	O
predictor	B
has	O
an	O
rse	O
of	O
in	O
contrast	B
the	O
model	B
that	O
contains	O
only	O
tv	O
has	O
an	O
rse	O
of	O
this	O
corroborates	O
our	O
previous	O
conclusion	O
that	O
a	O
model	B
that	O
uses	O
tv	O
and	O
radio	O
expenditures	O
to	O
predict	O
sales	O
is	O
much	O
more	O
accurate	O
the	O
training	O
data	B
than	O
one	O
that	O
only	O
uses	O
tv	O
spending	O
furthermore	O
given	O
that	O
tv	O
and	O
radio	O
expenditures	O
are	O
used	O
as	O
predictors	O
there	O
is	O
no	O
point	O
in	O
also	O
using	O
newspaper	O
spending	O
as	O
a	O
predictor	B
in	O
the	O
model	B
the	O
observant	O
reader	O
may	O
wonder	O
how	O
rse	O
can	O
increase	O
when	O
newspaper	O
is	O
added	O
to	O
the	O
model	B
given	O
that	O
rss	O
must	O
decrease	O
in	O
general	O
rse	O
is	O
defined	O
as	O
rse	O
n	O
p	O
rss	O
which	O
simplifies	O
to	O
for	O
a	O
simple	B
linear	B
regression	B
thus	O
models	O
with	O
more	O
variables	O
can	O
have	O
higher	O
rse	O
if	O
the	O
decrease	O
in	O
rss	O
is	O
small	O
relative	O
to	O
the	O
increase	O
in	O
p	O
in	O
addition	O
to	O
looking	O
at	O
the	O
rse	O
and	O
statistics	O
just	O
discussed	O
it	O
can	O
be	O
useful	O
to	O
plot	B
the	O
data	B
graphical	O
summaries	O
can	O
reveal	O
problems	O
with	O
a	O
model	B
that	O
are	O
not	O
visible	O
from	O
numerical	O
statistics	O
for	O
example	O
figure	O
displays	O
a	O
three-dimensional	O
plot	B
of	O
tv	O
and	O
radio	O
versus	O
sales	O
we	O
see	O
that	O
some	O
observations	B
lie	O
above	O
and	O
some	O
observations	B
lie	O
below	O
the	O
least	B
squares	I
regression	B
plane	O
in	O
particular	O
the	O
linear	B
model	B
seems	O
to	O
overestimate	O
sales	O
for	O
instances	O
in	O
which	O
most	O
of	O
the	O
advertising	B
money	O
was	O
spent	O
exclusively	O
on	O
either	O
tv	O
or	O
radio	O
it	O
underestimates	O
sales	O
for	O
instances	O
where	O
the	O
budget	O
was	O
split	O
between	O
the	O
two	O
media	O
this	O
pronounced	O
non-linear	B
pattern	O
cannot	O
be	O
modeled	O
accurately	O
using	O
linear	B
re	O
sales	O
multiple	B
linear	B
regression	B
tv	O
radio	O
figure	O
for	O
the	O
advertising	B
data	B
a	O
linear	B
regression	B
fit	O
to	O
sales	O
using	O
tv	O
and	O
radio	O
as	O
predictors	O
from	O
the	O
pattern	O
of	O
the	O
residuals	B
we	O
can	O
see	O
that	O
there	O
is	O
a	O
pronounced	O
non-linear	B
relationship	O
in	O
the	O
data	B
the	O
positive	O
residuals	B
visible	O
above	O
the	O
surface	O
tend	O
to	O
lie	O
along	O
the	O
line	B
where	O
tv	O
and	O
radio	O
budgets	O
are	O
split	O
evenly	O
the	O
negative	O
residuals	B
not	O
visible	O
tend	O
to	O
lie	O
away	O
from	O
this	O
line	B
where	O
budgets	O
are	O
more	O
lopsided	O
gression	O
it	O
suggests	O
a	O
synergy	B
or	O
interaction	B
effect	O
between	O
the	O
advertising	B
media	O
whereby	O
combining	O
the	O
media	O
together	O
results	O
in	O
a	O
bigger	O
boost	O
to	O
sales	O
than	O
using	O
any	O
single	B
medium	O
in	O
section	O
we	O
will	O
discuss	O
extending	O
the	O
linear	B
model	B
to	O
accommodate	O
such	O
synergistic	O
effects	O
through	O
the	O
use	O
of	O
interaction	B
terms	O
four	O
predictions	O
once	O
we	O
have	O
fit	O
the	O
multiple	B
regression	B
model	B
it	O
is	O
straightforward	O
to	O
apply	O
in	O
order	O
to	O
predict	O
the	O
response	B
y	O
on	O
the	O
basis	B
of	O
a	O
set	B
of	O
values	O
for	O
the	O
predictors	O
xp	O
however	O
there	O
are	O
three	O
sorts	O
of	O
uncertainty	O
associated	O
with	O
this	O
prediction	B
the	O
coefficient	O
estimates	O
p	O
are	O
estimates	O
for	O
p	O
that	O
is	O
the	O
least	B
squares	I
plane	O
y	O
pxp	O
is	O
only	O
an	O
estimate	O
for	O
the	O
true	O
population	O
regression	B
plane	O
f	O
pxp	O
the	O
inaccuracy	O
in	O
the	O
coefficient	O
estimates	O
is	O
related	O
to	O
the	O
reducible	B
error	B
from	O
chapter	O
we	O
can	O
compute	O
a	O
confidence	B
interval	B
in	O
order	O
to	O
determine	O
how	O
close	O
y	O
will	O
be	O
to	O
f	O
linear	B
regression	B
of	O
course	O
in	O
practice	O
assuming	O
a	O
linear	B
model	B
for	O
f	O
is	O
almost	O
always	O
an	O
approximation	O
of	O
reality	O
so	O
there	O
is	O
an	O
additional	O
source	O
of	O
potentially	O
reducible	B
error	B
which	O
we	O
call	O
model	B
bias	B
so	O
when	O
we	O
use	O
a	O
linear	B
model	B
we	O
are	O
in	O
fact	O
estimating	O
the	O
best	O
linear	B
approximation	O
to	O
the	O
true	O
surface	O
however	O
here	O
we	O
will	O
ignore	O
this	O
discrepancy	O
and	O
operate	O
as	O
if	O
the	O
linear	B
model	B
were	O
correct	O
even	O
if	O
we	O
knew	O
f	O
that	O
is	O
even	O
if	O
we	O
knew	O
the	O
true	O
values	O
for	O
p	O
the	O
response	B
value	O
cannot	O
be	O
predicted	O
perfectly	O
because	O
of	O
the	O
random	O
error	B
in	O
the	O
model	B
in	O
chapter	O
we	O
referred	O
to	O
this	O
as	O
the	O
irreducible	B
error	B
how	O
much	O
will	O
y	O
vary	O
from	O
y	O
we	O
use	O
prediction	B
intervals	O
to	O
answer	O
this	O
question	O
prediction	B
intervals	O
are	O
always	O
wider	O
than	O
confidence	O
intervals	O
because	O
they	O
incorporate	O
both	O
the	O
error	B
in	O
the	O
estimate	O
for	O
f	O
reducible	B
error	B
and	O
the	O
uncertainty	O
as	O
to	O
how	O
much	O
an	O
individual	O
point	O
will	O
differ	O
from	O
the	O
population	O
regression	B
plane	O
irreducible	B
error	B
we	O
use	O
a	O
confidence	B
interval	B
to	O
quantify	O
the	O
uncertainty	O
surrounding	O
the	O
average	B
sales	O
over	O
a	O
large	O
number	O
of	O
cities	O
for	O
example	O
given	O
that	O
is	O
spent	O
on	O
tv	O
advertising	B
and	O
is	O
spent	O
on	O
radio	O
advertising	B
in	O
each	O
city	O
the	O
confidence	B
interval	B
is	O
we	O
interpret	O
this	O
to	O
mean	O
that	O
of	O
intervals	O
of	O
this	O
form	O
will	O
contain	O
the	O
true	O
value	O
of	O
f	O
on	O
the	O
other	O
hand	O
a	O
prediction	B
interval	B
can	O
be	O
used	O
to	O
quantify	O
the	O
uncertainty	O
surrounding	O
sales	O
for	O
a	O
particular	O
city	O
given	O
that	O
is	O
spent	O
on	O
tv	O
advertising	B
and	O
is	O
spent	O
on	O
radio	O
advertising	B
in	O
that	O
city	O
the	O
prediction	B
interval	B
is	O
we	O
interpret	O
this	O
to	O
mean	O
that	O
of	O
intervals	O
of	O
this	O
form	O
will	O
contain	O
the	O
true	O
value	O
of	O
y	O
for	O
this	O
city	O
note	O
that	O
both	O
intervals	O
are	O
centered	O
at	O
but	O
that	O
the	O
prediction	B
interval	B
is	O
substantially	O
wider	O
than	O
the	O
confidence	B
interval	B
reflecting	O
the	O
increased	O
uncertainty	O
about	O
sales	O
for	O
a	O
given	O
city	O
in	O
comparison	O
to	O
the	O
average	B
sales	O
over	O
many	O
locations	O
confidence	B
interval	B
prediction	B
interval	B
other	O
considerations	O
in	O
the	O
regression	B
model	B
qualitative	B
predictors	O
in	O
our	O
discussion	O
so	O
far	O
we	O
have	O
assumed	O
that	O
all	O
variables	O
in	O
our	O
linear	B
regression	B
model	B
are	O
quantitative	B
but	O
in	O
practice	O
this	O
is	O
not	O
necessarily	O
the	O
case	O
often	O
some	O
predictors	O
are	O
qualitative	B
other	O
words	O
if	O
we	O
collect	O
a	O
large	O
number	O
of	O
data	B
sets	O
like	O
the	O
advertising	B
data	B
set	B
and	O
we	O
construct	O
a	O
confidence	B
interval	B
for	O
the	O
average	B
sales	O
on	O
the	O
basis	B
of	O
each	O
data	B
set	B
in	O
tv	O
and	O
in	O
radio	O
advertising	B
then	O
of	O
these	O
confidence	O
intervals	O
will	O
contain	O
the	O
true	O
value	O
of	O
average	B
sales	O
other	O
considerations	O
in	O
the	O
regression	B
model	B
for	O
example	O
the	O
credit	B
data	B
set	B
displayed	O
in	O
figure	O
records	O
balance	O
credit	B
card	O
debt	O
for	O
a	O
number	O
of	O
individuals	O
as	O
well	O
as	O
several	O
quantitative	B
predictors	O
age	O
cards	O
of	O
credit	B
cards	O
education	O
of	O
education	O
income	B
thousands	O
of	O
dollars	O
limit	O
limit	O
and	O
rating	O
rating	O
each	O
panel	O
of	O
figure	O
is	O
a	O
scatterplot	B
for	O
a	O
pair	O
of	O
variables	O
whose	O
identities	O
are	O
given	O
by	O
the	O
corresponding	O
row	O
and	O
column	O
labels	O
for	O
example	O
the	O
scatterplot	B
directly	O
to	O
the	O
right	O
of	O
the	O
word	O
balance	O
depicts	O
balance	O
versus	O
age	O
while	O
the	O
plot	B
directly	O
to	O
the	O
right	O
of	O
age	O
corresponds	O
to	O
age	O
versus	O
cards	O
in	O
addition	O
to	O
these	O
quantitative	B
variables	O
we	O
also	O
have	O
four	O
qualitative	B
variables	O
gender	O
student	O
status	O
status	O
status	O
and	O
ethnicity	O
african	O
american	O
or	O
asian	O
balance	O
age	O
cards	O
education	O
income	B
limit	O
rating	O
figure	O
the	O
credit	B
data	B
set	B
contains	O
information	O
about	O
balance	O
age	O
cards	O
education	O
income	B
limit	O
and	O
rating	O
for	O
a	O
number	O
of	O
potential	O
customers	O
linear	B
regression	B
intercept	B
genderfemale	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
table	O
least	B
squares	I
coefficient	O
estimates	O
associated	O
with	O
the	O
regression	B
of	O
balance	O
onto	O
gender	O
in	O
the	O
credit	B
data	B
set	B
the	O
linear	B
model	B
is	O
given	O
in	O
that	O
is	O
gender	O
is	O
encoded	O
as	O
a	O
dummy	B
variable	B
as	O
in	O
predictors	O
with	O
only	O
two	O
levels	O
suppose	O
that	O
we	O
wish	O
to	O
investigate	O
differences	O
in	O
credit	B
card	O
balance	O
between	O
males	O
and	O
females	O
ignoring	O
the	O
other	O
variables	O
for	O
the	O
moment	O
if	O
a	O
qualitative	B
predictor	B
known	O
as	O
a	O
factor	B
only	O
has	O
two	O
levels	O
or	O
possible	O
values	O
then	O
incorporating	O
it	O
into	O
a	O
regression	B
model	B
is	O
very	O
simple	B
we	O
simply	O
create	O
an	O
indicator	B
or	O
dummy	B
variable	B
that	O
takes	O
on	O
two	O
possible	O
numerical	O
values	O
for	O
example	O
based	O
on	O
the	O
gender	O
variable	B
we	O
can	O
create	O
a	O
new	O
variable	B
that	O
takes	O
the	O
form	O
factor	B
level	B
dummy	B
variable	B
xi	O
if	O
ith	O
person	O
is	O
female	O
if	O
ith	O
person	O
is	O
male	O
and	O
use	O
this	O
variable	B
as	O
a	O
predictor	B
in	O
the	O
regression	B
equation	O
this	O
results	O
in	O
the	O
model	B
yi	O
if	O
ith	O
person	O
is	O
female	O
if	O
ith	O
person	O
is	O
male	O
now	O
can	O
be	O
interpreted	O
as	O
the	O
average	B
credit	B
card	O
balance	O
among	O
males	O
as	O
the	O
average	B
credit	B
card	O
balance	O
among	O
females	O
and	O
as	O
the	O
average	B
difference	O
in	O
credit	B
card	O
balance	O
between	O
females	O
and	O
males	O
table	O
displays	O
the	O
coefficient	O
estimates	O
and	O
other	O
information	O
associated	O
with	O
the	O
model	B
the	O
average	B
credit	B
card	O
debt	O
for	O
males	O
is	O
estimated	O
to	O
be	O
whereas	O
females	O
are	O
estimated	O
to	O
carry	O
in	O
additional	O
debt	O
for	O
a	O
total	O
of	O
however	O
we	O
notice	O
that	O
the	O
p-value	B
for	O
the	O
dummy	B
variable	B
is	O
very	O
high	O
this	O
indicates	O
that	O
there	O
is	O
no	O
statistical	O
evidence	O
of	O
a	O
difference	O
in	O
average	B
credit	B
card	O
balance	O
between	O
the	O
genders	O
the	O
decision	O
to	O
code	O
females	O
as	O
and	O
males	O
as	O
in	O
is	O
arbitrary	O
and	O
has	O
no	O
effect	O
on	O
the	O
regression	B
fit	O
but	O
does	O
alter	O
the	O
interpretation	O
of	O
the	O
coefficients	O
if	O
we	O
had	O
coded	O
males	O
as	O
and	O
females	O
as	O
then	O
the	O
estimates	O
for	O
and	O
would	O
have	O
been	O
and	O
respectively	O
leading	O
once	O
again	O
to	O
a	O
prediction	B
of	O
credit	B
card	O
debt	O
of	O
for	O
males	O
and	O
a	O
prediction	B
of	O
for	O
females	O
alternatively	O
instead	O
of	O
a	O
coding	O
scheme	O
we	O
could	O
create	O
a	O
dummy	B
variable	B
other	O
considerations	O
in	O
the	O
regression	B
model	B
xi	O
if	O
ith	O
person	O
is	O
female	O
if	O
ith	O
person	O
is	O
male	O
and	O
use	O
this	O
variable	B
in	O
the	O
regression	B
equation	O
this	O
results	O
in	O
the	O
model	B
yi	O
if	O
ith	O
person	O
is	O
female	O
if	O
ith	O
person	O
is	O
male	O
now	O
can	O
be	O
interpreted	O
as	O
the	O
overall	O
average	B
credit	B
card	O
balance	O
the	O
gender	O
effect	O
and	O
is	O
the	O
amount	O
that	O
females	O
are	O
above	O
the	O
average	B
and	O
males	O
are	O
below	O
the	O
average	B
in	O
this	O
example	O
the	O
estimate	O
for	O
would	O
be	O
halfway	O
between	O
the	O
male	O
and	O
female	O
averages	O
of	O
and	O
the	O
estimate	O
for	O
would	O
be	O
which	O
is	O
half	O
of	O
the	O
average	B
difference	O
between	O
females	O
and	O
males	O
it	O
is	O
important	O
to	O
note	O
that	O
the	O
final	O
predictions	O
for	O
the	O
credit	B
balances	O
of	O
males	O
and	O
females	O
will	O
be	O
identical	O
regardless	O
of	O
the	O
coding	O
scheme	O
used	O
the	O
only	O
difference	O
is	O
in	O
the	O
way	O
that	O
the	O
coefficients	O
are	O
interpreted	O
qualitative	B
predictors	O
with	O
more	O
than	O
two	O
levels	O
when	O
a	O
qualitative	B
predictor	B
has	O
more	O
than	O
two	O
levels	O
a	O
single	B
dummy	B
variable	B
cannot	O
represent	O
all	O
possible	O
values	O
in	O
this	O
situation	O
we	O
can	O
create	O
additional	O
dummy	B
variables	O
for	O
example	O
for	O
the	O
ethnicity	O
variable	B
we	O
create	O
two	O
dummy	B
variables	O
the	O
first	O
could	O
be	O
if	O
ith	O
person	O
is	O
asian	O
if	O
ith	O
person	O
is	O
not	O
asian	O
and	O
the	O
second	O
could	O
be	O
if	O
ith	O
person	O
is	O
caucasian	O
if	O
ith	O
person	O
is	O
not	O
caucasian	O
then	O
both	O
of	O
these	O
variables	O
can	O
be	O
used	O
in	O
the	O
regression	B
equation	O
in	O
order	O
to	O
obtain	O
the	O
model	B
yi	O
if	O
ith	O
person	O
is	O
asian	O
if	O
ith	O
person	O
is	O
caucasian	O
if	O
ith	O
person	O
is	O
african	O
american	O
now	O
can	O
be	O
interpreted	O
as	O
the	O
average	B
credit	B
card	O
balance	O
for	O
african	O
americans	O
can	O
be	O
interpreted	O
as	O
the	O
difference	O
in	O
the	O
average	B
balance	O
between	O
the	O
asian	O
and	O
african	O
american	O
categories	O
and	O
can	O
be	O
interpreted	O
as	O
the	O
difference	O
in	O
the	O
average	B
balance	O
between	O
the	O
caucasian	O
and	O
linear	B
regression	B
intercept	B
ethnicityasian	O
ethnicitycaucasian	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
table	O
least	B
squares	I
coefficient	O
estimates	O
associated	O
with	O
the	O
regression	B
of	O
balance	O
onto	O
ethnicity	O
in	O
the	O
credit	B
data	B
set	B
the	O
linear	B
model	B
is	O
given	O
in	O
that	O
is	O
ethnicity	O
is	O
encoded	O
via	O
two	O
dummy	B
variables	O
and	O
african	O
american	O
categories	O
there	O
will	O
always	O
be	O
one	O
fewer	O
dummy	B
variable	B
than	O
the	O
number	O
of	O
levels	O
the	O
level	B
with	O
no	O
dummy	B
variable	B
african	O
american	O
in	O
this	O
example	O
is	O
known	O
as	O
the	O
baseline	B
from	O
table	O
we	O
see	O
that	O
the	O
estimated	O
balance	O
for	O
the	O
baseline	B
african	O
american	O
is	O
it	O
is	O
estimated	O
that	O
the	O
asian	O
category	O
will	O
have	O
less	O
debt	O
than	O
the	O
african	O
american	O
category	O
and	O
that	O
the	O
caucasian	O
category	O
will	O
have	O
less	O
debt	O
than	O
the	O
african	O
american	O
category	O
however	O
the	O
p-values	O
associated	O
with	O
the	O
coefficient	O
estimates	O
for	O
the	O
two	O
dummy	B
variables	O
are	O
very	O
large	O
suggesting	O
no	O
statistical	O
evidence	O
of	O
a	O
real	O
difference	O
in	O
credit	B
card	O
balance	O
between	O
the	O
ethnicities	O
once	O
again	O
the	O
level	B
selected	O
as	O
the	O
baseline	B
category	O
is	O
arbitrary	O
and	O
the	O
final	O
predictions	O
for	O
each	O
group	O
will	O
be	O
the	O
same	O
regardless	O
of	O
this	O
choice	O
however	O
the	O
coefficients	O
and	O
their	O
p-values	O
do	O
depend	O
on	O
the	O
choice	O
of	O
dummy	B
variable	B
coding	O
rather	O
than	O
rely	O
on	O
the	O
individual	O
coefficients	O
we	O
can	O
use	O
an	O
f-test	O
to	O
test	O
this	O
does	O
not	O
depend	O
on	O
the	O
coding	O
this	O
f-test	O
has	O
a	O
p-value	B
of	O
indicating	O
that	O
we	O
cannot	O
reject	O
the	O
null	B
hypothesis	B
that	O
there	O
is	O
no	O
relationship	O
between	O
balance	O
and	O
ethnicity	O
using	O
this	O
dummy	B
variable	B
approach	B
presents	O
no	O
difficulties	O
when	O
incorporating	O
both	O
quantitative	B
and	O
qualitative	B
predictors	O
for	O
example	O
to	O
regress	O
balance	O
on	O
both	O
a	O
quantitative	B
variable	B
such	O
as	O
income	B
and	O
a	O
qualitative	B
variable	B
such	O
as	O
student	O
we	O
must	O
simply	O
create	O
a	O
dummy	B
variable	B
for	O
student	O
and	O
then	O
fit	O
a	O
multiple	B
regression	B
model	B
using	O
income	B
and	O
the	O
dummy	B
variable	B
as	O
predictors	O
for	O
credit	B
card	O
balance	O
there	O
are	O
many	O
different	O
ways	O
of	O
coding	O
qualitative	B
variables	O
besides	O
the	O
dummy	B
variable	B
approach	B
taken	O
here	O
all	O
of	O
these	O
approaches	O
lead	O
to	O
equivalent	O
model	B
fits	O
but	O
the	O
coefficients	O
are	O
different	O
and	O
have	O
different	O
interpretations	O
and	O
are	O
designed	O
to	O
measure	O
particular	O
contrasts	O
this	O
topic	O
is	O
beyond	O
the	O
scope	O
of	O
the	O
book	O
and	O
so	O
we	O
will	O
not	O
pursue	O
it	O
further	O
extensions	O
of	O
the	O
linear	B
model	B
the	O
standard	O
linear	B
regression	B
model	B
provides	O
interpretable	O
results	O
and	O
works	O
quite	O
well	O
on	O
many	O
real-world	O
problems	O
however	O
it	O
makes	O
several	O
highly	O
restrictive	O
assumptions	O
that	O
are	O
often	O
violated	O
in	O
practice	O
two	O
of	O
the	O
most	O
important	O
assumptions	O
state	O
that	O
the	O
relationship	O
between	O
the	O
predictors	O
and	O
response	B
are	O
additive	B
and	O
linear	B
the	O
additive	B
assumption	O
baseline	B
contrast	B
additive	B
linear	B
other	O
considerations	O
in	O
the	O
regression	B
model	B
means	O
that	O
the	O
effect	O
of	O
changes	O
in	O
a	O
predictor	B
xj	O
on	O
the	O
response	B
y	O
is	O
independent	B
of	O
the	O
values	O
of	O
the	O
other	O
predictors	O
the	O
linear	B
assumption	O
states	O
that	O
the	O
change	O
in	O
the	O
response	B
y	O
due	O
to	O
a	O
one-unit	O
change	O
in	O
xj	O
is	O
constant	O
regardless	O
of	O
the	O
value	O
of	O
xj	O
in	O
this	O
book	O
we	O
examine	O
a	O
number	O
of	O
sophisticated	O
methods	O
that	O
relax	O
these	O
two	O
assumptions	O
here	O
we	O
briefly	O
examine	O
some	O
common	O
classical	O
approaches	O
for	O
extending	O
the	O
linear	B
model	B
removing	O
the	O
additive	B
assumption	O
in	O
our	O
previous	O
analysis	B
of	O
the	O
advertising	B
data	B
we	O
concluded	O
that	O
both	O
tv	O
and	O
radio	O
seem	O
to	O
be	O
associated	O
with	O
sales	O
the	O
linear	B
models	O
that	O
formed	O
the	O
basis	B
for	O
this	O
conclusion	O
assumed	O
that	O
the	O
effect	O
on	O
sales	O
of	O
increasing	O
one	O
advertising	B
medium	O
is	O
independent	B
of	O
the	O
amount	O
spent	O
on	O
the	O
other	O
media	O
for	O
example	O
the	O
linear	B
model	B
states	O
that	O
the	O
average	B
effect	O
on	O
sales	O
of	O
a	O
one-unit	O
increase	O
in	O
tv	O
is	O
always	O
regardless	O
of	O
the	O
amount	O
spent	O
on	O
radio	O
however	O
this	O
simple	B
model	B
may	O
be	O
incorrect	O
suppose	O
that	O
spending	O
money	O
on	O
radio	O
advertising	B
actually	O
increases	O
the	O
effectiveness	O
of	O
tv	O
advertising	B
so	O
that	O
the	O
slope	B
term	B
for	O
tv	O
should	O
increase	O
as	O
radio	O
increases	O
in	O
this	O
situation	O
given	O
a	O
fixed	O
budget	O
of	O
spending	O
half	O
on	O
radio	O
and	O
half	O
on	O
tv	O
may	O
increase	O
sales	O
more	O
than	O
allocating	O
the	O
entire	O
amount	O
to	O
either	O
tv	O
or	O
to	O
radio	O
in	O
marketing	O
this	O
is	O
known	O
as	O
a	O
synergy	B
effect	O
and	O
in	O
statistics	O
it	O
is	O
referred	O
to	O
as	O
an	O
interaction	B
effect	O
figure	O
suggests	O
that	O
such	O
an	O
effect	O
may	O
be	O
present	O
in	O
the	O
advertising	B
data	B
notice	O
that	O
when	O
levels	O
of	O
either	O
tv	O
or	O
radio	O
are	O
low	O
then	O
the	O
true	O
sales	O
are	O
lower	O
than	O
predicted	O
by	O
the	O
linear	B
model	B
but	O
when	O
advertising	B
is	O
split	O
between	O
the	O
two	O
media	O
then	O
the	O
model	B
tends	O
to	O
underestimate	O
sales	O
consider	O
the	O
standard	O
linear	B
regression	B
model	B
with	O
two	O
variables	O
y	O
according	O
to	O
this	O
model	B
if	O
we	O
increase	O
by	O
one	O
unit	O
then	O
y	O
will	O
increase	O
by	O
an	O
average	B
of	O
units	O
notice	O
that	O
the	O
presence	O
of	O
does	O
not	O
alter	O
this	O
statement	O
that	O
is	O
regardless	O
of	O
the	O
value	O
of	O
a	O
one-unit	O
increase	O
in	O
will	O
lead	O
to	O
a	O
increase	O
in	O
y	O
one	O
way	O
of	O
extending	O
this	O
model	B
to	O
allow	O
for	O
interaction	B
effects	O
is	O
to	O
include	O
a	O
third	O
predictor	B
called	O
an	O
interaction	B
term	B
which	O
is	O
constructed	O
by	O
computing	O
the	O
product	O
of	O
and	O
this	O
results	O
in	O
the	O
model	B
y	O
how	O
does	O
inclusion	O
of	O
this	O
interaction	B
term	B
relax	O
the	O
additive	B
assumption	O
notice	O
that	O
can	O
be	O
rewritten	O
as	O
y	O
linear	B
regression	B
intercept	B
tv	O
radio	O
tv	O
radio	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
table	O
for	O
the	O
advertising	B
data	B
least	B
squares	I
coefficient	O
estimates	O
associated	O
with	O
the	O
regression	B
of	O
sales	O
onto	O
tv	O
and	O
radio	O
with	O
an	O
interaction	B
term	B
as	O
in	O
where	O
since	O
changes	O
with	O
the	O
effect	O
of	O
on	O
y	O
is	O
no	O
longer	O
constant	O
adjusting	O
will	O
change	O
the	O
impact	O
of	O
on	O
y	O
for	O
example	O
suppose	O
that	O
we	O
are	O
interested	O
in	O
studying	O
the	O
productivity	O
of	O
a	O
factory	O
we	O
wish	O
to	O
predict	O
the	O
number	O
of	O
units	O
produced	O
on	O
the	O
basis	B
of	O
the	O
number	O
of	O
production	O
lines	O
and	O
the	O
total	O
number	O
of	O
workers	O
it	O
seems	O
likely	O
that	O
the	O
effect	O
of	O
increasing	O
the	O
number	O
of	O
production	O
lines	O
will	O
depend	O
on	O
the	O
number	O
of	O
workers	O
since	O
if	O
no	O
workers	O
are	O
available	O
to	O
operate	O
the	O
lines	O
then	O
increasing	O
the	O
number	O
of	O
lines	O
will	O
not	O
increase	O
production	O
this	O
suggests	O
that	O
it	O
would	O
be	O
appropriate	O
to	O
include	O
an	O
interaction	B
term	B
between	O
lines	O
and	O
workers	O
in	O
a	O
linear	B
model	B
to	O
predict	O
units	O
suppose	O
that	O
when	O
we	O
fit	O
the	O
model	B
we	O
obtain	O
units	O
lines	O
workers	O
workers	O
workers	O
lines	O
workers	O
in	O
other	O
words	O
adding	O
an	O
additional	O
line	B
will	O
increase	O
the	O
number	O
of	O
units	O
produced	O
by	O
workers	O
hence	O
the	O
more	O
workers	O
we	O
have	O
the	O
stronger	O
will	O
be	O
the	O
effect	O
of	O
lines	O
we	O
now	O
return	O
to	O
the	O
advertising	B
example	O
a	O
linear	B
model	B
that	O
uses	O
radio	O
tv	O
and	O
an	O
interaction	B
between	O
the	O
two	O
to	O
predict	O
sales	O
takes	O
the	O
form	O
sales	O
tv	O
radio	O
tv	O
radio	O
tv	O
radio	O
we	O
can	O
interpret	O
as	O
the	O
increase	O
in	O
the	O
effectiveness	O
of	O
tv	O
advertising	B
for	O
a	O
one	O
unit	O
increase	O
in	O
radio	O
advertising	B
vice-versa	O
the	O
coefficients	O
that	O
result	O
from	O
fitting	O
the	O
model	B
are	O
given	O
in	O
table	O
the	O
results	O
in	O
table	O
strongly	O
suggest	O
that	O
the	O
model	B
that	O
includes	O
the	O
interaction	B
term	B
is	O
superior	O
to	O
the	O
model	B
that	O
contains	O
only	O
main	B
effects	I
the	O
p-value	B
for	O
the	O
interaction	B
term	B
tv	O
radio	O
is	O
extremely	O
low	O
indicating	O
that	O
there	O
is	O
strong	O
evidence	O
for	O
ha	O
in	O
other	O
words	O
it	O
is	O
clear	O
that	O
the	O
true	O
relationship	O
is	O
not	O
additive	B
the	O
for	O
the	O
model	B
is	O
compared	O
to	O
only	O
for	O
the	O
model	B
that	O
predicts	O
sales	O
using	O
tv	O
and	O
radio	O
without	O
an	O
interaction	B
term	B
this	O
means	O
that	O
of	O
the	O
variability	O
in	O
sales	O
that	O
remains	O
after	O
fitting	O
the	O
additive	B
model	B
has	O
been	O
explained	B
by	O
the	O
interaction	B
term	B
the	O
coefficient	O
main	O
effect	O
other	O
considerations	O
in	O
the	O
regression	B
model	B
estimates	O
in	O
table	O
suggest	O
that	O
an	O
increase	O
in	O
tv	O
advertising	B
of	O
is	O
associated	O
with	O
increased	O
sales	O
of	O
radio	O
radio	O
an	O
increase	O
in	O
sales	O
of	O
tv	O
tv	O
units	O
units	O
and	O
an	O
increase	O
in	O
radio	O
advertising	B
of	O
will	O
be	O
associated	O
with	O
in	O
this	O
example	O
the	O
p-values	O
associated	O
with	O
tv	O
radio	O
and	O
the	O
interaction	B
term	B
all	O
are	O
statistically	O
significant	O
and	O
so	O
it	O
is	O
obvious	O
that	O
all	O
three	O
variables	O
should	O
be	O
included	O
in	O
the	O
model	B
however	O
it	O
is	O
sometimes	O
the	O
case	O
that	O
an	O
interaction	B
term	B
has	O
a	O
very	O
small	O
p-value	B
but	O
the	O
associated	O
main	B
effects	I
this	O
case	O
tv	O
and	O
radio	O
do	O
not	O
the	O
hierarchical	B
principle	I
states	O
that	O
if	O
we	O
include	O
an	O
interaction	B
in	O
a	O
model	B
we	O
should	O
also	O
include	O
the	O
main	B
effects	I
even	O
if	O
the	O
p-values	O
associated	O
with	O
their	O
coefficients	O
are	O
not	O
significant	O
in	O
other	O
words	O
if	O
the	O
interaction	B
between	O
and	O
seems	O
important	O
then	O
we	O
should	O
include	O
both	O
and	O
in	O
the	O
model	B
even	O
if	O
their	O
coefficient	O
estimates	O
have	O
large	O
p-values	O
the	O
rationale	O
for	O
this	O
principle	O
is	O
that	O
if	O
is	O
related	O
to	O
the	O
response	B
then	O
whether	O
or	O
not	O
the	O
coefficients	O
of	O
or	O
are	O
exactly	O
zero	O
is	O
of	O
little	O
interest	O
also	O
is	O
typically	O
correlated	O
with	O
and	O
and	O
so	O
leaving	O
them	O
out	O
tends	O
to	O
alter	O
the	O
meaning	O
of	O
the	O
interaction	B
hierarchical	B
principle	I
in	O
the	O
previous	O
example	O
we	O
considered	O
an	O
interaction	B
between	O
tv	O
and	O
radio	O
both	O
of	O
which	O
are	O
quantitative	B
variables	O
however	O
the	O
concept	O
of	O
interactions	O
applies	O
just	O
as	O
well	O
to	O
qualitative	B
variables	O
or	O
to	O
a	O
combination	O
of	O
quantitative	B
and	O
qualitative	B
variables	O
in	O
fact	O
an	O
interaction	B
between	O
a	O
qualitative	B
variable	B
and	O
a	O
quantitative	B
variable	B
has	O
a	O
particularly	O
nice	O
interpretation	O
consider	O
the	O
credit	B
data	B
set	B
from	O
section	O
and	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
balance	O
using	O
the	O
income	B
and	O
student	O
variables	O
in	O
the	O
absence	O
of	O
an	O
interaction	B
term	B
the	O
model	B
takes	O
the	O
form	O
balancei	O
incomei	O
if	O
ith	O
person	O
is	O
a	O
student	O
if	O
ith	O
person	O
is	O
not	O
a	O
student	O
incomei	O
if	O
ith	O
person	O
is	O
a	O
student	O
if	O
ith	O
person	O
is	O
not	O
a	O
student	O
notice	O
that	O
this	O
amounts	O
to	O
fitting	O
two	O
parallel	O
lines	O
to	O
the	O
data	B
one	O
for	O
students	O
and	O
one	O
for	O
non-students	O
the	O
lines	O
for	O
students	O
and	O
non-students	O
have	O
different	O
intercepts	O
versus	O
but	O
the	O
same	O
slope	B
this	O
is	O
illustrated	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
fact	O
that	O
the	O
lines	O
are	O
parallel	O
means	O
that	O
the	O
average	B
effect	O
on	O
balance	O
of	O
a	O
one-unit	O
increase	O
in	O
income	B
does	O
not	O
depend	O
on	O
whether	O
or	O
not	O
the	O
individual	O
is	O
a	O
student	O
this	O
represents	O
a	O
potentially	O
serious	O
limitation	O
of	O
the	O
model	B
since	O
in	O
fact	O
a	O
change	O
in	O
income	B
may	O
have	O
a	O
very	O
different	O
effect	O
on	O
the	O
credit	B
card	O
balance	O
of	O
a	O
student	O
versus	O
a	O
non-student	O
this	O
limitation	O
can	O
be	O
addressed	O
by	O
adding	O
an	O
interaction	B
variable	B
created	O
by	O
multiplying	O
income	B
with	O
the	O
dummy	B
variable	B
for	O
student	O
our	O
linear	B
regression	B
e	O
c	O
n	O
a	O
a	O
b	O
l	O
student	O
non	O
student	O
e	O
c	O
n	O
a	O
a	O
b	O
l	O
income	B
income	B
figure	O
for	O
the	O
credit	B
data	B
the	O
least	B
squares	I
lines	O
are	O
shown	O
for	O
prediction	B
of	O
balance	O
from	O
income	B
for	O
students	O
and	O
non-students	O
left	O
the	O
model	B
was	O
fit	O
there	O
is	O
no	O
interaction	B
between	O
income	B
and	O
student	O
right	O
the	O
model	B
was	O
fit	O
there	O
is	O
an	O
interaction	B
term	B
between	O
income	B
and	O
student	O
model	B
now	O
becomes	O
balancei	O
incomei	O
incomei	O
if	O
student	O
if	O
not	O
student	O
incomei	O
incomei	O
if	O
student	O
if	O
not	O
student	O
once	O
again	O
we	O
have	O
two	O
different	O
regression	B
lines	O
for	O
the	O
students	O
and	O
the	O
non-students	O
but	O
now	O
those	O
regression	B
lines	O
have	O
different	O
intercepts	O
versus	O
as	O
well	O
as	O
different	O
slopes	O
versus	O
this	O
allows	O
for	O
the	O
possibility	O
that	O
changes	O
in	O
income	B
may	O
affect	O
the	O
credit	B
card	O
balances	O
of	O
students	O
and	O
non-students	O
differently	O
the	O
right-hand	O
panel	O
of	O
figure	O
shows	O
the	O
estimated	O
relationships	O
between	O
income	B
and	O
balance	O
for	O
students	O
and	O
non-students	O
in	O
the	O
model	B
we	O
note	O
that	O
the	O
slope	B
for	O
students	O
is	O
lower	O
than	O
the	O
slope	B
for	O
non-students	O
this	O
suggests	O
that	O
increases	O
in	O
income	B
are	O
associated	O
with	O
smaller	O
increases	O
in	O
credit	B
card	O
balance	O
among	O
students	O
as	O
compared	O
to	O
non-students	O
non-linear	B
relationships	O
as	O
discussed	O
previously	O
the	O
linear	B
regression	B
model	B
assumes	O
a	O
linear	B
relationship	O
between	O
the	O
response	B
and	O
predictors	O
but	O
in	O
some	O
cases	O
the	O
true	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictors	O
may	O
be	O
nonlinear	O
here	O
we	O
present	O
a	O
very	O
simple	B
way	O
to	O
directly	O
extend	O
the	O
linear	B
model	B
to	O
accommodate	O
non-linear	B
relationships	O
using	O
polynomial	B
regression	B
in	O
later	O
chapters	O
we	O
will	O
present	O
more	O
complex	O
approaches	O
for	O
performing	O
non-linear	B
fits	O
in	O
more	O
general	O
settings	O
consider	O
figure	O
in	O
which	O
the	O
mpg	O
mileage	O
in	O
miles	O
per	O
gallon	O
versus	O
horsepower	O
is	O
shown	O
for	O
a	O
number	O
of	O
cars	O
in	O
the	O
auto	B
data	B
set	B
the	O
polynomial	B
regression	B
other	O
considerations	O
in	O
the	O
regression	B
model	B
linear	B
degree	O
degree	O
n	O
o	O
l	O
l	O
a	O
g	O
r	O
e	O
p	O
s	O
e	O
l	O
i	O
m	O
horsepower	O
figure	O
the	O
auto	B
data	B
set	B
for	O
a	O
number	O
of	O
cars	O
mpg	O
and	O
horsepower	O
are	O
shown	O
the	O
linear	B
regression	B
fit	O
is	O
shown	O
in	O
orange	O
the	O
linear	B
regression	B
fit	O
for	O
a	O
is	O
shown	O
as	O
a	O
blue	O
curve	O
the	O
linear	B
regression	B
model	B
that	O
includes	O
horsepower	O
fit	O
for	O
a	O
model	B
that	O
includes	O
all	O
polynomials	O
of	O
horsepower	O
up	O
to	O
fifth-degree	O
is	O
shown	O
in	O
green	O
orange	O
line	B
represents	O
the	O
linear	B
regression	B
fit	O
there	O
is	O
a	O
pronounced	O
relationship	O
between	O
mpg	O
and	O
horsepower	O
but	O
it	O
seems	O
clear	O
that	O
this	O
relationship	O
is	O
in	O
fact	O
non-linear	B
the	O
data	B
suggest	O
a	O
curved	O
relationship	O
a	O
simple	B
approach	B
for	O
incorporating	O
non-linear	B
associations	O
in	O
a	O
linear	B
model	B
is	O
to	O
include	O
transformed	O
versions	O
of	O
the	O
predictors	O
in	O
the	O
model	B
for	O
example	O
the	O
points	O
in	O
figure	O
seem	O
to	O
have	O
a	O
quadratic	B
shape	O
suggesting	O
that	O
a	O
model	B
of	O
the	O
form	O
mpg	O
horsepower	O
horsepower	O
quadratic	B
may	O
provide	O
a	O
better	O
fit	O
equation	O
involves	O
predicting	O
mpg	O
using	O
a	O
non-linear	B
function	B
of	O
horsepower	O
but	O
it	O
is	O
still	O
a	O
linear	B
model	B
that	O
is	O
is	O
simply	O
a	O
multiple	B
linear	B
regression	B
model	B
with	O
horsepower	O
so	O
we	O
can	O
use	O
standard	O
linear	B
regression	B
software	O
to	O
and	O
horsepower	O
estimate	O
and	O
in	O
order	O
to	O
produce	O
a	O
non-linear	B
fit	O
the	O
blue	O
curve	O
in	O
figure	O
shows	O
the	O
resulting	O
quadratic	B
fit	O
to	O
the	O
data	B
the	O
quadratic	B
fit	O
appears	O
to	O
be	O
substantially	O
better	O
than	O
the	O
fit	O
obtained	O
when	O
just	O
the	O
linear	B
term	B
is	O
included	O
the	O
of	O
the	O
quadratic	B
fit	O
is	O
compared	O
to	O
for	O
the	O
linear	B
fit	O
and	O
the	O
p-value	B
in	O
table	O
for	O
the	O
quadratic	B
term	B
is	O
highly	O
significant	O
if	O
including	O
horsepower	O
led	O
to	O
such	O
a	O
big	O
improvement	O
in	O
the	O
model	B
why	O
the	O
green	O
curve	O
or	O
even	O
horsepower	O
not	O
include	O
horsepower	O
horsepower	O
linear	B
regression	B
intercept	B
horsepower	O
horsepower	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
table	O
for	O
the	O
auto	B
data	B
set	B
least	B
squares	I
coefficient	O
estimates	O
associated	O
with	O
the	O
regression	B
of	O
mpg	O
onto	O
horsepower	O
and	O
horsepower	O
in	O
figure	O
displays	O
the	O
fit	O
that	O
results	O
from	O
including	O
all	O
polynomials	O
up	O
to	O
fifth	O
degree	O
in	O
the	O
model	B
the	O
resulting	O
fit	O
seems	O
unnecessarily	O
wiggly	O
that	O
is	O
it	O
is	O
unclear	O
that	O
including	O
the	O
additional	O
terms	O
really	O
has	O
led	O
to	O
a	O
better	O
fit	O
to	O
the	O
data	B
the	O
approach	B
that	O
we	O
have	O
just	O
described	O
for	O
extending	O
the	O
linear	B
model	B
to	O
accommodate	O
non-linear	B
relationships	O
is	O
known	O
as	O
polynomial	B
regression	B
since	O
we	O
have	O
included	O
polynomial	B
functions	O
of	O
the	O
predictors	O
in	O
the	O
regression	B
model	B
we	O
further	O
explore	O
this	O
approach	B
and	O
other	O
non-linear	B
extensions	O
of	O
the	O
linear	B
model	B
in	O
chapter	O
potential	O
problems	O
when	O
we	O
fit	O
a	O
linear	B
regression	B
model	B
to	O
a	O
particular	O
data	B
set	B
many	O
problems	O
may	O
occur	O
most	O
common	O
among	O
these	O
are	O
the	O
following	O
non-linearity	O
of	O
the	O
response-predictor	O
relationships	O
correlation	B
of	O
error	B
terms	O
non-constant	O
variance	B
of	O
error	B
terms	O
outliers	O
high-leverage	O
points	O
collinearity	B
in	O
practice	O
identifying	O
and	O
overcoming	O
these	O
problems	O
is	O
as	O
much	O
an	O
art	O
as	O
a	O
science	O
many	O
pages	O
in	O
countless	O
books	O
have	O
been	O
written	O
on	O
this	O
topic	O
since	O
the	O
linear	B
regression	B
model	B
is	O
not	O
our	O
primary	O
focus	O
here	O
we	O
will	O
provide	O
only	O
a	O
brief	O
summary	O
of	O
some	O
key	O
points	O
non-linearity	O
of	O
the	O
data	B
the	O
linear	B
regression	B
model	B
assumes	O
that	O
there	O
is	O
a	O
straight-line	O
relationship	O
between	O
the	O
predictors	O
and	O
the	O
response	B
if	O
the	O
true	O
relationship	O
is	O
far	O
from	O
linear	B
then	O
virtually	O
all	O
of	O
the	O
conclusions	O
that	O
we	O
draw	O
from	O
the	O
fit	O
are	O
suspect	O
in	O
addition	O
the	O
prediction	B
accuracy	O
of	O
the	O
model	B
can	O
be	O
significantly	O
reduced	O
residual	B
plots	O
are	O
a	O
useful	O
graphical	O
tool	O
for	O
identifying	O
non-linearity	O
given	O
a	O
simple	B
linear	B
regression	B
model	B
we	O
can	O
plot	B
the	O
residuals	B
ei	O
yi	O
yi	O
versus	O
the	O
predictor	B
xi	O
in	O
the	O
case	O
of	O
a	O
multiple	B
regression	B
model	B
residual	B
plot	B
other	O
considerations	O
in	O
the	O
regression	B
model	B
residual	B
plot	B
for	O
linear	B
fit	O
residual	B
plot	B
for	O
quadratic	B
fit	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
fitted	O
fitted	O
values	O
fitted	O
values	O
figure	O
plots	O
of	O
residuals	B
versus	O
predicted	O
fitted	O
values	O
for	O
the	O
auto	B
data	B
set	B
in	O
each	O
plot	B
the	O
red	O
line	B
is	O
a	O
smooth	O
fit	O
to	O
the	O
residuals	B
intended	O
to	O
make	O
it	O
easier	O
to	O
identify	O
a	O
trend	O
left	O
a	O
linear	B
regression	B
of	O
mpg	O
on	O
horsepower	O
a	O
strong	O
pattern	O
in	O
the	O
residuals	B
indicates	O
non-linearity	O
in	O
the	O
data	B
right	O
a	O
linear	B
there	O
is	O
little	O
pattern	O
in	O
the	O
regression	B
of	O
mpg	O
on	O
horsepower	O
and	O
horsepower	O
residuals	B
since	O
there	O
are	O
multiple	B
predictors	O
we	O
instead	O
plot	B
the	O
residuals	B
versus	O
the	O
predicted	O
fitted	O
values	O
yi	O
ideally	O
the	O
residual	B
plot	B
will	O
show	O
no	O
discernible	O
pattern	O
the	O
presence	O
of	O
a	O
pattern	O
may	O
indicate	O
a	O
problem	O
with	O
some	O
aspect	O
of	O
the	O
linear	B
model	B
the	O
left	O
panel	O
of	O
figure	O
displays	O
a	O
residual	B
plot	B
from	O
the	O
linear	B
regression	B
of	O
mpg	O
onto	O
horsepower	O
on	O
the	O
auto	B
data	B
set	B
that	O
was	O
illustrated	O
in	O
figure	O
the	O
red	O
line	B
is	O
a	O
smooth	O
fit	O
to	O
the	O
residuals	B
which	O
is	O
displayed	O
in	O
order	O
to	O
make	O
it	O
easier	O
to	O
identify	O
any	O
trends	O
the	O
residuals	B
exhibit	O
a	O
clear	O
u-shape	O
which	O
provides	O
a	O
strong	O
indication	O
of	O
non-linearity	O
in	O
the	O
data	B
in	O
contrast	B
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
the	O
residual	B
plot	B
that	O
results	O
from	O
the	O
model	B
which	O
contains	O
a	O
quadratic	B
term	B
there	O
appears	O
to	O
be	O
little	O
pattern	O
in	O
the	O
residuals	B
suggesting	O
that	O
the	O
quadratic	B
term	B
improves	O
the	O
fit	O
to	O
the	O
data	B
if	O
the	O
residual	B
plot	B
indicates	O
that	O
there	O
are	O
non-linear	B
associations	O
in	O
the	O
data	B
then	O
a	O
simple	B
approach	B
is	O
to	O
use	O
non-linear	B
transformations	O
of	O
the	O
x	O
and	O
x	O
in	O
the	O
regression	B
model	B
in	O
the	O
predictors	O
such	O
as	O
log	O
x	O
later	O
chapters	O
of	O
this	O
book	O
we	O
will	O
discuss	O
other	O
more	O
advanced	O
non-linear	B
approaches	O
for	O
addressing	O
this	O
issue	O
correlation	B
of	O
error	B
terms	O
an	O
important	O
assumption	O
of	O
the	O
linear	B
regression	B
model	B
is	O
that	O
the	O
error	B
terms	O
are	O
uncorrelated	O
what	O
does	O
this	O
mean	O
for	O
instance	O
if	O
the	O
errors	O
are	O
uncorrelated	O
then	O
the	O
fact	O
that	O
is	O
positive	O
provides	O
little	O
or	O
no	O
information	O
about	O
the	O
sign	O
of	O
the	O
standard	O
errors	O
that	O
are	O
computed	O
for	O
the	O
estimated	O
regression	B
coefficients	O
or	O
the	O
fitted	O
values	O
linear	B
regression	B
are	O
based	O
on	O
the	O
assumption	O
of	O
uncorrelated	O
error	B
terms	O
if	O
in	O
fact	O
there	O
is	O
correlation	B
among	O
the	O
error	B
terms	O
then	O
the	O
estimated	O
standard	O
errors	O
will	O
tend	O
to	O
underestimate	O
the	O
true	O
standard	O
errors	O
as	O
a	O
result	O
confidence	O
and	O
prediction	B
intervals	O
will	O
be	O
narrower	O
than	O
they	O
should	O
be	O
for	O
example	O
a	O
confidence	B
interval	B
may	O
in	O
reality	O
have	O
a	O
much	O
lower	O
probability	B
than	O
of	O
containing	O
the	O
true	O
value	O
of	O
the	O
parameter	B
in	O
addition	O
p-values	O
associated	O
with	O
the	O
model	B
will	O
be	O
lower	O
than	O
they	O
should	O
be	O
this	O
could	O
cause	O
us	O
to	O
erroneously	O
conclude	O
that	O
a	O
parameter	B
is	O
statistically	O
significant	O
in	O
short	O
if	O
the	O
error	B
terms	O
are	O
correlated	O
we	O
may	O
have	O
an	O
unwarranted	O
sense	O
of	O
confidence	O
in	O
our	O
model	B
as	O
an	O
extreme	O
example	O
suppose	O
we	O
accidentally	O
doubled	O
our	O
data	B
leading	O
to	O
observations	B
and	O
error	B
terms	O
identical	O
in	O
pairs	O
if	O
we	O
ignored	O
this	O
our	O
standard	B
error	B
calculations	O
would	O
be	O
as	O
if	O
we	O
had	O
a	O
sample	O
of	O
size	O
when	O
in	O
fact	O
we	O
have	O
only	O
n	O
samples	O
our	O
estimated	O
parameters	O
would	O
be	O
the	O
same	O
for	O
the	O
samples	O
as	O
for	O
the	O
n	O
samples	O
but	O
the	O
confidence	O
intervals	O
would	O
be	O
narrower	O
by	O
a	O
factor	B
of	O
why	O
might	O
correlations	O
among	O
the	O
error	B
terms	O
occur	O
such	O
correlations	O
frequently	O
occur	O
in	O
the	O
context	O
of	O
time	B
series	I
data	B
which	O
consists	O
of	O
observations	B
for	O
which	O
measurements	O
are	O
obtained	O
at	O
discrete	O
points	O
in	O
time	O
in	O
many	O
cases	O
observations	B
that	O
are	O
obtained	O
at	O
adjacent	O
time	O
points	O
will	O
have	O
positively	O
correlated	O
errors	O
in	O
order	O
to	O
determine	O
if	O
this	O
is	O
the	O
case	O
for	O
a	O
given	O
data	B
set	B
we	O
can	O
plot	B
the	O
residuals	B
from	O
our	O
model	B
as	O
a	O
function	B
of	O
time	O
if	O
the	O
errors	O
are	O
uncorrelated	O
then	O
there	O
should	O
be	O
no	O
discernible	O
pattern	O
on	O
the	O
other	O
hand	O
if	O
the	O
error	B
terms	O
are	O
positively	O
correlated	O
then	O
we	O
may	O
see	O
tracking	B
in	O
the	O
residuals	B
that	O
is	O
adjacent	O
residuals	B
may	O
have	O
similar	O
values	O
figure	O
provides	O
an	O
illustration	O
in	O
the	O
top	O
panel	O
we	O
see	O
the	O
residuals	B
from	O
a	O
linear	B
regression	B
fit	O
to	O
data	B
generated	O
with	O
uncorrelated	O
errors	O
there	O
is	O
no	O
evidence	O
of	O
a	O
time-related	O
trend	O
in	O
the	O
residuals	B
in	O
contrast	B
the	O
residuals	B
in	O
the	O
bottom	O
panel	O
are	O
from	O
a	O
data	B
set	B
in	O
which	O
adjacent	O
errors	O
had	O
a	O
correlation	B
of	O
now	O
there	O
is	O
a	O
clear	O
pattern	O
in	O
the	O
residuals	B
adjacent	O
residuals	B
tend	O
to	O
take	O
on	O
similar	O
values	O
finally	O
the	O
center	O
panel	O
illustrates	O
a	O
more	O
moderate	O
case	O
in	O
which	O
the	O
residuals	B
had	O
a	O
correlation	B
of	O
there	O
is	O
still	O
evidence	O
of	O
tracking	B
but	O
the	O
pattern	O
is	O
less	O
clear	O
many	O
methods	O
have	O
been	O
developed	O
to	O
properly	O
take	O
account	O
of	O
correlations	O
in	O
the	O
error	B
terms	O
in	O
time	B
series	I
data	B
correlation	B
among	O
the	O
error	B
terms	O
can	O
also	O
occur	O
outside	O
of	O
time	B
series	I
data	B
for	O
instance	O
consider	O
a	O
study	O
in	O
which	O
individuals	O
heights	O
are	O
predicted	O
from	O
their	O
weights	O
the	O
assumption	O
of	O
uncorrelated	O
errors	O
could	O
be	O
violated	O
if	O
some	O
of	O
the	O
individuals	O
in	O
the	O
study	O
are	O
members	O
of	O
the	O
same	O
family	O
or	O
eat	O
the	O
same	O
diet	O
or	O
have	O
been	O
exposed	O
to	O
the	O
same	O
environmental	O
factors	O
in	O
general	O
the	O
assumption	O
of	O
uncorrelated	O
errors	O
is	O
extremely	O
important	O
for	O
linear	B
regression	B
as	O
well	O
as	O
for	O
other	O
statistical	O
methods	O
and	O
good	O
experimental	O
design	O
is	O
crucial	O
in	O
order	O
to	O
mitigate	O
the	O
risk	O
of	O
such	O
correlations	O
time	B
series	I
tracking	B
l	O
i	O
a	O
u	O
d	O
s	O
e	O
r	O
l	O
i	O
a	O
u	O
d	O
s	O
e	O
r	O
l	O
i	O
a	O
u	O
d	O
s	O
e	O
r	O
other	O
considerations	O
in	O
the	O
regression	B
model	B
observation	O
figure	O
plots	O
of	O
residuals	B
from	O
simulated	O
time	B
series	I
data	B
sets	O
generated	O
with	O
differing	O
levels	O
of	O
correlation	B
between	O
error	B
terms	O
for	O
adjacent	O
time	O
points	O
non-constant	O
variance	B
of	O
error	B
terms	O
another	O
important	O
assumption	O
of	O
the	O
linear	B
regression	B
model	B
is	O
that	O
the	O
error	B
terms	O
have	O
a	O
constant	O
variance	B
vari	O
the	O
standard	O
errors	O
confidence	O
intervals	O
and	O
hypothesis	B
tests	O
associated	O
with	O
the	O
linear	B
model	B
rely	O
upon	O
this	O
assumption	O
unfortunately	O
it	O
is	O
often	O
the	O
case	O
that	O
the	O
variances	O
of	O
the	O
error	B
terms	O
are	O
non-constant	O
for	O
instance	O
the	O
variances	O
of	O
the	O
error	B
terms	O
may	O
increase	O
with	O
the	O
value	O
of	O
the	O
response	B
one	O
can	O
identify	O
non-constant	O
variances	O
in	O
the	O
errors	O
or	O
heteroscedasticity	B
from	O
the	O
presence	O
of	O
a	O
funnel	O
shape	O
in	O
the	O
residual	B
plot	B
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
in	O
which	O
the	O
magnitude	O
of	O
the	O
residuals	B
tends	O
to	O
increase	O
with	O
the	O
fitted	O
values	O
when	O
faced	O
with	O
this	O
problem	O
one	O
possible	O
solution	O
is	O
to	O
transy	O
such	O
form	O
the	O
response	B
y	O
using	O
a	O
concave	O
function	B
such	O
as	O
log	O
y	O
or	O
a	O
transformation	O
results	O
in	O
a	O
greater	O
amount	O
of	O
shrinkage	B
of	O
the	O
larger	O
responses	O
leading	O
to	O
a	O
reduction	O
in	O
heteroscedasticity	B
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
the	O
residual	B
plot	B
after	O
transforming	O
the	O
response	B
heteroscedasticity	B
linear	B
regression	B
response	B
y	O
response	B
logy	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
fitted	O
values	O
fitted	O
values	O
figure	O
residual	B
plots	O
in	O
each	O
plot	B
the	O
red	O
line	B
is	O
a	O
smooth	O
fit	O
to	O
the	O
residuals	B
intended	O
to	O
make	O
it	O
easier	O
to	O
identify	O
a	O
trend	O
the	O
blue	O
lines	O
track	O
the	O
outer	O
quantiles	O
of	O
the	O
residuals	B
and	O
emphasize	O
patterns	O
left	O
the	O
funnel	O
shape	O
indicates	O
heteroscedasticity	B
right	O
the	O
response	B
has	O
been	O
log	O
transformed	O
and	O
there	O
is	O
now	O
no	O
evidence	O
of	O
heteroscedasticity	B
using	O
log	O
y	O
the	O
residuals	B
now	O
appear	O
to	O
have	O
constant	O
variance	B
though	O
there	O
is	O
some	O
evidence	O
of	O
a	O
slight	O
non-linear	B
relationship	O
in	O
the	O
data	B
sometimes	O
we	O
have	O
a	O
good	O
idea	O
of	O
the	O
variance	B
of	O
each	O
response	B
for	O
example	O
the	O
ith	O
response	B
could	O
be	O
an	O
average	B
of	O
ni	O
raw	O
observations	B
if	O
each	O
of	O
these	O
raw	O
observations	B
is	O
uncorrelated	O
with	O
variance	B
then	O
their	O
average	B
has	O
variance	B
i	O
in	O
this	O
case	O
a	O
simple	B
remedy	O
is	O
to	O
fit	O
our	O
model	B
by	O
weighted	B
least	B
squares	I
with	O
weights	O
proportional	O
to	O
the	O
inverse	O
variances	O
i	O
e	O
wi	O
ni	O
in	O
this	O
case	O
most	O
linear	B
regression	B
software	O
allows	O
for	O
observation	O
weights	O
outliers	O
an	O
outlier	B
is	O
a	O
point	O
for	O
which	O
yi	O
is	O
far	O
from	O
the	O
value	O
predicted	O
by	O
the	O
model	B
outliers	O
can	O
arise	O
for	O
a	O
variety	O
of	O
reasons	O
such	O
as	O
incorrect	O
recording	O
of	O
an	O
observation	O
during	O
data	B
collection	O
the	O
red	O
point	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
illustrates	O
a	O
typical	O
outlier	B
the	O
red	O
solid	O
line	B
is	O
the	O
least	B
squares	I
regression	B
fit	O
while	O
the	O
blue	O
dashed	O
line	B
is	O
the	O
least	B
squares	I
fit	O
after	O
removal	O
of	O
the	O
outlier	B
in	O
this	O
case	O
removing	O
the	O
outlier	B
has	O
little	O
effect	O
on	O
the	O
least	B
squares	I
line	B
it	O
leads	O
to	O
almost	O
no	O
change	O
in	O
the	O
slope	B
and	O
a	O
miniscule	O
reduction	O
in	O
the	O
intercept	B
it	O
is	O
typical	O
for	O
an	O
outlier	B
that	O
does	O
not	O
have	O
an	O
unusual	O
predictor	B
value	O
to	O
have	O
little	O
effect	O
on	O
the	O
least	B
squares	I
fit	O
however	O
even	O
if	O
an	O
outlier	B
does	O
not	O
have	O
much	O
effect	O
on	O
the	O
least	B
squares	I
fit	O
it	O
can	O
cause	O
other	O
problems	O
for	O
instance	O
in	O
this	O
example	O
the	O
rse	O
is	O
when	O
the	O
outlier	B
is	O
included	O
in	O
the	O
regression	B
but	O
it	O
is	O
only	O
when	O
the	O
outlier	B
is	O
removed	O
since	O
the	O
rse	O
is	O
used	O
to	O
compute	O
all	O
confidence	O
intervals	O
and	O
weighted	B
least	B
squares	I
outlier	B
other	O
considerations	O
in	O
the	O
regression	B
model	B
y	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
l	O
i	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
d	O
e	O
z	O
i	O
t	O
n	O
e	O
d	O
u	O
s	O
t	O
x	O
fitted	O
values	O
fitted	O
values	O
figure	O
left	O
the	O
least	B
squares	I
regression	B
line	B
is	O
shown	O
in	O
red	O
and	O
the	O
regression	B
line	B
after	O
removing	O
the	O
outlier	B
is	O
shown	O
in	O
blue	O
center	O
the	O
residual	B
plot	B
clearly	O
identifies	O
the	O
outlier	B
right	O
the	O
outlier	B
has	O
a	O
studentized	B
residual	B
of	O
typically	O
we	O
expect	O
values	O
between	O
and	O
p-values	O
such	O
a	O
dramatic	O
increase	O
caused	O
by	O
a	O
single	B
data	B
point	O
can	O
have	O
implications	O
for	O
the	O
interpretation	O
of	O
the	O
fit	O
similarly	O
inclusion	O
of	O
the	O
outlier	B
causes	O
the	O
to	O
decline	O
from	O
to	O
residual	B
plots	O
can	O
be	O
used	O
to	O
identify	O
outliers	O
in	O
this	O
example	O
the	O
outlier	B
is	O
clearly	O
visible	O
in	O
the	O
residual	B
plot	B
illustrated	O
in	O
the	O
center	O
panel	O
of	O
figure	O
but	O
in	O
practice	O
it	O
can	O
be	O
difficult	O
to	O
decide	O
how	O
large	O
a	O
residual	B
needs	O
to	O
be	O
before	O
we	O
consider	O
the	O
point	O
to	O
be	O
an	O
outlier	B
to	O
address	O
this	O
problem	O
instead	O
of	O
plotting	O
the	O
residuals	B
we	O
can	O
plot	B
the	O
studentized	B
residuals	B
computed	O
by	O
dividing	O
each	O
residual	B
ei	O
by	O
its	O
estimated	O
standard	B
error	B
observations	B
whose	O
studentized	B
residuals	B
are	O
greater	O
than	O
in	O
absolute	O
value	O
are	O
possible	O
outliers	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
outlier	B
s	O
studentized	B
residual	B
exceeds	O
while	O
all	O
other	O
observations	B
have	O
studentized	B
residuals	B
between	O
and	O
if	O
we	O
believe	O
that	O
an	O
outlier	B
has	O
occurred	O
due	O
to	O
an	O
error	B
in	O
data	B
collection	O
or	O
recording	O
then	O
one	O
solution	O
is	O
to	O
simply	O
remove	O
the	O
observation	O
however	O
care	O
should	O
be	O
taken	O
since	O
an	O
outlier	B
may	O
instead	O
indicate	O
a	O
deficiency	O
with	O
the	O
model	B
such	O
as	O
a	O
missing	O
predictor	B
high	O
leverage	B
points	O
we	O
just	O
saw	O
that	O
outliers	O
are	O
observations	B
for	O
which	O
the	O
response	B
yi	O
is	O
unusual	O
given	O
the	O
predictor	B
xi	O
in	O
contrast	B
observations	B
with	O
high	O
leverage	B
have	O
an	O
unusual	O
value	O
for	O
xi	O
for	O
example	O
observation	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
has	O
high	O
leverage	B
in	O
that	O
the	O
predictor	B
value	O
for	O
this	O
observation	O
is	O
large	O
relative	O
to	O
the	O
other	O
observations	B
that	O
the	O
data	B
displayed	O
in	O
figure	O
are	O
the	O
same	O
as	O
the	O
data	B
displayed	O
in	O
figure	O
but	O
with	O
the	O
addition	O
of	O
a	O
single	B
high	O
leverage	B
observation	O
the	O
red	O
solid	O
line	B
is	O
the	O
least	B
squares	I
fit	O
to	O
the	O
data	B
while	O
the	O
blue	O
dashed	O
line	B
is	O
the	O
fit	O
produced	O
when	O
observation	O
is	O
removed	O
comparing	O
the	O
left-hand	O
panels	O
of	O
figures	O
and	O
we	O
observe	O
that	O
removing	O
the	O
high	O
leverage	B
observation	O
has	O
a	O
much	O
more	O
substantial	O
impact	O
on	O
the	O
least	B
squares	I
line	B
studentized	B
residual	B
high	O
leverage	B
linear	B
regression	B
y	O
x	O
l	O
i	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
d	O
e	O
z	O
i	O
t	O
n	O
e	O
d	O
u	O
s	O
t	O
x	O
leverage	B
figure	O
left	O
observation	O
is	O
a	O
high	O
leverage	B
point	O
while	O
is	O
not	O
the	O
red	O
line	B
is	O
the	O
fit	O
to	O
all	O
the	O
data	B
and	O
the	O
blue	O
line	B
is	O
the	O
fit	O
with	O
observation	O
removed	O
center	O
the	O
red	O
observation	O
is	O
not	O
unusual	O
in	O
terms	O
of	O
its	O
value	O
or	O
its	O
value	O
but	O
still	O
falls	O
outside	O
the	O
bulk	O
of	O
the	O
data	B
and	O
hence	O
has	O
high	O
leverage	B
right	O
observation	O
has	O
a	O
high	O
leverage	B
and	O
a	O
high	O
residual	B
than	O
removing	O
the	O
outlier	B
in	O
fact	O
high	O
leverage	B
observations	B
tend	O
to	O
have	O
a	O
sizable	O
impact	O
on	O
the	O
estimated	O
regression	B
line	B
it	O
is	O
cause	O
for	O
concern	O
if	O
the	O
least	B
squares	I
line	B
is	O
heavily	O
affected	O
by	O
just	O
a	O
couple	O
of	O
observations	B
because	O
any	O
problems	O
with	O
these	O
points	O
may	O
invalidate	O
the	O
entire	O
fit	O
for	O
this	O
reason	O
it	O
is	O
important	O
to	O
identify	O
high	O
leverage	B
observations	B
in	O
a	O
simple	B
linear	B
regression	B
high	O
leverage	B
observations	B
are	O
fairly	O
easy	O
to	O
identify	O
since	O
we	O
can	O
simply	O
look	O
for	O
observations	B
for	O
which	O
the	O
predictor	B
value	O
is	O
outside	O
of	O
the	O
normal	O
range	O
of	O
the	O
observations	B
but	O
in	O
a	O
multiple	B
linear	B
regression	B
with	O
many	O
predictors	O
it	O
is	O
possible	O
to	O
have	O
an	O
observation	O
that	O
is	O
well	O
within	O
the	O
range	O
of	O
each	O
individual	O
predictor	B
s	O
values	O
but	O
that	O
is	O
unusual	O
in	O
terms	O
of	O
the	O
full	O
set	B
of	O
predictors	O
an	O
example	O
is	O
shown	O
in	O
the	O
center	O
panel	O
of	O
figure	O
for	O
a	O
data	B
set	B
with	O
two	O
predictors	O
and	O
most	O
of	O
the	O
observations	B
predictor	B
values	O
fall	O
within	O
the	O
blue	O
dashed	O
ellipse	O
but	O
the	O
red	O
observation	O
is	O
well	O
outside	O
of	O
this	O
range	O
but	O
neither	O
its	O
value	O
for	O
nor	O
its	O
value	O
for	O
is	O
unusual	O
so	O
if	O
we	O
examine	O
just	O
or	O
just	O
we	O
will	O
fail	O
to	O
notice	O
this	O
high	O
leverage	B
point	O
this	O
problem	O
is	O
more	O
pronounced	O
in	O
multiple	B
regression	B
settings	O
with	O
more	O
than	O
two	O
predictors	O
because	O
then	O
there	O
is	O
no	O
simple	B
way	O
to	O
plot	B
all	O
dimensions	O
of	O
the	O
data	B
simultaneously	O
in	O
order	O
to	O
quantify	O
an	O
observation	O
s	O
leverage	B
we	O
compute	O
the	O
leverage	B
statistic	O
a	O
large	O
value	O
of	O
this	O
statistic	O
indicates	O
an	O
observation	O
with	O
high	O
leverage	B
for	O
a	O
simple	B
linear	B
regression	B
hi	O
n	O
n	O
it	O
is	O
clear	O
from	O
this	O
equation	O
that	O
hi	O
increases	O
with	O
the	O
distance	O
of	O
xi	O
from	O
x	O
there	O
is	O
a	O
simple	B
extension	O
of	O
hi	O
to	O
the	O
case	O
of	O
multiple	B
predictors	O
though	O
we	O
do	O
not	O
provide	O
the	O
formula	O
here	O
the	O
leverage	B
statistic	O
hi	O
is	O
always	O
between	O
and	O
and	O
the	O
average	B
leverage	B
for	O
all	O
the	O
observations	B
is	O
always	O
equal	O
to	O
so	O
if	O
a	O
given	O
observation	O
has	O
a	O
leverage	B
statistic	O
leverage	B
statistic	O
other	O
considerations	O
in	O
the	O
regression	B
model	B
e	O
g	O
a	O
g	O
n	O
i	O
t	O
a	O
r	O
limit	O
limit	O
figure	O
scatterplots	O
of	O
the	O
observations	B
from	O
the	O
credit	B
data	B
set	B
left	O
a	O
plot	B
of	O
age	O
versus	O
limit	O
these	O
two	O
variables	O
are	O
not	O
collinear	O
right	O
a	O
plot	B
of	O
rating	O
versus	O
limit	O
there	O
is	O
high	O
collinearity	B
that	O
greatly	O
exceeds	O
then	O
we	O
may	O
suspect	O
that	O
the	O
corresponding	O
point	O
has	O
high	O
leverage	B
the	O
right-hand	O
panel	O
of	O
figure	O
provides	O
a	O
plot	B
of	O
the	O
studentized	B
residuals	B
versus	O
hi	O
for	O
the	O
data	B
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
observation	O
stands	O
out	O
as	O
having	O
a	O
very	O
high	O
leverage	B
statistic	O
as	O
well	O
as	O
a	O
high	O
studentized	B
residual	B
in	O
other	O
words	O
it	O
is	O
an	O
outlier	B
as	O
well	O
as	O
a	O
high	O
leverage	B
observation	O
this	O
is	O
a	O
particularly	O
dangerous	O
combination	O
this	O
plot	B
also	O
reveals	O
the	O
reason	O
that	O
observation	O
had	O
relatively	O
little	O
effect	O
on	O
the	O
least	B
squares	I
fit	O
in	O
figure	O
it	O
has	O
low	O
leverage	B
collinearity	B
collinearity	B
refers	O
to	O
the	O
situation	O
in	O
which	O
two	O
or	O
more	O
predictor	B
variables	O
are	O
closely	O
related	O
to	O
one	O
another	O
the	O
concept	O
of	O
collinearity	B
is	O
illustrated	O
in	O
figure	O
using	O
the	O
credit	B
data	B
set	B
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
two	O
predictors	O
limit	O
and	O
age	O
appear	O
to	O
have	O
no	O
obvious	O
relationship	O
in	O
contrast	B
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
predictors	O
limit	O
and	O
rating	O
are	O
very	O
highly	O
correlated	O
with	O
each	O
other	O
and	O
we	O
say	O
that	O
they	O
are	O
collinear	O
the	O
presence	O
of	O
collinearity	B
can	O
pose	O
problems	O
in	O
the	O
regression	B
context	O
since	O
it	O
can	O
be	O
difficult	O
to	O
separate	O
out	O
the	O
individual	O
effects	O
of	O
collinear	O
variables	O
on	O
the	O
response	B
in	O
other	O
words	O
since	O
limit	O
and	O
rating	O
tend	O
to	O
increase	O
or	O
decrease	O
together	O
it	O
can	O
be	O
difficult	O
to	O
determine	O
how	O
each	O
one	O
separately	O
is	O
associated	O
with	O
the	O
response	B
balance	O
collinearity	B
figure	O
illustrates	O
some	O
of	O
the	O
difficulties	O
that	O
can	O
result	O
from	O
collinear	O
ity	O
the	O
left-hand	O
panel	O
of	O
figure	O
is	O
a	O
contour	B
plot	B
of	O
the	O
rss	O
associated	O
with	O
different	O
possible	O
coefficient	O
estimates	O
for	O
the	O
regression	B
of	O
balance	O
on	O
limit	O
and	O
age	O
each	O
ellipse	O
represents	O
a	O
set	B
of	O
coefficients	O
that	O
correspond	O
to	O
the	O
same	O
rss	O
with	O
ellipses	O
nearest	O
to	O
the	O
center	O
taking	O
on	O
the	O
lowest	O
values	O
of	O
rss	O
the	O
black	O
dots	O
and	O
associated	O
dashed	O
linear	B
regression	B
e	O
g	O
a	O
g	O
n	O
i	O
t	O
a	O
r	O
limit	O
limit	O
figure	O
contour	O
plots	O
for	O
the	O
rss	O
values	O
as	O
a	O
function	B
of	O
the	O
parameters	O
for	O
various	O
regressions	O
involving	O
the	O
credit	B
data	B
set	B
in	O
each	O
plot	B
the	O
black	O
dots	O
represent	O
the	O
coefficient	O
values	O
corresponding	O
to	O
the	O
minimum	O
rss	O
left	O
a	O
contour	B
plot	B
of	O
rss	O
for	O
the	O
regression	B
of	O
balance	O
onto	O
age	O
and	O
limit	O
the	O
minimum	O
value	O
is	O
well	O
defined	O
right	O
a	O
contour	B
plot	B
of	O
rss	O
for	O
the	O
regression	B
of	O
balance	O
onto	O
rating	O
and	O
limit	O
because	O
of	O
the	O
collinearity	B
there	O
are	O
many	O
pairs	O
limit	O
rating	O
with	O
a	O
similar	O
value	O
for	O
rss	O
lines	O
represent	O
the	O
coefficient	O
estimates	O
that	O
result	O
in	O
the	O
smallest	O
possible	O
rss	O
in	O
other	O
words	O
these	O
are	O
the	O
least	B
squares	I
estimates	O
the	O
axes	O
for	O
limit	O
and	O
age	O
have	O
been	O
scaled	O
so	O
that	O
the	O
plot	B
includes	O
possible	O
coefficient	O
estimates	O
that	O
are	O
up	O
to	O
four	O
standard	O
errors	O
on	O
either	O
side	O
of	O
the	O
least	B
squares	I
estimates	O
thus	O
the	O
plot	B
includes	O
all	O
plausible	O
values	O
for	O
the	O
coefficients	O
for	O
example	O
we	O
see	O
that	O
the	O
true	O
limit	O
coefficient	O
is	O
almost	O
certainly	O
somewhere	O
between	O
and	O
in	O
contrast	B
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
contour	O
plots	O
of	O
the	O
rss	O
associated	O
with	O
possible	O
coefficient	O
estimates	O
for	O
the	O
regression	B
of	O
balance	O
onto	O
limit	O
and	O
rating	O
which	O
we	O
know	O
to	O
be	O
highly	O
collinear	O
now	O
the	O
contours	O
run	O
along	O
a	O
narrow	O
valley	O
there	O
is	O
a	O
broad	O
range	O
of	O
values	O
for	O
the	O
coefficient	O
estimates	O
that	O
result	O
in	O
equal	O
values	O
for	O
rss	O
hence	O
a	O
small	O
change	O
in	O
the	O
data	B
could	O
cause	O
the	O
pair	O
of	O
coefficient	O
values	O
that	O
yield	O
the	O
smallest	O
rss	O
that	O
is	O
the	O
least	B
squares	I
estimates	O
to	O
move	O
anywhere	O
along	O
this	O
valley	O
this	O
results	O
in	O
a	O
great	O
deal	O
of	O
uncertainty	O
in	O
the	O
coefficient	O
estimates	O
notice	O
that	O
the	O
scale	O
for	O
the	O
limit	O
coefficient	O
now	O
runs	O
from	O
roughly	O
to	O
this	O
is	O
an	O
eight-fold	O
increase	O
over	O
the	O
plausible	O
range	O
of	O
the	O
limit	O
coefficient	O
in	O
the	O
regression	B
with	O
age	O
interestingly	O
even	O
though	O
the	O
limit	O
and	O
rating	O
coefficients	O
now	O
have	O
much	O
more	O
individual	O
uncertainty	O
they	O
will	O
almost	O
certainly	O
lie	O
somewhere	O
in	O
this	O
contour	O
valley	O
for	O
example	O
we	O
would	O
not	O
expect	O
the	O
true	O
value	O
of	O
the	O
limit	O
and	O
rating	O
coefficients	O
to	O
be	O
and	O
respectively	O
even	O
though	O
such	O
a	O
value	O
is	O
plausible	O
for	O
each	O
coefficient	O
individually	O
other	O
considerations	O
in	O
the	O
regression	B
model	B
model	B
model	B
intercept	B
age	O
limit	O
intercept	B
rating	O
limit	O
coefficient	O
std	O
error	B
t-statistic	B
p-value	B
table	O
the	O
results	O
for	O
two	O
multiple	B
regression	B
models	O
involving	O
the	O
credit	B
data	B
set	B
are	O
shown	O
model	B
is	O
a	O
regression	B
of	O
balance	O
on	O
age	O
and	O
limit	O
and	O
model	B
a	O
regression	B
of	O
balance	O
on	O
rating	O
and	O
limit	O
the	O
standard	B
error	B
of	O
limit	O
increases	O
in	O
the	O
second	O
regression	B
due	O
to	O
collinearity	B
since	O
collinearity	B
reduces	O
the	O
accuracy	O
of	O
the	O
estimates	O
of	O
the	O
regression	B
coefficients	O
it	O
causes	O
the	O
standard	B
error	B
for	O
j	O
to	O
grow	O
recall	B
that	O
the	O
t-statistic	B
for	O
each	O
predictor	B
is	O
calculated	O
by	O
dividing	O
j	O
by	O
its	O
standard	B
error	B
consequently	O
collinearity	B
results	O
in	O
a	O
decline	O
in	O
the	O
t-statistic	B
as	O
a	O
result	O
in	O
the	O
presence	O
of	O
collinearity	B
we	O
may	O
fail	O
to	O
reject	O
j	O
this	O
means	O
that	O
the	O
power	B
of	O
the	O
hypothesis	B
test	I
the	O
probability	B
of	O
correctly	O
power	B
detecting	O
a	O
non-zero	O
coefficient	O
is	O
reduced	O
by	O
collinearity	B
table	O
compares	O
the	O
coefficient	O
estimates	O
obtained	O
from	O
two	O
separate	O
multiple	B
regression	B
models	O
the	O
first	O
is	O
a	O
regression	B
of	O
balance	O
on	O
age	O
and	O
limit	O
and	O
the	O
second	O
is	O
a	O
regression	B
of	O
balance	O
on	O
rating	O
and	O
limit	O
in	O
the	O
first	O
regression	B
both	O
age	O
and	O
limit	O
are	O
highly	O
significant	O
with	O
very	O
small	O
pvalues	O
in	O
the	O
second	O
the	O
collinearity	B
between	O
limit	O
and	O
rating	O
has	O
caused	O
the	O
standard	B
error	B
for	O
the	O
limit	O
coefficient	O
estimate	O
to	O
increase	O
by	O
a	O
factor	B
of	O
and	O
the	O
p-value	B
to	O
increase	O
to	O
in	O
other	O
words	O
the	O
importance	B
of	O
the	O
limit	O
variable	B
has	O
been	O
masked	O
due	O
to	O
the	O
presence	O
of	O
collinearity	B
to	O
avoid	O
such	O
a	O
situation	O
it	O
is	O
desirable	O
to	O
identify	O
and	O
address	O
potential	O
collinearity	B
problems	O
while	O
fitting	O
the	O
model	B
a	O
simple	B
way	O
to	O
detect	O
collinearity	B
is	O
to	O
look	O
at	O
the	O
correlation	B
matrix	O
of	O
the	O
predictors	O
an	O
element	O
of	O
this	O
matrix	O
that	O
is	O
large	O
in	O
absolute	O
value	O
indicates	O
a	O
pair	O
of	O
highly	O
correlated	O
variables	O
and	O
therefore	O
a	O
collinearity	B
problem	O
in	O
the	O
data	B
unfortunately	O
not	O
all	O
collinearity	B
problems	O
can	O
be	O
detected	O
by	O
inspection	O
of	O
the	O
correlation	B
matrix	O
it	O
is	O
possible	O
for	O
collinearity	B
to	O
exist	O
between	O
three	O
or	O
more	O
variables	O
even	O
if	O
no	O
pair	O
of	O
variables	O
has	O
a	O
particularly	O
high	O
correlation	B
we	O
call	O
this	O
situation	O
multicollinearity	B
instead	O
of	O
inspecting	O
the	O
correlation	B
matrix	O
a	O
better	O
way	O
to	O
assess	O
multicollinearity	B
is	O
to	O
compute	O
the	O
variance	B
inflation	O
factor	B
the	O
vif	O
is	O
the	O
ratio	O
of	O
the	O
variance	B
of	O
j	O
when	O
fitting	O
the	O
full	O
model	B
divided	O
by	O
the	O
variance	B
of	O
j	O
if	O
fit	O
on	O
its	O
own	O
the	O
smallest	O
possible	O
value	O
for	O
vif	O
is	O
which	O
indicates	O
the	O
complete	B
absence	O
of	O
collinearity	B
typically	O
in	O
practice	O
there	O
is	O
a	O
small	O
amount	O
of	O
collinearity	B
among	O
the	O
predictors	O
as	O
a	O
rule	O
of	O
thumb	O
a	O
vif	O
value	O
that	O
exceeds	O
or	O
indicates	O
a	O
problematic	O
amount	O
of	O
multicollinearity	B
variance	B
inflation	O
factor	B
linear	B
regression	B
collinearity	B
the	O
vif	O
for	O
each	O
variable	B
can	O
be	O
computed	O
using	O
the	O
formula	O
vif	O
j	O
xjx	O
j	O
xjx	O
j	O
where	O
predictors	O
if	O
the	O
vif	O
will	O
be	O
large	O
is	O
the	O
from	O
a	O
regression	B
of	O
xj	O
onto	O
all	O
of	O
the	O
other	O
xjx	O
j	O
is	O
close	O
to	O
one	O
then	O
collinearity	B
is	O
present	O
and	O
so	O
in	O
the	O
credit	B
data	B
a	O
regression	B
of	O
balance	O
on	O
age	O
rating	O
and	O
limit	O
indicates	O
that	O
the	O
predictors	O
have	O
vif	O
values	O
of	O
and	O
as	O
we	O
suspected	O
there	O
is	O
considerable	O
collinearity	B
in	O
the	O
data	B
when	O
faced	O
with	O
the	O
problem	O
of	O
collinearity	B
there	O
are	O
two	O
simple	B
solutions	O
the	O
first	O
is	O
to	O
drop	O
one	O
of	O
the	O
problematic	O
variables	O
from	O
the	O
regression	B
this	O
can	O
usually	O
be	O
done	O
without	O
much	O
compromise	O
to	O
the	O
regression	B
fit	O
since	O
the	O
presence	O
of	O
collinearity	B
implies	O
that	O
the	O
information	O
that	O
this	O
variable	B
provides	O
about	O
the	O
response	B
is	O
redundant	O
in	O
the	O
presence	O
of	O
the	O
other	O
variables	O
for	O
instance	O
if	O
we	O
regress	O
balance	O
onto	O
age	O
and	O
limit	O
without	O
the	O
rating	O
predictor	B
then	O
the	O
resulting	O
vif	O
values	O
are	O
close	O
to	O
the	O
minimum	O
possible	O
value	O
of	O
and	O
the	O
drops	O
from	O
to	O
so	O
dropping	O
rating	O
from	O
the	O
set	B
of	O
predictors	O
has	O
effectively	O
solved	O
the	O
collinearity	B
problem	O
without	O
compromising	O
the	O
fit	O
the	O
second	O
solution	O
is	O
to	O
combine	O
the	O
collinear	O
variables	O
together	O
into	O
a	O
single	B
predictor	B
for	O
instance	O
we	O
might	O
take	O
the	O
average	B
of	O
standardized	O
versions	O
of	O
limit	O
and	O
rating	O
in	O
order	O
to	O
create	O
a	O
new	O
variable	B
that	O
measures	O
credit	B
worthiness	O
the	O
marketing	O
plan	O
we	O
now	O
briefly	O
return	O
to	O
the	O
seven	O
questions	O
about	O
the	O
advertising	B
data	B
that	O
we	O
set	B
out	O
to	O
answer	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
is	O
there	O
a	O
relationship	O
between	O
advertising	B
sales	O
and	O
budget	O
this	O
question	O
can	O
be	O
answered	O
by	O
fitting	O
a	O
multiple	B
regression	B
model	B
of	O
sales	O
onto	O
tv	O
radio	O
and	O
newspaper	O
as	O
in	O
and	O
testing	O
the	O
hypothesis	B
tv	O
radio	O
newspaper	O
in	O
section	O
we	O
showed	O
that	O
the	O
f-statistic	B
can	O
be	O
used	O
to	O
determine	O
whether	O
or	O
not	O
we	O
should	O
reject	O
this	O
null	B
hypothesis	B
in	O
this	O
case	O
the	O
p-value	B
corresponding	O
to	O
the	O
f-statistic	B
in	O
table	O
is	O
very	O
low	O
indicating	O
clear	O
evidence	O
of	O
a	O
relationship	O
between	O
advertising	B
and	O
sales	O
how	O
strong	O
is	O
the	O
relationship	O
we	O
discussed	O
two	O
measures	O
of	O
model	B
accuracy	O
in	O
section	O
first	O
the	O
rse	O
estimates	O
the	O
standard	O
deviation	O
of	O
the	O
response	B
from	O
the	O
population	B
regression	B
line	B
for	O
the	O
advertising	B
data	B
the	O
rse	O
is	O
the	O
marketing	O
plan	O
units	O
while	O
the	O
mean	O
value	O
for	O
the	O
response	B
is	O
indicating	O
a	O
percentage	O
error	B
of	O
roughly	O
second	O
the	O
statistic	O
records	O
the	O
percentage	O
of	O
variability	O
in	O
the	O
response	B
that	O
is	O
explained	B
by	O
the	O
predictors	O
the	O
predictors	O
explain	O
almost	O
of	O
the	O
variance	B
in	O
sales	O
the	O
rse	O
and	O
statistics	O
are	O
displayed	O
in	O
table	O
which	O
media	O
contribute	O
to	O
sales	O
to	O
answer	O
this	O
question	O
we	O
can	O
examine	O
the	O
p-values	O
associated	O
with	O
each	O
predictor	B
s	O
t-statistic	B
in	O
the	O
multiple	B
linear	B
regression	B
displayed	O
in	O
table	O
the	O
p-values	O
for	O
tv	O
and	O
radio	O
are	O
low	O
but	O
the	O
p-value	B
for	O
newspaper	O
is	O
not	O
this	O
suggests	O
that	O
only	O
tv	O
and	O
radio	O
are	O
related	O
to	O
sales	O
in	O
chapter	O
we	O
explore	O
this	O
question	O
in	O
greater	O
detail	O
how	O
large	O
is	O
the	O
effect	O
of	O
each	O
medium	O
on	O
sales	O
we	O
saw	O
in	O
section	O
that	O
the	O
standard	B
error	B
of	O
j	O
can	O
be	O
used	O
to	O
construct	O
confidence	O
intervals	O
for	O
j	O
for	O
the	O
advertising	B
data	B
the	O
confidence	O
intervals	O
are	O
as	O
follows	O
for	O
tv	O
for	O
radio	O
and	O
for	O
newspaper	O
the	O
confidence	O
intervals	O
for	O
tv	O
and	O
radio	O
are	O
narrow	O
and	O
far	O
from	O
zero	O
providing	O
evidence	O
that	O
these	O
media	O
are	O
related	O
to	O
sales	O
but	O
the	O
interval	B
for	O
newspaper	O
includes	O
zero	O
indicating	O
that	O
the	O
variable	B
is	O
not	O
statistically	O
significant	O
given	O
the	O
values	O
of	O
tv	O
and	O
radio	O
we	O
saw	O
in	O
section	O
that	O
collinearity	B
can	O
result	O
in	O
very	O
wide	O
standard	O
errors	O
could	O
collinearity	B
be	O
the	O
reason	O
that	O
the	O
confidence	B
interval	B
associated	O
with	O
newspaper	O
is	O
so	O
wide	O
the	O
vif	O
scores	O
are	O
and	O
for	O
tv	O
radio	O
and	O
newspaper	O
suggesting	O
no	O
evidence	O
of	O
collinearity	B
in	O
order	O
to	O
assess	O
the	O
association	O
of	O
each	O
medium	O
individually	O
on	O
sales	O
we	O
can	O
perform	O
three	O
separate	O
simple	B
linear	B
regressions	O
results	O
are	O
shown	O
in	O
tables	O
and	O
there	O
is	O
evidence	O
of	O
an	O
extremely	O
strong	O
association	O
between	O
tv	O
and	O
sales	O
and	O
between	O
radio	O
and	O
sales	O
there	O
is	O
evidence	O
of	O
a	O
mild	O
association	O
between	O
newspaper	O
and	O
sales	O
when	O
the	O
values	O
of	O
tv	O
and	O
radio	O
are	O
ignored	O
how	O
accurately	O
can	O
we	O
predict	O
future	O
sales	O
the	O
response	B
can	O
be	O
predicted	O
using	O
the	O
accuracy	O
associated	O
with	O
this	O
estimate	O
depends	O
on	O
whether	O
we	O
wish	O
to	O
predict	O
an	O
individual	O
response	B
y	O
f	O
or	O
the	O
average	B
response	B
f	O
if	O
the	O
former	O
we	O
use	O
a	O
prediction	B
interval	B
and	O
if	O
the	O
latter	O
we	O
use	O
a	O
confidence	B
interval	B
prediction	B
intervals	O
will	O
always	O
be	O
wider	O
than	O
confidence	O
intervals	O
because	O
they	O
account	O
for	O
the	O
uncertainty	O
associated	O
with	O
the	O
irreducible	B
error	B
linear	B
regression	B
is	O
the	O
relationship	O
linear	B
in	O
section	O
we	O
saw	O
that	O
residual	B
plots	O
can	O
be	O
used	O
in	O
order	O
to	O
identify	O
non-linearity	O
if	O
the	O
relationships	O
are	O
linear	B
then	O
the	O
residual	B
plots	O
should	O
display	O
no	O
pattern	O
in	O
the	O
case	O
of	O
the	O
advertising	B
data	B
we	O
observe	O
a	O
non-linear	B
effect	O
in	O
figure	O
though	O
this	O
effect	O
could	O
also	O
be	O
observed	O
in	O
a	O
residual	B
plot	B
in	O
section	O
we	O
discussed	O
the	O
inclusion	O
of	O
transformations	O
of	O
the	O
predictors	O
in	O
the	O
linear	B
regression	B
model	B
in	O
order	O
to	O
accommodate	O
non-linear	B
relationships	O
is	O
there	O
synergy	B
among	O
the	O
advertising	B
media	O
the	O
standard	O
linear	B
regression	B
model	B
assumes	O
an	O
additive	B
relationship	O
between	O
the	O
predictors	O
and	O
the	O
response	B
an	O
additive	B
model	B
is	O
easy	O
to	O
interpret	O
because	O
the	O
effect	O
of	O
each	O
predictor	B
on	O
the	O
response	B
is	O
unrelated	O
to	O
the	O
values	O
of	O
the	O
other	O
predictors	O
however	O
the	O
additive	B
assumption	O
may	O
be	O
unrealistic	O
for	O
certain	O
data	B
sets	O
in	O
section	O
we	O
showed	O
how	O
to	O
include	O
an	O
interaction	B
term	B
in	O
the	O
regression	B
model	B
in	O
order	O
to	O
accommodate	O
non-additive	O
relationships	O
a	O
small	O
p-value	B
associated	O
with	O
the	O
interaction	B
term	B
indicates	O
the	O
presence	O
of	O
such	O
relationships	O
figure	O
suggested	O
that	O
the	O
advertising	B
data	B
may	O
not	O
be	O
additive	B
including	O
an	O
interaction	B
term	B
in	O
the	O
model	B
results	O
in	O
a	O
substantial	O
increase	O
in	O
from	O
around	O
to	O
almost	O
comparison	O
of	O
linear	B
regression	B
with	O
k-nearest	O
neighbors	O
as	O
discussed	O
in	O
chapter	O
linear	B
regression	B
is	O
an	O
example	O
of	O
a	O
parametric	B
approach	B
because	O
it	O
assumes	O
a	O
linear	B
functional	O
form	O
for	O
f	O
parametric	B
methods	O
have	O
several	O
advantages	O
they	O
are	O
often	O
easy	O
to	O
fit	O
because	O
one	O
need	O
estimate	O
only	O
a	O
small	O
number	O
of	O
coefficients	O
in	O
the	O
case	O
of	O
linear	B
regression	B
the	O
coefficients	O
have	O
simple	B
interpretations	O
and	O
tests	O
of	O
statistical	O
significance	O
can	O
be	O
easily	O
performed	O
but	O
parametric	B
methods	O
do	O
have	O
a	O
disadvantage	O
by	O
construction	O
they	O
make	O
strong	O
assumptions	O
about	O
the	O
form	O
of	O
f	O
if	O
the	O
specified	O
functional	O
form	O
is	O
far	O
from	O
the	O
truth	O
and	O
prediction	B
accuracy	O
is	O
our	O
goal	O
then	O
the	O
parametric	B
method	O
will	O
perform	O
poorly	O
for	O
instance	O
if	O
we	O
assume	O
a	O
linear	B
relationship	O
between	O
x	O
and	O
y	O
but	O
the	O
true	O
relationship	O
is	O
far	O
from	O
linear	B
then	O
the	O
resulting	O
model	B
will	O
provide	O
a	O
poor	O
fit	O
to	O
the	O
data	B
and	O
any	O
conclusions	O
drawn	O
from	O
it	O
will	O
be	O
suspect	O
in	O
contrast	B
non-parametric	B
methods	O
do	O
not	O
explicitly	O
assume	O
a	O
parametric	B
form	O
for	O
f	O
and	O
thereby	O
provide	O
an	O
alternative	O
and	O
more	O
flexible	O
approach	B
for	O
performing	O
regression	B
we	O
discuss	O
various	O
non-parametric	B
methods	O
in	O
this	O
book	O
here	O
we	O
consider	O
one	O
of	O
the	O
simplest	O
and	O
best-known	O
non-parametric	B
methods	O
k-nearest	O
neighbors	O
regression	B
regression	B
k-nearest	O
neighbors	O
regression	B
comparison	O
of	O
linear	B
regression	B
with	O
k-nearest	O
neighbors	O
y	O
y	O
figure	O
plots	O
of	O
f	O
using	O
knn	O
regression	B
on	O
a	O
two-dimensional	O
data	B
set	B
with	O
observations	B
dots	O
left	O
k	O
results	O
in	O
a	O
rough	O
step	B
function	B
fit	O
right	O
k	O
produces	O
a	O
much	O
smoother	B
fit	O
the	O
knn	O
regression	B
method	O
is	O
closely	O
related	O
to	O
the	O
knn	O
classifier	B
discussed	O
in	O
chapter	O
given	O
a	O
value	O
for	O
k	O
and	O
a	O
prediction	B
point	O
knn	O
regression	B
first	O
identifies	O
the	O
k	O
training	O
observations	B
that	O
are	O
closest	O
to	O
represented	O
by	O
it	O
then	O
estimates	O
f	O
using	O
the	O
average	B
of	O
all	O
the	O
training	O
responses	O
in	O
in	O
other	O
words	O
f	O
k	O
yi	O
xi	O
figure	O
illustrates	O
two	O
knn	O
fits	O
on	O
a	O
data	B
set	B
with	O
p	O
predictors	O
the	O
fit	O
with	O
k	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
while	O
the	O
right-hand	O
panel	O
corresponds	O
to	O
k	O
we	O
see	O
that	O
when	O
k	O
the	O
knn	O
fit	O
perfectly	O
interpolates	O
the	O
training	O
observations	B
and	O
consequently	O
takes	O
the	O
form	O
of	O
a	O
step	B
function	B
when	O
k	O
the	O
knn	O
fit	O
still	O
is	O
a	O
step	B
function	B
but	O
averaging	O
over	O
nine	O
observations	B
results	O
in	O
much	O
smaller	O
regions	O
of	O
constant	O
prediction	B
and	O
consequently	O
a	O
smoother	B
fit	O
in	O
general	O
the	O
optimal	O
value	O
for	O
k	O
will	O
depend	O
on	O
the	O
bias-variance	O
tradeoff	O
which	O
we	O
introduced	O
in	O
chapter	O
a	O
small	O
value	O
for	O
k	O
provides	O
the	O
most	O
flexible	O
fit	O
which	O
will	O
have	O
low	O
bias	B
but	O
high	O
variance	B
this	O
variance	B
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
prediction	B
in	O
a	O
given	O
region	O
is	O
entirely	O
dependent	B
on	O
just	O
one	O
observation	O
in	O
contrast	B
larger	O
values	O
of	O
k	O
provide	O
a	O
smoother	B
and	O
less	O
variable	B
fit	O
the	O
prediction	B
in	O
a	O
region	O
is	O
an	O
average	B
of	O
several	O
points	O
and	O
so	O
changing	O
one	O
observation	O
has	O
a	O
smaller	O
effect	O
however	O
the	O
smoothing	B
may	O
cause	O
bias	B
by	O
masking	O
some	O
of	O
the	O
structure	O
in	O
f	O
in	O
chapter	O
we	O
introduce	O
several	O
approaches	O
for	O
estimating	O
test	O
error	B
rates	O
these	O
methods	O
can	O
be	O
used	O
to	O
identify	O
the	O
optimal	O
value	O
of	O
k	O
in	O
knn	O
regression	B
linear	B
regression	B
in	O
what	O
setting	O
will	O
a	O
parametric	B
approach	B
such	O
as	O
least	B
squares	I
linear	B
regression	B
outperform	O
a	O
non-parametric	B
approach	B
such	O
as	O
knn	O
regression	B
the	O
answer	O
is	O
simple	B
the	O
parametric	B
approach	B
will	O
outperform	O
the	O
nonparametric	O
approach	B
if	O
the	O
parametric	B
form	O
that	O
has	O
been	O
selected	O
is	O
close	O
to	O
the	O
true	O
form	O
of	O
f	O
figure	O
provides	O
an	O
example	O
with	O
data	B
generated	O
from	O
a	O
one-dimensional	O
linear	B
regression	B
model	B
the	O
black	O
solid	O
lines	O
represent	O
f	O
while	O
the	O
blue	O
curves	O
correspond	O
to	O
the	O
knn	O
fits	O
using	O
k	O
and	O
k	O
in	O
this	O
case	O
the	O
k	O
predictions	O
are	O
far	O
too	O
variable	B
while	O
the	O
smoother	B
k	O
fit	O
is	O
much	O
closer	O
to	O
f	O
however	O
since	O
the	O
true	O
relationship	O
is	O
linear	B
it	O
is	O
hard	O
for	O
a	O
non-parametric	B
approach	B
to	O
compete	O
with	O
linear	B
regression	B
a	O
non-parametric	B
approach	B
incurs	O
a	O
cost	O
in	O
variance	B
that	O
is	O
not	O
offset	O
by	O
a	O
reduction	O
in	O
bias	B
the	O
blue	O
dashed	O
line	B
in	O
the	O
lefthand	O
panel	O
of	O
figure	O
represents	O
the	O
linear	B
regression	B
fit	O
to	O
the	O
same	O
data	B
it	O
is	O
almost	O
perfect	O
the	O
right-hand	O
panel	O
of	O
figure	O
reveals	O
that	O
linear	B
regression	B
outperforms	O
knn	O
for	O
this	O
data	B
the	O
green	O
solid	O
line	B
plotted	O
as	O
a	O
function	B
of	O
represents	O
the	O
test	O
set	B
mean	B
squared	I
error	B
for	O
knn	O
the	O
knn	O
errors	O
are	O
well	O
above	O
the	O
black	O
dashed	O
line	B
which	O
is	O
the	O
test	O
mse	B
for	O
linear	B
regression	B
when	O
the	O
value	O
of	O
k	O
is	O
large	O
then	O
knn	O
performs	O
only	O
a	O
little	O
worse	O
than	O
least	B
squares	I
regression	B
in	O
terms	O
of	O
mse	B
it	O
performs	O
far	O
worse	O
when	O
k	O
is	O
small	O
in	O
practice	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
is	O
rarely	O
exactly	O
linear	B
figure	O
examines	O
the	O
relative	O
performances	O
of	O
least	B
squares	I
regression	B
and	O
knn	O
under	O
increasing	O
levels	O
of	O
non-linearity	O
in	O
the	O
relationship	O
between	O
x	O
and	O
y	O
in	O
the	O
top	O
row	O
the	O
true	O
relationship	O
is	O
nearly	O
linear	B
in	O
this	O
case	O
we	O
see	O
that	O
the	O
test	O
mse	B
for	O
linear	B
regression	B
is	O
still	O
superior	O
to	O
that	O
of	O
knn	O
for	O
low	O
values	O
of	O
k	O
however	O
for	O
k	O
knn	O
outperforms	O
linear	B
regression	B
the	O
second	O
row	O
illustrates	O
a	O
more	O
substantial	O
deviation	O
from	O
linearity	O
in	O
this	O
situation	O
knn	O
substantially	O
outperforms	O
linear	B
regression	B
for	O
all	O
values	O
of	O
k	O
note	O
that	O
as	O
the	O
extent	O
of	O
non-linearity	O
increases	O
there	O
is	O
little	O
change	O
in	O
the	O
test	O
set	B
mse	B
for	O
the	O
non-parametric	B
knn	O
method	O
but	O
there	O
is	O
a	O
large	O
increase	O
in	O
the	O
test	O
set	B
mse	B
of	O
linear	B
regression	B
figures	O
and	O
display	O
situations	O
in	O
which	O
knn	O
performs	O
slightly	O
worse	O
than	O
linear	B
regression	B
when	O
the	O
relationship	O
is	O
linear	B
but	O
much	O
better	O
than	O
linear	B
regression	B
for	O
non-linear	B
situations	O
in	O
a	O
real	O
life	O
situation	O
in	O
which	O
the	O
true	O
relationship	O
is	O
unknown	O
one	O
might	O
draw	O
the	O
conclusion	O
that	O
knn	O
should	O
be	O
favored	O
over	O
linear	B
regression	B
because	O
it	O
will	O
at	O
worst	O
be	O
slightly	O
inferior	O
than	O
linear	B
regression	B
if	O
the	O
true	O
relationship	O
is	O
linear	B
and	O
may	O
give	O
substantially	O
better	O
results	O
if	O
the	O
true	O
relationship	O
is	O
non-linear	B
but	O
in	O
reality	O
even	O
when	O
the	O
true	O
relationship	O
is	O
highly	O
non-linear	B
knn	O
may	O
still	O
provide	O
inferior	O
results	O
to	O
linear	B
regression	B
in	O
particular	O
both	O
figures	O
and	O
illustrate	O
settings	O
with	O
p	O
predictor	B
but	O
in	O
higher	O
dimensions	O
knn	O
often	O
performs	O
worse	O
than	O
linear	B
regression	B
figure	O
considers	O
the	O
same	O
strongly	O
non-linear	B
situation	O
as	O
in	O
the	O
second	O
row	O
of	O
figure	O
except	O
that	O
we	O
have	O
added	O
additional	O
noise	B
comparison	O
of	O
linear	B
regression	B
with	O
k-nearest	O
neighbors	O
y	O
y	O
x	O
x	O
figure	O
plots	O
of	O
f	O
using	O
knn	O
regression	B
on	O
a	O
one-dimensional	O
data	B
set	B
with	O
observations	B
the	O
true	O
relationship	O
is	O
given	O
by	O
the	O
black	O
solid	O
line	B
left	O
the	O
blue	O
curve	O
corresponds	O
to	O
k	O
and	O
interpolates	O
passes	O
directly	O
through	O
the	O
training	O
data	B
right	O
the	O
blue	O
curve	O
corresponds	O
to	O
k	O
and	O
represents	O
a	O
smoother	B
fit	O
y	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
x	O
figure	O
the	O
same	O
data	B
set	B
shown	O
in	O
figure	O
is	O
investigated	O
further	O
left	O
the	O
blue	O
dashed	O
line	B
is	O
the	O
least	B
squares	I
fit	O
to	O
the	O
data	B
since	O
f	O
is	O
in	O
fact	O
linear	B
as	O
the	O
black	O
line	B
the	O
least	B
squares	I
regression	B
line	B
provides	O
a	O
very	O
good	O
estimate	O
of	O
f	O
right	O
the	O
dashed	O
horizontal	O
line	B
represents	O
the	O
least	B
squares	I
test	O
set	B
mse	B
while	O
the	O
green	O
solid	O
line	B
corresponds	O
to	O
the	O
mse	B
for	O
knn	O
as	O
a	O
function	B
of	O
the	O
log	O
scale	O
linear	B
regression	B
achieves	O
a	O
lower	O
test	O
mse	B
than	O
does	O
knn	O
regression	B
since	O
f	O
is	O
in	O
fact	O
linear	B
for	O
knn	O
regression	B
the	O
best	O
results	O
occur	O
with	O
a	O
very	O
large	O
value	O
of	O
k	O
corresponding	O
to	O
a	O
small	O
value	O
of	O
linear	B
regression	B
x	O
y	O
y	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
x	O
figure	O
top	O
left	O
in	O
a	O
setting	O
with	O
a	O
slightly	O
non-linear	B
relationship	O
between	O
x	O
and	O
y	O
black	O
line	B
the	O
knn	O
fits	O
with	O
k	O
and	O
k	O
are	O
displayed	O
top	O
right	O
for	O
the	O
slightly	O
non-linear	B
data	B
the	O
test	O
set	B
mse	B
for	O
least	B
squares	I
regression	B
black	O
and	O
kn	O
n	O
with	O
various	O
values	O
of	O
are	O
displayed	O
bottom	O
left	O
and	O
bottom	O
right	O
as	O
in	O
the	O
top	O
panel	O
but	O
with	O
a	O
strongly	O
non-linear	B
relationship	O
between	O
x	O
and	O
y	O
predictors	O
that	O
are	O
not	O
associated	O
with	O
the	O
response	B
when	O
p	O
or	O
p	O
knn	O
outperforms	O
linear	B
regression	B
but	O
for	O
p	O
the	O
results	O
are	O
mixed	O
and	O
for	O
p	O
linear	B
regression	B
is	O
superior	O
to	O
knn	O
in	O
fact	O
the	O
increase	O
in	O
dimension	O
has	O
only	O
caused	O
a	O
small	O
deterioration	O
in	O
the	O
linear	B
regression	B
test	O
set	B
mse	B
but	O
it	O
has	O
caused	O
more	O
than	O
a	O
ten-fold	O
increase	O
in	O
the	O
mse	B
for	O
knn	O
this	O
decrease	O
in	O
performance	O
as	O
the	O
dimension	O
increases	O
is	O
a	O
common	O
problem	O
for	O
knn	O
and	O
results	O
from	O
the	O
fact	O
that	O
in	O
higher	O
dimensions	O
there	O
is	O
effectively	O
a	O
reduction	O
in	O
sample	O
size	O
in	O
this	O
data	B
set	B
there	O
are	O
training	O
observations	B
when	O
p	O
this	O
provides	O
enough	O
information	O
to	O
accurately	O
estimate	O
f	O
however	O
spreading	O
observations	B
over	O
p	O
dimensions	O
results	O
in	O
a	O
phenomenon	O
in	O
which	O
a	O
given	O
observation	O
has	O
no	O
nearby	O
neighbors	O
this	O
is	O
the	O
so-called	O
curse	B
of	I
dimensionality	I
that	O
is	O
the	O
k	O
observations	B
that	O
are	O
nearest	O
to	O
a	O
given	O
test	O
observation	O
may	O
be	O
very	O
far	O
away	O
from	O
in	O
p-dimensional	O
space	O
when	O
p	O
is	O
large	O
leading	O
to	O
a	O
curse	B
of	I
dimensionality	I
lab	O
linear	B
regression	B
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
figure	O
test	O
mse	B
for	O
linear	B
regression	B
dashed	O
lines	O
and	O
knn	O
curves	O
as	O
the	O
number	O
of	O
variables	O
p	O
increases	O
the	O
true	O
function	B
is	O
non	O
linear	B
in	O
the	O
first	O
variable	B
as	O
in	O
the	O
lower	O
panel	O
in	O
figure	O
and	O
does	O
not	O
depend	O
on	O
the	O
additional	O
variables	O
the	O
performance	O
of	O
linear	B
regression	B
deteriorates	O
slowly	O
in	O
the	O
presence	O
of	O
these	O
additional	O
noise	B
variables	O
whereas	O
knn	O
s	O
performance	O
degrades	O
much	O
more	O
quickly	O
as	O
p	O
increases	O
very	O
poor	O
prediction	B
of	O
f	O
and	O
hence	O
a	O
poor	O
knn	O
fit	O
as	O
a	O
general	O
rule	O
parametric	B
methods	O
will	O
tend	O
to	O
outperform	O
non-parametric	B
approaches	O
when	O
there	O
is	O
a	O
small	O
number	O
of	O
observations	B
per	O
predictor	B
even	O
in	O
problems	O
in	O
which	O
the	O
dimension	O
is	O
small	O
we	O
might	O
prefer	O
linear	B
regression	B
to	O
knn	O
from	O
an	O
interpretability	B
standpoint	O
if	O
the	O
test	O
mse	B
of	O
knn	O
is	O
only	O
slightly	O
lower	O
than	O
that	O
of	O
linear	B
regression	B
we	O
might	O
be	O
willing	O
to	O
forego	O
a	O
little	O
bit	O
of	O
prediction	B
accuracy	O
for	O
the	O
sake	O
of	O
a	O
simple	B
model	B
that	O
can	O
be	O
described	O
in	O
terms	O
of	O
just	O
a	O
few	O
coefficients	O
and	O
for	O
which	O
p-values	O
are	O
available	O
lab	O
linear	B
regression	B
libraries	O
the	O
library	O
function	B
is	O
used	O
to	O
load	O
libraries	O
or	O
groups	O
of	O
functions	O
and	O
data	B
sets	O
that	O
are	O
not	O
included	O
in	O
the	O
base	O
r	O
distribution	B
basic	O
functions	O
that	O
perform	O
least	B
squares	I
linear	B
regression	B
and	O
other	O
simple	B
analyses	O
come	O
standard	O
with	O
the	O
base	O
distribution	B
but	O
more	O
exotic	O
functions	O
require	O
additional	O
libraries	O
here	O
we	O
load	O
the	O
mass	O
package	O
which	O
is	O
a	O
very	O
large	O
collection	O
of	O
data	B
sets	O
and	O
functions	O
we	O
also	O
load	O
the	O
islr	O
package	O
which	O
includes	O
the	O
data	B
sets	O
associated	O
with	O
this	O
book	O
library	O
library	O
mass	O
library	O
islr	O
if	O
you	O
receive	O
an	O
error	B
message	O
when	O
loading	O
any	O
of	O
these	O
libraries	O
it	O
likely	O
indicates	O
that	O
the	O
corresponding	O
library	O
has	O
not	O
yet	O
been	O
installed	O
on	O
your	O
system	O
some	O
libraries	O
such	O
as	O
mass	O
come	O
with	O
r	O
and	O
do	O
not	O
need	O
to	O
be	O
separately	O
installed	O
on	O
your	O
computer	O
however	O
other	O
packages	O
such	O
as	O
linear	B
regression	B
islr	O
must	O
be	O
downloaded	O
the	O
first	O
time	O
they	O
are	O
used	O
this	O
can	O
be	O
done	O
directly	O
from	O
within	O
r	O
for	O
example	O
on	O
a	O
windows	O
system	O
select	O
the	O
install	O
package	O
option	O
under	O
the	O
packages	O
tab	O
after	O
you	O
select	O
any	O
mirror	O
site	O
a	O
list	O
of	O
available	O
packages	O
will	O
appear	O
simply	O
select	O
the	O
package	O
you	O
wish	O
to	O
install	O
and	O
r	O
will	O
automatically	O
download	O
the	O
package	O
alternatively	O
this	O
can	O
be	O
done	O
at	O
the	O
r	O
command	O
line	B
via	O
install	O
packagesislr	O
this	O
installation	O
only	O
needs	O
to	O
be	O
done	O
the	O
first	O
time	O
you	O
use	O
a	O
package	O
however	O
the	O
library	O
function	B
must	O
be	O
called	O
each	O
time	O
you	O
wish	O
to	O
use	O
a	O
given	O
package	O
simple	B
linear	B
regression	B
the	O
mass	O
library	O
contains	O
the	O
boston	B
data	B
set	B
which	O
records	O
medv	O
house	O
value	O
for	O
neighborhoods	O
around	O
boston	B
we	O
will	O
seek	O
to	O
predict	O
medv	O
using	O
predictors	O
such	O
as	O
rm	O
number	O
of	O
rooms	O
per	O
house	O
age	O
age	O
of	O
houses	O
and	O
lstat	O
of	O
households	O
with	O
low	O
socioeconomic	O
status	O
fix	O
boston	B
names	O
boston	B
crim	O
dis	O
zn	O
rad	O
indus	O
tax	O
chas	O
nox	O
ptratio	O
black	O
rm	O
lstat	O
age	O
medv	O
to	O
find	O
out	O
more	O
about	O
the	O
data	B
set	B
we	O
can	O
type	O
we	O
will	O
start	O
by	O
using	O
the	O
lm	O
function	B
to	O
fit	O
a	O
simple	B
linear	B
regression	B
model	B
with	O
medv	O
as	O
the	O
response	B
and	O
lstat	O
as	O
the	O
predictor	B
the	O
basic	O
syntax	O
is	O
lmy	O
xdata	O
where	O
y	O
is	O
the	O
response	B
x	O
is	O
the	O
predictor	B
and	O
data	B
is	O
the	O
data	B
set	B
in	O
which	O
these	O
two	O
variables	O
are	O
kept	O
lm	O
fit	O
lm	O
medv	O
lstat	O
error	B
in	O
eval	O
expr	O
envir	O
enclos	O
object	O
medv	O
not	O
found	O
lm	O
the	O
command	O
causes	O
an	O
error	B
because	O
r	O
does	O
not	O
know	O
where	O
to	O
find	O
the	O
variables	O
medv	O
and	O
lstat	O
the	O
next	O
line	B
tells	O
r	O
that	O
the	O
variables	O
are	O
in	O
boston	B
if	O
we	O
attach	O
boston	B
the	O
first	O
line	B
works	O
fine	O
because	O
r	O
now	O
recognizes	O
the	O
variables	O
lm	O
fit	O
lm	O
medv	O
lstat	O
data	B
boston	B
lm	O
fit	O
lm	O
medv	O
lstat	O
attach	O
boston	B
if	O
we	O
type	O
lm	O
fit	O
some	O
basic	O
information	O
about	O
the	O
model	B
is	O
output	B
for	O
more	O
detailed	O
information	O
we	O
use	O
summarylm	O
fit	O
this	O
gives	O
us	O
pvalues	O
and	O
standard	O
errors	O
for	O
the	O
coefficients	O
as	O
well	O
as	O
the	O
statistic	O
and	O
f-statistic	B
for	O
the	O
model	B
lm	O
fit	O
call	O
lm	O
formula	O
medv	O
lstat	O
lab	O
linear	B
regression	B
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
intercept	B
lstat	O
summary	O
lm	O
fit	O
call	O
lm	O
formula	O
medv	O
lstat	O
residuals	B
min	O
q	O
median	O
q	O
max	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
intercept	B
lstat	O
codes	O
residual	B
standard	B
error	B
on	O
degrees	B
of	I
freedom	I
multiple	B
r	O
squared	O
f	O
statistic	O
on	O
and	O
df	O
adjusted	O
r	O
squared	O
p	O
value	O
we	O
can	O
use	O
the	O
names	O
function	B
in	O
order	O
to	O
find	O
out	O
what	O
other	O
pieces	O
of	O
information	O
are	O
stored	O
in	O
lm	O
fit	O
although	O
we	O
can	O
extract	O
these	O
quantities	O
by	O
name	O
e	O
g	O
lm	O
fitcoefficients	O
it	O
is	O
safer	O
to	O
use	O
the	O
extractor	O
functions	O
like	O
coef	O
to	O
access	O
them	O
names	O
coef	O
names	O
lm	O
fit	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
rank	O
qr	O
call	O
coef	O
lm	O
fit	O
intercept	B
lstat	O
effects	O
residuals	B
fitted	O
values	O
assign	O
df	O
residual	B
terms	O
xlevels	O
model	B
in	O
order	O
to	O
obtain	O
a	O
confidence	B
interval	B
for	O
the	O
coefficient	O
estimates	O
we	O
can	O
use	O
the	O
confint	O
command	O
confint	O
confint	O
lm	O
fit	O
intercept	B
lstat	O
the	O
predict	O
function	B
can	O
be	O
used	O
to	O
produce	O
confidence	O
intervals	O
and	O
prediction	B
intervals	O
for	O
the	O
prediction	B
of	O
medv	O
for	O
a	O
given	O
value	O
of	O
lstat	O
predict	O
predict	O
lm	O
fit	O
data	B
frame	I
lstat	O
c	O
interval	B
confidenc	O
e	O
lwr	O
fit	O
upr	O
linear	B
regression	B
predict	O
lm	O
fit	O
data	B
frame	I
lstat	O
c	O
interval	B
predictio	O
n	O
lwr	O
fit	O
upr	O
for	O
instance	O
the	O
confidence	B
interval	B
associated	O
with	O
a	O
lstat	O
value	O
of	O
is	O
and	O
the	O
prediction	B
interval	B
is	O
as	O
expected	O
the	O
confidence	O
and	O
prediction	B
intervals	O
are	O
centered	O
around	O
the	O
same	O
point	O
predicted	O
value	O
of	O
for	O
medv	O
when	O
lstat	O
equals	O
but	O
the	O
latter	O
are	O
substantially	O
wider	O
we	O
will	O
now	O
plot	B
medv	O
and	O
lstat	O
along	O
with	O
the	O
least	B
squares	I
regression	B
line	B
using	O
the	O
plot	B
and	O
abline	O
functions	O
abline	O
plot	B
lstat	O
medv	O
abline	O
lm	O
fit	O
there	O
is	O
some	O
evidence	O
for	O
non-linearity	O
in	O
the	O
relationship	O
between	O
lstat	O
and	O
medv	O
we	O
will	O
explore	O
this	O
issue	O
later	O
in	O
this	O
lab	O
the	O
abline	O
function	B
can	O
be	O
used	O
to	O
draw	O
any	O
line	B
not	O
just	O
the	O
least	B
squares	I
regression	B
line	B
to	O
draw	O
a	O
line	B
with	O
intercept	B
a	O
and	O
slope	B
b	O
we	O
type	O
ablineab	O
below	O
we	O
experiment	O
with	O
some	O
additional	O
settings	O
for	O
plotting	O
lines	O
and	O
points	O
the	O
command	O
causes	O
the	O
width	O
of	O
the	O
regression	B
line	B
to	O
be	O
increased	O
by	O
a	O
factor	B
of	O
this	O
works	O
for	O
the	O
plot	B
and	O
lines	O
functions	O
also	O
we	O
can	O
also	O
use	O
the	O
pch	O
option	O
to	O
create	O
different	O
plotting	O
symbols	O
abline	O
lm	O
fit	O
lwd	O
abline	O
lm	O
fit	O
lwd	O
col	O
red	O
plot	B
lstat	O
medv	O
col	O
red	O
plot	B
lstat	O
medv	O
pch	O
plot	B
lstat	O
medv	O
pch	O
plot	B
pch	O
next	O
we	O
examine	O
some	O
diagnostic	O
plots	O
several	O
of	O
which	O
were	O
discussed	O
in	O
section	O
four	O
diagnostic	O
plots	O
are	O
automatically	O
produced	O
by	O
applying	O
the	O
plot	B
function	B
directly	O
to	O
the	O
output	B
from	O
lm	O
in	O
general	O
this	O
command	O
will	O
produce	O
one	O
plot	B
at	O
a	O
time	O
and	O
hitting	O
enter	O
will	O
generate	O
the	O
next	O
plot	B
however	O
it	O
is	O
often	O
convenient	O
to	O
view	O
all	O
four	O
plots	O
together	O
we	O
can	O
achieve	O
this	O
by	O
using	O
the	O
par	O
function	B
which	O
tells	O
r	O
to	O
split	O
the	O
display	O
screen	O
into	O
separate	O
panels	O
so	O
that	O
multiple	B
plots	O
can	O
be	O
viewed	O
simultaneously	O
for	O
example	O
divides	O
the	O
plotting	O
region	O
into	O
a	O
grid	O
of	O
panels	O
par	O
par	O
mfrow	O
c	O
plot	B
lm	O
fit	O
alternatively	O
we	O
can	O
compute	O
the	O
residuals	B
from	O
a	O
linear	B
regression	B
fit	O
using	O
the	O
residuals	B
function	B
the	O
function	B
rstudent	O
will	O
return	O
the	O
studentized	B
residuals	B
and	O
we	O
can	O
use	O
this	O
function	B
to	O
plot	B
the	O
residuals	B
against	O
the	O
fitted	O
values	O
residuals	B
rstudent	O
lab	O
linear	B
regression	B
plot	B
predict	O
lm	O
fit	O
residuals	B
lm	O
fit	O
plot	B
predict	O
lm	O
fit	O
rstudent	O
lm	O
fit	O
on	O
the	O
basis	B
of	O
the	O
residual	B
plots	O
there	O
is	O
some	O
evidence	O
of	O
non-linearity	O
leverage	B
statistics	O
can	O
be	O
computed	O
for	O
any	O
number	O
of	O
predictors	O
using	O
the	O
hatvalues	O
function	B
hatvalues	O
plot	B
hatvalues	O
lm	O
fit	O
which	O
max	O
hatvalues	O
lm	O
fit	O
the	O
which	O
max	O
function	B
identifies	O
the	O
index	O
of	O
the	O
largest	O
element	O
of	O
a	O
vector	B
in	O
this	O
case	O
it	O
tells	O
us	O
which	O
observation	O
has	O
the	O
largest	O
leverage	B
statistic	O
which	O
max	O
multiple	B
linear	B
regression	B
in	O
order	O
to	O
fit	O
a	O
multiple	B
linear	B
regression	B
model	B
using	O
least	B
squares	I
we	O
again	O
use	O
the	O
lm	O
function	B
the	O
syntax	O
lmy	O
is	O
used	O
to	O
fit	O
a	O
model	B
with	O
three	O
predictors	O
and	O
the	O
summary	O
function	B
now	O
outputs	O
the	O
regression	B
coefficients	O
for	O
all	O
the	O
predictors	O
lm	O
fit	O
lm	O
medv	O
lstat	O
age	O
data	B
boston	B
summary	O
lm	O
fit	O
call	O
lm	O
formula	O
medv	O
lstat	O
age	O
data	B
boston	B
residuals	B
min	O
q	O
median	O
q	O
max	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
intercept	B
lstat	O
age	O
codes	O
residual	B
standard	B
error	B
on	O
degrees	B
of	I
freedom	I
multiple	B
r	O
squared	O
f	O
statistic	O
on	O
and	O
df	O
adjusted	O
r	O
squared	O
p	O
value	O
the	O
boston	B
data	B
set	B
contains	O
variables	O
and	O
so	O
it	O
would	O
be	O
cumbersome	O
to	O
have	O
to	O
type	O
all	O
of	O
these	O
in	O
order	O
to	O
perform	O
a	O
regression	B
using	O
all	O
of	O
the	O
predictors	O
instead	O
we	O
can	O
use	O
the	O
following	O
short-hand	O
lm	O
fit	O
lm	O
medv	O
data	B
boston	B
summary	O
lm	O
fit	O
call	O
lm	O
formula	O
medv	O
data	B
boston	B
linear	B
regression	B
residuals	B
min	O
q	O
median	O
q	O
max	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
intercept	B
crim	O
zn	O
indus	O
chas	O
nox	O
rm	O
age	O
dis	O
rad	O
tax	O
ptratio	O
black	O
lstat	O
codes	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
residual	B
standard	B
error	B
on	O
degrees	B
of	I
freedom	I
multiple	B
r	O
squared	O
f	O
statistic	O
on	O
and	O
df	O
adjusted	O
r	O
squared	O
p	O
value	O
e	O
we	O
can	O
access	O
the	O
individual	O
components	O
of	O
a	O
summary	O
object	O
by	O
name	O
to	O
see	O
what	O
is	O
available	O
hence	O
summarylm	O
fitr	O
sq	O
gives	O
us	O
the	O
and	O
summarylm	O
fitsigma	O
gives	O
us	O
the	O
rse	O
the	O
vif	O
function	B
part	O
of	O
the	O
car	O
package	O
can	O
be	O
used	O
to	O
compute	O
variance	B
inflation	O
factors	O
most	O
vif	O
s	O
are	O
low	O
to	O
moderate	O
for	O
this	O
data	B
the	O
car	O
package	O
is	O
not	O
part	O
of	O
the	O
base	O
r	O
installation	O
so	O
it	O
must	O
be	O
downloaded	O
the	O
first	O
time	O
you	O
use	O
it	O
via	O
the	O
install	O
packages	O
option	O
in	O
r	O
vif	O
library	O
car	O
vif	O
lm	O
fit	O
crim	O
dis	O
zn	O
rad	O
indus	O
chas	O
tax	O
ptratio	O
nox	O
black	O
rm	O
lstat	O
age	O
what	O
if	O
we	O
would	O
like	O
to	O
perform	O
a	O
regression	B
using	O
all	O
of	O
the	O
variables	O
but	O
one	O
for	O
example	O
in	O
the	O
above	O
regression	B
output	B
age	O
has	O
a	O
high	O
p-value	B
so	O
we	O
may	O
wish	O
to	O
run	O
a	O
regression	B
excluding	O
this	O
predictor	B
the	O
following	O
syntax	O
results	O
in	O
a	O
regression	B
using	O
all	O
predictors	O
except	O
age	O
lm	O
lm	O
medv	O
age	O
data	B
boston	B
summary	O
lm	O
alternatively	O
the	O
update	O
function	B
can	O
be	O
used	O
update	O
lab	O
linear	B
regression	B
lm	O
update	O
lm	O
fit	O
age	O
interaction	B
terms	O
it	O
is	O
easy	O
to	O
include	O
interaction	B
terms	O
in	O
a	O
linear	B
model	B
using	O
the	O
lm	O
function	B
the	O
syntax	O
lstatblack	O
tells	O
r	O
to	O
include	O
an	O
interaction	B
term	B
between	O
lstat	O
and	O
black	O
the	O
syntax	O
lstatage	O
simultaneously	O
includes	O
lstat	O
age	O
and	O
the	O
interaction	B
term	B
lstat	O
age	O
as	O
predictors	O
it	O
is	O
a	O
shorthand	O
for	O
lstatagelstatage	O
summary	O
lm	O
medv	O
lstat	O
age	O
data	B
boston	B
call	O
lm	O
formula	O
medv	O
lstat	O
age	O
data	B
boston	B
residuals	B
min	O
q	O
median	O
q	O
max	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
intercept	B
lstat	O
age	O
lstat	O
age	O
codes	O
e	O
residual	B
standard	B
error	B
on	O
degrees	B
of	I
freedom	I
multiple	B
r	O
squared	O
f	O
statistic	O
adjusted	O
r	O
squared	O
on	O
and	O
df	O
p	O
value	O
non-linear	B
transformations	O
of	O
the	O
predictors	O
the	O
lm	O
function	B
can	O
also	O
accommodate	O
non-linear	B
transformations	O
of	O
the	O
predictors	O
for	O
instance	O
given	O
a	O
predictor	B
x	O
we	O
can	O
create	O
a	O
predictor	B
x	O
using	O
the	O
function	B
i	O
is	O
needed	O
since	O
the	O
has	O
a	O
special	O
meaning	O
in	O
a	O
formula	O
wrapping	O
as	O
we	O
do	O
allows	O
the	O
standard	O
usage	O
in	O
r	O
which	O
is	O
to	O
raise	O
x	O
to	O
the	O
power	B
we	O
now	O
perform	O
a	O
regression	B
of	O
medv	O
onto	O
lstat	O
and	O
lstat	O
lm	O
lm	O
medv	O
lstat	O
i	O
lstat	O
i	O
summary	O
lm	O
call	O
lm	O
formula	O
medv	O
lstat	O
i	O
lstat	O
residuals	B
min	O
q	O
median	O
q	O
max	O
linear	B
regression	B
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
intercept	B
lstat	O
i	O
lstat	O
codes	O
residual	B
standard	B
error	B
on	O
degrees	B
of	I
freedom	I
multiple	B
r	O
squared	O
f	O
statistic	O
adjusted	O
r	O
squared	O
on	O
and	O
df	O
p	O
value	O
the	O
near-zero	O
p-value	B
associated	O
with	O
the	O
quadratic	B
term	B
suggests	O
that	O
it	O
leads	O
to	O
an	O
improved	O
model	B
we	O
use	O
the	O
anova	O
function	B
to	O
further	O
quantify	O
the	O
extent	O
to	O
which	O
the	O
quadratic	B
fit	O
is	O
superior	O
to	O
the	O
linear	B
fit	O
lm	O
fit	O
lm	O
medv	O
lstat	O
anova	O
anova	O
lm	O
fit	O
lm	O
analysis	B
of	I
variance	B
table	O
model	B
medv	O
lstat	O
model	B
medv	O
lstat	O
i	O
lstat	O
res	O
df	O
rss	O
df	O
sum	O
of	O
sq	O
f	O
pr	O
f	O
codes	O
here	O
model	B
represents	O
the	O
linear	B
submodel	O
containing	O
only	O
one	O
predictor	B
lstat	O
while	O
model	B
corresponds	O
to	O
the	O
larger	O
quadratic	B
model	B
that	O
has	O
two	O
the	O
anova	O
function	B
performs	O
a	O
hypothesis	B
predictors	O
lstat	O
and	O
lstat	O
test	O
comparing	O
the	O
two	O
models	O
the	O
null	B
hypothesis	B
is	O
that	O
the	O
two	O
models	O
fit	O
the	O
data	B
equally	O
well	O
and	O
the	O
alternative	B
hypothesis	B
is	O
that	O
the	O
full	O
model	B
is	O
superior	O
here	O
the	O
f-statistic	B
is	O
and	O
the	O
associated	O
p-value	B
is	O
virtually	O
zero	O
this	O
provides	O
very	O
clear	O
evidence	O
that	O
the	O
model	B
containing	O
is	O
far	O
superior	O
to	O
the	O
model	B
that	O
only	O
the	O
predictors	O
lstat	O
and	O
lstat	O
contains	O
the	O
predictor	B
lstat	O
this	O
is	O
not	O
surprising	O
since	O
earlier	O
we	O
saw	O
evidence	O
for	O
non-linearity	O
in	O
the	O
relationship	O
between	O
medv	O
and	O
lstat	O
if	O
we	O
type	O
par	O
mfrow	O
c	O
plot	B
lm	O
then	O
we	O
see	O
that	O
when	O
the	O
lstat	O
little	O
discernible	O
pattern	O
in	O
the	O
residuals	B
term	B
is	O
included	O
in	O
the	O
model	B
there	O
is	O
in	O
order	O
to	O
create	O
a	O
cubic	B
fit	O
we	O
can	O
include	O
a	O
predictor	B
of	O
the	O
form	O
however	O
this	O
approach	B
can	O
start	O
to	O
get	O
cumbersome	O
for	O
higherorder	O
polynomials	O
a	O
better	O
approach	B
involves	O
using	O
the	O
poly	O
function	B
to	O
create	O
the	O
polynomial	B
within	O
lm	O
for	O
example	O
the	O
following	O
command	O
produces	O
a	O
fifth-order	O
polynomial	B
fit	O
poly	O
lab	O
linear	B
regression	B
lm	O
lm	O
medv	O
poly	O
lstat	O
summary	O
lm	O
call	O
lm	O
formula	O
medv	O
poly	O
lstat	O
residuals	B
min	O
q	O
median	O
q	O
max	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
intercept	B
poly	O
lstat	O
poly	O
lstat	O
poly	O
lstat	O
poly	O
lstat	O
poly	O
lstat	O
codes	O
e	O
e	O
residual	B
standard	B
error	B
on	O
degrees	B
of	I
freedom	I
multiple	B
r	O
squared	O
f	O
statistic	O
adjusted	O
r	O
squared	O
on	O
and	O
df	O
p	O
value	O
this	O
suggests	O
that	O
including	O
additional	O
polynomial	B
terms	O
up	O
to	O
fifth	O
order	O
leads	O
to	O
an	O
improvement	O
in	O
the	O
model	B
fit	O
however	O
further	O
investigation	O
of	O
the	O
data	B
reveals	O
that	O
no	O
polynomial	B
terms	O
beyond	O
fifth	O
order	O
have	O
significant	O
p-values	O
in	O
a	O
regression	B
fit	O
of	O
course	O
we	O
are	O
in	O
no	O
way	O
restricted	O
to	O
using	O
polynomial	B
transforma	O
tions	O
of	O
the	O
predictors	O
here	O
we	O
try	O
a	O
log	O
transformation	O
summary	O
lm	O
medv	O
log	O
rm	O
data	B
boston	B
qualitative	B
predictors	O
we	O
will	O
now	O
examine	O
the	O
carseats	B
data	B
which	O
is	O
part	O
of	O
the	O
islr	O
library	O
we	O
will	O
attempt	O
to	O
predict	O
sales	O
car	O
seat	O
sales	O
in	O
locations	O
based	O
on	O
a	O
number	O
of	O
predictors	O
fix	O
carseats	B
names	O
carseats	B
sales	O
population	O
education	O
compprice	O
price	O
urban	O
income	B
shelveloc	O
us	O
advertisi	O
n	O
g	O
age	O
the	O
carseats	B
data	B
includes	O
qualitative	B
predictors	O
such	O
as	O
shelveloc	O
an	O
indicator	B
of	O
the	O
quality	O
of	O
the	O
shelving	O
location	O
that	O
is	O
the	O
space	O
within	O
a	O
store	O
in	O
which	O
the	O
car	O
seat	O
is	O
displayed	O
at	O
each	O
location	O
the	O
predictor	B
shelveloc	O
takes	O
on	O
three	O
possible	O
values	O
bad	O
medium	O
and	O
good	O
linear	B
regression	B
given	O
a	O
qualitative	B
variable	B
such	O
as	O
shelveloc	O
r	O
generates	O
dummy	B
variables	O
automatically	O
below	O
we	O
fit	O
a	O
multiple	B
regression	B
model	B
that	O
includes	O
some	O
interaction	B
terms	O
lm	O
fit	O
lm	O
sales	O
income	B
advertisi	O
ng	O
price	O
age	O
data	B
carseats	B
summary	O
lm	O
fit	O
call	O
lm	O
formula	O
sales	O
income	B
advertisin	O
g	O
price	O
age	O
data	B
carseats	B
residuals	B
min	O
q	O
median	O
q	O
max	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
intercept	B
compprice	O
income	B
advertis	O
i	O
ng	O
populatio	O
n	O
price	O
s	O
h	O
e	O
l	O
v	O
e	O
l	O
o	O
c	O
g	O
o	O
o	O
d	O
s	O
h	O
e	O
l	O
v	O
e	O
l	O
o	O
c	O
m	O
e	O
d	O
i	O
u	O
m	O
age	O
education	O
urbanyes	O
usyes	O
income	B
advertisi	O
n	O
g	O
price	O
age	O
codes	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
e	O
e	O
residual	B
standard	B
error	B
on	O
degrees	B
of	I
freedom	I
multiple	B
r	O
squared	O
f	O
statistic	O
on	O
and	O
df	O
adjusted	O
r	O
squared	O
p	O
value	O
the	O
contrasts	O
function	B
returns	O
the	O
coding	O
that	O
r	O
uses	O
for	O
the	O
dummy	B
variables	O
contrasts	O
attach	O
carseats	B
contrasts	O
shelveloc	O
bad	O
good	O
medium	O
good	O
medium	O
use	O
to	O
learn	O
about	O
other	O
contrasts	O
and	O
how	O
to	O
set	B
them	O
r	O
has	O
created	O
a	O
shelvelocgood	O
dummy	B
variable	B
that	O
takes	O
on	O
a	O
value	O
of	O
if	O
the	O
shelving	O
location	O
is	O
good	O
and	O
otherwise	O
it	O
has	O
also	O
created	O
a	O
shelvelocmedium	O
dummy	B
variable	B
that	O
equals	O
if	O
the	O
shelving	O
location	O
is	O
medium	O
and	O
otherwise	O
a	O
bad	O
shelving	O
location	O
corresponds	O
to	O
a	O
zero	O
for	O
each	O
of	O
the	O
two	O
dummy	B
variables	O
the	O
fact	O
that	O
the	O
coefficient	O
for	O
lab	O
linear	B
regression	B
shelvelocgood	O
in	O
the	O
regression	B
output	B
is	O
positive	O
indicates	O
that	O
a	O
good	O
shelving	O
location	O
is	O
associated	O
with	O
high	O
sales	O
to	O
a	O
bad	O
location	O
and	O
shelvelocmedium	O
has	O
a	O
smaller	O
positive	O
coefficient	O
indicating	O
that	O
a	O
medium	O
shelving	O
location	O
leads	O
to	O
higher	O
sales	O
than	O
a	O
bad	O
shelving	O
location	O
but	O
lower	O
sales	O
than	O
a	O
good	O
shelving	O
location	O
writing	O
functions	O
as	O
we	O
have	O
seen	O
r	O
comes	O
with	O
many	O
useful	O
functions	O
and	O
still	O
more	O
functions	O
are	O
available	O
by	O
way	O
of	O
r	O
libraries	O
however	O
we	O
will	O
often	O
be	O
interested	O
in	O
performing	O
an	O
operation	O
for	O
which	O
no	O
function	B
is	O
available	O
in	O
this	O
setting	O
we	O
may	O
want	O
to	O
write	O
our	O
own	O
function	B
for	O
instance	O
below	O
we	O
provide	O
a	O
simple	B
function	B
that	O
reads	O
in	O
the	O
islr	O
and	O
mass	O
libraries	O
called	O
loadlibraries	O
before	O
we	O
have	O
created	O
the	O
function	B
r	O
returns	O
an	O
error	B
if	O
we	O
try	O
to	O
call	O
it	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
error	B
object	O
loadlibraries	O
not	O
found	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
error	B
could	O
not	O
find	O
function	B
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
we	O
now	O
create	O
the	O
function	B
note	O
that	O
the	O
symbols	O
are	O
printed	O
by	O
r	O
and	O
should	O
not	O
be	O
typed	O
in	O
the	O
symbol	O
informs	O
r	O
that	O
multiple	B
commands	O
are	O
about	O
to	O
be	O
input	B
hitting	O
enter	O
after	O
typing	O
will	O
cause	O
r	O
to	O
print	O
the	O
symbol	O
we	O
can	O
then	O
input	B
as	O
many	O
commands	O
as	O
we	O
wish	O
hitting	O
enter	O
after	O
each	O
one	O
finally	O
the	O
symbol	O
informs	O
r	O
that	O
no	O
further	O
commands	O
will	O
be	O
entered	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
function	B
library	O
islr	O
library	O
mass	O
print	O
the	O
libraries	O
have	O
been	O
loaded	O
now	O
if	O
we	O
type	O
in	O
loadlibraries	O
r	O
will	O
tell	O
us	O
what	O
is	O
in	O
the	O
function	B
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
function	B
library	O
islr	O
library	O
mass	O
print	O
the	O
libraries	O
have	O
been	O
loaded	O
if	O
we	O
call	O
the	O
function	B
the	O
libraries	O
are	O
loaded	O
in	O
and	O
the	O
print	O
statement	O
is	O
output	B
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
the	O
libraries	O
have	O
been	O
loaded	O
linear	B
regression	B
exercises	O
conceptual	O
describe	O
the	O
null	B
hypotheses	O
to	O
which	O
the	O
p-values	O
given	O
in	O
table	O
correspond	O
explain	O
what	O
conclusions	O
you	O
can	O
draw	O
based	O
on	O
these	O
p-values	O
your	O
explanation	O
should	O
be	O
phrased	O
in	O
terms	O
of	O
sales	O
tv	O
radio	O
and	O
newspaper	O
rather	O
than	O
in	O
terms	O
of	O
the	O
coefficients	O
of	O
the	O
linear	B
model	B
carefully	O
explain	O
the	O
differences	O
between	O
the	O
knn	O
classifier	B
and	O
knn	O
regression	B
methods	O
suppose	O
we	O
have	O
a	O
data	B
set	B
with	O
five	O
predictors	O
gpa	O
iq	O
gender	O
for	O
female	O
and	O
for	O
male	O
interaction	B
between	O
gpa	O
and	O
iq	O
and	O
interaction	B
between	O
gpa	O
and	O
gender	O
the	O
response	B
is	O
starting	O
salary	O
after	O
graduation	O
thousands	O
of	O
dollars	O
suppose	O
we	O
use	O
least	B
squares	I
to	O
fit	O
the	O
model	B
and	O
get	O
which	O
answer	O
is	O
correct	O
and	O
why	O
i	O
for	O
a	O
fixed	O
value	O
of	O
iq	O
and	O
gpa	O
males	O
earn	O
more	O
on	O
average	B
than	O
females	O
ii	O
for	O
a	O
fixed	O
value	O
of	O
iq	O
and	O
gpa	O
females	O
earn	O
more	O
on	O
average	B
than	O
males	O
iii	O
for	O
a	O
fixed	O
value	O
of	O
iq	O
and	O
gpa	O
males	O
earn	O
more	O
on	O
average	B
than	O
females	O
provided	O
that	O
the	O
gpa	O
is	O
high	O
enough	O
iv	O
for	O
a	O
fixed	O
value	O
of	O
iq	O
and	O
gpa	O
females	O
earn	O
more	O
on	O
average	B
than	O
males	O
provided	O
that	O
the	O
gpa	O
is	O
high	O
enough	O
predict	O
the	O
salary	O
of	O
a	O
female	O
with	O
iq	O
of	O
and	O
a	O
gpa	O
of	O
true	O
or	O
false	O
since	O
the	O
coefficient	O
for	O
the	O
gpaiq	O
interaction	B
term	B
is	O
very	O
small	O
there	O
is	O
very	O
little	O
evidence	O
of	O
an	O
interaction	B
effect	O
justify	O
your	O
answer	O
i	O
collect	O
a	O
set	B
of	O
data	B
observations	B
containing	O
a	O
single	B
predictor	B
and	O
a	O
quantitative	B
response	B
i	O
then	O
fit	O
a	O
linear	B
regression	B
model	B
to	O
the	O
data	B
as	O
well	O
as	O
a	O
separate	O
cubic	B
regression	B
i	O
e	O
y	O
suppose	O
that	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
is	O
linear	B
i	O
e	O
y	O
consider	O
the	O
training	O
residual	B
sum	B
of	I
squares	I
for	O
the	O
linear	B
regression	B
and	O
also	O
the	O
training	O
rss	O
for	O
the	O
cubic	B
regression	B
would	O
we	O
expect	O
one	O
to	O
be	O
lower	O
than	O
the	O
other	O
would	O
we	O
expect	O
them	O
to	O
be	O
the	O
same	O
or	O
is	O
there	O
not	O
enough	O
information	O
to	O
tell	O
justify	O
your	O
answer	O
exercises	O
answer	O
using	O
test	O
rather	O
than	O
training	O
rss	O
suppose	O
that	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
is	O
not	O
linear	B
but	O
we	O
don	O
t	O
know	O
how	O
far	O
it	O
is	O
from	O
linear	B
consider	O
the	O
training	O
rss	O
for	O
the	O
linear	B
regression	B
and	O
also	O
the	O
training	O
rss	O
for	O
the	O
cubic	B
regression	B
would	O
we	O
expect	O
one	O
to	O
be	O
lower	O
than	O
the	O
other	O
would	O
we	O
expect	O
them	O
to	O
be	O
the	O
same	O
or	O
is	O
there	O
not	O
enough	O
information	O
to	O
tell	O
justify	O
your	O
answer	O
answer	O
using	O
test	O
rather	O
than	O
training	O
rss	O
consider	O
the	O
fitted	O
values	O
that	O
result	O
from	O
performing	O
linear	B
regression	B
without	O
an	O
intercept	B
in	O
this	O
setting	O
the	O
ith	O
fitted	O
value	O
takes	O
the	O
form	O
where	O
yi	O
xi	O
xiyi	O
show	O
that	O
we	O
can	O
write	O
yi	O
what	O
is	O
note	O
we	O
interpret	O
this	O
result	O
by	O
saying	O
that	O
the	O
fitted	O
values	O
from	O
linear	B
regression	B
are	O
linear	B
combinations	O
of	O
the	O
response	B
values	O
using	O
argue	O
that	O
in	O
the	O
case	O
of	O
simple	B
linear	B
regression	B
the	O
least	B
squares	I
line	B
always	O
passes	O
through	O
the	O
point	O
x	O
y	O
it	O
is	O
claimed	O
in	O
the	O
text	O
that	O
in	O
the	O
case	O
of	O
simple	B
linear	B
regression	B
of	O
y	O
onto	O
x	O
the	O
statistic	O
is	O
equal	O
to	O
the	O
square	O
of	O
the	O
correlation	B
between	O
x	O
and	O
y	O
prove	O
that	O
this	O
is	O
the	O
case	O
for	O
simplicity	O
you	O
may	O
assume	O
that	O
x	O
y	O
applied	O
this	O
question	O
involves	O
the	O
use	O
of	O
simple	B
linear	B
regression	B
on	O
the	O
auto	B
data	B
set	B
use	O
the	O
lm	O
function	B
to	O
perform	O
a	O
simple	B
linear	B
regression	B
with	O
mpg	O
as	O
the	O
response	B
and	O
horsepower	O
as	O
the	O
predictor	B
use	O
the	O
summary	O
function	B
to	O
print	O
the	O
results	O
comment	O
on	O
the	O
output	B
for	O
example	O
linear	B
regression	B
i	O
is	O
there	O
a	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
re	O
sponse	O
ii	O
how	O
strong	O
is	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
iii	O
is	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
positive	O
or	O
negative	O
iv	O
what	O
is	O
the	O
predicted	O
mpg	O
associated	O
with	O
a	O
horsepower	O
of	O
what	O
are	O
the	O
associated	O
confidence	O
and	O
prediction	B
intervals	O
plot	B
the	O
response	B
and	O
the	O
predictor	B
use	O
the	O
abline	O
function	B
to	O
display	O
the	O
least	B
squares	I
regression	B
line	B
use	O
the	O
plot	B
function	B
to	O
produce	O
diagnostic	O
plots	O
of	O
the	O
least	B
squares	I
regression	B
fit	O
comment	O
on	O
any	O
problems	O
you	O
see	O
with	O
the	O
fit	O
this	O
question	O
involves	O
the	O
use	O
of	O
multiple	B
linear	B
regression	B
on	O
the	O
auto	B
data	B
set	B
produce	O
a	O
scatterplot	B
matrix	I
which	O
includes	O
all	O
of	O
the	O
variables	O
in	O
the	O
data	B
set	B
compute	O
the	O
matrix	O
of	O
correlations	O
between	O
the	O
variables	O
using	O
the	O
function	B
cor	O
you	O
will	O
need	O
to	O
exclude	O
the	O
name	O
variable	B
which	O
is	O
qualitative	B
cor	O
use	O
the	O
lm	O
function	B
to	O
perform	O
a	O
multiple	B
linear	B
regression	B
with	O
mpg	O
as	O
the	O
response	B
and	O
all	O
other	O
variables	O
except	O
name	O
as	O
the	O
predictors	O
use	O
the	O
summary	O
function	B
to	O
print	O
the	O
results	O
comment	O
on	O
the	O
output	B
for	O
instance	O
i	O
is	O
there	O
a	O
relationship	O
between	O
the	O
predictors	O
and	O
the	O
re	O
sponse	O
ii	O
which	O
predictors	O
appear	O
to	O
have	O
a	O
statistically	O
significant	O
relationship	O
to	O
the	O
response	B
iii	O
what	O
does	O
the	O
coefficient	O
for	O
the	O
year	O
variable	B
suggest	O
use	O
the	O
plot	B
function	B
to	O
produce	O
diagnostic	O
plots	O
of	O
the	O
linear	B
regression	B
fit	O
comment	O
on	O
any	O
problems	O
you	O
see	O
with	O
the	O
fit	O
do	O
the	O
residual	B
plots	O
suggest	O
any	O
unusually	O
large	O
outliers	O
does	O
the	O
leverage	B
plot	B
identify	O
any	O
observations	B
with	O
unusually	O
high	O
leverage	B
use	O
the	O
and	O
symbols	O
to	O
fit	O
linear	B
regression	B
models	O
with	O
interaction	B
effects	O
do	O
any	O
interactions	O
appear	O
to	O
be	O
statistically	O
significant	O
x	O
x	O
comment	O
on	O
your	O
findings	O
try	O
a	O
few	O
different	O
transformations	O
of	O
the	O
variables	O
such	O
as	O
logx	O
this	O
question	O
should	O
be	O
answered	O
using	O
the	O
carseats	B
data	B
set	B
exercises	O
fit	O
a	O
multiple	B
regression	B
model	B
to	O
predict	O
sales	O
using	O
price	O
urban	O
and	O
us	O
provide	O
an	O
interpretation	O
of	O
each	O
coefficient	O
in	O
the	O
model	B
be	O
careful	O
some	O
of	O
the	O
variables	O
in	O
the	O
model	B
are	O
qualitative	B
write	O
out	O
the	O
model	B
in	O
equation	O
form	O
being	O
careful	O
to	O
handle	O
the	O
qualitative	B
variables	O
properly	O
for	O
which	O
of	O
the	O
predictors	O
can	O
you	O
reject	O
the	O
null	B
hypothesis	B
j	O
on	O
the	O
basis	B
of	O
your	O
response	B
to	O
the	O
previous	O
question	O
fit	O
a	O
smaller	O
model	B
that	O
only	O
uses	O
the	O
predictors	O
for	O
which	O
there	O
is	O
evidence	O
of	O
association	O
with	O
the	O
outcome	O
how	O
well	O
do	O
the	O
models	O
in	O
and	O
fit	O
the	O
data	B
using	O
the	O
model	B
from	O
obtain	O
confidence	O
intervals	O
for	O
the	O
coefficients	O
is	O
there	O
evidence	O
of	O
outliers	O
or	O
high	O
leverage	B
observations	B
in	O
the	O
model	B
from	O
in	O
this	O
problem	O
we	O
will	O
investigate	O
the	O
t-statistic	B
for	O
the	O
null	B
hypothesis	B
in	O
simple	B
linear	B
regression	B
without	O
an	O
intercept	B
to	O
begin	O
we	O
generate	O
a	O
predictor	B
x	O
and	O
a	O
response	B
y	O
as	O
follows	O
set	B
seed	B
x	O
rnorm	O
y	O
x	O
rnorm	O
perform	O
a	O
simple	B
linear	B
regression	B
of	O
y	O
onto	O
x	O
without	O
an	O
intercept	B
report	O
the	O
coefficient	O
estimate	O
the	O
standard	B
error	B
of	O
this	O
coefficient	O
estimate	O
and	O
the	O
t-statistic	B
and	O
p-value	B
associated	O
with	O
the	O
null	B
hypothesis	B
comment	O
on	O
these	O
results	O
can	O
perform	O
regression	B
without	O
an	O
intercept	B
using	O
the	O
command	O
lmy	O
now	O
perform	O
a	O
simple	B
linear	B
regression	B
of	O
x	O
onto	O
y	O
without	O
an	O
intercept	B
and	O
report	O
the	O
coefficient	O
estimate	O
its	O
standard	B
error	B
and	O
the	O
corresponding	O
t-statistic	B
and	O
p-values	O
associated	O
with	O
the	O
null	B
hypothesis	B
comment	O
on	O
these	O
results	O
what	O
is	O
the	O
relationship	O
between	O
the	O
results	O
obtained	O
in	O
and	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
without	O
an	O
intercept	B
the	O
tstatistic	O
for	O
takes	O
the	O
form	O
where	O
is	O
given	O
by	O
and	O
where	O
n	O
se	O
xi	O
n	O
linear	B
regression	B
formulas	O
are	O
slightly	O
different	O
from	O
those	O
given	O
in	O
sections	O
and	O
since	O
here	O
we	O
are	O
performing	O
regression	B
without	O
an	O
intercept	B
show	O
algebraically	O
and	O
confirm	O
numerically	O
in	O
r	O
that	O
the	O
t-statistic	B
can	O
be	O
written	O
as	O
n	O
n	O
n	O
i	O
n	O
xiyi	O
n	O
using	O
the	O
results	O
from	O
argue	O
that	O
the	O
t-statistic	B
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
is	O
the	O
same	O
as	O
the	O
t-statistic	B
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
in	O
r	O
show	O
that	O
when	O
regression	B
is	O
performed	O
with	O
an	O
intercept	B
the	O
t-statistic	B
for	O
is	O
the	O
same	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
as	O
it	O
is	O
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
this	O
problem	O
involves	O
simple	B
linear	B
regression	B
without	O
an	O
intercept	B
recall	B
that	O
the	O
coefficient	O
estimate	O
for	O
the	O
linear	B
regression	B
of	O
y	O
onto	O
x	O
without	O
an	O
intercept	B
is	O
given	O
by	O
under	O
what	O
circumstance	O
is	O
the	O
coefficient	O
estimate	O
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
the	O
same	O
as	O
the	O
coefficient	O
estimate	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
generate	O
an	O
example	O
in	O
r	O
with	O
n	O
observations	B
in	O
which	O
the	O
coefficient	O
estimate	O
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
is	O
different	O
from	O
the	O
coefficient	O
estimate	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
generate	O
an	O
example	O
in	O
r	O
with	O
n	O
observations	B
in	O
which	O
the	O
coefficient	O
estimate	O
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
is	O
the	O
same	O
as	O
the	O
coefficient	O
estimate	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
in	O
this	O
exercise	O
you	O
will	O
create	O
some	O
simulated	O
data	B
and	O
will	O
fit	O
simple	B
linear	B
regression	B
models	O
to	O
it	O
make	O
sure	O
to	O
use	O
prior	O
to	O
starting	O
part	O
to	O
ensure	O
consistent	O
results	O
using	O
the	O
rnorm	O
function	B
create	O
a	O
vector	B
x	O
containing	O
observations	B
drawn	O
from	O
a	O
n	O
distribution	B
this	O
represents	O
a	O
feature	B
x	O
using	O
the	O
rnorm	O
function	B
create	O
a	O
vector	B
eps	O
containing	O
observations	B
drawn	O
from	O
a	O
n	O
distribution	B
i	O
e	O
a	O
normal	O
distribution	B
with	O
mean	O
zero	O
and	O
variance	B
using	O
x	O
and	O
eps	O
generate	O
a	O
vector	B
y	O
according	O
to	O
the	O
model	B
y	O
what	O
is	O
the	O
length	O
of	O
the	O
vector	B
y	O
what	O
are	O
the	O
values	O
of	O
and	O
in	O
this	O
linear	B
model	B
exercises	O
create	O
a	O
scatterplot	B
displaying	O
the	O
relationship	O
between	O
x	O
and	O
y	O
comment	O
on	O
what	O
you	O
observe	O
fit	O
a	O
least	B
squares	I
linear	B
model	B
to	O
predict	O
y	O
using	O
x	O
comment	O
on	O
the	O
model	B
obtained	O
how	O
do	O
and	O
compare	O
to	O
and	O
display	O
the	O
least	B
squares	I
line	B
on	O
the	O
scatterplot	B
obtained	O
in	O
draw	O
the	O
population	B
regression	B
line	B
on	O
the	O
plot	B
in	O
a	O
different	O
color	O
use	O
the	O
legend	O
command	O
to	O
create	O
an	O
appropriate	O
legend	O
now	O
fit	O
a	O
polynomial	B
regression	B
model	B
that	O
predicts	O
y	O
using	O
x	O
and	O
is	O
there	O
evidence	O
that	O
the	O
quadratic	B
term	B
improves	O
the	O
model	B
fit	O
explain	O
your	O
answer	O
repeat	O
after	O
modifying	O
the	O
data	B
generation	O
process	O
in	O
such	O
a	O
way	O
that	O
there	O
is	O
less	O
noise	B
in	O
the	O
data	B
the	O
model	B
should	O
remain	O
the	O
same	O
you	O
can	O
do	O
this	O
by	O
decreasing	O
the	O
variance	B
of	O
the	O
normal	O
distribution	B
used	O
to	O
generate	O
the	O
error	B
term	B
in	O
describe	O
your	O
results	O
repeat	O
after	O
modifying	O
the	O
data	B
generation	O
process	O
in	O
such	O
a	O
way	O
that	O
there	O
is	O
more	O
noise	B
in	O
the	O
data	B
the	O
model	B
should	O
remain	O
the	O
same	O
you	O
can	O
do	O
this	O
by	O
increasing	O
the	O
variance	B
of	O
the	O
normal	O
distribution	B
used	O
to	O
generate	O
the	O
error	B
term	B
in	O
describe	O
your	O
results	O
what	O
are	O
the	O
confidence	O
intervals	O
for	O
and	O
based	O
on	O
the	O
original	O
data	B
set	B
the	O
noisier	O
data	B
set	B
and	O
the	O
less	O
noisy	O
data	B
set	B
comment	O
on	O
your	O
results	O
this	O
problem	O
focuses	O
on	O
the	O
collinearity	B
problem	O
perform	O
the	O
following	O
commands	O
in	O
r	O
set	B
seed	B
runif	O
rnorm	O
y	O
rnorm	O
the	O
last	O
line	B
corresponds	O
to	O
creating	O
a	O
linear	B
model	B
in	O
which	O
y	O
is	O
a	O
function	B
of	O
and	O
write	O
out	O
the	O
form	O
of	O
the	O
linear	B
model	B
what	O
are	O
the	O
regression	B
coefficients	O
what	O
is	O
the	O
correlation	B
between	O
and	O
create	O
a	O
scatterplot	B
displaying	O
the	O
relationship	O
between	O
the	O
variables	O
using	O
this	O
data	B
fit	O
a	O
least	B
squares	I
regression	B
to	O
predict	O
y	O
using	O
and	O
describe	O
the	O
results	O
obtained	O
what	O
are	O
and	O
how	O
do	O
these	O
relate	O
to	O
the	O
true	O
and	O
can	O
you	O
reject	O
the	O
null	B
hypothesis	B
how	O
about	O
the	O
null	B
hypothesis	B
linear	B
regression	B
now	O
fit	O
a	O
least	B
squares	I
regression	B
to	O
predict	O
y	O
using	O
only	O
comment	O
on	O
your	O
results	O
can	O
you	O
reject	O
the	O
null	B
hypothesis	B
now	O
fit	O
a	O
least	B
squares	I
regression	B
to	O
predict	O
y	O
using	O
only	O
comment	O
on	O
your	O
results	O
can	O
you	O
reject	O
the	O
null	B
hypothesis	B
do	O
the	O
results	O
obtained	O
in	O
contradict	O
each	O
other	O
explain	O
your	O
answer	O
now	O
suppose	O
we	O
obtain	O
one	O
additional	O
observation	O
which	O
was	O
unfortunately	O
mismeasured	O
c	O
c	O
y	O
c	O
re-fit	O
the	O
linear	B
models	O
from	O
to	O
using	O
this	O
new	O
data	B
what	O
effect	O
does	O
this	O
new	O
observation	O
have	O
on	O
the	O
each	O
of	O
the	O
models	O
in	O
each	O
model	B
is	O
this	O
observation	O
an	O
outlier	B
a	O
high-leverage	O
point	O
both	O
explain	O
your	O
answers	O
this	O
problem	O
involves	O
the	O
boston	B
data	B
set	B
which	O
we	O
saw	O
in	O
the	O
lab	O
for	O
this	O
chapter	O
we	O
will	O
now	O
try	O
to	O
predict	O
per	O
capita	O
crime	O
rate	B
using	O
the	O
other	O
variables	O
in	O
this	O
data	B
set	B
in	O
other	O
words	O
per	O
capita	O
crime	O
rate	B
is	O
the	O
response	B
and	O
the	O
other	O
variables	O
are	O
the	O
predictors	O
for	O
each	O
predictor	B
fit	O
a	O
simple	B
linear	B
regression	B
model	B
to	O
predict	O
the	O
response	B
describe	O
your	O
results	O
in	O
which	O
of	O
the	O
models	O
is	O
there	O
a	O
statistically	O
significant	O
association	O
between	O
the	O
predictor	B
and	O
the	O
response	B
create	O
some	O
plots	O
to	O
back	O
up	O
your	O
assertions	O
fit	O
a	O
multiple	B
regression	B
model	B
to	O
predict	O
the	O
response	B
using	O
all	O
of	O
the	O
predictors	O
describe	O
your	O
results	O
for	O
which	O
predictors	O
can	O
we	O
reject	O
the	O
null	B
hypothesis	B
j	O
how	O
do	O
your	O
results	O
from	O
compare	O
to	O
your	O
results	O
from	O
create	O
a	O
plot	B
displaying	O
the	O
univariate	O
regression	B
coefficients	O
from	O
on	O
the	O
x-axis	O
and	O
the	O
multiple	B
regression	B
coefficients	O
from	O
on	O
the	O
y-axis	O
that	O
is	O
each	O
predictor	B
is	O
displayed	O
as	O
a	O
single	B
point	O
in	O
the	O
plot	B
its	O
coefficient	O
in	O
a	O
simple	B
linear	B
regression	B
model	B
is	O
shown	O
on	O
the	O
x-axis	O
and	O
its	O
coefficient	O
estimate	O
in	O
the	O
multiple	B
linear	B
regression	B
model	B
is	O
shown	O
on	O
the	O
y-axis	O
is	O
there	O
evidence	O
of	O
non-linear	B
association	O
between	O
any	O
of	O
the	O
predictors	O
and	O
the	O
response	B
to	O
answer	O
this	O
question	O
for	O
each	O
predictor	B
x	O
fit	O
a	O
model	B
of	O
the	O
form	O
y	O
classification	B
the	O
linear	B
regression	B
model	B
discussed	O
in	O
chapter	O
assumes	O
that	O
the	O
response	B
variable	B
y	O
is	O
quantitative	B
but	O
in	O
many	O
situations	O
the	O
response	B
variable	B
is	O
instead	O
qualitative	B
for	O
example	O
eye	O
color	O
is	O
qualitative	B
taking	O
on	O
values	O
blue	O
brown	O
or	O
green	O
often	O
qualitative	B
variables	O
are	O
referred	O
to	O
as	O
categorical	B
we	O
will	O
use	O
these	O
terms	O
interchangeably	O
in	O
this	O
chapter	O
we	O
study	O
approaches	O
for	O
predicting	O
qualitative	B
responses	O
a	O
process	O
that	O
is	O
known	O
as	O
classification	B
predicting	O
a	O
qualitative	B
response	B
for	O
an	O
observation	O
can	O
be	O
referred	O
to	O
as	O
classifying	O
that	O
observation	O
since	O
it	O
involves	O
assigning	O
the	O
observation	O
to	O
a	O
category	O
or	O
class	O
on	O
the	O
other	O
hand	O
often	O
the	O
methods	O
used	O
for	O
classification	B
first	O
predict	O
the	O
probability	B
of	O
each	O
of	O
the	O
categories	O
of	O
a	O
qualitative	B
variable	B
as	O
the	O
basis	B
for	O
making	O
the	O
classification	B
in	O
this	O
sense	O
they	O
also	O
behave	O
like	O
regression	B
methods	O
there	O
are	O
many	O
possible	O
classification	B
techniques	O
or	O
classifiers	O
that	O
one	O
might	O
use	O
to	O
predict	O
a	O
qualitative	B
response	B
we	O
touched	O
on	O
some	O
of	O
these	O
in	O
sections	O
and	O
in	O
this	O
chapter	O
we	O
discuss	O
three	O
of	O
the	O
most	O
widely-used	O
classifiers	O
logistic	B
regression	B
linear	B
discriminant	I
analysis	B
and	O
k-nearest	O
neighbors	O
we	O
discuss	O
more	O
computer-intensive	O
methods	O
in	O
later	O
chapters	O
such	O
as	O
generalized	O
additive	B
models	O
trees	O
random	O
forests	O
and	O
boosting	B
and	O
support	B
vector	B
machines	O
qualitative	B
classification	B
classifier	B
logistic	B
regression	B
linear	B
discriminant	I
analysis	B
k-nearest	O
neighbors	O
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
classification	B
an	O
overview	O
of	O
classification	B
classification	B
problems	O
occur	O
often	O
perhaps	O
even	O
more	O
so	O
than	O
regression	B
problems	O
some	O
examples	O
include	O
a	O
person	O
arrives	O
at	O
the	O
emergency	O
room	O
with	O
a	O
set	B
of	O
symptoms	O
that	O
could	O
possibly	O
be	O
attributed	O
to	O
one	O
of	O
three	O
medical	O
conditions	O
which	O
of	O
the	O
three	O
conditions	O
does	O
the	O
individual	O
have	O
an	O
online	O
banking	O
service	O
must	O
be	O
able	O
to	O
determine	O
whether	O
or	O
not	O
a	O
transaction	O
being	O
performed	O
on	O
the	O
site	O
is	O
fraudulent	O
on	O
the	O
basis	B
of	O
the	O
user	O
s	O
ip	O
address	O
past	O
transaction	O
history	O
and	O
so	O
forth	O
on	O
the	O
basis	B
of	O
dna	O
sequence	O
data	B
for	O
a	O
number	O
of	O
patients	O
with	O
and	O
without	O
a	O
given	O
disease	O
a	O
biologist	O
would	O
like	O
to	O
figure	O
out	O
which	O
dna	O
mutations	O
are	O
deleterious	O
and	O
which	O
are	O
not	O
just	O
as	O
in	O
the	O
regression	B
setting	O
in	O
the	O
classification	B
setting	O
we	O
have	O
a	O
set	B
of	O
training	O
observations	B
yn	O
that	O
we	O
can	O
use	O
to	O
build	O
a	O
classifier	B
we	O
want	O
our	O
classifier	B
to	O
perform	O
well	O
not	O
only	O
on	O
the	O
training	O
data	B
but	O
also	O
on	O
test	O
observations	B
that	O
were	O
not	O
used	O
to	O
train	B
the	O
classifier	B
in	O
this	O
chapter	O
we	O
will	O
illustrate	O
the	O
concept	O
of	O
classification	B
using	O
the	O
simulated	O
default	B
data	B
set	B
we	O
are	O
interested	O
in	O
predicting	O
whether	O
an	O
individual	O
will	O
default	B
on	O
his	O
or	O
her	O
credit	B
card	O
payment	O
on	O
the	O
basis	B
of	O
annual	O
income	B
and	O
monthly	O
credit	B
card	O
balance	O
the	O
data	B
set	B
is	O
displayed	O
in	O
figure	O
we	O
have	O
plotted	O
annual	O
income	B
and	O
monthly	O
credit	B
card	O
balance	O
for	O
a	O
subset	O
of	O
individuals	O
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
individuals	O
who	O
defaulted	O
in	O
a	O
given	O
month	O
in	O
orange	O
and	O
those	O
who	O
did	O
not	O
in	O
blue	O
overall	O
default	B
rate	B
is	O
about	O
so	O
we	O
have	O
plotted	O
only	O
a	O
fraction	O
of	O
the	O
individuals	O
who	O
did	O
not	O
default	B
it	O
appears	O
that	O
individuals	O
who	O
defaulted	O
tended	O
to	O
have	O
higher	O
credit	B
card	O
balances	O
than	O
those	O
who	O
did	O
not	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
two	O
pairs	O
of	O
boxplots	O
are	O
shown	O
the	O
first	O
shows	O
the	O
distribution	B
of	O
balance	O
split	O
by	O
the	O
binary	B
default	B
variable	B
the	O
second	O
is	O
a	O
similar	O
plot	B
for	O
income	B
in	O
this	O
chapter	O
we	O
learn	O
how	O
to	O
build	O
a	O
model	B
to	O
predict	O
default	B
for	O
any	O
given	O
value	O
of	O
balance	O
and	O
income	B
since	O
y	O
is	O
not	O
quantitative	B
the	O
simple	B
linear	B
regression	B
model	B
of	O
chapter	O
is	O
not	O
appropriate	O
it	O
is	O
worth	O
noting	O
that	O
figure	O
displays	O
a	O
very	O
pronounced	O
relationship	O
between	O
the	O
predictor	B
balance	O
and	O
the	O
response	B
default	B
in	O
most	O
real	O
applications	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
will	O
not	O
be	O
nearly	O
so	O
strong	O
however	O
for	O
the	O
sake	O
of	O
illustrating	O
the	O
classification	B
procedures	O
discussed	O
in	O
this	O
chapter	O
we	O
use	O
an	O
example	O
in	O
which	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
is	O
somewhat	O
exaggerated	O
e	O
m	O
o	O
c	O
n	O
i	O
why	O
not	O
linear	B
regression	B
e	O
c	O
n	O
a	O
a	O
b	O
l	O
e	O
m	O
o	O
c	O
n	O
i	O
balance	O
yes	O
no	O
default	B
yes	O
no	O
default	B
figure	O
the	O
default	B
data	B
set	B
left	O
the	O
annual	O
incomes	O
and	O
monthly	O
credit	B
card	O
balances	O
of	O
a	O
number	O
of	O
individuals	O
the	O
individuals	O
who	O
defaulted	O
on	O
their	O
credit	B
card	O
payments	O
are	O
shown	O
in	O
orange	O
and	O
those	O
who	O
did	O
not	O
are	O
shown	O
in	O
blue	O
center	O
boxplots	O
of	O
balance	O
as	O
a	O
function	B
of	O
default	B
status	O
right	O
boxplots	O
of	O
income	B
as	O
a	O
function	B
of	O
default	B
status	O
why	O
not	O
linear	B
regression	B
we	O
have	O
stated	O
that	O
linear	B
regression	B
is	O
not	O
appropriate	O
in	O
the	O
case	O
of	O
a	O
qualitative	B
response	B
why	O
not	O
suppose	O
that	O
we	O
are	O
trying	O
to	O
predict	O
the	O
medical	O
condition	O
of	O
a	O
patient	O
in	O
the	O
emergency	O
room	O
on	O
the	O
basis	B
of	O
her	O
symptoms	O
in	O
this	O
simplified	O
example	O
there	O
are	O
three	O
possible	O
diagnoses	O
stroke	O
drug	O
overdose	O
and	O
epileptic	O
seizure	O
we	O
could	O
consider	O
encoding	O
these	O
values	O
as	O
a	O
quantitative	B
response	B
variable	B
y	O
as	O
follows	O
y	O
if	O
stroke	O
if	O
drug	O
overdose	O
if	O
epileptic	O
seizure	O
using	O
this	O
coding	O
least	B
squares	I
could	O
be	O
used	O
to	O
fit	O
a	O
linear	B
regression	B
model	B
to	O
predict	O
y	O
on	O
the	O
basis	B
of	O
a	O
set	B
of	O
predictors	O
xp	O
unfortunately	O
this	O
coding	O
implies	O
an	O
ordering	O
on	O
the	O
outcomes	O
putting	O
drug	O
overdose	O
in	O
between	O
stroke	O
and	O
epileptic	O
seizure	O
and	O
insisting	O
that	O
the	O
difference	O
between	O
stroke	O
and	O
drug	O
overdose	O
is	O
the	O
same	O
as	O
the	O
difference	O
between	O
drug	O
overdose	O
and	O
epileptic	O
seizure	O
in	O
practice	O
there	O
is	O
no	O
particular	O
reason	O
that	O
this	O
needs	O
to	O
be	O
the	O
case	O
for	O
instance	O
one	O
could	O
choose	O
an	O
equally	O
reasonable	O
coding	O
y	O
if	O
epileptic	O
seizure	O
if	O
stroke	O
if	O
drug	O
overdose	O
classification	B
which	O
would	O
imply	O
a	O
totally	O
different	O
relationship	O
among	O
the	O
three	O
conditions	O
each	O
of	O
these	O
codings	O
would	O
produce	O
fundamentally	O
different	O
linear	B
models	O
that	O
would	O
ultimately	O
lead	O
to	O
different	O
sets	O
of	O
predictions	O
on	O
test	O
observations	B
if	O
the	O
response	B
variable	B
s	O
values	O
did	O
take	O
on	O
a	O
natural	B
ordering	O
such	O
as	O
mild	O
moderate	O
and	O
severe	O
and	O
we	O
felt	O
the	O
gap	O
between	O
mild	O
and	O
moderate	O
was	O
similar	O
to	O
the	O
gap	O
between	O
moderate	O
and	O
severe	O
then	O
a	O
coding	O
would	O
be	O
reasonable	O
unfortunately	O
in	O
general	O
there	O
is	O
no	O
natural	B
way	O
to	O
convert	O
a	O
qualitative	B
response	B
variable	B
with	O
more	O
than	O
two	O
levels	O
into	O
a	O
quantitative	B
response	B
that	O
is	O
ready	O
for	O
linear	B
regression	B
for	O
a	O
binary	B
level	B
qualitative	B
response	B
the	O
situation	O
is	O
better	O
for	O
instance	O
perhaps	O
there	O
are	O
only	O
two	O
possibilities	O
for	O
the	O
patient	O
s	O
medical	O
condition	O
stroke	O
and	O
drug	O
overdose	O
we	O
could	O
then	O
potentially	O
use	O
the	O
dummy	B
variable	B
approach	B
from	O
section	O
to	O
code	O
the	O
response	B
as	O
follows	O
binary	B
y	O
if	O
stroke	O
if	O
drug	O
overdose	O
we	O
could	O
then	O
fit	O
a	O
linear	B
regression	B
to	O
this	O
binary	B
response	B
and	O
predict	O
drug	O
overdose	O
if	O
y	O
and	O
stroke	O
otherwise	O
in	O
the	O
binary	B
case	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
even	O
if	O
we	O
flip	O
the	O
above	O
coding	O
linear	B
regression	B
will	O
produce	O
the	O
same	O
final	O
predictions	O
for	O
a	O
binary	B
response	B
with	O
a	O
coding	O
as	O
above	O
regression	B
by	O
least	B
squares	I
does	O
make	O
sense	O
it	O
can	O
be	O
shown	O
that	O
the	O
x	O
obtained	O
using	O
linear	B
regression	B
is	O
in	O
fact	O
an	O
estimate	O
of	O
prdrug	O
overdosex	O
in	O
this	O
special	O
case	O
however	O
if	O
we	O
use	O
linear	B
regression	B
some	O
of	O
our	O
estimates	O
might	O
be	O
outside	O
the	O
interval	B
figure	O
making	O
them	O
hard	O
to	O
interpret	O
as	O
probabilities	O
nevertheless	O
the	O
predictions	O
provide	O
an	O
ordering	O
and	O
can	O
be	O
interpreted	O
as	O
crude	O
probability	B
estimates	O
curiously	O
it	O
turns	O
out	O
that	O
the	O
classifications	O
that	O
we	O
get	O
if	O
we	O
use	O
linear	B
regression	B
to	O
predict	O
a	O
binary	B
response	B
will	O
be	O
the	O
same	O
as	O
for	O
the	O
linear	B
discriminant	I
analysis	B
procedure	O
we	O
discuss	O
in	O
section	O
however	O
the	O
dummy	B
variable	B
approach	B
cannot	O
be	O
easily	O
extended	O
to	O
accommodate	O
qualitative	B
responses	O
with	O
more	O
than	O
two	O
levels	O
for	O
these	O
reasons	O
it	O
is	O
preferable	O
to	O
use	O
a	O
classification	B
method	O
that	O
is	O
truly	O
suited	O
for	O
qualitative	B
response	B
values	O
such	O
as	O
the	O
ones	O
presented	O
next	O
logistic	B
regression	B
consider	O
again	O
the	O
default	B
data	B
set	B
where	O
the	O
response	B
default	B
falls	O
into	O
one	O
of	O
two	O
categories	O
yes	O
or	O
no	O
rather	O
than	O
modeling	O
this	O
response	B
y	O
directly	O
logistic	B
regression	B
models	O
the	O
probability	B
that	O
y	O
belongs	O
to	O
a	O
particular	O
category	O
t	O
l	O
f	O
u	O
a	O
e	O
d	O
f	O
o	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
t	O
l	O
f	O
u	O
a	O
e	O
d	O
f	O
o	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
logistic	B
regression	B
balance	O
balance	O
figure	O
classification	B
using	O
the	O
default	B
data	B
left	O
estimated	O
probability	B
of	O
default	B
using	O
linear	B
regression	B
some	O
estimated	O
probabilities	O
are	O
negative	O
the	O
orange	O
ticks	O
indicate	O
the	O
values	O
coded	O
for	O
defaultno	O
or	O
yes	O
right	O
predicted	O
probabilities	O
of	O
default	B
using	O
logistic	B
regression	B
all	O
probabilities	O
lie	O
between	O
and	O
for	O
the	O
default	B
data	B
logistic	B
regression	B
models	O
the	O
probability	B
of	O
default	B
for	O
example	O
the	O
probability	B
of	O
default	B
given	O
balance	O
can	O
be	O
written	O
as	O
prdefault	O
yesbalance	O
the	O
values	O
of	O
prdefault	O
yesbalance	O
which	O
we	O
abbreviate	O
pbalance	O
will	O
range	O
between	O
and	O
then	O
for	O
any	O
given	O
value	O
of	O
balance	O
a	O
prediction	B
can	O
be	O
made	O
for	O
default	B
for	O
example	O
one	O
might	O
predict	O
default	B
yes	O
for	O
any	O
individual	O
for	O
whom	O
pbalance	O
alternatively	O
if	O
a	O
company	O
wishes	O
to	O
be	O
conservative	O
in	O
predicting	O
individuals	O
who	O
are	O
at	O
risk	O
for	O
default	B
then	O
they	O
may	O
choose	O
to	O
use	O
a	O
lower	O
threshold	O
such	O
as	O
pbalance	O
the	O
logistic	O
model	B
how	O
should	O
we	O
model	B
the	O
relationship	O
between	O
px	O
pry	O
and	O
x	O
convenience	O
we	O
are	O
using	O
the	O
generic	O
coding	O
for	O
the	O
response	B
in	O
section	O
we	O
talked	O
of	O
using	O
a	O
linear	B
regression	B
model	B
to	O
represent	O
these	O
probabilities	O
px	O
if	O
we	O
use	O
this	O
approach	B
to	O
predict	O
defaultyes	O
using	O
balance	O
then	O
we	O
obtain	O
the	O
model	B
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
here	O
we	O
see	O
the	O
problem	O
with	O
this	O
approach	B
for	O
balances	O
close	O
to	O
zero	O
we	O
predict	O
a	O
negative	O
probability	B
of	O
default	B
if	O
we	O
were	O
to	O
predict	O
for	O
very	O
large	O
balances	O
we	O
would	O
get	O
values	O
bigger	O
than	O
these	O
predictions	O
are	O
not	O
sensible	O
since	O
of	O
course	O
the	O
true	O
probability	B
of	O
default	B
regardless	O
of	O
credit	B
card	O
balance	O
must	O
fall	O
between	O
and	O
this	O
problem	O
is	O
not	O
unique	O
to	O
the	O
credit	B
default	B
data	B
any	O
time	O
a	O
straight	O
line	B
is	O
fit	O
to	O
a	O
binary	B
response	B
that	O
is	O
coded	O
as	O
classification	B
or	O
in	O
principle	O
we	O
can	O
always	O
predict	O
px	O
for	O
some	O
values	O
of	O
x	O
and	O
px	O
for	O
others	O
the	O
range	O
of	O
x	O
is	O
limited	O
to	O
avoid	O
this	O
problem	O
we	O
must	O
model	B
px	O
using	O
a	O
function	B
that	O
gives	O
outputs	O
between	O
and	O
for	O
all	O
values	O
of	O
x	O
many	O
functions	O
meet	O
this	O
description	O
in	O
logistic	B
regression	B
we	O
use	O
the	O
logistic	O
function	B
px	O
e	O
e	O
to	O
fit	O
the	O
model	B
we	O
use	O
a	O
method	O
called	O
maximum	B
likelihood	I
which	O
we	O
discuss	O
in	O
the	O
next	O
section	O
the	O
right-hand	O
panel	O
of	O
figure	O
illustrates	O
the	O
fit	O
of	O
the	O
logistic	B
regression	B
model	B
to	O
the	O
default	B
data	B
notice	O
that	O
for	O
low	O
balances	O
we	O
now	O
predict	O
the	O
probability	B
of	O
default	B
as	O
close	O
to	O
but	O
never	O
below	O
zero	O
likewise	O
for	O
high	O
balances	O
we	O
predict	O
a	O
default	B
probability	B
close	O
to	O
but	O
never	O
above	O
one	O
the	O
logistic	O
function	B
will	O
always	O
produce	O
an	O
s-shaped	O
curve	O
of	O
this	O
form	O
and	O
so	O
regardless	O
of	O
the	O
value	O
of	O
x	O
we	O
will	O
obtain	O
a	O
sensible	O
prediction	B
we	O
also	O
see	O
that	O
the	O
logistic	O
model	B
is	O
better	O
able	O
to	O
capture	O
the	O
range	O
of	O
probabilities	O
than	O
is	O
the	O
linear	B
regression	B
model	B
in	O
the	O
left-hand	O
plot	B
the	O
average	B
fitted	O
probability	B
in	O
both	O
cases	O
is	O
over	O
the	O
training	O
data	B
which	O
is	O
the	O
same	O
as	O
the	O
overall	O
proportion	O
of	O
defaulters	O
in	O
the	O
data	B
set	B
after	O
a	O
bit	O
of	O
manipulation	O
of	O
we	O
find	O
that	O
px	O
px	O
e	O
the	O
quantity	O
px	O
is	O
called	O
the	O
odds	B
and	O
can	O
take	O
on	O
any	O
value	O
between	O
and	O
values	O
of	O
the	O
odds	B
close	O
to	O
and	O
indicate	O
very	O
low	O
and	O
very	O
high	O
probabilities	O
of	O
default	B
respectively	O
for	O
example	O
on	O
average	B
in	O
people	O
with	O
an	O
odds	B
of	O
will	O
default	B
since	O
px	O
implies	O
an	O
likewise	O
on	O
average	B
nine	O
out	O
of	O
every	O
ten	O
people	O
with	O
odds	B
of	O
an	O
odds	B
of	O
will	O
default	B
since	O
px	O
implies	O
an	O
odds	B
of	O
odds	B
are	O
traditionally	O
used	O
instead	O
of	O
probabilities	O
in	O
horse-racing	O
since	O
they	O
relate	O
more	O
naturally	O
to	O
the	O
correct	O
betting	O
strategy	O
logistic	O
function	B
maximum	B
likelihood	I
odds	B
by	O
taking	O
the	O
logarithm	O
of	O
both	O
sides	O
of	O
we	O
arrive	O
at	O
log	O
px	O
px	O
the	O
left-hand	O
side	O
is	O
called	O
the	O
log-odds	O
or	O
logit	B
we	O
see	O
that	O
the	O
logistic	B
regression	B
model	B
has	O
a	O
logit	B
that	O
is	O
linear	B
in	O
x	O
recall	B
from	O
chapter	O
that	O
in	O
a	O
linear	B
regression	B
model	B
gives	O
the	O
average	B
change	O
in	O
y	O
associated	O
with	O
a	O
one-unit	O
increase	O
in	O
x	O
in	O
contrast	B
in	O
a	O
logistic	B
regression	B
model	B
increasing	O
x	O
by	O
one	O
unit	O
changes	O
the	O
log	O
odds	B
by	O
or	O
equivalently	O
it	O
multiplies	O
the	O
odds	B
by	O
e	O
however	O
because	O
the	O
relationship	O
between	O
px	O
and	O
x	O
in	O
is	O
not	O
a	O
straight	O
line	B
log-odds	O
logit	B
logistic	B
regression	B
does	O
not	O
correspond	O
to	O
the	O
change	O
in	O
px	O
associated	O
with	O
a	O
one-unit	O
increase	O
in	O
x	O
the	O
amount	O
that	O
px	O
changes	O
due	O
to	O
a	O
one-unit	O
change	O
in	O
x	O
will	O
depend	O
on	O
the	O
current	O
value	O
of	O
x	O
but	O
regardless	O
of	O
the	O
value	O
of	O
x	O
if	O
is	O
positive	O
then	O
increasing	O
x	O
will	O
be	O
associated	O
with	O
increasing	O
px	O
and	O
if	O
is	O
negative	O
then	O
increasing	O
x	O
will	O
be	O
associated	O
with	O
decreasing	O
px	O
the	O
fact	O
that	O
there	O
is	O
not	O
a	O
straight-line	O
relationship	O
between	O
px	O
and	O
x	O
and	O
the	O
fact	O
that	O
the	O
rate	B
of	O
change	O
in	O
px	O
per	O
unit	O
change	O
in	O
x	O
depends	O
on	O
the	O
current	O
value	O
of	O
x	O
can	O
also	O
be	O
seen	O
by	O
inspection	O
of	O
the	O
right-hand	O
panel	O
of	O
figure	O
estimating	O
the	O
regression	B
coefficients	O
the	O
coefficients	O
and	O
in	O
are	O
unknown	O
and	O
must	O
be	O
estimated	O
based	O
on	O
the	O
available	O
training	O
data	B
in	O
chapter	O
we	O
used	O
the	O
least	B
squares	I
approach	B
to	O
estimate	O
the	O
unknown	O
linear	B
regression	B
coefficients	O
although	O
we	O
could	O
use	O
least	B
squares	I
to	O
fit	O
the	O
model	B
the	O
more	O
general	O
method	O
of	O
maximum	B
likelihood	I
is	O
preferred	O
since	O
it	O
has	O
better	O
statistical	O
properties	O
the	O
basic	O
intuition	O
behind	O
using	O
maximum	B
likelihood	I
to	O
fit	O
a	O
logistic	B
regression	B
model	B
is	O
as	O
follows	O
we	O
seek	O
estimates	O
for	O
and	O
such	O
that	O
the	O
predicted	O
probability	B
pxi	O
of	O
default	B
for	O
each	O
individual	O
using	O
corresponds	O
as	O
closely	O
as	O
possible	O
to	O
the	O
individual	O
s	O
observed	O
default	B
status	O
in	O
other	O
words	O
we	O
try	O
to	O
find	O
and	O
such	O
that	O
plugging	O
these	O
estimates	O
into	O
the	O
model	B
for	O
px	O
given	O
in	O
yields	O
a	O
number	O
close	O
to	O
one	O
for	O
all	O
individuals	O
who	O
defaulted	O
and	O
a	O
number	O
close	O
to	O
zero	O
for	O
all	O
individuals	O
who	O
did	O
not	O
this	O
intuition	O
can	O
be	O
formalized	O
using	O
a	O
mathematical	O
equation	O
called	O
a	O
likelihood	B
function	B
pxi	O
likelihood	B
function	B
the	O
estimates	O
and	O
are	O
chosen	O
to	O
maximize	O
this	O
likelihood	B
function	B
maximum	B
likelihood	I
is	O
a	O
very	O
general	O
approach	B
that	O
is	O
used	O
to	O
fit	O
many	O
of	O
the	O
non-linear	B
models	O
that	O
we	O
examine	O
throughout	O
this	O
book	O
in	O
the	O
linear	B
regression	B
setting	O
the	O
least	B
squares	I
approach	B
is	O
in	O
fact	O
a	O
special	O
case	O
of	O
maximum	B
likelihood	I
the	O
mathematical	O
details	O
of	O
maximum	B
likelihood	I
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
however	O
in	O
general	O
logistic	B
regression	B
and	O
other	O
models	O
can	O
be	O
easily	O
fit	O
using	O
a	O
statistical	O
software	O
package	O
such	O
as	O
r	O
and	O
so	O
we	O
do	O
not	O
need	O
to	O
concern	O
ourselves	O
with	O
the	O
details	O
of	O
the	O
maximum	B
likelihood	I
fitting	O
procedure	O
table	O
shows	O
the	O
coefficient	O
estimates	O
and	O
related	O
information	O
that	O
result	O
from	O
fitting	O
a	O
logistic	B
regression	B
model	B
on	O
the	O
default	B
data	B
in	O
order	O
to	O
predict	O
the	O
probability	B
of	O
defaultyes	O
using	O
balance	O
we	O
see	O
that	O
this	O
indicates	O
that	O
an	O
increase	O
in	O
balance	O
is	O
associated	O
with	O
an	O
increase	O
in	O
the	O
probability	B
of	O
default	B
to	O
be	O
precise	O
a	O
one-unit	O
increase	O
in	O
balance	O
is	O
associated	O
with	O
an	O
increase	O
in	O
the	O
log	O
odds	B
of	O
default	B
by	O
units	O
classification	B
intercept	B
balance	O
coefficient	O
std	O
error	B
z-statistic	O
p-value	B
table	O
for	O
the	O
default	B
data	B
estimated	O
coefficients	O
of	O
the	O
logistic	B
regression	B
model	B
that	O
predicts	O
the	O
probability	B
of	O
default	B
using	O
balance	O
a	O
one-unit	O
increase	O
in	O
balance	O
is	O
associated	O
with	O
an	O
increase	O
in	O
the	O
log	O
odds	B
of	O
default	B
by	O
units	O
many	O
aspects	O
of	O
the	O
logistic	B
regression	B
output	B
shown	O
in	O
table	O
are	O
similar	O
to	O
the	O
linear	B
regression	B
output	B
of	O
chapter	O
for	O
example	O
we	O
can	O
measure	O
the	O
accuracy	O
of	O
the	O
coefficient	O
estimates	O
by	O
computing	O
their	O
standard	O
errors	O
the	O
z-statistic	O
in	O
table	O
plays	O
the	O
same	O
role	O
as	O
the	O
t-statistic	B
in	O
the	O
linear	B
regression	B
output	B
for	O
example	O
in	O
table	O
on	O
page	O
for	O
instance	O
the	O
z-statistic	O
associated	O
with	O
is	O
equal	O
to	O
and	O
so	O
a	O
large	O
value	O
of	O
the	O
z-statistic	O
indicates	O
evidence	O
against	O
the	O
null	B
hypothesis	B
this	O
null	B
hypothesis	B
implies	O
that	O
px	O
e	O
in	O
other	O
words	O
that	O
the	O
probability	B
of	O
default	B
does	O
not	O
depend	O
on	O
balance	O
since	O
the	O
p-value	B
associated	O
with	O
balance	O
in	O
table	O
is	O
tiny	O
we	O
can	O
reject	O
in	O
other	O
words	O
we	O
conclude	O
that	O
there	O
is	O
indeed	O
an	O
association	O
between	O
balance	O
and	O
probability	B
of	O
default	B
the	O
estimated	O
intercept	B
in	O
table	O
is	O
typically	O
not	O
of	O
interest	O
its	O
main	O
purpose	O
is	O
to	O
adjust	O
the	O
average	B
fitted	O
probabilities	O
to	O
the	O
proportion	O
of	O
ones	O
in	O
the	O
data	B
making	O
predictions	O
once	O
the	O
coefficients	O
have	O
been	O
estimated	O
it	O
is	O
a	O
simple	B
matter	O
to	O
compute	O
the	O
probability	B
of	O
default	B
for	O
any	O
given	O
credit	B
card	O
balance	O
for	O
example	O
using	O
the	O
coefficient	O
estimates	O
given	O
in	O
table	O
we	O
predict	O
that	O
the	O
default	B
probability	B
for	O
an	O
individual	O
with	O
a	O
balance	O
of	O
is	O
px	O
e	O
e	O
e	O
e	O
which	O
is	O
below	O
in	O
contrast	B
the	O
predicted	O
probability	B
of	O
default	B
for	O
an	O
individual	O
with	O
a	O
balance	O
of	O
is	O
much	O
higher	O
and	O
equals	O
or	O
one	O
can	O
use	O
qualitative	B
predictors	O
with	O
the	O
logistic	B
regression	B
model	B
using	O
the	O
dummy	B
variable	B
approach	B
from	O
section	O
as	O
an	O
example	O
the	O
default	B
data	B
set	B
contains	O
the	O
qualitative	B
variable	B
student	O
to	O
fit	O
the	O
model	B
we	O
simply	O
create	O
a	O
dummy	B
variable	B
that	O
takes	O
on	O
a	O
value	O
of	O
for	O
students	O
and	O
for	O
non-students	O
the	O
logistic	B
regression	B
model	B
that	O
results	O
from	O
predicting	O
probability	B
of	O
default	B
from	O
student	O
status	O
can	O
be	O
seen	O
in	O
table	O
the	O
coefficient	O
associated	O
with	O
the	O
dummy	B
variable	B
is	O
positive	O
logistic	B
regression	B
intercept	B
studentyes	O
coefficient	O
std	O
error	B
z-statistic	O
p-value	B
table	O
for	O
the	O
default	B
data	B
estimated	O
coefficients	O
of	O
the	O
logistic	B
regression	B
model	B
that	O
predicts	O
the	O
probability	B
of	O
default	B
using	O
student	O
status	O
student	O
status	O
is	O
encoded	O
as	O
a	O
dummy	B
variable	B
with	O
a	O
value	O
of	O
for	O
a	O
student	O
and	O
a	O
value	O
of	O
for	O
a	O
non-student	O
and	O
represented	O
by	O
the	O
variable	B
studentyes	O
in	O
the	O
table	O
and	O
the	O
associated	O
p-value	B
is	O
statistically	O
significant	O
this	O
indicates	O
that	O
students	O
tend	O
to	O
have	O
higher	O
default	B
probabilities	O
than	O
non-students	O
e	O
e	O
e	O
e	O
multiple	B
logistic	B
regression	B
we	O
now	O
consider	O
the	O
problem	O
of	O
predicting	O
a	O
binary	B
response	B
using	O
multiple	B
predictors	O
by	O
analogy	O
with	O
the	O
extension	O
from	O
simple	B
to	O
multiple	B
linear	B
regression	B
in	O
chapter	O
we	O
can	O
generalize	O
as	O
follows	O
log	O
px	O
px	O
pxp	O
where	O
x	O
xp	O
are	O
p	O
predictors	O
equation	O
can	O
be	O
rewritten	O
as	O
px	O
e	O
pxp	O
e	O
pxp	O
just	O
as	O
in	O
section	O
we	O
use	O
the	O
maximum	B
likelihood	I
method	O
to	O
estimate	O
p	O
table	O
shows	O
the	O
coefficient	O
estimates	O
for	O
a	O
logistic	B
regression	B
model	B
that	O
uses	O
balance	O
income	B
thousands	O
of	O
dollars	O
and	O
student	O
status	O
to	O
predict	O
probability	B
of	O
default	B
there	O
is	O
a	O
surprising	O
result	O
here	O
the	O
pvalues	O
associated	O
with	O
balance	O
and	O
the	O
dummy	B
variable	B
for	O
student	O
status	O
are	O
very	O
small	O
indicating	O
that	O
each	O
of	O
these	O
variables	O
is	O
associated	O
with	O
the	O
probability	B
of	O
default	B
however	O
the	O
coefficient	O
for	O
the	O
dummy	B
variable	B
is	O
negative	O
indicating	O
that	O
students	O
are	O
less	O
likely	O
to	O
default	B
than	O
nonstudents	O
in	O
contrast	B
the	O
coefficient	O
for	O
the	O
dummy	B
variable	B
is	O
positive	O
in	O
table	O
how	O
is	O
it	O
possible	O
for	O
student	O
status	O
to	O
be	O
associated	O
with	O
an	O
increase	O
in	O
probability	B
of	O
default	B
in	O
table	O
and	O
a	O
decrease	O
in	O
probability	B
of	O
default	B
in	O
table	O
the	O
left-hand	O
panel	O
of	O
figure	O
provides	O
a	O
graphical	O
illustration	O
of	O
this	O
apparent	O
paradox	O
the	O
orange	O
and	O
blue	O
solid	O
lines	O
show	O
the	O
average	B
default	B
rates	O
for	O
students	O
and	O
non-students	O
respectively	O
classification	B
intercept	B
balance	O
income	B
studentyes	O
coefficient	O
std	O
error	B
z-statistic	O
p-value	B
table	O
for	O
the	O
default	B
data	B
estimated	O
coefficients	O
of	O
the	O
logistic	B
regression	B
model	B
that	O
predicts	O
the	O
probability	B
of	O
default	B
using	O
balance	O
income	B
and	O
student	O
status	O
student	O
status	O
is	O
encoded	O
as	O
a	O
dummy	B
variable	B
studentyes	O
with	O
a	O
value	O
of	O
for	O
a	O
student	O
and	O
a	O
value	O
of	O
for	O
a	O
non-student	O
in	O
fitting	O
this	O
model	B
income	B
was	O
measured	O
in	O
thousands	O
of	O
dollars	O
as	O
a	O
function	B
of	O
credit	B
card	O
balance	O
the	O
negative	O
coefficient	O
for	O
student	O
in	O
the	O
multiple	B
logistic	B
regression	B
indicates	O
that	O
for	O
a	O
fixed	O
value	O
of	O
balance	O
and	O
income	B
a	O
student	O
is	O
less	O
likely	O
to	O
default	B
than	O
a	O
non-student	O
indeed	O
we	O
observe	O
from	O
the	O
left-hand	O
panel	O
of	O
figure	O
that	O
the	O
student	O
default	B
rate	B
is	O
at	O
or	O
below	O
that	O
of	O
the	O
non-student	O
default	B
rate	B
for	O
every	O
value	O
of	O
balance	O
but	O
the	O
horizontal	O
broken	O
lines	O
near	O
the	O
base	O
of	O
the	O
plot	B
which	O
show	O
the	O
default	B
rates	O
for	O
students	O
and	O
non-students	O
averaged	O
over	O
all	O
values	O
of	O
balance	O
and	O
income	B
suggest	O
the	O
opposite	O
effect	O
the	O
overall	O
student	O
default	B
rate	B
is	O
higher	O
than	O
the	O
non-student	O
default	B
rate	B
consequently	O
there	O
is	O
a	O
positive	O
coefficient	O
for	O
student	O
in	O
the	O
single	B
variable	B
logistic	B
regression	B
output	B
shown	O
in	O
table	O
the	O
right-hand	O
panel	O
of	O
figure	O
provides	O
an	O
explanation	O
for	O
this	O
discrepancy	O
the	O
variables	O
student	O
and	O
balance	O
are	O
correlated	O
students	O
tend	O
to	O
hold	O
higher	O
levels	O
of	O
debt	O
which	O
is	O
in	O
turn	O
associated	O
with	O
higher	O
probability	B
of	O
default	B
in	O
other	O
words	O
students	O
are	O
more	O
likely	O
to	O
have	O
large	O
credit	B
card	O
balances	O
which	O
as	O
we	O
know	O
from	O
the	O
left-hand	O
panel	O
of	O
figure	O
tend	O
to	O
be	O
associated	O
with	O
high	O
default	B
rates	O
thus	O
even	O
though	O
an	O
individual	O
student	O
with	O
a	O
given	O
credit	B
card	O
balance	O
will	O
tend	O
to	O
have	O
a	O
lower	O
probability	B
of	O
default	B
than	O
a	O
non-student	O
with	O
the	O
same	O
credit	B
card	O
balance	O
the	O
fact	O
that	O
students	O
on	O
the	O
whole	O
tend	O
to	O
have	O
higher	O
credit	B
card	O
balances	O
means	O
that	O
overall	O
students	O
tend	O
to	O
default	B
at	O
a	O
higher	O
rate	B
than	O
non-students	O
this	O
is	O
an	O
important	O
distinction	O
for	O
a	O
credit	B
card	O
company	O
that	O
is	O
trying	O
to	O
determine	O
to	O
whom	O
they	O
should	O
offer	O
credit	B
a	O
student	O
is	O
riskier	O
than	O
a	O
non-student	O
if	O
no	O
information	O
about	O
the	O
student	O
s	O
credit	B
card	O
balance	O
is	O
available	O
however	O
that	O
student	O
is	O
less	O
risky	O
than	O
a	O
non-student	O
with	O
the	O
same	O
credit	B
card	O
balance	O
this	O
simple	B
example	O
illustrates	O
the	O
dangers	O
and	O
subtleties	O
associated	O
with	O
performing	O
regressions	O
involving	O
only	O
a	O
single	B
predictor	B
when	O
other	O
predictors	O
may	O
also	O
be	O
relevant	O
as	O
in	O
the	O
linear	B
regression	B
setting	O
the	O
results	O
obtained	O
using	O
one	O
predictor	B
may	O
be	O
quite	O
different	O
from	O
those	O
obtained	O
using	O
multiple	B
predictors	O
especially	O
when	O
there	O
is	O
correlation	B
among	O
the	O
predictors	O
in	O
general	O
the	O
phenomenon	O
seen	O
in	O
figure	O
is	O
known	O
as	O
confounding	B
confounding	B
e	O
t	O
a	O
r	O
t	O
l	O
f	O
u	O
a	O
e	O
d	O
logistic	B
regression	B
l	O
e	O
c	O
n	O
a	O
a	O
b	O
d	O
r	O
a	O
c	O
t	O
i	O
d	O
e	O
r	O
c	O
credit	B
card	O
balance	O
no	O
yes	O
student	O
status	O
figure	O
confounding	B
in	O
the	O
default	B
data	B
left	O
default	B
rates	O
are	O
shown	O
for	O
students	O
and	O
non-students	O
the	O
solid	O
lines	O
display	O
default	B
rate	B
as	O
a	O
function	B
of	O
balance	O
while	O
the	O
horizontal	O
broken	O
lines	O
display	O
the	O
overall	O
default	B
rates	O
right	O
boxplots	O
of	O
balance	O
for	O
students	O
and	O
non-students	O
are	O
shown	O
by	O
substituting	O
estimates	O
for	O
the	O
regression	B
coefficients	O
from	O
table	O
into	O
we	O
can	O
make	O
predictions	O
for	O
example	O
a	O
student	O
with	O
a	O
credit	B
card	O
balance	O
of	O
and	O
an	O
income	B
of	O
has	O
an	O
estimated	O
probability	B
of	O
default	B
of	O
px	O
e	O
e	O
e	O
a	O
non-student	O
with	O
the	O
same	O
balance	O
and	O
income	B
has	O
an	O
estimated	O
probability	B
of	O
default	B
of	O
px	O
e	O
we	O
multiply	O
the	O
income	B
coefficient	O
estimate	O
from	O
table	O
by	O
rather	O
than	O
by	O
because	O
in	O
that	O
table	O
the	O
model	B
was	O
fit	O
with	O
income	B
measured	O
in	O
units	O
of	O
logistic	B
regression	B
for	O
response	B
classes	O
we	O
sometimes	O
wish	O
to	O
classify	O
a	O
response	B
variable	B
that	O
has	O
more	O
than	O
two	O
classes	O
for	O
example	O
in	O
section	O
we	O
had	O
three	O
categories	O
of	O
medical	O
condition	O
in	O
the	O
emergency	O
room	O
stroke	O
drug	O
overdose	O
epileptic	O
seizure	O
in	O
this	O
setting	O
we	O
wish	O
to	O
model	B
both	O
pry	O
strokex	O
and	O
pry	O
drug	O
overdosex	O
with	O
the	O
remaining	O
pry	O
epileptic	O
seizurex	O
pry	O
strokex	O
pry	O
drug	O
overdosex	O
the	O
two-class	O
logistic	B
regression	B
models	O
discussed	O
in	O
the	O
previous	O
sections	O
have	O
multiple-class	O
extensions	O
but	O
in	O
practice	O
they	O
tend	O
not	O
to	O
be	O
used	O
all	O
that	O
often	O
one	O
of	O
the	O
reasons	O
is	O
that	O
the	O
method	O
we	O
discuss	O
in	O
the	O
next	O
section	O
discriminant	O
classification	B
analysis	B
is	O
popular	O
for	O
multiple-class	O
classification	B
so	O
we	O
do	O
not	O
go	O
into	O
the	O
details	O
of	O
multiple-class	O
logistic	B
regression	B
here	O
but	O
simply	O
note	O
that	O
such	O
an	O
approach	B
is	O
possible	O
and	O
that	O
software	O
for	O
it	O
is	O
available	O
in	O
r	O
linear	B
discriminant	I
analysis	B
logistic	B
regression	B
involves	O
directly	O
modeling	O
pry	O
kx	O
x	O
using	O
the	O
logistic	O
function	B
given	O
by	O
for	O
the	O
case	O
of	O
two	O
response	B
classes	O
in	O
statistical	O
jargon	O
we	O
model	B
the	O
conditional	O
distribution	B
of	O
the	O
response	B
y	O
given	O
the	O
predictors	O
x	O
we	O
now	O
consider	O
an	O
alternative	O
and	O
less	O
direct	O
approach	B
to	O
estimating	O
these	O
probabilities	O
in	O
this	O
alternative	O
approach	B
we	O
model	B
the	O
distribution	B
of	O
the	O
predictors	O
x	O
separately	O
in	O
each	O
of	O
the	O
response	B
classes	O
given	O
y	O
and	O
then	O
use	O
bayes	O
theorem	O
to	O
flip	O
these	O
around	O
into	O
estimates	O
for	O
pry	O
kx	O
x	O
when	O
these	O
distributions	O
are	O
assumed	O
to	O
be	O
normal	O
it	O
turns	O
out	O
that	O
the	O
model	B
is	O
very	O
similar	O
in	O
form	O
to	O
logistic	B
regression	B
why	O
do	O
we	O
need	O
another	O
method	O
when	O
we	O
have	O
logistic	B
regression	B
there	O
are	O
several	O
reasons	O
when	O
the	O
classes	O
are	O
well-separated	O
the	O
parameter	B
estimates	O
for	O
the	O
logistic	B
regression	B
model	B
are	O
surprisingly	O
unstable	O
linear	B
discriminant	I
analysis	B
does	O
not	O
suffer	O
from	O
this	O
problem	O
if	O
n	O
is	O
small	O
and	O
the	O
distribution	B
of	O
the	O
predictors	O
x	O
is	O
approximately	O
normal	O
in	O
each	O
of	O
the	O
classes	O
the	O
linear	B
discriminant	O
model	B
is	O
again	O
more	O
stable	O
than	O
the	O
logistic	B
regression	B
model	B
as	O
mentioned	O
in	O
section	O
linear	B
discriminant	I
analysis	B
is	O
popular	O
when	O
we	O
have	O
more	O
than	O
two	O
response	B
classes	O
using	O
bayes	O
theorem	O
for	O
classification	B
suppose	O
that	O
we	O
wish	O
to	O
classify	O
an	O
observation	O
into	O
one	O
of	O
k	O
classes	O
where	O
k	O
in	O
other	O
words	O
the	O
qualitative	B
response	B
variable	B
y	O
can	O
take	O
on	O
k	O
possible	O
distinct	O
and	O
unordered	O
values	O
let	O
k	O
represent	O
the	O
overall	O
or	O
prior	O
probability	B
that	O
a	O
randomly	O
chosen	O
observation	O
comes	O
from	O
the	O
kth	O
class	O
this	O
is	O
the	O
probability	B
that	O
a	O
given	O
observation	O
is	O
associated	O
with	O
the	O
kth	O
category	O
of	O
the	O
response	B
variable	B
y	O
let	O
fk	O
prx	O
xy	O
k	O
denote	O
x	O
prior	O
the	O
density	B
function	B
of	O
x	O
for	O
an	O
observation	O
that	O
comes	O
from	O
the	O
kth	O
class	O
in	O
other	O
words	O
fkx	O
is	O
relatively	O
large	O
if	O
there	O
is	O
a	O
high	O
probability	B
that	O
an	O
observation	O
in	O
the	O
kth	O
class	O
has	O
x	O
x	O
and	O
fkx	O
is	O
small	O
if	O
it	O
is	O
very	O
density	B
function	B
this	O
definition	O
is	O
only	O
correct	O
if	O
x	O
is	O
continuous	B
then	O
region	O
dx	O
around	O
x	O
fkxdx	O
would	O
correspond	O
to	O
the	O
probability	B
of	O
is	O
a	O
discrete	O
random	O
variabl	O
e	O
if	O
x	O
fa	O
ling	O
in	O
in	O
a	O
small	O
x	O
l	O
unlikely	O
that	O
an	O
observation	O
in	O
the	O
kth	O
class	O
has	O
x	O
x	O
then	O
bayes	O
theorem	O
states	O
that	O
linear	B
discriminant	I
analysis	B
pry	O
kx	O
x	O
kfkx	O
k	O
lflx	O
in	O
accordance	O
with	O
our	O
earlier	O
notation	O
we	O
will	O
use	O
the	O
abbreviation	O
pkx	O
pry	O
kx	O
this	O
suggests	O
that	O
instead	O
of	O
directly	O
computing	O
pkx	O
as	O
in	O
section	O
we	O
can	O
simply	O
plug	O
in	O
estimates	O
of	O
k	O
and	O
fkx	O
into	O
in	O
general	O
estimating	O
k	O
is	O
easy	O
if	O
we	O
have	O
a	O
random	O
sample	O
of	O
y	O
s	O
from	O
the	O
population	O
we	O
simply	O
compute	O
the	O
fraction	O
of	O
the	O
training	O
observations	B
that	O
belong	O
to	O
the	O
kth	O
class	O
however	O
estimating	O
fkx	O
tends	O
to	O
be	O
more	O
challenging	O
unless	O
we	O
assume	O
some	O
simple	B
forms	O
for	O
these	O
densities	O
we	O
refer	O
to	O
pkx	O
as	O
the	O
posterior	O
probability	B
that	O
an	O
observation	O
x	O
x	O
belongs	O
to	O
the	O
kth	O
class	O
that	O
is	O
it	O
is	O
the	O
probability	B
that	O
the	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
given	O
the	O
predictor	B
value	O
for	O
that	O
observation	O
we	O
know	O
from	O
chapter	O
that	O
the	O
bayes	O
classifier	B
which	O
classifies	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
pkx	O
is	O
largest	O
has	O
the	O
lowest	O
possible	O
error	B
rate	B
out	O
of	O
all	O
classifiers	O
is	O
of	O
course	O
only	O
true	O
if	O
the	O
terms	O
in	O
are	O
all	O
correctly	O
specified	O
therefore	O
if	O
we	O
can	O
find	O
a	O
way	O
to	O
estimate	O
fkx	O
then	O
we	O
can	O
develop	O
a	O
classifier	B
that	O
approximates	O
the	O
bayes	O
classifier	B
such	O
an	O
approach	B
is	O
the	O
topic	O
of	O
the	O
following	O
sections	O
linear	B
discriminant	I
analysis	B
for	O
p	O
for	O
now	O
assume	O
that	O
p	O
that	O
is	O
we	O
have	O
only	O
one	O
predictor	B
we	O
would	O
like	O
to	O
obtain	O
an	O
estimate	O
for	O
fkx	O
that	O
we	O
can	O
plug	O
into	O
in	O
order	O
to	O
estimate	O
pkx	O
we	O
will	O
then	O
classify	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
pkx	O
is	O
greatest	O
in	O
order	O
to	O
estimate	O
fkx	O
we	O
will	O
first	O
make	O
some	O
assumptions	O
about	O
its	O
form	O
suppose	O
we	O
assume	O
that	O
fkx	O
is	O
normal	O
or	O
gaussian	O
in	O
the	O
one	O
dimensional	O
setting	O
the	O
normal	O
density	O
takes	O
the	O
form	O
fkx	O
k	O
exp	O
k	O
bayes	O
theorem	O
posterior	O
normal	O
gaussian	O
where	O
k	O
and	O
k	O
are	O
the	O
mean	O
and	O
variance	B
parameters	O
for	O
the	O
kth	O
class	O
for	O
now	O
let	O
us	O
further	O
assume	O
that	O
k	O
that	O
is	O
there	O
is	O
a	O
shared	O
variance	B
term	B
across	O
all	O
k	O
classes	O
which	O
for	O
simplicity	O
we	O
can	O
denote	O
by	O
plugging	O
into	O
we	O
find	O
that	O
exp	O
pkx	O
k	O
k	O
l	O
exp	O
that	O
in	O
k	O
denotes	O
the	O
prior	O
probability	B
that	O
an	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
not	O
to	O
be	O
confused	O
with	O
the	O
mathematical	O
constant	O
the	O
bayes	O
classifier	B
involves	O
assigning	O
an	O
observation	O
classification	B
figure	O
left	O
two	O
one-dimensional	O
normal	O
density	O
functions	O
are	O
shown	O
the	O
dashed	O
vertical	O
line	B
represents	O
the	O
bayes	O
decision	B
boundary	I
right	O
observations	B
were	O
drawn	O
from	O
each	O
of	O
the	O
two	O
classes	O
and	O
are	O
shown	O
as	O
histograms	O
the	O
bayes	O
decision	B
boundary	I
is	O
again	O
shown	O
as	O
a	O
dashed	O
vertical	O
line	B
the	O
solid	O
vertical	O
line	B
represents	O
the	O
lda	O
decision	B
boundary	I
estimated	O
from	O
the	O
training	O
data	B
x	O
x	O
to	O
the	O
class	O
for	O
which	O
is	O
largest	O
taking	O
the	O
log	O
of	O
and	O
rearranging	O
the	O
terms	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
this	O
is	O
equivalent	O
to	O
assigning	O
the	O
observation	O
to	O
the	O
class	O
for	O
which	O
kx	O
x	O
k	O
k	O
log	O
k	O
is	O
largest	O
for	O
instance	O
if	O
k	O
and	O
then	O
the	O
bayes	O
classifier	B
assigns	O
an	O
observation	O
to	O
class	O
if	O
and	O
to	O
class	O
otherwise	O
in	O
this	O
case	O
the	O
bayes	O
decision	B
boundary	I
corresponds	O
to	O
the	O
point	O
where	O
x	O
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
two	O
normal	O
density	O
functions	O
that	O
are	O
displayed	O
and	O
represent	O
two	O
distinct	O
classes	O
the	O
mean	O
and	O
variance	B
parameters	O
for	O
the	O
two	O
density	O
functions	O
are	O
and	O
the	O
two	O
densities	O
overlap	O
and	O
so	O
given	O
that	O
x	O
x	O
there	O
is	O
some	O
uncertainty	O
about	O
the	O
class	O
to	O
which	O
the	O
observation	O
belongs	O
if	O
we	O
assume	O
that	O
an	O
observation	O
is	O
equally	O
likely	O
to	O
come	O
from	O
either	O
class	O
that	O
is	O
then	O
by	O
inspection	O
of	O
we	O
see	O
that	O
the	O
bayes	O
classifier	B
assigns	O
the	O
observation	O
to	O
class	O
if	O
x	O
and	O
class	O
otherwise	O
note	O
that	O
in	O
this	O
case	O
we	O
can	O
compute	O
the	O
bayes	O
classifier	B
because	O
we	O
know	O
that	O
x	O
is	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
within	O
each	O
class	O
and	O
we	O
know	O
all	O
of	O
the	O
parameters	O
involved	O
in	O
a	O
real-life	O
situation	O
we	O
are	O
not	O
able	O
to	O
calculate	O
the	O
bayes	O
classifier	B
in	O
practice	O
even	O
if	O
we	O
are	O
quite	O
certain	O
of	O
our	O
assumption	O
that	O
x	O
is	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
within	O
each	O
class	O
we	O
still	O
have	O
to	O
estimate	O
the	O
parameters	O
k	O
k	O
and	O
the	O
linear	B
discriminant	O
linear	B
discriminant	I
analysis	B
analysis	B
method	O
approximates	O
the	O
bayes	O
classifier	B
by	O
plugging	O
estimates	O
for	O
k	O
k	O
and	O
into	O
in	O
particular	O
the	O
following	O
estimates	O
are	O
used	O
linear	B
discriminant	I
analysis	B
k	O
nk	O
iyik	O
xi	O
n	O
k	O
iyik	O
where	O
n	O
is	O
the	O
total	O
number	O
of	O
training	O
observations	B
and	O
nk	O
is	O
the	O
number	O
of	O
training	O
observations	B
in	O
the	O
kth	O
class	O
the	O
estimate	O
for	O
k	O
is	O
simply	O
the	O
average	B
of	O
all	O
the	O
training	O
observations	B
from	O
the	O
kth	O
class	O
while	O
can	O
be	O
seen	O
as	O
a	O
weighted	B
average	B
of	O
the	O
sample	O
variances	O
for	O
each	O
of	O
the	O
k	O
classes	O
sometimes	O
we	O
have	O
knowledge	O
of	O
the	O
class	O
membership	O
probabilities	O
k	O
which	O
can	O
be	O
used	O
directly	O
in	O
the	O
absence	O
of	O
any	O
additional	O
information	O
lda	O
estimates	O
k	O
using	O
the	O
proportion	O
of	O
the	O
training	O
observations	B
that	O
belong	O
to	O
the	O
kth	O
class	O
in	O
other	O
words	O
k	O
nkn	O
the	O
lda	O
classifier	B
plugs	O
the	O
estimates	O
given	O
in	O
and	O
into	O
and	O
assigns	O
an	O
observation	O
x	O
x	O
to	O
the	O
class	O
for	O
which	O
kx	O
x	O
k	O
k	O
log	O
k	O
discriminant	B
function	B
is	O
largest	O
the	O
word	O
linear	B
in	O
the	O
classifier	B
s	O
name	O
stems	O
from	O
the	O
fact	O
that	O
the	O
discriminant	O
functions	O
kx	O
in	O
are	O
linear	B
functions	O
of	O
x	O
opposed	O
to	O
a	O
more	O
complex	O
function	B
of	O
x	O
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
a	O
histogram	B
of	O
a	O
random	O
sample	O
of	O
observations	B
from	O
each	O
class	O
to	O
implement	O
lda	O
we	O
began	O
by	O
estimating	O
k	O
k	O
and	O
using	O
and	O
we	O
then	O
computed	O
the	O
decision	B
boundary	I
shown	O
as	O
a	O
black	O
solid	O
line	B
that	O
results	O
from	O
assigning	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
is	O
largest	O
all	O
points	O
to	O
the	O
left	O
of	O
this	O
line	B
will	O
be	O
assigned	O
to	O
the	O
green	O
class	O
while	O
points	O
to	O
the	O
right	O
of	O
this	O
line	B
are	O
assigned	O
to	O
the	O
purple	O
class	O
in	O
this	O
case	O
since	O
we	O
have	O
as	O
a	O
result	O
the	O
decision	B
boundary	I
corresponds	O
to	O
the	O
midpoint	O
between	O
the	O
sample	O
means	O
for	O
the	O
two	O
classes	O
the	O
figure	O
indicates	O
that	O
the	O
lda	O
decision	B
boundary	I
is	O
slightly	O
to	O
the	O
left	O
of	O
the	O
optimal	O
bayes	O
decision	B
boundary	I
which	O
instead	O
equals	O
how	O
well	O
does	O
the	O
lda	O
classifier	B
perform	O
on	O
this	O
data	B
since	O
this	O
is	O
simulated	O
data	B
we	O
can	O
generate	O
a	O
large	O
number	O
of	O
test	O
observations	B
in	O
order	O
to	O
compute	O
the	O
bayes	O
error	B
rate	B
and	O
the	O
lda	O
test	O
error	B
rate	B
these	O
are	O
and	O
respectively	O
in	O
other	O
words	O
the	O
lda	O
classifier	B
s	O
error	B
rate	B
is	O
only	O
above	O
the	O
smallest	O
possible	O
error	B
rate	B
this	O
indicates	O
that	O
lda	O
is	O
performing	O
pretty	O
well	O
on	O
this	O
data	B
set	B
classification	B
figure	O
two	O
multivariate	B
gaussian	I
density	O
functions	O
are	O
shown	O
with	O
p	O
left	O
the	O
two	O
predictors	O
are	O
uncorrelated	O
right	O
the	O
two	O
variables	O
have	O
a	O
correlation	B
of	O
to	O
reiterate	O
the	O
lda	O
classifier	B
results	O
from	O
assuming	O
that	O
the	O
observations	B
within	O
each	O
class	O
come	O
from	O
a	O
normal	O
distribution	B
with	O
a	O
class-specific	O
mean	O
vector	B
and	O
a	O
common	O
variance	B
and	O
plugging	O
estimates	O
for	O
these	O
parameters	O
into	O
the	O
bayes	O
classifier	B
in	O
section	O
we	O
will	O
consider	O
a	O
less	O
stringent	O
set	B
of	O
assumptions	O
by	O
allowing	O
the	O
observations	B
in	O
the	O
kth	O
class	O
to	O
have	O
a	O
class-specific	O
variance	B
k	O
multivariate	B
gaussian	I
linear	B
discriminant	I
analysis	B
for	O
p	O
we	O
now	O
extend	O
the	O
lda	O
classifier	B
to	O
the	O
case	O
of	O
multiple	B
predictors	O
to	O
do	O
this	O
we	O
will	O
assume	O
that	O
x	O
xp	O
is	O
drawn	O
from	O
a	O
multivariate	B
gaussian	I
multivariate	B
normal	I
distribution	B
with	O
a	O
class-specific	O
mean	O
vector	B
and	O
a	O
common	O
covariance	O
matrix	O
we	O
begin	O
with	O
a	O
brief	O
review	O
of	O
such	O
a	O
distribution	B
the	O
multivariate	B
gaussian	I
distribution	B
assumes	O
that	O
each	O
individual	O
predictor	B
follows	O
a	O
one-dimensional	O
normal	O
distribution	B
as	O
in	O
with	O
some	O
correlation	B
between	O
each	O
pair	O
of	O
predictors	O
two	O
examples	O
of	O
multivariate	B
gaussian	I
distributions	O
with	O
p	O
are	O
shown	O
in	O
figure	O
the	O
height	O
of	O
the	O
surface	O
at	O
any	O
particular	O
point	O
represents	O
the	O
probability	B
that	O
both	O
and	O
fall	O
in	O
a	O
small	O
region	O
around	O
that	O
point	O
in	O
either	O
panel	O
if	O
the	O
surface	O
is	O
cut	O
along	O
the	O
axis	O
or	O
along	O
the	O
axis	O
the	O
resulting	O
cross-section	O
will	O
have	O
the	O
shape	O
of	O
a	O
one-dimensional	O
normal	O
distribution	B
the	O
left-hand	O
panel	O
of	O
figure	O
illustrates	O
an	O
example	O
in	O
which	O
and	O
this	O
surface	O
has	O
a	O
characteristic	O
bell	O
shape	O
however	O
the	O
bell	O
shape	O
will	O
be	O
distorted	O
if	O
the	O
predictors	O
are	O
correlated	O
or	O
have	O
unequal	O
variances	O
as	O
is	O
illustrated	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
in	O
this	O
situation	O
the	O
base	O
of	O
the	O
bell	O
will	O
have	O
an	O
elliptical	O
rather	O
than	O
circular	O
linear	B
discriminant	I
analysis	B
x	O
x	O
figure	O
an	O
example	O
with	O
three	O
classes	O
the	O
observations	B
from	O
each	O
class	O
are	O
drawn	O
from	O
a	O
multivariate	B
gaussian	I
distribution	B
with	O
p	O
with	O
a	O
class-specific	O
mean	O
vector	B
and	O
a	O
common	O
covariance	O
matrix	O
left	O
ellipses	O
that	O
contain	O
of	O
the	O
probability	B
for	O
each	O
of	O
the	O
three	O
classes	O
are	O
shown	O
the	O
dashed	O
lines	O
are	O
the	O
bayes	O
decision	O
boundaries	O
right	O
observations	B
were	O
generated	O
from	O
each	O
class	O
and	O
the	O
corresponding	O
lda	O
decision	O
boundaries	O
are	O
indicated	O
using	O
solid	O
black	O
lines	O
the	O
bayes	O
decision	O
boundaries	O
are	O
once	O
again	O
shown	O
as	O
dashed	O
lines	O
shape	O
to	O
indicate	O
that	O
a	O
p-dimensional	O
random	O
variable	B
x	O
has	O
a	O
multivariate	B
gaussian	I
distribution	B
we	O
write	O
x	O
n	O
here	O
ex	O
is	O
the	O
mean	O
of	O
x	O
vector	B
with	O
p	O
components	O
and	O
covx	O
is	O
the	O
p	O
p	O
covariance	O
matrix	O
of	O
x	O
formally	O
the	O
multivariate	B
gaussian	I
density	O
is	O
defined	O
as	O
f	O
exp	O
in	O
the	O
case	O
of	O
p	O
predictors	O
the	O
lda	O
classifier	B
assumes	O
that	O
the	O
observations	B
in	O
the	O
kth	O
class	O
are	O
drawn	O
from	O
a	O
multivariate	B
gaussian	I
distribution	B
n	O
k	O
where	O
k	O
is	O
a	O
class-specific	O
mean	O
vector	B
and	O
is	O
a	O
covariance	O
matrix	O
that	O
is	O
common	O
to	O
all	O
k	O
classes	O
plugging	O
the	O
density	B
function	B
for	O
the	O
kth	O
class	O
fkx	O
x	O
into	O
and	O
performing	O
a	O
little	O
bit	O
of	O
algebra	O
reveals	O
that	O
the	O
bayes	O
classifier	B
assigns	O
an	O
observation	O
x	O
x	O
to	O
the	O
class	O
for	O
which	O
kx	O
xt	O
k	O
k	O
log	O
k	O
t	O
k	O
is	O
largest	O
this	O
is	O
the	O
vectormatrix	O
version	O
of	O
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
three	O
equallysized	O
gaussian	O
classes	O
are	O
shown	O
with	O
class-specific	O
mean	O
vectors	O
and	O
a	O
common	O
covariance	O
matrix	O
the	O
three	O
ellipses	O
represent	O
regions	O
that	O
contain	O
of	O
the	O
probability	B
for	O
each	O
of	O
the	O
three	O
classes	O
the	O
dashed	O
lines	O
classification	B
are	O
the	O
bayes	O
decision	O
boundaries	O
in	O
other	O
words	O
they	O
represent	O
the	O
set	B
of	O
values	O
x	O
for	O
which	O
kx	O
i	O
e	O
xt	O
k	O
k	O
xt	O
t	O
k	O
l	O
l	O
t	O
l	O
for	O
k	O
l	O
log	O
k	O
term	B
from	O
has	O
disappeared	O
because	O
each	O
of	O
the	O
three	O
classes	O
has	O
the	O
same	O
number	O
of	O
training	O
observations	B
i	O
e	O
k	O
is	O
the	O
same	O
for	O
each	O
class	O
note	O
that	O
there	O
are	O
three	O
lines	O
representing	O
the	O
bayes	O
decision	O
boundaries	O
because	O
there	O
are	O
three	O
pairs	O
of	O
classes	O
among	O
the	O
three	O
classes	O
that	O
is	O
one	O
bayes	O
decision	B
boundary	I
separates	O
class	O
from	O
class	O
one	O
separates	O
class	O
from	O
class	O
and	O
one	O
separates	O
class	O
from	O
class	O
these	O
three	O
bayes	O
decision	O
boundaries	O
divide	O
the	O
predictor	B
space	O
into	O
three	O
regions	O
the	O
bayes	O
classifier	B
will	O
classify	O
an	O
observation	O
according	O
to	O
the	O
region	O
in	O
which	O
it	O
is	O
located	O
once	O
again	O
we	O
need	O
to	O
estimate	O
the	O
unknown	O
parameters	O
k	O
k	O
and	O
the	O
formulas	O
are	O
similar	O
to	O
those	O
used	O
in	O
the	O
onedimensional	O
case	O
given	O
in	O
to	O
assign	O
a	O
new	O
observation	O
x	O
x	O
lda	O
plugs	O
these	O
estimates	O
into	O
and	O
classifies	O
to	O
the	O
class	O
for	O
which	O
kx	O
is	O
largest	O
note	O
that	O
in	O
kx	O
is	O
a	O
linear	B
function	B
of	O
x	O
that	O
is	O
the	O
lda	O
decision	O
rule	O
depends	O
on	O
x	O
only	O
through	O
a	O
linear	B
combination	I
of	O
its	O
elements	O
once	O
again	O
this	O
is	O
the	O
reason	O
for	O
the	O
word	O
linear	B
in	O
lda	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
observations	B
drawn	O
from	O
each	O
of	O
the	O
three	O
classes	O
are	O
displayed	O
and	O
the	O
resulting	O
lda	O
decision	O
boundaries	O
are	O
shown	O
as	O
solid	O
black	O
lines	O
overall	O
the	O
lda	O
decision	O
boundaries	O
are	O
pretty	O
close	O
to	O
the	O
bayes	O
decision	O
boundaries	O
shown	O
again	O
as	O
dashed	O
lines	O
the	O
test	O
error	B
rates	O
for	O
the	O
bayes	O
and	O
lda	O
classifiers	O
are	O
and	O
respectively	O
this	O
indicates	O
that	O
lda	O
is	O
performing	O
well	O
on	O
this	O
data	B
we	O
can	O
perform	O
lda	O
on	O
the	O
default	B
data	B
in	O
order	O
to	O
predict	O
whether	O
or	O
not	O
an	O
individual	O
will	O
default	B
on	O
the	O
basis	B
of	O
credit	B
card	O
balance	O
and	O
student	O
status	O
the	O
lda	O
model	B
fit	O
to	O
the	O
training	O
samples	O
results	O
in	O
a	O
training	O
error	B
rate	B
of	O
this	O
sounds	O
like	O
a	O
low	O
error	B
rate	B
but	O
two	O
caveats	O
must	O
be	O
noted	O
first	O
of	O
all	O
training	O
error	B
rates	O
will	O
usually	O
be	O
lower	O
than	O
test	O
error	B
rates	O
which	O
are	O
the	O
real	O
quantity	O
of	O
interest	O
in	O
other	O
words	O
we	O
might	O
expect	O
this	O
classifier	B
to	O
perform	O
worse	O
if	O
we	O
use	O
it	O
to	O
predict	O
whether	O
or	O
not	O
a	O
new	O
set	B
of	O
individuals	O
will	O
default	B
the	O
reason	O
is	O
that	O
we	O
specifically	O
adjust	O
the	O
parameters	O
of	O
our	O
model	B
to	O
do	O
well	O
on	O
the	O
training	O
data	B
the	O
higher	O
the	O
ratio	O
of	O
parameters	O
p	O
to	O
number	O
of	O
samples	O
n	O
the	O
more	O
we	O
expect	O
this	O
overfitting	B
to	O
play	O
a	O
role	O
for	O
these	O
data	B
we	O
don	O
t	O
expect	O
this	O
to	O
be	O
a	O
problem	O
since	O
p	O
and	O
n	O
second	O
since	O
only	O
of	O
the	O
individuals	O
in	O
the	O
training	O
sample	O
defaulted	O
a	O
simple	B
but	O
useless	O
classifier	B
that	O
always	O
predicts	O
that	O
overfitting	B
linear	B
discriminant	I
analysis	B
predicted	O
default	B
status	O
no	O
yes	O
total	O
true	O
default	B
status	O
total	O
no	O
yes	O
table	O
a	O
confusion	B
matrix	I
compares	O
the	O
lda	O
predictions	O
to	O
the	O
true	O
default	B
statuses	O
for	O
the	O
training	O
observations	B
in	O
the	O
default	B
data	B
set	B
elements	O
on	O
the	O
diagonal	O
of	O
the	O
matrix	O
represent	O
individuals	O
whose	O
default	B
statuses	O
were	O
correctly	O
predicted	O
while	O
off-diagonal	O
elements	O
represent	O
individuals	O
that	O
were	O
misclassified	O
lda	O
made	O
incorrect	O
predictions	O
for	O
individuals	O
who	O
did	O
not	O
default	B
and	O
for	O
individuals	O
who	O
did	O
default	B
each	O
individual	O
will	O
not	O
default	B
regardless	O
of	O
his	O
or	O
her	O
credit	B
card	O
balance	O
and	O
student	O
status	O
will	O
result	O
in	O
an	O
error	B
rate	B
of	O
in	O
other	O
words	O
the	O
trivial	O
null	B
classifier	B
will	O
achieve	O
an	O
error	B
rate	B
that	O
is	O
only	O
a	O
bit	O
higher	O
than	O
the	O
lda	O
training	O
set	B
error	B
rate	B
null	B
in	O
practice	O
a	O
binary	B
classifier	B
such	O
as	O
this	O
one	O
can	O
make	O
two	O
types	O
of	O
errors	O
it	O
can	O
incorrectly	O
assign	O
an	O
individual	O
who	O
defaults	O
to	O
the	O
no	O
default	B
category	O
or	O
it	O
can	O
incorrectly	O
assign	O
an	O
individual	O
who	O
does	O
not	O
default	B
to	O
the	O
default	B
category	O
it	O
is	O
often	O
of	O
interest	O
to	O
determine	O
which	O
of	O
these	O
two	O
types	O
of	O
errors	O
are	O
being	O
made	O
a	O
confusion	B
matrix	I
shown	O
for	O
the	O
default	B
data	B
in	O
table	O
is	O
a	O
convenient	O
way	O
to	O
display	O
this	O
information	O
the	O
table	O
reveals	O
that	O
lda	O
predicted	O
that	O
a	O
total	O
of	O
people	O
would	O
default	B
of	O
these	O
people	O
actually	O
defaulted	O
and	O
did	O
not	O
hence	O
only	O
out	O
of	O
of	O
the	O
individuals	O
who	O
did	O
not	O
default	B
were	O
incorrectly	O
labeled	O
this	O
looks	O
like	O
a	O
pretty	O
low	O
error	B
rate	B
however	O
of	O
the	O
individuals	O
who	O
defaulted	O
were	O
missed	O
by	O
lda	O
so	O
while	O
the	O
overall	O
error	B
rate	B
is	O
low	O
the	O
error	B
rate	B
among	O
individuals	O
who	O
defaulted	O
is	O
very	O
high	O
from	O
the	O
perspective	O
of	O
a	O
credit	B
card	O
company	O
that	O
is	O
trying	O
to	O
identify	O
high-risk	O
individuals	O
an	O
error	B
rate	B
of	O
among	O
individuals	O
who	O
default	B
may	O
well	O
be	O
unacceptable	O
class-specific	O
performance	O
is	O
also	O
important	O
in	O
medicine	O
and	O
biology	O
where	O
the	O
terms	O
sensitivity	B
and	O
specificity	B
characterize	O
the	O
performance	O
of	O
a	O
classifier	B
or	O
screening	O
test	O
in	O
this	O
case	O
the	O
sensitivity	B
is	O
the	O
percentage	O
of	O
true	O
defaulters	O
that	O
are	O
identified	O
a	O
low	O
in	O
this	O
case	O
the	O
specificity	B
is	O
the	O
percentage	O
of	O
non-defaulters	O
that	O
are	O
correctly	O
identified	O
here	O
why	O
does	O
lda	O
do	O
such	O
a	O
poor	O
job	O
of	O
classifying	O
the	O
customers	O
who	O
default	B
in	O
other	O
words	O
why	O
does	O
it	O
have	O
such	O
a	O
low	O
sensitivity	B
as	O
we	O
have	O
seen	O
lda	O
is	O
trying	O
to	O
approximate	O
the	O
bayes	O
classifier	B
which	O
has	O
the	O
lowest	O
total	O
error	B
rate	B
out	O
of	O
all	O
classifiers	O
the	O
gaussian	O
model	B
is	O
correct	O
that	O
is	O
the	O
bayes	O
classifier	B
will	O
yield	O
the	O
smallest	O
possible	O
total	O
number	O
of	O
misclassified	O
observations	B
irrespective	O
of	O
which	O
class	O
the	O
errors	O
come	O
from	O
that	O
is	O
some	O
misclassifications	O
will	O
result	O
from	O
incorrectly	O
assigning	O
confusion	B
matrix	I
sensitivity	B
specificity	B
classification	B
predicted	O
default	B
status	O
no	O
yes	O
total	O
true	O
default	B
status	O
total	O
no	O
yes	O
table	O
a	O
confusion	B
matrix	I
compares	O
the	O
lda	O
predictions	O
to	O
the	O
true	O
default	B
statuses	O
for	O
the	O
training	O
observations	B
in	O
the	O
default	B
data	B
set	B
using	O
a	O
modified	O
threshold	O
value	O
that	O
predicts	O
default	B
for	O
any	O
individuals	O
whose	O
posterior	O
default	B
probability	B
exceeds	O
a	O
customer	O
who	O
does	O
not	O
default	B
to	O
the	O
default	B
class	O
and	O
others	O
will	O
result	O
from	O
incorrectly	O
assigning	O
a	O
customer	O
who	O
defaults	O
to	O
the	O
non-default	O
class	O
in	O
contrast	B
a	O
credit	B
card	O
company	O
might	O
particularly	O
wish	O
to	O
avoid	O
incorrectly	O
classifying	O
an	O
individual	O
who	O
will	O
default	B
whereas	O
incorrectly	O
classifying	O
an	O
individual	O
who	O
will	O
not	O
default	B
though	O
still	O
to	O
be	O
avoided	O
is	O
less	O
problematic	O
we	O
will	O
now	O
see	O
that	O
it	O
is	O
possible	O
to	O
modify	O
lda	O
in	O
order	O
to	O
develop	O
a	O
classifier	B
that	O
better	O
meets	O
the	O
credit	B
card	O
company	O
s	O
needs	O
the	O
bayes	O
classifier	B
works	O
by	O
assigning	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
the	O
posterior	O
probability	B
pkx	O
is	O
greatest	O
in	O
the	O
two-class	O
case	O
this	O
amounts	O
to	O
assigning	O
an	O
observation	O
to	O
the	O
default	B
class	O
if	O
prdefault	O
yesx	O
x	O
thus	O
the	O
bayes	O
classifier	B
and	O
by	O
extension	O
lda	O
uses	O
a	O
threshold	O
of	O
for	O
the	O
posterior	O
probability	B
of	O
default	B
in	O
order	O
to	O
assign	O
an	O
observation	O
to	O
the	O
default	B
class	O
however	O
if	O
we	O
are	O
concerned	O
about	O
incorrectly	O
predicting	O
the	O
default	B
status	O
for	O
individuals	O
who	O
default	B
then	O
we	O
can	O
consider	O
lowering	O
this	O
threshold	O
for	O
instance	O
we	O
might	O
label	O
any	O
customer	O
with	O
a	O
posterior	O
probability	B
of	O
default	B
above	O
to	O
the	O
default	B
class	O
in	O
other	O
words	O
instead	O
of	O
assigning	O
an	O
observation	O
to	O
the	O
default	B
class	O
if	O
holds	O
we	O
could	O
instead	O
assign	O
an	O
observation	O
to	O
this	O
class	O
if	O
prdefault	O
yesx	O
x	O
the	O
error	B
rates	O
that	O
result	O
from	O
taking	O
this	O
approach	B
are	O
shown	O
in	O
table	O
now	O
lda	O
predicts	O
that	O
individuals	O
will	O
default	B
of	O
the	O
individuals	O
who	O
default	B
lda	O
correctly	O
predicts	O
all	O
but	O
or	O
this	O
is	O
a	O
vast	O
improvement	O
over	O
the	O
error	B
rate	B
of	O
that	O
resulted	O
from	O
using	O
the	O
threshold	O
of	O
however	O
this	O
improvement	O
comes	O
at	O
a	O
cost	O
now	O
individuals	O
who	O
do	O
not	O
default	B
are	O
incorrectly	O
classified	O
as	O
a	O
result	O
the	O
overall	O
error	B
rate	B
has	O
increased	O
slightly	O
to	O
but	O
a	O
credit	B
card	O
company	O
may	O
consider	O
this	O
slight	O
increase	O
in	O
the	O
total	O
error	B
rate	B
to	O
be	O
a	O
small	O
price	O
to	O
pay	O
for	O
more	O
accurate	O
identification	O
of	O
individuals	O
who	O
do	O
indeed	O
default	B
figure	O
illustrates	O
the	O
trade-off	B
that	O
results	O
from	O
modifying	O
the	O
threshold	O
value	O
for	O
the	O
posterior	O
probability	B
of	O
default	B
various	O
error	B
rates	O
are	O
linear	B
discriminant	I
analysis	B
t	O
e	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
threshold	O
figure	O
for	O
the	O
default	B
data	B
set	B
error	B
rates	O
are	O
shown	O
as	O
a	O
function	B
of	O
the	O
threshold	O
value	O
for	O
the	O
posterior	O
probability	B
that	O
is	O
used	O
to	O
perform	O
the	O
assignment	O
the	O
black	O
solid	O
line	B
displays	O
the	O
overall	O
error	B
rate	B
the	O
blue	O
dashed	O
line	B
represents	O
the	O
fraction	O
of	O
defaulting	O
customers	O
that	O
are	O
incorrectly	O
classified	O
and	O
the	O
orange	O
dotted	O
line	B
indicates	O
the	O
fraction	O
of	O
errors	O
among	O
the	O
non-defaulting	O
customers	O
shown	O
as	O
a	O
function	B
of	O
the	O
threshold	O
value	O
using	O
a	O
threshold	O
of	O
as	O
in	O
minimizes	O
the	O
overall	O
error	B
rate	B
shown	O
as	O
a	O
black	O
solid	O
line	B
this	O
is	O
to	O
be	O
expected	O
since	O
the	O
bayes	O
classifier	B
uses	O
a	O
threshold	O
of	O
and	O
is	O
known	O
to	O
have	O
the	O
lowest	O
overall	O
error	B
rate	B
but	O
when	O
a	O
threshold	O
of	O
is	O
used	O
the	O
error	B
rate	B
among	O
the	O
individuals	O
who	O
default	B
is	O
quite	O
high	O
dashed	O
line	B
as	O
the	O
threshold	O
is	O
reduced	O
the	O
error	B
rate	B
among	O
individuals	O
who	O
default	B
decreases	O
steadily	O
but	O
the	O
error	B
rate	B
among	O
the	O
individuals	O
who	O
do	O
not	O
default	B
increases	O
how	O
can	O
we	O
decide	O
which	O
threshold	O
value	O
is	O
best	O
such	O
a	O
decision	O
must	O
be	O
based	O
on	O
domain	O
knowledge	O
such	O
as	O
detailed	O
information	O
about	O
the	O
costs	O
associated	O
with	O
default	B
the	O
roc	B
curve	I
is	O
a	O
popular	O
graphic	O
for	O
simultaneously	O
displaying	O
the	O
two	O
types	O
of	O
errors	O
for	O
all	O
possible	O
thresholds	O
the	O
name	O
roc	O
is	O
historic	O
and	O
comes	O
from	O
communications	O
theory	O
it	O
is	O
an	O
acronym	O
for	O
receiver	O
operating	O
characteristics	O
figure	O
displays	O
the	O
roc	B
curve	I
for	O
the	O
lda	O
classifier	B
on	O
the	O
training	O
data	B
the	O
overall	O
performance	O
of	O
a	O
classifier	B
summarized	O
over	O
all	O
possible	O
thresholds	O
is	O
given	O
by	O
the	O
area	B
under	I
the	I
curve	I
an	O
ideal	O
roc	B
curve	I
will	O
hug	O
the	O
top	O
left	O
corner	O
so	O
the	O
larger	O
the	O
auc	B
the	O
better	O
the	O
classifier	B
for	O
this	O
data	B
the	O
auc	B
is	O
which	O
is	O
close	O
to	O
the	O
maximum	O
of	O
one	O
so	O
would	O
be	O
considered	O
very	O
good	O
we	O
expect	O
a	O
classifier	B
that	O
performs	O
no	O
better	O
than	O
chance	O
to	O
have	O
an	O
auc	B
of	O
evaluated	O
on	O
an	O
independent	B
test	O
set	B
not	O
used	O
in	O
model	B
training	O
roc	O
curves	O
are	O
useful	O
for	O
comparing	O
different	O
classifiers	O
since	O
they	O
take	O
into	O
account	O
all	O
possible	O
thresholds	O
it	O
turns	O
out	O
that	O
the	O
roc	B
curve	I
for	O
the	O
logistic	B
regression	B
model	B
of	O
section	O
fit	O
to	O
these	O
data	B
is	O
virtually	O
indistinguishable	O
from	O
this	O
one	O
for	O
the	O
lda	O
model	B
so	O
we	O
do	O
not	O
display	O
it	O
here	O
as	O
we	O
have	O
seen	O
above	O
varying	O
the	O
classifier	B
threshold	O
changes	O
its	O
true	B
positive	I
and	O
false	B
positive	I
rate	B
these	O
are	O
also	O
called	O
the	O
sensitivity	B
and	O
one	O
roc	B
curve	I
area	B
under	I
the	I
curve	I
sensitivity	B
classification	B
roc	B
curve	I
t	O
e	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
false	B
positive	I
rate	B
figure	O
a	O
roc	B
curve	I
for	O
the	O
lda	O
classifier	B
on	O
the	O
default	B
data	B
it	O
traces	O
out	O
two	O
types	O
of	O
error	B
as	O
we	O
vary	O
the	O
threshold	O
value	O
for	O
the	O
posterior	O
probability	B
of	O
default	B
the	O
actual	O
thresholds	O
are	O
not	O
shown	O
the	O
true	B
positive	I
rate	B
is	O
the	O
sensitivity	B
the	O
fraction	O
of	O
defaulters	O
that	O
are	O
correctly	O
identified	O
using	O
a	O
given	O
threshold	O
value	O
the	O
false	B
positive	I
rate	B
is	O
the	O
fraction	O
of	O
non-defaulters	O
that	O
we	O
classify	O
incorrectly	O
as	O
defaulters	O
using	O
that	O
same	O
threshold	O
value	O
the	O
ideal	O
roc	B
curve	I
hugs	O
the	O
top	O
left	O
corner	O
indicating	O
a	O
high	O
true	B
positive	I
rate	B
and	O
a	O
low	O
false	B
positive	I
rate	B
the	O
dotted	O
line	B
represents	O
the	O
no	O
information	O
classifier	B
this	O
is	O
what	O
we	O
would	O
expect	O
if	O
student	O
status	O
and	O
credit	B
card	O
balance	O
are	O
not	O
associated	O
with	O
probability	B
of	O
default	B
or	O
non-null	O
true	O
false	O
pos	O
class	O
or	O
non-null	O
false	O
neg	O
true	O
pos	O
or	O
null	B
true	O
neg	O
total	O
n	O
p	O
predicted	O
class	O
or	O
null	B
total	O
n	O
p	O
table	O
possible	O
results	O
when	O
applying	O
a	O
classifier	B
or	O
diagnostic	O
test	O
to	O
a	O
population	O
minus	O
the	O
specificity	B
of	O
our	O
classifier	B
since	O
there	O
is	O
an	O
almost	O
bewildering	O
array	O
of	O
terms	O
used	O
in	O
this	O
context	O
we	O
now	O
give	O
a	O
summary	O
table	O
shows	O
the	O
possible	O
results	O
when	O
applying	O
a	O
classifier	B
diagnostic	O
test	O
to	O
a	O
population	O
to	O
make	O
the	O
connection	O
with	O
the	O
epidemiology	O
literature	O
we	O
think	O
of	O
as	O
the	O
disease	O
that	O
we	O
are	O
trying	O
to	O
detect	O
and	O
as	O
the	O
non-disease	O
state	O
to	O
make	O
the	O
connection	O
to	O
the	O
classical	O
hypothesis	B
testing	O
literature	O
we	O
think	O
of	O
as	O
the	O
null	B
hypothesis	B
and	O
as	O
the	O
alternative	B
hypothesis	B
in	O
the	O
context	O
of	O
the	O
default	B
data	B
indicates	O
an	O
individual	O
who	O
defaults	O
and	O
indicates	O
one	O
who	O
does	O
not	O
specificity	B
linear	B
discriminant	I
analysis	B
name	O
false	O
pos	O
rate	B
true	O
pos	O
rate	B
pos	O
pred	O
value	O
neg	O
pred	O
value	O
definition	O
synonyms	O
fpn	O
type	B
i	I
error	B
specificity	B
tpp	O
type	B
ii	I
error	B
power	B
sensitivity	B
recall	B
tpp	O
tnn	O
precision	B
false	B
discovery	I
proportion	I
table	O
important	O
measures	O
for	O
classification	B
and	O
diagnostic	O
testing	O
derived	O
from	O
quantities	O
in	O
table	O
table	O
lists	O
many	O
of	O
the	O
popular	O
performance	O
measures	O
that	O
are	O
used	O
in	O
this	O
context	O
the	O
denominators	O
for	O
the	O
false	B
positive	I
and	O
true	B
positive	I
rates	O
are	O
the	O
actual	O
population	O
counts	O
in	O
each	O
class	O
in	O
contrast	B
the	O
denominators	O
for	O
the	O
positive	B
predictive	I
value	I
and	O
the	O
negative	B
predictive	I
value	I
are	O
the	O
total	O
predicted	O
counts	O
for	O
each	O
class	O
quadratic	B
discriminant	O
analysis	B
as	O
we	O
have	O
discussed	O
lda	O
assumes	O
that	O
the	O
observations	B
within	O
each	O
class	O
are	O
drawn	O
from	O
a	O
multivariate	B
gaussian	I
distribution	B
with	O
a	O
classspecific	O
mean	O
vector	B
and	O
a	O
covariance	O
matrix	O
that	O
is	O
common	O
to	O
all	O
k	O
classes	O
quadratic	B
discriminant	O
analysis	B
provides	O
an	O
alternative	O
approach	B
like	O
lda	O
the	O
qda	O
classifier	B
results	O
from	O
assuming	O
that	O
the	O
observations	B
from	O
each	O
class	O
are	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
and	O
plugging	O
estimates	O
for	O
the	O
parameters	O
into	O
bayes	O
theorem	O
in	O
order	O
to	O
perform	O
prediction	B
however	O
unlike	O
lda	O
qda	O
assumes	O
that	O
each	O
class	O
has	O
its	O
own	O
covariance	O
matrix	O
that	O
is	O
it	O
assumes	O
that	O
an	O
observation	O
from	O
the	O
kth	O
class	O
is	O
of	O
the	O
form	O
x	O
n	O
k	O
k	O
where	O
k	O
is	O
a	O
covariance	O
matrix	O
for	O
the	O
kth	O
class	O
under	O
this	O
assumption	O
the	O
bayes	O
classifier	B
assigns	O
an	O
observation	O
x	O
x	O
to	O
the	O
class	O
for	O
which	O
quadratic	B
discriminant	O
analysis	B
kx	O
kt	O
xt	O
k	O
x	O
xt	O
k	O
k	O
log	O
k	O
log	O
k	O
log	O
k	O
log	O
k	O
k	O
k	O
k	O
k	O
t	O
k	O
is	O
largest	O
so	O
the	O
qda	O
classifier	B
involves	O
plugging	O
estimates	O
for	O
k	O
k	O
and	O
k	O
into	O
and	O
then	O
assigning	O
an	O
observation	O
x	O
x	O
to	O
the	O
class	O
for	O
which	O
this	O
quantity	O
is	O
largest	O
unlike	O
in	O
the	O
quantity	O
x	O
appears	O
as	O
a	O
quadratic	B
function	B
in	O
this	O
is	O
where	O
qda	O
gets	O
its	O
name	O
why	O
does	O
it	O
matter	O
whether	O
or	O
not	O
we	O
assume	O
that	O
the	O
k	O
classes	O
share	O
a	O
common	O
covariance	O
matrix	O
in	O
other	O
words	O
why	O
would	O
one	O
prefer	O
lda	O
to	O
qda	O
or	O
vice-versa	O
the	O
answer	O
lies	O
in	O
the	O
bias-variance	O
trade-off	B
when	O
there	O
are	O
p	O
predictors	O
then	O
estimating	O
a	O
covariance	O
matrix	O
requires	O
estimating	O
parameters	O
qda	O
estimates	O
a	O
separate	O
covariance	O
matrix	O
for	O
each	O
class	O
for	O
a	O
total	O
of	O
parameters	O
with	O
predictors	O
this	O
classification	B
x	O
x	O
figure	O
left	O
the	O
bayes	O
dashed	O
lda	O
dotted	O
and	O
qda	O
solid	O
decision	O
boundaries	O
for	O
a	O
two-class	O
problem	O
with	O
the	O
shading	O
indicates	O
the	O
qda	O
decision	O
rule	O
since	O
the	O
bayes	O
decision	B
boundary	I
is	O
linear	B
it	O
is	O
more	O
accurately	O
approximated	O
by	O
lda	O
than	O
by	O
qda	O
right	O
details	O
are	O
as	O
given	O
in	O
the	O
left-hand	O
panel	O
except	O
that	O
since	O
the	O
bayes	O
decision	B
boundary	I
is	O
non-linear	B
it	O
is	O
more	O
accurately	O
approximated	O
by	O
qda	O
than	O
by	O
lda	O
is	O
some	O
multiple	B
of	O
which	O
is	O
a	O
lot	O
of	O
parameters	O
by	O
instead	O
assuming	O
that	O
the	O
k	O
classes	O
share	O
a	O
common	O
covariance	O
matrix	O
the	O
lda	O
model	B
becomes	O
linear	B
in	O
x	O
which	O
means	O
there	O
are	O
kp	O
linear	B
coefficients	O
to	O
estimate	O
consequently	O
lda	O
is	O
a	O
much	O
less	O
flexible	O
classifier	B
than	O
qda	O
and	O
so	O
has	O
substantially	O
lower	O
variance	B
this	O
can	O
potentially	O
lead	O
to	O
improved	O
prediction	B
performance	O
but	O
there	O
is	O
a	O
trade-off	B
if	O
lda	O
s	O
assumption	O
that	O
the	O
k	O
classes	O
share	O
a	O
common	O
covariance	O
matrix	O
is	O
badly	O
off	O
then	O
lda	O
can	O
suffer	O
from	O
high	O
bias	B
roughly	O
speaking	O
lda	O
tends	O
to	O
be	O
a	O
better	O
bet	O
than	O
qda	O
if	O
there	O
are	O
relatively	O
few	O
training	O
observations	B
and	O
so	O
reducing	O
variance	B
is	O
crucial	O
in	O
contrast	B
qda	O
is	O
recommended	O
if	O
the	O
training	O
set	B
is	O
very	O
large	O
so	O
that	O
the	O
variance	B
of	O
the	O
classifier	B
is	O
not	O
a	O
major	O
concern	O
or	O
if	O
the	O
assumption	O
of	O
a	O
common	O
covariance	O
matrix	O
for	O
the	O
k	O
classes	O
is	O
clearly	O
untenable	O
figure	O
illustrates	O
the	O
performances	O
of	O
lda	O
and	O
qda	O
in	O
two	O
scenarios	O
in	O
the	O
left-hand	O
panel	O
the	O
two	O
gaussian	O
classes	O
have	O
a	O
common	O
correlation	B
of	O
between	O
and	O
as	O
a	O
result	O
the	O
bayes	O
decision	B
boundary	I
is	O
linear	B
and	O
is	O
accurately	O
approximated	O
by	O
the	O
lda	O
decision	B
boundary	I
the	O
qda	O
decision	B
boundary	I
is	O
inferior	O
because	O
it	O
suffers	O
from	O
higher	O
variance	B
without	O
a	O
corresponding	O
decrease	O
in	O
bias	B
in	O
contrast	B
the	O
right-hand	O
panel	O
displays	O
a	O
situation	O
in	O
which	O
the	O
orange	O
class	O
has	O
a	O
correlation	B
of	O
between	O
the	O
variables	O
and	O
the	O
blue	O
class	O
has	O
a	O
correlation	B
of	O
now	O
the	O
bayes	O
decision	B
boundary	I
is	O
quadratic	B
and	O
so	O
qda	O
more	O
accurately	O
approximates	O
this	O
boundary	O
than	O
does	O
lda	O
a	O
comparison	O
of	O
classification	B
methods	O
a	O
comparison	O
of	O
classification	B
methods	O
in	O
this	O
chapter	O
we	O
have	O
considered	O
three	O
different	O
classification	B
approaches	O
logistic	B
regression	B
lda	O
and	O
qda	O
in	O
chapter	O
we	O
also	O
discussed	O
the	O
k-nearest	O
neighbors	O
method	O
we	O
now	O
consider	O
the	O
types	O
of	O
scenarios	O
in	O
which	O
one	O
approach	B
might	O
dominate	O
the	O
others	O
though	O
their	O
motivations	O
differ	O
the	O
logistic	B
regression	B
and	O
lda	O
methods	O
are	O
closely	O
connected	O
consider	O
the	O
two-class	O
setting	O
with	O
p	O
predictor	B
and	O
let	O
and	O
be	O
the	O
probabilities	O
that	O
the	O
observation	O
x	O
x	O
belongs	O
to	O
class	O
and	O
class	O
respectively	O
in	O
the	O
lda	O
framework	O
we	O
can	O
see	O
from	O
to	O
a	O
bit	O
of	O
simple	B
algebra	O
that	O
the	O
log	O
odds	B
is	O
given	O
by	O
log	O
log	O
where	O
and	O
are	O
functions	O
of	O
and	O
from	O
we	O
know	O
that	O
in	O
logistic	B
regression	B
log	O
both	O
and	O
are	O
linear	B
functions	O
of	O
x	O
hence	O
both	O
logistic	B
regression	B
and	O
lda	O
produce	O
linear	B
decision	O
boundaries	O
the	O
only	O
difference	O
between	O
the	O
two	O
approaches	O
lies	O
in	O
the	O
fact	O
that	O
and	O
are	O
estimated	O
using	O
maximum	B
likelihood	I
whereas	O
and	O
are	O
computed	O
using	O
the	O
estimated	O
mean	O
and	O
variance	B
from	O
a	O
normal	O
distribution	B
this	O
same	O
connection	O
between	O
lda	O
and	O
logistic	B
regression	B
also	O
holds	O
for	O
multidimensional	O
data	B
with	O
p	O
since	O
logistic	B
regression	B
and	O
lda	O
differ	O
only	O
in	O
their	O
fitting	O
procedures	O
one	O
might	O
expect	O
the	O
two	O
approaches	O
to	O
give	O
similar	O
results	O
this	O
is	O
often	O
but	O
not	O
always	O
the	O
case	O
lda	O
assumes	O
that	O
the	O
observations	B
are	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
with	O
a	O
common	O
covariance	O
matrix	O
in	O
each	O
class	O
and	O
so	O
can	O
provide	O
some	O
improvements	O
over	O
logistic	B
regression	B
when	O
this	O
assumption	O
approximately	O
holds	O
conversely	O
logistic	B
regression	B
can	O
outperform	O
lda	O
if	O
these	O
gaussian	O
assumptions	O
are	O
not	O
met	O
recall	B
from	O
chapter	O
that	O
knn	O
takes	O
a	O
completely	O
different	O
approach	B
from	O
the	O
classifiers	O
seen	O
in	O
this	O
chapter	O
in	O
order	O
to	O
make	O
a	O
prediction	B
for	O
an	O
observation	O
x	O
x	O
the	O
k	O
training	O
observations	B
that	O
are	O
closest	O
to	O
x	O
are	O
identified	O
then	O
x	O
is	O
assigned	O
to	O
the	O
class	O
to	O
which	O
the	O
plurality	O
of	O
these	O
observations	B
belong	O
hence	O
knn	O
is	O
a	O
completely	O
non-parametric	B
approach	B
no	O
assumptions	O
are	O
made	O
about	O
the	O
shape	O
of	O
the	O
decision	B
boundary	I
therefore	O
we	O
can	O
expect	O
this	O
approach	B
to	O
dominate	O
lda	O
and	O
logistic	B
regression	B
when	O
the	O
decision	B
boundary	I
is	O
highly	O
non-linear	B
on	O
the	O
other	O
hand	O
knn	O
does	O
not	O
tell	O
us	O
which	O
predictors	O
are	O
important	O
we	O
don	O
t	O
get	O
a	O
table	O
of	O
coefficients	O
as	O
in	O
table	O
classification	B
scenario	O
scenario	O
scenario	O
knn	O
knn	O
cv	O
lda	O
logistic	O
qda	O
knn	O
knn	O
cv	O
lda	O
logistic	O
qda	O
knn	O
knn	O
cv	O
lda	O
logistic	O
qda	O
figure	O
boxplots	O
of	O
the	O
test	O
error	B
rates	O
for	O
each	O
of	O
the	O
linear	B
scenarios	O
described	O
in	O
the	O
main	O
text	O
scenario	O
scenario	O
scenario	O
knn	O
knn	O
cv	O
lda	O
logistic	O
qda	O
knn	O
knn	O
cv	O
lda	O
logistic	O
qda	O
knn	O
knn	O
cv	O
lda	O
logistic	O
qda	O
figure	O
boxplots	O
of	O
the	O
test	O
error	B
rates	O
for	O
each	O
of	O
the	O
non-linear	B
scenarios	O
described	O
in	O
the	O
main	O
text	O
finally	O
qda	O
serves	O
as	O
a	O
compromise	O
between	O
the	O
non-parametric	B
knn	O
method	O
and	O
the	O
linear	B
lda	O
and	O
logistic	B
regression	B
approaches	O
since	O
qda	O
assumes	O
a	O
quadratic	B
decision	B
boundary	I
it	O
can	O
accurately	O
model	B
a	O
wider	O
range	O
of	O
problems	O
than	O
can	O
the	O
linear	B
methods	O
though	O
not	O
as	O
flexible	O
as	O
knn	O
qda	O
can	O
perform	O
better	O
in	O
the	O
presence	O
of	O
a	O
limited	O
number	O
of	O
training	O
observations	B
because	O
it	O
does	O
make	O
some	O
assumptions	O
about	O
the	O
form	O
of	O
the	O
decision	B
boundary	I
to	O
illustrate	O
the	O
performances	O
of	O
these	O
four	O
classification	B
approaches	O
we	O
generated	O
data	B
from	O
six	O
different	O
scenarios	O
in	O
three	O
of	O
the	O
scenarios	O
the	O
bayes	O
decision	B
boundary	I
is	O
linear	B
and	O
in	O
the	O
remaining	O
scenarios	O
it	O
is	O
non-linear	B
for	O
each	O
scenario	O
we	O
produced	O
random	O
training	O
data	B
sets	O
on	O
each	O
of	O
these	O
training	O
sets	O
we	O
fit	O
each	O
method	O
to	O
the	O
data	B
and	O
computed	O
the	O
resulting	O
test	O
error	B
rate	B
on	O
a	O
large	O
test	O
set	B
results	O
for	O
the	O
linear	B
scenarios	O
are	O
shown	O
in	O
figure	O
and	O
the	O
results	O
for	O
the	O
non-linear	B
scenarios	O
are	O
in	O
figure	O
the	O
knn	O
method	O
requires	O
selection	B
of	O
k	O
the	O
number	O
of	O
neighbors	O
we	O
performed	O
knn	O
with	O
two	O
values	O
of	O
k	O
k	O
a	O
comparison	O
of	O
classification	B
methods	O
and	O
a	O
value	O
of	O
k	O
that	O
was	O
chosen	O
automatically	O
using	O
an	O
approach	B
called	O
cross-validation	B
which	O
we	O
discuss	O
further	O
in	O
chapter	O
in	O
each	O
of	O
the	O
six	O
scenarios	O
there	O
were	O
p	O
predictors	O
the	O
scenarios	O
were	O
as	O
follows	O
scenario	O
there	O
were	O
training	O
observations	B
in	O
each	O
of	O
two	O
classes	O
the	O
observations	B
within	O
each	O
class	O
were	O
uncorrelated	O
random	O
normal	O
variables	O
with	O
a	O
different	O
mean	O
in	O
each	O
class	O
the	O
left-hand	O
panel	O
of	O
figure	O
shows	O
that	O
lda	O
performed	O
well	O
in	O
this	O
setting	O
as	O
one	O
would	O
expect	O
since	O
this	O
is	O
the	O
model	B
assumed	O
by	O
lda	O
knn	O
performed	O
poorly	O
because	O
it	O
paid	O
a	O
price	O
in	O
terms	O
of	O
variance	B
that	O
was	O
not	O
offset	O
by	O
a	O
reduction	O
in	O
bias	B
qda	O
also	O
performed	O
worse	O
than	O
lda	O
since	O
it	O
fit	O
a	O
more	O
flexible	O
classifier	B
than	O
necessary	O
since	O
logistic	B
regression	B
assumes	O
a	O
linear	B
decision	B
boundary	I
its	O
results	O
were	O
only	O
slightly	O
inferior	O
to	O
those	O
of	O
lda	O
scenario	O
details	O
are	O
as	O
in	O
scenario	O
except	O
that	O
within	O
each	O
class	O
the	O
two	O
predictors	O
had	O
a	O
correlation	B
of	O
the	O
center	O
panel	O
of	O
figure	O
indicates	O
little	O
change	O
in	O
the	O
relative	O
performances	O
of	O
the	O
methods	O
as	O
compared	O
to	O
the	O
previous	O
scenario	O
scenario	O
we	O
generated	O
and	O
from	O
the	O
t-distribution	B
with	O
observations	B
per	O
class	O
the	O
t-distribution	B
has	O
a	O
similar	O
shape	O
to	O
the	O
normal	O
distribution	B
but	O
it	O
has	O
a	O
tendency	O
to	O
yield	O
more	O
extreme	O
points	O
that	O
is	O
more	O
points	O
that	O
are	O
far	O
from	O
the	O
mean	O
in	O
this	O
setting	O
the	O
decision	B
boundary	I
was	O
still	O
linear	B
and	O
so	O
fit	O
into	O
the	O
logistic	B
regression	B
framework	O
the	O
set-up	O
violated	O
the	O
assumptions	O
of	O
lda	O
since	O
the	O
observations	B
were	O
not	O
drawn	O
from	O
a	O
normal	O
distribution	B
the	O
right-hand	O
panel	O
of	O
figure	O
shows	O
that	O
logistic	B
regression	B
outperformed	O
lda	O
though	O
both	O
methods	O
were	O
superior	O
to	O
the	O
other	O
approaches	O
in	O
particular	O
the	O
qda	O
results	O
deteriorated	O
considerably	O
as	O
a	O
consequence	O
of	O
non-normality	O
scenario	O
the	O
data	B
were	O
generated	O
from	O
a	O
normal	O
distribution	B
with	O
a	O
correlation	B
of	O
between	O
the	O
predictors	O
in	O
the	O
first	O
class	O
and	O
correlation	B
of	O
between	O
the	O
predictors	O
in	O
the	O
second	O
class	O
this	O
setup	O
corresponded	O
to	O
the	O
qda	O
assumption	O
and	O
resulted	O
in	O
quadratic	B
decision	O
boundaries	O
the	O
left-hand	O
panel	O
of	O
figure	O
shows	O
that	O
qda	O
outperformed	O
all	O
of	O
the	O
other	O
approaches	O
scenario	O
within	O
each	O
class	O
the	O
observations	B
were	O
generated	O
from	O
a	O
normal	O
distribution	B
with	O
uncorrelated	O
predictors	O
however	O
the	O
responses	O
were	O
sampled	O
from	O
the	O
logistic	O
function	B
using	O
x	O
and	O
as	O
predictors	O
consequently	O
there	O
is	O
a	O
quadratic	B
decision	O
x	O
boundary	O
the	O
center	O
panel	O
of	O
figure	O
indicates	O
that	O
qda	O
once	O
again	O
performed	O
best	O
followed	O
closely	O
by	O
knn-cv	O
the	O
linear	B
methods	O
had	O
poor	O
performance	O
tdistribution	O
classification	B
scenario	O
details	O
are	O
as	O
in	O
the	O
previous	O
scenario	O
but	O
the	O
responses	O
were	O
sampled	O
from	O
a	O
more	O
complicated	O
non-linear	B
function	B
as	O
a	O
result	O
even	O
the	O
quadratic	B
decision	O
boundaries	O
of	O
qda	O
could	O
not	O
adequately	O
model	B
the	O
data	B
the	O
right-hand	O
panel	O
of	O
figure	O
shows	O
that	O
qda	O
gave	O
slightly	O
better	O
results	O
than	O
the	O
linear	B
methods	O
while	O
the	O
much	O
more	O
flexible	O
knn-cv	O
method	O
gave	O
the	O
best	O
results	O
but	O
knn	O
with	O
k	O
gave	O
the	O
worst	O
results	O
out	O
of	O
all	O
methods	O
this	O
highlights	O
the	O
fact	O
that	O
even	O
when	O
the	O
data	B
exhibits	O
a	O
complex	O
nonlinear	O
relationship	O
a	O
non-parametric	B
method	O
such	O
as	O
knn	O
can	O
still	O
give	O
poor	O
results	O
if	O
the	O
level	B
of	O
smoothness	O
is	O
not	O
chosen	O
correctly	O
these	O
six	O
examples	O
illustrate	O
that	O
no	O
one	O
method	O
will	O
dominate	O
the	O
others	O
in	O
every	O
situation	O
when	O
the	O
true	O
decision	O
boundaries	O
are	O
linear	B
then	O
the	O
lda	O
and	O
logistic	B
regression	B
approaches	O
will	O
tend	O
to	O
perform	O
well	O
when	O
the	O
boundaries	O
are	O
moderately	O
non-linear	B
qda	O
may	O
give	O
better	O
results	O
finally	O
for	O
much	O
more	O
complicated	O
decision	O
boundaries	O
a	O
non-parametric	B
approach	B
such	O
as	O
knn	O
can	O
be	O
superior	O
but	O
the	O
level	B
of	O
smoothness	O
for	O
a	O
non-parametric	B
approach	B
must	O
be	O
chosen	O
carefully	O
in	O
the	O
next	O
chapter	O
we	O
examine	O
a	O
number	O
of	O
approaches	O
for	O
choosing	O
the	O
correct	O
level	B
of	O
smoothness	O
and	O
in	O
general	O
for	O
selecting	O
the	O
best	O
overall	O
method	O
finally	O
recall	B
from	O
chapter	O
that	O
in	O
the	O
regression	B
setting	O
we	O
can	O
accommodate	O
a	O
non-linear	B
relationship	O
between	O
the	O
predictors	O
and	O
the	O
response	B
by	O
performing	O
regression	B
using	O
transformations	O
of	O
the	O
predictors	O
a	O
similar	O
approach	B
could	O
be	O
taken	O
in	O
the	O
classification	B
setting	O
for	O
instance	O
we	O
could	O
create	O
a	O
more	O
flexible	O
version	O
of	O
logistic	B
regression	B
by	O
including	O
x	O
x	O
and	O
even	O
x	O
as	O
predictors	O
this	O
may	O
or	O
may	O
not	O
improve	O
logistic	B
regression	B
s	O
performance	O
depending	O
on	O
whether	O
the	O
increase	O
in	O
variance	B
due	O
to	O
the	O
added	O
flexibility	O
is	O
offset	O
by	O
a	O
sufficiently	O
large	O
reduction	O
in	O
bias	B
we	O
could	O
do	O
the	O
same	O
for	O
lda	O
if	O
we	O
added	O
all	O
possible	O
quadratic	B
terms	O
and	O
cross-products	O
to	O
lda	O
the	O
form	O
of	O
the	O
model	B
would	O
be	O
the	O
same	O
as	O
the	O
qda	O
model	B
although	O
the	O
parameter	B
estimates	O
would	O
be	O
different	O
this	O
device	O
allows	O
us	O
to	O
move	O
somewhere	O
between	O
an	O
lda	O
and	O
a	O
qda	O
model	B
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
the	O
stock	O
market	O
data	B
we	O
will	O
begin	O
by	O
examining	O
some	O
numerical	O
and	O
graphical	O
summaries	O
of	O
the	O
smarket	B
data	B
which	O
is	O
part	O
of	O
the	O
islr	O
library	O
this	O
data	B
set	B
consists	O
of	O
percentage	O
returns	O
for	O
the	O
sp	O
stock	O
index	O
over	O
days	O
from	O
the	O
beginning	O
of	O
until	O
the	O
end	O
of	O
for	O
each	O
date	O
we	O
have	O
recorded	O
the	O
percentage	O
returns	O
for	O
each	O
of	O
the	O
five	O
previous	O
trading	O
days	O
through	O
we	O
have	O
also	O
recorded	O
volume	O
number	O
of	O
shares	O
traded	O
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
on	O
the	O
previous	O
day	O
in	O
billions	O
today	O
percentage	O
return	O
on	O
the	O
date	O
in	O
question	O
and	O
direction	O
the	O
market	O
was	O
up	O
or	O
down	O
on	O
this	O
date	O
library	O
islr	O
names	O
smarket	B
year	O
dim	O
smarket	B
summary	O
smarket	B
volume	O
today	O
direction	O
year	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
volume	O
today	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
min	O
st	O
qu	O
median	O
mean	O
rd	O
qu	O
max	O
pairs	O
smarket	B
direction	O
down	O
up	O
the	O
cor	O
function	B
produces	O
a	O
matrix	O
that	O
contains	O
all	O
of	O
the	O
pairwise	O
correlations	O
among	O
the	O
predictors	O
in	O
a	O
data	B
set	B
the	O
first	O
command	O
below	O
gives	O
an	O
error	B
message	O
because	O
the	O
direction	O
variable	B
is	O
qualitative	B
cor	O
smarket	B
error	B
in	O
cor	O
smarket	B
x	O
must	O
be	O
numeric	O
cor	O
smarket	B
year	O
volume	O
today	O
year	O
year	O
volume	O
today	O
classification	B
volume	O
today	O
as	O
one	O
would	O
expect	O
the	O
correlations	O
between	O
the	O
lag	O
variables	O
and	O
today	O
s	O
returns	O
are	O
close	O
to	O
zero	O
in	O
other	O
words	O
there	O
appears	O
to	O
be	O
little	O
correlation	B
between	O
today	O
s	O
returns	O
and	O
previous	O
days	O
returns	O
the	O
only	O
substantial	O
correlation	B
is	O
between	O
year	O
and	O
volume	O
by	O
plotting	O
the	O
data	B
we	O
see	O
that	O
volume	O
is	O
increasing	O
over	O
time	O
in	O
other	O
words	O
the	O
average	B
number	O
of	O
shares	O
traded	O
daily	O
increased	O
from	O
to	O
attach	O
smarket	B
plot	B
volume	O
logistic	B
regression	B
next	O
we	O
will	O
fit	O
a	O
logistic	B
regression	B
model	B
in	O
order	O
to	O
predict	O
direction	O
using	O
through	O
and	O
volume	O
the	O
glm	O
function	B
fits	O
generalized	O
linear	B
models	O
a	O
class	O
of	O
models	O
that	O
includes	O
logistic	B
regression	B
the	O
syntax	O
of	O
the	O
glm	O
function	B
is	O
similar	O
to	O
that	O
of	O
lm	O
except	O
that	O
we	O
must	O
pass	O
in	O
the	O
argument	B
familybinomial	O
in	O
order	O
to	O
tell	O
r	O
to	O
run	O
a	O
logistic	B
regression	B
rather	O
than	O
some	O
other	O
type	O
of	O
generalized	B
linear	B
model	B
glm	O
fit	O
s	O
glm	O
direction	O
volume	O
glm	O
generalized	B
linear	B
model	B
data	B
smarket	B
family	O
binomial	O
summary	O
glm	O
fit	O
s	O
call	O
glm	O
formula	O
direction	O
volume	O
family	O
binomial	O
data	B
smarket	B
deviance	B
residuals	B
min	O
q	O
median	O
q	O
max	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
intercept	B
volume	O
estimate	O
std	O
error	B
z	O
value	O
pr	O
z	O
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
dispersion	O
parameter	B
for	O
binomial	O
family	O
taken	O
to	O
be	O
null	B
deviance	B
residual	B
deviance	B
aic	O
on	O
on	O
degrees	B
of	I
freedom	I
degrees	B
of	I
freedom	I
number	O
of	O
fisher	O
scoring	O
iteration	O
s	O
the	O
smallest	O
p-value	B
here	O
is	O
associated	O
with	O
the	O
negative	O
coefficient	O
for	O
this	O
predictor	B
suggests	O
that	O
if	O
the	O
market	O
had	O
a	O
positive	O
return	O
yesterday	O
then	O
it	O
is	O
less	O
likely	O
to	O
go	O
up	O
today	O
however	O
at	O
a	O
value	O
of	O
the	O
p-value	B
is	O
still	O
relatively	O
large	O
and	O
so	O
there	O
is	O
no	O
clear	O
evidence	O
of	O
a	O
real	O
association	O
between	O
and	O
direction	O
we	O
use	O
the	O
coef	O
function	B
in	O
order	O
to	O
access	O
just	O
the	O
coefficients	O
for	O
this	O
fitted	O
model	B
we	O
can	O
also	O
use	O
the	O
summary	O
function	B
to	O
access	O
particular	O
aspects	O
of	O
the	O
fitted	O
model	B
such	O
as	O
the	O
p-values	O
for	O
the	O
coefficients	O
coef	O
glm	O
fit	O
s	O
intercept	B
volume	O
summary	O
glm	O
fit	O
s	O
estimate	O
std	O
error	B
z	O
value	O
pr	O
z	O
intercept	B
volume	O
summary	O
glm	O
fit	O
s	O
intercept	B
volume	O
the	O
predict	O
function	B
can	O
be	O
used	O
to	O
predict	O
the	O
probability	B
that	O
the	O
market	O
will	O
go	O
up	O
given	O
values	O
of	O
the	O
predictors	O
the	O
typeresponse	O
option	O
tells	O
r	O
to	O
output	B
probabilities	O
of	O
the	O
form	O
p	O
as	O
opposed	O
to	O
other	O
information	O
such	O
as	O
the	O
logit	B
if	O
no	O
data	B
set	B
is	O
supplied	O
to	O
the	O
predict	O
function	B
then	O
the	O
probabilities	O
are	O
computed	O
for	O
the	O
training	O
data	B
that	O
was	O
used	O
to	O
fit	O
the	O
logistic	B
regression	B
model	B
here	O
we	O
have	O
printed	O
only	O
the	O
first	O
ten	O
probabilities	O
we	O
know	O
that	O
these	O
values	O
correspond	O
to	O
the	O
probability	B
of	O
the	O
market	O
going	O
up	O
rather	O
than	O
down	O
because	O
the	O
contrasts	O
function	B
indicates	O
that	O
r	O
has	O
created	O
a	O
dummy	B
variable	B
with	O
a	O
for	O
up	O
glm	O
probs	O
predict	O
glm	O
fit	O
s	O
type	O
response	B
glm	O
probs	O
classification	B
contrasts	O
direction	O
up	O
down	O
up	O
in	O
order	O
to	O
make	O
a	O
prediction	B
as	O
to	O
whether	O
the	O
market	O
will	O
go	O
up	O
or	O
down	O
on	O
a	O
particular	O
day	O
we	O
must	O
convert	O
these	O
predicted	O
probabilities	O
into	O
class	O
labels	O
up	O
or	O
down	O
the	O
following	O
two	O
commands	O
create	O
a	O
vector	B
of	O
class	O
predictions	O
based	O
on	O
whether	O
the	O
predicted	O
probability	B
of	O
a	O
market	O
increase	O
is	O
greater	O
than	O
or	O
less	O
than	O
glm	O
pred	O
rep	O
down	O
glm	O
pred	O
glm	O
probs	O
up	O
the	O
first	O
command	O
creates	O
a	O
vector	B
of	O
down	O
elements	O
the	O
second	O
line	B
transforms	O
to	O
up	O
all	O
of	O
the	O
elements	O
for	O
which	O
the	O
predicted	O
probability	B
of	O
a	O
market	O
increase	O
exceeds	O
given	O
these	O
predictions	O
the	O
table	O
function	B
can	O
be	O
used	O
to	O
produce	O
a	O
confusion	B
matrix	I
in	O
order	O
to	O
determine	O
how	O
many	O
observations	B
were	O
correctly	O
or	O
incorrectly	O
classified	O
table	O
table	O
glm	O
pred	O
direction	O
glm	O
pred	O
down	O
direction	O
up	O
down	O
up	O
mean	O
glm	O
pred	O
direction	O
the	O
diagonal	O
elements	O
of	O
the	O
confusion	B
matrix	I
indicate	O
correct	O
predictions	O
while	O
the	O
off-diagonals	O
represent	O
incorrect	O
predictions	O
hence	O
our	O
model	B
correctly	O
predicted	O
that	O
the	O
market	O
would	O
go	O
up	O
on	O
days	O
and	O
that	O
it	O
would	O
go	O
down	O
on	O
days	O
for	O
a	O
total	O
of	O
correct	O
predictions	O
the	O
mean	O
function	B
can	O
be	O
used	O
to	O
compute	O
the	O
fraction	O
of	O
days	O
for	O
which	O
the	O
prediction	B
was	O
correct	O
in	O
this	O
case	O
logistic	B
regression	B
correctly	O
predicted	O
the	O
movement	O
of	O
the	O
market	O
of	O
the	O
time	O
at	O
first	O
glance	O
it	O
appears	O
that	O
the	O
logistic	B
regression	B
model	B
is	O
working	O
a	O
little	O
better	O
than	O
random	O
guessing	O
however	O
this	O
result	O
is	O
misleading	O
because	O
we	O
trained	O
and	O
tested	O
the	O
model	B
on	O
the	O
same	O
set	B
of	O
observations	B
in	O
other	O
words	O
is	O
the	O
training	O
error	B
rate	B
as	O
we	O
have	O
seen	O
previously	O
the	O
training	O
error	B
rate	B
is	O
often	O
overly	O
optimistic	O
it	O
tends	O
to	O
underestimate	O
the	O
test	O
error	B
rate	B
in	O
order	O
to	O
better	O
assess	O
the	O
accuracy	O
of	O
the	O
logistic	B
regression	B
model	B
in	O
this	O
setting	O
we	O
can	O
fit	O
the	O
model	B
using	O
part	O
of	O
the	O
data	B
and	O
then	O
examine	O
how	O
well	O
it	O
predicts	O
the	O
held	O
out	O
data	B
this	O
will	O
yield	O
a	O
more	O
realistic	O
error	B
rate	B
in	O
the	O
sense	O
that	O
in	O
practice	O
we	O
will	O
be	O
interested	O
in	O
our	O
model	B
s	O
performance	O
not	O
on	O
the	O
data	B
that	O
we	O
used	O
to	O
fit	O
the	O
model	B
but	O
rather	O
on	O
days	O
in	O
the	O
future	O
for	O
which	O
the	O
market	O
s	O
movements	O
are	O
unknown	O
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
to	O
implement	O
this	O
strategy	O
we	O
will	O
first	O
create	O
a	O
vector	B
corresponding	O
to	O
the	O
observations	B
from	O
through	O
we	O
will	O
then	O
use	O
this	O
vector	B
to	O
create	O
a	O
held	O
out	O
data	B
set	B
of	O
observations	B
from	O
train	B
year	O
smarket	B
smarket	B
train	B
dim	O
smarket	B
direction	O
direction	O
train	B
the	O
object	O
train	B
is	O
a	O
vector	B
of	O
elements	O
corresponding	O
to	O
the	O
observations	B
in	O
our	O
data	B
set	B
the	O
elements	O
of	O
the	O
vector	B
that	O
correspond	O
to	O
observations	B
that	O
occurred	O
before	O
are	O
set	B
to	O
true	O
whereas	O
those	O
that	O
correspond	O
to	O
observations	B
in	O
are	O
set	B
to	O
false	O
the	O
object	O
train	B
is	O
a	O
boolean	B
vector	B
since	O
its	O
elements	O
are	O
true	O
and	O
false	O
boolean	B
vectors	O
can	O
be	O
used	O
to	O
obtain	O
a	O
subset	O
of	O
the	O
rows	O
or	O
columns	O
of	O
a	O
matrix	O
for	O
instance	O
the	O
command	O
smarkettrain	O
would	O
pick	O
out	O
a	O
submatrix	O
of	O
the	O
stock	O
market	O
data	B
set	B
corresponding	O
only	O
to	O
the	O
dates	O
before	O
since	O
those	O
are	O
the	O
ones	O
for	O
which	O
the	O
elements	O
of	O
train	B
are	O
true	O
the	O
symbol	O
can	O
be	O
used	O
to	O
reverse	O
all	O
of	O
the	O
elements	O
of	O
a	O
boolean	B
vector	B
that	O
is	O
is	O
a	O
vector	B
similar	O
to	O
train	B
except	O
that	O
the	O
elements	O
that	O
are	O
true	O
in	O
train	B
get	O
swapped	O
to	O
false	O
in	O
and	O
the	O
elements	O
that	O
are	O
false	O
in	O
train	B
get	O
swapped	O
to	O
true	O
in	O
therefore	O
smarket	B
train	B
yields	O
a	O
submatrix	O
of	O
the	O
stock	O
market	O
data	B
containing	O
only	O
the	O
observations	B
for	O
which	O
train	B
is	O
false	O
that	O
is	O
the	O
observations	B
with	O
dates	O
in	O
the	O
output	B
above	O
indicates	O
that	O
there	O
are	O
such	O
observations	B
we	O
now	O
fit	O
a	O
logistic	B
regression	B
model	B
using	O
only	O
the	O
subset	O
of	O
the	O
observations	B
that	O
correspond	O
to	O
dates	O
before	O
using	O
the	O
subset	O
argument	B
we	O
then	O
obtain	O
predicted	O
probabilities	O
of	O
the	O
stock	O
market	O
going	O
up	O
for	O
each	O
of	O
the	O
days	O
in	O
our	O
test	O
set	B
that	O
is	O
for	O
the	O
days	O
in	O
glm	O
fit	O
s	O
glm	O
direction	O
volume	O
data	B
smarket	B
family	O
binomial	O
subset	O
train	B
glm	O
probs	O
predict	O
glm	O
fit	O
s	O
smarket	B
type	O
response	B
notice	O
that	O
we	O
have	O
trained	O
and	O
tested	O
our	O
model	B
on	O
two	O
completely	O
separate	O
data	B
sets	O
training	O
was	O
performed	O
using	O
only	O
the	O
dates	O
before	O
and	O
testing	O
was	O
performed	O
using	O
only	O
the	O
dates	O
in	O
finally	O
we	O
compute	O
the	O
predictions	O
for	O
and	O
compare	O
them	O
to	O
the	O
actual	O
movements	O
of	O
the	O
market	O
over	O
that	O
time	O
period	O
boolean	B
glm	O
pred	O
rep	O
down	O
glm	O
pred	O
glm	O
probs	O
up	O
table	O
glm	O
pred	O
direction	O
direction	O
glm	O
pred	O
down	O
up	O
down	O
up	O
mean	O
glm	O
pred	O
direction	O
classification	B
mean	O
glm	O
pred	O
direction	O
the	O
notation	O
means	O
not	O
equal	O
to	O
and	O
so	O
the	O
last	O
command	O
computes	O
the	O
test	O
set	B
error	B
rate	B
the	O
results	O
are	O
rather	O
disappointing	O
the	O
test	O
error	B
rate	B
is	O
which	O
is	O
worse	O
than	O
random	O
guessing	O
of	O
course	O
this	O
result	O
is	O
not	O
all	O
that	O
surprising	O
given	O
that	O
one	O
would	O
not	O
generally	O
expect	O
to	O
be	O
able	O
to	O
use	O
previous	O
days	O
returns	O
to	O
predict	O
future	O
market	O
performance	O
all	O
if	O
it	O
were	O
possible	O
to	O
do	O
so	O
then	O
the	O
authors	O
of	O
this	O
book	O
would	O
be	O
out	O
striking	O
it	O
rich	O
rather	O
than	O
writing	O
a	O
statistics	O
textbook	O
we	O
recall	B
that	O
the	O
logistic	B
regression	B
model	B
had	O
very	O
underwhelming	O
pvalues	O
associated	O
with	O
all	O
of	O
the	O
predictors	O
and	O
that	O
the	O
smallest	O
p-value	B
though	O
not	O
very	O
small	O
corresponded	O
to	O
perhaps	O
by	O
removing	O
the	O
variables	O
that	O
appear	O
not	O
to	O
be	O
helpful	O
in	O
predicting	O
direction	O
we	O
can	O
obtain	O
a	O
more	O
effective	O
model	B
after	O
all	O
using	O
predictors	O
that	O
have	O
no	O
relationship	O
with	O
the	O
response	B
tends	O
to	O
cause	O
a	O
deterioration	O
in	O
the	O
test	O
error	B
rate	B
such	O
predictors	O
cause	O
an	O
increase	O
in	O
variance	B
without	O
a	O
corresponding	O
decrease	O
in	O
bias	B
and	O
so	O
removing	O
such	O
predictors	O
may	O
in	O
turn	O
yield	O
an	O
improvement	O
below	O
we	O
have	O
refit	O
the	O
logistic	B
regression	B
using	O
just	O
and	O
which	O
seemed	O
to	O
have	O
the	O
highest	O
predictive	O
power	B
in	O
the	O
original	O
logistic	B
regression	B
model	B
glm	O
fit	O
s	O
glm	O
direction	O
data	B
smarket	B
family	O
binomial	O
subset	O
train	B
glm	O
probs	O
predict	O
glm	O
fit	O
s	O
smarket	B
type	O
response	B
glm	O
pred	O
rep	O
down	O
glm	O
pred	O
glm	O
probs	O
up	O
table	O
glm	O
pred	O
direction	O
direction	O
up	O
glm	O
pred	O
down	O
down	O
up	O
mean	O
glm	O
pred	O
direction	O
now	O
the	O
results	O
appear	O
to	O
be	O
a	O
little	O
better	O
of	O
the	O
daily	O
movements	O
have	O
been	O
correctly	O
predicted	O
it	O
is	O
worth	O
noting	O
that	O
in	O
this	O
case	O
a	O
much	O
simpler	O
strategy	O
of	O
predicting	O
that	O
the	O
market	O
will	O
increase	O
every	O
day	O
will	O
also	O
be	O
correct	O
of	O
the	O
time	O
hence	O
in	O
terms	O
of	O
overall	O
error	B
rate	B
the	O
logistic	B
regression	B
method	O
is	O
no	O
better	O
than	O
the	O
na	O
ve	O
approach	B
however	O
the	O
confusion	B
matrix	I
shows	O
that	O
on	O
days	O
when	O
logistic	B
regression	B
predicts	O
an	O
increase	O
in	O
the	O
market	O
it	O
has	O
a	O
accuracy	O
rate	B
this	O
suggests	O
a	O
possible	O
trading	O
strategy	O
of	O
buying	O
on	O
days	O
when	O
the	O
model	B
predicts	O
an	O
increasing	O
market	O
and	O
avoiding	O
trades	O
on	O
days	O
when	O
a	O
decrease	O
is	O
predicted	O
of	O
course	O
one	O
would	O
need	O
to	O
investigate	O
more	O
carefully	O
whether	O
this	O
small	O
improvement	O
was	O
real	O
or	O
just	O
due	O
to	O
random	O
chance	O
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
suppose	O
that	O
we	O
want	O
to	O
predict	O
the	O
returns	O
associated	O
with	O
particular	O
values	O
of	O
and	O
in	O
particular	O
we	O
want	O
to	O
predict	O
direction	O
on	O
a	O
day	O
when	O
and	O
equal	O
and	O
respectively	O
and	O
on	O
a	O
day	O
when	O
they	O
equal	O
and	O
we	O
do	O
this	O
using	O
the	O
predict	O
function	B
predict	O
glm	O
fit	O
s	O
newdata	O
data	B
frame	I
c	O
c	O
type	O
response	B
linear	B
discriminant	I
analysis	B
now	O
we	O
will	O
perform	O
lda	O
on	O
the	O
smarket	B
data	B
in	O
r	O
we	O
fit	O
an	O
lda	O
model	B
using	O
the	O
lda	O
function	B
which	O
is	O
part	O
of	O
the	O
mass	O
library	O
notice	O
that	O
the	O
syntax	O
for	O
the	O
lda	O
function	B
is	O
identical	O
to	O
that	O
of	O
lm	O
and	O
to	O
that	O
of	O
glm	O
except	O
for	O
the	O
absence	O
of	O
the	O
family	O
option	O
we	O
fit	O
the	O
model	B
using	O
only	O
the	O
observations	B
before	O
lda	O
library	O
mass	O
lda	O
fit	O
lda	O
direction	O
data	B
smarket	B
subset	O
train	B
lda	O
fit	O
call	O
lda	O
direction	O
data	B
smarket	B
subset	O
train	B
prior	O
p	O
r	O
o	O
b	O
a	O
b	O
i	O
l	O
i	O
t	O
i	O
e	O
s	O
of	O
groups	O
down	O
up	O
group	O
means	O
down	O
up	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
of	O
linear	B
d	O
i	O
s	O
c	O
r	O
i	O
m	O
i	O
n	O
a	O
n	O
t	O
s	O
plot	B
lda	O
fit	O
the	O
lda	O
output	B
indicates	O
that	O
and	O
in	O
other	O
words	O
of	O
the	O
training	O
observations	B
correspond	O
to	O
days	O
during	O
which	O
the	O
market	O
went	O
down	O
it	O
also	O
provides	O
the	O
group	O
means	O
these	O
are	O
the	O
average	B
of	O
each	O
predictor	B
within	O
each	O
class	O
and	O
are	O
used	O
by	O
lda	O
as	O
estimates	O
of	O
k	O
these	O
suggest	O
that	O
there	O
is	O
a	O
tendency	O
for	O
the	O
previous	O
days	O
returns	O
to	O
be	O
negative	O
on	O
days	O
when	O
the	O
market	O
increases	O
and	O
a	O
tendency	O
for	O
the	O
previous	O
days	O
returns	O
to	O
be	O
positive	O
on	O
days	O
when	O
the	O
market	O
declines	O
the	O
coefficients	O
of	O
linear	B
discriminants	O
output	B
provides	O
the	O
linear	B
combination	I
of	O
and	O
that	O
are	O
used	O
to	O
form	O
the	O
lda	O
decision	O
rule	O
in	O
other	O
words	O
these	O
are	O
the	O
multipliers	O
of	O
the	O
elements	O
of	O
x	O
x	O
in	O
if	O
is	O
large	O
then	O
the	O
lda	O
classifier	B
will	O
classification	B
predict	O
a	O
market	O
increase	O
and	O
if	O
it	O
is	O
small	O
then	O
the	O
lda	O
classifier	B
will	O
predict	O
a	O
market	O
decline	O
the	O
plot	B
function	B
produces	O
plots	O
of	O
the	O
linear	B
discriminants	O
obtained	O
by	O
computing	O
for	O
each	O
of	O
the	O
training	O
observations	B
the	O
predict	O
function	B
returns	O
a	O
list	O
with	O
three	O
elements	O
the	O
first	O
element	O
class	O
contains	O
lda	O
s	O
predictions	O
about	O
the	O
movement	O
of	O
the	O
market	O
the	O
second	O
element	O
posterior	O
is	O
a	O
matrix	O
whose	O
kth	O
column	O
contains	O
the	O
posterior	O
probability	B
that	O
the	O
corresponding	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
computed	O
from	O
finally	O
x	O
contains	O
the	O
linear	B
discriminants	O
described	O
earlier	O
lda	O
pred	O
predict	O
lda	O
fit	O
smarket	B
names	O
lda	O
pred	O
class	O
posterior	O
x	O
as	O
we	O
observed	O
in	O
section	O
the	O
lda	O
and	O
logistic	B
regression	B
predictions	O
are	O
almost	O
identical	O
lda	O
class	O
lda	O
predclass	O
table	O
lda	O
class	O
direction	O
direction	O
up	O
lda	O
pred	O
down	O
down	O
up	O
mean	O
lda	O
class	O
direction	O
applying	O
a	O
threshold	O
to	O
the	O
posterior	O
probabilities	O
allows	O
us	O
to	O
recreate	O
the	O
predictions	O
contained	O
in	O
lda	O
predclass	O
sum	O
lda	O
p	O
r	O
e	O
d	O
p	O
o	O
s	O
t	O
e	O
r	O
i	O
o	O
r	O
sum	O
lda	O
p	O
r	O
e	O
d	O
p	O
o	O
s	O
t	O
e	O
r	O
i	O
o	O
r	O
notice	O
that	O
the	O
posterior	O
probability	B
output	B
by	O
the	O
model	B
corresponds	O
to	O
the	O
probability	B
that	O
the	O
market	O
will	O
decrease	O
lda	O
p	O
r	O
e	O
d	O
p	O
o	O
s	O
t	O
e	O
r	O
i	O
o	O
r	O
lda	O
class	O
if	O
we	O
wanted	O
to	O
use	O
a	O
posterior	O
probability	B
threshold	O
other	O
than	O
in	O
order	O
to	O
make	O
predictions	O
then	O
we	O
could	O
easily	O
do	O
so	O
for	O
instance	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
a	O
market	O
decrease	O
only	O
if	O
we	O
are	O
very	O
certain	O
that	O
the	O
market	O
will	O
indeed	O
decrease	O
on	O
that	O
day	O
say	O
if	O
the	O
posterior	O
probability	B
is	O
at	O
least	O
sum	O
lda	O
p	O
r	O
e	O
d	O
p	O
o	O
s	O
t	O
e	O
r	O
i	O
o	O
r	O
no	O
days	O
in	O
meet	O
that	O
threshold	O
in	O
fact	O
the	O
greatest	O
posterior	O
probability	B
of	O
decrease	O
in	O
all	O
of	O
was	O
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
quadratic	B
discriminant	O
analysis	B
we	O
will	O
now	O
fit	O
a	O
qda	O
model	B
to	O
the	O
smarket	B
data	B
qda	O
is	O
implemented	O
in	O
r	O
using	O
the	O
qda	O
function	B
which	O
is	O
also	O
part	O
of	O
the	O
mass	O
library	O
the	O
syntax	O
is	O
identical	O
to	O
that	O
of	O
lda	O
qda	O
fit	O
qda	O
direction	O
data	B
smarket	B
subset	O
train	B
qda	O
qda	O
fit	O
call	O
qda	O
direction	O
data	B
smarket	B
subset	O
train	B
prior	O
p	O
r	O
o	O
b	O
a	O
b	O
i	O
l	O
i	O
t	O
i	O
e	O
s	O
of	O
groups	O
down	O
up	O
group	O
means	O
down	O
up	O
the	O
output	B
contains	O
the	O
group	O
means	O
but	O
it	O
does	O
not	O
contain	O
the	O
coefficients	O
of	O
the	O
linear	B
discriminants	O
because	O
the	O
qda	O
classifier	B
involves	O
a	O
quadratic	B
rather	O
than	O
a	O
linear	B
function	B
of	O
the	O
predictors	O
the	O
predict	O
function	B
works	O
in	O
exactly	O
the	O
same	O
fashion	O
as	O
for	O
lda	O
qda	O
class	O
predict	O
qda	O
fit	O
smarket	B
table	O
qda	O
class	O
direction	O
direction	O
qda	O
class	O
down	O
up	O
down	O
up	O
mean	O
qda	O
class	O
direction	O
interestingly	O
the	O
qda	O
predictions	O
are	O
accurate	O
almost	O
of	O
the	O
time	O
even	O
though	O
the	O
data	B
was	O
not	O
used	O
to	O
fit	O
the	O
model	B
this	O
level	B
of	O
accuracy	O
is	O
quite	O
impressive	O
for	O
stock	O
market	O
data	B
which	O
is	O
known	O
to	O
be	O
quite	O
hard	O
to	O
model	B
accurately	O
this	O
suggests	O
that	O
the	O
quadratic	B
form	O
assumed	O
by	O
qda	O
may	O
capture	O
the	O
true	O
relationship	O
more	O
accurately	O
than	O
the	O
linear	B
forms	O
assumed	O
by	O
lda	O
and	O
logistic	B
regression	B
however	O
we	O
recommend	O
evaluating	O
this	O
method	O
s	O
performance	O
on	O
a	O
larger	O
test	O
set	B
before	O
betting	O
that	O
this	O
approach	B
will	O
consistently	O
beat	O
the	O
market	O
k-nearest	O
neighbors	O
we	O
will	O
now	O
perform	O
knn	O
using	O
the	O
knn	O
function	B
which	O
is	O
part	O
of	O
the	O
class	O
library	O
this	O
function	B
works	O
rather	O
differently	O
from	O
the	O
other	O
modelfitting	O
functions	O
that	O
we	O
have	O
encountered	O
thus	O
far	O
rather	O
than	O
a	O
two-step	O
approach	B
in	O
which	O
we	O
first	O
fit	O
the	O
model	B
and	O
then	O
we	O
use	O
the	O
model	B
to	O
make	O
predictions	O
knn	O
forms	O
predictions	O
using	O
a	O
single	B
command	O
the	O
function	B
requires	O
four	O
inputs	O
knn	O
classification	B
a	O
matrix	O
containing	O
the	O
predictors	O
associated	O
with	O
the	O
training	O
data	B
labeled	O
train	B
x	O
below	O
a	O
matrix	O
containing	O
the	O
predictors	O
associated	O
with	O
the	O
data	B
for	O
which	O
we	O
wish	O
to	O
make	O
predictions	O
labeled	O
test	O
x	O
below	O
a	O
vector	B
containing	O
the	O
class	O
labels	O
for	O
the	O
training	O
observations	B
labeled	O
train	B
direction	O
below	O
a	O
value	O
for	O
k	O
the	O
number	O
of	O
nearest	O
neighbors	O
to	O
be	O
used	O
by	O
the	O
classifier	B
we	O
use	O
the	O
cbind	O
function	B
short	O
for	O
column	O
bind	O
to	O
bind	O
the	O
and	O
variables	O
together	O
into	O
two	O
matrices	O
one	O
for	O
the	O
training	O
set	B
and	O
the	O
other	O
for	O
the	O
test	O
set	B
cbind	O
library	O
class	O
train	B
x	O
cbind	O
train	B
test	O
x	O
cbind	O
train	B
train	B
direction	O
direction	O
train	B
now	O
the	O
knn	O
function	B
can	O
be	O
used	O
to	O
predict	O
the	O
market	O
s	O
movement	O
for	O
the	O
dates	O
in	O
we	O
set	B
a	O
random	O
seed	B
before	O
we	O
apply	O
knn	O
because	O
if	O
several	O
observations	B
are	O
tied	O
as	O
nearest	O
neighbors	O
then	O
r	O
will	O
randomly	O
break	O
the	O
tie	O
therefore	O
a	O
seed	B
must	O
be	O
set	B
in	O
order	O
to	O
ensure	O
reproducibility	O
of	O
results	O
set	B
seed	B
knn	O
pred	O
knn	O
train	B
test	O
train	B
direction	O
k	O
table	O
knn	O
pred	O
direction	O
direction	O
knn	O
pred	O
down	O
up	O
down	O
up	O
the	O
results	O
using	O
k	O
are	O
not	O
very	O
good	O
since	O
only	O
of	O
the	O
observations	B
are	O
correctly	O
predicted	O
of	O
course	O
it	O
may	O
be	O
that	O
k	O
results	O
in	O
an	O
overly	O
flexible	O
fit	O
to	O
the	O
data	B
below	O
we	O
repeat	O
the	O
analysis	B
using	O
k	O
knn	O
pred	O
knn	O
train	B
test	O
train	B
direction	O
k	O
table	O
knn	O
pred	O
direction	O
direction	O
knn	O
pred	O
down	O
up	O
down	O
up	O
mean	O
knn	O
pred	O
direction	O
the	O
results	O
have	O
improved	O
slightly	O
but	O
increasing	O
k	O
further	O
turns	O
out	O
to	O
provide	O
no	O
further	O
improvements	O
it	O
appears	O
that	O
for	O
this	O
data	B
qda	O
provides	O
the	O
best	O
results	O
of	O
the	O
methods	O
that	O
we	O
have	O
examined	O
so	O
far	O
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
an	O
application	O
to	O
caravan	B
insurance	O
data	B
finally	O
we	O
will	O
apply	O
the	O
knn	O
approach	B
to	O
the	O
caravan	B
data	B
set	B
which	O
is	O
part	O
of	O
the	O
islr	O
library	O
this	O
data	B
set	B
includes	O
predictors	O
that	O
measure	O
demographic	O
characteristics	O
for	O
individuals	O
the	O
response	B
variable	B
is	O
purchase	O
which	O
indicates	O
whether	O
or	O
not	O
a	O
given	O
individual	O
purchases	O
a	O
caravan	B
insurance	O
policy	O
in	O
this	O
data	B
set	B
only	O
of	O
people	O
purchased	O
caravan	B
insurance	O
dim	O
caravan	B
attach	O
caravan	B
summary	O
purchase	O
yes	O
no	O
because	O
the	O
knn	O
classifier	B
predicts	O
the	O
class	O
of	O
a	O
given	O
test	O
observation	O
by	O
identifying	O
the	O
observations	B
that	O
are	O
nearest	O
to	O
it	O
the	O
scale	O
of	O
the	O
variables	O
matters	O
any	O
variables	O
that	O
are	O
on	O
a	O
large	O
scale	O
will	O
have	O
a	O
much	O
larger	O
effect	O
on	O
the	O
distance	O
between	O
the	O
observations	B
and	O
hence	O
on	O
the	O
knn	O
classifier	B
than	O
variables	O
that	O
are	O
on	O
a	O
small	O
scale	O
for	O
instance	O
imagine	O
a	O
data	B
set	B
that	O
contains	O
two	O
variables	O
salary	O
and	O
age	O
in	O
dollars	O
and	O
years	O
respectively	O
as	O
far	O
as	O
knn	O
is	O
concerned	O
a	O
difference	O
of	O
in	O
salary	O
is	O
enormous	O
compared	O
to	O
a	O
difference	O
of	O
years	O
in	O
age	O
consequently	O
salary	O
will	O
drive	O
the	O
knn	O
classification	B
results	O
and	O
age	O
will	O
have	O
almost	O
no	O
effect	O
this	O
is	O
contrary	O
to	O
our	O
intuition	O
that	O
a	O
salary	O
difference	O
of	O
is	O
quite	O
small	O
compared	O
to	O
an	O
age	O
difference	O
of	O
years	O
furthermore	O
the	O
importance	B
of	O
scale	O
to	O
the	O
knn	O
classifier	B
leads	O
to	O
another	O
issue	O
if	O
we	O
measured	O
salary	O
in	O
japanese	O
yen	O
or	O
if	O
we	O
measured	O
age	O
in	O
minutes	O
then	O
we	O
d	O
get	O
quite	O
different	O
classification	B
results	O
from	O
what	O
we	O
get	O
if	O
these	O
two	O
variables	O
are	O
measured	O
in	O
dollars	O
and	O
years	O
a	O
good	O
way	O
to	O
handle	O
this	O
problem	O
is	O
to	O
standardize	B
the	O
data	B
so	O
that	O
all	O
variables	O
are	O
given	O
a	O
mean	O
of	O
zero	O
and	O
a	O
standard	O
deviation	O
of	O
one	O
then	O
all	O
variables	O
will	O
be	O
on	O
a	O
comparable	O
scale	O
the	O
scale	O
function	B
does	O
just	O
this	O
in	O
standardizing	O
the	O
data	B
we	O
exclude	O
column	O
because	O
that	O
is	O
the	O
qualitative	B
purchase	O
variable	B
standardize	B
scale	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
x	O
scale	O
caravan	B
var	O
caravan	B
var	O
caravan	B
var	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
x	O
var	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
x	O
now	O
every	O
column	O
of	O
standardized	O
x	O
has	O
a	O
standard	O
deviation	O
of	O
one	O
and	O
a	O
mean	O
of	O
zero	O
classification	B
we	O
now	O
split	O
the	O
observations	B
into	O
a	O
test	O
set	B
containing	O
the	O
first	O
observations	B
and	O
a	O
training	O
set	B
containing	O
the	O
remaining	O
observations	B
we	O
fit	O
a	O
knn	O
model	B
on	O
the	O
training	O
data	B
using	O
k	O
and	O
evaluate	O
its	O
performance	O
on	O
the	O
test	O
data	B
test	O
train	B
x	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
x	O
test	O
test	O
x	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
x	O
test	O
train	B
y	O
purchase	O
test	O
test	O
y	O
purchase	O
test	O
set	B
seed	B
knn	O
pred	O
knn	O
train	B
test	O
train	B
k	O
mean	O
test	O
y	O
knn	O
pred	O
mean	O
test	O
y	O
no	O
the	O
vector	B
test	O
is	O
numeric	O
with	O
values	O
from	O
through	O
typing	O
standardized	O
xtest	O
yields	O
the	O
submatrix	O
of	O
the	O
data	B
containing	O
the	O
observations	B
whose	O
indices	O
range	O
from	O
to	O
whereas	O
typing	O
standardized	O
x-test	O
yields	O
the	O
submatrix	O
containing	O
the	O
observations	B
whose	O
indices	O
do	O
not	O
range	O
from	O
to	O
the	O
knn	O
error	B
rate	B
on	O
the	O
test	O
observations	B
is	O
just	O
under	O
at	O
first	O
glance	O
this	O
may	O
appear	O
to	O
be	O
fairly	O
good	O
however	O
since	O
only	O
of	O
customers	O
purchased	O
insurance	O
we	O
could	O
get	O
the	O
error	B
rate	B
down	O
to	O
by	O
always	O
predicting	O
no	O
regardless	O
of	O
the	O
values	O
of	O
the	O
predictors	O
suppose	O
that	O
there	O
is	O
some	O
non-trivial	O
cost	O
to	O
trying	O
to	O
sell	O
insurance	O
to	O
a	O
given	O
individual	O
for	O
instance	O
perhaps	O
a	O
salesperson	O
must	O
visit	O
each	O
potential	O
customer	O
if	O
the	O
company	O
tries	O
to	O
sell	O
insurance	O
to	O
a	O
random	O
selection	B
of	O
customers	O
then	O
the	O
success	O
rate	B
will	O
be	O
only	O
which	O
may	O
be	O
far	O
too	O
low	O
given	O
the	O
costs	O
involved	O
instead	O
the	O
company	O
would	O
like	O
to	O
try	O
to	O
sell	O
insurance	O
only	O
to	O
customers	O
who	O
are	O
likely	O
to	O
buy	O
it	O
so	O
the	O
overall	O
error	B
rate	B
is	O
not	O
of	O
interest	O
instead	O
the	O
fraction	O
of	O
individuals	O
that	O
are	O
correctly	O
predicted	O
to	O
buy	O
insurance	O
is	O
of	O
interest	O
it	O
turns	O
out	O
that	O
knn	O
with	O
k	O
does	O
far	O
better	O
than	O
random	O
guessing	O
among	O
the	O
customers	O
that	O
are	O
predicted	O
to	O
buy	O
insurance	O
among	O
such	O
customers	O
or	O
actually	O
do	O
purchase	O
insurance	O
this	O
is	O
double	O
the	O
rate	B
that	O
one	O
would	O
obtain	O
from	O
random	O
guessing	O
table	O
knn	O
pred	O
test	O
y	O
knn	O
pred	O
no	O
yes	O
test	O
y	O
no	O
yes	O
using	O
k	O
the	O
success	O
rate	B
increases	O
to	O
and	O
with	O
k	O
the	O
rate	B
is	O
this	O
is	O
over	O
four	O
times	O
the	O
rate	B
that	O
results	O
from	O
random	O
guessing	O
it	O
appears	O
that	O
knn	O
is	O
finding	O
some	O
real	O
patterns	O
in	O
a	O
difficult	O
data	B
set	B
lab	O
logistic	B
regression	B
lda	O
qda	O
and	O
knn	O
knn	O
pred	O
knn	O
train	B
test	O
train	B
k	O
table	O
knn	O
pred	O
test	O
y	O
knn	O
pred	O
no	O
yes	O
test	O
y	O
no	O
yes	O
knn	O
pred	O
knn	O
train	B
test	O
train	B
k	O
table	O
knn	O
pred	O
test	O
y	O
knn	O
pred	O
no	O
yes	O
test	O
y	O
no	O
yes	O
as	O
a	O
comparison	O
we	O
can	O
also	O
fit	O
a	O
logistic	B
regression	B
model	B
to	O
the	O
data	B
if	O
we	O
use	O
as	O
the	O
predicted	O
probability	B
cut-off	O
for	O
the	O
classifier	B
then	O
we	O
have	O
a	O
problem	O
only	O
seven	O
of	O
the	O
test	O
observations	B
are	O
predicted	O
to	O
purchase	O
insurance	O
even	O
worse	O
we	O
are	O
wrong	O
about	O
all	O
of	O
these	O
however	O
we	O
are	O
not	O
required	O
to	O
use	O
a	O
cut-off	O
of	O
if	O
we	O
instead	O
predict	O
a	O
purchase	O
any	O
time	O
the	O
predicted	O
probability	B
of	O
purchase	O
exceeds	O
we	O
get	O
much	O
better	O
results	O
we	O
predict	O
that	O
people	O
will	O
purchase	O
insurance	O
and	O
we	O
are	O
correct	O
for	O
about	O
of	O
these	O
people	O
this	O
is	O
over	O
five	O
times	O
better	O
than	O
random	O
guessing	O
glm	O
fit	O
s	O
glm	O
purchase	O
data	B
caravan	B
family	O
binomial	O
subset	O
test	O
fitted	O
p	O
r	O
o	O
b	O
a	O
b	O
i	O
l	O
i	O
t	O
i	O
e	O
s	O
numerical	O
l	O
y	O
or	O
occurred	O
caravan	B
test	O
type	O
response	B
warning	O
message	O
g	O
lm	O
fit	O
s	O
glm	O
probs	O
predict	O
glm	O
pred	O
rep	O
no	O
glm	O
pred	O
glm	O
probs	O
yes	O
table	O
glm	O
pred	O
test	O
y	O
g	O
lm	O
fit	O
s	O
glm	O
pred	O
no	O
yes	O
test	O
y	O
no	O
yes	O
glm	O
pred	O
rep	O
no	O
glm	O
pred	O
glm	O
probs	O
yes	O
table	O
glm	O
pred	O
test	O
y	O
test	O
y	O
no	O
yes	O
glm	O
pred	O
no	O
yes	O
classification	B
exercises	O
conceptual	O
using	O
a	O
little	O
bit	O
of	O
algebra	O
prove	O
that	O
is	O
equivalent	O
to	O
in	O
other	O
words	O
the	O
logistic	O
function	B
representation	O
and	O
logit	B
representation	O
for	O
the	O
logistic	B
regression	B
model	B
are	O
equivalent	O
it	O
was	O
stated	O
in	O
the	O
text	O
that	O
classifying	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
is	O
largest	O
is	O
equivalent	O
to	O
classifying	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
is	O
largest	O
prove	O
that	O
this	O
is	O
the	O
case	O
in	O
other	O
words	O
under	O
the	O
assumption	O
that	O
the	O
observations	B
in	O
the	O
kth	O
class	O
are	O
drawn	O
from	O
a	O
n	O
k	O
distribution	B
the	O
bayes	O
classifier	B
assigns	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
the	O
discriminant	B
function	B
is	O
maximized	O
this	O
problem	O
relates	O
to	O
the	O
qda	O
model	B
in	O
which	O
the	O
observations	B
within	O
each	O
class	O
are	O
drawn	O
from	O
a	O
normal	O
distribution	B
with	O
a	O
classspecific	O
mean	O
vector	B
and	O
a	O
class	O
specific	O
covariance	O
matrix	O
we	O
consider	O
the	O
simple	B
case	O
where	O
p	O
i	O
e	O
there	O
is	O
only	O
one	O
feature	B
tribution	O
x	O
n	O
k	O
suppose	O
that	O
we	O
have	O
k	O
classes	O
and	O
that	O
if	O
an	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
then	O
x	O
comes	O
from	O
a	O
one-dimensional	O
normal	O
disk	O
recall	B
that	O
the	O
density	B
function	B
for	O
the	O
one-dimensional	O
normal	O
distribution	B
is	O
given	O
in	O
prove	O
that	O
in	O
this	O
case	O
the	O
bayes	O
classifier	B
is	O
not	O
linear	B
argue	O
that	O
it	O
is	O
in	O
fact	O
quadratic	B
hint	O
for	O
this	O
problem	O
you	O
should	O
follow	O
the	O
arguments	O
laid	O
out	O
in	O
section	O
but	O
without	O
making	O
the	O
assumption	O
that	O
k	O
when	O
the	O
number	O
of	O
features	O
p	O
is	O
large	O
there	O
tends	O
to	O
be	O
a	O
deterioration	O
in	O
the	O
performance	O
of	O
knn	O
and	O
other	O
local	B
approaches	O
that	O
perform	O
prediction	B
using	O
only	O
observations	B
that	O
are	O
near	O
the	O
test	O
observation	O
for	O
which	O
a	O
prediction	B
must	O
be	O
made	O
this	O
phenomenon	O
is	O
known	O
as	O
the	O
curse	B
of	I
dimensionality	I
and	O
it	O
ties	O
into	O
the	O
fact	O
that	O
non-parametric	B
approaches	O
often	O
perform	O
poorly	O
when	O
p	O
is	O
large	O
we	O
will	O
now	O
investigate	O
this	O
curse	O
curse	B
of	I
dimensionality	I
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
observations	B
each	O
with	O
measurements	O
on	O
p	O
feature	B
x	O
we	O
assume	O
that	O
x	O
is	O
uniformly	O
distributed	O
on	O
associated	O
with	O
each	O
observation	O
is	O
a	O
response	B
value	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
a	O
test	O
observation	O
s	O
response	B
using	O
only	O
observations	B
that	O
are	O
within	O
of	O
the	O
range	O
of	O
x	O
closest	O
to	O
that	O
test	O
observation	O
for	O
instance	O
in	O
order	O
to	O
predict	O
the	O
response	B
for	O
a	O
test	O
observation	O
with	O
x	O
exercises	O
we	O
will	O
use	O
observations	B
in	O
the	O
range	O
on	O
average	B
what	O
fraction	O
of	O
the	O
available	O
observations	B
will	O
we	O
use	O
to	O
make	O
the	O
prediction	B
now	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
observations	B
each	O
with	O
measurements	O
on	O
p	O
features	O
and	O
we	O
assume	O
that	O
are	O
uniformly	O
distributed	O
on	O
we	O
wish	O
to	O
predict	O
a	O
test	O
observation	O
s	O
response	B
using	O
only	O
observations	B
that	O
are	O
within	O
of	O
the	O
range	O
of	O
and	O
within	O
of	O
the	O
range	O
of	O
closest	O
to	O
that	O
test	O
observation	O
for	O
instance	O
in	O
order	O
to	O
predict	O
the	O
response	B
for	O
a	O
test	O
observation	O
with	O
and	O
we	O
will	O
use	O
observations	B
in	O
the	O
range	O
for	O
and	O
in	O
the	O
range	O
for	O
on	O
average	B
what	O
fraction	O
of	O
the	O
available	O
observations	B
will	O
we	O
use	O
to	O
make	O
the	O
prediction	B
now	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
observations	B
on	O
p	O
features	O
again	O
the	O
observations	B
are	O
uniformly	O
distributed	O
on	O
each	O
feature	B
and	O
again	O
each	O
feature	B
ranges	O
in	O
value	O
from	O
to	O
we	O
wish	O
to	O
predict	O
a	O
test	O
observation	O
s	O
response	B
using	O
observations	B
within	O
the	O
of	O
each	O
feature	B
s	O
range	O
that	O
is	O
closest	O
to	O
that	O
test	O
observation	O
what	O
fraction	O
of	O
the	O
available	O
observations	B
will	O
we	O
use	O
to	O
make	O
the	O
prediction	B
using	O
your	O
answers	O
to	O
parts	O
argue	O
that	O
a	O
drawback	O
of	O
knn	O
when	O
p	O
is	O
large	O
is	O
that	O
there	O
are	O
very	O
few	O
training	O
observations	B
near	O
any	O
given	O
test	O
observation	O
now	O
suppose	O
that	O
we	O
wish	O
to	O
make	O
a	O
prediction	B
for	O
a	O
test	O
observation	O
by	O
creating	O
a	O
p-dimensional	O
hypercube	O
centered	O
around	O
the	O
test	O
observation	O
that	O
contains	O
on	O
average	B
of	O
the	O
training	O
observations	B
for	O
p	O
and	O
what	O
is	O
the	O
length	O
of	O
each	O
side	O
of	O
the	O
hypercube	O
comment	O
on	O
your	O
answer	O
note	O
a	O
hypercube	O
is	O
a	O
generalization	O
of	O
a	O
cube	O
to	O
an	O
arbitrary	O
number	O
of	O
dimensions	O
when	O
p	O
a	O
hypercube	O
is	O
simply	O
a	O
line	B
segment	O
when	O
p	O
it	O
is	O
a	O
square	O
and	O
when	O
p	O
it	O
is	O
a	O
cube	O
we	O
now	O
examine	O
the	O
differences	O
between	O
lda	O
and	O
qda	O
if	O
the	O
bayes	O
decision	B
boundary	I
is	O
linear	B
do	O
we	O
expect	O
lda	O
or	O
qda	O
to	O
perform	O
better	O
on	O
the	O
training	O
set	B
on	O
the	O
test	O
set	B
if	O
the	O
bayes	O
decision	B
boundary	I
is	O
non-linear	B
do	O
we	O
expect	O
lda	O
or	O
qda	O
to	O
perform	O
better	O
on	O
the	O
training	O
set	B
on	O
the	O
test	O
set	B
in	O
general	O
as	O
the	O
sample	O
size	O
n	O
increases	O
do	O
we	O
expect	O
the	O
test	O
prediction	B
accuracy	O
of	O
qda	O
relative	O
to	O
lda	O
to	O
improve	O
decline	O
or	O
be	O
unchanged	O
why	O
classification	B
true	O
or	O
false	O
even	O
if	O
the	O
bayes	O
decision	B
boundary	I
for	O
a	O
given	O
problem	O
is	O
linear	B
we	O
will	O
probably	O
achieve	O
a	O
superior	O
test	O
error	B
rate	B
using	O
qda	O
rather	O
than	O
lda	O
because	O
qda	O
is	O
flexible	O
enough	O
to	O
model	B
a	O
linear	B
decision	B
boundary	I
justify	O
your	O
answer	O
suppose	O
we	O
collect	O
data	B
for	O
a	O
group	O
of	O
students	O
in	O
a	O
statistics	O
class	O
with	O
variables	O
hours	O
studied	O
undergrad	O
gpa	O
and	O
y	O
receive	O
an	O
a	O
we	O
fit	O
a	O
logistic	B
regression	B
and	O
produce	O
estimated	O
coefficient	O
estimate	O
the	O
probability	B
that	O
a	O
student	O
who	O
studies	O
for	O
h	O
and	O
has	O
an	O
undergrad	O
gpa	O
of	O
gets	O
an	O
a	O
in	O
the	O
class	O
how	O
many	O
hours	O
would	O
the	O
student	O
in	O
part	O
need	O
to	O
study	O
to	O
have	O
a	O
chance	O
of	O
getting	O
an	O
a	O
in	O
the	O
class	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
whether	O
a	O
given	O
stock	O
will	O
issue	O
a	O
dividend	O
this	O
year	O
yes	O
or	O
no	O
based	O
on	O
x	O
last	O
year	O
s	O
percent	O
profit	O
we	O
examine	O
a	O
large	O
number	O
of	O
companies	O
and	O
discover	O
that	O
the	O
mean	O
value	O
of	O
x	O
for	O
companies	O
that	O
issued	O
a	O
dividend	O
was	O
x	O
while	O
the	O
mean	O
for	O
those	O
that	O
didn	O
t	O
was	O
x	O
in	O
addition	O
the	O
variance	B
of	O
x	O
for	O
these	O
two	O
sets	O
of	O
companies	O
was	O
finally	O
of	O
companies	O
issued	O
dividends	O
assuming	O
that	O
x	O
follows	O
a	O
normal	O
distribution	B
predict	O
the	O
probability	B
that	O
a	O
company	O
will	O
issue	O
a	O
dividend	O
this	O
year	O
given	O
that	O
its	O
percentage	O
profit	O
was	O
x	O
last	O
year	O
hint	O
recall	B
that	O
the	O
density	B
function	B
for	O
a	O
normal	O
random	O
variable	B
is	O
f	O
you	O
will	O
need	O
to	O
use	O
bayes	O
theorem	O
e	O
suppose	O
that	O
we	O
take	O
a	O
data	B
set	B
divide	O
it	O
into	O
equally-sized	O
training	O
and	O
test	O
sets	O
and	O
then	O
try	O
out	O
two	O
different	O
classification	B
procedures	O
first	O
we	O
use	O
logistic	B
regression	B
and	O
get	O
an	O
error	B
rate	B
of	O
on	O
the	O
training	O
data	B
and	O
on	O
the	O
test	O
data	B
next	O
we	O
use	O
neighbors	O
k	O
and	O
get	O
an	O
average	B
error	B
rate	B
over	O
both	O
test	O
and	O
training	O
data	B
sets	O
of	O
based	O
on	O
these	O
results	O
which	O
method	O
should	O
we	O
prefer	O
to	O
use	O
for	O
classification	B
of	O
new	O
observations	B
why	O
this	O
problem	O
has	O
to	O
do	O
with	O
odds	B
on	O
average	B
what	O
fraction	O
of	O
people	O
with	O
an	O
odds	B
of	O
of	O
defaulting	O
on	O
their	O
credit	B
card	O
payment	O
will	O
in	O
fact	O
default	B
suppose	O
that	O
an	O
individual	O
has	O
a	O
chance	O
of	O
defaulting	O
on	O
her	O
credit	B
card	O
payment	O
what	O
are	O
the	O
odds	B
that	O
she	O
will	O
default	B
exercises	O
applied	O
this	O
question	O
should	O
be	O
answered	O
using	O
the	O
weekly	B
data	B
set	B
which	O
is	O
part	O
of	O
the	O
islr	O
package	O
this	O
data	B
is	O
similar	O
in	O
nature	O
to	O
the	O
smarket	B
data	B
from	O
this	O
chapter	O
s	O
lab	O
except	O
that	O
it	O
contains	O
weekly	B
returns	O
for	O
years	O
from	O
the	O
beginning	O
of	O
to	O
the	O
end	O
of	O
produce	O
some	O
numerical	O
and	O
graphical	O
summaries	O
of	O
the	O
weekly	B
data	B
do	O
there	O
appear	O
to	O
be	O
any	O
patterns	O
use	O
the	O
full	O
data	B
set	B
to	O
perform	O
a	O
logistic	B
regression	B
with	O
direction	O
as	O
the	O
response	B
and	O
the	O
five	O
lag	O
variables	O
plus	O
volume	O
as	O
predictors	O
use	O
the	O
summary	O
function	B
to	O
print	O
the	O
results	O
do	O
any	O
of	O
the	O
predictors	O
appear	O
to	O
be	O
statistically	O
significant	O
if	O
so	O
which	O
ones	O
compute	O
the	O
confusion	B
matrix	I
and	O
overall	O
fraction	O
of	O
correct	O
predictions	O
explain	O
what	O
the	O
confusion	B
matrix	I
is	O
telling	O
you	O
about	O
the	O
types	O
of	O
mistakes	O
made	O
by	O
logistic	B
regression	B
now	O
fit	O
the	O
logistic	B
regression	B
model	B
using	O
a	O
training	O
data	B
period	O
from	O
to	O
with	O
as	O
the	O
only	O
predictor	B
compute	O
the	O
confusion	B
matrix	I
and	O
the	O
overall	O
fraction	O
of	O
correct	O
predictions	O
for	O
the	O
held	O
out	O
data	B
is	O
the	O
data	B
from	O
and	O
repeat	O
using	O
lda	O
repeat	O
using	O
qda	O
repeat	O
using	O
knn	O
with	O
k	O
which	O
of	O
these	O
methods	O
appears	O
to	O
provide	O
the	O
best	O
results	O
on	O
this	O
data	B
experiment	O
with	O
different	O
combinations	O
of	O
predictors	O
including	O
possible	O
transformations	O
and	O
interactions	O
for	O
each	O
of	O
the	O
methods	O
report	O
the	O
variables	O
method	O
and	O
associated	O
confusion	B
matrix	I
that	O
appears	O
to	O
provide	O
the	O
best	O
results	O
on	O
the	O
held	O
out	O
data	B
note	O
that	O
you	O
should	O
also	O
experiment	O
with	O
values	O
for	O
k	O
in	O
the	O
knn	O
classifier	B
in	O
this	O
problem	O
you	O
will	O
develop	O
a	O
model	B
to	O
predict	O
whether	O
a	O
given	O
car	O
gets	O
high	O
or	O
low	O
gas	O
mileage	O
based	O
on	O
the	O
auto	B
data	B
set	B
create	O
a	O
binary	B
variable	B
that	O
contains	O
a	O
if	O
mpg	O
contains	O
a	O
value	O
above	O
its	O
median	O
and	O
a	O
if	O
mpg	O
contains	O
a	O
value	O
below	O
its	O
median	O
you	O
can	O
compute	O
the	O
median	O
using	O
the	O
median	O
function	B
note	O
you	O
may	O
find	O
it	O
helpful	O
to	O
use	O
the	O
data	B
frame	I
function	B
to	O
create	O
a	O
single	B
data	B
set	B
containing	O
both	O
and	O
the	O
other	O
auto	B
variables	O
classification	B
explore	O
the	O
data	B
graphically	O
in	O
order	O
to	O
investigate	O
the	O
association	O
between	O
and	O
the	O
other	O
features	O
which	O
of	O
the	O
other	O
features	O
seem	O
most	O
likely	O
to	O
be	O
useful	O
in	O
predicting	O
scatterplots	O
and	O
boxplots	O
may	O
be	O
useful	O
tools	O
to	O
answer	O
this	O
question	O
describe	O
your	O
findings	O
split	O
the	O
data	B
into	O
a	O
training	O
set	B
and	O
a	O
test	O
set	B
perform	O
lda	O
on	O
the	O
training	O
data	B
in	O
order	O
to	O
predict	O
using	O
the	O
variables	O
that	O
seemed	O
most	O
associated	O
with	O
in	O
what	O
is	O
the	O
test	O
error	B
of	O
the	O
model	B
obtained	O
perform	O
qda	O
on	O
the	O
training	O
data	B
in	O
order	O
to	O
predict	O
using	O
the	O
variables	O
that	O
seemed	O
most	O
associated	O
with	O
in	O
what	O
is	O
the	O
test	O
error	B
of	O
the	O
model	B
obtained	O
perform	O
logistic	B
regression	B
on	O
the	O
training	O
data	B
in	O
order	O
to	O
predict	O
using	O
the	O
variables	O
that	O
seemed	O
most	O
associated	O
with	O
in	O
what	O
is	O
the	O
test	O
error	B
of	O
the	O
model	B
obtained	O
perform	O
knn	O
on	O
the	O
training	O
data	B
with	O
several	O
values	O
of	O
k	O
in	O
order	O
to	O
predict	O
use	O
only	O
the	O
variables	O
that	O
seemed	O
most	O
associated	O
with	O
in	O
what	O
test	O
errors	O
do	O
you	O
obtain	O
which	O
value	O
of	O
k	O
seems	O
to	O
perform	O
the	O
best	O
on	O
this	O
data	B
set	B
this	O
problem	O
involves	O
writing	O
functions	O
write	O
a	O
function	B
power	B
that	O
prints	O
out	O
the	O
result	O
of	O
raising	O
to	O
the	O
power	B
in	O
other	O
words	O
your	O
function	B
should	O
compute	O
and	O
print	O
out	O
the	O
results	O
hint	O
recall	B
that	O
xa	O
raises	O
x	O
to	O
the	O
power	B
a	O
use	O
the	O
print	O
function	B
to	O
output	B
the	O
result	O
create	O
a	O
new	O
function	B
that	O
allows	O
you	O
to	O
pass	O
any	O
two	O
numbers	O
x	O
and	O
a	O
and	O
prints	O
out	O
the	O
value	O
of	O
xa	O
you	O
can	O
do	O
this	O
by	O
beginning	O
your	O
function	B
with	O
the	O
line	B
function	B
a	O
you	O
should	O
be	O
able	O
to	O
call	O
your	O
function	B
by	O
entering	O
for	O
instance	O
on	O
the	O
command	O
line	B
this	O
should	O
output	B
the	O
value	O
of	O
namely	O
using	O
the	O
function	B
that	O
you	O
just	O
wrote	O
compute	O
and	O
now	O
create	O
a	O
new	O
function	B
that	O
actually	O
returns	O
the	O
result	O
xa	O
as	O
an	O
r	O
object	O
rather	O
than	O
simply	O
printing	O
it	O
to	O
the	O
screen	O
that	O
is	O
if	O
you	O
store	O
the	O
value	O
xa	O
in	O
an	O
object	O
called	O
result	O
within	O
your	O
function	B
then	O
you	O
can	O
simply	O
return	O
this	O
result	O
using	O
the	O
following	O
line	B
return	O
exercises	O
return	O
result	O
the	O
line	B
above	O
should	O
be	O
the	O
last	O
line	B
in	O
your	O
function	B
before	O
the	O
symbol	O
now	O
using	O
the	O
function	B
create	O
a	O
plot	B
of	O
f	O
the	O
x-axis	O
should	O
display	O
a	O
range	O
of	O
integers	O
from	O
to	O
and	O
the	O
y-axis	O
should	O
display	O
label	O
the	O
axes	O
appropriately	O
and	O
use	O
an	O
appropriate	O
title	O
for	O
the	O
figure	O
consider	O
displaying	O
either	O
the	O
x-axis	O
the	O
y-axis	O
or	O
both	O
on	O
the	O
log-scale	O
you	O
can	O
do	O
this	O
by	O
using	O
log	O
x	O
log	O
y	O
or	O
log	O
xy	O
as	O
arguments	O
to	O
the	O
plot	B
function	B
create	O
a	O
function	B
plotpower	O
that	O
allows	O
you	O
to	O
create	O
a	O
plot	B
of	O
x	O
against	O
xa	O
for	O
a	O
fixed	O
a	O
and	O
for	O
a	O
range	O
of	O
values	O
of	O
x	O
for	O
instance	O
if	O
you	O
call	O
plotpower	O
then	O
a	O
plot	B
should	O
be	O
created	O
with	O
an	O
x-axis	O
taking	O
on	O
values	O
and	O
a	O
y-axis	O
taking	O
on	O
values	O
using	O
the	O
boston	B
data	B
set	B
fit	O
classification	B
models	O
in	O
order	O
to	O
predict	O
whether	O
a	O
given	O
suburb	O
has	O
a	O
crime	O
rate	B
above	O
or	O
below	O
the	O
median	O
explore	O
logistic	B
regression	B
lda	O
and	O
knn	O
models	O
using	O
various	O
subsets	O
of	O
the	O
predictors	O
describe	O
your	O
findings	O
resampling	B
methods	O
resampling	B
methods	O
are	O
an	O
indispensable	O
tool	O
in	O
modern	O
statistics	O
they	O
involve	O
repeatedly	O
drawing	O
samples	O
from	O
a	O
training	O
set	B
and	O
refitting	O
a	O
model	B
of	O
interest	O
on	O
each	O
sample	O
in	O
order	O
to	O
obtain	O
additional	O
information	O
about	O
the	O
fitted	O
model	B
for	O
example	O
in	O
order	O
to	O
estimate	O
the	O
variability	O
of	O
a	O
linear	B
regression	B
fit	O
we	O
can	O
repeatedly	O
draw	O
different	O
samples	O
from	O
the	O
training	O
data	B
fit	O
a	O
linear	B
regression	B
to	O
each	O
new	O
sample	O
and	O
then	O
examine	O
the	O
extent	O
to	O
which	O
the	O
resulting	O
fits	O
differ	O
such	O
an	O
approach	B
may	O
allow	O
us	O
to	O
obtain	O
information	O
that	O
would	O
not	O
be	O
available	O
from	O
fitting	O
the	O
model	B
only	O
once	O
using	O
the	O
original	O
training	O
sample	O
resampling	B
approaches	O
can	O
be	O
computationally	O
expensive	O
because	O
they	O
involve	O
fitting	O
the	O
same	O
statistical	O
method	O
multiple	B
times	O
using	O
different	O
subsets	O
of	O
the	O
training	O
data	B
however	O
due	O
to	O
recent	O
advances	O
in	O
computing	O
power	B
the	O
computational	O
requirements	O
of	O
resampling	B
methods	O
generally	O
are	O
not	O
prohibitive	O
in	O
this	O
chapter	O
we	O
discuss	O
two	O
of	O
the	O
most	O
commonly	O
used	O
resampling	B
methods	O
cross-validation	B
and	O
the	O
bootstrap	B
both	O
methods	O
are	O
important	O
tools	O
in	O
the	O
practical	O
application	O
of	O
many	O
statistical	O
learning	O
procedures	O
for	O
example	O
cross-validation	B
can	O
be	O
used	O
to	O
estimate	O
the	O
test	O
error	B
associated	O
with	O
a	O
given	O
statistical	O
learning	O
method	O
in	O
order	O
to	O
evaluate	O
its	O
performance	O
or	O
to	O
select	O
the	O
appropriate	O
level	B
of	O
flexibility	O
the	O
process	O
of	O
evaluating	O
a	O
model	B
s	O
performance	O
is	O
known	O
as	O
model	B
assessment	I
whereas	O
the	O
process	O
of	O
selecting	O
the	O
proper	O
level	B
of	O
flexibility	O
for	O
a	O
model	B
is	O
known	O
as	O
model	B
selection	B
the	O
bootstrap	B
is	O
used	O
in	O
several	O
contexts	O
most	O
commonly	O
to	O
provide	O
a	O
measure	O
of	O
accuracy	O
of	O
a	O
parameter	B
estimate	O
or	O
of	O
a	O
given	O
statistical	O
learning	O
method	O
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
model	B
assessment	I
model	B
selection	B
resampling	B
methods	O
cross-validation	B
in	O
chapter	O
we	O
discuss	O
the	O
distinction	O
between	O
the	O
test	O
error	B
rate	B
and	O
the	O
training	O
error	B
rate	B
the	O
test	O
error	B
is	O
the	O
average	B
error	B
that	O
results	O
from	O
using	O
a	O
statistical	O
learning	O
method	O
to	O
predict	O
the	O
response	B
on	O
a	O
new	O
observation	O
that	O
is	O
a	O
measurement	O
that	O
was	O
not	O
used	O
in	O
training	O
the	O
method	O
given	O
a	O
data	B
set	B
the	O
use	O
of	O
a	O
particular	O
statistical	O
learning	O
method	O
is	O
warranted	O
if	O
it	O
results	O
in	O
a	O
low	O
test	O
error	B
the	O
test	O
error	B
can	O
be	O
easily	O
calculated	O
if	O
a	O
designated	O
test	O
set	B
is	O
available	O
unfortunately	O
this	O
is	O
usually	O
not	O
the	O
case	O
in	O
contrast	B
the	O
training	O
error	B
can	O
be	O
easily	O
calculated	O
by	O
applying	O
the	O
statistical	O
learning	O
method	O
to	O
the	O
observations	B
used	O
in	O
its	O
training	O
but	O
as	O
we	O
saw	O
in	O
chapter	O
the	O
training	O
error	B
rate	B
often	O
is	O
quite	O
different	O
from	O
the	O
test	O
error	B
rate	B
and	O
in	O
particular	O
the	O
former	O
can	O
dramatically	O
underestimate	O
the	O
latter	O
in	O
the	O
absence	O
of	O
a	O
very	O
large	O
designated	O
test	O
set	B
that	O
can	O
be	O
used	O
to	O
directly	O
estimate	O
the	O
test	O
error	B
rate	B
a	O
number	O
of	O
techniques	O
can	O
be	O
used	O
to	O
estimate	O
this	O
quantity	O
using	O
the	O
available	O
training	O
data	B
some	O
methods	O
make	O
a	O
mathematical	O
adjustment	O
to	O
the	O
training	O
error	B
rate	B
in	O
order	O
to	O
estimate	O
the	O
test	O
error	B
rate	B
such	O
approaches	O
are	O
discussed	O
in	O
chapter	O
in	O
this	O
section	O
we	O
instead	O
consider	O
a	O
class	O
of	O
methods	O
that	O
estimate	O
the	O
test	O
error	B
rate	B
by	O
holding	O
out	O
a	O
subset	O
of	O
the	O
training	O
observations	B
from	O
the	O
fitting	O
process	O
and	O
then	O
applying	O
the	O
statistical	O
learning	O
method	O
to	O
those	O
held	O
out	O
observations	B
in	O
sections	O
for	O
simplicity	O
we	O
assume	O
that	O
we	O
are	O
interested	O
in	O
performing	O
regression	B
with	O
a	O
quantitative	B
response	B
in	O
section	O
we	O
consider	O
the	O
case	O
of	O
classification	B
with	O
a	O
qualitative	B
response	B
as	O
we	O
will	O
see	O
the	O
key	O
concepts	O
remain	O
the	O
same	O
regardless	O
of	O
whether	O
the	O
response	B
is	O
quantitative	B
or	O
qualitative	B
the	O
validation	B
set	B
approach	B
suppose	O
that	O
we	O
would	O
like	O
to	O
estimate	O
the	O
test	O
error	B
associated	O
with	O
fitting	O
a	O
particular	O
statistical	O
learning	O
method	O
on	O
a	O
set	B
of	O
observations	B
the	O
validation	B
set	B
approach	B
displayed	O
in	O
figure	O
is	O
a	O
very	O
simple	B
strategy	O
for	O
this	O
task	O
it	O
involves	O
randomly	O
dividing	O
the	O
available	O
set	B
of	O
observations	B
into	O
two	O
parts	O
a	O
training	O
set	B
and	O
a	O
validation	B
set	B
or	O
hold-out	B
set	B
the	O
model	B
is	O
fit	O
on	O
the	O
training	O
set	B
and	O
the	O
fitted	O
model	B
is	O
used	O
to	O
predict	O
the	O
responses	O
for	O
the	O
observations	B
in	O
the	O
validation	B
set	B
the	O
resulting	O
validation	B
set	B
error	B
rate	B
typically	O
assessed	O
using	O
mse	B
in	O
the	O
case	O
of	O
a	O
quantitative	B
response	B
provides	O
an	O
estimate	O
of	O
the	O
test	O
error	B
rate	B
we	O
illustrate	O
the	O
validation	B
set	B
approach	B
on	O
the	O
auto	B
data	B
set	B
recall	B
from	O
chapter	O
that	O
there	O
appears	O
to	O
be	O
a	O
non-linear	B
relationship	O
between	O
mpg	O
and	O
horsepower	O
and	O
that	O
a	O
model	B
that	O
predicts	O
mpg	O
using	O
horsepower	O
and	O
gives	O
better	O
results	O
than	O
a	O
model	B
that	O
uses	O
only	O
a	O
linear	B
term	B
horsepower	O
it	O
is	O
natural	B
to	O
wonder	O
whether	O
a	O
cubic	B
or	O
higher-order	O
fit	O
might	O
provide	O
validation	B
set	B
approach	B
validation	B
set	B
hold-out	B
set	B
cross-validation	B
n	O
figure	O
a	O
schematic	O
display	O
of	O
the	O
validation	B
set	B
approach	B
a	O
set	B
of	O
n	O
observations	B
are	O
randomly	O
split	O
into	O
a	O
training	O
set	B
in	O
blue	O
containing	O
observations	B
and	O
among	O
others	O
and	O
a	O
validation	B
set	B
in	O
beige	O
and	O
containing	O
observation	O
among	O
others	O
the	O
statistical	O
learning	O
method	O
is	O
fit	O
on	O
the	O
training	O
set	B
and	O
its	O
performance	O
is	O
evaluated	O
on	O
the	O
validation	B
set	B
even	O
better	O
results	O
we	O
answer	O
this	O
question	O
in	O
chapter	O
by	O
looking	O
at	O
the	O
p-values	O
associated	O
with	O
a	O
cubic	B
term	B
and	O
higher-order	O
polynomial	B
terms	O
in	O
a	O
linear	B
regression	B
but	O
we	O
could	O
also	O
answer	O
this	O
question	O
using	O
the	O
validation	O
method	O
we	O
randomly	O
split	O
the	O
observations	B
into	O
two	O
sets	O
a	O
training	O
set	B
containing	O
of	O
the	O
data	B
points	O
and	O
a	O
validation	B
set	B
containing	O
the	O
remaining	O
observations	B
the	O
validation	B
set	B
error	B
rates	O
that	O
result	O
from	O
fitting	O
various	O
regression	B
models	O
on	O
the	O
training	O
sample	O
and	O
evaluating	O
their	O
performance	O
on	O
the	O
validation	O
sample	O
using	O
mse	B
as	O
a	O
measure	O
of	O
validation	B
set	B
error	B
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
validation	B
set	B
mse	B
for	O
the	O
quadratic	B
fit	O
is	O
considerably	O
smaller	O
than	O
for	O
the	O
linear	B
fit	O
however	O
the	O
validation	B
set	B
mse	B
for	O
the	O
cubic	B
fit	O
is	O
actually	O
slightly	O
larger	O
than	O
for	O
the	O
quadratic	B
fit	O
this	O
implies	O
that	O
including	O
a	O
cubic	B
term	B
in	O
the	O
regression	B
does	O
not	O
lead	O
to	O
better	O
prediction	B
than	O
simply	O
using	O
a	O
quadratic	B
term	B
recall	B
that	O
in	O
order	O
to	O
create	O
the	O
left-hand	O
panel	O
of	O
figure	O
we	O
randomly	O
divided	O
the	O
data	B
set	B
into	O
two	O
parts	O
a	O
training	O
set	B
and	O
a	O
validation	B
set	B
if	O
we	O
repeat	O
the	O
process	O
of	O
randomly	O
splitting	O
the	O
sample	O
set	B
into	O
two	O
parts	O
we	O
will	O
get	O
a	O
somewhat	O
different	O
estimate	O
for	O
the	O
test	O
mse	B
as	O
an	O
illustration	O
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
ten	O
different	O
validation	B
set	B
mse	B
curves	O
from	O
the	O
auto	B
data	B
set	B
produced	O
using	O
ten	O
different	O
random	O
splits	O
of	O
the	O
observations	B
into	O
training	O
and	O
validation	O
sets	O
all	O
ten	O
curves	O
indicate	O
that	O
the	O
model	B
with	O
a	O
quadratic	B
term	B
has	O
a	O
dramatically	O
smaller	O
validation	B
set	B
mse	B
than	O
the	O
model	B
with	O
only	O
a	O
linear	B
term	B
furthermore	O
all	O
ten	O
curves	O
indicate	O
that	O
there	O
is	O
not	O
much	O
benefit	O
in	O
including	O
cubic	B
or	O
higher-order	O
polynomial	B
terms	O
in	O
the	O
model	B
but	O
it	O
is	O
worth	O
noting	O
that	O
each	O
of	O
the	O
ten	O
curves	O
results	O
in	O
a	O
different	O
test	O
mse	B
estimate	O
for	O
each	O
of	O
the	O
ten	O
regression	B
models	O
considered	O
and	O
there	O
is	O
no	O
consensus	O
among	O
the	O
curves	O
as	O
to	O
which	O
model	B
results	O
in	O
the	O
smallest	O
validation	B
set	B
mse	B
based	O
on	O
the	O
variability	O
among	O
these	O
curves	O
all	O
that	O
we	O
can	O
conclude	O
with	O
any	O
confidence	O
is	O
that	O
the	O
linear	B
fit	O
is	O
not	O
adequate	O
for	O
this	O
data	B
the	O
validation	B
set	B
approach	B
is	O
conceptually	O
simple	B
and	O
is	O
easy	O
to	O
imple	O
ment	O
but	O
it	O
has	O
two	O
potential	O
drawbacks	O
resampling	B
methods	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
degree	O
of	O
polynomial	B
degree	O
of	O
polynomial	B
figure	O
the	O
validation	B
set	B
approach	B
was	O
used	O
on	O
the	O
auto	B
data	B
set	B
in	O
order	O
to	O
estimate	O
the	O
test	O
error	B
that	O
results	O
from	O
predicting	O
mpg	O
using	O
polynomial	B
functions	O
of	O
horsepower	O
left	O
validation	O
error	B
estimates	O
for	O
a	O
single	B
split	O
into	O
training	O
and	O
validation	O
data	B
sets	O
right	O
the	O
validation	O
method	O
was	O
repeated	O
ten	O
times	O
each	O
time	O
using	O
a	O
different	O
random	O
split	O
of	O
the	O
observations	B
into	O
a	O
training	O
set	B
and	O
a	O
validation	B
set	B
this	O
illustrates	O
the	O
variability	O
in	O
the	O
estimated	O
test	O
mse	B
that	O
results	O
from	O
this	O
approach	B
as	O
is	O
shown	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
validation	O
estimate	O
of	O
the	O
test	O
error	B
rate	B
can	O
be	O
highly	O
variable	B
depending	O
on	O
precisely	O
which	O
observations	B
are	O
included	O
in	O
the	O
training	O
set	B
and	O
which	O
observations	B
are	O
included	O
in	O
the	O
validation	B
set	B
in	O
the	O
validation	O
approach	B
only	O
a	O
subset	O
of	O
the	O
observations	B
those	O
that	O
are	O
included	O
in	O
the	O
training	O
set	B
rather	O
than	O
in	O
the	O
validation	B
set	B
are	O
used	O
to	O
fit	O
the	O
model	B
since	O
statistical	O
methods	O
tend	O
to	O
perform	O
worse	O
when	O
trained	O
on	O
fewer	O
observations	B
this	O
suggests	O
that	O
the	O
validation	B
set	B
error	B
rate	B
may	O
tend	O
to	O
overestimate	O
the	O
test	O
error	B
rate	B
for	O
the	O
model	B
fit	O
on	O
the	O
entire	O
data	B
set	B
in	O
the	O
coming	O
subsections	O
we	O
will	O
present	O
cross-validation	B
a	O
refinement	O
of	O
the	O
validation	B
set	B
approach	B
that	O
addresses	O
these	O
two	O
issues	O
leave-one-out	B
cross-validation	B
leave-one-out	B
cross-validation	B
is	O
closely	O
related	O
to	O
the	O
validation	B
set	B
approach	B
of	O
section	O
but	O
it	O
attempts	O
to	O
address	O
that	O
method	O
s	O
drawbacks	O
like	O
the	O
validation	B
set	B
approach	B
loocv	O
involves	O
splitting	O
the	O
set	B
of	O
observations	B
into	O
two	O
parts	O
however	O
instead	O
of	O
creating	O
two	O
subsets	O
of	O
comparable	O
size	O
a	O
single	B
observation	O
is	O
used	O
for	O
the	O
validation	B
set	B
and	O
the	O
remaining	O
observations	B
yn	O
make	O
up	O
the	O
training	O
set	B
the	O
statistical	O
learning	O
method	O
is	O
fit	O
on	O
the	O
n	O
training	O
observations	B
and	O
a	O
prediction	B
is	O
made	O
for	O
the	O
excluded	O
observation	O
using	O
its	O
value	O
since	O
was	O
not	O
used	O
in	O
the	O
fitting	O
process	O
leave-oneout	O
crossvalidation	O
cross-validation	B
n	O
n	O
n	O
n	O
n	O
figure	O
a	O
schematic	O
display	O
of	O
loocv	O
a	O
set	B
of	O
n	O
data	B
points	O
is	O
repeatedly	O
split	O
into	O
a	O
training	O
set	B
in	O
blue	O
containing	O
all	O
but	O
one	O
observation	O
and	O
a	O
validation	B
set	B
that	O
contains	O
only	O
that	O
observation	O
in	O
beige	O
the	O
test	O
error	B
is	O
then	O
estimated	O
by	O
averaging	O
the	O
n	O
resulting	O
mse	B
s	O
the	O
first	O
training	O
set	B
contains	O
all	O
but	O
observation	O
the	O
second	O
training	O
set	B
contains	O
all	O
but	O
observation	O
and	O
so	O
forth	O
provides	O
an	O
approximately	O
unbiased	O
estimate	O
for	O
the	O
test	O
error	B
but	O
even	O
though	O
is	O
unbiased	O
for	O
the	O
test	O
error	B
it	O
is	O
a	O
poor	O
estimate	O
because	O
it	O
is	O
highly	O
variable	B
since	O
it	O
is	O
based	O
upon	O
a	O
single	B
observation	O
we	O
can	O
repeat	O
the	O
procedure	O
by	O
selecting	O
for	O
the	O
validation	O
data	B
training	O
the	O
statistical	O
learning	O
procedure	O
on	O
the	O
n	O
observations	B
yn	O
and	O
computing	O
repeating	O
this	O
approach	B
n	O
times	O
produces	O
n	O
squared	O
errors	O
msen	O
the	O
loocv	O
estimate	O
for	O
the	O
test	O
mse	B
is	O
the	O
average	B
of	O
these	O
n	O
test	O
error	B
estimates	O
cvn	O
n	O
msei	O
a	O
schematic	O
of	O
the	O
loocv	O
approach	B
is	O
illustrated	O
in	O
figure	O
loocv	O
has	O
a	O
couple	O
of	O
major	O
advantages	O
over	O
the	O
validation	B
set	B
approach	B
first	O
it	O
has	O
far	O
less	O
bias	B
in	O
loocv	O
we	O
repeatedly	O
fit	O
the	O
statistical	O
learning	O
method	O
using	O
training	O
sets	O
that	O
contain	O
n	O
observations	B
almost	O
as	O
many	O
as	O
are	O
in	O
the	O
entire	O
data	B
set	B
this	O
is	O
in	O
contrast	B
to	O
the	O
validation	B
set	B
approach	B
in	O
which	O
the	O
training	O
set	B
is	O
typically	O
around	O
half	O
the	O
size	O
of	O
the	O
original	O
data	B
set	B
consequently	O
the	O
loocv	O
approach	B
tends	O
not	O
to	O
overestimate	O
the	O
test	O
error	B
rate	B
as	O
much	O
as	O
the	O
validation	B
set	B
approach	B
does	O
second	O
in	O
contrast	B
to	O
the	O
validation	O
approach	B
which	O
will	O
yield	O
different	O
results	O
when	O
applied	O
repeatedly	O
due	O
to	O
randomness	O
in	O
the	O
trainingvalidation	O
set	B
splits	O
performing	O
loocv	O
multiple	B
times	O
will	O
resampling	B
methods	O
loocv	O
fold	O
cv	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
degree	O
of	O
polynomial	B
degree	O
of	O
polynomial	B
figure	O
cross-validation	B
was	O
used	O
on	O
the	O
auto	B
data	B
set	B
in	O
order	O
to	O
estimate	O
the	O
test	O
error	B
that	O
results	O
from	O
predicting	O
mpg	O
using	O
polynomial	B
functions	O
of	O
horsepower	O
left	O
the	O
loocv	O
error	B
curve	O
right	O
cv	O
was	O
run	O
nine	O
separate	O
times	O
each	O
with	O
a	O
different	O
random	O
split	O
of	O
the	O
data	B
into	O
ten	O
parts	O
the	O
figure	O
shows	O
the	O
nine	O
slightly	O
different	O
cv	O
error	B
curves	O
always	O
yield	O
the	O
same	O
results	O
there	O
is	O
no	O
randomness	O
in	O
the	O
trainingvalidation	O
set	B
splits	O
we	O
used	O
loocv	O
on	O
the	O
auto	B
data	B
set	B
in	O
order	O
to	O
obtain	O
an	O
estimate	O
of	O
the	O
test	O
set	B
mse	B
that	O
results	O
from	O
fitting	O
a	O
linear	B
regression	B
model	B
to	O
predict	O
mpg	O
using	O
polynomial	B
functions	O
of	O
horsepower	O
the	O
results	O
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
loocv	O
has	O
the	O
potential	O
to	O
be	O
expensive	O
to	O
implement	O
since	O
the	O
model	B
has	O
to	O
be	O
fit	O
n	O
times	O
this	O
can	O
be	O
very	O
time	O
consuming	O
if	O
n	O
is	O
large	O
and	O
if	O
each	O
individual	O
model	B
is	O
slow	O
to	O
fit	O
with	O
least	B
squares	I
linear	B
or	O
polynomial	B
regression	B
an	O
amazing	O
shortcut	O
makes	O
the	O
cost	O
of	O
loocv	O
the	O
same	O
as	O
that	O
of	O
a	O
single	B
model	B
fit	O
the	O
following	O
formula	O
holds	O
yi	O
yi	O
hi	O
n	O
cvn	O
where	O
yi	O
is	O
the	O
ith	O
fitted	O
value	O
from	O
the	O
original	O
least	B
squares	I
fit	O
and	O
hi	O
is	O
the	O
leverage	B
defined	O
in	O
on	O
page	O
this	O
is	O
like	O
the	O
ordinary	O
mse	B
except	O
the	O
ith	O
residual	B
is	O
divided	O
by	O
hi	O
the	O
leverage	B
lies	O
between	O
and	O
and	O
reflects	O
the	O
amount	O
that	O
an	O
observation	O
influences	O
its	O
own	O
fit	O
hence	O
the	O
residuals	B
for	O
high-leverage	O
points	O
are	O
inflated	O
in	O
this	O
formula	O
by	O
exactly	O
the	O
right	O
amount	O
for	O
this	O
equality	O
to	O
hold	O
loocv	O
is	O
a	O
very	O
general	O
method	O
and	O
can	O
be	O
used	O
with	O
any	O
kind	O
of	O
predictive	O
modeling	O
for	O
example	O
we	O
could	O
use	O
it	O
with	O
logistic	B
regression	B
or	O
linear	B
discriminant	I
analysis	B
or	O
any	O
of	O
the	O
methods	O
discussed	O
in	O
later	O
cross-validation	B
n	O
figure	O
a	O
schematic	O
display	O
of	O
cv	O
a	O
set	B
of	O
n	O
observations	B
is	O
randomly	O
split	O
into	O
five	O
non-overlapping	O
groups	O
each	O
of	O
these	O
fifths	O
acts	O
as	O
a	O
validation	B
set	B
in	O
beige	O
and	O
the	O
remainder	O
as	O
a	O
training	O
set	B
in	O
blue	O
the	O
test	O
error	B
is	O
estimated	O
by	O
averaging	O
the	O
five	O
resulting	O
mse	B
estimates	O
chapters	O
the	O
magic	O
formula	O
does	O
not	O
hold	O
in	O
general	O
in	O
which	O
case	O
the	O
model	B
has	O
to	O
be	O
refit	O
n	O
times	O
k-fold	B
cross-validation	B
an	O
alternative	O
to	O
loocv	O
is	O
k-fold	B
cv	O
this	O
approach	B
involves	O
randomly	O
dividing	O
the	O
set	B
of	O
observations	B
into	O
k	O
groups	O
or	O
folds	O
of	O
approximately	O
equal	O
size	O
the	O
first	O
fold	O
is	O
treated	O
as	O
a	O
validation	B
set	B
and	O
the	O
method	O
is	O
fit	O
on	O
the	O
remaining	O
k	O
folds	O
the	O
mean	B
squared	I
error	B
is	O
k-fold	B
cv	O
then	O
computed	O
on	O
the	O
observations	B
in	O
the	O
held-out	O
fold	O
this	O
procedure	O
is	O
repeated	O
k	O
times	O
each	O
time	O
a	O
different	O
group	O
of	O
observations	B
is	O
treated	O
as	O
a	O
validation	B
set	B
this	O
process	O
results	O
in	O
k	O
estimates	O
of	O
the	O
test	O
error	B
msek	O
the	O
k-fold	B
cv	O
estimate	O
is	O
computed	O
by	O
averaging	O
these	O
values	O
cvk	O
k	O
msei	O
figure	O
illustrates	O
the	O
k-fold	B
cv	O
approach	B
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
loocv	O
is	O
a	O
special	O
case	O
of	O
k-fold	B
cv	O
in	O
which	O
k	O
is	O
set	B
to	O
equal	O
n	O
in	O
practice	O
one	O
typically	O
performs	O
k-fold	B
cv	O
using	O
k	O
or	O
k	O
what	O
is	O
the	O
advantage	O
of	O
using	O
k	O
or	O
k	O
rather	O
than	O
k	O
n	O
the	O
most	O
obvious	O
advantage	O
is	O
computational	O
loocv	O
requires	O
fitting	O
the	O
statistical	O
learning	O
method	O
n	O
times	O
this	O
has	O
the	O
potential	O
to	O
be	O
computationally	O
expensive	O
for	O
linear	B
models	O
fit	O
by	O
least	B
squares	I
in	O
which	O
case	O
formula	O
can	O
be	O
used	O
but	O
cross-validation	B
is	O
a	O
very	O
general	O
approach	B
that	O
can	O
be	O
applied	O
to	O
almost	O
any	O
statistical	O
learning	O
method	O
some	O
statistical	O
learning	O
methods	O
have	O
computationally	O
intensive	O
fitting	O
procedures	O
and	O
so	O
performing	O
loocv	O
may	O
pose	O
computational	O
problems	O
especially	O
if	O
n	O
is	O
extremely	O
large	O
in	O
contrast	B
performing	O
resampling	B
methods	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
flexibility	O
flexibility	O
flexibility	O
figure	O
true	O
and	O
estimated	O
test	O
mse	B
for	O
the	O
simulated	O
data	B
sets	O
in	O
figures	O
left	O
center	O
and	O
right	O
the	O
true	O
test	O
mse	B
is	O
shown	O
in	O
blue	O
the	O
loocv	O
estimate	O
is	O
shown	O
as	O
a	O
black	O
dashed	O
line	B
and	O
the	O
cv	O
estimate	O
is	O
shown	O
in	O
orange	O
the	O
crosses	O
indicate	O
the	O
minimum	O
of	O
each	O
of	O
the	O
mse	B
curves	O
cv	O
requires	O
fitting	O
the	O
learning	O
procedure	O
only	O
ten	O
times	O
which	O
may	O
be	O
much	O
more	O
feasible	O
as	O
we	O
see	O
in	O
section	O
there	O
also	O
can	O
be	O
other	O
non-computational	O
advantages	O
to	O
performing	O
or	O
cv	O
which	O
involve	O
the	O
bias-variance	O
trade-off	B
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
nine	O
different	O
cv	O
estimates	O
for	O
the	O
auto	B
data	B
set	B
each	O
resulting	O
from	O
a	O
different	O
random	O
split	O
of	O
the	O
observations	B
into	O
ten	O
folds	O
as	O
we	O
can	O
see	O
from	O
the	O
figure	O
there	O
is	O
some	O
variability	O
in	O
the	O
cv	O
estimates	O
as	O
a	O
result	O
of	O
the	O
variability	O
in	O
how	O
the	O
observations	B
are	O
divided	O
into	O
ten	O
folds	O
but	O
this	O
variability	O
is	O
typically	O
much	O
lower	O
than	O
the	O
variability	O
in	O
the	O
test	O
error	B
estimates	O
that	O
results	O
from	O
the	O
validation	B
set	B
approach	B
panel	O
of	O
figure	O
when	O
we	O
examine	O
real	O
data	B
we	O
do	O
not	O
know	O
the	O
true	O
test	O
mse	B
and	O
so	O
it	O
is	O
difficult	O
to	O
determine	O
the	O
accuracy	O
of	O
the	O
cross-validation	B
estimate	O
however	O
if	O
we	O
examine	O
simulated	O
data	B
then	O
we	O
can	O
compute	O
the	O
true	O
test	O
mse	B
and	O
can	O
thereby	O
evaluate	O
the	O
accuracy	O
of	O
our	O
cross-validation	B
results	O
in	O
figure	O
we	O
plot	B
the	O
cross-validation	B
estimates	O
and	O
true	O
test	O
error	B
rates	O
that	O
result	O
from	O
applying	O
smoothing	B
splines	O
to	O
the	O
simulated	O
data	B
sets	O
illustrated	O
in	O
figures	O
of	O
chapter	O
the	O
true	O
test	O
mse	B
is	O
displayed	O
in	O
blue	O
the	O
black	O
dashed	O
and	O
orange	O
solid	O
lines	O
respectively	O
show	O
the	O
estimated	O
loocv	O
and	O
cv	O
estimates	O
in	O
all	O
three	O
plots	O
the	O
two	O
cross-validation	B
estimates	O
are	O
very	O
similar	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
true	O
test	O
mse	B
and	O
the	O
cross-validation	B
curves	O
are	O
almost	O
identical	O
in	O
the	O
center	O
panel	O
of	O
figure	O
the	O
two	O
sets	O
of	O
curves	O
are	O
similar	O
at	O
the	O
lower	O
degrees	O
of	O
flexibility	O
while	O
the	O
cv	O
curves	O
overestimate	O
the	O
test	O
set	B
mse	B
for	O
higher	O
degrees	O
of	O
flexibility	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
cv	O
curves	O
have	O
the	O
correct	O
general	O
shape	O
but	O
they	O
underestimate	O
the	O
true	O
test	O
mse	B
cross-validation	B
when	O
we	O
perform	O
cross-validation	B
our	O
goal	O
might	O
be	O
to	O
determine	O
how	O
well	O
a	O
given	O
statistical	O
learning	O
procedure	O
can	O
be	O
expected	O
to	O
perform	O
on	O
independent	B
data	B
in	O
this	O
case	O
the	O
actual	O
estimate	O
of	O
the	O
test	O
mse	B
is	O
of	O
interest	O
but	O
at	O
other	O
times	O
we	O
are	O
interested	O
only	O
in	O
the	O
location	O
of	O
the	O
minimum	O
point	O
in	O
the	O
estimated	O
test	O
mse	B
curve	O
this	O
is	O
because	O
we	O
might	O
be	O
performing	O
cross-validation	B
on	O
a	O
number	O
of	O
statistical	O
learning	O
methods	O
or	O
on	O
a	O
single	B
method	O
using	O
different	O
levels	O
of	O
flexibility	O
in	O
order	O
to	O
identify	O
the	O
method	O
that	O
results	O
in	O
the	O
lowest	O
test	O
error	B
for	O
this	O
purpose	O
the	O
location	O
of	O
the	O
minimum	O
point	O
in	O
the	O
estimated	O
test	O
mse	B
curve	O
is	O
important	O
but	O
the	O
actual	O
value	O
of	O
the	O
estimated	O
test	O
mse	B
is	O
not	O
we	O
find	O
in	O
figure	O
that	O
despite	O
the	O
fact	O
that	O
they	O
sometimes	O
underestimate	O
the	O
true	O
test	O
mse	B
all	O
of	O
the	O
cv	O
curves	O
come	O
close	O
to	O
identifying	O
the	O
correct	O
level	B
of	O
flexibility	O
that	O
is	O
the	O
flexibility	O
level	B
corresponding	O
to	O
the	O
smallest	O
test	O
mse	B
bias-variance	O
trade-off	B
for	O
k-fold	B
cross-validation	B
we	O
mentioned	O
in	O
section	O
that	O
k-fold	B
cv	O
with	O
k	O
n	O
has	O
a	O
computational	O
advantage	O
to	O
loocv	O
but	O
putting	O
computational	O
issues	O
aside	O
a	O
less	O
obvious	O
but	O
potentially	O
more	O
important	O
advantage	O
of	O
k-fold	B
cv	O
is	O
that	O
it	O
often	O
gives	O
more	O
accurate	O
estimates	O
of	O
the	O
test	O
error	B
rate	B
than	O
does	O
loocv	O
this	O
has	O
to	O
do	O
with	O
a	O
bias-variance	O
trade-off	B
it	O
was	O
mentioned	O
in	O
section	O
that	O
the	O
validation	B
set	B
approach	B
can	O
lead	O
to	O
overestimates	O
of	O
the	O
test	O
error	B
rate	B
since	O
in	O
this	O
approach	B
the	O
training	O
set	B
used	O
to	O
fit	O
the	O
statistical	O
learning	O
method	O
contains	O
only	O
half	O
the	O
observations	B
of	O
the	O
entire	O
data	B
set	B
using	O
this	O
logic	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
loocv	O
will	O
give	O
approximately	O
unbiased	O
estimates	O
of	O
the	O
test	O
error	B
since	O
each	O
training	O
set	B
contains	O
n	O
observations	B
which	O
is	O
almost	O
as	O
many	O
as	O
the	O
number	O
of	O
observations	B
in	O
the	O
full	O
data	B
set	B
and	O
performing	O
k-fold	B
cv	O
for	O
say	O
k	O
or	O
k	O
will	O
lead	O
to	O
an	O
intermediate	O
level	B
of	O
bias	B
since	O
each	O
training	O
set	B
contains	O
observations	B
fewer	O
than	O
in	O
the	O
loocv	O
approach	B
but	O
substantially	O
more	O
than	O
in	O
the	O
validation	B
set	B
approach	B
therefore	O
from	O
the	O
perspective	O
of	O
bias	B
reduction	O
it	O
is	O
clear	O
that	O
loocv	O
is	O
to	O
be	O
preferred	O
to	O
k-fold	B
cv	O
however	O
we	O
know	O
that	O
bias	B
is	O
not	O
the	O
only	O
source	O
for	O
concern	O
in	O
an	O
estimating	O
procedure	O
we	O
must	O
also	O
consider	O
the	O
procedure	O
s	O
variance	B
it	O
turns	O
out	O
that	O
loocv	O
has	O
higher	O
variance	B
than	O
does	O
k-fold	B
cv	O
with	O
k	O
n	O
why	O
is	O
this	O
the	O
case	O
when	O
we	O
perform	O
loocv	O
we	O
are	O
in	O
effect	O
averaging	O
the	O
outputs	O
of	O
n	O
fitted	O
models	O
each	O
of	O
which	O
is	O
trained	O
on	O
an	O
almost	O
identical	O
set	B
of	O
observations	B
therefore	O
these	O
outputs	O
are	O
highly	O
correlated	O
with	O
each	O
other	O
in	O
contrast	B
when	O
we	O
perform	O
k-fold	B
cv	O
with	O
k	O
n	O
we	O
are	O
averaging	O
the	O
outputs	O
of	O
k	O
fitted	O
models	O
that	O
are	O
somewhat	O
less	O
correlated	O
with	O
each	O
other	O
since	O
the	O
overlap	O
between	O
the	O
training	O
sets	O
in	O
each	O
model	B
is	O
smaller	O
since	O
the	O
mean	O
of	O
many	O
highly	O
correlated	O
quantities	O
resampling	B
methods	O
has	O
higher	O
variance	B
than	O
does	O
the	O
mean	O
of	O
many	O
quantities	O
that	O
are	O
not	O
as	O
highly	O
correlated	O
the	O
test	O
error	B
estimate	O
resulting	O
from	O
loocv	O
tends	O
to	O
have	O
higher	O
variance	B
than	O
does	O
the	O
test	O
error	B
estimate	O
resulting	O
from	O
k-fold	B
cv	O
to	O
summarize	O
there	O
is	O
a	O
bias-variance	O
trade-off	B
associated	O
with	O
the	O
choice	O
of	O
k	O
in	O
k-fold	B
cross-validation	B
typically	O
given	O
these	O
considerations	O
one	O
performs	O
k-fold	B
cross-validation	B
using	O
k	O
or	O
k	O
as	O
these	O
values	O
have	O
been	O
shown	O
empirically	O
to	O
yield	O
test	O
error	B
rate	B
estimates	O
that	O
suffer	O
neither	O
from	O
excessively	O
high	O
bias	B
nor	O
from	O
very	O
high	O
variance	B
cross-validation	B
on	O
classification	B
problems	O
in	O
this	O
chapter	O
so	O
far	O
we	O
have	O
illustrated	O
the	O
use	O
of	O
cross-validation	B
in	O
the	O
regression	B
setting	O
where	O
the	O
outcome	O
y	O
is	O
quantitative	B
and	O
so	O
have	O
used	O
mse	B
to	O
quantify	O
test	O
error	B
but	O
cross-validation	B
can	O
also	O
be	O
a	O
very	O
useful	O
approach	B
in	O
the	O
classification	B
setting	O
when	O
y	O
is	O
qualitative	B
in	O
this	O
setting	O
cross-validation	B
works	O
just	O
as	O
described	O
earlier	O
in	O
this	O
chapter	O
except	O
that	O
rather	O
than	O
using	O
mse	B
to	O
quantify	O
test	O
error	B
we	O
instead	O
use	O
the	O
number	O
of	O
misclassified	O
observations	B
for	O
instance	O
in	O
the	O
classification	B
setting	O
the	O
loocv	O
error	B
rate	B
takes	O
the	O
form	O
cvn	O
n	O
erri	O
where	O
erri	O
iyi	O
yi	O
the	O
k-fold	B
cv	O
error	B
rate	B
and	O
validation	B
set	B
error	B
rates	O
are	O
defined	O
analogously	O
as	O
an	O
example	O
we	O
fit	O
various	O
logistic	B
regression	B
models	O
on	O
the	O
twodimensional	O
classification	B
data	B
displayed	O
in	O
figure	O
in	O
the	O
top-left	O
panel	O
of	O
figure	O
the	O
black	O
solid	O
line	B
shows	O
the	O
estimated	O
decision	B
boundary	I
resulting	O
from	O
fitting	O
a	O
standard	O
logistic	B
regression	B
model	B
to	O
this	O
data	B
set	B
since	O
this	O
is	O
simulated	O
data	B
we	O
can	O
compute	O
the	O
true	O
test	O
error	B
rate	B
which	O
takes	O
a	O
value	O
of	O
and	O
so	O
is	O
substantially	O
larger	O
than	O
the	O
bayes	O
error	B
rate	B
of	O
clearly	O
logistic	B
regression	B
does	O
not	O
have	O
enough	O
flexibility	O
to	O
model	B
the	O
bayes	O
decision	B
boundary	I
in	O
this	O
setting	O
we	O
can	O
easily	O
extend	O
logistic	B
regression	B
to	O
obtain	O
a	O
non-linear	B
decision	B
boundary	I
by	O
using	O
polynomial	B
functions	O
of	O
the	O
predictors	O
as	O
we	O
did	O
in	O
the	O
regression	B
setting	O
in	O
section	O
for	O
example	O
we	O
can	O
fit	O
a	O
quadratic	B
logistic	B
regression	B
model	B
given	O
by	O
log	O
p	O
p	O
the	O
top-right	O
panel	O
of	O
figure	O
displays	O
the	O
resulting	O
decision	B
boundary	I
which	O
is	O
now	O
curved	O
however	O
the	O
test	O
error	B
rate	B
has	O
improved	O
only	O
slightly	O
to	O
a	O
much	O
larger	O
improvement	O
is	O
apparent	O
in	O
the	O
bottom-left	O
panel	O
cross-validation	B
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
logistic	B
regression	B
fits	O
on	O
the	O
two-dimensional	O
classification	B
data	B
displayed	O
in	O
figure	O
the	O
bayes	O
decision	B
boundary	I
is	O
represented	O
using	O
a	O
purple	O
dashed	O
line	B
estimated	O
decision	O
boundaries	O
from	O
linear	B
quadratic	B
cubic	B
and	O
quartic	O
logistic	O
regressions	O
are	O
displayed	O
in	O
black	O
the	O
test	O
error	B
rates	O
for	O
the	O
four	O
logistic	B
regression	B
fits	O
are	O
respectively	O
and	O
while	O
the	O
bayes	O
error	B
rate	B
is	O
of	O
figure	O
in	O
which	O
we	O
have	O
fit	O
a	O
logistic	B
regression	B
model	B
involving	O
cubic	B
polynomials	O
of	O
the	O
predictors	O
now	O
the	O
test	O
error	B
rate	B
has	O
decreased	O
to	O
going	O
to	O
a	O
quartic	O
polynomial	B
slightly	O
increases	O
the	O
test	O
error	B
in	O
practice	O
for	O
real	O
data	B
the	O
bayes	O
decision	B
boundary	I
and	O
the	O
test	O
error	B
rates	O
are	O
unknown	O
so	O
how	O
might	O
we	O
decide	O
between	O
the	O
four	O
logistic	B
regression	B
models	O
displayed	O
in	O
figure	O
we	O
can	O
use	O
cross-validation	B
in	O
order	O
to	O
make	O
this	O
decision	O
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
in	O
resampling	B
methods	O
e	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
e	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
order	O
of	O
polynomials	O
used	O
figure	O
test	O
error	B
training	O
error	B
and	O
cv	O
error	B
on	O
the	O
two-dimensional	O
classification	B
data	B
displayed	O
in	O
figure	O
left	O
logistic	B
regression	B
using	O
polynomial	B
functions	O
of	O
the	O
predictors	O
the	O
order	O
of	O
the	O
polynomials	O
used	O
is	O
displayed	O
on	O
the	O
x-axis	O
right	O
the	O
knn	O
classifier	B
with	O
different	O
values	O
of	O
k	O
the	O
number	O
of	O
neighbors	O
used	O
in	O
the	O
knn	O
classifier	B
black	O
the	O
cv	O
error	B
rates	O
that	O
result	O
from	O
fitting	O
ten	O
logistic	B
regression	B
models	O
to	O
the	O
data	B
using	O
polynomial	B
functions	O
of	O
the	O
predictors	O
up	O
to	O
tenth	O
order	O
the	O
true	O
test	O
errors	O
are	O
shown	O
in	O
brown	O
and	O
the	O
training	O
errors	O
are	O
shown	O
in	O
blue	O
as	O
we	O
have	O
seen	O
previously	O
the	O
training	O
error	B
tends	O
to	O
decrease	O
as	O
the	O
flexibility	O
of	O
the	O
fit	O
increases	O
figure	O
indicates	O
that	O
though	O
the	O
training	O
error	B
rate	B
doesn	O
t	O
quite	O
decrease	O
monotonically	O
it	O
tends	O
to	O
decrease	O
on	O
the	O
whole	O
as	O
the	O
model	B
complexity	O
increases	O
in	O
contrast	B
the	O
test	O
error	B
displays	O
a	O
characteristic	O
u-shape	O
the	O
cv	O
error	B
rate	B
provides	O
a	O
pretty	O
good	O
approximation	O
to	O
the	O
test	O
error	B
rate	B
while	O
it	O
somewhat	O
underestimates	O
the	O
error	B
rate	B
it	O
reaches	O
a	O
minimum	O
when	O
fourth-order	O
polynomials	O
are	O
used	O
which	O
is	O
very	O
close	O
to	O
the	O
minimum	O
of	O
the	O
test	O
curve	O
which	O
occurs	O
when	O
third-order	O
polynomials	O
are	O
used	O
in	O
fact	O
using	O
fourth-order	O
polynomials	O
would	O
likely	O
lead	O
to	O
good	O
test	O
set	B
performance	O
as	O
the	O
true	O
test	O
error	B
rate	B
is	O
approximately	O
the	O
same	O
for	O
third	O
fourth	O
fifth	O
and	O
sixth-order	O
polynomials	O
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
the	O
same	O
three	O
curves	O
using	O
the	O
knn	O
approach	B
for	O
classification	B
as	O
a	O
function	B
of	O
the	O
value	O
of	O
k	O
in	O
this	O
context	O
indicates	O
the	O
number	O
of	O
neighbors	O
used	O
in	O
the	O
knn	O
classifier	B
rather	O
than	O
the	O
number	O
of	O
cv	O
folds	O
used	O
again	O
the	O
training	O
error	B
rate	B
declines	O
as	O
the	O
method	O
becomes	O
more	O
flexible	O
and	O
so	O
we	O
see	O
that	O
the	O
training	O
error	B
rate	B
cannot	O
be	O
used	O
to	O
select	O
the	O
optimal	O
value	O
for	O
k	O
though	O
the	O
cross-validation	B
error	B
curve	O
slightly	O
underestimates	O
the	O
test	O
error	B
rate	B
it	O
takes	O
on	O
a	O
minimum	O
very	O
close	O
to	O
the	O
best	O
value	O
for	O
k	O
the	O
bootstrap	B
the	O
bootstrap	B
bootstrap	B
the	O
bootstrap	B
is	O
a	O
widely	O
applicable	O
and	O
extremely	O
powerful	O
statistical	O
tool	O
that	O
can	O
be	O
used	O
to	O
quantify	O
the	O
uncertainty	O
associated	O
with	O
a	O
given	O
estimator	O
or	O
statistical	O
learning	O
method	O
as	O
a	O
simple	B
example	O
the	O
bootstrap	B
can	O
be	O
used	O
to	O
estimate	O
the	O
standard	O
errors	O
of	O
the	O
coefficients	O
from	O
a	O
linear	B
regression	B
fit	O
in	O
the	O
specific	O
case	O
of	O
linear	B
regression	B
this	O
is	O
not	O
particularly	O
useful	O
since	O
we	O
saw	O
in	O
chapter	O
that	O
standard	O
statistical	O
software	O
such	O
as	O
r	O
outputs	O
such	O
standard	O
errors	O
automatically	O
however	O
the	O
power	B
of	O
the	O
bootstrap	B
lies	O
in	O
the	O
fact	O
that	O
it	O
can	O
be	O
easily	O
applied	O
to	O
a	O
wide	O
range	O
of	O
statistical	O
learning	O
methods	O
including	O
some	O
for	O
which	O
a	O
measure	O
of	O
variability	O
is	O
otherwise	O
difficult	O
to	O
obtain	O
and	O
is	O
not	O
automatically	O
output	B
by	O
statistical	O
software	O
in	O
this	O
section	O
we	O
illustrate	O
the	O
bootstrap	B
on	O
a	O
toy	O
example	O
in	O
which	O
we	O
wish	O
to	O
determine	O
the	O
best	O
investment	O
allocation	O
under	O
a	O
simple	B
model	B
in	O
section	O
we	O
explore	O
the	O
use	O
of	O
the	O
bootstrap	B
to	O
assess	O
the	O
variability	O
associated	O
with	O
the	O
regression	B
coefficients	O
in	O
a	O
linear	B
model	B
fit	O
suppose	O
that	O
we	O
wish	O
to	O
invest	O
a	O
fixed	O
sum	O
of	O
money	O
in	O
two	O
financial	O
assets	O
that	O
yield	O
returns	O
of	O
x	O
and	O
y	O
respectively	O
where	O
x	O
and	O
y	O
are	O
random	O
quantities	O
we	O
will	O
invest	O
a	O
fraction	O
of	O
our	O
money	O
in	O
x	O
and	O
will	O
invest	O
the	O
remaining	O
in	O
y	O
since	O
there	O
is	O
variability	O
associated	O
with	O
the	O
returns	O
on	O
these	O
two	O
assets	O
we	O
wish	O
to	O
choose	O
to	O
minimize	O
the	O
total	O
risk	O
or	O
variance	B
of	O
our	O
investment	O
in	O
other	O
words	O
we	O
want	O
to	O
minimize	O
var	O
x	O
one	O
can	O
show	O
that	O
the	O
value	O
that	O
minimizes	O
the	O
risk	O
is	O
given	O
by	O
xy	O
y	O
xy	O
x	O
y	O
where	O
x	O
varx	O
y	O
vary	O
and	O
xy	O
covx	O
y	O
in	O
reality	O
the	O
quantities	O
x	O
y	O
and	O
xy	O
are	O
unknown	O
we	O
can	O
compute	O
estimates	O
for	O
these	O
quantities	O
x	O
y	O
and	O
xy	O
using	O
a	O
data	B
set	B
that	O
contains	O
past	O
measurements	O
for	O
x	O
and	O
y	O
we	O
can	O
then	O
estimate	O
the	O
value	O
of	O
that	O
minimizes	O
the	O
variance	B
of	O
our	O
investment	O
using	O
xy	O
y	O
xy	O
x	O
y	O
figure	O
illustrates	O
this	O
approach	B
for	O
estimating	O
on	O
a	O
simulated	O
data	B
set	B
in	O
each	O
panel	O
we	O
simulated	O
pairs	O
of	O
returns	O
for	O
the	O
investments	O
x	O
and	O
y	O
we	O
used	O
these	O
returns	O
to	O
estimate	O
y	O
and	O
xy	O
which	O
we	O
then	O
substituted	O
into	O
in	O
order	O
to	O
obtain	O
estimates	O
for	O
the	O
value	O
of	O
resulting	O
from	O
each	O
simulated	O
data	B
set	B
ranges	O
from	O
to	O
x	O
it	O
is	O
natural	B
to	O
wish	O
to	O
quantify	O
the	O
accuracy	O
of	O
our	O
estimate	O
of	O
to	O
estimate	O
the	O
standard	O
deviation	O
of	O
we	O
repeated	O
the	O
process	O
of	O
simulating	O
paired	O
observations	B
of	O
x	O
and	O
y	O
and	O
estimating	O
using	O
resampling	B
methods	O
y	O
y	O
y	O
x	O
x	O
y	O
x	O
x	O
figure	O
each	O
panel	O
displays	O
simulated	O
returns	O
for	O
investments	O
x	O
and	O
y	O
from	O
left	O
to	O
right	O
and	O
top	O
to	O
bottom	O
the	O
resulting	O
estimates	O
for	O
are	O
and	O
times	O
we	O
thereby	O
obtained	O
estimates	O
for	O
which	O
we	O
can	O
call	O
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
a	O
histogram	B
of	O
the	O
resulting	O
estimates	O
for	O
these	O
simulations	O
the	O
parameters	O
were	O
set	B
to	O
x	O
y	O
and	O
xy	O
and	O
so	O
we	O
know	O
that	O
the	O
true	O
value	O
of	O
is	O
we	O
indicated	O
this	O
value	O
using	O
a	O
solid	O
vertical	O
line	B
on	O
the	O
histogram	B
the	O
mean	O
over	O
all	O
estimates	O
for	O
is	O
r	O
very	O
close	O
to	O
and	O
the	O
standard	O
deviation	O
of	O
the	O
estimates	O
is	O
r	O
this	O
gives	O
us	O
a	O
very	O
good	O
idea	O
of	O
the	O
accuracy	O
of	O
se	O
so	O
roughly	O
speaking	O
for	O
a	O
random	O
sample	O
from	O
the	O
population	O
we	O
would	O
expect	O
to	O
differ	O
from	O
by	O
approximately	O
on	O
average	B
in	O
practice	O
however	O
the	O
procedure	O
for	O
estimating	O
se	O
outlined	O
above	O
cannot	O
be	O
applied	O
because	O
for	O
real	O
data	B
we	O
cannot	O
generate	O
new	O
samples	O
from	O
the	O
original	O
population	O
however	O
the	O
bootstrap	B
approach	B
allows	O
us	O
to	O
use	O
a	O
computer	O
to	O
emulate	O
the	O
process	O
of	O
obtaining	O
new	O
sample	O
sets	O
the	O
bootstrap	B
true	O
bootstrap	B
figure	O
left	O
a	O
histogram	B
of	O
the	O
estimates	O
of	O
obtained	O
by	O
generating	O
simulated	O
data	B
sets	O
from	O
the	O
true	O
population	O
center	O
a	O
histogram	B
of	O
the	O
estimates	O
of	O
obtained	O
from	O
bootstrap	B
samples	O
from	O
a	O
single	B
data	B
set	B
right	O
the	O
estimates	O
of	O
displayed	O
in	O
the	O
left	O
and	O
center	O
panels	O
are	O
shown	O
as	O
boxplots	O
in	O
each	O
panel	O
the	O
pink	O
line	B
indicates	O
the	O
true	O
value	O
of	O
so	O
that	O
we	O
can	O
estimate	O
the	O
variability	O
of	O
without	O
generating	O
additional	O
samples	O
rather	O
than	O
repeatedly	O
obtaining	O
independent	B
data	B
sets	O
from	O
the	O
population	O
we	O
instead	O
obtain	O
distinct	O
data	B
sets	O
by	O
repeatedly	O
sampling	O
observations	B
from	O
the	O
original	O
data	B
set	B
this	O
approach	B
is	O
illustrated	O
in	O
figure	O
on	O
a	O
simple	B
data	B
set	B
which	O
we	O
call	O
z	O
that	O
contains	O
only	O
n	O
observations	B
we	O
randomly	O
select	O
n	O
observations	B
from	O
the	O
data	B
set	B
in	O
order	O
to	O
produce	O
a	O
bootstrap	B
data	B
set	B
the	O
sampling	O
is	O
performed	O
with	O
replacement	B
which	O
means	O
that	O
the	O
z	O
same	O
observation	O
can	O
occur	O
more	O
than	O
once	O
in	O
the	O
bootstrap	B
data	B
set	B
in	O
contains	O
the	O
third	O
observation	O
twice	O
the	O
first	O
observation	O
this	O
example	O
z	O
once	O
and	O
no	O
instances	O
of	O
the	O
second	O
observation	O
note	O
that	O
if	O
an	O
observation	O
then	O
both	O
its	O
x	O
and	O
y	O
values	O
are	O
included	O
we	O
can	O
use	O
is	O
contained	O
in	O
z	O
this	O
to	O
produce	O
a	O
new	O
bootstrap	B
estimate	O
for	O
which	O
we	O
call	O
z	O
procedure	O
is	O
repeated	O
b	O
times	O
for	O
some	O
large	O
value	O
of	O
b	O
in	O
order	O
to	O
produce	O
b	O
and	O
b	O
corresponding	O
b	O
different	O
bootstrap	B
data	B
sets	O
z	O
b	O
we	O
can	O
compute	O
the	O
standard	B
error	B
of	O
these	O
estimates	O
bootstrap	B
estimates	O
using	O
the	O
formula	O
z	O
z	O
seb	O
b	O
r	O
b	O
this	O
serves	O
as	O
an	O
estimate	O
of	O
the	O
standard	B
error	B
of	O
estimated	O
from	O
the	O
original	O
data	B
set	B
the	O
bootstrap	B
approach	B
is	O
illustrated	O
in	O
the	O
center	O
panel	O
of	O
figure	O
which	O
displays	O
a	O
histogram	B
of	O
bootstrap	B
estimates	O
of	O
each	O
computed	O
using	O
a	O
distinct	O
bootstrap	B
data	B
set	B
this	O
panel	O
was	O
constructed	O
on	O
the	O
basis	B
of	O
a	O
single	B
data	B
set	B
and	O
hence	O
could	O
be	O
created	O
using	O
real	O
data	B
replacement	B
resampling	B
methods	O
obs	O
x	O
y	O
original	O
data	B
zb	O
obs	O
x	O
obs	O
x	O
obs	O
x	O
y	O
y	O
y	O
ab	O
figure	O
a	O
graphical	O
illustration	O
of	O
the	O
bootstrap	B
approach	B
on	O
a	O
small	O
sample	O
containing	O
n	O
observations	B
each	O
bootstrap	B
data	B
set	B
contains	O
n	O
observations	B
sampled	O
with	O
replacement	B
from	O
the	O
original	O
data	B
set	B
each	O
bootstrap	B
data	B
set	B
is	O
used	O
to	O
obtain	O
an	O
estimate	O
of	O
note	O
that	O
the	O
histogram	B
looks	O
very	O
similar	O
to	O
the	O
left-hand	O
panel	O
which	O
displays	O
the	O
idealized	O
histogram	B
of	O
the	O
estimates	O
of	O
obtained	O
by	O
generating	O
simulated	O
data	B
sets	O
from	O
the	O
true	O
population	O
in	O
particular	O
the	O
bootstrap	B
estimate	O
se	O
from	O
is	O
very	O
close	O
to	O
the	O
estimate	O
of	O
obtained	O
using	O
simulated	O
data	B
sets	O
the	O
right-hand	O
panel	O
displays	O
the	O
information	O
in	O
the	O
center	O
and	O
left	O
panels	O
in	O
a	O
different	O
way	O
via	O
boxplots	O
of	O
the	O
estimates	O
for	O
obtained	O
by	O
generating	O
simulated	O
data	B
sets	O
from	O
the	O
true	O
population	O
and	O
using	O
the	O
bootstrap	B
approach	B
again	O
the	O
boxplots	O
are	O
quite	O
similar	O
to	O
each	O
other	O
indicating	O
that	O
the	O
bootstrap	B
approach	B
can	O
be	O
used	O
to	O
effectively	O
estimate	O
the	O
variability	O
associated	O
with	O
lab	O
cross-validation	B
and	O
the	O
bootstrap	B
in	O
this	O
lab	O
we	O
explore	O
the	O
resampling	B
techniques	O
covered	O
in	O
this	O
chapter	O
some	O
of	O
the	O
commands	O
in	O
this	O
lab	O
may	O
take	O
a	O
while	O
to	O
run	O
on	O
your	O
computer	O
lab	O
cross-validation	B
and	O
the	O
bootstrap	B
the	O
validation	B
set	B
approach	B
we	O
explore	O
the	O
use	O
of	O
the	O
validation	B
set	B
approach	B
in	O
order	O
to	O
estimate	O
the	O
test	O
error	B
rates	O
that	O
result	O
from	O
fitting	O
various	O
linear	B
models	O
on	O
the	O
auto	B
data	B
set	B
before	O
we	O
begin	O
we	O
use	O
the	O
set	B
seed	B
function	B
in	O
order	O
to	O
set	B
a	O
seed	B
for	O
r	O
s	O
random	O
number	O
generator	O
so	O
that	O
the	O
reader	O
of	O
this	O
book	O
will	O
obtain	O
precisely	O
the	O
same	O
results	O
as	O
those	O
shown	O
below	O
it	O
is	O
generally	O
a	O
good	O
idea	O
to	O
set	B
a	O
random	O
seed	B
when	O
performing	O
an	O
analysis	B
such	O
as	O
cross-validation	B
that	O
contains	O
an	O
element	O
of	O
randomness	O
so	O
that	O
the	O
results	O
obtained	O
can	O
be	O
reproduced	O
precisely	O
at	O
a	O
later	O
time	O
we	O
begin	O
by	O
using	O
the	O
sample	O
function	B
to	O
split	O
the	O
set	B
of	O
observations	B
into	O
two	O
halves	O
by	O
selecting	O
a	O
random	O
subset	O
of	O
observations	B
out	O
of	O
the	O
original	O
observations	B
we	O
refer	O
to	O
these	O
observations	B
as	O
the	O
training	O
set	B
seed	B
sample	O
library	O
islr	O
set	B
seed	B
train	B
sample	O
we	O
use	O
a	O
shortcut	O
in	O
the	O
sample	O
command	O
see	O
for	O
details	O
we	O
then	O
use	O
the	O
subset	O
option	O
in	O
lm	O
to	O
fit	O
a	O
linear	B
regression	B
using	O
only	O
the	O
observations	B
corresponding	O
to	O
the	O
training	O
set	B
lm	O
fit	O
lm	O
mpg	O
horsepower	O
data	B
auto	B
subset	O
train	B
we	O
now	O
use	O
the	O
predict	O
function	B
to	O
estimate	O
the	O
response	B
for	O
all	O
observations	B
and	O
we	O
use	O
the	O
mean	O
function	B
to	O
calculate	O
the	O
mse	B
of	O
the	O
observations	B
in	O
the	O
validation	B
set	B
note	O
that	O
the	O
index	O
below	O
selects	O
only	O
the	O
observations	B
that	O
are	O
not	O
in	O
the	O
training	O
set	B
attach	O
auto	B
mean	O
mpg	O
predict	O
lm	O
fit	O
auto	B
train	B
therefore	O
the	O
estimated	O
test	O
mse	B
for	O
the	O
linear	B
regression	B
fit	O
is	O
we	O
can	O
use	O
the	O
poly	O
function	B
to	O
estimate	O
the	O
test	O
error	B
for	O
the	O
quadratic	B
and	O
cubic	B
regressions	O
lm	O
lm	O
mpg	O
poly	O
horsepower	O
data	B
auto	B
subset	O
train	B
mean	O
mpg	O
predict	O
lm	O
auto	B
train	B
lm	O
lm	O
mpg	O
poly	O
horsepower	O
data	B
auto	B
subset	O
train	B
mean	O
mpg	O
predict	O
lm	O
auto	B
train	B
these	O
error	B
rates	O
are	O
and	O
respectively	O
if	O
we	O
choose	O
a	O
different	O
training	O
set	B
instead	O
then	O
we	O
will	O
obtain	O
somewhat	O
different	O
errors	O
on	O
the	O
validation	B
set	B
set	B
seed	B
train	B
sample	O
lm	O
fit	O
lm	O
mpg	O
horsepower	O
subset	O
train	B
resampling	B
methods	O
mean	O
mpg	O
predict	O
lm	O
fit	O
auto	B
train	B
lm	O
lm	O
mpg	O
poly	O
horsepower	O
data	B
auto	B
subset	O
train	B
mean	O
mpg	O
predict	O
lm	O
auto	B
train	B
lm	O
lm	O
mpg	O
poly	O
horsepower	O
data	B
auto	B
subset	O
train	B
mean	O
mpg	O
predict	O
lm	O
auto	B
train	B
using	O
this	O
split	O
of	O
the	O
observations	B
into	O
a	O
training	O
set	B
and	O
a	O
validation	B
set	B
we	O
find	O
that	O
the	O
validation	B
set	B
error	B
rates	O
for	O
the	O
models	O
with	O
linear	B
quadratic	B
and	O
cubic	B
terms	O
are	O
and	O
respectively	O
these	O
results	O
are	O
consistent	O
with	O
our	O
previous	O
findings	O
a	O
model	B
that	O
predicts	O
mpg	O
using	O
a	O
quadratic	B
function	B
of	O
horsepower	O
performs	O
better	O
than	O
a	O
model	B
that	O
involves	O
only	O
a	O
linear	B
function	B
of	O
horsepower	O
and	O
there	O
is	O
little	O
evidence	O
in	O
favor	O
of	O
a	O
model	B
that	O
uses	O
a	O
cubic	B
function	B
of	O
horsepower	O
leave-one-out	B
cross-validation	B
the	O
loocv	O
estimate	O
can	O
be	O
automatically	O
computed	O
for	O
any	O
generalized	B
linear	B
model	B
using	O
the	O
glm	O
and	O
cv	O
glm	O
functions	O
in	O
the	O
lab	O
for	O
chapter	O
we	O
used	O
the	O
glm	O
function	B
to	O
perform	O
logistic	B
regression	B
by	O
passing	O
in	O
the	O
familybinomial	O
argument	B
but	O
if	O
we	O
use	O
glm	O
to	O
fit	O
a	O
model	B
without	O
passing	O
in	O
the	O
family	O
argument	B
then	O
it	O
performs	O
linear	B
regression	B
just	O
like	O
the	O
lm	O
function	B
so	O
for	O
instance	O
glm	O
fit	O
glm	O
mpg	O
horsepower	O
data	B
auto	B
cv	O
glm	O
coef	O
glm	O
fit	O
intercept	B
horsepower	O
and	O
lm	O
fit	O
lm	O
mpg	O
horsepower	O
data	B
auto	B
coef	O
lm	O
fit	O
intercept	B
horsepower	O
yield	O
identical	O
linear	B
regression	B
models	O
in	O
this	O
lab	O
we	O
will	O
perform	O
linear	B
regression	B
using	O
the	O
glm	O
function	B
rather	O
than	O
the	O
lm	O
function	B
because	O
the	O
former	O
can	O
be	O
used	O
together	O
with	O
cv	O
the	O
cv	O
function	B
is	O
part	O
of	O
the	O
boot	O
library	O
library	O
boot	O
glm	O
fit	O
glm	O
mpg	O
horsepower	O
data	B
auto	B
cv	O
err	O
cv	O
glm	O
auto	B
glm	O
fit	O
cv	O
errdelta	O
the	O
cv	O
glm	O
function	B
produces	O
a	O
list	O
with	O
several	O
components	O
the	O
two	O
numbers	O
in	O
the	O
delta	O
vector	B
contain	O
the	O
cross-validation	B
results	O
in	O
this	O
lab	O
cross-validation	B
and	O
the	O
bootstrap	B
case	O
the	O
numbers	O
are	O
identical	O
to	O
two	O
decimal	O
places	O
and	O
correspond	O
to	O
the	O
loocv	O
statistic	O
given	O
in	O
below	O
we	O
discuss	O
a	O
situation	O
in	O
which	O
the	O
two	O
numbers	O
differ	O
our	O
cross-validation	B
estimate	O
for	O
the	O
test	O
error	B
is	O
approximately	O
we	O
can	O
repeat	O
this	O
procedure	O
for	O
increasingly	O
complex	O
polynomial	B
fits	O
to	O
automate	O
the	O
process	O
we	O
use	O
the	O
for	O
function	B
to	O
initiate	O
a	O
for	B
loop	I
which	O
iteratively	O
fits	O
polynomial	B
regressions	O
for	O
polynomials	O
of	O
order	O
i	O
to	O
i	O
computes	O
the	O
associated	O
cross-validation	B
error	B
and	O
stores	O
it	O
in	O
the	O
ith	O
element	O
of	O
the	O
vector	B
cv	O
error	B
we	O
begin	O
by	O
initializing	O
the	O
vector	B
this	O
command	O
will	O
likely	O
take	O
a	O
couple	O
of	O
minutes	O
to	O
run	O
for	O
for	B
loop	I
cv	O
error	B
rep	O
for	O
i	O
in	O
glm	O
fit	O
glm	O
mpg	O
poly	O
horsepower	O
i	O
data	B
auto	B
cv	O
error	B
i	O
cv	O
glm	O
auto	B
glm	O
fit	O
cv	O
error	B
as	O
in	O
figure	O
we	O
see	O
a	O
sharp	O
drop	O
in	O
the	O
estimated	O
test	O
mse	B
between	O
the	O
linear	B
and	O
quadratic	B
fits	O
but	O
then	O
no	O
clear	O
improvement	O
from	O
using	O
higher-order	O
polynomials	O
k-fold	B
cross-validation	B
the	O
cv	O
glm	O
function	B
can	O
also	O
be	O
used	O
to	O
implement	O
k-fold	B
cv	O
below	O
we	O
use	O
k	O
a	O
common	O
choice	O
for	O
k	O
on	O
the	O
auto	B
data	B
set	B
we	O
once	O
again	O
set	B
a	O
random	O
seed	B
and	O
initialize	O
a	O
vector	B
in	O
which	O
we	O
will	O
store	O
the	O
cv	O
errors	O
corresponding	O
to	O
the	O
polynomial	B
fits	O
of	O
orders	O
one	O
to	O
ten	O
set	B
seed	B
cv	O
error	B
rep	O
for	O
i	O
in	O
glm	O
fit	O
glm	O
mpg	O
poly	O
horsepower	O
i	O
data	B
auto	B
cv	O
error	B
i	O
cv	O
glm	O
auto	B
glm	O
fit	O
k	O
cv	O
error	B
notice	O
that	O
the	O
computation	O
time	O
is	O
much	O
shorter	O
than	O
that	O
of	O
loocv	O
principle	O
the	O
computation	O
time	O
for	O
loocv	O
for	O
a	O
least	B
squares	I
linear	B
model	B
should	O
be	O
faster	O
than	O
for	O
k-fold	B
cv	O
due	O
to	O
the	O
availability	O
of	O
the	O
formula	O
for	O
loocv	O
however	O
unfortunately	O
the	O
cv	O
glm	O
function	B
does	O
not	O
make	O
use	O
of	O
this	O
formula	O
we	O
still	O
see	O
little	O
evidence	O
that	O
using	O
cubic	B
or	O
higher-order	O
polynomial	B
terms	O
leads	O
to	O
lower	O
test	O
error	B
than	O
simply	O
using	O
a	O
quadratic	B
fit	O
we	O
saw	O
in	O
section	O
that	O
the	O
two	O
numbers	O
associated	O
with	O
delta	O
are	O
essentially	O
the	O
same	O
when	O
loocv	O
is	O
performed	O
when	O
we	O
instead	O
perform	O
k-fold	B
cv	O
then	O
the	O
two	O
numbers	O
associated	O
with	O
delta	O
differ	O
slightly	O
the	O
resampling	B
methods	O
first	O
is	O
the	O
standard	O
k-fold	B
cv	O
estimate	O
as	O
in	O
the	O
second	O
is	O
a	O
biascorrected	O
version	O
on	O
this	O
data	B
set	B
the	O
two	O
estimates	O
are	O
very	O
similar	O
to	O
each	O
other	O
the	O
bootstrap	B
we	O
illustrate	O
the	O
use	O
of	O
the	O
bootstrap	B
in	O
the	O
simple	B
example	O
of	O
section	O
as	O
well	O
as	O
on	O
an	O
example	O
involving	O
estimating	O
the	O
accuracy	O
of	O
the	O
linear	B
regression	B
model	B
on	O
the	O
auto	B
data	B
set	B
estimating	O
the	O
accuracy	O
of	O
a	O
statistic	O
of	O
interest	O
one	O
of	O
the	O
great	O
advantages	O
of	O
the	O
bootstrap	B
approach	B
is	O
that	O
it	O
can	O
be	O
applied	O
in	O
almost	O
all	O
situations	O
no	O
complicated	O
mathematical	O
calculations	O
are	O
required	O
performing	O
a	O
bootstrap	B
analysis	B
in	O
r	O
entails	O
only	O
two	O
steps	O
first	O
we	O
must	O
create	O
a	O
function	B
that	O
computes	O
the	O
statistic	O
of	O
interest	O
second	O
we	O
use	O
the	O
boot	O
function	B
which	O
is	O
part	O
of	O
the	O
boot	O
library	O
to	O
perform	O
the	O
bootstrap	B
by	O
repeatedly	O
sampling	O
observations	B
from	O
the	O
data	B
set	B
with	O
replacement	B
the	O
portfolio	B
data	B
set	B
in	O
the	O
islr	O
package	O
is	O
described	O
in	O
section	O
to	O
illustrate	O
the	O
use	O
of	O
the	O
bootstrap	B
on	O
this	O
data	B
we	O
must	O
first	O
create	O
a	O
function	B
alpha	O
fn	O
which	O
takes	O
as	O
input	B
the	O
y	O
data	B
as	O
well	O
as	O
a	O
vector	B
indicating	O
which	O
observations	B
should	O
be	O
used	O
to	O
estimate	O
the	O
function	B
then	O
outputs	O
the	O
estimate	O
for	O
based	O
on	O
the	O
selected	O
observations	B
alpha	O
fn	O
function	B
data	B
index	O
x	O
datax	O
index	O
y	O
datay	O
index	O
return	O
var	O
y	O
cov	O
y	O
var	O
x	O
var	O
y	O
cov	O
y	O
this	O
function	B
returns	O
or	O
outputs	O
an	O
estimate	O
for	O
based	O
on	O
applying	O
to	O
the	O
observations	B
indexed	O
by	O
the	O
argument	B
index	O
for	O
instance	O
the	O
following	O
command	O
tells	O
r	O
to	O
estimate	O
using	O
all	O
observations	B
alpha	O
fn	O
portfolio	B
the	O
next	O
command	O
uses	O
the	O
sample	O
function	B
to	O
randomly	O
select	O
observations	B
from	O
the	O
range	O
to	O
with	O
replacement	B
this	O
is	O
equivalent	O
to	O
constructing	O
a	O
new	O
bootstrap	B
data	B
set	B
and	O
recomputing	O
based	O
on	O
the	O
new	O
data	B
set	B
set	B
seed	B
alpha	O
fn	O
portfolio	B
sample	O
replace	O
t	O
we	O
can	O
implement	O
a	O
bootstrap	B
analysis	B
by	O
performing	O
this	O
command	O
many	O
times	O
recording	O
all	O
of	O
the	O
corresponding	O
estimates	O
for	O
and	O
computing	O
boot	O
lab	O
cross-validation	B
and	O
the	O
bootstrap	B
the	O
resulting	O
standard	O
deviation	O
however	O
the	O
boot	O
function	B
automates	O
this	O
approach	B
below	O
we	O
produce	O
r	O
bootstrap	B
estimates	O
for	O
boot	O
boot	O
portfolio	B
alpha	O
fn	O
r	O
ordinary	O
n	O
o	O
n	O
p	O
a	O
r	O
a	O
m	O
e	O
t	O
r	O
i	O
c	O
bootstrap	B
call	O
boot	O
data	B
portfolio	B
statistic	O
alpha	O
fn	O
r	O
bootstrap	B
statistics	O
original	O
bias	B
e	O
std	O
error	B
the	O
final	O
output	B
shows	O
that	O
using	O
the	O
original	O
data	B
and	O
that	O
the	O
bootstrap	B
estimate	O
for	O
se	O
is	O
estimating	O
the	O
accuracy	O
of	O
a	O
linear	B
regression	B
model	B
the	O
bootstrap	B
approach	B
can	O
be	O
used	O
to	O
assess	O
the	O
variability	O
of	O
the	O
coefficient	O
estimates	O
and	O
predictions	O
from	O
a	O
statistical	O
learning	O
method	O
here	O
we	O
use	O
the	O
bootstrap	B
approach	B
in	O
order	O
to	O
assess	O
the	O
variability	O
of	O
the	O
estimates	O
for	O
and	O
the	O
intercept	B
and	O
slope	B
terms	O
for	O
the	O
linear	B
regression	B
model	B
that	O
uses	O
horsepower	O
to	O
predict	O
mpg	O
in	O
the	O
auto	B
data	B
set	B
we	O
will	O
compare	O
the	O
estimates	O
obtained	O
using	O
the	O
bootstrap	B
to	O
those	O
obtained	O
using	O
the	O
formulas	O
for	O
se	O
and	O
se	O
described	O
in	O
section	O
we	O
first	O
create	O
a	O
simple	B
function	B
boot	O
fn	O
which	O
takes	O
in	O
the	O
auto	B
data	B
set	B
as	O
well	O
as	O
a	O
set	B
of	O
indices	O
for	O
the	O
observations	B
and	O
returns	O
the	O
intercept	B
and	O
slope	B
estimates	O
for	O
the	O
linear	B
regression	B
model	B
we	O
then	O
apply	O
this	O
function	B
to	O
the	O
full	O
set	B
of	O
observations	B
in	O
order	O
to	O
compute	O
the	O
estimates	O
of	O
and	O
on	O
the	O
entire	O
data	B
set	B
using	O
the	O
usual	O
linear	B
regression	B
coefficient	O
estimate	O
formulas	O
from	O
chapter	O
note	O
that	O
we	O
do	O
not	O
need	O
the	O
and	O
at	O
the	O
beginning	O
and	O
end	O
of	O
the	O
function	B
because	O
it	O
is	O
only	O
one	O
line	B
long	O
boot	O
fn	O
function	B
data	B
index	O
return	O
coef	O
lm	O
mpg	O
horsepower	O
data	B
data	B
subset	O
index	O
boot	O
fn	O
auto	B
intercept	B
horsepower	O
the	O
boot	O
fn	O
function	B
can	O
also	O
be	O
used	O
in	O
order	O
to	O
create	O
bootstrap	B
estimates	O
for	O
the	O
intercept	B
and	O
slope	B
terms	O
by	O
randomly	O
sampling	O
from	O
among	O
the	O
observations	B
with	O
replacement	B
here	O
we	O
give	O
two	O
examples	O
set	B
seed	B
boot	O
fn	O
auto	B
sample	O
replace	O
t	O
intercept	B
horsepower	O
boot	O
fn	O
auto	B
sample	O
replace	O
t	O
intercept	B
horsepower	O
resampling	B
methods	O
next	O
we	O
use	O
the	O
boot	O
function	B
to	O
compute	O
the	O
standard	O
errors	O
of	O
bootstrap	B
estimates	O
for	O
the	O
intercept	B
and	O
slope	B
terms	O
boot	O
auto	B
boot	O
fn	O
ordinary	O
n	O
o	O
n	O
p	O
a	O
r	O
a	O
m	O
e	O
t	O
r	O
i	O
c	O
bootstrap	B
call	O
boot	O
data	B
auto	B
statistic	O
boot	O
fn	O
r	O
bootstrap	B
statistics	O
original	O
bias	B
std	O
error	B
this	O
indicates	O
that	O
the	O
bootstrap	B
estimate	O
for	O
se	O
is	O
and	O
that	O
the	O
bootstrap	B
estimate	O
for	O
se	O
is	O
as	O
discussed	O
in	O
section	O
standard	O
formulas	O
can	O
be	O
used	O
to	O
compute	O
the	O
standard	O
errors	O
for	O
the	O
regression	B
coefficients	O
in	O
a	O
linear	B
model	B
these	O
can	O
be	O
obtained	O
using	O
the	O
summary	O
function	B
summary	O
lm	O
mpg	O
horsepower	O
data	B
auto	B
estimate	O
std	O
error	B
t	O
value	O
intercept	B
horsepowe	O
r	O
pr	O
t	O
e	O
e	O
the	O
standard	B
error	B
estimates	O
for	O
and	O
obtained	O
using	O
the	O
formulas	O
from	O
section	O
are	O
for	O
the	O
intercept	B
and	O
for	O
the	O
slope	B
interestingly	O
these	O
are	O
somewhat	O
different	O
from	O
the	O
estimates	O
obtained	O
using	O
the	O
bootstrap	B
does	O
this	O
indicate	O
a	O
problem	O
with	O
the	O
bootstrap	B
in	O
fact	O
it	O
suggests	O
the	O
opposite	O
recall	B
that	O
the	O
standard	O
formulas	O
given	O
in	O
equation	O
on	O
page	O
rely	O
on	O
certain	O
assumptions	O
for	O
example	O
they	O
depend	O
on	O
the	O
unknown	O
parameter	B
the	O
noise	B
variance	B
we	O
then	O
estimate	O
using	O
the	O
rss	O
now	O
although	O
the	O
formula	O
for	O
the	O
standard	O
errors	O
do	O
not	O
rely	O
on	O
the	O
linear	B
model	B
being	O
correct	O
the	O
estimate	O
for	O
does	O
we	O
see	O
in	O
figure	O
on	O
page	O
that	O
there	O
is	O
a	O
non-linear	B
relationship	O
in	O
the	O
data	B
and	O
so	O
the	O
residuals	B
from	O
a	O
linear	B
fit	O
will	O
be	O
inflated	O
and	O
so	O
will	O
secondly	O
the	O
standard	O
formulas	O
assume	O
unrealistically	O
that	O
the	O
xi	O
are	O
fixed	O
and	O
all	O
the	O
variability	O
comes	O
from	O
the	O
variation	O
in	O
the	O
errors	O
the	O
bootstrap	B
approach	B
does	O
not	O
rely	O
on	O
any	O
of	O
these	O
assumptions	O
and	O
so	O
it	O
is	O
likely	O
giving	O
a	O
more	O
accurate	O
estimate	O
of	O
the	O
standard	O
errors	O
of	O
and	O
than	O
is	O
the	O
summary	O
function	B
below	O
we	O
compute	O
the	O
bootstrap	B
standard	B
error	B
estimates	O
and	O
the	O
standard	O
linear	B
regression	B
estimates	O
that	O
result	O
from	O
fitting	O
the	O
quadratic	B
model	B
to	O
the	O
data	B
since	O
this	O
model	B
provides	O
a	O
good	O
fit	O
to	O
the	O
data	B
there	O
is	O
now	O
a	O
better	O
correspondence	O
between	O
the	O
bootstrap	B
estimates	O
and	O
the	O
standard	O
estimates	O
of	O
se	O
se	O
and	O
se	O
exercises	O
boot	O
fn	O
function	B
data	B
index	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
lm	O
mpg	O
horsepowe	O
r	O
i	O
horsepower	O
data	B
data	B
subset	O
index	O
set	B
seed	B
boot	O
auto	B
boot	O
fn	O
ordinary	O
n	O
o	O
n	O
p	O
a	O
r	O
a	O
m	O
e	O
t	O
r	O
i	O
c	O
bootstrap	B
call	O
boot	O
data	B
auto	B
statistic	O
boot	O
fn	O
r	O
bootstrap	B
statistics	O
original	O
bias	B
std	O
error	B
e	O
e	O
e	O
summary	O
lm	O
mpg	O
horsepowe	O
r	O
i	O
horsepower	O
data	B
auto	B
intercept	B
horsepowe	O
r	O
i	O
horsepowe	O
r	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
e	O
e	O
e	O
exercises	O
conceptual	O
using	O
basic	O
statistical	O
properties	O
of	O
the	O
variance	B
as	O
well	O
as	O
singlevariable	O
calculus	O
derive	O
in	O
other	O
words	O
prove	O
that	O
given	O
by	O
does	O
indeed	O
minimize	O
var	O
x	O
we	O
will	O
now	O
derive	O
the	O
probability	B
that	O
a	O
given	O
observation	O
is	O
part	O
of	O
a	O
bootstrap	B
sample	O
suppose	O
that	O
we	O
obtain	O
a	O
bootstrap	B
sample	O
from	O
a	O
set	B
of	O
n	O
observations	B
what	O
is	O
the	O
probability	B
that	O
the	O
first	O
bootstrap	B
observation	O
is	O
not	O
the	O
jth	O
observation	O
from	O
the	O
original	O
sample	O
justify	O
your	O
answer	O
what	O
is	O
the	O
probability	B
that	O
the	O
second	O
bootstrap	B
observation	O
argue	O
that	O
the	O
probability	B
that	O
the	O
jth	O
observation	O
is	O
not	O
in	O
the	O
is	O
not	O
the	O
jth	O
observation	O
from	O
the	O
original	O
sample	O
bootstrap	B
sample	O
is	O
when	O
n	O
what	O
is	O
the	O
probability	B
that	O
the	O
jth	O
observation	O
is	O
in	O
the	O
bootstrap	B
sample	O
when	O
n	O
what	O
is	O
the	O
probability	B
that	O
the	O
jth	O
observation	O
is	O
in	O
the	O
bootstrap	B
sample	O
resampling	B
methods	O
when	O
n	O
what	O
is	O
the	O
probability	B
that	O
the	O
jth	O
observa	O
tion	O
is	O
in	O
the	O
bootstrap	B
sample	O
create	O
a	O
plot	B
that	O
displays	O
for	O
each	O
integer	O
value	O
of	O
n	O
from	O
to	O
the	O
probability	B
that	O
the	O
jth	O
observation	O
is	O
in	O
the	O
bootstrap	B
sample	O
comment	O
on	O
what	O
you	O
observe	O
we	O
will	O
now	O
investigate	O
numerically	O
the	O
probability	B
that	O
a	O
bootstrap	B
sample	O
of	O
size	O
n	O
contains	O
the	O
jth	O
observation	O
here	O
j	O
we	O
repeatedly	O
create	O
bootstrap	B
samples	O
and	O
each	O
time	O
we	O
record	O
whether	O
or	O
not	O
the	O
fourth	O
observation	O
is	O
contained	O
in	O
the	O
bootstrap	B
sample	O
store	O
rep	O
na	O
for	O
i	O
in	O
store	O
i	O
sum	O
sample	O
rep	O
true	O
mean	O
store	O
comment	O
on	O
the	O
results	O
obtained	O
we	O
now	O
review	O
k-fold	B
cross-validation	B
explain	O
how	O
k-fold	B
cross-validation	B
is	O
implemented	O
what	O
are	O
the	O
advantages	O
and	O
disadvantages	O
of	O
k-fold	B
cross	O
validation	O
relative	O
to	O
i	O
the	O
validation	B
set	B
approach	B
ii	O
loocv	O
suppose	O
that	O
we	O
use	O
some	O
statistical	O
learning	O
method	O
to	O
make	O
a	O
prediction	B
for	O
the	O
response	B
y	O
for	O
a	O
particular	O
value	O
of	O
the	O
predictor	B
x	O
carefully	O
describe	O
how	O
we	O
might	O
estimate	O
the	O
standard	O
deviation	O
of	O
our	O
prediction	B
applied	O
in	O
chapter	O
we	O
used	O
logistic	B
regression	B
to	O
predict	O
the	O
probability	B
of	O
default	B
using	O
income	B
and	O
balance	O
on	O
the	O
default	B
data	B
set	B
we	O
will	O
now	O
estimate	O
the	O
test	O
error	B
of	O
this	O
logistic	B
regression	B
model	B
using	O
the	O
validation	B
set	B
approach	B
do	O
not	O
forget	O
to	O
set	B
a	O
random	O
seed	B
before	O
beginning	O
your	O
analysis	B
fit	O
a	O
logistic	B
regression	B
model	B
that	O
uses	O
income	B
and	O
balance	O
to	O
predict	O
default	B
using	O
the	O
validation	B
set	B
approach	B
estimate	O
the	O
test	O
error	B
of	O
this	O
model	B
in	O
order	O
to	O
do	O
this	O
you	O
must	O
perform	O
the	O
following	O
steps	O
i	O
split	O
the	O
sample	O
set	B
into	O
a	O
training	O
set	B
and	O
a	O
validation	B
set	B
exercises	O
ii	O
fit	O
a	O
multiple	B
logistic	B
regression	B
model	B
using	O
only	O
the	O
train	B
ing	O
observations	B
iii	O
obtain	O
a	O
prediction	B
of	O
default	B
status	O
for	O
each	O
individual	O
in	O
the	O
validation	B
set	B
by	O
computing	O
the	O
posterior	O
probability	B
of	O
default	B
for	O
that	O
individual	O
and	O
classifying	O
the	O
individual	O
to	O
the	O
default	B
category	O
if	O
the	O
posterior	O
probability	B
is	O
greater	O
than	O
iv	O
compute	O
the	O
validation	B
set	B
error	B
which	O
is	O
the	O
fraction	O
of	O
the	O
observations	B
in	O
the	O
validation	B
set	B
that	O
are	O
misclassified	O
repeat	O
the	O
process	O
in	O
three	O
times	O
using	O
three	O
different	O
splits	O
of	O
the	O
observations	B
into	O
a	O
training	O
set	B
and	O
a	O
validation	B
set	B
comment	O
on	O
the	O
results	O
obtained	O
now	O
consider	O
a	O
logistic	B
regression	B
model	B
that	O
predicts	O
the	O
probability	B
of	O
default	B
using	O
income	B
balance	O
and	O
a	O
dummy	B
variable	B
for	O
student	O
estimate	O
the	O
test	O
error	B
for	O
this	O
model	B
using	O
the	O
validation	B
set	B
approach	B
comment	O
on	O
whether	O
or	O
not	O
including	O
a	O
dummy	B
variable	B
for	O
student	O
leads	O
to	O
a	O
reduction	O
in	O
the	O
test	O
error	B
rate	B
we	O
continue	O
to	O
consider	O
the	O
use	O
of	O
a	O
logistic	B
regression	B
model	B
to	O
predict	O
the	O
probability	B
of	O
default	B
using	O
income	B
and	O
balance	O
on	O
the	O
default	B
data	B
set	B
in	O
particular	O
we	O
will	O
now	O
compute	O
estimates	O
for	O
the	O
standard	O
errors	O
of	O
the	O
income	B
and	O
balance	O
logistic	B
regression	B
coefficients	O
in	O
two	O
different	O
ways	O
using	O
the	O
bootstrap	B
and	O
using	O
the	O
standard	O
formula	O
for	O
computing	O
the	O
standard	O
errors	O
in	O
the	O
glm	O
function	B
do	O
not	O
forget	O
to	O
set	B
a	O
random	O
seed	B
before	O
beginning	O
your	O
analysis	B
using	O
the	O
summary	O
and	O
glm	O
functions	O
determine	O
the	O
estimated	O
standard	O
errors	O
for	O
the	O
coefficients	O
associated	O
with	O
income	B
and	O
balance	O
in	O
a	O
multiple	B
logistic	B
regression	B
model	B
that	O
uses	O
both	O
predictors	O
write	O
a	O
function	B
boot	O
fn	O
that	O
takes	O
as	O
input	B
the	O
default	B
data	B
set	B
as	O
well	O
as	O
an	O
index	O
of	O
the	O
observations	B
and	O
that	O
outputs	O
the	O
coefficient	O
estimates	O
for	O
income	B
and	O
balance	O
in	O
the	O
multiple	B
logistic	B
regression	B
model	B
use	O
the	O
boot	O
function	B
together	O
with	O
your	O
boot	O
fn	O
function	B
to	O
estimate	O
the	O
standard	O
errors	O
of	O
the	O
logistic	B
regression	B
coefficients	O
for	O
income	B
and	O
balance	O
comment	O
on	O
the	O
estimated	O
standard	O
errors	O
obtained	O
using	O
the	O
glm	O
function	B
and	O
using	O
your	O
bootstrap	B
function	B
in	O
sections	O
and	O
we	O
saw	O
that	O
the	O
cv	O
glm	O
function	B
can	O
be	O
used	O
in	O
order	O
to	O
compute	O
the	O
loocv	O
test	O
error	B
estimate	O
alternatively	O
one	O
could	O
compute	O
those	O
quantities	O
using	O
just	O
the	O
glm	O
and	O
resampling	B
methods	O
predict	O
glm	O
functions	O
and	O
a	O
for	B
loop	I
you	O
will	O
now	O
take	O
this	O
approach	B
in	O
order	O
to	O
compute	O
the	O
loocv	O
error	B
for	O
a	O
simple	B
logistic	B
regression	B
model	B
on	O
the	O
weekly	B
data	B
set	B
recall	B
that	O
in	O
the	O
context	O
of	O
classification	B
problems	O
the	O
loocv	O
error	B
is	O
given	O
in	O
fit	O
a	O
logistic	B
regression	B
model	B
that	O
predicts	O
direction	O
using	O
and	O
fit	O
a	O
logistic	B
regression	B
model	B
that	O
predicts	O
direction	O
using	O
and	O
using	O
all	O
but	O
the	O
first	O
observation	O
use	O
the	O
model	B
from	O
to	O
predict	O
the	O
direction	O
of	O
the	O
first	O
observation	O
you	O
can	O
do	O
this	O
by	O
predicting	O
that	O
the	O
first	O
observation	O
will	O
go	O
up	O
if	O
p	O
was	O
this	O
observation	O
correctly	O
classified	O
write	O
a	O
for	B
loop	I
from	O
i	O
to	O
i	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
observations	B
in	O
the	O
data	B
set	B
that	O
performs	O
each	O
of	O
the	O
following	O
steps	O
i	O
fit	O
a	O
logistic	B
regression	B
model	B
using	O
all	O
but	O
the	O
ith	O
obser	O
vation	O
to	O
predict	O
direction	O
using	O
and	O
ii	O
compute	O
the	O
posterior	O
probability	B
of	O
the	O
market	O
moving	O
up	O
for	O
the	O
ith	O
observation	O
iii	O
use	O
the	O
posterior	O
probability	B
for	O
the	O
ith	O
observation	O
in	O
order	O
to	O
predict	O
whether	O
or	O
not	O
the	O
market	O
moves	O
up	O
iv	O
determine	O
whether	O
or	O
not	O
an	O
error	B
was	O
made	O
in	O
predicting	O
the	O
direction	O
for	O
the	O
ith	O
observation	O
if	O
an	O
error	B
was	O
made	O
then	O
indicate	O
this	O
as	O
a	O
and	O
otherwise	O
indicate	O
it	O
as	O
a	O
take	O
the	O
average	B
of	O
the	O
n	O
numbers	O
obtained	O
in	O
in	O
order	O
to	O
obtain	O
the	O
loocv	O
estimate	O
for	O
the	O
test	O
error	B
comment	O
on	O
the	O
results	O
we	O
will	O
now	O
perform	O
cross-validation	B
on	O
a	O
simulated	O
data	B
set	B
generate	O
a	O
simulated	O
data	B
set	B
as	O
follows	O
set	B
seed	B
x	O
rnorm	O
y	O
x	O
rnorm	O
in	O
this	O
data	B
set	B
what	O
is	O
n	O
and	O
what	O
is	O
p	O
write	O
out	O
the	O
model	B
used	O
to	O
generate	O
the	O
data	B
in	O
equation	O
form	O
create	O
a	O
scatterplot	B
of	O
x	O
against	O
y	O
comment	O
on	O
what	O
you	O
find	O
set	B
a	O
random	O
seed	B
and	O
then	O
compute	O
the	O
loocv	O
errors	O
that	O
result	O
from	O
fitting	O
the	O
following	O
four	O
models	O
using	O
least	B
squares	I
exercises	O
i	O
y	O
ii	O
y	O
iii	O
y	O
iv	O
y	O
note	O
you	O
may	O
find	O
it	O
helpful	O
to	O
use	O
the	O
data	B
frame	I
function	B
to	O
create	O
a	O
single	B
data	B
set	B
containing	O
both	O
x	O
and	O
y	O
repeat	O
using	O
another	O
random	O
seed	B
and	O
report	O
your	O
results	O
are	O
your	O
results	O
the	O
same	O
as	O
what	O
you	O
got	O
in	O
why	O
which	O
of	O
the	O
models	O
in	O
had	O
the	O
smallest	O
loocv	O
error	B
is	O
this	O
what	O
you	O
expected	O
explain	O
your	O
answer	O
comment	O
on	O
the	O
statistical	O
significance	O
of	O
the	O
coefficient	O
estimates	O
that	O
results	O
from	O
fitting	O
each	O
of	O
the	O
models	O
in	O
using	O
least	B
squares	I
do	O
these	O
results	O
agree	O
with	O
the	O
conclusions	O
drawn	O
based	O
on	O
the	O
cross-validation	B
results	O
we	O
will	O
now	O
consider	O
the	O
boston	B
housing	O
data	B
set	B
from	O
the	O
mass	O
library	O
based	O
on	O
this	O
data	B
set	B
provide	O
an	O
estimate	O
for	O
the	O
population	O
mean	O
of	O
medv	O
call	O
this	O
estimate	O
provide	O
an	O
estimate	O
of	O
the	O
standard	B
error	B
of	O
interpret	O
this	O
result	O
hint	O
we	O
can	O
compute	O
the	O
standard	B
error	B
of	O
the	O
sample	O
mean	O
by	O
dividing	O
the	O
sample	O
standard	O
deviation	O
by	O
the	O
square	O
root	O
of	O
the	O
number	O
of	O
observations	B
now	O
estimate	O
the	O
standard	B
error	B
of	O
using	O
the	O
bootstrap	B
how	O
does	O
this	O
compare	O
to	O
your	O
answer	O
from	O
based	O
on	O
your	O
bootstrap	B
estimate	O
from	O
provide	O
a	O
confidence	B
interval	B
for	O
the	O
mean	O
of	O
medv	O
compare	O
it	O
to	O
the	O
results	O
obtained	O
using	O
t	O
testbostonmedv	O
hint	O
you	O
can	O
approximate	O
a	O
confidence	B
interval	B
using	O
the	O
formula	O
based	O
on	O
this	O
data	B
set	B
provide	O
an	O
estimate	O
med	O
for	O
the	O
median	O
value	O
of	O
medv	O
in	O
the	O
population	O
we	O
now	O
would	O
like	O
to	O
estimate	O
the	O
standard	B
error	B
of	O
med	O
unfortunately	O
there	O
is	O
no	O
simple	B
formula	O
for	O
computing	O
the	O
standard	B
error	B
of	O
the	O
median	O
instead	O
estimate	O
the	O
standard	B
error	B
of	O
the	O
median	O
using	O
the	O
bootstrap	B
comment	O
on	O
your	O
findings	O
based	O
on	O
this	O
data	B
set	B
provide	O
an	O
estimate	O
for	O
the	O
tenth	O
percentile	O
of	O
medv	O
in	O
boston	B
suburbs	O
call	O
this	O
quantity	O
can	O
use	O
the	O
quantile	O
function	B
use	O
the	O
bootstrap	B
to	O
estimate	O
the	O
standard	B
error	B
of	O
com	O
ment	O
on	O
your	O
findings	O
linear	B
model	B
selection	B
and	O
regularization	B
in	O
the	O
regression	B
setting	O
the	O
standard	O
linear	B
model	B
y	O
pxp	O
is	O
commonly	O
used	O
to	O
describe	O
the	O
relationship	O
between	O
a	O
response	B
y	O
and	O
a	O
set	B
of	O
variables	O
xp	O
we	O
have	O
seen	O
in	O
chapter	O
that	O
one	O
typically	O
fits	O
this	O
model	B
using	O
least	B
squares	I
in	O
the	O
chapters	O
that	O
follow	O
we	O
consider	O
some	O
approaches	O
for	O
extending	O
the	O
linear	B
model	B
framework	O
in	O
chapter	O
we	O
generalize	O
in	O
order	O
to	O
accommodate	O
non-linear	B
but	O
still	O
additive	B
relationships	O
while	O
in	O
chapter	O
we	O
consider	O
even	O
more	O
general	O
non-linear	B
models	O
however	O
the	O
linear	B
model	B
has	O
distinct	O
advantages	O
in	O
terms	O
of	O
inference	B
and	O
on	O
real-world	O
problems	O
is	O
often	O
surprisingly	O
competitive	O
in	O
relation	O
to	O
non-linear	B
methods	O
hence	O
before	O
moving	O
to	O
the	O
non-linear	B
world	O
we	O
discuss	O
in	O
this	O
chapter	O
some	O
ways	O
in	O
which	O
the	O
simple	B
linear	B
model	B
can	O
be	O
improved	O
by	O
replacing	O
plain	O
least	B
squares	I
fitting	O
with	O
some	O
alternative	O
fitting	O
procedures	O
why	O
might	O
we	O
want	O
to	O
use	O
another	O
fitting	O
procedure	O
instead	O
of	O
least	B
squares	I
as	O
we	O
will	O
see	O
alternative	O
fitting	O
procedures	O
can	O
yield	O
better	O
prediction	B
accuracy	O
and	O
model	B
interpretability	B
prediction	B
accuracy	O
provided	O
that	O
the	O
true	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictors	O
is	O
approximately	O
linear	B
the	O
least	B
squares	I
estimates	O
will	O
have	O
low	O
bias	B
if	O
n	O
p	O
that	O
is	O
if	O
n	O
the	O
number	O
of	O
observations	B
is	O
much	O
larger	O
than	O
p	O
the	O
number	O
of	O
variables	O
then	O
the	O
least	B
squares	I
estimates	O
tend	O
to	O
also	O
have	O
low	O
variance	B
and	O
hence	O
will	O
perform	O
well	O
on	O
test	O
observations	B
however	O
if	O
n	O
is	O
not	O
much	O
larger	O
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
linear	B
model	B
selection	B
and	O
regularization	B
than	O
p	O
then	O
there	O
can	O
be	O
a	O
lot	O
of	O
variability	O
in	O
the	O
least	B
squares	I
fit	O
resulting	O
in	O
overfitting	B
and	O
consequently	O
poor	O
predictions	O
on	O
future	O
observations	B
not	O
used	O
in	O
model	B
training	O
and	O
if	O
p	O
n	O
then	O
there	O
is	O
no	O
longer	O
a	O
unique	O
least	B
squares	I
coefficient	O
estimate	O
the	O
variance	B
is	O
infinite	O
so	O
the	O
method	O
cannot	O
be	O
used	O
at	O
all	O
by	O
constraining	O
or	O
shrinking	O
the	O
estimated	O
coefficients	O
we	O
can	O
often	O
substantially	O
reduce	O
the	O
variance	B
at	O
the	O
cost	O
of	O
a	O
negligible	O
increase	O
in	O
bias	B
this	O
can	O
lead	O
to	O
substantial	O
improvements	O
in	O
the	O
accuracy	O
with	O
which	O
we	O
can	O
predict	O
the	O
response	B
for	O
observations	B
not	O
used	O
in	O
model	B
training	O
model	B
interpretability	B
it	O
is	O
often	O
the	O
case	O
that	O
some	O
or	O
many	O
of	O
the	O
variables	O
used	O
in	O
a	O
multiple	B
regression	B
model	B
are	O
in	O
fact	O
not	O
associated	O
with	O
the	O
response	B
including	O
such	O
irrelevant	O
variables	O
leads	O
to	O
unnecessary	O
complexity	O
in	O
the	O
resulting	O
model	B
by	O
removing	O
these	O
variables	O
that	O
is	O
by	O
setting	O
the	O
corresponding	O
coefficient	O
estimates	O
to	O
zero	O
we	O
can	O
obtain	O
a	O
model	B
that	O
is	O
more	O
easily	O
interpreted	O
now	O
least	B
squares	I
is	O
extremely	O
unlikely	O
to	O
yield	O
any	O
coefficient	O
estimates	O
that	O
are	O
exactly	O
zero	O
in	O
this	O
chapter	O
we	O
see	O
some	O
approaches	O
for	O
automatically	O
performing	O
feature	B
selection	B
or	O
variable	B
selection	B
that	O
is	O
for	O
excluding	O
irrelevant	O
variables	O
from	O
a	O
multiple	B
regression	B
model	B
there	O
are	O
many	O
alternatives	O
both	O
classical	O
and	O
modern	O
to	O
using	O
least	B
squares	I
to	O
fit	O
in	O
this	O
chapter	O
we	O
discuss	O
three	O
important	O
classes	O
of	O
methods	O
subset	B
selection	B
this	O
approach	B
involves	O
identifying	O
a	O
subset	O
of	O
the	O
p	O
predictors	O
that	O
we	O
believe	O
to	O
be	O
related	O
to	O
the	O
response	B
we	O
then	O
fit	O
a	O
model	B
using	O
least	B
squares	I
on	O
the	O
reduced	O
set	B
of	O
variables	O
shrinkage	B
this	O
approach	B
involves	O
fitting	O
a	O
model	B
involving	O
all	O
p	O
predictors	O
however	O
the	O
estimated	O
coefficients	O
are	O
shrunken	O
towards	O
zero	O
relative	O
to	O
the	O
least	B
squares	I
estimates	O
this	O
shrinkage	B
known	O
as	O
regularization	B
has	O
the	O
effect	O
of	O
reducing	O
variance	B
depending	O
on	O
what	O
type	O
of	O
shrinkage	B
is	O
performed	O
some	O
of	O
the	O
coefficients	O
may	O
be	O
estimated	O
to	O
be	O
exactly	O
zero	O
hence	O
shrinkage	B
methods	O
can	O
also	O
perform	O
variable	B
selection	B
dimension	B
reduction	I
this	O
approach	B
involves	O
projecting	O
the	O
p	O
predictors	O
into	O
a	O
m	O
subspace	O
where	O
m	O
p	O
this	O
is	O
achieved	O
by	O
computing	O
m	O
different	O
linear	B
combinations	O
or	O
projections	O
of	O
the	O
variables	O
then	O
these	O
m	O
projections	O
are	O
used	O
as	O
predictors	O
to	O
fit	O
a	O
linear	B
regression	B
model	B
by	O
least	B
squares	I
in	O
the	O
following	O
sections	O
we	O
describe	O
each	O
of	O
these	O
approaches	O
in	O
greater	O
detail	O
along	O
with	O
their	O
advantages	O
and	O
disadvantages	O
although	O
this	O
chapter	O
describes	O
extensions	O
and	O
modifications	O
to	O
the	O
linear	B
model	B
for	O
regression	B
seen	O
in	O
chapter	O
the	O
same	O
concepts	O
apply	O
to	O
other	O
methods	O
such	O
as	O
the	O
classification	B
models	O
seen	O
in	O
chapter	O
feature	B
selection	B
variable	B
selection	B
subset	B
selection	B
subset	B
selection	B
in	O
this	O
section	O
we	O
consider	O
some	O
methods	O
for	O
selecting	O
subsets	O
of	O
predictors	O
these	O
include	O
best	O
subset	O
and	O
stepwise	B
model	B
selection	B
procedures	O
best	B
subset	B
selection	B
to	O
perform	O
best	B
subset	B
selection	B
we	O
fit	O
a	O
separate	O
least	B
squares	I
regression	B
for	O
each	O
possible	O
combination	O
of	O
the	O
p	O
predictors	O
that	O
is	O
we	O
fit	O
all	O
p	O
models	O
pp	O
models	O
that	O
contain	O
that	O
contain	O
exactly	O
one	O
predictor	B
all	O
exactly	O
two	O
predictors	O
and	O
so	O
forth	O
we	O
then	O
look	O
at	O
all	O
of	O
the	O
resulting	O
models	O
with	O
the	O
goal	O
of	O
identifying	O
the	O
one	O
that	O
is	O
best	O
p	O
the	O
problem	O
of	O
selecting	O
the	O
best	O
model	B
from	O
among	O
the	O
possibilities	O
considered	O
by	O
best	B
subset	B
selection	B
is	O
not	O
trivial	O
this	O
is	O
usually	O
broken	O
up	O
into	O
two	O
stages	O
as	O
described	O
in	O
algorithm	O
algorithm	O
best	B
subset	B
selection	B
let	O
denote	O
the	O
null	B
model	B
which	O
contains	O
no	O
predictors	O
this	O
model	B
simply	O
predicts	O
the	O
sample	O
mean	O
for	O
each	O
observation	O
best	B
subset	B
selection	B
for	O
k	O
p	O
p	O
k	O
fit	O
all	O
models	O
that	O
contain	O
exactly	O
k	O
predictors	O
pick	O
the	O
best	O
among	O
these	O
models	O
and	O
call	O
it	O
mk	O
here	O
best	O
is	O
defined	O
as	O
having	O
the	O
smallest	O
rss	O
or	O
equivalently	O
largest	O
select	O
a	O
single	B
best	O
model	B
from	O
among	O
using	O
cross	O
p	O
k	O
validated	O
prediction	B
error	B
cp	B
bic	O
or	O
adjusted	O
in	O
algorithm	O
step	O
identifies	O
the	O
best	O
model	B
the	O
training	O
data	B
for	O
each	O
subset	O
size	O
in	O
order	O
to	O
reduce	O
the	O
problem	O
from	O
one	O
of	O
possible	O
models	O
to	O
one	O
of	O
p	O
possible	O
models	O
in	O
figure	O
these	O
models	O
form	O
the	O
lower	O
frontier	O
depicted	O
in	O
red	O
now	O
in	O
order	O
to	O
select	O
a	O
single	B
best	O
model	B
we	O
must	O
simply	O
choose	O
among	O
these	O
p	O
options	O
this	O
task	O
must	O
be	O
performed	O
with	O
care	O
because	O
the	O
rss	O
of	O
these	O
p	O
models	O
decreases	O
monotonically	O
and	O
the	O
increases	O
monotonically	O
as	O
the	O
number	O
of	O
features	O
included	O
in	O
the	O
models	O
increases	O
therefore	O
if	O
we	O
use	O
these	O
statistics	O
to	O
select	O
the	O
best	O
model	B
then	O
we	O
will	O
always	O
end	O
up	O
with	O
a	O
model	B
involving	O
all	O
of	O
the	O
variables	O
the	O
problem	O
is	O
that	O
a	O
low	O
rss	O
or	O
a	O
high	O
indicates	O
a	O
model	B
with	O
a	O
low	O
training	O
error	B
whereas	O
we	O
wish	O
to	O
choose	O
a	O
model	B
that	O
has	O
a	O
low	O
test	O
error	B
shown	O
in	O
chapter	O
in	O
figures	O
training	O
error	B
tends	O
to	O
be	O
quite	O
a	O
bit	O
smaller	O
than	O
test	O
error	B
and	O
a	O
low	O
training	O
error	B
by	O
no	O
means	O
guarantees	O
a	O
low	O
test	O
error	B
therefore	O
in	O
step	O
we	O
use	O
cross-validated	O
prediction	B
linear	B
model	B
selection	B
and	O
regularization	B
s	O
e	O
r	O
a	O
u	O
q	O
s	O
f	O
o	O
m	O
u	O
s	O
l	O
i	O
a	O
u	O
d	O
s	O
e	O
r	O
e	O
e	O
e	O
e	O
r	O
number	O
of	O
predictors	O
number	O
of	O
predictors	O
figure	O
for	O
each	O
possible	O
model	B
containing	O
a	O
subset	O
of	O
the	O
ten	O
predictors	O
in	O
the	O
credit	B
data	B
set	B
the	O
rss	O
and	O
are	O
displayed	O
the	O
red	O
frontier	O
tracks	O
the	O
best	O
model	B
for	O
a	O
given	O
number	O
of	O
predictors	O
according	O
to	O
rss	O
and	O
though	O
the	O
data	B
set	B
contains	O
only	O
ten	O
predictors	O
the	O
x-axis	O
ranges	O
from	O
to	O
since	O
one	O
of	O
the	O
variables	O
is	O
categorical	B
and	O
takes	O
on	O
three	O
values	O
leading	O
to	O
the	O
creation	O
of	O
two	O
dummy	B
variables	O
error	B
cp	B
bic	O
or	O
adjusted	O
in	O
order	O
to	O
select	O
among	O
these	O
approaches	O
are	O
discussed	O
in	O
section	O
an	O
application	O
of	O
best	B
subset	B
selection	B
is	O
shown	O
in	O
figure	O
each	O
plotted	O
point	O
corresponds	O
to	O
a	O
least	B
squares	I
regression	B
model	B
fit	O
using	O
a	O
different	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
credit	B
data	B
set	B
discussed	O
in	O
chapter	O
here	O
the	O
variable	B
ethnicity	O
is	O
a	O
three-level	O
qualitative	B
variable	B
and	O
so	O
is	O
represented	O
by	O
two	O
dummy	B
variables	O
which	O
are	O
selected	O
separately	O
in	O
this	O
case	O
we	O
have	O
plotted	O
the	O
rss	O
and	O
statistics	O
for	O
each	O
model	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
variables	O
the	O
red	O
curves	O
connect	O
the	O
best	O
models	O
for	O
each	O
model	B
size	O
according	O
to	O
rss	O
or	O
the	O
figure	O
shows	O
that	O
as	O
expected	O
these	O
quantities	O
improve	O
as	O
the	O
number	O
of	O
variables	O
increases	O
however	O
from	O
the	O
three-variable	O
model	B
on	O
there	O
is	O
little	O
improvement	O
in	O
rss	O
and	O
as	O
a	O
result	O
of	O
including	O
additional	O
predictors	O
although	O
we	O
have	O
presented	O
best	B
subset	B
selection	B
here	O
for	O
least	B
squares	I
regression	B
the	O
same	O
ideas	O
apply	O
to	O
other	O
types	O
of	O
models	O
such	O
as	O
logistic	B
regression	B
in	O
the	O
case	O
of	O
logistic	B
regression	B
instead	O
of	O
ordering	O
models	O
by	O
rss	O
in	O
step	O
of	O
algorithm	O
we	O
instead	O
use	O
the	O
deviance	B
a	O
measure	O
that	O
plays	O
the	O
role	O
of	O
rss	O
for	O
a	O
broader	O
class	O
of	O
models	O
the	O
deviance	B
is	O
negative	O
two	O
times	O
the	O
maximized	O
log-likelihood	O
the	O
smaller	O
the	O
deviance	B
the	O
better	O
the	O
fit	O
while	O
best	B
subset	B
selection	B
is	O
a	O
simple	B
and	O
conceptually	O
appealing	O
approach	B
it	O
suffers	O
from	O
computational	O
limitations	O
the	O
number	O
of	O
possible	O
models	O
that	O
must	O
be	O
considered	O
grows	O
rapidly	O
as	O
p	O
increases	O
in	O
general	O
there	O
are	O
models	O
that	O
involve	O
subsets	O
of	O
p	O
predictors	O
so	O
if	O
p	O
then	O
there	O
are	O
approximately	O
possible	O
models	O
to	O
be	O
considered	O
and	O
if	O
deviance	B
subset	B
selection	B
p	O
then	O
there	O
are	O
over	O
one	O
million	O
possibilities	O
consequently	O
best	B
subset	B
selection	B
becomes	O
computationally	O
infeasible	O
for	O
values	O
of	O
p	O
greater	O
than	O
around	O
even	O
with	O
extremely	O
fast	O
modern	O
computers	O
there	O
are	O
computational	O
shortcuts	O
so	O
called	O
branch-and-bound	O
techniques	O
for	O
eliminating	O
some	O
choices	O
but	O
these	O
have	O
their	O
limitations	O
as	O
p	O
gets	O
large	O
they	O
also	O
only	O
work	O
for	O
least	B
squares	I
linear	B
regression	B
we	O
present	O
computationally	O
efficient	O
alternatives	O
to	O
best	B
subset	B
selection	B
next	O
stepwise	O
selection	B
for	O
computational	O
reasons	O
best	B
subset	B
selection	B
cannot	O
be	O
applied	O
with	O
very	O
large	O
p	O
best	B
subset	B
selection	B
may	O
also	O
suffer	O
from	O
statistical	O
problems	O
when	O
p	O
is	O
large	O
the	O
larger	O
the	O
search	O
space	O
the	O
higher	O
the	O
chance	O
of	O
finding	O
models	O
that	O
look	O
good	O
on	O
the	O
training	O
data	B
even	O
though	O
they	O
might	O
not	O
have	O
any	O
predictive	O
power	B
on	O
future	O
data	B
thus	O
an	O
enormous	O
search	O
space	O
can	O
lead	O
to	O
overfitting	B
and	O
high	O
variance	B
of	O
the	O
coefficient	O
estimates	O
for	O
both	O
of	O
these	O
reasons	O
stepwise	O
methods	O
which	O
explore	O
a	O
far	O
more	O
restricted	O
set	B
of	O
models	O
are	O
attractive	O
alternatives	O
to	O
best	B
subset	B
selection	B
forward	B
stepwise	I
selection	B
forward	B
stepwise	I
selection	B
is	O
a	O
computationally	O
efficient	O
alternative	O
to	O
best	B
subset	B
selection	B
while	O
the	O
best	B
subset	B
selection	B
procedure	O
considers	O
all	O
possible	O
models	O
containing	O
subsets	O
of	O
the	O
p	O
predictors	O
forward	O
stepwise	O
considers	O
a	O
much	O
smaller	O
set	B
of	O
models	O
forward	B
stepwise	I
selection	B
begins	O
with	O
a	O
model	B
containing	O
no	O
predictors	O
and	O
then	O
adds	O
predictors	O
to	O
the	O
model	B
one-at-a-time	O
until	O
all	O
of	O
the	O
predictors	O
are	O
in	O
the	O
model	B
in	O
particular	O
at	O
each	O
step	O
the	O
variable	B
that	O
gives	O
the	O
greatest	O
additional	O
improvement	O
to	O
the	O
fit	O
is	O
added	O
to	O
the	O
model	B
more	O
formally	O
the	O
forward	B
stepwise	I
selection	B
procedure	O
is	O
given	O
in	O
algorithm	O
forward	B
stepwise	I
selection	B
algorithm	O
forward	B
stepwise	I
selection	B
let	O
denote	O
the	O
null	B
model	B
which	O
contains	O
no	O
predictors	O
for	O
k	O
p	O
consider	O
all	O
p	O
k	O
models	O
that	O
augment	O
the	O
predictors	O
in	O
mk	O
choose	O
the	O
best	O
among	O
these	O
p	O
k	O
models	O
and	O
call	O
it	O
with	O
one	O
additional	O
predictor	B
here	O
best	O
is	O
defined	O
as	O
having	O
smallest	O
rss	O
or	O
highest	O
select	O
a	O
single	B
best	O
model	B
from	O
among	O
using	O
cross	O
validated	O
prediction	B
error	B
cp	B
bic	O
or	O
adjusted	O
linear	B
model	B
selection	B
and	O
regularization	B
unlike	O
best	B
subset	B
selection	B
which	O
involved	O
fitting	O
models	O
forward	B
stepwise	I
selection	B
involves	O
fitting	O
one	O
null	B
model	B
along	O
with	O
p	O
k	O
models	O
in	O
the	O
kth	O
iteration	O
for	O
k	O
p	O
this	O
amounts	O
to	O
a	O
total	O
of	O
k	O
pp	O
models	O
this	O
is	O
a	O
substantial	O
difference	O
when	O
p	O
p	O
best	B
subset	B
selection	B
requires	O
fitting	O
models	O
whereas	O
forward	B
stepwise	I
selection	B
requires	O
fitting	O
only	O
among	O
those	O
p	O
k	O
that	O
augment	O
mk	O
with	O
one	O
additional	O
predictor	B
we	O
can	O
in	O
step	O
of	O
algorithm	O
we	O
must	O
identify	O
the	O
best	O
model	B
from	O
do	O
this	O
by	O
simply	O
choosing	O
the	O
model	B
with	O
the	O
lowest	O
rss	O
or	O
the	O
highest	O
however	O
in	O
step	O
we	O
must	O
identify	O
the	O
best	O
model	B
among	O
a	O
set	B
of	O
models	O
with	O
different	O
numbers	O
of	O
variables	O
this	O
is	O
more	O
challenging	O
and	O
is	O
discussed	O
in	O
section	O
forward	B
stepwise	I
selection	B
s	O
computational	O
advantage	O
over	O
best	B
subset	B
selection	B
is	O
clear	O
though	O
forward	O
stepwise	O
tends	O
to	O
do	O
well	O
in	O
practice	O
it	O
is	O
not	O
guaranteed	O
to	O
find	O
the	O
best	O
possible	O
model	B
out	O
of	O
all	O
models	O
containing	O
subsets	O
of	O
the	O
p	O
predictors	O
for	O
instance	O
suppose	O
that	O
in	O
a	O
given	O
data	B
set	B
with	O
p	O
predictors	O
the	O
best	O
possible	O
one-variable	O
model	B
contains	O
and	O
the	O
best	O
possible	O
two-variable	O
model	B
instead	O
contains	O
and	O
then	O
forward	B
stepwise	I
selection	B
will	O
fail	O
to	O
select	O
the	O
best	O
possible	O
two-variable	O
model	B
because	O
will	O
contain	O
so	O
must	O
also	O
contain	O
together	O
with	O
one	O
additional	O
variable	B
table	O
which	O
shows	O
the	O
first	O
four	O
selected	O
models	O
for	O
best	O
subset	O
and	O
forward	B
stepwise	I
selection	B
on	O
the	O
credit	B
data	B
set	B
illustrates	O
this	O
phenomenon	O
both	O
best	B
subset	B
selection	B
and	O
forward	B
stepwise	I
selection	B
choose	O
rating	O
for	O
the	O
best	O
one-variable	O
model	B
and	O
then	O
include	O
income	B
and	O
student	O
for	O
the	O
two-	O
and	O
three-variable	O
models	O
however	O
best	B
subset	B
selection	B
replaces	O
rating	O
by	O
cards	O
in	O
the	O
four-variable	O
model	B
while	O
forward	B
stepwise	I
selection	B
must	O
maintain	O
rating	O
in	O
its	O
four-variable	O
model	B
in	O
this	O
example	O
figure	O
indicates	O
that	O
there	O
is	O
not	O
much	O
difference	O
between	O
the	O
threeand	O
four-variable	O
models	O
in	O
terms	O
of	O
rss	O
so	O
either	O
of	O
the	O
four-variable	O
models	O
will	O
likely	O
be	O
adequate	O
forward	B
stepwise	I
selection	B
can	O
be	O
applied	O
even	O
in	O
the	O
high-dimensional	B
setting	O
where	O
n	O
p	O
however	O
in	O
this	O
case	O
it	O
is	O
possible	O
to	O
construct	O
submodels	O
only	O
since	O
each	O
submodel	O
is	O
fit	O
using	O
least	B
squares	I
which	O
will	O
not	O
yield	O
a	O
unique	O
solution	O
if	O
p	O
n	O
backward	B
stepwise	I
selection	B
like	O
forward	B
stepwise	I
selection	B
backward	B
stepwise	I
selection	B
provides	O
an	O
efficient	O
alternative	O
to	O
best	B
subset	B
selection	B
however	O
unlike	O
forward	O
backward	B
stepwise	I
selection	B
forward	B
stepwise	I
selection	B
considers	O
pp	O
models	O
it	O
performs	O
a	O
guided	O
search	O
over	O
model	B
space	O
and	O
so	O
the	O
effective	O
model	B
space	O
considered	O
contains	O
substantially	O
more	O
than	O
pp	O
models	O
subset	B
selection	B
variables	O
best	O
subset	O
one	O
two	O
three	O
four	O
rating	O
rating	O
income	B
rating	O
income	B
student	O
cards	O
income	B
student	O
limit	O
forward	O
stepwise	O
rating	O
rating	O
income	B
rating	O
income	B
student	O
rating	O
income	B
student	O
limit	O
table	O
the	O
first	O
four	O
selected	O
models	O
for	O
best	B
subset	B
selection	B
and	O
forward	B
stepwise	I
selection	B
on	O
the	O
credit	B
data	B
set	B
the	O
first	O
three	O
models	O
are	O
identical	O
but	O
the	O
fourth	O
models	O
differ	O
stepwise	O
selection	B
it	O
begins	O
with	O
the	O
full	O
least	B
squares	I
model	B
containing	O
all	O
p	O
predictors	O
and	O
then	O
iteratively	O
removes	O
the	O
least	O
useful	O
predictor	B
one-at-a-time	O
details	O
are	O
given	O
in	O
algorithm	O
algorithm	O
backward	B
stepwise	I
selection	B
let	O
mp	O
denote	O
the	O
full	O
model	B
which	O
contains	O
all	O
p	O
predictors	O
for	O
k	O
p	O
p	O
consider	O
all	O
k	O
models	O
that	O
contain	O
all	O
but	O
one	O
of	O
the	O
predictors	O
in	O
mk	O
for	O
a	O
total	O
of	O
k	O
predictors	O
choose	O
the	O
best	O
among	O
these	O
k	O
models	O
and	O
call	O
it	O
mk	O
here	O
best	O
is	O
defined	O
as	O
having	O
smallest	O
rss	O
or	O
highest	O
select	O
a	O
single	B
best	O
model	B
from	O
among	O
using	O
cross	O
validated	O
prediction	B
error	B
cp	B
bic	O
or	O
adjusted	O
like	O
forward	B
stepwise	I
selection	B
the	O
backward	O
selection	B
approach	B
searches	O
through	O
only	O
pp	O
models	O
and	O
so	O
can	O
be	O
applied	O
in	O
settings	O
where	O
p	O
is	O
too	O
large	O
to	O
apply	O
best	O
subset	O
also	O
like	O
forward	B
stepwise	I
selection	B
backward	B
stepwise	I
selection	B
is	O
not	O
guaranteed	O
to	O
yield	O
the	O
best	O
model	B
containing	O
a	O
subset	O
of	O
the	O
p	O
predictors	O
backward	O
selection	B
requires	O
that	O
the	O
number	O
of	O
samples	O
n	O
is	O
larger	O
than	O
the	O
number	O
of	O
variables	O
p	O
that	O
the	O
full	O
model	B
can	O
be	O
fit	O
in	O
contrast	B
forward	O
stepwise	O
can	O
be	O
used	O
even	O
when	O
n	O
p	O
and	O
so	O
is	O
the	O
only	O
viable	O
subset	O
method	O
when	O
p	O
is	O
very	O
large	O
forward	B
stepwise	I
selection	B
backward	B
stepwise	I
selection	B
performs	O
a	O
guided	O
search	O
over	O
model	B
space	O
and	O
so	O
effectively	O
considers	O
substantially	O
more	O
than	O
models	O
linear	B
model	B
selection	B
and	O
regularization	B
hybrid	O
approaches	O
the	O
best	O
subset	O
forward	O
stepwise	O
and	O
backward	B
stepwise	I
selection	B
approaches	O
generally	O
give	O
similar	O
but	O
not	O
identical	O
models	O
as	O
another	O
alternative	O
hybrid	O
versions	O
of	O
forward	O
and	O
backward	B
stepwise	I
selection	B
are	O
available	O
in	O
which	O
variables	O
are	O
added	O
to	O
the	O
model	B
sequentially	O
in	O
analogy	O
to	O
forward	O
selection	B
however	O
after	O
adding	O
each	O
new	O
variable	B
the	O
method	O
may	O
also	O
remove	O
any	O
variables	O
that	O
no	O
longer	O
provide	O
an	O
improvement	O
in	O
the	O
model	B
fit	O
such	O
an	O
approach	B
attempts	O
to	O
more	O
closely	O
mimic	O
best	B
subset	B
selection	B
while	O
retaining	O
the	O
computational	O
advantages	O
of	O
forward	O
and	O
backward	B
stepwise	I
selection	B
choosing	O
the	O
optimal	O
model	B
best	B
subset	B
selection	B
forward	O
selection	B
and	O
backward	O
selection	B
result	O
in	O
the	O
creation	O
of	O
a	O
set	B
of	O
models	O
each	O
of	O
which	O
contains	O
a	O
subset	O
of	O
the	O
p	O
predictors	O
in	O
order	O
to	O
implement	O
these	O
methods	O
we	O
need	O
a	O
way	O
to	O
determine	O
which	O
of	O
these	O
models	O
is	O
best	O
as	O
we	O
discussed	O
in	O
section	O
the	O
model	B
containing	O
all	O
of	O
the	O
predictors	O
will	O
always	O
have	O
the	O
smallest	O
rss	O
and	O
the	O
largest	O
since	O
these	O
quantities	O
are	O
related	O
to	O
the	O
training	O
error	B
instead	O
we	O
wish	O
to	O
choose	O
a	O
model	B
with	O
a	O
low	O
test	O
error	B
as	O
is	O
evident	O
here	O
and	O
as	O
we	O
show	O
in	O
chapter	O
the	O
training	O
error	B
can	O
be	O
a	O
poor	O
estimate	O
of	O
the	O
test	O
error	B
therefore	O
rss	O
and	O
are	O
not	O
suitable	O
for	O
selecting	O
the	O
best	O
model	B
among	O
a	O
collection	O
of	O
models	O
with	O
different	O
numbers	O
of	O
predictors	O
in	O
order	O
to	O
select	O
the	O
best	O
model	B
with	O
respect	O
to	O
test	O
error	B
we	O
need	O
to	O
estimate	O
this	O
test	O
error	B
there	O
are	O
two	O
common	O
approaches	O
we	O
can	O
indirectly	O
estimate	O
test	O
error	B
by	O
making	O
an	O
adjustment	O
to	O
the	O
training	O
error	B
to	O
account	O
for	O
the	O
bias	B
due	O
to	O
overfitting	B
we	O
can	O
directly	O
estimate	O
the	O
test	O
error	B
using	O
either	O
a	O
validation	B
set	B
approach	B
or	O
a	O
cross-validation	B
approach	B
as	O
discussed	O
in	O
chapter	O
we	O
consider	O
both	O
of	O
these	O
approaches	O
below	O
cp	B
aic	O
bic	O
and	O
adjusted	O
we	O
show	O
in	O
chapter	O
that	O
the	O
training	O
set	B
mse	B
is	O
generally	O
an	O
underestimate	O
of	O
the	O
test	O
mse	B
that	O
mse	B
rssn	O
this	O
is	O
because	O
when	O
we	O
fit	O
a	O
model	B
to	O
the	O
training	O
data	B
using	O
least	B
squares	I
we	O
specifically	O
estimate	O
the	O
regression	B
coefficients	O
such	O
that	O
the	O
training	O
rss	O
not	O
the	O
test	O
rss	O
is	O
as	O
small	O
as	O
possible	O
in	O
particular	O
the	O
training	O
error	B
will	O
decrease	O
as	O
more	O
variables	O
are	O
included	O
in	O
the	O
model	B
but	O
the	O
test	O
error	B
may	O
not	O
therefore	O
training	O
set	B
rss	O
and	O
training	O
set	B
cannot	O
be	O
used	O
to	O
select	O
from	O
among	O
a	O
set	B
of	O
models	O
with	O
different	O
numbers	O
of	O
variables	O
however	O
a	O
number	O
of	O
techniques	O
for	O
adjusting	O
the	O
training	O
error	B
for	O
the	O
model	B
size	O
are	O
available	O
these	O
approaches	O
can	O
be	O
used	O
to	O
select	O
among	O
a	O
set	B
subset	B
selection	B
p	O
c	O
c	O
b	O
i	O
r	O
d	O
e	O
t	O
s	O
u	O
d	O
a	O
j	O
number	O
of	O
predictors	O
number	O
of	O
predictors	O
number	O
of	O
predictors	O
figure	O
cp	B
bic	O
and	O
adjusted	O
are	O
shown	O
for	O
the	O
best	O
models	O
of	O
each	O
size	O
for	O
the	O
credit	B
data	B
set	B
lower	O
frontier	O
in	O
figure	O
cp	B
and	O
bic	O
are	O
estimates	O
of	O
test	O
mse	B
in	O
the	O
middle	O
plot	B
we	O
see	O
that	O
the	O
bic	O
estimate	O
of	O
test	O
error	B
shows	O
an	O
increase	O
after	O
four	O
variables	O
are	O
selected	O
the	O
other	O
two	O
plots	O
are	O
rather	O
flat	O
after	O
four	O
variables	O
are	O
included	O
of	O
models	O
with	O
different	O
numbers	O
of	O
variables	O
we	O
now	O
consider	O
four	O
such	O
approaches	O
cp	B
akaike	B
information	I
criterion	I
bayesian	B
information	O
criterion	O
and	O
adjusted	O
figure	O
displays	O
cp	B
bic	O
and	O
adjusted	O
for	O
the	O
best	O
model	B
of	O
each	O
size	O
produced	O
by	O
best	B
subset	B
selection	B
on	O
the	O
credit	B
data	B
set	B
for	O
a	O
fitted	O
least	B
squares	I
model	B
containing	O
d	O
predictors	O
the	O
cp	B
estimate	O
of	O
test	O
mse	B
is	O
computed	O
using	O
the	O
equation	O
rss	O
cp	B
n	O
where	O
is	O
an	O
estimate	O
of	O
the	O
variance	B
of	O
the	O
error	B
associated	O
with	O
each	O
response	B
measurement	O
in	O
typically	O
is	O
estimated	O
using	O
the	O
full	O
essentially	O
the	O
cp	B
statistic	O
adds	O
a	O
penalty	B
model	B
containing	O
all	O
predictors	O
of	O
to	O
the	O
training	O
rss	O
in	O
order	O
to	O
adjust	O
for	O
the	O
fact	O
that	O
the	O
training	O
error	B
tends	O
to	O
underestimate	O
the	O
test	O
error	B
clearly	O
the	O
penalty	B
increases	O
as	O
the	O
number	O
of	O
predictors	O
in	O
the	O
model	B
increases	O
this	O
is	O
intended	O
to	O
adjust	O
for	O
the	O
corresponding	O
decrease	O
in	O
training	O
rss	O
though	O
it	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
one	O
can	O
show	O
that	O
if	O
is	O
an	O
unbiased	O
estimate	O
of	O
in	O
then	O
cp	B
is	O
an	O
unbiased	O
estimate	O
of	O
test	O
mse	B
as	O
a	O
consequence	O
the	O
cp	B
statistic	O
tends	O
to	O
take	O
on	O
a	O
small	O
value	O
for	O
models	O
with	O
a	O
low	O
test	O
error	B
so	O
when	O
determining	O
which	O
of	O
a	O
set	B
of	O
models	O
is	O
best	O
we	O
choose	O
the	O
model	B
with	O
the	O
lowest	O
cp	B
value	O
in	O
figure	O
cp	B
selects	O
the	O
six-variable	O
model	B
containing	O
the	O
predictors	O
income	B
limit	O
rating	O
cards	O
age	O
and	O
student	O
s	O
cp	B
is	O
sometimes	O
defined	O
as	O
c	O
the	O
definition	O
given	O
above	O
in	O
the	O
sense	O
that	O
cp	B
smallest	O
cp	B
also	O
has	O
smallest	O
c	O
p	O
p	O
rss	O
n	O
this	O
is	O
equivalent	O
to	O
p	O
n	O
and	O
so	O
the	O
model	B
with	O
n	O
cp	B
akaike	B
information	I
criterion	I
bayesian	B
information	O
criterion	O
adjusted	O
linear	B
model	B
selection	B
and	O
regularization	B
the	O
aic	O
criterion	O
is	O
defined	O
for	O
a	O
large	O
class	O
of	O
models	O
fit	O
by	O
maximum	B
likelihood	I
in	O
the	O
case	O
of	O
the	O
model	B
with	O
gaussian	O
errors	O
maximum	B
likelihood	I
and	O
least	B
squares	I
are	O
the	O
same	O
thing	O
in	O
this	O
case	O
aic	O
is	O
given	O
by	O
rss	O
aic	O
n	O
where	O
for	O
simplicity	O
we	O
have	O
omitted	O
an	O
additive	B
constant	O
hence	O
for	O
least	B
squares	I
models	O
cp	B
and	O
aic	O
are	O
proportional	O
to	O
each	O
other	O
and	O
so	O
only	O
cp	B
is	O
displayed	O
in	O
figure	O
bic	O
is	O
derived	O
from	O
a	O
bayesian	B
point	O
of	O
view	O
but	O
ends	O
up	O
looking	O
similar	O
to	O
cp	B
aic	O
as	O
well	O
for	O
the	O
least	B
squares	I
model	B
with	O
d	O
predictors	O
the	O
bic	O
is	O
up	O
to	O
irrelevant	O
constants	O
given	O
by	O
rss	O
lognd	O
bic	O
n	O
like	O
cp	B
the	O
bic	O
will	O
tend	O
to	O
take	O
on	O
a	O
small	O
value	O
for	O
a	O
model	B
with	O
a	O
low	O
test	O
error	B
and	O
so	O
generally	O
we	O
select	O
the	O
model	B
that	O
has	O
the	O
lowest	O
bic	O
value	O
notice	O
that	O
bic	O
replaces	O
the	O
used	O
by	O
cp	B
with	O
a	O
lognd	O
term	B
where	O
n	O
is	O
the	O
number	O
of	O
observations	B
since	O
log	O
n	O
for	O
any	O
n	O
the	O
bic	O
statistic	O
generally	O
places	O
a	O
heavier	O
penalty	B
on	O
models	O
with	O
many	O
variables	O
and	O
hence	O
results	O
in	O
the	O
selection	B
of	O
smaller	O
models	O
than	O
cp	B
in	O
figure	O
we	O
see	O
that	O
this	O
is	O
indeed	O
the	O
case	O
for	O
the	O
credit	B
data	B
set	B
bic	O
chooses	O
a	O
model	B
that	O
contains	O
only	O
the	O
four	O
predictors	O
income	B
limit	O
cards	O
and	O
student	O
in	O
this	O
case	O
the	O
curves	O
are	O
very	O
flat	O
and	O
so	O
there	O
does	O
not	O
appear	O
to	O
be	O
much	O
difference	O
in	O
accuracy	O
between	O
the	O
four-variable	O
and	O
six-variable	O
models	O
the	O
adjusted	O
statistic	O
is	O
another	O
popular	O
approach	B
for	O
selecting	O
among	O
a	O
set	B
of	O
models	O
that	O
contain	O
different	O
numbers	O
of	O
variables	O
recall	B
from	O
chapter	O
that	O
the	O
usual	O
is	O
defined	O
as	O
rsstss	O
where	O
tss	O
is	O
the	O
total	B
sum	B
of	I
squares	I
for	O
the	O
response	B
since	O
rss	O
always	O
decreases	O
as	O
more	O
variables	O
are	O
added	O
to	O
the	O
model	B
the	O
always	O
increases	O
as	O
more	O
variables	O
are	O
added	O
for	O
a	O
least	B
squares	I
model	B
with	O
d	O
variables	O
the	O
adjusted	O
statistic	O
is	O
calculated	O
as	O
adjusted	O
rssn	O
d	O
tssn	O
unlike	O
cp	B
aic	O
and	O
bic	O
for	O
which	O
a	O
small	O
value	O
indicates	O
a	O
model	B
with	O
a	O
low	O
test	O
error	B
a	O
large	O
value	O
of	O
adjusted	O
indicates	O
a	O
model	B
with	O
a	O
small	O
test	O
error	B
maximizing	O
the	O
adjusted	O
is	O
equivalent	O
to	O
minimizing	O
rss	O
n	O
d	O
while	O
rss	O
always	O
decreases	O
as	O
the	O
number	O
of	O
variables	O
in	O
the	O
model	B
increases	O
rss	O
n	O
d	O
may	O
increase	O
or	O
decrease	O
due	O
to	O
the	O
presence	O
of	O
d	O
in	O
the	O
denominator	O
the	O
intuition	O
behind	O
the	O
adjusted	O
is	O
that	O
once	O
all	O
of	O
the	O
correct	O
variables	O
have	O
been	O
included	O
in	O
the	O
model	B
adding	O
additional	O
noise	B
variables	O
subset	B
selection	B
will	O
lead	O
to	O
only	O
a	O
very	O
small	O
decrease	O
in	O
rss	O
since	O
adding	O
noise	B
variables	O
leads	O
to	O
an	O
increase	O
in	O
d	O
such	O
variables	O
will	O
lead	O
to	O
an	O
increase	O
in	O
rss	O
n	O
d	O
and	O
consequently	O
a	O
decrease	O
in	O
the	O
adjusted	O
therefore	O
in	O
theory	O
the	O
model	B
with	O
the	O
largest	O
adjusted	O
will	O
have	O
only	O
correct	O
variables	O
and	O
no	O
noise	B
variables	O
unlike	O
the	O
statistic	O
the	O
adjusted	O
statistic	O
pays	O
a	O
price	O
for	O
the	O
inclusion	O
of	O
unnecessary	O
variables	O
in	O
the	O
model	B
figure	O
displays	O
the	O
adjusted	O
for	O
the	O
credit	B
data	B
set	B
using	O
this	O
statistic	O
results	O
in	O
the	O
selection	B
of	O
a	O
model	B
that	O
contains	O
seven	O
variables	O
adding	O
gender	O
to	O
the	O
model	B
selected	O
by	O
cp	B
and	O
aic	O
cp	B
aic	O
and	O
bic	O
all	O
have	O
rigorous	O
theoretical	O
justifications	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
these	O
justifications	O
rely	O
on	O
asymptotic	O
arguments	O
where	O
the	O
sample	O
size	O
n	O
is	O
very	O
large	O
despite	O
its	O
popularity	O
and	O
even	O
though	O
it	O
is	O
quite	O
intuitive	O
the	O
adjusted	O
is	O
not	O
as	O
well	O
motivated	O
in	O
statistical	O
theory	O
as	O
aic	O
bic	O
and	O
cp	B
all	O
of	O
these	O
measures	O
are	O
simple	B
to	O
use	O
and	O
compute	O
here	O
we	O
have	O
presented	O
the	O
formulas	O
for	O
aic	O
bic	O
and	O
cp	B
in	O
the	O
case	O
of	O
a	O
linear	B
model	B
fit	O
using	O
least	B
squares	I
however	O
these	O
quantities	O
can	O
also	O
be	O
defined	O
for	O
more	O
general	O
types	O
of	O
models	O
validation	O
and	O
cross-validation	B
as	O
an	O
alternative	O
to	O
the	O
approaches	O
just	O
discussed	O
we	O
can	O
directly	O
estimate	O
the	O
test	O
error	B
using	O
the	O
validation	B
set	B
and	O
cross-validation	B
methods	O
discussed	O
in	O
chapter	O
we	O
can	O
compute	O
the	O
validation	B
set	B
error	B
or	O
the	O
cross-validation	B
error	B
for	O
each	O
model	B
under	O
consideration	O
and	O
then	O
select	O
the	O
model	B
for	O
which	O
the	O
resulting	O
estimated	O
test	O
error	B
is	O
smallest	O
this	O
procedure	O
has	O
an	O
advantage	O
relative	O
to	O
aic	O
bic	O
cp	B
and	O
adjusted	O
in	O
that	O
it	O
provides	O
a	O
direct	O
estimate	O
of	O
the	O
test	O
error	B
and	O
makes	O
fewer	O
assumptions	O
about	O
the	O
true	O
underlying	O
model	B
it	O
can	O
also	O
be	O
used	O
in	O
a	O
wider	O
range	O
of	O
model	B
selection	B
tasks	O
even	O
in	O
cases	O
where	O
it	O
is	O
hard	O
to	O
pinpoint	O
the	O
model	B
degrees	B
of	I
freedom	I
the	O
number	O
of	O
predictors	O
in	O
the	O
model	B
or	O
hard	O
to	O
estimate	O
the	O
error	B
variance	B
in	O
the	O
past	O
performing	O
cross-validation	B
was	O
computationally	O
prohibitive	O
for	O
many	O
problems	O
with	O
large	O
p	O
andor	O
large	O
n	O
and	O
so	O
aic	O
bic	O
cp	B
and	O
adjusted	O
were	O
more	O
attractive	O
approaches	O
for	O
choosing	O
among	O
a	O
set	B
of	O
models	O
however	O
nowadays	O
with	O
fast	O
computers	O
the	O
computations	O
required	O
to	O
perform	O
cross-validation	B
are	O
hardly	O
ever	O
an	O
issue	O
thus	O
crossvalidation	O
is	O
a	O
very	O
attractive	O
approach	B
for	O
selecting	O
from	O
among	O
a	O
number	O
of	O
models	O
under	O
consideration	O
figure	O
displays	O
as	O
a	O
function	B
of	O
d	O
the	O
bic	O
validation	B
set	B
errors	O
and	O
cross-validation	B
errors	O
on	O
the	O
credit	B
data	B
for	O
the	O
best	O
d-variable	O
model	B
the	O
validation	O
errors	O
were	O
calculated	O
by	O
randomly	O
selecting	O
three-quarters	O
of	O
the	O
observations	B
as	O
the	O
training	O
set	B
and	O
the	O
remainder	O
as	O
the	O
validation	B
set	B
the	O
cross-validation	B
errors	O
were	O
computed	O
using	O
k	O
folds	O
in	O
this	O
case	O
the	O
validation	O
and	O
cross-validation	B
methods	O
both	O
result	O
in	O
a	O
linear	B
model	B
selection	B
and	O
regularization	B
c	O
b	O
i	O
f	O
o	O
t	O
o	O
o	O
r	O
e	O
r	O
a	O
u	O
q	O
s	O
r	O
o	O
r	O
r	O
e	O
t	O
e	O
s	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
number	O
of	O
predictors	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
s	O
s	O
o	O
r	O
c	O
number	O
of	O
predictors	O
number	O
of	O
predictors	O
figure	O
for	O
the	O
credit	B
data	B
set	B
three	O
quantities	O
are	O
displayed	O
for	O
the	O
best	O
model	B
containing	O
d	O
predictors	O
for	O
d	O
ranging	O
from	O
to	O
the	O
overall	O
best	O
model	B
based	O
on	O
each	O
of	O
these	O
quantities	O
is	O
shown	O
as	O
a	O
blue	O
cross	O
left	O
square	O
root	O
of	O
bic	O
center	O
validation	B
set	B
errors	O
right	O
cross-validation	B
errors	O
six-variable	O
model	B
however	O
all	O
three	O
approaches	O
suggest	O
that	O
the	O
four-	O
five-	O
and	O
six-variable	O
models	O
are	O
roughly	O
equivalent	O
in	O
terms	O
of	O
their	O
test	O
errors	O
in	O
fact	O
the	O
estimated	O
test	O
error	B
curves	O
displayed	O
in	O
the	O
center	O
and	O
righthand	O
panels	O
of	O
figure	O
are	O
quite	O
flat	O
while	O
a	O
three-variable	O
model	B
clearly	O
has	O
lower	O
estimated	O
test	O
error	B
than	O
a	O
two-variable	O
model	B
the	O
estimated	O
test	O
errors	O
of	O
the	O
to	O
models	O
are	O
quite	O
similar	O
furthermore	O
if	O
we	O
repeated	O
the	O
validation	B
set	B
approach	B
using	O
a	O
different	O
split	O
of	O
the	O
data	B
into	O
a	O
training	O
set	B
and	O
a	O
validation	B
set	B
or	O
if	O
we	O
repeated	O
cross-validation	B
using	O
a	O
different	O
set	B
of	O
cross-validation	B
folds	O
then	O
the	O
precise	O
model	B
with	O
the	O
lowest	O
estimated	O
test	O
error	B
would	O
surely	O
change	O
in	O
this	O
setting	O
we	O
can	O
select	O
a	O
model	B
using	O
the	O
one-standard-error	B
rule	I
we	O
first	O
calculate	O
the	O
onestandard	O
error	B
of	O
the	O
estimated	O
test	O
mse	B
for	O
each	O
model	B
size	O
and	O
then	O
select	O
the	O
smallest	O
model	B
for	O
which	O
the	O
estimated	O
test	O
error	B
is	O
within	O
one	O
standard	B
error	B
of	O
the	O
lowest	O
point	O
on	O
the	O
curve	O
the	O
rationale	O
here	O
is	O
that	O
if	O
a	O
set	B
of	O
models	O
appear	O
to	O
be	O
more	O
or	O
less	O
equally	O
good	O
then	O
we	O
might	O
as	O
well	O
choose	O
the	O
simplest	O
model	B
that	O
is	O
the	O
model	B
with	O
the	O
smallest	O
number	O
of	O
predictors	O
in	O
this	O
case	O
applying	O
the	O
one-standard-error	B
rule	I
to	O
the	O
validation	B
set	B
or	O
cross-validation	B
approach	B
leads	O
to	O
selection	B
of	O
the	O
three-variable	O
model	B
standarderror	O
rule	O
shrinkage	B
methods	O
the	O
subset	B
selection	B
methods	O
described	O
in	O
section	O
involve	O
using	O
least	B
squares	I
to	O
fit	O
a	O
linear	B
model	B
that	O
contains	O
a	O
subset	O
of	O
the	O
predictors	O
as	O
an	O
alternative	O
we	O
can	O
fit	O
a	O
model	B
containing	O
all	O
p	O
predictors	O
using	O
a	O
technique	O
that	O
constrains	O
or	O
regularizes	O
the	O
coefficient	O
estimates	O
or	O
equivalently	O
that	O
shrinks	O
the	O
coefficient	O
estimates	O
towards	O
zero	O
it	O
may	O
not	O
be	O
immediately	O
shrinkage	B
methods	O
obvious	O
why	O
such	O
a	O
constraint	O
should	O
improve	O
the	O
fit	O
but	O
it	O
turns	O
out	O
that	O
shrinking	O
the	O
coefficient	O
estimates	O
can	O
significantly	O
reduce	O
their	O
variance	B
the	O
two	O
best-known	O
techniques	O
for	O
shrinking	O
the	O
regression	B
coefficients	O
towards	O
zero	O
are	O
ridge	B
regression	B
and	O
the	O
lasso	B
ridge	B
regression	B
recall	B
from	O
chapter	O
that	O
the	O
least	B
squares	I
fitting	O
procedure	O
estimates	O
p	O
using	O
the	O
values	O
that	O
minimize	O
rss	O
yi	O
jxij	O
ridge	B
regression	B
is	O
very	O
similar	O
to	O
least	B
squares	I
except	O
that	O
the	O
coefficients	O
are	O
estimated	O
by	O
minimizing	O
a	O
slightly	O
different	O
quantity	O
in	O
particular	O
the	O
ridge	B
regression	B
coefficient	O
estimates	O
r	O
are	O
the	O
values	O
that	O
minimize	O
yi	O
jxij	O
j	O
rss	O
j	O
j	O
where	O
is	O
a	O
tuning	B
parameter	B
to	O
be	O
determined	O
separately	O
equation	O
trades	O
off	O
two	O
different	O
criteria	O
as	O
with	O
least	B
squares	I
ridge	B
regression	B
seeks	O
coefficient	O
estimates	O
that	O
fit	O
the	O
data	B
well	O
by	O
making	O
the	O
rss	O
small	O
however	O
the	O
second	O
term	B
j	O
called	O
a	O
shrinkage	B
penalty	B
is	O
small	O
when	O
p	O
are	O
close	O
to	O
zero	O
and	O
so	O
it	O
has	O
the	O
effect	O
of	O
shrinking	O
the	O
estimates	O
of	O
j	O
towards	O
zero	O
the	O
tuning	B
parameter	B
serves	O
to	O
control	O
the	O
relative	O
impact	O
of	O
these	O
two	O
terms	O
on	O
the	O
regression	B
coefficient	O
estimates	O
when	O
the	O
penalty	B
term	B
has	O
no	O
effect	O
and	O
ridge	B
regression	B
will	O
produce	O
the	O
least	B
squares	I
estimates	O
however	O
as	O
the	O
impact	O
of	O
the	O
shrinkage	B
penalty	B
grows	O
and	O
the	O
ridge	B
regression	B
coefficient	O
estimates	O
will	O
approach	B
zero	O
unlike	O
least	B
squares	I
which	O
generates	O
only	O
one	O
set	B
of	O
coefficient	O
estimates	O
ridge	B
regression	B
will	O
produce	O
a	O
different	O
set	B
of	O
coefficient	O
estimates	O
r	O
for	O
each	O
value	O
of	O
selecting	O
a	O
good	O
value	O
for	O
is	O
critical	O
we	O
defer	O
this	O
discussion	O
to	O
section	O
where	O
we	O
use	O
cross-validation	B
note	O
that	O
in	O
the	O
shrinkage	B
penalty	B
is	O
applied	O
to	O
p	O
but	O
not	O
to	O
the	O
intercept	B
we	O
want	O
to	O
shrink	O
the	O
estimated	O
association	O
of	O
each	O
variable	B
with	O
the	O
response	B
however	O
we	O
do	O
not	O
want	O
to	O
shrink	O
the	O
intercept	B
which	O
is	O
simply	O
a	O
measure	O
of	O
the	O
mean	O
value	O
of	O
the	O
response	B
when	O
xip	O
if	O
we	O
assume	O
that	O
the	O
variables	O
that	O
is	O
the	O
columns	O
of	O
the	O
data	B
matrix	O
x	O
have	O
been	O
centered	O
to	O
have	O
mean	O
zero	O
before	O
ridge	B
regression	B
is	O
performed	O
then	O
the	O
estimated	O
intercept	B
will	O
take	O
the	O
form	O
y	O
n	O
yin	O
ridge	B
regression	B
tuning	B
parameter	B
shrinkage	B
penalty	B
linear	B
model	B
selection	B
and	O
regularization	B
i	O
s	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
i	O
income	B
limit	O
rating	O
student	O
i	O
s	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
i	O
r	O
figure	O
the	O
standardized	O
ridge	B
regression	B
coefficients	O
are	O
displayed	O
for	O
the	O
credit	B
data	B
set	B
as	O
a	O
function	B
of	O
and	O
r	O
an	O
application	O
to	O
the	O
credit	B
data	B
in	O
figure	O
the	O
ridge	B
regression	B
coefficient	O
estimates	O
for	O
the	O
credit	B
data	B
set	B
are	O
displayed	O
in	O
the	O
left-hand	O
panel	O
each	O
curve	O
corresponds	O
to	O
the	O
ridge	B
regression	B
coefficient	O
estimate	O
for	O
one	O
of	O
the	O
ten	O
variables	O
plotted	O
as	O
a	O
function	B
of	O
for	O
example	O
the	O
black	O
solid	O
line	B
represents	O
the	O
ridge	B
regression	B
estimate	O
for	O
the	O
income	B
coefficient	O
as	O
is	O
varied	O
at	O
the	O
extreme	O
left-hand	O
side	O
of	O
the	O
plot	B
is	O
essentially	O
zero	O
and	O
so	O
the	O
corresponding	O
ridge	O
coefficient	O
estimates	O
are	O
the	O
same	O
as	O
the	O
usual	O
least	B
squares	I
estimates	O
but	O
as	O
increases	O
the	O
ridge	O
coefficient	O
estimates	O
shrink	O
towards	O
zero	O
when	O
is	O
extremely	O
large	O
then	O
all	O
of	O
the	O
ridge	O
coefficient	O
estimates	O
are	O
basically	O
zero	O
this	O
corresponds	O
to	O
the	O
null	B
model	B
that	O
contains	O
no	O
predictors	O
in	O
this	O
plot	B
the	O
income	B
limit	O
rating	O
and	O
student	O
variables	O
are	O
displayed	O
in	O
distinct	O
colors	O
since	O
these	O
variables	O
tend	O
to	O
have	O
by	O
far	O
the	O
largest	O
coefficient	O
estimates	O
while	O
the	O
ridge	O
coefficient	O
estimates	O
tend	O
to	O
decrease	O
in	O
aggregate	O
as	O
increases	O
individual	O
coefficients	O
such	O
as	O
rating	O
and	O
income	B
may	O
occasionally	O
increase	O
as	O
increases	O
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
the	O
same	O
ridge	O
coefficient	O
estimates	O
as	O
the	O
left-hand	O
panel	O
but	O
instead	O
of	O
displaying	O
on	O
the	O
x-axis	O
where	O
denotes	O
the	O
vector	B
of	O
least	B
squares	I
we	O
now	O
display	O
r	O
coefficient	O
estimates	O
the	O
notation	O
denotes	O
the	O
norm	O
ell	O
of	O
a	O
vector	B
and	O
is	O
defined	O
as	O
it	O
measures	O
the	O
distance	O
of	O
from	O
zero	O
as	O
increases	O
the	O
norm	O
of	O
r	O
decrease	O
and	O
so	O
will	O
r	O
in	O
which	O
case	O
the	O
ridge	B
regression	B
coefficient	O
estimate	O
is	O
the	O
same	O
as	O
the	O
least	B
squares	I
estimate	O
and	O
so	O
their	O
norms	O
are	O
the	O
same	O
to	O
in	O
which	O
case	O
the	O
ridge	B
regression	B
coefficient	O
estimate	O
is	O
a	O
vector	B
of	O
zeros	O
with	O
norm	O
equal	O
to	O
zero	O
therefore	O
we	O
can	O
think	O
of	O
the	O
x-axis	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
as	O
the	O
amount	O
that	O
the	O
ridge	O
will	O
always	O
the	O
latter	O
quantity	O
ranges	O
from	O
p	O
j	O
norm	O
shrinkage	B
methods	O
regression	B
coefficient	O
estimates	O
have	O
been	O
shrunken	O
towards	O
zero	O
a	O
small	O
value	O
indicates	O
that	O
they	O
have	O
been	O
shrunken	O
very	O
close	O
to	O
zero	O
the	O
standard	O
least	B
squares	I
coefficient	O
estimates	O
discussed	O
in	O
chapter	O
are	O
scale	B
equivariant	I
multiplying	O
xj	O
by	O
a	O
constant	O
c	O
simply	O
leads	O
to	O
a	O
scaling	O
of	O
the	O
least	B
squares	I
coefficient	O
estimates	O
by	O
a	O
factor	B
of	O
in	O
other	O
words	O
regardless	O
of	O
how	O
the	O
jth	O
predictor	B
is	O
scaled	O
xj	O
j	O
will	O
remain	O
the	O
same	O
in	O
contrast	B
the	O
ridge	B
regression	B
coefficient	O
estimates	O
can	O
change	O
substantially	O
when	O
multiplying	O
a	O
given	O
predictor	B
by	O
a	O
constant	O
for	O
instance	O
consider	O
the	O
income	B
variable	B
which	O
is	O
measured	O
in	O
dollars	O
one	O
could	O
reasonably	O
have	O
measured	O
income	B
in	O
thousands	O
of	O
dollars	O
which	O
would	O
result	O
in	O
a	O
reduction	O
in	O
the	O
observed	O
values	O
of	O
income	B
by	O
a	O
factor	B
of	O
now	O
due	O
to	O
the	O
sum	O
of	O
squared	O
coefficients	O
term	B
in	O
the	O
ridge	B
regression	B
formulation	O
such	O
a	O
change	O
in	O
scale	O
will	O
not	O
simply	O
cause	O
the	O
ridge	B
regression	B
coefficient	O
estimate	O
for	O
income	B
to	O
change	O
by	O
a	O
factor	B
of	O
in	O
other	O
words	O
xj	O
r	O
j	O
will	O
depend	O
not	O
only	O
on	O
the	O
value	O
of	O
but	O
also	O
on	O
the	O
scaling	O
of	O
the	O
jth	O
predictor	B
in	O
fact	O
the	O
value	O
of	O
xj	O
r	O
j	O
may	O
even	O
depend	O
on	O
the	O
scaling	O
of	O
the	O
other	O
predictors	O
therefore	O
it	O
is	O
best	O
to	O
apply	O
ridge	B
regression	B
after	O
standardizing	O
the	O
predictors	O
using	O
the	O
formula	O
scale	B
equivariant	I
xij	O
n	O
xij	O
n	O
so	O
that	O
they	O
are	O
all	O
on	O
the	O
same	O
scale	O
in	O
the	O
denominator	O
is	O
the	O
estimated	O
standard	O
deviation	O
of	O
the	O
jth	O
predictor	B
consequently	O
all	O
of	O
the	O
standardized	O
predictors	O
will	O
have	O
a	O
standard	O
deviation	O
of	O
one	O
as	O
a	O
result	O
the	O
final	O
fit	O
will	O
not	O
depend	O
on	O
the	O
scale	O
on	O
which	O
the	O
predictors	O
are	O
measured	O
in	O
figure	O
the	O
y-axis	O
displays	O
the	O
standardized	O
ridge	B
regression	B
coefficient	O
estimates	O
that	O
is	O
the	O
coefficient	O
estimates	O
that	O
result	O
from	O
performing	O
ridge	B
regression	B
using	O
standardized	O
predictors	O
why	O
does	O
ridge	B
regression	B
improve	O
over	O
least	B
squares	I
ridge	B
regression	B
s	O
advantage	O
over	O
least	B
squares	I
is	O
rooted	O
in	O
the	O
bias-variance	O
trade-off	B
as	O
increases	O
the	O
flexibility	O
of	O
the	O
ridge	B
regression	B
fit	O
decreases	O
leading	O
to	O
decreased	O
variance	B
but	O
increased	O
bias	B
this	O
is	O
illustrated	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
using	O
a	O
simulated	O
data	B
set	B
containing	O
p	O
predictors	O
and	O
n	O
observations	B
the	O
green	O
curve	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
the	O
variance	B
of	O
the	O
ridge	B
regression	B
predictions	O
as	O
a	O
function	B
of	O
at	O
the	O
least	B
squares	I
coefficient	O
estimates	O
which	O
correspond	O
to	O
ridge	B
regression	B
with	O
the	O
variance	B
is	O
high	O
but	O
there	O
is	O
no	O
bias	B
but	O
as	O
increases	O
the	O
shrinkage	B
of	O
the	O
ridge	O
coefficient	O
estimates	O
leads	O
to	O
a	O
substantial	O
reduction	O
in	O
the	O
variance	B
of	O
the	O
predictions	O
at	O
the	O
expense	O
of	O
a	O
slight	O
increase	O
in	O
bias	B
recall	B
that	O
the	O
test	O
mean	B
squared	I
error	B
plotted	O
in	O
purple	O
is	O
a	O
function	B
of	O
the	O
variance	B
plus	O
the	O
squared	O
bias	B
for	O
values	O
linear	B
model	B
selection	B
and	O
regularization	B
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
figure	O
squared	O
bias	B
variance	B
and	O
test	O
mean	B
squared	I
error	B
for	O
the	O
ridge	B
regression	B
predictions	O
on	O
a	O
simulated	O
data	B
set	B
as	O
a	O
the	O
horizontal	O
dashed	O
lines	O
indicate	O
the	O
minimum	O
function	B
of	O
and	O
r	O
possible	O
mse	B
the	O
purple	O
crosses	O
indicate	O
the	O
ridge	B
regression	B
models	O
for	O
which	O
the	O
mse	B
is	O
smallest	O
of	O
up	O
to	O
about	O
the	O
variance	B
decreases	O
rapidly	O
with	O
very	O
little	O
increase	O
in	O
bias	B
plotted	O
in	O
black	O
consequently	O
the	O
mse	B
drops	O
considerably	O
as	O
increases	O
from	O
to	O
beyond	O
this	O
point	O
the	O
decrease	O
in	O
variance	B
due	O
to	O
increasing	O
slows	O
and	O
the	O
shrinkage	B
on	O
the	O
coefficients	O
causes	O
them	O
to	O
be	O
significantly	O
underestimated	O
resulting	O
in	O
a	O
large	O
increase	O
in	O
the	O
bias	B
the	O
minimum	O
mse	B
is	O
achieved	O
at	O
approximately	O
interestingly	O
because	O
of	O
its	O
high	O
variance	B
the	O
mse	B
associated	O
with	O
the	O
least	B
squares	I
fit	O
when	O
is	O
almost	O
as	O
high	O
as	O
that	O
of	O
the	O
null	B
model	B
for	O
which	O
all	O
coefficient	O
estimates	O
are	O
zero	O
when	O
however	O
for	O
an	O
intermediate	O
value	O
of	O
the	O
mse	B
is	O
considerably	O
lower	O
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
the	O
same	O
curves	O
as	O
the	O
lefthand	O
panel	O
this	O
time	O
plotted	O
against	O
the	O
norm	O
of	O
the	O
ridge	B
regression	B
coefficient	O
estimates	O
divided	O
by	O
the	O
norm	O
of	O
the	O
least	B
squares	I
estimates	O
now	O
as	O
we	O
move	O
from	O
left	O
to	O
right	O
the	O
fits	O
become	O
more	O
flexible	O
and	O
so	O
the	O
bias	B
decreases	O
and	O
the	O
variance	B
increases	O
in	O
general	O
in	O
situations	O
where	O
the	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictors	O
is	O
close	O
to	O
linear	B
the	O
least	B
squares	I
estimates	O
will	O
have	O
low	O
bias	B
but	O
may	O
have	O
high	O
variance	B
this	O
means	O
that	O
a	O
small	O
change	O
in	O
the	O
training	O
data	B
can	O
cause	O
a	O
large	O
change	O
in	O
the	O
least	B
squares	I
coefficient	O
estimates	O
in	O
particular	O
when	O
the	O
number	O
of	O
variables	O
p	O
is	O
almost	O
as	O
large	O
as	O
the	O
number	O
of	O
observations	B
n	O
as	O
in	O
the	O
example	O
in	O
figure	O
the	O
least	B
squares	I
estimates	O
will	O
be	O
extremely	O
variable	B
and	O
if	O
p	O
n	O
then	O
the	O
least	B
squares	I
estimates	O
do	O
not	O
even	O
have	O
a	O
unique	O
solution	O
whereas	O
ridge	B
regression	B
can	O
still	O
perform	O
well	O
by	O
trading	O
off	O
a	O
small	O
increase	O
in	O
bias	B
for	O
a	O
large	O
decrease	O
in	O
variance	B
hence	O
ridge	B
regression	B
works	O
best	O
in	O
situations	O
where	O
the	O
least	B
squares	I
estimates	O
have	O
high	O
variance	B
ridge	B
regression	B
also	O
has	O
substantial	O
computational	O
advantages	O
over	O
best	B
subset	B
selection	B
which	O
requires	O
searching	O
through	O
models	O
as	O
we	O
shrinkage	B
methods	O
discussed	O
previously	O
even	O
for	O
moderate	O
values	O
of	O
p	O
such	O
a	O
search	O
can	O
be	O
computationally	O
infeasible	O
in	O
contrast	B
for	O
any	O
fixed	O
value	O
of	O
ridge	B
regression	B
only	O
fits	O
a	O
single	B
model	B
and	O
the	O
model-fitting	O
procedure	O
can	O
be	O
performed	O
quite	O
quickly	O
in	O
fact	O
one	O
can	O
show	O
that	O
the	O
computations	O
required	O
to	O
solve	O
simultaneously	O
for	O
all	O
values	O
of	O
are	O
almost	O
identical	O
to	O
those	O
for	O
fitting	O
a	O
model	B
using	O
least	B
squares	I
the	O
lasso	B
ridge	B
regression	B
does	O
have	O
one	O
obvious	O
disadvantage	O
unlike	O
best	O
subset	O
forward	O
stepwise	O
and	O
backward	B
stepwise	I
selection	B
which	O
will	O
generally	O
select	O
models	O
that	O
involve	O
just	O
a	O
subset	O
of	O
the	O
variables	O
ridge	B
regression	B
will	O
include	O
all	O
p	O
predictors	O
in	O
the	O
final	O
model	B
the	O
penalty	B
j	O
in	O
will	O
shrink	O
all	O
of	O
the	O
coefficients	O
towards	O
zero	O
but	O
it	O
will	O
not	O
set	B
any	O
of	O
them	O
exactly	O
to	O
zero	O
this	O
may	O
not	O
be	O
a	O
problem	O
for	O
prediction	B
accuracy	O
but	O
it	O
can	O
create	O
a	O
challenge	O
in	O
model	B
interpretation	O
in	O
settings	O
in	O
which	O
the	O
number	O
of	O
variables	O
p	O
is	O
quite	O
large	O
for	O
example	O
in	O
the	O
credit	B
data	B
set	B
it	O
appears	O
that	O
the	O
most	O
important	O
variables	O
are	O
income	B
limit	O
rating	O
and	O
student	O
so	O
we	O
might	O
wish	O
to	O
build	O
a	O
model	B
including	O
just	O
these	O
predictors	O
however	O
ridge	B
regression	B
will	O
always	O
generate	O
a	O
model	B
involving	O
all	O
ten	O
predictors	O
increasing	O
the	O
value	O
of	O
will	O
tend	O
to	O
reduce	O
the	O
magnitudes	O
of	O
the	O
coefficients	O
but	O
will	O
not	O
result	O
in	O
exclusion	O
of	O
any	O
of	O
the	O
variables	O
the	O
lasso	B
is	O
a	O
relatively	O
recent	O
alternative	O
to	O
ridge	B
regression	B
that	O
over	O
minimize	O
the	O
quantity	O
comes	O
this	O
disadvantage	O
the	O
lasso	B
coefficients	O
l	O
lasso	B
jxij	O
j	O
rss	O
j	O
yi	O
comparing	O
to	O
we	O
see	O
that	O
the	O
lasso	B
and	O
ridge	B
regression	B
have	O
similar	O
formulations	O
the	O
only	O
difference	O
is	O
that	O
the	O
j	O
term	B
in	O
the	O
ridge	B
regression	B
penalty	B
has	O
been	O
replaced	O
by	O
j	O
in	O
the	O
lasso	B
penalty	B
in	O
statistical	O
parlance	O
the	O
lasso	B
uses	O
an	O
ell	O
penalty	B
instead	O
of	O
an	O
penalty	B
the	O
norm	O
of	O
a	O
coefficient	O
vector	B
is	O
given	O
by	O
j	O
as	O
with	O
ridge	B
regression	B
the	O
lasso	B
shrinks	O
the	O
coefficient	O
estimates	O
towards	O
zero	O
however	O
in	O
the	O
case	O
of	O
the	O
lasso	B
the	O
penalty	B
has	O
the	O
effect	O
of	O
forcing	O
some	O
of	O
the	O
coefficient	O
estimates	O
to	O
be	O
exactly	O
equal	O
to	O
zero	O
when	O
the	O
tuning	B
parameter	B
is	O
sufficiently	O
large	O
hence	O
much	O
like	O
best	B
subset	B
selection	B
the	O
lasso	B
performs	O
variable	B
selection	B
as	O
a	O
result	O
models	O
generated	O
from	O
the	O
lasso	B
are	O
generally	O
much	O
easier	O
to	O
interpret	O
than	O
those	O
produced	O
by	O
ridge	B
regression	B
we	O
say	O
that	O
the	O
lasso	B
yields	O
sparse	B
models	O
that	O
is	O
sparse	B
models	O
that	O
involve	O
only	O
a	O
subset	O
of	O
the	O
variables	O
as	O
in	O
ridge	B
regression	B
selecting	O
a	O
good	O
value	O
of	O
for	O
the	O
lasso	B
is	O
critical	O
we	O
defer	O
this	O
discussion	O
to	O
section	O
where	O
we	O
use	O
cross-validation	B
linear	B
model	B
selection	B
and	O
regularization	B
i	O
s	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
i	O
i	O
s	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
i	O
income	B
limit	O
rating	O
student	O
l	O
figure	O
the	O
standardized	O
lasso	B
coefficients	O
on	O
the	O
credit	B
data	B
set	B
are	O
shown	O
as	O
a	O
function	B
of	O
and	O
l	O
as	O
an	O
example	O
consider	O
the	O
coefficient	O
plots	O
in	O
figure	O
which	O
are	O
generated	O
from	O
applying	O
the	O
lasso	B
to	O
the	O
credit	B
data	B
set	B
when	O
then	O
the	O
lasso	B
simply	O
gives	O
the	O
least	B
squares	I
fit	O
and	O
when	O
becomes	O
sufficiently	O
large	O
the	O
lasso	B
gives	O
the	O
null	B
model	B
in	O
which	O
all	O
coefficient	O
estimates	O
equal	O
zero	O
however	O
in	O
between	O
these	O
two	O
extremes	O
the	O
ridge	B
regression	B
and	O
lasso	B
models	O
are	O
quite	O
different	O
from	O
each	O
other	O
moving	O
from	O
left	O
to	O
right	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
we	O
observe	O
that	O
at	O
first	O
the	O
lasso	B
results	O
in	O
a	O
model	B
that	O
contains	O
only	O
the	O
rating	O
predictor	B
then	O
student	O
and	O
limit	O
enter	O
the	O
model	B
almost	O
simultaneously	O
shortly	O
followed	O
by	O
income	B
eventually	O
the	O
remaining	O
variables	O
enter	O
the	O
model	B
hence	O
depending	O
on	O
the	O
value	O
of	O
the	O
lasso	B
can	O
produce	O
a	O
model	B
involving	O
any	O
number	O
of	O
variables	O
in	O
contrast	B
ridge	B
regression	B
will	O
always	O
include	O
all	O
of	O
the	O
variables	O
in	O
the	O
model	B
although	O
the	O
magnitude	O
of	O
the	O
coefficient	O
estimates	O
will	O
depend	O
on	O
another	O
formulation	O
for	O
ridge	B
regression	B
and	O
the	O
lasso	B
one	O
can	O
show	O
that	O
the	O
lasso	B
and	O
ridge	B
regression	B
coefficient	O
estimates	O
solve	O
the	O
problems	O
yi	O
subject	O
to	O
jxij	O
minimize	O
and	O
minimize	O
yi	O
subject	O
to	O
jxij	O
j	O
s	O
s	O
j	O
shrinkage	B
methods	O
respectively	O
in	O
other	O
words	O
for	O
every	O
value	O
of	O
there	O
is	O
some	O
s	O
such	O
that	O
the	O
equations	O
and	O
will	O
give	O
the	O
same	O
lasso	B
coefficient	O
estimates	O
similarly	O
for	O
every	O
value	O
of	O
there	O
is	O
a	O
corresponding	O
s	O
such	O
that	O
equations	O
and	O
will	O
give	O
the	O
same	O
ridge	B
regression	B
coefficient	O
estimates	O
when	O
p	O
then	O
indicates	O
that	O
the	O
lasso	B
coefficient	O
estimates	O
have	O
the	O
smallest	O
rss	O
out	O
of	O
all	O
points	O
that	O
lie	O
within	O
the	O
diamond	O
defined	O
by	O
s	O
similarly	O
the	O
ridge	B
regression	B
estimates	O
have	O
the	O
smallest	O
rss	O
out	O
of	O
all	O
points	O
that	O
lie	O
within	O
the	O
circle	O
defined	O
by	O
we	O
can	O
think	O
of	O
as	O
follows	O
when	O
we	O
perform	O
the	O
lasso	B
we	O
are	O
trying	O
to	O
find	O
the	O
set	B
of	O
coefficient	O
estimates	O
that	O
lead	O
to	O
the	O
smallest	O
rss	O
subject	O
to	O
the	O
constraint	O
that	O
there	O
is	O
a	O
budget	O
s	O
for	O
how	O
large	O
when	O
s	O
is	O
extremely	O
large	O
then	O
this	O
budget	O
is	O
not	O
very	O
restrictive	O
and	O
so	O
the	O
coefficient	O
estimates	O
can	O
be	O
large	O
in	O
fact	O
if	O
s	O
is	O
large	O
enough	O
that	O
the	O
least	B
squares	I
solution	O
falls	O
within	O
the	O
budget	O
then	O
will	O
simply	O
yield	O
the	O
least	B
squares	I
solution	O
in	O
contrast	B
if	O
s	O
is	O
small	O
then	O
small	O
in	O
order	O
to	O
avoid	O
violating	O
the	O
budget	O
similarly	O
indicates	O
that	O
when	O
we	O
perform	O
ridge	B
regression	B
we	O
seek	O
a	O
set	B
of	O
coefficient	O
estimates	O
such	O
that	O
the	O
rss	O
is	O
as	O
small	O
as	O
possible	O
subject	O
to	O
the	O
requirement	O
that	O
j	O
must	O
be	O
j	O
can	O
be	O
p	O
the	O
formulations	O
and	O
reveal	O
a	O
close	O
connection	O
between	O
the	O
j	O
not	O
exceed	O
the	O
budget	O
s	O
s	O
p	O
p	O
lasso	B
ridge	B
regression	B
and	O
best	B
subset	B
selection	B
consider	O
the	O
problem	O
subject	O
to	O
i	O
j	O
s	O
yi	O
jxij	O
minimize	O
here	O
i	O
j	O
is	O
an	O
indicator	B
variable	B
it	O
takes	O
on	O
a	O
value	O
of	O
if	O
j	O
and	O
equals	O
zero	O
otherwise	O
then	O
amounts	O
to	O
finding	O
a	O
set	B
of	O
coefficient	O
estimates	O
such	O
that	O
rss	O
is	O
as	O
small	O
as	O
possible	O
subject	O
to	O
the	O
constraint	O
that	O
no	O
more	O
than	O
s	O
coefficients	O
can	O
be	O
nonzero	O
the	O
problem	O
is	O
equivalent	O
to	O
best	B
subset	B
selection	B
unfortunately	O
solving	O
is	O
computationally	O
infeasible	O
when	O
p	O
is	O
large	O
since	O
it	O
requires	O
considering	O
all	O
models	O
containing	O
s	O
predictors	O
therefore	O
we	O
can	O
interpret	O
ridge	B
regression	B
and	O
the	O
lasso	B
as	O
computationally	O
feasible	O
alternatives	O
to	O
best	B
subset	B
selection	B
that	O
replace	O
the	O
intractable	O
form	O
of	O
the	O
budget	O
in	O
with	O
forms	O
that	O
are	O
much	O
easier	O
to	O
solve	O
of	O
course	O
the	O
lasso	B
is	O
much	O
more	O
closely	O
related	O
to	O
best	B
subset	B
selection	B
since	O
only	O
the	O
lasso	B
performs	O
feature	B
selection	B
for	O
s	O
sufficiently	O
small	O
in	O
p	O
s	O
the	O
variable	B
selection	B
property	O
of	O
the	O
lasso	B
why	O
is	O
it	O
that	O
the	O
lasso	B
unlike	O
ridge	B
regression	B
results	O
in	O
coefficient	O
estimates	O
that	O
are	O
exactly	O
equal	O
to	O
zero	O
the	O
formulations	O
and	O
can	O
be	O
used	O
to	O
shed	O
light	O
on	O
the	O
issue	O
figure	O
illustrates	O
the	O
situation	O
the	O
least	B
squares	I
solution	O
is	O
marked	O
as	O
while	O
the	O
blue	O
diamond	O
and	O
linear	B
model	B
selection	B
and	O
regularization	B
figure	O
contours	O
of	O
the	O
error	B
and	O
constraint	O
functions	O
for	O
the	O
lasso	B
and	O
ridge	B
regression	B
the	O
solid	O
blue	O
areas	O
are	O
the	O
constraint	O
regions	O
s	O
and	O
s	O
while	O
the	O
red	O
ellipses	O
are	O
the	O
contours	O
of	O
the	O
rss	O
circle	O
represent	O
the	O
lasso	B
and	O
ridge	B
regression	B
constraints	O
in	O
and	O
respectively	O
if	O
s	O
is	O
sufficiently	O
large	O
then	O
the	O
constraint	O
regions	O
will	O
contain	O
and	O
so	O
the	O
ridge	B
regression	B
and	O
lasso	B
estimates	O
will	O
be	O
the	O
same	O
as	O
the	O
least	B
squares	I
estimates	O
a	O
large	O
value	O
of	O
s	O
corresponds	O
to	O
in	O
and	O
however	O
in	O
figure	O
the	O
least	B
squares	I
estimates	O
lie	O
outside	O
of	O
the	O
diamond	O
and	O
the	O
circle	O
and	O
so	O
the	O
least	B
squares	I
estimates	O
are	O
not	O
the	O
same	O
as	O
the	O
lasso	B
and	O
ridge	B
regression	B
estimates	O
the	O
ellipses	O
that	O
are	O
centered	O
around	O
represent	O
regions	O
of	O
constant	O
rss	O
in	O
other	O
words	O
all	O
of	O
the	O
points	O
on	O
a	O
given	O
ellipse	O
share	O
a	O
common	O
value	O
of	O
the	O
rss	O
as	O
the	O
ellipses	O
expand	O
away	O
from	O
the	O
least	B
squares	I
coefficient	O
estimates	O
the	O
rss	O
increases	O
equations	O
and	O
indicate	O
that	O
the	O
lasso	B
and	O
ridge	B
regression	B
coefficient	O
estimates	O
are	O
given	O
by	O
the	O
first	O
point	O
at	O
which	O
an	O
ellipse	O
contacts	O
the	O
constraint	O
region	O
since	O
ridge	B
regression	B
has	O
a	O
circular	O
constraint	O
with	O
no	O
sharp	O
points	O
this	O
intersection	O
will	O
not	O
generally	O
occur	O
on	O
an	O
axis	O
and	O
so	O
the	O
ridge	B
regression	B
coefficient	O
estimates	O
will	O
be	O
exclusively	O
non-zero	O
however	O
the	O
lasso	B
constraint	O
has	O
corners	O
at	O
each	O
of	O
the	O
axes	O
and	O
so	O
the	O
ellipse	O
will	O
often	O
intersect	O
the	O
constraint	O
region	O
at	O
an	O
axis	O
when	O
this	O
occurs	O
one	O
of	O
the	O
coefficients	O
will	O
equal	O
zero	O
in	O
higher	O
dimensions	O
many	O
of	O
the	O
coefficient	O
estimates	O
may	O
equal	O
zero	O
simultaneously	O
in	O
figure	O
the	O
intersection	O
occurs	O
at	O
and	O
so	O
the	O
resulting	O
model	B
will	O
only	O
include	O
in	O
figure	O
we	O
considered	O
the	O
simple	B
case	O
of	O
p	O
when	O
p	O
then	O
the	O
constraint	O
region	O
for	O
ridge	B
regression	B
becomes	O
a	O
sphere	O
and	O
the	O
constraint	O
region	O
for	O
the	O
lasso	B
becomes	O
a	O
polyhedron	O
when	O
p	O
the	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
shrinkage	B
methods	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
on	O
training	O
data	B
figure	O
left	O
plots	O
of	O
squared	O
bias	B
variance	B
and	O
test	O
mse	B
for	O
the	O
lasso	B
on	O
a	O
simulated	O
data	B
set	B
right	O
comparison	O
of	O
squared	O
bias	B
variance	B
and	O
test	O
mse	B
between	O
lasso	B
and	O
ridge	O
both	O
are	O
plotted	O
against	O
their	O
on	O
the	O
training	O
data	B
as	O
a	O
common	O
form	O
of	O
indexing	O
the	O
crosses	O
in	O
both	O
plots	O
indicate	O
the	O
lasso	B
model	B
for	O
which	O
the	O
mse	B
is	O
smallest	O
constraint	O
for	O
ridge	B
regression	B
becomes	O
a	O
hypersphere	O
and	O
the	O
constraint	O
for	O
the	O
lasso	B
becomes	O
a	O
polytope	O
however	O
the	O
key	O
ideas	O
depicted	O
in	O
figure	O
still	O
hold	O
in	O
particular	O
the	O
lasso	B
leads	O
to	O
feature	B
selection	B
when	O
p	O
due	O
to	O
the	O
sharp	O
corners	O
of	O
the	O
polyhedron	O
or	O
polytope	O
comparing	O
the	O
lasso	B
and	O
ridge	B
regression	B
it	O
is	O
clear	O
that	O
the	O
lasso	B
has	O
a	O
major	O
advantage	O
over	O
ridge	B
regression	B
in	O
that	O
it	O
produces	O
simpler	O
and	O
more	O
interpretable	O
models	O
that	O
involve	O
only	O
a	O
subset	O
of	O
the	O
predictors	O
however	O
which	O
method	O
leads	O
to	O
better	O
prediction	B
accuracy	O
figure	O
displays	O
the	O
variance	B
squared	O
bias	B
and	O
test	O
mse	B
of	O
the	O
lasso	B
applied	O
to	O
the	O
same	O
simulated	O
data	B
as	O
in	O
figure	O
clearly	O
the	O
lasso	B
leads	O
to	O
qualitatively	O
similar	O
behavior	O
to	O
ridge	B
regression	B
in	O
that	O
as	O
increases	O
the	O
variance	B
decreases	O
and	O
the	O
bias	B
increases	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
dotted	O
lines	O
represent	O
the	O
ridge	B
regression	B
fits	O
here	O
we	O
plot	B
both	O
against	O
their	O
on	O
the	O
training	O
data	B
this	O
is	O
another	O
useful	O
way	O
to	O
index	O
models	O
and	O
can	O
be	O
used	O
to	O
compare	O
models	O
with	O
different	O
types	O
of	O
regularization	B
as	O
is	O
the	O
case	O
here	O
in	O
this	O
example	O
the	O
lasso	B
and	O
ridge	B
regression	B
result	O
in	O
almost	O
identical	O
biases	O
however	O
the	O
variance	B
of	O
ridge	B
regression	B
is	O
slightly	O
lower	O
than	O
the	O
variance	B
of	O
the	O
lasso	B
consequently	O
the	O
minimum	O
mse	B
of	O
ridge	B
regression	B
is	O
slightly	O
smaller	O
than	O
that	O
of	O
the	O
lasso	B
however	O
the	O
data	B
in	O
figure	O
were	O
generated	O
in	O
such	O
a	O
way	O
that	O
all	O
predictors	O
were	O
related	O
to	O
the	O
response	B
that	O
is	O
none	O
of	O
the	O
true	O
coefficients	O
equaled	O
zero	O
the	O
lasso	B
implicitly	O
assumes	O
that	O
a	O
number	O
of	O
the	O
coefficients	O
truly	O
equal	O
zero	O
consequently	O
it	O
is	O
not	O
surprising	O
that	O
ridge	B
regression	B
outperforms	O
the	O
lasso	B
in	O
terms	O
of	O
prediction	B
error	B
in	O
this	O
setting	O
figure	O
illustrates	O
a	O
similar	O
situation	O
except	O
that	O
now	O
the	O
response	B
is	O
a	O
linear	B
model	B
selection	B
and	O
regularization	B
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
on	O
training	O
data	B
figure	O
left	O
plots	O
of	O
squared	O
bias	B
variance	B
and	O
test	O
mse	B
for	O
the	O
lasso	B
the	O
simulated	O
data	B
is	O
similar	O
to	O
that	O
in	O
figure	O
except	O
that	O
now	O
only	O
two	O
predictors	O
are	O
related	O
to	O
the	O
response	B
right	O
comparison	O
of	O
squared	O
bias	B
variance	B
and	O
test	O
mse	B
between	O
lasso	B
and	O
ridge	O
both	O
are	O
plotted	O
against	O
their	O
on	O
the	O
training	O
data	B
as	O
a	O
common	O
form	O
of	O
indexing	O
the	O
crosses	O
in	O
both	O
plots	O
indicate	O
the	O
lasso	B
model	B
for	O
which	O
the	O
mse	B
is	O
smallest	O
function	B
of	O
only	O
out	O
of	O
predictors	O
now	O
the	O
lasso	B
tends	O
to	O
outperform	O
ridge	B
regression	B
in	O
terms	O
of	O
bias	B
variance	B
and	O
mse	B
these	O
two	O
examples	O
illustrate	O
that	O
neither	O
ridge	B
regression	B
nor	O
the	O
lasso	B
will	O
universally	O
dominate	O
the	O
other	O
in	O
general	O
one	O
might	O
expect	O
the	O
lasso	B
to	O
perform	O
better	O
in	O
a	O
setting	O
where	O
a	O
relatively	O
small	O
number	O
of	O
predictors	O
have	O
substantial	O
coefficients	O
and	O
the	O
remaining	O
predictors	O
have	O
coefficients	O
that	O
are	O
very	O
small	O
or	O
that	O
equal	O
zero	O
ridge	B
regression	B
will	O
perform	O
better	O
when	O
the	O
response	B
is	O
a	O
function	B
of	O
many	O
predictors	O
all	O
with	O
coefficients	O
of	O
roughly	O
equal	O
size	O
however	O
the	O
number	O
of	O
predictors	O
that	O
is	O
related	O
to	O
the	O
response	B
is	O
never	O
known	O
a	O
priori	O
for	O
real	O
data	B
sets	O
a	O
technique	O
such	O
as	O
cross-validation	B
can	O
be	O
used	O
in	O
order	O
to	O
determine	O
which	O
approach	B
is	O
better	O
on	O
a	O
particular	O
data	B
set	B
as	O
with	O
ridge	B
regression	B
when	O
the	O
least	B
squares	I
estimates	O
have	O
excessively	O
high	O
variance	B
the	O
lasso	B
solution	O
can	O
yield	O
a	O
reduction	O
in	O
variance	B
at	O
the	O
expense	O
of	O
a	O
small	O
increase	O
in	O
bias	B
and	O
consequently	O
can	O
generate	O
more	O
accurate	O
predictions	O
unlike	O
ridge	B
regression	B
the	O
lasso	B
performs	O
variable	B
selection	B
and	O
hence	O
results	O
in	O
models	O
that	O
are	O
easier	O
to	O
interpret	O
there	O
are	O
very	O
efficient	O
algorithms	O
for	O
fitting	O
both	O
ridge	O
and	O
lasso	B
models	O
in	O
both	O
cases	O
the	O
entire	O
coefficient	O
paths	O
can	O
be	O
computed	O
with	O
about	O
the	O
same	O
amount	O
of	O
work	O
as	O
a	O
single	B
least	B
squares	I
fit	O
we	O
will	O
explore	O
this	O
further	O
in	O
the	O
lab	O
at	O
the	O
end	O
of	O
this	O
chapter	O
a	O
simple	B
special	O
case	O
for	O
ridge	B
regression	B
and	O
the	O
lasso	B
in	O
order	O
to	O
obtain	O
a	O
better	O
intuition	O
about	O
the	O
behavior	O
of	O
ridge	B
regression	B
and	O
the	O
lasso	B
consider	O
a	O
simple	B
special	O
case	O
with	O
n	O
p	O
and	O
x	O
a	O
diagonal	O
matrix	O
with	O
s	O
on	O
the	O
diagonal	O
and	O
s	O
in	O
all	O
off-diagonal	O
elements	O
to	O
simplify	O
the	O
problem	O
further	O
assume	O
also	O
that	O
we	O
are	O
performing	O
regres	O
shrinkage	B
methods	O
sion	O
without	O
an	O
intercept	B
with	O
these	O
assumptions	O
the	O
usual	O
least	B
squares	I
problem	O
simplifies	O
to	O
finding	O
p	O
that	O
minimize	O
in	O
this	O
case	O
the	O
least	B
squares	I
solution	O
is	O
given	O
by	O
j	O
yj	O
and	O
in	O
this	O
setting	O
ridge	B
regression	B
amounts	O
to	O
finding	O
p	O
such	O
that	O
j	O
is	O
minimized	O
and	O
the	O
lasso	B
amounts	O
to	O
finding	O
the	O
coefficients	O
such	O
that	O
j	O
is	O
minimized	O
one	O
can	O
show	O
that	O
in	O
this	O
setting	O
the	O
ridge	B
regression	B
estimates	O
take	O
the	O
form	O
r	O
j	O
and	O
the	O
lasso	B
estimates	O
take	O
the	O
form	O
l	O
j	O
yj	O
yj	O
if	O
yj	O
if	O
yj	O
if	O
figure	O
displays	O
the	O
situation	O
we	O
can	O
see	O
that	O
ridge	B
regression	B
and	O
the	O
lasso	B
perform	O
two	O
very	O
different	O
types	O
of	O
shrinkage	B
in	O
ridge	B
regression	B
each	O
least	B
squares	I
coefficient	O
estimate	O
is	O
shrunken	O
by	O
the	O
same	O
proportion	O
in	O
contrast	B
the	O
lasso	B
shrinks	O
each	O
least	B
squares	I
coefficient	O
towards	O
zero	O
by	O
a	O
constant	O
amount	O
the	O
least	B
squares	I
coefficients	O
that	O
are	O
less	O
than	O
in	O
absolute	O
value	O
are	O
shrunken	O
entirely	O
to	O
zero	O
the	O
type	O
of	O
shrinkage	B
performed	O
by	O
the	O
lasso	B
in	O
this	O
simple	B
setting	O
is	O
known	O
as	O
softthresholding	O
the	O
fact	O
that	O
some	O
lasso	B
coefficients	O
are	O
shrunken	O
entirely	O
to	O
zero	O
explains	O
why	O
the	O
lasso	B
performs	O
feature	B
selection	B
in	O
the	O
case	O
of	O
a	O
more	O
general	O
data	B
matrix	O
x	O
the	O
story	O
is	O
a	O
little	O
more	O
complicated	O
than	O
what	O
is	O
depicted	O
in	O
figure	O
but	O
the	O
main	O
ideas	O
still	O
hold	O
approximately	O
ridge	B
regression	B
more	O
or	O
less	O
shrinks	O
every	O
dimension	O
of	O
the	O
data	B
by	O
the	O
same	O
proportion	O
whereas	O
the	O
lasso	B
more	O
or	O
less	O
shrinks	O
all	O
coefficients	O
toward	O
zero	O
by	O
a	O
similar	O
amount	O
and	O
sufficiently	O
small	O
coefficients	O
are	O
shrunken	O
all	O
the	O
way	O
to	O
zero	O
softthresholding	O
linear	B
model	B
selection	B
and	O
regularization	B
e	O
t	O
a	O
m	O
i	O
t	O
s	O
e	O
i	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
ridge	O
least	B
squares	I
e	O
t	O
a	O
m	O
i	O
t	O
s	O
e	O
i	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
lasso	B
least	B
squares	I
yj	O
yj	O
figure	O
the	O
ridge	B
regression	B
and	O
lasso	B
coefficient	O
estimates	O
for	O
a	O
simple	B
setting	O
with	O
n	O
p	O
and	O
x	O
a	O
diagonal	O
matrix	O
with	O
s	O
on	O
the	O
diagonal	O
left	O
the	O
ridge	B
regression	B
coefficient	O
estimates	O
are	O
shrunken	O
proportionally	O
towards	O
zero	O
relative	O
to	O
the	O
least	B
squares	I
estimates	O
right	O
the	O
lasso	B
coefficient	O
estimates	O
are	O
soft-thresholded	O
towards	O
zero	O
bayesian	B
interpretation	O
for	O
ridge	B
regression	B
and	O
the	O
lasso	B
we	O
now	O
show	O
that	O
one	O
can	O
view	O
ridge	B
regression	B
and	O
the	O
lasso	B
through	O
a	O
bayesian	B
lens	O
a	O
bayesian	B
viewpoint	O
for	O
regression	B
assumes	O
that	O
the	O
coefficient	O
vector	B
has	O
some	O
prior	O
distribution	B
say	O
p	O
where	O
pt	O
the	O
likelihood	O
of	O
the	O
data	B
can	O
be	O
written	O
as	O
f	O
where	O
x	O
xp	O
multiplying	O
the	O
prior	O
distribution	B
by	O
the	O
likelihood	O
gives	O
us	O
to	O
a	O
proportionality	O
constant	O
the	O
posterior	O
distribution	B
which	O
takes	O
the	O
form	O
posterior	O
distribution	B
p	O
y	O
f	O
f	O
where	O
the	O
proportionality	O
above	O
follows	O
from	O
bayes	O
theorem	O
and	O
the	O
equality	O
above	O
follows	O
from	O
the	O
assumption	O
that	O
x	O
is	O
fixed	O
we	O
assume	O
the	O
usual	O
linear	B
model	B
y	O
xp	O
p	O
and	O
suppose	O
that	O
the	O
errors	O
are	O
independent	B
and	O
drawn	O
from	O
a	O
normal	O
disp	O
tribution	O
furthermore	O
assume	O
that	O
p	O
g	O
j	O
for	O
some	O
density	B
function	B
g	O
it	O
turns	O
out	O
that	O
ridge	B
regression	B
and	O
the	O
lasso	B
follow	O
naturally	O
from	O
two	O
special	O
cases	O
of	O
g	O
if	O
g	O
is	O
a	O
gaussian	O
distribution	B
with	O
mean	O
zero	O
and	O
standard	O
deviation	O
a	O
function	B
of	O
then	O
it	O
follows	O
that	O
the	O
posterior	O
mode	B
for	O
that	O
is	O
the	O
most	O
likely	O
value	O
for	O
given	O
the	O
data	B
is	O
given	O
by	O
the	O
ridge	B
regression	B
solution	O
fact	O
the	O
ridge	B
regression	B
solution	O
is	O
also	O
the	O
posterior	O
mean	O
posterior	O
mode	B
shrinkage	B
methods	O
j	O
j	O
j	O
j	O
figure	O
left	O
ridge	B
regression	B
is	O
the	O
posterior	O
mode	B
for	O
under	O
a	O
gaussian	O
prior	O
right	O
the	O
lasso	B
is	O
the	O
posterior	O
mode	B
for	O
under	O
a	O
double-exponential	O
prior	O
if	O
g	O
is	O
a	O
double-exponential	O
distribution	B
with	O
mean	O
zero	O
and	O
scale	O
parameter	B
a	O
function	B
of	O
then	O
it	O
follows	O
that	O
the	O
posterior	O
mode	B
for	O
is	O
the	O
lasso	B
solution	O
the	O
lasso	B
solution	O
is	O
not	O
the	O
posterior	O
mean	O
and	O
in	O
fact	O
the	O
posterior	O
mean	O
does	O
not	O
yield	O
a	O
sparse	B
coefficient	O
vector	B
the	O
gaussian	O
and	O
double-exponential	O
priors	O
are	O
displayed	O
in	O
figure	O
therefore	O
from	O
a	O
bayesian	B
viewpoint	O
ridge	B
regression	B
and	O
the	O
lasso	B
follow	O
directly	O
from	O
assuming	O
the	O
usual	O
linear	B
model	B
with	O
normal	O
errors	O
together	O
with	O
a	O
simple	B
prior	O
distribution	B
for	O
notice	O
that	O
the	O
lasso	B
prior	O
is	O
steeply	O
peaked	O
at	O
zero	O
while	O
the	O
gaussian	O
is	O
flatter	O
and	O
fatter	O
at	O
zero	O
hence	O
the	O
lasso	B
expects	O
a	O
priori	O
that	O
many	O
of	O
the	O
coefficients	O
are	O
zero	O
while	O
ridge	O
assumes	O
the	O
coefficients	O
are	O
randomly	O
distributed	O
about	O
zero	O
selecting	O
the	O
tuning	B
parameter	B
just	O
as	O
the	O
subset	B
selection	B
approaches	O
considered	O
in	O
section	O
require	O
a	O
method	O
to	O
determine	O
which	O
of	O
the	O
models	O
under	O
consideration	O
is	O
best	O
implementing	O
ridge	B
regression	B
and	O
the	O
lasso	B
requires	O
a	O
method	O
for	O
selecting	O
a	O
value	O
for	O
the	O
tuning	B
parameter	B
in	O
and	O
or	O
equivalently	O
the	O
value	O
of	O
the	O
constraint	O
s	O
in	O
and	O
cross-validation	B
provides	O
a	O
simple	B
way	O
to	O
tackle	O
this	O
problem	O
we	O
choose	O
a	O
grid	O
of	O
values	O
and	O
compute	O
the	O
cross-validation	B
error	B
for	O
each	O
value	O
of	O
as	O
described	O
in	O
chapter	O
we	O
then	O
select	O
the	O
tuning	B
parameter	B
value	O
for	O
which	O
the	O
cross-validation	B
error	B
is	O
smallest	O
finally	O
the	O
model	B
is	O
re-fit	O
using	O
all	O
of	O
the	O
available	O
observations	B
and	O
the	O
selected	O
value	O
of	O
the	O
tuning	B
parameter	B
figure	O
displays	O
the	O
choice	O
of	O
that	O
results	O
from	O
performing	O
leaveone-out	O
cross-validation	B
on	O
the	O
ridge	B
regression	B
fits	O
from	O
the	O
credit	B
data	B
set	B
the	O
dashed	O
vertical	O
lines	O
indicate	O
the	O
selected	O
value	O
of	O
in	O
this	O
case	O
the	O
value	O
is	O
relatively	O
small	O
indicating	O
that	O
the	O
optimal	O
fit	O
only	O
involves	O
a	O
linear	B
model	B
selection	B
and	O
regularization	B
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
s	O
s	O
o	O
r	O
c	O
s	O
t	O
i	O
n	O
e	O
c	O
i	O
f	O
f	O
i	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
s	O
t	O
figure	O
left	O
cross-validation	B
errors	O
that	O
result	O
from	O
applying	O
ridge	B
regression	B
to	O
the	O
credit	B
data	B
set	B
with	O
various	O
value	O
of	O
right	O
the	O
coefficient	O
estimates	O
as	O
a	O
function	B
of	O
the	O
vertical	O
dashed	O
lines	O
indicate	O
the	O
value	O
of	O
selected	O
by	O
cross-validation	B
small	O
amount	O
of	O
shrinkage	B
relative	O
to	O
the	O
least	B
squares	I
solution	O
in	O
addition	O
the	O
dip	O
is	O
not	O
very	O
pronounced	O
so	O
there	O
is	O
rather	O
a	O
wide	O
range	O
of	O
values	O
that	O
would	O
give	O
very	O
similar	O
error	B
in	O
a	O
case	O
like	O
this	O
we	O
might	O
simply	O
use	O
the	O
least	B
squares	I
solution	O
figure	O
provides	O
an	O
illustration	O
of	O
ten-fold	O
cross-validation	B
applied	O
to	O
the	O
lasso	B
fits	O
on	O
the	O
sparse	B
simulated	O
data	B
from	O
figure	O
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
the	O
cross-validation	B
error	B
while	O
the	O
right-hand	O
panel	O
displays	O
the	O
coefficient	O
estimates	O
the	O
vertical	O
dashed	O
lines	O
indicate	O
the	O
point	O
at	O
which	O
the	O
cross-validation	B
error	B
is	O
smallest	O
the	O
two	O
colored	O
lines	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
represent	O
the	O
two	O
predictors	O
that	O
are	O
related	O
to	O
the	O
response	B
while	O
the	O
grey	O
lines	O
represent	O
the	O
unrelated	O
predictors	O
these	O
are	O
often	O
referred	O
to	O
as	O
signal	B
and	O
noise	B
variables	O
respectively	O
not	O
only	O
has	O
the	O
lasso	B
correctly	O
given	O
much	O
larger	O
coefficient	O
estimates	O
to	O
the	O
two	O
signal	B
predictors	O
but	O
also	O
the	O
minimum	O
crossvalidation	O
error	B
corresponds	O
to	O
a	O
set	B
of	O
coefficient	O
estimates	O
for	O
which	O
only	O
the	O
signal	B
variables	O
are	O
non-zero	O
hence	O
cross-validation	B
together	O
with	O
the	O
lasso	B
has	O
correctly	O
identified	O
the	O
two	O
signal	B
variables	O
in	O
the	O
model	B
even	O
though	O
this	O
is	O
a	O
challenging	O
setting	O
with	O
p	O
variables	O
and	O
only	O
n	O
observations	B
in	O
contrast	B
the	O
least	B
squares	I
solution	O
displayed	O
on	O
the	O
far	O
right	O
of	O
the	O
right-hand	O
panel	O
of	O
figure	O
assigns	O
a	O
large	O
coefficient	O
estimate	O
to	O
only	O
one	O
of	O
the	O
two	O
signal	B
variables	O
signal	B
dimension	B
reduction	I
methods	O
the	O
methods	O
that	O
we	O
have	O
discussed	O
so	O
far	O
in	O
this	O
chapter	O
have	O
controlled	O
variance	B
in	O
two	O
different	O
ways	O
either	O
by	O
using	O
a	O
subset	O
of	O
the	O
original	O
variables	O
or	O
by	O
shrinking	O
their	O
coefficients	O
toward	O
zero	O
all	O
of	O
these	O
methods	O
dimension	B
reduction	I
methods	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
s	O
s	O
o	O
r	O
c	O
s	O
t	O
i	O
n	O
e	O
c	O
i	O
f	O
f	O
i	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
s	O
t	O
l	O
l	O
dimension	B
reduction	I
linear	B
combination	I
figure	O
left	O
ten-fold	O
cross-validation	B
mse	B
for	O
the	O
lasso	B
applied	O
to	O
the	O
sparse	B
simulated	O
data	B
set	B
from	O
figure	O
right	O
the	O
corresponding	O
lasso	B
coefficient	O
estimates	O
are	O
displayed	O
the	O
vertical	O
dashed	O
lines	O
indicate	O
the	O
lasso	B
fit	O
for	O
which	O
the	O
cross-validation	B
error	B
is	O
smallest	O
are	O
defined	O
using	O
the	O
original	O
predictors	O
xp	O
we	O
now	O
explore	O
a	O
class	O
of	O
approaches	O
that	O
transform	O
the	O
predictors	O
and	O
then	O
fit	O
a	O
least	B
squares	I
model	B
using	O
the	O
transformed	O
variables	O
we	O
will	O
refer	O
to	O
these	O
techniques	O
as	O
dimension	B
reduction	I
methods	O
let	O
zm	O
represent	O
m	O
p	O
linear	B
combinations	O
of	O
our	O
original	O
p	O
predictors	O
that	O
is	O
zm	O
jmxj	O
for	O
some	O
constants	O
pm	O
m	O
m	O
we	O
can	O
then	O
fit	O
the	O
linear	B
regression	B
model	B
yi	O
mzim	O
i	O
n	O
using	O
least	B
squares	I
note	O
that	O
in	O
the	O
regression	B
coefficients	O
are	O
given	O
by	O
m	O
if	O
the	O
constants	O
pm	O
are	O
chosen	O
wisely	O
then	O
such	O
dimension	B
reduction	I
approaches	O
can	O
often	O
outperform	O
least	B
squares	I
regression	B
in	O
other	O
words	O
fitting	O
using	O
least	B
squares	I
can	O
lead	O
to	O
better	O
results	O
than	O
fitting	O
using	O
least	B
squares	I
the	O
term	B
dimension	B
reduction	I
comes	O
from	O
the	O
fact	O
that	O
this	O
approach	B
reduces	O
the	O
problem	O
of	O
estimating	O
the	O
p	O
coefficients	O
p	O
to	O
the	O
simpler	O
problem	O
of	O
estimating	O
the	O
m	O
coefficients	O
m	O
where	O
m	O
p	O
in	O
other	O
words	O
the	O
dimension	O
of	O
the	O
problem	O
has	O
been	O
reduced	O
from	O
p	O
to	O
m	O
notice	O
that	O
from	O
mzim	O
m	O
jmxij	O
m	O
jmxij	O
jxij	O
linear	B
model	B
selection	B
and	O
regularization	B
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
population	O
figure	O
the	O
population	O
size	O
and	O
ad	O
spending	O
for	O
different	O
cities	O
are	O
shown	O
as	O
purple	O
circles	O
the	O
green	O
solid	O
line	B
indicates	O
the	O
first	O
principal	O
component	O
and	O
the	O
blue	O
dashed	O
line	B
indicates	O
the	O
second	O
principal	O
component	O
where	O
j	O
m	O
jm	O
hence	O
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
case	O
of	O
the	O
original	O
linear	B
regression	B
model	B
given	O
by	O
dimension	B
reduction	I
serves	O
to	O
constrain	O
the	O
estimated	O
j	O
coefficients	O
since	O
now	O
they	O
must	O
take	O
the	O
form	O
this	O
constraint	O
on	O
the	O
form	O
of	O
the	O
coefficients	O
has	O
the	O
potential	O
to	O
bias	B
the	O
coefficient	O
estimates	O
however	O
in	O
situations	O
where	O
p	O
is	O
large	O
relative	O
to	O
n	O
selecting	O
a	O
value	O
of	O
m	O
p	O
can	O
significantly	O
reduce	O
the	O
variance	B
of	O
the	O
fitted	O
coefficients	O
if	O
m	O
p	O
and	O
all	O
the	O
zm	O
are	O
linearly	O
independent	B
then	O
poses	O
no	O
constraints	O
in	O
this	O
case	O
no	O
dimension	B
reduction	I
occurs	O
and	O
so	O
fitting	O
is	O
equivalent	O
to	O
performing	O
least	B
squares	I
on	O
the	O
original	O
p	O
predictors	O
all	O
dimension	B
reduction	I
methods	O
work	O
in	O
two	O
steps	O
first	O
the	O
transformed	O
predictors	O
zm	O
are	O
obtained	O
second	O
the	O
model	B
is	O
fit	O
using	O
these	O
m	O
predictors	O
however	O
the	O
choice	O
of	O
zm	O
or	O
equivalently	O
the	O
selection	B
of	O
the	O
jm	O
s	O
can	O
be	O
achieved	O
in	O
different	O
ways	O
in	O
this	O
chapter	O
we	O
will	O
consider	O
two	O
approaches	O
for	O
this	O
task	O
principal	B
components	I
and	O
partial	B
least	B
squares	I
principal	B
components	I
regression	B
principal	B
components	I
analysis	B
is	O
a	O
popular	O
approach	B
for	O
deriving	O
a	O
low-dimensional	B
set	B
of	O
features	O
from	O
a	O
large	O
set	B
of	O
variables	O
pca	O
is	O
discussed	O
in	O
greater	O
detail	O
as	O
a	O
tool	O
for	O
unsupervised	B
learning	I
in	O
chapter	O
here	O
we	O
describe	O
its	O
use	O
as	O
a	O
dimension	B
reduction	I
technique	O
for	O
regression	B
principal	B
components	I
analysis	B
dimension	B
reduction	I
methods	O
an	O
overview	O
of	O
principal	B
components	I
analysis	B
pca	O
is	O
a	O
technique	O
for	O
reducing	O
the	O
dimension	O
of	O
a	O
n	O
p	O
data	B
matrix	O
x	O
the	O
first	O
principal	O
component	O
direction	O
of	O
the	O
data	B
is	O
that	O
along	O
which	O
the	O
observations	B
vary	O
the	O
most	O
for	O
instance	O
consider	O
figure	O
which	O
shows	O
population	O
size	O
in	O
tens	O
of	O
thousands	O
of	O
people	O
and	O
ad	O
spending	O
for	O
a	O
particular	O
company	O
in	O
thousands	O
of	O
dollars	O
for	O
cities	O
the	O
green	O
solid	O
line	B
represents	O
the	O
first	O
principal	O
component	O
direction	O
of	O
the	O
data	B
we	O
can	O
see	O
by	O
eye	O
that	O
this	O
is	O
the	O
direction	O
along	O
which	O
there	O
is	O
the	O
greatest	O
variability	O
in	O
the	O
data	B
that	O
is	O
if	O
we	O
projected	O
the	O
observations	B
onto	O
this	O
line	B
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
then	O
the	O
resulting	O
projected	O
observations	B
would	O
have	O
the	O
largest	O
possible	O
variance	B
projecting	O
the	O
observations	B
onto	O
any	O
other	O
line	B
would	O
yield	O
projected	O
observations	B
with	O
lower	O
variance	B
projecting	O
a	O
point	O
onto	O
a	O
line	B
simply	O
involves	O
finding	O
the	O
location	O
on	O
the	O
line	B
which	O
is	O
closest	O
to	O
the	O
point	O
the	O
first	O
principal	O
component	O
is	O
displayed	O
graphically	O
in	O
figure	O
but	O
how	O
can	O
it	O
be	O
summarized	O
mathematically	O
it	O
is	O
given	O
by	O
the	O
formula	O
pop	O
ad	O
here	O
and	O
are	O
the	O
principal	O
component	O
loadings	O
which	O
define	O
the	O
direction	O
referred	O
to	O
above	O
in	O
pop	O
indicates	O
the	O
mean	O
of	O
all	O
pop	O
values	O
in	O
this	O
data	B
set	B
and	O
ad	O
indicates	O
the	O
mean	O
of	O
all	O
advertising	B
spending	O
the	O
idea	O
is	O
that	O
out	O
of	O
every	O
possible	O
linear	B
combination	I
of	O
pop	O
and	O
ad	O
such	O
that	O
this	O
particular	O
linear	B
combination	I
yields	O
the	O
highest	O
variance	B
i	O
e	O
this	O
is	O
the	O
linear	B
combination	I
for	O
which	O
var	O
pop	O
ad	O
is	O
maximized	O
it	O
is	O
necessary	O
to	O
consider	O
only	O
linear	B
combinations	O
of	O
the	O
form	O
since	O
otherwise	O
we	O
could	O
increase	O
and	O
arbitrarily	O
in	O
order	O
to	O
blow	O
up	O
the	O
variance	B
in	O
the	O
two	O
loadings	O
are	O
both	O
positive	O
and	O
have	O
similar	O
size	O
and	O
so	O
is	O
almost	O
an	O
average	B
of	O
the	O
two	O
variables	O
since	O
n	O
pop	O
and	O
ad	O
are	O
vectors	O
of	O
length	O
and	O
so	O
is	O
in	O
for	O
instance	O
pop	O
ad	O
the	O
values	O
of	O
are	O
known	O
as	O
the	O
principal	O
component	O
scores	O
and	O
can	O
be	O
seen	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
there	O
is	O
also	O
another	O
interpretation	O
for	O
pca	O
the	O
first	O
principal	O
component	O
vector	B
defines	O
the	O
line	B
that	O
is	O
as	O
close	O
as	O
possible	O
to	O
the	O
data	B
for	O
instance	O
in	O
figure	O
the	O
first	O
principal	O
component	O
line	B
minimizes	O
the	O
sum	O
of	O
the	O
squared	O
perpendicular	B
distances	O
between	O
each	O
point	O
and	O
the	O
line	B
these	O
distances	O
are	O
plotted	O
as	O
dashed	O
line	B
segments	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
in	O
which	O
the	O
crosses	O
represent	O
the	O
projection	B
of	O
each	O
point	O
onto	O
the	O
first	O
principal	O
component	O
line	B
the	O
first	O
principal	O
component	O
has	O
been	O
chosen	O
so	O
that	O
the	O
projected	O
observations	B
are	O
as	O
close	O
as	O
possible	O
to	O
the	O
original	O
observations	B
linear	B
model	B
selection	B
and	O
regularization	B
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
l	O
i	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
population	O
principal	O
component	O
figure	O
a	O
subset	O
of	O
the	O
advertising	B
data	B
the	O
mean	O
pop	O
and	O
ad	O
budgets	O
are	O
indicated	O
with	O
a	O
blue	O
circle	O
left	O
the	O
first	O
principal	O
component	O
direction	O
is	O
shown	O
in	O
green	O
it	O
is	O
the	O
dimension	O
along	O
which	O
the	O
data	B
vary	O
the	O
most	O
and	O
it	O
also	O
defines	O
the	O
line	B
that	O
is	O
closest	O
to	O
all	O
n	O
of	O
the	O
observations	B
the	O
distances	O
from	O
each	O
observation	O
to	O
the	O
principal	O
component	O
are	O
represented	O
using	O
the	O
black	O
dashed	O
line	B
segments	O
the	O
blue	O
dot	O
represents	O
ad	O
right	O
the	O
left-hand	O
panel	O
has	O
been	O
rotated	O
so	O
that	O
the	O
first	O
principal	O
component	O
direction	O
coincides	O
with	O
the	O
x-axis	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
left-hand	O
panel	O
has	O
been	O
rotated	O
so	O
that	O
the	O
first	O
principal	O
component	O
direction	O
coincides	O
with	O
the	O
x-axis	O
it	O
is	O
possible	O
to	O
show	O
that	O
the	O
first	O
principal	O
component	O
score	O
for	O
the	O
ith	O
observation	O
given	O
in	O
is	O
the	O
distance	O
in	O
the	O
x-direction	O
of	O
the	O
ith	O
cross	O
from	O
zero	O
so	O
for	O
example	O
the	O
point	O
in	O
the	O
bottom-left	O
corner	O
of	O
the	O
left-hand	O
panel	O
of	O
figure	O
has	O
a	O
large	O
negative	O
principal	O
component	O
score	O
while	O
the	O
point	O
in	O
the	O
top-right	O
corner	O
has	O
a	O
large	O
positive	O
score	O
these	O
scores	O
can	O
be	O
computed	O
directly	O
using	O
we	O
can	O
think	O
of	O
the	O
values	O
of	O
the	O
principal	O
component	O
as	O
singlenumber	O
summaries	O
of	O
the	O
joint	O
pop	O
and	O
ad	O
budgets	O
for	O
each	O
location	O
in	O
this	O
example	O
if	O
pop	O
ad	O
then	O
this	O
indicates	O
a	O
city	O
with	O
below-average	O
population	O
size	O
and	O
belowaverage	O
ad	O
spending	O
a	O
positive	O
score	O
suggests	O
the	O
opposite	O
how	O
well	O
can	O
a	O
single	B
number	O
represent	O
both	O
pop	O
and	O
ad	O
in	O
this	O
case	O
figure	O
indicates	O
that	O
pop	O
and	O
ad	O
have	O
approximately	O
a	O
linear	B
relationship	O
and	O
so	O
we	O
might	O
expect	O
that	O
a	O
single-number	O
summary	O
will	O
work	O
well	O
figure	O
displays	O
versus	O
both	O
pop	O
and	O
ad	O
the	O
plots	O
show	O
a	O
strong	O
relationship	O
between	O
the	O
first	O
principal	O
component	O
and	O
the	O
two	O
features	O
in	O
other	O
words	O
the	O
first	O
principal	O
component	O
appears	O
to	O
capture	O
most	O
of	O
the	O
information	O
contained	O
in	O
the	O
pop	O
and	O
ad	O
predictors	O
so	O
far	O
we	O
have	O
concentrated	O
on	O
the	O
first	O
principal	O
component	O
in	O
general	O
one	O
can	O
construct	O
up	O
to	O
p	O
distinct	O
principal	B
components	I
the	O
second	O
principal	O
component	O
is	O
a	O
linear	B
combination	I
of	O
the	O
variables	O
that	O
is	O
uncorrelated	O
with	O
and	O
has	O
largest	O
variance	B
subject	O
to	O
this	O
constraint	O
the	O
second	O
principal	O
component	O
direction	O
is	O
illustrated	O
as	O
a	O
dashed	O
blue	O
line	B
in	O
figure	O
it	O
turns	O
out	O
that	O
the	O
zero	O
correlation	B
condition	O
of	O
with	O
principal	B
components	I
were	O
calculated	O
after	O
first	O
standardizing	O
both	O
pop	O
and	O
ad	O
a	O
common	O
approach	B
hence	O
the	O
x-axes	O
on	O
figures	O
and	O
are	O
not	O
on	O
the	O
same	O
scale	O
dimension	B
reduction	I
methods	O
n	O
o	O
i	O
t	O
l	O
a	O
u	O
p	O
o	O
p	O
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
principal	O
component	O
principal	O
component	O
figure	O
plots	O
of	O
the	O
first	O
principal	O
component	O
scores	O
versus	O
pop	O
and	O
ad	O
the	O
relationships	O
are	O
strong	O
is	O
equivalent	O
to	O
the	O
condition	O
that	O
the	O
direction	O
must	O
be	O
perpendicular	B
or	O
orthogonal	B
to	O
the	O
first	O
principal	O
component	O
direction	O
the	O
second	O
principal	O
component	O
is	O
given	O
by	O
the	O
formula	O
pop	O
ad	O
perpendicular	B
orthogonal	B
since	O
the	O
advertising	B
data	B
has	O
two	O
predictors	O
the	O
first	O
two	O
principal	B
components	I
contain	O
all	O
of	O
the	O
information	O
that	O
is	O
in	O
pop	O
and	O
ad	O
however	O
by	O
construction	O
the	O
first	O
component	O
will	O
contain	O
the	O
most	O
information	O
consider	O
for	O
example	O
the	O
much	O
larger	O
variability	O
of	O
x-axis	O
versus	O
y-axis	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
fact	O
that	O
the	O
second	O
principal	O
component	O
scores	O
are	O
much	O
closer	O
to	O
zero	O
indicates	O
that	O
this	O
component	O
captures	O
far	O
less	O
information	O
as	O
another	O
illustration	O
figure	O
displays	O
versus	O
pop	O
and	O
ad	O
there	O
is	O
little	O
relationship	O
between	O
the	O
second	O
principal	O
component	O
and	O
these	O
two	O
predictors	O
again	O
suggesting	O
that	O
in	O
this	O
case	O
one	O
only	O
needs	O
the	O
first	O
principal	O
component	O
in	O
order	O
to	O
accurately	O
represent	O
the	O
pop	O
and	O
ad	O
budgets	O
with	O
two-dimensional	O
data	B
such	O
as	O
in	O
our	O
advertising	B
example	O
we	O
can	O
construct	O
at	O
most	O
two	O
principal	B
components	I
however	O
if	O
we	O
had	O
other	O
predictors	O
such	O
as	O
population	O
age	O
income	B
level	B
education	O
and	O
so	O
forth	O
then	O
additional	O
components	O
could	O
be	O
constructed	O
they	O
would	O
successively	O
maximize	O
variance	B
subject	O
to	O
the	O
constraint	O
of	O
being	O
uncorrelated	O
with	O
the	O
preceding	O
components	O
the	O
principal	B
components	I
regression	B
approach	B
the	O
predictors	O
the	O
principal	B
components	I
regression	B
approach	B
involves	O
constructing	O
the	O
first	O
m	O
principal	B
components	I
zm	O
and	O
then	O
using	O
these	O
components	O
as	O
is	O
fit	O
using	O
least	B
squares	I
the	O
key	O
idea	O
is	O
that	O
often	O
a	O
small	O
number	O
of	O
principal	B
components	I
suffice	O
to	O
explain	O
most	O
of	O
the	O
variability	O
in	O
the	O
data	B
as	O
well	O
as	O
the	O
relationship	O
with	O
the	O
response	B
in	O
other	O
words	O
we	O
assume	O
that	O
the	O
directions	O
in	O
which	O
xp	O
show	O
the	O
most	O
variation	O
are	O
the	O
directions	O
that	O
are	O
associated	O
with	O
y	O
while	O
this	O
assumption	O
is	O
not	O
guaranteed	O
regression	B
model	B
in	O
a	O
linear	B
that	O
principal	B
components	I
regression	B
linear	B
model	B
selection	B
and	O
regularization	B
n	O
o	O
i	O
t	O
l	O
a	O
u	O
p	O
o	O
p	O
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
principal	O
component	O
principal	O
component	O
figure	O
plots	O
of	O
the	O
second	O
principal	O
component	O
scores	O
versus	O
pop	O
and	O
ad	O
the	O
relationships	O
are	O
weak	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
squared	O
bias	B
test	O
mse	B
variance	B
number	O
of	O
components	O
number	O
of	O
components	O
figure	O
pcr	O
was	O
applied	O
to	O
two	O
simulated	O
data	B
sets	O
left	O
simulated	O
data	B
from	O
figure	O
right	O
simulated	O
data	B
from	O
figure	O
to	O
be	O
true	O
it	O
often	O
turns	O
out	O
to	O
be	O
a	O
reasonable	O
enough	O
approximation	O
to	O
give	O
good	O
results	O
if	O
the	O
assumption	O
underlying	O
pcr	O
holds	O
then	O
fitting	O
a	O
least	B
squares	I
model	B
to	O
zm	O
will	O
lead	O
to	O
better	O
results	O
than	O
fitting	O
a	O
least	B
squares	I
model	B
to	O
xp	O
since	O
most	O
or	O
all	O
of	O
the	O
information	O
in	O
the	O
data	B
that	O
relates	O
to	O
the	O
response	B
is	O
contained	O
in	O
zm	O
and	O
by	O
estimating	O
only	O
m	O
p	O
coefficients	O
we	O
can	O
mitigate	O
overfitting	B
in	O
the	O
advertising	B
data	B
the	O
first	O
principal	O
component	O
explains	O
most	O
of	O
the	O
variance	B
in	O
both	O
pop	O
and	O
ad	O
so	O
a	O
principal	O
component	O
regression	B
that	O
uses	O
this	O
single	B
variable	B
to	O
predict	O
some	O
response	B
of	O
interest	O
such	O
as	O
sales	O
will	O
likely	O
perform	O
quite	O
well	O
figure	O
displays	O
the	O
pcr	O
fits	O
on	O
the	O
simulated	O
data	B
sets	O
from	O
figures	O
and	O
recall	B
that	O
both	O
data	B
sets	O
were	O
generated	O
using	O
n	O
observations	B
and	O
p	O
predictors	O
however	O
while	O
the	O
response	B
in	O
the	O
first	O
data	B
set	B
was	O
a	O
function	B
of	O
all	O
the	O
predictors	O
the	O
response	B
in	O
the	O
second	O
data	B
set	B
was	O
generated	O
using	O
only	O
two	O
of	O
the	O
predictors	O
the	O
curves	O
are	O
plotted	O
as	O
a	O
function	B
of	O
m	O
the	O
number	O
of	O
principal	B
components	I
used	O
as	O
predictors	O
in	O
the	O
regression	B
model	B
as	O
more	O
principal	B
components	I
are	O
used	O
in	O
dimension	B
reduction	I
methods	O
pcr	O
ridge	B
regression	B
and	O
lasso	B
squared	O
bias	B
test	O
mse	B
variance	B
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
number	O
of	O
components	O
shrinkage	B
factor	B
figure	O
pcr	O
ridge	B
regression	B
and	O
the	O
lasso	B
were	O
applied	O
to	O
a	O
simulated	O
data	B
set	B
in	O
which	O
the	O
first	O
five	O
principal	B
components	I
of	O
x	O
contain	O
all	O
the	O
information	O
about	O
the	O
response	B
y	O
in	O
each	O
panel	O
the	O
irreducible	B
error	B
var	O
is	O
shown	O
as	O
a	O
horizontal	O
dashed	O
line	B
left	O
results	O
for	O
pcr	O
right	O
results	O
for	O
lasso	B
and	O
ridge	B
regression	B
the	O
x-axis	O
displays	O
the	O
shrinkage	B
factor	B
of	O
the	O
coefficient	O
estimates	O
defined	O
as	O
the	O
norm	O
of	O
the	O
shrunken	O
coefficient	O
estimates	O
divided	O
by	O
the	O
norm	O
of	O
the	O
least	B
squares	I
estimate	O
the	O
regression	B
model	B
the	O
bias	B
decreases	O
but	O
the	O
variance	B
increases	O
this	O
results	O
in	O
a	O
typical	O
u-shape	O
for	O
the	O
mean	B
squared	I
error	B
when	O
m	O
p	O
then	O
pcr	O
amounts	O
simply	O
to	O
a	O
least	B
squares	I
fit	O
using	O
all	O
of	O
the	O
original	O
predictors	O
the	O
figure	O
indicates	O
that	O
performing	O
pcr	O
with	O
an	O
appropriate	O
choice	O
of	O
m	O
can	O
result	O
in	O
a	O
substantial	O
improvement	O
over	O
least	B
squares	I
especially	O
in	O
the	O
left-hand	O
panel	O
however	O
by	O
examining	O
the	O
ridge	B
regression	B
and	O
lasso	B
results	O
in	O
figures	O
and	O
we	O
see	O
that	O
pcr	O
does	O
not	O
perform	O
as	O
well	O
as	O
the	O
two	O
shrinkage	B
methods	O
in	O
this	O
example	O
the	O
relatively	O
worse	O
performance	O
of	O
pcr	O
in	O
figure	O
is	O
a	O
consequence	O
of	O
the	O
fact	O
that	O
the	O
data	B
were	O
generated	O
in	O
such	O
a	O
way	O
that	O
many	O
principal	B
components	I
are	O
required	O
in	O
order	O
to	O
adequately	O
model	B
the	O
response	B
in	O
contrast	B
pcr	O
will	O
tend	O
to	O
do	O
well	O
in	O
cases	O
when	O
the	O
first	O
few	O
principal	B
components	I
are	O
sufficient	O
to	O
capture	O
most	O
of	O
the	O
variation	O
in	O
the	O
predictors	O
as	O
well	O
as	O
the	O
relationship	O
with	O
the	O
response	B
the	O
left-hand	O
panel	O
of	O
figure	O
illustrates	O
the	O
results	O
from	O
another	O
simulated	O
data	B
set	B
designed	O
to	O
be	O
more	O
favorable	O
to	O
pcr	O
here	O
the	O
response	B
was	O
generated	O
in	O
such	O
a	O
way	O
that	O
it	O
depends	O
exclusively	O
on	O
the	O
first	O
five	O
principal	B
components	I
now	O
the	O
bias	B
drops	O
to	O
zero	O
rapidly	O
as	O
m	O
the	O
number	O
of	O
principal	B
components	I
used	O
in	O
pcr	O
increases	O
the	O
mean	B
squared	I
error	B
displays	O
a	O
clear	O
minimum	O
at	O
m	O
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
the	O
results	O
on	O
these	O
data	B
using	O
ridge	B
regression	B
and	O
the	O
lasso	B
all	O
three	O
methods	O
offer	O
a	O
significant	O
improvement	O
over	O
least	B
squares	I
however	O
pcr	O
and	O
ridge	B
regression	B
slightly	O
outperform	O
the	O
lasso	B
we	O
note	O
that	O
even	O
though	O
pcr	O
provides	O
a	O
simple	B
way	O
to	O
perform	O
regression	B
using	O
m	O
p	O
predictors	O
it	O
is	O
not	O
a	O
feature	B
selection	B
method	O
this	O
is	O
because	O
each	O
of	O
the	O
m	O
principal	B
components	I
used	O
in	O
the	O
regression	B
linear	B
model	B
selection	B
and	O
regularization	B
s	O
t	O
i	O
n	O
e	O
c	O
i	O
f	O
f	O
i	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
s	O
t	O
income	B
limit	O
rating	O
student	O
e	O
s	O
m	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
s	O
s	O
o	O
r	O
c	O
number	O
of	O
components	O
number	O
of	O
components	O
figure	O
left	O
pcr	O
standardized	O
coefficient	O
estimates	O
on	O
the	O
credit	B
data	B
set	B
for	O
different	O
values	O
of	O
m	O
right	O
the	O
ten-fold	O
cross	O
validation	O
mse	B
obtained	O
using	O
pcr	O
as	O
a	O
function	B
of	O
m	O
is	O
a	O
linear	B
combination	I
of	O
all	O
p	O
of	O
the	O
original	O
features	O
for	O
instance	O
in	O
was	O
a	O
linear	B
combination	I
of	O
both	O
pop	O
and	O
ad	O
therefore	O
while	O
pcr	O
often	O
performs	O
quite	O
well	O
in	O
many	O
practical	O
settings	O
it	O
does	O
not	O
result	O
in	O
the	O
development	O
of	O
a	O
model	B
that	O
relies	O
upon	O
a	O
small	O
set	B
of	O
the	O
original	O
features	O
in	O
this	O
sense	O
pcr	O
is	O
more	O
closely	O
related	O
to	O
ridge	B
regression	B
than	O
to	O
the	O
lasso	B
in	O
fact	O
one	O
can	O
show	O
that	O
pcr	O
and	O
ridge	B
regression	B
are	O
very	O
closely	O
related	O
one	O
can	O
even	O
think	O
of	O
ridge	B
regression	B
as	O
a	O
continuous	B
sion	O
of	O
pcr	O
in	O
pcr	O
the	O
number	O
of	O
principal	B
components	I
m	O
is	O
typically	O
chosen	O
by	O
cross-validation	B
the	O
results	O
of	O
applying	O
pcr	O
to	O
the	O
credit	B
data	B
set	B
are	O
shown	O
in	O
figure	O
the	O
right-hand	O
panel	O
displays	O
the	O
cross-validation	B
errors	O
obtained	O
as	O
a	O
function	B
of	O
m	O
on	O
these	O
data	B
the	O
lowest	O
crossvalidation	O
error	B
occurs	O
when	O
there	O
are	O
m	O
components	O
this	O
corresponds	O
to	O
almost	O
no	O
dimension	B
reduction	I
at	O
all	O
since	O
pcr	O
with	O
m	O
is	O
equivalent	O
to	O
simply	O
performing	O
least	B
squares	I
when	O
performing	O
pcr	O
we	O
generally	O
recommend	O
standardizing	O
each	O
predictor	B
using	O
prior	O
to	O
generating	O
the	O
principal	B
components	I
this	O
standardization	O
ensures	O
that	O
all	O
variables	O
are	O
on	O
the	O
same	O
scale	O
in	O
the	O
absence	O
of	O
standardization	O
the	O
high-variance	O
variables	O
will	O
tend	O
to	O
play	O
a	O
larger	O
role	O
in	O
the	O
principal	B
components	I
obtained	O
and	O
the	O
scale	O
on	O
which	O
the	O
variables	O
are	O
measured	O
will	O
ultimately	O
have	O
an	O
effect	O
on	O
the	O
final	O
pcr	O
model	B
however	O
if	O
the	O
variables	O
are	O
all	O
measured	O
in	O
the	O
same	O
units	O
kilograms	O
or	O
inches	O
then	O
one	O
might	O
choose	O
not	O
to	O
standardize	B
them	O
more	O
details	O
can	O
be	O
found	O
in	O
section	O
of	O
elements	O
of	O
statistical	O
learning	O
by	O
hastie	O
tibshirani	O
and	O
friedman	O
dimension	B
reduction	I
methods	O
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
population	O
figure	O
for	O
the	O
advertising	B
data	B
the	O
first	O
pls	O
direction	O
line	B
and	O
first	O
pcr	O
direction	O
line	B
are	O
shown	O
partial	B
least	B
squares	I
the	O
pcr	O
approach	B
that	O
we	O
just	O
described	O
involves	O
identifying	O
linear	B
combinations	O
or	O
directions	O
that	O
best	O
represent	O
the	O
predictors	O
xp	O
these	O
directions	O
are	O
identified	O
in	O
an	O
unsupervised	O
way	O
since	O
the	O
response	B
y	O
is	O
not	O
used	O
to	O
help	O
determine	O
the	O
principal	O
component	O
directions	O
that	O
is	O
the	O
response	B
does	O
not	O
supervise	O
the	O
identification	O
of	O
the	O
principal	B
components	I
consequently	O
pcr	O
suffers	O
from	O
a	O
drawback	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
directions	O
that	O
best	O
explain	O
the	O
predictors	O
will	O
also	O
be	O
the	O
best	O
directions	O
to	O
use	O
for	O
predicting	O
the	O
response	B
unsupervised	O
methods	O
are	O
discussed	O
further	O
in	O
chapter	O
we	O
now	O
present	O
partial	B
least	B
squares	I
a	O
supervised	O
alternative	O
to	O
pcr	O
like	O
pcr	O
pls	O
is	O
a	O
dimension	B
reduction	I
method	O
which	O
first	O
identifies	O
a	O
new	O
set	B
of	O
features	O
zm	O
that	O
are	O
linear	B
combinations	O
of	O
the	O
original	O
features	O
and	O
then	O
fits	O
a	O
linear	B
model	B
via	O
least	B
squares	I
using	O
these	O
m	O
new	O
features	O
but	O
unlike	O
pcr	O
pls	O
identifies	O
these	O
new	O
features	O
in	O
a	O
supervised	O
way	O
that	O
is	O
it	O
makes	O
use	O
of	O
the	O
response	B
y	O
in	O
order	O
to	O
identify	O
new	O
features	O
that	O
not	O
only	O
approximate	O
the	O
old	O
features	O
well	O
but	O
also	O
that	O
are	O
related	O
to	O
the	O
response	B
roughly	O
speaking	O
the	O
pls	O
approach	B
attempts	O
to	O
find	O
directions	O
that	O
help	O
explain	O
both	O
the	O
response	B
and	O
the	O
predictors	O
we	O
now	O
describe	O
how	O
the	O
first	O
pls	O
direction	O
is	O
computed	O
after	O
standardizing	O
the	O
p	O
predictors	O
pls	O
computes	O
the	O
first	O
direction	O
by	O
setting	O
each	O
in	O
equal	O
to	O
the	O
coefficient	O
from	O
the	O
simple	B
linear	B
regression	B
of	O
y	O
onto	O
xj	O
one	O
can	O
show	O
that	O
this	O
coefficient	O
is	O
proportional	O
to	O
the	O
corp	O
relation	O
between	O
y	O
and	O
xj	O
hence	O
in	O
computing	O
pls	O
places	O
the	O
highest	O
weight	O
on	O
the	O
variables	O
that	O
are	O
most	O
strongly	O
related	O
to	O
the	O
response	B
partial	B
least	B
squares	I
figure	O
displays	O
an	O
example	O
of	O
pls	O
on	O
set	B
with	O
sales	O
in	O
each	O
of	O
regions	O
as	O
the	O
response	B
and	O
two	O
predictors	O
population	O
size	O
and	O
advertising	B
spending	O
the	O
solid	O
green	O
line	B
indicates	O
the	O
first	O
pls	O
direction	O
while	O
the	O
dotted	O
line	B
shows	O
the	O
first	O
principal	O
component	O
direction	O
ad	O
dimension	O
per	O
unit	O
pls	O
has	O
chosen	O
a	O
direction	O
that	O
has	O
less	O
change	O
in	O
the	O
a	O
synthetic	O
data	B
dataset	O
is	O
distinct	O
from	O
the	O
advertising	B
data	B
discussed	O
in	O
chapter	O
linear	B
model	B
selection	B
and	O
regularization	B
pop	O
is	O
ad	O
the	O
pls	O
direction	O
not	O
fit	O
the	O
predictors	O
as	O
closely	O
as	O
does	O
pca	O
but	O
it	O
does	O
a	O
better	O
job	O
change	O
in	O
the	O
pop	O
dimension	O
relative	O
to	O
pca	O
this	O
suggests	O
that	O
more	O
highly	O
correlated	O
with	O
the	O
response	B
than	O
is	O
does	O
explaining	O
the	O
response	B
to	O
identify	O
the	O
second	O
pls	O
direction	O
we	O
first	O
adjust	O
each	O
of	O
the	O
variables	O
for	O
by	O
regressing	O
each	O
variable	B
on	O
and	O
taking	O
residuals	B
these	O
residuals	B
can	O
be	O
interpreted	O
as	O
the	O
remaining	O
information	O
that	O
has	O
not	O
been	O
explained	B
by	O
the	O
first	O
pls	O
direction	O
we	O
then	O
compute	O
using	O
this	O
orthogonalized	O
data	B
in	O
exactly	O
the	O
same	O
fashion	O
as	O
was	O
computed	O
based	O
on	O
the	O
original	O
data	B
this	O
iterative	O
approach	B
can	O
be	O
repeated	O
m	O
times	O
to	O
identify	O
multiple	B
pls	O
components	O
zm	O
finally	O
at	O
the	O
end	O
of	O
this	O
procedure	O
we	O
use	O
least	B
squares	I
to	O
fit	O
a	O
linear	B
model	B
to	O
predict	O
y	O
using	O
zm	O
in	O
exactly	O
the	O
same	O
fashion	O
as	O
for	O
pcr	O
as	O
with	O
pcr	O
the	O
number	O
m	O
of	O
partial	B
least	B
squares	I
directions	O
used	O
in	O
pls	O
is	O
a	O
tuning	B
parameter	B
that	O
is	O
typically	O
chosen	O
by	O
cross-validation	B
we	O
generally	O
standardize	B
the	O
predictors	O
and	O
response	B
before	O
performing	O
pls	O
pls	O
is	O
popular	O
in	O
the	O
field	O
of	O
chemometrics	O
where	O
many	O
variables	O
arise	O
from	O
digitized	O
spectrometry	O
signals	O
in	O
practice	O
it	O
often	O
performs	O
no	O
better	O
than	O
ridge	B
regression	B
or	O
pcr	O
while	O
the	O
supervised	O
dimension	B
reduction	I
of	O
pls	O
can	O
reduce	O
bias	B
it	O
also	O
has	O
the	O
potential	O
to	O
increase	O
variance	B
so	O
that	O
the	O
overall	O
benefit	O
of	O
pls	O
relative	O
to	O
pcr	O
is	O
a	O
wash	O
considerations	O
in	O
high	O
dimensions	O
high-dimensional	B
data	B
most	O
traditional	O
statistical	O
techniques	O
for	O
regression	B
and	O
classification	B
are	O
intended	O
for	O
the	O
low-dimensional	B
setting	O
in	O
which	O
n	O
the	O
number	O
of	O
observations	B
is	O
much	O
greater	O
than	O
p	O
the	O
number	O
of	O
features	O
this	O
is	O
due	O
in	O
part	O
to	O
the	O
fact	O
that	O
throughout	O
most	O
of	O
the	O
field	O
s	O
history	O
the	O
bulk	O
of	O
scientific	O
problems	O
requiring	O
the	O
use	O
of	O
statistics	O
have	O
been	O
low-dimensional	B
for	O
instance	O
consider	O
the	O
task	O
of	O
developing	O
a	O
model	B
to	O
predict	O
a	O
patient	O
s	O
blood	O
pressure	O
on	O
the	O
basis	B
of	O
his	O
or	O
her	O
age	O
gender	O
and	O
body	O
mass	O
index	O
there	O
are	O
three	O
predictors	O
or	O
four	O
if	O
an	O
intercept	B
is	O
included	O
in	O
the	O
model	B
and	O
perhaps	O
several	O
thousand	O
patients	O
for	O
whom	O
blood	O
pressure	O
and	O
age	O
gender	O
and	O
bmi	O
are	O
available	O
hence	O
n	O
p	O
and	O
so	O
the	O
problem	O
is	O
low-dimensional	B
dimension	O
here	O
we	O
are	O
referring	O
to	O
the	O
size	O
of	O
p	O
in	O
the	O
past	O
years	O
new	O
technologies	O
have	O
changed	O
the	O
way	O
that	O
data	B
are	O
collected	O
in	O
fields	O
as	O
diverse	O
as	O
finance	O
marketing	O
and	O
medicine	O
it	O
is	O
now	O
commonplace	O
to	O
collect	O
an	O
almost	O
unlimited	O
number	O
of	O
feature	B
measurements	O
very	O
large	O
while	O
p	O
can	O
be	O
extremely	O
large	O
the	O
number	O
of	O
observations	B
n	O
is	O
often	O
limited	O
due	O
to	O
cost	O
sample	O
availability	O
or	O
other	O
considerations	O
two	O
examples	O
are	O
as	O
follows	O
rather	O
than	O
predicting	O
blood	O
pressure	O
on	O
the	O
basis	B
of	O
just	O
age	O
gender	O
and	O
bmi	O
one	O
might	O
also	O
collect	O
measurements	O
for	O
half	O
a	O
million	O
lowdimensional	O
highdimensional	O
considerations	O
in	O
high	O
dimensions	O
single	B
nucleotide	O
polymorphisms	O
these	O
are	O
individual	O
dna	O
mutations	O
that	O
are	O
relatively	O
common	O
in	O
the	O
population	O
for	O
inclusion	O
in	O
the	O
predictive	O
model	B
then	O
n	O
and	O
p	O
a	O
marketing	O
analyst	O
interested	O
in	O
understanding	O
people	O
s	O
online	O
shopping	O
patterns	O
could	O
treat	O
as	O
features	O
all	O
of	O
the	O
search	O
terms	O
entered	O
by	O
users	O
of	O
a	O
search	O
engine	O
this	O
is	O
sometimes	O
known	O
as	O
the	O
bag-ofwords	O
model	B
the	O
same	O
researcher	O
might	O
have	O
access	O
to	O
the	O
search	O
histories	O
of	O
only	O
a	O
few	O
hundred	O
or	O
a	O
few	O
thousand	O
search	O
engine	O
users	O
who	O
have	O
consented	O
to	O
share	O
their	O
information	O
with	O
the	O
researcher	O
for	O
a	O
given	O
user	O
each	O
of	O
the	O
p	O
search	O
terms	O
is	O
scored	O
present	O
or	O
absent	O
creating	O
a	O
large	O
binary	B
feature	B
vector	B
then	O
n	O
and	O
p	O
is	O
much	O
larger	O
data	B
sets	O
containing	O
more	O
features	O
than	O
observations	B
are	O
often	O
referred	O
to	O
as	O
high-dimensional	B
classical	O
approaches	O
such	O
as	O
least	B
squares	I
linear	B
regression	B
are	O
not	O
appropriate	O
in	O
this	O
setting	O
many	O
of	O
the	O
issues	O
that	O
arise	O
in	O
the	O
analysis	B
of	O
high-dimensional	B
data	B
were	O
discussed	O
earlier	O
in	O
this	O
book	O
since	O
they	O
apply	O
also	O
when	O
n	O
p	O
these	O
include	O
the	O
role	O
of	O
the	O
bias-variance	O
trade-off	B
and	O
the	O
danger	O
of	O
overfitting	B
though	O
these	O
issues	O
are	O
always	O
relevant	O
they	O
can	O
become	O
particularly	O
important	O
when	O
the	O
number	O
of	O
features	O
is	O
very	O
large	O
relative	O
to	O
the	O
number	O
of	O
observations	B
we	O
have	O
defined	O
the	O
high-dimensional	B
setting	O
as	O
the	O
case	O
where	O
the	O
number	O
of	O
features	O
p	O
is	O
larger	O
than	O
the	O
number	O
of	O
observations	B
n	O
but	O
the	O
considerations	O
that	O
we	O
will	O
now	O
discuss	O
certainly	O
also	O
apply	O
if	O
p	O
is	O
slightly	O
smaller	O
than	O
n	O
and	O
are	O
best	O
always	O
kept	O
in	O
mind	O
when	O
performing	O
supervised	B
learning	I
what	O
goes	O
wrong	O
in	O
high	O
dimensions	O
in	O
order	O
to	O
illustrate	O
the	O
need	O
for	O
extra	O
care	O
and	O
specialized	O
techniques	O
for	O
regression	B
and	O
classification	B
when	O
p	O
n	O
we	O
begin	O
by	O
examining	O
what	O
can	O
go	O
wrong	O
if	O
we	O
apply	O
a	O
statistical	O
technique	O
not	O
intended	O
for	O
the	O
highdimensional	O
setting	O
for	O
this	O
purpose	O
we	O
examine	O
least	B
squares	I
regression	B
but	O
the	O
same	O
concepts	O
apply	O
to	O
logistic	B
regression	B
linear	B
discriminant	I
analysis	B
and	O
other	O
classical	O
statistical	O
approaches	O
when	O
the	O
number	O
of	O
features	O
p	O
is	O
as	O
large	O
as	O
or	O
larger	O
than	O
the	O
number	O
of	O
observations	B
n	O
least	B
squares	I
as	O
described	O
in	O
chapter	O
cannot	O
rather	O
should	O
not	O
be	O
performed	O
the	O
reason	O
is	O
simple	B
regardless	O
of	O
whether	O
or	O
not	O
there	O
truly	O
is	O
a	O
relationship	O
between	O
the	O
features	O
and	O
the	O
response	B
least	B
squares	I
will	O
yield	O
a	O
set	B
of	O
coefficient	O
estimates	O
that	O
result	O
in	O
a	O
perfect	O
fit	O
to	O
the	O
data	B
such	O
that	O
the	O
residuals	B
are	O
zero	O
an	O
example	O
is	O
shown	O
in	O
figure	O
with	O
p	O
feature	B
an	O
intercept	B
in	O
two	O
cases	O
when	O
there	O
are	O
observations	B
and	O
when	O
there	O
are	O
only	O
two	O
observations	B
when	O
there	O
are	O
observations	B
n	O
p	O
and	O
the	O
least	O
linear	B
model	B
selection	B
and	O
regularization	B
y	O
y	O
x	O
x	O
figure	O
left	O
least	B
squares	I
regression	B
in	O
the	O
low-dimensional	B
setting	O
right	O
least	B
squares	I
regression	B
with	O
n	O
observations	B
and	O
two	O
parameters	O
to	O
be	O
estimated	O
intercept	B
and	O
a	O
coefficient	O
squares	O
regression	B
line	B
does	O
not	O
perfectly	O
fit	O
the	O
data	B
instead	O
the	O
regression	B
line	B
seeks	O
to	O
approximate	O
the	O
observations	B
as	O
well	O
as	O
possible	O
on	O
the	O
other	O
hand	O
when	O
there	O
are	O
only	O
two	O
observations	B
then	O
regardless	O
of	O
the	O
values	O
of	O
those	O
observations	B
the	O
regression	B
line	B
will	O
fit	O
the	O
data	B
exactly	O
this	O
is	O
problematic	O
because	O
this	O
perfect	O
fit	O
will	O
almost	O
certainly	O
lead	O
to	O
overfitting	B
of	O
the	O
data	B
in	O
other	O
words	O
though	O
it	O
is	O
possible	O
to	O
perfectly	O
fit	O
the	O
training	O
data	B
in	O
the	O
high-dimensional	B
setting	O
the	O
resulting	O
linear	B
model	B
will	O
perform	O
extremely	O
poorly	O
on	O
an	O
independent	B
test	O
set	B
and	O
therefore	O
does	O
not	O
constitute	O
a	O
useful	O
model	B
in	O
fact	O
we	O
can	O
see	O
that	O
this	O
happened	O
in	O
figure	O
the	O
least	B
squares	I
line	B
obtained	O
in	O
the	O
right-hand	O
panel	O
will	O
perform	O
very	O
poorly	O
on	O
a	O
test	O
set	B
comprised	O
of	O
the	O
observations	B
in	O
the	O
lefthand	O
panel	O
the	O
problem	O
is	O
simple	B
when	O
p	O
n	O
or	O
p	O
n	O
a	O
simple	B
least	B
squares	I
regression	B
line	B
is	O
too	O
flexible	O
and	O
hence	O
overfits	O
the	O
data	B
figure	O
further	O
illustrates	O
the	O
risk	O
of	O
carelessly	O
applying	O
least	B
squares	I
when	O
the	O
number	O
of	O
features	O
p	O
is	O
large	O
data	B
were	O
simulated	O
with	O
n	O
observations	B
and	O
regression	B
was	O
performed	O
with	O
between	O
and	O
features	O
each	O
of	O
which	O
was	O
completely	O
unrelated	O
to	O
the	O
response	B
as	O
shown	O
in	O
the	O
figure	O
the	O
model	B
increases	O
to	O
as	O
the	O
number	O
of	O
features	O
included	O
in	O
the	O
model	B
increases	O
and	O
correspondingly	O
the	O
training	O
set	B
mse	B
decreases	O
to	O
as	O
the	O
number	O
of	O
features	O
increases	O
even	O
though	O
the	O
features	O
are	O
completely	O
unrelated	O
to	O
the	O
response	B
on	O
the	O
other	O
hand	O
the	O
mse	B
on	O
an	O
independent	B
test	O
set	B
becomes	O
extremely	O
large	O
as	O
the	O
number	O
of	O
features	O
included	O
in	O
the	O
model	B
increases	O
because	O
including	O
the	O
additional	O
predictors	O
leads	O
to	O
a	O
vast	O
increase	O
in	O
the	O
variance	B
of	O
the	O
coefficient	O
estimates	O
looking	O
at	O
the	O
test	O
set	B
mse	B
it	O
is	O
clear	O
that	O
the	O
best	O
model	B
contains	O
at	O
most	O
a	O
few	O
variables	O
however	O
someone	O
who	O
carelessly	O
examines	O
only	O
the	O
or	O
the	O
training	O
set	B
mse	B
might	O
erroneously	O
conclude	O
that	O
the	O
model	B
with	O
the	O
greatest	O
number	O
of	O
variables	O
is	O
best	O
this	O
indicates	O
the	O
importance	B
of	O
applying	O
extra	O
care	O
r	O
considerations	O
in	O
high	O
dimensions	O
e	O
s	O
m	O
t	O
s	O
e	O
t	O
e	O
s	O
m	O
g	O
n	O
n	O
a	O
r	O
t	O
i	O
i	O
number	O
of	O
variables	O
number	O
of	O
variables	O
number	O
of	O
variables	O
figure	O
on	O
a	O
simulated	O
example	O
with	O
n	O
training	O
observations	B
features	O
that	O
are	O
completely	O
unrelated	O
to	O
the	O
outcome	O
are	O
added	O
to	O
the	O
model	B
left	O
the	O
increases	O
to	O
as	O
more	O
features	O
are	O
included	O
center	O
the	O
training	O
set	B
mse	B
decreases	O
to	O
as	O
more	O
features	O
are	O
included	O
right	O
the	O
test	O
set	B
mse	B
increases	O
as	O
more	O
features	O
are	O
included	O
when	O
analyzing	O
data	B
sets	O
with	O
a	O
large	O
number	O
of	O
variables	O
and	O
of	O
always	O
evaluating	O
model	B
performance	O
on	O
an	O
independent	B
test	O
set	B
in	O
section	O
we	O
saw	O
a	O
number	O
of	O
approaches	O
for	O
adjusting	O
the	O
training	O
set	B
rss	O
or	O
in	O
order	O
to	O
account	O
for	O
the	O
number	O
of	O
variables	O
used	O
to	O
fit	O
a	O
least	B
squares	I
model	B
unfortunately	O
the	O
cp	B
aic	O
and	O
bic	O
approaches	O
are	O
not	O
appropriate	O
in	O
the	O
high-dimensional	B
setting	O
because	O
estimating	O
is	O
problematic	O
instance	O
the	O
formula	O
for	O
from	O
chapter	O
yields	O
an	O
estimate	O
in	O
this	O
setting	O
similarly	O
problems	O
arise	O
in	O
the	O
application	O
of	O
adjusted	O
in	O
the	O
high-dimensional	B
setting	O
since	O
one	O
can	O
easily	O
obtain	O
a	O
model	B
with	O
an	O
adjusted	O
value	O
of	O
clearly	O
alternative	O
approaches	O
that	O
are	O
better-suited	O
to	O
the	O
high-dimensional	B
setting	O
are	O
required	O
regression	B
in	O
high	O
dimensions	O
it	O
turns	O
out	O
that	O
many	O
of	O
the	O
methods	O
seen	O
in	O
this	O
chapter	O
for	O
fitting	O
less	O
flexible	O
least	B
squares	I
models	O
such	O
as	O
forward	B
stepwise	I
selection	B
ridge	B
regression	B
the	O
lasso	B
and	O
principal	B
components	I
regression	B
are	O
particularly	O
useful	O
for	O
performing	O
regression	B
in	O
the	O
high-dimensional	B
setting	O
essentially	O
these	O
approaches	O
avoid	O
overfitting	B
by	O
using	O
a	O
less	O
flexible	O
fitting	O
approach	B
than	O
least	B
squares	I
figure	O
illustrates	O
the	O
performance	O
of	O
the	O
lasso	B
in	O
a	O
simple	B
simulated	O
example	O
there	O
are	O
p	O
or	O
features	O
of	O
which	O
are	O
truly	O
associated	O
with	O
the	O
outcome	O
the	O
lasso	B
was	O
performed	O
on	O
n	O
training	O
observations	B
and	O
the	O
mean	B
squared	I
error	B
was	O
evaluated	O
on	O
an	O
independent	B
test	O
set	B
as	O
the	O
number	O
of	O
features	O
increases	O
the	O
test	O
set	B
error	B
increases	O
when	O
p	O
the	O
lowest	O
validation	B
set	B
error	B
was	O
achieved	O
when	O
in	O
was	O
small	O
however	O
when	O
p	O
was	O
larger	O
then	O
the	O
lowest	O
validation	B
set	B
error	B
was	O
achieved	O
using	O
a	O
larger	O
value	O
of	O
in	O
each	O
boxplot	B
rather	O
than	O
reporting	O
the	O
values	O
of	O
used	O
the	O
degrees	B
of	I
freedom	I
of	O
the	O
resulting	O
linear	B
model	B
selection	B
and	O
regularization	B
p	O
p	O
p	O
degrees	B
of	I
freedom	I
degrees	B
of	I
freedom	I
degrees	B
of	I
freedom	I
figure	O
the	O
lasso	B
was	O
performed	O
with	O
n	O
observations	B
and	O
three	O
values	O
of	O
p	O
the	O
number	O
of	O
features	O
of	O
the	O
p	O
features	O
were	O
associated	O
with	O
the	O
response	B
the	O
boxplots	O
show	O
the	O
test	O
mses	O
that	O
result	O
using	O
three	O
different	O
values	O
of	O
the	O
tuning	B
parameter	B
in	O
for	O
ease	O
of	O
interpretation	O
rather	O
than	O
reporting	O
the	O
degrees	B
of	I
freedom	I
are	O
reported	O
for	O
the	O
lasso	B
this	O
turns	O
out	O
to	O
be	O
simply	O
the	O
number	O
of	O
estimated	O
non-zero	O
coefficients	O
when	O
p	O
the	O
lowest	O
test	O
mse	B
was	O
obtained	O
with	O
the	O
smallest	O
amount	O
of	O
regularization	B
when	O
p	O
the	O
lowest	O
test	O
mse	B
was	O
achieved	O
when	O
there	O
is	O
a	O
substantial	O
amount	O
of	O
regularization	B
when	O
p	O
the	O
lasso	B
performed	O
poorly	O
regardless	O
of	O
the	O
amount	O
of	O
regularization	B
due	O
to	O
the	O
fact	O
that	O
only	O
of	O
the	O
features	O
truly	O
are	O
associated	O
with	O
the	O
outcome	O
lasso	B
solution	O
is	O
displayed	O
this	O
is	O
simply	O
the	O
number	O
of	O
non-zero	O
coefficient	O
estimates	O
in	O
the	O
lasso	B
solution	O
and	O
is	O
a	O
measure	O
of	O
the	O
flexibility	O
of	O
the	O
lasso	B
fit	O
figure	O
highlights	O
three	O
important	O
points	O
regularization	B
or	O
shrinkage	B
plays	O
a	O
key	O
role	O
in	O
high-dimensional	B
problems	O
appropriate	O
tuning	B
parameter	B
selection	B
is	O
crucial	O
for	O
good	O
predictive	O
performance	O
and	O
the	O
test	O
error	B
tends	O
to	O
increase	O
as	O
the	O
dimensionality	O
of	O
the	O
problem	O
the	O
number	O
of	O
features	O
or	O
predictors	O
increases	O
unless	O
the	O
additional	O
features	O
are	O
truly	O
associated	O
with	O
the	O
response	B
the	O
third	O
point	O
above	O
is	O
in	O
fact	O
a	O
key	O
principle	O
in	O
the	O
analysis	B
of	O
highdimensional	O
data	B
which	O
is	O
known	O
as	O
the	O
curse	B
of	I
dimensionality	I
one	O
might	O
think	O
that	O
as	O
the	O
number	O
of	O
features	O
used	O
to	O
fit	O
a	O
model	B
increases	O
the	O
quality	O
of	O
the	O
fitted	O
model	B
will	O
increase	O
as	O
well	O
however	O
comparing	O
the	O
left-hand	O
and	O
right-hand	O
panels	O
in	O
figure	O
we	O
see	O
that	O
this	O
is	O
not	O
necessarily	O
the	O
case	O
in	O
this	O
example	O
the	O
test	O
set	B
mse	B
almost	O
doubles	O
as	O
p	O
increases	O
from	O
to	O
in	O
general	O
adding	O
additional	O
signal	B
features	O
that	O
are	O
truly	O
associated	O
with	O
the	O
response	B
will	O
improve	O
the	O
fitted	O
model	B
in	O
the	O
sense	O
of	O
leading	O
to	O
a	O
reduction	O
in	O
test	O
set	B
error	B
however	O
adding	O
noise	B
features	O
that	O
are	O
not	O
truly	O
associated	O
with	O
the	O
response	B
will	O
lead	O
to	O
a	O
deterioration	O
in	O
the	O
fitted	O
model	B
and	O
consequently	O
an	O
increased	O
test	O
set	B
error	B
this	O
is	O
because	O
noise	B
features	O
increase	O
the	O
dimensionality	O
of	O
the	O
curse	B
of	I
dimensionality	I
considerations	O
in	O
high	O
dimensions	O
problem	O
exacerbating	O
the	O
risk	O
of	O
overfitting	B
noise	B
features	O
may	O
be	O
assigned	O
nonzero	O
coefficients	O
due	O
to	O
chance	O
associations	O
with	O
the	O
response	B
on	O
the	O
training	O
set	B
without	O
any	O
potential	O
upside	O
in	O
terms	O
of	O
improved	O
test	O
set	B
error	B
thus	O
we	O
see	O
that	O
new	O
technologies	O
that	O
allow	O
for	O
the	O
collection	O
of	O
measurements	O
for	O
thousands	O
or	O
millions	O
of	O
features	O
are	O
a	O
double-edged	O
sword	O
they	O
can	O
lead	O
to	O
improved	O
predictive	O
models	O
if	O
these	O
features	O
are	O
in	O
fact	O
relevant	O
to	O
the	O
problem	O
at	O
hand	O
but	O
will	O
lead	O
to	O
worse	O
results	O
if	O
the	O
features	O
are	O
not	O
relevant	O
even	O
if	O
they	O
are	O
relevant	O
the	O
variance	B
incurred	O
in	O
fitting	O
their	O
coefficients	O
may	O
outweigh	O
the	O
reduction	O
in	O
bias	B
that	O
they	O
bring	O
interpreting	O
results	O
in	O
high	O
dimensions	O
when	O
we	O
perform	O
the	O
lasso	B
ridge	B
regression	B
or	O
other	O
regression	B
procedures	O
in	O
the	O
high-dimensional	B
setting	O
we	O
must	O
be	O
quite	O
cautious	O
in	O
the	O
way	O
that	O
we	O
report	O
the	O
results	O
obtained	O
in	O
chapter	O
we	O
learned	O
about	O
multicollinearity	B
the	O
concept	O
that	O
the	O
variables	O
in	O
a	O
regression	B
might	O
be	O
correlated	O
with	O
each	O
other	O
in	O
the	O
high-dimensional	B
setting	O
the	O
multicollinearity	B
problem	O
is	O
extreme	O
any	O
variable	B
in	O
the	O
model	B
can	O
be	O
written	O
as	O
a	O
linear	B
combination	I
of	O
all	O
of	O
the	O
other	O
variables	O
in	O
the	O
model	B
essentially	O
this	O
means	O
that	O
we	O
can	O
never	O
know	O
exactly	O
which	O
variables	O
any	O
truly	O
are	O
predictive	O
of	O
the	O
outcome	O
and	O
we	O
can	O
never	O
identify	O
the	O
best	O
coefficients	O
for	O
use	O
in	O
the	O
regression	B
at	O
most	O
we	O
can	O
hope	O
to	O
assign	O
large	O
regression	B
coefficients	O
to	O
variables	O
that	O
are	O
correlated	O
with	O
the	O
variables	O
that	O
truly	O
are	O
predictive	O
of	O
the	O
outcome	O
for	O
instance	O
suppose	O
that	O
we	O
are	O
trying	O
to	O
predict	O
blood	O
pressure	O
on	O
the	O
basis	B
of	O
half	O
a	O
million	O
snps	O
and	O
that	O
forward	B
stepwise	I
selection	B
indicates	O
that	O
of	O
those	O
snps	O
lead	O
to	O
a	O
good	O
predictive	O
model	B
on	O
the	O
training	O
data	B
it	O
would	O
be	O
incorrect	O
to	O
conclude	O
that	O
these	O
snps	O
predict	O
blood	O
pressure	O
more	O
effectively	O
than	O
the	O
other	O
snps	O
not	O
included	O
in	O
the	O
model	B
there	O
are	O
likely	O
to	O
be	O
many	O
sets	O
of	O
snps	O
that	O
would	O
predict	O
blood	O
pressure	O
just	O
as	O
well	O
as	O
the	O
selected	O
model	B
if	O
we	O
were	O
to	O
obtain	O
an	O
independent	B
data	B
set	B
and	O
perform	O
forward	B
stepwise	I
selection	B
on	O
that	O
data	B
set	B
we	O
would	O
likely	O
obtain	O
a	O
model	B
containing	O
a	O
different	O
and	O
perhaps	O
even	O
non-overlapping	O
set	B
of	O
snps	O
this	O
does	O
not	O
detract	O
from	O
the	O
value	O
of	O
the	O
model	B
obtained	O
for	O
instance	O
the	O
model	B
might	O
turn	O
out	O
to	O
be	O
very	O
effective	O
in	O
predicting	O
blood	O
pressure	O
on	O
an	O
independent	B
set	B
of	O
patients	O
and	O
might	O
be	O
clinically	O
useful	O
for	O
physicians	O
but	O
we	O
must	O
be	O
careful	O
not	O
to	O
overstate	O
the	O
results	O
obtained	O
and	O
to	O
make	O
it	O
clear	O
that	O
what	O
we	O
have	O
identified	O
is	O
simply	O
one	O
of	O
many	O
possible	O
models	O
for	O
predicting	O
blood	O
pressure	O
and	O
that	O
it	O
must	O
be	O
further	O
validated	O
on	O
independent	B
data	B
sets	O
it	O
is	O
also	O
important	O
to	O
be	O
particularly	O
careful	O
in	O
reporting	O
errors	O
and	O
measures	O
of	O
model	B
fit	O
in	O
the	O
high-dimensional	B
setting	O
we	O
have	O
seen	O
that	O
when	O
p	O
n	O
it	O
is	O
easy	O
to	O
obtain	O
a	O
useless	O
model	B
that	O
has	O
zero	O
residuals	B
therefore	O
one	O
should	O
never	O
use	O
sum	O
of	O
squared	O
errors	O
p-values	O
linear	B
model	B
selection	B
and	O
regularization	B
statistics	O
or	O
other	O
traditional	O
measures	O
of	O
model	B
fit	O
on	O
the	O
training	O
data	B
as	O
evidence	O
of	O
a	O
good	O
model	B
fit	O
in	O
the	O
high-dimensional	B
setting	O
for	O
instance	O
as	O
we	O
saw	O
in	O
figure	O
one	O
can	O
easily	O
obtain	O
a	O
model	B
with	O
when	O
p	O
n	O
reporting	O
this	O
fact	O
might	O
mislead	O
others	O
into	O
thinking	O
that	O
a	O
statistically	O
valid	O
and	O
useful	O
model	B
has	O
been	O
obtained	O
whereas	O
in	O
fact	O
this	O
provides	O
absolutely	O
no	O
evidence	O
of	O
a	O
compelling	O
model	B
it	O
is	O
important	O
to	O
instead	O
report	O
results	O
on	O
an	O
independent	B
test	O
set	B
or	O
cross-validation	B
errors	O
for	O
instance	O
the	O
mse	B
or	O
on	O
an	O
independent	B
test	O
set	B
is	O
a	O
valid	O
measure	O
of	O
model	B
fit	O
but	O
the	O
mse	B
on	O
the	O
training	O
set	B
certainly	O
is	O
not	O
lab	O
subset	B
selection	B
methods	O
best	B
subset	B
selection	B
here	O
we	O
apply	O
the	O
best	B
subset	B
selection	B
approach	B
to	O
the	O
hitters	B
data	B
we	O
wish	O
to	O
predict	O
a	O
baseball	O
player	O
s	O
salary	O
on	O
the	O
basis	B
of	O
various	O
statistics	O
associated	O
with	O
performance	O
in	O
the	O
previous	O
year	O
first	O
of	O
all	O
we	O
note	O
that	O
the	O
salary	O
variable	B
is	O
missing	O
for	O
some	O
of	O
the	O
players	O
the	O
is	O
na	O
function	B
can	O
be	O
used	O
to	O
identify	O
the	O
missing	O
observations	B
it	O
returns	O
a	O
vector	B
of	O
the	O
same	O
length	O
as	O
the	O
input	B
vector	B
with	O
a	O
true	O
for	O
any	O
elements	O
that	O
are	O
missing	O
and	O
a	O
false	O
for	O
non-missing	O
elements	O
the	O
sum	O
function	B
can	O
then	O
be	O
used	O
to	O
count	O
all	O
of	O
the	O
missing	O
elements	O
is	O
na	O
sum	O
library	O
islr	O
fix	O
hitters	B
names	O
hitters	B
hits	O
years	O
crbi	O
assists	O
atbat	O
walks	O
cruns	O
putouts	O
dim	O
hitters	B
sum	O
is	O
na	O
h	O
i	O
t	O
t	O
e	O
r	O
s	O
s	O
a	O
l	O
a	O
r	O
y	O
hmrun	O
catbat	O
cwalks	O
errors	O
runs	O
chits	O
league	O
salary	O
rbi	O
chmrun	O
division	O
newleague	O
hence	O
we	O
see	O
that	O
salary	O
is	O
missing	O
for	O
players	O
the	O
na	O
omit	O
function	B
removes	O
all	O
of	O
the	O
rows	O
that	O
have	O
missing	O
values	O
in	O
any	O
variable	B
hitters	B
na	O
omit	O
hitters	B
dim	O
hitters	B
sum	O
is	O
na	O
hitters	B
the	O
regsubsets	O
function	B
of	O
the	O
leaps	O
library	O
performs	O
best	B
subset	B
selection	B
by	O
identifying	O
the	O
best	O
model	B
that	O
contains	O
a	O
given	O
number	O
of	O
predictors	O
where	O
best	O
is	O
quantified	O
using	O
rss	O
the	O
syntax	O
is	O
the	O
same	O
as	O
for	O
lm	O
the	O
summary	O
command	O
outputs	O
the	O
best	O
set	B
of	O
variables	O
for	O
each	O
model	B
size	O
regsubsets	O
lab	O
subset	B
selection	B
methods	O
library	O
leaps	O
regfit	O
full	O
regsubset	O
s	O
salary	O
hitters	B
summary	O
regfit	O
full	O
subset	B
selection	B
object	O
call	O
regsubsets	O
formula	O
salary	O
hitters	B
and	O
intercept	B
variables	O
subsets	O
of	O
each	O
size	O
up	O
to	O
selection	B
algorithm	O
exhaustive	O
atbat	O
hits	O
hmrun	O
runs	O
rbi	O
walks	O
years	O
catbat	O
chits	O
chmrun	O
cruns	O
crbi	O
cwalks	O
leaguen	O
divisionw	O
putouts	O
assists	O
errors	O
newleaguen	O
an	O
asterisk	O
indicates	O
that	O
a	O
given	O
variable	B
is	O
included	O
in	O
the	O
corresponding	O
model	B
for	O
instance	O
this	O
output	B
indicates	O
that	O
the	O
best	O
two-variable	O
model	B
contains	O
only	O
hits	O
and	O
crbi	O
by	O
default	B
regsubsets	O
only	O
reports	O
results	O
up	O
to	O
the	O
best	O
eight-variable	O
model	B
but	O
the	O
nvmax	O
option	O
can	O
be	O
used	O
in	O
order	O
to	O
return	O
as	O
many	O
variables	O
as	O
are	O
desired	O
here	O
we	O
fit	O
up	O
to	O
a	O
model	B
regfit	O
full	O
regsubset	O
s	O
salary	O
data	B
hitters	B
nvmax	O
reg	O
summary	O
summary	O
regfit	O
full	O
the	O
summary	O
function	B
also	O
returns	O
rss	O
adjusted	O
cp	B
and	O
bic	O
we	O
can	O
examine	O
these	O
to	O
try	O
to	O
select	O
the	O
best	O
overall	O
model	B
names	O
reg	O
summary	O
which	O
rsq	O
outmat	O
obj	O
rss	O
cp	B
bic	O
linear	B
model	B
selection	B
and	O
regularization	B
for	O
instance	O
we	O
see	O
that	O
the	O
statistic	O
increases	O
from	O
when	O
only	O
one	O
variable	B
is	O
included	O
in	O
the	O
model	B
to	O
almost	O
when	O
all	O
variables	O
are	O
included	O
as	O
expected	O
the	O
statistic	O
increases	O
monotonically	O
as	O
more	O
variables	O
are	O
included	O
reg	O
summaryr	O
sq	O
plotting	O
rss	O
adjusted	O
cp	B
and	O
bic	O
for	O
all	O
of	O
the	O
models	O
at	O
once	O
will	O
help	O
us	O
decide	O
which	O
model	B
to	O
select	O
note	O
the	O
typel	O
option	O
tells	O
r	O
to	O
connect	O
the	O
plotted	O
points	O
with	O
lines	O
par	O
mfrow	O
c	O
plot	B
reg	O
summaryrss	O
xlab	O
number	O
of	O
variables	O
ylab	O
rss	O
type	O
l	O
plot	B
reg	O
xlab	O
number	O
of	O
variables	O
ylab	O
adjusted	O
rsq	O
type	O
l	O
the	O
points	O
command	O
works	O
like	O
the	O
plot	B
command	O
except	O
that	O
it	O
puts	O
points	O
on	O
a	O
plot	B
that	O
has	O
already	O
been	O
created	O
instead	O
of	O
creating	O
a	O
new	O
plot	B
the	O
which	O
max	O
function	B
can	O
be	O
used	O
to	O
identify	O
the	O
location	O
of	O
the	O
maximum	O
point	O
of	O
a	O
vector	B
we	O
will	O
now	O
plot	B
a	O
red	O
dot	O
to	O
indicate	O
the	O
model	B
with	O
the	O
largest	O
adjusted	O
statistic	O
points	O
which	O
max	O
reg	O
s	O
u	O
m	O
m	O
a	O
r	O
y	O
a	O
d	O
j	O
r	O
points	O
reg	O
s	O
u	O
m	O
m	O
a	O
r	O
y	O
a	O
d	O
j	O
r	O
col	O
red	O
cex	O
pch	O
in	O
a	O
similar	O
fashion	O
we	O
can	O
plot	B
the	O
cp	B
and	O
bic	O
statistics	O
and	O
indicate	O
the	O
models	O
with	O
the	O
smallest	O
statistic	O
using	O
which	O
min	O
which	O
min	O
plot	B
reg	O
summarycp	O
xlab	O
number	O
of	O
variables	O
ylab	O
cp	B
type	O
l	O
which	O
min	O
reg	O
summarycp	O
points	O
reg	O
summarycp	O
col	O
red	O
cex	O
pch	O
which	O
min	O
reg	O
summaryb	O
i	O
c	O
plot	B
reg	O
summarybic	O
xlab	O
number	O
of	O
variables	O
ylab	O
bic	O
type	O
l	O
points	O
reg	O
summaryb	O
ic	O
col	O
red	O
cex	O
pch	O
the	O
regsubsets	O
function	B
has	O
a	O
built-in	O
plot	B
command	O
which	O
can	O
be	O
used	O
to	O
display	O
the	O
selected	O
variables	O
for	O
the	O
best	O
model	B
with	O
a	O
given	O
number	O
of	O
predictors	O
ranked	O
according	O
to	O
the	O
bic	O
cp	B
adjusted	O
or	O
aic	O
to	O
find	O
out	O
more	O
about	O
this	O
function	B
type	O
plot	B
regfit	O
full	O
scale	O
plot	B
regfit	O
full	O
scale	O
plot	B
regfit	O
full	O
scale	O
cp	B
plot	B
regfit	O
full	O
scale	O
bic	O
lab	O
subset	B
selection	B
methods	O
the	O
top	O
row	O
of	O
each	O
plot	B
contains	O
a	O
black	O
square	O
for	O
each	O
variable	B
selected	O
according	O
to	O
the	O
optimal	O
model	B
associated	O
with	O
that	O
statistic	O
for	O
instance	O
we	O
see	O
that	O
several	O
models	O
share	O
a	O
bic	O
close	O
to	O
however	O
the	O
model	B
with	O
the	O
lowest	O
bic	O
is	O
the	O
six-variable	O
model	B
that	O
contains	O
only	O
atbat	O
hits	O
walks	O
crbi	O
divisionw	O
and	O
putouts	O
we	O
can	O
use	O
the	O
coef	O
function	B
to	O
see	O
the	O
coefficient	O
estimates	O
associated	O
with	O
this	O
model	B
coef	O
regfit	O
full	O
intercept	B
divisionw	O
atbat	O
putouts	O
hits	O
walks	O
crbi	O
forward	O
and	O
backward	B
stepwise	I
selection	B
we	O
can	O
also	O
use	O
the	O
regsubsets	O
function	B
to	O
perform	O
forward	O
stepwise	O
or	O
backward	B
stepwise	I
selection	B
using	O
the	O
argument	B
methodforward	O
or	O
methodbackward	O
regfit	O
fwd	O
regsubsets	O
salary	O
data	B
hitters	B
nvmax	O
method	O
forward	O
summary	O
regfit	O
fwd	O
regfit	O
bwd	O
regsubsets	O
salary	O
data	B
hitters	B
nvmax	O
method	O
backward	O
summary	O
regfit	O
bwd	O
for	O
instance	O
we	O
see	O
that	O
using	O
forward	B
stepwise	I
selection	B
the	O
best	O
onevariable	O
model	B
contains	O
only	O
crbi	O
and	O
the	O
best	O
two-variable	O
model	B
additionally	O
includes	O
hits	O
for	O
this	O
data	B
the	O
best	O
one-variable	O
through	O
sixvariable	O
models	O
are	O
each	O
identical	O
for	O
best	O
subset	O
and	O
forward	O
selection	B
however	O
the	O
best	O
seven-variable	O
models	O
identified	O
by	O
forward	B
stepwise	I
selection	B
backward	B
stepwise	I
selection	B
and	O
best	B
subset	B
selection	B
are	O
different	O
coef	O
regfit	O
full	O
intercept	B
chmrun	O
hits	O
divisionw	O
coef	O
regfit	O
fwd	O
intercept	B
cwalks	O
atbat	O
divisionw	O
coef	O
regfit	O
bwd	O
intercept	B
cwalks	O
atbat	O
divisionw	O
walks	O
putouts	O
hits	O
putouts	O
hits	O
putouts	O
catbat	O
chits	O
walks	O
crbi	O
walks	O
cruns	O
linear	B
model	B
selection	B
and	O
regularization	B
choosing	O
among	O
models	O
using	O
the	O
validation	B
set	B
approach	B
and	O
cross-validation	B
we	O
just	O
saw	O
that	O
it	O
is	O
possible	O
to	O
choose	O
among	O
a	O
set	B
of	O
models	O
of	O
different	O
sizes	O
using	O
cp	B
bic	O
and	O
adjusted	O
we	O
will	O
now	O
consider	O
how	O
to	O
do	O
this	O
using	O
the	O
validation	B
set	B
and	O
cross-validation	B
approaches	O
in	O
order	O
for	O
these	O
approaches	O
to	O
yield	O
accurate	O
estimates	O
of	O
the	O
test	O
error	B
we	O
must	O
use	O
only	O
the	O
training	O
observations	B
to	O
perform	O
all	O
aspects	O
of	O
model-fitting	O
including	O
variable	B
selection	B
therefore	O
the	O
determination	O
of	O
which	O
model	B
of	O
a	O
given	O
size	O
is	O
best	O
must	O
be	O
made	O
using	O
only	O
the	O
training	O
observations	B
this	O
point	O
is	O
subtle	O
but	O
important	O
if	O
the	O
full	O
data	B
set	B
is	O
used	O
to	O
perform	O
the	O
best	B
subset	B
selection	B
step	O
the	O
validation	B
set	B
errors	O
and	O
cross-validation	B
errors	O
that	O
we	O
obtain	O
will	O
not	O
be	O
accurate	O
estimates	O
of	O
the	O
test	O
error	B
in	O
order	O
to	O
use	O
the	O
validation	B
set	B
approach	B
we	O
begin	O
by	O
splitting	O
the	O
observations	B
into	O
a	O
training	O
set	B
and	O
a	O
test	O
set	B
we	O
do	O
this	O
by	O
creating	O
a	O
random	O
vector	B
train	B
of	O
elements	O
equal	O
to	O
true	O
if	O
the	O
corresponding	O
observation	O
is	O
in	O
the	O
training	O
set	B
and	O
false	O
otherwise	O
the	O
vector	B
test	O
has	O
a	O
true	O
if	O
the	O
observation	O
is	O
in	O
the	O
test	O
set	B
and	O
a	O
false	O
otherwise	O
note	O
the	O
in	O
the	O
command	O
to	O
create	O
test	O
causes	O
trues	O
to	O
be	O
switched	O
to	O
falses	O
and	O
vice	O
versa	O
we	O
also	O
set	B
a	O
random	O
seed	B
so	O
that	O
the	O
user	O
will	O
obtain	O
the	O
same	O
training	O
settest	O
set	B
split	O
set	B
seed	B
train	B
sample	O
c	O
true	O
false	O
nrow	O
hitters	B
rep	O
true	O
test	O
train	B
now	O
we	O
apply	O
regsubsets	O
to	O
the	O
training	O
set	B
in	O
order	O
to	O
perform	O
best	B
subset	B
selection	B
regfit	O
best	O
regsubset	O
s	O
salary	O
data	B
hitters	B
train	B
nvmax	O
notice	O
that	O
we	O
subset	O
the	O
hitters	B
data	B
frame	I
directly	O
in	O
the	O
call	O
in	O
order	O
to	O
access	O
only	O
the	O
training	O
subset	O
of	O
the	O
data	B
using	O
the	O
expression	O
hitterstrain	O
we	O
now	O
compute	O
the	O
validation	B
set	B
error	B
for	O
the	O
best	O
model	B
of	O
each	O
model	B
size	O
we	O
first	O
make	O
a	O
model	B
matrix	O
from	O
the	O
test	O
data	B
test	O
mat	O
model	B
matrix	O
salary	O
data	B
hitters	B
test	O
the	O
model	B
matrix	O
function	B
is	O
used	O
in	O
many	O
regression	B
packages	O
for	O
building	O
an	O
x	O
matrix	O
from	O
data	B
now	O
we	O
run	O
a	O
loop	O
and	O
for	O
each	O
size	O
i	O
we	O
extract	O
the	O
coefficients	O
from	O
regfit	O
best	O
for	O
the	O
best	O
model	B
of	O
that	O
size	O
multiply	O
them	O
into	O
the	O
appropriate	O
columns	O
of	O
the	O
test	O
model	B
matrix	O
to	O
form	O
the	O
predictions	O
and	O
compute	O
the	O
test	O
mse	B
model	B
matrix	O
val	O
errors	O
rep	O
na	O
for	O
i	O
in	O
coefi	O
coef	O
regfit	O
best	O
id	O
i	O
lab	O
subset	B
selection	B
methods	O
pred	O
test	O
mat	O
names	O
coefi	O
coefi	O
val	O
errors	O
i	O
mean	O
h	O
i	O
t	O
t	O
e	O
r	O
s	O
s	O
a	O
l	O
a	O
r	O
y	O
test	O
pred	O
we	O
find	O
that	O
the	O
best	O
model	B
is	O
the	O
one	O
that	O
contains	O
ten	O
variables	O
val	O
errors	O
which	O
min	O
val	O
errors	O
coef	O
regfit	O
best	O
intercept	B
chits	O
putouts	O
atbat	O
chmrun	O
hits	O
cwalks	O
walks	O
leaguen	O
catbat	O
divisionw	O
this	O
was	O
a	O
little	O
tedious	O
partly	O
because	O
there	O
is	O
no	O
predict	O
method	O
for	O
regsubsets	O
since	O
we	O
will	O
be	O
using	O
this	O
function	B
again	O
we	O
can	O
capture	O
our	O
steps	O
above	O
and	O
write	O
our	O
own	O
predict	O
method	O
predict	O
regsubset	O
s	O
function	B
object	O
newdata	O
id	O
form	O
as	O
formula	O
objectca	O
l	O
l	O
mat	O
model	B
matrix	O
form	O
newdata	O
coefi	O
coef	O
object	O
id	O
id	O
xvars	O
names	O
coefi	O
mat	O
xvars	O
coefi	O
our	O
function	B
pretty	O
much	O
mimics	O
what	O
we	O
did	O
above	O
the	O
only	O
complex	O
part	O
is	O
how	O
we	O
extracted	O
the	O
formula	O
used	O
in	O
the	O
call	O
to	O
regsubsets	O
we	O
demonstrate	O
how	O
we	O
use	O
this	O
function	B
below	O
when	O
we	O
do	O
cross-validation	B
finally	O
we	O
perform	O
best	B
subset	B
selection	B
on	O
the	O
full	O
data	B
set	B
and	O
select	O
the	O
best	O
ten-variable	O
model	B
it	O
is	O
important	O
that	O
we	O
make	O
use	O
of	O
the	O
full	O
data	B
set	B
in	O
order	O
to	O
obtain	O
more	O
accurate	O
coefficient	O
estimates	O
note	O
that	O
we	O
perform	O
best	B
subset	B
selection	B
on	O
the	O
full	O
data	B
set	B
and	O
select	O
the	O
best	O
tenvariable	O
model	B
rather	O
than	O
simply	O
using	O
the	O
variables	O
that	O
were	O
obtained	O
from	O
the	O
training	O
set	B
because	O
the	O
best	O
ten-variable	O
model	B
on	O
the	O
full	O
data	B
set	B
may	O
differ	O
from	O
the	O
corresponding	O
model	B
on	O
the	O
training	O
set	B
regfit	O
best	O
regsubset	O
s	O
salary	O
data	B
hitters	B
nvmax	O
coef	O
regfit	O
best	O
intercept	B
cruns	O
assists	O
atbat	O
crbi	O
hits	O
cwalks	O
walks	O
divisionw	O
catbat	O
putouts	O
linear	B
model	B
selection	B
and	O
regularization	B
in	O
fact	O
we	O
see	O
that	O
the	O
best	O
ten-variable	O
model	B
on	O
the	O
full	O
data	B
set	B
has	O
a	O
different	O
set	B
of	O
variables	O
than	O
the	O
best	O
ten-variable	O
model	B
on	O
the	O
training	O
set	B
we	O
now	O
try	O
to	O
choose	O
among	O
the	O
models	O
of	O
different	O
sizes	O
using	O
crossvalidation	O
this	O
approach	B
is	O
somewhat	O
involved	O
as	O
we	O
must	O
perform	O
best	B
subset	B
selection	B
within	O
each	O
of	O
the	O
k	O
training	O
sets	O
despite	O
this	O
we	O
see	O
that	O
with	O
its	O
clever	O
subsetting	O
syntax	O
r	O
makes	O
this	O
job	O
quite	O
easy	O
first	O
we	O
create	O
a	O
vector	B
that	O
allocates	O
each	O
observation	O
to	O
one	O
of	O
k	O
folds	O
and	O
we	O
create	O
a	O
matrix	O
in	O
which	O
we	O
will	O
store	O
the	O
results	O
k	O
set	B
seed	B
folds	O
sample	O
k	O
nrow	O
hitters	B
replace	O
true	O
cv	O
errors	O
matrix	O
na	O
dimnames	O
list	O
null	B
paste	O
now	O
we	O
write	O
a	O
for	B
loop	I
that	O
performs	O
cross-validation	B
in	O
the	O
jth	O
fold	O
the	O
elements	O
of	O
folds	O
that	O
equal	O
j	O
are	O
in	O
the	O
test	O
set	B
and	O
the	O
remainder	O
are	O
in	O
the	O
training	O
set	B
we	O
make	O
our	O
predictions	O
for	O
each	O
model	B
size	O
our	O
new	O
predict	O
method	O
compute	O
the	O
test	O
errors	O
on	O
the	O
appropriate	O
subset	O
and	O
store	O
them	O
in	O
the	O
appropriate	O
slot	O
in	O
the	O
matrix	O
cv	O
errors	O
for	O
j	O
in	O
k	O
best	O
fit	O
regsubsets	O
salary	O
data	B
hitters	B
folds	O
j	O
nvmax	O
for	O
i	O
in	O
pred	O
predict	O
best	O
fit	O
hitters	B
folds	O
j	O
id	O
i	O
cv	O
errors	O
i	O
mean	O
h	O
i	O
t	O
t	O
e	O
r	O
s	O
s	O
a	O
l	O
a	O
r	O
y	O
folds	O
j	O
pred	O
this	O
has	O
given	O
us	O
a	O
matrix	O
of	O
which	O
the	O
jth	O
element	O
corresponds	O
to	O
the	O
test	O
mse	B
for	O
the	O
ith	O
cross-validation	B
fold	O
for	O
the	O
best	O
j-variable	O
model	B
we	O
use	O
the	O
apply	O
function	B
to	O
average	B
over	O
the	O
columns	O
of	O
this	O
matrix	O
in	O
order	O
to	O
obtain	O
a	O
vector	B
for	O
which	O
the	O
jth	O
element	O
is	O
the	O
crossvalidation	O
error	B
for	O
the	O
j-variable	O
model	B
apply	O
mean	O
cv	O
errors	O
apply	O
cv	O
errors	O
mean	O
mean	O
cv	O
errors	O
par	O
mfrow	O
c	O
plot	B
mean	O
cv	O
errors	O
type	O
b	O
we	O
see	O
that	O
cross-validation	B
selects	O
an	O
model	B
we	O
now	O
perform	O
best	B
subset	B
selection	B
on	O
the	O
full	O
data	B
set	B
in	O
order	O
to	O
obtain	O
the	O
model	B
reg	O
best	O
regsubset	O
s	O
salary	O
data	B
hitters	B
nvmax	O
coef	O
reg	O
best	O
intercept	B
atbat	O
hits	O
walks	O
catbat	O
glmnet	O
lab	O
ridge	B
regression	B
and	O
the	O
lasso	B
cruns	O
putouts	O
crbi	O
assists	O
cwalks	O
leaguen	O
divisionw	O
lab	O
ridge	B
regression	B
and	O
the	O
lasso	B
we	O
will	O
use	O
the	O
glmnet	O
package	O
in	O
order	O
to	O
perform	O
ridge	B
regression	B
and	O
the	O
lasso	B
the	O
main	O
function	B
in	O
this	O
package	O
is	O
glmnet	O
which	O
can	O
be	O
used	O
to	O
fit	O
ridge	B
regression	B
models	O
lasso	B
models	O
and	O
more	O
this	O
function	B
has	O
slightly	O
different	O
syntax	O
from	O
other	O
model-fitting	O
functions	O
that	O
we	O
have	O
encountered	O
thus	O
far	O
in	O
this	O
book	O
in	O
particular	O
we	O
must	O
pass	O
in	O
an	O
x	O
matrix	O
as	O
well	O
as	O
a	O
y	O
vector	B
and	O
we	O
do	O
not	O
use	O
the	O
y	O
x	O
syntax	O
we	O
will	O
now	O
perform	O
ridge	B
regression	B
and	O
the	O
lasso	B
in	O
order	O
to	O
predict	O
salary	O
on	O
the	O
hitters	B
data	B
before	O
proceeding	O
ensure	O
that	O
the	O
missing	O
values	O
have	O
been	O
removed	O
from	O
the	O
data	B
as	O
described	O
in	O
section	O
x	O
model	B
matrix	O
salary	O
hitters	B
y	O
h	O
i	O
t	O
t	O
e	O
r	O
s	O
s	O
a	O
l	O
a	O
r	O
y	O
the	O
model	B
matrix	O
function	B
is	O
particularly	O
useful	O
for	O
creating	O
x	O
not	O
only	O
does	O
it	O
produce	O
a	O
matrix	O
corresponding	O
to	O
the	O
predictors	O
but	O
it	O
also	O
automatically	O
transforms	O
any	O
qualitative	B
variables	O
into	O
dummy	B
variables	O
the	O
latter	O
property	O
is	O
important	O
because	O
glmnet	O
can	O
only	O
take	O
numerical	O
quantitative	B
inputs	O
ridge	B
regression	B
the	O
glmnet	O
function	B
has	O
an	O
alpha	O
argument	B
that	O
determines	O
what	O
type	O
of	O
model	B
is	O
fit	O
if	O
then	O
a	O
ridge	B
regression	B
model	B
is	O
fit	O
and	O
if	O
then	O
a	O
lasso	B
model	B
is	O
fit	O
we	O
first	O
fit	O
a	O
ridge	B
regression	B
model	B
library	O
glmnet	O
grid	O
seq	O
length	O
ridge	O
mod	O
glmnet	O
alpha	O
lambda	O
grid	O
by	O
default	B
the	O
glmnet	O
function	B
performs	O
ridge	B
regression	B
for	O
an	O
automatically	O
selected	O
range	O
of	O
values	O
however	O
here	O
we	O
have	O
chosen	O
to	O
implement	O
esthe	O
function	B
over	O
a	O
grid	O
of	O
values	O
ranging	O
from	O
to	O
sentially	O
covering	O
the	O
full	O
range	O
of	O
scenarios	O
from	O
the	O
null	B
model	B
containing	O
only	O
the	O
intercept	B
to	O
the	O
least	B
squares	I
fit	O
as	O
we	O
will	O
see	O
we	O
can	O
also	O
compute	O
model	B
fits	O
for	O
a	O
particular	O
value	O
of	O
that	O
is	O
not	O
one	O
of	O
the	O
original	O
grid	O
values	O
note	O
that	O
by	O
default	B
the	O
glmnet	O
function	B
standardizes	O
the	O
variables	O
so	O
that	O
they	O
are	O
on	O
the	O
same	O
scale	O
to	O
turn	O
off	O
this	O
default	B
setting	O
use	O
the	O
argument	B
standardizefalse	O
associated	O
with	O
each	O
value	O
of	O
is	O
a	O
vector	B
of	O
ridge	B
regression	B
coefficients	O
stored	O
in	O
a	O
matrix	O
that	O
can	O
be	O
accessed	O
by	O
coef	O
in	O
this	O
case	O
it	O
is	O
a	O
linear	B
model	B
selection	B
and	O
regularization	B
matrix	O
with	O
rows	O
for	O
each	O
predictor	B
plus	O
an	O
intercept	B
and	O
columns	O
for	O
each	O
value	O
of	O
dim	O
coef	O
ridge	O
mod	O
we	O
expect	O
the	O
coefficient	O
estimates	O
to	O
be	O
much	O
smaller	O
in	O
terms	O
of	O
norm	O
when	O
a	O
large	O
value	O
of	O
is	O
used	O
as	O
compared	O
to	O
when	O
a	O
small	O
value	O
of	O
is	O
used	O
these	O
are	O
the	O
coefficients	O
when	O
along	O
with	O
their	O
norm	O
ridge	O
modlambd	O
a	O
coef	O
ridge	O
mod	O
intercept	B
rbi	O
chmrun	O
divisionw	O
atbat	O
walks	O
cruns	O
putouts	O
hits	O
years	O
crbi	O
assists	O
hmrun	O
catbat	O
cwalks	O
errors	O
runs	O
chits	O
leaguen	O
newleague	O
n	O
sqrt	O
sum	O
coef	O
ridge	O
mod	O
in	O
contrast	B
here	O
are	O
the	O
coefficients	O
when	O
along	O
with	O
their	O
norm	O
note	O
the	O
much	O
larger	O
norm	O
of	O
the	O
coefficients	O
associated	O
with	O
this	O
smaller	O
value	O
of	O
ridge	O
modlambd	O
a	O
coef	O
ridge	O
mod	O
intercept	B
rbi	O
chmrun	O
divisionw	O
atbat	O
walks	O
cruns	O
putouts	O
hits	O
years	O
crbi	O
assists	O
hmrun	O
catbat	O
cwalks	O
errors	O
runs	O
chits	O
leaguen	O
newleague	O
n	O
sqrt	O
sum	O
coef	O
ridge	O
mod	O
we	O
can	O
use	O
the	O
predict	O
function	B
for	O
a	O
number	O
of	O
purposes	O
for	O
instance	O
we	O
can	O
obtain	O
the	O
ridge	B
regression	B
coefficients	O
for	O
a	O
new	O
value	O
of	O
say	O
predict	O
ridge	O
mod	O
s	O
type	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
intercept	B
rbi	O
chmrun	O
divisionw	O
hits	O
years	O
crbi	O
assists	O
atbat	O
walks	O
cruns	O
putouts	O
hmrun	O
catbat	O
cwalks	O
errors	O
runs	O
chits	O
leaguen	O
newleague	O
n	O
lab	O
ridge	B
regression	B
and	O
the	O
lasso	B
we	O
now	O
split	O
the	O
samples	O
into	O
a	O
training	O
set	B
and	O
a	O
test	O
set	B
in	O
order	O
to	O
estimate	O
the	O
test	O
error	B
of	O
ridge	B
regression	B
and	O
the	O
lasso	B
there	O
are	O
two	O
common	O
ways	O
to	O
randomly	O
split	O
a	O
data	B
set	B
the	O
first	O
is	O
to	O
produce	O
a	O
random	O
vector	B
of	O
true	O
false	O
elements	O
and	O
select	O
the	O
observations	B
corresponding	O
to	O
true	O
for	O
the	O
training	O
data	B
the	O
second	O
is	O
to	O
randomly	O
choose	O
a	O
subset	O
of	O
numbers	O
between	O
and	O
n	O
these	O
can	O
then	O
be	O
used	O
as	O
the	O
indices	O
for	O
the	O
training	O
observations	B
the	O
two	O
approaches	O
work	O
equally	O
well	O
we	O
used	O
the	O
former	O
method	O
in	O
section	O
here	O
we	O
demonstrate	O
the	O
latter	O
approach	B
we	O
first	O
set	B
a	O
random	O
seed	B
so	O
that	O
the	O
results	O
obtained	O
will	O
be	O
repro	O
ducible	O
set	B
seed	B
train	B
sample	O
nrow	O
x	O
nrow	O
x	O
test	O
train	B
y	O
test	O
y	O
test	O
next	O
we	O
fit	O
a	O
ridge	B
regression	B
model	B
on	O
the	O
training	O
set	B
and	O
evaluate	O
its	O
mse	B
on	O
the	O
test	O
set	B
using	O
note	O
the	O
use	O
of	O
the	O
predict	O
function	B
again	O
this	O
time	O
we	O
get	O
predictions	O
for	O
a	O
test	O
set	B
by	O
replacing	O
typecoefficients	O
with	O
the	O
newx	O
argument	B
ridge	O
mod	O
glmnet	O
x	O
train	B
y	O
train	B
alpha	O
lambda	O
grid	O
thresh	O
e	O
ridge	O
pred	O
predict	O
ridge	O
mod	O
s	O
newx	O
x	O
test	O
mean	O
ridge	O
pred	O
y	O
test	O
the	O
test	O
mse	B
is	O
note	O
that	O
if	O
we	O
had	O
instead	O
simply	O
fit	O
a	O
model	B
with	O
just	O
an	O
intercept	B
we	O
would	O
have	O
predicted	O
each	O
test	O
observation	O
using	O
the	O
mean	O
of	O
the	O
training	O
observations	B
in	O
that	O
case	O
we	O
could	O
compute	O
the	O
test	O
set	B
mse	B
like	O
this	O
mean	O
mean	O
y	O
train	B
y	O
test	O
we	O
could	O
also	O
get	O
the	O
same	O
result	O
by	O
fitting	O
a	O
ridge	B
regression	B
model	B
with	O
a	O
very	O
large	O
value	O
of	O
note	O
that	O
means	O
ridge	O
pred	O
predict	O
ridge	O
mod	O
s	O
newx	O
x	O
test	O
mean	O
ridge	O
pred	O
y	O
test	O
so	O
fitting	O
a	O
ridge	B
regression	B
model	B
with	O
leads	O
to	O
a	O
much	O
lower	O
test	O
mse	B
than	O
fitting	O
a	O
model	B
with	O
just	O
an	O
intercept	B
we	O
now	O
check	O
whether	O
there	O
is	O
any	O
benefit	O
to	O
performing	O
ridge	B
regression	B
with	O
instead	O
of	O
just	O
performing	O
least	B
squares	I
regression	B
recall	B
that	O
least	B
squares	I
is	O
simply	O
ridge	B
regression	B
with	O
in	O
order	O
for	O
glmnet	O
to	O
yield	O
the	O
exact	O
least	B
squares	I
coefficients	O
when	O
we	O
use	O
the	O
argument	B
exactt	O
when	O
calling	O
the	O
predict	O
function	B
otherwise	O
the	O
interpolate	O
over	O
the	O
grid	O
of	O
values	O
used	O
in	O
fitting	O
the	O
predict	O
function	B
will	O
linear	B
model	B
selection	B
and	O
regularization	B
ridge	O
pred	O
predict	O
ridge	O
mod	O
s	O
newx	O
x	O
test	O
exact	O
t	O
mean	O
ridge	O
pred	O
y	O
test	O
lm	O
y	O
x	O
subset	O
train	B
predict	O
ridge	O
mod	O
s	O
exact	O
type	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
in	O
general	O
if	O
we	O
want	O
to	O
fit	O
a	O
least	B
squares	I
model	B
then	O
we	O
should	O
use	O
the	O
lm	O
function	B
since	O
that	O
function	B
provides	O
more	O
useful	O
outputs	O
such	O
as	O
standard	O
errors	O
and	O
p-values	O
for	O
the	O
coefficients	O
in	O
general	O
instead	O
of	O
arbitrarily	O
choosing	O
it	O
would	O
be	O
better	O
to	O
use	O
cross-validation	B
to	O
choose	O
the	O
tuning	B
parameter	B
we	O
can	O
do	O
this	O
using	O
the	O
built-in	O
cross-validation	B
function	B
cv	O
glmnet	O
by	O
default	B
the	O
function	B
performs	O
ten-fold	O
cross-validation	B
though	O
this	O
can	O
be	O
changed	O
using	O
the	O
argument	B
nfolds	O
note	O
that	O
we	O
set	B
a	O
random	O
seed	B
first	O
so	O
our	O
results	O
will	O
be	O
reproducible	O
since	O
the	O
choice	O
of	O
the	O
cross-validation	B
folds	O
is	O
random	O
set	B
seed	B
cv	O
out	O
cv	O
glmnet	O
x	O
train	B
y	O
train	B
alpha	O
plot	B
cv	O
out	O
bestlam	O
cv	O
outlambda	O
min	O
bestlam	O
therefore	O
we	O
see	O
that	O
the	O
value	O
of	O
that	O
results	O
in	O
the	O
smallest	O
crossvalidation	O
error	B
is	O
what	O
is	O
the	O
test	O
mse	B
associated	O
with	O
this	O
value	O
of	O
ridge	O
pred	O
predict	O
ridge	O
mod	O
s	O
bestlam	O
newx	O
x	O
test	O
mean	O
ridge	O
pred	O
y	O
test	O
this	O
represents	O
a	O
further	O
improvement	O
over	O
the	O
test	O
mse	B
that	O
we	O
got	O
using	O
finally	O
we	O
refit	O
our	O
ridge	B
regression	B
model	B
on	O
the	O
full	O
data	B
set	B
using	O
the	O
value	O
of	O
chosen	O
by	O
cross-validation	B
and	O
examine	O
the	O
coefficient	O
estimates	O
cv	O
glmnet	O
out	O
glmnet	O
alpha	O
predict	O
out	O
type	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
s	O
bestlam	O
intercept	B
rbi	O
chmrun	O
divisionw	O
hits	O
years	O
crbi	O
assists	O
atbat	O
walks	O
cruns	O
putouts	O
hmrun	O
catbat	O
cwalks	O
errors	O
runs	O
chits	O
leaguen	O
newleague	O
n	O
glmnet	O
model	B
yielding	O
approximate	O
results	O
when	O
we	O
use	O
exactt	O
there	O
remains	O
a	O
slight	O
discrepancy	O
in	O
the	O
third	O
decimal	O
place	O
between	O
the	O
output	B
of	O
glmnet	O
when	O
and	O
the	O
output	B
of	O
lm	O
this	O
is	O
due	O
to	O
numerical	O
approximation	O
on	O
the	O
part	O
of	O
glmnet	O
lab	O
ridge	B
regression	B
and	O
the	O
lasso	B
as	O
expected	O
none	O
of	O
the	O
coefficients	O
are	O
zero	O
ridge	B
regression	B
does	O
not	O
perform	O
variable	B
selection	B
the	O
lasso	B
we	O
saw	O
that	O
ridge	B
regression	B
with	O
a	O
wise	O
choice	O
of	O
can	O
outperform	O
least	B
squares	I
as	O
well	O
as	O
the	O
null	B
model	B
on	O
the	O
hitters	B
data	B
set	B
we	O
now	O
ask	O
whether	O
the	O
lasso	B
can	O
yield	O
either	O
a	O
more	O
accurate	O
or	O
a	O
more	O
interpretable	O
model	B
than	O
ridge	B
regression	B
in	O
order	O
to	O
fit	O
a	O
lasso	B
model	B
we	O
once	O
again	O
use	O
the	O
glmnet	O
function	B
however	O
this	O
time	O
we	O
use	O
the	O
argument	B
other	O
than	O
that	O
change	O
we	O
proceed	O
just	O
as	O
we	O
did	O
in	O
fitting	O
a	O
ridge	O
model	B
lasso	B
mod	O
glmnet	O
x	O
train	B
y	O
train	B
alpha	O
lambda	O
grid	O
plot	B
lasso	B
mod	O
we	O
can	O
see	O
from	O
the	O
coefficient	O
plot	B
that	O
depending	O
on	O
the	O
choice	O
of	O
tuning	B
parameter	B
some	O
of	O
the	O
coefficients	O
will	O
be	O
exactly	O
equal	O
to	O
zero	O
we	O
now	O
perform	O
cross-validation	B
and	O
compute	O
the	O
associated	O
test	O
error	B
set	B
seed	B
cv	O
out	O
cv	O
glmnet	O
x	O
train	B
y	O
train	B
alpha	O
plot	B
cv	O
out	O
bestlam	O
cv	O
outlambda	O
min	O
lasso	B
pred	O
predict	O
lasso	B
mod	O
s	O
bestlam	O
newx	O
x	O
test	O
mean	O
lasso	B
pred	O
y	O
test	O
this	O
is	O
substantially	O
lower	O
than	O
the	O
test	O
set	B
mse	B
of	O
the	O
null	B
model	B
and	O
of	O
least	B
squares	I
and	O
very	O
similar	O
to	O
the	O
test	O
mse	B
of	O
ridge	B
regression	B
with	O
chosen	O
by	O
cross-validation	B
however	O
the	O
lasso	B
has	O
a	O
substantial	O
advantage	O
over	O
ridge	B
regression	B
in	O
that	O
the	O
resulting	O
coefficient	O
estimates	O
are	O
sparse	B
here	O
we	O
see	O
that	O
of	O
the	O
coefficient	O
estimates	O
are	O
exactly	O
zero	O
so	O
the	O
lasso	B
model	B
with	O
chosen	O
by	O
cross-validation	B
contains	O
only	O
seven	O
variables	O
out	O
glmnet	O
alpha	O
lambda	O
grid	O
lasso	B
coef	O
predict	O
out	O
type	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
s	O
bestlam	O
lasso	B
coef	O
intercept	B
rbi	O
chmrun	O
divisionw	O
runs	O
chits	O
leaguen	O
newleague	O
n	O
hmrun	O
catbat	O
cwalks	O
errors	O
atbat	O
walks	O
cruns	O
putouts	O
hits	O
years	O
crbi	O
assists	O
lasso	B
coef	O
lasso	B
coef	O
intercept	B
leaguen	O
hits	O
divisionw	O
walks	O
putouts	O
cruns	O
crbi	O
linear	B
model	B
selection	B
and	O
regularization	B
lab	O
pcr	O
and	O
pls	O
regression	B
principal	B
components	I
regression	B
principal	B
components	I
regression	B
can	O
be	O
performed	O
using	O
the	O
pcr	O
function	B
which	O
is	O
part	O
of	O
the	O
pls	O
library	O
we	O
now	O
apply	O
pcr	O
to	O
the	O
hitters	B
data	B
in	O
order	O
to	O
predict	O
salary	O
again	O
ensure	O
that	O
the	O
missing	O
values	O
have	O
been	O
removed	O
from	O
the	O
data	B
as	O
described	O
in	O
section	O
pcr	O
library	O
pls	O
set	B
seed	B
pcr	O
fit	O
pcr	O
salary	O
data	B
hitters	B
scale	O
true	O
validation	O
cv	O
the	O
syntax	O
for	O
the	O
pcr	O
function	B
is	O
similar	O
to	O
that	O
for	O
lm	O
with	O
a	O
few	O
additional	O
options	O
setting	O
scaletrue	O
has	O
the	O
effect	O
of	O
standardizing	O
each	O
predictor	B
using	O
prior	O
to	O
generating	O
the	O
principal	B
components	I
so	O
that	O
the	O
scale	O
on	O
which	O
each	O
variable	B
is	O
measured	O
will	O
not	O
have	O
an	O
effect	O
setting	O
validationcv	O
causes	O
pcr	O
to	O
compute	O
the	O
ten-fold	O
cross-validation	B
error	B
for	O
each	O
possible	O
value	O
of	O
m	O
the	O
number	O
of	O
principal	B
components	I
used	O
the	O
resulting	O
fit	O
can	O
be	O
examined	O
using	O
summary	O
summary	O
pcr	O
fit	O
data	B
x	O
dimension	O
y	O
dimension	O
fit	O
method	O
svdpc	O
number	O
of	O
components	O
considered	O
validatio	O
n	O
rmsep	O
cross	O
validated	O
using	O
random	O
segments	O
intercept	B
comps	O
comps	O
comps	O
comps	O
cv	O
adjcv	O
training	O
variance	B
explained	B
comps	O
comps	O
comps	O
comps	O
comps	O
comps	O
x	O
salary	O
the	O
cv	O
score	O
is	O
provided	O
for	O
each	O
possible	O
number	O
of	O
components	O
ranging	O
from	O
m	O
onwards	O
have	O
printed	O
the	O
cv	O
output	B
only	O
up	O
to	O
m	O
note	O
that	O
pcr	O
reports	O
the	O
root	O
mean	B
squared	I
error	B
in	O
order	O
to	O
obtain	O
the	O
usual	O
mse	B
we	O
must	O
square	O
this	O
quantity	O
for	O
instance	O
a	O
root	O
mean	B
squared	I
error	B
of	O
corresponds	O
to	O
an	O
mse	B
of	O
one	O
can	O
also	O
plot	B
the	O
cross-validation	B
scores	O
using	O
the	O
validationplot	O
function	B
using	O
val	O
typemsep	O
will	O
cause	O
the	O
cross-validation	B
mse	B
to	O
be	O
plotted	O
v	O
a	O
l	O
i	O
d	O
a	O
t	O
i	O
o	O
n	O
p	O
l	O
o	O
t	O
pcr	O
fit	O
val	O
type	O
msep	O
validation	O
plot	B
lab	O
pcr	O
and	O
pls	O
regression	B
we	O
see	O
that	O
the	O
smallest	O
cross-validation	B
error	B
occurs	O
when	O
m	O
components	O
are	O
used	O
this	O
is	O
barely	O
fewer	O
than	O
m	O
which	O
amounts	O
to	O
simply	O
performing	O
least	B
squares	I
because	O
when	O
all	O
of	O
the	O
components	O
are	O
used	O
in	O
pcr	O
no	O
dimension	B
reduction	I
occurs	O
however	O
from	O
the	O
plot	B
we	O
also	O
see	O
that	O
the	O
cross-validation	B
error	B
is	O
roughly	O
the	O
same	O
when	O
only	O
one	O
component	O
is	O
included	O
in	O
the	O
model	B
this	O
suggests	O
that	O
a	O
model	B
that	O
uses	O
just	O
a	O
small	O
number	O
of	O
components	O
might	O
suffice	O
the	O
summary	O
function	B
also	O
provides	O
the	O
percentage	O
of	O
variance	B
explained	B
in	O
the	O
predictors	O
and	O
in	O
the	O
response	B
using	O
different	O
numbers	O
of	O
components	O
this	O
concept	O
is	O
discussed	O
in	O
greater	O
detail	O
in	O
chapter	O
briefly	O
we	O
can	O
think	O
of	O
this	O
as	O
the	O
amount	O
of	O
information	O
about	O
the	O
predictors	O
or	O
the	O
response	B
that	O
is	O
captured	O
using	O
m	O
principal	B
components	I
for	O
example	O
setting	O
m	O
only	O
captures	O
of	O
all	O
the	O
variance	B
or	O
information	O
in	O
the	O
predictors	O
in	O
contrast	B
using	O
m	O
increases	O
the	O
value	O
to	O
if	O
we	O
were	O
to	O
use	O
all	O
m	O
p	O
components	O
this	O
would	O
increase	O
to	O
we	O
now	O
perform	O
pcr	O
on	O
the	O
training	O
data	B
and	O
evaluate	O
its	O
test	O
set	B
performance	O
set	B
seed	B
pcr	O
fit	O
pcr	O
salary	O
data	B
hitters	B
subset	O
train	B
scale	O
true	O
validation	O
cv	O
v	O
a	O
l	O
i	O
d	O
a	O
t	O
i	O
o	O
n	O
p	O
l	O
o	O
t	O
pcr	O
fit	O
val	O
type	O
msep	O
now	O
we	O
find	O
that	O
the	O
lowest	O
cross-validation	B
error	B
occurs	O
when	O
m	O
component	O
are	O
used	O
we	O
compute	O
the	O
test	O
mse	B
as	O
follows	O
pcr	O
pred	O
predict	O
pcr	O
fit	O
x	O
test	O
ncomp	O
mean	O
pcr	O
pred	O
y	O
test	O
this	O
test	O
set	B
mse	B
is	O
competitive	O
with	O
the	O
results	O
obtained	O
using	O
ridge	B
regression	B
and	O
the	O
lasso	B
however	O
as	O
a	O
result	O
of	O
the	O
way	O
pcr	O
is	O
implemented	O
the	O
final	O
model	B
is	O
more	O
difficult	O
to	O
interpret	O
because	O
it	O
does	O
not	O
perform	O
any	O
kind	O
of	O
variable	B
selection	B
or	O
even	O
directly	O
produce	O
coefficient	O
estimates	O
finally	O
we	O
fit	O
pcr	O
on	O
the	O
full	O
data	B
set	B
using	O
m	O
the	O
number	O
of	O
components	O
identified	O
by	O
cross-validation	B
pcr	O
fit	O
pcr	O
y	O
x	O
scale	O
true	O
ncomp	O
summary	O
pcr	O
fit	O
data	B
x	O
dimension	O
y	O
dimension	O
fit	O
method	O
svdpc	O
number	O
of	O
components	O
considered	O
training	O
variance	B
explained	B
comps	O
comps	O
comps	O
comps	O
comps	O
comps	O
comps	O
x	O
y	O
x	O
y	O
linear	B
model	B
selection	B
and	O
regularization	B
partial	B
least	B
squares	I
we	O
implement	O
partial	B
least	B
squares	I
using	O
the	O
plsr	O
function	B
also	O
in	O
the	O
pls	O
library	O
the	O
syntax	O
is	O
just	O
like	O
that	O
of	O
the	O
pcr	O
function	B
plsr	O
set	B
seed	B
pls	O
fit	O
plsr	O
salary	O
data	B
hitters	B
subset	O
train	B
scale	O
true	O
validation	O
cv	O
summary	O
pls	O
fit	O
data	B
x	O
dimension	O
y	O
dimension	O
fit	O
method	O
kernelpls	O
number	O
of	O
components	O
considered	O
validatio	O
n	O
rmsep	O
cross	O
validated	O
using	O
random	O
segments	O
intercept	B
comps	O
comps	O
comps	O
comps	O
cv	O
adjcv	O
training	O
variance	B
explained	B
comps	O
comps	O
comps	O
comps	O
comps	O
comps	O
x	O
salary	O
v	O
a	O
l	O
i	O
d	O
a	O
t	O
i	O
o	O
n	O
p	O
l	O
o	O
t	O
pls	O
fit	O
val	O
type	O
msep	O
the	O
lowest	O
cross-validation	B
error	B
occurs	O
when	O
only	O
m	O
partial	B
least	B
squares	I
directions	O
are	O
used	O
we	O
now	O
evaluate	O
the	O
corresponding	O
test	O
set	B
mse	B
pls	O
pred	O
predict	O
pls	O
fit	O
x	O
test	O
ncomp	O
mean	O
pls	O
pred	O
y	O
test	O
the	O
test	O
mse	B
is	O
comparable	O
to	O
but	O
slightly	O
higher	O
than	O
the	O
test	O
mse	B
obtained	O
using	O
ridge	B
regression	B
the	O
lasso	B
and	O
pcr	O
finally	O
we	O
perform	O
pls	O
using	O
the	O
full	O
data	B
set	B
using	O
m	O
the	O
number	O
of	O
components	O
identified	O
by	O
cross-validation	B
pls	O
fit	O
plsr	O
salary	O
data	B
hitters	B
scale	O
true	O
ncomp	O
summary	O
pls	O
fit	O
data	B
x	O
dimension	O
y	O
dimension	O
fit	O
method	O
kernelpls	O
number	O
of	O
components	O
considered	O
training	O
variance	B
explained	B
x	O
salary	O
comps	O
comps	O
notice	O
that	O
the	O
percentage	O
of	O
variance	B
in	O
salary	O
that	O
the	O
two-component	O
pls	O
fit	O
explains	O
is	O
almost	O
as	O
much	O
as	O
that	O
explained	B
using	O
the	O
exercises	O
final	O
seven-component	O
model	B
pcr	O
fit	O
this	O
is	O
because	O
pcr	O
only	O
attempts	O
to	O
maximize	O
the	O
amount	O
of	O
variance	B
explained	B
in	O
the	O
predictors	O
while	O
pls	O
searches	O
for	O
directions	O
that	O
explain	O
variance	B
in	O
both	O
the	O
predictors	O
and	O
the	O
response	B
exercises	O
conceptual	O
we	O
perform	O
best	O
subset	O
forward	O
stepwise	O
and	O
backward	B
stepwise	I
selection	B
on	O
a	O
single	B
data	B
set	B
for	O
each	O
approach	B
we	O
obtain	O
p	O
models	O
containing	O
p	O
predictors	O
explain	O
your	O
answers	O
which	O
of	O
the	O
three	O
models	O
with	O
k	O
predictors	O
has	O
the	O
smallest	O
training	O
rss	O
which	O
of	O
the	O
three	O
models	O
with	O
k	O
predictors	O
has	O
the	O
smallest	O
test	O
rss	O
true	O
or	O
false	O
i	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identified	O
by	O
forward	O
stepwise	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
model	B
identified	O
by	O
forward	B
stepwise	I
selection	B
ii	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identified	O
by	O
backward	O
stepwise	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
model	B
identified	O
by	O
backward	B
stepwise	I
selection	B
iii	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identified	O
by	O
backward	O
stepwise	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
model	B
identified	O
by	O
forward	B
stepwise	I
selection	B
iv	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identified	O
by	O
forward	O
stepwise	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
model	B
identified	O
by	O
backward	B
stepwise	I
selection	B
v	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identified	O
by	O
best	O
subset	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
model	B
identified	O
by	O
best	B
subset	B
selection	B
for	O
parts	O
through	O
indicate	O
which	O
of	O
i	O
through	O
iv	O
is	O
correct	O
justify	O
your	O
answer	O
the	O
lasso	B
relative	O
to	O
least	B
squares	I
is	O
i	O
more	O
flexible	O
and	O
hence	O
will	O
give	O
improved	O
prediction	B
accuracy	O
when	O
its	O
increase	O
in	O
bias	B
is	O
less	O
than	O
its	O
decrease	O
in	O
variance	B
ii	O
more	O
flexible	O
and	O
hence	O
will	O
give	O
improved	O
prediction	B
accuracy	O
when	O
its	O
increase	O
in	O
variance	B
is	O
less	O
than	O
its	O
decrease	O
in	O
bias	B
linear	B
model	B
selection	B
and	O
regularization	B
iii	O
less	O
flexible	O
and	O
hence	O
will	O
give	O
improved	O
prediction	B
accuracy	O
when	O
its	O
increase	O
in	O
bias	B
is	O
less	O
than	O
its	O
decrease	O
in	O
variance	B
iv	O
less	O
flexible	O
and	O
hence	O
will	O
give	O
improved	O
prediction	B
accuracy	O
when	O
its	O
increase	O
in	O
variance	B
is	O
less	O
than	O
its	O
decrease	O
in	O
bias	B
repeat	O
for	O
ridge	B
regression	B
relative	O
to	O
least	B
squares	I
repeat	O
for	O
non-linear	B
methods	O
relative	O
to	O
least	B
squares	I
suppose	O
we	O
estimate	O
the	O
regression	B
coefficients	O
in	O
a	O
linear	B
regression	B
model	B
by	O
minimizing	O
yi	O
jxij	O
subject	O
to	O
j	O
s	O
for	O
a	O
particular	O
value	O
of	O
s	O
for	O
parts	O
through	O
indicate	O
which	O
of	O
i	O
through	O
v	O
is	O
correct	O
justify	O
your	O
answer	O
as	O
we	O
increase	O
s	O
from	O
the	O
training	O
rss	O
will	O
i	O
increase	O
initially	O
and	O
then	O
eventually	O
start	O
decreasing	O
in	O
an	O
inverted	O
u	O
shape	O
ii	O
decrease	O
initially	O
and	O
then	O
eventually	O
start	O
increasing	O
in	O
a	O
u	O
shape	O
iii	O
steadily	O
increase	O
iv	O
steadily	O
decrease	O
v	O
remain	O
constant	O
repeat	O
for	O
test	O
rss	O
repeat	O
for	O
variance	B
repeat	O
for	O
bias	B
repeat	O
for	O
the	O
irreducible	B
error	B
suppose	O
we	O
estimate	O
the	O
regression	B
coefficients	O
in	O
a	O
linear	B
regression	B
model	B
by	O
minimizing	O
yi	O
jxij	O
j	O
for	O
a	O
particular	O
value	O
of	O
for	O
parts	O
through	O
indicate	O
which	O
of	O
i	O
through	O
v	O
is	O
correct	O
justify	O
your	O
answer	O
exercises	O
as	O
we	O
increase	O
from	O
the	O
training	O
rss	O
will	O
i	O
increase	O
initially	O
and	O
then	O
eventually	O
start	O
decreasing	O
in	O
an	O
inverted	O
u	O
shape	O
ii	O
decrease	O
initially	O
and	O
then	O
eventually	O
start	O
increasing	O
in	O
a	O
u	O
shape	O
iii	O
steadily	O
increase	O
iv	O
steadily	O
decrease	O
v	O
remain	O
constant	O
repeat	O
for	O
test	O
rss	O
repeat	O
for	O
variance	B
repeat	O
for	O
bias	B
repeat	O
for	O
the	O
irreducible	B
error	B
it	O
is	O
well-known	O
that	O
ridge	B
regression	B
tends	O
to	O
give	O
similar	O
coefficient	O
values	O
to	O
correlated	O
variables	O
whereas	O
the	O
lasso	B
may	O
give	O
quite	O
different	O
coefficient	O
values	O
to	O
correlated	O
variables	O
we	O
will	O
now	O
explore	O
this	O
property	O
in	O
a	O
very	O
simple	B
setting	O
suppose	O
that	O
n	O
p	O
furthermore	O
suppose	O
that	O
and	O
and	O
so	O
that	O
the	O
estimate	O
for	O
the	O
intercept	B
in	O
a	O
least	B
squares	I
ridge	B
regression	B
or	O
lasso	B
model	B
is	O
zero	O
write	O
out	O
the	O
ridge	B
regression	B
optimization	O
problem	O
in	O
this	O
set	B
ting	O
argue	O
that	O
in	O
this	O
setting	O
the	O
ridge	O
coefficient	O
estimates	O
satisfy	O
write	O
out	O
the	O
lasso	B
optimization	O
problem	O
in	O
this	O
setting	O
argue	O
that	O
in	O
this	O
setting	O
the	O
lasso	B
coefficients	O
and	O
are	O
not	O
unique	O
in	O
other	O
words	O
there	O
are	O
many	O
possible	O
solutions	O
to	O
the	O
optimization	O
problem	O
in	O
describe	O
these	O
solutions	O
we	O
will	O
now	O
explore	O
and	O
further	O
consider	O
with	O
p	O
for	O
some	O
choice	O
of	O
and	O
plot	B
as	O
a	O
function	B
of	O
your	O
plot	B
should	O
confirm	O
that	O
is	O
solved	O
by	O
consider	O
with	O
p	O
for	O
some	O
choice	O
of	O
and	O
plot	B
as	O
a	O
function	B
of	O
your	O
plot	B
should	O
confirm	O
that	O
is	O
solved	O
by	O
linear	B
model	B
selection	B
and	O
regularization	B
we	O
will	O
now	O
derive	O
the	O
bayesian	B
connection	O
to	O
the	O
lasso	B
and	O
ridge	B
regression	B
discussed	O
in	O
section	O
suppose	O
that	O
yi	O
p	O
xij	O
j	O
where	O
are	O
independent	B
and	O
identically	O
distributed	O
from	O
a	O
n	O
distribution	B
write	O
out	O
the	O
likelihood	O
for	O
the	O
data	B
assume	O
the	O
following	O
prior	O
for	O
p	O
are	O
independent	B
and	O
identically	O
distributed	O
according	O
to	O
a	O
double-exponential	O
distribution	B
with	O
mean	O
and	O
common	O
scale	O
parameter	B
b	O
i	O
e	O
p	O
setting	O
exp	O
write	O
out	O
the	O
posterior	O
for	O
in	O
this	O
argue	O
that	O
the	O
lasso	B
estimate	O
is	O
the	O
mode	B
for	O
under	O
this	O
pos	O
terior	O
distribution	B
now	O
assume	O
the	O
following	O
prior	O
for	O
p	O
are	O
independent	B
and	O
identically	O
distributed	O
according	O
to	O
a	O
normal	O
distribution	B
with	O
mean	O
zero	O
and	O
variance	B
c	O
write	O
out	O
the	O
posterior	O
for	O
in	O
this	O
setting	O
argue	O
that	O
the	O
ridge	B
regression	B
estimate	O
is	O
both	O
the	O
mode	B
and	O
the	O
mean	O
for	O
under	O
this	O
posterior	O
distribution	B
applied	O
in	O
this	O
exercise	O
we	O
will	O
generate	O
simulated	O
data	B
and	O
will	O
then	O
use	O
this	O
data	B
to	O
perform	O
best	B
subset	B
selection	B
use	O
the	O
rnorm	O
function	B
to	O
generate	O
a	O
predictor	B
x	O
of	O
length	O
n	O
as	O
well	O
as	O
a	O
noise	B
vector	B
of	O
length	O
n	O
generate	O
a	O
response	B
vector	B
y	O
of	O
length	O
n	O
according	O
to	O
the	O
model	B
y	O
where	O
and	O
are	O
constants	O
of	O
your	O
choice	O
use	O
the	O
regsubsets	O
function	B
to	O
perform	O
best	B
subset	B
selection	B
in	O
order	O
to	O
choose	O
the	O
best	O
model	B
containing	O
the	O
predictors	O
x	O
x	O
x	O
what	O
is	O
the	O
best	O
model	B
obtained	O
according	O
to	O
cp	B
bic	O
and	O
adjusted	O
show	O
some	O
plots	O
to	O
provide	O
evidence	O
for	O
your	O
answer	O
and	O
report	O
the	O
coefficients	O
of	O
the	O
best	O
model	B
obtained	O
note	O
you	O
will	O
need	O
to	O
use	O
the	O
data	B
frame	I
function	B
to	O
create	O
a	O
single	B
data	B
set	B
containing	O
both	O
x	O
and	O
y	O
exercises	O
repeat	O
using	O
forward	B
stepwise	I
selection	B
and	O
also	O
using	O
backwards	O
stepwise	O
selection	B
how	O
does	O
your	O
answer	O
compare	O
to	O
the	O
results	O
in	O
now	O
fit	O
a	O
lasso	B
model	B
to	O
the	O
simulated	O
data	B
again	O
using	O
x	O
x	O
x	O
as	O
predictors	O
use	O
cross-validation	B
to	O
select	O
the	O
optimal	O
value	O
of	O
create	O
plots	O
of	O
the	O
cross-validation	B
error	B
as	O
a	O
function	B
of	O
report	O
the	O
resulting	O
coefficient	O
estimates	O
and	O
discuss	O
the	O
results	O
obtained	O
now	O
generate	O
a	O
response	B
vector	B
y	O
according	O
to	O
the	O
model	B
y	O
and	O
perform	O
best	B
subset	B
selection	B
and	O
the	O
lasso	B
discuss	O
the	O
results	O
obtained	O
in	O
this	O
exercise	O
we	O
will	O
predict	O
the	O
number	O
of	O
applications	O
received	O
using	O
the	O
other	O
variables	O
in	O
the	O
college	B
data	B
set	B
split	O
the	O
data	B
set	B
into	O
a	O
training	O
set	B
and	O
a	O
test	O
set	B
fit	O
a	O
linear	B
model	B
using	O
least	B
squares	I
on	O
the	O
training	O
set	B
and	O
report	O
the	O
test	O
error	B
obtained	O
fit	O
a	O
ridge	B
regression	B
model	B
on	O
the	O
training	O
set	B
with	O
chosen	O
by	O
cross-validation	B
report	O
the	O
test	O
error	B
obtained	O
fit	O
a	O
lasso	B
model	B
on	O
the	O
training	O
set	B
with	O
chosen	O
by	O
crossvalidation	O
report	O
the	O
test	O
error	B
obtained	O
along	O
with	O
the	O
number	O
of	O
non-zero	O
coefficient	O
estimates	O
fit	O
a	O
pcr	O
model	B
on	O
the	O
training	O
set	B
with	O
m	O
chosen	O
by	O
crossvalidation	O
report	O
the	O
test	O
error	B
obtained	O
along	O
with	O
the	O
value	O
of	O
m	O
selected	O
by	O
cross-validation	B
fit	O
a	O
pls	O
model	B
on	O
the	O
training	O
set	B
with	O
m	O
chosen	O
by	O
crossvalidation	O
report	O
the	O
test	O
error	B
obtained	O
along	O
with	O
the	O
value	O
of	O
m	O
selected	O
by	O
cross-validation	B
comment	O
on	O
the	O
results	O
obtained	O
how	O
accurately	O
can	O
we	O
predict	O
the	O
number	O
of	O
college	B
applications	O
received	O
is	O
there	O
much	O
difference	O
among	O
the	O
test	O
errors	O
resulting	O
from	O
these	O
five	O
approaches	O
we	O
have	O
seen	O
that	O
as	O
the	O
number	O
of	O
features	O
used	O
in	O
a	O
model	B
increases	O
the	O
training	O
error	B
will	O
necessarily	O
decrease	O
but	O
the	O
test	O
error	B
may	O
not	O
we	O
will	O
now	O
explore	O
this	O
in	O
a	O
simulated	O
data	B
set	B
generate	O
a	O
data	B
set	B
with	O
p	O
features	O
n	O
observations	B
and	O
an	O
associated	O
quantitative	B
response	B
vector	B
generated	O
according	O
to	O
the	O
model	B
where	O
has	O
some	O
elements	O
that	O
are	O
exactly	O
equal	O
to	O
zero	O
y	O
x	O
linear	B
model	B
selection	B
and	O
regularization	B
split	O
your	O
data	B
set	B
into	O
a	O
training	O
set	B
containing	O
observations	B
and	O
a	O
test	O
set	B
containing	O
observations	B
perform	O
best	B
subset	B
selection	B
on	O
the	O
training	O
set	B
and	O
plot	B
the	O
training	O
set	B
mse	B
associated	O
with	O
the	O
best	O
model	B
of	O
each	O
size	O
plot	B
the	O
test	O
set	B
mse	B
associated	O
with	O
the	O
best	O
model	B
of	O
each	O
size	O
for	O
which	O
model	B
size	O
does	O
the	O
test	O
set	B
mse	B
take	O
on	O
its	O
minimum	O
value	O
comment	O
on	O
your	O
results	O
if	O
it	O
takes	O
on	O
its	O
minimum	O
value	O
for	O
a	O
model	B
containing	O
only	O
an	O
intercept	B
or	O
a	O
model	B
containing	O
all	O
of	O
the	O
features	O
then	O
play	O
around	O
with	O
the	O
way	O
that	O
you	O
are	O
generating	O
the	O
data	B
in	O
until	O
you	O
come	O
up	O
with	O
a	O
scenario	O
in	O
which	O
the	O
test	O
set	B
mse	B
is	O
minimized	O
for	O
an	O
intermediate	O
model	B
size	O
how	O
does	O
the	O
model	B
at	O
which	O
the	O
test	O
set	B
mse	B
is	O
minimized	O
compare	O
to	O
the	O
true	O
model	B
used	O
to	O
generate	O
the	O
data	B
comment	O
on	O
the	O
coefficient	O
values	O
create	O
a	O
plot	B
displaying	O
j	O
for	O
a	O
range	O
of	O
values	O
of	O
r	O
where	O
r	O
j	O
is	O
the	O
jth	O
coefficient	O
estimate	O
for	O
the	O
best	O
model	B
containing	O
r	O
coefficients	O
comment	O
on	O
what	O
you	O
observe	O
how	O
does	O
this	O
compare	O
to	O
the	O
test	O
mse	B
plot	B
from	O
p	O
j	O
r	O
we	O
will	O
now	O
try	O
to	O
predict	O
per	O
capita	O
crime	O
rate	B
in	O
the	O
boston	B
data	B
set	B
try	O
out	O
some	O
of	O
the	O
regression	B
methods	O
explored	O
in	O
this	O
chapter	O
such	O
as	O
best	B
subset	B
selection	B
the	O
lasso	B
ridge	B
regression	B
and	O
pcr	O
present	O
and	O
discuss	O
results	O
for	O
the	O
approaches	O
that	O
you	O
consider	O
propose	O
a	O
model	B
set	B
of	O
models	O
that	O
seem	O
to	O
perform	O
well	O
on	O
this	O
data	B
set	B
and	O
justify	O
your	O
answer	O
make	O
sure	O
that	O
you	O
are	O
evaluating	O
model	B
performance	O
using	O
validation	B
set	B
error	B
crossvalidation	O
or	O
some	O
other	O
reasonable	O
alternative	O
as	O
opposed	O
to	O
using	O
training	O
error	B
does	O
your	O
chosen	O
model	B
involve	O
all	O
of	O
the	O
features	O
in	O
the	O
data	B
set	B
why	O
or	O
why	O
not	O
moving	O
beyond	O
linearity	O
so	O
far	O
in	O
this	O
book	O
we	O
have	O
mostly	O
focused	O
on	O
linear	B
models	O
linear	B
models	O
are	O
relatively	O
simple	B
to	O
describe	O
and	O
implement	O
and	O
have	O
advantages	O
over	O
other	O
approaches	O
in	O
terms	O
of	O
interpretation	O
and	O
inference	B
however	O
standard	O
linear	B
regression	B
can	O
have	O
significant	O
limitations	O
in	O
terms	O
of	O
predictive	O
power	B
this	O
is	O
because	O
the	O
linearity	O
assumption	O
is	O
almost	O
always	O
an	O
approximation	O
and	O
sometimes	O
a	O
poor	O
one	O
in	O
chapter	O
we	O
see	O
that	O
we	O
can	O
improve	O
upon	O
least	B
squares	I
using	O
ridge	B
regression	B
the	O
lasso	B
principal	B
components	I
regression	B
and	O
other	O
techniques	O
in	O
that	O
setting	O
the	O
improvement	O
is	O
obtained	O
by	O
reducing	O
the	O
complexity	O
of	O
the	O
linear	B
model	B
and	O
hence	O
the	O
variance	B
of	O
the	O
estimates	O
but	O
we	O
are	O
still	O
using	O
a	O
linear	B
model	B
which	O
can	O
only	O
be	O
improved	O
so	O
far	O
in	O
this	O
chapter	O
we	O
relax	O
the	O
linearity	O
assumption	O
while	O
still	O
attempting	O
to	O
maintain	O
as	O
much	O
interpretability	B
as	O
possible	O
we	O
do	O
this	O
by	O
examining	O
very	O
simple	B
extensions	O
of	O
linear	B
models	O
like	O
polynomial	B
regression	B
and	O
step	O
functions	O
as	O
well	O
as	O
more	O
sophisticated	O
approaches	O
such	O
as	O
splines	O
local	B
regression	B
and	O
generalized	O
additive	B
models	O
polynomial	B
regression	B
extends	O
the	O
linear	B
model	B
by	O
adding	O
extra	O
predictors	O
obtained	O
by	O
raising	O
each	O
of	O
the	O
original	O
predictors	O
to	O
a	O
power	B
for	O
example	O
a	O
cubic	B
regression	B
uses	O
three	O
variables	O
x	O
x	O
and	O
x	O
as	O
predictors	O
this	O
approach	B
provides	O
a	O
simple	B
way	O
to	O
provide	O
a	O
nonlinear	O
fit	O
to	O
data	B
step	O
functions	O
cut	O
the	O
range	O
of	O
a	O
variable	B
into	O
k	O
distinct	O
regions	O
in	O
order	O
to	O
produce	O
a	O
qualitative	B
variable	B
this	O
has	O
the	O
effect	O
of	O
fitting	O
a	O
piecewise	O
constant	O
function	B
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
moving	O
beyond	O
linearity	O
regression	B
splines	O
are	O
more	O
flexible	O
than	O
polynomials	O
and	O
step	O
functions	O
and	O
in	O
fact	O
are	O
an	O
extension	O
of	O
the	O
two	O
they	O
involve	O
dividing	O
the	O
range	O
of	O
x	O
into	O
k	O
distinct	O
regions	O
within	O
each	O
region	O
a	O
polynomial	B
function	B
is	O
fit	O
to	O
the	O
data	B
however	O
these	O
polynomials	O
are	O
constrained	O
so	O
that	O
they	O
join	O
smoothly	O
at	O
the	O
region	O
boundaries	O
or	O
knots	O
provided	O
that	O
the	O
interval	B
is	O
divided	O
into	O
enough	O
regions	O
this	O
can	O
produce	O
an	O
extremely	O
flexible	O
fit	O
smoothing	B
splines	O
are	O
similar	O
to	O
regression	B
splines	O
but	O
arise	O
in	O
a	O
slightly	O
different	O
situation	O
smoothing	B
splines	O
result	O
from	O
minimizing	O
a	O
residual	B
sum	B
of	I
squares	I
criterion	O
subject	O
to	O
a	O
smoothness	O
penalty	B
local	B
regression	B
is	O
similar	O
to	O
splines	O
but	O
differs	O
in	O
an	O
important	O
way	O
the	O
regions	O
are	O
allowed	O
to	O
overlap	O
and	O
indeed	O
they	O
do	O
so	O
in	O
a	O
very	O
smooth	O
way	O
generalized	O
additive	B
models	O
allow	O
us	O
to	O
extend	O
the	O
methods	O
above	O
to	O
deal	O
with	O
multiple	B
predictors	O
in	O
sections	O
we	O
present	O
a	O
number	O
of	O
approaches	O
for	O
modeling	O
the	O
relationship	O
between	O
a	O
response	B
y	O
and	O
a	O
single	B
predictor	B
x	O
in	O
a	O
flexible	O
way	O
in	O
section	O
we	O
show	O
that	O
these	O
approaches	O
can	O
be	O
seamlessly	O
integrated	O
in	O
order	O
to	O
model	B
a	O
response	B
y	O
as	O
a	O
function	B
of	O
several	O
predictors	O
xp	O
polynomial	B
regression	B
historically	O
the	O
standard	O
way	O
to	O
extend	O
linear	B
regression	B
to	O
settings	O
in	O
which	O
the	O
relationship	O
between	O
the	O
predictors	O
and	O
the	O
response	B
is	O
nonlinear	O
has	O
been	O
to	O
replace	O
the	O
standard	O
linear	B
model	B
yi	O
with	O
a	O
polynomial	B
function	B
yi	O
i	O
i	O
dxd	O
i	O
where	O
is	O
the	O
error	B
term	B
this	O
approach	B
is	O
known	O
as	O
polynomial	B
regression	B
and	O
in	O
fact	O
we	O
saw	O
an	O
example	O
of	O
this	O
method	O
in	O
section	O
for	O
large	O
enough	O
degree	O
d	O
a	O
polynomial	B
regression	B
allows	O
us	O
to	O
produce	O
an	O
extremely	O
non-linear	B
curve	O
notice	O
that	O
the	O
coefficients	O
in	O
can	O
be	O
easily	O
estimated	O
using	O
least	B
squares	I
linear	B
regression	B
because	O
this	O
is	O
just	O
a	O
standard	O
linear	B
model	B
with	O
predictors	O
xi	O
i	O
generally	O
speaking	O
it	O
is	O
unusual	O
to	O
use	O
d	O
greater	O
than	O
or	O
because	O
for	O
large	O
values	O
of	O
d	O
the	O
polynomial	B
curve	O
can	O
become	O
overly	O
flexible	O
and	O
can	O
take	O
on	O
some	O
very	O
strange	O
shapes	O
this	O
is	O
especially	O
true	O
near	O
the	O
boundary	O
of	O
the	O
x	O
variable	B
i	O
xd	O
i	O
polynomial	B
regression	B
polynomial	B
regression	B
degree	O
polynomial	B
e	O
g	O
a	O
e	O
g	O
a	O
w	O
r	O
p	O
e	O
g	O
a	O
w	O
age	O
age	O
figure	O
the	O
wage	B
data	B
left	O
the	O
solid	O
blue	O
curve	O
is	O
a	O
polynomial	B
of	O
wage	B
thousands	O
of	O
dollars	O
as	O
a	O
function	B
of	O
age	O
fit	O
by	O
least	B
squares	I
the	O
dotted	O
curves	O
indicate	O
an	O
estimated	O
confidence	B
interval	B
right	O
we	O
model	B
the	O
binary	B
event	O
using	O
logistic	B
regression	B
again	O
with	O
a	O
polynomial	B
the	O
fitted	O
posterior	O
probability	B
of	O
wage	B
exceeding	O
is	O
shown	O
in	O
blue	O
along	O
with	O
an	O
estimated	O
confidence	B
interval	B
the	O
left-hand	O
panel	O
in	O
figure	O
is	O
a	O
plot	B
of	O
wage	B
against	O
age	O
for	O
the	O
wage	B
data	B
set	B
which	O
contains	O
income	B
and	O
demographic	O
information	O
for	O
males	O
who	O
reside	O
in	O
the	O
central	O
atlantic	O
region	O
of	O
the	O
united	O
states	O
we	O
see	O
the	O
results	O
of	O
fitting	O
a	O
polynomial	B
using	O
least	B
squares	I
blue	O
curve	O
even	O
though	O
this	O
is	O
a	O
linear	B
regression	B
model	B
like	O
any	O
other	O
the	O
individual	O
coefficients	O
are	O
not	O
of	O
particular	O
interest	O
instead	O
we	O
look	O
at	O
the	O
entire	O
fitted	O
function	B
across	O
a	O
grid	O
of	O
values	O
for	O
age	O
from	O
to	O
in	O
order	O
to	O
understand	O
the	O
relationship	O
between	O
age	O
and	O
wage	B
in	O
figure	O
a	O
pair	O
of	O
dotted	O
curves	O
accompanies	O
the	O
fit	O
these	O
are	O
standard	B
error	B
curves	O
let	O
s	O
see	O
how	O
these	O
arise	O
suppose	O
we	O
have	O
computed	O
the	O
fit	O
at	O
a	O
particular	O
value	O
of	O
age	O
f	O
what	O
is	O
the	O
variance	B
of	O
the	O
fit	O
i	O
e	O
var	O
f	O
least	B
squares	I
returns	O
variance	B
estimates	O
for	O
each	O
of	O
the	O
fitted	O
coefficients	O
j	O
as	O
well	O
as	O
the	O
covariances	O
between	O
pairs	O
of	O
coefficient	O
estimates	O
we	O
can	O
use	O
these	O
to	O
compute	O
the	O
estimated	O
variance	B
of	O
f	O
the	O
estimated	O
pointwise	O
standard	B
error	B
of	O
f	O
is	O
the	O
square-root	O
of	O
this	O
variance	B
this	O
computation	O
is	O
repeated	O
c	O
is	O
the	O
covariance	O
matrix	O
of	O
the	O
j	O
and	O
if	O
then	O
var	O
f	O
moving	O
beyond	O
linearity	O
at	O
each	O
reference	O
point	O
and	O
we	O
plot	B
the	O
fitted	O
curve	O
as	O
well	O
as	O
twice	O
the	O
standard	B
error	B
on	O
either	O
side	O
of	O
the	O
fitted	O
curve	O
we	O
plot	B
twice	O
the	O
standard	B
error	B
because	O
for	O
normally	O
distributed	O
error	B
terms	O
this	O
quantity	O
corresponds	O
to	O
an	O
approximate	O
confidence	B
interval	B
it	O
seems	O
like	O
the	O
wages	O
in	O
figure	O
are	O
from	O
two	O
distinct	O
populations	O
there	O
appears	O
to	O
be	O
a	O
high	O
earners	O
group	O
earning	O
more	O
than	O
per	O
annum	O
as	O
well	O
as	O
a	O
low	O
earners	O
group	O
we	O
can	O
treat	O
wage	B
as	O
a	O
binary	B
variable	B
by	O
splitting	O
it	O
into	O
these	O
two	O
groups	O
logistic	B
regression	B
can	O
then	O
be	O
used	O
to	O
predict	O
this	O
binary	B
response	B
using	O
polynomial	B
functions	O
of	O
age	O
as	O
predictors	O
in	O
other	O
words	O
we	O
fit	O
the	O
model	B
pryi	O
exp	O
exp	O
i	O
dxd	O
i	O
i	O
dxd	O
i	O
the	O
result	O
is	O
shown	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
the	O
gray	O
marks	O
on	O
the	O
top	O
and	O
bottom	O
of	O
the	O
panel	O
indicate	O
the	O
ages	O
of	O
the	O
high	O
earners	O
and	O
the	O
low	O
earners	O
the	O
solid	O
blue	O
curve	O
indicates	O
the	O
fitted	O
probabilities	O
of	O
being	O
a	O
high	O
earner	O
as	O
a	O
function	B
of	O
age	O
the	O
estimated	O
confidence	B
interval	B
is	O
shown	O
as	O
well	O
we	O
see	O
that	O
here	O
the	O
confidence	O
intervals	O
are	O
fairly	O
wide	O
especially	O
on	O
the	O
right-hand	O
side	O
although	O
the	O
sample	O
size	O
for	O
this	O
data	B
set	B
is	O
substantial	O
there	O
are	O
only	O
high	O
earners	O
which	O
results	O
in	O
a	O
high	O
variance	B
in	O
the	O
estimated	O
coefficients	O
and	O
consequently	O
wide	O
confidence	O
intervals	O
step	O
functions	O
using	O
polynomial	B
functions	O
of	O
the	O
features	O
as	O
predictors	O
in	O
a	O
linear	B
model	B
imposes	O
a	O
global	O
structure	O
on	O
the	O
non-linear	B
function	B
of	O
x	O
we	O
can	O
instead	O
use	O
step	O
functions	O
in	O
order	O
to	O
avoid	O
imposing	O
such	O
a	O
global	O
structure	O
here	O
we	O
break	O
the	O
range	O
of	O
x	O
into	O
bins	O
and	O
fit	O
a	O
different	O
constant	O
in	O
each	O
bin	O
this	O
amounts	O
to	O
converting	O
a	O
continuous	B
variable	B
into	O
an	O
ordered	B
categorical	B
variable	B
in	O
greater	O
detail	O
we	O
create	O
cutpoints	O
ck	O
in	O
the	O
range	O
of	O
x	O
and	O
then	O
construct	O
k	O
new	O
variables	O
step	B
function	B
ordered	B
categorical	B
variable	B
ix	O
x	O
x	O
ick	O
x	O
ck	O
ick	O
x	O
ck	O
ckx	O
where	O
i	O
is	O
an	O
indicator	B
function	B
that	O
returns	O
a	O
if	O
the	O
condition	O
is	O
true	O
and	O
returns	O
a	O
otherwise	O
for	O
example	O
ick	O
x	O
equals	O
if	O
ck	O
x	O
and	O
indicator	B
function	B
e	O
g	O
a	O
w	O
step	O
functions	O
piecewise	O
constant	O
e	O
g	O
a	O
e	O
g	O
a	O
w	O
r	O
p	O
age	O
age	O
figure	O
the	O
wage	B
data	B
left	O
the	O
solid	O
curve	O
displays	O
the	O
fitted	O
value	O
from	O
a	O
least	B
squares	I
regression	B
of	O
wage	B
thousands	O
of	O
dollars	O
using	O
step	O
functions	O
of	O
age	O
the	O
dotted	O
curves	O
indicate	O
an	O
estimated	O
confidence	B
interval	B
right	O
we	O
model	B
the	O
binary	B
event	O
using	O
logistic	B
regression	B
again	O
using	O
step	O
functions	O
of	O
age	O
the	O
fitted	O
posterior	O
probability	B
of	O
wage	B
exceeding	O
is	O
shown	O
along	O
with	O
an	O
estimated	O
confidence	B
interval	B
equals	O
otherwise	O
these	O
are	O
sometimes	O
called	O
dummy	B
variables	O
notice	O
that	O
for	O
any	O
value	O
of	O
x	O
ck	O
since	O
x	O
must	O
be	O
in	O
exactly	O
one	O
of	O
the	O
k	O
intervals	O
we	O
then	O
use	O
least	B
squares	I
to	O
fit	O
a	O
linear	B
model	B
using	O
ck	O
as	O
yi	O
kckxi	O
for	O
a	O
given	O
value	O
of	O
x	O
at	O
most	O
one	O
of	O
ck	O
can	O
be	O
non-zero	O
note	O
that	O
when	O
x	O
all	O
of	O
the	O
predictors	O
in	O
are	O
zero	O
so	O
can	O
be	O
interpreted	O
as	O
the	O
mean	O
value	O
of	O
y	O
for	O
x	O
by	O
comparison	O
predicts	O
a	O
response	B
of	O
j	O
for	O
cj	O
x	O
so	O
j	O
represents	O
the	O
average	B
increase	O
in	O
the	O
response	B
for	O
x	O
in	O
cj	O
x	O
relative	O
to	O
x	O
an	O
example	O
of	O
fitting	O
step	O
functions	O
to	O
the	O
wage	B
data	B
from	O
figure	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
we	O
also	O
fit	O
the	O
logistic	B
regression	B
model	B
exclude	O
as	O
a	O
predictor	B
in	O
because	O
it	O
is	O
redundant	O
with	O
the	O
intercept	B
this	O
is	O
similar	O
to	O
the	O
fact	O
that	O
we	O
need	O
only	O
two	O
dummy	B
variables	O
to	O
code	O
a	O
qualitative	B
variable	B
with	O
three	O
levels	O
provided	O
that	O
the	O
model	B
will	O
contain	O
an	O
intercept	B
the	O
decision	O
to	O
exclude	O
instead	O
of	O
some	O
other	O
ckx	O
in	O
is	O
arbitrary	O
alternatively	O
we	O
could	O
include	O
ck	O
and	O
exclude	O
the	O
intercept	B
moving	O
beyond	O
linearity	O
pryi	O
exp	O
kck	O
exp	O
kckxi	O
in	O
order	O
to	O
predict	O
the	O
probability	B
that	O
an	O
individual	O
is	O
a	O
high	O
earner	O
on	O
the	O
basis	B
of	O
age	O
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
the	O
fitted	O
posterior	O
probabilities	O
obtained	O
using	O
this	O
approach	B
unfortunately	O
unless	O
there	O
are	O
natural	B
breakpoints	O
in	O
the	O
predictors	O
piecewise-constant	O
functions	O
can	O
miss	O
the	O
action	O
for	O
example	O
in	O
the	O
lefthand	O
panel	O
of	O
figure	O
the	O
first	O
bin	O
clearly	O
misses	O
the	O
increasing	O
trend	O
of	O
wage	B
with	O
age	O
nevertheless	O
step	B
function	B
approaches	O
are	O
very	O
popular	O
in	O
biostatistics	O
and	O
epidemiology	O
among	O
other	O
disciplines	O
for	O
example	O
age	O
groups	O
are	O
often	O
used	O
to	O
define	O
the	O
bins	O
basis	B
functions	O
polynomial	B
and	O
piecewise-constant	O
regression	B
models	O
are	O
in	O
fact	O
special	O
cases	O
of	O
a	O
basis	B
function	B
approach	B
the	O
idea	O
is	O
to	O
have	O
at	O
hand	O
a	O
family	O
of	O
functions	O
or	O
transformations	O
that	O
can	O
be	O
applied	O
to	O
a	O
variable	B
x	O
bkx	O
instead	O
of	O
fitting	O
a	O
linear	B
model	B
in	O
x	O
we	O
fit	O
the	O
model	B
basis	B
function	B
yi	O
kbkxi	O
note	O
that	O
the	O
basis	B
functions	O
bk	O
are	O
fixed	O
and	O
known	O
other	O
words	O
we	O
choose	O
the	O
functions	O
ahead	O
of	O
time	O
for	O
polynomial	B
regression	B
the	O
basis	B
functions	O
are	O
bjxi	O
xj	O
i	O
and	O
for	O
piecewise	O
constant	O
functions	O
they	O
are	O
bjxi	O
icj	O
xi	O
we	O
can	O
think	O
of	O
as	O
a	O
standard	O
linear	B
model	B
with	O
predictors	O
bkxi	O
hence	O
we	O
can	O
use	O
least	B
squares	I
to	O
estimate	O
the	O
unknown	O
regression	B
coefficients	O
in	O
importantly	O
this	O
means	O
that	O
all	O
of	O
the	O
inference	B
tools	O
for	O
linear	B
models	O
that	O
are	O
discussed	O
in	O
chapter	O
such	O
as	O
standard	O
errors	O
for	O
the	O
coefficient	O
estimates	O
and	O
f-statistics	O
for	O
the	O
model	B
s	O
overall	O
significance	O
are	O
available	O
in	O
this	O
setting	O
thus	O
far	O
we	O
have	O
considered	O
the	O
use	O
of	O
polynomial	B
functions	O
and	O
piecewise	O
constant	O
functions	O
for	O
our	O
basis	B
functions	O
however	O
many	O
alternatives	O
are	O
possible	O
for	O
instance	O
we	O
can	O
use	O
wavelets	O
or	O
fourier	O
series	O
to	O
construct	O
basis	B
functions	O
in	O
the	O
next	O
section	O
we	O
investigate	O
a	O
very	O
common	O
choice	O
for	O
a	O
basis	B
function	B
regression	B
splines	O
regression	B
spline	B
regression	B
splines	O
regression	B
splines	O
now	O
we	O
discuss	O
a	O
flexible	O
class	O
of	O
basis	B
functions	O
that	O
extends	O
upon	O
the	O
polynomial	B
regression	B
and	O
piecewise	O
constant	O
regression	B
approaches	O
that	O
we	O
have	O
just	O
seen	O
piecewise	O
polynomials	O
instead	O
of	O
fitting	O
a	O
high-degree	O
polynomial	B
over	O
the	O
entire	O
range	O
of	O
x	O
piecewise	B
polynomial	B
regression	B
involves	O
fitting	O
separate	O
low-degree	O
polynomials	O
over	O
different	O
regions	O
of	O
x	O
for	O
example	O
a	O
piecewise	O
cubic	B
polynomial	B
works	O
by	O
fitting	O
a	O
cubic	B
regression	B
model	B
of	O
the	O
form	O
piecewise	B
polynomial	B
regression	B
yi	O
i	O
i	O
where	O
the	O
coefficients	O
and	O
differ	O
in	O
different	O
parts	O
of	O
the	O
range	O
of	O
x	O
the	O
points	O
where	O
the	O
coefficients	O
change	O
are	O
called	O
knots	O
for	O
example	O
a	O
piecewise	O
cubic	B
with	O
no	O
knots	O
is	O
just	O
a	O
standard	O
cubic	B
polynomial	B
as	O
in	O
with	O
d	O
a	O
piecewise	O
cubic	B
polynomial	B
with	O
a	O
single	B
knot	B
at	O
a	O
point	O
c	O
takes	O
the	O
form	O
knot	B
yi	O
i	O
i	O
i	O
i	O
if	O
xi	O
c	O
if	O
xi	O
c	O
in	O
other	O
words	O
we	O
fit	O
two	O
different	O
polynomial	B
functions	O
to	O
the	O
data	B
one	O
on	O
the	O
subset	O
of	O
the	O
observations	B
with	O
xi	O
c	O
and	O
one	O
on	O
the	O
subset	O
of	O
the	O
observations	B
with	O
xi	O
c	O
the	O
first	O
polynomial	B
function	B
has	O
coefficients	O
and	O
the	O
second	O
has	O
coefficients	O
each	O
of	O
these	O
polynomial	B
functions	O
can	O
be	O
fit	O
using	O
least	B
squares	I
applied	O
to	O
simple	B
functions	O
of	O
the	O
original	O
predictor	B
using	O
more	O
knots	O
leads	O
to	O
a	O
more	O
flexible	O
piecewise	B
polynomial	B
in	O
general	O
if	O
we	O
place	O
k	O
different	O
knots	O
throughout	O
the	O
range	O
of	O
x	O
then	O
we	O
will	O
end	O
up	O
fitting	O
k	O
different	O
cubic	B
polynomials	O
note	O
that	O
we	O
do	O
not	O
need	O
to	O
use	O
a	O
cubic	B
polynomial	B
for	O
example	O
we	O
can	O
instead	O
fit	O
piecewise	O
linear	B
functions	O
in	O
fact	O
our	O
piecewise	O
constant	O
functions	O
of	O
section	O
are	O
piecewise	O
polynomials	O
of	O
degree	O
the	O
top	O
left	O
panel	O
of	O
figure	O
shows	O
a	O
piecewise	O
cubic	B
polynomial	B
fit	O
to	O
a	O
subset	O
of	O
the	O
wage	B
data	B
with	O
a	O
single	B
knot	B
at	O
we	O
immediately	O
see	O
a	O
problem	O
the	O
function	B
is	O
discontinuous	O
and	O
looks	O
ridiculous	O
since	O
each	O
polynomial	B
has	O
four	O
parameters	O
we	O
are	O
using	O
a	O
total	O
of	O
eight	O
degrees	B
of	I
freedom	I
in	O
fitting	O
this	O
piecewise	B
polynomial	B
model	B
constraints	O
and	O
splines	O
the	O
top	O
left	O
panel	O
of	O
figure	O
looks	O
wrong	O
because	O
the	O
fitted	O
curve	O
is	O
just	O
too	O
flexible	O
to	O
remedy	O
this	O
problem	O
we	O
can	O
fit	O
a	O
piecewise	B
polynomial	B
degrees	B
of	I
freedom	I
moving	O
beyond	O
linearity	O
piecewise	O
cubic	B
continuous	B
piecewise	O
cubic	B
e	O
g	O
a	O
w	O
e	O
g	O
a	O
w	O
e	O
g	O
a	O
w	O
age	O
cubic	B
spline	B
age	O
linear	B
spline	B
e	O
g	O
a	O
w	O
age	O
age	O
figure	O
various	O
piecewise	O
polynomials	O
are	O
fit	O
to	O
a	O
subset	O
of	O
the	O
wage	B
data	B
with	O
a	O
knot	B
at	O
top	O
left	O
the	O
cubic	B
polynomials	O
are	O
unconstrained	O
top	O
right	O
the	O
cubic	B
polynomials	O
are	O
constrained	O
to	O
be	O
continuous	B
at	O
bottom	O
left	O
the	O
cubic	B
polynomials	O
are	O
constrained	O
to	O
be	O
continuous	B
and	O
to	O
have	O
continuous	B
first	O
and	O
second	O
derivatives	O
bottom	O
right	O
a	O
linear	B
spline	B
is	O
shown	O
which	O
is	O
constrained	O
to	O
be	O
continuous	B
under	O
the	O
constraint	O
that	O
the	O
fitted	O
curve	O
must	O
be	O
continuous	B
in	O
other	O
words	O
there	O
cannot	O
be	O
a	O
jump	O
when	O
the	O
top	O
right	O
plot	B
in	O
figure	O
shows	O
the	O
resulting	O
fit	O
this	O
looks	O
better	O
than	O
the	O
top	O
left	O
plot	B
but	O
the	O
vshaped	O
join	O
looks	O
unnatural	O
in	O
the	O
lower	O
left	O
plot	B
we	O
have	O
added	O
two	O
additional	O
constraints	O
now	O
both	O
the	O
first	O
and	O
second	O
derivatives	O
of	O
the	O
piecewise	O
polynomials	O
are	O
continuous	B
at	O
in	O
other	O
words	O
we	O
are	O
requiring	O
that	O
the	O
piecewise	B
polynomial	B
be	O
not	O
only	O
continuous	B
when	O
but	O
also	O
very	O
smooth	O
each	O
constraint	O
that	O
we	O
impose	O
on	O
the	O
piecewise	O
cubic	B
polynomials	O
effectively	O
frees	O
up	O
one	O
degree	O
of	O
freedom	O
by	O
reducing	O
the	O
complexity	O
of	O
the	O
resulting	O
piecewise	B
polynomial	B
fit	O
so	O
in	O
the	O
top	O
left	O
plot	B
we	O
are	O
using	O
eight	O
degrees	B
of	I
freedom	I
but	O
in	O
the	O
bottom	O
left	O
plot	B
we	O
imposed	O
three	O
constraints	O
continuity	O
of	O
the	O
first	O
derivative	B
and	O
continuity	O
of	O
the	O
second	O
derivative	B
and	O
so	O
are	O
left	O
with	O
five	O
degrees	B
of	I
freedom	I
the	O
curve	O
in	O
the	O
bottom	O
left	O
derivative	B
regression	B
splines	O
plot	B
is	O
called	O
a	O
cubic	B
in	O
general	O
a	O
cubic	B
spline	B
with	O
k	O
knots	O
uses	O
a	O
total	O
of	O
k	O
degrees	B
of	I
freedom	I
in	O
figure	O
the	O
lower	O
right	O
plot	B
is	O
a	O
linear	B
spline	B
which	O
is	O
continuous	B
at	O
the	O
general	O
definition	O
of	O
a	O
degree-d	O
spline	B
is	O
that	O
it	O
is	O
a	O
piecewise	O
degree-d	O
polynomial	B
with	O
continuity	O
in	O
derivatives	O
up	O
to	O
degree	O
d	O
at	O
each	O
knot	B
therefore	O
a	O
linear	B
spline	B
is	O
obtained	O
by	O
fitting	O
a	O
line	B
in	O
each	O
region	O
of	O
the	O
predictor	B
space	O
defined	O
by	O
the	O
knots	O
requiring	O
continuity	O
at	O
each	O
knot	B
in	O
figure	O
there	O
is	O
a	O
single	B
knot	B
at	O
of	O
course	O
we	O
could	O
add	O
more	O
knots	O
and	O
impose	O
continuity	O
at	O
each	O
cubic	B
spline	B
linear	B
spline	B
the	O
spline	B
basis	B
representation	O
the	O
regression	B
splines	O
that	O
we	O
just	O
saw	O
in	O
the	O
previous	O
section	O
may	O
have	O
seemed	O
somewhat	O
complex	O
how	O
can	O
we	O
fit	O
a	O
piecewise	O
degree-d	O
polynomial	B
under	O
the	O
constraint	O
that	O
it	O
possibly	O
its	O
first	O
d	O
derivatives	O
be	O
continuous	B
it	O
turns	O
out	O
that	O
we	O
can	O
use	O
the	O
basis	B
model	B
to	O
represent	O
a	O
regression	B
spline	B
a	O
cubic	B
spline	B
with	O
k	O
knots	O
can	O
be	O
modeled	O
as	O
yi	O
for	O
an	O
appropriate	O
choice	O
of	O
basis	B
functions	O
the	O
model	B
can	O
then	O
be	O
fit	O
using	O
least	B
squares	I
truncated	B
power	B
basis	B
just	O
as	O
there	O
were	O
several	O
ways	O
to	O
represent	O
polynomials	O
there	O
are	O
also	O
many	O
equivalent	O
ways	O
to	O
represent	O
cubic	B
splines	O
using	O
different	O
choices	O
of	O
basis	B
functions	O
in	O
the	O
most	O
direct	O
way	O
to	O
represent	O
a	O
cubic	B
spline	B
using	O
is	O
to	O
start	O
off	O
with	O
a	O
basis	B
for	O
a	O
cubic	B
polynomial	B
namely	O
x	O
and	O
then	O
add	O
one	O
truncated	B
power	B
basis	B
function	B
per	O
knot	B
a	O
truncated	B
power	B
basis	B
function	B
is	O
defined	O
as	O
hx	O
if	O
x	O
otherwise	O
where	O
is	O
the	O
knot	B
one	O
can	O
show	O
that	O
adding	O
a	O
term	B
of	O
the	O
form	O
to	O
the	O
model	B
for	O
a	O
cubic	B
polynomial	B
will	O
lead	O
to	O
a	O
discontinuity	O
in	O
only	O
the	O
third	O
derivative	B
at	O
the	O
function	B
will	O
remain	O
continuous	B
with	O
continuous	B
first	O
and	O
second	O
derivatives	O
at	O
each	O
of	O
the	O
knots	O
in	O
other	O
words	O
in	O
order	O
to	O
fit	O
a	O
cubic	B
spline	B
to	O
a	O
data	B
set	B
with	O
k	O
knots	O
we	O
perform	O
least	B
squares	I
regression	B
with	O
an	O
intercept	B
and	O
k	O
predictors	O
of	O
the	O
form	O
x	O
x	O
x	O
hx	O
hx	O
hx	O
k	O
where	O
k	O
are	O
the	O
knots	O
this	O
amounts	O
to	O
estimating	O
a	O
total	O
of	O
k	O
regression	B
coefficients	O
for	O
this	O
reason	O
fitting	O
a	O
cubic	B
spline	B
with	O
k	O
knots	O
uses	O
k	O
degrees	B
of	I
freedom	I
splines	O
are	O
popular	O
because	O
most	O
human	O
eyes	O
cannot	O
detect	O
the	O
discontinuity	O
at	O
the	O
knots	O
natural	B
cubic	B
spline	B
cubic	B
spline	B
moving	O
beyond	O
linearity	O
e	O
g	O
a	O
w	O
age	O
figure	O
a	O
cubic	B
spline	B
and	O
a	O
natural	B
cubic	B
spline	B
with	O
three	O
knots	O
fit	O
to	O
a	O
subset	O
of	O
the	O
wage	B
data	B
unfortunately	O
splines	O
can	O
have	O
high	O
variance	B
at	O
the	O
outer	O
range	O
of	O
the	O
predictors	O
that	O
is	O
when	O
x	O
takes	O
on	O
either	O
a	O
very	O
small	O
or	O
very	O
large	O
value	O
figure	O
shows	O
a	O
fit	O
to	O
the	O
wage	B
data	B
with	O
three	O
knots	O
we	O
see	O
that	O
the	O
confidence	O
bands	O
in	O
the	O
boundary	O
region	O
appear	O
fairly	O
wild	O
a	O
natural	B
spline	B
is	O
a	O
regression	B
spline	B
with	O
additional	O
boundary	O
constraints	O
the	O
function	B
is	O
required	O
to	O
be	O
linear	B
at	O
the	O
boundary	O
the	O
region	O
where	O
x	O
is	O
smaller	O
than	O
the	O
smallest	O
knot	B
or	O
larger	O
than	O
the	O
largest	O
knot	B
this	O
additional	O
constraint	O
means	O
that	O
natural	B
splines	O
generally	O
produce	O
more	O
stable	O
estimates	O
at	O
the	O
boundaries	O
in	O
figure	O
a	O
natural	B
cubic	B
spline	B
is	O
also	O
displayed	O
as	O
a	O
red	O
line	B
note	O
that	O
the	O
corresponding	O
confidence	O
intervals	O
are	O
narrower	O
natural	B
spline	B
choosing	O
the	O
number	O
and	O
locations	O
of	O
the	O
knots	O
when	O
we	O
fit	O
a	O
spline	B
where	O
should	O
we	O
place	O
the	O
knots	O
the	O
regression	B
spline	B
is	O
most	O
flexible	O
in	O
regions	O
that	O
contain	O
a	O
lot	O
of	O
knots	O
because	O
in	O
those	O
regions	O
the	O
polynomial	B
coefficients	O
can	O
change	O
rapidly	O
hence	O
one	O
option	O
is	O
to	O
place	O
more	O
knots	O
in	O
places	O
where	O
we	O
feel	O
the	O
function	B
might	O
vary	O
most	O
rapidly	O
and	O
to	O
place	O
fewer	O
knots	O
where	O
it	O
seems	O
more	O
stable	O
while	O
this	O
option	O
can	O
work	O
well	O
in	O
practice	O
it	O
is	O
common	O
to	O
place	O
knots	O
in	O
a	O
uniform	O
fashion	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
specify	O
the	O
desired	O
degrees	B
of	I
freedom	I
and	O
then	O
have	O
the	O
software	O
automatically	O
place	O
the	O
corresponding	O
number	O
of	O
knots	O
at	O
uniform	O
quantiles	O
of	O
the	O
data	B
figure	O
shows	O
an	O
example	O
on	O
the	O
wage	B
data	B
as	O
in	O
figure	O
we	O
have	O
fit	O
a	O
natural	B
cubic	B
spline	B
with	O
three	O
knots	O
except	O
this	O
time	O
the	O
knot	B
locations	O
were	O
chosen	O
automatically	O
as	O
the	O
and	O
percentiles	O
e	O
g	O
a	O
w	O
regression	B
splines	O
natural	B
cubic	B
spline	B
e	O
g	O
a	O
e	O
g	O
a	O
w	O
r	O
p	O
age	O
age	O
figure	O
a	O
natural	B
cubic	B
spline	B
function	B
with	O
four	O
degrees	B
of	I
freedom	I
is	O
fit	O
to	O
the	O
wage	B
data	B
left	O
a	O
spline	B
is	O
fit	O
to	O
wage	B
thousands	O
of	O
dollars	O
as	O
a	O
function	B
of	O
age	O
right	O
logistic	B
regression	B
is	O
used	O
to	O
model	B
the	O
binary	B
event	O
as	O
a	O
function	B
of	O
age	O
the	O
fitted	O
posterior	O
probability	B
of	O
wage	B
exceeding	O
is	O
shown	O
of	O
age	O
this	O
was	O
specified	O
by	O
requesting	O
four	O
degrees	B
of	I
freedom	I
the	O
argument	B
by	O
which	O
four	O
degrees	B
of	I
freedom	I
leads	O
to	O
three	O
interior	O
knots	O
is	O
somewhat	O
how	O
many	O
knots	O
should	O
we	O
use	O
or	O
equivalently	O
how	O
many	O
degrees	B
of	I
freedom	I
should	O
our	O
spline	B
contain	O
one	O
option	O
is	O
to	O
try	O
out	O
different	O
numbers	O
of	O
knots	O
and	O
see	O
which	O
produces	O
the	O
best	O
looking	O
curve	O
a	O
somewhat	O
more	O
objective	O
approach	B
is	O
to	O
use	O
cross-validation	B
as	O
discussed	O
in	O
chapters	O
and	O
with	O
this	O
method	O
we	O
remove	O
a	O
portion	O
of	O
the	O
data	B
fit	O
a	O
spline	B
with	O
a	O
certain	O
number	O
of	O
knots	O
to	O
the	O
remaining	O
data	B
and	O
then	O
use	O
the	O
spline	B
to	O
make	O
predictions	O
for	O
the	O
held-out	O
portion	O
we	O
repeat	O
this	O
process	O
multiple	B
times	O
until	O
each	O
observation	O
has	O
been	O
left	O
out	O
once	O
and	O
then	O
compute	O
the	O
overall	O
cross-validated	O
rss	O
this	O
procedure	O
can	O
be	O
repeated	O
for	O
different	O
numbers	O
of	O
knots	O
k	O
then	O
the	O
value	O
of	O
k	O
giving	O
the	O
smallest	O
rss	O
is	O
chosen	O
are	O
actually	O
five	O
knots	O
including	O
the	O
two	O
boundary	O
knots	O
a	O
cubic	B
spline	B
with	O
five	O
knots	O
would	O
have	O
nine	O
degrees	B
of	I
freedom	I
but	O
natural	B
cubic	B
splines	O
have	O
two	O
additional	O
natural	B
constraints	O
at	O
each	O
boundary	O
to	O
enforce	O
linearity	O
resulting	O
in	O
degrees	B
of	I
freedom	I
since	O
this	O
includes	O
a	O
constant	O
which	O
is	O
absorbed	O
in	O
the	O
intercept	B
we	O
count	O
it	O
as	O
four	O
degrees	B
of	I
freedom	I
moving	O
beyond	O
linearity	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
degrees	B
of	I
freedom	I
of	O
natural	B
spline	B
degrees	B
of	I
freedom	I
of	O
cubic	B
spline	B
figure	O
ten-fold	O
cross-validated	O
mean	O
squared	O
errors	O
for	O
selecting	O
the	O
degrees	B
of	I
freedom	I
when	O
fitting	O
splines	O
to	O
the	O
wage	B
data	B
the	O
response	B
is	O
wage	B
and	O
the	O
predictor	B
age	O
left	O
a	O
natural	B
cubic	B
spline	B
right	O
a	O
cubic	B
spline	B
figure	O
shows	O
ten-fold	O
cross-validated	O
mean	O
squared	O
errors	O
for	O
splines	O
with	O
various	O
degrees	B
of	I
freedom	I
fit	O
to	O
the	O
wage	B
data	B
the	O
left-hand	O
panel	O
corresponds	O
to	O
a	O
natural	B
spline	B
and	O
the	O
right-hand	O
panel	O
to	O
a	O
cubic	B
spline	B
the	O
two	O
methods	O
produce	O
almost	O
identical	O
results	O
with	O
clear	O
evidence	O
that	O
a	O
one-degree	O
fit	O
linear	B
regression	B
is	O
not	O
adequate	O
both	O
curves	O
flatten	O
out	O
quickly	O
and	O
it	O
seems	O
that	O
three	O
degrees	B
of	I
freedom	I
for	O
the	O
natural	B
spline	B
and	O
four	O
degrees	B
of	I
freedom	I
for	O
the	O
cubic	B
spline	B
are	O
quite	O
adequate	O
in	O
section	O
we	O
fit	O
additive	B
spline	B
models	O
simultaneously	O
on	O
several	O
variables	O
at	O
a	O
time	O
this	O
could	O
potentially	O
require	O
the	O
selection	B
of	O
degrees	B
of	I
freedom	I
for	O
each	O
variable	B
in	O
cases	O
like	O
this	O
we	O
typically	O
adopt	O
a	O
more	O
pragmatic	O
approach	B
and	O
set	B
the	O
degrees	B
of	I
freedom	I
to	O
a	O
fixed	O
number	O
say	O
four	O
for	O
all	O
terms	O
comparison	O
to	O
polynomial	B
regression	B
regression	B
splines	O
often	O
give	O
superior	O
results	O
to	O
polynomial	B
regression	B
this	O
is	O
because	O
unlike	O
polynomials	O
which	O
must	O
use	O
a	O
high	O
degree	O
in	O
the	O
highest	O
monomial	O
term	B
e	O
g	O
x	O
to	O
produce	O
flexible	O
fits	O
splines	O
introduce	O
flexibility	O
by	O
increasing	O
the	O
number	O
of	O
knots	O
but	O
keeping	O
the	O
degree	O
fixed	O
generally	O
this	O
approach	B
produces	O
more	O
stable	O
estimates	O
splines	O
also	O
allow	O
us	O
to	O
place	O
more	O
knots	O
and	O
hence	O
flexibility	O
over	O
regions	O
where	O
the	O
function	B
f	O
seems	O
to	O
be	O
changing	O
rapidly	O
and	O
fewer	O
knots	O
where	O
f	O
appears	O
more	O
stable	O
figure	O
compares	O
a	O
natural	B
cubic	B
spline	B
with	O
degrees	B
of	I
freedom	I
to	O
a	O
polynomial	B
on	O
the	O
wage	B
data	B
set	B
the	O
extra	O
flexibility	O
in	O
the	O
polynomial	B
produces	O
undesirable	O
results	O
at	O
the	O
boundaries	O
while	O
the	O
natural	B
cubic	B
spline	B
still	O
provides	O
a	O
reasonable	O
fit	O
to	O
the	O
data	B
smoothing	B
splines	O
natural	B
cubic	B
spline	B
polynomial	B
e	O
g	O
a	O
w	O
age	O
figure	O
on	O
the	O
wage	B
data	B
set	B
a	O
natural	B
cubic	B
spline	B
with	O
degrees	B
of	I
freedom	I
is	O
compared	O
to	O
a	O
polynomial	B
polynomials	O
can	O
show	O
wild	O
behavior	O
especially	O
near	O
the	O
tails	O
smoothing	B
splines	O
an	O
overview	O
of	O
smoothing	B
splines	O
in	O
the	O
last	O
section	O
we	O
discussed	O
regression	B
splines	O
which	O
we	O
create	O
by	O
specifying	O
a	O
set	B
of	O
knots	O
producing	O
a	O
sequence	O
of	O
basis	B
functions	O
and	O
then	O
using	O
least	B
squares	I
to	O
estimate	O
the	O
spline	B
coefficients	O
we	O
now	O
introduce	O
a	O
somewhat	O
different	O
approach	B
that	O
also	O
produces	O
a	O
spline	B
n	O
to	O
be	O
small	O
however	O
there	O
is	O
a	O
problem	O
in	O
fitting	O
a	O
smooth	O
curve	O
to	O
a	O
set	B
of	O
data	B
what	O
we	O
really	O
want	O
to	O
do	O
is	O
find	O
some	O
function	B
say	O
gx	O
that	O
fits	O
the	O
observed	O
data	B
well	O
that	O
is	O
we	O
want	O
rss	O
with	O
this	O
approach	B
if	O
we	O
don	O
t	O
put	O
any	O
constraints	O
on	O
gxi	O
then	O
we	O
can	O
always	O
make	O
rss	O
zero	O
simply	O
by	O
choosing	O
g	O
such	O
that	O
it	O
interpolates	O
all	O
of	O
the	O
yi	O
such	O
a	O
function	B
would	O
woefully	O
overfit	O
the	O
data	B
it	O
would	O
be	O
far	O
too	O
flexible	O
what	O
we	O
really	O
want	O
is	O
a	O
function	B
g	O
that	O
makes	O
rss	O
small	O
but	O
that	O
is	O
also	O
smooth	O
how	O
might	O
we	O
ensure	O
that	O
g	O
is	O
smooth	O
there	O
are	O
a	O
number	O
of	O
ways	O
to	O
do	O
this	O
a	O
natural	B
approach	B
is	O
to	O
find	O
the	O
function	B
g	O
that	O
minimizes	O
g	O
where	O
is	O
a	O
nonnegative	O
tuning	B
parameter	B
the	O
function	B
g	O
that	O
minimizes	O
is	O
known	O
as	O
a	O
smoothing	B
spline	B
what	O
does	O
mean	O
equation	O
takes	O
the	O
losspenalty	O
formulation	O
that	O
we	O
encounter	O
in	O
the	O
context	O
of	O
ridge	B
regression	B
and	O
the	O
lasso	B
in	O
chapter	O
the	O
term	B
ages	O
g	O
to	O
fit	O
the	O
data	B
well	O
and	O
the	O
term	B
is	O
a	O
loss	B
function	B
that	O
is	O
a	O
penalty	B
term	B
g	O
n	O
smoothing	B
spline	B
loss	B
function	B
moving	O
beyond	O
linearity	O
g	O
will	O
be	O
close	O
to	O
constant	O
and	O
indicates	O
the	O
second	O
that	O
penalizes	O
the	O
variability	O
in	O
g	O
the	O
notation	O
g	O
derivative	B
of	O
the	O
function	B
g	O
the	O
first	O
derivative	B
g	O
measures	O
the	O
slope	B
of	O
a	O
function	B
at	O
t	O
and	O
the	O
second	O
derivative	B
corresponds	O
to	O
the	O
amount	O
by	O
which	O
the	O
slope	B
is	O
changing	O
hence	O
broadly	O
speaking	O
the	O
second	O
derivative	B
of	O
a	O
function	B
is	O
a	O
measure	O
of	O
its	O
roughness	O
it	O
is	O
large	O
in	O
absolute	O
value	O
if	O
gt	O
is	O
very	O
wiggly	O
near	O
t	O
and	O
it	O
is	O
close	O
to	O
zero	O
otherwise	O
second	O
derivative	B
of	O
a	O
straight	O
line	B
is	O
zero	O
note	O
that	O
a	O
line	B
is	O
perfectly	O
smooth	O
notation	O
is	O
an	O
integral	B
which	O
we	O
can	O
think	O
of	O
as	O
a	O
summation	O
over	O
the	O
is	O
simply	O
a	O
measure	O
of	O
the	O
total	O
the	O
range	O
of	O
t	O
in	O
other	O
words	O
change	O
in	O
the	O
function	B
g	O
over	O
its	O
entire	O
range	O
if	O
g	O
is	O
very	O
smooth	O
then	O
will	O
take	O
on	O
a	O
small	O
value	O
g	O
will	O
vary	O
significantly	O
and	O
conversely	O
if	O
g	O
is	O
jumpy	O
and	O
variable	B
then	O
g	O
encourages	O
g	O
to	O
be	O
smooth	O
the	O
larger	O
the	O
value	O
of	O
the	O
smoother	B
g	O
will	O
be	O
when	O
then	O
the	O
penalty	B
term	B
in	O
has	O
no	O
effect	O
and	O
so	O
the	O
function	B
g	O
will	O
be	O
very	O
jumpy	O
and	O
will	O
exactly	O
interpolate	O
the	O
training	O
observations	B
when	O
g	O
will	O
be	O
perfectly	O
smooth	O
it	O
will	O
just	O
be	O
a	O
straight	O
line	B
that	O
passes	O
as	O
closely	O
as	O
possible	O
to	O
the	O
training	O
points	O
in	O
fact	O
in	O
this	O
case	O
g	O
will	O
be	O
the	O
linear	B
least	B
squares	I
line	B
since	O
the	O
loss	B
function	B
in	O
amounts	O
to	O
minimizing	O
the	O
residual	B
sum	B
of	I
squares	I
for	O
an	O
intermediate	O
value	O
of	O
g	O
will	O
approximate	O
the	O
training	O
observations	B
but	O
will	O
be	O
somewhat	O
smooth	O
we	O
see	O
that	O
controls	O
the	O
bias-variance	O
trade-off	B
of	O
the	O
smoothing	B
spline	B
will	O
take	O
on	O
a	O
large	O
value	O
therefore	O
in	O
g	O
g	O
g	O
the	O
function	B
gx	O
that	O
minimizes	O
can	O
be	O
shown	O
to	O
have	O
some	O
special	O
properties	O
it	O
is	O
a	O
piecewise	O
cubic	B
polynomial	B
with	O
knots	O
at	O
the	O
unique	O
values	O
of	O
xn	O
and	O
continuous	B
first	O
and	O
second	O
derivatives	O
at	O
each	O
knot	B
furthermore	O
it	O
is	O
linear	B
in	O
the	O
region	O
outside	O
of	O
the	O
extreme	O
knots	O
in	O
other	O
words	O
the	O
function	B
gx	O
that	O
minimizes	O
is	O
a	O
natural	B
cubic	B
spline	B
with	O
knots	O
at	O
xn	O
however	O
it	O
is	O
not	O
the	O
same	O
natural	B
cubic	B
spline	B
that	O
one	O
would	O
get	O
if	O
one	O
applied	O
the	O
basis	B
function	B
approach	B
described	O
in	O
section	O
with	O
knots	O
at	O
xn	O
rather	O
it	O
is	O
a	O
shrunken	O
version	O
of	O
such	O
a	O
natural	B
cubic	B
spline	B
where	O
the	O
value	O
of	O
the	O
tuning	B
parameter	B
in	O
controls	O
the	O
level	B
of	O
shrinkage	B
choosing	O
the	O
smoothing	B
parameter	B
we	O
have	O
seen	O
that	O
a	O
smoothing	B
spline	B
is	O
simply	O
a	O
natural	B
cubic	B
spline	B
with	O
knots	O
at	O
every	O
unique	O
value	O
of	O
xi	O
it	O
might	O
seem	O
that	O
a	O
smoothing	B
spline	B
will	O
have	O
far	O
too	O
many	O
degrees	B
of	I
freedom	I
since	O
a	O
knot	B
at	O
each	O
data	B
point	O
allows	O
a	O
great	O
deal	O
of	O
flexibility	O
but	O
the	O
tuning	B
parameter	B
controls	O
the	O
roughness	O
of	O
the	O
smoothing	B
spline	B
and	O
hence	O
the	O
effective	B
degrees	B
of	I
freedom	I
it	O
is	O
possible	O
to	O
show	O
that	O
as	O
increases	O
from	O
to	O
the	O
effective	B
degrees	B
of	I
freedom	I
which	O
we	O
write	O
df	O
decrease	O
from	O
n	O
to	O
in	O
the	O
context	O
of	O
smoothing	B
splines	O
why	O
do	O
we	O
discuss	O
effective	B
degrees	B
of	I
freedom	I
instead	O
of	O
degrees	B
of	I
freedom	I
usually	O
degrees	B
of	I
freedom	I
refer	O
effective	B
degrees	B
of	I
freedom	I
smoothing	B
splines	O
to	O
the	O
number	O
of	O
free	O
parameters	O
such	O
as	O
the	O
number	O
of	O
coefficients	O
fit	O
in	O
a	O
polynomial	B
or	O
cubic	B
spline	B
although	O
a	O
smoothing	B
spline	B
has	O
n	O
parameters	O
and	O
hence	O
n	O
nominal	O
degrees	B
of	I
freedom	I
these	O
n	O
parameters	O
are	O
heavily	O
constrained	O
or	O
shrunk	O
down	O
hence	O
df	O
is	O
a	O
measure	O
of	O
the	O
flexibility	O
of	O
the	O
smoothing	B
spline	B
the	O
higher	O
it	O
is	O
the	O
more	O
flexible	O
the	O
lower-bias	O
but	O
higher-variance	O
the	O
smoothing	B
spline	B
the	O
definition	O
of	O
effective	B
degrees	B
of	I
freedom	I
is	O
somewhat	O
technical	O
we	O
can	O
write	O
g	O
s	O
y	O
where	O
g	O
is	O
the	O
solution	O
to	O
for	O
a	O
particular	O
choice	O
of	O
that	O
is	O
it	O
is	O
a	O
n-vector	O
containing	O
the	O
fitted	O
values	O
of	O
the	O
smoothing	B
spline	B
at	O
the	O
training	O
points	O
xn	O
equation	O
indicates	O
that	O
the	O
vector	B
of	O
fitted	O
values	O
when	O
applying	O
a	O
smoothing	B
spline	B
to	O
the	O
data	B
can	O
be	O
written	O
as	O
a	O
n	O
n	O
matrix	O
s	O
which	O
there	O
is	O
a	O
formula	O
times	O
the	O
response	B
vector	B
y	O
then	O
the	O
effective	B
degrees	B
of	I
freedom	I
is	O
defined	O
to	O
be	O
df	O
the	O
sum	O
of	O
the	O
diagonal	O
elements	O
of	O
the	O
matrix	O
s	O
in	O
fitting	O
a	O
smoothing	B
spline	B
we	O
do	O
not	O
need	O
to	O
select	O
the	O
number	O
or	O
location	O
of	O
the	O
knots	O
there	O
will	O
be	O
a	O
knot	B
at	O
each	O
training	O
observation	O
xn	O
instead	O
we	O
have	O
another	O
problem	O
we	O
need	O
to	O
choose	O
the	O
value	O
of	O
it	O
should	O
come	O
as	O
no	O
surprise	O
that	O
one	O
possible	O
solution	O
to	O
this	O
problem	O
is	O
cross-validation	B
in	O
other	O
words	O
we	O
can	O
find	O
the	O
value	O
of	O
that	O
makes	O
the	O
cross-validated	O
rss	O
as	O
small	O
as	O
possible	O
it	O
turns	O
out	O
that	O
the	O
leaveone-out	O
cross-validation	B
error	B
can	O
be	O
computed	O
very	O
efficiently	O
for	O
smoothing	B
splines	O
with	O
essentially	O
the	O
same	O
cost	O
as	O
computing	O
a	O
single	B
fit	O
using	O
the	O
following	O
formula	O
rsscv	O
g	O
i	O
yi	O
g	O
the	O
notation	O
g	O
i	O
indicates	O
the	O
fitted	O
value	O
for	O
this	O
smoothing	B
spline	B
evaluated	O
at	O
xi	O
where	O
the	O
fit	O
uses	O
all	O
of	O
the	O
training	O
observations	B
except	O
for	O
the	O
ith	O
observation	O
yi	O
in	O
contrast	B
g	O
indicates	O
the	O
smoothing	B
spline	B
function	B
fit	O
to	O
all	O
of	O
the	O
training	O
observations	B
and	O
evaluated	O
at	O
xi	O
this	O
remarkable	O
formula	O
says	O
that	O
we	O
can	O
compute	O
each	O
of	O
these	O
leaveone-out	O
fits	O
using	O
only	O
g	O
the	O
original	O
fit	O
to	O
all	O
of	O
the	O
we	O
have	O
a	O
very	O
similar	O
formula	O
on	O
page	O
in	O
chapter	O
for	O
least	B
squares	I
linear	B
regression	B
using	O
we	O
can	O
very	O
quickly	O
perform	O
loocv	O
for	O
the	O
regression	B
splines	O
discussed	O
earlier	O
in	O
this	O
chapter	O
as	O
well	O
as	O
for	O
least	B
squares	I
regression	B
using	O
arbitrary	O
basis	B
functions	O
exact	O
formulas	O
for	O
computing	O
gxi	O
and	O
s	O
are	O
very	O
technical	O
however	O
efficient	O
algorithms	O
are	O
available	O
for	O
computing	O
these	O
quantities	O
moving	O
beyond	O
linearity	O
smoothing	B
spline	B
degrees	B
of	I
freedom	I
degrees	B
of	I
freedom	I
e	O
g	O
a	O
w	O
age	O
figure	O
smoothing	B
spline	B
fits	O
to	O
the	O
wage	B
data	B
the	O
red	O
curve	O
results	O
from	O
specifying	O
effective	B
degrees	B
of	I
freedom	I
for	O
the	O
blue	O
curve	O
was	O
found	O
automatically	O
by	O
leave-one-out	B
cross-validation	B
which	O
resulted	O
in	O
effective	B
degrees	B
of	I
freedom	I
figure	O
shows	O
the	O
results	O
from	O
fitting	O
a	O
smoothing	B
spline	B
to	O
the	O
wage	B
data	B
the	O
red	O
curve	O
indicates	O
the	O
fit	O
obtained	O
from	O
pre-specifying	O
that	O
we	O
would	O
like	O
a	O
smoothing	B
spline	B
with	O
effective	B
degrees	B
of	I
freedom	I
the	O
blue	O
curve	O
is	O
the	O
smoothing	B
spline	B
obtained	O
when	O
is	O
chosen	O
using	O
loocv	O
in	O
this	O
case	O
the	O
value	O
of	O
chosen	O
results	O
in	O
effective	B
degrees	B
of	I
freedom	I
using	O
for	O
this	O
data	B
there	O
is	O
little	O
discernible	O
difference	O
between	O
the	O
two	O
smoothing	B
splines	O
beyond	O
the	O
fact	O
that	O
the	O
one	O
with	O
degrees	B
of	I
freedom	I
seems	O
slightly	O
wigglier	O
since	O
there	O
is	O
little	O
difference	O
between	O
the	O
two	O
fits	O
the	O
smoothing	B
spline	B
fit	O
with	O
degrees	B
of	I
freedom	I
is	O
preferable	O
since	O
in	O
general	O
simpler	O
models	O
are	O
better	O
unless	O
the	O
data	B
provides	O
evidence	O
in	O
support	O
of	O
a	O
more	O
complex	O
model	B
local	B
regression	B
local	B
regression	B
is	O
a	O
different	O
approach	B
for	O
fitting	O
flexible	O
non-linear	B
functions	O
which	O
involves	O
computing	O
the	O
fit	O
at	O
a	O
target	O
point	O
using	O
only	O
the	O
nearby	O
training	O
observations	B
figure	O
illustrates	O
the	O
idea	O
on	O
some	O
simulated	O
data	B
with	O
one	O
target	O
point	O
near	O
and	O
another	O
near	O
the	O
boundary	O
at	O
in	O
this	O
figure	O
the	O
blue	O
line	B
represents	O
the	O
function	B
f	O
from	O
which	O
the	O
data	B
were	O
generated	O
and	O
the	O
light	O
orange	O
line	B
corresponds	O
to	O
the	O
local	B
regression	B
estimate	O
f	O
local	B
regression	B
is	O
described	O
in	O
algorithm	O
note	O
that	O
in	O
step	O
of	O
algorithm	O
the	O
weights	O
will	O
differ	O
for	O
each	O
value	O
of	O
in	O
other	O
words	O
in	O
order	O
to	O
obtain	O
the	O
local	B
regression	B
fit	O
at	O
a	O
new	O
point	O
we	O
need	O
to	O
fit	O
a	O
new	O
weighted	B
least	B
squares	I
regression	B
model	B
by	O
local	B
regression	B
local	B
regression	B
local	B
regression	B
o	O
oo	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
ooo	O
ooo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
oo	O
oo	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
ooo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
figure	O
local	B
regression	B
illustrated	O
on	O
some	O
simulated	O
data	B
where	O
the	O
blue	O
curve	O
represents	O
f	O
from	O
which	O
the	O
data	B
were	O
generated	O
and	O
the	O
light	O
orange	O
curve	O
corresponds	O
to	O
the	O
local	B
regression	B
estimate	O
f	O
the	O
orange	O
colored	O
points	O
are	O
local	B
to	O
the	O
target	O
point	O
represented	O
by	O
the	O
orange	O
vertical	O
line	B
the	O
yellow	O
bell-shape	O
superimposed	O
on	O
the	O
plot	B
indicates	O
weights	O
assigned	O
to	O
each	O
point	O
decreasing	O
to	O
zero	O
with	O
distance	O
from	O
the	O
target	O
point	O
the	O
fit	O
f	O
at	O
is	O
obtained	O
by	O
fitting	O
a	O
weighted	B
linear	B
regression	B
line	B
segment	O
and	O
using	O
the	O
fitted	O
value	O
at	O
solid	O
dot	O
as	O
the	O
estimate	O
f	O
minimizing	O
for	O
a	O
new	O
set	B
of	O
weights	O
local	B
regression	B
is	O
sometimes	O
referred	O
to	O
as	O
a	O
memory-based	O
procedure	O
because	O
like	O
nearest-neighbors	O
we	O
need	O
all	O
the	O
training	O
data	B
each	O
time	O
we	O
wish	O
to	O
compute	O
a	O
prediction	B
we	O
will	O
avoid	O
getting	O
into	O
the	O
technical	O
details	O
of	O
local	B
regression	B
here	O
there	O
are	O
books	O
written	O
on	O
the	O
topic	O
in	O
order	O
to	O
perform	O
local	B
regression	B
there	O
are	O
a	O
number	O
of	O
choices	O
to	O
be	O
made	O
such	O
as	O
how	O
to	O
define	O
the	O
weighting	O
function	B
k	O
and	O
whether	O
to	O
fit	O
a	O
linear	B
constant	O
or	O
quadratic	B
regression	B
in	O
step	O
above	O
corresponds	O
to	O
a	O
linear	B
regression	B
while	O
all	O
of	O
these	O
choices	O
make	O
some	O
difference	O
the	O
most	O
important	O
choice	O
is	O
the	O
span	O
s	O
defined	O
in	O
step	O
above	O
the	O
span	O
plays	O
a	O
role	O
like	O
that	O
of	O
the	O
tuning	B
parameter	B
in	O
smoothing	B
splines	O
it	O
controls	O
the	O
flexibility	O
of	O
the	O
non-linear	B
fit	O
the	O
smaller	O
the	O
value	O
of	O
s	O
the	O
more	O
local	B
and	O
wiggly	O
will	O
be	O
our	O
fit	O
alternatively	O
a	O
very	O
large	O
value	O
of	O
s	O
will	O
lead	O
to	O
a	O
global	O
fit	O
to	O
the	O
data	B
using	O
all	O
of	O
the	O
training	O
observations	B
we	O
can	O
again	O
use	O
cross-validation	B
to	O
choose	O
s	O
or	O
we	O
can	O
specify	O
it	O
directly	O
figure	O
displays	O
local	B
linear	B
regression	B
fits	O
on	O
the	O
wage	B
data	B
using	O
two	O
values	O
of	O
s	O
and	O
as	O
expected	O
the	O
fit	O
obtained	O
using	O
s	O
is	O
smoother	B
than	O
that	O
obtained	O
using	O
s	O
the	O
idea	O
of	O
local	B
regression	B
can	O
be	O
generalized	O
in	O
many	O
different	O
ways	O
in	O
a	O
setting	O
with	O
multiple	B
features	O
xp	O
one	O
very	O
useful	O
generalization	O
involves	O
fitting	O
a	O
multiple	B
linear	B
regression	B
model	B
that	O
is	O
global	O
in	O
some	O
variables	O
but	O
local	B
in	O
another	O
such	O
as	O
time	O
such	O
varying	O
coefficient	O
moving	O
beyond	O
linearity	O
algorithm	O
local	B
regression	B
at	O
x	O
gather	O
the	O
fraction	O
s	O
kn	O
of	O
training	O
points	O
whose	O
xi	O
are	O
closest	O
to	O
assign	O
a	O
weight	O
kxi	O
to	O
each	O
point	O
in	O
this	O
neighborhood	O
so	O
that	O
the	O
point	O
furthest	O
from	O
has	O
weight	O
zero	O
and	O
the	O
closest	O
has	O
the	O
highest	O
weight	O
all	O
but	O
these	O
k	O
nearest	O
neighbors	O
get	O
weight	O
zero	O
fit	O
a	O
weighted	B
least	B
squares	I
regression	B
of	O
the	O
yi	O
on	O
the	O
xi	O
using	O
the	O
aforementioned	O
weights	O
by	O
finding	O
and	O
that	O
minimize	O
the	O
fitted	O
value	O
at	O
is	O
given	O
by	O
f	O
models	O
are	O
a	O
useful	O
way	O
of	O
adapting	O
a	O
model	B
to	O
the	O
most	O
recently	O
gathered	O
data	B
local	B
regression	B
also	O
generalizes	O
very	O
naturally	O
when	O
we	O
want	O
to	O
fit	O
models	O
that	O
are	O
local	B
in	O
a	O
pair	O
of	O
variables	O
and	O
rather	O
than	O
one	O
we	O
can	O
simply	O
use	O
two-dimensional	O
neighborhoods	O
and	O
fit	O
bivariate	O
linear	B
regression	B
models	O
using	O
the	O
observations	B
that	O
are	O
near	O
each	O
target	O
point	O
in	O
two-dimensional	O
space	O
theoretically	O
the	O
same	O
approach	B
can	O
be	O
implemented	O
in	O
higher	O
dimensions	O
using	O
linear	B
regressions	O
fit	O
to	O
p-dimensional	O
neighborhoods	O
however	O
local	B
regression	B
can	O
perform	O
poorly	O
if	O
p	O
is	O
much	O
larger	O
than	O
about	O
or	O
because	O
there	O
will	O
generally	O
be	O
very	O
few	O
training	O
observations	B
close	O
to	O
nearest-neighbors	O
regression	B
discussed	O
in	O
chapter	O
suffers	O
from	O
a	O
similar	O
problem	O
in	O
high	O
dimensions	O
varying	O
coefficient	O
model	B
generalized	O
additive	B
models	O
in	O
sections	O
we	O
present	O
a	O
number	O
of	O
approaches	O
for	O
flexibly	O
predicting	O
a	O
response	B
y	O
on	O
the	O
basis	B
of	O
a	O
single	B
predictor	B
x	O
these	O
approaches	O
can	O
be	O
seen	O
as	O
extensions	O
of	O
simple	B
linear	B
regression	B
here	O
we	O
explore	O
the	O
problem	O
of	O
flexibly	O
predicting	O
y	O
on	O
the	O
basis	B
of	O
several	O
predictors	O
xp	O
this	O
amounts	O
to	O
an	O
extension	O
of	O
multiple	B
linear	B
regression	B
generalized	O
additive	B
models	O
provide	O
a	O
general	O
framework	O
for	O
extending	O
a	O
standard	O
linear	B
model	B
by	O
allowing	O
non-linear	B
functions	O
of	O
each	O
of	O
the	O
variables	O
while	O
maintaining	O
additivity	B
just	O
like	O
linear	B
models	O
gams	O
can	O
be	O
applied	O
with	O
both	O
quantitative	B
and	O
qualitative	B
responses	O
we	O
first	O
examine	O
gams	O
for	O
a	O
quantitative	B
response	B
in	O
section	O
and	O
then	O
for	O
a	O
qualitative	B
response	B
in	O
section	O
generalized	B
additive	B
model	B
additivity	B
generalized	O
additive	B
models	O
local	B
linear	B
regression	B
span	O
is	O
degrees	B
of	I
freedom	I
span	O
is	O
degrees	B
of	I
freedom	I
e	O
g	O
a	O
w	O
age	O
figure	O
local	B
linear	B
fits	O
to	O
the	O
wage	B
data	B
the	O
span	O
specifies	O
the	O
fraction	O
of	O
the	O
data	B
used	O
to	O
compute	O
the	O
fit	O
at	O
each	O
target	O
point	O
gams	O
for	O
regression	B
problems	O
a	O
natural	B
way	O
to	O
extend	O
the	O
multiple	B
linear	B
regression	B
model	B
yi	O
pxip	O
in	O
order	O
to	O
allow	O
for	O
non-linear	B
relationships	O
between	O
each	O
feature	B
and	O
the	O
response	B
is	O
to	O
replace	O
each	O
linear	B
component	O
jxij	O
with	O
a	O
nonlinear	O
function	B
fjxij	O
we	O
would	O
then	O
write	O
the	O
model	B
as	O
yi	O
fjxij	O
fpxip	O
this	O
is	O
an	O
example	O
of	O
a	O
gam	O
it	O
is	O
called	O
an	O
additive	B
model	B
because	O
we	O
calculate	O
a	O
separate	O
fj	O
for	O
each	O
xj	O
and	O
then	O
add	O
together	O
all	O
of	O
their	O
contributions	O
in	O
sections	O
we	O
discuss	O
many	O
methods	O
for	O
fitting	O
functions	O
to	O
a	O
single	B
variable	B
the	O
beauty	O
of	O
gams	O
is	O
that	O
we	O
can	O
use	O
these	O
methods	O
as	O
building	O
blocks	O
for	O
fitting	O
an	O
additive	B
model	B
in	O
fact	O
for	O
most	O
of	O
the	O
methods	O
that	O
we	O
have	O
seen	O
so	O
far	O
in	O
this	O
chapter	O
this	O
can	O
be	O
done	O
fairly	O
trivially	O
take	O
for	O
example	O
natural	B
splines	O
and	O
consider	O
the	O
task	O
of	O
fitting	O
the	O
model	B
wage	B
moving	O
beyond	O
linearity	O
hs	O
coll	O
r	O
a	O
e	O
y	O
f	O
e	O
g	O
a	O
f	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
f	O
year	O
age	O
education	O
figure	O
for	O
the	O
wage	B
data	B
plots	O
of	O
the	O
relationship	O
between	O
each	O
feature	B
and	O
the	O
response	B
wage	B
in	O
the	O
fitted	O
model	B
each	O
plot	B
displays	O
the	O
fitted	O
function	B
and	O
pointwise	O
standard	O
errors	O
the	O
first	O
two	O
functions	O
are	O
natural	B
splines	O
in	O
year	O
and	O
age	O
with	O
four	O
and	O
five	O
degrees	B
of	I
freedom	I
respectively	O
the	O
third	O
function	B
is	O
a	O
step	B
function	B
fit	O
to	O
the	O
qualitative	B
variable	B
education	O
on	O
the	O
wage	B
data	B
here	O
year	O
and	O
age	O
are	O
quantitative	B
variables	O
and	O
education	O
is	O
a	O
qualitative	B
variable	B
with	O
five	O
levels	O
hs	O
coll	O
referring	O
to	O
the	O
amount	O
of	O
high	O
school	O
or	O
college	B
education	O
that	O
an	O
individual	O
has	O
completed	O
we	O
fit	O
the	O
first	O
two	O
functions	O
using	O
natural	B
splines	O
we	O
fit	O
the	O
third	O
function	B
using	O
a	O
separate	O
constant	O
for	O
each	O
level	B
via	O
the	O
usual	O
dummy	B
variable	B
approach	B
of	O
section	O
figure	O
shows	O
the	O
results	O
of	O
fitting	O
the	O
model	B
using	O
least	B
squares	I
this	O
is	O
easy	O
to	O
do	O
since	O
as	O
discussed	O
in	O
section	O
natural	B
splines	O
can	O
be	O
constructed	O
using	O
an	O
appropriately	O
chosen	O
set	B
of	O
basis	B
functions	O
hence	O
the	O
entire	O
model	B
is	O
just	O
a	O
big	O
regression	B
onto	O
spline	B
basis	B
variables	O
and	O
dummy	B
variables	O
all	O
packed	O
into	O
one	O
big	O
regression	B
matrix	O
figure	O
can	O
be	O
easily	O
interpreted	O
the	O
left-hand	O
panel	O
indicates	O
that	O
holding	O
age	O
and	O
education	O
fixed	O
wage	B
tends	O
to	O
increase	O
slightly	O
with	O
year	O
this	O
may	O
be	O
due	O
to	O
inflation	O
the	O
center	O
panel	O
indicates	O
that	O
holding	O
education	O
and	O
year	O
fixed	O
wage	B
tends	O
to	O
be	O
highest	O
for	O
intermediate	O
values	O
of	O
age	O
and	O
lowest	O
for	O
the	O
very	O
young	O
and	O
very	O
old	O
the	O
right-hand	O
panel	O
indicates	O
that	O
holding	O
year	O
and	O
age	O
fixed	O
wage	B
tends	O
to	O
increase	O
with	O
education	O
the	O
more	O
educated	O
a	O
person	O
is	O
the	O
higher	O
their	O
salary	O
on	O
average	B
all	O
of	O
these	O
findings	O
are	O
intuitive	O
figure	O
shows	O
a	O
similar	O
triple	O
of	O
plots	O
but	O
this	O
time	O
and	O
are	O
smoothing	B
splines	O
with	O
four	O
and	O
five	O
degrees	B
of	I
freedom	I
respectively	O
fitting	O
a	O
gam	O
with	O
a	O
smoothing	B
spline	B
is	O
not	O
quite	O
as	O
simple	B
as	O
fitting	O
a	O
gam	O
with	O
a	O
natural	B
spline	B
since	O
in	O
the	O
case	O
of	O
smoothing	B
splines	O
least	B
squares	I
cannot	O
be	O
used	O
however	O
standard	O
software	O
such	O
as	O
the	O
gam	O
function	B
in	O
r	O
can	O
be	O
used	O
to	O
fit	O
gams	O
using	O
smoothing	B
splines	O
via	O
an	O
approach	B
known	O
as	O
backfitting	B
this	O
method	O
fits	O
a	O
model	B
involving	O
multiple	B
predictors	O
by	O
backfitting	B
generalized	O
additive	B
models	O
hs	O
coll	O
r	O
a	O
e	O
y	O
f	O
e	O
g	O
a	O
f	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
f	O
year	O
age	O
education	O
figure	O
details	O
are	O
as	O
in	O
figure	O
but	O
now	O
and	O
are	O
smoothing	B
splines	O
with	O
four	O
and	O
five	O
degrees	B
of	I
freedom	I
respectively	O
repeatedly	O
updating	O
the	O
fit	O
for	O
each	O
predictor	B
in	O
turn	O
holding	O
the	O
others	O
fixed	O
the	O
beauty	O
of	O
this	O
approach	B
is	O
that	O
each	O
time	O
we	O
update	O
a	O
function	B
we	O
simply	O
apply	O
the	O
fitting	O
method	O
for	O
that	O
variable	B
to	O
a	O
partial	O
the	O
fitted	O
functions	O
in	O
figures	O
and	O
look	O
rather	O
similar	O
in	O
most	O
situations	O
the	O
differences	O
in	O
the	O
gams	O
obtained	O
using	O
smoothing	B
splines	O
versus	O
natural	B
splines	O
are	O
small	O
we	O
do	O
not	O
have	O
to	O
use	O
splines	O
as	O
the	O
building	O
blocks	O
for	O
gams	O
we	O
can	O
just	O
as	O
well	O
use	O
local	B
regression	B
polynomial	B
regression	B
or	O
any	O
combination	O
of	O
the	O
approaches	O
seen	O
earlier	O
in	O
this	O
chapter	O
in	O
order	O
to	O
create	O
a	O
gam	O
gams	O
are	O
investigated	O
in	O
further	O
detail	O
in	O
the	O
lab	O
at	O
the	O
end	O
of	O
this	O
chapter	O
pros	O
and	O
cons	O
of	O
gams	O
before	O
we	O
move	O
on	O
let	O
us	O
summarize	O
the	O
advantages	O
and	O
limitations	O
of	O
a	O
gam	O
gams	O
allow	O
us	O
to	O
fit	O
a	O
non-linear	B
fj	O
to	O
each	O
xj	O
so	O
that	O
we	O
can	O
automatically	O
model	B
non-linear	B
relationships	O
that	O
standard	O
linear	B
regression	B
will	O
miss	O
this	O
means	O
that	O
we	O
do	O
not	O
need	O
to	O
manually	O
try	O
out	O
many	O
different	O
transformations	O
on	O
each	O
variable	B
individually	O
the	O
non-linear	B
fits	O
can	O
potentially	O
make	O
more	O
accurate	O
predictions	O
for	O
the	O
response	B
y	O
because	O
the	O
model	B
is	O
additive	B
we	O
can	O
still	O
examine	O
the	O
effect	O
of	O
each	O
xj	O
on	O
y	O
individually	O
while	O
holding	O
all	O
of	O
the	O
other	O
variables	O
fixed	O
hence	O
if	O
we	O
are	O
interested	O
in	O
inference	B
gams	O
provide	O
a	O
useful	O
representation	O
partial	O
residual	B
for	O
for	O
example	O
has	O
the	O
form	O
ri	O
yi	O
if	O
we	O
know	O
and	O
then	O
we	O
can	O
fit	O
by	O
treating	O
this	O
residual	B
as	O
a	O
response	B
in	O
a	O
non-linear	B
regression	B
on	O
moving	O
beyond	O
linearity	O
the	O
smoothness	O
of	O
the	O
function	B
fj	O
for	O
the	O
variable	B
xj	O
can	O
be	O
sum	O
marized	O
via	O
degrees	B
of	I
freedom	I
the	O
main	O
limitation	O
of	O
gams	O
is	O
that	O
the	O
model	B
is	O
restricted	O
to	O
be	O
additive	B
with	O
many	O
variables	O
important	O
interactions	O
can	O
be	O
missed	O
however	O
as	O
with	O
linear	B
regression	B
we	O
can	O
manually	O
add	O
interaction	B
terms	O
to	O
the	O
gam	O
model	B
by	O
including	O
additional	O
predictors	O
of	O
the	O
form	O
xj	O
xk	O
in	O
addition	O
we	O
can	O
add	O
low-dimensional	B
interaction	B
functions	O
of	O
the	O
form	O
fjkxj	O
xk	O
into	O
the	O
model	B
such	O
terms	O
can	O
be	O
fit	O
using	O
two-dimensional	O
smoothers	O
such	O
as	O
local	B
regression	B
or	O
two-dimensional	O
splines	O
covered	O
here	O
for	O
fully	O
general	O
models	O
we	O
have	O
to	O
look	O
for	O
even	O
more	O
flexible	O
approaches	O
such	O
as	O
random	O
forests	O
and	O
boosting	B
described	O
in	O
chapter	O
gams	O
provide	O
a	O
useful	O
compromise	O
between	O
linear	B
and	O
fully	O
nonparametric	O
models	O
gams	O
for	O
classification	B
problems	O
gams	O
can	O
also	O
be	O
used	O
in	O
situations	O
where	O
y	O
is	O
qualitative	B
for	O
simplicity	O
here	O
we	O
will	O
assume	O
y	O
takes	O
on	O
values	O
zero	O
or	O
one	O
and	O
let	O
px	O
pry	O
be	O
the	O
conditional	B
probability	B
the	O
predictors	O
that	O
the	O
response	B
equals	O
one	O
recall	B
the	O
logistic	B
regression	B
model	B
log	O
px	O
px	O
pxp	O
this	O
logit	B
is	O
the	O
log	O
of	O
the	O
odds	B
of	O
p	O
versus	O
p	O
which	O
represents	O
as	O
a	O
linear	B
function	B
of	O
the	O
predictors	O
a	O
natural	B
way	O
to	O
extend	O
to	O
allow	O
for	O
non-linear	B
relationships	O
is	O
to	O
use	O
the	O
model	B
log	O
px	O
px	O
fpxp	O
equation	O
is	O
a	O
logistic	B
regression	B
gam	O
it	O
has	O
all	O
the	O
same	O
pros	O
and	O
cons	O
as	O
discussed	O
in	O
the	O
previous	O
section	O
for	O
quantitative	B
responses	O
we	O
fit	O
a	O
gam	O
to	O
the	O
wage	B
data	B
in	O
order	O
to	O
predict	O
the	O
probability	B
that	O
an	O
individual	O
s	O
income	B
exceeds	O
per	O
year	O
the	O
gam	O
that	O
we	O
fit	O
takes	O
the	O
form	O
year	O
px	O
px	O
log	O
where	O
px	O
prwage	O
age	O
education	O
lab	O
non-linear	B
modeling	O
hs	O
coll	O
r	O
a	O
e	O
y	O
f	O
e	O
g	O
a	O
f	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
f	O
year	O
age	O
education	O
figure	O
for	O
the	O
wage	B
data	B
the	O
logistic	B
regression	B
gam	O
given	O
in	O
is	O
fit	O
to	O
the	O
binary	B
response	B
each	O
plot	B
displays	O
the	O
fitted	O
function	B
and	O
pointwise	O
standard	O
errors	O
the	O
first	O
function	B
is	O
linear	B
in	O
year	O
the	O
second	O
function	B
a	O
smoothing	B
spline	B
with	O
five	O
degrees	B
of	I
freedom	I
in	O
age	O
and	O
the	O
third	O
a	O
step	B
function	B
for	O
education	O
there	O
are	O
very	O
wide	O
standard	O
errors	O
for	O
the	O
first	O
level	B
of	O
education	O
once	O
again	O
is	O
fit	O
using	O
a	O
smoothing	B
spline	B
with	O
five	O
degrees	B
of	I
freedom	I
and	O
is	O
fit	O
as	O
a	O
step	B
function	B
by	O
creating	O
dummy	B
variables	O
for	O
each	O
of	O
the	O
levels	O
of	O
education	O
the	O
resulting	O
fit	O
is	O
shown	O
in	O
figure	O
the	O
last	O
panel	O
looks	O
suspicious	O
with	O
very	O
wide	O
confidence	O
intervals	O
for	O
level	B
in	O
fact	O
there	O
are	O
no	O
ones	O
for	O
that	O
category	O
no	O
individuals	O
with	O
less	O
than	O
a	O
high	O
school	O
education	O
make	O
more	O
than	O
per	O
year	O
hence	O
we	O
refit	O
the	O
gam	O
excluding	O
the	O
individuals	O
with	O
less	O
than	O
a	O
high	O
school	O
education	O
the	O
resulting	O
model	B
is	O
shown	O
in	O
figure	O
as	O
in	O
figures	O
and	O
all	O
three	O
panels	O
have	O
the	O
same	O
vertical	O
scale	O
this	O
allows	O
us	O
to	O
visually	O
assess	O
the	O
relative	O
contributions	O
of	O
each	O
of	O
the	O
variables	O
we	O
observe	O
that	O
age	O
and	O
education	O
have	O
a	O
much	O
larger	O
effect	O
than	O
year	O
on	O
the	O
probability	B
of	O
being	O
a	O
high	O
earner	O
lab	O
non-linear	B
modeling	O
in	O
this	O
lab	O
we	O
re-analyze	O
the	O
wage	B
data	B
considered	O
in	O
the	O
examples	O
throughout	O
this	O
chapter	O
in	O
order	O
to	O
illustrate	O
the	O
fact	O
that	O
many	O
of	O
the	O
complex	O
non-linear	B
fitting	O
procedures	O
discussed	O
can	O
be	O
easily	O
implemented	O
in	O
r	O
we	O
begin	O
by	O
loading	O
the	O
islr	O
library	O
which	O
contains	O
the	O
data	B
library	O
islr	O
attach	O
wage	B
moving	O
beyond	O
linearity	O
hs	O
coll	O
r	O
a	O
e	O
y	O
f	O
e	O
g	O
a	O
f	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
f	O
ryea	O
age	O
education	O
figure	O
the	O
same	O
model	B
is	O
fit	O
as	O
in	O
figure	O
this	O
time	O
excluding	O
the	O
observations	B
for	O
which	O
education	O
is	O
now	O
we	O
see	O
that	O
increased	O
education	O
tends	O
to	O
be	O
associated	O
with	O
higher	O
salaries	O
polynomial	B
regression	B
and	O
step	O
functions	O
we	O
now	O
examine	O
how	O
figure	O
was	O
produced	O
we	O
first	O
fit	O
the	O
model	B
using	O
the	O
following	O
command	O
fit	O
lm	O
wage	B
poly	O
age	O
data	B
wage	B
coef	O
summary	O
fit	O
intercept	B
poly	O
age	O
poly	O
age	O
poly	O
age	O
poly	O
age	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
this	O
syntax	O
fits	O
a	O
linear	B
model	B
using	O
the	O
lm	O
function	B
in	O
order	O
to	O
predict	O
wage	B
using	O
a	O
fourth-degree	O
polynomial	B
in	O
age	O
the	O
poly	O
command	O
allows	O
us	O
to	O
avoid	O
having	O
to	O
write	O
out	O
a	O
long	O
formula	O
with	O
powers	O
of	O
age	O
the	O
function	B
returns	O
a	O
matrix	O
whose	O
columns	O
are	O
a	O
basis	B
of	O
orthogonal	B
polynomials	O
which	O
essentially	O
means	O
that	O
each	O
column	O
is	O
a	O
linear	B
combination	I
of	O
the	O
variables	O
age	O
and	O
however	O
we	O
can	O
also	O
use	O
poly	O
to	O
obtain	O
age	O
and	O
directly	O
if	O
we	O
prefer	O
we	O
can	O
do	O
this	O
by	O
using	O
the	O
rawtrue	O
argument	B
to	O
the	O
poly	O
function	B
later	O
we	O
see	O
that	O
this	O
does	O
not	O
affect	O
the	O
model	B
in	O
a	O
meaningful	O
way	O
though	O
the	O
choice	O
of	O
basis	B
clearly	O
affects	O
the	O
coefficient	O
estimates	O
it	O
does	O
not	O
affect	O
the	O
fitted	O
values	O
obtained	O
lm	O
wage	B
poly	O
age	O
raw	O
t	O
data	B
wage	B
coef	O
summary	O
orthogonal	B
polynomial	B
e	O
intercept	B
poly	O
age	O
raw	O
t	O
e	O
poly	O
age	O
raw	O
t	O
e	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
e	O
e	O
e	O
lab	O
non-linear	B
modeling	O
poly	O
age	O
raw	O
t	O
e	O
poly	O
age	O
raw	O
t	O
e	O
e	O
e	O
there	O
are	O
several	O
other	O
equivalent	O
ways	O
of	O
fitting	O
this	O
model	B
which	O
show	O
case	O
the	O
flexibility	O
of	O
the	O
formula	O
language	O
in	O
r	O
for	O
example	O
lm	O
wage	B
age	O
i	O
age	O
i	O
age	O
i	O
age	O
data	B
wage	B
coef	O
intercept	B
e	O
age	O
e	O
i	O
age	O
e	O
i	O
age	O
e	O
i	O
age	O
e	O
this	O
simply	O
creates	O
the	O
polynomial	B
basis	B
functions	O
on	O
the	O
fly	O
taking	O
care	O
to	O
protect	O
terms	O
like	O
via	O
the	O
wrapper	B
function	B
i	O
symbol	O
has	O
wrapper	B
a	O
special	O
meaning	O
in	O
formulas	O
lm	O
wage	B
cbind	O
age	O
age	O
age	O
age	O
data	B
wage	B
this	O
does	O
the	O
same	O
more	O
compactly	O
using	O
the	O
cbind	O
function	B
for	O
building	O
a	O
matrix	O
from	O
a	O
collection	O
of	O
vectors	O
any	O
function	B
call	O
such	O
as	O
cbind	O
inside	O
a	O
formula	O
also	O
serves	O
as	O
a	O
wrapper	B
we	O
now	O
create	O
a	O
grid	O
of	O
values	O
for	O
age	O
at	O
which	O
we	O
want	O
predictions	O
and	O
then	O
call	O
the	O
generic	O
predict	O
function	B
specifying	O
that	O
we	O
want	O
standard	O
errors	O
as	O
well	O
agelims	O
range	O
age	O
age	O
grid	O
seq	O
from	O
agelims	O
to	O
agelims	O
preds	O
predict	O
fit	O
newdata	O
list	O
age	O
age	O
grid	O
se	O
true	O
se	O
bands	O
cbind	O
predsfit	O
predsse	O
fit	O
predsfit	O
predsse	O
fit	O
finally	O
we	O
plot	B
the	O
data	B
and	O
add	O
the	O
fit	O
from	O
the	O
polynomial	B
par	O
mfrow	O
c	O
mar	O
c	O
oma	O
c	O
plot	B
age	O
wage	B
xlim	O
agelims	O
cex	O
col	O
darkgrey	O
title	O
degree	O
polynomial	B
outer	O
t	O
lines	O
age	O
grid	O
predsfit	O
lwd	O
col	O
blue	O
matlines	O
age	O
grid	O
se	O
bands	O
lwd	O
col	O
blue	O
lty	O
here	O
the	O
mar	O
and	O
oma	O
arguments	O
to	O
par	O
allow	O
us	O
to	O
control	O
the	O
margins	O
of	O
the	O
plot	B
and	O
the	O
title	O
function	B
creates	O
a	O
figure	O
title	O
that	O
spans	O
both	O
subplots	O
we	O
mentioned	O
earlier	O
that	O
whether	O
or	O
not	O
an	O
orthogonal	B
set	B
of	O
basis	B
functions	O
is	O
produced	O
in	O
the	O
poly	O
function	B
will	O
not	O
affect	O
the	O
model	B
obtained	O
in	O
a	O
meaningful	O
way	O
what	O
do	O
we	O
mean	O
by	O
this	O
the	O
fitted	O
values	O
obtained	O
in	O
either	O
case	O
are	O
identical	O
title	O
predict	O
newdata	O
list	O
age	O
age	O
grid	O
se	O
true	O
max	O
abs	O
predsfit	O
e	O
in	O
performing	O
a	O
polynomial	B
regression	B
we	O
must	O
decide	O
on	O
the	O
degree	O
of	O
the	O
polynomial	B
to	O
use	O
one	O
way	O
to	O
do	O
this	O
is	O
by	O
using	O
hypothesis	B
tests	O
we	O
now	O
fit	O
models	O
ranging	O
from	O
linear	B
to	O
a	O
polynomial	B
and	O
seek	O
to	O
determine	O
the	O
simplest	O
model	B
which	O
is	O
sufficient	O
to	O
explain	O
the	O
relationship	O
moving	O
beyond	O
linearity	O
anova	O
analysis	B
of	I
variance	B
between	O
wage	B
and	O
age	O
we	O
use	O
the	O
anova	O
function	B
which	O
performs	O
an	O
analysis	B
of	I
variance	B
using	O
an	O
f-test	O
in	O
order	O
to	O
test	O
the	O
null	B
hypothesis	B
that	O
a	O
model	B
is	O
sufficient	O
to	O
explain	O
the	O
data	B
against	O
the	O
alternative	B
hypothesis	B
that	O
a	O
more	O
complex	O
model	B
is	O
required	O
in	O
order	O
to	O
use	O
the	O
anova	O
function	B
and	O
must	O
be	O
nested	O
models	O
the	O
predictors	O
in	O
must	O
be	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
in	O
this	O
case	O
we	O
fit	O
five	O
different	O
models	O
and	O
sequentially	O
compare	O
the	O
simpler	O
model	B
to	O
the	O
more	O
complex	O
model	B
fit	O
lm	O
wage	B
age	O
data	B
wage	B
fit	O
lm	O
wage	B
poly	O
age	O
data	B
wage	B
fit	O
lm	O
wage	B
poly	O
age	O
data	B
wage	B
fit	O
lm	O
wage	B
poly	O
age	O
data	B
wage	B
fit	O
lm	O
wage	B
poly	O
age	O
data	B
wage	B
anova	O
fit	O
fit	O
fit	O
fit	O
fit	O
analysis	B
of	I
variance	B
table	O
model	B
wage	B
age	O
model	B
wage	B
poly	O
age	O
model	B
wage	B
poly	O
age	O
model	B
wage	B
poly	O
age	O
model	B
wage	B
poly	O
age	O
res	O
df	O
rss	O
df	O
sum	O
of	O
sq	O
f	O
pr	O
f	O
codes	O
the	O
p-value	B
comparing	O
the	O
linear	B
model	B
to	O
the	O
quadratic	B
model	B
is	O
indicating	O
that	O
a	O
linear	B
fit	O
is	O
not	O
sufficient	O
simessentially	O
zero	O
ilarly	O
the	O
p-value	B
comparing	O
the	O
quadratic	B
model	B
to	O
the	O
cubic	B
model	B
is	O
very	O
low	O
so	O
the	O
quadratic	B
fit	O
is	O
also	O
insufficient	O
the	O
p-value	B
comparing	O
the	O
cubic	B
and	O
polynomials	O
model	B
and	O
model	B
is	O
approximately	O
while	O
the	O
polynomial	B
model	B
seems	O
unnecessary	O
because	O
its	O
p-value	B
is	O
hence	O
either	O
a	O
cubic	B
or	O
a	O
quartic	O
polynomial	B
appear	O
to	O
provide	O
a	O
reasonable	O
fit	O
to	O
the	O
data	B
but	O
lower-	O
or	O
higher-order	O
models	O
are	O
not	O
justified	O
in	O
this	O
case	O
instead	O
of	O
using	O
the	O
anova	O
function	B
we	O
could	O
have	O
obtained	O
these	O
p-values	O
more	O
succinctly	O
by	O
exploiting	O
the	O
fact	O
that	O
poly	O
creates	O
orthogonal	B
polynomials	O
coef	O
summary	O
fit	O
estimate	O
std	O
error	B
intercept	B
poly	O
age	O
poly	O
age	O
poly	O
age	O
t	O
value	O
pr	O
t	O
e	O
e	O
e	O
e	O
lab	O
non-linear	B
modeling	O
poly	O
age	O
poly	O
age	O
e	O
e	O
notice	O
that	O
the	O
p-values	O
are	O
the	O
same	O
and	O
in	O
fact	O
the	O
square	O
of	O
the	O
t-statistics	O
are	O
equal	O
to	O
the	O
f-statistics	O
from	O
the	O
anova	O
function	B
for	O
example	O
however	O
the	O
anova	O
method	O
works	O
whether	O
or	O
not	O
we	O
used	O
orthogonal	B
polynomials	O
it	O
also	O
works	O
when	O
we	O
have	O
other	O
terms	O
in	O
the	O
model	B
as	O
well	O
for	O
example	O
we	O
can	O
use	O
anova	O
to	O
compare	O
these	O
three	O
models	O
fit	O
lm	O
wage	B
education	O
age	O
data	B
wage	B
fit	O
lm	O
wage	B
education	O
poly	O
age	O
data	B
wage	B
fit	O
lm	O
wage	B
education	O
poly	O
age	O
data	B
wage	B
anova	O
fit	O
fit	O
fit	O
as	O
an	O
alternative	O
to	O
using	O
hypothesis	B
tests	O
and	O
anova	O
we	O
could	O
choose	O
the	O
polynomial	B
degree	O
using	O
cross-validation	B
as	O
discussed	O
in	O
chapter	O
next	O
we	O
consider	O
the	O
task	O
of	O
predicting	O
whether	O
an	O
individual	O
earns	O
more	O
than	O
per	O
year	O
we	O
proceed	O
much	O
as	O
before	O
except	O
that	O
first	O
we	O
create	O
the	O
appropriate	O
response	B
vector	B
and	O
then	O
apply	O
the	O
glm	O
function	B
using	O
familybinomial	O
in	O
order	O
to	O
fit	O
a	O
polynomial	B
logistic	B
regression	B
model	B
fit	O
glm	O
i	O
wage	B
poly	O
age	O
data	B
wage	B
family	O
binomial	O
note	O
that	O
we	O
again	O
use	O
the	O
wrapper	B
i	O
to	O
create	O
this	O
binary	B
response	B
variable	B
on	O
the	O
fly	O
the	O
expression	O
evaluates	O
to	O
a	O
logical	O
variable	B
containing	O
trues	O
and	O
falses	O
which	O
glm	O
coerces	O
to	O
binary	B
by	O
setting	O
the	O
trues	O
to	O
and	O
the	O
falses	O
to	O
once	O
again	O
we	O
make	O
predictions	O
using	O
the	O
predict	O
function	B
preds	O
predict	O
fit	O
newdata	O
list	O
age	O
age	O
grid	O
se	O
t	O
however	O
calculating	O
the	O
confidence	O
intervals	O
is	O
slightly	O
more	O
involved	O
than	O
in	O
the	O
linear	B
regression	B
case	O
the	O
default	B
prediction	B
type	O
for	O
a	O
glm	O
model	B
is	O
typelink	O
which	O
is	O
what	O
we	O
use	O
here	O
this	O
means	O
we	O
get	O
predictions	O
for	O
the	O
logit	B
that	O
is	O
we	O
have	O
fit	O
a	O
model	B
of	O
the	O
form	O
log	O
pry	O
pry	O
x	O
and	O
the	O
predictions	O
given	O
are	O
of	O
the	O
form	O
x	O
the	O
standard	O
errors	O
given	O
are	O
also	O
of	O
this	O
form	O
in	O
order	O
to	O
obtain	O
confidence	O
intervals	O
for	O
pry	O
we	O
use	O
the	O
transformation	O
pry	O
expx	O
expx	O
moving	O
beyond	O
linearity	O
pfit	O
exp	O
predsfit	O
exp	O
predsfit	O
se	O
bands	O
logit	B
cbind	O
predsfit	O
predsse	O
fit	O
predsfit	O
predsse	O
fit	O
se	O
bands	O
exp	O
se	O
bands	O
logit	B
exp	O
se	O
bands	O
logit	B
note	O
that	O
we	O
could	O
have	O
directly	O
computed	O
the	O
probabilities	O
by	O
selecting	O
the	O
typeresponse	O
option	O
in	O
the	O
predict	O
function	B
preds	O
predict	O
fit	O
newdata	O
list	O
age	O
age	O
grid	O
type	O
response	B
se	O
t	O
however	O
the	O
corresponding	O
confidence	O
intervals	O
would	O
not	O
have	O
been	O
sensible	O
because	O
we	O
would	O
end	O
up	O
with	O
negative	O
probabilities	O
finally	O
the	O
right-hand	O
plot	B
from	O
figure	O
was	O
made	O
as	O
follows	O
plot	B
age	O
i	O
wage	B
xlim	O
agelims	O
type	O
n	O
ylim	O
c	O
points	O
jitter	O
age	O
i	O
wage	B
cex	O
pch	O
col	O
darkgrey	O
lines	O
age	O
grid	O
pfit	O
lwd	O
col	O
blue	O
matlines	O
age	O
grid	O
se	O
bands	O
lwd	O
col	O
blue	O
lty	O
we	O
have	O
drawn	O
the	O
age	O
values	O
corresponding	O
to	O
the	O
observations	B
with	O
wage	B
values	O
above	O
as	O
gray	O
marks	O
on	O
the	O
top	O
of	O
the	O
plot	B
and	O
those	O
with	O
wage	B
values	O
below	O
are	O
shown	O
as	O
gray	O
marks	O
on	O
the	O
bottom	O
of	O
the	O
plot	B
we	O
used	O
the	O
jitter	O
function	B
to	O
jitter	O
the	O
age	O
values	O
a	O
bit	O
so	O
that	O
observations	B
with	O
the	O
same	O
age	O
value	O
do	O
not	O
cover	O
each	O
other	O
up	O
this	O
is	O
often	O
called	O
a	O
rug	B
plot	B
in	O
order	O
to	O
fit	O
a	O
step	B
function	B
as	O
discussed	O
in	O
section	O
we	O
use	O
the	O
cut	O
function	B
jitter	O
rug	B
plot	B
cut	O
table	O
cut	O
age	O
fit	O
lm	O
wage	B
cut	O
age	O
data	B
wage	B
coef	O
summary	O
fit	O
intercept	B
cut	O
age	O
cut	O
age	O
cut	O
age	O
estimate	O
std	O
error	B
t	O
value	O
pr	O
t	O
e	O
e	O
e	O
e	O
here	O
cut	O
automatically	O
picked	O
the	O
cutpoints	O
at	O
and	O
years	O
of	O
age	O
we	O
could	O
also	O
have	O
specified	O
our	O
own	O
cutpoints	O
directly	O
using	O
the	O
breaks	O
option	O
the	O
function	B
cut	O
returns	O
an	O
ordered	B
categorical	B
variable	B
the	O
lm	O
function	B
then	O
creates	O
a	O
set	B
of	O
dummy	B
variables	O
for	O
use	O
in	O
the	O
regression	B
the	O
category	O
is	O
left	O
out	O
so	O
the	O
intercept	B
coefficient	O
of	O
can	O
be	O
interpreted	O
as	O
the	O
average	B
salary	O
for	O
those	O
under	O
years	O
of	O
age	O
and	O
the	O
other	O
coefficients	O
can	O
be	O
interpreted	O
as	O
the	O
average	B
additional	O
salary	O
for	O
those	O
in	O
the	O
other	O
age	O
groups	O
we	O
can	O
produce	O
predictions	O
and	O
plots	O
just	O
as	O
we	O
did	O
in	O
the	O
case	O
of	O
the	O
polynomial	B
fit	O
lab	O
non-linear	B
modeling	O
splines	O
in	O
order	O
to	O
fit	O
regression	B
splines	O
in	O
r	O
we	O
use	O
the	O
splines	O
library	O
in	O
section	O
we	O
saw	O
that	O
regression	B
splines	O
can	O
be	O
fit	O
by	O
constructing	O
an	O
appropriate	O
matrix	O
of	O
basis	B
functions	O
the	O
bs	O
function	B
generates	O
the	O
entire	O
matrix	O
of	O
basis	B
functions	O
for	O
splines	O
with	O
the	O
specified	O
set	B
of	O
knots	O
by	O
default	B
cubic	B
splines	O
are	O
produced	O
fitting	O
wage	B
to	O
age	O
using	O
a	O
regression	B
spline	B
is	O
simple	B
bs	O
library	O
splines	O
fit	O
lm	O
wage	B
bs	O
age	O
knots	O
c	O
data	B
wage	B
pred	O
predict	O
fit	O
newdata	O
list	O
age	O
age	O
grid	O
se	O
t	O
plot	B
age	O
wage	B
col	O
gray	O
lines	O
age	O
grid	O
predfit	O
lwd	O
lines	O
age	O
grid	O
predfit	O
predse	O
lty	O
dashed	O
lines	O
age	O
grid	O
predfit	O
predse	O
lty	O
dashed	O
here	O
we	O
have	O
prespecified	O
knots	O
at	O
ages	O
and	O
this	O
produces	O
a	O
spline	B
with	O
six	O
basis	B
functions	O
that	O
a	O
cubic	B
spline	B
with	O
three	O
knots	O
has	O
seven	O
degrees	B
of	I
freedom	I
these	O
degrees	B
of	I
freedom	I
are	O
used	O
up	O
by	O
an	O
intercept	B
plus	O
six	O
basis	B
functions	O
we	O
could	O
also	O
use	O
the	O
df	O
option	O
to	O
produce	O
a	O
spline	B
with	O
knots	O
at	O
uniform	O
quantiles	O
of	O
the	O
data	B
dim	O
bs	O
age	O
knots	O
c	O
dim	O
bs	O
age	O
df	O
attr	O
bs	O
age	O
df	O
knots	O
in	O
this	O
case	O
r	O
chooses	O
knots	O
at	O
ages	O
and	O
which	O
correspond	O
to	O
the	O
and	O
percentiles	O
of	O
age	O
the	O
function	B
bs	O
also	O
has	O
a	O
degree	O
argument	B
so	O
we	O
can	O
fit	O
splines	O
of	O
any	O
degree	O
rather	O
than	O
the	O
default	B
degree	O
of	O
yields	O
a	O
cubic	B
spline	B
in	O
order	O
to	O
instead	O
fit	O
a	O
natural	B
spline	B
we	O
use	O
the	O
ns	O
function	B
here	O
we	O
fit	O
a	O
natural	B
spline	B
with	O
four	O
degrees	B
of	I
freedom	I
lm	O
wage	B
ns	O
age	O
df	O
data	B
wage	B
predict	O
newdata	O
list	O
age	O
age	O
grid	O
se	O
t	O
lines	O
age	O
grid	O
col	O
red	O
lwd	O
ns	O
as	O
with	O
the	O
bs	O
function	B
we	O
could	O
instead	O
specify	O
the	O
knots	O
directly	O
using	O
the	O
knots	O
option	O
in	O
order	O
to	O
fit	O
a	O
smoothing	B
spline	B
we	O
use	O
the	O
smooth	O
spline	B
function	B
figure	O
was	O
produced	O
with	O
the	O
following	O
code	O
smooth	O
spline	B
plot	B
age	O
wage	B
xlim	O
agelims	O
cex	O
col	O
darkgrey	O
title	O
smoothing	B
spline	B
fit	O
smooth	O
spline	B
age	O
wage	B
df	O
smooth	O
spline	B
age	O
wage	B
cv	O
true	O
lines	O
fit	O
col	O
red	O
lwd	O
moving	O
beyond	O
linearity	O
lines	O
col	O
blue	O
lwd	O
legend	O
topright	O
legend	O
c	O
df	O
df	O
col	O
c	O
red	O
blue	O
lty	O
lwd	O
cex	O
notice	O
that	O
in	O
the	O
first	O
call	O
to	O
smooth	O
spline	B
we	O
specified	O
the	O
function	B
then	O
determines	O
which	O
value	O
of	O
leads	O
to	O
degrees	B
of	I
freedom	I
in	O
the	O
second	O
call	O
to	O
smooth	O
spline	B
we	O
select	O
the	O
smoothness	O
level	B
by	O
crossvalidation	O
this	O
results	O
in	O
a	O
value	O
of	O
that	O
yields	O
degrees	B
of	I
freedom	I
in	O
order	O
to	O
perform	O
local	B
regression	B
we	O
use	O
the	O
loess	O
function	B
loess	O
plot	B
age	O
wage	B
xlim	O
agelims	O
cex	O
col	O
darkgrey	O
title	O
local	B
regression	B
fit	O
loess	O
wage	B
age	O
span	O
data	B
wage	B
loess	O
wage	B
age	O
span	O
data	B
wage	B
lines	O
age	O
grid	O
predict	O
fit	O
data	B
frame	I
age	O
age	O
grid	O
col	O
red	O
lwd	O
lines	O
age	O
grid	O
predict	O
data	B
frame	I
age	O
age	O
grid	O
col	O
blue	O
lwd	O
legend	O
topright	O
legend	O
c	O
span	O
span	O
col	O
c	O
red	O
blue	O
lty	O
lwd	O
cex	O
here	O
we	O
have	O
performed	O
local	B
linear	B
regression	B
using	O
spans	O
of	O
and	O
that	O
is	O
each	O
neighborhood	O
consists	O
of	O
or	O
of	O
the	O
observations	B
the	O
larger	O
the	O
span	O
the	O
smoother	B
the	O
fit	O
the	O
locfit	O
library	O
can	O
also	O
be	O
used	O
for	O
fitting	O
local	B
regression	B
models	O
in	O
r	O
gams	O
we	O
now	O
fit	O
a	O
gam	O
to	O
predict	O
wage	B
using	O
natural	B
spline	B
functions	O
of	O
year	O
and	O
age	O
treating	O
education	O
as	O
a	O
qualitative	B
predictor	B
as	O
in	O
since	O
this	O
is	O
just	O
a	O
big	O
linear	B
regression	B
model	B
using	O
an	O
appropriate	O
choice	O
of	O
basis	B
functions	O
we	O
can	O
simply	O
do	O
this	O
using	O
the	O
lm	O
function	B
lm	O
wage	B
ns	O
year	O
ns	O
age	O
education	O
data	B
wage	B
we	O
now	O
fit	O
the	O
model	B
using	O
smoothing	B
splines	O
rather	O
than	O
natural	B
splines	O
in	O
order	O
to	O
fit	O
more	O
general	O
sorts	O
of	O
gams	O
using	O
smoothing	B
splines	O
or	O
other	O
components	O
that	O
cannot	O
be	O
expressed	O
in	O
terms	O
of	O
basis	B
functions	O
and	O
then	O
fit	O
using	O
least	B
squares	I
regression	B
we	O
will	O
need	O
to	O
use	O
the	O
gam	O
library	O
in	O
r	O
the	O
s	O
function	B
which	O
is	O
part	O
of	O
the	O
gam	O
library	O
is	O
used	O
to	O
indicate	O
that	O
we	O
would	O
like	O
to	O
use	O
a	O
smoothing	B
spline	B
we	O
specify	O
that	O
the	O
function	B
of	O
year	O
should	O
have	O
degrees	B
of	I
freedom	I
and	O
that	O
the	O
function	B
of	O
age	O
will	O
have	O
degrees	B
of	I
freedom	I
since	O
education	O
is	O
qualitative	B
we	O
leave	O
it	O
as	O
is	O
and	O
it	O
is	O
converted	O
into	O
four	O
dummy	B
variables	O
we	O
use	O
the	O
gam	O
function	B
in	O
order	O
to	O
fit	O
a	O
gam	O
using	O
these	O
components	O
all	O
of	O
the	O
terms	O
in	O
are	O
fit	O
simultaneously	O
taking	O
each	O
other	O
into	O
account	O
to	O
explain	O
the	O
response	B
s	O
gam	O
library	O
gam	O
gam	O
gam	O
wage	B
s	O
year	O
s	O
age	O
education	O
data	B
wage	B
lab	O
non-linear	B
modeling	O
in	O
order	O
to	O
produce	O
figure	O
we	O
simply	O
call	O
the	O
plot	B
function	B
par	O
mfrow	O
c	O
plot	B
gam	O
se	O
true	O
col	O
blue	O
the	O
generic	O
plot	B
function	B
recognizes	O
that	O
is	O
an	O
object	O
of	O
class	O
gam	O
and	O
invokes	O
the	O
appropriate	O
plot	B
gam	O
method	O
conveniently	O
even	O
though	O
is	O
not	O
of	O
class	O
gam	O
but	O
rather	O
of	O
class	O
lm	O
we	O
can	O
still	O
use	O
plot	B
gam	O
on	O
it	O
figure	O
was	O
produced	O
using	O
the	O
following	O
expression	O
plot	B
gam	O
plot	B
gam	O
se	O
true	O
col	O
red	O
notice	O
here	O
we	O
had	O
to	O
use	O
plot	B
gam	O
rather	O
than	O
the	O
generic	O
plot	B
function	B
in	O
these	O
plots	O
the	O
function	B
of	O
year	O
looks	O
rather	O
linear	B
we	O
can	O
perform	O
a	O
series	O
of	O
anova	O
tests	O
in	O
order	O
to	O
determine	O
which	O
of	O
these	O
three	O
models	O
is	O
best	O
a	O
gam	O
that	O
excludes	O
year	O
a	O
gam	O
that	O
uses	O
a	O
linear	B
function	B
of	O
year	O
or	O
a	O
gam	O
that	O
uses	O
a	O
spline	B
function	B
of	O
year	O
gam	O
gam	O
wage	B
s	O
age	O
education	O
data	B
wage	B
gam	O
gam	O
wage	B
year	O
s	O
age	O
education	O
data	B
wage	B
anova	O
gam	O
gam	O
gam	O
test	O
f	O
analysis	B
of	O
deviance	B
table	O
model	B
wage	B
s	O
age	O
education	O
model	B
wage	B
year	O
s	O
age	O
education	O
model	B
wage	B
s	O
year	O
s	O
age	O
education	O
resid	O
df	O
resid	O
dev	O
df	O
deviance	B
f	O
pr	O
f	O
codes	O
we	O
find	O
that	O
there	O
is	O
compelling	O
evidence	O
that	O
a	O
gam	O
with	O
a	O
linear	B
function	B
of	O
year	O
is	O
better	O
than	O
a	O
gam	O
that	O
does	O
not	O
include	O
year	O
at	O
all	O
however	O
there	O
is	O
no	O
evidence	O
that	O
a	O
non-linear	B
function	B
of	O
year	O
is	O
needed	O
in	O
other	O
words	O
based	O
on	O
the	O
results	O
of	O
this	O
anova	O
is	O
preferred	O
the	O
summary	O
function	B
produces	O
a	O
summary	O
of	O
the	O
gam	O
fit	O
summary	O
gam	O
call	O
gam	O
formula	O
wage	B
s	O
year	O
s	O
age	O
education	O
data	B
wage	B
deviance	B
residuals	B
min	O
q	O
median	O
q	O
max	O
dispersion	O
parameter	B
for	O
gaussian	O
family	O
taken	O
to	O
be	O
null	B
deviance	B
on	O
degrees	B
of	I
freedom	I
residual	B
deviance	B
on	O
degrees	B
of	I
freedom	I
moving	O
beyond	O
linearity	O
aic	O
number	O
of	O
local	B
scoring	O
iterations	O
df	O
for	O
terms	O
and	O
f	O
values	O
for	O
n	O
o	O
n	O
p	O
a	O
r	O
a	O
m	O
e	O
t	O
r	O
i	O
c	O
effects	O
df	O
npar	O
df	O
npar	O
f	O
pr	O
f	O
intercept	B
s	O
year	O
s	O
age	O
education	O
codes	O
the	O
p-values	O
for	O
year	O
and	O
age	O
correspond	O
to	O
a	O
null	B
hypothesis	B
of	O
a	O
linear	B
relationship	O
versus	O
the	O
alternative	O
of	O
a	O
non-linear	B
relationship	O
the	O
large	O
p-value	B
for	O
year	O
reinforces	O
our	O
conclusion	O
from	O
the	O
anova	O
test	O
that	O
a	O
linear	B
function	B
is	O
adequate	O
for	O
this	O
term	B
however	O
there	O
is	O
very	O
clear	O
evidence	O
that	O
a	O
non-linear	B
term	B
is	O
required	O
for	O
age	O
we	O
can	O
make	O
predictions	O
from	O
gam	O
objects	O
just	O
like	O
from	O
lm	O
objects	O
using	O
the	O
predict	O
method	O
for	O
the	O
class	O
gam	O
here	O
we	O
make	O
predictions	O
on	O
the	O
training	O
set	B
preds	O
predict	O
gam	O
newdata	O
wage	B
we	O
can	O
also	O
use	O
local	B
regression	B
fits	O
as	O
building	O
blocks	O
in	O
a	O
gam	O
using	O
the	O
lo	O
function	B
gam	O
lo	O
gam	O
wage	B
s	O
year	O
df	O
lo	O
age	O
span	O
education	O
lo	O
data	B
wage	B
plot	B
gam	O
gam	O
lo	O
se	O
true	O
col	O
green	O
here	O
we	O
have	O
used	O
local	B
regression	B
for	O
the	O
age	O
term	B
with	O
a	O
span	O
of	O
we	O
can	O
also	O
use	O
the	O
lo	O
function	B
to	O
create	O
interactions	O
before	O
calling	O
the	O
gam	O
function	B
for	O
example	O
gam	O
lo	O
i	O
gam	O
wage	B
lo	O
year	O
age	O
span	O
education	O
data	B
wage	B
fits	O
a	O
two-term	O
model	B
in	O
which	O
the	O
first	O
term	B
is	O
an	O
interaction	B
between	O
year	O
and	O
age	O
fit	O
by	O
a	O
local	B
regression	B
surface	O
we	O
can	O
plot	B
the	O
resulting	O
two-dimensional	O
surface	O
if	O
we	O
first	O
install	O
the	O
akima	O
package	O
library	O
akima	O
plot	B
gam	O
lo	O
i	O
in	O
order	O
to	O
fit	O
a	O
logistic	B
regression	B
gam	O
we	O
once	O
again	O
use	O
the	O
i	O
function	B
in	O
constructing	O
the	O
binary	B
response	B
variable	B
and	O
set	B
familybinomial	O
gam	O
lr	O
gam	O
i	O
wage	B
year	O
s	O
age	O
df	O
education	O
family	O
binomial	O
data	B
wage	B
par	O
mfrow	O
c	O
plot	B
gam	O
lr	O
se	O
col	O
green	O
it	O
is	O
easy	O
to	O
see	O
that	O
there	O
are	O
no	O
high	O
earners	O
in	O
the	O
category	O
exercises	O
table	O
education	O
i	O
wage	B
education	O
hs	O
grad	O
hs	O
grad	O
some	O
college	B
college	B
grad	O
advanced	O
degree	O
false	O
true	O
hence	O
we	O
fit	O
a	O
logistic	B
regression	B
gam	O
using	O
all	O
but	O
this	O
category	O
this	O
provides	O
more	O
sensible	O
results	O
gam	O
lr	O
s	O
gam	O
i	O
wage	B
year	O
s	O
age	O
df	O
education	O
family	O
binomial	O
data	B
wage	B
subset	O
education	O
hs	O
grad	O
plot	B
gam	O
lr	O
se	O
col	O
green	O
exercises	O
conceptual	O
it	O
was	O
mentioned	O
in	O
the	O
chapter	O
that	O
a	O
cubic	B
regression	B
spline	B
with	O
one	O
knot	B
at	O
can	O
be	O
obtained	O
using	O
a	O
basis	B
of	O
the	O
form	O
x	O
if	O
x	O
and	O
equals	O
otherwise	O
we	O
will	O
now	O
show	O
that	O
a	O
function	B
of	O
the	O
form	O
where	O
f	O
is	O
indeed	O
a	O
cubic	B
regression	B
spline	B
regardless	O
of	O
the	O
values	O
of	O
find	O
a	O
cubic	B
polynomial	B
such	O
that	O
f	O
for	O
all	O
x	O
express	O
in	O
terms	O
of	O
find	O
a	O
cubic	B
polynomial	B
such	O
that	O
f	O
for	O
all	O
x	O
express	O
in	O
terms	O
of	O
we	O
have	O
now	O
established	O
that	O
f	O
is	O
a	O
piecewise	B
polynomial	B
show	O
that	O
that	O
is	O
f	O
is	O
continuous	B
at	O
show	O
that	O
f	O
is	O
continuous	B
at	O
that	O
is	O
f	O
f	O
moving	O
beyond	O
linearity	O
show	O
that	O
f	O
f	O
that	O
is	O
f	O
is	O
continuous	B
at	O
therefore	O
f	O
is	O
indeed	O
a	O
cubic	B
spline	B
hint	O
parts	O
and	O
of	O
this	O
problem	O
require	O
knowledge	O
of	O
singlevariable	O
calculus	O
as	O
a	O
reminder	O
given	O
a	O
cubic	B
polynomial	B
the	O
first	O
derivative	B
takes	O
the	O
form	O
f	O
and	O
the	O
second	O
derivative	B
takes	O
the	O
form	O
f	O
suppose	O
that	O
a	O
curve	O
g	O
is	O
computed	O
to	O
smoothly	O
fit	O
a	O
set	B
of	O
n	O
points	O
using	O
the	O
following	O
formula	O
g	O
arg	O
min	O
g	O
gmx	O
dx	O
where	O
gm	O
represents	O
the	O
mth	O
derivative	B
of	O
g	O
g	O
provide	O
example	O
sketches	O
of	O
g	O
in	O
each	O
of	O
the	O
following	O
scenarios	O
m	O
m	O
m	O
m	O
m	O
suppose	O
we	O
fit	O
a	O
curve	O
with	O
basis	B
functions	O
x	O
that	O
ix	O
equals	O
for	O
x	O
and	O
otherwise	O
we	O
fit	O
the	O
linear	B
regression	B
model	B
y	O
and	O
obtain	O
coefficient	O
estimates	O
sketch	O
the	O
estimated	O
curve	O
between	O
x	O
and	O
x	O
note	O
the	O
intercepts	O
slopes	O
and	O
other	O
relevant	O
information	O
suppose	O
we	O
fit	O
a	O
curve	O
with	O
basis	B
functions	O
x	O
x	O
x	O
x	O
we	O
fit	O
the	O
linear	B
regression	B
model	B
y	O
and	O
obtain	O
coefficient	O
estimates	O
sketch	O
the	O
estimated	O
curve	O
between	O
x	O
and	O
x	O
note	O
the	O
intercepts	O
slopes	O
and	O
other	O
relevant	O
information	O
consider	O
two	O
curves	O
and	O
defined	O
by	O
arg	O
min	O
g	O
arg	O
min	O
g	O
exercises	O
dx	O
dx	O
where	O
gm	O
represents	O
the	O
mth	O
derivative	B
of	O
g	O
as	O
will	O
or	O
have	O
the	O
smaller	O
training	O
rss	O
as	O
will	O
or	O
have	O
the	O
smaller	O
test	O
rss	O
for	O
will	O
or	O
have	O
the	O
smaller	O
training	O
and	O
test	O
rss	O
applied	O
in	O
this	O
exercise	O
you	O
will	O
further	O
analyze	O
the	O
wage	B
data	B
set	B
considered	O
throughout	O
this	O
chapter	O
perform	O
polynomial	B
regression	B
to	O
predict	O
wage	B
using	O
age	O
use	O
cross-validation	B
to	O
select	O
the	O
optimal	O
degree	O
d	O
for	O
the	O
polynomial	B
what	O
degree	O
was	O
chosen	O
and	O
how	O
does	O
this	O
compare	O
to	O
the	O
results	O
of	O
hypothesis	B
testing	O
using	O
anova	O
make	O
a	O
plot	B
of	O
the	O
resulting	O
polynomial	B
fit	O
to	O
the	O
data	B
fit	O
a	O
step	B
function	B
to	O
predict	O
wage	B
using	O
age	O
and	O
perform	O
crossvalidation	O
to	O
choose	O
the	O
optimal	O
number	O
of	O
cuts	O
make	O
a	O
plot	B
of	O
the	O
fit	O
obtained	O
the	O
wage	B
data	B
set	B
contains	O
a	O
number	O
of	O
other	O
features	O
not	O
explored	O
in	O
this	O
chapter	O
such	O
as	O
marital	O
status	O
job	O
class	O
and	O
others	O
explore	O
the	O
relationships	O
between	O
some	O
of	O
these	O
other	O
predictors	O
and	O
wage	B
and	O
use	O
non-linear	B
fitting	O
techniques	O
in	O
order	O
to	O
fit	O
flexible	O
models	O
to	O
the	O
data	B
create	O
plots	O
of	O
the	O
results	O
obtained	O
and	O
write	O
a	O
summary	O
of	O
your	O
findings	O
fit	O
some	O
of	O
the	O
non-linear	B
models	O
investigated	O
in	O
this	O
chapter	O
to	O
the	O
auto	B
data	B
set	B
is	O
there	O
evidence	O
for	O
non-linear	B
relationships	O
in	O
this	O
data	B
set	B
create	O
some	O
informative	O
plots	O
to	O
justify	O
your	O
answer	O
this	O
question	O
uses	O
the	O
variables	O
dis	O
weighted	B
mean	O
of	O
distances	O
to	O
five	O
boston	B
employment	O
centers	O
and	O
nox	O
oxides	O
concentration	O
in	O
parts	O
per	O
million	O
from	O
the	O
boston	B
data	B
we	O
will	O
treat	O
dis	O
as	O
the	O
predictor	B
and	O
nox	O
as	O
the	O
response	B
use	O
the	O
poly	O
function	B
to	O
fit	O
a	O
cubic	B
polynomial	B
regression	B
to	O
predict	O
nox	O
using	O
dis	O
report	O
the	O
regression	B
output	B
and	O
plot	B
the	O
resulting	O
data	B
and	O
polynomial	B
fits	O
moving	O
beyond	O
linearity	O
plot	B
the	O
polynomial	B
fits	O
for	O
a	O
range	O
of	O
different	O
polynomial	B
degrees	O
from	O
to	O
and	O
report	O
the	O
associated	O
residual	B
sum	B
of	I
squares	I
perform	O
cross-validation	B
or	O
another	O
approach	B
to	O
select	O
the	O
opti	O
mal	O
degree	O
for	O
the	O
polynomial	B
and	O
explain	O
your	O
results	O
use	O
the	O
bs	O
function	B
to	O
fit	O
a	O
regression	B
spline	B
to	O
predict	O
nox	O
using	O
dis	O
report	O
the	O
output	B
for	O
the	O
fit	O
using	O
four	O
degrees	B
of	I
freedom	I
how	O
did	O
you	O
choose	O
the	O
knots	O
plot	B
the	O
resulting	O
fit	O
now	O
fit	O
a	O
regression	B
spline	B
for	O
a	O
range	O
of	O
degrees	B
of	I
freedom	I
and	O
plot	B
the	O
resulting	O
fits	O
and	O
report	O
the	O
resulting	O
rss	O
describe	O
the	O
results	O
obtained	O
perform	O
cross-validation	B
or	O
another	O
approach	B
in	O
order	O
to	O
select	O
the	O
best	O
degrees	B
of	I
freedom	I
for	O
a	O
regression	B
spline	B
on	O
this	O
data	B
describe	O
your	O
results	O
this	O
question	O
relates	O
to	O
the	O
college	B
data	B
set	B
split	O
the	O
data	B
into	O
a	O
training	O
set	B
and	O
a	O
test	O
set	B
using	O
out-of-state	O
tuition	O
as	O
the	O
response	B
and	O
the	O
other	O
variables	O
as	O
the	O
predictors	O
perform	O
forward	B
stepwise	I
selection	B
on	O
the	O
training	O
set	B
in	O
order	O
to	O
identify	O
a	O
satisfactory	O
model	B
that	O
uses	O
just	O
a	O
subset	O
of	O
the	O
predictors	O
fit	O
a	O
gam	O
on	O
the	O
training	O
data	B
using	O
out-of-state	O
tuition	O
as	O
the	O
response	B
and	O
the	O
features	O
selected	O
in	O
the	O
previous	O
step	O
as	O
the	O
predictors	O
plot	B
the	O
results	O
and	O
explain	O
your	O
findings	O
evaluate	O
the	O
model	B
obtained	O
on	O
the	O
test	O
set	B
and	O
explain	O
the	O
results	O
obtained	O
for	O
which	O
variables	O
if	O
any	O
is	O
there	O
evidence	O
of	O
a	O
non-linear	B
relationship	O
with	O
the	O
response	B
in	O
section	O
it	O
was	O
mentioned	O
that	O
gams	O
are	O
generally	O
fit	O
using	O
a	O
backfitting	B
approach	B
the	O
idea	O
behind	O
backfitting	B
is	O
actually	O
quite	O
simple	B
we	O
will	O
now	O
explore	O
backfitting	B
in	O
the	O
context	O
of	O
multiple	B
linear	B
regression	B
suppose	O
that	O
we	O
would	O
like	O
to	O
perform	O
multiple	B
linear	B
regression	B
but	O
we	O
do	O
not	O
have	O
software	O
to	O
do	O
so	O
instead	O
we	O
only	O
have	O
software	O
to	O
perform	O
simple	B
linear	B
regression	B
therefore	O
we	O
take	O
the	O
following	O
iterative	O
approach	B
we	O
repeatedly	O
hold	O
all	O
but	O
one	O
coefficient	O
estimate	O
fixed	O
at	O
its	O
current	O
value	O
and	O
update	O
only	O
that	O
coefficient	O
estimate	O
using	O
a	O
simple	B
linear	B
regression	B
the	O
process	O
is	O
continued	O
until	O
convergence	O
that	O
is	O
until	O
the	O
coefficient	O
estimates	O
stop	O
changing	O
we	O
now	O
try	O
this	O
out	O
on	O
a	O
toy	O
example	O
exercises	O
generate	O
a	O
response	B
y	O
and	O
two	O
predictors	O
and	O
with	O
n	O
initialize	O
to	O
take	O
on	O
a	O
value	O
of	O
your	O
choice	O
it	O
does	O
not	O
matter	O
what	O
value	O
you	O
choose	O
keeping	O
fixed	O
fit	O
the	O
model	B
y	O
you	O
can	O
do	O
this	O
as	O
follows	O
a	O
lm	O
a	O
keeping	O
fixed	O
fit	O
the	O
model	B
y	O
you	O
can	O
do	O
this	O
as	O
follows	O
a	O
lm	O
a	O
write	O
a	O
for	B
loop	I
to	O
repeat	O
and	O
times	O
report	O
the	O
estimates	O
of	O
and	O
at	O
each	O
iteration	O
of	O
the	O
for	B
loop	I
create	O
a	O
plot	B
in	O
which	O
each	O
of	O
these	O
values	O
is	O
displayed	O
with	O
and	O
each	O
shown	O
in	O
a	O
different	O
color	O
compare	O
your	O
answer	O
in	O
to	O
the	O
results	O
of	O
simply	O
performing	O
multiple	B
linear	B
regression	B
to	O
predict	O
y	O
using	O
and	O
use	O
the	O
abline	O
function	B
to	O
overlay	O
those	O
multiple	B
linear	B
regression	B
coefficient	O
estimates	O
on	O
the	O
plot	B
obtained	O
in	O
on	O
this	O
data	B
set	B
how	O
many	O
backfitting	B
iterations	O
were	O
required	O
in	O
order	O
to	O
obtain	O
a	O
good	O
approximation	O
to	O
the	O
multiple	B
regression	B
coefficient	O
estimates	O
this	O
problem	O
is	O
a	O
continuation	O
of	O
the	O
previous	O
exercise	O
in	O
a	O
toy	O
example	O
with	O
p	O
show	O
that	O
one	O
can	O
approximate	O
the	O
multiple	B
linear	B
regression	B
coefficient	O
estimates	O
by	O
repeatedly	O
performing	O
simple	B
linear	B
regression	B
in	O
a	O
backfitting	B
procedure	O
how	O
many	O
backfitting	B
iterations	O
are	O
required	O
in	O
order	O
to	O
obtain	O
a	O
good	O
approximation	O
to	O
the	O
multiple	B
regression	B
coefficient	O
estimates	O
create	O
a	O
plot	B
to	O
justify	O
your	O
answer	O
tree-based	O
methods	O
in	O
this	O
chapter	O
we	O
describe	O
tree-based	O
methods	O
for	O
regression	B
and	O
classification	B
these	O
involve	O
stratifying	O
or	O
segmenting	O
the	O
predictor	B
space	O
into	O
a	O
number	O
of	O
simple	B
regions	O
in	O
order	O
to	O
make	O
a	O
prediction	B
for	O
a	O
given	O
observation	O
we	O
typically	O
use	O
the	O
mean	O
or	O
the	O
mode	B
of	O
the	O
training	O
observations	B
in	O
the	O
region	O
to	O
which	O
it	O
belongs	O
since	O
the	O
set	B
of	O
splitting	O
rules	O
used	O
to	O
segment	O
the	O
predictor	B
space	O
can	O
be	O
summarized	O
in	O
a	O
tree	B
these	O
types	O
of	O
approaches	O
are	O
known	O
as	O
decision	B
tree	B
methods	O
tree-based	O
methods	O
are	O
simple	B
and	O
useful	O
for	O
interpretation	O
however	O
they	O
typically	O
are	O
not	O
competitive	O
with	O
the	O
best	O
supervised	B
learning	I
approaches	O
such	O
as	O
those	O
seen	O
in	O
chapters	O
and	O
in	O
terms	O
of	O
prediction	B
accuracy	O
hence	O
in	O
this	O
chapter	O
we	O
also	O
introduce	O
bagging	B
random	O
forests	O
and	O
boosting	B
each	O
of	O
these	O
approaches	O
involves	O
producing	O
multiple	B
trees	O
which	O
are	O
then	O
combined	O
to	O
yield	O
a	O
single	B
consensus	O
prediction	B
we	O
will	O
see	O
that	O
combining	O
a	O
large	O
number	O
of	O
trees	O
can	O
often	O
result	O
in	O
dramatic	O
improvements	O
in	O
prediction	B
accuracy	O
at	O
the	O
expense	O
of	O
some	O
loss	O
in	O
interpretation	O
decision	B
tree	B
the	O
basics	O
of	O
decision	O
trees	O
decision	O
trees	O
can	O
be	O
applied	O
to	O
both	O
regression	B
and	O
classification	B
problems	O
we	O
first	O
consider	O
regression	B
problems	O
and	O
then	O
move	O
on	O
to	O
classification	B
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
tree-based	O
methods	O
years	O
hits	O
figure	O
for	O
the	O
hitters	B
data	B
a	O
regression	B
tree	B
for	O
predicting	O
the	O
log	O
salary	O
of	O
a	O
baseball	O
player	O
based	O
on	O
the	O
number	O
of	O
years	O
that	O
he	O
has	O
played	O
in	O
the	O
major	O
leagues	O
and	O
the	O
number	O
of	O
hits	O
that	O
he	O
made	O
in	O
the	O
previous	O
year	O
at	O
a	O
given	O
internal	B
node	O
the	O
label	O
the	O
form	O
xj	O
tk	O
indicates	O
the	O
left-hand	O
branch	B
emanating	O
from	O
that	O
split	O
and	O
the	O
right-hand	O
branch	B
corresponds	O
to	O
xj	O
tk	O
for	O
instance	O
the	O
split	O
at	O
the	O
top	O
of	O
the	O
tree	B
results	O
in	O
two	O
large	O
branches	O
the	O
left-hand	O
branch	B
corresponds	O
to	O
and	O
the	O
right-hand	O
branch	B
corresponds	O
to	O
the	O
tree	B
has	O
two	O
internal	B
nodes	O
and	O
three	O
terminal	B
nodes	O
or	O
leaves	O
the	O
number	O
in	O
each	O
leaf	B
is	O
the	O
mean	O
of	O
the	O
response	B
for	O
the	O
observations	B
that	O
fall	O
there	O
regression	B
trees	O
in	O
order	O
to	O
motivate	O
regression	B
trees	O
we	O
begin	O
with	O
a	O
simple	B
example	O
regression	B
tree	B
predicting	O
baseball	O
players	O
salaries	O
using	O
regression	B
trees	O
we	O
use	O
the	O
hitters	B
data	B
set	B
to	O
predict	O
a	O
baseball	O
player	O
s	O
salary	O
based	O
on	O
years	O
number	O
of	O
years	O
that	O
he	O
has	O
played	O
in	O
the	O
major	O
leagues	O
and	O
hits	O
number	O
of	O
hits	O
that	O
he	O
made	O
in	O
the	O
previous	O
year	O
we	O
first	O
remove	O
observations	B
that	O
are	O
missing	O
salary	O
values	O
and	O
log-transform	O
salary	O
so	O
that	O
its	O
distribution	B
has	O
more	O
of	O
a	O
typical	O
bell-shape	O
that	O
salary	O
is	O
measured	O
in	O
thousands	O
of	O
dollars	O
figure	O
shows	O
a	O
regression	B
tree	B
fit	O
to	O
this	O
data	B
it	O
consists	O
of	O
a	O
series	O
of	O
splitting	O
rules	O
starting	O
at	O
the	O
top	O
of	O
the	O
tree	B
the	O
top	O
split	O
assigns	O
observations	B
having	O
to	O
the	O
left	O
the	O
predicted	O
salary	O
years	O
and	O
hits	O
are	O
integers	O
in	O
these	O
data	B
the	O
tree	B
function	B
in	O
r	O
labels	O
the	O
splits	O
at	O
the	O
midpoint	O
between	O
two	O
adjacent	O
values	O
the	O
basics	O
of	O
decision	O
trees	O
s	O
t	O
i	O
h	O
years	O
figure	O
the	O
three-region	O
partition	O
for	O
the	O
hitters	B
data	B
set	B
from	O
the	O
regression	B
tree	B
illustrated	O
in	O
figure	O
for	O
these	O
players	O
is	O
given	O
by	O
the	O
mean	O
response	B
value	O
for	O
the	O
players	O
in	O
the	O
data	B
set	B
with	O
for	O
such	O
players	O
the	O
mean	O
log	O
salary	O
is	O
and	O
so	O
we	O
make	O
a	O
prediction	B
of	O
thousands	O
of	O
dollars	O
i	O
e	O
for	O
these	O
players	O
players	O
with	O
are	O
assigned	O
to	O
the	O
right	O
branch	B
and	O
then	O
that	O
group	O
is	O
further	O
subdivided	O
by	O
hits	O
overall	O
the	O
tree	B
stratifies	O
or	O
segments	O
the	O
players	O
into	O
three	O
regions	O
of	O
predictor	B
space	O
players	O
who	O
have	O
played	O
for	O
four	O
or	O
fewer	O
years	O
players	O
who	O
have	O
played	O
for	O
five	O
or	O
more	O
years	O
and	O
who	O
made	O
fewer	O
than	O
hits	O
last	O
year	O
and	O
players	O
who	O
have	O
played	O
for	O
five	O
or	O
more	O
years	O
and	O
who	O
made	O
at	O
least	O
hits	O
last	O
year	O
these	O
three	O
regions	O
can	O
be	O
written	O
as	O
and	O
figure	O
illustrates	O
the	O
regions	O
as	O
a	O
function	B
of	O
years	O
and	O
hits	O
the	O
predicted	O
salaries	O
for	O
these	O
three	O
groups	O
are	O
and	O
respectively	O
in	O
keeping	O
with	O
the	O
tree	B
analogy	O
the	O
regions	O
and	O
are	O
known	O
as	O
terminal	B
nodes	O
or	O
leaves	O
of	O
the	O
tree	B
as	O
is	O
the	O
case	O
for	O
figure	O
decision	O
trees	O
are	O
typically	O
drawn	O
upside	O
down	O
in	O
the	O
sense	O
that	O
the	O
leaves	O
are	O
at	O
the	O
bottom	O
of	O
the	O
tree	B
the	O
points	O
along	O
the	O
tree	B
where	O
the	O
predictor	B
space	O
is	O
split	O
are	O
referred	O
to	O
as	O
internal	B
nodes	O
in	O
figure	O
the	O
two	O
internal	B
nodes	O
are	O
indicated	O
by	O
the	O
text	O
and	O
we	O
refer	O
to	O
the	O
segments	O
of	O
the	O
trees	O
that	O
connect	O
the	O
nodes	O
as	O
branches	O
we	O
might	O
interpret	O
the	O
regression	B
tree	B
displayed	O
in	O
figure	O
as	O
follows	O
years	O
is	O
the	O
most	O
important	O
factor	B
in	O
determining	O
salary	O
and	O
players	O
with	O
less	O
experience	O
earn	O
lower	O
salaries	O
than	O
more	O
experienced	O
players	O
given	O
that	O
a	O
player	O
is	O
less	O
experienced	O
the	O
number	O
of	O
hits	O
that	O
he	O
made	O
in	O
the	O
previous	O
year	O
seems	O
to	O
play	O
little	O
role	O
in	O
his	O
salary	O
but	O
among	O
players	O
who	O
terminal	B
node	O
leaf	B
internal	B
node	O
branch	B
tree-based	O
methods	O
have	O
been	O
in	O
the	O
major	O
leagues	O
for	O
five	O
or	O
more	O
years	O
the	O
number	O
of	O
hits	O
made	O
in	O
the	O
previous	O
year	O
does	O
affect	O
salary	O
and	O
players	O
who	O
made	O
more	O
hits	O
last	O
year	O
tend	O
to	O
have	O
higher	O
salaries	O
the	O
regression	B
tree	B
shown	O
in	O
figure	O
is	O
likely	O
an	O
over-simplification	O
of	O
the	O
true	O
relationship	O
between	O
hits	O
years	O
and	O
salary	O
however	O
it	O
has	O
advantages	O
over	O
other	O
types	O
of	O
regression	B
models	O
as	O
those	O
seen	O
in	O
chapters	O
and	O
it	O
is	O
easier	O
to	O
interpret	O
and	O
has	O
a	O
nice	O
graphical	O
representation	O
prediction	B
via	O
stratification	O
of	O
the	O
feature	B
space	O
we	O
now	O
discuss	O
the	O
process	O
of	O
building	O
a	O
regression	B
tree	B
roughly	O
speaking	O
there	O
are	O
two	O
steps	O
we	O
divide	O
the	O
predictor	B
space	O
that	O
is	O
the	O
set	B
of	O
possible	O
values	O
for	O
xp	O
into	O
j	O
distinct	O
and	O
non-overlapping	O
regions	O
rj	O
for	O
every	O
observation	O
that	O
falls	O
into	O
the	O
region	O
rj	O
we	O
make	O
the	O
same	O
prediction	B
which	O
is	O
simply	O
the	O
mean	O
of	O
the	O
response	B
values	O
for	O
the	O
training	O
observations	B
in	O
rj	O
for	O
instance	O
suppose	O
that	O
in	O
step	O
we	O
obtain	O
two	O
regions	O
and	O
and	O
that	O
the	O
response	B
mean	O
of	O
the	O
training	O
observations	B
in	O
the	O
first	O
region	O
is	O
while	O
the	O
response	B
mean	O
of	O
the	O
training	O
observations	B
in	O
the	O
second	O
region	O
is	O
then	O
for	O
a	O
given	O
observation	O
x	O
x	O
if	O
x	O
we	O
will	O
predict	O
a	O
value	O
of	O
and	O
if	O
x	O
we	O
will	O
predict	O
a	O
value	O
of	O
we	O
now	O
elaborate	O
on	O
step	O
above	O
how	O
do	O
we	O
construct	O
the	O
regions	O
rj	O
in	O
theory	O
the	O
regions	O
could	O
have	O
any	O
shape	O
however	O
we	O
choose	O
to	O
divide	O
the	O
predictor	B
space	O
into	O
high-dimensional	B
rectangles	O
or	O
boxes	O
for	O
simplicity	O
and	O
for	O
ease	O
of	O
interpretation	O
of	O
the	O
resulting	O
predictive	O
model	B
the	O
goal	O
is	O
to	O
find	O
boxes	O
rj	O
that	O
minimize	O
the	O
rss	O
given	O
by	O
yrj	O
i	O
rj	O
where	O
yrj	O
is	O
the	O
mean	O
response	B
for	O
the	O
training	O
observations	B
within	O
the	O
jth	O
box	O
unfortunately	O
it	O
is	O
computationally	O
infeasible	O
to	O
consider	O
every	O
possible	O
partition	O
of	O
the	O
feature	B
space	O
into	O
j	O
boxes	O
for	O
this	O
reason	O
we	O
take	O
a	O
top-down	O
greedy	O
approach	B
that	O
is	O
known	O
as	O
recursive	B
binary	B
splitting	I
the	O
approach	B
is	O
top-down	O
because	O
it	O
begins	O
at	O
the	O
top	O
of	O
the	O
tree	B
which	O
point	O
all	O
observations	B
belong	O
to	O
a	O
single	B
region	O
and	O
then	O
successively	O
splits	O
the	O
predictor	B
space	O
each	O
split	O
is	O
indicated	O
via	O
two	O
new	O
branches	O
further	O
down	O
on	O
the	O
tree	B
it	O
is	O
greedy	O
because	O
at	O
each	O
step	O
of	O
the	O
tree-building	O
process	O
the	O
best	O
split	O
is	O
made	O
at	O
that	O
particular	O
step	O
rather	O
than	O
looking	O
ahead	O
and	O
picking	O
a	O
split	O
that	O
will	O
lead	O
to	O
a	O
better	O
tree	B
in	O
some	O
future	O
step	O
recursive	B
binary	B
splitting	I
the	O
basics	O
of	O
decision	O
trees	O
in	O
order	O
to	O
perform	O
recursive	B
binary	B
splitting	I
we	O
first	O
select	O
the	O
predictor	B
xj	O
and	O
the	O
cutpoint	O
s	O
such	O
that	O
splitting	O
the	O
predictor	B
space	O
into	O
the	O
regions	O
s	O
and	O
s	O
leads	O
to	O
the	O
greatest	O
possible	O
reduction	O
in	O
rss	O
notation	O
s	O
means	O
the	O
region	O
of	O
predictor	B
space	O
in	O
which	O
xj	O
takes	O
on	O
a	O
value	O
less	O
than	O
s	O
that	O
is	O
we	O
consider	O
all	O
predictors	O
xp	O
and	O
all	O
possible	O
values	O
of	O
the	O
cutpoint	O
s	O
for	O
each	O
of	O
the	O
predictors	O
and	O
then	O
choose	O
the	O
predictor	B
and	O
cutpoint	O
such	O
that	O
the	O
resulting	O
tree	B
has	O
the	O
lowest	O
rss	O
in	O
greater	O
detail	O
for	O
any	O
j	O
and	O
s	O
we	O
define	O
the	O
pair	O
of	O
half-planes	O
s	O
s	O
and	O
s	O
s	O
and	O
we	O
seek	O
the	O
value	O
of	O
j	O
and	O
s	O
that	O
minimize	O
the	O
equation	O
i	O
xi	O
i	O
xi	O
where	O
is	O
the	O
mean	O
response	B
for	O
the	O
training	O
observations	B
in	O
s	O
and	O
is	O
the	O
mean	O
response	B
for	O
the	O
training	O
observations	B
in	O
s	O
finding	O
the	O
values	O
of	O
j	O
and	O
s	O
that	O
minimize	O
can	O
be	O
done	O
quite	O
quickly	O
especially	O
when	O
the	O
number	O
of	O
features	O
p	O
is	O
not	O
too	O
large	O
next	O
we	O
repeat	O
the	O
process	O
looking	O
for	O
the	O
best	O
predictor	B
and	O
best	O
cutpoint	O
in	O
order	O
to	O
split	O
the	O
data	B
further	O
so	O
as	O
to	O
minimize	O
the	O
rss	O
within	O
each	O
of	O
the	O
resulting	O
regions	O
however	O
this	O
time	O
instead	O
of	O
splitting	O
the	O
entire	O
predictor	B
space	O
we	O
split	O
one	O
of	O
the	O
two	O
previously	O
identified	O
regions	O
we	O
now	O
have	O
three	O
regions	O
again	O
we	O
look	O
to	O
split	O
one	O
of	O
these	O
three	O
regions	O
further	O
so	O
as	O
to	O
minimize	O
the	O
rss	O
the	O
process	O
continues	O
until	O
a	O
stopping	O
criterion	O
is	O
reached	O
for	O
instance	O
we	O
may	O
continue	O
until	O
no	O
region	O
contains	O
more	O
than	O
five	O
observations	B
once	O
the	O
regions	O
rj	O
have	O
been	O
created	O
we	O
predict	O
the	O
response	B
for	O
a	O
given	O
test	O
observation	O
using	O
the	O
mean	O
of	O
the	O
training	O
observations	B
in	O
the	O
region	O
to	O
which	O
that	O
test	O
observation	O
belongs	O
a	O
five-region	O
example	O
of	O
this	O
approach	B
is	O
shown	O
in	O
figure	O
tree	B
pruning	B
the	O
process	O
described	O
above	O
may	O
produce	O
good	O
predictions	O
on	O
the	O
training	O
set	B
but	O
is	O
likely	O
to	O
overfit	O
the	O
data	B
leading	O
to	O
poor	O
test	O
set	B
performance	O
this	O
is	O
because	O
the	O
resulting	O
tree	B
might	O
be	O
too	O
complex	O
a	O
smaller	O
tree	B
with	O
fewer	O
splits	O
is	O
fewer	O
regions	O
rj	O
might	O
lead	O
to	O
lower	O
variance	B
and	O
better	O
interpretation	O
at	O
the	O
cost	O
of	O
a	O
little	O
bias	B
one	O
possible	O
alternative	O
to	O
the	O
process	O
described	O
above	O
is	O
to	O
build	O
the	O
tree	B
only	O
so	O
long	O
as	O
the	O
decrease	O
in	O
the	O
rss	O
due	O
to	O
each	O
split	O
exceeds	O
some	O
threshold	O
this	O
strategy	O
will	O
result	O
in	O
smaller	O
trees	O
but	O
is	O
too	O
short-sighted	O
since	O
a	O
seemingly	O
worthless	O
split	O
early	O
on	O
in	O
the	O
tree	B
might	O
be	O
followed	O
by	O
a	O
very	O
good	O
split	O
that	O
is	O
a	O
split	O
that	O
leads	O
to	O
a	O
large	O
reduction	O
in	O
rss	O
later	O
on	O
tree-based	O
methods	O
x	O
x	O
y	O
x	O
figure	O
top	O
left	O
a	O
partition	O
of	O
two-dimensional	O
feature	B
space	O
that	O
could	O
not	O
result	O
from	O
recursive	B
binary	B
splitting	I
top	O
right	O
the	O
output	B
of	O
recursive	B
binary	B
splitting	I
on	O
a	O
two-dimensional	O
example	O
bottom	O
left	O
a	O
tree	B
corresponding	O
to	O
the	O
partition	O
in	O
the	O
top	O
right	O
panel	O
bottom	O
right	O
a	O
perspective	O
plot	B
of	O
the	O
prediction	B
surface	O
corresponding	O
to	O
that	O
tree	B
therefore	O
a	O
better	O
strategy	O
is	O
to	O
grow	O
a	O
very	O
large	O
tree	B
and	O
then	O
prune	O
it	O
back	O
in	O
order	O
to	O
obtain	O
a	O
subtree	B
how	O
do	O
we	O
determine	O
the	O
best	O
prune	O
way	O
to	O
prune	O
the	O
tree	B
intuitively	O
our	O
goal	O
is	O
to	O
select	O
a	O
subtree	B
that	O
subtree	B
leads	O
to	O
the	O
lowest	O
test	O
error	B
rate	B
given	O
a	O
subtree	B
we	O
can	O
estimate	O
its	O
test	O
error	B
using	O
cross-validation	B
or	O
the	O
validation	B
set	B
approach	B
however	O
estimating	O
the	O
cross-validation	B
error	B
for	O
every	O
possible	O
subtree	B
would	O
be	O
too	O
cumbersome	O
since	O
there	O
is	O
an	O
extremely	O
large	O
number	O
of	O
possible	O
subtrees	O
instead	O
we	O
need	O
a	O
way	O
to	O
select	O
a	O
small	O
set	B
of	O
subtrees	O
for	O
consideration	O
cost	B
complexity	I
pruning	B
also	O
known	O
as	O
weakest	B
link	I
pruning	B
gives	O
us	O
a	O
way	O
to	O
do	O
just	O
this	O
rather	O
than	O
considering	O
every	O
possible	O
subtree	B
we	O
consider	O
a	O
sequence	O
of	O
trees	O
indexed	O
by	O
a	O
nonnegative	O
tuning	B
parameter	B
cost	B
complexity	I
pruning	B
weakest	B
link	I
pruning	B
the	O
basics	O
of	O
decision	O
trees	O
algorithm	O
building	O
a	O
regression	B
tree	B
use	O
recursive	B
binary	B
splitting	I
to	O
grow	O
a	O
large	O
tree	B
on	O
the	O
training	O
data	B
stopping	O
only	O
when	O
each	O
terminal	B
node	O
has	O
fewer	O
than	O
some	O
minimum	O
number	O
of	O
observations	B
apply	O
cost	B
complexity	I
pruning	B
to	O
the	O
large	O
tree	B
in	O
order	O
to	O
obtain	O
a	O
sequence	O
of	O
best	O
subtrees	O
as	O
a	O
function	B
of	O
use	O
k-fold	B
cross-validation	B
to	O
choose	O
that	O
is	O
divide	O
the	O
training	O
observations	B
into	O
k	O
folds	O
for	O
each	O
k	O
k	O
repeat	O
steps	O
and	O
on	O
all	O
but	O
the	O
kth	O
fold	O
of	O
the	O
training	O
data	B
evaluate	O
the	O
mean	O
squared	O
prediction	B
error	B
on	O
the	O
data	B
in	O
the	O
left-out	O
kth	O
fold	O
as	O
a	O
function	B
of	O
average	B
the	O
results	O
for	O
each	O
value	O
of	O
and	O
pick	O
to	O
minimize	O
the	O
average	B
error	B
return	O
the	O
subtree	B
from	O
step	O
that	O
corresponds	O
to	O
the	O
chosen	O
value	O
of	O
for	O
each	O
value	O
of	O
there	O
corresponds	O
a	O
subtree	B
t	O
such	O
that	O
i	O
xi	O
rm	O
is	O
as	O
small	O
as	O
possible	O
here	O
indicates	O
the	O
number	O
of	O
terminal	B
nodes	O
of	O
the	O
tree	B
t	O
rm	O
is	O
the	O
rectangle	O
the	O
subset	O
of	O
predictor	B
space	O
corresponding	O
to	O
the	O
mth	O
terminal	B
node	O
and	O
yrm	O
is	O
the	O
predicted	O
response	B
associated	O
with	O
rm	O
that	O
is	O
the	O
mean	O
of	O
the	O
training	O
observations	B
in	O
rm	O
the	O
tuning	B
parameter	B
controls	O
a	O
trade-off	B
between	O
the	O
subtree	B
s	O
complexity	O
and	O
its	O
fit	O
to	O
the	O
training	O
data	B
when	O
then	O
the	O
subtree	B
t	O
will	O
simply	O
equal	O
because	O
then	O
just	O
measures	O
the	O
training	O
error	B
however	O
as	O
increases	O
there	O
is	O
a	O
price	O
to	O
pay	O
for	O
having	O
a	O
tree	B
with	O
many	O
terminal	B
nodes	O
and	O
so	O
the	O
quantity	O
will	O
tend	O
to	O
be	O
minimized	O
for	O
a	O
smaller	O
subtree	B
equation	O
is	O
reminiscent	O
of	O
the	O
lasso	B
from	O
chapter	O
in	O
which	O
a	O
similar	O
formulation	O
was	O
used	O
in	O
order	O
to	O
control	O
the	O
complexity	O
of	O
a	O
linear	B
model	B
it	O
turns	O
out	O
that	O
as	O
we	O
increase	O
from	O
zero	O
in	O
branches	O
get	O
pruned	O
from	O
the	O
tree	B
in	O
a	O
nested	O
and	O
predictable	O
fashion	O
so	O
obtaining	O
the	O
whole	O
sequence	O
of	O
subtrees	O
as	O
a	O
function	B
of	O
is	O
easy	O
we	O
can	O
select	O
a	O
value	O
of	O
using	O
a	O
validation	B
set	B
or	O
using	O
cross-validation	B
we	O
then	O
return	O
to	O
the	O
full	O
data	B
set	B
and	O
obtain	O
the	O
subtree	B
corresponding	O
to	O
this	O
process	O
is	O
summarized	O
in	O
algorithm	O
tree-based	O
methods	O
years	O
rbi	O
hits	O
putouts	O
years	O
years	O
walks	O
walks	O
runs	O
rbi	O
years	O
figure	O
regression	B
tree	B
analysis	B
for	O
the	O
hitters	B
data	B
the	O
unpruned	O
tree	B
that	O
results	O
from	O
top-down	O
greedy	O
splitting	O
on	O
the	O
training	O
data	B
is	O
shown	O
figures	O
and	O
display	O
the	O
results	O
of	O
fitting	O
and	O
pruning	B
a	O
regression	B
tree	B
on	O
the	O
hitters	B
data	B
using	O
nine	O
of	O
the	O
features	O
first	O
we	O
randomly	O
divided	O
the	O
data	B
set	B
in	O
half	O
yielding	O
observations	B
in	O
the	O
training	O
set	B
and	O
observations	B
in	O
the	O
test	O
set	B
we	O
then	O
built	O
a	O
large	O
regression	B
tree	B
on	O
the	O
training	O
data	B
and	O
varied	O
in	O
in	O
order	O
to	O
create	O
subtrees	O
with	O
different	O
numbers	O
of	O
terminal	B
nodes	O
finally	O
we	O
performed	O
six-fold	O
crossvalidation	O
in	O
order	O
to	O
estimate	O
the	O
cross-validated	O
mse	B
of	O
the	O
trees	O
as	O
a	O
function	B
of	O
chose	O
to	O
perform	O
six-fold	O
cross-validation	B
because	O
is	O
an	O
exact	O
multiple	B
of	O
six	O
the	O
unpruned	O
regression	B
tree	B
is	O
shown	O
in	O
figure	O
the	O
green	O
curve	O
in	O
figure	O
shows	O
the	O
cv	O
error	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
while	O
the	O
orange	O
curve	O
indicates	O
the	O
test	O
error	B
also	O
shown	O
are	O
standard	B
error	B
bars	O
around	O
the	O
estimated	O
errors	O
for	O
reference	O
the	O
training	O
error	B
curve	O
is	O
shown	O
in	O
black	O
the	O
cv	O
error	B
is	O
a	O
reasonable	O
approximation	O
of	O
the	O
test	O
error	B
the	O
cv	O
error	B
takes	O
on	O
its	O
cv	O
error	B
is	O
computed	O
as	O
a	O
function	B
of	O
it	O
is	O
convenient	O
to	O
display	O
the	O
result	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
leaves	O
this	O
is	O
based	O
on	O
the	O
relationship	O
between	O
and	O
in	O
the	O
original	O
tree	B
grown	O
to	O
all	O
the	O
training	O
data	B
the	O
basics	O
of	O
decision	O
trees	O
training	O
cross	O
validation	O
test	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
tree	B
size	O
figure	O
regression	B
tree	B
analysis	B
for	O
the	O
hitters	B
data	B
the	O
training	O
cross-validation	B
and	O
test	O
mse	B
are	O
shown	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
terminal	B
nodes	O
in	O
the	O
pruned	O
tree	B
standard	B
error	B
bands	O
are	O
displayed	O
the	O
minimum	O
cross-validation	B
error	B
occurs	O
at	O
a	O
tree	B
size	O
of	O
three	O
minimum	O
for	O
a	O
three-node	O
tree	B
while	O
the	O
test	O
error	B
also	O
dips	O
down	O
at	O
the	O
three-node	O
tree	B
it	O
takes	O
on	O
its	O
lowest	O
value	O
at	O
the	O
ten-node	O
tree	B
the	O
pruned	O
tree	B
containing	O
three	O
terminal	B
nodes	O
is	O
shown	O
in	O
figure	O
classification	B
trees	O
a	O
classification	B
tree	B
is	O
very	O
similar	O
to	O
a	O
regression	B
tree	B
except	O
that	O
it	O
is	O
used	O
to	O
predict	O
a	O
qualitative	B
response	B
rather	O
than	O
a	O
quantitative	B
one	O
recall	B
that	O
for	O
a	O
regression	B
tree	B
the	O
predicted	O
response	B
for	O
an	O
observation	O
is	O
given	O
by	O
the	O
mean	O
response	B
of	O
the	O
training	O
observations	B
that	O
belong	O
to	O
the	O
same	O
terminal	B
node	O
in	O
contrast	B
for	O
a	O
classification	B
tree	B
we	O
predict	O
that	O
each	O
observation	O
belongs	O
to	O
the	O
most	O
commonly	O
occurring	O
class	O
of	O
training	O
observations	B
in	O
the	O
region	O
to	O
which	O
it	O
belongs	O
in	O
interpreting	O
the	O
results	O
of	O
a	O
classification	B
tree	B
we	O
are	O
often	O
interested	O
not	O
only	O
in	O
the	O
class	O
prediction	B
corresponding	O
to	O
a	O
particular	O
terminal	B
node	O
region	O
but	O
also	O
in	O
the	O
class	O
proportions	O
among	O
the	O
training	O
observations	B
that	O
fall	O
into	O
that	O
region	O
the	O
task	O
of	O
growing	O
a	O
classification	B
tree	B
is	O
quite	O
similar	O
to	O
the	O
task	O
of	O
growing	O
a	O
regression	B
tree	B
just	O
as	O
in	O
the	O
regression	B
setting	O
we	O
use	O
recursive	B
binary	B
splitting	I
to	O
grow	O
a	O
classification	B
tree	B
however	O
in	O
the	O
classification	B
setting	O
rss	O
cannot	O
be	O
used	O
as	O
a	O
criterion	O
for	O
making	O
the	O
binary	B
splits	O
a	O
natural	B
alternative	O
to	O
rss	O
is	O
the	O
classification	B
error	B
rate	B
since	O
we	O
plan	O
to	O
assign	O
an	O
observation	O
in	O
a	O
given	O
region	O
to	O
the	O
most	O
commonly	O
occurring	O
class	O
of	O
training	O
observations	B
in	O
that	O
region	O
the	O
classification	B
error	B
rate	B
is	O
simply	O
the	O
fraction	O
of	O
the	O
training	O
observations	B
in	O
that	O
region	O
that	O
do	O
not	O
belong	O
to	O
the	O
most	O
common	O
class	O
classification	B
tree	B
classification	B
error	B
rate	B
tree-based	O
methods	O
e	O
max	O
k	O
pmk	O
here	O
pmk	O
represents	O
the	O
proportion	O
of	O
training	O
observations	B
in	O
the	O
mth	O
region	O
that	O
are	O
from	O
the	O
kth	O
class	O
however	O
it	O
turns	O
out	O
that	O
classification	B
error	B
is	O
not	O
sufficiently	O
sensitive	O
for	O
tree-growing	O
and	O
in	O
practice	O
two	O
other	O
measures	O
are	O
preferable	O
the	O
gini	B
index	I
is	O
defined	O
by	O
g	O
pmk	O
gini	B
index	I
entropy	B
a	O
measure	O
of	O
total	O
variance	B
across	O
the	O
k	O
classes	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
the	O
gini	B
index	I
takes	O
on	O
a	O
small	O
value	O
if	O
all	O
of	O
the	O
pmk	O
s	O
are	O
close	O
to	O
zero	O
or	O
one	O
for	O
this	O
reason	O
the	O
gini	B
index	I
is	O
referred	O
to	O
as	O
a	O
measure	O
of	O
node	O
purity	B
a	O
small	O
value	O
indicates	O
that	O
a	O
node	O
contains	O
predominantly	O
observations	B
from	O
a	O
single	B
class	O
an	O
alternative	O
to	O
the	O
gini	B
index	I
is	O
entropy	B
given	O
by	O
d	O
pmk	O
log	O
pmk	O
since	O
pmk	O
it	O
follows	O
that	O
pmk	O
log	O
pmk	O
one	O
can	O
show	O
that	O
the	O
entropy	B
will	O
take	O
on	O
a	O
value	O
near	O
zero	O
if	O
the	O
pmk	O
s	O
are	O
all	O
near	O
zero	O
or	O
near	O
one	O
therefore	O
like	O
the	O
gini	B
index	I
the	O
entropy	B
will	O
take	O
on	O
a	O
small	O
value	O
if	O
the	O
mth	O
node	O
is	O
pure	O
in	O
fact	O
it	O
turns	O
out	O
that	O
the	O
gini	B
index	I
and	O
the	O
entropy	B
are	O
quite	O
similar	O
numerically	O
when	O
building	O
a	O
classification	B
tree	B
either	O
the	O
gini	B
index	I
or	O
the	O
entropy	B
are	O
typically	O
used	O
to	O
evaluate	O
the	O
quality	O
of	O
a	O
particular	O
split	O
since	O
these	O
two	O
approaches	O
are	O
more	O
sensitive	O
to	O
node	O
purity	B
than	O
is	O
the	O
classification	B
error	B
rate	B
any	O
of	O
these	O
three	O
approaches	O
might	O
be	O
used	O
when	O
pruning	B
the	O
tree	B
but	O
the	O
classification	B
error	B
rate	B
is	O
preferable	O
if	O
prediction	B
accuracy	O
of	O
the	O
final	O
pruned	O
tree	B
is	O
the	O
goal	O
figure	O
shows	O
an	O
example	O
on	O
the	O
heart	B
data	B
set	B
these	O
data	B
contain	O
a	O
binary	B
outcome	O
hd	O
for	O
patients	O
who	O
presented	O
with	O
chest	O
pain	O
an	O
outcome	O
value	O
of	O
yes	O
indicates	O
the	O
presence	O
of	O
heart	B
disease	O
based	O
on	O
an	O
angiographic	O
test	O
while	O
no	O
means	O
no	O
heart	B
disease	O
there	O
are	O
predictors	O
including	O
age	O
sex	O
chol	O
cholesterol	O
measurement	O
and	O
other	O
heart	B
and	O
lung	O
function	B
measurements	O
cross-validation	B
results	O
in	O
a	O
tree	B
with	O
six	O
terminal	B
nodes	O
in	O
our	O
discussion	O
thus	O
far	O
we	O
have	O
assumed	O
that	O
the	O
predictor	B
variables	O
take	O
on	O
continuous	B
values	O
however	O
decision	O
trees	O
can	O
be	O
constructed	O
even	O
in	O
the	O
presence	O
of	O
qualitative	B
predictor	B
variables	O
for	O
instance	O
in	O
the	O
heart	B
data	B
some	O
of	O
the	O
predictors	O
such	O
as	O
sex	O
thal	O
stress	O
test	O
and	O
chestpain	O
are	O
qualitative	B
therefore	O
a	O
split	O
on	O
one	O
of	O
these	O
variables	O
amounts	O
to	O
assigning	O
some	O
of	O
the	O
qualitative	B
values	O
to	O
one	O
branch	B
and	O
the	O
basics	O
of	O
decision	O
trees	O
thala	O
ca	O
ca	O
maxhr	O
chestpainbc	O
age	O
thalb	O
restecg	O
slope	B
oldpeak	O
yes	O
no	O
no	O
chestpaina	O
yes	O
yes	O
yes	O
no	O
yes	O
maxhr	O
maxhr	O
no	O
no	O
restbp	O
chol	O
no	O
chol	O
sex	O
yes	O
no	O
no	O
no	O
yes	O
no	O
yes	O
r	O
o	O
r	O
r	O
e	O
training	O
cross	O
validation	O
test	O
thala	O
ca	O
ca	O
maxhr	O
chestpainbc	O
yes	O
yes	O
no	O
no	O
no	O
yes	O
tree	B
size	O
figure	O
heart	B
data	B
top	O
the	O
unpruned	O
tree	B
bottom	O
left	O
cross	O
error	B
training	O
and	O
test	O
error	B
for	O
different	O
sizes	O
of	O
the	O
pruned	O
tree	B
bottom	O
right	O
the	O
pruned	O
tree	B
corresponding	O
to	O
the	O
minimal	O
cross-validation	B
error	B
assigning	O
the	O
remaining	O
to	O
the	O
other	O
branch	B
in	O
figure	O
some	O
of	O
the	O
internal	B
nodes	O
correspond	O
to	O
splitting	O
qualitative	B
variables	O
for	O
instance	O
the	O
top	O
internal	B
node	O
corresponds	O
to	O
splitting	O
thal	O
the	O
text	O
thala	O
indicates	O
that	O
the	O
left-hand	O
branch	B
coming	O
out	O
of	O
that	O
node	O
consists	O
of	O
observations	B
with	O
the	O
first	O
value	O
of	O
the	O
thal	O
variable	B
and	O
the	O
right-hand	O
node	O
consists	O
of	O
the	O
remaining	O
observations	B
or	O
reversible	O
defects	O
the	O
text	O
chestpainbc	O
two	O
splits	O
down	O
the	O
tree	B
on	O
the	O
left	O
indicates	O
that	O
the	O
left-hand	O
branch	B
coming	O
out	O
of	O
that	O
node	O
consists	O
of	O
observations	B
with	O
the	O
second	O
and	O
third	O
values	O
of	O
the	O
chestpain	O
variable	B
where	O
the	O
possible	O
values	O
are	O
typical	O
angina	O
atypical	O
angina	O
non-anginal	O
pain	O
and	O
asymptomatic	O
tree-based	O
methods	O
figure	O
has	O
a	O
surprising	O
characteristic	O
some	O
of	O
the	O
splits	O
yield	O
two	O
terminal	B
nodes	O
that	O
have	O
the	O
same	O
predicted	O
value	O
for	O
instance	O
consider	O
the	O
split	O
near	O
the	O
bottom	O
right	O
of	O
the	O
unpruned	O
tree	B
regardless	O
of	O
the	O
value	O
of	O
restecg	O
a	O
response	B
value	O
of	O
yes	O
is	O
predicted	O
for	O
those	O
observations	B
why	O
then	O
is	O
the	O
split	O
performed	O
at	O
all	O
the	O
split	O
is	O
performed	O
because	O
it	O
leads	O
to	O
increased	O
node	O
purity	B
that	O
is	O
all	O
of	O
the	O
observations	B
corresponding	O
to	O
the	O
right-hand	O
leaf	B
have	O
a	O
response	B
value	O
of	O
yes	O
whereas	O
of	O
those	O
corresponding	O
to	O
the	O
left-hand	O
leaf	B
have	O
a	O
response	B
value	O
of	O
yes	O
why	O
is	O
node	O
purity	B
important	O
suppose	O
that	O
we	O
have	O
a	O
test	O
observation	O
that	O
belongs	O
to	O
the	O
region	O
given	O
by	O
that	O
right-hand	O
leaf	B
then	O
we	O
can	O
be	O
pretty	O
certain	O
that	O
its	O
response	B
value	O
is	O
yes	O
in	O
contrast	B
if	O
a	O
test	O
observation	O
belongs	O
to	O
the	O
region	O
given	O
by	O
the	O
left-hand	O
leaf	B
then	O
its	O
response	B
value	O
is	O
probably	O
yes	O
but	O
we	O
are	O
much	O
less	O
certain	O
even	O
though	O
the	O
split	O
does	O
not	O
reduce	O
the	O
classification	B
error	B
it	O
improves	O
the	O
gini	B
index	I
and	O
the	O
entropy	B
which	O
are	O
more	O
sensitive	O
to	O
node	O
purity	B
trees	O
versus	O
linear	B
models	O
regression	B
and	O
classification	B
trees	O
have	O
a	O
very	O
different	O
flavor	O
from	O
the	O
more	O
classical	O
approaches	O
for	O
regression	B
and	O
classification	B
presented	O
in	O
chapters	O
and	O
in	O
particular	O
linear	B
regression	B
assumes	O
a	O
model	B
of	O
the	O
form	O
f	O
xj	O
j	O
cm	O
rm	O
whereas	O
regression	B
trees	O
assume	O
a	O
model	B
of	O
the	O
form	O
f	O
where	O
rm	O
represent	O
a	O
partition	O
of	O
feature	B
space	O
as	O
in	O
figure	O
which	O
model	B
is	O
better	O
it	O
depends	O
on	O
the	O
problem	O
at	O
hand	O
if	O
the	O
relationship	O
between	O
the	O
features	O
and	O
the	O
response	B
is	O
well	O
approximated	O
by	O
a	O
linear	B
model	B
as	O
in	O
then	O
an	O
approach	B
such	O
as	O
linear	B
regression	B
will	O
likely	O
work	O
well	O
and	O
will	O
outperform	O
a	O
method	O
such	O
as	O
a	O
regression	B
tree	B
that	O
does	O
not	O
exploit	O
this	O
linear	B
structure	O
if	O
instead	O
there	O
is	O
a	O
highly	O
non-linear	B
and	O
complex	O
relationship	O
between	O
the	O
features	O
and	O
the	O
response	B
as	O
indicated	O
by	O
model	B
then	O
decision	O
trees	O
may	O
outperform	O
classical	O
approaches	O
an	O
illustrative	O
example	O
is	O
displayed	O
in	O
figure	O
the	O
relative	O
performances	O
of	O
tree-based	O
and	O
classical	O
approaches	O
can	O
be	O
assessed	O
by	O
estimating	O
the	O
test	O
error	B
using	O
either	O
cross-validation	B
or	O
the	O
validation	B
set	B
approach	B
of	O
course	O
other	O
considerations	O
beyond	O
simply	O
test	O
error	B
may	O
come	O
into	O
play	O
in	O
selecting	O
a	O
statistical	O
learning	O
method	O
for	O
instance	O
in	O
certain	O
settings	O
prediction	B
using	O
a	O
tree	B
may	O
be	O
preferred	O
for	O
the	O
sake	O
of	O
interpretability	B
and	O
visualization	O
the	O
basics	O
of	O
decision	O
trees	O
x	O
x	O
x	O
x	O
figure	O
top	O
row	O
a	O
two-dimensional	O
classification	B
example	O
in	O
which	O
the	O
true	O
decision	B
boundary	I
is	O
linear	B
and	O
is	O
indicated	O
by	O
the	O
shaded	O
regions	O
a	O
classical	O
approach	B
that	O
assumes	O
a	O
linear	B
boundary	O
will	O
outperform	O
a	O
decision	B
tree	B
that	O
performs	O
splits	O
parallel	O
to	O
the	O
axes	O
bottom	O
row	O
here	O
the	O
true	O
decision	B
boundary	I
is	O
non-linear	B
here	O
a	O
linear	B
model	B
is	O
unable	O
to	O
capture	O
the	O
true	O
decision	B
boundary	I
whereas	O
a	O
decision	B
tree	B
is	O
successful	O
advantages	O
and	O
disadvantages	O
of	O
trees	O
decision	O
trees	O
for	O
regression	B
and	O
classification	B
have	O
a	O
number	O
of	O
advantages	O
over	O
the	O
more	O
classical	O
approaches	O
seen	O
in	O
chapters	O
and	O
trees	O
are	O
very	O
easy	O
to	O
explain	O
to	O
people	O
in	O
fact	O
they	O
are	O
even	O
easier	O
to	O
explain	O
than	O
linear	B
regression	B
some	O
people	O
believe	O
that	O
decision	O
trees	O
more	O
closely	O
mirror	O
human	O
decision-making	O
than	O
do	O
the	O
regression	B
and	O
classification	B
approaches	O
seen	O
in	O
previous	O
chapters	O
trees	O
can	O
be	O
displayed	O
graphically	O
and	O
are	O
easily	O
interpreted	O
even	O
by	O
a	O
non-expert	O
if	O
they	O
are	O
small	O
trees	O
can	O
easily	O
handle	O
qualitative	B
predictors	O
without	O
the	O
need	O
to	O
create	O
dummy	B
variables	O
tree-based	O
methods	O
unfortunately	O
trees	O
generally	O
do	O
not	O
have	O
the	O
same	O
level	B
of	O
predictive	O
accuracy	O
as	O
some	O
of	O
the	O
other	O
regression	B
and	O
classification	B
approaches	O
seen	O
in	O
this	O
book	O
additionally	O
trees	O
can	O
be	O
very	O
non-robust	O
in	O
other	O
words	O
a	O
small	O
change	O
in	O
the	O
data	B
can	O
cause	O
a	O
large	O
change	O
in	O
the	O
final	O
estimated	O
tree	B
however	O
by	O
aggregating	O
many	O
decision	O
trees	O
using	O
methods	O
like	O
bagging	B
random	O
forests	O
and	O
boosting	B
the	O
predictive	O
performance	O
of	O
trees	O
can	O
be	O
substantially	O
improved	O
we	O
introduce	O
these	O
concepts	O
in	O
the	O
next	O
section	O
bagging	B
random	O
forests	O
boosting	B
bagging	B
random	O
forests	O
and	O
boosting	B
use	O
trees	O
as	O
building	O
blocks	O
to	O
construct	O
more	O
powerful	O
prediction	B
models	O
bagging	B
the	O
bootstrap	B
introduced	O
in	O
chapter	O
is	O
an	O
extremely	O
powerful	O
idea	O
it	O
is	O
used	O
in	O
many	O
situations	O
in	O
which	O
it	O
is	O
hard	O
or	O
even	O
impossible	O
to	O
directly	O
compute	O
the	O
standard	O
deviation	O
of	O
a	O
quantity	O
of	O
interest	O
we	O
see	O
here	O
that	O
the	O
bootstrap	B
can	O
be	O
used	O
in	O
a	O
completely	O
different	O
context	O
in	O
order	O
to	O
improve	O
statistical	O
learning	O
methods	O
such	O
as	O
decision	O
trees	O
the	O
decision	O
trees	O
discussed	O
in	O
section	O
suffer	O
from	O
high	O
variance	B
this	O
means	O
that	O
if	O
we	O
split	O
the	O
training	O
data	B
into	O
two	O
parts	O
at	O
random	O
and	O
fit	O
a	O
decision	B
tree	B
to	O
both	O
halves	O
the	O
results	O
that	O
we	O
get	O
could	O
be	O
quite	O
different	O
in	O
contrast	B
a	O
procedure	O
with	O
low	O
variance	B
will	O
yield	O
similar	O
results	O
if	O
applied	O
repeatedly	O
to	O
distinct	O
data	B
sets	O
linear	B
regression	B
tends	O
to	O
have	O
low	O
variance	B
if	O
the	O
ratio	O
of	O
n	O
to	O
p	O
is	O
moderately	O
large	O
bootstrap	B
aggregation	O
or	O
bagging	B
is	O
a	O
general-purpose	O
procedure	O
for	O
reducing	O
the	O
variance	B
of	O
a	O
statistical	O
learning	O
method	O
we	O
introduce	O
it	O
here	O
because	O
it	O
is	O
particularly	O
useful	O
and	O
frequently	O
used	O
in	O
the	O
context	O
of	O
decision	O
trees	O
recall	B
that	O
given	O
a	O
set	B
of	O
n	O
independent	B
observations	B
zn	O
each	O
with	O
variance	B
the	O
variance	B
of	O
the	O
mean	O
z	O
of	O
the	O
observations	B
is	O
given	O
by	O
in	O
other	O
words	O
averaging	O
a	O
set	B
of	O
observations	B
reduces	O
variance	B
hence	O
a	O
natural	B
way	O
to	O
reduce	O
the	O
variance	B
and	O
hence	O
increase	O
the	O
prediction	B
accuracy	O
of	O
a	O
statistical	O
learning	O
method	O
is	O
to	O
take	O
many	O
training	O
sets	O
from	O
the	O
population	O
build	O
a	O
separate	O
prediction	B
model	B
using	O
each	O
training	O
set	B
and	O
average	B
the	O
resulting	O
predictions	O
in	O
other	O
words	O
we	O
could	O
calculate	O
f	O
f	O
f	O
bx	O
using	O
b	O
separate	O
training	O
sets	O
and	O
average	B
them	O
in	O
order	O
to	O
obtain	O
a	O
single	B
low-variance	O
statistical	O
learning	O
model	B
bagging	B
bagging	B
random	O
forests	O
boosting	B
given	O
by	O
favgx	O
b	O
f	O
bx	O
of	O
course	O
this	O
is	O
not	O
practical	O
because	O
we	O
generally	O
do	O
not	O
have	O
access	O
to	O
multiple	B
training	O
sets	O
instead	O
we	O
can	O
bootstrap	B
by	O
taking	O
repeated	O
samples	O
from	O
the	O
training	O
data	B
set	B
in	O
this	O
approach	B
we	O
generate	O
b	O
different	O
bootstrapped	O
training	O
data	B
sets	O
we	O
then	O
train	B
our	O
method	O
on	O
bx	O
and	O
finally	O
average	B
the	O
bth	O
bootstrapped	O
training	O
set	B
in	O
order	O
to	O
get	O
f	O
all	O
the	O
predictions	O
to	O
obtain	O
fbagx	O
b	O
bx	O
f	O
this	O
is	O
called	O
bagging	B
while	O
bagging	B
can	O
improve	O
predictions	O
for	O
many	O
regression	B
methods	O
it	O
is	O
particularly	O
useful	O
for	O
decision	O
trees	O
to	O
apply	O
bagging	B
to	O
regression	B
trees	O
we	O
simply	O
construct	O
b	O
regression	B
trees	O
using	O
b	O
bootstrapped	O
training	O
sets	O
and	O
average	B
the	O
resulting	O
predictions	O
these	O
trees	O
are	O
grown	O
deep	O
and	O
are	O
not	O
pruned	O
hence	O
each	O
individual	O
tree	B
has	O
high	O
variance	B
but	O
low	O
bias	B
averaging	O
these	O
b	O
trees	O
reduces	O
the	O
variance	B
bagging	B
has	O
been	O
demonstrated	O
to	O
give	O
impressive	O
improvements	O
in	O
accuracy	O
by	O
combining	O
together	O
hundreds	O
or	O
even	O
thousands	O
of	O
trees	O
into	O
a	O
single	B
procedure	O
thus	O
far	O
we	O
have	O
described	O
the	O
bagging	B
procedure	O
in	O
the	O
regression	B
context	O
to	O
predict	O
a	O
quantitative	B
outcome	O
y	O
how	O
can	O
bagging	B
be	O
extended	O
to	O
a	O
classification	B
problem	O
where	O
y	O
is	O
qualitative	B
in	O
that	O
situation	O
there	O
are	O
a	O
few	O
possible	O
approaches	O
but	O
the	O
simplest	O
is	O
as	O
follows	O
for	O
a	O
given	O
test	O
observation	O
we	O
can	O
record	O
the	O
class	O
predicted	O
by	O
each	O
of	O
the	O
b	O
trees	O
and	O
take	O
a	O
majority	B
vote	I
the	O
overall	O
prediction	B
is	O
the	O
most	O
commonly	O
occurring	O
class	O
among	O
the	O
b	O
predictions	O
figure	O
shows	O
the	O
results	O
from	O
bagging	B
trees	O
on	O
the	O
heart	B
data	B
the	O
test	O
error	B
rate	B
is	O
shown	O
as	O
a	O
function	B
of	O
b	O
the	O
number	O
of	O
trees	O
constructed	O
using	O
bootstrapped	O
training	O
data	B
sets	O
we	O
see	O
that	O
the	O
bagging	B
test	O
error	B
rate	B
is	O
slightly	O
lower	O
in	O
this	O
case	O
than	O
the	O
test	O
error	B
rate	B
obtained	O
from	O
a	O
single	B
tree	B
the	O
number	O
of	O
trees	O
b	O
is	O
not	O
a	O
critical	O
parameter	B
with	O
bagging	B
using	O
a	O
very	O
large	O
value	O
of	O
b	O
will	O
not	O
lead	O
to	O
overfitting	B
in	O
practice	O
we	O
use	O
a	O
value	O
of	O
b	O
sufficiently	O
large	O
that	O
the	O
error	B
has	O
settled	O
down	O
using	O
b	O
is	O
sufficient	O
to	O
achieve	O
good	O
performance	O
in	O
this	O
example	O
out-of-bag	B
error	B
estimation	O
it	O
turns	O
out	O
that	O
there	O
is	O
a	O
very	O
straightforward	O
way	O
to	O
estimate	O
the	O
test	O
error	B
of	O
a	O
bagged	O
model	B
without	O
the	O
need	O
to	O
perform	O
cross-validation	B
or	O
the	O
validation	B
set	B
approach	B
recall	B
that	O
the	O
key	O
to	O
bagging	B
is	O
that	O
trees	O
are	O
repeatedly	O
fit	O
to	O
bootstrapped	O
subsets	O
of	O
the	O
observations	B
one	O
can	O
show	O
majority	B
vote	I
tree-based	O
methods	O
r	O
o	O
r	O
r	O
e	O
test	O
bagging	B
test	O
randomforest	O
oob	O
bagging	B
oob	O
randomforest	O
number	O
of	O
trees	O
figure	O
bagging	B
and	O
random	B
forest	I
results	O
for	O
the	O
heart	B
data	B
the	O
test	O
error	B
and	O
orange	O
is	O
shown	O
as	O
a	O
function	B
of	O
b	O
the	O
number	O
of	O
bootstrapped	O
p	O
the	O
dashed	O
line	B
training	O
sets	O
used	O
random	O
forests	O
were	O
applied	O
with	O
m	O
indicates	O
the	O
test	O
error	B
resulting	O
from	O
a	O
single	B
classification	B
tree	B
the	O
green	O
and	O
blue	O
traces	O
show	O
the	O
oob	O
error	B
which	O
in	O
this	O
case	O
is	O
considerably	O
lower	O
that	O
on	O
average	B
each	O
bagged	O
tree	B
makes	O
use	O
of	O
around	O
two-thirds	O
of	O
the	O
the	O
remaining	O
one-third	O
of	O
the	O
observations	B
not	O
used	O
to	O
fit	O
a	O
given	O
bagged	O
tree	B
are	O
referred	O
to	O
as	O
the	O
out-of-bag	B
observations	B
we	O
can	O
predict	O
the	O
response	B
for	O
the	O
ith	O
observation	O
using	O
each	O
of	O
the	O
trees	O
in	O
which	O
that	O
observation	O
was	O
oob	O
this	O
will	O
yield	O
around	O
predictions	O
for	O
the	O
ith	O
observation	O
in	O
order	O
to	O
obtain	O
a	O
single	B
prediction	B
for	O
the	O
ith	O
observation	O
we	O
can	O
average	B
these	O
predicted	O
responses	O
regression	B
is	O
the	O
goal	O
or	O
can	O
take	O
a	O
majority	B
vote	I
classification	B
is	O
the	O
goal	O
this	O
leads	O
to	O
a	O
single	B
oob	O
prediction	B
for	O
the	O
ith	O
observation	O
an	O
oob	O
prediction	B
can	O
be	O
obtained	O
in	O
this	O
way	O
for	O
each	O
of	O
the	O
n	O
observations	B
from	O
which	O
the	O
overall	O
oob	O
mse	B
a	O
regression	B
problem	O
or	O
classification	B
error	B
a	O
classification	B
problem	O
can	O
be	O
computed	O
the	O
resulting	O
oob	O
error	B
is	O
a	O
valid	O
estimate	O
of	O
the	O
test	O
error	B
for	O
the	O
bagged	O
model	B
since	O
the	O
response	B
for	O
each	O
observation	O
is	O
predicted	O
using	O
only	O
the	O
trees	O
that	O
were	O
not	O
fit	O
using	O
that	O
observation	O
figure	O
displays	O
the	O
oob	O
error	B
on	O
the	O
heart	B
data	B
it	O
can	O
be	O
shown	O
that	O
with	O
b	O
sufficiently	O
large	O
oob	O
error	B
is	O
virtually	O
equivalent	O
to	O
leave-one-out	B
cross-validation	B
error	B
the	O
oob	O
approach	B
for	O
estimating	O
out-of-bag	B
relates	O
to	O
exercise	O
of	O
chapter	O
bagging	B
random	O
forests	O
boosting	B
the	O
test	O
error	B
is	O
particularly	O
convenient	O
when	O
performing	O
bagging	B
on	O
large	O
data	B
sets	O
for	O
which	O
cross-validation	B
would	O
be	O
computationally	O
onerous	O
variable	B
importance	B
measures	O
as	O
we	O
have	O
discussed	O
bagging	B
typically	O
results	O
in	O
improved	O
accuracy	O
over	O
prediction	B
using	O
a	O
single	B
tree	B
unfortunately	O
however	O
it	O
can	O
be	O
difficult	O
to	O
interpret	O
the	O
resulting	O
model	B
recall	B
that	O
one	O
of	O
the	O
advantages	O
of	O
decision	O
trees	O
is	O
the	O
attractive	O
and	O
easily	O
interpreted	O
diagram	O
that	O
results	O
such	O
as	O
the	O
one	O
displayed	O
in	O
figure	O
however	O
when	O
we	O
bag	O
a	O
large	O
number	O
of	O
trees	O
it	O
is	O
no	O
longer	O
possible	O
to	O
represent	O
the	O
resulting	O
statistical	O
learning	O
procedure	O
using	O
a	O
single	B
tree	B
and	O
it	O
is	O
no	O
longer	O
clear	O
which	O
variables	O
are	O
most	O
important	O
to	O
the	O
procedure	O
thus	O
bagging	B
improves	O
prediction	B
accuracy	O
at	O
the	O
expense	O
of	O
interpretability	B
although	O
the	O
collection	O
of	O
bagged	O
trees	O
is	O
much	O
more	O
difficult	O
to	O
interpret	O
than	O
a	O
single	B
tree	B
one	O
can	O
obtain	O
an	O
overall	O
summary	O
of	O
the	O
importance	B
of	O
each	O
predictor	B
using	O
the	O
rss	O
bagging	B
regression	B
trees	O
or	O
the	O
gini	B
index	I
bagging	B
classification	B
trees	O
in	O
the	O
case	O
of	O
bagging	B
regression	B
trees	O
we	O
can	O
record	O
the	O
total	O
amount	O
that	O
the	O
rss	O
is	O
decreased	O
due	O
to	O
splits	O
over	O
a	O
given	O
predictor	B
averaged	O
over	O
all	O
b	O
trees	O
a	O
large	O
value	O
indicates	O
an	O
important	O
predictor	B
similarly	O
in	O
the	O
context	O
of	O
bagging	B
classification	B
trees	O
we	O
can	O
add	O
up	O
the	O
total	O
amount	O
that	O
the	O
gini	B
index	I
is	O
decreased	O
by	O
splits	O
over	O
a	O
given	O
predictor	B
averaged	O
over	O
all	O
b	O
trees	O
a	O
graphical	O
representation	O
of	O
the	O
variable	B
importances	O
in	O
the	O
heart	B
data	B
is	O
shown	O
in	O
figure	O
we	O
see	O
the	O
mean	O
decrease	O
in	O
gini	B
index	I
for	O
each	O
variable	B
relative	O
to	O
the	O
largest	O
the	O
variables	O
with	O
the	O
largest	O
mean	O
decrease	O
in	O
gini	B
index	I
are	O
thal	O
ca	O
and	O
chestpain	O
random	O
forests	O
random	O
forests	O
provide	O
an	O
improvement	O
over	O
bagged	O
trees	O
by	O
way	O
of	O
a	O
small	O
tweak	O
that	O
decorrelates	O
the	O
trees	O
as	O
in	O
bagging	B
we	O
build	O
a	O
number	O
of	O
decision	O
trees	O
on	O
bootstrapped	O
training	O
samples	O
but	O
when	O
building	O
these	O
decision	O
trees	O
each	O
time	O
a	O
split	O
in	O
a	O
tree	B
is	O
considered	O
a	O
random	O
sample	O
of	O
m	O
predictors	O
is	O
chosen	O
as	O
split	O
candidates	O
from	O
the	O
full	O
set	B
of	O
p	O
predictors	O
the	O
split	O
is	O
allowed	O
to	O
use	O
only	O
one	O
of	O
those	O
m	O
predictors	O
a	O
fresh	O
sample	O
of	O
p	O
that	O
is	O
the	O
number	O
of	O
predictors	O
considered	O
at	O
each	O
split	O
is	O
approximately	O
equal	O
to	O
the	O
square	O
root	O
of	O
the	O
total	O
number	O
of	O
predictors	O
out	O
of	O
the	O
for	O
the	O
heart	B
data	B
m	O
predictors	O
is	O
taken	O
at	O
each	O
split	O
and	O
typically	O
we	O
choose	O
m	O
in	O
other	O
words	O
in	O
building	O
a	O
random	B
forest	I
at	O
each	O
split	O
in	O
the	O
tree	B
the	O
algorithm	O
is	O
not	O
even	O
allowed	O
to	O
consider	O
a	O
majority	O
of	O
the	O
available	O
predictors	O
this	O
may	O
sound	O
crazy	O
but	O
it	O
has	O
a	O
clever	O
rationale	O
suppose	O
that	O
there	O
is	O
one	O
very	O
strong	O
predictor	B
in	O
the	O
data	B
set	B
along	O
with	O
a	O
number	O
of	O
other	O
moderately	O
strong	O
predictors	O
then	O
in	O
the	O
collection	O
of	O
bagged	O
variable	B
importance	B
random	B
forest	I
tree-based	O
methods	O
fbs	O
restecg	O
exang	O
sex	O
slope	B
chol	O
age	O
restbp	O
maxhr	O
oldpeak	O
chestpain	O
ca	O
thal	O
variable	B
importance	B
figure	O
a	O
variable	B
importance	B
plot	B
for	O
the	O
heart	B
data	B
variable	B
importance	B
is	O
computed	O
using	O
the	O
mean	O
decrease	O
in	O
gini	B
index	I
and	O
expressed	O
relative	O
to	O
the	O
maximum	O
trees	O
most	O
or	O
all	O
of	O
the	O
trees	O
will	O
use	O
this	O
strong	O
predictor	B
in	O
the	O
top	O
split	O
consequently	O
all	O
of	O
the	O
bagged	O
trees	O
will	O
look	O
quite	O
similar	O
to	O
each	O
other	O
hence	O
the	O
predictions	O
from	O
the	O
bagged	O
trees	O
will	O
be	O
highly	O
correlated	O
unfortunately	O
averaging	O
many	O
highly	O
correlated	O
quantities	O
does	O
not	O
lead	O
to	O
as	O
large	O
of	O
a	O
reduction	O
in	O
variance	B
as	O
averaging	O
many	O
uncorrelated	O
quantities	O
in	O
particular	O
this	O
means	O
that	O
bagging	B
will	O
not	O
lead	O
to	O
a	O
substantial	O
reduction	O
in	O
variance	B
over	O
a	O
single	B
tree	B
in	O
this	O
setting	O
random	O
forests	O
overcome	O
this	O
problem	O
by	O
forcing	O
each	O
split	O
to	O
consider	O
only	O
a	O
subset	O
of	O
the	O
predictors	O
therefore	O
on	O
average	B
mp	O
of	O
the	O
splits	O
will	O
not	O
even	O
consider	O
the	O
strong	O
predictor	B
and	O
so	O
other	O
predictors	O
will	O
have	O
more	O
of	O
a	O
chance	O
we	O
can	O
think	O
of	O
this	O
process	O
as	O
decorrelating	O
the	O
trees	O
thereby	O
making	O
the	O
average	B
of	O
the	O
resulting	O
trees	O
less	O
variable	B
and	O
hence	O
more	O
reliable	O
the	O
main	O
difference	O
between	O
bagging	B
and	O
random	O
forests	O
is	O
the	O
choice	O
of	O
predictor	B
subset	O
size	O
m	O
for	O
instance	O
if	O
a	O
random	B
forest	I
is	O
built	O
using	O
m	O
p	O
then	O
this	O
amounts	O
simply	O
to	O
bagging	B
on	O
the	O
heart	B
data	B
random	O
p	O
leads	O
to	O
a	O
reduction	O
in	O
both	O
test	O
error	B
and	O
oob	O
error	B
forests	O
using	O
m	O
over	O
bagging	B
using	O
a	O
small	O
value	O
of	O
m	O
in	O
building	O
a	O
random	B
forest	I
will	O
typically	O
be	O
helpful	O
when	O
we	O
have	O
a	O
large	O
number	O
of	O
correlated	O
predictors	O
we	O
applied	O
random	O
forests	O
to	O
a	O
high-dimensional	B
biological	O
data	B
set	B
consisting	O
of	O
expression	O
measurements	O
of	O
genes	O
measured	O
on	O
tissue	O
samples	O
from	O
patients	O
there	O
are	O
around	O
genes	O
in	O
humans	O
and	O
individual	O
genes	O
bagging	B
random	O
forests	O
boosting	B
have	O
different	O
levels	O
of	O
activity	O
or	O
expression	O
in	O
particular	O
cells	O
tissues	O
and	O
biological	O
conditions	O
in	O
this	O
data	B
set	B
each	O
of	O
the	O
patient	O
samples	O
has	O
a	O
qualitative	B
label	O
with	O
different	O
levels	O
either	O
normal	O
or	O
of	O
different	O
types	O
of	O
cancer	O
our	O
goal	O
was	O
to	O
use	O
random	O
forests	O
to	O
predict	O
cancer	O
type	O
based	O
on	O
the	O
genes	O
that	O
have	O
the	O
largest	O
variance	B
in	O
the	O
training	O
set	B
we	O
randomly	O
divided	O
the	O
observations	B
into	O
a	O
training	O
and	O
a	O
test	O
set	B
and	O
applied	O
random	O
forests	O
to	O
the	O
training	O
set	B
for	O
three	O
different	O
values	O
of	O
the	O
number	O
of	O
splitting	O
variables	O
m	O
the	O
results	O
are	O
shown	O
in	O
figure	O
the	O
error	B
rate	B
of	O
a	O
single	B
tree	B
is	O
and	O
the	O
null	B
rate	B
is	O
we	O
see	O
that	O
using	O
trees	O
is	O
sufficient	O
to	O
give	O
good	O
performance	O
and	O
that	O
the	O
choice	O
m	O
p	O
gave	O
a	O
small	O
improvement	O
in	O
test	O
error	B
over	O
bagging	B
p	O
in	O
this	O
example	O
as	O
with	O
bagging	B
random	O
forests	O
will	O
not	O
overfit	O
if	O
we	O
increase	O
b	O
so	O
in	O
practice	O
we	O
use	O
a	O
value	O
of	O
b	O
sufficiently	O
large	O
for	O
the	O
error	B
rate	B
to	O
have	O
settled	O
down	O
boosting	B
we	O
now	O
discuss	O
boosting	B
yet	O
another	O
approach	B
for	O
improving	O
the	O
predictions	O
resulting	O
from	O
a	O
decision	B
tree	B
like	O
bagging	B
boosting	B
is	O
a	O
general	O
approach	B
that	O
can	O
be	O
applied	O
to	O
many	O
statistical	O
learning	O
methods	O
for	O
regression	B
or	O
classification	B
here	O
we	O
restrict	O
our	O
discussion	O
of	O
boosting	B
to	O
the	O
context	O
of	O
decision	O
trees	O
recall	B
that	O
bagging	B
involves	O
creating	O
multiple	B
copies	O
of	O
the	O
original	O
training	O
data	B
set	B
using	O
the	O
bootstrap	B
fitting	O
a	O
separate	O
decision	B
tree	B
to	O
each	O
copy	O
and	O
then	O
combining	O
all	O
of	O
the	O
trees	O
in	O
order	O
to	O
create	O
a	O
single	B
predictive	O
model	B
notably	O
each	O
tree	B
is	O
built	O
on	O
a	O
bootstrap	B
data	B
set	B
independent	B
of	O
the	O
other	O
trees	O
boosting	B
works	O
in	O
a	O
similar	O
way	O
except	O
that	O
the	O
trees	O
are	O
grown	O
sequentially	O
each	O
tree	B
is	O
grown	O
using	O
information	O
from	O
previously	O
grown	O
trees	O
boosting	B
does	O
not	O
involve	O
bootstrap	B
sampling	O
instead	O
each	O
tree	B
is	O
fit	O
on	O
a	O
modified	O
version	O
of	O
the	O
original	O
data	B
set	B
consider	O
first	O
the	O
regression	B
setting	O
like	O
bagging	B
boosting	B
involves	O
combining	O
a	O
large	O
number	O
of	O
decision	O
trees	O
f	O
f	O
b	O
boosting	B
is	O
described	O
in	O
algorithm	O
what	O
is	O
the	O
idea	O
behind	O
this	O
procedure	O
unlike	O
fitting	O
a	O
single	B
large	O
decision	B
tree	B
to	O
the	O
data	B
which	O
amounts	O
to	O
fitting	O
the	O
data	B
hard	O
and	O
potentially	O
overfitting	B
the	O
boosting	B
approach	B
instead	O
learns	O
slowly	O
given	O
the	O
current	O
model	B
we	O
fit	O
a	O
decision	B
tree	B
to	O
the	O
residuals	B
from	O
the	O
model	B
that	O
is	O
we	O
fit	O
a	O
tree	B
using	O
the	O
current	O
residuals	B
rather	O
than	O
the	O
outcome	O
y	O
as	O
the	O
response	B
we	O
then	O
add	O
this	O
new	O
decision	B
tree	B
into	O
the	O
fitted	O
function	B
in	O
order	O
to	O
update	O
the	O
residuals	B
each	O
of	O
these	O
trees	O
can	O
be	O
rather	O
small	O
with	O
just	O
a	O
few	O
terminal	B
nodes	O
determined	O
by	O
the	O
parameter	B
d	O
in	O
the	O
algorithm	O
by	O
null	B
rate	B
results	O
from	O
simply	O
classifying	O
each	O
observation	O
to	O
the	O
dominant	O
class	O
overall	O
which	O
is	O
in	O
this	O
case	O
the	O
normal	O
class	O
boosting	B
tree-based	O
methods	O
mp	O
m	O
p	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
l	O
t	O
s	O
e	O
t	O
number	O
of	O
trees	O
figure	O
results	O
from	O
random	O
forests	O
for	O
the	O
gene	O
expression	O
data	B
set	B
with	O
p	O
predictors	O
the	O
test	O
error	B
is	O
displayed	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
each	O
colored	O
line	B
corresponds	O
to	O
a	O
different	O
value	O
of	O
m	O
the	O
number	O
of	O
predictors	O
available	O
for	O
splitting	O
at	O
each	O
interior	O
tree	B
node	O
random	O
forests	O
p	O
lead	O
to	O
a	O
slight	O
improvement	O
over	O
bagging	B
p	O
a	O
single	B
classification	B
tree	B
has	O
an	O
error	B
rate	B
of	O
fitting	O
small	O
trees	O
to	O
the	O
residuals	B
we	O
slowly	O
improve	O
f	O
in	O
areas	O
where	O
it	O
does	O
not	O
perform	O
well	O
the	O
shrinkage	B
parameter	B
slows	O
the	O
process	O
down	O
even	O
further	O
allowing	O
more	O
and	O
different	O
shaped	O
trees	O
to	O
attack	O
the	O
residuals	B
in	O
general	O
statistical	O
learning	O
approaches	O
that	O
learn	O
slowly	O
tend	O
to	O
perform	O
well	O
note	O
that	O
in	O
boosting	B
unlike	O
in	O
bagging	B
the	O
construction	O
of	O
each	O
tree	B
depends	O
strongly	O
on	O
the	O
trees	O
that	O
have	O
already	O
been	O
grown	O
we	O
have	O
just	O
described	O
the	O
process	O
of	O
boosting	B
regression	B
trees	O
boosting	B
classification	B
trees	O
proceeds	O
in	O
a	O
similar	O
but	O
slightly	O
more	O
complex	O
way	O
and	O
the	O
details	O
are	O
omitted	O
here	O
boosting	B
has	O
three	O
tuning	O
parameters	O
the	O
number	O
of	O
trees	O
b	O
unlike	O
bagging	B
and	O
random	O
forests	O
boosting	B
can	O
overfit	O
if	O
b	O
is	O
too	O
large	O
although	O
this	O
overfitting	B
tends	O
to	O
occur	O
slowly	O
if	O
at	O
all	O
we	O
use	O
cross-validation	B
to	O
select	O
b	O
the	O
shrinkage	B
parameter	B
a	O
small	O
positive	O
number	O
this	O
controls	O
the	O
rate	B
at	O
which	O
boosting	B
learns	O
typical	O
values	O
are	O
or	O
and	O
the	O
right	O
choice	O
can	O
depend	O
on	O
the	O
problem	O
very	O
small	O
can	O
require	O
using	O
a	O
very	O
large	O
value	O
of	O
b	O
in	O
order	O
to	O
achieve	O
good	O
performance	O
the	O
number	O
d	O
of	O
splits	O
in	O
each	O
tree	B
which	O
controls	O
the	O
complexity	O
of	O
the	O
boosted	O
ensemble	O
often	O
d	O
works	O
well	O
in	O
which	O
case	O
each	O
tree	B
is	O
a	O
stump	B
consisting	O
of	O
a	O
single	B
split	O
in	O
this	O
case	O
the	O
boosted	O
ensemble	O
is	O
fitting	O
an	O
additive	B
model	B
since	O
each	O
term	B
involves	O
only	O
a	O
single	B
variable	B
more	O
generally	O
d	O
is	O
the	O
interaction	B
depth	O
and	O
controls	O
stump	B
interaction	B
depth	O
lab	O
decision	O
trees	O
algorithm	O
boosting	B
for	O
regression	B
trees	O
set	B
f	O
and	O
ri	O
yi	O
for	O
all	O
i	O
in	O
the	O
training	O
set	B
for	O
b	O
b	O
repeat	O
fit	O
a	O
tree	B
f	O
b	O
with	O
d	O
splits	O
terminal	B
nodes	O
to	O
the	O
training	O
data	B
r	O
update	O
f	O
by	O
adding	O
in	O
a	O
shrunken	O
version	O
of	O
the	O
new	O
tree	B
f	O
f	O
f	O
bx	O
update	O
the	O
residuals	B
ri	O
ri	O
f	O
bxi	O
output	B
the	O
boosted	O
model	B
f	O
f	O
bx	O
the	O
interaction	B
order	O
of	O
the	O
boosted	O
model	B
since	O
d	O
splits	O
can	O
involve	O
at	O
most	O
d	O
variables	O
in	O
figure	O
we	O
applied	O
boosting	B
to	O
the	O
cancer	O
gene	O
expression	O
data	B
set	B
in	O
order	O
to	O
develop	O
a	O
classifier	B
that	O
can	O
distinguish	O
the	O
normal	O
class	O
from	O
the	O
cancer	O
classes	O
we	O
display	O
the	O
test	O
error	B
as	O
a	O
function	B
of	O
the	O
total	O
number	O
of	O
trees	O
and	O
the	O
interaction	B
depth	O
d	O
we	O
see	O
that	O
simple	B
stumps	O
with	O
an	O
interaction	B
depth	O
of	O
one	O
perform	O
well	O
if	O
enough	O
of	O
them	O
are	O
included	O
this	O
model	B
outperforms	O
the	O
depth-two	O
model	B
and	O
both	O
outperform	O
a	O
random	B
forest	I
this	O
highlights	O
one	O
difference	O
between	O
boosting	B
and	O
random	O
forests	O
in	O
boosting	B
because	O
the	O
growth	O
of	O
a	O
particular	O
tree	B
takes	O
into	O
account	O
the	O
other	O
trees	O
that	O
have	O
already	O
been	O
grown	O
smaller	O
trees	O
are	O
typically	O
sufficient	O
using	O
smaller	O
trees	O
can	O
aid	O
in	O
interpretability	B
as	O
well	O
for	O
instance	O
using	O
stumps	O
leads	O
to	O
an	O
additive	B
model	B
lab	O
decision	O
trees	O
fitting	O
classification	B
trees	O
the	O
tree	B
library	O
is	O
used	O
to	O
construct	O
classification	B
and	O
regression	B
trees	O
library	O
tree	B
tree-based	O
methods	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
l	O
t	O
s	O
e	O
t	O
boosting	B
boosting	B
randomforest	O
m	O
p	O
number	O
of	O
trees	O
figure	O
results	O
from	O
performing	O
boosting	B
and	O
random	O
forests	O
on	O
the	O
gene	O
expression	O
data	B
set	B
in	O
order	O
to	O
predict	O
cancer	O
versus	O
normal	O
the	O
test	O
error	B
is	O
displayed	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
for	O
the	O
two	O
boosted	O
models	O
trees	O
slightly	O
outperform	O
trees	O
and	O
both	O
outperform	O
the	O
random	B
forest	I
although	O
the	O
standard	O
errors	O
are	O
around	O
making	O
none	O
of	O
these	O
differences	O
significant	O
the	O
test	O
error	B
rate	B
for	O
a	O
single	B
tree	B
is	O
we	O
first	O
use	O
classification	B
trees	O
to	O
analyze	O
the	O
carseats	B
data	B
set	B
in	O
these	O
data	B
sales	O
is	O
a	O
continuous	B
variable	B
and	O
so	O
we	O
begin	O
by	O
recoding	O
it	O
as	O
a	O
binary	B
variable	B
we	O
use	O
the	O
ifelse	O
function	B
to	O
create	O
a	O
variable	B
called	O
high	O
which	O
takes	O
on	O
a	O
value	O
of	O
yes	O
if	O
the	O
sales	O
variable	B
exceeds	O
and	O
takes	O
on	O
a	O
value	O
of	O
no	O
otherwise	O
ifelse	O
library	O
islr	O
attach	O
carseats	B
high	O
ifelse	O
sales	O
no	O
yes	O
finally	O
we	O
use	O
the	O
data	B
frame	I
function	B
to	O
merge	O
high	O
with	O
the	O
rest	O
of	O
the	O
carseats	B
data	B
carseats	B
data	B
frame	I
carseats	B
high	O
we	O
now	O
use	O
the	O
tree	B
function	B
to	O
fit	O
a	O
classification	B
tree	B
in	O
order	O
to	O
predict	O
high	O
using	O
all	O
variables	O
but	O
sales	O
the	O
syntax	O
of	O
the	O
tree	B
function	B
is	O
quite	O
similar	O
to	O
that	O
of	O
the	O
lm	O
function	B
tree	B
carseats	B
tree	B
high	O
sales	O
carseats	B
tree	B
the	O
summary	O
function	B
lists	O
the	O
variables	O
that	O
are	O
used	O
as	O
internal	B
nodes	O
in	O
the	O
tree	B
the	O
number	O
of	O
terminal	B
nodes	O
and	O
the	O
error	B
rate	B
summary	O
tree	B
carseats	B
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
tree	B
tree	B
formula	O
high	O
sales	O
data	B
carseats	B
variables	O
actually	O
used	O
in	O
tree	B
c	O
o	O
n	O
s	O
t	O
r	O
u	O
c	O
t	O
i	O
o	O
n	O
shelveloc	O
income	B
price	O
compprice	O
lab	O
decision	O
trees	O
populatio	O
n	O
number	O
of	O
terminal	B
nodes	O
residual	B
mean	O
deviance	B
m	O
i	O
s	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
error	B
rate	B
advertisi	O
n	O
g	O
age	O
us	O
we	O
see	O
that	O
the	O
training	O
error	B
rate	B
is	O
for	O
classification	B
trees	O
the	O
deviance	B
reported	O
in	O
the	O
output	B
of	O
summary	O
is	O
given	O
by	O
nmk	O
log	O
pmk	O
m	O
k	O
where	O
nmk	O
is	O
the	O
number	O
of	O
observations	B
in	O
the	O
mth	O
terminal	B
node	O
that	O
belong	O
to	O
the	O
kth	O
class	O
a	O
small	O
deviance	B
indicates	O
a	O
tree	B
that	O
provides	O
a	O
good	O
fit	O
to	O
the	O
data	B
the	O
residual	B
mean	O
deviance	B
reported	O
is	O
simply	O
the	O
deviance	B
divided	O
by	O
n	O
which	O
in	O
this	O
case	O
is	O
one	O
of	O
the	O
most	O
attractive	O
properties	O
of	O
trees	O
is	O
that	O
they	O
can	O
be	O
graphically	O
displayed	O
we	O
use	O
the	O
plot	B
function	B
to	O
display	O
the	O
tree	B
structure	O
and	O
the	O
text	O
function	B
to	O
display	O
the	O
node	O
labels	O
the	O
argument	B
instructs	O
r	O
to	O
include	O
the	O
category	O
names	O
for	O
any	O
qualitative	B
predictors	O
rather	O
than	O
simply	O
displaying	O
a	O
letter	O
for	O
each	O
category	O
plot	B
tree	B
carseats	B
text	O
tree	B
carseats	B
pretty	O
the	O
most	O
important	O
indicator	B
of	O
sales	O
appears	O
to	O
be	O
shelving	O
location	O
since	O
the	O
first	O
branch	B
differentiates	O
good	O
locations	O
from	O
bad	O
and	O
medium	O
locations	O
if	O
we	O
just	O
type	O
the	O
name	O
of	O
the	O
tree	B
object	O
r	O
prints	O
output	B
corresponding	O
to	O
each	O
branch	B
of	O
the	O
tree	B
r	O
displays	O
the	O
split	O
criterion	O
the	O
number	O
of	O
observations	B
in	O
that	O
branch	B
the	O
deviance	B
the	O
overall	O
prediction	B
for	O
the	O
branch	B
or	O
no	O
and	O
the	O
fraction	O
of	O
observations	B
in	O
that	O
branch	B
that	O
take	O
on	O
values	O
of	O
yes	O
and	O
no	O
branches	O
that	O
lead	O
to	O
terminal	B
nodes	O
are	O
indicated	O
using	O
asterisks	O
tree	B
carseats	B
node	O
split	O
n	O
deviance	B
yval	O
yprob	O
denotes	O
terminal	B
node	O
root	O
no	O
shelveloc	O
bad	O
medium	O
no	O
price	O
income	B
yes	O
no	O
in	O
order	O
to	O
properly	O
evaluate	O
the	O
performance	O
of	O
a	O
classification	B
tree	B
on	O
these	O
data	B
we	O
must	O
estimate	O
the	O
test	O
error	B
rather	O
than	O
simply	O
computing	O
the	O
training	O
error	B
we	O
split	O
the	O
observations	B
into	O
a	O
training	O
set	B
and	O
a	O
test	O
set	B
build	O
the	O
tree	B
using	O
the	O
training	O
set	B
and	O
evaluate	O
its	O
performance	O
on	O
the	O
test	O
data	B
the	O
predict	O
function	B
can	O
be	O
used	O
for	O
this	O
purpose	O
in	O
the	O
case	O
of	O
a	O
classification	B
tree	B
the	O
argument	B
typeclass	O
instructs	O
r	O
to	O
return	O
the	O
actual	O
class	O
prediction	B
this	O
approach	B
leads	O
to	O
correct	O
predictions	O
for	O
around	O
of	O
the	O
locations	O
in	O
the	O
test	O
data	B
set	B
tree-based	O
methods	O
set	B
seed	B
train	B
sample	O
nrow	O
carseats	B
carseats	B
test	O
carseats	B
train	B
high	O
test	O
high	O
train	B
tree	B
carseats	B
tree	B
high	O
sales	O
carseats	B
subset	O
train	B
tree	B
pred	O
predict	O
tree	B
carseats	B
carseats	B
test	O
type	O
class	O
table	O
tree	B
pred	O
high	O
test	O
high	O
test	O
tree	B
pred	O
no	O
yes	O
no	O
yes	O
next	O
we	O
consider	O
whether	O
pruning	B
the	O
tree	B
might	O
lead	O
to	O
improved	O
results	O
the	O
function	B
cv	O
tree	B
performs	O
cross-validation	B
in	O
order	O
to	O
determine	O
the	O
optimal	O
level	B
of	O
tree	B
complexity	O
cost	B
complexity	I
pruning	B
is	O
used	O
in	O
order	O
to	O
select	O
a	O
sequence	O
of	O
trees	O
for	O
consideration	O
we	O
use	O
the	O
argument	B
funprune	O
misclass	O
in	O
order	O
to	O
indicate	O
that	O
we	O
want	O
the	O
classification	B
error	B
rate	B
to	O
guide	O
the	O
cross-validation	B
and	O
pruning	B
process	O
rather	O
than	O
the	O
default	B
for	O
the	O
cv	O
tree	B
function	B
which	O
is	O
deviance	B
the	O
cv	O
tree	B
function	B
reports	O
the	O
number	O
of	O
terminal	B
nodes	O
of	O
each	O
tree	B
considered	O
as	O
well	O
as	O
the	O
corresponding	O
error	B
rate	B
and	O
the	O
value	O
of	O
the	O
cost-complexity	O
parameter	B
used	O
which	O
corresponds	O
to	O
in	O
cv	O
tree	B
set	B
seed	B
cv	O
carseats	B
cv	O
tree	B
tree	B
carseats	B
fun	O
prune	O
misclass	O
names	O
cv	O
carseats	B
size	O
cv	O
carseats	B
method	O
dev	O
k	O
inf	O
misclass	O
attr	O
class	O
prune	O
tree	B
sequence	O
note	O
that	O
despite	O
the	O
name	O
dev	O
corresponds	O
to	O
the	O
cross-validation	B
error	B
rate	B
in	O
this	O
instance	O
the	O
tree	B
with	O
terminal	B
nodes	O
results	O
in	O
the	O
lowest	O
cross-validation	B
error	B
rate	B
with	O
cross-validation	B
errors	O
we	O
plot	B
the	O
error	B
rate	B
as	O
a	O
function	B
of	O
both	O
size	O
and	O
k	O
par	O
mfrow	O
c	O
lab	O
decision	O
trees	O
plot	B
cv	O
carseatssize	O
cv	O
carseatsdev	O
type	O
b	O
plot	B
cv	O
carseatsk	O
cv	O
carseatsdev	O
type	O
b	O
we	O
now	O
apply	O
the	O
prune	O
misclass	O
function	B
in	O
order	O
to	O
prune	O
the	O
tree	B
to	O
prune	O
obtain	O
the	O
nine-node	O
tree	B
misclass	O
prune	O
carseats	B
prune	O
misclass	O
tree	B
carseats	B
best	O
plot	B
prune	O
carseats	B
text	O
prune	O
carseats	B
pretty	O
how	O
well	O
does	O
this	O
pruned	O
tree	B
perform	O
on	O
the	O
test	O
data	B
set	B
once	O
again	O
we	O
apply	O
the	O
predict	O
function	B
tree	B
pred	O
predict	O
prune	O
carseats	B
carseats	B
test	O
type	O
class	O
table	O
tree	B
pred	O
high	O
test	O
high	O
test	O
tree	B
pred	O
no	O
yes	O
no	O
yes	O
now	O
of	O
the	O
test	O
observations	B
are	O
correctly	O
classified	O
so	O
not	O
only	O
has	O
the	O
pruning	B
process	O
produced	O
a	O
more	O
interpretable	O
tree	B
but	O
it	O
has	O
also	O
improved	O
the	O
classification	B
accuracy	O
if	O
we	O
increase	O
the	O
value	O
of	O
best	O
we	O
obtain	O
a	O
larger	O
pruned	O
tree	B
with	O
lower	O
classification	B
accuracy	O
prune	O
carseats	B
prune	O
misclass	O
tree	B
carseats	B
best	O
plot	B
prune	O
carseats	B
text	O
prune	O
carseats	B
pretty	O
tree	B
pred	O
predict	O
prune	O
carseats	B
carseats	B
test	O
type	O
class	O
table	O
tree	B
pred	O
high	O
test	O
high	O
test	O
tree	B
pred	O
no	O
yes	O
no	O
yes	O
fitting	O
regression	B
trees	O
here	O
we	O
fit	O
a	O
regression	B
tree	B
to	O
the	O
boston	B
data	B
set	B
first	O
we	O
create	O
a	O
training	O
set	B
and	O
fit	O
the	O
tree	B
to	O
the	O
training	O
data	B
library	O
mass	O
set	B
seed	B
train	B
sample	O
nrow	O
boston	B
nrow	O
boston	B
tree	B
boston	B
tree	B
medv	O
boston	B
subset	O
train	B
summary	O
tree	B
boston	B
regressio	O
n	O
tree	B
tree	B
formula	O
medv	O
data	B
boston	B
subset	O
train	B
tree-based	O
methods	O
variables	O
actually	O
used	O
in	O
tree	B
c	O
o	O
n	O
s	O
t	O
r	O
u	O
c	O
t	O
i	O
o	O
n	O
lstat	O
rm	O
dis	O
number	O
of	O
terminal	B
nodes	O
residual	B
mean	O
deviance	B
d	O
i	O
s	O
t	O
r	O
i	O
b	O
u	O
t	O
i	O
o	O
n	O
of	O
residuals	B
median	O
min	O
st	O
qu	O
mean	O
rd	O
qu	O
max	O
notice	O
that	O
the	O
output	B
of	O
summary	O
indicates	O
that	O
only	O
three	O
of	O
the	O
variables	O
have	O
been	O
used	O
in	O
constructing	O
the	O
tree	B
in	O
the	O
context	O
of	O
a	O
regression	B
tree	B
the	O
deviance	B
is	O
simply	O
the	O
sum	O
of	O
squared	O
errors	O
for	O
the	O
tree	B
we	O
now	O
plot	B
the	O
tree	B
plot	B
tree	B
boston	B
text	O
tree	B
boston	B
pretty	O
the	O
variable	B
lstat	O
measures	O
the	O
percentage	O
of	O
individuals	O
with	O
lower	O
socioeconomic	O
status	O
the	O
tree	B
indicates	O
that	O
lower	O
values	O
of	O
lstat	O
correspond	O
to	O
more	O
expensive	O
houses	O
the	O
tree	B
predicts	O
a	O
median	O
house	O
price	O
of	O
for	O
larger	O
homes	O
in	O
suburbs	O
in	O
which	O
residents	O
have	O
high	O
socioeconomic	O
status	O
and	O
now	O
we	O
use	O
the	O
cv	O
tree	B
function	B
to	O
see	O
whether	O
pruning	B
the	O
tree	B
will	O
improve	O
performance	O
cv	O
boston	B
cv	O
tree	B
tree	B
boston	B
plot	B
cv	O
bostonsize	O
cv	O
bostondev	O
type	O
b	O
in	O
this	O
case	O
the	O
most	O
complex	O
tree	B
is	O
selected	O
by	O
cross-validation	B
however	O
if	O
we	O
wish	O
to	O
prune	O
the	O
tree	B
we	O
could	O
do	O
so	O
as	O
follows	O
using	O
the	O
prune	O
tree	B
function	B
prune	O
tree	B
prune	O
boston	B
prune	O
tree	B
tree	B
boston	B
best	O
plot	B
prune	O
boston	B
text	O
prune	O
boston	B
pretty	O
in	O
keeping	O
with	O
the	O
cross-validation	B
results	O
we	O
use	O
the	O
unpruned	O
tree	B
to	O
make	O
predictions	O
on	O
the	O
test	O
set	B
yhat	O
predict	O
tree	B
boston	B
newdata	O
boston	B
train	B
boston	B
test	O
boston	B
train	B
medv	O
plot	B
yhat	O
boston	B
test	O
abline	O
mean	O
yhat	O
boston	B
test	O
in	O
other	O
words	O
the	O
test	O
set	B
mse	B
associated	O
with	O
the	O
regression	B
tree	B
is	O
the	O
square	O
root	O
of	O
the	O
mse	B
is	O
therefore	O
around	O
indicating	O
that	O
this	O
model	B
leads	O
to	O
test	O
predictions	O
that	O
are	O
within	O
around	O
of	O
the	O
true	O
median	O
home	O
value	O
for	O
the	O
suburb	O
bagging	B
and	O
random	O
forests	O
here	O
we	O
apply	O
bagging	B
and	O
random	O
forests	O
to	O
the	O
boston	B
data	B
using	O
the	O
randomforest	O
package	O
in	O
r	O
the	O
exact	O
results	O
obtained	O
in	O
this	O
section	O
may	O
lab	O
decision	O
trees	O
depend	O
on	O
the	O
version	O
of	O
r	O
and	O
the	O
version	O
of	O
the	O
randomforest	O
package	O
installed	O
on	O
your	O
computer	O
recall	B
that	O
bagging	B
is	O
simply	O
a	O
special	O
case	O
of	O
a	O
random	B
forest	I
with	O
m	O
p	O
therefore	O
the	O
randomforest	O
function	B
can	O
be	O
used	O
to	O
perform	O
both	O
random	O
forests	O
and	O
bagging	B
we	O
perform	O
bagging	B
as	O
follows	O
random	B
forest	I
library	O
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
set	B
seed	B
bag	O
boston	B
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
medv	O
data	B
boston	B
subset	O
train	B
mtry	O
importanc	O
e	O
true	O
bag	O
boston	B
call	O
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
formula	O
medv	O
data	B
boston	B
mtry	O
importance	B
true	O
subset	O
train	B
type	O
of	O
random	B
forest	I
regression	B
number	O
of	O
trees	O
no	O
of	O
variables	O
tried	O
at	O
each	O
split	O
mean	O
of	O
squared	O
residuals	B
var	O
explained	B
the	O
argument	B
indicates	O
that	O
all	O
predictors	O
should	O
be	O
considered	O
for	O
each	O
split	O
of	O
the	O
tree	B
in	O
other	O
words	O
that	O
bagging	B
should	O
be	O
done	O
how	O
well	O
does	O
this	O
bagged	O
model	B
perform	O
on	O
the	O
test	O
set	B
yhat	O
bag	O
predict	O
bag	O
boston	B
newdata	O
boston	B
train	B
plot	B
yhat	O
bag	O
boston	B
test	O
abline	O
mean	O
yhat	O
bag	O
boston	B
test	O
the	O
test	O
set	B
mse	B
associated	O
with	O
the	O
bagged	O
regression	B
tree	B
is	O
almost	O
half	O
that	O
obtained	O
using	O
an	O
optimally-pruned	O
single	B
tree	B
we	O
could	O
change	O
the	O
number	O
of	O
trees	O
grown	O
by	O
randomforest	O
using	O
the	O
ntree	O
argument	B
bag	O
boston	B
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
medv	O
data	B
boston	B
subset	O
train	B
mtry	O
ntree	O
yhat	O
bag	O
predict	O
bag	O
boston	B
newdata	O
boston	B
train	B
mean	O
yhat	O
bag	O
boston	B
test	O
growing	O
a	O
random	B
forest	I
proceeds	O
in	O
exactly	O
the	O
same	O
way	O
except	O
that	O
we	O
use	O
a	O
smaller	O
value	O
of	O
the	O
mtry	O
argument	B
by	O
default	B
randomforest	O
uses	O
variables	O
when	O
building	O
a	O
random	B
forest	I
of	O
regression	B
trees	O
and	O
p	O
variables	O
when	O
building	O
a	O
random	B
forest	I
of	O
classification	B
trees	O
here	O
we	O
use	O
mtry	O
set	B
seed	B
rf	O
boston	B
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
medv	O
data	B
boston	B
subset	O
train	B
mtry	O
importance	B
true	O
yhat	O
rf	O
predict	O
rf	O
boston	B
newdata	O
boston	B
train	B
mean	O
yhat	O
rf	O
boston	B
test	O
tree-based	O
methods	O
the	O
test	O
set	B
mse	B
is	O
this	O
indicates	O
that	O
random	O
forests	O
yielded	O
an	O
improvement	O
over	O
bagging	B
in	O
this	O
case	O
using	O
the	O
importance	B
function	B
we	O
can	O
view	O
the	O
importance	B
of	O
each	O
variable	B
importanc	O
e	O
rf	O
boston	B
crim	O
zn	O
indus	O
chas	O
nox	O
rm	O
age	O
dis	O
rad	O
tax	O
ptratio	O
black	O
lstat	O
incmse	O
i	O
n	O
c	O
n	O
o	O
d	O
e	O
p	O
u	O
r	O
i	O
t	O
y	O
importance	B
two	O
measures	O
of	O
variable	B
importance	B
are	O
reported	O
the	O
former	O
is	O
based	O
upon	O
the	O
mean	O
decrease	O
of	O
accuracy	O
in	O
predictions	O
on	O
the	O
out	O
of	O
bag	O
samples	O
when	O
a	O
given	O
variable	B
is	O
excluded	O
from	O
the	O
model	B
the	O
latter	O
is	O
a	O
measure	O
of	O
the	O
total	O
decrease	O
in	O
node	O
impurity	O
that	O
results	O
from	O
splits	O
over	O
that	O
variable	B
averaged	O
over	O
all	O
trees	O
was	O
plotted	O
in	O
figure	O
in	O
the	O
case	O
of	O
regression	B
trees	O
the	O
node	O
impurity	O
is	O
measured	O
by	O
the	O
training	O
rss	O
and	O
for	O
classification	B
trees	O
by	O
the	O
deviance	B
plots	O
of	O
these	O
importance	B
measures	O
can	O
be	O
produced	O
using	O
the	O
varimpplot	O
function	B
varimpplo	O
t	O
rf	O
boston	B
the	O
results	O
indicate	O
that	O
across	O
all	O
of	O
the	O
trees	O
considered	O
in	O
the	O
random	B
forest	I
the	O
wealth	O
level	B
of	O
the	O
community	O
and	O
the	O
house	O
size	O
are	O
by	O
far	O
the	O
two	O
most	O
important	O
variables	O
varimpplot	O
boosting	B
here	O
we	O
use	O
the	O
gbm	O
package	O
and	O
within	O
it	O
the	O
gbm	O
function	B
to	O
fit	O
boosted	O
regression	B
trees	O
to	O
the	O
boston	B
data	B
set	B
we	O
run	O
gbm	O
with	O
the	O
option	O
distributiongaussian	O
since	O
this	O
is	O
a	O
regression	B
problem	O
if	O
it	O
were	O
a	O
binary	B
classification	B
problem	O
we	O
would	O
use	O
distributionbernoulli	O
the	O
argument	B
indicates	O
that	O
we	O
want	O
trees	O
and	O
the	O
option	O
limits	O
the	O
depth	O
of	O
each	O
tree	B
gbm	O
library	O
gbm	O
set	B
seed	B
boost	O
boston	B
gbm	O
medv	O
data	B
boston	B
train	B
d	O
i	O
s	O
t	O
r	O
i	O
b	O
u	O
t	O
i	O
o	O
n	O
gaussian	O
n	O
trees	O
interactio	O
n	O
depth	O
the	O
summary	O
function	B
produces	O
a	O
relative	O
influence	O
plot	B
and	O
also	O
outputs	O
the	O
relative	O
influence	O
statistics	O
lab	O
decision	O
trees	O
summary	O
boost	O
boston	B
var	O
lstat	O
rm	O
dis	O
crim	O
nox	O
ptratio	O
black	O
age	O
tax	O
indus	O
chas	O
rad	O
zn	O
rel	O
inf	O
we	O
see	O
that	O
lstat	O
and	O
rm	O
are	O
by	O
far	O
the	O
most	O
important	O
variables	O
we	O
can	O
also	O
produce	O
partial	O
dependence	O
plots	O
for	O
these	O
two	O
variables	O
these	O
plots	O
illustrate	O
the	O
marginal	O
effect	O
of	O
the	O
selected	O
variables	O
on	O
the	O
response	B
after	O
integrating	O
out	O
the	O
other	O
variables	O
in	O
this	O
case	O
as	O
we	O
might	O
expect	O
median	O
house	O
prices	O
are	O
increasing	O
with	O
rm	O
and	O
decreasing	O
with	O
lstat	O
partial	O
dependence	O
plot	B
par	O
mfrow	O
c	O
plot	B
boost	O
boston	B
i	O
rm	O
plot	B
boost	O
boston	B
i	O
lstat	O
we	O
now	O
use	O
the	O
boosted	O
model	B
to	O
predict	O
medv	O
on	O
the	O
test	O
set	B
yhat	O
boost	O
predict	O
boost	O
boston	B
newdata	O
boston	B
train	B
n	O
trees	O
mean	O
yhat	O
boost	O
boston	B
test	O
the	O
test	O
mse	B
obtained	O
is	O
similar	O
to	O
the	O
test	O
mse	B
for	O
random	O
forests	O
and	O
superior	O
to	O
that	O
for	O
bagging	B
if	O
we	O
want	O
to	O
we	O
can	O
perform	O
boosting	B
with	O
a	O
different	O
value	O
of	O
the	O
shrinkage	B
parameter	B
in	O
the	O
default	B
value	O
is	O
but	O
this	O
is	O
easily	O
modified	O
here	O
we	O
take	O
boost	O
boston	B
gbm	O
medv	O
data	B
boston	B
train	B
d	O
i	O
s	O
t	O
r	O
i	O
b	O
u	O
t	O
i	O
o	O
n	O
gaussian	O
n	O
trees	O
interacti	O
on	O
depth	O
shrinkage	B
verbose	O
f	O
yhat	O
boost	O
predict	O
boost	O
boston	B
newdata	O
boston	B
train	B
n	O
trees	O
mean	O
yhat	O
boost	O
boston	B
test	O
in	O
this	O
case	O
using	O
leads	O
to	O
a	O
slightly	O
lower	O
test	O
mse	B
than	O
tree-based	O
methods	O
exercises	O
conceptual	O
draw	O
an	O
example	O
your	O
own	O
invention	O
of	O
a	O
partition	O
of	O
twodimensional	O
feature	B
space	O
that	O
could	O
result	O
from	O
recursive	B
binary	B
splitting	I
your	O
example	O
should	O
contain	O
at	O
least	O
six	O
regions	O
draw	O
a	O
decision	B
tree	B
corresponding	O
to	O
this	O
partition	O
be	O
sure	O
to	O
label	O
all	O
aspects	O
of	O
your	O
figures	O
including	O
the	O
regions	O
the	O
cutpoints	O
and	O
so	O
forth	O
hint	O
your	O
result	O
should	O
look	O
something	O
like	O
figures	O
and	O
it	O
is	O
mentioned	O
in	O
section	O
that	O
boosting	B
using	O
depth-one	O
trees	O
stumps	O
leads	O
to	O
an	O
additive	B
model	B
that	O
is	O
a	O
model	B
of	O
the	O
form	O
f	O
fjxj	O
explain	O
why	O
this	O
is	O
the	O
case	O
you	O
can	O
begin	O
with	O
in	O
algorithm	O
consider	O
the	O
gini	B
index	I
classification	B
error	B
and	O
entropy	B
in	O
a	O
simple	B
classification	B
setting	O
with	O
two	O
classes	O
create	O
a	O
single	B
plot	B
that	O
displays	O
each	O
of	O
these	O
quantities	O
as	O
a	O
function	B
of	O
the	O
xaxis	O
should	O
display	O
ranging	O
from	O
to	O
and	O
the	O
y-axis	O
should	O
display	O
the	O
value	O
of	O
the	O
gini	B
index	I
classification	B
error	B
and	O
entropy	B
hint	O
in	O
a	O
setting	O
with	O
two	O
classes	O
you	O
could	O
make	O
this	O
plot	B
by	O
hand	O
but	O
it	O
will	O
be	O
much	O
easier	O
to	O
make	O
in	O
r	O
this	O
question	O
relates	O
to	O
the	O
plots	O
in	O
figure	O
sketch	O
the	O
tree	B
corresponding	O
to	O
the	O
partition	O
of	O
the	O
predictor	B
space	O
illustrated	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
numbers	O
inside	O
the	O
boxes	O
indicate	O
the	O
mean	O
of	O
y	O
within	O
each	O
region	O
create	O
a	O
diagram	O
similar	O
to	O
the	O
left-hand	O
panel	O
of	O
figure	O
using	O
the	O
tree	B
illustrated	O
in	O
the	O
right-hand	O
panel	O
of	O
the	O
same	O
figure	O
you	O
should	O
divide	O
up	O
the	O
predictor	B
space	O
into	O
the	O
correct	O
regions	O
and	O
indicate	O
the	O
mean	O
for	O
each	O
region	O
suppose	O
we	O
produce	O
ten	O
bootstrapped	O
samples	O
from	O
a	O
data	B
set	B
containing	O
red	O
and	O
green	O
classes	O
we	O
then	O
apply	O
a	O
classification	B
tree	B
to	O
each	O
bootstrapped	O
sample	O
and	O
for	O
a	O
specific	O
value	O
of	O
x	O
produce	O
estimates	O
of	O
p	O
is	O
redx	O
and	O
exercises	O
figure	O
left	O
a	O
partition	O
of	O
the	O
predictor	B
space	O
corresponding	O
to	O
exercise	O
right	O
a	O
tree	B
corresponding	O
to	O
exercise	O
there	O
are	O
two	O
common	O
ways	O
to	O
combine	O
these	O
results	O
together	O
into	O
a	O
single	B
class	O
prediction	B
one	O
is	O
the	O
majority	B
vote	I
approach	B
discussed	O
in	O
this	O
chapter	O
the	O
second	O
approach	B
is	O
to	O
classify	O
based	O
on	O
the	O
average	B
probability	B
in	O
this	O
example	O
what	O
is	O
the	O
final	O
classification	B
under	O
each	O
of	O
these	O
two	O
approaches	O
provide	O
a	O
detailed	O
explanation	O
of	O
the	O
algorithm	O
that	O
is	O
used	O
to	O
fit	O
a	O
regression	B
tree	B
applied	O
in	O
the	O
lab	O
we	O
applied	O
random	O
forests	O
to	O
the	O
boston	B
data	B
using	O
and	O
using	O
and	O
create	O
a	O
plot	B
displaying	O
the	O
test	O
error	B
resulting	O
from	O
random	O
forests	O
on	O
this	O
data	B
set	B
for	O
a	O
more	O
comprehensive	O
range	O
of	O
values	O
for	O
mtry	O
and	O
ntree	O
you	O
can	O
model	B
your	O
plot	B
after	O
figure	O
describe	O
the	O
results	O
obtained	O
in	O
the	O
lab	O
a	O
classification	B
tree	B
was	O
applied	O
to	O
the	O
carseats	B
data	B
set	B
after	O
converting	O
sales	O
into	O
a	O
qualitative	B
response	B
variable	B
now	O
we	O
will	O
seek	O
to	O
predict	O
sales	O
using	O
regression	B
trees	O
and	O
related	O
approaches	O
treating	O
the	O
response	B
as	O
a	O
quantitative	B
variable	B
split	O
the	O
data	B
set	B
into	O
a	O
training	O
set	B
and	O
a	O
test	O
set	B
fit	O
a	O
regression	B
tree	B
to	O
the	O
training	O
set	B
plot	B
the	O
tree	B
and	O
inter	O
pret	O
the	O
results	O
what	O
test	O
mse	B
do	O
you	O
obtain	O
use	O
cross-validation	B
in	O
order	O
to	O
determine	O
the	O
optimal	O
level	B
of	O
tree	B
complexity	O
does	O
pruning	B
the	O
tree	B
improve	O
the	O
test	O
mse	B
use	O
the	O
bagging	B
approach	B
in	O
order	O
to	O
analyze	O
this	O
data	B
what	O
test	O
mse	B
do	O
you	O
obtain	O
use	O
the	O
importance	B
function	B
to	O
determine	O
which	O
variables	O
are	O
most	O
important	O
tree-based	O
methods	O
use	O
random	O
forests	O
to	O
analyze	O
this	O
data	B
what	O
test	O
mse	B
do	O
you	O
obtain	O
use	O
the	O
importance	B
function	B
to	O
determine	O
which	O
variables	O
are	O
most	O
important	O
describe	O
the	O
effect	O
of	O
m	O
the	O
number	O
of	O
variables	O
rate	B
obtained	O
considered	O
at	O
error	B
each	O
split	O
on	O
the	O
this	O
problem	O
involves	O
the	O
oj	B
data	B
set	B
which	O
is	O
part	O
of	O
the	O
islr	O
package	O
create	O
a	O
training	O
set	B
containing	O
a	O
random	O
sample	O
of	O
obser	O
vations	O
and	O
a	O
test	O
set	B
containing	O
the	O
remaining	O
observations	B
fit	O
a	O
tree	B
to	O
the	O
training	O
data	B
with	O
purchase	O
as	O
the	O
response	B
and	O
the	O
other	O
variables	O
as	O
predictors	O
use	O
the	O
summary	O
function	B
to	O
produce	O
summary	O
statistics	O
about	O
the	O
tree	B
and	O
describe	O
the	O
results	O
obtained	O
what	O
is	O
the	O
training	O
error	B
rate	B
how	O
many	O
terminal	B
nodes	O
does	O
the	O
tree	B
have	O
type	O
in	O
the	O
name	O
of	O
the	O
tree	B
object	O
in	O
order	O
to	O
get	O
a	O
detailed	O
text	O
output	B
pick	O
one	O
of	O
the	O
terminal	B
nodes	O
and	O
interpret	O
the	O
information	O
displayed	O
create	O
a	O
plot	B
of	O
the	O
tree	B
and	O
interpret	O
the	O
results	O
predict	O
the	O
response	B
on	O
the	O
test	O
data	B
and	O
produce	O
a	O
confusion	B
matrix	I
comparing	O
the	O
test	O
labels	O
to	O
the	O
predicted	O
test	O
labels	O
what	O
is	O
the	O
test	O
error	B
rate	B
apply	O
the	O
cv	O
tree	B
function	B
to	O
the	O
training	O
set	B
in	O
order	O
to	O
determine	O
the	O
optimal	O
tree	B
size	O
produce	O
a	O
plot	B
with	O
tree	B
size	O
on	O
the	O
x-axis	O
and	O
cross-validated	O
classification	B
error	B
rate	B
on	O
the	O
y-axis	O
which	O
tree	B
size	O
corresponds	O
to	O
the	O
lowest	O
cross-validated	O
classi	O
fication	O
error	B
rate	B
produce	O
a	O
pruned	O
tree	B
corresponding	O
to	O
the	O
optimal	O
tree	B
size	O
obtained	O
using	O
cross-validation	B
if	O
cross-validation	B
does	O
not	O
lead	O
to	O
selection	B
of	O
a	O
pruned	O
tree	B
then	O
create	O
a	O
pruned	O
tree	B
with	O
five	O
terminal	B
nodes	O
compare	O
the	O
training	O
error	B
rates	O
between	O
the	O
pruned	O
and	O
un	O
pruned	O
trees	O
which	O
is	O
higher	O
compare	O
the	O
test	O
error	B
rates	O
between	O
the	O
pruned	O
and	O
unpruned	O
trees	O
which	O
is	O
higher	O
we	O
now	O
use	O
boosting	B
to	O
predict	O
salary	O
in	O
the	O
hitters	B
data	B
set	B
remove	O
the	O
observations	B
for	O
whom	O
the	O
salary	O
information	O
is	O
unknown	O
and	O
then	O
log-transform	O
the	O
salaries	O
exercises	O
create	O
a	O
training	O
set	B
consisting	O
of	O
the	O
first	O
observations	B
and	O
a	O
test	O
set	B
consisting	O
of	O
the	O
remaining	O
observations	B
perform	O
boosting	B
on	O
the	O
training	O
set	B
with	O
trees	O
for	O
a	O
range	O
of	O
values	O
of	O
the	O
shrinkage	B
parameter	B
produce	O
a	O
plot	B
with	O
different	O
shrinkage	B
values	O
on	O
the	O
x-axis	O
and	O
the	O
corresponding	O
training	O
set	B
mse	B
on	O
the	O
y-axis	O
produce	O
a	O
plot	B
with	O
different	O
shrinkage	B
values	O
on	O
the	O
x-axis	O
and	O
the	O
corresponding	O
test	O
set	B
mse	B
on	O
the	O
y-axis	O
compare	O
the	O
test	O
mse	B
of	O
boosting	B
to	O
the	O
test	O
mse	B
that	O
results	O
the	O
regression	B
approaches	O
seen	O
in	O
from	O
applying	O
two	O
of	O
chapters	O
and	O
which	O
variables	O
appear	O
to	O
be	O
the	O
most	O
important	O
predictors	O
in	O
the	O
boosted	O
model	B
now	O
apply	O
bagging	B
to	O
the	O
training	O
set	B
what	O
is	O
the	O
test	O
set	B
mse	B
for	O
this	O
approach	B
this	O
question	O
uses	O
the	O
caravan	B
data	B
set	B
create	O
a	O
training	O
set	B
consisting	O
of	O
the	O
first	O
observations	B
and	O
a	O
test	O
set	B
consisting	O
of	O
the	O
remaining	O
observations	B
fit	O
a	O
boosting	B
model	B
to	O
the	O
training	O
set	B
with	O
purchase	O
as	O
the	O
response	B
and	O
the	O
other	O
variables	O
as	O
predictors	O
use	O
trees	O
and	O
a	O
shrinkage	B
value	O
of	O
which	O
predictors	O
appear	O
to	O
be	O
the	O
most	O
important	O
use	O
the	O
boosting	B
model	B
to	O
predict	O
the	O
response	B
on	O
the	O
test	O
data	B
predict	O
that	O
a	O
person	O
will	O
make	O
a	O
purchase	O
if	O
the	O
estimated	O
probability	B
of	O
purchase	O
is	O
greater	O
than	O
form	O
a	O
confusion	B
matrix	I
what	O
fraction	O
of	O
the	O
people	O
predicted	O
to	O
make	O
a	O
purchase	O
do	O
in	O
fact	O
make	O
one	O
how	O
does	O
this	O
compare	O
with	O
the	O
results	O
obtained	O
from	O
applying	O
knn	O
or	O
logistic	B
regression	B
to	O
this	O
data	B
set	B
apply	O
boosting	B
bagging	B
and	O
random	O
forests	O
to	O
a	O
data	B
set	B
of	O
your	O
choice	O
be	O
sure	O
to	O
fit	O
the	O
models	O
on	O
a	O
training	O
set	B
and	O
to	O
evaluate	O
their	O
performance	O
on	O
a	O
test	O
set	B
how	O
accurate	O
are	O
the	O
results	O
compared	O
to	O
simple	B
methods	O
like	O
linear	B
or	O
logistic	B
regression	B
which	O
of	O
these	O
approaches	O
yields	O
the	O
best	O
performance	O
support	B
vector	B
machines	O
in	O
this	O
chapter	O
we	O
discuss	O
the	O
support	B
vector	B
machine	B
an	O
approach	B
for	O
classification	B
that	O
was	O
developed	O
in	O
the	O
computer	O
science	O
community	O
in	O
the	O
and	O
that	O
has	O
grown	O
in	O
popularity	O
since	O
then	O
svms	O
have	O
been	O
shown	O
to	O
perform	O
well	O
in	O
a	O
variety	O
of	O
settings	O
and	O
are	O
often	O
considered	O
one	O
of	O
the	O
best	O
out	O
of	O
the	O
box	O
classifiers	O
the	O
support	B
vector	B
machine	B
is	O
a	O
generalization	O
of	O
a	O
simple	B
and	O
intuitive	O
classifier	B
called	O
the	O
maximal	O
margin	B
classifier	B
which	O
we	O
introduce	O
in	O
section	O
though	O
it	O
is	O
elegant	O
and	O
simple	B
we	O
will	O
see	O
that	O
this	O
classifier	B
unfortunately	O
cannot	O
be	O
applied	O
to	O
most	O
data	B
sets	O
since	O
it	O
requires	O
that	O
the	O
classes	O
be	O
separable	O
by	O
a	O
linear	B
boundary	O
in	O
section	O
we	O
introduce	O
the	O
support	B
vector	B
classifier	B
an	O
extension	O
of	O
the	O
maximal	O
margin	B
classifier	B
that	O
can	O
be	O
applied	O
in	O
a	O
broader	O
range	O
of	O
cases	O
section	O
introduces	O
the	O
support	B
vector	B
machine	B
which	O
is	O
a	O
further	O
extension	O
of	O
the	O
support	B
vector	B
classifier	B
in	O
order	O
to	O
accommodate	O
non-linear	B
class	O
boundaries	O
support	B
vector	B
machines	O
are	O
intended	O
for	O
the	O
binary	B
classification	B
setting	O
in	O
which	O
there	O
are	O
two	O
classes	O
in	O
section	O
we	O
discuss	O
extensions	O
of	O
support	B
vector	B
machines	O
to	O
the	O
case	O
of	O
more	O
than	O
two	O
classes	O
in	O
section	O
we	O
discuss	O
the	O
close	O
connections	O
between	O
support	B
vector	B
machines	O
and	O
other	O
statistical	O
methods	O
such	O
as	O
logistic	B
regression	B
people	O
often	O
loosely	O
refer	O
to	O
the	O
maximal	O
margin	B
classifier	B
the	O
support	B
vector	B
classifier	B
and	O
the	O
support	B
vector	B
machine	B
as	O
support	B
vector	B
machines	O
to	O
avoid	O
confusion	O
we	O
will	O
carefully	O
distinguish	O
between	O
these	O
three	O
notions	O
in	O
this	O
chapter	O
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
support	B
vector	B
machines	O
maximal	O
margin	B
classifier	B
in	O
this	O
section	O
we	O
define	O
a	O
hyperplane	B
and	O
introduce	O
the	O
concept	O
of	O
an	O
optimal	O
separating	B
hyperplane	B
what	O
is	O
a	O
hyperplane	B
in	O
a	O
p-dimensional	O
space	O
a	O
hyperplane	B
is	O
a	O
flat	O
affine	O
subspace	O
of	O
dimension	O
p	O
for	O
instance	O
in	O
two	O
dimensions	O
a	O
hyperplane	B
is	O
a	O
flat	O
one-dimensional	O
subspace	O
in	O
other	O
words	O
a	O
line	B
in	O
three	O
dimensions	O
a	O
hyperplane	B
is	O
a	O
flat	O
two-dimensional	O
subspace	O
that	O
is	O
a	O
plane	O
in	O
p	O
dimensions	O
it	O
can	O
be	O
hard	O
to	O
visualize	O
a	O
hyperplane	B
but	O
the	O
notion	O
of	O
a	O
flat	O
subspace	O
still	O
applies	O
the	O
mathematical	O
definition	O
of	O
a	O
hyperplane	B
is	O
quite	O
simple	B
in	O
two	O
di	O
mensions	O
a	O
hyperplane	B
is	O
defined	O
by	O
the	O
equation	O
hyperplane	B
for	O
parameters	O
and	O
when	O
we	O
say	O
that	O
defines	O
the	O
hyperplane	B
we	O
mean	O
that	O
any	O
x	O
for	O
which	O
holds	O
is	O
a	O
point	O
on	O
the	O
hyperplane	B
note	O
that	O
is	O
simply	O
the	O
equation	O
of	O
a	O
line	B
since	O
indeed	O
in	O
two	O
dimensions	O
a	O
hyperplane	B
is	O
a	O
line	B
equation	O
can	O
be	O
easily	O
extended	O
to	O
the	O
p-dimensional	O
setting	O
pxp	O
defines	O
a	O
p-dimensional	O
hyperplane	B
again	O
in	O
the	O
sense	O
that	O
if	O
a	O
point	O
x	O
xpt	O
in	O
p-dimensional	O
space	O
a	O
vector	B
of	O
length	O
p	O
satisfies	O
then	O
x	O
lies	O
on	O
the	O
hyperplane	B
now	O
suppose	O
that	O
x	O
does	O
not	O
satisfy	O
rather	O
pxp	O
then	O
this	O
tells	O
us	O
that	O
x	O
lies	O
to	O
one	O
side	O
of	O
the	O
hyperplane	B
on	O
the	O
other	O
hand	O
if	O
pxp	O
then	O
x	O
lies	O
on	O
the	O
other	O
side	O
of	O
the	O
hyperplane	B
so	O
we	O
can	O
think	O
of	O
the	O
hyperplane	B
as	O
dividing	O
p-dimensional	O
space	O
into	O
two	O
halves	O
one	O
can	O
easily	O
determine	O
on	O
which	O
side	O
of	O
the	O
hyperplane	B
a	O
point	O
lies	O
by	O
simply	O
calculating	O
the	O
sign	O
of	O
the	O
left	O
hand	O
side	O
of	O
a	O
hyperplane	B
in	O
two-dimensional	O
space	O
is	O
shown	O
in	O
figure	O
word	O
affine	O
indicates	O
that	O
the	O
subspace	O
need	O
not	O
pass	O
through	O
the	O
origin	O
maximal	O
margin	B
classifier	B
x	O
figure	O
the	O
hyperplane	B
is	O
shown	O
the	O
blue	O
region	O
is	O
the	O
set	B
of	O
points	O
for	O
which	O
and	O
the	O
purple	O
region	O
is	O
the	O
set	B
of	O
points	O
for	O
which	O
classification	B
using	O
a	O
separating	B
hyperplane	B
now	O
suppose	O
that	O
we	O
have	O
a	O
n	O
p	O
data	B
matrix	O
x	O
that	O
consists	O
of	O
n	O
training	O
observations	B
in	O
p-dimensional	O
space	O
xn	O
xnp	O
and	O
that	O
these	O
observations	B
fall	O
into	O
two	O
classes	O
that	O
is	O
yn	O
where	O
represents	O
one	O
class	O
and	O
the	O
other	O
class	O
we	O
also	O
have	O
a	O
test	O
observation	O
a	O
p-vector	O
of	O
observed	O
features	O
x	O
our	O
goal	O
is	O
to	O
develop	O
a	O
classifier	B
based	O
on	O
the	O
training	O
data	B
that	O
will	O
correctly	O
classify	O
the	O
test	O
observation	O
using	O
its	O
feature	B
measurements	O
we	O
have	O
seen	O
a	O
number	O
of	O
approaches	O
for	O
this	O
task	O
such	O
as	O
linear	B
discriminant	I
analysis	B
and	O
logistic	B
regression	B
in	O
chapter	O
and	O
classification	B
trees	O
bagging	B
and	O
boosting	B
in	O
chapter	O
we	O
will	O
now	O
see	O
a	O
new	O
approach	B
that	O
is	O
based	O
upon	O
the	O
concept	O
of	O
a	O
separating	B
hyperplane	B
x	O
p	O
x	O
suppose	O
that	O
it	O
is	O
possible	O
to	O
construct	O
a	O
hyperplane	B
that	O
separates	O
the	O
training	O
observations	B
perfectly	O
according	O
to	O
their	O
class	O
labels	O
examples	O
of	O
three	O
such	O
separating	O
hyperplanes	O
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
we	O
can	O
label	O
the	O
observations	B
from	O
the	O
blue	O
class	O
as	O
yi	O
and	O
separating	B
hyperplane	B
support	B
vector	B
machines	O
x	O
x	O
figure	O
left	O
there	O
are	O
two	O
classes	O
of	O
observations	B
shown	O
in	O
blue	O
and	O
in	O
purple	O
each	O
of	O
which	O
has	O
measurements	O
on	O
two	O
variables	O
three	O
separating	O
hyperplanes	O
out	O
of	O
many	O
possible	O
are	O
shown	O
in	O
black	O
right	O
a	O
separating	B
hyperplane	B
is	O
shown	O
in	O
black	O
the	O
blue	O
and	O
purple	O
grid	O
indicates	O
the	O
decision	O
rule	O
made	O
by	O
a	O
classifier	B
based	O
on	O
this	O
separating	B
hyperplane	B
a	O
test	O
observation	O
that	O
falls	O
in	O
the	O
blue	O
portion	O
of	O
the	O
grid	O
will	O
be	O
assigned	O
to	O
the	O
blue	O
class	O
and	O
a	O
test	O
observation	O
that	O
falls	O
into	O
the	O
purple	O
portion	O
of	O
the	O
grid	O
will	O
be	O
assigned	O
to	O
the	O
purple	O
class	O
those	O
from	O
the	O
purple	O
class	O
as	O
yi	O
then	O
a	O
separating	B
hyperplane	B
has	O
the	O
property	O
that	O
pxip	O
if	O
yi	O
and	O
pxip	O
if	O
yi	O
equivalently	O
a	O
separating	B
hyperplane	B
has	O
the	O
property	O
that	O
yi	O
pxip	O
for	O
all	O
i	O
n	O
if	O
a	O
separating	B
hyperplane	B
exists	O
we	O
can	O
use	O
it	O
to	O
construct	O
a	O
very	O
natural	B
classifier	B
a	O
test	O
observation	O
is	O
assigned	O
a	O
class	O
depending	O
on	O
which	O
side	O
of	O
the	O
hyperplane	B
it	O
is	O
located	O
the	O
right-hand	O
panel	O
of	O
figure	O
shows	O
an	O
example	O
of	O
such	O
a	O
classifier	B
that	O
is	O
we	O
classify	O
the	O
test	O
observation	O
x	O
based	O
on	O
the	O
sign	O
of	O
f	O
is	O
positive	O
then	O
we	O
assign	O
the	O
test	O
observation	O
to	O
class	O
and	O
if	O
f	O
is	O
negative	O
then	O
we	O
assign	O
it	O
to	O
class	O
we	O
can	O
also	O
make	O
use	O
of	O
the	O
magnitude	O
of	O
f	O
if	O
f	O
lies	O
far	O
from	O
the	O
hyperplane	B
and	O
so	O
we	O
can	O
be	O
confident	O
about	O
our	O
class	O
assignment	O
for	O
x	O
on	O
the	O
other	O
p	O
if	O
f	O
px	O
is	O
far	O
from	O
zero	O
then	O
this	O
means	O
that	O
x	O
maximal	O
margin	B
classifier	B
is	O
close	O
to	O
zero	O
then	O
x	O
hand	O
if	O
f	O
is	O
located	O
near	O
the	O
hyperplane	B
and	O
so	O
we	O
are	O
less	O
certain	O
about	O
the	O
class	O
assignment	O
for	O
x	O
not	O
surprisingly	O
and	O
as	O
we	O
see	O
in	O
figure	O
a	O
classifier	B
that	O
is	O
based	O
on	O
a	O
separating	B
hyperplane	B
leads	O
to	O
a	O
linear	B
decision	B
boundary	I
the	O
maximal	O
margin	B
classifier	B
in	O
general	O
if	O
our	O
data	B
can	O
be	O
perfectly	O
separated	O
using	O
a	O
hyperplane	B
then	O
there	O
will	O
in	O
fact	O
exist	O
an	O
infinite	O
number	O
of	O
such	O
hyperplanes	O
this	O
is	O
because	O
a	O
given	O
separating	B
hyperplane	B
can	O
usually	O
be	O
shifted	O
a	O
tiny	O
bit	O
up	O
or	O
down	O
or	O
rotated	O
without	O
coming	O
into	O
contact	O
with	O
any	O
of	O
the	O
observations	B
three	O
possible	O
separating	O
hyperplanes	O
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
in	O
order	O
to	O
construct	O
a	O
classifier	B
based	O
upon	O
a	O
separating	B
hyperplane	B
we	O
must	O
have	O
a	O
reasonable	O
way	O
to	O
decide	O
which	O
of	O
the	O
infinite	O
possible	O
separating	O
hyperplanes	O
to	O
use	O
a	O
natural	B
choice	O
is	O
the	O
maximal	O
margin	B
hyperplane	B
known	O
as	O
the	O
optimal	O
separating	B
hyperplane	B
which	O
is	O
the	O
separating	B
hyperplane	B
that	O
is	O
farthest	O
from	O
the	O
training	O
observations	B
that	O
is	O
we	O
can	O
compute	O
the	O
distance	O
from	O
each	O
training	O
observation	O
to	O
a	O
given	O
separating	B
hyperplane	B
the	O
smallest	O
such	O
distance	O
is	O
the	O
minimal	O
distance	O
from	O
the	O
observations	B
to	O
the	O
hyperplane	B
and	O
is	O
known	O
as	O
the	O
margin	B
the	O
maximal	O
margin	B
hyperplane	B
is	O
the	O
separating	B
hyperplane	B
for	O
which	O
the	O
margin	B
is	O
largest	O
that	O
is	O
it	O
is	O
the	O
hyperplane	B
that	O
has	O
the	O
farthest	O
minimum	O
distance	O
to	O
the	O
training	O
observations	B
we	O
can	O
then	O
classify	O
a	O
test	O
observation	O
based	O
on	O
which	O
side	O
of	O
the	O
maximal	O
margin	B
hyperplane	B
it	O
lies	O
this	O
is	O
known	O
as	O
the	O
maximal	O
margin	B
classifier	B
we	O
hope	O
that	O
a	O
classifier	B
that	O
has	O
a	O
large	O
margin	B
on	O
the	O
training	O
data	B
will	O
also	O
have	O
a	O
large	O
margin	B
on	O
the	O
test	O
data	B
and	O
hence	O
will	O
classify	O
the	O
test	O
observations	B
correctly	O
although	O
the	O
maximal	O
margin	B
classifier	B
is	O
often	O
successful	O
it	O
can	O
also	O
lead	O
to	O
overfitting	B
when	O
p	O
is	O
large	O
maximal	O
margin	B
hyperplane	B
optimal	O
separating	B
hyperplane	B
margin	B
maximal	O
margin	B
classifier	B
if	O
p	O
are	O
the	O
coefficients	O
of	O
the	O
maximal	O
margin	B
hyperplane	B
based	O
then	O
the	O
maximal	O
margin	B
classifier	B
classifies	O
the	O
test	O
observation	O
x	O
on	O
the	O
sign	O
of	O
f	O
px	O
p	O
figure	O
shows	O
the	O
maximal	O
margin	B
hyperplane	B
on	O
the	O
data	B
set	B
of	O
figure	O
comparing	O
the	O
right-hand	O
panel	O
of	O
figure	O
to	O
figure	O
we	O
see	O
that	O
the	O
maximal	O
margin	B
hyperplane	B
shown	O
in	O
figure	O
does	O
indeed	O
result	O
in	O
a	O
greater	O
minimal	O
distance	O
between	O
the	O
observations	B
and	O
the	O
separating	B
hyperplane	B
that	O
is	O
a	O
larger	O
margin	B
in	O
a	O
sense	O
the	O
maximal	O
margin	B
hyperplane	B
represents	O
the	O
mid-line	O
of	O
the	O
widest	O
slab	O
that	O
we	O
can	O
insert	O
between	O
the	O
two	O
classes	O
examining	O
figure	O
we	O
see	O
that	O
three	O
training	O
observations	B
are	O
equidistant	O
from	O
the	O
maximal	O
margin	B
hyperplane	B
and	O
lie	O
along	O
the	O
dashed	O
lines	O
indicating	O
the	O
width	O
of	O
the	O
margin	B
these	O
three	O
observations	B
are	O
known	O
as	O
support	B
vector	B
machines	O
x	O
figure	O
there	O
are	O
two	O
classes	O
of	O
observations	B
shown	O
in	O
blue	O
and	O
in	O
purple	O
the	O
maximal	O
margin	B
hyperplane	B
is	O
shown	O
as	O
a	O
solid	O
line	B
the	O
margin	B
is	O
the	O
distance	O
from	O
the	O
solid	O
line	B
to	O
either	O
of	O
the	O
dashed	O
lines	O
the	O
two	O
blue	O
points	O
and	O
the	O
purple	O
point	O
that	O
lie	O
on	O
the	O
dashed	O
lines	O
are	O
the	O
support	O
vectors	O
and	O
the	O
distance	O
from	O
those	O
points	O
to	O
the	O
hyperplane	B
is	O
indicated	O
by	O
arrows	O
the	O
purple	O
and	O
blue	O
grid	O
indicates	O
the	O
decision	O
rule	O
made	O
by	O
a	O
classifier	B
based	O
on	O
this	O
separating	B
hyperplane	B
support	O
vectors	O
since	O
they	O
are	O
vectors	O
in	O
p-dimensional	O
space	O
figure	O
p	O
and	O
they	O
support	O
the	O
maximal	O
margin	B
hyperplane	B
in	O
the	O
sense	O
that	O
if	O
these	O
points	O
were	O
moved	O
slightly	O
then	O
the	O
maximal	O
margin	B
hyperplane	B
would	O
move	O
as	O
well	O
interestingly	O
the	O
maximal	O
margin	B
hyperplane	B
depends	O
directly	O
on	O
the	O
support	O
vectors	O
but	O
not	O
on	O
the	O
other	O
observations	B
a	O
movement	O
to	O
any	O
of	O
the	O
other	O
observations	B
would	O
not	O
affect	O
the	O
separating	B
hyperplane	B
provided	O
that	O
the	O
observation	O
s	O
movement	O
does	O
not	O
cause	O
it	O
to	O
cross	O
the	O
boundary	O
set	B
by	O
the	O
margin	B
the	O
fact	O
that	O
the	O
maximal	O
margin	B
hyperplane	B
depends	O
directly	O
on	O
only	O
a	O
small	O
subset	O
of	O
the	O
observations	B
is	O
an	O
important	O
property	O
that	O
will	O
arise	O
later	O
in	O
this	O
chapter	O
when	O
we	O
discuss	O
the	O
support	B
vector	B
classifier	B
and	O
support	B
vector	B
machines	O
support	B
vector	B
construction	O
of	O
the	O
maximal	O
margin	B
classifier	B
we	O
now	O
consider	O
the	O
task	O
of	O
constructing	O
the	O
maximal	O
margin	B
hyperplane	B
based	O
on	O
a	O
set	B
of	O
n	O
training	O
observations	B
xn	O
rp	O
and	O
associated	O
class	O
labels	O
yn	O
briefly	O
the	O
maximal	O
margin	B
hyperplane	B
is	O
the	O
solution	O
to	O
the	O
optimization	O
problem	O
maximal	O
margin	B
classifier	B
j	O
maximize	O
p	O
m	O
subject	O
to	O
yi	O
pxip	O
m	O
i	O
n	O
this	O
optimization	O
problem	O
is	O
actually	O
simpler	O
than	O
it	O
looks	O
first	O
of	O
all	O
the	O
constraint	O
in	O
that	O
yi	O
pxip	O
m	O
i	O
n	O
guarantees	O
that	O
each	O
observation	O
will	O
be	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
provided	O
that	O
m	O
is	O
positive	O
for	O
each	O
observation	O
to	O
be	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
we	O
would	O
simply	O
need	O
yi	O
pxip	O
so	O
the	O
constraint	O
in	O
in	O
fact	O
requires	O
that	O
each	O
observation	O
be	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
with	O
some	O
cushion	O
provided	O
that	O
m	O
is	O
positive	O
second	O
note	O
that	O
is	O
not	O
really	O
a	O
constraint	O
on	O
the	O
hyperplane	B
since	O
if	O
pxip	O
defines	O
a	O
hyperplane	B
then	O
so	O
does	O
k	O
pxip	O
for	O
any	O
k	O
however	O
adds	O
meaning	O
to	O
one	O
can	O
show	O
that	O
with	O
this	O
constraint	O
the	O
perpendicular	B
distance	O
from	O
the	O
ith	O
observation	O
to	O
the	O
hyperplane	B
is	O
given	O
by	O
yi	O
pxip	O
therefore	O
the	O
constraints	O
and	O
ensure	O
that	O
each	O
observation	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
and	O
at	O
least	O
a	O
distance	O
m	O
from	O
the	O
hyperplane	B
hence	O
m	O
represents	O
the	O
margin	B
of	O
our	O
hyperplane	B
and	O
the	O
optimization	O
problem	O
chooses	O
p	O
to	O
maximize	O
m	O
this	O
is	O
exactly	O
the	O
definition	O
of	O
the	O
maximal	O
margin	B
hyperplane	B
the	O
problem	O
can	O
be	O
solved	O
efficiently	O
but	O
details	O
of	O
this	O
optimization	O
are	O
outside	O
of	O
the	O
scope	O
of	O
this	O
book	O
the	O
non-separable	O
case	O
the	O
maximal	O
margin	B
classifier	B
is	O
a	O
very	O
natural	B
way	O
to	O
perform	O
classification	B
if	O
a	O
separating	B
hyperplane	B
exists	O
however	O
as	O
we	O
have	O
hinted	O
in	O
many	O
cases	O
no	O
separating	B
hyperplane	B
exists	O
and	O
so	O
there	O
is	O
no	O
maximal	O
margin	B
classifier	B
in	O
this	O
case	O
the	O
optimization	O
problem	O
has	O
no	O
solution	O
with	O
m	O
an	O
example	O
is	O
shown	O
in	O
figure	O
in	O
this	O
case	O
we	O
cannot	O
exactly	O
separate	O
the	O
two	O
classes	O
however	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
section	O
we	O
can	O
extend	O
the	O
concept	O
of	O
a	O
separating	B
hyperplane	B
in	O
order	O
to	O
develop	O
a	O
hyperplane	B
that	O
almost	O
separates	O
the	O
classes	O
using	O
a	O
so-called	O
soft	O
margin	B
the	O
generalization	O
of	O
the	O
maximal	O
margin	B
classifier	B
to	O
the	O
non-separable	O
case	O
is	O
known	O
as	O
the	O
support	B
vector	B
classifier	B
support	B
vector	B
machines	O
x	O
figure	O
there	O
are	O
two	O
classes	O
of	O
observations	B
shown	O
in	O
blue	O
and	O
in	O
purple	O
in	O
this	O
case	O
the	O
two	O
classes	O
are	O
not	O
separable	O
by	O
a	O
hyperplane	B
and	O
so	O
the	O
maximal	O
margin	B
classifier	B
cannot	O
be	O
used	O
support	B
vector	B
classifiers	O
overview	O
of	O
the	O
support	B
vector	B
classifier	B
in	O
figure	O
we	O
see	O
that	O
observations	B
that	O
belong	O
to	O
two	O
classes	O
are	O
not	O
necessarily	O
separable	O
by	O
a	O
hyperplane	B
in	O
fact	O
even	O
if	O
a	O
separating	B
hyperplane	B
does	O
exist	O
then	O
there	O
are	O
instances	O
in	O
which	O
a	O
classifier	B
based	O
on	O
a	O
separating	B
hyperplane	B
might	O
not	O
be	O
desirable	O
a	O
classifier	B
based	O
on	O
a	O
separating	B
hyperplane	B
will	O
necessarily	O
perfectly	O
classify	O
all	O
of	O
the	O
training	O
observations	B
this	O
can	O
lead	O
to	O
sensitivity	B
to	O
individual	O
observations	B
an	O
example	O
is	O
shown	O
in	O
figure	O
the	O
addition	O
of	O
a	O
single	B
observation	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
leads	O
to	O
a	O
dramatic	O
change	O
in	O
the	O
maximal	O
margin	B
hyperplane	B
the	O
resulting	O
maximal	O
margin	B
hyperplane	B
is	O
not	O
satisfactory	O
for	O
one	O
thing	O
it	O
has	O
only	O
a	O
tiny	O
margin	B
this	O
is	O
problematic	O
because	O
as	O
discussed	O
previously	O
the	O
distance	O
of	O
an	O
observation	O
from	O
the	O
hyperplane	B
can	O
be	O
seen	O
as	O
a	O
measure	O
of	O
our	O
confidence	O
that	O
the	O
observation	O
was	O
correctly	O
classified	O
moreover	O
the	O
fact	O
that	O
the	O
maximal	O
margin	B
hyperplane	B
is	O
extremely	O
sensitive	O
to	O
a	O
change	O
in	O
a	O
single	B
observation	O
suggests	O
that	O
it	O
may	O
have	O
overfit	O
the	O
training	O
data	B
in	O
this	O
case	O
we	O
might	O
be	O
willing	O
to	O
consider	O
a	O
classifier	B
based	O
on	O
a	O
hyperplane	B
that	O
does	O
not	O
perfectly	O
separate	O
the	O
two	O
classes	O
in	O
the	O
interest	O
of	O
support	B
vector	B
classifiers	O
x	O
x	O
figure	O
left	O
two	O
classes	O
of	O
observations	B
are	O
shown	O
in	O
blue	O
and	O
in	O
purple	O
along	O
with	O
the	O
maximal	O
margin	B
hyperplane	B
right	O
an	O
additional	O
blue	O
observation	O
has	O
been	O
added	O
leading	O
to	O
a	O
dramatic	O
shift	O
in	O
the	O
maximal	O
margin	B
hyperplane	B
shown	O
as	O
a	O
solid	O
line	B
the	O
dashed	O
line	B
indicates	O
the	O
maximal	O
margin	B
hyperplane	B
that	O
was	O
obtained	O
in	O
the	O
absence	O
of	O
this	O
additional	O
point	O
greater	O
robustness	O
to	O
individual	O
observations	B
and	O
better	O
classification	B
of	O
most	O
of	O
the	O
training	O
observations	B
that	O
is	O
it	O
could	O
be	O
worthwhile	O
to	O
misclassify	O
a	O
few	O
training	O
observations	B
in	O
order	O
to	O
do	O
a	O
better	O
job	O
in	O
classifying	O
the	O
remaining	O
observations	B
the	O
support	B
vector	B
classifier	B
sometimes	O
called	O
a	O
soft	B
margin	B
classifier	B
does	O
exactly	O
this	O
rather	O
than	O
seeking	O
the	O
largest	O
possible	O
margin	B
so	O
that	O
every	O
observation	O
is	O
not	O
only	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
but	O
also	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
we	O
instead	O
allow	O
some	O
observations	B
to	O
be	O
on	O
the	O
incorrect	O
side	O
of	O
the	O
margin	B
or	O
even	O
the	O
incorrect	O
side	O
of	O
the	O
hyperplane	B
margin	B
is	O
soft	O
because	O
it	O
can	O
be	O
violated	O
by	O
some	O
of	O
the	O
training	O
observations	B
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
most	O
of	O
the	O
observations	B
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
however	O
a	O
small	O
subset	O
of	O
the	O
observations	B
are	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
an	O
observation	O
can	O
be	O
not	O
only	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
but	O
also	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
in	O
fact	O
when	O
there	O
is	O
no	O
separating	B
hyperplane	B
such	O
a	O
situation	O
is	O
inevitable	O
observations	B
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
correspond	O
to	O
training	O
observations	B
that	O
are	O
misclassified	O
by	O
the	O
support	B
vector	B
classifier	B
the	O
right-hand	O
panel	O
of	O
figure	O
illustrates	O
such	O
a	O
scenario	O
details	O
of	O
the	O
support	B
vector	B
classifier	B
the	O
support	B
vector	B
classifier	B
classifies	O
a	O
test	O
observation	O
depending	O
on	O
which	O
side	O
of	O
a	O
hyperplane	B
it	O
lies	O
the	O
hyperplane	B
is	O
chosen	O
to	O
correctly	O
support	B
vector	B
classifier	B
soft	B
margin	B
classifier	B
support	B
vector	B
machines	O
x	O
x	O
figure	O
left	O
a	O
support	B
vector	B
classifier	B
was	O
fit	O
to	O
a	O
small	O
data	B
set	B
the	O
hyperplane	B
is	O
shown	O
as	O
a	O
solid	O
line	B
and	O
the	O
margins	O
are	O
shown	O
as	O
dashed	O
lines	O
purple	O
observations	B
observations	B
and	O
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
observation	O
is	O
on	O
the	O
margin	B
and	O
observation	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
blue	O
observations	B
observations	B
and	O
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
observation	O
is	O
on	O
the	O
margin	B
and	O
observation	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
no	O
observations	B
are	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
right	O
same	O
as	O
left	O
panel	O
with	O
two	O
additional	O
points	O
and	O
these	O
two	O
observations	B
are	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
and	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
separate	O
most	O
of	O
the	O
training	O
observations	B
into	O
the	O
two	O
classes	O
but	O
may	O
misclassify	O
a	O
few	O
observations	B
it	O
is	O
the	O
solution	O
to	O
the	O
optimization	O
problem	O
maximize	O
m	O
subject	O
to	O
j	O
yi	O
pxip	O
m	O
c	O
where	O
c	O
is	O
a	O
nonnegative	O
tuning	B
parameter	B
as	O
in	O
m	O
is	O
the	O
width	O
of	O
the	O
margin	B
we	O
seek	O
to	O
make	O
this	O
quantity	O
as	O
large	O
as	O
possible	O
in	O
are	O
slack	O
variables	O
that	O
allow	O
individual	O
observations	B
to	O
be	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
or	O
the	O
hyperplane	B
we	O
will	O
explain	O
them	O
in	O
greater	O
detail	O
momentarily	O
once	O
we	O
have	O
solved	O
we	O
classify	O
a	O
test	O
observation	O
x	O
as	O
before	O
by	O
simply	O
determining	O
on	O
which	O
side	O
of	O
the	O
hyperplane	B
it	O
lies	O
that	O
is	O
we	O
classify	O
the	O
test	O
observation	O
based	O
on	O
the	O
sign	O
of	O
f	O
px	O
p	O
the	O
problem	O
seems	O
complex	O
but	O
insight	O
into	O
its	O
behavior	O
can	O
be	O
made	O
through	O
a	O
series	O
of	O
simple	B
observations	B
presented	O
below	O
first	O
of	O
all	O
the	O
slack	B
variable	B
tells	O
us	O
where	O
the	O
ith	O
observation	O
is	O
located	O
relative	O
to	O
the	O
hyperplane	B
and	O
relative	O
to	O
the	O
margin	B
if	O
then	O
the	O
ith	O
slack	B
variable	B
support	B
vector	B
classifiers	O
observation	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
as	O
we	O
saw	O
in	O
section	O
if	O
then	O
the	O
ith	O
observation	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
and	O
we	O
say	O
that	O
the	O
ith	O
observation	O
has	O
violated	O
the	O
margin	B
if	O
then	O
it	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
we	O
now	O
consider	O
the	O
role	O
of	O
the	O
tuning	B
parameter	B
c	O
in	O
c	O
bounds	O
the	O
sum	O
of	O
the	O
s	O
and	O
so	O
it	O
determines	O
the	O
number	O
and	O
severity	O
of	O
the	O
violations	O
to	O
the	O
margin	B
to	O
the	O
hyperplane	B
that	O
we	O
will	O
tolerate	O
we	O
can	O
think	O
of	O
c	O
as	O
a	O
budget	O
for	O
the	O
amount	O
that	O
the	O
margin	B
can	O
be	O
violated	O
by	O
the	O
n	O
observations	B
if	O
c	O
then	O
there	O
is	O
no	O
budget	O
for	O
violations	O
to	O
the	O
margin	B
and	O
it	O
must	O
be	O
the	O
case	O
that	O
in	O
which	O
case	O
simply	O
amounts	O
to	O
the	O
maximal	O
margin	B
hyperplane	B
optimization	O
problem	O
course	O
a	O
maximal	O
margin	B
hyperplane	B
exists	O
only	O
if	O
the	O
two	O
classes	O
are	O
separable	O
for	O
c	O
no	O
more	O
than	O
c	O
observations	B
can	O
be	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
because	O
if	O
an	O
observation	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
then	O
and	O
requires	O
that	O
violations	O
to	O
the	O
margin	B
and	O
so	O
the	O
margin	B
will	O
widen	O
conversely	O
as	O
c	O
decreases	O
we	O
become	O
less	O
tolerant	O
of	O
violations	O
to	O
the	O
margin	B
and	O
so	O
the	O
margin	B
narrows	O
an	O
example	O
in	O
shown	O
in	O
figure	O
c	O
as	O
the	O
budget	O
c	O
increases	O
we	O
become	O
more	O
tolerant	O
of	O
n	O
in	O
practice	O
c	O
is	O
treated	O
as	O
a	O
tuning	B
parameter	B
that	O
is	O
generally	O
chosen	O
via	O
cross-validation	B
as	O
with	O
the	O
tuning	O
parameters	O
that	O
we	O
have	O
seen	O
throughout	O
this	O
book	O
c	O
controls	O
the	O
bias-variance	O
trade-off	B
of	O
the	O
statistical	O
learning	O
technique	O
when	O
c	O
is	O
small	O
we	O
seek	O
narrow	O
margins	O
that	O
are	O
rarely	O
violated	O
this	O
amounts	O
to	O
a	O
classifier	B
that	O
is	O
highly	O
fit	O
to	O
the	O
data	B
which	O
may	O
have	O
low	O
bias	B
but	O
high	O
variance	B
on	O
the	O
other	O
hand	O
when	O
c	O
is	O
larger	O
the	O
margin	B
is	O
wider	O
and	O
we	O
allow	O
more	O
violations	O
to	O
it	O
this	O
amounts	O
to	O
fitting	O
the	O
data	B
less	O
hard	O
and	O
obtaining	O
a	O
classifier	B
that	O
is	O
potentially	O
more	O
biased	O
but	O
may	O
have	O
lower	O
variance	B
the	O
optimization	O
problem	O
has	O
a	O
very	O
interesting	O
property	O
it	O
turns	O
out	O
that	O
only	O
observations	B
that	O
either	O
lie	O
on	O
the	O
margin	B
or	O
that	O
violate	O
the	O
margin	B
will	O
affect	O
the	O
hyperplane	B
and	O
hence	O
the	O
classifier	B
obtained	O
in	O
other	O
words	O
an	O
observation	O
that	O
lies	O
strictly	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
does	O
not	O
affect	O
the	O
support	B
vector	B
classifier	B
changing	O
the	O
position	O
of	O
that	O
observation	O
would	O
not	O
change	O
the	O
classifier	B
at	O
all	O
provided	O
that	O
its	O
position	O
remains	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
observations	B
that	O
lie	O
directly	O
on	O
the	O
margin	B
or	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
for	O
their	O
class	O
are	O
known	O
as	O
support	O
vectors	O
these	O
observations	B
do	O
affect	O
the	O
support	B
vector	B
classifier	B
the	O
fact	O
that	O
only	O
support	O
vectors	O
affect	O
the	O
classifier	B
is	O
in	O
line	B
with	O
our	O
previous	O
assertion	O
that	O
c	O
controls	O
the	O
bias-variance	O
trade-off	B
of	O
the	O
support	B
vector	B
classifier	B
when	O
the	O
tuning	B
parameter	B
c	O
is	O
large	O
then	O
the	O
margin	B
is	O
wide	O
many	O
observations	B
violate	O
the	O
margin	B
and	O
so	O
there	O
are	O
many	O
support	O
vectors	O
in	O
this	O
case	O
many	O
observations	B
are	O
involved	O
in	O
determining	O
the	O
hyperplane	B
the	O
top	O
left	O
panel	O
in	O
figure	O
illustrates	O
this	O
setting	O
this	O
classifier	B
has	O
low	O
variance	B
many	O
observations	B
are	O
support	O
vectors	O
support	B
vector	B
machines	O
x	O
x	O
x	O
x	O
figure	O
a	O
support	B
vector	B
classifier	B
was	O
fit	O
using	O
four	O
different	O
values	O
of	O
the	O
tuning	B
parameter	B
c	O
in	O
the	O
largest	O
value	O
of	O
c	O
was	O
used	O
in	O
the	O
top	O
left	O
panel	O
and	O
smaller	O
values	O
were	O
used	O
in	O
the	O
top	O
right	O
bottom	O
left	O
and	O
bottom	O
right	O
panels	O
when	O
c	O
is	O
large	O
then	O
there	O
is	O
a	O
high	O
tolerance	O
for	O
observations	B
being	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
and	O
so	O
the	O
margin	B
will	O
be	O
large	O
as	O
c	O
decreases	O
the	O
tolerance	O
for	O
observations	B
being	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
decreases	O
and	O
the	O
margin	B
narrows	O
but	O
potentially	O
high	O
bias	B
in	O
contrast	B
if	O
c	O
is	O
small	O
then	O
there	O
will	O
be	O
fewer	O
support	O
vectors	O
and	O
hence	O
the	O
resulting	O
classifier	B
will	O
have	O
low	O
bias	B
but	O
high	O
variance	B
the	O
bottom	O
right	O
panel	O
in	O
figure	O
illustrates	O
this	O
setting	O
with	O
only	O
eight	O
support	O
vectors	O
the	O
fact	O
that	O
the	O
support	B
vector	B
classifier	B
s	O
decision	O
rule	O
is	O
based	O
only	O
on	O
a	O
potentially	O
small	O
subset	O
of	O
the	O
training	O
observations	B
support	O
vectors	O
means	O
that	O
it	O
is	O
quite	O
robust	B
to	O
the	O
behavior	O
of	O
observations	B
that	O
are	O
far	O
away	O
from	O
the	O
hyperplane	B
this	O
property	O
is	O
distinct	O
from	O
some	O
of	O
the	O
other	O
classification	B
methods	O
that	O
we	O
have	O
seen	O
in	O
preceding	O
chapters	O
such	O
as	O
linear	B
discriminant	I
analysis	B
recall	B
that	O
the	O
lda	O
classification	B
rule	O
support	B
vector	B
machines	O
x	O
x	O
figure	O
left	O
the	O
observations	B
fall	O
into	O
two	O
classes	O
with	O
a	O
non-linear	B
boundary	O
between	O
them	O
right	O
the	O
support	B
vector	B
classifier	B
seeks	O
a	O
linear	B
boundary	O
and	O
consequently	O
performs	O
very	O
poorly	O
depends	O
on	O
the	O
mean	O
of	O
all	O
of	O
the	O
observations	B
within	O
each	O
class	O
as	O
well	O
as	O
the	O
within-class	O
covariance	O
matrix	O
computed	O
using	O
all	O
of	O
the	O
observations	B
in	O
contrast	B
logistic	B
regression	B
unlike	O
lda	O
has	O
very	O
low	O
sensitivity	B
to	O
observations	B
far	O
from	O
the	O
decision	B
boundary	I
in	O
fact	O
we	O
will	O
see	O
in	O
section	O
that	O
the	O
support	B
vector	B
classifier	B
and	O
logistic	B
regression	B
are	O
closely	O
related	O
support	B
vector	B
machines	O
we	O
first	O
discuss	O
a	O
general	O
mechanism	O
for	O
converting	O
a	O
linear	B
classifier	B
into	O
one	O
that	O
produces	O
non-linear	B
decision	O
boundaries	O
we	O
then	O
introduce	O
the	O
support	B
vector	B
machine	B
which	O
does	O
this	O
in	O
an	O
automatic	O
way	O
classification	B
with	O
non-linear	B
decision	O
boundaries	O
the	O
support	B
vector	B
classifier	B
is	O
a	O
natural	B
approach	B
for	O
classification	B
in	O
the	O
two-class	O
setting	O
if	O
the	O
boundary	O
between	O
the	O
two	O
classes	O
is	O
linear	B
however	O
in	O
practice	O
we	O
are	O
sometimes	O
faced	O
with	O
non-linear	B
class	O
boundaries	O
for	O
instance	O
consider	O
the	O
data	B
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
it	O
is	O
clear	O
that	O
a	O
support	B
vector	B
classifier	B
or	O
any	O
linear	B
classifier	B
will	O
perform	O
poorly	O
here	O
indeed	O
the	O
support	B
vector	B
classifier	B
shown	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
is	O
useless	O
here	O
in	O
chapter	O
we	O
are	O
faced	O
with	O
an	O
analogous	O
situation	O
we	O
see	O
there	O
that	O
the	O
performance	O
of	O
linear	B
regression	B
can	O
suffer	O
when	O
there	O
is	O
a	O
nonlinear	O
relationship	O
between	O
the	O
predictors	O
and	O
the	O
outcome	O
in	O
that	O
case	O
we	O
consider	O
enlarging	O
the	O
feature	B
space	O
using	O
functions	O
of	O
the	O
predictors	O
support	B
vector	B
machines	O
such	O
as	O
quadratic	B
and	O
cubic	B
terms	O
in	O
order	O
to	O
address	O
this	O
non-linearity	O
in	O
the	O
case	O
of	O
the	O
support	B
vector	B
classifier	B
we	O
could	O
address	O
the	O
problem	O
of	O
possibly	O
non-linear	B
boundaries	O
between	O
classes	O
in	O
a	O
similar	O
way	O
by	O
enlarging	O
the	O
feature	B
space	O
using	O
quadratic	B
cubic	B
and	O
even	O
higher-order	O
polynomial	B
functions	O
of	O
the	O
predictors	O
for	O
instance	O
rather	O
than	O
fitting	O
a	O
support	B
vector	B
classifier	B
using	O
p	O
features	O
xp	O
we	O
could	O
instead	O
fit	O
a	O
support	B
vector	B
classifier	B
using	O
features	O
x	O
x	O
xp	O
x	O
p	O
then	O
would	O
become	O
m	O
maximize	O
m	O
ij	O
subject	O
to	O
yi	O
c	O
jk	O
for	O
j	O
j	O
why	O
does	O
this	O
lead	O
to	O
a	O
non-linear	B
decision	B
boundary	I
in	O
the	O
enlarged	O
feature	B
space	O
the	O
decision	B
boundary	I
that	O
results	O
from	O
is	O
in	O
fact	O
linear	B
but	O
in	O
the	O
original	O
feature	B
space	O
the	O
decision	B
boundary	I
is	O
of	O
the	O
form	O
qx	O
where	O
q	O
is	O
a	O
quadratic	B
polynomial	B
and	O
its	O
solutions	O
are	O
generally	O
non-linear	B
one	O
might	O
additionally	O
want	O
to	O
enlarge	O
the	O
feature	B
space	O
with	O
higher-order	O
polynomial	B
terms	O
or	O
with	O
interaction	B
terms	O
of	O
the	O
form	O
alternatively	O
other	O
functions	O
of	O
the	O
predictors	O
could	O
be	O
considered	O
rather	O
than	O
polynomials	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
there	O
are	O
many	O
possible	O
ways	O
to	O
enlarge	O
the	O
feature	B
space	O
and	O
that	O
unless	O
we	O
are	O
careful	O
we	O
could	O
end	O
up	O
with	O
a	O
huge	O
number	O
of	O
features	O
then	O
computations	O
would	O
become	O
unmanageable	O
the	O
support	B
vector	B
machine	B
which	O
we	O
present	O
next	O
allows	O
us	O
to	O
enlarge	O
the	O
feature	B
space	O
used	O
by	O
the	O
support	B
vector	B
classifier	B
in	O
a	O
way	O
that	O
leads	O
to	O
efficient	O
computations	O
the	O
support	B
vector	B
machine	B
the	O
support	B
vector	B
machine	B
is	O
an	O
extension	O
of	O
the	O
support	B
vector	B
classifier	B
that	O
results	O
from	O
enlarging	O
the	O
feature	B
space	O
in	O
a	O
specific	O
way	O
using	O
kernels	O
we	O
will	O
now	O
discuss	O
this	O
extension	O
the	O
details	O
of	O
which	O
are	O
somewhat	O
complex	O
and	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
however	O
the	O
main	O
idea	O
is	O
described	O
in	O
section	O
we	O
may	O
want	O
to	O
enlarge	O
our	O
feature	B
space	O
support	B
vector	B
machine	B
kernel	B
support	B
vector	B
machines	O
in	O
order	O
to	O
accommodate	O
a	O
non-linear	B
boundary	O
between	O
the	O
classes	O
the	O
kernel	B
approach	B
that	O
we	O
describe	O
here	O
is	O
simply	O
an	O
efficient	O
computational	O
approach	B
for	O
enacting	O
this	O
idea	O
we	O
have	O
not	O
discussed	O
exactly	O
how	O
the	O
support	B
vector	B
classifier	B
is	O
computed	O
because	O
the	O
details	O
become	O
somewhat	O
technical	O
however	O
it	O
turns	O
out	O
that	O
the	O
solution	O
to	O
the	O
support	B
vector	B
classifier	B
problem	O
involves	O
only	O
the	O
inner	O
products	O
of	O
the	O
observations	B
opposed	O
to	O
the	O
observations	B
themselves	O
the	O
inner	B
product	I
of	O
two	O
r-vectors	O
a	O
and	O
b	O
is	O
defined	O
as	O
r	O
aibi	O
thus	O
the	O
inner	B
product	I
of	O
two	O
observations	B
xi	O
is	O
given	O
by	O
xij	O
it	O
can	O
be	O
shown	O
that	O
the	O
linear	B
support	B
vector	B
classifier	B
can	O
be	O
represented	O
as	O
f	O
where	O
there	O
are	O
n	O
parameters	O
i	O
observation	O
to	O
estimate	O
the	O
parameters	O
n	O
and	O
all	O
we	O
need	O
are	O
the	O
inner	O
products	O
between	O
all	O
pairs	O
of	O
training	O
observations	B
means	O
nn	O
and	O
gives	O
the	O
number	O
of	O
pairs	O
i	O
n	O
one	O
per	O
training	O
n	O
n	O
notation	O
among	O
a	O
set	B
of	O
n	O
items	O
notice	O
that	O
in	O
in	O
order	O
to	O
evaluate	O
the	O
function	B
f	O
we	O
need	O
to	O
compute	O
the	O
inner	B
product	I
between	O
the	O
new	O
point	O
x	O
and	O
each	O
of	O
the	O
training	O
points	O
xi	O
however	O
it	O
turns	O
out	O
that	O
i	O
is	O
nonzero	O
only	O
for	O
the	O
support	O
vectors	O
in	O
the	O
solution	O
that	O
is	O
if	O
a	O
training	O
observation	O
is	O
not	O
a	O
support	B
vector	B
then	O
its	O
i	O
equals	O
zero	O
so	O
if	O
s	O
is	O
the	O
collection	O
of	O
indices	O
of	O
these	O
support	O
points	O
we	O
can	O
rewrite	O
any	O
solution	O
function	B
of	O
the	O
form	O
as	O
f	O
i	O
s	O
which	O
typically	O
involves	O
far	O
fewer	O
terms	O
than	O
in	O
to	O
summarize	O
in	O
representing	O
the	O
linear	B
classifier	B
f	O
and	O
in	O
computing	O
its	O
coefficients	O
all	O
we	O
need	O
are	O
inner	O
products	O
now	O
suppose	O
that	O
every	O
time	O
the	O
inner	B
product	I
appears	O
in	O
the	O
representation	O
or	O
in	O
a	O
calculation	O
of	O
the	O
solution	O
for	O
the	O
support	O
expanding	O
each	O
of	O
the	O
inner	O
products	O
in	O
it	O
is	O
easy	O
to	O
see	O
that	O
f	O
is	O
a	O
linear	B
function	B
of	O
the	O
coordinates	O
of	O
x	O
doing	O
so	O
also	O
establishes	O
the	O
correspondence	O
between	O
the	O
i	O
and	O
the	O
original	O
parameters	O
j	O
support	B
vector	B
machines	O
vector	B
classifier	B
we	O
replace	O
it	O
with	O
a	O
generalization	O
of	O
the	O
inner	B
product	I
of	O
the	O
form	O
kxi	O
where	O
k	O
is	O
some	O
function	B
that	O
we	O
will	O
refer	O
to	O
as	O
a	O
kernel	B
a	O
kernel	B
is	O
a	O
function	B
that	O
quantifies	O
the	O
similarity	O
of	O
two	O
observations	B
for	O
instance	O
we	O
could	O
simply	O
take	O
kernel	B
kxi	O
xij	O
which	O
would	O
just	O
give	O
us	O
back	O
the	O
support	B
vector	B
classifier	B
equation	O
is	O
known	O
as	O
a	O
linear	B
kernel	B
because	O
the	O
support	B
vector	B
classifier	B
is	O
linear	B
in	O
the	O
features	O
the	O
linear	B
kernel	B
essentially	O
quantifies	O
the	O
similarity	O
of	O
a	O
pair	O
of	O
observations	B
using	O
pearson	O
correlation	B
but	O
one	O
could	O
instead	O
choose	O
another	O
form	O
for	O
for	O
instance	O
one	O
could	O
replace	O
every	O
instance	O
of	O
p	O
xij	O
with	O
the	O
quantity	O
kxi	O
xij	O
this	O
is	O
known	O
as	O
a	O
polynomial	B
kernel	B
of	O
degree	O
d	O
where	O
d	O
is	O
a	O
positive	O
integer	O
using	O
such	O
a	O
kernel	B
with	O
d	O
instead	O
of	O
the	O
standard	O
linear	B
kernel	B
in	O
the	O
support	B
vector	B
classifier	B
algorithm	O
leads	O
to	O
a	O
much	O
more	O
flexible	O
decision	B
boundary	I
it	O
essentially	O
amounts	O
to	O
fitting	O
a	O
support	B
vector	B
classifier	B
in	O
a	O
higher-dimensional	O
space	O
involving	O
polynomials	O
of	O
degree	O
d	O
rather	O
than	O
in	O
the	O
original	O
feature	B
space	O
when	O
the	O
support	B
vector	B
classifier	B
is	O
combined	O
with	O
a	O
non-linear	B
kernel	B
such	O
as	O
the	O
resulting	O
classifier	B
is	O
known	O
as	O
a	O
support	B
vector	B
machine	B
note	O
that	O
in	O
this	O
case	O
the	O
function	B
has	O
the	O
form	O
f	O
ikx	O
xi	O
i	O
s	O
the	O
left-hand	O
panel	O
of	O
figure	O
shows	O
an	O
example	O
of	O
an	O
svm	O
with	O
a	O
polynomial	B
kernel	B
applied	O
to	O
the	O
non-linear	B
data	B
from	O
figure	O
the	O
fit	O
is	O
a	O
substantial	O
improvement	O
over	O
the	O
linear	B
support	B
vector	B
classifier	B
when	O
d	O
then	O
the	O
svm	O
reduces	O
to	O
the	O
support	B
vector	B
classifier	B
seen	O
earlier	O
in	O
this	O
chapter	O
the	O
polynomial	B
kernel	B
shown	O
in	O
is	O
one	O
example	O
of	O
a	O
possible	O
non-linear	B
kernel	B
but	O
alternatives	O
abound	O
another	O
popular	O
choice	O
is	O
the	O
radial	B
kernel	B
which	O
takes	O
the	O
form	O
kxi	O
exp	O
polynomial	B
kernel	B
radial	B
kernel	B
support	B
vector	B
machines	O
x	O
x	O
figure	O
left	O
an	O
svm	O
with	O
a	O
polynomial	B
kernel	B
of	O
degree	O
is	O
applied	O
to	O
the	O
non-linear	B
data	B
from	O
figure	O
resulting	O
in	O
a	O
far	O
more	O
appropriate	O
decision	O
rule	O
right	O
an	O
svm	O
with	O
a	O
radial	B
kernel	B
is	O
applied	O
in	O
this	O
example	O
either	O
kernel	B
is	O
capable	O
of	O
capturing	O
the	O
decision	B
boundary	I
in	O
is	O
a	O
positive	O
constant	O
the	O
right-hand	O
panel	O
of	O
figure	O
shows	O
an	O
example	O
of	O
an	O
svm	O
with	O
a	O
radial	B
kernel	B
on	O
this	O
non-linear	B
data	B
it	O
also	O
does	O
a	O
good	O
job	O
in	O
separating	O
the	O
two	O
classes	O
p	O
j	O
how	O
does	O
the	O
radial	B
kernel	B
actually	O
work	O
if	O
a	O
given	O
test	O
obser	O
pt	O
is	O
far	O
from	O
a	O
training	O
observation	O
xi	O
in	O
terms	O
of	O
x	O
vation	O
x	O
will	O
be	O
large	O
and	O
so	O
kxi	O
euclidean	B
distance	I
then	O
exp	O
xij	O
will	O
be	O
very	O
tiny	O
this	O
means	O
that	O
in	O
xi	O
p	O
j	O
will	O
play	O
virtually	O
no	O
role	O
in	O
f	O
recall	B
that	O
the	O
predicted	O
class	O
label	O
for	O
the	O
test	O
observation	O
x	O
in	O
other	O
words	O
training	O
observations	B
that	O
are	O
far	O
from	O
x	O
will	O
play	O
essentially	O
no	O
role	O
in	O
the	O
predicted	O
class	O
label	O
for	O
x	O
this	O
means	O
that	O
the	O
radial	B
kernel	B
has	O
very	O
local	B
behavior	O
in	O
the	O
sense	O
that	O
only	O
nearby	O
training	O
observations	B
have	O
an	O
effect	O
on	O
the	O
class	O
label	O
of	O
a	O
test	O
observation	O
is	O
based	O
on	O
the	O
sign	O
of	O
f	O
what	O
is	O
the	O
advantage	O
of	O
using	O
a	O
kernel	B
rather	O
than	O
simply	O
enlarging	O
the	O
feature	B
space	O
using	O
functions	O
of	O
the	O
original	O
features	O
as	O
in	O
one	O
advantage	O
is	O
computational	O
and	O
it	O
amounts	O
to	O
the	O
fact	O
that	O
using	O
kernels	O
one	O
need	O
only	O
compute	O
kxi	O
for	O
all	O
this	O
can	O
be	O
done	O
without	O
explicitly	O
working	O
in	O
the	O
enlarged	O
feature	B
space	O
this	O
is	O
important	O
because	O
in	O
many	O
applications	O
of	O
svms	O
the	O
enlarged	O
feature	B
space	O
is	O
so	O
large	O
that	O
computations	O
are	O
intractable	O
for	O
some	O
kernels	O
such	O
as	O
the	O
radial	B
kernel	B
the	O
feature	B
space	O
is	O
implicit	O
and	O
infinite-dimensional	O
so	O
we	O
could	O
never	O
do	O
the	O
computations	O
there	O
anyway	O
distinct	O
pairs	O
i	O
i	O
n	O
support	B
vector	B
machines	O
e	O
t	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
e	O
t	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
support	B
vector	B
classifier	B
lda	O
support	B
vector	B
classifier	B
svm	O
svm	O
svm	O
false	B
positive	I
rate	B
false	B
positive	I
rate	B
figure	O
roc	O
curves	O
for	O
the	O
heart	B
data	B
training	O
set	B
left	O
the	O
support	B
vector	B
classifier	B
and	O
lda	O
are	O
compared	O
right	O
the	O
support	B
vector	B
classifier	B
is	O
compared	O
to	O
an	O
svm	O
using	O
a	O
radial	B
basis	B
kernel	B
with	O
and	O
an	O
application	O
to	O
the	O
heart	B
disease	O
data	B
in	O
chapter	O
we	O
apply	O
decision	O
trees	O
and	O
related	O
methods	O
to	O
the	O
heart	B
data	B
the	O
aim	O
is	O
to	O
use	O
predictors	O
such	O
as	O
age	O
sex	O
and	O
chol	O
in	O
order	O
to	O
predict	O
whether	O
an	O
individual	O
has	O
heart	B
disease	O
we	O
now	O
investigate	O
how	O
an	O
svm	O
compares	O
to	O
lda	O
on	O
this	O
data	B
after	O
removing	O
missing	O
observations	B
the	O
data	B
consist	O
of	O
subjects	O
which	O
we	O
randomly	O
split	O
into	O
training	O
and	O
test	O
observations	B
we	O
first	O
fit	O
lda	O
and	O
the	O
support	B
vector	B
classifier	B
to	O
the	O
training	O
data	B
note	O
that	O
the	O
support	B
vector	B
classifier	B
is	O
equivalent	O
to	O
a	O
svm	O
using	O
a	O
polynomial	B
kernel	B
of	O
degree	O
d	O
the	O
left-hand	O
panel	O
of	O
figure	O
displays	O
roc	O
curves	O
in	O
section	O
for	O
the	O
training	O
set	B
predictions	O
for	O
both	O
lda	O
and	O
the	O
support	B
vector	B
classifier	B
both	O
classifiers	O
compute	O
scores	O
of	O
the	O
form	O
f	O
pxp	O
for	O
each	O
observation	O
for	O
any	O
given	O
cutoff	O
t	O
we	O
classify	O
observations	B
into	O
the	O
heart	B
disease	O
or	O
no	O
heart	B
disease	O
categories	O
depending	O
on	O
whether	O
f	O
t	O
or	O
f	O
t	O
the	O
roc	B
curve	I
is	O
obtained	O
by	O
forming	O
these	O
predictions	O
and	O
computing	O
the	O
false	B
positive	I
and	O
true	B
positive	I
rates	O
for	O
a	O
range	O
of	O
values	O
of	O
t	O
an	O
optimal	O
classifier	B
will	O
hug	O
the	O
top	O
left	O
corner	O
of	O
the	O
roc	O
plot	B
in	O
this	O
instance	O
lda	O
and	O
the	O
support	B
vector	B
classifier	B
both	O
perform	O
well	O
though	O
there	O
is	O
a	O
suggestion	O
that	O
the	O
support	B
vector	B
classifier	B
may	O
be	O
slightly	O
superior	O
the	O
right-hand	O
panel	O
of	O
figure	O
displays	O
roc	O
curves	O
for	O
svms	O
using	O
a	O
radial	B
kernel	B
with	O
various	O
values	O
of	O
as	O
increases	O
and	O
the	O
fit	O
becomes	O
appears	O
to	O
give	O
more	O
non-linear	B
the	O
roc	O
curves	O
improve	O
using	O
an	O
almost	O
perfect	O
roc	B
curve	I
however	O
these	O
curves	O
represent	O
training	O
error	B
rates	O
which	O
can	O
be	O
misleading	O
in	O
terms	O
of	O
performance	O
on	O
new	O
test	O
data	B
figure	O
displays	O
roc	O
curves	O
computed	O
on	O
the	O
test	O
observa	O
svms	O
with	O
more	O
than	O
two	O
classes	O
e	O
t	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
e	O
t	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
support	B
vector	B
classifier	B
lda	O
support	B
vector	B
classifier	B
svm	O
svm	O
svm	O
false	B
positive	I
rate	B
false	B
positive	I
rate	B
figure	O
roc	O
curves	O
for	O
the	O
test	O
set	B
of	O
the	O
heart	B
data	B
left	O
the	O
support	B
vector	B
classifier	B
and	O
lda	O
are	O
compared	O
right	O
the	O
support	B
vector	B
classifier	B
is	O
compared	O
to	O
an	O
svm	O
using	O
a	O
radial	B
basis	B
kernel	B
with	O
and	O
tions	O
we	O
observe	O
some	O
differences	O
from	O
the	O
training	O
roc	O
curves	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
support	B
vector	B
classifier	B
appears	O
to	O
have	O
a	O
small	O
advantage	O
over	O
lda	O
these	O
differences	O
are	O
not	O
statisti	O
which	O
cally	O
significant	O
in	O
the	O
right-hand	O
panel	O
the	O
svm	O
using	O
showed	O
the	O
best	O
results	O
on	O
the	O
training	O
data	B
produces	O
the	O
worst	O
estimates	O
on	O
the	O
test	O
data	B
this	O
is	O
once	O
again	O
evidence	O
that	O
while	O
a	O
more	O
flexible	O
method	O
will	O
often	O
produce	O
lower	O
training	O
error	B
rates	O
this	O
does	O
not	O
neces	O
sarily	O
lead	O
to	O
improved	O
performance	O
on	O
test	O
data	B
the	O
svms	O
with	O
perform	O
comparably	O
to	O
the	O
support	B
vector	B
classifier	B
and	O
all	O
and	O
three	O
outperform	O
the	O
svm	O
with	O
svms	O
with	O
more	O
than	O
two	O
classes	O
so	O
far	O
our	O
discussion	O
has	O
been	O
limited	O
to	O
the	O
case	O
of	O
binary	B
classification	B
that	O
is	O
classification	B
in	O
the	O
two-class	O
setting	O
how	O
can	O
we	O
extend	O
svms	O
to	O
the	O
more	O
general	O
case	O
where	O
we	O
have	O
some	O
arbitrary	O
number	O
of	O
classes	O
it	O
turns	O
out	O
that	O
the	O
concept	O
of	O
separating	O
hyperplanes	O
upon	O
which	O
svms	O
are	O
based	O
does	O
not	O
lend	O
itself	O
naturally	O
to	O
more	O
than	O
two	O
classes	O
though	O
a	O
number	O
of	O
proposals	O
for	O
extending	O
svms	O
to	O
the	O
k-class	O
case	O
have	O
been	O
made	O
the	O
two	O
most	O
popular	O
are	O
the	O
one-versus-one	B
and	O
one-versus-all	B
approaches	O
we	O
briefly	O
discuss	O
those	O
two	O
approaches	O
here	O
one-versus-one	B
classification	B
suppose	O
that	O
we	O
would	O
like	O
to	O
perform	O
classification	B
using	O
svms	O
and	O
there	O
are	O
k	O
classes	O
a	O
one-versus-one	B
or	O
all-pairs	O
approach	B
constructs	O
k	O
one-versusone	O
support	B
vector	B
machines	O
svms	O
each	O
of	O
which	O
compares	O
a	O
pair	O
of	O
classes	O
for	O
example	O
one	O
such	O
svm	O
might	O
compare	O
the	O
kth	O
class	O
coded	O
as	O
to	O
the	O
k	O
th	O
class	O
coded	O
as	O
we	O
classify	O
a	O
test	O
observation	O
using	O
each	O
of	O
the	O
k	O
classifiers	O
and	O
we	O
tally	O
the	O
number	O
of	O
times	O
that	O
the	O
test	O
observation	O
is	O
assigned	O
to	O
each	O
of	O
the	O
k	O
classes	O
the	O
final	O
classification	B
is	O
performed	O
by	O
assigning	O
the	O
test	O
observation	O
to	O
the	O
class	O
to	O
which	O
it	O
was	O
most	O
frequently	O
assigned	O
in	O
these	O
k	O
pairwise	O
classifications	O
one-versus-all	B
classification	B
the	O
one-versus-all	B
approach	B
is	O
an	O
alternative	O
procedure	O
for	O
applying	O
svms	O
one-versusin	O
the	O
case	O
of	O
k	O
classes	O
we	O
fit	O
k	O
svms	O
each	O
time	O
comparing	O
one	O
of	O
the	O
k	O
classes	O
to	O
the	O
remaining	O
k	O
classes	O
let	O
pk	O
denote	O
the	O
parameters	O
that	O
result	O
from	O
fitting	O
an	O
svm	O
comparing	O
the	O
kth	O
class	O
as	O
to	O
the	O
others	O
as	O
let	O
x	O
denote	O
a	O
test	O
observation	O
we	O
assign	O
the	O
observation	O
to	O
the	O
class	O
for	O
which	O
pkx	O
p	O
is	O
largest	O
as	O
this	O
amounts	O
to	O
a	O
high	O
level	B
of	O
confidence	O
that	O
the	O
test	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
rather	O
than	O
to	O
any	O
of	O
the	O
other	O
classes	O
all	O
relationship	O
to	O
logistic	B
regression	B
when	O
svms	O
were	O
first	O
introduced	O
in	O
the	O
they	O
made	O
quite	O
a	O
splash	O
in	O
the	O
statistical	O
and	O
machine	B
learning	O
communities	O
this	O
was	O
due	O
in	O
part	O
to	O
their	O
good	O
performance	O
good	O
marketing	O
and	O
also	O
to	O
the	O
fact	O
that	O
the	O
underlying	O
approach	B
seemed	O
both	O
novel	O
and	O
mysterious	O
the	O
idea	O
of	O
finding	O
a	O
hyperplane	B
that	O
separates	O
the	O
data	B
as	O
well	O
as	O
possible	O
while	O
allowing	O
some	O
violations	O
to	O
this	O
separation	O
seemed	O
distinctly	O
different	O
from	O
classical	O
approaches	O
for	O
classification	B
such	O
as	O
logistic	B
regression	B
and	O
linear	B
discriminant	I
analysis	B
moreover	O
the	O
idea	O
of	O
using	O
a	O
kernel	B
to	O
expand	O
the	O
feature	B
space	O
in	O
order	O
to	O
accommodate	O
non-linear	B
class	O
boundaries	O
appeared	O
to	O
be	O
a	O
unique	O
and	O
valuable	O
characteristic	O
however	O
since	O
that	O
time	O
deep	O
connections	O
between	O
svms	O
and	O
other	O
more	O
classical	O
statistical	O
methods	O
have	O
emerged	O
it	O
turns	O
out	O
that	O
one	O
can	O
rewrite	O
the	O
criterion	O
for	O
fitting	O
the	O
support	B
vector	B
classifier	B
f	O
pxp	O
as	O
max	O
yif	O
j	O
minimize	O
p	O
relationship	O
to	O
logistic	B
regression	B
where	O
is	O
a	O
nonnegative	O
tuning	B
parameter	B
when	O
is	O
large	O
then	O
p	O
are	O
small	O
more	O
violations	O
to	O
the	O
margin	B
are	O
tolerated	O
and	O
a	O
low-variance	O
but	O
high-bias	O
classifier	B
will	O
result	O
when	O
is	O
small	O
then	O
few	O
violations	O
to	O
the	O
margin	B
will	O
occur	O
this	O
amounts	O
to	O
a	O
high-variance	O
but	O
low-bias	O
classifier	B
thus	O
a	O
small	O
value	O
of	O
in	O
amounts	O
to	O
a	O
small	O
value	O
of	O
c	O
in	O
note	O
that	O
the	O
j	O
term	B
in	O
is	O
the	O
ridge	O
penalty	B
term	B
from	O
section	O
and	O
plays	O
a	O
similar	O
role	O
in	O
controlling	O
the	O
bias-variance	O
trade-off	B
for	O
the	O
support	B
vector	B
classifier	B
p	O
now	O
takes	O
the	O
loss	O
penalty	B
form	O
that	O
we	O
have	O
seen	O
repeatedly	O
throughout	O
this	O
book	O
minimize	O
p	O
y	O
p	O
in	O
lx	O
y	O
is	O
some	O
loss	B
function	B
quantifying	O
the	O
extent	O
to	O
which	O
the	O
model	B
parametrized	O
by	O
fits	O
the	O
data	B
y	O
and	O
p	O
is	O
a	O
penalty	B
function	B
on	O
the	O
parameter	B
vector	B
whose	O
effect	O
is	O
controlled	O
by	O
a	O
nonnegative	O
tuning	B
parameter	B
for	O
instance	O
ridge	B
regression	B
and	O
the	O
lasso	B
both	O
take	O
this	O
form	O
with	O
yi	O
xij	O
j	O
j	O
for	O
p	O
lx	O
y	O
p	O
and	O
with	O
p	O
the	O
lasso	B
in	O
the	O
case	O
of	O
the	O
loss	B
function	B
instead	O
takes	O
the	O
form	O
j	O
for	O
ridge	B
regression	B
and	O
p	O
lx	O
y	O
max	O
yi	O
pxip	O
hinge	B
loss	I
this	O
is	O
known	O
as	O
hinge	B
loss	I
and	O
is	O
depicted	O
in	O
figure	O
however	O
it	O
turns	O
out	O
that	O
the	O
hinge	B
loss	B
function	B
is	O
closely	O
related	O
to	O
the	O
loss	B
function	B
used	O
in	O
logistic	B
regression	B
also	O
shown	O
in	O
figure	O
an	O
interesting	O
characteristic	O
of	O
the	O
support	B
vector	B
classifier	B
is	O
that	O
only	O
support	O
vectors	O
play	O
a	O
role	O
in	O
the	O
classifier	B
obtained	O
observations	B
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
do	O
not	O
affect	O
it	O
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
loss	B
function	B
shown	O
in	O
figure	O
is	O
exactly	O
zero	O
for	O
observations	B
for	O
which	O
yi	O
pxip	O
these	O
correspond	O
to	O
observations	B
that	O
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
in	O
contrast	B
the	O
loss	B
function	B
for	O
logistic	B
regression	B
shown	O
in	O
figure	O
is	O
not	O
exactly	O
zero	O
anywhere	O
but	O
it	O
is	O
very	O
small	O
for	O
observations	B
that	O
are	O
far	O
from	O
the	O
decision	B
boundary	I
due	O
to	O
the	O
similarities	O
between	O
their	O
loss	O
functions	O
logistic	B
regression	B
and	O
the	O
support	B
vector	B
classifier	B
often	O
give	O
very	O
similar	O
results	O
when	O
the	O
classes	O
are	O
well	O
separated	O
svms	O
tend	O
to	O
behave	O
better	O
than	O
logistic	B
regression	B
in	O
more	O
overlapping	O
regimes	O
logistic	B
regression	B
is	O
often	O
preferred	O
this	O
hinge-loss	O
penalty	B
representation	O
the	O
margin	B
corresponds	O
to	O
the	O
value	O
one	O
and	O
the	O
width	O
of	O
the	O
margin	B
is	O
determined	O
by	O
j	O
support	B
vector	B
machines	O
s	O
s	O
o	O
l	O
svm	O
loss	O
logistic	B
regression	B
loss	O
yi	O
pxip	O
figure	O
the	O
svm	O
and	O
logistic	B
regression	B
loss	O
functions	O
are	O
compared	O
as	O
a	O
function	B
of	O
yi	O
pxip	O
when	O
yi	O
pxip	O
is	O
greater	O
than	O
then	O
the	O
svm	O
loss	O
is	O
zero	O
since	O
this	O
corresponds	O
to	O
an	O
observation	O
that	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
overall	O
the	O
two	O
loss	O
functions	O
have	O
quite	O
similar	O
behavior	O
when	O
the	O
support	B
vector	B
classifier	B
and	O
svm	O
were	O
first	O
introduced	O
it	O
was	O
thought	O
that	O
the	O
tuning	B
parameter	B
c	O
in	O
was	O
an	O
unimportant	O
nuisance	O
parameter	B
that	O
could	O
be	O
set	B
to	O
some	O
default	B
value	O
like	O
however	O
the	O
loss	O
penalty	B
formulation	O
for	O
the	O
support	B
vector	B
classifier	B
indicates	O
that	O
this	O
is	O
not	O
the	O
case	O
the	O
choice	O
of	O
tuning	B
parameter	B
is	O
very	O
important	O
and	O
determines	O
the	O
extent	O
to	O
which	O
the	O
model	B
underfits	O
or	O
overfits	O
the	O
data	B
as	O
illustrated	O
for	O
example	O
in	O
figure	O
we	O
have	O
established	O
that	O
the	O
support	B
vector	B
classifier	B
is	O
closely	O
related	O
to	O
logistic	B
regression	B
and	O
other	O
preexisting	O
statistical	O
methods	O
is	O
the	O
svm	O
unique	O
in	O
its	O
use	O
of	O
kernels	O
to	O
enlarge	O
the	O
feature	B
space	O
to	O
accommodate	O
non-linear	B
class	O
boundaries	O
the	O
answer	O
to	O
this	O
question	O
is	O
no	O
we	O
could	O
just	O
as	O
well	O
perform	O
logistic	B
regression	B
or	O
many	O
of	O
the	O
other	O
classification	B
methods	O
seen	O
in	O
this	O
book	O
using	O
non-linear	B
kernels	O
this	O
is	O
closely	O
related	O
to	O
some	O
of	O
the	O
non-linear	B
approaches	O
seen	O
in	O
chapter	O
however	O
for	O
historical	O
reasons	O
the	O
use	O
of	O
non-linear	B
kernels	O
is	O
much	O
more	O
widespread	O
in	O
the	O
context	O
of	O
svms	O
than	O
in	O
the	O
context	O
of	O
logistic	B
regression	B
or	O
other	O
methods	O
though	O
we	O
have	O
not	O
addressed	O
it	O
here	O
there	O
is	O
in	O
fact	O
an	O
extension	O
of	O
the	O
svm	O
for	O
regression	B
for	O
a	O
quantitative	B
rather	O
than	O
a	O
qualitative	B
response	B
called	O
support	B
vector	B
regression	B
in	O
chapter	O
we	O
saw	O
that	O
least	B
squares	I
regression	B
seeks	O
coefficients	O
p	O
such	O
that	O
the	O
sum	O
of	O
squared	O
residuals	B
is	O
as	O
small	O
as	O
possible	O
from	O
chapter	O
that	O
residuals	B
are	O
defined	O
as	O
yi	O
pxip	O
support	B
vector	B
regression	B
instead	O
seeks	O
coefficients	O
that	O
minimize	O
a	O
different	O
type	O
of	O
loss	O
where	O
only	O
residuals	B
larger	O
in	O
absolute	O
value	O
than	O
some	O
positive	O
constant	O
support	B
vector	B
regression	B
lab	O
support	B
vector	B
machines	O
contribute	O
to	O
the	O
loss	B
function	B
this	O
is	O
an	O
extension	O
of	O
the	O
margin	B
used	O
in	O
support	B
vector	B
classifiers	O
to	O
the	O
regression	B
setting	O
lab	O
support	B
vector	B
machines	O
we	O
use	O
the	O
library	O
in	O
r	O
to	O
demonstrate	O
the	O
support	B
vector	B
classifier	B
and	O
the	O
svm	O
another	O
option	O
is	O
the	O
liblinear	O
library	O
which	O
is	O
useful	O
for	O
very	O
large	O
linear	B
problems	O
svm	O
support	B
vector	B
classifier	B
the	O
library	O
contains	O
implementations	O
for	O
a	O
number	O
of	O
statistical	O
learning	O
methods	O
in	O
particular	O
the	O
svm	O
function	B
can	O
be	O
used	O
to	O
fit	O
a	O
support	B
vector	B
classifier	B
when	O
the	O
argument	B
kernellinear	O
is	O
used	O
this	O
function	B
uses	O
a	O
slightly	O
different	O
formulation	O
from	O
and	O
for	O
the	O
support	B
vector	B
classifier	B
a	O
cost	O
argument	B
allows	O
us	O
to	O
specify	O
the	O
cost	O
of	O
a	O
violation	O
to	O
the	O
margin	B
when	O
the	O
cost	O
argument	B
is	O
small	O
then	O
the	O
margins	O
will	O
be	O
wide	O
and	O
many	O
support	O
vectors	O
will	O
be	O
on	O
the	O
margin	B
or	O
will	O
violate	O
the	O
margin	B
when	O
the	O
cost	O
argument	B
is	O
large	O
then	O
the	O
margins	O
will	O
be	O
narrow	O
and	O
there	O
will	O
be	O
few	O
support	O
vectors	O
on	O
the	O
margin	B
or	O
violating	O
the	O
margin	B
we	O
now	O
use	O
the	O
svm	O
function	B
to	O
fit	O
the	O
support	B
vector	B
classifier	B
for	O
a	O
given	O
value	O
of	O
the	O
cost	O
parameter	B
here	O
we	O
demonstrate	O
the	O
use	O
of	O
this	O
function	B
on	O
a	O
two-dimensional	O
example	O
so	O
that	O
we	O
can	O
plot	B
the	O
resulting	O
decision	B
boundary	I
we	O
begin	O
by	O
generating	O
the	O
observations	B
which	O
belong	O
to	O
two	O
classes	O
and	O
checking	O
whether	O
the	O
classes	O
are	O
linearly	O
separable	O
set	B
seed	B
x	O
matrix	O
rnorm	O
ncol	O
y	O
c	O
rep	O
rep	O
x	O
y	O
x	O
y	O
plot	B
col	O
y	O
they	O
are	O
not	O
next	O
we	O
fit	O
the	O
support	B
vector	B
classifier	B
note	O
that	O
in	O
order	O
for	O
the	O
svm	O
function	B
to	O
perform	O
classification	B
opposed	O
to	O
svm-based	O
regression	B
we	O
must	O
encode	O
the	O
response	B
as	O
a	O
factor	B
variable	B
we	O
now	O
create	O
a	O
data	B
frame	I
with	O
the	O
response	B
coded	O
as	O
a	O
factor	B
dat	O
data	B
frame	I
x	O
y	O
as	O
factor	B
y	O
library	O
svmfit	O
svm	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
scale	O
false	O
support	B
vector	B
machines	O
the	O
argument	B
scalefalse	O
tells	O
the	O
svm	O
function	B
not	O
to	O
scale	O
each	O
feature	B
to	O
have	O
mean	O
zero	O
or	O
standard	O
deviation	O
one	O
depending	O
on	O
the	O
application	O
one	O
might	O
prefer	O
to	O
use	O
scaletrue	O
we	O
can	O
now	O
plot	B
the	O
support	B
vector	B
classifier	B
obtained	O
plot	B
svmfit	O
dat	O
note	O
that	O
the	O
two	O
arguments	O
to	O
the	O
plot	B
svm	O
function	B
are	O
the	O
output	B
of	O
the	O
call	O
to	O
svm	O
as	O
well	O
as	O
the	O
data	B
used	O
in	O
the	O
call	O
to	O
svm	O
the	O
region	O
of	O
feature	B
space	O
that	O
will	O
be	O
assigned	O
to	O
the	O
class	O
is	O
shown	O
in	O
light	O
blue	O
and	O
the	O
region	O
that	O
will	O
be	O
assigned	O
to	O
the	O
class	O
is	O
shown	O
in	O
purple	O
the	O
decision	B
boundary	I
between	O
the	O
two	O
classes	O
is	O
linear	B
we	O
used	O
the	O
argument	B
kernellinear	O
though	O
due	O
to	O
the	O
way	O
in	O
which	O
the	O
plotting	O
function	B
is	O
implemented	O
in	O
this	O
library	O
the	O
decision	B
boundary	I
looks	O
somewhat	O
jagged	O
in	O
the	O
plot	B
note	O
that	O
here	O
the	O
second	O
feature	B
is	O
plotted	O
on	O
the	O
x-axis	O
and	O
the	O
first	O
feature	B
is	O
plotted	O
on	O
the	O
y-axis	O
in	O
contrast	B
to	O
the	O
behavior	O
of	O
the	O
usual	O
plot	B
function	B
in	O
r	O
the	O
support	O
vectors	O
are	O
plotted	O
as	O
crosses	O
and	O
the	O
remaining	O
observations	B
are	O
plotted	O
as	O
circles	O
we	O
see	O
here	O
that	O
there	O
are	O
seven	O
support	O
vectors	O
we	O
can	O
determine	O
their	O
identities	O
as	O
follows	O
s	O
v	O
m	O
f	O
i	O
t	O
i	O
n	O
d	O
e	O
x	O
we	O
can	O
obtain	O
some	O
basic	O
information	O
about	O
the	O
support	B
vector	B
classifier	B
fit	O
using	O
the	O
summary	O
command	O
summary	O
svmfit	O
call	O
svm	O
formula	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
scale	O
false	O
parameter	B
s	O
svm	O
type	O
svm	O
kernel	B
cost	O
gamma	O
c	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
linear	B
number	O
of	O
support	O
vectors	O
number	O
of	O
classes	O
levels	O
this	O
tells	O
us	O
for	O
instance	O
that	O
a	O
linear	B
kernel	B
was	O
used	O
with	O
and	O
that	O
there	O
were	O
seven	O
support	O
vectors	O
four	O
in	O
one	O
class	O
and	O
three	O
in	O
the	O
other	O
what	O
if	O
we	O
instead	O
used	O
a	O
smaller	O
value	O
of	O
the	O
cost	O
parameter	B
svmfit	O
svm	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
scale	O
false	O
plot	B
svmfit	O
dat	O
s	O
v	O
m	O
f	O
i	O
t	O
i	O
n	O
d	O
e	O
x	O
lab	O
support	B
vector	B
machines	O
now	O
that	O
a	O
smaller	O
value	O
of	O
the	O
cost	O
parameter	B
is	O
being	O
used	O
we	O
obtain	O
a	O
larger	O
number	O
of	O
support	O
vectors	O
because	O
the	O
margin	B
is	O
now	O
wider	O
unfortunately	O
the	O
svm	O
function	B
does	O
not	O
explicitly	O
output	B
the	O
coefficients	O
of	O
the	O
linear	B
decision	B
boundary	I
obtained	O
when	O
the	O
support	B
vector	B
classifier	B
is	O
fit	O
nor	O
does	O
it	O
output	B
the	O
width	O
of	O
the	O
margin	B
the	O
library	O
includes	O
a	O
built-in	O
function	B
tune	O
to	O
perform	O
crossvalidation	O
by	O
default	B
tune	O
performs	O
ten-fold	O
cross-validation	B
on	O
a	O
set	B
of	O
models	O
of	O
interest	O
in	O
order	O
to	O
use	O
this	O
function	B
we	O
pass	O
in	O
relevant	O
information	O
about	O
the	O
set	B
of	O
models	O
that	O
are	O
under	O
consideration	O
the	O
following	O
command	O
indicates	O
that	O
we	O
want	O
to	O
compare	O
svms	O
with	O
a	O
linear	B
kernel	B
using	O
a	O
range	O
of	O
values	O
of	O
the	O
cost	O
parameter	B
set	B
seed	B
tune	O
out	O
tune	O
svm	O
y	O
data	B
dat	O
kernel	B
linear	B
ranges	O
list	O
cost	O
c	O
we	O
can	O
easily	O
access	O
the	O
cross-validation	B
errors	O
for	O
each	O
of	O
these	O
models	O
using	O
the	O
summary	O
command	O
tune	O
summary	O
tune	O
out	O
parameter	B
tuning	O
of	O
svm	O
sampling	O
method	O
fold	O
cross	O
validation	O
best	O
parameters	O
cost	O
best	O
performan	O
c	O
e	O
detailed	O
performa	O
nc	O
e	O
results	O
cost	O
error	B
dispersio	O
n	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
we	O
see	O
that	O
results	O
in	O
the	O
lowest	O
cross-validation	B
error	B
rate	B
the	O
tune	O
function	B
stores	O
the	O
best	O
model	B
obtained	O
which	O
can	O
be	O
accessed	O
as	O
follows	O
bestmod	O
tune	O
outbest	O
model	B
summary	O
bestmod	O
the	O
predict	O
function	B
can	O
be	O
used	O
to	O
predict	O
the	O
class	O
label	O
on	O
a	O
set	B
of	O
test	O
observations	B
at	O
any	O
given	O
value	O
of	O
the	O
cost	O
parameter	B
we	O
begin	O
by	O
generating	O
a	O
test	O
data	B
set	B
xtest	O
matrix	O
rnorm	O
ncol	O
ytest	O
sample	O
c	O
rep	O
true	O
xtest	O
ytest	O
xtest	O
ytest	O
testdat	O
data	B
frame	I
x	O
xtest	O
y	O
as	O
factor	B
ytest	O
now	O
we	O
predict	O
the	O
class	O
labels	O
of	O
these	O
test	O
observations	B
here	O
we	O
use	O
the	O
best	O
model	B
obtained	O
through	O
cross-validation	B
in	O
order	O
to	O
make	O
predictions	O
support	B
vector	B
machines	O
ypred	O
predict	O
bestmod	O
testdat	O
table	O
predict	O
ypred	O
truth	O
testdaty	O
truth	O
predict	O
thus	O
with	O
this	O
value	O
of	O
cost	O
of	O
the	O
test	O
observations	B
are	O
correctly	O
classified	O
what	O
if	O
we	O
had	O
instead	O
used	O
svmfit	O
svm	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
scale	O
false	O
ypred	O
predict	O
svmfit	O
testdat	O
table	O
predict	O
ypred	O
truth	O
testdaty	O
truth	O
predict	O
in	O
this	O
case	O
one	O
additional	O
observation	O
is	O
misclassified	O
now	O
consider	O
a	O
situation	O
in	O
which	O
the	O
two	O
classes	O
are	O
linearly	O
separable	O
then	O
we	O
can	O
find	O
a	O
separating	B
hyperplane	B
using	O
the	O
svm	O
function	B
we	O
first	O
further	O
separate	O
the	O
two	O
classes	O
in	O
our	O
simulated	O
data	B
so	O
that	O
they	O
are	O
linearly	O
separable	O
x	O
y	O
x	O
y	O
plot	B
col	O
y	O
pch	O
now	O
the	O
observations	B
are	O
just	O
barely	O
linearly	O
separable	O
we	O
fit	O
the	O
support	B
vector	B
classifier	B
and	O
plot	B
the	O
resulting	O
hyperplane	B
using	O
a	O
very	O
large	O
value	O
of	O
cost	O
so	O
that	O
no	O
observations	B
are	O
misclassified	O
dat	O
data	B
frame	I
x	O
y	O
as	O
factor	B
y	O
svmfit	O
svm	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
summary	O
svmfit	O
call	O
svm	O
formula	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
e	O
parameter	B
s	O
svm	O
type	O
svm	O
kernel	B
cost	O
gamma	O
c	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
linear	B
e	O
number	O
of	O
support	O
vectors	O
number	O
of	O
classes	O
levels	O
plot	B
svmfit	O
dat	O
no	O
training	O
errors	O
were	O
made	O
and	O
only	O
three	O
support	O
vectors	O
were	O
used	O
however	O
we	O
can	O
see	O
from	O
the	O
figure	O
that	O
the	O
margin	B
is	O
very	O
narrow	O
the	O
observations	B
that	O
are	O
not	O
support	O
vectors	O
indicated	O
as	O
circles	O
are	O
very	O
lab	O
support	B
vector	B
machines	O
close	O
to	O
the	O
decision	B
boundary	I
it	O
seems	O
likely	O
that	O
this	O
model	B
will	O
perform	O
poorly	O
on	O
test	O
data	B
we	O
now	O
try	O
a	O
smaller	O
value	O
of	O
cost	O
svmfit	O
svm	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
summary	O
svmfit	O
plot	B
svmfit	O
dat	O
using	O
we	O
misclassify	O
a	O
training	O
observation	O
but	O
we	O
also	O
obtain	O
a	O
much	O
wider	O
margin	B
and	O
make	O
use	O
of	O
seven	O
support	O
vectors	O
it	O
seems	O
likely	O
that	O
this	O
model	B
will	O
perform	O
better	O
on	O
test	O
data	B
than	O
the	O
model	B
with	O
support	B
vector	B
machine	B
in	O
order	O
to	O
fit	O
an	O
svm	O
using	O
a	O
non-linear	B
kernel	B
we	O
once	O
again	O
use	O
the	O
svm	O
function	B
however	O
now	O
we	O
use	O
a	O
different	O
value	O
of	O
the	O
parameter	B
kernel	B
to	O
fit	O
an	O
svm	O
with	O
a	O
polynomial	B
kernel	B
we	O
use	O
kernelpolynomial	O
and	O
to	O
fit	O
an	O
svm	O
with	O
a	O
radial	B
kernel	B
we	O
use	O
kernelradial	O
in	O
the	O
former	O
case	O
we	O
also	O
use	O
the	O
degree	O
argument	B
to	O
specify	O
a	O
degree	O
for	O
the	O
polynomial	B
kernel	B
is	O
d	O
in	O
and	O
in	O
the	O
latter	O
case	O
we	O
use	O
gamma	O
to	O
specify	O
a	O
value	O
of	O
for	O
the	O
radial	B
basis	B
kernel	B
we	O
first	O
generate	O
some	O
data	B
with	O
a	O
non-linear	B
class	O
boundary	O
as	O
follows	O
set	B
seed	B
x	O
matrix	O
rnorm	O
ncol	O
x	O
x	O
x	O
x	O
y	O
c	O
rep	O
rep	O
dat	O
data	B
frame	I
x	O
y	O
as	O
factor	B
y	O
plotting	O
the	O
data	B
makes	O
it	O
clear	O
that	O
the	O
class	O
boundary	O
is	O
indeed	O
nonlinear	O
plot	B
col	O
y	O
the	O
data	B
is	O
randomly	O
split	O
into	O
training	O
and	O
testing	O
groups	O
we	O
then	O
fit	O
the	O
training	O
data	B
using	O
the	O
svm	O
function	B
with	O
a	O
radial	B
kernel	B
and	O
train	B
sample	O
svmfit	O
svm	O
y	O
data	B
dat	O
train	B
kernel	B
radial	B
gamma	O
cost	O
plot	B
svmfit	O
dat	O
train	B
the	O
plot	B
shows	O
that	O
the	O
resulting	O
svm	O
has	O
a	O
decidedly	O
non-linear	B
boundary	O
the	O
summary	O
function	B
can	O
be	O
used	O
to	O
obtain	O
some	O
information	O
about	O
the	O
svm	O
fit	O
summary	O
svmfit	O
call	O
svm	O
formula	O
y	O
data	B
dat	O
kernel	B
radial	B
gamma	O
cost	O
parameter	B
s	O
svm	O
type	O
c	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
support	B
vector	B
machines	O
svm	O
kernel	B
cost	O
gamma	O
radial	B
number	O
of	O
support	O
vectors	O
number	O
of	O
classes	O
levels	O
we	O
can	O
see	O
from	O
the	O
figure	O
that	O
there	O
are	O
a	O
fair	O
number	O
of	O
training	O
errors	O
in	O
this	O
svm	O
fit	O
if	O
we	O
increase	O
the	O
value	O
of	O
cost	O
we	O
can	O
reduce	O
the	O
number	O
of	O
training	O
errors	O
however	O
this	O
comes	O
at	O
the	O
price	O
of	O
a	O
more	O
irregular	O
decision	B
boundary	I
that	O
seems	O
to	O
be	O
at	O
risk	O
of	O
overfitting	B
the	O
data	B
svmfit	O
svm	O
y	O
data	B
dat	O
train	B
kernel	B
radial	B
gamma	O
cost	O
plot	B
svmfit	O
dat	O
train	B
we	O
can	O
perform	O
cross-validation	B
using	O
tune	O
to	O
select	O
the	O
best	O
choice	O
of	O
and	O
cost	O
for	O
an	O
svm	O
with	O
a	O
radial	B
kernel	B
set	B
seed	B
tune	O
out	O
tune	O
svm	O
y	O
data	B
dat	O
train	B
kernel	B
radial	B
ranges	O
list	O
cost	O
c	O
gamma	O
c	O
summary	O
tune	O
out	O
parameter	B
tuning	O
of	O
svm	O
sampling	O
method	O
fold	O
cross	O
validation	O
best	O
parameters	O
cost	O
gamma	O
best	O
performan	O
c	O
e	O
detailed	O
performa	O
nc	O
e	O
results	O
cost	O
gamma	O
error	B
dispersion	O
e	O
e	O
e	O
e	O
e	O
therefore	O
the	O
best	O
choice	O
of	O
parameters	O
involves	O
and	O
we	O
can	O
view	O
the	O
test	O
set	B
predictions	O
for	O
this	O
model	B
by	O
applying	O
the	O
predict	O
function	B
to	O
the	O
data	B
notice	O
that	O
to	O
do	O
this	O
we	O
subset	O
the	O
dataframe	O
dat	O
using	O
as	O
an	O
index	O
set	B
table	O
true	O
dat	O
train	B
y	O
pred	O
predict	O
tune	O
outbest	O
model	B
newdata	O
dat	O
train	B
of	O
test	O
observations	B
are	O
misclassified	O
by	O
this	O
svm	O
lab	O
support	B
vector	B
machines	O
roc	O
curves	O
the	O
rocr	O
package	O
can	O
be	O
used	O
to	O
produce	O
roc	O
curves	O
such	O
as	O
those	O
in	O
figures	O
and	O
we	O
first	O
write	O
a	O
short	O
function	B
to	O
plot	B
an	O
roc	B
curve	I
given	O
a	O
vector	B
containing	O
a	O
numerical	O
score	O
for	O
each	O
observation	O
pred	O
and	O
a	O
vector	B
containing	O
the	O
class	O
label	O
for	O
each	O
observation	O
truth	O
library	O
rocr	O
rocplot	O
function	B
pred	O
truth	O
predob	O
prediction	B
pred	O
truth	O
perf	O
performa	O
nc	O
e	O
predob	O
tpr	O
fpr	O
plot	B
perf	O
svms	O
and	O
support	B
vector	B
classifiers	O
output	B
class	O
labels	O
for	O
each	O
observation	O
however	O
it	O
is	O
also	O
possible	O
to	O
obtain	O
fitted	O
values	O
for	O
each	O
observation	O
which	O
are	O
the	O
numerical	O
scores	O
used	O
to	O
obtain	O
the	O
class	O
labels	O
for	O
instance	O
in	O
the	O
case	O
of	O
a	O
support	B
vector	B
classifier	B
the	O
fitted	O
value	O
for	O
an	O
observation	O
x	O
xpt	O
takes	O
the	O
form	O
pxp	O
for	O
an	O
svm	O
with	O
a	O
non-linear	B
kernel	B
the	O
equation	O
that	O
yields	O
the	O
fitted	O
value	O
is	O
given	O
in	O
in	O
essence	O
the	O
sign	O
of	O
the	O
fitted	O
value	O
determines	O
on	O
which	O
side	O
of	O
the	O
decision	B
boundary	I
the	O
observation	O
lies	O
therefore	O
the	O
relationship	O
between	O
the	O
fitted	O
value	O
and	O
the	O
class	O
prediction	B
for	O
a	O
given	O
observation	O
is	O
simple	B
if	O
the	O
fitted	O
value	O
exceeds	O
zero	O
then	O
the	O
observation	O
is	O
assigned	O
to	O
one	O
class	O
and	O
if	O
it	O
is	O
less	O
than	O
zero	O
then	O
it	O
is	O
assigned	O
to	O
the	O
other	O
in	O
order	O
to	O
obtain	O
the	O
fitted	O
values	O
for	O
a	O
given	O
svm	O
model	B
fit	O
we	O
use	O
decision	O
valuestrue	O
when	O
fitting	O
svm	O
then	O
the	O
predict	O
function	B
will	O
output	B
the	O
fitted	O
values	O
svmfit	O
opt	O
svm	O
y	O
data	B
dat	O
train	B
kernel	B
radial	B
gamma	O
cost	O
decision	O
values	O
t	O
fitted	O
attributes	O
predict	O
svmfit	O
opt	O
dat	O
train	B
decision	O
values	O
true	O
values	O
now	O
we	O
can	O
produce	O
the	O
roc	O
plot	B
par	O
mfrow	O
c	O
rocplot	O
fitted	O
dat	O
train	B
y	O
main	O
training	O
data	B
svm	O
appears	O
to	O
be	O
producing	O
accurate	O
predictions	O
by	O
increasing	O
we	O
can	O
produce	O
a	O
more	O
flexible	O
fit	O
and	O
generate	O
further	O
improvements	O
in	O
accuracy	O
svmfit	O
flex	O
svm	O
y	O
data	B
dat	O
train	B
kernel	B
radial	B
gamma	O
cost	O
decision	O
values	O
t	O
fitted	O
attributes	O
predict	O
svmfit	O
flex	O
dat	O
train	B
decision	O
values	O
t	O
values	O
rocplot	O
fitted	O
dat	O
train	B
y	O
add	O
col	O
red	O
however	O
these	O
roc	O
curves	O
are	O
all	O
on	O
the	O
training	O
data	B
we	O
are	O
really	O
more	O
interested	O
in	O
the	O
level	B
of	O
prediction	B
accuracy	O
on	O
the	O
test	O
data	B
when	O
we	O
compute	O
the	O
roc	O
curves	O
on	O
the	O
test	O
data	B
the	O
model	B
with	O
appears	O
to	O
provide	O
the	O
most	O
accurate	O
results	O
support	B
vector	B
machines	O
fitted	O
attributes	O
predict	O
svmfit	O
opt	O
dat	O
train	B
decision	O
values	O
t	O
values	O
rocplot	O
fitted	O
dat	O
train	B
y	O
main	O
test	O
data	B
fitted	O
attributes	O
predict	O
svmfit	O
flex	O
dat	O
train	B
decision	O
values	O
t	O
values	O
rocplot	O
fitted	O
dat	O
train	B
y	O
add	O
col	O
red	O
svm	O
with	O
multiple	B
classes	O
if	O
the	O
response	B
is	O
a	O
factor	B
containing	O
more	O
than	O
two	O
levels	O
then	O
the	O
svm	O
function	B
will	O
perform	O
multi-class	O
classification	B
using	O
the	O
one-versus-one	B
approach	B
we	O
explore	O
that	O
setting	O
here	O
by	O
generating	O
a	O
third	O
class	O
of	O
observations	B
set	B
seed	B
x	O
rbind	O
matrix	O
rnorm	O
ncol	O
y	O
c	O
y	O
rep	O
x	O
y	O
x	O
y	O
dat	O
data	B
frame	I
x	O
y	O
as	O
factor	B
y	O
par	O
mfrow	O
c	O
plot	B
col	O
y	O
we	O
now	O
fit	O
an	O
svm	O
to	O
the	O
data	B
svmfit	O
svm	O
y	O
data	B
dat	O
kernel	B
radial	B
cost	O
gamma	O
plot	B
svmfit	O
dat	O
the	O
library	O
can	O
also	O
be	O
used	O
to	O
perform	O
support	B
vector	B
regression	B
if	O
the	O
response	B
vector	B
that	O
is	O
passed	O
in	O
to	O
svm	O
is	O
numerical	O
rather	O
than	O
a	O
factor	B
application	O
to	O
gene	O
expression	O
data	B
we	O
now	O
examine	O
the	O
khan	B
data	B
set	B
which	O
consists	O
of	O
a	O
number	O
of	O
tissue	O
samples	O
corresponding	O
to	O
four	O
distinct	O
types	O
of	O
small	O
round	O
blue	O
cell	O
tumors	O
for	O
each	O
tissue	O
sample	O
gene	O
expression	O
measurements	O
are	O
available	O
the	O
data	B
set	B
consists	O
of	O
training	O
data	B
xtrain	O
and	O
ytrain	O
and	O
testing	O
data	B
xtest	O
and	O
ytest	O
we	O
examine	O
the	O
dimension	O
of	O
the	O
data	B
xtest	O
ytrain	O
ytest	O
xtrain	O
library	O
islr	O
names	O
khan	B
dim	O
khanxtra	O
in	O
dim	O
khanxtest	O
length	O
khanytra	O
in	O
length	O
khanytest	O
lab	O
support	B
vector	B
machines	O
this	O
data	B
set	B
consists	O
of	O
expression	O
measurements	O
for	O
genes	O
the	O
training	O
and	O
test	O
sets	O
consist	O
of	O
and	O
observations	B
respectively	O
table	O
khanytra	O
i	O
n	O
table	O
khanytes	O
t	O
we	O
will	O
use	O
a	O
support	B
vector	B
approach	B
to	O
predict	O
cancer	O
subtype	O
using	O
gene	O
expression	O
measurements	O
in	O
this	O
data	B
set	B
there	O
are	O
a	O
very	O
large	O
number	O
of	O
features	O
relative	O
to	O
the	O
number	O
of	O
observations	B
this	O
suggests	O
that	O
we	O
should	O
use	O
a	O
linear	B
kernel	B
because	O
the	O
additional	O
flexibility	O
that	O
will	O
result	O
from	O
using	O
a	O
polynomial	B
or	O
radial	B
kernel	B
is	O
unnecessary	O
dat	O
data	B
frame	I
x	O
khanxtrain	O
y	O
as	O
factor	B
khanytra	O
in	O
out	O
svm	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
summary	O
out	O
call	O
svm	O
formula	O
y	O
data	B
dat	O
kernel	B
linear	B
cost	O
parameter	B
s	O
svm	O
type	O
svm	O
kernel	B
cost	O
gamma	O
c	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
linear	B
number	O
of	O
support	O
vectors	O
number	O
of	O
classes	O
levels	O
table	O
outfitted	O
daty	O
we	O
see	O
that	O
there	O
are	O
no	O
training	O
errors	O
in	O
fact	O
this	O
is	O
not	O
surprising	O
because	O
the	O
large	O
number	O
of	O
variables	O
relative	O
to	O
the	O
number	O
of	O
observations	B
implies	O
that	O
it	O
is	O
easy	O
to	O
find	O
hyperplanes	O
that	O
fully	O
separate	O
the	O
classes	O
we	O
are	O
most	O
interested	O
not	O
in	O
the	O
support	B
vector	B
classifier	B
s	O
performance	O
on	O
the	O
training	O
observations	B
but	O
rather	O
its	O
performance	O
on	O
the	O
test	O
observations	B
dat	O
te	O
data	B
frame	I
x	O
khanxtest	O
y	O
as	O
factor	B
khanytes	O
t	O
pred	O
te	O
predict	O
out	O
newdata	O
dat	O
te	O
table	O
pred	O
te	O
dat	O
tey	O
pred	O
te	O
support	B
vector	B
machines	O
we	O
see	O
that	O
using	O
yields	O
two	O
test	O
set	B
errors	O
on	O
this	O
data	B
exercises	O
conceptual	O
this	O
problem	O
involves	O
hyperplanes	O
in	O
two	O
dimensions	O
sketch	O
the	O
hyperplane	B
indicate	O
the	O
set	B
of	O
points	O
for	O
which	O
as	O
well	O
as	O
the	O
set	B
of	O
points	O
for	O
which	O
on	O
the	O
same	O
plot	B
sketch	O
the	O
hyperplane	B
indicate	O
the	O
set	B
of	O
points	O
for	O
which	O
as	O
well	O
as	O
the	O
set	B
of	O
points	O
for	O
which	O
we	O
have	O
seen	O
that	O
in	O
p	O
dimensions	O
a	O
linear	B
decision	B
boundary	I
takes	O
the	O
form	O
we	O
now	O
investigate	O
a	O
non-linear	B
decision	B
boundary	I
sketch	O
the	O
curve	O
on	O
your	O
sketch	O
indicate	O
the	O
set	B
of	O
points	O
for	O
which	O
as	O
well	O
as	O
the	O
set	B
of	O
points	O
for	O
which	O
suppose	O
that	O
a	O
classifier	B
assigns	O
an	O
observation	O
to	O
the	O
blue	O
class	O
if	O
and	O
to	O
the	O
red	O
class	O
otherwise	O
to	O
what	O
class	O
is	O
the	O
observation	O
classified	O
argue	O
that	O
while	O
the	O
decision	B
boundary	I
in	O
is	O
not	O
linear	B
in	O
and	O
terms	O
of	O
and	O
it	O
is	O
linear	B
in	O
terms	O
of	O
x	O
x	O
here	O
we	O
explore	O
the	O
maximal	O
margin	B
classifier	B
on	O
a	O
toy	O
data	B
set	B
we	O
are	O
given	O
n	O
observations	B
in	O
p	O
dimensions	O
for	O
each	O
observation	O
there	O
is	O
an	O
associated	O
class	O
label	O
obs	O
exercises	O
y	O
red	O
red	O
red	O
red	O
blue	O
blue	O
blue	O
sketch	O
the	O
observations	B
sketch	O
the	O
optimal	O
separating	B
hyperplane	B
and	O
provide	O
the	O
equa	O
tion	O
for	O
this	O
hyperplane	B
the	O
form	O
describe	O
the	O
classification	B
rule	O
for	O
the	O
maximal	O
margin	B
classifier	B
it	O
should	O
be	O
something	O
along	O
the	O
lines	O
of	O
classify	O
to	O
red	O
if	O
and	O
classify	O
to	O
blue	O
otherwise	O
provide	O
the	O
values	O
for	O
and	O
on	O
your	O
sketch	O
indicate	O
the	O
margin	B
for	O
the	O
maximal	O
margin	B
hyperplane	B
indicate	O
the	O
support	O
vectors	O
for	O
the	O
maximal	O
margin	B
classifier	B
argue	O
that	O
a	O
slight	O
movement	O
of	O
the	O
seventh	O
observation	O
would	O
not	O
affect	O
the	O
maximal	O
margin	B
hyperplane	B
sketch	O
a	O
hyperplane	B
that	O
is	O
not	O
the	O
optimal	O
separating	O
hyper	O
plane	O
and	O
provide	O
the	O
equation	O
for	O
this	O
hyperplane	B
draw	O
an	O
additional	O
observation	O
on	O
the	O
plot	B
so	O
that	O
the	O
two	O
classes	O
are	O
no	O
longer	O
separable	O
by	O
a	O
hyperplane	B
applied	O
generate	O
a	O
simulated	O
two-class	O
data	B
set	B
with	O
observations	B
and	O
two	O
features	O
in	O
which	O
there	O
is	O
a	O
visible	O
but	O
non-linear	B
separation	O
between	O
the	O
two	O
classes	O
show	O
that	O
in	O
this	O
setting	O
a	O
support	B
vector	B
machine	B
with	O
a	O
polynomial	B
kernel	B
degree	O
greater	O
than	O
or	O
a	O
radial	B
kernel	B
will	O
outperform	O
a	O
support	B
vector	B
classifier	B
on	O
the	O
training	O
data	B
which	O
technique	O
performs	O
best	O
on	O
the	O
test	O
data	B
make	O
plots	O
and	O
report	O
training	O
and	O
test	O
error	B
rates	O
in	O
order	O
to	O
back	O
up	O
your	O
assertions	O
we	O
have	O
seen	O
that	O
we	O
can	O
fit	O
an	O
svm	O
with	O
a	O
non-linear	B
kernel	B
in	O
order	O
to	O
perform	O
classification	B
using	O
a	O
non-linear	B
decision	B
boundary	I
we	O
will	O
now	O
see	O
that	O
we	O
can	O
also	O
obtain	O
a	O
non-linear	B
decision	B
boundary	I
by	O
performing	O
logistic	B
regression	B
using	O
non-linear	B
transformations	O
of	O
the	O
features	O
support	B
vector	B
machines	O
generate	O
a	O
data	B
set	B
with	O
n	O
and	O
p	O
such	O
that	O
the	O
observations	B
belong	O
to	O
two	O
classes	O
with	O
a	O
quadratic	B
decision	B
boundary	I
between	O
them	O
for	O
instance	O
you	O
can	O
do	O
this	O
as	O
follows	O
runif	O
runif	O
y	O
plot	B
the	O
observations	B
colored	O
according	O
to	O
their	O
class	O
labels	O
your	O
plot	B
should	O
display	O
on	O
the	O
x-axis	O
and	O
on	O
the	O
yaxis	O
fit	O
a	O
logistic	B
regression	B
model	B
to	O
the	O
data	B
using	O
and	O
as	O
predictors	O
apply	O
this	O
model	B
to	O
the	O
training	O
data	B
in	O
order	O
to	O
obtain	O
a	O
predicted	O
class	O
label	O
for	O
each	O
training	O
observation	O
plot	B
the	O
observations	B
colored	O
according	O
to	O
the	O
predicted	O
class	O
labels	O
the	O
decision	B
boundary	I
should	O
be	O
linear	B
now	O
fit	O
a	O
logistic	B
regression	B
model	B
to	O
the	O
data	B
using	O
non-linear	B
functions	O
of	O
and	O
as	O
predictors	O
x	O
and	O
so	O
forth	O
apply	O
this	O
model	B
to	O
the	O
training	O
data	B
in	O
order	O
to	O
obtain	O
a	O
predicted	O
class	O
label	O
for	O
each	O
training	O
observation	O
plot	B
the	O
observations	B
colored	O
according	O
to	O
the	O
predicted	O
class	O
labels	O
the	O
decision	B
boundary	I
should	O
be	O
obviously	O
non-linear	B
if	O
it	O
is	O
not	O
then	O
repeat	O
until	O
you	O
come	O
up	O
with	O
an	O
example	O
in	O
which	O
the	O
predicted	O
class	O
labels	O
are	O
obviously	O
non-linear	B
fit	O
a	O
support	B
vector	B
classifier	B
to	O
the	O
data	B
with	O
and	O
as	O
predictors	O
obtain	O
a	O
class	O
prediction	B
for	O
each	O
training	O
observation	O
plot	B
the	O
observations	B
colored	O
according	O
to	O
the	O
predicted	O
class	O
labels	O
fit	O
a	O
svm	O
using	O
a	O
non-linear	B
kernel	B
to	O
the	O
data	B
obtain	O
a	O
class	O
prediction	B
for	O
each	O
training	O
observation	O
plot	B
the	O
observations	B
colored	O
according	O
to	O
the	O
predicted	O
class	O
labels	O
comment	O
on	O
your	O
results	O
at	O
the	O
end	O
of	O
section	O
it	O
is	O
claimed	O
that	O
in	O
the	O
case	O
of	O
data	B
that	O
is	O
just	O
barely	O
linearly	O
separable	O
a	O
support	B
vector	B
classifier	B
with	O
a	O
small	O
value	O
of	O
cost	O
that	O
misclassifies	O
a	O
couple	O
of	O
training	O
observations	B
may	O
perform	O
better	O
on	O
test	O
data	B
than	O
one	O
with	O
a	O
huge	O
value	O
of	O
cost	O
that	O
does	O
not	O
misclassify	O
any	O
training	O
observations	B
you	O
will	O
now	O
investigate	O
this	O
claim	O
generate	O
two-class	O
data	B
with	O
p	O
in	O
such	O
a	O
way	O
that	O
the	O
classes	O
are	O
just	O
barely	O
linearly	O
separable	O
exercises	O
compute	O
the	O
cross-validation	B
error	B
rates	O
for	O
support	B
vector	B
classifiers	O
with	O
a	O
range	O
of	O
cost	O
values	O
how	O
many	O
training	O
errors	O
are	O
misclassified	O
for	O
each	O
value	O
of	O
cost	O
considered	O
and	O
how	O
does	O
this	O
relate	O
to	O
the	O
cross-validation	B
errors	O
obtained	O
generate	O
an	O
appropriate	O
test	O
data	B
set	B
and	O
compute	O
the	O
test	O
errors	O
corresponding	O
to	O
each	O
of	O
the	O
values	O
of	O
cost	O
considered	O
which	O
value	O
of	O
cost	O
leads	O
to	O
the	O
fewest	O
test	O
errors	O
and	O
how	O
does	O
this	O
compare	O
to	O
the	O
values	O
of	O
cost	O
that	O
yield	O
the	O
fewest	O
training	O
errors	O
and	O
the	O
fewest	O
cross-validation	B
errors	O
discuss	O
your	O
results	O
in	O
this	O
problem	O
you	O
will	O
use	O
support	B
vector	B
approaches	O
in	O
order	O
to	O
predict	O
whether	O
a	O
given	O
car	O
gets	O
high	O
or	O
low	O
gas	O
mileage	O
based	O
on	O
the	O
auto	B
data	B
set	B
create	O
a	O
binary	B
variable	B
that	O
takes	O
on	O
a	O
for	O
cars	O
with	O
gas	O
mileage	O
above	O
the	O
median	O
and	O
a	O
for	O
cars	O
with	O
gas	O
mileage	O
below	O
the	O
median	O
fit	O
a	O
support	B
vector	B
classifier	B
to	O
the	O
data	B
with	O
various	O
values	O
of	O
cost	O
in	O
order	O
to	O
predict	O
whether	O
a	O
car	O
gets	O
high	O
or	O
low	O
gas	O
mileage	O
report	O
the	O
cross-validation	B
errors	O
associated	O
with	O
different	O
values	O
of	O
this	O
parameter	B
comment	O
on	O
your	O
results	O
now	O
repeat	O
this	O
time	O
using	O
svms	O
with	O
radial	B
and	O
polynomial	B
basis	B
kernels	O
with	O
different	O
values	O
of	O
gamma	O
and	O
degree	O
and	O
cost	O
comment	O
on	O
your	O
results	O
make	O
some	O
plots	O
to	O
back	O
up	O
your	O
assertions	O
in	O
and	O
hint	O
in	O
the	O
lab	O
we	O
used	O
the	O
plot	B
function	B
for	O
svm	O
objects	O
only	O
in	O
cases	O
with	O
p	O
when	O
p	O
you	O
can	O
use	O
the	O
plot	B
function	B
to	O
create	O
plots	O
displaying	O
pairs	O
of	O
variables	O
at	O
a	O
time	O
essentially	O
instead	O
of	O
typing	O
plot	B
svmfit	O
dat	O
where	O
svmfit	O
contains	O
your	O
fitted	O
model	B
and	O
dat	O
is	O
a	O
data	B
frame	I
containing	O
your	O
data	B
you	O
can	O
type	O
plot	B
svmfit	O
dat	O
in	O
order	O
to	O
plot	B
just	O
the	O
first	O
and	O
fourth	O
variables	O
however	O
you	O
must	O
replace	O
and	O
with	O
the	O
correct	O
variable	B
names	O
to	O
find	O
out	O
more	O
type	O
this	O
problem	O
involves	O
the	O
oj	B
data	B
set	B
which	O
is	O
part	O
of	O
the	O
islr	O
package	O
support	B
vector	B
machines	O
create	O
a	O
training	O
set	B
containing	O
a	O
random	O
sample	O
of	O
remaining	O
containing	O
the	O
observations	B
and	O
a	O
test	O
observations	B
set	B
fit	O
a	O
support	B
vector	B
classifier	B
to	O
the	O
training	O
data	B
using	O
with	O
purchase	O
as	O
the	O
response	B
and	O
the	O
other	O
variables	O
as	O
predictors	O
use	O
the	O
summary	O
function	B
to	O
produce	O
summary	O
statistics	O
and	O
describe	O
the	O
results	O
obtained	O
what	O
are	O
the	O
training	O
and	O
test	O
error	B
rates	O
use	O
the	O
tune	O
function	B
to	O
select	O
an	O
optimal	O
cost	O
consider	O
val	O
ues	O
in	O
the	O
range	O
to	O
compute	O
the	O
training	O
and	O
test	O
error	B
rates	O
using	O
this	O
new	O
value	O
for	O
cost	O
repeat	O
parts	O
through	O
using	O
a	O
support	B
vector	B
machine	B
with	O
a	O
radial	B
kernel	B
use	O
the	O
default	B
value	O
for	O
gamma	O
repeat	O
parts	O
through	O
using	O
a	O
support	B
vector	B
machine	B
with	O
a	O
polynomial	B
kernel	B
set	B
overall	O
which	O
approach	B
seems	O
to	O
give	O
the	O
best	O
results	O
on	O
this	O
data	B
unsupervised	B
learning	I
this	O
book	O
concerns	O
such	O
as	O
most	O
of	O
regression	B
and	O
classification	B
in	O
the	O
supervised	B
learning	I
setting	O
we	O
typically	O
have	O
access	O
to	O
a	O
set	B
of	O
p	O
features	O
xp	O
measured	O
on	O
n	O
observations	B
and	O
a	O
response	B
y	O
also	O
measured	O
on	O
those	O
same	O
n	O
observations	B
the	O
goal	O
is	O
then	O
to	O
predict	O
y	O
using	O
xp	O
supervised	B
learning	I
methods	O
this	O
chapter	O
will	O
instead	O
focus	O
on	O
unsupervised	B
learning	I
a	O
set	B
of	O
statistical	O
tools	O
intended	O
for	O
the	O
setting	O
in	O
which	O
we	O
have	O
only	O
a	O
set	B
of	O
features	O
xp	O
measured	O
on	O
n	O
observations	B
we	O
are	O
not	O
interested	O
in	O
prediction	B
because	O
we	O
do	O
not	O
have	O
an	O
associated	O
response	B
variable	B
y	O
rather	O
the	O
goal	O
is	O
to	O
discover	O
interesting	O
things	O
about	O
the	O
measurements	O
on	O
xp	O
is	O
there	O
an	O
informative	O
way	O
to	O
visualize	O
the	O
data	B
can	O
we	O
discover	O
subgroups	O
among	O
the	O
variables	O
or	O
among	O
the	O
observations	B
unsupervised	B
learning	I
refers	O
to	O
a	O
diverse	O
set	B
of	O
techniques	O
for	O
answering	O
questions	O
such	O
as	O
these	O
in	O
this	O
chapter	O
we	O
will	O
focus	O
on	O
two	O
particular	O
types	O
of	O
unsupervised	B
learning	I
principal	B
components	I
analysis	B
a	O
tool	O
used	O
for	O
data	B
visualization	O
or	O
data	B
pre-processing	O
before	O
supervised	O
techniques	O
are	O
applied	O
and	O
clustering	B
a	O
broad	O
class	O
of	O
methods	O
for	O
discovering	O
unknown	O
subgroups	O
in	O
data	B
the	O
challenge	O
of	O
unsupervised	B
learning	I
supervised	B
learning	I
is	O
a	O
well-understood	O
area	O
in	O
fact	O
if	O
you	O
have	O
read	O
the	O
preceding	O
chapters	O
in	O
this	O
book	O
then	O
you	O
should	O
by	O
now	O
have	O
a	O
good	O
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
exploratory	B
data	B
analysis	B
unsupervised	B
learning	I
grasp	O
of	O
supervised	B
learning	I
for	O
instance	O
if	O
you	O
are	O
asked	O
to	O
predict	O
a	O
binary	B
outcome	O
from	O
a	O
data	B
set	B
you	O
have	O
a	O
very	O
well	O
developed	O
set	B
of	O
tools	O
at	O
your	O
disposal	O
as	O
logistic	B
regression	B
linear	B
discriminant	I
analysis	B
classification	B
trees	O
support	B
vector	B
machines	O
and	O
more	O
as	O
well	O
as	O
a	O
clear	O
understanding	O
of	O
how	O
to	O
assess	O
the	O
quality	O
of	O
the	O
results	O
obtained	O
cross-validation	B
validation	O
on	O
an	O
independent	B
test	O
set	B
and	O
so	O
forth	O
in	O
contrast	B
unsupervised	B
learning	I
is	O
often	O
much	O
more	O
challenging	O
the	O
exercise	O
tends	O
to	O
be	O
more	O
subjective	O
and	O
there	O
is	O
no	O
simple	B
goal	O
for	O
the	O
analysis	B
such	O
as	O
prediction	B
of	O
a	O
response	B
unsupervised	B
learning	I
is	O
often	O
performed	O
as	O
part	O
of	O
an	O
exploratory	B
data	B
analysis	B
furthermore	O
it	O
can	O
be	O
hard	O
to	O
assess	O
the	O
results	O
obtained	O
from	O
unsupervised	B
learning	I
methods	O
since	O
there	O
is	O
no	O
universally	O
accepted	O
mechanism	O
for	O
performing	O
crossvalidation	O
or	O
validating	O
results	O
on	O
an	O
independent	B
data	B
set	B
the	O
reason	O
for	O
this	O
difference	O
is	O
simple	B
if	O
we	O
fit	O
a	O
predictive	O
model	B
using	O
a	O
supervised	B
learning	I
technique	O
then	O
it	O
is	O
possible	O
to	O
check	O
our	O
work	O
by	O
seeing	O
how	O
well	O
our	O
model	B
predicts	O
the	O
response	B
y	O
on	O
observations	B
not	O
used	O
in	O
fitting	O
the	O
model	B
however	O
in	O
unsupervised	B
learning	I
there	O
is	O
no	O
way	O
to	O
check	O
our	O
work	O
because	O
we	O
don	O
t	O
know	O
the	O
true	O
answer	O
the	O
problem	O
is	O
unsupervised	O
techniques	O
for	O
unsupervised	B
learning	I
are	O
of	O
growing	O
importance	B
in	O
a	O
number	O
of	O
fields	O
a	O
cancer	O
researcher	O
might	O
assay	O
gene	O
expression	O
levels	O
in	O
patients	O
with	O
breast	O
cancer	O
he	O
or	O
she	O
might	O
then	O
look	O
for	O
subgroups	O
among	O
the	O
breast	O
cancer	O
samples	O
or	O
among	O
the	O
genes	O
in	O
order	O
to	O
obtain	O
a	O
better	O
understanding	O
of	O
the	O
disease	O
an	O
online	O
shopping	O
site	O
might	O
try	O
to	O
identify	O
groups	O
of	O
shoppers	O
with	O
similar	O
browsing	O
and	O
purchase	O
histories	O
as	O
well	O
as	O
items	O
that	O
are	O
of	O
particular	O
interest	O
to	O
the	O
shoppers	O
within	O
each	O
group	O
then	O
an	O
individual	O
shopper	O
can	O
be	O
preferentially	O
shown	O
the	O
items	O
in	O
which	O
he	O
or	O
she	O
is	O
particularly	O
likely	O
to	O
be	O
interested	O
based	O
on	O
the	O
purchase	O
histories	O
of	O
similar	O
shoppers	O
a	O
search	O
engine	O
might	O
choose	O
what	O
search	O
results	O
to	O
display	O
to	O
a	O
particular	O
individual	O
based	O
on	O
the	O
click	O
histories	O
of	O
other	O
individuals	O
with	O
similar	O
search	O
patterns	O
these	O
statistical	O
learning	O
tasks	O
and	O
many	O
more	O
can	O
be	O
performed	O
via	O
unsupervised	B
learning	I
techniques	O
principal	B
components	I
analysis	B
principal	B
components	I
are	O
discussed	O
in	O
section	O
in	O
the	O
context	O
of	O
principal	B
components	I
regression	B
when	O
faced	O
with	O
a	O
large	O
set	B
of	O
correlated	O
variables	O
principal	B
components	I
allow	O
us	O
to	O
summarize	O
this	O
set	B
with	O
a	O
smaller	O
number	O
of	O
representative	O
variables	O
that	O
collectively	O
explain	O
most	O
of	O
the	O
variability	O
in	O
the	O
original	O
set	B
the	O
principal	O
component	O
directions	O
are	O
presented	O
in	O
section	O
as	O
directions	O
in	O
feature	B
space	O
along	O
which	O
the	O
original	O
data	B
are	O
highly	O
variable	B
these	O
directions	O
also	O
define	O
lines	O
and	O
subspaces	O
that	O
are	O
as	O
close	O
as	O
possible	O
to	O
the	O
data	B
cloud	O
to	O
perform	O
principal	B
components	I
analysis	B
principal	B
components	I
regression	B
we	O
simply	O
use	O
principal	B
components	I
as	O
predictors	O
in	O
a	O
regression	B
model	B
in	O
place	O
of	O
the	O
original	O
larger	O
set	B
of	O
variables	O
principal	O
component	O
analysis	B
refers	O
to	O
the	O
process	O
by	O
which	O
principal	B
components	I
are	O
computed	O
and	O
the	O
subsequent	O
use	O
of	O
these	O
components	O
in	O
understanding	O
the	O
data	B
pca	O
is	O
an	O
unsupervised	O
approach	B
since	O
it	O
involves	O
only	O
a	O
set	B
of	O
features	O
xp	O
and	O
no	O
associated	O
response	B
y	O
apart	O
from	O
producing	O
derived	O
variables	O
for	O
use	O
in	O
supervised	B
learning	I
problems	O
pca	O
also	O
serves	O
as	O
a	O
tool	O
for	O
data	B
visualization	O
of	O
the	O
observations	B
or	O
visualization	O
of	O
the	O
variables	O
we	O
now	O
discuss	O
pca	O
in	O
greater	O
detail	O
focusing	O
on	O
the	O
use	O
of	O
pca	O
as	O
a	O
tool	O
for	O
unsupervised	O
data	B
exploration	O
in	O
keeping	O
with	O
the	O
topic	O
of	O
this	O
chapter	O
principal	O
component	O
analysis	B
what	O
are	O
principal	B
components	I
p	O
suppose	O
that	O
we	O
wish	O
to	O
visualize	O
n	O
observations	B
with	O
measurements	O
on	O
a	O
set	B
of	O
p	O
features	O
xp	O
as	O
part	O
of	O
an	O
exploratory	B
data	B
analysis	B
we	O
could	O
do	O
this	O
by	O
examining	O
two-dimensional	O
scatterplots	O
of	O
the	O
data	B
each	O
of	O
which	O
contains	O
the	O
n	O
observations	B
measurements	O
on	O
two	O
of	O
the	O
pp	O
such	O
scatterplots	O
for	O
example	O
features	O
however	O
there	O
are	O
with	O
p	O
there	O
are	O
plots	O
if	O
p	O
is	O
large	O
then	O
it	O
will	O
certainly	O
not	O
be	O
possible	O
to	O
look	O
at	O
all	O
of	O
them	O
moreover	O
most	O
likely	O
none	O
of	O
them	O
will	O
be	O
informative	O
since	O
they	O
each	O
contain	O
just	O
a	O
small	O
fraction	O
of	O
the	O
total	O
information	O
present	O
in	O
the	O
data	B
set	B
clearly	O
a	O
better	O
method	O
is	O
required	O
to	O
visualize	O
the	O
n	O
observations	B
when	O
p	O
is	O
large	O
in	O
particular	O
we	O
would	O
like	O
to	O
find	O
a	O
low-dimensional	B
representation	O
of	O
the	O
data	B
that	O
captures	O
as	O
much	O
of	O
the	O
information	O
as	O
possible	O
for	O
instance	O
if	O
we	O
can	O
obtain	O
a	O
two-dimensional	O
representation	O
of	O
the	O
data	B
that	O
captures	O
most	O
of	O
the	O
information	O
then	O
we	O
can	O
plot	B
the	O
observations	B
in	O
this	O
low-dimensional	B
space	O
pca	O
provides	O
a	O
tool	O
to	O
do	O
just	O
this	O
it	O
finds	O
a	O
low-dimensional	B
representation	O
of	O
a	O
data	B
set	B
that	O
contains	O
as	O
much	O
as	O
possible	O
of	O
the	O
variation	O
the	O
idea	O
is	O
that	O
each	O
of	O
the	O
n	O
observations	B
lives	O
in	O
p-dimensional	O
space	O
but	O
not	O
all	O
of	O
these	O
dimensions	O
are	O
equally	O
interesting	O
pca	O
seeks	O
a	O
small	O
number	O
of	O
dimensions	O
that	O
are	O
as	O
interesting	O
as	O
possible	O
where	O
the	O
concept	O
of	O
interesting	O
is	O
measured	O
by	O
the	O
amount	O
that	O
the	O
observations	B
vary	O
along	O
each	O
dimension	O
each	O
of	O
the	O
dimensions	O
found	O
by	O
pca	O
is	O
a	O
linear	B
combination	I
of	O
the	O
p	O
features	O
we	O
now	O
explain	O
the	O
manner	O
in	O
which	O
these	O
dimensions	O
or	O
principal	B
components	I
are	O
found	O
the	O
first	O
principal	O
component	O
of	O
a	O
set	B
of	O
features	O
xp	O
is	O
the	O
normalized	O
linear	B
combination	I
of	O
the	O
features	O
that	O
has	O
the	O
largest	O
variance	B
by	O
normalized	O
we	O
mean	O
that	O
we	O
refer	O
to	O
the	O
elements	O
as	O
the	O
loadings	O
of	O
the	O
first	O
principal	O
p	O
loading	O
unsupervised	B
learning	I
component	O
together	O
the	O
loadings	O
make	O
up	O
the	O
principal	O
component	O
loading	B
vector	B
we	O
constrain	O
the	O
loadings	O
so	O
that	O
their	O
sum	B
of	I
squares	I
is	O
equal	O
to	O
one	O
since	O
otherwise	O
setting	O
these	O
elements	O
to	O
be	O
arbitrarily	O
large	O
in	O
absolute	O
value	O
could	O
result	O
in	O
an	O
arbitrarily	O
large	O
variance	B
given	O
a	O
n	O
p	O
data	B
set	B
x	O
how	O
do	O
we	O
compute	O
the	O
first	O
principal	O
component	O
since	O
we	O
are	O
only	O
interested	O
in	O
variance	B
we	O
assume	O
that	O
each	O
of	O
the	O
variables	O
in	O
x	O
has	O
been	O
centered	O
to	O
have	O
mean	O
zero	O
is	O
the	O
column	O
means	O
of	O
x	O
are	O
zero	O
we	O
then	O
look	O
for	O
the	O
linear	B
combination	I
of	O
the	O
sample	O
feature	B
values	O
of	O
the	O
form	O
that	O
has	O
largest	O
sample	O
variance	B
subject	O
to	O
the	O
constraint	O
that	O
in	O
other	O
words	O
the	O
first	O
principal	O
component	O
loading	B
vector	B
solves	O
the	O
optimization	O
problem	O
p	O
n	O
subject	O
to	O
maximize	O
from	O
we	O
can	O
write	O
the	O
objective	O
in	O
as	O
since	O
n	O
n	O
xij	O
the	O
average	B
of	O
the	O
will	O
be	O
zero	O
as	O
well	O
hence	O
n	O
the	O
objective	O
that	O
we	O
are	O
maximizing	O
in	O
is	O
just	O
the	O
sample	O
variance	B
of	O
the	O
n	O
values	O
of	O
we	O
refer	O
to	O
as	O
the	O
scores	O
of	O
the	O
first	O
princi-	O
score	O
pal	O
component	O
problem	O
can	O
be	O
solved	O
via	O
an	O
eigen	O
decomposition	B
a	O
standard	O
technique	O
in	O
linear	B
algebra	O
but	O
details	O
are	O
outside	O
of	O
the	O
scope	O
of	O
this	O
book	O
n	O
there	O
is	O
a	O
nice	O
geometric	O
interpretation	O
for	O
the	O
first	O
principal	O
component	O
the	O
loading	B
vector	B
with	O
elements	O
defines	O
a	O
direction	O
in	O
feature	B
space	O
along	O
which	O
the	O
data	B
vary	O
the	O
most	O
if	O
we	O
project	O
the	O
n	O
data	B
points	O
xn	O
onto	O
this	O
direction	O
the	O
projected	O
values	O
are	O
the	O
principal	O
component	O
scores	O
themselves	O
for	O
instance	O
figure	O
on	O
page	O
displays	O
the	O
first	O
principal	O
component	O
loading	B
vector	B
solid	O
line	B
on	O
an	O
advertising	B
data	B
set	B
in	O
these	O
data	B
there	O
are	O
only	O
two	O
features	O
and	O
so	O
the	O
observations	B
as	O
well	O
as	O
the	O
first	O
principal	O
component	O
loading	B
vector	B
can	O
be	O
easily	O
displayed	O
as	O
can	O
be	O
seen	O
from	O
in	O
that	O
data	B
set	B
and	O
after	O
the	O
first	O
principal	O
component	O
of	O
the	O
features	O
has	O
been	O
determined	O
we	O
can	O
find	O
the	O
second	O
principal	O
component	O
the	O
second	O
principal	O
component	O
is	O
the	O
linear	B
combination	I
of	O
xp	O
that	O
has	O
maximal	O
variance	B
out	O
of	O
all	O
linear	B
combinations	O
that	O
are	O
uncorrelated	O
with	O
the	O
second	O
principal	O
component	O
scores	O
take	O
the	O
form	O
principal	B
components	I
analysis	B
murder	O
assault	O
urbanpop	O
rape	O
table	O
the	O
principal	O
component	O
loading	O
vectors	O
and	O
for	O
the	O
usarrests	B
data	B
these	O
are	O
also	O
displayed	O
in	O
figure	O
where	O
is	O
the	O
second	O
principal	O
component	O
loading	B
vector	B
with	O
elements	O
it	O
turns	O
out	O
that	O
constraining	O
to	O
be	O
uncorrelated	O
with	O
is	O
equivalent	O
to	O
constraining	O
the	O
direction	O
to	O
be	O
orthogonal	B
to	O
the	O
direction	O
in	O
the	O
example	O
in	O
figure	O
the	O
observations	B
lie	O
in	O
two-dimensional	O
space	O
p	O
and	O
so	O
once	O
we	O
have	O
found	O
there	O
is	O
only	O
one	O
possibility	O
for	O
which	O
is	O
shown	O
as	O
a	O
blue	O
dashed	O
line	B
section	O
we	O
know	O
that	O
and	O
but	O
in	O
a	O
larger	O
data	B
set	B
with	O
p	O
variables	O
there	O
are	O
multiple	B
distinct	O
principal	B
components	I
and	O
they	O
are	O
defined	O
in	O
a	O
similar	O
manner	O
to	O
find	O
we	O
solve	O
a	O
problem	O
similar	O
to	O
with	O
replacing	O
and	O
with	O
the	O
additional	O
constraint	O
that	O
is	O
orthogonal	B
to	O
once	O
we	O
have	O
computed	O
the	O
principal	B
components	I
we	O
can	O
plot	B
them	O
against	O
each	O
other	O
in	O
order	O
to	O
produce	O
low-dimensional	B
views	O
of	O
the	O
data	B
for	O
instance	O
we	O
can	O
plot	B
the	O
score	B
vector	B
against	O
against	O
against	O
and	O
so	O
forth	O
geometrically	O
this	O
amounts	O
to	O
projecting	O
the	O
original	O
data	B
down	O
onto	O
the	O
subspace	O
spanned	O
by	O
and	O
and	O
plotting	O
the	O
projected	O
points	O
we	O
illustrate	O
the	O
use	O
of	O
pca	O
on	O
the	O
usarrests	B
data	B
set	B
for	O
each	O
of	O
the	O
states	O
in	O
the	O
united	O
states	O
the	O
data	B
set	B
contains	O
the	O
number	O
of	O
arrests	O
per	O
residents	O
for	O
each	O
of	O
three	O
crimes	O
assault	O
murder	O
and	O
rape	O
we	O
also	O
record	O
urbanpop	O
percent	O
of	O
the	O
population	O
in	O
each	O
state	O
living	O
in	O
urban	O
areas	O
the	O
principal	O
component	O
score	O
vectors	O
have	O
length	O
n	O
and	O
the	O
principal	O
component	O
loading	O
vectors	O
have	O
length	O
p	O
pca	O
was	O
performed	O
after	O
standardizing	O
each	O
variable	B
to	O
have	O
mean	O
zero	O
and	O
standard	O
deviation	O
one	O
figure	O
plots	O
the	O
first	O
two	O
principal	B
components	I
of	O
these	O
data	B
the	O
figure	O
represents	O
both	O
the	O
principal	O
component	O
scores	O
and	O
the	O
loading	O
vectors	O
in	O
a	O
single	B
biplot	B
display	O
the	O
loadings	O
are	O
also	O
given	O
in	O
table	O
in	O
figure	O
we	O
see	O
that	O
the	O
first	O
loading	B
vector	B
places	O
approximately	O
equal	O
weight	O
on	O
assault	O
murder	O
and	O
rape	O
with	O
much	O
less	O
weight	O
on	O
biplot	B
a	O
technical	O
note	O
the	O
principal	O
component	O
directions	O
are	O
the	O
ordered	O
sequence	O
of	O
eigenvectors	O
of	O
the	O
matrix	O
xt	O
x	O
and	O
the	O
variances	O
of	O
the	O
components	O
are	O
the	O
eigenvalues	O
there	O
are	O
at	O
most	O
minn	O
p	O
principal	B
components	I
unsupervised	B
learning	I
urbanpop	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
l	O
i	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
o	O
c	O
e	O
s	O
hawaii	O
rhode	O
island	O
utah	O
massachusetts	O
new	O
jersey	O
connecticut	O
washington	O
wisconsin	O
minnesota	O
pennsylvania	O
ohio	O
oregon	O
delaware	O
nebraska	O
kansas	O
oklahoma	O
indiana	O
missouri	O
new	O
hampshire	O
iowa	O
maine	O
rth	O
dakota	O
idaho	O
virginia	O
wyoming	O
montana	O
south	O
dakota	O
kentucky	O
vermont	O
west	O
virginia	O
arkansas	O
alabama	O
alaska	O
georgia	O
murder	O
tennessee	O
louisiana	O
colorado	O
illinois	O
new	O
york	O
arizona	O
rape	O
texas	O
michigan	O
new	O
mexico	O
maryland	O
assault	O
california	O
nevada	O
florida	O
south	O
carolina	O
north	O
carolina	O
mississippi	O
first	O
principal	O
component	O
figure	O
the	O
first	O
two	O
principal	B
components	I
for	O
the	O
usarrests	B
data	B
the	O
blue	O
state	O
names	O
represent	O
the	O
scores	O
for	O
the	O
first	O
two	O
principal	B
components	I
the	O
orange	O
arrows	O
indicate	O
the	O
first	O
two	O
principal	O
component	O
loading	O
vectors	O
axes	O
on	O
the	O
top	O
and	O
right	O
for	O
example	O
the	O
loading	O
for	O
rape	O
on	O
the	O
first	O
component	O
is	O
and	O
its	O
loading	O
on	O
the	O
second	O
principal	O
component	O
word	O
rape	O
is	O
centered	O
at	O
the	O
point	O
this	O
figure	O
is	O
known	O
as	O
a	O
biplot	B
because	O
it	O
displays	O
both	O
the	O
principal	O
component	O
scores	O
and	O
the	O
principal	O
component	O
loadings	O
urbanpop	O
hence	O
this	O
component	O
roughly	O
corresponds	O
to	O
a	O
measure	O
of	O
overall	O
rates	O
of	O
serious	O
crimes	O
the	O
second	O
loading	B
vector	B
places	O
most	O
of	O
its	O
weight	O
on	O
urbanpop	O
and	O
much	O
less	O
weight	O
on	O
the	O
other	O
three	O
features	O
hence	O
this	O
component	O
roughly	O
corresponds	O
to	O
the	O
level	B
of	O
urbanization	O
of	O
the	O
state	O
overall	O
we	O
see	O
that	O
the	O
crime-related	O
variables	O
assault	O
and	O
rape	O
are	O
located	O
close	O
to	O
each	O
other	O
and	O
that	O
the	O
urbanpop	O
variable	B
is	O
far	O
from	O
the	O
other	O
three	O
this	O
indicates	O
that	O
the	O
crime-related	O
variables	O
are	O
correlated	O
with	O
each	O
other	O
states	O
with	O
high	O
murder	O
rates	O
tend	O
to	O
have	O
high	O
assault	O
and	O
rape	O
rates	O
and	O
that	O
the	O
urbanpop	O
variable	B
is	O
less	O
correlated	O
with	O
the	O
other	O
three	O
principal	B
components	I
analysis	B
we	O
can	O
examine	O
differences	O
between	O
the	O
states	O
via	O
the	O
two	O
principal	O
component	O
score	O
vectors	O
shown	O
in	O
figure	O
our	O
discussion	O
of	O
the	O
loading	O
vectors	O
suggests	O
that	O
states	O
with	O
large	O
positive	O
scores	O
on	O
the	O
first	O
component	O
such	O
as	O
california	O
nevada	O
and	O
florida	O
have	O
high	O
crime	O
rates	O
while	O
states	O
like	O
north	O
dakota	O
with	O
negative	O
scores	O
on	O
the	O
first	O
component	O
have	O
low	O
crime	O
rates	O
california	O
also	O
has	O
a	O
high	O
score	O
on	O
the	O
second	O
component	O
indicating	O
a	O
high	O
level	B
of	O
urbanization	O
while	O
the	O
opposite	O
is	O
true	O
for	O
states	O
like	O
mississippi	O
states	O
close	O
to	O
zero	O
on	O
both	O
components	O
such	O
as	O
indiana	O
have	O
approximately	O
average	B
levels	O
of	O
both	O
crime	O
and	O
urbanization	O
another	O
interpretation	O
of	O
principal	B
components	I
the	O
first	O
two	O
principal	O
component	O
loading	O
vectors	O
in	O
a	O
simulated	O
threedimensional	O
data	B
set	B
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
these	O
two	O
loading	O
vectors	O
span	O
a	O
plane	O
along	O
which	O
the	O
observations	B
have	O
the	O
highest	O
variance	B
in	O
the	O
previous	O
section	O
we	O
describe	O
the	O
principal	O
component	O
loading	O
vectors	O
as	O
the	O
directions	O
in	O
feature	B
space	O
along	O
which	O
the	O
data	B
vary	O
the	O
most	O
and	O
the	O
principal	O
component	O
scores	O
as	O
projections	O
along	O
these	O
directions	O
however	O
an	O
alternative	O
interpretation	O
for	O
principal	B
components	I
can	O
also	O
be	O
useful	O
principal	B
components	I
provide	O
low-dimensional	B
linear	B
surfaces	O
that	O
are	O
closest	O
to	O
the	O
observations	B
we	O
expand	O
upon	O
that	O
interpretation	O
here	O
the	O
first	O
principal	O
component	O
loading	B
vector	B
has	O
a	O
very	O
special	O
property	O
it	O
is	O
the	O
line	B
in	O
p-dimensional	O
space	O
that	O
is	O
closest	O
to	O
the	O
n	O
observations	B
average	B
squared	O
euclidean	B
distance	I
as	O
a	O
measure	O
of	O
closeness	O
this	O
interpretation	O
can	O
be	O
seen	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
dashed	O
lines	O
indicate	O
the	O
distance	O
between	O
each	O
observation	O
and	O
the	O
first	O
principal	O
component	O
loading	B
vector	B
the	O
appeal	O
of	O
this	O
interpretation	O
is	O
clear	O
we	O
seek	O
a	O
single	B
dimension	O
of	O
the	O
data	B
that	O
lies	O
as	O
close	O
as	O
possible	O
to	O
all	O
of	O
the	O
data	B
points	O
since	O
such	O
a	O
line	B
will	O
likely	O
provide	O
a	O
good	O
summary	O
of	O
the	O
data	B
the	O
notion	O
of	O
principal	B
components	I
as	O
the	O
dimensions	O
that	O
are	O
closest	O
to	O
the	O
n	O
observations	B
extends	O
beyond	O
just	O
the	O
first	O
principal	O
component	O
for	O
instance	O
the	O
first	O
two	O
principal	B
components	I
of	O
a	O
data	B
set	B
span	O
the	O
plane	O
that	O
is	O
closest	O
to	O
the	O
n	O
observations	B
in	O
terms	O
of	O
average	B
squared	O
euclidean	B
distance	I
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
the	O
first	O
three	O
principal	B
components	I
of	O
a	O
data	B
set	B
span	O
the	O
three-dimensional	O
hyperplane	B
that	O
is	O
closest	O
to	O
the	O
n	O
observations	B
and	O
so	O
forth	O
using	O
this	O
interpretation	O
together	O
the	O
first	O
m	O
principal	O
component	O
score	O
vectors	O
and	O
the	O
first	O
m	O
principal	O
component	O
loading	O
vectors	O
provide	O
the	O
best	O
m	O
approximation	O
terms	O
of	O
euclidean	B
distance	I
to	O
the	O
ith	O
observation	O
xij	O
this	O
representation	O
can	O
be	O
written	O
unsupervised	B
learning	I
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
l	O
i	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
o	O
c	O
e	O
s	O
first	O
principal	O
component	O
figure	O
ninety	O
observations	B
simulated	O
in	O
three	O
dimensions	O
left	O
the	O
first	O
two	O
principal	O
component	O
directions	O
span	O
the	O
plane	O
that	O
best	O
fits	O
the	O
data	B
it	O
minimizes	O
the	O
sum	O
of	O
squared	O
distances	O
from	O
each	O
point	O
to	O
the	O
plane	O
right	O
the	O
first	O
two	O
principal	O
component	O
score	O
vectors	O
give	O
the	O
coordinates	O
of	O
the	O
projection	B
of	O
the	O
observations	B
onto	O
the	O
plane	O
the	O
variance	B
in	O
the	O
plane	O
is	O
maximized	O
xij	O
zim	O
jm	O
the	O
original	O
data	B
matrix	O
x	O
is	O
column-centered	O
in	O
other	O
words	O
together	O
the	O
m	O
principal	O
component	O
score	O
vectors	O
and	O
m	O
principal	O
component	O
loading	O
vectors	O
can	O
give	O
a	O
good	O
approximation	O
to	O
the	O
data	B
when	O
m	O
is	O
sufficiently	O
large	O
when	O
m	O
minn	O
p	O
then	O
the	O
representation	O
is	O
exact	O
xij	O
m	O
zim	O
jm	O
more	O
on	O
pca	O
scaling	O
the	O
variables	O
we	O
have	O
already	O
mentioned	O
that	O
before	O
pca	O
is	O
performed	O
the	O
variables	O
should	O
be	O
centered	O
to	O
have	O
mean	O
zero	O
furthermore	O
the	O
results	O
obtained	O
when	O
we	O
perform	O
pca	O
will	O
also	O
depend	O
on	O
whether	O
the	O
variables	O
have	O
been	O
individually	O
scaled	O
multiplied	O
by	O
a	O
different	O
constant	O
this	O
is	O
in	O
contrast	B
to	O
some	O
other	O
supervised	O
and	O
unsupervised	B
learning	I
techniques	O
such	O
as	O
linear	B
regression	B
in	O
which	O
scaling	O
the	O
variables	O
has	O
no	O
effect	O
linear	B
regression	B
multiplying	O
a	O
variable	B
by	O
a	O
factor	B
of	O
c	O
will	O
simply	O
lead	O
to	O
multiplication	O
of	O
the	O
corresponding	O
coefficient	O
estimate	O
by	O
a	O
factor	B
of	O
and	O
thus	O
will	O
have	O
no	O
substantive	O
effect	O
on	O
the	O
model	B
obtained	O
for	O
instance	O
figure	O
was	O
obtained	O
after	O
scaling	O
each	O
of	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
this	O
is	O
reproduced	O
in	O
the	O
left-hand	O
plot	B
in	O
figure	O
why	O
does	O
it	O
matter	O
that	O
we	O
scaled	O
the	O
variables	O
in	O
these	O
data	B
principal	B
components	I
analysis	B
scaled	O
urbanpop	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
i	O
l	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
o	O
c	O
e	O
s	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
i	O
l	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
o	O
c	O
e	O
s	O
rape	O
assault	O
murder	O
unscaled	O
urbanpop	O
rape	O
murder	O
assau	O
first	O
principal	O
component	O
first	O
principal	O
component	O
figure	O
two	O
principal	O
component	O
biplots	O
for	O
the	O
usarrests	B
data	B
left	O
the	O
same	O
as	O
figure	O
with	O
the	O
variables	O
scaled	O
to	O
have	O
unit	O
standard	O
deviations	O
right	O
principal	B
components	I
using	O
unscaled	O
data	B
assault	O
has	O
by	O
far	O
the	O
largest	O
loading	O
on	O
the	O
first	O
principal	O
component	O
because	O
it	O
has	O
the	O
highest	O
variance	B
among	O
the	O
four	O
variables	O
in	O
general	O
scaling	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
is	O
recommended	O
the	O
variables	O
are	O
measured	O
in	O
different	O
units	O
murder	O
rape	O
and	O
assault	O
are	O
reported	O
as	O
the	O
number	O
of	O
occurrences	O
per	O
people	O
and	O
urbanpop	O
is	O
the	O
percentage	O
of	O
the	O
state	O
s	O
population	O
that	O
lives	O
in	O
an	O
urban	O
area	O
these	O
four	O
variables	O
have	O
variance	B
and	O
respectively	O
consequently	O
if	O
we	O
perform	O
pca	O
on	O
the	O
unscaled	O
variables	O
then	O
the	O
first	O
principal	O
component	O
loading	B
vector	B
will	O
have	O
a	O
very	O
large	O
loading	O
for	O
assault	O
since	O
that	O
variable	B
has	O
by	O
far	O
the	O
highest	O
variance	B
the	O
righthand	O
plot	B
in	O
figure	O
displays	O
the	O
first	O
two	O
principal	B
components	I
for	O
the	O
usarrests	B
data	B
set	B
without	O
scaling	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
as	O
predicted	O
the	O
first	O
principal	O
component	O
loading	B
vector	B
places	O
almost	O
all	O
of	O
its	O
weight	O
on	O
assault	O
while	O
the	O
second	O
principal	O
component	O
loading	B
vector	B
places	O
almost	O
all	O
of	O
its	O
weight	O
on	O
urpanpop	O
comparing	O
this	O
to	O
the	O
left-hand	O
plot	B
we	O
see	O
that	O
scaling	O
does	O
indeed	O
have	O
a	O
substantial	O
effect	O
on	O
the	O
results	O
obtained	O
however	O
this	O
result	O
is	O
simply	O
a	O
consequence	O
of	O
the	O
scales	O
on	O
which	O
the	O
variables	O
were	O
measured	O
for	O
instance	O
if	O
assault	O
were	O
measured	O
in	O
units	O
of	O
the	O
number	O
of	O
occurrences	O
per	O
people	O
than	O
number	O
of	O
occurrences	O
per	O
people	O
then	O
this	O
would	O
amount	O
to	O
dividing	O
all	O
of	O
the	O
elements	O
of	O
that	O
variable	B
by	O
then	O
the	O
variance	B
of	O
the	O
variable	B
would	O
be	O
tiny	O
and	O
so	O
the	O
first	O
principal	O
component	O
loading	B
vector	B
would	O
have	O
a	O
very	O
small	O
value	O
for	O
that	O
variable	B
because	O
it	O
is	O
undesirable	O
for	O
the	O
principal	B
components	I
obtained	O
to	O
depend	O
on	O
an	O
arbitrary	O
choice	O
of	O
scaling	O
we	O
typically	O
scale	O
each	O
variable	B
to	O
have	O
standard	O
deviation	O
one	O
before	O
we	O
perform	O
pca	O
unsupervised	B
learning	I
in	O
certain	O
settings	O
however	O
the	O
variables	O
may	O
be	O
measured	O
in	O
the	O
same	O
units	O
in	O
this	O
case	O
we	O
might	O
not	O
wish	O
to	O
scale	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
before	O
performing	O
pca	O
for	O
instance	O
suppose	O
that	O
the	O
variables	O
in	O
a	O
given	O
data	B
set	B
correspond	O
to	O
expression	O
levels	O
for	O
p	O
genes	O
then	O
since	O
expression	O
is	O
measured	O
in	O
the	O
same	O
units	O
for	O
each	O
gene	O
we	O
might	O
choose	O
not	O
to	O
scale	O
the	O
genes	O
to	O
each	O
have	O
standard	O
deviation	O
one	O
uniqueness	O
of	O
the	O
principal	B
components	I
each	O
principal	O
component	O
loading	B
vector	B
is	O
unique	O
up	O
to	O
a	O
sign	O
flip	O
this	O
means	O
that	O
two	O
different	O
software	O
packages	O
will	O
yield	O
the	O
same	O
principal	O
component	O
loading	O
vectors	O
although	O
the	O
signs	O
of	O
those	O
loading	O
vectors	O
may	O
differ	O
the	O
signs	O
may	O
differ	O
because	O
each	O
principal	O
component	O
loading	B
vector	B
specifies	O
a	O
direction	O
in	O
p-dimensional	O
space	O
flipping	O
the	O
sign	O
has	O
no	O
effect	O
as	O
the	O
direction	O
does	O
not	O
change	O
figure	O
the	O
principal	O
component	O
loading	B
vector	B
is	O
a	O
line	B
that	O
extends	O
in	O
either	O
direction	O
and	O
flipping	O
its	O
sign	O
would	O
have	O
no	O
effect	O
similarly	O
the	O
score	O
vectors	O
are	O
unique	O
up	O
to	O
a	O
sign	O
flip	O
since	O
the	O
variance	B
of	O
z	O
is	O
the	O
same	O
as	O
the	O
variance	B
of	O
z	O
it	O
is	O
worth	O
noting	O
that	O
when	O
we	O
use	O
to	O
approximate	O
xij	O
we	O
multiply	O
zim	O
by	O
jm	O
hence	O
if	O
the	O
sign	O
is	O
flipped	O
on	O
both	O
the	O
loading	O
and	O
score	O
vectors	O
the	O
final	O
product	O
of	O
the	O
two	O
quantities	O
is	O
unchanged	O
the	O
proportion	O
of	O
variance	B
explained	B
in	O
figure	O
we	O
performed	O
pca	O
on	O
a	O
three-dimensional	O
data	B
set	B
panel	O
and	O
projected	O
the	O
data	B
onto	O
the	O
first	O
two	O
principal	O
component	O
loading	O
vectors	O
in	O
order	O
to	O
obtain	O
a	O
two-dimensional	O
view	O
of	O
the	O
data	B
the	O
principal	O
component	O
score	O
vectors	O
right-hand	O
panel	O
we	O
see	O
that	O
this	O
two-dimensional	O
representation	O
of	O
the	O
three-dimensional	O
data	B
does	O
successfully	O
capture	O
the	O
major	O
pattern	O
in	O
the	O
data	B
the	O
orange	O
green	O
and	O
cyan	O
observations	B
that	O
are	O
near	O
each	O
other	O
in	O
three-dimensional	O
space	O
remain	O
nearby	O
in	O
the	O
two-dimensional	O
representation	O
similarly	O
we	O
have	O
seen	O
on	O
the	O
usarrests	B
data	B
set	B
that	O
we	O
can	O
summarize	O
the	O
observations	B
and	O
variables	O
using	O
just	O
the	O
first	O
two	O
principal	O
component	O
score	O
vectors	O
and	O
the	O
first	O
two	O
principal	O
component	O
loading	O
vectors	O
we	O
can	O
now	O
ask	O
a	O
natural	B
question	O
how	O
much	O
of	O
the	O
information	O
in	O
a	O
given	O
data	B
set	B
is	O
lost	O
by	O
projecting	O
the	O
observations	B
onto	O
the	O
first	O
few	O
principal	B
components	I
that	O
is	O
how	O
much	O
of	O
the	O
variance	B
in	O
the	O
data	B
is	O
not	O
contained	O
in	O
the	O
first	O
few	O
principal	B
components	I
more	O
generally	O
we	O
are	O
interested	O
in	O
knowing	O
the	O
proportion	O
of	O
variance	B
explained	B
by	O
each	O
principal	O
component	O
the	O
total	O
variance	B
present	O
in	O
a	O
data	B
set	B
that	O
the	O
variables	O
have	O
been	O
centered	O
to	O
have	O
mean	O
zero	O
is	O
defined	O
as	O
varxj	O
n	O
ij	O
proportion	O
of	O
variance	B
explained	B
principal	B
components	I
analysis	B
i	O
l	O
d	O
e	O
n	O
a	O
p	O
x	O
e	O
e	O
c	O
n	O
a	O
i	O
r	O
a	O
v	O
p	O
o	O
r	O
p	O
i	O
l	O
d	O
e	O
n	O
a	O
p	O
x	O
e	O
e	O
c	O
n	O
a	O
i	O
r	O
a	O
v	O
p	O
o	O
r	O
p	O
e	O
v	O
i	O
t	O
a	O
u	O
m	O
u	O
c	O
l	O
principal	O
component	O
principal	O
component	O
figure	O
left	O
a	O
scree	B
plot	B
depicting	O
the	O
proportion	O
of	O
variance	B
explained	B
by	O
each	O
of	O
the	O
four	O
principal	B
components	I
in	O
the	O
usarrests	B
data	B
right	O
the	O
cumulative	O
proportion	O
of	O
variance	B
explained	B
by	O
the	O
four	O
principal	B
components	I
in	O
the	O
usarrests	B
data	B
and	O
the	O
variance	B
explained	B
by	O
the	O
mth	O
principal	O
component	O
is	O
n	O
im	O
n	O
jmxij	O
therefore	O
the	O
pve	O
of	O
the	O
mth	O
principal	O
component	O
is	O
given	O
by	O
n	O
p	O
p	O
jmxij	O
n	O
ij	O
the	O
pve	O
of	O
each	O
principal	O
component	O
is	O
a	O
positive	O
quantity	O
in	O
order	O
to	O
compute	O
the	O
cumulative	O
pve	O
of	O
the	O
first	O
m	O
principal	B
components	I
we	O
can	O
simply	O
sum	O
over	O
each	O
of	O
the	O
first	O
m	O
pves	O
in	O
total	O
there	O
are	O
minn	O
p	O
principal	B
components	I
and	O
their	O
pves	O
sum	O
to	O
one	O
in	O
the	O
usarrests	B
data	B
the	O
first	O
principal	O
component	O
explains	O
of	O
the	O
variance	B
in	O
the	O
data	B
and	O
the	O
next	O
principal	O
component	O
explains	O
of	O
the	O
variance	B
together	O
the	O
first	O
two	O
principal	B
components	I
explain	O
almost	O
of	O
the	O
variance	B
in	O
the	O
data	B
and	O
the	O
last	O
two	O
principal	B
components	I
explain	O
only	O
of	O
the	O
variance	B
this	O
means	O
that	O
figure	O
provides	O
a	O
pretty	O
accurate	O
summary	O
of	O
the	O
data	B
using	O
just	O
two	O
dimensions	O
the	O
pve	O
of	O
each	O
principal	O
component	O
as	O
well	O
as	O
the	O
cumulative	O
pve	O
is	O
shown	O
in	O
figure	O
the	O
left-hand	O
panel	O
is	O
known	O
as	O
a	O
scree	B
plot	B
and	O
will	O
be	O
discussed	O
next	O
deciding	O
how	O
many	O
principal	B
components	I
to	O
use	O
in	O
general	O
a	O
n	O
p	O
data	B
matrix	O
x	O
has	O
minn	O
p	O
distinct	O
principal	B
components	I
however	O
we	O
usually	O
are	O
not	O
interested	O
in	O
all	O
of	O
them	O
rather	O
scree	B
plot	B
unsupervised	B
learning	I
we	O
would	O
like	O
to	O
use	O
just	O
the	O
first	O
few	O
principal	B
components	I
in	O
order	O
to	O
visualize	O
or	O
interpret	O
the	O
data	B
in	O
fact	O
we	O
would	O
like	O
to	O
use	O
the	O
smallest	O
number	O
of	O
principal	B
components	I
required	O
to	O
get	O
a	O
good	O
understanding	O
of	O
the	O
data	B
how	O
many	O
principal	B
components	I
are	O
needed	O
unfortunately	O
there	O
is	O
no	O
single	B
simple	B
answer	O
to	O
this	O
question	O
we	O
typically	O
decide	O
on	O
the	O
number	O
of	O
principal	B
components	I
required	O
to	O
visualize	O
the	O
data	B
by	O
examining	O
a	O
scree	B
plot	B
such	O
as	O
the	O
one	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
we	O
choose	O
the	O
smallest	O
number	O
of	O
principal	B
components	I
that	O
are	O
required	O
in	O
order	O
to	O
explain	O
a	O
sizable	O
amount	O
of	O
the	O
variation	O
in	O
the	O
data	B
this	O
is	O
done	O
by	O
eyeballing	O
the	O
scree	B
plot	B
and	O
looking	O
for	O
a	O
point	O
at	O
which	O
the	O
proportion	O
of	O
variance	B
explained	B
by	O
each	O
subsequent	O
principal	O
component	O
drops	O
off	O
this	O
is	O
often	O
referred	O
to	O
as	O
an	O
elbow	B
in	O
the	O
scree	B
plot	B
for	O
instance	O
by	O
inspection	O
of	O
figure	O
one	O
might	O
conclude	O
that	O
a	O
fair	O
amount	O
of	O
variance	B
is	O
explained	B
by	O
the	O
first	O
two	O
principal	B
components	I
and	O
that	O
there	O
is	O
an	O
elbow	B
after	O
the	O
second	O
component	O
after	O
all	O
the	O
third	O
principal	O
component	O
explains	O
less	O
than	O
ten	O
percent	O
of	O
the	O
variance	B
in	O
the	O
data	B
and	O
the	O
fourth	O
principal	O
component	O
explains	O
less	O
than	O
half	O
that	O
and	O
so	O
is	O
essentially	O
worthless	O
however	O
this	O
type	O
of	O
visual	O
analysis	B
is	O
inherently	O
ad	O
hoc	O
unfortunately	O
there	O
is	O
no	O
well-accepted	O
objective	O
way	O
to	O
decide	O
how	O
many	O
principal	B
components	I
are	O
enough	O
in	O
fact	O
the	O
question	O
of	O
how	O
many	O
principal	B
components	I
are	O
enough	O
is	O
inherently	O
ill-defined	O
and	O
will	O
depend	O
on	O
the	O
specific	O
area	O
of	O
application	O
and	O
the	O
specific	O
data	B
set	B
in	O
practice	O
we	O
tend	O
to	O
look	O
at	O
the	O
first	O
few	O
principal	B
components	I
in	O
order	O
to	O
find	O
interesting	O
patterns	O
in	O
the	O
data	B
if	O
no	O
interesting	O
patterns	O
are	O
found	O
in	O
the	O
first	O
few	O
principal	B
components	I
then	O
further	O
principal	B
components	I
are	O
unlikely	O
to	O
be	O
of	O
interest	O
conversely	O
if	O
the	O
first	O
few	O
principal	B
components	I
are	O
interesting	O
then	O
we	O
typically	O
continue	O
to	O
look	O
at	O
subsequent	O
principal	B
components	I
until	O
no	O
further	O
interesting	O
patterns	O
are	O
found	O
this	O
is	O
admittedly	O
a	O
subjective	O
approach	B
and	O
is	O
reflective	O
of	O
the	O
fact	O
that	O
pca	O
is	O
generally	O
used	O
as	O
a	O
tool	O
for	O
exploratory	B
data	B
analysis	B
on	O
the	O
other	O
hand	O
if	O
we	O
compute	O
principal	B
components	I
for	O
use	O
in	O
a	O
supervised	O
analysis	B
such	O
as	O
the	O
principal	B
components	I
regression	B
presented	O
in	O
section	O
then	O
there	O
is	O
a	O
simple	B
and	O
objective	O
way	O
to	O
determine	O
how	O
many	O
principal	B
components	I
to	O
use	O
we	O
can	O
treat	O
the	O
number	O
of	O
principal	O
component	O
score	O
vectors	O
to	O
be	O
used	O
in	O
the	O
regression	B
as	O
a	O
tuning	B
parameter	B
to	O
be	O
selected	O
via	O
cross-validation	B
or	O
a	O
related	O
approach	B
the	O
comparative	O
simplicity	O
of	O
selecting	O
the	O
number	O
of	O
principal	B
components	I
for	O
a	O
supervised	O
analysis	B
is	O
one	O
manifestation	O
of	O
the	O
fact	O
that	O
supervised	O
analyses	O
tend	O
to	O
be	O
more	O
clearly	O
defined	O
and	O
more	O
objectively	O
evaluated	O
than	O
unsupervised	O
analyses	O
clustering	B
clustering	B
methods	O
other	O
uses	O
for	O
principal	B
components	I
we	O
saw	O
in	O
section	O
that	O
we	O
can	O
perform	O
regression	B
using	O
the	O
principal	O
component	O
score	O
vectors	O
as	O
features	O
in	O
fact	O
many	O
statistical	O
techniques	O
such	O
as	O
regression	B
classification	B
and	O
clustering	B
can	O
be	O
easily	O
adapted	O
to	O
use	O
the	O
n	O
m	O
matrix	O
whose	O
columns	O
are	O
the	O
first	O
m	O
p	O
principal	O
component	O
score	O
vectors	O
rather	O
than	O
using	O
the	O
full	O
n	O
p	O
data	B
matrix	O
this	O
can	O
lead	O
to	O
less	O
noisy	O
results	O
since	O
it	O
is	O
often	O
the	O
case	O
that	O
the	O
signal	B
opposed	O
to	O
the	O
noise	B
in	O
a	O
data	B
set	B
is	O
concentrated	O
in	O
its	O
first	O
few	O
principal	B
components	I
clustering	B
methods	O
clustering	B
refers	O
to	O
a	O
very	O
broad	O
set	B
of	O
techniques	O
for	O
finding	O
subgroups	O
or	O
clusters	O
in	O
a	O
data	B
set	B
when	O
we	O
cluster	O
the	O
observations	B
of	O
a	O
data	B
set	B
we	O
seek	O
to	O
partition	O
them	O
into	O
distinct	O
groups	O
so	O
that	O
the	O
observations	B
within	O
each	O
group	O
are	O
quite	O
similar	O
to	O
each	O
other	O
while	O
observations	B
in	O
different	O
groups	O
are	O
quite	O
different	O
from	O
each	O
other	O
of	O
course	O
to	O
make	O
this	O
concrete	O
we	O
must	O
define	O
what	O
it	O
means	O
for	O
two	O
or	O
more	O
observations	B
to	O
be	O
similar	O
or	O
different	O
indeed	O
this	O
is	O
often	O
a	O
domain-specific	O
consideration	O
that	O
must	O
be	O
made	O
based	O
on	O
knowledge	O
of	O
the	O
data	B
being	O
studied	O
for	O
instance	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
n	O
observations	B
each	O
with	O
p	O
features	O
the	O
n	O
observations	B
could	O
correspond	O
to	O
tissue	O
samples	O
for	O
patients	O
with	O
breast	O
cancer	O
and	O
the	O
p	O
features	O
could	O
correspond	O
to	O
measurements	O
collected	O
for	O
each	O
tissue	O
sample	O
these	O
could	O
be	O
clinical	O
measurements	O
such	O
as	O
tumor	O
stage	O
or	O
grade	O
or	O
they	O
could	O
be	O
gene	O
expression	O
measurements	O
we	O
may	O
have	O
a	O
reason	O
to	O
believe	O
that	O
there	O
is	O
some	O
heterogeneity	O
among	O
the	O
n	O
tissue	O
samples	O
for	O
instance	O
perhaps	O
there	O
are	O
a	O
few	O
different	O
unknown	O
subtypes	O
of	O
breast	O
cancer	O
clustering	B
could	O
be	O
used	O
to	O
find	O
these	O
subgroups	O
this	O
is	O
an	O
unsupervised	O
problem	O
because	O
we	O
are	O
trying	O
to	O
discover	O
structure	O
in	O
this	O
case	O
distinct	O
clusters	O
on	O
the	O
basis	B
of	O
a	O
data	B
set	B
the	O
goal	O
in	O
supervised	O
problems	O
on	O
the	O
other	O
hand	O
is	O
to	O
try	O
to	O
predict	O
some	O
outcome	O
vector	B
such	O
as	O
survival	O
time	O
or	O
response	B
to	O
drug	O
treatment	O
both	O
clustering	B
and	O
pca	O
seek	O
to	O
simplify	O
the	O
data	B
via	O
a	O
small	O
number	O
of	O
summaries	O
but	O
their	O
mechanisms	O
are	O
different	O
pca	O
looks	O
to	O
find	O
a	O
low-dimensional	B
representation	O
of	O
the	O
observa	O
tions	O
that	O
explain	O
a	O
good	O
fraction	O
of	O
the	O
variance	B
clustering	B
looks	O
to	O
find	O
homogeneous	O
subgroups	O
among	O
the	O
observa	O
tions	O
another	O
application	O
of	O
clustering	B
arises	O
in	O
marketing	O
we	O
may	O
have	O
access	O
to	O
a	O
large	O
number	O
of	O
measurements	O
median	O
household	O
income	B
occupation	O
distance	O
from	O
nearest	O
urban	O
area	O
and	O
so	O
forth	O
for	O
a	O
large	O
k-means	B
clustering	B
hierarchical	B
clustering	B
dendrogram	B
unsupervised	B
learning	I
number	O
of	O
people	O
our	O
goal	O
is	O
to	O
perform	O
market	O
segmentation	O
by	O
identifying	O
subgroups	O
of	O
people	O
who	O
might	O
be	O
more	O
receptive	O
to	O
a	O
particular	O
form	O
of	O
advertising	B
or	O
more	O
likely	O
to	O
purchase	O
a	O
particular	O
product	O
the	O
task	O
of	O
performing	O
market	O
segmentation	O
amounts	O
to	O
clustering	B
the	O
people	O
in	O
the	O
data	B
set	B
since	O
clustering	B
is	O
popular	O
in	O
many	O
fields	O
there	O
exist	O
a	O
great	O
number	O
of	O
clustering	B
methods	O
in	O
this	O
section	O
we	O
focus	O
on	O
perhaps	O
the	O
two	O
best-known	O
clustering	B
approaches	O
k-means	B
clustering	B
and	O
hierarchical	B
clustering	B
in	O
k-means	B
clustering	B
we	O
seek	O
to	O
partition	O
the	O
observations	B
into	O
a	O
pre-specified	O
number	O
of	O
clusters	O
on	O
the	O
other	O
hand	O
in	O
hierarchical	B
clustering	B
we	O
do	O
not	O
know	O
in	O
advance	O
how	O
many	O
clusters	O
we	O
want	O
in	O
fact	O
we	O
end	O
up	O
with	O
a	O
tree-like	O
visual	O
representation	O
of	O
the	O
observations	B
called	O
a	O
dendrogram	B
that	O
allows	O
us	O
to	O
view	O
at	O
once	O
the	O
clusterings	O
obtained	O
for	O
each	O
possible	O
number	O
of	O
clusters	O
from	O
to	O
n	O
there	O
are	O
advantages	O
and	O
disadvantages	O
to	O
each	O
of	O
these	O
clustering	B
approaches	O
which	O
we	O
highlight	O
in	O
this	O
chapter	O
in	O
general	O
we	O
can	O
cluster	O
observations	B
on	O
the	O
basis	B
of	O
the	O
features	O
in	O
order	O
to	O
identify	O
subgroups	O
among	O
the	O
observations	B
or	O
we	O
can	O
cluster	O
features	O
on	O
the	O
basis	B
of	O
the	O
observations	B
in	O
order	O
to	O
discover	O
subgroups	O
among	O
the	O
features	O
in	O
what	O
follows	O
for	O
simplicity	O
we	O
will	O
discuss	O
clustering	B
observations	B
on	O
the	O
basis	B
of	O
the	O
features	O
though	O
the	O
converse	O
can	O
be	O
performed	O
by	O
simply	O
transposing	O
the	O
data	B
matrix	O
k-means	B
clustering	B
k-means	B
clustering	B
is	O
a	O
simple	B
and	O
elegant	O
approach	B
for	O
partitioning	O
a	O
data	B
set	B
into	O
k	O
distinct	O
non-overlapping	O
clusters	O
to	O
perform	O
k-means	B
clustering	B
we	O
must	O
first	O
specify	O
the	O
desired	O
number	O
of	O
clusters	O
k	O
then	O
the	O
k-means	B
algorithm	O
will	O
assign	O
each	O
observation	O
to	O
exactly	O
one	O
of	O
the	O
k	O
clusters	O
figure	O
shows	O
the	O
results	O
obtained	O
from	O
performing	O
k-means	B
clustering	B
on	O
a	O
simulated	O
example	O
consisting	O
of	O
observations	B
in	O
two	O
dimensions	O
using	O
three	O
different	O
values	O
of	O
k	O
the	O
k-means	B
clustering	B
procedure	O
results	O
from	O
a	O
simple	B
and	O
intuitive	O
mathematical	O
problem	O
we	O
begin	O
by	O
defining	O
some	O
notation	O
let	O
ck	O
denote	O
sets	O
containing	O
the	O
indices	O
of	O
the	O
observations	B
in	O
each	O
cluster	O
these	O
sets	O
satisfy	O
two	O
properties	O
ck	O
n	O
in	O
other	O
words	O
each	O
observation	O
belongs	O
to	O
at	O
least	O
one	O
of	O
the	O
k	O
clusters	O
ck	O
for	O
all	O
k	O
k	O
in	O
other	O
words	O
the	O
clusters	O
are	O
non	O
overlapping	O
no	O
observation	O
belongs	O
to	O
more	O
than	O
one	O
cluster	O
for	O
instance	O
if	O
the	O
ith	O
observation	O
is	O
in	O
the	O
kth	O
cluster	O
then	O
i	O
ck	O
the	O
idea	O
behind	O
k-means	B
clustering	B
is	O
that	O
a	O
good	O
clustering	B
is	O
one	O
for	O
which	O
the	O
within-cluster	O
variation	O
is	O
as	O
small	O
as	O
possible	O
the	O
within-cluster	O
variation	O
clustering	B
methods	O
figure	O
a	O
simulated	O
data	B
set	B
with	O
observations	B
in	O
two-dimensional	O
space	O
panels	O
show	O
the	O
results	O
of	O
applying	O
k-means	B
clustering	B
with	O
different	O
values	O
of	O
k	O
the	O
number	O
of	O
clusters	O
the	O
color	O
of	O
each	O
observation	O
indicates	O
the	O
cluster	O
to	O
which	O
it	O
was	O
assigned	O
using	O
the	O
k-means	B
clustering	B
algorithm	O
note	O
that	O
there	O
is	O
no	O
ordering	O
of	O
the	O
clusters	O
so	O
the	O
cluster	O
coloring	O
is	O
arbitrary	O
these	O
cluster	O
labels	O
were	O
not	O
used	O
in	O
clustering	B
instead	O
they	O
are	O
the	O
outputs	O
of	O
the	O
clustering	B
procedure	O
for	O
cluster	O
ck	O
is	O
a	O
measure	O
w	O
of	O
the	O
amount	O
by	O
which	O
the	O
observations	B
within	O
a	O
cluster	O
differ	O
from	O
each	O
other	O
hence	O
we	O
want	O
to	O
solve	O
the	O
problem	O
minimize	O
w	O
in	O
words	O
this	O
formula	O
says	O
that	O
we	O
want	O
to	O
partition	O
the	O
observations	B
into	O
k	O
clusters	O
such	O
that	O
the	O
total	O
within-cluster	O
variation	O
summed	O
over	O
all	O
k	O
clusters	O
is	O
as	O
small	O
as	O
possible	O
solving	O
seems	O
like	O
a	O
reasonable	O
idea	O
but	O
in	O
order	O
to	O
make	O
it	O
actionable	O
we	O
need	O
to	O
define	O
the	O
within-cluster	O
variation	O
there	O
are	O
many	O
possible	O
ways	O
to	O
define	O
this	O
concept	O
but	O
by	O
far	O
the	O
most	O
common	O
choice	O
involves	O
squared	O
euclidean	B
distance	I
that	O
is	O
we	O
define	O
w	O
ck	O
where	O
denotes	O
the	O
number	O
of	O
observations	B
in	O
the	O
kth	O
cluster	O
in	O
other	O
words	O
the	O
within-cluster	O
variation	O
for	O
the	O
kth	O
cluster	O
is	O
the	O
sum	O
of	O
all	O
of	O
the	O
pairwise	O
squared	O
euclidean	B
distances	O
between	O
the	O
observations	B
in	O
the	O
kth	O
cluster	O
divided	O
by	O
the	O
total	O
number	O
of	O
observations	B
in	O
the	O
kth	O
cluster	O
combining	O
and	O
gives	O
the	O
optimization	O
problem	O
that	O
defines	O
k-means	B
clustering	B
minimize	O
ck	O
unsupervised	B
learning	I
now	O
we	O
would	O
like	O
to	O
find	O
an	O
algorithm	O
to	O
solve	O
that	O
is	O
a	O
method	O
to	O
partition	O
the	O
observations	B
into	O
k	O
clusters	O
such	O
that	O
the	O
objective	O
of	O
is	O
minimized	O
this	O
is	O
in	O
fact	O
a	O
very	O
difficult	O
problem	O
to	O
solve	O
precisely	O
since	O
there	O
are	O
almost	O
k	O
n	O
ways	O
to	O
partition	O
n	O
observations	B
into	O
k	O
clusters	O
this	O
is	O
a	O
huge	O
number	O
unless	O
k	O
and	O
n	O
are	O
tiny	O
fortunately	O
a	O
very	O
simple	B
algorithm	O
can	O
be	O
shown	O
to	O
provide	O
a	O
local	B
optimum	O
a	O
pretty	O
good	O
solution	O
to	O
the	O
k-means	B
optimization	O
problem	O
this	O
approach	B
is	O
laid	O
out	O
in	O
algorithm	O
algorithm	O
k-means	B
clustering	B
randomly	O
assign	O
a	O
number	O
from	O
to	O
k	O
to	O
each	O
of	O
the	O
observations	B
these	O
serve	O
as	O
initial	O
cluster	O
assignments	O
for	O
the	O
observations	B
iterate	O
until	O
the	O
cluster	O
assignments	O
stop	O
changing	O
for	O
each	O
of	O
the	O
k	O
clusters	O
compute	O
the	O
cluster	O
centroid	B
the	O
kth	O
cluster	O
centroid	B
is	O
the	O
vector	B
of	O
the	O
p	O
feature	B
means	O
for	O
the	O
observations	B
in	O
the	O
kth	O
cluster	O
assign	O
each	O
observation	O
to	O
the	O
cluster	O
whose	O
centroid	B
is	O
closest	O
closest	O
is	O
defined	O
using	O
euclidean	B
distance	I
algorithm	O
is	O
guaranteed	O
to	O
decrease	O
the	O
value	O
of	O
the	O
objective	O
at	O
each	O
step	O
to	O
understand	O
why	O
the	O
following	O
identity	O
is	O
illuminating	O
ck	O
i	O
ck	O
xkj	O
i	O
ck	O
xij	O
where	O
xkj	O
is	O
the	O
mean	O
for	O
feature	B
j	O
in	O
cluster	O
ck	O
in	O
step	O
the	O
cluster	O
means	O
for	O
each	O
feature	B
are	O
the	O
constants	O
that	O
minimize	O
the	O
sum-of-squared	O
deviations	O
and	O
in	O
step	O
reallocating	O
the	O
observations	B
can	O
only	O
improve	O
this	O
means	O
that	O
as	O
the	O
algorithm	O
is	O
run	O
the	O
clustering	B
obtained	O
will	O
continually	O
improve	O
until	O
the	O
result	O
no	O
longer	O
changes	O
the	O
objective	O
of	O
will	O
never	O
increase	O
when	O
the	O
result	O
no	O
longer	O
changes	O
a	O
local	B
optimum	O
has	O
been	O
reached	O
figure	O
shows	O
the	O
progression	O
of	O
the	O
algorithm	O
on	O
the	O
toy	O
example	O
from	O
figure	O
k-means	B
clustering	B
derives	O
its	O
name	O
from	O
the	O
fact	O
that	O
in	O
step	O
the	O
cluster	O
centroids	O
are	O
computed	O
as	O
the	O
mean	O
of	O
the	O
observations	B
assigned	O
to	O
each	O
cluster	O
because	O
the	O
k-means	B
algorithm	O
finds	O
a	O
local	B
rather	O
than	O
a	O
global	O
optimum	O
the	O
results	O
obtained	O
will	O
depend	O
on	O
the	O
initial	O
cluster	O
assignment	O
of	O
each	O
observation	O
in	O
step	O
of	O
algorithm	O
for	O
this	O
reason	O
it	O
is	O
important	O
to	O
run	O
the	O
algorithm	O
multiple	B
times	O
from	O
different	O
random	O
data	B
step	O
iteration	O
step	O
clustering	B
methods	O
iteration	O
step	O
iteration	O
step	O
final	O
results	O
figure	O
the	O
progress	O
of	O
the	O
k-means	B
algorithm	O
on	O
the	O
example	O
of	O
figure	O
with	O
top	O
left	O
the	O
observations	B
are	O
shown	O
top	O
center	O
in	O
step	O
of	O
the	O
algorithm	O
each	O
observation	O
is	O
randomly	O
assigned	O
to	O
a	O
cluster	O
top	O
right	O
in	O
step	O
the	O
cluster	O
centroids	O
are	O
computed	O
these	O
are	O
shown	O
as	O
large	O
colored	O
disks	O
initially	O
the	O
centroids	O
are	O
almost	O
completely	O
overlapping	O
because	O
the	O
initial	O
cluster	O
assignments	O
were	O
chosen	O
at	O
random	O
bottom	O
left	O
in	O
step	O
each	O
observation	O
is	O
assigned	O
to	O
the	O
nearest	O
centroid	B
bottom	O
center	O
step	O
is	O
once	O
again	O
performed	O
leading	O
to	O
new	O
cluster	O
centroids	O
bottom	O
right	O
the	O
results	O
obtained	O
after	O
ten	O
iterations	O
initial	O
configurations	O
then	O
one	O
selects	O
the	O
best	O
solution	O
i	O
e	O
that	O
for	O
which	O
the	O
objective	O
is	O
smallest	O
figure	O
shows	O
the	O
local	B
optima	O
obtained	O
by	O
running	O
k-means	B
clustering	B
six	O
times	O
using	O
six	O
different	O
initial	O
cluster	O
assignments	O
using	O
the	O
toy	O
data	B
from	O
figure	O
in	O
this	O
case	O
the	O
best	O
clustering	B
is	O
the	O
one	O
with	O
an	O
objective	O
value	O
of	O
as	O
we	O
have	O
seen	O
to	O
perform	O
k-means	B
clustering	B
we	O
must	O
decide	O
how	O
many	O
clusters	O
we	O
expect	O
in	O
the	O
data	B
the	O
problem	O
of	O
selecting	O
k	O
is	O
far	O
from	O
simple	B
this	O
issue	O
along	O
with	O
other	O
practical	O
considerations	O
that	O
arise	O
in	O
performing	O
k-means	B
clustering	B
is	O
addressed	O
in	O
section	O
unsupervised	B
learning	I
figure	O
k-means	B
clustering	B
performed	O
six	O
times	O
on	O
the	O
data	B
from	O
figure	O
with	O
k	O
each	O
time	O
with	O
a	O
different	O
random	O
assignment	O
of	O
the	O
observations	B
in	O
step	O
of	O
the	O
k-means	B
algorithm	O
above	O
each	O
plot	B
is	O
the	O
value	O
of	O
the	O
objective	O
three	O
different	O
local	B
optima	O
were	O
obtained	O
one	O
of	O
which	O
resulted	O
in	O
a	O
smaller	O
value	O
of	O
the	O
objective	O
and	O
provides	O
better	O
separation	O
between	O
the	O
clusters	O
those	O
labeled	O
in	O
red	O
all	O
achieved	O
the	O
same	O
best	O
solution	O
with	O
an	O
objective	O
value	O
of	O
hierarchical	B
clustering	B
one	O
potential	O
disadvantage	O
of	O
k-means	B
clustering	B
is	O
that	O
it	O
requires	O
us	O
to	O
pre-specify	O
the	O
number	O
of	O
clusters	O
k	O
hierarchical	B
clustering	B
is	O
an	O
alternative	O
approach	B
which	O
does	O
not	O
require	O
that	O
we	O
commit	O
to	O
a	O
particular	O
choice	O
of	O
k	O
hierarchical	B
clustering	B
has	O
an	O
added	O
advantage	O
over	O
k-means	B
clustering	B
in	O
that	O
it	O
results	O
in	O
an	O
attractive	O
tree-based	O
representation	O
of	O
the	O
observations	B
called	O
a	O
dendrogram	B
in	O
this	O
section	O
we	O
describe	O
bottom-up	B
or	O
agglomerative	B
clustering	B
this	O
is	O
the	O
most	O
common	O
type	O
of	O
hierarchical	B
clustering	B
and	O
refers	O
to	O
the	O
fact	O
that	O
a	O
dendrogram	B
depicted	O
as	O
an	O
upside-down	O
tree	B
see	O
bottom-up	B
agglomerative	B
clustering	B
methods	O
x	O
figure	O
forty-five	O
observations	B
generated	O
in	O
two-dimensional	O
space	O
in	O
reality	O
there	O
are	O
three	O
distinct	O
classes	O
shown	O
in	O
separate	O
colors	O
however	O
we	O
will	O
treat	O
these	O
class	O
labels	O
as	O
unknown	O
and	O
will	O
seek	O
to	O
cluster	O
the	O
observations	B
in	O
order	O
to	O
discover	O
the	O
classes	O
from	O
the	O
data	B
figure	O
is	O
built	O
starting	O
from	O
the	O
leaves	O
and	O
combining	O
clusters	O
up	O
to	O
the	O
trunk	O
we	O
will	O
begin	O
with	O
a	O
discussion	O
of	O
how	O
to	O
interpret	O
a	O
dendrogram	B
and	O
then	O
discuss	O
how	O
hierarchical	B
clustering	B
is	O
actually	O
performed	O
that	O
is	O
how	O
the	O
dendrogram	B
is	O
built	O
interpreting	O
a	O
dendrogram	B
we	O
begin	O
with	O
the	O
simulated	O
data	B
set	B
shown	O
in	O
figure	O
consisting	O
of	O
observations	B
in	O
two-dimensional	O
space	O
the	O
data	B
were	O
generated	O
from	O
a	O
three-class	O
model	B
the	O
true	O
class	O
labels	O
for	O
each	O
observation	O
are	O
shown	O
in	O
distinct	O
colors	O
however	O
suppose	O
that	O
the	O
data	B
were	O
observed	O
without	O
the	O
class	O
labels	O
and	O
that	O
we	O
wanted	O
to	O
perform	O
hierarchical	B
clustering	B
of	O
the	O
data	B
hierarchical	B
clustering	B
complete	B
linkage	B
to	O
be	O
discussed	O
later	O
yields	O
the	O
result	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
how	O
can	O
we	O
interpret	O
this	O
dendrogram	B
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
each	O
leaf	B
of	O
the	O
dendrogram	B
represents	O
one	O
of	O
the	O
observations	B
in	O
figure	O
however	O
as	O
we	O
move	O
up	O
the	O
tree	B
some	O
leaves	O
begin	O
to	O
fuse	O
into	O
branches	O
these	O
correspond	O
to	O
observations	B
that	O
are	O
similar	O
to	O
each	O
other	O
as	O
we	O
move	O
higher	O
up	O
the	O
tree	B
branches	O
themselves	O
fuse	O
either	O
with	O
leaves	O
or	O
other	O
branches	O
the	O
earlier	O
in	O
the	O
tree	B
fusions	O
occur	O
the	O
more	O
similar	O
the	O
groups	O
of	O
observations	B
are	O
to	O
each	O
other	O
on	O
the	O
other	O
hand	O
observations	B
that	O
fuse	O
later	O
the	O
top	O
of	O
the	O
tree	B
can	O
be	O
quite	O
different	O
in	O
fact	O
this	O
statement	O
can	O
be	O
made	O
precise	O
for	O
any	O
two	O
observations	B
we	O
can	O
look	O
for	O
the	O
point	O
in	O
the	O
tree	B
where	O
branches	O
containing	O
those	O
two	O
observations	B
are	O
first	O
fused	O
the	O
height	O
of	O
this	O
fusion	O
as	O
measured	O
on	O
the	O
vertical	O
axis	O
indicates	O
how	O
unsupervised	B
learning	I
figure	O
left	O
dendrogram	B
obtained	O
from	O
hierarchically	O
clustering	B
the	O
data	B
from	O
figure	O
with	O
complete	B
linkage	B
and	O
euclidean	B
distance	I
center	O
the	O
dendrogram	B
from	O
the	O
left-hand	O
panel	O
cut	O
at	O
a	O
height	O
of	O
nine	O
by	O
the	O
dashed	O
line	B
this	O
cut	O
results	O
in	O
two	O
distinct	O
clusters	O
shown	O
in	O
different	O
colors	O
right	O
the	O
dendrogram	B
from	O
the	O
left-hand	O
panel	O
now	O
cut	O
at	O
a	O
height	O
of	O
five	O
this	O
cut	O
results	O
in	O
three	O
distinct	O
clusters	O
shown	O
in	O
different	O
colors	O
note	O
that	O
the	O
colors	O
were	O
not	O
used	O
in	O
clustering	B
but	O
are	O
simply	O
used	O
for	O
display	O
purposes	O
in	O
this	O
figure	O
different	O
the	O
two	O
observations	B
are	O
thus	O
observations	B
that	O
fuse	O
at	O
the	O
very	O
bottom	O
of	O
the	O
tree	B
are	O
quite	O
similar	O
to	O
each	O
other	O
whereas	O
observations	B
that	O
fuse	O
close	O
to	O
the	O
top	O
of	O
the	O
tree	B
will	O
tend	O
to	O
be	O
quite	O
different	O
this	O
highlights	O
a	O
very	O
important	O
point	O
in	O
interpreting	O
dendrograms	O
that	O
is	O
often	O
misunderstood	O
consider	O
the	O
left-hand	O
panel	O
of	O
figure	O
which	O
shows	O
a	O
simple	B
dendrogram	B
obtained	O
from	O
hierarchically	O
clustering	B
nine	O
observations	B
one	O
can	O
see	O
that	O
observations	B
and	O
are	O
quite	O
similar	O
to	O
each	O
other	O
since	O
they	O
fuse	O
at	O
the	O
lowest	O
point	O
on	O
the	O
dendrogram	B
observations	B
and	O
are	O
also	O
quite	O
similar	O
to	O
each	O
other	O
however	O
it	O
is	O
tempting	O
but	O
incorrect	O
to	O
conclude	O
from	O
the	O
figure	O
that	O
observations	B
and	O
are	O
quite	O
similar	O
to	O
each	O
other	O
on	O
the	O
basis	B
that	O
they	O
are	O
located	O
near	O
each	O
other	O
on	O
the	O
dendrogram	B
in	O
fact	O
based	O
on	O
the	O
information	O
contained	O
in	O
the	O
dendrogram	B
observation	O
is	O
no	O
more	O
similar	O
to	O
observation	O
than	O
it	O
is	O
to	O
observations	B
and	O
can	O
be	O
seen	O
from	O
the	O
right-hand	O
panel	O
of	O
figure	O
in	O
which	O
the	O
raw	O
data	B
are	O
displayed	O
to	O
put	O
it	O
mathematically	O
there	O
are	O
possible	O
reorderings	O
of	O
the	O
dendrogram	B
where	O
n	O
is	O
the	O
number	O
of	O
leaves	O
this	O
is	O
because	O
at	O
each	O
of	O
the	O
n	O
points	O
where	O
fusions	O
occur	O
the	O
positions	O
of	O
the	O
two	O
fused	O
branches	O
could	O
be	O
swapped	O
without	O
affecting	O
the	O
meaning	O
of	O
the	O
dendrogram	B
therefore	O
we	O
cannot	O
draw	O
conclusions	O
about	O
the	O
similarity	O
of	O
two	O
observations	B
based	O
on	O
their	O
proximity	O
along	O
the	O
horizontal	O
axis	O
rather	O
we	O
draw	O
conclusions	O
about	O
the	O
similarity	O
of	O
two	O
observations	B
based	O
on	O
the	O
location	O
on	O
the	O
vertical	O
axis	O
where	O
branches	O
containing	O
those	O
two	O
observations	B
first	O
are	O
fused	O
clustering	B
methods	O
x	O
figure	O
an	O
illustration	O
of	O
how	O
to	O
properly	O
interpret	O
a	O
dendrogram	B
with	O
nine	O
observations	B
in	O
two-dimensional	O
space	O
left	O
a	O
dendrogram	B
generated	O
using	O
euclidean	B
distance	I
and	O
complete	B
linkage	B
observations	B
and	O
are	O
quite	O
similar	O
to	O
each	O
other	O
as	O
are	O
observations	B
and	O
however	O
observation	O
is	O
no	O
more	O
similar	O
to	O
observation	O
than	O
it	O
is	O
to	O
observations	B
and	O
even	O
though	O
observations	B
and	O
are	O
close	O
together	O
in	O
terms	O
of	O
horizontal	O
distance	O
this	O
is	O
because	O
observations	B
and	O
all	O
fuse	O
with	O
observation	O
at	O
the	O
same	O
height	O
approximately	O
right	O
the	O
raw	O
data	B
used	O
to	O
generate	O
the	O
dendrogram	B
can	O
be	O
used	O
to	O
confirm	O
that	O
indeed	O
observation	O
is	O
no	O
more	O
similar	O
to	O
observation	O
than	O
it	O
is	O
to	O
observations	B
and	O
now	O
that	O
we	O
understand	O
how	O
to	O
interpret	O
the	O
left-hand	O
panel	O
of	O
figure	O
we	O
can	O
move	O
on	O
to	O
the	O
issue	O
of	O
identifying	O
clusters	O
on	O
the	O
basis	B
of	O
a	O
dendrogram	B
in	O
order	O
to	O
do	O
this	O
we	O
make	O
a	O
horizontal	O
cut	O
across	O
the	O
dendrogram	B
as	O
shown	O
in	O
the	O
center	O
and	O
right-hand	O
panels	O
of	O
figure	O
the	O
distinct	O
sets	O
of	O
observations	B
beneath	O
the	O
cut	O
can	O
be	O
interpreted	O
as	O
clusters	O
in	O
the	O
center	O
panel	O
of	O
figure	O
cutting	O
the	O
dendrogram	B
at	O
a	O
height	O
of	O
nine	O
results	O
in	O
two	O
clusters	O
shown	O
in	O
distinct	O
colors	O
in	O
the	O
right-hand	O
panel	O
cutting	O
the	O
dendrogram	B
at	O
a	O
height	O
of	O
five	O
results	O
in	O
three	O
clusters	O
further	O
cuts	O
can	O
be	O
made	O
as	O
one	O
descends	O
the	O
dendrogram	B
in	O
order	O
to	O
obtain	O
any	O
number	O
of	O
clusters	O
between	O
to	O
no	O
cut	O
and	O
n	O
to	O
a	O
cut	O
at	O
height	O
so	O
that	O
each	O
observation	O
is	O
in	O
its	O
own	O
cluster	O
in	O
other	O
words	O
the	O
height	O
of	O
the	O
cut	O
to	O
the	O
dendrogram	B
serves	O
the	O
same	O
role	O
as	O
the	O
k	O
in	O
k-means	B
clustering	B
it	O
controls	O
the	O
number	O
of	O
clusters	O
obtained	O
figure	O
therefore	O
highlights	O
a	O
very	O
attractive	O
aspect	O
of	O
hierarchical	B
clustering	B
one	O
single	B
dendrogram	B
can	O
be	O
used	O
to	O
obtain	O
any	O
number	O
of	O
clusters	O
in	O
practice	O
people	O
often	O
look	O
at	O
the	O
dendrogram	B
and	O
select	O
by	O
eye	O
a	O
sensible	O
number	O
of	O
clusters	O
based	O
on	O
the	O
heights	O
of	O
the	O
fusion	O
and	O
the	O
number	O
of	O
clusters	O
desired	O
in	O
the	O
case	O
of	O
figure	O
one	O
might	O
choose	O
to	O
select	O
either	O
two	O
or	O
three	O
clusters	O
however	O
often	O
the	O
choice	O
of	O
where	O
to	O
cut	O
the	O
dendrogram	B
is	O
not	O
so	O
clear	O
unsupervised	B
learning	I
the	O
term	B
hierarchical	B
refers	O
to	O
the	O
fact	O
that	O
clusters	O
obtained	O
by	O
cutting	O
the	O
dendrogram	B
at	O
a	O
given	O
height	O
are	O
necessarily	O
nested	O
within	O
the	O
clusters	O
obtained	O
by	O
cutting	O
the	O
dendrogram	B
at	O
any	O
greater	O
height	O
however	O
on	O
an	O
arbitrary	O
data	B
set	B
this	O
assumption	O
of	O
hierarchical	B
structure	O
might	O
be	O
unrealistic	O
for	O
instance	O
suppose	O
that	O
our	O
observations	B
correspond	O
to	O
a	O
group	O
of	O
people	O
with	O
a	O
split	O
of	O
males	O
and	O
females	O
evenly	O
split	O
among	O
americans	O
japanese	O
and	O
french	O
we	O
can	O
imagine	O
a	O
scenario	O
in	O
which	O
the	O
best	O
division	O
into	O
two	O
groups	O
might	O
split	O
these	O
people	O
by	O
gender	O
and	O
the	O
best	O
division	O
into	O
three	O
groups	O
might	O
split	O
them	O
by	O
nationality	O
in	O
this	O
case	O
the	O
true	O
clusters	O
are	O
not	O
nested	O
in	O
the	O
sense	O
that	O
the	O
best	O
division	O
into	O
three	O
groups	O
does	O
not	O
result	O
from	O
taking	O
the	O
best	O
division	O
into	O
two	O
groups	O
and	O
splitting	O
up	O
one	O
of	O
those	O
groups	O
consequently	O
this	O
situation	O
could	O
not	O
be	O
well-represented	O
by	O
hierarchical	B
clustering	B
due	O
to	O
situations	O
such	O
as	O
this	O
one	O
hierarchical	B
clustering	B
can	O
sometimes	O
yield	O
worse	O
less	O
accurate	O
results	O
than	O
k-means	B
clustering	B
for	O
a	O
given	O
number	O
of	O
clusters	O
the	O
hierarchical	B
clustering	B
algorithm	O
the	O
hierarchical	B
clustering	B
dendrogram	B
is	O
obtained	O
via	O
an	O
extremely	O
simple	B
algorithm	O
we	O
begin	O
by	O
defining	O
some	O
sort	O
of	O
dissimilarity	B
measure	O
between	O
each	O
pair	O
of	O
observations	B
most	O
often	O
euclidean	B
distance	I
is	O
used	O
we	O
will	O
discuss	O
the	O
choice	O
of	O
dissimilarity	B
measure	O
later	O
in	O
this	O
chapter	O
the	O
algorithm	O
proceeds	O
iteratively	O
starting	O
out	O
at	O
the	O
bottom	O
of	O
the	O
dendrogram	B
each	O
of	O
the	O
n	O
observations	B
is	O
treated	O
as	O
its	O
own	O
cluster	O
the	O
two	O
clusters	O
that	O
are	O
most	O
similar	O
to	O
each	O
other	O
are	O
then	O
fused	O
so	O
that	O
there	O
now	O
are	O
n	O
clusters	O
next	O
the	O
two	O
clusters	O
that	O
are	O
most	O
similar	O
to	O
each	O
other	O
are	O
fused	O
again	O
so	O
that	O
there	O
now	O
are	O
n	O
clusters	O
the	O
algorithm	O
proceeds	O
in	O
this	O
fashion	O
until	O
all	O
of	O
the	O
observations	B
belong	O
to	O
one	O
single	B
cluster	O
and	O
the	O
dendrogram	B
is	O
complete	B
figure	O
depicts	O
the	O
first	O
few	O
steps	O
of	O
the	O
algorithm	O
for	O
the	O
data	B
from	O
figure	O
to	O
summarize	O
the	O
hierarchical	B
clustering	B
algorithm	O
is	O
given	O
in	O
algorithm	O
this	O
algorithm	O
seems	O
simple	B
enough	O
but	O
one	O
issue	O
has	O
not	O
been	O
addressed	O
consider	O
the	O
bottom	O
right	O
panel	O
in	O
figure	O
how	O
did	O
we	O
determine	O
that	O
the	O
cluster	O
should	O
be	O
fused	O
with	O
the	O
cluster	O
we	O
have	O
a	O
concept	O
of	O
the	O
dissimilarity	B
between	O
pairs	O
of	O
observations	B
but	O
how	O
do	O
we	O
define	O
the	O
dissimilarity	B
between	O
two	O
clusters	O
if	O
one	O
or	O
both	O
of	O
the	O
clusters	O
contains	O
multiple	B
observations	B
the	O
concept	O
of	O
dissimilarity	B
between	O
a	O
pair	O
of	O
observations	B
needs	O
to	O
be	O
extended	O
to	O
a	O
pair	O
of	O
groups	O
of	O
observations	B
this	O
extension	O
is	O
achieved	O
by	O
developing	O
the	O
notion	O
of	O
linkage	B
which	O
defines	O
the	O
dissimilarity	B
between	O
two	O
groups	O
of	O
observations	B
the	O
four	O
most	O
common	O
types	O
of	O
linkage	B
complete	B
average	B
single	B
and	O
centroid	B
are	O
briefly	O
described	O
in	O
table	O
average	B
complete	B
and	O
single	B
linkage	B
are	O
most	O
popular	O
among	O
statisticians	O
average	B
and	O
complete	B
linkage	B
algorithm	O
hierarchical	B
clustering	B
clustering	B
methods	O
begin	O
with	O
n	O
observations	B
and	O
a	O
measure	O
as	O
euclidean	B
dis	O
nn	O
pairwise	O
dissimilarities	O
treat	O
each	O
tance	O
of	O
all	O
the	O
observation	O
as	O
its	O
own	O
cluster	O
n	O
for	O
i	O
n	O
n	O
examine	O
all	O
pairwise	O
inter-cluster	O
dissimilarities	O
among	O
the	O
i	O
clusters	O
and	O
identify	O
the	O
pair	O
of	O
clusters	O
that	O
are	O
least	O
dissimilar	O
is	O
most	O
similar	O
fuse	O
these	O
two	O
clusters	O
the	O
dissimilarity	B
between	O
these	O
two	O
clusters	O
indicates	O
the	O
height	O
in	O
the	O
dendrogram	B
at	O
which	O
the	O
fusion	O
should	O
be	O
placed	O
the	O
i	O
remaining	O
clusters	O
compute	O
the	O
new	O
pairwise	O
inter-cluster	O
dissimilarities	O
among	O
linkage	B
complete	B
single	B
average	B
centroid	B
description	O
maximal	O
intercluster	O
dissimilarity	B
compute	O
all	O
pairwise	O
dissimilarities	O
between	O
the	O
observations	B
in	O
cluster	O
a	O
and	O
the	O
observations	B
in	O
cluster	O
b	O
and	O
record	O
the	O
largest	O
of	O
these	O
dissimilarities	O
minimal	O
intercluster	O
dissimilarity	B
compute	O
all	O
pairwise	O
dissimilarities	O
between	O
the	O
observations	B
in	O
cluster	O
a	O
and	O
the	O
observations	B
in	O
cluster	O
b	O
and	O
record	O
the	O
smallest	O
of	O
these	O
dissimilarities	O
single	B
linkage	B
can	O
result	O
in	O
extended	O
trailing	O
clusters	O
in	O
which	O
single	B
observations	B
are	O
fused	O
one-at-a-time	O
mean	O
intercluster	O
dissimilarity	B
compute	O
all	O
pairwise	O
dissimilarities	O
between	O
the	O
observations	B
in	O
cluster	O
a	O
and	O
the	O
observations	B
in	O
cluster	O
b	O
and	O
record	O
the	O
average	B
of	O
these	O
dissimilarities	O
dissimilarity	B
between	O
the	O
centroid	B
for	O
cluster	O
a	O
mean	O
vector	B
of	O
length	O
p	O
and	O
the	O
centroid	B
for	O
cluster	O
b	O
centroid	B
linkage	B
can	O
result	O
in	O
undesirable	O
inversions	O
table	O
a	O
summary	O
of	O
the	O
four	O
most	O
commonly-used	O
types	O
of	O
linkage	B
in	O
hierarchical	B
clustering	B
linkage	B
are	O
generally	O
preferred	O
over	O
single	B
linkage	B
as	O
they	O
tend	O
to	O
yield	O
more	O
balanced	O
dendrograms	O
centroid	B
linkage	B
is	O
often	O
used	O
in	O
genomics	O
but	O
suffers	O
from	O
a	O
major	O
drawback	O
in	O
that	O
an	O
inversion	B
can	O
occur	O
whereby	O
two	O
clusters	O
are	O
fused	O
at	O
a	O
height	O
below	O
either	O
of	O
the	O
individual	O
clusters	O
in	O
the	O
dendrogram	B
this	O
can	O
lead	O
to	O
difficulties	O
in	O
visualization	O
as	O
well	O
as	O
in	O
interpretation	O
of	O
the	O
dendrogram	B
the	O
dissimilarities	O
computed	O
in	O
step	O
of	O
the	O
hierarchical	B
clustering	B
algorithm	O
will	O
depend	O
on	O
the	O
type	O
of	O
linkage	B
used	O
as	O
well	O
as	O
on	O
the	O
choice	O
of	O
dissimilarity	B
measure	O
hence	O
the	O
resulting	O
inversion	B
unsupervised	B
learning	I
x	O
x	O
x	O
x	O
figure	O
an	O
illustration	O
of	O
the	O
first	O
few	O
steps	O
of	O
the	O
hierarchical	B
clustering	B
algorithm	O
using	O
the	O
data	B
from	O
figure	O
with	O
complete	B
linkage	B
and	O
euclidean	B
distance	I
top	O
left	O
initially	O
there	O
are	O
nine	O
distinct	O
clusters	O
top	O
right	O
the	O
two	O
clusters	O
that	O
are	O
closest	O
together	O
and	O
are	O
fused	O
into	O
a	O
single	B
cluster	O
bottom	O
left	O
the	O
two	O
clusters	O
that	O
are	O
closest	O
together	O
and	O
are	O
fused	O
into	O
a	O
single	B
cluster	O
bottom	O
right	O
the	O
two	O
clusters	O
that	O
are	O
closest	O
together	O
using	O
complete	B
linkage	B
and	O
the	O
cluster	O
are	O
fused	O
into	O
a	O
single	B
cluster	O
dendrogram	B
typically	O
depends	O
quite	O
strongly	O
on	O
the	O
type	O
of	O
linkage	B
used	O
as	O
is	O
shown	O
in	O
figure	O
choice	O
of	O
dissimilarity	B
measure	O
thus	O
far	O
the	O
examples	O
in	O
this	O
chapter	O
have	O
used	O
euclidean	B
distance	I
as	O
the	O
dissimilarity	B
measure	O
but	O
sometimes	O
other	O
dissimilarity	B
measures	O
might	O
be	O
preferred	O
for	O
example	O
correlation-based	B
distance	O
considers	O
two	O
observations	B
to	O
be	O
similar	O
if	O
their	O
features	O
are	O
highly	O
correlated	O
even	O
though	O
the	O
observed	O
values	O
may	O
be	O
far	O
apart	O
in	O
terms	O
of	O
euclidean	B
distance	I
this	O
is	O
average	B
linkage	B
complete	B
linkage	B
single	B
linkage	B
clustering	B
methods	O
figure	O
average	B
complete	B
and	O
single	B
linkage	B
applied	O
to	O
an	O
example	O
data	B
set	B
average	B
and	O
complete	B
linkage	B
tend	O
to	O
yield	O
more	O
balanced	O
clusters	O
an	O
unusual	O
use	O
of	O
correlation	B
which	O
is	O
normally	O
computed	O
between	O
variables	O
here	O
it	O
is	O
computed	O
between	O
the	O
observation	O
profiles	O
for	O
each	O
pair	O
of	O
observations	B
figure	O
illustrates	O
the	O
difference	O
between	O
euclidean	B
and	O
correlation-based	B
distance	O
correlation-based	B
distance	O
focuses	O
on	O
the	O
shapes	O
of	O
observation	O
profiles	O
rather	O
than	O
their	O
magnitudes	O
the	O
choice	O
of	O
dissimilarity	B
measure	O
is	O
very	O
important	O
as	O
it	O
has	O
a	O
strong	O
effect	O
on	O
the	O
resulting	O
dendrogram	B
in	O
general	O
careful	O
attention	O
should	O
be	O
paid	O
to	O
the	O
type	O
of	O
data	B
being	O
clustered	O
and	O
the	O
scientific	O
question	O
at	O
hand	O
these	O
considerations	O
should	O
determine	O
what	O
type	O
of	O
dissimilarity	B
measure	O
is	O
used	O
for	O
hierarchical	B
clustering	B
for	O
instance	O
consider	O
an	O
online	O
retailer	O
interested	O
in	O
clustering	B
shoppers	O
based	O
on	O
their	O
past	O
shopping	O
histories	O
the	O
goal	O
is	O
to	O
identify	O
subgroups	O
of	O
similar	O
shoppers	O
so	O
that	O
shoppers	O
within	O
each	O
subgroup	O
can	O
be	O
shown	O
items	O
and	O
advertisements	O
that	O
are	O
particularly	O
likely	O
to	O
interest	O
them	O
suppose	O
the	O
data	B
takes	O
the	O
form	O
of	O
a	O
matrix	O
where	O
the	O
rows	O
are	O
the	O
shoppers	O
and	O
the	O
columns	O
are	O
the	O
items	O
available	O
for	O
purchase	O
the	O
elements	O
of	O
the	O
data	B
matrix	O
indicate	O
the	O
number	O
of	O
times	O
a	O
given	O
shopper	O
has	O
purchased	O
a	O
given	O
item	O
a	O
if	O
the	O
shopper	O
has	O
never	O
purchased	O
this	O
item	O
a	O
if	O
the	O
shopper	O
has	O
purchased	O
it	O
once	O
etc	O
what	O
type	O
of	O
dissimilarity	B
measure	O
should	O
be	O
used	O
to	O
cluster	O
the	O
shoppers	O
if	O
euclidean	B
distance	I
is	O
used	O
then	O
shoppers	O
who	O
have	O
bought	O
very	O
few	O
items	O
overall	O
infrequent	O
users	O
of	O
the	O
online	O
shopping	O
site	O
will	O
be	O
clustered	O
together	O
this	O
may	O
not	O
be	O
desirable	O
on	O
the	O
other	O
hand	O
if	O
correlation-based	B
distance	O
is	O
used	O
then	O
shoppers	O
with	O
similar	O
preferences	O
shoppers	O
who	O
have	O
bought	O
items	O
a	O
and	O
b	O
but	O
unsupervised	B
learning	I
observation	O
observation	O
observation	O
variable	B
index	O
figure	O
three	O
observations	B
with	O
measurements	O
on	O
variables	O
are	O
shown	O
observations	B
and	O
have	O
similar	O
values	O
for	O
each	O
variable	B
and	O
so	O
there	O
is	O
a	O
small	O
euclidean	B
distance	I
between	O
them	O
but	O
they	O
are	O
very	O
weakly	O
correlated	O
so	O
they	O
have	O
a	O
large	O
correlation-based	B
distance	O
on	O
the	O
other	O
hand	O
observations	B
and	O
have	O
quite	O
different	O
values	O
for	O
each	O
variable	B
and	O
so	O
there	O
is	O
a	O
large	O
euclidean	B
distance	I
between	O
them	O
but	O
they	O
are	O
highly	O
correlated	O
so	O
there	O
is	O
a	O
small	O
correlation-based	B
distance	O
between	O
them	O
never	O
items	O
c	O
or	O
d	O
will	O
be	O
clustered	O
together	O
even	O
if	O
some	O
shoppers	O
with	O
these	O
preferences	O
are	O
higher-volume	O
shoppers	O
than	O
others	O
therefore	O
for	O
this	O
application	O
correlation-based	B
distance	O
may	O
be	O
a	O
better	O
choice	O
in	O
addition	O
to	O
carefully	O
selecting	O
the	O
dissimilarity	B
measure	O
used	O
one	O
must	O
also	O
consider	O
whether	O
or	O
not	O
the	O
variables	O
should	O
be	O
scaled	O
to	O
have	O
standard	O
deviation	O
one	O
before	O
the	O
dissimilarity	B
between	O
the	O
observations	B
is	O
computed	O
to	O
illustrate	O
this	O
point	O
we	O
continue	O
with	O
the	O
online	O
shopping	O
example	O
just	O
described	O
some	O
items	O
may	O
be	O
purchased	O
more	O
frequently	O
than	O
others	O
for	O
instance	O
a	O
shopper	O
might	O
buy	O
ten	O
pairs	O
of	O
socks	O
a	O
year	O
but	O
a	O
computer	O
very	O
rarely	O
high-frequency	O
purchases	O
like	O
socks	O
therefore	O
tend	O
to	O
have	O
a	O
much	O
larger	O
effect	O
on	O
the	O
inter-shopper	O
dissimilarities	O
and	O
hence	O
on	O
the	O
clustering	B
ultimately	O
obtained	O
than	O
rare	O
purchases	O
like	O
computers	O
this	O
may	O
not	O
be	O
desirable	O
if	O
the	O
variables	O
are	O
scaled	O
to	O
have	O
standard	O
deviation	O
one	O
before	O
the	O
inter-observation	O
dissimilarities	O
are	O
computed	O
then	O
each	O
variable	B
will	O
in	O
effect	O
be	O
given	O
equal	O
importance	B
in	O
the	O
hierarchical	B
clustering	B
performed	O
we	O
might	O
also	O
want	O
to	O
scale	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
if	O
they	O
are	O
measured	O
on	O
different	O
scales	O
otherwise	O
the	O
choice	O
of	O
units	O
centimeters	O
versus	O
kilometers	O
for	O
a	O
particular	O
variable	B
will	O
greatly	O
affect	O
the	O
dissimilarity	B
measure	O
obtained	O
it	O
should	O
come	O
as	O
no	O
surprise	O
that	O
whether	O
or	O
not	O
it	O
is	O
a	O
good	O
decision	O
to	O
scale	O
the	O
variables	O
before	O
computing	O
the	O
dissimilarity	B
measure	O
depends	O
on	O
the	O
application	O
at	O
hand	O
an	O
example	O
is	O
shown	O
in	O
figure	O
we	O
note	O
that	O
the	O
issue	O
of	O
whether	O
or	O
not	O
to	O
scale	O
the	O
variables	O
before	O
performing	O
clustering	B
applies	O
to	O
k-means	B
clustering	B
as	O
well	O
clustering	B
methods	O
socks	O
computers	O
socks	O
computers	O
socks	O
computers	O
figure	O
an	O
eclectic	O
online	O
retailer	O
sells	O
two	O
items	O
socks	O
and	O
computers	O
left	O
the	O
number	O
of	O
pairs	O
of	O
socks	O
and	O
computers	O
purchased	O
by	O
eight	O
online	O
shoppers	O
is	O
displayed	O
each	O
shopper	O
is	O
shown	O
in	O
a	O
different	O
color	O
if	O
inter-observation	O
dissimilarities	O
are	O
computed	O
using	O
euclidean	B
distance	I
on	O
the	O
raw	O
variables	O
then	O
the	O
number	O
of	O
socks	O
purchased	O
by	O
an	O
individual	O
will	O
drive	O
the	O
dissimilarities	O
obtained	O
and	O
the	O
number	O
of	O
computers	O
purchased	O
will	O
have	O
little	O
effect	O
this	O
might	O
be	O
undesirable	O
since	O
computers	O
are	O
more	O
expensive	O
than	O
socks	O
and	O
so	O
the	O
online	O
retailer	O
may	O
be	O
more	O
interested	O
in	O
encouraging	O
shoppers	O
to	O
buy	O
computers	O
than	O
socks	O
and	O
a	O
large	O
difference	O
in	O
the	O
number	O
of	O
socks	O
purchased	O
by	O
two	O
shoppers	O
may	O
be	O
less	O
informative	O
about	O
the	O
shoppers	O
overall	O
shopping	O
preferences	O
than	O
a	O
small	O
difference	O
in	O
the	O
number	O
of	O
computers	O
purchased	O
center	O
the	O
same	O
data	B
is	O
shown	O
after	O
scaling	O
each	O
variable	B
by	O
its	O
standard	O
deviation	O
now	O
the	O
number	O
of	O
computers	O
purchased	O
will	O
have	O
a	O
much	O
greater	O
effect	O
on	O
the	O
inter-observation	O
dissimilarities	O
obtained	O
right	O
the	O
same	O
data	B
are	O
displayed	O
but	O
now	O
the	O
y-axis	O
represents	O
the	O
number	O
of	O
dollars	O
spent	O
by	O
each	O
online	O
shopper	O
on	O
socks	O
and	O
on	O
computers	O
since	O
computers	O
are	O
much	O
more	O
expensive	O
than	O
socks	O
now	O
computer	O
purchase	O
history	O
will	O
drive	O
the	O
inter-observation	O
dissimilarities	O
obtained	O
practical	O
issues	O
in	O
clustering	B
clustering	B
can	O
be	O
a	O
very	O
useful	O
tool	O
for	O
data	B
analysis	B
in	O
the	O
unsupervised	O
setting	O
however	O
there	O
are	O
a	O
number	O
of	O
issues	O
that	O
arise	O
in	O
performing	O
clustering	B
we	O
describe	O
some	O
of	O
these	O
issues	O
here	O
small	O
decisions	O
with	O
big	O
consequences	O
in	O
order	O
to	O
perform	O
clustering	B
some	O
decisions	O
must	O
be	O
made	O
should	O
the	O
observations	B
or	O
features	O
first	O
be	O
standardized	O
in	O
some	O
way	O
for	O
instance	O
maybe	O
the	O
variables	O
should	O
be	O
centered	O
to	O
have	O
mean	O
zero	O
and	O
scaled	O
to	O
have	O
standard	O
deviation	O
one	O
unsupervised	B
learning	I
in	O
the	O
case	O
of	O
hierarchical	B
clustering	B
what	O
dissimilarity	B
measure	O
should	O
be	O
used	O
what	O
type	O
of	O
linkage	B
should	O
be	O
used	O
where	O
should	O
we	O
cut	O
the	O
dendrogram	B
in	O
order	O
to	O
obtain	O
clusters	O
in	O
the	O
case	O
of	O
k-means	B
clustering	B
how	O
many	O
clusters	O
should	O
we	O
look	O
for	O
in	O
the	O
data	B
each	O
of	O
these	O
decisions	O
can	O
have	O
a	O
strong	O
impact	O
on	O
the	O
results	O
obtained	O
in	O
practice	O
we	O
try	O
several	O
different	O
choices	O
and	O
look	O
for	O
the	O
one	O
with	O
the	O
most	O
useful	O
or	O
interpretable	O
solution	O
with	O
these	O
methods	O
there	O
is	O
no	O
single	B
right	O
answer	O
any	O
solution	O
that	O
exposes	O
some	O
interesting	O
aspects	O
of	O
the	O
data	B
should	O
be	O
considered	O
validating	O
the	O
clusters	O
obtained	O
any	O
time	O
clustering	B
is	O
performed	O
on	O
a	O
data	B
set	B
we	O
will	O
find	O
clusters	O
but	O
we	O
really	O
want	O
to	O
know	O
whether	O
the	O
clusters	O
that	O
have	O
been	O
found	O
represent	O
true	O
subgroups	O
in	O
the	O
data	B
or	O
whether	O
they	O
are	O
simply	O
a	O
result	O
of	O
clustering	B
the	O
noise	B
for	O
instance	O
if	O
we	O
were	O
to	O
obtain	O
an	O
independent	B
set	B
of	O
observations	B
then	O
would	O
those	O
observations	B
also	O
display	O
the	O
same	O
set	B
of	O
clusters	O
this	O
is	O
a	O
hard	O
question	O
to	O
answer	O
there	O
exist	O
a	O
number	O
of	O
techniques	O
for	O
assigning	O
a	O
p-value	B
to	O
a	O
cluster	O
in	O
order	O
to	O
assess	O
whether	O
there	O
is	O
more	O
evidence	O
for	O
the	O
cluster	O
than	O
one	O
would	O
expect	O
due	O
to	O
chance	O
however	O
there	O
has	O
been	O
no	O
consensus	O
on	O
a	O
single	B
best	O
approach	B
more	O
details	O
can	O
be	O
found	O
in	O
hastie	O
et	O
al	O
other	O
considerations	O
in	O
clustering	B
both	O
k-means	B
and	O
hierarchical	B
clustering	B
will	O
assign	O
each	O
observation	O
to	O
a	O
cluster	O
however	O
sometimes	O
this	O
might	O
not	O
be	O
appropriate	O
for	O
instance	O
suppose	O
that	O
most	O
of	O
the	O
observations	B
truly	O
belong	O
to	O
a	O
small	O
number	O
of	O
subgroups	O
and	O
a	O
small	O
subset	O
of	O
the	O
observations	B
are	O
quite	O
different	O
from	O
each	O
other	O
and	O
from	O
all	O
other	O
observations	B
then	O
since	O
kmeans	O
and	O
hierarchical	B
clustering	B
force	O
every	O
observation	O
into	O
a	O
cluster	O
the	O
clusters	O
found	O
may	O
be	O
heavily	O
distorted	O
due	O
to	O
the	O
presence	O
of	O
outliers	O
that	O
do	O
not	O
belong	O
to	O
any	O
cluster	O
mixture	O
models	O
are	O
an	O
attractive	O
approach	B
for	O
accommodating	O
the	O
presence	O
of	O
such	O
outliers	O
these	O
amount	O
to	O
a	O
soft	O
version	O
of	O
k-means	B
clustering	B
and	O
are	O
described	O
in	O
hastie	O
et	O
al	O
in	O
addition	O
clustering	B
methods	O
generally	O
are	O
not	O
very	O
robust	B
to	O
perturbations	O
to	O
the	O
data	B
for	O
instance	O
suppose	O
that	O
we	O
cluster	O
n	O
observations	B
and	O
then	O
cluster	O
the	O
observations	B
again	O
after	O
removing	O
a	O
subset	O
of	O
the	O
n	O
observations	B
at	O
random	O
one	O
would	O
hope	O
that	O
the	O
two	O
sets	O
of	O
clusters	O
obtained	O
would	O
be	O
quite	O
similar	O
but	O
often	O
this	O
is	O
not	O
the	O
case	O
lab	O
principal	B
components	I
analysis	B
a	O
tempered	O
approach	B
to	O
interpreting	O
the	O
results	O
of	O
clustering	B
we	O
have	O
described	O
some	O
of	O
the	O
issues	O
associated	O
with	O
clustering	B
however	O
clustering	B
can	O
be	O
a	O
very	O
useful	O
and	O
valid	O
statistical	O
tool	O
if	O
used	O
properly	O
we	O
mentioned	O
that	O
small	O
decisions	O
in	O
how	O
clustering	B
is	O
performed	O
such	O
as	O
how	O
the	O
data	B
are	O
standardized	O
and	O
what	O
type	O
of	O
linkage	B
is	O
used	O
can	O
have	O
a	O
large	O
effect	O
on	O
the	O
results	O
therefore	O
we	O
recommend	O
performing	O
clustering	B
with	O
different	O
choices	O
of	O
these	O
parameters	O
and	O
looking	O
at	O
the	O
full	O
set	B
of	O
results	O
in	O
order	O
to	O
see	O
what	O
patterns	O
consistently	O
emerge	O
since	O
clustering	B
can	O
be	O
non-robust	O
we	O
recommend	O
clustering	B
subsets	O
of	O
the	O
data	B
in	O
order	O
to	O
get	O
a	O
sense	O
of	O
the	O
robustness	O
of	O
the	O
clusters	O
obtained	O
most	O
importantly	O
we	O
must	O
be	O
careful	O
about	O
how	O
the	O
results	O
of	O
a	O
clustering	B
analysis	B
are	O
reported	O
these	O
results	O
should	O
not	O
be	O
taken	O
as	O
the	O
absolute	O
truth	O
about	O
a	O
data	B
set	B
rather	O
they	O
should	O
constitute	O
a	O
starting	O
point	O
for	O
the	O
development	O
of	O
a	O
scientific	O
hypothesis	B
and	O
further	O
study	O
preferably	O
on	O
an	O
independent	B
data	B
set	B
lab	O
principal	B
components	I
analysis	B
in	O
this	O
lab	O
we	O
perform	O
pca	O
on	O
the	O
usarrests	B
data	B
set	B
which	O
is	O
part	O
of	O
the	O
base	O
r	O
package	O
the	O
rows	O
of	O
the	O
data	B
set	B
contain	O
the	O
states	O
in	O
alphabetical	O
order	O
states	O
row	O
names	O
usarrests	B
states	O
the	O
columns	O
of	O
the	O
data	B
set	B
contain	O
the	O
four	O
variables	O
names	O
usarrests	B
murder	O
assault	O
urbanpop	O
rape	O
we	O
first	O
briefly	O
examine	O
the	O
data	B
we	O
notice	O
that	O
the	O
variables	O
have	O
vastly	O
different	O
means	O
apply	O
usarrests	B
mean	O
murder	O
assault	O
urbanpop	O
rape	O
note	O
that	O
the	O
apply	O
function	B
allows	O
us	O
to	O
apply	O
a	O
function	B
in	O
this	O
case	O
the	O
mean	O
function	B
to	O
each	O
row	O
or	O
column	O
of	O
the	O
data	B
set	B
the	O
second	O
input	B
here	O
denotes	O
whether	O
we	O
wish	O
to	O
compute	O
the	O
mean	O
of	O
the	O
rows	O
or	O
the	O
columns	O
we	O
see	O
that	O
there	O
are	O
on	O
average	B
three	O
times	O
as	O
many	O
rapes	O
as	O
murders	O
and	O
more	O
than	O
eight	O
times	O
as	O
many	O
assaults	O
as	O
rapes	O
we	O
can	O
also	O
examine	O
the	O
variances	O
of	O
the	O
four	O
variables	O
using	O
the	O
apply	O
function	B
apply	O
usarrests	B
var	O
assault	O
urbanpop	O
murder	O
rape	O
prcomp	O
unsupervised	B
learning	I
not	O
surprisingly	O
the	O
variables	O
also	O
have	O
vastly	O
different	O
variances	O
the	O
urbanpop	O
variable	B
measures	O
the	O
percentage	O
of	O
the	O
population	O
in	O
each	O
state	O
living	O
in	O
an	O
urban	O
area	O
which	O
is	O
not	O
a	O
comparable	O
number	O
to	O
the	O
number	O
of	O
rapes	O
in	O
each	O
state	O
per	O
individuals	O
if	O
we	O
failed	O
to	O
scale	O
the	O
variables	O
before	O
performing	O
pca	O
then	O
most	O
of	O
the	O
principal	B
components	I
that	O
we	O
observed	O
would	O
be	O
driven	O
by	O
the	O
assault	O
variable	B
since	O
it	O
has	O
by	O
far	O
the	O
largest	O
mean	O
and	O
variance	B
thus	O
it	O
is	O
important	O
to	O
standardize	B
the	O
variables	O
to	O
have	O
mean	O
zero	O
and	O
standard	O
deviation	O
one	O
before	O
performing	O
pca	O
we	O
now	O
perform	O
principal	B
components	I
analysis	B
using	O
the	O
prcomp	O
func	O
tion	O
which	O
is	O
one	O
of	O
several	O
functions	O
in	O
r	O
that	O
perform	O
pca	O
pr	O
out	O
prcomp	O
usarrests	B
scale	O
true	O
by	O
default	B
the	O
prcomp	O
function	B
centers	O
the	O
variables	O
to	O
have	O
mean	O
zero	O
by	O
using	O
the	O
option	O
scaletrue	O
we	O
scale	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
the	O
output	B
from	O
prcomp	O
contains	O
a	O
number	O
of	O
useful	O
quantities	O
names	O
pr	O
out	O
sdev	O
rotation	O
center	O
scale	O
x	O
the	O
center	O
and	O
scale	O
components	O
correspond	O
to	O
the	O
means	O
and	O
standard	O
deviations	O
of	O
the	O
variables	O
that	O
were	O
used	O
for	O
scaling	O
prior	O
to	O
implementing	O
pca	O
pr	O
outcente	O
r	O
murder	O
assault	O
urbanpop	O
rape	O
pr	O
outscale	O
murder	O
assault	O
urbanpop	O
rape	O
the	O
rotation	O
matrix	O
provides	O
the	O
principal	O
component	O
loadings	O
each	O
column	O
of	O
pr	O
outrotation	O
contains	O
the	O
corresponding	O
principal	O
component	O
loading	O
pr	O
o	O
u	O
t	O
r	O
o	O
t	O
a	O
t	O
i	O
o	O
n	O
murder	O
assault	O
urbanpop	O
rape	O
we	O
see	O
that	O
there	O
are	O
four	O
distinct	O
principal	B
components	I
this	O
is	O
to	O
be	O
expected	O
because	O
there	O
are	O
in	O
general	O
minn	O
p	O
informative	O
principal	B
components	I
in	O
a	O
data	B
set	B
with	O
n	O
observations	B
and	O
p	O
variables	O
function	B
names	O
it	O
the	O
rotation	O
matrix	O
because	O
when	O
we	O
matrix-multiply	O
the	O
x	O
matrix	O
by	O
pr	O
outrotation	O
it	O
gives	O
us	O
the	O
coordinates	O
of	O
the	O
data	B
in	O
the	O
rotated	O
coordinate	O
system	O
these	O
coordinates	O
are	O
the	O
principal	O
component	O
scores	O
lab	O
principal	B
components	I
analysis	B
using	O
the	O
prcomp	O
function	B
we	O
do	O
not	O
need	O
to	O
explicitly	O
multiply	O
the	O
data	B
by	O
the	O
principal	O
component	O
loading	O
vectors	O
in	O
order	O
to	O
obtain	O
the	O
principal	O
component	O
score	O
vectors	O
rather	O
the	O
matrix	O
x	O
has	O
as	O
its	O
columns	O
the	O
principal	O
component	O
score	O
vectors	O
that	O
is	O
the	O
kth	O
column	O
is	O
the	O
kth	O
principal	O
component	O
score	B
vector	B
dim	O
pr	O
outx	O
we	O
can	O
plot	B
the	O
first	O
two	O
principal	B
components	I
as	O
follows	O
biplot	B
pr	O
out	O
scale	O
the	O
argument	B
to	O
biplot	B
ensures	O
that	O
the	O
arrows	O
are	O
scaled	O
to	O
represent	O
the	O
loadings	O
other	O
values	O
for	O
scale	O
give	O
slightly	O
different	O
biplots	O
with	O
different	O
interpretations	O
notice	O
that	O
this	O
figure	O
is	O
a	O
mirror	O
image	O
of	O
figure	O
recall	B
that	O
the	O
principal	B
components	I
are	O
only	O
unique	O
up	O
to	O
a	O
sign	O
change	O
so	O
we	O
can	O
reproduce	O
figure	O
by	O
making	O
a	O
few	O
small	O
changes	O
biplot	B
pr	O
o	O
u	O
t	O
r	O
o	O
t	O
a	O
t	O
i	O
o	O
n	O
pr	O
o	O
u	O
t	O
r	O
o	O
t	O
a	O
t	O
i	O
o	O
n	O
pr	O
outx	O
pr	O
outx	O
biplot	B
pr	O
out	O
scale	O
the	O
prcomp	O
function	B
also	O
outputs	O
the	O
standard	O
deviation	O
of	O
each	O
principal	O
component	O
for	O
instance	O
on	O
the	O
usarrests	B
data	B
set	B
we	O
can	O
access	O
these	O
standard	O
deviations	O
as	O
follows	O
pr	O
outsdev	O
the	O
variance	B
explained	B
by	O
each	O
principal	O
component	O
is	O
obtained	O
by	O
squaring	O
these	O
pr	O
var	O
pr	O
outsdev	O
pr	O
var	O
to	O
compute	O
the	O
proportion	O
of	O
variance	B
explained	B
by	O
each	O
principal	O
component	O
we	O
simply	O
divide	O
the	O
variance	B
explained	B
by	O
each	O
principal	O
component	O
by	O
the	O
total	O
variance	B
explained	B
by	O
all	O
four	O
principal	B
components	I
pve	O
pr	O
var	O
sum	O
pr	O
var	O
pve	O
we	O
see	O
that	O
the	O
first	O
principal	O
component	O
explains	O
of	O
the	O
variance	B
in	O
the	O
data	B
the	O
next	O
principal	O
component	O
explains	O
of	O
the	O
variance	B
and	O
so	O
forth	O
we	O
can	O
plot	B
the	O
pve	O
explained	B
by	O
each	O
component	O
as	O
well	O
as	O
the	O
cumulative	O
pve	O
as	O
follows	O
plot	B
pve	O
xlab	O
principal	O
component	O
ylab	O
proportio	O
n	O
of	O
variance	B
explained	B
ylim	O
c	O
type	O
b	O
plot	B
cumsum	O
pve	O
xlab	O
principal	O
component	O
ylab	O
cumulative	O
proportio	O
n	O
of	O
variance	B
explained	B
ylim	O
c	O
type	O
b	O
unsupervised	B
learning	I
the	O
result	O
is	O
shown	O
in	O
figure	O
note	O
that	O
the	O
function	B
cumsum	O
computes	O
the	O
cumulative	O
sum	O
of	O
the	O
elements	O
of	O
a	O
numeric	O
vector	B
for	O
instance	O
cumsum	O
a	O
c	O
cumsum	O
a	O
lab	O
clustering	B
k-means	B
clustering	B
the	O
function	B
kmeans	O
performs	O
k-means	B
clustering	B
in	O
r	O
we	O
begin	O
with	O
a	O
simple	B
simulated	O
example	O
in	O
which	O
there	O
truly	O
are	O
two	O
clusters	O
in	O
the	O
data	B
the	O
first	O
observations	B
have	O
a	O
mean	O
shift	O
relative	O
to	O
the	O
next	O
observations	B
kmeans	O
set	B
seed	B
x	O
matrix	O
rnorm	O
ncol	O
x	O
x	O
x	O
x	O
we	O
now	O
perform	O
k-means	B
clustering	B
with	O
k	O
km	O
out	O
kmeans	O
nstart	O
the	O
cluster	O
assignments	O
of	O
km	O
outcluster	O
km	O
outclust	O
e	O
r	O
the	O
observations	B
are	O
contained	O
in	O
the	O
k-means	B
clustering	B
perfectly	O
separated	O
the	O
observations	B
into	O
two	O
clusters	O
even	O
though	O
we	O
did	O
not	O
supply	O
any	O
group	O
information	O
to	O
kmeans	O
we	O
can	O
plot	B
the	O
data	B
with	O
each	O
observation	O
colored	O
according	O
to	O
its	O
cluster	O
assignment	O
plot	B
col	O
km	O
outclust	O
er	O
main	O
k	O
means	O
clustering	B
results	O
with	O
k	O
xlab	O
ylab	O
pch	O
cex	O
here	O
the	O
observations	B
can	O
be	O
easily	O
plotted	O
because	O
they	O
are	O
two-dimensional	O
if	O
there	O
were	O
more	O
than	O
two	O
variables	O
then	O
we	O
could	O
instead	O
perform	O
pca	O
and	O
plot	B
the	O
first	O
two	O
principal	B
components	I
score	O
vectors	O
in	O
this	O
example	O
we	O
knew	O
that	O
there	O
really	O
were	O
two	O
clusters	O
because	O
we	O
generated	O
the	O
data	B
however	O
for	O
real	O
data	B
in	O
general	O
we	O
do	O
not	O
know	O
the	O
true	O
number	O
of	O
clusters	O
we	O
could	O
instead	O
have	O
performed	O
k-means	B
clustering	B
on	O
this	O
example	O
with	O
k	O
set	B
seed	B
km	O
out	O
kmeans	O
nstart	O
km	O
out	O
k	O
means	O
clusterin	O
g	O
with	O
clusters	O
of	O
sizes	O
lab	O
clustering	B
cluster	O
means	O
clustering	B
vector	B
within	O
cluster	O
sum	B
of	I
squares	I
by	O
cluster	O
between	O
s	O
s	O
total	O
ss	O
available	O
components	O
cluster	O
centers	O
tot	O
withinss	O
betweenss	O
totss	O
size	O
withinss	O
plot	B
col	O
km	O
outclust	O
er	O
main	O
k	O
means	O
clustering	B
results	O
with	O
k	O
xlab	O
ylab	O
pch	O
cex	O
when	O
k	O
k-means	B
clustering	B
splits	O
up	O
the	O
two	O
clusters	O
to	O
run	O
the	O
kmeans	O
function	B
in	O
r	O
with	O
multiple	B
initial	O
cluster	O
assignments	O
we	O
use	O
the	O
nstart	O
argument	B
if	O
a	O
value	O
of	O
nstart	O
greater	O
than	O
one	O
is	O
used	O
then	O
k-means	B
clustering	B
will	O
be	O
performed	O
using	O
multiple	B
random	O
assignments	O
in	O
step	O
of	O
algorithm	O
and	O
the	O
kmeans	O
function	B
will	O
report	O
only	O
the	O
best	O
results	O
here	O
we	O
compare	O
using	O
to	O
set	B
seed	B
km	O
out	O
kmeans	O
nstart	O
km	O
outtot	O
withinss	O
km	O
out	O
kmeans	O
nstart	O
km	O
outtot	O
withinss	O
note	O
that	O
km	O
outtot	O
withinss	O
is	O
the	O
total	O
within-cluster	O
sum	B
of	I
squares	I
which	O
we	O
seek	O
to	O
minimize	O
by	O
performing	O
k-means	B
clustering	B
the	O
individual	O
within-cluster	O
sum-of-squares	O
are	O
contained	O
in	O
the	O
vector	B
km	O
outwithinss	O
we	O
strongly	O
recommend	O
always	O
running	O
k-means	B
clustering	B
with	O
a	O
large	O
value	O
of	O
nstart	O
such	O
as	O
or	O
since	O
otherwise	O
an	O
undesirable	O
local	B
optimum	O
may	O
be	O
obtained	O
when	O
performing	O
k-means	B
clustering	B
in	O
addition	O
to	O
using	O
multiple	B
initial	O
cluster	O
assignments	O
it	O
is	O
also	O
important	O
to	O
set	B
a	O
random	O
seed	B
using	O
the	O
set	B
seed	B
function	B
this	O
way	O
the	O
initial	O
cluster	O
assignments	O
in	O
step	O
can	O
be	O
replicated	O
and	O
the	O
k-means	B
output	B
will	O
be	O
fully	O
reproducible	O
unsupervised	B
learning	I
hierarchical	B
clustering	B
the	O
hclust	O
function	B
implements	O
hierarchical	B
clustering	B
in	O
r	O
in	O
the	O
following	O
example	O
we	O
use	O
the	O
data	B
from	O
section	O
to	O
plot	B
the	O
hierarchical	B
clustering	B
dendrogram	B
using	O
complete	B
single	B
and	O
average	B
linkage	B
clustering	B
with	O
euclidean	B
distance	I
as	O
the	O
dissimilarity	B
measure	O
we	O
begin	O
by	O
clustering	B
observations	B
using	O
complete	B
linkage	B
the	O
dist	O
function	B
is	O
used	O
to	O
compute	O
the	O
inter-observation	O
euclidean	B
distance	I
matrix	O
hclust	O
dist	O
hc	O
complete	B
hclust	O
dist	O
x	O
method	O
complete	B
we	O
could	O
just	O
as	O
easily	O
perform	O
hierarchical	B
clustering	B
with	O
average	B
or	O
single	B
linkage	B
instead	O
hc	O
average	B
hclust	O
dist	O
x	O
method	O
average	B
hc	O
single	B
hclust	O
dist	O
x	O
method	O
single	B
we	O
can	O
now	O
plot	B
the	O
dendrograms	O
obtained	O
using	O
the	O
usual	O
plot	B
function	B
the	O
numbers	O
at	O
the	O
bottom	O
of	O
the	O
plot	B
identify	O
each	O
observation	O
par	O
mfrow	O
c	O
plot	B
hc	O
complete	B
main	O
complete	B
linkage	B
xlab	O
sub	O
cex	O
plot	B
hc	O
average	B
main	O
average	B
linkage	B
xlab	O
sub	O
cex	O
plot	B
hc	O
single	B
main	O
single	B
linkage	B
xlab	O
sub	O
cex	O
to	O
determine	O
the	O
cluster	O
labels	O
for	O
each	O
observation	O
associated	O
with	O
a	O
given	O
cut	O
of	O
the	O
dendrogram	B
we	O
can	O
use	O
the	O
cutree	O
function	B
cutree	O
cutree	O
hc	O
complete	B
cutree	O
hc	O
average	B
cutree	O
hc	O
single	B
for	O
this	O
data	B
complete	B
and	O
average	B
linkage	B
generally	O
separate	O
the	O
observations	B
into	O
their	O
correct	O
groups	O
however	O
single	B
linkage	B
identifies	O
one	O
point	O
as	O
belonging	O
to	O
its	O
own	O
cluster	O
a	O
more	O
sensible	O
answer	O
is	O
obtained	O
when	O
four	O
clusters	O
are	O
selected	O
although	O
there	O
are	O
still	O
two	O
singletons	O
cutree	O
hc	O
single	B
to	O
scale	O
the	O
variables	O
before	O
performing	O
hierarchical	B
clustering	B
of	O
the	O
observations	B
we	O
use	O
the	O
scale	O
function	B
scale	O
xsc	O
scale	O
x	O
plot	B
hclust	O
dist	O
xsc	O
method	O
complete	B
main	O
h	O
i	O
e	O
r	O
a	O
r	O
c	O
h	O
i	O
c	O
a	O
l	O
clustering	B
with	O
scaled	O
features	O
lab	O
data	B
example	O
correlation-based	B
distance	O
can	O
be	O
computed	O
using	O
the	O
as	O
dist	O
function	B
which	O
converts	O
an	O
arbitrary	O
square	O
symmetric	O
matrix	O
into	O
a	O
form	O
that	O
the	O
hclust	O
function	B
recognizes	O
as	O
a	O
distance	O
matrix	O
however	O
this	O
only	O
makes	O
sense	O
for	O
data	B
with	O
at	O
least	O
three	O
features	O
since	O
the	O
absolute	O
correlation	B
between	O
any	O
two	O
observations	B
with	O
measurements	O
on	O
two	O
features	O
is	O
always	O
hence	O
we	O
will	O
cluster	O
a	O
three-dimensional	O
data	B
set	B
as	O
dist	O
x	O
matrix	O
rnorm	O
ncol	O
dd	O
as	O
dist	O
cor	O
t	O
x	O
plot	B
hclust	O
dd	O
method	O
complete	B
main	O
complete	B
linkage	B
with	O
correlation	B
based	O
distance	O
xlab	O
sub	O
lab	O
data	B
example	O
unsupervised	O
techniques	O
are	O
often	O
used	O
in	O
the	O
analysis	B
of	O
genomic	O
data	B
in	O
particular	O
pca	O
and	O
hierarchical	B
clustering	B
are	O
popular	O
tools	O
we	O
illustrate	O
these	O
techniques	O
on	O
the	O
cancer	O
cell	O
line	B
microarray	O
data	B
which	O
consists	O
of	O
gene	O
expression	O
measurements	O
on	O
cancer	O
cell	O
lines	O
library	O
islr	O
nci	O
labs	O
s	O
nci	O
data	B
a	O
each	O
cell	O
line	B
is	O
labeled	O
with	O
a	O
cancer	O
type	O
we	O
do	O
not	O
make	O
use	O
of	O
the	O
cancer	O
types	O
in	O
performing	O
pca	O
and	O
clustering	B
as	O
these	O
are	O
unsupervised	O
techniques	O
but	O
after	O
performing	O
pca	O
and	O
clustering	B
we	O
will	O
check	O
to	O
see	O
the	O
extent	O
to	O
which	O
these	O
cancer	O
types	O
agree	O
with	O
the	O
results	O
of	O
these	O
unsupervised	O
techniques	O
the	O
data	B
has	O
rows	O
and	O
columns	O
dim	O
nci	O
data	B
we	O
begin	O
by	O
examining	O
the	O
cancer	O
types	O
for	O
the	O
cell	O
lines	O
nci	O
labs	O
cns	O
cns	O
table	O
nci	O
labs	O
nci	O
labs	O
cns	O
renal	O
breast	O
ovarian	O
leukemia	O
repro	O
repro	O
renal	O
prostate	O
melanoma	O
unknown	O
cns	O
colon	O
repro	O
repro	O
nsclc	O
unsupervised	B
learning	I
pca	O
on	O
the	O
data	B
we	O
first	O
perform	O
pca	O
on	O
the	O
data	B
after	O
scaling	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
although	O
one	O
could	O
reasonably	O
argue	O
that	O
it	O
is	O
better	O
not	O
to	O
scale	O
the	O
genes	O
pr	O
out	O
prcomp	O
nci	O
data	B
scale	O
true	O
we	O
now	O
plot	B
the	O
first	O
few	O
principal	O
component	O
score	O
vectors	O
in	O
order	O
to	O
visualize	O
the	O
data	B
the	O
observations	B
lines	O
corresponding	O
to	O
a	O
given	O
cancer	O
type	O
will	O
be	O
plotted	O
in	O
the	O
same	O
color	O
so	O
that	O
we	O
can	O
see	O
to	O
what	O
extent	O
the	O
observations	B
within	O
a	O
cancer	O
type	O
are	O
similar	O
to	O
each	O
other	O
we	O
first	O
create	O
a	O
simple	B
function	B
that	O
assigns	O
a	O
distinct	O
color	O
to	O
each	O
element	O
of	O
a	O
numeric	O
vector	B
the	O
function	B
will	O
be	O
used	O
to	O
assign	O
a	O
color	O
to	O
each	O
of	O
the	O
cell	O
lines	O
based	O
on	O
the	O
cancer	O
type	O
to	O
which	O
it	O
corresponds	O
cols	O
function	B
vec	O
cols	O
rainbow	O
length	O
unique	O
vec	O
return	O
cols	O
as	O
numeric	O
as	O
factor	B
vec	O
note	O
that	O
the	O
rainbow	O
function	B
takes	O
as	O
its	O
argument	B
a	O
positive	O
integer	O
and	O
returns	O
a	O
vector	B
containing	O
that	O
number	O
of	O
distinct	O
colors	O
we	O
now	O
can	O
plot	B
the	O
principal	O
component	O
score	O
vectors	O
rainbow	O
par	O
mfrow	O
c	O
plot	B
pr	O
outx	O
col	O
cols	O
nci	O
labs	O
pch	O
xlab	O
ylab	O
plot	B
pr	O
outx	O
c	O
col	O
cols	O
nci	O
labs	O
pch	O
xlab	O
ylab	O
the	O
resulting	O
plots	O
are	O
shown	O
in	O
figure	O
on	O
the	O
whole	O
cell	O
lines	O
corresponding	O
to	O
a	O
single	B
cancer	O
type	O
do	O
tend	O
to	O
have	O
similar	O
values	O
on	O
the	O
first	O
few	O
principal	O
component	O
score	O
vectors	O
this	O
indicates	O
that	O
cell	O
lines	O
from	O
the	O
same	O
cancer	O
type	O
tend	O
to	O
have	O
pretty	O
similar	O
gene	O
expression	O
levels	O
we	O
can	O
obtain	O
a	O
summary	O
of	O
the	O
proportion	O
of	O
variance	B
explained	B
of	O
the	O
first	O
few	O
principal	B
components	I
using	O
the	O
summary	O
method	O
for	O
a	O
prcomp	O
object	O
have	O
truncated	O
the	O
printout	O
summary	O
pr	O
out	O
importanc	O
e	O
of	O
component	O
s	O
standard	O
deviation	O
proportio	O
n	O
of	O
variance	B
cumulativ	O
e	O
proportio	O
n	O
using	O
the	O
plot	B
function	B
we	O
can	O
also	O
plot	B
the	O
variance	B
explained	B
by	O
the	O
first	O
few	O
principal	B
components	I
plot	B
pr	O
out	O
note	O
that	O
the	O
height	O
of	O
each	O
bar	O
in	O
the	O
bar	O
plot	B
is	O
given	O
by	O
squaring	O
the	O
corresponding	O
element	O
of	O
pr	O
outsdev	O
however	O
it	O
is	O
more	O
informative	O
to	O
lab	O
data	B
example	O
z	O
z	O
figure	O
projections	O
of	O
the	O
cancer	O
cell	O
lines	O
onto	O
the	O
first	O
three	O
principal	B
components	I
other	O
words	O
the	O
scores	O
for	O
the	O
first	O
three	O
principal	B
components	I
on	O
the	O
whole	O
observations	B
belonging	O
to	O
a	O
single	B
cancer	O
type	O
tend	O
to	O
lie	O
near	O
each	O
other	O
in	O
this	O
low-dimensional	B
space	O
it	O
would	O
not	O
have	O
been	O
possible	O
to	O
visualize	O
the	O
data	B
without	O
using	O
a	O
dimension	B
reduction	I
method	O
such	O
as	O
pca	O
possible	O
scatterplots	O
none	O
of	O
since	O
based	O
on	O
the	O
full	O
data	B
set	B
there	O
are	O
which	O
would	O
have	O
been	O
particularly	O
informative	O
plot	B
the	O
pve	O
of	O
each	O
principal	O
component	O
a	O
scree	B
plot	B
and	O
the	O
cumulative	O
pve	O
of	O
each	O
principal	O
component	O
this	O
can	O
be	O
done	O
with	O
just	O
a	O
little	O
work	O
pve	O
pr	O
outsdev	O
sum	O
pr	O
outsdev	O
par	O
mfrow	O
c	O
plot	B
pve	O
type	O
o	O
ylab	O
pve	O
xlab	O
principal	O
component	O
col	O
blue	O
plot	B
cumsum	O
pve	O
type	O
o	O
ylab	O
cumulative	O
pve	O
xlab	O
principal	O
component	O
col	O
that	O
the	O
elements	O
of	O
pve	O
can	O
also	O
be	O
computed	O
directly	O
from	O
the	O
summary	O
and	O
the	O
elements	O
of	O
cumsumpve	O
are	O
given	O
by	O
the	O
resulting	O
plots	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
together	O
the	O
first	O
seven	O
principal	B
components	I
explain	O
around	O
of	O
the	O
variance	B
in	O
the	O
data	B
this	O
is	O
not	O
a	O
huge	O
amount	O
of	O
the	O
variance	B
however	O
looking	O
at	O
the	O
scree	B
plot	B
we	O
see	O
that	O
while	O
each	O
of	O
the	O
first	O
seven	O
principal	B
components	I
explain	O
a	O
substantial	O
amount	O
of	O
variance	B
there	O
is	O
a	O
marked	O
decrease	O
in	O
the	O
variance	B
explained	B
by	O
further	O
principal	B
components	I
that	O
is	O
there	O
is	O
an	O
elbow	B
in	O
the	O
plot	B
after	O
approximately	O
the	O
seventh	O
principal	O
component	O
this	O
suggests	O
that	O
there	O
may	O
be	O
little	O
benefit	O
to	O
examining	O
more	O
than	O
seven	O
or	O
so	O
principal	B
components	I
even	O
examining	O
seven	O
principal	B
components	I
may	O
be	O
difficult	O
unsupervised	B
learning	I
e	O
v	O
p	O
e	O
v	O
p	O
e	O
v	O
i	O
t	O
l	O
a	O
u	O
m	O
u	O
c	O
principal	O
component	O
principal	O
component	O
figure	O
the	O
pve	O
of	O
the	O
principal	B
components	I
of	O
the	O
cancer	O
cell	O
line	B
microarray	O
data	B
set	B
left	O
the	O
pve	O
of	O
each	O
principal	O
component	O
is	O
shown	O
right	O
the	O
cumulative	O
pve	O
of	O
the	O
principal	B
components	I
is	O
shown	O
together	O
all	O
principal	B
components	I
explain	O
of	O
the	O
variance	B
clustering	B
the	O
observations	B
of	O
the	O
data	B
we	O
now	O
proceed	O
to	O
hierarchically	O
cluster	O
the	O
cell	O
lines	O
in	O
the	O
data	B
with	O
the	O
goal	O
of	O
finding	O
out	O
whether	O
or	O
not	O
the	O
observations	B
cluster	O
into	O
distinct	O
types	O
of	O
cancer	O
to	O
begin	O
we	O
standardize	B
the	O
variables	O
to	O
have	O
mean	O
zero	O
and	O
standard	O
deviation	O
one	O
as	O
mentioned	O
earlier	O
this	O
step	O
is	O
optional	O
and	O
should	O
be	O
performed	O
only	O
if	O
we	O
want	O
each	O
gene	O
to	O
be	O
on	O
the	O
same	O
scale	O
sd	O
data	B
scale	O
nci	O
data	B
we	O
now	O
perform	O
hierarchical	B
clustering	B
of	O
the	O
observations	B
using	O
complete	B
single	B
and	O
average	B
linkage	B
euclidean	B
distance	I
is	O
used	O
as	O
the	O
dissimilarity	B
measure	O
par	O
mfrow	O
c	O
data	B
dist	O
dist	O
sd	O
data	B
plot	B
hclust	O
data	B
dist	O
labels	O
nci	O
labs	O
main	O
complete	B
linkage	B
xlab	O
sub	O
ylab	O
plot	B
hclust	O
data	B
dist	O
method	O
average	B
labels	O
nci	O
labs	O
main	O
average	B
linkage	B
xlab	O
sub	O
ylab	O
plot	B
hclust	O
data	B
dist	O
method	O
single	B
labels	O
nci	O
labs	O
main	O
single	B
linkage	B
xlab	O
sub	O
ylab	O
the	O
results	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
choice	O
of	O
linkage	B
certainly	O
does	O
affect	O
the	O
results	O
obtained	O
typically	O
single	B
linkage	B
will	O
tend	O
to	O
yield	O
trailing	O
clusters	O
very	O
large	O
clusters	O
onto	O
which	O
individual	O
observations	B
attach	O
one-by-one	O
on	O
the	O
other	O
hand	O
complete	B
and	O
average	B
linkage	B
tend	O
to	O
yield	O
more	O
balanced	O
attractive	O
clusters	O
for	O
this	O
reason	O
complete	B
and	O
average	B
linkage	B
are	O
generally	O
preferred	O
to	O
single	B
linkage	B
clearly	O
cell	O
lines	O
within	O
a	O
single	B
cancer	O
type	O
do	O
tend	O
to	O
cluster	O
together	O
although	O
the	O
lab	O
data	B
example	O
complete	B
linkage	B
l	O
a	O
n	O
e	O
r	O
s	O
n	O
c	O
s	O
n	O
c	O
c	O
l	O
c	O
s	O
n	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
a	O
r	O
a	O
v	O
o	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
s	O
n	O
c	O
c	O
l	O
c	O
s	O
n	O
s	O
n	O
c	O
s	O
n	O
c	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
w	O
o	O
n	O
k	O
n	O
u	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
t	O
s	O
a	O
e	O
r	O
b	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
ol	O
r	O
p	O
e	O
r	O
a	O
k	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
o	O
r	O
p	O
e	O
r	O
b	O
k	O
t	O
s	O
a	O
e	O
r	O
b	O
o	O
r	O
p	O
e	O
r	O
a	O
f	O
c	O
m	O
o	O
r	O
p	O
e	O
r	O
d	O
f	O
c	O
m	O
average	B
linkage	B
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
o	O
r	O
p	O
e	O
r	O
b	O
k	O
o	O
r	O
p	O
e	O
r	O
a	O
k	O
t	O
s	O
a	O
e	O
r	O
b	O
o	O
r	O
p	O
e	O
r	O
a	O
f	O
c	O
m	O
o	O
r	O
p	O
e	O
r	O
d	O
f	O
c	O
m	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
l	O
a	O
n	O
e	O
r	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
c	O
l	O
c	O
s	O
n	O
i	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
a	O
r	O
a	O
v	O
o	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
w	O
o	O
n	O
k	O
n	O
u	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
s	O
n	O
c	O
c	O
l	O
c	O
s	O
n	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
s	O
n	O
c	O
s	O
n	O
c	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
c	O
l	O
c	O
s	O
n	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
s	O
n	O
c	O
s	O
n	O
c	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
single	B
linkage	B
s	O
n	O
c	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
i	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
c	O
l	O
c	O
s	O
n	O
t	O
s	O
a	O
e	O
r	O
b	O
n	O
o	O
l	O
o	O
c	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
l	O
a	O
n	O
e	O
r	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
s	O
n	O
c	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
o	O
r	O
p	O
e	O
r	O
b	O
k	O
o	O
r	O
p	O
e	O
r	O
a	O
k	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
n	O
o	O
l	O
o	O
c	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
n	O
o	O
l	O
o	O
c	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
n	O
o	O
l	O
o	O
c	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
s	O
n	O
c	O
s	O
n	O
c	O
s	O
n	O
c	O
l	O
a	O
n	O
e	O
r	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
n	O
o	O
l	O
o	O
c	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
w	O
o	O
n	O
k	O
n	O
u	O
t	O
s	O
a	O
e	O
r	O
b	O
o	O
r	O
p	O
e	O
r	O
a	O
f	O
c	O
m	O
o	O
r	O
p	O
e	O
r	O
d	O
f	O
c	O
m	O
figure	O
the	O
cancer	O
cell	O
line	B
microarray	O
data	B
clustered	O
with	O
average	B
complete	B
and	O
single	B
linkage	B
and	O
using	O
euclidean	B
distance	I
as	O
the	O
dissimilarity	B
measure	O
complete	B
and	O
average	B
linkage	B
tend	O
to	O
yield	O
evenly	O
sized	O
clusters	O
whereas	O
single	B
linkage	B
tends	O
to	O
yield	O
extended	O
clusters	O
to	O
which	O
single	B
leaves	O
are	O
fused	O
one	O
by	O
one	O
unsupervised	B
learning	I
clustering	B
is	O
not	O
perfect	O
we	O
will	O
use	O
complete	B
linkage	B
hierarchical	B
clustering	B
for	O
the	O
analysis	B
that	O
follows	O
we	O
can	O
cut	O
the	O
dendrogram	B
at	O
the	O
height	O
that	O
will	O
yield	O
a	O
particular	O
number	O
of	O
clusters	O
say	O
four	O
hc	O
out	O
hclust	O
dist	O
sd	O
data	B
hc	O
clusters	O
cutree	O
hc	O
out	O
table	O
hc	O
clusters	O
nci	O
labs	O
there	O
are	O
some	O
clear	O
patterns	O
all	O
the	O
leukemia	O
cell	O
lines	O
fall	O
in	O
cluster	O
while	O
the	O
breast	O
cancer	O
cell	O
lines	O
are	O
spread	O
out	O
over	O
three	O
different	O
clusters	O
we	O
can	O
plot	B
the	O
cut	O
on	O
the	O
dendrogram	B
that	O
produces	O
these	O
four	O
clusters	O
par	O
mfrow	O
c	O
plot	B
hc	O
out	O
labels	O
nci	O
labs	O
abline	O
h	O
col	O
red	O
the	O
abline	O
function	B
draws	O
a	O
straight	O
line	B
on	O
top	O
of	O
any	O
existing	O
plot	B
in	O
r	O
the	O
argument	B
plots	O
a	O
horizontal	O
line	B
at	O
height	O
on	O
the	O
dendrogram	B
this	O
is	O
the	O
height	O
that	O
results	O
in	O
four	O
distinct	O
clusters	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
resulting	O
clusters	O
are	O
the	O
same	O
as	O
the	O
ones	O
we	O
obtained	O
using	O
printing	O
the	O
output	B
of	O
hclust	O
gives	O
a	O
useful	O
brief	O
summary	O
of	O
the	O
object	O
hc	O
out	O
call	O
hclust	O
d	O
dist	O
dat	O
cluster	O
method	O
distance	O
number	O
of	O
objects	O
complete	B
euclidean	B
we	O
claimed	O
earlier	O
in	O
section	O
that	O
k-means	B
clustering	B
and	O
hierarchical	B
clustering	B
with	O
the	O
dendrogram	B
cut	O
to	O
obtain	O
the	O
same	O
number	O
of	O
clusters	O
can	O
yield	O
very	O
different	O
results	O
how	O
do	O
these	O
hierarchical	B
clustering	B
results	O
compare	O
to	O
what	O
we	O
get	O
if	O
we	O
perform	O
k-means	B
clustering	B
with	O
k	O
set	B
seed	B
km	O
out	O
kmeans	O
sd	O
data	B
nstart	O
km	O
clusters	O
km	O
outcluste	O
r	O
table	O
km	O
clusters	O
hc	O
clusters	O
hc	O
clusters	O
km	O
clusters	O
we	O
see	O
that	O
the	O
four	O
clusters	O
obtained	O
using	O
hierarchical	B
clustering	B
and	O
kmeans	O
clustering	B
are	O
somewhat	O
different	O
cluster	O
in	O
k-means	B
clustering	B
is	O
identical	O
to	O
cluster	O
in	O
hierarchical	B
clustering	B
however	O
the	O
other	O
clusters	O
exercises	O
differ	O
for	O
instance	O
cluster	O
in	O
k-means	B
clustering	B
contains	O
a	O
portion	O
of	O
the	O
observations	B
assigned	O
to	O
cluster	O
by	O
hierarchical	B
clustering	B
as	O
well	O
as	O
all	O
of	O
the	O
observations	B
assigned	O
to	O
cluster	O
by	O
hierarchical	B
clustering	B
rather	O
than	O
performing	O
hierarchical	B
clustering	B
on	O
the	O
entire	O
data	B
matrix	O
we	O
can	O
simply	O
perform	O
hierarchical	B
clustering	B
on	O
the	O
first	O
few	O
principal	O
component	O
score	O
vectors	O
as	O
follows	O
hc	O
out	O
hclust	O
dist	O
pr	O
outx	O
plot	B
hc	O
out	O
labels	O
nci	O
labs	O
main	O
hier	O
clust	O
on	O
first	O
five	O
score	O
vectors	O
table	O
cutree	O
hc	O
out	O
nci	O
labs	O
not	O
surprisingly	O
these	O
results	O
are	O
different	O
from	O
the	O
ones	O
that	O
we	O
obtained	O
when	O
we	O
performed	O
hierarchical	B
clustering	B
on	O
the	O
full	O
data	B
set	B
sometimes	O
performing	O
clustering	B
on	O
the	O
first	O
few	O
principal	O
component	O
score	O
vectors	O
can	O
give	O
better	O
results	O
than	O
performing	O
clustering	B
on	O
the	O
full	O
data	B
in	O
this	O
situation	O
we	O
might	O
view	O
the	O
principal	O
component	O
step	O
as	O
one	O
of	O
denoising	O
the	O
data	B
we	O
could	O
also	O
perform	O
k-means	B
clustering	B
on	O
the	O
first	O
few	O
principal	O
component	O
score	O
vectors	O
rather	O
than	O
the	O
full	O
data	B
set	B
exercises	O
conceptual	O
this	O
problem	O
involves	O
the	O
k-means	B
clustering	B
algorithm	O
prove	O
on	O
the	O
basis	B
of	O
this	O
identity	O
argue	O
that	O
the	O
k-means	B
clustering	B
algorithm	O
decreases	O
the	O
objective	O
at	O
each	O
iteration	O
suppose	O
that	O
we	O
have	O
four	O
observations	B
for	O
which	O
we	O
compute	O
a	O
dissimilarity	B
matrix	O
given	O
by	O
for	O
instance	O
the	O
dissimilarity	B
between	O
the	O
first	O
and	O
second	O
observations	B
is	O
and	O
the	O
dissimilarity	B
between	O
the	O
second	O
and	O
fourth	O
observations	B
is	O
on	O
the	O
basis	B
of	O
this	O
dissimilarity	B
matrix	O
sketch	O
the	O
dendrogram	B
that	O
results	O
from	O
hierarchically	O
clustering	B
these	O
four	O
observations	B
using	O
complete	B
linkage	B
be	O
sure	O
to	O
indicate	O
on	O
the	O
plot	B
the	O
height	O
at	O
which	O
each	O
fusion	O
occurs	O
as	O
well	O
as	O
the	O
observations	B
corresponding	O
to	O
each	O
leaf	B
in	O
the	O
dendrogram	B
unsupervised	B
learning	I
repeat	O
this	O
time	O
using	O
single	B
linkage	B
clustering	B
suppose	O
that	O
we	O
cut	O
the	O
dendogram	O
obtained	O
in	O
such	O
that	O
two	O
clusters	O
result	O
which	O
observations	B
are	O
in	O
each	O
cluster	O
suppose	O
that	O
we	O
cut	O
the	O
dendogram	O
obtained	O
in	O
such	O
that	O
two	O
clusters	O
result	O
which	O
observations	B
are	O
in	O
each	O
cluster	O
it	O
is	O
mentioned	O
in	O
the	O
chapter	O
that	O
at	O
each	O
fusion	O
in	O
the	O
dendrogram	B
the	O
position	O
of	O
the	O
two	O
clusters	O
being	O
fused	O
can	O
be	O
swapped	O
without	O
changing	O
the	O
meaning	O
of	O
the	O
dendrogram	B
draw	O
a	O
dendrogram	B
that	O
is	O
equivalent	O
to	O
the	O
dendrogram	B
in	O
for	O
which	O
two	O
or	O
more	O
of	O
the	O
leaves	O
are	O
repositioned	O
but	O
for	O
which	O
the	O
meaning	O
of	O
the	O
dendrogram	B
is	O
the	O
same	O
in	O
this	O
problem	O
you	O
will	O
perform	O
k-means	B
clustering	B
manually	O
with	O
k	O
on	O
a	O
small	O
example	O
with	O
n	O
observations	B
and	O
p	O
features	O
the	O
observations	B
are	O
as	O
follows	O
obs	O
plot	B
the	O
observations	B
randomly	O
assign	O
a	O
cluster	O
label	O
to	O
each	O
observation	O
you	O
can	O
use	O
the	O
sample	O
command	O
in	O
r	O
to	O
do	O
this	O
report	O
the	O
cluster	O
labels	O
for	O
each	O
observation	O
compute	O
the	O
centroid	B
for	O
each	O
cluster	O
assign	O
each	O
observation	O
to	O
the	O
centroid	B
to	O
which	O
it	O
is	O
closest	O
in	O
terms	O
of	O
euclidean	B
distance	I
report	O
the	O
cluster	O
labels	O
for	O
each	O
observation	O
repeat	O
and	O
until	O
the	O
answers	O
obtained	O
stop	O
changing	O
in	O
your	O
plot	B
from	O
color	O
the	O
observations	B
according	O
to	O
the	O
cluster	O
labels	O
obtained	O
suppose	O
that	O
for	O
a	O
particular	O
data	B
set	B
we	O
perform	O
hierarchical	B
clustering	B
using	O
single	B
linkage	B
and	O
using	O
complete	B
linkage	B
we	O
obtain	O
two	O
dendrograms	O
at	O
a	O
certain	O
point	O
on	O
the	O
single	B
linkage	B
dendrogram	B
the	O
clusters	O
and	O
fuse	O
on	O
the	O
complete	B
linkage	B
dendrogram	B
the	O
clusters	O
and	O
also	O
fuse	O
at	O
a	O
certain	O
point	O
which	O
fusion	O
will	O
occur	O
higher	O
on	O
the	O
tree	B
or	O
will	O
they	O
fuse	O
at	O
the	O
same	O
height	O
or	O
is	O
there	O
not	O
enough	O
information	O
to	O
tell	O
exercises	O
at	O
a	O
certain	O
point	O
on	O
the	O
single	B
linkage	B
dendrogram	B
the	O
clusters	O
and	O
fuse	O
on	O
the	O
complete	B
linkage	B
dendrogram	B
the	O
clusters	O
and	O
also	O
fuse	O
at	O
a	O
certain	O
point	O
which	O
fusion	O
will	O
occur	O
higher	O
on	O
the	O
tree	B
or	O
will	O
they	O
fuse	O
at	O
the	O
same	O
height	O
or	O
is	O
there	O
not	O
enough	O
information	O
to	O
tell	O
in	O
words	O
describe	O
the	O
results	O
that	O
you	O
would	O
expect	O
if	O
you	O
performed	O
k-means	B
clustering	B
of	O
the	O
eight	O
shoppers	O
in	O
figure	O
on	O
the	O
basis	B
of	O
their	O
sock	O
and	O
computer	O
purchases	O
with	O
k	O
give	O
three	O
answers	O
one	O
for	O
each	O
of	O
the	O
variable	B
scalings	O
displayed	O
explain	O
a	O
researcher	O
collects	O
expression	O
measurements	O
for	O
genes	O
in	O
tissue	O
samples	O
the	O
data	B
can	O
be	O
written	O
as	O
a	O
matrix	O
which	O
we	O
call	O
x	O
in	O
which	O
each	O
row	O
represents	O
a	O
gene	O
and	O
each	O
column	O
a	O
tissue	O
sample	O
each	O
tissue	O
sample	O
was	O
processed	O
on	O
a	O
different	O
day	O
and	O
the	O
columns	O
of	O
x	O
are	O
ordered	O
so	O
that	O
the	O
samples	O
that	O
were	O
processed	O
earliest	O
are	O
on	O
the	O
left	O
and	O
the	O
samples	O
that	O
were	O
processed	O
later	O
are	O
on	O
the	O
right	O
the	O
tissue	O
samples	O
belong	O
to	O
two	O
groups	O
control	O
and	O
treatment	O
the	O
c	O
and	O
t	O
samples	O
were	O
processed	O
in	O
a	O
random	O
order	O
across	O
the	O
days	O
the	O
researcher	O
wishes	O
to	O
determine	O
whether	O
each	O
gene	O
s	O
expression	O
measurements	O
differ	O
between	O
the	O
treatment	O
and	O
control	O
groups	O
as	O
a	O
pre-analysis	O
comparing	O
t	O
versus	O
c	O
the	O
researcher	O
performs	O
a	O
principal	O
component	O
analysis	B
of	O
the	O
data	B
and	O
finds	O
that	O
the	O
first	O
principal	O
component	O
vector	B
of	O
length	O
has	O
a	O
strong	O
linear	B
trend	O
from	O
left	O
to	O
right	O
and	O
explains	O
of	O
the	O
variation	O
the	O
researcher	O
now	O
remembers	O
that	O
each	O
patient	O
sample	O
was	O
run	O
on	O
one	O
of	O
two	O
machines	O
a	O
and	O
b	O
and	O
machine	B
a	O
was	O
used	O
more	O
often	O
in	O
the	O
earlier	O
times	O
while	O
b	O
was	O
used	O
more	O
often	O
later	O
the	O
researcher	O
has	O
a	O
record	O
of	O
which	O
sample	O
was	O
run	O
on	O
which	O
machine	B
explain	O
what	O
it	O
means	O
that	O
the	O
first	O
principal	O
component	O
ex	O
plains	O
of	O
the	O
variation	O
the	O
researcher	O
decides	O
to	O
replace	O
the	O
ith	O
element	O
of	O
x	O
with	O
xji	O
where	O
is	O
the	O
ith	O
score	O
and	O
is	O
the	O
jth	O
loading	O
for	O
the	O
first	O
principal	O
component	O
he	O
will	O
then	O
perform	O
a	O
two-sample	O
t-test	O
on	O
each	O
gene	O
in	O
this	O
new	O
data	B
set	B
in	O
order	O
to	O
determine	O
whether	O
its	O
expression	O
differs	O
between	O
the	O
two	O
conditions	O
critique	O
this	O
idea	O
and	O
suggest	O
a	O
better	O
approach	B
principal	O
component	O
analysis	B
is	O
performed	O
on	O
xt	O
design	O
and	O
run	O
a	O
small	O
simulation	O
experiment	O
to	O
demonstrate	O
the	O
superiority	O
of	O
your	O
idea	O
unsupervised	B
learning	I
applied	O
in	O
the	O
chapter	O
we	O
mentioned	O
the	O
use	O
of	O
correlation-based	B
distance	O
and	O
euclidean	B
distance	I
as	O
dissimilarity	B
measures	O
for	O
hierarchical	B
clustering	B
it	O
turns	O
out	O
that	O
these	O
two	O
measures	O
are	O
almost	O
equivalent	O
if	O
each	O
observation	O
has	O
been	O
centered	O
to	O
have	O
mean	O
zero	O
and	O
standard	O
deviation	O
one	O
and	O
if	O
we	O
let	O
rij	O
denote	O
the	O
correlation	B
between	O
the	O
ith	O
and	O
jth	O
observations	B
then	O
the	O
quantity	O
rij	O
is	O
proportional	O
to	O
the	O
squared	O
euclidean	B
distance	I
between	O
the	O
ith	O
and	O
jth	O
observations	B
on	O
the	O
usarrests	B
data	B
show	O
that	O
this	O
proportionality	O
holds	O
hint	O
the	O
euclidean	B
distance	I
can	O
be	O
calculated	O
using	O
the	O
dist	O
function	B
and	O
correlations	O
can	O
be	O
calculated	O
using	O
the	O
cor	O
function	B
in	O
section	O
a	O
formula	O
for	O
calculating	O
pve	O
was	O
given	O
in	O
equation	O
we	O
also	O
saw	O
that	O
the	O
pve	O
can	O
be	O
obtained	O
using	O
the	O
sdev	O
output	B
of	O
the	O
prcomp	O
function	B
on	O
the	O
usarrests	B
data	B
calculate	O
pve	O
in	O
two	O
ways	O
using	O
the	O
sdev	O
output	B
of	O
the	O
prcomp	O
function	B
as	O
was	O
done	O
in	O
section	O
by	O
applying	O
equation	O
directly	O
that	O
is	O
use	O
the	O
prcomp	O
function	B
to	O
compute	O
the	O
principal	O
component	O
loadings	O
then	O
use	O
those	O
loadings	O
in	O
equation	O
to	O
obtain	O
the	O
pve	O
these	O
two	O
approaches	O
should	O
give	O
the	O
same	O
results	O
hint	O
you	O
will	O
only	O
obtain	O
the	O
same	O
results	O
in	O
and	O
if	O
the	O
same	O
data	B
is	O
used	O
in	O
both	O
cases	O
for	O
instance	O
if	O
in	O
you	O
performed	O
prcomp	O
using	O
centered	O
and	O
scaled	O
variables	O
then	O
you	O
must	O
center	O
and	O
scale	O
the	O
variables	O
before	O
applying	O
equation	O
in	O
consider	O
the	O
usarrests	B
data	B
we	O
will	O
now	O
perform	O
hierarchical	B
clus	O
tering	O
on	O
the	O
states	O
using	O
hierarchical	B
clustering	B
with	O
complete	B
linkage	B
and	O
euclidean	B
distance	I
cluster	O
the	O
states	O
cut	O
the	O
dendrogram	B
at	O
a	O
height	O
that	O
results	O
in	O
three	O
distinct	O
clusters	O
which	O
states	O
belong	O
to	O
which	O
clusters	O
hierarchically	O
cluster	O
the	O
states	O
using	O
complete	B
linkage	B
and	O
euclidean	B
distance	I
after	O
scaling	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
what	O
effect	O
does	O
scaling	O
the	O
variables	O
have	O
on	O
the	O
hierarchical	B
clustering	B
obtained	O
in	O
your	O
opinion	O
should	O
the	O
variables	O
be	O
scaled	O
before	O
the	O
inter-observation	O
dissimilarities	O
are	O
computed	O
provide	O
a	O
justification	O
for	O
your	O
answer	O
in	O
this	O
problem	O
you	O
will	O
generate	O
simulated	O
data	B
and	O
then	O
perform	O
pca	O
and	O
k-means	B
clustering	B
on	O
the	O
data	B
exercises	O
generate	O
a	O
simulated	O
data	B
set	B
with	O
observations	B
in	O
each	O
of	O
three	O
classes	O
observations	B
total	O
and	O
variables	O
hint	O
there	O
are	O
a	O
number	O
of	O
functions	O
in	O
r	O
that	O
you	O
can	O
use	O
to	O
generate	O
data	B
one	O
example	O
is	O
the	O
rnorm	O
function	B
runif	O
is	O
another	O
option	O
be	O
sure	O
to	O
add	O
a	O
mean	O
shift	O
to	O
the	O
observations	B
in	O
each	O
class	O
so	O
that	O
there	O
are	O
three	O
distinct	O
classes	O
perform	O
pca	O
on	O
the	O
observations	B
and	O
plot	B
the	O
first	O
two	O
principal	O
component	O
score	O
vectors	O
use	O
a	O
different	O
color	O
to	O
indicate	O
the	O
observations	B
in	O
each	O
of	O
the	O
three	O
classes	O
if	O
the	O
three	O
classes	O
appear	O
separated	O
in	O
this	O
plot	B
then	O
continue	O
on	O
to	O
part	O
if	O
not	O
then	O
return	O
to	O
part	O
and	O
modify	O
the	O
simulation	O
so	O
that	O
there	O
is	O
greater	O
separation	O
between	O
the	O
three	O
classes	O
do	O
not	O
continue	O
to	O
part	O
until	O
the	O
three	O
classes	O
show	O
at	O
least	O
some	O
separation	O
in	O
the	O
first	O
two	O
principal	O
component	O
score	O
vectors	O
perform	O
k-means	B
clustering	B
of	O
the	O
observations	B
with	O
k	O
how	O
well	O
do	O
the	O
clusters	O
that	O
you	O
obtained	O
in	O
k-means	B
clustering	B
compare	O
to	O
the	O
true	O
class	O
labels	O
hint	O
you	O
can	O
use	O
the	O
table	O
function	B
in	O
r	O
to	O
compare	O
the	O
true	O
class	O
labels	O
to	O
the	O
class	O
labels	O
obtained	O
by	O
clustering	B
be	O
careful	O
how	O
you	O
interpret	O
the	O
results	O
k-means	B
clustering	B
will	O
arbitrarily	O
number	O
the	O
clusters	O
so	O
you	O
cannot	O
simply	O
check	O
whether	O
the	O
true	O
class	O
labels	O
and	O
clustering	B
labels	O
are	O
the	O
same	O
perform	O
k-means	B
clustering	B
with	O
k	O
describe	O
your	O
results	O
now	O
perform	O
k-means	B
clustering	B
with	O
k	O
and	O
describe	O
your	O
results	O
now	O
perform	O
k-means	B
clustering	B
with	O
k	O
on	O
the	O
first	O
two	O
principal	O
component	O
score	O
vectors	O
rather	O
than	O
on	O
the	O
raw	O
data	B
that	O
is	O
perform	O
k-means	B
clustering	B
on	O
the	O
matrix	O
of	O
which	O
the	O
first	O
column	O
is	O
the	O
first	O
principal	O
component	O
score	B
vector	B
and	O
the	O
second	O
column	O
is	O
the	O
second	O
principal	O
component	O
score	B
vector	B
comment	O
on	O
the	O
results	O
using	O
the	O
scale	O
function	B
perform	O
k-means	B
clustering	B
with	O
k	O
on	O
the	O
data	B
after	O
scaling	O
each	O
variable	B
to	O
have	O
standard	O
deviation	O
one	O
how	O
do	O
these	O
results	O
compare	O
to	O
those	O
obtained	O
in	O
explain	O
on	O
the	O
book	O
website	O
www	O
statlearning	O
com	O
there	O
is	O
a	O
gene	O
expression	O
data	B
set	B
that	O
consists	O
of	O
tissue	O
samples	O
with	O
measurements	O
on	O
genes	O
the	O
first	O
samples	O
are	O
from	O
healthy	O
patients	O
while	O
the	O
second	O
are	O
from	O
a	O
diseased	O
group	O
unsupervised	B
learning	I
load	O
in	O
the	O
data	B
using	O
read	O
csv	O
you	O
will	O
need	O
to	O
select	O
headerf	O
apply	O
hierarchical	B
clustering	B
to	O
the	O
samples	O
using	O
correlationbased	O
distance	O
and	O
plot	B
the	O
dendrogram	B
do	O
the	O
genes	O
separate	O
the	O
samples	O
into	O
the	O
two	O
groups	O
do	O
your	O
results	O
depend	O
on	O
the	O
type	O
of	O
linkage	B
used	O
your	O
collaborator	O
wants	O
to	O
know	O
which	O
genes	O
differ	O
the	O
most	O
across	O
the	O
two	O
groups	O
suggest	O
a	O
way	O
to	O
answer	O
this	O
question	O
and	O
apply	O
it	O
here	O
index	O
cp	B
norm	O
norm	O
additive	B
additivity	B
adjusted	O
advertising	B
data	B
set	B
agglomerative	B
clustering	B
akaike	B
information	I
criterion	I
alternative	B
hypothesis	B
analysis	B
of	I
variance	B
area	B
under	I
the	I
curve	I
argument	B
auc	B
auto	B
data	B
set	B
backfitting	B
backward	B
stepwise	I
selection	B
bagging	B
baseline	B
basis	B
function	B
bayes	O
classifier	B
decision	B
boundary	I
error	B
bayes	O
theorem	O
bayesian	B
bayesian	B
information	O
criterion	O
best	B
subset	B
selection	B
bias	B
bias-variance	O
decomposition	B
trade-off	B
binary	B
biplot	B
g	O
james	O
et	O
al	O
an	O
introduction	O
to	O
statistical	O
learning	O
with	O
applications	O
in	O
r	O
springer	B
texts	I
in	I
statistics	I
doi	O
springer	O
sciencebusiness	O
media	O
new	O
york	O
index	O
boolean	B
boosting	B
bootstrap	B
cross-validation	B
k-fold	B
leave-one-out	B
boston	B
data	B
set	B
curse	B
of	I
dimensionality	I
bottom-up	B
clustering	B
boxplot	B
branch	B
caravan	B
data	B
set	B
carseats	B
data	B
set	B
categorical	B
classification	B
error	B
rate	B
tree	B
classifier	B
cluster	B
analysis	B
clustering	B
k-means	B
agglomerative	B
bottom-up	B
hierarchical	B
coefficient	O
college	B
data	B
set	B
collinearity	B
conditional	B
probability	B
confidence	B
interval	B
confounding	B
confusion	B
matrix	I
continuous	B
contour	B
plot	B
contrast	B
correlation	B
credit	B
data	B
set	B
data	B
frame	I
data	B
sets	O
advertising	B
auto	B
boston	B
caravan	B
carseats	B
college	B
credit	B
default	B
heart	B
hitters	B
income	B
khan	B
oj	B
portfolio	B
smarket	B
usarrests	B
wage	B
weekly	B
decision	B
tree	B
default	B
data	B
set	B
degrees	B
of	I
freedom	I
dendrogram	B
density	B
function	B
dependent	B
variable	B
derivative	B
deviance	B
dimension	B
reduction	I
discriminant	B
function	B
dissimilarity	B
distance	O
correlation-based	B
euclidean	B
double-exponential	O
distribution	B
dummy	B
variable	B
effective	B
degrees	B
of	I
freedom	I
elbow	B
entropy	B
error	B
irreducible	B
rate	B
reducible	B
term	B
euclidean	B
distance	I
expected	B
value	I
exploratory	B
data	B
analysis	B
f-statistic	B
factor	B
false	B
discovery	I
proportion	I
index	O
false	B
negative	I
false	B
positive	I
false	B
positive	I
rate	B
feature	B
feature	B
selection	B
fisher	O
s	O
linear	B
discriminant	O
fit	O
fitted	O
value	O
flexible	O
for	B
loop	I
forward	B
stepwise	I
selection	B
function	B
gaussian	O
distribution	B
generalized	B
additive	B
model	B
generalized	B
linear	B
model	B
gini	B
index	I
heart	B
data	B
set	B
heatmap	B
heteroscedasticity	B
hierarchical	B
clustering	B
dendrogram	B
inversion	B
linkage	B
hierarchical	B
principle	I
high-dimensional	B
hinge	B
loss	I
histogram	B
hitters	B
data	B
set	B
hold-out	B
set	B
hyperplane	B
hypothesis	B
test	I
income	B
data	B
set	B
independent	B
variable	B
indicator	B
function	B
inference	B
index	O
inner	B
product	I
input	B
variable	B
integral	B
interaction	B
intercept	B
interpretability	B
inversion	B
irreducible	B
error	B
k-means	B
clustering	B
k-nearest	O
neighbors	O
classifier	B
regression	B
kernel	B
linear	B
non-linear	B
polynomial	B
radial	B
khan	B
data	B
set	B
knot	B
laplace	B
distribution	B
lasso	B
leaf	B
least	B
squares	I
line	B
weighted	B
level	B
leverage	B
likelihood	B
function	B
linear	B
linear	B
combination	I
linear	B
discriminant	I
analysis	B
linear	B
kernel	B
linear	B
model	B
linear	B
regression	B
multiple	B
simple	B
linkage	B
average	B
centroid	B
complete	B
single	B
local	B
regression	B
logistic	O
function	B
logistic	B
regression	B
multiple	B
logit	B
loss	B
function	B
low-dimensional	B
main	B
effects	I
majority	B
vote	I
mallow	O
s	O
cp	B
margin	B
matrix	B
multiplication	I
maximal	O
margin	B
classifier	B
hyperplane	B
maximum	B
likelihood	I
mean	B
squared	I
error	B
misclassification	B
error	B
missing	B
data	B
mixed	B
selection	B
model	B
assessment	I
model	B
selection	B
multicollinearity	B
multivariate	B
gaussian	I
multivariate	B
normal	I
natural	B
spline	B
data	B
set	B
negative	B
predictive	I
value	I
node	O
internal	B
purity	B
terminal	B
noise	B
non-linear	B
decision	B
boundary	I
kernel	B
non-parametric	B
normal	O
distribution	B
null	B
hypothesis	B
model	B
odds	B
oj	B
data	B
set	B
one-standard-error	B
rule	I
one-versus-all	B
one-versus-one	B
optimal	O
separating	B
hyperplane	B
optimism	B
of	I
training	I
error	B
ordered	B
categorical	B
variable	B
orthogonal	B
basis	B
out-of-bag	B
outlier	B
output	B
variable	B
overfitting	B
p-value	B
parameter	B
parametric	B
partial	B
least	B
squares	I
path	B
algorithm	I
perpendicular	B
polynomial	B
kernel	B
regression	B
population	B
regression	B
line	B
portfolio	B
data	B
set	B
positive	B
predictive	I
value	I
index	O
posterior	O
distribution	B
mode	B
probability	B
power	B
precision	B
prediction	B
interval	B
predictor	B
principal	B
components	I
analysis	B
loading	B
vector	B
proportion	O
of	O
variance	B
explained	B
regression	B
score	B
vector	B
scree	B
plot	B
prior	O
distribution	B
probability	B
projection	B
pruning	B
cost	B
complexity	I
weakest	B
link	I
quadratic	B
quadratic	B
discriminant	O
analysis	B
qualitative	B
variable	B
quantitative	B
r	O
functions	O
abline	O
anova	O
apply	O
as	O
dist	O
as	O
factor	B
attach	O
biplot	B
boot	O
index	O
bs	O
c	O
cbind	O
coef	O
confint	O
contour	O
contrasts	O
cor	O
cumsum	O
cut	O
cutree	O
cv	O
glm	O
cv	O
glmnet	O
cv	O
tree	B
data	B
frame	I
dev	O
off	O
dim	O
dist	O
fix	O
for	O
gam	O
gbm	O
glm	O
glmnet	O
hatvalues	O
hclust	O
hist	O
i	O
identify	O
ifelse	O
image	O
importance	B
is	O
na	O
jitter	O
jpeg	O
kmeans	O
knn	O
lda	O
legend	O
length	O
library	O
lines	O
lm	O
lo	O
loadhistory	O
loess	O
ls	O
matrix	O
mean	O
median	O
model	B
matrix	O
na	O
omit	O
names	O
ns	O
pairs	O
par	O
pcr	O
pdf	O
persp	O
plot	B
plot	B
gam	O
plot	B
svm	O
plsr	O
points	O
poly	O
prcomp	O
predict	O
print	O
prune	O
misclass	O
prune	O
tree	B
q	O
qda	O
quantile	O
rainbow	O
randomforest	O
range	O
read	O
csv	O
read	O
table	O
regsubsets	O
residuals	B
return	O
rm	O
rnorm	O
rstudent	O
runif	O
s	O
sample	O
savehistory	O
scale	O
sd	O
seq	O
set	B
seed	B
smooth	O
spline	B
sqrt	O
sum	O
summary	O
svm	O
table	O
text	O
title	O
tree	B
tune	O
update	O
var	O
varimpplot	O
vif	O
which	O
max	O
which	O
min	O
write	O
table	O
radial	B
kernel	B
random	B
forest	I
recall	B
receiver	O
operating	O
characteristic	O
recursive	B
binary	B
splitting	I
index	O
reducible	B
error	B
regression	B
local	B
piecewise	B
polynomial	B
polynomial	B
spline	B
tree	B
regularization	B
replacement	B
resampling	B
residual	B
plot	B
standard	B
error	B
studentized	B
sum	B
of	I
squares	I
residuals	B
response	B
ridge	B
regression	B
robust	B
roc	B
curve	I
rug	B
plot	B
scale	B
equivariant	I
scatterplot	B
scatterplot	B
matrix	I
scree	B
plot	B
elbow	B
seed	B
semi-supervised	B
learning	I
sensitivity	B
separating	B
hyperplane	B
shrinkage	B
penalty	B
signal	B
slack	B
variable	B
slope	B
smarket	B
data	B
set	B
smoother	B
smoothing	B
spline	B
soft	B
margin	B
classifier	B
index	O
soft-thresholding	B
sparse	B
sparsity	B
specificity	B
spline	B
cubic	B
linear	B
natural	B
regression	B
smoothing	B
thin-plate	B
standard	B
error	B
standardize	B
statistical	B
model	B
step	B
function	B
stepwise	B
model	B
selection	B
stump	B
subset	B
selection	B
subtree	B
supervised	B
learning	I
support	B
vector	B
classifier	B
machine	B
regression	B
synergy	B
systematic	B
t-distribution	B
t-statistic	B
test	O
error	B
mse	B
observations	B
set	B
time	B
series	I
total	B
sum	B
of	I
squares	I
tracking	B
train	B
training	O
data	B
error	B
mse	B
tree	B
tree-based	B
method	I
true	B
negative	I
true	B
positive	I
true	B
positive	I
rate	B
truncated	B
power	B
basis	B
tuning	B
parameter	B
type	B
i	I
error	B
type	B
ii	I
error	B
unsupervised	B
learning	I
usarrests	B
data	B
set	B
validation	B
set	B
approach	B
variable	B
dependent	B
dummy	B
importance	B
independent	B
indicator	B
input	B
output	B
qualitative	B
selection	B
variance	B
inflation	O
factor	B
varying	O
coefficient	O
model	B
vector	B
wage	B
data	B
set	B
weakest	B
link	I
pruning	B
weekly	B
data	B
set	B
weighted	B
least	B
squares	I
within	B
class	I
covariance	I
workspace	B
wrapper	B
