stochastic	O
mechanics	O
applications	O
of	O
random	O
media	O
mathematics	O
signal	O
processing	O
stochastic	O
modelling	O
and	O
image	O
synthesis	O
and	O
applied	O
probability	O
mathematical	O
economics	O
stochastic	O
optimization	O
stochastic	O
control	O
edited	B
by	O
i	O
karatzas	O
m	O
yor	O
advisory	O
board	O
p	O
bremaud	O
e	O
carlen	O
r	O
dobrushin	O
w	O
fleming	O
d	O
geman	O
g	O
grimmett	O
g	O
papanicolaou	O
j	O
scheinkman	O
springer	O
new	O
york	O
berlin	O
heidelberg	O
barcelona	O
budapest	O
hong	O
kong	O
london	O
milan	O
paris	O
santa	O
clara	O
singapore	O
tokyo	O
applications	O
of	O
mathematics	O
flemingrishel	O
deterministic	O
and	O
stochastic	O
optimal	O
control	O
marchuk	O
methods	O
of	O
numerical	O
mathematics	O
second	O
ed	O
balakrishnan	O
applied	O
functional	O
analysis	O
second	O
ed	O
borovkov	O
stochastic	O
processes	O
in	O
queueing	O
theory	O
liptserlshiryayev	O
statistics	O
of	O
random	O
processes	O
i	O
general	O
theory	O
liptserlshiryayev	O
statistics	O
of	O
random	O
processes	O
ii	O
applications	O
vorobev	O
game	O
theory	O
lectures	O
for	O
economists	O
and	O
systems	O
scientists	O
shiryayev	O
optimal	O
stopping	O
rules	O
ibragimovrozanov	O
gaussian	B
random	O
processes	O
wonham	O
linear	O
multivariable	O
control	O
a	O
geometric	B
approach	O
third	O
ed	O
hida	O
brownian	O
motion	O
hestenes	O
conjugate	O
direction	O
methods	O
in	O
optimization	O
kallianpur	O
stochastic	O
filtering	O
theory	O
krylov	O
controlled	O
diffusion	O
processes	O
prabhu	O
stochastic	O
storage	O
processes	O
queues	O
insurance	O
risk	O
and	O
dams	O
ibragimovhasminskii	O
statistical	O
estimation	B
asymptotic	O
theory	O
cesari	O
optimization	O
theory	O
and	O
applications	O
elliott	O
stochastic	O
calculus	O
and	O
applications	O
marchukshaidourov	O
difference	O
methods	O
and	O
their	O
extrapolations	O
hijab	O
stabilization	O
of	O
control	O
systems	O
protter	O
stochastic	O
integration	O
and	O
differential	O
equations	O
benvenistemetivierpriouret	O
adaptive	O
algorithms	O
and	O
stochastic	O
approximations	O
kloedeniplaten	O
numerical	O
solution	O
of	O
stochastic	O
differential	O
equations	O
kushnerdupuis	O
numerical	O
methods	O
for	O
stochastic	O
control	O
problems	O
in	O
continuous	O
time	O
flemingsoner	O
controlled	O
markov	O
processes	O
and	O
viscosity	O
solutions	O
baccellilbremaud	O
elements	O
of	O
queueing	O
theory	O
winkler	O
image	O
analysis	O
random	O
fields	O
and	O
dynamic	O
monte	O
carlo	O
methods	O
an	O
introduction	O
to	O
mathematical	O
aspects	O
kalpazidou	O
cycle	O
representations	O
of	O
markov	O
processes	O
elliott	O
aggounimoore	O
hidden	O
markov	O
models	O
estimation	B
and	O
control	O
hernandez-lermailasserre	O
discrete-time	O
markov	O
control	O
processes	O
basic	O
optimality	O
criteria	O
devroyegyorfllugosi	O
a	O
probabilistic	O
theory	O
of	O
pattern	O
recognition	O
maitraisudderth	O
discrete	O
gambling	O
and	O
stochastic	O
games	O
luc	O
devroye	O
laszlo	O
gyorfi	O
gabor	O
lugosi	O
a	O
probabilistic	O
theory	O
of	O
pattern	O
recognition	O
with	O
figures	O
springer	O
gyorfi	O
gabor	O
lugosi	O
department	O
of	O
mathematics	O
and	O
computer	O
science	O
technical	O
university	O
of	O
budapest	O
budapest	O
hungary	O
luc	O
devroye	O
school	O
of	O
computer	O
science	O
mcgill	O
university	O
montreal	O
quebec	O
canada	O
managing	O
editors	O
i	O
karatzas	O
department	O
of	O
statistics	O
columbia	O
university	O
new	O
york	O
ny	O
usa	O
m	O
yor	O
cnrs	O
laboratoire	O
de	O
probabilites	O
universite	O
pierre	O
et	O
marie	O
curie	O
place	O
jussieu	O
tour	O
paris	O
cedex	O
os	O
france	O
mathematics	O
subject	O
classification	O
library	O
of	O
congress	O
cataloging-in-publication	O
data	O
devroye	O
luc	O
a	O
probabilistic	O
theory	O
of	O
pattern	O
recognitionluc	O
devroye	O
laszlo	O
gyorfi	O
gabor	O
lugosi	O
p	O
cm	O
includes	O
bibliographical	O
references	O
and	O
index	O
isbn	O
pattern	O
perception	O
probabilities	O
gyorfi	O
laszlo	O
ii	O
lugosi	O
gabor	O
iii	O
title	O
printed	O
on	O
acid-free	O
paper	O
springer-verlag	O
new	O
york	O
inc	O
all	O
rights	O
reserved	O
this	O
work	O
may	O
not	O
be	O
translated	O
or	O
copied	O
in	O
whole	O
or	O
in	O
part	O
without	O
the	O
written	O
permission	O
of	O
the	O
publisher	O
new	O
york	O
inc	O
fifth	O
avenue	O
new	O
york	O
ny	O
usa	O
except	O
for	O
brief	O
excerpts	O
in	O
connection	O
with	O
reviews	O
or	O
scholarly	O
analysis	O
use	O
in	O
connection	O
with	O
any	O
form	O
of	O
information	O
storage	O
and	O
retrieval	O
electronic	O
adaptation	O
computer	O
software	O
or	O
by	O
similar	O
or	O
dissimilar	O
methodology	O
now	O
known	O
or	O
after	O
developed	O
is	O
forbidden	O
the	O
use	O
of	O
general	O
descriptive	O
names	O
trade	O
names	O
trademarks	O
etc	O
in	O
this	O
publication	O
even	O
if	O
the	O
former	O
are	O
not	O
especially	O
identified	O
is	O
not	O
to	O
be	O
taken	O
as	O
a	O
sign	O
that	O
such	O
names	O
as	O
understood	O
by	O
the	O
trade	O
marks	O
and	O
merchandise	O
marks	O
act	O
may	O
accordingly	O
be	O
used	O
freely	O
by	O
anyone	O
production	O
managed	O
by	O
francine	O
mcneill	O
manufacturing	O
supervised	B
by	O
jeffrey	O
taub	O
photocomposed	O
copy	O
prepared	O
using	O
springers	O
svsing	O
sty	O
macro	O
printed	O
and	O
bound	O
by	O
braun-brumfield	O
inc	O
ann	O
arbor	O
mi	O
printed	O
in	O
the	O
united	O
states	O
of	O
america	O
second	O
printing	O
isbn	O
springer-verlag	O
new	O
york	O
berlin	O
heidelberg	O
spin	O
preface	O
life	O
is	O
just	O
a	O
long	O
random	O
walk	O
things	O
are	O
created	O
because	O
the	O
circumstances	O
happen	O
to	O
be	O
right	O
more	O
often	O
than	O
not	O
creations	O
such	O
as	O
this	O
book	O
are	O
dental	O
nonparametric	O
estimation	B
came	O
to	O
life	O
in	O
the	O
fifties	O
and	O
sixties	O
and	O
started	O
developing	O
at	O
a	O
frenzied	O
pace	O
in	O
the	O
late	O
sixties	O
engulfing	O
pattern	O
recognition	O
in	O
its	O
growth	O
in	O
the	O
mid-sixties	O
two	O
young	O
men	O
tom	O
cover	O
and	O
peter	O
hart	O
showed	O
the	O
world	O
that	O
the	O
nearest	B
neighbor	I
rule	B
in	O
all	O
its	O
simplicity	O
was	O
guaranteed	O
to	O
err	O
at	O
most	O
twice	O
as	O
often	O
as	O
the	O
best	O
possible	O
discrimination	O
method	O
toms	O
results	O
had	O
a	O
profound	O
influence	O
on	O
terry	O
wagner	O
who	O
became	O
a	O
professor	O
at	O
the	O
university	O
of	O
texas	O
at	O
austin	O
and	O
brought	O
probabilistic	O
rigor	O
to	O
the	O
young	O
field	O
of	O
metric	O
estimation	B
around	O
vapnik	O
and	O
chervonenkis	O
started	O
publishing	O
a	O
revolutionary	O
series	O
of	O
papers	O
with	O
deep	B
implications	O
in	O
pattern	O
recognition	O
but	O
their	O
work	O
was	O
not	O
well	O
known	O
at	O
the	O
time	O
however	O
tom	O
and	O
terry	O
had	O
noticed	O
the	O
potential	O
of	O
the	O
work	O
and	O
terry	O
asked	O
luc	O
devroye	O
to	O
read	O
that	O
work	O
in	O
ration	O
for	O
his	O
ph	O
d	O
dissertation	O
at	O
the	O
university	O
of	O
texas	O
the	O
year	O
was	O
luc	O
ended	O
up	O
in	O
texas	O
quite	O
by	O
accident	O
thanks	O
to	O
a	O
tip	O
by	O
his	O
friend	O
and	O
fellow	O
belgian	O
willy	O
wouters	O
who	O
matched	O
him	O
up	O
with	O
terry	O
by	O
the	O
time	O
lucs	O
dissertation	O
was	O
published	O
in	O
pattern	O
recognition	O
had	O
taken	O
off	O
in	O
earnest	O
on	O
the	O
ical	O
side	O
important	O
properties	O
were	O
still	O
being	O
discovered	O
in	O
stone	O
stunned	O
the	O
nonparametric	O
community	O
by	O
showing	O
that	O
there	O
are	O
iionparametric	O
rules	O
that	O
are	O
convergent	O
for	O
all	O
distributions	O
of	O
the	O
data	O
this	O
is	O
called	O
distribution-free	O
or	O
universal	B
consistency	B
and	O
it	O
is	O
what	O
makes	O
nonparametric	O
methods	O
so	O
attractive	O
yet	O
very	O
few	O
researchers	O
were	O
concerned	O
with	O
universal	O
consistency-one	O
notable	O
exception	O
was	O
laci	O
gyorfi	O
who	O
at	O
that	O
time	O
worked	O
in	O
budapest	O
amid	O
an	O
energetic	O
group	O
of	O
nonparametric	O
specialists	O
that	O
included	O
sandor	O
csibi	O
fritz	O
and	O
pal	O
revesz	O
vi	O
preface	O
so	O
linked	O
by	O
a	O
common	O
vision	O
luc	O
and	O
laci	O
decided	O
to	O
join	O
forces	O
in	O
the	O
early	O
eighties	O
in	O
they	O
wrote	O
six	O
chapters	O
of	O
a	O
book	O
on	O
nonparametric	O
regression	B
function	I
estimation	B
but	O
these	O
were	O
never	O
published	O
in	O
fact	O
the	O
notes	O
are	O
still	O
in	O
drawers	O
in	O
their	O
offices	O
today	O
they	O
felt	O
that	O
the	O
subject	O
had	O
not	O
matured	O
yet	O
a	O
book	O
on	O
nonparametric	O
density	B
estimation	B
saw	O
the	O
light	O
in	O
unfortunately	O
as	O
true	O
baby-boomers	O
neither	O
luc	O
nor	O
laci	O
had	O
the	O
time	O
after	O
to	O
write	O
a	O
text	O
on	O
nonpararnetric	O
pattern	O
recognition	O
enter	O
gabor	O
lugosi	O
who	O
obtained	O
his	O
doctoral	O
degree	O
under	O
lacis	O
supervision	O
in	O
gabor	O
had	O
prepared	O
a	O
set	O
of	O
rough	O
course	O
notes	O
on	O
the	O
subject	O
around	O
and	O
proposed	O
to	O
coordinate	O
the	O
project-this	O
book-in	O
with	O
renewed	O
energy	O
we	O
set	O
out	O
to	O
write	O
the	O
book	O
that	O
we	O
should	O
have	O
written	O
at	O
least	O
ten	O
years	O
ago	O
discussions	O
and	O
work	O
sessions	O
were	O
held	O
in	O
budapest	O
montreal	O
leuven	O
and	O
louvain-la-neuve	O
in	O
leuven	O
our	O
gracious	O
hosts	O
were	O
ed	O
van	O
der	O
meulen	O
and	O
jan	O
beirlant	O
and	O
in	O
la-neuve	O
we	O
were	O
gastronomically	O
and	O
spiritually	O
supported	O
by	O
leopold	O
simar	O
and	O
irene	O
gijbels	O
we	O
thank	O
all	O
of	O
them	O
new	O
results	O
accumulated	O
and	O
we	O
had	O
to	O
resist	O
the	O
temptation	O
to	O
publish	O
these	O
in	O
journals	O
finally	O
in	O
may	O
the	O
manuscript	O
had	O
bloated	O
to	O
such	O
extent	O
that	O
it	O
had	O
to	O
be	O
sent	O
to	O
the	O
publisher	O
for	O
otherwise	O
it	O
would	O
have	O
become	O
an	O
encyclopedia	O
some	O
important	O
unanswered	O
questions	O
were	O
quickly	O
turned	O
into	O
masochistic	O
exercises	O
or	O
wild	O
conjectures	O
we	O
will	O
explain	O
subject	O
selection	O
classroom	O
use	O
chapter	O
dependence	O
and	O
personal	O
viewpoints	O
in	O
the	O
introduction	O
we	O
do	O
apologize	O
of	O
course	O
for	O
all	O
remaining	O
errors	O
we	O
were	O
touched	O
influenced	O
guided	O
and	O
taught	O
by	O
many	O
people	O
terry	O
ners	O
rigor	O
and	O
taste	O
for	O
beautiful	O
nonparametric	O
problems	O
have	O
infected	O
us	O
for	O
life	O
we	O
thank	O
our	O
past	O
and	O
present	O
coauthors	O
on	O
nonpararnetric	O
papers	O
alain	O
berlinet	O
michel	O
broniatowski	O
ricardo	O
cao	O
paul	O
deheuvels	O
andras	O
farago	O
adam	O
krzyzak	O
tamas	O
linder	O
andrew	O
nobel	O
mirek	O
pawlak	O
igor	O
vajda	O
harro	O
walk	O
and	O
ken	O
zeger	O
tamas	O
linder	O
read	O
most	O
of	O
the	O
book	O
and	O
provided	O
able	O
feedback	O
his	O
help	O
is	O
especially	O
appreciated	O
several	O
chapters	O
were	O
critically	O
read	O
by	O
students	O
in	O
budapest	O
we	O
thank	O
all	O
of	O
them	O
especially	O
andras	O
antos	O
miklos	O
csuros	O
balazs	O
kegl	O
istvan	O
pali	O
and	O
marti	O
pinter	O
finally	O
here	O
is	O
an	O
phabetically	O
ordered	B
list	O
of	O
friends	O
who	O
directly	O
or	O
indirectly	O
contributed	O
to	O
our	O
knowledge	O
and	O
love	O
of	O
nonparametrics	O
andrew	O
and	O
roger	O
barron	O
denis	O
bosq	O
prabhir	O
burman	O
tom	O
cover	O
antonio	O
cuevas	O
pierre	O
devijver	O
ricardo	O
fraiman	O
ned	O
glick	O
wenceslao	O
gonzalez-manteiga	O
peter	O
hall	O
eiichi	O
isogai	O
ed	O
mack	O
arthur	O
nadas	O
georg	O
pflug	O
george	O
roussas	O
winfried	O
stute	O
tamas	O
szabados	O
godfried	O
toussaint	O
sid	O
yakowitz	O
and	O
yannis	O
yatracos	O
gabor	O
diligently	O
typed	O
the	O
entire	O
manuscript	O
and	O
coordinated	O
all	O
contributions	O
he	O
became	O
quite	O
a	O
texpert	O
in	O
the	O
process	O
several	O
figures	O
were	O
made	O
by	O
idraw	O
and	O
xi	O
ig	O
by	O
gabor	O
and	O
luc	O
most	O
of	O
the	O
drawings	O
were	O
directly	O
programmed	O
in	O
postscript	O
by	O
luc	O
and	O
an	O
undergraduate	O
student	O
at	O
mcgill	O
university	O
hisham	O
petry	O
to	O
whom	O
we	O
are	O
grateful	O
for	O
gabor	O
this	O
book	O
comes	O
at	O
the	O
beginning	O
of	O
his	O
career	O
unfortunately	O
the	O
other	O
two	O
authors	O
are	O
not	O
so	O
lucky	O
as	O
both	O
luc	O
and	O
laci	O
felt	O
that	O
they	O
would	O
probably	O
not	O
write	O
another	O
book	O
on	O
nonparametric	O
pattern	O
recognition-the	O
random	O
walk	O
must	O
go	O
on-they	O
decided	O
to	O
put	O
their	O
general	O
preface	O
vii	O
view	O
of	O
the	O
subject	O
area	O
on	O
paper	O
while	O
trying	O
to	O
separate	O
the	O
important	O
from	O
the	O
irrelevant	O
surely	O
this	O
has	O
contributed	O
to	O
the	O
length	O
of	O
the	O
text	O
so	O
far	O
our	O
random	O
excursions	O
have	O
been	O
happy	O
ones	O
coincidentally	O
luc	O
is	O
married	O
to	O
bea	O
the	O
most	O
understanding	O
woman	O
in	O
the	O
world	O
and	O
happens	O
to	O
have	O
two	O
great	O
daughters	O
natasha	O
and	O
birgit	O
who	O
do	O
not	O
stray	O
off	O
their	O
random	O
courses	O
similarly	O
laci	O
has	O
an	O
equally	O
wonderful	O
wife	O
kati	O
and	O
two	O
children	O
with	O
steady	O
compasses	O
kati	O
and	O
janos	O
during	O
the	O
preparations	O
of	O
this	O
book	O
gabor	O
met	O
a	O
wonderful	O
girl	O
arrate	O
they	O
have	O
recently	O
decided	O
to	O
tie	O
their	O
lives	O
together	O
on	O
the	O
less	O
amorous	O
and	O
glamorous	O
side	O
we	O
gratefully	O
acknowledge	O
the	O
research	O
support	B
of	O
nserc	O
canada	O
fcar	O
quebec	O
otka	O
hungary	O
and	O
the	O
exchange	O
program	O
between	O
the	O
hungarian	O
academy	O
of	O
sciences	O
and	O
the	O
royal	O
belgian	O
academy	O
of	O
sciences	O
early	O
versions	O
of	O
this	O
text	O
were	O
tried	O
out	O
in	O
some	O
classes	O
at	O
the	O
technical	O
university	O
of	O
budapest	O
katholieke	O
universiteit	O
leuven	O
universitat	O
stuttgart	O
and	O
universite	O
montpellier	O
ii	O
we	O
would	O
like	O
to	O
thank	O
those	O
students	O
for	O
their	O
help	O
in	O
making	O
this	O
a	O
better	O
book	O
montreal	O
quebec	O
canada	O
budapest	O
hungary	O
budapest	O
hungary	O
luc	O
devroye	O
laci	O
gyorfi	O
gabor	O
lugosi	O
contents	O
preface	O
introduction	O
the	O
bayes	B
error	I
another	O
simple	O
example	O
the	O
bayes	B
problem	I
a	O
simple	O
example	O
other	O
formulas	O
for	O
the	O
bayes	O
risk	O
problems	O
and	O
exercises	O
plug-in	O
decisions	O
bayes	B
error	I
versus	O
dimension	B
inequalities	O
and	O
alternate	O
distance	O
measures	O
measuring	O
discriminatory	O
information	O
the	O
kolmogorov	B
variational	I
distance	I
the	O
nearest	B
neighbor	I
error	I
the	O
bhattacharyya	B
affinity	I
entropy	B
jeffreysdivergence	O
f-errors	O
the	O
mahalanobis	B
distance	I
i-divergences	O
problems	O
and	O
exercises	O
v	O
x	O
contents	O
linear	O
discrimination	O
univariate	O
discrimination	O
and	O
stoller	O
splits	O
linear	O
discriminants	O
the	O
fisher	B
linear	I
discriminant	I
the	O
normal	B
distribution	I
empirical	B
risk	I
minimization	I
minimizing	O
other	O
criteria	O
problems	O
and	O
exercises	O
nearest	B
neighbor	I
rules	O
introduction	O
notation	O
and	O
simple	O
asymptotics	O
proof	O
of	O
stones	O
lemma	O
the	O
asymptotic	O
probability	O
of	O
error	O
the	O
asymptotic	O
error	O
probability	O
of	O
weighted	B
nearest	B
neighbor	I
rules	O
k-nearest	O
neighbor	O
rules	O
even	O
k	O
inequalities	O
for	O
the	O
probability	O
of	O
error	O
behavior	O
when	O
l	O
is	O
small	O
nearest	B
neighbor	I
rules	O
when	O
l	O
admissibility	O
of	O
the	O
nearest	B
neighbor	I
rule	B
the	O
i-nearest	O
neighbor	O
rule	B
problems	O
and	O
exercises	O
consistency	B
universal	B
consistency	B
classification	O
and	O
regression	O
estimation	B
partitioning	O
rules	O
the	O
histogram	B
rule	B
stones	O
theorem	O
the	O
k-nearest	O
neighbor	O
rule	B
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	B
smart	O
rules	O
problems	O
and	O
exercises	O
slow	O
rates	O
of	O
convergence	O
finite	O
training	O
sequence	O
slow	O
rates	O
problems	O
and	O
exercises	O
error	B
estimation	B
error	B
counting	I
hoeffdings	O
inequality	B
error	B
estimation	B
without	O
testing	O
data	O
selecting	O
classifiers	O
contents	O
xi	O
estimating	O
the	O
bayes	B
error	I
problems	O
and	O
exercises	O
the	O
regular	B
histogram	B
rule	B
the	O
method	B
of	I
bounded	I
differences	I
strong	B
universal	B
consistency	B
problems	O
and	O
exercises	O
kernel	B
rules	O
consistency	B
proof	O
of	O
the	O
consistency	B
theorem	O
potential	O
function	O
rules	O
problems	O
and	O
exercises	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
strong	B
consistency	B
breaking	O
distance	O
ties	O
recursive	B
methods	O
scale-invariant	O
rules	O
weighted	B
nearest	B
neighbor	I
rules	O
rotation-invariant	B
rules	O
relabeling	B
rules	O
problems	O
and	O
exercises	O
vapnik-chervonenkis	B
theory	O
empirical	B
error	I
minimization	O
fingering	B
the	O
glivenko-cantelli	B
theorem	I
uniform	O
deviations	O
of	O
relative	O
frequencies	O
from	O
probabilities	O
classifier	B
selection	I
sample	B
complexity	I
the	O
zero-error	O
case	O
extensions	O
problems	O
and	O
exercises	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	B
theory	O
shatter	O
coefficients	O
and	O
vc	B
dimension	B
shatter	O
coefficients	O
of	O
some	O
classes	O
linear	O
and	O
generalized	B
linear	O
discrimination	O
rules	O
convex	O
sets	O
and	O
monotone	O
layers	O
problems	O
and	O
exercises	O
lower	B
bounds	I
for	I
empirical	B
classifier	B
selection	I
minimax	O
lower	O
bounds	O
the	O
case	O
lc	O
classes	O
with	O
infinite	O
vc	B
dimension	B
xii	O
contents	O
the	O
case	O
lc	O
sample	B
complexity	I
problems	O
and	O
exercises	O
the	O
maximum	B
likelihood	I
principle	O
maximum	B
likelihood	I
the	O
formats	O
the	O
maximum	B
likelihood	I
method	O
regression	B
format	I
consistency	B
examples	O
classical	O
maximum	B
likelihood	I
distribution	B
format	I
problems	O
and	O
exercises	O
parametric	B
classification	I
example	O
exponential	B
families	O
standard	B
plug-in	O
rules	O
minimum	O
distance	O
estimates	O
empirical	B
error	I
minimization	O
problems	O
and	O
exercises	O
generalized	B
linear	O
discrimination	O
fourier	O
series	O
classification	O
generalized	B
linear	O
classification	O
problems	O
and	O
exercises	O
complexity	B
regularization	I
structural	O
risk	O
minimization	O
poor	O
approximation	O
properties	O
of	O
vc	O
classes	O
simple	B
empirical	B
covering	O
problems	O
and	O
exercises	O
condensed	B
and	O
edited	B
nearest	B
neighbor	I
rules	O
condensed	B
nearest	B
neighbor	I
rules	O
edited	B
nearest	B
neighbor	I
rules	O
sieves	O
and	O
prototypes	O
problems	O
and	O
exercises	O
tree	O
classifiers	O
invariance	O
trees	O
with	O
the	O
x-property	B
balanced	B
search	O
trees	O
binary	B
search	O
trees	O
the	O
chronological	B
k-d	B
tree	I
the	O
deep	B
k-d	B
tree	I
quadtrees	O
best	O
possible	O
perpendicular	O
splits	O
splitting	O
criteria	O
based	O
on	O
impurity	O
functions	O
a	O
consistent	O
splitting	B
criterion	I
bsp	O
trees	O
primitive	O
selection	O
constructing	O
consistent	O
tree	O
classifiers	O
a	O
greedy	B
classifier	B
problems	O
and	O
exercises	O
data-dependent	B
partitioning	O
introduction	O
a	O
vapnik-chervonenkis	B
inequality	B
for	O
partitions	O
consistency	B
statistically	B
equivalent	I
blocks	I
partitioning	O
rules	O
based	O
on	O
clustering	B
data-based	B
scaling	O
classification	O
trees	O
problems	O
and	O
exercises	O
splitting	B
the	I
data	I
the	O
holdout	B
estimate	O
consistency	B
and	O
asymptotic	B
optimality	I
nearest	B
neighbor	I
rules	O
with	O
automatic	B
scaling	O
classification	O
based	O
on	O
clustering	B
statistically	B
equivalent	I
blocks	I
binary	B
tree	O
classifiers	O
problems	O
and	O
exercises	O
the	O
resubstitution	B
estimate	O
the	O
resubstitution	B
estimate	O
histogram	O
rules	O
data-based	B
histograms	O
and	O
rule	B
selection	O
problems	O
and	O
exercises	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
a	O
general	O
lower	O
bound	O
a	O
general	O
upper	O
bound	O
for	O
deleted	O
estimates	O
nearest	B
neighbor	I
rules	O
kernel	B
rules	O
histogram	O
rules	O
problems	O
and	O
exercises	O
automatic	B
kernel	B
rules	O
consistency	B
data	O
splitting	O
kernel	B
complexity	I
multiparameter	B
kernel	B
rules	O
contents	O
xiii	O
xiv	O
contents	O
kernels	O
of	O
infinite	O
complexity	O
on	O
minimizing	O
the	O
apparent	O
error	O
rate	O
minimizing	O
the	O
deleted	O
estimate	O
sieve	O
methods	O
squared	B
error	I
minimization	O
problems	O
and	O
exercises	O
automatic	B
nearest	B
neighbor	I
rules	O
consistency	B
data	O
splitting	O
data	O
splitting	O
for	O
weighted	B
nn	O
rules	O
reference	O
data	O
and	O
data	O
splitting	O
variable	B
metric	I
nn	O
rules	O
selection	O
of	O
k	O
based	O
on	O
the	O
deleted	O
estimate	O
problems	O
and	O
exercises	O
independent	O
components	O
hypercubes	O
and	O
discrete	O
spaces	O
multinomial	B
discrimination	I
quantization	B
boolean	O
classifiers	O
series	O
methods	O
for	O
the	O
hypercube	O
maximum	B
likelihood	I
kernel	B
methods	O
problems	O
and	O
exercises	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
definitions	O
examples	O
totally	O
bounded	O
classes	O
skeleton	O
estimates	O
rate	B
of	I
convergence	I
problems	O
and	O
exercises	O
uniform	B
laws	I
of	I
large	I
numbers	I
minimizing	O
the	O
empirical	B
squared	B
error	I
uniform	O
deviations	O
of	O
averages	O
from	O
expectations	O
empirical	B
squared	B
error	I
minimization	O
proof	O
of	O
theorem	O
covering	O
numbers	O
and	O
shatter	O
coefficients	O
generalized	B
linear	O
classification	O
problems	O
and	O
exercises	O
neural	O
networks	O
multilayer	B
perceptrons	O
arrangements	O
approximation	O
by	O
neural	O
networks	O
vc	B
dimension	B
error	O
minimization	O
the	O
adaline	B
and	O
padaline	B
polynomial	B
networks	O
kolmogorov-lorentz	O
networks	O
and	O
additive	O
models	O
projection	B
pursuit	I
radial	B
basis	B
function	I
networks	O
problems	O
and	O
exercises	O
other	O
error	O
estimates	O
smoothing	O
the	O
error	O
count	O
posterior	B
probability	I
estimates	O
rotation	B
estimate	O
bootstrap	B
problems	O
and	O
exercises	O
feature	B
extraction	I
dimensionality	O
reduction	O
transformations	O
with	O
small	O
distortion	O
admissible	O
and	O
sufficient	O
transformations	O
problems	O
and	O
exercises	O
appendix	O
probability	O
inequalities	O
a	O
l	O
basics	O
of	O
measure	O
theory	O
the	O
lebesgue	O
integral	O
denseness	O
results	O
convergence	O
of	O
random	O
variables	O
conditional	O
expectation	O
the	O
binomial	B
distribution	I
the	O
hypergeometric	B
distribution	I
the	O
multinomial	B
distribution	I
a	O
ll	O
the	O
exponential	B
and	O
gamma	B
distributions	O
the	O
multivariate	O
normal	B
distribution	I
notation	O
references	O
author	O
index	O
subject	O
index	O
contents	O
xv	O
introduction	O
pattern	O
recognition	O
or	O
discrimination	O
is	O
about	O
guessing	O
or	O
predicting	O
the	O
unknown	O
nature	O
of	O
an	O
observation	O
a	O
discrete	O
quantity	O
such	O
as	O
black	O
or	O
white	O
one	O
or	O
zero	O
sick	O
or	O
healthy	O
real	O
or	O
fake	O
an	O
observation	O
is	O
a	O
collection	O
of	O
numerical	O
measurements	O
such	O
as	O
an	O
image	O
is	O
a	O
sequence	O
of	O
bits	O
one	O
per	O
pixel	O
a	O
vector	O
of	O
weather	O
data	O
an	O
electrocardiogram	O
or	O
a	O
signature	O
on	O
a	O
check	O
suitably	O
digitized	O
more	O
formally	O
an	O
observation	O
is	O
a	O
d-dimensional	O
vector	O
x	O
the	O
unknown	O
nature	O
of	O
the	O
observation	O
is	O
called	O
a	O
class	O
it	O
is	O
denoted	O
by	O
y	O
and	O
takes	O
values	O
in	O
a	O
finite	O
set	O
m	O
in	O
pattern	O
recognition	O
one	O
creates	O
a	O
function	O
gx	O
nd	O
m	O
which	O
represents	O
ones	O
guess	O
of	O
y	O
given	O
x	O
the	O
mapping	O
g	O
is	O
called	O
a	O
classifier	B
our	O
classifier	B
errs	O
on	O
x	O
if	O
gx	O
i	O
y	O
how	O
one	O
creates	O
a	O
rule	B
g	O
depends	O
upon	O
the	O
problem	O
at	O
hand	O
experts	O
can	O
be	O
called	O
for	O
medical	O
diagnoses	O
or	O
earthquake	O
predictions-they	O
try	O
to	O
mold	O
g	O
to	O
their	O
own	O
knowledge	O
and	O
experience	O
often	O
by	O
trial	O
and	O
error	O
theoretically	O
each	O
expert	O
operates	O
with	O
a	O
built-in	O
classifier	B
g	O
but	O
describing	O
this	O
g	O
explicitly	O
in	O
mathematical	O
form	O
is	O
not	O
a	O
sinecure	O
the	O
sheer	O
magnitude	O
and	O
richness	O
of	O
the	O
space	O
of	O
x	O
may	O
defeat	O
even	O
the	O
best	O
expert-it	O
is	O
simply	O
impossible	O
to	O
specify	O
g	O
for	O
all	O
possible	O
xs	O
one	O
is	O
likely	O
to	O
see	O
in	O
the	O
future	O
we	O
have	O
to	O
be	O
prepared	O
to	O
live	O
with	O
imperfect	O
classifiers	O
in	O
fact	O
how	O
should	O
we	O
measure	O
the	O
quality	O
of	O
a	O
classifier	B
we	O
cant	O
just	O
dismiss	O
a	O
classifier	B
just	O
because	O
it	O
misclassifies	O
a	O
particular	O
x	O
for	O
one	O
thing	O
if	O
the	O
observation	O
does	O
not	O
fully	O
describe	O
the	O
underlying	O
process	O
is	O
if	O
y	O
is	O
not	O
a	O
deterministic	O
function	O
of	O
x	O
it	O
is	O
possible	O
that	O
the	O
same	O
x	O
may	O
give	O
rise	O
to	O
two	O
different	O
ys	O
on	O
different	O
occasions	O
for	O
example	O
if	O
we	O
just	O
measure	O
water	O
content	O
of	O
a	O
persons	O
body	O
and	O
we	O
find	O
that	O
the	O
person	O
is	O
dehydrated	O
then	O
the	O
cause	O
class	O
may	O
range	O
from	O
a	O
low	O
water	O
intake	O
in	O
hot	O
weather	O
to	O
severe	O
diarrhea	O
thus	O
we	O
introduce	O
a	O
probabilistic	O
setting	O
and	O
let	O
y	O
be	O
an	O
n	O
d	O
x	O
m-valued	O
introduction	O
random	O
pair	O
the	O
distribution	O
of	O
y	O
describes	O
the	O
frequency	O
of	O
encountering	O
particular	O
pairs	O
in	O
practice	O
an	O
error	O
occurs	O
if	O
gx	O
i	O
y	O
and	O
the	O
probability	O
of	O
error	O
for	O
a	O
classifier	B
g	O
is	O
lg	O
pgx	O
i	O
y	O
there	O
is	O
a	O
best	O
possible	O
classifier	B
g	O
which	O
is	O
defined	O
by	O
g	O
arg	O
min	O
pgx	O
i	O
y	O
grd-i	O
note	O
that	O
g	O
depends	O
upon	O
the	O
distribution	O
of	O
y	O
if	O
this	O
distribution	O
is	O
known	O
g	O
may	O
be	O
computed	O
the	O
problem	O
of	O
finding	O
g	O
is	O
bayes	O
j	O
problem	O
and	O
the	O
sifier	O
g	O
is	O
called	O
the	O
bayes	O
classifier	B
the	O
bayes	O
rule	B
the	O
minimal	O
probability	O
of	O
error	O
is	O
called	O
the	O
bayes	B
error	I
and	O
is	O
denoted	O
by	O
l	O
lg	O
mostly	O
the	O
distribution	O
of	O
y	O
is	O
unknown	O
so	O
that	O
g	O
is	O
unknown	O
too	O
we	O
do	O
not	O
consult	O
an	O
expert	O
to	O
try	O
to	O
reconstruct	O
g	O
but	O
have	O
access	O
to	O
a	O
good	O
database	O
of	O
pairs	O
yi	O
i	O
n	O
observed	O
in	O
the	O
past	O
this	O
database	O
may	O
be	O
the	O
result	O
of	O
experimental	O
observation	O
for	O
meteorological	O
data	O
fingerprint	O
data	O
ecg	O
data	O
or	O
handwritten	O
characters	O
it	O
could	O
also	O
be	O
obtained	O
through	O
an	O
expert	O
or	O
a	O
teacher	O
who	O
filled	O
in	O
the	O
yis	O
after	O
having	O
seen	O
the	O
xso	O
to	O
find	O
a	O
classifier	B
g	O
with	O
a	O
small	O
probability	O
of	O
error	O
is	O
hopeless	O
unless	O
there	O
is	O
some	O
assurance	O
that	O
the	O
yisjointly	O
are	O
somehow	O
representative	O
of	O
the	O
unknown	O
distribution	O
we	O
shall	O
assume	O
in	O
this	O
book	O
that	O
yn	O
the	O
data	O
is	O
a	O
sequence	O
of	O
independent	O
identically	O
distributed	O
random	O
pairs	O
with	O
the	O
same	O
distribution	O
as	O
that	O
of	O
y	O
this	O
is	O
a	O
very	O
strong	O
assumption	O
indeed	O
however	O
some	O
retical	O
results	O
are	O
emerging	O
that	O
show	O
that	O
classifiers	O
based	O
on	O
slightly	O
dependent	O
data	O
pairs	O
and	O
on	O
i	O
i	O
d	O
data	O
pairs	O
behave	O
roughly	O
the	O
same	O
also	O
simple	O
models	O
are	O
easier	O
to	O
understand	O
are	O
more	O
amenable	O
to	O
interpretation	O
a	O
classifier	B
is	O
constructed	O
on	O
the	O
basis	O
of	O
xl	O
x	O
n	O
yn	O
and	O
is	O
denoted	O
by	O
gn	O
y	O
is	O
guessed	O
by	O
gnx	O
xl	O
yi	O
xn	O
yn	O
the	O
process	O
of	O
constructing	O
gn	O
is	O
called	O
learning	B
supervised	B
learning	B
or	O
learning	B
with	B
a	I
teacher	I
the	O
performance	O
of	O
gn	O
is	O
measured	O
by	O
the	O
conditional	O
probability	O
of	O
error	O
this	O
is	O
a	O
random	O
variable	B
because	O
it	O
depends	O
upon	O
the	O
data	O
so	O
ln	O
averages	O
over	O
the	O
distribution	O
of	O
y	O
but	O
the	O
data	O
is	O
held	O
fixed	O
averaging	O
over	O
the	O
data	O
as	O
well	O
would	O
be	O
unnatural	O
because	O
in	O
a	O
given	O
application	O
one	O
has	O
to	O
live	O
with	O
the	O
data	O
at	O
hand	O
it	O
would	O
be	O
marginally	O
useful	O
to	O
know	O
the	O
number	O
eln	O
as	O
this	O
number	O
would	O
indicate	O
the	O
quality	O
of	O
an	O
average	O
data	O
sequence	O
not	O
your	O
data	O
sequence	O
this	O
text	O
is	O
thus	O
about	O
l	O
n	O
the	O
conditional	O
probability	O
of	O
error	O
an	O
individual	O
mapping	O
gn	O
nd	O
x	O
x	O
mn	O
m	O
is	O
still	O
called	O
a	O
classifier	B
a	O
sequence	O
n	O
i	O
is	O
called	O
a	O
rule	B
thus	O
classifiers	O
are	O
functions	O
and	O
rules	O
are	O
sequences	O
of	O
functions	O
a	O
novice	O
might	O
ask	O
simple	O
questions	O
like	O
this	O
how	O
does	O
one	O
construct	O
a	O
good	O
classifier	B
how	O
good	O
can	O
a	O
classifier	B
be	O
is	O
classifier	B
a	O
better	O
than	O
classifier	B
b	O
can	O
we	O
estimate	O
how	O
good	O
a	O
classifier	B
is	O
what	O
is	O
the	O
best	O
classifier	B
this	O
book	O
partially	O
introduction	O
answers	O
such	O
simple	O
questions	O
a	O
good	O
deal	O
of	O
energy	O
is	O
spent	O
on	O
the	O
mathematical	O
formulations	O
of	O
the	O
novices	O
questions	O
for	O
us	O
a	O
rule-not	O
a	O
classifier-is	O
good	O
if	O
it	O
is	O
consistent	O
that	O
is	O
if	O
lim	O
eln	O
l	O
n-oo	O
or	O
equivalently	O
if	O
ln	O
l	O
in	O
probability	O
as	O
n	O
we	O
assume	O
that	O
the	O
reader	O
has	O
a	O
good	O
grasp	O
of	O
the	O
basic	O
elements	O
of	O
probability	O
including	O
notions	O
such	O
as	O
convergence	O
in	O
probability	O
strong	O
laws	O
of	O
large	O
numbers	O
for	O
averages	O
and	O
conditional	O
probability	O
a	O
selection	O
of	O
results	O
and	O
definitions	O
that	O
may	O
be	O
useful	O
for	O
this	O
text	O
is	O
given	O
in	O
the	O
appendix	O
a	O
consistent	O
rule	B
guarantees	O
us	O
that	O
taking	O
more	O
samples	O
essentially	O
suffices	O
to	O
roughly	O
reconstruct	O
the	O
unknown	O
distribution	O
of	O
y	O
because	O
ln	O
can	O
be	O
pushed	O
as	O
close	O
as	O
desired	O
to	O
l	O
in	O
other	O
words	O
infinite	O
amounts	O
of	O
information	O
can	O
be	O
gleaned	O
from	O
finite	O
samples	O
without	O
this	O
guarantee	O
we	O
would	O
not	O
be	O
motivated	O
to	O
take	O
more	O
samples	O
we	O
should	O
be	O
careful	O
and	O
not	O
impose	O
conditions	O
on	O
y	O
for	O
the	O
consistency	B
of	O
a	O
rule	B
because	O
such	O
conditions	O
may	O
not	O
be	O
verifiable	O
if	O
a	O
rule	B
is	O
consistent	O
for	O
all	O
distributions	O
of	O
y	O
itis	O
said	O
to	O
be	O
universally	O
consistent	O
interestingly	O
until	O
it	O
was	O
not	O
known	O
if	O
a	O
universally	O
consistent	O
rule	B
existed	O
all	O
consistency	B
results	O
came	O
with	O
restrictions	O
on	O
y	O
in	O
stone	O
showed	O
that	O
one	O
could	O
just	O
take	O
any	O
k-nearest	O
neighbor	O
rule	B
with	O
k	O
kn	O
and	O
kin	O
the	O
k-nearest	O
neighbor	O
classifier	B
gnx	O
takes	O
a	O
majority	B
vote	I
over	O
the	O
ys	O
in	O
the	O
subset	O
of	O
k	O
pairs	O
yi	O
from	O
yi	O
yn	O
that	O
have	O
the	O
smallest	O
values	O
for	O
ii	O
xi	O
x	O
ii	O
for	O
which	O
xi	O
is	O
closest	O
to	O
x	O
since	O
stones	O
proof	O
of	O
the	O
universal	B
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
several	O
other	O
rules	O
have	O
been	O
shown	O
to	O
be	O
universally	O
consistent	O
as	O
well	O
this	O
book	O
stresses	O
universality	O
and	O
hopefully	O
gives	O
a	O
reasonable	O
account	O
of	O
the	O
developments	O
in	O
this	O
direction	O
probabilists	O
may	O
wonder	O
why	O
we	O
did	O
not	O
use	O
convergence	O
with	O
probability	O
one	O
in	O
our	O
definition	B
of	I
consistency	B
indeed	O
strong	O
consistency-convergence	O
of	O
ln	O
to	O
l	O
with	O
probability	O
one-implies	O
convergence	O
for	O
almost	O
every	O
sample	O
as	O
it	O
grows	O
fortunately	O
for	O
most	O
well-behaved	O
rules	O
consistency	B
and	O
strong	B
consistency	B
are	O
equivalent	O
for	O
example	O
for	O
the	O
k-nearest	O
neighbor	O
rule	B
k	O
and	O
kin	O
together	O
imply	O
ln	O
l	O
with	O
probability	O
one	O
the	O
equivalence	O
will	O
be	O
dealt	O
with	O
but	O
it	O
will	O
not	O
be	O
a	O
major	O
focus	O
of	O
attention	O
most	O
if	O
not	O
all	O
equivalence	O
results	O
are	O
based	O
upon	O
some	O
powerful	O
concentration	O
inequalities	O
such	O
as	O
mcdiarmids	O
for	O
example	O
we	O
will	O
be	O
able	O
to	O
show	O
that	O
for	O
the	O
k-nearest	O
neighbor	O
rule	B
there	O
exists	O
a	O
number	O
c	O
such	O
that	O
for	O
all	O
e	O
there	O
exists	O
n	O
depending	O
upon	O
the	O
distribution	O
of	O
y	O
such	O
that	O
pln	O
l	O
e	O
n	O
ne	O
this	O
illustrates	O
yet	O
another	O
focus	O
of	O
the	O
book-inequalities	O
whenever	O
possible	O
we	O
make	O
a	O
case	O
or	O
conclude	O
a	O
proof	O
via	O
explicit	O
inequalities	O
various	O
parameters	O
can	O
be	O
substituted	O
in	O
these	O
inequalities	O
to	O
allow	O
the	O
user	O
to	O
draw	O
conclusions	O
regarding	O
sample	O
size	O
or	O
to	O
permit	O
identification	O
of	O
the	O
most	O
important	O
parameters	O
the	O
material	O
in	O
the	O
book	O
is	O
often	O
technical	O
and	O
dry	O
so	O
to	O
stay	O
focused	O
on	O
the	O
main	O
issues	O
we	O
keep	O
the	O
problem	O
simple	O
introduction	O
a	O
we	O
only	O
deal	O
with	O
binary	B
classification	O
the	O
class	O
y	O
takes	O
values	O
in	O
i	O
and	O
a	O
classifier	B
gn	O
is	O
a	O
mapping	O
rd	O
x	O
x	O
ln	O
i	O
b	O
we	O
only	O
consider	O
i	O
i	O
d	O
data	O
sequences	O
we	O
also	O
disallow	O
active	O
learning	B
a	O
set-up	O
in	O
which	O
the	O
user	O
can	O
select	O
the	O
xis	O
deterministically	O
c	O
we	O
do	O
not	O
consider	O
infinite	O
spaces	O
for	O
example	O
x	O
cannot	O
be	O
a	O
random	O
function	O
such	O
as	O
a	O
cardiogram	O
x	O
must	O
be	O
a	O
rd-valued	O
random	O
vector	O
the	O
reader	O
should	O
be	O
aware	O
that	O
many	O
results	O
given	O
here	O
may	O
be	O
painlessly	O
extended	O
to	O
certain	O
metric	O
spaces	O
of	O
infinite	O
dimension	B
let	O
us	O
return	O
to	O
our	O
novices	O
questions	O
we	O
know	O
that	O
there	O
are	O
good	O
rules	O
but	O
just	O
how	O
good	O
can	O
a	O
classifier	B
be	O
obviously	O
ln	O
l	O
in	O
all	O
cases	O
it	O
is	O
thus	O
important	O
to	O
know	O
l	O
or	O
to	O
estimate	O
it	O
for	O
if	O
l	O
is	O
large	O
any	O
classifier	B
including	O
yours	O
will	O
perform	O
poorly	O
but	O
even	O
if	O
l	O
were	O
zero	O
ln	O
could	O
still	O
be	O
large	O
thus	O
it	O
would	O
be	O
nice	O
to	O
have	O
explicit	O
inequalities	O
for	O
probabilities	O
such	O
as	O
pln	O
l	O
however	O
such	O
inequalities	O
must	O
necessarily	O
depend	O
upon	O
the	O
distribution	O
of	O
y	O
that	O
is	O
for	O
any	O
rule	B
lim	O
inf	O
n-oo	O
all	O
distributions	O
of	O
with	O
sup	O
pln	O
l	O
e	O
universal	O
rate	B
of	I
convergence	I
guarantees	O
do	O
not	O
exist	O
rate	B
of	I
convergence	I
studies	O
must	O
involve	O
certain	O
subclasses	O
of	O
distributions	O
of	O
y	O
for	O
this	O
reason	O
with	O
few	O
exceptions	O
we	O
will	O
steer	O
clear	O
of	O
the	O
rate	B
of	I
convergence	I
quicksand	O
even	O
if	O
there	O
are	O
no	O
universal	O
performance	O
guarantees	O
we	O
might	O
still	O
be	O
able	O
to	O
satisfy	O
our	O
novices	O
curiosity	O
if	O
we	O
could	O
satisfactorily	O
estimate	O
ln	O
for	O
the	O
rule	B
at	O
hand	O
by	O
a	O
function	O
ln	O
of	O
the	O
data	O
such	O
functions	O
are	O
called	O
error	O
estimates	O
for	O
example	O
for	O
the	O
k-nearest	O
neighbor	O
classifier	B
we	O
could	O
use	O
the	O
deleted	O
estimate	O
where	O
gnixi	O
classifies	O
xi	O
by	O
the	O
k-nearest	O
neighbor	O
method	O
based	O
upon	O
the	O
data	O
yd	O
yn	O
with	O
yi	O
deleted	O
if	O
this	O
is	O
done	O
we	O
have	O
a	O
free	O
inequality	B
i	O
piln	O
ln	O
i	O
e	O
nf	O
rogers-wagner	B
inequality	B
provided	O
that	O
distance	O
ties	O
are	O
broken	O
in	O
an	O
propriate	O
manner	O
in	O
other	O
words	O
without	O
knowing	O
the	O
distribution	O
of	O
y	O
we	O
can	O
state	O
with	O
a	O
certain	O
confidence	B
that	O
ln	O
is	O
contained	O
in	O
f	O
f	O
thus	O
for	O
many	O
classifiers	O
it	O
is	O
indeed	O
possible	O
to	O
estimate	O
ln	O
from	O
the	O
data	O
at	O
hand	O
however	O
it	O
is	O
impossible	O
to	O
estimate	O
l	O
universally	O
well	O
for	O
any	O
n	O
and	O
any	O
timate	O
of	O
l	O
based	O
upon	O
the	O
data	O
sequence	O
there	O
always	O
exists	O
a	O
distribution	O
of	O
y	O
for	O
which	O
the	O
estimate	O
is	O
arbitrarily	O
poor	O
can	O
we	O
compare	O
rules	O
and	O
again	O
the	O
answer	O
is	O
negative	O
there	O
exists	O
introduction	O
no	O
classifier	B
superclassifier	B
as	O
for	O
any	O
rule	B
there	O
exists	O
a	O
bution	O
of	O
y	O
and	O
another	O
rule	B
such	O
that	O
for	O
all	O
n	O
elg	O
elgn	O
if	O
there	O
had	O
been	O
a	O
universally	O
best	O
classifier	B
this	O
book	O
would	O
have	O
been	O
essary	O
we	O
would	O
all	O
have	O
to	O
use	O
it	O
all	O
the	O
time	O
this	O
nonexistence	O
implies	O
that	O
the	O
debate	O
between	O
practicing	O
pattern	O
recognizers	O
will	O
never	O
end	O
and	O
that	O
simulations	O
on	O
particular	O
examples	O
should	O
never	O
be	O
used	O
to	O
compare	O
classifiers	O
as	O
an	O
ple	O
consider	O
the	O
i-nearest	O
neighbor	O
rule	B
a	O
simple	O
but	O
not	O
universally	O
consistent	O
rule	B
yet	O
among	O
all	O
k-nearest	O
neighbor	O
classifiers	O
the	O
i-nearest	O
neighbor	O
classifier	B
is	O
admissible-there	O
are	O
distributions	O
for	O
which	O
its	O
expected	O
probability	O
of	O
error	O
is	O
better	O
than	O
for	O
any	O
k-nearest	O
neighbor	O
classifier	B
with	O
k	O
so	O
it	O
can	O
never	O
be	O
totally	O
dismissed	O
thus	O
we	O
must	O
study	O
all	O
simple	O
rules	O
and	O
we	O
will	O
reserve	O
many	O
pages	O
for	O
the	O
nearest	B
neighbor	I
rule	B
and	O
its	O
derivatives	O
we	O
will	O
for	O
example	O
prove	O
the	O
cover-hart	B
inequality	B
and	O
hart	O
which	O
states	O
that	O
for	O
all	O
distributions	O
of	O
y	O
lim	O
sup	O
eln	O
n-oo	O
where	O
ln	O
is	O
the	O
probability	O
of	O
error	O
with	O
the	O
i-nearest	O
neighbor	O
rule	B
as	O
l	O
is	O
usually	O
small	O
otherwise	O
you	O
would	O
not	O
want	O
to	O
do	O
discrimination	O
is	O
small	O
too	O
and	O
the	O
i-nearest	O
neighbor	O
rule	B
will	O
do	O
just	O
fine	O
the	O
nonexistence	O
of	O
a	O
best	O
classifier	B
may	O
disappoint	O
our	O
novice	O
however	O
we	O
may	O
change	O
the	O
setting	O
somewhat	O
and	O
limit	O
the	O
classifiers	O
to	O
a	O
certain	O
class	O
c	O
such	O
as	O
all	O
k-nearest	O
neighbor	O
classifiers	O
with	O
all	O
possible	O
values	O
for	O
k	O
is	O
it	O
possible	O
to	O
select	O
the	O
best	O
classifier	B
from	O
this	O
class	O
phrased	O
in	O
this	O
manner	O
we	O
cannot	O
possibly	O
do	O
better	O
than	O
def	O
l	O
mf	O
pgnx	O
y	O
gn	O
ec	O
typically	O
l	O
l	O
interestingly	O
there	O
is	O
a	O
general	O
paradigm	O
for	O
picking	O
classifiers	O
from	O
c	O
and	O
to	O
obtain	O
universal	O
performance	O
guarantees	O
it	O
uses	O
empirical	B
risk	O
imization	O
a	O
method	O
studied	O
in	O
great	O
detail	O
in	O
the	O
work	O
of	O
vapnik	O
and	O
chervonenkis	O
for	O
example	O
if	O
we	O
select	O
gn	O
from	O
c	O
by	O
minimizing	O
then	O
the	O
corresponding	O
probability	O
of	O
error	O
ln	O
satisfies	O
the	O
following	O
inequality	B
for	O
all	O
e	O
pln	O
les	O
v	O
here	O
v	O
is	O
an	O
integer	O
depending	O
upon	O
the	O
massiveness	O
of	O
c	O
only	O
v	O
is	O
called	O
the	O
vc	B
dimension	B
of	O
c	O
and	O
may	O
be	O
infinite	O
for	O
large	O
classes	O
c	O
for	O
sufficiently	O
restricted	O
classes	O
c	O
v	O
is	O
finite	O
and	O
the	O
explicit	O
universal	O
bound	O
given	O
above	O
can	O
be	O
used	O
to	O
obtain	O
performance	O
guarantees	O
for	O
the	O
selected	O
g	O
n	O
to	O
l	O
not	O
l	O
the	O
bound	O
above	O
is	O
only	O
valid	O
if	O
c	O
is	O
independent	O
of	O
the	O
data	O
pairs	O
yi	O
yn	O
fixed	O
classes	O
such	O
as	O
all	O
classifiers	O
that	O
decide	O
on	O
a	O
halfspace	O
and	O
on	O
its	O
complement	O
are	O
fine	O
we	O
may	O
also	O
sample	O
m	O
more	O
pairs	O
addition	O
to	O
the	O
n	O
pairs	O
introduction	O
already	O
present	O
and	O
use	O
the	O
n	O
pairs	O
as	O
above	O
to	O
select	O
the	O
best	O
k	O
for	O
use	O
in	O
the	O
k-nearest	O
neighbor	O
classifier	B
based	O
on	O
the	O
m	O
pairs	O
as	O
we	O
will	O
see	O
the	O
selected	O
rule	B
is	O
universally	O
consistent	O
if	O
both	O
m	O
and	O
n	O
diverge	O
and	O
n	O
flog	O
m	O
and	O
we	O
have	O
automatically	O
solved	O
the	O
problem	O
of	O
picking	O
k	O
recall	O
that	O
stones	O
universal	B
consistency	B
theorem	O
only	O
told	O
us	O
to	O
pick	O
k	O
om	O
and	O
to	O
let	O
k	O
but	O
it	O
does	O
not	O
tell	O
us	O
whether	O
k	O
m	O
is	O
preferable	O
over	O
k	O
m	O
empirical	B
risk	I
minimization	I
produces	O
a	O
random	O
data-dependent	B
k	O
that	O
is	O
not	O
even	O
guaranteed	O
to	O
tend	O
to	O
infinity	O
or	O
to	O
be	O
oem	O
yet	O
the	O
selected	O
rule	B
is	O
universally	O
consistent	O
we	O
offer	O
virtually	O
no	O
help	O
with	O
algorithms	O
as	O
in	O
standard	B
texts	O
with	O
two	O
notable	O
exceptions	O
ease	O
of	O
computation	O
storage	O
and	O
interpretation	O
has	O
spurred	O
the	O
velopment	O
of	O
certain	O
rules	O
for	O
example	O
tree	O
classifiers	O
construct	O
a	O
tree	O
for	O
storing	O
the	O
data	O
and	O
partition	B
r	O
by	O
certain	O
cuts	O
that	O
are	O
typically	O
perpendicular	O
to	O
a	O
ordinate	O
axis	O
we	O
say	O
that	O
a	O
coordinate	O
axis	O
is	O
such	O
classifiers	O
have	O
obvious	O
computational	O
advantages	O
and	O
are	O
amenable	O
to	O
interpretation-the	O
components	O
of	O
the	O
vector	O
x	O
that	O
are	O
cut	O
at	O
the	O
early	O
stages	O
of	O
the	O
tree	O
are	O
most	O
crucial	O
in	O
reaching	O
a	O
decision	O
expert	O
systems	O
automated	O
medical	O
diagnosis	O
and	O
a	O
host	O
of	O
other	O
nition	O
rules	O
use	O
tree	O
classification	O
for	O
example	O
in	O
automated	O
medical	O
diagnosis	O
one	O
may	O
first	O
check	O
a	O
patients	O
pulse	O
if	O
this	O
is	O
zero	O
the	O
patient	O
is	O
dead	O
if	O
it	O
is	O
below	O
the	O
patient	O
is	O
weak	B
the	O
first	O
component	O
is	O
cut	O
twice	O
in	O
each	O
case	O
we	O
may	O
then	O
consider	O
another	O
component	O
and	O
continue	O
the	O
breakdown	O
into	O
more	O
and	O
more	O
specific	O
cases	O
several	O
interesting	O
new	O
universally	O
consistent	O
tree	O
classifiers	O
are	O
described	O
in	O
chapter	O
the	O
second	O
group	O
of	O
classifiers	O
whose	O
development	O
was	O
partially	O
based	O
upon	O
easy	O
implementations	O
is	O
the	O
class	O
of	B
neural	B
network	I
classifiers	I
descendants	O
of	O
rosenblatts	O
perceptron	B
these	O
classifiers	O
have	O
unknown	O
rameters	O
that	O
must	O
be	O
trained	O
or	O
selected	O
by	O
the	O
data	O
in	O
the	O
way	O
we	O
let	O
the	O
data	O
pick	O
k	O
in	O
the	O
k-nearest	O
neighbor	O
classifier	B
most	O
research	O
papers	O
on	O
neural	O
networks	O
deal	O
with	O
the	O
training	O
aspect	O
but	O
we	O
will	O
not	O
when	O
we	O
say	O
the	O
parameters	O
by	O
empirical	B
risk	I
minimization	I
we	O
will	O
leave	O
the	O
important	O
algorithmic	O
complexity	O
questions	O
unanswered	O
perceptrons	O
divide	O
the	O
space	O
by	O
one	O
hyperplane	O
and	O
attach	O
decisions	O
and	O
to	O
the	O
two	O
halfspaces	O
such	O
simple	O
classifiers	O
are	O
not	O
consistent	O
except	O
for	O
a	O
few	O
distributions	O
this	O
is	O
the	O
case	O
for	O
example	O
when	O
x	O
takes	O
ues	O
on	O
hypercube	O
and	O
the	O
components	O
of	O
x	O
are	O
independent	O
neural	O
networks	O
with	B
one	I
hidden	I
layer	I
are	O
universally	O
consistent	O
if	O
the	O
parameters	O
are	O
well-chosen	O
we	O
will	O
see	O
that	O
there	O
is	O
also	O
some	O
gain	O
in	O
considering	O
two	O
hidden	O
layers	O
but	O
that	O
it	O
is	O
not	O
really	O
necessary	O
to	O
go	O
beyond	O
two	O
complexity	O
of	O
the	O
training	O
algorithm-the	O
phase	O
in	O
which	O
a	O
classifier	B
gn	O
is	O
selected	O
from	O
c-is	O
of	O
course	O
important	O
sometimes	O
one	O
would	O
like	O
to	O
obtain	O
classifiers	O
that	O
are	O
invariant	O
under	O
certain	O
transformations	O
for	O
example	O
the	O
nearest	B
neighbor	I
classifier	B
is	O
not	O
invariant	O
under	O
nonlinear	O
transformations	O
of	O
the	O
coordinate	O
axes	O
this	O
is	O
a	O
drawback	O
as	O
components	O
are	O
often	O
measurements	O
in	O
an	O
arbitrary	O
scale	O
switching	O
to	O
a	O
logarithmic	O
scale	O
or	O
stretching	O
a	O
scale	O
out	O
by	O
using	O
fahrenheit	O
instead	O
of	O
celsius	O
should	O
not	O
affect	O
good	O
discrimination	O
rules	O
there	O
exist	O
variants	O
of	O
the	O
k-nearest	O
neighbor	O
rule	B
that	O
have	O
the	O
given	O
invariance	O
in	O
character	O
recognition	O
sometimes	O
all	O
components	O
of	O
a	O
vector	O
x	O
that	O
represents	O
introduction	O
a	O
character	O
are	O
true	O
measurements	O
involving	O
only	O
vector	O
differences	O
between	O
lected	O
points	O
such	O
as	O
the	O
leftmost	O
and	O
rightmost	O
points	O
the	O
geometric	B
center	O
the	O
weighted	B
center	O
of	O
all	O
black	O
pixels	O
the	O
topmost	O
and	O
bottommost	O
points	O
in	O
this	O
case	O
the	O
scale	O
has	O
essential	O
information	O
and	O
invariance	O
with	O
respect	O
to	O
changes	O
of	O
scale	O
would	O
be	O
detrimental	O
here	O
however	O
some	O
invariance	O
with	O
respect	O
to	O
orthonormal	O
rotations	O
is	O
healthy	O
we	O
follow	O
the	O
standard	B
notation	O
from	O
textbooks	O
on	O
probability	O
thus	O
random	O
variables	O
are	O
uppercase	O
characters	O
such	O
as	O
x	O
y	O
and	O
z	O
probability	O
measures	O
are	O
denoted	O
by	O
greek	O
letters	O
such	O
as	O
jl	O
and	O
numbers	O
and	O
vectors	O
are	O
denoted	O
by	O
lowercase	O
letters	O
such	O
as	O
a	O
b	O
c	O
x	O
and	O
y	O
sets	O
are	O
also	O
denoted	O
by	O
roman	O
capitals	O
but	O
there	O
are	O
obvious	O
mnemonics	O
s	O
denotes	O
a	O
sphere	B
b	O
denotes	O
a	O
borel	O
set	O
and	O
so	O
forth	O
if	O
we	O
need	O
many	O
kinds	O
of	O
sets	O
we	O
will	O
typically	O
use	O
the	O
beginning	O
of	O
the	O
alphabet	O
b	O
c	O
most	O
functions	O
are	O
denoted	O
by	O
j	O
g	O
and	O
calligraphic	O
letters	O
such	O
as	O
a	O
c	O
andf	O
are	O
used	O
to	O
denote	O
classes	O
of	O
functions	O
or	O
sets	O
a	O
short	O
list	O
of	O
frequently	O
used	O
symbols	O
is	O
found	O
at	O
the	O
end	O
of	O
the	O
book	O
at	O
the	O
end	O
of	O
this	O
chapter	O
you	O
will	O
find	O
a	O
directed	O
acyclic	O
graph	O
that	O
describes	O
the	O
dependence	O
between	O
chapters	O
clearly	O
prospective	O
teachers	O
will	O
have	O
to	O
select	O
small	O
subsets	O
of	O
chapters	O
all	O
chapters	O
without	O
exception	O
are	O
unashamedly	O
retical	O
we	O
did	O
not	O
scar	O
the	O
pages	O
with	O
backbreaking	O
simulations	O
or	O
quick	O
engineering	O
solutions	O
the	O
methods	O
gleaned	O
from	O
this	O
text	O
must	O
be	O
supplemented	O
with	O
a	O
healthy	O
dose	O
of	O
engineering	O
savvy	O
ideally	O
students	O
should	O
have	O
a	O
panion	O
text	O
filled	O
with	O
beautiful	O
applications	O
such	O
as	O
automated	O
virus	O
recognition	O
telephone	O
eavesdropping	O
language	O
recognition	O
voice	O
recognition	O
in	O
security	O
tems	O
fingerprint	O
recognition	O
or	O
handwritten	O
character	O
recognition	O
to	O
run	O
a	O
real	O
pattern	O
recognition	O
project	O
from	O
scratch	O
several	O
classical	O
texts	O
on	O
statistical	O
tern	O
recognition	O
could	O
and	O
should	O
be	O
consulted	O
as	O
our	O
work	O
is	O
limited	O
to	O
general	O
probability-theoretical	O
aspects	O
of	O
pattern	O
recognition	O
we	O
have	O
over	O
exercises	O
to	O
help	O
the	O
scholars	O
these	O
include	O
skill	O
honing	O
exercises	O
brainteasers	O
cute	O
zles	O
open	O
problems	O
and	O
serious	O
mathematical	O
challenges	O
there	O
is	O
no	O
solution	O
manual	O
this	O
book	O
is	O
only	O
a	O
start	O
use	O
it	O
as	O
a	O
toy-read	O
some	O
proofs	O
enjoy	O
some	O
inequalities	O
learn	O
new	O
tricks	O
and	O
study	O
the	O
art	O
of	O
camouflaging	O
one	O
problem	O
to	O
look	O
like	O
another	O
learn	O
for	O
the	O
sake	O
of	O
learning	B
introduction	O
introduction	O
inequalities	O
and	O
alternate	O
distance	O
measures	O
the	O
bayes	B
error	I
linear	O
discrimination	O
nearest	B
neighbor	I
rules	O
consistency	B
slow	O
rates	O
of	O
convergence	O
error	B
estimation	B
the	O
regular	B
histogram	B
rule	B
kernel	B
rules	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
vapnik-chervonenkis	B
theoi	O
combinatorial	O
aspects	O
of	O
v	O
apnik	O
theory	O
lower	B
bounds	I
for	I
empirical	B
classifier	B
selection	I
the	O
maximum	B
likelihood	I
principle	O
parametric	B
classification	I
generalized	B
linear	O
discrimination	O
complexity	B
regularization	I
condensed	B
and	O
edited	B
nearest	B
neighbor	I
rules	O
tree	O
classifiers	O
data-dependent	B
partitioning	O
splitting	B
the	I
data	I
the	O
resubstitution	B
estimate	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
automatic	B
kernel	B
rules	O
automatic	B
nearest	B
neighbor	I
rules	O
hypercubes	O
and	O
discrete	O
spaces	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
uniform	B
laws	I
of	I
large	I
numbers	I
neural	O
networks	O
other	O
error	O
estimates	O
feature	B
extraction	I
figure	O
the	O
bayes	B
error	I
the	O
bayes	B
problem	I
in	O
this	O
section	O
we	O
define	O
the	O
mathematical	O
model	O
and	O
introduce	O
the	O
notation	O
we	O
will	O
use	O
for	O
the	O
entire	O
book	O
let	O
y	O
be	O
a	O
pair	O
of	O
random	O
variables	O
taking	O
their	O
respective	O
values	O
from	O
rd	O
and	O
i	O
the	O
random	O
pair	O
y	O
may	O
be	O
described	O
in	O
a	O
variety	O
of	O
ways	O
for	O
example	O
it	O
is	O
defined	O
by	O
the	O
pair	O
where	O
il	O
is	O
the	O
probability	O
measure	O
for	O
x	O
and	O
is	O
the	O
regression	O
of	O
y	O
on	O
x	O
more	O
precisely	O
for	O
a	O
borel-measurable	O
set	O
a	O
s	O
r	O
d	O
ila	O
px	O
e	O
a	O
and	O
for	O
any	O
x	O
e	O
r	O
d	O
ply	O
llx	O
x	O
eyix	O
x	O
thus	O
is	O
the	O
conditional	O
probability	O
that	O
y	O
is	O
given	O
x	O
x	O
to	O
see	O
that	O
this	O
suffices	O
to	O
describe	O
the	O
distribution	O
of	O
y	O
observe	O
that	O
for	O
any	O
c	O
s	O
rd	O
x	O
i	O
we	O
have	O
c	O
n	O
x	O
u	O
n	O
x	O
dt	O
co	O
x	O
u	O
x	O
and	O
px	O
y	O
e	O
c	O
px	O
e	O
co	O
y	O
o	O
px	O
e	O
y	O
i	O
lco	O
lci	O
the	O
bayes	B
error	I
as	O
this	O
is	O
valid	O
for	O
any	O
borel-measurable	O
set	O
c	O
the	O
distribution	O
of	O
y	O
is	O
mined	O
by	O
ry	O
the	O
function	O
ry	O
is	O
sometimes	O
called	O
the	O
a	B
posteriori	I
probability	I
any	O
function	O
g	O
n	O
d	O
defines	O
a	O
classifier	B
or	O
a	O
decision	O
function	O
the	O
error	O
probability	O
of	O
g	O
is	O
lg	O
pgx	O
y	O
of	O
particular	O
interest	O
is	O
the	O
bayes	O
decision	O
function	O
g	O
if	O
ryx	O
otherwise	O
this	O
decision	O
function	O
minimizes	O
the	O
error	O
probability	O
theorem	O
for	O
any	O
decision	O
function	O
g	O
nd	O
pgx	O
y	O
pgx	O
y	O
that	O
is	O
g	O
is	O
the	O
optimal	O
decision	O
proof	O
given	O
x	O
x	O
the	O
conditional	O
error	O
probability	O
of	O
any	O
decision	O
g	O
may	O
be	O
expressed	O
as	O
pgx	O
yix	O
x	O
py	O
gxix	O
x	O
gx	O
x	O
gx	O
xd	O
iix	O
x	O
igxopy	O
oix	O
x	O
fgxo	O
ryx	O
where	O
fa	O
denotes	O
the	O
indicator	O
of	O
the	O
set	O
a	O
thus	O
for	O
every	O
x	O
e	O
n	O
d	O
pgx	O
yix	O
x	O
pgx	O
yix	O
x	O
ryx	O
fgexl	O
ryx	O
igxo	O
by	O
the	O
definition	B
of	I
g	O
the	O
statement	O
now	O
follows	O
by	O
integrating	O
both	O
sides	O
with	O
respect	O
to	O
il-dx	O
decide	O
class	O
figure	O
the	O
bayes	O
decision	O
in	O
the	O
example	O
on	O
the	O
left	O
is	O
if	O
x	O
a	O
and	O
otherwise	O
a	O
simple	O
example	O
remark	O
g	O
is	O
called	O
the	O
bayes	O
decision	O
and	O
l	O
pgx	O
y	O
is	O
referred	O
to	O
as	O
the	O
bayes	O
probability	O
of	O
error	O
bayes	B
error	I
or	O
bayes	O
risk	O
the	O
proof	O
given	O
above	O
reveals	O
that	O
lg	O
e	O
igxol	O
ryx	O
and	O
in	O
particular	O
l	O
e	O
ryx	O
we	O
observe	O
that	O
the	O
a	B
posteriori	I
probability	I
ryx	O
py	O
lx	O
x	O
eyx	O
x	O
minimizes	O
the	O
squared	B
error	I
when	O
y	O
is	O
to	O
be	O
predicted	O
by	O
fx	O
for	O
some	O
function	O
f	O
rd	O
r	O
e	O
e	O
to	O
see	O
why	O
the	O
above	O
inequality	B
is	O
true	O
observe	O
that	O
for	O
each	O
x	O
e	O
r	O
d	O
e	O
x	O
e	O
ryx	O
x	O
ryx	O
ryx	O
x	O
ryx	O
eryx	O
yix	O
x	O
ryx	O
e	O
x	O
the	O
conditional	O
median	O
i	O
e	O
the	O
function	O
minimizing	O
the	O
absolute	B
error	I
e	O
i	O
f	O
y	O
i	O
is	O
even	O
more	O
closely	O
related	O
to	O
the	O
bayes	O
rule	B
problem	O
a	O
simple	O
example	O
let	O
us	O
consider	O
the	O
prediction	O
of	O
a	O
students	O
performance	O
in	O
a	O
course	O
when	O
given	O
a	O
number	O
of	O
important	O
factors	O
first	O
let	O
y	O
denote	O
a	O
pass	O
and	O
let	O
y	O
stand	O
for	O
failure	O
the	O
sole	O
observation	O
x	O
is	O
the	O
number	O
of	O
hours	O
of	O
study	O
per	O
week	O
this	O
in	O
itself	O
is	O
not	O
a	O
foolproof	O
predictor	O
of	O
a	O
students	O
performance	O
because	O
for	O
that	O
we	O
would	O
need	O
more	O
information	O
about	O
the	O
students	O
quickness	O
of	O
mind	O
health	O
and	O
social	O
habits	O
the	O
regression	B
function	I
ryx	O
py	O
x	O
is	O
probably	O
monotonically	O
increasing	O
in	O
x	O
if	O
it	O
were	O
known	O
to	O
be	O
ryx	O
x	O
x	O
c	O
say	O
our	O
problem	O
would	O
be	O
solved	O
because	O
the	O
bayes	O
decision	O
is	O
gx	O
if	O
ryx	O
x	O
c	O
otherwise	O
the	O
bayes	B
error	I
the	O
corresponding	O
bayes	B
error	I
is	O
l	O
lg	O
e	O
x	O
ex	O
while	O
we	O
could	O
deduce	O
the	O
bayes	O
decision	O
from	O
alone	O
the	O
same	O
cannot	O
be	O
said	O
for	O
the	O
bayes	B
error	I
l	O
requires	O
knowledge	O
of	O
the	O
distribution	O
of	O
x	O
if	O
x	O
e	O
with	O
probability	O
one	O
in	O
an	O
army	O
school	O
where	O
all	O
students	O
are	O
forced	O
to	O
study	O
e	O
hours	O
per	O
week	O
then	O
l	O
if	O
we	O
have	O
a	O
population	O
that	O
is	O
nicely	O
spread	O
out	O
say	O
x	O
is	O
uniform	O
on	O
then	O
the	O
situation	O
improves	O
mince	O
x	O
e	O
x	O
l	O
dx	O
far	O
away	O
from	O
x	O
e	O
discrimination	O
is	O
really	O
simple	O
in	O
general	O
discrimination	O
is	O
much	O
easier	O
than	O
estimation	B
because	O
of	O
this	O
phenomenon	O
another	O
simple	O
example	O
let	O
us	O
work	O
out	O
a	O
second	O
simple	O
example	O
in	O
which	O
y	O
or	O
y	O
according	O
to	O
whether	O
a	O
student	O
fails	O
or	O
passes	O
a	O
course	O
x	O
represents	O
one	O
or	O
more	O
observations	O
regarding	O
the	O
student	O
the	O
components	O
of	O
x	O
in	O
our	O
example	O
will	O
be	O
denoted	O
by	O
t	O
b	O
and	O
e	O
respectively	O
where	O
t	O
is	O
the	O
average	O
number	O
of	O
hours	O
the	O
students	O
watches	O
tv	O
b	O
is	O
the	O
average	O
number	O
of	O
beers	O
downed	O
each	O
day	O
and	O
e	O
is	O
an	O
intangible	O
quantity	O
measuring	O
extra	O
negative	O
factors	O
such	O
as	O
laziness	O
and	O
learning	B
difficulties	O
in	O
our	O
cooked-up	O
example	O
we	O
have	O
yl	O
otherwise	O
thus	O
if	O
t	O
b	O
and	O
e	O
are	O
known	O
y	O
is	O
known	O
as	O
well	O
the	O
bayes	O
classifier	B
decides	O
if	O
t	O
b	O
e	O
and	O
otherwise	O
the	O
corresponding	O
bayes	O
probability	O
of	O
error	O
is	O
zero	O
unfortunately	O
e	O
is	O
intangible	O
and	O
not	O
available	O
to	O
the	O
observer	O
we	O
only	O
have	O
access	O
to	O
t	O
and	O
b	O
given	O
t	O
and	O
b	O
when	O
should	O
we	O
guess	O
that	O
y	O
i	O
to	O
answer	O
this	O
question	O
one	O
must	O
know	O
the	O
joint	O
distribution	O
of	O
b	O
e	O
or	O
equivalently	O
the	O
joint	O
distribution	O
of	O
b	O
y	O
so	O
let	O
us	O
assume	O
that	O
t	O
b	O
and	O
e	O
are	O
i	O
i	O
d	O
exponential	B
random	O
variables	O
they	O
have	O
density	O
e-u	O
on	O
the	O
bayes	O
rule	B
compares	O
py	O
lit	O
b	O
with	O
py	O
b	O
and	O
makes	O
a	O
decision	O
consistent	O
with	O
the	O
maximum	O
of	O
these	O
two	O
values	O
a	O
simple	O
calculation	O
shows	O
that	O
py	O
lit	O
b	O
pt	O
b	O
e	O
b	O
pre	O
t	O
bit	O
b	O
max	O
the	O
crossover	O
between	O
two	O
decisions	O
occurs	O
when	O
this	O
value	O
equals	O
thus	O
the	O
bayes	O
classifier	B
is	O
as	O
follows	O
gt	O
b	O
if	O
t	O
o	O
otherwise	O
another	O
simple	O
example	O
of	O
course	O
this	O
classifier	B
is	O
not	O
perfect	O
the	O
probability	O
of	O
error	O
is	O
pgt	O
b	O
y	O
pt	O
e	O
b	O
t	O
b	O
e	O
e	O
p	O
e	O
i	O
xe-x	O
dx	O
xe-x	O
dx	O
o	O
the	O
density	O
of	O
t	O
b	O
is	O
ue-u	O
on	O
log	O
e	O
jxoo	O
ue-udu	O
xe-	O
x	O
e	O
if	O
we	O
have	O
only	O
access	O
to	O
t	O
then	O
the	O
bayes	O
classifier	B
is	O
allowed	O
to	O
use	O
t	O
only	O
first	O
we	O
find	O
pyiit	O
max	O
the	O
crossover	O
at	O
occurs	O
at	O
t	O
c	O
so	O
that	O
the	O
bayes	O
classifier	B
is	O
given	O
by	O
def	O
gt	O
if	O
t	O
otherwise	O
the	O
probability	O
of	O
error	O
is	O
pgt	O
y	O
pt	O
c	O
t	O
b	O
e	O
pt	O
c	O
t	O
b	O
e	O
e	O
itcd	O
p	O
e-xl	O
dx	O
e-x	O
dx	O
e	O
the	O
bayes	B
error	I
has	O
increased	O
slightly	O
but	O
not	O
by	O
much	O
finally	O
if	O
we	O
do	O
not	O
have	O
access	O
to	O
any	O
of	O
the	O
three	O
variables	O
t	O
b	O
and	O
e	O
the	O
best	O
we	O
can	O
do	O
is	O
see	O
which	O
the	O
bayes	B
error	I
class	O
is	O
most	O
likely	O
to	O
this	O
end	O
we	O
compute	O
pry	O
o	O
pt	O
b	O
e	O
if	O
we	O
set	O
g	O
all	O
the	O
time	O
we	O
make	O
an	O
error	O
with	O
probability	O
in	O
practice	O
bayes	O
classifiers	O
are	O
unknown	O
simply	O
because	O
the	O
distribution	O
of	O
y	O
is	O
unknown	O
consider	O
a	O
classifier	B
based	O
upon	O
b	O
rosenblatts	O
tron	O
chapter	O
looks	O
for	O
the	O
best	O
linear	B
classifier	B
based	O
upon	O
the	O
data	O
that	O
is	O
the	O
decision	O
is	O
of	O
the	O
form	O
i	O
get	O
b	O
if	O
at	O
bb	O
c	O
otherwise	O
for	O
some	O
data-based	B
choices	O
for	O
a	O
band	O
c	O
if	O
we	O
have	O
lots	O
of	O
data	O
at	O
our	O
disposal	O
then	O
it	O
is	O
possible	O
to	O
pick	O
out	O
a	O
linear	B
classifier	B
that	O
is	O
nearly	O
optimal	O
as	O
we	O
have	O
seen	O
above	O
the	O
bayes	O
classifier	B
happens	O
to	O
be	O
linear	O
that	O
is	O
a	O
sheer	O
coincidence	O
of	O
course	O
if	O
the	O
bayes	O
classifier	B
had	O
not	O
been	O
linear-for	O
example	O
if	O
we	O
had	O
y	O
even	O
the	O
best	O
perceptron	B
would	O
be	O
suboptimal	O
regardless	O
of	O
how	O
many	O
data	O
pairs	O
one	O
would	O
have	O
if	O
we	O
use	O
the	O
neighbor	O
rule	B
the	O
asymptotic	O
probability	O
of	O
error	O
is	O
not	O
more	O
than	O
times	O
the	O
bayes	B
error	I
which	O
in	O
our	O
example	O
is	O
about	O
the	O
example	O
above	O
also	O
shows	O
the	O
need	O
to	O
look	O
at	O
individual	O
components	O
and	O
to	O
evaluate	O
how	O
many	O
and	O
which	O
components	O
would	O
be	O
most	O
useful	O
for	O
discrimination	O
this	O
subject	O
is	O
covered	O
in	O
the	O
chapter	O
on	O
feature	B
extraction	I
other	O
formulas	O
for	O
the	O
bayes	O
risk	O
the	O
following	O
forms	O
of	O
the	O
bayes	B
error	I
are	O
often	O
convenient	O
l	O
inf	O
pgx	O
y	O
grcol	O
e	O
e	O
ii	O
figure	O
the	O
bayes	O
decision	O
when	O
conditional	O
densities	O
exist	O
in	O
the	O
figure	O
on	O
the	O
left	O
the	O
decision	O
is	O
on	O
b	O
and	O
elsewhere	O
pi	O
class	O
i	O
class	O
class	O
plug-in	O
decisions	O
in	O
special	O
cases	O
we	O
may	O
obtain	O
other	O
helpful	O
forms	O
for	O
example	O
if	O
x	O
has	O
a	O
density	O
f	O
then	O
l	O
f	O
f	O
minl	O
pfox	O
plixdx	O
where	O
p	O
py	O
i	O
and	O
fix	O
is	O
the	O
density	O
of	O
x	O
given	O
that	O
y	O
i	O
p	O
and	O
p	O
are	O
called	O
the	O
class	O
probabilities	O
and	O
fa	O
fl	O
are	O
the	O
class-conditional	B
densities	O
if	O
fa	O
and	O
ii	O
are	O
nonoverlapping	O
that	O
is	O
f	O
fofl	O
then	O
obviously	O
l	O
assume	O
moreover	O
that	O
p	O
then	O
l	O
f	O
min	O
flxdx	O
f	O
ii	O
f	O
iflx	O
foxldx	O
fox	O
dx	O
here	O
g	O
denotes	O
the	O
positive	O
part	O
of	O
a	O
function	O
g	O
thus	O
the	O
bayes	B
error	I
is	O
directly	O
related	O
to	O
the	O
l	O
distance	O
between	O
the	O
class	O
densities	O
figure	O
the	O
shaded	O
area	O
is	O
the	O
l	O
distance	O
between	O
the	O
class-conditional	B
densities	O
plug-in	O
decisions	O
the	O
best	O
guess	O
of	O
y	O
from	O
the	O
observation	O
x	O
is	O
the	O
bayes	O
decision	O
if	O
if	O
otherwise	O
g	O
otherwise	O
the	O
function	O
is	O
typically	O
unknown	O
assume	O
that	O
we	O
have	O
access	O
to	O
nonnegative	O
functions	O
ix	O
respectively	O
in	O
this	O
case	O
it	O
seems	O
natural	O
to	O
use	O
the	O
plug-in	B
decision	I
function	O
ix	O
that	O
approximate	O
and	O
gx	O
if	O
ix	O
otherwise	O
the	O
bayes	B
error	I
to	O
approximate	O
the	O
bayes	O
decision	O
the	O
next	O
well-known	O
theorem	O
e	O
g	O
van	O
ryzin	O
wolverton	O
and	O
wagner	O
glick	O
csibi	O
gyorfi	O
devroye	O
and	O
wagner	O
devroye	O
and	O
devroye	O
and	O
gyorfi	O
states	O
that	O
if	O
is	O
close	O
to	O
the	O
real	O
a	B
posteriori	I
probability	I
in	O
l	O
then	O
the	O
error	O
probability	O
of	O
decision	O
g	O
is	O
near	O
the	O
optimal	O
decision	O
g	O
theorem	O
for	O
the	O
error	O
probability	O
of	O
the	O
plug-in	B
decision	I
g	O
defined	O
above	O
we	O
have	O
pgx	O
y	O
l	O
lrd	O
and	O
pgx	O
y	O
l	O
s	O
r	O
lrd	O
proof	O
if	O
for	O
some	O
x	O
e	O
nd	O
gx	O
gx	O
then	O
clearly	O
the	O
difference	O
between	O
the	O
conditional	O
error	O
probabilities	O
of	O
g	O
and	O
g	O
is	O
zero	O
pgx	O
yix	O
x	O
pgx	O
yix	O
x	O
o	O
otherwise	O
if	O
gx	O
gx	O
then	O
as	O
seen	O
in	O
the	O
proof	O
of	O
theorem	O
the	O
difference	O
may	O
be	O
written	O
as	O
pgx	O
yix	O
x	O
pgx	O
yix	O
x	O
igxl	O
liigx	O
gx	O
thus	O
pgx	O
y	O
l	O
jrd	O
since	O
gx	O
gx	O
implies	O
jrf	O
when	O
the	O
classifier	B
gx	O
can	O
be	O
put	O
in	O
the	O
form	O
s	O
g	O
otherwise	O
where	O
are	O
some	O
approximations	O
of	O
and	O
respectively	O
the	O
situation	O
differs	O
from	O
that	O
discussed	O
in	O
theorem	O
if	O
does	O
not	O
necessarily	O
equal	O
to	O
one	O
however	O
an	O
inequality	B
analogous	O
to	O
that	O
of	O
theorem	O
remains	O
true	O
bayes	B
error	I
versus	O
dimension	B
theorem	O
the	O
error	O
probability	O
of	O
the	O
decision	O
defined	O
above	O
is	O
bounded	O
from	O
above	O
by	O
pgx	O
y	O
l	O
ljx	O
jrd	O
iloxll-ldx	O
jrd	O
iljx	O
ill	O
the	O
proof	O
is	O
left	O
to	O
the	O
reader	O
remark	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
ii	O
exist	O
and	O
are	O
mated	O
by	O
the	O
densities	O
ir	O
assume	O
furthermore	O
that	O
the	O
class	O
probabilities	O
p	O
pry	O
and	O
p	O
py	O
o	O
are	O
approximated	O
by	O
pi	O
and	O
po	O
respectively	O
then	O
for	O
the	O
error	O
probability	O
of	O
the	O
plug-in	B
decision	I
function	O
gx	O
if	O
pijix	O
pojox	O
otherwise	O
pgx	O
y	O
l	O
pfoex	O
pojoexldx	O
jrd	O
jrd	O
ipfiex	O
piirexldx	O
see	O
problem	O
bayes	B
error	I
versus	O
dimension	B
the	O
components	O
of	O
x	O
that	O
matter	O
in	O
the	O
bayes	O
classifier	B
are	O
those	O
that	O
explicitly	O
appear	O
in	O
ljx	O
in	O
fact	O
then	O
all	O
discrimination	O
problems	O
are	O
one-dimensional	O
as	O
we	O
could	O
equally	O
well	O
replace	O
x	O
by	O
ljex	O
or	O
by	O
any	O
strictly	O
monotone	O
function	O
of	O
such	O
as	O
lj	O
ex	O
ljx	O
unfortunately	O
lj	O
is	O
unknown	O
in	O
general	O
in	O
the	O
example	O
in	O
section	O
we	O
had	O
in	O
one	O
case	O
ljt	O
b	O
max	O
t-	O
b	O
and	O
in	O
another	O
case	O
ljet	O
max	O
the	O
former	O
format	O
suggests	O
that	O
we	O
could	O
base	O
all	O
decisions	O
on	O
t	O
b	O
this	O
means	O
that	O
if	O
we	O
had	O
no	O
access	O
to	O
t	O
and	O
b	O
individually	O
but	O
to	O
t	O
b	O
jointly	O
we	O
would	O
be	O
able	O
to	O
achieve	O
the	O
same	O
results	O
since	O
lj	O
is	O
unknown	O
all	O
of	O
this	O
is	O
really	O
irrelevant	O
in	O
general	O
the	O
bayes	O
risk	O
increases	O
if	O
we	O
replace	O
x	O
by	O
tex	O
for	O
any	O
formation	O
t	O
problem	O
as	O
this	O
destroys	O
information	O
on	O
the	O
other	O
hand	O
there	O
exist	O
transformations	O
as	O
ljx	O
that	O
leave	O
the	O
bayes	B
error	I
untouched	O
for	O
more	O
on	O
the	O
relationship	O
between	O
the	O
bayes	B
error	I
and	O
the	O
dimension	B
refer	O
to	O
chapter	O
the	O
bayes	B
error	I
problems	O
and	O
exercises	O
problem	O
let	O
t	O
x	O
x	O
be	O
an	O
arbitrary	O
measurable	O
function	O
if	O
l	O
and	O
lx	O
denote	O
the	O
bayes	B
error	I
probabilities	O
for	O
y	O
and	O
y	O
respectively	O
then	O
prove	O
that	O
shows	O
that	O
transformations	O
of	O
x	O
destroy	O
information	O
because	O
the	O
bayes	O
risk	O
creases	O
problem	O
let	O
x	O
be	O
independent	O
of	O
y	O
prove	O
that	O
lexxf	O
l	O
problem	O
show	O
that	O
l	O
minp	O
p	O
where	O
p	O
p	O
are	O
the	O
class	O
probabilities	O
show	O
that	O
equality	O
holds	O
if	O
x	O
and	O
y	O
are	O
independent	O
exhibit	O
a	O
distribution	O
where	O
x	O
is	O
not	O
independent	O
of	O
y	O
but	O
l	O
rninp	O
p	O
problem	O
neyman-pearson	B
lemma	I
consider	O
again	O
the	O
decision	O
problem	O
but	O
with	O
a	O
decision	O
g	O
we	O
now	O
assign	O
two	O
error	O
probabilities	O
log	O
pgx	O
o	O
and	O
pgx	O
oiy	O
i	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
fl	O
exist	O
for	O
c	O
define	O
the	O
decision	O
if	O
cflx	O
otherwise	O
prove	O
that	O
for	O
any	O
decision	O
g	O
if	O
log	O
logc	O
then	O
llgc	O
in	O
other	O
words	O
if	O
l	O
is	O
required	O
to	O
be	O
kept	O
under	O
a	O
certain	O
level	O
then	O
the	O
decision	O
minimizing	O
l	O
has	O
the	O
form	O
of	O
gc	O
for	O
some	O
c	O
note	O
that	O
g	O
is	O
like	O
that	O
problem	O
decisions	O
with	O
rejection	O
sometimes	O
in	O
decision	O
problems	O
one	O
is	O
allowed	O
to	O
say	O
dont	O
know	O
if	O
this	O
does	O
not	O
happen	O
frequently	O
these	O
decisions	O
are	O
called	O
decisions	O
with	O
a	O
reject	O
option	O
e	O
g	O
forney	O
chow	O
formally	O
a	O
decision	O
gx	O
can	O
have	O
three	O
values	O
and	O
there	O
are	O
two	O
performance	O
measures	O
the	O
probability	O
of	O
rejection	O
pgx	O
and	O
the	O
error	O
probability	O
pgx	O
i	O
ylgx	O
i	O
for	O
a	O
c	O
define	O
the	O
decision	O
gcx	O
if	O
ryx	O
c	O
ifryx	O
c	O
otherwise	O
show	O
that	O
for	O
any	O
decision	O
g	O
if	O
pgx	O
pgcx	O
then	O
pgx	O
i	O
y	O
gx	O
i	O
pgcx	O
i	O
ylgcx	O
i	O
thus	O
to	O
keep	O
the	O
probability	O
of	O
rejection	O
under	O
a	O
certain	O
level	O
decisions	O
of	O
the	O
form	O
of	O
gc	O
are	O
optimal	O
gyorfi	O
and	O
vajda	O
problems	O
and	O
exercises	O
problem	O
consider	O
the	O
prediction	O
of	O
a	O
students	O
failure	O
based	O
upon	O
variables	O
t	O
and	O
b	O
where	O
y	O
and	O
e	O
is	O
an	O
inaccessible	O
variable	B
section	O
let	O
t	O
b	O
and	O
e	O
be	O
independent	O
merely	O
by	O
changing	O
the	O
distribution	O
of	O
e	O
show	O
that	O
the	O
bayes	B
error	I
for	O
classification	O
based	O
upon	O
b	O
can	O
be	O
made	O
as	O
close	O
as	O
desired	O
to	O
let	O
t	O
and	O
b	O
be	O
independent	O
and	O
exponentially	O
distributed	O
find	O
ajoint	O
distribution	O
of	O
b	O
e	O
such	O
that	O
the	O
bayes	O
classifier	B
is	O
not	O
a	O
linear	B
classifier	B
let	O
t	O
and	O
b	O
be	O
independent	O
and	O
exponentially	O
distributed	O
find	O
a	O
joint	O
distribution	O
of	O
b	O
e	O
such	O
that	O
the	O
bayes	O
classifier	B
is	O
given	O
by	O
gt	O
b	O
if	O
otherwise	O
find	O
the	O
bayes	O
classifier	B
and	O
bayes	B
error	I
for	O
classification	O
based	O
on	O
b	O
y	O
as	O
above	O
if	O
b	O
e	O
is	O
uniformly	O
distributed	O
on	O
problem	O
assume	O
that	O
t	O
b	O
and	O
e	O
are	O
independent	O
uniform	O
random	O
variables	O
with	O
interpretations	O
as	O
in	O
section	O
let	O
y	O
denote	O
whether	O
a	O
student	O
passes	O
a	O
course	O
assume	O
that	O
y	O
if	O
and	O
only	O
if	O
t	O
b	O
e	O
find	O
the	O
bayes	O
decision	O
if	O
no	O
variable	B
is	O
available	O
if	O
only	O
t	O
is	O
available	O
and	O
if	O
only	O
t	O
and	O
b	O
are	O
available	O
determine	O
in	O
all	O
three	O
cases	O
the	O
bayes	B
error	I
determine	O
the	O
best	O
linear	B
classifier	B
based	O
upon	O
t	O
and	O
b	O
only	O
problem	O
let	O
nd	O
be	O
arbitrary	O
measurable	O
functions	O
and	O
define	O
the	O
corresponding	O
decisions	O
by	O
gx	O
itx	O
and	O
gx	O
itix	O
prove	O
that	O
lgff	O
pgx	O
i	O
gffx	O
and	O
e	O
problem	O
prove	O
theorem	O
problem	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
and	O
fi	O
exist	O
and	O
are	O
proximated	O
by	O
the	O
densities	O
fo	O
and	O
a	O
respectively	O
assume	O
furthermore	O
that	O
the	O
class	O
probabilities	O
p	O
pry	O
i	O
and	O
p	O
pry	O
o	O
are	O
approximated	O
by	O
pi	O
and	O
po	O
prove	O
that	O
for	O
the	O
error	O
probability	O
of	O
the	O
plug-in	B
decision	I
function	O
if	O
pi	O
a	O
po	O
ex	O
otherwise	O
gx	O
we	O
have	O
pgx	O
i	O
y	O
l	O
llpfl	O
pia	O
exldx	O
pfox	O
poloxldx	O
nd	O
problem	O
using	O
the	O
notation	O
of	O
problem	O
show	O
that	O
if	O
for	O
a	O
sequence	O
of	O
fmn	O
and	O
pmn	O
nd	O
the	O
bayes	B
error	I
then	O
for	O
the	O
corresponding	O
sequence	O
of	O
plug-in	O
decisions	O
limn---oo	O
pgnx	O
yj	O
l	O
and	O
wagner	O
hint	O
according	O
to	O
problem	O
it	O
suffices	O
to	O
show	O
that	O
if	O
we	O
are	O
given	O
a	O
deterministic	O
sequence	O
of	O
density	O
functions	O
f	O
h	O
then	O
implies	O
n---oo	O
lim	O
f	O
lim	O
f	O
ifnx	O
fx	O
dx	O
fxldx	O
o	O
function	O
f	O
is	O
called	O
a	O
density	O
function	O
if	O
it	O
is	O
nonnegative	O
and	O
f	O
fxdx	O
to	O
see	O
this	O
observe	O
that	O
where	O
ai	O
a	O
is	O
a	O
partition	B
of	O
n	O
d	O
into	O
unit	O
cubes	O
and	O
f	O
denotes	O
the	O
positive	O
part	O
of	O
a	O
function	O
f	O
the	O
key	O
observation	O
is	O
that	O
convergence	O
to	O
zero	O
of	O
each	O
term	O
of	O
the	O
infinite	O
sum	O
implies	O
convergence	O
of	O
the	O
whole	O
integral	O
by	O
the	O
dominated	B
convergence	I
theorem	I
since	O
f	O
fx	O
dx	O
f	O
fnxdx	O
handle	O
the	O
right-hand	O
side	O
by	O
the	O
cauchy-schwarz	B
inequality	B
problem	O
define	O
the	O
ll	O
error	O
of	O
a	O
function	O
f	O
n	O
d	O
n	O
by	O
jf	O
elfx	O
yi	O
show	O
that	O
a	O
function	O
minimizing	O
j	O
is	O
the	O
bayes	O
rule	B
g	O
that	O
is	O
j	O
inf	O
f	O
j	O
j	O
thus	O
j	O
l	O
define	O
a	O
decision	O
by	O
gx	O
if	O
fxs	O
otherwise	O
prove	O
that	O
its	O
error	O
probability	O
lg	O
pgx	O
y	O
satisfies	O
the	O
inequality	B
lg	O
l	O
jf	O
j	O
inequalities	O
and	O
alternate	O
distance	O
measures	O
measuring	O
discriminatory	O
information	O
in	O
our	O
two-class	O
discrimination	O
problem	O
the	O
best	O
rule	B
has	O
probability	O
of	O
error	O
l	O
e	O
this	O
quantity	O
measures	O
how	O
difficult	O
the	O
discrimination	O
problem	O
is	O
it	O
also	O
serves	O
as	O
a	O
gauge	O
of	O
the	O
quality	O
of	O
the	O
distribution	O
of	O
y	O
for	O
pattern	O
recognition	O
put	O
differently	O
if	O
and	O
are	O
certain	O
many-to-one	O
mappings	O
l	O
may	O
be	O
used	O
to	O
compare	O
discrimination	O
based	O
on	O
y	O
with	O
that	O
based	O
on	O
y	O
when	O
projects	O
n	O
d	O
to	O
n	O
d	O
by	O
taking	O
the	O
first	O
d	O
coordinates	O
and	O
takes	O
the	O
last	O
coordinates	O
the	O
corresponding	O
bayes	O
errors	O
will	O
help	O
us	O
decide	O
which	O
projection	O
is	O
better	O
in	O
this	O
sense	O
l	O
is	O
the	O
fundamental	O
quantity	O
in	O
feature	B
extraction	I
other	O
quantities	O
have	O
been	O
suggested	O
over	O
the	O
years	O
that	O
measure	O
the	O
natory	O
power	O
hidden	O
in	O
the	O
distribution	O
of	O
y	O
these	O
may	O
be	O
helpful	O
in	O
some	O
settings	O
for	O
example	O
in	O
theoretical	B
studies	O
or	O
in	O
certain	O
proofs	O
the	O
relationship	O
tween	O
l	O
and	O
the	O
distribution	O
of	O
y	O
may	O
become	O
clearer	O
via	O
certain	O
inequalities	O
that	O
link	O
l	O
with	O
other	O
functionals	O
of	O
the	O
distribution	O
we	O
all	O
understand	O
moments	O
and	O
variances	O
but	O
how	O
do	O
these	O
simple	O
functionals	O
relate	O
to	O
l	O
perhaps	O
we	O
may	O
even	O
learn	O
a	O
thing	O
or	O
two	O
about	O
what	O
it	O
is	O
that	O
makes	O
l	O
small	O
in	O
feature	O
selection	O
some	O
explicit	O
inequalities	O
involving	O
l	O
may	O
provide	O
just	O
the	O
kind	O
of	O
numerical	O
formation	O
that	O
will	O
allow	O
one	O
to	O
make	O
certain	O
judgements	O
on	O
what	O
kind	O
of	O
feature	O
is	O
preferable	O
in	O
practice	O
in	O
short	O
we	O
will	O
obtain	O
more	O
information	O
about	O
l	O
with	O
a	O
variety	O
of	O
uses	O
in	O
pattern	O
recognition	O
inequalities	O
and	O
alternate	O
distance	O
measures	O
in	O
the	O
next	O
few	O
sections	O
we	O
avoid	O
putting	O
any	O
conditions	O
on	O
the	O
distribution	O
of	O
y	O
the	O
kolmogorov	B
variational	I
distance	I
inspired	O
by	O
the	O
total	B
variation	B
distance	O
between	O
distributions	O
the	O
kolmogorov	B
variational	I
distance	I
bko	O
iix	O
pry	O
dixi	O
e	O
captures	O
the	O
distance	O
between	O
the	O
two	O
classes	O
we	O
will	O
not	O
need	O
anything	O
special	O
to	O
deal	O
with	O
bko	O
as	O
l	O
e	O
g	O
ii	O
i	O
i	O
okq	O
the	O
nearest	B
neighbor	I
error	I
the	O
asymptotic	O
error	O
of	O
the	O
nearest	B
neighbor	I
rule	B
is	O
lnn	O
ryx	O
chapter	O
clearly	O
as	O
also	O
using	O
the	O
notation	O
a	O
ryx	O
we	O
have	O
l	O
lnn	O
a	O
a	O
the	O
second	O
association	B
inequality	B
of	O
theorem	O
which	O
are	O
well-known	O
inequalities	O
of	O
cover	O
and	O
hart	O
lnn	O
provides	O
us	O
with	O
quite	O
a	O
bit	O
of	O
information	O
about	O
l	O
the	O
measure	O
lnn	O
has	O
been	O
rediscovered	O
under	O
other	O
guises	O
devijver	O
and	O
kittler	O
and	O
vajda	O
call	O
it	O
the	O
quadratic	O
entropy	B
and	O
mathai	O
and	O
rathie	O
refer	O
to	O
it	O
as	O
the	O
harmonic	O
mean	O
coefficient	O
the	O
bhattacharyya	B
affinity	I
figure	O
relationship	O
between	O
the	O
bayes	B
error	I
and	O
the	O
asymptotic	O
est	O
neighbor	O
error	O
every	O
point	O
in	O
the	O
unshaded	O
region	O
is	O
possible	O
lnn	O
o	O
the	O
bhattacharyya	B
affinity	I
the	O
bhattacharyya	O
measure	O
of	O
affinity	O
is	O
log	O
p	O
where	O
p	O
e	O
will	O
be	O
referred	O
to	O
as	O
the	O
matushita	B
error	I
it	O
does	O
not	O
occur	O
naturally	O
as	O
the	O
limit	O
of	O
any	O
standard	B
discrimination	O
rule	B
however	O
problem	O
p	O
was	O
suggested	O
as	O
a	O
distance	O
measure	O
for	O
pattern	O
recognition	O
by	O
matushita	O
it	O
also	O
occurs	O
under	O
other	O
guises	O
in	O
mathematical	O
statistics-see	O
for	O
example	O
the	O
hellinger	B
distance	I
literature	O
cam	O
beran	O
clearly	O
p	O
if	O
and	O
only	O
if	O
e	O
i	O
with	O
probability	O
one	O
that	O
is	O
if	O
l	O
o	O
furthermore	O
p	O
takes	O
its	O
maximal	O
value	O
if	O
and	O
only	O
if	O
with	O
probability	O
one	O
the	O
relationship	O
between	O
p	O
and	O
l	O
is	O
not	O
linear	O
though	O
we	O
will	O
show	O
that	O
for	O
all	O
distributions	O
lnn	O
is	O
more	O
useful	O
than	O
p	O
if	O
it	O
is	O
to	O
be	O
used	O
as	O
an	O
approximation	O
of	O
l	O
theorem	O
for	O
all	O
distributions	O
we	O
have	O
t	O
nn	O
l	O
p	O
proof	O
first	O
of	O
all	O
e	O
jensens	O
inequality	B
inequalities	O
and	O
alternate	O
distance	O
measures	O
l	O
l	O
the	O
cover-hart	B
inequality	B
second	O
as	O
finally	O
by	O
the	O
cover-hart	B
inequality	B
for	O
all	O
e	O
we	O
see	O
that	O
p	O
lnn	O
l	O
l	O
putting	O
all	O
these	O
things	O
together	O
establishes	O
the	O
chain	O
of	O
inequalities	O
figure	O
the	O
inequalities	O
linking	O
p	O
to	O
l	O
are	O
illustrated	O
note	O
that	O
the	O
gion	O
is	O
larger	O
than	O
that	O
cut	O
out	O
in	O
the	O
lnn	O
plane	O
in	O
figure	O
p	O
the	O
inequality	B
lnn	O
p	O
is	O
due	O
to	O
ito	O
the	O
inequality	B
lnn	O
is	O
due	O
to	O
horibe	O
the	O
inequality	B
l	O
p	O
can	O
be	O
found	O
in	O
kailath	O
the	O
left-hand	O
side	O
of	O
the	O
last	O
inequality	B
was	O
shown	O
by	O
hudimoto	O
all	O
these	O
inequalities	O
are	O
tight	O
problem	O
the	O
appeal	O
of	O
quantities	O
like	O
lnn	O
and	O
p	O
is	O
that	O
they	O
involve	O
polynomials	O
of	O
whereas	O
l	O
is	O
nonpolynomial	O
for	O
certain	O
discrimination	O
problems	O
in	O
which	O
x	O
has	O
a	O
distribution	O
that	O
is	O
known	O
up	O
to	O
certain	O
parameters	O
one	O
may	O
be	O
able	O
to	O
compute	O
lnn	O
and	O
p	O
explicitly	O
as	O
a	O
function	O
of	O
these	O
parameters	O
via	O
inequalities	O
this	O
may	O
then	O
be	O
used	O
to	O
obtain	O
performance	O
guarantees	O
for	O
parametric	O
discrimination	O
rules	O
of	O
the	O
plug-in	O
type	O
chapter	O
for	O
completeness	O
we	O
mention	O
a	O
generalization	O
of	O
bhattacharyyas	O
measure	O
of	O
affinity	O
first	O
suggested	O
by	O
chernoff	O
where	O
ex	O
e	O
is	O
fixed	O
for	O
ex	O
be	O
p	O
the	O
asymmetry	O
introduced	O
by	O
taking	O
ex	O
has	O
no	O
practical	O
interpretation	O
however	O
entropy	B
the	O
entropy	B
of	O
a	O
discrete	O
probability	O
distribution	O
is	O
defined	O
by	O
entropy	B
lpi	O
log	O
pi	O
where	O
by	O
definition	O
log	O
the	O
key	O
quantity	O
in	O
information	O
theory	O
cover	O
and	O
thomas	O
it	O
has	O
countless	O
applications	O
in	O
many	O
branches	O
of	O
computer	O
science	O
mathematical	O
statistics	O
and	O
physics	O
the	O
entropys	O
main	O
properties	O
may	O
be	O
summarized	O
as	O
follows	O
a	O
with	O
equality	O
if	O
and	O
only	O
if	O
pi	O
for	O
some	O
i	O
proof	O
log	O
pi	O
for	O
all	O
i	O
with	O
equality	O
if	O
and	O
only	O
if	O
pi	O
for	O
some	O
i	O
thus	O
entropy	B
is	O
minimal	O
for	O
a	O
degenerate	O
distribution	O
i	O
e	O
a	O
distribution	O
with	O
the	O
least	O
amount	O
of	O
b	O
pk	O
log	O
k	O
with	O
equality	O
if	O
and	O
only	O
if	O
pi	O
pk	O
k	O
in	O
other	O
words	O
the	O
entropy	B
is	O
maximal	O
when	O
the	O
distribution	O
is	O
mally	O
smeared	O
out	O
proof	O
by	O
the	O
inequality	B
logx	O
x	O
x	O
o	O
c	O
for	O
a	O
bernoulli	O
distribution	O
p	O
the	O
binary	B
entropy	B
p	O
p	O
log	O
p	O
p	O
p	O
is	O
concave	O
in	O
p	O
assume	O
that	O
x	O
is	O
a	O
discrete	O
random	O
variable	B
that	O
must	O
be	O
guessed	O
by	O
asking	O
questions	O
of	O
the	O
type	O
x	O
e	O
a	O
for	O
some	O
sets	O
a	O
let	O
n	O
be	O
the	O
minimum	O
expected	O
number	O
of	O
questions	O
required	O
to	O
determine	O
x	O
with	O
certainty	O
it	O
is	O
well	O
known	O
that	O
log	O
cover	O
and	O
thomas	O
thus	O
not	O
only	O
measures	O
how	O
spread	O
out	O
the	O
mass	O
of	O
x	O
is	O
but	O
also	O
provides	O
us	O
with	O
concrete	O
computational	O
bounds	O
for	O
certain	O
algorithms	O
in	O
the	O
simple	O
example	O
above	O
is	O
in	O
fact	O
proportional	O
to	O
the	O
expected	O
computational	O
time	O
of	O
the	O
best	O
algorithm	B
we	O
are	O
not	O
interested	O
in	O
information	O
theory	O
per	O
se	O
but	O
rather	O
in	O
its	O
usefulness	O
in	O
pattern	O
recognition	O
for	O
our	O
discussion	O
if	O
we	O
fix	O
x	O
x	O
then	O
y	O
is	O
bernoulli	O
hence	O
the	O
conditional	O
entropy	B
of	O
y	O
given	O
x	O
x	O
is	O
rx	O
rx	O
it	O
measures	O
the	O
amount	O
of	O
uncertainty	O
or	O
chaos	O
in	O
y	O
given	O
x	O
x	O
as	O
we	O
know	O
it	O
takes	O
values	O
between	O
rjx	O
e	O
i	O
and	O
log	O
rx	O
and	O
is	O
inequalities	O
and	O
alternate	O
distance	O
measures	O
concave	O
in	O
we	O
define	O
the	O
expected	O
conditional	O
entropy	B
by	O
e	O
e	O
log	O
for	O
brevity	O
we	O
will	O
refer	O
to	O
e	O
as	O
the	O
entropy	B
as	O
pointed	O
out	O
above	O
e	O
if	O
and	O
only	O
if	O
e	O
l	O
with	O
probability	O
one	O
thus	O
e	O
and	O
l	O
are	O
related	O
to	O
each	O
other	O
theorem	O
a	O
e	O
l	O
l	O
equality	O
fano	O
see	O
cover	O
and	O
thomas	O
p	O
b	O
e	O
l	O
nn	O
l	O
c	O
e	O
log	O
nn	O
log	O
l	O
figure	O
inequalities	O
and	O
of	O
theorem	O
are	O
trated	O
here	O
o	O
o	O
log	O
proof	O
part	O
a	O
define	O
a	O
then	O
e	O
a	O
ea	O
is	O
concave	O
by	O
jensens	O
inequality	B
l	O
partb	O
e	O
log	O
a	O
a	O
logl	O
a	O
a	O
jensens	O
inequality	B
a	O
jensens	O
inequality	B
jeffreys	O
divergence	O
l	O
nn	O
l	O
part	O
c	O
by	O
the	O
concavity	O
of	O
ha	O
a	O
as	O
a	O
function	O
of	O
a	O
and	O
taylor	O
series	O
expansion	O
a	O
log	O
therefore	O
by	O
part	O
a	O
log	O
l	O
the	O
cover-hart	B
inequality	B
log	O
remark	O
the	O
nearly	O
monotone	O
relationship	O
between	O
and	O
l	O
will	O
see	O
lots	O
of	O
uses	O
we	O
warn	O
the	O
reader	O
that	O
near	O
the	O
origin	O
l	O
may	O
decrease	O
linearly	O
in	O
but	O
it	O
may	O
also	O
decrease	O
much	O
faster	O
than	O
such	O
wide	O
variation	B
was	O
not	O
observed	O
in	O
the	O
relationship	O
between	O
l	O
and	O
lnn	O
it	O
is	O
linear	O
or	O
l	O
and	O
p	O
it	O
is	O
between	O
linear	O
and	O
quadratic	O
jeffreys	O
divergence	O
jeffreys	O
divergence	O
is	O
a	O
symmetric	O
form	O
of	O
the	O
kullback-leibler	B
divergence	I
okl	O
e	O
rjx	O
log	O
rjx	O
rjx	O
it	O
will	O
be	O
denoted	O
by	O
j	O
e	O
log	O
rjx	O
rjx	O
to	O
understand	O
j	O
note	O
that	O
the	O
function	O
log	O
is	O
symmetric	O
about	O
convex	O
and	O
has	O
minimum	O
at	O
rj	O
as	O
rj	O
rj	O
t	O
the	O
function	O
becomes	O
unbounded	O
therefore	O
j	O
if	O
prjx	O
e	O
i	O
for	O
this	O
reason	O
its	O
inequalities	O
and	O
alternate	O
distance	O
measures	O
use	O
in	O
discrimination	O
is	O
necessarily	O
limited	O
for	O
generalizations	O
of	O
j	O
see	O
renyi	O
burbea	O
and	O
rao	O
taneja	O
and	O
burbea	O
it	O
is	O
thus	O
impossible	O
to	O
bound	O
j	O
from	O
above	O
by	O
a	O
function	O
of	O
lnn	O
andor	O
l	O
however	O
lower	O
bounds	O
are	O
easy	O
to	O
obtain	O
as	O
x	O
x	O
is	O
convex	O
in	O
x	O
and	O
log	O
log	O
we	O
note	O
that	O
by	O
jensens	O
inequality	B
j	O
log	O
ii	O
log	O
l	O
the	O
first	O
bound	O
cannot	O
be	O
universally	O
bettered	O
is	O
achieved	O
when	O
is	O
constant	O
over	O
the	O
space	O
also	O
for	O
fixed	O
l	O
any	O
value	O
of	O
j	O
above	O
the	O
lower	O
bound	O
is	O
possible	O
for	O
some	O
distribution	O
of	O
y	O
from	O
the	O
definition	B
of	I
j	O
we	O
see	O
that	O
j	O
if	O
and	O
only	O
if	O
with	O
probability	O
one	O
or	O
l	O
figure	O
this	O
figure	O
illustrates	O
the	O
above	O
lower	O
bound	O
on	O
jeffreys	O
divergence	O
in	O
terms	O
of	O
the	O
bayes	O
ror	O
j	O
logl-cl	O
l	O
related	O
bounds	O
were	O
obtained	O
by	O
toussaint	O
jl	O
j	O
v	O
log	O
jl	O
the	O
last	O
bound	O
is	O
strictly	O
better	O
than	O
our	O
l	O
bound	O
given	O
above	O
see	O
problem	O
f-errors	O
the	O
error	O
measures	O
discussed	O
so	O
far	O
are	O
all	O
related	O
to	O
expected	O
values	O
of	O
concave	O
functions	O
of	O
py	O
lix	O
in	O
general	O
if	O
f	O
is	O
a	O
concave	O
function	O
on	O
f-errors	O
we	O
define	O
the	O
f	O
corresponding	O
to	O
y	O
by	O
dfx	O
y	O
e	O
examples	O
of	O
f	O
are	O
the	O
bayes	B
error	I
l	O
fx	O
minx	O
x	O
the	O
asymptotic	O
nearest	B
neighbor	I
error	I
lnn	O
fx	O
x	O
the	O
matushita	B
error	I
p	O
fx	O
x	O
the	O
expected	O
conditional	O
entropy	B
f	O
log	O
x	O
x	O
x	O
the	O
negated	O
jeffreys	O
divergence	O
fx	O
log	O
lx	O
hashlamoun	O
varshney	O
and	O
samarasooriya	O
point	O
out	O
that	O
if	O
fx	O
minxl	O
x	O
for	O
each	O
x	O
e	O
then	O
the	O
corresponding	O
f-error	O
is	O
an	O
upper	O
bound	O
on	O
the	O
bayes	B
error	I
the	O
closer	O
f	O
is	O
to	O
minx	O
x	O
the	O
tighter	O
the	O
upper	O
bound	O
is	O
for	O
example	O
fx	O
sinjtx	O
x	O
yields	O
an	O
upper	O
bound	O
tighter	O
than	O
l	O
nn	O
all	O
these	O
errors	O
share	O
the	O
property	O
that	O
the	O
error	O
increases	O
if	O
x	O
is	O
transformed	O
by	O
an	O
arbitrary	O
function	O
theorem	O
let	O
t	O
nd	O
nk	O
be	O
an	O
arbitrary	O
measurable	O
function	O
thenfor	O
any	O
distribution	O
of	O
y	O
proof	O
define	O
nk	O
by	O
py	O
x	O
and	O
observe	O
that	O
e	O
thus	O
e	O
e	O
e	O
ltx	O
jensens	O
inequality	B
dfx	O
y	O
remark	O
we	O
also	O
see	O
from	O
the	O
proof	O
that	O
the	O
f	O
remains	O
unchanged	O
if	O
the	O
transformation	O
t	O
is	O
invertible	O
theorem	O
states	O
that	O
f	O
are	O
a	O
bit	O
like	O
bayes	O
errors-when	O
information	O
is	O
lost	O
replacing	O
x	O
by	O
tx	O
f	O
increase	O
inequalities	O
and	O
alternate	O
distance	O
measures	O
the	O
mahalanobis	B
distance	I
two	O
conditional	O
distributions	O
with	O
about	O
the	O
same	O
covariance	O
matrices	O
and	O
means	O
that	O
are	O
far	O
away	O
from	O
each	O
other	O
are	O
probably	O
so	O
well	O
separated	O
that	O
l	O
is	O
small	O
an	O
interesting	O
measure	O
of	O
the	O
visual	O
distance	O
between	O
two	O
random	O
variables	O
xo	O
and	O
xl	O
is	O
the	O
so-called	O
mahalanobis	B
distance	I
given	O
by	O
whereml	O
mo	O
exo	O
are	O
the	O
means	O
e	O
mdxl	O
mil	O
and	O
e	O
moxo	O
mol	O
are	O
the	O
covariance	O
matrices	O
p	O
p	O
l	O
is	O
the	O
transpose	O
of	O
a	O
vector	O
and	O
p	O
p	O
is	O
a	O
mixture	O
parameter	O
if	O
i	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
then	O
m	O
l	O
is	O
a	O
scaled	O
version	O
of	O
the	O
distance	O
between	O
the	O
means	O
if	O
i	O
i	O
then	O
iiml	O
moll	O
j	O
pjf	O
varies	O
between	O
ilml	O
and	O
ilml	O
as	O
p	O
changes	O
from	O
too	O
assume	O
that	O
we	O
have	O
a	O
discrimination	O
problem	O
in	O
which	O
given	O
y	O
x	O
is	O
distributed	O
as	O
xl	O
given	O
y	O
x	O
is	O
distributed	O
as	O
xo	O
and	O
p	O
py	O
p	O
are	O
the	O
class	O
probabilities	O
then	O
interestingly	O
is	O
related	O
to	O
the	O
bayes	B
error	I
in	O
a	O
general	O
sense	O
if	O
the	O
mahalanobis	B
distance	I
between	O
the	O
class-conditional	B
distributions	O
is	O
large	O
then	O
l	O
is	O
small	O
theorem	O
and	O
kittler	O
p	O
for	O
all	O
distributions	O
of	O
yfor	O
which	O
e	O
we	O
have	O
ll	O
nn	O
p	O
remark	O
for	O
a	O
distribution	O
with	O
mean	O
m	O
and	O
covariance	O
matrix	O
the	O
mahalanobis	B
distance	I
from	O
a	O
point	O
x	O
e	O
n	O
d	O
to	O
m	O
is	O
in	O
one	O
dimension	B
this	O
is	O
simply	O
interpreted	O
as	O
distance	O
from	O
the	O
mean	O
as	O
measured	O
in	O
units	O
of	O
standard	B
deviation	O
the	O
use	O
of	O
mahalanobis	B
distance	I
in	O
discrimination	O
is	O
based	O
upon	O
the	O
intuitive	O
notion	O
that	O
we	O
should	O
classify	O
according	O
to	O
the	O
class	O
for	O
which	O
we	O
are	O
within	O
the	O
least	O
units	O
of	O
standard	B
deviations	O
at	O
least	O
for	O
distributions	O
that	O
look	O
like	O
nice	O
globular	O
clouds	O
such	O
a	O
recommendation	O
may	O
make	O
sense	O
f-divergences	O
proof	O
first	O
assume	O
that	O
d	O
that	O
is	O
x	O
is	O
real	O
valued	O
let	O
u	O
and	O
c	O
be	O
real	O
numbers	O
and	O
consider	O
the	O
quantity	O
e	O
c	O
we	O
will	O
show	O
that	O
if	O
u	O
and	O
c	O
are	O
chosen	O
to	O
minimize	O
this	O
number	O
then	O
it	O
satisfies	O
o	O
e	O
c	O
lnn	O
which	O
proves	O
the	O
theorem	O
for	O
d	O
to	O
see	O
this	O
note	O
that	O
the	O
expression	B
is	O
minimized	O
for	O
c	O
ex	O
e	O
c	O
then	O
e	O
c	O
i	O
varx	O
covx	O
i	O
covx	O
z	O
ex	O
exz	O
ez-which	O
is	O
in	O
turn	O
minimized	O
for	O
u	O
covx	O
straightforward	O
calculation	O
shows	O
that	O
indeed	O
holds	O
to	O
extend	O
the	O
inequality	B
to	O
multidimensional	O
problems	O
apply	O
it	O
to	O
the	O
one-dimensional	O
decision	O
problem	O
y	O
where	O
z	O
xtb-lml	O
mo	O
then	O
the	O
theorem	O
follows	O
by	O
noting	O
that	O
by	O
theorem	O
where	O
lnnx	O
y	O
denotes	O
the	O
nearest-neighbor	O
error	O
corresponding	O
to	O
y	O
in	O
case	O
x	O
i	O
and	O
xo	O
are	O
both	O
normal	B
with	O
the	O
same	O
covariance	O
matrices	O
we	O
have	O
theorem	O
see	O
problem	O
when	O
xl	O
and	O
xo	O
are	O
multivariate	O
normal	B
random	O
variables	O
with	O
bl	O
bo	O
b	O
then	O
if	O
the	O
class-conditional	B
densities	O
and	O
may	O
be	O
written	O
as	O
functions	O
of	O
md	O
and	O
molbolx	O
mo	O
respectively	O
remains	O
relatively	O
tightly	O
linked	O
with	O
l	O
and	O
krzanowski	O
but	O
such	O
tributions	O
are	O
the	O
exception	O
rather	O
than	O
the	O
rule	B
in	O
general	O
when	O
is	O
small	O
it	O
is	O
impossible	O
to	O
deduce	O
whether	O
l	O
is	O
small	O
or	O
not	O
problem	O
f-divergences	O
we	O
have	O
defined	O
error	O
measures	O
as	O
the	O
expected	O
value	O
of	O
a	O
concave	O
function	O
of	O
this	O
makes	O
it	O
easier	O
to	O
relate	O
these	O
measures	O
to	O
the	O
bayes	B
error	I
l	O
and	O
other	O
error	O
probabilities	O
in	O
this	O
section	O
we	O
briefly	O
make	O
the	O
connection	O
to	O
the	O
more	O
classical	O
statistical	O
theory	O
of	O
distances	O
between	O
probability	O
measures	O
a	O
general	O
concept	O
of	O
these	O
distance	O
measures	O
called	O
i-divergences	O
was	O
introduced	O
by	O
csiszar	O
inequalities	O
and	O
alternate	O
distance	O
measures	O
the	O
corresponding	O
theory	O
is	O
summarized	O
in	O
vajda	O
f	O
defined	O
earlier	O
may	O
be	O
calculated	O
if	O
one	O
knows	O
the	O
class	O
probabilities	O
p	O
p	O
and	O
the	O
conditional	O
distributions	O
flo	O
fl	O
of	O
x	O
given	O
o	O
and	O
i	O
that	O
is	O
flia	O
px	O
e	O
aiy	O
i	O
i	O
for	O
fixed	O
class	O
probabilities	O
an	O
f	O
is	O
small	O
if	O
the	O
two	O
conditional	O
distributions	O
are	O
away	O
from	O
each	O
other	O
a	O
metric	O
quantifying	O
this	O
distance	O
may	O
be	O
defined	O
as	O
follows	O
let	O
f	O
r	O
u	O
oo	O
be	O
a	O
convex	O
function	O
with	O
fl	O
the	O
f	O
between	O
two	O
probability	O
measures	O
fl	O
and	O
v	O
on	O
rd	O
is	O
defined	O
by	O
difl	O
v	O
sup	O
l	O
vajf	O
j	O
aa	O
j	O
j	O
vaj	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
finite	O
measurable	O
partitions	O
a	O
of	O
rd	O
if	O
a	O
is	O
a	O
measure	O
dominating	O
fl	O
and	O
v-that	O
is	O
both	O
fl	O
and	O
v	O
are	O
absolutely	O
continuous	O
with	O
respect	O
to	O
a-and	O
p	O
dflda	O
and	O
q	O
dvda	O
are	O
the	O
corresponding	O
densities	O
then	O
the	O
f	O
may	O
be	O
put	O
in	O
the	O
form	O
difl	O
v	O
f	O
qxf	O
adx	O
qx	O
clearly	O
this	O
quantity	O
is	O
independent	O
of	O
the	O
choice	O
of	O
a	O
for	O
example	O
we	O
may	O
take	O
a	O
fl	O
v	O
if	O
fl	O
and	O
v	O
are	O
absolutely	O
continuous	O
with	O
respect	O
to	O
the	O
lebesgue	O
measure	O
then	O
a	O
may	O
be	O
chosen	O
to	O
be	O
the	O
lebesgue	O
measure	O
by	O
jensens	O
inequality	B
difl	O
v	O
and	O
difl	O
fl	O
an	O
important	O
example	O
of	O
f	O
is	O
the	O
total	B
variation	B
or	O
variational	O
distance	O
obtained	O
by	O
choosing	O
f	O
ix	O
yielding	O
vfl	O
v	O
sup	O
l	O
ifla	O
j	O
vajl	O
aa	O
j	O
j	O
for	O
this	O
divergence	O
the	O
equivalence	O
of	O
the	O
two	O
definitions	O
is	O
stated	O
by	O
scheffes	O
theorem	O
problem	O
theorem	O
vfl	O
v	O
sp	O
ifla	O
vai	O
f	O
ipx	O
qxiadx	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
borel	O
subsets	O
of	O
rd	O
another	O
important	O
example	O
is	O
the	O
hellinger	B
distance	I
given	O
by	O
fx	O
h	O
v	O
sup	O
l	O
j	O
fla	O
j	O
j	O
aa	O
j	O
j	O
v	O
pxqxadx	O
i-divergences	O
the	O
quantity	O
v	O
f	O
is	O
often	O
called	O
the	O
hellinger	B
integral	I
we	O
mention	O
two	O
useful	O
inequalities	O
in	O
this	O
respect	O
for	O
the	O
sake	O
of	O
simplicity	O
we	O
state	O
their	O
discrete	O
form	O
integral	O
forms	O
are	O
analogous	O
see	O
problem	O
lemma	O
for	O
positive	O
sequences	O
ai	O
and	O
bi	O
both	O
summing	O
to	O
one	O
proof	O
by	O
the	O
cauchy-schwarz	B
inequality	B
l	O
t	O
yuiui	O
re	O
iaibi	O
this	O
together	O
with	O
the	O
inequality	B
and	O
symmetry	O
implies	O
cb	O
ai	O
bi	O
lminai	O
bi	O
lemma	O
and	O
gyorfi	O
p	O
let	O
al	O
ak	O
bt	O
bk	O
be	O
nonnegative	O
numbers	O
such	O
that	O
li	O
ai	O
li	O
bi	O
then	O
proof	O
the	O
cauchy-schwarz	B
inequality	B
inequalities	O
and	O
alternate	O
distance	O
measures	O
k	O
il	O
l	O
yai	O
sl-tjah	O
which	O
proves	O
the	O
lemma	O
information	O
divergence	O
is	O
obtained	O
by	O
taking	O
f	O
x	O
log	O
x	O
i	O
v	O
is	O
also	O
called	O
the	O
kullback-leibler	O
number	O
our	O
last	O
example	O
is	O
the	O
defined	O
by	O
fx	O
va	O
sup	O
aa	O
j	O
j	O
vaj	O
f	O
adx	O
qx	O
next	O
we	O
highlight	O
the	O
connection	O
between	O
f	O
and	O
f	O
let	O
ilo	O
and	O
ill	O
denote	O
the	O
conditional	O
distributions	O
of	O
x	O
given	O
o	O
and	O
i	O
assume	O
that	O
the	O
class	O
probabilities	O
are	O
equal	O
p	O
if	O
f	O
is	O
a	O
concave	O
function	O
then	O
the	O
f	O
dfx	O
y	O
may	O
be	O
written	O
as	O
where	O
fx	O
f	O
lx	O
and	O
d	O
f	O
is	O
the	O
corresponding	O
f	O
it	O
is	O
easy	O
to	O
see	O
that	O
f	O
is	O
convex	O
whenever	O
f	O
is	O
concave	O
a	O
special	O
case	O
of	O
this	O
correspondence	O
is	O
l	O
v	O
il	O
if	O
p	O
also	O
it	O
is	O
easy	O
to	O
verify	O
that	O
where	O
p	O
is	O
the	O
matushita	B
error	I
for	O
further	O
connections	O
we	O
refer	O
the	O
reader	O
to	O
the	O
exercises	O
problems	O
and	O
exercises	O
problems	O
and	O
exercises	O
showthatforeveryi	O
is	O
i	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
with	O
lnn	O
ii	O
andl	O
therefore	O
the	O
cover-hart	B
inequalities	O
are	O
not	O
universally	O
improvable	O
problem	O
tightness	O
of	O
the	O
bounds	O
theorem	O
cannot	O
be	O
improved	O
show	O
that	O
for	O
all	O
a	O
e	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
such	O
that	O
lnn	O
l	O
a	O
show	O
that	O
for	O
all	O
a	O
e	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
such	O
that	O
lnn	O
a	O
l	O
show	O
that	O
for	O
all	O
a	O
e	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
such	O
that	O
lpa	O
show	O
that	O
for	O
all	O
a	O
e	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
such	O
that	O
l	O
nn-a	O
l-	O
problem	O
show	O
that	O
e	O
l	O
problem	O
for	O
any	O
a	O
find	O
a	O
sequence	O
of	O
distributions	O
of	O
yn	O
having	O
expected	O
conditional	O
entropies	O
en	O
and	O
bayes	O
errors	O
l	O
such	O
that	O
l	O
as	O
n	O
and	O
en	O
decreases	O
to	O
zero	O
at	O
the	O
same	O
rate	O
as	O
a	O
problem	O
concavity	O
of	O
error	O
measures	O
let	O
y	O
denote	O
the	O
mixture	O
random	O
variable	B
taking	O
the	O
value	O
with	O
probability	O
p	O
and	O
the	O
value	O
with	O
probability	O
let	O
x	O
be	O
a	O
fixed	O
rd-valuedrandom	O
variable	B
and	O
define	O
llx	O
x	O
x	O
where	O
are	O
bernoulli	O
random	O
variables	O
clearly	O
prjl	O
p	O
which	O
of	O
the	O
error	O
measures	O
l	O
p	O
lnn	O
e	O
are	O
concave	O
in	O
p	O
for	O
fixed	O
joint	O
distribution	O
of	O
x	O
can	O
every	O
discrimination	O
problem	O
y	O
be	O
decomposed	O
this	O
way	O
for	O
some	O
p	O
where	O
e	O
i	O
for	O
all	O
x	O
if	O
not	O
will	O
the	O
condition	O
e	O
i	O
for	O
all	O
x	O
do	O
problem	O
show	O
that	O
for	O
every	O
e	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
with	O
l	O
l	O
and	O
e	O
hl	O
l	O
thus	O
fanos	O
inequality	B
is	O
tight	O
problem	O
toussaints	O
inequalities	O
mimic	O
a	O
proof	O
in	O
the	O
text	O
to	O
show	O
that	O
j	O
log	O
j	O
i	O
problem	O
show	O
that	O
l	O
e-oc	O
where	O
dc	O
is	O
chernoffs	O
measure	O
of	O
affinity	O
with	O
parameter	O
a	O
e	O
problem	O
prove	O
that	O
l	O
p	O
where	O
p	O
pry	O
i	O
and	O
maxx	O
problem	O
show	O
that	O
j	O
p	O
where	O
p	O
pry	O
i	O
problem	O
let	O
fl	O
and	O
fo	O
be	O
two	O
multivariate	O
normal	B
densities	O
with	O
means	O
rno	O
rn	O
and	O
common	O
covariance	O
matrix	O
l	O
ifpy	O
i	O
p	O
and	O
ii	O
fo	O
are	O
the	O
conditional	O
densities	O
of	O
inequalities	O
and	O
alternate	O
distance	O
measures	O
x	O
given	O
y	O
and	O
y	O
respectively	O
then	O
show	O
that	O
p	O
where	O
p	O
is	O
the	O
matushita	B
error	I
and	O
is	O
the	O
mahalanobis	B
distance	I
problem	O
for	O
every	O
e	O
and	O
e	O
with	O
find	O
tions	O
ilo	O
and	O
ill	O
for	O
x	O
given	O
y	O
y	O
such	O
that	O
the	O
distance	O
tl	O
yet	O
l	O
therefore	O
the	O
distance	O
is	O
not	O
universally	O
related	O
to	O
the	O
bayes	O
risk	O
problem	O
show	O
that	O
the	O
mahalanobis	B
distance	I
is	O
invariant	O
under	O
linear	O
invertible	O
transformations	O
of	O
x	O
problem	O
lissack	O
and	O
fu	O
have	O
suggested	O
the	O
measures	O
for	O
ex	O
this	O
is	O
twice	O
the	O
distance	O
show	O
the	O
following	O
ifo	O
ex	O
then	O
olf	O
l	O
oa	O
if	O
ex	O
then	O
ja	O
l	O
olf	O
problem	O
hashlamoun	O
varshney	O
and	O
samarasooriya	O
suggest	O
using	O
the	O
error	O
with	O
the	O
function	O
f	O
x	O
sill	O
trx	O
e	O
to	O
obtain	O
tight	O
upper	O
bounds	O
on	O
l	O
show	O
that	O
f	O
minx	O
x	O
so	O
that	O
the	O
sponding	O
f	O
is	O
indeed	O
an	O
upper	O
bound	O
on	O
the	O
bayes	O
risk	O
problem	O
prove	O
that	O
l	O
p	O
vilo	O
ild	O
problem	O
prove	O
that	O
l	O
philolll	O
hint	O
mina	O
b	O
jqb	O
problem	O
assume	O
that	O
the	O
components	O
of	O
x	O
xed	O
are	O
conditionally	O
independent	O
y	O
and	O
identically	O
distributed	O
that	O
is	O
pxu	O
e	O
aiy	O
j	O
vja	O
for	O
i	O
d	O
and	O
j	O
use	O
the	O
previous	O
exercise	O
to	O
show	O
that	O
problem	O
show	O
that	O
fili	O
hint	O
x-i	O
problem	O
show	O
the	O
following	O
analog	O
of	O
theorem	O
let	O
t	O
n	O
d	O
n	O
k	O
be	O
a	O
surable	O
function	O
and	O
il	O
v	O
probability	O
measures	O
on	O
nd	O
define	O
the	O
measures	O
ilt	O
and	O
vt	O
on	O
n	O
k	O
by	O
ilta	O
ilt-a	O
and	O
vta	O
vet-lea	O
show	O
that	O
for	O
any	O
convex	O
function	O
j	O
djil	O
v	O
djilt	O
vt	O
problem	O
prove	O
the	O
following	O
connections	O
between	O
the	O
hellinger	B
integral	I
and	O
the	O
total	B
variation	B
and	O
hint	O
proceed	O
analogously	O
to	O
lemmas	O
and	O
v	O
hil	O
v	O
problems	O
and	O
exercises	O
problem	O
pinskers	O
inequality	B
show	O
that	O
v	O
kullback	O
and	O
kemperman	O
hint	O
first	O
prove	O
the	O
inequality	B
if	O
and	O
v	O
are	O
concentrated	O
on	O
the	O
same	O
two	O
atoms	O
then	O
define	O
a	O
px	O
q	O
and	O
the	O
measures	O
v	O
on	O
the	O
set	O
i	O
by	O
va	O
and	O
apply	O
the	O
previous	O
result	O
conclude	O
by	O
pointing	O
out	O
that	O
scheffes	O
theorem	O
states	O
v	O
v	O
and	O
that	O
v	O
v	O
and	O
vo	O
linear	O
discrimination	O
in	O
this	O
chapter	O
we	O
split	O
the	O
space	O
by	O
a	O
hyperplane	O
and	O
assign	O
a	O
different	O
class	O
to	O
each	O
halfspace	O
such	O
rules	O
offer	O
tremendous	O
advantages-they	O
are	O
easy	O
to	O
interpret	O
as	O
each	O
decision	O
is	O
based	O
upon	O
the	O
sign	O
of	O
ll	O
aixi	O
ao	O
where	O
x	O
xed	O
and	O
the	O
ais	O
are	O
weights	O
the	O
weight	O
vector	O
determines	O
the	O
relative	O
importance	O
of	O
the	O
components	O
the	O
decision	O
is	O
also	O
easily	O
implemented-in	O
a	O
standard	B
software	O
solution	O
the	O
time	O
of	O
a	O
decision	O
is	O
proportional	O
to	O
d-and	O
the	O
prospect	O
that	O
a	O
small	O
chip	O
can	O
be	O
built	O
to	O
make	O
a	O
virtually	O
instantaneous	O
decision	O
is	O
particularly	O
exciting	O
rosenblatt	O
realized	O
the	O
tremendous	O
potential	O
of	O
such	O
linear	O
rules	O
and	O
called	O
themperceptrons	O
changing	O
one	O
or	O
more	O
weights	O
as	O
new	O
data	O
arrive	O
allows	O
us	O
to	O
quickly	O
and	O
easily	O
adapt	O
the	O
weights	O
to	O
new	O
situations	O
training	O
or	O
ing	O
patterned	O
after	O
the	O
human	O
brain	O
thus	O
became	O
a	O
reality	O
this	O
chapter	O
merely	O
looks	O
at	O
some	O
theoretical	B
properties	O
of	O
perceptrons	O
we	O
begin	O
with	O
the	O
simple	O
dimensional	O
situation	O
and	O
deal	O
with	O
the	O
choice	O
of	O
weights	O
in	O
nd	O
further	O
on	O
unless	O
one	O
is	O
terribly	O
lucky	O
linear	O
discrimination	O
rules	O
cannot	O
provide	O
error	O
probabilities	O
close	O
to	O
the	O
bayes	O
risk	O
but	O
that	O
should	O
not	O
diminish	O
the	O
value	O
of	O
this	O
chapter	O
linear	O
discrimination	O
is	O
at	O
the	O
heart	O
of	O
nearly	O
every	O
successful	O
pattern	O
recognition	O
method	O
including	O
tree	O
classifiers	O
and	O
generalized	B
linear	O
fiers	O
and	O
neural	O
networks	O
we	O
also	O
encounter	O
for	O
the	O
first	O
time	O
rules	O
in	O
which	O
the	O
parameters	O
are	O
dependent	O
upon	O
the	O
data	O
linear	O
discrimination	O
input	O
oarl	O
weights	O
figure	O
rosenblatts	O
perceptron	B
the	O
decision	O
is	O
based	O
upon	O
a	O
linear	O
combination	O
of	O
the	O
components	O
of	O
the	O
input	O
vector	O
univariate	O
discrimination	O
and	O
stoller	O
splits	O
as	O
an	O
introductory	O
example	O
let	O
x	O
be	O
univariate	O
the	O
crudest	O
and	O
simplest	O
possible	O
rule	B
is	O
the	O
linear	O
discrimination	O
rule	B
gx	O
y	O
y	O
otherwise	O
if	O
x	O
x	O
where	O
x	O
is	O
a	O
split	O
point	O
and	O
y	O
e	O
is	O
aclass	O
ingeneralx	O
and	O
y	O
are	O
measurable	O
functions	O
of	O
the	O
data	O
dn	O
within	O
this	O
class	O
of	O
simple	O
rules	O
there	O
is	O
of	O
course	O
a	O
best	O
possible	O
rule	B
that	O
can	O
be	O
determined	O
if	O
we	O
know	O
the	O
distribution	O
assume	O
for	O
example	O
that	O
y	O
is	O
described	O
in	O
the	O
standard	B
manner	O
let	O
py	O
p	O
given	O
y	O
x	O
has	O
a	O
distribution	B
function	I
flx	O
px	O
xly	O
and	O
given	O
y	O
x	O
has	O
a	O
distribution	B
function	I
fox	O
px	O
xly	O
o	O
where	O
fo	O
and	O
fl	O
are	O
the	O
class-conditional	B
distribution	I
functions	O
then	O
a	O
theoretically	O
optimal	O
rule	B
is	O
determined	O
by	O
the	O
split	O
point	O
x	O
and	O
class	O
y	O
given	O
by	O
y	O
arg	O
min	O
pgx	O
y	O
minimum	O
is	O
always	O
reached	O
if	O
we	O
allow	O
the	O
values	O
x	O
and	O
x	O
we	O
call	O
the	O
corresponding	O
minimal	O
probability	O
of	O
error	O
l	O
and	O
note	O
that	O
l	O
inf	O
fl	O
fox	O
ivl	O
fl	O
p	O
a	O
split	O
defined	O
by	O
y	O
will	O
be	O
called	O
a	O
theoretical	B
stoller	B
split	I
univariate	O
discrimination	O
and	O
stoller	O
splits	O
lemma	O
l	O
with	O
equality	O
if	O
and	O
only	O
if	O
l	O
proof	O
take	O
y	O
then	O
the	O
probability	O
of	O
erroris	O
p	O
py	O
oj	O
take	O
y	O
then	O
the	O
probability	O
of	O
error	O
is	O
p	O
clearly	O
l	O
l	O
minp	O
p	O
this	O
proves	O
the	O
first	O
part	O
of	O
the	O
lemma	O
for	O
the	O
second	O
part	O
if	O
l	O
then	O
p	O
and	O
for	O
every	O
x	O
pflx	O
fox	O
and	O
fix	O
p	O
the	O
first	O
inequality	B
implies	O
p	O
fi	O
p	O
p	O
p	O
fox	O
p	O
therefore	O
l	O
while	O
the	O
second	O
implies	O
p	O
fi	O
pfox	O
p	O
thus	O
for	O
all	O
x	O
means	O
that	O
for	O
every	O
x	O
pfix	O
fi	O
fox	O
and	O
therefore	O
l	O
d	O
lemma	O
l	O
sup	O
ipfix	O
x	O
pfox	O
p	O
in	O
particular	O
if	O
p	O
then	O
l	O
sup	O
ifix	O
foxl	O
x	O
proof	O
set	O
px	O
p	O
fi	O
p	O
then	O
by	O
definition	O
l	O
inf	O
min	O
p	O
p	O
px	O
x	O
sup	O
i	O
px	O
p	O
i	O
mina	O
b	O
b	O
x	O
la	O
d	O
the	O
last	O
property	O
relates	O
the	O
quality	O
of	O
theoretical	B
stoller	O
splits	O
to	O
the	O
mogorov-smirnov	O
distance	O
supx	O
i	O
fi	O
fox	O
i	O
between	O
the	O
class-conditional	B
distribution	I
functions	O
as	O
a	O
fun	O
exercise	O
consider	O
two	O
classes	O
with	O
means	O
mo	O
exiy	O
oj	O
ml	O
exiy	O
l	O
and	O
variances	O
varxiy	O
o	O
and	O
af	O
var	O
x	O
i	O
y	O
i	O
then	O
the	O
following	O
inequality	B
holds	O
theorem	O
remark	O
when	O
p	O
chernoff	O
proved	O
l	O
linear	O
discrimination	O
moreover	O
becker	O
pointed	O
out	O
that	O
this	O
is	O
the	O
best	O
possible	O
bound	O
problem	O
proof	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
mo	O
mi	O
clearly	O
l	O
is	O
smaller	O
than	O
the	O
probability	O
of	O
error	O
for	O
the	O
rule	B
that	O
decides	O
when	O
x	O
mo	O
and	O
otherwise	O
where	O
ml	O
mo	O
o	O
decide	O
class	O
decide	O
class	O
figure	O
the	O
split	O
providing	O
the	O
bound	O
of	O
theorem	O
the	O
probability	O
of	O
error	O
of	O
the	O
latter	O
rule	B
is	O
ppx	O
ml	O
i	O
ppx	O
mo	O
o	O
p	O
the	O
chebyshev-cantelli	B
inequality	B
see	O
appendix	O
theorem	O
p	O
p	O
lli	O
a	O
o	O
and	O
molaoao	O
we	O
have	O
yet	O
another	O
example	O
of	O
the	O
principle	O
that	O
well-separated	O
classes	O
yield	O
small	O
values	O
for	O
l	O
and	O
thus	O
l	O
separation	O
is	O
now	O
measured	O
in	O
terms	O
of	O
the	O
largeness	O
of	O
mo	O
i	O
with	O
respect	O
to	O
another	O
inequality	B
in	O
the	O
same	O
spirit	O
is	O
given	O
in	O
problem	O
the	O
limitations	O
of	O
theoretical	B
stoller	O
splits	O
are	O
best	O
shown	O
in	O
a	O
simple	O
example	O
consider	O
a	O
uniform	O
random	O
variable	B
x	O
and	O
define	O
y	O
if	O
x	O
e	O
ifiex-e	O
if	O
for	O
some	O
small	O
e	O
o	O
as	O
y	O
is	O
a	O
function	O
of	O
x	O
we	O
have	O
l	O
o	O
if	O
we	O
are	O
forced	O
to	O
make	O
a	O
trivial	O
x	O
decision	O
then	O
the	O
best	O
we	O
can	O
do	O
is	O
to	O
set	O
gx	O
univariate	O
discrimination	O
and	O
stoller	O
splits	O
the	O
probability	O
of	O
error	O
is	O
p	O
e	O
x	O
e	O
consider	O
next	O
a	O
theoretical	B
stoller	B
split	I
one	O
sees	O
quickly	O
that	O
the	O
best	O
split	O
occurs	O
at	O
x	O
or	O
x	O
and	O
thus	O
that	O
l	O
in	O
other	O
words	O
even	O
the	O
best	O
theoretical	B
split	O
is	O
superfluous	O
note	O
also	O
that	O
in	O
the	O
above	O
example	O
mo	O
ml	O
so	O
that	O
the	O
inequality	B
of	O
theorem	O
says	O
l	O
i-it	O
degenerates	O
we	O
now	O
consider	O
what	O
to	O
do	O
when	O
a	O
split	O
must	O
be	O
data-based	B
stoller	O
suggests	O
taking	O
y	O
such	O
that	O
the	O
empirical	B
error	I
is	O
minimal	O
he	O
finds	O
y	O
such	O
that	O
argrrun	O
l	O
ixsxydy	O
ixxydl-y	O
n	O
il	O
and	O
y	O
are	O
now	O
random	O
variables	O
but	O
in	O
spite	O
of	O
our	O
convention	O
we	O
keep	O
the	O
lowercase	O
notation	O
for	O
now	O
we	O
will	O
call	O
this	O
stollers	O
rule	B
the	O
split	O
is	O
referred	O
to	O
as	O
an	O
empirical	B
stoller	B
split	I
denote	O
the	O
set	O
x	O
x	O
u	O
x	O
y	O
by	O
cx	O
y	O
then	O
y	O
argmin	O
vncx	O
y	O
where	O
vn	O
is	O
the	O
empirical	B
measure	I
for	O
the	O
data	O
dn	O
yi	O
yn	O
that	O
is	O
for	O
every	O
measurable	O
set	O
a	O
e	O
r	O
x	O
i	O
vna	O
ixiyiea	O
denoting	O
the	O
measure	O
of	O
y	O
in	O
r	O
x	O
i	O
by	O
v	O
it	O
is	O
clear	O
that	O
evnc	O
vc	O
px	O
x	O
y	O
y	O
px	O
x	O
y	O
y	O
let	O
ln	O
pgnx	O
yidn	O
be	O
the	O
error	O
probability	O
of	O
the	O
splitting	O
rule	B
gn	O
with	O
the	O
data-dependent	B
choice	O
y	O
given	O
above	O
conditioned	O
on	O
the	O
data	O
then	O
ln	O
vcx	O
y	O
vcx	O
y	O
vncx	O
y	O
vncx	O
y	O
sup	O
y	O
vncx	O
y	O
vncx	O
y	O
y	O
minimizes	O
vcx	O
y	O
over	O
all	O
y	O
sup	O
ivcx	O
y	O
vncx	O
vcx	O
y	O
sup	O
ivcx	O
y	O
vncx	O
y	O
l	O
from	O
the	O
next	O
theorem	O
we	O
see	O
that	O
the	O
supremum	O
above	O
is	O
small	O
even	O
for	O
ately	O
large	O
n	O
and	O
therefore	O
stollers	O
rule	B
performs	O
closely	O
to	O
the	O
best	O
split	O
less	O
of	O
the	O
distribution	O
of	O
y	O
theorem	O
for	O
stollers	O
rule	B
and	O
e	O
and	O
pln	O
l	O
e	O
eln	O
l	O
n	O
linear	O
discrimination	O
proof	O
by	O
the	O
inequality	B
given	O
just	O
above	O
the	O
theorem	O
p	O
ivcx	O
y	O
vncx	O
y	O
p	O
vncx	O
i	O
p	O
vncx	O
h	O
by	O
a	O
double	O
application	O
of	O
massarts	O
tightened	O
version	O
of	O
the	O
kiefer-wolfowitz	O
inequality	B
see	O
problem	O
we	O
do	O
not	O
prove	O
this	O
inequality	B
here	O
but	O
we	O
will	O
thoroughly	O
discuss	O
several	O
such	O
inequalities	O
in	O
chapter	O
in	O
a	O
greater	O
generality	O
the	O
second	O
inequality	B
follows	O
from	O
the	O
first	O
via	O
problem	O
the	O
probability	O
of	O
error	O
of	O
stollers	O
rule	B
is	O
uniformly	O
close	O
to	O
l	O
over	O
all	O
possible	O
distributions	O
this	O
is	O
just	O
a	O
preview	O
of	O
things	O
to	O
come	O
as	O
we	O
may	O
be	O
able	O
to	O
obtain	O
good	O
performance	O
guarantees	O
within	O
a	O
limited	O
class	O
of	O
rules	O
linear	O
discriminants	O
rosenblatts	O
perceptron	B
see	O
nilsson	O
for	O
a	O
good	O
sion	O
is	O
based	O
upon	O
a	O
dichotomy	O
of	O
rd	O
into	O
two	O
parts	O
by	O
a	O
hyperplane	O
the	O
linear	O
discrimination	O
rule	B
with	O
weights	O
ao	O
ai	O
ad	O
is	O
given	O
by	O
figure	O
a	O
linear	O
discriminant	O
in	O
that	O
rectly	O
classifies	O
all	O
but	O
four	O
data	O
points	O
if	O
laixi	O
il	O
otherwise	O
gx	O
l	O
d	O
where	O
x	O
o	O
xed	O
linear	O
discriminants	O
its	O
probability	O
of	O
error	O
is	O
for	O
now	O
denoted	O
by	O
la	O
ao	O
where	O
a	O
ad	O
again	O
we	O
set	O
l	O
inf	O
aerdaoer	O
la	O
ao	O
for	O
the	O
best	O
possible	O
probability	O
of	O
error	O
within	O
this	O
class	O
let	O
the	O
class-conditional	B
distribution	I
functions	O
of	O
a	O
i	O
xl	O
xed	O
be	O
denoted	O
by	O
foa	O
and	O
fia	O
depending	O
upon	O
whether	O
y	O
or	O
y	O
for	O
la	O
ao	O
we	O
may	O
use	O
the	O
bounds	O
of	O
lemma	O
and	O
apply	O
them	O
to	O
foa	O
and	O
fia	O
thus	O
l	O
spsp	O
ipfiax	O
pfoax	O
p	O
which	O
for	O
p	O
reduces	O
to	O
a	O
l	O
sup	O
sup	O
i	O
foaxi	O
x	O
therefore	O
l	O
if	O
and	O
only	O
if	O
p	O
and	O
for	O
all	O
a	O
fia	O
foa	O
then	O
apply	O
the	O
following	O
simple	O
lemma	O
lemma	O
and	O
wold	O
xl	O
and	O
x	O
random	O
variables	O
taking	O
values	O
in	O
r	O
d	O
are	O
identically	O
distributed	O
if	O
and	O
only	O
if	O
at	O
xl	O
and	O
at	O
have	O
the	O
same	O
distribution	O
for	O
all	O
vectors	O
a	O
e	O
rd	O
proof	O
two	O
random	O
variables	O
have	O
identical	O
distributions	O
if	O
and	O
only	O
if	O
they	O
have	O
the	O
same	O
characteristic	O
function-see	O
for	O
example	O
lukacs	O
and	O
laha	O
now	O
the	O
characteristic	B
function	I
of	O
x	O
i	O
xid	O
is	O
e	O
e	O
eiaixi	O
assumption	O
the	O
characteristic	B
function	I
of	O
x	O
thus	O
we	O
have	O
proved	O
the	O
following	O
theorem	O
l	O
with	O
equality	O
if	O
and	O
only	O
if	O
l	O
thus	O
as	O
in	O
the	O
one-dimensional	O
case	O
whenever	O
l	O
a	O
meaningful	O
cut	O
by	O
a	O
hyperplane	O
is	O
possible	O
there	O
are	O
also	O
examples	O
in	O
which	O
no	O
cut	O
improves	O
over	O
a	O
rule	B
in	O
which	O
gx	O
y	O
for	O
some	O
y	O
and	O
all	O
x	O
yet	O
l	O
and	O
l	O
to	O
generalize	O
theorem	O
we	O
offer	O
the	O
following	O
result	O
a	O
related	O
inequality	B
is	O
shown	O
in	O
problem	O
the	O
idea	O
of	O
using	O
chebyshevs	O
inequality	B
to	O
obtain	O
such	O
bounds	O
is	O
due	O
to	O
yau	O
and	O
lin	O
also	O
devijver	O
and	O
kittler	O
linear	O
discrimination	O
theorem	O
let	O
xo	O
and	O
x	O
i	O
be	O
random	O
variables	O
distributed	O
as	O
x	O
given	O
y	O
and	O
y	O
respectively	O
set	O
mo	O
exo	O
ml	O
exd	O
define	O
also	O
the	O
covariance	O
matrices	O
e	O
mlx	O
i	O
mil	O
and	O
e	O
moxo	O
mol	O
then	O
ll	O
inf	O
aerd	O
proof	O
for	O
any	O
a	O
end	O
we	O
may	O
apply	O
theorem	O
to	O
at	O
xo	O
and	O
at	O
xl	O
theorem	O
follows	O
by	O
noting	O
that	O
e	O
xo	O
atexo	O
at	O
mo	O
eatxi	O
and	O
that	O
var	O
xo	O
e	O
moxo	O
mot	O
a	O
atoa	O
varatxi	O
we	O
may	O
obtain	O
explicit	O
inequalities	O
by	O
different	O
choices	O
of	O
a	O
a	O
ml	O
mo	O
yields	O
a	O
convenient	O
formula	O
we	O
see	O
from	O
the	O
next	O
section	O
that	O
a	O
mo	O
with	O
p	O
is	O
also	O
a	O
meaningful	O
choice	O
also	O
problem	O
the	O
fisher	B
linear	I
discriminant	I
data-based	B
values	O
for	O
a	O
may	O
be	O
found	O
by	O
various	O
criteria	O
one	O
of	O
the	O
first	O
methods	O
was	O
suggested	O
by	O
fisher	O
let	O
ml	O
and	O
mo	O
be	O
the	O
sample	O
means	O
for	O
the	O
two	O
classes	O
ml	O
xdli	O
yi	O
picture	O
projecting	O
xl	O
xn	O
to	O
a	O
line	O
in	O
the	O
direction	O
of	O
a	O
note	O
that	O
this	O
is	O
perpendicular	O
to	O
the	O
hyperplane	O
given	O
by	O
a	O
t	O
x	O
ao	O
o	O
the	O
projected	O
values	O
are	O
a	O
t	O
xl	O
a	O
t	O
x	O
n	O
these	O
are	O
all	O
equal	O
to	O
o	O
for	O
those	O
xi	O
on	O
the	O
hyperplane	O
at	O
x	O
through	O
the	O
origin	O
and	O
grow	O
in	O
absolute	O
value	O
as	O
we	O
flee	O
that	O
hyperplane	O
let	O
and	O
be	O
the	O
sample	O
scatters	O
for	O
classes	O
and	O
respectively	O
that	O
is	O
tx	O
a	O
t---	O
i-a	O
ml	O
asia	O
t	O
and	O
similarly	O
for	O
ag	O
where	O
iyil	O
sl	O
l	O
mdxi	O
mdt	O
is	O
the	O
scatter	B
matrix	I
for	O
class	O
the	O
fisher	B
linear	I
discriminant	I
is	O
that	O
linear	O
function	O
a	O
t	O
x	O
for	O
which	O
the	O
criterion	O
the	O
normal	B
distribution	I
ja	O
t	O
ml	O
a	O
mo	O
t	O
t-a	O
ml	O
mo	O
atsl	O
soa	O
is	O
maximum	O
this	O
corresponds	O
to	O
finding	O
a	O
direction	O
a	O
that	O
best	O
separates	O
at	O
rn	O
from	O
at	O
rno	O
relative	O
to	O
the	O
sample	B
scatter	I
luckily	O
to	O
find	O
that	O
a	O
we	O
need	O
not	O
resort	O
to	O
numerical	O
iteration-the	O
solution	O
is	O
given	O
by	O
fishers	O
suggestion	O
is	O
to	O
replace	O
yn	O
by	O
xl	O
x	O
n	O
yn	O
and	O
to	O
perform	O
one-dimensional	O
discrimination	O
usually	O
the	O
rule	B
uses	O
a	O
simple	O
split	O
gao	O
if	O
at	O
x	O
ao	O
i	O
otherwise	O
for	O
some	O
constant	O
ao	O
unfortunately	O
fisher	O
discriminants	O
can	O
be	O
arbitrarily	O
bad	O
there	O
are	O
distributions	O
such	O
that	O
even	O
though	O
the	O
two	O
classes	O
are	O
linearly	O
separable	O
l	O
the	O
fisher	B
linear	I
discriminant	I
has	O
an	O
error	O
probability	O
close	O
to	O
problem	O
the	O
normal	B
distribution	I
there	O
are	O
a	O
few	O
situations	O
in	O
which	O
by	O
sheer	O
accident	O
the	O
bayes	O
rule	B
is	O
a	O
ear	O
discriminant	O
while	O
this	O
is	O
not	O
a	O
major	O
issue	O
it	O
is	O
interesting	O
to	O
identify	O
the	O
most	O
important	O
case	O
i	O
e	O
that	O
of	O
the	O
multivariate	O
normal	B
distribution	I
the	O
general	O
multivariate	O
normal	B
density	O
is	O
written	O
as	O
where	O
m	O
is	O
the	O
mean	O
x	O
and	O
m	O
are	O
d-component	O
column	O
vectors	O
l	O
is	O
the	O
d	O
x	O
d	O
covariance	O
matrix	O
l-l	O
is	O
the	O
inverse	O
of	O
l	O
and	O
detl	O
is	O
its	O
determinant	O
nm	O
l	O
clearly	O
if	O
x	O
has	O
density	O
j	O
then	O
m	O
ex	O
and	O
l	O
we	O
write	O
j	O
ex	O
mx	O
ml	O
the	O
multivariate	O
normal	B
density	O
is	O
completely	O
specified	O
by	O
d	O
e	O
formal	O
rameters	O
and	O
l	O
a	O
sample	O
from	O
the	O
density	O
is	O
clustered	O
in	O
an	O
elliptical	O
cloud	O
the	O
loci	O
of	O
points	O
of	O
constant	O
density	O
are	O
ellipsoids	O
described	O
by	O
for	O
some	O
constant	O
r	O
o	O
the	O
number	O
r	O
is	O
the	O
mahalanobis	B
distance	I
from	O
x	O
to	O
m	O
and	O
is	O
in	O
fact	O
useful	O
even	O
when	O
the	O
underlying	O
distribution	O
is	O
not	O
normal	B
it	O
takes	O
into	O
account	O
the	O
directional	O
stretch	O
of	O
the	O
space	O
determined	O
by	O
l	O
linear	O
discrimination	O
figure	O
points	O
at	O
equal	O
mahalanobis	B
distance	I
from	O
m	O
given	O
a	O
two-class	O
problem	O
in	O
which	O
x	O
has	O
a	O
density	O
p	O
p	O
i	O
and	O
and	O
are	O
both	O
multivariate	O
normal	B
with	O
parameters	O
mi	O
li	O
i	O
the	O
bayes	O
rule	B
is	O
easily	O
described	O
by	O
gx	O
if	O
p	O
lx	O
p	O
ox	O
otherwise	O
take	O
logarithms	O
and	O
note	O
that	O
gx	O
if	O
and	O
only	O
if	O
ml	O
motlolx	O
mo	O
p	O
logdetlo	O
in	O
practice	O
one	O
might	O
wish	O
to	O
estimate	O
m	O
mo	O
l	O
lo	O
and	O
p	O
from	O
the	O
data	O
and	O
use	O
these	O
estimates	O
in	O
the	O
formula	O
for	O
g	O
interestingly	O
as	O
mi	O
f	O
li	O
mi	O
is	O
the	O
squared	O
mahalanobis	B
distance	I
from	O
x	O
to	O
mi	O
in	O
class	O
i	O
rl	O
the	O
bayes	O
rule	B
is	O
simply	O
g	O
otherwise	O
ifrl	O
pplogdetlodetld	O
in	O
particular	O
when	O
p	O
lo	O
ll	O
l	O
we	O
have	O
x	O
g	O
f	O
r	O
ro	O
otherwise	O
just	O
classify	O
according	O
to	O
the	O
class	O
whose	O
mean	O
is	O
at	O
the	O
nearest	O
mahalanobis	B
distance	I
from	O
x	O
when	O
lo	O
l	O
l	O
the	O
bayes	O
rule	B
becomes	O
linear	O
if	O
a	O
t	O
x	O
ao	O
otherwise	O
g	O
where	O
a	O
mol-l	O
and	O
ao	O
p	O
mt	O
l-lmo	O
mfl-lml	O
thus	O
linear	O
discrimination	O
rules	O
occur	O
as	O
special	O
cases	O
of	O
bayes	O
rules	O
for	O
variate	O
normal	B
distributions	O
our	O
intuition	O
that	O
a	O
should	O
be	O
in	O
the	O
direction	O
m	O
mo	O
to	O
best	O
separate	O
the	O
classes	O
is	O
almost	O
right	O
note	O
nevertheless	O
that	O
a	O
is	O
not	O
perpendicular	O
in	O
general	O
to	O
the	O
hyperplane	O
of	O
loci	O
at	O
equal	O
distance	O
from	O
mo	O
and	O
mi	O
when	O
l	O
is	O
replaced	O
by	O
empirical	B
risk	I
minimization	I
the	O
standard	B
data-based	B
estimate	O
we	O
obtain	O
in	O
fact	O
the	O
fisher	B
linear	I
discriminant	I
furthermore	O
when	O
i	O
the	O
decision	O
boundary	O
is	O
usually	O
not	O
linear	O
and	O
fishers	O
linear	O
discriminant	O
must	O
therefore	O
be	O
suboptimal	O
in	O
early	O
statistical	O
work	O
on	O
discrimination	O
the	O
normal	B
historical	O
remarks	O
distribution	O
plays	O
a	O
central	O
role	O
for	O
a	O
simple	O
introduction	O
we	O
refer	O
to	O
duda	O
and	O
hart	O
mclachlan	O
has	O
more	O
details	O
and	O
raudys	O
relates	O
the	O
error	O
dimensionality	O
and	O
sample	O
size	O
for	O
normal	B
and	O
nearly	O
normal	B
models	O
see	O
also	O
raudys	O
and	O
pikelis	O
empirical	B
risk	I
minimization	I
in	O
this	O
section	O
we	O
present	O
an	O
algorithm	B
that	O
yields	O
a	O
classifier	B
whose	O
error	O
ity	O
is	O
very	O
close	O
to	O
the	O
minimal	O
error	O
probability	O
l	O
achievable	O
by	O
linear	O
classifiers	O
provided	O
that	O
x	O
has	O
a	O
density	O
the	O
algorithm	B
selects	O
a	O
classifier	B
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
finitely	O
classifiers	O
for	O
a	O
rule	B
if	O
at	O
x	O
ao	O
otherwise	O
the	O
probability	O
of	O
error	O
is	O
l	O
p	O
i	O
y	O
l	O
may	O
be	O
estimated	O
by	O
the	O
empirical	B
risk	O
that	O
is	O
the	O
number	O
of	O
errors	O
made	O
by	O
the	O
classifier	B
is	O
counted	O
and	O
normalized	O
assume	O
that	O
x	O
has	O
a	O
density	O
and	O
consider	O
d	O
arbitrary	O
data	O
points	O
xii	O
x	O
xid	O
among	O
xn	O
and	O
let	O
at	O
x	O
ao	O
be	O
a	O
hyperplane	O
containing	O
these	O
points	O
because	O
of	O
the	O
density	O
assumption	O
the	O
d	O
points	O
are	O
in	O
general	O
position	O
with	O
probability	O
one	O
and	O
this	O
hyperplane	O
is	O
unique	O
this	O
hyperplane	O
determines	O
two	O
classifiers	O
if	O
at	O
x	O
ao	O
otherwise	O
and	O
i	O
if	O
at	O
x	O
ao	O
otherwise	O
whose	O
empirical	B
errors	O
ln	O
l	O
and	O
l	O
n	O
may	O
be	O
calculated	O
to	O
each	O
d-tuple	O
xii	O
xid	O
of	O
data	O
points	O
we	O
may	O
assign	O
two	O
classifiers	O
in	O
this	O
manner	O
yielding	O
altogether	O
classifiers	O
denote	O
these	O
classifiers	O
by	O
l	O
let	O
be	O
a	O
linear	B
classifier	B
that	O
minimizes	O
ln	O
i	O
over	O
all	O
i	O
d	O
linear	O
discrimination	O
we	O
denote	O
the	O
best	O
possible	O
error	O
probability	O
by	O
l	O
inf	O
lfjj	O
over	O
the	O
class	O
of	O
all	O
linear	O
rules	O
and	O
define	O
fjj	O
arg	O
min	O
l	O
fjj	O
as	O
the	O
best	O
linear	O
rule	B
if	O
there	O
are	O
several	O
classifiers	O
with	O
lfjj	O
l	O
then	O
we	O
choose	O
fjj	O
among	O
these	O
in	O
an	O
arbitrary	O
fixed	O
manner	O
next	O
we	O
show	O
that	O
the	O
classifier	B
corresponding	O
to	O
is	O
really	O
very	O
good	O
figure	O
if	O
the	O
data	O
points	O
are	O
in	O
general	O
position	O
thenfor	O
each	O
linear	O
rule	B
there	O
exists	O
a	O
linear	O
split	O
defined	O
by	O
a	O
hyperplane	O
crossing	O
d	O
points	O
such	O
that	O
the	O
difference	O
between	O
the	O
empirical	B
errors	O
is	O
at	O
most	O
din	O
first	O
note	O
that	O
there	O
is	O
no	O
linear	B
classifier	B
fjj	O
whose	O
empirical	B
error	I
is	O
smaller	O
than	O
din	O
this	O
follows	O
from	O
the	O
fact	O
that	O
since	O
the	O
data	O
points	O
are	O
in	O
general	O
position	O
the	O
density	O
assumption	O
then	O
for	O
each	O
linear	B
classifier	B
we	O
may	O
find	O
one	O
whose	O
defining	O
hyperplane	O
contains	O
exactly	O
d	O
data	O
points	O
such	O
that	O
the	O
two	O
decisions	O
agree	O
on	O
all	O
data	O
points	O
except	O
possibly	O
for	O
these	O
d	O
see	O
figure	O
thus	O
we	O
may	O
view	O
minimization	O
of	O
the	O
empirical	B
error	I
over	O
the	O
finite	O
set	O
as	O
approximate	O
minimization	O
over	O
the	O
infinite	O
set	O
of	O
linear	O
classifiers	O
in	O
chapters	O
and	O
we	O
will	O
develop	O
the	O
full	O
theory	O
for	O
rules	O
that	O
are	O
found	O
by	O
empirical	B
risk	I
minimization	I
theorem	O
just	O
gives	O
you	O
a	O
taste	O
of	O
things	O
to	O
come	O
other-more	O
involved	O
but	O
also	O
more	O
general-proofs	O
go	O
back	O
to	O
vapnik	O
and	O
chervonenkis	O
theorem	O
assume	O
that	O
x	O
has	O
a	O
density	O
isfound	O
by	O
empirical	B
error	I
mization	O
as	O
described	O
above	O
thenjor	O
all	O
possible	O
distributions	O
ojx	O
y	O
ifn	O
d	O
and	O
e	O
we	O
have	O
moreover	O
ifn	O
d	O
then	O
e	O
l	O
log	O
n	O
n	O
remark	O
with	O
some	O
care	O
theorem	O
and	O
theorem	O
below	O
can	O
be	O
extended	O
so	O
that	O
the	O
density	O
assumption	O
may	O
be	O
dropped	O
one	O
needs	O
to	O
ensure	O
that	O
the	O
selected	O
empirical	B
risk	I
minimization	I
linear	O
rule	B
has	O
empirical	B
error	I
close	O
to	O
that	O
of	O
the	O
best	O
possible	O
linear	O
rule	B
with	O
the	O
classifier	B
suggested	O
above	O
this	O
property	O
may	O
fail	O
to	O
hold	O
if	O
the	O
data	O
points	O
are	O
not	O
necessarily	O
of	O
general	O
position	O
the	O
ideas	O
presented	O
here	O
are	O
generalized	B
in	O
chapter	O
theorem	O
d	O
proof	O
we	O
begin	O
with	O
the	O
following	O
simple	O
inequality	B
lj	O
l	O
lj	O
ln	O
ln	O
l	O
ln	O
ln	O
din	O
for	O
any	O
max	O
i	O
ln	O
j	O
ln	O
l	O
d	O
n	O
therefore	O
by	O
the	O
union-of-events	O
bound	O
we	O
have	O
p	O
l	O
l	O
e	O
lp	B
l	O
i-ln	O
j-	O
ln	O
n	O
e	O
il	O
to	O
bound	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
observe	O
that	O
nln	O
is	O
nomially	O
distributed	O
with	O
parameters	O
nand	O
l	O
by	O
an	O
inequality	B
due	O
to	O
chernoff	O
and	O
okamoto	O
for	O
the	O
tail	O
of	O
the	O
binomial	B
distribution	I
p	O
ln	O
l	O
e	O
we	O
prove	O
this	O
inequality	B
later	O
theorem	O
next	O
we	O
bound	O
one	O
term	O
of	O
the	O
sum	O
on	O
the	O
right-hand	O
side	O
note	O
that	O
by	O
symmetry	O
all	O
terms	O
are	O
equal	O
assume	O
that	O
the	O
classifier	B
is	O
determined	O
by	O
the	O
d-tuple	O
of	O
the	O
first	O
d	O
data	O
points	O
xl	O
xd	O
we	O
write	O
p	O
l	O
l	O
n	O
l	O
e	O
l	O
ln	O
d	O
i	O
xl	O
xd	O
and	O
bound	O
the	O
conditional	O
probability	O
inside	O
let	O
y	O
yj	O
be	O
independent	O
of	O
the	O
data	O
and	O
be	O
distributed	O
as	O
the	O
data	O
yd	O
yd	O
define	O
y	O
y	O
yi	O
l	O
i	O
if	O
i	O
d	O
ifi	O
d	O
then	O
p	O
l	O
l	O
l	O
n	O
l	O
i	O
xl	O
xd	O
p	O
d	O
t	O
i	O
lxiyd	O
xl	O
xd	O
n	O
idl	O
linear	O
discrimination	O
p	O
l	O
l	O
p	O
l	O
l	O
l	O
ijixjyi	O
xl	O
xd	O
n	O
n	O
il	O
ei	O
d	O
n	O
i	O
i	O
binomlaln	O
l	O
l	O
x	O
i	O
xd	O
e	O
dj	O
l	O
l	O
depends	O
upon	O
xl	O
xd	O
only	O
and	O
y	O
y	O
are	O
independent	O
of	O
xl	O
xd	O
e	O
ii	O
theorem	O
use	O
the	O
fact	O
that	O
e	O
the	O
inequality	B
for	O
the	O
expected	O
value	O
follows	O
from	O
the	O
probability	O
inequality	B
by	O
the	O
following	O
simple	O
argument	O
by	O
the	O
cauchy-schwarz	B
inequality	B
e	O
denoting	O
z	O
for	O
any	O
u	O
ez	O
eziz	O
upz	O
u	O
upz	O
u	O
pz	O
u	O
u	O
e-nu	O
u	O
if	O
u	O
by	O
the	O
probability	O
inequality	B
and	O
since	O
g	O
n	O
d	O
choosing	O
u	O
to	O
minimize	O
the	O
obtained	O
expression	B
yields	O
the	O
desired	O
inequality	B
first	O
verify	O
that	O
the	O
minimum	O
occurs	O
for	O
ne	O
u	O
n	O
where	O
e	O
check	O
that	O
if	O
n	O
d	O
then	O
u	O
n	O
then	O
note	O
that	O
the	O
bound	O
ee-nu	O
u	O
equals	O
n	O
nee	O
n	O
n	O
log	O
n	O
observe	O
for	O
now	O
that	O
the	O
bound	O
on	O
p	O
l	O
e	O
decreases	O
rapidly	O
with	O
n	O
to	O
have	O
an	O
impact	O
it	O
must	O
become	O
less	O
than	O
for	O
smallo	O
this	O
happens	O
roughly	O
speaking	O
when	O
n	O
e	O
log	O
for	O
some	O
constant	O
e	O
doubling	O
d	O
the	O
dimension	B
causes	O
this	O
minimal	O
sample	O
size	O
to	O
roughly	O
double	O
as	O
well	O
empiiical	O
risk	O
minimization	O
an	O
important	O
special	O
case	O
is	O
when	O
the	O
distribution	O
is	O
linearly	O
separable	O
that	O
is	O
l	O
o	O
in	O
such	O
cases	O
the	O
empirical	B
risk	I
minimization	I
above	O
performs	O
even	O
better	O
as	O
the	O
size	O
of	O
the	O
error	O
improves	O
to	O
log	O
n	O
in	O
from	O
j	O
d	O
log	O
n	O
in	O
clearly	O
the	O
data	O
points	O
are	O
linearly	O
separable	O
as	O
well	O
that	O
is	O
with	O
probability	O
one	O
and	O
therefore	O
din	O
with	O
probability	O
one	O
theorem	O
assume	O
that	O
x	O
has	O
a	O
density	O
and	O
that	O
the	O
best	O
linear	B
classifier	B
has	O
zero	O
probability	O
of	O
error	O
then	O
for	O
the	O
empirical	B
risk	I
minimization	I
algorithm	B
of	O
theorem	O
for	O
all	O
n	O
d	O
and	O
e	O
p	O
j	O
and	O
e	O
l	O
dlogn	O
n-d	O
proof	O
by	O
the	O
union	O
bound	O
pl	O
p	O
li	O
li	O
d	O
by	O
symmetry	O
this	O
sum	O
equals	O
ni	O
li	O
n	O
xi	O
xd	O
where	O
as	O
in	O
theorem	O
l	O
is	O
determined	O
by	O
the	O
data	O
points	O
xl	O
x	O
d	O
ever	O
p	O
no	O
xi	O
xd	O
p	O
ixdl	O
ydl	O
l	O
l	O
ei	O
xl	O
xd	O
all	O
of	O
the	O
most	O
d	O
errors	O
committed	O
by	O
l	O
occur	O
for	O
yi	O
yd	O
since	O
the	O
probability	O
that	O
no	O
yi	O
pair	O
i	O
d	O
n	O
falls	O
in	O
the	O
set	O
y	O
y	O
is	O
less	O
than	O
e	O
if	O
the	O
probability	O
of	O
the	O
set	O
is	O
larger	O
than	O
e	O
the	O
x	O
e-x	O
proof	O
of	O
the	O
probability	O
inequality	B
may	O
be	O
completed	O
by	O
noting	O
that	O
linear	O
discrimination	O
for	O
the	O
expected	O
error	O
probability	O
note	O
that	O
for	O
any	O
u	O
pl	O
tdt	O
el	O
u	O
rx	O
u	O
d	O
the	O
probability	O
inequality	B
and	O
g	O
nd	O
u	O
pl	O
tdt	O
e-n-dt	O
dt	O
n	O
we	O
choose	O
u	O
to	O
minimize	O
the	O
obtained	O
bound	O
which	O
yields	O
the	O
desired	O
inequality	B
o	O
minimizing	O
other	O
criteria	O
empirical	B
risk	I
minimization	I
uses	O
extensive	O
computations	O
because	O
ln	O
is	O
not	O
a	O
unimodal	O
function	O
in	O
general	O
problems	O
and	O
also	O
gradient	O
timization	O
is	O
difficult	O
because	O
the	O
gradients	O
are	O
zero	O
almost	O
everywhere	O
in	O
fact	O
given	O
n	O
labeled	O
points	O
in	O
n	O
d	O
finding	O
the	O
best	O
linear	O
dichotomy	O
is	O
np	O
hard	O
johnson	O
and	O
preparata	O
to	O
aid	O
in	O
the	O
optimization	O
some	O
have	O
suggested	O
minimizing	O
a	O
modified	O
empirical	B
error	I
such	O
as	O
or	O
n	O
a	O
xi	O
ao	O
t	O
n	O
il	O
where	O
is	O
a	O
positive	O
convex	O
function	O
of	O
particular	O
importance	O
here	O
is	O
the	O
mean	O
square	O
error	O
criterion	O
e	O
g	O
widrow	O
and	O
hoff	O
one	O
can	O
easily	O
verify	O
that	O
ln	O
has	O
a	O
gradient	O
respect	O
to	O
ao	O
that	O
may	O
aid	O
in	O
locating	O
a	O
local	O
minimum	O
let	O
denote	O
the	O
linear	O
discrimination	O
rule	B
minimizing	O
over	O
all	O
a	O
and	O
ao	O
a	O
description	O
of	O
the	O
solution	O
is	O
given	O
in	O
problem	O
even	O
in	O
a	O
one-dimensional	O
situation	O
the	O
mean	O
square	O
error	O
criterion	O
muddles	O
the	O
issue	O
and	O
does	O
not	O
give	O
any	O
performance	O
guarantees	O
minimizing	O
other	O
criteria	O
theorem	O
if	O
supxy	O
denotes	O
the	O
supremum	O
with	O
respect	O
to	O
all	O
distributions	O
on	O
r	O
x	O
i	O
then	O
sup	O
l	O
where	O
is	O
a	O
linear	O
discriminant	O
obtained	O
by	O
minimizing	O
over	O
all	O
al	O
and	O
ao	O
remark	O
this	O
theorem	O
establishes	O
the	O
existence	O
of	O
distributions	O
of	O
y	O
for	O
which	O
e	O
and	O
l	O
e	O
simultaneously	O
for	O
arbitrarily	O
small	O
e	O
therefore	O
minimizing	O
the	O
mean	O
square	O
error	O
criterion	O
is	O
not	O
recommended	O
unless	O
one	O
has	O
additional	O
information	O
regarding	O
the	O
distribution	O
of	O
y	O
proof	O
let	O
e	O
and	O
consider	O
a	O
triatomic	O
distribution	O
of	O
y	O
px	O
y	O
i	O
px	O
y	O
i	O
px	O
y	O
o	O
e	O
figure	O
a	O
distribution	O
for	O
which	O
squared	B
error	I
minimization	O
fails	O
probability	O
probability	O
probability	O
for	O
e	O
the	O
best	O
linear	O
rule	B
decides	O
class	O
on	O
and	O
elsewhere	O
for	O
a	O
probability	O
of	O
error	O
l	O
e	O
the	O
mean	O
square	O
error	O
criterion	O
asks	O
that	O
we	O
minimize	O
l	O
e	O
with	O
respect	O
to	O
ao	O
v	O
and	O
a	O
u	O
setting	O
the	O
derivatives	O
with	O
respect	O
to	O
u	O
and	O
v	O
equal	O
to	O
zero	O
yields	O
v	O
u	O
and	O
v	O
u	O
e	O
for	O
v	O
if	O
we	O
let	O
e	O
and	O
let	O
t	O
then	O
v	O
thus	O
for	O
e	O
small	O
enough	O
and	O
large	O
enough	O
considering	O
the	O
decision	O
at	O
only	O
e	O
because	O
at	O
x	O
ux	O
v	O
v	O
thus	O
l	O
for	O
e	O
small	O
enough	O
and	O
large	O
enough	O
linear	O
discrimination	O
others	O
have	O
suggested	O
minimizing	O
n	O
l	O
t	O
xi	O
ao	O
il	O
where	O
au	O
is	O
a	O
sigmoid	B
that	O
is	O
an	O
increasing	O
function	O
from	O
to	O
such	O
as	O
see	O
for	O
example	O
wassel	O
and	O
sklansky	O
do	O
tu	O
and	O
installe	O
e-	O
u	O
fritz	O
and	O
gyorfi	O
and	O
sklansky	O
and	O
wassel	O
clearly	O
au	O
iuco	O
provides	O
the	O
empirical	B
en-or	O
probability	O
however	O
the	O
point	O
here	O
is	O
to	O
use	O
smooth	O
sigmoids	O
so	O
that	O
gradient	O
algorithms	O
may	O
be	O
used	O
to	O
find	O
the	O
optimum	O
this	O
may	O
be	O
viewed	O
as	O
a	O
compromise	O
between	O
the	O
mean	O
squared	O
en-or	O
criteria	O
and	O
empirical	B
en-or	O
minimization	O
here	O
too	O
anomalies	O
can	O
occur	O
and	O
the	O
en-or	O
space	O
is	O
not	O
well	O
behaved	O
displaying	O
many	O
local	O
minima	O
krogh	O
and	O
palmer	O
see	O
however	O
problems	O
and	O
problems	O
and	O
exercises	O
problem	O
with	O
the	O
notation	O
of	O
theorem	O
show	O
that	O
the	O
error	O
probability	O
l	O
of	O
a	O
one-dimensional	O
theoretical	B
stoller	B
split	I
satisfies	O
p	O
l	O
p	O
p	O
and	O
vajda	O
is	O
this	O
bound	O
better	O
than	O
that	O
of	O
theorem	O
hint	O
for	O
any	O
threshold	B
rule	B
gcx	O
ix	O
c	O
and	O
u	O
write	O
lgj	O
pix	O
c	O
pix	O
c	O
i	O
plux	O
c	O
e	O
c	O
i	O
by	O
chebyshevs	O
inequality	B
choose	O
u	O
and	O
c	O
to	O
minimize	O
the	O
upper	O
bound	O
problem	O
let	O
p	O
if	O
l	O
is	O
the	O
error	O
probability	O
of	O
the	O
one-dimensional	O
theoretical	B
stoller	B
split	I
show	O
that	O
l	O
show	O
that	O
the	O
bound	O
is	O
achieved	O
for	O
some	O
distribution	O
when	O
the	O
class-conditional	B
tions	O
of	O
x	O
is	O
given	O
y	O
and	O
y	O
are	O
concentrated	O
on	O
two	O
points	O
each	O
one	O
of	O
which	O
is	O
shared	O
by	O
both	O
classes	O
becker	O
problem	O
let	O
x	O
be	O
a	O
univariate	O
random	O
variable	B
the	O
distribution	O
functions	O
for	O
x	O
given	O
y	O
and	O
y	O
are	O
and	O
fo	O
respectively	O
assume	O
that	O
the	O
moment	O
generating	O
functions	O
for	O
x	O
exist	O
that	O
is	O
e	O
olt	O
e	O
o	O
ten	O
where	O
are	O
finite	O
for	O
all	O
t	O
in	O
the	O
spirit	O
of	O
theorem	O
derive	O
an	O
upper	O
bound	O
for	O
l	O
in	O
function	O
of	O
apply	O
your	O
bound	O
to	O
the	O
case	O
that	O
and	O
fo	O
are	O
both	O
normal	B
with	O
possibly	O
different	O
means	O
and	O
variances	O
problems	O
and	O
exercises	O
problem	O
signals	O
in	O
additne	O
gaussian	B
noise	I
let	O
so	O
sl	O
e	O
rd	O
be	O
fixed	O
and	O
let	O
n	O
be	O
a	O
multivariate	O
gaussian	B
random	O
variable	B
with	O
zero	O
mean	O
and	O
covariance	O
matrix	O
let	O
pry	O
o	O
pry	O
i	O
and	O
define	O
x	O
so	O
si	O
if	O
y	O
if	O
y	O
construct	O
the	O
bayes	O
decision	O
and	O
calculate	O
l	O
prove	O
that	O
if	O
is	O
the	O
identity	O
matrix	O
and	O
so	O
and	O
si	O
have	O
constant	O
components	O
then	O
l	O
exponentially	O
rapidly	O
as	O
d	O
problem	O
in	O
the	O
last	O
step	O
of	O
the	O
proof	O
of	O
theorem	O
we	O
used	O
the	O
wolfowitz-massart	O
inequality	B
this	O
result	O
states	O
that	O
if	O
zi	O
zn	O
are	O
i	O
i	O
d	O
random	O
variables	O
on	O
the	O
real	O
line	O
with	O
distribution	B
function	I
fz	O
p	O
zi	O
z	O
and	O
empirical	B
distribution	B
function	I
fnz	O
iziozj	O
then	O
use	O
this	O
inequality	B
to	O
conclude	O
that	O
p	O
ivcx	O
vncx	O
i	O
hint	O
map	O
y	O
on	O
the	O
real	O
line	O
by	O
a	O
one-to-one	O
function	O
x	O
i	O
r	O
such	O
that	O
z	O
y	O
if	O
and	O
only	O
if	O
y	O
o	O
use	O
the	O
dvoretzky-kiefer-wolfowitz-massart	O
inequality	B
for	O
z	O
problem	O
let	O
l	O
be	O
the	O
probability	O
of	O
error	O
for	O
the	O
best	O
sphere	B
rule	B
that	O
is	O
for	O
the	O
rule	B
that	O
associates	O
a	O
class	O
with	O
the	O
inside	O
of	O
a	O
sphere	B
sxn	O
and	O
the	O
other	O
class	O
with	O
the	O
outside	O
here	O
the	O
center	O
x	O
and	O
radius	O
r	O
are	O
both	O
variable	B
show	O
that	O
l	O
if	O
and	O
only	O
if	O
l	O
and	O
that	O
l	O
problem	O
with	O
the	O
notation	O
of	O
theorem	O
show	O
that	O
the	O
probability	O
of	O
error	O
l	O
of	O
the	O
best	O
linear	O
discriminant	O
satisfies	O
p	O
l	O
where	O
fl	O
jrnl	O
rno	O
is	O
the	O
mahalanobis	B
distance	I
with	O
and	O
vajda	O
interestingly	O
the	O
upper	O
bound	O
is	O
just	O
twice	O
the	O
bound	O
of	O
theorem	O
for	O
the	O
asymptotic	O
nearest	B
neighbor	I
error	I
thus	O
a	O
large	O
mahalanobis	B
distance	I
does	O
not	O
only	O
imply	O
that	O
the	O
bayes	B
error	I
is	O
small	O
but	O
also	O
small	O
error	O
probabilities	O
may	O
be	O
achieved	O
by	O
simple	O
linear	O
classifiers	O
hint	O
apply	O
the	O
inequality	B
of	O
problem	O
for	O
the	O
univariate	O
random	O
variable	B
x	O
at	O
x	O
a	O
rno	O
problem	O
if	O
rni	O
and	O
a	O
are	O
the	O
mean	O
and	O
variance	B
of	I
at	O
x	O
given	O
that	O
y	O
i	O
i	O
where	O
a	O
is	O
a	O
column	O
vector	O
of	O
weights	O
then	O
show	O
that	O
the	O
criterion	O
linear	O
discrimination	O
is	O
minimized	O
for	O
a	O
mo	O
where	O
mi	O
and	O
are	O
the	O
mean	O
vector	O
and	O
covariance	O
matrix	O
of	O
x	O
given	O
y	O
i	O
also	O
show	O
that	O
is	O
minimized	O
for	O
a	O
po-jmj	O
mo	O
where	O
p	O
p	O
are	O
the	O
class	O
probabilities	O
this	O
exercise	O
shows	O
that	O
if	O
discrimination	O
is	O
attempted	O
in	O
one	O
dimension	B
we	O
might	O
consider	O
projections	O
a	O
t	O
x	O
where	O
a	O
maximizes	O
the	O
weighted	B
distance	O
between	O
the	O
projected	O
means	O
problem	O
in	O
the	O
fisher	B
linear	I
discriminant	I
rule	B
with	O
free	O
parameter	O
ao	O
show	O
that	O
for	O
any	O
e	O
there	O
exists	O
a	O
distribution	O
for	O
y	O
x	O
e	O
n	O
such	O
that	O
infao	O
elgao	O
e	O
moreover	O
if	O
ao	O
is	O
chosen	O
to	O
minimize	O
the	O
squared	B
error	I
with	O
l	O
and	O
eiixii	O
then	O
elgao	O
e	O
problem	O
find	O
a	O
distribution	O
of	O
y	O
with	O
x	O
e	O
n	O
such	O
that	O
with	O
probability	O
at	O
least	O
one	O
half	O
ln	O
is	O
not	O
unimodal	O
with	O
respect	O
to	O
the	O
weight	O
vector	O
ao	O
problem	O
the	O
following	O
observation	O
may	O
help	O
in	O
developing	O
a	O
fast	O
algorithm	B
to	O
find	O
the	O
best	O
linear	B
classifier	B
in	O
certain	O
cases	O
assume	O
that	O
the	O
bayes	O
rule	B
is	O
a	O
linear	O
split	O
cutting	O
through	O
the	O
origin	O
that	O
is	O
l	O
la	O
for	O
some	O
coefficient	O
vector	O
a	O
e	O
n	O
d	O
where	O
la	O
denotes	O
the	O
error	O
probability	O
of	O
the	O
classifier	B
if	O
aixi	O
otherwise	O
and	O
a	O
ad	O
show	O
that	O
la	O
is	O
unimodal	O
as	O
a	O
function	O
of	O
a	O
end	O
and	O
la	O
is	O
monotone	O
increasing	O
along	O
rays	O
pointing	O
from	O
a	O
that	O
is	O
for	O
any	O
e	O
and	O
a	O
e	O
n	O
d	O
la	O
laa	O
and	O
gyorfi	O
hint	O
use	O
the	O
expression	B
la	O
j	O
sign	O
aixu	O
idx	O
to	O
show	O
that	O
la	O
lcaa	O
fa	O
for	O
some	O
set	O
a	O
end	O
problem	O
let	O
a	O
aj	O
and	O
a	O
arg	O
min	O
e	O
ajx	O
iydgaxid	O
a	O
and	O
gax	O
ialxaoo	O
show	O
that	O
for	O
every	O
e	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
on	O
n	O
x	O
i	O
such	O
that	O
leii	O
l	O
e	O
where	O
la	O
is	O
the	O
error	O
probability	O
for	O
gao	O
hint	O
argue	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
a	O
distribution	O
with	O
four	O
atoms	O
suffices	O
problem	O
repeat	O
the	O
previous	O
exercise	O
for	O
a	O
argmine	O
ajx	O
aol	O
a	O
problems	O
and	O
exercises	O
problem	O
let	O
denote	O
the	O
linear	O
discrimination	O
rule	B
that	O
minimizes	O
the	O
mean	O
square	O
error	O
e	O
i	O
at	O
x	O
over	O
all	O
a	O
and	O
ao	O
as	O
this	O
criterion	O
is	O
quadratic	O
in	O
ao	O
it	O
is	O
unimodal	O
one	O
usually	O
approximates	O
by	O
by	O
minimizing	O
li	O
i	O
at	O
xi	O
over	O
all	O
a	O
and	O
ao	O
show	O
that	O
the	O
minimal	O
column	O
vector	O
ao	O
is	O
given	O
by	O
lx	O
where	O
x	O
is	O
a	O
i-dimensional	O
column	O
vector	O
problem	O
the	O
perceptron	B
criterion	I
is	O
j	O
l	O
xiao	O
latxi	O
find	O
a	O
distribution	O
for	O
which	O
l	O
l	O
yet	O
liminfn---ooeln	O
where	O
is	O
the	O
linear	O
discrimination	O
rule	B
obtained	O
by	O
using	O
the	O
a	O
and	O
ao	O
that	O
minimize	O
j	O
problem	O
let	O
be	O
a	O
monotone	O
nondecreasing	O
function	O
on	O
n	O
satisfying	O
limu---	O
oo	O
and	O
limu---oo	O
ou	O
for	O
h	O
define	O
ohu	O
ohu	O
consider	O
the	O
linear	O
nation	O
rule	B
with	O
a	O
and	O
ao	O
chosen	O
to	O
minimize	O
t	O
t	O
il	O
xi	O
ao	O
for	O
every	O
fixed	O
h	O
and	O
e	O
exhibit	O
a	O
distribution	O
with	O
l	O
e	O
and	O
liminfeln	O
e	O
n-oo	O
on	O
the	O
other	O
hand	O
show	O
that	O
if	O
h	O
depends	O
on	O
the	O
sample	O
size	O
n	O
such	O
that	O
h	O
as	O
n	O
then	O
for	O
all	O
distributions	O
eln	O
l	O
problem	O
given	O
y	O
i	O
let	O
x	O
be	O
normal	B
with	O
mean	O
mi	O
and	O
covariance	O
matrix	O
li	O
i	O
consider	O
discrimination	O
based	O
upon	O
the	O
minimization	O
of	O
the	O
criterion	O
with	O
respect	O
to	O
a	O
w	O
and	O
c	O
a	O
d	O
x	O
d	O
matrix	O
d	O
x	O
vector	O
and	O
constant	O
respectively	O
where	O
ou	O
e-u	O
is	O
the	O
standard	B
sigmoid	B
function	O
show	O
that	O
this	O
is	O
minimized	O
for	O
the	O
same	O
a	O
w	O
and	O
c	O
that	O
minimize	O
the	O
probability	O
of	O
error	O
and	O
conclude	O
that	O
in	O
this	O
particular	O
case	O
the	O
squared	B
error	I
criterion	O
may	O
be	O
used	O
to	O
obtain	O
a	O
bayes-optimal	O
classifier	B
and	O
hush	O
nearest	B
neighbor	I
rules	O
introduction	O
simple	O
rules	O
survive	O
the	O
k-nearest	O
neighbor	O
rule	B
since	O
its	O
conception	O
in	O
and	O
and	O
hodges	O
has	O
thus	O
attracted	O
many	O
followers	O
and	O
continues	O
to	O
be	O
studied	O
by	O
many	O
researchers	O
formally	O
we	O
define	O
the	O
k	O
rule	B
by	O
gnx	O
if	O
wni	O
iyill	O
wni	O
iyiol	O
otherwise	O
where	O
wni	O
k	O
if	O
xi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
and	O
wni	O
elsewhere	O
xi	O
is	O
said	O
to	O
be	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
if	O
the	O
distance	O
iix	O
xi	O
ii	O
is	O
the	O
k-th	O
smallest	O
among	O
ilx	O
xiii	O
ilx	O
xn	O
ii	O
in	O
case	O
of	O
a	O
distance	O
tie	O
the	O
candidate	O
with	O
the	O
smaller	O
index	O
is	O
said	O
to	O
be	O
closer	O
to	O
x	O
the	O
decision	O
is	O
based	O
upon	O
a	O
majority	B
vote	I
it	O
is	O
convenient	O
to	O
let	O
k	O
be	O
odd	O
to	O
avoid	O
voting	O
ties	O
several	O
issues	O
are	O
worth	O
considering	O
universal	B
consistency	B
establish	O
convergence	O
to	O
the	O
bayes	O
rule	B
if	O
k	O
and	O
k	O
n	O
as	O
n	O
this	O
is	O
dealt	O
with	O
in	O
chapter	O
finite	O
k	O
performance	O
what	O
happens	O
if	O
we	O
hold	O
k	O
fixed	O
and	O
let	O
n	O
tend	O
to	O
infinity	O
the	O
choice	O
of	O
the	O
weight	O
vector	O
l	O
wnn	O
are	O
equal	O
weights	O
for	O
the	O
k	O
nearest	O
neighbors	O
better	O
than	O
unequal	O
weights	O
in	O
some	O
sense	O
nearest	B
neighbor	I
rules	O
the	O
choice	O
of	O
a	O
distance	O
metric	O
achieve	O
invariance	O
with	O
respect	O
to	O
a	O
certain	O
family	B
of	I
transformations	O
the	O
reduction	B
of	I
the	O
data	O
size	O
can	O
we	O
obtain	O
good	O
performance	O
when	O
the	O
data	O
set	O
is	O
edited	B
andor	O
reduced	O
in	O
size	O
to	O
lessen	O
the	O
storage	O
load	O
figure	O
at	O
every	O
point	O
the	O
decision	O
is	O
the	O
label	O
of	O
the	O
closest	O
data	O
point	O
the	O
set	O
of	O
points	O
whose	O
nearest	B
neighbor	I
is	O
xi	O
is	O
called	O
the	O
voronoi	B
cell	I
of	O
xi	O
the	O
partition	B
induced	O
by	O
the	O
voronoi	O
cells	O
is	O
a	O
varona	O
partition	B
a	O
voronoi	B
partition	B
of	O
random	O
points	O
is	O
shown	O
here	O
in	O
the	O
first	O
couple	O
of	O
sections	O
we	O
will	O
be	O
concerned	O
with	O
convergence	O
issues	O
for	O
k	O
nearest	B
neighbor	I
rules	O
when	O
k	O
does	O
not	O
change	O
with	O
n	O
in	O
particular	O
we	O
will	O
see	O
that	O
for	O
all	O
distributions	O
the	O
expected	O
error	O
probability	O
el	O
n	O
tends	O
to	O
a	O
limit	O
lknn	O
that	O
is	O
in	O
general	O
close	O
to	O
but	O
larger	O
than	O
l	O
the	O
methodology	O
for	O
obtaining	O
this	O
result	O
is	O
interesting	O
in	O
its	O
own	O
right	O
the	O
expression	B
for	O
lknn	O
is	O
then	O
studied	O
and	O
several	O
key	O
inequalities	O
such	O
as	O
lnn	O
and	O
hart	O
and	O
lknn	O
l	O
k	O
are	O
proved	O
and	O
applied	O
the	O
other	O
issues	O
mentioned	O
above	O
are	O
dealt	O
with	O
in	O
the	O
remaining	O
sections	O
for	O
surveys	O
of	O
various	O
aspects	O
of	O
the	O
nearest	B
neighbor	I
or	O
related	O
methods	O
see	O
dasarathy	O
devijver	O
or	O
devroye	O
and	O
wagner	O
remark	O
computational	O
concerns	O
storing	O
the	O
n	O
data	O
pairs	O
in	O
an	O
array	O
and	O
searching	O
for	O
the	O
k	O
nearest	O
neighbors	O
may	O
take	O
time	O
proportional	O
to	O
nkd	O
if	O
done	O
in	O
a	O
naive	B
manner-the	O
accounts	O
for	O
the	O
cost	O
of	O
one	O
distance	O
computation	O
this	O
complexity	O
may	O
be	O
reduced	O
in	O
terms	O
of	O
one	O
or	O
more	O
of	O
the	O
three	O
factors	O
involved	O
typically	O
with	O
k	O
and	O
d	O
fixed	O
on	O
lid	O
worst-case	O
time	O
and	O
bentley	O
and	O
n	O
expected	O
time	O
bentley	O
and	O
finkel	O
may	O
be	O
achieved	O
multidimensional	O
search	O
trees	O
that	O
partition	B
the	O
space	O
and	O
guide	O
the	O
search	O
are	O
invaluable-for	O
this	O
approach	O
see	O
fukunaga	O
and	O
n	O
arendra	O
friedman	O
bentley	O
and	O
finkel	O
niemann	O
and	O
goppert	O
kim	O
and	O
park	O
and	O
broder	O
we	O
refer	O
to	O
a	O
survey	O
in	O
dasarathy	O
for	O
more	O
references	O
other	O
approaches	O
are	O
described	O
by	O
yunck	O
friedman	O
baskett	O
and	O
shustek	O
vidal	O
sethi	O
and	O
linder	O
and	O
lugosi	O
generally	O
with	O
preprocessing	O
one	O
may	O
considerably	O
reduce	O
the	O
overall	O
complexity	O
in	O
terms	O
of	O
nand	O
d	O
notation	O
and	O
simple	O
asymptotics	O
notation	O
and	O
simple	O
asymptotics	O
we	O
fix	O
x	O
e	O
rdandreorderthedatax	O
yd	O
ynaccordingtoincreasing	O
values	O
of	O
ii	O
xi	O
x	O
ii	O
the	O
reordered	O
data	O
sequence	O
is	O
denoted	O
by	O
if	O
no	O
confusion	O
is	O
possible	O
xkx	O
is	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
remark	O
we	O
note	O
here	O
that	O
rather	O
arbitrarily	O
we	O
defined	O
neighbors	O
in	O
terms	O
of	O
y	O
ii	O
surprisingly	O
the	O
asymptotic	O
properties	O
derived	O
in	O
the	O
euclidean	B
distance	O
ilx	O
this	O
chapter	O
remain	O
valid	O
to	O
a	O
wide	O
variety	O
of	O
metrics-the	O
asymptotic	O
probability	O
of	O
error	O
is	O
independent	O
of	O
the	O
distance	O
measure	O
problem	O
denote	O
the	O
probability	O
measure	O
for	O
x	O
by	O
fj	O
and	O
let	O
sxe	O
be	O
the	O
closed	O
ball	O
centered	O
at	O
x	O
of	O
radius	O
e	O
o	O
the	O
collection	O
of	O
all	O
x	O
with	O
fj	O
sx	O
for	O
all	O
e	O
is	O
called	O
the	O
support	B
of	O
x	O
or	O
fj	O
this	O
set	O
plays	O
a	O
key	O
role	O
because	O
of	O
the	O
following	O
property	O
lemma	O
if	O
x	O
e	O
supportfj	O
and	O
limn-oo	O
kin	O
then	O
iixkx	O
x	O
ii	O
with	O
probability	O
one	O
if	O
x	O
is	O
independent	O
of	O
the	O
data	O
and	O
has	O
probability	O
measure	O
then	O
iixkx	O
xii	O
with	O
probability	O
one	O
whenever	O
kin	O
o	O
proof	O
take	O
e	O
o	O
by	O
definition	O
x	O
e	O
supportfj	O
implies	O
that	O
fj	O
sxe	O
o	O
observe	O
that	O
iixkx	O
x	O
ii	O
e	O
if	O
and	O
only	O
if	O
n	O
lixes	O
n	O
il	O
k	O
n	O
i	O
xe	O
by	O
the	O
strong	O
law	O
of	O
large	O
numbers	O
the	O
left-hand	O
side	O
converges	O
to	O
fj	O
sxe	O
with	O
probability	O
one	O
while	O
by	O
assumption	O
the	O
right-hand	O
side	O
tends	O
to	O
zero	O
therefore	O
iixkx	O
xii	O
with	O
probability	O
one	O
the	O
second	O
statement	O
follows	O
from	O
the	O
previous	O
argument	O
as	O
well	O
first	O
note	O
that	O
by	O
lemma	O
in	O
the	O
appendix	O
px	O
e	O
supportfj	O
therefore	O
for	O
every	O
e	O
p	O
xii	O
e	O
e	O
xii	O
eix	O
e	O
supportfj	O
which	O
converges	O
to	O
zero	O
by	O
the	O
dominated	B
convergence	I
theorem	I
proving	O
gence	O
in	O
probability	O
if	O
k	O
does	O
not	O
change	O
with	O
n	O
then	O
ii	O
xkx	O
x	O
ii	O
is	O
monotone	O
decreasing	O
for	O
n	O
k	O
therefore	O
it	O
converges	O
with	O
probability	O
one	O
as	O
well	O
if	O
k	O
kn	O
is	O
allowed	O
to	O
grow	O
with	O
n	O
such	O
that	O
kin	O
then	O
using	O
the	O
notation	O
xknnx	O
xklx	O
we	O
see	O
by	O
a	O
similar	O
argument	O
that	O
the	O
sequence	O
of	O
monotone	O
decreasing	O
random	O
variables	O
sup	O
iixkmmx	O
xii	O
iixknnx	O
xii	O
mn	O
nearest	B
neighbor	I
rules	O
converges	O
to	O
zero	O
in	O
probability	O
and	O
therefore	O
with	O
probability	O
one	O
as	O
well	O
this	O
completes	O
the	O
proof	O
because	O
is	O
measurable	O
thus	O
well-behaved	O
in	O
a	O
general	O
sense	O
and	O
iixkx	O
xii	O
is	O
small	O
the	O
values	O
should	O
be	O
close	O
to	O
for	O
all	O
i	O
small	O
enough	O
we	O
now	O
introduce	O
a	O
proof	O
method	O
that	O
exploits	O
this	O
fact	O
and	O
will	O
make	O
subsequent	O
analyses	O
very	O
simple-it	O
suffices	O
to	O
look	O
at	O
data	O
samples	O
in	O
a	O
new	O
way	O
via	O
embedding	B
the	O
basic	O
idea	O
is	O
to	O
define	O
an	O
auxiliary	O
rule	B
gl	O
in	O
which	O
the	O
yixs	O
are	O
replaced	O
by	O
k	O
i	O
i	O
d	O
bernoulli	O
random	O
variables	O
with	O
parameter	O
the	O
yixs	O
behave	O
in	O
such	O
a	O
way	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
error	O
probabilities	O
of	O
the	O
two	O
rules	O
are	O
close	O
and	O
analyzing	O
the	O
behavior	O
of	O
the	O
auxiliary	O
rule	B
is	O
much	O
more	O
convenient	O
to	O
make	O
things	O
more	O
precise	O
we	O
assume	O
that	O
we	O
are	O
given	O
i	O
i	O
d	O
data	O
pairs	O
ud	O
un	O
all	O
distributed	O
as	O
u	O
where	O
x	O
is	O
as	O
before	O
has	O
probability	O
measure	O
f	O
l	O
on	O
the	O
borel	O
sets	O
of	O
r	O
d	O
and	O
u	O
is	O
uniformly	O
distributed	O
on	O
and	O
independent	O
of	O
x	O
if	O
we	O
set	O
y	O
i	O
luitjxi	O
then	O
y	O
i	O
y	O
n	O
are	O
i	O
i	O
d	O
and	O
distributed	O
as	O
the	O
prototype	B
pair	O
y	O
so	O
why	O
bother	O
with	O
the	O
uis	O
in	O
embedding	B
arguments	O
we	O
will	O
use	O
the	O
same	O
uis	O
to	O
construct	O
a	O
second	O
data	O
sequence	O
that	O
is	O
heavily	O
correlated	O
with	O
the	O
original	O
data	O
sequence	O
and	O
is	O
more	O
convenient	O
to	O
analyze	O
for	O
example	O
for	O
fixed	O
x	O
e	O
r	O
d	O
we	O
may	O
define	O
yx	O
luiryxl	O
we	O
now	O
have	O
an	O
i	O
i	O
d	O
sequence	O
with	O
i-th	O
vector	O
given	O
by	O
xi	O
yi	O
yx	O
ui	O
ordering	O
the	O
data	O
sequence	O
according	O
to	O
increasing	O
values	O
of	O
iixi	O
x	O
ii	O
yields	O
a	O
new	O
sequence	O
with	O
the	O
i-th	O
vector	O
denoted	O
by	O
xix	O
yix	O
yix	O
uix	O
if	O
no	O
confusion	O
is	O
possible	O
the	O
argument	O
x	O
will	O
be	O
dropped	O
a	O
rule	B
is	O
called	O
k-local	O
if	O
for	O
n	O
k	O
gn	O
is	O
of	O
the	O
form	O
x	O
if	O
ljrx	O
ylx	O
ykx	O
otherwise	O
gil	O
for	O
some	O
function	O
for	O
the	O
k-nn	B
rule	B
we	O
have	O
for	O
example	O
in	O
other	O
words	O
gn	O
takes	O
a	O
majority	B
vote	I
over	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
and	O
breaks	O
ties	O
in	O
favor	O
of	O
class	O
to	O
study	O
gn	O
turns	O
out	O
to	O
be	O
almost	O
equivalent	O
to	O
studying	O
the	O
approximate	O
rule	B
g	O
n	O
ifljrx	O
ylx	O
ykx	O
i	O
gil	O
otherwise	O
the	O
latter	O
rule	B
is	O
of	O
no	O
practical	O
value	O
because	O
it	O
requires	O
the	O
knowledge	O
of	O
interestingly	O
however	O
it	O
is	O
easier	O
to	O
study	O
as	O
ylx	O
ykx	O
are	O
i	O
i	O
d	O
whereas	O
ylx	O
yk	O
are	O
not	O
note	O
in	O
particular	O
the	O
following	O
lemma	O
for	O
all	O
x	O
n	O
k	O
notation	O
and	O
simple	O
asymptotics	O
p	O
ylx	O
yk	O
yclx	O
yck	O
le	O
k	O
il	O
and	O
pgnx	O
gx	O
le	O
k	O
il	O
proof	O
both	O
statements	O
follow	O
directly	O
from	O
the	O
observation	O
that	O
yclx	O
ykx	O
yclx	O
yckx	O
yk	O
yck	O
c	O
c	O
u	O
uixs	O
u	O
u	O
hxs	O
ucixs	O
k	O
k	O
il	O
il	O
and	O
using	O
the	O
union	O
bound	O
and	O
the	O
fact	O
that	O
the	O
uixs	O
are	O
uniform	O
we	O
need	O
the	O
following	O
result	O
in	O
which	O
x	O
is	O
distributed	O
as	O
xl	O
but	O
independent	O
of	O
the	O
data	O
sequence	O
lemma	O
for	O
any	O
integrable	O
function	O
f	O
any	O
n	O
and	O
any	O
k	O
n	O
k	O
le	O
kydelfxi	O
il	O
depends	O
upon	O
the	O
dimension	B
only	O
where	O
yd	O
the	O
proof	O
of	O
this	O
lemma	O
is	O
beautiful	O
but	O
a	O
bit	O
technical-it	O
is	O
given	O
in	O
a	O
separate	O
section	O
here	O
is	O
how	O
it	O
is	O
applied	O
and	O
why	O
for	O
fixed	O
k	O
we	O
may	O
think	O
of	O
fxkx	O
as	O
fx	O
for	O
all	O
practical	O
purposes	O
lemma	O
for	O
any	O
integrable	O
function	O
f	O
k	O
k	O
il	O
fxixi	O
as	O
n	O
whenever	O
kin	O
o	O
proof	O
given	O
e	O
find	O
a	O
uniformly	O
continuous	O
function	O
g	O
vanishing	O
off	O
a	O
boundedsetasuchthatelgx-	O
fxi	O
e	O
theorem	O
in	O
the	O
appendix	O
nearest	B
neighbor	I
rules	O
thenforeache	O
othereisao	O
such	O
that	O
ilx-zll	O
o	O
implies	O
e	O
thus	O
fxix	O
i	O
k	O
k	O
il	O
e	O
k	O
l	O
e	O
gxix	O
i	O
k	O
il	O
k	O
k	O
il	O
fxix	O
i	O
l	O
e	O
yde	O
e	O
iigiioop	O
xkx	O
ii	O
o	O
lemma	O
where	O
depends	O
on	O
e	O
only	O
yde	O
lemma	O
proof	O
of	O
stones	O
lemma	O
in	O
this	O
section	O
we	O
prove	O
lemma	O
for	O
e	O
e	O
a	O
cone	O
cx	O
e	O
is	O
the	O
lection	O
of	O
all	O
y	O
e	O
nd	O
for	O
which	O
anglex	O
y	O
e	O
equivalently	O
in	O
vector	O
notation	O
x	O
t	O
yiixillyi	O
cose	O
the	O
set	O
z	O
cx	O
e	O
is	O
the	O
translation	O
of	O
cx	O
e	O
by	O
z	O
figure	O
a	O
cone	O
of	O
angle	O
e	O
cx	O
e	O
ifyz	O
e	O
iizllthenily-zll	O
ilzllaswewillnowshow	O
indeed	O
ii	O
y	O
liz	O
y	O
ililz	O
ii	O
cosn	O
ii	O
z	O
ilzll	O
figure	O
proof	O
of	O
stones	O
lemma	O
figure	O
the	O
key	O
geometrical	O
erty	O
of	O
cones	O
of	O
angle	O
n	O
the	O
following	O
covering	B
lemma	I
is	O
needed	O
in	O
what	O
follows	O
lemma	O
lete	O
e	O
then	O
there	O
exists	O
a	O
set	O
end	O
such	O
that	O
yd	O
nd	O
u	O
cxi	O
e	O
il	O
furthermore	O
it	O
is	O
always	O
possible	O
to	O
take	O
ld	O
yd	O
since	O
for	O
e	O
n	O
we	O
have	O
yd	O
proof	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
iixi	O
ii	O
for	O
all	O
i	O
each	O
xi	O
is	O
the	O
center	O
of	O
a	O
sphere	B
si	O
of	O
radius	O
r	O
since	O
si	O
has	O
the	O
property	O
that	O
iixll	O
lnsi	O
iixll	O
lncxie	O
let	O
us	O
only	O
look	O
at	O
xi	O
such	O
that	O
iixi	O
j	O
r	O
for	O
all	O
j	O
i	O
i	O
in	O
that	O
case	O
u	O
cxi	O
e	O
covers	O
nd	O
if	O
and	O
only	O
if	O
u	O
si	O
covers	O
iix	O
ii	O
i	O
then	O
the	O
spheres	O
s	O
of	O
radius	O
centered	O
at	O
the	O
xis	O
are	O
disjoint	O
and	O
u	O
s	O
s	O
figure	O
nearest	B
neighbor	I
rules	O
figure	O
bounding	O
yd	O
thus	O
if	O
vd	O
volumesol	O
or	O
ydvdd	O
vdld	O
yd	O
l	O
sme	O
r	O
the	O
last	O
inequality	B
follows	O
from	O
the	O
fact	O
that	O
sin	O
d	O
v	O
figure	O
covering	O
the	O
space	O
by	O
cones	O
with	O
the	O
preliminary	O
results	O
out	O
of	O
the	O
way	O
we	O
cover	O
nd	O
by	O
yd	O
cones	O
x	O
j	O
yd	O
and	O
mark	O
in	O
each	O
cone	O
the	O
xi	O
that	O
is	O
nearest	O
to	O
the	O
asymptotic	O
probability	O
of	O
error	O
x	O
if	O
such	O
an	O
xi	O
exists	O
if	O
xi	O
belongs	O
to	O
x	O
cxj	O
and	O
is	O
not	O
marked	O
then	O
x	O
cannot	O
be	O
the	O
nearest	B
neighbor	I
of	O
xi	O
in	O
xi-i	O
x	O
xil	O
x	O
n	O
similarly	O
we	O
might	O
mark	O
all	O
k	O
nearest	O
neighbors	O
of	O
x	O
in	O
each	O
cone	O
there	O
are	O
less	O
than	O
k	O
points	O
in	O
a	O
cone	O
mark	O
all	O
of	O
them	O
by	O
a	O
similar	O
argument	O
if	O
xi	O
e	O
x	O
c	O
j	O
j	O
is	O
not	O
marked	O
then	O
x	O
cannot	O
be	O
among	O
the	O
k	O
nearest	O
bors	O
of	O
xi	O
in	O
xi	O
x	O
xi	O
x	O
n	O
order	O
of	O
this	O
set	O
of	O
points	O
is	O
important	O
if	O
distance	O
ties	O
occur	O
with	O
positive	O
probability	O
and	O
they	O
are	O
broken	O
by	O
comparing	O
indices	O
therefore	O
if	O
is	O
a	O
nonnegative	O
function	O
k	O
le	O
il	O
e	O
ixi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
xin	O
e	O
t	O
e	O
fx	O
t	O
ik	O
i	O
ix	O
is	O
among	O
the	O
knearest	O
neighbors	O
of	O
xi	O
in	O
exchanging	O
x	O
and	O
xi	O
kyde	O
x	O
as	O
we	O
can	O
mark	O
at	O
most	O
k	O
nodes	O
in	O
each	O
cone	O
and	O
the	O
number	O
of	O
cones	O
is	O
at	O
most	O
yd-see	O
lemma	O
this	O
concludes	O
the	O
proof	O
of	O
stones	O
lemma	O
the	O
asymptotic	O
probability	O
of	O
error	O
we	O
return	O
to	O
k-iocal	O
rules	O
in	O
particular	O
to	O
k-nearest	O
neighbor	O
rules	O
let	O
d	O
i	O
yi	O
vi	O
yn	O
vn	O
be	O
the	O
i	O
i	O
d	O
data	O
augmented	O
by	O
the	O
uniform	O
random	O
variables	O
vi	O
vn	O
as	O
described	O
earlier	O
for	O
a	O
decision	O
gn	O
based	O
on	O
dn	O
we	O
have	O
the	O
probability	O
of	O
error	O
ln	O
pgnx	O
i	O
yid	O
p	O
ylx	O
ykx	O
i	O
where	O
is	O
the	O
function	O
whose	O
sign	O
determines	O
gn	O
see	O
define	O
the	O
random	O
variables	O
yclx	O
yckx	O
as	O
we	O
did	O
earlier	O
and	O
set	O
l	O
p	O
yclx	O
yckx	O
nearest	B
neighbor	I
rules	O
by	O
lemmas	O
and	O
eiln	O
lzl	O
p	O
ylx	O
ykx	O
f	O
yclx	O
yck	O
l	O
e	O
k	O
il	O
because	O
limn---ooelz	O
eln	O
we	O
need	O
only	O
study	O
the	O
rule	B
gz	O
gn	O
otherwise	O
o	O
zk	O
are	O
i	O
i	O
d	O
bernoulli	O
unless	O
we	O
are	O
concerned	O
with	O
the	O
closeness	O
of	O
ln	O
to	O
eln	O
as	O
well	O
we	O
now	O
illustrate	O
this	O
important	O
time-saving	O
device	O
on	O
the	O
i-nearest	O
neighbor	O
rule	B
clearly	O
zi	O
and	O
therefore	O
el	O
pzj	O
f	O
y	O
e	O
we	O
have	O
without	O
further	O
work	O
theorem	O
for	O
the	O
nearest	B
neighbor	I
rulejor	O
any	O
distribution	O
ojx	O
y	O
lim	O
el	O
n	O
e	O
l	O
nn	O
n---oo	O
under	O
various	O
continuity	O
conditions	O
has	O
a	O
density	O
j	O
and	O
both	O
j	O
and	O
are	O
almost	O
everywhere	O
continuous	O
this	O
result	O
is	O
due	O
to	O
cover	O
and	O
hart	O
in	O
the	O
present	O
generality	O
it	O
essentially	O
appears	O
in	O
stone	O
see	O
also	O
devroye	O
elsewhere	O
we	O
show	O
that	O
l	O
lnn	O
l	O
hence	O
the	O
previous	O
result	O
says	O
that	O
the	O
nearest	B
neighbor	I
rule	B
is	O
asymptotically	O
at	O
most	O
twice	O
as	O
bad	O
as	O
the	O
bayes	O
rule-especially	O
for	O
small	O
l	O
this	O
property	O
should	O
be	O
useful	O
we	O
formally	O
define	O
the	O
quantity	O
when	O
k	O
is	O
odd	O
lknn	O
we	O
have	O
the	O
following	O
result	O
theorem	O
let	O
k	O
be	O
odd	O
andfixed	O
thenjor	O
the	O
k-nn	B
rule	B
lim	O
el	O
n	O
l	O
knn	O
n---oo	O
the	O
asymptotic	O
error	O
probability	O
of	O
weighted	B
nearest	B
neighbor	I
rules	O
proof	O
we	O
note	O
that	O
it	O
suffices	O
to	O
show	O
that	O
limnoo	O
el	O
lknn	O
the	O
ously	O
introduced	O
notation	O
but	O
for	O
every	O
n	O
el	O
p	O
zk	O
y	O
o	O
p	O
zk	O
y	O
i	O
p	O
zk	O
zo	O
o	O
p	O
zk	O
zo	O
i	O
zo	O
zk	O
are	O
i	O
i	O
d	O
bernoulli	O
random	O
variables	O
which	O
leads	O
directly	O
to	O
the	O
sought	O
result	O
several	O
representations	O
of	O
lknn	O
will	O
be	O
useful	O
for	O
later	O
analysis	O
for	O
example	O
we	O
have	O
lknn	O
e	O
ryxp	O
binomialk	O
i	O
x	O
e	O
ryxp	O
binomialk	O
ryx	O
i	O
x	O
e	O
e	O
ryx	O
binomialk	O
ryx	O
i	O
x	O
it	O
should	O
be	O
stressed	O
that	O
the	O
limit	O
result	O
in	O
theorem	O
is	O
distribution-free	O
the	O
limit	O
lknn	O
depends	O
upon	O
only	O
the	O
continuity	O
or	O
lack	O
of	O
smoothness	O
of	O
is	O
immaterial-it	O
only	O
matters	O
for	O
the	O
speed	O
with	O
which	O
el	O
n	O
approaches	O
the	O
limit	O
l	O
knn	O
the	O
asymptotic	O
error	O
probability	O
of	O
weighted	B
nearest	B
neighbor	I
rules	O
following	O
royall	O
a	O
weighted	B
nearest	B
neighbor	I
rule	B
with	O
weights	O
wi	O
wk	O
makes	O
a	O
decision	O
according	O
to	O
gnx	O
if	O
o	O
otherwise	O
l	O
-lcixi	O
w	O
l	O
l	O
-lycixo	O
w	O
l	O
in	O
case	O
of	O
a	O
voting	O
tie	O
this	O
rule	B
is	O
not	O
symnletric	O
we	O
may	O
modify	O
it	O
so	O
that	O
gn	O
dt	O
if	O
we	O
have	O
a	O
voting	O
tie	O
the	O
should	O
be	O
considered	O
as	O
an	O
indecision	O
by	O
nearest	B
neighbor	I
rules	O
previous	O
arguments	O
the	O
asymptotic	O
probability	O
of	O
error	O
is	O
a	O
function	O
of	O
wi	O
wk	O
given	O
by	O
lwl	O
wk	O
where	O
ap	O
p	O
wiy	O
t	O
wl	O
y	O
p	O
wi	O
y	O
t	O
wl-	O
y	O
p	O
where	O
now	O
y	O
y	O
are	O
i	O
i	O
d	O
bernoulli	O
equivalently	O
with	O
zi	O
e	O
ap	O
pp	O
wizi	O
o	O
pp	O
wizi	O
o	O
assume	O
that	O
p	O
wi	O
zi	O
o	O
for	O
now	O
then	O
if	O
p	O
ap	O
p	O
wizi	O
o	O
and	O
an	O
antisymmetric	O
expression	B
is	O
valid	O
when	O
p	O
note	O
next	O
the	O
following	O
ifweletnz	O
be	O
the	O
number	O
of	O
vectors	O
z	O
zk	O
e	O
lk	O
with	O
l	O
izil	O
i	O
and	O
l	O
wizi	O
then	O
nz	O
nk-z	O
e	O
thus	O
k	O
l	O
pk-z	O
l	O
pi	O
l	O
nz	O
pl-l	O
pi	O
zo	O
i	O
k	O
p	O
i	O
i	O
i	O
i	O
i	O
i	O
p	O
ik	O
even	O
note	O
that	O
i	O
i	O
i	O
i	O
does	O
not	O
depend	O
on	O
the	O
vector	O
of	O
weights	O
and	O
represents	O
pbinomialkl	O
p	O
pbinomialk	O
p	O
finally	O
since	O
p	O
i	O
i	O
l	O
nz	O
pl-z	O
pi	O
l	O
pi	O
o	O
the	O
asymptotic	O
error	O
probability	O
of	O
weighted	B
nearest	B
neighbor	I
rules	O
this	O
term	O
is	O
zero	O
if	O
and	O
only	O
if	O
nz	O
for	O
alii	O
in	O
other	O
words	O
it	O
vanishes	O
if	O
and	O
only	O
if	O
no	O
numerical	O
minority	O
of	O
wis	O
can	O
sum	O
to	O
a	O
majority	O
in	O
the	O
case	O
where	O
alone	O
a	O
numerical	O
minority	O
outweighs	O
the	O
others	O
but	O
such	O
cases	O
are	O
equivalent	O
to	O
ordinaryk-nearest	O
neighbor	O
rules	O
if	O
k	O
is	O
odd	O
when	O
k	O
is	O
even	O
and	O
we	O
add	O
a	O
tiny	O
weight	O
to	O
one	O
wi	O
as	O
in	O
e	O
ek	O
ek	O
k	O
k	O
k	O
for	O
small	O
e	O
then	O
no	O
numerical	O
minority	O
can	O
win	O
either	O
and	O
we	O
have	O
an	O
optimal	O
rule	B
i	O
we	O
have	O
thus	O
shown	O
the	O
following	O
theorem	O
and	O
jain	O
let	O
lwi	O
wk	O
be	O
the	O
asymptotic	O
probability	O
of	O
error	O
of	O
the	O
weighted	B
k-nn	B
rule	B
with	O
weights	O
wi	O
wk	O
let	O
the	O
k-nn	B
rule	B
be	O
defined	O
by	O
k	O
ii	O
k	O
ii	O
k	O
if	O
k	O
is	O
odd	O
and	O
by	O
k	O
ii	O
k	O
ii	O
k	O
for	O
e	O
i	O
k	O
when	O
k	O
is	O
even	O
denoting	O
the	O
asymptotic	O
probability	O
of	O
error	O
by	O
lknn	O
for	O
the	O
latter	O
rule	B
we	O
have	O
if	O
then	O
equality	O
occurs	O
ifand	O
only	O
if	O
every	O
numerical	O
minority	O
of	O
the	O
wis	O
carries	O
less	O
than	O
half	O
of	O
the	O
total	O
weight	O
the	O
result	O
states	O
that	O
standard	B
k-nearest	O
neighbor	O
rules	O
are	O
to	O
be	O
preferred	O
in	O
an	O
asymptotic	O
sense	O
this	O
does	O
not	O
mean	O
that	O
for	O
a	O
particular	O
sample	O
size	O
one	O
should	O
steer	O
clear	O
of	O
nonuniform	O
weights	O
in	O
fact	O
if	O
k	O
is	O
allowed	O
to	O
vary	O
with	O
n	O
then	O
nonuniform	O
weights	O
are	O
advantageous	O
consider	O
the	O
space	O
wofall	O
weight	O
vectors	O
wk	O
with	O
wi	O
ll	O
wi	O
is	O
it	O
totally	O
ordered	B
with	O
respect	O
to	O
lwi	O
wk	O
or	O
not	O
to	O
answer	O
this	O
tion	O
we	O
must	O
return	O
to	O
ap	O
once	O
again	O
the	O
weight	O
vector	O
only	O
influences	O
the	O
term	O
i	O
i	O
given	O
there	O
consider	O
for	O
example	O
the	O
weight	O
vectors	O
and	O
numerical	O
minorities	O
are	O
made	O
up	O
of	O
one	O
two	O
or	O
three	O
components	O
for	O
both	O
weight	O
vectors	O
ni	O
however	O
in	O
the	O
former	O
case	O
and	O
in	O
the	O
latter	O
thus	O
the	O
i	O
term	O
is	O
uniformly	O
smaller	O
over	O
all	O
p	O
in	O
the	O
latter	O
case	O
and	O
we	O
see	O
that	O
for	O
all	O
distributions	O
the	O
second	O
weight	O
vector	O
is	O
better	O
when	O
the	O
nts	O
are	O
not	O
strictly	O
nested	O
such	O
a	O
universal	O
comparison	O
becomes	O
impossible	O
as	O
in	O
the	O
example	O
of	O
problem	O
hence	O
w	O
is	O
only	O
partially	O
ordered	B
unwittingly	O
we	O
have	O
also	O
shown	O
the	O
following	O
theorem	O
theorem	O
for	O
all	O
distributions	O
l	O
l	O
l	O
lnn	O
nearest	B
neighbor	I
rules	O
proof	O
it	O
suffices	O
once	O
again	O
to	O
look	O
at	O
ap	O
consider	O
the	O
weight	O
vector	O
wi	O
normalization	O
as	O
for	O
the	O
i-nn	B
rule	B
the	O
term	O
i	O
i	O
is	O
zero	O
as	O
no	O
nl	O
nk	O
however	O
the	O
l-nn	O
rule	B
with	O
vector	O
wi	O
has	O
a	O
nonzero	O
term	O
i	O
i	O
because	O
no	O
nk-	O
yet	O
nk	O
ekl	O
hence	O
l	O
l	O
remark	O
we	O
have	O
strict	O
inequality	B
whenever	O
rj	O
when	O
l	O
we	O
have	O
lnn	O
lsnn	O
as	O
well	O
k-nearest	O
neighbor	O
rules	O
even	O
k	O
until	O
now	O
we	O
assumed	O
throughout	O
that	O
k	O
was	O
odd	O
so	O
that	O
voting	O
ties	O
were	O
avoided	O
the	O
tie-breaking	O
procedure	O
we	O
follow	O
forthe	O
neighbor	O
rule	B
is	O
as	O
follows	O
gnx	O
i	O
yclx	O
if	O
ycix	O
k	O
if	O
yix	O
k	O
if	O
ycix	O
k	O
formally	O
this	O
is	O
equivalent	O
to	O
a	O
weighted	B
neighbor	O
rule	B
with	O
weight	O
vector	O
it	O
is	O
easy	O
to	O
check	O
from	O
theorem	O
that	O
this	O
is	O
the	O
asymptotically	O
best	O
weight	O
vector	O
even	O
values	O
do	O
not	O
decrease	O
the	O
probability	O
of	O
error	O
in	O
particular	O
we	O
have	O
the	O
following	O
theorem	O
for	O
all	O
distributions	O
and	O
all	O
integers	O
k	O
l	O
proof	O
recall	O
that	O
lknn	O
may	O
be	O
written	O
in	O
the	O
form	O
lknn	O
where	O
lim	O
p	O
yix	O
x	O
n---oo	O
is	O
the	O
pointwise	O
asymptotic	O
error	O
probability	O
of	O
the	O
k-nn	B
rule	B
gk	O
it	O
is	O
convenient	O
to	O
consider	O
zl	O
i	O
i	O
d	O
l-valuedrandom	O
variables	O
withpzi	O
i	O
p	O
and	O
to	O
base	O
the	O
decision	O
upon	O
the	O
sign	O
of	O
zi	O
from	O
the	O
general	O
formula	O
for	O
weighted	B
nearest	B
neighbor	I
rules	O
the	O
pointwise	O
asymptotic	O
error	O
probability	O
of	O
the	O
rule	B
is	O
n---oo	O
n	O
lim	O
p	O
yix	O
x	O
pptziopptziozio	O
pp	O
pp	O
zi	O
inequalities	O
for	O
the	O
probability	O
of	O
error	O
pp	O
pp	O
o	O
lim	O
p	O
yjx	O
x	O
n----oo	O
n	O
therefore	O
inequalities	O
for	O
the	O
probability	O
of	O
error	O
we	O
return	O
to	O
the	O
case	O
when	O
k	O
is	O
odd	O
recall	O
that	O
where	O
rninp	O
p	O
lip	O
rninp	O
p	O
since	O
l	O
e	O
we	O
may	O
exploit	O
this	O
representation	O
to	O
obtain	O
a	O
variety	O
of	O
inequalities	O
on	O
lknn	O
l	O
we	O
begin	O
with	O
one	O
that	O
is	O
very	O
easy	O
to	O
prove	O
but	O
perhaps	O
not	O
the	O
strongest	O
theorem	O
for	O
all	O
odd	O
k	O
and	O
all	O
distributions	O
lknn	O
l	O
proof	O
by	O
the	O
above	O
representation	O
sup	O
pp	O
is	O
binomial	B
p	O
sup	O
kp	O
p	O
k	O
sup	O
p	O
the	O
okamoto-hoeffding	O
inequality-theorem	O
sup	O
ue-	O
o	O
nearest	B
neighbor	I
rules	O
theorem	O
and	O
gyorfi	O
for	O
all	O
distributions	O
and	O
all	O
odd	O
k	O
proof	O
we	O
note	O
that	O
for	O
p	O
with	O
b	O
binomial	B
p	O
e	O
kpl	O
p	O
jvarb	O
p	O
inequality	B
inequality	B
p	O
hence	O
lknn	O
l	O
e	O
ryx	O
inequality	B
nn	O
k	O
remark	O
for	O
large	O
k	O
b	O
is	O
approximately	O
normal	B
p	O
and	O
thus	O
eib	O
kpl	O
as	O
the	O
first	O
absolute	O
moment	O
of	O
a	O
normal	B
random	O
variable	B
is	O
problem	O
working	O
this	O
through	O
yields	O
an	O
approximate	O
bound	O
of	O
j	O
lnn	O
k	O
the	O
bound	O
is	O
proportional	O
to	O
this	O
can	O
be	O
improved	O
to	O
l	O
if	O
instead	O
of	O
bounding	O
it	O
from	O
above	O
by	O
markovs	O
inequality	B
we	O
directly	O
approximate	O
p	O
kp	O
k	O
p	O
as	O
shown	O
below	O
theorem	O
for	O
all	O
distributions	O
and	O
k	O
odd	O
where	O
y	O
supro	O
r	O
n	O
is	O
normal	B
and	O
refers	O
to	O
k	O
constants	O
are	O
given	O
in	O
the	O
proof	O
inequalities	O
for	O
the	O
probability	O
of	O
error	O
the	O
constant	O
y	O
in	O
the	O
proof	O
cannot	O
be	O
improved	O
a	O
slightly	O
weaker	O
bound	O
was	O
obtained	O
by	O
devijver	O
lknn	O
lnn	O
e	O
l	O
k	O
nk	O
l	O
lnn	O
k	O
see	O
lemma	O
see	O
also	O
devijver	O
and	O
kittler	O
lemma	O
for	O
p	O
and	O
with	O
k	O
odd	O
p	O
p	O
k	O
dx	O
a	O
proof	O
consider	O
k	O
i	O
i	O
d	O
uniform	O
random	O
variables	O
on	O
the	O
number	O
of	O
values	O
in	O
p	O
is	O
binomial	B
p	O
the	O
number	O
exceeds	O
if	O
and	O
only	O
ifthe	O
order	O
statistic	O
of	O
the	O
at	O
most	O
p	O
the	O
latter	O
is	O
beta	B
distributed	O
explaining	O
the	O
first	O
equality	O
note	O
that	O
we	O
have	O
written	O
a	O
discrete	O
sum	O
as	O
an	O
integral-in	O
some	O
cases	O
such	O
tricks	O
payoff	O
handsome	O
rewards	O
to	O
show	O
the	O
inequality	B
replace	O
x	O
by	O
ar	O
and	O
use	O
the	O
inequality	B
us	O
e-u	O
to	O
obtain	O
a	O
bound	O
as	O
shown	O
with	O
finally	O
a	O
p	O
kl	O
kl	O
is	O
binomial	B
k	O
k	O
kl	O
k-l	O
y	O
l	O
nearest	B
neighbor	I
rules	O
proof	O
of	O
theorem	O
from	O
earlier	O
remarks	O
lknn	O
l	O
e	O
e	O
cninryx	O
ryx	O
minryx	O
ryx	O
k	O
sup	O
b	O
p	O
l	O
is	O
binomial	B
p	O
we	O
merely	O
bound	O
the	O
factor	O
in	O
brackets	O
clearly	O
by	O
lemma	O
lknn	O
l	O
l	O
sup	O
a	O
i-jk-l	O
p	O
take	O
a	O
as	O
the	O
solution	O
of	O
y	O
which	O
is	O
possible	O
if	O
k	O
n	O
setting	O
v	O
p	O
we	O
have	O
i-jk-l	O
sup	O
p	O
max	O
sup	O
a-jk-l	O
sv	O
v	O
j	O
ovsa-jk-l	O
v	O
j	O
v	O
e-	O
sup	O
max	O
max	O
y	O
y	O
for	O
all	O
u	O
y	O
collect	O
all	O
bounds	O
and	O
note	O
that	O
a	O
behavior	O
when	O
l	O
is	O
small	O
in	O
this	O
section	O
we	O
look	O
more	O
closely	O
at	O
lknn	O
when	O
l	O
is	O
small	O
recalling	O
that	O
lknn	O
with	O
minp	O
minp	O
p	O
for	O
odd	O
k	O
it	O
is	O
easily	O
seen	O
that	O
lknn	O
function	O
because	O
for	O
some	O
behavior	O
when	O
l	O
is	O
small	O
mlllp	O
p	O
p	O
we	O
also	O
have	O
lknn	O
e	O
for	O
some	O
other	O
function	O
ljfk	O
out	O
forms	O
of	O
l	O
knn	O
include	O
l	O
exjl-	O
k-jl	O
l	O
ee	O
as	O
pa	O
pa	O
is	O
a	O
function	O
of	O
p	O
p	O
for	O
integer	O
a	O
this	O
may	O
be	O
further	O
reduced	O
to	O
simplified	O
forms	O
such	O
as	O
lnn	O
lsnn	O
e	O
the	O
behavior	O
of	O
ak	O
near	O
zero	O
is	O
very	O
informative	O
as	O
p	O
we	O
have	O
p	O
asp	O
pl	O
p	O
while	O
for	O
the	O
bayes	B
error	I
l	O
eaoo	O
where	O
a	O
oo	O
rninp	O
p	O
aj	O
pas	O
p	O
o	O
assume	O
that	O
p	O
at	O
all	O
x	O
then	O
as	O
p	O
lnn	O
aj	O
and	O
aj	O
l	O
moreover	O
lnn	O
l	O
aj	O
l	O
l	O
aj	O
assume	O
that	O
l	O
p	O
then	O
whereas	O
l	O
for	O
all	O
practical	O
purposes	O
the	O
rule	B
is	O
virtually	O
perfect	O
for	O
this	O
reason	O
the	O
rule	B
is	O
highly	O
recommended	O
little	O
is	O
gained	O
by	O
considering	O
the	O
rule	B
when	O
p	O
is	O
small	O
as	O
lsnn	O
l	O
let	O
ak	O
be	O
the	O
smallest	O
number	O
such	O
that	O
akp	O
ak	O
minp	O
p	O
for	O
all	O
p	O
tangents	O
in	O
figure	O
then	O
nearest	B
neighbor	I
rules	O
this	O
is	O
precisely	O
at	O
the	O
basis	O
of	O
the	O
inequalities	O
of	O
theorems	O
through	O
where	O
it	O
was	O
shown	O
that	O
ak	O
i	O
figure	O
akp	O
as	O
a	O
function	O
ofp	O
and	O
thus	O
the	O
classes	O
are	O
separated	O
this	O
does	O
not	O
imply	O
that	O
the	O
support	B
of	O
x	O
given	O
for	O
every	O
fixed	O
k	O
the	O
k-nearest	O
neighbor	O
rule	B
is	O
consistent	O
cover	O
has	O
a	O
beautiful	O
nearest	B
neighbor	I
rules	O
when	O
l	O
from	O
theorem	O
we	O
retain	O
that	O
if	O
l	O
then	O
lknn	O
for	O
all	O
k	O
in	O
fact	O
then	O
example	O
to	O
illustrate	O
this	O
remarkable	O
fact	O
l	O
implies	O
that	O
e	O
i	O
for	O
all	O
x	O
y	O
is	O
different	O
from	O
the	O
support	B
of	O
x	O
given	O
y	O
take	O
for	O
example	O
a	O
random	O
rational	O
number	O
from	O
generate	O
i	O
j	O
independently	O
and	O
at	O
random	O
from	O
the	O
geometric	B
distribution	I
on	O
and	O
set	O
x	O
min	O
j	O
maxi	O
j	O
every	O
rational	O
number	O
on	O
has	O
positive	O
probability	O
given	O
y	O
x	O
is	O
as	O
above	O
and	O
given	O
y	O
x	O
is	O
uniform	O
on	O
let	O
py	O
i	O
py	O
o	O
the	O
support	B
of	O
x	O
is	O
identical	O
in	O
both	O
cases	O
as	O
if	O
x	O
is	O
irrational	O
if	O
x	O
is	O
rational	O
i	O
we	O
see	O
that	O
l	O
and	O
that	O
the	O
nearest	B
neighbor	I
rule	B
is	O
consistent	O
if	O
someone	O
shows	O
us	O
a	O
number	O
x	O
drawn	O
from	O
the	O
same	O
distribution	O
as	O
the	O
data	O
then	O
we	O
may	O
decide	O
the	O
rationality	O
of	O
x	O
merely	O
by	O
looking	O
at	O
the	O
rationality	O
of	O
the	O
nearest	B
neighbor	I
of	O
x	O
although	O
we	O
did	O
not	O
show	O
this	O
the	O
same	O
is	O
true	O
if	O
we	O
are	O
given	O
any	O
x	O
e	O
lim	O
px	O
is	O
rational	O
ylx	O
is	O
not	O
rational	O
n---oo	O
lim	O
px	O
is	O
not	O
rational	O
ylx	O
is	O
rational	O
n---oo	O
problem	O
the	O
i-nearest	O
neighbor	O
rule	B
admissibility	O
of	O
the	O
nearest	B
neighbor	I
rule	B
the	O
consistency	B
theorems	O
of	O
chapter	O
show	O
us	O
that	O
we	O
should	O
take	O
k	O
k	O
n	O
in	O
the	O
k-nn	B
rule	B
the	O
decreasing	O
nature	O
ofl	O
knn	O
corroborates	O
this	O
yet	O
there	O
exist	O
distributions	O
for	O
which	O
for	O
all	O
n	O
the	O
i-nn	B
rule	B
is	O
better	O
than	O
the	O
k-nn	B
rule	B
for	O
any	O
k	O
this	O
observation	O
due	O
to	O
cover	O
and	O
hart	O
rests	O
on	O
the	O
following	O
class	O
of	O
examples	O
let	O
so	O
and	O
sl	O
be	O
two	O
spheres	O
of	O
radius	O
centered	O
at	O
a	O
and	O
b	O
where	O
iia	O
bll	O
given	O
y	O
x	O
is	O
uniform	O
on	O
sl	O
while	O
given	O
y	O
x	O
is	O
uniform	O
on	O
so	O
whereas	O
py	O
i	O
py	O
o	O
we	O
note	O
that	O
given	O
n	O
observations	O
with	O
the	O
i-nn	B
rule	B
eln	O
py	O
yn	O
i	O
py	O
yn	O
o	O
for	O
the	O
k-nn	B
rule	B
k	O
being	O
odd	O
we	O
have	O
el	O
n	O
p	O
y	O
t	O
p	O
t	O
p	O
s	O
n	O
when	O
k	O
jo	O
hence	O
the	O
k-nn	B
rule	B
is	O
worse	O
than	O
the	O
i-nn	B
rule	B
for	O
every	O
n	O
when	O
the	O
distribution	O
is	O
given	O
above	O
we	O
refer	O
to	O
the	O
exercises	O
regarding	O
some	O
interesting	O
admissibility	O
questions	O
for	O
k-nn	B
rules	O
the	O
i-nearest	O
neighbor	O
rule	B
in	O
hellman	O
proposed	O
the	O
i-nearest	O
neighbor	O
rule	B
which	O
is	O
tical	O
to	O
the	O
k-nearest	O
neighbor	O
rule	B
but	O
refuses	O
to	O
make	O
a	O
decision	O
unless	O
at	O
least	O
i	O
k	O
observations	O
are	O
from	O
the	O
same	O
class	O
formally	O
we	O
set	O
gnx	O
if	O
ll	O
yix	O
ski	O
if	O
ll	O
yix	O
i	O
i	O
otherwise	O
decision	O
define	O
the	O
pseudoprobability	O
of	O
error	O
by	O
ln	O
pgnx	O
yidj	O
that	O
is	O
the	O
probability	O
that	O
we	O
reach	O
a	O
decision	O
and	O
correctly	O
classify	O
x	O
clearly	O
ln	O
s	O
pgnx	O
yidn	O
our	O
standard	B
probability	O
of	O
error	O
the	O
latter	O
inequality	B
nearest	B
neighbor	I
rules	O
is	O
only	O
superficially	O
interesting	O
as	O
the	O
probability	O
of	O
not	O
reaching	O
a	O
decision	O
is	O
not	O
taken	O
into	O
account	O
in	O
ln	O
we	O
may	O
extend	O
theorem	O
to	O
show	O
the	O
following	O
theorem	O
for	O
the	O
i-nearest	O
neighbor	O
rule	B
the	O
pseudoprobability	O
of	O
error	O
ln	O
satisfies	O
n-oo	O
lim	O
el	O
n	O
k	O
pbinomialk	O
iix	O
def	O
lkl	O
the	O
above	O
result	O
is	O
distribution-free	O
note	O
that	O
the	O
k-nearest	O
neighbor	O
rule	B
for	O
odd	O
k	O
corresponds	O
to	O
the	O
limit	O
lkl	O
by	O
itself	O
is	O
not	O
interesting	O
but	O
it	O
was	O
shown	O
by	O
devijver	O
that	O
lkl	O
holds	O
information	O
regarding	O
the	O
bayes	B
error	I
l	O
theorem	O
for	O
all	O
distributions	O
and	O
with	O
k	O
odd	O
also	O
lknn	O
l	O
k	O
l	O
l	O
knn	O
this	O
theorem	O
which	O
we	O
refer	O
to	O
problem	O
shows	O
that	O
l	O
is	O
tightly	O
sandwiched	O
between	O
lknn	O
the	O
asymptotic	O
probability	O
of	O
error	O
of	O
the	O
k-nearest	O
neighbor	O
rule	B
and	O
the	O
rule	B
which	O
requires	O
that	O
the	O
difference	O
of	O
votes	O
between	O
the	O
two	O
classes	O
among	O
the	O
k	O
nearest	O
neighbors	O
be	O
at	O
least	O
two	O
if	O
ln	O
is	O
close	O
to	O
its	O
limit	O
and	O
if	O
we	O
can	O
estimate	O
ln	O
the	O
chapters	O
on	O
error	B
estimation	B
then	O
we	O
may	O
be	O
able	O
to	O
use	O
devijvers	O
inequalities	O
to	O
obtain	O
estimates	O
of	O
the	O
bayes	B
error	I
l	O
for	O
additional	O
results	O
see	O
loizou	O
and	O
maybank	O
as	O
a	O
corollary	O
of	O
devijvers	O
inequalities	O
we	O
note	O
that	O
l	O
knn	O
we	O
have	O
l	O
lknn	O
lkl	O
l	O
x	O
pbinomialk	O
ilx	O
and	O
therefore	O
i	O
x	O
pbinomialk	O
l	O
ix	O
problems	O
and	O
exercises	O
e	O
x	O
g	O
minryx	O
ryxjl	O
minryx	O
ryxk-l	O
llk	O
lk-l	O
kk	O
l	O
uk-l	O
reaches	O
its	O
maximum	O
on	O
at	O
u	O
lj	O
k	O
k	O
llk	O
zk-l	O
kj	O
lkl	O
by	O
stirlings	O
formula	O
with	O
i	O
we	O
thus	O
obtain	O
lknn	O
l	O
k	O
v	O
vk	O
improving	O
on	O
theorem	O
various	O
other	O
inequalities	O
may	O
be	O
derived	O
in	O
this	O
ner	O
as	O
well	O
problems	O
and	O
exercises	O
problem	O
let	O
ii	O
ii	O
be	O
an	O
arbitrary	O
norm	O
on	O
n	O
d	O
and	O
define	O
the	O
k-nearest	O
neighbor	O
rule	B
in	O
terms	O
of	O
the	O
distance	O
px	O
z	O
iix	O
z	O
ii	O
show	O
that	O
theorems	O
and	O
remain	O
valid	O
hint	O
only	O
stones	O
lemma	O
needs	O
adjusting	O
the	O
role	O
of	O
cones	O
cx	O
jt	O
used	O
in	O
the	O
proof	O
are	O
now	O
played	O
by	O
sets	O
with	O
the	O
following	O
property	O
x	O
and	O
z	O
belong	O
to	O
the	O
same	O
set	O
if	O
and	O
only	O
if	O
problem	O
does	O
there	O
exist	O
a	O
distribution	O
for	O
which	O
supn	O
i	O
e	O
ln	O
for	O
the	O
nearest	B
neighbor	I
rule	B
problem	O
show	O
that	O
and	O
that	O
lsnn	O
problem	O
show	O
that	O
if	O
c	O
is	O
a	O
compact	O
subset	O
of	O
n	O
d	O
and	O
c	O
is	O
the	O
support	B
set	O
for	O
the	O
probability	O
measure	O
fl	O
iixl	O
xii	O
sup	O
xecnc	O
with	O
probability	O
one	O
where	O
xl	O
is	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
xi	O
xn	O
nearest	B
neighbor	I
rules	O
problem	O
let	O
fl	O
be	O
the	O
probability	O
measure	O
of	O
x	O
given	O
y	O
and	O
let	O
v	O
be	O
the	O
probability	O
measure	O
of	O
x	O
given	O
y	O
assume	O
that	O
x	O
is	O
real-valued	O
and	O
that	O
py	O
o	O
py	O
i	O
find	O
a	O
pair	O
fl	O
such	O
that	O
supportfl	O
support	B
v	O
l	O
conclude	O
that	O
l	O
does	O
not	O
tell	O
us	O
a	O
lot	O
about	O
the	O
support	B
sets	O
of	O
fl	O
and	O
v	O
problem	O
consider	O
the	O
i-nearest	O
neighbor	O
rule	B
for	O
distributions	O
y	O
with	O
p	O
constant	O
and	O
y	O
independent	O
of	O
x	O
this	O
exercise	O
explores	O
the	O
behavior	O
of	O
as	O
p	O
to	O
for	O
fixed	O
integer	O
as	O
p	O
to	O
show	O
that	O
use	O
a	O
convenient	O
representation	O
of	O
to	O
conclude	O
that	O
as	O
p	O
t	O
p	O
kl	O
k	O
k	O
p	O
op	O
kl	O
problem	O
das	O
gupta	O
and	O
lin	O
proposed	O
the	O
following	O
rule	B
for	O
data	O
with	O
x	O
e	O
r	O
assume	O
x	O
is	O
nonatomic	O
first	O
reorder	O
xi	O
x	O
n	O
x	O
according	O
to	O
increasing	O
values	O
and	O
denote	O
the	O
ordered	B
set	O
by	O
x	O
xci	O
x	O
xil	O
xn	O
theyisare	O
permuted	O
so	O
that	O
yj	O
is	O
the	O
label	O
of	O
xj	O
take	O
votes	O
among	O
yil	O
until	O
for	O
the	O
first	O
time	O
there	O
is	O
agreement	B
yijl	O
at	O
which	O
time	O
we	O
decide	O
that	O
class	O
that	O
is	O
gn	O
yi	O
j	O
yi	O
j	O
this	O
rule	B
is	O
invariant	O
under	O
monotone	O
transformations	O
of	O
the	O
x-axis	O
if	O
l	O
denotes	O
the	O
asymptotic	O
expected	O
probability	O
of	O
error	O
show	O
that	O
for	O
all	O
atomic	O
x	O
l	O
e	O
i	O
show	O
that	O
l	O
is	O
the	O
same	O
as	O
for	O
the	O
rule	B
in	O
which	O
x	O
e	O
rd	O
and	O
we	O
consider	O
etc	O
rules	O
in	O
turn	O
stopping	O
at	O
the	O
first	O
rule	B
for	O
which	O
there	O
is	O
no	O
voting	O
tie	O
assume	O
for	O
simplicity	O
that	O
x	O
has	O
a	O
density	O
a	O
good	O
distance-tie	O
breaking	O
rule	B
this	O
may	O
be	O
dropped	O
show	O
that	O
l	O
lnn	O
l	O
nn	O
and	O
thus	O
that	O
l	O
l	O
show	O
that	O
l	O
l	O
nn	O
hence	O
the	O
rule	B
performs	O
somewhere	O
in	O
between	O
the	O
i-nn	B
and	O
rules	O
problem	O
let	O
y	O
be	O
independent	O
of	O
x	O
and	O
p	O
constant	O
consider	O
a	O
weighted	B
i-nearest	O
neighbor	O
rule	B
with	O
weights	O
are	O
where	O
k	O
m	O
for	O
m	O
we	O
obtain	O
the	O
rule	B
let	O
lk	O
m	O
be	O
the	O
asymptotic	O
probability	O
of	O
error	O
using	O
results	O
from	O
problem	O
show	O
that	O
lk	O
m	O
p	O
opk-ml	O
k-m	O
kml	O
as	O
p	O
t	O
conclude	O
that	O
within	O
this	O
class	O
of	O
rules	O
for	O
small	O
p	O
the	O
goodness	O
of	O
a	O
rule	B
is	O
measured	O
by	O
k	O
m	O
let	O
be	O
small	O
and	O
set	O
p	O
o	O
show	O
that	O
if	O
x	O
is	O
binomial	B
and	O
z	O
is	O
binomial	B
then	O
for	O
fixed	O
i	O
as	O
problems	O
and	O
exercises	O
pix	O
l	O
pz	O
l	O
l	O
and	O
pix	O
i	O
pz	O
l	O
i	O
conclude	O
that	O
for	O
fixed	O
k	O
m	O
as	O
lk	O
m	O
k	O
m	O
kpz	O
k	O
m	O
i	O
p	O
k	O
m	O
p	O
k	O
m	O
i	O
take	O
weight	O
vector	O
w	O
with	O
k	O
fixed	O
and	O
m	O
l	O
j	O
and	O
compare	O
it	O
with	O
weight	O
vector	O
w	O
with	O
components	O
and	O
m	O
as	O
p	O
and	O
p	O
t	O
assume	O
that	O
k	O
is	O
very	O
large	O
but	O
fixed	O
in	O
particular	O
show	O
that	O
w	O
is	O
better	O
as	O
p	O
and	O
w	O
is	O
better	O
as	O
p	O
t	O
for	O
the	O
last	O
example	O
note	O
that	O
for	O
fixed	O
c	O
as	O
k	O
oo	O
by	O
the	O
central	B
limit	I
theorem	I
conclude	O
that	O
there	O
exist	O
different	O
weight	O
vectors	O
w	O
wi	O
for	O
which	O
there	O
exists	O
a	O
pair	O
of	O
distributions	O
of	O
y	O
such	O
that	O
their	O
asymptotic	O
error	O
probabilities	O
are	O
differently	O
ordered	B
thus	O
w	O
is	O
not	O
totally	O
ordered	B
with	O
respect	O
to	O
the	O
probability	O
of	O
error	O
problem	O
patrick	O
and	O
fisher	O
find	O
the	O
k-th	O
nearest	B
neighbor	I
in	O
each	O
of	O
the	O
two	O
classes	O
and	O
classify	O
according	O
to	O
which	O
is	O
nearest	O
show	O
that	O
their	O
rule	B
is	O
equivalent	O
to	O
a	O
i-nearest	O
neighbor	O
rule	B
problem	O
rabiner	O
et	O
al	O
generalize	O
the	O
rule	B
of	O
problem	O
so	O
as	O
to	O
classify	O
according	O
to	O
the	O
average	O
distance	O
to	O
the	O
k-th	O
nearest	B
neighbor	I
within	O
each	O
class	O
assume	O
that	O
x	O
has	O
a	O
density	O
for	O
fixed	O
k	O
find	O
the	O
asymptotic	O
probability	O
of	O
error	O
problem	O
if	O
n	O
is	O
normal	B
then	O
eini	O
prove	O
this	O
problem	O
show	O
that	O
if	O
then	O
llllnn	O
problem	O
show	O
that	O
l	O
l	O
for	O
all	O
distributions	O
hint	O
find	O
the	O
smallest	O
constanta	O
such	O
that	O
l	O
a	O
using	O
the	O
representation	O
of	O
in	O
terms	O
of	O
the	O
binomial	B
tail	O
problem	O
show	O
that	O
if	O
x	O
has	O
a	O
density	O
j	O
then	O
for	O
all	O
u	O
lim	O
p	O
xii	O
ux	O
e-fxvu	O
n--oo	O
d	O
with	O
probability	O
one	O
where	O
v	O
is	O
dx	O
is	O
the	O
volume	O
ofthe	O
unit	O
ball	O
in	O
n	O
d	O
nearest	B
neighbor	I
rules	O
problem	O
consider	O
a	O
rule	B
that	O
takes	O
a	O
majority	B
vote	I
over	O
all	O
yi	O
for	O
which	O
iixi	O
v	O
n	O
j	O
d	O
where	O
v	O
is	O
dx	O
is	O
the	O
volume	O
of	O
the	O
unit	O
ball	O
and	O
c	O
is	O
fixed	O
in	O
case	O
of	O
a	O
tie	O
decide	O
gnx	O
o	O
if	O
x	O
has	O
a	O
density	O
j	O
show	O
thatliminfn--ooeln	O
e	O
hint	O
use	O
the	O
obvious	O
inequality	B
el	O
n	O
py	O
fj--nsxcjvn	O
oj	O
if	O
y	O
is	O
independent	O
of	O
x	O
and	O
p	O
then	O
e	O
l	O
e	O
too	O
p	O
as	O
p	O
t	O
show	O
this	O
conclude	O
that	O
sup	O
lim	O
infn--oo	O
el	O
n	O
l	O
and	O
thus	O
that	O
distribution-free	O
bounds	O
of	O
the	O
form	O
limn--	O
oo	O
e	O
ln	O
c	O
l	O
obtained	O
for	O
k-nearest	O
neighbor	O
estimates	O
do	O
not	O
exist	O
for	O
these	O
simple	O
rules	O
problem	O
take	O
an	O
example	O
with	O
and	O
show	O
that	O
the	O
bound	O
lknn	O
l	O
ijke	O
cannot	O
be	O
essentially	O
bettered	O
for	O
large	O
values	O
of	O
k	O
that	O
is	O
there	O
exists	O
a	O
sequence	O
of	O
distributions	O
by	O
k	O
for	O
which	O
as	O
k	O
where	O
n	O
is	O
a	O
normal	B
random	O
variable	B
problem	O
if	O
b	O
is	O
binomial	B
p	O
then	O
sup	O
pb	O
i	O
i	O
n	O
v	O
p	O
i	O
n	O
problem	O
show	O
that	O
for	O
k	O
k	O
k	O
problem	O
show	O
that	O
there	O
exists	O
a	O
sequence	O
of	O
distributions	O
of	O
y	O
by	O
k	O
in	O
which	O
y	O
is	O
independent	O
of	O
x	O
and	O
p	O
p	O
depending	O
on	O
k	O
only	O
such	O
that	O
lim	O
inf	O
n--oo	O
lknn	O
l	O
l	O
v	O
k	O
y	O
where	O
y	O
is	O
the	O
constant	O
of	O
theorem	O
hint	O
verify	O
the	O
proof	O
of	O
orem	O
but	O
bound	O
things	O
from	O
below	O
sluds	O
inequality	B
lemma	O
in	O
the	O
appendix	O
may	O
be	O
of	O
use	O
here	O
problem	O
consider	O
a	O
weighted	B
nearest	B
neighbor	I
rule	B
with	O
weights	O
p	O
for	O
p	O
show	O
that	O
the	O
expected	O
probability	O
of	O
error	O
tends	O
for	O
all	O
distributions	O
to	O
a	O
limit	O
lp	B
hint	O
truncate	O
at	O
k	O
fixed	O
but	O
large	O
and	O
argue	O
that	O
the	O
tail	O
has	O
asymptotically	O
negligible	O
weight	O
problems	O
and	O
exercises	O
problem	O
continued	O
with	O
lp	B
as	O
in	O
the	O
previous	O
exercise	O
show	O
that	O
lp	B
lnn	O
whenever	O
p	O
problem	O
continued	O
prove	O
or	O
disprove	O
as	O
p	O
increases	O
from	O
to	O
lp	B
decreases	O
monotonically	O
from	O
lnn	O
to	O
l	O
question	O
is	O
difficult	O
problem	O
show	O
that	O
in	O
the	O
weighted	B
nn	O
rule	B
with	O
weights	O
p	O
p	O
the	O
asymptotic	O
probability	O
of	O
error	O
is	O
lnn	O
if	O
p	O
and	O
is	O
if	O
p	O
problem	O
is	O
there	O
any	O
k	O
other	O
than	O
one	O
for	O
which	O
the	O
k-nn	B
rule	B
is	O
admissible	O
that	O
is	O
for	O
which	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
such	O
that	O
el	O
n	O
for	O
the	O
k-nn	B
rule	B
is	O
smaller	O
than	O
el	O
n	O
for	O
any	O
k-nn	B
rule	B
with	O
k	O
k	O
for	O
all	O
n	O
hint	O
this	O
is	O
difficult	O
note	O
that	O
if	O
this	O
is	O
to	O
hold	O
for	O
all	O
n	O
then	O
it	O
must	O
hold	O
for	O
the	O
limits	O
from	O
this	O
deduce	O
that	O
with	O
probability	O
one	O
e	O
i	O
for	O
any	O
such	O
distribution	O
problem	O
for	O
every	O
fixed	O
n	O
and	O
odd	O
k	O
with	O
n	O
loook	O
find	O
a	O
distribution	O
of	O
y	O
such	O
that	O
eln	O
for	O
the	O
k-nn	B
rule	B
is	O
smaller	O
than	O
eln	O
for	O
any	O
k-nn	B
rule	B
with	O
k	O
k	O
k	O
odd	O
thus	O
for	O
a	O
given	O
n	O
no	O
k	O
can	O
be	O
a	O
priori	O
discarded	O
from	O
consideration	O
problem	O
let	O
x	O
be	O
uniform	O
on	O
x	O
and	O
pry	O
o	O
pry	O
i	O
show	O
that	O
for	O
the	O
nearest	B
neighbor	I
rule	B
eln	O
and	O
hart	O
peterson	O
problem	O
for	O
the	O
nearest	B
neighbor	I
rule	B
if	O
x	O
has	O
a	O
density	O
then	O
ieln	O
elndl	O
nl	O
problem	O
let	O
x	O
have	O
a	O
density	O
f	O
c	O
on	O
and	O
assume	O
that	O
f	O
and	O
f	O
exist	O
and	O
are	O
uniformly	O
bounded	O
show	O
that	O
for	O
the	O
nearest	B
neighbor	I
rule	B
e	O
ln	O
lnn	O
for	O
d-dimensional	O
problems	O
this	O
result	O
was	O
generalized	B
by	O
psaltis	O
snapp	O
and	O
venkatesh	O
problem	O
show	O
that	O
lknn	O
is	O
the	O
best	O
possible	O
bound	O
of	O
the	O
form	O
lknn	O
a	O
valid	O
simultaneously	O
for	O
all	O
k	O
problem	O
show	O
that	O
lknn	O
for	O
all	O
k	O
problem	O
let	O
x	O
e	O
consider	O
the	O
nearest	B
neighbor	I
rule	B
based	O
upon	O
vectors	O
with	O
components	O
x	O
x	O
show	O
that	O
this	O
is	O
asymptotically	O
not	O
better	O
than	O
if	O
we	O
had	O
used	O
show	O
by	O
example	O
that	O
x	O
x	O
may	O
yield	O
a	O
worse	O
asymptotic	O
error	O
probability	O
than	O
problem	O
uniform	O
order	B
statistics	I
let	O
un	O
be	O
order	B
statistics	I
of	O
n	O
i	O
i	O
d	O
uniform	O
random	O
variables	O
show	O
the	O
following	O
uk	O
is	O
beta	B
n	O
k	O
that	O
is	O
uk	O
has	O
density	O
f	O
x	O
n	O
x	O
k-	O
xn-k	O
x	O
k	O
nearest	B
neighbor	I
rules	O
a	O
e	O
rk	O
arn	O
rkrn	O
a	O
for	O
any	O
a	O
l	O
a	O
a	O
eu	O
n	O
k	O
for	O
a	O
where	O
l	O
a	O
is	O
a	O
function	O
of	O
a	O
only	O
problem	O
dudanis	O
rule	B
dudani	O
proposes	O
a	O
weighted	B
k-nn	B
rule	B
where	O
yix	O
receives	O
weight	O
iixkx	O
xii	O
iixix	O
xii	O
i	O
k	O
why	O
is	O
this	O
roughly	O
speaking	O
equivalent	O
to	O
attaching	O
weight	O
i	O
kld	O
to	O
the	O
i-th	O
nearest	B
neighbor	I
if	O
x	O
has	O
a	O
density	O
hint	O
if	O
is	O
the	O
probability	O
measure	O
of	O
x	O
then	O
are	O
distributed	O
like	O
uk	O
where	O
ul	O
un	O
are	O
the	O
order	B
statistics	I
of	O
n	O
i	O
i	O
d	O
uniform	O
random	O
variables	O
replace	O
by	O
a	O
good	O
local	O
approximation	O
and	O
use	O
results	O
from	O
the	O
previous	O
exercise	O
problem	O
show	O
devijvers	O
theorem	O
in	O
two	O
parts	O
first	O
establish	O
the	O
inequality	B
l	O
for	O
the	O
tennis	B
rule	B
and	O
then	O
establish	O
the	O
monotonicity	O
problem	O
show	O
theorem	O
for	O
the	O
i	O
nearest	B
neighbor	I
rule	B
problem	O
let	O
r	O
be	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
neighbor	O
rule	B
prove	O
that	O
r	O
l	O
nn	O
problem	O
for	O
the	O
nearest	B
neighbor	I
rule	B
show	O
that	O
for	O
all	O
distributions	O
lim	O
pgnx	O
y	O
i	O
n-oo	O
lim	O
pgnx	O
y	O
o	O
n-oo	O
and	O
kittler	O
thus	O
errors	O
of	O
both	O
kinds	O
are	O
equally	O
likely	O
problem	O
let	O
py	O
i	O
py	O
o	O
and	O
let	O
x	O
be	O
a	O
random	O
rational	O
if	O
y	O
defined	O
in	O
section	O
such	O
that	O
every	O
rational	O
number	O
has	O
positive	O
probability	O
and	O
let	O
x	O
be	O
uniform	O
if	O
y	O
show	O
that	O
for	O
every	O
x	O
e	O
not	O
rational	O
pylx	O
i	O
as	O
n	O
while	O
for	O
every	O
x	O
e	O
rational	O
pylx	O
o	O
as	O
n	O
problem	O
let	O
xl	O
xn	O
be	O
i	O
i	O
d	O
and	O
have	O
a	O
common	O
density	O
show	O
that	O
for	O
fixed	O
k	O
np	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
xl	O
and	O
in	O
xn	O
show	O
that	O
the	O
same	O
result	O
remains	O
valid	O
whenever	O
k	O
varies	O
with	O
n	O
such	O
that	O
kl	O
in	O
problems	O
and	O
exercises	O
problem	O
imperfect	B
training	I
let	O
y	O
z	O
yi	O
zd	O
quence	O
of	O
i	O
i	O
d	O
triples	O
in	O
rd	O
x	O
i	O
x	O
i	O
with	O
pry	O
llx	O
x	O
and	O
pz	O
llx	O
x	O
let	O
zlx	O
be	O
z	O
j	O
if	O
x	O
j	O
is	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
x	O
i	O
x	O
n	O
show	O
that	O
lim	O
pzlx	O
y	O
e	O
n-oo	O
problem	O
improve	O
the	O
bound	O
in	O
lemma	O
to	O
yd	O
then	O
yd	O
problem	O
show	O
that	O
if	O
cxyd	O
is	O
a	O
collection	O
of	O
cones	O
ing	O
r	O
d	O
problem	O
recalling	O
that	O
lknn	O
e	O
where	O
akp	O
minp	O
p	O
pip	O
minp	O
p	O
show	O
that	O
for	O
every	O
fixed	O
p	O
p	O
minp	O
p	O
as	O
k	O
is	O
the	O
mono	O
tonicity	O
that	O
is	O
harder	O
to	O
show	O
how	O
would	O
you	O
then	O
prove	O
that	O
limk---oo	O
lknn	O
l	O
problem	O
show	O
that	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
rule	B
that	O
decides	O
gnx	O
is	O
identical	O
to	O
that	O
of	O
the	O
rule	B
in	O
which	O
gnx	O
problem	O
show	O
that	O
for	O
all	O
distributions	O
lsnn	O
e	O
where	O
vrsu	O
u	O
problem	O
show	O
that	O
for	O
all	O
distributions	O
and	O
that	O
problem	O
let	O
xl	O
be	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
x	O
n	O
construct	O
an	O
example	O
for	O
which	O
e	O
ii	O
xl	O
x	O
ii	O
for	O
all	O
x	O
e	O
rd	O
we	O
have	O
to	O
steer	O
clear	O
of	O
convergence	O
in	O
the	O
mean	O
in	O
lemma	O
let	O
x	O
be	O
the	O
nearest	B
neighbor	I
of	O
x	O
i	O
among	O
x	O
x	O
n	O
construct	O
a	O
distribution	O
such	O
that	O
eiixl	O
xiii	O
for	O
all	O
n	O
problem	O
consider	O
the	O
weighted	B
nearest	B
neighbor	I
rule	B
with	O
weights	O
wk	O
define	O
a	O
new	O
weight	O
vector	O
wk-l	O
vi	O
vz	O
where	O
vi	O
wk	O
thus	O
the	O
weight	O
vectors	O
are	O
partially	O
ordered	B
by	O
the	O
operation	O
assume	O
that	O
all	O
weights	O
are	O
nonnegative	O
let	O
the	O
asymptotic	O
expected	O
probability	O
of	O
errors	O
be	O
land	O
l	O
respectively	O
true	O
or	O
false	O
for	O
all	O
distributions	O
of	O
y	O
l	O
l	O
problem	O
gabriel	O
neighbors	O
given	O
xl	O
xn	O
e	O
r	O
d	O
we	O
say	O
that	O
xi	O
and	O
xj	O
are	O
gabriel	O
neighbors	O
if	O
the	O
ball	O
centered	O
at	O
x	O
j	O
of	O
radius	O
iixi	O
xj	O
contains	O
no	O
x	O
k	O
k	O
i	O
j	O
and	O
sokal	O
matula	O
and	O
sokal	O
clearly	O
if	O
xj	O
is	O
the	O
nearest	B
neighbor	I
of	O
xi	O
then	O
xi	O
and	O
xj	O
are	O
gabriel	O
neighbors	O
show	O
that	O
if	O
x	O
has	O
a	O
density	O
and	O
x	O
i	O
xn	O
are	O
i	O
i	O
d	O
and	O
drawn	O
from	O
the	O
distribution	O
of	O
x	O
then	O
the	O
expected	O
number	O
of	O
gabriel	O
neighbors	O
of	O
xl	O
tends	O
to	O
as	O
n	O
nearest	B
neighbor	I
rules	O
figure	O
the	O
gabriel	B
graph	I
of	O
points	O
on	O
the	O
plane	O
is	O
shown	O
gabriel	O
neighbors	O
are	O
connected	O
by	O
an	O
edge	O
note	O
that	O
all	O
circles	O
dling	O
these	O
edges	O
have	O
no	O
data	O
point	O
in	O
their	O
interior	O
problem	O
gabriel	B
neighbor	I
rule	B
define	O
the	O
gabriel	B
neighbor	I
rule	B
simply	O
as	O
the	O
rule	B
that	O
takes	O
a	O
majority	B
vote	I
over	O
all	O
yi	O
for	O
the	O
gabriel	O
neighbors	O
of	O
x	O
among	O
xl	O
x	O
n	O
ties	O
are	O
broken	O
by	O
flipping	O
a	O
coin	O
let	O
ln	O
be	O
the	O
conditional	O
probability	O
of	O
error	O
for	O
the	O
gabriel	O
rule	B
using	O
the	O
result	O
of	O
the	O
previous	O
exercise	O
show	O
that	O
if	O
l	O
is	O
the	O
bayes	B
error	I
then	O
for	O
determine	O
the	O
best	O
possible	O
value	O
of	O
c	O
hint	O
use	O
theorem	O
and	O
try	O
obtaining	O
for	O
d	O
a	O
lower	O
bound	O
for	O
p	O
x	O
where	O
n	O
x	O
is	O
the	O
number	O
of	O
gabriel	O
neighbors	O
of	O
x	O
among	O
xl	O
xn	O
lim	O
el	O
n	O
if	O
l	O
n---oo	O
limsupel	O
n	O
lnn	O
if	O
l	O
d	O
n--oo	O
lim	O
sup	O
eln	O
cl	O
for	O
some	O
c	O
if	O
d	O
n--oo	O
consistency	B
universal	B
consistency	B
if	O
we	O
are	O
given	O
a	O
sequence	O
dn	O
yi	O
yn	O
of	O
training	O
data	O
the	O
best	O
we	O
can	O
expect	O
from	O
a	O
classification	O
function	O
is	O
to	O
achieve	O
the	O
bayes	B
error	I
probability	O
l	O
generally	O
we	O
cannot	O
hope	O
to	O
obtain	O
a	O
function	O
that	O
exactly	O
achieves	O
the	O
bayes	B
error	I
probability	O
but	O
it	O
is	O
possible	O
to	O
construct	O
a	O
sequence	O
of	O
classification	O
functions	O
that	O
is	O
a	O
classification	O
rule	B
such	O
that	O
the	O
error	O
probability	O
gets	O
arbitrarily	O
close	O
to	O
l	O
with	O
large	O
probability	O
is	O
for	O
dn	O
this	O
idea	O
is	O
formulated	O
in	O
the	O
definitions	O
of	O
consistency	B
definition	O
and	O
strong	B
consistency	B
a	O
classification	O
rule	B
is	O
sistent	O
asymptotically	O
bayes-risk	O
efficient	O
for	O
a	O
certain	O
distribution	O
of	O
y	O
if	O
and	O
strongly	O
consistent	O
if	O
lim	O
ln	O
l	O
with	O
probability	O
n-oo	O
remark	O
consistency	B
is	O
defined	O
as	O
the	O
convergence	O
of	O
the	O
expected	O
value	O
of	O
ln	O
to	O
l	O
since	O
ln	O
is	O
a	O
random	O
variable	B
bounded	O
between	O
l	O
and	O
this	O
convergence	O
consistency	B
every	O
e	O
is	O
equivalent	O
to	O
the	O
convergence	O
of	O
ln	O
to	O
l	O
in	O
probability	O
which	O
means	O
that	O
for	O
lim	O
p	O
l	O
e	O
n---oo	O
obviously	O
since	O
almost	O
sure	O
convergence	O
always	O
implies	O
convergence	O
in	O
bility	O
strong	B
consistency	B
implies	O
consistency	B
d	O
a	O
consistent	O
rule	B
guarantees	O
that	O
by	O
increasing	O
the	O
amount	O
of	O
data	O
the	O
probability	O
that	O
the	O
error	O
probability	O
is	O
within	O
a	O
very	O
small	O
distance	O
of	O
the	O
optimal	O
achievable	O
gets	O
arbitrarily	O
close	O
to	O
one	O
intuitively	O
the	O
rule	B
can	O
eventually	O
learn	O
the	O
optimal	O
decision	O
from	O
a	O
large	O
amount	O
of	O
training	O
data	O
with	O
high	O
probability	O
strong	O
sistency	O
means	O
that	O
by	O
using	O
more	O
data	O
the	O
error	O
probability	O
gets	O
arbitrarily	O
close	O
to	O
the	O
optimum	O
for	O
every	O
training	O
sequence	O
except	O
for	O
a	O
set	O
of	O
sequences	O
that	O
has	O
zero	O
probability	O
altogether	O
a	O
decision	O
rule	B
can	O
be	O
consistent	O
for	O
a	O
certain	O
class	O
of	O
distributions	O
of	O
y	O
but	O
may	O
not	O
be	O
consistent	O
for	O
others	O
it	O
is	O
clearly	O
desirable	O
to	O
have	O
a	O
rule	B
that	O
is	O
consistent	O
for	O
a	O
large	O
class	O
of	O
distributions	O
since	O
in	O
many	O
situations	O
we	O
do	O
not	O
have	O
any	O
prior	O
information	O
about	O
the	O
distribution	O
it	O
is	O
essential	O
to	O
have	O
a	O
rule	B
that	O
gives	O
good	O
performance	O
for	O
all	O
distributions	O
this	O
very	O
strong	O
requirement	O
of	O
universal	O
goodness	O
is	O
formulated	O
as	O
follows	O
definition	O
consistency	B
a	O
sequence	O
of	O
decision	O
rules	O
is	O
called	O
universally	O
consistent	O
if	O
it	O
is	O
consistent	O
for	O
any	O
distribution	O
of	O
the	O
pair	O
y	O
in	O
this	O
chapter	O
we	O
show	O
that	O
such	O
universally	O
consistent	O
classification	O
rules	O
exist	O
at	O
first	O
this	O
may	O
seem	O
very	O
surprising	O
for	O
some	O
distributions	O
are	O
very	O
and	O
seem	O
hard	O
to	O
learn	O
for	O
example	O
let	O
x	O
be	O
uniformly	O
distributed	O
on	O
with	O
probability	O
and	O
let	O
x	O
be	O
atomic	O
on	O
the	O
rationals	O
with	O
probability	O
for	O
example	O
if	O
the	O
rationals	O
are	O
enumerated	O
rl	O
then	O
px	O
rd	O
let	O
y	O
if	O
x	O
is	O
rational	O
and	O
y	O
if	O
x	O
is	O
irrational	O
obviously	O
l	O
if	O
a	O
classification	O
rule	B
gn	O
is	O
consistent	O
then	O
the	O
probability	O
of	O
incorrectly	O
guessing	O
the	O
rationality	O
of	O
x	O
tends	O
to	O
zero	O
note	O
here	O
that	O
we	O
cannot	O
whether	O
x	O
is	O
rational	O
or	O
not	O
but	O
we	O
should	O
base	O
our	O
decision	O
solely	O
on	O
the	O
data	O
dn	O
given	O
to	O
us	O
one	O
consistent	O
rule	B
is	O
the	O
following	O
gnx	O
dn	O
yk	O
if	O
x	O
k	O
is	O
the	O
closest	O
point	O
to	O
x	O
among	O
xl	O
x	O
n	O
the	O
fact	O
that	O
the	O
rationals	O
are	O
dense	O
in	O
makes	O
the	O
statement	O
even	O
more	O
surprising	O
see	O
problem	O
classification	O
and	O
regression	O
estimation	B
in	O
this	O
section	O
we	O
show	O
how	O
consistency	B
of	O
classification	O
rules	O
can	O
be	O
deduced	O
from	O
consistent	O
regression	O
estimation	B
in	O
many	O
cases	O
the	O
a	B
posteriori	I
probability	I
rjx	O
is	O
estimated	O
from	O
the	O
training	O
data	O
dn	O
by	O
some	O
function	O
rjnx	O
rjnx	O
dn	O
classification	O
and	O
regression	O
estimation	B
in	O
this	O
case	O
the	O
error	O
probability	O
lgn	O
pgnx	O
yidn	O
of	O
the	O
plug-in	O
rule	B
if	O
rjnx	O
gnx	O
otherwise	O
is	O
a	O
random	O
variable	B
then	O
a	O
simple	O
corollary	O
of	O
theorem	O
is	O
as	O
follows	O
corollary	O
the	O
error	O
probability	O
of	O
the	O
classifier	B
gn	O
defined	O
above	O
fies	O
the	O
inequality	B
the	O
next	O
corollary	O
follows	O
from	O
the	O
cauchy-schwarz	B
inequality	B
corollary	O
if	O
if	O
rjnx	O
h	O
ot	O
erwise	O
gnx	O
then	O
its	O
error	O
probability	O
satisfies	O
clearly	O
rjx	O
py	O
iix	O
x	O
eyix	O
x	O
is	O
just	O
the	O
regression	B
function	I
of	O
y	O
on	O
x	O
therefore	O
the	O
most	O
interesting	O
consequence	O
of	O
theorem	O
is	O
that	O
the	O
mere	O
existence	O
of	O
a	O
regression	B
function	I
estimate	O
rjn	O
for	O
which	O
in	O
probability	O
or	O
with	O
probability	O
one	O
implies	O
that	O
the	O
plug-in	B
decision	I
rule	B
gn	O
is	O
consistent	O
or	O
strongly	O
consistent	O
respectively	O
clearly	O
from	O
theorem	O
one	O
can	O
arrive	O
at	O
a	O
conclusion	O
analogous	O
to	O
corollary	O
when	O
the	O
probabilities	O
rjox	O
py	O
x	O
and	O
rjlx	O
py	O
iix	O
x	O
are	O
estimated	O
from	O
data	O
separately	O
by	O
some	O
rjon	O
and	O
rjln	O
respectively	O
usually	O
a	O
key	O
part	O
of	O
proving	O
consistency	B
of	O
classification	O
rules	O
is	O
writing	O
the	O
rules	O
in	O
one	O
of	O
the	O
plug-in	O
forms	O
and	O
showing	O
of	O
the	O
approximating	O
functions	O
to	O
the	O
a	O
posteriori	O
probabilities	O
here	O
we	O
have	O
some	O
freedom	O
as	O
for	O
any	O
positive	O
function	O
in	O
we	O
have	O
for	O
example	O
g	O
n	O
if	O
rjlnx	O
rjonx	O
otherwise	O
if	O
llnx	O
ronx	O
tnx	O
tnx	O
otherwise	O
consistency	B
partitioning	O
rules	O
many	O
important	O
classification	O
rules	O
partition	B
nd	O
into	O
disjoint	O
cells	O
ai	O
a	O
and	O
classify	O
in	O
each	O
cell	O
according	O
to	O
the	O
majority	B
vote	I
among	O
the	O
labels	O
of	O
the	O
xis	O
falling	O
in	O
the	O
same	O
cell	O
more	O
precisely	O
gn	O
otherwise	O
if	O
iylixeax	O
iyoixeax	O
where	O
ax	O
denotes	O
the	O
cell	O
containing	O
x	O
the	O
decision	O
is	O
zero	O
if	O
the	O
number	O
of	O
ones	O
does	O
not	O
exceed	O
the	O
number	O
of	O
zeros	O
in	O
the	O
cell	O
where	O
x	O
falls	O
and	O
vice	O
versa	O
the	O
partitions	O
we	O
consider	O
in	O
this	O
section	O
may	O
change	O
with	O
n	O
and	O
they	O
may	O
also	O
depend	O
on	O
the	O
points	O
xi	O
x	O
n	O
but	O
we	O
assume	O
that	O
the	O
labels	O
do	O
not	O
playa	O
role	O
in	O
constructing	O
the	O
partition	B
the	O
next	O
theorem	O
is	O
a	O
general	O
consistency	B
result	O
for	O
such	O
partitioning	O
rules	O
it	O
requires	O
two	O
properties	O
of	O
the	O
partition	B
first	O
cells	O
should	O
be	O
small	O
enough	O
so	O
that	O
local	O
changes	O
of	O
the	O
distribution	O
can	O
be	O
detected	O
on	O
the	O
other	O
hand	O
cells	O
should	O
be	O
large	O
enough	O
to	O
contain	O
a	O
large	O
number	O
of	O
points	O
so	O
that	O
averaging	O
among	O
the	O
labels	O
is	O
effective	O
diama	O
denotes	O
the	O
diameter	O
of	O
a	O
set	O
a	O
that	O
is	O
diama	O
sup	O
iix	O
yii	O
xyea	O
let	O
nx	O
nlnax	O
l	O
ixeax	O
n	O
denote	O
the	O
number	O
of	O
xis	O
falling	O
in	O
the	O
same	O
cell	O
as	O
x	O
the	O
conditions	O
of	O
the	O
theorem	O
below	O
require	O
that	O
a	O
random	O
cell-selected	O
according	O
to	O
the	O
distribution	O
of	O
x	O
a	O
small	O
diameter	O
and	O
contains	O
many	O
points	O
with	O
large	O
probability	O
theorem	O
consider	O
a	O
partitioning	O
classification	O
rule	B
as	O
defined	O
above	O
then	O
el	O
n	O
l	O
if	O
diamax	O
in	O
probability	O
n	O
in	O
probability	O
proof	O
define	O
py	O
llx	O
x	O
from	O
corollary	O
we	O
recall	O
that	O
we	O
need	O
only	O
show	O
where	O
lnx	O
exeax	O
yi	O
introduce	O
ijx	O
e	O
ax	O
by	O
the	O
triangle	O
inequality	B
elrynx	O
elrynx	O
elijx	O
by	O
conditioning	O
on	O
the	O
random	O
variable	B
nx	O
it	O
is	O
easy	O
to	O
see	O
that	O
nxrynx	O
is	O
distributed	O
as	O
bnx	O
ijx	O
a	O
binomial	B
random	O
variable	B
with	O
parameters	O
nx	O
and	O
ijx	O
thus	O
the	O
histogram	B
rule	B
ijxiix	O
ixl	O
eax	O
ixileax	O
i	O
e	O
i	O
bnx	O
ijx	O
nx	O
inxo	O
i	O
inx	O
o	O
x	O
ix	O
ixileax	O
ijxl	O
ijx	O
nx	O
i	O
inx	O
o	O
x	O
ixeax	O
ixileax	O
e	O
inxo	O
by	O
the	O
cauchy-schwarz	B
inequality	B
taking	O
expectations	O
we	O
see	O
that	O
e	O
oi	O
pnx	O
o	O
nx	O
ipnx	O
k	O
pnx	O
o	O
for	O
any	O
k	O
and	O
this	O
can	O
be	O
made	O
small	O
first	O
by	O
choosing	O
k	O
large	O
enough	O
and	O
then	O
by	O
using	O
condition	O
for	O
e	O
find	O
a	O
uniformly	O
continuous	O
i-valued	O
function	O
on	O
a	O
bounded	O
set	O
c	O
and	O
vanishing	O
off	O
c	O
so	O
that	O
e	O
e	O
next	O
we	O
employ	O
the	O
triangle	O
inequality	B
e	O
e	O
e	O
i	O
i	O
i	O
i	O
i	O
i	O
where	O
ijex	O
e	O
ax	O
clearly	O
i	O
i	O
i	O
e	O
by	O
choice	O
of	O
since	O
is	O
uniformly	O
continuous	O
we	O
can	O
find	O
a	O
such	O
that	O
i	O
i	O
e	O
pdiamax	O
therefore	O
i	O
i	O
for	O
n	O
large	O
enough	O
by	O
condition	O
finally	O
i	O
taken	O
together	O
these	O
steps	O
prove	O
the	O
theorem	O
i	O
i	O
i	O
e	O
the	O
histogram	B
rule	B
in	O
this	O
section	O
we	O
describe	O
the	O
cubic	B
histogram	B
rule	B
and	O
show	O
its	O
universal	O
sistency	O
by	O
checking	O
the	O
conditions	O
of	O
theorem	O
the	O
rule	B
partitions	O
rd	O
into	O
consistency	B
cubes	O
of	O
the	O
same	O
size	O
and	O
makes	O
a	O
decision	O
according	O
to	O
the	O
majority	B
vote	I
among	O
the	O
ys	O
such	O
that	O
the	O
corresponding	O
xi	O
falls	O
in	O
the	O
same	O
cube	O
as	O
x	O
formally	O
let	O
p	O
n	O
a	O
beapartitionofrd	O
into	O
cubes	O
ofsizeh	O
n	O
that	O
is	O
into	O
sets	O
of	O
the	O
type	O
n	O
where	O
the	O
ks	O
are	O
integers	O
for	O
every	O
x	O
e	O
rd	O
let	O
an	O
ani	O
if	O
x	O
e	O
ani	O
the	O
histogram	B
rule	B
is	O
defined	O
by	O
gn	O
x	O
if	O
iyilixieanx	O
iyioixieanx	O
otherwise	O
figure	O
a	O
cubic	B
histogram	B
rule	B
the	O
decision	O
is	O
in	O
the	O
shaded	O
area	O
oe	O
e	O
o	O
consistency	B
of	O
the	O
histogram	B
rule	B
was	O
established	O
under	O
some	O
additional	O
ditions	O
by	O
glick	O
universal	B
consistency	B
follows	O
from	O
the	O
results	O
of	O
gordon	O
and	O
olshen	O
a	O
direct	O
proof	O
of	O
strong	B
universal	B
consistency	B
is	O
given	O
in	O
chapter	O
the	O
next	O
theorem	O
establishes	O
universal	B
consistency	B
of	O
certain	O
cubic	B
histogram	O
rules	O
theorem	O
if	O
hn	O
rule	B
is	O
universally	O
consistent	O
and	O
nh	O
as	O
n	O
then	O
the	O
cubic	B
histogram	O
proof	O
we	O
check	O
the	O
two	O
simple	O
conditions	O
of	O
theorem	O
clearly	O
the	O
diameter	O
of	O
each	O
cell	O
is	O
therefore	O
condition	O
follows	O
trivially	O
to	O
show	O
condition	O
we	O
need	O
to	O
prove	O
that	O
for	O
any	O
m	O
pnx	O
m	O
o	O
let	O
s	O
be	O
an	O
arbitrary	O
ball	O
centered	O
at	O
the	O
origin	O
then	O
the	O
number	O
of	O
cells	O
intersecting	O
s	O
is	O
not	O
more	O
than	O
ci	O
hd	O
for	O
some	O
positive	O
constants	O
ci	O
then	O
pnx	O
m	O
l	O
px	O
e	O
a	O
nj	O
nx	O
m	O
px	O
esc	O
ja	O
nj	O
ja	O
nj	O
fj	O
a	O
nj	O
ja	O
nj	O
fj	O
a	O
nj	O
stones	O
theorem	O
jalsi	O
fla	O
nj	O
chebyshevs	O
inequality	B
ja	O
nj	O
nsi	O
fla	O
nj	O
cl	O
hd	O
c	O
because	O
nh	O
d	O
since	O
s	O
is	O
arbitrary	O
the	O
proof	O
of	O
the	O
theorem	O
is	O
complete	O
stones	O
theorem	O
a	O
general	O
theorem	O
by	O
stone	O
allows	O
us	O
to	O
deduce	O
universal	B
consistency	B
of	O
several	O
classification	O
rules	O
consider	O
a	O
rule	B
based	O
on	O
an	O
estimate	O
of	O
the	O
a	B
posteriori	I
probability	I
of	O
the	O
form	O
n	O
l	O
iyil	O
wnix	O
lyi	O
wnix	O
n	O
where	O
the	O
weights	O
wnix	O
wnix	O
xl	O
xn	O
are	O
nonnegative	O
and	O
sum	O
to	O
one	O
il	O
il	O
il	O
the	O
classification	O
rule	B
is	O
defined	O
as	O
gn	O
x	O
if	O
iyil	O
wnix	O
iyio	O
wnix	O
otherwise	O
if	O
yi	O
wnix	O
otherwise	O
consistency	B
is	O
a	O
weighted	B
average	I
estimator	I
of	O
it	O
is	O
intuitively	O
clear	O
that	O
pairs	O
yi	O
such	O
that	O
xi	O
is	O
close	O
to	O
x	O
should	O
provide	O
more	O
information	O
about	O
than	O
those	O
far	O
from	O
x	O
thus	O
the	O
weights	O
are	O
typically	O
much	O
larger	O
in	O
the	O
neighborhood	O
of	O
x	O
so	O
is	O
roughly	O
a	O
relative	O
frequency	O
of	O
the	O
xis	O
that	O
have	O
label	O
among	O
points	O
in	O
the	O
neighborhood	O
of	O
x	O
thus	O
might	O
be	O
viewed	O
as	O
a	O
local	B
average	I
estimator	I
and	O
gn	O
a	O
local	O
majority	B
vote	I
examples	O
of	O
such	O
rules	O
include	O
the	O
histogram	O
kernel	B
and	O
nearest	B
neighbor	I
rules	O
these	O
rules	O
will	O
be	O
studied	O
in	O
depth	O
later	O
theorem	O
satisfy	O
the	O
following	O
three	O
conditions	O
assume	O
that	O
for	O
any	O
distribution	O
of	O
x	O
the	O
weights	O
there	O
is	O
a	O
constant	O
c	O
such	O
thatfor	O
every	O
nonnegative	O
measurable	O
function	O
f	O
satisfying	O
efx	O
e	O
wnixfx	O
s	O
cefx	O
for	O
all	O
a	O
lim	O
e	O
i	O
mx	O
wnix	O
n---oo	O
then	O
gn	O
is	O
universally	O
consistent	O
remark	O
condition	O
requires	O
that	O
the	O
overall	O
weight	O
of	O
xs	O
outside	O
of	O
any	O
ball	O
of	O
a	O
fixed	O
radius	O
centered	O
at	O
x	O
must	O
go	O
to	O
zero	O
in	O
other	O
words	O
only	O
points	O
in	O
a	O
shrinking	O
neighborhood	O
of	O
x	O
should	O
be	O
taken	O
into	O
account	O
in	O
the	O
averaging	O
condition	O
requires	O
that	O
no	O
single	O
xi	O
has	O
too	O
large	O
a	O
contribution	O
to	O
the	O
estimate	O
hence	O
the	O
number	O
of	O
points	O
encountered	O
in	O
the	O
averaging	O
must	O
tend	O
to	O
infinity	O
condition	O
is	O
technical	O
proof	O
by	O
corollary	O
it	O
suffices	O
to	O
show	O
that	O
for	O
every	O
distribution	O
of	O
y	O
introduce	O
the	O
notation	O
lim	O
e	O
n---oo	O
n	O
rnx	O
l	O
il	O
stones	O
theorem	O
then	O
by	O
the	O
simple	O
inequality	B
we	O
have	O
e	O
rynx	O
e	O
ilnx	O
rynx	O
tfnx	O
e	O
rynx	O
therefore	O
it	O
is	O
enough	O
to	O
show	O
that	O
both	O
terms	O
on	O
the	O
right-hand	O
side	O
tend	O
to	O
zero	O
since	O
the	O
wnis	O
are	O
nonnegative	O
and	O
sum	O
to	O
one	O
by	O
jensens	O
inequality	B
the	O
first	O
term	O
is	O
e	O
wnixx	O
r	O
e	O
t	O
wnixx	O
if	O
the	O
function	O
ry	O
is	O
continuous	O
with	O
bounded	O
support	B
then	O
it	O
is	O
uniformly	O
continuous	O
as	O
well	O
for	O
every	O
e	O
there	O
is	O
an	O
a	O
such	O
that	O
for	O
ilxi	O
x	O
ii	O
a	O
iryxd	O
e	O
recall	O
here	O
that	O
ilxli	O
denotes	O
the	O
norm	O
ofa	O
vector	O
x	O
e	O
rd	O
thus	O
since	O
iryxd	O
ryx	O
e	O
t	O
wnixx	O
ryxi	O
e	O
wnxjiix-xlia	O
e	O
wnixe	O
e	O
by	O
since	O
the	O
set	O
of	O
continuous	O
functions	O
with	O
bounded	O
support	B
is	O
dense	O
in	O
for	O
every	O
e	O
we	O
can	O
choose	O
ry	O
such	O
that	O
e	O
ryx	O
e	O
by	O
this	O
choice	O
using	O
the	O
inequality	B
b	O
c	O
follows	O
from	O
the	O
cauchy-schwarz	B
inequality	B
e	O
tfnx	O
e	O
t	O
wnixryx	O
wnix	O
ryxi	O
ryx	O
wnixx	O
ryxi	O
consistency	B
where	O
we	O
used	O
therefore	O
lim	O
sup	O
e	O
c	O
n-oo	O
to	O
handle	O
the	O
second	O
term	O
of	O
the	O
right	O
side	O
of	O
observe	O
that	O
by	O
independence	O
therefore	O
e	O
e	O
wnxryxi	O
yir	O
n	O
l	O
le	O
yi	O
j	O
yj	O
n	O
il	O
jl	O
n	O
il	O
le	O
e	O
w	O
ex	O
e	O
wnix	O
wnjx	O
e	O
wnx	O
by	O
and	O
the	O
theorem	O
is	O
proved	O
the	O
k-nearest	O
neighbor	O
rule	B
in	O
chapter	O
we	O
discussed	O
asymptotic	O
properties	O
of	O
the	O
k-nearest	O
neighbor	O
rule	B
when	O
k	O
remains	O
fixed	O
as	O
the	O
sample	O
size	O
n	O
grows	O
in	O
such	O
cases	O
the	O
expected	O
probability	O
of	O
error	O
converges	O
to	O
a	O
number	O
between	O
l	O
and	O
in	O
this	O
section	O
we	O
show	O
that	O
if	O
k	O
is	O
allowed	O
to	O
grow	O
with	O
n	O
such	O
that	O
kin	O
the	O
rule	B
is	O
weakly	O
universally	O
consistent	O
the	O
proof	O
is	O
a	O
very	O
simple	O
application	O
of	O
stones	O
theorem	O
this	O
result	O
appearing	O
in	O
stones	O
paper	O
was	O
the	O
first	O
universal	B
consistency	B
result	O
for	O
any	O
rule	B
strong	B
consistency	B
and	O
many	O
other	O
different	O
aspects	O
of	O
the	O
k-nn	B
rule	B
are	O
studied	O
in	O
chapters	O
and	O
recall	O
the	O
definition	B
of	I
the	O
k-nearest	O
neighbor	O
rule	B
first	O
the	O
data	O
are	O
ordered	B
according	O
to	O
increasing	O
euclidean	B
distances	O
of	O
the	O
x	O
j	O
s	O
to	O
x	O
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	B
that	O
is	O
xix	O
is	O
the	O
i-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
the	O
points	O
xl	O
x	O
n	O
distance	O
ties	O
are	O
broken	O
by	O
comparing	O
indices	O
that	O
is	O
in	O
case	O
of	O
ii	O
xi	O
x	O
ii	O
j	O
x	O
ii	O
xi	O
is	O
considered	O
to	O
be	O
to	O
x	O
if	O
i	O
j	O
the	O
k-nn	B
classification	O
rule	B
is	O
defined	O
as	O
gn	O
if	O
ll	O
iyixl	O
ll	O
iyixo	O
otherwise	O
in	O
other	O
words	O
gn	O
is	O
a	O
majority	B
vote	I
among	O
the	O
labels	O
of	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
theorem	O
ifk	O
and	O
kn	O
thenforall	O
distributions	O
eln-	O
l	O
proof	O
we	O
proceed	O
by	O
checking	O
the	O
conditions	O
of	O
stones	O
weak	B
convergence	O
rem	O
the	O
weight	O
wnix	O
in	O
theorem	O
equals	O
k	O
iff	O
xi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
and	O
equals	O
otherwise	O
condition	O
is	O
obvious	O
since	O
k	O
for	O
condition	O
observe	O
that	O
holds	O
whenever	O
p	O
xii	O
e	O
where	O
xkx	O
denotes	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
x	O
n	O
but	O
we	O
know	O
from	O
lemma	O
that	O
this	O
is	O
true	O
for	O
all	O
e	O
whenever	O
kn	O
finally	O
we	O
consider	O
condition	O
we	O
have	O
to	O
show	O
that	O
for	O
any	O
nonnegative	O
measurable	O
function	O
f	O
with	O
efx	O
e	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
xfxd	O
e	O
for	O
some	O
constant	O
c	O
but	O
we	O
have	O
shown	O
in	O
lemma	O
that	O
this	O
inequality	B
always	O
holds	O
with	O
c	O
yd	O
thus	O
condition	O
is	O
verified	O
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	B
once	O
again	O
assume	O
that	O
our	O
decision	O
is	O
based	O
on	O
some	O
estimate	O
of	O
the	O
a	B
posteriori	I
probability	I
function	O
that	O
is	O
otherwise	O
gn	O
consistency	B
the	O
bounds	O
of	O
theorems	O
and	O
corollary	O
point	O
out	O
that	O
if	O
is	O
a	O
consistent	O
estimate	O
of	O
then	O
the	O
resulting	O
rule	B
is	O
also	O
consistent	O
for	O
example	O
writing	O
ln	O
pgnx	O
i	O
yidn	O
we	O
have	O
that	O
is	O
l	O
estimation	B
of	I
the	O
regression	B
function	I
leads	O
to	O
consistent	O
classification	O
and	O
in	O
fact	O
this	O
is	O
the	O
main	O
tool	O
used	O
in	O
the	O
proof	O
of	O
theorem	O
while	O
the	O
said	O
bounds	O
are	O
useful	O
for	O
proving	O
consistency	B
they	O
are	O
almost	O
useless	O
when	O
it	O
comes	O
to	O
studying	O
rates	O
of	O
convergence	O
as	O
theorem	O
below	O
shows	O
for	O
consistent	O
rules	O
rates	O
of	O
convergence	O
of	O
pgn	O
i	O
y	O
to	O
l	O
are	O
always	O
orders	O
of	O
magnitude	O
better	O
than	O
rates	O
of	O
convergence	O
of	O
je	O
to	O
zero	O
o	O
x	O
figure	O
the	O
difference	O
between	O
the	O
error	O
probabilities	O
grows	O
roughly	O
in	O
proportion	O
to	O
the	O
shaded	O
area	O
elsewhere	O
does	O
not	O
need	O
to	O
be	O
close	O
pattern	O
recognition	O
is	O
thus	O
easier	O
than	O
regression	B
function	I
estimation	B
this	O
will	O
be	O
a	O
recurring	O
theme-to	O
achieve	O
acceptable	O
results	O
in	O
pattern	O
recognition	O
we	O
can	O
do	O
more	O
with	O
smaller	O
sample	O
sizes	O
than	O
in	O
regression	B
function	I
estimation	B
this	O
is	O
really	O
just	O
a	O
consequence	O
of	O
the	O
fact	O
that	O
less	O
is	O
required	O
in	O
pattern	O
recognition	O
it	O
also	O
corroborates	O
our	O
belief	O
that	O
pattern	O
recognition	O
is	O
dramatically	O
different	O
from	O
regression	B
function	I
estimation	B
and	O
that	O
it	O
deserves	O
separate	O
treatment	O
in	O
the	O
statistical	O
community	O
theorem	O
let	O
be	O
a	O
weakly	O
consistent	O
regression	O
estimate	O
that	O
is	O
define	O
lim	O
e	O
n-oo	O
o	O
gnx	O
otherwise	O
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	B
then	O
el	O
lim	O
n-oo	O
n	O
flx	O
that	O
is	O
eln	O
l	O
converges	O
to	O
zero	O
faster	O
than	O
the	O
l	O
of	O
the	O
regression	O
estimate	O
proof	O
we	O
start	O
with	O
the	O
equality	O
of	O
theorem	O
eln	O
l	O
gx	O
fix	O
e	O
o	O
we	O
may	O
bound	O
the	O
last	O
factor	O
by	O
e	O
i	O
gxd	O
e	O
flnxiigx	O
gx	O
e	O
e	O
ignx	O
gx	O
flnxiigx	O
flx	O
x	O
jplflx	O
e	O
flex	O
jpgnx	O
gx	O
iflx	O
e	O
the	O
cauchy-schwarz	B
inequality	B
since	O
gnx	O
gx	O
and	O
iflx	O
e	O
imply	O
that	O
irjnx	O
consistency	B
of	O
the	O
regression	O
estimate	O
implies	O
that	O
for	O
any	O
fixed	O
e	O
e	O
lim	O
pgnx	O
gx	O
iflx	O
e	O
o	O
n-oo	O
on	O
the	O
other	O
hand	O
plflx	O
e	O
flex	O
as	O
e	O
which	O
completes	O
the	O
proof	O
the	O
actual	O
value	O
of	O
the	O
ratio	O
eln	O
l	O
pn	O
flx	O
cannot	O
be	O
universally	O
bounded	O
in	O
fact	O
pn	O
may	O
tend	O
to	O
zero	O
arbitrarily	O
slowly	O
problem	O
on	O
the	O
other	O
hand	O
pn	O
may	O
tend	O
to	O
zero	O
extremely	O
quickly	O
in	O
problems	O
and	O
and	O
in	O
the	O
theorem	O
below	O
upper	O
bounds	O
on	O
pn	O
are	O
given	O
that	O
consistency	B
may	O
be	O
used	O
in	O
deducing	O
rate-of-convergence	O
results	O
theorem	O
in	O
particular	O
states	O
that	O
eln	O
l	O
tends	O
to	O
zero	O
as	O
fast	O
as	O
the	O
square	O
of	O
the	O
error	O
of	O
the	O
regression	O
estimate	O
i	O
e	O
e	O
whenever	O
l	O
o	O
just	O
how	O
slowly	O
pn	O
tends	O
to	O
zero	O
depends	O
upon	O
two	O
things	O
basically	O
the	O
rate	B
of	I
convergence	I
of	O
to	O
and	O
the	O
behavior	O
of	O
e	O
as	O
a	O
function	O
of	O
e	O
when	O
e	O
the	O
behavior	O
of	O
at	O
those	O
xs	O
where	O
theorem	O
assume	O
that	O
l	O
and	O
consider	O
the	O
decision	O
gn	O
otherwise	O
then	O
pgnx	O
y	O
e	O
proof	O
by	O
theorem	O
pgnxj	O
y	O
gx	O
y	O
by	O
the	O
assumption	O
l	O
hpgnx	O
y	O
the	O
cauchy-schwarz	B
inequality	B
dividing	O
both	O
sides	O
by	O
y	O
yields	O
the	O
result	O
the	O
results	O
above	O
show	O
that	O
the	O
bounds	O
of	O
theorems	O
and	O
corollary	O
may	O
be	O
arbitrarily	O
loose	O
and	O
the	O
error	O
probability	O
converges	O
to	O
l	O
faster	O
than	O
the	O
l	O
of	O
the	O
regression	O
estimate	O
converges	O
to	O
zero	O
in	O
some	O
cases	O
consistency	B
may	O
even	O
occur	O
without	O
convergence	O
of	O
to	O
zero	O
consider	O
for	O
example	O
a	O
strictly	B
separable	I
distribution	I
that	O
is	O
a	O
distribution	O
such	O
that	O
there	O
exist	O
two	O
sets	O
a	O
bend	O
with	O
inf	O
xeayeb	O
y	O
ii	O
for	O
some	O
and	O
having	O
the	O
property	O
that	O
px	O
e	O
aiy	O
i	O
e	O
biy	O
in	O
such	O
cases	O
there	O
is	O
a	O
version	O
of	O
that	O
has	O
on	O
a	O
and	O
on	O
b	O
we	O
say	O
version	O
because	O
is	O
not	O
defined	O
on	O
sets	O
of	O
measure	O
zero	O
for	O
such	O
strictly	B
separable	I
distributions	O
l	O
o	O
let	O
ij	O
be	O
e	O
on	O
band	O
e	O
on	O
a	O
then	O
with	O
gx	O
if	O
ijx	O
otherwise	O
if	O
x	O
e	O
b	O
if	O
x	O
e	O
a	O
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	B
we	O
have	O
pgx	O
y	O
l	O
o	O
yet	O
is	O
arbitrarily	O
close	O
to	O
one	O
in	O
a	O
more	O
realistic	O
example	O
we	O
consider	O
the	O
kernel	B
rule	B
chapter	O
gn	O
ifrynx	O
otherwise	O
in	O
which	O
where	O
k	O
is	O
the	O
standard	B
normal	B
density	O
in	O
nd	O
ku	O
i	O
e-	O
iiu	O
assume	O
that	O
a	O
and	O
b	O
consist	O
of	O
one	O
point	O
each	O
at	O
distance	O
from	O
each	O
other-that	O
is	O
the	O
distribution	O
of	O
x	O
is	O
concentrated	O
on	O
two	O
points	O
if	O
py	O
o	O
py	O
i	O
we	O
see	O
that	O
lim	O
l	O
kx	O
xi	O
n	O
n	O
ko	O
ko	O
with	O
probability	O
one	O
at	O
x	O
e	O
a	O
u	O
b	O
by	O
the	O
law	O
of	O
large	O
numbers	O
also	O
if	O
x	O
e	O
a	O
if	O
x	O
e	O
b	O
with	O
probability	O
one	O
thus	O
lim	O
rynx	O
if	O
x	O
e	O
a	O
if	O
x	O
e	O
b	O
hence	O
as	O
ryx	O
ion	O
a	O
and	O
ryx	O
on	O
b	O
n--oo	O
ko	O
with	O
probability	O
one	O
lim	O
n--oo	O
ko	O
yet	O
l	O
and	O
pgnx	O
y	O
o	O
in	O
fact	O
if	O
dn	O
denotes	O
the	O
training	O
data	O
lim	O
pgnx	O
yidn	O
l	O
with	O
probability	O
one	O
n--oo	O
consistency	B
and	O
iljnx	O
dn	O
ko	O
with	O
probability	O
one	O
i	O
this	O
shows	O
very	O
strongly	O
that	O
for	O
any	O
for	O
many	O
practical	O
classification	O
rules	O
we	O
do	O
not	O
need	O
convergence	O
of	O
ljn	O
to	O
lj	O
at	O
all	O
as	O
all	O
the	O
consistency	B
proofs	O
in	O
chapters	O
through	O
rely	O
on	O
the	O
convergence	O
of	O
ljn	O
to	O
lj	O
we	O
will	O
create	O
unnecessary	O
conditions	O
for	O
some	O
distributions	O
although	O
it	O
will	O
always	O
be	O
possible	O
to	O
find	O
distributions	O
of	O
y	O
for	O
which	O
the	O
conditions	O
are	O
needed-in	O
the	O
latter	O
sense	O
the	O
conditions	O
of	O
these	O
universal	B
consistency	B
results	O
are	O
not	O
improvable	O
smart	O
rules	O
a	O
rule	B
is	O
a	O
sequence	O
of	O
mappings	O
gn	O
rd	O
x	O
x	O
to	O
lf	O
to	O
i	O
most	O
rules	O
are	O
expected	O
to	O
perform	O
better	O
when	O
n	O
increases	O
so	O
we	O
say	O
that	O
a	O
rule	B
is	O
smart	O
if	O
for	O
all	O
distributions	O
of	O
y	O
elgn	O
is	O
nonincreasing	O
where	O
some	O
dumb	O
rules	O
are	O
smart	O
such	O
as	O
the	O
rule	B
that	O
for	O
each	O
n	O
takes	O
a	O
majority	O
over	O
all	O
yi	O
ignoring	O
the	O
xis	O
this	O
follows	O
from	O
the	O
fact	O
that	O
p	O
t	O
y	O
or	O
y	O
is	O
monotone	O
in	O
n	O
this	O
is	O
a	O
property	O
of	O
the	O
binomial	B
distribution	I
problem	O
a	O
histogram	B
rule	B
with	O
a	O
fixed	O
partition	B
is	O
smart	O
the	O
neighbor	O
rule	B
is	O
not	O
smart	O
to	O
see	O
this	O
let	O
y	O
be	O
and	O
with	O
probabilities	O
p	O
and	O
p	O
respectively	O
where	O
z	O
is	O
uniform	O
on	O
verify	O
that	O
for	O
n	O
eln	O
p	O
while	O
for	O
n	O
eizi	O
p	O
p	O
which	O
is	O
larger	O
than	O
p	O
whenever	O
p	O
e	O
this	O
shows	O
that	O
in	O
all	O
these	O
cases	O
it	O
is	O
better	O
to	O
have	O
n	O
than	O
n	O
similarly	O
the	O
standard	B
kernel	B
rule-discussed	O
in	O
chapter	O
fixed	O
h	O
is	O
not	O
smart	O
problems	O
the	O
error	O
probabilities	O
of	O
the	O
above	O
examples	O
of	O
smart	O
rules	O
do	O
not	O
change	O
dramatically	O
with	O
n	O
however	O
change	O
is	O
necessary	O
to	O
guarantee	O
bayes	O
risk	O
tency	O
at	O
the	O
places	O
of	O
change-for	O
example	O
when	O
hn	O
jumps	O
to	O
a	O
new	O
value	O
in	O
the	O
histogram	O
rule-the	O
monotonicity	O
may	O
be	O
lost	O
this	O
leads	O
to	O
the	O
conjecture	O
that	O
no	O
universally	O
consistent	O
rule	B
can	O
be	O
smart	O
problems	O
and	O
exercises	O
problems	O
and	O
exercises	O
problem	O
let	O
the	O
i	O
i	O
d	O
random	O
variables	O
xl	O
xn	O
be	O
distributed	O
on	O
rd	O
according	O
to	O
the	O
density	O
f	O
estimate	O
f	O
by	O
fn	O
a	O
function	O
of	O
x	O
and	O
xl	O
x	O
n	O
and	O
assume	O
that	O
f	O
i	O
fn	O
f	O
idx	O
in	O
probability	O
with	O
probability	O
one	O
then	O
show	O
that	O
there	O
exists	O
a	O
consistent	O
strongly	O
consistent	O
classification	O
rule	B
whenever	O
the	O
conditional	O
densities	O
fa	O
and	O
fl	O
exist	O
problem	O
histogram	B
density	B
estimation	B
let	O
xl	O
xn	O
be	O
i	O
i	O
d	O
random	O
variables	O
in	O
rd	O
with	O
density	O
f	O
let	O
p	O
n	O
be	O
a	O
partition	B
of	O
rd	O
into	O
cubes	O
of	O
size	O
hn	O
and	O
define	O
the	O
histogram	O
density	O
estimate	O
by	O
where	O
an	O
is	O
the	O
set	O
in	O
p	O
n	O
that	O
contains	O
x	O
prove	O
that	O
the	O
estimate	O
is	O
universally	O
consistent	O
in	O
ll	O
if	O
hn	O
and	O
nh	O
as	O
n	O
that	O
is	O
for	O
any	O
f	O
the	O
ll	O
error	O
of	O
the	O
estimate	O
f	O
ifnx	O
fxldx	O
converges	O
to	O
zero	O
in	O
probability	O
or	O
equivalently	O
e	O
ifnx	O
fxldx	O
o	O
hint	O
the	O
following	O
suggestions	O
may	O
be	O
helpful	O
fl	O
e	O
ifn	O
efnl	O
f	O
iefn	O
fl	O
e	O
ifn	O
e	O
itl	O
efn	O
i	O
lj	O
first	O
show	O
f	O
iefn	O
arbitrary	O
densities	O
f-inanjl	O
fl	O
for	O
uniformly	O
continuous	O
f	O
and	O
then	O
extend	O
it	O
to	O
problem	O
let	O
x	O
be	O
uniformly	O
distributed	O
on	O
with	O
probability	O
and	O
let	O
x	O
be	O
atomic	O
on	O
the	O
rationals	O
with	O
probability	O
if	O
the	O
rationals	O
are	O
enumerated	O
rl	O
then	O
px	O
rd	O
let	O
y	O
if	O
x	O
is	O
rational	O
and	O
y	O
if	O
x	O
is	O
irrational	O
give	O
a	O
direct	O
proof	O
of	O
consistency	B
of	O
the	O
i-nearest	O
neighbor	O
rule	B
hint	O
given	O
y	O
the	O
conditional	O
distribution	O
of	O
x	O
is	O
discrete	O
thus	O
for	O
every	O
e	O
there	O
is	O
an	O
integer	O
k	O
such	O
that	O
given	O
y	O
x	O
equals	O
one	O
of	O
k	O
rationals	O
with	O
probability	O
at	O
least	O
i-e	O
now	O
if	O
n	O
is	O
large	O
enough	O
every	O
point	O
in	O
this	O
set	O
captures	O
data	O
points	O
with	O
label	O
i	O
with	O
large	O
probability	O
also	O
for	O
large	O
n	O
the	O
space	O
between	O
these	O
points	O
is	O
filled	O
with	O
data	O
points	O
labeled	O
with	O
zeros	O
problem	O
prove	O
the	O
consistency	B
of	O
the	O
cubic	B
histogram	B
rule	B
by	O
checking	O
the	O
conditions	O
of	O
stones	O
theorem	O
hint	O
to	O
check	O
first	O
bound	O
wnix	O
by	O
since	O
n	O
jl	O
ixieanx	O
l	O
ixjeanx	O
lin	O
e	O
efx	O
it	O
suffices	O
to	O
show	O
that	O
there	O
is	O
a	O
constant	O
c	O
such	O
that	O
for	O
any	O
nonnegative	O
function	O
f	O
with	O
efx	O
consistency	B
in	O
doing	O
so	O
you	O
may	O
need	O
to	O
use	O
lemma	O
to	O
prove	O
that	O
condition	O
holds	O
write	O
and	O
use	O
lemma	O
problem	O
let	O
be	O
a	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
give	O
an	O
ple	O
of	O
an	O
a	B
posteriori	I
probability	I
function	O
and	O
a	O
sequence	O
of	O
functions	O
approximating	O
such	O
that	O
pgnx	O
y	O
l	O
f	O
in	O
anje	O
where	O
gnx	O
if	O
otherwise	O
thus	O
the	O
rate	B
of	I
convergence	I
in	O
theorem	O
may	O
be	O
arbitrarily	O
slow	O
hint	O
define	O
hex	O
where	O
hex	O
is	O
a	O
very	O
slowly	O
increasing	O
nonnegative	O
function	O
problem	O
let	O
and	O
assume	O
that	O
for	O
all	O
x	O
consider	O
the	O
decision	O
if	O
otherwise	O
prove	O
that	O
pgnx	O
y	O
l	O
this	O
shows	O
that	O
the	O
rate	B
of	I
convergence	I
implied	O
by	O
the	O
inequality	B
of	O
theorem	O
may	O
be	O
preserved	O
for	O
very	O
general	O
classes	O
of	O
distributions	O
problem	O
assume	O
that	O
l	O
and	O
consider	O
the	O
decision	O
show	O
that	O
for	O
all	O
p	O
if	O
otherwise	O
hint	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
but	O
use	O
holders	O
inequality	B
problem	O
theorem	O
cannot	O
be	O
generalized	B
to	O
the	O
ll	O
error	O
in	O
particular	O
show	O
by	O
example	O
that	O
it	O
is	O
not	O
always	O
true	O
that	O
el	O
lim	O
n---oo	O
e	O
n	O
when	O
e	O
as	O
for	O
some	O
regression	B
function	I
estimate	O
thus	O
the	O
inequality	B
eln	O
l	O
cannot	O
be	O
universally	O
improved	O
problems	O
and	O
exercises	O
problem	O
let	O
rd	O
and	O
define	O
gx	O
irlx	O
assume	O
that	O
the	O
random	O
that	O
as	O
n	O
prove	O
that	O
lgn	O
lg	O
for	O
all	O
distributions	O
of	O
variable	B
x	O
satisfies	O
that	O
let	O
be	O
a	O
sequence	O
of	O
functions	O
such	O
y	O
satisfying	O
the	O
condition	O
on	O
x	O
abovewhere	O
gnx	O
illnx	O
problem	O
a	O
lying	O
teacher	O
sometimes	O
the	O
training	O
labels	O
yj	O
yn	O
are	O
not	O
able	O
but	O
can	O
only	O
be	O
observed	O
through	O
a	O
noisy	O
binary	B
channel	B
still	O
we	O
want	O
to	O
decide	O
on	O
y	O
consider	O
the	O
following	O
model	O
assume	O
that	O
the	O
yi	O
s	O
in	O
the	O
training	O
data	O
are	O
replaced	O
by	O
the	O
i	O
i	O
d	O
binary-valued	O
random	O
variables	O
zi	O
whose	O
distribution	O
is	O
given	O
by	O
pzi	O
llyi	O
o	O
p	O
pzi	O
i	O
q	O
pzi	O
llyi	O
xi	O
x	O
pzi	O
oiyi	O
xi	O
x	O
consider	O
the	O
decision	O
gx	O
where	O
pzi	O
ixi	O
x	O
show	O
that	O
pgx	O
y	O
l	O
if	O
otherwise	O
q	O
i	O
q	O
use	O
problem	O
to	O
conclude	O
that	O
if	O
the	O
binary	B
channel	B
is	O
symmetric	O
p	O
q	O
and	O
p	O
then	O
l	O
i-consistent	O
estimation	B
leads	O
to	O
a	O
consistent	O
rule	B
in	O
spite	O
of	O
the	O
fact	O
that	O
the	O
labels	O
yi	O
were	O
not	O
available	O
in	O
the	O
training	O
sequence	O
problem	O
develop	O
a	O
discrimination	O
rule	B
which	O
has	O
the	O
property	O
lim	O
eln	O
p	O
e	O
n-oo	O
for	O
all	O
distributions	O
such	O
that	O
x	O
has	O
a	O
density	O
note	O
clearly	O
since	O
p	O
l	O
this	O
rule	B
is	O
not	O
universally	O
consistent	O
but	O
it	O
will	O
aid	O
you	O
in	O
the	O
matushita	B
error	I
problem	O
if	O
zn	O
is	O
binomial	B
p	O
and	O
z	O
is	O
bernoulli	O
independent	O
of	O
zn	O
then	O
show	O
that	O
pzn	O
z	O
o	O
pzn	O
z	O
is	O
nonincreasing	O
in	O
n	O
problem	O
let	O
gn	O
be	O
the	O
histogram	B
rule	B
based	O
on	O
a	O
fixed	O
partition	B
p	O
show	O
that	O
gn	O
is	O
smart	O
problem	O
show	O
that	O
the	O
kernel	B
rule	B
with	O
gaussian	B
kernel	B
and	O
h	O
d	O
is	O
not	O
smart	O
rules	O
are	O
discussed	O
in	O
chapter	O
hint	O
consider	O
n	O
and	O
n	O
only	O
problem	O
show	O
that	O
the	O
kernel	B
rule	B
on	O
r	O
with	O
kx	O
i-lljx	O
and	O
h	O
t	O
such	O
that	O
nh	O
is	O
not	O
smart	O
problem	O
conjecture	O
no	O
universally	O
consistent	O
rule	B
is	O
smart	O
rd	O
x	O
x	O
l	O
r	O
is	O
called	O
symmetric	O
if	O
gn	O
dn	O
problem	O
a	O
rule	B
gn	O
gn	O
d	O
for	O
every	O
x	O
and	O
every	O
training	O
sequence	O
dn	O
where	O
d	O
is	O
an	O
arbitrary	O
permutation	O
of	O
the	O
pairs	O
yi	O
in	O
dn	O
any	O
nonsymmetric	O
rule	B
gn	O
may	O
be	O
symmetrized	O
by	O
taking	O
a	O
jority	O
vote	O
at	O
every	O
x	O
e	O
rd	O
over	O
all	O
gnx	O
d	O
obtained	O
by	O
then	O
permutations	O
of	O
dn	O
it	O
may	O
intuitively	O
be	O
expected	O
that	O
symmetrized	O
rules	O
perform	O
better	O
prove	O
that	O
this	O
is	O
false	O
that	O
is	O
exhibit	O
a	O
distribution	O
and	O
a	O
nonsymmetric	O
classifier	B
gn	O
such	O
that	O
its	O
expected	O
probability	O
of	O
error	O
is	O
smaller	O
than	O
that	O
of	O
the	O
symmetrized	O
version	O
of	O
gn	O
hint	O
take	O
slow	O
rates	O
of	O
convergence	O
in	O
this	O
chapter	O
we	O
consider	O
the	O
general	O
pattern	O
recognition	O
problem	O
given	O
the	O
observation	O
x	O
and	O
the	O
training	O
data	O
dn	O
yi	O
yn	O
of	O
dent	O
identically	O
distributed	O
random	O
variable	B
pairs	O
we	O
estimate	O
the	O
label	O
y	O
by	O
the	O
decision	O
the	O
error	O
probability	O
is	O
obviously	O
the	O
average	O
error	O
probability	O
eln	O
py	O
gnx	O
is	O
completely	O
determined	O
by	O
the	O
distribution	O
of	O
the	O
pair	O
y	O
and	O
the	O
classifier	B
gn	O
we	O
have	O
seen	O
in	O
chapter	O
that	O
there	O
exist	O
classification	O
rules	O
such	O
as	O
the	O
cubic	B
histogram	B
rule	B
with	O
properly	O
chosen	O
cube	O
sizes	O
such	O
that	O
limn---	O
oo	O
eln	O
l	O
for	O
all	O
possible	O
distributions	O
the	O
next	O
question	O
is	O
whether	O
there	O
are	O
classification	O
rules	O
with	O
eln	O
tending	O
to	O
the	O
bayes	O
risk	O
at	O
a	O
specified	O
rate	O
for	O
all	O
distributions	O
disappointingly	O
such	O
rules	O
do	O
not	O
exist	O
finite	O
training	O
sequence	O
the	O
first	O
negative	O
result	O
shows	O
that	O
for	O
any	O
classification	O
rule	B
and	O
for	O
any	O
fixed	O
n	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
the	O
difference	O
between	O
the	O
error	O
probability	O
of	O
the	O
rule	B
and	O
l	O
is	O
larger	O
than	O
to	O
explain	O
this	O
note	O
that	O
for	O
fixed	O
n	O
we	O
can	O
find	O
a	O
sufficiently	O
complex	O
distribution	O
for	O
which	O
the	O
sample	O
size	O
n	O
is	O
hopelessly	O
small	O
slow	O
rates	O
of	O
convergence	O
theorem	O
let	O
e	O
be	O
an	O
arbitrarily	O
small	O
number	O
with	O
bayes	O
risk	O
l	O
such	O
that	O
for	O
any	O
integer	O
n	O
and	O
classification	O
rule	B
gn	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
eln	O
e	O
proof	O
first	O
we	O
construct	O
a	O
family	B
of	I
distributions	O
of	O
y	O
then	O
we	O
show	O
that	O
the	O
error	O
probability	O
of	O
any	O
classifier	B
is	O
large	O
for	O
at	O
least	O
one	O
member	O
of	O
the	O
family	O
for	O
every	O
member	O
of	O
the	O
family	O
x	O
is	O
uniformly	O
distributed	O
on	O
the	O
set	O
k	O
of	O
positive	O
integers	O
px	O
i	O
k	O
pi	O
otherwise	O
if	O
i	O
e	O
k	O
where	O
k	O
is	O
a	O
large	O
integer	O
specified	O
later	O
now	O
the	O
family	B
of	I
distributions	O
of	O
y	O
is	O
parameterized	O
by	O
a	O
number	O
b	O
e	O
that	O
is	O
every	O
b	O
determines	O
a	O
distribution	O
as	O
follows	O
let	O
b	O
e	O
have	O
binary	B
expansion	O
b	O
and	O
define	O
y	O
bx	O
as	O
the	O
label	O
y	O
is	O
a	O
function	O
of	O
x	O
there	O
exists	O
a	O
perfect	O
decision	O
and	O
thus	O
l	O
we	O
show	O
that	O
for	O
any	O
decision	O
rule	B
gn	O
there	O
is	O
a	O
b	O
such	O
that	O
if	O
y	O
bx	O
then	O
gn	O
has	O
very	O
poor	O
performance	O
denote	O
the	O
average	O
error	O
probability	O
corresponding	O
to	O
the	O
distribution	O
determined	O
by	O
b	O
by	O
rnb	O
eln	O
the	O
proof	O
of	O
the	O
existence	O
of	O
a	O
bad	O
distribution	O
is	O
based	O
on	O
the	O
so-called	O
abilistic	O
method	O
here	O
the	O
key	O
trick	O
is	O
the	O
randomization	O
of	O
b	O
define	O
a	O
random	O
variable	B
b	O
which	O
is	O
uniformly	O
distributed	O
in	O
and	O
independent	O
of	O
x	O
and	O
xl	O
x	O
n	O
then	O
we	O
may	O
compute	O
the	O
expected	O
value	O
of	O
the	O
random	O
variable	B
rnb	O
since	O
for	O
any	O
decision	O
rule	B
gn	O
sup	O
rnb	O
ernb	O
beoi	O
a	O
lower	O
bound	O
for	O
ernb	O
proves	O
the	O
existence	O
of	O
abe	O
whose	O
sponding	O
error	O
probability	O
exceeds	O
the	O
lower	O
bound	O
since	O
b	O
is	O
uniformly	O
distributed	O
in	O
its	O
binary	B
extension	O
b	O
is	O
a	O
sequence	O
of	O
independent	O
binary	B
random	O
variables	O
with	O
pbi	O
o	O
pbi	O
i	O
but	O
ernb	O
p	O
dn	O
bx	O
p	O
xl	O
bxl	O
xn	O
bxj	O
bx	O
e	O
xl	O
bx	O
x	O
n	O
bxj	O
bx	O
x	O
xl	O
xn	O
xl	O
x	O
x	O
x	O
xn	O
since	O
if	O
x	O
xi	O
for	O
all	O
i	O
n	O
then	O
given	O
x	O
xl	O
x	O
n	O
y	O
bx	O
is	O
conditionally	O
independent	O
of	O
gnx	O
dn	O
and	O
y	O
takes	O
values	O
and	O
with	O
probability	O
slow	O
rates	O
but	O
clearly	O
px	O
i	O
xl	O
x	O
i	O
x	O
x	O
i	O
xnl	O
x	O
px	O
i	O
xiixn	O
ikt	O
in	O
summary	O
sup	O
rnb	O
kt	O
heol	O
the	O
lower	O
bound	O
tends	O
to	O
as	O
k	O
d	O
theorem	O
states	O
that	O
even	O
though	O
we	O
have	O
rules	O
that	O
are	O
universally	O
consistent	O
that	O
is	O
they	O
asymptotically	O
provide	O
the	O
optimal	O
performance	O
for	O
any	O
distribution	O
their	O
finite	O
sample	O
performance	O
is	O
always	O
extremely	O
bad	O
for	O
some	O
distributions	O
this	O
means	O
that	O
no	O
classifier	B
guarantees	O
that	O
with	O
a	O
sample	O
size	O
of	O
n	O
we	O
get	O
within	O
of	O
the	O
bayes	B
error	I
probability	O
for	O
all	O
distributions	O
however	O
as	O
the	O
bad	O
distribution	O
depends	O
upon	O
n	O
theorem	O
does	O
not	O
allow	O
us	O
to	O
conclude	O
that	O
there	O
is	O
one	O
distribution	O
for	O
which	O
the	O
error	O
probability	O
is	O
more	O
than	O
l	O
for	O
all	O
n	O
indeed	O
that	O
would	O
contradict	O
the	O
very	O
existence	O
of	O
universally	O
consistent	O
rules	O
slow	O
rates	O
the	O
next	O
question	O
is	O
whether	O
a	O
certain	O
universal	O
rate	B
of	I
convergence	I
to	O
l	O
is	O
achievable	O
for	O
some	O
classifier	B
for	O
example	O
theorem	O
does	O
not	O
exclude	O
the	O
existence	O
of	O
a	O
classifier	B
such	O
that	O
for	O
every	O
n	O
eln	O
l	O
c	O
n	O
for	O
all	O
distributions	O
for	O
some	O
constant	O
c	O
depending	O
upon	O
the	O
actual	O
distribution	O
the	O
next	O
negative	O
result	O
is	O
that	O
this	O
cannot	O
be	O
the	O
case	O
theorem	O
below	O
states	O
that	O
the	O
error	O
probability	O
eln	O
of	O
any	O
classifier	B
is	O
larger	O
than	O
l	O
clog	O
log	O
log	O
n	O
for	O
every	O
n	O
for	O
some	O
distribution	O
even	O
if	O
c	O
depends	O
on	O
the	O
distribution	O
can	O
be	O
seen	O
by	O
considering	O
that	O
by	O
theorem	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
such	O
that	O
eln	O
l	O
log	O
log	O
n	O
for	O
every	O
n	O
moreover	O
there	O
is	O
no	O
sequence	O
of	O
numbers	O
an	O
converging	O
to	O
zero	O
such	O
that	O
there	O
is	O
a	O
classification	O
rule	B
with	O
error	O
probability	O
below	O
l	O
plus	O
an	O
for	O
all	O
distributions	O
thus	O
in	O
practice	O
no	O
classifier	B
assures	O
us	O
that	O
its	O
error	O
probability	O
is	O
close	O
to	O
l	O
unless	O
the	O
actual	O
distribution	O
is	O
known	O
to	O
be	O
a	O
member	O
of	O
a	O
restricted	O
class	O
of	O
distributions	O
now	O
it	O
is	O
easily	O
seen	O
that	O
in	O
the	O
proof	O
of	O
both	O
theorems	O
we	O
could	O
take	O
x	O
to	O
have	O
uniform	O
distribution	O
on	O
or	O
any	O
other	O
density	O
problem	O
therefore	O
putting	O
restrictions	O
on	O
the	O
distribution	O
of	O
x	O
alone	O
does	O
not	O
suffice	O
to	O
obtain	O
rate-of-convergence	O
results	O
for	O
such	O
results	O
one	O
needs	O
conditions	O
on	O
the	O
a	B
posteriori	I
probability	I
as	O
well	O
however	O
if	O
only	O
training	O
data	O
give	O
information	O
about	O
the	O
joint	O
distribution	O
then	O
theorems	O
with	O
extra	O
conditions	O
on	O
the	O
distribution	O
have	O
little	O
practical	O
value	O
as	O
it	O
is	O
impossible	O
to	O
detect	O
whether	O
for	O
example	O
the	O
a	B
posteriori	I
probability	I
is	O
twice	O
differentiable	O
or	O
not	O
now	O
the	O
situation	O
may	O
look	O
hopeless	O
but	O
this	O
is	O
not	O
so	O
simply	O
put	O
the	O
bayes	B
error	I
is	O
too	O
difficult	O
a	O
target	O
to	O
shoot	O
at	O
slow	O
rates	O
of	O
convergence	O
weaker	O
versions	O
of	O
theorem	O
appeared	O
earlier	O
in	O
the	O
literature	O
first	O
cover	O
showed	O
that	O
for	O
any	O
sequence	O
of	O
classification	O
rules	O
for	O
sequences	O
converging	O
to	O
zero	O
at	O
arbitrarily	O
slow	O
algebraic	O
rates	O
as	O
for	O
arbitrarily	O
small	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
eln	O
l	O
an	O
infinitely	O
often	O
devroye	O
strengthened	O
covers	O
result	O
allowing	O
sequences	O
tending	O
to	O
zero	O
arbitrarily	O
slowly	O
the	O
next	O
result	O
asserts	O
that	O
eln	O
l	O
an	O
for	O
every	O
n	O
theorem	O
let	O
be	O
a	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
a	O
i	O
for	O
every	O
sequence	O
of	O
classification	O
rules	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
with	O
l	O
such	O
that	O
for	O
all	O
n	O
this	O
result	O
shows	O
that	O
universally	O
good	O
classification	O
rules	O
do	O
not	O
exist	O
rate	B
of	I
convergence	I
studies	O
for	O
particular	O
rules	O
must	O
necessarily	O
be	O
accompanied	O
by	O
conditions	O
on	O
y	O
that	O
these	O
conditions	O
too	O
are	O
necessarily	O
restrictive	O
follows	O
from	O
examples	O
suggested	O
in	O
problem	O
under	O
certain	O
regularity	O
conditions	O
it	O
is	O
possible	O
to	O
obtain	O
upper	O
bounds	O
for	O
the	O
rates	O
of	O
convergence	O
for	O
the	O
probability	O
of	O
error	O
of	O
certain	O
rules	O
to	O
l	O
then	O
it	O
is	O
natural	O
to	O
ask	O
what	O
the	O
fastest	O
achievable	O
rate	O
is	O
for	O
the	O
given	O
class	O
of	O
distributions	O
a	O
theory	O
for	O
regression	B
function	I
estimation	B
was	O
worked	O
out	O
by	O
stone	O
related	O
results	O
for	O
classification	O
were	O
obtained	O
by	O
marron	O
in	O
the	O
proof	O
of	O
theorem	O
we	O
will	O
need	O
the	O
following	O
simple	O
lemma	O
lemma	O
for	O
any	O
monotone	O
decreasing	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
al	O
a	O
probability	O
distribution	O
may	O
be	O
found	O
such	O
that	O
pi	O
and	O
for	O
all	O
n	O
l	O
pi	O
max	O
proof	O
it	O
suffices	O
to	O
look	O
for	O
pis	O
such	O
that	O
l	O
pi	O
max	O
these	O
conditions	O
are	O
easily	O
satisfied	O
for	O
positive	O
integers	O
u	O
v	O
define	O
the	O
tion	O
h	O
v	O
u	O
ll	O
ii	O
first	O
we	O
find	O
a	O
sequence	O
n	O
of	O
integers	O
with	O
the	O
following	O
properties	O
hnkl	O
nk	O
is	O
monotonically	O
increasing	O
nr	O
for	O
all	O
k	O
note	O
that	O
may	O
only	O
be	O
satisfied	O
if	O
anl	O
al	O
to	O
this	O
end	O
define	O
constants	O
cl	O
by	O
slow	O
rates	O
so	O
that	O
the	O
ck	O
s	O
are	O
decreasing	O
in	O
k	O
and	O
for	O
n	O
e	O
nkl	O
we	O
define	O
pn	O
we	O
claim	O
that	O
these	O
numbers	O
have	O
the	O
required	O
properties	O
indeed	O
is	O
decreasing	O
and	O
finally	O
if	O
n	O
e	O
nkl	O
then	O
p	O
n	O
i	O
inl	O
j	O
j	O
c	O
jkl	O
jkl	O
clearly	O
on	O
the	O
one	O
hand	O
by	O
the	O
monotonicity	O
of	O
hnkl	O
nk	O
ck	O
on	O
the	O
other	O
hand	O
this	O
concludes	O
the	O
proof	O
proof	O
of	O
theorem	O
we	O
introduce	O
some	O
notation	O
let	O
b	O
o	O
b	O
l	O
be	O
a	O
real	O
number	O
on	O
with	O
the	O
shown	O
binary	B
expansion	O
and	O
let	O
b	O
be	O
a	O
random	O
variable	B
uniformly	O
distributed	O
on	O
with	O
expansion	O
b	O
o	O
bi	O
let	O
us	O
restrict	O
ourselves	O
to	O
a	O
random	O
variable	B
x	O
with	O
px	O
i	O
pi	O
i	O
where	O
pi	O
and	O
lnl	O
pi	O
max	O
for	O
every	O
n	O
that	O
such	O
pis	O
exist	O
follows	O
from	O
lemma	O
set	O
y	O
bx	O
as	O
y	O
is	O
a	O
function	O
of	O
x	O
we	O
see	O
that	O
l	O
each	O
b	O
e	O
however	O
describes	O
a	O
different	O
distribution	O
with	O
b	O
replaced	O
by	O
b	O
we	O
have	O
a	O
random	O
distribution	O
introduce	O
the	O
short	O
notation	O
bxj	O
bxj	O
and	O
define	O
g	O
ni	O
gni	O
probabilityoferrorpgnx	O
yib	O
xl	O
xn	O
for	O
the	O
random	O
distribution	O
then	O
we	O
note	O
that	O
we	O
may	O
write	O
lnb	O
l	O
pjgni	O
bd	O
il	O
slow	O
rates	O
of	O
convergence	O
if	O
lnb	O
is	O
the	O
probability	O
of	O
error	O
for	O
a	O
distribution	O
parametrized	O
by	O
b	O
then	O
lnb	O
b	O
n	O
we	O
consider	O
only	O
the	O
conditional	O
expectation	O
for	O
now	O
we	O
have	O
e	O
lnb	O
i	O
xl	O
x	O
n	O
p	O
xl	O
lplnb	O
xl	O
x	O
lp	B
xl	O
x	O
xn	O
l	O
e	O
ln	O
i	O
i	O
x	O
i	O
x	O
x	O
n	O
we	O
bound	O
the	O
conditional	O
probabilities	O
inside	O
the	O
sum	O
p	O
p	O
l	O
i	O
pjgltijb	O
i	O
noting	O
that	O
g	O
ni	O
xl	O
xn	O
are	O
all	O
functions	O
of	O
we	O
have	O
piibil	O
i	O
p	O
l	O
p	O
pi	O
ibil	O
p	O
pibi	O
the	O
pis	O
are	O
decreasing	O
by	O
stochastic	O
dominance	O
now	O
everything	O
boils	O
down	O
to	O
bounding	O
these	O
probabilities	O
from	O
above	O
we	O
ceed	O
by	O
chernoffs	O
bounding	O
technique	O
the	O
idea	O
is	O
the	O
following	O
for	O
any	O
random	O
variable	B
x	O
and	O
s	O
by	O
markovs	O
inequality	B
slow	O
rates	O
by	O
cleverly	O
choosing	O
s	O
one	O
can	O
often	O
obtain	O
very	O
sharp	O
bounds	O
for	O
more	O
discussion	O
and	O
examples	O
of	O
chernoffs	O
method	O
refer	O
to	O
chapter	O
in	O
our	O
case	O
e-x	O
x	O
x	O
for	O
x	O
x	O
e-	O
x	O
sb	O
exp	O
b	O
lnl	O
pj	O
exp	O
bpnl	O
taking	O
s	O
l	O
and	O
the	O
fact	O
that	O
b	O
exp	O
b	O
pnll	O
pnl	O
b	O
thus	O
we	O
conclude	O
that	O
supinfe--	O
lncb	O
b	O
n	O
nl	O
so	O
that	O
there	O
exists	O
a	O
b	O
for	O
which	O
elnb	O
an	O
for	O
all	O
n	O
slow	O
rates	O
of	O
convergence	O
problems	O
and	O
exercises	O
problem	O
extend	O
theorem	O
for	O
distributions	O
with	O
l	O
show	O
that	O
if	O
an	O
is	O
a	O
sequence	O
of	O
positive	O
numbers	O
as	O
in	O
theorem	O
then	O
for	O
any	O
classification	O
rule	B
there	O
is	O
a	O
distribution	O
such	O
that	O
eln	O
l	O
an	O
for	O
every	O
n	O
for	O
which	O
l	O
an	O
problem	O
prove	O
theorems	O
and	O
under	O
one	O
of	O
the	O
following	O
additional	O
tions	O
which	O
make	O
the	O
case	O
that	O
one	O
will	O
need	O
very	O
restrictive	O
conditions	O
indeed	O
to	O
study	O
rates	O
of	O
convergence	O
x	O
has	O
a	O
uniform	O
density	O
on	O
x	O
has	O
a	O
uniform	O
density	O
on	O
and	O
is	O
infinitely	O
many	O
times	O
continuously	O
is	O
unimodal	O
in	O
x	O
e	O
n	O
that	O
is	O
decreases	O
as	O
a	O
increases	O
for	O
any	O
differentiable	O
on	O
x	O
e	O
is	O
l-valued	O
x	O
is	O
n	O
and	O
the	O
set	O
i	O
is	O
a	O
compact	O
convex	O
set	O
containing	O
the	O
origin	O
problem	O
there	O
is	O
no	O
super-classifier	O
show	O
that	O
for	O
every	O
sequence	O
of	O
tion	O
rules	O
there	O
is	O
a	O
universally	O
consistent	O
sequence	O
of	O
rules	O
such	O
that	O
for	O
some	O
distribution	O
of	O
y	O
pgnx	O
y	O
pgx	O
y	O
for	O
all	O
n	O
problem	O
the	O
next	O
two	O
exercises	O
are	O
intended	O
to	O
demonstrate	O
that	O
the	O
weaponry	O
of	O
pattern	O
recognition	O
can	O
often	O
be	O
successfully	O
used	O
for	O
attacking	O
other	O
statistical	O
problems	O
for	O
example	O
a	O
consequence	O
of	O
theorem	O
is	O
that	O
estimating	O
infinite	O
discrete	O
distributions	O
is	O
hard	O
consider	O
the	O
problem	O
of	O
estimating	O
a	O
distribution	O
on	O
the	O
positive	O
integers	O
from	O
a	O
sample	O
xl	O
xn	O
of	O
i	O
i	O
d	O
random	O
variables	O
with	O
i	O
pi	O
i	O
show	O
that	O
for	O
any	O
decreasing	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
al	O
and	O
any	O
estimate	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
e	O
ipi	O
pinl	O
an	O
hint	O
consider	O
a	O
classification	O
problem	O
with	O
l	O
pry	O
o	O
and	O
x	O
concentrated	O
on	O
assume	O
that	O
the	O
class-conditional	B
probabilities	O
pia	O
px	O
ily	O
o	O
and	O
pil	O
px	O
ily	O
i	O
are	O
estimated	O
from	O
two	O
i	O
i	O
d	O
samples	O
xiol	O
xo	O
and	O
xill	O
xl	O
distributed	O
according	O
to	O
and	O
respectively	O
use	O
theorem	O
to	O
show	O
that	O
for	O
the	O
classification	O
rule	B
obtained	O
from	O
these	O
estimates	O
in	O
a	O
natural	O
way	O
therefore	O
the	O
lower	O
bound	O
of	O
theorem	O
can	O
be	O
applied	O
problem	O
a	O
similar	O
slow-rate	O
result	O
appears	O
in	O
density	B
estimation	B
consider	O
the	O
lem	O
of	O
estimating	O
a	O
density	O
on	O
n	O
from	O
an	O
i	O
i	O
d	O
sample	O
x	O
i	O
xn	O
having	O
density	O
show	O
that	O
for	O
any	O
decreasing	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
a	O
i	O
and	O
any	O
density	O
estimate	O
in	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
problems	O
and	O
exercises	O
e	O
ifx	O
fnxldx	O
an	O
this	O
result	O
was	O
proved	O
by	O
birge	O
using	O
a	O
different-and	O
in	O
our	O
view	O
much	O
more	O
d	O
comp	O
lcate	O
hint	O
put	O
pi	O
fnxdx	O
and	O
apply	O
problem	O
and	O
pin	O
ji	O
ril	O
f	O
r	O
error	B
estimation	B
error	B
counting	I
estimating	O
the	O
error	O
probability	O
ln	O
pgn	O
y	O
i	O
dn	O
of	O
a	O
classification	O
function	O
gn	O
is	O
of	O
essential	O
importance	O
the	O
designer	O
always	O
wants	O
to	O
know	O
what	O
performance	O
can	O
be	O
expected	O
from	O
a	O
classifier	B
as	O
the	O
designer	O
does	O
not	O
know	O
the	O
distribution	O
of	O
the	O
data-otherwise	O
there	O
would	O
not	O
be	O
any	O
need	O
to	O
design	O
a	O
classifier-it	O
is	O
important	O
to	O
find	O
error	B
estimation	B
methods	O
that	O
work	O
well	O
without	O
any	O
condition	O
on	O
the	O
distribution	O
of	O
y	O
this	O
motivates	O
us	O
to	O
search	O
for	O
distribution-free	O
performance	O
bounds	O
for	O
error	B
estimation	B
methods	O
suppose	O
that	O
we	O
want	O
to	O
estimate	O
the	O
error	O
probability	O
of	O
a	O
classifier	B
gn	O
designed	O
from	O
the	O
training	O
sequence	O
dn	O
yl	O
yn	O
assume	O
first	O
that	O
a	O
testing	B
sequence	I
tm	O
ynl	O
ynm	O
is	O
available	O
which	O
is	O
a	O
sequence	O
of	O
i	O
i	O
d	O
pairs	O
that	O
are	O
independent	O
of	O
y	O
and	O
dn	O
and	O
that	O
are	O
distributed	O
as	O
y	O
an	O
obvious	O
way	O
to	O
estimate	O
ln	O
is	O
to	O
count	O
the	O
number	O
of	O
errors	O
that	O
gn	O
commits	O
on	O
tm	O
the	O
error-counting	O
estimator	O
lnm	O
is	O
defined	O
by	O
the	O
relative	O
frequency	O
the	O
estimator	O
is	O
clearly	O
unbiased	O
in	O
the	O
sense	O
that	O
error	B
estimation	B
and	O
the	O
conditional	O
distribution	O
of	O
mlnm	O
given	O
the	O
training	O
data	O
dn	O
is	O
binomial	B
with	O
parameters	O
m	O
and	O
ln	O
this	O
makes	O
analysis	O
easy	O
for	O
properties	O
of	O
the	O
binomial	B
distribution	I
are	O
well	O
known	O
one	O
main	O
tool	O
in	O
the	O
analysis	O
is	O
hoeffdings	O
inequality	B
which	O
we	O
will	O
use	O
many	O
many	O
times	O
throughout	O
this	O
book	O
hoeffdings	O
inequality	B
the	O
following	O
inequality	B
was	O
proved	O
for	O
binomial	B
random	O
variables	O
by	O
chernoff	O
and	O
okamoto	O
the	O
general	O
format	O
is	O
due	O
to	O
hoeffding	O
theorem	O
let	O
xl	O
xn	O
be	O
independent	O
bounded	O
random	O
variables	O
such	O
that	O
xi	O
falls	O
in	O
the	O
interval	O
bd	O
with	O
probability	O
one	O
denote	O
their	O
sum	O
by	O
sn	O
xi	O
thenfor	O
any	O
e	O
we	O
have	O
psn	O
esn	O
e	O
and	O
the	O
proof	O
uses	O
a	O
simple	O
auxiliary	O
inequality	B
lemma	O
let	O
x	O
be	O
a	O
random	O
variable	B
with	O
ex	O
a	O
x	O
b	O
then	O
for	O
s	O
proof	O
note	O
that	O
by	O
convexity	O
of	O
the	O
exponential	B
function	O
esx	O
esb	O
esa	O
x-a	O
b-a	O
b-x	O
b-a	O
for	O
a	O
x	O
b	O
exploiting	O
ex	O
and	O
introducing	O
the	O
notation	O
p	O
a	O
we	O
get	O
b	O
a	O
esa	O
esb	O
b-a	O
p	O
pesb-a	O
e-psb-a	O
b-a	O
def	O
e	O
where	O
u	O
sb	O
a	O
and	O
pu	O
p	O
pe	O
u	O
calculation	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
derivative	O
of	O
is	O
but	O
by	O
straightforward	O
p	O
p	O
p	O
p	O
e	O
u	O
therefore	O
moreover	O
p	O
pe-u	O
p	O
e	O
thus	O
by	O
taylor	O
series	O
expansion	O
with	O
remainder	O
for	O
some	O
e	O
e	O
u	O
hoeffdings	O
inequality	B
proof	O
of	O
theorem	O
the	O
proof	O
is	O
based	O
on	O
chernoffs	O
bounding	O
method	O
by	O
markovs	O
inequality	B
for	O
any	O
nonnegative	O
random	O
variable	B
x	O
and	O
any	O
e	O
ex	O
pxes-	O
e	O
therefore	O
if	O
s	O
is	O
an	O
arbitrary	O
positive	O
number	O
then	O
for	O
any	O
random	O
variable	B
x	O
in	O
chernoffs	O
method	O
we	O
find	O
an	O
s	O
that	O
minimizes	O
the	O
upper	O
bound	O
or	O
makes	O
the	O
upper	O
bound	O
small	O
in	O
our	O
case	O
we	O
have	O
e-se	O
n	O
e	O
e-se	O
n	O
il	O
n	O
n	O
independence	O
lemma	O
il	O
the	O
second	O
inequality	B
is	O
proved	O
analogouslo	O
the	O
two	O
inequalities	O
in	O
theorem	O
may	O
be	O
combined	O
to	O
get	O
now	O
we	O
can	O
apply	O
this	O
inequality	B
to	O
get	O
a	O
distribution-free	O
performance	O
bound	O
for	O
the	O
counting	O
error	O
estimate	O
corollary	O
for	O
every	O
e	O
error	B
estimation	B
the	O
variance	B
of	I
the	O
estimate	O
can	O
easily	O
be	O
computed	O
using	O
the	O
fact	O
that	O
tioned	O
on	O
the	O
data	O
dn	O
mlnm	O
is	O
binomially	O
distributed	O
these	O
are	O
just	O
the	O
types	O
of	O
inequalities	O
we	O
want	O
for	O
these	O
are	O
valid	O
for	O
any	O
bution	O
and	O
data	O
size	O
and	O
the	O
bounds	O
do	O
not	O
even	O
depend	O
on	O
gn	O
consider	O
a	O
special	O
case	O
in	O
which	O
all	O
the	O
xs	O
take	O
values	O
on	O
c	O
and	O
have	O
zero	O
mean	O
then	O
hoeffdings	O
inequality	B
states	O
that	O
p	O
n	O
e	O
s	O
e-nr	O
this	O
bound	O
while	O
useful	O
for	O
e	O
larger	O
than	O
c	O
ignores	O
variance	O
information	O
when	O
varxi	O
it	O
is	O
indeed	O
possible	O
to	O
outperform	O
hoeffdings	O
inequality	B
in	O
particular	O
we	O
have	O
theorem	O
and	O
bernstein	O
let	O
xl	O
xn	O
be	O
independent	O
real-valued	O
random	O
variables	O
with	O
zero	O
mean	O
and	O
assume	O
that	O
xi	O
c	O
with	O
probability	O
one	O
let	O
lvarxd	O
n	O
n	O
il	O
then	O
for	O
any	O
e	O
and	O
p	O
t	O
xi	O
e	O
exp	O
n	O
il	O
the	O
proofs	O
are	O
left	O
as	O
exercises	O
we	O
note	O
that	O
bernsteins	O
inequality	B
kicks	O
in	O
when	O
e	O
is	O
larger	O
than	O
about	O
max	O
cl	O
it	O
is	O
typically	O
better	O
than	O
hoeffdings	O
inequality	B
when	O
c	O
error	B
estimation	B
without	O
testing	O
data	O
a	O
serious	O
problem	O
concerning	O
the	O
practical	O
applicability	O
of	O
the	O
estimate	O
introduced	O
above	O
is	O
that	O
it	O
requires	O
a	O
large	O
independent	O
testing	B
sequence	I
in	O
practice	O
ever	O
an	O
additional	O
sample	O
is	O
rarely	O
available	O
one	O
usually	O
wants	O
to	O
incorporate	O
all	O
available	O
yi	O
pairs	O
in	O
the	O
decision	O
function	O
in	O
such	O
cases	O
to	O
estimate	O
l	O
n	O
we	O
have	O
to	O
rely	O
on	O
the	O
training	O
data	O
only	O
there	O
are	O
well-known	O
methods	O
that	O
we	O
selecting	O
classifiers	O
will	O
discuss	O
later	O
that	O
are	O
based	O
on	O
cross-validation	B
leave-one-out	B
and	O
brailovsky	O
stone	O
and	O
holdout	B
resubstitution	B
rotation	B
smoothing	O
and	O
bootstrapping	O
which	O
may	O
be	O
employed	O
to	O
construct	O
an	O
empirical	B
risk	O
from	O
the	O
training	O
sequence	O
thus	O
obviating	O
the	O
need	O
for	O
a	O
testing	B
sequence	I
kanal	O
cover	O
and	O
wagner	O
toussaint	O
glick	O
hand	O
jain	O
dubes	O
and	O
chen	O
and	O
mclachlan	O
for	O
surveys	O
discussion	O
and	O
empirical	B
comparison	O
analysis	O
of	O
these	O
methods	O
in	O
general	O
is	O
clearly	O
a	O
much	O
harder	O
problem	O
as	O
can	O
depend	O
on	O
dn	O
in	O
a	O
rather	O
complicated	O
way	O
if	O
we	O
construct	O
some	O
estimator	O
ln	O
from	O
d	O
n	O
then	O
it	O
would	O
be	O
desirable	O
to	O
obtain	O
distribution-free	O
bounds	O
on	O
or	O
on	O
e	O
lnl	O
q	O
for	O
some	O
q	O
conditional	O
probabilities	O
and	O
expectations	O
given	O
dn	O
are	O
ingless	O
since	O
everything	O
is	O
a	O
funseion	O
of	O
dn	O
here	O
however	O
we	O
have	O
to	O
be	O
much	O
more	O
careful	O
as	O
we	O
do	O
not	O
want	O
ln	O
to	O
be	O
optimistically	O
biased	O
because	O
the	O
same	O
data	O
are	O
used	O
both	O
for	O
training	O
and	O
testing	O
distribution-free	O
bounds	O
for	O
the	O
above	O
quantities	O
would	O
be	O
extremely	O
helpful	O
as	O
we	O
usually	O
do	O
not	O
know	O
the	O
distribution	O
of	O
y	O
while	O
for	O
some	O
rules	O
such	O
estimates	O
exist-we	O
will	O
exhibit	O
several	O
avenues	O
in	O
chapters	O
and	O
i-it	O
is	O
disappointing	O
that	O
a	O
single	O
error	B
estimation	B
method	O
cannot	O
possibly	O
work	O
for	O
all	O
discrimination	O
rules	O
it	O
is	O
therefore	O
important	O
to	O
point	O
out	O
that	O
we	O
have	O
to	O
consider	O
ln	O
pairs-for	O
every	O
rule	B
one	O
or	O
more	O
error	O
estimates	O
must	O
be	O
found	O
if	O
possible	O
and	O
vice	O
versa	O
for	O
every	O
error	O
estimate	O
its	O
limitations	O
have	O
to	O
be	O
stated	O
secondly	O
rules	O
for	O
which	O
no	O
good	O
error	O
estimates	O
are	O
known	O
should	O
be	O
avoided	O
luckily	O
most	O
popular	O
rules	O
do	O
not	O
fall	O
into	O
this	O
category	O
on	O
the	O
other	O
hand	O
proven	O
distribution-free	O
performance	O
guarantees	O
are	O
rarely	O
available-see	O
chapters	O
and	O
for	O
examples	O
selecting	O
classifiers	O
probably	O
the	O
most	O
important	O
application	O
of	O
error	B
estimation	B
is	O
in	O
the	O
selection	O
of	O
a	O
classification	O
function	O
from	O
a	O
class	O
c	O
of	O
functions	O
if	O
a	O
class	O
c	O
of	O
classifiers	O
is	O
given	O
then	O
it	O
is	O
tempting	O
to	O
pick	O
the	O
one	O
that	O
minimizes	O
an	O
estimate	O
of	O
the	O
error	O
probability	O
over	O
the	O
class	O
a	O
good	O
method	O
should	O
pick	O
a	O
classifier	B
with	O
an	O
error	O
probability	O
that	O
is	O
close	O
to	O
the	O
minimal	O
error	O
probability	O
in	O
the	O
class	O
here	O
we	O
require	O
much	O
more	O
than	O
distribution-free	O
performance	O
bounds	O
of	O
the	O
error	O
estimator	O
for	O
each	O
of	O
the	O
classifiers	O
in	O
the	O
class	O
problem	O
demonstrates	O
that	O
it	O
is	O
not	O
sufficient	O
to	O
be	O
able	O
to	O
estimate	O
the	O
error	O
probability	O
of	O
all	O
classifiers	O
in	O
the	O
class	O
intuitively	O
if	O
we	O
can	O
estimate	O
the	O
error	O
probability	O
for	O
the	O
classifiers	O
in	O
c	O
uniformly	O
well	O
then	O
the	O
classification	O
function	O
that	O
minimizes	O
the	O
estimated	O
error	O
probability	O
is	O
likely	O
to	O
have	O
an	O
error	O
probability	O
that	O
is	O
close	O
to	O
the	O
best	O
in	O
the	O
class	O
to	O
certify	O
this	O
error	B
estimation	B
intuition	O
consider	O
the	O
following	O
situation	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
that	O
is	O
a	O
class	O
of	O
mappings	O
of	O
the	O
form	O
n	O
d	O
i	O
assume	O
that	O
the	O
error	O
count	O
n	O
ln	O
n	O
j	O
ipx	O
y	O
j	O
jl	O
is	O
used	O
to	O
estimate	O
the	O
error	O
probability	O
l	O
p	O
i	O
y	O
of	O
each	O
classifier	B
e	O
c	O
denote	O
by	O
the	O
classifier	B
that	O
minimizes	O
the	O
estimated	O
error	O
probability	O
over	O
the	O
class	O
ln	O
ln	O
for	O
all	O
e	O
c	O
then	O
for	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
we	O
have	O
lemma	O
and	O
chervonenkis	O
see	O
also	O
devroye	O
l	O
inf	O
l	O
sup	O
iln	O
l	O
iln	O
l	O
sup	O
iln	O
l	O
proof	O
l	O
l	O
n	O
ln	O
inf	O
l	O
l	O
ln	O
sup	O
iln	O
l	O
sup	O
iln	O
l	O
the	O
second	O
inequality	B
is	O
trivially	O
true	O
we	O
see	O
that	O
upper	O
bounds	O
for	O
suppec	O
iln	O
l	O
provide	O
us	O
with	O
upper	O
bounds	O
for	O
two	O
things	O
simultaneously	O
an	O
upper	O
bound	O
for	O
the	O
suboptimality	O
of	O
within	O
c	O
that	O
is	O
a	O
bound	O
for	O
l	O
infpec	O
l	O
an	O
upper	O
bound	O
for	O
the	O
error	O
iln	O
l	O
committed	O
when	O
l	O
n	O
is	O
used	O
to	O
estimate	O
the	O
probability	O
of	O
error	O
l	O
of	O
the	O
selected	O
rule	B
in	O
other	O
words	O
by	O
bounding	O
suppec	O
iln	O
l	O
we	O
kill	O
two	O
flies	O
at	O
once	O
it	O
is	O
particularly	O
useful	O
to	O
know	O
that	O
even	O
though	O
ln	O
is	O
usually	O
optimistically	O
biased	O
it	O
is	O
within	O
given	O
bounds	O
of	O
the	O
unknown	O
probability	O
of	O
error	O
with	O
and	O
that	O
no	O
other	O
test	O
sample	O
is	O
needed	O
to	O
estimate	O
this	O
probability	O
of	O
error	O
whenever	O
selecting	O
classifiers	O
our	O
bounds	O
indicate	O
that	O
we	O
are	O
close	O
to	O
the	O
optimum	O
in	O
c	O
we	O
must	O
at	O
the	O
same	O
time	O
have	O
a	O
good	O
estimate	O
of	O
the	O
probability	O
of	O
error	O
and	O
vice	O
versa	O
as	O
a	O
simple	O
but	O
interesting	O
application	O
of	O
lemma	O
we	O
consider	O
the	O
case	O
when	O
the	O
class	O
c	O
contains	O
finitely	O
many	O
classifiers	O
theorem	O
assume	O
that	O
the	O
cardinality	O
oj	O
c	O
is	O
bounded	O
by	O
n	O
then	O
we	O
have	O
for	O
all	O
e	O
proof	O
p	O
iln	O
l	O
e	O
lpiln	O
l	O
e	O
where	O
we	O
used	O
hoeffding	O
s	O
inequality	B
and	O
the	O
fact	O
that	O
the	O
random	O
variable	B
nln	O
is	O
binomially	O
distributed	O
with	O
parameters	O
nand	O
l	O
remark	O
distribution-free	O
properties	O
theorem	O
shows	O
that	O
the	O
problem	O
studied	O
here	O
is	O
purely	O
combinatorial	O
the	O
actual	O
distribution	O
of	O
the	O
data	O
does	O
not	O
playa	O
role	O
at	O
all	O
in	O
the	O
upper	O
bounds	O
remark	O
without	O
testing	O
data	O
very	O
often	O
a	O
class	O
of	O
rules	O
c	O
of	O
the	O
form	O
n	O
nx	O
dn	O
is	O
given	O
and	O
the	O
same	O
data	O
dn	O
are	O
used	O
to	O
select	O
a	O
rule	B
by	O
minimizing	O
some	O
estimates	O
ln	O
n	O
of	O
the	O
error	O
probabilities	O
l	O
n	O
p	O
nx	O
yidn	O
a	O
similar	O
analysis	O
can	O
be	O
carried	O
out	O
in	O
this	O
case	O
in	O
particular	O
if	O
denotes	O
the	O
selected	O
rule	B
then	O
we	O
have	O
similar	O
to	O
lemma	O
theorem	O
and	O
iln	O
l	O
sup	O
iln	O
n	O
l	O
nl	O
ec	O
if	O
c	O
is	O
finite	O
then	O
again	O
similar	O
to	O
theorem	O
we	O
have	O
for	O
example	O
error	B
estimation	B
estimating	O
the	O
bayes	B
error	I
it	O
is	O
also	O
important	O
to	O
have	O
a	O
good	O
estimate	O
of	O
the	O
optimal	O
error	O
probability	O
l	O
first	O
of	O
all	O
if	O
l	O
is	O
large	O
we	O
would	O
know	O
beforehand	O
that	O
any	O
rule	B
is	O
going	O
to	O
perform	O
poorly	O
then	O
perhaps	O
the	O
information	O
might	O
be	O
used	O
to	O
return	O
to	O
the	O
feature	O
selection	O
stage	O
also	O
a	O
comparison	O
of	O
estimates	O
of	O
ln	O
and	O
l	O
gives	O
us	O
an	O
idea	O
how	O
much	O
room	O
is	O
left	O
for	O
improvement	O
typically	O
l	O
is	O
estimated	O
by	O
an	O
estimate	O
of	O
the	O
error	O
probability	O
of	O
some	O
consistent	O
classification	O
rule	B
fukunaga	O
and	O
kessel	O
chen	O
and	O
fu	O
fukunaga	O
and	O
hummels	O
and	O
garnett	O
and	O
yau	O
clearly	O
if	O
the	O
estimate	O
we	O
use	O
is	O
consistent	O
in	O
the	O
sense	O
that	O
in	O
ln	O
with	O
probability	O
one	O
as	O
n	O
and	O
the	O
rule	B
is	O
strongly	O
consistent	O
then	O
in	O
l	O
with	O
probability	O
one	O
in	O
other	O
words	O
we	O
have	O
a	O
consistent	O
estimate	O
of	O
the	O
bayes	B
error	I
probability	O
there	O
are	O
two	O
problems	O
with	O
this	O
approach	O
the	O
first	O
problem	O
is	O
that	O
if	O
our	O
purpose	O
is	O
comparing	O
l	O
with	O
l	O
n	O
then	O
using	O
the	O
same	O
estimate	O
for	O
both	O
of	O
them	O
does	O
not	O
any	O
information	O
the	O
other	O
problem	O
is	O
that	O
even	O
though	O
for	O
many	O
classifiers	O
ln	O
ln	O
can	O
be	O
guaranteed	O
to	O
converge	O
to	O
zero	O
rapidly	O
regardless	O
what	O
the	O
distribution	O
of	O
y	O
is	O
chters	O
and	O
in	O
view	O
of	O
the	O
results	O
of	O
chapter	O
the	O
rate	B
of	I
convergence	I
of	O
ln	O
to	O
l	O
using	O
such	O
a	O
method	O
may	O
be	O
arbitrarily	O
slow	O
thus	O
we	O
cannot	O
expect	O
good	O
performance	O
for	O
all	O
distributions	O
from	O
such	O
a	O
method	O
the	O
question	O
is	O
whether	O
it	O
is	O
possible	O
to	O
come	O
up	O
with	O
a	O
method	O
of	O
estimating	O
l	O
such	O
that	O
the	O
difference	O
in	O
l	O
converges	O
to	O
zero	O
rapidly	O
for	O
all	O
distributions	O
unfortunately	O
there	O
is	O
no	O
method	O
that	O
guarantees	O
a	O
certain	O
finite	O
sample	O
performance	O
for	O
all	O
distributions	O
this	O
disappointing	O
fact	O
is	O
reflected	O
in	O
the	O
following	O
negative	O
result	O
theorem	O
for	O
every	O
n	O
for	O
any	O
estimate	O
in	O
of	O
the	O
bayes	B
error	I
probability	O
l	O
and	O
for	O
every	O
e	O
there	O
exists	O
a	O
distribution	O
of	O
x	O
y	O
such	O
that	O
e	O
li	O
e	O
proof	O
for	O
a	O
fixed	O
n	O
we	O
construct	O
a	O
family	O
f	O
of	O
distributions	O
and	O
show	O
that	O
for	O
at	O
least	O
one	O
member	O
of	O
the	O
family	O
e	O
i	O
in	O
l	O
i	O
e	O
the	O
family	O
contains	O
distributions	O
where	O
m	O
is	O
a	O
large	O
integer	O
specified	O
later	O
in	O
all	O
cases	O
xl	O
xn	O
are	O
drawn	O
independently	O
by	O
a	O
uniform	O
distribution	O
from	O
the	O
set	O
m	O
let	O
bo	O
bi	O
bn	O
be	O
i	O
i	O
d	O
bernoulli	O
random	O
variables	O
independent	O
of	O
the	O
xis	O
with	O
pbi	O
o	O
pbi	O
i	O
for	O
the	O
first	O
member	O
of	O
the	O
family	O
f	O
let	O
yi	O
bi	O
for	O
i	O
n	O
thus	O
for	O
this	O
distribution	O
l	O
the	O
bayes	B
error	I
for	O
the	O
other	O
members	O
of	O
the	O
family	O
is	O
zero	O
these	O
distributions	O
are	O
determined	O
by	O
m	O
binary	B
parameters	O
ai	O
e	O
i	O
as	O
follows	O
ri	O
py	O
i	O
ai	O
in	O
other	O
words	O
yi	O
a	O
xi	O
for	O
every	O
i	O
n	O
clearly	O
l	O
for	O
these	O
tions	O
note	O
also	O
that	O
all	O
distributions	O
with	O
x	O
distributed	O
uniformly	O
on	O
m	O
problems	O
and	O
exercises	O
and	O
l	O
are	O
members	O
of	O
the	O
family	O
just	O
as	O
in	O
the	O
proofs	O
of	O
theorems	O
and	O
we	O
randomize	O
over	O
the	O
family	O
f	O
of	O
distributions	O
however	O
the	O
way	O
of	O
ization	O
is	O
different	O
here	O
the	O
trick	O
is	O
to	O
use	O
bo	O
b	O
i	O
bn	O
in	O
randomly	O
picking	O
a	O
distribution	O
that	O
these	O
random	O
variables	O
are	O
just	O
the	O
labels	O
yi	O
yn	O
in	O
the	O
training	O
sequence	O
for	O
the	O
first	O
distribution	O
in	O
the	O
family	O
we	O
choose	O
a	O
tion	O
randomly	O
as	O
follows	O
if	O
bo	O
then	O
we	O
choose	O
the	O
first	O
member	O
of	O
f	O
one	O
with	O
l	O
if	O
bo	O
then	O
the	O
labels	O
of	O
the	O
training	O
sequence	O
are	O
given	O
by	O
if	O
xi	O
xl	O
xi	O
x	O
xi	O
xi	O
l	O
if	O
j	O
i	O
is	O
the	O
smallest	O
index	O
such	O
that	O
xi	O
x	O
j	O
note	O
that	O
in	O
case	O
of	O
bo	O
for	O
any	O
fixed	O
realization	O
hi	O
hn	O
e	O
i	O
of	O
b	O
i	O
bn	O
the	O
bayes	O
risk	O
is	O
zero	O
therefore	O
the	O
distribution	O
is	O
in	O
the	O
family	O
f	O
now	O
let	O
a	O
be	O
the	O
event	O
that	O
all	O
the	O
xis	O
are	O
different	O
observe	O
that	O
under	O
a	O
ln	O
is	O
a	O
function	O
of	O
xl	O
x	O
n	O
b	O
i	O
bn	O
only	O
but	O
not	O
bo	O
therefore	O
supeiln	O
li	O
eiln	O
li	O
bo	O
b	O
i	O
bn	O
random	O
e	O
i	O
a	O
i	O
ln	O
l	O
i	O
e	O
ii	O
ib	O
ii	O
e	O
lin	O
e	O
now	O
if	O
we	O
pick	O
m	O
large	O
enough	O
pa	O
can	O
be	O
as	O
close	O
to	O
as	O
desired	O
hence	O
sup	O
e	O
i	O
ln	O
l	O
i	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
distributions	O
of	O
y	O
problems	O
and	O
exercises	O
let	O
b	O
be	O
a	O
binomial	B
random	O
variable	B
with	O
parameters	O
nand	O
p	O
show	O
that	O
and	O
pb	O
e	O
ee-np-elogejnp	O
np	O
pb	O
e	O
ee-np-elogejnp	O
np	O
hint	O
proceed	O
by	O
chernoffs	O
bounding	O
method	O
error	B
estimation	B
problem	O
prove	O
the	O
inequalities	O
of	O
bennett	O
and	O
bernstein	O
given	O
in	O
theorem	O
to	O
help	O
you	O
we	O
will	O
guide	O
you	O
through	O
different	O
stages	O
show	O
that	O
for	O
any	O
s	O
and	O
any	O
random	O
variable	B
x	O
with	O
ex	O
x	O
where	O
show	O
that	O
fflu	O
for	O
u	O
by	O
chernoffs	O
bounding	O
method	O
show	O
that	O
feu	O
log	O
e	O
csu	O
ecs	O
lu	O
lu	O
show	O
that	O
feu	O
fo	O
ufo	O
cs	O
using	O
the	O
bound	O
of	O
find	O
the	O
optimal	O
value	O
of	O
s	O
and	O
derive	O
bennetts	O
inequality	B
cs	O
u	O
problem	O
use	O
bernsteins	O
inequality	B
to	O
show	O
that	O
if	O
b	O
is	O
a	O
binomial	B
p	O
random	O
variable	B
then	O
for	O
e	O
and	O
problem	O
letx	O
xn	O
be	O
independent	O
binary-valued	O
random	O
variables	O
withpxi	O
i	O
pxi	O
o	O
pi	O
set	O
p	O
ll	O
pi	O
and	O
sn	O
ll	O
xi	O
prove	O
that	O
and	O
valiant	O
see	O
also	O
hagerup	O
and	O
rub	O
compare	O
the	O
results	O
with	O
bernsteins	O
inequality	B
for	O
this	O
case	O
hint	O
put	O
s	O
e	O
and	O
s	O
e	O
in	O
the	O
chernoff	O
bounding	O
argument	O
prove	O
and	O
exploit	O
the	O
elementary	O
inequalities	O
and	O
e	O
e-lo	O
problem	O
let	O
b	O
be	O
a	O
binomial	B
p	O
random	O
variable	B
show	O
that	O
for	O
p	O
a	O
show	O
that	O
for	O
a	O
p	O
the	O
same	O
upper	O
bounds	O
hold	O
for	O
pb	O
an	O
see	O
also	O
hagerup	O
and	O
rub	O
hint	O
use	O
chernoffs	O
method	O
with	O
parameter	O
s	O
and	O
set	O
s	O
a	O
problem	O
let	O
b	O
be	O
a	O
binomial	B
p	O
random	O
variable	B
show	O
that	O
if	O
p	O
problems	O
and	O
exercises	O
and	O
if	O
p	O
hint	O
use	O
chernoffs	O
method	O
and	O
the	O
inequality	B
pb	O
np	O
e-	O
x	O
x	O
log	O
xlog	O
p	O
p	O
x	O
p	O
for	O
p	O
x	O
and	O
for	O
x	O
p	O
problem	O
let	O
b	O
be	O
a	O
binomial	B
p	O
random	O
variable	B
prove	O
that	O
and	O
p-jb	O
fp	O
eyn	O
p-jb	O
fp	O
hint	O
use	O
chernoffs	O
method	O
and	O
the	O
inequalities	O
x	O
log	O
x	O
log	O
yp	O
x	O
e	O
and	O
x	O
log	O
x	O
log	O
yp	O
x	O
e	O
p	O
x	O
p	O
x	O
p	O
p	O
x	O
p	O
problem	O
give	O
a	O
class	O
c	O
of	O
decision	O
functions	O
of	O
the	O
form	O
n	O
d	O
i	O
the	O
training	O
data	O
do	O
not	O
play	O
any	O
role	O
in	O
the	O
decision	O
such	O
that	O
for	O
every	O
e	O
supp	O
l	O
e	O
for	O
every	O
distribution	O
where	O
ln	O
is	O
the	O
error-counting	O
estimate	O
of	O
the	O
error	O
probability	O
l	O
p	O
y	O
of	O
decision	O
and	O
at	O
the	O
same	O
time	O
if	O
fn	O
is	O
the	O
class	O
of	O
mappings	O
minimizing	O
the	O
error	O
count	O
ln	O
over	O
the	O
class	O
c	O
then	O
there	O
exists	O
one	O
distribution	O
such	O
that	O
p	O
sup	O
l	O
for	O
all	O
n	O
inf	O
l	O
problem	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
that	O
is	O
a	O
class	O
of	O
mappings	O
of	O
the	O
form	O
nx	O
dn	O
nx	O
assume	O
that	O
an	O
independent	O
testing	B
sequence	I
tm	O
is	O
given	O
and	O
that	O
the	O
error	O
count	O
m	O
lnm	O
n	O
l	O
i	O
m	O
j	O
is	O
used	O
to	O
estimate	O
the	O
error	O
probability	O
l	O
n	O
p	O
nx	O
yidn	O
of	O
each	O
classifier	B
n	O
e	O
c	O
denote	O
by	O
the	O
classifier	B
that	O
minimizes	O
the	O
estimated	O
error	O
probability	O
over	O
the	O
class	O
prove	O
that	O
for	O
the	O
error	O
probability	O
error	O
of	O
the	O
selected	O
rule	B
we	O
have	O
also	O
if	O
c	O
is	O
of	O
finite	O
cardinality	O
with	O
lei	O
n	O
then	O
problem	O
show	O
that	O
if	O
a	O
rule	B
gn	O
is	O
consistent	O
then	O
we	O
can	O
always	O
find	O
an	O
estimate	O
of	O
the	O
error	O
such	O
that	O
elin	O
ln	O
iq	O
for	O
all	O
q	O
hint	O
split	O
the	O
data	O
sequence	O
dn	O
and	O
use	O
the	O
second	O
half	O
to	O
estimate	O
the	O
error	O
probability	O
of	O
problem	O
open-ended	O
problem	O
is	O
there	O
a	O
rule	B
for	O
which	O
no	O
error	O
estimate	O
works	O
for	O
all	O
distributions	O
more	O
specifically	O
is	O
there	O
a	O
sequence	O
of	O
classification	O
rules	O
gn	O
such	O
that	O
for	O
all	O
n	O
large	O
enough	O
infsupein	O
lni	O
c	O
in	O
xy	O
for	O
some	O
constant	O
c	O
where	O
the	O
infimum	O
is	O
taken	O
over	O
all	O
possible	O
error	O
estimates	O
are	O
such	O
rules	O
necessarily	O
inconsistent	O
problem	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
asymptotic	O
probability	O
of	O
error	O
of	O
the	O
nearest	B
neighbor	I
rule	B
lnn	O
x	O
x	O
show	O
that	O
for	O
every	O
n	O
for	O
any	O
estimate	O
ln	O
of	O
l	O
nn	O
and	O
for	O
every	O
e	O
there	O
exists	O
a	O
distribution	O
of	O
ex	O
y	O
such	O
that	O
the	O
regular	B
histogram	B
rule	B
in	O
this	O
chapter	O
we	O
study	O
the	O
cubic	B
histogram	B
rule	B
recall	O
that	O
this	O
rule	B
partitions	O
rd	O
into	O
cubes	O
of	O
the	O
same	O
size	O
and	O
gives	O
the	O
decision	O
according	O
to	O
the	O
number	O
of	O
zeros	O
and	O
ones	O
among	O
the	O
yis	O
such	O
that	O
the	O
corresponding	O
xi	O
falls	O
in	O
the	O
same	O
cube	O
as	O
x	O
pn	O
a	O
denotes	O
a	O
partition	B
of	O
rd	O
into	O
cubes	O
of	O
size	O
h	O
n	O
that	O
is	O
into	O
sets	O
of	O
the	O
type	O
lhn	O
where	O
the	O
kis	O
are	O
integers	O
and	O
the	O
histogram	B
rule	B
is	O
defined	O
by	O
where	O
for	O
every	O
x	O
e	O
rd	O
anx	O
ani	O
if	O
x	O
e	O
ani	O
that	O
is	O
the	O
decision	O
is	O
zero	O
if	O
the	O
number	O
of	O
ones	O
does	O
not	O
exceed	O
the	O
number	O
of	O
zeros	O
in	O
the	O
cell	O
in	O
which	O
x	O
falls	O
weak	B
universal	B
consistency	B
of	O
this	O
rule	B
was	O
shown	O
in	O
chapter	O
under	O
the	O
conditions	O
hn	O
and	O
nh	O
as	O
n	O
the	O
purpose	O
of	O
this	O
chapter	O
is	O
to	O
introduce	O
some	O
techniques	O
by	O
proving	O
strong	B
universal	B
consistency	B
of	O
this	O
rule	B
these	O
techniques	O
will	O
prove	O
very	O
useful	O
in	O
handling	O
other	O
problems	O
as	O
well	O
first	O
we	O
introduce	O
the	O
method	B
of	I
bounded	I
differences	I
the	O
method	B
of	I
bounded	I
differences	I
in	O
this	O
section	O
we	O
present	O
a	O
generalization	O
of	O
hoeffdings	O
inequality	B
due	O
to	O
mcdiarmid	O
the	O
result	O
will	O
equip	O
us	O
with	O
a	O
powerful	O
tool	O
to	O
handle	O
plicated	O
functions	O
of	O
independent	O
random	O
variables	O
this	O
inequality	B
follows	O
by	O
results	O
of	O
hoeffding	O
and	O
azuma	O
who	O
observed	O
that	O
theorem	O
the	O
regular	B
histogram	B
rule	B
can	O
be	O
generalized	B
to	O
bounded	O
martingale	B
difference	O
sequences	O
the	O
inequality	B
has	O
found	O
many	O
applications	O
in	O
combinatorics	O
as	O
well	O
as	O
in	O
nonparametric	O
statistics	O
mcdiarmid	O
and	O
devroye	O
for	O
surveys	O
let	O
us	O
first	O
recall	O
the	O
notion	O
of	O
martingales	O
consider	O
a	O
probability	O
space	O
f	O
p	O
definition	O
a	O
sequence	O
of	O
random	O
variables	O
z	O
is	O
called	O
a	O
martingale	B
if	O
e	O
zd	O
zi	O
with	O
probability	O
one	O
for	O
each	O
i	O
let	O
xl	O
x	O
be	O
an	O
arbitrary	O
sequence	O
of	O
random	O
variables	O
zl	O
is	O
called	O
a	O
martingale	B
with	O
respect	O
to	O
the	O
sequence	O
xl	O
x	O
iffor	O
every	O
i	O
zi	O
is	O
afunction	O
of	O
xl	O
xi	O
and	O
eziiixi	O
xd	O
zi	O
with	O
probability	O
one	O
obviously	O
if	O
zl	O
is	O
a	O
martingale	B
with	O
respect	O
to	O
xl	O
then	O
zl	O
is	O
a	O
martingale	B
since	O
e	O
zd	O
e	O
xd	O
zi	O
zd	O
eziizl	O
the	O
most	O
important	O
examples	O
of	O
martingales	O
are	O
sums	O
of	O
independent	O
mean	O
random	O
variables	O
let	O
be	O
independent	O
random	O
variables	O
with	O
zero	O
mean	O
then	O
the	O
random	O
variables	O
i	O
si	O
l	O
uj	O
jl	O
i	O
form	O
a	O
martingale	B
problem	O
martingales	O
share	O
many	O
properties	O
of	O
sums	O
of	O
independent	O
variables	O
our	O
purpose	O
here	O
is	O
to	O
extend	O
hoeffdings	O
inequality	B
to	O
martingales	O
the	O
role	O
of	O
the	O
independent	O
random	O
variables	O
is	O
played	O
here	O
by	O
a	O
so-called	O
martingale	B
difference	I
sequence	I
definition	O
a	O
sequence	O
of	O
random	O
variables	O
vi	O
is	O
a	O
martingale	B
ference	O
sequence	O
if	O
e	O
vi	O
vi	O
with	O
probability	O
one	O
for	O
every	O
i	O
a	O
sequence	O
of	O
random	O
variables	O
vi	O
is	O
called	O
a	O
martingale	B
difference	I
sequence	I
with	O
respect	O
to	O
a	O
sequence	O
of	O
random	O
variables	O
xl	O
x	O
iffor	O
every	O
i	O
vi	O
is	O
a	O
function	O
of	O
xl	O
xi	O
and	O
e	O
ix	O
xd	O
with	O
probability	O
one	O
the	O
method	B
of	I
bounded	I
differences	I
again	O
it	O
is	O
easily	O
seen	O
that	O
if	O
vi	O
is	O
a	O
martingale	B
difference	I
sequence	I
with	O
respect	O
to	O
a	O
sequence	O
xl	O
x	O
of	O
random	O
variables	O
then	O
it	O
is	O
a	O
martingale	B
ference	O
sequence	O
also	O
any	O
martingale	B
zl	O
leads	O
naturally	O
to	O
a	O
martingale	B
difference	I
sequence	I
by	O
defining	O
for	O
i	O
o	O
the	O
key	O
result	O
in	O
the	O
method	B
of	I
bounded	I
differences	I
is	O
the	O
following	O
inequality	B
that	O
relaxes	O
the	O
independence	O
assumption	O
in	O
theorem	O
allowing	O
martingale	B
difference	O
sequences	O
theorem	O
azuma	O
let	O
xl	O
x	O
be	O
a	O
sequence	O
of	O
random	O
variables	O
and	O
assume	O
that	O
vi	O
is	O
a	O
martingale	B
difference	O
quence	O
with	O
respect	O
to	O
x	O
i	O
x	O
assume	O
furthermore	O
that	O
there	O
exist	O
random	O
variables	O
z	O
and	O
nonnegative	O
constants	O
such	O
that	O
for	O
every	O
i	O
zi	O
is	O
afunction	O
of	O
xl	O
xi-i	O
and	O
then	O
for	O
any	O
e	O
and	O
n	O
and	O
the	O
proof	O
is	O
a	O
rather	O
straightforward	O
extension	O
of	O
that	O
of	O
hoeffding	O
s	O
inequality	B
first	O
we	O
need	O
an	O
analog	O
of	O
lemma	O
lemma	O
assume	O
that	O
the	O
random	O
variables	O
v	O
and	O
z	O
satisfy	O
with	O
probability	O
one	O
that	O
e	O
v	O
i	O
z	O
and	O
for	O
some	O
function	O
f	O
and	O
constant	O
c	O
fez	O
v	O
fez	O
c	O
then	O
for	O
every	O
s	O
the	O
proof	O
of	O
the	O
lemma	O
is	O
left	O
as	O
an	O
exercise	O
proof	O
of	O
theorem	O
as	O
in	O
the	O
proof	O
of	O
hoeffding	O
s	O
inequality	B
we	O
proceed	O
by	O
chernoffs	O
bounding	O
method	O
set	O
sk	O
then	O
for	O
any	O
s	O
ps	O
n	O
e	O
e-see	O
ssn	O
the	O
rygular	O
histogram	B
rule	B
e-se	O
ct	O
e	O
e	O
l-il	O
ci	O
previous	O
argument	O
s	O
c	O
lzl	O
z	O
n	O
the	O
second	O
inequality	B
is	O
proved	O
analogously	O
now	O
we	O
are	O
ready	O
to	O
state	O
the	O
main	O
inequality	B
of	O
this	O
section	O
it	O
is	O
a	O
large	O
deviation-type	O
inequality	B
for	O
functions	O
of	O
independent	O
random	O
variables	O
such	O
that	O
the	O
function	O
is	O
relatively	O
robust	O
to	O
individual	O
changes	O
in	O
the	O
values	O
of	O
the	O
random	O
variables	O
the	O
condition	O
of	O
the	O
function	O
requires	O
that	O
by	O
changing	O
the	O
value	O
of	O
its	O
i	O
variable	B
the	O
value	O
of	O
the	O
function	O
cannot	O
change	O
by	O
more	O
than	O
a	O
constant	O
ci	O
theorem	O
let	O
xl	O
xn	O
be	O
independent	O
random	O
riables	O
taking	O
values	O
in	O
a	O
set	O
a	O
and	O
assume	O
that	O
f	O
an	O
r	O
satisfies	O
sup	O
ifxl	O
xn	O
fxi	O
xi-i	O
x	O
ci	O
i	O
n	O
xl	O
xea	O
then	O
for	O
all	O
e	O
xn	O
efx	O
xn	O
e	O
e-	O
and	O
p	O
x	O
n	O
fx	O
x	O
n	O
e	O
e	O
cf	O
proof	O
define	O
v	O
fx	O
xn-efxi	O
xn	O
introduce	O
vi	O
ev	O
and	O
for	O
k	O
so	O
that	O
v	O
ll	O
vk	O
clearly	O
vi	O
vn	O
form	O
a	O
martingale	B
difference	I
sequence	I
with	O
respect	O
to	O
xl	O
x	O
n	O
define	O
the	O
random	O
variables	O
and	O
vk	O
hkx	O
i	O
x	O
k	O
f	O
hkx	O
x	O
k-	O
i	O
xfkdx	O
where	O
the	O
integration	O
is	O
with	O
respect	O
to	O
fk	O
the	O
probability	O
measure	O
of	O
x	O
k	O
introduce	O
the	O
random	O
variables	O
wk	O
sp	O
hkx	O
i	O
xk-i	O
u	O
f	O
hkx	O
xk-j	O
xfkdx	O
and	O
the	O
method	B
of	I
bounded	I
differences	I
clearly	O
zk	O
vk	O
wk	O
with	O
probability	O
one	O
since	O
for	O
every	O
k	O
zk	O
is	O
a	O
function	O
of	O
xl	O
xk-l	O
we	O
can	O
apply	O
theorem	O
directly	O
to	O
v	O
vk	O
if	O
we	O
can	O
show	O
that	O
wk	O
zk	O
ck	O
but	O
this	O
follows	O
from	O
wk	O
zk	O
sup	O
sup	O
xk-iu	O
hkx	O
i	O
xk-l	O
v	O
u	O
by	O
the	O
condition	O
of	O
the	O
theorem	O
clearly	O
if	O
the	O
xis	O
are	O
bounded	O
then	O
the	O
choice	O
ixl	O
xn	O
xi	O
yields	O
hoeffdings	O
inequality	B
many	O
times	O
the	O
inequality	B
can	O
be	O
used	O
to	O
handle	O
very	O
complicated	O
functions	O
of	O
independent	O
random	O
variables	O
with	O
great	O
elegance	O
for	O
examples	O
in	O
nonparametric	O
statistics	O
see	O
problems	O
similar	O
methods	O
to	O
those	O
used	O
in	O
the	O
proof	O
of	O
theorem	O
may	O
be	O
used	O
to	O
bound	O
the	O
variance	O
xn	O
other	O
inequalities	O
for	O
the	O
variance	B
of	I
general	O
functions	O
of	O
independent	O
random	O
variables	O
were	O
derived	O
by	O
efron	O
and	O
stein	O
and	O
steele	O
theorem	O
assume	O
that	O
the	O
conditions	O
of	O
theorem	O
hold	O
then	O
proof	O
using	O
the	O
notations	O
of	O
the	O
proof	O
of	O
theorem	O
we	O
have	O
to	O
show	O
that	O
varv	O
z	O
cf	O
n	O
il	O
observe	O
that	O
varv	O
e	O
where	O
in	O
the	O
last	O
step	O
we	O
used	O
the	O
martingale	B
property	O
in	O
the	O
following	O
way	O
for	O
i	O
j	O
we	O
have	O
with	O
probability	O
one	O
the	O
regular	B
histogram	B
rule	B
thus	O
the	O
theorem	O
follows	O
if	O
we	O
can	O
show	O
that	O
introducing	O
wi	O
and	O
zi	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
we	O
see	O
that	O
with	O
bility	O
one	O
zi	O
s	O
vi	O
s	O
zi	O
ci	O
since	O
zi	O
is	O
a	O
function	O
of	O
xl	O
xi	O
therefore	O
conditioned	O
on	O
xl	O
xi-i	O
is	O
a	O
zero	O
mean	O
random	O
variable	B
taking	O
values	O
in	O
the	O
interval	O
zi	O
cd	O
but	O
an	O
arbitrary	O
random	O
variable	B
u	O
taking	O
values	O
in	O
an	O
interval	O
b	O
has	O
variance	O
not	O
exceeding	O
so	O
that	O
cf	O
evi	O
ixi	O
xi-d	O
s	O
which	O
concludes	O
the	O
proof	O
strong	B
universal	B
consistency	B
the	O
purpose	O
of	O
this	O
section	O
is	O
to	O
prove	O
strong	B
universal	B
consistency	B
of	O
the	O
histogram	B
rule	B
this	O
is	O
the	O
first	O
such	O
result	O
that	O
we	O
mention	O
later	O
we	O
will	O
prove	O
the	O
same	O
property	O
for	O
other	O
rules	O
too	O
the	O
theorem	O
stated	O
here	O
for	O
cubic	B
partitions	O
is	O
essentially	O
due	O
to	O
devroye	O
and	O
gyorfi	O
for	O
more	O
general	O
sequences	O
of	O
partitions	O
see	O
problem	O
an	O
alternative	O
proof	O
of	O
the	O
theorem	O
based	O
on	O
the	O
vapnik-chervonenkis	B
inequality	B
will	O
be	O
given	O
later-see	O
the	O
remark	O
following	O
theorem	O
theorem	O
assume	O
that	O
the	O
sequence	O
of	O
partitions	O
satisfies	O
the	O
following	O
two	O
conditions	O
as	O
n	O
and	O
nh	O
for	O
any	O
distribution	O
of	O
y	O
andfor	O
every	O
e	O
there	O
is	O
an	O
integer	O
no	O
such	O
that	O
for	O
n	O
no	O
for	O
the	O
error	O
probability	O
ln	O
of	O
the	O
histogram	B
rule	B
pln	O
l	O
e	O
s	O
thus	O
the	O
cubic	B
histogram	B
rule	B
is	O
strongly	O
universally	O
consistent	O
proof	O
define	O
y	O
lcl	O
c	O
ry	O
x	O
n	O
nflanx	O
clearly	O
the	O
decision	O
based	O
on	O
strong	B
universal	B
consistency	B
is	O
just	O
the	O
histogram	B
rule	B
therefore	O
by	O
theorem	O
it	O
suffices	O
to	O
prove	O
that	O
for	O
n	O
large	O
enough	O
p	O
iryx	O
ryxijldx	O
decompose	O
the	O
difference	O
as	O
the	O
convergence	O
of	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
implies	O
weak	B
consistency	B
of	O
the	O
histogram	B
rule	B
the	O
technique	O
we	O
use	O
to	O
bound	O
this	O
term	O
is	O
similar	O
to	O
that	O
which	O
we	O
already	O
saw	O
in	O
the	O
proof	O
of	O
theorem	O
for	O
completeness	O
we	O
give	O
the	O
details	O
here	O
however	O
new	O
ideas	O
have	O
to	O
appear	O
in	O
our	O
handling	O
of	O
the	O
second	O
term	O
we	O
begin	O
with	O
the	O
first	O
term	O
since	O
the	O
set	O
of	O
continuous	O
functions	O
with	O
bounded	O
support	B
is	O
dense	O
in	O
l	O
it	O
is	O
possible	O
to	O
find	O
a	O
continuous	O
function	O
of	O
bounded	O
support	B
rx	O
such	O
that	O
f	O
rxif	O
ldx	O
note	O
that	O
rx	O
is	O
uniformly	O
continuous	O
introduce	O
the	O
function	O
rx	O
e	O
n	O
f	O
lanx	O
then	O
we	O
can	O
further	O
decompose	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
as	O
irx	O
rx	O
i	O
irx	O
we	O
proceed	O
term	O
by	O
term	O
first	O
term	O
the	O
integral	O
of	O
by	O
the	O
definition	B
of	I
rx	O
respect	O
to	O
f	O
l	O
is	O
smaller	O
than	O
second	O
term	O
using	O
fubinis	O
theorem	O
we	O
have	O
e	O
i	O
f	O
ldx	O
f	O
l	O
j	O
rxif	O
ldx	O
f	O
irx	O
l	O
ahiaj	O
o	O
j	O
aj	O
the	O
regular	B
histogram	B
rule	B
as	O
rex	O
is	O
uniformly	O
continuous	O
if	O
hn	O
is	O
small	O
enough	O
then	O
ry	O
for	O
every	O
x	O
yea	O
for	O
any	O
cell	O
a	O
e	O
pn	O
then	O
the	O
double	O
integral	O
in	O
the	O
above	O
expression	B
can	O
be	O
bounded	O
from	O
above	O
as	O
follows	O
l	O
l	O
irx	O
ryllldxlldy	O
j	O
note	O
that	O
we	O
used	O
the	O
condition	O
hn	O
here	O
summing	O
over	O
the	O
cells	O
we	O
get	O
f	O
irx	O
rzxllldx	O
third	O
term	O
we	O
have	O
f	O
l	O
ie	O
j	O
y	O
ixea	O
j	O
i	O
ajepn	O
a	O
ili	O
li	O
f	O
irx	O
fourth	O
term	O
our	O
aim	O
is	O
to	O
show	O
that	O
for	O
n	O
large	O
enough	O
e	O
f	O
to	O
this	O
end	O
let	O
s	O
be	O
an	O
arbitrary	O
large	O
ball	O
centered	O
at	O
the	O
origin	O
denote	O
by	O
mn	O
the	O
number	O
of	O
cells	O
of	O
the	O
partition	B
pn	O
that	O
intersect	O
s	O
clearly	O
mn	O
is	O
proportional	O
to	O
h	O
as	O
hn	O
o	O
using	O
the	O
notation	O
vna	O
iyilxea	O
it	O
is	O
clear	O
that	O
vna	O
fa	O
now	O
we	O
can	O
write	O
e	O
f	O
e	O
l	O
e	O
l	O
ievnanj	O
j	O
strong	B
universal	B
consistency	B
e	O
l	O
ievnanj	O
ja	O
sc	O
denotes	O
the	O
complement	O
of	O
s	O
l	O
je	O
ievnanj	O
ja	O
the	O
cauchy-schwarz	B
inequality	B
mn	O
l	O
j	O
mn	O
ja	O
n	O
mn	O
l	O
mn	O
j	O
a	O
n	O
n	O
tlan	O
jensens	O
inequality	B
if	O
n	O
and	O
the	O
radius	O
of	O
s	O
are	O
large	O
enough	O
since	O
mnn	O
converges	O
to	O
zero	O
by	O
the	O
condition	O
nh	O
and	O
tlsc	O
can	O
be	O
made	O
arbitrarily	O
small	O
by	O
choice	O
of	O
s	O
we	O
have	O
proved	O
for	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
that	O
for	O
n	O
large	O
enough	O
e	O
f	O
finally	O
we	O
handle	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
by	O
obtaining	O
an	O
exponential	B
bound	O
for	O
f	O
e	O
f	O
using	O
theorem	O
fix	O
the	O
training	O
data	O
yl	O
yn	O
e	O
rd	O
x	O
and	O
replace	O
yi	O
by	O
yi	O
changing	O
the	O
value	O
of	O
to	O
then	O
differs	O
from	O
zero	O
only	O
on	O
anxi	O
and	O
anx	O
and	O
thus	O
f	O
f	O
f	O
lxltldx	O
the	O
regular	B
histogram	B
rule	B
so	O
by	O
theorem	O
for	O
sufficiently	O
large	O
n	O
p	O
iryx	O
ryxijldx	O
p	O
iryx	O
ryxijldx	O
e	O
f	O
iryx	O
e-n	O
ryxijldx	O
remark	O
strong	B
universal	B
consistency	B
follows	O
from	O
the	O
exponential	B
bound	O
on	O
the	O
probability	O
pln	O
l	O
e	O
via	O
the	O
borel-cantelli	B
lemma	I
the	O
inequality	B
in	O
theorem	O
may	O
seem	O
universal	O
in	O
nature	O
however	O
it	O
is	O
distribution-dependent	O
in	O
a	O
surreptitious	O
way	O
because	O
its	O
range	O
of	O
validity	O
n	O
no	O
depends	O
heavily	O
on	O
e	O
h	O
n	O
and	O
the	O
distribution	O
we	O
know	O
that	O
distribution-free	O
upper	O
bounds	O
could	O
not	O
exist	O
anyway	O
in	O
view	O
of	O
theorem	O
problems	O
and	O
exercises	O
problem	O
let	O
vi	O
be	O
independent	O
random	O
variables	O
with	O
zero	O
mean	O
show	O
that	O
the	O
random	O
variables	O
si	O
vj	O
i	O
form	O
a	O
martingale	B
problem	O
prove	O
lemma	O
problem	O
let	O
xl	O
x	O
n	O
be	O
real	O
valued	O
i	O
i	O
d	O
random	O
variables	O
with	O
distribution	O
tion	O
f	O
and	O
corresponding	O
empirical	B
distribution	B
function	I
fn	O
denote	O
the	O
kolmogorov-smirnov	B
statistic	I
by	O
vn	O
sup	O
ifnx	O
fxl	O
xer	O
use	O
theorem	O
to	O
show	O
that	O
compare	O
this	O
result	O
with	O
theorem	O
of	O
them	O
implies	O
the	O
other	O
also	O
consider	O
a	O
class	O
a	O
of	O
subsets	O
of	O
nd	O
let	O
zl	O
zn	O
be	O
i	O
i	O
d	O
random	O
variables	O
in	O
n	O
d	O
with	O
common	O
distribution	O
pzl	O
e	O
a	O
va	O
and	O
consider	O
the	O
random	O
variable	B
wn	O
sup	O
ivna	O
vai	O
aea	O
where	O
vnca	O
denotes	O
the	O
standard	B
empirical	B
measure	I
of	O
a	O
prove	O
that	O
compare	O
this	O
result	O
with	O
theorem	O
and	O
note	O
that	O
this	O
result	O
is	O
true	O
even	O
if	O
sea	O
n	O
for	O
all	O
n	O
problems	O
and	O
exercises	O
problem	O
the	O
lazy	B
histogram	B
rule	B
let	O
pn	O
be	O
a	O
sequence	O
of	O
tions	O
satisfying	O
the	O
conditions	O
of	O
the	O
convergence	O
theorem	O
define	O
the	O
lazy	B
histogram	B
rule	B
as	O
follows	O
gnx	O
yj	O
x	O
e	O
ani	O
where	O
xj	O
is	O
the	O
minimum-index	O
point	O
among	O
xl	O
xn	O
for	O
which	O
xj	O
e	O
ani	O
in	O
other	O
words	O
we	O
ignore	O
all	O
but	O
one	O
point	O
in	O
each	O
set	O
of	O
the	O
partition	B
if	O
ln	O
is	O
the	O
conditional	O
probability	O
of	O
error	O
for	O
the	O
lazy	B
histogram	B
rule	B
then	O
show	O
that	O
for	O
any	O
distribution	O
of	O
y	O
lim	O
sup	O
eln	O
n-oo	O
problem	O
assume	O
that	O
pn	O
p	O
a	O
k	O
is	O
a	O
fixed	O
partition	B
into	O
k	O
sets	O
consider	O
the	O
lazy	B
histogram	B
rule	B
defined	O
in	O
problem	O
based	O
on	O
p	O
show	O
that	O
for	O
all	O
distributions	O
of	O
y	O
limn-	O
oo	O
eln	O
exists	O
and	O
satisfies	O
lim	O
eln	O
pipaj	O
n-oo	O
k	O
l	O
ii	O
where	O
fl	O
is	O
the	O
probability	O
measure	O
for	O
x	O
and	O
pi	O
fa	O
show	O
that	O
the	O
limit	O
of	O
the	O
probability	O
of	O
error	O
el	O
for	O
the	O
ordinary	B
histogram	B
rule	B
is	O
lim	O
el	O
minpi	O
i	O
pi	O
n-oo	O
k	O
l	O
ii	O
and	O
show	O
that	O
lim	O
eln	O
lim	O
el	O
n-oo	O
n-oo	O
problem	O
histogram	B
density	B
estimation	B
let	O
xl	O
xn	O
be	O
i	O
i	O
d	O
random	O
variables	O
in	O
n	O
d	O
with	O
density	O
f	O
let	O
p	O
be	O
a	O
partition	B
of	O
n	O
d	O
and	O
define	O
the	O
histogram	O
density	O
estimate	O
by	O
n	O
fnx	O
naax	O
ixeax	O
where	O
ax	O
is	O
the	O
set	O
in	O
p	O
that	O
contains	O
x	O
and	O
a	O
is	O
the	O
lebesgue	O
measure	O
prove	O
for	O
the	O
l	O
of	O
the	O
estimate	O
that	O
p	O
iflx	O
fxldx	O
e	O
f	O
ifnx	O
fxdxi	O
e	O
conclude	O
that	O
weak	B
of	O
the	O
estimate	O
implies	O
strong	O
sistency	O
see	O
also	O
problem	O
problem	O
general	O
partitions	O
extend	O
the	O
consistency	B
result	O
of	O
theorem	O
for	O
quences	O
of	O
general	O
not	O
necessarily	O
cubic	B
partitions	O
actually	O
cells	O
of	O
the	O
partitions	O
need	O
not	O
even	O
be	O
hyperrectangles	O
assume	O
that	O
the	O
sequence	O
of	O
partitions	O
satisfies	O
the	O
following	O
two	O
conditions	O
for	O
every	O
ball	O
s	O
centered	O
at	O
the	O
origin	O
and	O
lim	O
max	O
ilx	O
yii	O
n-oo	O
xyean	O
lim	O
ani	O
n	O
s	O
o	O
n-oo	O
n	O
prove	O
that	O
the	O
corresponding	O
histogram	O
classification	O
rule	B
is	O
strongly	O
universally	O
consistent	O
the	O
regular	B
histogram	B
rule	B
problem	O
show	O
that	O
for	O
cubic	B
histograms	O
the	O
conditions	O
of	O
problem	O
on	O
the	O
partition	B
are	O
equivalent	O
to	O
the	O
conditions	O
hn	O
and	O
nh	O
respectively	O
problem	O
linear	O
scaling	O
partition	B
n	O
d	O
into	O
congruent	O
rectangles	O
of	O
the	O
form	O
where	O
kl	O
are	O
integers	O
and	O
hi	O
denote	O
the	O
size	O
of	O
the	O
edges	O
of	O
the	O
if	O
hi	O
for	O
every	O
i	O
d	O
and	O
nh	O
hd	O
as	O
n	O
hint	O
this	O
is	O
a	O
corollary	O
of	O
problem	O
problem	O
nonlinear	O
scaling	O
let	O
fi	O
fd	O
n	O
n	O
be	O
invertible	O
strictly	O
tone	O
increasing	O
functions	O
consider	O
the	O
partition	B
of	O
n	O
d	O
whose	O
cells	O
are	O
rectangles	O
of	O
the	O
form	O
rectangles	O
prove	O
that	O
the	O
corresponding	O
histogram	B
rule	B
is	O
strongly	O
universally	O
consistent	O
fi-ikl	O
l	O
x	O
x	O
fkd	O
lhd	O
problem	O
prove	O
that	O
the	O
histogram	B
rule	B
corresponding	O
to	O
this	O
partition	B
is	O
strongly	O
universally	O
consistent	O
under	O
the	O
conditions	O
of	O
problem	O
hint	O
use	O
problem	O
problem	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
the	O
bias	B
a	O
sequence	O
of	O
titions	O
is	O
called	O
if	O
for	O
every	O
measurable	O
set	O
a	O
for	O
every	O
e	O
and	O
for	O
all	O
sufficiently	O
large	O
n	O
there	O
is	O
a	O
set	O
an	O
e	O
apn	O
denotes	O
the	O
a-algebra	O
generated	O
by	O
cells	O
of	O
the	O
partition	B
p	O
such	O
that	O
e	O
prove	O
that	O
the	O
bias	B
term	O
f	O
converges	O
to	O
zero	O
for	O
all	O
distributions	O
of	O
y	O
if	O
and	O
only	O
if	O
the	O
sequence	O
of	O
partitions	O
is	O
for	O
every	O
probability	O
measure	O
on	O
n	O
d	O
conclude	O
that	O
the	O
first	O
condition	O
of	O
problem	O
implies	O
that	O
is	O
for	O
every	O
probability	O
measure	O
problem	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
the	O
variation	B
assume	O
that	O
for	O
every	O
probability	O
measure	O
fj-	O
on	O
n	O
d	O
every	O
measurable	O
set	O
a	O
every	O
c	O
and	O
every	O
e	O
there	O
is	O
an	O
ne	O
c	O
a	O
such	O
that	O
for	O
all	O
n	O
ne	O
c	O
a	O
l	O
n	O
a	O
e	O
nasc	O
n	O
prove	O
that	O
the	O
variation	B
term	O
f	O
converges	O
to	O
zero	O
for	O
all	O
distributions	O
of	O
ex	O
y	O
if	O
and	O
only	O
if	O
the	O
sequence	O
of	O
partitions	O
satisfies	O
the	O
condition	O
above	O
jaoude	O
problem	O
the	O
e-effective	O
cardinality	O
mp	O
ilv	O
a	O
e	O
of	O
a	O
partition	B
with	O
respect	O
to	O
the	O
probability	O
measure	O
restricted	O
to	O
a	O
set	O
a	O
is	O
the	O
minimum	O
number	O
of	O
sets	O
in	O
p	O
such	O
that	O
the	O
union	O
of	O
the	O
remaining	O
sets	O
intersected	O
with	O
a	O
has	O
less	O
than	O
e	O
prove	O
that	O
the	O
sequence	O
of	O
partitions	O
satisfies	O
the	O
condition	O
of	O
problem	O
if	O
and	O
only	O
if	O
for	O
every	O
e	O
mpn	O
a	O
e	O
n	O
gyorfi	O
and	O
van	O
der	O
meulen	O
problem	O
in	O
n	O
partition	B
the	O
plane	O
by	O
taking	O
three	O
fixed	O
points	O
not	O
on	O
a	O
line	O
x	O
y	O
and	O
z	O
at	O
each	O
of	O
these	O
points	O
partition	B
n	O
by	O
considering	O
k	O
equal	O
sectors	O
of	O
angle	O
k	O
each	O
sets	O
in	O
the	O
histogram	O
partition	B
are	O
obtained	O
as	O
intersections	O
of	O
cones	O
is	O
the	O
induced	O
histogram	B
rule	B
strongly	O
universally	O
consistent	O
if	O
yes	O
state	O
the	O
conditions	O
on	O
k	O
and	O
if	O
no	O
provide	O
a	O
counterexample	O
problems	O
and	O
exercises	O
problem	O
partition	B
n	O
into	O
shells	O
of	O
size	O
h	O
each	O
the	O
i-th	O
shell	O
contains	O
all	O
points	O
at	O
lh	O
ih	O
from	O
the	O
origin	O
let	O
h	O
and	O
nh	O
as	O
n	O
consider	O
distance	O
d	O
e	O
the	O
histogram	B
rule	B
as	O
n	O
to	O
what	O
does	O
eln	O
converge	O
kernel	B
rules	O
histogram	O
rules	O
have	O
the	O
somewhat	O
undesirable	O
property	O
that	O
the	O
rule	B
is	O
less	O
curate	O
at	O
borders	O
of	O
cells	O
of	O
the	O
partition	B
than	O
in	O
the	O
middle	O
of	O
cells	O
looked	O
at	O
intuitively	O
this	O
is	O
because	O
points	O
near	O
the	O
border	O
of	O
a	O
cell	O
should	O
have	O
less	O
weight	O
in	O
a	O
decision	O
regarding	O
the	O
cells	O
center	O
to	O
remedy	O
this	O
problem	O
one	O
might	O
troduce	O
the	O
moving	B
window	I
rule	B
which	O
is	O
smoother	O
than	O
the	O
histogram	B
rule	B
this	O
classifier	B
simply	O
takes	O
the	O
data	O
points	O
within	O
a	O
certain	O
distance	O
of	O
the	O
point	O
to	O
be	O
classified	O
and	O
decides	O
according	O
to	O
majority	B
vote	I
working	O
formally	O
let	O
h	O
be	O
a	O
positive	O
number	O
then	O
the	O
moving	B
window	I
rule	B
is	O
defined	O
as	O
gnx	O
if	O
iyioxiesxh	O
iyi	O
esxh	O
otherwise	O
where	O
sxh	O
denotes	O
the	O
closed	O
ball	O
of	O
radius	O
h	O
centered	O
at	O
x	O
it	O
is	O
possible	O
to	O
make	O
the	O
decision	O
even	O
smoother	O
by	O
giving	O
more	O
weight	O
to	O
closer	O
points	O
than	O
to	O
more	O
distant	O
ones	O
let	O
k	O
rd	O
r	O
be	O
a	O
kernel	B
junction	O
which	O
is	O
usually	O
nonnegative	O
and	O
monotone	O
decreasing	O
along	O
rays	O
starting	O
from	O
the	O
origin	O
the	O
kernel	B
classification	O
rule	B
is	O
given	O
by	O
gnx	O
i	O
l	O
il	O
otherwise	O
k	O
i	O
l	O
il	O
k	O
kernel	B
rules	O
figure	O
the	O
moving	O
dow	O
rule	B
in	O
the	O
decision	O
is	O
in	O
the	O
shaded	O
area	O
g	O
o	O
class	O
class	O
the	O
number	O
h	O
is	O
called	O
the	O
smoothing	B
factor	I
or	O
bandwidth	O
it	O
provides	O
some	O
form	O
of	O
distance	O
weighting	O
figure	O
kernel	B
rule	B
on	O
the	O
real	O
line	O
the	O
figure	O
shows	O
xi	O
hfor	O
n	O
ku	O
u	O
epanechnikov	O
kernel	B
and	O
three	O
smoothing	O
factors	O
h	O
one	O
definitely	O
undersmooths	O
and	O
one	O
oversmooths	O
we	O
took	O
p	O
and	O
the	O
class-cconditional	O
ties	O
are	O
fox	O
x	O
and	O
flex	O
on	O
clearly	O
the	O
kernel	B
rule	B
is	O
a	O
generalization	O
of	O
the	O
moving	B
window	I
rule	B
since	O
taking	O
the	O
special	O
kernel	B
k	O
ixesod	O
yields	O
the	O
moving	B
window	I
rule	B
this	O
kernel	B
is	O
sometimes	O
called	O
the	O
naive	B
kernel	B
other	O
popular	O
kernels	O
include	O
the	O
sian	O
kernel	B
kx	O
e-	O
the	O
cauchy	B
kernel	B
kx	O
iixll	O
dl	O
and	O
the	O
epanechnikov	O
kernel	B
kx	O
distance	O
where	O
ii	O
ii	O
denotes	O
euclidean	B
consistency	B
gaussian	B
kernel	B
cauchy	B
kernel	B
epanechnikov	O
kernel	B
uniform	O
kernel	B
o	O
figure	O
various	O
kernels	O
on	O
r	O
kernel-based	O
rules	O
are	O
derived	O
from	O
the	O
kernel	B
estimate	O
in	O
density	B
estimation	B
originally	O
studied	O
by	O
parzen	O
rosenblatt	O
akaike	O
and	O
coullos	O
problems	O
and	O
and	O
in	O
regression	O
estimation	B
troduced	O
by	O
nadaraya	O
and	O
watson	O
for	O
particular	O
choices	O
of	O
k	O
rules	O
of	O
this	O
sort	O
have	O
been	O
proposed	O
by	O
fix	O
and	O
hodges	O
sebestyen	O
van	O
ryzin	O
and	O
meisel	O
statistical	O
analysis	O
of	O
these	O
rules	O
andor	O
the	O
corresponding	O
regression	B
function	I
estimate	O
can	O
be	O
found	O
in	O
nadaraya	O
rejto	O
and	O
revesz	O
devroye	O
and	O
wagner	O
greblicki	O
krzyzak	O
and	O
pawlak	O
and	O
devroye	O
and	O
krzyzak	O
usage	O
of	O
cauchy	B
kernels	O
in	O
discrimination	O
is	O
investigated	O
by	O
arkadjew	O
and	O
braverman	O
hand	O
and	O
coomans	O
and	O
broeckaert	O
consistency	B
in	O
this	O
section	O
we	O
demonstrate	O
strong	B
universal	B
consistency	B
of	O
kernel-based	O
rules	O
under	O
general	O
conditions	O
on	O
hand	O
k	O
let	O
h	O
be	O
a	O
smoothing	B
factor	I
depending	O
only	O
on	O
n	O
and	O
let	O
k	O
be	O
a	O
kernel	B
function	O
if	O
the	O
conditional	O
densities	O
fo	O
exist	O
then	O
weak	B
and	O
strong	B
consistency	B
follow	O
from	O
problems	O
and	O
tively	O
via	O
problem	O
we	O
state	O
the	O
universal	B
consistency	B
theorem	O
for	O
a	O
large	O
class	O
of	O
kernel	B
functions	O
namely	O
for	O
all	O
regular	B
kernels	O
definition	O
the	O
kernel	B
k	O
is	O
called	O
regular	B
if	O
it	O
is	O
nonnegative	O
and	O
there	O
is	O
a	O
ball	O
sor	O
of	O
radius	O
r	O
centered	O
at	O
the	O
origin	O
and	O
constant	O
b	O
such	O
that	O
kx	O
bisor	O
and	O
f	O
supyexsor	O
kydx	O
we	O
provide	O
three	O
informative	O
exercises	O
on	O
regular	B
kernels	O
in	O
all	O
cases	O
regular	B
kernels	O
are	O
bounded	O
and	O
integrable	O
the	O
last	O
condition	O
holds	O
whenever	O
k	O
is	O
integrable	O
and	O
uniformly	O
continuous	O
introduce	O
the	O
short	O
notation	O
khx	O
kkv	O
the	O
next	O
theorem	O
states	O
strong	O
universal	O
sistency	O
of	B
kernel	B
rules	I
the	O
theorem	O
is	O
essentially	O
due	O
to	O
devroye	O
and	O
krzyzak	O
kernel	B
rules	O
under	O
the	O
assumption	O
that	O
x	O
has	O
a	O
density	O
it	O
was	O
proven	O
by	O
devroye	O
and	O
gyorfi	O
and	O
zhao	O
theorem	O
and	O
krzyzak	O
assume	O
that	O
k	O
is	O
a	O
regular	B
kernel	B
lf	O
h	O
and	O
nhd	O
as	O
n	O
thenfor	O
any	O
distribution	O
of	O
y	O
andfor	O
every	O
e	O
there	O
is	O
an	O
integer	O
no	O
such	O
that	O
for	O
n	O
no	O
for	O
the	O
error	O
probability	O
ln	O
of	O
the	O
kernel	B
rule	B
pln	O
l	O
e	O
where	O
the	O
constant	O
p	O
depends	O
on	O
the	O
kernel	B
k	O
and	O
the	O
dimension	B
only	O
thus	O
the	O
kernel	B
rule	B
is	O
strongly	O
universally	O
consistent	O
clearly	O
naive	B
kernels	O
are	O
regular	B
and	O
moving	O
window	O
rules	O
are	O
thus	O
strongly	O
universally	O
consistent	O
for	O
the	O
sake	O
of	O
readability	O
we	O
give	O
the	O
proof	O
for	O
this	O
special	O
case	O
only	O
and	O
leave	O
the	O
extension	O
to	O
regular	B
kernels	O
to	O
the	O
reader-see	O
problems	O
and	O
before	O
we	O
embark	O
on	O
the	O
proof	O
in	O
the	O
next	O
section	O
we	O
should	O
warn	O
the	O
reader	O
that	O
theorem	O
is	O
of	O
no	O
help	O
whatsoever	O
regarding	O
the	O
choice	O
of	O
k	O
or	O
h	O
one	O
possible	O
solution	O
is	O
to	O
derive	O
explicit	O
upper	O
bounds	O
for	O
the	O
probability	O
of	O
error	O
as	O
a	O
function	O
of	O
descriptors	O
of	O
the	O
distribution	O
of	O
y	O
and	O
of	O
k	O
nand	O
h	O
minimizing	O
such	O
bounds	O
with	O
respect	O
to	O
k	O
and	O
h	O
will	O
lead	O
to	O
some	O
expedient	O
choices	O
typically	O
such	O
bounds	O
would	O
be	O
based	O
upon	O
the	O
inequality	B
eln	O
l	O
e	O
p	O
ox	O
pno	O
noxldx	O
f	O
ip	O
lx	O
pnjinlxldx	O
chapter	O
where	O
fo	O
are	O
the	O
class	O
densities	O
fno	O
are	O
their	O
kernel	B
mates	O
problem	O
p	O
and	O
p	O
are	O
the	O
class	O
probabilities	O
and	O
pno	O
pnl	O
are	O
their	O
relative-frequency	O
estimates	O
bounds	O
for	O
the	O
expected	O
in	O
sity	O
estimation	B
may	O
be	O
found	O
in	O
devroye	O
for	O
d	O
and	O
holmstrom	O
and	O
klemehi	O
for	O
d	O
under	O
regularity	O
conditions	O
on	O
the	O
distribution	O
the	O
choice	O
h	O
cn-d	O
for	O
some	O
constant	O
is	O
asymptotically	O
optimal	O
in	O
density	O
timation	O
however	O
c	O
depends	O
upon	O
unknown	O
distributional	O
parameters	O
rather	O
than	O
following	O
this	O
roundabout	O
process	O
we	O
ask	O
the	O
reader	O
to	O
be	O
patient	O
and	O
to	O
wait	O
until	O
chapter	O
where	O
we	O
study	O
automatic	B
kernel	B
rules	O
i	O
e	O
rules	O
in	O
which	O
h	O
and	O
sometimes	O
k	O
as	O
well	O
is	O
picked	O
by	O
the	O
data	O
without	O
intervention	O
from	O
the	O
statistician	O
it	O
is	O
still	O
too	O
early	O
to	O
say	O
meaningful	O
things	O
about	O
the	O
choice	O
of	O
a	O
kernel	B
the	O
kernel	B
density	O
estimate	O
fnx	O
nd	O
c	O
xi	O
based	O
upon	O
an	O
i	O
i	O
d	O
sample	O
xl	O
drawn	O
from	O
an	O
unknown	O
density	O
f	O
is	O
clearly	O
a	O
density	O
in	O
its	O
own	O
right	O
if	O
k	O
and	O
j	O
k	O
also	O
there	O
are	O
certain	O
consistency	B
popular	O
choices	O
of	O
k	O
that	O
are	O
based	O
upon	O
various	O
optimality	O
criteria	O
in	O
pattern	O
recognition	O
the	O
story	O
is	O
much	O
more	O
confused	O
as	O
there	O
is	O
no	O
compelling	O
a	O
priori	O
reason	O
to	O
pick	O
a	O
function	O
k	O
that	O
is	O
nonnegative	O
or	O
integrable	O
let	O
us	O
make	O
a	O
few	O
points	O
with	O
the	O
trivial	O
case	O
n	O
ting	O
h	O
the	O
kernel	B
rule	B
is	O
given	O
by	O
ifyi	O
gi	O
otherwise	O
if	O
k	O
then	O
gnx	O
if	O
yi	O
or	O
if	O
yi	O
and	O
kx	O
xd	O
o	O
as	O
we	O
would	O
obviously	O
like	O
gnx	O
if	O
and	O
only	O
if	O
it	O
seems	O
necessary	O
to	O
insist	O
on	O
k	O
everywhere	O
however	O
this	O
restriction	O
makes	O
the	O
kernel	B
estimate	O
nonlocal	O
in	O
nature	O
for	O
n	O
and	O
d	O
consider	O
next	O
a	O
negative-valued	O
kernel	B
such	O
as	O
the	O
hermite	B
kernel	B
figure	O
hermite	B
kernel	B
it	O
is	O
easy	O
to	O
verify	O
that	O
k	O
if	O
and	O
only	O
if	O
i	O
x	O
i	O
also	O
j	O
k	O
o	O
nevertheless	O
we	O
note	O
that	O
it	O
yields	O
a	O
simple	O
rule	B
gl	O
otherwise	O
if	O
yi	O
ix	O
xii	O
or	O
if	O
yi	O
i	O
ix	O
xii	O
if	O
we	O
have	O
a	O
biatomic	O
distribution	O
for	O
x	O
with	O
equally	O
likely	O
atoms	O
at	O
and	O
and	O
and	O
y	O
if	O
x	O
and	O
y	O
if	O
x	O
then	O
l	O
and	O
the	O
probability	O
of	O
error	O
for	O
this	O
kernel	B
rule	B
i	O
is	O
as	O
well	O
note	O
also	O
that	O
for	O
all	O
n	O
gn	O
gi	O
if	O
we	O
keep	O
the	O
same	O
k	O
consider	O
now	O
any	O
positive	O
kernel	B
in	O
the	O
same	O
example	O
if	O
xl	O
xn	O
are	O
all	O
zero	O
then	O
the	O
decision	O
is	O
gnx	O
for	O
all	O
x	O
hence	O
ln	O
ipxl	O
xn	O
o	O
our	O
negative	O
zero-integral	O
kernel	B
is	O
strictly	O
better	O
for	O
all	O
n	O
than	O
any	O
positive	O
kernel	B
such	O
kernels	O
should	O
not	O
be	O
discarded	O
without	O
further	O
thought	O
in	O
density	B
estimation	B
negative-valued	O
kernels	O
are	O
used	O
to	O
reduce	O
the	O
bias	B
under	O
some	O
smoothness	O
conditions	O
here	O
as	O
shown	O
above	O
there	O
is	O
an	O
additional	O
reason-negative	O
weights	O
given	O
to	O
points	O
far	O
away	O
from	O
the	O
xis	O
may	O
actually	O
be	O
beneficial	O
staying	O
with	O
the	O
same	O
example	O
if	O
k	O
everywhere	O
then	O
eli	O
y	O
i	O
y	O
o	O
which	O
maybe	O
o	O
happens	O
when	O
e	O
i	O
everywhere	O
for	O
this	O
particular	O
example	O
we	O
would	O
have	O
obtained	O
the	O
same	O
result	O
kernel	B
rules	O
even	O
if	O
k	O
everywhere	O
with	O
k	O
we	O
simply	O
ignore	O
the	O
xis	O
and	O
take	O
a	O
majority	B
vote	I
among	O
the	O
yis	O
k	O
it	O
would	O
be	O
a	O
minority	O
vote	O
if	O
iyio	O
iyil	O
otherwise	O
gn	O
let	O
nn	O
be	O
the	O
number	O
of	O
yis	O
equal	O
to	O
zero	O
as	O
nn	O
is	O
binomial	B
p	O
with	O
p	O
py	O
i	O
we	O
see	O
that	O
eln	O
pp	O
pp	O
minp	O
p	O
simply	O
by	O
invoking	O
the	O
law	O
of	O
large	O
numbers	O
thus	O
eln	O
minp	O
p	O
as	O
in	O
the	O
case	O
with	O
n	O
the	O
limit	O
is	O
when	O
p	O
even	O
though	O
l	O
when	O
e	O
i	O
everywhere	O
it	O
is	O
interesting	O
to	O
note	O
the	O
following	O
though	O
eli	O
p	O
minp	O
p	O
minp	O
p	O
p	O
lim	O
elno	O
n-oo	O
the	O
expected	O
error	O
with	O
one	O
observation	O
is	O
at	O
most	O
twice	O
as	O
bad	O
as	O
the	O
expected	O
error	O
with	O
an	O
infinite	O
sequence	O
we	O
have	O
seen	O
various	O
versions	O
of	O
this	O
inequality	B
at	O
work	O
in	O
many	O
instances	O
such	O
as	O
the	O
nearest	B
neighbor	I
rule	B
let	O
us	O
apply	O
the	O
inequality	B
for	O
el	O
to	O
each	O
part	O
in	O
a	O
fixed	O
partition	B
p	O
of	O
rd	O
on	O
each	O
of	O
the	O
k	O
sets	O
ai	O
ak	O
ofp	O
we	O
apply	O
a	O
simple	O
majority	B
vote	I
among	O
the	O
yis	O
as	O
in	O
the	O
histogram	B
rule	B
if	O
we	O
define	O
the	O
lazy	B
histogram	B
rule	B
as	O
the	O
one	O
in	O
which	O
in	O
each	O
set	O
ai	O
we	O
assign	O
the	O
class	O
according	O
to	O
the	O
yj	O
for	O
which	O
x	O
j	O
e	O
ai	O
and	O
j	O
is	O
the	O
lowest	O
such	O
index	O
first	O
point	O
to	O
fall	O
in	O
a	O
it	O
is	O
clear	O
problems	O
and	O
that	O
lim	O
ellazyn	O
lim	O
eln	O
n-oo	O
n-oo	O
t	O
l	O
ryxldx	O
where	O
ln	O
is	O
the	O
probability	O
of	O
error	O
for	O
the	O
ordinary	B
histogram	B
rule	B
again	O
the	O
vast	O
majority	O
of	O
observations	O
is	O
barely	O
needed	O
to	O
reach	O
a	O
good	O
decision	O
just	O
for	O
fun	O
let	O
us	O
return	O
to	O
a	O
majority	B
vote	I
rule	B
now	O
applied	O
to	O
the	O
first	O
three	O
observations	O
only	O
with	O
p	O
p	O
i	O
we	O
see	O
that	O
p	O
pfp	O
p	O
by	O
just	O
writing	O
down	O
binomial	B
probabilities	O
observe	O
that	O
p	O
p	O
minp	O
pl	O
lim	O
eln	O
lim	O
n-oo	O
n-oo	O
p	O
proof	O
of	O
the	O
consistency	B
theorem	O
if	O
limn--oo	O
eln	O
is	O
small	O
to	O
start	O
with	O
e	O
g	O
limn--oo	O
eln	O
then	O
x	O
in	O
such	O
cases	O
it	O
just	O
does	O
not	O
pay	O
to	O
take	O
more	O
than	O
three	O
observations	O
kernels	O
with	O
fixed	O
smoothing	O
factors	O
have	O
no	O
local	O
sensitivity	O
and	O
except	O
in	O
some	O
circumstances	O
have	O
probabilities	O
of	O
error	O
that	O
do	O
not	O
converge	O
to	O
l	O
the	O
versal	O
consistency	B
theorem	O
makes	O
a	O
strong	O
case	O
for	O
decreasing	O
smoothing	O
there	O
is	O
no	O
hope	O
in	O
general	O
of	O
approaching	O
l	O
unless	O
decisions	O
are	O
asymptotically	O
local	O
the	O
consistency	B
theorem	O
describes	O
kernel	B
rules	O
with	O
h	O
these	O
rules	O
become	O
more	O
and	O
more	O
local	O
in	O
nature	O
as	O
n	O
the	O
necessity	O
oflocal	O
rules	O
is	O
not	O
ent	O
from	O
the	O
previous	O
biatomic	O
example	O
however	O
it	O
is	O
clear	O
that	O
if	O
we	O
consider	O
a	O
distribution	O
in	O
which	O
given	O
y	O
x	O
is	O
uniform	O
on	O
and	O
given	O
y	O
x	O
is	O
uniform	O
on	O
that	O
is	O
with	O
the	O
two	O
classes	O
intimately	O
interwoven	O
a	O
kernel	B
rule	B
with	O
k	O
of	O
compact	O
support	B
and	O
h	O
will	O
have	O
ln	O
i	O
where	O
ni	O
is	O
the	O
number	O
of	O
xs	O
at	O
the	O
i-th	O
atom	O
hence	O
eln	O
goes	O
to	O
zero	O
nentially	O
fast	O
if	O
in	O
the	O
above	O
example	O
we	O
assign	O
x	O
by	O
a	O
geometric	B
distribution	I
on	O
when	O
y	O
and	O
by	O
a	O
geometric	B
distribution	I
on	O
when	O
y	O
then	O
to	O
obtain	O
eln	O
it	O
is	O
necessary	O
that	O
h	O
problem	O
remark	O
it	O
is	O
worthwhile	O
to	O
investigate	O
what	O
happens	O
for	O
negative-valued	O
kernels	O
k	O
when	O
h	O
nhd	O
and	O
k	O
has	O
compact	O
support	B
every	O
decision	O
becomes	O
an	O
average	O
over	O
many	O
local	O
decisions	O
if	O
fj	O
has	O
a	O
density	O
f	O
then	O
at	O
almost	O
all	O
points	O
x	O
f	O
may	O
be	O
approximated	O
very	O
nicely	O
by	O
f	O
fasxo	O
for	O
small	O
where	O
s	O
xo	O
is	O
the	O
closed	O
ball	O
of	O
radius	O
about	O
x	O
this	O
implies	O
roughly	O
speaking	O
that	O
the	O
number	O
of	O
weighted	B
votes	O
from	O
class	O
observations	O
in	O
a	O
neighborhood	O
of	O
x	O
is	O
about	O
fxnhd	O
f	O
k	O
while	O
for	O
class	O
the	O
weight	O
is	O
about	O
f	O
k	O
the	O
correct	O
decision	O
is	O
nearly	O
always	O
made	O
for	O
nh	O
d	O
large	O
enough	O
provided	O
that	O
j	O
k	O
o	O
see	O
problem	O
on	O
why	O
kernels	O
with	O
j	O
k	O
should	O
be	O
avoided	O
proof	O
of	O
the	O
consistency	B
theorem	O
in	O
the	O
proof	O
we	O
can	O
proceed	O
as	O
for	O
the	O
histogram	O
the	O
crucial	O
difference	O
is	O
captured	O
in	O
the	O
following	O
covering	O
lemmas	O
let	O
denote	O
the	O
minimum	O
number	O
of	O
balls	O
of	O
radius	O
that	O
cover	O
the	O
ball	O
so	O
if	O
k	O
is	O
the	O
naive	B
kernel	B
then	O
p	O
in	O
theorem	O
lemma	O
lemma	O
f	O
kx	O
ixesoil	O
thenforany	O
y	O
end	O
h	O
and	O
probability	O
measure	O
fj	O
f	O
khx	O
y	O
f	O
khx	O
zfjdz	O
fjdx	O
kernel	B
rules	O
proof	O
cover	O
the	O
ball	O
syh	O
by	O
fjd	O
balls	O
of	O
radius	O
denote	O
their	O
centers	O
by	O
xl	O
then	O
x	O
e	O
implies	O
c	O
sxh	O
and	O
thus	O
we	O
may	O
write	O
jlsxh	O
f	O
ixesyh	O
jldx	O
t	O
f	O
jldx	O
t	O
f	O
ixesxi	O
jldx	O
jl	O
xh	O
il	O
il	O
lemma	O
let	O
h	O
r	O
and	O
let	O
s	O
c	O
rd	O
be	O
a	O
ball	O
of	O
radius	O
r	O
then	O
for	O
every	O
probability	O
measure	O
jl	O
where	O
cd	O
depends	O
upon	O
the	O
dimension	B
d	O
only	O
proof	O
cover	O
s	O
with	O
balls	O
of	O
radius	O
h	O
centered	O
at	O
center	O
points	O
of	O
a	O
regular	B
grid	O
of	O
dimension	B
x	O
x	O
denote	O
these	O
centers	O
by	O
xl	O
where	O
m	O
is	O
the	O
number	O
of	O
balls	O
that	O
cover	O
s	O
clearly	O
m	O
volumesorh	O
volumegrid	O
cell	O
vdr	O
hd	O
is	O
the	O
volume	O
of	O
the	O
unit	O
ball	O
in	O
r	O
d	O
d	O
c	O
where	O
the	O
constant	O
c	O
depends	O
upon	O
the	O
dimension	B
only	O
every	O
x	O
gets	O
covered	O
at	O
most	O
kl	O
times	O
where	O
kl	O
depends	O
upon	O
d	O
only	O
then	O
we	O
have	O
proof	O
of	O
the	O
consistency	B
theorem	O
m	O
f	O
i	O
l	O
il	O
j	O
ttdx	O
the	O
same	O
argument	O
as	O
in	O
lemma	O
m	O
l	O
j	O
il	O
m	O
m	O
l	O
il	O
the	O
cauchy-schwarz	B
inequality	B
jklm	O
he	O
d	O
where	O
cd	O
depends	O
upon	O
the	O
dimension	B
only	O
proof	O
of	O
theorem	O
define	O
yjkhx	O
x	O
j	O
nekhx	O
x	O
since	O
the	O
decision	O
rule	B
can	O
be	O
written	O
as	O
gnx	O
if	O
yjkhx	O
x	O
j	O
yjkhx	O
x	O
j	O
nekhx	O
x	O
nekhx	O
x	O
otherwise	O
by	O
theorem	O
what	O
we	O
have	O
to	O
prove	O
is	O
that	O
for	O
n	O
large	O
enough	O
p	O
iryx	O
rynxiildx	O
we	O
use	O
a	O
decomposition	O
as	O
in	O
the	O
proof	O
of	O
strong	B
consistency	B
of	O
the	O
histogram	B
rule	B
to	O
handle	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
fix	O
e	O
and	O
let	O
r	O
rd	O
r	O
be	O
a	O
continuous	O
function	O
of	O
bounded	O
support	B
satisfying	O
f	O
rxlttdx	O
e	O
kernel	B
rules	O
obviously	O
we	O
can	O
choose	O
the	O
function	O
r	O
such	O
that	O
s	O
rx	O
s	O
for	O
all	O
x	O
e	O
rd	O
then	O
we	O
have	O
the	O
following	O
simple	O
upper	O
bound	O
e	O
x	O
i	O
rex	O
ekhx	O
x	O
i	O
e	O
x	O
i	O
ekhx	O
x	O
i	O
next	O
we	O
bound	O
the	O
integral	O
of	O
each	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
inequality	B
above	O
first	O
term	O
by	O
the	O
definition	B
of	I
r	O
f	O
rxltldx	O
fl	O
second	O
term	O
since	O
rx	O
is	O
continuous	O
and	O
zero	O
outside	O
of	O
a	O
bounded	O
set	O
it	O
is	O
also	O
uniformly	O
continuous	O
that	O
is	O
there	O
exists	O
a	O
such	O
that	O
ilx	O
y	O
implies	O
irx	O
ft	O
also	O
rex	O
is	O
bounded	O
thus	O
rex	O
tldx	O
ekhx	O
x	O
e	O
x	O
i	O
f	O
i	O
f	O
irx	O
f	O
rykhx	O
ytldy	O
i	O
jldx	O
s	O
f	O
khx	O
y	O
ekhx	O
x	O
ekhx	O
x	O
ry	O
tldytldx	O
s	O
clearly	O
we	O
have	O
in	O
the	O
last	O
step	O
we	O
used	O
the	O
fact	O
that	O
supxy	O
fsxii	O
eh	O
ltldy	O
s	O
and	O
by	O
the	O
uniform	O
continuity	O
of	O
rex	O
supzesxo	O
irx	O
ft	O
thus	O
the	O
first	O
term	O
at	O
the	O
end	O
of	O
the	O
chain	O
of	O
inequalities	O
above	O
is	O
bounded	O
by	O
ft	O
the	O
second	O
term	O
converges	O
to	O
zero	O
since	O
h	O
for	O
all	O
n	O
large	O
enough	O
which	O
in	O
tum	O
implies	O
fs	O
ez	O
tldy	O
o	O
is	O
obvious	O
for	O
the	O
naive	B
kernel	B
for	O
regular	B
kernels	O
convergence	O
to	O
zero	O
follows	O
from	O
problem	O
the	O
convergence	O
of	O
the	O
integral	O
respect	O
to	O
tldx	O
follows	O
from	O
the	O
dominated	O
proof	O
of	O
the	O
consistency	B
theorem	O
convergence	O
theorem	O
in	O
summary	O
we	O
have	O
shown	O
that	O
e	O
x	O
i	O
lim	O
sup	O
n--oo	O
rx	O
ekhx	O
x	O
i	O
i	O
pdx	O
e	O
third	O
term	O
e	O
x	O
ekhx	O
x	O
i	O
elnx	O
pdx	O
i	O
i	O
iry	O
lyi	O
khx	O
zpdzpdypdx	O
pdx	O
khx	O
y	O
lykhx	O
ypdy	O
i	O
ekhx	O
x	O
i	O
i	O
i	O
i	O
i	O
f	O
k	O
iry	O
i	O
plry	O
lyipdy	O
pe	O
fubinis	O
theorem	O
ryylldy	O
where	O
in	O
the	O
last	O
two	O
steps	O
we	O
used	O
the	O
covering	B
lemma	I
lemma	O
for	O
the	O
naive	B
kernel	B
and	O
problem	O
for	O
general	O
kernels	O
and	O
the	O
definition	B
of	I
rex	O
p	O
is	O
the	O
constant	O
appearing	O
in	O
the	O
covering	B
lemma	I
fourth	O
term	O
we	O
show	O
that	O
e	O
ierynx	O
for	O
the	O
naive	B
kernel	B
we	O
have	O
ryn	O
i	O
idx	O
o	O
e	O
je	O
e	O
x	O
j	O
ely	O
khx	O
xl	O
n	O
eykhx	O
x	O
eykhx	O
nekhx	O
kernel	B
rules	O
where	O
we	O
used	O
the	O
cauchy-schwarz	B
inequality	B
and	O
properties	O
of	O
the	O
naive	B
kernel	B
extension	O
to	O
regular	B
kernels	O
is	O
straightforward	O
next	O
we	O
use	O
the	O
inequality	B
above	O
to	O
show	O
that	O
the	O
integral	O
converges	O
to	O
zero	O
divide	O
the	O
integral	O
over	O
nd	O
into	O
two	O
terms	O
namely	O
an	O
integral	O
over	O
a	O
large	O
ball	O
s	O
centered	O
at	O
the	O
origin	O
of	O
radius	O
r	O
and	O
an	O
integral	O
over	O
sc	O
for	O
the	O
integral	O
outside	O
of	O
the	O
ball	O
we	O
have	O
e	O
fldx	O
with	O
probability	O
one	O
as	O
n	O
which	O
can	O
be	O
shown	O
in	O
the	O
same	O
way	O
we	O
proved	O
jrd	O
jrd	O
the	O
first	O
second	O
and	O
third	O
terms	O
of	O
clearly	O
the	O
radius	O
r	O
of	O
the	O
ball	O
s	O
can	O
be	O
chosen	O
such	O
that	O
to	O
bound	O
the	O
integral	O
over	O
s	O
we	O
employ	O
lemma	O
f	O
e	O
fldx	O
fldx	O
s	O
j	O
flsxh	O
the	O
inequality	B
obtained	O
above	O
h	O
cd	O
by	O
assumption	O
nhd	O
therefore	O
if	O
n	O
is	O
sufficiently	O
large	O
then	O
for	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
we	O
have	O
e	O
iryx	O
if	O
we	O
take	O
e	O
ryn	O
ii-dx	O
ep	O
it	O
remains	O
to	O
show	O
that	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
is	O
small	O
with	O
large	O
probability	O
to	O
do	O
this	O
we	O
use	O
mcdiarmids	O
inequality	B
for	O
i	O
iryx	O
rynxll-dx	O
e	O
iryx	O
rynxii-dx	O
fix	O
the	O
training	O
data	O
at	O
yi	O
yn	O
and	O
replace	O
the	O
i-th	O
pair	O
yi	O
by	O
changing	O
the	O
value	O
of	O
to	O
clearly	O
by	O
the	O
covering	B
lemma	I
i	O
i	O
i	O
sup	O
i	O
y	O
fldx	O
yerd	O
nekhx	O
x	O
n	O
potential	O
function	O
rules	O
so	O
by	O
theorem	O
p	O
ix	O
i	O
p	O
ix	O
rynxidx	O
e	O
iryx	O
the	O
proof	O
is	O
now	O
completed	O
potential	O
function	O
rules	O
kernel	B
classification	O
rules	O
may	O
be	O
formulated	O
in	O
terms	O
of	O
the	O
so-called	O
potential	O
function	O
rules	O
these	O
rules	O
were	O
originally	O
introduced	O
and	O
studied	O
by	O
bashkirov	O
braverman	O
and	O
muchnik	O
aizerman	O
braverman	O
and	O
rozonoer	O
braverman	O
and	O
braverman	O
and	O
pyatniskii	O
the	O
original	O
idea	O
was	O
the	O
following	O
put	O
a	O
unit	O
of	O
positive	O
electrical	O
charge	O
at	O
every	O
data	O
point	O
xi	O
where	O
yi	O
and	O
a	O
unit	O
of	O
negative	O
charge	O
at	O
data	O
points	O
xi	O
where	O
yi	O
o	O
the	O
resulting	O
potential	O
field	O
defines	O
an	O
intuitively	O
appealing	O
rule	B
the	O
decision	O
at	O
a	O
point	O
x	O
is	O
one	O
if	O
the	O
potential	O
at	O
that	O
point	O
is	O
positive	O
and	O
zero	O
if	O
it	O
is	O
negative	O
this	O
idea	O
leads	O
to	O
a	O
rule	B
that	O
can	O
be	O
generalized	B
to	O
obtain	O
rules	O
of	O
the	O
form	O
gnx	O
if	O
i	O
otherwise	O
where	O
n	O
fnx	O
l	O
rnidnknix	O
xi	O
il	O
where	O
the	O
kns	O
describe	O
the	O
potential	O
field	O
around	O
xi	O
and	O
the	O
rns	O
are	O
their	O
weights	O
rules	O
that	O
can	O
be	O
put	O
into	O
this	O
form	O
are	O
often	O
called	O
potential	O
function	O
rules	O
here	O
we	O
give	O
a	O
brief	O
survey	O
of	O
these	O
rules	O
kernel	B
rules	O
clearly	O
kernel	B
rules	O
studied	O
in	O
the	O
previous	O
section	O
are	O
potential	O
function	O
rules	O
with	O
knix	O
y	O
k	O
t	O
x	O
y	O
here	O
k	O
is	O
a	O
fixed	O
kernel	B
function	O
and	O
hi	O
is	O
a	O
sequence	O
of	O
positive	O
numbers	O
histogram	O
rules	O
similarly	O
histogram	O
rules	O
chapters	O
and	O
can	O
be	O
put	O
in	O
this	O
form	O
by	O
choosing	O
kernel	B
rules	O
and	O
rnidn	O
recall	O
that	O
an	O
denotes	O
the	O
cell	O
of	O
the	O
partition	B
in	O
which	O
x	O
falls	O
knjx	O
y	O
iyeanx	O
polynomial	B
discriminant	O
functions	O
specht	O
suggested	O
applying	O
a	O
nomial	O
expansion	O
to	O
the	O
kernel	B
k	O
cy	O
this	O
led	O
to	O
the	O
choice	O
knix	O
y	O
l	O
v-rxljfjy	O
k	O
jl	O
and	O
rnidn	O
where	O
v-rk	O
are	O
fixed	O
real-valued	O
functions	O
on	O
nd	O
when	O
these	O
functions	O
are	O
polynomials	O
the	O
corresponding	O
classifier	B
gn	O
is	O
called	O
a	O
polynomial	B
discriminant	I
function	I
the	O
potential	B
function	I
rule	B
obtained	O
this	O
way	O
is	O
a	O
generalized	B
linear	O
rule	B
chapter	O
with	O
k	O
fnx	O
l	O
anjljljx	O
jl	O
where	O
the	O
coefficients	O
an	O
depend	O
on	O
the	O
data	O
dn	O
only	O
through	O
n	O
anj	O
il	O
lv-rjxi	O
this	O
choice	O
of	O
the	O
coefficients	O
does	O
not	O
necessarily	O
lead	O
to	O
a	O
consistent	O
rule	B
unless	O
the	O
functions	O
v-rk	O
are	O
allowed	O
to	O
change	O
with	O
n	O
or	O
k	O
is	O
allowed	O
to	O
vary	O
with	O
n	O
nevertheless	O
the	O
rule	B
has	O
some	O
computational	O
advantages	O
over	O
kernel	B
rules	O
in	O
many	O
practical	O
situations	O
there	O
is	O
enough	O
time	O
to	O
preprocess	O
the	O
data	O
dn	O
but	O
once	O
the	O
observation	O
x	O
becomes	O
known	O
the	O
decision	O
has	O
to	O
be	O
made	O
very	O
quickly	O
clearly	O
the	O
coefficients	O
an	O
ann	O
can	O
be	O
computed	O
by	O
knowing	O
the	O
training	O
data	O
dn	O
only	O
and	O
if	O
the	O
values	O
v-rkx	O
are	O
easily	O
computable	O
then	O
fnx	O
can	O
be	O
computed	O
much	O
more	O
quickly	O
than	O
in	O
a	O
kernel-based	O
decision	O
where	O
all	O
n	O
terms	O
of	O
the	O
sum	O
have	O
to	O
be	O
computed	O
in	O
real	O
time	O
if	O
no	O
preprocessing	O
is	O
done	O
however	O
using	O
preprocessing	O
of	O
the	O
data	O
may	O
also	O
help	O
with	O
kernel	B
rules	O
cially	O
when	O
d	O
for	O
a	O
survey	O
of	O
computational	O
speed-up	O
with	O
kernel	B
methods	O
see	O
devroye	O
and	O
machell	O
recursive	B
kernel	B
rules	O
consider	O
the	O
choice	O
knixyk	O
x	O
y	O
observe	O
that	O
the	O
only	O
difference	O
between	O
this	O
and	O
the	O
ordinary	B
kernel	B
rule	B
is	O
that	O
in	O
the	O
expression	B
of	O
kni	O
the	O
smoothing	O
parameter	O
hn	O
is	O
replaced	O
with	O
hi	O
with	O
this	O
change	O
we	O
can	O
compute	O
the	O
rule	B
recursively	O
by	O
observing	O
that	O
fnlx	O
fnx	O
lk	O
x	O
xnl	O
hnl	O
problems	O
and	O
exercises	O
the	O
computational	O
advantage	O
of	O
this	O
rule	B
is	O
that	O
if	O
one	O
collects	O
additional	O
data	O
then	O
the	O
rule	B
does	O
not	O
have	O
to	O
be	O
entirely	O
recomputed	O
it	O
can	O
be	O
adjusted	O
using	O
the	O
formula	O
above	O
consistency	B
properties	O
of	O
this	O
rule	B
were	O
studied	O
by	O
devroye	O
and	O
wagner	O
krzyzak	O
and	O
pawlak	O
krzyzak	O
and	O
greblicki	O
and	O
pawlak	O
several	O
similar	O
recursive	B
kernel	B
rules	O
have	O
been	O
studied	O
in	O
the	O
literature	O
wolverton	O
and	O
wagner	O
greblicki	O
and	O
krzyzak	O
and	O
pawlak	O
studied	O
the	O
situation	O
when	O
y	O
knix	O
y	O
k	O
t	O
the	O
corresponding	O
rule	B
can	O
be	O
computed	O
recursively	O
by	O
fnlx	O
fnx	O
k	O
hnl	O
xnl	O
hnl	O
motivated	O
by	O
stochastic	B
approximation	I
methods	O
chapter	O
revesz	O
suggested	O
and	O
studied	O
the	O
rule	B
obtained	O
from	O
fnlx	O
fnx	O
n	O
fnx-d-k	O
hnl	O
xnl	O
hnl	O
a	O
similar	O
rule	B
was	O
studied	O
by	O
gyorfi	O
fnlx	O
fnx	O
n	O
fnxk	O
xnl	O
hnl	O
problem	O
let	O
k	O
be	O
a	O
nonnegative	O
kernel	B
with	O
compact	O
support	B
on	O
show	O
problems	O
and	O
exercises	O
that	O
for	O
some	O
distribution	O
h	O
is	O
necessary	O
for	O
consistency	B
of	O
the	O
kernel	B
rule	B
to	O
this	O
end	O
consider	O
the	O
following	O
example	O
given	O
y	O
x	O
has	O
a	O
geometric	B
distribution	I
on	O
and	O
given	O
y	O
x	O
has	O
a	O
geometric	B
distribution	I
on	O
then	O
show	O
that	O
to	O
obtain	O
eln	O
l	O
it	O
is	O
necessary	O
that	O
h	O
nd	O
with	O
density	O
f	O
let	O
k	O
be	O
a	O
kernel	B
function	O
integrating	O
to	O
one	O
and	O
let	O
hn	O
be	O
a	O
problem	O
kernel	B
density	B
estimation	B
let	O
xl	O
xn	O
be	O
i	O
i	O
d	O
random	O
variables	O
in	O
smoothing	B
factor	I
the	O
kernel	B
density	O
estimate	O
is	O
defined	O
by	O
fnx	O
nh	O
ft	O
k	O
t	O
parzen	O
prove	O
that	O
the	O
estimate	O
is	O
weakly	O
universally	O
consistent	O
in	O
ll	O
if	O
h	O
n	O
and	O
nh	O
as	O
n	O
hint	O
proceed	O
as	O
in	O
problem	O
kernel	B
rules	O
problem	O
strong	B
consistency	B
of	O
kernel	B
density	B
estimation	B
let	O
xl	O
xn	O
be	O
i	O
i	O
d	O
random	O
variables	O
in	O
nd	O
with	O
density	O
f	O
let	O
k	O
be	O
a	O
nonnegative	O
function	O
integrating	O
to	O
one	O
kernel	B
and	O
h	O
a	O
smoothing	B
factor	I
as	O
in	O
the	O
previous	O
exercise	O
the	O
kernel	B
density	O
estimate	O
is	O
defined	O
by	O
fnx	O
k-h	O
i	O
n	O
i	O
prove	O
for	O
the	O
ll-error	O
of	O
the	O
estimate	O
that	O
p	O
l	O
hlx	O
fxldx	O
e	O
f	O
ifnx	O
fxdxi	O
e	O
s	O
conclude	O
that	O
weak	B
ll-consistency	O
of	O
the	O
estimate	O
implies	O
strong	O
sistency	O
problem	O
this	O
is	O
a	O
way	O
to	O
show	O
that	O
weak	B
and	O
strong	O
ll-consistencies	O
of	O
the	O
kernel	B
density	O
estimate	O
are	O
equivalent	O
also	O
for	O
d	O
since	O
if	O
k	O
is	O
nonnegative	O
then	O
e	O
f	O
ifnx	O
fxldx	O
cannot	O
converge	O
to	O
zero	O
faster	O
than	O
for	O
any	O
density	O
devroye	O
and	O
gyorfi	O
therefore	O
the	O
inequality	B
above	O
implies	O
that	O
for	O
any	O
density	O
lim	O
f	O
ifnx	O
n-co	O
e	O
f	O
ifnx	O
fxldx	O
fxldx	O
with	O
probability	O
one	O
this	O
property	O
is	O
called	O
the	O
relative	B
stability	I
of	O
the	O
l	O
i	O
error	O
it	O
means	O
that	O
the	O
asymptotic	O
behavior	O
of	O
the	O
l	O
i	O
is	O
the	O
same	O
as	O
that	O
of	O
its	O
expected	O
value	O
hint	O
use	O
mcdiarmids	O
inequality	B
problem	O
if	O
f	O
k	O
show	O
that	O
under	O
the	O
assumption	O
that	O
fj	O
has	O
a	O
density	O
f	O
and	O
that	O
h	O
nhd	O
the	O
kernel	B
rule	B
has	O
lim	O
eln	O
e	O
i	O
l	O
n-co	O
thus	O
the	O
rule	B
makes	O
the	O
wrong	O
decisions	O
and	O
such	O
kernels	O
should	O
be	O
avoided	O
hint	O
you	O
may	O
use	O
the	O
fact	O
that	O
for	O
the	O
kernel	B
density	O
estimate	O
with	O
kernel	B
l	O
satisfying	O
f	O
l	O
f	O
ifnx	O
fxldx	O
with	O
probability	O
one	O
if	O
h	O
and	O
nhd	O
problems	O
and	O
problem	O
consider	O
a	O
devilish	B
kernel	B
that	O
attaches	O
counterproductive	O
weight	O
to	O
the	O
origin	O
kx	O
if	O
iixll	O
s	O
if	O
ilxll	O
s	O
if	O
ilxll	O
assume	O
that	O
l	O
o	O
show	O
that	O
ln	O
with	O
assume	O
that	O
h	O
yet	O
nhd	O
probability	O
one	O
concession	O
if	O
you	O
find	O
that	O
you	O
cant	O
handle	O
the	O
universality	O
try	O
first	O
proving	O
the	O
statement	O
for	O
strictly	B
separable	I
distributions	O
problem	O
show	O
that	O
for	O
the	O
distribution	O
depicted	O
in	O
figure	O
the	O
kernel	B
rule	B
with	O
kernel	B
ku	O
is	O
consistent	O
whenever	O
h	O
the	O
smoothing	B
factor	I
remains	O
fixed	O
and	O
h	O
s	O
problem	O
the	O
limit	O
for	O
fixed	O
h	O
consider	O
a	O
kernel	B
rule	B
with	O
fixed	O
h	O
and	O
fixed	O
kernel	B
k	O
find	O
a	O
simple	O
argument	O
that	O
proves	O
problems	O
and	O
exercises	O
lim	O
eln	O
looi	O
n-oo	O
where	O
loo	O
is	O
the	O
probability	O
of	O
error	O
for	O
the	O
decision	O
goo	O
defined	O
by	O
if	O
ekx	O
if	O
ekx	O
l	O
i	O
find	O
a	O
distribution	O
such	O
that	O
for	O
the	O
window	O
kernel	B
loo	O
yet	O
l	O
is	O
there	O
such	O
a	O
distribution	O
for	O
any	O
kernel	B
hint	O
try	O
proving	O
a	O
convergence	O
result	O
at	O
each	O
x	O
by	O
invoking	O
the	O
law	O
of	O
large	O
numbers	O
and	O
then	O
replace	O
x	O
by	O
x	O
problem	O
show	O
that	O
the	O
conditions	O
h	O
n	O
and	O
nhd	O
of	O
theorem	O
are	O
not	O
necessary	O
for	O
consistency	B
that	O
is	O
exhibit	O
a	O
distribution	O
such	O
that	O
the	O
kernel	B
rule	B
is	O
consistent	O
with	O
hn	O
and	O
exhibit	O
another	O
distribution	O
for	O
which	O
the	O
kernel	B
rule	B
is	O
consistent	O
with	O
hn	O
rv	O
lnljd	O
problem	O
prove	O
that	O
the	O
conditions	O
hn	O
and	O
nhd	O
of	O
theorem	O
are	O
necessary	O
for	O
universal	B
consistency	B
that	O
is	O
show	O
that	O
if	O
one	O
of	O
these	O
conditions	O
are	O
violated	O
then	O
there	O
is	O
a	O
distribution	O
for	O
which	O
the	O
kernel	B
rule	B
is	O
not	O
consistent	O
problem	O
this	O
exercise	O
provides	O
an	O
argument	O
in	O
favor	O
of	O
monotonicity	O
of	O
the	O
kernel	B
k	O
in	O
find	O
a	O
nonatornic	O
distribution	O
for	O
y	O
and	O
a	O
positive	O
kernel	B
with	O
f	O
k	O
k	O
vanishing	O
off	O
so	O
o	O
for	O
some	O
such	O
that	O
for	O
all	O
h	O
and	O
all	O
n	O
the	O
kernel	B
rule	B
b	O
in	O
the	O
universal	B
consistency	B
theorem	O
cannot	O
be	O
abolished	O
altogether	O
has	O
eln	O
while	O
l	O
o	O
this	O
result	O
says	O
that	O
the	O
condition	O
kx	O
bisool	O
for	O
some	O
problem	O
with	O
k	O
as	O
in	O
the	O
previous	O
problem	O
and	O
taking	O
h	O
show	O
that	O
lim	O
eln	O
l	O
n-oo	O
under	O
the	O
following	O
conditions	O
k	O
has	O
compact	O
support	B
vanishing	O
off	O
soo	O
k	O
and	O
k	O
bisol	O
for	O
some	O
e	O
o	O
we	O
say	O
that	O
we	O
have	O
agreement	B
on	O
sxo	O
when	O
for	O
all	O
z	O
e	O
sxo	O
either	O
or	O
we	O
ask	O
that	O
pagreement	O
on	O
sxo	O
problem	O
the	O
previous	O
exercise	O
shows	O
that	O
at	O
points	O
where	O
there	O
is	O
agreement	B
we	O
make	O
asymptotically	O
the	O
correct	O
decision	O
with	O
kernels	O
with	O
fixed	O
smoothing	B
factor	I
let	O
d	O
be	O
the	O
set	O
and	O
let	O
the	O
of	O
d	O
be	O
defined	O
by	O
do	O
ii	O
y	O
x	O
ii	O
for	O
some	O
xed	O
let	O
fl	O
be	O
the	O
probability	O
measure	O
for	O
x	O
take	O
k	O
h	O
as	O
in	O
the	O
previous	O
exercise	O
noting	O
that	O
x	O
f	O
do	O
means	O
that	O
we	O
have	O
agreement	B
on	O
sxo	O
show	O
that	O
for	O
all	O
distributions	O
of	O
y	O
lim	O
sup	O
eln	O
l	O
fldo	O
n-oo	O
kernel	B
rules	O
figure	O
of	O
a	O
set	O
d	O
problem	O
continuation	O
clearly	O
as	O
when	O
fj	O
d	O
o	O
convince	O
yourself	O
that	O
fj	O
d	O
for	O
most	O
problems	O
if	O
you	O
knew	O
how	O
fast	O
tended	O
to	O
zero	O
then	O
the	O
previous	O
exercise	O
would	O
enable	O
you	O
to	O
pick	O
h	O
as	O
a	O
function	O
of	O
n	O
such	O
that	O
h	O
and	O
such	O
that	O
the	O
upper	O
bound	O
for	O
eln	O
obtained	O
by	O
analogy	O
from	O
the	O
previous	O
exercise	O
is	O
approximately	O
minimal	O
if	O
in	O
n	O
d	O
d	O
is	O
the	O
surface	O
of	O
the	O
unit	O
ball	O
x	O
has	O
a	O
bounded	O
density	O
f	O
and	O
is	O
lipschitz	O
determine	O
a	O
bound	O
for	O
by	O
considering	O
the	O
proof	O
ofthe	O
universal	B
consistency	B
theorem	O
show	O
how	O
to	O
choose	O
h	O
such	O
that	O
problem	O
extension	O
of	O
the	O
covering	B
lemma	I
to	O
regular	B
kernels	O
let	O
k	O
be	O
a	O
regular	B
kernel	B
and	O
let	O
fj	O
be	O
an	O
arbitrary	O
probability	O
measure	O
prove	O
that	O
there	O
exists	O
a	O
finite	O
constant	O
p	O
pk	O
only	O
depending	O
upon	O
k	O
such	O
that	O
for	O
any	O
y	O
and	O
h	O
and	O
krzyzak	O
hint	O
prove	O
this	O
by	O
checking	O
the	O
following	O
first	O
take	O
a	O
bounded	O
overlap	O
cover	O
of	O
nd	O
with	O
translates	O
of	O
where	O
r	O
is	O
the	O
constant	O
appearing	O
in	O
the	O
definition	B
of	I
a	O
regular	B
kernel	B
this	O
cover	O
has	O
an	O
infinite	O
number	O
of	O
member	O
balls	O
but	O
every	O
x	O
gets	O
covered	O
at	O
most	O
k	O
times	O
where	O
k	O
depends	O
upon	O
d	O
only	O
the	O
centers	O
of	O
the	O
balls	O
are	O
called	O
xi	O
i	O
the	O
integral	O
condition	O
on	O
k	O
implies	O
that	O
l	O
sup	O
kxs	O
il	O
for	O
another	O
finite	O
constant	O
show	O
that	O
k	O
j	O
dx	O
sup	O
kydxs	O
khx	O
y	O
l	O
sup	O
khx	O
yixeyhxi	O
il	O
xeyhxi	O
and	O
from	O
conclude	O
problems	O
and	O
exercises	O
l	O
supzehxiso	O
khz	O
il	O
bfly	O
hxi	O
f	O
fly	O
hxi	O
khz	O
fl	O
bfly	O
hxi	O
il	O
sup	O
b	O
il	O
khz	O
b	O
where	O
depends	O
on	O
k	O
and	O
d	O
only	O
problem	O
let	O
k	O
be	O
a	O
regular	B
kernel	B
and	O
let	O
fl	O
be	O
an	O
arbitrary	O
probability	O
measure	O
prove	O
that	O
for	O
any	O
r	O
h	O
sp	O
f	O
khx	O
yillx-yllo	O
f	O
khx	O
zfld	O
fl	O
x	O
hint	O
substitute	O
khz	O
in	O
the	O
proof	O
of	O
problem	O
by	O
khzillzil	O
and	O
notice	O
that	O
f	O
khx	O
yillx-yllo	O
khx	O
zfldz	O
f	O
sup	O
y	O
fldx	O
l	O
il	O
sup	O
khzilizll	O
as	O
h	O
o	O
problem	O
use	O
problems	O
and	O
to	O
extend	O
the	O
proof	O
of	O
theorem	O
for	O
arbitrary	O
regular	B
kernels	O
problem	O
show	O
that	O
the	O
constant	O
in	O
lemma	O
is	O
never	O
more	O
than	O
problem	O
show	O
that	O
if	O
l	O
is	O
a	O
bounded	O
function	O
that	O
is	O
monotonically	O
decreasing	O
on	O
with	O
the	O
property	O
that	O
f	O
u	O
ludu	O
and	O
if	O
k	O
n	O
d	O
is	O
a	O
function	O
with	O
kx	O
llix	O
ii	O
then	O
k	O
is	O
regular	B
d	O
problem	O
find	O
a	O
kernel	B
k	O
that	O
is	O
monotonically	O
decreasing	O
along	O
rays	O
krx	O
kx	O
for	O
all	O
x	O
e	O
n	O
d	O
and	O
all	O
r	O
such	O
that	O
k	O
is	O
not	O
regular	B
exercise	O
is	O
intended	O
to	O
convince	O
you	O
that	O
it	O
is	O
very	O
difficult	O
to	O
find	O
well-behaved	O
kernels	O
that	O
are	O
not	O
regular	B
problem	O
let	O
kx	O
lllxll	O
for	O
some	O
bounded	O
function	O
l	O
o	O
show	O
that	O
k	O
is	O
regular	B
if	O
l	O
is	O
decreasing	O
on	O
and	O
f	O
kxdx	O
conclude	O
that	O
the	O
gaussian	B
and	O
cauchy	B
kernels	O
are	O
regular	B
problem	O
regularity	O
of	O
the	O
kernel	B
is	O
not	O
necessary	O
for	O
universal	B
consistency	B
gate	O
universal	B
consistency	B
with	O
a	O
nonintegrable	O
kernel-that	O
is	O
for	O
which	O
f	O
k	O
as	O
kx	O
ixl	O
greblicki	O
krzyzak	O
and	O
pawlak	O
proved	O
consistency	B
of	O
the	O
kernel	B
rule	B
with	O
smoothing	B
factor	I
hn	O
satisfying	O
hn	O
and	O
nh	O
if	O
the	O
nel	O
k	O
satisfies	O
the	O
following	O
conditions	O
kx	O
cilixliolj	O
for	O
some	O
c	O
and	O
for	O
some	O
cl	O
ii	O
kx	O
where	O
h	O
is	O
a	O
nonincreasing	O
function	O
on	O
with	O
ud	O
hu	O
as	O
u	O
kernel	B
rules	O
problem	O
consider	O
the	O
kernel	B
rule	B
with	O
kernel	B
kx	O
lllx	O
ilr	O
r	O
such	O
kernels	O
are	O
useless	O
for	O
atomic	O
distributions	O
unless	O
we	O
take	O
limits	O
and	O
define	O
gn	O
as	O
usual	O
when	O
x	O
tj	O
s	O
the	O
collection	O
of	O
points	O
z	O
with	O
xi	O
x	O
z	O
for	O
some	O
pair	O
j	O
for	O
xes	O
we	O
take	O
a	O
majority	B
vote	I
over	O
the	O
yis	O
for	O
which	O
xi	O
x	O
discuss	O
the	O
weak	B
universal	B
consistency	B
of	O
this	O
rule	B
which	O
has	O
the	O
curious	O
property	O
that	O
gn	O
is	O
invariant	O
to	O
the	O
smoothing	B
factor	I
h-so	O
we	O
might	O
as	O
well	O
set	O
h	O
without	O
loss	O
of	O
generality	O
note	O
also	O
that	O
for	O
r	O
d	O
is	O
kxdx	O
and	O
for	O
r	O
d	O
isc	O
kxdx	O
where	O
soi	O
is	O
the	O
unit	O
ball	O
of	O
nd	O
centered	O
at	O
the	O
origin	O
in	O
particular	O
if	O
r	O
d	O
by	O
considering	O
x	O
uniform	O
on	O
so	O
i	O
if	O
y	O
and	O
x	O
uniform	O
on	O
the	O
surface	O
of	O
so	O
i	O
if	O
y	O
show	O
that	O
even	O
though	O
l	O
the	O
probability	O
of	O
error	O
of	O
the	O
rule	B
may	O
tend	O
to	O
a	O
nonzero	O
limit	O
for	O
certain	O
values	O
of	O
p	O
i	O
hence	O
the	O
rule	B
is	O
not	O
universally	O
consistent	O
for	O
r	O
d	O
prove	O
or	O
disprove	O
the	O
weak	B
universal	B
consistency	B
noting	O
that	O
the	O
rules	O
decisions	O
are	O
by-and-iarge	O
based	O
on	O
the	O
few	O
nearest	O
neighbors	O
prove	O
the	O
rule	B
is	O
weakly	O
consistent	O
for	O
all	O
r	O
d	O
whenever	O
x	O
has	O
a	O
density	O
o	O
problem	O
assume	O
that	O
the	O
class	O
densities	O
coincide	O
that	O
is	O
ii	O
for	O
every	O
x	O
e	O
n	O
and	O
assume	O
p	O
p	O
show	O
that	O
the	O
expected	O
probability	O
of	O
error	O
of	O
the	O
kernel	B
rule	B
with	O
k	O
is	O
smaller	O
than	O
that	O
with	O
any	O
unimodal	O
regular	B
kernel	B
for	O
every	O
n	O
and	O
h	O
small	O
enough	O
exhibit	O
a	O
distribution	O
such	O
that	O
the	O
kernel	B
rule	B
with	O
a	O
symmetric	O
kernel	B
such	O
that	O
k	O
i	O
is	O
monotone	O
increasing	O
has	O
smaller	O
expected	O
error	O
probability	O
than	O
that	O
with	O
any	O
unimodal	O
regular	B
kernel	B
problem	O
scaling	O
assume	O
that	O
the	O
kernel	B
k	O
can	O
be	O
written	O
into	O
the	O
following	O
product	B
form	O
of	O
one-dimensional	O
kernels	O
kx	O
kxj	O
xed	O
n	O
kixi	O
d	O
i-i	O
assume	O
also	O
that	O
k	O
is	O
regular	B
one	O
can	O
use	O
different	O
smoothing	O
factors	O
along	O
the	O
different	O
coordinate	O
axes	O
to	O
define	O
a	O
kernel	B
rule	B
by	O
n	O
if	O
l	O
yi	O
k	O
i-i	O
d	O
xj	O
h	O
in	O
otherwise	O
where	O
xj	O
denotes	O
the	O
j	O
component	O
of	O
xi	O
prove	O
that	O
gn	O
is	O
strongly	O
universally	O
consistent	O
if	O
hin	O
for	O
all	O
i	O
d	O
and	O
hdn	O
problem	O
let	O
k	O
be	O
a	O
function	O
and	O
l	O
a	O
symmetric	O
positive	O
definite	O
d	O
x	O
d	O
matrix	O
for	O
x	O
e	O
n	O
d	O
define	O
kx	O
k	O
find	O
conditions	O
on	O
k	O
such	O
that	O
the	O
kernel	B
rule	B
with	O
kernel	B
k	O
is	O
universally	O
consistent	O
problem	O
prove	O
that	O
the	O
recursive	B
kernel	B
rule	B
defined	O
by	O
is	O
strongly	O
universally	O
consistent	O
if	O
k	O
is	O
a	O
regular	B
kernel	B
hn	O
and	O
nh	O
as	O
n	O
and	O
pawlak	O
problem	O
show	O
that	O
the	O
recursive	B
kernel	B
rule	B
of	O
is	O
strongly	O
universally	O
sistent	O
whenever	O
k	O
is	O
a	O
regular	B
kernel	B
limn	O
hxl	O
hn	O
and	O
ll	O
h	O
and	O
weaker	O
assumptions	O
on	O
the	O
kernel	B
they	O
assume	O
that	O
kx	O
c	O
lllxlllj	O
for	O
some	O
c	O
and	O
pawlak	O
note	O
greblicki	O
and	O
pawlak	O
showed	O
convergence	O
under	O
significantly	O
tion	O
on	O
with	O
ud	O
hu	O
as	O
u	O
they	O
also	O
showed	O
that	O
under	O
the	O
additional	O
that	O
for	O
some	O
ci	O
ci	O
hlixll	O
kx	O
where	O
h	O
is	O
a	O
nonincreasing	O
assumption	O
f	O
k	O
the	O
following	O
conditions	O
on	O
hn	O
are	O
necessary	O
and	O
sufficient	O
for	O
universal	B
consistency	B
problems	O
and	O
exercises	O
problem	O
open-ended	O
problem	O
let	O
py	O
i	O
given	O
y	O
let	O
x	O
be	O
formly	O
distributed	O
on	O
given	O
y	O
let	O
x	O
be	O
atomic	O
on	O
the	O
rationals	O
with	O
the	O
following	O
distribution	O
let	O
x	O
v	O
w	O
where	O
v	O
and	O
ware	O
independent	O
identically	O
distributed	O
and	O
pv	O
i	O
i	O
consider	O
the	O
kernel	B
rule	B
with	O
the	O
window	O
kernel	B
what	O
is	O
the	O
behavior	O
of	O
the	O
smoothing	B
factor	I
h	O
that	O
minimizes	O
the	O
expected	O
probability	O
of	O
error	O
eln	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
in	O
chapter	O
we	O
discuss	O
results	O
about	O
the	O
asymptotic	O
behavior	O
of	O
k-nearest	O
neighbor	O
classification	O
rules	O
where	O
the	O
value	O
of	O
k-the	O
number	O
of	O
neighbors	O
taken	O
into	O
account	O
at	O
the	O
decision-is	O
kept	O
at	O
a	O
fixed	O
number	O
as	O
the	O
size	O
of	O
the	O
training	O
data	O
n	O
increases	O
this	O
choice	O
leads	O
to	O
asymptotic	O
error	O
probabilities	O
smaller	O
than	O
but	O
no	O
universal	B
consistency	B
in	O
chapter	O
we	O
showed	O
that	O
if	O
we	O
let	O
k	O
grow	O
to	O
infinity	O
as	O
n	O
such	O
that	O
k	O
n	O
then	O
the	O
resulting	O
rule	B
is	O
weakly	O
consistent	O
the	O
main	O
purpose	O
of	O
this	O
chapter	O
is	O
to	O
demonstrate	O
strong	B
consistency	B
and	O
to	O
discuss	O
various	O
versions	O
of	O
the	O
rule	B
we	O
are	O
not	O
concerned	O
here	O
with	O
the	O
data-based	B
choice	O
of	O
k-that	O
subject	O
deserves	O
a	O
chapter	O
of	O
its	O
own	O
we	O
are	O
also	O
not	O
tackling	O
the	O
problem	O
of	O
the	O
selection	O
of	O
a	O
suitable-even	O
data-based-metric	O
at	O
the	O
end	O
of	O
this	O
chapter	O
and	O
in	O
the	O
exercises	O
we	O
draw	O
the	O
attention	O
to	O
i-nearest	O
neighbor	O
relabeling	B
rules	O
which	O
combine	O
the	O
computational	O
comfort	O
of	O
the	O
i-nearest	O
neighbor	O
rule	B
with	O
the	O
asymptotic	O
performance	O
of	O
variable-k	O
nearest	B
neighbor	I
rules	O
consistency	B
of	O
k-nearest	O
neighbor	O
classification	O
and	O
corresponding	O
regression	O
and	O
density	B
estimation	B
has	O
been	O
studied	O
by	O
many	O
researchers	O
see	O
fix	O
and	O
hodges	O
cover	O
stone	O
beck	O
gyorfi	O
and	O
gyorfi	O
devroye	O
collomb	O
bickel	O
and	O
breiman	O
mack	O
stute	O
devroye	O
and	O
gyorfi	O
bhattacharya	O
and	O
mack	O
zhao	O
and	O
devroye	O
gyorfi	O
krzyzak	O
and	O
lugosi	O
recall	O
the	O
definition	B
of	I
the	O
k-nearest	O
neighbor	O
rule	B
first	O
reorder	O
the	O
data	O
ylx	O
y	O
x	O
according	O
to	O
increasing	O
euclidean	B
distances	O
of	O
the	O
x	O
j	O
s	O
to	O
x	O
in	O
other	O
words	O
x	O
co	O
is	O
the	O
i	O
nearest	B
neighbor	I
of	O
x	O
among	O
the	O
points	O
xl	O
x	O
n	O
if	O
distance	O
ties	O
occur	O
consstency	O
of	O
the	O
k-nearest	O
neighbor	O
rule	B
a	O
tie-breaking	O
strategy	O
must	O
be	O
defined	O
if	O
fjv	O
is	O
absolutely	O
continuous	O
with	O
respect	O
to	O
the	O
lebesgue	O
measure	O
that	O
is	O
it	O
has	O
a	O
density	O
then	O
no	O
ties	O
occur	O
with	O
probability	O
one	O
so	O
formally	O
we	O
break	O
ties	O
by	O
comparing	O
indices	O
however	O
for	O
general	O
fjv	O
the	O
problem	O
of	O
distance	O
ties	O
turns	O
out	O
to	O
be	O
important	O
and	O
its	O
solution	O
is	O
messy	O
the	O
issue	O
of	O
tie	B
breaking	I
becomes	O
important	O
when	O
one	O
is	O
concerned	O
with	O
convergence	O
of	O
ln	O
with	O
probability	O
one	O
for	O
weak	B
universal	B
consistency	B
it	O
suffices	O
to	O
break	O
ties	O
by	O
comparing	O
indices	O
the	O
k-nn	B
classification	O
rule	B
is	O
defined	O
as	O
gnx	O
if	O
iyuxl	O
iycixo	O
otherwise	O
in	O
other	O
words	O
gn	O
is	O
a	O
majority	B
vote	I
among	O
the	O
labels	O
of	O
the	O
k	O
nearest	O
neighbors	O
ofx	O
strong	B
consistency	B
in	O
this	O
section	O
we	O
prove	O
theorem	O
we	O
assume	O
the	O
existence	O
of	O
a	O
density	O
for	O
fjv	O
so	O
that	O
we	O
can	O
avoid	O
messy	O
technicalities	O
necessary	O
to	O
handle	O
distance	O
ties	O
we	O
discuss	O
this	O
issue	O
briefly	O
in	O
the	O
next	O
section	O
the	O
following	O
result	O
implies	O
strong	B
consistency	B
whenever	O
x	O
has	O
an	O
absolutely	O
continuous	O
distribution	O
the	O
result	O
was	O
proved	O
by	O
devroye	O
and	O
gyorfi	O
and	O
zhao	O
the	O
proof	O
presented	O
here	O
basically	O
appears	O
in	O
devroye	O
gyorfi	O
krzyzak	O
and	O
lugosi	O
where	O
strong	B
universal	B
consistency	B
is	O
proved	O
under	O
an	O
appropriate	O
tie-breaking	O
strategy	O
discussion	O
later	O
some	O
of	O
the	O
main	O
ideas	O
appeared	O
in	O
the	O
proof	O
of	O
the	O
strong	B
universal	B
consistency	B
of	O
the	O
regular	B
histogram	B
rule	B
theorem	O
and	O
gyorfi	O
zhao	O
assume	O
that	O
fjv	O
has	O
a	O
density	O
if	O
k	O
and	O
k	O
n	O
then	O
for	O
every	O
e	O
there	O
is	O
an	O
no	O
such	O
that	O
forn	O
no	O
pln	O
l	O
e	O
where	O
yd	O
is	O
the	O
minimal	O
number	O
of	O
cones	O
centered	O
at	O
the	O
origin	O
of	O
angle	O
jr	O
that	O
cover	O
rd	O
the	O
definition	B
of	I
a	O
cone	O
see	O
chapter	O
thus	O
the	O
k-nn	B
rule	B
is	O
strongly	O
consistent	O
remark	O
at	O
first	O
glance	O
the	O
upper	O
bound	O
in	O
the	O
theorem	O
does	O
not	O
seem	O
to	O
depend	O
on	O
k	O
it	O
is	O
no	O
that	O
depends	O
on	O
the	O
sequence	O
of	O
ks	O
what	O
we	O
really	O
prove	O
is	O
the	O
following	O
for	O
every	O
e	O
there	O
exists	O
a	O
e	O
such	O
that	O
for	O
any	O
there	O
is	O
an	O
no	O
such	O
that	O
if	O
n	O
no	O
k	O
and	O
kin	O
the	O
exponential	B
inequality	B
holds	O
for	O
the	O
proof	O
we	O
need	O
a	O
generalization	O
of	O
lemma	O
the	O
role	O
of	O
this	O
covering	B
lemma	I
is	O
analogous	O
to	O
that	O
of	O
lemma	O
in	O
the	O
proof	O
of	O
consistency	B
of	B
kernel	B
rules	I
strong	B
consistency	B
lemma	O
and	O
gyorfi	O
let	O
bax	O
tlsxl	O
x-xll	O
a	O
then	O
for	O
all	O
x	O
e	O
rd	O
proof	O
for	O
x	O
e	O
rd	O
let	O
cx	O
s	O
c	O
rd	O
be	O
a	O
cone	O
of	O
angle	O
jr	O
centered	O
at	O
x	O
the	O
cone	O
consists	O
of	O
all	O
y	O
with	O
the	O
property	O
that	O
either	O
y	O
x	O
or	O
angley	O
x	O
s	O
jr	O
where	O
s	O
is	O
a	O
fixed	O
direction	O
if	O
y	O
y	O
e	O
cx	O
s	O
and	O
iix	O
yll	O
iix	O
yli	O
then	O
y	O
y	O
y	O
ii	O
this	O
follows	O
from	O
a	O
simple	O
geometric	B
argument	O
in	O
the	O
vector	O
space	O
spanned	O
by	O
x	O
y	O
and	O
y	O
the	O
proof	O
of	O
lemma	O
now	O
let	O
c	O
i	O
cyd	O
be	O
a	O
collection	O
of	O
cones	O
centered	O
at	O
x	O
with	O
different	O
central	O
direction	O
covering	O
rd	O
then	O
yd	O
tlbax	O
l	O
tlci	O
n	O
bax	O
il	O
let	O
x	O
e	O
ci	O
n	O
ba	O
then	O
by	O
the	O
property	O
of	O
the	O
cones	O
mentioned	O
above	O
we	O
have	O
fj	O
ci	O
n	O
sx	O
l	O
ii	O
n	O
bax	O
a	O
where	O
we	O
use	O
the	O
fact	O
that	O
x	O
e	O
bax	O
since	O
x	O
is	O
arbitrary	O
tlci	O
n	O
bax	O
a	O
which	O
completes	O
the	O
proof	O
of	O
the	O
lemma	O
an	O
immediate	O
consequence	O
of	O
the	O
lemma	O
is	O
that	O
the	O
number	O
of	O
points	O
among	O
x	O
i	O
xn	O
such	O
that	O
x	O
is	O
one	O
of	O
their	O
k	O
nearest	O
neighbors	O
is	O
not	O
more	O
than	O
a	O
constant	O
times	O
k	O
corollary	O
n	O
xi	O
in	O
kyd	O
il	O
proof	O
apply	O
lemma	O
with	O
a	O
k	O
n	O
and	O
let	O
tl	O
be	O
the	O
empirical	B
measure	I
tln	O
of	O
xl	O
xn	O
that	O
is	O
for	O
each	O
borel	O
set	O
a	O
r	O
d	O
tlna	O
ixi	O
ea	O
proof	O
of	O
theorem	O
since	O
the	O
decision	O
rule	B
gn	O
may	O
be	O
rewritten	O
as	O
gn	O
otherwise	O
where	O
is	O
the	O
corresponding	O
regression	B
function	I
estimate	O
k	O
k	O
lyix	O
il	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
the	O
statement	O
follows	O
from	O
theorem	O
if	O
we	O
show	O
that	O
for	O
sufficiently	O
large	O
n	O
p	O
ix	O
s	O
define	O
pn	O
as	O
the	O
solution	O
of	O
the	O
equation	O
note	O
that	O
the	O
absolute	O
continuity	O
of	O
j-l	O
implies	O
that	O
the	O
solution	O
always	O
exists	O
is	O
the	O
only	O
point	O
in	O
the	O
proof	O
where	O
we	O
use	O
this	O
assumption	O
also	O
define	O
the	O
basis	O
of	O
the	O
proof	O
is	O
the	O
following	O
decomposition	O
irjnx	O
irjx	O
rjxl	O
for	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
observe	O
that	O
denoting	O
rn	O
ii	O
xk	O
xii	O
where	O
fi	O
is	O
defined	O
as	O
with	O
y	O
replaced	O
by	O
the	O
constant	O
random	O
variable	B
y	O
and	O
is	O
the	O
corresponding	O
regression	B
function	I
thus	O
rx	O
first	O
we	O
show	O
that	O
the	O
expected	O
values	O
of	O
the	O
integrals	O
of	O
both	O
terms	O
on	O
the	O
hand	O
side	O
converge	O
to	O
zero	O
then	O
we	O
use	O
mcdiarmids	O
inequality	B
to	O
prove	O
that	O
both	O
terms	O
are	O
very	O
close	O
to	O
their	O
expected	O
values	O
with	O
large	O
probability	O
for	O
the	O
expected	O
value	O
of	O
the	O
first	O
term	O
on	O
the	O
right	O
side	O
of	O
using	O
the	O
cauchy-schwarz	B
inequality	B
we	O
have	O
e	O
f	O
e	O
f	O
iryx	O
f	O
je	O
strong	B
consistency	B
n	O
varlxes	O
k	O
xpn	O
x	O
n-isxpnx-idx	O
i	O
s	O
i	O
i	O
n	O
n	O
which	O
converges	O
to	O
zero	O
for	O
the	O
expected	O
value	O
of	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
note	O
that	O
in	O
the	O
proof	O
of	O
theorems	O
and	O
we	O
already	O
showed	O
that	O
lim	O
ei	O
o	O
n-oo	O
therefore	O
e	O
i	O
s	O
e	O
i	O
e	O
i	O
o	O
assume	O
now	O
that	O
n	O
is	O
so	O
large	O
that	O
e	O
i	O
ix	O
e	O
i	O
then	O
by	O
we	O
have	O
p	O
iryx	O
u	O
ix	O
ryxlfldx	O
e	O
i	O
ix	O
ixlfldx	O
e	O
i	O
ryxlfldx	O
ixlfldx	O
next	O
we	O
get	O
an	O
exponential	B
bound	O
for	O
the	O
first	O
probability	O
on	O
the	O
right-hand	O
side	O
of	O
by	O
mcdiarmids	O
inequality	B
fix	O
an	O
arbitrary	O
realization	O
of	O
the	O
data	O
dn	O
yi	O
yn	O
and	O
replace	O
yj	O
by	O
yi	O
changing	O
the	O
value	O
of	O
to	O
then	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
but	O
is	O
bounded	O
by	O
k	O
and	O
can	O
differ	O
from	O
zero	O
only	O
if	O
iix	O
xi	O
ii	O
pnx	O
or	O
iix	O
xi	O
ii	O
pnx	O
observe	O
that	O
iix	O
xiii	O
pnx	O
if	O
and	O
only	O
if	O
f	O
lsxlix-xlj	O
kin	O
but	O
the	O
measure	O
of	O
such	O
xs	O
is	O
bounded	O
by	O
ydkln	O
by	O
lemma	O
therefore	O
and	O
by	O
theorem	O
finally	O
we	O
need	O
a	O
bound	O
for	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
this	O
probability	O
may	O
be	O
bounded	O
by	O
mcdiarmids	O
inequality	B
exactly	O
the	O
same	O
way	O
as	O
for	O
the	O
first	O
term	O
obtaining	O
and	O
the	O
proof	O
is	O
completed	O
remark	O
the	O
conditions	O
k	O
and	O
kin	O
are	O
optimal	O
in	O
the	O
sense	O
that	O
they	O
are	O
also	O
necessary	O
for	O
consistency	B
for	O
some	O
distributions	O
with	O
a	O
density	O
however	O
for	O
some	O
distributions	O
they	O
are	O
not	O
necessary	O
for	O
consistency	B
and	O
in	O
fact	O
keeping	O
k	O
for	O
all	O
n	O
may	O
be	O
a	O
better	O
choice	O
this	O
latter	O
property	O
dealt	O
with	O
in	O
problem	O
shows	O
that	O
the	O
i-nearest	O
neighbor	O
rule	B
is	O
admissible	O
breaking	O
distance	O
ties	O
theorem	O
provides	O
strong	B
consistency	B
under	O
the	O
assumption	O
that	O
x	O
has	O
a	O
density	O
this	O
assumption	O
was	O
needed	O
to	O
avoid	O
problems	O
caused	O
by	O
equal	O
distances	O
turning	O
to	O
the	O
general	O
case	O
we	O
see	O
that	O
if	O
f	O
l	O
does	O
not	O
have	O
a	O
density	O
then	O
distance	O
ties	O
can	O
occur	O
with	O
nonzero	O
probability	O
so	O
we	O
have	O
to	O
deal	O
with	O
the	O
problem	O
of	O
breaking	O
them	O
to	O
see	O
that	O
the	O
density	O
assumption	O
cannot	O
be	O
relaxed	O
to	O
the	O
condition	O
that	O
f	O
l	O
is	O
merely	O
nonatomic	O
without	O
facing	O
frequent	O
distance	O
ties	O
consider	O
the	O
following	O
distribution	O
on	O
n	O
d	O
x	O
n	O
d	O
with	O
d	O
d	O
f	O
l	O
x	O
ad	O
x	O
td	O
where	O
td	O
denotes	O
the	O
uniform	O
distribution	O
on	O
the	O
surface	O
of	O
the	O
unit	O
sphere	B
of	O
n	O
d	O
and	O
ad	O
denotes	O
the	O
unit	O
point	O
mass	O
at	O
the	O
origin	O
of	O
nd	O
observe	O
that	O
if	O
x	O
has	O
distribution	O
td	O
x	O
ad	O
and	O
x	O
has	O
distribution	O
ad	O
x	O
td	O
then	O
ilx	O
xii	O
ji	O
breaking	O
distance	O
ties	O
hence	O
if	O
xl	O
x	O
x	O
are	O
independent	O
with	O
distribution	O
j-l	O
thenpiix	O
i	O
next	O
we	O
list	O
some	O
methods	O
of	O
breaking	O
distance	O
ties	O
ne-breaking	O
by	O
indices	O
if	O
xi	O
and	O
xj	O
are	O
equidistant	O
from	O
x	O
then	O
xi	O
is	O
declared	O
closer	O
if	O
i	O
j	O
this	O
method	O
has	O
some	O
undesirable	O
properties	O
for	O
example	O
if	O
x	O
is	O
monoatomic	O
with	O
then	O
x	O
is	O
the	O
nearest	B
neighbor	I
of	O
all	O
x	O
j	O
j	O
but	O
x	O
j	O
is	O
only	O
the	O
j	O
i-st	O
nearest	B
neighbor	I
of	O
x	O
the	O
influence	O
of	O
x	O
in	O
such	O
a	O
situation	O
is	O
too	O
large	O
making	O
the	O
estimate	O
very	O
unstable	O
and	O
thus	O
undesirable	O
in	O
fact	O
in	O
this	O
monoatomic	O
case	O
if	O
l	O
e	O
p	O
l	O
e	O
e	O
for	O
some	O
c	O
problem	O
thus	O
we	O
cannot	O
expect	O
a	O
free	O
version	O
of	O
theorem	O
with	O
this	O
tie-breaking	O
method	O
stones	O
tie-breaking	O
stone	O
introduced	O
a	O
version	O
of	O
the	O
nearest	B
neighbor	I
rule	B
where	O
the	O
labels	O
of	O
the	O
points	O
having	O
the	O
same	O
distance	O
from	O
x	O
as	O
the	O
k-th	O
nearest	B
neighbor	I
are	O
averaged	O
if	O
we	O
denote	O
the	O
distance	O
of	O
the	O
k-th	O
nearest	B
neighbor	I
to	O
x	O
by	O
rn	O
then	O
stones	O
rule	B
is	O
the	O
following	O
o	O
l	O
ii	O
lix	O
xi	O
ii	O
iyio	O
il	O
ilx	O
xi	O
ii	O
r	O
x	O
xl	O
n	O
x	O
k	O
gnx	O
k	O
x	O
i	O
illx-xdlrx	O
otherwise	O
this	O
is	O
not	O
a	O
k-nearest	O
neighbor	O
rule	B
in	O
a	O
strict	O
sense	O
since	O
this	O
estimate	O
in	O
general	O
uses	O
more	O
than	O
k	O
neighbors	O
stone	O
proved	O
weak	B
universal	B
consistency	B
of	O
this	O
rule	B
adding	O
a	O
random	O
component	O
to	O
circumvent	O
the	O
aforementioned	O
ties	O
we	O
may	O
artificially	O
increase	O
the	O
dimension	B
of	O
the	O
feature	O
vector	O
by	O
one	O
define	O
the	O
the	O
d	O
i-dimensional	O
random	O
vectors	O
where	O
the	O
randomizing	O
variables	O
v	O
vi	O
vn	O
are	O
real-valued	O
i	O
i	O
d	O
dom	O
variables	O
independent	O
of	O
x	O
y	O
and	O
dn	O
and	O
their	O
common	O
distribution	O
has	O
a	O
density	O
clearly	O
because	O
of	O
the	O
independence	O
of	O
v	O
the	O
bayes	B
error	I
corresponding	O
to	O
the	O
pair	O
y	O
is	O
the	O
same	O
as	O
that	O
of	O
y	O
the	O
algorithm	B
performs	O
the	O
k-nearest	O
neighbor	O
rule	B
on	O
the	O
modified	O
data	O
set	O
d	O
x	O
yd	O
yn	O
it	O
finds	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
and	O
uses	O
a	O
majority	B
vote	I
among	O
these	O
labels	O
to	O
guess	O
y	O
since	O
v	O
has	O
a	O
density	O
and	O
is	O
independent	O
of	O
x	O
distance	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
ties	O
occur	O
with	O
zero	O
probability	O
strong	B
universal	B
consistency	B
of	O
this	O
rule	B
can	O
be	O
seen	O
by	O
observing	O
that	O
the	O
proof	O
of	O
theorem	O
used	O
the	O
existence	O
of	O
the	O
density	O
in	O
the	O
definition	B
of	I
pnx	O
only	O
with	O
our	O
randomization	O
pnx	O
is	O
well-defined	O
and	O
the	O
same	O
proof	O
yields	O
strong	B
universal	B
consistency	B
estingly	O
this	O
rule	B
is	O
consistent	O
whenever	O
u	O
has	O
a	O
density	O
and	O
is	O
independent	O
of	O
y	O
if	O
for	O
example	O
the	O
magnitude	O
of	O
u	O
is	O
much	O
larger	O
than	O
that	O
of	O
ii	O
x	O
ii	O
then	O
the	O
rule	B
defined	O
this	O
way	O
will	O
significantly	O
differ	O
from	O
the	O
nearest	B
neighbor	I
rule	B
though	O
it	O
still	O
preserves	O
universal	B
consistency	B
one	O
should	O
expect	O
however	O
a	O
dramatic	O
decrease	O
in	O
the	O
performance	O
of	O
course	O
if	O
u	O
is	O
very	O
small	O
then	O
the	O
rule	B
remains	O
intuitively	O
appealing	O
tie-breaking	O
by	O
randomization	O
there	O
is	O
another	O
perhaps	O
more	O
natural	O
way	O
of	O
breaking	O
ties	O
via	O
randomization	O
we	O
assume	O
that	O
u	O
is	O
a	O
random	O
vector	O
independent	O
of	O
the	O
data	O
where	O
u	O
is	O
independent	O
of	O
x	O
and	O
uniformly	O
distributed	O
on	O
we	O
also	O
artificially	O
enlarge	O
the	O
data	O
by	O
introducing	O
un	O
where	O
the	O
us	O
are	O
i	O
i	O
d	O
uniform	O
as	O
well	O
thus	O
each	O
ui	O
is	O
distributed	O
as	O
u	O
let	O
u	O
yclx	O
u	O
u	O
ycnx	O
u	O
be	O
a	O
reordering	O
of	O
the	O
data	O
according	O
to	O
increasing	O
values	O
of	O
ilx	O
xi	O
ii	O
in	O
case	O
of	O
distance	O
ties	O
we	O
declare	O
ui	O
closer	O
to	O
u	O
than	O
j	O
uj	O
provided	O
that	O
lui	O
ul	O
juj	O
uj	O
define	O
the	O
k-nn	B
classification	O
rule	B
as	O
gnx	O
if	O
ll	O
iycicxul	O
ll	O
iyicxuo	O
otherwise	O
and	O
denote	O
the	O
error	O
probability	O
of	O
gn	O
by	O
devroye	O
gyorfi	O
krzyzak	O
and	O
lugosi	O
proved	O
that	O
ln	O
l	O
with	O
probability	O
one	O
for	O
all	O
distributions	O
if	O
k	O
and	O
k	O
n	O
the	O
basic	O
argument	O
in	O
is	O
the	O
same	O
as	O
that	O
of	O
theorem	O
except	O
that	O
the	O
covering	B
lemma	I
has	O
to	O
be	O
appropriately	O
modified	O
it	O
should	O
be	O
stressed	O
again	O
that	O
if	O
fl	O
has	O
a	O
density	O
or	O
just	O
has	O
an	O
absolutely	O
tinuous	O
component	O
then	O
tie-breaking	O
is	O
needed	O
with	O
zero	O
probability	O
and	O
becomes	O
therefore	O
irrelevant	O
recursive	B
methods	O
to	O
find	O
the	O
nearest	B
neighbor	I
of	O
a	O
point	O
x	O
among	O
xl	O
x	O
n	O
we	O
may	O
preprocess	O
the	O
data	O
in	O
log	O
n	O
time	O
such	O
that	O
each	O
query	O
may	O
be	O
answered	O
in	O
n	O
scale-invariant	O
rules	O
worst-case	O
time-see	O
for	O
example	O
preparata	O
and	O
shamos	O
other	O
recent	O
developments	O
in	O
computational	O
geometry	O
have	O
made	O
the	O
nearest	B
neighbor	I
rules	O
computationally	O
feasible	O
even	O
when	O
n	O
is	O
formidable	O
without	O
preprocessing	O
ever	O
one	O
must	O
resort	O
to	O
slow	O
methods	O
if	O
we	O
need	O
to	O
find	O
a	O
decision	O
at	O
x	O
and	O
want	O
to	O
process	O
the	O
data	O
file	O
once	O
when	O
doing	O
so	O
a	O
simple	O
rule	B
was	O
proposed	O
by	O
vroye	O
and	O
wise	O
it	O
is	O
a	O
fully	O
recursive	B
rule	B
that	O
may	O
be	O
updated	O
as	O
more	O
observations	O
become	O
available	O
split	O
the	O
data	O
sequence	O
dn	O
into	O
disjoint	O
blocks	O
of	O
length	O
ii	O
in	O
where	O
ii	O
in	O
are	O
positive	O
integers	O
satisfying	O
ii	O
n	O
in	O
each	O
block	O
find	O
the	O
nearest	B
neighbor	I
of	O
x	O
and	O
denote	O
the	O
nearest	B
neighbor	I
of	O
x	O
from	O
the	O
i-th	O
block	O
by	O
let	O
ytx	O
be	O
the	O
corresponding	O
label	O
ties	O
are	O
broken	O
by	O
comparing	O
indices	O
the	O
classification	O
rule	B
is	O
defined	O
as	O
a	O
majority	B
vote	I
among	O
the	O
nearest	O
neighbors	O
from	O
each	O
block	O
note	O
that	O
we	O
have	O
only	O
defined	O
the	O
rule	B
gn	O
for	O
n	O
satisfying	O
ll	O
ii	O
n	O
for	O
some	O
n	O
a	O
possible	O
extension	O
for	O
all	O
ns	O
is	O
given	O
by	O
gnx	O
gmx	O
where	O
m	O
is	O
the	O
largest	O
integer	O
not	O
exceeding	O
n	O
that	O
can	O
be	O
written	O
as	O
ll	O
ii	O
for	O
some	O
n	O
the	O
rule	B
is	O
weakly	O
universally	O
consistent	O
if	O
lim	O
in	O
n---oo	O
and	O
wise	O
see	O
problem	O
scale-invariant	O
rules	O
a	O
scale-invariant	O
rule	B
is	O
a	O
rule	B
that	O
is	O
invariant	O
under	O
rescalings	O
of	O
the	O
components	O
it	O
is	O
motivated	O
by	O
the	O
lack	O
of	O
a	O
universal	O
yardstick	O
when	O
components	O
of	O
a	O
tor	O
represent	O
physically	O
different	O
quantities	O
such	O
as	O
temperature	O
blood	O
pressure	O
alcohol	O
and	O
the	O
number	O
of	O
lost	O
teeth	O
more	O
formally	O
let	O
xi	O
xed	O
be	O
the	O
d	O
components	O
of	O
a	O
vector	O
x	O
if	O
d	O
are	O
strictly	O
monotone	O
mappings	O
n	O
n	O
and	O
if	O
we	O
define	O
dxd	O
then	O
gn	O
is	O
scale-invariant	O
if	O
gnx	O
dn	O
dl	O
where	O
yn	O
in	O
other	O
words	O
if	O
all	O
the	O
xis	O
and	O
x	O
are	O
transformed	O
in	O
the	O
same	O
manner	O
the	O
decision	O
does	O
not	O
change	O
some	O
rules	O
based	O
on	O
statistically	B
equivalent	I
blocks	I
in	O
chapters	O
and	O
are	O
scale-invariant	O
while	O
the	O
k-nearest	O
neighbor	O
rule	B
clearly	O
is	O
not	O
here	O
we	O
describe	O
a	O
scale-invariant	O
modification	O
of	O
the	O
k-nearest	O
neighbor	O
rule	B
suggested	O
by	O
olshen	O
and	O
devroye	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
the	O
scale-invariant	O
k-nearest	O
neighbor	O
rule	B
is	O
based	O
upon	O
empirical	B
distances	O
that	O
are	O
defined	O
in	O
terms	O
of	O
the	O
order	B
statistics	I
along	O
the	O
d	O
coordinate	O
axes	O
first	O
order	O
the	O
points	O
x	O
xl	O
xn	O
according	O
to	O
increasing	O
values	O
of	O
their	O
first	O
ponents	O
xi	O
xii	O
xl	O
breaking	O
ties	O
via	O
randomization	O
denote	O
the	O
rank	O
of	O
xii	O
by	O
r	O
and	O
the	O
rank	O
of	O
xl	O
by	O
rl	O
repeating	O
the	O
same	O
procedure	O
for	O
the	O
other	O
coordinates	O
we	O
obtain	O
the	O
ranks	O
ri	O
d	O
l	O
n	O
define	O
the	O
empirical	B
distance	O
between	O
x	O
and	O
xi	O
by	O
px	O
xi	O
max	O
irj	O
ijd	O
i	O
a	O
k-nn	B
rule	B
can	O
be	O
defined	O
based	O
on	O
these	O
distances	O
by	O
a	O
majority	B
vote	I
among	O
the	O
yis	O
with	O
the	O
corresponding	O
xis	O
whose	O
empirical	B
distance	O
from	O
x	O
are	O
among	O
the	O
k	O
smallest	O
since	O
these	O
distances	O
are	O
integer-valued	O
ties	O
frequently	O
occur	O
these	O
ties	O
should	O
be	O
broken	O
by	O
randomization	O
devroye	O
proved	O
that	O
this	O
rule	B
randomized	B
tie-breaking	O
is	O
weakly	O
universally	O
consistent	O
when	O
k	O
and	O
kj	O
n	O
problem	O
for	O
another	O
consistent	O
scale-invariant	O
nearest	B
neighbor	I
rule	B
we	O
refer	O
to	O
problem	O
ii	O
ji	O
figure	O
scale-invariant	O
distances	O
of	O
points	O
from	O
a	O
fixed	O
point	O
are	O
shown	O
here	O
weighted	B
nearest	B
neighbor	I
rules	O
in	O
the	O
k-nn	B
rule	B
each	O
of	O
the	O
k	O
nearest	O
neighbors	O
of	O
a	O
point	O
x	O
plays	O
an	O
equally	O
tant	O
role	O
in	O
the	O
decision	O
however	O
intuitively	O
speaking	O
nearer	O
neighbors	O
should	O
rotation-invariant	B
rules	O
provide	O
more	O
information	O
than	O
more	O
distant	O
ones	O
royall	O
first	O
suggested	O
using	O
rules	O
in	O
which	O
the	O
labels	O
yi	O
are	O
given	O
unequal	O
voting	O
powers	O
in	O
the	O
decision	O
according	O
to	O
the	O
distances	O
of	O
the	O
xis	O
from	O
x	O
the	O
i	O
nearest	B
neighbor	I
receives	O
weight	O
wni	O
where	O
usually	O
wnl	O
w	O
whn	O
and	O
wni	O
the	O
rule	B
is	O
defined	O
as	O
wnjyixl	O
wni	O
iyixol	O
gn	O
otherwise	O
we	O
get	O
the	O
ordinary	B
k-nearest	O
neighbor	O
rule	B
back	O
by	O
the	O
choice	O
wni	O
otherwise	O
if	O
i	O
k	O
k	O
the	O
following	O
conditions	O
for	O
consistency	B
were	O
established	O
by	O
stone	O
and	O
lim	O
max	O
wni	O
n--hx	O
lin	O
wni	O
lim	O
n-oo	O
for	O
some	O
k	O
with	O
kin	O
problem	O
weighted	B
versions	O
of	O
the	O
recursive	B
and	O
scale-invariant	O
methods	O
described	O
above	O
can	O
also	O
be	O
defined	O
similarly	O
kin	O
rotation-invariant	B
rules	O
assume	O
that	O
an	O
affine	O
transformation	O
t	O
is	O
applied	O
to	O
x	O
and	O
xl	O
xn	O
any	O
number	O
of	O
combinations	O
of	O
rotations	O
translations	O
and	O
linear	O
rescalings	O
and	O
that	O
for	O
any	O
such	O
linear	O
transformation	O
t	O
gnx	O
dn	O
gntx	O
d	O
where	O
d	O
txr	O
yr	O
then	O
we	O
call	O
gn	O
rotation-invariant	B
rotation-invariance	O
is	O
indeed	O
a	O
very	O
strong	O
property	O
in	O
r	O
d	O
in	O
the	O
context	O
of	O
k-nn	B
estimates	O
it	O
suffices	O
to	O
be	O
able	O
to	O
define	O
a	O
rotation-invariant	B
distance	O
measure	O
these	O
are	O
necessarily	O
data-dependent	B
an	O
example	O
of	O
this	O
goes	O
as	O
follows	O
any	O
collection	O
of	O
d	O
points	O
in	O
general	O
position	O
defines	O
a	O
polyhedron	O
in	O
a	O
hyperplane	O
of	O
rd	O
for	O
points	O
xij	O
we	O
denote	O
this	O
polyhedron	O
by	O
pul	O
id	O
then	O
we	O
define	O
the	O
distance	O
pxi	O
x	O
l	O
isegmentxi	O
xintersectspil	O
il	O
ihil	O
idl	O
near	O
points	O
have	O
few	O
intersections	O
using	O
p	O
in	O
a	O
k-nn	B
rule	B
with	O
k	O
and	O
k	O
n	O
we	O
expect	O
weak	B
universal	B
consistency	B
under	O
an	O
appropriate	O
scheme	O
of	O
tie-breaking	O
the	O
answer	O
to	O
this	O
is	O
left	O
as	O
an	O
open	O
problem	O
for	O
the	O
scholars	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
figure	O
rotation-invariant	B
dis	O
tancesfrom	O
x	O
relabeling	B
rules	O
the	O
i-nn	B
rule	B
appeals	O
to	O
the	O
masses	O
who	O
crave	O
simplicity	O
and	O
attracts	O
the	O
grammers	O
who	O
want	O
to	O
write	O
short	O
understandable	O
code	O
nearest	O
neighbors	O
may	O
be	O
found	O
efficiently	O
if	O
the	O
data	O
are	O
preprocessed	O
chapter	O
for	O
references	O
can	O
we	O
make	O
the	O
i-nn	B
rule	B
universally	O
consistent	O
as	O
well	O
in	O
this	O
section	O
we	O
introduce	O
a	O
tool	O
called	O
relabeling	B
which	O
works	O
as	O
follows	O
assume	O
that	O
we	O
have	O
a	O
tion	O
rule	B
dn	O
x	O
e	O
n	O
d	O
n	O
i	O
where	O
dn	O
is	O
the	O
data	O
yl	O
yn	O
this	O
rule	B
will	O
be	O
called	O
the	O
ancestral	B
rule	B
define	O
the	O
labels	O
these	O
are	O
the	O
decisions	O
for	O
the	O
xis	O
themselves	O
obtained	O
by	O
mere	O
resubstitution	B
in	O
the	O
relabeling	B
method	O
we	O
apply	O
the	O
i-nn	B
rule	B
to	O
the	O
new	O
data	O
zl	O
zn	O
if	O
all	O
goes	O
well	O
when	O
the	O
ancestral	B
rule	B
gn	O
is	O
universally	O
consistent	O
so	O
should	O
the	O
relabeling	B
rule	B
we	O
will	O
show	O
this	O
by	O
example	O
starting	O
from	O
a	O
consistent	O
k-nn	B
rule	B
as	O
ancestral	B
rule	B
k	O
kin	O
unfortunately	O
relabeling	B
rules	O
do	O
not	O
always	O
inherit	O
consistency	B
from	O
their	O
ancestral	O
rules	O
so	O
that	O
a	O
more	O
general	O
theorem	O
is	O
more	O
difficult	O
to	O
obtain	O
unless	O
one	O
adds	O
in	O
a	O
lot	O
of	O
regularity	O
conditions-this	O
does	O
not	O
seem	O
to	O
be	O
the	O
right	O
time	O
for	O
that	O
sort	O
of	O
effort	O
to	O
see	O
that	O
universal	B
consistency	B
of	O
the	O
ancestral	B
rule	B
does	O
not	O
imply	O
consistency	B
of	O
the	O
relabeling	B
rule	B
consider	O
the	O
following	O
rule	B
hn	O
if	O
x	O
xi	O
and	O
x	O
x	O
j	O
all	O
j	O
i	O
otherwise	O
where	O
gn	O
is	O
a	O
weakly	O
universally	O
consistent	O
rule	B
it	O
is	O
easy	O
to	O
show	O
problem	O
that	O
hn	O
is	O
universally	O
consistent	O
as	O
well	O
changing	O
a	O
rule	B
on	O
a	O
set	O
of	O
measure	O
zero	O
indeed	O
does	O
not	O
affect	O
ln	O
also	O
if	O
x	O
xi	O
is	O
at	O
an	O
atom	O
of	O
the	O
distribution	O
of	O
x	O
we	O
only	O
change	O
gn	O
to	O
yi	O
if	O
xi	O
is	O
the	O
sole	O
occurrence	O
of	O
that	O
atom	O
in	O
the	O
data	O
this	O
has	O
asymptotically	O
no	O
impact	O
on	O
ln	O
however	O
if	O
hn	O
is	O
used	O
as	O
an	O
ancestral	B
rule	B
and	O
x	O
is	O
nonatomic	O
then	O
hnxi	O
dn	O
yi	O
for	O
all	O
i	O
and	O
therefore	O
the	O
relabeling	B
rule	B
is	O
a	O
i-nn	B
rule	B
based	O
on	O
the	O
data	O
yl	O
yn	O
if	O
l	O
for	O
the	O
distribution	O
of	O
y	O
then	O
the	O
relabeling	B
rule	B
has	O
probability	O
of	O
error	O
converging	O
to	O
one	O
for	O
most	O
nonpathological	O
ancestral	O
rules	O
relabeling	B
does	O
indeed	O
preserve	O
versal	O
consistency	B
we	O
offer	O
a	O
prototype	B
proof	O
for	O
the	O
k-nn	B
rule	B
relabeling	B
rules	O
theorem	O
let	O
gn	O
be	O
the	O
k-nn	B
rule	B
in	O
which	O
tie-breaking	O
is	O
done	O
by	O
ization	O
as	O
in	O
assume	O
that	O
k	O
and	O
k	O
n	O
that	O
gn	O
is	O
weakly	O
universally	O
consistent	O
then	O
the	O
relabeling	B
rule	B
based	O
upon	O
gn	O
is	O
weakly	O
sally	O
consistent	O
as	O
well	O
proof	O
we	O
verify	O
the	O
conditions	O
of	O
stones	O
weak	B
convergence	O
theorem	O
rem	O
to	O
keep	O
things	O
simple	O
we	O
assume	O
that	O
the	O
distribution	O
of	O
x	O
has	O
a	O
density	O
so	O
that	O
distance	O
ties	O
happen	O
with	O
probability	O
zero	O
in	O
our	O
case	O
the	O
weight	O
wnix	O
of	O
theorem	O
equals	O
k	O
iff	O
xi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
xlx	O
where	O
xlx	O
is	O
the	O
nearest	B
neighbor	I
of	O
x	O
it	O
is	O
zero	O
otherwise	O
condition	O
is	O
trivially	O
satisfied	O
since	O
k	O
for	O
condition	O
we	O
note	O
that	O
if	O
xix	O
denotes	O
the	O
i-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
x	O
n	O
then	O
just	O
note	O
that	O
and	O
that	O
the	O
latter	O
sphere	B
contains	O
k	O
data	O
points	O
but	O
we	O
already	O
know	O
from	O
the	O
proof	O
of	O
weak	B
consistency	B
of	O
the	O
k-nn	B
rule	B
that	O
if	O
k	O
n	O
then	O
for	O
all	O
e	O
p	O
xii	O
e	O
finally	O
we	O
consider	O
condition	O
here	O
we	O
have	O
arguing	O
partially	O
as	O
in	O
stone	O
e	O
ixisamongtheknnsofxcjxiin	O
the	O
roles	O
of	O
xi	O
and	O
x	O
n	O
n	O
ek	O
llixj	O
is	O
the	O
nn	O
of	O
xi	O
in	O
xn	O
x-xd	O
il	O
jl	O
x	O
ix	O
is	O
among	O
the	O
k	O
nns	O
of	O
xj	O
in	O
however	O
by	O
lemma	O
n	O
l	O
il	O
ixj	O
is	O
thenn	O
of	O
xi	O
in	O
yd	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
also	O
n	O
l	O
ix	O
is	O
among	O
thek	O
nns	O
of	O
xj	O
in	O
kyd	O
jl	O
therefore	O
by	O
a	O
double	O
application	O
of	O
lemma	O
e	O
t	O
irx	O
jong	O
thok	O
nn	O
of	O
xoxjj	O
x	O
yj	O
efx	O
and	O
condition	O
is	O
verified	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
the	O
conditions	O
k	O
and	O
k	O
n	O
are	O
necessary	O
for	O
universal	O
bounded	O
lim	O
inf	O
n-hx	O
eln	O
l	O
exhibit	O
a	O
second	O
distribution	O
such	O
that	O
if	O
k	O
n	O
e	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
that	O
is	O
exhibit	O
a	O
distribution	O
such	O
that	O
if	O
k	O
remains	O
for	O
all	O
n	O
and	O
some	O
e	O
then	O
lim	O
infn-	O
hx	O
eln	O
l	O
problem	O
let	O
x	O
be	O
monoatomic	O
with	O
show	O
that	O
for	O
e	O
for	O
some	O
c	O
where	O
ln	O
is	O
the	O
error	O
probability	O
of	O
the	O
k-nn	B
rule	B
with	O
tie-breaking	O
by	O
indices	O
problem	O
prove	O
that	O
the	O
recursive	B
nearest	B
neighbor	I
rule	B
is	O
universally	O
consistent	O
vided	O
that	O
limn-oo	O
in	O
and	O
wise	O
hint	O
check	O
the	O
conditions	O
of	O
theorem	O
problem	O
prove	O
that	O
the	O
nearest	B
neighbor	I
rule	B
defined	O
by	O
any	O
l	O
p-distance	O
measure	O
p	O
is	O
universally	O
consistent	O
under	O
the	O
usual	O
conditions	O
on	O
k	O
the	O
l	O
p	O
between	O
x	O
y	O
end	O
is	O
defined	O
by	O
yiip	O
rip	O
foro	O
p	O
ooandbysupi	O
yi	O
i	O
for	O
p	O
where	O
x	O
xed	O
hint	O
check	O
the	O
conditions	O
of	O
theorem	O
problem	O
let	O
o-x	O
z	O
xi	O
zd	O
px	O
xi	O
izi	O
zl	O
be	O
a	O
generalized	B
distance	O
between	O
z	O
and	O
zi	O
where	O
x	O
xl	O
xn	O
are	O
as	O
in	O
the	O
description	O
of	O
the	O
scale-invariantk-nn	O
rule	B
px	O
xi	O
is	O
the	O
empirical	B
distance	O
defined	O
there	O
and	O
z	O
zi	O
e	O
are	O
real	O
numbers	O
added	O
to	O
break	O
ties	O
at	O
random	O
the	O
sequence	O
zl	O
zn	O
is	O
i	O
i	O
d	O
uniform	O
and	O
is	O
independent	O
of	O
the	O
data	O
dn	O
with	O
the	O
k-nn	B
rule	B
based	O
on	O
the	O
artificial	O
distances	O
show	O
that	O
the	O
rule	B
is	O
universally	O
consistent	O
by	O
verifying	O
the	O
conditions	O
of	O
theorem	O
when	O
k	O
and	O
kin	O
in	O
particular	O
show	O
first	O
that	O
if	O
z	O
is	O
uniform	O
and	O
independent	O
of	O
x	O
y	O
dn	O
and	O
zi	O
zn	O
and	O
if	O
wnix	O
z	O
is	O
the	O
weight	O
of	O
zi	O
in	O
this	O
k-nn	B
rule	B
it	O
is	O
k	O
iff	O
zi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
z	O
according	O
to	O
then	O
e	O
wnix	O
zfxd	O
efx	O
for	O
all	O
nonnegative	O
measurable	O
f	O
with	O
efx	O
problems	O
and	O
exercises	O
if	O
kin	O
then	O
for	O
all	O
a	O
o	O
hint	O
check	O
the	O
conditions	O
of	O
theorem	O
problem	O
the	O
layered	B
nearest	B
neighbor	I
rule	B
partitions	O
the	O
space	O
at	O
x	O
into	O
rants	O
in	O
each	O
quadrant	O
the	O
outer-layer	O
points	O
are	O
marked	O
that	O
is	O
those	O
xi	O
for	O
which	O
the	O
hyperrectangle	O
defined	O
by	O
x	O
and	O
xi	O
contains	O
no	O
other	O
data	O
point	O
then	O
it	O
takes	O
a	O
majority	B
vote	I
over	O
the	O
yi	O
for	O
the	O
marked	O
points	O
observe	O
that	O
this	O
rule	B
is	O
scale-invariant	O
show	O
that	O
whenever	O
x	O
has	O
nonatornic	O
marginals	O
avoid	O
ties	O
eln	O
l	O
in	O
probability	O
i	O
figure	O
the	O
layered	B
nearest	B
neighbor	I
rule	B
takes	O
a	O
majority	B
vote	I
over	O
the	O
marked	O
points	O
xl	O
i	O
empty	O
i	O
hint	O
it	O
suffices	O
to	O
show	O
that	O
the	O
number	O
of	O
marked	O
points	O
increases	O
unboundedly	O
in	O
ability	O
and	O
that	O
its	O
proportion	O
to	O
unmarked	O
points	O
tends	O
to	O
zero	O
in	O
probability	O
problem	O
prove	O
weak	B
universal	B
consistency	B
of	O
the	O
weighted	B
nearest	B
neighbor	I
rule	B
if	O
the	O
weights	O
satisfy	O
lim	O
max	O
wni	O
n---oo	O
lsisn	O
and	O
lim	O
n---oo	O
ksisn	O
wni	O
for	O
some	O
k	O
with	O
kin	O
hint	O
check	O
the	O
conditions	O
of	O
theorem	O
problem	O
if	O
wnn	O
is	O
a	O
probability	O
vector	O
then	O
limn---oo	O
lino	O
wni	O
for	O
all	O
if	O
and	O
only	O
if	O
there	O
exists	O
a	O
sequence	O
of	O
integers	O
k	O
kn	O
such	O
that	O
k	O
on	O
k	O
and	O
lik	O
wni	O
show	O
this	O
conclude	O
that	O
the	O
conditions	O
of	O
problem	O
are	O
equivalent	O
to	O
lim	O
max	O
wni	O
n---oo	O
lsisn	O
lim	O
wni	O
for	O
all	O
o	O
n---oo	O
ino	O
problem	O
verify	O
the	O
conditions	O
of	O
problems	O
and	O
for	O
weight	O
vectors	O
of	O
the	O
form	O
wni	O
cn	O
i	O
i	O
ci	O
where	O
a	O
is	O
a	O
constant	O
and	O
cn	O
is	O
a	O
normalizing	O
constant	O
in	O
particular	O
check	O
that	O
they	O
do	O
not	O
hold	O
for	O
a	O
but	O
that	O
they	O
do	O
hold	O
for	O
a	O
problem	O
consider	O
as	O
weight	O
vector	O
show	O
that	O
the	O
conditions	O
of	O
problem	O
hold	O
if	O
pn	O
and	O
npn	O
iii	O
i	O
n	O
w	O
pn	O
pny	O
x	O
pn-n	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
problem	O
let	O
wni	O
pz	O
ipz	O
n	O
i	O
n	O
where	O
z	O
is	O
a	O
poisson	O
random	O
variable	B
show	O
that	O
there	O
is	O
no	O
choice	O
of	O
an	O
such	O
that	O
i	O
n	O
is	O
a	O
consistent	O
weight	O
sequence	O
following	O
the	O
conditions	O
of	O
problem	O
problem	O
let	O
wni	O
pbinomialn	O
pn	O
i	O
pnn-i	O
derive	O
conditions	O
on	O
pn	O
for	O
this	O
choice	O
of	O
weight	O
sequence	O
to	O
be	O
consistent	O
in	O
the	O
sense	O
of	O
problem	O
problem	O
k-nn	B
density	B
estimation	B
we	O
recall	O
from	O
problem	O
that	O
if	O
the	O
ditional	O
densities	O
exist	O
then	O
l	O
density	B
estimation	B
leads	O
to	O
a	O
consistent	O
classification	O
rule	B
consider	O
now	O
the	O
k-nearest	O
neighbor	O
density	O
estimate	O
introduced	O
by	O
lofts	O
gaarden	O
and	O
quesenberry	O
c	O
let	O
xi	O
x	O
n	O
be	O
independent	O
identically	O
tributed	O
random	O
variables	O
in	O
r	O
d	O
by	O
with	O
common	O
density	O
the	O
k-nn	B
estimate	O
of	O
is	O
defined	O
fncx	O
a	O
sxlix-	O
xckxlll	O
k	O
where	O
xklcx	O
is	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
x	O
n	O
show	O
that	O
for	O
every	O
n	O
f	O
ifx	O
fnxldx	O
so	O
that	O
the	O
density	O
estimate	O
is	O
never	O
consistent	O
in	O
l	O
on	O
the	O
other	O
hand	O
according	O
to	O
problem	O
the	O
corresponding	O
classification	O
rule	B
is	O
consistent	O
so	O
read	O
on	O
problem	O
assume	O
that	O
the	O
conditional	O
densities	O
fo	O
and	O
exist	O
then	O
we	O
can	O
use	O
a	O
rule	B
suggested	O
by	O
patrick	O
and	O
fischer	O
let	O
no	O
iyio	O
and	O
n	O
no	O
be	O
the	O
number	O
of	O
zeros	O
and	O
ones	O
in	O
the	O
training	O
data	O
denote	O
by	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
the	O
xs	O
with	O
yi	O
define	O
xiklcx	O
similarly	O
if	O
aca	O
denotes	O
the	O
volume	O
of	O
a	O
set	O
a	O
c	O
rd	O
then	O
the	O
rule	B
is	O
defined	O
as	O
if	O
noa	O
nda	O
otherwise	O
this	O
estimate	O
is	O
based	O
on	O
the	O
k-nearest	O
neighbor	O
density	O
estimate	O
introduced	O
by	O
loftsgaarden	O
and	O
quesenberry	O
their	O
estimate	O
of	O
fo	O
is	O
then	O
the	O
rule	B
gn	O
can	O
be	O
re-written	O
as	O
if	O
ponlon	O
plnlrn	O
otherwise	O
where	O
pon	O
no	O
nand	O
pln	O
nd	O
n	O
are	O
the	O
obvious	O
estimates	O
of	O
the	O
class	O
probabilities	O
show	O
that	O
gn	O
is	O
weakly	O
consistent	O
if	O
k	O
and	O
kin	O
whenever	O
the	O
conditional	O
densities	O
exist	O
problem	O
consider	O
a	O
weakly	O
universally	O
consistent	O
rule	B
gn	O
and	O
define	O
the	O
rule	B
if	O
x	O
xi	O
and	O
x	O
x	O
j	O
all	O
j	O
i	O
otherwise	O
show	O
that	O
h	O
n	O
too	O
is	O
weakly	O
universally	O
consistent	O
note	O
it	O
is	O
the	O
atomic	O
cor	O
partially	O
atomic	O
distributions	O
of	O
x	O
that	O
make	O
this	O
exercise	O
interesting	O
hint	O
the	O
next	O
exercise	O
may	O
help	O
problems	O
and	O
exercises	O
problem	O
let	O
x	O
have	O
an	O
atomic	O
distribution	O
which	O
puts	O
probability	O
pi	O
at	O
atom	O
i	O
let	O
xl	O
xn	O
be	O
an	O
u	O
d	O
sample	O
drawn	O
from	O
this	O
distribution	O
then	O
show	O
the	O
following	O
pn	O
en	O
e	O
s	O
where	O
e	O
and	O
n	O
is	O
the	O
number	O
of	O
atoms	O
number	O
of	O
different	O
values	O
in	O
the	O
data	O
sequence	O
en	O
in	O
o	O
n	O
i	O
n	O
almost	O
surely	O
lix	O
rli	O
for	O
all	O
j	O
sn	O
pi	O
almost	O
surely	O
problem	O
royalls	O
rule	B
royall	O
proposes	O
the	O
regression	B
function	I
estimate	O
lnhnj	O
lnx	O
l	O
yix	O
n	O
ii	O
n	O
n	O
where	O
leu	O
is	O
a	O
smooth	O
kernel-like	O
function	O
on	O
with	O
f	O
ludu	O
and	O
h	O
n	O
is	O
a	O
smoothing	B
factor	I
suggestions	O
included	O
leu	O
d	O
define	O
that	O
this	O
function	O
becomes	O
negative	O
if	O
otherwise	O
assume	O
that	O
hn	O
nhn	O
derive	O
sufficient	O
conditions	O
on	O
that	O
guarantee	O
the	O
weak	B
universal	B
consistency	B
of	O
royalls	O
rule	B
in	O
particular	O
insure	O
that	O
choice	O
is	O
weakly	O
universally	O
consistent	O
hint	O
try	O
adding	O
an	O
appropriate	O
smoothness	O
condition	O
to	O
l	O
problem	O
let	O
k	O
be	O
a	O
kernel	B
and	O
let	O
rnx	O
denote	O
the	O
distance	O
between	O
x	O
and	O
its	O
k-th	O
nearest	B
neighbor	I
x	O
among	O
x	O
i	O
x	O
n	O
the	O
discrimination	O
rule	B
that	O
corresponds	O
to	O
a	O
kernel-type	O
nearest	B
neighbor	I
regression	B
function	I
estimate	O
of	O
mack	O
is	O
if	O
otherwise	O
i	O
ik	O
rnx	O
idea	O
of	O
replacing	O
the	O
smoothing	B
factor	I
in	O
the	O
kernel	B
estimate	O
by	O
a	O
local	O
rank-based	O
value	O
such	O
as	O
rnx	O
is	O
due	O
to	O
breiman	O
meisel	O
and	O
purcell	O
for	O
the	O
kernel	B
k	O
iso	O
this	O
rule	B
coincides	O
with	O
the	O
k-nn	B
rule	B
for	O
regular	B
kernels	O
chapter	O
show	O
that	O
the	O
rule	B
remains	O
weakly	O
universally	O
consistent	O
whenever	O
k	O
and	O
kin	O
by	O
verifying	O
stones	O
conditions	O
of	O
theorem	O
problem	O
let	O
be	O
a	O
weakly	O
universally	O
consistent	O
sequence	O
of	O
classifiers	O
split	O
the	O
data	O
sequence	O
dn	O
into	O
two	O
parts	O
dm	O
yi	O
ym	O
and	O
tn-	O
m	O
xmi	O
yn	O
use	O
gn-m	O
and	O
the	O
second	O
part	O
tn	O
m	O
to	O
relabel	O
the	O
first	O
part	O
i	O
e	O
define	O
y	O
gn-mxi	O
tn-	O
m	O
for	O
i	O
m	O
prove	O
that	O
the	O
i-nn	B
rule	B
based	O
on	O
the	O
data	O
y	O
y	O
is	O
weakly	O
universally	O
consistent	O
whenever	O
m	O
and	O
n	O
m	O
hint	O
use	O
problem	O
problem	O
consider	O
the	O
k-nn	B
rule	B
with	O
a	O
fixed	O
k	O
as	O
the	O
ancestral	B
rule	B
and	O
apply	O
the	O
rule	B
using	O
the	O
relabeled	O
data	O
investigate	O
the	O
convergence	O
of	O
eln	O
is	O
the	O
limit	O
lknn	O
or	O
something	O
else	O
vapnik-chervonenkis	B
theory	O
empirical	B
error	I
minimization	O
in	O
this	O
chapter	O
we	O
select	O
a	O
decision	O
rule	B
from	O
a	O
class	O
of	O
rules	O
with	O
the	O
help	O
of	O
training	O
data	O
working	O
formally	O
let	O
c	O
be	O
a	O
class	O
of	O
functions	O
nd	O
one	O
wishes	O
to	O
select	O
a	O
function	O
from	O
c	O
with	O
small	O
error	O
probability	O
assume	O
that	O
the	O
training	O
data	O
dn	O
n	O
yn	O
are	O
given	O
to	O
pick	O
one	O
of	O
the	O
functions	O
from	O
c	O
to	O
be	O
used	O
as	O
a	O
classifier	B
perhaps	O
the	O
most	O
natural	O
way	O
of	O
selecting	O
a	O
function	O
is	O
to	O
minimize	O
the	O
empirical	B
error	I
probability	O
n	O
ln	O
l	O
ilxi	O
yd	O
n	O
il	O
over	O
the	O
class	O
c	O
denote	O
the	O
empirically	O
optimal	O
rule	B
by	O
argmin	O
ln	O
thus	O
is	O
the	O
classifier	B
that	O
according	O
to	O
the	O
data	O
d	O
n	O
best	O
among	O
the	O
classifiers	O
in	O
c	O
this	O
idea	O
of	O
minimizing	O
the	O
empirical	B
risk	O
in	O
the	O
construction	O
of	O
a	O
rule	B
was	O
developed	O
to	O
great	O
extent	O
by	O
vapnik	O
and	O
chervonenkis	O
intuitively	O
the	O
selected	O
classifier	B
should	O
be	O
good	O
in	O
the	O
sense	O
that	O
its	O
true	O
error	O
probability	O
l	O
p	O
yidn	O
is	O
expected	O
to	O
be	O
close	O
to	O
the	O
optimal	O
error	O
probability	O
within	O
the	O
class	O
their	O
difference	O
is	O
the	O
quantity	O
that	O
primarily	O
interests	O
us	O
in	O
this	O
chapter	O
l	O
inf	O
l	O
vapnik-chervonenkis	B
theory	O
the	O
latter	O
difference	O
may	O
be	O
bounded	O
in	O
a	O
distribution-free	O
manner	O
and	O
a	O
rate	B
of	I
convergence	I
results	O
that	O
only	O
depends	O
on	O
the	O
structure	O
of	O
c	O
while	O
this	O
is	O
very	O
exciting	O
we	O
must	O
add	O
that	O
l	O
may	O
be	O
far	O
away	O
from	O
the	O
bayes	B
error	I
l	O
note	O
that	O
l	O
l	O
inf	O
l	O
l	O
l	O
ec	O
ec	O
the	O
size	O
of	O
c	O
is	O
a	O
compromise	O
when	O
c	O
is	O
large	O
inf	O
ec	O
l	O
may	O
be	O
close	O
to	O
l	O
but	O
the	O
former	O
error	O
the	O
estimation	B
error	I
is	O
probably	O
large	O
as	O
well	O
if	O
c	O
is	O
too	O
small	O
there	O
is	O
no	O
hope	O
to	O
make	O
the	O
approximation	B
error	I
inf	O
ec	O
l	O
l	O
small	O
for	O
example	O
if	O
c	O
is	O
the	O
class	O
of	O
all	O
decision	O
functions	O
then	O
we	O
can	O
always	O
find	O
a	O
classifier	B
in	O
c	O
with	O
zero	O
empirical	B
error	I
but	O
it	O
may	O
have	O
arbitrary	O
values	O
outside	O
of	O
the	O
points	O
xl	O
x	O
n	O
for	O
example	O
an	O
empirically	B
optimal	I
classifier	B
is	O
if	O
x	O
xi	O
i	O
otherwise	O
this	O
is	O
clearly	O
not	O
what	O
we	O
are	O
looking	O
for	O
this	O
phenomenon	O
is	O
called	O
overjitting	O
as	O
the	O
overly	O
large	O
class	O
c	O
overfits	O
the	O
data	O
we	O
will	O
give	O
precise	O
conditions	O
on	O
c	O
that	O
allow	O
us	O
to	O
avoid	O
this	O
anomaly	O
the	O
choice	O
of	O
c	O
such	O
that	O
inf	O
ec	O
l	O
is	O
close	O
to	O
l	O
has	O
been	O
the	O
subject	O
of	O
various	O
chapters	O
on	O
consistency-just	O
assume	O
that	O
c	O
is	O
allowed	O
to	O
grow	O
with	O
n	O
in	O
some	O
manner	O
here	O
we	O
take	O
the	O
point	O
of	O
view	O
that	O
c	O
is	O
fixed	O
and	O
that	O
we	O
have	O
to	O
live	O
with	O
the	O
functions	O
in	O
c	O
the	O
best	O
we	O
may	O
then	O
hope	O
for	O
is	O
to	O
minimize	O
l	O
inf	O
ec	O
l	O
a	O
typical	O
situation	O
is	O
shown	O
in	O
figure	O
picked	O
rule	B
best	O
rule	B
in	O
class	O
inf	O
stimation	O
error	O
be	O
controlled	O
approximation	B
error	I
controllable	O
larger	O
than	O
estimation	B
error	I
figure	O
various	O
errors	O
in	O
empirical	B
classifier	B
selection	I
empirical	B
error	I
minimization	O
consider	O
first	O
a	O
finite	O
collection	O
c	O
and	O
assume	O
that	O
one	O
of	O
the	O
classifiers	O
in	O
c	O
has	O
zero	O
error	O
probability	O
that	O
is	O
min	O
ec	O
l	O
o	O
then	O
clearly	O
ln	O
with	O
probability	O
one	O
we	O
then	O
have	O
the	O
following	O
performance	O
bound	O
theorem	O
and	O
chervonenkis	O
assume	O
ici	O
and	O
min	O
ec	O
l	O
o	O
then	O
for	O
every	O
nand	O
e	O
and	O
proof	O
clearly	O
el	O
log	O
ici	O
n	O
pl	O
e	O
p	O
illax	O
ecl	O
l	O
e	O
i	O
e	O
l	O
e	O
e	O
i	O
ecl	O
e	O
l	O
pln	O
since	O
the	O
probability	O
that	O
no	O
yi	O
pair	O
falls	O
in	O
the	O
set	O
y	O
y	O
is	O
less	O
than	O
en	O
if	O
the	O
probability	O
of	O
the	O
set	O
is	O
larger	O
than	O
e	O
the	O
probability	O
inequality	B
of	O
the	O
theorem	O
follows	O
from	O
the	O
simple	O
inequality	B
x	O
e-x	O
to	O
bound	O
the	O
expected	O
error	O
probability	O
note	O
that	O
for	O
any	O
u	O
el	O
i	O
f	O
pl	O
tdt	O
u	O
pl	O
tdt	O
u	O
ici	O
oo	O
e-	O
nt	O
dt	O
u-e	O
ici	O
n	O
since	O
u	O
was	O
arbitrary	O
we	O
may	O
choose	O
it	O
to	O
minimize	O
the	O
obtained	O
upper	O
bound	O
the	O
optimal	O
choice	O
is	O
u	O
log	O
icin	O
which	O
yields	O
the	O
desired	O
inequality	B
theorem	O
shows	O
that	O
empirical	B
selection	O
works	O
very	O
well	O
if	O
the	O
sample	O
size	O
n	O
is	O
much	O
larger	O
than	O
the	O
logarithm	O
of	O
the	O
size	O
of	O
the	O
family	O
c	O
unfortunately	O
the	O
vapnik-chervonenkis	B
theory	O
assumption	O
on	O
the	O
distribution	O
of	O
y	O
that	O
is	O
that	O
min	O
ec	O
l	O
is	O
very	O
restrictive	O
in	O
the	O
sequel	O
we	O
drop	O
this	O
assumption	O
and	O
deal	O
with	O
the	O
free	O
problem	O
one	O
of	O
our	O
main	O
tools	O
is	O
taken	O
from	O
lemma	O
this	O
leads	O
to	O
the	O
study	O
of	O
uniform	O
deviations	O
of	O
relative	O
frequencies	O
from	O
their	O
probabilities	O
by	O
the	O
following	O
simple	O
observation	O
let	O
v	O
be	O
a	O
probability	O
measure	O
of	O
y	O
on	O
nd	O
x	O
i	O
and	O
let	O
vn	O
be	O
the	O
empirical	B
measure	I
based	O
upon	O
dn	O
that	O
is	O
for	O
any	O
fixed	O
measurable	O
set	O
a	O
c	O
nd	O
x	O
i	O
va	O
px	O
y	O
e	O
a	O
and	O
vna	O
ixiyieaj	O
then	O
l	O
vx	O
y	O
i	O
y	O
is	O
just	O
the	O
v-measure	O
of	O
the	O
set	O
of	O
pairs	O
y	O
e	O
n	O
d	O
x	O
i	O
where	O
i	O
y	O
formally	O
l	O
is	O
the	O
v-measure	O
of	O
the	O
set	O
i	O
x	O
u	O
o	O
x	O
similarly	O
ln	O
vnx	O
y	O
i	O
y	O
thus	O
sup	O
iln	O
l	O
sup	O
ivna	O
vai	O
ec	O
aea	O
where	O
a	O
is	O
the	O
collection	O
of	O
all	O
sets	O
i	O
x	O
u	O
o	O
x	O
e	O
c	O
for	O
a	O
fixed	O
set	O
a	O
for	O
any	O
probability	O
measure	O
v	O
by	O
the	O
law	O
of	O
large	O
numbers	O
vna	O
va	O
almost	O
surely	O
as	O
n	O
moreover	O
by	O
hoeffdings	O
inequality	B
plvna	O
e	O
however	O
it	O
is	O
a	O
much	O
harder	O
problem	O
to	O
obtain	O
such	O
results	O
for	O
supaea	O
ivna	O
vai	O
if	O
the	O
class	O
of	O
sets	O
a	O
analogously	O
in	O
the	O
pattern	O
recognition	O
context	O
c	O
is	O
of	O
finite	O
cardinality	O
then	O
the	O
union	O
bound	O
trivially	O
gives	O
however	O
if	O
a	O
contains	O
infinitely	O
many	O
sets	O
in	O
many	O
of	O
the	O
interesting	O
cases	O
then	O
the	O
problem	O
becomes	O
nontrivial	O
spawning	O
a	O
vast	O
literature	O
the	O
most	O
erful	O
weapons	O
to	O
attack	O
these	O
problems	O
are	O
distribution-free	O
large	O
deviation-type	O
inequalities	O
first	O
proved	O
by	O
vapnik	O
and	O
chervonenkis	O
in	O
their	O
pigneering	O
work	O
however	O
in	O
some	O
situations	O
we	O
can	O
handle	O
the	O
problem	O
in	O
a	O
much	O
simpler	O
way	O
we	O
have	O
already	O
seen	O
such	O
an	O
example	O
in	O
section	O
fingering	B
fingering	B
recall	O
that	O
in	O
section	O
we	O
studied	O
a	O
specific	O
rule	B
that	O
selects	O
a	O
linear	B
classifier	B
by	O
minimizing	O
the	O
empirical	B
error	I
the	O
performance	O
bounds	O
provided	O
by	O
theorems	O
and	O
show	O
that	O
the	O
selected	O
rule	B
performs	O
very	O
closely	O
to	O
the	O
best	O
possible	O
linear	O
rule	B
these	O
bounds	O
apply	O
only	O
to	O
the	O
specific	O
algorithm	B
used	O
to	O
find	O
the	O
pirical	O
minima-we	O
have	O
not	O
showed	O
that	O
any	O
classifier	B
minimizing	O
the	O
empirical	B
error	I
performs	O
well	O
this	O
matter	O
will	O
be	O
dealt	O
with	O
in	O
later	O
sections	O
in	O
this	O
section	O
we	O
extend	O
theorems	O
and	O
to	O
classes	O
other	O
than	O
linear	O
classifiers	O
let	O
c	O
be	O
the	O
class	O
of	O
classifiers	O
assigning	O
to	O
those	O
x	O
contained	O
in	O
a	O
closed	O
perrectangle	O
and	O
to	O
all	O
other	O
points	O
then	O
a	O
classifier	B
minimizing	O
the	O
empirical	B
error	I
ln	O
over	O
all	O
e	O
c	O
may	O
be	O
obtained	O
by	O
the	O
following	O
algorithm	B
to	O
each	O
x	O
of	O
points	O
from	O
xl	O
x	O
n	O
assign	O
the	O
smallest	O
angle	O
containing	O
these	O
points	O
if	O
we	O
assume	O
that	O
x	O
has	O
a	O
density	O
then	O
the	O
points	O
x	O
xn	O
are	O
in	O
general	O
position	O
with	O
probability	O
one	O
this	O
way	O
we	O
obtain	O
at	O
most	O
sets	O
let	O
i	O
be	O
the	O
classifier	B
corresponding	O
to	O
the	O
i-th	O
such	O
gle	O
that	O
is	O
the	O
one	O
assigning	O
to	O
those	O
x	O
contained	O
in	O
the	O
hyperrectangle	O
and	O
to	O
other	O
points	O
clearly	O
for	O
each	O
e	O
c	O
there	O
exists	O
a	O
i	O
i	O
such	O
that	O
for	O
all	O
x	O
j	O
except	O
possibly	O
for	O
those	O
on	O
the	O
boundary	O
of	O
the	O
hyperrectangle	O
since	O
the	O
points	O
are	O
in	O
general	O
position	O
there	O
are	O
at	O
most	O
such	O
exceptional	O
points	O
therefore	O
if	O
we	O
select	O
a	O
classifier	B
among	O
g	O
to	O
minimize	O
the	O
empirical	B
error	I
then	O
it	O
approximately	O
minimizes	O
the	O
empirical	B
error	I
over	O
the	O
whole	O
class	O
c	O
as	O
well	O
a	O
quick	O
scan	O
through	O
the	O
proof	O
of	O
theorem	O
reveals	O
that	O
by	O
similar	O
arguments	O
we	O
may	O
obtain	O
the	O
performance	O
bound	O
for	O
n	O
and	O
e	O
the	O
idea	O
may	O
be	O
generalized	B
it	O
always	O
works	O
if	O
for	O
some	O
k	O
k-tuples	O
of	O
points	O
determine	O
classifiers	O
from	O
c	O
such	O
that	O
no	O
matter	O
where	O
the	O
other	O
data	O
points	O
fall	O
the	O
minimal	O
empirical	B
error	I
over	O
these	O
sets	O
coincides	O
with	O
the	O
overall	O
minimum	O
then	O
we	O
may	O
fix	O
these	O
sets-put	O
our	O
finger	O
on	O
them-and	O
look	O
for	O
the	O
empirical	B
minima	O
over	O
this	O
finite	O
collection	O
the	O
next	O
theorems	O
whose	O
proofs	O
are	O
left	O
as	O
an	O
exercise	O
show	O
that	O
if	O
c	O
has	O
this	O
property	O
then	O
works	O
extremely	O
well	O
whenever	O
n	O
k	O
theorem	O
assume	O
that	O
the	O
class	O
c	O
of	O
classifiers	O
has	O
the	O
following	O
property	O
c	O
such	O
that	O
for	O
all	O
for	O
some	O
integer	O
k	O
there	O
exists	O
a	O
function	O
xl	O
xn	O
e	O
rd	O
and	O
all	O
e	O
c	O
there	O
exists	O
a	O
k-tuple	O
ii	O
ik	O
e	O
of	O
different	O
indices	O
such	O
that	O
wxi	O
xhx	O
for	O
all	O
j	O
n	O
with	O
j	O
ii	O
k	O
vapnik-chervonenkis	B
theory	O
with	O
probability	O
one	O
let	O
be	O
found	O
by	O
fingering	B
that	O
is	O
by	O
empirical	B
error	I
mization	O
over	O
the	O
collection	O
of	O
n	O
k	O
classifiers	O
of	O
the	O
form	O
ii	O
ik	O
e	O
n	O
different	O
then	O
for	O
n	O
k	O
and	O
n	O
e	O
p	O
inf	O
l	O
e	O
k	O
eepec	O
moreover	O
if	O
n	O
k	O
then	O
e	O
inf	O
l	O
epec	O
log	O
n	O
n	O
the	O
smallest	O
k	O
for	O
which	O
c	O
has	O
the	O
property	O
described	O
in	O
the	O
theorem	O
may	O
be	O
called	O
the	O
fingering	B
dimension	B
of	O
c	O
in	O
most	O
interesting	O
cases	O
it	O
is	O
independent	O
of	O
n	O
problem	O
offers	O
a	O
few	O
such	O
classes	O
we	O
will	O
see	O
later	O
in	O
this	O
chapter	O
that	O
the	O
fingering	B
dimension	B
is	O
closely	O
related	O
the	O
so-called	O
vc	B
dimension	B
of	O
c	O
also	O
problem	O
again	O
we	O
get	O
much	O
smaller	O
errors	O
if	O
infepec	O
l	O
o	O
the	O
next	O
inequality	B
generalizes	O
theorem	O
theorem	O
assume	O
that	O
c	O
has	O
the	O
property	O
described	O
in	O
theorem	O
with	O
fingering	B
dimension	B
k	O
assume	O
in	O
addition	O
that	O
infepec	O
l	O
o	O
then	O
for	O
all	O
n	O
and	O
e	O
and	O
e	O
l	O
k	O
logn	O
n-k	O
remark	O
even	O
though	O
the	O
results	O
in	O
the	O
next	O
few	O
sections	O
based	O
on	O
the	O
chervonenkis	O
theory	O
supersede	O
those	O
of	O
this	O
section	O
requiring	O
less	O
from	O
the	O
class	O
c	O
and	O
being	O
able	O
to	O
bound	O
the	O
error	O
of	O
any	O
classifier	B
minimizing	O
the	O
empirical	B
risk	O
we	O
must	O
remark	O
that	O
the	O
exponents	O
in	O
the	O
above	O
probability	O
inequalities	O
are	O
the	O
best	O
possible	O
and	O
bounds	O
of	O
the	O
same	O
type	O
for	O
the	O
general	O
case	O
can	O
only	O
be	O
obtained	O
with	O
significantly	O
more	O
effort	O
d	O
the	O
glivenko-cantelli	B
theorem	I
in	O
the	O
next	O
two	O
sections	O
we	O
prove	O
the	O
vapnik-chervonenkis	B
inequality	B
a	O
powerful	O
generalization	O
of	O
the	O
classical	O
glivenko-cantelli	B
theorem	I
it	O
provides	O
upper	O
bounds	O
on	O
random	O
variables	O
of	O
the	O
type	O
sup	O
ivna	O
vai	O
aea	O
the	O
glivenko-cantelli	B
theorem	I
as	O
we	O
noted	O
in	O
section	O
such	O
bounds	O
yield	O
performance	O
bounds	O
for	O
any	O
classifier	B
selected	O
by	O
minimizing	O
the	O
empirical	B
error	I
to	O
make	O
the	O
material	O
more	O
digestible	O
we	O
first	O
present	O
the	O
main	O
ideas	O
in	O
a	O
simple	O
one-dimensional	O
setting	O
and	O
then	O
prove	O
the	O
general	O
theorem	O
in	O
the	O
next	O
section	O
we	O
drop	O
the	O
pattern	O
recognition	O
setting	O
momentarily	O
and	O
return	O
to	O
probability	O
theory	O
the	O
following	O
theorem	O
is	O
sometimes	O
referred	O
to	O
as	O
the	O
fundamental	O
theorem	O
of	O
mathematical	O
statistics	O
stating	O
uniform	O
almost	O
sure	O
convergence	O
of	O
the	O
empirical	B
distribution	B
function	I
to	O
the	O
true	O
one	O
theorem	O
theorem	O
let	O
zl	O
zn	O
be	O
i	O
i	O
d	O
valued	O
random	O
variables	O
with	O
distribution	B
function	I
fz	O
pzl	O
z	O
denote	O
the	O
standard	B
empirical	B
distribution	B
function	I
by	O
then	O
p	O
ifz	O
e	O
zer	O
and	O
in	O
particular	O
by	O
the	O
borel-cantelli	B
lemma	I
lim	O
sup	O
ifz	O
with	O
probability	O
one	O
n-foo	O
zer	O
proof	O
the	O
proof	O
presented	O
here	O
is	O
not	O
the	O
simplest	O
possible	O
but	O
it	O
contains	O
the	O
main	O
ideas	O
leading	O
to	O
a	O
powerful	O
generalization	O
introduce	O
the	O
notation	O
va	O
pzl	O
e	O
a	O
and	O
vnca	O
izjea	O
for	O
all	O
measurable	O
sets	O
a	O
c	O
r	O
let	O
a	O
denote	O
the	O
class	O
of	O
sets	O
of	O
form	O
z	O
for	O
z	O
e	O
r	O
with	O
these	O
notations	O
sup	O
ifz	O
sup	O
ivna	O
vai	O
zer	O
aea	O
we	O
prove	O
the	O
theorem	O
in	O
several	O
steps	O
following	O
symmetrization	B
ideas	O
of	O
dudley	O
and	O
pollard	O
we	O
assume	O
that	O
ne	O
since	O
otherwise	O
the	O
bound	O
is	O
trivial	O
in	O
the	O
first	O
step	O
we	O
introduce	O
a	O
symmetrization	B
step	O
first	O
symmetrization	B
by	O
a	O
ghost	B
sample	I
define	O
the	O
random	O
variables	O
zi	O
z	O
e	O
r	O
such	O
that	O
zl	O
zn	O
zi	O
are	O
all	O
independent	O
and	O
tically	O
distributed	O
denote	O
by	O
v	O
the	O
empirical	B
measure	I
corresponding	O
to	O
the	O
new	O
sample	O
then	O
for	O
ne	O
we	O
have	O
p	O
ivna	O
e	O
ivna	O
va	O
i	O
aea	O
aea	O
vapnik-chervonenkis	B
theory	O
to	O
see	O
this	O
let	O
a	O
e	O
a	O
be	O
a	O
set	O
for	O
which	O
ivn	O
v	O
e	O
if	O
such	O
a	O
set	O
exists	O
and	O
let	O
a	O
be	O
a	O
fixed	O
set	O
in	O
a	O
otherwise	O
then	O
p	O
ivna	O
e	O
aea	O
p	O
va	O
e	O
p	O
va	O
e	O
iva	O
va	O
e	O
ilvna-vaiep	O
iv	O
va	O
j	O
zl	O
zn	O
the	O
conditional	O
probability	O
inside	O
may	O
be	O
bounded	O
by	O
chebyshevs	O
inequality	B
as	O
follows	O
p	O
ivna	O
va	O
zl	O
zn	O
ej	O
f	O
va	O
va	O
ne	O
whenever	O
ne	O
in	O
summary	O
p	O
ivna	O
e	O
aea	O
plvna	O
va	O
e	O
p	O
ivna	O
e	O
aea	O
step	O
second	O
symmetrization	B
by	O
random	O
signs	O
let	O
be	O
i	O
i	O
d	O
sign	O
variables	O
independent	O
of	O
zl	O
zn	O
and	O
z	O
z	O
with	O
pi	O
pi	O
i	O
clearly	O
because	O
zl	O
zi	O
zn	O
z	O
are	O
all	O
independent	O
and	O
identically	O
distributed	O
the	O
distribution	O
of	O
is	O
the	O
same	O
as	O
the	O
distribution	O
of	O
ituazi	O
iazi	O
sup	O
it	O
iazi	O
aea	O
il	O
thus	O
by	O
step	O
p	O
ivna	O
e	O
aea	O
ituazi	O
aea	O
n	O
il	O
iazi	O
the	O
glivenko-cantelli	B
theorem	I
simply	O
applying	O
the	O
union	O
bound	O
we	O
can	O
remove	O
the	O
auxiliary	O
random	O
variables	O
zi	O
z	O
step	O
conditioning	O
to	O
bound	O
the	O
probability	O
we	O
condition	O
on	O
z	O
zn	O
fix	O
z	O
e	O
n	O
d	O
and	O
note	O
that	O
as	O
z	O
ranges	O
over	O
n	O
the	O
number	O
of	O
different	O
vectors	O
izn	O
is	O
at	O
most	O
n	O
thus	O
conditional	O
on	O
zl	O
zn	O
the	O
supremum	O
in	O
the	O
probability	O
above	O
is	O
just	O
a	O
maximum	O
taken	O
over	O
at	O
most	O
n	O
random	O
variables	O
thus	O
applying	O
the	O
union	O
bound	O
gives	O
with	O
the	O
supremum	O
now	O
outside	O
the	O
probability	O
it	O
suffices	O
to	O
find	O
an	O
exponential	B
bound	O
on	O
the	O
conditional	O
probability	O
step	O
hoeffdinos	O
inequality	B
with	O
zl	O
zn	O
fixed	O
ajazi	O
is	O
the	O
sum	O
of	O
n	O
independent	O
zero	O
mean	O
random	O
variables	O
bounded	O
between	O
and	O
therefore	O
theorem	O
applies	O
in	O
a	O
straightforward	O
manner	O
thus	O
p	O
i	O
i	O
e	O
i	O
sup	O
zl	O
zn	O
aea	O
n	O
il	O
le	O
vapnik-chervonenkis	B
theory	O
taking	O
the	O
expected	O
value	O
on	O
both	O
sides	O
we	O
have	O
in	O
summary	O
p	O
ivna	O
e	O
d	O
aea	O
uniform	O
deviations	O
of	O
relative	O
frequencies	O
from	O
probabilities	O
in	O
this	O
section	O
we	O
prove	O
the	O
vapnik-chervonenkis	B
inequality	B
a	O
mighty	O
ization	O
of	O
theorem	O
in	O
the	O
proof	O
we	O
need	O
only	O
a	O
slight	O
adjustment	O
of	O
the	O
proof	O
above	O
in	O
the	O
general	O
setting	O
let	O
the	O
independent	O
identically	O
distributed	O
random	O
variables	O
zl	O
zn	O
take	O
their	O
values	O
from	O
rd	O
again	O
we	O
use	O
the	O
tation	O
va	O
pzi	O
e	O
a	O
and	O
vna	O
ll	O
izjea	O
for	O
all	O
measurable	O
sets	O
a	O
c	O
rd	O
the	O
vapnik-chervonenkis	B
theory	O
begins	O
with	O
the	O
concepts	O
of	O
shatter	B
coefficient	I
and	O
vapnik-chervonenkis	B
vc	B
dimension	B
definition	O
let	O
a	O
be	O
a	O
collection	O
of	O
measurable	O
sets	O
for	O
e	O
let	O
n	O
azl	O
zn	O
be	O
the	O
number	O
of	O
different	O
sets	O
in	O
zn	O
n	O
a	O
a	O
e	O
a	O
the	O
n-th	O
shatter	B
coefficient	I
of	O
a	O
is	O
that	O
is	O
the	O
shatter	B
coefficient	I
is	O
the	O
maximal	O
number	O
of	O
different	O
subsets	O
of	O
n	O
points	O
that	O
can	O
be	O
picked	O
out	O
by	O
the	O
class	O
of	O
sets	O
a	O
the	O
shatter	O
coefficients	O
measure	O
the	O
richness	O
of	O
the	O
class	O
a	O
clearly	O
sea	O
n	O
as	O
there	O
are	O
subsets	O
of	O
a	O
set	O
with	O
n	O
elements	O
if	O
n	O
azl	O
zn	O
for	O
some	O
zn	O
then	O
we	O
say	O
that	O
a	O
shatters	O
zn	O
if	O
sea	O
n	O
then	O
any	O
set	O
of	O
n	O
points	O
has	O
a	O
subset	O
such	O
that	O
there	O
is	O
no	O
set	O
in	O
a	O
that	O
contains	O
exactly	O
that	O
subset	O
of	O
the	O
n	O
points	O
clearly	O
if	O
sea	O
k	O
for	O
some	O
integer	O
k	O
then	O
sea	O
n	O
for	O
all	O
n	O
k	O
the	O
first	O
time	O
when	O
this	O
happens	O
is	O
important	O
definition	O
let	O
a	O
be	O
a	O
collection	O
of	O
sets	O
with	O
iai	O
the	O
largest	O
integer	O
k	O
for	O
which	O
sea	O
k	O
is	O
denoted	O
by	O
va	O
and	O
it	O
is	O
called	O
the	O
chervonenkis	O
dimension	B
vc	B
dimension	B
of	O
the	O
class	O
a	O
if	O
sea	O
n	O
for	O
all	O
n	O
then	O
by	O
definition	O
va	O
uniform	O
deviations	O
of	O
relative	O
frequencies	O
from	O
probabilities	O
for	O
example	O
if	O
a	O
contains	O
allhalfiines	O
of	O
form	O
x	O
x	O
e	O
r	O
thensa	O
and	O
va	O
this	O
is	O
easily	O
seen	O
by	O
observing	O
that	O
for	O
any	O
two	O
different	O
points	O
zl	O
there	O
is	O
no	O
set	O
of	O
the	O
form	O
x	O
that	O
contains	O
but	O
not	O
zl	O
a	O
class	O
of	O
sets	O
a	O
for	O
which	O
va	O
is	O
called	O
a	O
vapnik-chervonenkis	B
class	O
in	O
a	O
sense	O
v	O
a	O
may	O
be	O
considered	O
as	O
the	O
complexity	O
or	O
size	O
of	O
a	O
several	O
properties	O
of	O
the	O
shatter	O
coefficients	O
and	O
the	O
ve	O
dimension	B
will	O
be	O
shown	O
in	O
chapter	O
the	O
main	O
purpose	O
of	O
this	O
section	O
is	O
to	O
prove	O
the	O
following	O
important	O
result	O
by	O
vapnik	O
and	O
chervonenkis	O
theorem	O
and	O
chervonenkis	O
for	O
any	O
probability	O
measure	O
v	O
and	O
class	O
of	O
sets	O
a	O
and	O
for	O
any	O
nand	O
e	O
p	O
ivna	O
e	O
aea	O
proof	O
the	O
proof	O
parallels	O
that	O
of	O
theorem	O
we	O
may	O
again	O
assume	O
that	O
in	O
the	O
first	O
two	O
steps	O
we	O
prove	O
that	O
this	O
may	O
be	O
done	O
exactly	O
the	O
same	O
way	O
as	O
in	O
theorem	O
we	O
do	O
not	O
repeat	O
the	O
argument	O
the	O
only	O
difference	O
appears	O
in	O
step	O
step	O
conditioning	O
to	O
bound	O
the	O
probability	O
again	O
we	O
condition	O
on	O
z	O
zn	O
fix	O
z	O
e	O
r	O
d	O
and	O
observe	O
that	O
as	O
a	O
ranges	O
over	O
a	O
the	O
number	O
of	O
different	O
vectors	O
iazn	O
is	O
just	O
the	O
number	O
of	O
different	O
subsets	O
of	O
zn	O
produced	O
by	O
intersecting	O
it	O
with	O
sets	O
in	O
a	O
which	O
by	O
definition	O
cannot	O
exceed	O
sea	O
n	O
therefore	O
with	O
zl	O
zn	O
fixed	O
the	O
supremum	O
in	O
the	O
above	O
probability	O
is	O
a	O
maximum	O
of	O
at	O
most	O
n	O
a	O
zn	O
random	O
variables	O
this	O
number	O
by	O
definition	O
is	O
bounded	O
from	O
above	O
by	O
sea	O
n	O
by	O
the	O
union	O
bound	O
we	O
get	O
i	O
therefore	O
as	O
before	O
it	O
suffices	O
to	O
bound	O
the	O
conditional	O
probability	O
vapnik-chervonenkis	B
theory	O
this	O
may	O
be	O
done	O
by	O
hoeffdings	O
inequality	B
exactly	O
as	O
in	O
step	O
of	O
the	O
proof	O
of	O
theorem	O
finally	O
we	O
obtain	O
p	O
ivna	O
e	O
s	O
aea	O
the	O
bound	O
of	O
theorem	O
is	O
useful	O
when	O
the	O
shatter	O
coefficients	O
do	O
not	O
increase	O
too	O
quickly	O
with	O
n	O
for	O
example	O
if	O
a	O
contains	O
all	O
borel	O
sets	O
of	O
nd	O
then	O
we	O
can	O
shatter	O
any	O
collection	O
of	O
n	O
different	O
points	O
at	O
will	O
and	O
obtain	O
sea	O
n	O
this	O
would	O
be	O
useless	O
of	O
course	O
the	O
smaller	O
a	O
the	O
smaller	O
the	O
shatter	B
coefficient	I
is	O
to	O
apply	O
the	O
vc	O
bound	O
it	O
suffices	O
to	O
compute	O
shatter	O
coefficients	O
for	O
certain	O
families	O
of	O
sets	O
examples	O
may	O
be	O
found	O
in	O
cover	O
vapnik	O
and	O
chervonenkis	O
devroye	O
and	O
wagner	O
feinholz	O
devroye	O
massart	O
dudley	O
simon	O
and	O
stengle	O
and	O
yukich	O
this	O
list	O
of	O
references	O
is	O
far	O
from	O
exhaustive	O
more	O
information	O
about	O
shatter	O
coefficients	O
is	O
given	O
in	O
chapter	O
remark	O
measurability	O
the	O
supremum	O
in	O
theorem	O
is	O
not	O
always	O
surable	O
measurability	O
must	O
be	O
verified	O
for	O
every	O
family	O
a	O
for	O
all	O
our	O
examples	O
the	O
quantities	O
are	O
indeed	O
measurable	O
for	O
more	O
on	O
the	O
measurability	O
question	O
see	O
dudley	O
massart	O
and	O
gaenssler	O
gine	O
andzinn	O
and	O
yukich	O
provide	O
further	O
work	O
on	O
suprema	O
of	O
the	O
type	O
shown	O
in	O
theorem	O
remark	O
optimal	O
exponent	O
for	O
the	O
sake	O
of	O
readability	O
we	O
followed	O
the	O
line	O
of	O
pollards	O
proof	O
instead	O
of	O
the	O
original	O
by	O
vapnik	O
and	O
chervonenkis	O
in	O
particular	O
the	O
exponent	O
in	O
theorem	O
is	O
worse	O
than	O
the	O
established	O
in	O
the	O
original	O
paper	O
the	O
best	O
known	O
exponents	O
together	O
with	O
some	O
other	O
related	O
results	O
are	O
mentioned	O
in	O
section	O
the	O
basic	O
ideas	O
of	O
the	O
original	O
proof	O
by	O
vapnik	O
and	O
chervonenkis	O
appear	O
in	O
the	O
proof	O
of	O
theorem	O
below	O
remark	O
necessary	O
and	O
sufficient	O
conditions	O
the	O
theorem	O
that	O
it	O
can	O
be	O
strengthened	O
to	O
it	O
is	O
clear	O
from	O
the	O
proof	O
of	O
p	O
ivna	O
e	O
s	O
azl	O
zn	O
e	O
aea	O
where	O
z	O
zn	O
are	O
i	O
i	O
d	O
random	O
variables	O
with	O
probability	O
measure	O
v	O
although	O
this	O
upper	O
bound	O
is	O
tighter	O
than	O
that	O
in	O
the	O
stated	O
inequality	B
it	O
is	O
usually	O
more	O
difficult	O
to	O
handle	O
since	O
the	O
coefficient	O
in	O
front	O
of	O
the	O
exponential	B
term	O
depends	O
on	O
the	O
distribution	O
of	O
zl	O
while	O
sea	O
n	O
is	O
purely	O
combinatorial	O
in	O
nature	O
however	O
this	O
form	O
is	O
important	O
in	O
a	O
different	O
setting	O
we	O
say	O
that	O
the	O
uniform	O
law	O
of	O
large	O
numbers	O
holds	O
if	O
sup	O
ivna	O
aea	O
in	O
probability	O
classifier	B
selection	I
it	O
follows	O
from	O
this	O
form	O
of	O
theorem	O
that	O
the	O
uniform	O
law	O
of	O
large	O
numbers	O
holds	O
if	O
e	O
zn	O
o	O
n	O
vapnik	O
and	O
chervonenkis	O
showed	O
that	O
this	O
condition	O
is	O
also	O
necessary	O
for	O
the	O
uniform	O
law	O
of	O
large	O
numbers	O
another	O
characterization	O
of	O
the	O
uniform	O
law	O
of	O
large	O
numbers	O
is	O
given	O
by	O
talagrand	O
who	O
showed	O
that	O
the	O
uniform	O
law	O
of	O
large	O
numbers	O
holds	O
if	O
and	O
only	O
if	O
there	O
does	O
not	O
exist	O
a	O
set	O
a	O
c	O
rd	O
with	O
va	O
such	O
that	O
with	O
probability	O
one	O
the	O
set	O
zn	O
n	O
a	O
is	O
shattered	O
by	O
a	O
d	O
classifier	B
selection	I
the	O
following	O
theorem	O
relates	O
the	O
results	O
of	O
the	O
previous	O
sections	O
to	O
empirical	B
classifier	B
selection	I
that	O
is	O
when	O
the	O
empirical	B
error	I
probability	O
ln	O
is	O
minimized	O
over	O
a	O
class	O
of	O
classifiers	O
c	O
we	O
emphasize	O
that	O
unlike	O
in	O
section	O
here	O
we	O
allow	O
any	O
classifier	B
with	O
the	O
property	O
that	O
has	O
minimal	O
empirical	B
error	I
l	O
n	O
in	O
c	O
first	O
introduce	O
the	O
shatter	O
coefficients	O
and	O
vc	B
dimension	B
of	O
c	O
definition	O
let	O
c	O
be	O
a	O
class	O
of	O
decision	O
functions	O
of	O
the	O
form	O
cj	O
rd	O
i	O
define	O
a	O
as	O
the	O
collection	O
of	O
all	O
sets	O
cjx	O
i	O
x	O
u	O
cjx	O
o	O
x	O
cj	O
e	O
c	O
define	O
the	O
n-th	O
shatter	B
coefficient	I
sc	O
n	O
of	O
the	O
class	O
of	O
classifiers	O
cas	O
furthermore	O
define	O
the	O
vc	B
dimension	B
vc	O
of	O
c	O
as	O
sc	O
n	O
sea	O
n	O
for	O
the	O
performance	O
of	O
the	O
empirically	O
selected	O
decision	O
cj	O
we	O
have	O
the	O
lowing	O
theorem	O
letc	O
be	O
a	O
class	O
of	O
decision	O
functions	O
oftheformcj	O
rd	O
i	O
then	O
using	O
the	O
notation	O
lncj	O
i	O
and	O
lcj	O
pcjx	O
y	O
we	O
have	O
and	O
therefore	O
p	O
inf	O
lcj	O
e	O
ec	O
where	O
cj	O
denotes	O
the	O
classifier	B
minimizing	O
ln	O
over	O
the	O
class	O
c	O
vapnik-chervonenkis	B
theory	O
proof	O
the	O
statements	O
are	O
immediate	O
consequences	O
of	O
theorem	O
and	O
lemma	O
the	O
next	O
corollary	O
an	O
easy	O
application	O
of	O
theorem	O
problem	O
makes	O
things	O
a	O
little	O
more	O
transparent	O
corollary	O
in	O
the	O
notation	O
of	O
theorem	O
e	O
n	O
inf	O
l	O
epec	O
n	O
if	O
s	O
n	O
increases	O
polynomially	O
with	O
n	O
then	O
the	O
average	O
error	O
probability	O
of	O
the	O
selected	O
classifier	B
is	O
within	O
jlog	O
n	O
n	O
of	O
the	O
error	O
of	O
the	O
best	O
rule	B
in	O
the	O
class	O
we	O
point	O
out	O
that	O
this	O
result	O
is	O
completely	O
distribution-free	O
furthermore	O
note	O
the	O
nonasymptotic	O
nature	O
of	O
these	O
inequalities	O
they	O
hold	O
for	O
every	O
n	O
from	O
here	O
on	O
the	O
problem	O
is	O
purely	O
combinatorial-one	O
has	O
to	O
estimate	O
the	O
shatter	O
coefficients	O
many	O
properties	O
are	O
given	O
in	O
chapter	O
in	O
particular	O
if	O
vc	O
then	O
sc	O
n	O
n	O
vc	O
that	O
is	O
if	O
the	O
class	O
c	O
has	O
finite	O
vc	B
dimension	B
then	O
sc	O
n	O
increases	O
at	O
a	O
polynomial	B
rate	O
and	O
e	O
inf	O
l	O
epec	O
vc	O
logn	O
remark	O
theorem	O
provides	O
a	O
bound	O
for	O
the	O
behavior	O
of	O
the	O
empirically	O
mal	O
classifier	B
in	O
practice	O
finding	O
an	O
empirically	B
optimal	I
classifier	B
is	O
often	O
tationally	O
very	O
expensive	O
in	O
such	O
cases	O
the	O
designer	O
is	O
often	O
forced	O
to	O
put	O
up	O
with	O
algorithms	O
yielding	O
suboptimal	O
classifiers	O
assume	O
for	O
example	O
that	O
we	O
have	O
an	O
algorithm	B
which	O
selects	O
a	O
classifier	B
gn	O
such	O
that	O
its	O
empirical	B
error	I
is	O
not	O
too	O
far	O
from	O
the	O
optimum	O
with	O
large	O
probability	O
where	O
and	O
are	O
sequences	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
then	O
it	O
is	O
easy	O
to	O
see	O
problem	O
that	O
p	O
inf	O
l	O
e	O
iln	O
l	O
e	O
en	O
epec	O
epec	O
and	O
theorem	O
may	O
be	O
used	O
to	O
obtain	O
bounds	O
for	O
the	O
error	O
probability	O
of	O
gn	O
interestingly	O
the	O
empirical	B
error	I
probability	O
of	O
the	O
empirically	B
optimal	I
classifier	B
is	O
always	O
close	O
to	O
its	O
expected	O
value	O
as	O
may	O
be	O
seen	O
from	O
the	O
following	O
example	O
corollary	O
let	O
c	O
be	O
an	O
arbitrary	O
class	O
of	O
classification	O
rules	O
is	O
tions	O
of	O
the	O
form	O
n	O
d	O
i	O
let	O
e	O
c	O
be	O
the	O
rule	B
that	O
minimizes	O
the	O
number	O
of	O
errors	O
committed	O
on	O
the	O
training	O
sequence	O
dn	O
among	O
the	O
classifiers	O
in	O
c	O
in	O
other	O
words	O
sample	B
complexity	I
where	O
ln	O
n	O
lil	O
i	O
thenfor	O
every	O
nand	O
e	O
n	O
the	O
corollary	O
follows	O
immediately	O
from	O
theorem	O
by	O
observing	O
that	O
ing	O
the	O
value	O
of	O
one	O
yi	O
pair	O
in	O
the	O
training	O
sequence	O
results	O
in	O
a	O
change	O
of	O
at	O
most	O
lin	O
in	O
the	O
value	O
of	O
ln	O
the	O
corollary	O
is	O
true	O
even	O
if	O
the	O
vc	B
dimension	B
of	O
c	O
is	O
infinite	O
the	O
result	O
shows	O
that	O
ln	O
is	O
always	O
very	O
close	O
to	O
its	O
expected	O
value	O
with	O
large	O
probability	O
even	O
if	O
e	O
is	O
far	O
from	O
inf	O
ec	O
l	O
also	O
problem	O
sample	B
complexity	I
in	O
his	O
theory	O
of	O
learning	B
valiant	O
rephrases	O
the	O
empirical	B
classifier	B
selection	I
problem	O
as	O
follows	O
for	O
e	O
define	O
an	O
learning	B
algorithm	B
as	O
a	O
method	O
that	O
selects	O
a	O
classifier	B
gn	O
from	O
c	O
using	O
the	O
data	O
dn	O
such	O
that	O
for	O
the	O
selected	O
rule	B
sup	O
p	O
inf	O
l	O
e	O
ec	O
whenever	O
n	O
ne	O
here	O
ne	O
is	O
the	O
sample	B
complexity	I
of	O
the	O
algorithm	B
defined	O
as	O
the	O
smallest	O
integer	O
with	O
the	O
above	O
property	O
since	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
possible	O
distributions	O
of	O
y	O
the	O
integer	O
ne	O
is	O
the	O
number	O
of	O
data	O
pairs	O
that	O
guarantees	O
e	O
accuracy	B
with	O
confidence	B
for	O
any	O
distribution	O
note	O
that	O
we	O
use	O
the	O
notation	O
gn	O
and	O
not	O
i	O
in	O
the	O
definition	O
above	O
as	O
the	O
definition	O
does	O
not	O
force	O
us	O
to	O
take	O
the	O
empirical	B
risk	O
minimizer	O
i	O
we	O
may	O
use	O
theorem	O
to	O
get	O
an	O
upper	O
bound	O
on	O
the	O
sample	B
complexity	I
of	O
the	O
selection	O
algorithm	B
based	O
on	O
empirical	B
error	I
minimization	O
the	O
classifier	B
corollary	O
the	O
sample	B
complexity	I
of	O
the	O
method	O
based	O
on	O
empirical	B
error	I
minimization	O
is	O
bounded	O
from	O
above	O
by	O
ne	O
max	O
log	O
e	O
e	O
e	O
the	O
corollary	O
is	O
a	O
direct	O
consequence	O
of	O
theorem	O
the	O
details	O
are	O
left	O
as	O
an	O
exercise	O
the	O
constants	O
may	O
be	O
improved	O
by	O
using	O
refined	O
versions	O
of	O
theorem	O
e	O
g	O
theorem	O
the	O
sample	O
size	O
that	O
guarantees	O
the	O
prescribed	O
accuracy	B
and	O
confidence	B
is	O
proportional	O
to	O
the	O
maximum	O
of	O
vc	O
log	O
vc	O
and	O
log	O
vapnik-chervonenkis	B
theory	O
here	O
we	O
have	O
our	O
first	O
practical	O
interpretation	O
of	O
the	O
vc	B
dimension	B
doubling	O
the	O
vc	B
dimension	B
requires	O
that	O
we	O
basically	O
double	O
the	O
sample	O
size	O
to	O
obtain	O
the	O
same	O
accuracy	B
and	O
confidence	B
doubling	O
the	O
accuracy	B
however	O
forces	O
us	O
to	O
quadruple	O
the	O
sample	O
size	O
on	O
the	O
other	O
hand	O
the	O
confidence	B
level	O
has	O
little	O
influence	O
on	O
the	O
sample	O
size	O
as	O
it	O
is	O
hidden	O
behind	O
a	O
logarithmic	O
term	O
thanks	O
to	O
the	O
exponential	B
nature	O
of	O
the	O
vapnik-chervonenkis	B
inequality	B
the	O
vapnik-chervonenkis	B
bound	O
and	O
the	O
sample	B
complexity	I
ne	O
also	O
allow	O
us	O
to	O
compare	O
different	O
classes	O
in	O
a	O
unified	O
manner	O
for	O
example	O
if	O
we	O
pick	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
all	O
hyperrectangles	O
of	O
n	O
will	O
we	O
need	O
a	O
sample	O
size	O
that	O
exceeds	O
that	O
of	O
the	O
rule	B
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
all	O
linear	O
halfspaces	O
of	O
with	O
the	O
sample	B
complexity	I
in	O
hand	O
it	O
is	O
just	O
a	O
matter	O
of	O
comparing	O
vc	O
dimensions	O
as	O
a	O
function	O
of	O
e	O
the	O
above	O
bound	O
grows	O
as	O
itis	O
possible	O
interestingly	O
to	O
get	O
rid	O
of	O
the	O
term	O
at	O
the	O
expense	O
of	O
increasing	O
the	O
linearity	O
in	O
the	O
vc	B
dimension	B
problem	O
the	O
zero-error	O
case	O
theorem	O
is	O
completely	O
general	O
as	O
it	O
applies	O
to	O
any	O
class	O
of	O
classifiers	O
and	O
all	O
distributions	O
in	O
some	O
cases	O
however	O
when	O
we	O
have	O
some	O
additional	O
information	O
about	O
the	O
distribution	O
it	O
is	O
possible	O
to	O
obtain	O
even	O
better	O
bounds	O
for	O
example	O
in	O
the	O
theory	O
of	O
concept	O
learning	B
one	O
commonly	O
assumes	O
that	O
l	O
and	O
that	O
the	O
bayes	O
decision	O
is	O
contained	O
in	O
c	O
e	O
g	O
valiant	O
blumer	O
ehrenfeucht	O
haussler	O
and	O
warmuth	O
natarajan	O
the	O
following	O
theorem	O
provides	O
significant	O
improvement	O
its	O
various	O
forms	O
have	O
been	O
proved	O
by	O
devroye	O
and	O
wagner	O
vapnik	O
and	O
blumer	O
ehrenfeucht	O
haussler	O
and	O
warmuth	O
for	O
a	O
sharper	O
result	O
see	O
problem	O
theorem	O
let	O
c	O
be	O
a	O
class	O
of	O
decision	O
functions	O
mapping	O
n	O
d	O
to	O
i	O
and	O
let	O
i	O
be	O
a	O
function	O
in	O
c	O
that	O
minimizes	O
the	O
empirical	B
error	I
based	O
on	O
the	O
training	O
sample	O
dn	O
suppose	O
that	O
infjjec	O
l	O
i	O
e	O
the	O
bayes	O
decision	O
is	O
contained	O
in	O
c	O
and	O
l	O
then	O
to	O
contrast	O
this	O
with	O
theorem	O
observe	O
that	O
the	O
exponent	O
in	O
the	O
upper	O
bound	O
for	O
the	O
empirically	O
optimal	O
rule	B
is	O
proportional	O
to	O
instead	O
of	O
to	O
see	O
the	O
significance	O
of	O
this	O
difference	O
note	O
that	O
theorem	O
implies	O
that	O
the	O
error	O
bility	O
of	O
the	O
selected	O
classifier	B
is	O
within	O
n	O
n	O
of	O
the	O
optimal	O
rule	B
in	O
the	O
class	O
equals	O
zero	O
in	O
this	O
case	O
see	O
problem	O
as	O
opposed	O
to	O
o	O
ylognn	O
from	O
theorem	O
we	O
show	O
in	O
chapter	O
that	O
this	O
is	O
not	O
a	O
technical	O
coincidence	O
but	O
since	O
both	O
bounds	O
are	O
essentially	O
tight	O
it	O
is	O
a	O
mathematical	O
witness	O
to	O
the	O
fact	O
that	O
it	O
is	O
remarkably	O
easier	O
to	O
select	O
a	O
good	O
classifier	B
when	O
l	O
the	O
proof	O
is	O
the	O
zero-error	O
case	O
based	O
on	O
the	O
random	O
permutation	O
argument	O
developed	O
in	O
the	O
original	O
proof	O
of	O
the	O
vapnik	O
inequality	B
proof	O
for	O
ne	O
the	O
inequality	B
is	O
clearly	O
true	O
so	O
we	O
assume	O
that	O
ne	O
first	O
observe	O
that	O
since	O
infqec	O
l	O
oln	O
o	O
with	O
probability	O
one	O
it	O
is	O
easily	O
seen	O
that	O
l	O
is	O
sup	O
il	O
ln	O
qlnq	O
now	O
we	O
return	O
to	O
the	O
notation	O
of	O
the	O
previous	O
sections	O
that	O
is	O
zi	O
denotes	O
the	O
pair	O
yi	O
v	O
denotes	O
its	O
measure	O
and	O
vn	O
is	O
the	O
empirical	B
measure	I
based	O
on	O
zl	O
zn	O
also	O
a	O
consists	O
of	O
all	O
sets	O
of	O
the	O
form	O
a	O
y	O
y	O
for	O
e	O
c	O
with	O
these	O
notations	O
sup	O
qlnq	O
il	O
ln	O
sup	O
ivna	O
vai	O
avnao	O
step	O
symmetrization	B
by	O
a	O
ghost	B
sample	I
the	O
first	O
step	O
ofthe	O
proof	O
is	O
similar	O
to	O
that	O
of	O
theorem	O
introduce	O
the	O
auxiliary	O
sample	O
z	O
z	O
such	O
that	O
the	O
random	O
variables	O
z	O
zn	O
zi	O
z	O
are	O
i	O
i	O
d	O
and	O
let	O
v	O
be	O
the	O
empirical	B
measure	I
for	O
zi	O
z	O
then	O
for	O
ne	O
p	O
vall	O
e	O
a	O
vall	O
the	O
proof	O
of	O
this	O
inequality	B
parallels	O
that	O
of	O
the	O
corresponding	O
one	O
in	O
the	O
proof	O
of	O
theorem	O
however	O
an	O
important	O
difference	O
is	O
that	O
the	O
condition	O
there	O
is	O
more	O
restrictive	O
than	O
the	O
condition	O
ne	O
here	O
the	O
details	O
of	O
the	O
proof	O
are	O
left	O
to	O
the	O
reader	O
problem	O
step	O
symmetrization	B
by	O
permuting	O
note	O
that	O
the	O
distribution	O
of	O
i	O
a	O
vna-o	O
is	O
the	O
same	O
as	O
the	O
distribution	O
of	O
n	O
sup	O
ivna	O
n	O
sup	O
iazi	O
l	O
iaz	O
sup	O
t	O
i	O
t	O
df	O
a	O
liiiazi-o	O
n	O
a	O
where	O
is	O
an	O
arbitrary	O
permutation	O
of	O
the	O
random	O
variables	O
zl	O
zn	O
zi	O
z	O
the	O
possible	O
permutations	O
are	O
denoted	O
by	O
therefore	O
p	O
sup	O
avllao	O
ivna	O
e	O
i	O
j	O
h	O
vapnik-chervonenkis	B
theory	O
i	O
e-l	O
jl	O
sup	O
i	O
i	O
is	O
a	O
j	O
a	O
step	O
conditioning	O
next	O
we	O
fix	O
z	O
zn	O
z	O
z	O
and	O
bound	O
the	O
value	O
of	O
the	O
random	O
variable	B
above	O
let	O
a	O
c	O
a	O
be	O
a	O
collection	O
of	O
sets	O
such	O
that	O
any	O
two	O
sets	O
in	O
a	O
pick	O
different	O
subsets	O
of	O
zn	O
z	O
z	O
and	O
its	O
cardinality	O
is	O
n	O
azi	O
zn	O
z	O
z	O
that	O
is	O
all	O
possible	O
subsets	O
are	O
represented	O
exactly	O
once	O
then	O
it	O
suffices	O
to	O
take	O
the	O
supremum	O
over	O
a	O
instead	O
of	O
a	O
a	O
ii	O
iatijzd-	O
i	O
sup	O
j	O
iu	O
iatijz	O
aea	O
aea	O
step	O
counting	O
clearly	O
the	O
expression	B
behind	O
the	O
first	O
summation	O
sign	O
is	O
just	O
the	O
number	O
of	O
permutations	O
of	O
the	O
points	O
z	O
z	O
zn	O
z	O
with	O
the	O
property	O
divided	O
by	O
the	O
total	O
number	O
of	O
permutations	O
observe	O
that	O
if	O
l	O
a	O
iazd	O
is	O
the	O
total	O
number	O
of	O
points	O
in	O
a	O
among	O
zi	O
z	O
zn	O
z	O
for	O
a	O
fixed	O
set	O
a	O
then	O
the	O
number	O
of	O
permutations	O
such	O
that	O
is	O
zero	O
if	O
l	O
ne	O
if	O
l	O
ne	O
then	O
the	O
fraction	O
of	O
the	O
number	O
of	O
permutations	O
with	O
the	O
above	O
property	O
and	O
the	O
number	O
of	O
all	O
permutations	O
can	O
not	O
exceed	O
to	O
see	O
this	O
note	O
that	O
for	O
the	O
above	O
product	B
of	O
indicators	O
to	O
be	O
all	O
the	O
points	O
falling	O
in	O
a	O
have	O
to	O
be	O
in	O
the	O
second	O
half	O
of	O
the	O
permuted	O
sample	O
now	O
clearly	O
the	O
zero-error	O
case	O
l	O
summarizing	O
we	O
have	O
p	O
e	O
n	O
sup	O
a	O
ii	O
iaitjzml	O
aea	O
e	O
zn	O
z	O
sea	O
and	O
the	O
theorem	O
is	O
proved	O
again	O
we	O
can	O
bound	O
the	O
sample	B
complexity	I
n	O
restricted	O
to	O
the	O
class	O
of	O
distributions	O
with	O
inf	O
ec	O
l	O
o	O
just	O
as	O
theorem	O
implies	O
corollary	O
theorem	O
yields	O
corollary	O
the	O
sample	B
complexity	I
ne	O
that	O
guarantees	O
inf	O
l	O
e	O
p	O
sup	O
l	O
ec	O
for	O
n	O
ne	O
is	O
bounded	O
by	O
ne	O
max	O
e	O
e	O
e	O
a	O
quick	O
comparison	O
with	O
corollary	O
shows	O
that	O
the	O
factors	O
in	O
the	O
inators	O
there	O
are	O
now	O
replaced	O
by	O
e	O
for	O
the	O
same	O
accuracy	B
much	O
smaller	O
samples	O
suffice	O
if	O
we	O
know	O
that	O
inf	O
ec	O
l	O
o	O
interestingly	O
the	O
sample	B
complexity	I
is	O
still	O
roughly	O
linear	O
in	O
the	O
vc	B
dimension	B
remark	O
we	O
also	O
note	O
that	O
under	O
the	O
condition	O
of	O
theorem	O
p	O
e	O
zn	O
z	O
z	O
vapnik-chervonenkis	B
theory	O
where	O
the	O
class	O
a	O
of	O
sets	O
is	O
defined	O
in	O
the	O
proof	O
of	O
theorem	O
remark	O
as	O
theorem	O
shows	O
the	O
difference	O
between	O
the	O
error	O
probability	O
of	O
an	O
empirically	B
optimal	I
classifier	B
and	O
that	O
of	O
the	O
optimal	O
in	O
the	O
class	O
is	O
much	O
smaller	O
if	O
the	O
latter	O
quantity	O
is	O
known	O
to	O
be	O
zero	O
than	O
if	O
no	O
restriction	O
is	O
imposed	O
on	O
the	O
distribution	O
to	O
bridge	O
the	O
gap	O
between	O
the	O
two	O
bounds	O
one	O
may	O
put	O
the	O
restriction	O
inicpec	O
lt	O
l	O
on	O
the	O
distribution	O
where	O
l	O
e	O
is	O
a	O
fixed	O
number	O
devroye	O
and	O
wagner	O
and	O
vapnik	O
obtained	O
such	O
bounds	O
for	O
example	O
it	O
follows	O
from	O
a	O
result	O
by	O
vapnik	O
that	O
as	O
expected	O
the	O
bound	O
becomes	O
smaller	O
as	O
l	O
decreases	O
we	O
face	O
the	O
same	O
nomenon	O
in	O
chapter	O
where	O
lower	O
bounds	O
are	O
obtained	O
for	O
the	O
probability	O
above	O
o	O
extensions	O
we	O
mentioned	O
earlier	O
that	O
the	O
constant	O
in	O
the	O
exponent	O
in	O
theorem	O
can	O
be	O
improved	O
at	O
the	O
expense	O
of	O
a	O
more	O
complicated	O
argument	O
the	O
best	O
possible	O
ponent	O
appears	O
in	O
the	O
following	O
result	O
whose	O
proof	O
is	O
left	O
as	O
an	O
exercise	O
theorem	O
p	O
ivna	O
e	O
csa	O
aea	O
where	O
the	O
constant	O
c	O
does	O
not	O
exceed	O
e	O
even	O
though	O
the	O
coefficient	O
in	O
front	O
is	O
larger	O
than	O
in	O
theorem	O
it	O
becomes	O
very	O
quickly	O
absorbed	O
by	O
the	O
exponential	B
term	O
we	O
will	O
see	O
in	O
chapter	O
that	O
for	O
va	O
sea	O
n	O
n	O
va	O
so	O
sea	O
this	O
difference	O
is	O
negligible	O
compared	O
to	O
the	O
difference	O
between	O
the	O
exponential	B
terms	O
even	O
for	O
moderately	O
large	O
values	O
of	O
both	O
theorem	O
and	O
theorem	O
imply	O
that	O
e	O
ivna	O
vai	O
o	O
ylognn	O
aea	O
extensions	O
problem	O
however	O
it	O
is	O
possible	O
to	O
get	O
rid	O
of	O
the	O
logarithmic	O
term	O
to	O
obtain	O
fn	O
for	O
example	O
for	O
the	O
kolmogorov-srnirnov	O
statistic	O
we	O
have	O
the	O
following	O
result	O
by	O
dvoretzky	O
kiefer	O
and	O
wolfowitz	O
sharpened	O
by	O
massart	O
theorem	O
kiefer	O
and	O
wolfowitz	O
massart	O
using	O
the	O
notation	O
of	O
theorem	O
we	O
have	O
for	O
every	O
nand	O
e	O
p	O
ifz	O
e	O
zer	O
for	O
the	O
general	O
case	O
we	O
also	O
have	O
alexanders	O
bound	O
theorem	O
for	O
ne	O
p	O
sup	O
ivna	O
e	O
aea	O
v	O
ne	O
r	O
e-	O
ne	O
the	O
theorem	O
implies	O
the	O
following	O
corollary	O
e	O
sup	O
ivna	O
aea	O
fn	O
n	O
the	O
bound	O
in	O
theorem	O
is	O
theoretically	O
interesting	O
since	O
it	O
implies	O
problem	O
that	O
for	O
fixed	O
v	O
a	O
the	O
expected	O
value	O
of	O
the	O
supremum	O
decreases	O
as	O
a	O
fn	O
instead	O
of	O
jlog	O
n	O
n	O
however	O
a	O
quick	O
comparison	O
reveals	O
that	O
alexanders	O
bound	O
is	O
larger	O
than	O
that	O
of	O
theorem	O
unless	O
n	O
an	O
astronomically	O
large	O
value	O
recently	O
talagrand	O
obtained	O
a	O
very	O
strong	O
result	O
he	O
proved	O
that	O
there	O
exists	O
a	O
universal	O
constant	O
c	O
such	O
that	O
for	O
more	O
information	O
about	O
these	O
inequalities	O
see	O
also	O
vapnik	O
gaenssler	O
gaenssler	O
and	O
stute	O
and	O
massart	O
it	O
is	O
only	O
natural	O
to	O
ask	O
whether	O
the	O
uniform	O
law	O
of	O
large	O
numbers	O
sup	O
ivna	O
aea	O
holds	O
if	O
we	O
allow	O
a	O
to	O
be	O
the	O
class	O
of	O
all	O
measurable	O
subsets	O
of	O
nd	O
in	O
this	O
case	O
the	O
supremum	O
above	O
is	O
called	O
the	O
total	B
variation	B
between	O
the	O
measures	O
vn	O
and	O
v	O
the	O
convergence	O
clearly	O
can	O
not	O
hold	O
if	O
vn	O
is	O
the	O
standard	B
empirical	B
measure	I
vapnik-chevonenkis	O
theory	O
but	O
is	O
there	O
another	O
empirical	B
measure	I
such	O
that	O
the	O
convergence	O
holds	O
the	O
somewhat	O
amusing	O
answer	O
is	O
no	O
as	O
devroye	O
and	O
gyorfi	O
proved	O
for	O
any	O
empirical	B
measure	I
vn-that	O
is	O
a	O
function	O
depending	O
on	O
zl	O
zn	O
assigning	O
a	O
nonnegative	O
number	O
to	O
any	O
measurable	O
set-there	O
exists	O
a	O
distribution	O
of	O
z	O
such	O
that	O
for	O
all	O
n	O
sup	O
ivna	O
aea	O
almost	O
surely	O
thus	O
in	O
this	O
generality	O
the	O
problem	O
is	O
hopeless	O
for	O
meaningful	O
results	O
either	O
a	O
or	O
v	O
must	O
be	O
restricted	O
for	O
example	O
if	O
we	O
assume	O
that	O
v	O
is	O
absolutely	O
continuous	O
with	O
density	O
f	O
and	O
that	O
vn	O
is	O
absolutely	O
continuous	O
too	O
density	O
fn	O
then	O
by	O
scheffes	O
theorem	O
rd	O
sup	O
ivna	O
aea	O
i	O
fnx	O
fx	O
problem	O
but	O
as	O
we	O
see	O
from	O
problems	O
and	O
there	O
exist	O
density	O
estimators	O
as	O
histogram	O
and	O
kernel	B
estimates	O
such	O
that	O
the	O
l	O
converges	O
to	O
zero	O
almost	O
surely	O
for	O
all	O
possible	O
densities	O
therefore	O
the	O
total	B
variation	B
between	O
the	O
empirical	B
measures	O
derived	O
from	O
these	O
density	O
estimates	O
and	O
the	O
true	O
measure	O
converges	O
to	O
zero	O
almost	O
surely	O
for	O
all	O
distributions	O
with	O
a	O
density	O
for	O
other	O
large	O
classes	O
of	O
distributions	O
that	O
can	O
be	O
estimated	O
consistently	O
in	O
total	B
variation	B
we	O
refer	O
to	O
barron	O
gyorfi	O
and	O
van	O
der	O
meulen	O
problems	O
and	O
exercises	O
for	O
all	O
t	O
and	O
some	O
c	O
then	O
problem	O
prove	O
that	O
if	O
a	O
nonnegative	O
random	O
variable	B
z	O
satisfies	O
pz	O
t	O
furthermore	O
ez	O
jloe	O
hint	O
use	O
the	O
identity	O
tdt	O
and	O
set	O
iou	O
luoo	O
bound	O
the	O
first	O
integral	O
by	O
u	O
and	O
the	O
second	O
by	O
the	O
exponential	B
inequality	B
find	O
the	O
value	O
of	O
u	O
that	O
minimizes	O
the	O
obtained	O
upper	O
bound	O
problem	O
generalize	O
the	O
arguments	O
of	O
theorems	O
and	O
to	O
prove	O
theorems	O
and	O
problem	O
determine	O
the	O
fingering	B
dimension	B
of	O
classes	O
of	O
classifiers	O
c	O
ixeaa	O
e	O
a	O
if	O
the	O
class	O
ais	O
the	O
class	O
of	O
all	O
closed	O
intervals	O
in	O
n	O
the	O
class	O
of	O
all	O
sets	O
obtained	O
as	O
the	O
union	O
of	O
m	O
closed	O
intervals	O
in	O
n	O
the	O
class	O
of	O
balls	O
in	O
n	O
d	O
centered	O
at	O
the	O
origin	O
the	O
class	O
of	O
all	O
balls	O
in	O
n	O
d	O
problems	O
and	O
exercises	O
the	O
class	O
of	O
sets	O
of	O
the	O
form	O
xd	O
x	O
x	O
xd	O
in	O
n	O
d	O
and	O
the	O
class	O
of	O
all	O
convex	O
polygons	O
in	O
problem	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
with	O
fingering	B
dimension	B
k	O
of	O
n	O
show	O
that	O
vc	O
k	O
log	O
k	O
problem	O
prove	O
that	O
theorem	O
implies	O
corollary	O
hint	O
find	O
ne	O
such	O
that	O
vc	O
whenever	O
n	O
n	O
to	O
see	O
this	O
first	O
show	O
that	O
n	O
vc	O
is	O
satisfied	O
for	O
n	O
log	O
which	O
follows	O
from	O
the	O
fact	O
that	O
log	O
x	O
x	O
if	O
x	O
but	O
in	O
this	O
case	O
vc	O
the	O
upper	O
bound	O
does	O
not	O
exceed	O
if	O
n	O
log	O
problem	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
n	O
d	O
and	O
let	O
be	O
a	O
fier	O
minimizing	O
the	O
empirical	B
error	I
probability	O
measured	O
on	O
dn	O
assume	O
that	O
we	O
have	O
an	O
algorithm	B
which	O
selects	O
a	O
classifier	B
gn	O
such	O
that	O
where	O
and	O
are	O
sequences	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
show	O
that	O
p	O
inf	O
l	O
e	O
on	O
iln	O
l	O
e	O
en	O
ec	O
ec	O
find	O
conditions	O
on	O
and	O
so	O
that	O
elgn	O
inf	O
l	O
ec	O
ogn	O
n	O
that	O
is	O
elgn	O
converges	O
to	O
the	O
optimum	O
at	O
the	O
same	O
order	O
as	O
the	O
error	O
probability	O
of	O
problem	O
prove	O
that	O
p	O
sup	O
avnao	O
ivna	O
e	O
sup	O
avnao	O
ivna	O
holds	O
if	O
ne	O
this	O
inequality	B
is	O
needed	O
to	O
complete	O
the	O
proof	O
of	O
theorem	O
hint	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
introduce	O
a	O
with	O
and	O
justify	O
the	O
validity	O
of	O
the	O
steps	O
of	O
the	O
following	O
chain	O
of	O
inequalities	O
p	O
lfo	O
ivna	O
e	O
ejp	O
va	O
i	O
zi	O
zn	O
p	O
e	O
p	O
e	O
where	O
bn	O
e	O
is	O
a	O
binomial	B
random	O
variable	B
with	O
parameters	O
nand	O
e	O
finish	O
the	O
proof	O
by	O
showing	O
that	O
the	O
probability	O
on	O
the	O
right-hand	O
side	O
is	O
greater	O
than	O
or	O
equal	O
to	O
if	O
ne	O
the	O
slightly	O
more	O
restrictive	O
condition	O
ne	O
this	O
follows	O
from	O
chebyshevs	O
inequality	B
vapq	O
ik-chervonenkis	O
theory	O
problem	O
prove	O
that	O
theorem	O
implies	O
that	O
if	O
inf	O
ec	O
then	O
i	O
n	O
og	O
we	O
note	O
here	O
that	O
haussler	O
littlestone	O
and	O
warmuth	O
demonstrated	O
the	O
existence	O
of	O
a	O
classifier	B
with	O
when	O
inf	O
ec	O
o	O
hint	O
use	O
the	O
identity	O
fooo	O
px	O
tdt	O
for	O
nonnegative	O
random	O
variables	O
x	O
and	O
employ	O
the	O
fact	O
that	O
ex	O
sc	O
n	O
nvc	O
theorem	O
problem	O
prove	O
the	O
following	O
version	O
of	O
theorem	O
let	O
be	O
a	O
function	O
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
a	O
class	O
in	O
c	O
assume	O
that	O
inf	O
ec	O
o	O
then	O
p	O
e	O
anthony	O
and	O
biggs	O
hint	O
modify	O
the	O
proof	O
of	O
theorem	O
by	O
introducing	O
a	O
ghost	B
sample	I
z	O
with	O
size	O
m	O
be	O
specified	O
after	O
optimization	O
only	O
the	O
first	O
symmetrization	B
step	O
needs	O
adjusting	O
show	O
that	O
for	O
any	O
a	O
e	O
p	O
sup	O
va	O
es	O
i	O
mete	O
p	O
sup	O
vna	O
ae	O
aiia-o	O
e	O
avla-o	O
where	O
vi	O
is	O
the	O
empirical	B
measure	I
based	O
on	O
the	O
ghost	B
sample	I
bernsteins	O
theorem	O
the	O
rest	O
of	O
the	O
proof	O
is	O
similar	O
to	O
that	O
of	O
theorem	O
choose	O
m	O
n	O
and	O
a	O
nn	O
m	O
problem	O
prove	O
that	O
alexanders	O
bound	O
implies	O
that	O
if	O
a	O
is	O
a	O
vapnik-chervonenkis	B
class	O
with	O
vc	B
dimension	B
v	O
va	O
then	O
e	O
sup	O
ivna	O
aea	O
v	O
loge	O
v	O
r	O
v	O
n	O
hint	O
justify	O
the	O
following	O
steps	O
if	O
is	O
a	O
negative	O
decreasing	O
concave	O
function	O
then	O
eii	O
hint	O
bound	O
li	O
by	O
using	O
its	O
taylor	O
series	O
expansion	O
etdt	O
f	O
oo	O
ii	O
let	O
b	O
be	O
fixed	O
then	O
for	O
u	O
hint	O
use	O
the	O
previous	O
step	O
let	O
x	O
be	O
a	O
positive	O
random	O
variable	B
for	O
which	O
px	O
u	O
au	O
h	O
e-	O
u	O
where	O
a	O
b	O
c	O
are	O
positive	O
constants	O
then	O
if	O
b	O
e	O
ex	O
jblogb	O
a	O
hint	O
use	O
ex	O
fooo	O
px	O
u	O
and	O
bound	O
the	O
probability	O
either	O
by	O
one	O
or	O
by	O
the	O
bound	O
of	O
the	O
previous	O
step	O
problem	O
use	O
alexanders	O
inequality	B
to	O
obtain	O
the	O
following	O
sample	O
size	O
bound	O
for	O
empirical	B
error	I
minimization	O
problems	O
and	O
exercises	O
ne	O
s	O
max	O
zlog	O
e	O
e	O
for	O
what	O
values	O
of	O
e	O
does	O
this	O
bound	O
beat	O
corollary	O
problem	O
let	O
zi	O
zn	O
be	O
i	O
i	O
d	O
random	O
variables	O
in	O
rd	O
with	O
measure	O
v	O
and	O
standard	B
empirical	B
measure	I
vn	O
let	O
a	O
be	O
an	O
arbitrary	O
class	O
of	O
subsets	O
of	O
rd	O
show	O
that	O
as	O
n	O
if	O
and	O
only	O
if	O
sup	O
ivna	O
aea	O
in	O
probability	O
sup	O
ivna	O
with	O
probability	O
one	O
aea	O
hint	O
use	O
mcdiarmids	O
inequality	B
problem	O
prove	O
scheffes	O
theorem	O
let	O
p	O
and	O
v	O
be	O
absolute	O
continuous	O
ability	O
measures	O
on	O
nd	O
with	O
densities	O
i	O
and	O
g	O
respectively	O
prove	O
that	O
sup	O
f	O
ix	O
gxldx	O
aea	O
where	O
a	O
is	O
the	O
class	O
of	O
all	O
borel-measurable	O
sets	O
hint	O
show	O
that	O
the	O
supremum	O
is	O
achieved	O
for	O
the	O
set	O
ix	O
gx	O
problem	O
learning	B
based	O
on	O
empirical	B
covering	O
this	O
problem	O
demonstrates	O
an	O
alternative	O
method	O
of	O
picking	O
a	O
classifier	B
which	O
works	O
as	O
well	O
as	O
empirical	B
error	I
tion	O
the	O
method	O
based	O
on	O
empirical	B
covering	O
of	O
the	O
class	O
of	O
classifiers	O
was	O
introduced	O
by	O
buescher	O
and	O
kumar	O
the	O
idea	O
of	O
covering	O
the	O
class	O
goes	O
back	O
to	O
vapnik	O
see	O
also	O
benedek	O
and	O
itai	O
kulkarni	O
and	O
dudley	O
kulkarni	O
richardson	O
and	O
zeitouni	O
letc	O
be	O
a	O
class	O
ofclassifierscp	O
rd	O
i	O
thedatasetdn	O
is	O
split	O
into	O
two	O
parts	O
dm	O
xi	O
yi	O
ym	O
and	O
tt	O
xmi	O
ymi	O
yn	O
where	O
n	O
m	O
i	O
we	O
use	O
the	O
first	O
part	O
dm	O
to	O
cover	O
c	O
as	O
follows	O
define	O
the	O
random	O
variable	B
n	O
as	O
the	O
number	O
of	O
different	O
values	O
the	O
binary	B
vector	O
bmcp	O
i	O
cpxm	O
takes	O
as	O
cp	O
is	O
varied	O
over	O
c	O
clearly	O
n	O
s	O
sc	O
m	O
take	O
n	O
classifiers	O
from	O
c	O
such	O
that	O
all	O
n	O
possible	O
values	O
of	O
the	O
binary	B
vector	O
b	O
m	O
are	O
represented	O
exactly	O
once	O
denote	O
these	O
classifiers	O
by	O
n	O
among	O
these	O
functions	O
pick	O
one	O
that	O
minimizes	O
the	O
empirical	B
error	I
on	O
the	O
second	O
part	O
of	O
the	O
data	O
set	O
tt	O
denote	O
the	O
selected	O
classifier	B
by	O
show	O
that	O
for	O
every	O
n	O
m	O
and	O
e	O
the	O
difference	O
between	O
the	O
error	O
probability	O
lpl	O
p	O
yidn	O
and	O
the	O
minimal	O
error	O
probability	O
in	O
the	O
class	O
satisfies	O
vapnik-chervonenkis	B
theory	O
and	O
kumar	O
for	O
example	O
by	O
taking	O
m	O
we	O
get	O
e	O
ljn	O
f	O
clogn	O
inf	O
lj	O
c	O
n	O
where	O
c	O
is	O
a	O
universal	O
constant	O
the	O
fact	O
that	O
the	O
number	O
of	O
samples	O
m	O
used	O
for	O
covering	O
c	O
is	O
very	O
small	O
compared	O
to	O
n	O
may	O
make	O
the	O
algorithm	B
computationally	O
more	O
attractive	O
than	O
the	O
method	O
of	O
empirical	B
error	I
minimization	O
hint	O
use	O
the	O
decomposition	O
p	O
n	O
inf	O
lj	O
e	O
bound	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
by	O
using	O
lemma	O
and	O
hoeffdings	O
inequality	B
to	O
bound	O
the	O
second	O
term	O
of	O
the	O
decomposition	O
observe	O
that	O
inf	O
l	O
i	O
i-i	O
inf	O
lj	O
sup	O
ilj	O
l	O
sup	O
pjxi	O
sup	O
va	O
aeavmao	O
where	O
a	O
i	O
ix	O
i	O
e	O
c	O
and	O
vma	O
lxea	O
bound	O
the	O
latter	O
quantity	O
by	O
applying	O
theorem	O
to	O
do	O
this	O
you	O
will	O
need	O
to	O
bound	O
the	O
shatter	O
coefficients	O
sea	O
in	O
chapter	O
we	O
introduce	O
simple	O
tools	O
for	O
this	O
for	O
example	O
it	O
is	O
easy	O
to	O
deduce	O
from	O
parts	O
and	O
of	O
theorem	O
that	O
sea	O
problem	O
prove	O
that	O
for	O
all	O
e	O
e	O
p	O
va	O
e	O
csa	O
aea	O
where	O
c	O
hint	O
proceed	O
as	O
indicated	O
by	O
the	O
following	O
steps	O
introduce	O
an	O
i	O
i	O
d	O
ghost	B
sample	I
zi	O
z	O
of	O
size	O
m	O
n	O
where	O
zi	O
is	O
distributed	O
as	O
z	O
denote	O
the	O
corresponding	O
empirical	B
measure	I
by	O
v	O
as	O
in	O
the	O
proof	O
of	O
the	O
first	O
step	O
of	O
theorem	O
prove	O
that	O
for	O
a	O
e	O
e	O
p	O
ivna	O
ae	O
aea	O
p	O
ivna	O
va	O
e	O
e	O
m	O
aea	O
introduce	O
permutations	O
it	O
r	O
itnm	O
of	O
the	O
n	O
m	O
random	O
variables	O
as	O
in	O
step	O
of	O
the	O
proof	O
of	O
theorem	O
show	O
that	O
problems	O
and	O
exercises	O
ii	O
ll	O
iatijczi-	O
lrl	O
jactijczici-ae	O
show	O
that	O
for	O
each	O
a	O
e	O
a	O
by	O
using	O
hoeffdings	O
inequality	B
for	O
sampling	B
without	I
replacement	I
from	O
valued	O
elements	O
theorem	O
choose	O
a	O
ine	O
combinatorial	O
aspects	O
of	O
vapnik	O
theory	O
shatter	O
coefficients	O
and	O
vc	B
dimension	B
in	O
this	O
section	O
we	O
list	O
a	O
few	O
interesting	O
properties	O
of	O
shatter	O
coefficients	O
sea	O
n	O
and	O
of	O
the	O
vc	B
dimension	B
v	O
a	O
of	O
a	O
class	O
of	O
sets	O
a	O
we	O
begin	O
with	O
a	O
property	O
that	O
makes	O
things	O
easier	O
in	O
chapter	O
we	O
noted	O
the	O
importance	O
of	O
classes	O
of	O
the	O
form	O
a	O
x	O
u	O
a	O
c	O
x	O
a	O
e	O
a	O
sets	O
a	O
are	O
of	O
the	O
form	O
i	O
and	O
the	O
sets	O
in	O
a	O
are	O
sets	O
of	O
pairs	O
y	O
for	O
which	O
y	O
recall	O
that	O
if	O
c	O
is	O
a	O
class	O
of	O
classifiers	O
rd	O
i	O
then	O
by	O
definition	O
sc	O
n	O
sea	O
n	O
and	O
vc	O
va	O
the	O
first	O
result	O
states	O
that	O
sc	O
n	O
sea	O
n	O
so	O
it	O
suffices	O
to	O
investigate	O
properties	O
of	O
a	O
a	O
class	O
of	O
subsets	O
of	O
rd	O
theorem	O
for	O
every	O
n	O
we	O
have	O
sea	O
n	O
sea	O
n	O
and	O
therefore	O
va	O
va	O
proof	O
let	O
n	O
be	O
a	O
positive	O
integer	O
we	O
show	O
that	O
for	O
any	O
n	O
pairs	O
from	O
rd	O
x	O
i	O
if	O
n	O
sets	O
from	O
a	O
pick	O
n	O
different	O
subsets	O
of	O
the	O
n	O
pairs	O
then	O
there	O
are	O
n	O
corresponding	O
sets	O
in	O
a	O
that	O
pick	O
n	O
different	O
subsets	O
of	O
n	O
points	O
in	O
r	O
d	O
and	O
vice	O
versa	O
fix	O
n	O
pairs	O
note	O
that	O
since	O
ordering	O
does	O
not	O
matter	O
we	O
may	O
arrange	O
any	O
n	O
pairs	O
in	O
this	O
manner	O
assume	O
that	O
for	O
a	O
certain	O
set	O
a	O
e	O
a	O
the	O
corresponding	O
set	O
a	O
a	O
x	O
u	O
ac	O
x	O
e	O
a	O
picks	O
out	O
the	O
pairs	O
that	O
is	O
the	O
set	O
of	O
these	O
pairs	O
is	O
the	O
intersection	O
of	O
a	O
and	O
the	O
n	O
pairs	O
again	O
we	O
can	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
pairs	O
are	O
ordered	B
in	O
this	O
way	O
this	O
means	O
that	O
a	O
picks	O
from	O
the	O
set	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	B
theory	O
the	O
subset	O
xmll	O
and	O
the	O
two	O
subsets	O
uniquely	O
determine	O
each	O
other	O
this	O
proves	O
sea	O
n	O
sea	O
n	O
to	O
prove	O
the	O
other	O
direction	O
notice	O
that	O
if	O
a	O
picks	O
a	O
subset	O
of	O
k	O
points	O
xl	O
xb	O
then	O
the	O
corresponding	O
set	O
a	O
e	O
a	O
picks	O
the	O
pairs	O
with	O
the	O
same	O
indices	O
from	O
o	O
equality	O
of	O
the	O
vc	O
dimensions	O
follows	O
from	O
the	O
equality	O
of	O
the	O
shatter	O
coefficients	O
the	O
following	O
theorem	O
attributed	O
to	O
vapnik	O
and	O
chervonenkis	O
and	O
sauer	O
describes	O
the	O
relationship	O
between	O
the	O
vc	B
dimension	B
and	O
shatter	O
cients	O
of	O
a	O
class	O
of	O
sets	O
this	O
is	O
the	O
most	O
important	O
tool	O
for	O
obtaining	O
useful	O
upper	O
bounds	O
on	O
the	O
shatter	O
coefficients	O
in	O
terms	O
of	O
the	O
vc	B
dimension	B
theorem	O
if	O
a	O
is	O
a	O
class	O
of	O
sets	O
with	O
vc	B
dimension	B
v	O
a	O
then	O
for	O
every	O
n	O
sea	O
n	O
t	O
io	O
proof	O
recall	O
the	O
definition	B
of	I
the	O
shatter	O
coefficients	O
sea	O
n	O
max	O
naxl	O
xn	O
where	O
naxi	O
ixi	O
e	O
all	O
clearly	O
it	O
suffices	O
to	O
prove	O
that	O
for	O
every	O
xl	O
x	O
n	O
but	O
since	O
n	O
axi	O
xn	O
is	O
just	O
the	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
finite	O
sets	O
we	O
need	O
only	O
to	O
prove	O
the	O
theorem	O
for	O
finite	O
sets	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
a	O
is	O
a	O
class	O
of	O
subsets	O
of	O
xn	O
with	O
vc	B
dimension	B
va	O
note	O
that	O
in	O
this	O
case	O
sea	O
n	O
iai	O
we	O
prove	O
the	O
theorem	O
by	O
induction	O
with	O
respect	O
to	O
n	O
and	O
va	O
the	O
statement	O
is	O
obviously	O
true	O
for	O
n	O
for	O
any	O
class	O
with	O
v	O
a	O
it	O
is	O
also	O
true	O
for	O
any	O
n	O
if	O
va	O
since	O
sea	O
n	O
for	O
all	O
n	O
in	O
this	O
case	O
thus	O
we	O
assume	O
va	O
assume	O
that	O
the	O
statement	O
is	O
true	O
for	O
all	O
k	O
n	O
for	O
all	O
classes	O
of	O
subsets	O
of	O
xd	O
with	O
vc	B
dimension	B
not	O
exceeding	O
va	O
and	O
for	O
n	O
for	O
all	O
classes	O
with	O
vc	B
dimension	B
smaller	O
than	O
va	O
define	O
the	O
following	O
two	O
classes	O
of	O
subsets	O
of	O
xn	O
a	O
a	O
e	O
a	O
and	O
shatter	O
coefficients	O
and	O
vc	B
dimension	B
note	O
that	O
both	O
a	O
and	O
a	O
contain	O
subsets	O
of	O
xn-d	O
a	O
contains	O
all	O
sets	O
a	O
that	O
are	O
members	O
of	O
a	O
such	O
that	O
au	O
is	O
also	O
in	O
a	O
but	O
xn	O
tj	O
a	O
then	O
iai	O
iai	O
iai	O
to	O
see	O
this	O
write	O
a	O
xn	O
e	O
a	O
a	O
e	O
aru	O
xn	O
tj	O
a	O
a	O
e	O
a	O
u	O
thus	O
iai	O
n	O
ia	O
xn	O
e	O
a	O
a	O
e	O
ai	O
ia	O
xn	O
tj	O
a	O
a	O
e	O
ai	O
ia	O
xn	O
e	O
a	O
a	O
e	O
ai	O
ia	O
xn	O
tj	O
a	O
a	O
e	O
ai	O
iai	O
iai	O
iai-iai	O
since	O
iai	O
iai	O
and	O
a	O
is	O
a	O
class	O
of	O
subsets	O
of	O
the	O
induction	O
hypothesis	O
implies	O
that	O
iai	O
sea	O
n	O
io	O
next	O
we	O
show	O
that	O
va	O
s	O
va	O
which	O
will	O
imply	O
by	O
the	O
induction	O
hypothesis	O
to	O
see	O
this	O
consider	O
a	O
set	O
s	O
c	O
xn-d	O
that	O
is	O
shattered	O
by	O
a	O
then	O
s	O
u	O
is	O
shattered	O
by	O
a	O
to	O
prove	O
this	O
we	O
have	O
to	O
show	O
that	O
any	O
set	O
s	O
c	O
sand	O
s	O
u	O
is	O
the	O
intersection	O
of	O
s	O
u	O
and	O
a	O
set	O
from	O
a	O
since	O
s	O
is	O
shattered	O
by	O
a	O
if	O
s	O
c	O
s	O
then	O
there	O
exists	O
a	O
set	O
a	O
e	O
a	O
such	O
that	O
s	O
s	O
n	O
a	O
but	O
since	O
by	O
definition	O
xn	O
tj	O
a	O
we	O
must	O
have	O
s	O
u	O
n	O
a	O
and	O
sf	O
u	O
u	O
n	O
since	O
by	O
the	O
definition	B
of	I
a	O
both	O
a	O
and	O
au	O
are	O
in	O
a	O
we	O
see	O
that	O
s	O
u	O
is	O
indeed	O
shattered	O
by	O
a	O
but	O
any	O
set	O
that	O
is	O
shattered	O
by	O
a	O
must	O
have	O
cardinality	O
not	O
exceeding	O
va	O
therefore	O
lsi	O
va	O
l	O
but	O
s	O
was	O
an	O
arbitrary	O
set	O
shattered	O
by	O
a	O
which	O
means	O
va	O
va	O
thus	O
we	O
have	O
shown	O
that	O
va-l	O
sea	O
n	O
iai	O
iai	O
iai	O
l	O
n	O
l	O
n	O
va	O
io	O
io	O
straightforward	O
application	O
of	O
the	O
identity	O
cl	O
shows	O
that	O
comhinatorial	O
aspects	O
of	O
vapnik-chervonenkis	B
theory	O
theorem	O
has	O
some	O
very	O
surprising	O
implications	O
for	O
example	O
it	O
follows	O
immediately	O
from	O
the	O
binomial	B
theorem	I
that	O
sea	O
n	O
va	O
this	O
means	O
that	O
a	O
shatter	B
coefficient	I
falls	O
in	O
one	O
of	O
two	O
categories	O
either	O
sea	O
n	O
for	O
all	O
n	O
or	O
sea	O
n	O
lva	O
which	O
happens	O
if	O
the	O
vc	B
dimension	B
of	O
a	O
is	O
finite	O
we	O
cannot	O
have	O
sea	O
n	O
for	O
example	O
if	O
va	O
the	O
upper	O
bound	O
in	O
theorem	O
decreases	O
exponentially	O
quickly	O
with	O
n	O
other	O
sharper	O
bounds	O
are	O
given	O
below	O
theorem	O
for	O
all	O
n	O
v	O
sea	O
n	O
l	O
va	O
en	O
va	O
io	O
l	O
va	O
theorem	O
follows	O
from	O
theorem	O
below	O
we	O
leave	O
the	O
details	O
as	O
an	O
exercise	O
problem	O
theorem	O
for	O
all	O
n	O
and	O
va	O
n	O
h	O
sea	O
n	O
en	O
n	O
where	O
hx	O
logx	O
x	O
logo	O
xfor	O
x	O
e	O
and	O
ho	O
ho	O
is	O
the	O
binary	B
entropy	B
function	O
theorem	O
is	O
a	O
consequence	O
of	O
theorem	O
and	O
the	O
inequality	B
below	O
a	O
different	O
probabilistic	O
proof	O
is	O
sketched	O
in	O
problem	O
also	O
problem	O
lemma	O
for	O
k	O
n	O
proof	O
introduce	O
kin	O
by	O
the	O
binomial	B
theorem	I
e	O
t	O
io	O
l	O
the	O
desired	O
inequality	B
shatter	O
coefficients	O
of	O
some	O
classes	O
remark	O
the	O
binary	B
entropy	B
function	O
hx	O
plays	O
a	O
central	O
role	O
in	O
information	O
theory	O
e	O
g	O
csiszar	O
and	O
korner	O
cover	O
and	O
thomas	O
its	O
main	O
properties	O
are	O
the	O
following	O
hx	O
is	O
symmetric	O
around	O
where	O
it	O
takes	O
its	O
maximum	O
it	O
is	O
continuous	O
concave	O
strictly	O
monotone	O
increasing	O
in	O
decreasing	O
in	O
and	O
equals	O
zero	O
for	O
x	O
and	O
x	O
next	O
we	O
present	O
some	O
simple	O
results	O
about	O
shatter	O
coefficients	O
of	O
classes	O
that	O
are	O
obtained	O
by	O
combinations	O
of	O
classes	O
of	O
sets	O
theorem	O
if	O
a	O
al	O
u	O
a	O
then	O
sea	O
n	O
sai	O
n	O
n	O
given	O
a	O
class	O
a	O
define	O
ac	O
c	O
for	O
a	O
n	O
a	O
a	O
e	O
a	O
a	O
e	O
a	O
sea	O
n	O
sea	O
nsa	O
n	O
for	O
a	O
a	O
a	O
e	O
a	O
a	O
e	O
a	O
sea	O
n	O
sea	O
nsa	O
n	O
for	O
a	O
x	O
a	O
a	O
e	O
a	O
a	O
e	O
a	O
sea	O
n	O
sea	O
nsa	O
n	O
a	O
e	O
a	O
then	O
sac	O
n	O
sea	O
n	O
proof	O
and	O
are	O
trivial	O
to	O
prove	O
fix	O
n	O
points	O
xl	O
x	O
n	O
and	O
assume	O
that	O
a	O
picks	O
n	O
sea	O
n	O
subsets	O
c	O
i	O
cn	O
then	O
a	O
picks	O
from	O
c	O
at	O
most	O
sea	O
icil	O
subsets	O
therefore	O
sets	O
of	O
the	O
form	O
ana	O
pick	O
at	O
most	O
n	O
l	O
sea	O
i	O
ci	O
i	O
sea	O
n	O
n	O
il	O
subsets	O
here	O
we	O
used	O
the	O
obvious	O
monotonicity	O
property	O
sea	O
n	O
sea	O
n	O
m	O
to	O
prove	O
observe	O
that	O
a	O
u	O
a	O
a	O
e	O
a	O
a	O
e	O
a	O
n	O
a	O
c	O
c	O
a	O
e	O
a	O
a	O
e	O
a	O
the	O
statement	O
now	O
follows	O
from	O
and	O
shatter	O
coefficients	O
of	O
some	O
classes	O
here	O
we	O
calculate	O
shatter	O
coefficients	O
of	O
some	O
simple	O
but	O
important	O
examples	O
of	O
classes	O
of	O
subsets	O
of	O
nd	O
we	O
begin	O
with	O
a	O
simple	O
observation	O
theorem	O
if	O
a	O
contains	O
finitely	O
many	O
sets	O
then	O
va	O
sea	O
n	O
iai	O
for	O
every	O
n	O
lai	O
and	O
proof	O
the	O
first	O
inequality	B
follows	O
from	O
the	O
fact	O
that	O
at	O
least	O
sets	O
are	O
necessary	O
to	O
shatter	O
n	O
points	O
the	O
second	O
inequality	B
is	O
trivial	O
combinatorial	O
aspects	O
of	O
vapnik	O
theory	O
in	O
the	O
next	O
example	O
it	O
is	O
interesting	O
to	O
observe	O
that	O
the	O
bound	O
of	O
theorem	O
is	O
tight	O
theorem	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
half	O
lines	O
a	O
x	O
x	O
e	O
r	O
then	O
va	O
and	O
sa	O
n	O
n	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
intervals	O
in	O
r	O
then	O
va	O
and	O
sea	O
n	O
nn	O
proof	O
is	O
easy	O
to	O
see	O
that	O
va	O
in	O
observe	O
that	O
if	O
we	O
fix	O
three	O
different	O
points	O
in	O
r	O
then	O
there	O
is	O
no	O
interval	O
that	O
does	O
not	O
contain	O
the	O
middle	O
point	O
but	O
does	O
contain	O
the	O
other	O
two	O
the	O
shatter	B
coefficient	I
can	O
be	O
calculated	O
by	O
counting	O
that	O
there	O
are	O
at	O
most	O
n	O
k	O
sets	O
in	O
n	O
x	O
n	O
a	O
e	O
a	O
such	O
that	O
ia	O
n	O
k	O
for	O
k	O
n	O
and	O
one	O
set	O
such	O
that	O
ia	O
n	O
xn	O
i	O
this	O
gives	O
altogether	O
o	O
l	O
-n	O
k	O
kl	O
nn	O
now	O
we	O
can	O
generalize	O
the	O
result	O
above	O
for	O
classes	O
of	O
intervals	O
and	O
rectangles	O
in	O
rd	O
theorem	O
if	O
a	O
xd	O
x	O
x	O
xd	O
then	O
va	O
d	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
rectangles	O
in	O
r	O
d	O
then	O
va	O
proof	O
we	O
prove	O
the	O
first	O
part	O
is	O
left	O
as	O
an	O
exercise	O
we	O
have	O
to	O
show	O
that	O
there	O
are	O
points	O
that	O
can	O
be	O
shattered	O
by	O
a	O
but	O
for	O
any	O
set	O
of	O
points	O
there	O
is	O
a	O
subset	O
of	O
it	O
that	O
can	O
not	O
be	O
picked	O
by	O
sets	O
in	O
a	O
to	O
see	O
the	O
first	O
part	O
just	O
consider	O
the	O
following	O
points	O
figure	O
shatter	O
coefficients	O
of	O
some	O
classes	O
figure	O
rectangles	O
shatter	O
points	O
in	O
gj	O
on	O
the	O
other	O
hand	O
for	O
any	O
given	O
set	O
of	O
points	O
we	O
can	O
choose	O
a	O
subset	O
of	O
at	O
most	O
points	O
with	O
the	O
property	O
that	O
it	O
contains	O
a	O
point	O
with	O
largest	O
first	O
coordinate	O
a	O
point	O
with	O
smallest	O
first	O
coordinate	O
a	O
point	O
with	O
largest	O
second	O
coordinate	O
and	O
so	O
forth	O
clearly	O
there	O
is	O
no	O
set	O
in	O
a	O
that	O
contains	O
these	O
points	O
but	O
does	O
not	O
contain	O
the	O
others	O
d	O
max	O
y	O
figure	O
no	O
points	O
can	O
be	O
shattered	O
by	O
rectangles	O
in	O
minx	O
o	O
z	O
max	O
x	O
miny	O
theorem	O
dudley	O
let	O
be	O
afinite-dimensional	O
vector	O
space	O
of	O
real	O
functions	O
on	O
nd	O
the	O
class	O
of	O
sets	O
a	O
gx	O
o	O
g	O
e	O
g	O
has	O
vc	B
dimension	B
va	O
r	O
where	O
r	O
dimensiong	O
proof	O
it	O
suffices	O
to	O
show	O
that	O
no	O
set	O
of	O
size	O
m	O
r	O
can	O
be	O
shattered	O
by	O
sets	O
of	O
the	O
form	O
gx	O
o	O
fix	O
m	O
arbitrary	O
points	O
xl	O
x	O
m	O
and	O
define	O
the	O
linear	O
mapping	O
l	O
nm	O
as	O
combinatorial	O
aspects	O
of	O
vapnik	O
theory	O
then	O
the	O
image	O
of	O
q	O
lq	O
is	O
a	O
linear	O
subspace	O
of	O
nm	O
of	O
dimension	B
not	O
ceeding	O
the	O
dimension	B
of	O
q	O
that	O
is	O
m	O
then	O
there	O
exists	O
a	O
nonzero	O
vector	O
y	O
ym	O
e	O
n	O
m	O
that	O
is	O
orthogonal	O
to	O
lq	O
that	O
is	O
for	O
every	O
g	O
e	O
q	O
we	O
can	O
assume	O
that	O
at	O
least	O
one	O
of	O
the	O
yi	O
is	O
negative	O
rearrange	O
this	O
equality	O
so	O
that	O
terms	O
with	O
nonnegative	O
yi	O
stay	O
on	O
the	O
left-hand	O
side	O
l	O
yigxi	O
l	O
iyio	O
iyio	O
now	O
suppose	O
that	O
there	O
exists	O
age	O
q	O
such	O
that	O
the	O
set	O
gx	O
o	O
picks	O
exactly	O
the	O
xis	O
on	O
the	O
left-hand	O
side	O
then	O
all	O
terms	O
on	O
the	O
left-hand	O
side	O
are	O
nonnegative	O
while	O
the	O
terms	O
on	O
the	O
right-hand	O
side	O
must	O
be	O
negative	O
which	O
is	O
a	O
contradiction	O
so	O
xl	O
xm	O
cannot	O
be	O
shattered	O
and	O
the	O
proof	O
is	O
completed	O
remark	O
theorem	O
implies	O
that	O
the	O
shatter	O
coefficients	O
of	O
the	O
class	O
of	O
sets	O
in	O
theorem	O
are	O
bounded	O
as	O
follows	O
sea	O
n	O
t	O
io	O
in	O
many	O
cases	O
it	O
is	O
possible	O
to	O
get	O
sharper	O
estimates	O
let	O
be	O
the	O
linear	O
space	O
of	O
functions	O
spanned	O
by	O
some	O
fixed	O
functions	O
r	O
n	O
d	O
n	O
and	O
define	O
orx	O
cover	O
showed	O
that	O
if	O
for	O
some	O
xl	O
e	O
nd	O
every	O
r-element	O
subset	O
of	O
is	O
linearly	O
independent	O
then	O
the	O
n-th	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
sets	O
a	O
gx	O
o	O
g	O
e	O
q	O
actually	O
equals	O
sea	O
n	O
l	O
n	O
io	O
problem	O
by	O
using	O
the	O
last	O
identity	O
in	O
the	O
proof	O
of	O
theorem	O
it	O
is	O
easily	O
seen	O
that	O
the	O
difference	O
between	O
the	O
bound	O
obtained	O
from	O
theorem	O
and	O
the	O
true	O
value	O
is	O
using	O
covers	O
result	O
for	O
the	O
shatter	O
coefficients	O
we	O
can	O
actually	O
improve	O
theorem	O
in	O
that	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
a	O
equals	O
r	O
to	O
see	O
this	O
note	O
that	O
shatter	O
coefficients	O
of	O
some	O
classes	O
while	O
sea	O
r	O
t	O
io	O
l	O
io	O
l	O
r	O
therefore	O
no	O
r	O
points	O
are	O
shattered	O
it	O
is	O
interesting	O
that	O
theorem	O
above	O
combined	O
with	O
theorem	O
and	O
theorem	O
gives	O
the	O
bound	O
san	O
s	O
nr	O
when	O
r	O
covers	O
result	O
however	O
improves	O
it	O
to	O
sea	O
n	O
s	O
perhaps	O
the	O
most	O
important	O
class	O
of	O
sets	O
is	O
the	O
class	O
of	O
halfspaces	O
in	O
rd	O
that	O
is	O
sets	O
containing	O
points	O
falling	O
on	O
one	O
side	O
of	O
a	O
hyperplane	O
the	O
shatter	O
coefficients	O
of	O
this	O
class	O
can	O
be	O
obtained	O
from	O
the	O
results	O
above	O
corollary	O
let	O
a	O
be	O
the	O
class	O
of	O
half	O
spaces	O
that	O
is	O
subsets	O
ofrd	O
of	O
the	O
form	O
ax	O
b	O
where	O
a	O
e	O
rd	O
be	O
rtakeallpossiblevalues	O
then	O
va	O
and	O
proof	O
this	O
is	O
an	O
immediate	O
consequence	O
of	O
the	O
remark	O
above	O
if	O
we	O
take	O
to	O
be	O
the	O
linear	O
space	O
spanned	O
by	O
the	O
functions	O
xl	O
xed	O
and	O
where	O
xl	O
xed	O
denote	O
the	O
d	O
components	O
of	O
the	O
vector	O
x	O
it	O
is	O
equally	O
simple	O
now	O
to	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
of	O
all	O
closed	O
balls	O
in	O
rd	O
cover	O
devroye	O
or	O
dudley	O
for	O
more	O
information	O
corollary	O
let	O
a	O
be	O
the	O
class	O
of	O
all	O
closed	O
balls	O
in	O
r	O
d	O
that	O
is	O
subsets	O
of	O
rd	O
of	O
the	O
form	O
where	O
ai	O
ad	O
b	O
e	O
r	O
take	O
all	O
possible	O
values	O
then	O
va	O
s	O
d	O
proof	O
if	O
we	O
write	O
d	O
l	O
il	O
ixi	O
ail	O
d	O
b	O
l	O
il	O
d	O
d	O
lxiai	O
lal-	O
b	O
il	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	B
theory	O
then	O
it	O
is	O
clear	O
that	O
theorem	O
yields	O
the	O
result	O
by	O
setting	O
to	O
be	O
the	O
linear	O
space	O
spanned	O
by	O
d	O
l	O
ixui	O
z	O
xl	O
dlx	O
xed	O
and	O
dzx	O
il	O
it	O
follows	O
from	O
theorems	O
and	O
that	O
the	O
class	O
of	O
all	O
polytopes	O
with	O
a	O
bounded	O
number	O
of	O
faces	O
has	O
finite	O
vc	B
dimension	B
the	O
next	O
negative	O
example	O
demonstrates	O
that	O
this	O
boundedness	O
is	O
necessary	O
theorem	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
convex	O
polygons	O
in	O
r	O
z	O
then	O
va	O
figure	O
any	O
subset	O
of	O
n	O
points	O
on	O
the	O
unit	O
circle	O
can	O
be	O
picked	O
by	O
a	O
convex	O
polygon	O
proof	O
let	O
xl	O
e	O
lie	O
on	O
the	O
unit	O
circle	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
for	O
any	O
subset	O
of	O
these	O
points	O
there	O
is	O
a	O
polygon	O
that	O
picks	O
that	O
subset	O
linear	O
and	O
generalized	B
linear	O
discrimination	O
rules	O
recall	O
from	O
chapter	O
that	O
a	O
linear	O
classification	O
rule	B
classifies	O
x	O
into	O
one	O
of	O
the	O
two	O
classes	O
according	O
to	O
whether	O
d	O
ao	O
laixi	O
il	O
is	O
positive	O
or	O
negative	O
where	O
xci	O
denote	O
the	O
components	O
of	O
x	O
e	O
rd	O
the	O
coefficients	O
ai	O
are	O
determined	O
by	O
the	O
training	O
sequence	O
these	O
decisions	O
chotomize	O
the	O
space	O
r	O
d	O
by	O
virtue	O
of	O
a	O
halfspace	O
and	O
assign	O
class	O
to	O
one	O
halfspace	O
and	O
class	O
to	O
the	O
other	O
points	O
on	O
the	O
border	O
are	O
treated	O
as	O
belonging	O
to	O
class	O
o	O
consider	O
a	O
classifier	B
that	O
adjusts	O
the	O
coefficients	O
by	O
minimizing	O
the	O
number	O
of	O
linear	O
and	O
generalized	B
linear	O
discrimination	O
rules	O
errors	O
committed	O
on	O
dn	O
in	O
the	O
terminology	O
of	O
chapter	O
c	O
is	O
the	O
collection	O
of	O
all	O
linear	O
classifiers	O
o	O
figure	O
an	O
empirically	O
mal	O
linear	B
classifier	B
o	O
x	O
xs	O
o	O
decide	O
class	O
o	O
decide	O
class	O
xii	O
glick	O
pointed	O
out	O
that	O
for	O
the	O
error	O
probability	O
l	O
l	O
of	O
this	O
classifier	B
l	O
l	O
inf	O
ec	O
l	O
almost	O
surely	O
however	O
from	O
theorems	O
and	O
corollary	O
we	O
can	O
now	O
provide	O
more	O
details	O
theorem	O
for	O
all	O
nand	O
e	O
the	O
error	O
probability	O
l	O
of	O
the	O
empirically	O
optimal	O
linear	B
classifier	B
satisfies	O
p	O
n	O
inf	O
l	O
e	O
ec	O
comparing	O
the	O
above	O
inequality	B
with	O
theorem	O
note	O
that	O
there	O
we	O
selected	O
by	O
a	O
specific	O
algorithm	B
while	O
this	O
result	O
holds	O
for	O
any	O
linear	B
classifier	B
whose	O
empirical	B
error	I
is	O
minimal	O
generalized	B
linear	O
classification	O
rules	O
duda	O
and	O
hart	O
are	O
defined	O
by	O
where	O
d	O
is	O
a	O
positive	O
integer	O
the	O
functions	O
d	O
are	O
fixed	O
and	O
the	O
ficients	O
ao	O
are	O
functions	O
of	O
the	O
data	O
dn	O
these	O
include	O
for	O
example	O
all	O
quadratic	O
discrimination	O
rules	O
in	O
n	O
d	O
when	O
we	O
choose	O
all	O
functions	O
that	O
are	O
either	O
components	O
of	O
x	O
or	O
squares	O
of	O
components	O
of	O
x	O
or	O
products	O
of	O
two	O
components	O
of	O
x	O
that	O
is	O
the	O
functions	O
oi	O
are	O
of	O
the	O
form	O
either	O
xj	O
or	O
xj	O
xk	O
in	O
all	O
d	O
dd	O
the	O
argument	O
used	O
for	O
linear	O
discriminants	O
remain	O
valid	O
and	O
we	O
obtain	O
theorem	O
let	O
c	O
be	O
the	O
class	O
of	O
generalized	B
linear	O
discriminants	O
the	O
coefficients	O
vary	O
the	O
basis	O
functions	O
oi	O
are	O
fixed	O
for	O
the	O
error	O
probability	O
l	O
l	O
of	O
the	O
empirically	B
optimal	I
classifier	B
for	O
all	O
d	O
and	O
e	O
we	O
have	O
combinatorial	O
aspects	O
of	O
vapnik	O
theory	O
also	O
for	O
n	O
the	O
second	O
inequality	B
is	O
obtained	O
by	O
using	O
the	O
bound	O
of	O
theorem	O
for	O
the	O
shatter	O
coefficients	O
note	O
nevertheless	O
that	O
unless	O
d	O
therefore	O
c	O
is	O
allowed	O
to	O
increase	O
with	O
n	O
there	O
is	O
no	O
hope	O
of	O
obtaining	O
universal	B
consistency	B
the	O
question	O
of	O
universal	B
consistency	B
will	O
be	O
addressed	O
in	O
chapters	O
and	O
convex	O
sets	O
and	O
monotone	O
layers	O
classes	O
of	O
infinite	O
vc	B
dimension	B
are	O
not	O
hopeless	O
by	O
any	O
means	O
in	O
this	O
section	O
we	O
offer	O
examples	O
that	O
will	O
show	O
how	O
they	O
may	O
be	O
useful	O
in	O
pattern	O
recognition	O
the	O
classes	O
of	O
interest	O
to	O
us	O
for	O
now	O
are	O
c	O
all	O
convex	O
sets	O
of	O
n	O
c	O
all	O
monotone	O
layers	O
of	O
n	O
i	O
e	O
all	O
sets	O
of	O
the	O
form	O
for	O
some	O
nonincreasing	O
function	O
in	O
discrimination	O
this	O
corresponds	O
to	O
making	O
decisions	O
of	O
the	O
form	O
ixec	O
c	O
e	O
c	O
or	O
c	O
e	O
c	O
and	O
similarly	O
for	O
c	O
decisions	O
of	O
these	O
forms	O
are	O
important	O
in	O
many	O
situations	O
for	O
example	O
if	O
is	O
monotone	O
decreasing	O
in	O
both	O
components	O
of	O
x	O
e	O
then	O
the	O
bayes	O
rule	B
is	O
of	O
the	O
form	O
gx	O
hxel	O
for	O
some	O
l	O
e	O
c	O
we	O
have	O
pointed	O
out	O
elsewhere	O
and	O
problem	O
that	O
vc	O
v	O
c	O
ex	O
to	O
see	O
this	O
note	O
that	O
any	O
set	O
of	O
n	O
points	O
on	O
the	O
unit	O
circle	O
is	O
shattered	O
by	O
c	O
while	O
any	O
set	O
of	O
n	O
points	O
on	O
the	O
antidiagonal	O
is	O
shattered	O
by	O
c	O
nevertheless	O
shattering	B
becomes	O
unlikely	O
if	O
x	O
has	O
a	O
density	O
our	O
starting	O
point	O
here	O
is	O
the	O
bound	O
obtained	O
while	O
proving	O
theorem	O
where	O
nax	O
xnis	O
the	O
number	O
of	O
sets	O
in	O
n	O
xn	O
a	O
e	O
a	O
the	O
following	O
theorem	O
is	O
essential	O
theorem	O
if	O
x	O
has	O
a	O
density	O
f	O
on	O
n	O
then	O
when	O
a	O
is	O
either	O
c	O
or	O
this	O
theorem	O
a	O
proof	O
of	O
which	O
is	O
a	O
must	O
for	O
the	O
reader	O
implies	O
the	O
following	O
convex	O
sets	O
and	O
monotone	O
layers	O
corollary	O
let	O
x	O
have	O
a	O
density	O
f	O
on	O
let	O
be	O
picked	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
all	O
classifiers	O
of	O
the	O
form	O
if	O
x	O
e	O
a	O
if	O
x	O
tf	O
a	O
where	O
a	O
or	O
a	O
c	O
is	O
in	O
c	O
then	O
l	O
inf	O
l	O
for	O
c	O
or	O
cc	O
in	O
c	O
with	O
probability	O
one	O
similarly	O
for	O
proof	O
this	O
follows	O
from	O
the	O
inequality	B
of	O
lemma	O
theorem	O
and	O
the	O
borel-cantelli	B
lemma	I
d	O
remark	O
theorem	O
and	O
the	O
corollary	O
may	O
be	O
extended	O
to	O
n	O
d	O
but	O
this	O
eralization	O
holds	O
nothing	O
new	O
and	O
will	O
only	O
result	O
in	O
tedious	O
notations	O
d	O
proof	O
of	O
theorem	O
we	O
show	O
the	O
theorem	O
for	O
and	O
indicate	O
the	O
proof	O
for	O
c	O
take	O
two	O
sequences	O
of	O
integers	O
m	O
and	O
r	O
where	O
m	O
and	O
r	O
m	O
so	O
that	O
m	O
yet	O
as	O
n	O
consider	O
the	O
set	O
and	O
partition	B
each	O
side	O
into	O
m	O
equal	O
intervals	O
thus	O
obtaining	O
an	O
m	O
x	O
m	O
grid	O
of	O
square	O
cells	O
cl	O
cm	O
denote	O
co	O
let	O
no	O
nl	O
be	O
the	O
number	O
of	O
points	O
among	O
xl	O
xn	O
that	O
belong	O
to	O
these	O
cells	O
the	O
vector	O
n	O
l	O
n	O
is	O
clearly	O
multinomially	O
distributed	O
let	O
be	O
a	O
nonincreasing	O
function	O
n	O
n	O
defining	O
a	O
set	O
in	O
by	O
l	O
let	O
be	O
the	O
collection	O
of	O
all	O
cell	O
sets	O
cut	O
by	O
that	O
is	O
all	O
cells	O
with	O
a	O
nonempty	O
intersection	O
with	O
both	O
land	O
l	O
c	O
the	O
collection	O
c	O
is	O
shaded	O
in	O
figure	O
figure	O
a	O
monotone	B
layer	I
and	O
bordering	O
cells	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	B
theory	O
we	O
bound	O
n	O
dx	O
xn	O
from	O
above	O
conservatively	O
as	O
follows	O
the	O
number	O
of	O
different	O
collections	O
c	O
cannot	O
exceed	O
because	O
each	O
cell	O
in	O
c	O
may	O
be	O
obtained	O
from	O
its	O
predecessor	O
cell	O
by	O
either	O
moving	O
right	O
on	O
the	O
same	O
row	O
or	O
moving	O
down	O
one	O
cell	O
in	O
the	O
same	O
column	O
for	O
a	O
particular	O
collection	O
denoting	O
pi	O
px	O
e	O
c	O
we	O
have	O
pi	O
po	O
pi	O
po	O
n	O
ci	O
ci	O
applying	O
lemma	O
exp	O
co	O
p	O
po	O
exp	O
c	O
en	O
sets	O
a	O
witufa	O
f	O
l	O
f	O
a	O
u	O
ci	O
s	O
t	O
ci	O
because	O
i	O
m	O
and	O
r	O
and	O
by	O
the	O
absolute	O
continuity	O
of	O
x	O
as	O
this	O
estimate	O
is	O
uniform	O
over	O
all	O
collections	O
c	O
we	O
see	O
that	O
the	O
expected	O
value	O
of	O
is	O
not	O
more	O
than	O
eon	O
eon	O
the	O
argument	O
for	O
the	O
collection	O
c	O
of	O
convex	O
sets	O
is	O
analogous	O
remark	O
the	O
theorem	O
implies	O
that	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
convex	O
sets	O
then	O
with	O
probability	O
one	O
whenever	O
fl	O
has	O
a	O
density	O
this	O
supaea	O
iflna	O
is	O
a	O
special	O
case	O
of	O
a	O
result	O
of	O
ranga	O
rao	O
assume	O
now	O
that	O
the	O
density	O
f	O
of	O
x	O
is	O
bounded	O
and	O
of	O
bounded	O
support	B
then	O
in	O
the	O
proof	O
above	O
we	O
may	O
take	O
r	O
fixed	O
so	O
that	O
contains	O
the	O
support	B
of	O
i	O
then	O
the	O
estimate	O
of	O
theorem	O
is	O
e	O
ai	O
n	O
x	O
denotes	O
the	O
bound	O
on	O
i	O
problems	O
and	O
exercises	O
ii	O
flloo	O
we	O
take	O
m	O
for	O
a	O
constant	O
a	O
this	O
implies	O
by	O
corollary	O
that	O
e	O
inf	O
rjlc	O
for	O
coree	O
in	O
c	O
l	O
this	O
latter	O
inequality	B
was	O
proved	O
by	O
steele	O
to	O
see	O
that	O
it	O
cannot	O
be	O
extended	O
to	O
arbitrary	O
densities	O
observe	O
that	O
the	O
data	O
points	O
falling	O
on	O
the	O
convex	B
hull	I
upper	O
layer	O
of	O
the	O
points	O
x	O
i	O
x	O
n	O
can	O
always	O
be	O
shattered	O
by	O
convex	O
sets	O
monotone	O
layers	O
respectively	O
thus	O
nax	O
i	O
xn	O
is	O
at	O
least	O
where	O
mn	O
is	O
the	O
number	O
of	O
points	O
among	O
xl	O
x	O
n	O
falling	O
on	O
the	O
convex	B
hull	I
upper	O
layer	O
of	O
x	O
i	O
x	O
n	O
thus	O
but	O
it	O
follows	O
from	O
results	O
of	O
carnal	O
and	O
devroye	O
that	O
for	O
each	O
a	O
there	O
exists	O
a	O
density	O
such	O
that	O
emn	O
hmsup	O
n-oo	O
na	O
the	O
important	O
point	O
of	O
this	O
interlude	O
is	O
that	O
with	O
infinite	O
vc	B
dimension	B
we	O
may	O
under	O
some	O
circumstances	O
get	O
expected	O
error	O
rates	O
that	O
are	O
but	O
larger	O
than	O
however	O
the	O
bounds	O
are	O
sometimes	O
rather	O
loose	O
the	O
reason	O
is	O
the	O
looseness	O
of	O
the	O
vapnik	O
inequality	B
when	O
the	O
collections	O
a	O
become	O
very	O
big	O
to	O
get	O
such	O
results	O
for	O
classes	O
with	O
infinite	O
vc	B
dimension	B
it	O
is	O
necessary	O
to	O
impose	O
some	O
conditions	O
on	O
the	O
distribution	O
we	O
will	O
prove	O
this	O
in	O
chapter	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
the	O
inequality	B
of	O
theorem	O
is	O
tight	O
that	O
is	O
exhibit	O
a	O
class	O
a	O
of	O
sets	O
such	O
that	O
for	O
each	O
n	O
sea	O
n	O
g	O
problem	O
show	O
that	O
for	O
all	O
n	O
and	O
that	O
combinatorial	O
aspects	O
of	O
vapnik	O
theory	O
if	O
va	O
hint	O
there	O
are	O
several	O
ways	O
to	O
prove	O
the	O
statements	O
one	O
can	O
proceed	O
rectly	O
by	O
using	O
the	O
recurrence	O
l	O
lfo	O
el	O
lfo-l	O
e	O
a	O
simpler	O
way	O
to	O
prove	O
g	O
va	O
is	O
by	O
using	O
theorem	O
the	O
third	O
inequality	B
is	O
an	O
immediate	O
consequence	O
of	O
the	O
first	O
two	O
problem	O
give	O
an	O
alternative	O
proof	O
of	O
lemma	O
by	O
completing	O
the	O
following	O
probabilistic	O
argument	O
observe	O
that	O
for	O
k	O
n	O
k	O
where	O
bn	O
is	O
a	O
binomial	B
random	O
variable	B
with	O
parameters	O
nand	O
then	O
chernoffs	O
bounding	O
technique	O
the	O
proof	O
of	O
theorem	O
may	O
be	O
used	O
to	O
bound	O
this	O
probability	O
for	O
all	O
s	O
pbn	O
k	O
e-ske	O
sell	O
exp	O
s	O
take	O
the	O
derivative	O
of	O
the	O
exponent	O
with	O
respect	O
to	O
s	O
to	O
minimize	O
the	O
upper	O
bound	O
substitute	O
the	O
obtained	O
value	O
into	O
the	O
bound	O
to	O
get	O
the	O
desired	O
inequality	B
problem	O
let	O
b	O
be	O
a	O
binomial	B
random	O
variable	B
with	O
parameters	O
nand	O
p	O
prove	O
that	O
for	O
k	O
np	O
pb	O
k	O
exp	O
n	O
p	O
hint	O
use	O
chernoffs	O
bounding	O
technique	O
as	O
in	O
the	O
previous	O
problem	O
problem	O
prove	O
part	O
of	O
theorem	O
problem	O
prove	O
that	O
for	O
the	O
class	O
of	O
sets	O
defined	O
in	O
the	O
remark	O
following	O
theorem	O
sea	O
n	O
l	O
n	O
io	O
l	O
hint	O
proceed	O
by	O
induction	O
with	O
respect	O
to	O
nand	O
r	O
in	O
particular	O
show	O
that	O
the	O
recurrence	O
scar	O
n	O
scan	O
n	O
sar-	O
n	O
holds	O
where	O
ar	O
denotes	O
the	O
class	O
of	O
sets	O
defined	O
as	O
gx	O
o	O
where	O
g	O
runs	O
through	O
a	O
vector	O
space	O
spanned	O
by	O
the	O
first	O
r	O
of	O
the	O
sequence	O
of	O
functions	O
problem	O
let	O
a	O
and	O
b	O
be	O
two	O
families	O
of	O
subsets	O
of	O
nd	O
assume	O
that	O
for	O
some	O
r	O
sea	O
n	O
nrsb	O
n	O
for	O
all	O
n	O
show	O
that	O
if	O
vb	O
then	O
va	O
r	O
log	O
r	O
hint	O
by	O
theorem	O
sea	O
n	O
n	O
vsr-l	O
clearly	O
va	O
is	O
not	O
larger	O
than	O
any	O
k	O
for	O
which	O
kvsr-l	O
problem	O
determine	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
of	O
subsets	O
of	O
the	O
real	O
line	O
such	O
that	O
each	O
set	O
in	O
the	O
class	O
can	O
be	O
written	O
as	O
a	O
union	O
of	O
k	O
intervals	O
problems	O
and	O
exercises	O
problem	O
determine	O
the	O
vc	B
dimension	B
of	O
the	O
collection	O
of	O
all	O
polygons	O
with	O
k	O
vertices	O
in	O
the	O
plane	O
problem	O
what	O
is	O
the	O
vc	B
dimension	B
of	O
the	O
collection	O
of	O
all	O
ellipsoids	O
ofnd	O
problem	O
determine	O
the	O
vc	B
dimension	B
of	O
the	O
collection	O
of	O
all	O
subsets	O
of	O
kd	O
where	O
k	O
and	O
d	O
are	O
fixed	O
how	O
does	O
the	O
answer	O
change	O
if	O
we	O
restrict	O
the	O
subsets	O
to	O
those	O
of	O
cardinality	O
l	O
kd	O
problem	O
let	O
a	O
consist	O
of	O
all	O
simplices	O
ofnd	O
that	O
is	O
all	O
sets	O
of	O
the	O
form	O
where	O
xl	O
are	O
fixed	O
points	O
of	O
nd	O
determine	O
the	O
vc	B
dimension	B
of	O
a	O
problem	O
let	O
axi	O
be	O
the	O
set	O
of	O
all	O
x	O
en	O
that	O
are	O
of	O
the	O
form	O
where	O
xl	O
xk	O
are	O
fixed	O
numbers	O
and	O
are	O
fixed	O
functions	O
on	O
the	O
integers	O
let	O
a	O
xl	O
xk	O
en	O
determine	O
the	O
vc	B
dimension	B
of	O
a	O
problem	O
in	O
some	O
sense	O
vc	B
dimension	B
measures	O
the	O
of	O
a	O
class	O
of	O
sets	O
ever	O
it	O
has	O
little	O
to	O
do	O
with	O
cardinalities	O
as	O
this	O
exercise	O
demonstrates	O
exhibit	O
a	O
class	O
of	O
subsets	O
of	O
the	O
integers	O
with	O
uncountably	O
many	O
sets	O
yet	O
vc	B
dimension	B
property	O
was	O
pointed	O
out	O
to	O
us	O
by	O
andras	O
farago	O
note	O
on	O
the	O
other	O
hand	O
the	O
class	O
of	O
all	O
subsets	O
of	O
integers	O
cannot	O
be	O
written	O
as	O
a	O
countable	O
union	O
of	O
classes	O
with	O
finite	O
vc	B
dimension	B
rem	O
hint	O
find	O
a	O
class	O
of	O
subsets	O
of	O
the	O
reals	O
with	O
the	O
desired	O
properties	O
and	O
make	O
a	O
proper	O
correspondence	O
between	O
sets	O
of	O
integers	O
and	O
sets	O
in	O
the	O
class	O
problem	O
show	O
that	O
if	O
a	O
class	O
of	O
sets	O
a	O
is	O
linearly	O
ordered	B
by	O
inclusion	O
that	O
is	O
for	O
any	O
pair	O
of	O
sets	O
a	O
b	O
e	O
a	O
either	O
a	O
c	O
b	O
or	O
b	O
c	O
a	O
and	O
iai	O
then	O
va	O
conversely	O
assume	O
that	O
va	O
and	O
for	O
every	O
set	O
b	O
with	O
i	O
b	O
i	O
b	O
e	O
n	O
b	O
a	O
e	O
a	O
prove	O
that	O
then	O
a	O
is	O
linearly	O
ordered	B
by	O
inclusion	O
problem	O
we	O
say	O
that	O
four	O
sets	O
a	O
b	O
c	O
d	O
form	O
a	O
diamond	B
if	O
a	O
c	O
b	O
c	O
c	O
a	O
c	O
dec	O
but	O
b	O
ct	O
d	O
and	O
d	O
ct	O
c	O
let	O
a	O
be	O
a	O
class	O
of	O
sets	O
show	O
that	O
va	O
if	O
and	O
only	O
if	O
for	O
some	O
set	O
r	O
the	O
class	O
n	O
r	O
a	O
e	O
a	O
includes	O
a	O
diamond	B
problem	O
let	O
a	O
be	O
a	O
class	O
of	O
sets	O
and	O
define	O
its	O
density	O
by	O
da	O
inf	O
sup	O
sea	O
n	O
oo	O
nl	O
nr	O
verify	O
the	O
following	O
properties	O
das	O
va	O
for	O
each	O
positive	O
integer	O
k	O
there	O
exists	O
a	O
class	O
a	O
of	O
sets	O
such	O
that	O
va	O
k	O
yet	O
da	O
and	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	B
theory	O
da	O
if	O
and	O
only	O
if	O
va	O
problem	O
continued	O
let	O
a	O
and	O
ai	O
be	O
classes	O
of	O
sets	O
and	O
define	O
b	O
a	O
u	O
a	O
show	O
that	O
db	O
max	O
da	O
vbs	O
va	O
va	O
and	O
for	O
every	O
pair	O
of	O
positive	O
integers	O
k	O
m	O
there	O
exist	O
classes	O
a	O
and	O
ai	O
such	O
that	O
va	O
va	O
and	O
vb	O
problem	O
a	O
set	O
a	O
c	O
n	O
d	O
is	O
called	O
a	O
monotone	B
layer	I
if	O
x	O
e	O
a	O
implies	O
that	O
yea	O
for	O
all	O
y	O
x	O
each	O
component	O
of	O
x	O
is	O
not	O
larger	O
than	O
the	O
corresponding	O
component	O
of	O
y	O
show	O
that	O
the	O
class	O
of	O
all	O
monotone	O
layers	O
has	O
infinite	O
vc	B
dimension	B
problem	O
letx	O
lsuchthaty	O
ixea	O
where	O
a	O
is	O
a	O
convex	O
set	O
let	O
dn	O
xl	O
yd	O
yn	O
be	O
an	O
i	O
i	O
d	O
training	O
sequence	O
and	O
consider	O
the	O
classifier	B
if	O
x	O
is	O
in	O
the	O
convex	B
hull	I
of	O
the	O
xis	O
with	O
y	O
otherwise	O
find	O
a	O
distribution	O
for	O
which	O
gil	O
is	O
not	O
consistent	O
and	O
find	O
conditions	O
for	O
consistency	B
hint	O
recall	O
theorem	O
and	O
its	O
proof	O
lower	B
bounds	I
for	I
empirical	B
classifier	B
selection	I
in	O
chapter	O
a	O
classifier	B
was	O
selected	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
a	O
class	O
of	O
classifiers	O
c	O
with	O
the	O
help	O
of	O
the	O
vapnik	O
theory	O
we	O
have	O
been	O
able	O
to	O
obtain	O
distribution-free	O
performance	O
guarantees	O
for	O
the	O
selected	O
rule	B
for	O
example	O
it	O
was	O
shown	O
that	O
the	O
difference	O
between	O
the	O
expected	O
error	O
bility	O
of	O
the	O
selected	O
rule	B
and	O
the	O
best	O
error	O
probability	O
in	O
the	O
class	O
behaves	O
at	O
least	O
as	O
well	O
as	O
o	O
lognn	O
where	O
vc	O
is	O
the	O
vapnik-chervonenkis	B
dimension	B
of	O
c	O
and	O
n	O
is	O
the	O
size	O
of	O
the	O
training	O
data	O
dn	O
upper	O
bound	O
is	O
obtained	O
from	O
theorem	O
corollary	O
may	O
be	O
used	O
to	O
replace	O
the	O
log	O
n	O
term	O
with	O
log	O
vc	O
two	O
questions	O
arise	O
immediately	O
are	O
these	O
upper	O
bounds	O
least	O
up	O
to	O
the	O
order	O
of	O
magnitude	O
tight	O
is	O
there	O
a	O
much	O
better	O
way	O
of	O
selecting	O
a	O
classifier	B
than	O
mizing	O
the	O
empirical	B
error	I
this	O
chapter	O
attempts	O
to	O
answer	O
these	O
questions	O
as	O
it	O
turns	O
out	O
the	O
answer	O
is	O
essentially	O
affirmative	O
for	O
the	O
first	O
question	O
and	O
negative	O
for	O
the	O
second	O
these	O
questions	O
were	O
also	O
asked	O
in	O
the	O
learning	B
theory	O
setup	O
where	O
it	O
is	O
usually	O
assumed	O
that	O
the	O
error	O
probability	O
of	O
the	O
best	O
classifier	B
in	O
the	O
class	O
is	O
zero	O
blumer	O
ehrenfeucht	O
haussler	O
and	O
warmuth	O
haussler	O
littlestone	O
and	O
warmuth	O
and	O
ehrenfeucht	O
haussler	O
kearns	O
and	O
valiant	O
in	O
this	O
case	O
as	O
the	O
bound	O
of	O
theorem	O
implies	O
the	O
error	O
of	O
the	O
rule	B
selected	O
by	O
minimizing	O
the	O
empirical	B
error	I
is	O
within	O
log	O
n	O
n	O
of	O
that	O
of	O
the	O
best	O
in	O
the	O
class	O
equals	O
zero	O
by	O
assumption	O
we	O
will	O
see	O
that	O
essentially	O
there	O
is	O
no	O
way	O
to	O
beat	O
this	O
upper	O
bound	O
either	O
loer	O
bounds	O
for	O
empirical	B
classifier	B
selection	I
minimax	O
lower	O
bounds	O
let	O
us	O
formulate	O
exactly	O
what	O
we	O
are	O
interested	O
in	O
let	O
c	O
be	O
a	O
class	O
of	O
decision	O
functions	O
nd	O
i	O
the	O
training	O
sequence	O
dn	O
yi	O
yn	O
is	O
used	O
to	O
select	O
the	O
classifier	B
gnx	O
gnx	O
dn	O
from	O
c	O
where	O
the	O
selection	O
is	O
based	O
on	O
the	O
data	O
dn	O
we	O
emphasize	O
here	O
that	O
gn	O
can	O
be	O
an	O
arbitrary	O
function	O
of	O
the	O
data	O
we	O
do	O
not	O
restrict	O
our	O
attention	O
to	O
empirical	B
error	I
minimization	O
where	O
gn	O
is	O
a	O
classifier	B
in	O
c	O
that	O
minimizes	O
the	O
number	O
errors	O
committed	O
on	O
the	O
data	O
dn	O
as	O
before	O
we	O
measure	O
the	O
performance	O
of	O
the	O
selected	O
classifier	B
by	O
the	O
ference	O
between	O
the	O
error	O
probability	O
lgn	O
pgnx	O
i	O
yidn	O
of	O
the	O
selected	O
classifier	B
and	O
that	O
of	O
the	O
best	O
in	O
the	O
class	O
to	O
save	O
space	O
further	O
on	O
denote	O
this	O
optimum	O
by	O
ijec	O
in	O
particular	O
we	O
seek	O
lower	B
bounds	I
for	I
lc	O
df	O
inf	O
p	O
i	O
y	O
and	O
supplgn	O
lc	O
e	O
supelgn	O
lc	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
possible	O
distributions	O
of	O
the	O
pair	O
y	O
a	O
lower	O
bound	O
for	O
one	O
of	O
these	O
quantities	O
means	O
that	O
no	O
matter	O
what	O
our	O
method	O
of	O
picking	O
a	O
rule	B
from	O
c	O
is	O
we	O
may	O
face	O
a	O
distribution	O
such	O
that	O
our	O
method	O
performs	O
worse	O
than	O
the	O
bound	O
this	O
view	O
may	O
be	O
criticized	O
as	O
too	O
pessimistic	O
however	O
it	O
is	O
clearly	O
a	O
perfectly	O
meaningful	O
question	O
to	O
pursue	O
as	O
typically	O
we	O
have	O
no	O
other	O
information	O
available	O
than	O
the	O
training	O
data	O
so	O
we	O
have	O
to	O
be	O
prepared	O
for	O
the	O
worst	O
situation	O
actually	O
we	O
investigate	O
a	O
stronger	O
problem	O
in	O
that	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
distributions	O
with	O
lc	O
kept	O
at	O
a	O
fixed	O
value	O
between	O
zero	O
and	O
we	O
will	O
see	O
that	O
the	O
bounds	O
depend	O
on	O
n	O
vc	O
and	O
lc	O
jointly	O
as	O
it	O
turns	O
out	O
the	O
situations	O
for	O
lc	O
and	O
lc	O
are	O
quite	O
different	O
because	O
of	O
its	O
relative	O
simplicity	O
we	O
first	O
treat	O
the	O
case	O
lc	O
o	O
all	O
the	O
proofs	O
are	O
based	O
on	O
a	O
technique	O
called	O
probabilistic	B
method	I
the	O
basic	O
idea	O
here	O
is	O
that	O
the	O
existence	O
of	O
a	O
distribution	O
is	O
proved	O
by	O
considering	O
a	O
large	O
class	O
of	O
distributions	O
and	O
bounding	O
the	O
average	O
behavior	O
over	O
the	O
class	O
lower	O
bounds	O
on	O
the	O
probabilities	O
plgn	O
lc	O
e	O
may	O
be	O
translated	O
into	O
lower	O
bounds	O
on	O
the	O
sample	B
complexity	I
ne	O
we	O
obtain	O
lower	B
bounds	I
for	I
the	O
size	O
ofthe	O
training	O
sequence	O
such	O
that	O
for	O
any	O
classifier	B
plgn	O
lc	O
e	O
cannot	O
be	O
smaller	O
than	O
for	O
all	O
distributions	O
if	O
n	O
is	O
smaller	O
than	O
this	O
bound	O
the	O
case	O
lc	O
in	O
this	O
section	O
we	O
obtain	O
lower	O
bounds	O
under	O
the	O
assumption	O
that	O
the	O
best	O
classifier	B
in	O
the	O
class	O
has	O
zero	O
error	O
probability	O
in	O
view	O
of	O
theorem	O
we	O
see	O
that	O
the	O
the	O
case	O
lc	O
situation	O
here	O
is	O
different	O
from	O
when	O
lc	O
there	O
exist	O
methods	O
of	O
picking	O
a	O
classifier	B
from	O
c	O
minimization	O
of	O
the	O
empirical	B
error	I
such	O
that	O
the	O
error	O
probability	O
decreases	O
to	O
zero	O
at	O
a	O
rate	O
of	O
log	O
n	O
in	O
we	O
obtain	O
minimax	O
lower	O
bounds	O
close	O
to	O
the	O
upper	O
bounds	O
obtained	O
for	O
empirical	B
error	I
minimization	O
for	O
example	O
theorem	O
shows	O
that	O
if	O
lc	O
then	O
the	O
expected	O
error	O
probability	O
cannot	O
decrease	O
faster	O
than	O
a	O
sequence	O
proportional	O
to	O
vc	O
i	O
n	O
for	O
some	O
distributions	O
theorem	O
and	O
chervonenkis	O
haussler	O
littlestone	O
and	O
warmuth	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	O
mension	O
v	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
y	O
for	O
which	O
lc	O
then	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
yi	O
x	O
n	O
yn	O
and	O
n	O
v	O
sup	O
eln	O
v	O
n	O
proof	O
the	O
idea	O
is	O
to	O
construct	O
a	O
family	O
f	O
of	O
v-i	O
distributions	O
within	O
the	O
butions	O
with	O
lc	O
as	O
follows	O
first	O
find	O
points	O
x	O
i	O
xv	O
that	O
are	O
shattered	O
by	O
c	O
each	O
distribution	O
in	O
f	O
is	O
concentrated	O
on	O
the	O
set	O
of	O
these	O
points	O
a	O
member	O
in	O
f	O
is	O
described	O
by	O
v	O
bits	O
bi	O
bv	O
for	O
convenience	O
this	O
is	O
represented	O
as	O
a	O
bit	O
vector	O
b	O
assume	O
v	O
n	O
for	O
a	O
particular	O
bit	O
vector	O
we	O
let	O
x	O
xi	O
v	O
with	O
probability	O
lin	O
each	O
while	O
x	O
xv	O
with	O
probability	O
ln	O
then	O
set	O
y	O
fbx	O
where	O
fb	O
is	O
defined	O
as	O
follows	O
j	O
hx	O
if	O
x	O
xv	O
if	O
x	O
xi	O
i	O
v	O
note	O
that	O
since	O
y	O
is	O
a	O
function	O
of	O
x	O
we	O
must	O
have	O
l	O
also	O
lc	O
as	O
the	O
set	O
xv	O
is	O
shattered	O
by	O
c	O
i	O
e	O
there	O
is	O
age	O
c	O
with	O
gxi	O
fbxi	O
for	O
i	O
v	O
clearly	O
sup	O
el	O
n	O
lc	O
sup	O
el	O
n	O
lc	O
supeln	O
lc	O
b	O
el	O
n	O
lc	O
b	O
is	O
replaced	O
by	O
b	O
uniformly	O
distributed	O
over	O
v-i	O
el	O
n	O
pgnx	O
xl	O
yi	O
x	O
n	O
yn	O
fbx	O
the	O
last	O
probability	O
may	O
be	O
viewed	O
as	O
the	O
error	O
probability	O
of	O
the	O
decision	O
function	O
gn	O
n	O
d	O
x	O
x	O
ln	O
in	O
predicting	O
the	O
value	O
of	O
the	O
random	O
variable	B
fbx	O
based	O
on	O
the	O
observation	O
zn	O
xl	O
yi	O
x	O
n	O
yn	O
naturally	O
this	O
probability	O
is	O
bounded	O
from	O
below	O
by	O
the	O
bayes	O
probability	O
of	O
error	O
l	O
fbx	O
inf	O
pgnzn	O
fbx	O
gil	O
lm	O
yer	O
bounds	O
for	O
empirical	B
classifier	B
selection	I
corresponding	O
to	O
the	O
decision	O
problem	O
fbx	O
by	O
the	O
results	O
of	O
chapter	O
where	O
pfbx	O
lizn	O
observe	O
that	O
z	O
n	O
or	O
otherwise	O
if	O
x	O
xl	O
x	O
xn	O
x	O
xv	O
thus	O
we	O
see	O
that	O
sup	O
el	O
n	O
lc	O
l	O
fbx	O
p	O
xl	O
x	O
x	O
n	O
x	O
xv	O
v-i	O
pix	O
pix	O
il	O
v-i	O
v-i	O
n	O
lie	O
this	O
concludes	O
the	O
proof	O
minimax	O
lower	O
bounds	O
on	O
the	O
probability	O
pln	O
e	O
can	O
also	O
be	O
obtained	O
these	O
bounds	O
have	O
evolved	O
through	O
several	O
papers	O
see	O
ehrenfeucht	O
haussler	O
kearns	O
and	O
valiant	O
and	O
blumer	O
ehrenfeucht	O
haussler	O
and	O
warmuth	O
the	O
tightest	O
bounds	O
we	O
are	O
aware	O
of	O
thus	O
far	O
are	O
given	O
by	O
the	O
next	O
theorem	O
theorem	O
and	O
lugosi	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	B
dimension	B
v	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
yfor	O
which	O
lc	O
o	O
assume	O
e	O
assume	O
n	O
v-i	O
then	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
yi	O
x	O
n	O
yn	O
sup	O
pln	O
e	O
e-jrrv	O
v-i	O
if	O
on	O
the	O
other	O
hand	O
n	O
and	O
n	O
then	O
sup	O
pln	O
e	O
proof	O
we	O
randomize	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
the	O
difference	O
now	O
is	O
that	O
we	O
pick	O
xi	O
with	O
probability	O
peach	O
thuspx	O
pv	O
we	O
inherit	O
the	O
notation	O
from	O
the	O
proof	O
of	O
theorem	O
for	O
a	O
fixed	O
b	O
denote	O
the	O
error	O
probability	O
by	O
the	O
case	O
lc	O
we	O
now	O
randomize	O
and	O
replace	O
b	O
by	O
b	O
clearly	O
sup	O
pln	O
e	O
sup	O
plnb	O
e	O
b	O
e	O
eib	O
plnb	O
e	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
observe	O
that	O
lnb	O
cannot	O
be	O
smaller	O
than	O
the	O
bayes	O
risk	O
corresponding	O
to	O
the	O
decision	O
problem	O
ibex	O
where	O
thus	O
lnb	O
e	O
i	O
xn	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
we	O
see	O
that	O
e	O
i	O
xn	O
f	O
x	O
i	O
x	O
f	O
xn	O
x	O
f	O
xvix	O
i	O
xn	O
v-i	O
p	O
l	O
ii	O
for	O
fixed	O
xl	O
xn	O
we	O
denote	O
by	O
the	O
collection	O
s	O
j	O
s	O
v-i	O
x	O
j	O
this	O
is	O
the	O
collection	O
of	O
empty	O
cells	O
xi	O
we	O
summarize	O
we	O
consider	O
two	O
choices	O
for	O
p	O
choice	O
a	O
take	O
p	O
lin	O
and	O
assume	O
s	O
v-i	O
e	O
note	O
that	O
for	O
n	O
eill	O
pn	O
also	O
since	O
sill	O
s	O
v-i	O
we	O
have	O
var	O
iii	O
s	O
by	O
the	O
chebyshev-cantelli	B
inequality	B
plll	O
plll	O
plll	O
plll	O
eill	O
s	O
lowr	O
bounds	O
for	O
empirical	B
classifier	B
selection	I
var	O
iii	O
this	O
proves	O
the	O
second	O
inequality	B
for	O
supplil	O
e	O
choice	O
b	O
assume	O
that	O
e	O
by	O
the	O
pigeonhole	B
principle	I
iii	O
p	O
if	O
the	O
number	O
of	O
points	O
xi	O
i	O
n	O
that	O
are	O
not	O
equal	O
to	O
xv	O
does	O
not	O
exceed	O
v-i	O
p	O
therefore	O
we	O
have	O
a	O
further	O
lower	O
bound	O
plll	O
pbinomialn	O
v-i	O
define	O
v	O
rv	O
take	O
p	O
by	O
assumption	O
n	O
then	O
the	O
lower	O
bound	O
is	O
pbinomialn	O
v	O
v	O
v	O
v	O
ek	O
by	O
stirlings	O
formula	O
v	O
v	O
v	O
v	O
lv	O
n	O
v	O
x	O
x	O
v	O
n	O
this	O
concludes	O
the	O
proof	O
classes	O
with	O
infinite	O
vc	B
dimension	B
the	O
results	O
presented	O
in	O
the	O
previous	O
section	O
may	O
also	O
be	O
applied	O
to	O
classes	O
with	O
infinite	O
vc	B
dimension	B
for	O
example	O
it	O
is	O
not	O
hard	O
to	O
derive	O
the	O
following	O
result	O
from	O
theorem	O
theorem	O
assume	O
that	O
vc	O
for	O
every	O
n	O
and	O
classification	O
rule	B
gn	O
there	O
is	O
a	O
distribution	O
with	O
lc	O
such	O
that	O
the	O
case	O
lc	O
elgn	O
j	O
for	O
the	O
proof	O
see	O
problem	O
thus	O
when	O
vc	O
distribution-free	O
nontrivial	O
performance	O
guarantees	O
for	O
lg	O
n	O
lc	O
do	O
not	O
exist	O
this	O
generalizes	O
theorem	O
where	O
a	O
similar	O
result	O
is	O
shown	O
if	O
c	O
is	O
the	O
class	O
of	O
all	O
measurable	O
discrimination	O
functions	O
we	O
have	O
also	O
seen	O
in	O
theorem	O
that	O
if	O
c	O
is	O
the	O
class	O
of	O
all	O
measurable	O
classifiers	O
then	O
no	O
universal	O
rate	B
of	I
convergence	I
exists	O
however	O
we	O
will	O
see	O
in	O
chapter	O
that	O
for	O
some	O
classes	O
with	O
infinite	O
vc	B
dimension	B
it	O
is	O
possible	O
to	O
find	O
a	O
classification	O
rule	B
such	O
that	O
lgn	O
l	O
cjlog	O
nn	O
for	O
any	O
distribution	O
such	O
that	O
the	O
bayes	O
classifier	B
is	O
in	O
c	O
the	O
constant	O
c	O
however	O
necessarily	O
depends	O
on	O
the	O
distribution	O
as	O
is	O
apparent	O
from	O
theorem	O
infinite	O
vc	B
dimension	B
means	O
that	O
the	O
class	O
c	O
shatters	O
finite	O
sets	O
of	O
any	O
size	O
on	O
the	O
other	O
hand	O
if	O
c	O
shatters	O
infinitely	O
many	O
points	O
then	O
no	O
universal	O
rate	B
of	I
convergence	I
exists	O
this	O
may	O
be	O
seen	O
by	O
an	O
appropriate	O
modification	O
of	O
theorem	O
as	O
follows	O
see	O
problem	O
for	O
the	O
proof	O
theorem	O
let	O
be	O
a	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
al	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
with	O
the	O
property	O
that	O
there	O
exists	O
a	O
set	O
a	O
c	O
nd	O
of	O
infinite	O
cardinality	O
such	O
that	O
for	O
any	O
subset	O
b	O
of	O
a	O
there	O
exists	O
e	O
c	O
such	O
that	O
if	O
x	O
e	O
band	O
if	O
x	O
e	O
a-b	O
then	O
for	O
every	O
sequence	O
of	O
classification	O
rules	O
there	O
exists	O
a	O
distribution	O
of	O
y	O
with	O
le	O
such	O
that	O
for	O
all	O
n	O
note	O
that	O
the	O
basic	O
difference	O
between	O
this	O
result	O
and	O
all	O
others	O
in	O
this	O
chapter	O
is	O
that	O
in	O
theorem	O
the	O
distribution	O
does	O
not	O
vary	O
with	O
n	O
this	O
theorem	O
shows	O
that	O
selecting	O
a	O
classifier	B
from	O
a	O
class	O
shattering	B
infinitely	O
many	O
points	O
is	O
essentially	O
as	O
hard	O
as	O
selecting	O
one	O
from	O
the	O
class	O
of	O
all	O
classifiers	O
the	O
case	O
lc	O
in	O
the	O
more	O
general	O
case	O
when	O
the	O
best	O
decision	O
in	O
the	O
class	O
c	O
has	O
positive	O
error	O
probability	O
the	O
upper	O
bounds	O
derived	O
in	O
chapter	O
for	O
the	O
expected	O
error	O
bility	O
of	O
the	O
classifier	B
obtained	O
by	O
minimizing	O
the	O
empirical	B
risk	O
are	O
much	O
larger	O
than	O
when	O
le	O
o	O
in	O
this	O
section	O
we	O
show	O
that	O
these	O
upper	O
bounds	O
are	O
necessarily	O
large	O
and	O
they	O
may	O
be	O
tight	O
for	O
some	O
distributions	O
moreover	O
there	O
is	O
no	O
other	O
classifier	B
that	O
performs	O
significantly	O
better	O
than	O
empirical	B
error	I
minimization	O
theorem	O
below	O
gives	O
a	O
lower	O
bound	O
for	O
supexylc	O
fixed	O
elgn	O
le	O
as	O
a	O
function	O
of	O
nand	O
vc	O
the	O
bound	O
decreases	O
basically	O
as	O
in	O
the	O
upper	O
bound	O
tained	O
from	O
theorem	O
interestingly	O
the	O
lower	O
bound	O
becomes	O
smaller	O
as	O
le	O
lower	B
bounds	I
for	I
empirical	B
classifier	B
selection	I
decreases	O
as	O
should	O
be	O
expected	O
the	O
bound	O
is	O
largest	O
when	O
lc	O
is	O
close	O
to	O
the	O
constants	O
in	O
the	O
bound	O
may	O
be	O
tightened	O
at	O
the	O
expense	O
of	O
more	O
complicated	O
expressions	O
the	O
theorem	O
is	O
essentially	O
due	O
to	O
devroye	O
and	O
lugosi	O
though	O
the	O
proof	O
given	O
here	O
is	O
different	O
similar	O
bounds-without	O
making	O
the	O
dependence	O
on	O
lc	O
explicit-have	O
been	O
proved	O
by	O
vapnik	O
and	O
chervonenkis	O
and	O
simon	O
theorem	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	B
dimension	B
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
y	O
for	O
which	O
for	O
fixed	O
v	O
l	O
e	O
l	O
inf	O
pgx	O
i	O
y	O
gee	O
then	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
x	O
n	O
yn	O
sup	O
el	O
n	O
l	O
e-	O
ifn	O
il	O
proof	O
again	O
we	O
consider	O
the	O
finite	O
family	O
f	O
from	O
the	O
previous	O
section	O
the	O
notation	O
band	O
b	O
is	O
also	O
as	O
above	O
x	O
now	O
puts	O
mass	O
p	O
at	O
xi	O
i	O
v	O
and	O
mass	O
lp	B
which	O
will	O
be	O
satisfied	O
next	O
introduce	O
the	O
constant	O
c	O
e	O
we	O
no	O
longer	O
have	O
y	O
as	O
a	O
function	O
of	O
x	O
instead	O
we	O
have	O
a	O
uniform	O
random	O
variable	B
u	O
independent	O
of	O
x	O
and	O
define	O
lp	B
at	O
xv	O
this	O
imposes	O
the	O
condition	O
y	O
if	O
u	O
c	O
x	O
xi	O
i	O
v	O
otherwise	O
thus	O
when	O
x	O
xi	O
i	O
v	O
y	O
is	O
with	O
probability	O
cor	O
c	O
a	O
simple	O
argument	O
shows	O
that	O
the	O
best	O
rule	B
for	O
b	O
is	O
the	O
one	O
which	O
sets	O
f	O
if	O
x	O
i	O
v	O
bi	O
otherwise	O
b	O
also	O
observe	O
that	O
noting	O
that	O
we	O
may	O
write	O
l	O
c	O
c	O
for	O
i	O
v	O
for	O
fixed	O
b	O
by	O
the	O
equality	O
in	O
theorem	O
v-i	O
ln	O
l	O
l	O
fbxdl	O
il	O
it	O
is	O
sometimes	O
convenient	O
to	O
make	O
the	O
dependence	O
of	O
gn	O
upon	O
b	O
explicit	O
by	O
sidering	O
gnxi	O
as	O
a	O
function	O
of	O
xi	O
xl	O
x	O
n	O
u	O
un	O
i	O
i	O
d	O
sequence	O
of	O
uniform	O
random	O
variables	O
and	O
bi	O
the	O
proof	O
given	O
here	O
is	O
based	O
on	O
the	O
case	O
lc	O
the	O
ideas	O
used	O
in	O
the	O
proofs	O
of	O
theorems	O
and	O
we	O
replace	O
b	O
by	O
a	O
formly	O
distributed	O
random	O
b	O
over	O
i	O
v-i	O
after	O
this	O
randomization	O
denote	O
zn	O
xl	O
yi	O
x	O
n	O
yn	O
thus	O
sup	O
eln	O
l	O
supeln	O
l	O
b	O
v-i	O
ii	O
random	O
b	O
eln	O
l	O
l	O
j	O
ibex	O
ibex	O
ibex	O
where	O
as	O
before	O
l	O
ibex	O
denotes	O
the	O
bayes	O
probability	O
of	O
error	O
of	O
dicting	O
the	O
value	O
of	O
ibex	O
based	O
on	O
observing	O
zn	O
all	O
we	O
have	O
to	O
do	O
is	O
to	O
find	O
a	O
suitable	O
lower	O
bound	O
for	O
where	O
pibx	O
lizn	O
observe	O
that	O
next	O
we	O
compute	O
pbi	O
llyij	O
yi	O
yik	O
yk	O
for	O
yi	O
yk	O
e	O
i	O
denoting	O
the	O
numbers	O
of	O
zeros	O
and	O
ones	O
by	O
ko	O
i	O
s	O
k	O
y	O
j	O
o	O
i	O
and	O
k	O
i	O
s	O
k	O
yj	O
we	O
see	O
that	O
pbi	O
llyij	O
yi	O
yik	O
yd	O
therefore	O
if	O
x	O
xij	O
x	O
ik	O
xi	O
i	O
v	O
then	O
min	O
min	O
kj	O
lower	B
bounds	I
for	I
empirical	B
classifier	B
selection	I
in	O
summary	O
denoting	O
a	O
we	O
have	O
l	O
ibex	O
e	O
e	O
i	O
px	O
xde	O
il	O
jensens	O
inequality	B
lpa	O
next	O
we	O
bound	O
e	O
ljxjxi	O
clearly	O
if	O
bk	O
q	O
denotes	O
a	O
binomial	B
random	O
variable	B
with	O
parameters	O
k	O
and	O
q	O
e	O
gpkl-	O
pn-ke	O
c	O
klj	O
however	O
by	O
straightforward	O
calculation	O
we	O
see	O
that	O
c	O
kl	O
je	O
c	O
jkl	O
therefore	O
applying	O
jensens	O
inequality	B
once	O
again	O
we	O
get	O
t	O
pt-ke	O
c	O
kl	O
ko	O
k	O
summarizing	O
what	O
we	O
have	O
obtained	O
so	O
far	O
we	O
have	O
supeln	O
l	O
b	O
ibex	O
p	O
p	O
cv	O
the	O
inequality	B
x	O
ex	O
cv	O
a	O
rough	O
asymptotic	O
analysis	O
shows	O
that	O
the	O
best	O
asymptotic	O
choice	O
for	O
c	O
is	O
given	O
by	O
c	O
the	O
case	O
lc	O
then	O
the	O
constraint	O
l	O
p	O
c	O
leaves	O
us	O
with	O
a	O
quadratic	O
equation	O
in	O
c	O
instead	O
of	O
solving	O
this	O
equation	O
it	O
is	O
more	O
convenient	O
to	O
take	O
c	O
j	O
with	O
this	O
choiceforc	O
using	O
l	O
straightforward	O
calculation	O
provides	O
sup	O
el	O
n	O
l	O
jv	O
ll	O
the	O
condition	O
p	O
v-i	O
implies	O
that	O
we	O
need	O
to	O
ask	O
that	O
n	O
this	O
concludes	O
the	O
proof	O
of	O
theorem	O
d	O
next	O
we	O
obtain	O
a	O
probabilistic	O
bound	O
its	O
proof	O
below	O
is	O
based	O
upon	O
hellinger	O
distances	O
and	O
its	O
methodology	O
is	O
essentially	O
due	O
to	O
assouad	O
for	O
ments	O
and	O
applications	O
we	O
refer	O
to	O
birge	O
and	O
devroye	O
theorem	O
and	O
lugosi	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	B
dimension	B
v	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
y	O
for	O
which	O
for	O
fixed	O
l	O
e	O
l	O
inf	O
pgx	O
y	O
gee	O
then	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
x	O
n	O
yn	O
and	O
any	O
e	O
l	O
sup	O
pln	O
proof	O
the	O
method	O
of	O
randomization	O
here	O
is	O
similar	O
to	O
that	O
in	O
the	O
proof	O
of	O
theorem	O
using	O
the	O
same	O
notation	O
as	O
there	O
it	O
is	O
clear	O
that	O
sup	O
pln	O
l	O
e	O
ei	O
i	O
fbxie	O
exlxvxoi	O
first	O
observe	O
that	O
if	O
then	O
lower	B
bounds	I
for	I
empirical	B
classifier	B
selection	I
where	O
be	O
denotes	O
the	O
binary	B
vector	O
h	O
bv	O
that	O
is	O
the	O
complement	O
of	O
b	O
therefore	O
for	O
e	O
pcv	O
the	O
last	O
expression	B
in	O
the	O
lower	O
bound	O
above	O
is	O
bounded	O
from	O
below	O
by	O
lecams	O
inequality	B
lemma	O
l	O
l	O
j	O
pbx	O
ypbcx	O
y	O
b	O
it	O
is	O
easy	O
to	O
see	O
that	O
for	O
x	O
xv	O
pbxypbcxy	O
and	O
for	O
x	O
xi	O
i	O
v	O
pbx	O
ypbcx	O
y	O
p	O
c	O
thus	O
we	O
have	O
the	O
equality	O
summarizing	O
since	O
l	O
pv	O
c	O
we	O
have	O
sup	O
pln	O
l	O
e	O
c	O
exp	O
where	O
we	O
used	O
the	O
inequality	B
x	O
e-xl-x	O
again	O
we	O
may	O
choose	O
c	O
as	O
it	O
is	O
easy	O
to	O
verify	O
that	O
condition	O
holds	O
also	O
pv	O
from	O
the	O
condition	O
l	O
e	O
we	O
deduce	O
that	O
c	O
the	O
exponent	O
in	O
the	O
expression	B
above	O
may	O
be	O
bounded	O
as	O
sample	B
complexity	I
substituting	O
c	O
e	O
thus	O
as	O
desired	O
sup	O
pln	O
l	O
e	O
exp	O
l	O
sample	B
complexity	I
we	O
may	O
rephrase	O
the	O
probability	O
bounds	O
above	O
in	O
terms	O
of	O
the	O
sample	B
complexity	I
of	O
algorithms	O
for	O
selecting	O
a	O
classifier	B
from	O
a	O
class	O
recall	O
that	O
for	O
given	O
e	O
the	O
sample	B
complexity	I
of	O
a	O
selection	O
rule	B
gn	O
is	O
the	O
smallest	O
integer	O
ne	O
such	O
that	O
sup	O
plgn	O
lc	O
e	O
for	O
all	O
n	O
ne	O
the	O
supremum	O
is	O
taken	O
over	O
a	O
class	O
of	O
distributions	O
of	O
y	O
here	O
we	O
are	O
interested	O
in	O
distributions	O
such	O
that	O
lc	O
is	O
fixed	O
we	O
start	O
with	O
the	O
case	O
lc	O
by	O
checking	O
the	O
implications	O
of	O
theorem	O
for	O
the	O
sample	B
complexity	I
n	O
first	O
blumer	O
ehrenfeucht	O
haussler	O
and	O
warmuth	O
showed	O
that	O
for	O
any	O
algorithm	B
neocgioggvc	O
where	O
c	O
is	O
a	O
universal	O
constant	O
in	O
ehrenfeucht	O
haussler	O
kearns	O
and	O
valiant	O
the	O
lower	O
bound	O
was	O
partially	O
improved	O
to	O
vc	O
and	O
it	O
may	O
be	O
combined	O
with	O
the	O
previous	O
bound	O
when	O
e	O
theorem	O
provides	O
the	O
following	O
bounds	O
corollary	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	B
dimension	B
v	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
yfor	O
which	O
lc	O
assume	O
lower	B
bounds	I
for	I
empirical	B
classifier	B
selection	I
e	O
and	O
denote	O
v	O
rev	O
thenfor	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
f	O
l	O
x	O
n	O
fn	O
when	O
e	O
and	O
then	O
n	O
log	O
g	O
finallyjor	O
and	O
e	O
v-i	O
ne	O
proof	O
the	O
second	O
bound	O
follows	O
trivially	O
from	O
the	O
second	O
inequality	B
of	O
theorem	O
by	O
the	O
first	O
inequality	B
there	O
sup	O
pln	O
e	O
v-i	O
v	O
v	O
n	O
e	O
ls	O
we	O
assume	O
log	O
g	O
v	O
yv	O
the	O
function	O
nv	O
e-	O
sile	O
varies	O
unimodally	O
in	O
n	O
and	O
achieves	O
a	O
peak	O
at	O
n	O
v	O
ise	O
for	O
n	O
below	O
this	O
threshold	B
by	O
monotonicity	O
we	O
apply	O
the	O
bound	O
at	O
n	O
vise	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
value	O
of	O
the	O
bound	O
at	O
v	O
is	O
always	O
at	O
least	O
if	O
on	O
the	O
other	O
hand	O
n	O
v	O
the	O
lower	O
bound	O
achieves	O
its	O
minimal	O
value	O
at	O
ise	O
and	O
the	O
value	O
there	O
is	O
this	O
proves	O
the	O
first	O
bound	O
corollary	O
shows	O
that	O
for	O
any	O
classifier	B
at	O
least	O
vc	O
max	O
se	O
log	O
training	O
samples	O
are	O
necessary	O
to	O
achieve	O
e	O
accuracy	B
with	O
confidence	B
for	O
all	O
distributions	O
apart	O
from	O
a	O
log	O
factor	O
the	O
order	O
of	O
magnitude	O
of	O
this	O
expression	B
is	O
the	O
same	O
as	O
that	O
of	O
the	O
upper	O
bound	O
for	O
empirical	B
error	I
minimization	O
obtained	O
in	O
corollary	O
that	O
the	O
upper	O
and	O
lower	O
bounds	O
are	O
very	O
close	O
has	O
two	O
important	O
messages	O
on	O
the	O
one	O
hand	O
it	O
gives	O
a	O
very	O
good	O
estimate	O
for	O
the	O
number	O
of	O
training	O
samples	O
needed	O
to	O
achieve	O
a	O
certain	O
performance	O
on	O
the	O
other	O
hand	O
it	O
shows	O
that	O
there	O
is	O
essentially	O
no	O
better	O
method	O
than	O
minimizing	O
the	O
empirical	B
error	I
probability	O
in	O
the	O
case	O
lc	O
we	O
may	O
derive	O
lower	B
bounds	I
for	I
ne	O
from	O
theorems	O
and	O
problems	O
and	O
exercises	O
corollary	O
let	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	B
dimension	B
v	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
y	O
for	O
which	O
for	O
fixed	O
l	O
e	O
l	O
infpgxly	O
gee	O
then	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
x	O
n	O
yn	O
lv	O
le-	O
lo	O
also	O
and	O
in	O
particular	O
for	O
e	O
l	O
xmin	O
l	O
ne	O
log	O
proof	O
the	O
first	O
bound	O
may	O
be	O
obtained	O
easily	O
from	O
the	O
expectation-bound	O
of	O
theorem	O
problem	O
setting	O
the	O
bound	O
of	O
theorem	O
equal	O
to	O
provides	O
the	O
second	O
bound	O
on	O
n	O
these	O
bounds	O
may	O
of	O
course	O
be	O
combined	O
they	O
show	O
that	O
n	O
is	O
bounded	O
from	O
below	O
by	O
terms	O
like	O
of	O
vc	O
and	O
as	O
i	O
is	O
typically	O
much	O
smaller	O
than	O
e	O
by	O
comparing	O
these	O
bounds	O
with	O
the	O
upper	O
bounds	O
of	O
corollary	O
we	O
see	O
that	O
the	O
only	O
difference	O
between	O
the	O
orders	O
of	O
magnitude	O
is	O
a	O
log	O
ie	O
so	O
all	O
remarks	O
made	O
for	O
the	O
case	O
le	O
remain	O
valid	O
interestingly	O
all	O
bounds	O
depend	O
on	O
the	O
class	O
c	O
only	O
through	O
its	O
vc	B
dimension	B
this	O
fact	O
suggests	O
that	O
when	O
studying	O
distribution-free	O
properties	O
of	O
lgn	O
le	O
the	O
vc	B
dimension	B
is	O
the	O
most	O
important	O
characteristic	O
of	O
the	O
class	O
also	O
all	O
bounds	O
are	O
linear	O
in	O
the	O
vc	B
dimension	B
which	O
links	O
it	O
conveniently	O
to	O
sample	O
size	O
remark	O
it	O
is	O
easy	O
to	O
see	O
from	O
the	O
proofs	O
that	O
all	O
results	O
remain	O
valid	O
if	O
we	O
allow	O
randomization	O
in	O
the	O
rules	O
gn	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
theorem	O
implies	O
that	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
d	O
lv	O
le-	O
io	O
ne	O
x	O
mill	O
hint	O
assume	O
that	O
pln	O
l	O
e	O
then	O
clearly	O
eln	O
l	O
e	O
problem	O
prove	O
theorem	O
first	O
apply	O
the	O
proof	O
method	O
of	O
theorem	O
to	O
show	O
that	O
for	O
every	O
n	O
andg	O
there	O
is	O
a	O
distribution	O
with	O
lc	O
such	O
thatelgn	O
use	O
a	O
monotonicity	O
argument	O
to	O
finish	O
the	O
proof	O
problem	O
prove	O
theorem	O
by	O
modifying	O
the	O
proof	O
of	O
theorem	O
the	O
maximum	B
likelihood	I
principle	O
in	O
this	O
chapter	O
we	O
explore	O
the	O
various	O
uses	O
of	O
the	O
maximum	B
likelihood	I
principle	O
in	O
discrimination	O
in	O
general	O
the	O
principle	O
is	O
only	O
applicable	O
if	O
we	O
have	O
some	O
a	O
priori	O
knowledge	O
of	O
the	O
problem	O
at	O
hand	O
we	O
offer	O
definitions	O
consistency	B
results	O
and	O
examples	O
that	O
highlight	O
the	O
advantages	O
and	O
shortcomings	O
maximum	B
likelihood	I
the	O
formats	O
sometimes	O
advance	O
information	O
takes	O
a	O
very	O
specific	O
form	O
y	O
x	O
is	O
normal	B
a	O
often	O
it	O
is	O
rather	O
vague	O
believe	O
that	O
x	O
has	O
a	O
density	O
or	O
is	O
thought	O
to	O
be	O
a	O
monotone	O
function	O
of	O
x	O
e	O
r	O
if	O
we	O
have	O
information	O
in	O
set	B
format	I
the	O
maximum	B
likelihood	I
principle	O
is	O
less	O
appropriate	O
here	O
we	O
know	O
that	O
the	O
bayes	O
rule	B
gx	O
is	O
of	O
the	O
form	O
gx	O
ixea	O
where	O
a	O
e	O
a	O
and	O
a	O
is	O
a	O
class	O
of	O
sets	O
of	O
rd	O
we	O
refer	O
to	O
the	O
chapters	O
on	O
empirical	B
risk	I
minimization	I
chapter	O
and	O
also	O
chapter	O
for	O
this	O
situation	O
if	O
we	O
know	O
that	O
the	O
true	O
belongs	O
to	O
a	O
class	O
of	O
functions	O
that	O
map	O
n	O
d	O
to	O
then	O
we	O
say	O
that	O
we	O
are	O
given	O
information	O
in	O
regressionformat	O
with	O
each	O
e	O
we	O
associate	O
a	O
set	O
a	O
and	O
a	O
discrimination	O
rule	B
gx	O
ixea	O
the	O
class	O
of	O
these	O
rules	O
is	O
denoted	O
by	O
c	O
assume	O
that	O
we	O
somehow	O
could	O
estimate	O
by	O
then	O
it	O
makes	O
sense	O
to	O
use	O
the	O
associated	O
rule	B
gnx	O
the	O
maximum	B
likelihood	I
method	O
suggests	O
a	O
way	O
of	O
picking	O
the	O
from	O
that	O
in	O
some	O
sense	O
is	O
most	O
likely	O
given	O
the	O
data	O
it	O
is	O
fully	O
automatic-the	O
user	O
does	O
not	O
have	O
to	O
pick	O
any	O
parameters-but	O
it	O
does	O
require	O
a	O
serious	O
implementation	O
effort	O
in	O
many	O
cases	O
in	O
a	O
sense	O
the	O
regression	B
format	I
is	O
more	O
powerful	O
than	O
the	O
the	O
maximum	B
likelihood	I
principle	O
set	B
format	I
as	O
there	O
is	O
more	O
information	O
in	O
knowing	O
a	O
function	O
than	O
in	O
knowing	O
the	O
indicator	O
function	O
still	O
no	O
structure	O
is	O
assumed	O
on	O
the	O
part	O
of	O
x	O
and	O
none	O
is	O
needed	O
to	O
obtain	O
consistency	B
results	O
a	O
third	O
format	O
even	O
more	O
detailed	O
is	O
that	O
in	O
which	O
we	O
know	O
that	O
the	O
distribution	O
of	O
y	O
belongs	O
to	O
a	O
class	O
v	O
of	O
distributions	O
on	O
nd	O
x	O
i	O
for	O
a	O
given	O
tion	O
we	O
know	O
so	O
we	O
may	O
once	O
again	O
deduce	O
a	O
rule	B
g	O
by	O
setting	O
gx	O
this	O
distributionformat	O
is	O
even	O
more	O
powerful	O
as	O
the	O
positions	O
x	O
i	O
xn	O
alone	O
in	O
some	O
cases	O
may	O
determine	O
the	O
unknown	O
parameters	O
in	O
the	O
model	O
this	O
tion	O
fits	O
in	O
squarely	O
with	O
classical	O
parameter	B
estimation	B
in	O
mathematical	O
statistics	O
once	O
again	O
we	O
may	O
apply	O
the	O
maximum	B
likelihood	I
principle	O
to	O
select	O
a	O
tion	O
from	O
v	O
unfortunately	O
as	O
we	O
move	O
to	O
more	O
restrictive	O
and	O
stronger	O
formats	O
the	O
number	O
of	O
conditions	O
under	O
which	O
the	O
maximum	B
likelihood	I
principle	O
is	O
tent	O
increases	O
as	O
well	O
we	O
will	O
only	O
superficially	O
deal	O
with	O
the	O
distribution	B
format	I
chapter	O
for	O
more	O
detail	O
the	O
maximum	B
likelihood	I
method	O
regression	B
format	I
given	O
xl	O
x	O
n	O
the	O
probability	O
of	O
observing	O
yi	O
yn	O
yn	O
is	O
n	O
l-	O
yi	O
il	O
if	O
is	O
unknown	O
but	O
belongs	O
to	O
a	O
family	O
f	O
of	O
functions	O
we	O
may	O
wish	O
to	O
select	O
that	O
from	O
f	O
it	O
exists	O
for	O
which	O
that	O
likelihood	B
product	B
is	O
maximal	O
more	O
formally	O
we	O
select	O
so	O
that	O
the	O
logarithm	O
is	O
maximal	O
if	O
the	O
family	O
f	O
is	O
too	O
rich	O
this	O
will	O
overfit	O
and	O
consequently	O
the	O
selected	O
function	O
has	O
a	O
probability	O
of	O
error	O
that	O
does	O
not	O
tend	O
to	O
l	O
for	O
convenience	O
we	O
assume	O
here	O
that	O
there	O
exists	O
an	O
element	O
of	O
f	O
maximizing	O
we	O
do	O
not	O
assume	O
here	O
that	O
the	O
class	O
f	O
is	O
very	O
small	O
classes	O
in	O
which	O
each	O
in	O
f	O
is	O
known	O
up	O
to	O
one	O
or	O
a	O
few	O
parameters	O
are	O
loosely	O
called	O
parametric	O
sometimes	O
f	O
is	O
defined	O
via	O
a	O
generic	O
description	O
such	O
as	O
f	O
is	O
the	O
class	O
of	O
all	O
nd	O
that	O
are	O
lipschitz	O
with	O
constant	O
c	O
such	O
classes	O
are	O
called	O
nonparametric	O
in	O
certain	O
cases	O
the	O
boundary	O
between	O
parametric	O
and	O
nonparametric	O
is	O
unclear	O
we	O
will	O
be	O
occupied	O
with	O
the	O
consistency	B
question	O
does	O
with	O
probability	O
one	O
for	O
all	O
distribution	O
of	O
y	O
p	O
i	O
y	O
is	O
the	O
maximum	B
likelihood	I
method	O
regression	B
format	I
the	O
probability	O
of	O
error	O
of	O
the	O
natural	O
rule	B
that	O
corresponds	O
to	O
rj	O
if	O
additionally	O
f	O
is	O
rich	O
enough	O
or	O
our	O
prior	O
information	O
is	O
good	O
enough	O
we	O
may	O
have	O
inf	O
ef	O
l	O
l	O
but	O
that	O
is	O
not	O
our	O
concern	O
here	O
as	O
f	O
is	O
given	O
to	O
us	O
we	O
will	O
not	O
be	O
concerned	O
with	O
the	O
computational	O
problems	O
related	O
to	O
the	O
mization	O
of	O
lnrj	O
over	O
f	O
gradient	O
methods	O
or	O
variations	O
of	O
them	O
are	O
sometimes	O
used-refer	O
to	O
mclachlan	O
for	O
a	O
bibliography	O
it	O
suffices	O
to	O
say	O
that	O
in	O
simple	O
cases	O
an	O
explicit	O
form	O
for	O
rjn	O
may	O
be	O
available	O
an	O
example	O
follows	O
our	O
first	O
lemma	O
illustrates	O
that	O
the	O
maximum	B
likelihood	I
method	O
should	O
only	O
be	O
used	O
when	O
the	O
true	O
unknown	O
rj	O
indeed	O
belongs	O
to	O
f	O
recall	O
that	O
the	O
same	O
was	O
not	O
true	O
for	O
empirical	B
risk	I
minimization	I
over	O
vc	O
classes	O
chapter	O
lemma	O
consider	O
the	O
class	O
f	O
with	O
two	O
functions	O
rja	O
rjb	O
let	O
rjn	O
be	O
the	O
function	O
picked	O
by	O
the	O
maximum	B
likelihood	I
method	O
there	O
exists	O
a	O
distribution	O
for	O
y	O
with	O
rj	O
tj	O
f	O
such	O
that	O
with	O
probability	O
one	O
as	O
n	O
lrjn	O
max	O
lrj	O
min	O
lrj	O
thus	O
maximum	B
likelihood	I
picks	O
the	O
wrong	O
classifier	B
proof	O
define	O
the	O
distribution	O
of	O
y	O
on	O
x	O
by	O
px	O
y	O
o	O
p	O
y	O
o	O
p	O
y	O
and	O
p	O
y	O
then	O
one	O
may	O
quickly	O
verify	O
that	O
that	O
l	O
but	O
this	O
is	O
irrelevant	O
here	O
within	O
f	O
rjb	O
is	O
the	O
better	O
for	O
our	O
distribution	O
by	O
the	O
strong	O
law	O
of	O
large	O
numbers	O
we	O
have	O
with	O
probability	O
one	O
similarly	O
for	O
rjb	O
if	O
one	O
works	O
out	O
the	O
values	O
it	O
is	O
seen	O
that	O
with	O
probability	O
one	O
rjn	O
rja	O
for	O
all	O
n	O
large	O
enough	O
hence	O
lrjn	O
lrj	O
with	O
probability	O
one	O
remark	O
besides	O
the	O
clear	O
theoretical	B
hazard	O
of	O
not	O
capturing	O
rj	O
in	O
f	O
the	O
maximum	B
likelihood	I
method	O
runs	O
into	O
a	O
practical	O
problem	O
with	O
for	O
example	O
take	O
rjb	O
and	O
assume	O
rj	O
for	O
all	O
n	O
large	O
enough	O
both	O
classes	O
are	O
represented	O
in	O
the	O
data	O
sample	O
with	O
probability	O
one	O
this	O
implies	O
that	O
lnrja	O
lnrjb	O
the	O
maximum	B
likelihood	I
estimate	O
rjn	O
is	O
ill-defined	O
while	O
any	O
reasonable	O
rule	B
should	O
quickly	O
be	O
able	O
to	O
pick	O
rja	O
over	O
rjb	O
the	O
lemma	O
shows	O
that	O
when	O
rj	O
tj	O
f	O
the	O
maximum	B
likelihood	I
method	O
is	O
not	O
even	O
capable	O
of	O
selecting	O
one	O
of	O
two	O
choices	O
the	O
situation	O
changes	O
dramatically	O
when	O
rj	O
e	O
f	O
for	O
finite	O
classes	O
f	O
nothing	O
can	O
go	O
wrong	O
noting	O
that	O
whenever	O
rj	O
e	O
f	O
we	O
have	O
l	O
lrj	O
we	O
may	O
now	O
expect	O
that	O
lrjn	O
l	O
in	O
probability	O
or	O
with	O
probability	O
one	O
the	O
maximum	B
likelihood	I
principle	O
theorem	O
lflfi	O
k	O
and	O
e	O
f	O
then	O
the	O
maximum	B
likelihood	I
method	O
is	O
consistent	O
that	O
is	O
l	O
in	O
probability	O
proof	O
for	O
a	O
fixed	O
distribution	O
of	O
y	O
we	O
rank	O
the	O
members	O
of	O
f	O
by	O
increasing	O
values	O
of	O
put	O
let	O
io	O
be	O
the	O
largest	O
index	O
for	O
which	O
l	O
let	O
be	O
the	O
maximum	B
likelihood	I
choice	O
from	O
f	O
for	O
any	O
a	O
we	O
have	O
l	O
p	O
rp	O
x	O
a	O
a	O
iio	O
define	O
the	O
entropy	B
of	O
y	O
by	O
log	O
chapter	O
recall	O
that	O
log	O
we	O
also	O
need	O
the	O
negative	O
divergences	O
di	O
e	O
log	O
log	O
i	O
which	O
are	O
easily	O
seen	O
to	O
be	O
nonpositive	O
for	O
all	O
i	O
jensens	O
inequality	B
more	O
di	O
if	O
and	O
only	O
if	O
with	O
probability	O
one	O
observe	O
that	O
for	O
i	O
io	O
we	O
cannot	O
have	O
this	O
let	O
e	O
maxiio	O
di	O
io	O
k	O
this	O
set	O
is	O
empty	O
but	O
then	O
the	O
theorem	O
is	O
trivially	O
true	O
it	O
is	O
advantageous	O
to	O
take	O
a	O
e	O
observe	O
that	O
e	O
di	O
c	O
e	O
f	O
if	O
i	O
lo	O
thus	O
plryn	O
i	O
l	O
p	O
l	O
p	O
n	O
by	O
the	O
law	O
of	O
large	O
numbers	O
we	O
see	O
that	O
both	O
terms	O
converge	O
to	O
zero	O
note	O
in	O
particular	O
that	O
it	O
is	O
true	O
even	O
if	O
for	O
some	O
i	O
io	O
di	O
d	O
for	O
infinite	O
classes	O
many	O
things	O
can	O
go	O
wrong	O
assume	O
that	O
f	O
is	O
the	O
class	O
of	O
all	O
with	O
fa	O
and	O
a	O
is	O
a	O
convex	O
set	O
containing	O
the	O
origin	O
pick	O
x	O
uniformly	O
on	O
the	O
perimeter	O
of	O
the	O
unit	O
circle	O
then	O
the	O
maximum	B
likelihood	I
estimate	O
matches	O
the	O
data	O
as	O
we	O
may	O
always	O
find	O
a	O
closed	O
polygon	O
pn	O
with	O
consistency	B
vertices	O
at	O
the	O
xis	O
with	O
yi	O
for	O
rn	O
lpn	O
we	O
have	O
lnrn	O
maximal	O
value	O
yet	O
lrn	O
py	O
i	O
p	O
and	O
l	O
the	O
class	O
f	O
is	O
plainly	O
too	O
rich	O
for	O
distributions	O
of	O
x	O
on	O
the	O
positive	O
integers	O
maximum	B
likelihood	I
does	O
not	O
behave	O
as	O
poorly	O
even	O
though	O
it	O
must	O
pick	O
among	O
infinitely	O
many	O
possibilities	O
assume	O
f	O
is	O
the	O
class	O
of	O
all	O
r	O
but	O
we	O
know	O
that	O
x	O
puts	O
all	O
its	O
mass	O
on	O
the	O
positive	O
integers	O
then	O
maximum	B
likelihood	I
tries	O
to	O
maximize	O
n	O
il	O
r	O
over	O
r	O
e	O
f	O
where	O
nli	O
ll	O
ixjiyjl	O
and	O
noi	O
ll	O
ixjiyjo	O
the	O
maximization	O
is	O
to	O
be	O
done	O
over	O
all	O
from	O
fortunately	O
thus	O
if	O
noi	O
we	O
set	O
rni	O
while	O
if	O
noi	O
nli	O
we	O
this	O
is	O
turned	O
into	O
a	O
maximization	O
for	O
each	O
individual	O
i	O
we	O
are	O
not	O
so	O
lucky	O
pick	O
rni	O
as	O
the	O
value	O
u	O
that	O
maximizes	O
nli	O
log	O
u	O
noi	O
u	O
setting	O
the	O
derivative	O
with	O
respect	O
to	O
u	O
equal	O
to	O
zero	O
shows	O
that	O
nli	O
rn	O
l	O
nli	O
noi	O
in	O
other	O
words	O
rn	O
is	O
the	O
familiar	O
histogram	O
estimate	O
with	O
bin	O
width	O
less	O
than	O
one	O
half	O
it	O
is	O
known	O
to	O
be	O
universally	O
consistent	O
theorem	O
thus	O
maximum	B
likelihood	I
may	O
work	O
for	O
large	O
f	O
if	O
we	O
restrict	O
the	O
distribution	O
of	O
y	O
a	O
bit	O
consistency	B
finally	O
we	O
are	O
ready	O
for	O
the	O
main	O
consistency	B
result	O
for	O
rn	O
when	O
f	O
may	O
have	O
infinitely	O
many	O
elements	O
the	O
conditions	O
of	O
the	O
theorem	O
involve	O
the	O
bracketing	B
metric	B
entropy	B
of	O
f	O
defined	O
as	O
follows	O
for	O
every	O
distribution	O
of	O
x	O
and	O
e	O
let	O
fxe	O
be	O
a	O
set	O
of	O
functions	O
such	O
that	O
for	O
each	O
r	O
e	O
f	O
there	O
exist	O
functions	O
r	O
r	O
e	O
fxe	O
such	O
that	O
for	O
all	O
x	O
end	O
and	O
rx	O
rx	O
erx	O
rx	O
e	O
that	O
is	O
every	O
r	O
e	O
f	O
is	O
between	O
two	O
members	O
of	O
fxe	O
whose	O
distance	O
is	O
not	O
larger	O
than	O
e	O
let	O
n	O
e	O
denote	O
the	O
cardinality	O
of	O
the	O
smallest	O
such	O
fxe	O
if	O
nx	O
e	O
log	O
nx	O
e	O
is	O
called	O
the	O
bracketing	O
e	O
of	O
f	O
corresponding	O
to	O
x	O
the	O
maximum	B
likelihood	I
principle	O
theorem	O
let	O
f	O
be	O
a	O
class	O
of	O
regression	O
functions	O
n	O
d	O
assume	O
that	O
for	O
every	O
distribution	O
of	O
x	O
and	O
e	O
nx	O
e	O
then	O
the	O
maximum	B
likelihood	I
choice	O
rjn	O
satisfies	O
lim	O
l	O
n--oo	O
in	O
probability	O
for	O
all	O
distributions	O
of	O
y	O
with	O
rj	O
e	O
f	O
thus	O
consistency	B
is	O
guaranteed	O
if	O
f	O
has	O
a	O
finite	O
bracketing	O
e	O
for	O
every	O
x	O
and	O
e	O
we	O
provide	O
examples	O
in	O
the	O
next	O
section	O
for	O
the	O
proof	O
first	O
we	O
need	O
a	O
simple	O
corollary	O
of	O
lemma	O
corollary	O
letrj	O
rj	O
nd	O
and	O
let	O
ybeanndxo	O
random	O
variable	B
pair	O
with	O
py	O
llx	O
x	O
rjx	O
define	O
l	O
eminrjx	O
lrj	O
p	O
y	O
then	O
e	O
rjx	O
l	O
proof	O
the	O
first	O
inequality	B
follows	O
by	O
lemma	O
and	O
the	O
second	O
by	O
theorem	O
now	O
we	O
are	O
ready	O
to	O
prove	O
the	O
theorem	O
some	O
of	O
the	O
ideas	O
used	O
here	O
appear	O
in	O
wong	O
and	O
shen	O
proof	O
of	O
theorem	O
look	O
again	O
at	O
the	O
proof	O
for	O
the	O
case	O
i	O
fi	O
define	O
e	O
as	O
there	O
let	O
fe	O
be	O
the	O
collection	O
of	O
those	O
rj	O
e	O
f	O
with	O
lrj	O
l	O
e	O
recalling	O
that	O
lrj	O
p	O
y	O
for	O
every	O
a	O
for	O
reasons	O
that	O
will	O
be	O
obvious	O
later	O
we	O
take	O
a	O
thus	O
consistency	B
noting	O
that	O
e	O
the	O
law	O
of	O
large	O
numbers	O
implies	O
that	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
converges	O
to	O
zero	O
for	O
every	O
e	O
problem	O
for	O
more	O
information	O
next	O
we	O
bound	O
the	O
second	O
term	O
for	O
a	O
fixed	O
distribution	O
let	O
be	O
the	O
smallest	O
set	O
of	O
functions	O
such	O
that	O
for	O
each	O
ry	O
e	O
f	O
there	O
exist	O
ry	O
ry	O
e	O
with	O
ryx	O
ryx	O
ryx	O
x	O
end	O
and	O
eryx	O
ryx	O
where	O
will	O
be	O
specified	O
later	O
by	O
assumption	O
nx	O
we	O
have	O
ry	O
l-y	O
e	O
ryxi	O
ryxi	O
y	O
p	O
sup	O
ryefe	O
il	O
nn	O
ryxjy	O
n	O
sup	O
p	O
n	O
ryu	O
ryefe	O
il	O
ry	O
ry	O
e	O
f	O
x	O
ry	O
ry	O
ry	O
and	O
eryx	O
ry	O
n	O
i	O
y	O
ryl	O
i	O
l-y	O
i	O
the	O
union	O
bound	O
nx	O
p	O
x	O
sup	O
ryxn	O
ryefe	O
the	O
maximum	B
likelihood	I
principle	O
where	O
in	O
the	O
last	O
step	O
we	O
used	O
markovs	O
inequality	B
and	O
independence	O
we	O
now	O
find	O
a	O
good	O
bound	O
for	O
e	O
ryxryx	O
jl	O
ryx	O
ryx	O
for	O
each	O
e	O
fee	O
as	O
follows	O
e	O
ryxryx	O
jl	O
ry	O
ryx	O
e	O
j	O
ryx	O
ryx	O
ryx	O
ry	O
we	O
used	O
rab	O
fa	O
jb	O
e	O
the	O
cauchy-schwarz	B
inequality	B
e	O
corollary	O
summarizing	O
we	O
obtain	O
ryefe	O
p	O
sup	O
nx	O
r	O
by	O
taking	O
we	O
see	O
that	O
the	O
probability	O
converges	O
to	O
zero	O
exponentially	O
rapidly	O
which	O
concludes	O
the	O
proof	O
examples	O
in	O
this	O
section	O
we	O
show	O
by	O
example	O
how	O
to	O
apply	O
the	O
previous	O
consistency	B
results	O
in	O
all	O
cases	O
we	O
assume	O
that	O
e	O
f	O
and	O
we	O
are	O
concerned	O
with	O
the	O
weak	B
convergence	O
examples	O
of	O
lcrj	O
to	O
l	O
for	O
all	O
such	O
distributions	O
of	O
y	O
the	O
classes	O
are	O
as	O
follows	O
a	O
b	O
oo	O
cab	O
c	O
e	O
a	O
b	O
oo	O
ai	O
bi	O
i	O
d	O
fl	O
f	O
ryx	O
cxl	O
x	O
c	O
o	O
is	O
monotone	O
decreasing	O
on	O
i	O
ry	O
ryx	O
ryx	O
e	O
e	O
r	O
ilx	O
m	O
e	O
r	O
d	O
ry	O
ryx	O
eaoo	O
x	O
t	O
eaoa	O
x	O
e	O
r	O
x	O
a	O
e	O
rd	O
these	O
are	O
all	O
rather	O
simple	O
yet	O
they	O
will	O
illustrate	O
various	O
points	O
of	O
these	O
classes	O
is	O
nonparametric	O
yet	O
it	O
behaves	O
than	O
the	O
one-parameter	O
class	O
for	O
example	O
we	O
emphasize	O
again	O
that	O
we	O
are	O
interested	O
in	O
consistency	B
for	O
all	O
distributions	O
of	O
x	O
for	O
f	O
ryn	O
will	O
agree	O
with	O
the	O
samples	O
that	O
is	O
rynx	O
i	O
yi	O
for	O
all	O
i	O
and	O
therefore	O
ryn	O
e	O
fl	O
is	O
any	O
function	O
of	O
the	O
form	O
with	O
xl-i	O
a	O
xl	O
xr	O
b	O
xrl	O
where	O
xn	O
are	O
the	O
order	B
statistics	I
for	O
xi	O
xo	O
xnl	O
xl	O
is	O
the	O
smallest	O
data	O
point	O
with	O
yl	O
and	O
xr	O
is	O
the	O
largest	O
data	O
point	O
with	O
yr	O
as	O
l	O
we	O
claim	O
that	O
elryn	O
pxl-l	O
x	O
xl	O
x	O
xrl	O
the	O
rule	B
is	O
simply	O
excellent	O
and	O
has	O
universal	O
performance	O
guarantees	O
remark	O
note	O
that	O
in	O
this	O
case	O
maximum	B
likelihood	I
minimizes	O
the	O
empirical	B
risk	O
over	O
the	O
class	O
of	O
classifiers	O
c	O
a	O
b	O
as	O
c	O
has	O
vc	B
dimension	B
vc	O
and	O
inf	O
ec	O
l	O
theorem	O
implies	O
that	O
for	O
all	O
e	O
plryn	O
e	O
and	O
that	O
elryn	O
problem	O
with	O
the	O
analysis	O
given	O
here	O
we	O
have	O
gotten	O
rid	O
of	O
the	O
log	O
n	O
factor	O
the	O
maximum	B
likelihood	I
principle	O
the	O
class	O
f	O
is	O
much	O
more	O
interesting	O
here	O
you	O
will	O
observe	O
a	O
dramatic	O
ference	O
with	O
empirical	B
risk	I
minimization	I
as	O
the	O
parameter	O
c	O
plays	O
a	O
key	O
role	O
that	O
will	O
aid	O
a	O
lot	O
in	O
the	O
selection	O
note	O
that	O
the	O
likelihood	B
product	B
is	O
zero	O
if	O
ni	O
or	O
if	O
yi	O
for	O
some	O
i	O
with	O
xi	O
tj	O
b	O
and	O
it	O
is	O
cno	O
otherwise	O
where	O
no	O
is	O
the	O
number	O
of	O
yi	O
pairs	O
with	O
a	O
xi	O
b	O
yi	O
and	O
ni	O
is	O
the	O
number	O
of	O
yi	O
pairs	O
with	O
a	O
xi	O
b	O
yi	O
for	O
fixed	O
no	O
n	O
i	O
this	O
is	O
maximal	O
when	O
c---noni	O
resubstitution	B
yields	O
that	O
the	O
likelihood	B
product	B
is	O
zero	O
if	O
ni	O
or	O
if	O
yi	O
for	O
some	O
i	O
with	O
xi	O
tj	O
b	O
and	O
equals	O
exp	O
ni	O
no	O
nl	O
no	O
log	O
no	O
no	O
ni	O
if	O
otherwise	O
thus	O
we	O
should	O
pick	O
ciab	O
such	O
thatc	O
ninoni	O
and	O
b	O
maximizes	O
the	O
divergence	O
nilog	O
see	O
problem	O
ni	O
noni	O
no	O
no	O
log	O
noni	O
for	O
by	O
a	O
similar	O
argument	O
as	O
for	O
let	O
bd	O
x	O
x	O
bd	O
be	O
the	O
smallest	O
rectangle	O
of	O
r	O
d	O
that	O
encloses	O
all	O
xis	O
for	O
which	O
yi	O
then	O
we	O
know	O
that	O
xadbd	O
agrees	O
with	O
the	O
data	O
furthermore	O
l	O
and	O
problem	O
the	O
logarithm	O
of	O
the	O
likelihood	B
product	B
for	O
is	O
l	O
logcxi	O
cxi	O
n	O
iyil	O
il	O
setting	O
the	O
derivative	O
with	O
respect	O
to	O
c	O
equal	O
to	O
zero	O
in	O
the	O
hope	O
of	O
obtaining	O
an	O
equation	O
that	O
must	O
be	O
satisfied	O
by	O
the	O
maximum	O
we	O
see	O
that	O
c	O
must	O
satisfy	O
nitt	O
xi	O
il	O
cxi	O
xi	O
unless	O
xl	O
xn	O
where	O
nl	O
iyii	O
this	O
equation	O
has	O
a	O
unique	O
solution	O
for	O
c	O
the	O
rule	B
corresponding	O
to	O
is	O
of	O
the	O
form	O
gx	O
icxi	O
ixlc	O
equivalently	O
g	O
if	O
nl	O
xxi	O
otherwise	O
examples	O
this	O
surprising	O
example	O
shows	O
that	O
we	O
do	O
not	O
even	O
have	O
to	O
know	O
the	O
parameter	O
c	O
in	O
order	O
to	O
describe	O
the	O
maximum	B
likelihood	I
rule	B
in	O
quite	O
a	O
few	O
cases	O
this	O
shortcut	O
makes	O
such	O
rules	O
very	O
appealing	O
indeed	O
furthermore	O
as	O
the	O
condition	O
of	O
theorem	O
is	O
fulfilled	O
e	O
implies	O
that	O
the	O
rule	B
is	O
consistent	O
as	O
well	O
for	O
it	O
is	O
convenient	O
to	O
order	O
the	O
xis	O
from	O
small	O
to	O
large	O
and	O
to	O
identify	O
k	O
consecutive	O
groups	O
each	O
group	O
consisting	O
of	O
any	O
number	O
of	O
xis	O
with	O
yi	O
and	O
one	O
xi	O
with	O
yi	O
thus	O
k	O
iyii	O
then	O
a	O
moments	O
thought	O
shows	O
that	O
ln	O
must	O
be	O
piecewise	O
constant	O
taking	O
values	O
al	O
ak	O
on	O
the	O
consecutive	O
groups	O
and	O
the	O
value	O
zero	O
past	O
the	O
k-th	O
group	O
can	O
only	O
consist	O
of	O
xis	O
with	O
yi	O
the	O
likelihood	B
product	B
thus	O
is	O
of	O
the	O
form	O
aknk	O
where	O
n	O
i	O
n	O
k	O
are	O
the	O
cardinalities	O
of	O
the	O
groups	O
minus	O
one	O
the	O
number	O
of	O
yj	O
o-elements	O
in	O
the	O
i	O
group	O
finally	O
we	O
have	O
ak	O
argmax	O
it	O
ajn	O
j	O
k	O
lal	O
jl	O
to	O
check	O
consistency	B
we	O
see	O
that	O
for	O
every	O
x	O
and	O
e	O
nx	O
e	O
problem	O
thus	O
the	O
condition	O
of	O
theorem	O
is	O
satisfied	O
and	O
lln	O
l	O
in	O
probability	O
whenever	O
e	O
in	O
the	O
maximum	B
likelihood	I
method	O
will	O
attempt	O
to	O
place	O
m	O
at	O
the	O
center	O
of	O
the	O
highest	O
concentration	O
in	O
n	O
d	O
of	O
xis	O
with	O
yi	O
while	O
staying	O
away	O
from	O
xis	O
with	O
yi	O
certainly	O
there	O
are	O
computational	O
problems	O
but	O
it	O
takes	O
little	O
thought	O
to	O
verify	O
that	O
the	O
conditions	O
of	O
theorem	O
are	O
satisfied	O
explicit	O
description	O
of	O
the	O
rule	B
is	O
not	O
necessary	O
for	O
some	O
theoretical	B
analysis	O
class	O
is	O
a	O
simple	O
one-parameter	O
class	O
that	O
does	O
not	O
satisfy	O
the	O
condition	O
of	O
theorem	O
in	O
fact	O
maximum	B
likelihood	I
fails	O
here	O
for	O
the	O
following	O
reason	O
let	O
x	O
be	O
uniform	O
on	O
then	O
the	O
likelihood	B
product	B
is	O
it	O
x	O
it	O
iyio	O
this	O
product	B
reaches	O
a	O
degenerate	O
global	O
maximum	O
as	O
regardless	O
of	O
the	O
true	O
value	O
of	O
that	O
gave	O
rise	O
to	O
the	O
data	O
see	O
problem	O
class	O
is	O
used	O
in	O
the	O
popular	O
logistic	B
discrimination	I
problem	O
reviewed	O
and	O
studied	O
by	O
anderson	O
see	O
also	O
mclachlan	O
chapter	O
it	O
is	O
larly	O
important	O
to	O
observe	O
that	O
with	O
this	O
model	O
the	O
maximum	B
likelihood	I
principle	O
where	O
log	O
thus	O
subsumes	O
linear	O
discriminants	O
it	O
does	O
also	O
force	O
a	O
bit	O
more	O
structure	O
on	O
the	O
problem	O
making	O
rj	O
approach	O
zero	O
or	O
one	O
as	O
we	O
move	O
away	O
from	O
the	O
separating	O
hyperplane	O
day	O
and	O
kerridge	O
point	O
out	O
that	O
model	O
is	O
appropriate	O
if	O
the	O
class-conditional	B
densities	O
take	O
the	O
form	O
cfx	O
exp	O
mte-x	O
m	O
where	O
c	O
is	O
a	O
normalizing	O
constant	O
f	O
is	O
a	O
density	O
m	O
is	O
a	O
vector	O
and	O
b	O
is	O
a	O
positive	O
definite	O
matrix	O
f	O
and	O
must	O
be	O
the	O
same	O
for	O
the	O
two	O
densities	O
but	O
c	O
and	O
m	O
may	O
be	O
different	O
unfortunately	O
obtaining	O
the	O
best	O
values	O
for	O
cio	O
and	O
ci	O
by	O
maximum	B
likelihood	I
takes	O
a	O
serious	O
computational	O
effort	O
had	O
we	O
tried	O
to	O
estimate	O
f	O
m	O
and	O
in	O
the	O
last	O
example	O
we	O
would	O
have	O
done	O
more	O
than	O
what	O
is	O
needed	O
as	O
both	O
f	O
and	O
b	O
drop	O
out	O
of	O
the	O
picture	O
in	O
this	O
respect	O
the	O
regression	B
format	I
is	O
both	O
parsimonious	O
and	O
lightweight	O
classical	O
maximum	B
likelihood	I
distribution	B
format	I
in	O
a	O
more	O
classical	O
approach	O
we	O
assume	O
that	O
given	O
y	O
x	O
has	O
a	O
density	O
ii	O
and	O
given	O
y	O
x	O
has	O
a	O
density	O
fo	O
where	O
both	O
fo	O
and	O
ii	O
belong	O
to	O
a	O
given	O
family	B
of	I
densities	O
a	O
similar	O
setup	O
may	O
be	O
used	O
for	O
atomic	O
distributions	O
but	O
this	O
will	O
not	O
add	O
anything	O
new	O
here	O
and	O
is	O
rather	O
routine	O
the	O
likelihood	B
product	B
for	O
the	O
data	O
yd	O
yn	O
is	O
n	O
n	O
pfoxil-yi	O
il	O
where	O
p	O
p	O
i	O
is	O
assumed	O
to	O
be	O
unknown	O
as	O
well	O
the	O
maximum	B
likelihood	I
choices	O
for	O
p	O
fo	O
fl	O
are	O
given	O
by	O
f	O
ft	O
arg	O
max	O
lnp	O
fo	O
ii	O
peo	O
fo	O
ii	O
ef	O
where	O
having	O
determined	O
p	O
fo	O
ft	O
that	O
the	O
solution	O
is	O
not	O
necessarily	O
unique	O
the	O
maximum	B
likelihood	I
rule	B
is	O
i	O
if	O
pfox	O
p	O
ftx	O
otherwise	O
gn	O
generally	O
speaking	O
the	O
distribution	B
format	I
is	O
more	O
sensitive	O
than	O
the	O
regression	B
format	I
it	O
may	O
work	O
better	O
under	O
the	O
correct	O
circumstances	O
however	O
we	O
give	O
problems	O
and	O
exercises	O
up	O
our	O
universality	O
as	O
the	O
nature	O
of	O
the	O
distribution	O
of	O
x	O
must	O
be	O
known	O
for	O
example	O
if	O
x	O
is	O
distributed	O
on	O
a	O
lower	O
dimensional	O
nonlinear	O
manifold	O
of	O
n	O
d	O
the	O
distribution	B
format	I
is	O
particularly	O
inconvenient	O
consistency	B
results	O
and	O
examples	O
are	O
provided	O
in	O
a	O
few	O
exercises	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
if	O
ifi	O
k	O
and	O
e	O
f	O
then	O
l	O
with	O
probability	O
one	O
where	O
is	O
obtained	O
by	O
the	O
maximum	B
likelihood	I
method	O
also	O
prove	O
that	O
convergence	O
with	O
probability	O
one	O
holds	O
in	O
theorem	O
problem	O
prove	O
that	O
if	O
are	O
regression	O
functions	O
then	O
the	O
divergence	O
e	O
log	O
log	O
is	O
nonpositive	O
and	O
that	O
d	O
if	O
and	O
only	O
if	O
with	O
probability	O
one	O
in	O
this	O
sense	O
d	O
measures	O
the	O
distance	O
between	O
and	O
problem	O
let	O
l	O
p	O
y	O
and	O
e	O
log	O
where	O
py	O
llx	O
x	O
and	O
nd	O
is	O
arbitrary	O
prove	O
that	O
also	O
problem	O
hint	O
first	O
prove	O
that	O
for	O
p	O
q	O
e	O
plog-	O
plog	O
q	O
p	O
p	O
taylors	O
series	O
with	O
remainder	O
term	O
for	O
h	O
problem	O
show	O
that	O
for	O
each	O
there	O
exists	O
an	O
eo	O
such	O
that	O
for	O
all	O
e	O
e	O
eo	O
hint	O
proceed	O
by	O
chernoffs	O
bounding	O
technique	O
problem	O
consider	O
e	O
f	O
and	O
let	O
be	O
a	O
maximum	B
likelihood	I
estimate	O
over	O
f	O
let	O
p	O
py	O
i	O
show	O
that	O
c	O
l	O
pmin	O
derive	O
an	O
upper	O
bound	O
for	O
e	O
l	O
the	O
maximum	B
likelihood	I
principle	O
where	O
in	O
case	O
of	O
multiple	O
choices	O
for	O
you	O
take	O
the	O
smallest	O
in	O
the	O
equivalence	O
class	O
this	O
is	O
an	O
important	O
problem	O
as	O
f	O
picks	O
a	O
histogram	O
cell	O
in	O
a	O
data-based	B
manner	O
f	O
may	O
be	O
generalized	B
to	O
the	O
automatic	B
selection	O
of	O
the	O
best	O
k-cell	O
histogram	O
let	O
f	O
be	O
the	O
collection	O
of	O
that	O
are	O
constant	O
on	O
the	O
k	O
intervals	O
determined	O
by	O
breakpoints	O
al	O
ak-l	O
problem	O
show	O
that	O
for	O
the	O
class	O
s	O
nl	O
when	O
yj	O
e	O
and	O
yjn	O
is	O
the	O
maximum	B
likelihood	I
estimate	O
problem	O
show	O
that	O
for	O
the	O
class	O
holds	O
for	O
all	O
x	O
and	O
e	O
that	O
is	O
the	O
bracketing	O
e-entropy	O
of	O
is	O
bounded	O
by	O
hint	O
cover	O
the	O
class	O
by	O
a	O
class	O
of	O
monotone	O
decreasing	O
piecewise	O
constant	O
functions	O
whose	O
values	O
are	O
multiples	O
of	O
e	O
and	O
whose	O
breakpoints	O
are	O
at	O
the	O
ke	O
of	O
the	O
distribution	O
of	O
x	O
e	O
l	O
problem	O
discuss	O
the	O
maximum	B
likelihood	I
method	O
for	O
the	O
class	O
if	O
at	O
x	O
otherwise	O
ee	O
e	O
a	O
e	O
rd	O
e	O
r	O
what	O
do	O
the	O
discrimination	O
rules	O
look	O
like	O
if	O
yj	O
e	O
f	O
is	O
the	O
rule	B
consistent	O
can	O
you	O
guarantee	O
a	O
certain	O
rate	B
of	I
convergence	I
for	O
elyjn	O
if	O
yj	O
f	O
can	O
you	O
prove	O
that	O
does	O
not	O
converge	O
in	O
probability	O
to	O
lyj	O
for	O
some	O
distribution	O
of	O
y	O
with	O
p	O
x	O
x	O
how	O
would	O
you	O
obtain	O
the	O
values	O
of	O
e	O
e	O
a	O
for	O
the	O
maximum	B
likelihood	I
choice	O
problem	O
let	O
xl	O
be	O
i	O
i	O
d	O
uniform	O
random	O
variables	O
let	O
yi	O
yn	O
be	O
arbitrary	O
numbers	O
show	O
that	O
with	O
probability	O
one	O
lim	O
sup	O
n	O
x	O
n	O
cos	O
e-oo	O
iyil	O
iyio	O
while	O
for	O
any	O
t	O
with	O
probability	O
one	O
sup	O
n	O
sinexi	O
x	O
n	O
cos	O
osest	O
iyi	O
iyio	O
l	O
parametric	B
classification	I
what	O
do	O
you	O
do	O
if	O
you	O
believe	O
someone	O
tells	O
you	O
that	O
the	O
conditional	O
tions	O
of	O
x	O
given	O
y	O
and	O
y	O
are	O
members	O
of	O
a	O
given	O
family	B
of	I
distributions	O
described	O
by	O
finitely	O
many	O
real-valued	O
parameters	O
of	O
course	O
it	O
does	O
not	O
make	O
sense	O
to	O
say	O
that	O
there	O
are	O
say	O
six	O
parameters	O
by	O
interleaving	O
the	O
bits	O
of	O
binary	B
expansions	O
we	O
can	O
always	O
make	O
one	O
parameter	O
out	O
of	O
six	O
and	O
by	O
splitting	O
binary	B
expressions	O
we	O
may	O
make	O
a	O
countable	O
number	O
of	O
parameters	O
out	O
of	O
one	O
parameter	O
writing	O
the	O
bits	O
down	O
in	O
triangular	O
fashion	O
as	O
shown	O
below	O
bs	O
bs	O
bi	O
ho	O
thus	O
we	O
must	O
proceed	O
with	O
care	O
the	O
number	O
of	O
parameters	O
of	O
a	O
family	O
really	O
is	O
measured	O
more	O
by	O
the	O
sheer	O
size	O
or	O
vastness	O
of	O
the	O
family	O
than	O
by	O
mere	O
sentation	O
of	O
numbers	O
if	O
the	O
family	O
is	O
relatively	O
small	O
we	O
will	O
call	O
it	O
parametric	O
but	O
we	O
will	O
not	O
give	O
you	O
a	O
formal	O
definition	B
of	I
for	O
now	O
we	O
let	O
the	O
set	O
of	O
all	O
possible	O
values	O
of	O
the	O
parameter	O
e	O
be	O
a	O
subset	O
of	O
a	O
finite-dimensional	O
euclidean	B
space	O
formally	O
let	O
pe	O
e	O
e	O
be	O
a	O
class	O
of	O
probability	O
distributions	O
on	O
the	O
borel	O
sets	O
of	O
rd	O
typically	O
the	O
family	O
pe	O
is	O
parametrized	O
in	O
a	O
smooth	O
way	O
that	O
is	O
two	O
distributions	O
corresponding	O
to	O
two	O
parameter	O
vectors	O
close	O
to	O
each	O
other	O
are	O
in	O
some	O
sense	O
close	O
to	O
each	O
other	O
parametric	B
classification	I
as	O
well	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
and	O
fi	O
exist	O
and	O
that	O
both	O
belong	O
to	O
the	O
class	O
of	O
densities	O
fe	O
e	O
e	O
e	O
discrete	O
examples	O
may	O
be	O
handled	O
similarly	O
take	O
for	O
example	O
all	O
gaussian	B
butions	O
on	O
n	O
d	O
in	O
which	O
where	O
mend	O
is	O
the	O
vector	O
of	O
means	O
and	O
l	O
is	O
the	O
covariance	O
matrix	O
recall	O
that	O
x	O
t	O
denotes	O
the	O
transposition	O
ofthe	O
column	O
vector	O
x	O
and	O
detl	O
is	O
the	O
determinant	O
of	O
l	O
this	O
class	O
is	O
conveniently	O
parametrized	O
bye	O
l	O
that	O
is	O
a	O
vector	O
of	O
d	O
dd	O
real	O
numbers	O
knowing	O
that	O
the	O
class-conditional	B
distributions	O
are	O
in	O
pe	O
makes	O
discrimination	O
so	O
much	O
easier-rates	O
of	O
convergence	O
to	O
l	O
are	O
excellent	O
take	O
fe	O
as	O
the	O
class	O
of	O
uniform	O
densities	O
on	O
hyperrectangles	O
of	O
n	O
d	O
this	O
has	O
natural	O
parameters	O
the	O
coordinates	O
of	O
the	O
lower	O
left	O
and	O
upper	O
right	O
vertices	O
figure	O
the	O
class-conditional	B
densities	O
are	O
form	O
on	O
hyperrectangles	O
class	O
given	O
yd	O
yn	O
a	O
child	O
could	O
not	O
do	O
things	O
wrong-for	O
class	O
estimate	O
the	O
upper	O
right	O
vertex	O
by	O
and	O
similarly	O
for	O
the	O
upper	O
right	O
vertex	O
of	O
the	O
class	O
density	O
lower	O
left	O
vertices	O
are	O
estimated	O
by	O
considering	O
minima	O
if	O
ad	O
a	O
are	O
the	O
two	O
unknown	O
hyperrectangles	O
and	O
p	O
py	O
the	O
bayes	O
rule	B
is	O
simply	O
if	O
x	O
e	O
al	O
ad	O
o	O
if	O
x	O
e	O
ad	O
al	O
p	O
if	O
x	O
e	O
al	O
n	O
ad	O
aao	O
aai	O
p	O
gx	O
o	O
if	O
x	O
e	O
al	O
nao	O
p	O
aad	O
aao	O
p	O
parametric	B
classification	I
in	O
reality	O
replace	O
ao	O
ai	O
and	O
p	O
by	O
the	O
sample	O
estimates	O
ao	O
al	O
above	O
and	O
p	O
n	O
fyilj	O
this	O
way	O
of	O
doing	O
things	O
works	O
very	O
well	O
and	O
we	O
will	O
pick	O
up	O
the	O
example	O
a	O
bit	O
further	O
on	O
however	O
it	O
is	O
a	O
bit	O
ad	O
hoc	O
there	O
are	O
indeed	O
a	O
few	O
main	O
principles	O
that	O
may	O
be	O
used	O
in	O
the	O
design	O
of	O
classifiers	O
under	O
the	O
additional	O
information	O
given	O
here	O
in	O
no	O
particular	O
order	O
here	O
are	O
a	O
few	O
methodologies	O
as	O
the	O
bayes	O
classifiers	O
belong	O
to	O
the	O
class	O
c	O
fp	O
el	O
p	O
e	O
e	O
e	O
it	O
suffices	O
to	O
consider	O
classifiers	O
in	O
c	O
for	O
example	O
if	O
fe	O
is	O
the	O
normal	B
family	O
then	O
c	O
coincides	O
with	O
indicators	O
of	O
functions	O
in	O
the	O
set	O
x	O
t	O
ax	O
bt	O
x	O
c	O
a	O
is	O
a	O
d	O
x	O
d	O
matrix	O
b	O
e	O
r	O
d	O
c	O
e	O
r	O
that	O
is	O
the	O
family	B
of	I
quadratic	O
decisions	O
in	O
the	O
hyperrectangular	O
example	O
above	O
every	O
is	O
of	O
the	O
form	O
f	O
al	O
where	O
al	O
and	O
are	O
hyperrectangles	O
of	O
rd	O
finding	O
the	O
best	O
classifier	B
ofthe	O
form	O
fa	O
where	O
a	O
e	O
a	O
is	O
something	O
we	O
can	O
do	O
in	O
a	O
variety	O
of	O
ways	O
one	O
such	O
way	O
empirical	B
risk	I
minimization	I
is	O
dealt	O
with	O
in	O
chapter	O
for	O
example	O
plug-in	O
rules	O
estimate	O
by	O
ih	O
and	O
p	O
pry	O
i	O
by	O
pfrom	O
the	O
data	O
and	O
form	O
the	O
rule	B
gn	O
otherwise	O
i	O
if	O
pfejx	O
the	O
rule	B
here	O
is	O
within	O
the	O
class	O
c	O
described	O
in	O
the	O
previous	O
paragraph	O
we	O
are	O
hopeful	O
that	O
the	O
performance	O
with	O
gn	O
is	O
close	O
to	O
the	O
performance	O
with	O
the	O
bayes	O
rule	B
g	O
when	O
is	O
close	O
to	O
for	O
this	O
strategy	O
to	O
work	O
it	O
is	O
absolutely	O
essential	O
that	O
lp	B
probability	O
of	O
error	O
when	O
p	O
are	O
the	O
parameters	O
be	O
continuous	O
in	O
robustness	O
is	O
a	O
key	O
ingredient	O
if	O
the	O
coj	O
tinuity	O
can	O
be	O
captured	O
in	O
an	O
inequality	B
then	O
we	O
may	O
get	O
performance	O
guarantees	O
for	O
e	O
ln	O
l	O
in	O
terms	O
of	O
the	O
distance	O
between	O
and	O
methods	O
of	O
estimating	O
the	O
parameters	O
include	O
maximum	B
likelihood	I
this	O
methodology	O
is	O
dealt	O
with	O
in	O
chapter	O
this	O
method	O
is	O
rather	O
sensitive	O
to	O
incorrect	O
hypotheses	O
if	O
we	O
were	O
wrong	O
about	O
our	O
assumption	O
that	O
the	O
class-conditional	B
distributions	O
were	O
in	O
p	O
e	O
another	O
strategy	O
minimum	B
distance	I
estimation	B
picks	O
that	O
member	O
from	O
p	O
e	O
that	O
is	O
closest	O
in	O
some	O
sense	O
to	O
the	O
raw	B
empirical	B
measure	I
that	O
puts	O
mass	O
n	O
at	O
each	O
if	O
the	O
n	O
data	O
points	O
see	O
section	O
this	O
approach	O
does	O
not	O
care	O
about	O
continuity	O
of	O
lp	B
as	O
it	O
judges	O
members	O
of	O
pe	O
by	O
closeness	O
in	O
some	O
space	O
under	O
a	O
metric	O
that	O
is	O
directly	O
related	O
to	O
the	O
probability	O
of	O
error	O
robustness	O
will	O
drop	O
out	O
naturally	O
a	O
general	O
approach	O
should	O
not	O
have	O
to	O
know	O
whether	O
e	O
can	O
be	O
described	O
by	O
a	O
nite	O
number	O
of	O
parameters	O
for	O
example	O
it	O
should	O
equally	O
well	O
handle	O
descriptions	O
parametric	B
classification	I
as	O
pe	O
is	O
the	O
class	O
of	O
all	O
distributions	O
of	O
y	O
in	O
which	O
py	O
llx	O
x	O
is	O
monotonically	O
increasing	O
in	O
all	O
the	O
components	O
of	O
x	O
universal	O
paradigms	O
such	O
as	O
maximum	B
likelihood	I
minimum	B
distance	I
estimation	B
and	O
empirical	B
error	I
imization	O
are	O
all	O
applicable	O
here	O
this	O
particular	O
pe	O
is	O
dealt	O
with	O
in	O
chapter	O
just	O
to	O
show	O
you	O
that	O
the	O
description	O
of	O
the	O
class	O
does	O
invalidate	O
the	O
underlying	O
principles	O
example	O
exponential	B
families	O
a	O
class	O
pe	O
is	O
exponential	B
if	O
every	O
class-conditional	B
density	I
fe	O
can	O
be	O
written	O
in	O
the	O
form	O
where	O
ok	O
rd	O
r	O
a	O
jtk	O
e	O
r	O
are	O
fixed	O
tions	O
and	O
c	O
is	O
a	O
normalizing	O
constant	O
examples	O
of	O
exponential	B
families	O
include	O
the	O
gaussian	B
gamma	B
beta	B
rayleigh	B
and	O
maxwell	B
densities	O
problem	O
the	O
bayes-rule	O
can	O
be	O
rewritten	O
as	O
gx	O
this	O
is	O
equivalent	O
to	O
p	O
ex	O
if	O
log	O
otherwise	O
gx	O
jtie	O
log	O
otherwise	O
the	O
bayes-rule	O
is	O
a	O
so-called	O
generalized	B
linear	O
rule	B
with	O
ok	O
as	O
basis	O
functions	O
such	O
rules	O
are	O
easily	O
dealt	O
with	O
by	O
empirical	B
risk	I
minimization	I
and	O
related	O
methods	O
such	O
as	O
complexity	B
regularization	I
another	O
important	O
point	O
is	O
that	O
g	O
does	O
not	O
involve	O
the	O
function	O
for	O
all	O
we	O
know	O
may	O
be	O
some	O
esoteric	O
ill-behaved	O
function	O
that	O
would	O
make	O
estimating	O
fex	O
all	O
but	O
impossible	O
if	O
were	O
unknown	O
even	O
if	O
pe	O
is	O
the	O
huge	O
family	O
in	O
which	O
is	O
left	O
undetermined	O
but	O
it	O
is	O
known	O
to	O
be	O
identical	O
for	O
the	O
two	O
conditional	O
densities	O
ace	O
is	O
just	O
a	O
normalization	O
factor	O
we	O
would	O
still	O
only	O
have	O
to	O
look	O
at	O
the	O
same	O
small	O
class	O
of	O
generalized	B
linear	O
discrimination	O
rules	O
so	O
densities	O
do	O
not	O
matter-ratios	O
of	O
densities	O
do	O
pattern	O
recognition	O
should	O
be	O
and	O
is	O
easier	O
than	O
density	B
estimation	B
those	O
who	O
first	O
estimate	O
by	O
it	O
and	O
then	O
construct	O
rules	O
based	O
on	O
the	O
sign	O
of	O
fij-	O
fj	O
do	O
themselves	O
a	O
disservice	O
standard	B
plug-in	O
rules	O
standard	B
plug-in	O
rules	O
in	O
standard	B
plug-in	O
methods	O
we	O
construct	O
estimates	O
bt	O
and	O
pfrom	O
the	O
data	O
and	O
use	O
them	O
to	O
form	O
a	O
classifier	B
gn	O
otherwise	O
it	O
is	O
generally	O
not	O
true	O
that	O
if	O
p	O
p	O
and	O
bt	O
in	O
probability	O
then	O
lgn	O
l	O
in	O
probability	O
where	O
lgn	O
is	O
the	O
probability	O
of	O
error	O
with	O
gn	O
consider	O
the	O
following	O
simple	O
example	O
example	O
letfe	O
be	O
the	O
class	O
of	O
all	O
uniform	O
densities	O
on	O
if	O
and	O
on	O
if	O
let	O
p	O
then	O
a	O
reasonable	O
estimate	O
of	O
would	O
be	O
fh	O
maxiyil	O
ixi	O
i	O
clearly	O
fh	O
in	O
probability	O
however	O
as	O
fh	O
with	O
probability	O
one	O
we	O
note	O
that	O
gn	O
for	O
x	O
and	O
thus	O
even	O
though	O
l	O
the	O
supports	O
of	O
fed	O
and	O
fel	O
are	O
not	O
overlapping	O
lgn	O
py	O
i	O
p	O
the	O
problem	O
with	O
this	O
is	O
that	O
there	O
is	O
no	O
continuity	O
with	O
respect	O
to	O
in	O
basic	O
consistency	B
based	O
upon	O
continuity	O
considerations	O
is	O
indeed	O
easy	O
to	O
lish	O
as	O
ratios	O
of	O
densities	O
matter	O
it	O
helps	O
to	O
introduce	O
pfel	O
p	O
fed	O
p	O
fel	O
py	O
llx	O
x	O
where	O
or	O
as	O
the	O
case	O
may	O
be	O
we	O
recall	O
that	O
if	O
gnx	O
where	O
ois	O
an	O
estimate	O
of	O
then	O
lgn	O
l	O
dn	O
where	O
dn	O
is	O
the	O
data	O
sequence	O
thus	O
e	O
l	O
by	O
the	O
lebesgue	O
dominated	B
convergence	I
theorem	I
we	O
have	O
without	O
further	O
ado	O
theorem	O
if	O
is	O
continuous	O
in	O
in	O
the	O
l	O
sense	O
where	O
m	O
is	O
the	O
measure	O
of	O
x	O
and	O
in	O
probability	O
then	O
elgn	O
l	O
for	O
the	O
standard	B
plug-in	O
rule	B
in	O
some	O
cases	O
we	O
can	O
do	O
better	O
and	O
derive	O
rates	O
of	O
convergence	O
by	O
examining	O
the	O
local	O
behavior	O
of	O
for	O
example	O
if	O
e-ex	O
x	O
e	O
r	O
then	O
and	O
elgn	O
l	O
parametric	B
classification	I
yielding	O
an	O
explicit	O
bound	O
in	O
general	O
is	O
multivariate	O
consisting	O
at	O
least	O
of	O
the	O
triple	O
but	O
the	O
above	O
example	O
shows	O
the	O
way	O
to	O
happy	O
analysis	O
for	O
the	O
simple	O
example	O
given	O
here	O
e	O
lgn	O
l	O
if	O
e	O
e	O
o	O
in	O
fact	O
this	O
seems	O
to	O
suggest	O
that	O
for	O
this	O
family	O
e	O
should	O
be	O
found	O
to	O
minimize	O
e	O
this	O
is	O
false	O
one	O
is	O
always	O
best	O
off	O
minimizing	O
the	O
probability	O
of	O
error	O
other	O
criteria	O
may	O
be	O
relevant	O
via	O
continuity	O
but	O
should	O
be	O
considered	O
with	O
care	O
how	O
certain	O
parameters	O
are	O
estimated	O
for	O
given	O
families	O
of	O
distributions	O
is	O
what	O
mathematical	O
statistics	O
is	O
all	O
about	O
the	O
maximum	B
likelihood	I
principle	O
looms	O
large	O
is	O
estimated	O
for	O
densities	O
ie	O
by	O
argmax	O
n	O
lexj	O
e	O
iyio	O
for	O
example	O
if	O
you	O
work	O
out	O
this	O
product	B
you	O
will	O
often	O
discover	O
a	O
simple	O
form	O
for	O
the	O
estimate	O
of	O
the	O
data	O
in	O
discrimination	O
only	O
matters	O
not	O
the	O
class-conditional	B
densities	O
maximum	B
likelihood	I
in	O
function	O
of	O
the	O
was	O
studied	O
in	O
chapter	O
we	O
saw	O
there	O
that	O
this	O
is	O
often	O
consistent	O
but	O
that	O
maximum	B
likelihood	I
behaves	O
poorly	O
when	O
the	O
true	O
distribution	O
is	O
not	O
in	O
pe	O
we	O
will	O
work	O
out	O
two	O
simple	O
examples	O
as	O
an	O
example	O
of	O
the	O
maximum	B
likelihood	I
method	O
in	O
discrimination	O
we	O
assume	O
that	O
fe	O
x	O
a	O
l	O
x	O
e	O
ixo	O
a	O
oj	O
is	O
the	O
class	O
of	O
all	O
gamma	B
densities	O
with	O
the	O
likelihood	B
product	B
given	O
yl	O
yn	O
is	O
if	O
are	O
the	O
unknown	O
parameters	O
n	O
n	O
peoxil-yi	O
ii	O
this	O
is	O
the	O
probability	O
of	O
observing	O
the	O
data	O
sequence	O
if	O
the	O
ies	O
were	O
in	O
fact	O
discrete	O
probabilities	O
this	O
product	B
is	O
simply	O
where	O
the	O
first	O
thing	O
we	O
notice	O
is	O
that	O
this	O
expression	B
depends	O
only	O
on	O
certain	O
functions	O
of	O
the	O
data	O
notably	O
l	O
yi	O
xi	O
ll	O
yi	O
l	O
yi	O
log	O
xi	O
ll	O
yi	O
log	O
xi	O
and	O
l	O
yi	O
these	O
are	O
called	O
the	O
sufficient	B
statistics	I
for	O
the	O
problem	O
at	O
hand	O
we	O
may	O
in	O
fact	O
throwaway	O
the	O
data	O
and	O
just	O
store	O
the	O
sufficient	B
statistics	I
the	O
likelihood	B
product	B
has	O
to	O
be	O
maximized	O
even	O
in	O
this	O
rather	O
simple	O
univariate	O
example	O
this	O
is	O
a	O
nontrivial	O
task	O
luckily	O
we	O
immediately	O
note	O
that	O
p	O
occurs	O
in	O
the	O
factor	O
pn	O
p	O
where	O
n	O
l	O
yi	O
this	O
is	O
maximal	O
at	O
p	O
n	O
in	O
a	O
well-known	O
result	O
for	O
fixed	O
ao	O
ai	O
we	O
can	O
also	O
get	O
but	O
for	O
standard	B
plug-in	O
rules	O
variable	B
ao	O
ai	O
the	O
optimization	O
is	O
difficult	O
in	O
d-dimensional	O
cases	O
one	O
has	O
nearly	O
always	O
to	O
resort	O
to	O
specialized	O
algorithms	O
as	O
a	O
last	O
example	O
we	O
return	O
to	O
fe	O
densities	O
on	O
rectangles	O
of	O
rd	O
here	O
the	O
likelihood	B
product	B
once	O
again	O
has	O
pn	O
p	O
as	O
a	O
factor	O
leading	O
to	O
p	O
n	O
in	O
the	O
other	O
factor	O
bl	O
bo	O
are	O
the	O
lower	O
left	O
upper	O
right	O
vertices	O
of	O
the	O
rectangles	O
for	O
is	O
zero	O
if	O
for	O
some	O
i	O
yi	O
xi	O
tf-	O
rectangleal	O
bd	O
or	O
yi	O
xi	O
tf-	O
rectangleao	O
bo	O
otherwise	O
it	O
is	O
where	O
ilbl	O
al	O
ii	O
til	O
aij	O
denotes	O
the	O
volume	O
of	O
the	O
rectangle	O
h	O
and	O
similarly	O
for	O
iibo	O
ii	O
this	O
is	O
maximal	O
if	O
iibl	O
ii	O
and	O
iibo	O
ii	O
are	O
minimal	O
thus	O
the	O
maximum	B
likelihood	I
estimates	O
are	O
min	O
xk	O
k	O
d	O
i	O
ai	O
jyji	O
jk	O
max	O
xk	O
jyfi	O
k	O
d	O
i	O
where	O
ai	O
ai	O
bi	O
bi	O
and	O
xi	O
xi	O
rates	O
of	O
convergence	O
may	O
be	O
obtained	O
via	O
some	O
of	O
the	O
of	O
chapter	O
such	O
as	O
elgn	O
l	O
pfeo	O
elgn	O
l	O
elgn	O
l	O
i	O
ryx	O
i	O
the	O
rate	O
with	O
which	O
e	O
approaches	O
in	O
e	O
with	O
some	O
metric	O
may	O
be	O
very	O
different	O
from	O
that	O
with	O
which	O
lgn	O
approaches	O
l	O
as	O
shown	O
in	O
theorem	O
the	O
inequality	B
is	O
always	O
loose	O
yet	O
it	O
is	O
this	O
inequality	B
that	O
is	O
often	O
used	O
to	O
derive	O
rates	O
of	O
convergence	O
by	O
authors	O
and	O
researchers	O
let	O
us	O
take	O
a	O
simple	O
example	O
to	O
illustrate	O
this	O
point	O
assume	O
fe	O
is	O
the	O
family	B
of	I
nonnal	O
densities	O
on	O
the	O
real	O
line	O
if	O
ao	O
al	O
are	O
the	O
unknown	O
means	O
and	O
standard	B
deviations	O
of	O
the	O
class-conditional	B
densities	O
and	O
p	O
py	O
is	O
also	O
unknown	O
then	O
we	O
may	O
estimate	O
p	O
by	O
p	O
ll	O
yj	O
in	O
and	O
mi	O
and	O
ai	O
by	O
respectively	O
when	O
denominators	O
are	O
positive	O
if	O
a	O
denominator	O
is	O
set	O
mi	O
from	O
chebyshevs	O
inequality	B
we	O
can	O
verify	O
that	O
for	O
fixed	O
p	O
e	O
i	O
parametric	B
classification	I
elp	O
ph	O
elmi	O
mil	O
elai	O
if	O
we	O
compute	O
e	O
we	O
will	O
discover	O
that	O
elgn	O
l	O
o	O
however	O
if	O
we	O
compute	O
e	O
ignxtgxd	O
we	O
will	O
find	O
that	O
elgn	O
l	O
thus	O
while	O
the	O
parameters	O
converge	O
at	O
a	O
rate	O
o	O
i	O
dictated	O
by	O
the	O
central	B
limit	I
theorem	I
and	O
while	O
converges	O
to	O
in	O
l	O
with	O
the	O
same	O
rate	O
the	O
error	O
rate	O
in	O
discrimination	O
is	O
much	O
smaller	O
see	O
problems	O
to	O
for	O
some	O
practice	O
in	O
this	O
respect	O
bibliographic	O
remarks	O
mclachlan	O
has	O
a	O
comprehensive	O
treatment	O
on	O
parametric	B
classification	I
duda	O
and	O
hart	O
have	O
many	O
good	O
introductory	O
examples	O
and	O
a	O
nice	O
discussion	O
on	O
sufficient	B
statistics	I
a	O
topic	O
we	O
do	O
not	O
deal	O
with	O
in	O
this	O
text	O
for	O
maximum	B
likelihood	I
estimation	B
see	O
hjort	O
minimum	O
distance	O
estimates	O
here	O
we	O
describe	O
a	O
general	O
parameter	B
estimation	B
principle	O
that	O
appears	O
to	O
be	O
more	O
suitable	O
for	O
plug-in	O
classification	O
rules	O
than	O
the	O
maximum	B
likelihood	I
method	O
the	O
estimated	O
parameter	O
is	O
obtained	O
by	O
the	O
projection	O
of	O
the	O
empirical	B
measure	I
on	O
the	O
parametric	O
family	O
the	O
principle	O
of	O
minimum	B
distance	I
estimation	B
may	O
be	O
described	O
as	O
follows	O
let	O
e	O
be	O
a	O
parametric	O
family	B
of	I
distributions	O
and	O
assume	O
that	O
pe	O
is	O
the	O
unknown	O
distribution	O
of	O
the	O
i	O
i	O
d	O
observations	O
zl	O
zn	O
denote	O
by	O
vn	O
the	O
empirical	B
measure	I
let	O
d	O
be	O
a	O
metric	O
on	O
the	O
set	O
of	O
all	O
probability	O
distributions	O
on	O
nd	O
the	O
minimum	O
distance	O
estimate	O
of	O
is	O
defined	O
as	O
argmin	O
dvn	O
pe	O
eee	O
if	O
it	O
exists	O
and	O
is	O
unique	O
if	O
it	O
is	O
not	O
unique	O
select	O
one	O
candidate	O
for	O
which	O
the	O
minimum	O
is	O
attained	O
consider	O
for	O
example	O
the	O
kolmogorov-smirnov	B
distance	I
dksp	O
q	O
sup	O
ifz	O
gz	O
zend	O
minimum	O
distance	O
estimates	O
pen	O
vn	O
figure	O
the	O
member	O
oips	O
closest	O
to	O
the	O
empirical	B
measure	I
vn	O
is	O
chosen	O
by	O
the	O
minimum	O
distance	O
estimate	O
where	O
f	O
and	O
g	O
are	O
the	O
distribution	O
functions	O
of	O
the	O
measures	O
p	O
and	O
q	O
on	O
n	O
d	O
respectively	O
it	O
is	O
easy	O
to	O
see	O
that	O
d	O
ks	O
is	O
a	O
metric	O
note	O
that	O
sup	O
ifz	O
sup	O
lpa	O
qai	O
zerd	O
aea	O
where	O
a	O
is	O
the	O
class	O
of	O
sets	O
of	O
the	O
form	O
x	O
x	O
zed	O
for	O
z	O
zed	O
e	O
nd	O
for	O
the	O
kolmogorov-smirnov	B
distance	I
between	O
the	O
estimated	O
and	O
the	O
true	O
distributions	O
by	O
the	O
triangle	O
inequality	B
we	O
have	O
dkspen	O
pe	O
dkspell	O
vn	O
dksvn	O
pe	O
pe	O
where	O
in	O
the	O
second	O
inequality	B
we	O
used	O
the	O
definition	B
of	I
en	O
now	O
notice	O
that	O
the	O
upper	O
bound	O
is	O
just	O
twice	O
the	O
kolmogorov-smirnov	B
distance	I
between	O
an	O
empirical	B
distribution	O
and	O
the	O
true	O
distribution	O
by	O
a	O
straightforward	O
application	O
of	O
theorem	O
for	O
every	O
nand	O
e	O
since	O
the	O
n-th	O
shatter	B
coefficient	I
sea	O
n	O
of	O
the	O
class	O
of	O
sets	O
cannot	O
exceed	O
dd	O
this	O
can	O
easily	O
be	O
seen	O
by	O
an	O
argument	O
similar	O
to	O
that	O
in	O
the	O
proof	O
of	O
theorem	O
from	O
the	O
inequalities	O
above	O
we	O
see	O
that	O
the	O
smirnov	O
distance	O
between	O
the	O
estimated	O
and	O
the	O
true	O
distributions	O
is	O
always	O
o	O
jlog	O
n	O
n	O
the	O
only	O
condition	O
we	O
require	O
is	O
that	O
en	O
be	O
well	O
defined	O
parametric	B
classification	I
of	O
course	O
rather	O
than	O
the	O
kolmogorov-smirnov	B
distance	I
it	O
is	O
the	O
error	O
ability	O
of	O
the	O
plug-in	O
classification	O
rule	B
in	O
which	O
we	O
are	O
primarily	O
interested	O
in	O
order	O
to	O
make	O
the	O
connection	O
we	O
adapt	O
the	O
notions	O
of	O
minimum	O
distance	O
tion	O
and	O
kolmogorov-smirnov	B
distance	I
to	O
better	O
suit	O
the	O
classification	O
problem	O
we	O
are	O
after	O
every	O
parametric	O
family	B
of	I
distributions	O
defines	O
a	O
class	O
of	O
sets	O
in	O
nd	O
as	O
the	O
collection	O
a	O
of	O
sets	O
of	O
the	O
form	O
i	O
where	O
the	O
classifiers	O
are	O
the	O
possible	O
plug-in	O
rules	O
defined	O
by	O
the	O
parametric	O
family	O
the	O
idea	O
here	O
is	O
to	O
perform	O
minimum	B
distance	I
estimation	B
with	O
the	O
generalized	B
kolmogorov-smirnov	B
distance	I
with	O
respect	O
to	O
a	O
class	O
of	O
sets	O
closely	O
related	O
to	O
a	O
assume	O
that	O
both	O
class-conditional	B
distributions	O
pea	O
pel	O
belong	O
to	O
a	O
parametric	O
family	O
pe	O
and	O
the	O
class-conditional	B
densities	O
feo	O
fe	O
exist	O
then	O
the	O
bayes	O
rule	B
may	O
be	O
written	O
as	O
ifaex	O
g	O
where	O
aex	O
pfelx	O
pfeax	O
and	O
p	O
py	O
i	O
we	O
use	O
the	O
short	O
notation	O
the	O
function	O
ae	O
may	O
be	O
thought	O
of	O
as	O
the	O
radon-nikodym	B
derivative	I
of	O
the	O
signed	B
measure	I
qe	O
ppeo	O
in	O
other	O
words	O
to	O
each	O
borel	O
set	O
a	O
c	O
n	O
d	O
qe	O
assigns	O
the	O
real	O
number	O
qea	O
ppeoa	O
given	O
the	O
data	O
dn	O
xl	O
yd	O
yn	O
we	O
define	O
the	O
empirical	B
counterpart	O
of	O
qe	O
by	O
otherwise	O
the	O
minimum	O
distance	O
classification	O
rule	B
we	O
propose	O
projects	O
the	O
empirical	B
signed	B
measure	I
on	O
the	O
set	O
of	O
measures	O
qe	O
the	O
metric	O
we	O
use	O
is	O
also	O
specifically	O
fitted	O
to	O
the	O
given	O
pattern	O
recognition	O
problem	O
define	O
the	O
class	O
of	O
sets	O
a	O
e	O
nd	O
aex	O
o	O
p	O
e	O
e	O
e	O
a	O
is	O
just	O
the	O
class	O
of	O
sets	O
a	O
c	O
nd	O
such	O
that	O
ixea	O
is	O
the	O
bayes	O
rule	B
for	O
some	O
also	O
introduce	O
n	O
be	O
a	O
b	O
e	O
a	O
given	O
two	O
signed	O
measures	O
q	O
q	O
we	O
define	O
their	O
generalized	B
smirnov	O
distance	O
by	O
dbq	O
q	O
sup	O
iqa	O
qai	O
aeb	O
that	O
is	O
instead	O
of	O
the	O
class	O
of	O
half-infinite	O
intervals	O
as	O
in	O
the	O
definition	B
of	I
the	O
ordinary	B
kolmogorov-smirnov	B
distance	I
here	O
we	O
take	O
the	O
supremum	O
over	O
a	O
class	O
tailored	O
to	O
our	O
discrimination	O
problem	O
now	O
we	O
are	O
ready	O
to	O
define	O
our	O
minimum	O
distance	O
estimate	O
e	O
argmin	O
dbqe	O
e	O
minimum	O
distance	O
estimates	O
where	O
the	O
minimum	O
is	O
taken	O
over	O
all	O
triples	O
e	O
eo	O
with	O
p	O
e	O
e	O
e	O
the	O
corresponding	O
classification	O
rule	B
is	O
if	O
ae	O
x	O
ggx	O
otherwise	O
i	O
the	O
next	O
theorem	O
shows	O
that	O
if	O
the	O
parametric	O
assumption	O
is	O
valid	O
then	O
ge	O
performs	O
extremely	O
well	O
the	O
theorem	O
shows	O
that	O
if	O
a	O
has	O
finite	O
vc	B
dimension	B
v	O
a	O
then	O
out	O
any	O
additional	O
conditions	O
imposed	O
on	O
the	O
parametric	O
class	O
the	O
corresponding	O
error	O
probability	O
lge	O
is	O
not	O
large	O
lge	O
l	O
log	O
nn	O
theorem	O
assume	O
that	O
both	O
conditional	O
densities	O
feo	O
fe	O
l	O
are	O
in	O
the	O
metric	O
class	O
fe	O
then	O
for	O
the	O
classification	O
rule	B
defined	O
above	O
we	O
have	O
for	O
every	O
nand	O
e	O
that	O
p	O
l	O
e	O
where	O
n	O
is	O
the	O
n-th	O
shatter	B
coefficient	I
of	O
furthermore	O
e	O
l	O
n	O
remark	O
recalling	O
from	O
chapter	O
that	O
n	O
n	O
and	O
that	O
sea	O
n	O
lva	O
where	O
va	O
is	O
the	O
vc	B
dimension	B
of	O
a	O
we	O
obtain	O
the	O
bound	O
p	O
l	O
e	O
the	O
proof	O
of	O
the	O
theorem	O
is	O
based	O
upon	O
two	O
key	O
observations	O
the	O
first	O
lemma	O
provides	O
a	O
bound	O
on	O
lge	O
l	O
in	O
terms	O
of	O
the	O
generalized	B
kolmogorov-smirnov	B
distance	I
between	O
the	O
estimated	O
and	O
the	O
true	O
parameters	O
the	O
second	O
lemma	O
is	O
a	O
straightforward	O
extension	O
of	O
the	O
vapnik-chervonenkis	B
inequality	B
to	O
signed	O
sures	O
lemma	O
proof	O
denote	O
ae	O
aex	O
o	O
and	O
ae	O
aex	O
a	O
that	O
is	O
ggx	O
iagx	O
o	O
gx	O
iaex	O
o	O
and	O
ae	O
ae	O
e	O
a	O
at	O
the	O
first	O
crucial	O
step	O
we	O
use	O
the	O
equality	O
of	O
theorem	O
lge	O
l	O
f	O
igexfgex	O
laexldx	O
r	O
c	O
aexdx	O
j	O
aenae	O
j	O
aenae	O
aexdx	O
parametric	B
classification	I
r	O
aexdx	O
r	O
aexdx	O
j	O
aena	O
r	O
aexdx	O
r	O
aexdx	O
janae	O
janae	O
j	O
aena	O
qeae	O
n	O
a	O
qeae	O
n	O
a	O
qea	O
n	O
ae	O
qea	O
n	O
ae	O
qe	O
since	O
both	O
ae	O
n	O
a	O
and	O
a	O
n	O
ae	O
are	O
members	O
of	O
be	O
the	O
next	O
lemma	O
is	O
an	O
extension	O
of	O
the	O
vapnik	O
inequality	B
the	O
proof	O
is	O
left	O
to	O
the	O
reader	O
lemma	O
for	O
every	O
nand	O
e	O
p	O
e	O
the	O
rest	O
of	O
the	O
proof	O
of	O
theorem	O
shows	O
that	O
the	O
generalized	B
smimov	O
distance	O
with	O
respect	O
to	O
b	O
between	O
the	O
estimated	O
and	O
true	O
distributions	O
is	O
small	O
with	O
large	O
probability	O
this	O
can	O
be	O
done	O
as	O
we	O
proved	O
for	O
the	O
smimov	O
distance	O
at	O
the	O
beginning	O
of	O
the	O
section	O
proof	O
of	O
theorem	O
by	O
lemma	O
lge	O
l	O
qe	O
the	O
definition	B
of	I
e	O
the	O
triangle	O
inequality	B
the	O
theorem	O
now	O
follows	O
by	O
lemma	O
finally	O
we	O
examine	O
robustness	O
of	O
the	O
minimum	O
distance	O
rule	B
against	O
modeling	O
errors	O
that	O
is	O
what	O
happens	O
if	O
the	O
distributions	O
are	O
not	O
in	O
pe	O
a	O
good	O
rule	B
should	O
still	O
work	O
reasonably	O
well	O
if	O
the	O
distributions	O
are	O
in	O
some	O
sense	O
close	O
to	O
the	O
modeled	O
parametric	O
class	O
pe	O
observe	O
that	O
if	O
for	O
some	O
e	O
eo	O
the	O
bayes	O
rule	B
can	O
be	O
written	O
as	O
if	O
ae	O
g	O
otherwise	O
then	O
lemma	O
remains	O
valid	O
even	O
when	O
the	O
class-conditional	B
distributions	O
are	O
not	O
in	O
p	O
e	O
denote	O
the	O
true	O
class-conditional	B
distributions	O
by	O
p	O
pt	O
let	O
p	O
pry	O
and	O
introduce	O
q	O
p	O
pt	O
pp	O
thus	O
lge	O
l	O
lemma	O
qe	O
qe	O
qe	O
vn	O
qe	O
the	O
triangle	O
inequality	B
the	O
definition	B
of	I
qe	O
by	O
the	O
triangle	O
inequality	B
empirical	B
error	I
minimization	O
lemma	O
now	O
applies	O
to	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
thus	O
we	O
conclude	O
that	O
if	O
the	O
bayes	O
rule	B
is	O
in	O
a	O
then	O
for	O
all	O
e	O
inf	O
qe	O
ds	O
q	O
qe	O
p	O
l	O
ejs	O
the	O
constant	O
in	O
the	O
exponent	O
may	O
be	O
improved	O
significantly	O
by	O
more	O
careful	O
ysis	O
in	O
other	O
words	O
if	O
the	O
bayes	O
rule	B
is	O
in	O
a	O
and	O
the	O
true	O
distribution	O
is	O
close	O
to	O
the	O
parametric	O
family	O
in	O
the	O
generalized	B
kolmogorov-smimov	O
distance	O
specified	O
above	O
then	O
the	O
minimum	O
distance	O
rule	B
still	O
performs	O
close	O
to	O
the	O
bayes	B
error	I
unfortunately	O
we	O
cannot	O
say	O
the	O
same	O
if	O
a	O
does	O
not	O
contain	O
the	O
bayes	O
rule	B
pirical	O
error	O
minimization	O
discussed	O
in	O
the	O
next	O
section	O
is	O
however	O
very	O
robust	O
in	O
all	O
situations	O
empirical	B
error	I
minimization	O
in	O
this	O
section	O
we	O
explore	O
the	O
connection	O
between	O
parametric	B
classification	I
and	O
rule	B
selection	O
by	O
minimizing	O
the	O
empirical	B
error	I
studied	O
in	O
chapter	O
consider	O
the	O
class	O
c	O
of	O
classifiers	O
of	O
the	O
form	O
if	O
pfex	O
otherwise	O
where	O
p	O
e	O
and	O
eo	O
el	O
e	O
the	O
parametric	O
assumption	O
means	O
that	O
the	O
bayes	O
rule	B
is	O
contained	O
in	O
c	O
then	O
it	O
is	O
a	O
very	O
natural	O
approach	O
to	O
minimize	O
the	O
empirical	B
error	I
probability	O
measured	O
on	O
the	O
training	O
data	O
dn	O
over	O
classifiers	O
in	O
the	O
class	O
c	O
denote	O
the	O
empirically	O
selected	O
rule	B
the	O
one	O
minimizing	O
ln	O
by	O
for	O
most	O
typical	O
parametric	O
classes	O
the	O
vc	B
dimension	B
vc	O
is	O
finite	O
therefore	O
as	O
a	O
straightforward	O
consequence	O
of	O
theorem	O
we	O
have	O
corollary	O
if	O
both	O
conditional	O
distributions	O
are	O
contained	O
in	O
the	O
ric	O
family	O
pes	O
then	O
for	O
the	O
error	O
probability	O
l	O
p	O
yidn	O
of	O
the	O
empirically	O
optimal	O
rule	B
we	O
have	O
for	O
every	O
nand	O
e	O
p	O
l	O
e	O
the	O
result	O
above	O
means	O
that	O
n	O
n	O
rate	B
of	I
convergence	I
to	O
the	O
bayes	O
rule	B
is	O
guaranteed	O
for	O
the	O
empirically	O
optimal	O
rule	B
whenever	O
the	O
vc	B
dimension	B
vc	O
is	O
finite	O
this	O
is	O
the	O
case	O
for	O
example	O
for	O
exponential	B
families	O
if	O
pes	O
is	O
an	O
exponential	B
family	I
with	O
densities	O
of	O
the	O
form	O
jex	O
caejjxexp	O
parametric	B
classification	I
then	O
by	O
results	O
from	O
chapter	O
sc	O
n	O
observe	O
that	O
in	O
this	O
approach	O
nothing	O
but	O
properties	O
of	O
the	O
class	O
c	O
are	O
used	O
to	O
derive	O
the	O
a	O
jlog	O
n	O
in	O
rate	B
of	I
convergence	I
remark	O
robustness	O
the	O
method	O
of	O
empirical	B
minimization	O
is	O
clearly	O
extremely	O
robust	O
against	O
errors	O
in	O
the	O
parametric	O
model	O
obviously	O
theorem	O
if	O
the	O
true	O
conditional	O
distributions	O
are	O
not	O
contained	O
in	O
the	O
class	O
p	O
e	O
then	O
l	O
can	O
be	O
replaced	O
in	O
corollary	O
by	O
inf	O
p	O
y	O
if	O
the	O
model	O
is	O
fairly	O
good	O
then	O
this	O
number	O
should	O
be	O
very	O
close	O
to	O
the	O
bayes	O
risk	O
l	O
d	O
remark	O
lower	O
bounds	O
the	O
results	O
of	O
chapter	O
imply	O
that	O
for	O
some	O
butions	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
is	O
about	O
a	O
i	O
vfn	O
away	O
from	O
the	O
bayes	O
risk	O
in	O
the	O
parametric	O
case	O
however	O
since	O
the	O
class	O
of	O
distributions	O
is	O
restricted	O
by	O
the	O
parametric	O
model	O
this	O
is	O
not	O
necessarily	O
true	O
in	O
some	O
cases	O
a	O
much	O
faster	O
rate	B
of	I
convergence	I
is	O
possible	O
than	O
the	O
a	O
jlog	O
n	O
in	O
rate	O
guaranteed	O
by	O
corollary	O
see	O
for	O
example	O
problem	O
where	O
an	O
example	O
is	O
given	O
in	O
which	O
the	O
error	O
rate	O
is	O
d	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
if	O
both	O
conditional	O
distributions	O
of	O
x	O
given	O
y	O
and	O
y	O
are	O
gaussian	B
then	O
the	O
bayes	O
decision	O
is	O
quadratic	O
that	O
is	O
it	O
can	O
be	O
written	O
as	O
gx	O
atjix	O
ao	O
otherwise	O
where	O
the	O
functions	O
ljix	O
are	O
either	O
of	O
the	O
form	O
xi	O
i-th	O
component	O
of	O
the	O
vector	O
x	O
or	O
x	O
and	O
ao	O
e	O
r	O
problem	O
let	O
be	O
the	O
normal	B
density	O
on	O
the	O
real	O
line	O
with	O
mean	O
m	O
and	O
standard	B
deviation	O
and	O
we	O
draw	O
an	O
i	O
i	O
d	O
sample	O
x	O
i	O
xn	O
from	O
and	O
set	O
and	O
lxi	O
m	O
n	O
show	O
that	O
e	O
mil	O
in	O
and	O
e	O
cti	O
in	O
by	O
using	O
chebyshevs	O
inequality	B
show	O
that	O
this	O
rate	O
is	O
in	O
fact	O
tight	O
prove	O
also	O
that	O
the	O
result	O
remains	O
true	O
if	O
n	O
is	O
replaced	O
by	O
n	O
a	O
binomial	B
p	O
random	O
variable	B
independent	O
of	O
xi	O
x	O
n	O
where	O
p	O
e	O
that	O
is	O
m	O
becomes	O
n	O
xi	O
if	O
n	O
and	O
otherwise	O
and	O
similarly	O
for	O
ct	O
problem	O
assume	O
that	O
p	O
and	O
that	O
both	O
class-conditional	B
densities	O
and	O
are	O
gaussian	B
on	O
r	O
with	O
unit	O
variance	O
but	O
different	O
means	O
we	O
use	O
the	O
maximum	B
likelihood	I
estimates	O
mo	O
iii	O
of	O
the	O
conditional	O
means	O
mo	O
exiy	O
o	O
and	O
m	O
exiy	O
i	O
to	O
obtain	O
the	O
plug-in	O
classifier	B
ge	O
show	O
that	O
e	O
n	O
then	O
go	O
on	O
to	O
show	O
thate	O
l	O
oon	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
the	O
following	O
classes	O
of	O
densities	O
on	O
r	O
constitute	O
exponential	B
families	O
gaussian	B
family	O
gamma	B
family	O
beta	B
family	O
rayleigh	B
family	O
rea	O
x	O
tj-i	O
x	O
ixeo	O
i	O
a	O
ae-tj	O
e	O
problem	O
this	O
exercise	O
shows	O
that	O
one-parameter	O
classes	O
may	O
be	O
incredibly	O
rich	O
let	O
c	O
be	O
the	O
class	O
of	O
rules	O
of	O
the	O
form	O
gex	O
if	O
x	O
e	O
ae	O
if	O
x	O
ae	O
where	O
x	O
e	O
r	O
e	O
r	O
is	O
a	O
parameter	O
and	O
a	O
is	O
a	O
union	O
of	O
intervals	O
on	O
the	O
real	O
line	O
equivalently	O
ge	O
iae	O
let	O
a	O
u	O
i	O
where	O
b	O
i	O
are	O
bits	O
obtained	O
as	O
follows	O
first	O
list	O
all	O
sequences	O
of	O
length	O
l	O
then	O
those	O
of	O
length	O
et	O
cetera	O
so	O
that	O
i	O
is	O
a	O
concatenation	O
of	O
et	O
cetera	O
show	O
that	O
for	O
all	O
n	O
there	O
exists	O
a	O
set	O
x	O
n	O
c	O
r	O
that	O
can	O
be	O
shattered	O
by	O
a	O
set	O
from	O
e	O
e	O
en	O
conclude	O
that	O
the	O
vc	B
dimension	B
of	O
c	O
is	O
infinite	O
if	O
we	O
use	O
c	O
for	O
empirical	B
error	I
minimization	O
and	O
l	O
what	O
can	O
you	O
say	O
about	O
el	O
n	O
the	O
probability	O
of	O
error	O
of	O
the	O
selected	O
rule	B
problem	O
continuation	O
let	O
x	O
be	O
uniform	O
on	O
i	O
with	O
probability	O
i	O
set	O
y	O
bi	O
if	O
x	O
e	O
i	O
so	O
that	O
l	O
o	O
derive	O
the	O
class	O
of	O
bayes	O
rules	O
work	O
out	O
the	O
details	O
of	O
the	O
generalized	B
kolmogorov-smirnov	B
distance	I
minimizer	O
based	O
on	O
the	O
class	O
of	O
provide	O
the	O
best	O
upper	O
bound	O
on	O
el	O
n	O
you	O
can	O
get	O
with	O
any	O
method	O
regardless	O
of	O
discrimination	O
how	O
would	O
you	O
estimate	O
e	O
derive	O
the	O
asymptotic	O
behavior	O
of	O
e	O
ln	O
for	O
the	O
plug-in	O
rule	B
based	O
on	O
your	O
estimate	O
of	O
e	O
problem	O
if	O
fe	O
uniform	O
densities	O
on	O
rectangles	O
of	O
rd	O
and	O
if	O
we	O
use	O
the	O
maximum	B
likelihood	I
estimates	O
of	O
p	O
derived	O
in	O
the	O
text	O
show	O
that	O
e	O
ln	O
l	O
parametric	B
classification	I
problem	O
continued	O
assume	O
that	O
the	O
true	O
class-conditional	B
densities	O
are	O
fa	O
and	O
fa	O
ii	O
with	O
the	O
same	O
maximum	B
likelihood	I
estimates	O
given	O
above	O
find	O
fa	O
for	O
which	O
el	O
n	O
yet	O
l	O
o	O
problem	O
continued	O
show	O
that	O
the	O
rate	O
above	O
can	O
in	O
general	O
not	O
be	O
proved	O
problem	O
show	O
that	O
by	O
noting	O
that	O
the	O
supremum	O
in	O
the	O
definition	B
of	I
d	O
ks	O
may	O
be	O
replaced	O
by	O
a	O
maximum	O
over	O
nd	O
carefully	O
selected	O
points	O
of	O
nd	O
hint	O
use	O
the	O
idea	O
of	O
fingering	B
from	O
chapters	O
and	O
problem	O
prove	O
lemma	O
by	O
following	O
the	O
line	O
of	O
the	O
proof	O
of	O
the	O
chervonenkis	O
inequality	B
hint	O
in	O
the	O
second	O
symmetrization	B
step	O
observe	O
that	O
has	O
the	O
same	O
distribution	O
as	O
that	O
of	O
sup	O
t	O
ai	O
ixi	O
ea	O
i	O
aeb	O
n	O
il	O
problem	O
minimum	O
distance	O
density	B
estimation	B
letfe	O
e	O
e	O
be	O
a	O
parametric	O
class	O
of	O
densities	O
on	O
rd	O
and	O
assume	O
that	O
the	O
i	O
i	O
d	O
sample	O
xl	O
xn	O
is	O
drawn	O
from	O
the	O
density	O
fe	O
e	O
define	O
the	O
class	O
of	O
sets	O
and	O
define	O
the	O
minimum	O
distance	O
estimate	O
of	O
e	O
by	O
arg	O
min	O
d	O
ape	O
fin	O
eee	O
where	O
pe	O
is	O
the	O
distribution	O
belonging	O
to	O
the	O
density	O
fe	O
fin	O
is	O
the	O
empirical	B
distribution	O
defined	O
by	O
xl	O
x	O
n	O
and	O
dais	O
the	O
generalized	B
kolmogorov-smimov	O
distance	O
defined	O
by	O
aea	O
prove	O
that	O
if	O
a	O
has	O
finite	O
vc	B
dimension	B
then	O
dap	O
q	O
sup	O
ipa	O
qai	O
e	O
ifg-x	O
fexldx	O
for	O
more	O
information	O
on	O
minimum	O
distance	O
density	B
estimation	B
see	O
yatracos	O
and	O
devroye	O
hint	O
follow	O
the	O
steps	O
listed	O
below	O
f	O
ifg-	O
fe	O
i	O
pe	O
theorem	O
dapg-	O
pe	O
fin	O
as	O
in	O
the	O
text	O
finish	O
the	O
proof	O
by	O
applying	O
alexanders	O
improvement	O
of	O
the	O
kis	O
inequality	B
generalized	B
linear	O
discrimination	O
the	O
classifiers	O
we	O
study	O
here	O
have	O
their	O
roots	O
in	O
the	O
fourier	O
series	O
estimate	O
or	O
other	O
series	O
estimates	O
of	O
an	O
unknown	O
density	O
potential	O
function	O
methods	O
chapter	O
and	O
generalized	B
linear	O
classifiers	O
all	O
these	O
estimators	O
can	O
be	O
put	O
into	O
the	O
following	O
form	O
classify	O
x	O
as	O
belonging	O
to	O
class	O
if	O
k	O
l	O
anjojx	O
jl	O
where	O
the	O
js	O
are	O
fixed	O
functions	O
forming	O
a	O
base	O
for	O
the	O
series	O
estimate	O
anj	O
is	O
a	O
fixed	O
function	O
of	O
the	O
training	O
data	O
and	O
k	O
controls	O
the	O
amount	O
of	O
smoothing	O
when	O
the	O
os	O
are	O
the	O
usual	O
trigonometric	B
basis	O
then	O
this	O
leads	O
to	O
the	O
fourier	B
series	I
classifier	B
studied	O
by	O
greblicki	O
and	O
pawlak	O
when	O
the	O
os	O
form	O
an	O
orthonormal	O
system	O
based	O
upon	O
hermite	B
polynomials	O
we	O
obtain	O
the	O
classifiers	O
studied	O
by	O
greblicki	O
and	O
greblicki	O
and	O
pawlak	O
when	O
j	O
is	O
the	O
collection	O
of	O
all	O
products	O
of	O
components	O
of	O
x	O
as	O
etcetera	O
we	O
obtain	O
the	O
polynomial	B
method	O
of	O
specht	O
we	O
start	O
with	O
classification	O
based	O
on	O
fourier	O
series	O
expansion	O
which	O
has	O
its	O
origins	O
in	O
fourier	B
series	I
density	B
estimation	B
which	O
in	O
tum	O
was	O
developed	O
by	O
cencov	O
schwartz	O
kronmal	O
and	O
tarter	O
tarter	O
and	O
kronmal	O
and	O
specht	O
its	O
use	O
in	O
classification	O
was	O
considered	O
by	O
greblicki	O
specht	O
greblicki	O
and	O
pawlak	O
and	O
others	O
generalized	B
linear	O
discrimination	O
fourier	O
series	O
classification	O
let	O
f	O
be	O
the	O
density	O
of	O
x	O
and	O
assume	O
that	O
f	O
e	O
that	O
is	O
f	O
f	O
fxdx	O
and	O
f	O
and	O
recall	O
that	O
a	O
denotes	O
the	O
lebesgue	O
measure	O
on	O
nd	O
let	O
be	O
a	O
complete	B
orthonormal	I
set	O
of	O
bounded	O
functions	O
in	O
l	O
with	O
sup	O
jx	O
b	O
an	O
orthonormal	O
set	O
of	O
functions	O
in	O
l	O
is	O
such	O
that	O
f	O
ljjoi	O
iuj	O
for	O
all	O
i	O
j	O
it	O
is	O
complete	O
if	O
f	O
alji	O
for	O
all	O
i	O
for	O
some	O
function	O
a	O
e	O
l	O
implies	O
that	O
a	O
almost	O
everywhere	O
respect	O
to	O
a	O
if	O
f	O
e	O
l	O
then	O
the	O
class-conditional	B
densities	O
fo	O
and	O
fl	O
exist	O
and	O
are	O
in	O
then	O
the	O
function	O
ax	O
pfox	O
lfx	O
is	O
in	O
l	O
as	O
well	O
recall	O
that	O
the	O
bayes	O
decision	O
is	O
given	O
by	O
g	O
if	O
ax	O
otherwise	O
let	O
the	O
bounded	O
functions	O
ljl	O
form	O
a	O
complete	B
orthonormal	I
basis	O
and	O
let	O
the	O
fourier	O
representation	O
of	O
a	O
be	O
given	O
by	O
a	O
lajljj	O
jl	O
the	O
above	O
convergence	O
is	O
understood	O
in	O
l	O
that	O
is	O
the	O
infinite	O
sum	O
means	O
that	O
the	O
coefficients	O
are	O
given	O
by	O
we	O
approximate	O
ax	O
by	O
a	O
truncation	O
of	O
its	O
fourier	O
representation	O
to	O
finitely	O
many	O
terms	O
and	O
use	O
the	O
data	O
dn	O
to	O
estimate	O
the	O
coefficients	O
appearing	O
in	O
this	O
sum	O
formally	O
consider	O
the	O
classification	O
rule	B
where	O
the	O
estimates	O
anj	O
of	O
a	O
j	O
are	O
very	O
easy	O
to	O
compute	O
before	O
discussing	O
the	O
properties	O
of	O
these	O
rules	O
we	O
list	O
a	O
few	O
examples	O
of	O
complete	B
orthonormal	I
systems	O
on	O
the	O
real	O
line	O
some	O
of	O
these	O
systems	O
contain	O
functions	O
fourier	O
series	O
classification	O
on	O
a	O
bounded	O
interval	O
these	O
of	O
course	O
can	O
only	O
be	O
used	O
if	O
the	O
distribution	O
of	O
x	O
is	O
concentrated	O
on	O
an	O
interval	O
the	O
completeness	O
of	O
these	O
systems	O
may	O
typically	O
be	O
checked	O
via	O
the	O
stone-weierstrass	B
theorem	I
a	O
general	O
way	O
of	O
producing	O
complete	B
orthonormal	I
systems	O
on	O
n	O
d	O
is	O
taking	O
products	O
of	O
dimensional	O
functions	O
of	O
the	O
d	O
components	O
as	O
sketched	O
in	O
problem	O
for	O
more	O
information	O
on	O
orthogonal	O
series	O
we	O
refer	O
to	O
sansone	O
and	O
zygmund	O
the	O
standard	B
trigonometric	B
basis	O
on	O
the	O
interval	O
jr	O
is	O
formed	O
by	O
the	O
functions	O
v	O
cosix	O
r	O
vjr	O
r	O
i	O
vjr	O
sinix	O
this	O
is	O
a	O
complete	B
orthonormal	I
system	O
in	O
l	O
n	O
the	O
legendre	B
polynomials	O
form	O
a	O
complete	B
orthonormal	I
basis	O
over	O
the	O
terval	O
i	O
x	O
x	O
i	O
i	O
f	O
ii	O
d	O
i	O
ol	O
the	O
set	O
of	O
hermite	B
functions	O
is	O
complete	O
and	O
orthonormal	O
over	O
the	O
whole	O
real	O
line	O
functions	O
of	O
the	O
laguerre	B
basis	O
are	O
defined	O
on	O
by	O
rei	O
oix	O
rz	O
d	O
i	O
x	O
x	O
e	O
z	O
dx	O
iex	O
e	O
where	O
a	O
is	O
a	O
parameter	O
of	O
the	O
system	O
a	O
complete	B
orthonormal	I
system	O
over	O
the	O
whole	O
real	O
line	O
may	O
be	O
obtained	O
by	O
the	O
haar	B
basis	O
contains	O
functions	O
on	O
that	O
take	O
three	O
different	O
values	O
it	O
is	O
orthonormal	O
and	O
complete	O
functions	O
with	O
finitely	O
many	O
values	O
have	O
computational	O
advantages	O
in	O
pattern	O
recognition	O
write	O
the	O
integer	O
i	O
as	O
i	O
j	O
where	O
k	O
i	O
j	O
j	O
then	O
oix	O
if	O
x	O
e	O
if	O
x	O
e	O
otherwise	O
generalized	B
linear	O
discrimination	O
functions	O
on	O
of	O
the	O
rademacher	B
basis	O
take	O
only	O
two	O
values	O
and	O
it	O
is	O
an	O
orthonormal	O
system	O
but	O
it	O
is	O
not	O
complete	O
lixj	O
i	O
the	O
basis	O
may	O
be	O
completed	O
as	O
follows	O
write	O
i	O
such	O
that	O
il	O
ik	O
this	O
form	O
is	O
unique	O
define	O
where	O
the	O
j	O
are	O
the	O
rademacher	B
functions	O
the	O
resulting	O
basis	O
is	O
orthonormal	O
and	O
complete	O
and	O
is	O
called	O
the	O
walsh	B
basis	O
there	O
is	O
no	O
system	O
of	O
basis	O
functions	O
that	O
is	O
better	O
than	O
another	O
for	O
all	O
tions	O
in	O
selecting	O
the	O
basis	O
of	O
a	O
fourier	O
series	O
rule	B
the	O
designer	O
must	O
use	O
a	O
mixture	O
of	O
intuition	O
error	B
estimation	B
and	O
computational	O
concerns	O
we	O
have	O
the	O
following	O
consistency	B
theorem	O
theorem	O
let	O
be	O
a	O
complete	B
orthonormal	I
basis	O
on	O
rd	O
such	O
that	O
for	O
some	O
b	O
b	O
for	O
each	O
i	O
and	O
x	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
and	O
fl	O
exist	O
and	O
are	O
in	O
if	O
kn	O
and	O
as	O
n	O
kn	O
n	O
then	O
the	O
fourier	O
series	O
classification	O
rule	B
defined	O
in	O
is	O
consistent	O
lim	O
elgn	O
l	O
n---oo	O
if	O
then	O
kn	O
and	O
as	O
n	O
kn	O
logn	O
n	O
lim	O
lgn	O
l	O
n---oo	O
with	O
probability	O
one	O
that	O
is	O
the	O
rule	B
is	O
strongly	O
consistent	O
proof	O
first	O
observe	O
that	O
the	O
an	O
are	O
unbiased	O
estimates	O
of	O
the	O
a	O
j	O
eanj	O
e	O
e	O
e	O
lix	O
e	O
j	O
and	O
that	O
their	O
variance	O
can	O
be	O
bounded	O
from	O
above	O
as	O
follows	O
fourier	O
series	O
classification	O
n	O
fxdx	O
ct	O
n	O
j	O
n	O
ct	O
n	O
where	O
we	O
used	O
the	O
boundedness	O
of	O
the	O
ljs	O
by	O
parsevals	O
identity	O
therefore	O
exploiting	O
orthonormality	O
of	O
the	O
j	O
we	O
have	O
j	O
t	O
a	O
dx	O
j	O
a	O
j	O
dx	O
thus	O
the	O
expected	O
l	O
is	O
bounded	O
as	O
follows	O
k	O
lvarctnj	O
l	O
jl	O
jknl	O
ctj	O
generalized	B
linear	O
discrimination	O
since	O
ll	O
a	O
the	O
second	O
term	O
tends	O
to	O
zero	O
if	O
kn	O
if	O
at	O
the	O
same	O
time	O
kn	O
n	O
then	O
the	O
expected	O
converges	O
to	O
zero	O
that	O
is	O
the	O
estimate	O
is	O
consistent	O
in	O
l	O
now	O
convergence	O
of	O
the	O
error	O
probability	O
follows	O
from	O
problem	O
to	O
prove	O
strong	B
consistency	B
convergence	O
with	O
probability	O
one	O
fix	O
e	O
and	O
assume	O
that	O
n	O
is	O
so	O
large	O
that	O
l	O
a	O
jknl	O
then	O
kn	O
l	O
p	O
jl	O
the	O
union	O
bound	O
where	O
we	O
used	O
hoeffdings	O
inequality	B
because	O
k	O
n	O
log	O
n	O
n	O
the	O
upper	O
bound	O
is	O
eventually	O
smaller	O
than	O
which	O
is	O
summable	O
the	O
borel-cantelli	B
lemma	I
yields	O
strong	B
consistency	B
strong	B
consistency	B
of	O
the	O
classifier	B
then	O
follows	O
from	O
problem	O
remark	O
it	O
is	O
clear	O
from	O
the	O
inequality	B
that	O
the	O
rate	B
of	I
convergence	I
is	O
determined	O
by	O
the	O
choice	O
of	O
kn	O
if	O
kn	O
is	O
small	O
then	O
the	O
first	O
term	O
which	O
corresponds	O
to	O
the	O
estimation	B
error	I
is	O
small	O
but	O
the	O
approximation	B
error	I
expressed	O
by	O
the	O
second	O
term	O
is	O
larger	O
for	O
large	O
kn	O
the	O
generalized	B
linear	O
classification	O
situation	O
is	O
reversed	O
the	O
optimal	O
choice	O
of	O
kn	O
depends	O
on	O
how	O
fast	O
the	O
second	O
term	O
goes	O
to	O
zero	O
as	O
kn	O
grows	O
this	O
depends	O
on	O
other	O
properties	O
of	O
f	O
such	O
as	O
the	O
size	O
of	O
its	O
tail	O
and	O
its	O
smoothness	O
remark	O
consistency	B
in	O
theorem	O
is	O
not	O
universal	O
since	O
we	O
needed	O
to	O
sume	O
the	O
existence	O
of	O
square	O
integrable	O
conditional	O
densities	O
this	O
however	O
is	O
not	O
a	O
restrictive	O
assumption	O
in	O
some	O
practical	O
situations	O
for	O
example	O
if	O
the	O
tions	O
are	O
corrupted	O
by	O
additive	O
gaussian	B
noise	I
then	O
the	O
conditions	O
of	O
the	O
theorem	O
hold	O
however	O
if	O
does	O
not	O
have	O
a	O
density	O
the	O
method	O
may	O
be	O
inconsistent	O
problem	O
fourier	O
series	O
classifiers	O
have	O
two	O
rather	O
unattractive	O
features	O
in	O
general	O
they	O
are	O
not	O
invariant	O
under	O
translations	O
of	O
the	O
coordinate	O
space	O
most	O
series	O
classifiers	O
are	O
not	O
local	O
in	O
nature-points	O
at	O
arbitrary	O
distances	O
from	O
x	O
affect	O
the	O
decision	O
at	O
x	O
in	O
kernel	B
and	O
nearest	B
neighbor	I
rules	O
the	O
locality	O
is	O
easily	O
controlled	O
generalized	B
linear	O
classification	O
when	O
x	O
is	O
purely	O
atomic	O
or	O
singular	O
continuous	O
theorem	O
is	O
not	O
applicable	O
a	O
theme	O
of	O
this	O
book	O
is	O
that	O
pattern	O
recognition	O
can	O
be	O
developed	O
in	O
a	O
free	O
manner	O
since	O
after	O
all	O
the	O
distribution	O
of	O
y	O
is	O
not	O
known	O
besides	O
even	O
if	O
we	O
had	O
an	O
i	O
i	O
d	O
sample	O
yn	O
at	O
our	O
disposal	O
we	O
do	O
not	O
know	O
of	O
any	O
test	O
for	O
verifying	O
whether	O
x	O
has	O
a	O
square	O
integrable	O
density	O
so	O
we	O
proceed	O
a	O
bit	O
differently	O
to	O
develop	O
universally	O
consistent	O
rules	O
to	O
generalize	O
fourier	O
series	O
classifiers	O
let	O
be	O
bounded	O
functions	O
on	O
nd	O
these	O
functions	O
do	O
not	O
necessarily	O
form	O
an	O
orthonormal	O
basis	O
of	O
consider	O
the	O
classifier	B
where	O
the	O
coefficients	O
an	O
are	O
some	O
functions	O
of	O
the	O
data	O
dn	O
this	O
may	O
be	O
viewed	O
as	O
a	O
generalization	O
of	O
fourier	O
series	O
rules	O
where	O
the	O
coefficients	O
were	O
unbiased	O
estimates	O
ofthe	O
fourier	O
coefficients	O
of	O
ax	O
here	O
we	O
will	O
sider	O
some	O
other	O
choices	O
of	O
the	O
coefficients	O
observe	O
that	O
gn	O
is	O
a	O
generalized	B
linear	B
classifier	B
as	O
defined	O
in	O
chapter	O
an	O
intuitively	O
appealing	O
way	O
to	O
determine	O
the	O
coefficients	O
is	O
to	O
pick	O
them	O
to	O
minimize	O
the	O
empirical	B
error	I
committed	O
on	O
dn	O
as	O
in	O
empirical	B
risk	I
minimization	I
the	O
critical	O
parameter	O
here	O
is	O
kn	O
the	O
number	O
of	O
basis	O
functions	O
used	O
in	O
the	O
linear	O
combination	O
if	O
we	O
keep	O
k	O
n	O
fixed	O
then	O
as	O
we	O
saw	O
in	O
chapter	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
converges	O
quickly	O
to	O
that	O
of	O
the	O
best	O
classifier	B
of	O
the	O
above	O
form	O
however	O
for	O
some	O
distributions	O
this	O
infimum	O
may	O
be	O
far	O
from	O
the	O
bayes	O
risk	O
so	O
it	O
is	O
useful	O
to	O
let	O
kn	O
grow	O
as	O
n	O
becomes	O
generalized	B
linear	O
discrimination	O
larger	O
however	O
choosing	O
kn	O
too	O
large	O
may	O
result	O
in	O
overfitting	B
the	O
data	O
using	O
the	O
terminology	O
introduced	O
in	O
chapter	O
let	O
ckn	O
be	O
the	O
class	O
of	O
classifiers	O
of	O
the	O
form	O
where	O
the	O
a	O
j	O
range	O
through	O
n	O
choose	O
the	O
coefficients	O
to	O
minimize	O
the	O
empirical	B
error	I
let	O
gn	O
be	O
the	O
corresponding	O
classifier	B
we	O
recall	O
from	O
chapter	O
that	O
the	O
is	O
not	O
more	O
than	O
kn	O
therefore	O
by	O
theorem	O
for	O
every	O
vc	B
dimension	B
of	O
ckll	O
n	O
where	O
h	O
is	O
the	O
binary	B
entropy	B
function	O
the	O
right-hand	O
side	O
is	O
if	O
kn	O
on	O
however	O
to	O
obtain	O
consistency	B
we	O
need	O
to	O
know	O
how	O
close	O
infecknj	O
l	O
is	O
to	O
l	O
this	O
obviously	O
depends	O
on	O
the	O
choice	O
of	O
the	O
s	O
as	O
well	O
as	O
on	O
the	O
distribution	O
if	O
kn	O
is	O
not	O
allowed	O
to	O
grow	O
with	O
n	O
and	O
is	O
bounded	O
by	O
a	O
number	O
k	O
then	O
it	O
follows	O
from	O
theorem	O
that	O
for	O
every	O
b	O
universal	B
consistency	B
eludes	O
us	O
as	O
for	O
some	O
distribution	O
infeck	O
l	O
l	O
inf	O
l	O
l	O
inf	O
al	O
p	O
dx	O
jl	O
p	O
dx	O
the	O
limit	O
supremum	O
of	O
the	O
right-hand	O
side	O
can	O
be	O
arbitrarily	O
close	O
to	O
zero	O
for	O
all	O
distributions	O
if	O
kn	O
as	O
n	O
and	O
the	O
set	O
of	O
functions	O
is	O
dense	O
in	O
ll	O
on	O
balls	O
of	O
the	O
form	O
ilx	O
ii	O
b	O
for	O
all	O
p	O
this	O
means	O
that	O
given	O
any	O
probability	O
measure	O
p	O
and	O
function	O
f	O
with	O
j	O
ifldp	O
for	O
every	O
e	O
there	O
exists	O
an	O
integer	O
k	O
and	O
coefficients	O
ai	O
ak	O
such	O
that	O
thus	O
we	O
have	O
obtained	O
the	O
following	O
consistency	B
result	O
problems	O
and	O
exercises	O
theorem	O
let	O
be	O
a	O
sequence	O
of	O
functions	O
such	O
that	O
the	O
set	O
of	O
all	O
finite	O
linear	O
combinations	O
of	O
the	O
j	O
is	O
dense	O
in	O
l	O
on	O
balls	O
of	O
the	O
form	O
ilx	O
ii	O
s	O
b	O
for	O
any	O
probability	O
measure	O
jl	O
then	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
when	O
kn	O
and	O
as	O
n	O
kn	O
n	O
remark	O
to	O
see	O
that	O
the	O
statement	O
of	O
theorem	O
is	O
not	O
vacuous	O
note	O
that	O
ness	O
in	O
l	O
on	O
balls	O
follows	O
from	O
denseness	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
on	O
balls	O
denseness	O
in	O
the	O
loo	O
sense	O
may	O
be	O
checked	O
by	O
the	O
stone-weierstrass	O
orem	O
for	O
example	O
the	O
class	O
of	O
all	O
polynomial	B
classifiers	O
satisfies	O
the	O
conditions	O
of	O
the	O
theorem	O
this	O
class	O
can	O
be	O
obtained	O
by	O
choosing	O
the	O
tions	O
as	O
the	O
simple	O
polynomials	O
xl	O
x	O
cd	O
xci	O
similarly	O
the	O
functions	O
oj	O
can	O
be	O
chosen	O
as	O
bases	O
of	O
a	O
trigonometric	B
series	O
remark	O
the	O
histogram	B
rule	B
it	O
is	O
clear	O
that	O
theorem	O
can	O
be	O
modified	O
in	O
the	O
following	O
way	O
let	O
jb	O
j	O
k	O
k	O
be	O
functions	O
such	O
that	O
for	O
every	O
fell	O
f-l	O
concentrated	O
on	O
a	O
bounded	O
ball	O
and	O
e	O
there	O
is	O
a	O
ko	O
such	O
that	O
for	O
all	O
k	O
ko	O
there	O
is	O
a	O
function	O
of	O
the	O
form	O
ll	O
a	O
j	O
jk	O
whose	O
distance	O
from	O
f	O
in	O
l	O
is	O
smaller	O
than	O
e	O
let	O
be	O
the	O
class	O
of	O
generalized	B
linear	O
classifiers	O
based	O
on	O
the	O
functions	O
olkn	O
okkn	O
if	O
k	O
n	O
and	O
kn	O
n	O
then	O
the	O
classifier	B
gn	O
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
ckn	O
is	O
strongly	O
universally	O
consistent	O
this	O
modification	O
has	O
an	O
interesting	O
implication	O
consider	O
for	O
example	O
functions	O
olkn	O
ok	O
that	O
are	O
indicators	O
of	O
sets	O
of	O
a	O
partition	B
of	O
nd	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
classifier	B
that	O
minimizes	O
the	O
empirical	B
error	I
is	O
just	O
the	O
histogram	O
classifier	B
based	O
on	O
the	O
same	O
partition	B
the	O
denseness	O
assumption	O
requires	O
that	O
the	O
partition	B
becomes	O
infinitesimally	O
fine	O
as	O
n	O
in	O
fact	O
we	O
have	O
obtained	O
an	O
alternative	O
proof	O
for	O
the	O
strong	B
universal	B
consistency	B
of	O
the	O
regular	B
histogram	B
rule	B
the	O
details	O
are	O
left	O
as	O
an	O
exercise	O
problem	O
let	O
yrl	O
be	O
a	O
sequence	O
of	O
real-valued	O
functions	O
on	O
a	O
bounded	O
problems	O
and	O
exercises	O
terval	O
b	O
such	O
that	O
f	O
yrixyrxdx	O
ii	O
and	O
the	O
set	O
of	O
finite	O
linear	O
combinations	O
ai	O
yri	O
is	O
dense	O
in	O
the	O
class	O
of	O
continuous	O
functions	O
on	O
b	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
define	O
the	O
functions	O
bd	O
n	O
by	O
show	O
that	O
these	O
functions	O
form	O
a	O
complete	B
orthonormal	I
set	O
of	O
functions	O
on	O
bd	O
problem	O
let	O
z	O
be	O
an	O
arbitrary	O
random	O
variable	B
on	O
n	O
and	O
v	O
be	O
a	O
real	O
random	O
variable	B
independent	O
of	O
z	O
that	O
has	O
a	O
density	O
h	O
e	O
show	O
that	O
the	O
density	O
f	O
of	O
the	O
generalized	B
linear	O
discrimination	O
random	O
variable	B
x	O
z	O
v	O
exists	O
and	O
f	O
hint	O
use	O
jensens	O
inequality	B
to	O
prove	O
that	O
f	O
f	O
problem	O
derive	O
the	O
strong	B
universal	B
consistency	B
of	O
the	O
regular	B
histogram	B
rule	B
from	O
theorem	O
as	O
indicated	O
in	O
the	O
remark	O
following	O
it	O
problem	O
let	O
be	O
the	O
standard	B
trigonometric	B
basis	O
and	O
consider	O
the	O
classification	O
rule	B
defined	O
in	O
show	O
that	O
the	O
rule	B
is	O
not	O
consistent	O
for	O
the	O
following	O
distribution	O
given	O
y	O
l	O
let	O
x	O
be	O
and	O
given	O
y	O
let	O
x	O
be	O
uniformly	O
distributed	O
on	O
jt	O
assume	O
furthermore	O
that	O
pry	O
problem	O
the	O
haar	B
basis	O
is	O
not	O
bounded	O
determine	O
whether	O
or	O
not	O
the	O
laguerre	B
hermite	B
and	O
legendre	B
bases	O
are	O
bounded	O
complexity	B
regularization	I
this	O
chapter	O
offers	O
key	O
theoretical	B
results	O
that	O
confirm	O
the	O
existence	O
of	O
certain	O
rules	O
although	O
the	O
proofs	O
are	O
constructive-we	O
do	O
tell	O
you	O
how	O
you	O
may	O
design	O
such	O
rules-the	O
computational	O
requirements	O
are	O
often	O
prohibitive	O
many	O
of	O
these	O
rules	O
are	O
thus	O
not	O
likely	O
to	O
filter	O
down	O
to	O
the	O
software	O
packages	O
and	O
pattern	O
recognition	O
implementations	O
an	O
attempt	O
at	O
reducing	O
the	O
computational	O
ity	O
somewhat	O
is	O
described	O
in	O
the	O
section	O
entitled	O
empirical	B
covering	O
nevertheless	O
we	O
feel	O
that	O
much	O
more	O
serious	O
work	O
on	O
discovering	O
practical	O
rithms	O
for	O
empirical	B
risk	I
minimization	I
is	O
sorely	O
needed	O
in	O
chapter	O
the	O
empirical	B
error	I
probability	O
was	O
minimized	O
over	O
a	O
class	O
e	O
of	O
decision	O
rules	O
n	O
d	O
i	O
we	O
saw	O
that	O
if	O
the	O
vc	B
dimension	B
vc	O
of	O
the	O
class	O
is	O
finite	O
then	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
is	O
within	O
constant	O
times	O
log	O
n	O
n	O
of	O
that	O
of	O
the	O
best	O
rule	B
in	O
e	O
unfortunately	O
classes	O
with	O
finite	O
vc	B
dimension	B
are	O
nearly	O
always	O
too	O
small	O
and	O
thus	O
the	O
error	O
probability	O
of	O
the	O
best	O
rule	B
in	O
e	O
is	O
typically	O
far	O
from	O
the	O
bayes	O
risk	O
l	O
for	O
some	O
distribution	O
theorem	O
in	O
chapter	O
we	O
investigated	O
the	O
special	O
classes	O
of	B
generalized	B
linear	I
rules	I
theorem	O
for	O
example	O
shows	O
that	O
if	O
we	O
increase	O
the	O
size	O
of	O
the	O
class	O
in	O
a	O
controlled	O
fashion	O
as	O
the	O
sample	O
size	O
n	O
increases	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
approaches	O
l	O
for	O
any	O
distribution	O
thus	O
vc	O
increases	O
with	O
n	O
theorem	O
may	O
be	O
generalized	B
straightforwardly	O
for	O
other	O
types	O
of	O
classifiers	O
consider	O
a	O
sequence	O
of	O
classes	O
el	O
containing	O
classifiers	O
of	O
the	O
form	O
n	O
d	O
i	O
the	O
training	O
data	O
dn	O
yd	O
yn	O
are	O
used	O
to	O
select	O
a	O
classifier	B
by	O
minimizing	O
the	O
empirical	B
error	I
probability	O
ln	O
over	O
the	O
class	O
ekn	O
where	O
the	O
integer	O
kn	O
depends	O
on	O
n	O
in	O
a	O
specified	O
way	O
the	O
following	O
generalization	O
is	O
based	O
on	O
the	O
proof	O
of	O
theorem	O
problem	O
complexity	B
regularization	I
theorem	O
assume	O
thatel	O
is	O
a	O
sequence	O
of	O
classes	O
of	O
decision	O
rules	O
such	O
that	O
for	O
any	O
distribution	O
of	O
y	O
lim	O
inf	O
l	O
l	O
i--oo	O
and	O
that	O
the	O
vc	O
dimensions	O
vcc	O
are	O
all	O
finite	O
if	O
kn	O
and	O
vckn	O
log	O
n	O
as	O
n	O
n	O
then	O
the	O
classifier	B
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
the	O
class	O
e	O
ckn	O
is	O
strongly	O
universally	O
consistent	O
theorem	O
is	O
the	O
missing	O
link-we	O
are	O
now	O
ready	O
to	O
apply	O
the	O
rich	O
theory	O
of	O
vapnik	O
classes	O
in	O
the	O
construction	O
of	O
universally	O
consistent	O
rules	O
the	O
theorem	O
does	O
not	O
help	O
us	O
however	O
with	O
the	O
choice	O
of	O
the	O
classes	O
eco	O
or	O
the	O
choice	O
of	O
the	O
sequence	O
examples	O
for	O
sequences	O
of	O
classes	O
satisfying	O
the	O
condition	O
of	O
theorem	O
include	O
generalized	B
linear	O
decisions	O
with	O
properly	O
chosen	O
basis	O
functions	O
or	O
neural	O
networks	O
a	O
word	O
of	O
warning	O
here	O
the	O
universally	O
consistent	O
rules	O
obtained	O
via	O
theorem	O
may	O
come	O
at	O
a	O
tremendous	O
computational	O
price	O
as	O
we	O
will	O
see	O
further	O
on	O
we	O
will	O
often	O
have	O
exponential	B
complexities	O
in	O
n	O
instead	O
of	O
polynomial	B
time	O
that	O
would	O
be	O
obtained	O
if	O
we	O
just	O
minimized	O
the	O
empirical	B
risk	O
over	O
a	O
fixed	O
vc	B
class	I
the	O
computational	O
complexity	O
of	O
these	O
rules	O
are	O
often	O
incomparably	O
larger	O
than	O
that	O
of	O
some	O
simple	O
universally	O
consistent	O
rules	O
as	O
k-nearest	O
neighbor	O
kernel	B
or	O
partitioning	O
rules	O
structural	O
risk	O
minimization	O
let	O
ecl	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
classifiers	O
from	O
which	O
we	O
wish	O
to	O
select	O
a	O
sequence	O
of	O
classifiers	O
with	O
the	O
help	O
of	O
the	O
training	O
data	O
dn	O
previously	O
we	O
picked	O
l	O
from	O
the	O
kn	O
class	O
eckn	O
where	O
the	O
integer	O
kn	O
is	O
some	O
prespecified	O
tion	O
of	O
the	O
sample	O
size	O
n	O
only	O
the	O
integer	O
kn	O
basically	O
determines	O
the	O
complexity	O
of	O
the	O
class	O
from	O
which	O
the	O
decision	O
rule	B
is	O
selected	O
theorem	O
shows	O
that	O
under	O
mild	O
conditions	O
on	O
the	O
sequence	O
of	O
classes	O
it	O
is	O
possible	O
to	O
find	O
sequences	O
such	O
that	O
the	O
rule	B
is	O
universally	O
consistent	O
typically	O
kn	O
should	O
grow	O
with	O
n	O
in	O
order	O
to	O
assure	O
convergence	O
of	O
the	O
approximation	B
error	I
infjjecknj	O
l	O
l	O
but	O
it	O
cannot	O
grow	O
too	O
rapidly	O
for	O
otherwise	O
the	O
estimation	B
error	I
l	O
inf	O
j	O
l	O
might	O
fail	O
to	O
converge	O
to	O
zero	O
ideally	O
to	O
get	O
best	O
performance	O
the	O
two	O
types	O
of	O
error	O
should	O
be	O
about	O
the	O
same	O
order	O
of	O
magnitude	O
clearly	O
a	O
pre	O
specified	O
choice	O
of	O
the	O
complexity	O
kn	O
cannot	O
balance	O
the	O
two	O
sides	O
of	O
the	O
trade-off	O
for	O
all	O
tions	O
therefore	O
it	O
is	O
important	O
to	O
find	O
methods	O
such	O
that	O
the	O
classifier	B
is	O
selected	O
from	O
a	O
class	O
whose	O
index	O
is	O
automatically	O
determined	O
by	O
the	O
data	O
dn	O
this	O
section	O
deals	O
with	O
such	O
methods	O
structural	O
risk	O
minimization	O
the	O
most	O
obvious	O
method	O
would	O
be	O
based	O
on	O
selecting	O
a	O
candidate	O
decision	O
rule	B
n	O
from	O
every	O
class	O
cj	O
example	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
cj	O
and	O
then	O
minimizing	O
the	O
empirical	B
error	I
over	O
these	O
rules	O
however	O
typically	O
the	O
vc	B
dimension	B
of	O
c	O
ucj	O
equals	O
infinity	O
which	O
in	O
view	O
of	O
results	O
in	O
chapter	O
implies	O
that	O
this	O
approach	O
will	O
not	O
work	O
in	O
fact	O
in	O
order	O
to	O
guarantee	O
it	O
is	O
necessary	O
that	O
inf	O
inf	O
l	O
l	O
vc	O
theorems	O
and	O
a	O
possible	O
solution	O
for	O
the	O
problem	O
is	O
a	O
method	O
proposed	O
by	O
vapnik	O
and	O
vonenkis	O
and	O
vapnik	O
called	O
structural	O
risk	O
minimization	O
first	O
we	O
select	O
a	O
classifier	B
from	O
every	O
class	O
cj	O
which	O
minimizes	O
the	O
empirical	B
error	I
over	O
the	O
class	O
then	O
we	O
know	O
from	O
theorem	O
that	O
for	O
every	O
j	O
with	O
very	O
large	O
probability	O
vcuogn	O
now	O
pick	O
a	O
classifier	B
that	O
minimizes	O
the	O
upper	O
bound	O
over	O
j	O
to	O
make	O
things	O
more	O
precise	O
for	O
every	O
nand	O
j	O
we	O
introduce	O
the	O
complexity	B
penalty	I
rj	O
n	O
n	O
vcj	O
logen	O
let	O
nl	O
be	O
classifiers	O
minimizing	O
the	O
empirical	B
error	I
ln	O
over	O
the	O
classes	O
cl	O
respectively	O
for	O
e	O
cj	O
define	O
the	O
complexity-penalized	O
error	O
estimate	O
ln	O
ln	O
rj	O
n	O
finally	O
select	O
the	O
classifier	B
i	O
minimizing	O
the	O
complexity	B
penalized	I
error	O
estimate	O
in	O
n	O
over	O
j	O
the	O
nexttheorem	O
states	O
thatthis	O
method	O
avoids	O
overfitting	B
the	O
data	O
the	O
only	O
condition	O
is	O
that	O
each	O
class	O
in	O
the	O
sequence	O
has	O
finite	O
vc	B
dimension	B
theorem	O
let	O
cl	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
classifiers	O
such	O
that	O
for	O
any	O
distribution	O
of	O
y	O
lim	O
inf	O
l	O
l	O
assume	O
also	O
that	O
the	O
vc	O
dimensions	O
vc	O
are	O
finite	O
and	O
satisfy	O
l	O
e-vcj	O
regularization	O
then	O
the	O
classification	O
rule	B
cp	O
based	O
on	O
structural	O
risk	O
minimization	O
as	O
defined	O
above	O
is	O
strongly	O
universally	O
consistent	O
remark	O
note	O
that	O
the	O
condition	O
on	O
the	O
vc	O
dimensions	O
is	O
satisfied	O
if	O
we	O
insist	O
that	O
vcji	O
vcj	O
for	O
all	O
j	O
a	O
natural	O
assumption	O
see	O
also	O
problem	O
n	O
instead	O
of	O
minimizing	O
the	O
empirical	B
error	I
ln	O
over	O
the	O
set	O
of	O
candidates	O
c	O
the	O
method	O
of	O
structural	O
risk	O
minimization	O
minimizes	O
in	O
the	O
sum	O
of	O
the	O
empirical	B
error	I
and	O
a	O
term	O
vcj	O
logen	O
which	O
increases	O
as	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
cj	O
containing	O
cp	O
increases	O
since	O
classes	O
with	O
larger	O
vc	B
dimension	B
can	O
be	O
considered	O
as	O
more	O
complex	O
than	O
those	O
with	O
smaller	O
vc	B
dimension	B
the	O
term	O
added	O
to	O
the	O
empirical	B
error	I
may	O
be	O
considered	O
as	O
a	O
penalty	O
for	O
complexity	O
the	O
idea	O
of	O
minimizing	O
the	O
sum	O
of	O
the	O
empirical	B
error	I
and	O
a	O
term	O
penalizing	O
the	O
complexity	O
has	O
been	O
investigated	O
in	O
various	O
statistical	O
problems	O
by	O
for	O
example	O
rissanen	O
akaike	O
barron	O
barron	O
and	O
cover	O
and	O
lugosi	O
and	O
zeger	O
barron	O
minimizes	O
the	O
penalized	O
empirical	B
risk	O
over	O
a	O
suitably	O
chosen	O
countably	O
infinite	O
list	O
of	O
candidates	O
this	O
approach	O
is	O
close	O
in	O
spirit	O
to	O
the	O
skeleton	O
estimates	O
discussed	O
in	O
chapter	O
he	O
makes	O
the	O
connection	O
between	O
structural	O
risk	O
minimization	O
and	O
the	O
minimum	B
description	I
length	I
principle	I
and	O
obtains	O
results	O
similar	O
to	O
those	O
discussed	O
in	O
this	O
section	O
the	O
theorems	O
presented	O
here	O
were	O
essentially	O
developed	O
in	O
lugosi	O
and	O
zeger	O
proof	O
of	O
theorem	O
we	O
show	O
that	O
both	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
following	O
decomposition	O
converge	O
to	O
zero	O
with	O
probability	O
one	O
for	O
the	O
first	O
term	O
we	O
have	O
p	O
z	O
i	O
pj	O
e	O
p	O
lncp	O
e	O
p	O
nj	O
in	O
ll	O
j	O
e	O
l	O
p	O
nj	O
ln	O
e	O
rj	O
n	O
l	O
vcj	O
theorem	O
jl	O
jl	O
structural	O
risk	O
minimization	O
l	O
vcj	O
jl	O
l	O
e	O
f	O
e	O
jl	O
using	O
the	O
defining	O
expression	B
for	O
rj	O
n	O
where	O
f	O
ll	O
e-	O
vcj	O
by	O
assumption	O
thus	O
it	O
follows	O
from	O
the	O
borel-cantelli	B
lemma	I
that	O
l	O
in	O
nj	O
j	O
with	O
probability	O
one	O
as	O
n	O
now	O
we	O
can	O
tum	O
to	O
investigating	O
the	O
second	O
term	O
inffl	O
inc	O
nj	O
l	O
by	O
our	O
assumptions	O
for	O
every	O
e	O
there	O
exists	O
an	O
integer	O
k	O
such	O
that	O
inf	O
lc	O
l	O
e	O
fix	O
such	O
a	O
k	O
thus	O
it	O
suffices	O
to	O
prove	O
that	O
lim	O
sup	O
inf	O
inc	O
nj	O
n-oo	O
j	O
l	O
inf	O
lc	O
with	O
probability	O
one	O
clearly	O
for	O
any	O
fixed	O
k	O
if	O
n	O
is	O
large	O
enough	O
then	O
rck	O
n	O
e	O
vck	O
logc	O
en	O
thus	O
for	O
such	O
large	O
n	O
p	O
inc	O
nj	O
j	O
i	O
inf	O
lc	O
e	O
inf	O
lc	O
e	O
p	O
nk	O
p	O
n	O
c	O
n	O
k	O
r	O
c	O
k	O
n	O
inf	O
l	O
c	O
e	O
p	O
nk	O
inf	O
lc	O
p	O
sup	O
iln	O
vck	O
e-	O
by	O
theorem	O
therefore	O
since	O
vck	O
the	O
proof	O
is	O
completed	O
theorem	O
shows	O
that	O
the	O
method	O
of	O
structural	O
risk	O
minimization	O
is	O
sally	O
consistent	O
under	O
very	O
mild	O
conditions	O
on	O
the	O
sequence	O
of	O
classes	O
el	O
complexity	B
regularization	I
this	O
property	O
however	O
is	O
shared	O
with	O
the	O
minimizer	O
of	O
the	O
empirical	B
error	I
over	O
the	O
where	O
kn	O
is	O
a	O
properly	O
chosen	O
function	O
of	O
the	O
sample	O
size	O
n	O
class	O
ckn	O
the	O
real	O
strength	O
then	O
of	O
structural	O
risk	O
minimization	O
is	O
seen	O
from	O
the	O
next	O
result	O
theorem	O
let	O
cl	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
classifiers	O
such	O
that	O
the	O
vc	O
dimensions	O
vcl	O
v	O
are	O
all	O
finite	O
assume	O
further	O
that	O
the	O
bayes	O
rule	B
g	O
e	O
c	O
ucj	O
jl	O
that	O
is	O
a	O
bayes	O
rule	B
is	O
contained	O
in	O
one	O
of	O
the	O
classes	O
let	O
k	O
be	O
the	O
smallest	O
integer	O
such	O
that	O
g	O
e	O
ck	O
thenfor	O
every	O
nand	O
e	O
satisfying	O
the	O
error	O
probability	O
of	O
the	O
classification	O
rule	B
based	O
on	O
structural	O
risk	O
minimization	O
satisfies	O
proof	O
theorem	O
follows	O
by	O
examining	O
the	O
proof	O
of	O
theorem	O
the	O
only	O
difference	O
is	O
that	O
by	O
assumption	O
there	O
is	O
an	O
integer	O
k	O
such	O
that	O
inf	O
rjjeck	O
l	O
l	O
therefore	O
for	O
this	O
k	O
l	O
l	O
in	O
nj	O
in	O
nj	O
inf	O
l	O
rjjeck	O
and	O
the	O
two	O
terms	O
on	O
the	O
right-hand	O
side	O
may	O
be	O
bounded	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
theorem	O
implies	O
that	O
if	O
g	O
e	O
c	O
there	O
is	O
a	O
universal	O
constant	O
co	O
and	O
a	O
finite	O
k	O
such	O
that	O
vck	O
logn	O
n	O
that	O
is	O
the	O
rate	B
of	I
convergence	I
is	O
always	O
of	O
the	O
order	O
of	O
jlog	O
n	O
n	O
and	O
the	O
constant	O
factor	O
vck	O
depends	O
on	O
the	O
distribution	O
the	O
number	O
vck	O
may	O
be	O
viewed	O
as	O
the	O
inherent	O
complexity	O
of	O
the	O
bayes	O
rule	B
for	O
the	O
distribution	O
the	O
intuition	O
is	O
that	O
the	O
simplest	O
rules	O
are	O
contained	O
in	O
cl	O
and	O
more	O
and	O
more	O
complex	O
rules	O
are	O
added	O
to	O
the	O
class	O
as	O
the	O
index	O
of	O
the	O
class	O
increases	O
the	O
size	O
of	O
the	O
error	O
is	O
about	O
the	O
same	O
as	O
if	O
we	O
had	O
known	O
k	O
beforehand	O
and	O
minimized	O
the	O
empirical	B
error	I
over	O
ck	O
in	O
view	O
of	O
the	O
results	O
of	O
chapter	O
it	O
is	O
clear	O
that	O
the	O
classifier	B
described	O
in	O
theorem	O
does	O
not	O
share	O
this	O
property	O
since	O
if	O
l	O
then	O
the	O
error	O
probability	O
of	O
the	O
structural	O
risk	O
minimization	O
rule	B
selected	O
from	O
ckn	O
is	O
larger	O
than	O
a	O
constant	O
times	O
jvckn	O
log	O
n	O
n	O
for	O
some	O
distribution-even	O
if	O
g	O
e	O
ck	O
for	O
some	O
fixed	O
k	O
just	O
as	O
in	O
designs	O
based	O
upon	O
minimum	O
description	O
length	O
automatic	B
model	O
selection	O
and	O
other	O
complexity	B
regularization	I
methods	O
akaike	O
barron	O
and	O
barron	O
and	O
cover	O
structural	O
risk	O
minimization	O
automatically	O
finds	O
where	O
to	O
look	O
for	O
the	O
optimal	O
classifier	B
the	O
constants	O
appearing	O
in	O
theorem	O
may	O
be	O
improved	O
by	O
using	O
refined	O
versions	O
of	O
the	O
vapnik	O
inequality	B
the	O
condition	O
g	O
e	O
c	O
in	O
theorem	O
is	O
not	O
very	O
severe	O
as	O
c	O
can	O
be	O
a	O
large	O
class	O
with	O
infinite	O
vc	B
dimension	B
the	O
only	O
requirement	O
is	O
that	O
it	O
should	O
be	O
written	O
as	O
a	O
countable	O
union	O
of	O
classes	O
of	O
finite	O
vc	B
dimension	B
note	O
however	O
that	O
the	O
class	O
of	O
all	O
decision	O
rules	O
can	O
not	O
be	O
decomposed	O
as	O
such	O
theorem	O
we	O
also	O
emphasize	O
that	O
in	O
order	O
to	O
achieve	O
the	O
jlog	O
n	O
n	O
rate	B
of	I
convergence	I
we	O
do	O
not	O
have	O
to	O
assume	O
that	O
the	O
distribution	O
is	O
a	O
member	O
of	O
a	O
known	O
finite-dimensional	O
parametric	O
family	O
chapter	O
the	O
condition	O
is	O
imposed	O
solely	O
on	O
the	O
form	O
of	O
the	O
bayes	O
classifier	B
g	O
by	O
inspecting	O
the	O
proof	O
of	O
theorem	O
we	O
see	O
that	O
for	O
every	O
k	O
el	O
l	O
co	O
vck	O
log	O
n	O
inf	O
l	O
l	O
n	O
eck	O
in	O
fact	O
theorem	O
is	O
the	O
consequence	O
of	O
this	O
inequality	B
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
which	O
may	O
be	O
called	O
estimation	B
error	I
usually	O
increases	O
with	O
growing	O
k	O
while	O
the	O
second	O
the	O
approximation	B
error	I
usually	O
decreases	O
with	O
it	O
importantly	O
the	O
above	O
inequality	B
is	O
true	O
for	O
every	O
k	O
so	O
that	O
vck	O
logn	O
inf	O
l	O
l	O
eck	O
n	O
thus	O
structural	O
risk	O
minimization	O
finds	O
a	O
nearly	O
optimal	O
balance	O
between	O
the	O
two	O
terms	O
see	O
also	O
problem	O
remark	O
it	O
is	O
worthwhile	O
mentioning	O
that	O
under	O
the	O
conditions	O
of	O
theorem	O
an	O
even	O
faster	O
rate	B
of	I
convergence	I
is	O
achievable	O
at	O
the	O
expense	O
of	O
nifying	O
the	O
constant	O
factor	O
more	O
precisely	O
it	O
is	O
possible	O
to	O
define	O
the	O
complexity	O
penalties	O
rj	O
n	O
such	O
that	O
the	O
resulting	O
classifier	B
satisfies	O
ela	O
l	O
where	O
the	O
constant	O
cl	O
depends	O
on	O
the	O
distribution	O
these	O
penalties	O
may	O
be	O
defined	O
by	O
exploiting	O
alexanders	O
inequality	B
and	O
the	O
inequality	B
above	O
can	O
be	O
proved	O
by	O
using	O
the	O
bound	O
in	O
problem	O
see	O
problem	O
remark	O
we	O
see	O
from	O
the	O
proof	O
of	O
theorem	O
that	O
p	O
in	O
e	O
complexity	B
regularization	I
this	O
means	O
that	O
in	O
does	O
not	O
underestimate	O
the	O
true	O
error	O
probability	O
of	O
by	O
much	O
this	O
is	O
a	O
very	O
attractive	O
feature	O
of	O
in	O
as	O
an	O
error	O
estimate	O
as	O
the	O
designer	O
can	O
be	O
confident	O
about	O
the	O
performance	O
of	O
the	O
rule	B
the	O
initial	O
excitement	O
over	O
a	O
consistent	O
rule	B
with	O
a	O
guaranteed	O
n	O
n	O
rate	B
of	I
convergence	I
to	O
the	O
bayes	B
error	I
is	O
tempered	O
by	O
a	O
few	O
sobering	O
facts	O
the	O
user	O
needs	O
to	O
choose	O
the	O
sequence	O
cm	O
and	O
has	O
to	O
know	O
vcw	O
however	O
problems	O
and	O
the	O
method	O
of	O
simple	B
empirical	B
covering	O
below	O
it	O
is	O
difficult	O
to	O
find	O
the	O
structural	O
risk	O
minimizer	O
efficiently	O
after	O
all	O
the	O
minimization	O
is	O
done	O
over	O
an	O
infinite	O
sequence	O
of	O
infinite	O
sets	O
the	O
second	O
concern	O
above	O
deserves	O
more	O
attention	O
consider	O
the	O
following	O
simple	O
example	O
let	O
d	O
and	O
let	O
cj	O
be	O
the	O
class	O
of	O
classifiers	O
for	O
which	O
ua	O
i	O
j	O
il	O
where	O
each	O
ai	O
is	O
an	O
interval	O
ocr	O
then	O
from	O
theorem	O
vcw	O
and	O
we	O
may	O
take	O
rj	O
n	O
n	O
in	O
structural	O
risk	O
minimization	O
we	O
find	O
those	O
j	O
empty	O
intervals	O
that	O
minimize	O
ln	O
rj	O
n	O
and	O
call	O
the	O
corresponding	O
classifier	B
nj	O
for	O
j	O
we	O
have	O
rj	O
n	O
loge	O
en	O
as	O
rn	O
n	O
rl	O
n	O
and	O
rj	O
n	O
is	O
monotone	O
in	O
j	O
it	O
is	O
easily	O
seen	O
that	O
to	O
pick	O
the	O
best	O
j	O
as	O
well	O
we	O
need	O
only	O
consider	O
j	O
n	O
still	O
this	O
is	O
a	O
formidable	O
exercise	O
for	O
fixed	O
j	O
the	O
best	O
j	O
intervals	O
may	O
be	O
found	O
by	O
considering	O
all	O
possible	O
insertions	O
of	O
interval	O
boundaries	O
among	O
the	O
n	O
xso	O
this	O
brute	O
force	O
method	O
takes	O
computation	O
time	O
bounded	O
from	O
below	O
by	O
if	O
we	O
let	O
j	O
run	O
up	O
to	O
n	O
then	O
we	O
have	O
as	O
a	O
lower	O
bound	O
just	O
the	O
last	O
term	O
alone	O
g	O
grows	O
as	O
a	O
ul	O
r	O
and	O
is	O
prohibitively	O
large	O
for	O
n	O
fortunately	O
in	O
this	O
particular	O
case	O
there	O
is	O
an	O
algorithm	B
which	O
finds	O
a	O
classifier	B
minimizing	O
ln	O
rj	O
n	O
over	O
c	O
ul	O
cj	O
in	O
computational	O
time	O
polynomial	B
in	O
n	O
see	O
problem	O
another	O
example	O
when	O
the	O
structural	O
risk	O
minimizer	O
is	O
easy	O
to	O
find	O
is	O
described	O
in	O
problem	O
however	O
such	O
fast	O
algorithms	O
are	O
not	O
always	O
available	O
and	O
exponential	B
running	O
time	O
prohibits	O
the	O
use	O
of	O
structural	O
risk	O
minimization	O
even	O
for	O
relatively	O
small	O
values	O
of	O
n	O
t	O
jl	O
simple	B
empirical	B
covering	O
poor	O
approximation	O
properties	O
of	O
vc	O
classes	O
we	O
pause	O
here	O
for	O
a	O
moment	O
to	O
summarize	O
some	O
interesting	O
by-products	O
that	O
readily	O
follow	O
from	O
theorem	O
and	O
the	O
slow	O
convergence	O
results	O
of	O
chapter	O
theorem	O
states	O
that	O
for	O
a	O
large	O
class	O
of	O
distributions	O
an	O
n	O
n	O
rate	B
of	I
convergence	I
to	O
the	O
bayes	B
error	I
l	O
is	O
achievable	O
on	O
the	O
other	O
hand	O
theorem	O
asserts	O
that	O
no	O
universal	O
rates	O
of	O
convergence	O
to	O
l	O
exist	O
therefore	O
the	O
class	O
of	O
distributions	O
for	O
which	O
theorem	O
is	O
valid	O
cannot	O
be	O
that	O
large	O
after	O
all	O
the	O
combination	O
of	O
these	O
facts	O
results	O
in	O
the	O
following	O
three	O
theorems	O
which	O
say	O
that	O
vc	O
classes-classes	O
of	O
subsets	O
of	O
n	O
d	O
with	O
finite	O
vc	O
dimension-have	O
necessarily	O
very	O
poor	O
approximation	O
properties	O
the	O
proofs	O
are	O
left	O
to	O
the	O
reader	O
as	O
easy	O
exercises	O
problems	O
to	O
for	O
direct	O
proofs	O
see	O
for	O
example	O
benedek	O
and	O
itai	O
theorem	O
letc	O
be	O
any	O
class	O
of	O
classifiers	O
of	O
the	O
fo	O
rm	O
nd	O
i	O
with	O
vc	B
dimension	B
vc	O
then	O
for	O
every	O
e	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
mf	O
l	O
l	O
ec	O
e	O
theorem	O
let	O
be	O
a	O
sequence	O
of	O
classifiers	O
such	O
that	O
the	O
vc	O
dimensions	O
vcl	O
are	O
all	O
finite	O
then	O
for	O
any	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
arbitrarily	O
slowly	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
for	O
every	O
k	O
large	O
enough	O
inf	O
l	O
l	O
ak	O
eck	O
theorem	O
the	O
class	O
c	O
of	O
all	O
measurable	O
decision	O
rules	O
of	O
form	O
rd	O
i	O
cannot	O
be	O
written	O
as	O
c	O
u	O
cen	O
jl	O
where	O
the	O
vc	B
dimension	B
of	O
each	O
class	O
cj	O
is	O
finite	O
in	O
other	O
words	O
the	O
class	O
b	O
of	O
all	O
borel	O
subsets	O
ofnd	O
cannot	O
be	O
written	O
as	O
a	O
union	O
of	O
countably	O
many	O
vc	O
classes	O
in	O
fact	O
the	O
same	O
is	O
true	O
for	O
the	O
class	O
of	O
all	O
subsets	O
of	O
the	O
set	O
of	O
positive	O
integers	O
simple	B
empirical	B
covering	O
as	O
theorem	O
shows	O
the	O
method	O
of	O
structural	O
risk	O
minimization	O
provides	O
tomatic	O
protection	O
against	O
the	O
danger	O
of	O
overfitting	B
the	O
data	O
by	O
penalizing	O
plex	O
candidate	O
classifiers	O
one	O
of	O
the	O
disadvantages	O
of	O
the	O
method	O
is	O
that	O
the	O
penalty	O
terms	O
rj	O
n	O
require	O
knowledge	O
of	O
the	O
vc	O
dimensions	O
of	O
the	O
classes	O
cj	O
or	O
upper	O
bounds	O
of	O
these	O
dimensions	O
next	O
we	O
discuss	O
a	O
method	O
proposed	O
by	O
complexity	B
regularization	I
buescher	O
and	O
kumar	O
which	O
is	O
applicable	O
even	O
if	O
the	O
vc	O
dimensions	O
of	O
the	O
classes	O
are	O
completely	O
unknown	O
the	O
method	O
called	O
simple	B
empirical	B
cov	O
ering	O
is	O
closely	O
related	O
to	O
the	O
method	O
based	O
on	O
empirical	B
covering	O
studied	O
in	O
problem	O
in	O
simple	B
empirical	B
covering	O
we	O
first	O
split	O
the	O
data	O
sequence	O
dn	O
into	O
two	O
parts	O
the	O
first	O
part	O
is	O
dm	O
yd	O
ym	O
and	O
the	O
second	O
part	O
is	O
yn	O
the	O
positive	O
integers	O
m	O
and	O
i	O
n	O
m	O
will	O
be	O
specified	O
later	O
the	O
first	O
part	O
dm	O
is	O
used	O
to	O
cover	O
c	O
cen	O
as	O
follows	O
for	O
every	O
e	O
c	O
define	O
the	O
binary	B
m-vector	O
bm	O
by	O
there	O
are	O
n	O
different	O
values	O
of	O
bm	O
usually	O
as	O
ve	O
n	O
that	O
is	O
all	O
possible	O
values	O
of	O
bm	O
occur	O
as	O
is	O
varied	O
over	O
c	O
but	O
of	O
course	O
n	O
depends	O
on	O
the	O
values	O
of	O
x	O
i	O
x	O
m	O
we	O
call	O
a	O
classifier	B
simpler	O
than	O
another	O
classifier	B
if	O
the	O
smallest	O
index	O
i	O
such	O
that	O
e	O
ci	O
is	O
smaller	O
than	O
or	O
equal	O
to	O
the	O
index	O
j	O
such	O
that	O
e	O
cj	O
for	O
every	O
binary	B
m-vector	O
b	O
e	O
lm	O
that	O
can	O
be	O
written	O
as	O
b	O
bm	O
for	O
some	O
e	O
c	O
we	O
pick	O
a	O
candidate	O
classifier	B
with	O
k	O
e	O
n	O
such	O
that	O
bmpmk	O
b	O
and	O
it	O
is	O
the	O
simplest	O
such	O
classifier	B
that	O
is	O
there	O
is	O
no	O
e	O
c	O
such	O
that	O
simultaneously	O
bmpmk	O
b	O
and	O
is	O
simpler	O
than	O
m	O
k	O
this	O
yields	O
n	O
candidates	O
among	O
these	O
we	O
select	O
one	O
that	O
minimizes	O
the	O
empirical	B
error	I
measured	O
on	O
the	O
independent	O
testing	B
sequence	I
denote	O
the	O
selected	O
classifier	B
by	O
the	O
next	O
theorem	O
asserts	O
that	O
the	O
method	O
works	O
under	O
circumstances	O
similar	O
to	O
structural	O
risk	O
minimization	O
theorem	O
and	O
kumar	O
let	O
cl	O
be	O
a	O
nested	O
sequence	O
of	O
classes	O
of	O
classifiers	O
such	O
that	O
for	O
any	O
distribution	O
of	O
y	O
lim	O
joo	O
eej	O
inf	O
l	O
l	O
assume	O
also	O
that	O
the	O
vc	O
dimensions	O
vel	O
are	O
all	O
finite	O
if	O
m	O
i	O
log	O
n	O
and	O
min	O
then	O
the	O
classification	O
rule	B
based	O
on	O
simple	B
empirical	B
covering	O
as	O
defined	O
above	O
is	O
strongly	O
universally	O
consistent	O
proof	O
we	O
decompose	O
the	O
difference	O
between	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
and	O
the	O
bayes	O
risk	O
as	O
follows	O
the	O
first	O
term	O
can	O
be	O
handled	O
by	O
lemma	O
and	O
hoeffdings	O
inequality	B
simple	B
empirical	B
covering	O
p	O
inf	O
l	O
mk	O
e	O
l-sk-sn	O
p	O
sup	O
il	O
mk	O
e	O
dm	O
lz	O
e	O
l-sk-sn	O
e	O
e	O
because	O
m	O
on	O
the	O
latter	O
expression	B
converges	O
to	O
zero	O
exponentially	O
rapidly	O
thus	O
it	O
remains	O
to	O
show	O
that	O
inf	O
l	O
mk	O
l	O
l-sk-sn	O
with	O
probability	O
one	O
by	O
our	O
assumptions	O
for	O
every	O
e	O
there	O
is	O
an	O
integer	O
k	O
such	O
that	O
inf	O
lep	O
l	O
e	O
eck	O
then	O
there	O
exists	O
a	O
classifier	B
epee	O
e	O
ek	O
with	O
lepe	O
l	O
e	O
therefore	O
we	O
are	O
done	O
if	O
we	O
prove	O
that	O
lim	O
sup	O
inf	O
l	O
mk	O
lepe	O
with	O
probability	O
one	O
n-hx	O
l-sk-sn	O
clearly	O
there	O
is	O
a	O
classifier	B
mj	O
among	O
the	O
candidates	O
ml	O
mn	O
such	O
that	O
since	O
by	O
definition	O
mj	O
is	O
simpler	O
than	O
epe	O
and	O
the	O
classes	O
el	O
are	O
nested	O
it	O
follows	O
that	O
mj	O
e	O
ek	O
therefore	O
sup	O
ilep	O
lep	O
i	O
where	O
the	O
last	O
supremum	O
is	O
taken	O
over	O
all	O
pairs	O
of	O
classifiers	O
such	O
that	O
their	O
sponding	O
binary	B
vectors	O
bm	O
and	O
bm	O
are	O
equal	O
but	O
from	O
problem	O
gomplexity	O
regularization	O
which	O
is	O
summable	O
if	O
m	O
log	O
n	O
remark	O
as	O
in	O
theorem	O
we	O
may	O
assume	O
again	O
that	O
there	O
is	O
an	O
integer	O
k	O
such	O
that	O
infeok	O
l	O
l	O
then	O
from	O
the	O
proof	O
of	O
the	O
theorem	O
above	O
we	O
see	O
that	O
the	O
error	O
probability	O
of	O
the	O
classifier	B
obtained	O
by	O
the	O
method	O
of	O
simple	B
empirical	B
covering	O
satisfies	O
p	O
l	O
e	O
e-n-me	O
unfortunately	O
for	O
any	O
choice	O
of	O
m	O
this	O
bound	O
is	O
much	O
larger	O
than	O
the	O
analogous	O
bound	O
obtained	O
for	O
structural	O
risk	O
minimization	O
in	O
particular	O
it	O
does	O
not	O
yield	O
an	O
ocjlognn	O
rate	B
of	I
convergence	I
thus	O
it	O
appears	O
that	O
the	O
price	O
paid	O
for	O
the	O
advantages	O
of	O
simple	B
empirical	B
covering-no	O
knowledge	O
of	O
the	O
vc	O
dimensions	O
is	O
required	O
and	O
the	O
implementation	O
may	O
require	O
significantly	O
less	O
computational	O
time	O
in	O
general-is	O
a	O
slower	O
rate	B
of	I
convergence	I
see	O
problem	O
problems	O
and	O
exercises	O
problem	O
prove	O
theorem	O
problem	O
define	O
the	O
complexity	O
penalties	O
rj	O
n	O
so	O
that	O
under	O
the	O
conditions	O
of	O
theorem	O
the	O
classification	O
rule	B
based	O
upon	O
structural	O
risk	O
minimization	O
satisfies	O
where	O
the	O
constant	O
cl	O
depends	O
on	O
the	O
distribution	O
hint	O
use	O
alexanders	O
bound	O
and	O
the	O
inequality	B
of	O
problem	O
problem	O
let	O
cl	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
decision	O
rules	O
with	O
finite	O
vc	O
veuj	O
on	O
the	O
vc	O
dimensions	O
are	O
known	O
dimensions	O
assume	O
that	O
only	O
upper	O
bounds	O
a	O
j	O
define	O
the	O
complexity	O
penalties	O
by	O
rj	O
n	O
logen	O
n	O
show	O
that	O
if	O
li	O
e-cxj	O
then	O
theorems	O
and	O
carryover	O
to	O
the	O
classifier	B
based	O
on	O
structural	O
risk	O
minimization	O
defined	O
by	O
these	O
penalties	O
this	O
points	O
out	O
that	O
knowledge	O
of	O
relatively	O
rough	O
estimates	O
of	O
the	O
vc	O
dimensions	O
suffice	O
problem	O
let	O
cci	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
classifiers	O
such	O
that	O
the	O
vc	O
dimensions	O
vel	O
are	O
all	O
finite	O
assume	O
furthermore	O
that	O
the	O
bayes	O
rule	B
is	O
tained	O
in	O
one	O
of	O
the	O
classes	O
and	O
that	O
l	O
o	O
let	O
be	O
the	O
rule	B
obtained	O
by	O
structural	O
risk	O
minimization	O
using	O
the	O
positive	O
penalties	O
rj	O
n	O
satisfying	O
problems	O
and	O
exercises	O
for	O
each	O
n	O
rj	O
n	O
is	O
strictly	O
monotone	O
increasing	O
in	O
j	O
for	O
each	O
j	O
limn-oo	O
rj	O
n	O
show	O
that	O
el	O
olognn	O
and	O
zeger	O
for	O
related	O
work	O
see	O
benedek	O
and	O
itai	O
problem	O
let	O
cj	O
be	O
the	O
class	O
of	O
classifiers	O
n	O
d	O
to	O
i	O
satisfying	O
i	O
u	O
ai	O
j	O
il	O
where	O
the	O
ais	O
are	O
bounded	O
intervals	O
in	O
n	O
the	O
purpose	O
of	O
this	O
exercise	O
is	O
to	O
point	O
out	O
that	O
there	O
is	O
a	O
fast	O
algorithm	B
to	O
find	O
the	O
structural	O
risk	O
minimizer	O
over	O
c	O
ul	O
cen	O
that	O
is	O
which	O
minimizes	O
in	O
ln	O
rj	O
n	O
over	O
c	O
where	O
the	O
penalties	O
rj	O
n	O
are	O
defined	O
as	O
in	O
theorems	O
and	O
the	O
property	O
below	O
was	O
pointed	O
out	O
to	O
us	O
by	O
miklos	O
csuros	O
and	O
szabo	O
let	O
al	O
aj	O
be	O
the	O
j	O
intervals	O
defining	O
the	O
classifier	B
minimizing	O
ln	O
over	O
cj	O
show	O
that	O
the	O
optimal	O
intervals	O
for	O
cjl	O
arl	O
a	O
jll	O
satisfy	O
the	O
following	O
property	O
either	O
j	O
of	O
the	O
intervals	O
coincide	O
with	O
a	O
l	O
a	O
j	O
or	O
of	O
them	O
are	O
among	O
a	O
r	O
a	O
j	O
and	O
the	O
remaining	O
two	O
intervals	O
are	O
j	O
subsets	O
of	O
the	O
j	O
interval	O
use	O
property	O
to	O
define	O
an	O
algorithm	B
that	O
finds	O
in	O
running	O
time	O
polynomial	B
in	O
the	O
sample	O
size	O
n	O
problem	O
assume	O
that	O
the	O
distribution	O
of	O
x	O
is	O
concentrated	O
on	O
the	O
unit	O
cube	O
that	O
is	O
px	O
e	O
ld	O
let	O
p	O
be	O
a	O
partition	B
of	O
ld	O
into	O
cubes	O
of	O
size	O
j	O
that	O
is	O
p	O
contains	O
cubic	B
cells	O
let	O
cj	O
be	O
the	O
class	O
of	O
all	O
histogram	O
classifiers	O
ld	O
to	O
i	O
based	O
on	O
p	O
in	O
other	O
words	O
p	O
contains	O
all	O
classifiers	O
which	O
assign	O
the	O
same	O
label	O
to	O
points	O
falling	O
in	O
the	O
same	O
cell	O
of	O
p	O
what	O
is	O
the	O
vc	B
dimension	B
vej	O
of	O
cen	O
point	O
out	O
that	O
the	O
classifier	B
minimizing	O
ln	O
over	O
cj	O
is	O
just	O
the	O
regular	B
histogram	B
rule	B
based	O
on	O
p	O
chapter	O
thus	O
we	O
have	O
another	O
example	O
in	O
which	O
the	O
empirically	B
optimal	I
classifier	B
is	O
computationally	O
inexpensive	O
the	O
structural	O
risk	O
minimizer	O
based	O
on	O
c	O
ul	O
cj	O
is	O
also	O
easy	O
to	O
find	O
assume	O
that	O
the	O
a	B
posteriori	I
probability	I
is	O
uniformly	O
lipschitz	O
that	O
is	O
for	O
any	O
x	O
y	O
e	O
ld	O
cllx	O
where	O
c	O
is	O
some	O
constant	O
find	O
upper	O
bounds	O
for	O
the	O
rate	B
of	I
convergence	I
of	O
el	O
to	O
l	O
problem	O
prove	O
theorem	O
hint	O
use	O
theorem	O
problem	O
prove	O
theorem	O
hint	O
use	O
theorem	O
problem	O
prove	O
theorem	O
hint	O
use	O
theorems	O
and	O
problem	O
assume	O
that	O
the	O
bayes	O
rule	B
g	O
is	O
contained	O
in	O
c	O
ul	O
cj	O
let	O
be	O
the	O
classifier	B
obtained	O
by	O
simple	B
empirical	B
covering	O
determine	O
the	O
value	O
of	O
the	O
design	O
parameter	O
m	O
that	O
minimizes	O
the	O
bounds	O
obtained	O
in	O
the	O
proof	O
of	O
theorem	O
obtain	O
a	O
tight	O
upper	O
bound	O
for	O
el	O
l	O
compare	O
your	O
results	O
with	O
theorem	O
condensed	B
and	O
edited	B
nearest	B
neighbor	I
rules	O
condensed	B
nearest	B
neighbor	I
rules	O
condensing	O
is	O
the	O
process	O
by	O
which	O
we	O
eliminate	O
data	O
points	O
yet	O
keep	O
the	O
same	O
behavior	O
for	O
example	O
in	O
the	O
nearest	B
neighbor	I
rule	B
by	O
condensing	O
we	O
might	O
mean	O
the	O
reduction	B
of	I
yn	O
to	O
y	O
yl	O
such	O
that	O
for	O
all	O
x	O
e	O
n	O
d	O
the	O
l-nn	O
rule	B
is	O
identical	O
based	O
on	O
the	O
two	O
samples	O
this	O
will	O
be	O
called	O
pure	O
condensing	O
this	O
operation	O
has	O
no	O
effect	O
on	O
l	O
n	O
and	O
therefore	O
is	O
recommended	O
whenever	O
space	O
is	O
at	O
a	O
premium	O
the	O
space	O
savings	O
should	O
be	O
substantial	O
whenever	O
the	O
classes	O
are	O
separated	O
unfortunately	O
pure	O
condensing	O
is	O
computationally	O
expensive	O
and	O
offers	O
no	O
hope	O
of	O
improving	O
upon	O
the	O
performance	O
of	O
the	O
ordinary	B
l-nn	O
rule	B
o	O
o	O
class	O
under	O
i-nn	B
rule	B
figure	O
pure	O
condensing	O
eliminating	O
the	O
marked	O
points	O
does	O
not	O
change	O
the	O
decision	O
class	O
under	O
rule	B
o	O
gondensed	O
and	O
edited	B
nearest	B
neighbor	I
rules	O
hart	O
has	O
the	O
first	O
simple	O
algorithm	B
for	O
condensing	O
he	O
picks	O
a	O
subset	O
that	O
correctly	O
classifies	O
the	O
remaining	O
data	O
by	O
the	O
i-nn	B
rule	B
finding	O
a	O
minimal	O
such	O
subset	O
is	O
computationally	O
difficult	O
but	O
heuristics	O
may	O
do	O
the	O
job	O
harts	O
heuristic	O
is	O
also	O
discussed	O
in	O
devijver	O
and	O
kittler	O
for	O
probabilistic	O
analysis	O
we	O
take	O
a	O
more	O
abstract	O
setting	O
let	O
y	O
be	O
a	O
sequence	O
that	O
depends	O
in	O
an	O
arbitrary	O
fashion	O
on	O
the	O
data	O
dn	O
and	O
let	O
gn	O
be	O
the	O
i-nearest	O
neighbor	O
rule	B
with	O
y	O
yi	O
where	O
for	O
simplicity	O
m	O
is	O
fixed	O
beforehand	O
the	O
data	O
might	O
for	O
example	O
be	O
obtained	O
by	O
finding	O
the	O
subset	O
of	O
the	O
data	O
of	O
size	O
m	O
for	O
which	O
the	O
error	O
with	O
the	O
i-nn	B
rule	B
committed	O
on	O
the	O
remaining	O
n	O
m	O
data	O
is	O
minimal	O
will	O
be	O
called	O
harts	O
rule	B
regardless	O
if	O
in	O
n	O
ignxy	O
and	O
ln	O
pgnx	O
yidnl	O
then	O
we	O
have	O
the	O
following	O
theorem	O
and	O
wagner	O
for	O
all	O
e	O
and	O
all	O
butions	O
piln	O
lnl	O
es	O
d	O
ne	O
remark	O
the	O
estimate	O
in	O
is	O
called	O
the	O
resubstitution	B
estimate	O
of	O
the	O
error	O
ability	O
it	O
is	O
thoroughly	O
studied	O
in	O
chapter	O
where	O
several	O
results	O
of	O
the	O
mentioned	O
kind	O
are	O
stated	O
proof	O
observe	O
that	O
where	O
bi	O
is	O
the	O
voronoi	B
cell	I
of	O
x	O
in	O
the	O
voronoi	B
partition	B
corresponding	O
to	O
xi	O
xl	O
that	O
is	O
bi	O
is	O
the	O
set	O
of	O
points	O
of	O
nd	O
closer	O
to	O
x	O
than	O
to	O
any	O
other	O
x	O
appropriate	O
distance-tie	O
breaking	O
similarly	O
we	O
use	O
the	O
simple	O
upper	O
bound	O
iln	O
inl	O
sup	O
ivna	O
vai	O
aeam	O
where	O
v	O
denotes	O
the	O
measure	O
of	O
y	O
vn	O
is	O
the	O
corresponding	O
empirical	B
measure	I
and	O
am	O
is	O
the	O
family	B
of	I
all	O
subsets	O
ofnd	O
x	O
i	O
of	O
the	O
form	O
ul	O
bi	O
x	O
where	O
b	O
l	O
bm	O
are	O
voronoi	O
cells	O
corresponding	O
to	O
xl	O
x	O
m	O
xi	O
end	O
yi	O
i	O
we	O
use	O
the	O
vapnik-chervonenkis	B
inequality	B
to	O
bound	O
the	O
above	O
supremum	O
by	O
theorem	O
condensed	B
nearest	B
neighbor	I
rules	O
where	O
a	O
is	O
the	O
class	O
of	O
sets	O
x	O
but	O
each	O
set	O
in	O
a	O
is	O
an	O
intersection	O
of	O
at	O
most	O
m	O
hyperplanes	O
therefore	O
by	O
theorems	O
and	O
sea	O
n	O
sup	O
d	O
n	O
e	O
d	O
where	O
n	O
denotes	O
the	O
number	O
of	O
points	O
in	O
r	O
d	O
x	O
the	O
result	O
now	O
follows	O
from	O
theorem	O
remark	O
with	O
harts	O
rule	B
at	O
least	O
m	O
data	O
points	O
are	O
correctly	O
classified	O
by	O
the	O
rule	B
we	O
handle	O
distance	O
ties	O
satisfactorily	O
therefore	O
ln	O
min	O
the	O
following	O
is	O
a	O
special	O
case	O
let	O
m	O
n	O
be	O
fixed	O
and	O
let	O
dm	O
be	O
an	O
arbitrary	O
random	O
subsetofm	O
pairs	O
from	O
yn	O
usedingn	O
let	O
the	O
remaining	O
n	O
m	O
points	O
be	O
denoted	O
by	O
tn-	O
m	O
we	O
write	O
lndm	O
for	O
the	O
probability	O
of	O
error	O
with	O
the	O
based	O
upon	O
dm	O
and	O
we	O
define	O
in	O
harts	O
rule	B
ln	O
would	O
be	O
zero	O
for	O
example	O
then	O
we	O
have	O
theorem	O
for	O
all	O
e	O
where	O
ln	O
is	O
the	O
probability	O
of	O
error	O
with	O
gn	O
that	O
dm	O
depends	O
in	O
an	O
arbitrary	O
fashion	O
upon	O
dn	O
and	O
l	O
n	O
m	O
is	O
ln	O
mdm	O
tn-	O
m	O
with	O
the	O
data	O
set	O
dm	O
proof	O
list	O
the	O
m-element	O
subsets	O
im	O
of	O
n	O
and	O
define	O
d	O
as	O
the	O
sequence	O
of	O
m	O
pairs	O
from	O
dn	O
indexed	O
by	O
i	O
ii	O
i	O
in	O
i	O
a	O
ccor	O
mg	O
y	O
enote	O
n-m	O
d	O
d	O
en	O
di	O
th	O
in	O
ti	O
d	O
n	O
condensed	B
and	O
edited	B
nearest	B
neighbor	I
rules	O
hoeffdings	O
inequality	B
because	O
given	O
d	O
mlnmdi	O
tm	O
is	O
binomial	B
m	O
lnd	O
by	O
checking	O
the	O
proof	O
we	O
also	O
see	O
that	O
if	O
dm	O
is	O
selected	O
to	O
minimize	O
the	O
error	O
estimate	O
lnm	O
t-m	O
then	O
the	O
error	O
probability	O
ln	O
of	O
the	O
obtained	O
rule	B
satisfies	O
p	O
inf	O
e	O
p	O
sup	O
ilndnj	O
lnmdm	O
e	O
d	O
theorem	O
in	O
the	O
proof	O
of	O
theorem	O
thus	O
for	O
the	O
particular	O
rule	B
that	O
mimics	O
harts	O
rule	B
the	O
exception	O
that	O
m	O
is	O
fixed	O
if	O
m	O
is	O
not	O
too	O
large-it	O
must	O
be	O
much	O
smaller	O
than	O
n	O
log	O
n-ln	O
is	O
likely	O
to	O
be	O
close	O
to	O
the	O
best	O
possible	O
we	O
can	O
hope	O
for	O
with	O
a	O
rule	B
based	O
upon	O
a	O
subsample	O
of	O
size	O
m	O
with	O
some	O
work	O
problem	O
we	O
see	O
that	O
by	O
theorem	O
where	O
lm	O
is	O
the	O
probability	O
of	O
error	O
with	O
the	O
rule	B
based	O
upon	O
a	O
sample	O
of	O
m	O
data	O
pairs	O
hence	O
if	O
m	O
on	O
log	O
n	O
m	O
lim	O
sup	O
el	O
n	O
lnn	O
for	O
the	O
i-nn	B
rule	B
based	O
on	O
m	O
data	O
pairs	O
dm	O
selected	O
to	O
minimize	O
the	O
error	O
estimate	O
lnmdm	O
tn-m	O
however	O
this	O
is	O
very	O
pessimistic	O
indeed	O
it	O
reassures	O
us	O
that	O
with	O
only	O
a	O
small	O
fraction	O
of	O
the	O
original	O
data	O
we	O
obtain	O
at	O
least	O
as	O
good	O
a	O
performance	O
as	O
with	O
the	O
full	O
data	O
set-so	O
this	O
method	O
of	O
condensing	O
is	O
worthwhile	O
this	O
is	O
not	O
very	O
surprising	O
interestingly	O
however	O
the	O
following	O
much	O
stronger	O
result	O
is	O
true	O
condensed	B
nearest	B
neighbor	I
rules	O
theorem	O
if	O
m	O
on	O
log	O
n	O
and	O
m	O
and	O
if	O
lns	O
the	O
probability	O
of	O
error	O
for	O
the	O
condensed	B
nearest	B
neighbor	I
rule	B
in	O
which	O
lnmdm	O
tn-	O
m	O
is	O
minimized	O
over	O
all	O
data	O
sets	O
dm	O
then	O
lim	O
eln	O
l	O
n--oo	O
proof	O
by	O
it	O
suffices	O
to	O
establish	O
that	O
as	O
m	O
such	O
that	O
m	O
on	O
where	O
lndm	O
is	O
the	O
probability	O
of	O
error	O
of	O
the	O
i-nn	B
rule	B
with	O
dm	O
as	O
this	O
is	O
one	O
of	O
the	O
fundamental	O
properties	O
of	B
nearest	B
neighbor	I
rules	I
not	O
previously	O
found	O
in	O
texts	O
we	O
offer	O
a	O
thorough	O
analysis	O
and	O
proof	O
of	O
this	O
result	O
in	O
the	O
remainder	O
of	O
this	O
section	O
culminating	O
in	O
theorem	O
the	O
distribution-free	O
result	O
of	O
theorem	O
sets	O
the	O
stage	O
for	O
many	O
consistency	B
proofs	O
for	O
rules	O
that	O
use	O
condensing	O
editing	O
or	O
proto	O
typing	O
as	O
defined	O
in	O
the	O
next	O
two	O
sections	O
it	O
states	O
that	O
inherently	O
partitions	O
of	O
the	O
space	O
by	O
i-nn	B
rules	O
are	O
rich	O
historical	O
remark	O
other	O
condensed	B
nearest	B
neighbor	I
rules	O
are	O
presented	O
by	O
gates	O
ullmann	O
ritter	O
woodruff	O
lowry	O
and	O
isenhour	O
tomek	O
swonger	O
gowda	O
and	O
krishna	O
and	O
fukunaga	O
and	O
mantock	O
define	O
z	O
let	O
yi	O
zi	O
i	O
n	O
be	O
i	O
i	O
d	O
tuples	O
pendent	O
of	O
y	O
z	O
where	O
x	O
may	O
have	O
a	O
distribution	O
different	O
from	O
x	O
but	O
the	O
support	B
set	O
of	O
the	O
distribution	O
j-	O
of	O
x	O
is	O
identical	O
to	O
that	O
of	O
j-	O
the	O
distribution	O
of	O
x	O
furthermore	O
pyi	O
x	O
and	O
zi	O
is	O
the	O
bayes	O
decision	O
at	O
x	O
lemma	O
let	O
ln	O
p	O
zlx	O
zixi	O
zi	O
x	O
zn	O
be	O
the	O
probability	O
of	O
error	O
for	O
the	O
j-nn	O
rule	B
based	O
on	O
zi	O
i	O
n	O
that	O
is	O
zlx	O
zi	O
if	O
x	O
is	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
xi	O
x	O
then	O
lim	O
el	O
n	O
o	O
n--oo	O
proof	O
denote	O
by	O
xlx	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
xi	O
x	O
notice	O
that	O
the	O
proof	O
of	O
lemma	O
may	O
be	O
extended	O
in	O
a	O
straightforward	O
way	O
to	O
show	O
that	O
iixlx	O
xii	O
with	O
probability	O
one	O
since	O
this	O
is	O
the	O
only	O
property	O
of	O
the	O
nearest	B
neighbor	I
of	O
x	O
that	O
we	O
used	O
in	O
deriving	O
the	O
asymptotic	O
formula	O
for	O
the	O
ordinary	B
i-nn	B
error	O
limn--oo	O
eln	O
equals	O
lnn	O
corresponding	O
to	O
the	O
pair	O
z	O
but	O
we	O
have	O
p	O
x	O
x	O
thus	O
the	O
bayes	O
probability	O
of	O
error	O
l	O
condensed	B
and	O
edited	B
nearest	B
neighbor	I
rules	O
for	O
the	O
pattern	O
recognition	O
problem	O
with	O
z	O
is	O
zero	O
hence	O
for	O
this	O
distribution	O
the	O
i-nn	B
rule	B
is	O
consistent	O
as	O
lnn	O
l	O
o	O
lemma	O
let	O
zix	O
be	O
as	O
in	O
the	O
previous	O
lemma	O
let	O
ln	O
p	O
i	O
yix	O
x	O
yn	O
be	O
the	O
probability	O
of	O
error	O
for	O
the	O
discrimination	O
problem	O
for	O
y	O
z	O
then	O
lim	O
e	O
ln	O
l	O
where	O
l	O
is	O
the	O
bayes	B
error	I
corresponding	O
to	O
y	O
proof	O
p	O
i	O
y	O
p	O
i	O
z	O
i	O
z	O
l	O
by	O
lemma	O
theorem	O
let	O
dm	O
be	O
a	O
subset	O
of	O
size	O
m	O
drawn	O
from	O
dn	O
if	O
m	O
and	O
min	O
as	O
n	O
then	O
lim	O
pinflndm	O
le	O
n-oo	O
for	O
all	O
e	O
where	O
ln	O
denotes	O
the	O
conditional	O
probability	O
of	O
error	O
of	O
the	O
nearest	B
neighbor	I
rule	B
with	O
dnu	O
and	O
the	O
infimum	O
ranges	O
over	O
all	O
subsets	O
proof	O
let	O
d	O
be	O
the	O
subset	O
of	O
dn	O
consisting	O
of	O
those	O
pairs	O
yi	O
for	O
which	O
yi	O
irjxi	O
zi	O
if	O
jdj	O
m	O
let	O
d	O
be	O
the	O
first	O
m	O
pairs	O
of	O
d	O
and	O
if	O
jdi	O
m	O
let	O
d	O
yi	O
ym	O
then	O
if	O
n	O
jdi	O
m	O
then	O
we	O
know	O
that	O
the	O
pairs	O
in	O
d	O
are	O
i	O
i	O
d	O
and	O
drawn	O
from	O
the	O
distribution	O
of	O
z	O
where	O
x	O
has	O
the	O
same	O
support	B
set	O
as	O
x	O
see	O
problem	O
for	O
properties	O
of	O
x	O
in	O
particular	O
s	O
pn	O
m	O
m	O
lnd	O
l	O
sieves	O
and	O
prototypes	O
pnmplnd	O
le	O
pbinomialn	O
p	O
m	O
n	O
el	O
l	O
p	O
py	O
z	O
l	O
and	O
by	O
markovs	O
inequality	B
e	O
by	O
the	O
law	O
of	O
large	O
numbers	O
we	O
use	O
mn	O
and	O
by	O
lemma	O
we	O
use	O
m	O
edited	B
nearest	B
neighbor	I
rules	O
edited	B
nearest	B
neighbor	I
rules	O
are	O
i-nn	B
rules	O
that	O
are	O
based	O
upon	O
carefully	O
selected	O
subsets	O
y	O
yi	O
this	O
situation	O
is	O
partially	O
dealt	O
with	O
in	O
the	O
previous	O
section	O
as	O
the	O
frontier	O
between	O
condensed	B
and	O
edited	B
nearest	B
neighbor	I
rules	O
is	O
defined	O
the	O
idea	O
of	O
editing	O
based	O
upon	O
the	O
k-nn	B
rule	B
was	O
first	O
suggested	O
by	O
wilson	O
and	O
later	O
studied	O
by	O
wagner	O
and	O
penrod	O
and	O
wagner	O
wilson	O
suggests	O
the	O
following	O
scheme	O
compute	O
yi	O
zi	O
where	O
zi	O
is	O
the	O
k-nn	B
decision	O
at	O
xi	O
based	O
on	O
the	O
full	O
data	O
set	O
with	O
yi	O
deleted	O
then	O
eliminate	O
all	O
data	O
pairs	O
for	O
which	O
yi	O
zi	O
the	O
remaining	O
data	O
pairs	O
are	O
used	O
with	O
the	O
i-nn	B
rule	B
the	O
k-nn	B
rule	B
another	O
rule	B
based	O
upon	O
data	O
splitting	O
is	O
dealt	O
with	O
by	O
devijver	O
and	O
kittler	O
a	O
survey	O
is	O
given	O
by	O
dasarathy	O
devijver	O
and	O
devijver	O
and	O
kittler	O
repeated	O
editing	O
was	O
investigated	O
by	O
tomek	O
devijver	O
and	O
kittler	O
propose	O
a	O
modification	O
of	O
wilsons	O
leave-one-out	B
method	O
of	O
editing	O
based	O
upon	O
data	O
splitting	O
sieves	O
and	O
prototypes	O
let	O
gn	O
be	O
a	O
rule	B
that	O
uses	O
the	O
i-nn	B
classification	O
based	O
upon	O
prototype	B
data	O
pairs	O
y	O
ythatdependinsomefashionontheoriginaldata	O
ifthepairs	O
form	O
a	O
subset	O
of	O
the	O
data	O
pairs	O
thus	O
m	O
n	O
we	O
have	O
edited	B
or	O
condensed	B
nearest	B
neighbor	I
rules	O
however	O
the	O
yf	O
pairs	O
may	O
be	O
strategically	O
picked	O
outside	O
the	O
original	O
data	O
set	O
for	O
example	O
in	O
relabeling	B
section	O
m	O
n	O
x	O
xi	O
and	O
y	O
gxi	O
where	O
gxi	O
is	O
the	O
k-nn	B
decision	O
at	O
xi	O
under	O
some	O
conditions	O
the	O
relabeling	B
rule	B
is	O
consistent	O
theorem	O
the	O
true	O
objective	O
of	O
proto	O
typing	O
is	O
to	O
extract	O
information	O
from	O
the	O
data	O
by	O
insisting	O
that	O
m	O
be	O
much	O
smaller	O
than	O
n	O
condensed	B
and	O
edited	B
nearest	B
neighbor	I
rules	O
figure	O
a	O
rule	B
based	O
upon	O
prototypes	O
in	O
this	O
example	O
all	O
the	O
data	O
points	O
are	O
correctly	O
classified	O
based	O
upon	O
the	O
prototype	B
rule	B
chang	O
describes	O
a	O
rule	B
in	O
which	O
we	O
iterate	O
the	O
following	O
step	O
until	O
a	O
given	O
stopping	O
rule	B
is	O
satisfied	O
merge	O
the	O
two	O
closest	O
nearest	O
neighbors	O
of	O
the	O
same	O
class	O
and	O
replace	O
both	O
pairs	O
by	O
a	O
new	O
average	O
prototype	B
pair	O
kohonen	O
recognizes	O
the	O
advantages	O
of	O
such	O
prototyping	O
in	O
general	O
as	O
a	O
device	O
for	O
partitioning	O
rd-he	O
calls	O
this	O
learning	B
vector	I
quantization	B
this	O
theme	O
was	O
picked	O
up	O
again	O
by	O
geva	O
and	O
sitte	O
who	O
pick	O
x	O
x	O
as	O
a	O
random	O
subset	O
of	O
x	O
i	O
xn	O
and	O
allow	O
y	O
to	O
be	O
different	O
from	O
yi	O
diverging	O
a	O
bit	O
from	O
geva	O
and	O
sitte	O
we	O
might	O
minimize	O
the	O
empirical	B
error	I
with	O
the	O
prototyped	O
rule	B
over	O
all	O
y	O
y	O
where	O
the	O
empirical	B
error	I
is	O
that	O
committed	O
on	O
the	O
remaining	O
data	O
we	O
show	O
that	O
this	O
simple	O
strategy	O
leads	O
to	O
a	O
bayes-risk	O
consistent	O
rule	B
whenever	O
m	O
and	O
min	O
o	O
note	O
in	O
particular	O
that	O
we	O
may	O
take	O
x	O
xm	O
and	O
that	O
we	O
yi	O
y	O
m	O
as	O
these	O
are	O
not	O
used	O
we	O
may	O
in	O
fact	O
use	O
additional	O
data	O
with	O
missing	O
yi-values	O
for	O
this	O
purpose-the	O
unclassified	O
data	O
are	O
thus	O
efficiently	O
used	O
to	O
partition	B
the	O
space	O
let	O
be	O
the	O
empirical	B
risk	O
on	O
the	O
remaining	O
data	O
where	O
gn	O
is	O
the	O
rule	B
based	O
upon	O
yd	O
y	O
let	O
g	O
be	O
the	O
i-nn	B
rule	B
with	O
the	O
choice	O
of	O
y	O
y	O
that	O
minimizes	O
lngn	O
let	O
lg	O
denote	O
its	O
probability	O
of	O
error	O
theorem	O
lg	O
l	O
in	O
probability	O
for	O
all	O
distributions	O
whenever	O
m	O
and	O
min	O
o	O
proof	O
there	O
are	O
different	O
possible	O
functions	O
gn	O
thus	O
p	O
sp	O
ilngn	O
i	O
xl	O
xn	O
supp	O
ei	O
xl	O
xm	O
gn	O
by	O
hoeffdings	O
inequality	B
also	O
p	O
l	O
p	O
ln	O
e	O
p	O
lgn	O
e	O
p	O
l	O
e	O
sieves	O
and	O
prototypes	O
minimizes	O
lgn	O
sp	O
ilngn	O
xl	O
xn	O
we	O
used	O
lngn	O
lng	O
p	O
l	O
e	O
g	O
is	O
the	O
rule	B
based	O
on	O
zl	O
zm	O
with	O
zi	O
iyx	O
as	O
in	O
lemma	O
m	O
by	O
lemma	O
if	O
m	O
and	O
m	O
n	O
if	O
we	O
let	O
xi	O
xn	O
have	O
arbitrary	O
values-not	O
only	O
among	O
those	O
taken	O
by	O
xl	O
xn-then	O
we	O
get	O
a	O
much	O
larger	O
more	O
flexible	O
class	O
of	O
classifiers	O
for	O
ample	O
every	O
linear	O
discriminant	O
is	O
nothing	O
but	O
a	O
prototype	B
i-nn	B
rule	B
with	O
m	O
take	O
and	O
place	O
x	O
and	O
x	O
in	O
the	O
right	O
places	O
in	O
this	O
sense	O
prototype	B
rules	O
generalize	O
a	O
vast	O
class	O
of	O
rules	O
the	O
most	O
promising	O
strategy	O
of	O
choosing	O
prototypes	O
is	O
to	O
minimize	O
the	O
empirical	B
error	I
committed	O
on	O
the	O
ing	O
sequence	O
dn	O
finding	O
this	O
optimum	O
may	O
be	O
computationally	O
very	O
expensive	O
nevertheless	O
the	O
theoretical	B
properties	O
provided	O
in	O
the	O
next	O
result	O
may	O
provide	O
useful	O
guidance	O
theorem	O
let	O
c	O
be	O
the	O
class	O
of	B
nearest	B
neighbor	I
rules	I
based	O
on	O
prototype	B
pairs	O
yd	O
ym	O
m	O
where	O
the	O
yi	O
range	O
through	O
nd	O
x	O
i	O
given	O
the	O
training	O
data	O
dn	O
i	O
yd	O
yn	O
let	O
gn	O
be	O
the	O
nearest	B
neighbor	I
rule	B
from	O
c	O
minimizing	O
the	O
error	O
estimate	O
then	O
for	O
each	O
e	O
the	O
rule	B
is	O
consistent	O
if	O
m	O
such	O
that	O
m	O
log	O
m	O
on	O
for	O
d	O
and	O
d	O
condensed	B
and	O
edited	B
nearest	B
neighbor	I
rules	O
the	O
probability	O
bound	O
may	O
be	O
improved	O
significantly	O
for	O
d	O
andford	O
in	O
both	O
cases	O
the	O
rule	B
is	O
consistent	O
if	O
m	O
and	O
m	O
log	O
m	O
on	O
proof	O
it	O
follows	O
from	O
theorem	O
that	O
where	O
sc	O
n	O
is	O
the	O
n-th	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
sets	O
i	O
e	O
c	O
all	O
we	O
need	O
is	O
to	O
find	O
suitable	O
upper	O
bounds	O
for	O
s	O
n	O
each	O
classifier	B
is	O
a	O
partitioning	O
rule	B
based	O
on	O
the	O
m	O
voronoi	O
cells	O
defined	O
by	O
xl	O
x	O
m	O
therefore	O
sc	O
n	O
is	O
not	O
more	O
than	O
times	O
the	O
number	O
of	O
different	O
ways	O
n	O
points	O
in	O
nd	O
can	O
be	O
partitioned	O
by	O
voronoi	O
partitions	O
defined	O
by	O
m	O
points	O
in	O
each	O
partition	B
there	O
are	O
at	O
most	O
mm	O
cell	O
boundaries	O
that	O
are	O
subsets	O
of	O
d	O
i-dimensional	O
hyperplanes	O
thus	O
the	O
sought	O
number	O
is	O
not	O
greater	O
than	O
the	O
number	O
of	O
different	O
ways	O
mm	O
hyperplanes	O
can	O
partition	B
n	O
points	O
by	O
results	O
of	O
chapter	O
this	O
is	O
at	O
most	O
proving	O
the	O
first	O
inequality	B
the	O
other	O
two	O
inequalities	O
follow	O
by	O
sharper	O
bounds	O
on	O
the	O
number	O
of	O
cell	O
boundaries	O
for	O
d	O
this	O
is	O
clearly	O
at	O
most	O
m	O
to	O
prove	O
the	O
third	O
inequality	B
for	O
each	O
voronoi	B
partition	B
construct	O
a	O
graph	O
whose	O
vertices	O
are	O
xl	O
x	O
m	O
and	O
two	O
vertices	O
are	O
connected	O
with	O
an	O
edge	O
if	O
and	O
only	O
if	O
their	O
corresponding	O
voronoi	O
cells	O
are	O
connected	O
it	O
is	O
easy	O
to	O
see	O
that	O
this	O
graph	O
is	O
planar	O
but	O
the	O
number	O
of	O
edges	O
of	O
a	O
planar	B
graph	I
with	O
m	O
vertices	O
cannot	O
exceed	O
nishizeki	O
and	O
chiba	O
which	O
proves	O
the	O
inequality	B
the	O
consistency	B
results	O
follow	O
from	O
the	O
stated	O
inequalities	O
and	O
from	O
the	O
fact	O
that	O
inf	O
ec	O
l	O
tends	O
to	O
l	O
as	O
m	O
the	O
proof	O
of	O
theorem	O
again	O
problems	O
and	O
exercises	O
lim	O
infn-cxj	O
enn	O
whenever	O
l	O
true	O
or	O
false	O
if	O
l	O
then	O
en	O
on	O
problem	O
let	O
n	O
n	O
be	O
the	O
size	O
of	O
the	O
data	O
set	O
after	O
pure	O
condensing	O
show	O
that	O
hint	O
consider	O
the	O
real	O
line	O
and	O
note	O
that	O
all	O
points	O
whose	O
right	O
and	O
left	O
neighbors	O
are	O
of	O
the	O
same	O
class	O
are	O
eliminated	O
problem	O
let	O
yd	O
be	O
an	O
i	O
i	O
d	O
sequence	O
of	O
pairs	O
of	O
random	O
variables	O
in	O
n	O
d	O
x	O
l	O
with	O
pry	O
llx	O
x	O
ryx	O
let	O
z	O
be	O
the	O
first	O
pair	O
yi	O
in	O
problems	O
and	O
exercises	O
the	O
sequence	O
such	O
that	O
yi	O
iix	O
show	O
that	O
the	O
distribution	O
of	O
x	O
is	O
absolutely	O
continuous	O
with	O
respect	O
to	O
the	O
common	O
distribution	O
of	O
the	O
xis	O
with	O
density	O
nikodym	O
derivative	O
d	O
d	O
l	O
where	O
l	O
is	O
the	O
bayes	B
error	I
corresponding	O
to	O
yl	O
let	O
y	O
be	O
a	O
l-valued	O
random	O
variable	B
with	O
py	O
x	O
if	O
l	O
denotes	O
the	O
bayes	B
error	I
corresponding	O
to	O
y	O
then	O
show	O
that	O
l	O
l	O
problem	O
consider	O
the	O
following	O
edited	B
nn	O
rule	B
the	O
pair	O
yz	O
is	O
eliminated	O
from	O
the	O
training	O
sequence	O
if	O
the	O
k-nn	B
rule	B
on	O
the	O
remaining	O
n	O
pairs	O
incorrectly	O
classifies	O
xi	O
the	O
i-nn	B
rule	B
is	O
used	O
with	O
the	O
edited	B
data	O
show	O
that	O
this	O
rule	B
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
if	O
k	O
and	O
k	O
n	O
o	O
related	O
papers	O
wilson	O
wagner	O
penrod	O
and	O
wagner	O
and	O
devijver	O
and	O
kittler	O
tree	O
classifiers	O
classification	O
trees	O
partition	B
nd	O
into	O
regions	O
often	O
hyperrectangles	O
parallel	O
to	O
the	O
axes	O
among	O
these	O
the	O
most	O
important	O
are	O
the	O
binary	B
classification	O
trees	O
since	O
they	O
have	O
just	O
two	O
children	O
per	O
node	O
and	O
are	O
thus	O
easiest	O
to	O
manipulate	O
and	O
update	O
we	O
recall	O
the	O
simple	O
terminology	O
of	O
books	O
on	O
data	O
structures	O
the	O
top	O
of	O
a	O
binary	B
tree	O
is	O
called	O
the	O
root	O
each	O
node	O
has	O
either	O
no	O
child	O
that	O
case	O
it	O
is	O
called	O
a	O
terminal	O
node	O
or	O
leaf	O
a	O
left	O
child	O
a	O
right	O
child	O
or	O
a	O
left	O
child	O
and	O
a	O
right	O
child	O
each	O
node	O
is	O
the	O
root	O
of	O
a	O
tree	O
itself	O
the	O
trees	O
rooted	O
at	O
the	O
children	O
of	O
a	O
node	O
are	O
called	O
the	O
left	O
and	O
right	O
subtrees	O
of	O
that	O
node	O
the	O
depth	B
of	I
a	O
node	O
is	O
the	O
length	O
of	O
the	O
path	O
from	O
the	O
node	O
to	O
the	O
root	O
the	O
height	B
of	I
a	O
tree	O
is	O
the	O
maximal	O
depth	B
of	I
any	O
node	O
trees	O
with	O
more	O
than	O
two	O
children	O
per	O
node	O
can	O
be	O
reduced	O
to	O
binary	B
trees	O
by	O
root	O
figure	O
a	O
binary	B
tree	O
right	O
child	O
leaf	O
tree	O
classifiers	O
a	O
simple	O
device-just	O
associate	O
a	O
left	O
child	O
with	O
each	O
node	O
by	O
selecting	O
the	O
oldest	O
child	O
in	O
the	O
list	O
of	O
children	O
call	O
the	O
right	O
child	O
of	O
a	O
node	O
its	O
next	O
sibling	O
figures	O
and	O
the	O
new	O
binary	B
tree	O
is	O
called	O
the	O
oldest-childlnext-sibling	O
binary	B
tree	O
e	O
g	O
cormen	O
leiserson	O
and	O
rivest	O
for	O
a	O
general	O
introduction	O
we	O
only	O
mention	O
this	O
particular	O
mapping	O
because	O
it	O
enables	O
us	O
to	O
only	O
consider	O
binary	B
trees	O
for	O
simplicity	O
a	O
a	O
b	O
f	O
g	O
h	O
h	O
figure	O
ordered	B
tree	O
the	O
dren	O
are	O
ordered	B
from	O
oldest	O
to	O
youngest	O
figure	O
the	O
corresponding	O
nary	O
tree	O
in	O
a	O
classification	O
tree	O
each	O
node	O
represents	O
a	O
set	O
in	O
the	O
space	O
nd	O
also	O
each	O
node	O
has	O
exactly	O
two	O
or	O
zero	O
children	O
if	O
a	O
node	O
u	O
represents	O
the	O
set	O
a	O
and	O
its	O
children	O
u	O
u	O
represent	O
a	O
and	O
a	O
then	O
we	O
require	O
that	O
a	O
a	O
u	O
a	O
and	O
a	O
n	O
a	O
the	O
root	O
represents	O
n	O
d	O
and	O
the	O
leaves	O
taken	O
together	O
form	O
a	O
partition	B
of	O
nd	O
assume	O
that	O
we	O
know	O
x	O
e	O
a	O
then	O
the	O
question	O
x	O
e	O
a	O
should	O
be	O
answered	O
in	O
a	O
computationally	O
simple	O
manner	O
so	O
as	O
to	O
conserve	O
time	O
therefore	O
if	O
x	O
xed	O
we	O
may	O
just	O
limit	O
ourselves	O
to	O
questions	O
of	O
the	O
following	O
forms	O
is	O
xu	O
ex	O
this	O
leads	O
to	O
ordinary	B
binary	B
classification	O
trees	O
with	O
partitions	O
into	O
hyperrectangles	O
is	O
alxl	O
adxd	O
ex	O
this	O
leads	O
to	O
bsp	O
trees	O
space	O
partition	B
trees	O
each	O
decision	O
is	O
more	O
time	O
consuming	O
but	O
the	O
space	O
is	O
more	O
flexibly	O
cut	O
up	O
into	O
convex	O
polyhedral	O
cells	O
is	O
ilx	O
z	O
ex	O
z	O
is	O
a	O
point	O
of	O
n	O
d	O
to	O
be	O
picked	O
for	O
each	O
node	O
this	O
induces	O
a	O
partition	B
into	O
pieces	O
of	O
spheres	O
such	O
trees	O
are	O
called	O
sphere	B
trees	O
is	O
ljjx	O
o	O
here	O
ljj	O
is	O
a	O
nonlinear	O
function	O
different	O
for	O
each	O
node	O
every	O
classifier	B
can	O
be	O
thought	O
of	O
as	O
being	O
described	O
in	O
this	O
format-decide	O
class	O
one	O
if	O
ljjx	O
o	O
however	O
this	O
misses	O
the	O
point	O
as	O
tree	O
classifiers	O
should	O
really	O
be	O
built	O
up	O
from	O
fundamental	O
atomic	O
operations	O
and	O
queries	O
such	O
as	O
those	O
listed	O
in	O
we	O
will	O
not	O
consider	O
such	O
trees	O
any	O
further	O
tree	O
classifiers	O
i	O
i--	O
figure	O
partition	B
induced	O
by	O
an	O
ordinary	B
binary	B
tree	O
figure	O
corresponding	O
tree	O
figure	O
partition	B
induced	O
by	O
a	O
bsp	O
tree	O
figure	O
partition	B
induced	O
by	O
a	O
sphere	B
tree	O
we	O
associate	O
a	O
class	O
in	O
some	O
manner	O
with	O
each	O
leaf	O
in	O
a	O
classification	O
tree	O
the	O
tree	O
structure	O
is	O
usually	O
data	O
dependent	O
as	O
well	O
and	O
indeed	O
it	O
is	O
in	O
the	O
construction	O
itself	O
where	O
methods	O
differ	O
if	O
a	O
leaf	O
represents	O
region	O
a	O
then	O
we	O
say	O
that	O
the	O
classifier	B
gn	O
is	O
natural	O
if	O
if	O
l	O
yi	O
l	O
yi	O
x	O
e	O
a	O
ixiea	O
ixiea	O
otherwise	O
that	O
is	O
in	O
every	O
leaf	O
region	O
we	O
take	O
a	O
majority	B
vote	I
over	O
all	O
yis	O
with	O
xi	O
in	O
the	O
same	O
region	O
ties	O
are	O
broken	O
as	O
usual	O
in	O
favor	O
of	O
class	O
o	O
in	O
this	O
set-up	O
natural	O
tree	O
classifiers	O
are	O
but	O
special	O
cases	O
of	O
data-dependent	B
partitioning	O
rules	O
the	O
latter	O
are	O
further	O
described	O
in	O
detail	O
in	O
chapter	O
tree	O
classifiers	O
figure	O
a	O
natural	B
classifier	B
based	O
on	O
an	O
ordinary	B
binary	B
tree	O
the	O
sion	O
is	O
in	O
regions	O
where	O
points	O
with	O
label	O
form	O
a	O
majority	O
these	O
areas	O
are	O
shaded	O
regular	B
histograms	O
can	O
also	O
be	O
thought	O
of	O
as	O
natural	O
binary	B
tree	O
classifiers-the	O
construction	O
and	O
relationship	O
is	O
obvious	O
however	O
as	O
n	O
histograms	O
change	O
size	O
and	O
usually	O
histogram	O
partitions	O
are	O
not	O
nested	O
as	O
n	O
grows	O
trees	O
offer	O
the	O
exciting	O
perspective	O
of	O
fully	O
dynamic	O
classification-as	O
data	O
are	O
added	O
we	O
may	O
update	O
the	O
tree	O
slightly	O
say	O
by	O
splitting	O
a	O
leaf	O
or	O
so	O
to	O
obtain	O
an	O
updated	O
classifier	B
the	O
most	O
compelling	O
reason	O
for	O
using	O
binary	B
tree	O
classifiers	O
is	O
to	O
explain	O
plicated	O
data	O
and	O
to	O
have	O
a	O
classifier	B
that	O
is	O
easy	O
to	O
analyze	O
and	O
understand	O
in	O
fact	O
expert	O
system	O
design	O
is	O
based	O
nearly	O
exclusively	O
upon	O
decisions	O
obtained	O
by	O
going	O
down	O
a	O
binary	B
classification	O
tree	O
some	O
argue	O
that	O
binary	B
classification	O
trees	O
are	O
preferable	O
over	O
bsp	O
trees	O
for	O
this	O
simple	O
reason	O
as	O
argued	O
in	O
breiman	O
man	O
olshen	O
and	O
stone	O
trees	O
allow	O
mixing	O
component	O
variables	O
that	O
are	O
heterogeneous-some	O
components	O
may	O
be	O
of	O
a	O
nonnumerical	O
nature	O
others	O
may	O
represent	O
integers	O
and	O
still	O
others	O
may	O
be	O
real	O
numbers	O
invariance	O
nearly	O
all	O
rules	O
in	O
this	O
chapter	O
and	O
in	O
chapters	O
and	O
show	O
some	O
sort	O
of	O
invariance	O
with	O
respect	O
to	O
certain	O
transformations	O
of	O
the	O
input	O
this	O
is	O
often	O
a	O
major	O
asset	O
in	O
pattern	O
recognition	O
methods	O
we	O
say	O
a	O
rule	B
gn	O
is	O
invariant	O
under	O
transformation	O
t	O
if	O
for	O
all	O
values	O
of	O
the	O
arguments	O
in	O
this	O
sense	O
we	O
may	O
require	O
translation	O
invariance	O
rotation	B
invariance	O
linear	O
translation	O
invariance	O
and	O
monotone	O
transformation	B
invariance	I
maps	O
each	O
coordinate	O
separately	O
by	O
a	O
strictly	O
increasing	O
but	O
sibly	O
nonlinear	O
function	O
monotone	O
transformation	B
invariance	I
frees	O
us	O
from	O
worries	O
about	O
the	O
kind	O
of	O
measuring	O
unit	O
for	O
example	O
it	O
would	O
not	O
matter	O
whether	O
earthquakes	O
were	O
sured	O
on	O
a	O
logarithmic	O
scale	O
or	O
a	O
linear	O
scale	O
rotation	B
invariance	O
matters	O
of	O
course	O
in	O
situations	O
in	O
which	O
input	O
data	O
have	O
no	O
natural	O
coordinate	O
axis	O
system	O
in	O
many	O
cases	O
data	O
are	O
of	O
the	O
ordinal	O
form-colors	O
and	O
names	O
spring	O
to	O
mind-and	O
trees	O
with	O
the	O
x-property	B
ordinal	O
values	O
may	O
be	O
translated	O
into	O
numeric	O
values	O
by	O
creating	O
bit	O
vectors	O
here	O
distance	O
loses	O
its	O
physical	O
meaning	O
and	O
any	O
rule	B
that	O
uses	O
ordinal	O
data	O
perhaps	O
mixed	O
in	O
with	O
numerical	O
data	O
should	O
be	O
monotone	O
transformation	O
invariant	O
tree	O
methods	O
that	O
are	O
based	O
upon	O
perpendicular	O
splits	O
are	O
usually	O
not	O
ways	O
monotone	O
transformation	O
invariant	O
and	O
translation	O
invariant	O
tree	O
methods	O
based	O
upon	O
linear	O
hyperplane	O
splits	O
are	O
sometimes	O
linear	O
transformation	O
invariant	O
the	O
partitions	O
of	O
space	O
cause	O
some	O
problems	O
if	O
the	O
data	O
points	O
can	O
line	O
up	O
along	O
hyperplanes	O
this	O
is	O
just	O
a	O
matter	O
of	O
housekeeping	O
of	O
course	O
but	O
the	O
fact	O
that	O
some	O
projections	O
of	O
x	O
to	O
a	O
line	O
have	O
atoms	O
or	O
some	O
components	O
of	O
x	O
have	O
atoms	O
will	O
make	O
the	O
proofs	O
heavier	O
to	O
digest	O
for	O
this	O
reason	O
only	O
we	O
assume	O
throughout	O
this	O
chapter	O
that	O
x	O
has	O
a	O
density	O
f	O
as	O
typically	O
no	O
conditions	O
are	O
put	O
on	O
f	O
in	O
our	O
consistency	B
theorems	O
it	O
will	O
be	O
relatively	O
easy	O
to	O
generalize	O
them	O
to	O
all	O
distributions	O
the	O
density	O
assumption	O
affords	O
us	O
the	O
luxury	O
of	O
being	O
able	O
to	O
say	O
that	O
with	O
probability	O
one	O
no	O
d	O
points	O
fall	O
in	O
a	O
hyperplane	O
no	O
d	O
points	O
fall	O
iii	O
a	O
hyperplane	O
perpendicular	O
to	O
one	O
axis	O
no	O
d	O
points	O
fall	O
in	O
a	O
hyperplane	O
perpendicular	O
to	O
two	O
axes	O
etcetera	O
if	O
a	O
rule	B
is	O
monotone	O
transformation	O
invariant	O
we	O
can	O
without	O
harm	O
transform	O
all	O
the	O
data	O
as	O
follows	O
for	O
the	O
purpose	O
of	O
analysis	O
only	O
let	O
ii	O
fd	O
be	O
the	O
marginal	O
densities	O
of	O
x	O
problem	O
with	O
corresponding	O
distribution	O
functions	O
f	O
i	O
fd	O
then	O
replace	O
in	O
the	O
data	O
each	O
xi	O
by	O
txi	O
where	O
each	O
component	O
of	O
txi	O
is	O
now	O
uniformly	O
distributed	O
on	O
of	O
course	O
as	O
we	O
do	O
not	O
know	O
t	O
beforehand	O
this	O
device	O
could	O
only	O
be	O
used	O
in	O
the	O
analysis	O
the	O
transformation	O
t	O
will	O
be	O
called	O
the	O
uniform	B
marginal	I
transformation	I
observe	O
that	O
the	O
original	O
density	O
is	O
now	O
transformed	O
into	O
another	O
density	O
trees	O
with	O
the	O
x	O
it	O
is	O
possible	O
to	O
prove	O
the	O
convergence	O
of	O
many	O
tree	O
classifiers	O
all	O
at	O
once	O
what	O
is	O
needed	O
clearly	O
is	O
a	O
partition	B
into	O
small	O
regions	O
yet	O
most	O
majority	O
votes	O
should	O
be	O
over	O
sufficiently	O
large	O
sample	O
in	O
many	O
of	O
the	O
cases	O
considered	O
the	O
form	O
of	O
the	O
tree	O
is	O
determined	O
by	O
the	O
xs	O
only	O
that	O
is	O
the	O
labels	O
yi	O
do	O
not	O
playa	O
role	O
in	O
constructing	O
the	O
partition	B
but	O
they	O
are	O
used	O
in	O
voting	O
this	O
is	O
of	O
course	O
rather	O
simplistic	O
but	O
as	O
a	O
start	O
it	O
is	O
very	O
convenient	O
we	O
will	O
say	O
that	O
the	O
classification	O
tree	O
has	O
the	O
x	O
for	O
lack	O
of	O
a	O
better	O
mnemonic	O
let	O
the	O
leaf	O
regions	O
be	O
an	O
n	O
possibly	O
random	O
define	O
n	O
j	O
as	O
the	O
number	O
of	O
xis	O
falling	O
in	O
a	O
j	O
as	O
the	O
leaf	O
regions	O
form	O
a	O
partition	B
we	O
have	O
li	O
n	O
j	O
n	O
by	O
diama	O
j	O
we	O
mean	O
the	O
diameter	O
of	O
the	O
cell	O
a	O
j	O
that	O
is	O
the	O
maximal	O
distance	O
between	O
two	O
points	O
of	O
a	O
j	O
finally	O
decisions	O
are	O
taken	O
by	O
majority	B
vote	I
so	O
for	O
x	O
e	O
a	O
j	O
s	O
j	O
s	O
n	O
tree	O
classifiers	O
the	O
rule	B
is	O
i	O
gnx	O
if	O
l	O
yi	O
l	O
yi	O
x	O
e	O
a	O
j	O
ixeaj	O
ixeaj	O
otherwise	O
ax	O
denotes	O
the	O
set	O
ofthe	O
partition	B
an	O
into	O
which	O
x	O
falls	O
and	O
nx	O
is	O
the	O
number	O
of	O
data	O
points	O
falling	O
in	O
this	O
set	O
recall	O
that	O
the	O
general	O
consistency	B
result	O
given	O
in	O
theorem	O
is	O
applicable	O
in	O
such	O
cases	O
consider	O
a	O
natural	O
classification	O
tree	O
as	O
defined	O
above	O
and	O
assume	O
the	O
x	O
theorem	O
states	O
that	O
then	O
el	O
n	O
l	O
if	O
diamax	O
in	O
probability	O
nx	O
in	O
probability	O
a	O
more	O
general	O
but	O
also	O
more	O
complicated	O
consistency	B
theorem	O
is	O
proved	O
in	O
ter	O
let	O
us	O
start	O
with	O
the	O
simplest	O
possible	O
example	O
we	O
verify	O
the	O
conditions	O
of	O
theorem	O
for	O
the	O
k-spacing	B
rule	B
in	O
one	O
dimension	B
this	O
rule	B
partitions	O
the	O
real	O
line	O
by	O
using	O
the	O
k-th	O
so	O
on	O
order	B
statistics	I
see	O
also	O
parthasarathy	O
and	O
bhattacharya	O
o	O
figure	O
a	O
classifier	B
formally	O
let	O
k	O
n	O
be	O
a	O
positive	O
integer	O
and	O
let	O
xl	O
xn	O
be	O
the	O
order	B
statistics	I
of	O
the	O
data	O
points	O
recall	O
that	O
xl	O
xn	O
are	O
obtained	O
by	O
permuting	O
xl	O
xn	O
in	O
such	O
a	O
way	O
that	O
xl	O
xn	O
note	O
that	O
this	O
ordering	O
is	O
unique	O
with	O
probability	O
one	O
as	O
x	O
has	O
a	O
density	O
we	O
partition	B
r	O
into	O
n	O
intervals	O
ai	O
an	O
where	O
n	O
r	O
n	O
i	O
k	O
l	O
such	O
that	O
for	O
j	O
n	O
a	O
j	O
satisfies	O
xku-li	O
xkj	O
e	O
a	O
j	O
and	O
the	O
rightmost	O
cell	O
an	O
satisfies	O
we	O
have	O
not	O
specified	O
the	O
endpoints	O
of	O
each	O
cell	O
of	O
the	O
partition	B
for	O
simplicity	O
let	O
the	O
borders	O
between	O
a	O
j	O
and	O
a	O
ji	O
be	O
put	O
halfway	O
between	O
the	O
rightmost	O
data	O
point	O
in	O
a	O
j	O
and	O
leftmost	O
data	O
point	O
in	O
a	O
ji	O
j	O
n	O
the	O
classification	O
rule	B
gn	O
is	O
defined	O
in	O
the	O
usual	O
way	O
theorem	O
let	O
gn	O
be	O
the	O
k-spacing	O
classifier	B
given	O
above	O
assume	O
that	O
the	O
distribution	O
of	O
x	O
has	O
a	O
density	O
f	O
on	O
r	O
then	O
the	O
classification	O
rule	B
gn	O
is	O
consistent	O
that	O
is	O
limn--oo	O
el	O
n	O
l	O
if	O
k	O
and	O
kin	O
as	O
n	O
tends	O
to	O
infinity	O
trees	O
with	O
the	O
x	O
remark	O
le	O
discuss	O
various	O
generalizations	O
of	O
this	O
rule	B
in	O
chapter	O
proof	O
we	O
check	O
the	O
conditions	O
of	O
theorem	O
as	O
the	O
partition	B
has	O
the	O
x	O
condition	O
is	O
obvious	O
from	O
k	O
to	O
establish	O
condition	O
fix	O
e	O
o	O
note	O
that	O
by	O
the	O
invariance	O
of	O
the	O
rule	B
under	O
monotone	O
transformations	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
f	O
is	O
the	O
uniform	O
density	O
on	O
among	O
the	O
intervals	O
ai	O
an	O
there	O
are	O
at	O
most	O
lie	O
disjoint	O
intervals	O
of	O
length	O
greater	O
than	O
e	O
in	O
thus	O
pdiamax	O
e	O
max	O
fla	O
j	O
e	O
ljn	O
e	O
mx	O
flna	O
j	O
m	O
ax	O
ifla	O
j	O
h	O
e	O
hp	O
ifla	O
flnai	O
ljn	O
ljn	O
flna	O
ji	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
intervals	O
in	O
r	O
the	O
first	O
term	O
within	O
the	O
parentheses	O
converges	O
to	O
zero	O
by	O
the	O
second	O
condition	O
of	O
the	O
theorem	O
while	O
the	O
second	O
term	O
goes	O
to	O
zero	O
by	O
an	O
obvious	O
extension	O
of	O
the	O
classical	O
glivenko-cantelli	B
theorem	I
this	O
completes	O
the	O
proof	O
we	O
will	O
encounter	O
several	O
trees	O
in	O
which	O
the	O
partition	B
is	O
determined	O
by	O
a	O
small	O
fraction	O
of	O
the	O
data	O
such	O
as	O
binary	B
k	O
trees	O
and	O
quadtrees	O
in	O
these	O
cases	O
condition	O
of	O
theorem	O
may	O
be	O
verified	O
with	O
the	O
help	O
of	O
the	O
following	O
lemma	O
lemma	O
let	O
pi	O
pk	O
be	O
a	O
probability	O
vector	O
let	O
n	O
l	O
nk	O
be	O
mially	O
distributed	O
random	O
variables	O
with	O
parameters	O
n	O
and	O
pi	O
pk	O
then	O
if	O
the	O
random	O
variable	B
x	O
is	O
independent	O
oj	O
n	O
l	O
nb	O
andpx	O
i	O
pi	O
we	O
have	O
for	O
any	O
m	O
pnx	O
m	O
n	O
this	O
probability	O
goes	O
to	O
if	O
kin	O
uniformly	O
over	O
all	O
probability	O
vectors	O
with	O
k	O
components	O
proof	O
let	O
zi	O
be	O
binomial	B
with	O
parameters	O
n	O
and	O
pi	O
then	O
pnx	O
m	O
n	O
l	O
pipzi	O
ezi	O
m	O
npi	O
n	O
c	O
e	O
p	O
p	O
z	O
l	O
i	O
tree	O
classifiers	O
p-	O
var	O
zd	O
inm	O
i	O
n	O
chebyshevs	O
inequality	B
l	O
n	O
inpi	O
npi	O
o	O
n	O
the	O
previous	O
lemma	O
implies	O
that	O
for	O
any	O
binary	B
tree	O
classifier	B
constructed	O
on	O
the	O
basis	O
of	O
xl	O
x	O
k	O
with	O
k	O
regions	O
nx	O
in	O
probability	O
whenever	O
kln	O
k	O
kin	O
it	O
suffices	O
to	O
note	O
that	O
we	O
may	O
take	O
m	O
arbitrarily	O
large	O
but	O
fixed	O
in	O
lemma	O
this	O
remark	O
saves	O
us	O
the	O
trouble	O
of	O
having	O
to	O
verify	O
just	O
how	O
large	O
or	O
small	O
the	O
probability	O
mass	O
of	O
the	O
region	O
is	O
in	O
fact	O
it	O
also	O
implies	O
that	O
we	O
should	O
not	O
worry	O
so	O
much	O
about	O
regions	O
with	O
few	O
data	O
points	O
what	O
matters	O
more	O
than	O
anything	O
else	O
is	O
the	O
number	O
of	O
regions	O
stopping	O
rules	O
based	O
upon	O
cardinalities	O
of	O
regions	O
can	O
effectively	O
be	O
dropped	O
in	O
many	O
cases	O
balanced	B
search	O
trees	O
balanced	B
multidimensional	O
search	O
trees	O
are	O
computationally	O
attractive	O
binary	B
trees	O
with	O
n	O
leaves	O
have	O
n	O
height	O
for	O
example	O
when	O
at	O
each	O
node	O
the	O
size	O
of	O
every	O
subtree	O
is	O
at	O
least	O
ex	O
times	O
the	O
size	O
of	O
the	O
other	O
subtree	O
rooted	O
at	O
the	O
parent	O
for	O
some	O
constant	O
ex	O
o	O
it	O
is	O
thus	O
important	O
to	O
verify	O
the	O
consistency	B
of	O
balanced	B
search	O
trees	O
used	O
in	O
classification	O
we	O
again	O
consider	O
binary	B
classification	O
trees	O
with	O
the	O
x	O
and	O
majority	O
votes	O
over	O
the	O
leaf	O
regions	O
take	O
for	O
example	O
a	O
tree	O
in	O
which	O
we	O
split	O
every	O
node	O
perfectly	O
that	O
is	O
if	O
there	O
are	O
n	O
points	O
we	O
find	O
the	O
median	O
according	O
to	O
one	O
coordinate	O
and	O
create	O
two	O
subtrees	O
of	O
sizes	O
ln	O
and	O
rn	O
the	O
median	O
itself	O
stays	O
behind	O
and	O
is	O
not	O
sent	O
down	O
to	O
the	O
subtrees	O
repeat	O
this	O
for	O
k	O
levels	O
of	O
nodes	O
at	O
each	O
level	O
cutting	O
along	O
the	O
next	O
coordinate	O
axe	O
in	O
a	O
rotational	O
manner	O
this	O
leads	O
to	O
leaf	O
regions	O
each	O
having	O
at	O
least	O
n	O
k	O
points	O
and	O
at	O
most	O
points	O
such	O
a	O
tree	O
will	O
be	O
called	O
a	O
median	B
tree	I
figure	O
median	B
tree	I
withfour	O
leaf	O
regions	O
in	O
setting	O
up	O
such	O
a	O
tree	O
is	O
very	O
easy	O
and	O
hence	O
such	O
trees	O
may	O
appeal	O
to	O
certain	O
grammers	O
in	O
hypothesis	O
testing	O
median	O
trees	O
were	O
studied	O
by	O
anderson	O
theorem	O
natural	O
classifiers	O
based	O
upon	O
median	O
trees	O
with	O
k	O
levels	O
leaf	O
regions	O
are	O
consistent	O
ln	O
l	O
whenever	O
x	O
has	O
a	O
density	O
if	O
balanced	B
search	O
trees	O
n	O
and	O
k-oo	O
the	O
conditions	O
of	O
k	O
are	O
fulfilled	O
if	O
k	O
n	O
n	O
k	O
we	O
may	O
prove	O
the	O
theorem	O
by	O
checking	O
the	O
conditions	O
of	O
theorem	O
condition	O
follows	O
trivially	O
by	O
the	O
fact	O
that	O
each	O
leaf	O
region	O
contains	O
at	O
least	O
n	O
k	O
points	O
and	O
the	O
condition	O
thus	O
we	O
need	O
only	O
verify	O
the	O
first	O
condition	O
of	O
theorem	O
to	O
make	O
the	O
proof	O
more	O
transparent	O
we	O
first	O
analyze	O
a	O
closely	O
related	O
hypothetical	O
tree	O
the	O
theoretical	B
median	B
tree	I
also	O
we	O
restrict	O
the	O
analysis	O
to	O
d	O
the	O
multidimensional	O
extension	O
is	O
straightforward	O
the	O
theoretical	B
median	B
tree	I
rotates	O
through	O
the	O
coordinates	O
and	O
cuts	O
each	O
hyperrectangle	O
precisely	O
so	O
that	O
the	O
two	O
new	O
hyperrectangles	O
have	O
equal	O
jl-measure	O
figure	O
theoretical	B
median	B
tree	I
with	O
three	O
levels	O
of	O
cuts	O
i	O
observe	O
that	O
the	O
rule	B
is	O
invariant	O
under	O
monotone	O
transformations	O
of	O
the	O
coordinate	O
axes	O
recall	O
that	O
in	O
such	O
cases	O
there	O
is	O
no	O
harm	O
in	O
assuming	O
that	O
the	O
marginal	O
distributions	O
are	O
all	O
uniform	O
on	O
we	O
let	O
vd	O
denote	O
the	O
horizontal	O
and	O
vertical	O
sizes	O
of	O
the	O
rectangles	O
after	O
k	O
levels	O
of	O
cuts	O
of	O
course	O
we	O
begin	O
with	O
hi	O
vi	O
when	O
k	O
o	O
we	O
now	O
show	O
that	O
for	O
the	O
theoretical	B
median	B
tree	I
diamax	O
in	O
probability	O
as	O
k	O
note	O
that	O
diamax	O
hx	O
vex	O
where	O
hx	O
and	O
vex	O
are	O
the	O
horizontal	O
and	O
vertical	O
sizes	O
of	O
the	O
rectangle	O
ax	O
we	O
show	O
that	O
if	O
k	O
is	O
even	O
ehx	O
vex	O
from	O
which	O
the	O
claim	O
follows	O
after	O
the	O
k-th	O
round	O
of	O
splits	O
since	O
rectangles	O
have	O
equal	O
probability	O
measure	O
we	O
have	O
apply	O
another	O
round	O
of	O
splits	O
all	O
vertical	O
then	O
each	O
term	O
vj	O
spawns	O
so	O
to	O
speak	O
two	O
new	O
rectangles	O
with	O
horizontal	O
and	O
vertical	O
sizes	O
vi	O
and	O
tree	O
classifiers	O
vi	O
with	O
hi	O
h	O
h	O
that	O
contribute	O
the	O
next	O
round	O
yields	O
horizontal	O
splits	O
with	O
total	O
contribution	O
now	O
figure	O
v	O
h	O
v	O
h	O
v	O
h	O
v	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
vi	O
thus	O
over	O
two	O
iterations	O
of	O
splits	O
we	O
see	O
that	O
e	O
h	O
v	O
is	O
halved	O
and	O
the	O
claim	O
follows	O
by	O
simple	O
induction	O
we	O
show	O
now	O
what	O
happens	O
in	O
the	O
real	O
median	B
tree	I
when	O
cuts	O
are	O
based	O
upon	O
a	O
random	O
sample	O
we	O
deviate	O
of	O
course	O
from	O
the	O
theoretical	B
median	B
tree	I
but	O
consistency	B
is	O
preserved	O
the	O
reason	O
seen	O
intuitively	O
is	O
that	O
if	O
the	O
number	O
of	O
points	O
in	O
a	O
cell	O
is	O
large	O
then	O
the	O
sample	O
median	O
will	O
be	O
close	O
to	O
the	O
theoretical	B
median	O
so	O
that	O
the	O
shrinking-diameter	O
property	O
is	O
preserved	O
the	O
methodology	O
followed	O
here	O
shows	O
how	O
one	O
may	O
approach	O
the	O
analysis	O
in	O
general	O
by	O
separating	O
the	O
theoretical	B
model	O
from	O
the	O
sample-based	O
model	O
proof	O
of	O
theorem	O
as	O
we	O
noted	O
before	O
all	O
we	O
have	O
to	O
show	O
is	O
that	O
diamax	O
in	O
probability	O
again	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
marginals	O
of	O
x	O
are	O
uniform	O
and	O
that	O
d	O
again	O
we	O
show	O
that	O
ehx	O
vex	O
o	O
figure	O
a	O
rectangle	O
after	O
two	O
rounds	O
asplits	O
h	O
i	O
p	O
i	O
v	O
i	O
h	O
i	O
p	O
i	O
hi	O
tvt	O
v	O
i	O
i	O
p	O
tvy	O
i	O
h	O
p	O
v	O
i	O
i	O
h	O
i	O
if	O
a	O
rectangle	O
of	O
probability	O
mass	O
pi	O
and	O
sizes	O
hi	O
vi	O
is	O
split	O
into	O
four	O
rectangles	O
as	O
in	O
figure	O
with	O
probability	O
masses	O
p	O
p	O
p	O
pt	O
then	O
the	O
contribution	O
pichi	O
vi	O
to	O
ehx	O
vex	O
becomes	O
after	O
two	O
levels	O
of	O
cuts	O
this	O
does	O
not	O
exceed	O
if	O
and	O
balanced	B
search	O
trees	O
v	O
e	O
pi	O
i	O
max	O
pi	O
pi	O
pi	O
ii	O
iii	O
i	O
max	O
pi	O
pi	O
v	O
e	O
pi	O
pi	O
maxp	O
plli	O
z	O
z	O
pz	O
pz	O
that	O
is	O
when	O
all	O
three	O
cuts	O
are	O
within	O
of	O
the	O
true	O
median	O
we	O
call	O
such	O
cuts	O
good	O
if	O
all	O
cuts	O
are	O
good	O
we	O
thus	O
note	O
that	O
in	O
two	O
levels	O
of	O
cuts	O
ehx	O
vex	O
is	O
reduced	O
by	O
also	O
all	O
pis	O
decrease	O
at	O
a	O
controlled	O
rate	O
let	O
g	O
be	O
the	O
event	O
that	O
all	O
cuts	O
in	O
a	O
median	B
tree	I
with	O
k	O
levels	O
are	O
good	O
then	O
at	O
level	O
k	O
all	O
ps	O
are	O
at	O
most	O
thus	O
lpihi	O
vi	O
il	O
since	O
ll	O
vj	O
if	O
k	O
is	O
even	O
hence	O
after	O
k	O
levels	O
of	O
cuts	O
the	O
last	O
term	O
tends	O
to	O
zero	O
if	O
e	O
is	O
small	O
enough	O
we	O
bound	O
p	O
g	O
e	O
by	O
times	O
the	O
probability	O
that	O
one	O
cut	O
is	O
bad	O
let	O
us	O
cut	O
a	O
cell	O
with	O
n	O
points	O
and	O
probability	O
content	O
p	O
in	O
a	O
given	O
direction	O
a	O
quick	O
check	O
of	O
the	O
median	B
tree	I
shows	O
that	O
given	O
the	O
position	O
and	O
size	O
of	O
the	O
cell	O
the	O
n	O
points	O
inside	O
the	O
cell	O
are	O
distributed	O
in	O
an	O
i	O
i	O
d	O
manner	O
according	O
to	O
the	O
restriction	O
of	O
fl	O
to	O
the	O
cell	O
after	O
the	O
cut	O
we	O
have	O
and	O
rn	O
points	O
in	O
the	O
new	O
cells	O
and	O
probability	O
contents	O
ln	O
pi	O
and	O
p	O
it	O
is	O
clear	O
that	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
p	O
thus	O
if	O
all	O
points	O
are	O
projected	O
down	O
in	O
the	O
direction	O
of	O
the	O
cut	O
and	O
f	O
and	O
f	O
n	O
denote	O
the	O
distribution	B
function	I
and	O
empirical	B
distribution	B
function	I
of	O
the	O
obtained	O
one-dimensional	O
data	O
then	O
p	O
is	O
not	O
goodl	O
n	O
p	O
p	O
n	O
i	O
ii	O
tree	O
classifiers	O
phpfx-fnx-lin	O
theorem	O
n	O
k	O
hence	O
for	O
n	O
large	O
enough	O
binary	B
search	O
trees	O
the	O
simplest	O
trees	O
to	O
analyze	O
are	O
those	O
whose	O
structure	O
depends	O
in	O
a	O
straightforward	O
way	O
on	O
the	O
data	O
to	O
make	O
this	O
point	O
we	O
begin	O
with	O
the	O
binary	B
search	B
tree	I
and	O
its	O
multivariate	O
extension	O
the	O
k-d	B
tree	I
cormen	O
leiserson	O
and	O
rivest	O
for	O
the	O
binary	B
search	B
tree	I
for	O
multivariate	O
binary	B
trees	O
we	O
refer	O
to	O
samet	O
a	O
full-fledged	O
k-d	B
tree	I
is	O
defined	O
as	O
follows	O
we	O
promote	O
the	O
first	O
data	O
point	O
xl	O
to	O
the	O
root	O
and	O
partition	B
xn	O
into	O
two	O
sets	O
those	O
whose	O
first	O
coordinate	O
exceeds	O
that	O
of	O
xl	O
and	O
the	O
remaining	O
points	O
within	O
each	O
set	O
points	O
are	O
ordered	B
by	O
original	O
index	O
the	O
former	O
set	O
is	O
used	O
to	O
build	O
the	O
right	O
subtree	O
of	O
xl	O
and	O
the	O
latter	O
to	O
construct	O
the	O
left	O
subtree	O
of	O
xl	O
for	O
each	O
subtree	O
the	O
same	O
construction	O
is	O
applied	O
recursively	O
with	O
only	O
one	O
variant	O
at	O
depth	O
l	O
in	O
the	O
tree	O
the	O
mod	O
d	O
l-st	O
coordinate	O
is	O
used	O
to	O
split	O
the	O
data	O
in	O
this	O
manner	O
we	O
rotate	O
through	O
the	O
coordinate	O
axes	O
periodically	O
attach	O
to	O
each	O
leaf	O
two	O
new	O
nodes	O
and	O
to	O
each	O
node	O
with	O
one	O
child	O
a	O
ond	O
child	O
call	O
these	O
new	O
nodes	O
external	O
nodes	O
each	O
of	O
the	O
n	O
external	O
nodes	O
correspond	O
to	O
a	O
region	O
of	O
r	O
d	O
and	O
collectively	O
the	O
external	O
nodes	O
define	O
a	O
tion	O
ofrd	O
we	O
define	O
exactly	O
what	O
happens	O
on	O
the	O
boundaries	O
between	O
regions	O
binary	B
search	O
trees	O
figure	O
a	O
k-d	B
tree	I
random	O
points	O
on	O
the	O
plane	O
and	O
the	O
induced	O
partition	B
put	O
differently	O
we	O
may	O
look	O
at	O
the	O
external	O
nodes	O
as	O
the	O
leaves	O
of	O
a	O
new	O
tree	O
with	O
nodes	O
and	O
declare	O
this	O
new	O
tree	O
to	O
be	O
our	O
new	O
binary	B
classification	O
tree	O
as	O
there	O
are	O
n	O
leaf	O
regions	O
and	O
n	O
data	O
points	O
the	O
natural	O
binary	B
tree	O
classifier	B
it	O
induces	O
is	O
degenerate-indeed	O
all	O
external	O
regions	O
contain	O
very	O
few	O
points	O
clearly	O
we	O
must	O
have	O
a	O
mechanism	O
for	O
trimming	O
the	O
tree	O
to	O
insure	O
better	O
populated	O
leaves	O
let	O
us	O
look	O
at	O
just	O
three	O
naive	B
strategies	O
for	O
convenience	O
we	O
assume	O
that	O
the	O
data	O
points	O
determining	O
the	O
tree	O
are	O
not	O
counted	O
when	O
taking	O
a	O
majority	B
vote	I
over	O
the	O
cells	O
as	O
the	O
number	O
of	O
these	O
points	O
is	O
typically	O
much	O
smaller	O
than	O
n	O
this	O
restriction	O
does	O
not	O
make	O
a	O
significant	O
difference	O
fix	O
k	O
n	O
and	O
construct	O
a	O
k	O
tree	O
with	O
k	O
internal	O
nodes	O
and	O
k	O
external	O
nodes	O
based	O
on	O
the	O
first	O
k	O
data	O
points	O
xl	O
xk	O
classify	O
by	O
majority	B
vote	I
over	O
all	O
k	O
regions	O
as	O
in	O
natural	O
classification	O
tees	O
the	O
data	O
pairs	O
ykd	O
yn	O
into	O
account	O
call	O
this	O
the	O
chronological	B
k-d	B
tree	I
fix	O
k	O
and	O
truncate	O
the	O
k-d	B
tree	I
to	O
k	O
levels	O
all	O
nodes	O
at	O
level	O
k	O
are	O
declared	O
leaves	O
and	O
classification	O
is	O
again	O
by	O
majority	B
vote	I
over	O
the	O
leaf	O
regions	O
call	O
this	O
the	O
deep	B
k-d	B
tree	I
fix	O
k	O
and	O
trim	O
the	O
tree	O
so	O
that	O
each	O
node	O
represents	O
at	O
least	O
k	O
points	O
in	O
the	O
original	O
construction	O
consider	O
the	O
maximal	O
such	O
tree	O
the	O
number	O
of	O
regions	O
here	O
is	O
random	O
with	O
between	O
and	O
nj	O
k	O
regions	O
call	O
this	O
the	O
populated	O
k-d	B
tree	I
tree	O
classifiers	O
let	O
the	O
leaf	O
regions	O
be	O
an	O
n	O
possibly	O
random	O
and	O
denote	O
the	O
leaf	O
nodes	O
by	O
ui	O
un	O
the	O
strict	O
descendants	O
of	O
ui	O
in	O
the	O
full	O
k-d	B
tree	I
have	O
indices	O
that	O
we	O
will	O
collect	O
in	O
an	O
index	O
set	O
ii	O
define	O
i	O
ii	O
i	O
ni	O
as	O
the	O
leaf	O
regions	O
form	O
a	O
partition	B
we	O
have	O
n	O
lnin-n	O
il	O
because	O
the	O
leaf	O
nodes	O
themselves	O
are	O
not	O
counted	O
in	O
ii	O
voting	O
is	O
by	O
majority	B
vote	I
so	O
the	O
rule	B
is	O
the	O
chronological	B
k-d	B
tree	I
here	O
we	O
have	O
n	O
k	O
also	O
l	O
are	O
distributed	O
as	O
uniform	O
spacings	O
that	O
is	O
if	O
u	O
i	O
uk	O
are	O
i	O
i	O
d	O
uniform	O
random	O
variables	O
ing	O
k	O
spacings	O
ul	O
ul	O
uk	O
uk-i	O
uk	O
by	O
their	O
order	B
statistics	I
ul	O
uk	O
then	O
these	O
spacings	O
are	O
jointly	O
distributed	O
as	O
this	O
can	O
be	O
shown	O
by	O
induction	O
when	O
is	O
added	O
first	O
picks	O
a	O
spacing	B
with	O
probability	O
equal	O
to	O
the	O
size	O
of	O
the	O
spacing	B
then	O
it	O
cuts	O
that	O
spacing	B
in	O
a	O
uniform	O
manner	O
as	O
the	O
same	O
is	O
true	O
when	O
the	O
chronological	B
k-d	B
tree	I
grows	O
by	O
one	O
leaf	O
the	O
property	O
follows	O
by	O
induction	O
on	O
k	O
theorem	O
we	O
have	O
e	O
ln	O
l	O
for	O
all	O
distributions	O
of	O
x	O
with	O
a	O
density	O
for	O
the	O
chronological	B
k-d	B
tree	I
classifier	B
whenever	O
k	O
and	O
kin	O
o	O
proof	O
we	O
verify	O
the	O
conditions	O
of	O
theorem	O
as	O
the	O
number	O
of	O
regions	O
is	O
k	O
and	O
the	O
partition	B
is	O
determined	O
by	O
the	O
first	O
k	O
data	O
points	O
condition	O
immediately	O
follows	O
from	O
lemma	O
and	O
the	O
remark	O
following	O
it	O
condition	O
of	O
theorem	O
requires	O
significantly	O
more	O
work	O
we	O
verify	O
dition	O
for	O
d	O
leaving	O
the	O
straightforward	O
extension	O
to	O
r	O
d	O
d	O
to	O
the	O
reader	O
throughout	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
marginal	O
butions	O
are	O
uniform	O
we	O
may	O
do	O
so	O
by	O
the	O
invariance	O
properties	O
discussed	O
earlier	O
fix	O
a	O
point	O
x	O
e	O
rd	O
we	O
insert	O
points	O
xl	O
xk	O
into	O
an	O
initially	O
empty	O
k	O
tree	O
and	O
let	O
r	O
rk	O
be	O
the	O
rectangles	O
containing	O
x	O
just	O
after	O
these	O
points	O
were	O
inserted	O
note	O
that	O
rl	O
rk	O
assume	O
for	O
simplicity	O
that	O
the	O
integer	O
k	O
is	O
a	O
perfect	O
cube	O
and	O
set	O
l	O
k	O
define	O
the	O
distances	O
from	O
x	O
to	O
the	O
sides	O
of	O
ri	O
by	O
hi	O
h	O
vi	O
v	O
figure	O
m	O
k	O
the	O
chronological	B
k-d	B
tree	I
figure	O
a	O
rectangle	O
ri	O
containing	O
x	O
with	O
its	O
distances	O
to	O
the	O
sides	O
hi	O
v	O
i	O
x	O
h	O
vi	O
we	O
construct	O
a	O
sequence	O
of	O
events	O
that	O
forces	O
the	O
diameter	O
of	O
rk	O
to	O
be	O
small	O
with	O
high	O
probability	O
let	O
e	O
be	O
a	O
small	O
positive	O
number	O
to	O
be	O
specified	O
later	O
denote	O
the	O
four	O
squares	O
with	O
opposite	O
vertices	O
x	O
x	O
e	O
e	O
by	O
then	O
define	O
the	O
following	O
events	O
el	O
n	O
n	O
xd	O
il	O
h	O
h	O
e	O
v	O
maxh	O
h	O
e	O
maxv	O
v	O
h	O
h	O
least	O
three	O
of	O
vm	O
v	O
hm	O
hl	O
are	O
e	O
v	O
hm	O
hs	O
e	O
v	O
hk	O
hd	O
e	O
if	O
or	O
hold	O
then	O
diamr	O
k	O
assume	O
that	O
we	O
find	O
a	O
set	O
bend	O
such	O
that	O
px	O
e	O
b	O
and	O
for	O
all	O
x	O
e	O
b	O
and	O
sufficiently	O
small	O
e	O
u	O
u	O
as	O
k	O
then	O
by	O
the	O
lebesgue	O
dominated	B
convergence	I
theorem	I
diamax	O
in	O
probability	O
and	O
condition	O
of	O
theorem	O
would	O
follow	O
in	O
the	O
remaining	O
part	O
of	O
the	O
proof	O
we	O
define	O
such	O
a	O
set	O
b	O
and	O
show	O
that	O
u	O
u	O
this	O
will	O
require	O
some	O
work	O
we	O
define	O
the	O
set	O
b	O
in	O
terms	O
of	O
the	O
density	O
f	O
of	O
x	O
x	O
e	O
b	O
if	O
and	O
only	O
if	O
fe	O
i	O
fzdz	O
for	O
all	O
e	O
small	O
enough	O
f	O
fzdz	O
r	O
ar	O
fx	O
for	O
all	O
e	O
small	O
enough	O
inf	O
rectangles	O
r	O
containing	O
x	O
of	O
diameter	O
fx	O
o	O
that	O
px	O
e	O
b	O
follows	O
from	O
a	O
property	O
of	O
the	O
support	B
a	O
corollary	O
of	O
the	O
lessen-marcinkiewicz-zygmund	O
theorem	O
this	O
implies	O
for	O
almost	O
all	O
x	O
and	O
the	O
fact	O
that	O
for	O
j	O
i-almost	O
all	O
x	O
f	O
o	O
it	O
is	O
easy	O
to	O
verify	O
the	O
following	O
pe	O
ped	O
n	O
e	O
n	O
e	O
tree	O
classifiers	O
n	O
e	O
n	O
n	O
e	O
n	O
e	O
n	O
n	O
n	O
e	O
n	O
we	O
show	O
that	O
each	O
term	O
tends	O
to	O
at	O
x	O
e	O
b	O
term	O
by	O
the	O
union	O
bound	O
pef	O
ti	O
ci	O
ti	O
cd	O
il	O
il	O
exp	O
min	O
lsia	O
by	O
part	O
of	O
the	O
definition	B
of	I
b	O
term	O
by	O
a	O
simple	O
geometric	B
argument	O
hence	O
pei	O
n	O
en	O
o	O
term	O
to	O
show	O
that	O
n	O
e	O
n	O
n	O
e	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
maxvz	O
v	O
e	O
while	O
maxhz	O
hi	O
a	O
e	O
let	O
be	O
a	O
subset	O
of	O
i	O
i	O
m	O
consisting	O
of	O
those	O
xis	O
that	O
fall	O
in	O
re	O
we	O
introduce	O
three	O
notions	O
in	O
this	O
sequence	O
first	O
zi	O
is	O
the	O
absolute	O
value	O
of	O
the	O
difference	O
of	O
the	O
of	O
x	O
and	O
x	O
let	O
wi	O
be	O
the	O
absolute	O
value	O
of	O
the	O
difference	O
of	O
the	O
xl-coordinates	O
of	O
x	O
and	O
x	O
we	O
re-index	O
the	O
sequence	O
x	O
wi	O
and	O
zi	O
so	O
that	O
i	O
runs	O
from	O
to	O
n	O
where	O
n	O
ixertl	O
m	O
izl	O
to	O
avoid	O
trivialities	O
assume	O
that	O
n	O
will	O
be	O
shown	O
to	O
happen	O
with	O
ability	O
tending	O
to	O
one	O
call	O
x	O
a	O
record	B
if	O
zi	O
minzl	O
zi	O
call	O
x	O
a	O
good	O
point	O
if	O
wi	O
e	O
an	O
x	O
causes	O
minhm	O
hl	O
e	O
if	O
that	O
x	O
is	O
a	O
good	O
point	O
and	O
a	O
record	B
and	O
if	O
it	O
defines	O
a	O
vertical	O
cut	O
the	O
alternating	O
nature	O
of	O
the	O
cuts	O
makes	O
our	O
analysis	O
a	O
bit	O
heavier	O
than	O
needed	O
we	O
show	O
here	O
what	O
happens	O
when	O
all	O
directions	O
are	O
picked	O
independently	O
of	O
each	O
other	O
leaving	O
the	O
rotating-cuts	O
case	O
to	O
the	O
reader	O
thus	O
if	O
we	O
set	O
si	O
i	O
is	O
a	O
record	B
x	O
defines	O
a	O
vertical	O
cut	O
we	O
have	O
the	O
chronological	B
k-d	B
tree	I
re-index	O
again	O
and	O
let	O
x	O
x	O
all	O
be	O
records	O
note	O
that	O
given	O
x	O
xl	O
is	O
distributed	O
according	O
to	O
f	O
restricted	O
to	O
the	O
rectangle	O
r	O
of	O
height	O
minv	O
ziabove	O
x	O
height	O
min	O
z	O
i	O
below	O
x	O
width	O
hz	O
to	O
the	O
left	O
of	O
x	O
width	O
h	O
to	O
the	O
right	O
of	O
x	O
call	O
these	O
four	O
quantities	O
v	O
v	O
h	O
h	O
respectively	O
then	O
pw	O
eix	O
efx	O
v	O
v	O
because	O
the	O
marginal	O
distribution	O
of	O
an	O
independent	O
x	O
is	O
uniform	O
and	O
thus	O
p	O
e	O
rir	O
v	O
v	O
while	O
by	O
property	O
of	O
b	O
e	O
r	O
wi	O
eir	O
recall	O
the	O
re-indexing	O
let	O
m	O
be	O
the	O
number	O
of	O
records	O
m	O
is	O
the	O
length	O
of	O
our	O
sequence	O
x	O
then	O
e	O
n	O
n	O
n	O
e	O
n	O
oj	O
iwsix	O
o	O
but	O
as	O
cuts	O
have	O
independently	O
picked	O
directions	O
and	O
since	O
eixj	O
we	O
see	O
that	O
n	O
n	O
n	O
n	O
o	O
e	O
c	O
c	O
efx	O
m	O
fno	O
we	O
rewrite	O
m	O
fix	O
is	O
a	O
record	B
and	O
recall	O
that	O
the	O
indicator	O
variables	O
in	O
this	O
sum	O
are	O
independent	O
and	O
are	O
of	O
mean	O
i	O
i	O
hence	O
for	O
c	O
e	O
ll	O
iii	O
x	O
e-x	O
e	O
ii	O
i	O
logn	O
l	O
the	O
latter	O
formula	O
remains	O
valid	O
even	O
if	O
n	O
o	O
thus	O
with	O
c	O
pie	O
n	O
n	O
n	O
ej	O
e	O
efxr	O
tree	O
classifiers	O
l	O
flrz	O
we	O
know	O
from	O
the	O
introduction	O
n	O
is	O
binomial	B
with	O
parameters	O
of	O
this	O
section	O
that	O
flrt	O
is	O
distributed	O
as	O
the	O
minimum	O
of	O
l	O
i	O
i	O
d	O
uniform	O
random	O
variables	O
thus	O
for	O
eflrz	O
and	O
pflrz	O
l	O
hence	O
e	O
pflrz	O
i	O
e	O
i	O
ll-c	O
l	O
l-c	O
p	O
binomialm	O
l	O
i	O
l	O
the	O
first	O
term	O
is	O
small	O
by	O
choice	O
of	O
the	O
second	O
one	O
is	O
the	O
third	O
one	O
is	O
bounded	O
from	O
above	O
by	O
chebyshevs	O
inequality	B
by	O
i	O
term	O
this	O
term	O
is	O
handled	O
exactly	O
as	O
term	O
note	O
however	O
that	O
i	O
and	O
m	O
now	O
become	O
m	O
and	O
k	O
respectively	O
the	O
convergence	O
to	O
requires	O
now	O
m	O
i	O
m	O
which	O
is	O
still	O
the	O
case	O
this	O
concludes	O
the	O
proof	O
of	O
theorem	O
the	O
deep	B
k-d	B
tree	I
theorem	O
the	O
deep	B
k-d	B
tree	I
classifier	B
is	O
consistent	O
el	O
n	O
l	O
for	O
all	O
distributions	O
such	O
that	O
x	O
has	O
a	O
density	O
whenever	O
lim	O
k	O
and	O
n----oo	O
k	O
hmsup--	O
n----oo	O
log	O
n	O
proof	O
in	O
problem	O
you	O
are	O
asked	O
to	O
show	O
that	O
k	O
implies	O
diamax	O
in	O
probability	O
theorem	O
may	O
be	O
invoked	O
here	O
we	O
now	O
show	O
that	O
the	O
assumption	O
lim	O
supn----oo	O
kl	O
log	O
n	O
implies	O
that	O
nx	O
in	O
probability	O
let	O
d	O
be	O
the	O
depth	O
from	O
the	O
root	O
of	O
x	O
when	O
x	O
is	O
inserted	O
into	O
a	O
k-d	B
tree	I
having	O
n	O
elements	O
clearly	O
nx	O
d	O
k	O
so	O
it	O
suffices	O
to	O
show	O
that	O
d	O
k	O
in	O
probability	O
we	O
know	O
that	O
d	O
log	O
n	O
in	O
probability	O
e	O
g	O
devroye	O
and	O
problem	O
this	O
concludes	O
the	O
proof	O
of	O
the	O
theorem	O
quadtrees	O
quadtrees	O
quadtrees	O
or	O
hyperquadtrees	O
are	O
unquestionably	O
the	O
most	O
prominent	O
trees	O
in	O
puter	O
graphics	O
easy	O
to	O
manipulate	O
and	O
compact	O
to	O
store	O
they	O
have	O
found	O
their	O
way	O
into	O
mainstream	O
computer	O
science	O
discovered	O
by	O
finkel	O
and	O
bentley	O
and	O
surveyed	O
by	O
samet	O
they	O
take	O
several	O
forms	O
we	O
are	O
given	O
dimensional	O
data	O
x	O
i	O
x	O
n	O
the	O
tree	O
is	O
constructed	O
as	O
the	O
k	O
tree	O
in	O
particular	O
xl	O
becomes	O
the	O
root	O
of	O
the	O
tree	O
it	O
partitions	O
x	O
xn	O
into	O
empty	O
sets	O
according	O
to	O
membership	O
in	O
one	O
of	O
the	O
quadrants	O
centered	O
at	O
xl	O
figure	O
r---	O
i	O
t	O
figure	O
quadtree	B
and	O
the	O
induced	O
partition	B
of	O
the	O
points	O
on	O
the	O
right	O
are	O
shown	O
in	O
the	O
position	O
in	O
space	O
the	O
root	O
is	O
specially	O
marked	O
the	O
partitioning	O
process	O
is	O
repeated	O
at	O
the	O
child	O
nodes	O
until	O
a	O
certain	O
stopping	O
rule	B
is	O
satisfied	O
in	O
analogy	O
with	O
the	O
k-d	B
tree	I
we	O
may	O
define	O
the	O
chronological	B
quadtree	B
k	O
splits	O
are	O
allowed	O
defined	O
by	O
the	O
first	O
k	O
points	O
x	O
i	O
x	O
k	O
and	O
the	O
deep	B
quadtree	B
levels	O
of	O
splits	O
are	O
allowed	O
other	O
more	O
balanced	B
versions	O
may	O
also	O
be	O
introduced	O
classification	O
is	O
by	O
majority	B
vote	I
over	O
all	O
yi	O
k	O
i	O
n	O
in	O
the	O
chronological	B
quadtree-that	O
fall	O
in	O
the	O
same	O
region	O
as	O
x	O
ties	O
are	O
broken	O
in	O
favor	O
of	O
class	O
o	O
we	O
will	O
refer	O
to	O
this	O
as	O
the	O
deep	B
quadtree	B
classifier	B
theorem	O
whenever	O
x	O
has	O
a	O
density	O
the	O
chronological	B
quadtree	B
classifier	B
is	O
consistent	O
ln	O
l	O
provided	O
that	O
k	O
and	O
kin	O
o	O
proof	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
all	O
marginal	O
distributions	O
are	O
form	O
as	O
the	O
x-property	B
holds	O
theorem	O
applies	O
by	O
lemma	O
since	O
we	O
have	O
external	O
regions	O
pnx	O
m	O
n	O
k	O
for	O
all	O
m	O
provided	O
that	O
kin	O
o	O
hence	O
we	O
need	O
only	O
verify	O
the	O
condition	O
tree	O
classifiers	O
diamax	O
in	O
probability	O
this	O
is	O
a	O
bit	O
easier	O
than	O
in	O
the	O
proof	O
of	O
theorem	O
for	O
the	O
k-d	B
tree	I
and	O
is	O
thus	O
left	O
to	O
the	O
reader	O
remark	O
full-fledged	O
random	O
quadtrees	O
with	O
n	O
nodes	O
have	O
expected	O
height	O
o	O
n	O
whenever	O
x	O
has	O
a	O
density	O
e	O
g	O
devroye	O
and	O
laforest	O
with	O
k	O
nodes	O
every	O
region	O
is	O
thus	O
reached	O
in	O
only	O
o	O
k	O
steps	O
on	O
the	O
average	O
thermore	O
quadtrees	O
enjoy	O
the	O
same	O
monotone	O
transformation	B
invariance	I
that	O
we	O
observed	O
for	O
k-d	O
trees	O
and	O
median	O
trees	O
best	O
possible	O
perpendicular	O
splits	O
for	O
computational	O
reasons	O
classification	O
trees	O
are	O
most	O
often	O
produced	O
by	O
mining	O
the	O
splits	O
recursively	O
at	O
a	O
given	O
stage	O
of	O
the	O
tree-growing	O
algorithm	B
some	O
criterion	O
is	O
used	O
to	O
determine	O
which	O
node	O
of	O
the	O
tree	O
should	O
be	O
split	O
next	O
and	O
where	O
the	O
split	O
should	O
be	O
made	O
as	O
these	O
criteria	O
typically	O
use	O
all	O
the	O
data	O
the	O
resulting	O
trees	O
no	O
longer	O
have	O
the	O
x	O
in	O
this	O
section	O
we	O
examine	O
perhaps	O
the	O
most	O
natural	O
criterion	O
in	O
the	O
following	O
sections	O
we	O
introduce	O
some	O
alternative	O
splitting	O
criteria	O
a	O
binary	B
classification	O
tree	O
can	O
be	O
obtained	O
by	O
associating	O
with	O
each	O
node	O
a	O
splitting	B
function	I
obtained	O
in	O
a	O
top-down	O
fashion	O
from	O
the	O
data	O
for	O
example	O
at	O
the	O
root	O
we	O
may	O
select	O
the	O
function	O
sxi	O
ex	O
where	O
i	O
the	O
component	O
cut	O
ex	O
e	O
n	O
the	O
threshold	B
and	O
s	O
e	O
i	O
a	O
larization	O
are	O
all	O
dependent	O
upon	O
the	O
data	O
the	O
root	O
then	O
splits	O
the	O
data	O
dn	O
yd	O
yn	O
into	O
two	O
sets	O
d	O
d	O
dn	O
idiidi	O
n	O
such	O
that	O
d	O
y	O
e	O
dn	O
oj	O
d	O
y	O
e	O
dn	O
o	O
a	O
decision	O
is	O
made	O
whether	O
to	O
split	O
a	O
node	O
or	O
not	O
and	O
the	O
procedure	O
is	O
applied	O
recursively	O
to	O
the	O
subtrees	O
natural	O
majority	B
vote	I
decisions	O
are	O
taken	O
at	O
the	O
leaf	O
level	O
all	O
such	O
trees	O
will	O
be	O
called	O
perpendicular	B
splitting	I
trees	O
in	O
chapter	O
we	O
introduced	O
univariate	O
stoller	O
splits	O
that	O
is	O
splits	O
that	O
minimize	O
the	O
empirical	B
error	I
this	O
could	O
be	O
at	O
the	O
basis	O
of	O
a	O
perpendicular	B
splitting	I
tree	O
one	O
realizes	O
immediately	O
that	O
the	O
number	O
of	O
possibilities	O
for	O
stopping	O
is	O
endless	O
to	O
name	O
two	O
we	O
could	O
stop	O
after	O
k	O
splitting	O
nodes	O
have	O
been	O
defined	O
or	O
we	O
could	O
make	O
a	O
tree	O
with	O
k	O
full	O
levels	O
of	O
splits	O
that	O
all	O
leaves	O
are	O
at	O
distance	O
k	O
from	O
the	O
root	O
we	O
first	O
show	O
that	O
for	O
d	O
any	O
such	O
strategy	O
is	O
virtually	O
doomed	O
to	O
fail	O
to	O
make	O
this	O
case	O
we	O
will	O
argue	O
on	O
the	O
basis	O
of	O
distribution	O
functions	O
only	O
for	O
convenience	O
we	O
consider	O
a	O
two-dimensional	O
problem	O
given	O
a	O
rectangle	O
r	O
now	O
best	O
possible	O
perpendicular	O
splits	O
assigned	O
to	O
one	O
class	O
y	O
e	O
i	O
we	O
see	O
that	O
the	O
current	O
probability	O
of	O
error	O
in	O
r	O
before	O
splitting	O
is	O
p	O
e	O
r	O
y	O
y	O
let	O
r	O
range	O
over	O
all	O
rectangles	O
of	O
the	O
form	O
r	O
n-oo	O
a	O
x	O
r	O
r	O
n	O
x	O
r	O
r	O
n	O
x	O
ad	O
or	O
r	O
n	O
x	O
and	O
let	O
r	O
r	O
r	O
then	O
after	O
asplit	O
based	O
upon	O
r	O
the	O
probability	O
of	O
error	O
over	O
the	O
rectangle	O
r	O
is	O
px	O
e	O
r	O
y	O
i	O
px	O
e	O
r	O
y	O
o	O
as	O
we	O
assign	O
class	O
to	O
r	O
and	O
class	O
to	O
r	O
the	O
decrease	O
in	O
probability	O
of	O
error	O
if	O
we	O
minimize	O
over	O
all	O
r	O
computelj	O
r	O
for	O
all	O
leaf	O
rectangles	O
and	O
then	O
proceed	O
to	O
split	O
that	O
rectangle	O
leaf	O
for	O
which	O
ij	O
r	O
is	O
maximal	O
the	O
data-based	B
rule	B
based	O
upon	O
this	O
would	O
proceed	O
similarly	O
if	O
pa	O
is	O
replaced	O
everywhere	O
by	O
the	O
empirical	B
estimate	O
ixiyieaj	O
where	O
a	O
is	O
of	O
the	O
form	O
r	O
x	O
r	O
x	O
or	O
r	O
x	O
y	O
as	O
the	O
case	O
may	O
be	O
let	O
us	O
denote	O
by	O
l	O
o	O
l	O
l	O
the	O
sequence	O
of	O
the	O
overall	O
probabilities	O
of	O
error	O
for	O
the	O
theoretically	O
optimal	O
sequence	O
of	O
cuts	O
described	O
above	O
here	O
we	O
start	O
with	O
r	O
and	O
y	O
for	O
example	O
for	O
fixed	O
e	O
we	O
now	O
construct	O
a	O
simple	O
example	O
in	O
which	O
l	O
and	O
lk	O
as	O
k	O
l-e	O
thus	O
applying	O
the	O
best	O
split	O
incrementally	O
even	O
if	O
we	O
use	O
the	O
true	O
probability	O
of	O
error	O
as	O
our	O
criterion	O
for	O
splitting	O
is	O
not	O
advisable	O
the	O
example	O
is	O
very	O
simple	O
x	O
has	O
uniform	O
distribution	O
on	O
with	O
bility	O
e	O
and	O
on	O
with	O
probability	O
e	O
also	O
y	O
is	O
a	O
deterministic	O
function	O
of	O
x	O
so	O
that	O
l	O
figure	O
repeated	O
stoller	O
splits	O
are	O
not	O
consistent	O
in	O
this	O
two-dimensional	O
example	O
cuts	O
will	O
always	O
be	O
made	O
in	O
the	O
leftmost	O
square	O
probability	O
e	O
o	O
the	O
way	O
y	O
depends	O
on	O
x	O
is	O
shown	O
in	O
figure	O
y	O
if	O
x	O
e	O
u	O
u	O
u	O
u	O
x	O
tree	O
classifiers	O
where	O
al	O
ak	O
and	O
so	O
forth	O
we	O
verify	O
easily	O
that	O
py	O
i	O
also	O
the	O
error	O
probability	O
before	O
any	O
cut	O
is	O
made	O
is	O
lo	O
the	O
best	O
split	O
has	O
r	O
x	O
r	O
so	O
that	O
al	O
is	O
cut	O
off	O
therefore	O
li	O
we	O
continue	O
and	O
split	O
off	O
and	O
so	O
forth	O
leading	O
to	O
the	O
tree	O
of	O
figure	O
figure	O
the	O
tree	O
obtained	O
by	O
repeated	O
stoller	O
splits	O
yes	O
o	O
verify	O
that	O
and	O
in	O
general	O
that	O
lk	O
e	O
as	O
claimed	O
splitting	O
criteria	O
based	O
on	O
impurity	O
functions	O
in	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
presented	O
their	O
cart	B
program	O
for	O
constructing	O
classification	O
trees	O
with	O
perpendicular	O
splits	O
one	O
of	O
the	O
key	O
ideas	O
in	O
their	O
approach	O
is	O
the	O
notion	O
that	O
trees	O
should	O
be	O
constructed	O
from	O
the	O
bottom	O
up	O
by	O
combining	O
small	O
subtrees	O
the	O
starting	O
point	O
is	O
a	O
tree	O
with	O
n	O
leaf	O
regions	O
c	O
defined	O
by	O
a	O
partition	B
of	O
the	O
space	O
based	O
on	O
the	O
n	O
data	O
points	O
such	O
a	O
tree	O
is	O
much	O
too	O
large	O
and	O
is	O
pruned	O
by	O
some	O
methods	O
that	O
will	O
not	O
be	O
explored	O
here	O
when	O
splitting	O
criteria	O
based	O
on	O
impurity	O
functions	O
constructing	O
a	O
starting	O
tree	O
a	O
certain	O
splitting	B
criterion	I
is	O
applied	O
recursively	O
the	O
criterion	O
determines	O
which	O
rectangle	O
should	O
be	O
split	O
and	O
where	O
the	O
cut	O
should	O
be	O
made	O
to	O
keep	O
the	O
classifier	B
invariant	O
under	O
monotone	O
transformation	O
of	O
the	O
coordinate	O
axes	O
the	O
criterion	O
should	O
only	O
depend	O
on	O
the	O
coordinatewise	O
ranks	O
of	O
the	O
points	O
and	O
their	O
labels	O
typically	O
the	O
criterion	O
is	O
a	O
function	O
of	O
the	O
numbers	O
of	O
points	O
labeled	O
by	O
and	O
in	O
the	O
rectangles	O
after	O
the	O
cut	O
is	O
made	O
one	O
such	O
class	O
of	O
let	O
a	O
e	O
n	O
and	O
let	O
i	O
be	O
a	O
given	O
coordinate	O
i	O
d	O
let	O
r	O
be	O
a	O
hyperrectangle	O
to	O
be	O
cut	O
define	O
the	O
following	O
quantities	O
for	O
a	O
split	O
at	O
a	O
perpendicular	O
to	O
the	O
i	O
coordinate	O
criteria	O
is	O
described	O
here	O
xtr	O
a	O
j	O
yj	O
xj	O
e	O
r	O
xji	O
a	O
are	O
the	O
sets	O
of	O
pairs	O
falling	O
to	O
the	O
left	O
and	O
to	O
the	O
right	O
of	O
the	O
cut	O
respectively	O
are	O
the	O
numbers	O
of	O
such	O
pairs	O
finally	O
the	O
numbers	O
of	O
points	O
with	O
label	O
and	O
label	O
in	O
these	O
sets	O
are	O
denoted	O
respectively	O
by	O
niocr	O
a	O
ixir	O
a	O
n	O
j	O
yj	O
yj	O
oi	O
ixicr	O
a	O
n	O
j	O
yj	O
yj	O
nilcr	O
a	O
nocra	O
nlra	O
i	O
xtranxj	O
following	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
we	O
define	O
an	O
impurity	O
junction	O
for	O
a	O
possible	O
split	O
a	O
by	O
ii	O
a	O
ljj	O
nio	O
nil	O
ni	O
ljj	O
nio	O
nil	O
nio	O
nil	O
nl	O
i	O
i	O
i	O
i	O
ni	O
i	O
nio	O
nil	O
nio	O
nil	O
where	O
we	O
dropped	O
the	O
argument	O
a	O
throughout	O
here	O
ljj	O
is	O
a	O
nonnegative	O
tion	O
with	O
the	O
following	O
properties	O
ljj	O
g	O
d	O
ljjp	O
p	O
for	O
any	O
p	O
e	O
ljjo	O
p	O
increases	O
in	O
p	O
on	O
and	O
decreases	O
in	O
p	O
on	O
a	O
rectangle	O
r	O
is	O
split	O
at	O
a	O
along	O
the	O
i-th	O
coordinate	O
if	O
iir	O
a	O
is	O
minimal	O
ii	O
penalizes	O
splits	O
in	O
which	O
the	O
subregions	O
have	O
about	O
equal	O
proportions	O
from	O
both	O
classes	O
examples	O
of	O
such	O
functions	O
ljj	O
include	O
the	O
entropy	B
junction	O
p	O
p	O
log	O
p	O
p	O
p	O
et	O
al	O
tree	O
classifiers	O
the	O
gini	O
function	O
ljfp	O
p	O
p	O
leading	O
to	O
the	O
gini	O
index	O
of	O
diversity	O
f	O
i	O
et	O
al	O
the	O
probability	O
ofmisclassification	O
tp	O
p	O
minp	O
p	O
in	O
this	O
case	O
the	O
splitting	B
criterion	I
leads	O
to	O
the	O
empirical	B
stoller	O
splits	O
studied	O
in	O
the	O
previous	O
section	O
we	O
have	O
two	O
kinds	O
of	O
splits	O
theforced	O
split	O
force	O
a	O
split	O
along	O
the	O
i-th	O
coordinate	O
but	O
minimize	O
fica	O
r	O
over	O
all	O
a	O
and	O
rectangles	O
r	O
the	O
free	O
split	O
choose	O
the	O
most	O
advantageous	O
coordinate	O
for	O
splitting	O
that	O
is	O
minimize	O
fica	O
r	O
over	O
all	O
a	O
i	O
and	O
r	O
unfortunately	O
regardless	O
of	O
which	O
kind	O
of	O
policy	O
we	O
choose	O
there	O
are	O
distributions	O
for	O
which	O
no	O
splitting	O
based	O
on	O
an	O
impurity	B
function	I
leads	O
to	O
a	O
consistent	O
classifier	B
to	O
see	O
this	O
note	O
that	O
the	O
two-dimensional	O
example	O
of	O
the	O
previous	O
section	O
applies	O
to	O
all	O
impurity	O
functions	O
assume	O
that	O
we	O
had	O
infinite	O
sample	O
size	O
then	O
fica	O
r	O
would	O
approach	O
aljfp	O
p	O
btq	O
q	O
where	O
p	O
is	O
the	O
probability	O
content	O
of	O
r	O
of	O
the	O
rectangles	O
obtained	O
after	O
the	O
cut	O
is	O
made	O
b	O
is	O
that	O
of	O
r	O
other	O
rectangle	O
p	O
py	O
e	O
r	O
and	O
q	O
py	O
llx	O
e	O
r	O
if	O
x	O
is	O
uniformly	O
distributed	O
on	O
the	O
checkerboard	O
shown	O
in	O
figure	O
regardless	O
where	O
we	O
try	O
to	O
cut	O
p	O
q	O
and	O
every	O
cut	O
seems	O
to	O
be	O
undesirable	O
figure	O
no	O
cut	O
decreases	O
the	O
value	O
of	O
the	O
impurity	B
function	I
this	O
simple	O
example	O
may	O
be	O
made	O
more	O
interesting	O
by	O
mixing	O
it	O
with	O
a	O
tion	O
with	O
much	O
less	O
weight	O
in	O
which	O
xl-	O
and	O
x	O
splits	O
are	O
alternatingly	O
encouraged	O
all	O
the	O
time	O
therefore	O
impurity	O
functions	O
should	O
be	O
avoided	O
in	O
their	O
raw	B
form	O
for	O
splitting	O
this	O
may	O
explain	O
partially	O
why	O
in	O
cart	B
the	O
original	O
tree	O
is	O
undesirable	O
and	O
must	O
be	O
pruned	O
from	O
the	O
bottom	O
up	O
see	O
problems	O
to	O
for	O
more	O
information	O
in	O
the	O
next	O
section	O
and	O
in	O
the	O
last	O
section	O
of	O
this	O
chapter	O
we	O
propose	O
other	O
ways	O
of	O
growing	O
trees	O
with	O
much	O
more	O
desirable	O
properties	O
the	O
derivation	O
shown	O
above	O
does	O
not	O
indicate	O
that	O
the	O
empirical	B
version	O
will	O
not	O
work	O
properly	O
but	O
simple	O
versions	O
of	O
it	O
will	O
certainly	O
not	O
see	O
problem	O
remark	O
malicious	O
splitting	O
the	O
impurity	O
functions	O
suggested	O
above	O
all	O
avoid	O
leaving	O
the	O
proportions	O
of	O
zeros	O
and	O
ones	O
intact	O
through	O
splitting	O
they	O
push	O
wards	O
more	O
homogeneous	O
regions	O
assume	O
now	O
that	O
we	O
do	O
the	O
opposite	O
through	O
such	O
splits	O
we	O
can	O
in	O
fact	O
create	O
hyperplane	O
classification	O
trees	O
that	O
are	O
globally	O
poor	O
that	O
is	O
that	O
are	O
such	O
that	O
every	O
trimmed	O
version	O
of	O
the	O
tree	O
is	O
also	O
a	O
poor	O
classifier	B
such	O
splitting	O
methods	O
must	O
of	O
course	O
use	O
the	O
yi	O
our	O
example	O
shows	O
splitting	O
criteria	O
based	O
on	O
impurity	O
functions	O
that	O
any	O
general	O
consistency	B
theorem	O
for	O
hyperplane	O
classification	O
trees	O
must	O
come	O
with	O
certain	O
restrictions	O
on	O
the	O
splitting	O
process-the	O
x	O
property	O
is	O
good	O
times	O
it	O
is	O
necessary	O
to	O
force	O
cells	O
to	O
shrink	O
sometimes	O
the	O
position	O
of	O
the	O
split	O
is	O
restricted	O
by	O
empirical	B
error	I
minimization	O
or	O
some	O
other	O
criterion	O
the	O
ham-sandwich	B
theorem	I
edelsbrunner	O
states	O
that	O
given	O
classo	O
points	O
and	O
class-l	O
points	O
in	O
nd	O
d	O
there	O
exists	O
a	O
hyperplane	O
cut	O
that	O
leaves	O
n	O
class-o	O
points	O
and	O
m	O
class-l	O
points	O
in	O
each	O
halfspace	O
so	O
assume	O
that	O
x	O
has	O
a	O
density	O
and	O
that	O
p	O
py	O
i	O
in	O
a	O
sample	O
of	O
size	O
n	O
let	O
y	O
be	O
the	O
majority	O
class	O
are	O
broken	O
in	O
favor	O
of	O
class	O
figure	O
ham-sandwich	O
cut	O
each	O
halfspace	O
contains	O
exactly	O
half	O
the	O
points	O
from	O
each	O
class	O
o	O
o	O
regardless	O
of	O
the	O
sample	O
make-up	O
if	O
y	O
we	O
may	O
construct	O
a	O
hyperplane-tree	O
classifier	B
in	O
which	O
during	O
the	O
construction	O
every	O
node	O
represents	O
a	O
region	O
in	O
which	O
the	O
majority	B
vote	I
would	O
be	O
o	O
this	O
property	O
has	O
nothing	O
to	O
do	O
with	O
the	O
distribution	O
of	O
y	O
and	O
therefore	O
for	O
any	O
trimmed	O
version	O
of	O
the	O
tree	O
classifier	B
and	O
pln	O
p	O
py	O
o	O
if	O
p	O
obviously	O
as	O
we	O
may	O
take	O
l	O
these	O
classifiers	O
are	O
hopeless	O
bibliographic	O
remarks	O
empirical	B
stoller	O
splits	O
without	O
forced	O
rotation	B
were	O
ommended	O
by	O
payne	O
and	O
meisel	O
and	O
rounds	O
but	O
their	O
failure	O
to	O
be	O
universally	O
good	O
was	O
noted	O
by	O
gordon	O
and	O
olshen	O
the	O
last	O
two	O
thors	O
recommended	O
a	O
splitting	O
scheme	O
that	O
combined	O
several	O
ideas	O
but	O
roughly	O
speaking	O
they	O
perform	O
empirical	B
stoller	O
splits	O
with	O
forced	O
rotation	B
through	O
the	O
ordinate	O
axes	O
and	O
olshen	O
other	O
splitting	O
criteria	O
include	O
the	O
aid	B
criterion	I
of	O
morgan	O
and	O
sonquist	O
which	O
is	O
a	O
predecessor	O
of	O
the	O
gini	O
index	O
of	O
diversity	O
used	O
in	O
cart	B
friedman	O
olshen	O
and	O
stone	O
see	O
also	O
gelfand	O
and	O
delp	O
guo	O
and	O
gelfand	O
gelfand	O
ravishankar	O
and	O
delp	O
and	O
ciampi	O
michel-briand	O
and	O
milhaud	O
also	O
observed	O
the	O
failure	O
of	O
multivariate	O
classification	O
trees	O
based	O
on	O
the	O
aid	B
criterion	I
the	O
shannon	O
entropy	B
or	O
modifications	O
of	O
it	O
are	O
recommended	O
by	O
talmon	O
sethi	O
and	O
sarvarayudu	O
wang	O
and	O
suen	O
goodman	O
and	O
smyth	O
and	O
chou	O
permutation	O
statistics	O
are	O
used	O
in	O
li	O
and	O
dubes	O
still	O
without	O
forced	O
rotations	O
through	O
the	O
coordinate	O
axes	O
quinlan	O
has	O
a	O
more	O
involved	O
splitting	B
criterion	I
a	O
general	O
discussion	O
on	O
tree	O
splitting	O
may	O
be	O
tree	O
classifiers	O
found	O
in	O
the	O
paper	O
by	O
sethi	O
a	O
class	O
of	O
impurity	O
functions	O
is	O
studied	O
in	O
burshtein	O
della	O
pietra	O
kanevsky	O
and	O
nadas	O
among	O
the	O
pioneers	O
of	O
tree	O
splitting	O
perpendicular	O
cuts	O
are	O
sebestyen	O
and	O
henrichon	O
and	O
fu	O
for	O
related	O
work	O
we	O
refer	O
to	O
stoffel	O
sethi	O
and	O
chatterjee	O
argentiero	O
chin	O
and	O
beaudet	O
you	O
andfu	O
anderson	O
andfu	O
qing-yun	O
and	O
fu	O
hartmann	O
varshney	O
mehrotra	O
and	O
gerberich	O
and	O
casey	O
and	O
nagy	O
references	O
on	O
nonperpendicular	O
splitting	O
methods	O
are	O
given	O
below	O
in	O
the	O
section	O
on	O
esp	O
trees	O
a	O
consistent	O
splitting	B
criterion	I
there	O
is	O
no	O
reason	O
for	O
pessimism	O
after	O
the	O
previous	O
sections	O
rest	O
assured	O
there	O
are	O
several	O
consistent	O
splitting	O
strategies	O
that	O
are	O
fully	O
automatic	B
and	O
depend	O
only	O
upon	O
the	O
populations	O
of	O
the	O
regions	O
in	O
this	O
section	O
we	O
provide	O
a	O
solution	O
for	O
the	O
simple	O
case	O
when	O
x	O
is	O
univariate	O
and	O
nonatomic	O
it	O
is	O
possible	O
to	O
generalize	O
the	O
method	O
for	O
d	O
if	O
we	O
force	O
cuts	O
to	O
alternate	O
directions	O
we	O
omit	O
here	O
the	O
detailed	O
analysis	O
for	O
multidimensional	O
cases	O
for	O
two	O
reasons	O
first	O
it	O
is	O
significantly	O
heavier	O
than	O
for	O
d	O
secondly	O
in	O
the	O
last	O
section	O
of	O
this	O
chapter	O
we	O
introduce	O
a	O
fully	O
automatic	B
way	O
of	O
building	O
up	O
consistent	O
trees	O
that	O
is	O
without	O
forcing	O
the	O
directions	O
of	O
the	O
splits	O
to	O
a	O
partition	B
ai	O
an	O
of	O
r	O
we	O
assign	O
the	O
quantity	O
n	O
q	O
l	O
noai	O
where	O
n	O
noa	O
l	O
ixjeayjoj	O
jl	O
n	O
ni	O
l	O
ixjeayjl	O
jl	O
are	O
the	O
respective	O
numbers	O
of	O
points	O
labeled	O
with	O
and	O
falling	O
in	O
the	O
region	O
a	O
the	O
tree-growing	O
algorithm	B
starts	O
with	O
the	O
trivial	O
partition	B
and	O
at	O
each	O
step	O
it	O
makes	O
a	O
cut	O
that	O
yields	O
the	O
minimal	O
value	O
of	O
q	O
it	O
proceeds	O
recursively	O
until	O
the	O
improvement	O
in	O
the	O
value	O
of	O
q	O
falls	O
below	O
a	O
threshold	B
remark	O
notice	O
that	O
this	O
criterion	O
always	O
splits	O
a	O
cell	O
that	O
has	O
many	O
points	O
from	O
both	O
classes	O
the	O
proof	O
of	O
the	O
theorem	O
below	O
thus	O
it	O
avoids	O
the	O
anomalies	O
of	O
impurity-function	O
criteria	O
described	O
in	O
the	O
previous	O
section	O
on	O
the	O
other	O
hand	O
it	O
does	O
not	O
necessarily	O
split	O
large	O
cells	O
if	O
they	O
are	O
almost	O
homogeneous	O
for	O
a	O
comparison	O
recall	O
that	O
the	O
gini	O
criterion	O
minimizes	O
the	O
quantity	O
noai	O
o	O
noai	O
ni	O
thus	O
favoring	O
cutting	O
cells	O
with	O
very	O
few	O
points	O
we	O
realize	O
that	O
the	O
criterion	O
q	O
introduced	O
here	O
is	O
just	O
one	O
of	O
many	O
with	O
similarly	O
good	O
properties	O
and	O
albeit	O
probably	O
imperfect	O
it	O
is	O
certainly	O
one	O
of	O
the	O
simplest	O
bsp	O
trees	O
theorem	O
let	O
x	O
have	O
a	O
nonatomic	O
distribution	O
on	O
the	O
real	O
line	O
and	O
consider	O
the	O
tree	O
classifier	B
obtained	O
by	O
the	O
algorithm	B
described	O
above	O
if	O
the	O
threshold	B
satisfies	O
n	O
and	O
n	O
then	O
the	O
classification	O
rule	B
is	O
strongly	O
consistent	O
proof	O
there	O
are	O
two	O
key	O
properties	O
of	O
the	O
algorithm	B
that	O
we	O
exploit	O
property	O
if	O
minnoa	O
ni	O
for	O
a	O
cell	O
a	O
then	O
a	O
gets	O
cut	O
by	O
the	O
algorithm	B
to	O
see	O
this	O
observe	O
that	O
the	O
argument	O
a	O
from	O
the	O
notation	O
if	O
no	O
n	O
and	O
we	O
cut	O
a	O
so	O
that	O
the	O
number	O
of	O
o-labeled	O
points	O
in	O
the	O
two	O
child	O
regions	O
differ	O
by	O
at	O
most	O
one	O
then	O
the	O
contribution	O
of	O
these	O
two	O
new	O
regions	O
to	O
q	O
is	O
r	O
n	O
n	O
r	O
where	O
n	O
and	O
n	O
are	O
the	O
numbers	O
of	O
points	O
in	O
the	O
two	O
child	O
regions	O
thus	O
the	O
decrease	O
of	O
q	O
if	O
a	O
is	O
split	O
is	O
at	O
least	O
if	O
minno	O
nd	O
then	O
d	O
n	O
and	O
a	O
cut	O
is	O
made	O
property	O
no	O
leaf	O
node	O
has	O
less	O
than	O
n	O
points	O
assume	O
that	O
after	O
a	O
region	O
is	O
cut	O
in	O
one	O
of	O
the	O
child	O
regions	O
the	O
total	O
number	O
of	O
points	O
is	O
n	O
n	O
k	O
then	O
the	O
improvement	O
in	O
q	O
caused	O
by	O
the	O
split	O
is	O
bounded	O
by	O
nonl	O
n	O
n	O
s	O
nonl	O
knl	O
k	O
s	O
s	O
nk	O
therefore	O
if	O
k	O
in	O
then	O
the	O
improvement	O
is	O
smaller	O
than	O
thus	O
no	O
cut	O
is	O
made	O
that	O
leaves	O
behind	O
a	O
child	O
region	O
with	O
fewer	O
than	O
i	O
n	O
points	O
it	O
follows	O
from	O
property	O
that	O
if	O
a	O
leaf	O
region	O
has	O
more	O
than	O
points	O
then	O
since	O
the	O
class	O
in	O
minority	O
has	O
less	O
than	O
points	O
in	O
it	O
it	O
may	O
be	O
cut	O
into	O
intervals	O
containing	O
between	O
and	O
points	O
without	O
altering	O
the	O
decision	O
since	O
the	O
majority	B
vote	I
within	O
each	O
region	O
remains	O
the	O
same	O
summarizing	O
we	O
see	O
that	O
the	O
tree	O
classifier	B
is	O
equivalent	O
to	O
a	O
classifier	B
that	O
partitions	O
the	O
real	O
line	O
into	O
intervals	O
each	O
containing	O
at	O
least	O
and	O
at	O
most	O
data	O
points	O
thus	O
in	O
this	O
partition	B
each	O
interval	O
has	O
a	O
number	O
of	O
points	O
growing	O
to	O
infinity	O
as	O
on	O
we	O
emphasize	O
that	O
the	O
number	O
of	O
points	O
in	O
a	O
region	O
of	O
the	O
studied	O
tree	O
classifier	B
may	O
be	O
large	O
but	O
such	O
regions	O
are	O
almost	O
homogeneous	O
and	O
therefore	O
the	O
classifier	B
is	O
equivalent	O
to	O
another	O
classifier	B
which	O
has	O
on	O
points	O
in	O
each	O
region	O
consistency	B
of	O
such	O
partitioning	O
classifiers	O
is	O
proved	O
in	O
the	O
next	O
chapter-see	O
theorem	O
d	O
bsp	O
trees	O
binary	B
space	O
partition	B
trees	O
bsp	O
trees	O
partition	B
rd	O
by	O
hyperplanes	O
trees	O
of	O
this	O
nature	O
have	O
evolved	O
in	O
the	O
computer	O
graphics	O
literature	O
via	O
the	O
work	O
of	O
fuchs	O
tree	O
classifiers	O
kedem	O
and	O
naylor	O
and	O
fuchs	O
abram	O
and	O
grant	O
also	O
samet	O
kaplan	O
and	O
sung	O
and	O
shirley	O
in	O
discrimination	O
they	O
are	O
at	O
the	O
same	O
time	O
generalizations	O
of	O
linear	O
discriminants	O
of	O
histograms	O
and	O
of	O
binary	B
tree	O
classifiers	O
bsp	O
trees	O
were	O
recommended	O
for	O
use	O
in	O
discrimination	O
by	O
henrichon	O
and	O
fu	O
mizoguchi	O
kizawa	O
and	O
shimura	O
and	O
friedman	O
further	O
studies	O
include	O
sklansky	O
and	O
michelotti	O
argentiero	O
chin	O
and	O
beaudet	O
qing-yun	O
and	O
fu	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
loh	O
and	O
vanichsetakul	O
and	O
park	O
and	O
sklansky	O
there	O
are	O
numerous	O
ways	O
of	O
constructing	O
bsp	O
trees	O
most	O
methods	O
of	O
course	O
use	O
the	O
y	O
to	O
determine	O
good	O
splits	O
nevertheless	O
we	O
should	O
mention	O
first	O
simple	O
splits	O
with	O
the	O
x	O
if	O
only	O
to	O
better	O
understand	O
the	O
bsp	O
trees	O
figure	O
a	O
raw	B
bsp	O
tree	O
and	O
its	O
induced	O
partition	B
in	O
the	O
plane	O
every	O
region	O
is	O
split	O
by	O
a	O
line	O
determined	O
by	O
the	O
two	O
data	O
points	O
with	O
smallest	O
index	O
in	O
the	O
region	O
we	O
call	O
the	O
raw	B
bsp	O
tree	O
the	O
tree	O
obtained	O
by	O
letting	O
xl	O
xd	O
determine	O
the	O
first	O
hyperplane	O
the	O
d	O
data	O
points	O
remain	O
associated	O
with	O
the	O
root	O
and	O
the	O
others	O
x	O
k	O
are	O
sent	O
down	O
to	O
the	O
subtrees	O
where	O
the	O
process	O
is	O
repeated	O
as	O
far	O
as	O
possible	O
the	O
remaining	O
points	O
xkl	O
xn	O
are	O
used	O
in	O
a	O
majority	B
vote	I
in	O
the	O
external	O
regions	O
note	O
that	O
the	O
number	O
of	O
regions	O
is	O
not	O
more	O
than	O
kid	O
thus	O
by	O
lemma	O
if	O
kin	O
we	O
have	O
nx	O
in	O
probability	O
combining	O
this	O
with	O
problem	O
we	O
have	O
our	O
first	O
result	O
theorem	O
the	O
natural	O
binary	B
tree	O
classifier	B
based	O
upon	O
a	O
raw	B
bsp	O
tree	O
with	O
k	O
and	O
kin	O
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
hyperplanes	O
may	O
also	O
be	O
selected	O
by	O
optimization	O
of	O
a	O
criterion	O
typically	O
this	O
would	O
involve	O
separating	O
the	O
classes	O
in	O
some	O
way	O
all	O
that	O
was	O
said	O
for	O
perpen	O
dicular	O
splitting	O
remains	O
valid	O
here	O
it	O
is	O
worthwhile	O
recalling	O
therefore	O
that	O
there	O
primitive	O
selection	O
are	O
distributions	O
for	O
which	O
the	O
best	O
empirical	B
stoller	B
split	I
does	O
not	O
improve	O
the	O
probabilit	O
of	O
error	O
take	O
for	O
example	O
the	O
uniform	O
distribution	O
in	O
the	O
unit	O
ball	O
of	O
nd	O
in	O
which	O
y	O
if	O
iixii	O
o	O
if	O
iixii	O
figure	O
no	O
single	O
split	O
improves	O
on	O
the	O
error	O
ability	O
for	O
this	O
distribution	O
here	O
no	O
linear	O
split	O
would	O
be	O
helpful	O
as	O
the	O
l	O
s	O
would	O
always	O
hold	O
a	O
strong	O
majority	O
minimizing	O
other	O
impurity	O
functions	O
such	O
as	O
the	O
gini	O
criterion	O
may	O
be	O
helpful	O
however	O
bibliographic	O
remarks	O
hyperplane	O
splits	O
may	O
be	O
generalized	B
to	O
include	O
quadratic	O
splits	O
and	O
fu	O
for	O
example	O
mui	O
and	O
fu	O
suggest	O
taking	O
d	O
d	O
and	O
forming	O
quadratic	O
classifiers	O
as	O
in	O
normal	B
discrimination	O
chapter	O
based	O
upon	O
vectors	O
in	O
r	O
d	O
the	O
cuts	O
are	O
thus	O
perpendicular	O
to	O
d	O
d	O
axes	O
but	O
quadratic	O
in	O
the	O
subspace	O
r	O
d	O
tance	O
or	O
clustering	B
for	O
determining	O
splits	O
as	O
a	O
novelty	O
within	O
each	O
leaf	O
region	O
the	O
decision	O
is	O
not	O
by	O
majority	B
vote	I
but	O
rather	O
by	O
a	O
slightly	O
more	O
cated	O
rule	B
such	O
as	O
the	O
k-nn	B
rule	B
or	O
linear	O
discrimination	O
here	O
no	O
optimization	O
is	O
required	O
along	O
the	O
way	O
loh	O
and	O
vanichsetakul	O
allow	O
linear	O
splits	O
but	O
use	O
f	O
ratios	O
to	O
select	O
desirable	O
hyperplanes	O
lin	O
and	O
fu	O
employ	O
the	O
bhattacharyya	O
primitive	O
selection	O
there	O
are	O
two	O
reasons	O
for	O
optimizing	O
a	O
tree	O
configuration	O
first	O
of	O
all	O
it	O
just	O
does	O
not	O
make	O
sense	O
to	O
ignore	O
the	O
class	O
labels	O
when	O
constructing	O
a	O
tree	O
classifier	B
so	O
the	O
yis	O
must	O
be	O
used	O
to	O
help	O
in	O
the	O
selection	O
of	O
a	O
best	O
tree	O
secondly	O
some	O
trees	O
may	O
not	O
be	O
consistent	O
provably	O
consistent	O
yet	O
when	O
optimized	O
over	O
a	O
family	B
of	I
trees	O
consistency	B
drops	O
out	O
we	O
take	O
the	O
following	O
example	O
let	O
be	O
a	O
class	O
of	O
binary	B
tree	O
classifiers	O
with	O
the	O
x	O
with	O
the	O
space	O
partitioned	O
into	O
k	O
regions	O
determined	O
by	O
xl	O
xk	O
only	O
examples	O
include	O
the	O
chronological	B
k-d	B
tree	I
and	O
some	O
kinds	O
of	O
bsp	O
trees	O
we	O
estimate	O
the	O
error	O
for	O
g	O
e	O
by	O
realizing	O
the	O
danger	O
of	O
using	O
the	O
same	O
data	O
that	O
were	O
used	O
to	O
obtain	O
majority	O
votes	O
to	O
estimate	O
the	O
error	O
an	O
optimistic	O
bias	B
will	O
be	O
introduced	O
more	O
on	O
such	O
error	O
estimates	O
see	O
chapter	O
let	O
g	O
be	O
the	O
classifier	B
one	O
of	O
the	O
classifiers	O
in	O
for	O
which	O
ln	O
is	O
minimum	O
assume	O
that	O
oo-for	O
example	O
could	O
tree	O
classifiers	O
be	O
all	O
k	O
chronological	B
k-d	O
trees	O
obtained	O
by	O
permuting	O
xl	O
xk	O
we	O
call	O
g	O
then	O
the	O
permutation-optimized	O
chronological	B
k-d	O
classifier	B
when	O
k	O
l	O
jiognj	O
one	O
can	O
verify	O
that	O
k	O
one	O
for	O
any	O
e	O
so	O
that	O
the	O
computational	O
burden-at	O
least	O
on	O
paper-is	O
not	O
out	O
of	O
sight	O
we	O
assume	O
that	O
has	O
a	O
consistent	O
classifier	B
sequence	O
that	O
is	O
as	O
n	O
k	O
usually	O
grows	O
unbounded	O
and	O
elngk	O
l	O
for	O
a	O
sequence	O
gk	O
e	O
example	O
among	O
the	O
chronological	B
k-d	O
trees	O
modulo	O
permutations	O
the	O
first	O
one	O
the	O
one	O
corresponding	O
to	O
the	O
identity	O
permutation	O
was	O
shown	O
to	O
be	O
consistent	O
in	O
theorem	O
if	O
x	O
has	O
a	O
density	O
k	O
and	O
kin	O
o	O
example	O
let	O
be	O
the	O
class	O
of	O
bsp	O
trees	O
in	O
which	O
we	O
take	O
as	O
possible	O
hyperplanes	O
for	O
splitting	O
the	O
root	O
the	O
hyperplane	O
through	O
x	O
i	O
x	O
d	O
the	O
d	O
hyperplanes	O
through	O
x	O
i	O
x	O
d	O
l	O
that	O
are	O
parallel	O
to	O
one	O
axis	O
the	O
hyperplanes	O
through	O
x	O
i	O
that	O
are	O
parallel	O
to	O
two	O
axes	O
the	O
cl	O
d	O
hyperplanes	O
through	O
xl	O
that	O
are	O
parallel	O
to	O
d	O
axes	O
thus	O
conservatively	O
estimated	O
i	O
k	O
because	O
there	O
are	O
at	O
most	O
possible	O
choices	O
at	O
each	O
node	O
and	O
there	O
are	O
k	O
permutations	O
of	O
x	O
i	O
x	O
k	O
granted	O
the	O
number	O
of	O
external	O
regions	O
is	O
very	O
variable	B
but	O
it	O
remains	O
bounded	O
by	O
k	O
in	O
any	O
case	O
as	O
contains	O
the	O
chronological	B
k-d	B
tree	I
it	O
has	O
a	O
consistent	O
sequence	O
of	O
classifiers	O
when	O
k	O
and	O
nlk	O
log	O
k	O
theorem	O
let	O
gl	O
lng	O
then	O
has	O
a	O
consistent	O
sequence	O
of	O
classifiers	O
if	O
the	O
number	O
of	O
regions	O
in	O
the	O
partitions	O
for	O
all	O
g	O
e	O
are	O
at	O
most	O
k	O
andifk	O
and	O
nl	O
log	O
then	O
elng	O
l	O
where	O
lng	O
is	O
the	O
conditional	O
probability	O
of	O
error	O
of	O
g	O
in	O
the	O
examples	O
cited	O
above	O
we	O
must	O
take	O
k	O
n	O
i	O
k	O
furthermore	O
log	O
i	O
o	O
kl	O
ok	O
log	O
k	O
in	O
both	O
cases	O
thus	O
gl	O
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
k	O
and	O
k	O
onl	O
log	O
n	O
this	O
is	O
a	O
simple	O
way	O
of	O
constructing	O
a	O
basically	O
universally	O
consistent	O
bsp	O
tree	O
proof	O
let	O
g	O
arg	O
ln	O
lng	O
l	O
lng	O
lng	O
lng	O
lng	O
lng	O
l	O
clearly	O
def	O
i	O
i	O
i	O
obviously	O
i	O
in	O
the	O
mean	O
by	O
our	O
assumption	O
and	O
for	O
e	O
primitive	O
selection	O
next	O
we	O
bound	O
the	O
probabilities	O
on	O
the	O
right-hand	O
side	O
let	O
pw	O
pil	O
denote	O
px	O
e	O
region	O
i	O
y	O
o	O
and	O
px	O
e	O
region	O
i	O
y	O
respectively	O
with	O
regions	O
determined	O
by	O
g	O
i	O
k	O
let	O
n	O
nw	O
l	O
ixjefegion	O
iyjo	O
jkl	O
and	O
nil	O
l	O
ixjeregion	O
iyji	O
n	O
jkl	O
then	O
and	O
thus	O
lng	O
l	O
pilinionid	O
l	O
pwinionid	O
kl	O
ilng	O
l	O
pil	O
l	O
pig	O
io	O
i	O
i	O
n	O
n	O
k	O
i	O
il	O
i	O
il	O
n	O
n	O
k	O
introduce	O
the	O
notation	O
z	O
for	O
the	O
random	O
variable	B
on	O
the	O
right-hand	O
side	O
of	O
the	O
above	O
inequality	B
by	O
the	O
cauchy-schwarz	B
inequality	B
kl	O
l	O
il	O
e	O
nokr	O
ix	O
i	O
xc	O
given	O
xl	O
xb	O
nil	O
is	O
binomial	B
k	O
pil	O
np	O
j	O
npok	O
rnk	O
another	O
use	O
of	O
the	O
cauchy-schwarz	B
inequality	B
n-k	O
tree	O
classifiers	O
thus	O
ezixl	O
x	O
k	O
as	O
kin	O
o	O
note	O
that	O
pz	O
xd	O
p	O
z	O
ezixi	O
i	O
xl	O
xk	O
e	O
ezixi	O
xk	O
which	O
happens	O
when	O
n	O
exp	O
k	O
mcdiarmids	O
inequality	B
as	O
changing	O
the	O
value	O
of	O
an	O
x	O
j	O
j	O
k	O
changes	O
z	O
by	O
at	O
most	O
k	O
see	O
theorem	O
exp	O
thus	O
taking	O
expected	O
values	O
we	O
see	O
that	O
if	O
k	O
this	O
tends	O
to	O
for	O
all	O
e	O
if	O
kin	O
and	O
nllog	O
constructing	O
consistent	O
tree	O
classifiers	O
thus	O
far	O
we	O
have	O
taken	O
you	O
through	O
a	O
forest	O
of	O
beautiful	O
trees	O
and	O
we	O
have	O
shown	O
you	O
a	O
few	O
tricks	O
of	O
the	O
trade	O
when	O
you	O
read	O
write	O
a	O
research	O
paper	O
on	O
tree	O
classifiers	O
and	O
try	O
to	O
directly	O
apply	O
a	O
consistency	B
theorem	O
you	O
will	O
get	O
frustrated	O
however-most	O
real-life	O
tree	O
classifiers	O
use	O
the	O
data	O
in	O
intricate	O
ways	O
to	O
suit	O
a	O
certain	O
application	O
it	O
really	O
helps	O
to	O
have	O
a	O
few	O
truly	O
general	O
results	O
that	O
have	O
universal	O
impact	O
in	O
this	O
section	O
we	O
will	O
point	O
you	O
to	O
three	O
different	O
places	O
in	O
the	O
book	O
where	O
you	O
may	O
find	O
useful	O
results	O
in	O
this	O
respect	O
first	O
of	O
all	O
there	O
is	O
a	O
consistency	B
theorem-theorem	O
applies	O
to	O
rules	O
that	O
partition	B
the	O
space	O
and	O
decide	O
by	O
majority	B
vote	I
the	O
partition	B
is	O
arbitrary	O
and	O
may	O
thus	O
be	O
generated	O
by	O
using	O
some	O
or	O
all	O
of	O
xl	O
x	O
n	O
yn	O
if	O
a	O
rule	B
satisfies	O
the	O
two	O
conditions	O
of	O
theorem	O
it	O
must	O
be	O
universally	O
consistent	O
to	O
put	O
it	O
differently	O
even	O
the	O
worst	O
rule	B
within	O
the	O
boundaries	O
of	O
the	O
theorems	O
conditions	O
must	O
perform	O
well	O
asymptotically	O
second	O
we	O
will	O
briefly	O
discuss	O
the	O
design	O
of	O
tree	O
classifiers	O
obtained	O
by	O
mizing	O
the	O
empirical	B
error	I
estimate	O
over	O
possibly	O
infinite	O
classes	O
of	O
classifiers	O
such	O
classifiers	O
however	O
hard	O
to	O
find	O
by	O
an	O
algorithm	B
have	O
asymptotic	O
properties	O
that	O
are	O
related	O
to	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
of	O
rules	O
consistency	B
follows	O
most	O
without	O
work	O
if	O
one	O
can	O
calculate	O
or	O
bound	O
the	O
vc	B
dimension	B
appropriately	O
constructing	O
consistent	O
tree	O
classifiers	O
while	O
chapters	O
and	O
deal	O
in	O
more	O
detail	O
with	O
the	O
vc	B
dimension	B
it	O
is	O
necessary	O
to	O
give	O
a	O
few	O
examples	O
here	O
third	O
we	O
point	O
the	O
reader	O
to	O
chapter	O
on	O
data	O
splitting	O
where	O
the	O
previous	O
approach	O
is	O
applied	O
to	O
the	O
minimization	O
of	O
the	O
holdout	B
estimate	O
obtained	O
by	O
trees	O
based	O
upon	O
part	O
of	O
the	O
sample	O
and	O
using	O
another	O
part	O
to	O
select	O
the	O
best	O
tree	O
in	O
the	O
bunch	O
here	O
too	O
the	O
vc	B
dimension	B
plays	O
a	O
crucial	O
role	O
theorem	O
allows	O
space	O
partitions	O
that	O
depend	O
quite	O
arbitrarily	O
on	O
all	O
the	O
data	O
and	O
extends	O
earlier	O
universally	O
applicable	O
results	O
of	O
gordon	O
and	O
olshen	O
and	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
a	O
particularly	O
useful	O
format	O
is	O
given	O
in	O
theorem	O
if	O
the	O
partition	B
is	O
by	O
recursive	B
hyperplane	O
splits	O
as	O
in	O
bsp	O
trees	O
and	O
the	O
number	O
of	O
splits	O
is	O
at	O
most	O
m	O
n	O
if	O
mn	O
log	O
n	O
i	O
n	O
and	O
if	O
diamax	O
n	O
sb	O
with	O
probability	O
one	O
for	O
all	O
sb	O
sb	O
is	O
the	O
ball	O
of	O
radius	O
b	O
centered	O
at	O
the	O
origin	O
then	O
the	O
classification	O
rule	B
is	O
strongly	O
consistent	O
the	O
last	O
condition	O
forces	O
a	O
randomly	O
picked	O
region	O
in	O
the	O
partition	B
to	O
be	O
small	O
however	O
mn	O
log	O
n	O
i	O
n	O
guarantees	O
that	O
no	O
devilish	B
partition	B
can	O
be	O
inconsistent	O
the	O
latter	O
condition	O
is	O
certainly	O
satisfied	O
if	O
each	O
region	O
contains	O
at	O
least	O
kn	O
points	O
where	O
knl	O
log	O
n	O
next	O
we	O
take	O
a	O
look	O
at	O
full-fledged	O
minimization	O
of	O
ln	O
the	O
empirical	B
error	I
over	O
certain	O
classes	O
of	O
tree	O
classifiers	O
here	O
we	O
are	O
not	O
concerned	O
with	O
the	O
unacceptable	O
computational	O
effort	O
for	O
example	O
let	O
qk	O
be	O
the	O
class	O
of	O
all	O
binary	B
tree	O
classifiers	O
based	O
upon	O
a	O
tree	O
consisting	O
of	O
k	O
internal	O
nodes	O
each	O
representing	O
a	O
hyperplane	O
cut	O
in	O
the	O
bsp	O
tree	O
and	O
all	O
possible	O
labelings	O
of	O
the	O
k	O
leaf	O
regions	O
pick	O
such	O
a	O
classifier	B
for	O
which	O
is	O
minimal	O
observe	O
that	O
the	O
chosen	O
tree	O
is	O
always	O
natural	O
that	O
is	O
it	O
takes	O
majority	O
votes	O
over	O
the	O
leaf	O
regions	O
thus	O
the	O
minimization	O
is	O
equivalent	O
to	O
the	O
minimization	O
of	O
the	O
resubstitution	B
error	O
estimate	O
in	O
chapter	O
over	O
the	O
corresponding	O
class	O
of	O
natural	O
tree	O
classifiers	O
we	O
say	O
that	O
a	O
sequence	O
of	O
classes	O
is	O
rich	O
if	O
we	O
can	O
find	O
a	O
sequence	O
gk	O
e	O
qk	O
such	O
that	O
lgk	O
l	O
for	O
hyperplanes	O
this	O
is	O
the	O
case	O
if	O
k	O
as	O
n	O
oo-just	O
make	O
the	O
hyperplane	O
cuts	O
form	O
a	O
regular	B
histogram	O
grid	O
and	O
recall	O
theorem	O
let	O
sqb	O
n	O
be	O
the	O
shatter	B
coefficient	I
of	O
qk	O
a	O
definition	O
see	O
chapter	O
definition	O
for	O
example	O
for	O
the	O
hyperplane	O
family	O
sqk	O
n	O
nkdl	O
then	O
by	O
corollary	O
we	O
have	O
elng	O
l	O
for	O
the	O
selected	O
classifier	B
g	O
when	O
qk	O
is	O
rich	O
here	O
and	O
log	O
sqk	O
n	O
on	O
that	O
is	O
ko	O
logn	O
n	O
tree	O
classifiers	O
observe	O
that	O
no	O
conditions	O
are	O
placed	O
on	O
the	O
distribution	O
of	O
y	O
here	O
tency	O
follows	O
from	O
basic	O
notions-one	O
combinatorial	O
to	O
keep	O
us	O
from	O
overfitting	B
and	O
one	O
approximation-theoretical	O
richness	O
the	O
above	O
result	O
remains	O
valid	O
under	O
the	O
same	O
conditions	O
on	O
k	O
in	O
the	O
following	O
classes	O
all	O
trees	O
based	O
upon	O
k	O
internal	O
nodes	O
each	O
representing	O
a	O
perpendicular	O
split	O
sh	O
n	O
ldk	O
all	O
trees	O
based	O
upon	O
k	O
internal	O
nodes	O
each	O
representing	O
a	O
quadtree	B
split	O
sh	O
n	O
d	O
ll	O
a	O
greedy	B
classifier	B
in	O
this	O
section	O
we	O
define	O
simply	O
a	O
binary	B
tree	O
classifier	B
that	O
is	O
grown	O
via	O
tion	O
of	O
a	O
simple	O
criterion	O
it	O
has	O
the	O
remarkable	O
property	O
that	O
it	O
does	O
not	O
require	O
a	O
forced	O
rotation	B
through	O
the	O
coordinate	O
axes	O
or	O
special	O
safeguards	O
against	O
small	O
or	O
large	O
regions	O
or	O
the	O
like	O
it	O
remains	O
entirely	O
parameter-free	O
is	O
picked	O
by	O
the	O
user	O
is	O
monotone	O
transformation	O
invariant	O
and	O
fully	O
automatic	B
we	O
show	O
that	O
in	O
nd	O
it	O
is	O
always	O
consistent	O
it	O
serves	O
as	O
a	O
prototype	B
for	O
teaching	O
about	O
such	O
rules	O
and	O
should	O
not	O
be	O
considered	O
as	O
more	O
than	O
that	O
for	O
fully	O
practical	O
methods	O
we	O
believe	O
one	O
will	O
have	O
to	O
tinker	O
with	O
the	O
approach	O
the	O
space	O
is	O
partitioned	O
into	O
rectangles	O
as	O
shown	O
below	O
cj	O
figure	O
a	O
tree	O
based	O
on	O
partitioning	O
the	O
plane	O
into	O
rectangles	O
the	O
right	O
subtree	O
of	O
each	O
internal	O
node	O
belongs	O
to	O
the	O
inside	O
of	O
a	O
rectangle	O
and	O
the	O
left	O
subtree	O
belongs	O
to	O
the	O
complement	O
of	O
the	O
same	O
rectangle	O
denotes	O
the	O
complement	O
of	O
i	O
rectangles	O
are	O
not	O
allowed	O
to	O
overlap	O
a	O
hyperrectangle	O
defines	O
a	O
split	O
in	O
a	O
natural	O
way	O
the	O
theory	O
presented	O
here	O
applies	O
for	O
many	O
other	O
types	O
of	O
cuts	O
these	O
will	O
be	O
discussed	O
after	O
the	O
main	O
sistency	O
theorem	O
is	O
stated	O
a	O
greedy	B
classifier	B
a	O
partition	B
is	O
denoted	O
by	O
p	O
and	O
a	O
decision	O
on	O
a	O
set	O
a	O
e	O
p	O
is	O
by	O
majority	B
vote	I
we	O
write	O
gp	O
for	O
such	O
a	O
rule	B
gpx	O
if	O
l	O
yi	O
yi	O
x	O
e	O
a	O
ixea	O
otherwise	O
ixea	O
given	O
a	O
partition	B
p	O
a	O
legal	O
rectangle	O
a	O
is	O
one	O
for	O
which	O
a	O
n	O
b	O
or	O
a	O
s	O
b	O
for	O
all	O
sets	O
b	O
e	O
p	O
if	O
we	O
refine	O
p	O
by	O
adding	O
a	O
legal	O
rectangle	O
t	O
somewhere	O
then	O
we	O
obtain	O
the	O
partition	B
t	O
the	O
decision	O
gy	O
agrees	O
with	O
gp	O
except	O
on	O
the	O
set	O
b	O
e	O
p	O
that	O
contains	O
t	O
we	O
introduce	O
the	O
convenient	O
notation	O
px	O
e	O
a	O
y	O
j	O
e	O
i	O
an	O
estimate	O
of	O
the	O
quality	O
of	O
gp	O
is	O
where	O
n	O
l	O
ixergpxj	O
y	O
minvonr	O
vinr	O
n	O
ii	O
here	O
we	O
use	O
two	O
different	O
arguments	O
for	O
ln	O
and	O
p	O
but	O
the	O
distinction	O
should	O
be	O
clear	O
we	O
may	O
similarly	O
define	O
lnt	O
given	O
a	O
partition	B
p	O
the	O
greedy	B
classifier	B
selects	O
that	O
legal	O
rectangle	O
t	O
for	O
which	O
lnt	O
is	O
minimal	O
any	O
appropriate	O
policy	O
for	O
breaking	O
ties	O
let	O
r	O
be	O
the	O
set	O
of	O
p	O
containing	O
t	O
then	O
the	O
greedy	B
classifier	B
picks	O
that	O
t	O
for	O
which	O
is	O
minimal	O
starting	O
with	O
the	O
trivial	O
partition	B
po	O
d	O
we	O
repeat	O
the	O
previous	O
step	O
k	O
times	O
leading	O
thus	O
to	O
k	O
regions	O
the	O
sequence	O
of	O
partitions	O
is	O
denoted	O
by	O
po	O
pi	O
pk	O
we	O
put	O
no	O
safeguards	O
in	O
place-the	O
rectangles	O
are	O
not	O
forced	O
to	O
shrink	O
and	O
in	O
fact	O
it	O
is	O
easy	O
to	O
construct	O
examples	O
in	O
which	O
most	O
rectangles	O
do	O
not	O
shrink	O
the	O
main	O
result	O
of	O
the	O
section	O
and	O
indeed	O
of	O
this	O
chapter	O
is	O
that	O
the	O
obtained	O
classifier	B
is	O
consistent	O
theorem	O
for	O
the	O
greedy	B
classifier	B
with	O
k	O
and	O
k	O
j	O
n	O
log	O
n	O
assuming	O
that	O
x	O
has	O
nonatomic	O
marginals	O
we	O
have	O
ln	O
l	O
with	O
probability	O
one	O
tree	O
classifiers	O
remark	O
we	O
note	O
that	O
with	O
techniques	O
presented	O
in	O
the	O
next	O
chapter	O
it	O
is	O
possible	O
to	O
improve	O
the	O
second	O
condition	O
on	O
k	O
to	O
k	O
vi	O
n	O
log	O
n	O
problem	O
o	O
before	O
proving	O
the	O
theorem	O
we	O
mention	O
that	O
the	O
same	O
argument	O
may	O
be	O
used	O
to	O
establish	O
consistency	B
of	O
greedily	O
grown	O
trees	O
with	O
many	O
other	O
types	O
of	O
cuts	O
we	O
have	O
seen	O
in	O
section	O
that	O
repeated	O
stoller	O
splits	O
do	O
not	O
result	O
in	O
good	O
classifiers	O
the	O
reason	O
is	O
that	O
optimization	O
is	O
over	O
a	O
collection	O
of	O
sets	O
that	O
is	O
not	O
guaranteed	O
to	O
improve	O
matters-witness	O
the	O
examples	O
provided	O
in	O
previous	O
sections	O
a	O
good	O
cutting	O
method	O
is	O
one	O
that	O
includes	O
somehow	O
many	O
not	O
too	O
many	O
small	O
sets	O
for	O
example	O
let	O
us	O
split	O
at	O
the	O
root	O
making	O
d	O
hyperplane	O
cuts	O
at	O
once	O
that	O
is	O
by	O
finding	O
the	O
d	O
cuts	O
that	O
together	O
produce	O
the	O
largest	O
decrease	O
in	O
the	O
empirical	B
error	I
probability	O
then	O
repeat	O
this	O
step	O
recursively	O
in	O
each	O
region	O
k	O
times	O
the	O
procedure	O
is	O
consistent	O
under	O
the	O
same	O
conditions	O
on	O
k	O
as	O
in	O
theorem	O
whenever	O
x	O
has	O
a	O
density	O
problem	O
the	O
d	O
hyperplane	O
cuts	O
may	O
be	O
considered	O
as	O
an	O
elementary	B
cut	I
which	O
is	O
repeated	O
in	O
a	O
greedy	B
manner	O
in	O
figures	O
to	O
we	O
show	O
a	O
few	O
elementary	O
cuts	O
that	O
may	O
be	O
repeated	O
greedily	O
for	O
a	O
consistent	O
classifier	B
the	O
straightforward	O
proofs	O
of	O
consistency	B
are	O
left	O
to	O
the	O
reader	O
in	O
problem	O
proof	O
of	O
theorem	O
we	O
restrict	O
ourselves	O
to	O
n	O
but	O
the	O
proof	O
remains	O
similar	O
in	O
n	O
d	O
the	O
notation	O
ln	O
was	O
introduced	O
above	O
where	O
the	O
argument	O
is	O
allowed	O
to	O
be	O
a	O
partition	B
or	O
a	O
set	O
in	O
a	O
partition	B
we	O
similarly	O
define	O
lr	O
min	O
px	O
e	O
r	O
y	O
iy	O
yeo	O
i	O
lp	B
minvor	O
vir	O
l	O
lr	O
rep	O
figure	O
an	O
elementary	B
cut	I
here	O
is	O
composed	O
of	O
d	O
hyperplane	O
cuts	O
they	O
are	O
jointly	O
optimized	O
figure	O
rectangular	O
cuts	O
determine	O
an	O
elementary	B
cut	I
all	O
cuts	O
have	O
arbitrary	O
directions	O
there	O
are	O
no	O
forced	O
directions	O
a	O
greedy	B
classifier	B
figure	O
simplex	O
cuts	O
a	O
cut	O
is	O
determined	O
by	O
a	O
polyhedron	O
with	O
d	O
vertices	O
the	O
simplices	O
are	O
not	O
allowed	O
to	O
overlap	O
just	O
as	O
for	O
rectangular	O
cuts	O
in	O
the	O
greedy	B
classifier	B
three	O
consecutive	O
simplex	O
cuts	O
in	O
are	O
shown	O
here	O
mal	O
two	O
consecutive	O
rectangular	O
grid	O
figure	O
at	O
every	O
iteration	O
we	O
fine	O
the	O
partition	B
by	O
selecting	O
that	O
angle	O
r	O
in	O
the	O
partition	B
and	O
that	O
x	O
x	O
x	O
rectangular	O
grid	O
cut	O
of	O
r	O
for	O
which	O
the	O
empirical	B
error	I
is	O
cuts	O
are	O
shown	O
for	O
example	O
let	O
denote	O
a	O
partition	B
of	O
r	O
into	O
a	O
rectangular	O
l	O
x	O
i	O
grid	O
figure	O
an	O
i	O
x	O
i	O
grid	O
i	O
here	O
it	O
is	O
clear	O
that	O
for	O
all	O
e	O
there	O
exists	O
an	O
i	O
i	O
and	O
an	O
i	O
x	O
i	O
grid	O
such	O
that	O
if	O
q	O
is	O
another	O
finer	O
partition	B
into	O
rectangles	O
each	O
set	O
of	O
q	O
is	O
a	O
rectangle	O
and	O
intersects	O
at	O
most	O
one	O
rectangle	O
of	O
then	O
necessarily	O
lq	O
l	O
we	O
will	O
call	O
q	O
a	O
refinement	O
of	O
the	O
next	O
lemma	O
is	O
a	O
key	O
property	O
of	O
partitioning	O
classifiers	O
in	O
our	O
eyes	O
it	O
is	O
the	O
main	O
technical	O
property	O
of	O
this	O
entire	O
chapter	O
we	O
say	O
that	O
the	O
partition	B
t	O
is	O
an	O
extension	O
of	O
p	O
by	O
a	O
set	O
q-where	O
q	O
rep-if	O
t	O
contains	O
all	O
cells	O
of	O
p	O
other	O
than	O
r	O
plus	O
q	O
and	O
r	O
q	O
tree	O
classifiers	O
lemma	O
let	O
oz	O
be	O
a	O
finite	O
partition	B
with	O
l	O
l	O
e	O
let	O
p	O
be	O
a	O
finite	O
partition	B
oird	O
and	O
let	O
q	O
be	O
a	O
refinement	O
oiboth	O
p	O
and	O
oz	O
then	O
there	O
exists	O
a	O
set	O
q	O
e	O
q	O
q	O
is	O
contained	O
in	O
one	O
set	O
oip	O
only	O
and	O
an	O
extension	O
oip	O
by	O
q	O
to	O
tq	O
such	O
that	O
if	O
lp	B
l	O
e	O
proof	O
first	O
fix	O
rep	O
and	O
let	O
q	O
q	O
n	O
be	O
the	O
sets	O
of	O
q	O
contained	O
in	O
r	O
define	O
il	O
lr	O
minp	O
q	O
first	O
we	O
show	O
that	O
there	O
exists	O
an	O
integer	O
i	O
such	O
that	O
lr	O
lq	O
lr	O
q	O
lr	O
lq	O
or	O
equivalently	O
where	O
il	O
i	O
minp	O
q	O
minpi	O
qi	O
minp	O
pi	O
q	O
qi	O
to	O
see	O
this	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
p	O
q	O
if	O
pi	O
qi	O
for	O
all	O
i	O
then	O
minp	O
q	O
i	O
minpi	O
qi	O
p	O
i	O
pi	O
n	O
n	O
ll	O
il	O
so	O
we	O
are	O
done	O
assume	O
therefore	O
that	O
pi	O
qi	O
for	O
i	O
e	O
a	O
where	O
a	O
is	O
a	O
set	O
of	O
indices	O
with	O
i	O
a	O
i	O
for	O
such	O
i	O
and	O
thus	O
i	O
ili	O
ipi	O
qj	O
p	O
i	O
pi	O
iqi	O
p	O
lminpi	O
qj	O
n	O
iea	O
lea	O
i	O
iea	O
il	O
but	O
then	O
ail	O
a	O
iai	O
iea	O
i	O
max	O
u	O
lsisn	O
u	O
i	O
p	O
ll	O
minpi	O
qi	O
iai	O
p	O
ll	O
minpi	O
qi	O
n	O
and	O
the	O
claim	O
follows	O
to	O
prove	O
the	O
lemma	O
notice	O
that	O
since	O
lq	O
l	O
e	O
it	O
suffices	O
to	O
show	O
that	O
a	O
greedy	B
classifier	B
maxlp	O
ltq	O
qeq	O
lp	B
lq	O
q	O
i	O
i	O
or	O
equivalently	O
that	O
maxlr	O
q	O
lq	O
lr	O
q	O
q	O
qeq	O
lp	B
lq	O
q	O
i	O
i	O
where	O
rq	O
is	O
the	O
unique	O
cell	O
of	O
p	O
containing	O
q	O
however	O
maxlr	O
q	O
lq	O
lr	O
q	O
q	O
qeq	O
max	O
lq	O
lr	O
q	O
repqr	O
lr	O
lqcr	O
lq	O
iri	O
max	O
rep	O
the	O
inequality	B
shown	O
above	O
where	O
i	O
r	O
i	O
denotes	O
the	O
number	O
of	O
sets	O
of	O
q	O
contained	O
in	O
r	O
lrep	O
lqr	O
lq	O
lrep	O
iri	O
lrep	O
lr	O
lqeq	O
lq	O
iqi	O
lp	B
lq	O
iqi	O
and	O
the	O
lemma	O
is	O
proved	O
let	O
us	O
return	O
to	O
the	O
proof	O
of	O
theorem	O
the	O
previous	O
lemma	O
applied	O
to	O
our	O
situation	O
pi	O
shows	O
that	O
we	O
may	O
extend	O
pi	O
by	O
a	O
rectangle	O
q	O
e	O
q	O
q	O
will	O
e	O
is	O
be	O
a	O
collection	O
of	O
rectangles	O
refining	O
both	O
qz	O
and	O
pi	O
such	O
that	O
ltq	O
smaller	O
by	O
a	O
guaranteed	O
amount	O
than	O
lpi	O
e	O
is	O
the	O
partition	B
obtained	O
by	O
extending	O
pi	O
to	O
describe	O
q	O
we	O
do	O
the	O
following	O
that	O
q	O
must	O
consist	O
entirely	O
of	O
rectangles	O
for	O
otherwise	O
the	O
lemma	O
is	O
useless	O
take	O
the	O
rectangles	O
of	O
pi	O
and	O
extend	O
all	O
four	O
sides	O
the	O
order	O
of	O
birth	O
of	O
the	O
rectangles	O
until	O
they	O
hit	O
a	O
side	O
of	O
another	O
rectangle	O
or	O
an	O
extended	O
border	O
of	O
a	O
previous	O
rectangle	O
if	O
they	O
hit	O
anything	O
at	O
all	O
figure	O
illustrates	O
the	O
partition	B
into	O
rectangles	O
note	O
that	O
this	O
partition	B
consists	O
of	O
line	O
segments	O
and	O
at	O
most	O
rectangles	O
can	O
be	O
seen	O
by	O
noting	O
that	O
we	O
can	O
write	O
in	O
each	O
non	O
original	O
rectangle	O
of	O
pi	O
the	O
number	O
of	O
an	O
original	O
neighboring	O
rectangle	O
each	O
number	O
appearing	O
at	O
most	O
times	O
tree	O
classifiers	O
figure	O
extensions	O
of	O
rectangles	O
to	O
get	O
a	O
rectangular	O
grid	O
the	O
rectangular	O
grid	O
partition	B
thus	O
obtained	O
is	O
intersected	O
with	O
to	O
yield	O
q	O
to	O
apply	O
lemma	O
we	O
only	O
need	O
a	O
bound	O
on	O
i	O
qi	O
to	O
the	O
rectangular	O
partition	B
just	O
created	O
we	O
add	O
each	O
of	O
the	O
lines	O
of	O
the	O
grid	O
one	O
by	O
one	O
start	O
with	O
the	O
horizontal	O
lines	O
each	O
time	O
one	O
is	O
added	O
it	O
creates	O
at	O
most	O
new	O
rectangles	O
then	O
each	O
vertical	O
line	O
adds	O
at	O
most	O
new	O
rectangles	O
for	O
a	O
total	O
of	O
not	O
more	O
than	O
l	O
hence	O
i	O
qi	O
apply	O
the	O
lemma	O
and	O
observe	O
that	O
there	O
is	O
a	O
rectangle	O
q	O
e	O
q	O
such	O
that	O
the	O
extension	O
of	O
pi	O
by	O
q	O
to	O
p	O
would	O
yield	O
at	O
this	O
point	O
in	O
the	O
proof	O
the	O
reader	O
can	O
safely	O
forget	O
q	O
it	O
has	O
done	O
what	O
it	O
was	O
supposed	O
to	O
do	O
of	O
course	O
we	O
cannot	O
hope	O
to	O
find	O
p	O
because	O
we	O
do	O
not	O
know	O
the	O
distribution	O
let	O
us	O
denote	O
the	O
actual	O
new	O
partition	B
by	O
and	O
let	O
ril	O
be	O
the	O
selected	O
rectangle	O
by	O
the	O
empirical	B
minimization	O
observe	O
that	O
lpid	O
lp	B
lnpid	O
lnp	O
lp	B
lp	B
as	O
pi	O
and	O
n	O
have	O
most	O
sets	O
in	O
comm	O
many	O
terms	O
cancel	O
in	O
the	O
last	O
ble	O
sum	O
we	O
are	O
left	O
with	O
just	O
l	O
and	O
ln	O
terms	O
for	O
sets	O
of	O
the	O
form	O
r	O
ril	O
r	O
q	O
q	O
with	O
r	O
e	O
pi	O
thus	O
lp	B
sup	O
ilnr	O
lri	O
r	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
rectangles	O
of	O
the	O
above	O
described	O
form	O
these	O
are	O
sets	O
of	O
the	O
form	O
obtainable	O
in	O
pi	O
every	O
such	O
set	O
can	O
be	O
written	O
as	O
the	O
difference	O
of	O
a	O
infinite	O
rectangle	O
and	O
at	O
most	O
i	O
nonoverlapping	O
contained	O
rectangles	O
as	O
i	O
k	O
in	O
our	O
analysis	O
we	O
call	O
zk	O
the	O
class	O
of	O
all	O
sets	O
a	O
greedy	B
classifier	B
zk	O
where	O
zo	O
zl	O
zk	O
that	O
may	O
be	O
described	O
in	O
this	O
form	O
zo	O
zl	O
are	O
rectangles	O
each	O
zi	O
zo	O
zl	O
zk	O
are	O
mutually	O
disjoint	O
rectangles	O
may	O
be	O
infinite	O
or	O
half	O
infinite	O
hence	O
for	O
i	O
k	O
lf	O
sup	O
ilnz	O
lzi	O
zezk	O
fof	O
a	O
fixed	O
z	O
e	O
zk	O
l	O
nz	O
lzi	O
minvonz	O
minvoz	O
ivonz	O
thus	O
l	O
j	O
vn	O
sup	O
p	O
def	O
ivonz	O
define	O
the	O
event	O
zezk	O
g	O
where	O
will	O
be	O
picked	O
later	O
on	O
g	O
we	O
see	O
that	O
for	O
all	O
i	O
k	O
e	O
lp	B
e	O
e	O
l	O
we	O
now	O
introduce	O
a	O
convenient	O
lemma	O
lemma	O
let	O
an	O
bn	O
be	O
sequences	O
of	O
positive	O
numbers	O
with	O
bn	O
t	O
bo	O
and	O
let	O
be	O
fixed	O
if	O
an	O
bn	O
then	O
proof	O
we	O
have	O
an	O
aoe-ejob	O
j	O
ao	O
bj	O
i	O
i	O
i	O
i	O
i	O
i	O
clearly	O
i	O
ao	O
exp	O
lo	O
b	O
j	O
a	O
trivial	O
bound	O
for	O
i	O
i	O
i	O
i	O
i	O
is	O
conclude	O
that	O
on	O
g	O
e	O
e-	O
llsl	O
tree	O
classifiers	O
thus	O
plpk	O
l	O
p	O
ge	O
o	O
when	O
ok	O
and	O
e	O
finally	O
introduce	O
the	O
notation	O
lnr	O
lnp	O
px	O
e	O
r	O
y	O
i	O
gpx	O
l	O
lnr	O
rep	O
as	O
lpk	O
takes	O
the	O
partition	B
into	O
account	O
but	O
not	O
the	O
majority	B
vote	I
and	O
lnpk	O
does	O
we	O
need	O
a	O
small	O
additional	O
argument	O
lnpk	O
lnpk	O
lpk	O
lpk	O
l	O
ivi	O
vlnri	O
lpk	O
repk	O
putting	O
things	O
together	O
p	O
l	O
p	O
e	O
p	O
l	O
p	O
e	O
p	O
o	O
under	O
the	O
given	O
conditions	O
on	O
and	O
k	O
observe	O
that	O
if	O
for	O
a	O
set	O
z	O
def	O
unz	O
ivoz	O
ivlz	O
vlnzi	O
then	O
vn	O
sup	O
unz	O
sup	O
rectangles	O
z	O
disjoint	O
sets	O
of	O
k	O
rectangles	O
zi	O
k	O
lunzi	O
il	O
sup	O
rectangles	O
z	O
wn	O
sup	O
unz	O
rectangles	O
z	O
we	O
may	O
now	O
use	O
the	O
vapnik-chervonenkis	B
inequality	B
and	O
to	O
conclude	O
that	O
for	O
all	O
e	O
p	O
sup	O
unz	O
e	O
rectangles	O
z	O
problems	O
and	O
exercises	O
armed	O
with	O
this	O
we	O
have	O
pwn	O
e	O
pvn	O
o	O
e	O
both	O
terms	O
tend	O
to	O
with	O
n	O
if	O
n	O
and	O
n	O
take	O
e	O
i	O
to	O
satisfy	O
our	O
earlier	O
side	O
condition	O
and	O
note	O
that	O
we	O
need	O
l	O
by	O
log	O
n	O
we	O
in	O
fact	O
have	O
strong	O
convergence	O
to	O
for	O
lnpk	O
the	O
borel-cantelli	B
lemma	I
the	O
crucial	O
element	O
in	O
the	O
proof	O
of	O
theorem	O
is	O
the	O
fact	O
that	O
the	O
number	O
of	O
sets	O
in	O
the	O
partition	B
q	O
grows	O
at	O
most	O
linearly	O
in	O
i	O
the	O
iteration	O
number	O
had	O
it	O
grown	O
quadratically	O
say	O
it	O
would	O
not	O
have	O
been	O
good	O
enough-we	O
would	O
not	O
have	O
had	O
guaranteed	O
improvements	O
of	O
large	O
enough	O
sizes	O
to	O
push	O
the	O
error	O
probability	O
towards	O
l	O
in	O
the	O
multidimensional	O
version	O
and	O
in	O
extension	O
to	O
other	O
types	O
of	O
cuts	O
this	O
is	O
virtually	O
the	O
only	O
thing	O
that	O
must	O
be	O
verified	O
for	O
hyperplane	O
cuts	O
an	O
additional	O
inequality	B
of	O
the	O
vc	O
type	O
is	O
needed	O
extending	O
that	O
for	O
classes	O
of	O
hyperplanes	O
our	O
proof	O
is	O
entirely	O
combinatorial	O
and	O
geometric	B
and	O
comes	O
with	O
explicit	O
bounds	O
the	O
only	O
reference	O
to	O
the	O
bayes	B
error	I
we	O
need	O
is	O
the	O
quantity	O
ie	O
which	O
is	O
the	O
smallest	O
value	O
l	O
for	O
which	O
inf	O
alllxl	O
grids	O
lyl	O
l	O
e	O
it	O
depends	O
heavily	O
on	O
the	O
distribution	O
of	O
course	O
calli	O
the	O
grid	B
complexity	I
of	O
the	O
distribution	O
for	O
lack	O
of	O
a	O
better	O
term	O
for	O
example	O
if	O
x	O
is	O
discrete	O
and	O
takes	O
values	O
on	O
the	O
hypercube	O
ld	O
then	O
ie	O
if	O
x	O
takes	O
values	O
on	O
amd	O
then	O
ie	O
ld	O
if	O
x	O
has	O
an	O
arbitrary	O
distribution	O
on	O
the	O
real	O
line	O
and	O
is	O
monotone	O
then	O
l	O
e	O
however	O
if	O
is	O
unimodal	O
l	O
e	O
in	O
one	O
dimension	B
ie	O
is	O
sensitive	O
to	O
the	O
number	O
of	O
places	O
where	O
the	O
bayes	O
decision	O
changes	O
in	O
two	O
dimensions	O
however	O
l	O
measures	O
the	O
complexity	O
of	O
the	O
distribution	O
especially	O
near	O
regions	O
with	O
when	O
x	O
is	O
uniform	O
on	O
and	O
y	O
if	O
and	O
only	O
if	O
the	O
components	O
of	O
x	O
sum	O
to	O
less	O
than	O
one	O
then	O
i	O
i	O
as	O
e	O
for	O
example	O
the	O
grid	B
complexity	I
is	O
eminently	O
suited	O
to	O
study	O
rates	O
of	O
convergence	O
as	O
it	O
is	O
explicitly	O
featured	O
in	O
the	O
inequalities	O
of	O
our	O
proof	O
there	O
is	O
no	O
room	O
in	O
this	O
book	O
for	O
following	O
this	O
thread	O
however	O
problems	O
and	O
exercises	O
problem	O
let	O
x	O
be	O
a	O
random	O
vector	O
with	O
density	O
f	O
on	O
nd	O
show	O
that	O
each	O
component	O
has	O
a	O
density	O
as	O
well	O
problem	O
show	O
that	O
both	O
condition	O
and	O
condition	O
of	O
theorem	O
are	O
necessary	O
for	O
consistency	B
of	O
trees	O
with	O
the	O
x	O
tree	O
classifiers	O
problem	O
for	O
a	O
theoretical	B
median	B
tree	I
in	O
n	O
d	O
with	O
uniform	O
marginals	O
and	O
k	O
levels	O
of	O
splitting	O
show	O
that	O
ehx	O
vex	O
when	O
k	O
is	O
a	O
multiple	O
of	O
d	O
d	O
problem	O
a-balanced	B
trees	O
consider	O
the	O
following	O
generalization	O
of	O
the	O
median	B
tree	I
at	O
a	O
node	O
in	O
the	O
tree	O
that	O
represents	O
n	O
points	O
if	O
a	O
split	O
occurs	O
both	O
subtrees	O
must	O
be	O
of	O
size	O
at	O
least	O
an	O
for	O
some	O
fixed	O
a	O
o	O
repeat	O
the	O
splits	O
for	O
k	O
levels	O
resulting	O
in	O
leaf	O
regions	O
the	O
tree	O
has	O
the	O
x	O
and	O
the	O
classifier	B
is	O
natural	O
however	O
the	O
points	O
at	O
which	O
cuts	O
are	O
made	O
are	O
picked	O
among	O
the	O
data	O
points	O
in	O
an	O
arbitrary	O
fashion	O
the	O
splits	O
rotate	O
through	O
the	O
coordinate	O
axes	O
generalize	O
the	O
consistency	B
theorem	O
for	O
median	O
trees	O
problem	O
consider	O
the	O
following	O
tree	O
classifier	B
first	O
we	O
find	O
the	O
median	O
according	O
to	O
one	O
coordinate	O
and	O
create	O
two	O
subtrees	O
of	O
sizes	O
len	O
and	O
rn	O
repeat	O
this	O
at	O
each	O
level	O
cutting	O
the	O
next	O
coordinate	O
axis	O
in	O
a	O
rotational	O
manner	O
do	O
not	O
cut	O
a	O
node	O
any	O
further	O
if	O
either	O
all	O
data	O
points	O
in	O
the	O
corresponding	O
region	O
have	O
identical	O
yi	O
values	O
or	O
the	O
region	O
contains	O
less	O
than	O
k	O
points	O
prove	O
that	O
the	O
obtained	O
natural	B
classifier	B
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
if	O
k	O
and	O
log	O
n	O
k	O
o	O
problem	O
let	O
m	O
be	O
a	O
probability	O
measure	O
with	O
a	O
density	O
f	O
on	O
n	O
d	O
and	O
define	O
c	O
x	O
cd	O
m	O
xci	O
e	O
x	O
x	O
xed	O
en	O
all	O
e	O
o	O
show	O
that	O
m	O
c	O
hint	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
lemma	O
a	O
l	O
in	O
the	O
appendix	O
problem	O
let	O
ul	O
u	O
cn	O
be	O
uniform	O
order	B
statistics	I
defining	O
spacings	O
sl	O
snl	O
with	O
si	O
ui	O
uu-l	O
if	O
unl	O
and	O
uo	O
o	O
show	O
that	O
sl	O
snl	O
are	O
identically	O
distributed	O
psl	O
x	O
xy	O
x	O
e	O
esil	O
ln	O
problem	O
in	O
the	O
proof	O
of	O
theorem	O
we	O
assumed	O
in	O
the	O
second	O
part	O
that	O
horizontal	O
and	O
vertical	O
cuts	O
were	O
meted	O
out	O
independently	O
return	O
to	O
the	O
proof	O
and	O
see	O
how	O
you	O
can	O
modify	O
it	O
to	O
take	O
care	O
of	O
the	O
forced	O
alternating	O
cuts	O
problem	O
let	O
xl	O
x	O
xn	O
be	O
i	O
i	O
d	O
with	O
common	O
density	O
f	O
on	O
the	O
real	O
line	O
call	O
xi	O
a	O
record	B
if	O
xi	O
minx	O
i	O
xj	O
let	O
k	O
fx	O
is	O
a	O
record	B
show	O
that	O
r	O
l	O
rn	O
are	O
independent	O
and	O
that	O
p	O
i	O
i	O
conclude	O
that	O
the	O
expected	O
number	O
of	O
records	O
is	O
logn	O
problem	O
the	O
deep	B
k-d	B
tree	I
assume	O
that	O
x	O
has	O
a	O
density	O
f	O
and	O
that	O
all	O
marginal	O
densities	O
are	O
uniform	O
show	O
that	O
k	O
implies	O
that	O
diamax	O
in	O
probability	O
do	O
this	O
by	O
arguing	O
that	O
diamax	O
in	O
probability	O
for	O
the	O
chronological	B
k-d	B
tree	I
with	O
the	O
same	O
parameter	O
k	O
in	O
a	O
random	O
k	O
tree	O
with	O
n	O
elements	O
show	O
that	O
the	O
depth	O
d	O
of	O
the	O
last	O
inserted	O
node	O
satisfies	O
d	O
i	O
log	O
n	O
i	O
in	O
probability	O
argue	O
first	O
that	O
you	O
may	O
restrict	O
yourself	O
to	O
d	O
then	O
write	O
d	O
as	O
a	O
sum	O
of	O
indicators	O
of	O
records	O
conclude	O
by	O
computing	O
ed	O
and	O
vard	O
or	O
at	O
least	O
bounding	O
these	O
quantities	O
in	O
an	O
appropriate	O
manner	O
improve	O
the	O
second	O
condition	O
of	O
theorem	O
to	O
log	O
nijlog	O
n	O
that	O
it	O
is	O
possible	O
that	O
lim	O
supn-hxl	O
k	O
i	O
log	O
n	O
hint	O
show	O
that	O
i	O
d	O
log	O
n	O
i	O
jlog	O
n	O
in	O
probability	O
by	O
referring	O
to	O
the	O
previous	O
part	O
of	O
this	O
exercise	O
problems	O
and	O
exercises	O
problem	O
consider	O
a	O
full-fledged	O
k-d	B
tree	I
with	O
n	O
external	O
node	O
regions	O
let	O
ln	O
be	O
the	O
probability	O
of	O
error	O
if	O
classification	O
is	O
based	O
upon	O
this	O
tree	O
and	O
if	O
external	O
node	O
regions	O
are	O
assigned	O
the	O
labels	O
of	O
their	O
immediate	O
parents	O
figure	O
figure	O
the	O
external	O
nodes	O
are	O
signed	O
the	O
classes	O
of	O
their	O
parents	O
leaf	O
region	O
show	O
that	O
whenever	O
x	O
has	O
a	O
density	O
e	O
ln	O
lnn	O
just	O
as	O
for	O
the	O
i-nearest	O
neighbor	O
rule	B
problem	O
continuation	O
show	O
that	O
ln	O
lnn	O
in	O
probability	O
problem	O
meisel	O
and	O
propose	O
a	O
binary	B
tree	O
classifier	B
with	O
perpendicular	O
cuts	O
in	O
which	O
all	O
leaf	O
regions	O
are	O
homogeneous	O
that	O
is	O
all	O
yis	O
for	O
the	O
xis	O
in	O
the	O
same	O
region	O
are	O
identical	O
figure	O
example	O
of	O
a	O
tree	O
partition	B
into	O
homogeneous	O
regions	O
if	O
l	O
give	O
a	O
stopping	O
rule	B
for	O
constructing	O
a	O
consistent	O
rule	B
whenever	O
x	O
has	O
a	O
density	O
if	O
l	O
show	O
that	O
there	O
exists	O
a	O
consistent	O
homogeneous	O
partition	B
classifier	B
with	O
on	O
expected	O
number	O
of	O
cells	O
if	O
l	O
show	O
a	O
complicated	O
way	O
of	O
constructing	O
a	O
homogeneous	O
tition	O
that	O
yields	O
a	O
tree	O
classifier	B
with	O
e	O
ln	O
l	O
whenever	O
x	O
has	O
a	O
density	O
hint	O
first	O
make	O
a	O
consistent	O
nonhomogeneous	O
binary	B
tree	O
classifier	B
and	O
refine	O
it	O
to	O
make	O
it	O
homogeneous	O
if	O
l	O
then	O
show	O
that	O
the	O
expected	O
number	O
of	O
homogeneous	O
regions	O
is	O
at	O
least	O
en	O
for	O
some	O
e	O
such	O
rules	O
are	O
therefore	O
not	O
practical	O
unless	O
l	O
overfitting	B
will	O
occur	O
problem	O
let	O
x	O
be	O
uniform	O
on	O
and	O
let	O
y	O
be	O
independent	O
of	O
x	O
with	O
py	O
i	O
p	O
find	O
a	O
tree	O
classifier	B
based	O
upon	O
simple	O
interval	O
splitting	O
for	O
which	O
each	O
region	O
has	O
one	O
data	O
point	O
and	O
hmmf	O
p-o	O
lim	O
infn-oo	O
eln	O
p	O
tree	O
classifiers	O
we	O
know	O
that	O
for	O
the	O
nearest	B
neighbor	I
rule	B
hm	O
p-o	O
lim	O
supn-cxl	O
eln	O
p	O
so	O
that	O
the	O
given	O
interval	O
splitting	O
method	O
is	O
worse	O
than	O
the	O
nearest	B
neighbor	I
method	O
provides	O
a	O
counterexample	O
to	O
a	O
conjecture	O
of	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
hint	O
sort	O
the	O
points	O
and	O
adjust	O
the	O
sizes	O
of	O
the	O
intervals	O
given	O
to	O
the	O
class	O
and	O
class	O
points	O
in	O
favor	O
of	O
the	O
by	O
doing	O
so	O
the	O
asymptotic	O
probability	O
of	O
error	O
can	O
be	O
made	O
as	O
close	O
as	O
desired	O
to	O
p	O
problem	O
continuation	O
let	O
x	O
be	O
uniform	O
on	O
and	O
let	O
y	O
be	O
independent	O
of	O
x	O
with	O
p	O
i	O
p	O
find	O
a	O
data-based	B
tree	O
classifier	B
based	O
upon	O
perpendicular	O
cuts	O
for	O
which	O
each	O
region	O
has	O
one	O
data	O
point	O
diamax	O
in	O
probability	O
theorems	O
and	O
and	O
hmmf	O
p-o	O
lim	O
infn-	O
cxl	O
eln	O
p	O
conclude	O
that	O
in	O
n	O
d	O
we	O
can	O
construct	O
tree	O
classifiers	O
with	O
one	O
point	O
per	O
cell	O
that	O
are	O
much	O
worse	O
than	O
the	O
nearest	B
neighbor	I
rule	B
hint	O
the	O
next	O
two	O
problems	O
may	O
help	O
you	O
with	O
the	O
construction	O
and	O
analysis	O
problem	O
continuation	O
draw	O
ani	O
i	O
d	O
sample	O
xl	O
xn	O
from	O
the	O
butionon	O
and	O
let	O
yi	O
yn	O
bei	O
i	O
d	O
and	O
independent	O
of	O
the	O
xis	O
withpy	O
i	O
p	O
construct	O
a	O
binary	B
tree	O
partition	B
with	O
perpendicular	O
cuts	O
for	O
yi	O
i	O
such	O
that	O
every	O
leaf	O
region	O
has	O
one	O
and	O
only	O
one	O
point	O
and	O
diamax	O
in	O
probability	O
how	O
would	O
you	O
proceed	O
avoiding	O
putting	O
xis	O
on	O
borders	O
of	O
regions	O
prove	O
diamax	O
in	O
probability	O
add	O
the	O
yi	O
pairs	O
with	O
yi	O
to	O
the	O
leaf	O
regions	O
so	O
that	O
every	O
region	O
has	O
one	O
observation	O
and	O
zero	O
or	O
more	O
class-o	O
observations	O
give	O
the	O
observation	O
the	O
largest	O
area	O
containing	O
no	O
class-o	O
points	O
as	O
shown	O
in	O
figure	O
show	O
that	O
this	O
can	O
always	O
be	O
done	O
by	O
adding	O
perpendicular	O
cuts	O
and	O
keeping	O
at	O
least	O
one	O
observation	O
per	O
region	O
figure	O
cutting	O
a	O
rectangle	O
by	O
giving	O
a	O
large	O
area	O
to	O
the	O
single	O
point	O
partition	B
all	O
rectangles	O
with	O
more	O
than	O
one	O
point	O
further	O
to	O
finally	O
obtain	O
a	O
point-per-leaf-region	O
partition	B
if	O
there	O
are	O
n	O
points	O
in	O
a	O
region	O
of	O
the	O
tree	O
before	O
the	O
class-l	O
and	O
class-o	O
points	O
are	O
separated	O
n	O
class-o	O
points	O
and	O
one	O
point	O
then	O
show	O
that	O
the	O
expected	O
proportion	O
of	O
the	O
regions	O
area	O
given	O
to	O
class	O
given	O
n	O
times	O
n	O
tends	O
to	O
explicit	O
lower	O
bound	O
will	O
be	O
helpful	O
hint	O
use	O
the	O
next	O
problem	O
write	O
the	O
probability	O
of	O
error	O
for	O
the	O
rule	B
in	O
terms	O
of	O
areas	O
of	O
rectangles	O
and	O
use	O
part	O
to	O
get	O
an	O
asymptotic	O
lower	O
bound	O
problems	O
and	O
exercises	O
now	O
let	O
p	O
tend	O
to	O
zero	O
and	O
get	O
an	O
asymptotic	O
expression	B
for	O
your	O
lower	O
bound	O
in	O
terms	O
of	O
p	O
compare	O
this	O
with	O
p	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
i-nearest	O
neighbor	O
rule	B
problem	O
continuation	O
draw	O
a	O
sample	O
xl	O
xn	O
of	O
n	O
i	O
i	O
d	O
observations	O
formly	O
distributed	O
on	O
id	O
the	O
rectangles	O
defined	O
by	O
the	O
origin	O
and	O
x	O
i	O
xn	O
as	O
posite	O
vertices	O
are	O
denoted	O
by	O
r	O
i	O
rn	O
respectively	O
the	O
probability	O
content	O
of	O
ri	O
is	O
clearly	O
flri	O
nl	O
xij	O
study	O
mn	O
maxflri	O
i	O
n	O
ri	O
does	O
not	O
contain	O
r	O
j	O
for	O
any	O
j	O
i	O
the	O
probability	O
content	O
of	O
the	O
largest	O
empty	O
rectangle	O
with	O
a	O
vertex	O
at	O
the	O
origin	O
for	O
d	O
mil	O
is	O
just	O
the	O
minimum	O
of	O
the	O
xs	O
and	O
thus	O
nmn	O
e	O
where	O
e	O
is	O
the	O
exponential	B
distribution	I
also	O
emn	O
ln	O
for	O
d	O
mn	O
is	O
larger	O
show	O
that	O
nmn	O
in	O
probability	O
and	O
try	O
obtaining	O
the	O
first	O
term	O
in	O
the	O
rate	O
of	O
increase	O
problem	O
show	O
that	O
diamax	O
in	O
probability	O
for	O
the	O
chronological	B
quadtree	B
whenever	O
k	O
and	O
x	O
has	O
a	O
density	O
hint	O
mimic	O
the	O
proof	O
for	O
the	O
chronological	B
quadtree	B
problem	O
show	O
that	O
the	O
deep	B
quadtree	B
is	O
consistent	O
if	O
x	O
has	O
a	O
density	O
and	O
k	O
levels	O
of	O
splits	O
are	O
applied	O
where	O
k	O
and	O
kl	O
log	O
n	O
o	O
leaf	O
problem	O
consider	O
a	O
full-fledged	O
quadtree	B
with	O
n	O
nodes	O
thus	O
regions	O
assign	O
to	O
each	O
region	O
the	O
y-iabel	O
of	O
its	O
parent	O
in	O
the	O
quadtree	B
with	O
this	O
simple	O
classifier	B
show	O
that	O
whenever	O
x	O
has	O
a	O
density	O
eln	O
l	O
nn	O
in	O
particular	O
lim	O
supn-	O
do	O
e	O
ln	O
and	O
the	O
classifier	B
is	O
consistent	O
when	O
l	O
o	O
problem	O
in	O
r	O
partition	B
the	O
space	O
as	O
follows	O
xl	O
define	O
nine	O
regions	O
by	O
vertical	O
and	O
horizontal	O
lines	O
through	O
them	O
x	O
x	O
k	O
are	O
sent	O
down	O
to	O
the	O
appropriate	O
subtrees	O
in	O
the	O
tree	O
and	O
within	O
each	O
subtree	O
with	O
at	O
least	O
two	O
points	O
the	O
process	O
is	O
repeated	O
recursively	O
a	O
decision	O
at	O
x	O
is	O
by	O
a	O
majority	B
vote	I
yki	O
yn	O
among	O
those	O
xis	O
in	O
the	O
same	O
rectangle	O
of	O
the	O
partition	B
as	O
x	O
show	O
that	O
if	O
k	O
kin	O
the	O
rule	B
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
problem	O
on	O
the	O
two-dimensional	O
counterexample	O
shown	O
in	O
the	O
text	O
for	O
multivariate	O
stoller	O
splits	O
prove	O
that	O
if	O
splits	O
are	O
performed	O
based	O
upon	O
a	O
sample	O
drawn	O
from	O
the	O
bution	O
and	O
if	O
we	O
stop	O
after	O
k	O
splits	O
with	O
k	O
depending	O
on	O
n	O
in	O
such	O
a	O
way	O
that	O
k	O
i	O
log	O
n	O
then	O
l	O
n	O
the	O
conditional	O
probability	O
of	O
error	O
satisfies	O
lim	O
infhdo	O
elj	O
hint	O
bound	O
the	O
probability	O
of	O
ever	O
splitting	O
anywhere	O
by	O
noting	O
that	O
the	O
maximal	O
ence	O
between	O
the	O
empirical	B
distribution	O
functions	O
for	O
the	O
first	O
coordinate	O
of	O
x	O
given	O
y	O
and	O
y	O
is	O
i	O
in	O
when	O
restricted	O
to	O
problem	O
let	O
x	O
have	O
the	O
uniform	O
distribution	O
on	O
and	O
let	O
y	O
so	O
that	O
l	O
o	O
construct	O
a	O
binary	B
classification	O
tree	O
by	O
selecting	O
at	O
each	O
iteration	O
the	O
split	O
that	O
minimizes	O
the	O
impurity	B
function	I
i	O
where	O
is	O
the	O
gini	O
criterion	O
consider	O
just	O
the	O
first	O
three	O
splits	O
made	O
in	O
this	O
manner	O
let	O
ln	O
be	O
the	O
probability	O
of	O
error	O
with	O
the	O
given	O
rule	B
a	O
majority	B
vote	I
over	O
the	O
leaf	O
regions	O
show	O
that	O
ln	O
in	O
probability	O
analyze	O
the	O
algorithm	B
when	O
the	O
gini	O
criterion	O
is	O
replaced	O
by	O
the	O
probability-of-error	O
criterion	O
problem	O
let	O
x	O
be	O
uniform	O
on	O
and	O
let	O
y	O
be	O
independent	O
of	O
x	O
with	O
ply	O
i	O
draw	O
a	O
sample	O
of	O
size	O
n	O
from	O
this	O
distribution	O
investigate	O
where	O
the	O
first	O
cut	O
tree	O
classifiers	O
might	O
take	O
place	O
based	O
upon	O
minimizing	O
the	O
impurity	B
function	I
with	O
p	O
gini	O
criterion	O
this	O
is	O
established	O
you	O
will	O
have	O
discovered	O
the	O
nature	O
of	O
the	O
classification	O
tree	O
roughly	O
speaking	O
problem	O
complete	O
the	O
consistency	B
proof	O
of	O
theorem	O
for	O
the	O
raw	B
bsp	O
tree	O
for	O
n	O
by	O
showing	O
that	O
diamax	O
in	O
probability	O
problem	O
balanced	B
bsp	O
trees	O
we	O
generalize	O
median	O
trees	O
to	O
allow	O
splitting	O
the	O
space	O
along	O
hyperplanes	O
figure	O
a	O
balanced	B
bsp	O
tree	O
each	O
hyperplane	O
cut	O
splits	O
a	O
region	O
into	O
two	O
cells	O
of	O
the	O
same	O
cardinality	O
assume	O
that	O
x	O
has	O
a	O
density	O
and	O
that	O
the	O
tree	O
possesses	O
the	O
x	O
keep	O
splitting	O
until	O
there	O
are	O
leaf	O
regions	O
as	O
with	O
median	O
trees	O
call	O
the	O
trees	O
balanced	B
bsp	O
trees	O
show	O
that	O
there	O
are	O
ways	O
of	O
splitting	O
in	O
n	O
that	O
lead	O
to	O
nonconsistent	O
rules	O
gardless	O
how	O
k	O
varies	O
with	O
n	O
if	O
every	O
splitting	O
hyperplane	O
is	O
forced	O
to	O
contain	O
d	O
data	O
points	O
nd	O
and	O
these	O
data	O
points	O
stay	O
with	O
the	O
splitting	O
node	O
are	O
not	O
sent	O
down	O
to	O
subtrees	O
then	O
show	O
that	O
once	O
again	O
there	O
exists	O
a	O
splitting	O
method	O
that	O
leads	O
to	O
nonconsistent	O
rules	O
regardless	O
of	O
how	O
k	O
varies	O
with	O
n	O
problem	O
letx	O
have	O
a	O
uniform	O
distribution	O
on	O
the	O
unit	O
ball	O
of	O
rd	O
let	O
y	O
so	O
that	O
l	O
o	O
assume	O
that	O
we	O
split	O
the	O
space	O
by	O
a	O
hyperplane	O
by	O
minimizing	O
an	O
impurity	B
function	I
based	O
upon	O
the	O
gini	O
criterion	O
if	O
is	O
very	O
large	O
where	O
approximately	O
would	O
the	O
cut	O
take	O
place	O
a	O
rotation	B
of	O
course	O
problem	O
there	O
exist	O
singular	O
continuous	O
distributions	O
that	O
admit	O
uniform	O
marginals	O
in	O
n	O
d	O
show	O
for	O
example	O
that	O
if	O
x	O
is	O
uniformly	O
distributed	O
on	O
the	O
surface	O
of	O
the	O
unit	O
sphere	B
of	O
then	O
its	O
three	O
components	O
are	O
all	O
uniformly	O
distributed	O
on	O
problem	O
verify	O
that	O
theorem	O
remains	O
valid	O
in	O
rd	O
problem	O
prove	O
that	O
theorem	O
remains	O
valid	O
if	O
rectangular	O
cuts	O
are	O
replaced	O
by	O
any	O
of	O
the	O
elementary	O
cuts	O
shown	O
on	O
figures	O
to	O
and	O
such	O
cuts	O
are	O
performed	O
recursively	O
k	O
times	O
always	O
by	O
maximizing	O
the	O
decrease	O
of	O
the	O
empirical	B
error	I
problem	O
show	O
that	O
theorem	O
remains	O
valid	O
if	O
k	O
ex	O
and	O
k	O
j	O
n	O
log	O
n	O
hint	O
in	O
the	O
proof	O
of	O
the	O
theorem	O
the	O
bound	O
on	O
and	O
w	O
is	O
loose	O
you	O
may	O
get	O
more	O
efficient	O
bounds	O
by	O
applying	O
theorem	O
from	O
the	O
next	O
chapter	O
problem	O
study	O
the	O
behavior	O
of	O
the	O
grid	B
complexity	I
as	O
e	O
t	O
for	O
the	O
following	O
cases	O
x	O
is	O
uniform	O
on	O
the	O
perimeter	O
of	O
the	O
unit	O
circle	O
of	O
r	O
with	O
probability	O
and	O
x	O
is	O
uniform	O
on	O
otherwise	O
let	O
y	O
if	O
and	O
only	O
if	O
x	O
is	O
on	O
the	O
perimeter	O
of	O
that	O
circle	O
that	O
l	O
x	O
x	O
is	O
uniform	O
on	O
f	O
and	O
y	O
if	O
and	O
only	O
if	O
xl	O
data-dependent	B
partitioning	O
introduction	O
in	O
chapter	O
we	O
investigated	O
properties	O
of	O
the	O
regular	B
histogram	B
rule	B
histogram	O
classifiers	O
partition	B
the	O
observation	O
space	O
nd	O
and	O
classify	O
the	O
input	O
vector	O
x	O
cording	O
to	O
a	O
majority	B
vote	I
among	O
the	O
ys	O
whose	O
corresponding	O
xs	O
fall	O
in	O
the	O
same	O
cell	O
of	O
the	O
partition	B
as	O
x	O
partitions	O
discussed	O
in	O
chapter	O
could	O
depend	O
on	O
the	O
sample	O
size	O
n	O
but	O
were	O
not	O
allowed	O
to	O
depend	O
on	O
the	O
data	O
dn	O
itself	O
we	O
dealt	O
mostly	O
with	O
grid	O
partitions	O
but	O
will	O
now	O
allow	O
other	O
partitions	O
as	O
well	O
just	O
consider	O
clustered	O
training	O
observations	O
xl	O
x	O
n	O
near	O
the	O
clusters	O
center	O
finer	O
partitions	O
are	O
called	O
for	O
similarly	O
when	O
the	O
components	O
have	O
different	O
physical	O
dimensions	O
the	O
scale	O
of	O
one	O
coordinate	O
axis	O
is	O
not	O
related	O
at	O
all	O
to	O
the	O
other	O
scales	O
and	O
some	O
data-adaptive	O
stretching	O
is	O
necessary	O
sometimes	O
the	O
data	O
are	O
trated	O
on	O
or	O
around	O
a	O
hyperplane	O
in	O
all	O
these	O
cases	O
although	O
consistent	O
the	O
regular	B
histogram	O
method	O
behaves	O
rather	O
poorly	O
especially	O
if	O
the	O
dimension	B
of	O
the	O
space	O
is	O
large	O
therefore	O
it	O
is	O
useful	O
to	O
allow	O
data-dependent	B
partitions	O
while	O
keeping	O
a	O
majority	O
voting	O
scheme	O
within	O
each	O
cell	O
the	O
simplest	O
data-dependent	B
partitioning	O
methods	O
are	O
based	O
on	O
statistically	B
equivalent	I
blocks	I
in	O
which	O
each	O
cell	O
contains	O
the	O
same	O
number	O
of	O
points	O
in	O
dimensional	O
problems	O
statistically	B
equivalent	I
blocks	I
reduce	O
to	O
k-spacing	O
estimates	O
where	O
the	O
k-th	O
etc	O
order	B
statistics	I
determine	O
the	O
partition	B
of	O
the	O
real	O
line	O
sometimes	O
it	O
makes	O
sense	O
to	O
cluster	O
the	O
data	O
points	O
into	O
groups	O
such	O
that	O
points	O
in	O
a	O
group	O
are	O
close	O
to	O
each	O
other	O
and	O
define	O
the	O
partition	B
so	O
that	O
each	O
group	O
is	O
in	O
a	O
different	O
cell	O
many	O
other	O
data-dependent	B
partitioning	O
schemes	O
have	O
been	O
introduced	O
in	O
the	O
lit	O
data-dependent	B
partitioning	O
erature	O
in	O
most	O
of	O
these	O
algorithms	O
the	O
cells	O
of	O
the	O
partition	B
correspond	O
to	O
the	O
leaves	O
of	O
a	O
binary	B
tree	O
which	O
makes	O
computation	O
of	O
the	O
corresponding	O
classification	O
rule	B
fast	O
and	O
convenient	O
tree	O
classifiers	O
were	O
dealt	O
with	O
in	O
chapter	O
analysis	O
of	O
versal	O
consistency	B
for	O
these	O
algorithms	O
and	O
corresponding	O
density	O
estimates	O
was	O
begun	O
by	O
abou-jaoude	O
and	O
gordon	O
and	O
olshen	O
in	O
a	O
general	O
framework	O
and	O
was	O
extended	O
for	O
example	O
by	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
chen	O
and	O
zhao	O
zhao	O
krishnaiah	O
and	O
chen	O
lugosi	O
and	O
nobel	O
and	O
nobel	O
this	O
chapter	O
is	O
more	O
general	O
than	O
the	O
chapter	O
on	O
tree	O
classifiers	O
as	O
every	O
partition	B
induced	O
by	O
a	O
tree	O
classifier	B
is	O
a	O
valid	O
partition	B
of	O
space	O
but	O
not	O
vice	O
versa	O
the	O
example	O
below	O
shows	O
a	O
rectangular	O
partition	B
of	O
the	O
plane	O
that	O
cannot	O
be	O
obtained	O
by	O
consecutive	O
perpendicular	O
cuts	O
in	O
a	O
binary	B
classification	O
tree	O
figure	O
a	O
rectangular	O
partition	B
of	O
that	O
cannot	O
be	O
achieved	O
by	O
a	O
tree	O
of	O
consecutive	O
cuts	O
in	O
this	O
chapter	O
we	O
first	O
establish	O
general	O
sufficient	O
conditions	O
for	O
the	O
consistency	B
of	O
data-dependent	B
histogram	O
classifiers	O
because	O
of	O
the	O
complicated	O
dependence	O
of	O
the	O
partition	B
on	O
the	O
data	O
methods	O
useful	O
for	O
handling	O
regular	B
histograms	O
have	O
to	O
be	O
significantly	O
refined	O
the	O
main	O
tool	O
is	O
a	O
large	B
deviation	I
inequality	B
for	O
families	O
of	O
partitions	O
that	O
is	O
related	O
to	O
the	O
vapnik	O
inequality	B
for	O
families	O
of	O
sets	O
the	O
reader	O
is	O
asked	O
for	O
forgiveness-we	O
want	O
to	O
present	O
a	O
very	O
generally	O
applicable	O
theorem	O
and	O
have	O
to	O
sacrifice	O
by	O
increasing	O
the	O
density	O
of	O
the	O
text	O
however	O
as	O
you	O
will	O
discover	O
the	O
rewards	O
will	O
be	O
sweet	O
a	O
vapnik	O
inequality	B
for	O
partitions	O
this	O
is	O
a	O
technical	O
section	O
we	O
will	O
use	O
its	O
results	O
in	O
the	O
next	O
section	O
to	O
establish	O
a	O
general	O
consistency	B
theorem	O
for	O
histogram	O
rules	O
with	O
data-dependent	B
partitions	O
the	O
main	O
goal	O
of	O
this	O
section	O
is	O
to	O
extend	O
the	O
basic	O
vapnik-chervonenkis	B
inequality	B
to	O
families	O
of	O
partitions	O
from	O
families	O
of	O
sets	O
the	O
line	O
of	O
thought	O
followed	O
here	O
essentially	O
appears	O
in	O
zhao	O
krishnaiah	O
and	O
chen	O
for	O
angular	O
partitions	O
and	O
more	O
generally	O
in	O
lugosi	O
and	O
nobel	O
a	O
substantial	O
simplification	O
in	O
the	O
proof	O
was	O
pointed	O
out	O
to	O
us	O
by	O
andrew	O
barron	O
by	O
a	O
partition	B
of	O
rd	O
we	O
mean	O
a	O
countable	O
collection	O
p	O
of	O
subsets	O
of	O
rd	O
such	O
that	O
ujl	O
a	O
j	O
rd	O
and	O
ai	O
n	O
a	O
j	O
if	O
i	O
j	O
each	O
set	O
a	O
j	O
is	O
called	O
a	O
cell	O
of	O
the	O
partition	B
p	O
a	O
vapnik-chervonenkis	B
inequality	B
for	O
partitions	O
let	O
m	O
be	O
a	O
positive	O
number	O
for	O
each	O
partition	B
p	O
we	O
define	O
pm	O
as	O
the	O
tion	O
ofp	O
to	O
the	O
ball	O
sm	O
that	O
sm	O
c	O
rd	O
denotes	O
the	O
closed	O
ball	O
of	O
radius	O
m	O
centered	O
at	O
the	O
origin	O
in	O
other	O
words	O
pm	O
is	O
a	O
partition	B
of	O
sm	O
whose	O
cells	O
are	O
obtained	O
by	O
intersecting	O
the	O
cells	O
oip	O
with	O
sm	O
we	O
assume	O
throughout	O
that	O
pis	O
such	O
that	O
ipm	O
i	O
for	O
each	O
m	O
we	O
denote	O
by	O
bpm	O
the	O
collection	O
of	O
all	O
sets	O
obtained	O
by	O
unions	O
of	O
cells	O
of	O
pm	O
just	O
as	O
we	O
dealt	O
with	O
classes	O
of	O
sets	O
in	O
chapter	O
here	O
we	O
introduce	O
lies	O
of	O
partitions	O
let	O
f	O
be	O
a	O
infinite	O
collection	O
of	O
partitions	O
of	O
rd	O
fm	O
p	O
e	O
f	O
denotes	O
the	O
family	B
of	I
partitions	O
of	O
sm	O
obtained	O
by	O
stricting	O
members	O
of	O
f	O
to	O
sm	O
for	O
each	O
m	O
we	O
will	O
measure	O
the	O
complexity	B
of	I
a	I
family	B
of	I
partitions	O
fm	O
by	O
the	O
shatter	O
coefficients	O
of	O
the	O
class	O
of	O
sets	O
obtained	O
as	O
unions	O
of	O
cells	O
of	O
partitions	O
taken	O
from	O
the	O
family	O
pm	O
formally	O
we	O
define	O
the	O
combinatorial	O
quantity	O
fl	O
n	O
as	O
follows	O
introduce	O
the	O
class	O
a	O
of	O
subsets	O
ofrd	O
by	O
am	O
e	O
bpm	O
for	O
some	O
pm	O
e	O
pm	O
and	O
define	O
fl	O
npm	O
sam	O
n	O
the	O
shatter	B
coefficient	I
of	O
a	O
a	O
is	O
thus	O
the	O
class	O
of	O
all	O
sets	O
that	O
can	O
be	O
obtained	O
as	O
unions	O
of	O
cells	O
of	O
some	O
partition	B
of	O
s	O
m	O
in	O
the	O
collection	O
pm	O
for	O
example	O
if	O
all	O
members	O
of	O
fm	O
partition	B
sm	O
into	O
two	O
sets	O
then	O
fl	O
npm	O
is	O
just	O
the	O
shatter	B
coefficient	I
of	O
all	O
sets	O
in	O
these	O
partitions	O
and	O
sm	O
included	O
in	O
the	O
collection	O
of	O
sets	O
let	O
jl	O
be	O
a	O
probability	O
measure	O
on	O
rd	O
and	O
let	O
xl	O
x	O
be	O
i	O
i	O
d	O
random	O
vectors	O
in	O
rd	O
with	O
distribution	O
jl	O
for	O
n	O
let	O
jln	O
denote	O
the	O
empirical	B
distribution	O
of	O
xl	O
x	O
n	O
which	O
places	O
mass	O
lin	O
at	O
each	O
of	O
xl	O
x	O
n	O
to	O
establish	O
the	O
consistency	B
of	O
data-driven	O
histogram	O
methods	O
we	O
require	O
information	O
about	O
the	O
large	O
deviations	O
of	O
random	O
variables	O
of	O
the	O
form	O
sup	O
l	O
pmefm	O
aepm	O
where	O
f	O
is	O
an	O
appropriate	O
family	B
of	I
partitions	O
remark	O
just	O
as	O
in	O
chapter	O
the	O
supremum	O
above	O
is	O
not	O
guaranteed	O
to	O
be	O
measurable	O
in	O
order	O
to	O
insure	O
measurability	O
it	O
is	O
necessary	O
to	O
impose	O
regularity	O
conditions	O
on	O
uncountable	O
collections	O
of	O
partitions	O
it	O
suffices	O
to	O
mention	O
that	O
in	O
all	O
our	O
applications	O
the	O
measurability	O
can	O
be	O
verified	O
by	O
checking	O
conditions	O
given	O
e	O
g	O
in	O
dudley	O
or	O
pollard	O
the	O
following	O
theorem	O
is	O
a	O
consequence	O
of	O
the	O
vapnik	O
inequality	B
theorem	O
and	O
nobel	O
let	O
xl	O
xn	O
be	O
i	O
i	O
d	O
random	O
vectors	O
in	O
rd	O
with	O
measure	O
jl	O
and	O
empirical	B
measure	I
jln	O
let	O
f	O
be	O
a	O
collection	O
partitions	O
ard	O
thenor	O
each	O
m	O
and	O
e	O
p	O
sup	O
l	O
ijlna	O
jlai	O
e	O
e-ne	O
pmepm	O
aepm	O
data-dependent	B
partitioning	O
proof	O
for	O
a	O
fixed	O
partition	B
p	O
define	O
a	O
as	O
the	O
set	O
a	O
u	O
a	O
then	O
l	O
iflna	O
aepm	O
fla	O
fla	O
flsm	O
flnsm	O
sup	O
aebpm	O
iflna	O
flsm	O
flnsm	O
recall	O
that	O
the	O
class	O
of	O
sets	O
bpm	O
contains	O
sets	O
obtained	O
by	O
unions	O
of	O
cells	O
of	O
pm	O
therefore	O
sup	O
pm	O
efw	O
aebpmusm	O
sup	O
iflna	O
flsm	O
flnsm	O
observe	O
that	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
inequality	B
is	O
a	O
uniform	O
deviation	O
of	O
the	O
empirical	B
measure	I
fln	O
from	O
fl	O
over	O
a	O
specific	O
class	O
of	O
sets	O
the	O
class	O
contains	O
all	O
sets	O
that	O
can	O
be	O
written	O
as	O
unions	O
of	O
cells	O
of	O
a	O
partition	B
pm	O
in	O
the	O
class	O
of	O
partitions	O
this	O
class	O
of	O
sets	O
is	O
just	O
a	O
defined	O
above	O
the	O
theorem	O
now	O
follows	O
from	O
the	O
vapnik-chervonenkis	B
inequality	B
the	O
definition	B
of	I
and	O
hoeffdings	O
inequality	B
we	O
will	O
use	O
a	O
special	O
application	O
of	O
theorem	O
summarized	O
in	O
the	O
following	O
corollary	O
corollary	O
let	O
yl	O
be	O
a	O
sequence	O
ofi	O
i	O
d	O
random	O
pairs	O
in	O
rd	O
x	O
i	O
and	O
let	O
be	O
a	O
sequence	O
offamilies	O
of	O
partitions	O
of	O
rd	O
if	O
form	O
lim	O
log	O
n-ioo	O
n	O
then	O
sup	O
l	O
ltyjxiea-eyixeaio	O
pmefm	O
aepm	O
n	O
il	O
with	O
probability	O
one	O
as	O
n	O
tends	O
to	O
infinity	O
a	O
vapnik-chervonenkis	B
inequality	B
for	O
partitions	O
proof	O
let	O
v	O
be	O
the	O
measure	O
of	O
y	O
on	O
rd	O
x	O
i	O
and	O
let	O
vn	O
be	O
the	O
empirical	B
measure	I
corresponding	O
to	O
the	O
sequence	O
yd	O
yn	O
using	O
fm	O
we	O
define	O
a	O
family	O
gm	O
of	O
partitions	O
ofrd	O
x	O
i	O
by	O
gm	O
x	O
pme	O
fm	O
u	O
x	O
pm	O
e	O
fm	O
where	O
p	O
x	O
a	O
x	O
x	O
x	O
we	O
apply	O
theorem	O
for	O
these	O
families	O
of	O
partitions	O
def	O
sup	O
l	O
i	O
tyjxi	O
ea	O
sup	O
l	O
ivna	O
x	O
va	O
x	O
sup	O
l	O
ivna	O
vai	O
pcmefjm	O
aepcm	O
pcmefjm	O
aepcm	O
n	O
il	O
pcmeym	O
aepcm	O
it	O
is	O
easy	O
to	O
see	O
that	O
therefore	O
the	O
stated	O
convergence	O
follows	O
from	O
theorem	O
and	O
the	O
borel-cantelli	B
lemma	I
d	O
lemma	O
assume	O
that	O
the	O
family	O
fm	O
is	O
such	O
that	O
the	O
number	O
of	O
cells	O
of	O
the	O
partitions	O
in	O
the	O
family	O
are	O
uniformly	O
bounded	O
that	O
is	O
there	O
is	O
a	O
constant	O
n	O
such	O
that	O
ipmi	O
n	O
for	O
each	O
pm	O
e	O
fm	O
then	O
where	O
is	O
maximal	O
number	O
of	O
different	O
ways	O
n	O
points	O
can	O
be	O
partitioned	O
by	O
members	O
of	O
fm	O
example	O
flexible	O
grids	O
as	O
a	O
first	O
simple	O
example	O
let	O
us	O
take	O
in	O
fn	O
all	O
partitions	O
into	O
d-dimensional	O
grids	O
flexible	O
grids	O
as	O
they	O
may	O
be	O
visualized	O
as	O
wire	O
fences	O
with	O
unequally	O
spaced	O
vertical	O
and	O
horizontal	O
wires	O
in	O
which	O
cells	O
are	O
made	O
up	O
as	O
cartesian	O
products	O
of	O
d	O
intervals	O
and	O
each	O
coordinate	O
axis	O
contributes	O
one	O
of	O
mn	O
intervals	O
to	O
these	O
products	O
clearly	O
if	O
p	O
is	O
a	O
member	O
partition	B
of	O
fn	O
then	O
ipi	O
m	O
nevertheless	O
there	O
are	O
uncountably	O
many	O
ps	O
as	O
there	O
are	O
uncountably	O
many	O
intervals	O
of	O
the	O
real	O
line	O
this	O
is	O
why	O
the	O
finite	O
quantity	O
comes	O
in	O
so	O
handy	O
we	O
will	O
verify	O
later	O
that	O
for	O
each	O
m	O
s	O
d	O
so	O
that	O
the	O
condition	O
of	O
corollary	O
is	O
fulfilled	O
when	O
m	O
d	O
lim	O
o	O
n-oo	O
n	O
data-dependent	B
partitioning	O
figure	O
afiexible	O
grid	O
partition	B
consistency	B
in	O
this	O
section	O
we	O
establish	O
a	O
general	O
consistency	B
theorem	O
for	O
a	O
large	O
class	O
of	O
data-based	B
partitioning	O
rules	O
using	O
the	O
training	O
data	O
dn	O
we	O
produce	O
a	O
partition	B
pn	O
jrnx	O
r	O
yr	O
x	O
n	O
yn	O
according	O
to	O
a	O
prescribed	O
rule	B
jrn	O
we	O
then	O
use	O
the	O
partition	B
p	O
n	O
in	O
conjunction	O
with	O
dn	O
to	O
produce	O
a	O
classification	O
rule	B
based	O
on	O
a	O
majority	B
vote	I
within	O
the	O
cells	O
of	O
the	O
partition	B
that	O
is	O
the	O
training	O
set	O
is	O
used	O
twice	O
and	O
it	O
is	O
this	O
feature	O
of	O
data-dependent	B
histogram	O
methods	O
that	O
distinguishes	O
them	O
from	O
regular	B
histogram	O
methods	O
formally	O
an	O
n-sample	O
partitioning	O
rule	B
for	O
nd	O
is	O
a	O
function	O
jrn	O
that	O
associates	O
every	O
n-tuple	O
of	O
pairs	O
yi	O
yn	O
e	O
n	O
d	O
x	O
i	O
with	O
a	O
measurable	O
partition	B
of	O
nd	O
associated	O
with	O
every	O
partitioning	O
rule	B
jrn	O
there	O
is	O
a	O
fixed	O
random	O
family	B
of	I
partitions	O
yi	O
xn	O
yn	O
yi	O
yn	O
e	O
nd	O
x	O
i	O
is	O
the	O
family	B
of	I
partitions	O
produced	O
by	O
the	O
partitioning	O
rule	B
jrn	O
for	O
all	O
possible	O
realizations	O
of	O
the	O
training	O
sequence	O
dn	O
when	O
a	O
partitioning	O
rule	B
jrn	O
is	O
applied	O
to	O
the	O
sequence	O
dn	O
yr	O
yn	O
it	O
produces	O
a	O
random	O
partition	B
pn	O
jrncdn	O
e	O
in	O
what	O
follows	O
we	O
suppress	O
the	O
dependence	O
of	O
pn	O
on	O
dn	O
for	O
notational	O
simplicity	O
for	O
every	O
x	O
e	O
nd	O
let	O
anx	O
be	O
the	O
unique	O
cell	O
of	O
pn	O
that	O
contains	O
the	O
point	O
x	O
now	O
let	O
be	O
a	O
fixed	O
sequence	O
of	B
partitioning	I
rules	I
the	O
classification	O
rule	B
gn	O
gnc	O
dn	O
is	O
defined	O
by	O
taking	O
a	O
majority	B
vote	I
among	O
the	O
classes	O
appearing	O
in	O
a	O
given	O
cell	O
of	O
pn	O
that	O
is	O
we	O
emphasize	O
here	O
that	O
the	O
partition	B
pn	O
can	O
depend	O
on	O
the	O
vectors	O
xi	O
and	O
the	O
labels	O
yi	O
first	O
we	O
establish	O
the	O
strong	B
consistency	B
of	O
the	O
rules	O
for	O
a	O
wide	O
class	O
of	B
partitioning	I
rules	I
as	O
always	O
diama	O
denotes	O
the	O
diameter	O
of	O
a	O
set	O
a	O
that	O
is	O
the	O
maximum	O
euclidean	B
distance	O
between	O
any	O
two	O
points	O
of	O
a	O
diama	O
sup	O
yi	O
xyea	O
consistency	B
theorem	O
and	O
nobel	O
let	O
be	O
afixed	O
sequence	O
of	B
partitioning	I
rules	I
and	O
for	O
each	O
n	O
let	O
fn	O
be	O
the	O
collection	O
of	O
partitions	O
associated	O
with	O
the	O
n-sample	O
partitioning	O
rule	B
if	O
for	O
each	O
m	O
hm	O
n--oo	O
log	O
n	O
and	O
for	O
all	O
balls	O
sb	O
and	O
all	O
y	O
lim	O
jj	O
-	O
diamanx	O
n	O
sm	O
y	O
n--oo	O
with	O
probability	O
one	O
then	O
the	O
classification	O
rule	B
corresponding	O
to	O
satisfies	O
with	O
probability	O
one	O
in	O
other	O
words	O
the	O
rule	B
is	O
strongly	O
consistent	O
remark	O
in	O
some	O
applications	O
we	O
need	O
a	O
weaker	O
condition	O
to	O
replace	O
in	O
the	O
theorem	O
the	O
following	O
condition	O
will	O
do	O
for	O
every	O
y	O
and	O
e	O
jj	O
-x	O
diamailx	O
n	O
t	O
y	O
with	O
probability	O
one	O
lim	O
n-oo	O
tcrdjltl-o	O
inf	O
the	O
verification	O
of	O
this	O
is	O
left	O
to	O
the	O
reader	O
the	O
proof	O
given	O
below	O
requires	O
quite	O
some	O
effort	O
the	O
utility	O
of	O
the	O
theorem	O
is	O
not	O
immediately	O
apparent	O
the	O
length	O
of	O
the	O
proof	O
is	O
indicative	O
of	O
the	O
generality	O
of	O
the	O
conditions	O
in	O
the	O
theorem	O
given	O
a	O
data-dependent	B
partitioning	O
rule	B
we	O
must	O
verify	O
two	O
things	O
condition	O
merely	O
relates	O
to	O
the	O
richness	O
of	O
the	O
class	O
of	O
partitions	O
of	O
rd	O
that	O
may	O
possibly	O
occur	O
such	O
as	O
flexible	O
grids	O
condition	O
tells	O
us	O
that	O
the	O
rule	B
should	O
eventually	O
make	O
local	O
decisions	O
from	O
examples	O
in	O
earlier	O
chapters	O
it	O
should	O
be	O
obvious	O
that	O
is	O
not	O
necessary	O
finite	O
partitions	O
of	O
rd	O
necessarily	O
have	O
component	O
sets	O
with	O
infinite	O
diameter	O
hence	O
we	O
need	O
a	O
condition	O
that	O
states	O
that	O
such	O
sets	O
have	O
small	O
jj	O
--measure	O
condition	O
requires	O
that	O
a	O
randomly	O
chosen	O
cell	O
have	O
a	O
small	O
diameter	O
thus	O
it	O
may	O
be	O
viewed	O
as	O
the	O
version	O
of	O
condition	O
of	O
theorem	O
however	O
the	O
weaker	O
version	O
of	O
condition	O
stated	O
in	O
the	O
above	O
remark	O
is	O
more	O
subtle	O
by	O
considering	O
examples	O
in	O
which	O
jj	O
-	O
has	O
bounded	O
support	B
more	O
than	O
just	O
balls	O
s	O
m	O
are	O
needed	O
as	O
the	O
sets	O
of	O
the	O
partition	B
near	O
the	O
boundary	O
of	O
the	O
support	B
may	O
all	O
have	O
infinite	O
diameter	O
as	O
well	O
hence	O
we	O
introduced	O
an	O
infimum	O
with	O
respect	O
to	O
sets	O
t	O
over	O
all	O
t	O
with	O
jj	O
-t	O
o	O
it	O
suffices	O
to	O
mention	O
that	O
is	O
satisfied	O
for	O
all	O
distributions	O
in	O
some	O
of	O
the	O
examples	O
that	O
follow	O
theorem	O
then	O
allows	O
us	O
to	O
conclude	O
that	O
such	O
rules	O
are	O
strongly	O
universally	O
consistent	O
the	O
theorem	O
has	O
done	O
most	O
of	O
the	O
digestion	O
of	O
data-dependent	B
partitioning	O
such	O
proofs	O
so	O
we	O
are	O
left	O
with	O
virtually	O
no	O
work	O
at	O
all	O
consistency	B
results	O
win	O
follow	O
like	O
dominos	O
falling	O
proof	O
of	O
theorem	O
observe	O
that	O
the	O
partitioning	O
classifier	B
gn	O
can	O
be	O
rewritten	O
in	O
the	O
form	O
if	O
ll	O
iyilixeanx	O
ll	O
iyoixiea	O
lx	O
flanx	O
otherwise	O
flanx	O
introduce	O
the	O
notation	O
x	O
yi	O
l	O
-ll	O
i	O
nmanx	O
for	O
any	O
e	O
there	O
is	O
an	O
m	O
e	O
such	O
that	O
px	O
tf	O
sm	O
e	O
thus	O
by	O
an	O
application	O
of	O
theorem	O
we	O
see	O
that	O
lgn	O
l	O
pgnx	O
y	O
x	O
e	O
smidn	O
pgx	O
y	O
x	O
e	O
sm	O
e	O
jsm	O
jsm	O
e	O
where	O
ydixeanx	O
n	O
nmanx	O
by	O
symmetry	O
since	O
e	O
is	O
arbitrary	O
it	O
suffices	O
to	O
show	O
that	O
for	O
each	O
m	O
with	O
probability	O
one	O
js	O
m	O
fix	O
e	O
and	O
let	O
r	O
rd	O
r	O
be	O
a	O
continuous	O
function	O
with	O
bounded	O
support	B
such	O
that	O
ism	O
rximdx	O
e	O
such	O
r	O
exists	O
by	O
theorem	O
now	O
define	O
the	O
auxiliary	O
functions	O
and	O
note	O
that	O
both	O
ryn	O
and	O
fn	O
are	O
piecewise	O
constant	O
on	O
the	O
cells	O
of	O
the	O
random	O
partition	B
pn	O
we	O
may	O
decompose	O
the	O
error	O
as	O
follows	O
i	O
consistency	B
the	O
integral	O
of	O
the	O
first	O
term	O
on	O
the	O
right	O
side	O
is	O
smaller	O
than	O
e	O
by	O
the	O
definition	B
of	I
r	O
for	O
the	O
third	O
term	O
we	O
have	O
r	O
ifnx	O
l	O
ie	O
y	O
ixea	O
i	O
dn	O
e	O
rxixea	O
i	O
dn	O
i	O
a	O
epm	O
r	O
rxipdx	O
e	O
consider	O
the	O
fourth	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
clearly	O
it	O
follows	O
from	O
the	O
first	O
condition	O
of	O
the	O
theorem	O
and	O
corollary	O
of	O
theorem	O
that	O
r	O
with	O
probability	O
one	O
finally	O
we	O
consider	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
using	O
fubinis	O
theorem	O
we	O
have	O
data-dependent	B
partitioning	O
fix	O
e	O
as	O
r	O
is	O
uniformly	O
continuous	O
there	O
exists	O
a	O
number	O
y	O
osuch	O
that	O
if	O
diama	O
y	O
then	O
irx	O
for	O
every	O
x	O
yea	O
in	O
addition	O
there	O
is	O
a	O
constant	O
m	O
such	O
that	O
i	O
rx	O
i	O
m	O
for	O
every	O
x	O
e	O
rd	O
fix	O
n	O
now	O
and	O
consider	O
the	O
integrals	O
fla	O
ja	O
ja	O
ryimdxmdy	O
appearing	O
in	O
the	O
sum	O
above	O
we	O
always	O
have	O
the	O
upper	O
bound	O
majaja	O
ryimdxmdy	O
assume	O
now	O
that	O
a	O
e	O
pm	O
has	O
diama	O
y	O
then	O
we	O
can	O
write	O
summing	O
over	O
the	O
cells	O
a	O
e	O
pim	O
with	O
ma	O
these	O
bounds	O
give	O
irx	O
jsm	O
rnximdx	O
l	O
diamanx	O
y	O
letting	O
n	O
tend	O
to	O
infinity	O
gives	O
lim	O
sup	O
n--oo	O
rnximdx	O
with	O
probability	O
one	O
by	O
the	O
second	O
condition	O
of	O
the	O
theorem	O
summarizing	O
lim	O
sup	O
with	O
probability	O
one	O
j	O
sm	O
since	O
e	O
and	O
are	O
arbitrary	O
the	O
theorem	O
is	O
proved	O
statistically	B
equivalent	I
blocks	I
in	O
this	O
section	O
we	O
apply	O
theorem	O
both	O
to	O
classifiers	O
based	O
on	O
uniform	O
spacings	O
in	O
one	O
dimension	B
and	O
to	O
their	O
extension	O
to	O
multidimensional	O
problems	O
we	O
refer	O
to	O
these	O
as	O
rules	O
based	O
on	O
statistically	B
equivalent	I
blocks	I
the	O
order	B
statistics	I
of	O
the	O
components	O
of	O
the	O
training	O
data	O
are	O
used	O
to	O
construct	O
a	O
partition	B
into	O
rectangles	O
all	O
statistically	B
equivalent	I
blocks	I
such	O
classifiers	O
are	O
invariant	O
with	O
respect	O
to	O
all	O
strictly	O
monotone	O
transformations	O
of	O
the	O
coordinate	O
axes	O
the	O
simplest	O
such	O
rule	B
is	O
the	O
k-spacing	B
rule	B
studied	O
in	O
chapter	O
theorem	O
generalizations	O
are	O
possible	O
in	O
several	O
ways	O
theorem	O
allows	O
us	O
to	O
handle	O
partitions	O
depending	O
on	O
the	O
whole	O
data	O
sequence-and	O
not	O
only	O
on	O
the	O
xis	O
the	O
next	O
simple	O
result	O
is	O
sometimes	O
useful	O
theorem	O
consider	O
a	O
data-dependent	B
partitioning	O
classifier	B
on	O
the	O
real	O
line	O
that	O
partitions	O
r	O
into	O
intervals	O
in	O
such	O
a	O
way	O
that	O
each	O
interval	O
contains	O
at	O
least	O
an	O
and	O
at	O
most	O
bn	O
points	O
assume	O
that	O
x	O
has	O
a	O
nonatomic	O
distribution	O
then	O
the	O
classifier	B
is	O
strongly	O
consistent	O
whenever	O
an	O
and	O
bnln	O
as	O
n	O
proof	O
we	O
check	O
the	O
conditions	O
of	O
theorem	O
let	O
contain	O
all	O
partitions	O
of	O
n	O
into	O
m	O
r	O
n	O
i	O
an	O
l	O
intervals	O
since	O
for	O
each	O
m	O
all	O
partitions	O
in	O
fm	O
have	O
at	O
most	O
m	O
cells	O
we	O
can	O
bound	O
according	O
to	O
the	O
lemma	O
by	O
the	O
lemma	O
d	O
nfm	O
does	O
not	O
exceed	O
times	O
the	O
number	O
of	O
different	O
ways	O
n	O
points	O
can	O
be	O
partitioned	O
into	O
m	O
intervals	O
a	O
little	O
thought	O
confirms	O
that	O
this	O
number	O
is	O
and	O
therefore	O
leth	O
denote	O
the	O
binary	B
entropy	B
function	O
hx	O
logx	O
x	O
loge	O
x	O
for	O
x	O
e	O
note	O
that	O
h	O
is	O
symmetric	O
about	O
and	O
that	O
h	O
is	O
increasing	O
for	O
x	O
by	O
the	O
inequality	B
of	O
theorem	O
log	O
g	O
shtls	O
therefore	O
it	O
is	O
easy	O
to	O
see	O
that	O
mnmh	O
nm	O
nlan	O
as	O
when	O
x	O
the	O
condition	O
an	O
implies	O
that	O
which	O
establishes	O
condition	O
to	O
establish	O
condition	O
of	O
theorem	O
we	O
proceed	O
similarly	O
to	O
the	O
proof	O
of	O
theorem	O
fix	O
y	O
e	O
o	O
there	O
exists	O
an	O
interval	O
m	O
such	O
that	O
jl	O
m	O
my	O
e	O
and	O
consequently	O
jlx	O
diamanx	O
y	O
e	O
flx	O
diamanx	O
y	O
n	O
md	O
where	O
anx	O
denotes	O
the	O
cell	O
of	O
the	O
partition	B
p	O
n	O
containing	O
x	O
among	O
the	O
intervals	O
of	O
pn	O
there	O
can	O
be	O
at	O
most	O
i	O
y	O
disjoint	O
intervals	O
of	O
length	O
greater	O
than	O
y	O
in	O
data-dependent	B
partitioning	O
mj	O
thus	O
we	O
may	O
bound	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
above	O
by	O
flx	O
diamanx	O
y	O
n	O
m	O
aepn	O
max	O
fla	O
flna	O
max	O
ifla	O
sup	O
ifla	O
y	O
y	O
y	O
flnai	O
n	O
aea	O
flnai	O
where	O
a	O
is	O
the	O
set	O
of	O
all	O
intervals	O
in	O
r	O
the	O
first	O
term	O
in	O
the	O
parenthesis	O
converges	O
to	O
zero	O
by	O
the	O
second	O
condition	O
of	O
the	O
theorem	O
while	O
the	O
second	O
term	O
goes	O
to	O
zero	O
with	O
probability	O
one	O
by	O
an	O
obvious	O
extension	O
of	O
the	O
classical	O
glivenko-cantelli	B
theorem	I
summarizing	O
we	O
have	O
shown	O
that	O
for	O
any	O
y	O
e	O
lim	O
sup	O
fl	O
diam	O
an	O
y	O
e	O
with	O
probability	O
one	O
n---cxj	O
this	O
completes	O
the	O
proof	O
the	O
d-dimensional	O
generalizations	O
of	O
k-spacing	O
rules	O
include	O
rules	O
based	O
upon	O
statistically	B
equivalent	I
blocks	I
that	O
is	O
partitions	O
with	O
sets	O
that	O
contain	O
k	O
points	O
each	O
it	O
is	O
obvious	O
that	O
one	O
can	O
proceed	O
in	O
many	O
ways	O
see	O
for	O
example	O
anderson	O
patrick	O
patrick	O
and	O
fisher	O
quesenberry	O
and	O
gessaman	O
and	O
gessaman	O
and	O
gessaman	O
as	O
a	O
first	O
example	O
consider	O
the	O
following	O
algorithm	B
the	O
k-th	O
smallest	O
xlt	O
coordinate	O
among	O
the	O
training	O
data	O
defines	O
the	O
first	O
cut	O
the	O
rectangle	O
with	O
n	O
k	O
points	O
is	O
cut	O
according	O
to	O
the	O
isolating	O
another	O
k	O
points	O
this	O
can	O
be	O
repeated	O
on	O
a	O
rotational	O
basis	O
for	O
all	O
coordinate	O
axes	O
unfortunately	O
the	O
classifier	B
obtained	O
this	O
way	O
is	O
not	O
consistent	O
to	O
see	O
this	O
observe	O
that	O
if	O
k	O
is	O
much	O
smaller	O
than	O
n-a	O
clearly	O
necessary	O
requirement	O
for	O
consistency-then	O
almost	O
all	O
cells	O
produced	O
by	O
the	O
cuts	O
are	O
long	O
and	O
thin	O
we	O
sketched	O
a	O
distribution	O
in	O
figure	O
for	O
which	O
the	O
error	O
probability	O
of	O
this	O
classifier	B
fails	O
to	O
converge	O
to	O
l	O
the	O
details	O
are	O
left	O
to	O
the	O
reader	O
this	O
example	O
highlights	O
the	O
importance	O
of	O
condition	O
of	O
theorem	O
that	O
is	O
that	O
the	O
diameters	O
of	O
the	O
cells	O
should	O
shrink	O
in	O
some	O
sense	O
as	O
n	O
statistically	B
equivalent	I
blocks	I
figure	O
a	O
nonconsistent	O
k-block	O
rithm	O
k	O
in	O
the	O
picture	O
in	O
the	O
shaded	O
area	O
and	O
in	O
the	O
white	O
area	O
o	O
o	O
o	O
rules	O
have	O
been	O
developed	O
in	O
which	O
the	O
rectangular	O
partition	B
depends	O
not	O
only	O
upon	O
the	O
xis	O
in	O
the	O
training	O
sequence	O
but	O
also	O
upon	O
the	O
yis	O
e	O
g	O
henrichon	O
and	O
fu	O
meisel	O
and	O
michalopoulos	O
and	O
friedman	O
for	O
ample	O
friedman	O
cuts	O
the	O
axes	O
at	O
the	O
places	O
where	O
the	O
absolute	O
differences	O
between	O
the	O
marginal	O
empirical	B
distribution	O
functions	O
are	O
largest	O
to	O
insure	O
minimal	O
ical	O
error	O
after	O
the	O
cut	O
his	O
procedure	O
is	O
based	O
upon	O
stoller	O
splits	O
chapters	O
and	O
rules	O
depending	O
on	O
the	O
coordinatewise	O
ranks	O
of	O
data	O
points	O
are	O
interesting	O
cause	O
they	O
are	O
invariant	O
under	O
monotone	O
transformations	O
of	O
the	O
coordinate	O
axes	O
this	O
is	O
particularly	O
important	O
in	O
practice	O
when	O
the	O
components	O
are	O
not	O
physically	O
comparable	O
is	O
an	O
adjective	O
often	O
used	O
to	O
point	O
out	O
a	O
property	O
that	O
is	O
universally	O
valid	O
for	O
such	O
methods	O
in	O
statistics	O
see	O
the	O
survey	O
of	O
das	O
gupta	O
statistically	O
equivalent	O
sets	O
in	O
partitions	O
are	O
called	O
free	O
because	O
the	O
measure	O
ma	O
of	O
a	O
set	O
in	O
the	O
partition	B
does	O
not	O
depend	O
upon	O
the	O
distribution	O
of	O
x	O
we	O
already	O
noted	O
a	O
similar	O
distribution-free	O
behavior	O
for	O
k	O
trees	O
and	O
median	O
trees	O
there	O
is	O
no	O
reason	O
to	O
stay	O
with	O
rectangular-shaped	O
sets	O
and	O
benning	O
beakley	O
and	O
tuteur	O
but	O
doing	O
so	O
greatly	O
simplifies	O
the	O
interpretation	O
of	O
a	O
classifier	B
in	O
this	O
book	O
to	O
avoid	O
confusion	O
we	O
reserve	O
the	O
term	O
for	O
consistency	B
results	O
or	O
other	O
theoretical	B
properties	O
that	O
hold	O
for	O
all	O
distributions	O
of	O
y	O
it	O
is	O
possible	O
to	O
define	O
consistent	O
partitions	O
that	O
have	O
statistically	O
equivalent	O
sets	O
to	O
fix	O
the	O
ideas	O
we	O
take	O
gessamans	O
rule	B
as	O
our	O
prototype	B
rule	B
for	O
further	O
study	O
for	O
hypothesis	O
testing	O
this	O
partition	B
was	O
already	O
noted	O
by	O
anderson	O
for	O
each	O
n	O
let	O
m	O
injknldl	O
project	O
the	O
vectors	O
xl	O
xn	O
onto	O
the	O
first	O
coordinate	O
axis	O
and	O
then	O
partition	B
the	O
data	O
into	O
m	O
sets	O
using	O
hyperplanes	O
figure	O
gessamans	O
partition	B
with	O
m	O
data-dependent	B
partitioning	O
o	O
o	O
o	O
o	O
l------	O
po	O
perpendicular	O
to	O
that	O
axis	O
in	O
such	O
a	O
way	O
that	O
each	O
set	O
contains	O
an	O
equal	O
number	O
of	O
points	O
possibly	O
the	O
rightmost	O
set	O
where	O
fewer	O
points	O
may	O
fall	O
if	O
n	O
is	O
not	O
a	O
multiple	O
of	O
m	O
we	O
obtain	O
m	O
cylindrical	O
sets	O
in	O
the	O
same	O
fashion	O
cut	O
each	O
of	O
these	O
cylindrical	O
sets	O
along	O
the	O
second	O
axis	O
into	O
m	O
boxes	O
such	O
that	O
each	O
box	O
contains	O
the	O
same	O
number	O
of	O
data	O
points	O
continuing	O
in	O
the	O
same	O
way	O
along	O
the	O
remaining	O
coordinate	O
axes	O
we	O
obtain	O
m	O
d	O
rectangular	O
cells	O
each	O
of	O
which	O
the	O
exception	O
ofthose	O
on	O
the	O
boundary	O
contains	O
about	O
kn	O
points	O
the	O
classification	O
rule	B
gn	O
uses	O
a	O
majority	B
vote	I
among	O
those	O
yi	O
for	O
which	O
xi	O
lies	O
within	O
a	O
given	O
cell	O
consistency	B
of	O
this	O
classification	O
rule	B
can	O
be	O
established	O
by	O
an	O
argument	O
similar	O
to	O
that	O
used	O
for	O
the	O
kn	O
rule	B
above	O
one	O
needs	O
to	O
check	O
that	O
the	O
conditions	O
of	O
theorem	O
are	O
satisfied	O
the	O
only	O
minor	O
difference	O
appears	O
in	O
the	O
computation	O
of	O
lln	O
which	O
in	O
this	O
case	O
is	O
bounded	O
from	O
above	O
by	O
the	O
following	O
theorem	O
summarizes	O
the	O
result	O
theorem	O
assume	O
that	O
the	O
marginal	O
distributions	O
of	O
x	O
in	O
nd	O
are	O
nonatomic	O
then	O
the	O
partitioning	O
classification	O
rule	B
based	O
on	O
gessamans	O
rule	B
is	O
strongly	O
consistent	O
if	O
kn	O
and	O
kn	O
n	O
as	O
n	O
tends	O
to	O
infinity	O
to	O
consider	O
distributions	O
with	O
possibly	O
atomic	O
marginals	O
the	O
partitioning	O
rithm	O
must	O
be	O
modified	O
since	O
every	O
atom	O
has	O
more	O
than	O
kn	O
points	O
falling	O
on	O
it	O
for	O
large	O
n	O
with	O
a	O
proper	O
modification	O
a	O
strongly	O
universally	O
consistent	O
rule	B
can	O
be	O
obtained	O
we	O
leave	O
the	O
details	O
to	O
the	O
reader	O
remark	O
consistency	B
of	O
gessamans	O
classification	O
scheme	O
can	O
also	O
be	O
derived	O
from	O
the	O
results	O
of	O
gordon	O
and	O
olshen	O
under	O
the	O
additional	O
condition	O
kn	O
fo	O
results	O
of	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
can	O
be	O
partitioning	O
rules	O
based	O
on	O
clustering	B
used	O
to	O
improve	O
this	O
condition	O
to	O
kn	O
log	O
n	O
theorem	O
guarantees	O
sistency	O
under	O
the	O
weakest	O
possible	O
condition	O
kn	O
partitioning	O
rules	O
based	O
on	O
clustering	B
clustering	B
is	O
one	O
of	O
the	O
most	O
widely	O
used	O
methods	O
in	O
statistical	O
data	O
analysis	O
typical	O
clustering	B
schemes	O
divide	O
the	O
data	O
into	O
a	O
finite	O
number	O
of	O
disjoint	O
groups	O
by	O
minimizing	O
some	O
empirical	B
error	I
measure	O
such	O
as	O
the	O
average	O
squared	O
distance	O
from	O
cluster	O
centers	O
hartigan	O
in	O
this	O
section	O
we	O
outline	O
the	O
application	O
of	O
our	O
results	O
to	O
classification	O
rules	O
based	O
on	O
k-means	B
clustering	B
of	O
unlabeled	O
observations	O
as	O
a	O
first	O
step	O
we	O
divide	O
xl	O
xn	O
into	O
kn	O
disjoint	O
groups	O
having	O
cluster	O
centers	O
ai	O
akn	O
e	O
rd	O
the	O
vectors	O
ai	O
akn	O
are	O
chosen	O
to	O
minimize	O
the	O
empirical	B
squared	O
euclidean	B
distance	O
error	O
over	O
all	O
the	O
nearest-neighbor	O
clustering	B
rules	O
having	O
kn	O
representatives	O
bi	O
e	O
rd	O
where	O
ii	O
ii	O
denotes	O
the	O
usual	O
euclidean	B
norm	I
note	O
that	O
the	O
choice	O
of	O
bkn	O
cluster	O
centers	O
depends	O
only	O
on	O
the	O
vectors	O
xi	O
not	O
on	O
their	O
labels	O
for	O
the	O
behavior	O
of	O
enai	O
akj	O
see	O
problem	O
the	O
vectors	O
ai	O
akll	O
give	O
rise	O
to	O
a	O
voronoi	B
partition	B
p	O
n	O
akn	O
in	O
a	O
natural	O
way	O
for	O
each	O
j	O
e	O
kn	O
let	O
ties	O
are	O
broken	O
by	O
assigning	O
points	O
on	O
the	O
boundaries	O
to	O
the	O
vector	O
that	O
has	O
the	O
smallest	O
index	O
the	O
classification	O
rule	B
gn	O
is	O
defined	O
in	O
the	O
usual	O
way	O
is	O
a	O
majority	B
vote	I
among	O
those	O
yjs	O
such	O
that	O
x	O
j	O
falls	O
in	O
an	O
if	O
the	O
measure	O
fl	O
of	O
x	O
has	O
a	O
bounded	O
support	B
theorem	O
shows	O
that	O
the	O
classification	O
rule	B
is	O
strongly	O
consistent	O
if	O
kn	O
grows	O
with	O
n	O
at	O
an	O
appropriate	O
rate	O
note	O
that	O
this	O
rule	B
is	O
just	O
another	O
of	O
the	O
prototype	B
nearest	B
neighbor	I
rules	O
that	O
we	O
discussed	O
in	O
chapter	O
data-dependent	B
partitioning	O
figure	O
example	O
of	O
partition	B
ing	O
based	O
on	O
clustering	B
with	O
kn	O
the	O
criterion	O
we	O
minimize	O
is	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
distances	O
of	O
the	O
xs	O
to	O
the	O
aso	O
theorem	O
and	O
nobel	O
assume	O
that	O
there	O
is	O
a	O
bounded	O
set	O
a	O
c	O
rd	O
such	O
that	O
pix	O
e	O
a	O
let	O
be	O
a	O
sequence	O
of	O
integers	O
for	O
which	O
kn	O
and	O
as	O
n	O
k	O
logn	O
n	O
let	O
gn	O
dn	O
be	O
the	O
histogram	O
classification	O
rule	B
based	O
on	O
the	O
voronoi	B
partition	B
of	O
kn	O
cluster	O
centers	O
minimizing	O
the	O
empirical	B
squared	O
euclidean	B
distance	O
error	O
then	O
with	O
probability	O
one	O
as	O
n	O
tends	O
to	O
infinity	O
if	O
d	O
or	O
then	O
the	O
second	O
condition	O
on	O
kn	O
can	O
be	O
relaxed	O
to	O
kn	O
logn	O
n	O
proof	O
again	O
we	O
check	O
the	O
conditions	O
of	O
theorem	O
let	O
fn	O
consist	O
of	O
au	O
voronoi	O
partitions	O
of	O
kn	O
points	O
in	O
rd	O
as	O
each	O
partition	B
consists	O
of	O
kn	O
cells	O
we	O
may	O
use	O
lemma	O
to	O
bound	O
clearly	O
boundaries	O
between	O
cells	O
are	O
subsets	O
of	O
hyperplanes	O
since	O
there	O
are	O
at	O
most	O
knckn	O
boundaries	O
between	O
the	O
kn	O
voronoi	O
cells	O
each	O
cell	O
of	O
a	O
partition	B
in	O
fn	O
is	O
a	O
polytope	O
with	O
at	O
most	O
knckn	O
k	O
faces	O
by	O
theorem	O
n	O
fixed	O
points	O
in	O
r	O
d	O
d	O
can	O
be	O
split	O
by	O
hyperplanes	O
in	O
at	O
most	O
different	O
ways	O
it	O
follows	O
that	O
for	O
each	O
partitioning	O
rules	O
based	O
on	O
clustering	B
m	O
fj	O
nfm	O
and	O
consequently	O
fj	O
nfn	O
n	O
kn	O
n	O
lk	O
log	O
n	O
n	O
by	O
the	O
second	O
condition	O
on	O
the	O
sequence	O
thus	O
condition	O
of	O
theorem	O
is	O
satisfied	O
it	O
remains	O
to	O
establish	O
condition	O
of	O
theorem	O
this	O
time	O
we	O
need	O
the	O
weaker	O
condition	O
mentioned	O
in	O
the	O
remark	O
after	O
the	O
theorem	O
that	O
is	O
that	O
for	O
every	O
y	O
and	O
e	O
inf	O
p	O
diam	O
an	O
n	O
t	O
y	O
with	O
probability	O
one	O
clearly	O
we	O
are	O
done	O
if	O
we	O
can	O
prove	O
that	O
there	O
is	O
a	O
sequence	O
of	O
subsets	O
tn	O
of	O
nd	O
depending	O
on	O
the	O
data	O
dn	O
such	O
that	O
fltn	O
with	O
probability	O
one	O
and	O
flx	O
diamanx	O
n	O
tn	O
y	O
o	O
to	O
this	O
end	O
let	O
a	O
i	O
akn	O
denote	O
the	O
optimal	O
cluster	O
centers	O
corresponding	O
to	O
d	O
n	O
and	O
define	O
kn	O
u	O
n	O
a	O
j	O
jl	O
implies	O
where	O
sxr	O
is	O
the	O
ball	O
of	O
radius	O
r	O
around	O
the	O
vector	O
x	O
clearly	O
x	O
e	O
that	O
ilx	O
ax	O
ii	O
y	O
where	O
ax	O
denotes	O
the	O
closest	O
cluster	O
center	O
to	O
x	O
among	O
al	O
akn	O
but	O
since	O
it	O
follows	O
that	O
and	O
fl	O
diamanx	O
n	O
tn	O
y	O
o	O
it	O
remains	O
to	O
show	O
that	O
fltn	O
with	O
probability	O
one	O
as	O
n	O
using	O
markovs	O
inequality	B
we	O
may	O
write	O
e	O
iix	O
dn	O
using	O
a	O
large-deviation	O
inequality	B
for	O
the	O
empirical	B
squared	B
error	I
of	O
neighbor	O
clustering	B
schemes	O
it	O
can	O
be	O
shown	O
problem	O
that	O
if	O
x	O
has	O
bounded	O
support	B
then	O
e	O
iix	O
ajfl	O
dn	O
ijkn	O
min	O
erd	O
e	O
iix	O
ljkn	O
data-dependent	B
partitioning	O
with	O
probability	O
one	O
if	O
log	O
n	O
n	O
as	O
n	O
moreover	O
it	O
is	O
easy	O
to	O
see	O
that	O
ber	O
e	O
iix	O
bj	O
as	O
kn	O
it	O
follows	O
that	O
fltn	O
as	O
desired	O
if	O
d	O
then	O
the	O
cells	O
of	O
the	O
voronoi	B
partition	B
are	O
intervals	O
on	O
the	O
real	O
line	O
and	O
therefore	O
nkn	O
similarly	O
if	O
d	O
then	O
the	O
number	O
of	O
hyperplanes	O
defining	O
the	O
voronoi	B
partition	B
increases	O
linearly	O
with	O
kn	O
to	O
see	O
this	O
observe	O
that	O
if	O
we	O
connect	O
centers	O
of	O
neighboring	O
clusters	O
by	O
edges	O
then	O
we	O
obtain	O
a	O
planar	B
graph	I
from	O
eulers	O
theorem	O
e	O
g	O
edelsbrunner	O
the	O
number	O
of	O
edges	O
in	O
a	O
planar	B
graph	I
is	O
bounded	O
by	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
vertices	O
thus	O
in	O
order	O
to	O
satisfy	O
condition	O
it	O
suffices	O
that	O
kn	O
log	O
n	O
n	O
in	O
both	O
cases	O
remark	O
in	O
the	O
theorem	O
above	O
we	O
assume	O
that	O
the	O
cluster	O
centers	O
a	O
are	O
empirically	O
optimal	O
in	O
the	O
sense	O
that	O
they	O
are	O
chosen	O
to	O
minimize	O
the	O
empirical	B
squared	O
euclidean	B
distance	O
error	O
practically	O
speaking	O
it	O
is	O
hard	O
to	O
determine	O
minimum	O
to	O
get	O
around	O
this	O
difficulty	O
several	O
fast	O
algorithms	O
have	O
been	O
proposed	O
that	O
approximate	O
the	O
optimum	O
hartigan	O
for	O
a	O
survey	O
perhaps	O
the	O
most	O
popular	O
algorithm	B
is	O
the	O
so-called	O
k-means	B
clustering	B
method	O
also	O
known	O
in	O
the	O
theory	O
of	O
quantization	B
as	O
the	O
max	O
algorithm	B
max	O
linde	O
buzo	O
and	O
gray	O
the	O
iterative	O
method	O
works	O
as	O
follows	O
step	O
s	O
tep	O
x	O
x	O
t	O
k	O
k	O
llutla	O
centers	O
a	O
ak	O
e	O
a	O
e	O
cl	O
h	O
d	O
uster	O
t	O
e	O
ata	O
pomts	O
ak	O
into	O
k	O
sets	O
such	O
that	O
the	O
m-th	O
set	O
ci	O
contains	O
the	O
x	O
js	O
that	O
are	O
closer	O
to	O
a	O
than	O
to	O
any	O
other	O
center	O
ties	O
are	O
broken	O
in	O
favor	O
of	O
smaller	O
indices	O
d	O
an	O
set	O
l	O
d	O
h	O
t	O
e	O
centers	O
a	O
n	O
aroun	O
h	O
as	O
t	O
e	O
averages	O
f	O
h	O
t	O
e	O
ak	O
step	O
determme	O
t	O
e	O
new	O
centers	O
a	O
data	O
points	O
within	O
the	O
clusters	O
h	O
ail	O
m	O
l	O
j	O
j	O
xjecn	O
icii	O
step	O
increase	O
i	O
by	O
one	O
and	O
repeat	O
steps	O
and	O
until	O
there	O
are	O
no	O
changes	O
in	O
the	O
cluster	O
centers	O
it	O
is	O
easy	O
to	O
see	O
that	O
each	O
step	O
of	O
the	O
algorithm	B
decreases	O
the	O
empirical	B
squared	O
euclidean	B
distance	O
error	O
on	O
the	O
other	O
hand	O
the	O
empirical	B
squared	B
error	I
can	O
take	O
finitely	O
many	O
different	O
values	O
during	O
the	O
execution	O
of	O
the	O
algorithm	B
therefore	O
data-based	B
scaling	O
the	O
algorithm	B
halts	O
in	O
finite	O
time	O
by	O
inspecting	O
the	O
proof	O
of	O
theorem	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
consistency	B
can	O
also	O
be	O
proved	O
for	O
partitions	O
given	O
by	O
the	O
suboptimal	O
cluster	O
centers	O
obtained	O
by	O
the	O
k-means	O
method	O
provided	O
that	O
it	O
is	O
initialized	O
appropriately	O
problem	O
data-based	B
scaling	O
we	O
now	O
choose	O
the	O
grid	O
size	O
h	O
in	O
a	O
cubic	B
histogram	B
rule	B
in	O
a	O
data-dependent	B
manner	O
and	O
denote	O
its	O
value	O
by	O
hn	O
theorem	O
implies	O
the	O
following	O
general	O
result	O
theorem	O
let	O
gn	O
be	O
the	O
cubic	B
histogram	O
classifier	B
based	O
on	O
a	O
partition	B
into	O
cubes	O
of	O
size	O
hn	O
if	O
lim	O
hn	O
and	O
lim	O
nh	O
with	O
probability	O
one	O
then	O
the	O
partitioning	O
rule	B
is	O
strongly	O
universally	O
consistent	O
to	O
prove	O
the	O
theorem	O
we	O
need	O
the	O
following	O
auxiliary	O
result	O
lemma	O
let	O
z	O
be	O
a	O
sequence	O
of	O
nonnegative	O
random	O
variables	O
if	O
limn--fcxj	O
zn	O
with	O
probability	O
one	O
then	O
there	O
exists	O
a	O
sequence	O
an	O
of	O
positive	O
numbers	O
such	O
that	O
limn--fcxj	O
izn	O
an	O
with	O
probability	O
one	O
proof	O
define	O
vn	O
supm	O
n	O
zm	O
then	O
clearly	O
vn	O
can	O
find	O
a	O
subsequence	O
nl	O
of	O
positive	O
integers	O
such	O
that	O
for	O
each	O
k	O
with	O
probability	O
one	O
we	O
then	O
the	O
borel-cantelli	B
lemma	I
implies	O
that	O
lim	O
ivllk	O
k	O
with	O
probability	O
one	O
k	O
cxj	O
the	O
fact	O
that	O
vn	O
zn	O
and	O
imply	O
the	O
statement	O
proof	O
of	O
theorem	O
let	O
and	O
be	O
sequences	O
of	O
positive	O
numbers	O
with	O
an	O
bn	O
then	O
data-dependent	B
partitioning	O
it	O
follows	O
from	O
lemma	O
that	O
there	O
exist	O
sequences	O
of	O
positive	O
numbers	O
and	O
satisfying	O
an	O
bn	O
bn	O
and	O
na	O
as	O
n	O
such	O
that	O
lim	O
ihilanbzl	O
with	O
probability	O
one	O
n-oo	O
therefore	O
we	O
may	O
assume	O
that	O
for	O
each	O
n	O
phn	O
e	O
bn	O
since	O
bn	O
condition	O
of	O
theorem	O
holds	O
trivially	O
as	O
all	O
diameters	O
of	O
all	O
cells	O
are	O
inferior	O
to	O
bnji	O
it	O
remains	O
to	O
check	O
condition	O
clearly	O
for	O
each	O
m	O
each	O
partition	B
in	O
contains	O
less	O
than	O
cells	O
where	O
the	O
constant	O
c	O
depends	O
on	O
m	O
and	O
d	O
only	O
on	O
the	O
other	O
hand	O
it	O
is	O
easy	O
to	O
see	O
that	O
n	O
points	O
can	O
not	O
be	O
partitioned	O
more	O
than	O
different	O
ways	O
by	O
cubic-grid	O
partitions	O
with	O
cube	O
size	O
h	O
an	O
for	O
some	O
other	O
constant	O
therefore	O
for	O
each	O
m	O
n	O
a	O
n	O
and	O
condition	O
is	O
satisfied	O
in	O
many	O
applications	O
different	O
components	O
of	O
the	O
feature	O
vector	O
x	O
correspond	O
to	O
different	O
physical	O
measurements	O
for	O
example	O
in	O
a	O
medical	O
application	O
the	O
first	O
coordinate	O
could	O
represent	O
blood	O
pressure	O
the	O
second	O
cholesterol	O
level	O
and	O
the	O
third	O
the	O
weight	O
of	O
the	O
patient	O
in	O
such	O
cases	O
there	O
is	O
no	O
reason	O
to	O
use	O
cubic	B
histograms	O
because	O
the	O
resolution	O
of	O
the	O
partition	B
hn	O
along	O
the	O
coordinate	O
axes	O
pends	O
on	O
the	O
apparent	O
scaling	O
of	O
the	O
measurements	O
which	O
is	O
rather	O
arbitrary	O
then	O
one	O
can	O
use	O
scale-independent	O
partitions	O
such	O
as	O
methods	O
based	O
on	O
order	B
statistics	I
described	O
earlier	O
alternatively	O
one	O
might	O
use	O
rectangular	O
partitions	O
instead	O
of	O
cubic	B
ones	O
and	O
let	O
the	O
data	O
decide	O
the	O
scaling	O
along	O
the	O
different	O
coordinate	O
axes	O
again	O
theorem	O
can	O
be	O
used	O
to	O
establish	O
conditions	O
of	O
universal	B
consistency	B
of	O
the	O
classification	O
rule	B
corresponding	O
to	O
data-based	B
rectangular	O
partitions	O
theorem	O
consider	O
a	O
data-dependent	B
histogram	B
rule	B
when	O
the	O
cells	O
of	O
the	O
partition	B
are	O
all	O
rectangles	O
of	O
the	O
form	O
x	O
x	O
lhnd	O
where	O
k	O
kd	O
run	O
through	O
the	O
set	O
of	O
integers	O
and	O
the	O
edge	O
sizes	O
of	O
the	O
rectangles	O
hnd	O
are	O
determinedfrom	O
the	O
data	O
n	O
hni	O
for	O
each	O
i	O
d	O
and	O
nhnl	O
hnd	O
with	O
probability	O
one	O
then	O
the	O
data-dependent	B
rectangular	O
partitioning	O
rule	B
is	O
strongly	O
universally	O
sistent	O
to	O
prove	O
this	O
just	O
check	O
the	O
conditions	O
of	O
theorem	O
we	O
may	O
pick	O
for	O
example	O
hnd	O
to	O
minimize	O
the	O
resubstitution	B
estimate	O
n	O
l	O
ignxi	O
il	O
subject	O
of	O
course	O
to	O
certain	O
conditions	O
so	O
that	O
hni	O
with	O
probability	O
one	O
yet	O
n	O
hni	O
with	O
probability	O
one	O
see	O
problem	O
problems	O
and	O
exercises	O
classification	O
trees	O
consider	O
a	O
partition	B
of	O
the	O
space	O
obtained	O
by	O
a	O
binary	B
classification	O
tree	O
in	O
which	O
each	O
node	O
dichotomizes	O
its	O
set	O
by	O
ahyperplane	O
chapter	O
for	O
more	O
on	O
cation	O
trees	O
the	O
construction	O
of	O
the	O
tree	O
is	O
stopped	O
according	O
to	O
some	O
unspecified	O
rule	B
and	O
classification	O
is	O
by	O
majority	B
vote	I
over	O
the	O
convex	O
polytopes	O
of	O
the	O
partition	B
the	O
following	O
corollary	O
of	O
theorem	O
generalizes	O
the	O
consis	O
tency	O
results	O
in	O
the	O
book	O
by	O
breiman	O
friedman	O
olshen	O
and	O
stone	O
theorem	O
and	O
nobel	O
let	O
gn	O
be	O
a	O
binary	B
tree	O
classifier	B
based	O
upon	O
at	O
most	O
mn	O
hyperplane	O
splits	O
where	O
mn	O
onlogn	O
if	O
in	O
addition	O
condition	O
of	O
theorem	O
is	O
satisfied	O
then	O
gn	O
is	O
strongly	O
consistent	O
in	O
particular	O
the	O
rule	B
is	O
strongly	O
consistent	O
if	O
condition	O
otheorem	O
holds	O
and	O
every	O
cell	O
of	O
the	O
partition	B
contains	O
at	O
least	O
kn	O
points	O
where	O
kn	O
log	O
n	O
proof	O
to	O
check	O
condition	O
of	O
theorem	O
recall	O
theorem	O
which	O
implies	O
that	O
n	O
points	O
in	O
a	O
d-dimensional	O
euclidean	B
space	O
can	O
be	O
dichotomized	O
by	O
hyperplanes	O
in	O
at	O
most	O
different	O
ways	O
from	O
this	O
we	O
see	O
that	O
the	O
number	O
of	O
different	O
ways	O
n	O
points	O
of	O
rd	O
can	O
be	O
partitioned	O
by	O
the	O
rule	B
gil	O
can	O
be	O
bounded	O
by	O
as	O
there	O
are	O
not	O
more	O
than	O
mn	O
cells	O
in	O
the	O
partition	B
thus	O
by	O
the	O
assumption	O
that	O
mn	O
log	O
n	O
n	O
we	O
have	O
ognjn	O
n	O
mn	O
mnd	O
n	O
n	O
so	O
that	O
condition	O
of	O
theorem	O
is	O
satisfied	O
for	O
the	O
second	O
part	O
of	O
the	O
statement	O
observe	O
that	O
there	O
are	O
no	O
more	O
than	O
n	O
k	O
n	O
cells	O
in	O
any	O
partition	B
and	O
that	O
the	O
tree-structured	O
nature	O
of	O
the	O
partitions	O
assures	O
that	O
gn	O
is	O
based	O
on	O
at	O
most	O
n	O
kn	O
hyperplane	O
splits	O
this	O
completes	O
the	O
proof	O
d	O
problems	O
and	O
exercises	O
problem	O
let	O
p	O
be	O
a	O
partition	B
of	O
n	O
d	O
prove	O
that	O
l	O
iilna	O
aep	O
ilai	O
sup	O
iilnb	O
ilbi	O
besp	O
where	O
the	O
class	O
of	O
sets	O
bp	O
contains	O
all	O
sets	O
obtained	O
by	O
unions	O
of	O
cells	O
of	O
p	O
this	O
is	O
scheffes	O
theorem	O
for	O
partitions	O
see	O
also	O
problem	O
data-dependent	B
partitioning	O
problem	O
show	O
that	O
condition	O
of	O
theorem	O
may	O
be	O
replaced	O
by	O
the	O
following	O
for	O
every	O
y	O
and	O
e	O
lim	O
tcrd	O
inf	O
diama	O
n	O
t	O
y	O
with	O
probability	O
one	O
problem	O
let	O
x	O
be	O
uniformly	O
distributed	O
on	O
the	O
unit	O
square	O
let	O
if	O
xl	O
and	O
otherwise	O
figure	O
consider	O
the	O
algorithm	B
when	O
first	O
the	O
xl-coordinate	O
is	O
cut	O
at	O
the	O
k-th	O
smallest	O
xl-value	O
among	O
the	O
training	O
data	O
next	O
the	O
rectangle	O
with	O
n	O
k	O
points	O
is	O
cut	O
according	O
to	O
the	O
isolating	O
another	O
k	O
points	O
this	O
is	O
repeated	O
on	O
a	O
rotational	O
basis	O
for	O
the	O
two	O
coordinate	O
axes	O
show	O
that	O
the	O
error	O
probability	O
of	O
the	O
obtained	O
partitioning	O
classifier	B
does	O
not	O
tend	O
to	O
l	O
can	O
you	O
determine	O
the	O
asymptotic	O
error	O
probability	O
problem	O
modify	O
gessamans	O
rule	B
based	O
on	O
statistically	B
equivalent	I
blocks	I
so	O
that	O
the	O
rule	B
is	O
strongly	O
universally	O
consistent	O
problem	O
cut	O
each	O
axis	O
independently	O
into	O
intervals	O
containing	O
exactly	O
k	O
of	O
the	O
jected	O
data	O
points	O
the	O
i	O
axis	O
has	O
intervals	O
a	O
li	O
form	O
a	O
histogram	B
rule	B
that	O
takes	O
a	O
majority	B
vote	I
over	O
the	O
product	B
sets	O
a	O
il	O
x	O
x	O
aidd	O
figure	O
a	O
partition	B
based	O
upon	O
the	O
method	O
obtained	O
above	O
with	O
k	O
cii	O
this	O
rule	B
does	O
not	O
guarantee	O
a	O
minimal	O
number	O
of	O
points	O
in	O
every	O
cell	O
nevertheless	O
if	O
k	O
d	O
on	O
k	O
show	O
that	O
this	O
decision	O
rule	B
is	O
consistent	O
i	O
e	O
that	O
e	O
la	O
l	O
in	O
probability	O
problem	O
show	O
that	O
each	O
step	O
of	O
the	O
k-means	B
clustering	B
algorithm	B
decreases	O
the	O
empirical	B
squared	B
error	I
conclude	O
that	O
theorem	O
is	O
also	O
true	O
if	O
the	O
clusters	O
are	O
given	O
by	O
the	O
k-means	O
algorithm	B
hint	O
observe	O
that	O
the	O
only	O
property	O
of	O
the	O
clusters	O
used	O
in	O
the	O
proof	O
of	O
theorem	O
is	O
that	O
e	O
iix	O
dn	O
with	O
probability	O
one	O
this	O
can	O
be	O
proven	O
for	O
clusters	O
given	O
by	O
the	O
k-means	O
algorithm	B
if	O
it	O
is	O
appropriately	O
initialized	O
to	O
this	O
end	O
use	O
the	O
techniques	O
of	O
problem	O
problem	O
prove	O
theorem	O
problem	O
consider	O
the	O
hni	O
in	O
theorem	O
the	O
interval	O
sizes	O
for	O
cubic	B
histograms	O
let	O
the	O
hni	O
be	O
found	O
by	O
minimizing	O
n	O
l	O
ignxljyil	O
i	O
problems	O
and	O
exercises	O
subject	O
to	O
the	O
condition	O
that	O
each	O
marginal	O
interval	O
contain	O
at	O
least	O
k	O
data	O
points	O
is	O
at	O
least	O
k	O
data	O
points	O
have	O
that	O
one	O
coordinate	O
in	O
the	O
interval	O
under	O
which	O
condition	O
on	O
k	O
is	O
the	O
rule	B
consistent	O
problem	O
take	O
a	O
histogram	B
rule	B
with	O
data-dependent	B
sizes	O
hnl	O
hnd	O
as	O
in	O
rem	O
defined	O
as	O
follows	O
ni	O
max	O
ljn	O
hni	O
i	O
d	O
where	O
and	O
wni	O
are	O
and	O
percentiles	O
mm	O
ljn	O
j	O
y	O
n	O
ls	O
were	O
d	O
x	O
j	O
xi	O
r	O
xd	O
j	O
j	O
h	O
xu	O
j	O
of	O
xii	O
xi	O
assume	O
for	O
convenience	O
that	O
x	O
has	O
nonatomic	O
marginals	O
show	O
that	O
leads	O
sometimes	O
to	O
an	O
inconsistent	O
rule	B
even	O
if	O
d	O
show	O
that	O
always	O
yields	O
a	O
scale-invariant	O
consistent	O
rule	B
splitting	B
the	I
data	I
the	O
holdout	B
estimate	O
universal	B
consistency	B
gives	O
us	O
a	O
partial	O
satisfaction-without	O
knowing	O
the	O
lying	O
distribution	O
taking	O
more	O
samples	O
is	O
guaranteed	O
to	O
push	O
us	O
close	O
to	O
the	O
bayes	O
rule	B
in	O
the	O
long	O
run	O
unfortunately	O
we	O
will	O
never	O
know	O
just	O
how	O
close	O
we	O
are	O
to	O
the	O
bayes	O
rule	B
unless	O
we	O
are	O
given	O
more	O
information	O
about	O
the	O
unknown	O
distribution	O
chapter	O
a	O
more	O
modest	O
goal	O
is	O
to	O
do	O
as	O
well	O
as	O
possible	O
within	O
a	O
given	O
class	O
of	O
rules	O
to	O
fix	O
the	O
ideas	O
consider	O
all	O
nearest	B
neighbor	I
rules	O
based	O
upon	O
metrics	O
of	O
the	O
form	O
d	O
il	O
where	O
ai	O
for	O
all	O
i	O
and	O
x	O
xd	O
here	O
the	O
ais	O
are	O
variable	B
scale	O
tors	O
let	O
be	O
a	O
particular	O
nearest	B
neighbor	I
rule	B
for	O
a	O
given	O
choice	O
of	O
ad	O
and	O
let	O
gn	O
be	O
a	O
data-based	B
rule	B
chosen	O
from	O
this	O
class	O
the	O
best	O
we	O
can	O
hope	O
for	O
now	O
is	O
something	O
like	O
lgn	O
infpn	O
lpn	O
in	O
probability	O
for	O
all	O
distributions	O
where	O
lgn	O
pgnx	O
yidn	O
is	O
the	O
conditional	O
ity	O
of	O
error	O
for	O
gn	O
this	O
sort	O
of	O
optimality-within-a-class	O
is	O
definitely	O
achievable	O
however	O
proving	O
such	O
optimality	O
is	O
generally	O
not	O
easy	O
as	O
gn	O
depends	O
on	O
the	O
data	O
in	O
this	O
chapter	O
we	O
present	O
one	O
possible	O
methodology	O
for	O
selecting	O
provably	O
good	O
rules	O
from	O
restricted	O
classes	O
this	O
is	O
achieved	O
by	O
splitting	B
the	I
data	I
into	O
a	O
training	O
splitting	B
the	I
data	I
sequence	O
and	O
a	O
testing	B
sequence	I
this	O
idea	O
was	O
explored	O
and	O
analyzed	O
in	O
depth	O
in	O
devroye	O
and	O
is	O
now	O
formalized	O
the	O
data	O
sequence	O
dn	O
is	O
split	O
into	O
a	O
training	O
sequence	O
dm	O
yd	O
ym	O
and	O
a	O
testing	B
sequence	I
tt	O
ymr	O
yml	O
where	O
l	O
m	O
n	O
the	O
sequence	O
dm	O
defines	O
a	O
class	O
of	O
classifiers	O
en	O
whose	O
members	O
are	O
denoted	O
by	O
m	O
m	O
dm	O
the	O
testing	B
sequence	I
is	O
used	O
to	O
select	O
a	O
classifier	B
from	O
cm	O
that	O
minimizes	O
the	O
error	O
count	O
this	O
error	O
estimate	O
is	O
called	O
the	O
holdout	B
estimate	O
as	O
the	O
testing	B
sequence	I
is	O
out	O
of	O
the	O
design	O
of	O
m	O
thus	O
the	O
selected	O
classifier	B
gn	O
e	O
cm	O
satisfies	O
lmlgn	O
lml	O
m	O
for	O
all	O
m	O
e	O
cm	O
the	O
subscript	O
n	O
in	O
gn	O
may	O
be	O
a	O
little	O
confusing	O
since	O
gn	O
is	O
in	O
cm	O
a	O
class	O
of	O
classifiers	O
depending	O
on	O
the	O
first	O
m	O
pairs	O
dm	O
only	O
however	O
gil	O
depends	O
on	O
the	O
entire	O
data	O
dn	O
as	O
the	O
rest	O
of	O
the	O
data	O
is	O
used	O
for	O
testing	O
the	O
classifiers	O
in	O
cm	O
we	O
are	O
interested	O
in	O
the	O
difference	O
between	O
the	O
error	O
probability	O
and	O
that	O
of	O
the	O
best	O
classifier	B
in	O
cm	O
inf	O
ecn	O
l	O
m	O
note	O
that	O
l	O
m	O
p	O
m	O
y	O
i	O
dm	O
denotes	O
the	O
error	O
probability	O
conditioned	O
on	O
dm	O
the	O
conditional	O
bility	O
is	O
small	O
when	O
most	O
testing	O
sequences	O
tt	O
pick	O
a	O
rule	B
gn	O
whose	O
error	O
probability	O
is	O
within	O
e	O
of	O
the	O
best	O
classifier	B
in	O
cm	O
we	O
have	O
already	O
addressed	O
similar	O
questions	O
in	O
chapters	O
and	O
there	O
we	O
have	O
seen	O
that	O
lgn	O
inf	O
l	O
m	O
sup	O
ilml	O
m	O
l	O
mi	O
if	O
cm	O
contains	O
finitely	O
many	O
rules	O
then	O
the	O
bound	O
of	O
theorem	O
may	O
be	O
useful	O
if	O
we	O
take	O
m	O
then	O
theorem	O
shows	O
problem	O
that	O
on	O
the	O
average	O
we	O
are	O
within	O
icm	O
i	O
i	O
n	O
of	O
the	O
best	O
possible	O
error	O
rate	O
whatever	O
it	O
is	O
if	O
cm	O
is	O
of	O
infinite	O
cardinality	O
then	O
we	O
can	O
use	O
the	O
vapnik-chervonenkis	B
theory	O
to	O
get	O
similar	O
inequalities	O
for	O
example	O
from	O
theorem	O
we	O
get	O
consistency	B
and	O
asymptotic	B
optimality	I
and	O
consequently	O
p	O
inf	O
l	O
m	O
ei	O
dm	O
s	O
where	O
scm	O
i	O
is	O
the	O
l-th	O
shatter	B
coefficient	I
corresponding	O
to	O
the	O
class	O
of	O
classifiers	O
em	O
theorem	O
for	O
the	O
definition	O
since	O
cm	O
depends	O
on	O
the	O
training	O
data	O
dm	O
the	O
shatter	O
coefficients	O
scm	O
l	O
may	O
depend	O
on	O
d	O
m	O
too	O
however	O
usually	O
itis	O
possible	O
to	O
find	O
upper	O
bounds	O
on	O
the	O
random	O
variable	B
scm	O
i	O
that	O
depend	O
on	O
m	O
and	O
i	O
only	O
but	O
not	O
on	O
the	O
actual	O
values	O
of	O
the	O
random	O
variables	O
xl	O
x	O
m	O
y	O
m	O
both	O
upper	O
bounds	O
above	O
are	O
distribution-free	O
and	O
the	O
problem	O
now	O
is	O
purely	O
combinatorial	O
count	O
icm	O
i	O
is	O
usually	O
trivial	O
or	O
compute	O
scm	O
l	O
remark	O
with	O
much	O
more	O
effort	O
it	O
is	O
possible	O
to	O
obtain	O
performance	O
bounds	O
of	O
the	O
holdout	B
estimate	O
in	O
the	O
form	O
of	O
bounds	O
for	O
for	O
some	O
special	O
rules	O
where	O
m	O
and	O
n	O
are	O
carefully	O
defined	O
for	O
example	O
devroye	O
and	O
wagner	O
give	O
upper	O
bounds	O
when	O
both	O
m	O
and	O
n	O
are	O
k-nearest	O
neighbor	O
classifiers	O
with	O
the	O
same	O
k	O
working	O
with	O
different	O
sample	O
size	O
d	O
remark	O
minimizing	O
the	O
holdout	B
estimate	O
is	O
not	O
the	O
only	O
possibility	O
other	O
error	O
estimates	O
that	O
do	O
not	O
split	O
the	O
data	O
may	O
be	O
used	O
in	O
classifier	B
selection	I
as	O
well	O
such	O
estimates	O
are	O
discussed	O
in	O
chapters	O
and	O
however	O
these	O
estimates	O
are	O
usually	O
tailored	O
to	O
work	O
well	O
for	O
specific	O
discrimination	O
rules	O
the	O
most	O
general	O
and	O
robust	O
method	O
is	O
certainly	O
the	O
data	O
splitting	O
described	O
here	O
d	O
consistency	B
and	O
asymptotic	B
optimality	I
typically	O
cm	O
becomes	O
richer	O
as	O
m	O
grows	O
and	O
it	O
is	O
natural	O
to	O
ask	O
whether	O
the	O
empirically	O
best	O
classifier	B
in	O
cm	O
is	O
consistent	O
theorem	O
assume	O
that	O
from	O
each	O
cm	O
we	O
can	O
pick	O
one	O
m	O
such	O
that	O
the	O
sequence	O
of	O
m	O
is	O
consistent	O
for	O
a	O
certain	O
class	O
of	O
distributions	O
then	O
the	O
matic	O
rule	B
gn	O
defined	O
above	O
is	O
consistent	O
for	O
the	O
same	O
class	O
of	O
distributions	O
elgn	O
l	O
as	O
n	O
if	O
l	O
l	O
o	O
proof	O
decompose	O
the	O
difference	O
between	O
the	O
actual	O
error	O
probability	O
and	O
the	O
bayes	B
error	I
as	O
splitting	B
the	I
data	I
the	O
convergence	O
of	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
is	O
a	O
direct	O
corollary	O
of	O
the	O
second	O
term	O
converges	O
to	O
zero	O
by	O
assumption	O
theorem	O
shows	O
that	O
a	O
consistent	O
rule	B
is	O
picked	O
if	O
the	O
sequence	O
of	O
cms	O
contains	O
a	O
consistent	O
rule	B
even	O
if	O
we	O
do	O
not	O
know	O
which	O
functions	O
from	O
em	O
lead	O
to	O
consistency	B
if	O
we	O
are	O
just	O
worried	O
about	O
consistency	B
theorem	O
reassures	O
us	O
that	O
nothing	O
is	O
lost	O
as	O
long	O
as	O
we	O
take	O
l	O
much	O
larger	O
than	O
logescm	O
l	O
often	O
this	O
reduces	O
to	O
a	O
very	O
weak	B
condition	O
on	O
the	O
size	O
l	O
of	O
the	O
testing	O
set	O
let	O
us	O
now	O
introduce	O
the	O
notion	O
of	O
asymptotic	B
optimality	I
a	O
sequence	O
of	O
rules	O
gn	O
is	O
said	O
to	O
be	O
asymptotically	O
optimal	O
for	O
a	O
given	O
distribution	O
of	O
y	O
when	O
elgn	O
l	O
hm	O
e	O
mecm	O
l	O
m	O
l	O
l	O
our	O
definition	O
is	O
not	O
entirely	O
fair	O
because	O
gn	O
uses	O
n	O
observations	O
whereas	O
the	O
family	B
of	I
rules	O
in	O
the	O
denominator	O
is	O
restricted	O
to	O
using	O
m	O
observations	O
if	O
gn	O
is	O
not	O
taken	O
from	O
the	O
same	O
em	O
then	O
it	O
is	O
possible	O
to	O
have	O
a	O
ratio	O
which	O
is	O
smaller	O
than	O
one	O
but	O
if	O
gn	O
e	O
cm	O
then	O
the	O
ratio	O
always	O
is	O
at	O
least	O
one	O
that	O
is	O
why	O
the	O
definition	O
makes	O
sense	O
in	O
our	O
setup	O
when	O
our	O
selected	O
rule	B
is	O
asymptotically	O
optimal	O
we	O
have	O
achieved	O
something	O
very	O
strong	O
we	O
have	O
in	O
effect	O
picked	O
a	O
rule	B
better	O
a	O
sequence	O
of	O
rules	O
which	O
has	O
a	O
probability	O
of	O
error	O
converging	O
at	O
the	O
optimal	O
rate	O
attainable	O
within	O
the	O
sequence	O
of	O
ems	O
and	O
we	O
do	O
not	O
even	O
have	O
to	O
know	O
what	O
the	O
optimal	O
rate	B
of	I
convergence	I
is	O
this	O
is	O
especially	O
important	O
in	O
nonparametric	O
rules	O
where	O
some	O
researchers	O
choose	O
smoothing	O
factors	O
based	O
on	O
theoretical	B
results	O
about	O
the	O
optimal	O
attainable	O
rate	B
of	I
convergence	I
for	O
certain	O
classes	O
of	O
distributions	O
we	O
are	O
constantly	O
faced	O
with	O
the	O
problem	O
of	O
choosing	O
between	O
parametric	O
and	O
nonparametric	O
discriminators	O
parametric	O
discriminators	O
are	O
based	O
upon	O
an	O
lying	O
model	O
in	O
which	O
a	O
finite	O
number	O
of	O
unknown	O
parameters	O
is	O
estimated	O
from	O
the	O
data	O
a	O
case	O
in	O
point	O
is	O
the	O
multivariate	O
normal	B
distribution	I
which	O
leads	O
to	O
linear	O
or	O
quadratic	O
discriminators	O
if	O
the	O
model	O
is	O
wrong	O
parametric	O
methods	O
can	O
perform	O
very	O
poorly	O
when	O
the	O
model	O
is	O
right	O
their	O
performance	O
is	O
difficult	O
to	O
beat	O
the	O
method	O
based	O
on	O
splitting	B
the	I
data	I
chooses	O
among	O
the	O
best	O
discriminator	O
depending	O
upon	O
which	O
happens	O
to	O
be	O
best	O
for	O
the	O
given	O
data	O
we	O
can	O
throw	O
in	O
em	O
a	O
variety	O
of	O
rules	O
including	O
nearest	B
neighbor	I
rules	O
a	O
few	O
linear	O
discriminators	O
a	O
couple	O
of	O
tree	O
classifiers	O
and	O
perhaps	O
a	O
kernel	B
rule	B
the	O
probability	O
bounds	O
above	O
can	O
be	O
used	O
when	O
the	O
complexity	O
of	O
em	O
by	O
its	O
shatter	O
coefficients	O
does	O
not	O
get	O
out	O
of	O
hand	O
the	O
notion	O
of	O
asymptotic	B
optimality	I
can	O
be	O
too	O
strong	O
in	O
many	O
cases	O
the	O
reason	O
for	O
this	O
is	O
that	O
in	O
some	O
rare	O
lucky	O
situations	O
e	O
mecm	O
l	O
m	O
l	O
may	O
be	O
very	O
small	O
in	O
these	O
cases	O
it	O
is	O
impossible	O
to	O
achieve	O
asymptotic	B
optimality	I
we	O
can	O
fix	O
this	O
problem	O
by	O
introducing	O
the	O
notion	O
of	O
em	O
where	O
em	O
is	O
a	O
positive	O
sequence	O
decreasing	O
to	O
with	O
m	O
a	O
rule	B
is	O
said	O
to	O
be	O
em-optimal	O
when	O
elgn	O
l	O
hm	O
n-oo	O
e	O
mecm	O
l	O
m	O
l	O
l	O
nearest	B
neighbor	I
rules	O
with	O
automatic	B
scaling	O
for	O
all	O
distributions	O
of	O
y	O
for	O
which	O
lim	O
e	O
l	O
m	O
l	O
in	O
what	O
follows	O
we	O
apply	O
the	O
idea	O
of	O
data	O
splitting	O
to	O
scaled	O
nearest	B
neighbor	I
rules	O
and	O
to	O
rules	O
closely	O
related	O
to	O
the	O
data-dependent	B
partitioning	O
classifiers	O
studied	O
in	O
chapter	O
many	O
more	O
examples	O
are	O
presented	O
in	O
chapters	O
and	O
nearest	B
neighbor	I
rules	O
with	O
automatic	B
scaling	O
let	O
us	O
work	O
out	O
the	O
simple	O
example	O
introduced	O
above	O
the	O
rule	B
is	O
the	O
nearest	B
neighbor	I
rule	B
for	O
the	O
metric	O
ilxll	O
where	O
x	O
xed	O
the	O
class	O
em	O
is	O
the	O
class	O
of	O
all	O
ad-nn	O
rules	O
for	O
dm	O
yi	O
ym	O
the	O
testing	B
sequence	I
tz	O
is	O
used	O
to	O
choose	O
ad	O
so	O
as	O
to	O
minimize	O
the	O
holdout	B
error	O
estimate	O
in	O
order	O
to	O
have	O
it	O
suffices	O
that	O
log	O
e	O
l	O
l	O
this	O
puts	O
a	O
lower	O
bound	O
on	O
l	O
to	O
get	O
this	O
lower	O
bound	O
one	O
must	O
compute	O
s	O
clearly	O
seem	O
l	O
is	O
bounded	O
by	O
the	O
number	O
of	O
ways	O
of	O
classifying	O
xml	O
xmz	O
using	O
rules	O
picked	O
from	O
em	O
that	O
is	O
the	O
total	O
number	O
of	O
different	O
values	O
for	O
we	O
now	O
show	O
that	O
regardless	O
of	O
dm	O
and	O
xml	O
xmz	O
for	O
n	O
we	O
have	O
this	O
sort	O
of	O
result	O
is	O
typical-the	O
bound	O
does	O
not	O
depend	O
upon	O
dm	O
or	O
tz	O
when	O
plugged	O
back	O
into	O
the	O
condition	O
of	O
convergence	O
it	O
yields	O
the	O
simple	O
condition	O
logm	O
o	O
l	O
in	O
fact	O
we	O
may	O
thus	O
take	O
slightly	O
larger	O
than	O
log	O
m	O
it	O
would	O
plainly	O
be	O
silly	O
to	O
take	O
as	O
we	O
would	O
thereby	O
in	O
fact	O
throwaway	O
most	O
of	O
the	O
data	O
splitting	B
the	I
data	I
set	O
am	O
mxmd	O
mxmz	O
m	O
e	O
cm	O
to	O
count	O
the	O
number	O
of	O
values	O
in	O
am	O
note	O
that	O
all	O
squared	O
distances	O
can	O
be	O
written	O
as	O
x	O
j	O
laipkij	O
d	O
kl	O
where	O
pkij	O
is	O
a	O
nonnegative	O
number	O
only	O
depending	O
upon	O
xi	O
and	O
x	O
j	O
note	O
that	O
each	O
squared	O
distance	O
is	O
linear	O
in	O
aj	O
now	O
consider	O
the	O
space	O
of	O
a	O
observe	O
that	O
in	O
this	O
space	O
mxmd	O
is	O
constant	O
within	O
each	O
cell	O
of	O
the	O
partition	B
determined	O
by	O
the	O
hyperplanes	O
laipkjml	O
laipkilml	O
s	O
i	O
if	O
sm	O
d	O
kl	O
d	O
kl	O
to	O
see	O
this	O
note	O
that	O
within	O
each	O
set	O
in	O
the	O
partition	B
keeps	O
the	O
same	O
nearest	B
neighbor	I
among	O
xl	O
x	O
m	O
it	O
is	O
known	O
that	O
k	O
hyperplanes	O
in	O
rd	O
create	O
partitions	O
of	O
cardinality	O
not	O
exceeding	O
k	O
d	O
problem	O
now	O
overlay	O
the	O
i	O
partitions	O
obtained	O
for	O
mxmd	O
mxnhz	O
respectively	O
this	O
yields	O
at	O
most	O
sets	O
as	O
the	O
overlays	O
are	O
determined	O
by	O
i	O
g	O
hyperplanes	O
but	O
clearly	O
on	O
each	O
of	O
these	O
sets	O
mxmz	O
is	O
constant	O
therefore	O
scm	O
iaml	O
d	O
classification	O
based	O
on	O
clustering	B
recall	O
the	O
classification	O
rule	B
based	O
on	O
clustering	B
that	O
was	O
introduced	O
in	O
chapter	O
the	O
data	O
points	O
xl	O
xn	O
are	O
grouped	O
into	O
k	O
clusters	O
where	O
k	O
is	O
a	O
predetermined	O
integer	O
and	O
a	O
majority	B
vote	I
decides	O
within	O
the	O
k	O
clusters	O
if	O
k	O
is	O
chosen	O
such	O
that	O
k	O
and	O
k	O
log	O
n	O
n	O
then	O
the	O
rule	B
is	O
consistent	O
for	O
a	O
given	O
finite	O
n	O
however	O
these	O
conditions	O
give	O
little	O
guidance	O
also	O
the	O
choice	O
of	O
k	O
could	O
dramatically	O
affect	O
the	O
performance	O
of	O
the	O
rule	B
as	O
there	O
may	O
be	O
a	O
mismatch	O
between	O
k	O
and	O
some	O
unknown	O
natural	O
number	O
of	O
clusters	O
for	O
example	O
one	O
may	O
construct	O
distributions	O
in	O
which	O
the	O
optimal	O
number	O
of	O
clusters	O
does	O
not	O
increase	O
with	O
n	O
let	O
us	O
split	O
the	O
data	O
and	O
let	O
the	O
testing	B
sequence	I
decide	O
the	O
value	O
of	O
k	O
in	O
the	O
framework	O
of	O
this	O
chapter	O
em	O
contains	O
the	O
classifiers	O
based	O
on	O
the	O
first	O
m	O
pairs	O
dm	O
of	O
the	O
data	O
with	O
all	O
possible	O
values	O
of	O
k	O
clearly	O
en	O
is	O
a	O
finite	O
family	O
with	O
icml	O
m	O
in	O
this	O
case	O
by	O
problem	O
we	O
have	O
i	O
the	O
consistency	B
result	O
theorem	O
implies	O
that	O
statistically	B
equivalent	I
blocks	I
for	O
all	O
distributions	O
ifthe	O
xis	O
have	O
bounded	O
support	B
thus	O
we	O
see	O
that	O
our	O
strategy	O
leads	O
to	O
a	O
universally	O
consistent	O
rule	B
whenever	O
mj	O
o	O
this	O
is	O
a	O
very	O
mild	O
condition	O
since	O
we	O
can	O
take	O
l	O
equal	O
to	O
a	O
small	O
fraction	O
of	O
n	O
without	O
sacrificing	O
consistency	B
if	O
l	O
is	O
small	O
compared	O
to	O
n	O
then	O
m	O
is	O
close	O
to	O
n	O
so	O
inf	O
mecm	O
is	O
likely	O
to	O
be	O
close	O
to	O
inf	O
ii	O
ecli	O
thus	O
we	O
do	O
not	O
lose	O
much	O
by	O
sacrificing	O
a	O
part	O
of	O
the	O
data	O
for	O
testing	O
purposes	O
but	O
the	O
gain	O
can	O
be	O
tremendous	O
as	O
we	O
are	O
guaranteed	O
to	O
be	O
within	O
m	O
j	O
of	O
the	O
optimum	O
in	O
the	O
class	O
em	O
that	O
we	O
cannot	O
take	O
i	O
and	O
hope	O
to	O
obtain	O
consistency	B
should	O
be	O
obvious	O
it	O
should	O
also	O
be	O
noted	O
that	O
for	O
i	O
m	O
we	O
are	O
roughly	O
within	O
of	O
the	O
best	O
possible	O
probability	O
of	O
error	O
within	O
the	O
family	O
also	O
the	O
empirical	B
selection	O
rule	B
is	O
jfogm	O
j	O
i-optimal	O
statistically	B
equivalent	I
blocks	I
recall	O
from	O
chapter	O
that	O
classifiers	O
based	O
on	O
statistically	B
equivalent	I
blocks	I
typically	O
partition	B
the	O
feature	O
space	O
rd	O
into	O
rectangles	O
such	O
that	O
each	O
rectangle	O
contains	O
k	O
points	O
where	O
k	O
is	O
a	O
certain	O
positive	O
integer	O
the	O
parameter	O
of	O
the	O
rule	B
this	O
may	O
be	O
done	O
in	O
several	O
different	O
ways	O
one	O
of	O
many	O
such	O
rules-the	O
rule	B
introduced	O
by	O
gessaman	O
is	O
consistent	O
if	O
k	O
and	O
k	O
j	O
n	O
again	O
we	O
can	O
let	O
the	O
data	O
pick	O
the	O
value	O
of	O
k	O
by	O
minimizing	O
the	O
holdout	B
estimate	O
just	O
as	O
in	O
the	O
previous	O
section	O
lem	O
i	O
m	O
and	O
every	O
remark	O
mentioned	O
there	O
about	O
consistency	B
and	O
asymptotic	B
optimality	I
remains	O
true	O
for	O
this	O
case	O
as	O
well	O
we	O
can	O
enlarge	O
the	O
family	O
em	O
by	O
allowing	O
partitions	O
without	O
restrictions	O
on	O
cardinalities	O
of	O
cells	O
this	O
leads	O
very	O
quickly	O
to	O
oversized	O
families	O
of	O
rules	O
and	O
we	O
have	O
to	O
impose	O
reasonable	O
restrictions	O
consider	O
cuts	O
into	O
at	O
most	O
k	O
rectangles	O
where	O
k	O
is	O
a	O
number	O
picked	O
beforehand	O
recall	O
that	O
for	O
a	O
fixed	O
partition	B
the	O
class	O
assigned	O
to	O
every	O
rectangle	O
is	O
decided	O
upon	O
by	O
a	O
majority	B
vote	I
among	O
the	O
training	O
points	O
on	O
the	O
real	O
line	O
choosing	O
a	O
partition	B
into	O
at	O
most	O
k	O
sets	O
is	O
equivalent	O
to	O
choosing	O
k	O
cut	O
positions	O
from	O
m	O
i	O
n	O
spacings	O
between	O
all	O
test	O
and	O
training	O
points	O
hence	O
seml	O
n	O
for	O
consistency	B
k	O
has	O
to	O
grow	O
as	O
n	O
grows	O
it	O
is	O
easily	O
seen	O
that	O
splitting	B
the	I
data	I
if	O
k	O
and	O
m	O
to	O
achieve	O
consistency	B
of	O
the	O
selected	O
rule	B
however	O
we	O
also	O
need	O
log	O
scm	O
l	O
l	O
llogn	O
now	O
consider	O
d-dimensional	O
partitions	O
defined	O
by	O
at	O
most	O
k	O
consecutive	O
orthogonal	O
cuts	O
that	O
is	O
the	O
first	O
cut	O
divides	O
rd	O
into	O
two	O
halfspaces	O
along	O
a	O
hyper	O
plane	O
perpendicular	O
to	O
one	O
of	O
the	O
coordinate	O
axes	O
the	O
second	O
cut	O
splits	O
one	O
of	O
the	O
halfspaces	O
into	O
two	O
parts	O
along	O
another	O
orthogonal	O
hyperplane	O
and	O
so	O
forth	O
this	O
procedure	O
yields	O
a	O
partition	B
of	O
the	O
space	O
into	O
k	O
rectangles	O
we	O
see	O
that	O
for	O
the	O
first	O
cut	O
there	O
are	O
at	O
most	O
dn	O
possible	O
combinations	O
to	O
choose	O
from	O
this	O
yields	O
the	O
loose	O
upper	O
bound	O
this	O
bound	O
is	O
also	O
valid	O
for	O
all	O
grids	O
defined	O
by	O
at	O
most	O
k	O
cuts	O
the	O
main	O
difference	O
here	O
is	O
that	O
every	O
cut	O
defines	O
two	O
halfspaces	O
of	O
r	O
d	O
and	O
not	O
two	O
rectangles	O
of	O
a	O
cells	O
so	O
that	O
we	O
usually	O
end	O
up	O
with	O
rectangles	O
in	O
the	O
partition	B
assume	O
that	O
cm	O
contains	O
all	O
histograms	O
with	O
partitions	O
into	O
at	O
most	O
k	O
infinite	O
rectangles	O
then	O
considering	O
that	O
a	O
rectangle	O
in	O
rd	O
requires	O
choosing	O
spacings	O
between	O
all	O
test	O
and	O
training	O
points	O
two	O
per	O
coordinate	O
axis	O
see	O
feinholz	O
for	O
more	O
work	O
on	O
such	O
partitions	O
binary	B
tree	O
classifiers	O
we	O
can	O
analyze	O
binary	B
tree	O
classifiers	O
from	O
the	O
same	O
viewpoint	O
recall	O
that	O
such	O
classifiers	O
are	O
represented	O
by	O
binary	B
trees	O
where	O
each	O
internal	O
node	O
corresponds	O
to	O
a	O
split	O
of	O
a	O
cell	O
by	O
a	O
hyperplane	O
and	O
the	O
terminal	O
nodes	O
represent	O
the	O
cells	O
of	O
the	O
partition	B
assume	O
that	O
there	O
are	O
k	O
cells	O
therefore	O
k	O
splits	O
leading	O
to	O
the	O
partition	B
if	O
every	O
split	O
is	O
perpendicular	O
to	O
one	O
of	O
the	O
axes	O
then	O
the	O
situation	O
is	O
the	O
same	O
as	O
in	O
the	O
previous	O
section	O
for	O
smaller	O
families	O
of	O
rules	O
whose	O
cuts	O
depend	O
upon	O
the	O
training	O
sequence	O
only	O
the	O
bound	O
is	O
pessimistic	O
others	O
have	O
proposed	O
generalizing	O
orthogonal	O
cuts	O
by	O
using	O
general	O
hyperplane	O
cuts	O
recall	O
that	O
there	O
are	O
at	O
most	O
ways	O
of	O
dichotomizing	O
n	O
points	O
in	O
nd	O
by	O
hyperplanes	O
theorem	O
thus	O
if	O
we	O
allow	O
up	O
to	O
k	O
internal	O
nodes	O
hyperplane	O
cuts	O
problems	O
and	O
exercises	O
sem	O
l	O
the	O
number	O
of	O
internal	O
nodes	O
has	O
to	O
be	O
restricted	O
in	O
order	O
to	O
obtain	O
consistency	B
from	O
this	O
bound	O
refer	O
to	O
chapter	O
for	O
more	O
details	O
problems	O
and	O
exercises	O
problem	O
prove	O
that	O
n	O
hyperplanes	O
partition	B
n	O
d	O
into	O
at	O
most	O
contiguous	O
regions	O
when	O
d	O
n	O
cover	O
hint	O
proceed	O
by	O
induction	O
problem	O
assume	O
that	O
gn	O
is	O
selected	O
so	O
as	O
to	O
minimize	O
the	O
holdout	B
error	O
estimate	O
ltm	O
m	O
over	O
cm	O
the	O
class	O
of	O
rules	O
based	O
upon	O
the	O
first	O
m	O
data	O
points	O
assume	O
furthermore	O
that	O
we	O
vary	O
lover	O
n	O
and	O
that	O
we	O
pick	O
the	O
best	O
i	O
m	O
n	O
by	O
minimizing	O
the	O
holdout	B
error	O
estimate	O
again	O
show	O
that	O
if	O
scm	O
i	O
ony	O
for	O
some	O
y	O
then	O
the	O
obtained	O
rule	B
is	O
strongly	O
universally	O
consistent	O
problem	O
let	O
cm	O
be	O
the	O
class	O
of	O
ad-nn	O
rules	O
based	O
upon	O
dm	O
show	O
that	O
if	O
min	O
then	O
inf	O
l	O
m	O
inf	O
l	O
n	O
in	O
probability	O
for	O
all	O
distributions	O
of	O
y	O
for	O
which	O
x	O
has	O
a	O
density	O
conclude	O
that	O
if	O
ljlogn	O
m	O
n	O
on	O
then	O
lgn	O
inf	O
l	O
n	O
in	O
probability	O
problem	O
finding	O
the	O
best	O
split	O
this	O
exercise	O
is	O
concerned	O
with	O
the	O
automatic	B
selection	O
of	O
m	O
and	O
i	O
n	O
m	O
if	O
gn	O
is	O
the	O
selected	O
rule	B
minimizing	O
the	O
holdout	B
estimate	O
then	O
mf	O
l	O
m	O
l	O
i	O
since	O
in	O
most	O
interesting	O
cases	O
scm	O
i	O
is	O
bounded	O
from	O
above	O
as	O
a	O
polynomial	B
ofm	O
and	O
i	O
the	O
estimation	B
error	I
typically	O
decreases	O
as	O
i	O
increases	O
on	O
the	O
other	O
hand	O
the	O
approximation	B
error	I
infpmecm	O
l	O
m	O
l	O
typically	O
decreases	O
as	O
m	O
increases	O
as	O
the	O
class	O
cm	O
gets	O
richer	O
some	O
kind	O
of	O
balance	O
between	O
the	O
two	O
terms	O
is	O
required	O
to	O
get	O
optimum	O
performance	O
we	O
may	O
use	O
the	O
empirical	B
estimates	O
lml	O
m	O
again	O
to	O
decide	O
which	O
value	O
of	O
m	O
we	O
wish	O
to	O
choose	O
however	O
as	O
m	O
gets	O
large-and	O
therefore	O
i	O
small-the	O
class	O
cm	O
will	O
tend	O
to	O
overfit	O
the	O
data	O
tz	O
providing	O
strongly	O
optimistically	O
biased	O
estimates	O
for	O
infpmecm	O
l	O
m	O
to	O
prevent	O
overfitting	B
we	O
may	O
apply	O
the	O
method	O
of	B
complexity	B
regularization	I
by	O
we	O
may	O
define	O
the	O
penalty	O
term	O
by	O
rem	O
i	O
logn	O
i	O
splitting	B
the	I
data	I
and	O
minimize	O
the	O
penalized	O
error	O
estimate	O
lml	O
lml	O
rem	O
l	O
over	O
all	O
e	O
ullcm	O
denote	O
the	O
selected	O
rule	B
by	O
prove	O
that	O
for	O
every	O
n	O
and	O
all	O
distributions	O
of	O
y	O
mm	O
m	O
l	O
inf	O
l	O
m	O
l	O
y	O
n	O
hint	O
proceed	O
similarly	O
to	O
the	O
proof	O
of	O
theorem	O
the	O
resubstitution	B
estimate	O
estimating	O
the	O
error	O
probability	O
is	O
of	O
primordial	O
importance	O
for	O
classifier	B
selection	I
the	O
method	O
explored	O
in	O
the	O
previous	O
chapter	O
attempts	O
to	O
solve	O
this	O
problem	O
by	O
using	O
a	O
testing	B
sequence	I
to	O
obtain	O
a	O
reliable	O
holdout	B
estimate	O
the	O
independence	O
of	O
testing	O
and	O
training	O
sequences	O
leads	O
to	O
a	O
rather	O
straightforward	O
analysis	O
for	O
a	O
good	O
performance	O
the	O
testing	B
sequence	I
has	O
to	O
be	O
sufficiently	O
large	O
we	O
often	O
get	O
away	O
with	O
testing	O
sequences	O
as	O
small	O
as	O
about	O
log	O
n	O
when	O
data	O
are	O
expensive	O
this	O
constitutes	O
a	O
waste	O
assume	O
that	O
we	O
do	O
not	O
split	O
the	O
data	O
and	O
use	O
the	O
same	O
sequence	O
for	O
testing	O
and	O
training	O
often	O
dangerous	O
this	O
strategy	O
nevertheless	O
works	O
if	O
the	O
class	O
of	O
rules	O
from	O
which	O
we	O
select	O
is	O
sufficiently	O
restricted	O
the	O
error	O
estimate	O
in	O
this	O
case	O
is	O
appropriately	O
called	O
the	O
resubstitution	B
estimate	O
and	O
it	O
will	O
be	O
denoted	O
by	O
lr	O
this	O
chapter	O
explores	O
its	O
virtues	O
and	O
pitfalls	O
a	O
third	O
error	O
estimate	O
the	O
deleted	O
estimate	O
is	O
discussed	O
in	O
the	O
next	O
chapter	O
estimates	O
based	O
upon	O
other	O
paradigms	O
are	O
treated	O
briefly	O
in	O
chapter	O
the	O
resubstitution	B
estimate	O
the	O
resubstitution	B
estimate	O
lr	O
counts	O
the	O
number	O
of	O
errors	O
committed	O
on	O
the	O
training	O
sequence	O
by	O
the	O
classification	O
rule	B
expressed	O
formally	O
sometimes	O
lr	O
is	O
called	O
the	O
apparent	O
error	O
rate	O
it	O
is	O
usually	O
strongly	O
optimisti	O
the	O
resubstitution	B
estimate	O
cally	O
biased	O
since	O
the	O
classifier	B
gn	O
is	O
tuned	O
by	O
dn	O
it	O
is	O
intuitively	O
clear	O
that	O
gn	O
may	O
behave	O
better	O
on	O
dn	O
than	O
on	O
independent	O
data	O
the	O
best	O
way	O
to	O
demonstrate	O
this	O
biasedness	O
is	O
to	O
consider	O
the	O
i-nearest	O
neigh	O
bor	O
rule	B
if	O
x	O
has	O
a	O
density	O
then	O
the	O
nearest	B
neighbor	I
of	O
xi	O
among	O
xl	O
xn	O
is	O
xi	O
itself	O
with	O
probability	O
one	O
therefore	O
lr	O
regardless	O
of	O
the	O
value	O
of	O
ln	O
lgn	O
in	O
this	O
case	O
the	O
resubstitution	B
estimate	O
is	O
useless	O
for	O
k-nearest	O
neigh	O
bor	O
rules	O
with	O
large	O
k	O
lr	O
is	O
close	O
to	O
ln	O
this	O
was	O
demonstrated	O
by	O
devroye	O
and	O
wagner	O
who	O
obtained	O
upper	O
bounds	O
on	O
the	O
performance	O
of	O
the	O
resubsti	O
tution	O
estimate	O
for	O
the	O
k-nearest	O
neighbor	O
rule	B
without	O
posing	O
any	O
assumption	O
on	O
the	O
distribution	O
also	O
if	O
the	O
classifier	B
whose	O
error	O
probability	O
is	O
to	O
be	O
estimated	O
is	O
a	O
member	O
of	B
a	I
class	I
of	I
classifiers	I
with	O
finite	O
vapnik-chervonenkis	B
dimension	B
the	O
definitions	O
in	O
chapter	O
then	O
we	O
can	O
get	O
good	O
performance	O
bounds	O
for	O
the	O
resubstitution	B
estimate	O
to	O
see	O
this	O
consider	O
any	O
generalized	B
linear	O
classification	O
rule	B
that	O
is	O
any	O
rule	B
that	O
can	O
be	O
put	O
into	O
the	O
following	O
form	O
nx	O
g	O
if	O
ao	O
n	O
otherwise	O
where	O
the	O
are	O
fixed	O
functions	O
and	O
the	O
coefficients	O
ain	O
depend	O
on	O
the	O
data	O
dn	O
in	O
an	O
arbitrary	O
but	O
measurable	O
way	O
we	O
have	O
the	O
following	O
estimate	O
for	O
the	O
performance	O
of	O
the	O
resubstitution	B
estimate	O
l	O
theorem	O
and	O
wagner	O
for	O
all	O
nand	O
e	O
the	O
substitution	O
estimate	O
l	O
of	O
the	O
error	O
probability	O
ln	O
of	O
a	O
generalized	B
linear	O
rule	B
satisfies	O
proof	O
define	O
the	O
set	O
an	O
c	O
rd	O
x	O
i	O
as	O
the	O
set	O
of	O
all	O
pairs	O
y	O
e	O
rd	O
x	O
on	O
which	O
gn	O
errs	O
an	O
y	O
gnx	O
y	O
observe	O
that	O
and	O
or	O
denoting	O
the	O
measure	O
of	O
y	O
by	O
and	O
the	O
corresponding	O
empirical	B
measure	I
by	O
ln	O
and	O
histogram	O
rules	O
the	O
set	O
an	O
depends	O
on	O
the	O
data	O
dn	O
so	O
that	O
for	O
example	O
e	O
vnan	O
e	O
van	O
fortunately	O
the	O
powerful	O
vapnik-chervonenkis	B
theory	O
comes	O
to	O
the	O
rescue	O
via	O
the	O
inequality	B
iln	O
lrissup	O
ivnc	O
vci	O
cec	O
where	O
c	O
is	O
the	O
family	B
of	I
all	O
sets	O
of	O
the	O
form	O
y	O
y	O
where	O
rd	O
is	O
a	O
generalized	B
linear	B
classifier	B
based	O
on	O
the	O
functions	O
d	O
by	O
theorems	O
and	O
we	O
have	O
p	O
ivnc	O
e	O
cec	O
similar	O
inequalities	O
can	O
be	O
obtained	O
for	O
other	O
classifiers	O
for	O
example	O
for	O
titioning	O
rules	O
with	O
fixed	O
partitions	O
we	O
have	O
the	O
following	O
theorem	O
let	O
gn	O
be	O
a	O
classifier	B
whose	O
value	O
is	O
constant	O
over	O
cells	O
of	O
a	O
fixed	O
partition	B
ofrd	O
into	O
k	O
cells	O
then	O
p	O
lnl	O
e	O
the	O
proof	O
is	O
left	O
as	O
an	O
exercise	O
from	O
theorems	O
and	O
we	O
get	O
bounds	O
for	O
the	O
expected	O
difference	O
between	O
the	O
resubstitution	B
estimate	O
and	O
the	O
actual	O
error	O
probability	O
ln	O
for	O
example	O
theorem	O
implies	O
problem	O
that	O
e	O
lnl	O
v	O
in	O
some	O
special	O
cases	O
the	O
expected	O
behavior	O
of	O
the	O
resubstitution	B
estimate	O
can	O
be	O
analyzed	O
in	O
more	O
detail	O
for	O
example	O
mclachlan	O
proved	O
that	O
if	O
the	O
conditional	O
distributions	O
of	O
x	O
given	O
y	O
and	O
y	O
are	O
both	O
normal	B
with	O
the	O
same	O
covariance	O
matrices	O
and	O
the	O
rule	B
is	O
linear	O
and	O
based	O
on	O
the	O
estimated	O
parameters	O
then	O
the	O
bias	B
of	I
the	O
estimate	O
is	O
of	O
the	O
order	O
n	O
mclachlan	O
also	O
showed	O
for	O
this	O
case	O
that	O
for	O
large	O
n	O
the	O
expected	O
value	O
of	O
the	O
substitution	O
estimate	O
is	O
smaller	O
than	O
that	O
of	O
l	O
n	O
that	O
is	O
the	O
estimate	O
is	O
optimistically	O
biased	O
as	O
expected	O
histogram	O
rules	O
in	O
this	O
section	O
we	O
explore	O
the	O
properties	O
of	O
the	O
resubstitution	B
estimate	O
for	O
togram	O
rules	O
let	O
p	O
az	O
be	O
a	O
fixed	O
partition	B
of	O
rd	O
and	O
let	O
gn	O
be	O
the	O
the	O
resubstitution	B
estimate	O
corresponding	O
histogram	O
classifier	B
chapters	O
and	O
introduce	O
the	O
notation	O
the	O
analysis	O
is	O
simplified	O
if	O
we	O
rewrite	O
the	O
estimate	O
lr	O
in	O
the	O
following	O
form	O
problem	O
it	O
is	O
also	O
interesting	O
to	O
observe	O
that	O
lr	O
can	O
be	O
put	O
in	O
the	O
following	O
form	O
lr	O
f	O
where	O
ljl	O
i	O
and	O
i	O
ljl	O
we	O
can	O
compare	O
this	O
form	O
with	O
the	O
following	O
expression	B
of	O
ln	O
ln	O
f	O
ln	O
problem	O
we	O
begin	O
the	O
analysis	O
of	O
performance	O
of	O
the	O
estimate	O
by	O
showing	O
that	O
its	O
mean	O
squared	B
error	I
is	O
not	O
larger	O
than	O
a	O
constant	O
times	O
the	O
number	O
of	O
cells	O
in	O
the	O
partition	B
over	O
n	O
theorem	O
for	O
any	O
distribution	O
of	O
y	O
andfor	O
all	O
n	O
the	O
estimate	O
lr	O
of	O
the	O
error	O
probability	O
of	O
any	O
histogram	B
rule	B
satisfies	O
also	O
the	O
estimate	O
is	O
optimistically	O
biased	O
that	O
is	O
if	O
in	O
addition	O
the	O
histogram	B
rule	B
is	O
based	O
on	O
a	O
partition	B
p	O
ofrd	O
into	O
at	O
most	O
k	O
cells	O
then	O
proof	O
the	O
first	O
inequality	B
is	O
an	O
immediate	O
consequence	O
of	O
theorem	O
and	O
introduce	O
the	O
auxiliary	O
quantity	O
r	O
lminvoaj	O
vi	O
where	O
voa	O
py	O
x	O
e	O
a	O
and	O
vi	O
py	O
x	O
e	O
a	O
we	O
use	O
the	O
decomposition	O
histogram	O
rules	O
first	O
we	O
bound	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
observe	O
that	O
r	O
is	O
just	O
the	O
bayes	B
error	I
corresponding	O
to	O
the	O
pair	O
of	O
random	O
variables	O
y	O
where	O
the	O
function	O
t	O
transforms	O
x	O
according	O
to	O
t	O
i	O
if	O
x	O
e	O
ai	O
since	O
the	O
histogram	O
classification	O
rule	B
gnx	O
can	O
be	O
written	O
as	O
a	O
function	O
of	O
tx	O
its	O
error	O
probability	O
ln	O
cannot	O
be	O
smaller	O
than	O
r	O
furthermore	O
by	O
the	O
first	O
identity	O
of	O
theorem	O
we	O
have	O
ln	O
r	O
l	O
isignvlnai-vonai	O
signvlai-voai	O
if	O
the	O
partition	B
has	O
at	O
most	O
k	O
cells	O
then	O
by	O
the	O
cauchy-schwarz	B
inequality	B
e	O
e	O
voa	O
vona	O
i	O
k	O
lvar	O
we	O
bound	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
as	O
we	O
have	O
seen	O
earlier	O
varl	O
lin	O
so	O
it	O
suffices	O
to	O
bound	O
ir	O
elri	O
by	O
jensens	O
inequality	B
i	O
min	O
i	O
evl	O
nai	O
i	O
r	O
the	O
resubstitution	B
estimate	O
so	O
we	O
bound	O
r	O
elr	O
from	O
above	O
by	O
the	O
inequality	B
imina	O
b	O
mince	O
la	O
el	O
ib	O
dl	O
we	O
have	O
r	O
el	O
le	O
ivla	O
i	O
vinadl	O
l	O
i	O
the	O
cauchy-schwarz	B
inequality	B
voai	O
voai	O
viai	O
vi	O
voain	O
viai-	O
via	O
i	O
tila	O
i	O
where	O
we	O
used	O
the	O
elementary	O
inequality	B
fa	O
b	O
therefore	O
elr	O
to	O
complete	O
the	O
proof	O
of	O
the	O
third	O
inequality	B
observe	O
that	O
if	O
there	O
are	O
at	O
most	O
k	O
cells	O
then	O
k	O
k	O
l	O
n	O
jensens	O
inequality	B
therefore	O
finally	O
eln	O
elr	O
n	O
r	O
e	O
o	O
we	O
see	O
that	O
if	O
the	O
partition	B
contains	O
a	O
small	O
number	O
of	O
cells	O
then	O
the	O
tution	O
estimate	O
performs	O
very	O
nicely	O
however	O
if	O
the	O
partition	B
has	O
a	O
large	O
number	O
of	O
cells	O
then	O
the	O
resubstitution	B
estimate	O
of	O
ln	O
can	O
be	O
very	O
misleading	O
as	O
the	O
next	O
result	O
indicates	O
data-based	B
histograms	O
and	O
rule	B
selection	O
theorem	O
and	O
wagner	O
for	O
every	O
n	O
there	O
exists	O
a	O
titioning	O
rule	B
and	O
a	O
distribution	O
such	O
that	O
proof	O
let	O
ai	O
be	O
cells	O
of	O
the	O
partition	B
such	O
that	O
flaj	O
i	O
assume	O
further	O
that	O
for	O
every	O
x	O
e	O
n	O
d	O
that	O
is	O
y	O
with	O
probability	O
one	O
then	O
clearly	O
lr	O
o	O
on	O
the	O
other	O
hand	O
if	O
a	O
cell	O
does	O
not	O
contain	O
any	O
of	O
the	O
data	O
points	O
xl	O
x	O
n	O
then	O
gnx	O
in	O
that	O
cell	O
but	O
since	O
the	O
number	O
of	O
points	O
is	O
only	O
half	O
of	O
the	O
number	O
of	O
cells	O
at	O
least	O
half	O
of	O
the	O
cells	O
are	O
empty	O
therefore	O
ln	O
and	O
ilr	O
ln	O
i	O
this	O
concludes	O
the	O
proof	O
o	O
data-based	B
histograms	O
and	O
rule	B
selection	O
theorem	O
demonstrates	O
the	O
usefulness	O
of	O
lr	O
for	O
histogram	O
rules	O
with	O
a	O
fixed	O
partition	B
provided	O
that	O
the	O
number	O
of	O
cells	O
in	O
the	O
partition	B
is	O
not	O
too	O
large	O
if	O
we	O
want	O
to	O
use	O
lr	O
to	O
select	O
a	O
good	O
classifier	B
the	O
estimate	O
should	O
work	O
uniformly	O
well	O
over	O
the	O
class	O
from	O
which	O
we	O
select	O
a	O
rule	B
in	O
this	O
section	O
we	O
explore	O
such	O
data-based	B
histogram	O
rules	O
let	O
f	O
be	O
a	O
class	O
of	O
partitions	O
of	O
nd	O
we	O
will	O
assume	O
that	O
each	O
member	O
of	O
f	O
titions	O
n	O
d	O
into	O
at	O
most	O
k	O
cells	O
for	O
each	O
partition	B
p	O
e	O
f	O
define	O
the	O
corresponding	O
histogram	B
rule	B
by	O
if	O
iylxeax	O
iyoxeax	O
otherwise	O
where	O
ax	O
is	O
the	O
cell	O
of	O
p	O
that	O
contains	O
x	O
denote	O
the	O
error	O
probability	O
of	O
gpx	O
by	O
the	O
corresponding	O
error	O
estimate	O
is	O
denoted	O
by	O
l	O
p	O
l	O
minvona	O
vlna	O
aep	O
by	O
analogy	O
with	O
theorems	O
and	O
we	O
can	O
derive	O
the	O
following	O
result	O
which	O
gives	O
a	O
useful	O
bound	O
for	O
the	O
largest	O
difference	O
between	O
the	O
estimate	O
and	O
the	O
error	O
probability	O
within	O
the	O
class	O
of	O
histogram	O
classifiers	O
defined	O
by	O
f	O
the	O
combinatorial	O
coefficient	O
defined	O
in	O
chapter	O
appears	O
as	O
a	O
coefficient	O
in	O
the	O
upper	O
bound	O
the	O
computation	O
of	O
for	O
several	O
different	O
classes	O
of	O
partitions	O
is	O
illustrated	O
in	O
chapter	O
the	O
resubstitution	B
estimate	O
theorem	O
assume	O
that	O
each	O
member	O
of	O
f	O
partitions	O
nd	O
into	O
at	O
most	O
k	O
cells	O
for	O
every	O
nand	O
e	O
psup	O
ilrp	O
e	O
pef	O
proof	O
we	O
can	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
the	O
shatter	B
coefficient	I
sc	O
n	O
corresponding	O
to	O
the	O
class	O
of	O
histogram	O
classifiers	O
defined	O
by	O
partitions	O
in	O
f	O
can	O
clearly	O
be	O
bounded	O
from	O
above	O
by	O
the	O
number	O
of	O
different	O
ways	O
in	O
which	O
n	O
points	O
can	O
be	O
partitioned	O
by	O
members	O
of	O
f	O
times	O
as	O
there	O
are	O
at	O
most	O
different	O
ways	O
to	O
assign	O
labels	O
to	O
cells	O
of	O
a	O
partition	B
of	O
at	O
most	O
k	O
cells	O
the	O
theorem	O
has	O
two	O
interesting	O
implications	O
the	O
error	O
estimate	O
lr	O
can	O
also	O
be	O
used	O
to	O
estimate	O
the	O
perfolmance	O
of	O
histogram	O
rules	O
based	O
on	O
data-dependent	B
partitions	O
chapter	O
the	O
argument	O
ofthe	O
proof	O
of	O
theorem	O
is	O
not	O
valid	O
for	O
these	O
rules	O
however	O
theorem	O
provides	O
performance	O
guarantees	O
for	O
these	O
rules	O
in	O
the	O
following	O
corollaries	O
corollary	O
let	O
gnx	O
be	O
a	O
histogram	O
classifier	B
based	O
on	O
a	O
random	O
partition	B
pn	O
into	O
at	O
most	O
k	O
cells	O
which	O
is	O
determined	O
by	O
the	O
data	O
dn	O
assume	O
that	O
for	O
any	O
possible	O
realization	O
ot	O
the	O
training	O
data	O
dn	O
the	O
partition	B
p	O
n	O
is	O
a	O
member	O
of	O
a	O
class	O
of	O
partitions	O
f	O
if	O
ln	O
is	O
the	O
error	O
probability	O
of	O
gn	O
then	O
p	O
i	O
l	O
l	O
i	O
e	O
tj	O
n	O
n	O
n	O
and	O
e	O
ln	O
ln	O
n	O
proof	O
the	O
first	O
inequality	B
follows	O
from	O
theorem	O
by	O
the	O
obvious	O
inequality	B
p	O
ln	O
i	O
e	O
p	O
sup	O
ilrp	O
e	O
pef	O
the	O
second	O
inequality	B
follows	O
from	O
the	O
first	O
one	O
via	O
problem	O
perhaps	O
the	O
most	O
important	O
application	O
of	O
theorem	O
is	O
in	O
classifier	B
selection	I
let	O
cn	O
be	O
a	O
class	O
of	O
data-dependent	B
histogram	O
rules	O
we	O
may	O
use	O
the	O
error	O
estimate	O
lr	O
to	O
select	O
a	O
classifier	B
that	O
minimizes	O
the	O
estimated	O
error	O
probability	O
denote	O
the	O
selected	O
histogram	B
rule	B
by	O
that	O
is	O
lr	O
lr	O
n	O
for	O
all	O
n	O
e	O
cn	O
here	O
lr	O
n	O
denotes	O
the	O
estimated	O
error	O
probability	O
of	O
the	O
classification	O
rule	B
in	O
the	O
question	O
is	O
how	O
well	O
the	O
selection	O
method	O
works	O
in	O
other	O
words	O
how	O
close	O
the	O
error	O
probability	O
of	O
the	O
selected	O
classifier	B
l	O
is	O
to	O
the	O
error	O
probability	O
of	O
the	O
best	O
rule	B
in	O
the	O
class	O
infcpnecn	O
l	O
n	O
it	O
turns	O
out	O
that	O
if	O
the	O
possible	O
partitions	O
are	O
not	O
too	O
complex	O
then	O
the	O
method	O
works	O
very	O
well	O
problems	O
and	O
exercises	O
corollary	O
assume	O
that	O
jor	O
any	O
realization	O
oj	O
the	O
data	O
dn	O
the	O
possible	O
partitions	O
that	O
define	O
the	O
histogram	O
classifiers	O
in	O
en	O
belong	O
to	O
a	O
class	O
ojpartitions	O
f	O
whose	O
members	O
partition	B
nd	O
into	O
at	O
most	O
k	O
cells	O
then	O
p	O
l	O
z	O
l	O
n	O
e	O
proof	O
by	O
lemma	O
l	O
inf	O
l	O
n	O
sup	O
ilr	O
n	O
l	O
sup	O
ilrp	O
lpi	O
therefore	O
the	O
statement	O
follows	O
from	O
theorem	O
problems	O
and	O
exercises	O
problem	O
prove	O
theorem	O
hint	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
problem	O
show	O
that	O
for	O
histogram	O
rules	O
the	O
resubstitution	B
estimate	O
may	O
be	O
written	O
as	O
problem	O
consider	O
the	O
resubstitution	B
estimate	O
lr	O
of	O
the	O
error	O
probability	O
ln	O
of	O
a	O
histogram	B
rule	B
based	O
on	O
a	O
fixed	O
sequence	O
of	O
partitions	O
pn	O
show	O
that	O
if	O
the	O
regression	B
function	I
estimate	O
ljjeax	O
is	O
consistent	O
that	O
is	O
it	O
satisfies	O
e	O
as	O
n	O
then	O
lim	O
e	O
lnll	O
n-oo	O
andalsoelr	O
l	O
problem	O
let	O
y	O
yl	O
be	O
a	O
sequence	O
that	O
depends	O
in	O
an	O
arbitrary	O
ion	O
on	O
the	O
data	O
din	O
and	O
let	O
gn	O
be	O
the	O
nearest	B
neighbor	I
rule	B
with	O
y	O
y	O
where	O
m	O
is	O
fixed	O
let	O
lr	O
denote	O
the	O
resubstitution	B
estimate	O
of	O
ln	O
lgn	O
show	O
that	O
for	O
all	O
e	O
and	O
all	O
distributions	O
p	O
li	O
e	O
problem	O
find	O
a	O
rule	B
for	O
y	O
e	O
r	O
x	O
i	O
such	O
that	O
for	O
all	O
nonatomic	O
distributions	O
with	O
l	O
we	O
have	O
eln	O
yete	O
eln	O
lr	O
maybe	O
pessimistically	O
biased	O
even	O
for	O
a	O
consistent	O
rule	B
problem	O
for	O
histogram	O
rules	O
on	O
fixed	O
partitions	O
that	O
do	O
not	O
change	O
with	O
n	O
and	O
are	O
independent	O
of	O
the	O
data	O
show	O
that	O
e	O
is	O
monotonically	O
nonincreasing	O
problem	O
assume	O
that	O
x	O
has	O
a	O
density	O
and	O
investigate	O
the	O
resubstitution	B
estimate	O
of	O
the	O
rule	B
what	O
is	O
the	O
limit	O
of	O
e	O
as	O
n	O
oo	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
the	O
deleted	O
estimate	O
called	O
cross-validation	B
leave-one-out	B
or	O
u-method	O
attempts	O
to	O
avoid	O
the	O
bias	B
present	O
in	O
the	O
resubstitution	B
estimate	O
proposed	O
and	O
developed	O
by	O
lunts	O
and	O
brailovsky	O
lachenbruch	O
cover	O
and	O
stone	O
the	O
method	O
deletes	O
the	O
first	O
pair	O
yl	O
from	O
the	O
training	O
data	O
and	O
makes	O
a	O
decision	O
gn-l	O
using	O
the	O
remaining	O
n	O
pairs	O
it	O
tests	O
for	O
an	O
an	O
error	O
on	O
yl	O
and	O
repeats	O
this	O
procedure	O
for	O
all	O
n	O
pairs	O
of	O
the	O
training	O
data	O
dn	O
the	O
estimate	O
ld	O
is	O
the	O
average	O
the	O
number	O
of	O
errors	O
we	O
formally	O
denote	O
the	O
training	O
set	O
with	O
yj	O
deleted	O
by	O
then	O
we	O
define	O
clearly	O
the	O
deleted	O
estimate	O
is	O
almost	O
unbiased	O
in	O
the	O
sense	O
that	O
thus	O
ld	O
should	O
be	O
viewed	O
as	O
an	O
estimator	O
of	O
ln-	O
l	O
rather	O
than	O
of	O
ln	O
in	O
most	O
of	O
the	O
interesting	O
cases	O
ln	O
converges	O
with	O
probability	O
one	O
so	O
that	O
the	O
difference	O
between	O
ln-	O
l	O
and	O
ln	O
becomes	O
negligible	O
for	O
large	O
n	O
the	O
designer	O
has	O
the	O
luxury	O
of	O
being	O
able	O
to	O
pick	O
the	O
most	O
convenient	O
gn-l	O
in	O
some	O
cases	O
the	O
choice	O
is	O
very	O
natural	O
in	O
other	O
cases	O
it	O
is	O
not	O
for	O
example	O
if	O
gn	O
is	O
the	O
i-nearest	O
neighbor	O
rule	B
then	O
letting	O
gn-l	O
be	O
the	O
i-nearest	O
neighbor	O
rule	B
based	O
on	O
n	O
data	O
pairs	O
seems	O
to	O
be	O
an	O
obvious	O
choice	O
we	O
will	O
see	O
later	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
that	O
this	O
indeed	O
yields	O
an	O
extremely	O
good	O
estimator	O
but	O
what	O
should	O
gn-l	O
be	O
if	O
gn	O
is	O
for	O
example	O
a	O
k-nn	B
rule	B
should	O
it	O
be	O
the	O
k-nearest	O
neighbor	O
classifier	B
the	O
i-nearest	O
neighbor	O
classifier	B
or	O
maybe	O
something	O
else	O
well	O
the	O
choice	O
is	O
k	O
typically	O
nontrivial	O
and	O
needs	O
careful	O
attention	O
if	O
the	O
designer	O
wants	O
a	O
distribution	O
free	O
performance	O
guarantee	O
for	O
the	O
resulting	O
estimate	O
because	O
of	O
the	O
variety	O
of	O
choices	O
for	O
gn-l	O
we	O
should	O
not	O
speak	O
of	O
the	O
deleted	O
estimate	O
but	O
rather	O
of	O
a	O
deleted	O
estimate	O
in	O
this	O
chapter	O
we	O
analyze	O
the	O
performance	O
of	O
deleted	O
estimates	O
for	O
a	O
few	O
type	O
classifiers	O
such	O
as	O
the	O
kernel	B
nearest	B
neighbor	I
and	O
histogram	O
rules	O
in	O
most	O
cases	O
studied	O
here	O
deleted	O
estimates	O
have	O
good	O
distribution-free	O
properties	O
a	O
general	O
lower	O
bound	O
nonparametric	O
rules	O
an	O
error	O
estimate	O
ln	O
is	O
merely	O
a	O
function	O
x	O
lr	O
we	O
begin	O
by	O
exploring	O
general	O
limitations	O
of	O
error	O
estimates	O
for	O
some	O
important	O
which	O
is	O
applied	O
to	O
the	O
data	O
dn	O
i	O
yi	O
yn	O
theorem	O
let	O
gn	O
be	O
one	O
of	O
the	O
following	O
classifiers	O
the	O
kernel	B
rule	B
gnx	O
if	O
i	O
i	O
l	O
dl	O
otherwise	O
k	O
i	O
l	O
il	O
k	O
with	O
a	O
nonnegative	O
kernel	B
k	O
of	O
compact	O
support	B
and	O
smoothing	B
factor	I
h	O
the	O
histogram	B
rule	B
iyilixieax	O
iyioixieax	O
gil	O
otherwise	O
based	O
on	O
a	O
fixed	O
partition	B
p	O
a	O
containing	O
at	O
least	O
n	O
cells	O
the	O
lazy	B
histogram	B
rule	B
where	O
xj	O
is	O
the	O
minimum-index	O
point	O
among	O
xl	O
xnforwhich	O
xj	O
e	O
ai	O
wherep	O
a	O
isajixedpartition	O
containing	O
atleastn	O
cells	O
denote	O
the	O
probability	O
of	O
error	O
for	O
gil	O
by	O
ln	O
pgnx	O
dn	O
ydn	O
thenor	O
with	O
l	O
such	O
that	O
every	O
n	O
andor	O
every	O
error	O
estimatorln	O
there	O
exists	O
a	O
distribution	O
ox	O
y	O
the	O
theorem	O
says	O
that	O
for	O
any	O
estimate	O
ln	O
there	O
exists	O
a	O
distribution	O
with	O
the	O
property	O
thate	O
ln	O
i	O
for	O
the	O
rules	O
gn	O
given	O
in	O
the	O
theorem	O
a	O
general	O
lower	O
bound	O
no	O
error	O
estimate	O
can	O
possibly	O
give	O
better	O
distribution-free	O
performance	O
guarantees	O
error	O
estimates	O
are	O
necessarily	O
going	O
to	O
perform	O
at	O
least	O
this	O
poorly	O
for	O
some	O
distributions	O
proof	O
of	O
theorem	O
let	O
ln	O
be	O
an	O
arbitrary	O
fixed	O
error	O
estimate	O
the	O
proof	O
relies	O
on	O
randomization	O
ideas	O
similar	O
to	O
those	O
of	O
the	O
proofs	O
of	O
theorems	O
and	O
we	O
construct	O
a	O
family	B
of	I
distributions	O
forcx	O
y	O
forb	O
e	O
be	O
its	O
binary	B
expansion	O
in	O
all	O
cases	O
the	O
distribution	O
of	O
x	O
is	O
uniform	O
on	O
n	O
points	O
x	O
n	O
for	O
the	O
histogram	O
and	O
lazy	B
histogram	O
rules	O
choose	O
xl	O
xn	O
such	O
that	O
they	O
fall	O
into	O
different	O
cells	O
for	O
the	O
kernel	B
rule	B
choose	O
xl	O
xn	O
so	O
that	O
they	O
are	O
isolated	O
from	O
each	O
other	O
that	O
is	O
k	O
cxixj	O
for	O
all	O
i	O
j	O
e	O
n	O
i	O
j	O
to	O
simplify	O
the	O
notation	O
we	O
will	O
refer	O
to	O
these	O
points	O
by	O
their	O
indices	O
that	O
is	O
we	O
will	O
write	O
x	O
i	O
instead	O
of	O
x	O
xi	O
for	O
a	O
fixed	O
b	O
define	O
y	O
we	O
may	O
create	O
infinitely	O
many	O
samples	O
cone	O
for	O
each	O
b	O
e	O
drawn	O
from	O
the	O
distribution	O
of	O
cx	O
y	O
as	O
follows	O
let	O
x	O
i	O
xn	O
be	O
i	O
i	O
d	O
and	O
uniformly	O
distributed	O
on	O
n	O
all	O
the	O
samples	O
share	O
the	O
same	O
xl	O
x	O
n	O
but	O
differ	O
in	O
their	O
ys	O
for	O
given	O
xi	O
define	O
yi	O
b	O
xi	O
write	O
zn	O
cx	O
xn	O
and	O
ni	O
ixji	O
observe	O
that	O
dn	O
is	O
a	O
function	O
of	O
zn	O
and	O
b	O
it	O
is	O
clear	O
that	O
for	O
all	O
classifiers	O
covered	O
by	O
our	O
assumptions	O
for	O
a	O
fixed	O
b	O
where	O
s	O
i	O
n	O
ni	O
o	O
is	O
the	O
set	O
of	O
empty	O
bins	O
we	O
randomize	O
the	O
distribution	O
as	O
follows	O
let	O
b	O
be	O
a	O
uniform	O
random	O
variable	B
independent	O
of	O
cx	O
y	O
then	O
clearly	O
b	O
i	O
b	O
are	O
independent	O
uniform	O
binary	B
random	O
variables	O
note	O
that	O
supeilncdn-lnl	O
eilncdn-lnl	O
b	O
cwhere	O
b	O
is	O
replaced	O
by	O
b	O
e	O
ilncdn	O
ln	O
ii	O
zn	O
in	O
what	O
follows	O
we	O
bound	O
the	O
conditional	O
expectation	O
within	O
the	O
brackets	O
to	O
make	O
the	O
dependence	O
upon	O
b	O
explicit	O
we	O
write	O
lnczn	O
b	O
lncdn	O
and	O
lncb	O
ln	O
thus	O
e	O
ilnzn	O
b	O
zn	O
b	O
ilnczn	O
b	O
zn	O
b	O
bi	O
for	O
i	O
with	O
ni	O
and	O
b	O
bi	O
for	O
i	O
with	O
ni	O
zn	O
lnzn	O
b	O
lnczn	O
b	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
the	O
expression	B
for	O
ln	O
given	O
above	O
lsi	O
bisi	O
is	O
a	O
binomial	B
random	O
variable	B
fist	O
khintchines	O
inequality	B
see	O
lemma	O
a	O
s	O
in	O
summary	O
we	O
have	O
we	O
have	O
only	O
to	O
bound	O
the	O
right-hand	O
side	O
we	O
apply	O
lemma	O
aa	O
to	O
the	O
random	O
variable	B
jist	O
clearly	O
eisi	O
no	O
lnn	O
and	O
e	O
lnioj	O
l	O
lnionioj	O
il	O
ij	O
eisi	O
nn	O
e	O
so	O
that	O
in	O
nn	O
vn----	O
i	O
the	O
proof	O
is	O
now	O
complete	O
a	O
general	O
upper	O
bound	O
for	O
deleted	O
estimates	O
a	O
general	O
upper	O
bound	O
for	O
deleted	O
estimates	O
the	O
following	O
inequality	B
is	O
a	O
general	O
tool	O
for	O
obtaining	O
distribution-free	O
upper	O
bounds	O
for	O
the	O
difference	O
between	O
the	O
deleted	O
estimate	O
and	O
the	O
true	O
error	O
probability	O
ln	O
theorem	O
and	O
wagner	O
devroye	O
and	O
wagner	O
assume	O
that	O
gn	O
is	O
a	O
symmetric	O
classifier	B
that	O
is	O
gnx	O
dn	O
gnx	O
d	O
where	O
d	O
is	O
obtained	O
by	O
permuting	O
the	O
pairs	O
of	O
dn	O
arbitrarily	O
then	O
proof	O
first	O
we	O
express	O
the	O
three	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
first	O
term	O
can	O
be	O
bounded	O
by	O
using	O
symmetry	O
of	O
gn	O
by	O
e	O
e	O
jg	O
y	O
r	O
e	O
t	O
ign	O
lxidni	O
yd	O
n	O
il	O
the	O
second	O
term	O
is	O
written	O
as	O
e	O
ln	O
e	O
t	O
ign-lxidili	O
yd	O
n	O
il	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
n	O
le	O
dn	O
y	O
gn-lxi	O
dnj	O
yiidn	O
n	O
il	O
pgnx	O
dn	O
y	O
gn-ixl	O
dnl	O
yd	O
for	O
the	O
third	O
term	O
we	O
introduce	O
the	O
pair	O
yl	O
independent	O
of	O
x	O
y	O
and	O
d	O
n	O
having	O
the	O
same	O
distribution	O
as	O
y	O
then	O
el	O
epgnxdn	O
yidnf	O
e	O
dn	O
yidnpgnxi	O
dn	O
yiidn	O
e	O
dn	O
y	O
gnxi	O
dn	O
yiidn	O
where	O
we	O
used	O
independence	O
of	O
yl	O
we	O
introduce	O
the	O
notation	O
d	O
akij	O
yn	O
yi	O
yj	O
d	O
yd	O
bki	O
yd	O
and	O
we	O
formally	O
replace	O
y	O
and	O
yl	O
by	O
ya	O
and	O
so	O
that	O
we	O
may	O
work	O
with	O
the	O
indices	O
a	O
and	O
with	O
this	O
notation	O
we	O
have	O
shown	O
thus	O
far	O
the	O
following	O
e	O
l	O
n	O
n	O
n	O
note	O
that	O
also	O
pa	O
a	O
pa	O
a	O
symmetry	O
i	O
ii	O
pba	O
symmetry	O
nearest	B
neighbor	I
rules	O
pbatl	O
btl	O
a	O
btla	O
btla	O
iiiiv	O
using	O
the	O
fact	O
that	O
for	O
events	O
ipci	O
c	O
j	O
pci	O
pcj	O
we	O
bound	O
i	O
def	O
ii	O
v	O
iii	O
v	O
iv	O
pbtl	O
the	O
upper	O
bounds	O
for	O
ii	O
and	O
iii	O
are	O
identical	O
by	O
symmetry	O
also	O
and	O
iv	O
for	O
a	O
grand	O
total	O
of	O
v	O
this	O
concludes	O
the	O
proof	O
d	O
nearest	B
neighbor	I
rules	O
theorem	O
can	O
be	O
used	O
to	O
obtain	O
distribution-free	O
upper	O
bounds	O
for	O
specific	O
rules	O
here	O
is	O
the	O
most	O
important	O
example	O
theorem	O
and	O
wagner	O
let	O
gn	O
be	O
the	O
k-nearest	O
neighbor	O
rule	B
with	O
randomized	B
tie-breaking	O
if	O
ld	O
is	O
the	O
deleted	O
estimate	O
with	O
gn-l	O
chosen	O
as	O
the	O
k-nn	B
rule	B
the	O
same	O
k	O
and	O
with	O
the	O
same	O
randomizing	O
random	O
variables	O
then	O
e	O
ln	O
n	O
proof	O
because	O
of	O
the	O
randomized	B
tie-breaking	O
the	O
k-nn	B
rule	B
is	O
symmetric	O
and	O
theorem	O
is	O
applicable	O
we	O
only	O
have	O
to	O
show	O
that	O
clearly	O
gnx	O
dn	O
gn-l	O
dn-	O
l	O
can	O
happen	O
only	O
if	O
xn	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
but	O
the	O
probability	O
of	O
this	O
event	O
is	O
just	O
kin	O
since	O
by	O
symmetry	O
all	O
points	O
are	O
equally	O
likely	O
to	O
be	O
among	O
the	O
k	O
nearest	O
neighbors	O
d	O
deleted	O
estimates	O
ofthe	O
error	O
probability	O
remark	O
if	O
gn	O
is	O
the	O
k-nn	B
rule	B
such	O
that	O
distance	O
ties	O
are	O
broken	O
by	O
compaling	O
indices	O
then	O
gn	O
is	O
not	O
symmetric	O
and	O
theorem	O
is	O
no	O
longer	O
applicable	O
e	O
g	O
x	O
has	O
a	O
density	O
another	O
non	O
symmetric	O
classifier	B
is	O
the	O
lazy	B
histogram	B
rule	B
o	O
remark	O
applying	O
theorem	O
to	O
the	O
i-nn	B
rule	B
the	O
cauchy-schwarz	B
inequality	B
implies	O
e	O
i	O
ld	O
ln	O
i	O
n	O
for	O
all	O
distributions	O
remark	O
clearly	O
the	O
inequality	B
of	O
theorem	O
holds	O
for	O
any	O
rule	B
that	O
is	O
some	O
function	O
of	O
the	O
k	O
nearest	O
points	O
for	O
the	O
k-nearest	O
neighbor	O
rule	B
with	O
a	O
more	O
careful	O
analysis	O
devroye	O
and	O
wagner	O
improved	O
theorem	O
to	O
e	O
l	O
n	O
problem	O
n	O
n	O
nhii	O
probability	O
inequalities	O
for	O
ild	O
ln	O
i	O
can	O
also	O
be	O
obtained	O
with	O
further	O
work	O
by	O
chebyshevs	O
inequality	B
we	O
immediately	O
get	O
so	O
that	O
the	O
above	O
bounds	O
on	O
the	O
expected	O
squared	B
error	I
can	O
be	O
used	O
sharper	O
distribution-free	O
inequalities	O
were	O
obtained	O
by	O
devroye	O
and	O
wagner	O
for	O
several	O
nonparametric	O
rules	O
here	O
we	O
present	O
a	O
result	O
that	O
follows	O
immediately	O
from	O
what	O
we	O
have	O
already	O
seen	O
theorem	O
consider	O
the	O
k-nearest	O
neighbor	O
rule	B
with	O
randomized	B
ing	O
if	O
ld	O
is	O
the	O
deleted	O
estimate	O
with	O
gn-l	O
chosen	O
as	O
the	O
k-nn	B
rule	B
with	O
the	O
same	O
tie-breaking	O
then	O
proof	O
the	O
result	O
follows	O
immediately	O
from	O
mcdiarmids	O
inequality	B
by	O
the	O
lowing	O
argument	O
from	O
lemma	O
given	O
n	O
points	O
in	O
nd	O
a	O
particular	O
point	O
can	O
be	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
at	O
most	O
kyd	O
points	O
to	O
see	O
this	O
just	O
set	O
fjv	O
equal	O
to	O
the	O
empirical	B
measure	I
of	O
the	O
n	O
points	O
in	O
lemma	O
therefore	O
changing	O
the	O
value	O
of	O
one	O
pair	O
from	O
the	O
training	O
data	O
can	O
change	O
the	O
value	O
of	O
the	O
estimate	O
by	O
at	O
most	O
now	O
since	O
eld	O
eln-l	O
theorem	O
yields	O
the	O
result	O
exponential	B
upper	O
bounds	O
for	O
the	O
probability	O
pild	O
ln	O
i	O
e	O
are	O
typically	O
much	O
harder	O
to	O
obtain	O
we	O
mention	O
one	O
result	O
without	O
proof	O
kernel	B
rules	O
theorem	O
and	O
wagner	O
for	O
the	O
k-nearest	O
neighbor	O
rule	B
pild	O
lnl	O
e	O
one	O
of	O
the	O
drawbacks	O
of	O
the	O
deleted	O
estimate	O
is	O
that	O
it	O
requires	O
much	O
more	O
computation	O
than	O
the	O
resubstitution	B
estimate	O
if	O
conditional	O
on	O
y	O
and	O
y	O
x	O
is	O
gaussian	B
and	O
the	O
classification	O
rule	B
is	O
the	O
appropriate	O
parametric	O
rule	B
then	O
the	O
estimate	O
can	O
be	O
computed	O
quickly	O
see	O
lachenbruch	O
and	O
mickey	O
fukunaga	O
and	O
kessel	O
and	O
mclachlan	O
for	O
further	O
references	O
another	O
and	O
probably	O
more	O
serious	O
disadvantage	O
of	O
the	O
deleted	O
estimate	O
is	O
its	O
large	O
variance	O
this	O
fact	O
can	O
be	O
illustrated	O
by	O
the	O
following	O
example	O
from	O
devroye	O
and	O
wagner	O
let	O
n	O
be	O
even	O
and	O
let	O
the	O
distribution	O
of	O
y	O
be	O
such	O
that	O
y	O
is	O
independent	O
of	O
x	O
with	O
py	O
o	O
py	O
i	O
consider	O
the	O
k-nearest	O
neighbor	O
rule	B
with	O
k	O
n	O
then	O
obviously	O
ln	O
clearly	O
if	O
the	O
number	O
of	O
zeros	O
and	O
ones	O
among	O
the	O
labels	O
yn	O
are	O
equal	O
then	O
ld	O
thus	O
for	O
e	O
pil	O
lnl	O
e	O
p	O
iyll	O
by	O
stirlings	O
formula	O
we	O
have	O
piln	O
lnl	O
e	O
v	O
e	O
therefore	O
for	O
this	O
simple	O
rule	B
and	O
certain	O
distributions	O
the	O
probability	O
above	O
can	O
not	O
decrease	O
to	O
zero	O
faster	O
than	O
note	O
that	O
in	O
the	O
example	O
above	O
eld	O
so	O
the	O
lower	O
bound	O
holds	O
for	O
pild	O
eldi	O
e	O
as	O
well	O
also	O
in	O
this	O
example	O
we	O
have	O
e	O
el	O
lyo	O
e	O
il	O
in	O
chapter	O
we	O
describe	O
other	O
estimates	O
with	O
much	O
smaller	O
variances	O
kernel	B
rules	O
theorem	O
may	O
also	O
be	O
used	O
to	O
obtain	O
tight	O
distribution-free	O
upper	O
bounds	O
for	O
the	O
performance	O
of	O
the	O
deleted	O
estimate	O
of	O
the	O
error	O
probability	O
of	B
kernel	B
rules	I
we	O
have	O
the	O
following	O
bound	O
theorem	O
assume	O
that	O
k	O
is	O
a	O
regular	B
kernel	B
of	O
bounded	O
support	B
that	O
is	O
it	O
is	O
a	O
function	O
satisfying	O
kx	O
kx	O
b	O
kx	O
iixlls	O
p	O
ilxll	O
r	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
for	O
some	O
positive	O
finite	O
constants	O
p	O
band	O
r	O
let	O
the	O
kernel	B
rule	B
be	O
defined	O
by	O
iyiok	O
xi	O
iyilk	O
xi	O
gn	O
otherwise	O
and	O
define	O
similarly	O
then	O
there	O
exist	O
constants	O
depending	O
upon	O
d	O
only	O
and	O
depending	O
upon	O
k	O
only	O
such	O
that	O
for	O
all	O
n	O
one	O
may	O
take	O
rlpd	O
b	O
i	O
remark	O
since	O
is	O
a	O
scale-invariant	O
factor	O
the	O
theorem	O
applies	O
to	O
the	O
rule	B
with	O
ku	O
replaced	O
by	O
khu	O
tk	O
for	O
any	O
smoothing	B
factor	I
as	O
itis	O
is	O
minimal	O
and	O
equal	O
to	O
if	O
we	O
let	O
k	O
be	O
the	O
uniform	O
kernel	B
on	O
the	O
unit	O
ball	O
p	O
b	O
the	O
assumptions	O
of	O
the	O
theorem	O
require	O
that	O
gn-l	O
is	O
defined	O
with	O
the	O
same	O
kernel	B
and	O
smoothing	B
factor	I
as	O
gn	O
remark	O
the	O
theorem	O
applies	O
to	O
virtually	O
any	O
kernel	B
of	O
compact	O
support	B
that	O
is	O
of	O
interest	O
to	O
the	O
practitioners	O
note	O
however	O
that	O
the	O
gaussian	B
kernel	B
is	O
not	O
covered	O
by	O
the	O
result	O
the	O
theorem	O
generalizes	O
an	O
earlier	O
result	O
of	O
devroye	O
and	O
wagner	O
in	O
which	O
a	O
more	O
restricted	O
class	O
of	O
kernels	O
was	O
considered	O
they	O
showed	O
that	O
if	O
k	O
is	O
the	O
uniform	O
kernel	B
then	O
cd	O
e	O
ln	O
ln	O
jfi	O
see	O
problem	O
we	O
need	O
the	O
following	O
auxiliary	O
inequality	B
which	O
we	O
quote	O
without	O
proof	O
lemma	O
let	O
zl	O
zn	O
be	O
real-valuedi	O
i	O
d	O
random	O
variables	O
for	O
e	O
where	O
c	O
is	O
a	O
universal	O
constant	O
corollary	O
let	O
zl	O
zn	O
be	O
real-valued	O
i	O
i	O
d	O
random	O
variables	O
fore	O
a	O
i	O
e	O
ce	O
z	O
p	O
n	O
i	O
jfi	O
a	O
z	O
where	O
c	O
is	O
a	O
universal	O
constant	O
proof	O
of	O
theorem	O
we	O
apply	O
theorem	O
by	O
finding	O
an	O
upper	O
bound	O
for	O
histogram	O
rules	O
for	O
the	O
kernel	B
rule	B
with	O
kernel	B
k	O
in	O
whichh	O
is	O
absorbed	O
p	O
dn	O
gn-l	O
dn-	O
l	O
p	O
lkx	O
xill	O
kx	O
xn	O
kx	O
xn	O
o	O
define	O
b	O
b	O
we	O
have	O
p	O
lkx	O
x	O
i	O
kx	O
xn	O
kx	O
xn	O
o	O
p	O
ikx	O
b	O
xn	O
e	O
sx	O
r	O
sxr	O
is	O
the	O
ball	O
of	O
radius	O
r	O
centered	O
at	O
x	O
e	O
ixnesxr	O
lkx	O
xdl	O
fjix	O
corollary	O
since	O
b	O
e	O
ixnesxr	O
fjjnflsxp	O
cb	O
j	O
that	O
ku	O
fj	O
for	O
liull	O
p	O
so	O
that	O
ikx	O
xii	O
fj	O
px	O
i	O
sxp	O
cb	O
f	O
is	O
fj	O
jn	O
fldy	O
fldx	O
sxr	O
j	O
flsxp	O
ccd	O
b	O
fj	O
jn	O
where	O
we	O
used	O
lemma	O
the	O
constant	O
cd	O
depends	O
upon	O
the	O
dimension	B
only	O
o	O
histogram	O
rules	O
in	O
this	O
section	O
we	O
discuss	O
properties	O
of	O
the	O
deleted	O
estimate	O
of	O
the	O
error	O
probability	O
of	O
histogram	O
rules	O
let	O
p	O
a	O
be	O
a	O
partition	B
of	O
r	O
d	O
and	O
let	O
gn	O
be	O
the	O
corresponding	O
histogram	O
classifier	B
chapters	O
and	O
to	O
get	O
a	O
performance	O
bound	O
for	O
the	O
deleted	O
estimate	O
we	O
can	O
simply	O
apply	O
theorem	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
theorem	O
for	O
the	O
histogram	B
rule	B
gn	O
corresponding	O
to	O
any	O
partition	B
p	O
and	O
for	O
all	O
n	O
e	O
l	O
f-l	O
n	O
n	O
l	O
j	O
lr	O
i	O
i	O
i	O
n	O
n	O
i	O
i	O
and	O
in	O
particular	O
d	O
e	O
ln	O
ln	O
e	O
n	O
proof	O
the	O
first	O
inequality	B
follows	O
from	O
theorem	O
if	O
we	O
can	O
find	O
an	O
upper	O
bound	O
for	O
pgnx	O
gn-l	O
we	O
introduce	O
the	O
notation	O
clearly	O
gn-l	O
can	O
differ	O
from	O
gnx	O
only	O
if	O
both	O
xn	O
and	O
x	O
fall	O
in	O
the	O
same	O
cell	O
of	O
the	O
partition	B
and	O
if	O
the	O
number	O
of	O
zeros	O
in	O
the	O
cell	O
is	O
either	O
equal	O
or	O
less	O
by	O
one	O
than	O
the	O
number	O
of	O
ones	O
therefore	O
by	O
independence	O
we	O
have	O
pgnx	O
gn-lx	O
l	O
p	O
x	O
e	O
ai	O
xn	O
e	O
ad	O
if-ln-iajji	O
p	O
von-lai	O
xeaixneai	O
f-laj	O
the	O
terms	O
in	O
the	O
sum	O
above	O
may	O
be	O
bounded	O
as	O
follows	O
p	O
i	O
xeaxnea	O
o	O
p	O
independence	O
e	O
vo-i	O
if	O
n-	O
j	O
i	O
xi	O
xn	O
j	O
a	O
o	O
f	O
a	O
l	O
e	O
i	O
o	O
lemma	O
f-lar	O
e	O
iln	O
la	O
o	O
jensens	O
inequality	B
e-nla	O
problems	O
and	O
exercises	O
where	O
in	O
the	O
last	O
step	O
we	O
use	O
lemma	O
this	O
concludes	O
the	O
proof	O
of	O
the	O
first	O
inequality	B
the	O
second	O
one	O
follows	O
trivially	O
by	O
noting	O
that	O
xe-x	O
ie	O
for	O
all	O
x	O
o	O
remark	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
inequalities	O
of	O
theorem	O
are	O
tight	O
up	O
to	O
a	O
constant	O
factor	O
in	O
the	O
sense	O
that	O
for	O
any	O
partition	B
p	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
d	O
e	O
ln	O
ln	O
e	O
problem	O
remark	O
the	O
second	O
inequality	B
in	O
theorem	O
points	O
out	O
an	O
important	O
difference	O
between	O
the	O
behavior	O
of	O
the	O
resubstitution	B
and	O
the	O
deleted	O
estimates	O
for	O
histogram	O
rules	O
as	O
mentioned	O
above	O
for	O
some	O
distributions	O
the	O
variance	B
of	I
l	O
can	O
be	O
of	O
the	O
order	O
i	O
jli	O
this	O
should	O
be	O
contrasted	O
with	O
the	O
much	O
smaller	O
variance	B
of	I
the	O
resubstitution	B
estimate	O
the	O
small	O
variance	B
of	I
lr	O
comes	O
often	O
with	O
a	O
larger	O
bias	B
other	O
types	O
of	O
error	O
estimates	O
with	O
small	O
variance	O
are	O
discussed	O
in	O
chapter	O
remark	O
theorem	O
shows	O
that	O
for	O
any	O
partition	B
sup	O
e	O
on	O
the	O
other	O
hand	O
if	O
k	O
o	O
where	O
k	O
is	O
the	O
number	O
of	O
cells	O
in	O
the	O
partition	B
then	O
for	O
the	O
resubstitution	B
estimate	O
we	O
have	O
a	O
better	O
guaranteed	O
distribution-free	O
performance	O
sup	O
e	O
at	O
first	O
sight	O
the	O
resubstitution	B
estimate	O
seems	O
preferable	O
to	O
the	O
deleted	O
estimate	O
however	O
if	O
the	O
partition	B
has	O
a	O
large	O
number	O
of	O
cells	O
lr	O
may	O
be	O
off	O
the	O
mark	O
see	O
theorem	O
problems	O
and	O
exercises	O
problem	O
show	O
the	O
following	O
variant	O
of	O
theorem	O
for	O
all	O
symmetric	O
classifiers	O
e	O
dn	O
gn-l	O
dn-	O
pgnx	O
dn	O
gnx	O
d	O
pgn-lx	O
dn-	O
gn-lx	O
d	O
j	O
deleted	O
estimates	O
of	O
the	O
error	O
probability	O
where	O
d	O
and	O
dz-	O
i	O
are	O
just	O
dn	O
and	O
dn-	O
l	O
with	O
yj	O
replaced	O
by	O
an	O
independent	O
copy	O
yo	O
problem	O
let	O
gn	O
be	O
the	O
relabeling	B
nn	O
rule	B
with	O
the	O
k-nn	B
classifier	B
as	O
ancestral	B
rule	B
as	O
defined	O
in	O
chapter	O
provide	O
an	O
upper	O
bound	O
for	O
the	O
squared	B
error	I
e	O
of	O
the	O
deleted	O
estimate	O
problem	O
let	O
gil	O
be	O
the	O
rule	B
obtained	O
by	O
choosing	O
the	O
best	O
k	O
ko	O
in	O
the	O
k-nn	B
rule	B
is	O
a	O
constant	O
by	O
minimizing	O
the	O
standard	B
deleted	O
estimate	O
l	O
with	O
respect	O
to	O
k	O
how	O
would	O
you	O
estimate	O
the	O
probability	O
of	O
error	O
for	O
this	O
rule	B
give	O
the	O
best	O
possible	O
distribution-free	O
performance	O
guarantees	O
you	O
can	O
find	O
problem	O
consider	O
the	O
kernel	B
rule	B
with	O
the	O
window	O
kernel	B
k	O
so	O
i	O
show	O
that	O
eld-l	O
n	O
n	O
n	O
inn	O
hint	O
follow	O
the	O
line	O
of	O
the	O
proof	O
of	O
theorem	O
problem	O
show	O
that	O
for	O
any	O
partition	B
p	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
for	O
the	O
deleted	O
estimate	O
of	O
the	O
error	O
probability	O
of	O
the	O
corresponding	O
histogram	B
rule	B
e	O
ln	O
ln	O
e	O
l	O
hint	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
the	O
similar	O
inequality	B
for	O
k-nearest	O
neighbor	O
rules	O
problem	O
consider	O
the	O
k-spacings	O
method	O
chapter	O
we	O
estimate	O
the	O
proba	O
bility	O
of	O
error	O
by	O
a	O
modified	O
deleted	O
estimate	O
ln	O
as	O
follows	O
where	O
gni	O
is	O
a	O
histogram	B
rule	B
based	O
upon	O
the	O
same	O
k-spacings	O
partition	B
used	O
for	O
gn-that	O
is	O
the	O
partition	B
determined	O
by	O
k-spacings	O
of	O
the	O
data	O
points	O
x	O
i	O
xn-but	O
in	O
which	O
a	O
majority	B
vote	I
is	O
based	O
upon	O
the	O
yjs	O
in	O
the	O
same	O
cell	O
of	O
the	O
partition	B
with	O
yi	O
deleted	O
show	O
that	O
eln	O
n	O
hint	O
condition	O
on	O
the	O
xis	O
and	O
verify	O
that	O
the	O
inequality	B
of	O
theorem	O
remains	O
valid	O
problem	O
consider	O
a	O
rule	B
in	O
which	O
we	O
rank	O
the	O
real-valued	O
observations	O
xi	O
xn	O
from	O
small	O
to	O
large	O
to	O
obtain	O
xc	O
xn	O
assume	O
that	O
xi	O
has	O
a	O
density	O
derive	O
an	O
inequality	B
for	O
the	O
error	O
ild	O
lili	O
for	O
some	O
deleted	O
estimate	O
ld	O
your	O
choice	O
when	O
the	O
rule	B
is	O
defined	O
by	O
a	O
majority	B
vote	I
over	O
the	O
data-dependent	B
partition	B
defined	O
by	O
points	O
respectively	O
problems	O
and	O
exercises	O
problem	O
prove	O
that	O
for	O
the	O
k-nearest	O
neighbor	O
rule	B
e	O
n	O
n-jiii	O
and	O
wagner	O
hint	O
obtain	O
a	O
refined	O
upper	O
bound	O
for	O
using	O
techniques	O
not	O
unlike	O
those	O
of	O
the	O
proof	O
of	O
theorem	O
problem	O
open-ended	O
problem	O
investigate	O
if	O
theorem	O
can	O
be	O
extended	O
to	O
nels	O
with	O
unbounded	O
support	B
such	O
as	O
the	O
gaussian	B
kernel	B
automatic	B
kernel	B
rules	O
we	O
saw	O
in	O
chapter	O
that	O
for	O
a	O
large	O
class	O
of	O
kernels	O
if	O
the	O
smoothing	O
parameter	O
h	O
converges	O
to	O
zero	O
such	O
that	O
nh	O
d	O
goes	O
to	O
infinity	O
as	O
n	O
then	O
the	O
kernel	B
cation	O
rule	B
is	O
universally	O
consistent	O
for	O
a	O
particular	O
n	O
asymptotic	O
results	O
provide	O
little	O
guidance	O
in	O
the	O
selection	O
of	O
h	O
on	O
the	O
other	O
hand	O
selecting	O
the	O
wrong	O
value	O
of	O
h	O
may	O
lead	O
to	O
catastrophic	O
error	O
rates-in	O
fact	O
the	O
crux	O
of	O
every	O
nonparametric	O
estimation	B
problem	O
is	O
the	O
choice	O
of	O
an	O
appropriate	O
smoothing	B
factor	I
it	O
tells	O
us	O
how	O
far	O
we	O
generalize	O
each	O
data	O
point	O
xi	O
in	O
the	O
space	O
purely	O
atomic	O
distributions	O
require	O
little	O
smoothing	O
will	O
generally	O
be	O
fine	O
while	O
distributions	O
with	O
densities	O
require	O
a	O
lot	O
of	O
smoothing	O
as	O
there	O
are	O
no	O
simple	O
tests	O
for	O
verifying	O
whether	O
the	O
data	O
are	O
drawn	O
from	O
an	O
absolutely	O
continuous	O
distribution-let	O
alone	O
a	O
distribution	O
with	O
a	O
lipschitz	O
density-it	O
is	O
important	O
to	O
let	O
the	O
data	O
dn	O
mine	O
h	O
a	O
data-dependent	B
smoothing	B
factor	I
is	O
merely	O
a	O
mathematical	O
function	O
hn	O
x	O
lf	O
for	O
brevity	O
we	O
will	O
simply	O
write	O
hn	O
to	O
denote	O
the	O
random	O
variable	B
hndn	O
this	O
chapter	O
develops	O
results	O
regarding	O
such	O
functions	O
hn	O
this	O
chapter	O
is	O
not	O
a	O
luxury	O
but	O
a	O
necessity	O
anybody	O
developing	O
software	O
for	O
pattern	O
recognition	O
must	O
necessarily	O
let	O
the	O
data	O
do	O
the	O
talking-in	O
fact	O
good	O
universally	O
applicable	O
programs	O
can	O
have	O
only	O
data-dependent	B
parameters	O
consider	O
the	O
family	B
of	I
kernel	B
decision	O
rules	O
gn	O
and	O
let	O
the	O
smoothing	B
factor	I
h	O
play	O
the	O
role	O
of	O
parameter	O
the	O
best	O
parameter	O
is	O
the	O
one	O
that	O
minimizes	O
ln	O
unfortunately	O
it	O
is	O
unknown	O
as	O
is	O
l	O
opt	O
the	O
corresponding	O
minimal	O
probability	O
of	O
error	O
the	O
first	O
goal	O
of	O
any	O
data-dependent	B
smoothing	B
factor	I
hn	O
should	O
be	O
to	O
approach	O
the	O
performance	O
of	O
hopt	O
we	O
are	O
careful	O
here	O
to	O
avoid	O
saying	O
that	O
hn	O
should	O
be	O
close	O
to	O
hopt	O
as	O
closeness	O
of	O
smoothing	O
factors	O
does	O
not	O
necessarily	O
imply	O
closeness	O
of	O
error	O
probabilities	O
and	O
vice	O
versa	O
guarantees	O
one	O
might	O
want	O
automatic	B
kernel	B
rules	O
in	O
this	O
respect	O
are	O
eln	O
l	O
opt	O
an	O
for	O
some	O
suitable	O
sequence	O
an	O
or	O
better	O
still	O
eln	O
l	O
l	O
for	O
another	O
sequence	O
bn	O
o	O
but	O
before	O
one	O
even	O
attempts	O
to	O
develop	O
such	O
dependent	O
smoothing	O
factors	O
ones	O
first	O
concern	O
should	O
be	O
with	O
consistency	B
is	O
it	O
true	O
that	O
with	O
the	O
given	O
hn	O
ln	O
l	O
in	O
probability	O
or	O
with	O
probability	O
one	O
this	O
question	O
is	O
dealt	O
with	O
in	O
the	O
next	O
section	O
in	O
subsequent	O
sections	O
we	O
give	O
various	O
examples	O
of	O
data-dependent	B
smoothing	O
factors	O
consistency	B
we	O
start	O
with	O
consistency	B
results	O
that	O
generalize	O
theorem	O
the	O
first	O
result	O
assumes	O
that	O
the	O
value	O
of	O
the	O
smoothing	O
parameter	O
is	O
picked	O
from	O
a	O
discrete	O
set	O
theorem	O
assume	O
that	O
the	O
random	O
variable	B
hn	O
takes	O
its	O
values	O
from	O
the	O
set	O
of	O
real	O
numbers	O
of	O
the	O
form	O
where	O
k	O
is	O
a	O
nonnegative	O
integer	O
and	O
o	O
let	O
k	O
be	O
a	O
regular	B
kernel	B
function	O
definition	O
define	O
the	O
kernel	B
classification	O
rule	B
corresponding	O
to	O
the	O
random	O
smoothing	O
parameter	O
hn	O
by	O
gnx	O
k	O
lil	O
i	O
k	O
ifn	O
otherwise	O
i	O
lil	O
if	O
and	O
hn	O
and	O
nhi	O
with	O
probability	O
one	O
as	O
n	O
then	O
lgn	O
l	O
with	O
probability	O
one	O
that	O
is	O
gn	O
is	O
strongly	O
universally	O
consistent	O
proof	O
the	O
theorem	O
is	O
a	O
straightforward	O
extension	O
of	O
theorem	O
clearly	O
lgn	O
l	O
with	O
probability	O
one	O
if	O
and	O
only	O
if	O
for	O
every	O
e	O
ilgn-le	O
with	O
probability	O
one	O
now	O
for	O
any	O
ilgn-le	O
we	O
have	O
to	O
show	O
that	O
the	O
random	O
variables	O
on	O
the	O
right-hand	O
side	O
converge	O
to	O
zero	O
with	O
probability	O
one	O
the	O
convergence	O
of	O
the	O
second	O
and	O
third	O
terms	O
follows	O
from	O
since	O
it	O
states	O
that	O
for	O
any	O
e	O
there	O
exist	O
b	O
and	O
no	O
such	O
that	O
for	O
the	O
the	O
conditions	O
on	O
hn	O
the	O
convergence	O
of	O
the	O
first	O
term	O
follows	O
from	O
theorem	O
error	O
probability	O
lnk	O
of	O
the	O
kernel	B
rule	B
with	O
smoothing	O
parameter	O
h	O
pl	O
nk	O
l	O
e	O
consistency	B
for	O
some	O
constant	O
c	O
depending	O
on	O
the	O
dimension	B
only	O
provided	O
that	O
n	O
no	O
h	O
fj	O
and	O
nh	O
d	O
fj	O
now	O
clearly	O
p	O
l	O
elhn	O
fj	O
nh	O
fj	O
p	O
sup	O
kl	O
jjnl	O
jj	O
lnk	O
l	O
e	O
cn	O
sup	O
plnk	O
l	O
e	O
kl	O
by	O
the	O
union	O
bound	O
where	O
cn	O
is	O
the	O
number	O
of	O
possible	O
values	O
of	O
hn	O
in	O
the	O
given	O
range	O
as	O
we	O
note	O
that	O
cn	O
log	O
log	O
fj	O
logl	O
on	O
eon	O
n	O
by	O
the	O
condition	O
on	O
the	O
sequence	O
combining	O
this	O
with	O
theorem	O
for	O
n	O
no	O
we	O
get	O
p	O
l	O
e	O
hn	O
fj	O
n	O
h	O
fj	O
e	O
which	O
is	O
summable	O
in	O
n	O
the	O
borel-cantelli	B
lemma	I
implies	O
that	O
with	O
probability	O
one	O
and	O
the	O
theorem	O
is	O
proved	O
for	O
weak	B
consistency	B
it	O
suffices	O
to	O
require	O
convergence	O
of	O
and	O
nh	O
in	O
probability	O
theorem	O
assume	O
that	O
the	O
random	O
variable	B
hn	O
takes	O
its	O
values	O
from	O
the	O
set	O
of	O
real	O
numbers	O
of	O
the	O
form	O
where	O
k	O
is	O
a	O
nonnegative	O
integer	O
and	O
on	O
o	O
let	O
k	O
be	O
a	O
regular	B
kernel	B
if	O
and	O
and	O
nhl	O
in	O
probability	O
as	O
n	O
then	O
the	O
kernel	B
classification	O
rule	B
corresponding	O
to	O
the	O
random	O
smoothing	O
eter	O
hn	O
is	O
universally	O
consistent	O
that	O
is	O
lgn	O
l	O
in	O
probability	O
we	O
are	O
now	O
prepared	O
to	O
prove	O
a	O
result	O
similar	O
to	O
theorem	O
without	O
restricting	O
the	O
possible	O
values	O
of	O
the	O
random	O
smoothing	O
parameter	O
hn	O
for	O
technical	O
reasons	O
automatic	B
kernel	B
rules	O
we	O
need	O
to	O
assume	O
some	O
additional	O
regularity	O
conditions	O
on	O
the	O
kernel	B
function	O
k	O
must	O
be	O
decreasing	O
along	O
rays	O
starting	O
from	O
the	O
origin	O
but	O
it	O
should	O
not	O
decrease	O
too	O
rapidly	O
rapidly	O
decreasing	O
functions	O
such	O
as	O
the	O
gaussian	B
kernel	B
or	O
functions	O
of	O
bounded	O
support	B
such	O
as	O
the	O
window	O
kernel	B
are	O
excluded	O
theorem	O
let	O
k	O
be	O
a	O
regular	B
kernel	B
that	O
is	O
monotone	O
decreasing	O
along	O
rays	O
that	O
is	O
for	O
any	O
x	O
e	O
nd	O
and	O
a	O
kax	O
kx	O
assume	O
in	O
addition	O
that	O
there	O
exists	O
a	O
constant	O
c	O
such	O
that	O
for	O
every	O
sufficiently	O
small	O
and	O
x	O
e	O
rd	O
kl	O
let	O
be	O
a	O
sequence	O
of	O
random	O
variables	O
satisfying	O
hn	O
and	O
nh	O
with	O
probability	O
one	O
as	O
n	O
then	O
the	O
error	O
probability	O
lgn	O
of	O
the	O
kernel	B
classification	O
rule	B
with	O
kernel	B
k	O
and	O
smoothing	O
parameter	O
hn	O
converges	O
to	O
l	O
with	O
probability	O
one	O
that	O
is	O
the	O
rule	B
is	O
strongly	O
universally	O
consistent	O
remark	O
the	O
technical	O
condition	O
on	O
k	O
is	O
needed	O
to	O
ensure	O
that	O
small	O
changes	O
in	O
h	O
do	O
not	O
cause	O
dramatic	O
changes	O
in	O
lgn	O
we	O
expect	O
some	O
smooth	O
behavior	O
of	O
lgn	O
as	O
a	O
function	O
of	O
h	O
the	O
conditions	O
are	O
rather	O
restrictive	O
as	O
the	O
kernels	O
must	O
have	O
infinite	O
support	B
and	O
decrease	O
slower	O
than	O
at	O
a	O
polynomial	B
rate	O
an	O
example	O
satisfying	O
the	O
conditions	O
is	O
kx	O
if	O
iixll	O
otherwise	O
where	O
r	O
problem	O
the	O
conditions	O
on	O
hn	O
are	O
by	O
no	O
means	O
necessary	O
we	O
have	O
already	O
seen	O
that	O
consistency	B
occurs	O
for	O
atomic	O
distributions	O
if	O
k	O
and	O
hn	O
or	O
for	O
distributions	O
with	O
l	O
when	O
hn	O
takes	O
any	O
value	O
however	O
theorem	O
provides	O
us	O
with	O
a	O
simple	O
collection	O
of	O
sufficient	O
conditions	O
proof	O
of	O
theorem	O
first	O
we	O
discretize	O
hn	O
define	O
a	O
sequence	O
on	O
satisfying	O
the	O
condition	O
in	O
theorem	O
and	O
introduce	O
the	O
random	O
variables	O
h	O
n	O
and	O
hn	O
as	O
follows	O
h	O
n	O
where	O
kn	O
is	O
the	O
smallest	O
integer	O
such	O
that	O
hn	O
and	O
let	O
h	O
n	O
onh	O
n	O
thus	O
h	O
n	O
hn	O
h	O
n	O
note	O
that	O
bothh	O
n	O
and	O
h	O
n	O
satisfy	O
the	O
conditions	O
of	O
theorem	O
as	O
usual	O
the	O
consistency	B
proof	O
is	O
based	O
on	O
theorem	O
here	O
however	O
we	O
need	O
a	O
somewhat	O
tricky	O
choice	O
of	O
the	O
denominator	O
of	O
the	O
functions	O
that	O
approximate	O
introduce	O
l	O
il	O
j	O
k	O
k	O
i	O
h	O
clearly	O
the	O
value	O
of	O
the	O
classification	O
rule	B
gnx	O
equals	O
one	O
if	O
and	O
only	O
is	O
greater	O
than	O
the	O
function	O
defined	O
similarly	O
with	O
the	O
iyill	O
replaced	O
with	O
iyioi	O
then	O
by	O
theorem	O
it	O
suffices	O
to	O
show	O
that	O
f	O
consistency	B
with	O
probability	O
one	O
we	O
use	O
the	O
following	O
decomposition	O
f	O
f	O
f	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
converges	O
to	O
zero	O
with	O
probability	O
one	O
which	O
can	O
be	O
seen	O
by	O
repeating	O
the	O
argument	O
of	O
the	O
proof	O
of	O
theorem	O
using	O
the	O
observation	O
that	O
in	O
the	O
proof	O
of	O
theorem	O
we	O
proved	O
consistency	B
via	O
an	O
exponential	B
probability	O
inequality	B
for	O
f	O
ll	O
the	O
first	O
term	O
may	O
be	O
bounded	O
as	O
the	O
following	O
simple	O
chain	O
of	O
inequalities	O
cates	O
h	O
n	O
n	O
and	O
the	O
condition	O
on	O
k	O
if	O
n	O
is	O
large	O
enough	O
f	O
k	O
j	O
k	O
fldz	O
h	O
since	O
h	O
n	O
satisfies	O
the	O
conditions	O
of	O
theorem	O
the	O
integral	O
on	O
the	O
right-hand	O
side	O
converges	O
to	O
one	O
with	O
probability	O
one	O
just	O
as	O
we	O
argued	O
for	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
but	O
converges	O
to	O
zero	O
therefore	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
tends	O
to	O
zero	O
with	O
probability	O
one	O
remark	O
a	O
quick	O
inspection	O
of	O
the	O
proof	O
above	O
shows	O
that	O
if	O
and	O
are	O
deterministic	O
sequences	O
with	O
the	O
property	O
that	O
an	O
bn	O
bn	O
and	O
na	O
automatic	B
kernel	B
rules	O
then	O
for	O
the	O
kernel	B
estimate	O
with	O
kernel	B
as	O
in	O
theorem	O
we	O
have	O
sup	O
lnh	O
l	O
with	O
probability	O
one	O
allshsbll	O
for	O
all	O
distributions	O
one	O
would	O
never	O
use	O
the	O
worst	O
smoothing	B
factor	I
over	O
the	O
range	O
bn	O
but	O
this	O
corollary	O
points	O
out	O
just	O
how	O
powerful	O
theorem	O
is	O
data	O
splitting	O
our	O
first	O
example	O
of	O
a	O
data-dependent	B
hn	O
is	O
based	O
upon	O
the	O
minimization	O
of	O
a	O
suitable	O
error	O
estimate	O
you	O
should	O
have	O
read	O
chapter	O
on	O
data	O
splitting	O
if	O
you	O
want	O
to	O
understand	O
the	O
remainder	O
of	O
this	O
section	O
the	O
data	O
sequence	O
dn	O
yn	O
is	O
divided	O
into	O
two	O
parts	O
the	O
first	O
part	O
dm	O
yl	O
m	O
ynj	O
is	O
used	O
for	O
training	O
while	O
the	O
remaining	O
l	O
n	O
m	O
pairs	O
constitute	O
the	O
testing	B
sequence	I
tz	O
ymz	O
the	O
training	O
sequence	O
dm	O
is	O
used	O
to	O
design	O
a	O
class	O
of	O
classifiers	O
em	O
which	O
in	O
our	O
case	O
is	O
the	O
class	O
of	B
kernel	B
rules	I
based	O
on	O
dm	O
with	O
all	O
possible	O
values	O
of	O
h	O
for	O
fixed	O
kernel	B
k	O
note	O
that	O
the	O
value	O
of	O
the	O
kernel	B
rule	B
gmx	O
with	O
smoothing	O
parameter	O
h	O
is	O
zero	O
if	O
and	O
only	O
if	O
x	O
h	O
jmex	O
lyi	O
i	O
so	O
m	O
il	O
classifiers	O
in	O
em	O
are	O
denoted	O
by	O
a	O
classifier	B
is	O
selected	O
from	O
em	O
that	O
minimizes	O
the	O
holdout	B
estimate	O
of	O
the	O
error	O
probability	O
lml	O
m	O
l	O
irpmxmi	O
ymd	O
l	O
il	O
the	O
particular	O
rule	B
selected	O
in	O
this	O
manner	O
is	O
called	O
gil	O
the	O
question	O
is	O
how	O
far	O
the	O
error	O
probability	O
lgn	O
of	O
the	O
obtained	O
rule	B
is	O
from	O
that	O
of	O
the	O
optimal	O
rule	B
in	O
cm	O
finite	O
collections	O
it	O
is	O
computationally	O
attractive	O
to	O
restrict	O
the	O
possible	O
values	O
of	O
h	O
to	O
a	O
finite	O
set	O
of	O
real	O
numbers	O
for	O
example	O
em	O
could	O
consist	O
of	O
all	O
kernel	B
rules	O
with	O
h	O
e	O
for	O
some	O
positive	O
integer	O
km	O
the	O
advantage	O
of	O
this	O
choice	O
of	O
cm	O
is	O
that	O
the	O
best	O
h	O
in	O
this	O
class	O
is	O
within	O
a	O
factor	O
of	O
two	O
of	O
the	O
best	O
h	O
among	O
all	O
possible	O
real	O
smoothing	O
factors	O
unless	O
the	O
best	O
h	O
is	O
smaller	O
than	O
or	O
larger	O
than	O
clearly	O
lem	O
i	O
and	O
as	O
pointed	O
out	O
in	O
chapter	O
for	O
the	O
selected	O
rule	B
gn	O
hoeffdings	O
inequality	B
and	O
the	O
union	O
bound	O
imply	O
that	O
p	O
inf	O
l	O
m	O
ej	O
dm	O
rpmecm	O
if	O
km	O
eol	O
then	O
the	O
upper	O
bound	O
decreases	O
exponentially	O
in	O
i	O
and	O
in	O
fact	O
data	O
splitting	O
e	O
lgn	O
inf	O
lcpm	O
mecm	O
i	O
by	O
theorem	O
em	O
contains	O
a	O
subsequence	O
of	O
consistent	O
rules	O
if	O
m	O
and	O
km	O
as	O
n	O
to	O
make	O
sure	O
that	O
gn	O
is	O
strongly	O
universally	O
consistent	O
as	O
well	O
we	O
only	O
need	O
that	O
limn--oo	O
i	O
and	O
km	O
eol	O
theorem	O
under	O
these	O
conditions	O
the	O
rule	B
is	O
i-optimal	O
chapter	O
the	O
discussion	O
above	O
does	O
little	O
to	O
help	O
us	O
with	O
the	O
selection	O
of	O
m	O
i	O
and	O
km	O
safe	O
but	O
possibly	O
suboptimal	O
choices	O
might	O
be	O
i	O
n	O
m	O
n	O
km	O
n	O
note	O
that	O
the	O
argument	O
above	O
is	O
valid	O
for	O
any	O
regular	B
kernel	B
k	O
infinite	O
collections	O
if	O
we	O
do	O
not	O
want	O
to	O
exclude	O
any	O
value	O
of	O
the	O
smoothing	O
parameter	O
and	O
pick	O
h	O
from	O
then	O
em	O
is	O
of	O
infinite	O
cardinality	O
here	O
we	O
need	O
something	O
stronger	O
like	O
the	O
vapnik	O
theory	O
for	O
example	O
from	O
chapter	O
we	O
have	O
where	O
sem	O
i	O
is	O
the	O
l-th	O
shatter	B
coefficient	I
corresponding	O
to	O
the	O
class	O
of	O
classifiers	O
em	O
we	O
now	O
obtain	O
upper	O
bounds	O
for	O
seem	O
i	O
for	O
different	O
choices	O
of	O
k	O
define	O
the	O
function	O
recall	O
that	O
for	O
the	O
kernel	B
rule	B
based	O
on	O
d	O
m	O
if	O
fmx	O
dm	O
otherwise	O
gm	O
we	O
introduce	O
the	O
kernel	B
complexity	I
km	O
km	O
sup	O
of	O
sign	O
changes	O
of	O
jnx	O
yi	O
ym	O
as	O
h	O
varies	O
from	O
to	O
infinity	O
suppose	O
we	O
have	O
a	O
kernel	B
with	O
kernel	B
complexity	I
km	O
then	O
as	O
h	O
varies	O
from	O
to	O
infinity	O
the	O
binary	B
i-vector	O
changes	O
at	O
most	O
ikm	O
times	O
it	O
can	O
thus	O
take	O
at	O
most	O
ikm	O
different	O
values	O
fore	O
automatic	B
kernel	B
rules	O
we	O
postpone	O
the	O
issue	O
of	O
computing	O
kernel	B
complexities	O
until	O
the	O
next	O
section	O
it	O
suffices	O
to	O
note	O
that	O
if	O
gn	O
is	O
obtained	O
by	O
minimizing	O
the	O
holdout	B
error	O
estimate	O
lml	O
m	O
by	O
varying	O
h	O
then	O
i	O
various	O
probability	O
bounds	O
may	O
also	O
be	O
derived	O
from	O
the	O
results	O
of	O
chapter	O
for	O
example	O
we	O
have	O
theorem	O
assume	O
that	O
gn	O
minimizes	O
the	O
holdout	B
estimate	O
lml	O
m	O
over	O
all	O
kernel	B
rules	O
withfixed	O
kernel	B
k	O
of	O
kernel	B
complexity	I
km	O
and	O
overall	O
smoothing	O
factors	O
h	O
o	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
if	O
limn-oo	O
m	O
lim	O
log	O
km	O
n-oo	O
hm	O
n-oo	O
log	O
n	O
k	O
is	O
a	O
regular	B
kernel	B
i	O
for	O
weak	B
universal	B
consistency	B
may	O
be	O
replaced	O
by	O
limn-oo	O
i	O
proof	O
note	O
that	O
cm	O
contains	O
a	O
strongly	O
universally	O
consistent	O
take	O
h	O
m-	O
for	O
example	O
and	O
apply	O
theorem	O
noting	O
that	O
h	O
yet	O
mhd	O
thus	O
lim	O
n-oo	O
mecm	O
inf	O
l	O
m	O
l	O
with	O
probability	O
one	O
it	O
suffices	O
to	O
apply	O
theorem	O
and	O
to	O
note	O
that	O
the	O
bound	O
in	O
is	O
summable	O
in	O
n	O
when	O
lilog	O
n	O
and	O
log	O
km	O
ol	O
for	O
weak	B
universal	B
consistency	B
a	O
simple	O
application	O
of	O
suffices	O
to	O
note	O
that	O
we	O
only	O
need	O
instead	O
of	O
lilogn	O
approximation	O
errors	O
decrease	O
with	O
m	O
for	O
example	O
if	O
class	O
densities	O
exist	O
we	O
may	O
combine	O
the	O
inequality	B
of	O
problem	O
with	O
bounds	O
from	O
devroye	O
and	O
gyorfi	O
and	O
holmstrom	O
and	O
klemehi	O
to	O
conclude	O
thate	O
mecm	O
l	O
l	O
is	O
of	O
the	O
order	O
of	O
with	O
ex	O
e	O
under	O
suitable	O
conditions	O
on	O
k	O
kernel	B
complexity	I
and	O
the	O
densities	O
by	O
the	O
estimation	B
error	I
is	O
jlogikm	O
i	O
requiring	O
instead	O
large	O
values	O
for	O
i	O
clearly	O
some	O
sort	O
of	O
balance	O
is	O
called	O
for	O
ignoring	O
the	O
logarithmic	O
term	O
for	O
now	O
we	O
see	O
that	O
i	O
should	O
be	O
roughly	O
m	O
if	O
we	O
are	O
to	O
balance	O
errors	O
of	O
both	O
kinds	O
unfortunately	O
all	O
of	O
this	O
is	O
ad	O
hoc	O
and	O
based	O
upon	O
unverifiable	O
distributional	O
conditions	O
ideally	O
one	O
should	O
let	O
the	O
data	O
select	O
i	O
and	O
m	O
see	O
problem	O
and	O
problem	O
on	O
optimal	O
data	O
splitting	O
for	O
some	O
distributions	O
the	O
estimation	B
error	I
is	O
just	O
too	O
large	O
to	O
obtain	O
totic	O
optimality	O
as	O
defined	O
in	O
chapter	O
for	O
example	O
the	O
best	O
bound	O
on	O
the	O
estimation	B
error	I
is	O
jlog	O
n	O
n	O
attained	O
when	O
km	O
i	O
i	O
n	O
if	O
the	O
tion	O
of	O
x	O
is	O
atomic	O
with	O
finitely	O
many	O
atoms	O
then	O
the	O
expected	O
approximation	B
error	I
is	O
jiii	O
hence	O
the	O
error	O
introduced	O
by	O
the	O
selection	O
process	O
smothers	O
the	O
approximation	B
error	I
when	O
m	O
is	O
linear	O
in	O
n	O
similar	O
conclusions	O
may	O
even	O
be	O
with	O
if	O
x	O
e	O
and	O
if	O
x	O
e	O
for	O
the	O
kernel	B
rule	B
with	O
drawn	O
when	O
x	O
has	O
a	O
density	O
consider	O
the	O
uniform	O
distribution	O
on	O
u	O
h	O
k	O
the	O
expected	O
approximation	B
error	I
tends	O
to	O
l	O
exponentially	O
quickly	O
in	O
m	O
and	O
this	O
is	O
always	O
far	O
better	O
than	O
the	O
estimation	B
error	I
which	O
at	O
best	O
is	O
jlog	O
n	O
n	O
kernel	B
complexity	I
we	O
now	O
tum	O
to	O
the	O
kernel	B
complexity	I
km	O
the	O
following	O
lemmas	O
are	O
useful	O
in	O
our	O
computations	O
if	O
l	O
log	O
n	O
we	O
note	O
that	O
for	O
strong	B
consistency	B
it	O
suffices	O
that	O
km	O
for	O
some	O
finite	O
y	O
gust	O
verify	O
the	O
proof	O
again	O
this	O
as	O
it	O
turns	O
out	O
is	O
satisfied	O
for	O
nearly	O
all	O
practical	O
kernels	O
lemma	O
let	O
b	O
bm	O
be	O
fixed	O
numbers	O
and	O
let	O
ai	O
e	O
r	O
i	O
m	O
befixed	O
with	O
the	O
restriction	O
that	O
am	O
i	O
then	O
the	O
function	O
fx	O
aixh	O
has	O
at	O
most	O
m	O
nonzero	O
positive	O
roots	O
proof	O
note	O
first	O
that	O
f	O
cannot	O
be	O
identically	O
zero	O
on	O
any	O
interval	O
of	O
nonzero	O
length	O
let	O
z	O
denote	O
the	O
number	O
of	O
nonzero	O
positive	O
roots	O
of	O
a	O
function	O
g	O
we	O
have	O
z	O
z	O
aixci	O
ci	O
bi	O
bi	O
all	O
i	O
thus	O
cl	O
automatic	B
kernel	B
rules	O
a	O
continuously	O
differentiable	O
g	O
we	O
have	O
zg	O
zgl	O
z	O
note	O
that	O
the	O
b	O
are	O
increasing	O
b	O
and	O
a	O
as	O
z	O
bm	O
we	O
derive	O
our	O
claim	O
by	O
simple	O
induction	O
on	O
m	O
for	O
am	O
lemma	O
let	O
a	O
i	O
am	O
be	O
fixed	O
real	O
numbers	O
and	O
let	O
bi	O
bm	O
be	O
different	O
nonnegative	O
reals	O
then	O
if	O
ex	O
the	O
function	O
fx	O
l	O
ai	O
e-	O
bix	O
x	O
m	O
is	O
either	O
identically	O
zero	O
or	O
takes	O
the	O
value	O
at	O
most	O
m	O
times	O
proof	O
define	O
y	O
e-xct	O
if	O
ex	O
y	O
ranges	O
from	O
to	O
by	O
lemma	O
gy	O
ai	O
ybi	O
takes	O
the	O
value	O
at	O
at	O
most	O
m	O
positive	O
y-values	O
unless	O
it	O
is	O
identically	O
zero	O
everywhere	O
this	O
concludes	O
the	O
proof	O
of	O
the	O
lemma	O
a	O
star-shaped	B
kernel	B
is	O
one	O
of	O
the	O
form	O
kx	O
lxea	O
where	O
a	O
is	O
a	O
star-shaped	B
set	O
of	O
unit	O
lebesgue	O
measure	O
that	O
is	O
x	O
f	O
a	O
implies	O
cx	O
f	O
a	O
for	O
all	O
c	O
it	O
is	O
clear	O
that	O
km	O
m	O
by	O
a	O
simple	O
thresholding	O
argument	O
on	O
the	O
real	O
line	O
the	O
kernel	B
kx	O
l-oo	O
for	O
ai	O
oscillates	O
infinitely	O
often	O
and	O
has	O
km	O
for	O
all	O
values	O
of	O
m	O
we	O
must	O
therefore	O
disallow	O
such	O
kernels	O
for	O
the	O
same	O
reason	O
kernels	O
such	O
as	O
k	O
xx	O
y	O
on	O
the	O
real	O
line	O
are	O
not	O
good	O
problem	O
if	O
k	O
ll	O
ai	O
hi	O
for	O
some	O
finite	O
k	O
some	O
numbers	O
ai	O
and	O
some	O
star-shaped	B
sets	O
ai	O
then	O
km	O
km	O
consider	O
next	O
kernels	O
of	O
the	O
form	O
where	O
a	O
is	O
star-shaped	B
and	O
r	O
is	O
a	O
constant	O
sebestyen	O
we	O
se	O
that	O
h	O
m	O
m	O
lyi	O
i	O
hr	O
iyi	O
xill-	O
r	O
kernel	B
complexity	I
which	O
changes	O
sign	O
at	O
most	O
as	O
often	O
as	O
f	O
m	O
hr	O
from	O
our	O
earlier	O
remarks	O
it	O
is	O
easy	O
to	O
see	O
that	O
km	O
m	O
as	O
km	O
is	O
the	O
same	O
as	O
for	O
the	O
kernel	B
k	O
i	O
a	O
if	O
a	O
is	O
replaced	O
by	O
nd	O
then	O
the	O
kernel	B
is	O
not	O
integrable	O
but	O
clearly	O
km	O
o	O
assume	O
next	O
that	O
we	O
have	O
kx	O
if	O
ilx	O
ii	O
otherwise	O
where	O
r	O
o	O
for	O
r	O
d	O
these	O
kernels	O
are	O
integrable	O
and	O
thus	O
regular	B
note	O
that	O
x	O
hr	O
k	O
h	O
illxiish	O
illxllh	O
iixll	O
r	O
as	O
h	O
increases	O
f	O
m	O
which	O
is	O
of	O
the	O
form	O
i	O
illx-xilish	O
ilx	O
xl	O
llr	O
i	O
illx-xillh	O
transfers	O
an	O
xi	O
from	O
one	O
sum	O
to	O
the	O
other	O
at	O
most	O
m	O
times	O
on	O
an	O
interval	O
on	O
which	O
no	O
such	O
transfer	O
occurs	O
fm	O
varies	O
as	O
a	O
r	O
and	O
has	O
at	O
most	O
one	O
sign	O
change	O
therefore	O
km	O
cannot	O
be	O
more	O
than	O
m	O
for	O
each	O
h-interval	O
plus	O
m	O
for	O
each	O
transfer	O
so	O
that	O
km	O
l	O
for	O
more	O
practice	O
with	O
such	O
computations	O
we	O
refer	O
to	O
the	O
exercises	O
we	O
now	O
continue	O
with	O
a	O
few	O
important	O
classes	O
of	O
kernels	O
consider	O
next	O
exponential	B
kernels	O
such	O
as	O
for	O
some	O
a	O
where	O
ii	O
ii	O
is	O
any	O
norm	O
on	O
nd	O
these	O
kernels	O
include	O
the	O
popular	O
gaussian	B
kernels	O
as	O
the	O
decision	O
rule	B
based	O
on	O
dm	O
is	O
of	O
the	O
form	O
a	O
simple	O
application	O
of	O
lemma	O
shows	O
that	O
km	O
m	O
the	O
entire	O
class	O
of	O
kernels	O
behaves	O
nicely	O
among	O
compact	O
support	B
kernels	O
kernels	O
of	O
the	O
form	O
kx	O
ai	O
iixll	O
b	O
iljxjjsli	O
for	O
real	O
numbers	O
ai	O
and	O
hi	O
are	O
important	O
a	O
particularly	O
popular	O
kernel	B
in	O
d-dimensional	O
density	B
estimation	B
is	O
deheuvels	O
kernel	B
if	O
the	O
kernel	B
was	O
kx	O
ai	O
ilx	O
ilhi	O
without	O
the	O
indicator	O
function	O
then	O
lemma	O
would	O
immediately	O
yield	O
the	O
estimate	O
km	O
k	O
uniformly	O
over	O
all	O
m	O
such	O
kernels	O
would	O
be	O
particularly	O
interesting	O
with	O
the	O
indicator	O
function	O
automatic	B
kernel	B
rules	O
multiplied	O
in	O
we	O
have	O
km	O
km	O
simply	O
because	O
fmx	O
at	O
each	O
h	O
is	O
based	O
upon	O
a	O
subset	O
of	O
the	O
xjs	O
j	O
m	O
with	O
the	O
subset	O
growing	O
monotonically	O
with	O
h	O
for	O
each	O
subset	O
the	O
function	O
fm	O
is	O
a	O
polynomial	B
in	O
iix	O
ii	O
with	O
powers	O
bi	O
bk	O
and	O
changes	O
sign	O
at	O
most	O
k	O
times	O
therefore	O
polynomial	B
kernels	O
of	O
compact	O
support	B
also	O
have	O
small	O
complexities	O
observe	O
that	O
the	O
in	O
the	O
bound	O
km	O
km	O
refers	O
to	O
the	O
number	O
of	O
terms	O
in	O
the	O
polynomial	B
and	O
not	O
the	O
maximal	O
power	O
a	O
large	O
class	O
of	O
kernels	O
of	O
finite	O
complexity	O
may	O
be	O
obtained	O
by	O
applying	O
the	O
rich	O
theory	O
of	O
total	B
positivity	I
see	O
karlin	O
for	O
a	O
thorough	O
treatment	O
a	O
valued	O
function	O
l	O
of	O
two	O
real	O
variables	O
is	O
said	O
to	O
be	O
totally	O
positive	O
on	O
a	O
x	O
b	O
c	O
if	O
for	O
all	O
n	O
and	O
all	O
sn	O
si	O
e	O
a	O
tl	O
tn	O
ti	O
e	O
b	O
the	O
determinant	O
of	O
the	O
matrix	O
with	O
elements	O
lsi	O
tj	O
is	O
nonnegative	O
a	O
key	O
property	O
of	O
such	O
functions	O
is	O
the	O
following	O
result	O
which	O
we	O
cite	O
without	O
proof	O
theorem	O
let	O
l	O
be	O
a	O
totally	O
positive	O
function	O
on	O
a	O
x	O
b	O
e	O
r	O
and	O
let	O
a	O
b	O
r	O
be	O
a	O
bounded	O
function	O
define	O
the	O
function	O
ls	O
tatadt	O
on	O
a	O
where	O
a	O
is	O
a	O
finite	O
measure	O
on	O
b	O
then	O
changes	O
sign	O
at	O
most	O
as	O
many	O
times	O
as	O
as	O
does	O
number	O
of	O
sign	O
changes	O
of	O
a	O
function	O
is	O
defined	O
as	O
the	O
supremum	O
of	O
sign	O
changes	O
of	O
sequences	O
of	O
the	O
form	O
where	O
n	O
is	O
arbitrary	O
and	O
sn	O
corollary	O
assume	O
that	O
the	O
kernel	B
k	O
is	O
such	O
that	O
the	O
function	O
ls	O
t	O
kst	O
is	O
totally	O
positive	O
for	O
s	O
and	O
t	O
e	O
rd	O
then	O
the	O
kernel	B
complexity	I
of	O
k	O
satisfies	O
km	O
m	O
proof	O
we	O
apply	O
theorem	O
we	O
are	O
interested	O
in	O
the	O
number	O
of	O
sign	O
changes	O
of	O
the	O
function	O
m	O
ikxi	O
xs	O
il	O
on	O
s	O
e	O
plays	O
the	O
role	O
of	O
h	O
but	O
may	O
be	O
written	O
as	O
ls	O
tatadt	O
where	O
ls	O
t	O
kst	O
the	O
measure	O
a	O
puts	O
mass	O
ion	O
each	O
t	O
xi	O
i	O
and	O
act	O
is	O
defined	O
at	O
these	O
points	O
as	O
a	O
yi	O
if	O
t	O
xi	O
x	O
other	O
values	O
of	O
act	O
are	O
irrelevant	O
for	O
the	O
integral	O
above	O
clearly	O
act	O
can	O
be	O
defined	O
such	O
that	O
it	O
changes	O
sign	O
at	O
most	O
m	O
times	O
then	O
theorem	O
implies	O
that	O
changes	O
sign	O
at	O
most	O
m	O
times	O
as	O
desired	O
this	O
corollary	O
equips	O
us	O
with	O
a	O
whole	O
army	O
of	O
kernels	O
with	O
small	O
complexity	O
for	O
examples	O
refer	O
to	O
the	O
monograph	O
of	O
karlin	O
multiparameter	B
kernel	B
rules	O
multiparameter	B
kernel	B
rules	O
assume	O
that	O
in	O
the	O
kernel	B
rules	O
considered	O
in	O
em	O
we	O
perform	O
an	O
optimization	O
with	O
respect	O
to	O
more	O
than	O
one	O
parameter	O
collect	O
these	O
parameters	O
in	O
e	O
and	O
write	O
the	O
discrimination	O
function	O
as	O
fmx	O
t	O
kex	O
xi	O
il	O
examples	O
product	B
kernels	O
take	O
kextik	O
h	O
d	O
jl	O
where	O
e	O
hd	O
is	O
a	O
vector	O
of	O
smoothing	O
factors-one	O
per	O
sion-and	O
k	O
is	O
a	O
fixed	O
one-dimensional	O
kernel	B
kernels	O
of	O
variable	B
form	O
define	O
kex	O
e-uxlllh	O
where	O
a	O
is	O
a	O
shape	O
parameter	O
and	O
h	O
is	O
the	O
standard	B
smoothing	O
parameter	O
here	O
e	O
h	O
is	O
two-dimensional	O
define	O
i	O
kex	O
rrtx	O
iirxii	O
where	O
x	O
t	O
is	O
the	O
transpose	O
of	O
x	O
and	O
r	O
is	O
an	O
orthogonal	O
transformation	O
matrix	O
all	O
of	O
whose	O
free	O
components	O
taken	O
together	O
are	O
collected	O
in	O
e	O
kernels	O
of	O
this	O
kind	O
may	O
be	O
used	O
to	O
adjust	O
automatically	O
to	O
a	O
certain	O
variance-covariance	O
structure	O
in	O
the	O
data	O
we	O
will	O
not	O
spend	O
a	O
lot	O
of	O
time	O
on	O
these	O
cases	O
clearly	O
one	O
route	O
is	O
to	O
properly	O
generalize	O
the	O
definition	B
of	I
kernel	B
complexity	I
in	O
some	O
cases	O
it	O
is	O
more	O
convenient	O
to	O
directly	O
find	O
upper	O
bounds	O
for	O
seem	O
l	O
in	O
the	O
product	B
kernel	B
case	O
with	O
dimensional	O
kernel	B
we	O
claim	O
for	O
example	O
that	O
seem	O
i	O
the	O
corresponding	O
rule	B
takes	O
a	O
majority	B
vote	I
over	O
centered	O
rectangles	O
with	O
sides	O
equal	O
to	O
to	O
see	O
why	O
the	O
inequality	B
is	O
true	O
consider	O
the	O
dimensional	O
quadrant	O
of	O
points	O
obtained	O
by	O
taking	O
the	O
absolute	O
values	O
of	O
the	O
vectors	O
xj	O
xi	O
m	O
j	O
m	O
i	O
n	O
i	O
m	O
where	O
the	O
absolute	O
value	O
of	O
a	O
vector	O
is	O
a	O
vector	O
whose	O
components	O
are	O
the	O
absolute	O
values	O
of	O
the	O
components	O
ofthe	O
vector	O
to	O
compute	O
seem	O
l	O
it	O
suffices	O
to	O
count	O
how	O
many	O
different	O
subsets	O
can	O
be	O
obtained	O
from	O
these	O
lm	O
points	O
by	O
considering	O
all	O
possible	O
rectangles	O
with	O
one	O
vertex	O
at	O
the	O
origin	O
and	O
the	O
diagonally	O
opposite	O
vertex	O
in	O
the	O
quadrant	O
this	O
is	O
the	O
strong	B
universal	B
consistency	B
of	O
the	O
latter	O
family	O
em	O
is	O
insured	O
when	O
m	O
and	O
i	O
log	O
n	O
automatic	B
kernel	B
rules	O
kernels	O
of	O
infinite	O
complexity	O
in	O
this	O
section	O
we	O
demonstrate	O
that	O
not	O
every	O
kernel	B
function	O
supports	O
smoothing	B
factor	I
selection	O
based	O
on	O
data	O
splitting	O
these	O
kernels	O
have	O
infinite	O
kernel	B
plexity	O
or	O
even	O
worse	O
infinite	O
vc	B
dimension	B
some	O
of	O
the	O
examples	O
may	O
appear	O
rather	O
artificial	O
but	O
some	O
kernels	O
will	O
surprise	O
us	O
by	O
misbehaving	O
we	O
begin	O
with	O
a	O
kernel	B
k	O
having	O
the	O
property	O
that	O
the	O
class	O
of	O
sets	O
has	O
vc	B
dimension	B
va	O
for	O
any	O
fixed	O
value	O
of	O
xl	O
using	O
the	O
notation	O
of	O
the	O
previous	O
sections	O
vel	O
hence	O
with	O
one	O
sample	O
point	O
all	O
hope	O
is	O
lost	O
to	O
use	O
the	O
vapnik-chervonenkis	B
inequality	B
in	O
any	O
meaningful	O
way	O
unfortunately	O
the	O
kernel	B
k	O
takes	O
alternately	O
positive	O
and	O
negative	O
values	O
in	O
the	O
second	O
part	O
of	O
this	O
section	O
a	O
kernel	B
is	O
constructed	O
that	O
is	O
unimodal	O
and	O
symmetric	O
and	O
has	O
vem	O
for	O
m	O
when	O
xl	O
takes	O
certain	O
values	O
finally	O
in	O
the	O
last	O
part	O
we	O
construct	O
a	O
positive	O
kernel	B
with	O
the	O
property	O
that	O
for	O
any	O
m	O
and	O
any	O
nondegenerate	O
non	O
atomic	O
distribution	O
limmoo	O
p	O
oo	O
we	O
return	O
to	O
a	O
our	O
function	O
is	O
picked	O
as	O
follows	O
kx	O
axgi	O
s	O
x	O
il	O
i	O
where	O
gi	O
e	O
i	O
for	O
all	O
i	O
ax	O
for	O
all	O
x	O
ax	O
as	O
x	O
t	O
x	O
and	O
ax	O
as	O
x	O
x	O
monotonicity	O
is	O
not	O
essential	O
and	O
may	O
be	O
dropped	O
but	O
the	O
resulting	O
class	O
of	O
kernels	O
will	O
be	O
even	O
less	O
esting	O
we	O
enumerate	O
all	O
binary	B
strings	O
in	O
lexicographical	O
order	O
replace	O
all	O
os	O
by	O
and	O
map	O
the	O
bit	O
sequence	O
to	O
gl	O
hence	O
the	O
bit	O
sequence	O
becomes	O
call	O
this	O
sequence	O
s	O
for	O
every	O
n	O
we	O
can	O
find	O
a	O
set	O
i	O
x	O
n	O
that	O
can	O
be	O
shattered	O
by	O
sets	O
from	O
a	O
take	O
xl	O
a	O
subset	O
of	O
this	O
may	O
be	O
characterized	O
by	O
a	O
string	O
sn	O
from	O
denoting	O
membership	O
and	O
denoting	O
absence	O
we	O
find	O
the	O
first	O
occurrence	O
of	O
sn	O
in	O
s	O
and	O
let	O
the	O
starting	O
point	O
be	O
gk	O
take	O
h	O
observe	O
that	O
k	O
x	O
i	O
k	O
xn	O
x	O
i	O
n	O
which	O
agrees	O
in	O
sign	O
with	O
sn	O
as	O
desired	O
hence	O
the	O
vc	B
dimension	B
is	O
infinite	O
the	O
next	O
kernel	B
is	O
symmetric	O
unimodal	O
and	O
piecewise	O
quadratic	O
the	O
intervals	O
into	O
which	O
is	O
divided	O
are	O
denoted	O
by	O
ao	O
ai	O
from	O
left	O
to	O
right	O
kernels	O
of	O
infinite	O
complexity	O
where	O
i	O
i	O
o	O
on	O
each	O
ai	O
k	O
is	O
of	O
the	O
form	O
ax	O
bx	O
c	O
observe	O
that	O
kf	O
takes	O
the	O
sign	O
of	O
a	O
also	O
any	O
finite	O
symmetric	O
difference	O
of	O
order	O
has	O
the	O
sign	O
of	O
a	O
as	O
k	O
k	O
x	O
x	O
x	O
e	O
ai	O
we	O
take	O
four	O
points	O
and	O
construct	O
the	O
class	O
a	O
t	O
k	O
x	O
x	O
o	O
h	O
o	O
where	O
xl	O
yi	O
are	O
fixed	O
for	O
now	O
on	O
ai	O
we	O
let	O
the	O
coefficient	O
have	O
the	O
same	O
sign	O
as	O
gi	O
known	O
from	O
the	O
previous	O
example	O
all	O
three	O
quadratic	O
coefficients	O
are	O
picked	O
so	O
that	O
k	O
and	O
k	O
is	O
unimodal	O
for	O
each	O
n	O
we	O
show	O
that	O
the	O
set	O
can	O
be	O
shattered	O
by	O
intersecting	O
with	O
sets	O
from	O
a	O
subset	O
is	O
again	O
identified	O
by	O
a	O
ln-valued	O
string	O
sn	O
and	O
its	O
first	O
match	O
in	O
sis	O
gk	O
gk	O
n	O
we	O
take	O
h	O
note	O
that	O
for	O
i	O
n	O
sign	O
k	O
k	O
sign	O
k	O
l	O
sign	O
by	O
the	O
finite-difference	O
property	O
of	O
quadratics	O
gki	O
as	O
desired	O
hence	O
any	O
subset	O
of	O
can	O
be	O
picked	O
out	O
the	O
previous	O
example	O
works	O
whenever	O
it	O
takes	O
just	O
a	O
little	O
thought	O
to	O
see	O
that	O
if	O
yr	O
are	O
i	O
i	O
d	O
and	O
drawn	O
from	O
the	O
distribution	O
of	O
y	O
then	O
p	O
ivai	O
oo	O
pxi	O
yl	O
yz	O
i	O
and	O
this	O
is	O
positive	O
if	O
given	O
y	O
x	O
has	O
atoms	O
at	O
and	O
for	O
some	O
and	O
given	O
y	O
x	O
has	O
an	O
atom	O
at	O
o	O
however	O
with	O
some	O
work	O
we	O
may	O
even	O
remove	O
these	O
restrictions	O
problem	O
we	O
also	O
draw	O
the	O
readers	O
attention	O
to	O
an	O
example	O
in	O
which	O
k	O
is	O
metric	O
unimodal	O
and	O
convex	O
on	O
yet	O
ves	O
problem	O
next	O
we	O
turn	O
to	O
our	O
general	O
m-point	O
example	O
let	O
the	O
class	O
of	O
rules	O
be	O
given	O
by	O
g	O
mh	O
if	O
i	O
otherwise	O
i	O
lk	O
h	O
automatic	B
kernel	B
rules	O
h	O
where	O
the	O
data	O
yi	O
ym	O
are	O
fixed	O
for	O
now	O
we	O
exhibit	O
a	O
nonnegative	O
kernel	B
such	O
that	O
if	O
the	O
xis	O
are	O
different	O
and	O
not	O
all	O
the	O
yis	O
are	O
the	O
same	O
vcm	O
this	O
situation	O
occurs	O
with	O
probability	O
tending	O
to	O
one	O
whenever	O
x	O
is	O
nonatomic	O
and	O
py	O
i	O
e	O
it	O
is	O
stressed	O
here	O
that	O
the	O
same	O
kernel	B
k	O
is	O
used	O
regardless	O
of	O
the	O
data	O
the	O
kernel	B
is	O
of	O
the	O
form	O
kx	O
koxiax	O
where	O
kox	O
and	O
a	O
c	O
r	O
will	O
be	O
specially	O
picked	O
order	O
x	O
i	O
xm	O
into	O
x	O
cm	O
and	O
find	O
the	O
first	O
pair	O
xi	O
xui	O
with	O
opposite	O
values	O
for	O
yi	O
without	O
loss	O
of	O
generality	O
assume	O
that	O
xci	O
and	O
is	O
the	O
y-value	O
that	O
corresponds	O
to	O
xci	O
let	O
the	O
smallest	O
value	O
of	O
j	O
i	O
j	O
i	O
be	O
denoted	O
by	O
the	O
contribution	O
of	O
all	O
such	O
xjs	O
for	O
x	O
e	O
is	O
not	O
more	O
than	O
h	O
mkol	O
h	O
the	O
contribution	O
of	O
either	O
xci	O
or	O
xil	O
is	O
at	O
least	O
h	O
for	O
fixed	O
and	O
m	O
all	O
they	O
are	O
given	O
we	O
first	O
find	O
h	O
such	O
that	O
h	O
h	O
implies	O
ko	O
h	O
h	O
for	O
h	O
h	O
the	O
rule	B
for	O
x	O
e	O
is	O
equivalent	O
to	O
a	O
rule	B
based	O
on	O
if	O
k	O
ezi	O
k	O
e-i	O
gmh	O
otherwise	O
we	O
define	O
the	O
set	O
a	O
by	O
au	O
u	O
kl	O
kl	O
where	O
the	O
sets	O
akl	O
are	O
specified	O
later	O
c	O
and	O
b	O
c	O
r	O
c	O
b	O
e	O
r	O
x	O
c	O
e	O
b	O
here	O
k	O
represents	O
the	O
length	O
of	O
a	O
bit	O
string	O
and	O
i	O
cycles	O
through	O
all	O
bit	O
strings	O
of	O
length	O
k	O
for	O
a	O
particular	O
such	O
bit	O
string	O
bi	O
bk	O
represented	O
by	O
l	O
we	O
define	O
akl	O
as	O
follows	O
first	O
of	O
all	O
akl	O
so	O
that	O
all	O
sets	O
akl	O
are	O
nonoverlapping	O
akl	O
consists	O
of	O
the	O
sets	O
u	O
j	O
akl	O
is	O
completed	O
by	O
symmetry	O
we	O
now	O
exhibit	O
for	O
each	O
integer	O
k	O
a	O
set	O
of	O
that	O
size	O
that	O
can	O
be	O
shattered	O
these	O
sets	O
will	O
be	O
positioned	O
in	O
at	O
coordinate	O
values	O
p	O
where	O
p	O
is	O
a	O
suitable	O
large	O
integer	O
such	O
that	O
assume	O
that	O
we	O
wish	O
to	O
extract	O
the	O
set	O
indexed	O
by	O
the	O
bit	O
vector	O
bk	O
means	O
that	O
must	O
be	O
extracted	O
to	O
do	O
so	O
we	O
are	O
only	O
allowed	O
to	O
vary	O
h	O
first	O
we	O
find	O
a	O
pair	O
i	O
where	O
k	O
is	O
at	O
least	O
k	O
and	O
on	O
minimizing	O
the	O
apparent	O
error	O
rate	O
s	O
h	O
is	O
needed	O
too	O
also	O
is	O
such	O
that	O
it	O
matches	O
bi	O
h	O
in	O
its	O
first	O
k	O
s	O
k	O
bits	O
take	O
h	O
and	O
p	O
k	O
and	O
observe	O
that	O
gnh	O
k	O
if	O
k	O
otherwise	O
c-	O
c-	O
if	O
e	O
akl	O
if	O
akl	O
unimportant	O
otherwise	O
if	O
bi	O
if	O
bi	O
akl	O
e	O
akl	O
i	O
s	O
k	O
thus	O
we	O
pick	O
the	O
desired	O
set	O
this	O
construction	O
may	O
be	O
repeated	O
for	O
all	O
values	O
of	O
of	O
course	O
we	O
have	O
shown	O
the	O
following	O
theorem	O
if	O
x	O
is	O
nonatomic	O
py	O
i	O
e	O
and	O
if	O
vcm	O
denotes	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
of	B
kernel	B
rules	I
based	O
on	O
m	O
i	O
i	O
d	O
data	O
drawn	O
from	O
the	O
distribution	O
of	O
y	O
and	O
with	O
the	O
kernel	B
specified	O
above	O
then	O
lim	O
pvcm	O
oo	O
m-oo	O
on	O
minimizing	O
the	O
apparent	O
error	O
rate	O
in	O
this	O
section	O
we	O
look	O
more	O
closely	O
at	O
kernel	B
rules	O
that	O
are	O
picked	O
by	O
minimizing	O
the	O
resubstitution	B
estimate	O
lr	O
over	O
kernel	B
rules	O
with	O
smoothing	B
factor	I
h	O
o	O
we	O
make	O
two	O
remarks	O
in	O
this	O
respect	O
the	O
procedure	O
is	O
generally	O
inconsistent	O
if	O
x	O
is	O
nonatomic	O
the	O
method	O
is	O
consistent	O
if	O
x	O
is	O
purely	O
atomic	O
to	O
see	O
take	O
k	O
lsod	O
we	O
note	O
that	O
lr	O
if	O
h	O
miniij	O
iixi	O
xj	O
ii	O
as	O
gnxi	O
yi	O
for	O
such	O
h	O
in	O
fact	O
if	O
hn	O
is	O
the	O
minimizing	O
h	O
it	O
may	O
take	O
any	O
value	O
on	O
if	O
x	O
is	O
independent	O
of	O
y	O
py	O
i	O
and	O
x	O
has	O
a	O
density	O
then	O
lim	O
lim	O
inf	O
p	O
ii	O
xi	O
x	O
j	O
ii	O
d	O
c--oo	O
n--oo	O
lj	O
y-lyj-o	O
n	O
thus	O
nhd	O
in	O
probability	O
and	O
therefore	O
in	O
this	O
case	O
elgj	O
ties	O
are	O
broken	O
by	O
favoring	O
the	O
class	O
hence	O
the	O
inconsistency	O
automatic	B
kernel	B
rules	O
consider	O
next	O
x	O
purely	O
atomic	O
fix	O
yd	O
yn	O
the	O
data	O
and	O
sider	O
the	O
class	O
en	O
of	B
kernel	B
rules	I
in	O
which	O
h	O
is	O
a	O
free	O
parameter	O
if	O
a	O
typical	O
rule	B
is	O
gnh	O
let	O
an	O
be	O
the	O
class	O
of	O
sets	O
gnhx	O
i	O
h	O
o	O
if	O
gn	O
is	O
the	O
rule	B
that	O
minimizes	O
lrgnh	O
we	O
have	O
sup	O
gizec	O
sup	O
sup	O
aean	O
denotes	O
the	O
collection	O
of	O
sets	O
gnhx	O
od	O
aea	O
sup	O
beb	O
where	O
b	O
is	O
the	O
collection	O
of	O
all	O
borel	O
sets	O
however	O
the	O
latter	O
quantity	O
tends	O
to	O
zero	O
with	O
probability	O
one-as	O
denoting	O
the	O
set	O
of	O
the	O
atoms	O
of	O
x	O
by	O
t	O
we	O
have	O
sup	O
beb	O
l	O
l	O
tnx	O
d	O
d	O
xet	O
l	O
xea	O
flxdi	O
c	O
flac	O
a	O
is	O
an	O
arbitrary	O
finite	O
subset	O
of	O
t	O
l	O
xea	O
the	O
first	O
term	O
is	O
small	O
by	O
choice	O
of	O
a	O
and	O
the	O
last	O
two	O
terms	O
are	O
small	O
by	O
applying	O
hoeffdings	O
inequality	B
to	O
each	O
of	O
the	O
terms	O
for	O
yet	O
a	O
different	O
proof	O
the	O
reader	O
is	O
referred	O
to	O
problems	O
and	O
note	O
next	O
that	O
but	O
if	O
ko	O
and	O
kx	O
as	O
iixll	O
along	O
any	O
ray	O
then	O
we	O
have	O
infgnhecn	O
lgnh	O
lgl	O
where	O
gz	O
is	O
the	O
fundamental	B
rule	B
discussed	O
in	O
chapter	O
which	O
h	O
theorem	O
shows	O
that	O
l	O
with	O
probability	O
one	O
and	O
therefore	O
lgn	O
with	O
probability	O
one	O
proving	O
theorem	O
take	O
any	O
kernel	B
k	O
with	O
ko	O
and	O
kx	O
as	O
i	O
xll	O
along	O
any	O
ray	O
and	O
let	O
gn	O
be	O
selected	O
by	O
minimizing	O
lr	O
over	O
all	O
h	O
o	O
then	O
lgn	O
l	O
almost	O
surely	O
whenever	O
the	O
distribution	O
of	O
x	O
is	O
purely	O
atomic	O
minimizing	O
the	O
deleted	O
estimate	O
remark	O
the	O
above	O
theorem	O
is	O
all	O
the	O
more	O
surprising	O
since	O
we	O
may	O
take	O
just	O
about	O
any	O
kernel	B
such	O
as	O
kx	O
e-	O
kx	O
siniix	O
ii	O
or	O
kx	O
also	O
if	O
x	O
puts	O
its	O
mass	O
on	O
a	O
dense	O
subset	O
of	O
n	O
d	O
the	O
data	O
will	O
look	O
and	O
feel	O
like	O
data	O
from	O
an	O
absolutely	O
continuous	O
distribution	O
very	O
roughly	O
speaking	O
yet	O
there	O
is	O
a	O
dramatic	O
difference	O
with	O
rules	O
that	O
minimize	O
lr	O
when	O
the	O
xs	O
are	O
indeed	O
drawn	O
from	O
a	O
distribution	O
with	O
a	O
density	O
minimizing	O
the	O
deleted	O
estimate	O
we	O
have	O
seen	O
in	O
chapter	O
that	O
the	O
deleted	O
estimate	O
of	O
the	O
error	O
probability	O
of	O
a	O
kernel	B
rule	B
is	O
generally	O
very	O
reliable	O
this	O
suggests	O
using	O
the	O
estimate	O
as	O
a	O
basis	O
of	O
selecting	O
a	O
smoothing	B
factor	I
for	O
the	O
kernel	B
rule	B
let	O
ldgnh	O
be	O
the	O
deleted	O
estimate	O
of	O
lgnh	O
obtained	O
by	O
using	O
in	O
gn-lh	O
the	O
same	O
kernel	B
k	O
and	O
smoothing	B
factor	I
h	O
define	O
the	O
set	O
of	O
h	O
s	O
for	O
which	O
l	O
n	O
gnh	O
heooo	O
f	O
ld	O
in	O
gnh	O
by	O
a	O
set	O
hn	O
inf	O
h	O
e	O
a	O
two	O
fundamental	O
questions	O
regarding	O
hn	O
must	O
be	O
asked	O
if	O
we	O
use	O
hn	O
in	O
the	O
kernel	B
estimate	O
is	O
the	O
rule	B
universally	O
consistent	O
note	O
in	O
this	O
respect	O
that	O
theorem	O
cannot	O
be	O
used	O
because	O
it	O
is	O
not	O
true	O
that	O
hn	O
in	O
probability	O
in	O
all	O
cases	O
if	O
hn	O
is	O
used	O
as	O
smoothing	B
factor	I
how	O
does	O
elgn	O
l	O
compare	O
to	O
e	O
lgnlj	O
l	O
to	O
our	O
knowledge	O
both	O
questions	O
have	O
been	O
unanswered	O
so	O
far	O
we	O
believe	O
that	O
this	O
way	O
of	O
selecting	O
h	O
is	O
very	O
effective	O
below	O
generalizing	O
a	O
result	O
by	O
tutz	O
we	O
show	O
that	O
the	O
kernel	B
rule	B
obtained	O
by	O
minimizing	O
the	O
deleted	O
estimate	O
is	O
consistent	O
when	O
the	O
distribution	O
of	O
x	O
is	O
atomic	O
with	O
finitely	O
many	O
atoms	O
remark	O
if	O
i	O
everywhere	O
so	O
that	O
yi	O
i	O
for	O
all	O
i	O
and	O
if	O
k	O
has	O
support	B
equal	O
to	O
the	O
unit	O
ball	O
in	O
n	O
d	O
and	O
k	O
on	O
this	O
ball	O
then	O
ldgnh	O
is	O
minimal	O
and	O
zero	O
if	O
where	O
x	O
f	O
n	O
denotes	O
the	O
nearest	B
neighbor	I
of	O
x	O
j	O
among	O
the	O
data	O
points	O
xl	O
xj-l	O
xjl	O
x	O
n	O
but	O
this	O
shows	O
that	O
hn	O
mx	O
iixj	O
xfnii	O
l-s-sn	O
automatic	B
kernel	B
rules	O
we	O
leave	O
it	O
as	O
an	O
exercise	O
to	O
show	O
that	O
for	O
any	O
nonatomic	O
distribution	O
of	O
x	O
n	O
hl	O
with	O
probability	O
one	O
however	O
it	O
is	O
also	O
true	O
that	O
for	O
some	O
distributions	O
hn	O
with	O
probability	O
one	O
nevertheless	O
for	O
the	O
rule	B
is	O
consistent	O
if	O
everywhere	O
then	O
yi	O
for	O
all	O
i	O
under	O
the	O
same	O
condition	O
on	O
k	O
as	O
above	O
with	O
tie	B
breaking	I
in	O
favor	O
of	O
class	O
in	O
the	O
entire	O
book	O
it	O
is	O
clear	O
that	O
hn	O
o	O
interestingly	O
here	O
too	O
the	O
rule	B
is	O
consistent	O
despite	O
the	O
strange	O
value	O
of	O
hn	O
o	O
we	O
state	O
the	O
following	O
theorem	O
for	O
window	O
kernels	O
though	O
it	O
can	O
be	O
extended	O
to	O
include	O
kernels	O
taking	O
finitely	O
many	O
different	O
values	O
problem	O
theorem	O
let	O
gn	O
be	O
the	O
kernel	B
rule	B
with	O
kernel	B
k	O
iafor	O
some	O
bounded	O
set	O
a	O
c	O
nd	O
containing	O
the	O
origin	O
and	O
with	O
smoothing	B
factor	I
chosen	O
by	O
minimizing	O
the	O
deleted	O
estimate	O
as	O
described	O
above	O
then	O
e	O
lgn	O
l	O
whenever	O
x	O
has	O
a	O
discrete	O
distribution	O
with	O
finitely	O
many	O
atoms	O
proof	O
let	O
ui	O
un	O
be	O
independent	O
uniform	O
random	O
variables	O
dent	O
of	O
the	O
data	O
d	O
n	O
and	O
consider	O
the	O
augmented	O
sample	O
where	O
x	O
vi	O
for	O
each	O
h	O
introduce	O
the	O
rule	B
ghx	O
if	O
lil	O
k	O
lil	O
o	O
otherwise	O
liuui	O
where	O
xl	O
u	O
e	O
nd	O
x	O
observe	O
that	O
minimizing	O
the	O
resubstitution	B
estimate	O
lrgh	O
over	O
these	O
rules	O
yields	O
the	O
same	O
h	O
as	O
minimizing	O
ldgnh	O
over	O
the	O
original	O
kernel	B
rules	O
furthermore	O
e	O
lgn	O
e	O
lg	O
if	O
g	O
is	O
obtained	O
by	O
minimizing	O
lrglh	O
it	O
follows	O
from	O
the	O
universal	B
consistency	B
of	B
kernel	B
rules	I
that	O
lim	O
inf	O
lg	O
h	O
l	O
with	O
probability	O
one	O
n--oo	O
h	O
thus	O
it	O
suffices	O
to	O
investigate	O
lg	O
subsets	O
al	O
c	O
nd	O
x	O
by	O
infh	O
lgh	O
for	O
a	O
given	O
d	O
define	O
the	O
a	O
ghxi	O
i	O
h	O
e	O
arguing	O
as	O
in	O
the	O
previous	O
section	O
we	O
see	O
that	O
minimizing	O
the	O
deleted	O
estimate	O
where	O
v	O
is	O
the	O
measure	O
of	O
x	O
on	O
nd	O
x	O
and	O
vn	O
is	O
the	O
empirical	B
measure	I
determined	O
by	O
d	O
observe	O
that	O
each	O
a	O
can	O
be	O
written	O
as	O
ah	O
e	O
rd	O
t	O
k	O
e	O
xi	O
o	O
u	O
end	O
x	O
un	O
t	O
k	O
e	O
xi	O
ko	O
liuu	O
where	O
and	O
clearly	O
as	O
it	O
suffices	O
to	O
prove	O
that	O
suph	O
in	O
probability	O
as	O
n	O
then	O
sup	O
ivnb	O
h	O
sup	O
ivnb	O
sup	O
ivcb	O
h	O
h	O
sup	O
ivncb	O
sup	O
ivb	O
mahl	O
h	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
tends	O
to	O
zero	O
in	O
probability	O
which	O
may	O
be	O
seen	O
by	O
problem	O
and	O
the	O
independence	O
of	O
the	O
ui	O
s	O
of	O
dn	O
to	O
bound	O
the	O
second	O
term	O
observe	O
that	O
for	O
each	O
h	O
ivb	O
fl	O
it	O
k	O
e	O
xi	O
o	O
-	O
ko	O
t	O
px	O
xj	O
k	O
xjxi	O
n	O
where	O
x	O
i	O
x	O
n	O
are	O
the	O
atoms	O
of	O
x	O
thus	O
the	O
proof	O
is	O
finished	O
if	O
we	O
show	O
that	O
for	O
each	O
xj	O
automatic	B
kernel	B
rules	O
now	O
we	O
exploit	O
the	O
special	O
form	O
k	O
fa	O
of	O
the	O
kernel	B
observe	O
that	O
ii	O
i	O
h	O
l	O
ll	O
a	O
su	O
p	O
fin	O
i	O
n	O
l	O
fivivk	O
isi	O
where	O
kl	O
vk	O
l	O
ixixk	O
and	O
xl	O
is	O
an	O
ordering	O
of	O
the	O
atoms	O
of	O
x	O
such	O
that	O
x	O
j	O
and	O
xk	O
h	O
e	O
a	O
implies	O
xk-l	O
h	O
e	O
a	O
for	O
k	O
n	O
by	O
properties	O
of	O
the	O
binomial	B
distribution	I
as	O
n	O
this	O
completes	O
the	O
proof	O
historical	O
remark	O
in	O
an	O
early	O
paper	O
by	O
habbema	O
hermans	O
and	O
van	O
den	O
broek	O
the	O
deleted	O
estimate	O
is	O
used	O
to	O
select	O
an	O
appropriate	O
subspace	O
for	O
the	O
kernel	B
rule	B
the	O
kernel	B
rules	O
in	O
tum	O
have	O
smoothing	O
factors	O
that	O
are	O
selected	O
by	O
maximum	B
likelihood	I
sieve	O
methods	O
sieve	O
methods	O
pick	O
a	O
best	O
estimate	O
or	O
rule	B
from	O
a	O
limited	O
class	O
of	O
rules	O
for	O
example	O
our	O
sieve	O
ck	O
might	O
consist	O
of	O
rules	O
of	O
the	O
form	O
where	O
the	O
ai	O
are	O
real	O
numbers	O
the	O
xi	O
are	O
points	O
from	O
n	O
d	O
and	O
the	O
his	O
are	O
positive	O
numbers	O
there	O
are	O
formally	O
free	O
scalar	O
parameters	O
if	O
we	O
pick	O
a	O
rule	B
that	O
minimizes	O
the	O
empirical	B
error	I
on	O
yi	O
yn	O
we	O
are	O
governed	O
by	O
the	O
theorems	O
of	O
chapters	O
and	O
and	O
we	O
will	O
need	O
to	O
find	O
the	O
vc	B
dimension	B
of	O
ck	O
for	O
this	O
conditions	O
on	O
k	O
will	O
be	O
needed	O
we	O
will	O
return	O
to	O
this	O
question	O
in	O
chapter	O
on	O
neural	O
networks	O
as	O
they	O
are	O
closely	O
related	O
to	O
the	O
sieves	O
described	O
here	O
squared	O
eltor	O
minimization	O
squared	B
error	I
minimization	O
many	O
researchers	O
have	O
considered	O
the	O
problem	O
of	O
selecting	O
h	O
in	O
order	O
to	O
minimize	O
the	O
error	O
squared	B
error	I
of	O
the	O
kernel	B
estimate	O
y	O
k	O
n	O
l	O
ll	O
k	O
n	O
l	O
ll	O
h	O
h	O
of	O
the	O
regression	B
function	I
p	O
x	O
x	O
for	O
example	O
hardie	O
and	O
marron	O
proposed	O
and	O
studied	O
a	O
cross-validation	B
method	O
for	O
choosing	O
the	O
optimal	O
h	O
for	O
the	O
kernel	B
regression	O
estimate	O
they	O
obtain	O
asymptotic	B
optimality	I
for	O
the	O
integrated	O
squared	B
error	I
although	O
their	O
method	O
gives	O
us	O
a	O
choice	O
for	O
h	O
if	O
we	O
consider	O
py	O
llx	O
x	O
as	O
the	O
regression	B
function	I
it	O
is	O
not	O
clear	O
that	O
the	O
h	O
thus	O
obtained	O
is	O
optimal	O
for	O
the	O
probability	O
of	O
error	O
in	O
fact	O
as	O
the	O
following	O
theorem	O
illustrates	O
for	O
some	O
distributions	O
the	O
smoothing	O
parameter	O
that	O
minimizes	O
the	O
error	O
yields	O
a	O
rather	O
poor	O
error	O
probability	O
compared	O
to	O
that	O
corresponding	O
to	O
the	O
optimal	O
h	O
theorem	O
let	O
d	O
l	O
consider	O
the	O
kernel	B
classification	O
rule	B
with	O
the	O
window	O
kernel	B
k	O
and	O
smoothing	O
parameter	O
h	O
o	O
denote	O
its	O
error	O
probability	O
by	O
lnh	O
let	O
h	O
be	O
the	O
smoothing	O
parameter	O
that	O
minimizes	O
the	O
mean	O
integrated	O
squared	B
error	I
then	O
for	O
some	O
distributions	O
elnh	O
hm	O
n-oo	O
infh	O
elnh	O
and	O
the	O
convergence	O
is	O
exponentially	O
fast	O
we	O
leave	O
the	O
details	O
of	O
the	O
proof	O
to	O
the	O
reader	O
only	O
a	O
rough	O
sketch	O
is	O
given	O
here	O
consider	O
x	O
uniform	O
on	O
u	O
and	O
define	O
if	O
x	O
e	O
x	O
otherwise	O
i	O
x	O
the	O
optimal	O
value	O
of	O
h	O
the	O
value	O
minimizing	O
the	O
error	O
probability	O
is	O
one	O
it	O
is	O
constant	O
independent	O
of	O
n	O
this	O
shows	O
that	O
we	O
should	O
not	O
a	O
priori	O
exclude	O
any	O
values	O
of	O
h	O
as	O
is	O
commonly	O
done	O
in	O
studies	O
on	O
regression	O
and	O
density	B
estimation	B
the	O
minimal	O
error	O
probability	O
can	O
be	O
bounded	O
from	O
above	O
hoeffdings	O
inequality	B
by	O
e-	O
for	O
some	O
constant	O
cl	O
on	O
the	O
other	O
hand	O
straightforward	O
calculations	O
show	O
that	O
the	O
smoothing	B
factor	I
h	O
that	O
minimizes	O
the	O
mean	O
integrated	O
automatic	B
kernel	B
rules	O
squared	B
error	I
goes	O
to	O
zero	O
as	O
n	O
as	O
i	O
the	O
corresponding	O
error	O
probability	O
is	O
larger	O
than	O
for	O
some	O
constant	O
the	O
order-of-magnitude	O
difference	O
between	O
the	O
exponents	O
explains	O
the	O
exponential	B
speed	O
of	O
convergence	O
to	O
infinity	O
problems	O
and	O
exercises	O
problem	O
prove	O
theorem	O
problem	O
show	O
that	O
for	O
any	O
r	O
the	O
kernel	B
kx	O
if	O
ilxll	O
s	O
otherwise	O
satisfies	O
the	O
conditions	O
of	O
theorem	O
problem	O
assume	O
that	O
k	O
is	O
a	O
regular	B
kernel	B
with	O
kernel	B
complexity	I
km	O
s	O
my	O
for	O
some	O
constant	O
y	O
let	O
gn	O
be	O
selected	O
so	O
as	O
to	O
minimize	O
the	O
holdout	B
error	O
estimate	O
over	O
cm	O
the	O
class	O
of	B
kernel	B
rules	I
based	O
upon	O
the	O
first	O
m	O
data	O
points	O
with	O
smoothing	B
factor	I
h	O
assume	O
furthermore	O
that	O
we	O
vary	O
lover	O
n	O
and	O
that	O
we	O
pick	O
the	O
best	O
l	O
m	O
n	O
by	O
minimizing	O
the	O
holdout	B
error	O
estimate	O
again	O
show	O
that	O
the	O
obtained	O
rule	B
is	O
strongly	O
universally	O
consistent	O
problem	O
prove	O
that	O
the	O
kernel	B
complexity	I
km	O
of	O
the	O
de	B
la	I
vallee-poussin	I
kernel	B
kx	O
x	O
e	O
r	O
is	O
infinite	O
when	O
m	O
problem	O
show	O
that	O
km	O
for	O
m	O
when	O
kx	O
x	O
e	O
r	O
problem	O
compute	O
an	O
upper	O
bound	O
for	O
the	O
kernel	B
complexity	I
km	O
for	O
the	O
following	O
kernels	O
where	O
x	O
xed	O
e	O
rd	O
a	O
b	O
c	O
d	O
kx	O
d	O
ti	O
il	O
i-x	O
iuxiil	O
d	O
kx	O
ex	O
kx	O
ti	O
kx	O
ti	O
il	O
d	O
il	O
problem	O
can	O
you	O
construct	O
a	O
kernel	B
on	O
r	O
with	O
the	O
property	O
that	O
its	O
complexity	O
satisfies	O
s	O
km	O
for	O
all	O
m	O
prove	O
your	O
claim	O
problem	O
show	O
that	O
for	O
kernel	B
classes	O
cm	O
with	O
kernel	B
rules	O
having	O
a	O
fixed	O
training	O
sequence	O
dm	O
but	O
variable	B
h	O
we	O
have	O
vcm	O
s	O
km	O
problems	O
and	O
exercises	O
problem	O
calculate	O
upper	O
bounds	O
for	O
sem	O
when	O
em	O
is	O
the	O
class	O
of	B
kernel	B
rules	I
based	O
on	O
fixed	O
training	O
data	O
but	O
with	O
variable	B
parameter	O
e	O
in	O
the	O
following	O
cases	O
definitions	O
see	O
section	O
kg	O
is	O
a	O
product	B
kernel	B
of	O
r	O
d	O
where	O
the	O
unidimensional	O
kernel	B
k	O
has	O
kernel	B
complexity	I
km	O
kg	O
llixlaha	O
where	O
e	O
a	O
h	O
a	O
are	O
two	O
parameters	O
kgx	O
lxea	O
where	O
a	O
is	O
any	O
ellipsoid	O
ofrd	O
centered	O
at	O
the	O
origin	O
problem	O
prove	O
or	O
disprove	O
if	O
dm	O
is	O
fixed	O
and	O
em	O
is	O
the	O
class	O
of	O
all	O
kernel	B
rules	O
based	O
on	O
dm	O
with	O
k	O
la	O
a	O
being	O
any	O
convex	O
set	O
ofrd	O
containing	O
the	O
origin	O
is	O
it	O
possible	O
that	O
vcm	O
or	O
is	O
vcm	O
for	O
all	O
possible	O
configurations	O
of	O
dm	O
problem	O
let	O
as	O
t	O
k	O
x	O
i	O
o	O
h	O
o	O
with	O
xl	O
x	O
xs	O
yi	O
ys	O
and	O
let	O
k	O
be	O
piecewise	O
cubic	B
extending	O
the	O
quadratic	O
example	O
in	O
the	O
text	O
show	O
that	O
the	O
vc	B
dimension	B
of	O
is	O
infinite	O
for	O
some	O
k	O
in	O
this	O
class	O
that	O
is	O
symmetric	O
unimodal	O
positive	O
and	O
convex	O
on	O
problem	O
draw	O
yd	O
the	O
distribution	O
of	O
y	O
on	O
rd	O
xo	O
i	O
where	O
x	O
has	O
a	O
density	O
and	O
e	O
at	O
all	O
x	O
find	O
a	O
symmetric	O
unimodal	O
k	O
such	O
that	O
as	O
x	O
t	O
k	O
x	O
x	O
i	O
h	O
has	O
vc	B
dimension	B
satisfying	O
p	O
v	O
oo	O
can	O
you	O
find	O
such	O
a	O
kernel	B
k	O
with	O
a	O
bounded	O
support	B
problem	O
let	O
x	O
have	O
a	O
density	O
f	O
on	O
rd	O
and	O
let	O
xl	O
xn	O
bei	O
i	O
d	O
drawn	O
from	O
f	O
show	O
that	O
lim	O
liminfp	O
iixi	O
xjllds	O
n	O
apply	O
this	O
result	O
in	O
the	O
following	O
situation	O
define	O
hn	O
minijylyjo	O
iixi	O
xi	O
ii	O
where	O
yd	O
yn	O
are	O
i	O
i	O
d	O
rd	O
x	O
random	O
variables	O
distributed	O
as	O
y	O
with	O
x	O
absolutely	O
continuous	O
y	O
independent	O
of	O
x	O
and	O
py	O
i	O
e	O
show	O
that	O
lim	O
lim	O
inf	O
p	O
c	O
n	O
conclude	O
that	O
nhd	O
in	O
probability	O
if	O
you	O
have	O
a	O
kernel	B
rule	B
with	O
kernel	B
on	O
r	O
d	O
and	O
if	O
the	O
smoothing	B
factor	I
hn	O
is	O
random	O
but	O
satisfies	O
nh	O
in	O
probability	O
then	O
lim	O
el	O
n	O
pry	O
whenever	O
x	O
has	O
a	O
density	O
show	O
this	O
problem	O
consider	O
the	O
variable	B
kernel	B
rule	B
based	O
upon	O
the	O
variable	B
kernel	B
density	O
estimate	O
of	O
breiman	O
meisel	O
and	O
purcell	O
and	O
studied	O
by	O
krzyzak	O
gnx	O
otherwise	O
if	O
khx	O
xi	O
liyo	O
khx	O
xi	O
automatic	B
kernel	B
rules	O
here	O
k	O
is	O
a	O
positive-valued	O
kernel	B
ku	O
for	O
u	O
and	O
hi	O
is	O
the	O
distance	O
between	O
xi	O
and	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
xi	O
among	O
xj	O
j	O
ii	O
j	O
n	O
investigate	O
the	O
consistency	B
of	O
this	O
rule	B
when	O
k	O
n	O
and	O
k	O
and	O
x	O
has	O
a	O
density	O
problem	O
continuation	O
fix	O
k	O
and	O
let	O
k	O
be	O
the	O
normal	B
density	O
in	O
nd	O
if	O
xl	O
has	O
a	O
density	O
what	O
can	O
you	O
say	O
about	O
the	O
asymptotic	O
probability	O
of	O
error	O
of	O
the	O
variable	B
kernel	B
rule	B
is	O
the	O
inequality	B
of	O
cover	O
and	O
hart	O
still	O
valid	O
repeat	O
the	O
exercise	O
for	O
the	O
uniform	O
kernel	B
on	O
the	O
unit	O
ball	O
of	O
nd	O
problem	O
if	O
x	O
xl	O
are	O
discrete	O
i	O
i	O
d	O
random	O
variables	O
and	O
n	O
denotes	O
the	O
ber	O
of	O
different	O
values	O
taken	O
by	O
x	O
i	O
x	O
n	O
then	O
en	O
hm	O
n	O
and	O
n	O
n	O
with	O
probability	O
one	O
hint	O
for	O
the	O
weak	B
convergence	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
probabilities	O
are	O
monotone	O
for	O
the	O
strong	O
convergence	O
use	O
mcdiarmids	O
inequality	B
problem	O
let	O
b	O
be	O
the	O
class	O
of	O
all	O
borel	O
subsets	O
of	O
nd	O
using	O
the	O
previous	O
exercise	O
show	O
that	O
for	O
any	O
discrete	O
distribution	O
sup	O
ijlna	O
jlai	O
with	O
probability	O
one	O
aeb	O
hint	O
recall	O
the	O
necessary	O
and	O
sufficient	O
condition	O
e	O
n	O
a	O
xn	O
n	O
from	O
chapter	O
problem	O
prove	O
theorem	O
allowing	O
kernel	B
functions	O
taking	O
finitely	O
many	O
ferent	O
values	O
problem	O
let	O
x	O
i	O
xn	O
be	O
an	O
li	O
d	O
sample	O
drawn	O
from	O
the	O
distribution	O
of	O
x	O
let	O
xfn	O
denote	O
the	O
nearest	B
neighbor	I
of	O
xj	O
among	O
xl	O
x	O
j	O
x	O
j	O
x	O
n	O
define	O
bn	O
max	O
iix	O
xfnii	O
j	O
show	O
that	O
for	O
all	O
nonatomic	O
distributions	O
of	O
x	O
nb	O
with	O
probability	O
one	O
is	O
it	O
true	O
that	O
for	O
every	O
x	O
with	O
a	O
density	O
there	O
exists	O
a	O
constant	O
c	O
such	O
that	O
with	O
probability	O
one	O
nb	O
clog	O
n	O
for	O
all	O
n	O
large	O
enough	O
exhibit	O
distributions	O
on	O
the	O
real	O
line	O
for	O
which	O
bn	O
with	O
probability	O
one	O
hint	O
look	O
at	O
the	O
difference	O
between	O
the	O
first	O
and	O
second	O
order	B
statistics	I
problem	O
prove	O
theorem	O
hint	O
use	O
the	O
example	O
given	O
in	O
the	O
text	O
get	O
an	O
upper	O
bound	O
for	O
the	O
error	O
probability	O
corresponding	O
to	O
h	O
by	O
hoeffdings	O
inequality	B
the	O
mean	O
integrated	O
squared	B
error	I
can	O
be	O
computed	O
for	O
every	O
h	O
in	O
a	O
straightforward	O
way	O
by	O
observing	O
that	O
split	O
the	O
integral	O
between	O
and	O
in	O
three	O
parts	O
to	O
h	O
h	O
to	O
h	O
and	O
h	O
to	O
setting	O
the	O
derivative	O
of	O
the	O
obtained	O
expression	B
with	O
respect	O
to	O
h	O
equal	O
to	O
zero	O
leads	O
to	O
a	O
third-order	O
problems	O
and	O
exercises	O
to	O
get	O
a	O
lower	O
bound	O
for	O
the	O
corresponding	O
error	O
equation	O
in	O
h	O
whose	O
roots	O
are	O
on-	O
i	O
probability	O
use	O
the	O
crude	O
bound	O
iii	O
i-h	O
elnh	O
i	O
yix	O
x	O
now	O
estimate	O
the	O
tail	O
of	O
a	O
binomial	B
distribution	I
from	O
below	O
and	O
use	O
stirlings	O
formula	O
to	O
show	O
that	O
modulo	O
polynomial	B
factors	O
the	O
error	O
probability	O
is	O
larger	O
than	O
automatic	B
nearest	B
neighbor	I
rules	O
the	O
error	O
probability	O
of	O
the	O
k-nearest	O
neighbor	O
rule	B
converges	O
to	O
the	O
bayes	O
risk	O
for	O
all	O
distributions	O
when	O
k	O
and	O
kin	O
as	O
n	O
the	O
convergence	O
result	O
is	O
extended	O
here	O
to	O
include	O
data-dependent	B
choices	O
of	O
k	O
we	O
also	O
look	O
at	O
the	O
data-based	B
selection	O
of	O
a	O
metric	O
and	O
of	O
weights	O
in	O
weighted	B
nearest	B
neighbor	I
rules	O
to	O
keep	O
the	O
notation	O
consistent	O
with	O
that	O
of	O
earlier	O
chapters	O
random	O
values	O
of	O
k	O
are	O
denoted	O
by	O
kn	O
in	O
most	O
instances	O
kn	O
is	O
merely	O
a	O
function	O
of	O
dn	O
the	O
data	O
sequence	O
yd	O
yn	O
the	O
reader	O
should	O
not	O
confuse	O
kn	O
with	O
the	O
kernel	B
k	O
in	O
other	O
chapters	O
consistency	B
we	O
start	O
with	O
a	O
general	O
theorem	O
assessing	O
strong	B
consistency	B
of	O
the	O
k-nearest	O
neighbor	O
rule	B
with	O
data-dependent	B
choices	O
of	O
k	O
for	O
the	O
sake	O
of	O
simplicity	O
we	O
assume	O
the	O
existence	O
of	O
the	O
density	O
of	O
x	O
the	O
general	O
case	O
can	O
be	O
taken	O
care	O
of	O
by	O
introducing	O
an	O
appropriate	O
tie-breaking	O
method	O
as	O
in	O
chapter	O
theorem	O
let	O
k	O
k	O
be	O
integer	O
valued	O
random	O
variables	O
and	O
let	O
gn	O
be	O
the	O
kn-nearest	O
neighbor	O
rule	B
if	O
x	O
has	O
a	O
density	O
and	O
kn	O
and	O
knn	O
with	O
probability	O
one	O
as	O
n	O
then	O
lgn	O
l	O
with	O
probability	O
one	O
automatic	B
nearest	B
neighbor	I
rules	O
proof	O
lgj	O
l	O
with	O
probability	O
one	O
if	O
and	O
only	O
if	O
for	O
every	O
e	O
oilgn-ue	O
with	O
probability	O
one	O
clearly	O
for	O
any	O
we	O
are	O
done	O
if	O
both	O
random	O
variables	O
on	O
the	O
right-hand	O
side	O
converge	O
to	O
zero	O
with	O
probability	O
one	O
the	O
convergence	O
of	O
the	O
second	O
term	O
for	O
all	O
follows	O
trivially	O
from	O
the	O
conditions	O
of	O
the	O
theorem	O
the	O
convergence	O
of	O
the	O
first	O
term	O
follows	O
from	O
the	O
remark	O
following	O
theorem	O
which	O
states	O
that	O
for	O
any	O
e	O
there	O
exist	O
and	O
no	O
such	O
that	O
for	O
the	O
error	O
probability	O
lnk	O
of	O
the	O
k-nearest	O
neighbor	O
rule	B
pl	O
nk	O
l	O
e	O
for	O
some	O
constant	O
c	O
depending	O
on	O
the	O
dimension	B
only	O
provided	O
that	O
n	O
no	O
k	O
and	O
kin	O
now	O
clearly	O
plgn	O
l	O
e	O
ii	O
kn	O
knln	O
p	O
sup	O
lnk	O
l	O
e	O
n	O
sup	O
plnk	O
l	O
e	O
by	O
the	O
union	O
bound	O
combining	O
this	O
with	O
theorem	O
we	O
get	O
plgn	O
l	O
e	O
ii	O
kn	O
knln	O
n	O
no	O
the	O
borel-cantelli	B
lemma	I
implies	O
that	O
with	O
probability	O
one	O
and	O
the	O
theorem	O
is	O
proved	O
sometimes	O
we	O
only	O
know	O
that	O
kn	O
and	O
knln	O
in	O
probability	O
in	O
such	O
cases	O
weak	B
consistency	B
is	O
guaranteed	O
the	O
proof	O
is	O
left	O
as	O
an	O
exercise	O
theorem	O
let	O
ki	O
be	O
integer	O
valued	O
random	O
variables	O
and	O
let	O
gn	O
be	O
the	O
kn	O
neighbor	O
rule	B
if	O
x	O
has	O
a	O
density	O
and	O
kn	O
and	O
knln	O
in	O
probability	O
as	O
n	O
then	O
limn--	O
oo	O
elgn	O
l	O
that	O
is	O
gn	O
is	O
weakly	O
consistent	O
data	O
splitting	O
consistency	B
by	O
itself	O
may	O
be	O
obtained	O
by	O
choosing	O
k	O
l	O
fn	O
j	O
but	O
few-if	O
users	O
will	O
want	O
to	O
blindly	O
use	O
such	O
recipes	O
instead	O
a	O
healthy	O
dose	O
of	O
back	O
from	O
the	O
data	O
is	O
preferable	O
if	O
we	O
proceed	O
as	O
in	O
chapter	O
we	O
may	O
split	O
data	O
splitting	O
for	O
weighted	B
nn	O
rules	O
the	O
data	O
sequence	O
dn	O
yi	O
yn	O
into	O
a	O
training	O
sequence	O
dm	O
yd	O
ym	O
and	O
a	O
testing	B
sequence	I
tz	O
yml	O
yn	O
where	O
m	O
i	O
n	O
the	O
training	O
sequence	O
dm	O
is	O
used	O
to	O
design	O
a	O
class	O
of	O
classifiers	O
em	O
the	O
testing	B
sequence	I
is	O
used	O
to	O
select	O
a	O
classifier	B
from	O
em	O
that	O
minimizes	O
the	O
holdout	B
estimate	O
of	O
the	O
error	O
probability	O
if	O
em	O
contains	O
all	O
k-nearest	O
neighbor	O
rules	O
with	O
i	O
k	O
m	O
then	O
m	O
therefore	O
we	O
have	O
where	O
gn	O
is	O
the	O
selected	O
k-nearest	O
neighbor	O
rule	B
by	O
combining	O
theorems	O
and	O
we	O
immediately	O
deduce	O
that	O
gn	O
is	O
universally	O
consistent	O
if	O
lim	O
m	O
n---oo	O
i	O
n---oo	O
log	O
m	O
it	O
is	O
strongly	O
universally	O
consistent	O
if	O
limn---oo	O
log	O
n	O
also	O
note	O
too	O
that	O
e	O
inf	O
l	O
problem	O
so	O
that	O
it	O
is	O
indeed	O
important	O
to	O
pick	O
i	O
much	O
larger	O
than	O
log	O
m	O
data	O
splitting	O
for	O
weighted	B
nn	O
rules	O
royall	O
introduced	O
the	O
weighted	B
nn	O
rule	B
in	O
which	O
the	O
i-th	O
nearest	B
neighbor	I
receives	O
weight	O
wi	O
where	O
wi	O
wk	O
and	O
the	O
wis	O
sum	O
to	O
one	O
we	O
assume	O
that	O
wkl	O
wn	O
if	O
there	O
are	O
n	O
data	O
points	O
besides	O
the	O
natural	O
appeal	O
of	O
attaching	O
more	O
weight	O
to	O
nearer	O
neighbors	O
there	O
is	O
also	O
a	O
practical	O
product	B
if	O
the	O
wis	O
are	O
all	O
of	O
the	O
form	O
zi	O
where	O
zi	O
is	O
a	O
prime	O
integer	O
then	O
no	O
two	O
subsums	O
of	O
wis	O
are	O
equal	O
and	O
therefore	O
voting	O
ties	O
are	O
avoided	O
altogether	O
consider	O
now	O
data	O
splitting	O
in	O
which	O
em	O
consists	O
of	O
all	O
weighted	B
k-nn	B
rules	O
as	O
described	O
above-clearly	O
k	O
m	O
now	O
as	O
lem	O
i	O
we	O
compute	O
the	O
shatter	O
coefficients	O
seem	O
i	O
we	O
claim	O
that	O
if	O
i	O
k	O
if	O
i	O
k	O
this	O
result	O
is	O
true	O
even	O
if	O
we	O
do	O
not	O
insist	O
that	O
wi	O
wk	O
o	O
proof	O
of	O
each	O
xj	O
in	O
the	O
testing	B
sequence	I
is	O
classified	O
based	O
upon	O
the	O
sign	O
of	O
ll	O
aij	O
wi	O
where	O
aij	O
e	O
i	O
depends	O
upon	O
the	O
class	O
of	O
the	O
i	O
nearest	O
automatic	B
nearest	B
neighbor	I
rules	O
neighbor	O
of	O
xj	O
among	O
xl	O
xm	O
does	O
not	O
depend	O
upon	O
the	O
wis	O
consider	O
the	O
i-vector	O
of	O
signs	O
of	O
aijwi	O
m	O
j	O
s	O
n	O
in	O
the	O
computation	O
of	O
seem	O
we	O
consider	O
the	O
aijs	O
as	O
fixed	O
numbers	O
and	O
vary	O
the	O
wis	O
subject	O
to	O
the	O
condition	O
laid	O
out	O
above	O
here	O
is	O
the	O
crucial	O
step	O
in	O
the	O
argument	O
the	O
collection	O
of	O
all	O
vectors	O
wk	O
for	O
which	O
xj	O
is	O
assigned	O
to	O
class	O
is	O
a	O
linear	O
halfspace	O
of	O
rk	O
therefore	O
seem	O
l	O
is	O
bounded	O
from	O
above	O
by	O
the	O
number	O
of	O
cells	O
in	O
the	O
partition	B
of	O
rk	O
defined	O
by	O
llinear	O
halfspaces	O
this	O
is	O
bounded	O
by	O
g	O
problem	O
if	O
k	O
s	O
l	O
let	O
gn	O
be	O
the	O
rule	B
in	O
lem	O
i	O
that	O
minimizes	O
the	O
empirical	B
error	I
committed	O
on	O
the	O
test	O
sequence	O
yn	O
then	O
by	O
if	O
n	O
m	O
k	O
we	O
have	O
implies	O
m	O
we	O
have	O
universal	B
consistency	B
when	O
if	O
k	O
k	O
logl	O
l	O
o	O
the	O
estimation	B
error	I
is	O
of	O
the	O
order	O
of	O
jk	O
log	O
the	O
nology	O
of	O
chapter	O
the	O
rule	B
is	O
j	O
k	O
log	O
l	O
i-optimal	O
this	O
error	O
must	O
be	O
weighed	O
against	O
the	O
unknown	O
approximation	B
error	I
let	O
us	O
present	O
a	O
quick	O
heuristic	O
ment	O
on	O
the	O
real	O
line	O
there	O
is	O
compelling	O
evidence	O
to	O
suggest	O
that	O
when	O
x	O
has	O
a	O
smooth	O
density	O
k	O
is	O
nearly	O
optimal	O
with	O
this	O
choice	O
if	O
both	O
and	O
m	O
grow	O
linearly	O
in	O
n	O
the	O
estimation	B
error	I
is	O
of	O
the	O
order	O
of	O
jlog	O
n	O
n	O
this	O
is	O
painfully	O
large-to	O
reduce	O
this	O
error	O
by	O
a	O
factor	O
of	O
two	O
sample	O
sizes	O
must	O
rise	O
by	O
a	O
factor	O
of	O
about	O
the	O
reason	O
for	O
this	O
disappointing	O
result	O
is	O
that	O
em	O
is	O
just	O
too	O
rich	O
for	O
the	O
values	O
of	O
k	O
that	O
interest	O
us	O
automatic	B
selection	O
may	O
lead	O
to	O
rules	O
that	O
overfit	O
the	O
data	O
if	O
we	O
restrict	O
em	O
by	O
making	O
m	O
very	O
small	O
the	O
following	O
rough	O
argument	O
may	O
be	O
used	O
to	O
glean	O
information	O
about	O
the	O
size	O
of	O
m	O
and	O
i	O
n	O
m	O
we	O
will	O
take	O
k	O
m	O
n	O
for	O
smooth	O
regression	B
function	I
the	O
estimation	B
error	I
may	O
be	O
anywhere	O
between	O
m	O
and	O
m	O
on	O
the	O
real	O
line	O
as	O
the	O
estimation	B
error	I
is	O
of	O
the	O
order	O
of	O
jmlognn	O
equating	O
the	O
errors	O
leads	O
to	O
the	O
rough	O
recipe	O
that	O
m	O
and	O
m	O
respectively	O
both	O
errors	O
are	O
then	O
about	O
and	O
respectively	O
this	O
is	O
better	O
than	O
with	O
the	O
previous	O
example	O
with	O
m	O
linear	O
in	O
n	O
unfortunately	O
it	O
is	O
difficult	O
to	O
test	O
whether	O
the	O
conditions	O
on	O
that	O
guarantee	O
certain	O
errors	O
are	O
satisfied	O
the	O
above	O
procedure	O
is	O
thus	O
doomed	O
to	O
remain	O
heuristic	O
reference	O
data	O
and	O
data	O
splitting	O
split	O
the	O
data	O
into	O
dm	O
and	O
tz	O
as	O
is	O
done	O
in	O
the	O
previous	O
section	O
let	O
em	O
contain	O
alli-nn	O
rules	O
that	O
are	O
based	O
upon	O
the	O
data	O
yi	O
yk	O
where	O
k	O
s	O
m	O
is	O
to	O
be	O
picked	O
xk	O
c	O
xm	O
and	O
yk	O
e	O
ik	O
note	O
that	O
because	O
the	O
ys	O
are	O
free	O
parameters	O
yi	O
yk	O
is	O
not	O
necessarily	O
a	O
subset	O
of	O
i	O
yi	O
ym-this	O
allows	O
us	O
to	O
flip	O
certain	O
y-values	O
at	O
some	O
data	O
points	O
trivially	O
lem	O
i	O
hence	O
variable	B
metric	I
nn	O
rules	O
where	O
i	O
n	O
m	O
and	O
gn	O
e	O
em	O
minimizes	O
the	O
empirical	B
error	I
on	O
the	O
test	O
sequence	O
tz	O
the	O
best	O
rule	B
in	O
em	O
is	O
universally	O
consistent	O
when	O
k	O
theorem	O
therefore	O
gn	O
is	O
universally	O
consistent	O
when	O
the	O
above	O
bound	O
converges	O
to	O
zero	O
sufficient	O
conditions	O
are	O
lim	O
i	O
klogm	O
lim	O
o	O
n-oo	O
i	O
as	O
the	O
estimation	B
error	I
is	O
j	O
k	O
log	O
m	O
i	O
it	O
is	O
important	O
to	O
make	O
i	O
large	O
while	O
keeping	O
k	O
small	O
the	O
selected	O
sequence	O
yi	O
yk	O
may	O
be	O
called	O
reference	O
data	O
as	O
it	O
captures	O
the	O
information	O
in	O
the	O
larger	O
data	O
set	O
if	O
k	O
is	O
sufficiently	O
small	O
the	O
computation	O
of	O
gn	O
is	O
extremely	O
fast	O
the	O
idea	O
of	O
selecting	O
reference	O
data	O
or	O
throwing	O
out	O
useless	O
or	O
data	O
points	O
has	O
been	O
proposed	O
and	O
studied	O
by	O
many	O
researchers	O
under	O
names	O
such	O
as	O
condensed	B
nn	O
rules	O
edited	B
nn	O
rules	O
and	O
selective	B
nn	O
rules	O
see	O
hart	O
gates	O
wilson	O
wagner	O
ullmann	O
ritter	O
et	O
al	O
tomek	O
and	O
devijver	O
and	O
kittler	O
see	O
also	O
section	O
variable	B
metric	I
nn	O
rules	O
the	O
data	O
may	O
also	O
be	O
used	O
to	O
select	O
a	O
suitable	O
metric	O
for	O
use	O
with	O
the	O
k-nn	B
rule	B
the	O
metric	O
adapts	O
itself	O
for	O
certain	O
scale	O
information	O
gleaned	O
from	O
the	O
data	O
for	O
example	O
we	O
may	O
compute	O
the	O
distance	O
between	O
xl	O
and	O
by	O
the	O
formula	O
aat	O
lxi	O
where	O
is	O
a	O
column	O
vector	O
l	O
denotes	O
its	O
transpose	O
a	O
is	O
a	O
d	O
x	O
d	O
transformation	O
matrix	O
and	O
l	O
aa	O
t	O
is	O
a	O
positive	O
definite	O
matrix	O
the	O
elements	O
of	O
a	O
or	O
l	O
may	O
be	O
determined	O
from	O
the	O
data	O
according	O
to	O
some	O
heuristic	O
formulas	O
we	O
refer	O
to	O
fukunaga	O
and	O
hostetler	O
short	O
and	O
fukunaga	O
fukunaga	O
and	O
flick	O
and	O
myles	O
and	O
hand	O
for	O
more	O
information	O
for	O
example	O
the	O
object	O
of	O
principal	B
component	I
analysis	I
is	O
to	O
find	O
a	O
tion	O
matrix	O
a	O
such	O
that	O
the	O
components	O
of	O
the	O
vector	O
a	O
t	O
x	O
have	O
unit	O
variance	O
and	O
are	O
uncorrelated	O
these	O
methods	O
are	O
typically	O
based	O
on	O
estimating	O
the	O
eigenvalues	O
of	O
the	O
covariance	O
matrix	O
of	O
x	O
for	O
such	O
situations	O
we	O
prove	O
the	O
following	O
general	O
consistency	B
result	O
automatic	B
nearest	B
neighbor	I
rules	O
theorem	O
let	O
the	O
random	O
metric	O
pn	O
be	O
of	O
the	O
form	O
pnx	O
y	O
iia	O
yli	O
where	O
the	O
matrix	O
an	O
is	O
a	O
function	O
of	O
xl	O
x	O
n	O
assume	O
that	O
distance	O
ties	O
occur	O
with	O
zero	O
probability	O
and	O
there	O
are	O
two	O
sequences	O
of	O
nonnegative	O
random	O
variables	O
and	O
such	O
thatfor	O
any	O
n	O
and	O
x	O
y	O
end	O
and	O
if	O
p	O
mn	O
o	O
o	O
n-oo	O
mn	O
lim	O
kn	O
and	O
n-oo	O
kn	O
hm	O
n-oo	O
n	O
then	O
the	O
kn	O
neighbor	O
rule	B
based	O
on	O
the	O
metric	O
pn	O
is	O
consistent	O
proof	O
we	O
verify	O
the	O
three	O
conditions	O
of	O
theorem	O
in	O
this	O
case	O
wnjx	O
kn	O
if	O
xi	O
is	O
one	O
of	O
the	O
kn	O
nearest	O
neighbors	O
of	O
x	O
to	O
pn	O
and	O
zero	O
otherwise	O
condition	O
holds	O
trivially	O
just	O
as	O
in	O
the	O
proof	O
of	O
consistency	B
of	O
the	O
ordinary	B
knnearest	O
neighbor	O
rule	B
for	O
condition	O
we	O
need	O
the	O
property	O
that	O
the	O
number	O
of	O
data	O
points	O
that	O
can	O
be	O
among	O
the	O
kn	O
nearest	O
neighbors	O
of	O
a	O
particular	O
point	O
is	O
at	O
most	O
kn	O
yd	O
where	O
the	O
constant	O
yd	O
depends	O
on	O
the	O
dimension	B
only	O
this	O
is	O
a	O
deterministic	O
property	O
and	O
it	O
can	O
be	O
proven	O
exactly	O
the	O
same	O
way	O
as	O
for	O
the	O
standard	B
nearest	B
neighbor	I
rule	B
the	O
only	O
condition	O
of	O
theorem	O
whose	O
justification	O
needs	O
extra	O
work	O
is	O
condition	O
we	O
need	O
to	O
show	O
that	O
for	O
any	O
a	O
lim	O
e	O
o	O
n-oo	O
il	O
denote	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
according	O
to	O
pn	O
by	O
xk	O
and	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
according	O
to	O
the	O
euclidean	B
metric	O
by	O
xk	O
then	O
e	O
wnixiiix-xdlaj	O
mn	O
iix	O
yll	O
pnx	O
y	O
selection	O
of	O
k	O
based	O
on	O
the	O
deleted	O
estimate	O
p	O
pnx	O
xkn	O
p	O
ipnxxmna	O
kn	O
mnllx	O
y	O
ii	O
pnx	O
y	O
piix-xdl	O
but	O
we	O
know	O
from	O
the	O
consistency	B
proof	O
of	O
the	O
ordinary	B
neighbor	O
rule	B
in	O
chapter	O
that	O
for	O
each	O
a	O
lim	O
p	O
xkn	O
ii	O
a	O
o	O
it	O
follows	O
from	O
the	O
condition	O
on	O
mn	O
mn	O
that	O
the	O
probability	O
above	O
converges	O
to	O
zero	O
as	O
well	O
the	O
conditions	O
of	O
the	O
theorem	O
hold	O
if	O
for	O
example	O
the	O
elements	O
of	O
the	O
matrix	O
an	O
converge	O
to	O
the	O
elements	O
of	O
an	O
invertible	O
matrix	O
a	O
in	O
probability	O
in	O
that	O
case	O
we	O
may	O
take	O
mn	O
as	O
the	O
smallest	O
and	O
as	O
the	O
largest	O
eigenvalues	O
of	O
a	O
an	O
then	O
mn	O
converges	O
to	O
the	O
ratio	O
of	O
the	O
smallest	O
and	O
largest	O
eigenvalues	O
of	O
at	O
a	O
a	O
positive	O
number	O
if	O
we	O
pick	O
the	O
elements	O
of	O
a	O
by	O
minimizing	O
the	O
empirical	B
error	I
of	O
a	O
test	O
sequence	O
tz	O
over	O
cn	O
where	O
cm	O
contains	O
all	O
k-nn	B
rules	O
based	O
upon	O
a	O
training	O
sequence	O
dm	O
the	O
elements	O
of	O
a	O
are	O
the	O
free	O
parameters	O
the	O
value	O
of	O
scm	O
l	O
is	O
too	O
large	O
to	O
be	O
useful-see	O
problem	O
furthermore	O
such	O
minimization	O
is	O
not	O
computationally	O
feasible	O
selection	O
of	O
k	O
based	O
on	O
the	O
deleted	O
estimate	O
if	O
you	O
wish	O
to	O
use	O
all	O
the	O
available	O
data	O
in	O
the	O
training	O
sequence	O
without	O
splitting	O
then	O
empirical	B
selection	O
based	O
on	O
minimization	O
of	O
other	O
estimates	O
of	O
the	O
error	O
probability	O
may	O
be	O
your	O
solution	O
unfortunately	O
performance	O
guarantees	O
for	O
the	O
selected	O
rule	B
are	O
rarely	O
available	O
if	O
the	O
class	O
of	O
rules	O
is	O
finite	O
as	O
when	O
k	O
is	O
selected	O
from	O
n	O
there	O
are	O
useful	O
inequalities	O
we	O
will	O
show	O
you	O
how	O
this	O
works	O
automatic	B
nearest	B
neighbor	I
rules	O
let	O
en	O
be	O
the	O
class	O
of	O
all	O
k-nearest	O
neighbor	O
rules	O
based	O
on	O
a	O
fixed	O
training	O
sequence	O
dn	O
but	O
with	O
k	O
variable	B
clearly	O
i	O
en	O
i	O
n	O
assume	O
that	O
the	O
deleted	O
estimate	O
is	O
used	O
to	O
pick	O
a	O
classifier	B
gn	O
from	O
en	O
we	O
can	O
derive	O
performance	O
bounds	O
for	O
gn	O
from	O
theorem	O
since	O
the	O
result	O
gives	O
poor	O
bounds	O
for	O
large	O
values	O
of	O
k	O
the	O
range	O
of	O
ks	O
has	O
to	O
be	O
restricted-see	O
the	O
discussion	O
following	O
theorem	O
let	O
ko	O
denote	O
the	O
value	O
of	O
the	O
largest	O
k	O
allowed	O
that	O
is	O
en	O
now	O
contains	O
all	O
k-nearest	O
neighbor	O
rules	O
with	O
k	O
ranging	O
from	O
to	O
ko	O
theorem	O
let	O
gn	O
be	O
the	O
classijierminimizing	O
the	O
deleted	O
estimate	O
of	O
the	O
error	O
probability	O
over	O
en	O
the	O
class	O
of	O
k-nearest	O
neighbor	O
rules	O
with	O
k	O
ko	O
then	O
where	O
c	O
is	O
a	O
constant	O
depending	O
on	O
the	O
dimension	B
only	O
if	O
ko	O
ko	O
log	O
nj	O
n	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
and	O
proof	O
from	O
theorem	O
we	O
recall	O
the	O
inequality	B
now	O
follows	O
from	O
theorem	O
via	O
the	O
union	O
bound	O
universal	B
consistency	B
follows	O
from	O
the	O
previous	O
inequality	B
and	O
the	O
fact	O
that	O
the	O
ko-nn	O
rule	B
is	O
strongly	O
universally	O
consistent	O
theorem	O
problems	O
and	O
exercises	O
problem	O
let	O
k	O
i	O
k	O
be	O
integer	O
valued	O
random	O
variables	O
and	O
let	O
gn	O
be	O
the	O
nearest	B
neighbor	I
rule	B
show	O
that	O
if	O
x	O
has	O
a	O
density	O
and	O
kn	O
and	O
kill	O
n	O
in	O
probability	O
as	O
n	O
then	O
elgn	O
l	O
problem	O
let	O
c	O
be	O
the	O
class	O
of	O
all	O
l-nn	O
rules	O
based	O
upon	O
pairs	O
yi	O
yk	O
where	O
k	O
is	O
a	O
fixed	O
parameter	O
varying	O
with	O
n	O
and	O
the	O
yis	O
are	O
variable	B
pairs	O
from	O
r	O
d	O
x	O
l	O
let	O
gn	O
be	O
the	O
rule	B
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
c	O
or	O
equivalently	O
let	O
gn	O
be	O
the	O
rule	B
that	O
minimizes	O
the	O
resubstitution	B
estimate	O
lr	O
over	O
c	O
compute	O
a	O
suitable	O
upper	O
bound	O
for	O
sc	O
n	O
compute	O
a	O
good	O
upper	O
bound	O
for	O
vc	O
as	O
a	O
function	O
of	O
k	O
and	O
d	O
if	O
k	O
with	O
n	O
show	O
that	O
the	O
sequence	O
of	O
classes	O
c	O
contains	O
a	O
strongly	O
um	O
versally	O
consistent	O
subsequence	O
of	O
rules	O
may	O
assume	O
for	O
convenience	O
that	O
x	O
has	O
a	O
density	O
to	O
avoid	O
distance	O
ties	O
under	O
what	O
condition	O
on	O
k	O
can	O
you	O
guarantee	O
strong	B
universal	B
consistency	B
of	O
gn	O
give	O
an	O
upper	O
bound	O
for	O
problems	O
and	O
exercises	O
problem	O
let	O
cm	O
contain	O
all	O
k-nn	B
rules	O
based	O
upon	O
data	O
pairs	O
yl	O
ym	O
the	O
metric	O
used	O
in	O
computing	O
the	O
neighbors	O
is	O
derived	O
from	O
the	O
norm	O
d	O
ii	O
l	O
l	O
x	O
xu	O
x	O
o	O
xed	O
d	O
il	O
jl	O
where	O
forms	O
a	O
positive	O
definite	O
matrix	O
the	O
elements	O
of	O
are	O
the	O
free	O
parameters	O
in	O
cm	O
compute	O
upper	O
and	O
lower	B
bounds	I
for	I
seem	O
i	O
as	O
a	O
function	O
of	O
m	O
i	O
k	O
and	O
d	O
problem	O
let	O
gn	O
be	O
the	O
rule	B
obtained	O
by	O
minimizing	O
ld	O
over	O
all	O
k-nn	B
rules	O
with	O
k	O
n	O
prove	O
or	O
disprove	O
gn	O
is	O
strongly	O
universally	O
consistent	O
note	O
that	O
in	O
view	O
of	O
theorem	O
it	O
suffices	O
to	O
consider	O
en	O
log	O
n	O
k	O
n	O
for	O
all	O
e	O
o	O
hypercubes	O
and	O
discrete	O
spaces	O
in	O
many	O
situations	O
the	O
pair	O
y	O
is	O
purely	O
binary	B
taking	O
values	O
in	O
id	O
x	O
i	O
examples	O
include	O
boolean	O
settings	O
component	O
of	O
x	O
represents	O
or	O
representations	O
of	O
continuous	O
variables	O
through	O
quantization	B
variables	O
are	O
always	O
represented	O
by	O
bit	O
strings	O
in	O
computers	O
and	O
ordinal	O
data	O
component	O
of	O
x	O
is	O
i	O
if	O
and	O
only	O
if	O
a	O
certain	O
item	O
is	O
present	O
the	O
components	O
of	O
x	O
are	O
denoted	O
by	O
x	O
xd	O
in	O
this	O
chapter	O
we	O
review	O
pattern	O
recognition	O
briefly	O
in	O
this	O
setup	O
without	O
any	O
particular	O
structure	O
in	O
the	O
distribution	O
of	O
y	O
or	O
the	O
function	O
py	O
iix	O
x	O
x	O
e	O
id	O
the	O
pattern	O
recognition	O
problem	O
might	O
as	O
well	O
be	O
cast	O
in	O
the	O
space	O
of	O
the	O
first	O
positive	O
integers	O
y	O
e	O
x	O
i	O
this	O
is	O
dealt	O
with	O
in	O
the	O
first	O
section	O
however	O
things	O
become	O
more	O
interesting	O
under	O
certain	O
structural	O
assumptions	O
such	O
as	O
the	O
assumption	O
that	O
the	O
components	O
of	O
x	O
be	O
independent	O
this	O
is	O
dealt	O
with	O
in	O
the	O
third	O
section	O
general	O
discrimination	O
rules	O
on	O
hypercubes	O
are	O
treated	O
in	O
the	O
rest	O
of	O
the	O
chapter	O
multinomial	B
discrimination	I
at	O
first	O
sight	O
discrimination	O
on	O
a	O
finite	O
set	O
multinomial	B
mination-may	O
seem	O
utterly	O
trivial	O
let	O
us	O
call	O
the	O
following	O
rule	B
the	O
fundamental	B
rule	B
as	O
it	O
captures	O
what	O
most	O
of	O
us	O
would	O
do	O
in	O
the	O
absence	O
of	O
any	O
additional	O
information	O
hypercubes	O
and	O
discrete	O
spaces	O
the	O
fundamental	B
rule	B
coincides	O
with	O
the	O
standard	B
kernel	B
and	O
histogram	O
rules	O
if	O
the	O
smoothing	B
factor	I
or	O
bin	O
width	O
are	O
taken	O
small	O
enough	O
if	O
pi	O
px	O
i	O
and	O
yj	O
is	O
as	O
usual	O
then	O
it	O
takes	O
just	O
a	O
second	O
to	O
see	O
that	O
yjxpx	O
and	O
eln	O
l	O
yjxpxcl	O
pxn	O
x	O
where	O
ln	O
lg	O
assume	O
yjx	O
at	O
all	O
x	O
then	O
l	O
and	O
eln	O
l	O
pxl	O
pxn	O
x	O
if	O
px	O
ii	O
k	O
for	O
all	O
x	O
we	O
have	O
eln	O
if	O
k	O
this	O
simple	O
calculation	O
shows	O
that	O
we	O
cannot	O
say	O
anything	O
useful	O
about	O
fundamental	O
rules	O
unless	O
k	O
at	O
the	O
very	O
least	O
on	O
the	O
positive	O
side	O
the	O
following	O
universal	O
bound	O
is	O
useful	O
ii	O
kn	O
theorem	O
for	O
the	O
fundamental	B
rule	B
we	O
have	O
ln	O
l	O
with	O
probability	O
one	O
as	O
n	O
and	O
in	O
fact	O
for	O
all	O
distributions	O
eln	O
l	O
k	O
and	O
eln	O
l	O
proof	O
the	O
first	O
statement	O
follows	O
trivially	O
from	O
the	O
strong	B
universal	B
consistency	B
of	O
histogram	O
rules	O
theorem	O
it	O
is	O
the	O
universal	O
inequality	B
that	O
is	O
of	O
interest	O
here	O
if	O
bn	O
p	O
denotes	O
a	O
binomial	B
random	O
variable	B
with	O
parameters	O
nand	O
p	O
then	O
we	O
have	O
k	O
eln	O
lpx	O
yjx	O
pbnx	O
yjx	O
xl	O
nx	O
is	O
a	O
binomial	B
px	O
random	O
variable	B
k	O
x	O
from	O
this	O
if	O
sex	O
multinomial	B
discrimination	I
sex	O
nxsxl	O
g	O
sex	O
k	O
xl	O
xl	O
sex	O
the	O
chebyshev-cantelli	B
inequality-theorem	O
l	O
px	O
sex	O
l	O
t	O
l	O
t	O
pal	O
l	O
nx	O
l	O
t	O
pxe	O
o	O
l	O
t	O
pxn	O
t	O
px	O
e	O
l	O
t	O
p	O
the	O
function	O
u	O
n	O
jensens	O
inequality	B
nx	O
k	O
xv	O
xl	O
for	O
u	O
lemma	O
and	O
the	O
fact	O
that	O
the	O
worst	O
distribution	O
has	O
px	O
k	O
for	O
all	O
x	O
l	O
k	O
ljp	O
xl	O
l	O
fk	O
e-u	O
for	O
u	O
en	O
v	O
and	O
by	O
the	O
cauchy-schwarz	B
inequality	B
this	O
concludes	O
the	O
proof	O
as	O
e	O
for	O
several	O
key	O
properties	O
of	O
the	O
fundamental	B
rule	B
the	O
reader	O
is	O
referred	O
to	O
glick	O
and	O
to	O
problem	O
other	O
references	O
include	O
krzanowski	O
stein	O
and	O
goldstein	O
and	O
dillon	O
note	O
also	O
the	O
following	O
extension	O
of	O
theorem	O
which	O
shows	O
that	O
the	O
fundamental	B
rule	B
can	O
handle	O
all	O
discrete	O
distributions	O
hypercubes	O
and	O
discrete	O
spaces	O
theorem	O
if	O
x	O
is	O
purely	O
atomic	O
possibly	O
infinitely	O
many	O
atoms	O
then	O
ln	O
l	O
with	O
probability	O
one	O
for	O
the	O
fundamental	B
rule	B
proof	O
number	O
the	O
atoms	O
define	O
xi	O
minx	O
k	O
and	O
replace	O
yt	O
by	O
yi	O
where	O
x	O
minxi	O
k	O
apply	O
the	O
fundamental	B
rule	B
to	O
the	O
new	O
problem	O
and	O
note	O
that	O
by	O
theorem	O
if	O
k	O
is	O
fixed	O
ln	O
l	O
with	O
probability	O
one	O
for	O
the	O
new	O
rule	B
new	O
distribution	O
however	O
the	O
difference	O
in	O
lns	O
and	O
in	O
l	O
between	O
the	O
truncated	O
and	O
nontruncated	O
versions	O
cannot	O
be	O
more	O
than	O
p	O
k	O
which	O
may	O
be	O
made	O
as	O
small	O
as	O
desired	O
by	O
choice	O
of	O
k	O
quantization	B
consider	O
a	O
fixed	O
partition	B
of	O
space	O
nd	O
into	O
k	O
sets	O
ad	O
and	O
let	O
gn	O
be	O
the	O
standard	B
partitioning	O
rule	B
based	O
upon	O
majority	O
votes	O
in	O
the	O
ais	O
are	O
broken	O
by	O
favoring	O
the	O
response	O
as	O
elsewhere	O
in	O
the	O
book	O
we	O
consider	O
two	O
rules	O
the	O
rule	B
gn	O
considered	O
above	O
the	O
data	O
are	O
yi	O
yn	O
with	O
y	O
e	O
nd	O
x	O
to	O
i	O
the	O
probability	O
of	O
error	O
is	O
denoted	O
by	O
l	O
n	O
and	O
the	O
bayes	O
probability	O
of	O
error	O
is	O
l	O
with	O
py	O
llx	O
end	O
the	O
fundamental	B
rule	B
operating	O
on	O
the	O
quantized	O
data	O
yn	O
with	O
y	O
e	O
k	O
x	O
to	O
i	O
x	O
j	O
if	O
x	O
e	O
a	O
j	O
the	O
bayes	O
probability	O
of	O
error	O
is	O
l	O
with	O
py	O
xi	O
xl	O
xl	O
e	O
k	O
the	O
probability	O
of	O
error	O
is	O
denoted	O
by	O
clearly	O
g	O
is	O
nothing	O
but	O
the	O
fundamental	B
rule	B
for	O
the	O
quantized	O
data	O
as	O
gnx	O
where	O
xl	O
j	O
if	O
x	O
e	O
a	O
j	O
we	O
see	O
that	O
however	O
the	O
bayes	B
error	I
probabilities	O
are	O
different	O
in	O
the	O
two	O
situations	O
we	O
claim	O
however	O
the	O
following	O
theorem	O
for	O
the	O
standard	B
partitioning	O
rule	B
eln	O
l	O
where	O
e	O
furthermore	O
l	O
l	O
l	O
o	O
quantization	B
proof	O
clearly	O
l	O
l	O
problem	O
also	O
l	O
eminnx	O
nx	O
eminnx	O
nx	O
furthermore	O
eln	O
s	O
l	O
s	O
l	O
s	O
by	O
theorem	O
as	O
an	O
immediate	O
corollary	O
we	O
show	O
how	O
to	O
use	O
the	O
last	O
bound	O
to	O
get	O
useful	O
distribution-free	O
performance	O
bounds	O
theorem	O
let	O
f	O
be	O
a	O
class	O
of	O
distributions	O
of	O
yfor	O
which	O
x	O
e	O
ld	O
with	O
probability	O
one	O
and	O
for	O
some	O
constants	O
c	O
ci	O
i	O
cllx	O
zw	O
x	O
z	O
e	O
rd	O
then	O
ifwe	O
consider	O
all	O
cubic	B
histogram	O
rules	O
gn	O
chapter	O
for	O
a	O
definition	O
we	O
have	O
inf	O
cubic	B
histogram	B
rule	B
gn	O
elgn	O
l	O
a	O
fn	O
b	O
n	O
where	O
a	O
and	O
b	O
are	O
constants	O
depending	O
upon	O
c	O
ci	O
and	O
d	O
only	O
the	O
prooffor	O
explicit	O
expressions	O
theorem	O
establishes	O
the	O
existence	O
of	O
rules	O
that	O
perform	O
uniformly	O
at	O
rate	O
o	O
a	O
over	O
f	O
results	O
like	O
this	O
have	O
an	O
impact	O
on	O
the	O
number	O
of	O
data	O
points	O
required	O
to	O
guarantee	O
a	O
given	O
performance	O
for	O
any	O
y	O
e	O
f	O
proof	O
consider	O
a	O
cubic	B
grid	O
with	O
cells	O
of	O
volume	O
hd	O
covering	O
ld	O
does	O
not	O
exceed	O
h	O
we	O
apply	O
theorem	O
to	O
obtain	O
as	O
the	O
number	O
of	O
cells	O
elgn	O
s	O
l	O
s	O
where	O
sup	O
c	O
sup	O
sup	O
liz	O
xlla	O
c	O
x	O
ai	O
xzeai	O
the	O
right-hand	O
side	O
is	O
approximately	O
maximal	O
when	O
h	O
in	O
def	O
c	O
n	O
resubstitution	B
yields	O
the	O
result	O
hypercubes	O
and	O
discrete	O
spaces	O
independent	O
components	O
let	O
x	O
xcd	O
have	O
components	O
that	O
are	O
conditionally	O
independent	O
given	O
and	O
also	O
given	O
oj	O
introduce	O
the	O
notation	O
pxci	O
pi	O
qi	O
pxi	O
o	O
p	O
py	O
with	O
x	O
xed	O
e	O
ld	O
we	O
see	O
that	O
px	O
x	O
ppx	O
xly	O
ppx	O
xly	O
o	O
and	O
px	O
xly	O
it	O
piti	O
pil-xi	O
d	O
il	O
px	O
xly	O
o	O
it	O
qiti	O
qil-xu	O
d	O
simple	O
consideration	O
shows	O
that	O
the	O
bayes	O
rule	B
is	O
given	O
by	O
il	O
if	O
p	O
nl	O
pixi	O
pil-xi	O
gx	O
p	O
nl	O
qil-xi	O
o	O
otherwise	O
taking	O
logarithms	O
it	O
is	O
easy	O
to	O
see	O
that	O
this	O
is	O
equivalent	O
to	O
the	O
following	O
rule	B
g	O
otherwise	O
l	O
il	O
x	O
ci	O
where	O
log	O
log	O
pi	O
ft	O
p	O
qi	O
log	O
pi	O
qi	O
qi	O
pi	O
l	O
d	O
in	O
other	O
words	O
the	O
bayes	O
classifier	B
is	O
linear	O
this	O
beautiful	O
fact	O
was	O
noted	O
by	O
minsky	O
winder	O
and	O
chow	O
having	O
identified	O
the	O
bayes	O
rule	B
as	O
a	O
linear	O
discrimination	O
rule	B
we	O
may	O
apply	O
the	O
full	O
force	O
of	O
the	O
vapnik-chervonenkis	B
theory	O
let	O
gn	O
be	O
the	O
rule	B
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
the	O
class	O
of	O
all	O
linear	O
discrimination	O
rules	O
as	O
the	O
class	O
independent	O
components	O
of	O
linear	O
halfspaces	O
of	O
rd	O
has	O
vc	B
dimension	B
d	O
corollary	O
we	O
recall	O
from	O
theorem	O
that	O
for	O
gn	O
plgn	O
l	O
e	O
elgn	O
l	O
corollary	O
plgn	O
l	O
e	O
ne	O
and	O
where	O
elgn	O
ls	O
yfi	O
c	O
c	O
these	O
bounds	O
are	O
useful	O
they	O
tend	O
to	O
zero	O
with	O
n	O
if	O
d	O
onlogn	O
in	O
contrast	O
without	O
the	O
independence	O
assumption	O
we	O
have	O
pointed	O
out	O
that	O
no	O
trivial	O
guarantee	O
can	O
be	O
given	O
about	O
elgn	O
unless	O
n	O
the	O
independence	O
assumption	O
has	O
led	O
us	O
out	O
of	O
the	O
high-dimensional	O
quagmire	O
one	O
may	O
wish	O
to	O
attempt	O
to	O
estimate	O
the	O
pis	O
and	O
qis	O
by	O
pi	O
and	O
qi	O
and	O
to	O
use	O
these	O
in	O
the	O
plug-in	O
rule	B
gn	O
that	O
decides	O
if	O
d	O
p	O
n	O
putu	O
pil-xi	O
fi	O
n	O
qixi	O
qul-xi	O
d	O
il	O
il	O
where	O
p	O
is	O
the	O
standard	B
sample-based	O
estimate	O
of	O
p	O
the	O
maximum-likelihood	O
estimate	O
of	O
pi	O
is	O
given	O
by	O
while	O
q-i	O
ll	O
ixil	O
yo	O
n	O
ljl	O
l	O
j	O
with	O
equal	O
to	O
problem	O
note	O
that	O
the	O
plug-in	O
rule	B
too	O
is	O
linear	O
with	O
nx	O
g	O
ao	O
l	O
il	O
ai	O
x	O
l	O
otherwise	O
where	O
ao	O
p	O
log	O
log	O
p	O
il	O
no	O
no	O
nll	O
nooi	O
nloi	O
nooi	O
ai	O
log	O
nll	O
nooi	O
nol	O
nloi	O
l	O
d	O
hypercubes	O
and	O
discrete	O
spaces	O
and	O
n	O
nooi	O
l	O
ixjoyjo	O
jl	O
n	O
n	O
loi	O
l	O
ixjlyjo	O
jl	O
n	O
noli	O
l	O
ixjoyjl	O
jl	O
n	O
nlli	O
l	O
ixjilyjl	O
jl	O
d	O
p	O
l	O
nll	O
il	O
for	O
all	O
this	O
we	O
refer	O
to	O
warner	O
toronto	O
veasey	O
and	O
stephenson	O
also	O
mclachlan	O
use	O
the	O
inequality	B
elgn	O
l	O
where	O
is	O
as	O
given	O
in	O
the	O
text	O
and	O
is	O
as	O
but	O
with	O
p	O
pi	O
qi	O
replaced	O
by	O
p	O
pi	O
qi	O
to	O
establish	O
consistency	B
theorem	O
for	O
the	O
plug-in	O
rule	B
lgn	O
l	O
with	O
probability	O
one	O
and	O
elgn	O
l	O
whenever	O
the	O
components	O
are	O
independent	O
we	O
refer	O
to	O
the	O
problem	O
section	O
for	O
an	O
evaluation	O
of	O
an	O
upper	O
bound	O
for	O
e	O
l	O
problem	O
linear	O
discrimination	O
on	O
the	O
hypercube	O
has	O
of	O
course	O
limited	O
value	O
the	O
world	O
is	O
full	O
of	O
examples	O
that	O
are	O
not	O
linearly	O
separable	O
for	O
example	O
on	O
if	O
y	O
xo	O
that	O
y	O
implements	O
the	O
boolean	O
or	O
or	O
function	O
the	O
problem	O
is	O
not	O
linearly	O
separable	O
if	O
all	O
four	O
possible	O
values	O
of	O
x	O
have	O
positive	O
probability	O
however	O
the	O
exclusive	O
or	O
function	O
may	O
be	O
dealt	O
with	O
very	O
nicely	O
if	O
one	O
considers	O
quadratic	O
discriminants	O
dealt	O
with	O
in	O
a	O
later	O
section	O
on	O
series	O
methods	O
boolean	O
classifiers	O
by	O
a	O
boolean	B
classification	I
problem	O
we	O
mean	O
a	O
pattern	O
recognition	O
problem	O
on	O
the	O
hypercube	O
for	O
which	O
l	O
this	O
setup	O
relates	O
to	O
the	O
fact	O
that	O
if	O
we	O
consider	O
y	O
i	O
as	O
a	O
circuit	O
failure	O
and	O
y	O
as	O
an	O
operable	O
circuit	O
then	O
y	O
is	O
a	O
deterministic	O
function	O
of	O
the	O
xis	O
which	O
may	O
be	O
considered	O
as	O
gates	O
or	O
switches	O
in	O
that	O
case	O
y	O
may	O
be	O
written	O
as	O
a	O
boolean	O
function	O
of	O
x	O
xcd	O
we	O
may	O
limit	O
boolean	O
classifiers	O
in	O
various	O
ways	O
by	O
partially	O
specifying	O
this	O
function	O
for	O
example	O
lowing	O
natarajan	O
we	O
first	O
consider	O
all	O
monomials	O
that	O
is	O
all	O
functions	O
g	O
ld	O
i	O
of	O
the	O
form	O
boolean	O
classifiers	O
for	O
some	O
k	O
d	O
and	O
some	O
indices	O
il	O
ik	O
d	O
that	O
algebraic	O
tiplication	O
corresponds	O
to	O
a	O
boolean	O
in	O
such	O
situations	O
one	O
might	O
attempt	O
to	O
minimize	O
the	O
empirical	B
error	I
as	O
we	O
know	O
that	O
g	O
is	O
also	O
a	O
monomial	B
it	O
is	O
clear	O
that	O
the	O
minimal	O
empirical	B
error	I
is	O
zero	O
one	O
such	O
minimizing	O
monomial	B
is	O
given	O
by	O
where	O
d	O
an	O
mm	O
ljon	O
xu	O
i	O
d	O
ll	O
lk	O
thus	O
gn	O
picks	O
those	O
components	O
for	O
which	O
every	O
data	O
point	O
has	O
a	O
clearly	O
the	O
empirical	B
error	I
is	O
zero	O
the	O
number	O
of	O
possible	O
functions	O
is	O
therefore	O
by	O
theorem	O
and	O
elgn	O
n	O
here	O
again	O
we	O
have	O
avoided	O
the	O
curse	B
of	I
dimensionality	I
for	O
good	O
performance	O
it	O
suffices	O
that	O
n	O
be	O
a	O
bit	O
larger	O
than	O
d	O
regardless	O
of	O
the	O
distribution	O
of	O
the	O
data	O
assume	O
that	O
we	O
limit	O
the	O
complexity	O
of	O
a	O
boolean	O
classifier	B
g	O
by	O
requiring	O
that	O
g	O
must	O
be	O
written	O
as	O
an	O
expression	B
having	O
at	O
most	O
k	O
operations	O
or	O
with	O
the	O
xis	O
as	O
inputs	O
to	O
avoid	O
problems	O
with	O
precedence	O
rules	O
we	O
assume	O
that	O
any	O
number	O
of	O
parentheses	O
is	O
allowed	O
in	O
the	O
expression	B
one	O
may	O
visualize	O
each	O
expression	B
as	O
an	O
expression	B
tree	O
that	O
is	O
a	O
tree	O
in	O
which	O
internal	O
nodes	O
represent	O
operations	O
and	O
leaves	O
represent	O
operands	O
the	O
number	O
of	O
such	O
binary	B
trees	O
with	O
k	O
internal	O
nodes	O
thus	O
k	O
leaves	O
is	O
given	O
by	O
the	O
catalan	B
number	I
k	O
k	O
e	O
g	O
kemp	O
furthermore	O
we	O
may	O
associate	O
any	O
of	O
the	O
xis	O
with	O
the	O
leaves	O
preceded	O
by	O
and	O
or	O
with	O
each	O
binary	B
internal	O
node	O
thus	O
obtaining	O
a	O
total	O
of	O
not	O
more	O
than	O
k	O
possible	O
boolean	O
functions	O
of	O
this	O
kind	O
as	O
k	O
this	O
bound	O
is	O
not	O
more	O
than	O
for	O
k	O
large	O
enough	O
again	O
for	O
k	O
large	O
enough	O
hypercubes	O
and	O
discrete	O
spaces	O
and	O
elgn	O
k	O
n	O
note	O
that	O
k	O
is	O
much	O
more	O
important	O
than	O
d	O
in	O
determining	O
the	O
sample	O
size	O
for	O
historic	O
reasons	O
we	O
mention	O
that	O
if	O
g	O
is	O
any	O
boolean	O
expression	B
consisting	O
of	O
at	O
most	O
k	O
or	O
operations	O
then	O
the	O
number	O
of	O
such	O
functions	O
was	O
shown	O
by	O
pippenger	O
not	O
to	O
exceed	O
k	O
y	O
pearl	O
used	O
pippengers	O
estimate	O
to	O
obtain	O
performance	O
bounds	O
such	O
as	O
the	O
ones	O
given	O
above	O
series	O
methods	O
for	O
the	O
hypercube	O
it	O
is	O
interesting	O
to	O
note	O
that	O
we	O
may	O
write	O
any	O
function	O
rt	O
on	O
the	O
hypercube	O
as	O
a	O
linear	O
combination	O
of	O
rademacher-walsh	B
polynomials	O
io	O
i	O
id	O
i	O
d	O
i	O
d	O
i	O
we	O
verify	O
easily	O
that	O
so	O
that	O
the	O
form	O
an	O
orthogonal	O
system	O
therefore	O
we	O
may	O
write	O
fl-xrtx	O
l	O
ai	O
io	O
where	O
series	O
methods	O
for	O
the	O
hypercube	O
and	O
px	O
x	O
also	O
l	O
biljix	O
io	O
with	O
bi	O
lrtxl	O
ryx	O
jlx	O
e	O
liyoi	O
sample-based	O
estimates	O
of	O
ai	O
and	O
bi	O
are	O
the	O
bayes	O
rule	B
is	O
given	O
by	O
replacing	O
ai	O
bi	O
formally	O
by	O
b	O
yields	O
the	O
plug-in	O
rule	B
observe	O
that	O
this	O
is	O
just	O
a	O
discrete	O
version	O
of	O
the	O
fourier	O
series	O
rules	O
discussed	O
in	O
chapter	O
this	O
rule	B
requires	O
the	O
estimation	B
of	I
differences	O
ai	O
bi	O
therefore	O
we	O
might	O
as	O
well	O
have	O
used	O
the	O
fundamental	B
rule	B
when	O
our	O
hand	O
is	O
forced	O
by	O
the	O
dimension	B
we	O
may	O
wish	O
to	O
consider	O
only	O
rules	O
in	O
the	O
class	O
c	O
given	O
by	O
gx	O
i	O
if	O
b	O
lr	O
otherwise	O
i	O
i	O
where	O
k	O
d	O
is	O
a	O
positive	O
integer	O
and	O
ab	O
bb	O
ai	O
b	O
are	O
arbitrary	O
constants	O
we	O
have	O
seen	O
that	O
the	O
vc	B
dimension	B
of	O
this	O
class	O
is	O
not	O
more	O
than	O
d	O
k	O
within	O
this	O
class	O
estimation	B
errors	O
of	O
the	O
order	O
of	O
dk	O
log	O
n	O
n	O
are	O
thus	O
possible	O
if	O
we	O
minimize	O
the	O
empirical	B
error	I
this	O
in	O
effect	O
forces	O
us	O
to	O
take	O
k	O
logd	O
n	O
for	O
larger	O
k	O
pattern	O
recognition	O
is	O
all	O
but	O
impossible	O
as	O
an	O
interesting	O
side	O
note	O
observe	O
that	O
for	O
a	O
given	O
parameter	O
k	O
each	O
member	O
of	O
c	O
is	O
a	O
k-th	O
degree	O
polynomial	B
in	O
x	O
remark	O
performance	O
of	O
the	O
plug-in	O
rule	B
define	O
the	O
plug-in	O
rule	B
by	O
gnx	O
i	O
o	O
otherwise	O
if	O
t	O
lr	O
i	O
i	O
hypercubes	O
and	O
discrete	O
spaces	O
as	O
n	O
by	O
the	O
law	O
oflarge	O
numbers	O
gn	O
approaches	O
goo	O
goox	O
ai	O
o	O
otherwise	O
lio	O
b	O
i	O
x	O
where	O
ai	O
bi	O
are	O
as	O
defined	O
above	O
interestingly	O
goo	O
may	O
be	O
much	O
worse	O
than	O
the	O
best	O
rule	B
in	O
c	O
consider	O
the	O
simple	O
example	O
for	O
d	O
k	O
ii	O
xl	O
xl	O
table	O
of	O
ii	O
ii	O
xl	O
xi	O
table	O
of	O
ilx	O
px	O
x	O
p	O
ii	O
s-lop	O
p	O
a	O
simple	O
calculation	O
shows	O
that	O
for	O
any	O
choice	O
p	O
e	O
either	O
goo	O
or	O
goo	O
o	O
we	O
have	O
goo	O
if	O
p	O
and	O
goo	O
otherwise	O
however	O
in	O
obvious	O
notation	O
lg	O
lg	O
the	O
best	O
constant	O
rule	B
is	O
g	O
when	O
p	O
and	O
g	O
otherwise	O
for	O
p	O
the	O
plug-in	O
rule	B
does	O
not	O
even	O
pick	O
the	O
best	O
constant	O
rule	B
let	O
alone	O
the	O
best	O
rule	B
in	O
c	O
with	O
k	O
which	O
it	O
was	O
intended	O
to	O
pick	O
this	O
example	O
highlights	O
the	O
danger	O
of	O
parametric	O
rules	O
or	O
plug-in	O
rules	O
when	O
applied	O
to	O
incorrect	O
or	O
incomplete	O
models	O
remark	O
historical	O
notes	O
the	O
rademacher-walsh	B
expansion	O
occurs	O
frequently	O
in	O
switching	O
theory	O
and	O
was	O
given	O
in	O
duda	O
and	O
hart	O
the	O
feid	O
expansion	O
is	O
similar	O
in	O
nature	O
ito	O
presents	O
error	O
bounds	O
for	O
discrimination	O
based	O
upon	O
a	O
k-term	O
truncation	O
of	O
the	O
series	O
ott	O
and	O
kronmal	O
provide	O
further	O
statistical	O
properties	O
the	O
rules	O
described	O
here	O
with	O
k	O
defining	O
the	O
number	O
of	O
interactions	O
are	O
also	O
obtained	O
if	O
we	O
model	O
p	O
x	O
i	O
y	O
i	O
and	O
p	O
x	O
i	O
y	O
by	O
functions	O
of	O
the	O
form	O
and	O
exp	O
bi	O
oi	O
the	O
latter	O
model	O
is	O
called	O
the	O
log-linear	B
model	I
mclachlan	O
section	O
maximum	B
likelihood	I
the	O
maximum	B
likelihood	I
method	O
chapter	O
should	O
not	O
be	O
used	O
for	O
picking	O
the	O
best	O
rule	B
from	O
a	O
class	O
c	O
that	O
is	O
not	O
guaranteed	O
to	O
include	O
the	O
bayes	O
rule	B
perhaps	O
a	O
simple	O
example	O
will	O
suffice	O
to	O
make	O
the	O
point	O
consider	O
the	O
following	O
hypercube	O
setting	O
with	O
d	O
maximum	B
likelihood	I
px	O
ii	O
xl	O
i	O
xl	O
ii	O
xl	O
p	O
r	O
q	O
s	O
in	O
this	O
example	O
l	O
o	O
apply	O
maximum	B
likelihood	I
to	O
a	O
class	O
f	O
with	O
two	O
members	O
where	O
bx	O
lis	O
if	O
xl	O
if	O
xl	O
if	O
xl	O
if	O
xl	O
then	O
maximum	B
likelihood	I
wont	O
even	O
pick	O
the	O
best	O
member	O
from	O
f	O
to	O
verify	O
this	O
with	O
gax	O
gbx	O
we	O
see	O
that	O
lga	O
p	O
q	O
lg	O
b	O
r	O
s	O
however	O
if	O
is	O
given	O
by	O
and	O
if	O
we	O
write	O
nij	O
for	O
the	O
number	O
of	O
data	O
pairs	O
k	O
yk	O
having	O
x	O
k	O
i	O
yk	O
j	O
then	O
a	O
if	O
o	O
if	O
nol	O
nlo	O
if	O
n	O
n	O
and	O
otherwise	O
equivalently	O
if	O
and	O
only	O
if	O
nlo	O
o	O
apply	O
the	O
strong	O
law	O
of	O
large	O
numbers	O
to	O
note	O
that	O
nolin	O
r	O
nloln	O
s	O
nooln	O
p	O
and	O
nlln	O
q	O
with	O
probability	O
one	O
as	O
n	O
thus	O
lim	O
p	O
n-oo	O
if	O
r	O
s	O
otherwise	O
take	O
r	O
s	O
e	O
very	O
small	O
p	O
q	O
e	O
then	O
for	O
the	O
maximum	B
likelihood	I
rule	B
however	O
when	O
f	O
contains	O
the	O
bayes	O
rule	B
maximum	B
likelihood	I
is	O
consistent	O
theorem	O
hypercubes	O
and	O
discrete	O
spaces	O
kernel	B
methods	O
sometimes	O
d	O
is	O
so	O
large	O
with	O
respect	O
to	O
n	O
that	O
the	O
atoms	O
in	O
the	O
hypercube	O
are	O
sparsely	O
populated	O
some	O
amount	O
of	O
smoothing	O
may	O
help	O
under	O
some	O
stances	O
consider	O
a	O
kernel	B
k	O
and	O
define	O
the	O
kernel	B
rule	B
gnx	O
if	O
xill	O
h	O
i	O
o	O
otherwise	O
n	O
l	O
-i-l	O
where	O
ilx	O
z	O
ii	O
is	O
just	O
the	O
hamming	B
distance	I
the	O
number	O
of	O
disagreements	O
between	O
components	O
of	O
x	O
and	O
z	O
with	O
k	O
e	O
the	O
rule	B
above	O
reduces	O
to	O
a	O
rule	B
given	O
in	O
aitchison	O
and	O
aitken	O
in	O
their	O
paper	O
different	O
h	O
s	O
are	O
considered	O
for	O
the	O
two	O
classes	O
but	O
we	O
wont	O
consider	O
that	O
distinction	O
here	O
observe	O
that	O
at	O
h	O
we	O
obtain	O
the	O
fundamental	B
rule	B
as	O
h	O
we	O
obtain	O
a	O
majority	O
rule	B
over	O
the	O
entire	O
sample	O
the	O
weight	O
given	O
to	O
an	O
observation	O
xi	O
decreases	O
exponentially	O
in	O
ilx	O
xi	O
for	O
consistency	B
we	O
merely	O
need	O
h	O
condition	O
nh	O
d	O
of	O
theorem	O
is	O
no	O
longer	O
needed	O
and	O
in	O
fact	O
we	O
even	O
have	O
consistency	B
with	O
h	O
as	O
this	O
yields	O
the	O
fundamental	B
rule	B
the	O
data-based	B
choice	O
of	O
h	O
has	O
been	O
the	O
object	O
of	O
several	O
papers	O
including	O
hall	O
and	O
hall	O
and	O
wand	O
in	O
the	O
latter	O
paper	O
a	O
mean	O
squared	B
error	I
criterion	O
is	O
minimized	O
we	O
only	O
mention	O
the	O
work	O
of	O
tutz	O
who	O
picks	O
h	O
so	O
as	O
to	O
minimize	O
the	O
deleted	O
estimate	O
ld	O
theorem	O
let	O
hn	O
be	O
the	O
smoothing	B
factor	I
in	O
the	O
aitken	O
rule	B
that	O
minimizes	O
l	O
then	O
the	O
rule	B
is	O
weakly	O
consistent	O
for	O
all	O
butions	O
of	O
y	O
on	O
the	O
hypercube	O
proof	O
see	O
theorem	O
problems	O
and	O
exercises	O
problem	O
the	O
fundamental	B
rule	B
let	O
g	O
be	O
the	O
fundamental	B
rule	B
on	O
a	O
finite	O
set	O
k	O
and	O
define	O
lg	O
let	O
g	O
be	O
the	O
bayes	O
rule	B
error	O
probability	O
l	O
and	O
let	O
inf	O
minrx	O
rx	O
let	O
lr	O
be	O
the	O
resubstitution	B
error	O
estimate	O
apparent	O
error	O
rate	O
show	O
the	O
following	O
e	O
eln	O
apparent	O
error	O
rate	O
is	O
always	O
optimistic	O
hills	O
e	O
l	O
apparent	O
error	O
rate	O
is	O
in	O
fact	O
very	O
optimistic	O
glick	O
eln	O
l	O
a	O
similar	O
inequality	B
related	O
to	O
hellinger	O
distances	O
see	O
glick	O
this	O
is	O
an	O
exponential	B
but	O
distribution-dependent	O
error	O
rate	O
problem	O
discrete	O
lipschitz	O
classes	O
consider	O
the	O
class	O
of	O
regression	O
functions	O
r	O
e	O
with	O
i	O
rx	O
rz	O
i	O
cpx	O
zt	O
where	O
x	O
z	O
e	O
ld	O
p	O
denotes	O
the	O
hamming	B
distance	I
and	O
c	O
and	O
a	O
are	O
constants	O
that	O
a	O
is	O
not	O
bounded	O
from	O
above	O
the	O
purpose	O
is	O
to	O
design	O
a	O
discrimination	O
rule	B
for	O
which	O
uniformly	O
over	O
all	O
distributions	O
of	O
y	O
on	O
x	O
i	O
with	O
such	O
pry	O
x	O
we	O
have	O
problems	O
and	O
exercises	O
el	O
l	O
a	O
d	O
in	O
n	O
where	O
the	O
function	O
a	O
d	O
is	O
as	O
small	O
as	O
possible	O
note	O
for	O
e	O
the	O
class	O
contains	O
all	O
regression	O
functions	O
on	O
the	O
hypercube	O
and	O
thus	O
a	O
d	O
how	O
small	O
should	O
e	O
be	O
to	O
make	O
polynomial	B
in	O
d	O
problem	O
with	O
a	O
cubic	B
histogram	O
partition	B
of	O
into	O
kd	O
cells	O
volume	O
kd	O
each	O
we	O
have	O
for	O
the	O
lipschitz	O
a	O
classf	O
of	O
theorem	O
sup	O
e	O
cxyef	O
k	O
this	O
grows	O
as	O
d	O
a	O
hint	O
consult	O
conway	O
and	O
sloane	O
can	O
you	O
define	O
a	O
partition	B
into	O
k	O
d	O
cells	O
for	O
which	O
supcx	O
yef	O
is	O
smaller	O
problem	O
consider	O
the	O
following	O
randomized	B
histogram	B
rule	B
xl	O
x	O
k	O
partition	B
into	O
polyhedra	O
based	O
on	O
the	O
nearest	B
neighbor	I
rule	B
within	O
each	O
cell	O
we	O
employ	O
a	O
majority	O
rule	B
based	O
upon	O
x	O
kl	O
x	O
n	O
if	O
x	O
is	O
uniform	O
on	O
and	O
is	O
lipschitz	O
a	O
in	O
theorem	O
then	O
can	O
you	O
derive	O
an	O
upper	O
bound	O
for	O
el	O
n	O
l	O
as	O
a	O
function	O
of	O
k	O
n	O
e	O
a	O
and	O
d	O
how	O
does	O
your	O
bound	O
compare	O
with	O
the	O
cubic	B
histogram	B
rule	B
that	O
uses	O
the	O
same	O
number	O
of	O
cells	O
problem	O
letf	O
be	O
the	O
class	O
of	O
all	O
lipschitz	O
a	O
functions	O
e	O
nd	O
let	O
y	O
e	O
denote	O
the	O
fact	O
that	O
y	O
has	O
regression	B
function	I
py	O
x	O
in	O
f	O
then	O
for	O
any	O
cubic	B
histogram	B
rule	B
show	O
that	O
sup	O
e	O
cxyef	O
ln	O
l	O
l	O
thus	O
the	O
compactness	O
condition	O
on	O
the	O
space	O
is	O
essential	O
for	O
the	O
distribution-free	O
error	O
bound	O
given	O
in	O
theorem	O
problem	O
independent	O
model	O
show	O
that	O
in	O
the	O
independent	O
model	O
the	O
maximum	B
likelihood	I
estimate	O
pi	O
of	O
pi	O
is	O
given	O
by	O
rl	O
ixjiliyjl	O
rl	O
iyjl	O
problem	O
independent	O
model	O
for	O
the	O
plug-in	O
rule	B
in	O
the	O
independent	O
model	O
is	O
it	O
true	O
that	O
eln	O
l	O
in	O
uniformly	O
over	O
all	O
pairs	O
y	O
on	O
id	O
x	O
i	O
if	O
so	O
find	O
a	O
constant	O
e	O
depending	O
upon	O
d	O
only	O
such	O
that	O
el	O
n	O
l	O
e	O
if	O
not	O
provide	O
a	O
counterexample	O
problem	O
consider	O
a	O
hypercube	O
problem	O
in	O
which	O
x	O
xcd	O
and	O
each	O
xci	O
e	O
ternary	O
generalization	O
assume	O
that	O
the	O
xus	O
are	O
independent	O
but	O
not	O
hypercubes	O
and	O
discrete	O
spaces	O
necessarily	O
identically	O
distributed	O
show	O
that	O
there	O
exists	O
a	O
quadratic	O
bayes	O
rule	B
i	O
e	O
gx	O
is	O
on	O
the	O
set	O
d	O
ao	O
l	O
aixi	O
l	O
aijxi	O
xj	O
d	O
i	O
ijl	O
where	O
ao	O
and	O
are	O
some	O
weights	O
and	O
steinbuch	O
problem	O
let	O
a	O
be	O
the	O
class	O
of	O
all	O
sets	O
on	O
the	O
hypercube	O
of	O
the	O
form	O
xci	O
xik	O
where	O
xed	O
e	O
ld	O
ik	O
d	O
a	O
is	O
the	O
class	O
of	O
all	O
sets	O
carved	O
out	O
by	O
the	O
monomials	O
show	O
that	O
the	O
vc	B
dimension	B
of	O
cis	O
d	O
hint	O
the	O
set	O
a	O
is	O
shattered	O
by	O
a	O
no	O
set	O
of	O
size	O
d	O
can	O
be	O
shattered	O
by	O
a	O
by	O
the	O
pigeonhole	B
principle	I
problem	O
show	O
that	O
the	O
catalan	B
number	I
n	O
i	O
n	O
e	O
g	O
kemp	O
problem	O
provide	O
an	O
argument	O
to	O
show	O
that	O
the	O
number	O
of	O
boolean	O
functions	O
with	O
at	O
most	O
k	O
operations	O
or	O
and	O
d	O
operands	O
of	O
the	O
form	O
xu	O
or	O
i	O
xi	O
xci	O
e	O
i	O
is	O
not	O
more	O
than	O
k	O
k	O
is	O
times	O
less	O
than	O
the	O
bound	O
given	O
in	O
the	O
text	O
problem	O
provide	O
upper	O
and	O
lower	O
bounds	O
on	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
of	O
sets	O
a	O
on	O
the	O
hypercube	O
ld	O
that	O
can	O
be	O
described	O
by	O
a	O
boolean	O
expression	B
with	O
the	O
xis	O
or	O
xis	O
as	O
operands	O
and	O
with	O
at	O
most	O
k	O
operations	O
or	O
problem	O
linear	O
discrimination	O
on	O
the	O
hypercube	O
let	O
gn	O
be	O
the	O
rule	B
that	O
mizes	O
the	O
empirical	B
error	I
ln	O
over	O
all	O
linear	O
rules	O
when	O
the	O
data	O
are	O
drawn	O
from	O
any	O
distribution	O
on	O
ld	O
x	O
i	O
let	O
ln	O
be	O
its	O
probability	O
of	O
error	O
and	O
let	O
l	O
be	O
the	O
minimum	O
error	O
probability	O
over	O
all	O
linear	O
rules	O
show	O
that	O
for	O
e	O
deduce	O
that	O
el	O
n	O
l	O
j	O
ffn	O
d	O
ffn	O
compare	O
this	O
result	O
with	O
the	O
general	O
vapnik-chervonenkis	B
bound	O
for	O
linear	O
rules	O
and	O
deduce	O
when	O
the	O
bound	O
given	O
above	O
is	O
better	O
hint	O
count	O
the	O
number	O
of	O
possible	O
linear	O
rules	O
problem	O
on	O
the	O
hypercube	O
to	O
ld	O
show	O
that	O
the	O
kernel	B
rule	B
of	O
aitchison	O
and	O
aitken	O
is	O
strongly	O
consistent	O
when	O
limn-oo	O
h	O
problem	O
pick	O
h	O
in	O
the	O
kernel	B
estimate	O
by	O
minimizing	O
the	O
resubstitution	B
estimate	O
lr	O
and	O
call	O
it	O
hy	O
for	O
ld	O
we	O
call	O
it	O
hd	O
assume	O
that	O
the	O
kernel	B
function	O
is	O
of	O
the	O
form	O
kii	O
iii	O
h	O
with	O
k	O
ku	O
as	O
u	O
t	O
let	O
ln	O
be	O
the	O
error	O
estimate	O
for	O
the	O
kernel	B
rule	B
with	O
one	O
of	O
these	O
two	O
choices	O
is	O
it	O
possible	O
to	O
find	O
a	O
constant	O
c	O
depending	O
upon	O
d	O
and	O
k	O
only	O
such	O
that	O
problems	O
and	O
exercises	O
e	O
inf	O
lnhs	O
h	O
o	O
yn	O
if	O
so	O
give	O
a	O
proof	O
if	O
not	O
provide	O
a	O
counterexample	O
note	O
if	O
the	O
answer	O
is	O
positive	O
a	O
minor	O
corollary	O
of	O
this	O
result	O
is	O
tutzs	O
theorem	O
however	O
an	O
explicit	O
constant	O
c	O
may	O
aid	O
in	O
determining	O
appropriate	O
sample	O
sizes	O
it	O
may	O
also	O
be	O
minimized	O
with	O
respect	O
to	O
k	O
problem	O
construct	O
a	O
partition	B
of	O
the	O
hypercube	O
ld	O
in	O
the	O
lowing	O
manner	O
based	O
upon	O
a	O
binary	B
classification	O
tree	O
with	O
perpendicular	O
splits	O
every	O
node	O
at	O
level	O
i	O
splits	O
the	O
subset	O
according	O
to	O
xi	O
or	O
xi	O
so	O
that	O
there	O
are	O
at	O
most	O
d	O
levels	O
of	O
nodes	O
practice	O
the	O
most	O
important	O
component	O
should	O
be	O
xl	O
for	O
example	O
all	O
possible	O
partitions	O
of	O
obtainable	O
with	O
cuts	O
are	O
i	O
assign	O
to	O
each	O
internal	O
node	O
region	O
a	O
class	O
define	O
the	O
horton-strahler	B
number	I
of	I
a	O
tree	O
as	O
follows	O
if	O
a	O
tree	O
has	O
one	O
node	O
then	O
if	O
the	O
root	O
of	O
the	O
tree	O
has	O
left	O
and	O
right	O
subtrees	O
with	O
horton-strahler	O
numbers	O
and	O
then	O
set	O
let	O
c	O
be	O
the	O
class	O
of	O
classifiers	O
g	O
described	O
above	O
with	O
horton-strahler	O
number	O
let	O
s	O
e	O
ilx	O
ii	O
n	O
where	O
ii	O
ii	O
denotes	O
hamming	B
distance	I
from	O
the	O
all-zero	O
vector	O
show	O
that	O
s	O
is	O
shattered	O
by	O
the	O
class	O
of	O
sets	O
g	O
e	O
c	O
show	O
that	O
lsi	O
ito	O
e	O
conclude	O
that	O
the	O
vc	B
dimension	B
of	O
c	O
is	O
at	O
least	O
has	O
shown	O
that	O
the	O
vc	B
dimension	B
of	O
c	O
is	O
exactly	O
this	O
but	O
that	O
proof	O
is	O
more	O
involved	O
assuming	O
l	O
obtain	O
an	O
upper	O
bound	O
for	O
el	O
n	O
as	O
afunctionof	O
andd	O
where	O
ln	O
is	O
the	O
probability	O
of	O
eltor	O
for	O
the	O
rule	B
picked	O
by	O
minimizing	O
the	O
empirical	B
eltor	O
over	O
c	O
interpret	O
as	O
the	O
height	B
of	I
the	O
largest	O
complete	O
binary	B
tree	O
that	O
can	O
be	O
embedded	O
in	O
the	O
classification	O
tree	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
definitions	O
this	O
chapter	O
deals	O
with	O
discrimination	O
rules	O
that	O
are	O
picked	O
from	O
a	O
certain	O
class	O
of	O
classifiers	O
by	O
minimizing	O
the	O
empirical	B
probability	O
of	O
error	O
over	O
a	O
finite	O
set	O
of	O
carefully	O
selected	O
rules	O
we	O
begin	O
with	O
a	O
class	O
f	O
of	O
regression	O
functions	O
a	B
posteriori	I
probability	I
functions	O
tj	O
nd	O
from	O
which	O
tjn	O
will	O
be	O
picked	O
by	O
the	O
data	O
the	O
massiveness	O
of	O
f	O
can	O
be	O
measured	O
in	O
many	O
ways-the	O
route	O
followed	O
here	O
is	O
suggested	O
in	O
the	O
work	O
of	O
kolmogorov	O
and	O
tikhomirov	O
we	O
will	O
depart	O
from	O
their	O
work	O
only	O
in	O
details	O
we	O
suggest	O
comparing	O
the	O
results	O
here	O
with	O
those	O
from	O
chapters	O
and	O
let	O
fe	O
tjn	O
be	O
a	O
finite	O
collection	O
of	O
functions	O
nd	O
such	O
that	O
where	O
stie	O
is	O
the	O
ball	O
of	O
all	O
functions	O
nd	O
with	O
ii	O
tjiioo	O
sup	O
ix	O
e	O
x	O
in	O
other	O
words	O
for	O
each	O
tj	O
e	O
f	O
there	O
exists	O
an	O
tju	O
e	O
fe	O
with	O
supx	O
itjx	O
tjix	O
i	O
e	O
the	O
fewer	O
tjis	O
needed	O
to	O
cover	O
f	O
the	O
smaller	O
f	O
is	O
in	O
a	O
certain	O
sense	O
fe	O
is	O
called	O
an	O
e	O
r	O
of	O
f	O
the	O
minimal	O
value	O
of	O
i	O
fe	O
lover	O
all	O
e	O
is	O
called	O
the	O
e-covering	O
number	O
following	O
kolmogorov	O
and	O
tikhomirov	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
is	O
called	O
the	O
e	O
of	O
f	O
we	O
will	O
also	O
call	O
it	O
the	O
metric	B
entropy	B
a	O
collection	O
f	O
is	O
totally	O
bounded	O
if	O
ne	O
for	O
all	O
e	O
o	O
it	O
is	O
with	O
such	O
classes	O
that	O
we	O
are	O
concerned	O
in	O
this	O
chapter	O
the	O
next	O
section	O
gives	O
a	O
few	O
examples	O
in	O
the	O
following	O
section	O
we	O
define	O
the	O
skeleton	B
estimate	I
based	O
upon	O
picking	O
the	O
empirically	O
best	O
member	O
from	O
fe	O
figure	O
an	O
e	O
of	O
the	O
unit	O
square	O
examples	O
totally	O
bounded	O
classes	O
the	O
simple	O
scalar	O
parametric	O
class	O
f	O
x	O
e	O
r	O
o	O
is	O
not	O
totally	O
bounded	O
this	O
is	O
due	O
simply	O
to	O
the	O
presence	O
of	O
an	O
unrestricted	O
scale	O
factor	O
it	O
would	O
still	O
fail	O
to	O
be	O
totally	O
bounded	O
if	O
we	O
restricted	O
to	O
or	O
however	O
if	O
we	O
force	O
e	O
and	O
change	O
the	O
class	O
f	O
to	O
have	O
functions	O
if	O
ixl	O
otherwise	O
then	O
the	O
class	O
is	O
totally	O
bounded	O
while	O
it	O
is	O
usually	O
difficult	O
to	O
compute	O
n	O
exactly	O
it	O
is	O
often	O
simple	O
to	O
obtain	O
matching	O
upper	O
and	O
lower	O
bounds	O
here	O
is	O
a	O
simple	O
argument	O
take	O
i	O
i	O
l	O
i	O
j	O
j	O
and	O
define	O
each	O
of	O
these	O
defines	O
a	O
function	O
collect	O
these	O
in	O
fe	O
note	O
that	O
with	O
e	O
f	O
with	O
parameter	O
if	O
e	O
is	O
the	O
nearest	O
value	O
among	O
i	O
l	O
j	O
j	O
u	O
then	O
ie	O
e	O
but	O
then	O
sup	O
el	O
e	O
ixisl	O
hence	O
fe	O
is	O
an	O
e-cover	O
of	O
f	O
of	O
cardinality	O
we	O
conclude	O
that	O
f	O
is	O
totally	O
bounded	O
and	O
that	O
problem	O
for	O
a	O
d-dimensional	O
generalization	O
ne	O
examples	O
totally	O
bounded	O
classes	O
for	O
a	O
lower	O
bound	O
we	O
use	O
once	O
again	O
an	O
idea	O
from	O
kolmogorov	O
and	O
tikhomirov	O
let	O
oe	O
c	O
f	O
be	O
a	O
subset	O
with	O
the	O
property	O
that	O
for	O
every	O
i	O
j	O
supx	O
e	O
the	O
set	O
oe	O
is	O
thus	O
e-separated	O
the	O
maximal	O
cardinality	O
e	O
set	O
is	O
called	O
the	O
e	O
number	O
e	O
number	O
me	O
it	O
is	O
easy	O
to	O
see	O
that	O
me	O
with	O
this	O
in	O
hand	O
we	O
see	O
that	O
oe	O
may	O
be	O
constructed	O
as	O
follows	O
for	O
our	O
example	O
begin	O
with	O
then	O
define	O
by	O
e	O
etcetera	O
until	O
it	O
is	O
clear	O
that	O
this	O
wayan	O
e	O
set	O
oe	O
may	O
be	O
constructed	O
with	O
ioe	O
i	O
l	O
e	O
e	O
j	O
thus	O
ne	O
the	O
e-entropy	O
of	O
f	O
grows	O
as	O
as	O
e	O
t	O
consider	O
next	O
a	O
larger	O
class	O
not	O
of	O
a	O
parametric	O
nature	O
let	O
f	O
be	O
a	O
class	O
of	O
functions	O
on	O
ll	O
satisfying	O
the	O
lipschitz	O
condition	O
i	O
clx	O
xii	O
and	O
taking	O
values	O
on	O
kolmogorov	O
and	O
tikhomirov	O
showed	O
that	O
if	O
e	O
min	O
then	O
n	O
e	O
llc	O
e	O
i	O
e	O
problem	O
observe	O
that	O
the	O
metric	B
entropy	B
is	O
exponentially	O
larger	O
than	O
for	O
the	O
parametric	O
class	O
considered	O
above	O
this	O
has	O
a	O
major	O
impact	O
on	O
sample	O
sizes	O
needed	O
for	O
similar	O
performances	O
the	O
next	O
sections	O
another	O
class	O
of	O
functions	O
of	O
interest	O
is	O
that	O
containing	O
functions	O
that	O
are	O
s-times	O
differentiable	O
and	O
for	O
which	O
the	O
s-th	O
derivative	O
satisfies	O
a	O
holder	O
condition	O
of	O
order	O
ex	O
clx	O
xll	O
a	O
x	O
xl	O
e	O
where	O
c	O
is	O
a	O
constant	O
in	O
that	O
case	O
me	O
and	O
are	O
both	O
as	O
e	O
t	O
where	O
an	O
means	O
that	O
an	O
obn	O
and	O
bn	O
oan	O
this	O
result	O
also	O
due	O
to	O
kolmogorov	O
and	O
tikhomirov	O
establishes	O
a	O
continuum	O
of	O
rates	O
of	O
increase	O
of	O
the	O
e-entropy	O
in	O
n	O
d	O
with	O
functions	O
n	O
ld	O
if	O
the	O
holder	O
condition	O
holds	O
for	O
all	O
derivatives	O
of	O
order	O
s	O
then	O
ne	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
here	O
we	O
have	O
an	O
interesting	O
interpretation	O
of	O
dimension	B
doubling	O
the	O
dimension	B
roughly	O
offsets	O
the	O
effect	O
of	O
doubling	O
the	O
number	O
of	O
derivatives	O
the	O
degree	O
of	O
smoothness	O
of	O
the	O
working	O
with	O
lipschitz	O
functions	O
on	O
rl	O
is	O
roughly	O
equivalent	O
to	O
working	O
with	O
functions	O
on	O
r	O
for	O
which	O
all	O
order	O
derivatives	O
are	O
lipschitz	O
as	O
there	O
are	O
such	O
derivatives	O
we	O
note	O
immediately	O
how	O
much	O
we	O
must	O
pay	O
for	O
certain	O
performances	O
in	O
high	O
dimensions	O
let	O
be	O
the	O
class	O
of	O
all	O
entire	O
analytic	O
functions	O
whose	O
periodic	O
continuation	O
satisfies	O
for	O
some	O
constants	O
c	O
a	O
is	O
a	O
complex	O
variable	B
is	O
its	O
imaginary	O
part	O
for	O
this	O
class	O
we	O
know	O
that	O
as	O
eo	O
and	O
tikhomirov	O
the	O
class	O
appears	O
to	O
be	O
as	O
small	O
as	O
our	O
parametric	O
class	O
see	O
also	O
vitushkin	O
skeleton	O
estimates	O
the	O
members	O
of	O
form	O
a	O
representative	O
skeleton	O
of	O
f	O
we	O
assume	O
that	O
fe	O
c	O
f	O
condition	O
was	O
not	O
imposed	O
in	O
the	O
definition	B
of	I
an	O
e-cover	O
for	O
each	O
e	O
f	O
we	O
define	O
its	O
discrimination	O
rule	B
as	O
g	O
if	O
otherwise	O
thus	O
we	O
will	O
take	O
the	O
liberty	O
of	O
referring	O
to	O
as	O
a	O
rule	B
for	O
each	O
such	O
we	O
define	O
the	O
probability	O
of	O
error	O
as	O
usual	O
pgx	O
y	O
the	O
empirical	B
probability	O
of	O
error	O
of	O
is	O
denoted	O
by	O
we	O
define	O
the	O
skeleton	B
estimate	I
by	O
argminln	O
refe	O
one	O
of	O
the	O
best	O
rules	O
in	O
is	O
denoted	O
by	O
e	O
f	O
skeleton	O
estimates	O
the	O
first	O
objective	O
as	O
in	O
standard	B
empirical	B
risk	I
minimization	I
is	O
to	O
ensure	O
that	O
is	O
close	O
to	O
lrj	O
if	O
the	O
true	O
a	B
posteriori	I
probability	I
function	O
rj	O
is	O
in	O
f	O
that	O
py	O
x	O
n	O
then	O
it	O
is	O
clear	O
that	O
l	O
lrj	O
it	O
will	O
be	O
seen	O
from	O
the	O
theorem	O
below	O
that	O
under	O
this	O
condition	O
the	O
skeleton	B
estimate	I
has	O
nice	O
consistency	B
and	O
rate-of-convergence	O
properties	O
the	O
result	O
is	O
distribution-free	O
in	O
the	O
sense	O
that	O
no	O
structure	O
on	O
the	O
distribution	O
of	O
x	O
is	O
assumed	O
problems	O
and	O
show	O
that	O
convergence	O
of	O
lrjn	O
to	O
lrj	O
for	O
all	O
rjs-that	O
is	O
not	O
only	O
for	O
those	O
in	O
f-holds	O
if	O
x	O
has	O
a	O
density	O
in	O
any	O
case	O
because	O
the	O
skeleton	B
estimate	I
is	O
selected	O
from	O
a	O
finite	O
deterministic	O
set	O
may	O
be	O
constructed	O
before	O
data	O
are	O
collected	O
probability	O
bounding	O
is	O
trivial	O
for	O
all	O
e	O
we	O
have	O
p	O
inf	O
lrj	O
refe	O
ife	O
i	O
sup	O
p	O
tjefe	O
lemma	O
le-	O
inequality	B
theorem	O
let	O
f	O
be	O
a	O
totally	O
bounded	O
class	O
offunctions	O
rj	O
nd	O
there	O
is	O
a	O
sequence	O
o	O
and	O
a	O
sequence	O
of	O
skeletons	O
fen	O
c	O
f	O
such	O
that	O
if	O
rjn	O
is	O
the	O
skeleton	B
estimate	I
drawn	O
from	O
fen	O
then	O
lrjn	O
l	O
with	O
probability	O
one	O
whenever	O
the	O
true	O
regression	B
function	I
py	O
x	O
is	O
in	O
f	O
it	O
suffices	O
to	O
take	O
fe	O
as	O
an	O
e	O
of	O
f	O
that	O
ife	O
i	O
need	O
not	O
equal	O
the	O
e	O
number	O
ne	O
and	O
to	O
define	O
en	O
as	O
the	O
smallest	O
positive	O
number	O
for	O
which	O
finally	O
with	O
en	O
picked	O
in	O
this	O
manner	O
e	O
l	O
vs	O
n	O
tf	O
proof	O
we	O
note	O
that	O
inftjefe	O
lrj	O
s	O
l	O
because	O
if	O
rj	O
e	O
stje	O
then	O
elrjx	O
s	O
sup	O
irx	O
s	O
e	O
x	O
and	O
thus	O
by	O
theorem	O
l	O
l	O
s	O
then	O
for	O
any	O
l	O
plrjn	O
inf	O
le-	O
above	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
which	O
is	O
summable	O
in	O
n	O
as	O
en	O
this	O
shows	O
the	O
first	O
part	O
of	O
the	O
theorem	O
for	O
the	O
second	O
part	O
we	O
have	O
l	O
inf	O
r	O
efen	O
min	O
i	O
dt	O
vsen	O
jsen	O
jsen	O
t	O
e-	O
e	O
t	O
for	O
t	O
the	O
proof	O
is	O
completed	O
observe	O
that	O
the	O
estimate	O
is	O
picked	O
from	O
a	O
deterministic	O
class	O
this	O
of	O
course	O
requires	O
quite	O
a	O
bit	O
of	O
preparation	O
and	O
knowledge	O
on	O
behalf	O
of	O
the	O
user	O
knowledge	O
of	O
the	O
e-entropy	O
at	O
least	O
an	O
upper	O
bound	O
is	O
absolutely	O
essential	O
furthermore	O
one	O
must	O
be	O
able	O
to	O
construct	O
fe	O
this	O
is	O
certainly	O
not	O
computationally	O
simple	O
skeleton	O
estimates	O
should	O
therefore	O
be	O
mainly	O
of	O
theoretical	B
importance	O
they	O
may	O
be	O
used	O
for	O
example	O
to	O
establish	O
the	O
existence	O
of	O
estimates	O
with	O
a	O
guaranteed	O
error	O
bound	O
as	O
given	O
in	O
theorem	O
a	O
similar	O
idea	O
in	O
nonparametric	O
density	B
estimation	B
was	O
proposed	O
and	O
worked	O
out	O
in	O
devroye	O
remark	O
in	O
the	O
first	O
step	O
of	O
the	O
proof	O
we	O
used	O
the	O
inequality	B
sup	O
i	O
i	O
e	O
x	O
it	O
is	O
clear	O
from	O
this	O
that	O
what	O
we	O
really	O
need	O
is	O
not	O
an	O
e	O
of	O
f	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
but	O
rather	O
with	O
respect	O
to	O
the	O
l	O
norm	O
in	O
other	O
words	O
the	O
skeleton	B
estimate	I
works	O
equally	O
well	O
if	O
the	O
skeleton	O
is	O
an	O
e	O
of	O
f	O
with	O
respect	O
to	O
the	O
ll	O
norm	O
that	O
is	O
it	O
is	O
a	O
list	O
of	O
finitely	O
many	O
candidates	O
with	O
the	O
property	O
that	O
for	O
each	O
e	O
f	O
there	O
exists	O
an	O
lji	O
in	O
the	O
list	O
such	O
that	O
i	O
e	O
it	O
follows	O
from	O
the	O
inequality	B
above	O
that	O
the	O
smallest	O
such	O
covering	O
has	O
fewer	O
elements	O
than	O
that	O
of	O
any	O
e	O
of	O
f	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
therefore	O
estimates	O
based	O
on	O
such	O
skeletons	O
perform	O
better	O
in	O
fact	O
the	O
difference	O
may	O
be	O
essential	O
as	O
an	O
example	O
consider	O
the	O
class	O
f	O
of	O
all	O
regression	O
functions	O
on	O
that	O
are	O
monotone	O
increasing	O
for	O
e	O
this	O
class	O
does	O
not	O
have	O
a	O
finite	O
e	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
however	O
for	O
any	O
m	O
it	O
is	O
possible	O
to	O
find	O
an	O
e-cover	O
of	O
f	O
with	O
respect	O
to	O
with	O
not	O
more	O
rate	B
of	I
convergence	I
than	O
elements	O
unfortunately	O
an	O
e-cover	O
with	O
respect	O
to	O
llll	O
depends	O
on	O
il	O
the	O
distribution	O
of	O
x	O
since	O
il	O
is	O
not	O
known	O
in	O
advance	O
we	O
cannot	O
construct	O
this	O
better	O
skeleton	O
however	O
in	O
some	O
cases	O
we	O
may	O
have	O
some	O
a	O
priori	O
information	O
about	O
j	O
l	O
forexample	O
if	O
we	O
know	O
that	O
j	O
l	O
is	O
a	O
member	O
of	O
a	O
known	O
class	O
of	O
distributions	O
then	O
we	O
may	O
be	O
able	O
to	O
construct	O
a	O
skeleton	O
that	O
is	O
an	O
e	O
for	O
all	O
measures	O
in	O
the	O
class	O
in	O
the	O
above	O
example	O
if	O
we	O
know	O
that	O
il	O
has	O
a	O
density	O
bounded	O
by	O
a	O
known	O
number	O
then	O
there	O
is	O
a	O
finite	O
skeleton	O
with	O
this	O
property	O
problem	O
we	O
note	O
here	O
that	O
the	O
basic	O
idea	O
behind	O
the	O
empirical	B
covering	O
method	O
of	O
buescher	O
and	O
kumar	O
described	O
in	O
problem	O
is	O
to	O
find	O
a	O
good	O
skeleton	O
based	O
on	O
a	O
fraction	O
of	O
the	O
data	O
investigating	O
this	O
question	O
further	O
we	O
notice	O
that	O
even	O
covering	O
in	O
l	O
i	O
is	O
more	O
than	O
what	O
we	O
really	O
need	O
from	O
the	O
proof	O
of	O
theorem	O
we	O
see	O
that	O
all	O
we	O
need	O
is	O
a	O
skeleton	O
fe	O
such	O
that	O
infrylefe	O
l	O
e	O
for	O
all	O
e	O
f	O
staying	O
with	O
the	O
example	O
of	O
the	O
class	O
of	O
monotonically	O
increasing	O
we	O
see	O
that	O
we	O
may	O
take	O
in	O
fe	O
the	O
functions	O
ix	O
q	O
where	O
qi	O
is	O
the	O
i-th	O
e-quantile	O
of	O
j	O
l	O
that	O
is	O
qi	O
is	O
the	O
smallest	O
number	O
z	O
such	O
that	O
p	O
z	O
i	O
e	O
this	O
collection	O
of	O
functions	O
forms	O
a	O
skeleton	O
in	O
the	O
required	O
sense	O
with	O
about	O
elements	O
instead	O
of	O
the	O
obtained	O
by	O
covering	O
in	O
l	O
i	O
a	O
significant	O
improvement	O
problem	O
illustrates	O
another	O
application	O
of	O
this	O
idea	O
for	O
more	O
work	O
on	O
this	O
we	O
refer	O
to	O
vapnik	O
benedek	O
and	O
itai	O
kulkarni	O
dudley	O
kulkarni	O
richardson	O
and	O
zeitouni	O
and	O
buescher	O
and	O
kumar	O
rate	B
of	I
convergence	I
in	O
this	O
section	O
we	O
take	O
a	O
closer	O
look	O
at	O
the	O
distribution-free	O
upper	O
bound	O
e	O
iln	O
l	O
viscn	O
tf	O
for	O
typical	O
parametric	O
classes	O
as	O
the	O
one	O
discussed	O
in	O
a	O
section	O
we	O
have	O
if	O
we	O
take	O
ife	O
i	O
close	O
enough	O
to	O
ne	O
then	O
ell	O
is	O
the	O
solution	O
of	O
n	O
e	O
or	O
en	O
glog	O
n	O
and	O
we	O
achieve	O
a	O
guaranteed	O
rate	O
of	O
n	O
the	O
same	O
is	O
true	O
for	O
the	O
example	O
of	O
the	O
class	O
of	O
analytic	O
functions	O
discussed	O
earlier	O
the	O
situation	O
is	O
different	O
for	O
massive	O
classes	O
such	O
as	O
the	O
lipschitz	O
functions	O
on	O
ld	O
recalling	O
that	O
as	O
e	O
we	O
note	O
that	O
ell	O
gn-	O
for	O
this	O
class	O
we	O
have	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
here	O
once	O
again	O
we	O
encounter	O
the	O
phenomenon	O
called	O
the	O
of	O
ality	O
in	O
order	O
to	O
achieve	O
the	O
performance	O
e	O
l	O
s	O
e	O
we	O
need	O
a	O
sample	O
of	O
size	O
n	O
exponentially	O
large	O
in	O
d	O
note	O
that	O
the	O
class	O
of	O
classifiers	O
defined	O
by	O
this	O
class	O
of	O
functions	O
has	O
infinite	O
vc	B
dimension	B
the	O
skeleton	O
estimates	O
thus	O
provide	O
a	O
vehicle	O
for	O
dealing	O
with	O
very	O
large	O
classes	O
finally	O
if	O
we	O
take	O
all	O
on	O
forwhichs	O
derivatives	O
exist	O
and	O
is	O
lipschitz	O
with	O
a	O
given	O
constant	O
c	O
similar	O
considerations	O
show	O
that	O
the	O
rate	B
of	I
convergence	I
is	O
which	O
ranges	O
from	O
s	O
to	O
s	O
as	O
the	O
class	O
becomes	O
smaller	O
we	O
can	O
guarantee	O
better	O
rates	O
of	O
convergence	O
of	O
course	O
this	O
requires	O
more	O
a	O
priori	O
knowledge	O
about	O
the	O
true	O
we	O
also	O
note	O
that	O
if	O
log	O
in	O
theorem	O
then	O
the	O
bound	O
is	O
vsen	O
vs	O
logll	O
v	O
v	O
the	O
error	O
grows	O
only	O
sub-logarithmically	O
in	O
the	O
size	O
of	O
the	O
skeleton	O
set	O
it	O
grows	O
as	O
the	O
square	O
root	O
of	O
the	O
e	O
roughly	O
speaking	O
ignoring	O
the	O
dependence	O
of	O
en	O
upon	O
n	O
we	O
may	O
say	O
that	O
for	O
the	O
same	O
performance	O
guarantees	O
doubling	O
the	O
e	O
implies	O
that	O
we	O
should	O
double	O
the	O
sample	O
size	O
keep	O
log	O
n	O
constant	O
when	O
referring	O
to	O
e-entropy	O
it	O
is	O
important	O
to	O
keep	O
this	O
sample	O
size	O
interpretation	O
in	O
mind	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
the	O
class	O
of	O
functions	O
on	O
the	O
real	O
line	O
with	O
parameter	O
is	O
not	O
totally	O
bounded	O
problem	O
compute	O
a	O
good	O
upper	O
bound	O
for	O
n	O
as	O
a	O
function	O
of	O
d	O
and	O
e	O
for	O
the	O
class	O
f	O
of	O
all	O
functions	O
on	O
n	O
d	O
given	O
by	O
if	O
ilxll	O
otherwise	O
where	O
e	O
is	O
a	O
parameter	O
repeat	O
this	O
question	O
if	O
are	O
in	O
and	O
hint	O
both	O
answers	O
are	O
polynomial	B
in	O
lie	O
as	O
e	O
problem	O
show	O
that	O
me	O
for	O
any	O
totally	O
bounded	O
set	O
f	O
and	O
tikhomirov	O
problem	O
find	O
a	O
class	O
f	O
of	O
functions	O
rd	O
such	O
that	O
for	O
every	O
e	O
it	O
has	O
a	O
finite	O
e-cover	O
the	O
vc	B
dimension	B
of	O
a	O
e	O
f	O
is	O
infinite	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
if	O
f	O
ljl	O
is	O
lipschitz	O
with	O
constant	O
c	O
then	O
for	O
e	O
small	O
enough	O
ne	O
s	O
ale	O
where	O
a	O
is	O
a	O
constant	O
depending	O
upon	O
and	O
c	O
is	O
not	O
as	O
precise	O
as	O
the	O
statement	O
in	O
the	O
text	O
obtained	O
by	O
kolmogorov	O
and	O
tikhomirov	O
but	O
it	O
will	O
give	O
you	O
excellent	O
practice	O
problem	O
obtain	O
an	O
estimate	O
for	O
the	O
cardinality	O
of	O
the	O
smallest	O
e-cover	O
with	O
respect	O
to	O
the	O
l	O
norm	O
for	O
the	O
class	O
of	O
ls	O
on	O
that	O
are	O
increasing	O
in	O
particular	O
show	O
that	O
for	O
any	O
f	O
l	O
it	O
is	O
possible	O
to	O
find	O
an	O
e-cover	O
with	O
elements	O
can	O
you	O
do	O
something	O
similar	O
for	O
ljs	O
on	O
that	O
are	O
increasing	O
in	O
each	O
coordinate	O
problem	O
consider	O
the	O
class	O
of	O
ls	O
on	O
that	O
are	O
increasing	O
show	O
that	O
for	O
every	O
e	O
there	O
is	O
a	O
finite	O
list	O
ll	O
ljn	O
such	O
that	O
for	O
all	O
lj	O
in	O
the	O
class	O
whenever	O
x	O
has	O
a	O
density	O
bounded	O
by	O
b	O
estimate	O
the	O
smallest	O
such	O
n	O
problem	O
assume	O
that	O
x	O
has	O
a	O
bounded	O
density	O
on	O
and	O
that	O
lj	O
is	O
monotonically	O
increasing	O
in	O
both	O
coordinates	O
is	O
a	O
reasonable	O
assumption	O
in	O
many	O
applications	O
then	O
the	O
set	O
gx	O
o	O
is	O
a	O
monotone	B
layer	I
consider	O
the	O
following	O
classification	O
rule	B
take	O
a	O
k	O
x	O
k	O
grid	O
in	O
and	O
minimize	O
the	O
empirical	B
error	I
over	O
all	O
classifiers	O
such	O
that	O
o	O
is	O
a	O
monotone	B
layer	I
and	O
it	O
is	O
a	O
union	O
of	O
cells	O
in	O
the	O
k	O
x	O
k	O
grid	O
what	O
is	O
the	O
optimal	O
choice	O
of	O
k	O
obtain	O
an	O
upper	O
bound	O
for	O
lgn	O
l	O
compare	O
your	O
result	O
with	O
that	O
obtained	O
for	O
empirical	B
error	I
minimization	O
over	O
the	O
class	O
of	O
all	O
monotone	O
layers	O
in	O
section	O
hint	O
count	O
the	O
number	O
of	O
different	O
classifiers	O
in	O
the	O
class	O
use	O
hoeffdings	O
inequality	B
and	O
the	O
union-of-events	O
bound	O
for	O
the	O
estimation	B
error	I
bound	O
the	O
approximation	B
error	I
using	O
the	O
bounded	O
density	O
assumption	O
problem	O
apply	O
problem	O
to	O
extend	O
the	O
consistency	B
result	O
in	O
theorem	O
as	O
follows	O
let	O
f	O
be	O
a	O
totally	O
bounded	O
class	O
of	O
functions	O
l	O
r	O
such	O
that	O
j	O
x	O
ljlx	O
for	O
each	O
ljl	O
e	O
f	O
is	O
the	O
lebesgue	O
measure	O
on	O
r	O
d	O
show	O
that	O
there	O
is	O
a	O
sequence	O
o	O
and	O
a	O
sequence	O
of	O
skeletons	O
fen	O
such	O
that	O
if	O
ln	O
is	O
the	O
skeleton	B
estimate	I
drawn	O
from	O
fell	O
then	O
lljj	O
ll	O
with	O
probability	O
one	O
whenever	O
x	O
has	O
a	O
density	O
in	O
particular	O
lljn	O
l	O
with	O
probability	O
one	O
if	O
the	O
bayes	O
rule	B
takes	O
the	O
for	O
some	O
l	O
e	O
f	O
note	O
the	O
true	O
regression	B
function	I
l	O
is	O
not	O
required	O
form	O
gx	O
to	O
be	O
in	O
f	O
uniform	B
laws	I
of	I
large	I
numbers	I
minimizing	O
the	O
empirical	B
squared	B
error	I
in	O
chapter	O
the	O
data	O
dn	O
were	O
used	O
to	O
select	O
a	O
function	O
from	O
a	O
class	O
f	O
of	O
candidate	O
regression	O
functions	O
nd	O
the	O
corresponding	O
classification	O
rule	B
gn	O
is	O
selecting	O
was	O
done	O
in	O
two	O
steps	O
a	O
skeleton-an	O
of	O
f	O
was	O
formed	O
and	O
the	O
empirical	B
error	I
count	O
was	O
minimized	O
over	O
the	O
skeleton	O
this	O
method	O
is	O
computationally	O
cumbersome	O
it	O
is	O
tempting	O
to	O
use	O
some	O
other	O
empirical	B
quantity	O
to	O
select	O
a	O
classifier	B
perhaps	O
the	O
most	O
popular	O
among	O
these	O
measures	O
is	O
the	O
empirical	B
squared	B
error	I
assume	O
now	O
that	O
the	O
function	O
is	O
selected	O
by	O
minimizing	O
the	O
empirical	B
squared	B
error	I
over	O
f	O
that	O
is	O
as	O
always	O
we	O
are	O
interested	O
in	O
the	O
error	O
probability	O
of	O
the	O
resulting	O
classifier	B
if	O
the	O
true	O
regression	B
function	I
p	O
y	O
x	O
x	O
is	O
not	O
in	O
the	O
class	O
f	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
empirical	B
squared	B
error	I
minimization	O
may	O
fail	O
miserably	O
problem	O
however	O
if	O
e	O
f	O
then	O
for	O
every	O
e	O
f	O
uniform	B
laws	I
of	I
large	I
numbers	I
we	O
have	O
lr	O
l	O
e	O
lx	O
corollary	O
e	O
e	O
ief	O
where	O
the	O
two	O
equalities	O
follow	O
from	O
the	O
fact	O
that	O
lx	O
eyix	O
thus	O
we	O
have	O
lln	O
l	O
e	O
inf	O
e	O
ief	O
by	O
an	O
argument	O
as	O
in	O
the	O
proof	O
of	O
lemma	O
thus	O
the	O
method	O
is	O
consistent	O
if	O
the	O
supremum	O
above	O
converges	O
to	O
zero	O
if	O
we	O
define	O
zi	O
yd	O
and	O
izd	O
then	O
we	O
see	O
that	O
we	O
need	O
only	O
to	O
bound	O
where	O
f	O
is	O
a	O
class	O
of	O
bounded	O
functions	O
in	O
the	O
next	O
four	O
sections	O
we	O
develop	O
upper	O
bounds	O
for	O
such	O
uniform	O
deviations	O
of	O
averages	O
from	O
their	O
expectations	O
then	O
we	O
apply	O
these	O
techniques	O
to	O
establish	O
consistency	B
of	O
generalized	B
linear	O
classifiers	O
obtained	O
by	O
minimization	O
of	O
the	O
empirical	B
squared	B
error	I
uniform	O
deviations	O
of	O
averages	O
from	O
expectations	O
let	O
f	O
be	O
a	O
class	O
of	O
real-valued	O
functions	O
defined	O
on	O
n	O
d	O
and	O
let	O
z	O
zn	O
be	O
i	O
i	O
d	O
nd-valued	O
random	O
variables	O
we	O
assume	O
that	O
for	O
each	O
i	O
e	O
f	O
ix	O
m	O
for	O
all	O
x	O
e	O
nd	O
and	O
some	O
m	O
by	O
hoeffdings	O
inequality	B
for	O
any	O
i	O
e	O
f	O
however	O
it	O
is	O
much	O
less	O
trivial	O
to	O
obtain	O
information	O
about	O
the	O
probabilities	O
p	O
i	O
t	O
izj	O
e	O
ief	O
n	O
il	O
uniform	O
deviations	O
of	O
averages	O
from	O
expectations	O
vapnik	O
and	O
chervonenkis	O
were	O
the	O
first	O
to	O
obtain	O
bounds	O
for	O
the	O
probability	O
above	O
for	O
example	O
the	O
following	O
simple	O
observation	O
makes	O
theorems	O
and	O
easy	O
to	O
apply	O
in	O
the	O
new	O
situation	O
lemma	O
sup	O
l	O
fzi	O
efz	O
m	O
i	O
n	O
fef	O
n	O
il	O
l	O
iuz	O
t	O
pfz	O
t	O
i	O
sup	O
n	O
fefto	O
n	O
il	O
proof	O
exploiting	O
the	O
identity	O
fooo	O
px	O
t	O
ex	O
for	O
nonnegative	O
random	O
variables	O
we	O
have	O
fef	O
n	O
il	O
sup	O
i	O
t	O
fzi	O
sup	O
i	O
t	O
iuz	O
t	O
pfz	O
t	O
dtl	O
fef	O
sup	O
i	O
t	O
iuz	O
t	O
pfz	O
tll	O
m	O
n	O
il	O
fefto	O
n	O
il	O
for	O
example	O
from	O
theorem	O
and	O
lemma	O
we	O
get	O
corollary	O
define	O
the	O
collection	O
of	O
sets	O
f	O
ft	O
f	O
e	O
f	O
t	O
e	O
m	O
where	O
for	O
every	O
f	O
e	O
f	O
and	O
t	O
e	O
m	O
the	O
set	O
a	O
ft	O
e	O
nd	O
is	O
defined	O
as	O
a	O
ft	O
fez	O
t	O
then	O
example	O
consider	O
the	O
empirical	B
squared	B
error	I
minimization	O
problem	O
sketched	O
in	O
the	O
previous	O
section	O
let	O
f	O
be	O
the	O
class	O
of	O
monotone	O
increasing	O
functions	O
r	O
and	O
let	O
be	O
the	O
function	O
selected	O
by	O
minimizing	O
the	O
empirical	B
squared	B
error	I
by	O
if	O
pry	O
llx	O
x	O
is	O
also	O
monotone	O
increasing	O
then	O
uniform	B
laws	I
of	I
large	I
numbers	I
if	O
t	O
contains	O
all	O
subsets	O
of	O
r	O
x	O
i	O
of	O
the	O
form	O
a	O
y	O
t	O
ry	O
e	O
f	O
t	O
e	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
its	O
n-th	O
shatter	B
coefficient	I
satisfies	O
set	O
n	O
thus	O
corollary	O
can	O
be	O
applied	O
and	O
the	O
empirical	B
squared	B
error	I
minimization	O
is	O
consistent	O
in	O
many	O
cases	O
corollary	O
does	O
not	O
provide	O
the	O
best	O
possible	O
bound	O
to	O
state	O
a	O
similar	O
but	O
sometimes	O
more	O
useful	O
result	O
we	O
introduce	O
ii-covering	O
numbers	O
the	O
notion	O
is	O
very	O
similar	O
to	O
that	O
of	O
covering	O
numbers	O
discussed	O
in	O
chapter	O
the	O
main	O
difference	O
is	O
that	O
here	O
the	O
balls	O
are	O
defined	O
in	O
terms	O
of	O
an	O
ii	O
rather	O
than	O
the	O
supremum	O
norm	O
definition	O
let	O
a	O
be	O
a	O
bounded	O
subset	O
of	O
rd	O
for	O
every	O
e	O
the	O
covering	B
number	I
denoted	O
by	O
ne	O
a	O
is	O
defined	O
as	O
the	O
cardinality	O
of	O
the	O
smallest	O
finite	O
set	O
in	O
rd	O
such	O
that	O
for	O
every	O
z	O
e	O
a	O
there	O
is	O
a	O
point	O
t	O
e	O
rd	O
in	O
the	O
finite	O
till	O
e	O
ll	O
denotes	O
the	O
ii-norm	O
of	O
the	O
set	O
such	O
that	O
vector	O
x	O
xed	O
in	O
rd	O
in	O
other	O
words	O
ne	O
a	O
is	O
the	O
smallest	O
number	O
of	O
ii-balls	O
of	O
radius	O
ed	O
whose	O
union	O
contains	O
a	O
logne	O
a	O
is	O
often	O
called	O
the	O
metric	B
entropy	B
of	O
a	O
we	O
will	O
mainly	O
be	O
interested	O
in	O
covering	O
numbers	O
of	O
special	O
sets	O
let	O
zl	O
zn	O
be	O
n	O
fixed	O
points	O
in	O
r	O
d	O
and	O
define	O
the	O
following	O
set	O
the	O
h	O
number	O
of	O
fzl	O
is	O
ne	O
fzl	O
if	O
z	O
zn	O
is	O
a	O
sequence	O
ofi	O
i	O
d	O
random	O
variables	O
thenne	O
fcz	O
is	O
a	O
random	O
variable	B
whose	O
expected	O
value	O
plays	O
a	O
central	O
role	O
in	O
our	O
problem	O
theorem	O
for	O
any	O
nand	O
e	O
the	O
proof	O
of	O
the	O
theorem	O
is	O
given	O
in	O
section	O
remark	O
theorem	O
is	O
a	O
generalization	O
of	O
the	O
basic	O
vapnik	O
equality	O
to	O
see	O
this	O
define	O
loo-covering	O
numbers	O
based	O
on	O
the	O
maximum	O
norm	O
and	O
chervonenkis	O
nooe	O
a	O
is	O
the	O
cardinality	O
of	O
the	O
smallest	O
finite	O
set	O
in	O
rd	O
such	O
that	O
for	O
every	O
z	O
e	O
a	O
there	O
is	O
a	O
point	O
t	O
e	O
rd	O
in	O
the	O
set	O
such	O
that	O
maxiid	O
izi	O
e	O
if	O
the	O
functions	O
in	O
f	O
are	O
indicators	O
of	O
sets	O
from	O
a	O
class	O
a	O
of	O
sbsets	O
of	O
r	O
d	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
for	O
every	O
e	O
e	O
empirical	B
squared	B
error	I
minimization	O
where	O
n	O
azl	O
zn	O
is	O
the	O
combinatorial	O
quantity	O
that	O
was	O
used	O
in	O
definition	B
of	I
shatter	O
coefficients	O
since	O
theorem	O
remains	O
true	O
with	O
loo-covering	O
numbers	O
therefore	O
it	O
is	O
a	O
ization	O
of	O
theorem	O
to	O
see	O
this	O
notice	O
that	O
if	O
f	O
contains	O
indicators	O
of	O
sets	O
of	O
the	O
class	O
a	O
then	O
sup	O
i	O
t	O
fzi	O
sup	O
i	O
t	O
iziea	O
pzi	O
e	O
ai	O
aea	O
n	O
il	O
ief	O
n	O
il	O
for	O
inequalities	O
sharper	O
and	O
more	O
general	O
than	O
theorem	O
we	O
refer	O
to	O
vapnik	O
pollard	O
haussler	O
and	O
anthony	O
and	O
shawe-taylor	O
empirical	B
squared	B
error	I
minimization	O
we	O
return	O
to	O
the	O
minimization	O
of	O
the	O
empirical	B
squared	B
error	I
let	O
f	O
be	O
a	O
class	O
of	O
functions	O
r	O
nd	O
containing	O
the	O
true	O
a	O
posteriori	O
function	O
the	O
empirical	B
squared	B
error	I
is	O
minimized	O
over	O
e	O
f	O
to	O
obtain	O
the	O
estimate	O
the	O
next	O
result	O
shows	O
that	O
empirical	B
squared	B
error	I
minimization	O
is	O
consistent	O
under	O
general	O
conditions	O
serve	O
that	O
these	O
are	O
the	O
same	O
conditions	O
that	O
we	O
assumed	O
in	O
theorem	O
to	O
prove	O
consistency	B
of	B
skeleton	I
estimates	I
theorem	O
assume	O
that	O
f	O
is	O
a	O
totally	O
bounded	O
class	O
of	O
functions	O
the	O
definition	O
see	O
chapter	O
if	O
e	O
f	O
then	O
the	O
classification	O
rule	B
obtained	O
by	O
minimizing	O
the	O
empirical	B
squared	B
error	I
over	O
f	O
is	O
strongly	O
consistent	O
that	O
is	O
lim	O
l	O
with	O
probability	O
one	O
n--oo	O
proof	O
recall	O
that	O
by	O
we	O
apply	O
theorem	O
to	O
show	O
that	O
for	O
every	O
e	O
the	O
probability	O
on	O
the	O
hand	O
side	O
converges	O
to	O
zero	O
exponentially	O
as	O
n	O
to	O
this	O
end	O
we	O
need	O
to	O
find	O
a	O
suitable	O
upper	O
bound	O
on	O
ene	O
where	O
j	O
is	O
the	O
class	O
of	O
functions	O
uniform	B
laws	I
of	I
large	I
numbers	I
ix	O
y	O
y	O
froeu	O
nd	O
x	O
i	O
to	O
where	O
e	O
f	O
and	O
zi	O
yi	O
observe	O
that	O
for	O
any	O
i	O
i	O
e	O
sup	O
x	O
this	O
inequality	B
implies	O
that	O
for	O
every	O
f	O
enf	O
where	O
is	O
the	O
covering	B
number	I
of	O
f	O
defined	O
in	O
chapter	O
by	O
the	O
assumption	O
of	O
total	B
boundedness	I
for	O
every	O
e	O
since	O
does	O
not	O
depend	O
on	O
n	O
the	O
theorem	O
is	O
proved	O
remark	O
the	O
nonasymptotic	O
exponential	B
nature	O
of	O
the	O
inequality	B
in	O
theorem	O
makes	O
it	O
possible	O
to	O
obtain	O
upper	O
bounds	O
for	O
the	O
rate	B
of	I
convergence	I
of	O
to	O
l	O
in	O
terms	O
of	O
the	O
covering	O
numbers	O
of	O
the	O
class	O
f	O
however	O
since	O
we	O
started	O
our	O
analysis	O
by	O
the	O
loose	O
inequality	B
l	O
the	O
resulting	O
rates	O
are	O
likely	O
to	O
be	O
suboptimal	O
theorem	O
also	O
the	O
inequality	B
of	O
theorem	O
may	O
be	O
loose	O
in	O
this	O
case	O
in	O
a	O
somewhat	O
different	O
setup	O
barron	O
developed	O
a	O
proof	O
method	O
based	O
on	O
bernsteins	O
inequality	B
that	O
is	O
useful	O
for	O
obtaining	O
tighter	O
upper	O
bounds	O
for	O
l	O
in	O
certain	O
cases	O
proof	O
of	O
theorem	O
the	O
main	O
tricks	O
in	O
the	O
proof	O
resemble	O
those	O
of	O
theorem	O
we	O
can	O
show	O
that	O
for	O
nf	O
p	O
j	O
t	O
fey	O
n	O
il	O
izj	O
eizij	O
f	O
j	O
taiizij	O
fey	O
n	O
il	O
where	O
an	O
are	O
i	O
i	O
d	O
l-valued	O
random	O
variables	O
independent	O
of	O
the	O
zis	O
with	O
pai	O
i	O
pai	O
the	O
only	O
minor	O
difference	O
with	O
theorem	O
appears	O
when	O
chebyshevs	O
inequality	B
is	O
applied	O
we	O
use	O
the	O
fact	O
that	O
by	O
boundedness	O
varizi	O
for	O
every	O
i	O
e	O
f	O
now	O
take	O
a	O
minimal	O
of	O
fz	O
that	O
is	O
m	O
fz	O
functions	O
gl	O
gm	O
such	O
that	O
for	O
every	O
i	O
e	O
f	O
there	O
is	O
a	O
g	O
e	O
gm	O
with	O
n	O
iizi	O
f	O
proof	O
of	O
theorem	O
for	O
any	O
function	O
f	O
we	O
have	O
and	O
thus	O
i	O
i	O
taigczih	O
tyfczi	O
gczi	O
n	O
i	O
l	O
n	O
l	O
oigzi	O
l	O
ifzi	O
gzil	O
n	O
il	O
n	O
il	O
s	O
fef	O
n	O
il	O
as	O
igzi	O
s	O
p	O
i	O
t	O
zl	O
zn	O
s	O
p	O
sup	O
i	O
t	O
t	O
s	O
p	O
i	O
toigjzji	O
zl	O
zn	O
fef	O
n	O
il	O
n	O
il	O
g	O
n	O
il	O
ifczj	O
zl	O
zn	O
now	O
that	O
we	O
have	O
been	O
able	O
to	O
convert	O
the	O
into	O
a	O
we	O
can	O
use	O
the	O
union	O
bound	O
we	O
need	O
only	O
find	O
a	O
uniform	O
bound	O
for	O
the	O
probability	O
following	O
the	O
this	O
however	O
is	O
easy	O
since	O
after	O
conditioning	O
on	O
zl	O
zn	O
oigzi	O
is	O
the	O
sum	O
of	O
independent	O
bounded	O
random	O
variables	O
whose	O
expected	O
value	O
is	O
zero	O
therefore	O
hoeffdings	O
inequality	B
gives	O
i	O
f	O
oigjzi	O
zl	O
zn	O
s	O
e	O
i	O
p	O
in	O
summary	O
uniform	B
laws	I
of	I
large	I
numbers	I
fzn	O
the	O
theorem	O
is	O
proved	O
properties	O
of	O
ne	O
will	O
be	O
studied	O
in	O
the	O
next	O
section	O
covering	O
numbers	O
and	O
shatter	O
coefficients	O
in	O
this	O
section	O
we	O
study	O
covering	O
numbers	O
and	O
relate	O
them	O
to	O
shatter	O
coefficients	O
of	O
certain	O
classes	O
of	O
sets	O
as	O
in	O
chapter	O
we	O
introduce	O
ii-packing	O
numbers	O
let	O
fbe	O
a	O
class	O
of	O
functions	O
on	O
n	O
d	O
taking	O
their	O
values	O
in	O
mj	O
let	O
be	O
an	O
arbitrary	O
probability	O
measure	O
on	O
nd	O
let	O
gl	O
gm	O
be	O
a	O
finite	O
collection	O
of	O
functions	O
from	O
f	O
with	O
the	O
property	O
that	O
for	O
any	O
two	O
of	O
them	O
the	O
largest	O
m	O
for	O
which	O
such	O
a	O
collection	O
exists	O
is	O
called	O
the	O
packing	B
number	I
of	O
f	O
to	O
and	O
is	O
denoted	O
by	O
me	O
f	O
if	O
places	O
probability	O
lin	O
on	O
each	O
of	O
zl	O
zn	O
then	O
by	O
definition	O
me	O
f	O
me	O
fzi	O
and	O
it	O
is	O
easy	O
to	O
see	O
that	O
fzid	O
ne	O
fz	O
me	O
an	O
important	O
feature	O
of	B
a	I
class	I
of	I
functions	I
f	O
is	O
the	O
vc	B
dimension	B
v	O
r	O
of	O
f	O
t	O
t	O
f	O
f	O
e	O
f	O
this	O
is	O
clarified	O
by	O
the	O
following	O
theorem	O
which	O
is	O
a	O
slight	O
refinement	O
of	O
a	O
result	O
by	O
pollard	O
which	O
is	O
based	O
on	O
dudleys	O
work	O
it	O
connects	O
the	O
packing	B
number	I
of	O
f	O
with	O
the	O
shatter	O
coefficients	O
of	O
f	O
see	O
also	O
haussler	O
for	O
a	O
somewhat	O
different	O
argument	O
theorem	O
let	O
f	O
be	O
a	O
class	O
of	O
m-valued	O
functions	O
on	O
nd	O
for	O
every	O
e	O
and	O
probability	O
measure	O
j	O
l	O
fl	O
me	O
f	O
s	O
k	O
where	O
k	O
fm	O
e	O
proof	O
let	O
be	O
an	O
arbitrary	O
e-packing	O
of	O
f	O
of	O
size	O
m	O
me	O
f	O
the	O
proof	O
is	O
in	O
the	O
spirit	O
of	O
the	O
probabilistic	B
method	I
of	O
combinatorics	O
e	O
g	O
spencer	O
to	O
prove	O
the	O
inequality	B
we	O
create	O
k	O
random	O
points	O
on	O
nd	O
x	O
m	O
in	O
the	O
following	O
way	O
where	O
k	O
is	O
a	O
positive	O
integer	O
specified	O
later	O
we	O
ate	O
k	O
independent	O
random	O
variables	O
sl	O
sk	O
on	O
nd	O
with	O
common	O
distribution	O
covering	O
numbers	O
and	O
shatter	O
coefficients	O
jk	O
and	O
independently	O
of	O
this	O
we	O
generate	O
another	O
k	O
independent	O
random	O
ables	O
tl	O
tb	O
uniformly	O
distributed	O
on	O
m	O
this	O
yields	O
k	O
random	O
pairs	O
tl	O
tk	O
for	O
any	O
two	O
functions	O
gi	O
and	O
gj	O
in	O
an	O
e-packing	O
the	O
bility	O
that	O
the	O
sets	O
gi	O
t	O
t	O
and	O
g	O
j	O
t	O
t	O
gjx	O
pick	O
the	O
same	O
points	O
from	O
our	O
random	O
set	O
of	O
k	O
points	O
is	O
bounded	O
as	O
follows	O
p	O
g	O
i	O
and	O
g	O
j	O
pick	O
the	O
same	O
points	O
k	O
ri	O
p	O
tr	O
e	O
gil	O
g	O
j	O
e	O
p	O
e	O
gil	O
g	O
j	O
i	O
s	O
d	O
gjsjlil	O
emi	O
e-ke	O
m	O
where	O
we	O
used	O
the	O
definition	B
of	I
the	O
functions	O
gl	O
gm	O
observe	O
that	O
the	O
expected	O
number	O
of	O
pairs	O
gj	O
of	O
these	O
functions	O
such	O
that	O
the	O
corresponding	O
sets	O
g	O
i	O
t	O
gixandg	O
j	O
t	O
gjx	O
pick	O
the	O
same	O
points	O
is	O
bounded	O
by	O
e	O
gj	O
g	O
i	O
and	O
g	O
j	O
pick	O
the	O
same	O
points	O
i	O
g	O
and	O
g	O
j	O
pick	O
the	O
same	O
point	O
since	O
for	O
k	O
randomly	O
chosen	O
points	O
the	O
average	O
number	O
of	O
pairs	O
that	O
pick	O
the	O
same	O
points	O
is	O
bounded	O
by	O
ge-ke	O
m	O
there	O
exist	O
k	O
points	O
in	O
rd	O
x	O
m	O
such	O
that	O
the	O
number	O
of	O
pairs	O
gj	O
that	O
pick	O
the	O
same	O
points	O
is	O
actually	O
bounded	O
by	O
ce-ke	O
m	O
for	O
each	O
such	O
pair	O
we	O
can	O
add	O
one	O
more	O
point	O
in	O
rd	O
x	O
m	O
such	O
that	O
the	O
point	O
is	O
contained	O
in	O
gil	O
g	O
j	O
thus	O
we	O
have	O
obtained	O
a	O
set	O
of	O
no	O
more	O
than	O
k	O
points	O
such	O
that	O
the	O
sets	O
g	O
i	O
g	O
m	O
pick	O
different	O
subsets	O
of	O
it	O
since	O
k	O
was	O
arbitrary	O
we	O
can	O
choose	O
it	O
to	O
minimize	O
this	O
expression	B
this	O
yields	O
l	O
log	O
g	O
m	O
j	O
points	O
so	O
the	O
shatter	B
coefficient	I
of	O
f	O
corresponding	O
to	O
this	O
number	O
must	O
be	O
greater	O
than	O
m	O
which	O
proves	O
the	O
statement	O
the	O
meaning	O
of	O
theorem	O
is	O
best	O
seen	O
from	O
the	O
following	O
simple	O
corollary	O
corollary	O
let	O
f	O
be	O
a	O
class	O
m-valuedunctions	O
on	O
rd	O
for	O
every	O
e	O
and	O
probability	O
measure	O
jk	O
me	O
f	O
uniform	B
laws	I
of	I
large	I
numbers	I
proof	O
recall	O
that	O
theorem	O
implies	O
the	O
inequality	B
follows	O
from	O
theorem	O
by	O
straightforward	O
calculation	O
the	O
details	O
are	O
left	O
as	O
an	O
exercise	O
d	O
recently	O
haussler	O
was	O
able	O
to	O
get	O
rid	O
of	O
the	O
factor	O
in	O
the	O
above	O
upper	O
bound	O
he	O
proved	O
that	O
if	O
e	O
kin	O
for	O
an	O
integer	O
k	O
then	O
me	O
f	O
ed	O
v	O
f	O
e	O
e	O
the	O
quantity	O
v	O
f	O
is	O
sometimes	O
called	O
the	O
pseudo	B
dimension	B
of	O
f	O
problem	O
it	O
follows	O
immediately	O
from	O
theorem	O
that	O
if	O
f	O
is	O
a	O
linear	O
space	O
of	O
functions	O
of	O
dimension	B
r	O
then	O
its	O
pseudo	B
dimension	B
is	O
at	O
most	O
r	O
a	O
few	O
more	O
properties	O
are	O
worth	O
mentioning	O
theorem	O
and	O
dudley	O
let	O
g	O
r	O
d	O
r	O
be	O
an	O
arbitrary	O
junction	O
and	O
consider	O
the	O
class	O
ojjunctions	O
j	O
j	O
e	O
f	O
then	O
vg	O
vf	O
proof	O
if	O
the	O
points	O
tl	O
tk	O
e	O
rd	O
x	O
r	O
are	O
shattered	O
by	O
f	O
then	O
the	O
points	O
tl	O
gsl	O
tk	O
gsk	O
are	O
shattered	O
by	O
g	O
this	O
proves	O
the	O
proof	O
of	O
the	O
other	O
inequality	B
is	O
similar	O
d	O
theorem	O
and	O
pollard	O
dudley	O
let	O
g	O
m	O
r	O
be	O
a	O
fixed	O
nondecreasing	O
junction	O
and	O
define	O
the	O
class	O
j	O
j	O
e	O
f	O
then	O
vg	O
vp	O
proof	O
assume	O
that	O
n	O
vg	O
and	O
let	O
the	O
functions	O
fr	O
e	O
f	O
be	O
such	O
that	O
the	O
binary	B
vector	O
takes	O
all	O
values	O
if	O
j	O
for	O
all	O
i	O
n	O
define	O
the	O
numbers	O
ctd	O
igjsn	O
and	O
covering	O
numbers	O
and	O
shatter	O
coefficients	O
by	O
the	O
monotonicity	O
of	O
g	O
ui	O
li	O
then	O
the	O
binary	B
vector	O
takes	O
the	O
same	O
value	O
as	O
for	O
every	O
j	O
therefore	O
the	O
pairs	O
sn	O
sl	O
are	O
shattered	O
by	O
f	O
which	O
proves	O
the	O
theorem	O
next	O
we	O
present	O
a	O
few	O
results	O
about	O
covering	O
numbers	O
of	O
classes	O
of	O
functions	O
whose	O
members	O
are	O
sums	O
or	O
products	O
of	O
functions	O
from	O
other	O
classes	O
similar	O
results	O
can	O
be	O
found	O
in	O
nobel	O
nolan	O
and	O
pollard	O
and	O
pollard	O
theorem	O
let	O
fl	O
fk	O
be	O
classes	O
of	O
real	O
functions	O
on	O
rd	O
for	O
n	O
arbitrary	O
fixed	O
points	O
z	O
zn	O
in	O
r	O
d	O
define	O
the	O
sets	O
fl	O
in	O
rn	O
by	O
j	O
k	O
also	O
introduce	O
fzn	O
f	O
e	O
f	O
for	O
the	O
class	O
offunctions	O
thenfor	O
every	O
e	O
and	O
ne	O
it	O
ne	O
k	O
k	O
jl	O
proof	O
let	O
sl	O
sk	O
c	O
rn	O
be	O
minimal	O
e	O
k-coverings	O
of	O
fl	O
fkzj	O
spectively	O
this	O
implies	O
that	O
for	O
any	O
e	O
fj	O
there	O
is	O
a	O
vector	O
s	O
j	O
sjn	O
e	O
sj	O
such	O
that	O
for	O
every	O
j	O
k	O
moreover	O
isj	O
i	O
ne	O
k	O
fjzj	O
we	O
show	O
that	O
ssl	O
esjji	O
uniform	B
laws	I
of	I
large	I
numbers	I
is	O
an	O
e-covering	O
of	O
this	O
follows	O
immediately	O
from	O
the	O
triangle	O
inequality	B
since	O
for	O
any	O
fl	O
there	O
is	O
si	O
sk	O
such	O
that	O
i	O
l	O
-	O
hzl	O
sl	O
l	O
-	O
sk	O
n	O
i	O
n	O
ci	O
i	O
ci	O
i	O
k-	O
e	O
k	O
theorem	O
letf	O
and	O
be	O
classes	O
of	O
real	O
functions	O
on	O
r	O
d	O
bounded	O
by	O
ml	O
and	O
m	O
respectively	O
is	O
e	O
g	O
s	O
ml	O
for	O
every	O
x	O
e	O
rd	O
and	O
f	O
e	O
forarbitraryjixedpoints	O
zn	O
in	O
rd	O
dejine	O
the	O
sets	O
and	O
gz	O
in	O
rh	O
as	O
in	O
theorem	O
introduce	O
for	O
the	O
class	O
of	O
functions	O
f	O
e	O
g	O
e	O
g	O
then	O
for	O
every	O
e	O
and	O
z	O
proof	O
let	O
s	O
c	O
md	O
n	O
be	O
an	O
of	O
that	O
is	O
for	O
any	O
f	O
e	O
there	O
is	O
a	O
vector	O
s	O
sen	O
e	O
s	O
such	O
that	O
n	O
i	O
i	O
l	O
n	O
ii	O
e	O
fzt	O
s	O
ci	O
i	O
it	O
is	O
easy	O
to	O
see	O
that	O
s	O
can	O
be	O
chosen	O
such	O
that	O
lsi	O
similarly	O
let	O
t	O
c	O
m	O
be	O
an	O
of	O
with	O
iti	O
gzd	O
such	O
that	O
for	O
any	O
g	O
e	O
there	O
is	O
at	O
ten	O
e	O
t	O
with	O
we	O
show	O
that	O
the	O
set	O
u	O
s	O
e	O
s	O
t	O
e	O
t	O
is	O
an	O
e-covering	O
of	O
let	O
f	O
e	O
and	O
g	O
e	O
be	O
arbitrary	O
and	O
s	O
e	O
sand	O
t	O
e	O
t	O
the	O
corresponding	O
vectors	O
such	O
that	O
then	O
generalized	B
linear	O
classification	O
generalized	B
linear	O
classification	O
in	O
this	O
section	O
we	O
use	O
the	O
uniform	B
laws	I
of	I
large	I
numbers	I
discussed	O
in	O
this	O
chapter	O
to	O
prove	O
that	O
squared	B
error	I
minimization	O
over	O
an	O
appropriately	O
chosen	O
class	O
of	O
generalized	B
linear	O
classifiers	O
yields	O
a	O
universally	O
consistent	O
rule	B
consider	O
the	O
class	O
ckn	O
of	O
generalized	B
linear	O
classifiers	O
whose	O
members	O
are	O
functions	O
nd	O
of	O
the	O
form	O
where	O
are	O
fixed	O
basis	O
functions	O
and	O
the	O
coefficients	O
ai	O
are	O
arbitrary	O
real	O
numbers	O
the	O
training	O
sequence	O
dn	O
is	O
used	O
to	O
determine	O
the	O
cients	O
ai	O
in	O
chapter	O
we	O
studied	O
the	O
behavior	O
of	O
the	O
classifier	B
whose	O
coefficients	O
are	O
picked	O
to	O
minimize	O
the	O
empirical	B
error	I
probability	O
instead	O
of	O
minimizing	O
the	O
empirical	B
error	I
probability	O
ln	O
several	O
authors	O
gested	O
minimizing	O
the	O
empirical	B
squared	B
error	I
e	O
g	O
duda	O
and	O
hart	O
vapnik	O
this	O
is	O
rather	O
dangerous	O
for	O
example	O
for	O
k	O
and	O
d	O
it	O
is	O
easy	O
to	O
find	O
a	O
distribution	O
such	O
that	O
the	O
error	O
ability	O
of	O
the	O
linear	B
classifier	B
that	O
minimizes	O
the	O
empirical	B
squared	B
error	I
converges	O
to	O
e	O
while	O
the	O
error	O
probability	O
of	O
the	O
best	O
linear	B
classifier	B
is	O
e	O
where	O
e	O
is	O
an	O
arbitrarily	O
small	O
positive	O
number	O
clearly	O
similar	O
examples	O
can	O
uniform	B
laws	I
of	I
large	I
numbers	I
be	O
found	O
for	O
any	O
fixed	O
k	O
this	O
demonstrates	O
powerfully	O
the	O
danger	O
of	O
minimizing	O
squared	B
error	I
instead	O
of	O
error	O
count	O
minimizing	O
the	O
latter	O
yields	O
a	O
classifier	B
whose	O
average	O
error	O
probability	O
is	O
always	O
within	O
jlog	O
n	O
n	O
of	O
the	O
optimum	O
in	O
the	O
class	O
for	O
fixed	O
k	O
we	O
note	O
here	O
that	O
in	O
some	O
special	O
cases	O
minimization	O
of	O
the	O
two	O
types	O
of	O
error	O
are	O
equivalent	O
problem	O
interestingly	O
though	O
if	O
kn	O
as	O
n	O
increases	O
we	O
can	O
obtain	O
universal	B
consistency	B
by	O
minimizing	O
the	O
empirical	B
squared	B
error	I
theorem	O
let	O
vrl	O
be	O
a	O
sequence	O
of	O
bounded	O
functions	O
with	O
i	O
such	O
that	O
the	O
set	O
of	O
all	O
finite	O
linear	O
combinations	O
of	O
the	O
vr	O
j	O
q	O
e	O
r	O
is	O
dense	O
in	O
l	O
on	O
all	O
balls	O
of	O
the	O
form	O
ilx	O
ii	O
m	O
for	O
any	O
probability	O
measure	O
let	O
the	O
coefficients	O
at	O
at	O
minimize	O
the	O
empirical	B
squared	B
error	I
under	O
the	O
constraint	O
ll	O
la	O
j	O
i	O
bit	O
bn	O
define	O
the	O
generalized	B
linear	O
sifier	O
gn	O
by	O
if	O
kn	O
and	O
bn	O
satisfy	O
then	O
e	O
lgn	O
l	O
for	O
all	O
distributions	O
of	O
y	O
that	O
is	O
the	O
rule	B
gn	O
is	O
universally	O
consistent	O
if	O
we	O
assume	O
additionally	O
that	O
b	O
log	O
n	O
on	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
proof	O
let	O
be	O
arbitrary	O
then	O
there	O
exists	O
a	O
constant	O
m	O
such	O
that	O
piixii	O
m	O
o	O
thus	O
lgn	O
l	O
pgnx	O
i	O
y	O
iixii	O
midn	O
pgx	O
i	O
y	O
iixii	O
m	O
it	O
suffices	O
to	O
show	O
that	O
pgnx	O
i	O
y	O
iixii	O
midn	O
pgx	O
i	O
y	O
iixii	O
m	O
in	O
the	O
required	O
sense	O
for	O
every	O
m	O
o	O
introduce	O
the	O
notation	O
fnx	O
ll	O
ajvrjx	O
by	O
corollary	O
we	O
see	O
that	O
pgnx	O
i	O
y	O
iixii	O
midn	O
pgx	O
i	O
y	O
iixii	O
m	O
generalized	B
linear	O
classification	O
we	O
prove	O
that	O
the	O
right-hand	O
side	O
converges	O
to	O
zero	O
in	O
probability	O
observe	O
that	O
since	O
llx	O
x	O
for	O
any	O
function	O
hex	O
ehx	O
x	O
x	O
chapter	O
therefore	O
denoting	O
the	O
class	O
of	O
functions	O
over	O
which	O
we	O
minimize	O
by	O
we	O
have	O
jiixiism	O
fj	O
dx	O
e	O
i	O
dn	O
e	O
i	O
dn	O
inf	O
e	O
fe	O
inf	O
e	O
fefn	O
ijlx	O
ii	O
sm	O
e	O
the	O
last	O
two	O
terms	O
may	O
be	O
combined	O
to	O
yield	O
inf	O
fefn	O
jiixiism	O
fj	O
dx	O
which	O
converges	O
to	O
zero	O
by	O
the	O
denseness	O
assumption	O
to	O
prove	O
that	O
the	O
first	O
term	O
converges	O
to	O
zero	O
in	O
probability	O
observe	O
that	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
p	O
ii	O
x	O
ii	O
m	O
o	O
as	O
in	O
the	O
proof	O
of	O
lemma	O
it	O
is	O
easy	O
to	O
show	O
that	O
e	O
i	O
dn	O
inf	O
e	O
fe	O
e	O
dn	O
inf	O
e	O
fe	O
n	O
e	O
i	O
sup	O
i	O
thxi	O
yi	O
ehx	O
yi	O
hej	O
n	O
il	O
uniform	B
laws	I
of	I
large	I
numbers	I
where	O
the	O
class	O
of	O
functions	O
j	O
is	O
defined	O
by	O
j	O
y	O
f	O
e	O
fn	O
observe	O
that	O
since	O
and	O
i	O
i	O
we	O
have	O
therefore	O
theorem	O
asserts	O
that	O
p	O
dn	O
inf	O
e	O
e	O
fefn	O
p	O
i	O
thxi	O
yi	O
ehx	O
hej	O
n	O
il	O
where	O
yn	O
next	O
for	O
fixed	O
we	O
estimate	O
the	O
ing	O
number	O
n	O
j	O
for	O
arbitrary	O
fl	O
e	O
consider	O
the	O
functions	O
hix	O
y	O
then	O
for	O
any	O
probability	O
measure	O
v	O
on	O
nd	O
x	O
i	O
and	O
h	O
y	O
f	O
ih	O
y	O
ylvdx	O
y	O
f	O
iflx	O
f	O
hxlbn	O
lvdx	O
y	O
f	O
i	O
fi	O
hex	O
vdx	O
y	O
where	O
fj	O
is	O
the	O
marginal	O
measure	O
for	O
v	O
on	O
nd	O
thus	O
for	O
any	O
yd	O
yn	O
and	O
e	O
ne	O
jzl	O
n	O
fx	O
therefore	O
it	O
suffices	O
to	O
estimate	O
the	O
covering	B
number	I
corresponding	O
to	O
fn	O
since	O
kn	O
fn	O
is	O
a	O
subset	O
of	O
a	O
linear	O
space	O
of	O
functions	O
we	O
have	O
corollary	O
n	O
e	O
ll	O
n	O
xl	O
b	O
e	O
n	O
b	O
e	O
n	O
og	O
e	O
problems	O
and	O
exercises	O
summarizing	O
we	O
have	O
p	O
dn	O
inf	O
e	O
e	O
which	O
goes	O
to	O
zero	O
if	O
knb	O
logbnn	O
the	O
proof	O
of	O
the	O
theorem	O
is	O
completed	O
it	O
is	O
easy	O
to	O
see	O
that	O
if	O
we	O
assume	O
additionally	O
that	O
b	O
log	O
n	O
n	O
then	O
strong	B
universal	B
consistency	B
follows	O
by	O
applying	O
the	O
borel-cantelli	B
lemma	I
to	O
the	O
last	O
probability	O
remark	O
minimization	O
of	O
the	O
squared	B
error	I
is	O
attractive	O
because	O
there	O
are	O
efficient	O
algorithms	O
to	O
find	O
the	O
minimizing	O
coefficients	O
while	O
minimizing	O
the	O
number	O
of	O
errors	O
committed	O
on	O
the	O
training	O
sequence	O
is	O
computationally	O
more	O
difficult	O
if	O
the	O
dimension	B
k	O
of	O
the	O
generalized	B
linear	B
classifier	B
is	O
fixed	O
then	O
stochastic	O
mation	O
asymptotically	O
provides	O
the	O
minimizing	O
coefficients	O
for	O
more	O
information	O
about	O
this	O
we	O
refer	O
to	O
robbins	O
and	O
monro	O
kiefer	O
and	O
wolfowitz	O
dvoretzky	O
fabian	O
tsypkin	O
nevelson	O
and	O
khasminskii	O
kushner	O
ruppert	O
and	O
ljung	O
pflug	O
and	O
walk	O
for	O
example	O
gyorfi	O
proved	O
that	O
if	O
vi	O
form	O
a	O
stationary	O
and	O
ergodic	O
sequence	O
in	O
which	O
each	O
pair	O
is	O
distributed	O
as	O
the	O
bounded	O
random	O
variable	B
pair	O
v	O
e	O
nk	O
x	O
n	O
and	O
the	O
vector	O
of	O
coefficients	O
a	O
ak	O
minimizes	O
and	O
ao	O
e	O
nk	O
is	O
arbitrary	O
then	O
the	O
sequence	O
of	O
coefficient	O
vectors	O
defined	O
by	O
an	O
u	O
n	O
v	O
u	O
satisfies	O
lim	O
ran	O
ra	O
a	O
s	O
n-oo	O
problems	O
and	O
exercises	O
problem	O
find	O
a	O
class	O
containing	O
two	O
functions	O
n	O
and	O
a	O
bution	O
of	O
y	O
such	O
that	O
l	O
but	O
as	O
n	O
the	O
probability	O
converges	O
to	O
one	O
where	O
is	O
selected	O
from	O
by	O
minimizing	O
the	O
empirical	B
squared	B
error	I
uniform	B
laws	I
of	I
large	I
numbers	I
problem	O
prove	O
corollary	O
problem	O
let	O
fbe	O
a	O
class	O
of	O
functions	O
on	O
n	O
d	O
taking	O
their	O
values	O
in	O
m	O
haussler	O
defines	O
the	O
pseudo	B
dimension	B
of	O
f	O
as	O
the	O
largest	O
integer	O
n	O
for	O
which	O
there	O
exist	O
n	O
points	O
in	O
n	O
d	O
zj	O
zn	O
and	O
a	O
vector	O
v	O
vn	O
e	O
nn	O
such	O
that	O
the	O
binary	B
n-vector	O
takes	O
possible	O
values	O
as	O
f	O
ranges	O
through	O
f	O
prove	O
that	O
the	O
pseudo	B
dimension	B
of	O
f	O
equals	O
the	O
quantity	O
v	O
f	O
defined	O
in	O
the	O
text	O
problem	O
consistency	B
of	O
clustering	B
let	O
x	O
xi	O
xn	O
be	O
i	O
i	O
d	O
random	O
variables	O
in	O
nd	O
and	O
assume	O
that	O
there	O
is	O
a	O
number	O
m	O
such	O
that	O
px	O
e	O
md	O
l	O
take	O
the	O
empirically	O
optimal	O
clustering	B
of	O
xl	O
x	O
n	O
that	O
is	O
find	O
the	O
points	O
ai	O
ak	O
that	O
minimize	O
the	O
empirical	B
squared	B
error	I
the	O
error	O
of	O
the	O
clustering	B
is	O
defined	O
by	O
the	O
mean	O
squared	B
error	I
prove	O
that	O
if	O
aj	O
ak	O
denote	O
the	O
empirically	O
optimal	O
cluster	O
centers	O
then	O
and	O
that	O
for	O
every	O
e	O
p	O
s	O
p	O
u	O
bj	O
erd	O
ie	O
ii	O
k	O
i	O
b	O
b	O
e	O
k	O
conclude	O
that	O
the	O
error	O
of	O
the	O
empirically	O
optimal	O
clustering	B
converges	O
to	O
that	O
of	O
the	O
truly	O
optimal	O
one	O
as	O
n	O
linder	O
lugosi	O
and	O
zeger	O
hint	O
for	O
the	O
first	O
part	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
lemma	O
for	O
the	O
second	O
part	O
use	O
the	O
technique	O
shown	O
in	O
corollary	O
to	O
compute	O
the	O
vc	B
dimension	B
exploit	O
corollary	O
problem	O
let	O
v	O
k	O
be	O
indicator	O
functions	O
of	O
cells	O
of	O
a	O
k-way	O
partition	B
of	O
nd	O
consider	O
generalized	B
linear	O
classifiers	O
based	O
on	O
these	O
functions	O
show	O
that	O
the	O
classifier	B
obtained	O
by	O
minimizing	O
the	O
number	O
of	O
errors	O
made	O
on	O
the	O
training	O
sequence	O
is	O
the	O
same	O
as	O
for	O
the	O
classifier	B
obtained	O
by	O
minimizing	O
the	O
empirical	B
squared	B
error	I
point	O
out	O
that	O
this	O
is	O
just	O
the	O
histogram	O
classifier	B
based	O
on	O
the	O
partition	B
defined	O
by	O
the	O
v	O
is	O
neural	O
networks	O
multilayer	B
perceptrons	O
the	O
linear	O
discriminant	O
or	O
perceptron	B
chapter	O
makes	O
a	O
decision	O
if	O
otherwlse	O
based	O
upon	O
a	O
linear	O
combination	O
of	O
the	O
inputs	O
co	O
l	O
cixi	O
co	O
ct	O
x	O
d	O
il	O
where	O
the	O
cis	O
are	O
weights	O
x	O
xdl	O
and	O
c	O
cdt	O
this	O
is	O
called	O
a	O
neural	B
network	I
without	O
hidden	O
layers	O
figure	O
in	O
a	O
neural	B
network	I
with	B
one	I
hidden	I
layer	I
one	O
takes	O
co	O
l	O
ci	O
cr	O
k	O
il	O
where	O
the	O
cs	O
are	O
as	O
before	O
and	O
each	O
is	O
of	O
the	O
form	O
given	O
in	O
hi	O
ll	O
aijxj	O
for	O
some	O
constants	O
hi	O
and	O
aij	O
the	O
function	O
cr	O
is	O
called	O
a	O
sigmoid	B
we	O
define	O
sigmoids	O
to	O
be	O
nondecreasing	O
functions	O
with	O
crx	O
as	O
x	O
and	O
crx	O
as	O
x	O
t	O
examples	O
include	O
the	O
threshold	B
sigmoid	B
crx	O
if	O
x	O
if	O
x	O
neural	O
networks	O
the	O
standard	B
or	O
logistic	O
sigmoid	B
o-x	O
e-	O
x	O
the	O
arctan	B
sigmoid	B
o-x	O
arctanx	O
tc	O
the	O
gaussian	B
sigmoid	B
o-x	O
e-	O
u	O
j	O
x	O
oot	O
figure	O
a	O
neural	B
network	I
with	B
one	I
hidden	I
layer	I
the	O
hidden	O
neurons	O
are	O
those	O
within	O
the	O
frame	O
ax	O
f	O
dv	O
i-e-x	O
ax	O
ax	O
arctanx	O
x	O
figure	O
the	O
threshold	B
standard	B
arctan	B
and	O
gaussian	B
moids	O
multilayer	B
perceptrons	O
for	O
early	O
discussion	O
of	O
multilayer	B
perceptrons	O
see	O
rosenblatt	O
barron	O
nilsson	O
and	O
minsky	O
and	O
papert	O
surveys	O
may	O
be	O
found	O
in	O
barron	O
and	O
barron	O
ripley	O
hertz	O
krogh	O
and	O
palmer	O
and	O
weiss	O
and	O
kulikowski	O
in	O
the	O
perceptron	B
with	B
one	I
hidden	I
layer	I
we	O
say	O
that	O
there	O
are	O
k	O
hidden	O
the	O
output	O
of	O
the	O
i-th	O
hidden	O
neuron	O
is	O
ui	O
cj	O
oix	O
thus	O
may	O
be	O
rewritten	O
as	O
co	O
l	O
ciui	O
k	O
il	O
which	O
is	O
similar	O
in	O
form	O
to	O
we	O
may	O
continue	O
this	O
process	O
and	O
create	O
layer	O
feed-forward	O
neural	O
networks	O
for	O
example	O
a	O
two-hidden-iayer	O
perceptron	B
uses	O
co	O
l	O
cizi	O
i	O
il	O
where	O
and	O
u	O
cj	O
a	O
xi	O
j	O
k	O
and	O
the	O
dijs	O
bs	O
and	O
ajs	O
are	O
constants	O
the	O
first	O
hidden	O
layer	O
has	O
k	O
hidden	O
neurons	O
while	O
the	O
second	O
hidden	O
layer	O
has	O
i	O
hidden	O
neurons	O
il	O
figure	O
afeed-forward	O
neural	B
network	I
with	B
two	I
hidden	I
layers	I
neural	O
networks	O
the	O
step	O
from	O
perceptron	B
to	O
a	O
one-hidden-iayer	O
neural	B
network	I
is	O
nontrivial	O
we	O
know	O
that	O
linear	O
discriminants	O
cannot	O
possibly	O
lead	O
to	O
universally	O
consistent	O
rules	O
fortunately	O
one-hidden-iayer	O
neural	O
networks	O
yield	O
universally	O
consistent	O
discriminants	O
provided	O
that	O
we	O
allow	O
k	O
the	O
number	O
of	O
hidden	O
neurons	O
to	O
grow	O
unboundedly	O
with	O
n	O
the	O
interest	O
in	O
neural	O
networks	O
is	O
undoubtedly	O
due	O
to	O
the	O
possibility	O
of	O
implementing	O
them	O
directly	O
via	O
processors	O
and	O
circuits	O
as	O
the	O
ware	O
is	O
fixed	O
beforehand	O
one	O
does	O
not	O
have	O
the	O
luxury	O
to	O
let	O
k	O
become	O
a	O
function	O
of	O
n	O
and	O
thus	O
the	O
claimed	O
universal	B
consistency	B
is	O
a	O
moot	O
point	O
we	O
will	O
deal	O
with	O
both	O
fixed	O
architectures	O
and	O
variable-sized	O
neural	O
networks	O
because	O
of	O
the	O
universal	B
consistency	B
of	O
one-hidden-iayer	O
neural	O
networks	O
there	O
is	O
little	O
ical	O
gain	O
in	O
considering	O
neural	O
networks	O
with	O
more	O
than	O
one	O
hidden	O
layer	O
there	O
may	O
however	O
be	O
an	O
information-theoretic	O
gain	O
as	O
the	O
number	O
of	O
hidden	O
neurons	O
needed	O
to	O
achieve	O
the	O
same	O
performance	O
may	O
be	O
substantially	O
reduced	O
in	O
fact	O
we	O
will	O
make	O
a	O
case	O
for	O
two	O
hidden	O
layers	O
and	O
show	O
that	O
after	O
two	O
hidden	O
layers	O
little	O
is	O
gained	O
for	O
classification	O
for	O
theoretical	B
analysis	O
the	O
neural	O
networks	O
are	O
rooted	O
in	O
a	O
classical	O
theorem	O
by	O
kolmogorov	O
and	O
lorentz	O
which	O
states	O
that	O
every	O
continuous	O
function	O
f	O
on	O
ld	O
can	O
be	O
written	O
as	O
where	O
the	O
g	O
ij	O
s	O
and	O
the	O
fi	O
s	O
are	O
continuous	O
functions	O
whose	O
form	O
depends	O
on	O
f	O
we	O
will	O
see	O
that	O
neural	O
networks	O
approximate	O
any	O
measurable	O
function	O
with	O
arbitrary	O
precision	O
despite	O
the	O
fact	O
that	O
the	O
form	O
of	O
the	O
sigmoids	O
is	O
fixed	O
beforehand	O
as	O
an	O
example	O
consider	O
d	O
the	O
function	O
is	O
rewritten	O
as	O
which	O
is	O
in	O
the	O
desired	O
form	O
however	O
it	O
is	O
much	O
less	O
obvious	O
how	O
one	O
would	O
rewrite	O
more	O
general	O
continuous	O
functions	O
in	O
fact	O
in	O
neural	O
networks	O
we	O
imate	O
the	O
gijs	O
and	O
fis	O
by	O
functions	O
of	O
the	O
form	O
aba	O
t	O
x	O
and	O
allow	O
the	O
number	O
of	O
tunable	O
coefficients	O
to	O
be	O
high	O
enough	O
such	O
that	O
any	O
continuous	O
function	O
may	O
be	O
represented-though	O
no	O
longer	O
rewritten	O
exactly	O
in	O
the	O
form	O
of	O
kolmogorov	O
and	O
lorentz	O
we	O
discuss	O
other	O
examples	O
of	O
approximations	O
based	O
upon	O
such	O
resentations	O
in	O
a	O
later	O
section	O
arrangements	O
input	O
figure	O
the	O
general	O
kolmogorov-lorentz	B
representation	I
of	O
a	O
continuous	O
function	O
arrangements	O
a	O
finite	O
set	O
a	O
of	O
hyperplanes	O
in	O
n	O
d	O
partitions	O
the	O
space	O
into	O
connected	O
convex	O
polyhedral	O
pieces	O
of	O
various	O
dimensions	O
such	O
a	O
partition	B
p	O
pea	O
is	O
called	O
an	O
arrangement	B
an	O
arrangement	B
is	O
called	O
simple	O
if	O
any	O
d	O
hyperplanes	O
of	O
a	O
have	O
a	O
unique	O
point	O
in	O
common	O
and	O
if	O
d	O
hyperplanes	O
have	O
no	O
point	O
in	O
common	O
figure	O
an	O
arrangement	B
of	O
five	O
lines	O
in	O
the	O
plane	O
figure	O
an	O
arrangement	B
sifier	O
a	O
simple	O
arrangement	B
creates	O
polyhedral	O
cells	O
interestingly	O
the	O
number	O
of	O
these	O
cells	O
is	O
independent	O
of	O
the	O
actual	O
configuration	O
of	O
the	O
hyperplanes	O
in	O
particular	O
neural	O
networks	O
the	O
number	O
of	O
cells	O
is	O
exactly	O
if	O
d	O
k	O
and	O
where	O
iai	O
k	O
for	O
a	O
proof	O
of	O
this	O
see	O
problem	O
or	O
lemma	O
of	O
edelsbrunner	O
for	O
general	O
arrangements	O
this	O
is	O
merely	O
an	O
upper	O
bound	O
we	O
may	O
of	O
course	O
use	O
arrangements	O
for	O
designing	O
classifiers	O
we	O
let	O
ga	O
be	O
the	O
natural	B
classifier	B
obtained	O
by	O
taking	O
majority	O
votes	O
over	O
all	O
yis	O
for	O
which	O
xi	O
is	O
in	O
the	O
same	O
cell	O
of	O
the	O
arrangement	B
p	O
pea	O
as	O
x	O
all	O
classifiers	O
discussed	O
in	O
this	O
section	O
possess	O
the	O
property	O
that	O
they	O
are	O
invariant	O
under	O
linear	O
transformations	O
and	O
universally	O
consistent	O
some	O
cases	O
we	O
assume	O
that	O
x	O
has	O
a	O
density	O
but	O
that	O
is	O
only	O
done	O
to	O
avoid	O
messy	O
technicalities	O
if	O
we	O
fix	O
k	O
and	O
find	O
that	O
a	O
with	O
iai	O
k	O
for	O
which	O
the	O
empirical	B
error	I
is	O
minimal	O
we	O
obtain-perhaps	O
at	O
great	O
computational	O
expense-the	O
empirical	B
risk	O
optimized	O
classifier	B
there	O
is	O
a	O
general	O
theorem	O
for	O
such	O
classifiers-see	O
for	O
example	O
corollary	O
conditions	O
of	O
which	O
are	O
as	O
follows	O
it	O
must	O
be	O
possible	O
to	O
select	O
a	O
given	O
sequence	O
of	O
as	O
for	O
which	O
lnga	O
conditional	O
probability	O
of	O
error	O
with	O
ga	O
tends	O
to	O
l	O
in	O
probability	O
but	O
if	O
k	O
we	O
may	O
align	O
the	O
hyperplanes	O
with	O
the	O
axes	O
and	O
create	O
a	O
cubic	B
histogram	O
for	O
which	O
by	O
theorem	O
we	O
have	O
consistency	B
if	O
the	O
grid	O
expands	O
to	O
and	O
the	O
cell	O
sizes	O
in	O
the	O
grid	O
shrink	O
to	O
o	O
thus	O
as	O
k	O
this	O
condition	O
holds	O
trivially	O
the	O
collection	O
is	O
not	O
too	O
rich	O
in	O
the	O
sense	O
that	O
n	O
where	O
n	O
denotes	O
the	O
shatter	B
coefficient	I
of	O
that	O
is	O
the	O
maximal	O
number	O
of	O
ways	O
yi	O
yn	O
can	O
be	O
split	O
by	O
sets	O
of	O
the	O
form	O
u	O
a	O
x	O
u	O
u	O
a	O
x	O
aepa	O
a	O
epa	O
if	O
iai	O
we	O
know	O
that	O
n	O
d	O
chapter	O
for	O
iai	O
k	O
a	O
trivial	O
upper	O
bound	O
is	O
d	O
the	O
consistency	B
condition	O
is	O
fulfilled	O
if	O
k	O
onj	O
log	O
n	O
we	O
have	O
theorem	O
the	O
empirical-risk-optimized	O
arrangement	B
classifier	B
based	O
upon	O
arrangements	O
with	O
iai	O
k	O
has	O
el	O
n	O
l	O
for	O
all	O
distributions	O
if	O
k	O
and	O
k	O
onj	O
log	O
n	O
anangements	O
arrangements	O
can	O
also	O
be	O
made	O
from	O
the	O
data	O
at	O
hand	O
in	O
a	O
simpler	O
way	O
fix	O
k	O
points	O
xl	O
x	O
k	O
in	O
general	O
position	O
and	O
look	O
at	O
all	O
possible	O
e	O
hyperplanes	O
you	O
can	O
form	O
with	O
these	O
points	O
these	O
form	O
your	O
collection	O
a	O
which	O
defines	O
your	O
arrangement	B
no	O
optimization	O
of	O
any	O
kind	O
is	O
performed	O
we	O
take	O
the	O
ural	O
classifier	B
obtained	O
by	O
a	O
majority	B
vote	I
within	O
the	O
cells	O
of	O
the	O
partition	B
over	O
ykd	O
yn	O
figure	O
arrangement	B
determined	O
by	O
k	O
data	O
points	O
on	O
the	O
plane	O
here	O
we	O
cannot	O
apply	O
the	O
powerful	O
consistency	B
theorem	O
mentioned	O
above	O
also	O
the	O
arrangement	B
is	O
no	O
longer	O
simple	O
nevertheless	O
the	O
partition	B
of	O
space	O
depends	O
on	O
the	O
xis	O
only	O
and	O
thus	O
theorem	O
with	O
lemma	O
is	O
useful	O
the	O
rule	B
thus	O
obtained	O
is	O
consistent	O
if	O
diamax	O
in	O
probability	O
and	O
the	O
number	O
of	O
cells	O
is	O
on	O
where	O
ax	O
is	O
the	O
cell	O
to	O
which	O
x	O
belongs	O
in	O
the	O
arrangement	B
as	O
the	O
number	O
of	O
cells	O
is	O
certainly	O
not	O
more	O
than	O
d	O
l	O
io	O
l	O
where	O
k	O
e	O
we	O
see	O
that	O
the	O
number	O
of	O
cells	O
divided	O
by	O
n	O
tends	O
to	O
zero	O
if	O
this	O
puts	O
a	O
severe	O
restriction	O
on	O
the	O
growth	O
of	O
k	O
however	O
it	O
is	O
easy	O
to	O
prove	O
the	O
following	O
lemma	O
if	O
k	O
then	O
diamax	O
in	O
probability	O
whenever	O
x	O
has	O
a	O
density	O
proof	O
as	O
noted	O
in	O
chapter	O
problem	O
the	O
set	O
of	O
all	O
x	O
for	O
which	O
for	O
all	O
e	O
we	O
have	O
jlx	O
e	O
qi	O
for	O
all	O
quadrants	O
ql	O
having	O
one	O
vertex	O
at	O
and	O
sides	O
of	O
length	O
one	O
has	O
jl-measure	O
one	O
for	O
such	O
x	O
if	O
at	O
least	O
one	O
of	O
the	O
xs	O
k	O
falls	O
in	O
each	O
of	O
the	O
quadrants	O
x	O
eqi	O
then	O
diamax	O
figure	O
neural	O
networks	O
figure	O
the	O
diameter	O
of	O
the	O
cell	O
taining	O
x	O
is	O
less	O
than	O
if	O
there	O
is	O
a	O
data	O
point	O
in	O
each	O
of	O
the	O
four	O
quadrants	O
of	O
size	O
e	O
around	O
x	O
therefore	O
for	O
arbitrary	O
e	O
pdiamax	O
min	O
mx	O
eqdk	O
o	O
thus	O
by	O
the	O
lebesgue	O
dominated	B
convergence	I
theorem	I
pdiamax	O
o	O
theorem	O
the	O
arrangement	B
classifier	B
defined	O
above	O
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
and	O
the	O
theorem	O
points	O
out	O
that	O
empirical	B
error	I
minimization	O
over	O
a	O
finite	O
set	O
of	O
arrangements	O
can	O
also	O
be	O
consistent	O
such	O
a	O
set	O
may	O
be	O
formed	O
as	O
the	O
collection	O
of	O
arrangements	O
consisting	O
of	O
hyperplanes	O
through	O
d	O
points	O
of	O
xl	O
x	O
k	O
as	O
nothing	O
new	O
is	O
added	O
here	O
to	O
the	O
discussion	O
we	O
refer	O
the	O
reader	O
to	O
problem	O
so	O
how	O
do	O
we	O
deal	O
with	O
arrangements	O
in	O
a	O
computer	O
clearly	O
to	O
reach	O
a	O
cell	O
we	O
find	O
for	O
each	O
hyperplane	O
a	O
e	O
a	O
the	O
side	O
to	O
which	O
x	O
belongs	O
if	O
fx	O
at	O
x	O
ao	O
then	O
fx	O
in	O
one	O
halfplane	O
fx	O
on	O
the	O
hyperplane	O
and	O
fx	O
in	O
the	O
other	O
halfplane	O
if	O
a	O
ad	O
the	O
vector	O
o	O
ihkx	O
o	O
describes	O
the	O
cell	O
to	O
which	O
x	O
belongs	O
where	O
hi	O
is	O
a	O
linear	O
function	O
that	O
is	O
positive	O
if	O
x	O
is	O
on	O
one	O
side	O
of	O
the	O
hyperplane	O
ai	O
negative	O
if	O
x	O
is	O
on	O
the	O
other	O
side	O
of	O
ai	O
and	O
if	O
x	O
e	O
ai	O
a	O
decision	O
is	O
thus	O
reached	O
in	O
time	O
okd	O
more	O
importantly	O
the	O
whole	O
process	O
is	O
easily	O
parallelizable	O
and	O
can	O
be	O
pictured	O
as	O
a	O
battery	O
of	O
perceptrons	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
classifier	B
depicted	O
in	O
figure	O
is	O
identical	O
to	O
the	O
arrangement	B
classifier	B
in	O
neural	B
network	I
terminology	O
the	O
first	O
hidden	O
layer	O
of	O
neurons	O
corresponds	O
to	O
just	O
k	O
perceptrons	O
has	O
k	O
d	O
weights	O
or	O
parameters	O
if	O
you	O
wish	O
the	O
first	O
layer	O
outputs	O
a	O
k-vector	O
of	O
bits	O
that	O
pinpoints	O
the	O
precise	O
location	O
of	O
x	O
in	O
the	O
cells	O
of	O
the	O
arrangement	B
the	O
second	O
layer	O
only	O
assigns	O
a	O
class	O
to	O
each	O
cell	O
of	O
the	O
arrangement	B
by	O
firing	O
up	O
one	O
neuron	O
it	O
has	O
neurons	O
class	O
assignments	O
but	O
of	O
course	O
in	O
natural	O
classifiers	O
these	O
neurons	O
do	O
not	O
require	O
training	O
or	O
learning-the	O
majority	B
vote	I
takes	O
care	O
of	O
that	O
arrangements	O
i-oorljl	O
x	O
o	O
or	O
oor	O
figure	O
arrangement	B
classifier	B
realized	O
by	O
a	O
layer	O
neural	B
network	I
each	O
of	O
the	O
cells	O
in	O
the	O
second	O
hidden	O
layer	O
peiforms	O
an	O
operation	O
the	O
output	O
of	O
node	O
is	O
ifits	O
three	O
inputs	O
are	O
and	O
respectively	O
otherwise	O
its	O
output	O
is	O
o	O
thus	O
one	O
and	O
only	O
one	O
of	O
the	O
outputs	O
is	O
if	O
a	O
more	O
classical	O
second	O
layer	O
is	O
needed-without	O
boolean	O
operations-let	O
b	O
bk	O
be	O
the	O
k-vector	O
of	O
bits	O
seen	O
at	O
the	O
output	O
of	O
the	O
first	O
layer	O
assign	O
a	O
perceptron	B
in	O
the	O
second	O
layer	O
to	O
each	O
region	O
of	O
the	O
arrangement	B
and	O
define	O
the	O
output	O
z	O
e	O
to	O
be	O
c	O
j	O
b	O
j	O
k	O
where	O
c	O
j	O
e	O
are	O
weights	O
for	O
each	O
region	O
of	O
the	O
arrangement	B
we	O
have	O
a	O
description	O
in	O
terms	O
of	O
c	O
ck	O
the	O
argument	O
of	O
the	O
sigmoid	B
function	O
is	O
if	O
j	O
c	O
j	O
for	O
all	O
j	O
and	O
is	O
negative	O
otherwise	O
hence	O
z	O
if	O
and	O
only	O
if	O
c	O
assume	O
we	O
now	O
take	O
a	O
decision	O
based	O
upon	O
the	O
sign	O
of	O
lwzzz	O
wo	O
z	O
where	O
the	O
wzs	O
are	O
weights	O
and	O
the	O
zz	O
are	O
the	O
outputs	O
of	O
the	O
second	O
hidden	O
layer	O
assume	O
that	O
we	O
wish	O
to	O
assign	O
class	O
to	O
s	O
regions	O
in	O
the	O
arrangement	B
and	O
class	O
to	O
t	O
other	O
regions	O
for	O
a	O
class	O
region	O
l	O
set	O
wz	O
and	O
for	O
a	O
class	O
region	O
set	O
wz	O
define	O
wo	O
s	O
t	O
then	O
if	O
zj	O
zi	O
i	O
j	O
l	O
wzzz	O
wo	O
w	O
j	O
wo	O
l	O
wi	O
if	O
wj	O
if	O
wj	O
z	O
ij	O
neural	O
networks	O
second	O
hidden	O
layer	O
oor	O
l	O
figure	O
the	O
second	O
hidden	O
layer	O
of	O
a	O
two-hidden-layer	O
neural	B
network	I
with	O
threshold	B
sigmoids	O
in	O
the	O
first	O
layer	O
for	O
each	O
k-vector	O
of	O
bits	O
b	O
bk	O
at	O
the	O
output	O
of	O
the	O
first	O
layer	O
we	O
may	O
find	O
a	O
decision	O
gb	O
e	O
i	O
now	O
return	O
to	O
the	O
hidden-layer	O
network	O
of	O
figure	O
and	O
assign	O
the	O
values	O
gb	O
to	O
the	O
neurons	O
in	O
the	O
second	O
hidden	O
layer	O
to	O
obtain	O
an	O
equivalent	O
network	O
thus	O
every	O
arrangement	B
classifier	B
corresponds	O
to	O
a	O
neural	B
network	I
with	B
two	I
hidden	I
layers	I
and	O
threshold	B
units	O
the	O
correspondence	O
is	O
also	O
reciprocal	O
assume	O
someone	O
shows	O
a	O
two-hidden-iayer	O
neural	B
network	I
with	O
the	O
first	O
hidden	O
layer	O
as	O
above-thus	O
outputs	O
consist	O
of	O
a	O
vector	O
of	O
k	O
bits-and	O
with	O
the	O
second	O
hidden	O
layer	O
consisting	O
once	O
again	O
a	O
battery	O
of	O
perceptrons	O
figure	O
whatever	O
happens	O
in	O
the	O
second	O
hidden	O
layer	O
the	O
decision	O
is	O
just	O
a	O
function	O
of	O
the	O
uration	O
of	O
k	O
input	O
bits	O
the	O
output	O
of	O
the	O
first	O
hidden	O
layer	O
is	O
constant	O
over	O
each	O
region	O
of	O
the	O
arrangement	B
defined	O
by	O
the	O
hyperplanes	O
given	O
by	O
the	O
input	O
weights	O
of	O
the	O
units	O
of	O
the	O
first	O
layer	O
thus	O
the	O
neural	B
network	I
classifier	B
with	O
threshold	B
units	O
in	O
the	O
first	O
hidden	O
layer	O
is	O
equivalent	O
to	O
an	O
arrangement	B
classifier	B
with	O
the	O
same	O
number	O
of	O
hyperplanes	O
as	O
units	O
in	O
the	O
first	O
hidden	O
layer	O
the	O
equivalence	O
with	O
tree	O
classifiers	O
is	O
described	O
in	O
problem	O
of	O
course	O
equivalence	O
is	O
only	O
valid	O
up	O
to	O
a	O
certain	O
point	O
if	O
the	O
number	O
of	O
neurons	O
in	O
the	O
second	O
layer	O
is	O
small	O
then	O
neural	O
networks	O
are	O
more	O
restricted	O
this	O
could	O
be	O
an	O
advantage	O
in	O
training	O
however	O
the	O
majority	B
vote	I
in	O
an	O
arrangement	B
classifier	B
avoids	O
training	O
of	O
the	O
second	O
layers	O
neurons	O
altogether	O
and	O
offers	O
at	O
the	O
same	O
time	O
an	O
easier	O
interpretation	O
of	O
the	O
classifier	B
conditions	O
on	O
consistency	B
of	O
general	O
two-hidden-layer	O
neural	O
networks	O
will	O
be	O
given	O
in	O
section	O
approximation	O
by	O
neural	O
networks	O
approximation	O
by	O
neural	O
networks	O
consider	O
first	O
the	O
class	O
ck	O
of	O
classifiers	O
that	O
contains	O
all	O
neural	B
network	I
classifiers	O
with	O
the	O
threshold	B
sigmoid	B
and	O
k	O
hidden	O
nodes	O
in	O
two	O
hidden	O
layers	O
the	O
training	O
data	O
dn	O
are	O
used	O
to	O
select	O
a	O
classifier	B
from	O
ck	O
for	O
good	O
performance	O
of	O
the	O
selected	O
rule	B
it	O
is	O
necessary	O
that	O
the	O
best	O
rule	B
in	O
ck	O
has	O
probability	O
of	O
error	O
close	O
to	O
l	O
that	O
is	O
that	O
inf	O
l	O
l	O
eck	O
is	O
small	O
we	O
call	O
this	O
quantity	O
the	O
approximation	B
error	I
naturally	O
for	O
fixed	O
k	O
the	O
approximation	B
error	I
is	O
positive	O
for	O
most	O
distributions	O
however	O
for	O
large	O
k	O
it	O
is	O
expected	O
to	O
be	O
small	O
the	O
question	O
is	O
whether	O
the	O
last	O
statement	O
is	O
true	O
for	O
all	O
distributions	O
of	O
y	O
we	O
showed	O
in	O
the	O
section	O
on	O
arrangements	O
that	O
the	O
class	O
of	O
two-hid	O
den-layer	O
neural	B
network	I
classifiers	O
with	O
m	O
nodes	O
in	O
the	O
first	O
layer	O
and	O
nodes	O
in	O
the	O
second	O
layer	O
contains	O
all	O
arrangement	B
classifiers	O
with	O
m	O
hyperplanes	O
therefore	O
for	O
k	O
m	O
the	O
class	O
of	O
all	O
arrangement	B
classifiers	O
with	O
m	O
hyperplanes	O
is	O
a	O
subclass	O
of	O
ck	O
from	O
this	O
we	O
easily	O
obtain	O
the	O
following	O
approximation	O
result	O
theorem	O
ijck	O
is	O
the	O
class	O
of	O
all	O
neural	B
network	I
classifiers	O
with	O
the	O
old	O
sigmoid	B
and	O
k	O
neurons	O
in	O
two	O
hidden	O
layers	O
then	O
inf	O
l	O
l	O
lim	O
k-oo	O
eck	O
for	O
all	O
distributions	O
of	O
y	O
it	O
is	O
more	O
surprising	O
that	O
the	O
same	O
property	O
holds	O
if	O
ck	O
is	O
the	O
class	O
of	O
hidden-layer	O
neural	O
networks	O
with	O
k	O
hidden	O
neurons	O
and	O
an	O
arbitrary	O
sigmoid	B
more	O
precisely	O
ck	O
is	O
the	O
class	O
of	O
classifiers	O
if	O
otherwise	O
where	O
is	O
as	O
in	O
by	O
theorem	O
we	O
have	O
l	O
l	O
where	O
yjx	O
py	O
iix	O
x	O
thus	O
inf	O
eck	O
l	O
l	O
as	O
k	O
if	O
for	O
some	O
sequence	O
with	O
k	O
e	O
ck	O
for	O
kx	O
h	O
ikx	O
for	O
universal	B
consistency	B
we	O
need	O
only	O
assure	O
that	O
the	O
family	B
of	I
can	O
approximate	O
any	O
in	O
the	O
lim	O
sense	O
in	O
other	O
words	O
the	O
approximation	B
error	I
infcjeck	O
l	O
l	O
converges	O
to	O
zero	O
if	O
the	O
class	O
of	O
functions	O
is	O
dense	O
in	O
li	O
for	O
every	O
m	O
another	O
sufficient	O
condition	O
for	O
this-but	O
of	O
course	O
much	O
too	O
severe-is	O
that	O
the	O
class	O
neural	O
networks	O
of	O
functions	O
becomes	O
dense	O
in	O
the	O
loo	O
norm	O
in	O
the	O
space	O
of	O
continuous	O
functions	O
ca	O
bd	O
on	O
bd	O
where	O
bd	O
denotes	O
the	O
hyperrectangle	O
of	O
rd	O
defined	O
by	O
its	O
opposite	O
vertices	O
a	O
and	O
b	O
for	O
any	O
a	O
and	O
b	O
lemma	O
assume	O
that	O
a	O
sequence	O
of	O
classes	O
of	O
functions	O
becomes	O
dense	O
in	O
the	O
loo	O
norm	O
in	O
the	O
space	O
of	O
continuous	O
functions	O
ca	O
bd	O
bd	O
is	O
the	O
hyperrectangle	O
ofrd	O
defined	O
by	O
a	O
b	O
in	O
other	O
words	O
assume	O
that	O
for	O
every	O
a	O
b	O
e	O
r	O
d	O
and	O
every	O
bounded	O
function	O
g	O
lim	O
inf	O
k-oo	O
jefk	O
xeabd	O
sup	O
ifx	O
then	O
for	O
any	O
distribution	O
of	O
y	O
lim	O
k-oo	O
rjjeck	O
inf	O
l	O
l	O
where	O
ck	O
is	O
the	O
class	O
of	O
classifiers	O
for	O
e	O
proof	O
for	O
fixed	O
e	O
find	O
a	O
b	O
such	O
that	O
fla	O
bd	O
where	O
fl	O
is	O
the	O
probability	O
measure	O
of	O
x	O
choose	O
a	O
continuous	O
function	O
off	O
bd	O
such	O
that	O
elryx	O
find	O
k	O
and	O
f	O
e	O
such	O
that	O
e	O
sup	O
xeabjd	O
ix	O
i	O
e	O
for	O
iux	O
we	O
have	O
by	O
theorem	O
l	O
e	O
ryxiixeabdd	O
ryxi	O
sup	O
ifx	O
xeabd	O
e	O
o	O
e	O
ryx	O
i	O
this	O
text	O
is	O
basically	O
about	O
all	O
such	O
good	O
families	O
such	O
as	O
families	O
that	O
are	O
tainable	O
by	O
summing	O
kernel	B
functions	O
and	O
histogram	O
families	O
the	O
first	O
results	O
for	O
approximation	O
with	O
neural	O
networks	O
with	B
one	I
hidden	I
layer	I
appeared	O
in	O
when	O
cybenko	O
hornik	O
stinchcombe	O
and	O
white	O
and	O
funahashi	O
proved	O
independently	O
that	O
feedforward	O
neural	O
networks	O
with	B
one	I
hidden	I
layer	I
are	O
dense	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
on	O
bounded	O
sets	O
in	O
the	O
set	O
of	O
continuous	O
functions	O
in	O
other	O
words	O
by	O
taking	O
k	O
large	O
enough	O
every	O
continuous	O
function	O
approximation	O
by	O
neural	O
networks	O
on	O
nd	O
can	O
be	O
approximated	O
arbitrarily	O
closely	O
uniformly	O
over	O
any	O
bounded	O
set	O
by	O
functions	O
realized	O
by	O
neural	O
networks	O
with	B
one	I
hidden	I
layer	I
for	O
a	O
survey	O
of	O
various	O
denseness	O
results	O
we	O
refer	O
to	O
barron	O
and	O
hornik	O
the	O
proof	O
given	O
here	O
uses	O
ideas	O
of	O
chen	O
chen	O
and	O
liu	O
it	O
uses	O
the	O
denseness	O
of	O
the	O
class	O
of	O
trigonometric	B
polynomials	O
in	O
the	O
loo	O
sense	O
for	O
co	O
ld	O
a	O
special	O
case	O
of	O
the	O
stone-weierstrass	B
theorem	I
see	O
theorem	O
in	O
the	O
appendix	O
that	O
is	O
by	O
functions	O
of	O
the	O
form	O
l	O
at	O
x	O
bt	O
x	O
i	O
where	O
ai	O
bi	O
are	O
integer-valued	O
vectors	O
of	O
rd	O
theorem	O
for	O
every	O
continuous	O
function	O
f	O
bd	O
nand	O
for	O
every	O
e	O
there	O
exists	O
a	O
neural	B
network	I
with	B
one	I
hidden	I
layer	I
and	O
function	O
tx	O
as	O
in	O
such	O
that	O
sup	O
ifx	O
e	O
xeabd	O
proof	O
we	O
prove	O
the	O
theorem	O
for	O
the	O
threshold	B
sigmoid	B
ax	O
if	O
x	O
if	O
x	O
o	O
the	O
extension	O
to	O
general	O
non	O
decreasing	O
sigmoids	O
is	O
left	O
as	O
an	O
exercise	O
lem	O
fix	O
e	O
o	O
we	O
take	O
the	O
fourier	O
series	O
approximation	O
of	O
fx	O
by	O
the	O
stone-weierstrass	B
theorem	I
there	O
exists	O
a	O
large	O
positive	O
ger	O
m	O
nonzero	O
real	O
coefficients	O
al	O
am	O
fh	O
and	O
integers	O
mi	O
for	O
i	O
m	O
j	O
d	O
such	O
that	O
sup	O
it	O
i	O
cos	O
x	O
sin	O
x	O
xeabd	O
il	O
a	O
a	O
where	O
mi	O
i	O
m	O
it	O
is	O
clear	O
that	O
every	O
continuous	O
tion	O
on	O
the	O
real	O
line	O
that	O
is	O
zero	O
outside	O
some	O
bounded	O
interval	O
can	O
be	O
arbitrarily	O
closely	O
approximated	O
uniformly	O
on	O
the	O
interval	O
by	O
one-dimensional	O
neural	O
works	O
that	O
is	O
by	O
functions	O
of	O
the	O
form	O
k	O
l	O
ciaai	O
x	O
hi	O
co	O
il	O
just	O
observe	O
that	O
the	O
indicator	O
function	O
of	O
an	O
interval	O
c	O
may	O
be	O
written	O
as	O
ax	O
b	O
a	O
c	O
this	O
implies	O
that	O
bounded	O
functions	O
such	O
as	O
sin	O
and	O
cos	O
can	O
be	O
approximated	O
arbitrarily	O
closely	O
by	O
neural	O
networks	O
in	O
particular	O
there	O
exist	O
neural	O
networks	O
uicx	O
vix	O
with	O
i	O
m	O
mappings	O
from	O
nd	O
to	O
n	O
such	O
that	O
sup	O
xeabd	O
juix	O
cos	O
x	O
j	O
e	O
a	O
i	O
i	O
a	O
neural	O
networks	O
and	O
sup	O
xeabjd	O
sin	O
x	O
e	O
i	O
a	O
therefore	O
applying	O
the	O
triangle	O
inequality	B
we	O
get	O
since	O
the	O
uis	O
and	O
vis	O
are	O
neural	O
networks	O
their	O
linear	O
combination	O
m	O
l	O
il	O
is	O
a	O
neural	B
network	I
too	O
and	O
in	O
fact	O
sup	O
ifx	O
xeabd	O
sup	O
xeabd	O
ifx	O
t	O
i	O
cos	O
x	O
sin	O
x	O
i	O
sup	O
it	O
cos	O
x	O
sin	O
x	O
xeabd	O
il	O
il	O
a	O
a	O
a	O
a	O
the	O
convergence	O
may	O
be	O
arbitrarily	O
slow	O
for	O
some	O
f	O
by	O
restricting	O
the	O
class	O
of	O
functions	O
it	O
is	O
possible	O
to	O
obtain	O
upper	O
bounds	O
for	O
the	O
rate	B
of	I
convergence	I
for	O
an	O
example	O
see	O
barron	O
the	O
following	O
corollary	O
of	O
theorem	O
is	O
obtained	O
via	O
lemma	O
corollary	O
let	O
ck	O
contain	O
all	O
neural	B
network	I
classifiers	O
defined	O
by	O
works	O
of	O
one	O
hidden	O
layer	O
with	O
k	O
hidden	O
nodes	O
and	O
an	O
arbitrary	O
sigmoid	B
then	O
for	O
any	O
distribution	O
of	O
y	O
lim	O
k-oo	O
cpeck	O
inf	O
l	O
l	O
o	O
the	O
above	O
convergence	O
also	O
holds	O
if	O
the	O
range	O
of	O
the	O
parameters	O
aij	O
bi	O
ci	O
is	O
restricted	O
to	O
an	O
interval	O
where	O
limk-oobk	O
remark	O
it	O
is	O
also	O
true	O
that	O
the	O
class	O
of	O
one-hidden-iayer	O
neural	O
networks	O
with	O
k	O
hidden	O
neurons	O
becomes	O
dense	O
in	O
l	O
for	O
every	O
probability	O
measure	O
j-l	O
on	O
nd	O
as	O
k	O
problem	O
then	O
theorem	O
may	O
be	O
used	O
directly	O
to	O
prove	O
corollary	O
vc	B
dimension	B
in	O
practice	O
the	O
network	O
architecture	O
k	O
in	O
our	O
case	O
is	O
given	O
to	O
the	O
designer	O
who	O
can	O
only	O
adjust	O
the	O
parameters	O
aij	O
bi	O
and	O
ci	O
depending	O
on	O
the	O
data	O
dn	O
in	O
this	O
respect	O
the	O
above	O
results	O
are	O
only	O
of	O
theoretical	B
interest	O
it	O
is	O
more	O
interesting	O
to	O
find	O
out	O
how	O
far	O
the	O
error	O
probability	O
of	O
the	O
chosen	O
rule	B
is	O
from	O
inf	O
ceck	O
l	O
we	O
discuss	O
this	O
problem	O
in	O
the	O
next	O
few	O
sections	O
vc	B
dimension	B
assume	O
now	O
that	O
the	O
data	O
dn	O
yd	O
yn	O
are	O
used	O
to	O
tune	O
the	O
parameters	O
of	O
the	O
network	O
to	O
choose	O
a	O
classifier	B
from	O
ck	O
we	O
focus	O
on	O
the	O
ference	O
between	O
the	O
probability	O
of	O
error	O
of	O
the	O
selected	O
rule	B
and	O
that	O
of	O
the	O
best	O
classifier	B
in	O
ck	O
recall	O
from	O
chapters	O
and	O
that	O
the	O
vc	B
dimension	B
vck	O
of	O
the	O
class	O
ck	O
determines	O
the	O
performance	O
of	O
some	O
learning	B
algorithms	O
theorem	O
tells	O
us	O
that	O
no	O
method	O
of	O
picking	O
a	O
classifier	B
from	O
ck	O
can	O
guarantee	O
better	O
than	O
q	O
n	O
performance	O
uniformly	O
for	O
all	O
distributions	O
thus	O
for	O
meaningful	O
distribution-free	O
performance	O
guarantees	O
the	O
sample	O
size	O
n	O
has	O
to	O
be	O
significantly	O
larger	O
than	O
the	O
vc	B
dimension	B
on	O
the	O
other	O
hand	O
by	O
corollary	O
there	O
exists	O
a	O
way	O
of	O
choosing	O
the	O
parameters	O
of	O
the	O
network-namely	O
by	O
minimization	O
of	O
the	O
empirical	B
error	I
probability-such	O
that	O
the	O
obtained	O
classifier	B
satisfies	O
e	O
inf	O
lp	B
ceck	O
vck	O
log	O
n	O
for	O
all	O
distributions	O
on	O
the	O
other	O
hand	O
if	O
vck	O
then	O
for	O
any	O
n	O
and	O
any	O
rule	B
some	O
bad	O
distributions	O
exist	O
that	O
induce	O
very	O
large	O
error	O
probabilities	O
theorem	O
we	O
start	O
with	O
a	O
universal	O
lower	O
bound	O
on	O
the	O
vc	B
dimension	B
of	O
networks	O
with	B
one	I
hidden	I
layer	I
theorem	O
let	O
a	O
be	O
an	O
arbitrary	O
sigmoid	B
and	O
consider	O
the	O
class	O
ck	O
of	O
neural	O
net	O
classifiers	O
with	O
k	O
nodes	O
in	O
one	O
hidden	O
layer	O
then	O
proof	O
we	O
prove	O
the	O
statement	O
for	O
the	O
threshold	B
sigmoid	B
and	O
leave	O
the	O
extension	O
as	O
an	O
exercise	O
we	O
need	O
to	O
show	O
that	O
there	O
is	O
a	O
set	O
of	O
n	O
points	O
in	O
rd	O
that	O
can	O
be	O
shattered	O
by	O
sets	O
of	O
the	O
form	O
ljfx	O
where	O
ljf	O
is	O
a	O
one-layer	O
neural	B
network	I
of	O
k	O
hidden	O
nodes	O
clearly	O
it	O
suffices	O
to	O
prove	O
this	O
for	O
even	O
k	O
in	O
fact	O
we	O
prove	O
more	O
if	O
k	O
is	O
even	O
any	O
set	O
of	O
n	O
kd	O
points	O
in	O
general	O
position	O
can	O
be	O
shattered	O
are	O
in	O
general	O
position	O
if	O
no	O
d	O
points	O
fall	O
on	O
i-dimensional	O
hyperplane	O
let	O
xn	O
be	O
a	O
set	O
of	O
n	O
kd	O
such	O
the	O
same	O
d	O
points	O
for	O
each	O
subset	O
of	O
this	O
set	O
we	O
construct	O
a	O
neural	B
network	I
ljf	O
with	O
k	O
hidden	O
nodes	O
such	O
that	O
if	O
and	O
only	O
if	O
xi	O
is	O
a	O
member	O
of	O
this	O
subset	O
we	O
may	O
neural	O
networks	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
cardinality	O
of	O
the	O
subset	O
to	O
be	O
picked	O
out	O
is	O
at	O
most	O
since	O
otherwise	O
we	O
can	O
use	O
vrx	O
where	O
vr	O
picks	O
out	O
the	O
complement	O
of	O
the	O
subset	O
partition	B
the	O
subset	O
into	O
at	O
most	O
n	O
k	O
groups	O
each	O
containing	O
at	O
most	O
d	O
points	O
for	O
each	O
such	O
group	O
there	O
exists	O
a	O
hyperplane	O
a	O
t	O
x	O
b	O
that	O
contains	O
these	O
points	O
but	O
no	O
other	O
point	O
from	O
xn	O
moreover	O
there	O
exists	O
a	O
small	O
positive	O
number	O
h	O
such	O
that	O
at	O
xi	O
b	O
e	O
h	O
if	O
and	O
only	O
if	O
xi	O
is	O
among	O
this	O
group	O
of	O
at	O
most	O
d	O
points	O
therefore	O
the	O
simple	O
network	O
aat	O
x	O
b	O
h	O
a-a	O
t	O
x	O
b	O
h	O
is	O
larger	O
than	O
on	O
xi	O
for	O
exactly	O
these	O
xis	O
denote	O
the	O
vectors	O
a	O
and	O
parameters	O
b	O
h	O
obtained	O
for	O
the	O
groups	O
by	O
ai	O
bt	O
and	O
hi	O
let	O
h	O
min	O
j	O
h	O
j	O
it	O
is	O
easy	O
to	O
see	O
that	O
vrx	O
l	O
b	O
j	O
jl	O
is	O
larger	O
than	O
for	O
exactly	O
the	O
desired	O
xis	O
this	O
network	O
has	O
k	O
hidden	O
nodes	O
theorem	O
implies	O
that	O
there	O
is	O
no	O
hope	O
for	O
good	O
performance	O
guarantees	O
unless	O
the	O
sample	O
size	O
is	O
much	O
larger	O
than	O
kd	O
recall	O
chapter	O
where	O
we	O
showed	O
that	O
n	O
vck	O
is	O
necessary	O
for	O
a	O
guaranteed	O
small	O
eltor	O
probability	O
regardless	O
of	O
the	O
method	O
of	O
tuning	O
the	O
parameters	O
bartlett	O
improved	O
theorem	O
in	O
several	O
ways	O
for	O
example	O
he	O
proved	O
that	O
vck	O
d	O
min	O
d	O
d	O
bartlett	O
also	O
obtained	O
similar	O
lower	B
bounds	I
for	I
not	O
fully	O
connected	O
networks-see	O
problem	O
for	O
two-hidden-layer	O
networks	O
next	O
we	O
show	O
that	O
for	O
the	O
threshold	B
sigmoid	B
the	O
bound	O
of	O
theorem	O
is	O
tight	O
up	O
to	O
a	O
logarithmic	O
factor	O
that	O
is	O
the	O
vc	B
dimension	B
is	O
at	O
most	O
of	O
the	O
order	O
of	O
kdlogk	O
theorem	O
and	O
haussler	O
let	O
be	O
the	O
threshold	B
sigmoid	B
and	O
let	O
ck	O
be	O
the	O
class	O
of	O
neural	O
net	O
classifiers	O
with	O
k	O
nodes	O
in	O
the	O
hidden	O
layer	O
then	O
the	O
shatter	O
coefficients	O
satisfy	O
ne	O
kl	O
dl	O
which	O
implies	O
that	O
for	O
all	O
k	O
d	O
vck	O
vc	B
dimension	B
proof	O
fix	O
n	O
points	O
xl	O
e	O
nd	O
we	O
bound	O
the	O
number	O
of	O
different	O
values	O
of	O
the	O
vector	O
as	O
ranges	O
through	O
ck	O
a	O
node	O
j	O
in	O
the	O
hidden	O
layer	O
realizes	O
a	O
dichotomy	O
of	O
the	O
n	O
points	O
by	O
a	O
hyperplane	O
split	O
by	O
theorems	O
and	O
this	O
can	O
be	O
done	O
at	O
mostefj	O
different	O
ways	O
the	O
different	O
splittings	O
obtained	O
at	O
the	O
k	O
nodes	O
determine	O
the	O
k-dimensional	O
input	O
of	O
the	O
output	O
node	O
different	O
choices	O
of	O
the	O
parameters	O
co	O
ci	O
ck	O
of	O
the	O
output	O
node	O
determine	O
different	O
k-dimensionallinear	O
splits	O
of	O
the	O
n	O
input	O
vectors	O
this	O
cannot	O
be	O
done	O
in	O
more	O
than	O
lj	O
different	O
ways	O
for	O
a	O
fixed	O
setting	O
of	O
the	O
au	O
and	O
bi	O
parameters	O
this	O
altogether	O
yields	O
at	O
most	O
different	O
dichotomies	O
of	O
the	O
n	O
points	O
x	O
i	O
x	O
n	O
as	O
desired	O
the	O
bound	O
on	O
the	O
vc	B
dimension	B
follows	O
from	O
the	O
fact	O
that	O
vck	O
n	O
if	O
sck	O
n	O
for	O
threshold	B
sigmoids	O
the	O
gap	O
between	O
the	O
lower	O
and	O
upper	O
bounds	O
above	O
is	O
logarithmic	O
in	O
kd	O
notice	O
that	O
the	O
vc	B
dimension	B
is	O
about	O
the	O
number	O
of	O
weights	O
tunable	O
parameters	O
w	O
kd	O
of	O
the	O
network	O
surprisingly	O
maass	O
proved	O
that	O
for	O
networks	O
with	O
at	O
least	O
two	O
hidden	O
layers	O
the	O
upper	O
bound	O
has	O
the	O
right	O
order	O
of	O
magnitude	O
that	O
is	O
the	O
vc	B
dimension	B
is	O
q	O
w	O
log	O
w	O
a	O
simple	O
application	O
of	O
theorems	O
and	O
provides	O
the	O
next	O
consistency	B
result	O
that	O
was	O
pointed	O
out	O
in	O
farago	O
and	O
lugosi	O
theorem	O
let	O
be	O
the	O
threshold	B
sigmoid	B
let	O
gn	O
be	O
a	O
classifier	B
from	O
ck	O
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
e	O
ck	O
if	O
k	O
such	O
that	O
k	O
log	O
n	O
n	O
as	O
n	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
that	O
is	O
with	O
probability	O
one	O
for	O
all	O
distributions	O
of	O
y	O
lim	O
lgn	O
l	O
n-oo	O
proof	O
by	O
the	O
usual	O
decomposition	O
into	O
approximation	O
and	O
estimation	B
errors	O
lgn	O
l	O
inf	O
l	O
l	O
l	O
fjjeck	O
fjjeck	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
tends	O
to	O
zero	O
by	O
corollary	O
for	O
the	O
estimation	B
error	I
by	O
theorems	O
and	O
p	O
inf	O
l	O
e	O
fjjeck	O
neural	O
networks	O
which	O
is	O
summable	O
if	O
k	O
onj	O
log	O
n	O
the	O
theorem	O
assures	O
us	O
that	O
if	O
a	O
is	O
the	O
threshold	B
sigmoid	B
then	O
a	O
sequence	O
of	O
properly	O
sized	O
networks	O
may	O
be	O
trained	O
to	O
asymptotically	O
achieve	O
the	O
optimum	O
probability	O
of	O
error	O
regardless	O
of	O
what	O
the	O
distribution	O
of	O
y	O
is	O
for	O
example	O
k	O
fn	O
will	O
do	O
the	O
job	O
however	O
this	O
is	O
clearly	O
not	O
the	O
optimal	O
choice	O
in	O
the	O
majority	O
of	O
cases	O
since	O
theorem	O
provides	O
suitable	O
upper	O
bounds	O
on	O
the	O
vc	B
dimension	B
of	O
each	O
class	O
ck	O
one	O
may	O
use	O
complexity	B
regularization	I
as	O
described	O
in	O
chapter	O
to	O
find	O
a	O
near-optimum	O
size	O
network	O
unfortunately	O
the	O
situation	O
is	O
much	O
less	O
clear	O
for	O
more	O
general	O
continuous	O
sigmoids	O
the	O
vc	B
dimension	B
then	O
depends	O
on	O
the	O
specific	O
sigmoid	B
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
the	O
vc	B
dimension	B
of	O
ck	O
with	O
an	O
arbitrary	O
nondecreasing	O
sigmoid	B
is	O
always	O
larger	O
than	O
or	O
equal	O
to	O
that	O
with	O
the	O
threshold	B
sigmoid	B
typically	O
the	O
vc	B
dimension	B
of	O
a	O
class	O
of	O
such	O
networks	O
is	O
significantly	O
larger	O
than	O
that	O
for	O
the	O
threshold	B
sigmoid	B
in	O
fact	O
it	O
can	O
even	O
be	O
infinite	O
macintyre	O
and	O
sontag	O
demonstrated	O
the	O
existence	O
of	O
continuous	O
infinitely	O
many	O
times	O
differentiable	O
monotone	O
increasing	O
sigmoids	O
such	O
that	O
the	O
vc	B
dimension	B
of	O
ck	O
is	O
infinite	O
if	O
k	O
their	O
sigmoids	O
have	O
little	O
squiggles	O
creating	O
the	O
large	O
variability	O
it	O
is	O
even	O
more	O
surprising	O
that	O
infinite	O
vc	B
dimension	B
may	O
occur	O
for	O
even	O
smoother	O
sigmoids	O
whose	O
second	O
derivative	O
is	O
negative	O
for	O
x	O
and	O
positive	O
for	O
x	O
o	O
in	O
chapter	O
problem	O
we	O
basically	O
proved	O
the	O
following	O
result	O
the	O
details	O
are	O
left	O
to	O
the	O
reader	O
theorem	O
there	O
exists	O
a	O
sigmoid	B
a	O
that	O
is	O
monotone	O
increasing	O
continuous	O
concave	O
on	O
and	O
convex	O
on	O
such	O
that	O
vck	O
for	O
each	O
k	O
we	O
recall	O
once	O
again	O
that	O
infinite	O
vc	B
dimension	B
implies	O
that	O
there	O
is	O
no	O
hope	O
of	O
obtaining	O
nontrivial	O
distribution-free	O
upper	O
bounds	O
on	O
no	O
matter	O
how	O
the	O
training	O
sequence	O
dn	O
is	O
used	O
to	O
select	O
the	O
parameters	O
of	O
the	O
neural	B
network	I
however	O
as	O
we	O
will	O
see	O
later	O
it	O
is	O
still	O
possible	O
to	O
obtain	O
universal	B
consistency	B
finiteness	O
of	O
the	O
vc	B
dimension	B
has	O
been	O
proved	O
for	O
many	O
types	O
of	O
sigmoids	O
maass	O
and	O
goldberg	O
and	O
jerrum	O
obtain	O
upper	O
bounds	O
for	O
piecewise	B
polynomial	B
sigmoids	O
the	O
results	O
of	O
goldberg	O
and	O
jerrum	O
apply	O
for	O
general	O
classes	O
parametrized	O
by	O
real	O
numbers	O
e	O
g	O
for	O
classes	O
of	O
neural	O
networks	O
with	O
the	O
sigmoid	B
ax	O
if	O
x	O
if	O
x	O
o	O
macintyre	O
and	O
sontag	O
prove	O
vck	O
for	O
a	O
large	O
class	O
of	O
sigmoids	O
which	O
includes	O
the	O
standard	B
arctan	B
and	O
gaussian	B
sigmoids	O
while	O
finiteness	O
is	O
useful	O
the	O
vc	B
dimension	B
lack	O
of	O
an	O
explicit	O
tight	O
upper	O
bound	O
on	O
vck	O
prevents	O
us	O
from	O
getting	O
meaningful	O
upper	O
bounds	O
on	O
the	O
performance	O
of	O
gn	O
and	O
also	O
from	O
applying	O
the	O
structural	O
risk	O
minimization	O
of	O
chapter	O
for	O
the	O
standard	B
sigmoid	B
and	O
for	O
networks	O
with	O
k	O
hidden	O
nodes	O
and	O
w	O
tunable	O
weights	O
karpinski	O
and	O
macintyre	O
recently	O
reported	O
the	O
upper	O
bound	O
v	O
kwkw	O
wl	O
see	O
also	O
shawe-taylor	O
unfortunately	O
the	O
consistency	B
result	O
of	O
theorem	O
is	O
only	O
of	O
theoretical	B
interest	O
as	O
there	O
is	O
no	O
efficient	O
algorithm	B
to	O
find	O
a	O
classifier	B
that	O
minimizes	O
the	O
empirical	B
error	I
probability	O
relatively	O
little	O
effort	O
has	O
been	O
made	O
to	O
solve	O
this	O
important	O
problem	O
farago	O
and	O
lugosi	O
exhibit	O
an	O
algorithm	B
that	O
finds	O
the	O
empirically	O
optimal	O
network	O
however	O
their	O
method	O
takes	O
time	O
exponential	B
in	O
kd	O
which	O
is	O
intractable	O
even	O
for	O
the	O
smallest	O
toy	O
problems	O
much	O
more	O
effort	O
has	O
been	O
invested	O
in	O
the	O
tuning	O
of	O
networks	O
by	O
minimizing	O
the	O
empirical	B
squared	B
error	I
or	O
the	O
empirical	B
l	O
error	O
these	O
problems	O
are	O
also	O
computationally	O
demanding	O
but	O
numerous	O
suboptimal	O
hill-climbing	O
algorithms	O
have	O
been	O
used	O
with	O
some	O
success	O
most	O
famous	O
among	O
these	O
is	O
the	O
back	B
propagation	I
algorithm	B
of	O
rumelhart	O
hinton	O
and	O
williams	O
nearly	O
all	O
known	O
algorithms	O
that	O
run	O
in	O
reasonable	O
time	O
may	O
get	O
stuck	O
at	O
local	O
optima	O
which	O
results	O
in	O
classifiers	O
whose	O
probability	O
of	O
error	O
is	O
hard	O
to	O
predict	O
in	O
the	O
next	O
section	O
we	O
study	O
the	O
error	O
probability	O
of	O
neural	O
net	O
classifiers	O
obtained	O
by	O
minimizing	O
empirical	B
l	O
p	O
errors	O
we	O
end	O
this	O
section	O
with	O
a	O
very	O
simple	O
kind	O
of	O
one-layer	O
network	O
the	O
committee	B
machine	I
e	O
g	O
nilsson	O
and	O
schmidt	O
is	O
a	O
special	O
case	O
of	O
a	O
hidden-layer	O
neural	B
network	I
of	O
the	O
form	O
with	O
co	O
cl	O
ck	O
and	O
the	O
threshold	B
sigmoid	B
o	O
or	O
figure	O
the	O
committee	B
machine	I
has	O
fixed	O
weights	O
at	O
the	O
output	O
of	O
the	O
hidden	O
layer	O
committee	O
machines	O
thus	O
use	O
a	O
majority	B
vote	I
over	O
the	O
outcomes	O
of	O
the	O
hidden	O
neurons	O
it	O
is	O
interesting	O
that	O
the	O
lower	O
bound	O
of	O
theorem	O
remains	O
valid	O
when	O
ck	O
is	O
the	O
class	O
of	O
all	O
committee	O
machines	O
with	O
k	O
neurons	O
in	O
the	O
hidden	O
neural	O
layer	O
it	O
is	O
less	O
obvious	O
however	O
that	O
the	O
class	O
of	O
committee	O
machines	O
is	O
large	O
enough	O
for	O
the	O
asymptotic	O
property	O
lim	O
k-hx	O
inf	O
lrp	O
l	O
for	O
all	O
distributions	O
of	O
y	O
problem	O
figure	O
a	O
partition	B
of	O
the	O
plane	O
determined	O
by	O
a	O
committee	B
machine	I
with	O
hidden	O
neurons	O
the	O
total	O
vote	O
is	O
shown	O
in	O
each	O
region	O
the	O
region	O
in	O
which	O
we	O
decide	O
is	O
shaded	O
ll	O
error	O
minimization	O
in	O
the	O
previous	O
section	O
we	O
obtained	O
consistency	B
for	O
the	O
standard	B
threshold	B
sigmoid	B
networks	O
by	O
empirical	B
risk	I
minimization	I
we	O
could	O
not	O
apply	O
the	O
same	O
ogy	O
for	O
general	O
sigmoids	O
simply	O
because	O
the	O
vc	B
dimension	B
for	O
general	O
sigmoidal	O
networks	O
is	O
not	O
bounded	O
it	O
is	O
bounded	O
for	O
certain	O
classes	O
of	O
sigmoids	O
and	O
for	O
those	O
empirical	B
risk	I
minimization	I
yields	O
universally	O
consistent	O
classifiers	O
even	O
if	O
the	O
vc	B
dimension	B
is	O
infinite	O
we	O
may	O
get	O
consistency	B
but	O
this	O
must	O
then	O
be	O
proved	O
by	O
other	O
methods	O
such	O
as	O
methods	O
based	O
upon	O
metric	B
entropy	B
and	O
covering	O
bers	O
chapters	O
and	O
as	O
well	O
as	O
the	O
survey	O
by	O
haussler	O
one	O
could	O
also	O
train	O
the	O
classifier	B
by	O
minimizing	O
another	O
empirical	B
criterion	O
which	O
is	O
exactly	O
what	O
we	O
will	O
do	O
in	O
this	O
section	O
we	O
will	O
be	O
rewarded	O
with	O
a	O
general	O
consistency	B
theorem	O
for	O
all	O
sigmoids	O
for	O
p	O
the	O
empirical	B
l	B
p	I
error	I
of	O
a	O
neural	B
network	I
lf	O
is	O
defined	O
by	O
the	O
most	O
interesting	O
cases	O
are	O
p	O
and	O
p	O
for	O
p	O
this	O
is	O
just	O
the	O
empirical	B
squared	B
error	I
while	O
p	O
yields	O
the	O
empirical	B
absolute	B
error	I
often	O
it	O
makes	O
sense	O
to	O
attempt	O
to	O
choose	O
the	O
parameters	O
of	O
the	O
network	O
lf	O
such	O
that	O
jp	O
lf	O
is	O
ll	O
error	O
minimization	O
minimized	O
in	O
situations	O
where	O
one	O
is	O
not	O
only	O
interested	O
in	O
the	O
number	O
of	O
errors	O
but	O
also	O
how	O
robust	O
the	O
decision	O
is	O
such	O
error	O
measures	O
may	O
be	O
meaningful	O
in	O
other	O
words	O
these	O
error	O
measures	O
penalize	O
even	O
good	O
decisions	O
if	O
is	O
close	O
to	O
the	O
threshold	B
value	O
o	O
minimizing	O
jpis	O
like	O
finding	O
a	O
good	O
regression	B
function	I
mate	O
our	O
concern	O
is	O
primarily	O
with	O
the	O
error	O
probability	O
in	O
chapter	O
we	O
already	O
highlighted	O
the	O
dangers	O
of	B
squared	B
error	I
minimization	I
and	O
l	O
p	O
errors	O
in	O
general	O
here	O
we	O
will	O
concentrate	O
on	O
the	O
consistency	B
properties	O
we	O
minimize	O
the	O
empirical	B
error	I
over	O
a	O
class	O
of	O
functions	O
which	O
should	O
not	O
be	O
too	O
large	O
to	O
avoid	O
overfitting	B
however	O
the	O
class	O
should	O
be	O
large	O
enough	O
to	O
contain	O
a	O
good	O
approximation	O
of	O
the	O
target	O
function	O
thus	O
we	O
let	O
the	O
class	O
of	O
candidate	O
functions	O
grow	O
with	O
the	O
sample	O
size	O
n	O
as	O
in	O
grenanders	O
of	O
sieves	O
its	O
consistency	B
and	O
rates	O
of	O
convergence	O
have	O
been	O
widely	O
studied	O
primarily	O
for	O
least	O
squares	O
regression	B
function	I
estimation	B
and	O
nonparametric	O
maximum	B
likelihood	I
density	O
estimation-see	O
geman	O
and	O
hwang	O
gallant	O
and	O
wong	O
and	O
shen	O
remark	O
regression	B
function	I
estimation	B
in	O
the	O
regression	B
function	I
tion	O
setup	O
white	O
proved	O
consistency	B
of	O
neural	B
network	I
estimates	O
based	O
on	O
squared	B
error	I
minimization	O
barron	O
used	O
a	O
complexity-regularized	O
modification	O
of	O
these	O
error	O
measures	O
to	O
obtain	O
the	O
fastest	O
possible	O
rate	O
of	O
vergence	O
for	O
nonparametric	O
neural	B
network	I
estimates	O
haussler	O
provides	O
a	O
general	O
framework	O
for	O
empirical	B
error	I
minimization	O
and	O
provides	O
useful	O
tools	O
for	O
handling	O
neural	O
networks	O
various	O
consistency	B
properties	O
of	O
nonparametric	O
ral	O
network	O
estimates	O
have	O
been	O
proved	O
by	O
white	O
mielniczuk	O
and	O
tyrcha	O
and	O
lugosi	O
and	O
zeger	O
we	O
only	O
consider	O
the	O
p	O
case	O
as	O
the	O
generalization	O
to	O
other	O
values	O
of	O
p	O
is	O
straightforward	O
define	O
the	O
ll	O
error	O
of	O
a	O
function	O
rd	O
r	O
by	O
jljf	O
eiljfx	O
yi	O
we	O
pointed	O
out	O
in	O
problem	O
that	O
one	O
of	O
the	O
functions	O
minimizing	O
j	O
is	O
the	O
bayes	O
rule	B
g	O
whose	O
error	O
is	O
denoted	O
by	O
then	O
clearly	O
j	O
l	O
we	O
have	O
also	O
seen	O
that	O
if	O
we	O
define	O
a	O
decision	O
by	O
j	O
inf	O
jljf	O
jg	O
gx	O
if	O
otherwise	O
then	O
its	O
error	O
probability	O
lg	O
pgx	O
y	O
satisfies	O
the	O
inequality	B
lg	O
l	O
jljf	O
j	O
our	O
approach	O
is	O
to	O
select	O
a	O
neural	B
network	I
from	O
a	O
suitably	O
chosen	O
class	O
of	O
networks	O
by	O
minimizing	O
the	O
empirical	B
error	I
neural	O
networks	O
denoting	O
this	O
function	O
by	O
on	O
according	O
to	O
the	O
inequality	B
above	O
the	O
classifier	B
gnx	O
if	O
onx	O
h	O
ot	O
erwlse	O
is	O
consistent	O
if	O
the	O
lienor	O
converges	O
to	O
j	O
in	O
probability	O
convergence	O
with	O
probability	O
one	O
provides	O
strong	B
consistency	B
for	O
universal	O
convergence	O
the	O
class	O
over	O
which	O
the	O
minimization	O
is	O
performed	O
has	O
to	O
be	O
defined	O
carefully	O
the	O
following	O
theorem	O
shows	O
that	O
this	O
may	O
be	O
achieved	O
by	O
neural	O
networks	O
with	O
k	O
nodes	O
in	O
which	O
the	O
range	O
of	O
the	O
output	O
weights	O
co	O
ci	O
ck	O
is	O
restricted	O
theorem	O
and	O
zeger	O
let	O
be	O
an	O
arbitrary	O
sigmoid	B
define	O
the	O
class	O
of	O
neural	O
networks	O
by	O
and	O
let	O
on	O
be	O
afunction	O
that	O
minimizes	O
the	O
empirical	B
ll	O
error	O
over	O
e	O
if	O
kn	O
and	O
satisfy	O
lim	O
k	O
n	O
n---oo	O
lim	O
and	O
n---oo	O
lull	O
n---oo	O
logkn	O
n	O
then	O
the	O
classification	O
rule	B
gn	O
if	O
onx	O
otherwise	O
is	O
universally	O
consistent	O
remark	O
strong	B
universal	B
consistency	B
may	O
also	O
be	O
shown	O
by	O
imposing	O
slightly	O
more	O
restrictive	O
conditions	O
on	O
k	O
n	O
and	O
problem	O
proof	O
by	O
the	O
argument	O
preceding	O
the	O
theorem	O
it	O
suffices	O
to	O
prove	O
that	O
jtln	O
j	O
in	O
probability	O
write	O
jljin	O
j	O
inf	O
jlji	O
jo	O
j	O
lj	O
error	O
minimization	O
to	O
handle	O
the	O
approximation	O
error-the	O
second	O
term	O
on	O
the	O
right-hand	O
side-let	O
t	O
e	O
be	O
a	O
function	O
such	O
that	O
eltx	O
eltx	O
for	O
each	O
t	O
e	O
the	O
existence	O
of	O
such	O
a	O
function	O
may	O
be	O
seen	O
by	O
noting	O
that	O
eltx	O
is	O
a	O
continuous	O
function	O
of	O
the	O
parameters	O
ai	O
hi	O
ci	O
of	O
the	O
neural	B
network	I
t	O
clearly	O
inf	O
jt	O
j	O
jt	O
j	O
eltx	O
yi	O
elgx	O
yi	O
eltx	O
gxi	O
which	O
converges	O
to	O
zero	O
as	O
n	O
by	O
problem	O
we	O
start	O
the	O
analysis	O
of	O
the	O
estimation	B
error	I
by	O
noting	O
that	O
as	O
in	O
lemma	O
we	O
have	O
jtn	O
inf	O
jt	O
sup	O
ijt	O
sup	O
eltx	O
yi	O
l	O
l	O
itxd	O
yil	O
i	O
in	O
n	O
il	O
define	O
the	O
class	O
mn	O
of	O
functions	O
on	O
nd	O
x	O
i	O
by	O
mil	O
y	O
itcigarx	O
yl	O
ai	O
e	O
rd	O
hi	O
e	O
r	O
fjn	O
then	O
the	O
previous	O
bound	O
becomes	O
sup	O
emx	O
y	O
mem	O
l	O
i	O
in	O
lmxi	O
yi	O
n	O
il	O
such	O
quantities	O
may	O
be	O
handled	O
by	O
the	O
uniform	O
law	O
of	O
large	O
numbers	O
of	O
theorem	O
which	O
applies	O
to	O
classes	O
of	O
uniformly	O
bounded	O
functions	O
indeed	O
for	O
each	O
m	O
emn	O
lcd	O
neural	O
networks	O
if	O
n	O
is	O
so	O
large	O
that	O
for	O
such	O
ns	O
theorem	O
states	O
that	O
p	O
sup	O
iemx	O
y	O
yii	O
e	O
memn	O
where	O
ne	O
mndn	O
denotes	O
the	O
h	O
number	O
of	O
the	O
random	O
set	O
mndn	O
defined	O
in	O
chapter	O
all	O
we	O
need	O
now	O
is	O
to	O
estimate	O
these	O
covering	O
numbers	O
observe	O
that	O
for	O
e	O
mn	O
with	O
y	O
yl	O
and	O
y	O
yl	O
for	O
any	O
probability	O
measure	O
v	O
on	O
n	O
d	O
x	O
i	O
f	O
y	O
ylvdx	O
y	O
f	O
where	O
fl	O
is	O
the	O
marginal	O
of	O
von	O
nd	O
therefore	O
it	O
follows	O
thatne	O
mndn	O
ne	O
where	O
xn	O
it	O
means	O
that	O
an	O
upper	O
bound	O
on	O
the	O
covering	B
number	I
of	O
the	O
class	O
of	O
neural	O
networks	O
is	O
also	O
an	O
upper	O
bound	O
on	O
the	O
quantity	O
that	O
interests	O
us	O
this	O
bounding	O
may	O
be	O
done	O
by	O
applying	O
lemmas	O
from	O
chapters	O
and	O
define	O
the	O
following	O
three	O
collections	O
of	O
functions	O
qi	O
x	O
b	O
a	O
end	O
ben	O
t	O
x	O
b	O
a	O
end	O
ben	O
x	O
b	O
a	O
end	O
ben	O
c	O
e	O
by	O
theorem	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
of	O
sets	O
qt	O
t	O
t	O
e	O
qd	O
is	O
vgt	O
d	O
this	O
implies	O
by	O
lemma	O
that	O
vg	O
d	O
so	O
by	O
corollary	O
for	O
any	O
xn	O
ne	O
e	O
e	O
where	O
e	O
nn	O
z	O
gxn	O
g	O
e	O
now	O
using	O
similar	O
notations	O
theorem	O
allows	O
us	O
to	O
estimate	O
covering	O
numbers	O
of	O
ne	O
n	O
if	O
fin	O
finally	O
we	O
can	O
apply	O
lemma	O
to	O
obtain	O
the	O
adaline	B
and	O
padaline	B
thus	O
substituting	O
this	O
bound	O
into	O
the	O
probability	O
inequality	B
above	O
we	O
get	O
for	O
n	O
large	O
enough	O
which	O
tends	O
to	O
zero	O
if	O
concluding	O
the	O
proof	O
n	O
there	O
are	O
yet	O
other	O
ways	O
to	O
obtain	O
consistency	B
for	O
general	O
sigmoidal	O
networks	O
we	O
may	O
restrict	O
the	O
network	O
by	O
discretizing	O
the	O
values	O
of	O
the	O
coefficients	O
in	O
some	O
way-thus	O
creating	O
a	O
sieve	O
with	O
a	O
number	O
of	O
members	O
that	O
is	O
easy	O
to	O
and	O
applying	O
complexity	B
regularization	I
this	O
is	O
the	O
method	O
followed	O
by	O
barron	O
the	O
adaline	B
and	O
padaline	B
widrow	O
and	O
widrow	O
and	O
hoff	O
introduced	O
the	O
adaline	B
and	O
specht	O
studied	O
polynomial	B
discriminant	O
functions	O
such	O
as	O
the	O
padaline	B
looked	O
at	O
formally	O
the	O
discriminant	O
function	O
used	O
in	O
the	O
decision	O
gx	O
o	O
is	O
of	O
a	O
polynomial	B
nature	O
with	O
consisting	O
of	O
sums	O
of	O
monomials	O
like	O
a	O
x	O
x	O
where	O
ii	O
id	O
are	O
integers	O
and	O
a	O
is	O
a	O
coefficient	O
the	O
order	O
of	O
a	O
monomial	B
is	O
i	O
id	O
usually	O
all	O
terms	O
up	O
to	O
and	O
including	O
those	O
of	O
order	O
r	O
are	O
included	O
widrows	O
adaline	B
has	O
r	O
the	O
total	O
number	O
of	O
monomials	O
of	O
order	O
r	O
or	O
less	O
does	O
not	O
exceed	O
the	O
motivation	O
for	O
developing	O
these	O
discriminants	O
is	O
that	O
only	O
up	O
to	O
ld	O
coefficients	O
need	O
to	O
be	O
trained	O
and	O
stored	O
in	O
applications	O
in	O
which	O
data	O
continuously	O
arrive	O
the	O
coefficients	O
may	O
be	O
updated	O
on-line	O
and	O
the	O
data	O
can	O
be	O
discarded	O
this	O
property	O
is	O
of	O
course	O
shared	O
with	O
standard	B
ral	O
networks	O
in	O
most	O
cases	O
order	O
r	O
polynomial	B
discriminants	O
are	O
not	O
translation	O
invariant	O
minimizing	O
a	O
given	O
criterion	O
on-line	O
is	O
a	O
phenomenal	O
task	O
so	O
specht	O
noted	O
that	O
training	O
is	O
not	O
necessary	O
if	O
the	O
as	O
are	O
chosen	O
so	O
as	O
to	O
give	O
decisions	O
that	O
are	O
close	O
to	O
those	O
of	O
the	O
kernel	B
method	O
with	O
normal	B
kernels	O
for	O
example	O
if	O
ku	O
e-	O
the	O
kernel	B
method	O
picks	O
a	O
smoothing	B
factor	I
h	O
neural	O
networks	O
that	O
h	O
and	O
nh	O
d	O
see	O
chapter	O
and	O
uses	O
xiii	O
n	O
il	O
h	O
the	O
same	O
decision	O
is	O
obtained	O
if	O
we	O
use	O
now	O
approximate	O
this	O
by	O
using	O
taylors	O
series	O
expansion	O
and	O
truncating	O
to	O
the	O
order	O
r	O
terms	O
for	O
example	O
the	O
coefficient	O
of	O
d	O
in	O
the	O
expansion	B
of	I
would	O
be	O
if	O
ll	O
i	O
j	O
i	O
these	O
sums	O
are	O
easy	O
to	O
update	O
on-line	O
and	O
decisions	O
are	O
based	O
on	O
the	O
sign	O
of	O
the	O
order	O
r	O
truncation	O
r	O
of	O
the	O
classifier	B
is	O
called	O
the	O
padaline	B
specht	O
notes	O
that	O
overfitting	B
in	O
does	O
not	O
occur	O
due	O
to	O
the	O
fact	O
that	O
overfitting	B
does	O
not	O
occur	O
for	O
the	O
kemel	O
method	O
based	O
on	O
his	O
method	O
interpolates	O
between	O
the	O
latter	O
method	O
generalized	B
linear	O
discrimination	O
and	O
generalizations	O
of	O
the	O
perceptron	B
for	O
fixed	O
r	O
the	O
pad	O
aline	O
defined	O
above	O
is	O
not	O
universally	O
consistent	O
the	O
same	O
reason	O
linear	O
discriminants	O
are	O
not	O
universally	O
consistent	O
but	O
if	O
r	O
is	O
allowed	O
to	O
grow	O
with	O
n	O
the	O
decision	O
based	O
on	O
the	O
sign	O
of	O
becomes	O
universally	O
consistent	O
recall	O
however	O
that	O
padaline	B
was	O
not	O
designed	O
with	O
a	O
variable	B
r	O
in	O
mind	O
polynomial	B
networks	O
besides	O
adaline	B
and	O
padaline	B
there	O
are	O
several	O
ways	O
of	O
constructing	O
mial	O
many-layered	O
networks	O
in	O
which	O
basic	O
units	O
are	O
of	O
the	O
form	O
for	O
inputs	O
xci	O
xk	O
to	O
that	O
level	O
pioneers	O
in	O
this	O
respect	O
are	O
gabor	O
ivakhnenko	O
who	O
invented	O
the	O
gmdh	O
method-the	O
group	O
method	O
of	O
data	O
handling-and	O
barron	O
see	O
also	O
ivakhnenko	O
konovalenko	O
tulupchuk	O
and	O
tymchenko	O
and	O
ivakhnenko	O
petrache	O
and	O
krasytskyy	O
these	O
networks	O
can	O
be	O
visualized	O
in	O
the	O
following	O
way	O
polynomial	B
networks	O
figure	O
simple	O
polynomial	B
network	I
each	O
gi	O
represents	O
a	O
simple	O
polynomialfunction	O
of	O
its	O
input	O
in	O
barrons	O
work	O
barron	O
and	O
barron	O
the	O
gi	O
are	O
sometimes	O
elements	O
of	O
the	O
form	O
gix	O
y	O
ai	O
bix	O
ciy	O
dixy	O
if	O
the	O
gi	O
are	O
barrons	O
quadratic	O
elements	O
then	O
the	O
network	O
shown	O
in	O
figure	O
represents	O
a	O
particular	O
polynomial	B
of	O
order	O
in	O
which	O
the	O
largest	O
degree	O
of	O
any	O
xci	O
is	O
at	O
most	O
the	O
number	O
of	O
unknown	O
coefficients	O
is	O
in	O
the	O
example	O
shown	O
in	O
the	O
figure	O
while	O
in	O
a	O
full-fledged	O
polynomial	B
network	I
it	O
would	O
be	O
much	O
larger	O
for	O
training	O
these	O
networks	O
many	O
strategies	O
have	O
been	O
proposed	O
by	O
barron	O
and	O
his	O
associates	O
ivakhnenko	O
for	O
example	O
trains	O
one	O
layer	O
at	O
a	O
time	O
and	O
lets	O
only	O
the	O
best	O
neurons	O
in	O
each	O
layer	O
survive	O
for	O
use	O
as	O
input	O
in	O
the	O
next	O
layer	O
it	O
is	O
easy	O
to	O
see	O
that	O
polynomial	B
networks	O
even	O
with	O
only	O
two	O
inputs	O
per	O
node	O
and	O
with	O
degree	O
in	O
each	O
cell	O
restricted	O
to	O
two	O
but	O
with	O
an	O
unrestricted	O
number	O
of	O
layers	O
can	O
implement	O
any	O
polynomial	B
in	O
d	O
variables	O
as	O
the	O
polynomials	O
are	O
dense	O
in	O
the	O
loo	O
sense	O
on	O
ca	O
bd	O
for	O
all	O
a	O
bend	O
we	O
note	O
by	O
lemma	O
that	O
such	O
networks	O
include	O
a	O
sequence	O
of	O
classifiers	O
approaching	O
the	O
bayes	B
error	I
for	O
any	O
distribution	O
consider	O
several	O
classes	O
of	O
polynomials	O
where	O
ok	O
are	O
fixed	O
monomials	O
but	O
the	O
ais	O
are	O
free	O
coefficients	O
e	O
ok	O
are	O
monomials	O
of	O
order	O
r	O
order	O
of	O
a	O
monomial	B
d	O
is	O
il	O
id	O
e	O
ok	O
are	O
monomials	O
of	O
order	O
r	O
but	O
k	O
is	O
not	O
fixed	O
as	O
k	O
does	O
not	O
generally	O
become	O
dense	O
in	O
ca	O
bd	O
in	O
the	O
loo	O
sense	O
unless	O
we	O
add	O
okl	O
in	O
a	O
special	O
way	O
however	O
becomes	O
dense	O
as	O
r	O
by	O
theorem	O
and	O
becomes	O
dense	O
as	O
both	O
k	O
and	O
r	O
contains	O
a	O
subclass	O
of	O
for	O
a	O
smaller	O
r	O
depending	O
upon	O
k	O
obtained	O
by	O
including	O
in	O
lii	O
ok	O
all	O
monomials	O
in	O
increasing	O
order	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
of	O
classifiers	O
based	O
on	O
is	O
not	O
more	O
than	O
k	O
theorem	O
the	O
vc	B
dimension	B
of	O
classifiers	O
based	O
upon	O
does	O
not	O
exceed	O
neural	O
networks	O
those	O
of	O
which	O
in	O
tum	O
is	O
nothing	O
but	O
the	O
number	O
of	O
possible	O
monomials	O
of	O
order	O
r	O
a	O
simple	O
counting	O
argument	O
shows	O
that	O
this	O
is	O
bounded	O
by	O
see	O
also	O
anthony	O
and	O
holden	O
these	O
simple	O
bounds	O
may	O
be	O
used	O
to	O
study	O
the	O
consistency	B
of	O
polynomial	B
works	O
let	O
us	O
take	O
a	O
fixed	O
structure	O
network	O
in	O
which	O
all	O
nodes	O
are	O
fixed-they	O
have	O
at	O
most	O
k	O
inputs	O
with	O
k	O
fixed	O
and	O
represent	O
polynomials	O
of	O
order	O
r	O
with	O
r	O
fixed	O
for	O
example	O
with	O
r	O
each	O
cell	O
with	O
input	O
z	O
i	O
zk	O
computes	O
li	O
ai	O
oi	O
zk	O
where	O
the	O
ais	O
are	O
coefficients	O
and	O
the	O
oi	O
are	O
fixed	O
als	O
of	O
order	O
r	O
or	O
less	O
and	O
all	O
such	O
monomials	O
are	O
included	O
assume	O
that	O
the	O
layout	O
is	O
fixed	O
and	O
is	O
such	O
that	O
it	O
can	O
realize	O
all	O
polynomials	O
of	O
order	O
s	O
on	O
the	O
input	O
xl	O
xd	O
one	O
way	O
of	O
doing	O
this	O
is	O
to	O
realize	O
all	O
polynomials	O
of	O
order	O
r	O
by	O
taking	O
all	O
possible	O
input	O
combinations	O
and	O
to	O
repeat	O
at	O
the	O
second	O
level	O
with	O
cells	O
of	O
neurons	O
and	O
so	O
forth	O
for	O
a	O
total	O
of	O
s	O
r	O
layers	O
of	O
cells	O
this	O
tion	O
is	O
obviously	O
redundant	O
but	O
it	O
will	O
do	O
for	O
now	O
then	O
note	O
that	O
the	O
vc	B
dimension	B
is	O
not	O
more	O
than	O
as	O
noted	O
above	O
if	O
we	O
choose	O
the	O
best	O
coefficients	O
in	O
the	O
cells	O
by	O
empirical	B
risk	I
minimization	I
then	O
the	O
method	O
is	O
consistent	O
theorem	O
in	O
the	O
fixed-structure	O
polynomial	B
network	I
described	O
above	O
if	O
ln	O
is	O
the	O
probability	O
of	O
error	O
of	O
the	O
empirical	B
risk	O
minimizer	O
then	O
e	O
ln	O
l	O
if	O
s	O
and	O
s	O
on	O
lid	O
proof	O
apply	O
lemma	O
and	O
theorem	O
assume	O
a	O
fixed-structure	O
network	O
as	O
above	O
such	O
that	O
all	O
polynomials	O
of	O
order	O
s	O
are	O
realized	O
plus	O
some	O
other	O
ones	O
while	O
the	O
number	O
of	O
layers	O
of	O
cells	O
is	O
not	O
more	O
than	O
i	O
then	O
the	O
vc	B
dimension	B
is	O
not	O
more	O
than	O
i	O
ld	O
because	O
the	O
maximal	O
order	O
is	O
not	O
more	O
than	O
ir	O
hence	O
we	O
have	O
consistency	B
under	O
the	O
same	O
conditions	O
as	O
above	O
that	O
is	O
s	O
and	O
i	O
on	O
lid	O
similar	O
considerations	O
can	O
now	O
be	O
used	O
in	O
a	O
variety	O
of	O
situations	O
kolmogorov-lorentz	O
networks	O
and	O
additive	O
models	O
answering	O
one	O
of	O
hilberts	O
famous	O
questions	O
kolmogorov	O
and	O
lorentz	O
also	O
sprecher	O
and	O
hecht-nielsen	O
obtained	O
the	O
following	O
interesting	O
representation	O
of	O
any	O
continuous	O
function	O
on	O
theorem	O
lorentz	O
let	O
f	O
be	O
continuous	O
on	O
then	O
f	O
can	O
be	O
rewritten	O
asfollows	O
let	O
be	O
an	O
arbitrary	O
constant	O
and	O
choose	O
e	O
rational	O
then	O
f	O
l	O
gzk	O
kl	O
kolmogorov-lorentz	O
networks	O
and	O
additive	O
models	O
where	O
g	O
r	O
r	O
is	O
a	O
continuous	O
function	O
upon	O
f	O
and	O
e	O
and	O
each	O
zk	O
is	O
rewritten	O
as	O
d	O
zk	O
lak	O
ek	O
k	O
jl	O
here	O
a	O
is	O
real	O
and	O
is	O
monotonic	O
and	O
increasing	O
in	O
its	O
argument	O
also	O
both	O
a	O
and	O
are	O
universal	O
of	O
f	O
and	O
is	O
lipschitz	O
clx	O
y	O
for	O
some	O
c	O
the	O
kolmogorov-lorentz	O
theorem	O
states	O
that	O
f	O
may	O
be	O
represented	O
by	O
a	O
very	O
simple	O
network	O
that	O
we	O
will	O
call	O
the	O
kolmogorov-lorentz	B
network	I
what	O
is	O
ing	O
is	O
that	O
the	O
first	O
layer	O
is	O
fixed	O
and	O
known	O
beforehand	O
only	O
the	O
mapping	O
g	O
depends	O
on	O
f	O
this	O
representation	O
immediately	O
opens	O
up	O
new	O
revenues	O
of	O
we	O
need	O
not	O
mix	O
the	O
input	O
variables	O
simple	O
additive	O
functions	O
of	O
the	O
input	O
variables	O
suffice	O
to	O
represent	O
all	O
continuous	O
functions	O
input	O
figure	O
the	O
kolmogorov-lorentz	B
network	I
of	O
theorem	O
to	O
explain	O
what	O
is	O
happening	O
here	O
we	O
look	O
at	O
the	O
interleaving	O
of	O
bits	O
to	O
make	O
one-dimensional	O
numbers	O
out	O
of	O
d-dimensional	O
vectors	O
for	O
the	O
sake	O
of	O
simplicity	O
let	O
f	O
r	O
let	O
be	O
the	O
binary	B
expansions	O
of	O
x	O
and	O
y	O
and	O
consider	O
a	O
representation	O
for	O
the	O
function	O
fx	O
y	O
the	O
bit-interleaved	O
number	O
z	O
e	O
has	O
binary	B
expansion	O
and	O
may	O
thus	O
be	O
written	O
as	O
z	O
where	O
neural	O
networks	O
and	O
thus	O
we	O
can	O
also	O
retrieve	O
x	O
and	O
y	O
from	O
z	O
by	O
noting	O
that	O
x	O
y	O
and	O
therefore	O
jx	O
y	O
def	O
gz	O
one-dimensional	O
function	O
of	O
z	O
g	O
the	O
function	O
cp	O
is	O
strictly	O
monotone	O
increasing	O
unfortunately	O
cp	O
and	O
are	O
not	O
continuous	O
kolmogorovs	O
theorem	O
for	O
this	O
special	O
case	O
is	O
as	O
follows	O
theorem	O
there	O
exist	O
jive	O
monotone	O
junctions	O
cpi	O
cps	O
rsatisfying	O
ixi	O
withthejollowing	O
property	O
jor	O
every	O
j	O
e	O
eo	O
continuous	O
junctions	O
on	O
there	O
exists	O
a	O
continuous	O
junction	O
g	O
such	O
thatjor	O
all	O
e	O
the	O
difference	O
with	O
pure	O
bit-interleaving	O
is	O
that	O
now	O
the	O
cpi	O
are	O
continuous	O
and	O
g	O
is	O
continuous	O
whenever	O
j	O
is	O
also	O
just	O
as	O
in	O
our	O
simle	O
example	O
kolmogorov	O
gives	O
an	O
explicit	O
construction	O
for	O
cpl	O
cps	O
kolmogorovs	O
theorem	O
may	O
be	O
used	O
to	O
show	O
the	O
denseness	O
of	O
certain	O
classes	O
of	O
functions	O
that	O
may	O
be	O
described	O
by	O
networks	O
there	O
is	O
one	O
pitfall	O
however	O
any	O
such	O
result	O
must	O
involve	O
at	O
least	O
one	O
neuron	O
or	O
cell	O
that	O
has	O
a	O
general	O
function	O
in	O
it	O
and	O
we	O
are	O
back	O
at	O
square	O
one	O
because	O
a	O
general	O
function	O
even	O
on	O
only	O
one	O
input	O
may	O
be	O
arbitrarily	O
complicated	O
and	O
wild	O
additive	O
models	O
include	O
for	O
example	O
models	O
such	O
as	O
d	O
ex	O
l	O
oixi	O
il	O
where	O
the	O
ois	O
are	O
unspecified	O
univariate	O
functions	O
and	O
silverman	O
hastie	O
and	O
tibshirani	O
these	O
are	O
not	O
powerful	O
enough	O
to	O
mate	O
all	O
functions	O
a	O
generalized	B
additive	B
model	I
is	O
kolmogorov-lorentz	O
networks	O
and	O
additive	O
models	O
and	O
tibshirani	O
where	O
is	O
now	O
a	O
given	O
or	O
unspecified	O
function	O
from	O
kolmogorovs	O
theorem	O
we	O
know	O
that	O
the	O
model	O
with	O
lfik	O
lf	O
unspecified	O
functions	O
includes	O
all	O
continuous	O
functions	O
on	O
compact	O
sets	O
and	O
is	O
thus	O
ideally	O
suited	O
for	O
constructing	O
networks	O
in	O
fact	O
we	O
may	O
take	O
ctk	O
k	O
and	O
take	O
alllfiks	O
as	O
specified	O
in	O
kolmogorovs	O
theorem	O
this	O
leaves	O
only	O
lf	O
as	O
the	O
unknown	O
now	O
consider	O
the	O
following	O
any	O
continuous	O
univariate	O
function	O
f	O
can	O
be	O
proximated	O
on	O
bounded	O
sets	O
to	O
within	O
e	O
by	O
simple	O
combinations	O
of	O
threshold	B
sigmoids	O
of	O
the	O
form	O
k	O
l	O
ci	O
il	O
where	O
ai	O
ci	O
k	O
are	O
variable	B
this	O
leads	O
to	O
a	O
two-hidden-layer	O
neural	B
network	I
sentation	O
related	O
to	O
that	O
of	O
kurkova	O
where	O
only	O
the	O
last	O
layer	O
has	O
unknown	O
coefficients	O
for	O
a	O
total	O
of	O
theorem	O
consider	O
a	O
network	O
classifier	B
of	O
the	O
form	O
described	O
above	O
in	O
which	O
is	O
the	O
threshold	B
sigmoid	B
and	O
the	O
ais	O
and	O
ci	O
are	O
found	O
by	O
empirical	B
error	I
minimization	O
then	O
el	O
n	O
l	O
for	O
all	O
distributions	O
of	O
y	O
if	O
k	O
and	O
klognjn	O
o	O
proof	O
we	O
will	O
only	O
outline	O
the	O
proof	O
first	O
observe	O
that	O
we	O
may	O
approximate	O
all	O
functions	O
on	O
ca	O
bd	O
by	O
selecting	O
k	O
large	O
enough	O
by	O
lemma	O
and	O
orem	O
it	O
suffices	O
to	O
show	O
that	O
the	O
vc	B
dimension	B
of	O
our	O
class	O
of	O
classifiers	O
is	O
on	O
j	O
log	O
n	O
considering	O
cik	O
lfikxci	O
as	O
new	O
input	O
elements	O
called	O
yb	O
s	O
k	O
s	O
we	O
note	O
that	O
the	O
vc	B
dimension	B
is	O
not	O
more	O
than	O
that	O
of	O
the	O
classifiers	O
based	O
on	O
k	O
l	O
l	O
ci	O
kl	O
il	O
which	O
in	O
turn	O
is	O
not	O
more	O
than	O
that	O
of	O
the	O
classifiers	O
given	O
by	O
l	O
dz	O
il	O
where	O
are	O
parameters	O
and	O
is	O
an	O
input	O
sequence	O
by	O
theorem	O
the	O
vc	B
dimension	B
is	O
not	O
more	O
than	O
this	O
concludes	O
the	O
proof	O
for	O
more	O
results	O
along	O
these	O
lines	O
we	O
refer	O
to	O
kurkova	O
neural	O
networks	O
projection	B
pursuit	I
in	O
projection	B
pursuit	I
and	O
tukey	O
friedman	O
and	O
stuetzle	O
friedman	O
stuetzle	O
and	O
schroeder	O
huber	O
hall	O
flick	O
jones	O
priest	O
and	O
herman	O
one	O
considers	O
functions	O
of	O
the	O
form	O
k	O
ox	O
l	O
ojbj	O
aj	O
x	O
jl	O
where	O
b	O
j	O
e	O
r	O
a	O
j	O
e	O
rd	O
are	O
constants	O
and	O
ok	O
are	O
fixed	O
functions	O
this	O
is	O
related	O
to	O
but	O
not	O
a	O
special	O
case	O
of	O
one-hidden-iayer	O
neural	O
networks	O
based	O
upon	O
the	O
kolmogorov-lorentz	B
representation	I
theorem	O
we	O
may	O
also	O
consider	O
d	O
ox	O
l	O
ojxj	O
jl	O
for	O
fixed	O
functions	O
oj	O
and	O
silverman	O
hastie	O
and	O
tibshirani	O
in	O
and	O
the	O
ojs	O
may	O
be	O
approximated	O
in	O
tum	O
by	O
spline	O
functions	O
or	O
other	O
nonparametric	O
constructs	O
this	O
approach	O
is	O
covered	O
in	O
the	O
ature	O
on	O
generalized	B
additive	O
models	O
hastie	O
and	O
tibshirani	O
the	O
class	O
of	O
functions	O
eat	O
x	O
a	O
e	O
r	O
d	O
satisfies	O
the	O
conditions	O
of	O
the	O
weierstrass	O
theorem	O
and	O
is	O
therefore	O
dense	O
in	O
the	O
loo	O
norm	O
on	O
ca	O
bd	O
for	O
any	O
a	O
b	O
e	O
rd	O
e	O
g	O
diaconis	O
and	O
shahshahani	O
as	O
a	O
corollary	O
we	O
note	O
that	O
the	O
same	O
denseness	O
result	O
applies	O
to	O
the	O
family	O
k	O
l	O
oiat	O
x	O
il	O
where	O
k	O
is	O
arbitrary	O
and	O
are	O
general	O
functions	O
the	O
latter	O
result	O
is	O
at	O
the	O
basis	O
of	O
projection	B
pursuit	I
methods	O
for	O
approximating	O
functions	O
where	O
one	O
tries	O
to	O
find	O
vectors	O
ai	O
and	O
functions	O
oi	O
that	O
approximate	O
a	O
given	O
function	O
very	O
well	O
remark	O
in	O
some	O
cases	O
approximations	O
by	O
functions	O
as	O
in	O
may	O
be	O
exact	O
for	O
example	O
and	O
theorem	O
and	O
shahshahani	O
let	O
m	O
be	O
a	O
positive	O
teger	O
there	O
are	O
distinct	O
vectors	O
a	O
j	O
e	O
rd	O
such	O
that	O
any	O
homogeneous	O
polynomial	B
f	O
of	O
order	O
m	O
can	O
be	O
written	O
as	O
projection	B
pursuit	I
c	O
-l	O
fx	O
l	O
cjaj	O
xm	O
jl	O
for	O
some	O
real	O
numbers	O
c	O
j	O
every	O
polynomial	B
of	O
order	O
mover	O
n	O
d	O
is	O
a	O
homogeneous	O
polynomial	B
of	O
order	O
m	O
over	O
n	O
d	O
by	O
replacing	O
the	O
constant	O
by	O
a	O
component	O
x	O
raised	O
to	O
an	O
appropriate	O
power	O
thus	O
any	O
polynomial	B
of	O
order	O
mover	O
nd	O
may	O
be	O
decomposed	O
exactly	O
by	O
f	O
t	O
c	O
j	O
j	O
x	O
b	O
j	O
m	O
jl	O
for	O
some	O
real	O
numbers	O
b	O
j	O
c	O
j	O
and	O
vectors	O
a	O
j	O
e	O
nd	O
polynomials	O
may	O
thus	O
be	O
represented	O
exactly	O
in	O
the	O
form	O
cfjiaf	O
x	O
with	O
k	O
as	O
the	O
polynomials	O
are	O
dense	O
in	O
ca	O
bd	O
we	O
have	O
yet	O
another	O
proof	O
that	O
cfjiaf	O
x	O
is	O
dense	O
in	O
ca	O
bd	O
see	O
logan	O
and	O
shepp	O
or	O
logan	O
for	O
other	O
proofs	O
the	O
previous	O
discussion	O
suggests	O
at	O
least	O
two	O
families	O
from	O
which	O
to	O
select	O
a	O
discriminant	O
function	O
as	O
usual	O
we	O
let	O
gx	O
o	O
for	O
a	O
discriminant	O
function	O
here	O
could	O
be	O
picked	O
from	O
or	O
where	O
m	O
is	O
sufficiently	O
large	O
if	O
we	O
draw	O
by	O
minimizing	O
the	O
empirical	B
error	I
at	O
a	O
tremendous	O
computational	O
cost	O
then	O
convergence	O
may	O
result	O
if	O
m	O
is	O
not	O
too	O
large	O
we	O
need	O
to	O
know	O
the	O
vc	B
dimension	B
of	O
the	O
classes	O
of	O
classifiers	O
corresponding	O
ton	O
and	O
f	O
note	O
that	O
fm	O
coincides	O
with	O
all	O
polynomials	O
of	O
order	O
m	O
and	O
each	O
such	O
polynomial	B
is	O
the	O
sum	O
of	O
at	O
most	O
ld	O
monomials	O
if	O
we	O
invoke	O
lemma	O
and	O
theorem	O
then	O
we	O
get	O
theorem	O
empirical	B
risk	I
minimization	I
to	O
determine	O
j	O
b	O
j	O
c	O
j	O
in	O
fm	O
leads	O
to	O
a	O
universally	O
consistent	O
classifier	B
provided	O
that	O
m	O
and	O
m	O
on	O
lid	O
log	O
n	O
projection	B
pursuit	I
is	O
very	O
powerful	O
and	O
not	O
at	O
all	O
confined	O
to	O
our	O
limited	O
sion	O
above	O
in	O
particular	O
there	O
are	O
many	O
other	O
ways	O
of	O
constructing	O
good	O
consistent	O
classifiers	O
that	O
do	O
not	O
require	O
extensive	O
computations	O
such	O
as	O
empirical	B
error	I
imization	O
neural	O
networks	O
radial	B
basis	B
function	I
networks	O
we	O
may	O
perform	O
discrimination	O
based	O
upon	O
networks	O
with	O
functions	O
of	O
the	O
form	O
decision	O
gx	O
o	O
where	O
k	O
is	O
an	O
integer	O
ai	O
ak	O
hi	O
hk	O
are	O
constants	O
xl	O
xk	O
e	O
rd	O
and	O
k	O
is	O
a	O
kernel	B
function	O
as	O
ku	O
or	O
ku	O
io	O
in	O
this	O
form	O
covers	O
several	O
well-known	O
methodologies	O
the	O
kernel	B
rule	B
take	O
k	O
n	O
ai	O
hi	O
h	O
xi	O
xi	O
with	O
this	O
choice	O
for	O
a	O
large	O
class	O
of	O
kernels	O
we	O
are	O
guaranteed	O
convergence	O
if	O
h	O
and	O
nh	O
d	O
this	O
approach	O
is	O
attractive	O
as	O
no	O
difficult	O
optimization	O
problem	O
needs	O
to	O
be	O
solved	O
the	O
potential	O
function	O
method	O
in	O
bashkirov	O
braverman	O
and	O
muchnik	O
the	O
parameters	O
are	O
k	O
n	O
hi	O
h	O
xi	O
xi	O
the	O
weights	O
ai	O
are	O
picked	O
to	O
minimize	O
the	O
empirical	B
error	I
on	O
the	O
data	O
and	O
h	O
is	O
held	O
fixed	O
the	O
original	O
kernel	B
suggested	O
there	O
is	O
ku	O
io	O
linear	O
discrimination	O
for	O
k	O
ku	O
e-	O
hi	O
h	O
al	O
the	O
set	O
o	O
is	O
a	O
linear	O
halfspace	O
this	O
of	O
course	O
is	O
not	O
versally	O
consistent	O
observe	O
that	O
the	O
separating	O
hyperplane	O
is	O
the	O
collection	O
of	O
all	O
points	O
x	O
at	O
equal	O
distance	O
from	O
xl	O
and	O
by	O
varying	O
xl	O
and	O
all	O
hyperplanes	O
may	O
be	O
obtained	O
radial	B
basis	B
function	I
neural	O
networks	O
powell	O
head	O
and	O
lowe	O
moody	O
and	O
darken	O
poggio	O
and	O
girosi	O
xu	O
krzyzak	O
and	O
oja	O
xu	O
krzyzak	O
and	O
yuille	O
and	O
krzyzak	O
linder	O
and	O
lugosi	O
an	O
even	O
more	O
general	O
function	O
is	O
usually	O
employed	O
here	O
k	O
l	O
ci	O
k	O
xd	O
t	O
aix	O
xi	O
co	O
il	O
where	O
the	O
as	O
are	O
tunable	O
d	O
x	O
d	O
matrices	O
sieve	O
methods	O
grenander	O
and	O
geman	O
and	O
hwang	O
advocate	O
the	O
use	O
of	B
maximum	B
likelihood	I
methods	O
to	O
find	O
suitable	O
values	O
for	O
the	O
tunable	O
parameters	O
in	O
k	O
k	O
fixed	O
beforehand	O
subject	O
to	O
certain	O
compactness	O
constraints	O
on	O
these	O
parameters	O
to	O
control	O
the	O
abundance	O
of	O
choices	O
one	O
may	O
have	O
if	O
we	O
were	O
to	O
use	O
empirical	B
error	I
minimization	O
we	O
would	O
find	O
if	O
k	O
n	O
that	O
all	O
data	O
points	O
can	O
be	O
correctly	O
classified	O
the	O
his	O
small	O
enough	O
setai	O
xi	O
xi	O
k	O
n	O
causing	O
overfitting	B
hence	O
k	O
must	O
be	O
smaller	O
than	O
n	O
if	O
parameters	O
are	O
picked	O
in	O
this	O
manner	O
practical	O
ways	O
of	O
choosing	O
the	O
parameters	O
are	O
discussed	O
by	O
kraaijveld	O
and	O
duin	O
radial	B
basis	B
function	I
networks	O
and	O
chou	O
and	O
chen	O
in	O
both	O
and	O
the	O
xis	O
may	O
be	O
thought	O
of	O
as	O
representative	O
prototypes	O
the	O
ais	O
as	O
weights	O
and	O
the	O
his	O
as	O
the	O
radii	O
of	O
influence	O
as	O
a	O
rule	B
k	O
is	O
much	O
smaller	O
than	O
n	O
as	O
xl	O
xk	O
summarizes	O
the	O
information	O
present	O
at	O
the	O
data	O
to	O
design	O
a	O
consistent	O
rbf	O
neural	B
network	I
classifier	B
we	O
may	O
proceed	O
as	O
in	O
we	O
may	O
also	O
take	O
k	O
but	O
k	O
on	O
just	O
let	O
xd	O
xk	O
ak	O
and	O
choose	O
ai	O
or	O
hi	O
to	O
minimize	O
a	O
given	O
error	O
criterion	O
based	O
upon	O
x	O
k	O
i	O
x	O
n	O
such	O
as	O
a	O
ln	O
l	O
igx	O
n	O
n	O
ikl	O
where	O
gx	O
o	O
and	O
is	O
as	O
in	O
this	O
is	O
nothing	O
but	O
data	O
splitting	O
convergence	O
conditions	O
are	O
described	O
in	O
theorem	O
a	O
more	O
ambitious	O
person	O
might	O
try	O
empirical	B
risk	I
minimization	I
to	O
find	O
the	O
best	O
xl	O
xb	O
ai	O
ab	O
ai	O
ak	O
x	O
d	O
matrices	O
as	O
in	O
based	O
upon	O
n	O
igx	O
yd	O
n	O
ii	O
if	O
k	O
the	O
class	O
of	O
rules	O
contains	O
a	O
consistent	O
subsequence	O
and	O
therefore	O
it	O
suffices	O
only	O
to	O
show	O
that	O
the	O
vc	B
dimension	B
is	O
on	O
log	O
n	O
this	O
is	O
a	O
difficult	O
task	O
and	O
some	O
kernels	O
yield	O
infinite	O
vc	B
dimension	B
even	O
if	O
d	O
i	O
and	O
k	O
is	O
very	O
small	O
chapter	O
however	O
there	O
is	O
a	O
simple	O
argument	O
if	O
k	O
ir	O
for	O
a	O
simple	O
set	O
r	O
let	O
a	O
x	O
a	O
ay	O
y	O
e	O
r	O
a	O
end	O
a	O
a	O
d	O
x	O
d	O
matrix	O
if	O
r	O
is	O
a	O
sphere	B
then	O
a	O
is	O
the	O
class	O
of	O
all	O
ellipsoids	O
the	O
number	O
of	O
ways	O
of	O
shattering	B
a	O
set	O
xn	O
by	O
intersecting	O
with	O
members	O
from	O
a	O
is	O
not	O
more	O
than	O
ndd	O
theorem	O
problem	O
the	O
number	O
of	O
ways	O
of	O
shattering	B
a	O
set	O
x	O
by	O
intersecting	O
with	O
sets	O
of	O
the	O
form	O
is	O
not	O
more	O
than	O
the	O
product	B
of	O
all	O
ways	O
of	O
shattering	B
by	O
intersections	O
with	O
ri	O
with	O
and	O
so	O
forth	O
that	O
is	O
the	O
logarithm	O
of	O
the	O
shatter	B
coefficient	I
is	O
on	O
if	O
k	O
on	O
log	O
n	O
thus	O
by	O
corollary	O
we	O
have	O
theorem	O
linder	O
and	O
lugosi	O
lfwe	O
take	O
k	O
k	O
on	O
log	O
n	O
in	O
the	O
rbf	O
classifier	B
in	O
which	O
k	O
ir	O
r	O
being	O
the	O
unit	O
ball	O
of	O
neural	O
networks	O
r	O
d	O
and	O
in	O
which	O
all	O
the	O
parameters	O
are	O
chosen	O
by	O
empirical	B
risk	I
minimization	I
then	O
el	O
n	O
l	O
for	O
all	O
distributions	O
of	O
y	O
furthermore	O
ijqk	O
is	O
the	O
class	O
of	O
all	O
rbf	O
classifiers	O
with	O
k	O
prototypes	O
el	O
n	O
inf	O
lg	O
geqk	O
kdd	O
log	O
n	O
the	O
theorem	O
remains	O
valid	O
modified	O
constants	O
in	O
the	O
error	O
estimate	O
when	O
r	O
is	O
a	O
hyperrectangle	O
or	O
polytope	O
with	O
a	O
bounded	O
number	O
of	O
faces	O
however	O
for	O
more	O
general	O
k	O
the	O
vc	B
dimension	B
is	O
more	O
difficult	O
to	O
evaluate	O
for	O
general	O
kernels	O
consistent	O
rbf	O
classifiers	O
can	O
be	O
obtained	O
by	O
empirical	B
l	O
i	O
or	O
error	O
minimization	O
however	O
no	O
efficient	O
practical	O
algorithms	O
are	O
known	O
to	O
compute	O
the	O
minima	O
finally	O
as	O
suggested	O
by	O
chou	O
and	O
chen	O
and	O
kraaijveld	O
and	O
duin	O
it	O
is	O
a	O
good	O
idea	O
to	O
place	O
xl	O
by	O
k-means	B
clustering	B
or	O
another	O
clustering	B
method	O
and	O
to	O
build	O
an	O
rbf	O
classifier	B
with	O
those	O
values	O
or	O
by	O
optimization	O
started	O
at	O
the	O
given	O
cluster	O
centers	O
problems	O
and	O
exercises	O
problem	O
let	O
k	O
i	O
be	O
integers	O
with	O
d	O
k	O
n	O
and	O
i	O
assume	O
x	O
has	O
a	O
density	O
let	O
a	O
be	O
a	O
collection	O
of	O
hyperplanes	O
drawn	O
from	O
the	O
e	O
possible	O
hyperplanes	O
through	O
d	O
points	O
of	O
x	O
k	O
and	O
let	O
ga	O
be	O
the	O
corresponding	O
natural	B
classifier	B
based	O
upon	O
the	O
arrangement	B
pea	O
take	O
i	O
such	O
collections	O
a	O
at	O
random	O
and	O
with	O
replacement	O
and	O
pick	O
the	O
best	O
a	O
by	O
minimizing	O
lnga	O
where	O
show	O
that	O
the	O
selected	O
classifier	B
is	O
consistent	O
if	O
i	O
ld	O
on	O
n	O
k	O
this	O
is	O
applicable	O
with	O
k	O
l	O
n	O
j	O
that	O
is	O
half	O
the	O
sample	O
is	O
used	O
to	O
define	O
a	O
and	O
the	O
other	O
half	O
is	O
used	O
to	O
pick	O
a	O
classifier	B
empilically	O
problem	O
we	O
are	O
given	O
a	O
tree	O
classifier	B
with	O
k	O
internal	O
linear	O
splits	O
and	O
k	O
i	O
leaf	O
regions	O
bsp	O
tree	O
show	O
how	O
to	O
combine	O
the	O
neurons	O
in	O
a	O
two-hidden-iayer	O
perceptron	B
with	O
k	O
and	O
k	O
i	O
hidden	O
neurons	O
in	O
the	O
two	O
hidden	O
layers	O
so	O
as	O
to	O
obtain	O
a	O
decision	O
that	O
is	O
identical	O
to	O
the	O
tree-based	O
classifier	B
sethi	O
for	O
more	O
on	O
the	O
equivalence	O
of	O
decision	O
trees	O
and	O
neural	O
networks	O
see	O
meisel	O
koutsougeras	O
and	O
papachristou	O
or	O
golea	O
and	O
marchand	O
hint	O
mimic	O
the	O
argument	O
for	O
arrangements	O
in	O
text	O
problem	O
extend	O
the	O
proof	O
of	O
theorem	O
so	O
that	O
it	O
includes	O
any	O
nondecreasing	O
sigmoid	B
with	O
limx	O
oo	O
o-x	O
and	O
limxoo	O
o-x	O
hint	O
if	O
t	O
is	O
large	O
o-tx	O
imates	O
the	O
threshold	B
sigmoid	B
problem	O
this	O
exercise	O
states	O
denseness	O
in	O
l	O
i	O
for	O
any	O
probability	O
measure	O
fj	O
show	O
that	O
for	O
every	O
probability	O
measure	O
fj	O
on	O
r	O
d	O
every	O
measurable	O
function	O
f	O
rd	O
r	O
problems	O
and	O
exercises	O
with	O
j	O
and	O
every	O
e	O
there	O
exists	O
a	O
neural	B
network	I
with	B
one	I
hidden	I
layer	I
and	O
function	O
as	O
in	O
such	O
that	O
f	O
i	O
x	O
e	O
hint	O
proceed	O
as	O
in	O
theorem	O
considering	O
the	O
following	O
approximate	O
in	O
l	O
by	O
a	O
continuous	O
function	O
gx	O
that	O
is	O
zero	O
outside	O
some	O
bounded	O
set	O
b	O
c	O
since	O
gx	O
is	O
bounded	O
its	O
maximum	O
maxxerd	O
is	O
finite	O
now	O
choose	O
a	O
to	O
be	O
a	O
positive	O
number	O
large	O
enough	O
so	O
that	O
both	O
b	O
c	O
ad	O
andu	O
ad	O
is	O
large	O
extend	O
the	O
restriction	O
of	O
gx	O
to	O
ad	O
periodically	O
by	O
tiling	O
over	O
all	O
of	O
nd	O
the	O
obtained	O
function	O
ix	O
is	O
a	O
good	O
approximation	O
of	O
gx	O
in	O
l	O
j	O
take	O
the	O
fourier	O
series	O
approximation	O
of	O
ix	O
and	O
use	O
the	O
stone-weierstrass	B
theorem	I
as	O
in	O
theorem	O
observe	O
that	O
every	O
continuous	O
function	O
on	O
the	O
real	O
line	O
that	O
is	O
zero	O
outside	O
some	O
bounded	O
interval	O
can	O
be	O
arbitrarily	O
closely	O
approximated	O
uniformly	O
over	O
the	O
whole	O
real	O
line	O
by	O
one-dimensional	O
neural	O
networks	O
thus	O
bounded	O
functions	O
such	O
as	O
the	O
sine	O
and	O
cosine	O
functions	O
can	O
be	O
approximated	O
arbitrarily	O
closely	O
by	O
neural	O
networks	O
in	O
l	O
for	O
any	O
probability	O
measure	O
j	O
on	O
n	O
apply	O
the	O
triangle	O
inequality	B
to	O
finish	O
the	O
proof	O
problem	O
generalize	O
the	O
previous	O
exercise	O
for	O
denseness	O
in	O
lpu	O
more	O
precisely	O
let	O
p	O
show	O
that	O
for	O
every	O
probability	O
measure	O
on	O
nd	O
every	O
measurable	O
function	O
nd	O
nwithjl	O
xipudx	O
ooandeverye	O
othereexistsaneural	O
network	O
with	B
one	I
hidden	I
layer	I
hx	O
such	O
that	O
ix	O
hex	O
lip	O
e	O
problem	O
committee	O
machines	O
let	O
ck	O
be	O
the	O
class	O
of	O
all	O
committee	O
machines	O
prove	O
that	O
for	O
all	O
distributions	O
of	O
y	O
lim	O
inf	O
l	O
l	O
k---oo	O
hint	O
for	O
a	O
one-hidden-layer	O
neural	B
network	I
with	O
coefficients	O
ci	O
in	O
approximate	O
the	O
cis	O
by	O
discretization	O
to	O
a	O
grid	O
of	O
values	O
and	O
note	O
that	O
ci	O
may	O
thus	O
be	O
approximated	O
in	O
a	O
committee	B
machine	I
by	O
a	O
sufficient	O
number	O
of	O
identical	O
copies	O
of	O
this	O
only	O
forces	O
the	O
number	O
of	O
neurons	O
to	O
be	O
a	O
bit	O
larger	O
problem	O
prove	O
theorem	O
for	O
arbitrary	O
sigmoids	O
hint	O
approximate	O
the	O
old	O
sigmoid	B
by	O
a	O
x	O
for	O
a	O
sufficiently	O
large	O
t	O
problem	O
let	O
a	O
be	O
a	O
nondecreasing	O
sigmoid	B
with	O
ax	O
if	O
x	O
and	O
ax	O
if	O
x	O
denote	O
by	O
ck	O
the	O
class	O
of	O
corresponding	O
neural	B
network	I
classifiers	O
with	O
k	O
hidden	O
layers	O
show	O
that	O
vck	O
vck	O
where	O
ck	O
is	O
the	O
class	O
corresponding	O
to	O
the	O
threshold	B
sigmoid	B
neural	O
networks	O
problem	O
this	O
exercise	O
generalizes	O
theorem	O
for	O
not	O
fully	O
connected	O
neural	O
works	O
with	B
one	I
hidden	I
layer	I
consider	O
the	O
class	O
ck	O
of	O
one-hidden-iayer	O
neural	O
networks	O
with	O
the	O
threshold	B
sigmoid	B
such	O
that	O
each	O
of	O
the	O
k	O
nodes	O
in	O
the	O
hidden	O
layer	O
are	O
connected	O
to	O
d	O
dk	O
inputs	O
where	O
di	O
d	O
more	O
precisely	O
ck	O
contains	O
all	O
classifiers	O
based	O
on	O
functions	O
of	O
the	O
form	O
co	O
l	O
where	O
l	O
d	O
k	O
i	O
where	O
for	O
each	O
i	O
mid	O
is	O
a	O
vector	O
of	O
distinct	O
positive	O
integers	O
not	O
exceeding	O
d	O
show	O
that	O
if	O
the	O
as	O
cs	O
and	O
mis	O
are	O
the	O
tunable	O
parameters	O
i	O
problem	O
let	O
be	O
a	O
sigmoid	B
that	O
takes	O
m	O
different	O
values	O
find	O
upper	O
bounds	O
on	O
the	O
vc	B
dimension	B
of	O
the	O
class	O
ck	O
problem	O
consider	O
a	O
one-hidden-iayer	O
neural	B
network	I
if	O
y	O
yn	O
are	O
fixed	O
and	O
all	O
xis	O
are	O
different	O
show	O
that	O
with	O
n	O
hidden	O
neurons	O
we	O
are	O
always	O
able	O
to	O
tune	O
the	O
weights	O
such	O
that	O
yi	O
for	O
all	O
i	O
remains	O
true	O
if	O
yen	O
instead	O
of	O
y	O
e	O
i	O
the	O
property	O
above	O
describes	O
a	O
situation	O
of	O
overfitting	B
that	O
occurs	O
when	O
the	O
neural	B
network	I
becomes	O
too	O
also	O
that	O
the	O
vc	B
dimension	B
which	O
is	O
at	O
least	O
d	O
times	O
the	O
number	O
of	O
hidden	O
neurons	O
must	O
remain	O
smaller	O
than	O
n	O
for	O
any	O
meaningful	O
training	O
problem	O
the	O
bernstein	B
perceptron	B
consider	O
the	O
following	O
perceptron	B
for	O
dimensional	O
data	O
if	O
xk-i	O
otherwise	O
let	O
us	O
call	O
this	O
the	O
bernstein	B
perceptron	B
since	O
it	O
involves	O
bernstein	O
polynomials	O
if	O
n	O
data	O
points	O
are	O
collected	O
how	O
would	O
you	O
choose	O
k	O
a	O
function	O
of	O
n	O
and	O
how	O
would	O
you	O
adjust	O
the	O
weights	O
ais	O
to	O
make	O
sure	O
that	O
the	O
bernstein	B
perceptron	B
is	O
consistent	O
for	O
all	O
distributions	O
of	O
y	O
with	O
pix	O
e	O
in	O
i	O
can	O
you	O
make	O
the	O
bernstein	B
perceptron	B
consistent	O
for	O
all	O
distributions	O
of	O
y	O
on	O
n	O
x	O
i	O
figure	O
the	O
bernstein	O
ceptron	O
problem	O
use	O
the	O
ideas	O
of	O
section	O
and	O
problem	O
to	O
prove	O
theorem	O
problem	O
consider	O
spechts	O
padaline	B
with	O
r	O
rn	O
t	O
let	O
h	O
hll	O
and	O
nhd	O
show	O
that	O
for	O
any	O
distribution	O
of	O
y	O
elj	O
l	O
problems	O
and	O
exercises	O
problem	O
bounded	O
first	O
layers	O
consider	O
a	O
feed-forward	O
neural	B
network	I
with	O
any	O
number	O
of	O
layers	O
with	O
only	O
one	O
restriction	O
that	O
is	O
the	O
first	O
layer	O
has	O
at	O
most	O
k	O
d	O
outputs	O
zk	O
where	O
x	O
xd	O
is	O
the	O
input	O
and	O
is	O
an	O
arbitrary	O
function	O
n	O
n	O
just	O
a	O
sigmoid	B
the	O
integer	O
k	O
remains	O
fixed	O
let	O
a	O
denote	O
the	O
k	O
x	O
matrix	O
of	O
weights	O
aji	O
bj	O
if	O
l	O
is	O
the	O
bayes	B
error	I
for	O
a	O
recognition	O
problem	O
based	O
upon	O
y	O
with	O
z	O
zk	O
and	O
then	O
show	O
that	O
for	O
some	O
distribution	O
of	O
y	O
inf	O
a	O
l	O
l	O
where	O
l	O
is	O
the	O
bayes	O
probability	O
of	O
error	O
for	O
y	O
if	O
k	O
d	O
however	O
show	O
that	O
for	O
any	O
strictly	O
monotonically	O
increasing	O
sigmoid	B
inf	O
a	O
l	O
l	O
use	O
to	O
conclude	O
that	O
any	O
neural	B
network	I
based	O
upon	O
a	O
first	O
layer	O
with	O
k	O
d	O
outputs	O
is	O
not	O
consistent	O
for	O
some	O
distribution	O
regardless	O
of	O
how	O
many	O
layers	O
it	O
has	O
however	O
that	O
the	O
inputs	O
of	O
each	O
layer	O
are	O
restricted	O
to	O
be	O
the	O
outputs	O
of	O
the	O
previous	O
layer	O
figure	O
the	O
first	O
layer	O
is	O
stricted	O
to	O
have	O
k	O
outputs	O
it	O
has	O
ked	O
tunable	O
parameters	O
zj	O
problem	O
find	O
conditions	O
on	O
kll	O
and	O
that	O
guarantee	O
strong	B
universal	B
consistency	B
in	O
theorem	O
problem	O
barron	O
networks	O
call	O
a	O
barron	B
network	I
a	O
network	O
of	O
any	O
number	O
of	O
layers	O
in	O
figure	O
with	O
inputs	O
per	O
cell	O
and	O
cells	O
that	O
perform	O
the	O
operation	O
a	O
yy	O
on	O
inputs	O
x	O
yen	O
with	O
trainable	O
weights	O
a	O
y	O
if	O
is	O
the	O
output	O
neural	O
networks	O
of	O
a	O
network	O
with	O
d	O
inputs	O
and	O
k	O
cells	O
in	O
any	O
way	O
compute	O
an	O
upper	O
bound	O
for	O
the	O
vc	B
dimension	B
of	O
the	O
classifier	B
gx	O
o	O
as	O
a	O
function	O
of	O
d	O
and	O
k	O
note	O
that	O
the	O
structure	O
of	O
the	O
network	O
the	O
positions	O
of	O
the	O
cells	O
and	O
the	O
connections	O
is	O
variable	B
problem	O
continued	O
restrict	O
the	O
balton	O
network	O
to	O
llayers	O
and	O
k	O
cells	O
per	O
layer	O
kl	O
cells	O
total	O
and	O
repeat	O
the	O
previous	O
exercise	O
problem	O
continued	O
find	O
conditions	O
on	O
k	O
and	O
l	O
in	O
the	O
previous	O
exercises	O
that	O
would	O
guarantee	O
the	O
universal	B
consistency	B
of	O
the	O
barron	B
network	I
if	O
we	O
train	O
to	O
minimize	O
the	O
empirical	B
eltor	O
problem	O
consider	O
the	O
family	B
of	I
functions	O
of	O
the	O
form	O
i	O
l	O
l	O
wij	O
x	O
i	O
x	O
k	O
d	O
i	O
i	O
k	O
j	O
where	O
all	O
the	O
wi	O
and	O
wil	O
j	O
are	O
tunable	O
parameters	O
show	O
that	O
for	O
every	O
i	O
e	O
co	O
ld	O
and	O
e	O
there	O
exist	O
k	O
llarge	O
enough	O
so	O
that	O
for	O
some	O
g	O
e	O
supxeoljd	O
ix	O
e	O
problem	O
continued	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
vc	B
dimension	B
of	O
the	O
above	O
two-hidden-iayer	O
network	O
hint	O
the	O
vc	B
dimension	B
is	O
usually	O
about	O
equal	O
to	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
is	O
dlk	O
here	O
problem	O
continued	O
if	O
gn	O
is	O
the	O
rule	B
obtained	O
by	O
empirical	B
error	I
minimization	O
over	O
then	O
show	O
that	O
lgn	O
l	O
in	O
probability	O
if	O
k	O
l	O
and	O
kl	O
onj	O
log	O
n	O
problem	O
how	O
many	O
different	O
monomials	O
of	O
order	O
r	O
in	O
xl	O
xed	O
are	O
there	O
how	O
does	O
this	O
grow	O
with	O
r	O
when	O
d	O
is	O
held	O
fixed	O
problem	O
show	O
that	O
and	O
in	O
the	O
bit-interleaving	O
example	O
in	O
the	O
section	O
on	O
kolmogorov-lorentz	O
representations	O
are	O
not	O
continuous	O
which	O
are	O
the	O
points	O
of	O
nuity	O
problem	O
let	O
pick	O
c	O
j	O
by	O
empirical	B
risk	I
minimization	I
for	O
the	O
classifier	B
gx	O
o	O
e	O
showthateln	O
l	O
for	O
all	O
distributions	O
of	O
ywhenm	O
ooandm	O
on	O
d	O
jlogn	O
problem	O
write	O
as	O
and	O
identify	O
the	O
coefficients	O
show	O
that	O
there	O
is	O
an	O
entire	O
subspace	O
of	O
solutions	O
and	O
shahshahani	O
problem	O
show	O
that	O
ex	O
and	O
cannot	O
be	O
written	O
in	O
the	O
form	O
x	O
for	O
any	O
finite	O
k	O
where	O
x	O
and	O
ai	O
e	O
i	O
k	O
and	O
shahshahani	O
thus	O
the	O
projection	B
pursuit	I
representation	O
of	O
functions	O
can	O
only	O
at	O
best	O
approximate	O
all	O
continuous	O
functions	O
on	O
bounded	O
sets	O
problem	O
letf	O
be	O
the	O
class	O
of	O
classifiers	O
of	O
the	O
form	O
g	O
where	O
al	O
ii	O
for	O
arbitrary	O
functions	O
and	O
coefficients	O
e	O
r	O
show	O
that	O
for	O
some	O
distribution	O
of	O
y	O
on	O
x	O
infge	O
f	O
lg	O
l	O
so	O
that	O
there	O
is	O
no	O
hope	O
of	O
meaningful	O
distribution-free	O
classification	O
based	O
on	O
additive	O
functions	O
only	O
problems	O
and	O
exercises	O
problem	O
continued	O
repeat	O
the	O
previous	O
exercise	O
for	O
functions	O
of	O
the	O
form	O
ai	O
and	O
distributions	O
of	O
y	O
on	O
x	O
l	O
thus	O
additive	O
functions	O
of	O
pairs	O
do	O
not	O
suffice	O
either	O
problem	O
let	O
k	O
r	O
r	O
be	O
a	O
nonnegative	O
bounded	O
kernel	B
with	O
f	O
k	O
show	O
that	O
for	O
any	O
e	O
any	O
measurable	O
function	O
i	O
rd	O
r	O
and	O
probability	O
measure	O
fj-	O
such	O
that	O
f	O
iixifj-dx	O
there	O
exists	O
a	O
function	O
of	O
the	O
form	O
ci	O
k	O
bit	O
aix	O
bi	O
co	O
such	O
that	O
f	O
iix	O
e	O
poggio	O
and	O
girosi	O
park	O
and	O
sandberg	O
darken	O
donahue	O
gurvits	O
and	O
sontag	O
krzyzak	O
linder	O
and	O
lugosi	O
for	O
such	O
denseness	O
results	O
hint	O
relate	O
the	O
problem	O
to	O
a	O
similar	O
result	O
for	O
kernel	B
estimates	O
problem	O
let	O
ck	O
be	O
the	O
class	O
of	O
classifiers	O
defined	O
by	O
the	O
functions	O
k	O
l	O
ci	O
k	O
bil	O
aix	O
bi	O
co	O
il	O
find	O
upper	O
bounds	O
on	O
its	O
vc	B
dimension	B
when	O
k	O
is	O
an	O
indicator	O
of	O
an	O
interval	O
containing	O
the	O
origin	O
problem	O
consider	O
the	O
class	O
of	O
radial	B
basis	B
function	I
networks	O
where	O
k	O
is	O
nonnegative	O
unimodal	O
bounded	O
and	O
continuous	O
let	O
vr	O
n	O
be	O
a	O
function	O
that	O
minimizes	O
i	O
n-	O
i	O
yi	O
lover	O
e	O
f	O
n	O
and	O
define	O
gn	O
as	O
the	O
corresponding	O
classifier	B
prove	O
that	O
if	O
kn	O
fjn	O
and	O
knfj	O
on	O
as	O
n	O
then	O
gn	O
is	O
universally	O
consistent	O
linder	O
and	O
lugosi	O
hint	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
use	O
problem	O
to	O
handle	O
the	O
approximation	B
error	I
bounding	O
the	O
covering	O
numbers	O
needs	O
a	O
little	O
additional	O
work	O
other	O
error	O
estimates	O
in	O
this	O
chapter	O
we	O
discuss	O
some	O
alternative	O
error	O
estimates	O
that	O
have	O
been	O
troduced	O
to	O
improve	O
on	O
the	O
performance	O
of	O
the	O
standard	B
estimates-holdout	O
substitution	O
and	O
deleted-we	O
have	O
encountered	O
so	O
far	O
the	O
first	O
group	O
of	O
these	O
estimates-smoothed	O
and	O
posterior	B
probability	I
estimates-are	O
used	O
for	O
their	O
small	O
variance	O
however	O
we	O
will	O
give	O
examples	O
that	O
show	O
that	O
classifier	B
selection	I
based	O
on	O
the	O
minimization	O
of	O
these	O
estimates	O
may	O
fail	O
even	O
in	O
the	O
simplest	O
situations	O
among	O
other	O
alternatives	O
we	O
deal	O
briefly	O
with	O
the	O
rich	O
class	O
of	O
bootstrap	B
estimates	O
smoothing	O
the	O
error	O
count	O
the	O
resubstitution	B
deleted	O
and	O
holdout	B
estimates	O
of	O
the	O
error	O
probability	O
chapters	O
and	O
are	O
all	O
based	O
on	O
counting	O
the	O
number	O
of	O
errors	O
committed	O
by	O
the	O
classifier	B
to	O
be	O
tested	O
this	O
is	O
the	O
reason	O
for	O
the	O
relatively	O
large	O
variance	O
inherently	O
present	O
in	O
these	O
estimates	O
this	O
intuition	O
is	O
based	O
on	O
the	O
following	O
most	O
classification	O
rules	O
can	O
be	O
written	O
into	O
the	O
form	O
gn	O
dn	O
otherwise	O
where	O
dn	O
is	O
either	O
an	O
estimate	O
of	O
the	O
a	B
posteriori	I
probability	I
as	O
in	O
the	O
case	O
of	O
histogram	O
kernel	B
or	O
nearest	B
neighbor	I
rules	O
or	O
something	O
else	O
as	O
for	O
generalized	B
linear	O
or	O
neural	B
network	I
classifiers	O
in	O
any	O
case	O
if	O
dn	O
is	O
close	O
to	O
then	O
we	O
feel	O
that	O
the	O
decision	O
is	O
less	O
robust	O
compared	O
to	O
when	O
the	O
value	O
of	O
dn	O
is	O
far	O
from	O
in	O
other	O
words	O
intuitively	O
inverting	O
the	O
value	O
of	O
the	O
other	O
error	O
estimates	O
decision	O
gn	O
at	O
a	O
point	O
x	O
where	O
dn	O
is	O
close	O
to	O
makes	O
less	O
difference	O
in	O
the	O
eltor	O
probability	O
than	O
if	O
dn	O
is	O
large	O
the	O
eltor	O
estimators	O
based	O
on	O
counting	O
the	O
number	O
of	O
eltors	O
do	O
not	O
take	O
the	O
value	O
of	O
into	O
account	O
they	O
eltofs	O
with	O
the	O
same	O
amount	O
no	O
matter	O
what	O
the	O
value	O
of	O
is	O
for	O
example	O
in	O
the	O
case	O
of	O
the	O
resubstitution	B
estimate	O
each	O
eltor	O
contributes	O
with	O
n	O
to	O
the	O
overall	O
count	O
now	O
if	O
dn	O
is	O
close	O
to	O
a	O
small	O
perturbation	O
of	O
xi	O
can	O
flip	O
the	O
decision	O
gnxi	O
and	O
therefore	O
change	O
the	O
value	O
of	O
the	O
estimate	O
by	O
n	O
although	O
the	O
eltor	O
probability	O
of	O
the	O
rule	B
gn	O
probably	O
does	O
not	O
change	O
by	O
this	O
much	O
this	O
phenomenon	O
is	O
what	O
causes	O
the	O
relatively	O
large	O
variance	B
of	I
eltor	O
counting	O
estimators	O
glick	O
proposed	O
a	O
modification	O
of	O
the	O
counting	O
eltor	O
estimates	O
the	O
eral	O
form	O
of	O
his	O
estimate	O
is	O
where	O
r	O
is	O
a	O
monotone	O
increasing	O
function	O
satisfying	O
r	O
u	O
r	O
u	O
possible	O
choices	O
of	O
the	O
smoothing	O
function	O
ru	O
are	O
ru	O
u	O
or	O
ru	O
io	O
cu	O
where	O
the	O
parameter	O
c	O
may	O
be	O
adjusted	O
to	O
improve	O
the	O
behavior	O
of	O
the	O
estimate	O
also	O
glick	O
knoke	O
or	O
tutz	O
both	O
of	O
these	O
estimates	O
give	O
less	O
penalty	O
to	O
eltofs	O
close	O
to	O
the	O
decision	O
boundary	O
that	O
is	O
to	O
eltors	O
where	O
is	O
close	O
to	O
note	O
that	O
taking	O
ru	O
coltesponds	O
to	O
the	O
resubstitution	B
estimate	O
we	O
will	O
see	O
in	O
theorem	O
below	O
that	O
if	O
r	O
is	O
smooth	O
then	O
ls	O
indeed	O
has	O
a	O
very	O
small	O
variance	O
in	O
many	O
situations	O
just	O
like	O
the	O
resubstitution	B
estimate	O
the	O
estimate	O
ls	O
may	O
be	O
strongly	O
mistically	O
biased	O
just	O
consider	O
the	O
i-nearest	O
neighbor	O
rule	B
when	O
ls	O
with	O
probability	O
one	O
whenever	O
x	O
has	O
a	O
density	O
to	O
combat	O
this	O
defect	O
one	O
may	O
define	O
the	O
deleted	O
version	O
of	O
the	O
smoothed	B
estimate	O
where	O
dni	O
is	O
the	O
training	O
sequence	O
with	O
the	O
i-th	O
pair	O
yi	O
deleted	O
the	O
first	O
thing	O
we	O
notice	O
is	O
that	O
this	O
estimate	O
is	O
still	O
biased	O
even	O
asymptotically	O
to	O
illustrate	O
this	O
point	O
consider	O
ru	O
u	O
in	O
this	O
case	O
e	O
e	O
dn-l	O
d	O
n	O
e	O
dn-	O
dn-	O
if	O
the	O
estimate	O
dn-l	O
was	O
perfect	O
that	O
is	O
equal	O
to	O
for	O
every	O
x	O
then	O
the	O
expected	O
value	O
above	O
would	O
be	O
nx	O
which	O
is	O
the	O
asymptotic	O
error	O
probability	O
lnn	O
of	O
the	O
i-nn	B
rule	B
in	O
fact	O
smoothing	O
the	O
error	O
count	O
ie	O
lnni	O
ie	O
d	O
n-	O
dn-	O
dn-	O
this	O
means	O
that	O
when	O
the	O
estimate	O
of	O
the	O
a	B
posteriori	I
probability	I
of	O
is	O
consistent	O
in	O
then	O
lsd	O
converges	O
to	O
l	O
nn	O
and	O
not	O
to	O
l	O
biasedness	O
of	O
an	O
error	O
estimate	O
is	O
not	O
necessarily	O
a	O
bad	O
property	O
in	O
most	O
plications	O
all	O
we	O
care	O
about	O
is	O
how	O
the	O
classification	O
rule	B
selected	O
by	O
minimizing	O
the	O
error	O
estimate	O
works	O
unfortunately	O
in	O
this	O
respect	O
smoothed	B
estimates	O
form	O
poorly	O
even	O
compared	O
to	O
other	O
strongly	O
biased	O
error	O
estimates	O
such	O
as	O
the	O
empirical	B
squared	B
error	I
problem	O
the	O
next	O
example	O
illustrates	O
our	O
point	O
theorem	O
let	O
the	O
distribution	O
of	O
x	O
be	O
concentrated	O
on	O
two	O
values	O
such	O
that	O
px	O
a	O
px	O
b	O
and	O
let	O
and	O
assume	O
that	O
the	O
smoothed	B
error	O
estimate	O
is	O
minimized	O
over	O
e	O
f	O
to	O
select	O
a	O
classifier	B
from	O
f	O
where	O
the	O
class	O
f	O
contains	O
two	O
functions	O
the	O
true	O
a	B
posteriori	I
probability	I
function	O
and	O
ij	O
where	O
ija	O
then	O
the	O
probability	O
that	O
ij	O
is	O
selected	O
converges	O
to	O
one	O
as	O
n	O
proof	O
straightforward	O
calculation	O
shows	O
that	O
the	O
statement	O
follows	O
from	O
the	O
law	O
of	O
large	O
numbers	O
remark	O
the	O
theorem	O
shows	O
that	O
even	O
if	O
the	O
true	O
a	B
posteriori	I
probability	I
function	O
is	O
contained	O
in	O
a	O
finite	O
class	O
of	O
candidates	O
the	O
smoothed	B
estimate	O
with	O
ru	O
u	O
is	O
unable	O
to	O
select	O
a	O
good	O
discrimination	O
rule	B
the	O
result	O
may	O
be	O
extended	O
to	O
general	O
smooth	O
rs	O
as	O
theorems	O
and	O
show	O
empirical	B
squared	B
error	I
minimization	O
or	O
maximum	B
likelihood	I
never	O
fail	O
in	O
this	O
situation	O
finally	O
we	O
demonstrate	O
that	O
if	O
r	O
is	O
smooth	O
then	O
the	O
variance	B
of	I
ls	O
is	O
indeed	O
small	O
our	O
analysis	O
is	O
based	O
on	O
the	O
work	O
of	O
lugosi	O
and	O
pawlak	O
the	O
bounds	O
other	O
error	O
estimates	O
for	O
the	O
variance	B
of	I
ls	O
remain	O
valid	O
for	O
lsd	O
consider	O
classification	O
rules	O
of	O
the	O
form	O
ifrjnx	O
dn	O
gn	O
otherwise	O
where	O
dn	O
is	O
an	O
estimate	O
of	O
the	O
a	B
posteriori	I
probability	I
examples	O
include	O
the	O
histogram	B
rule	B
where	O
chapters	O
and	O
the	O
k-nearest	O
neighbor	O
rule	B
where	O
and	O
or	O
the	O
kernel	B
rule	B
where	O
in	O
the	O
sequel	O
we	O
concentrate	O
on	O
the	O
performance	O
of	O
the	O
smoothed	B
estimate	O
of	O
the	O
error	O
probability	O
of	O
these	O
nonparametric	O
rules	O
the	O
next	O
theorem	O
shows	O
that	O
for	O
these	O
rules	O
the	O
variance	B
of	I
the	O
smoothed	B
error	O
estimate	O
is	O
n	O
no	O
matter	O
what	O
the	O
distribution	O
is	O
this	O
is	O
a	O
significant	O
improvement	O
over	O
the	O
variance	B
of	I
the	O
deleted	O
estimate	O
which	O
as	O
pointed	O
out	O
in	O
chapter	O
can	O
be	O
larger	O
than	O
theorem	O
assume	O
that	O
the	O
smoothing	O
function	O
ru	O
satisfies	O
ru	O
for	O
u	O
e	O
and	O
is	O
uniformly	O
lipschitz	O
continuous	O
that	O
is	O
iru	O
clu	O
vi	O
for	O
all	O
u	O
v	O
and	O
for	O
some	O
constant	O
c	O
then	O
the	O
smoothed	B
estimate	O
ls	O
of	O
the	O
histogram	O
k-nearest	O
neighbor	O
and	O
moving	O
window	O
rules	O
kernel	B
k	O
iso	O
satisfies	O
and	O
varls	O
n	O
c	O
where	O
c	O
is	O
a	O
constant	O
depending	O
on	O
the	O
rule	B
only	O
in	O
the	O
case	O
of	O
the	O
histogram	B
rule	B
the	O
value	O
of	O
c	O
is	O
c	O
for	O
the	O
k-nearest	O
neighbor	O
rule	B
c	O
and	O
for	O
the	O
moving	B
window	I
rule	B
c	O
here	O
c	O
is	O
the	O
constant	O
in	O
the	O
lipschitz	O
condition	O
yd	O
is	O
the	O
minimal	O
number	O
of	O
cones	O
centered	O
at	O
the	O
origin	O
of	O
angle	O
n	O
that	O
coverrd	O
and	O
is	O
the	O
minimal	O
number	O
of	O
balls	O
of	O
radius	O
that	O
cover	O
the	O
unit	O
ball	O
in	O
rd	O
smoothing	O
the	O
error	O
count	O
remark	O
notice	O
that	O
the	O
inequalities	O
of	O
theorem	O
are	O
valid	O
for	O
all	O
n	O
e	O
and	O
h	O
for	O
the	O
histogram	O
and	O
moving	O
window	O
rules	O
and	O
k	O
for	O
the	O
nearest	B
neighbor	I
rules	O
interestingly	O
the	O
constant	O
c	O
does	O
not	O
change	O
with	O
the	O
dimension	B
in	O
the	O
histogram	O
case	O
but	O
grows	O
exponentially	O
with	O
d	O
for	O
the	O
k-nearest	O
neighbor	O
and	O
moving	O
window	O
rules	O
proof	O
the	O
probability	O
inequalities	O
follow	O
from	O
appropriate	O
applications	O
of	O
diarmids	O
inequality	B
the	O
upper	O
bound	O
on	O
the	O
variance	O
follows	O
similarly	O
from	O
theorem	O
we	O
consider	O
each	O
of	O
the	O
three	O
rules	O
in	O
tum	O
the	O
histogram	B
rule	B
let	O
yd	O
yn	O
be	O
a	O
fixed	O
training	O
sequence	O
if	O
we	O
can	O
show	O
that	O
by	O
replacing	O
the	O
value	O
of	O
a	O
pair	O
yi	O
in	O
the	O
training	O
sequence	O
by	O
some	O
y	O
the	O
value	O
of	O
the	O
estimate	O
ls	O
can	O
change	O
by	O
at	O
most	O
then	O
the	O
inequality	B
follows	O
by	O
applying	O
theorem	O
with	O
ci	O
sin	O
the	O
i	O
term	O
of	O
the	O
sum	O
in	O
ls	O
can	O
change	O
by	O
one	O
causing	O
in	O
change	O
in	O
the	O
average	O
obviously	O
all	O
the	O
other	O
terms	O
in	O
the	O
sum	O
that	O
can	O
change	O
are	O
the	O
ones	O
corresponding	O
to	O
the	O
xjs	O
that	O
are	O
in	O
the	O
same	O
set	O
of	O
the	O
partition	B
as	O
either	O
xi	O
or	O
x	O
denoting	O
the	O
number	O
of	O
points	O
in	O
the	O
same	O
set	O
with	O
xi	O
and	O
xi	O
by	O
k	O
and	O
k	O
respectively	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
estimate	O
of	O
the	O
a	O
posteriori	O
probabilities	O
in	O
these	O
points	O
can	O
change	O
by	O
at	O
most	O
k	O
and	O
k	O
respectively	O
it	O
means	O
that	O
the	O
overall	O
change	O
in	O
the	O
value	O
of	O
the	O
sum	O
can	O
not	O
exceed	O
f	O
in	O
in	O
the	O
k-nearest	O
neighbor	O
rule	B
to	O
avoid	O
difficulties	O
caused	O
by	O
breaking	O
distance	O
ties	O
assume	O
that	O
x	O
has	O
a	O
density	O
then	O
recall	O
that	O
the	O
application	O
of	O
lemma	O
for	O
the	O
empirical	B
distribution	O
implies	O
that	O
no	O
x	O
j	O
can	O
be	O
one	O
of	O
the	O
k	O
nearest	O
neighbors	O
of	O
more	O
than	O
kyd	O
points	O
from	O
dn	O
thus	O
changing	O
the	O
value	O
of	O
one	O
pair	O
in	O
the	O
training	O
sequence	O
can	O
change	O
at	O
most	O
terms	O
in	O
the	O
expression	B
of	O
ld	O
one	O
of	O
them	O
by	O
at	O
most	O
and	O
all	O
the	O
others	O
by	O
at	O
most	O
c	O
i	O
k	O
theorem	O
yields	O
the	O
result	O
the	O
moving	B
window	I
rule	B
again	O
we	O
only	O
have	O
to	O
check	O
the	O
condition	O
of	O
theorem	O
with	O
ci	O
fix	O
a	O
training	O
sequence	O
yl	O
yn	O
and	O
replace	O
the	O
pair	O
yi	O
by	O
y	O
then	O
the	O
i	O
term	O
in	O
the	O
sum	O
of	O
the	O
expression	B
of	O
ls	O
can	O
change	O
by	O
at	O
most	O
one	O
clearly	O
the	O
j-th	O
term	O
for	O
which	O
xj	O
fj	O
sxi	O
h	O
and	O
xj	O
fj	O
sx	O
h	O
keeps	O
its	O
value	O
it	O
is	O
easy	O
to	O
see	O
that	O
all	O
the	O
other	O
terms	O
can	O
change	O
by	O
at	O
most	O
c	O
max	O
i	O
k	O
j	O
i	O
kj	O
where	O
k	O
j	O
and	O
kj	O
are	O
the	O
numbers	O
of	O
points	O
xk	O
k	O
i	O
j	O
from	O
the	O
training	O
sequence	O
that	O
fall	O
in	O
sxi	O
h	O
and	O
sx	O
h	O
respectively	O
thus	O
the	O
overall	O
change	O
in	O
the	O
sum	O
does	O
not	O
exceed	O
l	O
k	O
l	O
k	O
xjesxi	O
h	O
xjesx	O
h	O
it	O
suffices	O
to	O
show	O
by	O
symmetry	O
that	O
lxes	O
i	O
j	O
xk	O
e	O
sxih	O
n	O
sxjhl	O
then	O
clearly	O
xih	O
llkj	O
s	O
let	O
nj	O
ixb	O
k	O
lk-sln	O
xjesxih	O
xjesxj	O
h	O
other	O
error	O
estimates	O
to	O
bound	O
the	O
right-hand	O
side	O
from	O
above	O
cover	O
sxjh	O
by	O
fjd	O
balls	O
sl	O
of	O
radius	O
denote	O
the	O
number	O
of	O
points	O
falling	O
in	O
them	O
by	O
lm	O
fjd	O
lm	O
ixk	O
k	O
i	O
i	O
xk	O
e	O
sxjh	O
n	O
sml	O
then	O
and	O
the	O
theorem	O
is	O
proved	O
posterior	B
probability	I
estimates	O
the	O
error	O
estimates	O
discussed	O
in	O
this	O
section	O
improve	O
on	O
the	O
biasedness	O
of	O
smoothed	B
estimates	O
while	O
preserving	O
their	O
small	O
variance	O
still	O
these	O
estimates	O
are	O
of	O
tionable	O
utility	O
in	O
classifier	B
selection	I
considering	O
the	O
formula	O
for	O
the	O
error	O
probability	O
of	O
a	O
classification	O
rule	B
gnx	O
it	O
is	O
plausible	O
to	O
introduce	O
the	O
estimate	O
lp	B
n	O
n	O
l	O
dn	O
n	O
il	O
dn	O
that	O
is	O
the	O
expected	O
value	O
is	O
estimated	O
by	O
a	O
sample	O
average	O
and	O
instead	O
of	O
the	O
a	B
posteriori	I
probability	I
yjx	O
its	O
estimate	O
dn	O
is	O
plugged	O
into	O
the	O
formula	O
of	O
ln	O
the	O
estimate	O
l	O
is	O
usually	O
called	O
the	O
posterior	B
probability	I
error	O
estimate	O
in	O
the	O
case	O
of	O
nonparametric	O
rules	O
such	O
as	O
histogram	O
kernel	B
and	O
k-nn	B
rules	O
it	O
is	O
natural	O
to	O
use	O
the	O
corresponding	O
nonparametric	O
estimates	O
of	O
the	O
a	O
riori	O
probabilities	O
for	O
plugging	O
in	O
the	O
expression	B
of	O
the	O
error	O
probability	O
this	O
and	O
similar	O
estimates	O
of	O
ln	O
have	O
been	O
introduced	O
and	O
studied	O
by	O
fukunaga	O
and	O
kessel	O
rora	O
and	O
wilcox	O
fitzmaurice	O
and	O
rand	O
ganesalingam	O
and	O
mclachlan	O
kittler	O
and	O
devijver	O
matloff	O
and	O
pruitt	O
moore	O
whitsitt	O
and	O
landgrebe	O
pawlak	O
schwemer	O
and	O
dunn	O
and	O
lugosi	O
and	O
pawlak	O
it	O
is	O
interesting	O
to	O
notice	O
the	O
similarity	O
between	O
the	O
estimates	O
ls	O
and	O
lp	B
although	O
they	O
were	O
developed	O
from	O
different	O
scenarios	O
to	O
reduce	O
the	O
bias	B
we	O
can	O
use	O
the	O
leave-one-out	B
deleted	O
version	O
of	O
the	O
estimate	O
lpd	O
n	O
n	O
l	O
dnj	O
n	O
il	O
dni	O
the	O
deleted	O
version	O
lp	B
d	O
has	O
a	O
much	O
better	O
bias	B
than	O
lsd	O
we	O
have	O
the	O
bound	O
posterior	B
probability	I
estimates	O
ielpd	O
ie	O
dn-	O
dn-d	O
this	O
means	O
that	O
if	O
the	O
estimate	O
of	O
the	O
a	B
posteriori	I
probability	I
of	O
is	O
sistent	O
in	O
l	O
then	O
elp	O
d	O
converges	O
to	O
l	O
this	O
is	O
the	O
case	O
for	O
all	O
distributions	O
for	O
the	O
histogram	O
and	O
moving	O
window	O
rules	O
if	O
h	O
and	O
nh	O
d	O
and	O
the	O
k-nearest	O
neighbor	O
rule	B
if	O
k	O
and	O
k	O
n	O
as	O
it	O
is	O
seen	O
in	O
chapters	O
and	O
for	O
specific	O
cases	O
it	O
is	O
possible	O
to	O
obtain	O
sharper	O
bounds	O
on	O
the	O
bias	B
of	I
lp	B
d	O
for	O
the	O
histogram	B
rule	B
lugosi	O
and	O
pawlak	O
carried	O
out	O
such	O
analysis	O
they	O
showed	O
for	O
example	O
that	O
the	O
estimate	O
lpd	O
is	O
optimistically	O
biased	O
problem	O
posterior	B
probability	I
estimates	O
of	O
ln	O
share	O
the	O
good	O
stability	O
properties	O
of	O
smoothed	B
estimates	O
finally	O
let	O
us	O
select	O
a	O
function	O
nd	O
a	O
corresponding	O
rule	B
gx	O
ih	O
a	O
class	O
f	O
based	O
on	O
the	O
minimization	O
of	O
the	O
posterior	B
probability	I
error	O
estimate	O
observe	O
that	O
when	O
e	O
i	O
for	O
all	O
x	O
that	O
is	O
rule	B
selection	O
based	O
on	O
this	O
estimate	O
just	O
does	O
not	O
make	O
sense	O
the	O
reason	O
is	O
that	O
lp	B
ignores	O
the	O
yis	O
of	O
the	O
data	O
sequence	O
fukunaga	O
and	O
kessel	O
argued	O
that	O
efficient	O
posterior	B
probability	I
tors	O
can	O
be	O
obtained	O
if	O
additional	O
unclassified	O
observations	O
are	O
available	O
very	O
often	O
in	O
practice	O
in	O
addition	O
to	O
the	O
training	O
sequence	O
dn	O
further	O
feature	O
vectors	O
xnz	O
are	O
given	O
without	O
their	O
labels	O
ynz	O
where	O
the	O
xni	O
are	O
i	O
i	O
d	O
independent	O
from	O
x	O
and	O
dn	O
and	O
they	O
have	O
the	O
same	O
distribution	O
as	O
that	O
of	O
x	O
this	O
situation	O
is	O
typical	O
in	O
medical	O
applications	O
when	O
large	O
sets	O
of	O
medical	O
records	O
are	O
available	O
but	O
it	O
is	O
usually	O
very	O
expensive	O
to	O
get	O
their	O
correct	O
diagnosis	O
these	O
unclassified	O
samples	O
can	O
be	O
efficiently	O
used	O
for	O
testing	O
the	O
performance	O
of	O
a	O
classifier	B
designed	O
from	O
dn	O
by	O
using	O
the	O
estimate	O
z	O
i	O
l	O
dn	O
il	O
dn	O
again	O
using	O
l	O
for	O
rule	B
selection	O
is	O
meaningless	O
other	O
error	O
estimates	O
rotation	B
estimate	O
this	O
method	O
suggested	O
by	O
toussaint	O
and	O
donaldson	O
is	O
a	O
combination	O
of	O
the	O
holdout	B
and	O
deleted	O
estimates	O
it	O
is	O
sometimes	O
called	O
the	O
n	O
let	O
s	O
n	O
be	O
a	O
positive	O
integer	O
much	O
smaller	O
than	O
n	O
and	O
assume	O
for	O
the	O
sake	O
of	O
simplicity	O
that	O
q	O
n	O
s	O
is	O
an	O
integer	O
the	O
rotation	B
method	O
forms	O
the	O
holdout	B
estimate	O
by	O
holding	O
the	O
first	O
s	O
pairs	O
of	O
the	O
training	O
data	O
out	O
then	O
the	O
second	O
s	O
pairs	O
and	O
so	O
forth	O
the	O
estimate	O
is	O
defined	O
by	O
averaging	O
the	O
q	O
numbers	O
obtained	O
this	O
way	O
to	O
formalize	O
denote	O
by	O
dj	O
the	O
training	O
data	O
with	O
the	O
j-th	O
s-block	O
held	O
out	O
q	O
dj	O
yd	O
ysj-l	O
ysl	O
yn	O
the	O
estimate	O
is	O
defined	O
by	O
q	O
l	O
l	O
l	O
i	O
s	O
n	O
s	O
q	O
s	O
isj-ll	O
s	O
yields	O
the	O
deleted	O
estimate	O
if	O
s	O
then	O
the	O
estimate	O
is	O
usually	O
more	O
biased	O
than	O
the	O
deleted	O
estimate	O
as	O
but	O
usually	O
exhibits	O
smaller	O
variance	O
elj	O
eln-	O
s	O
bootstrap	B
bootstrap	B
methods	O
for	O
estimating	O
the	O
error	O
became	O
popular	O
lowing	O
the	O
revolutionary	O
work	O
of	O
efron	O
all	O
bootstrap	B
estimates	O
introduce	O
artificial	O
randomization	O
the	O
bootstrap	B
sample	O
d	O
yb	O
in	O
ycb	O
in	O
in	O
is	O
a	O
sequence	O
of	O
random	O
variable	B
pairs	O
drawn	O
randomly	O
with	O
replacement	O
from	O
the	O
set	O
yd	O
yn	O
in	O
other	O
words	O
conditionally	O
on	O
the	O
training	O
sample	O
dn	O
yi	O
yn	O
the	O
pairs	O
y	O
are	O
drawn	O
independently	O
from	O
vn	O
the	O
empirical	B
distribution	O
based	O
on	O
dn	O
in	O
n	O
d	O
x	O
i	O
one	O
of	O
the	O
standard	B
bootstrap	B
estimates	O
aims	O
to	O
compensate	O
the	O
mistic	O
bias	B
bgn	O
elgn	O
l	O
gn	O
of	O
the	O
resubstitution	B
estimate	O
lr	O
to	O
estimate	O
bgn	O
a	O
bootstrap	B
sample	O
of	O
size	O
m	O
n	O
may	O
be	O
used	O
often	O
bootstrap	B
sampling	O
is	O
repeated	O
several	O
times	O
to	O
average	O
out	O
effects	O
of	O
the	O
additional	O
randomization	O
in	O
our	O
case	O
bootstrap	B
yields	O
the	O
estimator	O
lbgn	O
lrgn	O
bngn	O
another	O
instance	O
of	O
a	O
bootstrap	B
sample	O
of	O
size	O
n	O
the	O
so-called	O
eo	B
estimator	I
uses	O
resubstitution	B
on	O
the	O
training	O
pairs	O
not	O
appearing	O
in	O
the	O
bootstrap	B
sample	O
the	O
estimator	O
is	O
defined	O
by	O
here	O
too	O
averages	O
may	O
be	O
taken	O
after	O
generating	O
the	O
bootstrap	B
sample	O
several	O
times	O
many	O
other	O
versions	O
of	O
bootstrap	B
estimates	O
have	O
been	O
reported	O
such	O
as	O
the	O
estimate	O
bootstrap	B
and	O
bootstrap	B
hand	O
jain	O
dubes	O
and	O
chen	O
and	O
mclachlan	O
for	O
surveys	O
and	O
ditional	O
references	O
clearly	O
none	O
of	O
these	O
estimates	O
provides	O
a	O
universal	O
remedy	O
but	O
for	O
several	O
specific	O
classification	O
rules	O
bootstrap	B
estimates	O
have	O
been	O
mentally	O
found	O
to	O
outperform	O
the	O
deleted	O
and	O
resubstitution	B
estimates	O
however	O
one	O
point	O
has	O
to	O
be	O
made	O
clear	O
we	O
always	O
lose	O
information	O
with	O
the	O
additional	O
randomization	O
we	O
summarize	O
this	O
in	O
the	O
following	O
simple	O
general	O
result	O
theorem	O
let	O
x	O
i	O
xn	O
be	O
drawn	O
from	O
an	O
unknown	O
distribution	O
jl	O
and	O
let	O
afl	O
be	O
afunctional	O
to	O
be	O
estimated	O
let	O
r	O
be	O
a	O
convex	O
risk	O
function	O
as	O
ru	O
or	O
ru	O
lui	O
let	O
xib	O
xi	O
be	O
a	O
bootstrap	B
sample	O
drawn	O
from	O
xl	O
x	O
n	O
then	O
the	O
theorem	O
states	O
that	O
no	O
matter	O
how	O
large	O
m	O
is	O
the	O
class	O
of	O
estimators	O
that	O
are	O
functions	O
of	O
the	O
original	O
sample	O
is	O
always	O
at	O
least	O
as	O
good	O
as	O
the	O
class	O
of	O
all	O
estimators	O
that	O
are	O
based	O
upon	O
bootstrap	B
samples	O
in	O
our	O
case	O
ajl	O
plays	O
the	O
role	O
of	O
the	O
expected	O
error	O
probability	O
eln	O
pgnx	O
y	O
if	O
we	O
take	O
ru	O
then	O
it	O
follows	O
from	O
the	O
theorem	O
that	O
there	O
is	O
no	O
estimator	O
ln	O
based	O
on	O
the	O
bootstrap	B
sample	O
d	O
whose	O
squared	B
error	I
e	O
is	O
smaller	O
than	O
that	O
of	O
some	O
nonbootstrap	O
estimate	O
in	O
the	O
proof	O
of	O
the	O
theorem	O
we	O
construct	O
such	O
a	O
non-bootstrap	O
estimator	O
it	O
is	O
clear	O
however	O
that	O
in	O
general	O
the	O
latter	O
estimator	O
is	O
too	O
complex	O
to	O
have	O
any	O
practical	O
value	O
the	O
randomization	O
of	O
bootstrap	B
methods	O
may	O
provide	O
a	O
useful	O
tool	O
to	O
overcome	O
the	O
computational	O
difficulties	O
in	O
finding	O
good	O
estimators	O
other	O
error	O
estimates	O
proof	O
let	O
be	O
any	O
mapping	O
taking	O
m	O
arguments	O
then	O
e	O
x	O
ajl	O
lxi	O
xn	O
m	O
n	O
i	O
imci	O
xi	O
ajl	O
jensens	O
inequality	B
and	O
the	O
convexity	O
of	O
r	O
r	O
i	O
r	O
xn	O
ajl	O
n	O
xij	O
ajl	O
where	O
now	O
after	O
taking	O
expectations	O
with	O
respect	O
to	O
xl	O
x	O
n	O
we	O
see	O
that	O
for	O
every	O
we	O
start	O
out	O
with	O
there	O
is	O
a	O
that	O
is	O
at	O
least	O
as	O
good	O
i	O
xm	O
ym	O
if	O
m	O
then	O
the	O
bootstrap	B
has	O
an	O
additional	O
problem	O
related	O
to	O
the	O
coupon	O
collector	O
problem	O
let	O
n	O
be	O
the	O
number	O
of	O
different	O
pairs	O
in	O
the	O
bootstrap	B
sample	O
yb	O
en	O
m	O
en	O
lor	O
some	O
constant	O
c	O
t	O
en	O
n	O
n	O
e-c	O
with	O
probability	O
one	O
to	O
see	O
this	O
note	O
that	O
lcn	O
thf	O
ne-c	O
h	O
en	O
n	O
n	O
so	O
e	O
n	O
n	O
e	O
furthermore	O
if	O
one	O
of	O
the	O
m	O
drawings	O
is	O
varied	O
n	O
changes	O
by	O
at	O
most	O
one	O
hence	O
by	O
mcdiarmids	O
inequality	B
for	O
e	O
from	O
which	O
we	O
conclude	O
that	O
n	O
n	O
e-c	O
with	O
probability	O
one	O
as	O
n	O
e-c	O
of	O
the	O
original	O
data	O
pairs	O
do	O
not	O
appear	O
in	O
the	O
bootstrap	B
sample	O
a	O
considerable	O
loss	O
of	O
information	O
takes	O
place	O
that	O
will	O
be	O
reflected	O
in	O
the	O
performance	O
this	O
phenomenon	O
is	O
well-known	O
and	O
motivated	O
several	O
modifications	O
of	O
the	O
simplest	O
bootstrap	B
estimate	O
for	O
more	O
information	O
see	O
the	O
surveys	O
by	O
hand	O
jain	O
dubes	O
and	O
chen	O
and	O
mclachlan	O
problems	O
and	O
exercises	O
problems	O
and	O
exercises	O
problem	O
show	O
that	O
the	O
posterior	B
probability	I
estimate	O
lp	B
of	O
the	O
histogram	O
k-nearest	O
neighbor	O
and	O
moving	O
window	O
rules	O
satisfies	O
and	O
varl	O
n	O
where	O
c	O
is	O
a	O
constant	O
depending	O
on	O
the	O
rule	B
in	O
the	O
case	O
of	O
the	O
histogram	B
rule	B
the	O
value	O
of	O
c	O
is	O
c	O
for	O
the	O
k-nearest	O
neighbor	O
rule	B
c	O
and	O
for	O
the	O
moving	B
window	I
rule	B
c	O
also	O
show	O
that	O
the	O
deleted	O
version	O
lp	B
d	O
of	O
the	O
estimate	O
satisfies	O
the	O
same	O
inequalities	O
and	O
pawlak	O
hint	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
problem	O
show	O
that	O
the	O
deleted	O
posterior	B
probability	I
estimate	O
of	O
the	O
error	O
probability	O
of	O
a	O
histogram	B
rule	B
is	O
always	O
optimistically	O
biased	O
that	O
is	O
for	O
all	O
n	O
and	O
all	O
distributions	O
e	O
el	O
n-	O
l	O
problem	O
show	O
that	O
for	O
any	O
classification	O
rule	B
and	O
any	O
estimate	O
tjnx	O
dn	O
of	O
the	O
a	O
posteriori	O
probabilities	O
for	O
all	O
distributions	O
of	O
y	O
for	O
alli	O
n	O
and	O
e	O
p	O
il	O
e	O
i	O
e	O
i	O
dn	O
and	O
var	O
i	O
further	O
show	O
that	O
for	O
alii	O
e	O
e	O
problem	O
empirical	B
squared	B
error	I
consider	O
the	O
deleted	O
empirical	B
squared	B
error	I
show	O
that	O
een	O
e	O
dn-d	O
lnn	O
where	O
lnn	O
is	O
the	O
asymptotic	O
error	O
probability	O
on	O
the	O
i-nearest	O
neighbor	O
rule	B
show	O
that	O
if	O
tjn-j	O
is	O
the	O
histogram	O
kernel	B
or	O
k-nn	B
estimate	O
then	O
varen	O
cn	O
for	O
some	O
constant	O
depending	O
on	O
the	O
dimension	B
only	O
we	O
see	O
that	O
en	O
is	O
an	O
asymptotically	O
optimistically	O
biased	O
estimate	O
of	O
lgn	O
when	O
tjn-j	O
is	O
an	O
l	O
estimate	O
of	O
tj	O
still	O
this	O
estimate	O
is	O
useful	O
in	O
classifier	B
selection	I
theorem	O
feature	B
extraction	I
dimensionality	O
reduction	O
so	O
far	O
we	O
have	O
not	O
addressed	O
the	O
question	O
of	O
how	O
the	O
components	O
of	O
the	O
feature	O
vector	O
x	O
are	O
obtained	O
in	O
general	O
these	O
components	O
are	O
based	O
on	O
d	O
measurements	O
of	O
the	O
object	O
to	O
be	O
classified	O
how	O
many	O
measurements	O
should	O
be	O
made	O
what	O
should	O
these	O
measurements	O
be	O
we	O
study	O
these	O
questions	O
in	O
this	O
chapter	O
general	O
recipes	O
are	O
hard	O
to	O
give	O
as	O
the	O
answers	O
depend	O
on	O
the	O
specific	O
problem	O
however	O
there	O
are	O
some	O
rules	O
of	O
thumb	O
that	O
should	O
be	O
followed	O
one	O
such	O
rule	B
is	O
that	O
noisy	O
measurements	O
that	O
is	O
components	O
that	O
are	O
independent	O
of	O
y	O
should	O
be	O
avoided	O
also	O
adding	O
a	O
component	O
that	O
is	O
a	O
function	O
of	O
other	O
components	O
is	O
useless	O
a	O
essary	O
and	O
sufficient	O
condition	O
for	O
measurements	O
providing	O
additional	O
information	O
is	O
given	O
in	O
problem	O
our	O
goal	O
of	O
course	O
is	O
to	O
make	O
the	O
error	O
probability	O
lgn	O
as	O
small	O
as	O
possible	O
this	O
depends	O
on	O
many	O
things	O
such	O
as	O
the	O
joint	O
distribution	O
of	O
the	O
selected	O
nents	O
and	O
the	O
label	O
y	O
the	O
sample	O
size	O
and	O
the	O
classification	O
rule	B
gn	O
to	O
make	O
things	O
a	O
bit	O
simpler	O
we	O
first	O
investigate	O
the	O
bayes	O
errors	O
corresponding	O
to	O
the	O
selected	O
components	O
this	O
approach	O
makes	O
sense	O
since	O
the	O
bayes	B
error	I
is	O
the	O
theoretical	B
limit	O
of	O
the	O
performance	O
of	O
any	O
classifier	B
as	O
problem	O
indicates	O
collecting	O
more	O
measurements	O
cannot	O
increase	O
the	O
bayes	B
error	I
on	O
the	O
other	O
hand	O
having	O
too	O
many	O
components	O
is	O
not	O
desirable	O
just	O
recall	O
the	O
curse	B
of	I
dimensionality	I
that	O
we	O
often	O
faced	O
to	O
get	O
good	O
error	O
rates	O
the	O
number	O
of	O
training	O
samples	O
should	O
be	O
exponentially	O
large	O
in	O
the	O
number	O
of	O
components	O
also	O
computational	O
and	O
storage	O
limitations	O
may	O
prohibit	O
us	O
from	O
working	O
with	O
many	O
components	O
we	O
may	O
formulate	O
the	O
feature	O
selection	O
problem	O
as	O
follows	O
let	O
xl	O
xed	O
feature	B
extraction	I
be	O
random	O
variables	O
representing	O
d	O
measurements	O
for	O
a	O
set	O
a	O
of	O
indices	O
let	O
x	O
a	O
denote	O
the	O
i	O
a	O
i-dimensional	O
random	O
vector	O
whose	O
components	O
are	O
the	O
xis	O
with	O
i	O
e	O
a	O
the	O
order	O
of	O
increasing	O
indices	O
define	O
l	O
inf	O
grjai--o	O
i	O
pgxa	O
y	O
as	O
the	O
bayes	B
error	I
corresponding	O
to	O
the	O
pair	O
y	O
figure	O
a	O
possible	O
rangement	O
of	O
las	O
for	O
d	O
l	O
o	O
obviously	O
l	O
l	O
whenever	O
b	O
c	O
a	O
and	O
l	O
minpy	O
oj	O
py	O
i	O
the	O
problem	O
is	O
to	O
find	O
an	O
efficient	O
way	O
of	O
selecting	O
an	O
index	O
set	O
a	O
with	O
iai	O
k	O
whose	O
corresponding	O
bayes	B
error	I
is	O
the	O
smallest	O
here	O
k	O
d	O
is	O
a	O
fixed	O
integer	O
exhaustive	O
search	O
through	O
the	O
possibilities	O
is	O
often	O
sirable	O
because	O
of	O
the	O
imposed	O
computational	O
burden	O
many	O
attempts	O
have	O
been	O
made	O
to	O
find	O
fast	O
algorithms	O
to	O
obtain	O
the	O
best	O
subset	O
of	O
features	O
see	O
fu	O
min	O
and	O
li	O
kanal	O
ben-bassat	O
and	O
devijver	O
and	O
kittler	O
for	O
surveys	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
best	O
k	O
individual	O
features-that	O
is	O
components	O
corresponding	O
to	O
the	O
k	O
smallest	O
values	O
of	O
l	O
i	O
not	O
necessarily	O
constitute	O
the	O
best	O
k-dimensional	O
vector	O
just	O
consider	O
a	O
case	O
in	O
which	O
xl	O
xk	O
cover	O
and	O
van	O
campenhout	O
showed	O
that	O
any	O
ordering	O
of	O
the	O
subsets	O
of	O
d	O
consistent	O
with	O
the	O
obvious	O
requirement	O
l	O
l	O
if	O
b	O
a	O
is	O
possible	O
more	O
precisely	O
they	O
proved	O
the	O
following	O
surprising	O
result	O
theorem	O
and	O
van	O
campenhout	O
let	O
ai	O
a	O
be	O
an	O
ordering	O
of	O
the	O
subsets	O
of	O
satisfying	O
the	O
consistency	B
property	O
i	O
j	O
if	O
ai	O
c	O
a	O
j	O
al	O
and	O
d	O
then	O
there	O
exists	O
a	O
distribution	O
of	O
the	O
random	O
variables	O
y	O
xed	O
y	O
such	O
that	O
the	O
theorem	O
shows	O
that	O
every	O
feature	O
selection	O
algorithm	B
that	O
finds	O
the	O
best	O
element	O
subset	O
has	O
to	O
search	O
exhaustively	O
through	O
all	O
k-element	O
subsets	O
for	O
some	O
distributions	O
any	O
other	O
method	O
is	O
doomed	O
to	O
failure	O
for	O
some	O
distribution	O
many	O
suboptimal	O
heuristic	O
algorithms	O
have	O
been	O
introduced	O
trying	O
to	O
avoid	O
the	O
tational	O
demand	O
of	O
exhaustive	O
search	O
e	O
g	O
sebestyen	O
meisel	O
dimensionality	O
reduction	O
chang	O
vilmansen	O
and	O
devijver	O
and	O
kittler	O
narendra	O
and	O
fukunaga	O
introduced	O
an	O
efficient	O
branch-and-bound	B
method	I
that	O
finds	O
the	O
optimal	O
set	O
of	O
features	O
their	O
method	O
avoids	O
searching	O
through	O
all	O
subsets	O
in	O
many	O
cases	O
by	O
making	O
use	O
of	O
the	O
monotonicity	O
of	O
the	O
bayes	B
error	I
with	O
respect	O
to	O
the	O
partial	O
ordering	O
of	O
the	O
subsets	O
the	O
key	O
of	O
our	O
proof	O
ofthe	O
theorem	O
is	O
the	O
following	O
simple	O
lemma	O
lemma	O
let	O
ai	O
be	O
as	O
in	O
theorem	O
let	O
i	O
assume	O
that	O
the	O
distribution	O
of	O
y	O
on	O
nd	O
x	O
i	O
is	O
such	O
that	O
the	O
distribution	O
of	O
x	O
is	O
concentrated	O
on	O
a	O
finite	O
set	O
l	O
l	O
and	O
l	O
j	O
for	O
each	O
i	O
then	O
there	O
exists	O
another	O
finite	O
distribution	O
such	O
that	O
l	O
j	O
remains	O
unchanged	O
for	O
each	O
i	O
and	O
la	O
j	O
foreachi	O
proof	O
we	O
denote	O
the	O
original	O
distribution	O
of	O
x	O
by	O
and	O
the	O
a	B
posteriori	I
probability	I
function	O
by	O
fj	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
every	O
atom	O
of	O
the	O
distribution	O
of	O
x	O
is	O
in	O
md	O
for	O
some	O
m	O
since	O
l	O
d	O
the	O
value	O
of	O
fjx	O
is	O
either	O
zero	O
or	O
one	O
at	O
each	O
atom	O
we	O
construct	O
the	O
new	O
distribution	O
by	O
duplicating	O
each	O
atom	O
in	O
a	O
special	O
way	O
we	O
describe	O
the	O
new	O
distribution	O
by	O
defining	O
a	O
measure	O
on	O
nd	O
and	O
an	O
a	O
posteriori	O
function	O
fj	O
nd	O
i	O
define	O
the	O
vector	O
vai	O
e	O
nd	O
such	O
that	O
its	O
m-th	O
component	O
equals	O
m	O
if	O
m	O
ai	O
and	O
zero	O
if	O
m	O
e	O
ai	O
the	O
new	O
measure	O
has	O
twice	O
as	O
many	O
atoms	O
as	O
for	O
each	O
atom	O
x	O
e	O
nd	O
of	O
the	O
new	O
measure	O
has	O
two	O
atoms	O
namely	O
xl	O
x	O
and	O
x	O
va	O
i	O
the	O
new	O
distribution	O
is	O
specified	O
by	O
q	O
q	O
fj	O
fjx	O
and	O
fj	O
fjx	O
where	O
q	O
e	O
is	O
specified	O
later	O
it	O
remains	O
for	O
us	O
to	O
verify	O
that	O
this	O
distribution	O
satisfies	O
the	O
requirements	O
of	O
the	O
lemma	O
for	O
some	O
q	O
first	O
observe	O
that	O
the	O
values	O
l	O
j	O
remain	O
unchanged	O
for	O
all	O
i	O
this	O
follows	O
from	O
the	O
fact	O
that	O
there	O
is	O
at	O
least	O
one	O
component	O
in	O
a	O
j	O
along	O
which	O
the	O
new	O
set	O
of	O
atoms	O
is	O
strictly	O
separated	O
from	O
the	O
old	O
one	O
leaving	O
the	O
corresponding	O
contribution	O
to	O
the	O
bayes	B
error	I
unchanged	O
on	O
the	O
other	O
hand	O
as	O
we	O
vary	O
q	O
from	O
zero	O
to	O
the	O
new	O
value	O
of	O
l	O
grows	O
continuously	O
from	O
the	O
old	O
value	O
of	O
l	O
to	O
therefore	O
since	O
by	O
assumption	O
max	O
ji	O
l	O
j	O
there	O
exists	O
a	O
value	O
of	O
q	O
such	O
that	O
the	O
new	O
l	O
satisfies	O
maxji	O
l	O
j	O
l	O
as	O
desired	O
proof	O
of	O
theorem	O
we	O
construct	O
the	O
desired	O
distribution	O
in	O
steps	O
applying	O
lemma	O
in	O
each	O
step	O
the	O
procedure	O
for	O
d	O
is	O
illustrated	O
in	O
figure	O
feature	B
extraction	I
cd	O
ag	O
cd	O
cd	O
cd	O
cd	O
cd	O
i	O
i	O
i	O
i	O
cd	O
figure	O
construction	O
of	O
a	O
prespecijied	O
ordering	O
in	O
this	O
three-dimensional	O
example	O
the	O
first	O
four	O
steps	O
of	O
the	O
procedure	O
are	O
shown	O
when	O
the	O
desired	O
ordering	O
is	O
black	O
circles	O
represent	O
atoms	O
with	O
and	O
white	O
circles	O
are	O
those	O
with	O
o	O
we	O
start	O
with	O
a	O
monoatomic	O
distribution	O
concentrated	O
at	O
the	O
origin	O
with	O
then	O
clearly	O
l	O
o	O
by	O
lemma	O
we	O
construct	O
a	O
distribution	O
such	O
that	O
by	O
applying	O
the	O
lemma	O
again	O
we	O
can	O
construct	O
a	O
distribution	O
with	O
l	O
l	O
l	O
after	O
i	O
steps	O
we	O
have	O
a	O
distribution	O
satisfying	O
the	O
last	O
i	O
inequalities	O
of	O
the	O
desired	O
ordering	O
the	O
construction	O
is	O
finished	O
after	O
steps	O
remark	O
the	O
original	O
example	O
of	O
cover	O
and	O
van	O
campenhout	O
uses	O
the	O
multidimensional	O
gaussian	B
distribution	O
van	O
campenhout	O
developed	O
the	O
idea	O
further	O
by	O
showing	O
that	O
not	O
only	O
all	O
possible	O
orderings	O
but	O
all	O
possible	O
values	O
of	O
the	O
l	O
can	O
be	O
achieved	O
by	O
some	O
distributions	O
the	O
distribution	O
constructed	O
in	O
the	O
above	O
proof	O
is	O
discrete	O
it	O
has	O
atoms	O
one	O
may	O
suspect	O
that	O
feature	B
extraction	I
is	O
much	O
easier	O
if	O
given	O
y	O
the	O
ponents	O
xd	O
are	O
conditionally	O
independent	O
however	O
three	O
and	O
dimensional	O
examples	O
given	O
by	O
elashoff	O
elashoff	O
and	O
goldman	O
toussaint	O
and	O
cover	O
show	O
that	O
even	O
the	O
individually	O
best	O
two	O
independent	O
components	O
are	O
not	O
the	O
best	O
pair	O
of	O
components	O
we	O
do	O
not	O
know	O
if	O
theorem	O
dimensionality	O
reduction	O
generalizes	O
to	O
the	O
case	O
when	O
the	O
components	O
are	O
conditionally	O
independent	O
in	O
the	O
next	O
example	O
the	O
pair	O
of	O
components	O
consisting	O
of	O
the	O
two	O
worst	O
single	O
features	O
is	O
the	O
best	O
pair	O
and	O
vice	O
versa	O
theorem	O
there	O
exist	O
binary-valued	O
random	O
variables	O
xl	O
x	O
y	O
e	O
i	O
such	O
that	O
xl	O
x	O
and	O
are	O
conditionally	O
independent	O
y	O
and	O
l	O
l	O
l	O
but	O
l	O
l	O
l	O
proof	O
let	O
py	O
i	O
then	O
the	O
joint	O
distribution	O
of	O
xl	O
y	O
is	O
ified	O
by	O
the	O
conditional	O
probabilities	O
pxi	O
o	O
and	O
pxi	O
lly	O
i	O
straightforward	O
calculation	O
shows	O
that	O
the	O
values	O
pxi	O
lly	O
o	O
iiy	O
o	O
iiy	O
o	O
pxi	O
iiy	O
i	O
iiy	O
iiy	O
i	O
satisfy	O
the	O
stated	O
inequalities	O
as	O
our	O
ultimate	O
goal	O
is	O
to	O
minimize	O
the	O
error	O
probability	O
finding	O
the	O
feature	O
set	O
minimizing	O
the	O
bayes	B
error	I
is	O
not	O
the	O
best	O
we	O
can	O
do	O
for	O
example	O
we	O
know	O
that	O
we	O
will	O
use	O
the	O
neighbor	O
rule	B
then	O
it	O
makes	O
more	O
sense	O
to	O
select	O
the	O
set	O
of	O
features	O
that	O
minimizes	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
neighbor	O
rule	B
recall	O
from	O
chapter	O
that	O
e	O
the	O
situation	O
here	O
is	O
even	O
messier	O
than	O
for	O
the	O
bayes	B
error	I
as	O
the	O
next	O
example	O
shows	O
it	O
is	O
not	O
even	O
true	O
that	O
a	O
c	O
b	O
implies	O
where	O
a	O
b	O
are	O
two	O
subsets	O
of	O
components	O
and	O
denotes	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
neighbor	O
rule	B
for	O
y	O
in	O
other	O
words	O
adding	O
components	O
may	O
increase	O
this	O
can	O
never	O
happen	O
to	O
the	O
bayes	O
error-and	O
in	O
fact	O
to	O
any	O
f	O
the	O
anomaly	O
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
function	O
x	O
is	O
convex	O
near	O
zero	O
and	O
one	O
example	O
let	O
the	O
joint	O
distribution	O
of	O
x	O
be	O
uniform	O
on	O
the	O
joint	O
distribution	O
of	O
y	O
is	O
defined	O
by	O
the	O
a	O
posteriori	O
probabilities	O
given	O
by	O
if	O
x	O
e	O
x	O
if	O
x	O
e	O
x	O
if	O
x	O
e	O
x	O
if	O
x	O
e	O
x	O
x	O
feature	B
extraction	I
straightforward	O
calculations	O
show	O
that	O
while	O
a	O
smaller	O
value	O
figure	O
anexamplewhen	O
an	O
additionalfeature	O
increases	O
the	O
error	O
probability	O
of	O
the	O
rule	B
lnn	O
of	O
course	O
the	O
real	O
measure	O
of	O
the	O
goodness	O
of	O
the	O
selected	O
feature	O
set	O
is	O
the	O
error	O
probability	O
lgn	O
of	O
the	O
classifier	B
designed	O
by	O
using	O
training	O
data	O
dn	O
if	O
the	O
classification	O
rule	B
gn	O
is	O
not	O
known	O
at	O
the	O
stage	O
of	O
feature	O
selection	O
then	O
the	O
best	O
one	O
can	O
do	O
is	O
to	O
estimate	O
the	O
bayes	O
errors	O
l	O
for	O
each	O
set	O
a	O
of	O
features	O
and	O
select	O
a	O
feature	O
set	O
by	O
minimizing	O
the	O
estimate	O
unfortunately	O
as	O
theorem	O
shows	O
no	O
method	O
of	O
estimating	O
the	O
bayes	O
errors	O
can	O
guarantee	O
good	O
performance	O
if	O
we	O
know	O
what	O
classifier	B
will	O
be	O
used	O
after	O
feature	O
selection	O
then	O
the	O
best	O
strategy	O
is	O
to	O
select	O
a	O
set	O
of	O
measurements	O
based	O
on	O
comparing	O
estimates	O
of	O
the	O
error	O
probabilities	O
we	O
do	O
not	O
pursue	O
this	O
question	O
further	O
for	O
special	O
cases	O
we	O
do	O
not	O
need	O
to	O
mount	O
a	O
big	O
search	O
for	O
the	O
best	O
features	O
here	O
is	O
a	O
simple	O
example	O
given	O
y	O
i	O
let	O
x	O
xed	O
have	O
d	O
independent	O
components	O
where	O
given	O
y	O
i	O
xj	O
is	O
normal	B
ji	O
a	O
it	O
is	O
easy	O
to	O
verify	O
that	O
if	O
p	O
i	O
then	O
lpn	O
where	O
n	O
is	O
a	O
standard	B
normal	B
random	O
variable	B
and	O
l	O
jl	O
cyj	O
is	O
the	O
square	O
of	O
the	O
mahalanobis	B
distance	I
also	O
duda	O
and	O
hart	O
pp	O
for	O
this	O
case	O
the	O
quality	O
of	O
the	O
j-th	O
feature	O
is	O
measured	O
by	O
we	O
may	O
as	O
well	O
rank	O
these	O
values	O
and	O
given	O
that	O
we	O
need	O
only	O
d	O
d	O
features	O
we	O
are	O
best	O
off	O
taking	O
the	O
d	O
features	O
with	O
the	O
highest	O
quality	O
index	O
transformations	O
with	O
small	O
distortion	O
it	O
is	O
possible	O
to	O
come	O
up	O
with	O
analytic	O
solutions	O
in	O
other	O
special	O
cases	O
as	O
well	O
for	O
example	O
raudys	O
and	O
raudys	O
and	O
pikelis	O
investigated	O
the	O
pendence	O
of	O
e	O
lgn	O
on	O
the	O
dimension	B
of	O
the	O
feature	O
space	O
in	O
the	O
case	O
of	O
certain	O
linear	O
classifiers	O
and	O
normal	B
distributions	O
they	O
point	O
out	O
that	O
for	O
a	O
fixed	O
n	O
by	O
increasing	O
the	O
number	O
of	O
features	O
the	O
expected	O
error	O
probability	O
elgn	O
first	O
decreases	O
and	O
then	O
after	O
reaching	O
an	O
optimum	O
grows	O
again	O
transformations	O
with	O
small	O
distortion	O
one	O
may	O
view	O
the	O
problem	O
of	O
feature	B
extraction	I
in	O
general	O
as	O
the	O
problem	O
of	O
ing	O
a	O
transformation	O
a	O
function	O
t	O
nd	O
nd	O
so	O
that	O
the	O
bayes	B
error	I
l	O
corresponding	O
to	O
the	O
pair	O
y	O
is	O
close	O
to	O
the	O
bayes	B
error	I
l	O
corresponding	O
to	O
the	O
pair	O
y	O
one	O
typical	O
example	O
of	O
such	O
transformations	O
is	O
fine	O
quantization	B
discretization	O
of	O
x	O
when	O
t	O
maps	O
the	O
observed	O
values	O
into	O
a	O
set	O
of	O
finitely	O
or	O
countably	O
infinitely	O
many	O
values	O
reduction	B
of	I
dimensionality	O
of	O
the	O
tions	O
can	O
be	O
put	O
in	O
this	O
framework	O
as	O
well	O
in	O
the	O
following	O
result	O
we	O
show	O
that	O
small	O
distortion	O
of	O
the	O
observation	O
cannot	O
cause	O
large	O
increase	O
in	O
the	O
bayes	B
error	I
probability	O
theorem	O
and	O
gyorfi	O
assume	O
that	O
for	O
a	O
sequence	O
of	O
transformations	O
tn	O
n	O
in	O
probability	O
where	O
ii	O
ii	O
denotes	O
the	O
euclidean	B
norm	I
in	O
nd	O
then	O
if	O
l	O
is	O
the	O
bayes	B
error	I
for	O
y	O
and	O
ln	O
is	O
the	O
bayes	B
error	I
for	O
y	O
l	O
l	O
t	O
proof	O
for	O
arbitrarily	O
small	O
e	O
we	O
can	O
choose	O
a	O
uniformly	O
continuous	O
function	O
o	O
ijx	O
such	O
that	O
ijxi	O
e	O
for	O
any	O
transformation	O
t	O
consider	O
now	O
the	O
decision	O
problem	O
of	O
y	O
where	O
the	O
random	O
variable	B
y	O
satisfies	O
py	O
x	O
x	O
ijx	O
for	O
every	O
x	O
e	O
nd	O
denote	O
the	O
corresponding	O
bayes	B
error	I
by	O
i	O
and	O
the	O
bayes	B
error	I
corresponding	O
to	O
the	O
pair	O
y	O
by	O
i	O
obviously	O
tn	O
t	O
tn	O
tn	O
to	O
bound	O
the	O
first	O
and	O
third	O
terms	O
on	O
the	O
right-hand	O
side	O
observe	O
that	O
for	O
any	O
transformation	O
t	O
py	O
tx	O
e	O
tx	O
e	O
tx	O
feature	B
extraction	I
therefore	O
the	O
bayes	B
error	I
corresponding	O
to	O
the	O
pair	O
y	O
equals	O
l	O
e	O
e	O
thus	O
il	O
ie	O
tx	O
e	O
tx	O
e	O
e	O
e	O
e	O
e	O
ryxi	O
jensens	O
inequality	B
e	O
so	O
the	O
first	O
and	O
third	O
terms	O
of	O
are	O
less	O
than	O
e	O
for	O
the	O
second	O
term	O
define	O
the	O
decision	O
function	O
gn	O
if	O
ryx	O
otherwise	O
which	O
has	O
error	O
probability	O
ign	O
pgntnx	O
y	O
then	O
ign	O
ii	O
and	O
we	O
have	O
by	O
theorem	O
all	O
we	O
have	O
to	O
show	O
now	O
is	O
that	O
the	O
limit	O
supremum	O
of	O
the	O
above	O
quantity	O
does	O
not	O
exceed	O
e	O
let	O
o	O
e	O
be	O
the	O
inverse	O
modulus	O
of	O
continuity	O
of	O
the	O
a	B
posteriori	I
probability	I
ry	O
that	O
is	O
oe	O
sup	O
yll	O
for	O
every	O
e	O
we	O
have	O
oe	O
by	O
the	O
uniform	O
continuity	O
of	O
ry	O
now	O
we	O
have	O
irytnx	O
ryxi	O
clearly	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
converges	O
to	O
zero	O
by	O
assumption	O
while	O
the	O
second	O
term	O
does	O
not	O
exceed	O
e	O
by	O
the	O
definition	B
of	I
oe	O
remark	O
it	O
is	O
clear	O
from	O
the	O
proof	O
of	O
theorem	O
that	O
everything	O
remains	O
true	O
if	O
the	O
observation	O
x	O
takes	O
its	O
values	O
from	O
a	O
separable	O
metric	O
space	O
with	O
metric	O
p	O
and	O
the	O
condition	O
of	O
the	O
theorem	O
is	O
modified	O
to	O
ptx	O
x	O
in	O
probability	O
this	O
generalization	O
has	O
significance	O
in	O
curve	O
recognition	O
when	O
x	O
is	O
a	O
stochastic	O
process	O
then	O
theorem	O
asserts	O
that	O
one	O
does	O
not	O
lose	O
much	O
information	O
by	O
using	O
usual	O
discretization	O
methods	O
such	O
as	O
for	O
example	O
karhunen-loeve	O
series	O
expansion	O
problems	O
to	O
admissible	O
and	O
sufficient	O
transformations	O
admissible	O
and	O
sufficient	O
transformations	O
sometimes	O
the	O
cost	O
of	O
guessing	O
zero	O
while	O
the	O
true	O
value	O
of	O
y	O
is	O
one	O
is	O
different	O
from	O
the	O
cost	O
of	O
guessing	O
one	O
while	O
y	O
o	O
these	O
situations	O
may	O
be	O
handled	O
as	O
follows	O
define	O
the	O
costs	O
crn	O
i	O
rn	O
l	O
here	O
cy	O
gx	O
is	O
the	O
cost	O
of	O
deciding	O
on	O
gx	O
when	O
the	O
true	O
label	O
is	O
y	O
the	O
risk	O
of	O
a	O
decision	O
function	O
g	O
is	O
defined	O
as	O
the	O
expected	O
value	O
of	O
the	O
cost	O
note	O
that	O
if	O
rg	O
ecy	O
gx	O
l	O
if	O
rn	O
otherwise	O
c	O
rn	O
then	O
the	O
risk	O
is	O
just	O
the	O
probability	O
of	O
error	O
introduce	O
the	O
notation	O
qmx	O
rn	O
co	O
rn	O
rn	O
then	O
we	O
have	O
the	O
following	O
extension	O
of	O
theorem	O
theorem	O
define	O
gx	O
if	O
ql	O
qox	O
otherwise	O
then	O
for	O
all	O
decision	O
functions	O
g	O
we	O
have	O
rg	O
rg	O
rg	O
is	O
called	O
the	O
bayes	O
risk	O
the	O
proof	O
is	O
left	O
as	O
an	O
exercise	O
problem	O
which	O
transformations	O
preserve	O
all	O
the	O
necessary	O
information	O
in	O
the	O
sense	O
that	O
the	O
bayes	B
error	I
probability	O
corresponding	O
to	O
the	O
pair	O
y	O
equals	O
that	O
of	O
y	O
clearly	O
every	O
invertible	O
mapping	O
t	O
has	O
this	O
property	O
however	O
the	O
practically	O
interesting	O
transformations	O
are	O
the	O
ones	O
that	O
provide	O
some	O
compression	O
of	O
the	O
data	O
the	O
most	O
efficient	O
of	O
such	O
transformations	O
is	O
the	O
bayes	O
decision	O
g	O
itself	O
g	O
is	O
specifically	O
designed	O
to	O
minimize	O
the	O
error	O
probability	O
if	O
the	O
goal	O
is	O
to	O
minimize	O
the	O
bayes	O
risk	O
with	O
respect	O
to	O
some	O
other	O
cost	O
function	O
than	O
the	O
error	O
probability	O
then	O
g	O
generally	O
fails	O
to	O
preserve	O
the	O
bayes	O
risk	O
it	O
is	O
natural	O
to	O
ask	O
what	O
transformations	O
preserve	O
the	O
bayes	O
risk	O
for	O
all	O
possible	O
cost	O
functions	O
this	O
question	O
has	O
a	O
practical	O
significance	O
when	O
collecting	O
data	O
and	O
construction	O
of	O
the	O
decision	O
are	O
separated	O
in	O
space	O
or	O
in	O
time	O
in	O
such	O
cases	O
the	O
data	O
should	O
be	O
transmitted	O
via	O
a	O
communication	O
channel	B
should	O
be	O
stored	O
in	O
both	O
cases	O
there	O
is	O
a	O
need	O
for	O
an	O
efficient	O
data	O
compression	O
rule	B
in	O
this	O
problem	O
formulation	O
when	O
getting	O
the	O
data	O
one	O
may	O
not	O
know	O
the	O
final	O
cost	O
function	O
therefore	O
a	O
desirable	O
data	O
compression	O
does	O
not	O
increase	O
the	O
bayes	O
risk	O
for	O
any	O
cost	O
function	O
c-	O
here	O
t	O
is	O
a	O
measurable	O
function	O
mapping	O
from	O
rd	O
to	O
rd	O
for	O
some	O
positive	O
integer	O
d	O
feature	B
extraction	I
definition	O
let	O
r	O
t	O
denote	O
the	O
bayes	O
riskfor	O
the	O
costfunction	O
c	O
and	O
formed	O
observation	O
tx	O
a	O
transformation	O
t	O
is	O
called	O
admissible	O
iffor	O
any	O
cost	O
function	O
c	O
rt	O
r	O
where	O
r	O
denotes	O
the	O
bayes	O
risk	O
for	O
the	O
original	O
observation	O
obviously	O
each	O
invertible	O
transformation	O
t	O
is	O
admissible	O
a	O
nontrivial	O
example	O
of	O
an	O
admissible	B
transformation	I
is	O
tx	O
since	O
according	O
to	O
theorem	O
the	O
bayes	O
decision	O
for	O
any	O
cost	O
function	O
can	O
be	O
constructed	O
by	O
the	O
a	B
posteriori	I
probability	I
and	O
by	O
the	O
cost	O
function	O
ingly	O
this	O
is	O
basically	O
the	O
only	O
such	O
transformation	O
in	O
the	O
following	O
sense	O
theorem	O
a	O
transformation	O
t	O
is	O
admissible	O
if	O
and	O
only	O
if	O
there	O
is	O
a	O
mapping	O
g	O
such	O
that	O
gtx	O
with	O
probability	O
one	O
proof	O
the	O
converse	O
is	O
easy	O
since	O
for	O
such	O
g	O
r	O
rtx	O
rgtx	O
r	O
assume	O
now	O
that	O
t	O
is	O
admissible	O
but	O
such	O
function	O
g	O
does	O
not	O
exist	O
then	O
there	O
is	O
a	O
set	O
a	O
c	O
nd	O
such	O
that	O
fla	O
tx	O
is	O
constant	O
on	O
a	O
while	O
all	O
values	O
then	O
there	O
are	O
real	O
numbers	O
c	O
and	O
e	O
and	O
sets	O
b	O
dca	O
such	O
that	O
of	O
are	O
different	O
on	O
a	O
that	O
is	O
if	O
x	O
yea	O
then	O
x	O
y	O
implies	O
flb	O
fld	O
and	O
c	O
e	O
if	O
x	O
e	O
b	O
c-e	O
if	O
xed	O
now	O
choose	O
a	O
cost	O
function	O
with	O
the	O
following	O
values	O
coo	O
cii	O
coi	O
and	O
then	O
cio	O
c	O
c	O
qox	O
qix	O
and	O
the	O
bayes	O
decision	O
on	O
bud	O
is	O
given	O
by	O
if	O
c	O
otherwise	O
g	O
admissible	O
and	O
sufficient	O
transformations	O
now	O
let	O
get	O
be	O
an	O
arbitrary	O
decision	O
without	O
loss	O
of	O
generality	O
we	O
can	O
assume	O
that	O
gtx	O
if	O
x	O
e	O
a	O
then	O
the	O
difference	O
between	O
the	O
risk	O
of	O
gtx	O
and	O
the	O
bayes	O
risk	O
is	O
jrd	O
r	O
qgxx	O
mdx	O
i	O
qlx	O
mdx	O
i	O
mdx	O
i	O
cx	O
jldx	O
o	O
d	O
we	O
can	O
give	O
another	O
characterization	O
of	O
admissible	O
transformations	O
by	O
virtue	O
of	O
a	O
well-known	O
concept	O
of	O
mathematical	O
statistics	O
definition	O
tx	O
is	O
called	O
a	O
sufficient	O
statistic	O
if	O
the	O
random	O
variables	O
x	O
tx	O
and	O
y	O
form	O
a	O
markov	O
chain	O
in	O
this	O
order	O
that	O
is	O
for	O
any	O
set	O
a	O
py	O
e	O
aitx	O
x	O
pry	O
e	O
aitx	O
theorem	O
a	O
transformation	O
t	O
is	O
admissible	O
if	O
and	O
only	O
ift	O
is	O
a	O
sufficient	O
statistic	O
proof	O
assume	O
that	O
t	O
is	O
admissible	O
then	O
according	O
to	O
theorem	O
there	O
is	O
a	O
mapping	O
g	O
such	O
that	O
then	O
and	O
gtx	O
with	O
probability	O
one	O
py	O
iitx	O
x	O
py	O
llx	O
gtx	O
py	O
iitx	O
epy	O
litx	O
xitx	O
e	O
itx	O
gtx	O
thus	O
py	O
iitx	O
x	O
py	O
litx	O
therefore	O
tx	O
is	O
sufficient	O
on	O
the	O
other	O
hand	O
if	O
tx	O
is	O
sufficient	O
then	O
py	O
llx	O
py	O
iitx	O
x	O
py	O
iitx	O
feature	B
extraction	I
so	O
for	O
the	O
choice	O
gtx	O
py	O
iitx	O
we	O
have	O
the	O
desired	O
function	O
g	O
and	O
therefore	O
t	O
is	O
admissible	O
theorem	O
states	O
that	O
we	O
may	O
replace	O
x	O
by	O
any	O
sufficient	O
statistic	O
tx	O
without	O
altering	O
the	O
bayes	B
error	I
the	O
problem	O
with	O
this	O
in	O
practice	O
is	O
that	O
we	O
do	O
not	O
know	O
the	O
sufficient	B
statistics	I
because	O
we	O
do	O
not	O
know	O
the	O
distribution	O
if	O
the	O
distribution	O
of	O
y	O
is	O
known	O
to	O
some	O
extent	O
then	O
theorem	O
may	O
be	O
useful	O
example	O
assume	O
that	O
it	O
is	O
known	O
that	O
e-cllxll	O
for	O
some	O
unknown	O
c	O
o	O
then	O
ii	O
x	O
ii	O
is	O
a	O
sufficient	O
statistic	O
thus	O
for	O
discrimination	O
we	O
may	O
replace	O
the	O
d-dimensional	O
vector	O
x	O
by	O
the	O
i-dimensional	O
random	O
variable	B
iixii	O
without	O
loss	O
of	O
discrimination	O
power	O
example	O
if	O
for	O
some	O
function	O
then	O
is	O
a	O
sufficient	O
statistic	O
for	O
discrimination	O
there	O
is	O
no	O
need	O
to	O
deal	O
with	O
xl	O
it	O
suffices	O
to	O
extract	O
the	O
features	O
x	O
x	O
and	O
x	O
example	O
if	O
given	O
y	O
i	O
x	O
is	O
normal	B
with	O
unknown	O
mean	O
mi	O
and	O
diagonal	O
covariance	O
matrix	O
a	O
i	O
unknown	O
a	O
then	O
is	O
a	O
function	O
of	O
iix	O
ii	O
x	O
mo	O
only	O
for	O
unknown	O
mo	O
mi	O
here	O
we	O
have	O
no	O
obvious	O
sufficient	O
statistic	O
however	O
if	O
mo	O
and	O
m	O
are	O
both	O
known	O
then	O
a	O
quick	O
inspection	O
shows	O
that	O
x	O
t	O
mo	O
is	O
a	O
i-dimensional	O
sufficient	O
statistic	O
again	O
it	O
suffices	O
to	O
look	O
for	O
the	O
simplest	O
possible	O
argument	O
for	O
if	O
mo	O
m	O
but	O
the	O
covariance	O
matrices	O
are	O
ag	O
i	O
and	O
al	O
i	O
given	O
that	O
y	O
or	O
y	O
then	O
iixii	O
is	O
a	O
sufficient	O
statistic	O
in	O
summary	O
the	O
results	O
of	O
this	O
section	O
are	O
useful	O
for	O
picking	O
out	O
features	O
when	O
some	O
theoretical	B
information	O
is	O
available	O
regarding	O
the	O
distribution	O
of	O
y	O
problems	O
and	O
exercises	O
consider	O
the	O
pair	O
y	O
e	O
rdxo	O
lofrandomvariablesandletxcdl	O
e	O
r	O
be	O
an	O
additional	O
component	O
define	O
the	O
augmented	O
random	O
vector	O
x	O
x	O
cdl	O
denote	O
the	O
bayes	O
errors	O
corresponding	O
to	O
the	O
pairs	O
y	O
and	O
y	O
by	O
l	O
and	O
l	O
respectively	O
clearly	O
l	O
l	O
prove	O
that	O
equality	O
holds	O
if	O
and	O
only	O
if	O
p	O
itcx	O
where	O
the	O
a	B
posteriori	I
probability	I
functions	O
r	O
rd	O
i	O
and	O
r	O
r	O
i	O
are	O
defined	O
as	O
rx	O
py	O
llx	O
x	O
and	O
rx	O
py	O
llx	O
x	O
hint	O
consult	O
with	O
the	O
proof	O
of	O
theorem	O
problem	O
letpy	O
i	O
ii	O
xcd	O
have	O
d	O
independent	O
components	O
where	O
xj	O
is	O
normal	B
ji	O
a	O
prove	O
that	O
l	O
p	O
n	O
where	O
n	O
is	O
a	O
standard	B
normal	B
random	O
variable	B
and	O
is	O
the	O
square	O
of	O
the	O
mahalanobis	B
distance	I
ll	O
jl	O
m	O
a	O
and	O
hart	O
pp	O
problems	O
and	O
exercises	O
problem	O
sampling	B
of	I
a	O
stochastic	O
process	O
let	O
xt	O
t	O
e	O
be	O
a	O
tic	O
process	O
a	O
collection	O
of	O
real-valued	O
random	O
variables	O
indexed	O
by	O
t	O
and	O
let	O
y	O
be	O
a	O
binary	B
random	O
variable	B
for	O
integer	O
n	O
define	O
xw	O
x	O
n	O
i	O
n	O
find	O
sufficient	O
conditions	O
on	O
the	O
function	O
met	O
and	O
on	O
the	O
covariance	O
function	O
kt	O
s	O
ext	O
extxs	O
exs	O
such	O
that	O
lim	O
l	O
l	O
n--oo	O
where	O
lx	O
is	O
the	O
bayes	B
error	I
corresponding	O
to	O
xf	O
and	O
lx	O
is	O
the	O
infimum	O
of	O
the	O
error	O
probabilities	O
of	O
decision	O
functions	O
that	O
map	O
measurable	O
functions	O
into	O
i	O
hint	O
introduce	O
the	O
stochastic	O
process	O
xnt	O
as	O
the	O
linear	O
interpolation	O
of	O
xl	O
xn	O
find	O
conditions	O
under	O
which	O
lim	O
e	O
n--oo	O
and	O
use	O
theorem	O
and	O
the	O
remark	O
following	O
it	O
problem	O
expansion	B
of	I
a	O
stochastic	O
process	O
let	O
xt	O
met	O
and	O
kt	O
s	O
be	O
as	O
in	O
the	O
previous	O
problem	O
let	O
be	O
a	O
complete	B
orthonormal	I
system	O
of	O
functions	O
on	O
define	O
find	O
conditions	O
under	O
which	O
lim	O
l	O
xn	O
l	O
n--oo	O
problem	O
extend	O
theorem	O
such	O
that	O
the	O
transformations	O
tnx	O
dj	O
are	O
allowed	O
to	O
depend	O
on	O
the	O
training	O
data	O
this	O
extension	O
has	O
significance	O
because	O
feature	B
extraction	I
algorithms	O
use	O
the	O
training	O
data	O
problem	O
for	O
discrete	O
x	O
prove	O
that	O
tx	O
is	O
sufficient	O
iff	O
y	O
tx	O
x	O
form	O
a	O
markov	O
chain	O
this	O
order	O
problem	O
prove	O
theorem	O
problem	O
recall	O
the	O
definition	B
of	I
f	O
from	O
chapter	O
let	O
f	O
be	O
a	O
strictly	O
cave	O
function	O
show	O
that	O
dfx	O
y	O
dftx	O
y	O
if	O
and	O
only	O
if	O
the	O
transformation	O
t	O
is	O
admissible	O
conclude	O
that	O
lnnx	O
y	O
lnntx	O
y	O
construct	O
a	O
t	O
and	O
a	O
distribution	O
of	O
y	O
such	O
that	O
l	O
y	O
l	O
y	O
but	O
t	O
is	O
not	O
admissible	O
problem	O
find	O
sufficient	B
statistics	I
of	O
minimal	O
dimension	B
for	O
the	O
following	O
nation	O
problems	O
it	O
is	O
known	O
that	O
for	O
two	O
given	O
sets	O
a	O
and	O
b	O
with	O
a	O
n	O
b	O
if	O
y	O
we	O
have	O
x	O
e	O
a	O
and	O
if	O
y	O
then	O
x	O
e	O
b	O
or	O
vice	O
versa	O
given	O
y	O
x	O
is	O
a	O
vector	O
of	O
independent	O
gamma	B
random	O
variables	O
with	O
common	O
unknown	O
shape	O
parameter	O
a	O
and	O
common	O
unknown	O
scale	O
parameter	O
b	O
the	O
marginal	O
density	O
of	O
each	O
component	O
of	O
x	O
is	O
of	O
the	O
form	O
xa-je-xjb	O
a	O
x	O
the	O
parameters	O
a	O
and	O
b	O
depend	O
upon	O
y	O
problem	O
let	O
x	O
have	O
support	B
on	O
the	O
surface	O
of	O
a	O
ball	O
of	O
n	O
d	O
centered	O
at	O
the	O
origin	O
of	O
unknown	O
radius	O
find	O
a	O
sufficient	O
statistic	O
for	O
discrimination	O
of	O
dimension	B
smaller	O
than	O
d	O
feature	B
extraction	I
problem	O
assume	O
that	O
the	O
distribution	O
of	O
x	O
is	O
such	O
that	O
x	O
and	O
xl	O
with	O
probability	O
one	O
find	O
a	O
simple	O
sufficient	O
statistic	O
appendix	O
in	O
this	O
appendix	O
we	O
summarize	O
some	O
basic	O
definitions	O
and	O
results	O
from	O
the	O
theory	O
of	O
probability	O
most	O
proofs	O
are	O
omitted	O
as	O
they	O
may	O
be	O
found	O
in	O
standard	B
textbooks	O
on	O
probability	O
such	O
as	O
ash	O
shiryayev	O
chow	O
and	O
teicher	O
durrett	O
grimmett	O
and	O
stirzaker	O
and	O
zygmund	O
we	O
also	O
give	O
a	O
list	O
of	O
useful	O
inequalities	O
that	O
are	O
used	O
in	O
the	O
text	O
a	O
i	O
basics	O
of	O
measure	O
theory	O
definition	O
a	O
l	O
let	O
s	O
be	O
a	O
set	O
and	O
let	O
f	O
be	O
a	O
family	B
of	I
subsets	O
of	O
s	O
f	O
is	O
called	O
a	O
a	O
if	O
e	O
f	O
a	O
e	O
f	O
ai	O
e	O
f	O
implies	O
ul	O
ai	O
e	O
f	O
implies	O
a	O
c	O
e	O
f	O
a	O
a	O
is	O
closed	O
under	O
complement	O
and	O
union	O
of	O
countably	O
infinitely	O
many	O
sets	O
conditions	O
and	O
imply	O
that	O
s	O
e	O
f	O
moreover	O
and	O
imply	O
that	O
a	O
a-algebra	O
is	O
closed	O
under	O
countably	O
infinite	O
intersections	O
definition	O
let	O
s	O
be	O
a	O
set	O
and	O
let	O
f	O
be	O
a	O
a	O
of	O
subsets	O
of	O
s	O
then	O
is	O
called	O
a	O
measurable	O
space	O
the	O
elements	O
of	O
f	O
are	O
called	O
measurable	O
sets	O
definition	O
if	O
s	O
nd	O
is	O
the	O
smallest	O
a-algebra	O
containing	O
all	O
angles	O
is	O
called	O
the	O
borel	O
a-algebra	O
the	O
elements	O
are	O
called	O
borel	O
sets	O
appendix	O
definition	O
aa	O
let	O
f	O
be	O
a	O
measurable	O
space	O
and	O
let	O
f	O
function	O
f	O
is	O
called	O
measurable	O
if	O
for	O
all	O
b	O
e	O
b	O
s	O
n	O
be	O
a	O
f-lb	O
fs	O
e	O
b	O
e	O
f	O
that	O
is	O
the	O
inverse	O
image	O
of	O
any	O
borel	O
set	O
b	O
is	O
in	O
f	O
obviously	O
if	O
a	O
is	O
a	O
measurable	O
set	O
then	O
the	O
indicator	O
variable	B
ia	O
is	O
a	O
able	O
function	O
moreover	O
finite	O
linear	O
combinations	O
of	O
indicators	O
of	O
measurable	O
sets	O
simple	O
functions	O
are	O
also	O
measurable	O
functions	O
it	O
can	O
be	O
shown	O
that	O
the	O
set	O
of	O
measurable	O
functions	O
is	O
closed	O
under	O
addition	O
subtraction	O
multiplication	O
and	O
division	O
moreover	O
the	O
supremum	O
and	O
infimum	O
of	O
a	O
sequence	O
of	O
measurable	O
functions	O
as	O
well	O
as	O
its	O
pointwise	O
limit	O
supremum	O
and	O
limit	O
infimum	O
are	O
able	O
definition	O
let	O
f	O
be	O
a	O
measurable	O
space	O
and	O
let	O
v	O
f	O
be	O
a	O
function	O
v	O
is	O
a	O
measure	O
on	O
f	O
if	O
v	O
is	O
that	O
is	O
ai	O
e	O
f	O
and	O
ai	O
n	O
aj	O
i	O
j	O
imply	O
that	O
vulai	O
vai	O
in	O
other	O
words	O
a	O
measure	O
is	O
a	O
nonnegative	O
set	O
function	O
definition	O
v	O
is	O
a	O
finite	O
measure	O
if	O
vs	O
v	O
is	O
a	O
measure	O
if	O
there	O
are	O
countably	O
many	O
measurable	O
sets	O
ai	O
a	O
such	O
that	O
ul	O
ai	O
sand	O
vai	O
i	O
definition	O
the	O
triple	O
f	O
v	O
is	O
a	O
measure	O
space	O
ifs	O
f	O
is	O
a	O
measurable	O
space	O
and	O
v	O
is	O
a	O
measure	O
on	O
f	O
definition	O
the	O
lebesgue	O
measure	O
a	O
on	O
nd	O
is	O
a	O
measure	O
on	O
the	O
borel	O
ofnd	O
such	O
that	O
the	O
a	O
measure	O
of	O
each	O
rectangle	O
equals	O
to	O
its	O
volume	O
the	O
lebesgue	O
integral	O
definition	O
let	O
f	O
v	O
be	O
a	O
measure	O
space	O
and	O
f	O
xjai	O
a	O
simple	O
function	O
such	O
that	O
the	O
measurable	O
sets	O
ai	O
an	O
are	O
disjoint	O
then	O
the	O
integral	O
of	O
f	O
with	O
respect	O
to	O
v	O
is	O
defined	O
by	O
f	O
fdv	O
is	O
fsvds	O
txivai	O
if	O
f	O
is	O
a	O
nonnegative-valued	O
measurable	O
function	O
then	O
introduce	O
a	O
sequence	O
of	O
simple	O
functions	O
as	O
follows	O
us	O
ln	O
fs	O
kin	O
k	O
if	O
if	O
fs	O
the	O
lebesgue	O
integral	O
then	O
the	O
fns	O
are	O
simple	O
functions	O
and	O
fns	O
fs	O
in	O
a	O
monotone	O
decreasing	O
fashion	O
therefore	O
the	O
sequence	O
of	O
integrals	O
f	O
fndv	O
is	O
monotone	O
decreasing	O
with	O
a	O
limit	O
the	O
integral	O
f	O
is	O
then	O
defined	O
by	O
f	O
fdv	O
fsvds	O
lim	O
f	O
fn	O
dv	O
j	O
s	O
n---oo	O
if	O
f	O
is	O
an	O
arbitrary	O
measurable	O
function	O
then	O
decompose	O
it	O
as	O
a	O
difference	O
of	O
its	O
positive	O
and	O
negative	O
parts	O
fs	O
fest	O
fs-	O
fest	O
fst	O
where	O
f	O
and	O
f-	O
are	O
both	O
nonnegative	O
functions	O
define	O
the	O
integral	O
of	O
f	O
by	O
if	O
at	O
least	O
one	O
term	O
on	O
the	O
right-hand	O
side	O
is	O
finite	O
then	O
we	O
say	O
that	O
the	O
integral	O
exists	O
if	O
the	O
integral	O
is	O
finite	O
then	O
f	O
is	O
integrable	O
definition	O
a	O
io	O
iff	O
jdv	O
exists	O
and	O
a	O
is	O
a	O
measurablefunjinthen	O
fa	O
fdv	O
is	O
defined	O
by	O
c	O
i	O
fdv	O
f	O
fjadv	O
definition	O
a	O
ii	O
we	O
say	O
that	O
fn	O
f	O
v	O
if	O
v	O
i	O
fs	O
o	O
theorem	O
a	O
i	O
theorem	O
if	O
fns	O
fs	O
in	O
a	O
monotone	O
ing	O
way	O
for	O
some	O
nonnegative	O
integrable	O
function	O
f	O
then	O
f	O
lim	O
fn	O
dv	O
lim	O
f	O
fn	O
dv	O
iz---oo	O
theorem	O
dominated	B
convergence	I
theorem	I
assume	O
that	O
gsfors	O
e	O
s	O
n	O
where	O
f	O
gdv	O
fn	O
f	O
then	O
f	O
lim	O
h	O
lim	O
f	O
dv	O
theorem	O
lemma	O
let	O
fl	O
be	O
measurable	O
functions	O
if	O
there	O
exists	O
a	O
measurable	O
function	O
g	O
with	O
f	O
gdv	O
such	O
thatjor	O
every	O
n	O
hi	O
gs	O
then	O
lim	O
inf	O
f	O
hi	O
d	O
v	O
f	O
lim	O
inf	O
d	O
v	O
n---oo	O
iz---oo	O
appendix	O
if	O
there	O
is	O
a	O
a	O
measurable	O
function	O
g	O
with	O
j	O
gdv	O
such	O
that	O
fns	O
gs	O
for	O
every	O
n	O
then	O
lim	O
sup	O
f	O
fn	O
dv	O
f	O
lim	O
sup	O
fn	O
dv	O
n---oo	O
n---oo	O
definition	O
let	O
vi	O
and	O
be	O
measures	O
on	O
a	O
measurable	O
space	O
f	O
we	O
say	O
that	O
vi	O
is	O
absolutely	O
continuous	O
with	O
respect	O
to	O
if	O
and	O
only	O
if	O
implies	O
vi	O
e	O
f	O
we	O
denote	O
this	O
relation	O
by	O
vi	O
theorem	O
theorem	O
let	O
vi	O
and	O
be	O
measures	O
on	O
the	O
measurable	O
space	O
f	O
such	O
that	O
vi	O
and	O
is	O
a-finite	O
then	O
there	O
exists	O
a	O
measurable	O
function	O
f	O
such	O
that	O
for	O
all	O
a	O
e	O
f	O
via	O
f	O
f	O
is	O
unique	O
if	O
vi	O
is	O
finite	O
then	O
f	O
has	O
a	O
finite	O
integral	O
definition	O
f	O
is	O
called	O
the	O
density	O
or	O
radon-nikodym	B
derivative	I
of	O
vi	O
with	O
respect	O
to	O
we	O
use	O
the	O
notation	O
f	O
definition	O
let	O
vi	O
and	O
be	O
measures	O
on	O
a	O
measurable	O
space	O
f	O
if	O
and	O
then	O
vi	O
is	O
singular	O
there	O
exists	O
a	O
set	O
a	O
e	O
f	O
such	O
that	O
via	O
c	O
with	O
respect	O
to	O
vice	O
versa	O
theorem	O
a	O
s	O
decomposition	O
theorem	O
if	O
fj	O
is	O
a	O
a-finite	O
measure	O
on	O
a	O
measurable	O
space	O
f	O
then	O
there	O
exist	O
two	O
unique	O
measures	O
vi	O
such	O
that	O
fjv	O
vi	O
where	O
vi	O
fj	O
and	O
is	O
singular	O
with	O
respect	O
to	O
fj	O
definition	O
a	O
is	O
let	O
f	O
v	O
be	O
a	O
measure	O
space	O
and	O
let	O
f	O
be	O
a	O
measurable	O
function	O
then	O
f	O
induces	O
a	O
measure	O
fj	O
on	O
the	O
borel	O
a	O
as	O
follows	O
fj	O
b	O
vf-ib	O
b	O
e	O
b	O
theorem	O
let	O
v	O
be	O
a	O
measure	O
on	O
the	O
borel	O
a	O
b	O
ofr	O
and	O
let	O
f	O
and	O
g	O
be	O
measurable	O
functions	O
then	O
for	O
all	O
b	O
e	O
b	O
r	O
gxfj	O
dx	O
r	O
ib	O
if-icb	O
gfsvds	O
where	O
fjv	O
is	O
induced	O
by	O
f	O
definition	O
let	O
vi	O
and	O
be	O
measures	O
on	O
the	O
measurable	O
spaces	O
fd	O
and	O
f	O
respectively	O
let	O
f	O
be	O
a	O
measurable	O
space	O
such	O
that	O
s	O
sl	O
x	O
and	O
fi	O
x	O
e	O
f	O
whenever	O
fi	O
e	O
fi	O
and	O
e	O
f	O
v	O
is	O
called	O
the	O
product	B
measure	O
of	O
vi	O
and	O
on	O
f	O
iffor	O
fi	O
e	O
and	O
e	O
x	O
vi	O
the	O
product	B
of	O
more	O
than	O
two	O
measures	O
can	O
be	O
defined	O
similarly	O
denseness	O
results	O
theorem	O
a	O
theorem	O
let	O
h	O
be	O
a	O
measurable	O
function	O
on	O
the	O
product	B
space	O
f	O
then	O
is	O
hu	O
vvdu	O
v	O
is	O
hu	O
vldu	O
is	O
hu	O
v	O
v	O
assuming	O
that	O
one	O
of	O
the	O
three	O
integrals	O
is	O
finite	O
denseness	O
results	O
lemma	O
a	O
t	O
and	O
hart	O
let	O
fl	O
be	O
a	O
probability	O
measure	O
on	O
nd	O
and	O
define	O
its	O
support	B
set	O
by	O
a	O
supportfl	O
for	O
all	O
r	O
flsxr	O
o	O
then	O
fla	O
proof	O
by	O
the	O
definition	B
of	I
a	O
a	O
c	O
flsxrj	O
for	O
some	O
rx	O
o	O
let	O
q	O
denote	O
the	O
set	O
of	O
vectors	O
in	O
nd	O
with	O
rational	O
components	O
any	O
countable	O
dense	O
set	O
then	O
for	O
each	O
x	O
e	O
ac	O
there	O
is	O
a	O
yx	O
e	O
q	O
with	O
ilx	O
yxlls	O
this	O
implies	O
c	O
sxrx	O
therefore	O
flsyxrx	O
and	O
c	O
c	O
u	O
a	O
xeac	O
the	O
right-hand	O
side	O
is	O
a	O
union	O
of	O
countably	O
many	O
sets	O
of	O
zero	O
measure	O
and	O
therefore	O
fla	O
definition	O
let	O
f	O
v	O
be	O
a	O
measure	O
space	O
for	O
a	O
fixed	O
number	O
p	O
l	O
p	O
denotes	O
the	O
set	O
of	O
all	O
measurable	O
functions	O
satisfying	O
f	O
i	O
f	O
i	O
p	O
d	O
v	O
theorem	O
a	O
s	O
for	O
every	O
probability	O
measure	O
v	O
on	O
n	O
d	O
the	O
set	O
of	O
tions	O
with	O
bounded	O
support	B
is	O
dense	O
in	O
l	O
p	O
in	O
other	O
words	O
for	O
every	O
e	O
and	O
f	O
e	O
l	O
p	O
there	O
is	O
a	O
continuous	O
function	O
with	O
bounded	O
support	B
gel	O
p	O
such	O
that	O
the	O
following	O
theorem	O
is	O
a	O
rich	O
source	O
of	O
denseness	O
results	O
appendix	O
theorem	O
theorem	O
let	O
f	O
be	O
afamily	O
of	O
real-valued	O
continuous	O
functions	O
on	O
a	O
closed	O
bounded	O
subset	O
b	O
of	O
rd	O
assume	O
that	O
f	O
is	O
an	O
algebra	O
that	O
is	O
for	O
any	O
ii	O
h	O
e	O
f	O
and	O
a	O
b	O
e	O
r	O
we	O
have	O
ali	O
bh	O
e	O
f	O
and	O
fl	O
h	O
e	O
f	O
assume	O
furthermore	O
that	O
if	O
x	O
y	O
then	O
there	O
is	O
an	O
f	O
e	O
f	O
such	O
that	O
f	O
f	O
and	O
that	O
for	O
each	O
x	O
e	O
b	O
there	O
exists	O
an	O
f	O
e	O
f	O
with	O
f	O
o	O
then	O
for	O
every	O
e	O
and	O
continuous	O
function	O
g	O
b	O
r	O
there	O
exists	O
an	O
f	O
e	O
f	O
such	O
that	O
sup	O
igx	O
xeb	O
e	O
the	O
following	O
two	O
theorems	O
concern	O
differentiation	O
of	O
integrals	O
good	O
general	O
references	O
are	O
whee	O
den	O
and	O
zygmund	O
and	O
de	O
guzman	O
theorem	O
a	O
io	O
lebesgue	O
density	O
theorem	O
let	O
f	O
be	O
a	O
density	O
on	O
rd	O
let	O
be	O
a	O
sequence	O
of	O
closed	O
cubes	O
centered	O
at	O
x	O
and	O
contracting	O
to	O
x	O
then	O
hm	O
k--oo	O
fqkx	O
ifx	O
fyldy	O
aqkx	O
at	O
almost	O
all	O
x	O
where	O
a	O
denotes	O
the	O
lebesgue	O
measure	O
note	O
that	O
this	O
implies	O
at	O
almost	O
all	O
x	O
corollary	O
a	O
i	O
let	O
a	O
be	O
a	O
collection	O
of	O
subsets	O
of	O
so	O
with	O
the	O
property	O
that	O
for	O
all	O
a	O
e	O
a	O
aa	O
casol	O
for	O
some	O
fixed	O
c	O
o	O
then	O
for	O
almost	O
all	O
x	O
if	O
x	O
r	O
a	O
x	O
rea	O
sup	O
r-o	O
aea	O
ax	O
r	O
a	O
i	O
fxra	O
fydy	O
fi	O
x	O
the	O
lebesgue	O
density	O
theorem	O
also	O
holds	O
if	O
is	O
replaced	O
by	O
a	O
sequence	O
of	O
contracting	O
balls	O
centered	O
at	O
x	O
or	O
indeed	O
by	O
any	O
sequence	O
of	O
sets	O
that	O
satisfy	O
x	O
brks	O
s	O
qkx	O
s	O
x	O
arks	O
where	O
s	O
is	O
the	O
unit	O
ball	O
of	O
rd	O
rk	O
and	O
b	O
a	O
are	O
fixed	O
constants	O
this	O
follows	O
from	O
the	O
lebesgue	O
density	O
theorem	O
it	O
does	O
not	O
hold	O
in	O
general	O
when	O
is	O
a	O
sequence	O
of	O
hyperrectangles	O
containing	O
x	O
and	O
contracting	O
to	O
x	O
for	O
that	O
an	O
additional	O
restriction	O
is	O
needed	O
theorem	O
jessen-marcinkiewicz-zygmund	O
theorem	O
let	O
f	O
be	O
a	O
density	O
on	O
rd	O
with	O
f	O
f	O
logd-l	O
fdx	O
let	O
be	O
a	O
sequence	O
of	O
hyperrectangles	O
containing	O
x	O
andfor	O
which	O
diamqkx	O
o	O
then	O
at	O
almost	O
all	O
x	O
probability	O
corollary	O
iff	O
is	O
a	O
density	O
and	O
is	O
as	O
in	O
theorema	O
ll	O
then	O
fydy	O
fx	O
lim	O
inf	O
k-oo	O
aqkx	O
qkx	O
i	O
at	O
almost	O
all	O
x	O
to	O
see	O
this	O
take	O
g	O
minf	O
m	O
for	O
large	O
fixed	O
m	O
as	O
f	O
g	O
logd-ll	O
g	O
by	O
the	O
lessen-marcinkiewicz-zygmund	O
theorem	O
i	O
gydy	O
gx	O
lim	O
inf	O
k-oo	O
aqkx	O
qkx	O
at	O
almost	O
all	O
x	O
conclude	O
by	O
letting	O
m	O
along	O
the	O
integers	O
probability	O
definition	O
a	O
measure	O
space	O
f	O
p	O
is	O
called	O
a	O
probability	O
space	O
if	O
pq	O
q	O
is	O
the	O
sample	O
space	O
or	O
sure	O
event	O
the	O
measurable	O
sets	O
are	O
called	O
events	O
and	O
the	O
measurable	O
functions	O
are	O
called	O
random	O
variables	O
if	O
xl	O
xn	O
are	O
random	O
variables	O
then	O
x	O
xn	O
is	O
a	O
vector-valued	O
random	O
variable	B
definition	O
let	O
x	O
be	O
a	O
random	O
variable	B
then	O
x	O
induces	O
the	O
measure	O
fl	O
on	O
the	O
borel	O
ofr	O
by	O
flb	O
p	O
xw	O
e	O
bll	O
px	O
e	O
b	O
b	O
e	O
b	O
the	O
probability	O
measure	O
fl	O
is	O
called	O
the	O
distribution	O
of	O
the	O
random	O
variable	B
x	O
definition	O
let	O
x	O
be	O
a	O
random	O
variable	B
the	O
expectation	O
of	O
x	O
is	O
the	O
integral	O
of	O
x	O
with	O
respect	O
to	O
the	O
distribution	O
fl	O
of	O
x	O
ex	O
l	O
xfldx	O
if	O
it	O
exists	O
definition	O
let	O
x	O
be	O
a	O
random	O
variable	B
the	O
variance	B
of	I
x	O
is	O
varx	O
e	O
ifex	O
isjinite	O
and	O
ifex	O
is	O
notjinite	O
or	O
does	O
not	O
exist	O
definition	O
let	O
xl	O
xn	O
be	O
random	O
variables	O
they	O
induce	O
the	O
measure	O
on	O
the	O
borel	O
o-algebra	O
ofrn	O
with	O
the	O
property	O
appendix	O
pn	O
is	O
called	O
the	O
joint	O
distribution	O
of	O
the	O
random	O
variables	O
x	O
i	O
x	O
n	O
let	O
fji	O
be	O
the	O
distribution	O
of	O
xi	O
n	O
the	O
random	O
variables	O
xl	O
xn	O
are	O
independent	O
if	O
their	O
joint	O
distribution	O
fj	O
is	O
the	O
product	B
measure	O
of	O
fjn	O
the	O
events	O
ai	O
an	O
e	O
are	O
independent	O
if	O
the	O
random	O
variables	O
iaj	O
ian	O
are	O
independent	O
fubinis	O
theorem	O
implies	O
the	O
following	O
theorem	O
if	O
the	O
random	O
variables	O
xl	O
xn	O
are	O
independent	O
and	O
have	O
finite	O
expectations	O
then	O
a	O
s	O
inequalities	O
theorem	O
inequality	B
if	O
the	O
random	O
variables	O
x	O
and	O
y	O
have	O
finite	O
second	O
moments	O
then	O
theorem	O
inequality	B
let	O
p	O
q	O
e	O
such	O
that	O
let	O
x	O
and	O
y	O
be	O
random	O
variables	O
such	O
that	O
and	O
then	O
theorem	O
a	O
is	O
inequality	B
let	O
x	O
be	O
a	O
nonnegative-valued	O
random	O
variable	B
then	O
for	O
each	O
t	O
px	O
t	O
s	O
ex	O
t	O
theorem	O
inequality	B
let	O
x	O
be	O
a	O
random	O
variable	B
then	O
for	O
each	O
t	O
pix	O
exi	O
t	O
s	O
varx	O
t	O
theorem	O
inequality	B
let	O
t	O
then	O
px	O
ex	O
t	O
varx	O
varx	O
t	O
proof	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
ex	O
then	O
for	O
all	O
t	O
t	O
et	O
x	O
s	O
et	O
xixst	O
inequalities	O
thus	O
for	O
t	O
from	O
the	O
cauchy-schwarz	B
inequality	B
t	O
et	O
xfeilxst	O
et	O
t	O
t	O
t	O
that	O
is	O
and	O
the	O
claim	O
follows	O
t	O
px	O
t	O
varx	O
theorem	O
a	O
ls	O
inequality	B
if	O
f	O
is	O
a	O
real-valued	O
convex	O
function	O
on	O
afinite	O
or	O
infinite	O
interval	O
ofr	O
and	O
x	O
is	O
a	O
random	O
variable	B
withfinite	O
expectation	O
taking	O
its	O
values	O
in	O
this	O
interval	O
then	O
fex	O
efx	O
theorem	O
inequalities	O
let	O
x	O
be	O
a	O
real-valued	O
random	O
variable	B
and	O
let	O
fx	O
and	O
gx	O
be	O
monotone	O
nondecreasing	O
real-valuedfunctions	O
then	O
efxgx	O
efxegx	O
provided	O
that	O
all	O
expectations	O
exist	O
and	O
are	O
finite	O
if	O
f	O
is	O
monotone	O
increasing	O
and	O
g	O
is	O
monotone	O
decreasing	O
then	O
efxgx	O
efxegx	O
proof	O
we	O
prove	O
the	O
first	O
inequality	B
the	O
second	O
follows	O
by	O
symmetry	O
let	O
x	O
have	O
distribution	O
fl	O
then	O
we	O
write	O
efxgx	O
efxegx	O
f	O
fxgx	O
fldx	O
f	O
fey	O
fldy	O
f	O
gx	O
fldx	O
f	O
fygx	O
f	O
f	O
hex	O
ygx	O
where	O
hex	O
y	O
fx	O
fey	O
by	O
fubinis	O
theorem	O
the	O
last	O
integral	O
equals	O
r	O
hex	O
ygx	O
ir	O
ly	O
hex	O
ygx	O
ly	O
hex	O
ygx	O
appendix	O
since	O
hx	O
x	O
for	O
all	O
x	O
here	O
idy	O
the	O
second	O
integral	O
on	O
the	O
right-hand	O
side	O
is	O
just	O
thus	O
we	O
have	O
efxgx	O
efxegx	O
hex	O
ygx	O
ygx	O
hey	O
xgy	O
hex	O
ygx	O
gy	O
xy	O
xy	O
y	O
y	O
since	O
hey	O
x	O
y	O
and	O
by	O
the	O
fact	O
that	O
hex	O
y	O
and	O
gx	O
gy	O
if	O
x	O
y	O
convergence	O
of	O
random	O
variables	O
definition	O
let	O
n	O
be	O
a	O
sequence	O
of	O
random	O
variables	O
we	O
say	O
that	O
lim	O
xn	O
x	O
n-oo	O
in	O
probability	O
iffor	O
each	O
e	O
we	O
say	O
that	O
lim	O
pixn	O
xi	O
e	O
o	O
n-oo	O
lim	O
xn	O
x	O
with	O
probability	O
one	O
almost	O
surely	O
n-oo	O
if	O
xn	O
x	O
p	O
that	O
is	O
p	O
lim	O
xnw	O
xw	O
for	O
a	O
fixed	O
number	O
p	O
we	O
say	O
that	O
if	O
lim	O
xn	O
x	O
n-oo	O
in	O
l	O
p	O
lim	O
e	O
xi	O
p	O
o	O
conditional	O
expectation	O
theorem	O
convergence	O
in	O
l	O
p	O
implies	O
convergence	O
in	O
probability	O
theorem	O
limnoo	O
xn	O
x	O
with	O
probability	O
one	O
if	O
and	O
only	O
if	O
lim	O
sup	O
ixm	O
xi	O
noo	O
nsm	O
in	O
probability	O
thus	O
convergence	O
with	O
probability	O
one	O
implies	O
convergence	O
in	O
probability	O
theorem	O
lemma	O
let	O
an	O
n	O
be	O
a	O
sequence	O
of	O
events	O
introduce	O
the	O
notation	O
i	O
o	O
lim	O
sup	O
an	O
nl	O
un	O
am	O
ncx	O
stands	O
for	O
often	O
if	O
then	O
lpan	O
nl	O
pan	O
i	O
o	O
o	O
by	O
theorems	O
and	O
we	O
have	O
theorem	O
if	O
for	O
each	O
e	O
cx	O
lpixn	O
xi	O
e	O
nl	O
then	O
limncx	O
xn	O
x	O
with	O
probability	O
one	O
conditional	O
expectation	O
if	O
y	O
is	O
a	O
random	O
variable	B
with	O
finite	O
expectation	O
and	O
a	O
is	O
an	O
event	O
with	O
positive	O
probability	O
then	O
the	O
conditional	O
expectation	O
of	O
y	O
given	O
a	O
is	O
defined	O
by	O
eyia	O
ey	O
ia	O
pa	O
the	O
conditional	O
probability	O
of	O
an	O
event	O
b	O
given	O
a	O
is	O
pb	O
ia	O
elb	O
ia	O
pa	O
pa	O
n	O
b	O
appendix	O
definition	O
let	O
y	O
be	O
a	O
random	O
variable	B
with	O
finite	O
expectation	O
and	O
x	O
be	O
a	O
d	O
vector-valued	O
random	O
variable	B
let	O
f	O
x	O
be	O
the	O
a	O
generated	O
by	O
x	O
fx	O
b	O
e	O
sn	O
the	O
conditional	O
expectation	O
e	O
y	O
i	O
x	O
of	O
y	O
given	O
x	O
is	O
a	O
random	O
variable	B
with	O
the	O
property	O
that	O
for	O
all	O
a	O
e	O
fx	O
i	O
y	O
dp	O
i	O
eyixdp	O
the	O
existence	O
and	O
uniqueness	O
probability	O
one	O
of	O
e	O
y	O
i	O
x	O
is	O
a	O
consequence	O
of	O
the	O
radon-nikodym	B
theorem	I
if	O
we	O
apply	O
it	O
to	O
the	O
measures	O
such	O
that	O
and	O
definition	O
let	O
c	O
be	O
an	O
event	O
and	O
x	O
be	O
a	O
d-dimensional	O
vector-valued	O
random	O
variable	B
then	O
the	O
conditional	O
probability	O
of	O
c	O
given	O
x	O
is	O
pcix	O
x	O
theorem	O
let	O
y	O
be	O
a	O
random	O
variable	B
with	O
finite	O
expectation	O
let	O
c	O
be	O
an	O
event	O
and	O
let	O
x	O
and	O
z	O
be	O
vector-valued	O
random	O
variables	O
then	O
there	O
is	O
a	O
measurable	O
function	O
g	O
on	O
nd	O
such	O
that	O
eyix	O
gx	O
with	O
probability	O
one	O
ey	O
eeyix	O
pc	O
epcix	O
eyix	O
eeyix	O
zix	O
pcix	O
epcix	O
yix	O
ify	O
is	O
a	O
function	O
of	O
x	O
theneyix	O
y	O
ify	O
fx	O
zforameasurablefunction	O
f	O
and	O
x	O
and	O
z	O
are	O
independent	O
ify	O
x	O
and	O
z	O
are	O
independent	O
then	O
eyix	O
z	O
eyix	O
then	O
eyix	O
gx	O
where	O
gx	O
efx	O
z	O
the	O
binomial	B
distribution	I
an	O
integer-valued	O
random	O
variable	B
x	O
is	O
said	O
to	O
be	O
binomially	O
distributed	O
with	O
parameters	O
nand	O
p	O
if	O
px	O
k	O
k	O
p	O
p	O
n-k	O
k	O
n	O
k	O
if	O
ai	O
an	O
are	O
independent	O
events	O
with	O
pad	O
p	O
then	O
x	O
is	O
binomial	B
p	O
iai	O
is	O
called	O
a	O
bernoulli	O
random	O
variable	B
with	O
parameter	O
p	O
the	O
binomial	B
distribution	I
lemma	O
let	O
the	O
random	O
variable	B
bn	O
p	O
be	O
binomially	O
distributed	O
with	O
rameters	O
nand	O
p	O
then	O
and	O
ii	O
proof	O
follows	O
from	O
the	O
following	O
simple	O
calculation	O
k	O
t	O
pk	O
p	O
l	O
n	O
p	O
ko	O
n	O
pn-kl	O
pt-k	O
k	O
p	O
ko	O
k	O
p	O
for	O
we	O
have	O
e	O
pbnp	O
o	O
s	O
e	O
l	O
bn	O
p	O
s	O
by	O
lemma	O
let	O
b	O
be	O
a	O
binomial	B
random	O
variable	B
with	O
parameters	O
nand	O
p	O
then	O
for	O
every	O
p	O
andfor	O
p	O
p	O
b	O
lj	O
proof	O
the	O
lemma	O
follows	O
from	O
stirlings	O
formula	O
n	O
e	O
e	O
g	O
feller	O
appendix	O
lemma	O
and	O
gyorfi	O
p	O
for	O
any	O
random	O
variable	B
x	O
with	O
finite	O
fourth	O
moment	O
proof	O
fix	O
a	O
o	O
the	O
function	O
lx	O
is	O
minimal	O
on	O
whenx	O
thus	O
x	O
a	O
x	O
replace	O
x	O
by	O
ixi	O
and	O
take	O
expectations	O
the	O
lower	O
bound	O
considered	O
as	O
a	O
function	O
of	O
a	O
is	O
maximized	O
if	O
we	O
take	O
a	O
resubstitution	B
yields	O
the	O
given	O
inequality	B
lemma	O
a	O
s	O
let	O
b	O
be	O
a	O
binomial	B
random	O
variable	B
then	O
proof	O
this	O
bound	O
is	O
a	O
special	O
case	O
of	O
khintchines	O
inequality	B
szarek	O
haagerup	O
and	O
also	O
devroye	O
and	O
gyorfi	O
p	O
rather	O
than	O
proving	O
the	O
given	O
inequality	B
we	O
will	O
show	O
how	O
to	O
apply	O
the	O
previous	O
lemma	O
to	O
get	O
further	O
work	O
the	O
inequality	B
indeed	O
e	O
and	O
e	O
thus	O
n	O
i	O
in	O
v	O
e	O
b	O
i	O
lemma	O
let	O
b	O
be	O
a	O
binomial	B
p	O
random	O
variable	B
with	O
p	O
thenfor	O
p	O
k	O
np	O
pb	O
k	O
p	O
k	O
np	O
where	O
n	O
is	O
normal	B
the	O
multinomial	B
distribution	I
the	O
hypergeometric	B
distribution	I
let	O
n	O
b	O
and	O
n	O
be	O
positive	O
integers	O
with	O
n	O
n	O
and	O
n	O
b	O
a	O
random	O
variable	B
x	O
taking	O
values	O
on	O
the	O
integers	O
b	O
is	O
hypergeometric	B
with	O
parameters	O
n	O
b	O
and	O
n	O
if	O
k	O
b	O
x	O
models	O
the	O
number	O
of	O
blue	O
balls	O
in	O
a	O
sample	O
of	O
n	O
balls	O
drawn	O
without	O
replacement	O
from	O
an	O
urn	O
containing	O
b	O
blue	O
and	O
n	O
b	O
red	O
balls	O
theorem	O
let	O
the	O
set	O
a	O
consist	O
of	O
n	O
numbers	O
aj	O
an	O
let	O
zi	O
zn	O
denote	O
a	O
random	O
sample	O
taken	O
without	O
replacement	O
from	O
a	O
where	O
n	O
n	O
denote	O
then	O
for	O
any	O
e	O
we	O
have	O
specifically	O
if	O
x	O
is	O
hypergeometrically	O
distributed	O
with	O
parameters	O
n	O
b	O
and	O
n	O
then	O
for	O
more	O
inequalities	O
of	O
this	O
type	O
see	O
hoeffding	O
and	O
serfling	O
the	O
multinomial	B
distribution	I
a	O
vector	O
nk	O
of	O
integer-valued	O
random	O
variables	O
is	O
multinomially	O
tributed	O
with	O
parameters	O
pi	O
pk	O
if	O
if	O
ll	O
i	O
j	O
k	O
otherwise	O
i	O
j	O
lemma	O
the	O
moment-generating	O
function	O
of	O
a	O
multinomial	B
pi	O
pk	O
vector	O
is	O
appendix	O
a	O
ii	O
the	O
exponential	B
and	O
gamma	B
distributions	O
a	O
nonnegative	O
random	O
variable	B
has	O
exponential	B
distribution	I
with	O
parameter	O
if	O
it	O
has	O
a	O
density	O
fx	O
ax	O
x	O
o	O
a	O
nonnegative-valued	O
random	O
variable	B
has	O
the	O
gamma	B
distribution	I
with	O
parameters	O
a	O
b	O
if	O
it	O
has	O
density	O
the	O
sum	O
of	O
n	O
i	O
i	O
d	O
exponential	B
random	O
variables	O
has	O
gamma	B
distribution	I
with	O
parameters	O
nand	O
the	O
multivariate	O
normal	B
distribution	I
a	O
d-dimensional	O
random	O
variable	B
x	O
xed	O
has	O
the	O
multivariate	O
mal	O
distribution	O
if	O
it	O
has	O
a	O
density	O
where	O
mend	O
is	O
a	O
positive	O
definite	O
symmetric	O
d	O
x	O
d	O
matrix	O
with	O
entries	O
and	O
dete	O
denotes	O
the	O
determinant	O
of	O
then	O
ex	O
m	O
and	O
for	O
all	O
i	O
j	O
d	O
is	O
called	O
the	O
covariance	O
matrix	O
of	O
x	O
notation	O
indicator	O
of	O
an	O
event	O
a	O
indicator	O
function	O
of	O
a	O
set	O
b	O
i	O
a	O
i	O
b	O
ixeb	O
iai	O
cardinality	O
of	O
a	O
finite	O
set	O
a	O
a	O
c	O
complement	O
of	O
a	O
set	O
a	O
alb	O
symmetric	O
difference	O
of	O
sets	O
a	O
b	O
jog	O
composition	O
of	O
functions	O
j	O
g	O
log	O
natural	O
logarithm	O
e	O
lx	O
j	O
integer	O
part	O
of	O
the	O
real	O
number	O
x	O
i	O
x	O
l	O
upper	O
integer	O
part	O
of	O
the	O
real	O
number	O
x	O
x	O
z	O
xi	O
components	O
of	O
the	O
d-dimensional	O
column	O
vector	O
x	O
ilx	O
ii	O
jlfl	O
l	O
of	O
x	O
e	O
nd	O
x	O
e	O
rd	O
observation	O
vector-valued	O
random	O
variable	B
y	O
e	O
i	O
dn	O
yl	O
yn	O
training	O
data	O
sequence	O
of	O
i	O
i	O
d	O
pairs	O
that	O
are	O
independent	O
of	O
y	O
and	O
have	O
the	O
same	O
distribution	O
as	O
that	O
of	O
y	O
if	O
x	O
and	O
z	O
have	O
the	O
same	O
distribution	O
label	O
binary	B
random	O
variable	B
py	O
iix	O
x	O
py	O
x	O
a	O
posteriori	O
probabilities	O
p	O
py	O
i	O
p	O
py	O
o	O
class	O
probabilities	O
rd	O
x	O
x	O
l	O
r	O
l	O
classification	O
g	O
rd	O
i	O
bayes	O
decision	O
function	O
rd	O
i	O
gn	O
functions	O
the	O
short	O
notation	O
gnx	O
gnx	O
dn	O
is	O
also	O
used	O
l	O
pgx	O
y	O
bayes	O
risk	O
the	O
error	O
probability	O
of	O
the	O
bayes	O
decision	O
notation	O
ln	O
lgn	O
pgjx	O
dn	O
yidn	O
error	O
probability	O
of	O
a	O
classification	O
in	O
i	O
yi	O
empirical	B
error	I
probability	O
of	O
a	O
classifier	B
function	O
gn	O
ixiea	O
empirical	B
measure	I
corresponding	O
to	O
xl	O
x	O
n	O
a	O
lebesgue	O
measure	O
on	O
nd	O
i	O
density	O
of	O
x	O
radon-nikodym	B
derivative	I
of	O
with	O
respect	O
to	O
a	O
it	O
px	O
e	O
a	O
probability	O
measure	O
of	O
x	O
exists	O
conditional	O
densities	O
of	O
x	O
given	O
y	O
and	O
y	O
respectively	O
they	O
exist	O
p	O
partition	B
of	O
nd	O
xklx	O
xk	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
x	O
n	O
k	O
nd	O
n	O
kernel	B
function	O
h	O
hn	O
smoothing	B
factor	I
for	O
a	O
kernel	B
rule	B
khx	O
hkxl	O
h	O
scaled	O
kernel	B
function	O
tm	O
ynl	O
ynm	O
testing	O
data	O
sequence	O
of	O
i	O
i	O
d	O
pairs	O
that	O
are	O
independent	O
of	O
y	O
and	O
d	O
n	O
and	O
have	O
the	O
same	O
distribution	O
as	O
that	O
of	O
y	O
a	O
class	O
of	O
sets	O
c	O
cn	O
classes	O
of	O
classification	O
functions	O
sea	O
n	O
n-th	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
sets	O
a	O
va	O
vapnik-chervonenkis	B
dimension	B
of	O
the	O
class	O
of	O
sets	O
a	O
sc	O
n	O
n-th	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
classifiers	O
c	O
vc	O
vapnik-chervonenkis	B
dimension	B
of	O
the	O
class	O
of	O
classifiers	O
c	O
sxr	O
e	O
n	O
d	O
ii	O
y	O
x	O
ii	O
r	O
closed	O
euclidean	B
ball	O
in	O
nd	O
centered	O
at	O
x	O
e	O
n	O
d	O
with	O
radius	O
r	O
o	O
references	O
abou-jaoude	O
s	O
conditions	O
necessaires	O
et	O
suffisantes	O
de	O
convergence	O
en	O
abilite	O
de	O
lhistogramme	O
pour	O
une	O
densite	O
annales	O
de	O
l	O
institut	O
henri	O
poincare	O
abou-jaoude	O
s	O
la	O
convergence	O
li	O
et	O
loo	O
de	O
lestimateur	O
de	O
la	O
partition	B
aleatoire	O
pour	O
une	O
densite	O
annales	O
de	O
linstitut	O
henri	O
poincare	O
abou-jaoude	O
s	O
sur	O
une	O
condition	O
necessaire	O
et	O
suffisante	O
de	O
li-convergence	O
presque	O
complete	O
de	O
l	O
estimateur	O
de	O
la	O
partition	B
fixe	O
pour	O
une	O
densite	O
comptes	O
rendus	O
de	O
l	O
academie	O
des	O
sciences	O
de	O
paris	O
aitchison	O
j	O
and	O
aitken	O
c	O
multivariate	O
binary	B
discrimination	O
by	O
the	O
kernel	B
method	O
aizerman	O
m	O
braverman	O
e	O
and	O
rozonoer	O
l	O
the	O
method	O
of	O
potential	O
functions	O
for	O
the	O
problem	O
of	O
restoring	O
the	O
characteristic	O
of	O
a	O
function	O
converter	O
from	O
randomly	O
observed	O
points	O
automation	O
and	O
remote	O
control	O
aizerman	O
m	O
braverman	O
e	O
and	O
rozonoer	O
l	O
the	O
probability	O
problem	O
of	O
pattern	O
recognition	O
learning	B
and	O
the	O
method	O
of	O
potential	O
functions	O
automation	O
and	O
remote	O
aizerman	O
m	O
braverman	O
e	O
and	O
rozonoer	O
l	O
theoretical	B
foundations	O
of	O
the	O
tential	O
function	O
method	O
in	O
pattern	O
recognition	O
learning	B
automation	O
and	O
remote	O
control	O
aizerman	O
m	O
braverman	O
e	O
and	O
rozonoer	O
l	O
extrapolative	O
problems	O
in	O
matic	O
control	O
and	O
the	O
method	O
of	O
potential	O
functions	O
american	O
mathematical	O
society	O
translations	O
akaike	O
h	O
an	O
approximation	O
to	O
the	O
density	O
function	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
akaike	O
h	O
a	O
new	O
look	O
at	O
the	O
statistical	O
model	O
identification	O
ieee	O
transactions	O
on	O
automatic	B
control	O
references	O
alexander	O
k	O
probability	O
inequalities	O
for	O
empirical	B
processes	O
and	O
a	O
law	O
of	O
the	O
iterated	O
logarithm	O
annals	O
of	O
probability	O
anderson	O
a	O
and	O
fu	O
k	O
design	O
and	O
development	O
of	O
a	O
linear	O
binary	B
tree	O
classifier	B
for	O
leukocytes	O
technical	O
report	O
purdue	O
university	O
lafayette	O
in	O
anderson	O
j	O
logistic	B
discrimination	I
in	O
handbook	O
of	O
statistics	O
krishnaiah	O
p	O
and	O
kanal	O
l	O
editors	O
volume	O
pages	O
north-holland	O
amsterdam	O
anderson	O
m	O
and	O
benning	O
r	O
a	O
distribution-free	O
discrimination	O
procedure	O
based	O
on	O
clustering	B
ieee	O
transactions	O
on	O
information	O
theory	O
anderson	O
t	O
an	O
introduction	O
to	O
multivariate	O
statistical	O
analysis	O
john	O
wiley	O
new	O
york	O
anderson	O
t	O
some	O
nonparametric	O
multivariate	O
procedures	O
based	O
on	O
statistically	B
equivalent	I
blocks	I
in	O
multivariate	O
analysis	O
krishnaiah	O
p	O
editor	O
pages	O
academic	O
press	O
new	O
york	O
angluin	O
d	O
and	O
valiant	O
l	O
fast	O
probabilistic	O
algorithms	O
for	O
hamiltonian	O
circuits	O
and	O
matchings	O
journal	O
of	O
computing	O
system	O
science	O
anthony	O
m	O
and	O
holden	O
s	O
on	O
the	O
power	O
of	O
polynomial	B
discriminators	O
and	O
radial	B
basis	B
function	I
networks	O
in	O
proceedings	O
of	O
the	O
sixth	O
annual	O
acm	O
conference	O
on	O
putational	O
learning	B
theory	O
pages	O
association	B
for	O
computing	O
machinery	O
new	O
york	O
anthony	O
m	O
and	O
shawe-taylor	O
j	O
a	O
result	O
ofvapnik	O
with	O
applications	O
technical	O
report	O
university	O
of	O
london	O
surrey	O
argentiero	O
p	O
chin	O
r	O
and	O
beaudet	O
p	O
an	O
automated	O
approach	O
to	O
the	O
design	O
of	O
cision	O
tree	O
classifiers	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
arkadjew	O
a	O
and	O
braverman	O
e	O
zeichenerkennung	O
und	O
maschinelles	O
lernen	O
old	O
enburg	O
verlag	O
miinchen	O
wien	O
ash	O
r	O
real	O
analysis	O
and	O
probability	O
academic	O
press	O
new	O
york	O
assouad	O
p	O
densite	O
et	O
dimension	B
annales	O
de	O
linstitut	O
fourier	O
assouad	O
p	O
deux	O
remarques	O
sur	O
i	O
comptes	O
rendus	O
de	O
lacademie	O
des	O
sciences	O
de	O
paris	O
azuma	O
k	O
weighted	B
sums	O
of	O
certain	O
dependent	O
random	O
variables	O
tohoku	O
matical	O
journal	O
bahadur	O
r	O
a	O
representation	O
of	O
the	O
joint	O
distribution	O
of	O
responses	O
to	O
n	O
dichotomous	O
items	O
in	O
studies	O
in	O
item	O
analysis	O
and	O
prediction	O
solomon	O
h	O
editor	O
pages	O
stanford	O
university	O
press	O
stanford	O
ca	O
bailey	O
t	O
and	O
jain	O
a	O
a	O
note	O
on	O
distance-weighted	O
k-nearest	O
neighbor	O
rules	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
barron	O
a	O
logically	O
smooth	O
density	B
estimation	B
technical	O
report	O
tr	O
ment	O
of	O
statistics	O
stanford	O
university	O
stanford	O
ca	O
barron	O
a	O
statistical	O
properties	O
of	O
artificial	O
neural	O
networks	O
in	O
proceedings	O
of	O
the	O
conference	O
on	O
decision	O
and	O
control	O
pages	O
tampa	O
fl	O
barron	O
a	O
complexity	B
regularization	I
with	O
application	O
to	O
artificial	O
neural	O
networks	O
in	O
nonparametric	O
functional	O
estimation	B
and	O
related	O
topics	O
roussas	O
g	O
editor	O
pages	O
nato	O
asi	O
series	O
kluwer	O
academic	O
publishers	O
dordrecht	O
barron	O
a	O
universal	O
approximation	O
bounds	O
for	O
superpositions	O
of	O
a	O
sigmoidal	O
tion	O
ieee	O
transactions	O
on	O
information	O
theory	O
barron	O
a	O
approximation	O
and	O
estimation	B
bounds	O
for	O
artificial	O
neural	O
networks	O
machine	O
learning	B
references	O
barron	O
a	O
and	O
barron	O
r	O
statistical	O
learning	B
networks	O
a	O
unifying	O
view	O
in	O
ceedings	O
of	O
the	O
symposium	O
on	O
the	O
interface	O
computing	O
science	O
and	O
statistics	O
wegman	O
e	O
gantz	O
d	O
and	O
miller	O
j	O
editors	O
pages	O
ams	O
alexandria	O
va	O
barron	O
a	O
and	O
cover	O
t	O
minimum	O
complexity	O
density	B
estimation	B
ieee	O
tions	O
on	O
information	O
theory	O
barron	O
a	O
gyorfi	O
l	O
and	O
van	O
der	O
meulen	O
e	O
distribution	O
estimation	B
consistent	O
in	O
total	B
variation	B
and	O
in	O
two	O
types	O
of	O
information	O
divergence	O
ieee	O
transactions	O
on	O
information	O
theory	O
barron	O
r	O
learning	B
networks	O
improve	O
computer-aided	O
prediction	O
and	O
control	O
puter	O
design	O
bartlett	O
p	O
lower	O
bounds	O
on	O
the	O
vapnik-chervonenkis	B
dimension	B
of	O
multi-layer	O
threshold	B
networks	O
in	O
proceedings	O
of	O
the	O
sixth	O
annual	O
acm	O
conference	O
on	O
tional	O
learning	B
theory	O
pages	O
association	B
for	O
computing	O
machinery	O
new	O
york	O
bashkirov	O
braverman	O
e	O
and	O
muchnik	O
i	O
potential	O
function	O
algorithms	O
for	O
pattern	O
recognition	O
learning	B
machines	O
automation	O
and	O
remote	O
control	O
baum	O
e	O
on	O
the	O
capabilities	O
of	O
multilayer	B
perceptrons	O
journal	O
of	O
complexity	O
baum	O
e	O
and	O
haussler	O
d	O
what	O
size	O
net	O
gives	O
valid	O
generalization	O
neural	O
putation	O
beakley	O
g	O
and	O
tuteur	O
f	O
distribution-free	O
pattern	O
verification	O
using	O
statistically	B
equivalent	I
blocks	I
ieee	O
transactions	O
on	O
computers	O
beck	O
j	O
the	O
exponential	B
rate	B
of	I
convergence	I
of	O
error	O
for	O
kn-nn	O
nonparametric	O
regression	O
and	O
decision	O
problems	O
of	O
control	O
and	O
information	O
theory	O
becker	O
p	O
recognition	O
of	O
patterns	O
polyteknisk	O
forlag	O
copenhagen	O
ben-bassat	O
m	O
use	O
of	O
distance	O
measures	O
information	O
measures	O
and	O
error	O
bounds	O
in	O
feature	O
evaluation	O
in	O
handbook	O
of	O
statistics	O
krishnaiah	O
p	O
and	O
kanal	O
l	O
editors	O
volume	O
pages	O
north-holland	O
amsterdam	O
benedek	O
g	O
and	O
itai	O
a	O
learnability	O
by	O
fixed	O
distributions	O
in	O
computational	O
learning	B
theory	O
proceedings	O
of	O
the	O
workshop	O
pages	O
morgan	O
kaufman	O
san	O
mateo	O
ca	O
benedek	O
g	O
and	O
itai	O
a	O
nonuniform	O
learnability	O
journal	O
of	O
computer	O
and	O
systems	O
sciences	O
bennett	O
g	O
probability	O
inequalities	O
for	O
the	O
sum	O
of	O
independent	O
random	O
variables	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
beran	O
r	O
minimum	O
hellinger	B
distance	I
estimates	O
for	O
parametric	O
models	O
annals	O
of	O
statistics	O
beran	O
r	O
comments	O
on	O
new	O
theoretical	B
and	O
algorithmical	O
basis	O
for	O
estimation	B
identification	O
and	O
control	O
by	O
p	O
kovanic	O
automatica	O
bernstein	O
s	O
the	O
theory	O
of	O
probabilities	O
gastehizdat	O
publishing	O
house	O
moscow	O
bhattacharya	O
p	O
and	O
mack	O
y	O
weak	B
convergence	O
of	O
k-nn	B
density	O
and	O
regression	O
estimators	O
with	O
varying	O
k	O
and	O
applications	O
annals	O
of	O
statistics	O
bhattacharyya	O
a	O
on	O
a	O
measure	O
of	O
divergence	O
between	O
two	O
multinomial	B
tions	O
sankhya	O
series	O
a	O
bickel	O
p	O
and	O
breiman	O
l	O
sums	O
of	O
functions	O
of	O
nearest	B
neighbor	I
distances	O
moment	O
bounds	O
limit	O
theorems	O
and	O
a	O
goodness	O
of	O
fit	O
test	O
annals	O
of	O
probability	O
birge	O
l	O
approximation	O
dans	O
les	O
espaces	O
metriques	O
et	O
theorie	O
de	O
lestimation	O
zeitschrijt	O
for	O
wahrscheinlichkeitstheorie	O
und	O
verwandte	O
gebiete	O
references	O
birge	O
l	O
on	O
estimating	O
a	O
density	O
using	O
hellinger	B
distance	I
and	O
some	O
other	O
strange	O
facts	O
probability	O
theory	O
and	O
related	O
fields	O
blumer	O
a	O
ehrenfeucht	O
a	O
haussler	O
d	O
and	O
warmuth	O
m	O
learnability	O
and	O
the	O
vapnik-chervonenkis	B
dimension	B
journal	O
of	O
the	O
acm	O
braverman	O
e	O
the	O
method	O
of	O
potential	O
functions	O
automation	O
and	O
remote	O
control	O
braverman	O
e	O
and	O
pyatniskii	O
e	O
estimation	B
of	I
the	O
rate	B
of	I
convergence	I
of	O
algorithms	O
based	O
on	O
the	O
potential	O
function	O
method	O
automation	O
and	O
remote	O
control	O
breiman	O
l	O
friedman	O
j	O
olshen	O
r	O
and	O
stone	O
c	O
classification	O
and	O
regression	O
trees	O
wadsworth	O
international	O
belmont	O
ca	O
breiman	O
l	O
meisel	O
w	O
and	O
purcell	O
e	O
variable	B
kernel	B
estimates	O
of	O
multivariate	O
densities	O
technometrics	O
brent	O
r	O
fast	O
training	O
algorithms	O
for	O
multilayer	B
neural	O
nets	O
ieee	O
transactions	O
on	O
neural	O
networks	O
broder	O
a	O
strategies	O
for	O
efficient	O
incremental	O
nearest	B
neighbor	I
search	O
pattern	O
recognition	O
broornhead	O
d	O
and	O
lowe	O
d	O
multivariable	O
functional	O
interpolation	O
and	O
adaptive	O
networks	O
complex	O
systems	O
buescher	O
k	O
and	O
kumar	O
p	O
learning	B
by	O
canonical	O
smooth	O
estimation	B
part	O
i	O
simultaneous	O
estimation	B
ieee	O
transactions	O
on	O
automatic	B
control	O
buescher	O
k	O
and	O
kumar	O
p	O
learning	B
by	O
canonical	O
smooth	O
estimation	B
part	O
ii	O
learning	B
and	O
choice	O
of	O
model	O
complexity	O
ieee	O
transactions	O
on	O
automatic	B
control	O
burbea	O
j	O
the	O
convexity	O
with	O
respect	O
to	O
gaussian	B
distributions	O
of	O
divergences	O
of	O
order	O
ex	O
utilitas	O
mathematica	O
burbea	O
j	O
and	O
rao	O
c	O
on	O
the	O
convexity	O
of	O
some	O
divergence	O
measures	O
based	O
on	O
entropy	B
functions	O
ieee	O
transactions	O
on	O
information	O
theory	O
burshtein	O
d	O
della	O
pietra	O
v	O
kanevsky	O
d	O
and	O
nadas	O
a	O
minimum	O
impurity	O
partitions	O
annals	O
of	O
statistics	O
cacoullos	O
t	O
estimation	B
of	I
a	O
multivariate	O
density	O
annals	O
of	O
the	O
institute	O
of	O
tical	O
mathematics	O
carnal	O
h	O
die	O
konvexe	O
hiille	O
von	O
n	O
rotationssymmetrisch	O
verteilten	O
punkten	O
zeitschrijt	O
for	O
wahrscheinlichkeitstheorie	O
und	O
verwandte	O
gebiete	O
casey	O
r	O
and	O
nagy	O
g	O
decision	O
tree	O
design	O
using	O
a	O
probabilistic	O
model	O
ieee	O
transactions	O
on	O
information	O
theory	O
cencov	O
n	O
evaluation	O
of	O
an	O
unknown	O
distribution	O
density	O
from	O
observations	O
soviet	O
math	O
doklady	O
chang	O
c	O
finding	O
prototypes	O
for	O
nearest	B
neighbor	I
classifiers	O
ieee	O
transactions	O
on	O
computers	O
chang	O
c	O
dynamic	O
programming	O
as	O
applied	O
to	O
feature	O
selection	O
in	O
pattern	O
nition	O
systems	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
chen	O
t	O
chen	O
h	O
and	O
liu	O
r	O
a	O
constructive	O
proof	O
and	O
an	O
extension	O
of	O
cybenkos	O
approximation	O
theorem	O
in	O
proceedings	O
of	O
the	O
symposium	O
of	O
the	O
interface	O
puting	O
science	O
and	O
statistics	O
pages	O
american	O
statistical	O
association	B
dria	O
va	O
chen	O
x	O
and	O
zhao	O
l	O
almost	O
sure	O
l	O
i-norm	O
convergence	O
for	O
data-based	B
histogram	O
density	O
estimates	O
journal	O
of	O
multivariate	O
analysis	O
references	O
chen	O
z	O
and	O
fu	O
k	O
nonparametric	O
bayes	O
risk	O
estimation	B
for	O
pattern	O
classification	O
in	O
proceedings	O
of	O
the	O
ieee	O
conference	O
on	O
systems	O
man	O
and	O
cybernetics	O
boston	O
ma	O
chernoff	O
h	O
a	O
measure	O
of	O
asymptotic	O
efficiency	O
of	O
tests	O
of	O
a	O
hypothesis	O
based	O
on	O
the	O
sum	O
of	O
observations	O
annals	O
of	O
mathematical	O
statistics	O
chernoff	O
h	O
a	O
bound	O
on	O
the	O
classification	O
error	O
for	O
discriminating	O
between	O
lations	O
with	O
specified	O
means	O
and	O
variances	O
in	O
studi	O
di	O
probabilita	O
statistica	O
e	O
ricerca	O
operativa	O
in	O
onare	O
di	O
giuseppe	O
pompilj	O
pages	O
oderisi	O
gubbio	O
chou	O
p	O
optimal	O
partitioning	O
for	O
classification	O
and	O
regression	O
trees	O
ieee	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
chou	O
w	O
and	O
chen	O
y	O
a	O
new	O
fast	O
algorithm	B
for	O
effective	O
training	O
of	O
neural	O
fiers	O
pattern	O
recognition	O
chow	O
c	O
statistical	O
independence	O
and	O
threshold	B
functions	O
ieee	O
transactions	O
on	O
computers	O
chow	O
c	O
on	O
optimum	O
recognition	O
error	O
and	O
rejection	O
tradeoff	O
ieee	O
transactions	O
on	O
information	O
theory	O
chow	O
y	O
and	O
teicher	O
h	O
probability	O
theory	O
independence	O
interchangeability	O
martingales	O
springer-verlag	O
new	O
york	O
ciampi	O
a	O
generalized	B
regression	O
trees	O
computational	O
statistics	O
and	O
data	O
ysis	O
collomb	O
g	O
estimation	B
de	O
la	O
regression	O
par	O
la	O
methode	O
des	O
k	O
points	O
les	O
plus	O
proches	O
proprietes	O
de	O
convergence	O
ponctuelle	O
comptes	O
rendus	O
de	O
l	O
academie	O
des	O
sciences	O
de	O
paris	O
collomb	O
g	O
estimation	B
de	O
la	O
regression	O
par	O
la	O
methode	O
des	O
k	O
points	O
les	O
plus	O
proches	O
avec	O
noyau	O
lecture	O
notes	O
in	O
mathematics	O
springer-verlag	O
berlin	O
collomb	O
g	O
estimation	B
non	O
parametrique	O
de	O
la	O
regression	O
revue	O
bibliographique	O
international	O
statistical	O
review	O
conway	O
j	O
and	O
sloane	O
n	O
sphere-packings	O
lattices	O
and	O
groups	O
springer-verlag	O
berlin	O
coomans	O
d	O
and	O
broeckaert	O
i	O
potential	O
pattern	O
recognition	O
in	O
chemical	O
and	O
medical	O
decision	O
making	O
research	O
studies	O
press	O
letchworth	O
hertfordshire	O
england	O
cormen	O
t	O
leiserson	O
c	O
and	O
rivest	O
r	O
introduction	O
to	O
algorithms	O
mit	O
press	O
boston	O
ma	O
cover	O
t	O
geometrical	O
and	O
statistical	O
properties	O
of	O
systems	O
of	O
linear	O
inequalities	O
with	O
applications	O
in	O
pattern	O
recognition	O
ieee	O
transactions	O
on	O
electronic	O
computers	O
cover	O
t	O
estimation	B
by	O
the	O
nearest	B
neighbor	I
rule	B
ieee	O
transactions	O
on	O
tion	O
theory	O
cover	O
t	O
rates	O
of	O
convergence	O
for	O
nearest	B
neighbor	I
procedures	O
in	O
proceedings	O
of	O
the	O
hawaii	O
international	O
conference	O
on	O
systems	O
sciences	O
pages	O
honolulu	O
cover	O
t	O
learning	B
in	O
pattern	O
recognition	O
in	O
methodologies	O
of	O
pattern	O
recognition	O
watanabe	O
s	O
editor	O
pages	O
academic	O
press	O
new	O
york	O
cover	O
t	O
the	O
best	O
two	O
independent	O
measurements	O
are	O
not	O
the	O
two	O
best	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
cover	O
t	O
and	O
hart	O
p	O
nearest	B
neighbor	I
pattern	O
classification	O
ieee	O
transactions	O
on	O
information	O
theory	O
cover	O
t	O
and	O
thomas	O
j	O
elements	O
of	O
information	O
theory	O
john	O
wiley	O
new	O
york	O
cover	O
t	O
and	O
van	O
campenhout	O
j	O
on	O
the	O
possible	O
orderings	O
in	O
the	O
measurement	O
selection	O
problem	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
references	O
cover	O
t	O
and	O
wagner	O
t	O
topics	O
in	O
statistical	O
pattern	O
recognition	O
communication	O
and	O
cybernetics	O
cramer	O
h	O
and	O
wold	O
h	O
some	O
theorems	O
on	O
distribution	O
functions	O
journal	O
of	O
the	O
london	O
mathematical	O
society	O
csibi	O
s	O
simple	O
and	O
compound	O
processes	O
in	O
iterative	O
machine	O
learning	B
technical	O
report	O
cism	O
summer	O
course	O
udine	O
italy	O
csibi	O
s	O
using	O
indicators	O
as	O
a	O
base	O
for	O
estimating	O
optimal	O
decision	O
functions	O
in	O
colloquia	O
mathematica	O
societatis	O
janos	O
botyai	O
topics	O
in	O
information	O
theory	O
pages	O
keszthely	O
hungary	O
csiszar	O
i	O
information-type	O
measures	O
of	O
difference	O
of	O
probability	O
distributions	O
and	O
indirect	O
observations	O
studia	O
scientiarium	O
mathematicarum	O
hungarica	O
csiszar	O
i	O
generalized	B
entropy	B
and	O
quantization	B
problems	O
in	O
transactions	O
of	O
the	O
sixth	O
prague	O
conference	O
on	O
information	O
theory	O
statistical	O
decision	O
functions	O
dom	O
processes	O
pages	O
academia	O
prague	O
csiszar	O
i	O
and	O
korner	O
j	O
information	O
theory	O
coding	O
theoremsfor	O
discrete	O
oryless	O
systems	O
academic	O
press	O
new	O
york	O
cybenko	O
g	O
approximations	O
by	O
superpositions	O
of	O
sigmoidal	O
functions	O
math	O
trol	O
signals	O
systems	O
darken	O
c	O
donahue	O
m	O
gurvits	O
l	O
and	O
sontag	O
e	O
rate	O
of	O
approximation	O
results	O
motivated	O
by	O
robust	O
neural	B
network	I
learning	B
in	O
proceedings	O
of	O
the	O
sixth	O
acm	O
shop	O
on	O
computational	O
learning	B
theory	O
pages	O
association	B
for	O
computing	O
machinery	O
new	O
york	O
das	O
gupta	O
s	O
nonparametric	O
classification	O
rules	O
sankhya	O
series	O
a	O
das	O
gupta	O
s	O
and	O
lin	O
h	O
nearest	B
neighbor	I
rules	O
of	O
statistical	O
classification	O
based	O
on	O
ranks	O
sankhya	O
series	O
a	O
dasarathy	O
b	O
nearest	B
neighbor	I
pattern	O
classification	O
techniques	O
ieee	O
computer	O
society	O
press	O
los	O
alamitos	O
ca	O
day	O
n	O
and	O
kerridge	O
d	O
a	O
general	O
maximum	B
likelihood	I
discriminant	O
biometrics	O
de	O
guzman	O
m	O
differentiation	O
of	O
integrals	O
in	O
rn	O
lecture	O
notes	O
in	O
mathematics	O
springer-verlag	O
berlin	O
deheuvels	O
p	O
estimation	B
nonparametrique	O
de	O
la	O
densite	O
par	O
histogrammes	O
alises	O
publications	O
de	O
linstitut	O
de	O
statistique	O
de	O
l	O
universite	O
de	O
paris	O
devijver	O
p	O
a	O
note	O
on	O
ties	O
in	O
voting	O
with	O
the	O
k-nn	B
rule	B
pattern	O
recognition	O
devijver	O
p	O
new	O
error	O
bounds	O
with	O
the	O
nearest	B
neighbor	I
rule	B
ieee	O
transactions	O
on	O
information	O
theory	O
devijver	O
p	O
an	O
overview	O
of	O
asymptotic	O
properties	O
of	B
nearest	B
neighbor	I
rules	I
in	O
pattern	O
recognition	O
in	O
practice	O
gelsema	O
e	O
and	O
kanal	O
l	O
editors	O
pages	O
elsevier	O
science	O
publishers	O
amsterdam	O
devijver	O
p	O
and	O
kittler	O
j	O
on	O
the	O
edited	B
nearest	B
neighbor	I
rule	B
in	O
proceedings	O
of	O
the	O
fifth	O
international	O
conference	O
on	O
pattern	O
recognition	O
pages	O
pattern	O
recognition	O
society	O
los	O
alamitos	O
ca	O
devijver	O
p	O
and	O
kittler	O
j	O
pattern	O
recognition	O
a	O
statistical	O
approach	O
hall	O
englewood	O
cliffs	O
nj	O
devroye	O
l	O
a	O
universal	O
k-nearest	O
neighbor	O
procedure	O
in	O
discrimination	O
in	O
ceedings	O
of	O
the	O
ieee	O
computer	O
society	O
conference	O
on	O
pattern	O
recognition	O
and	O
image	O
processing	O
pages	O
ieee	O
computer	O
society	O
long	O
beach	O
ca	O
references	O
devroye	O
l	O
on	O
the	O
almost	O
everywhere	O
convergence	O
of	O
nonparametric	O
regression	B
function	I
estimates	O
annals	O
of	O
statistics	O
devroye	O
l	O
b	O
on	O
the	O
asymptotic	O
probability	O
of	O
error	O
in	O
nonparametric	O
tion	O
annals	O
of	O
statistics	O
devroye	O
l	O
c	O
on	O
the	O
inequality	B
of	O
cover	O
and	O
hart	O
in	O
nearest	B
neighbor	I
discrimination	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
devroye	O
l	O
bounds	O
for	O
the	O
uniform	O
deviation	O
of	O
empirical	B
measures	O
journal	O
of	O
multivariate	O
analysis	O
devroye	O
l	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
the	O
almost	O
everywhere	O
vergence	O
of	O
nearest	B
neighbor	I
regression	B
function	I
estimates	O
zeitschrijt	O
for	O
lichkeitstheorie	O
und	O
verwandte	O
gebiete	O
devroye	O
l	O
the	O
equivalence	O
of	O
weak	B
strong	O
and	O
complete	O
convergence	O
in	O
for	O
kernel	B
density	O
estimates	O
annals	O
of	O
statistics	O
devroye	O
l	O
a	O
course	O
in	O
density	B
estimation	B
birkhauser	O
boston	O
ma	O
devroye	O
l	O
applications	O
of	O
the	O
theory	O
of	O
records	O
in	O
the	O
study	O
of	O
random	O
trees	O
acta	O
informatica	O
devroye	O
l	O
automatic	B
pattern	O
recognition	O
a	O
study	O
of	O
the	O
probability	O
of	O
error	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
devroye	O
l	O
the	O
expected	O
size	O
of	O
some	O
graphs	O
in	O
computational	O
geometry	O
puters	O
and	O
mathematics	O
with	O
applications	O
devroye	O
l	O
the	O
kernel	B
estimate	O
is	O
relatively	O
stable	O
probability	O
theory	O
and	O
related	O
fields	O
devroye	O
l	O
a	O
exponential	B
inequalities	O
in	O
nonparametric	O
estimation	B
in	O
n	O
ric	O
functional	O
estimation	B
and	O
related	O
topics	O
roussas	O
g	O
editor	O
pages	O
nato	O
asi	O
series	O
kluwer	O
academic	O
publishers	O
dordrecht	O
devroye	O
l	O
on	O
the	O
oscillation	O
of	O
the	O
expected	O
number	O
of	O
points	O
on	O
a	O
random	O
convex	B
hull	I
statistics	O
and	O
probability	O
letters	O
devroye	O
l	O
and	O
gyorfi	O
l	O
distribution-free	O
exponential	B
bound	O
on	O
the	O
l	O
error	O
of	O
partitioning	O
estimates	O
of	O
a	O
regression	B
function	I
in	O
proceedings	O
of	O
the	O
fourth	O
pannonian	O
symposium	O
on	O
mathematical	O
statistics	O
konecny	O
e	O
j	O
and	O
wertz	O
w	O
editors	O
pages	O
akademiai	O
budapest	O
hungary	O
devroye	O
l	O
and	O
gyorfi	O
l	O
nonparametric	O
density	B
estimation	B
the	O
view	O
john	O
wiley	O
new	O
york	O
devroye	O
l	O
and	O
gyorfi	O
l	O
no	O
empirical	B
probability	O
measure	O
can	O
converge	O
in	O
the	O
total	B
variation	B
sense	O
for	O
all	O
distributions	O
annals	O
of	O
statistics	O
devroye	O
l	O
gyorfi	O
l	O
krzyzak	O
a	O
and	O
lugosi	O
g	O
on	O
the	O
strong	O
universal	O
tency	O
of	O
nearest	B
neighbor	I
regression	B
function	I
estimates	O
annals	O
of	O
statistics	O
devroye	O
l	O
and	O
krzyzak	O
a	O
an	O
equivalence	O
theorem	O
for	O
convergence	O
of	O
the	O
kernel	B
regression	O
estimate	O
journal	O
of	O
statistical	O
planning	O
and	O
inference	O
devroye	O
l	O
and	O
laforest	O
l	O
an	O
analysis	O
of	O
random	O
d-dimensional	O
quadtrees	O
siam	O
journal	O
on	O
computing	O
devroye	O
l	O
and	O
lugosi	O
g	O
lower	O
bounds	O
in	O
pattern	O
recognition	O
and	O
learning	B
pattern	O
recognition	O
devroye	O
l	O
and	O
machell	O
e	O
data	O
structures	O
in	O
kernel	B
density	B
estimation	B
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
devroye	O
l	O
and	O
wagner	O
t	O
a	O
distribution-free	O
performance	O
bound	O
in	O
error	O
mation	O
ieee	O
transactions	O
information	O
theory	O
references	O
devroye	O
l	O
and	O
wagner	O
t	O
nonparametric	O
discrimination	O
and	O
density	B
estimation	B
technical	O
report	O
electronics	O
research	O
center	O
university	O
of	O
texas	O
devroye	O
l	O
and	O
wagner	O
t	O
distribution-free	O
inequalities	O
for	O
the	O
deleted	O
and	O
out	O
error	O
estimates	O
ieee	O
transactions	O
on	O
information	O
theory	O
devroye	O
l	O
and	O
wagner	O
t	O
distribution-free	O
performance	O
bounds	O
for	O
potential	O
function	O
rules	O
ieee	O
transactions	O
on	O
information	O
theory	O
devroye	O
l	O
and	O
wagner	O
t	O
distribution-free	O
performance	O
bounds	O
with	O
the	O
stitution	O
error	O
estimate	O
ieee	O
transactions	O
on	O
information	O
theory	O
devroye	O
l	O
and	O
wagner	O
t	O
distribution-free	O
consistency	B
results	O
in	O
nonparametric	O
discrimination	O
and	O
regression	B
function	I
estimation	B
annals	O
of	O
statistics	O
devroye	O
l	O
and	O
wagner	O
t	O
on	O
the	O
ll	O
convergence	O
ofkemel	O
estimators	O
of	O
reg	O
sion	O
functions	O
with	O
applications	O
in	O
discrimination	O
zeitschriji	O
for	O
theorie	O
und	O
verwandte	O
gebiete	O
devroye	O
l	O
and	O
wagner	O
t	O
nearest	B
neighbor	I
methods	O
in	O
discrimination	O
in	O
book	O
of	O
statistics	O
krishnaiah	O
p	O
and	O
kanal	O
l	O
editors	O
volume	O
pages	O
north	O
holland	O
amsterdam	O
devroye	O
l	O
and	O
wise	O
g	O
consistency	B
of	O
a	O
recursive	B
nearest	B
neighbor	I
regression	B
function	I
estimate	O
journal	O
of	O
multivariate	O
analysis	O
diaconis	O
p	O
and	O
shahshahani	O
m	O
on	O
nonlinear	O
functions	O
of	O
linear	O
combinations	O
siam	O
journal	O
on	O
scientific	O
and	O
statistical	O
computing	O
do-tu	O
h	O
and	O
installe	O
m	O
on	O
adaptive	O
solution	O
of	O
piecewise	O
linear	O
tion	O
problem-application	O
to	O
modeling	O
and	O
identification	O
in	O
milwaukee	O
symposium	O
on	O
automatic	B
computation	O
and	O
control	O
pages	O
milwaukee	O
wi	O
duda	O
r	O
and	O
hart	O
p	O
pattern	O
classification	O
and	O
scene	O
analysis	O
john	O
wiley	O
new	O
york	O
dudani	O
s	O
the	O
distance-weighted	O
k-nearest-neighbor	O
rule	B
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
dudley	O
r	O
central	O
limit	O
theorems	O
for	O
empirical	B
measures	O
annals	O
of	O
probability	O
dudleyr	O
balls	O
in	O
rk	O
do	O
not	O
cut	O
all	O
subsets	O
of	O
points	O
advances	O
in	O
mathematics	O
dudley	O
r	O
empirical	B
processes	O
in	O
ecole	O
de	O
probabilite	O
de	O
st	O
flour	O
lecture	O
notes	O
in	O
mathematics	O
springer-verlag	O
new	O
york	O
dudley	O
r	O
universal	O
donsker	O
classes	O
and	O
metric	B
entropy	B
annals	O
of	O
probability	O
dudley	O
r	O
kulkarni	O
s	O
richardson	O
t	O
and	O
zeitouni	O
o	O
a	O
metric	B
entropy	B
bound	O
is	O
not	O
sufficient	O
for	O
learnability	O
ieee	O
transactions	O
on	O
information	O
theory	O
durrett	O
r	O
probability	O
theory	O
and	O
examples	O
wadsworth	O
and	O
brookscole	O
pacific	O
grove	O
ca	O
dvoretzky	O
a	O
on	O
stochastic	B
approximation	I
in	O
proceedings	O
of	O
the	O
first	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	O
and	O
probability	O
neyman	O
j	O
editor	O
pages	O
university	O
of	O
california	O
press	O
berkeley	O
los	O
angeles	O
dvoretzky	O
a	O
kiefer	O
j	O
and	O
wolfowitz	O
j	O
asymptotic	O
minimax	O
character	O
of	O
a	O
sample	O
distribution	B
function	I
and	O
of	O
the	O
classical	O
multinomial	B
estimator	O
annals	O
of	O
ematical	O
statistics	O
edelsbrunner	O
h	O
algorithmsfor	O
computational	O
geometry	O
springer-verlag	O
berlin	O
efron	O
b	O
bootstrap	B
methods	O
another	O
look	O
at	O
the	O
jackknife	O
annals	O
of	O
statistics	O
references	O
efron	O
b	O
estimating	O
the	O
error	O
rate	O
of	O
a	O
prediction	O
rule	B
improvement	O
on	O
cross	O
validation	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
efron	O
b	O
and	O
stein	O
c	O
the	O
jackknife	O
estimate	O
of	O
variance	O
annals	O
of	O
statistics	O
ehrenfeucht	O
a	O
haussler	O
d	O
kearns	O
m	O
and	O
valiant	O
l	O
a	O
general	O
lower	O
bound	O
on	O
the	O
number	O
of	O
examples	O
needed	O
for	O
learning	B
information	O
and	O
computation	O
elashoff	O
j	O
elashoff	O
r	O
and	O
goldmann	O
g	O
on	O
the	O
choice	O
of	O
variables	O
in	O
cation	O
problems	O
with	O
dichotomous	O
variables	O
biometrika	O
fabian	O
v	O
stochastic	B
approximation	I
in	O
optimizing	O
methods	O
in	O
statistics	O
rustagi	O
j	O
editor	O
pages	O
academic	O
press	O
new	O
york	O
london	O
fano	O
r	O
class	O
notes	O
for	O
transmission	O
of	O
information	O
course	O
mit	O
bridge	O
ma	O
farago	O
a	O
linder	O
t	O
and	O
lugosi	O
g	O
fast	O
nearest	B
neighbor	I
search	O
in	O
dissimilarity	O
spaces	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
farago	O
a	O
and	O
lugosi	O
g	O
strong	B
universal	B
consistency	B
of	B
neural	B
network	I
classifiers	I
ieee	O
transactions	O
on	O
information	O
theory	O
farago	O
t	O
and	O
gyorfi	O
l	O
on	O
the	O
continuity	O
of	O
the	O
error	O
distortion	O
function	O
for	O
multiple	O
hypothesis	O
decisions	O
ieee	O
transactions	O
on	O
information	O
theory	O
feinholz	O
l	O
estimation	B
of	I
the	O
performance	O
of	O
partitioning	O
algorithms	O
in	O
pattern	O
classification	O
masters	O
thesis	O
department	O
of	O
mathematics	O
mcgill	O
university	O
treal	O
feller	O
w	O
an	O
introduction	O
to	O
probability	O
theory	O
and	O
its	O
applications	O
you	O
john	O
wiley	O
new	O
york	O
finkel	O
r	O
and	O
bentley	O
j	O
quad	O
trees	O
a	O
data	O
structure	O
for	O
retrieval	O
on	O
composite	O
keys	O
acta	O
informatica	O
fisher	O
r	O
the	O
case	O
of	O
multiple	O
measurements	O
in	O
taxonomic	O
problems	O
annals	O
of	O
eugenics	O
part	O
ii	O
fitzmaurice	O
g	O
and	O
hand	O
d	O
a	O
comparison	O
of	O
two	O
average	O
conditional	O
error	O
rate	O
estimators	O
pattern	O
recognition	O
letters	O
fix	O
e	O
and	O
hodges	O
j	O
discriminatory	O
analysis	O
nonparametric	O
discrimination	O
consistency	B
properties	O
technical	O
report	O
project	O
number	O
usaf	O
school	O
of	O
aviation	O
medicine	O
randolph	O
field	O
tx	O
fix	O
e	O
and	O
hodges	O
j	O
discriminatory	O
analysis	O
small	O
sample	O
performance	O
nical	O
report	O
usaf	O
school	O
of	O
aviation	O
medicine	O
randolph	O
field	O
tx	O
fix	O
e	O
and	O
hodges	O
j	O
a	O
discriminatory	O
analysis	O
nonparametric	O
discrimination	O
sistency	O
properties	O
in	O
nearest	B
neighbor	I
pattern	O
classification	O
techniques	O
dasarathy	O
b	O
editor	O
pages	O
ieee	O
computer	O
society	O
press	O
los	O
alamitos	O
ca	O
fix	O
e	O
and	O
hodges	O
j	O
discriminatory	O
analysis	O
small	O
sample	O
performance	O
in	O
nearest	B
neighbor	I
pattern	O
classification	O
techniques	O
dasarathy	O
b	O
editor	O
pages	O
ieee	O
computer	O
society	O
press	O
los	O
alamitos	O
ca	O
flick	O
t	O
jones	O
l	O
priest	O
r	O
and	O
herman	O
c	O
pattern	O
classification	O
using	O
protection	O
pursuit	O
pattern	O
recognition	O
forney	O
g	O
exponential	B
error	O
bounds	O
for	O
erasure	O
list	O
and	O
decision	O
feedback	O
schemes	O
ieee	O
transactions	O
on	O
information	O
theory	O
friedman	O
j	O
a	O
recursive	B
partitioning	O
decision	O
rule	B
for	O
nonparametric	O
classification	O
ieee	O
transactions	O
on	O
computers	O
references	O
friedman	O
j	O
baskett	O
f	O
and	O
shustek	O
l	O
an	O
algorithm	B
for	O
finding	O
nearest	B
neighbor	I
ieee	O
transactions	O
on	O
computers	O
friedman	O
j	O
bentley	O
j	O
and	O
finkel	O
r	O
an	O
algorithm	B
for	O
finding	O
best	O
matches	O
in	O
logarithmic	O
expected	O
time	O
acm	O
transactions	O
on	O
mathematical	O
software	O
friedman	O
j	O
and	O
silverman	O
b	O
flexible	O
parsimonious	O
smoothing	O
and	O
additive	O
eling	O
technometrics	O
friedman	O
j	O
and	O
stuetzle	O
w	O
projection	B
pursuit	I
regression	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
friedman	O
j	O
stuetzle	O
w	O
and	O
schroeder	O
a	O
projection	B
pursuit	I
density	B
estimation	B
journal	O
of	O
the	O
american	O
statistical	O
association	B
friedman	O
and	O
tukey	O
a	O
projection	B
pursuit	I
algorithm	B
for	O
exploratory	O
data	O
analysis	O
ieee	O
transactions	O
on	O
computers	O
fritz	O
and	O
gyorfi	O
l	O
on	O
the	O
minimization	O
of	O
classification	O
error	O
probability	O
in	O
statistical	O
pattern	O
recognition	O
problems	O
of	O
control	O
and	O
information	O
theory	O
fu	O
k	O
min	O
p	O
and	O
li	O
t	O
feature	O
selection	O
in	O
pattern	O
recognition	O
ieee	O
transactions	O
on	O
systems	O
science	O
and	O
cybernetics	O
fuchs	O
h	O
abram	O
g	O
and	O
grant	O
e	O
near	O
real-time	O
shaded	O
display	O
of	O
rigid	O
objects	O
computer	O
graphics	O
fuchs	O
h	O
kedem	O
z	O
and	O
naylor	O
b	O
on	O
visible	O
surface	O
generation	O
by	O
a	O
priori	O
tree	O
structures	O
in	O
proceedings	O
siggraph	O
pages	O
published	O
as	O
computer	O
graphics	O
volume	O
fukunaga	O
k	O
and	O
flick	O
t	O
an	O
optimal	O
global	O
nearest	B
neighbor	I
metric	O
ieee	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
fukunaga	O
k	O
and	O
hostetler	O
l	O
optimization	O
of	O
k-nearest	O
neighbor	O
density	O
estimates	O
ieee	O
transactions	O
on	O
information	O
theory	O
fukunaga	O
k	O
and	O
hummels	O
d	O
bayes	B
error	B
estimation	B
using	O
parzen	O
and	O
k-nn	B
procedures	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
fukunaga	O
k	O
and	O
kessel	O
d	O
estimation	B
of	I
classification	O
error	O
ieee	O
transactions	O
on	O
computers	O
fukunaga	O
k	O
and	O
kessel	O
d	O
nonparametric	O
bayes	B
error	B
estimation	B
using	O
sified	O
samples	O
ieee	O
transactions	O
information	O
theory	O
fukunaga	O
k	O
and	O
mantock	O
j	O
nonparametric	O
data	O
reduction	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
fukunaga	O
k	O
and	O
narendra	O
p	O
a	O
branch	O
and	O
bound	O
algorithm	B
for	O
computing	O
nearest	O
neighbors	O
ieee	O
transactions	O
on	O
computers	O
funahashi	O
k	O
on	O
the	O
approximate	O
realization	O
of	O
continuous	O
mappings	O
by	O
neural	O
networks	O
neural	O
networks	O
gabor	O
d	O
a	O
universal	O
nonlinear	O
filter	O
predictor	O
and	O
simulator	O
which	O
optimizes	O
itself	O
proceedings	O
of	O
the	O
institute	O
of	O
electrical	O
engineers	O
gabriel	O
k	O
and	O
sokal	O
r	O
a	O
new	O
statistical	O
approach	O
to	O
geographic	O
variation	B
analysis	O
systematic	O
zoology	O
gaenssler	O
p	O
empirical	B
processes	O
lecture	O
notes-monograph	O
series	O
institute	O
of	O
mathematical	O
statistics	O
hayward	O
ca	O
gaenssler	O
p	O
and	O
stute	O
w	O
empirical	B
processes	O
a	O
survey	O
of	O
results	O
for	O
independent	O
identically	O
distributed	O
random	O
variables	O
annals	O
of	O
probability	O
gallant	O
a	O
nonlinear	O
statistical	O
models	O
john	O
wiley	O
new	O
york	O
references	O
ganesalingam	O
s	O
and	O
mclachlan	O
g	O
error	O
rate	O
estimation	B
on	O
the	O
basis	O
of	O
posterior	O
probabilities	O
pattern	O
recognition	O
garnett	O
j	O
and	O
yau	O
s	O
nonparametric	O
estimation	B
of	I
the	O
bayes	B
error	I
offeature	O
tors	O
using	O
ordered	B
nearest	O
neighbour	O
sets	O
ieee	O
transactions	O
on	O
computers	O
gates	O
g	O
the	O
reduced	O
nearest	B
neighbor	I
rule	B
ieee	O
transactions	O
on	O
information	O
theory	O
gelfand	O
s	O
and	O
delp	O
e	O
on	O
tree	O
structured	O
classifiers	O
in	O
artificial	O
neural	O
networks	O
and	O
statistical	O
pattern	O
recognition	O
old	O
and	O
new	O
connections	O
sethi	O
i	O
and	O
jain	O
a	O
editors	O
pages	O
elsevier	O
science	O
publishers	O
amsterdam	O
gelfand	O
s	O
ravishankar	O
c	O
and	O
delp	O
e	O
an	O
iterative	O
growing	O
and	O
pruning	O
gorithm	O
for	O
classification	O
tree	O
design	O
in	O
proceedings	O
of	O
the	O
ieee	O
international	O
conference	O
on	O
systems	O
man	O
and	O
cybernetics	O
pages	O
ieee	O
press	O
piscataway	O
nj	O
gelfand	O
s	O
ravishankar	O
c	O
and	O
delp	O
e	O
an	O
iterative	O
growing	O
and	O
pruning	O
rithm	O
for	O
classification	O
tree	O
design	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
geman	O
s	O
and	O
hwang	O
c	O
nonparametric	O
maximum	B
likelihood	I
estimation	B
by	O
the	O
method	O
of	O
sieves	O
annals	O
of	O
statistics	O
gessaman	O
m	O
a	O
consistent	O
nonparametric	O
multivariate	O
density	O
estimator	O
based	O
on	O
statistically	B
equivalent	I
blocks	I
annals	O
of	O
mathematical	O
statistics	O
gessaman	O
m	O
and	O
gessaman	O
p	O
a	O
comparison	O
of	O
some	O
multivariate	O
discrimination	O
procedures	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
geva	O
s	O
and	O
sitte	O
j	O
adaptive	O
nearest	B
neighbor	I
pattern	O
classification	O
ieee	O
actions	O
on	O
neural	O
networks	O
gine	O
e	O
and	O
zinn	O
j	O
some	O
limit	O
theorems	O
for	O
empirical	B
processes	O
annals	O
of	O
ability	O
glick	O
n	O
sample-based	O
multinomial	B
classification	O
biometrics	O
glick	O
n	O
sample-based	O
classification	O
procedures	O
related	O
to	O
empiric	O
distributions	O
ieee	O
transactions	O
on	O
information	O
theory	O
glick	O
n	O
additive	O
estimators	O
for	O
probabilities	O
of	O
correct	O
classification	O
pattern	O
recognition	O
goldberg	O
p	O
and	O
jerrum	O
m	O
bounding	O
the	O
vapnik-chervonenkis	B
dimension	B
of	O
concept	O
classes	O
parametrized	O
by	O
real	O
numbers	O
in	O
proceedings	O
of	O
the	O
sixth	O
acm	O
shop	O
on	O
computational	O
learning	B
theory	O
pages	O
association	B
for	O
computing	O
machinery	O
new	O
york	O
goldstein	O
m	O
a	O
two-group	O
classification	O
procedure	O
for	O
multivariate	O
dichotomous	O
responses	O
multivariate	O
behavorial	O
research	O
goldstein	O
m	O
and	O
dillon	O
w	O
discrete	O
discriminant	O
analysis	O
john	O
wiley	O
new	O
york	O
golea	O
m	O
and	O
marchand	O
m	O
a	O
growth	O
algorithm	B
for	O
neural	B
network	I
decision	O
trees	O
europhysics	O
letters	O
goodman	O
r	O
and	O
smyth	O
p	O
decision	O
tree	O
design	O
from	O
a	O
communication	O
theory	O
viewpoint	O
ieee	O
transactions	O
on	O
information	O
theory	O
gordon	O
l	O
and	O
olshen	O
r	O
almost	O
surely	O
consistent	O
nonparametric	O
regression	O
from	O
recursive	B
partitioning	O
schemes	O
ournal	O
of	O
multivariate	O
analysis	O
gordon	O
l	O
and	O
olshen	O
r	O
asymptotically	O
efficient	O
solutions	O
to	O
the	O
classification	O
problem	O
annals	O
of	O
statistics	O
references	O
gordon	O
l	O
and	O
olshen	O
r	O
consistent	O
nonparametric	O
regression	O
from	O
recursive	B
partitioning	O
schemes	O
journal	O
of	O
multivariate	O
analysis	O
gowda	O
k	O
and	O
krishna	O
g	O
the	O
condensed	B
nearest	B
neighbor	I
rule	B
using	O
the	O
concept	O
of	O
mutual	O
nearest	O
neighborhood	O
ieee	O
transactions	O
on	O
information	O
theory	O
greblicki	O
w	O
asymptotically	O
optimal	O
probabilistic	O
algorithms	O
for	O
pattern	O
tion	O
and	O
identification	O
technical	O
report	O
monografie	O
prace	O
naukowe	O
instytutu	O
cybernetyki	O
technicznej	O
politechniki	O
wroclawsjiej	O
no	O
wroclaw	O
poland	O
greblicki	O
w	O
asymptotically	O
optimal	O
pattern	O
recognition	O
procedures	O
with	O
density	O
estimates	O
ieee	O
transactions	O
on	O
information	O
theory	O
greblicki	O
w	O
pattern	O
recognition	O
procedures	O
with	O
nonparametric	O
density	O
estimates	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
greblicki	O
w	O
asymptotic	O
efficiency	O
of	O
classifying	O
procedures	O
using	O
the	O
hermite	B
series	O
estimate	O
of	O
multivariate	O
probability	O
densities	O
ieee	O
transactions	O
on	O
information	O
theory	O
greblicki	O
w	O
krzyzak	O
a	O
and	O
pawlak	O
m	O
distribution-free	O
pointwise	O
consistency	B
of	O
kernel	B
regression	O
estimate	O
annals	O
of	O
statistics	O
greblicki	O
w	O
and	O
pawlak	O
m	O
classification	O
using	O
the	O
fourier	O
series	O
estimate	O
of	O
multivariate	O
density	O
functions	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
greblicki	O
w	O
and	O
pawlak	O
m	O
a	O
classification	O
procedure	O
using	O
the	O
multiple	O
fourier	O
series	O
information	O
sciences	O
greblicki	O
w	O
and	O
pawlak	O
m	O
almost	O
sure	O
convergence	O
of	O
classification	O
procedures	O
using	O
hermite	B
series	O
density	O
estimates	O
pattern	O
recognition	O
letters	O
greblicki	O
w	O
and	O
pawlak	O
m	O
pointwise	O
consistency	B
of	O
the	O
hermite	B
series	O
density	O
estimate	O
statistics	O
and	O
probability	O
letters	O
greblicki	O
w	O
and	O
pawlak	O
m	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
bayes	O
risk	O
consistency	B
of	O
a	O
recursive	B
kernel	B
classification	O
rule	B
ieee	O
transactions	O
on	O
information	O
theory	O
grenander	O
u	O
abstract	O
inference	O
john	O
wiley	O
new	O
york	O
grimmett	O
g	O
and	O
stirzaker	O
d	O
probability	O
and	O
random	O
processes	O
oxford	O
sity	O
press	O
oxford	O
guo	O
h	O
and	O
gelfand	O
s	O
classification	O
trees	O
with	O
neural	B
network	I
feature	B
extraction	I
ieee	O
transactions	O
on	O
neural	O
networks	O
gyorfi	O
l	O
an	O
upper	O
bound	O
of	O
error	O
probabilities	O
for	O
multihypothesis	O
testing	O
and	O
its	O
application	O
in	O
adaptive	O
pattern	O
recognition	O
problems	O
of	O
control	O
and	O
information	O
theory	O
gyorfi	O
l	O
on	O
the	O
rate	B
of	I
convergence	I
of	B
nearest	B
neighbor	I
rules	I
ieee	O
transactions	O
on	O
information	O
theory	O
gyorfi	O
l	O
recent	O
results	O
on	O
nonparametric	O
regression	O
estimate	O
and	O
multiple	O
sification	O
problems	O
of	O
control	O
and	O
information	O
theory	O
gyorfi	O
l	O
adaptive	O
linear	O
procedures	O
under	O
general	O
conditions	O
ieee	O
transactions	O
on	O
information	O
theory	O
gyorfi	O
l	O
and	O
gyorfi	O
z	O
on	O
the	O
nonparametric	O
estimate	O
of	O
a	O
posteriori	O
probabilities	O
of	O
simple	O
statistical	O
hypotheses	O
in	O
colloquia	O
mathematica	O
societatis	O
janos	O
bolyai	O
topics	O
in	O
information	O
theory	O
pages	O
keszthely	O
hungary	O
gyorfi	O
l	O
and	O
gyorfi	O
z	O
an	O
upper	O
bound	O
on	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
k-nearest	O
neighbor	O
rule	B
for	O
multiple	O
classes	O
ieee	O
transactions	O
on	O
information	O
theory	O
references	O
gyorfi	O
l	O
gyorfi	O
z	O
and	O
vajda	O
i	O
bayesian	O
decision	B
with	I
rejection	I
problems	O
of	O
control	O
and	O
information	O
theory	O
gyorfi	O
l	O
and	O
vajda	O
i	O
upper	O
bound	O
on	O
the	O
error	O
probability	O
of	O
detection	O
in	O
gaussian	B
noise	I
problems	O
of	O
control	O
and	O
information	O
theory	O
haagerup	O
u	O
les	O
meilleures	O
constantes	O
de	O
linegalite	O
de	O
khintchine	O
comptes	O
rendus	O
de	O
academie	O
des	O
sciences	O
de	O
paris	O
a	O
habbema	O
j	O
hermans	O
j	O
and	O
van	O
den	O
broek	O
k	O
a	O
stepwise	O
discriminant	O
analysis	O
program	O
using	O
density	B
estimation	B
in	O
compstat	O
bruckmann	O
g	O
editor	O
pages	O
physica	O
verlag	O
wien	O
hagerup	O
t	O
and	O
riib	O
c	O
a	O
guided	O
tour	O
of	O
chernoff	O
bounds	O
information	O
processing	O
letters	O
hall	O
p	O
on	O
nonparametric	O
multivariate	O
binary	B
discrimination	O
biometrika	O
hall	O
p	O
on	O
projection	B
pursuit	I
regression	O
annals	O
of	O
statistics	O
hall	O
p	O
and	O
wand	O
m	O
on	O
nonparametric	O
discrimination	O
using	O
density	O
differences	O
biometrika	O
hand	O
d	O
discrimination	O
and	O
classification	O
john	O
wiley	O
chichester	O
u	O
k	O
hand	O
d	O
recent	O
advances	O
in	O
error	O
rate	O
estimation	B
pattern	O
recognition	O
letters	O
hardie	O
w	O
and	O
marron	O
j	O
optimal	O
bandwidth	O
selection	O
in	O
nonparametric	O
regression	B
function	I
estimation	B
annals	O
of	O
statistics	O
hart	O
p	O
the	O
condensed	B
nearest	B
neighbor	I
rule	B
ieee	O
transactions	O
on	O
information	O
theory	O
hartigan	O
j	O
clustering	B
algorithms	O
john	O
wiley	O
new	O
york	O
hartmann	O
c	O
varshney	O
p	O
mehrotra	O
k	O
and	O
gerberich	O
c	O
application	O
of	O
formation	O
theory	O
to	O
the	O
construction	O
of	O
efficient	O
decision	O
trees	O
ieee	O
transactions	O
on	O
information	O
theory	O
hashlamoun	O
w	O
varshney	O
p	O
and	O
samarasooriya	O
v	O
a	O
tight	O
upper	O
bound	O
on	O
the	O
bayesian	O
probability	O
of	O
error	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
hastie	O
t	O
and	O
tibshirani	O
r	O
generalized	B
additive	O
models	O
chapman	O
and	O
hall	O
london	O
u	O
k	O
haussler	O
d	O
sphere	B
packing	O
numbers	O
for	O
subsets	O
of	O
the	O
boolean	O
n-cube	O
with	O
bounded	O
v	O
apnik	O
dimension	B
technical	O
report	O
computer	O
research	O
oratory	O
university	O
of	O
california	O
santa	O
cruz	O
haussler	O
d	O
decision	O
theoretic	O
generalizations	O
of	O
the	O
pac	O
model	O
for	O
neural	O
net	O
and	O
other	O
learning	B
applications	O
information	O
and	O
computation	O
haussler	O
d	O
littlestone	O
n	O
and	O
warmuth	O
m	O
predicting	O
functions	O
from	O
randomly	O
drawn	O
points	O
in	O
proceedings	O
of	O
the	O
ieee	O
symposium	O
on	O
the	O
foundations	O
of	O
computer	O
science	O
pages	O
ieee	O
computer	O
society	O
press	O
los	O
alamitos	O
ca	O
hecht-nielsen	O
r	O
kolmogorovs	O
mapping	O
network	O
existence	O
theorem	O
in	O
ieee	O
first	O
international	O
conference	O
on	O
neural	O
networks	O
volume	O
pages	O
ieee	O
piscataway	O
nj	O
hellman	O
m	O
the	O
nearest	B
neighbor	I
classification	O
rule	B
with	O
a	O
reject	O
option	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
henrichon	O
e	O
and	O
fu	O
k	O
a	O
nonparametric	O
partitioning	O
procedure	O
for	O
pattern	O
sification	O
ieee	O
transactions	O
on	O
computers	O
references	O
hertz	O
j	O
krogh	O
a	O
and	O
palmer	O
r	O
introduction	O
to	O
the	O
theory	O
of	O
neural	O
tation	O
addison-wesley	O
redwood	O
city	O
ca	O
hills	O
m	O
allocation	O
rules	O
and	O
their	O
error	O
rates	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
hjort	O
n	O
contribution	O
to	O
the	O
discussion	O
of	O
a	O
paper	O
by	O
p	O
diaconis	O
and	O
freeman	O
annals	O
of	O
statistics	O
hjort	O
n	O
notes	O
on	O
the	O
theory	O
of	O
statistical	O
symbol	O
recognition	O
technical	O
report	O
norwegian	O
computing	O
centre	O
oslo	O
hoeffding	O
w	O
probability	O
inequalities	O
for	O
sums	O
of	O
bounded	O
random	O
variables	O
nal	O
of	O
the	O
american	O
statistical	O
association	B
holmstrom	O
l	O
and	O
klemeui	O
j	O
asymptotic	O
bounds	O
for	O
the	O
expected	O
error	O
of	O
a	O
multivariate	O
kernel	B
density	O
estimator	O
journal	O
of	O
multivariate	O
analysis	O
hora	O
s	O
and	O
wilcox	O
j	O
estimation	B
of	I
error	O
rates	O
in	O
several-population	O
discriminant	O
analysis	O
journal	O
of	O
marketing	O
research	O
horibe	O
y	O
on	O
zero	O
error	O
probability	O
of	O
binary	B
decisions	O
ieee	O
transactions	O
on	O
information	O
theory	O
home	O
b	O
and	O
hush	O
d	O
on	O
the	O
optimality	O
of	O
the	O
sigmoid	B
perceptron	B
in	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
volume	O
pages	O
lawrence	O
erlbaum	O
associates	O
hillsdale	O
nj	O
hornik	O
k	O
approximation	O
capabilities	O
of	O
multilayer	B
feedforward	O
networks	O
neural	O
networks	O
hornik	O
k	O
some	O
new	O
results	O
on	O
neural	B
network	I
approximation	O
neural	O
networks	O
hornik	O
k	O
stinchcombe	O
m	O
and	O
white	O
h	O
multi-layer	O
feedforward	O
networks	O
are	O
universal	O
approximators	O
neural	O
networks	O
huber	O
p	O
projection	B
pursuit	I
annals	O
of	O
statistics	O
hudimoto	O
h	O
a	O
note	O
on	O
the	O
probability	O
of	O
the	O
correct	O
classification	O
when	O
the	O
distributions	O
are	O
not	O
specified	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
ito	O
t	O
note	O
on	O
a	O
class	O
of	O
statistical	O
recognition	O
functions	O
ieee	O
transactions	O
on	O
computers	O
ito	O
t	O
approximate	O
error	O
bounds	O
in	O
pattern	O
recognition	O
in	O
machine	O
intelligence	O
meltzer	O
b	O
and	O
mitchie	O
d	O
editors	O
pages	O
edinburgh	O
university	O
press	O
burgh	O
ivakhnenko	O
a	O
the	O
group	O
method	O
of	O
data	O
handling-a	O
rival	O
of	O
the	O
method	O
of	O
stochastic	B
approximation	I
soviet	O
automatic	B
control	O
ivakhnenko	O
a	O
polynomial	B
theory	O
of	O
complex	O
systems	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
ivakhnenko	O
a	O
konovalenko	O
v	O
tulupchuk	O
y	O
and	O
tymchenko	O
i	O
the	O
group	B
method	I
of	I
data	I
handling	I
in	O
pattern	O
recognition	O
and	O
decision	O
problems	O
soviet	O
automatic	B
control	O
ivakhnenko	O
a	O
petrache	O
g	O
and	O
krasytskyy	O
m	O
a	O
gmdh	O
algorithm	B
with	O
random	O
selection	O
of	O
pairs	O
soviet	O
automatic	B
control	O
jain	O
a	O
dubes	O
r	O
and	O
chen	O
c	O
bootstrap	B
techniques	O
for	O
error	B
estimation	B
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
jeffreys	O
h	O
theory	O
of	O
probability	O
clarendon	O
press	O
oxford	O
johnson	O
d	O
and	O
preparata	O
f	O
the	O
densest	O
hemisphere	O
problem	O
theoretical	B
puter	O
science	O
references	O
kurkova	O
v	O
kolmogorovs	O
theorem	O
and	O
multilayer	B
neural	O
networks	O
neural	O
works	O
kailath	O
t	O
the	O
divergence	O
and	O
bhattacharyya	O
distance	O
measures	O
in	O
signal	O
detection	O
ieee	O
transactions	O
on	O
communication	O
technology	O
kanal	O
l	O
patterns	O
in	O
pattern	O
recognition	O
ieee	O
transactions	O
on	O
information	O
theory	O
kaplan	O
m	O
the	O
uses	O
of	O
spatial	O
coherence	O
in	O
ray	O
tracing	O
siggraph	O
course	O
notes	O
karlin	O
a	O
total	B
positivity	I
volume	O
stanford	O
university	O
press	O
stanford	O
ca	O
karp	O
r	O
probabilistic	O
analysis	O
of	O
algorithms	O
class	O
notes	O
university	O
of	O
california	O
berkeley	O
karpinski	O
m	O
and	O
macintyre	O
a	O
quadratic	O
bounds	O
for	O
vc	B
dimension	B
of	O
sigmoidal	O
neural	O
networks	O
submitted	O
to	O
the	O
acm	O
symposium	O
on	O
theory	O
of	O
computing	O
kazmierczak	O
h	O
and	O
steinbuch	O
k	O
adaptive	O
systems	O
in	O
pattern	O
recognition	O
ieee	O
transactions	O
on	O
electronic	O
computers	O
kemp	O
r	O
fundamentals	O
of	O
the	O
average	O
case	O
analysis	O
of	O
particular	O
algorithms	O
b	O
g	O
teubner	O
stuttgart	O
kemperman	O
j	O
on	O
the	O
optimum	O
rate	O
of	O
transmitting	O
information	O
in	O
probability	O
and	O
information	O
theory	O
pages	O
springer	O
lecture	O
notes	O
in	O
mathematics	O
verlag	O
berlin	O
kiefer	O
j	O
and	O
wolfowitz	O
j	O
stochastic	O
estimation	B
of	I
the	O
maximum	O
of	O
a	O
regression	B
function	I
annals	O
of	O
mathematical	O
statistics	O
kim	O
b	O
and	O
park	O
s	O
a	O
fast	O
k-nearest	O
neighbor	O
finding	O
algorithm	B
based	O
on	O
the	O
ordered	B
partition	B
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
kittler	O
j	O
and	O
devijver	O
p	O
an	O
efficient	O
estimator	O
of	O
pattern	O
recognition	O
system	O
error	O
probability	O
pattern	O
recognition	O
knoke	O
j	O
the	O
robust	O
estimation	B
of	I
classification	O
error	O
rates	O
computers	O
and	O
ematics	O
with	O
applications	O
kohonen	O
t	O
self-organization	O
and	O
associative	O
memory	O
springer-verlag	O
berlin	O
kohonen	O
t	O
statistical	O
pattern	O
recognition	O
revisited	O
in	O
advanced	O
neural	O
ers	O
eckmiller	O
r	O
editor	O
pages	O
north-holland	O
amsterdam	O
kolmogorov	O
a	O
on	O
the	O
representation	O
of	O
continuous	O
functions	O
of	O
many	O
variables	O
by	O
superposition	O
of	O
continuous	O
functions	O
of	O
one	O
variable	B
and	O
addition	O
doklady	O
akademii	O
nauk	O
ussr	O
kolmogorov	O
a	O
and	O
tikhomirov	O
v	O
e-entropy	O
and	O
e-capacity	O
of	O
sets	O
in	O
function	O
spaces	O
translations	O
of	O
the	O
american	O
mathematical	O
society	O
koutsougeras	O
c	O
and	O
papachristou	O
c	O
training	O
of	O
a	O
neural	B
network	I
for	O
pattern	O
classification	O
based	O
on	O
an	O
entropy	B
measure	O
in	O
proceedings	O
of	O
ieee	O
international	O
ference	O
on	O
neural	O
networks	O
volume	O
pages	O
ieee	O
san	O
diego	O
section	O
san	O
diego	O
ca	O
kraaijveld	O
m	O
and	O
duin	O
r	O
generalization	O
capabilities	O
of	O
minimal	O
kernel-based	O
methods	O
in	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
volume	O
pages	O
piscataway	O
nj	O
kronmal	O
r	O
and	O
tarter	O
m	O
the	O
estimation	B
of	I
probability	O
densities	O
and	O
cumulatives	O
by	O
fourier	O
series	O
methods	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
krzanowski	O
w	O
a	O
comparison	O
between	O
two	O
distance-based	O
discriminant	O
principles	O
journal	O
of	O
classification	O
references	O
krzyzak	O
a	O
classification	O
procedures	O
using	O
multivariate	O
variable	B
kernel	B
density	O
estimate	O
pattern	O
recognition	O
letters	O
krzyzak	O
a	O
the	O
rates	O
of	O
convergence	O
of	O
kernel	B
regression	O
estimates	O
and	O
tion	O
rules	O
ieee	O
transactions	O
on	O
information	O
theory	O
krzyzak	O
a	O
on	O
exponential	B
bounds	O
on	O
the	O
bayes	O
risk	O
of	O
the	O
kernel	B
classification	O
rule	B
ieee	O
transactions	O
on	O
information	O
theory	O
krzyzak	O
a	O
linder	O
t	O
and	O
lugosi	O
g	O
nonparametric	O
estimation	B
and	O
classification	O
using	O
radial	B
basis	B
function	I
nets	O
and	O
empirical	B
risk	I
minimization	I
ieee	O
transactions	O
on	O
neural	O
networks	O
to	O
appear	O
krzyzak	O
a	O
and	O
pawlak	O
m	O
universal	B
consistency	B
results	O
for	O
wolverton-wagner	O
regression	B
function	I
estimate	O
with	O
application	O
in	O
discrimination	O
problems	O
of	O
control	O
and	O
information	O
theory	O
krzyzak	O
a	O
and	O
pawlak	O
m	O
almost	O
everywhere	O
convergence	O
of	O
recursive	B
kernel	B
regression	B
function	I
estimates	O
ieee	O
transactions	O
on	O
information	O
theory	O
krzyzak	O
a	O
and	O
pawlak	O
m	O
distribution-free	O
consistency	B
of	O
a	O
nonparametric	O
kernel	B
regression	O
estimate	O
and	O
classification	O
ieee	O
transactions	O
on	O
information	O
theory	O
kulkarni	O
s	O
problems	O
of	O
computational	O
and	O
information	O
complexity	O
in	O
machine	O
vision	O
and	O
learning	B
phd	O
thesis	O
department	O
of	O
electrical	O
engineering	O
and	O
computer	O
science	O
mit	O
cambridge	O
ma	O
kullback	O
s	O
a	O
lower	O
bound	O
for	O
discrimination	O
information	O
in	O
terms	O
of	O
variation	B
ieee	O
transactions	O
on	O
information	O
theory	O
kullback	O
s	O
and	O
leibler	O
a	O
on	O
information	O
and	O
sufficiency	O
annals	O
of	O
mathematical	O
statistics	O
kushner	O
h	O
approximation	O
and	O
weak	B
convergence	O
methods	O
for	O
random	O
processes	O
with	O
applications	O
to	O
stochastic	O
systems	O
theory	O
mit	O
press	O
cambridge	O
ma	O
lachenbruch	O
p	O
an	O
almost	O
unbiased	O
method	O
of	O
obtaining	O
confidence	B
intervals	O
for	O
the	O
probability	O
of	O
misclassification	O
in	O
discriminant	O
analysis	O
biometrics	O
lachenbruch	O
p	O
and	O
mickey	O
m	O
estimation	B
of	I
error	O
rates	O
in	O
discriminant	O
analysis	O
technometrics	O
lecam	O
l	O
on	O
the	O
assumptions	O
used	O
to	O
prove	O
asymptotic	O
normality	O
of	B
maximum	B
likelihood	I
estimates	O
annals	O
of	O
mathematical	O
statistics	O
lecam	O
l	O
convergence	O
of	O
estimates	O
under	O
dimensionality	O
restrictions	O
annals	O
of	O
statistics	O
li	O
x	O
and	O
dubes	O
r	O
tree	O
classifier	B
design	O
with	O
a	O
permutation	O
statistic	O
pattern	O
recognition	O
lin	O
y	O
and	O
fu	O
k	O
automatic	B
classification	O
of	O
cervical	O
cells	O
using	O
a	O
binary	B
tree	O
classifier	B
pattern	O
recognition	O
linde	O
y	O
buzo	O
a	O
and	O
gray	O
r	O
an	O
algorithm	B
for	O
vector	O
quantizer	O
design	O
ieee	O
transactions	O
on	O
communications	O
linder	O
t	O
lugosi	O
g	O
and	O
zeger	O
k	O
rates	O
of	O
convergence	O
in	O
the	O
source	O
coding	O
rem	O
empirical	B
quantizer	O
design	O
and	O
universal	O
lossy	O
source	O
coding	O
ieee	O
transactions	O
on	O
information	O
theory	O
lissack	O
t	O
and	O
fu	O
k	O
error	B
estimation	B
in	O
pattern	O
recognition	O
via	O
za	O
distance	O
between	O
posterior	O
density	O
functions	O
ieee	O
transactions	O
on	O
information	O
theory	O
ljung	O
l	O
pflug	O
g	O
and	O
walk	O
h	O
stochastic	B
approximation	I
and	O
optimization	O
of	O
random	O
systems	O
birkhauser	O
basel	O
boston	O
berlin	O
references	O
lloyd	O
s	O
least	O
squares	O
quantization	B
in	O
pcm	O
ieee	O
transactions	O
on	O
information	O
theory	O
loftsgaarden	O
d	O
and	O
quesenberry	O
c	O
a	O
nonparametric	O
estimate	O
of	O
a	O
multivariate	O
density	O
function	O
annals	O
of	O
mathematical	O
statistics	O
logan	O
b	O
the	O
uncertainty	O
principle	O
in	O
reconstructing	O
functions	O
from	O
projections	O
duke	O
mathematical	O
journal	O
logan	O
b	O
and	O
shepp	O
l	O
optimal	O
reconstruction	O
of	O
a	O
function	O
from	O
its	O
projections	O
duke	O
mathematical	O
journal	O
loh	O
w	O
and	O
vanichsetakul	O
n	O
tree-structured	O
classification	O
via	O
generalized	B
criminant	O
analysis	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
loizou	O
g	O
and	O
maybank	O
s	O
the	O
nearest	B
neighbor	I
and	O
the	O
bayes	B
error	I
rates	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
lorentz	O
g	O
the	O
thirteenth	O
problem	O
of	O
hilbert	O
in	O
proceedings	O
of	O
symposia	O
in	O
pure	O
mathematics	O
volume	O
pages	O
providence	O
ri	O
lugosi	O
g	O
learning	B
with	O
an	O
unreliable	O
teacher	O
pattern	O
recognition	O
lugosi	O
g	O
and	O
nobel	O
a	O
consistency	B
of	O
data-driven	O
histogram	O
methods	O
for	O
density	B
estimation	B
and	O
classification	O
annals	O
of	O
statistics	O
lugosi	O
g	O
and	O
pawlak	O
m	O
on	O
the	O
posterior-probability	O
estimate	O
of	O
the	O
error	O
rate	O
of	O
nonparametric	O
classification	O
rules	O
ieee	O
transactions	O
on	O
information	O
theory	O
lugosi	O
g	O
and	O
zeger	O
k	O
nonparametric	O
estimation	B
via	O
empirical	B
risk	I
minimization	I
ieee	O
transactions	O
on	O
information	O
theory	O
lugosi	O
g	O
and	O
zeger	O
k	O
concept	O
learning	B
using	O
complexity	B
regularization	I
ieee	O
transactions	O
on	O
information	O
theory	O
lukacs	O
e	O
and	O
laha	O
r	O
applications	O
of	O
characteristic	O
functions	O
in	O
probability	O
theory	O
griffin	O
london	O
lunts	O
a	O
and	O
brailovsky	O
v	O
evaluation	O
of	O
attributes	O
obtained	O
in	O
statistical	O
decision	O
rules	O
engineering	O
cybernetics	O
maass	O
w	O
bounds	O
for	O
the	O
computational	O
power	O
and	O
learning	B
complexity	O
of	O
log	O
neural	O
nets	O
in	O
proceedings	O
of	O
the	O
annual	O
acm	O
symposium	O
on	O
the	O
theory	O
of	O
computing	O
pages	O
association	B
of	O
computing	O
machinery	O
new	O
york	O
maass	O
w	O
neural	O
nets	O
with	O
superlinear	O
vc-dimension	O
neural	O
computation	O
macintyre	O
a	O
and	O
sontag	O
e	O
finiteness	O
results	O
for	O
sigmoidal	O
networks	O
in	O
proceedings	O
of	O
the	O
annual	O
acm	O
symposium	O
on	O
the	O
theory	O
of	O
computing	O
pages	O
association	B
of	O
computing	O
machinery	O
new	O
york	O
mack	O
y	O
local	O
properties	O
of	O
k	O
neighbor	O
regression	O
estimates	O
siam	O
journal	O
on	O
algebraic	O
and	O
discrete	O
methods	O
mahalanobis	O
p	O
on	O
the	O
generalized	B
distance	O
in	O
statistics	O
proceedings	O
of	O
the	O
national	O
institute	O
of	O
sciences	O
of	O
india	O
mahalanobis	O
p	O
a	O
method	O
offractile	O
graphical	O
analysis	O
sankhya	O
series	O
a	O
marron	O
j	O
optimal	O
rates	O
of	O
convergence	O
to	O
bayes	O
risk	O
in	O
nonparametric	O
nation	O
annals	O
of	O
statistics	O
massart	O
p	O
vitesse	O
de	O
convergence	O
dans	O
le	O
theoreme	O
de	O
la	O
limite	O
centrale	O
pour	O
le	O
processus	O
empirique	O
phd	O
thesis	O
universite	O
paris-sud	O
orsay	O
france	O
massart	O
p	O
the	O
tight	O
constant	O
in	O
the	O
dvoretzky-kiefer-wolfowitz	O
inequality	B
annals	O
of	O
probability	O
references	O
mathai	O
a	O
and	O
rathie	O
p	O
basic	O
concepts	O
in	O
information	O
theory	O
and	O
statistics	O
wiley	O
eastern	O
ltd	O
new	O
delhi	O
matloff	O
n	O
and	O
pruitt	O
r	O
the	O
asymptotic	O
distribution	O
of	O
an	O
estimator	O
of	O
the	O
bayes	B
error	I
rate	O
pattern	O
recognition	O
letters	O
matula	O
d	O
and	O
sokal	O
r	O
properties	O
of	O
gabriel	O
graphs	O
relevant	O
to	O
geographic	O
tion	O
research	O
and	O
the	O
clustering	B
of	O
points	O
in	O
the	O
plane	O
geographical	O
analysis	O
matushita	O
k	O
decision	O
rule	B
based	O
on	O
distance	O
for	O
the	O
classification	O
problem	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
matushita	O
k	O
discrimination	O
and	O
the	O
affinity	O
of	O
distributions	O
in	O
discriminant	O
ysis	O
and	O
applications	O
cacoullos	O
t	O
editor	O
pages	O
academic	O
press	O
new	O
york	O
max	O
j	O
quantizing	O
for	O
minimum	O
distortion	O
ieee	O
transactions	O
on	O
information	O
theory	O
mcdiarmid	O
c	O
on	O
the	O
method	B
of	I
bounded	I
differences	I
in	O
surveys	O
in	O
combinatorics	O
pages	O
cambridge	O
university	O
press	O
cambridge	O
mclachlan	O
g	O
the	O
bias	B
of	I
the	O
apparent	O
error	O
rate	O
in	O
discriminant	O
analysis	O
biometrika	O
mclachlan	O
g	O
discriminant	O
analysis	O
and	O
statistical	O
pattern	O
recognition	O
john	O
wiley	O
new	O
york	O
meisel	O
w	O
potential	O
functions	O
in	O
mathematical	O
pattern	O
recognition	O
ieee	O
tions	O
on	O
computers	O
meisel	O
w	O
computer	O
oriented	O
approaches	O
to	O
pattern	O
recognition	O
academic	O
press	O
new	O
york	O
meisel	O
w	O
parsimony	O
in	O
neural	O
networks	O
in	O
international	O
conference	O
on	O
neural	O
networks	O
pages	O
lawrence	O
erlbaum	O
associates	O
hillsdale	O
nj	O
meisel	O
w	O
and	O
michalopoulos	O
d	O
a	O
partitioning	O
algorithm	B
with	O
application	O
in	O
pattern	O
classification	O
and	O
the	O
optimization	O
of	O
decision	O
tree	O
ieee	O
transactions	O
on	O
puters	O
michel-briand	O
c	O
and	O
milhaud	O
x	O
asymptotic	O
behavior	O
of	O
the	O
aid	O
method	O
nical	O
report	O
universite	O
montpellier	O
montpellier	O
mielniczuk	O
j	O
and	O
tyrcha	O
j	O
consistency	B
of	O
multilayer	B
perceptron	B
regression	O
mators	O
neural	O
networks	O
to	O
appear	O
minsky	O
m	O
steps	O
towards	O
artificial	O
intelligence	O
in	O
proceedings	O
of	O
the	O
ire	O
ume	O
pages	O
minsky	O
m	O
and	O
papert	O
s	O
perceptrons	O
an	O
introduction	O
to	O
computational	O
geometry	O
mit	O
press	O
cambridge	O
ma	O
mitchell	O
a	O
and	O
krzanowski	O
w	O
the	O
mahalanobis	B
distance	I
and	O
elliptic	O
tions	O
biometrika	O
mizoguchi	O
r	O
kizawa	O
m	O
and	O
shimura	O
m	O
piecewise	O
linear	O
discriminant	O
tions	O
in	O
pattern	O
recognition	O
systems-computers-controls	O
moody	O
j	O
and	O
darken	O
j	O
fast	O
learning	B
in	O
networks	O
of	O
locally-tuned	O
processing	O
units	O
neural	O
computation	O
moore	O
d	O
whitsitt	O
s	O
and	O
landgrebe	O
d	O
variance	O
comparisons	O
for	O
unbiased	O
estimators	O
of	O
probability	O
of	O
correct	O
classification	O
ieee	O
transactions	O
on	O
information	O
theory	O
morgan	O
j	O
and	O
sonquist	O
j	O
problems	O
in	O
the	O
analysis	O
of	O
survey	O
data	O
and	O
a	O
proposal	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
references	O
mui	O
and	O
fu	O
k	O
automated	O
classification	O
of	O
nucleated	O
blood	O
cells	O
using	O
a	O
binary	B
tree	O
classifier	B
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
myles	O
and	O
hand	O
d	O
the	O
multi-class	O
metric	O
problem	O
in	O
nearest	O
neighbour	O
crimination	O
rules	O
pattern	O
recognition	O
nadaraya	O
e	O
on	O
estimating	O
regression	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
nadaraya	O
e	O
remarks	O
on	O
nonparametric	O
estimates	O
for	O
density	O
functions	O
and	O
sion	O
curves	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
narendra	O
p	O
and	O
fukunaga	O
k	O
a	O
branch	O
and	O
bound	O
algorithm	B
for	O
feature	O
subset	O
selection	O
ieee	O
transactions	O
on	O
computers	O
natarajan	O
b	O
machine	O
learning	B
a	O
theoretical	B
approach	O
morgan	O
kaufmann	O
san	O
mateo	O
ca	O
nevelson	O
m	O
and	O
khasminskii	O
r	O
stochastic	B
approximation	I
and	O
recursive	B
timation	O
translations	O
of	O
mathematical	O
monographs	O
vol	O
american	O
mathematical	O
society	O
providence	O
ri	O
niemann	O
h	O
and	O
goppert	O
r	O
an	O
efficient	O
branch-and-bound	O
nearest	O
neighbour	O
classifier	B
pattern	O
recognition	O
letters	O
nilsson	O
n	O
learning	B
machines	O
foundations	O
of	O
trainable	O
pattern	O
classifying	O
tems	O
mcgraw-hill	O
new	O
york	O
nishizeki	O
t	O
and	O
chiba	O
n	O
planar	O
graphs	O
theory	O
and	O
algorithms	O
north	O
holland	O
amsterdam	O
nobel	O
a	O
histogram	O
regression	O
estimates	O
using	O
data-dependent	B
partitions	O
cal	O
report	O
beckman	O
institute	O
university	O
of	O
illinois	O
urbana-champaign	O
nobel	O
a	O
on	O
uniform	O
laws	O
of	O
averages	O
phd	O
thesis	O
department	O
of	O
statistics	O
stanford	O
university	O
stanford	O
ca	O
nolan	O
d	O
and	O
pollard	O
d	O
u-processes	O
rates	O
of	O
convergence	O
annals	O
of	O
statistics	O
okamoto	O
m	O
some	O
inequalities	O
relating	O
to	O
the	O
partial	O
sum	O
of	O
binomial	B
probabilities	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
olshen	O
r	O
comments	O
on	O
a	O
paper	O
by	O
c	O
l	O
stone	O
annals	O
of	O
statistics	O
ott	O
and	O
kronmal	O
r	O
some	O
classification	O
procedures	O
for	O
multivariate	O
binary	B
data	O
using	O
orthogonal	O
functions	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
papadimitriou	O
c	O
and	O
bentley	O
a	O
worst-case	O
analysis	O
of	O
nearest	B
neighbor	I
ing	O
by	O
projection	O
in	O
automata	O
languages	O
and	O
programming	O
pages	O
lecture	O
notes	O
in	O
computer	O
science	O
springer-verlag	O
berlin	O
park	O
and	O
sandberg	O
i	O
universal	O
approximation	O
using	O
radial-basis-function	O
works	O
neural	O
computation	O
park	O
and	O
sandberg	O
i	O
approximation	O
and	O
radial-basis-function	O
networks	O
neural	O
computation	O
park	O
y	O
and	O
sklansky	O
automated	O
design	O
of	O
linear	O
tree	O
classifiers	O
pattern	O
nition	O
parthasarathy	O
k	O
and	O
bhattacharya	O
p	O
some	O
limit	O
theorems	O
in	O
regression	O
theory	O
sankhya	O
series	O
a	O
parzen	O
e	O
on	O
the	O
estimation	B
of	I
a	O
probability	O
density	O
function	O
and	O
the	O
mode	O
annals	O
of	O
mathematical	O
statistics	O
references	O
patrick	O
e	O
distribution-free	O
minimum	O
conditional	O
risk	O
learning	B
systems	O
technical	O
report	O
purdue	O
university	O
lafayette	O
in	O
patrick	O
e	O
and	O
fischer	O
f	O
a	O
generalized	B
k-nearest	O
neighbor	O
rule	B
information	O
and	O
patrick	O
e	O
and	O
fisher	O
f	O
introduction	O
to	O
the	O
performance	O
of	O
distribution-free	O
conditional	O
risk	O
learning	B
systems	O
technical	O
report	O
purdue	O
university	O
lafayette	O
in	O
pawlak	O
m	O
on	O
the	O
asymptotic	O
properties	O
of	O
smoothed	B
estimators	O
of	O
the	O
classification	O
error	O
rate	O
pattern	O
recognition	O
payne	O
h	O
and	O
meisel	O
w	O
an	O
algorithm	B
for	O
constructing	O
optimal	O
binary	B
decision	O
trees	O
ieee	O
transactions	O
on	O
computers	O
pearl	O
j	O
capacity	O
and	O
error	O
estimates	O
for	O
boolean	O
classifiers	O
with	O
limited	O
complexity	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
penrod	O
c	O
and	O
wagner	O
t	O
another	O
look	O
at	O
the	O
edited	B
nearest	B
neighbor	I
rule	B
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
peterson	O
d	O
some	O
convergence	O
properties	O
of	O
a	O
nearest	B
neighbor	I
decision	O
rule	B
ieee	O
transactions	O
on	O
information	O
theory	O
petrov	O
v	O
sums	O
of	O
independent	O
random	O
variables	O
springer-verlag	O
berlin	O
pippenger	O
n	O
information	O
theory	O
and	O
the	O
complexity	O
of	O
boolean	O
functions	O
ematical	O
systems	O
theory	O
poggio	O
t	O
and	O
girosi	O
f	O
a	O
theory	O
of	O
networks	O
for	O
approximation	O
and	O
learning	B
proceedings	O
of	O
the	O
ieee	O
pollard	O
d	O
strong	B
consistency	B
of	O
k-means	B
clustering	B
annals	O
of	O
statistics	O
pollard	O
d	O
quantization	B
and	O
the	O
method	O
of	O
k-means	O
ieee	O
transactions	O
on	O
mation	O
theory	O
pollard	O
d	O
convergence	O
of	O
stochastic	O
processes	O
springer-verlag	O
new	O
york	O
pollard	O
d	O
rates	O
of	O
uniform	O
almost	O
sure	O
convergence	O
for	O
empirical	B
processes	O
dexed	O
by	O
unbounded	O
classes	O
of	O
functions	O
manuscript	O
pollard	O
d	O
empirical	B
processes	O
theory	O
and	O
applications	O
nsf-cbms	O
regional	O
conference	O
series	O
in	O
probability	O
and	O
statistics	O
institute	O
of	O
mathematical	O
statistics	O
hayward	O
ca	O
powell	O
m	O
radial	O
basis	O
functions	O
for	O
multivariable	O
interpolation	O
a	O
review	O
rithms	O
for	O
approximation	O
clarendon	O
press	O
oxford	O
prep	O
arata	O
f	O
and	O
shamos	O
m	O
computational	O
geometry-an	O
introduction	O
verlag	O
new	O
york	O
psaltis	O
d	O
snapp	O
r	O
and	O
venkatesh	O
s	O
on	O
the	O
finite	O
sample	O
performance	O
of	O
the	O
nearest	B
neighbor	I
classifier	B
ieee	O
transactions	O
on	O
information	O
theory	O
qing-yun	O
s	O
and	O
fu	O
k	O
a	O
method	O
for	O
the	O
design	O
of	O
binary	B
tree	O
classifiers	O
pattern	O
recognition	O
quesenberry	O
c	O
and	O
gessaman	O
m	O
nonparametric	O
discrimination	O
using	O
tolerance	O
regions	O
annals	O
of	O
mathematical	O
statistics	O
quinlan	O
j	O
programs	O
for	O
machine	O
learning	B
morgan	O
kaufmann	O
san	O
mateo	O
rabiner	O
l	O
levinson	O
s	O
rosenberg	O
a	O
and	O
wilson	O
j	O
nition	O
of	O
isolated	O
words	O
using	O
clustering	B
techniques	O
ieee	O
transactions	O
on	O
acoustics	O
speech	O
and	O
signal	O
processing	O
references	O
rao	O
r	O
relations	O
between	O
weak	B
and	O
uniform	O
convergence	O
of	O
measures	O
with	O
cations	O
annals	O
of	O
mathematical	O
statistics	O
raudys	O
s	O
on	O
the	O
amount	O
of	O
a	O
priori	O
information	O
in	O
designing	O
the	O
classification	O
algorithm	B
technical	O
cybernetics	O
raudys	O
s	O
on	O
dimensionality	O
learning	B
sample	O
size	O
and	O
complexity	O
of	O
classification	O
algorithms	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
pattern	O
recognition	O
pages	O
ieee	O
computer	O
society	O
long	O
beach	O
ca	O
raudys	O
s	O
and	O
pikelis	O
v	O
on	O
dimensionality	O
sample	O
size	O
classification	O
error	O
and	O
complexity	O
of	O
classification	O
algorithm	B
in	O
pattern	O
recognition	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
raudys	O
s	O
and	O
pikelis	O
v	O
collective	O
selection	O
of	O
the	O
best	O
version	O
of	O
a	O
pattern	O
recognition	O
system	O
pattern	O
recognition	O
letters	O
rejto	O
l	O
and	O
revesz	O
p	O
density	B
estimation	B
and	O
pattern	O
classification	O
problems	O
of	O
control	O
and	O
information	O
theory	O
renyi	O
a	O
on	O
measures	O
of	O
entropy	B
and	O
information	O
in	O
proceedings	O
of	O
the	O
fourth	O
berkeley	O
symposium	O
pages	O
university	O
of	O
california	O
press	O
berkeley	O
revesz	O
p	O
robbins-monroe	O
procedures	O
in	O
a	O
hilbert	O
space	O
and	O
its	O
application	O
in	O
the	O
theory	O
of	O
learning	B
processes	O
studia	O
scientiarium	O
mathematicarum	O
hungarica	O
ripley	O
b	O
statistical	O
aspects	O
of	O
neural	O
networks	O
in	O
networks	O
and	O
chaos--statistical	O
and	O
probabilistic	O
aspects	O
barndorff-nielsen	O
jensen	O
j	O
and	O
kendall	O
w	O
editors	O
pages	O
chapman	O
and	O
hall	O
london	O
u	O
k	O
ripley	O
b	O
neural	O
networks	O
and	O
related	O
methods	O
for	O
classification	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
rissanen	O
l	O
a	O
universal	O
prior	O
for	O
integers	O
and	O
estimation	B
by	O
minimum	O
description	O
length	O
annals	O
of	O
statistics	O
ritter	O
g	O
woodruff	O
h	O
lowry	O
s	O
and	O
isenhour	O
t	O
an	O
algorithm	B
for	O
a	O
selective	B
nearest	B
neighbor	I
decision	O
rule	B
ieee	O
transactions	O
on	O
information	O
theory	O
robbins	O
h	O
and	O
monro	O
s	O
a	O
stochastic	B
approximation	I
method	O
annals	O
of	O
matical	O
statistics	O
rogers	O
w	O
and	O
wagner	O
t	O
a	O
finite	O
sample	O
distribution-free	O
performance	O
bound	O
for	O
local	O
discrimination	O
rules	O
annals	O
of	O
statistics	O
rosenblatt	O
f	O
principles	O
ofneurodynamics	O
perceptrons	O
and	O
the	O
theory	O
of	O
brain	O
mechanisms	O
spartan	O
books	O
washington	O
dc	O
rosenblatt	O
m	O
remarks	O
on	O
some	O
nonparametric	O
estimates	O
of	O
a	O
density	O
function	O
annals	O
of	O
mathematical	O
statistics	O
rounds	O
e	O
a	O
combined	O
nonparametric	O
approach	O
to	O
feature	O
selection	O
and	O
binary	B
decision	O
tree	O
design	O
pattern	O
recognition	O
royall	O
r	O
a	O
class	O
of	O
n	O
onparametric	O
estimators	O
of	O
a	O
smooth	O
regression	B
function	I
phd	O
thesis	O
stanford	O
university	O
stanford	O
ca	O
rumelhart	O
d	O
hinton	O
g	O
and	O
williams	O
r	O
learning	B
internal	O
representations	O
by	O
ror	O
propagation	O
in	O
parallel	O
distributed	O
processing	O
vol	O
i	O
rumelhart	O
d	O
j	O
and	O
the	O
pdp	O
research	O
group	O
editors	O
mit	O
press	O
cambridge	O
ma	O
reprinted	O
in	O
l	O
a	O
anderson	O
and	O
e	O
rosenfeld	O
neurocomputing--foundations	O
of	O
research	O
mit	O
press	O
cambridge	O
ma	O
pp	O
ruppert	O
d	O
stochastic	B
approximation	I
in	O
handbook	O
of	O
sequential	O
analysis	O
ghosh	O
b	O
and	O
sen	O
p	O
editors	O
pages	O
marcel	O
dekker	O
new	O
york	O
references	O
samet	O
h	O
the	O
quadtree	B
and	O
related	O
hierarchical	O
data	O
structures	O
computing	O
surveys	O
samet	O
h	O
applications	O
of	O
spatial	O
data	O
structures	O
addison-wesley	O
reading	O
ma	O
samet	O
h	O
the	O
design	O
and	O
analysis	O
of	O
spatial	O
data	O
structures	O
addison-wesley	O
reading	O
ma	O
sansone	O
g	O
orthogonal	O
functions	O
interscience	O
new	O
york	O
sauer	O
n	O
on	O
the	O
density	O
of	O
families	O
of	O
sets	O
journal	O
of	O
combinatorial	O
theory	O
series	O
h	O
a	O
useful	O
convergence	O
theorem	O
for	O
probability	O
distributions	O
annals	O
of	O
mathematical	O
statistics	O
schiaffli	O
l	O
gesammelte	O
mathematische	O
abhandlungen	O
birkhauser-verlag	O
basel	O
schmidt	O
w	O
neural	O
pattern	O
classifying	O
systems	O
phd	O
thesis	O
technical	O
university	O
delft	O
the	O
netherlands	O
schoenberg	O
i	O
on	O
frequency	O
functions	O
ii	O
variation-diminishing	O
integral	O
operators	O
of	O
the	O
convolution	O
type	O
acta	O
scientiarium	O
mathematicarum	O
szeged	O
schwartz	O
s	O
estimation	B
of	I
probability	O
density	O
by	O
an	O
orthogonal	O
series	O
annals	O
of	O
mathematical	O
statistics	O
schwemer	O
g	O
and	O
dunn	O
o	O
posterior	B
probability	I
estimators	O
in	O
classification	O
ulations	O
communications	O
in	O
statistics--simulation	O
sebestyen	O
g	O
decision-making	O
processes	O
in	O
pattern	O
recognition	O
macmillan	O
new	O
york	O
serfiing	O
r	O
probability	O
inequalities	O
for	O
the	O
sum	O
in	O
sampling	B
without	I
replacement	I
annals	O
of	O
statistics	O
sethi	O
i	O
a	O
fast	O
algorithm	B
for	O
recognizing	O
nearest	O
neighbors	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
sethi	O
i	O
entropy	B
nets	O
from	O
decision	O
trees	O
to	O
neural	O
nets	O
proceedings	O
of	O
the	O
ieee	O
sethi	O
i	O
decision	O
tree	O
performance	O
enhancement	O
using	O
an	O
artificial	O
neural	B
network	I
interpretation	O
in	O
artificial	O
neural	O
networks	O
and	O
statistical	O
pattern	O
recognition	O
old	O
and	O
new	O
connections	O
sethi	O
i	O
and	O
jain	O
a	O
editors	O
pages	O
elsevier	O
science	O
publishers	O
amsterdam	O
sethi	O
i	O
and	O
chatterjee	O
b	O
efficient	O
decision	O
tree	O
design	O
for	O
discrete	O
variable	B
pattern	O
recognition	O
problems	O
pattern	O
recognition	O
sethi	O
i	O
and	O
sarvarayudu	O
g	O
hierarchical	O
classifier	B
design	O
using	O
mutual	O
tion	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
shannon	O
c	O
a	O
mathematical	O
theory	O
of	O
communication	O
bell	O
systems	O
technical	O
shawe-taylor	O
j	O
sample	O
sizes	O
for	O
sigmoidal	O
neural	O
networks	O
technical	O
report	O
department	O
of	O
computer	O
science	O
royal	O
holloway	O
university	O
of	O
london	O
egham	O
land	O
shawe-taylor	O
j	O
anthony	O
m	O
and	O
biggs	O
n	O
l	O
bounding	O
sample	O
size	O
with	O
the	O
vapnik-chervonenkis	B
dimension	B
discrete	O
applied	O
mathematics	O
shiryayev	O
a	O
probability	O
springer-verlag	O
new	O
york	O
short	O
r	O
and	O
fukunaga	O
k	O
the	O
optimal	O
distance	O
measure	O
for	O
nearest	B
neighbor	I
classification	O
ieee	O
transactions	O
on	O
information	O
theory	O
references	O
simon	O
h	O
the	O
vapnik-chervonenkis	B
dimension	B
of	O
decision	O
trees	O
with	O
bounded	O
rank	O
information	O
processing	O
letters	O
simon	O
h	O
general	O
lower	O
bounds	O
on	O
the	O
number	O
of	O
examples	O
needed	O
for	O
learning	B
probabilistic	O
concepts	O
in	O
proceedings	O
of	O
the	O
sixth	O
annual	O
acm	O
conference	O
on	O
tational	O
learning	B
theory	O
pages	O
association	B
for	O
computing	O
machinery	O
new	O
york	O
sklansky	O
j	O
and	O
michelotti	O
locally	O
trained	O
piecewise	O
linear	O
classifiers	O
ieee	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
sklansky	O
j	O
and	O
wassel	O
g	O
pattern	O
classifiers	O
and	O
trainable	O
machines	O
verlag	O
new	O
york	O
slud	O
e	O
distribution	O
inequalities	O
for	O
the	O
binomial	B
law	O
annals	O
of	O
probability	O
specht	O
d	O
generation	O
of	O
polynomial	B
discriminant	O
functions	O
for	O
pattern	O
tion	O
ieee	O
transactions	O
on	O
electronic	O
computers	O
specht	O
d	O
series	O
estimation	B
of	I
a	O
probability	O
density	O
function	O
technometrics	O
specht	O
d	O
probabilistic	O
neural	O
networks	O
and	O
the	O
polynomial	B
adaline	B
as	O
tary	O
techniques	O
for	O
classification	O
ieee	O
transactions	O
on	O
neural	O
networks	O
spencer	O
j	O
ten	O
lectures	O
on	O
the	O
probabilistic	B
method	I
siam	O
philadelphia	O
pa	O
sprecher	O
d	O
on	O
the	O
structure	O
of	O
continuous	O
functions	O
of	O
several	O
variables	O
actions	O
of	O
the	O
american	O
mathematical	O
society	O
steele	O
j	O
combinatorial	O
entropy	B
and	O
uniform	O
limit	O
laws	O
phd	O
thesis	O
stanford	O
university	O
stanford	O
ca	O
steele	O
j	O
an	O
efron-stein	O
inequality	B
for	O
nonsymmetric	O
statistics	O
annals	O
of	O
statistics	O
stengle	O
g	O
and	O
yukich	O
j	O
some	O
new	O
vapnik-chervonenkis	B
classes	O
annals	O
of	O
statistics	O
stoffel	O
j	O
a	O
classifier	B
design	O
technique	O
for	O
discrete	O
variable	B
pattern	O
recognition	O
problems	O
ieee	O
transactions	O
on	O
computers	O
stoller	O
d	O
univariate	O
two-population	O
distribution-free	O
discrimination	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
stone	O
c	O
consistent	O
nonparametric	O
regression	O
annals	O
of	O
statistics	O
stone	O
c	O
optimal	O
global	O
rates	O
of	O
convergence	O
for	O
nonparametric	O
regression	O
annals	O
of	O
statistics	O
stone	O
c	O
additive	O
regression	O
and	O
other	O
nonparametric	O
models	O
annals	O
of	O
statistics	O
stone	O
m	O
cross-validatory	O
choice	O
and	O
assessment	O
of	O
statistical	O
predictions	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
stute	O
w	O
asymptotic	O
normality	O
of	O
nearest	B
neighbor	I
regression	B
function	I
estimates	O
annals	O
of	O
statistics	O
sung	O
k	O
and	O
shirley	O
p	O
ray	O
tracing	O
with	O
the	O
bsp	O
tree	O
in	O
graphics	O
gems	O
iii	O
kirk	O
d	O
editor	O
pages	O
academic	O
press	O
boston	O
ma	O
swonger	O
c	O
sample	O
set	O
condensation	O
for	O
a	O
condensed	B
nearest	B
neighbor	I
decision	O
rule	B
for	O
pattern	O
recognition	O
in	O
frontiers	O
of	O
pattern	O
recognition	O
watanabe	O
s	O
editor	O
pages	O
academic	O
press	O
new	O
york	O
szarek	O
s	O
on	O
the	O
best	O
constants	O
in	O
the	O
khintchine	O
inequality	B
studia	O
mathematica	O
references	O
szego	O
g	O
orthogonal	O
polynomials	O
volume	O
american	O
mathematical	O
society	O
providence	O
ri	O
talagrand	O
m	O
the	O
glivenko-cantelli	O
problem	O
annals	O
of	O
probability	O
talagrand	O
m	O
sharper	O
bounds	O
for	O
gaussian	B
and	O
empirical	B
processes	O
annals	O
of	O
probability	O
talmon	O
j	O
a	O
multiclass	O
nonparametric	O
partitioning	O
algorithm	B
in	O
pattern	O
nition	O
in	O
practice	O
ii	O
gelsema	O
e	O
and	O
kanal	O
l	O
editors	O
elsevier	O
science	O
publishers	O
amsterdam	O
taneja	O
on	O
characterization	O
of	O
j-divergence	O
and	O
its	O
generalizations	O
journal	O
of	O
combinatorics	O
information	O
and	O
system	O
sciences	O
statistical	O
aspects	O
of	O
divergence	O
measures	O
journal	O
of	O
statistical	O
planning	O
and	O
inference	O
tarter	O
m	O
and	O
kronmal	O
r	O
on	O
multivariate	O
density	O
estimates	O
based	O
on	O
orthogonal	O
expansions	O
annals	O
of	O
mathematical	O
statistics	O
tomek	O
a	O
generalization	O
of	O
the	O
k-nn	B
rule	B
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
tomek	O
two	O
modifications	O
of	O
cnn	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
toussaint	O
g	O
note	O
on	O
optimal	O
selection	O
of	O
independent	O
binary-valued	O
features	O
for	O
pattern	O
recognition	O
ieee	O
transactions	O
on	O
information	O
theory	O
toussaint	O
g	O
bibliography	O
on	O
estimation	B
ofmisclassification	O
ieee	O
transactions	O
on	O
information	O
theory	O
toussaint	O
g	O
on	O
the	O
divergence	O
between	O
two	O
distributions	O
and	O
the	O
probability	O
of	O
misclassification	O
of	O
several	O
decision	O
rules	O
in	O
proceedings	O
of	O
the	O
second	O
international	O
joint	O
conference	O
on	O
pattern	O
recognition	O
pages	O
copenhagen	O
toussaint	O
g	O
and	O
donaldson	O
r	O
algorithms	O
for	O
recognizing	O
contour-traced	O
printed	O
characters	O
ieee	O
transactions	O
on	O
computers	O
tsypkin	O
y	O
adaptation	O
and	O
learning	B
in	O
automatic	B
systems	O
academic	O
press	O
new	O
york	O
tutz	O
g	O
smoothed	B
additive	O
estimators	O
for	O
non-error	O
rates	O
in	O
multiple	O
discriminant	O
analysis	O
pattern	O
recognition	O
tutz	O
g	O
an	O
alternative	O
choice	O
of	O
smoothing	O
for	O
kernel-based	O
density	O
estimates	O
in	O
discrete	O
discriminant	O
analysis	O
biometrika	O
tutz	O
g	O
smoothing	O
for	O
discrete	O
kernels	O
in	O
discrimination	O
biometrics	O
journal	O
tutz	O
g	O
on	O
cross-validation	B
for	O
discrete	O
kernel	B
estimates	O
in	O
discrimination	O
munications	O
in	O
statistics-theory	O
and	O
methods	O
ullmann	O
j	O
automatic	B
selection	O
of	O
reference	O
data	O
for	O
use	O
in	O
a	O
nearest-neighbor	O
method	O
of	O
pattern	O
classification	O
ieee	O
transactions	O
on	O
information	O
theory	O
vajda	O
i	O
the	O
estimation	B
of	I
minimal	O
error	O
probability	O
for	O
testing	O
finite	O
or	O
countable	O
number	O
of	O
hypotheses	O
problemy	O
peredaci	O
informacii	O
vajda	O
theory	O
of	O
statistical	O
inference	O
and	O
information	O
kluwer	O
academic	O
lishers	O
dordrecht	O
valiant	O
l	O
a	O
theory	O
of	O
the	O
learnable	O
communications	O
of	O
the	O
acm	O
van	O
campenhout	O
j	O
the	O
arbitrary	O
relation	O
between	O
probability	O
of	O
error	O
and	O
surement	O
subset	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
references	O
van	O
ryzin	O
j	O
bayes	O
risk	O
consistency	B
of	O
classification	O
procedures	O
using	O
density	B
estimation	B
sankhya	O
series	O
a	O
vapnik	O
v	O
estimation	B
of	I
dependencies	O
based	O
on	O
empirical	B
data	O
springer-verlag	O
new	O
york	O
vapnik	O
v	O
and	O
chervonenkis	O
a	O
on	O
the	O
uniform	O
convergence	O
of	O
relative	O
frequencies	O
of	O
events	O
to	O
their	O
probabilities	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
vapnik	O
v	O
and	O
chervonenkis	O
a	O
ordered	B
risk	O
minimization	O
i	O
automation	O
and	O
remote	O
control	O
vapnik	O
v	O
and	O
chervonenkis	O
a	O
ordered	B
risk	O
minimization	O
ii	O
automation	O
and	O
remote	O
control	O
vapnik	O
v	O
and	O
chervonenkis	O
a	O
theory	O
of	O
pattern	O
recognition	O
nauka	O
moscow	O
russian	O
german	O
translation	O
theorie	O
der	O
zeichenerkennung	O
akademie	O
verlag	O
berlin	O
vapnik	O
v	O
and	O
chervonenkis	O
a	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
the	O
uniform	O
convergence	O
of	O
means	O
to	O
their	O
expectations	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
vidal	O
e	O
an	O
algorithm	B
for	O
finding	O
nearest	O
neighbors	O
in	O
constant	O
average	O
time	O
pattern	O
recognition	O
letters	O
vilmansen	O
t	O
feature	O
evaluation	O
with	O
measures	O
of	O
probabilistic	O
dependence	O
ieee	O
transactions	O
on	O
computers	O
vitushkin	O
a	O
the	O
absolute	O
e-entropy	O
of	O
metric	O
spaces	O
translations	O
of	O
the	O
american	O
mathematical	O
society	O
wagner	O
t	O
convergence	O
of	O
the	O
nearest	B
neighbor	I
rule	B
ieee	O
transactions	O
on	O
mation	O
theory	O
wagner	O
t	O
convergence	O
of	O
the	O
edited	B
nearest	B
neighbor	I
ieee	O
transactions	O
on	O
information	O
theory	O
wang	O
q	O
and	O
suen	O
c	O
analysis	O
and	O
design	O
of	O
decision	O
tree	O
based	O
on	O
entropy	B
reduction	O
and	O
its	O
application	O
to	O
large	O
character	O
set	O
recognition	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
warner	O
h	O
toronto	O
a	O
veasey	O
l	O
and	O
stephenson	O
r	O
a	O
mathematical	O
approach	O
to	O
medical	O
diagnosis	O
journal	O
of	O
jhe	O
american	O
medical	O
association	B
wassel	O
g	O
and	O
sklansky	O
j	O
training	O
a	O
one-dimensional	O
classifier	B
to	O
minimize	O
the	O
probability	O
of	O
error	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
watson	O
g	O
smooth	O
regression	O
analysis	O
sankhya	O
series	O
a	O
weiss	O
s	O
and	O
kulikowski	O
c	O
computer	O
systems	O
that	O
learn	O
morgan	O
kaufmann	O
san	O
mateo	O
ca	O
wenocur	O
r	O
and	O
dudley	O
r	O
some	O
special	O
vapnik-chervonenkis	B
classes	O
discrete	O
mathematics	O
wheeden	O
r	O
and	O
zygmund	O
a	O
measure	O
and	O
integral	O
marcel	O
dekker	O
new	O
york	O
white	O
h	O
connectionist	O
nonparametric	O
regression	O
multilayer	B
feedforward	O
works	O
can	O
learn	O
arbitrary	O
mappings	O
neural	O
networks	O
white	O
h	O
nonparametric	O
estimation	B
of	I
conditional	O
quantiles	O
using	O
neural	O
networks	O
in	O
proceedings	O
of	O
the	O
symposium	O
of	O
the	O
interface	O
computing	O
science	O
and	O
tics	O
pages	O
american	O
statistical	O
association	B
alexandria	O
va	O
widrow	O
b	O
adaptive	O
sampled-data	O
systems-a	O
statistical	O
theory	O
of	O
adaptation	O
in	O
ire	O
wescon	O
convention	O
record	B
volume	O
part	O
pages	O
references	O
widrow	O
b	O
and	O
hoff	O
m	O
adaptive	O
switching	O
circuits	O
in	O
ire	O
wescon	O
convention	O
record	B
volume	O
part	O
pages	O
reprinted	O
in	O
j	O
anderson	O
and	O
e	O
rosenfeld	O
neurocomputing	O
foundations	O
of	O
research	O
mit	O
press	O
cambridge	O
ma	O
wilson	O
d	O
asymptotic	O
properties	O
of	B
nearest	B
neighbor	I
rules	I
using	O
edited	B
data	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
winder	O
r	O
threshold	B
logic	O
in	O
artificial	O
intelligence	O
in	O
artificial	O
intelligence	O
pages	O
ieee	O
special	O
publication	O
wolverton	O
c	O
and	O
wagner	O
t	O
asymptotically	O
optimal	O
discriminant	O
functions	O
for	O
pattern	O
classification	O
ieee	O
transactions	O
on	O
systems	O
science	O
and	O
cybernetics	O
wolverton	O
c	O
and	O
wagner	O
t	O
recursive	B
estimates	O
of	O
probability	O
densities	O
ieee	O
transactions	O
on	O
systems	O
science	O
and	O
cybernetics	O
wong	O
w	O
and	O
shen	O
x	O
probability	O
inequalities	O
for	O
likelihood	O
ratios	O
and	O
convergence	O
rates	O
of	O
sieve	O
mles	O
technical	O
report	O
department	O
of	O
statistics	O
university	O
of	O
chicago	O
chicago	O
il	O
xu	O
l	O
krzyzak	O
a	O
and	O
oja	O
e	O
rival	O
penalized	O
competitive	O
learning	B
for	O
clustering	B
analysis	O
rbf	O
net	O
and	O
curve	O
detection	O
ieee	O
transactions	O
on	O
neural	O
networks	O
xu	O
l	O
krzyzak	O
a	O
and	O
yuille	O
a	O
on	O
radial	B
basis	B
function	I
nets	O
and	O
kernel	B
sion	O
approximation	O
ability	O
convergence	O
rate	O
and	O
receptive	O
field	O
size	O
neural	O
networks	O
to	O
appear	O
yatracos	O
y	O
rates	O
of	O
convergence	O
of	O
minimum	O
distance	O
estimators	O
and	O
mogorovs	O
entropy	B
annals	O
of	O
statistics	O
yau	O
s	O
and	O
lin	O
t	O
on	O
the	O
upper	O
bound	O
of	O
the	O
probability	O
of	O
error	O
of	O
a	O
linear	O
pattern	O
classifier	B
for	O
probabilistic	O
pattern	O
classes	O
proceedings	O
of	O
the	O
ieee	O
you	O
k	O
and	O
fu	O
k	O
an	O
approach	O
to	O
the	O
design	O
of	O
a	O
linear	O
binary	B
tree	O
classifier	B
in	O
proceedings	O
of	O
the	O
symposium	O
of	O
machine	O
processing	O
of	O
remotely	O
sensed	O
data	O
pages	O
purdue	O
university	O
lafayette	O
in	O
yukich	O
j	O
laws	O
of	O
large	O
numbers	O
for	O
classes	O
of	O
functions	O
journal	O
of	O
multivariate	O
analysis	O
yunck	O
t	O
a	O
technique	O
to	O
identify	O
nearest	O
neighbors	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
zhao	O
l	O
exponential	B
bounds	O
of	O
mean	O
error	O
for	O
the	O
nearest	B
neighbor	I
estimates	O
of	O
regression	O
functions	O
journal	O
of	O
multivariate	O
analysis	O
zhao	O
l	O
exponential	B
bounds	O
of	O
mean	O
error	O
for	O
the	O
kernel	B
estimates	O
of	O
regression	O
functions	O
journal	O
of	O
multivariate	O
analysis	O
zhao	O
l	O
krishnaiah	O
p	O
and	O
chen	O
x	O
almost	O
sure	O
lr-norm	O
convergence	O
for	O
based	O
histogram	O
estimates	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
zygmund	O
a	O
trigonometric	B
series	O
i	O
university	O
press	O
cambridge	O
author	O
index	O
abou-jaoude	O
s	O
abram	O
g	O
d	O
aitchison	O
aitken	O
cg	O
g	O
aizerman	O
m	O
a	O
akaike	O
h	O
alexander	O
k	O
anderson	O
a	O
c	O
anderson	O
j	O
a	O
anderson	O
m	O
w	O
anderson	O
t	O
w	O
angluin	O
d	O
anthony	O
m	O
antos	O
a	O
vi	O
argentiero	O
p	O
arkadjew	O
a	O
g	O
ash	O
rb	O
assouad	O
p	O
azuma	O
k	O
bahadur	O
rr	O
bailey	O
t	O
barron	O
a	O
r	O
vi	O
barron	O
rl	O
vi	O
bartlett	O
p	O
bashkirov	O
baskett	O
f	O
baum	O
e	O
b	O
beakley	O
g	O
w	O
beaudet	O
p	O
beck	O
j	O
becker	O
p	O
beiriant	O
j	O
vi	O
ben-bassat	O
m	O
benedek	O
g	O
m	O
bennett	O
benning	O
rd	O
bentley	O
j	O
l	O
beran	O
rh	O
beriinet	O
a	O
vi	O
bernstein	O
s	O
n	O
bhattacharya	O
p	O
k	O
bhattacharyya	O
a	O
bickel	O
p	O
j	O
birge	O
l	O
blumer	O
a	O
bosq	O
d	O
vi	O
brailovsky	O
v	O
l	O
braverman	O
e	O
m	O
breiman	O
l	O
brent	O
rp	O
broder	O
a	O
j	O
author	O
index	O
broeckaert	O
i	O
broniatowski	O
m	O
vi	O
broomhead	O
d	O
s	O
buescher	O
k	O
l	O
burbea	O
j	O
burman	O
p	O
vi	O
burshtein	O
d	O
buzo	O
a	O
cacoullos	O
t	O
cao	O
r	O
vi	O
carnal	O
h	O
casey	O
rg	O
cencov	O
n	O
n	O
chang	O
e	O
l	O
chang	O
c	O
y	O
chatterjee	O
b	O
chen	O
c	O
chen	O
h	O
chen	O
t	O
chen	O
x	O
r	O
chen	O
ye	O
chen	O
z	O
chernoff	O
h	O
chervonenkis	O
ay	O
v	O
chin	O
r	O
chou	O
p	O
a	O
chou	O
w	O
s	O
chow	O
c	O
k	O
chow	O
ys	O
ciampi	O
a	O
collomb	O
g	O
conway	O
j	O
h	O
coomans	O
d	O
cormen	O
th	O
cover	O
tm	O
v	O
vi	O
cramer	O
h	O
csibi	O
s	O
v	O
csiszar	O
csuros	O
m	O
vi	O
cuevas	O
a	O
vi	O
cybenko	O
g	O
darken	O
e	O
dasarathy	O
b	O
v	O
das	O
gupta	O
s	O
day	O
n	O
e	O
de	O
guzman	O
m	O
deheuvels	O
p	O
vi	O
della	O
pietra	O
vd	O
delp	O
e	O
j	O
devijver	O
p	O
a	O
vi	O
devroyel	O
diaconis	O
p	O
dillon	O
w	O
r	O
donahue	O
m	O
donaldson	O
rw	O
do-tu	O
h	O
dubes	O
rc	O
duda	O
ro	O
dudani	O
s	O
a	O
dudley	O
r	O
m	O
duin	O
rp	O
w	O
dunn	O
oj	O
durrett	O
r	O
dvoretzky	O
a	O
edelsbrunner	O
h	O
efron	O
b	O
ehrenfeucht	O
a	O
elashoff	O
j	O
d	O
elashoff	O
rm	O
fabian	O
v	O
fano	O
rm	O
farago	O
a	O
vi	O
farago	O
t	O
feinholz	O
l	O
feller	O
w	O
finkel	O
ra	O
fisher	O
f	O
p	O
fisher	O
ra	O
fitzmaurice	O
g	O
m	O
fix	O
e	O
flick	O
t	O
e	O
forney	O
g	O
d	O
fraiman	O
r	O
vi	O
friedman	O
j	O
h	O
fritz	O
j	O
v	O
fu	O
ks	O
fuchs	O
h	O
fukunagak	O
funahashi	O
k	O
gabor	O
d	O
gabriel	O
kr	O
gaenssler	O
p	O
gallant	O
ar	O
ganesalingam	O
s	O
garnett	O
j	O
m	O
gates	O
g	O
w	O
gelfand	O
s	O
b	O
geman	O
s	O
gerberich	O
cl	O
gessaman	O
m	O
p	O
gessaman	O
p	O
h	O
geva	O
s	O
gijbels	O
i	O
vi	O
gine	O
e	O
girosi	O
e	O
glick	O
n	O
vi	O
goldberg	O
p	O
goldmann	O
g	O
e	O
goldstein	O
m	O
golea	O
m	O
gonzalez-manteiga	O
w	O
vi	O
goodman	O
rm	O
goppert	O
r	O
gordon	O
l	O
gowda	O
kc	O
grant	O
e	O
d	O
gray	O
rm	O
greblicki	O
w	O
grenander	O
u	O
grimmett	O
g	O
r	O
guo	O
h	O
gurvits	O
l	O
author	O
index	O
gyorfi	O
l	O
gyorfi	O
z	O
haagerup	O
u	O
habbema	O
j	O
d	O
e	O
hagerup	O
t	O
hall	O
p	O
vi	O
hand	O
d	O
j	O
hardie	O
w	O
halt	O
p	O
e	O
v	O
hartigan	O
j	O
a	O
hartmann	O
crp	O
hashlamoun	O
w	O
a	O
hastie	O
t	O
haussler	O
d	O
hecht-nielsen	O
r	O
hellman	O
m	O
e	O
henrichon	O
e	O
g	O
herman	O
c	O
hermans	O
j	O
hertz	O
j	O
hills	O
m	O
hinton	O
g	O
e	O
hjort	O
n	O
hodges	O
j	O
l	O
hoeffding	O
w	O
hoff	O
m	O
e	O
holden	O
s	O
b	O
holmstrom	O
l	O
hora	O
s	O
c	O
horibe	O
y	O
home	O
b	O
hornik	O
k	O
hostetler	O
l	O
d	O
huber	O
p	O
j	O
hudimoto	O
h	O
hummels	O
d	O
m	O
hush	O
d	O
hwang	O
cr	O
installe	O
m	O
isenhour	O
t	O
l	O
isogai	O
e	O
vi	O
itai	O
a	O
author	O
index	O
ito	O
t	O
ivakhnenko	O
a	O
g	O
jain	O
a	O
k	O
jeffreys	O
h	O
jerrum	O
m	O
johnson	O
d	O
s	O
jones	O
l	O
k	O
kailath	O
t	O
kanal	O
l	O
n	O
kanevsky	O
d	O
kaplan	O
m	O
r	O
karlin	O
a	O
s	O
karp	O
rm	O
karpinski	O
m	O
kazmierczak	O
h	O
kearns	O
m	O
kedem	O
z	O
m	O
kegl	O
b	O
vi	O
kemp	O
r	O
kemperman	O
j	O
h	O
b	O
kerridge	O
n	O
e	O
kessel	O
d	O
l	O
khasminskii	O
rz	O
kiefer	O
j	O
kim	O
b	O
s	O
kittler	O
j	O
kizawa	O
m	O
klemehi	O
j	O
knoke	O
j	O
d	O
kohonen	O
t	O
kolmogorov	O
a	O
n	O
konovalenko	O
v	O
v	O
komer	O
j	O
koutsougeras	O
c	O
kraaijveld	O
m	O
a	O
krasitskyy	O
m	O
s	O
krishna	O
g	O
krishnaiah	O
p	O
r	O
kronmal	O
ra	O
krzanowski	O
w	O
j	O
krzyzak	O
a	O
vi	O
kulikowski	O
c	O
a	O
kulkarni	O
s	O
r	O
kullback	O
s	O
kumar	O
p	O
r	O
kurkova	O
v	O
kushner	O
h	O
j	O
lachenbruch	O
p	O
a	O
laforest	O
l	O
laha	O
rg	O
landgrebe	O
d	O
a	O
lecam	O
l	O
leibler	O
a	O
leiserson	O
c	O
e	O
levinson	O
s	O
e	O
li	O
t	O
l	O
li	O
x	O
lin	O
h	O
e	O
lin	O
t	O
t	O
lin	O
y	O
k	O
linde	O
y	O
linder	O
t	O
vi	O
lissack	O
t	O
littlestone	O
n	O
liu	O
r	O
ljung	O
l	O
lloyd	O
s	O
p	O
loftsgaarden	O
d	O
o	O
logan	O
b	O
e	O
loh	O
w	O
y	O
loizou	O
g	O
lorentz	O
g	O
g	O
lowe	O
d	O
lowry	O
s	O
l	O
lugosi	O
g	O
lukacs	O
e	O
lunts	O
a	O
l	O
maass	O
w	O
machell	O
e	O
macintyre	O
a	O
mack	O
y	O
p	O
vi	O
mahalanobis	O
p	O
c	O
mantock	O
j	O
m	O
marchand	O
m	O
marron	O
j	O
s	O
massart	O
p	O
mathai	O
a	O
m	O
matloff	O
n	O
matula	O
d	O
w	O
matushita	O
k	O
max	O
j	O
maybank	O
s	O
j	O
mcdiarmid	O
c	O
mclachlan	O
g	O
j	O
mehrotra	O
kg	O
meisel	O
w	O
michalopoulos	O
w	O
s	O
michel-briand	O
c	O
michelotti	O
mickey	O
p	O
a	O
mielniczuk	O
j	O
milhaud	O
x	O
min	O
p	O
j	O
minsky	O
m	O
l	O
mitchell	O
a	O
f	O
s	O
mizoguchi	O
r	O
monro	O
s	O
moody	O
j	O
moore	O
d	O
s	O
morgan	O
j	O
n	O
muchnik	O
i	O
e	O
mui	O
j	O
k	O
myles	O
j	O
p	O
nadaraya	O
e	O
a	O
nadas	O
a	O
vi	O
nagy	O
g	O
narendra	O
p	O
m	O
natarajan	O
b	O
k	O
naylor	O
b	O
m	O
b	O
niemann	O
h	O
nilsson	O
n	O
j	O
nobel	O
a	O
b	O
vi	O
nolan	O
d	O
oja	O
e	O
okamoto	O
m	O
olshen	O
ra	O
ott	O
l	O
author	O
index	O
paii	O
i	O
vi	O
palmer	O
rg	O
papachristou	O
c	O
a	O
papadimitriou	O
c	O
h	O
papert	O
s	O
park	O
j	O
park	O
s	O
b	O
park	O
y	O
parthasarathy	O
kr	O
pmzen	O
e	O
patrick	O
e	O
a	O
pawlak	O
m	O
vi	O
payne	O
h	O
j	O
pearl	O
j	O
penrod	O
c	O
s	O
peterson	O
d	O
w	O
petrache	O
g	O
petrov	O
v	O
v	O
petry	O
h	O
vi	O
pflug	O
g	O
vi	O
pikelis	O
v	O
pinsker	O
m	O
pinter	O
m	O
vi	O
pippinger	O
n	O
poggio	O
t	O
pollard	O
d	O
powell	O
m	O
j	O
d	O
prep	O
arata	O
p	O
p	O
priest	O
rg	O
pruitt	O
r	O
psaltis	O
d	O
purcell	O
e	O
pyatniskii	O
e	O
s	O
qing-yun	O
s	O
quesenberry	O
c	O
p	O
quinlan	O
j	O
r	O
rabiner	O
l	O
r	O
rao	O
c	O
r	O
rao	O
rr	O
rathie	O
p	O
n	O
raudys	O
s	O
ravishankar	O
c	O
s	O
rejto	O
l	O
renyi	O
a	O
revesz	O
p	O
v	O
author	O
index	O
richardson	O
t	O
ripley	O
b	O
d	O
rissanen	O
j	O
ritter	O
g	O
l	O
rivest	O
rl	O
robbins	O
h	O
rogers	O
w	O
h	O
rosenberg	O
a	O
e	O
rosenblatt	O
e	O
rosenblatt	O
m	O
rounds	O
e	O
m	O
roussas	O
g	O
vi	O
royall	O
rm	O
rozonoer	O
l	O
i	O
rub	O
c	O
rumelhart	O
d	O
e	O
ruppert	O
d	O
samarasooriya	O
vn	O
s	O
samet	O
h	O
sandberg	O
i	O
w	O
sansone	O
g	O
sarvarayudu	O
g	O
p	O
r	O
sauer	O
n	O
scheffe	O
h	O
schhiffli	O
l	O
schmidt	O
w	O
e	O
schoenberg	O
lj	O
schroeder	O
a	O
schwartz	O
s	O
c	O
schwemer	O
g	O
t	O
sebestyen	O
g	O
serfling	O
r	O
i	O
sethi	O
i	O
k	O
shahshahani	O
m	O
shamos	O
m	O
i	O
shannon	O
c	O
e	O
shawe-taylor	O
j	O
shen	O
x	O
shepp	O
l	O
shimura	O
m	O
shirley	O
p	O
shiryayev	O
a	O
n	O
short	O
rd	O
shustek	O
l	O
j	O
silverman	O
b	O
w	O
simar	O
l	O
vi	O
simon	O
h	O
d	O
sitte	O
j	O
sklansky	O
sloane	O
n	O
j	O
a	O
slud	O
e	O
v	O
smyth	O
p	O
snapp	O
rr	O
sokal	O
rr	O
sonquist	O
j	O
a	O
sontag	O
e	O
specht	O
d	O
e	O
spencer	O
j	O
sprecher	O
d	O
a	O
steele	O
j	O
m	O
stein	O
c	O
steinbuch	O
k	O
stengle	O
g	O
stephenson	O
r	O
stinchcombe	O
m	O
stirzaker	O
d	O
r	O
stoffel	O
j	O
c	O
stoller	O
d	O
s	O
stone	O
c	O
i	O
v	O
stone	O
m	O
stuetzle	O
w	O
stute	O
w	O
vi	O
suen	O
c	O
y	O
sung	O
k	O
swonger	O
c	O
w	O
szabados	O
t	O
vi	O
r	O
szarek	O
s	O
i	O
szego	O
g	O
talagrand	O
m	O
talmon	O
taneja	O
i	O
j	O
tarter	O
m	O
e	O
thomas	O
j	O
a	O
tibshirani	O
r	O
i	O
tikhomirov	O
vm	O
tomek	O
i	O
toronto	O
a	O
e	O
toussaint	O
g	O
t	O
vi	O
tsypkin	O
y	O
z	O
tukey	O
j	O
tulupchuk	O
y	O
m	O
author	O
index	O
tuteur	O
f	O
b	O
tutz	O
g	O
e	O
tymchenko	O
lk	O
tyrcha	O
j	O
ullmann	O
j	O
r	O
vajda	O
igor	O
vi	O
vajda	O
istvan	O
valiant	O
l	O
g	O
van	O
campenhout	O
j	O
m	O
van	O
den	O
broek	O
k	O
van	O
der	O
meulen	O
e	O
c	O
vi	O
vanichsetakul	O
n	O
van	O
ryzin	O
j	O
vapnik	O
v	O
n	O
v	O
varshney	O
p	O
k	O
veasey	O
l	O
g	O
venkatesh	O
s	O
s	O
vidal	O
e	O
vilmansen	O
t	O
e	O
vitushkin	O
ag	O
wagner	O
tj	O
v	O
vi	O
walk	O
h	O
vi	O
wand	O
m	O
p	O
wang	O
q	O
r	O
warmuth	O
m	O
k	O
warner	O
h	O
r	O
wassel	O
g	O
n	O
watson	O
g	O
s	O
weiss	O
s	O
m	O
wenocur	O
rs	O
white	O
h	O
whitsitt	O
s	O
j	O
widrow	O
b	O
wilcox	O
j	O
b	O
williams	O
rj	O
wilson	O
d	O
l	O
wilson	O
j	O
g	O
winder	O
ro	O
wise	O
g	O
l	O
wold	O
h	O
wolfowitz	O
j	O
wolverton	O
c	O
t	O
wong	O
w	O
h	O
woodruff	O
h	O
b	O
wouters	O
w	O
v	O
xu	O
l	O
yakowitz	O
s	O
vi	O
yatracos	O
yg	O
vi	O
yau	O
s	O
s	O
you	O
kc	O
yuille	O
al	O
yukich	O
j	O
e	O
yunck	O
t	O
p	O
zeitouni	O
zhaol	O
c	O
zeger	O
k	O
vi	O
zinn	O
j	O
zygmund	O
a	O
subject	O
index	O
absolute	B
error	I
accuracy	B
adaline	B
additive	B
model	I
admissible	B
transformation	I
agreement	B
aid	B
criterion	I
alexanders	O
inequality	B
ancestral	B
rule	B
a	B
posteriori	I
probability	I
apparent	O
error	O
rate	O
see	O
error	B
estimation	B
resubstitution	B
approximation	B
error	I
arrangement	B
arrangement	B
classifier	B
association	B
inequality	B
asymptotic	B
optimality	I
back	B
propagation	I
bahadur-lazarsfeld	B
expansion	I
bandwidth	O
see	O
smoothing	B
factor	I
barron	B
network	I
basis	B
function	I
complete	B
orthonormal	I
haar	B
hermite	B
laguerre	B
legendre	B
rademacher	B
rademacher-walsh	B
trigonometric	B
walsh	B
bayes	O
classifier	B
rule	B
bayes	O
decision	O
see	O
bayes	O
classifier	B
bayes	B
error	I
subject	O
index	O
estimation	B
of	I
bayes	B
problem	I
bayes	O
risk	O
see	O
bayes	B
error	I
bennetts	O
inequality	B
beppo-levy	B
theorem	I
bernstein	B
perceptron	B
bernstein	B
polynomial	B
bernsteins	O
inequality	B
beta	B
distribution	I
bhattacharyya	B
affinity	I
bias	B
binary	B
space	I
partition	B
tree	I
balanced	B
raw	B
binomial	B
distribution	I
binomial	B
theorem	I
boolean	B
classification	I
borel-cantelli	B
lemma	I
bracketing	B
metric	B
entropy	B
branch-and-bound	B
method	I
esp	O
tree	O
see	O
binary	B
space	I
partition	B
tree	I
cart	B
catalan	B
number	I
cauchy-schwarz	B
inequality	B
central	B
limit	I
theorem	I
channel	B
characteristic	B
function	I
chebyshev-cantelli	B
inequality	B
chebyshevsinequality	O
chernoffs	O
affinity	O
chernoffs	O
bounding	O
method	O
class-conditional	B
density	I
class-conditional	B
distribution	I
classifier	B
selection	I
lower	B
bounds	I
for	I
class	B
probability	I
clustering	B
committee	B
machine	I
complexity	B
penalty	I
complexity	B
regularization	I
concept	O
learning	B
see	O
learning	B
confidence	B
consistency	B
definition	B
of	I
of	B
fourier	I
series	I
rule	B
of	B
generalized	B
linear	I
rules	I
of	B
kernel	B
rules	I
of	B
maximum	B
likelihood	I
of	B
nearest	B
neighbor	I
rules	I
of	B
neural	B
network	I
classifiers	I
of	B
partitioning	I
rules	I
subject	O
index	O
of	B
skeleton	I
estimates	I
of	B
squared	B
error	I
minimization	I
strong	O
see	O
strong	B
consistency	B
strong	O
universal	O
see	O
strong	O
universal	O
tie	B
breaking	I
consistency	B
distribution	O
universal	O
see	O
universal	B
consistency	B
consistent	O
rule	B
see	O
consistency	B
convex	B
hull	I
cover-hart	B
inequality	B
covering	O
of	B
a	I
class	I
of	I
classifiers	I
of	B
a	I
class	I
of	I
functions	I
empirical	B
simple	B
empirical	B
covering	B
lemma	I
covering	B
number	I
cross-validation	B
curse	B
of	I
dimensionality	I
decision	B
with	I
rejection	I
denseness	O
in	O
l	O
in	O
l	O
in	B
lp	B
with	B
respect	I
to	I
the	I
supremum	I
norm	I
density	B
estimation	B
density	B
of	I
a	I
class	I
of	I
sets	I
diamond	B
dimension	B
reduction	B
of	I
distance	O
data-based	B
empirical	B
euclidean	B
generalized	B
lp	B
rotation-invariant	B
beta	B
binomial	B
class-conditional	B
exponential	B
geometric	B
hypergeometric	B
maxwell	B
multinomial	B
normal	B
rayleigh	B
strictly	B
separable	I
distribution	B
function	I
class-conditional	B
empirical	B
divergence	O
see	O
kullback-leibler	B
divergence	I
dominated	B
convergence	I
theorem	I
dvoretzky-kiefer-wolfowitz-massart	O
inequality	B
elementary	B
cut	I
embedding	B
empirical	B
classifier	B
selection	I
see	O
classifier	B
selection	I
empirical	B
covering	O
see	O
covering	O
empirical	B
error	I
empirically	B
optimal	I
classifier	B
empirical	B
measure	I
subject	O
index	O
empirical	B
risk	O
see	O
empirical	B
error	I
empirical	B
risk	I
minimization	I
empirical	B
squared	O
euclidean	B
distance	O
error	O
bias	B
of	I
rotation	B
smoothed	B
u	O
see	O
leave-one-out	B
estimate	O
estimation	B
error	I
euclidean	B
norm	I
eulers	O
theorem	O
exponential	B
distribution	I
exponential	B
family	I
fanos	O
inequality	B
fatous	O
lemma	O
f-divergence	B
feature	B
extraction	I
entropy	B
e	O
number	O
see	O
covering	B
number	I
e	O
see	O
metric	B
entropy	B
em-optimality	B
e	O
number	O
see	O
packing	B
number	I
error	B
estimation	B
estimator	O
bootstrap	B
complexity	B
penalized	I
cross	O
validated	O
see	O
leave-one-out	B
estimate	O
deleted	O
see	O
leave-one-out	B
estimate	O
double	B
bootstrap	B
eo	B
estimator	I
error	B
counting	I
holdout	B
variance	B
of	I
posterior	B
probability	I
randomized	B
bootstrap	B
resubstitution	B
f	O
fingering	B
fingering	B
dimension	B
fisher	B
linear	I
discriminant	I
flexible	B
grid	I
fourier	B
series	I
classifier	B
fourier	B
series	I
density	B
estimation	B
fubinis	O
theorem	O
fundamental	B
rule	B
fundamental	O
theorem	O
of	O
mathematical	O
statistics	O
see	O
glivenko-cantelli	B
theorem	I
gabriel	B
graph	I
gabriel	B
neighbor	I
gamma	B
distribution	I
gaussian	B
distribution	O
see	O
normal	B
distribution	I
gaussian	B
noise	I
geometric	B
distribution	I
gessamans	O
rule	B
ghost	B
sample	I
giniindex	B
glivenko-cantelli	B
theorem	I
gradient	B
optimization	I
grenanders	O
method	O
of	O
sieves	O
grid	B
complexity	I
group	B
method	I
of	I
data	I
handling	I
hamming	B
distance	I
ham-sandwich	B
theorem	I
harts	O
rule	B
hellinger	B
distance	I
hellinger	B
integral	I
histogram	B
density	B
estimation	B
histogram	B
rule	B
cubic	B
data-dependent	B
lazy	B
randomized	B
hoeffdings	O
inequality	B
holders	O
inequality	B
hypergeometric	B
distribution	I
imperfect	B
training	I
impurity	B
function	I
inequality	B
subject	O
index	O
cover-hart	B
dvoretzky-kiefer-wolfowitz-massart	O
fanos	O
hoeffdings	O
holders	O
jensens	O
khinchines	O
large	B
deviation	I
lecams	O
markovs	O
mcdiarmids	O
pinskers	O
rogers-wagner	B
vapnik-chervonenkis	B
information	O
divergence	O
see	O
leibler	O
divergence	O
jeffreys	O
divergence	O
jensens	O
inequality	B
j	O
essen-marcinkiewitz-z	O
ygmund	O
theorem	O
alexanders	O
association	B
bennetts	O
bernsteins	O
cauchy-schwarz	B
chebyshev-cantelli	B
chebyshevs	O
karhunen-loeve	B
expansion	I
k-d	B
tree	I
chronological	B
deep	B
permutation-optimized	O
chronological	B
well-populated	B
kernel	B
subject	O
index	O
cauchy	B
de	B
la	I
vallee-poussin	I
devilish	B
epanechniko	O
exponential	B
gaussian	B
hermite	B
multiparameter	B
naive	B
negative	B
valued	I
polynomial	B
product	B
regular	B
star-shaped	B
uniform	O
see	O
naive	B
kernel	B
window	O
see	O
naive	B
kernel	B
kernel	B
complexity	I
kernel	B
density	B
estimation	B
kernel	B
rule	B
automatic	B
variable	B
khinchines	O
inequality	B
k-local	B
rule	B
k-means	B
clustering	B
kolmogorov-lorentz	B
network	I
kolmogorov-lorentz	B
representation	I
kolmogorov-smirnov	B
distance	I
generalized	B
kolmogorov-smirnov	B
statistic	I
kolmogorov	B
variational	I
distance	I
k-spacing	B
rule	B
kullback-leibler	B
divergence	I
l	O
consistency	B
l	O
convergence	O
l	O
distance	O
l	O
error	O
consistency	B
large	B
deviation	I
inequality	B
learning	B
algorithm	B
supervised	B
with	B
a	I
teacher	I
learning	B
vector	I
quantization	B
lebesgues	O
density	O
theorem	O
lecams	O
inequality	B
likelihood	B
product	B
linear	B
classifier	B
generalized	B
linear	O
discriminant	O
see	O
linear	B
classifier	B
linear	B
independence	I
linear	B
ordering	I
by	I
inclusion	I
lloyd-max	B
algorithm	B
local	B
average	I
estimator	I
logistic	B
discrimination	I
log-linear	B
model	I
l	B
p	I
error	I
lying	O
teacher	O
see	O
imperfect	B
training	I
mahalanobis	B
distance	I
majority	B
vote	I
markovs	O
inequality	B
martingale	B
martingale	B
difference	I
sequence	I
matushita	B
error	I
maximum	B
likelihood	I
distribution	B
format	I
regression	B
format	I
set	B
format	I
maxwell	B
distribution	I
mcdiarmids	O
inequality	B
median	B
tree	I
theoretical	B
method	B
of	I
bounded	I
differences	I
metric	O
see	O
distance	O
metric	B
entropy	B
minimax	B
lower	I
bound	I
minimum	B
description	I
length	I
principle	I
minimum	B
distance	I
estimation	B
moment	B
generating	I
function	I
monomial	B
monotone	B
layer	I
moving	B
window	I
rule	B
multinomial	B
discrimination	I
multinomial	B
distribution	I
multivariate	O
normal	B
distribution	I
see	O
normal	B
distribution	I
subject	O
index	O
nearest	B
neighbor	I
clustering	B
nearest	B
neighbor	I
density	B
estimation	B
nearest	B
neighbor	I
error	I
nearest	B
neighbor	I
rule	B
admissibility	O
of	O
the	O
rule	B
asymptotic	O
error	O
probability	O
of	O
the	O
rule	B
see	O
nearest	B
neighbor	I
error	I
asymptotic	O
error	O
probability	O
of	O
the	O
k-nn	B
rule	B
automatic	B
based	O
on	O
reference	O
data	O
see	O
prototype	B
nn	O
rule	B
condensed	B
data-based	B
see	O
automatic	B
nearest	B
neighbor	I
rule	B
edited	B
k-nn	B
l-nn	O
layered	B
prototype	B
recursive	B
relabeling	B
selective	B
variable	B
metric	I
weighted	B
neural	B
network	I
natural	B
classifier	B
nearest	B
neighbor	I
classifier	B
multilayer	B
with	B
one	I
hidden	I
layer	I
subject	O
index	O
with	B
two	I
hidden	I
layers	I
neyman-pearson	B
lemma	I
normal	B
distribution	I
order	B
statistics	I
outer	O
layer	O
see	O
monotone	B
layer	I
overfitting	B
packing	B
number	I
parameter	B
estimation	B
parametric	B
classification	I
parsevals	O
identity	O
partition	B
complexity	B
of	I
a	I
family	B
of	I
cubic	B
data-dependent	B
e	O
cardinality	O
of	O
family	B
of	I
partitioning	O
rule	B
see	O
histogram	B
rule	B
perceptron	B
perceptron	B
criterion	I
pigeonhole	B
principle	I
pinskers	O
inequality	B
planar	B
graph	I
plug-in	B
decision	I
polynomial	B
discriminant	I
function	I
polynomial	B
network	I
potential	B
function	I
rule	B
principal	B
component	I
analysis	I
probabilistic	B
method	I
projection	B
pursuit	I
pseudo	B
dimension	B
pseudo	B
probability	I
of	I
error	I
quadratic	B
discrimination	I
rule	B
quadtree	B
chronological	B
deep	B
quantization	B
radial	B
basis	B
function	I
radon-nikodym	B
derivative	I
radon-nikodym	B
theorem	I
rate	B
of	I
convergence	I
rayleigh	B
distribution	I
record	B
recursive	B
kernel	B
rule	B
regression	B
function	I
estimation	B
relative	B
stability	I
rogers-wagner	B
inequality	B
rotation	B
invariance	O
see	O
transformation	B
invariance	I
royalls	O
rule	B
sample	B
complexity	I
sample	B
scatter	I
sampling	B
without	I
replacement	I
subject	O
index	O
scale	O
invariance	O
see	O
transformation	B
invariance	I
standard	B
empirical	B
measure	I
see	O
empirical	B
scing	O
measure	O
scatter	B
matrix	I
scheffes	O
theorem	O
search	B
tree	I
balanced	B
shatter	B
coefficient	I
shattering	B
sieve	B
method	I
sigmoid	B
arctan	B
gaussian	B
logistic	O
see	O
standard	B
sigmoid	B
piecewise	B
polynomial	B
standard	B
threshold	B
signed	B
measure	I
skeleton	B
estimate	I
smart	B
rule	B
smoothing	B
factor	I
data-dependent	B
spacing	B
splitting	B
criterion	I
splitting	B
function	I
splitting	B
the	I
data	I
squared	B
error	I
statistically	B
equivalent	I
blocks	I
stirlings	O
formula	O
stochastic	B
approximation	I
stochastic	O
process	O
expansion	B
of	I
sampling	B
of	I
stoller	B
split	I
empirical	B
theoretical	B
stollers	O
rule	B
stones	O
lemma	O
stones	O
theorem	O
stone-weierstrass	B
theorem	I
strictly	B
separable	I
distribution	I
strong	B
consistency	B
definition	B
of	I
of	B
fourier	I
series	I
rule	B
of	B
kernel	B
rules	I
of	B
nearest	B
neighbor	I
rules	I
of	B
neural	B
network	I
classifiers	I
of	B
partitioning	I
rules	I
strong	B
universal	B
consistency	B
of	B
complexity	B
regularization	I
definition	B
of	I
of	B
generalized	B
linear	I
rules	I
of	B
kernel	B
rules	I
of	B
nearest	B
neighbor	I
rules	I
of	B
neural	B
network	I
classifiers	I
of	B
partitioning	I
rules	I
structural	O
risk	O
minimization	O
see	O
complexity	B
regularization	I
sufficient	B
statistics	I
superclassifier	B
subject	O
index	O
support	B
symmetric	B
rule	B
symmetrization	B
tennis	B
rule	B
testing	B
sequence	I
total	B
boundedness	I
total	B
positivity	I
total	B
variation	B
transformation	B
invariance	I
tree	O
a-balanced	B
binary	B
bsp	O
see	O
binary	B
space	I
partition	B
tree	I
classifier	B
depth	B
of	I
expression	B
greedy	B
height	B
of	I
horton-strahler	B
number	I
of	I
hyperplane	O
see	O
binary	B
space	I
partition	B
tree	I
k-d	O
see	O
k-d	B
tree	I
median	O
see	O
median	B
tree	I
ordered	B
ordinary	B
perpendicular	B
splitting	I
pruning	O
of	O
see	O
trimming	B
of	I
a	O
tree	O
sphere	B
trimming	B
of	I
unbiased	B
estimator	I
uniform	O
deviations	O
see	O
uniform	B
laws	I
of	I
large	I
numbers	I
uniform	B
marginal	I
transformation	I
universal	B
consistency	B
of	B
complexity	B
regularization	I
definition	B
of	I
of	B
generalized	B
linear	I
rules	I
ofkemel	B
rules	I
of	B
nearest	B
neighbor	I
rules	I
of	B
neural	B
network	I
classifiers	I
of	B
partitioning	I
rules	I
vapnik-chervonenkis	B
dimension	B
see	O
vc	B
dimension	B
vapnik-chervonenkis	B
inequality	B
variation	B
vc	B
class	I
vc	B
dimension	B
voronoi	B
cell	I
voronoi	B
partition	B
weighted	B
average	I
estimator	I
uniform	B
laws	I
of	I
large	I
numbers	I
x-property	B
pattern	O
recognition	O
presents	O
one	O
of	O
the	O
most	O
significant	O
challenges	O
for	O
scientists	O
and	O
engineers	O
and	O
many	O
different	O
approaches	O
have	O
been	O
proposed	O
the	O
aim	O
of	O
this	O
book	O
is	O
to	O
provide	O
a	O
self-contained	O
account	O
of	O
probabilistic	O
analysis	O
of	O
these	O
approaches	O
the	O
book	O
includes	O
a	O
discussion	O
of	O
distance	O
measures	O
nonparametric	O
methods	O
based	O
on	O
kernels	O
or	O
nearest	O
neighbors	O
vapnik-chervonenkis	B
ry	O
epsilon	O
entropy	B
parametric	B
classification	I
error	B
estimation	B
tree	O
classifiers	O
and	O
neural	O
networks	O
wherever	O
possible	O
distribution-free	O
properties	O
and	O
inequalities	O
are	O
derived	O
a	O
substantial	O
portion	O
of	O
the	O
results	O
or	O
the	O
analysis	O
is	O
new	O
over	O
problems	O
and	O
exercises	O
complement	O
the	O
material	O
isbn	O
isb	O
