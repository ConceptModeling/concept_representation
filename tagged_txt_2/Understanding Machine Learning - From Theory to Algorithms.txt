understanding	O
machine	O
learning	O
from	O
theory	O
to	O
algorithms	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
this	O
copy	O
is	O
for	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
please	O
note	O
this	O
copy	O
is	O
almost	O
but	O
not	O
entirely	O
identical	O
to	O
the	O
printed	O
version	O
of	O
the	O
book	O
in	O
particular	O
page	O
numbers	O
are	O
not	O
identical	O
section	O
numbers	O
are	O
the	O
same	O
understandingmachinelearningmachinelearningisoneofthefastestgrowingareasofcomputersciencewithfar-reachingapplications	O
theaimofthistextbookistointroducemachinelearningandthealgorithmicparadigmsitoffersinaprinci-pledway	O
thebookprovidesanextensivetheoreticalaccountofthefundamentalideasunderlyingmachinelearningandthemathematicalderivationsthattransformtheseprinciplesintopracticalalgorithms	O
fol-lowingapresentationofthebasicsofthefieldthebookcoversawidearrayofcentraltopicsthathavenotbeenaddressedbyprevioustext-books	O
theseincludeadiscussionofthecomputationalcomplexityoflearningandtheconceptsofconvexityandstabilityimportantalgorith-micparadigmsincludingstochasticgradientdescentneuralnetworksandstructuredoutputlearningandemergingtheoreticalconceptssuchasthepac-bayesapproachandcompression-basedbounds	O
designedforanadvancedundergraduateorbeginninggraduatecoursethetextmakesthefundamentalsandalgorithmsofmachinelearningaccessibletostu-dentsandnonexpertreadersinstatisticscomputersciencemathematicsandengineering	O
shaishalev-shwartzisanassociateprofessorattheschoolofcomputerscienceandengineeringatthehebrewuniversityisrael	O
shaiben-davidisaprofessorintheschoolofcomputerscienceattheuniversityofwaterloocanada	O
understandingmachinelearningfromtheorytoalgorithmsshaishalev-shwartzthehebrewuniversityjerusalemshaiben-daviduniversityofwaterloocanada	O
triple-sdedicatesthebooktotriple-m	O
vii	O
preface	O
the	O
term	O
machine	O
learning	O
refers	O
to	O
the	O
automated	O
detection	O
of	O
meaningful	O
patterns	O
in	O
data	O
in	O
the	O
past	O
couple	O
of	O
decades	O
it	O
has	O
become	O
a	O
common	O
tool	O
in	O
almost	O
any	O
task	O
that	O
requires	O
information	O
extraction	O
from	O
large	O
data	O
sets	O
we	O
are	O
surrounded	O
by	O
a	O
machine	O
learning	O
based	O
technology	O
search	O
engines	O
learn	O
how	O
to	O
bring	O
us	O
the	O
best	O
results	O
placing	O
profitable	O
ads	O
anti-spam	O
software	O
learns	O
to	O
filter	O
our	O
email	O
messages	O
and	O
credit	O
card	O
transactions	O
are	O
secured	O
by	O
a	O
software	O
that	O
learns	O
how	O
to	O
detect	O
frauds	O
digital	O
cameras	O
learn	O
to	O
detect	O
faces	O
and	O
intelligent	O
personal	O
assistance	O
applications	O
on	O
smart-phones	O
learn	O
to	O
recognize	O
voice	O
commands	O
cars	O
are	O
equipped	O
with	O
accident	O
prevention	O
systems	O
that	O
are	O
built	O
using	O
machine	O
learning	O
algorithms	O
machine	O
learning	O
is	O
also	O
widely	O
used	O
in	O
scientific	O
applications	O
such	O
as	O
bioinformatics	O
medicine	O
and	O
astronomy	O
one	O
common	O
feature	B
of	O
all	O
of	O
these	O
applications	O
is	O
that	O
in	O
contrast	O
to	O
more	O
traditional	O
uses	O
of	O
computers	O
in	O
these	O
cases	O
due	O
to	O
the	O
complexity	O
of	O
the	O
patterns	O
that	O
need	O
to	O
be	O
detected	O
a	O
human	O
programmer	O
cannot	O
provide	O
an	O
explicit	O
finedetailed	O
specification	O
of	O
how	O
such	O
tasks	O
should	O
be	O
executed	O
taking	O
example	O
from	O
intelligent	O
beings	O
many	O
of	O
our	O
skills	O
are	O
acquired	O
or	O
refined	O
through	O
learning	O
from	O
our	O
experience	O
than	O
following	O
explicit	O
instructions	O
given	O
to	O
us	O
machine	O
learning	O
tools	O
are	O
concerned	O
with	O
endowing	O
programs	O
with	O
the	O
ability	O
to	O
learn	O
and	O
adapt	O
the	O
first	O
goal	O
of	O
this	O
book	O
is	O
to	O
provide	O
a	O
rigorous	O
yet	O
easy	O
to	O
follow	O
introduction	O
to	O
the	O
main	O
concepts	O
underlying	O
machine	O
learning	O
what	O
is	O
learning	O
how	O
can	O
a	O
machine	O
learn	O
how	O
do	O
we	O
quantify	O
the	O
resources	O
needed	O
to	O
learn	O
a	O
given	O
concept	O
is	O
learning	O
always	O
possible	O
can	O
we	O
know	O
if	O
the	O
learning	O
process	O
succeeded	O
or	O
failed	O
the	O
second	O
goal	O
of	O
this	O
book	O
is	O
to	O
present	O
several	O
key	O
machine	O
learning	O
algorithms	O
we	O
chose	O
to	O
present	O
algorithms	O
that	O
on	O
one	O
hand	O
are	O
successfully	O
used	O
in	O
practice	O
and	O
on	O
the	O
other	O
hand	O
give	O
a	O
wide	O
spectrum	O
of	O
different	O
learning	O
techniques	O
additionally	O
we	O
pay	O
specific	O
attention	O
to	O
algorithms	O
appropriate	O
for	O
large	O
scale	O
learning	O
big	O
data	O
since	O
in	O
recent	O
years	O
our	O
world	O
has	O
become	O
increasingly	O
digitized	O
and	O
the	O
amount	O
of	O
data	O
available	O
for	O
learning	O
is	O
dramatically	O
increasing	O
as	O
a	O
result	O
in	O
many	O
applications	O
data	O
is	O
plentiful	O
and	O
computation	O
time	O
is	O
the	O
main	O
bottleneck	O
we	O
therefore	O
explicitly	O
quantify	O
both	O
the	O
amount	O
of	O
data	O
and	O
the	O
amount	O
of	O
computation	O
time	O
needed	O
to	O
learn	O
a	O
given	O
concept	O
the	O
book	O
is	O
divided	O
into	O
four	O
parts	O
the	O
first	O
part	O
aims	O
at	O
giving	O
an	O
initial	O
rigorous	O
answer	O
to	O
the	O
fundamental	O
questions	O
of	O
learning	O
we	O
describe	O
a	O
generalization	O
of	O
valiant	O
s	O
probably	O
approximately	O
correct	O
learning	O
model	O
which	O
is	O
a	O
first	O
solid	O
answer	O
to	O
the	O
question	O
what	O
is	O
learning	O
we	O
describe	O
the	O
empirical	B
risk	B
minimization	O
structural	O
risk	B
minimization	O
and	O
minimum	O
description	O
length	O
learning	O
rules	O
which	O
shows	O
how	O
can	O
a	O
machine	O
learn	O
we	O
quantify	O
the	O
amount	O
of	O
data	O
needed	O
for	O
learning	O
using	O
the	O
erm	B
srm	B
and	O
mdl	B
rules	O
and	O
show	O
how	O
learning	O
might	O
fail	O
by	O
deriving	O
viii	O
a	O
no-free-lunch	B
theorem	O
we	O
also	O
discuss	O
how	O
much	O
computation	O
time	O
is	O
required	O
for	O
learning	O
in	O
the	O
second	O
part	O
of	O
the	O
book	O
we	O
describe	O
various	O
learning	O
algorithms	O
for	O
some	O
of	O
the	O
algorithms	O
we	O
first	O
present	O
a	O
more	O
general	O
learning	O
principle	O
and	O
then	O
show	O
how	O
the	O
algorithm	O
follows	O
the	O
principle	O
while	O
the	O
first	O
two	O
parts	O
of	O
the	O
book	O
focus	O
on	O
the	O
pac	B
model	O
the	O
third	O
part	O
extends	O
the	O
scope	O
by	O
presenting	O
a	O
wider	O
variety	O
of	O
learning	O
models	O
finally	O
the	O
last	O
part	O
of	O
the	O
book	O
is	O
devoted	O
to	O
advanced	O
theory	O
we	O
made	O
an	O
attempt	O
to	O
keep	O
the	O
book	O
as	O
self-contained	O
as	O
possible	O
however	O
the	O
reader	O
is	O
assumed	O
to	O
be	O
comfortable	O
with	O
basic	O
notions	O
of	O
probability	O
linear	O
algebra	O
analysis	O
and	O
algorithms	O
the	O
first	O
three	O
parts	O
of	O
the	O
book	O
are	O
intended	O
for	O
first	O
year	O
graduate	O
students	O
in	O
computer	O
science	O
engineering	O
mathematics	O
or	O
statistics	O
it	O
can	O
also	O
be	O
accessible	O
to	O
undergraduate	O
students	O
with	O
the	O
adequate	O
background	O
the	O
more	O
advanced	O
chapters	O
can	O
be	O
used	O
by	O
researchers	O
intending	O
to	O
gather	O
a	O
deeper	O
theoretical	O
understanding	O
acknowledgements	O
the	O
book	O
is	O
based	O
on	O
introduction	O
to	O
machine	O
learning	O
courses	O
taught	O
by	O
shai	O
shalev-shwartz	O
at	O
the	O
hebrew	O
university	O
and	O
by	O
shai	O
ben-david	O
at	O
the	O
university	O
of	O
waterloo	O
the	O
first	O
draft	O
of	O
the	O
book	O
grew	O
out	O
of	O
the	O
lecture	O
notes	O
for	O
the	O
course	O
that	O
was	O
taught	O
at	O
the	O
hebrew	O
university	O
by	O
shai	O
shalev-shwartz	O
during	O
we	O
greatly	O
appreciate	O
the	O
help	O
of	O
ohad	O
shamir	O
who	O
served	O
as	O
a	O
ta	O
for	O
the	O
course	O
in	O
and	O
of	O
alon	O
gonen	O
who	O
served	O
as	O
a	O
ta	O
for	O
the	O
course	O
in	O
ohad	O
and	O
alon	O
prepared	O
few	O
lecture	O
notes	O
and	O
many	O
of	O
the	O
exercises	O
alon	O
to	O
whom	O
we	O
are	O
indebted	O
for	O
his	O
help	O
throughout	O
the	O
entire	O
making	O
of	O
the	O
book	O
has	O
also	O
prepared	O
a	O
solution	O
manual	O
we	O
are	O
deeply	O
grateful	O
for	O
the	O
most	O
valuable	O
work	O
of	O
dana	O
rubinstein	O
dana	O
has	O
scientifically	O
proofread	O
and	O
edited	O
the	O
manuscript	O
transforming	O
it	O
from	O
lecture-based	O
chapters	O
into	O
fluent	O
and	O
coherent	O
text	O
special	O
thanks	O
to	O
amit	O
daniely	O
who	O
helped	O
us	O
with	O
a	O
careful	O
read	O
of	O
the	O
advanced	O
part	O
of	O
the	O
book	O
and	O
also	O
wrote	O
the	O
advanced	O
chapter	O
on	O
multiclass	B
learnability	O
we	O
are	O
also	O
grateful	O
for	O
the	O
members	O
of	O
a	O
book	O
reading	O
club	O
in	O
jerusalem	O
that	O
have	O
carefully	O
read	O
and	O
constructively	O
criticized	O
every	O
line	O
of	O
the	O
manuscript	O
the	O
members	O
of	O
the	O
reading	O
club	O
are	O
maya	O
alroy	O
yossi	O
arjevani	O
aharon	O
birnbaum	O
alon	O
cohen	O
alon	O
gonen	O
roi	O
livni	O
ofer	O
meshi	O
dan	O
rosenbaum	O
dana	O
rubinstein	O
shahar	O
somin	O
alon	O
vinnikov	O
and	O
yoav	O
wald	O
we	O
would	O
also	O
like	O
to	O
thank	O
gal	O
elidan	O
amir	O
globerson	O
nika	O
haghtalab	O
shie	O
mannor	O
amnon	O
shashua	O
nati	O
srebro	O
and	O
ruth	O
urner	O
for	O
helpful	O
discussions	O
shai	O
shalev-shwartz	O
jerusalem	O
israel	O
shai	O
ben-david	O
waterloo	O
canada	O
contents	O
preface	O
page	O
vii	O
introduction	O
what	O
is	O
learning	O
when	O
do	O
we	O
need	O
machine	O
learning	O
types	O
of	O
learning	O
relations	O
to	O
other	O
fields	O
how	O
to	O
read	O
this	O
book	O
notation	O
possible	O
course	O
plans	O
based	O
on	O
this	O
book	O
part	O
i	O
foundations	O
a	O
gentle	O
start	O
a	O
formal	O
model	O
the	O
statistical	O
learning	O
framework	O
empirical	B
risk	B
minimization	O
something	O
may	O
go	O
wrong	O
overfitting	B
empirical	B
risk	B
minimization	O
with	O
inductive	O
bias	B
exercises	O
finite	O
hypothesis	B
classes	O
a	O
formal	O
learning	O
model	O
pac	B
learning	O
a	O
more	O
general	O
learning	O
model	O
releasing	O
the	O
realizability	B
assumption	O
agnostic	B
pac	B
learning	O
the	O
scope	O
of	O
learning	O
problems	O
modeled	O
summary	O
bibliographic	O
remarks	O
exercises	O
learning	O
via	O
uniform	B
convergence	I
uniform	B
convergence	I
is	O
sufficient	O
for	O
learnability	O
finite	O
classes	O
are	O
agnostic	B
pac	B
learnable	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
x	O
contents	O
summary	O
bibliographic	O
remarks	O
exercises	O
the	O
bias-complexity	B
tradeoff	I
the	O
no-free-lunch	B
theorem	O
no-free-lunch	B
and	O
prior	B
knowledge	I
error	B
decomposition	I
summary	O
bibliographic	O
remarks	O
exercises	O
the	O
vc-dimension	O
intervals	O
finite	O
classes	O
infinite-size	O
classes	O
can	O
be	O
learnable	O
the	O
vc-dimension	O
examples	O
threshold	O
functions	O
axis	O
aligned	O
rectangles	O
vc-dimension	O
and	O
the	O
number	O
of	O
parameters	O
the	O
fundamental	O
theorem	O
of	O
pac	B
learning	O
proof	O
of	O
theorem	O
uniform	B
convergence	I
for	O
classes	O
of	O
small	O
effective	O
size	O
summary	O
bibliographic	O
remarks	O
exercises	O
sauer	O
s	O
lemma	O
and	O
the	O
growth	B
function	B
nonuniform	O
learnability	O
nonuniform	O
learnability	O
characterizing	O
nonuniform	O
learnability	O
structural	O
risk	B
minimization	O
minimum	O
description	O
length	O
and	O
occam	O
s	O
razor	O
occam	O
s	O
razor	O
other	O
notions	O
of	O
learnability	O
consistency	B
discussing	O
the	O
different	O
notions	O
of	O
learnability	O
the	O
no-free-lunch	B
theorem	O
revisited	O
summary	O
bibliographic	O
remarks	O
exercises	O
the	O
runtime	O
of	O
learning	O
computational	B
complexity	I
of	O
learning	O
contents	O
xi	O
finite	O
classes	O
formal	O
definition	O
boolean	B
conjunctions	I
learning	O
dnf	O
implementing	O
the	O
erm	B
rule	O
axis	O
aligned	O
rectangles	O
efficiently	O
learnable	O
but	O
not	O
by	O
a	O
proper	B
erm	B
hardness	O
of	O
learning	O
summary	O
bibliographic	O
remarks	O
exercises	O
part	O
ii	O
from	O
theory	O
to	O
algorithms	O
linear	B
programming	I
for	O
the	O
class	O
of	O
halfspaces	O
perceptron	B
for	O
halfspaces	O
least	B
squares	I
linear	B
regression	B
for	O
polynomial	B
regression	B
tasks	O
linear	B
predictors	I
halfspaces	O
the	O
vc	B
dimension	I
of	O
halfspaces	O
linear	B
regression	B
logistic	B
regression	B
summary	O
bibliographic	O
remarks	O
exercises	O
boosting	B
weak	O
learnability	O
efficient	O
implementation	O
of	O
erm	B
for	O
decision	B
stumps	I
adaboost	B
linear	O
combinations	O
of	O
base	O
hypotheses	O
the	O
vc-dimension	O
of	O
lb	O
t	O
adaboost	B
for	O
face	O
recognition	O
summary	O
bibliographic	O
remarks	O
exercises	O
model	B
selection	I
and	O
validation	B
model	B
selection	I
using	O
srm	B
validation	B
hold	B
out	I
set	B
validation	B
for	O
model	B
selection	I
the	O
model-selection	O
curve	O
xii	O
contents	O
k-fold	O
cross	B
validation	B
train-validation-test	B
split	I
what	O
to	O
do	O
if	O
learning	O
fails	O
summary	O
exercises	O
convex	B
learning	O
problems	O
convexity	O
lipschitzness	B
and	O
smoothness	B
convexity	O
lipschitzness	B
smoothness	B
convex	B
learning	O
problems	O
learnability	O
of	O
convex	B
learning	O
problems	O
convex-lipschitzsmooth-bounded	O
learning	O
problems	O
surrogate	B
loss	B
functions	O
summary	O
bibliographic	O
remarks	O
exercises	O
regularization	B
and	O
stability	B
regularized	O
loss	B
minimization	O
ridge	B
regression	B
stable	O
rules	O
do	O
not	O
overfit	O
tikhonov	B
regularization	B
as	O
a	O
stabilizer	O
lipschitz	B
loss	B
smooth	O
and	O
nonnegative	O
loss	B
controlling	O
the	O
fitting-stability	O
tradeoff	O
summary	O
bibliographic	O
remarks	O
exercises	O
stochastic	O
gradient	B
descent	I
gradient	B
descent	I
analysis	O
of	O
gd	O
for	O
convex-lipschitz	O
functions	O
subgradients	O
calculating	O
subgradients	O
subgradients	O
of	O
lipschitz	O
functions	O
subgradient	O
descent	O
stochastic	O
gradient	B
descent	I
analysis	O
of	O
sgd	B
for	O
convex-lipschitz-bounded	O
functions	O
variants	O
adding	O
a	O
projection	B
step	O
variable	O
step	O
size	O
other	O
averaging	O
techniques	O
contents	O
xiii	O
strongly	B
convex	B
functions	O
learning	O
with	O
sgd	B
sgd	B
for	O
risk	B
minimization	O
analyzing	O
sgd	B
for	O
convex-smooth	O
learning	O
problems	O
sgd	B
for	O
regularized	O
loss	B
minimization	O
summary	O
bibliographic	O
remarks	O
exercises	O
support	O
vector	O
machines	O
margin	B
and	O
hard-svm	B
the	O
homogenous	B
case	O
the	O
sample	B
complexity	I
of	O
hard-svm	B
soft-svm	B
and	O
norm	O
regularization	B
the	O
sample	B
complexity	I
of	O
soft-svm	B
margin	B
and	O
norm-based	O
bounds	O
versus	O
dimension	O
the	O
ramp	B
loss	B
implementing	O
soft-svm	B
using	O
sgd	B
optimality	O
conditions	O
and	O
support	B
vectors	I
duality	B
summary	O
bibliographic	O
remarks	O
exercises	O
kernel	O
methods	O
embeddings	O
into	O
feature	B
spaces	O
the	O
kernel	B
trick	I
kernels	B
as	O
a	O
way	O
to	O
express	O
prior	B
knowledge	I
characterizing	O
kernel	O
functions	O
implementing	O
soft-svm	B
with	O
kernels	B
summary	O
bibliographic	O
remarks	O
exercises	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
one-versus-all	O
and	O
all-pairs	B
linear	O
multiclass	B
predictors	O
how	O
to	O
construct	O
cost-sensitive	B
classification	O
erm	B
generalized	O
hinge	B
loss	B
multiclass	B
svm	B
and	O
sgd	B
structured	B
output	I
prediction	I
ranking	B
xiv	O
contents	O
linear	B
predictors	I
for	O
ranking	B
bipartite	B
ranking	B
and	O
multivariate	B
performance	I
measures	I
linear	B
predictors	I
for	O
bipartite	B
ranking	B
summary	O
bibliographic	O
remarks	O
exercises	O
decision	B
trees	I
sample	B
complexity	I
decision	O
tree	O
algorithms	O
implementations	O
of	O
the	O
gain	B
measure	O
pruning	B
threshold-based	O
splitting	O
rules	O
for	O
real-valued	O
features	O
random	B
forests	I
summary	O
bibliographic	O
remarks	O
exercises	O
nearest	B
neighbor	I
k	O
nearest	O
neighbors	O
analysis	O
a	O
generalization	O
bound	O
for	O
the	O
rule	O
the	O
curse	B
of	I
dimensionality	I
efficient	O
implementation	O
summary	O
bibliographic	O
remarks	O
exercises	O
neural	B
networks	I
feedforward	O
neural	B
networks	I
learning	O
neural	B
networks	I
the	O
expressive	O
power	O
of	O
neural	B
networks	I
geometric	O
intuition	O
the	O
sample	B
complexity	I
of	O
neural	B
networks	I
the	O
runtime	O
of	O
learning	O
neural	B
networks	I
sgd	B
and	O
backpropagation	B
summary	O
bibliographic	O
remarks	O
exercises	O
part	O
iii	O
additional	O
learning	O
models	O
online	B
learning	I
online	B
classification	O
in	O
the	O
realizable	O
case	O
contents	O
xv	O
online	B
learnability	O
online	B
classification	O
in	O
the	O
unrealizable	O
case	O
weighted-majority	B
online	B
convex	B
optimization	I
the	O
online	B
perceptron	B
algorithm	O
summary	O
bibliographic	O
remarks	O
exercises	O
clustering	B
linkage-based	O
clustering	B
algorithms	O
k-means	B
and	O
other	O
cost	O
minimization	O
clusterings	O
the	O
k-means	B
algorithm	O
spectral	B
clustering	B
graph	O
cut	O
graph	O
laplacian	O
and	O
relaxed	O
graph	O
cuts	O
unnormalized	O
spectral	B
clustering	B
information	B
bottleneck	I
a	O
high	O
level	O
view	O
of	O
clustering	B
summary	O
bibliographic	O
remarks	O
exercises	O
dimensionality	B
reduction	I
principal	O
component	O
analysis	O
a	O
more	O
efficient	O
solution	O
for	O
the	O
case	O
d	O
m	O
implementation	O
and	O
demonstration	O
random	B
projections	I
compressed	B
sensing	I
proofs	O
pca	B
or	O
compressed	B
sensing	I
summary	O
bibliographic	O
remarks	O
exercises	O
generative	B
models	I
maximum	B
likelihood	I
estimator	O
maximum	B
likelihood	I
estimation	O
for	O
continuous	O
ran	O
dom	O
variables	O
maximum	B
likelihood	I
and	O
empirical	B
risk	B
minimization	O
generalization	O
analysis	O
naive	B
bayes	I
linear	O
discriminant	O
analysis	O
latent	B
variables	I
and	O
the	O
em	B
algorithm	O
xvi	O
contents	O
em	B
as	O
an	O
alternate	O
maximization	O
algorithm	O
em	B
for	O
mixture	B
of	I
gaussians	I
k-means	B
bayesian	B
reasoning	I
summary	O
bibliographic	O
remarks	O
exercises	O
feature	B
selection	I
and	O
generation	O
feature	B
selection	I
filters	O
greedy	O
selection	O
approaches	O
sparsity-inducing	B
norms	I
feature	B
manipulation	O
and	O
normalization	O
examples	O
of	O
feature	B
transformations	I
feature	B
learning	I
dictionary	B
learning	I
using	O
auto-encoders	B
summary	O
bibliographic	O
remarks	O
exercises	O
part	O
iv	O
advanced	O
theory	O
rademacher	O
complexities	O
the	O
rademacher	B
complexity	I
rademacher	O
calculus	O
rademacher	B
complexity	I
of	O
linear	O
classes	O
generalization	B
bounds	I
for	O
svm	B
generalization	B
bounds	I
for	O
predictors	O
with	O
low	O
norm	O
bibliographic	O
remarks	O
covering	B
numbers	I
covering	O
properties	O
from	O
covering	O
to	O
rademacher	B
complexity	I
via	O
chaining	B
bibliographic	O
remarks	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
the	O
upper	O
bound	O
for	O
the	O
agnostic	O
case	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
showing	O
that	O
m	O
showing	O
that	O
m	O
the	O
upper	O
bound	O
for	O
the	O
realizable	O
case	O
from	O
to	O
pac	B
learnability	O
multiclass	B
learnability	O
the	O
natarajan	B
dimension	I
the	O
multiclass	B
fundamental	O
theorem	O
on	O
the	O
proof	O
of	O
theorem	O
calculating	O
the	O
natarajan	B
dimension	I
one-versus-all	O
based	O
classes	O
general	O
multiclass-to-binary	O
reductions	B
linear	O
multiclass	B
predictors	O
on	O
good	O
and	O
bad	O
erms	O
bibliographic	O
remarks	O
exercises	O
compression	B
bounds	I
compression	B
bounds	I
examples	O
axis	O
aligned	O
rectangles	O
halfspaces	O
separating	O
polynomials	O
separation	O
with	O
margin	B
bibliographic	O
remarks	O
pac-bayes	B
pac-bayes	B
bounds	O
bibliographic	O
remarks	O
exercises	O
appendix	O
a	O
technical	O
lemmas	O
appendix	O
b	O
measure	B
concentration	I
appendix	O
c	O
linear	O
algebra	O
notes	O
references	O
index	O
contents	O
xvii	O
introduction	O
the	O
subject	O
of	O
this	O
book	O
is	O
automated	O
learning	O
or	O
as	O
we	O
will	O
more	O
often	O
call	O
it	O
machine	O
learning	O
that	O
is	O
we	O
wish	O
to	O
program	O
computers	O
so	O
that	O
they	O
can	O
learn	O
from	O
input	O
available	O
to	O
them	O
roughly	O
speaking	O
learning	O
is	O
the	O
process	O
of	O
converting	O
experience	O
into	O
expertise	O
or	O
knowledge	O
the	O
input	O
to	O
a	O
learning	O
algorithm	O
is	O
training	O
data	O
representing	O
experience	O
and	O
the	O
output	O
is	O
some	O
expertise	O
which	O
usually	O
takes	O
the	O
form	O
of	O
another	O
computer	O
program	O
that	O
can	O
perform	O
some	O
task	O
seeking	O
a	O
formal-mathematical	O
understanding	O
of	O
this	O
concept	O
we	O
ll	O
have	O
to	O
be	O
more	O
explicit	O
about	O
what	O
we	O
mean	O
by	O
each	O
of	O
the	O
involved	O
terms	O
what	O
is	O
the	O
training	O
data	O
our	O
programs	O
will	O
access	O
how	O
can	O
the	O
process	O
of	O
learning	O
be	O
automated	O
how	O
can	O
we	O
evaluate	O
the	O
success	O
of	O
such	O
a	O
process	O
the	O
quality	O
of	O
the	O
output	O
of	O
a	O
learning	O
program	O
what	O
is	O
learning	O
let	O
us	O
begin	O
by	O
considering	O
a	O
couple	O
of	O
examples	O
from	O
naturally	O
occurring	O
animal	O
learning	O
some	O
of	O
the	O
most	O
fundamental	O
issues	O
in	O
ml	O
arise	O
already	O
in	O
that	O
context	O
which	O
we	O
are	O
all	O
familiar	O
with	O
bait	O
shyness	O
rats	O
learning	O
to	O
avoid	O
poisonous	O
baits	O
when	O
rats	O
encounter	O
food	O
items	O
with	O
novel	O
look	O
or	O
smell	O
they	O
will	O
first	O
eat	O
very	O
small	O
amounts	O
and	O
subsequent	O
feeding	O
will	O
depend	O
on	O
the	O
flavor	O
of	O
the	O
food	O
and	O
its	O
physiological	O
effect	O
if	O
the	O
food	O
produces	O
an	O
ill	O
effect	O
the	O
novel	O
food	O
will	O
often	O
be	O
associated	O
with	O
the	O
illness	O
and	O
subsequently	O
the	O
rats	O
will	O
not	O
eat	O
it	O
clearly	O
there	O
is	O
a	O
learning	O
mechanism	O
in	O
play	O
here	O
the	O
animal	O
used	O
past	O
experience	O
with	O
some	O
food	O
to	O
acquire	O
expertise	O
in	O
detecting	O
the	O
safety	O
of	O
this	O
food	O
if	O
past	O
experience	O
with	O
the	O
food	O
was	O
negatively	O
labeled	O
the	O
animal	O
predicts	O
that	O
it	O
will	O
also	O
have	O
a	O
negative	O
effect	O
when	O
encountered	O
in	O
the	O
future	O
inspired	O
by	O
the	O
preceding	O
example	O
of	O
successful	O
learning	O
let	O
us	O
demonstrate	O
a	O
typical	O
machine	O
learning	O
task	O
suppose	O
we	O
would	O
like	O
to	O
program	O
a	O
machine	O
that	O
learns	O
how	O
to	O
filter	O
spam	O
e-mails	O
a	O
naive	O
solution	O
would	O
be	O
seemingly	O
similar	O
to	O
the	O
way	O
rats	O
learn	O
how	O
to	O
avoid	O
poisonous	O
baits	O
the	O
machine	O
will	O
simply	O
memorize	O
all	O
previous	O
e-mails	O
that	O
had	O
been	O
labeled	O
as	O
spam	O
e-mails	O
by	O
the	O
human	O
user	O
when	O
a	O
new	O
e-mail	O
arrives	O
the	O
machine	O
will	O
search	O
for	O
it	O
in	O
the	O
set	B
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
introduction	O
of	O
previous	O
spam	O
e-mails	O
if	O
it	O
matches	O
one	O
of	O
them	O
it	O
will	O
be	O
trashed	O
otherwise	O
it	O
will	O
be	O
moved	O
to	O
the	O
user	O
s	O
inbox	O
folder	O
while	O
the	O
preceding	O
learning	O
by	O
memorization	O
approach	O
is	O
sometimes	O
useful	O
it	O
lacks	O
an	O
important	O
aspect	O
of	O
learning	O
systems	O
the	O
ability	O
to	O
label	B
unseen	O
e-mail	O
messages	O
a	O
successful	O
learner	O
should	O
be	O
able	O
to	O
progress	O
from	O
individual	O
examples	O
to	O
broader	O
generalization	O
this	O
is	O
also	O
referred	O
to	O
as	O
inductive	O
reasoning	O
or	O
inductive	O
inference	O
in	O
the	O
bait	O
shyness	O
example	O
presented	O
previously	O
after	O
the	O
rats	O
encounter	O
an	O
example	O
of	O
a	O
certain	O
type	O
of	O
food	O
they	O
apply	O
their	O
attitude	O
toward	O
it	O
on	O
new	O
unseen	O
examples	O
of	O
food	O
of	O
similar	O
smell	O
and	O
taste	O
to	O
achieve	O
generalization	O
in	O
the	O
spam	O
filtering	O
task	O
the	O
learner	O
can	O
scan	O
the	O
previously	O
seen	O
e-mails	O
and	O
extract	O
a	O
set	B
of	O
words	O
whose	O
appearance	O
in	O
an	O
e-mail	O
message	O
is	O
indicative	O
of	O
spam	O
then	O
when	O
a	O
new	O
e-mail	O
arrives	O
the	O
machine	O
can	O
check	O
whether	O
one	O
of	O
the	O
suspicious	O
words	O
appears	O
in	O
it	O
and	O
predict	O
its	O
label	B
accordingly	O
such	O
a	O
system	O
would	O
potentially	O
be	O
able	O
correctly	O
to	O
predict	O
the	O
label	B
of	O
unseen	O
e-mails	O
however	O
inductive	O
reasoning	O
might	O
lead	O
us	O
to	O
false	O
conclusions	O
to	O
illustrate	O
this	O
let	O
us	O
consider	O
again	O
an	O
example	O
from	O
animal	O
learning	O
pigeon	O
superstition	O
in	O
an	O
experiment	O
performed	O
by	O
the	O
psychologist	O
b	O
f	O
skinner	O
he	O
placed	O
a	O
bunch	O
of	O
hungry	O
pigeons	O
in	O
a	O
cage	O
an	O
automatic	O
mechanism	O
had	O
been	O
attached	O
to	O
the	O
cage	O
delivering	O
food	O
to	O
the	O
pigeons	O
at	O
regular	O
intervals	O
with	O
no	O
reference	O
whatsoever	O
to	O
the	O
birds	O
behavior	O
the	O
hungry	O
pigeons	O
went	O
around	O
the	O
cage	O
and	O
when	O
food	O
was	O
first	O
delivered	O
it	O
found	O
each	O
pigeon	O
engaged	O
in	O
some	O
activity	O
turning	O
the	O
head	O
etc	O
the	O
arrival	O
of	O
food	O
reinforced	O
each	O
bird	O
s	O
specific	O
action	O
and	O
consequently	O
each	O
bird	O
tended	O
to	O
spend	O
some	O
more	O
time	O
doing	O
that	O
very	O
same	O
action	O
that	O
in	O
turn	O
increased	O
the	O
chance	O
that	O
the	O
next	O
random	O
food	O
delivery	O
would	O
find	O
each	O
bird	O
engaged	O
in	O
that	O
activity	O
again	O
what	O
results	O
is	O
a	O
chain	O
of	O
events	O
that	O
reinforces	O
the	O
pigeons	O
association	O
of	O
the	O
delivery	O
of	O
the	O
food	O
with	O
whatever	O
chance	O
actions	O
they	O
had	O
been	O
performing	O
when	O
it	O
was	O
first	O
delivered	O
they	O
subsequently	O
continue	O
to	O
perform	O
these	O
same	O
actions	O
what	O
distinguishes	O
learning	O
mechanisms	O
that	O
result	O
in	O
superstition	O
from	O
useful	O
learning	O
this	O
question	O
is	O
crucial	O
to	O
the	O
development	O
of	O
automated	O
learners	O
while	O
human	O
learners	O
can	O
rely	O
on	O
common	O
sense	O
to	O
filter	O
out	O
random	O
meaningless	O
learning	O
conclusions	O
once	O
we	O
export	O
the	O
task	O
of	O
learning	O
to	O
a	O
machine	O
we	O
must	O
provide	O
well	O
defined	O
crisp	O
principles	O
that	O
will	O
protect	O
the	O
program	O
from	O
reaching	O
senseless	O
or	O
useless	O
conclusions	O
the	O
development	O
of	O
such	O
principles	O
is	O
a	O
central	O
goal	O
of	O
the	O
theory	O
of	O
machine	O
learning	O
what	O
then	O
made	O
the	O
rats	O
learning	O
more	O
successful	O
than	O
that	O
of	O
the	O
pigeons	O
as	O
a	O
first	O
step	O
toward	O
answering	O
this	O
question	O
let	O
us	O
have	O
a	O
closer	O
look	O
at	O
the	O
bait	O
shyness	O
phenomenon	O
in	O
rats	O
bait	O
shyness	O
revisited	O
rats	O
fail	O
to	O
acquire	O
conditioning	O
between	O
food	O
and	O
electric	O
shock	O
or	O
between	O
sound	O
and	O
nausea	O
the	O
bait	O
shyness	O
mechanism	O
in	O
see	O
httppsychclassics	O
yorku	O
caskinnerpigeon	O
when	O
do	O
we	O
need	O
machine	O
learning	O
rats	O
turns	O
out	O
to	O
be	O
more	O
complex	O
than	O
what	O
one	O
may	O
expect	O
in	O
experiments	O
carried	O
out	O
by	O
garcia	O
koelling	O
it	O
was	O
demonstrated	O
that	O
if	O
the	O
unpleasant	O
stimulus	O
that	O
follows	O
food	O
consumption	O
is	O
replaced	O
by	O
say	O
electrical	O
shock	O
than	O
nausea	O
then	O
no	O
conditioning	O
occurs	O
even	O
after	O
repeated	O
trials	O
in	O
which	O
the	O
consumption	O
of	O
some	O
food	O
is	O
followed	O
by	O
the	O
administration	O
of	O
unpleasant	O
electrical	O
shock	O
the	O
rats	O
do	O
not	O
tend	O
to	O
avoid	O
that	O
food	O
similar	O
failure	O
of	O
conditioning	O
occurs	O
when	O
the	O
characteristic	O
of	O
the	O
food	O
that	O
implies	O
nausea	O
as	O
taste	O
or	O
smell	O
is	O
replaced	O
by	O
a	O
vocal	O
signal	O
the	O
rats	O
seem	O
to	O
have	O
some	O
built	O
in	O
prior	B
knowledge	I
telling	O
them	O
that	O
while	O
temporal	O
correlation	O
between	O
food	O
and	O
nausea	O
can	O
be	O
causal	O
it	O
is	O
unlikely	O
that	O
there	O
would	O
be	O
a	O
causal	O
relationship	O
between	O
food	O
consumption	O
and	O
electrical	O
shocks	O
or	O
between	O
sounds	O
and	O
nausea	O
we	O
conclude	O
that	O
one	O
distinguishing	O
feature	B
between	O
the	O
bait	O
shyness	O
learning	O
and	O
the	O
pigeon	O
superstition	O
is	O
the	O
incorporation	O
of	O
prior	B
knowledge	I
that	O
biases	O
the	O
learning	O
mechanism	O
this	O
is	O
also	O
referred	O
to	O
as	O
inductive	O
bias	B
the	O
pigeons	O
in	O
the	O
experiment	O
are	O
willing	O
to	O
adopt	O
any	O
explanation	O
for	O
the	O
occurrence	O
of	O
food	O
however	O
the	O
rats	O
know	O
that	O
food	O
cannot	O
cause	O
an	O
electric	O
shock	O
and	O
that	O
the	O
co-occurrence	O
of	O
noise	O
with	O
some	O
food	O
is	O
not	O
likely	O
to	O
affect	O
the	O
nutritional	O
value	O
of	O
that	O
food	O
the	O
rats	O
learning	O
process	O
is	O
biased	O
toward	O
detecting	O
some	O
kind	O
of	O
patterns	O
while	O
ignoring	O
other	O
temporal	O
correlations	O
between	O
events	O
it	O
turns	O
out	O
that	O
the	O
incorporation	O
of	O
prior	B
knowledge	I
biasing	O
the	O
learning	O
process	O
is	O
inevitable	O
for	O
the	O
success	O
of	O
learning	O
algorithms	O
is	O
formally	O
stated	O
and	O
proved	O
as	O
the	O
no-free-lunch	B
theorem	O
in	O
chapter	O
the	O
development	O
of	O
tools	O
for	O
expressing	O
domain	B
expertise	O
translating	O
it	O
into	O
a	O
learning	O
bias	B
and	O
quantifying	O
the	O
effect	O
of	O
such	O
a	O
bias	B
on	O
the	O
success	O
of	O
learning	O
is	O
a	O
central	O
theme	O
of	O
the	O
theory	O
of	O
machine	O
learning	O
roughly	O
speaking	O
the	O
stronger	O
the	O
prior	B
knowledge	I
prior	O
assumptions	O
that	O
one	O
starts	O
the	O
learning	O
process	O
with	O
the	O
easier	O
it	O
is	O
to	O
learn	O
from	O
further	O
examples	O
however	O
the	O
stronger	O
these	O
prior	O
assumptions	O
are	O
the	O
less	O
flexible	O
the	O
learning	O
is	O
it	O
is	O
bound	O
a	O
priori	O
by	O
the	O
commitment	O
to	O
these	O
assumptions	O
we	O
shall	O
discuss	O
these	O
issues	O
explicitly	O
in	O
chapter	O
when	O
do	O
we	O
need	O
machine	O
learning	O
when	O
do	O
we	O
need	O
machine	O
learning	O
rather	O
than	O
directly	O
program	O
our	O
computers	O
to	O
carry	O
out	O
the	O
task	O
at	O
hand	O
two	O
aspects	O
of	O
a	O
given	O
problem	O
may	O
call	O
for	O
the	O
use	O
of	O
programs	O
that	O
learn	O
and	O
improve	O
on	O
the	O
basis	O
of	O
their	O
experience	O
the	O
problem	O
s	O
complexity	O
and	O
the	O
need	O
for	O
adaptivity	O
tasks	O
that	O
are	O
too	O
complex	O
to	O
program	O
tasks	O
performed	O
by	O
animalshumans	O
there	O
are	O
numerous	O
tasks	O
that	O
we	O
human	O
beings	O
perform	O
routinely	O
yet	O
our	O
introspection	O
concerning	O
how	O
we	O
do	O
them	O
is	O
not	O
sufficiently	O
elaborate	O
to	O
extract	O
a	O
well	O
introduction	O
defined	O
program	O
examples	O
of	O
such	O
tasks	O
include	O
driving	O
speech	O
recognition	O
and	O
image	O
understanding	O
in	O
all	O
of	O
these	O
tasks	O
state	O
of	O
the	O
art	O
machine	O
learning	O
programs	O
programs	O
that	O
learn	O
from	O
their	O
experience	O
achieve	O
quite	O
satisfactory	O
results	O
once	O
exposed	O
to	O
sufficiently	O
many	O
training	O
examples	O
tasks	O
beyond	O
human	O
capabilities	O
another	O
wide	O
family	O
of	O
tasks	O
that	O
benefit	O
from	O
machine	O
learning	O
techniques	O
are	O
related	O
to	O
the	O
analysis	O
of	O
very	O
large	O
and	O
complex	O
data	O
sets	O
astronomical	O
data	O
turning	O
medical	O
archives	O
into	O
medical	O
knowledge	O
weather	O
prediction	O
analysis	O
of	O
genomic	O
data	O
web	O
search	O
engines	O
and	O
electronic	O
commerce	O
with	O
more	O
and	O
more	O
available	O
digitally	O
recorded	O
data	O
it	O
becomes	O
obvious	O
that	O
there	O
are	O
treasures	O
of	O
meaningful	O
information	O
buried	O
in	O
data	O
archives	O
that	O
are	O
way	O
too	O
large	O
and	O
too	O
complex	O
for	O
humans	O
to	O
make	O
sense	O
of	O
learning	O
to	O
detect	O
meaningful	O
patterns	O
in	O
large	O
and	O
complex	O
data	O
sets	O
is	O
a	O
promising	O
domain	B
in	O
which	O
the	O
combination	O
of	O
programs	O
that	O
learn	O
with	O
the	O
almost	O
unlimited	O
memory	O
capacity	O
and	O
ever	O
increasing	O
processing	O
speed	O
of	O
computers	O
opens	O
up	O
new	O
horizons	O
adaptivity	O
one	O
limiting	O
feature	B
of	O
programmed	O
tools	O
is	O
their	O
rigidity	O
once	O
the	O
program	O
has	O
been	O
written	O
down	O
and	O
installed	O
it	O
stays	O
unchanged	O
however	O
many	O
tasks	O
change	O
over	O
time	O
or	O
from	O
one	O
user	O
to	O
another	O
machine	O
learning	O
tools	O
programs	O
whose	O
behavior	O
adapts	O
to	O
their	O
input	O
data	O
offer	O
a	O
solution	O
to	O
such	O
issues	O
they	O
are	O
by	O
nature	O
adaptive	O
to	O
changes	O
in	O
the	O
environment	O
they	O
interact	O
with	O
typical	O
successful	O
applications	O
of	O
machine	O
learning	O
to	O
such	O
problems	O
include	O
programs	O
that	O
decode	O
handwritten	O
text	O
where	O
a	O
fixed	O
program	O
can	O
adapt	O
to	O
variations	O
between	O
the	O
handwriting	O
of	O
different	O
users	O
spam	O
detection	O
programs	O
adapting	O
automatically	O
to	O
changes	O
in	O
the	O
nature	O
of	O
spam	O
e-mails	O
and	O
speech	O
recognition	O
programs	O
types	O
of	O
learning	O
learning	O
is	O
of	O
course	O
a	O
very	O
wide	O
domain	B
consequently	O
the	O
field	O
of	O
machine	O
learning	O
has	O
branched	O
into	O
several	O
subfields	O
dealing	O
with	O
different	O
types	O
of	O
learning	O
tasks	O
we	O
give	O
a	O
rough	O
taxonomy	O
of	O
learning	O
paradigms	O
aiming	O
to	O
provide	O
some	O
perspective	O
of	O
where	O
the	O
content	O
of	O
this	O
book	O
sits	O
within	O
the	O
wide	O
field	O
of	O
machine	O
learning	O
we	O
describe	O
four	O
parameters	O
along	O
which	O
learning	O
paradigms	O
can	O
be	O
classified	O
supervised	O
versus	O
unsupervised	O
since	O
learning	O
involves	O
an	O
interaction	O
between	O
the	O
learner	O
and	O
the	O
environment	O
one	O
can	O
divide	O
learning	O
tasks	O
according	O
to	O
the	O
nature	O
of	O
that	O
interaction	O
the	O
first	O
distinction	O
to	O
note	O
is	O
the	O
difference	O
between	O
supervised	O
and	O
unsupervised	B
learning	I
as	O
an	O
types	O
of	O
learning	O
illustrative	O
example	O
consider	O
the	O
task	O
of	O
learning	O
to	O
detect	O
spam	O
e-mail	O
versus	O
the	O
task	O
of	O
anomaly	O
detection	O
for	O
the	O
spam	O
detection	O
task	O
we	O
consider	O
a	O
setting	O
in	O
which	O
the	O
learner	O
receives	O
training	O
e-mails	O
for	O
which	O
the	O
label	B
spamnot-spam	O
is	O
provided	O
on	O
the	O
basis	O
of	O
such	O
training	O
the	O
learner	O
should	O
figure	O
out	O
a	O
rule	O
for	O
labeling	O
a	O
newly	O
arriving	O
e-mail	O
message	O
in	O
contrast	O
for	O
the	O
task	O
of	O
anomaly	O
detection	O
all	O
the	O
learner	O
gets	O
as	O
training	O
is	O
a	O
large	O
body	O
of	O
e-mail	O
messages	O
no	O
labels	O
and	O
the	O
learner	O
s	O
task	O
is	O
to	O
detect	O
unusual	O
messages	O
more	O
abstractly	O
viewing	O
learning	O
as	O
a	O
process	O
of	O
using	O
experience	O
to	O
gain	B
expertise	O
supervised	O
learning	O
describes	O
a	O
scenario	O
in	O
which	O
the	O
experience	O
a	O
training	O
example	O
contains	O
significant	O
information	O
the	O
spamnot-spam	O
labels	O
that	O
is	O
missing	O
in	O
the	O
unseen	O
test	O
examples	O
to	O
which	O
the	O
learned	O
expertise	O
is	O
to	O
be	O
applied	O
in	O
this	O
setting	O
the	O
acquired	O
expertise	O
is	O
aimed	O
to	O
predict	O
that	O
missing	O
information	O
for	O
the	O
test	O
data	O
in	O
such	O
cases	O
we	O
can	O
think	O
of	O
the	O
environment	O
as	O
a	O
teacher	O
that	O
supervises	O
the	O
learner	O
by	O
providing	O
the	O
extra	O
information	O
in	O
unsupervised	B
learning	I
however	O
there	O
is	O
no	O
distinction	O
between	O
training	O
and	O
test	O
data	O
the	O
learner	O
processes	O
input	O
data	O
with	O
the	O
goal	O
of	O
coming	O
up	O
with	O
some	O
summary	O
or	O
compressed	O
version	O
of	O
that	O
data	O
clustering	B
a	O
data	O
set	B
into	O
subsets	O
of	O
similar	O
objets	O
is	O
a	O
typical	O
example	O
of	O
such	O
a	O
task	O
there	O
is	O
also	O
an	O
intermediate	O
learning	O
setting	O
in	O
which	O
while	O
the	O
training	O
examples	O
contain	O
more	O
information	O
than	O
the	O
test	O
examples	O
the	O
learner	O
is	O
required	O
to	O
predict	O
even	O
more	O
information	O
for	O
the	O
test	O
examples	O
for	O
example	O
one	O
may	O
try	O
to	O
learn	O
a	O
value	O
function	B
that	O
describes	O
for	O
each	O
setting	O
of	O
a	O
chess	O
board	O
the	O
degree	O
by	O
which	O
white	O
s	O
position	O
is	O
better	O
than	O
the	O
black	O
s	O
yet	O
the	O
only	O
information	O
available	O
to	O
the	O
learner	O
at	O
training	O
time	O
is	O
positions	O
that	O
occurred	O
throughout	O
actual	O
chess	O
games	O
labeled	O
by	O
who	O
eventually	O
won	O
that	O
game	O
such	O
learning	O
frameworks	O
are	O
mainly	O
investigated	O
under	O
the	O
title	O
of	O
reinforcement	O
learning	O
active	O
versus	O
passive	O
learners	O
learning	O
paradigms	O
can	O
vary	O
by	O
the	O
role	O
played	O
by	O
the	O
learner	O
we	O
distinguish	O
between	O
active	O
and	O
passive	O
learners	O
an	O
active	O
learner	O
interacts	O
with	O
the	O
environment	O
at	O
training	O
time	O
say	O
by	O
posing	O
queries	O
or	O
performing	O
experiments	O
while	O
a	O
passive	O
learner	O
only	O
observes	O
the	O
information	O
provided	O
by	O
the	O
environment	O
the	O
teacher	O
without	O
influencing	O
or	O
directing	O
it	O
note	O
that	O
the	O
learner	O
of	O
a	O
spam	O
filter	O
is	O
usually	O
passive	O
waiting	O
for	O
users	O
to	O
mark	O
the	O
e-mails	O
coming	O
to	O
them	O
in	O
an	O
active	O
setting	O
one	O
could	O
imagine	O
asking	O
users	O
to	O
label	B
specific	O
e-mails	O
chosen	O
by	O
the	O
learner	O
or	O
even	O
composed	O
by	O
the	O
learner	O
to	O
enhance	O
what	O
spam	O
is	O
understanding	O
its	O
of	O
helpfulness	O
of	O
the	O
teacher	O
when	O
one	O
thinks	O
about	O
human	O
learning	O
of	O
a	O
baby	O
at	O
home	O
or	O
a	O
student	O
at	O
school	O
the	O
process	O
often	O
involves	O
a	O
helpful	O
teacher	O
who	O
is	O
trying	O
to	O
feed	O
the	O
learner	O
with	O
the	O
information	O
most	O
use	O
introduction	O
ful	O
for	O
achieving	O
the	O
learning	O
goal	O
in	O
contrast	O
when	O
a	O
scientist	O
learns	O
about	O
nature	O
the	O
environment	O
playing	O
the	O
role	O
of	O
the	O
teacher	O
can	O
be	O
best	O
thought	O
of	O
as	O
passive	O
apples	O
drop	O
stars	O
shine	O
and	O
the	O
rain	O
falls	O
without	O
regard	O
to	O
the	O
needs	O
of	O
the	O
learner	O
we	O
model	O
such	O
learning	O
scenarios	O
by	O
postulating	O
that	O
the	O
training	O
data	O
the	O
learner	O
s	O
experience	O
is	O
generated	O
by	O
some	O
random	O
process	O
this	O
is	O
the	O
basic	O
building	O
block	O
in	O
the	O
branch	O
of	O
statistical	O
learning	O
finally	O
learning	O
also	O
occurs	O
when	O
the	O
learner	O
s	O
input	O
is	O
generated	O
by	O
an	O
adversarial	O
teacher	O
this	O
may	O
be	O
the	O
case	O
in	O
the	O
spam	O
filtering	O
example	O
the	O
spammer	O
makes	O
an	O
effort	O
to	O
mislead	O
the	O
spam	O
filtering	O
designer	O
or	O
in	O
learning	O
to	O
detect	O
fraud	O
one	O
also	O
uses	O
an	O
adversarial	O
teacher	O
model	O
as	O
a	O
worst-case	O
scenario	O
when	O
no	O
milder	O
setup	O
can	O
be	O
safely	O
assumed	O
if	O
you	O
can	O
learn	O
against	O
an	O
adversarial	O
teacher	O
you	O
are	O
guaranteed	O
to	O
succeed	O
interacting	O
any	O
odd	O
teacher	O
online	B
versus	O
batch	O
learning	O
protocol	O
the	O
last	O
parameter	O
we	O
mention	O
is	O
the	O
distinction	O
between	O
situations	O
in	O
which	O
the	O
learner	O
has	O
to	O
respond	O
online	B
throughout	O
the	O
learning	O
process	O
and	O
settings	O
in	O
which	O
the	O
learner	O
has	O
to	O
engage	O
the	O
acquired	O
expertise	O
only	O
after	O
having	O
a	O
chance	O
to	O
process	O
large	O
amounts	O
of	O
data	O
for	O
example	O
a	O
stockbroker	O
has	O
to	O
make	O
daily	O
decisions	O
based	O
on	O
the	O
experience	O
collected	O
so	O
far	O
he	O
may	O
become	O
an	O
expert	O
over	O
time	O
but	O
might	O
have	O
made	O
costly	O
mistakes	O
in	O
the	O
process	O
in	O
contrast	O
in	O
many	O
data	O
mining	O
settings	O
the	O
learner	O
the	O
data	O
miner	O
has	O
large	O
amounts	O
of	O
training	O
data	O
to	O
play	O
with	O
before	O
having	O
to	O
output	O
conclusions	O
in	O
this	O
book	O
we	O
shall	O
discuss	O
only	O
a	O
subset	O
of	O
the	O
possible	O
learning	O
paradigms	O
our	O
main	O
focus	O
is	O
on	O
supervised	O
statistical	O
batch	O
learning	O
with	O
a	O
passive	O
learner	O
example	O
trying	O
to	O
learn	O
how	O
to	O
generate	O
patients	O
prognoses	O
based	O
on	O
large	O
archives	O
of	O
records	O
of	O
patients	O
that	O
were	O
independently	O
collected	O
and	O
are	O
already	O
labeled	O
by	O
the	O
fate	O
of	O
the	O
recorded	O
patients	O
we	O
shall	O
also	O
briefly	O
discuss	O
online	B
learning	I
and	O
batch	O
unsupervised	B
learning	I
particular	O
clustering	B
relations	O
to	O
other	O
fields	O
as	O
an	O
interdisciplinary	O
field	O
machine	O
learning	O
shares	O
common	O
threads	O
with	O
the	O
mathematical	O
fields	O
of	O
statistics	O
information	O
theory	O
game	O
theory	O
and	O
optimization	O
it	O
is	O
naturally	O
a	O
subfield	O
of	O
computer	O
science	O
as	O
our	O
goal	O
is	O
to	O
program	O
machines	O
so	O
that	O
they	O
will	O
learn	O
in	O
a	O
sense	O
machine	O
learning	O
can	O
be	O
viewed	O
as	O
a	O
branch	O
of	O
ai	O
intelligence	O
since	O
after	O
all	O
the	O
ability	O
to	O
turn	O
experience	O
into	O
expertise	O
or	O
to	O
detect	O
meaningful	O
patterns	O
in	O
complex	O
sensory	O
data	O
is	O
a	O
cornerstone	O
of	O
human	O
animal	O
intelligence	O
however	O
one	O
should	O
note	O
that	O
in	O
contrast	O
with	O
traditional	O
ai	O
machine	O
learning	O
is	O
not	O
trying	O
to	O
build	O
automated	O
imitation	O
of	O
intelligent	O
behavior	O
but	O
rather	O
to	O
use	O
the	O
strengths	O
and	O
how	O
to	O
read	O
this	O
book	O
special	O
abilities	O
of	O
computers	O
to	O
complement	O
human	O
intelligence	O
often	O
performing	O
tasks	O
that	O
fall	O
way	O
beyond	O
human	O
capabilities	O
for	O
example	O
the	O
ability	O
to	O
scan	O
and	O
process	O
huge	O
databases	O
allows	O
machine	O
learning	O
programs	O
to	O
detect	O
patterns	O
that	O
are	O
outside	O
the	O
scope	O
of	O
human	O
perception	O
the	O
component	O
of	O
experience	O
or	O
training	O
in	O
machine	O
learning	O
often	O
refers	O
to	O
data	O
that	O
is	O
randomly	O
generated	O
the	O
task	O
of	O
the	O
learner	O
is	O
to	O
process	O
such	O
randomly	O
generated	O
examples	O
toward	O
drawing	O
conclusions	O
that	O
hold	O
for	O
the	O
environment	O
from	O
which	O
these	O
examples	O
are	O
picked	O
this	O
description	O
of	O
machine	O
learning	O
highlights	O
its	O
close	O
relationship	O
with	O
statistics	O
indeed	O
there	O
is	O
a	O
lot	O
in	O
common	O
between	O
the	O
two	O
disciplines	O
in	O
terms	O
of	O
both	O
the	O
goals	O
and	O
techniques	O
used	O
there	O
are	O
however	O
a	O
few	O
significant	O
differences	O
of	O
emphasis	O
if	O
a	O
doctor	O
comes	O
up	O
with	O
the	O
hypothesis	B
that	O
there	O
is	O
a	O
correlation	O
between	O
smoking	O
and	O
heart	O
disease	O
it	O
is	O
the	O
statistician	O
s	O
role	O
to	O
view	O
samples	O
of	O
patients	O
and	O
check	O
the	O
validity	O
of	O
that	O
hypothesis	B
is	O
the	O
common	O
statistical	O
task	O
of	O
hypothesis	B
testing	O
in	O
contrast	O
machine	O
learning	O
aims	O
to	O
use	O
the	O
data	O
gathered	O
from	O
samples	O
of	O
patients	O
to	O
come	O
up	O
with	O
a	O
description	O
of	O
the	O
causes	O
of	O
heart	O
disease	O
the	O
hope	O
is	O
that	O
automated	O
techniques	O
may	O
be	O
able	O
to	O
figure	O
out	O
meaningful	O
patterns	O
hypotheses	O
that	O
may	O
have	O
been	O
missed	O
by	O
the	O
human	O
observer	O
in	O
contrast	O
with	O
traditional	O
statistics	O
in	O
machine	O
learning	O
in	O
general	O
and	O
in	O
this	O
book	O
in	O
particular	O
algorithmic	O
considerations	O
play	O
a	O
major	O
role	O
machine	O
learning	O
is	O
about	O
the	O
execution	O
of	O
learning	O
by	O
computers	O
hence	O
algorithmic	O
issues	O
are	O
pivotal	O
we	O
develop	O
algorithms	O
to	O
perform	O
the	O
learning	O
tasks	O
and	O
are	O
concerned	O
with	O
their	O
computational	O
efficiency	O
another	O
difference	O
is	O
that	O
while	O
statistics	O
is	O
often	O
interested	O
in	O
asymptotic	O
behavior	O
the	O
convergence	O
of	O
sample-based	O
statistical	O
estimates	O
as	O
the	O
sample	O
sizes	O
grow	O
to	O
infinity	O
the	O
theory	O
of	O
machine	O
learning	O
focuses	O
on	O
finite	O
sample	O
bounds	O
namely	O
given	O
the	O
size	O
of	O
available	O
samples	O
machine	O
learning	O
theory	O
aims	O
to	O
figure	O
out	O
the	O
degree	O
of	O
accuracy	B
that	O
a	O
learner	O
can	O
expect	O
on	O
the	O
basis	O
of	O
such	O
samples	O
there	O
are	O
further	O
differences	O
between	O
these	O
two	O
disciplines	O
of	O
which	O
we	O
shall	O
mention	O
only	O
one	O
more	O
here	O
while	O
in	O
statistics	O
it	O
is	O
common	O
to	O
work	O
under	O
the	O
assumption	O
of	O
certain	O
presubscribed	O
data	O
models	O
as	O
assuming	O
the	O
normality	O
of	O
data-generating	O
distributions	O
or	O
the	O
linearity	O
of	O
functional	O
dependencies	O
in	O
machine	O
learning	O
the	O
emphasis	O
is	O
on	O
working	O
under	O
a	O
distribution-free	O
setting	O
where	O
the	O
learner	O
assumes	O
as	O
little	O
as	O
possible	O
about	O
the	O
nature	O
of	O
the	O
data	O
distribution	O
and	O
allows	O
the	O
learning	O
algorithm	O
to	O
figure	O
out	O
which	O
models	O
best	O
approximate	O
the	O
data-generating	O
process	O
a	O
precise	O
discussion	O
of	O
this	O
issue	O
requires	O
some	O
technical	O
preliminaries	O
and	O
we	O
will	O
come	O
back	O
to	O
it	O
later	O
in	O
the	O
book	O
and	O
in	O
particular	O
in	O
chapter	O
how	O
to	O
read	O
this	O
book	O
the	O
first	O
part	O
of	O
the	O
book	O
provides	O
the	O
basic	O
theoretical	O
principles	O
that	O
underlie	O
machine	O
learning	O
in	O
a	O
sense	O
this	O
is	O
the	O
foundation	O
upon	O
which	O
the	O
rest	O
introduction	O
of	O
the	O
book	O
is	O
built	O
this	O
part	O
could	O
serve	O
as	O
a	O
basis	O
for	O
a	O
minicourse	O
on	O
the	O
theoretical	O
foundations	O
of	O
ml	O
the	O
second	O
part	O
of	O
the	O
book	O
introduces	O
the	O
most	O
commonly	O
used	O
algorithmic	O
approaches	O
to	O
supervised	O
machine	O
learning	O
a	O
subset	O
of	O
these	O
chapters	O
may	O
also	O
be	O
used	O
for	O
introducing	O
machine	O
learning	O
in	O
a	O
general	O
ai	O
course	O
to	O
computer	O
science	O
math	O
or	O
engineering	O
students	O
the	O
third	O
part	O
of	O
the	O
book	O
extends	O
the	O
scope	O
of	O
discussion	O
from	O
statistical	O
classification	O
to	O
other	O
learning	O
models	O
it	O
covers	O
online	B
learning	I
unsupervised	B
learning	I
dimensionality	B
reduction	I
generative	B
models	I
and	O
feature	B
learning	I
the	O
fourth	O
part	O
of	O
the	O
book	O
advanced	O
theory	O
is	O
geared	O
toward	O
readers	O
who	O
have	O
interest	O
in	O
research	O
and	O
provides	O
the	O
more	O
technical	O
mathematical	O
techniques	O
that	O
serve	O
to	O
analyze	O
and	O
drive	O
forward	O
the	O
field	O
of	O
theoretical	O
machine	O
learning	O
the	O
appendixes	O
provide	O
some	O
technical	O
tools	O
used	O
in	O
the	O
book	O
in	O
particular	O
we	O
list	O
basic	O
results	O
from	O
measure	B
concentration	I
and	O
linear	O
algebra	O
a	O
few	O
sections	O
are	O
marked	O
by	O
an	O
asterisk	O
which	O
means	O
they	O
are	O
addressed	O
to	O
more	O
advanced	O
students	O
each	O
chapter	O
is	O
concluded	O
with	O
a	O
list	O
of	O
exercises	O
a	O
solution	O
manual	O
is	O
provided	O
in	O
the	O
course	O
web	O
site	O
possible	O
course	O
plans	O
based	O
on	O
this	O
book	O
a	O
week	O
introduction	O
course	O
for	O
graduate	O
students	O
chapters	O
chapter	O
the	O
vc	O
calculation	O
chapters	O
proofs	O
chapter	O
chapters	O
proofs	O
chapters	O
some	O
of	O
the	O
easier	O
proofs	O
chapter	O
some	O
of	O
the	O
easier	O
proofs	O
chapter	O
chapter	O
chapter	O
chapter	O
chapter	O
proofs	O
for	O
compressed	B
sensing	I
chapter	O
chapter	O
a	O
week	O
advanced	O
course	O
for	O
graduate	O
students	O
chapters	O
chapters	O
chapter	O
chapter	O
notation	O
chapter	O
chapters	O
chapter	O
chapter	O
chapter	O
chapter	O
chapter	O
chapter	O
chapter	O
notation	O
most	O
of	O
the	O
notation	O
we	O
use	O
throughout	O
the	O
book	O
is	O
either	O
standard	O
or	O
defined	O
on	O
the	O
spot	O
in	O
this	O
section	O
we	O
describe	O
our	O
main	O
conventions	O
and	O
provide	O
a	O
table	O
summarizing	O
our	O
notation	O
the	O
reader	O
is	O
encouraged	O
to	O
skip	O
this	O
section	O
and	O
return	O
to	O
it	O
if	O
during	O
the	O
reading	O
of	O
the	O
book	O
some	O
notation	O
is	O
unclear	O
we	O
denote	O
scalars	O
and	O
abstract	O
objects	O
with	O
lowercase	O
letters	O
x	O
and	O
often	O
we	O
would	O
like	O
to	O
emphasize	O
that	O
some	O
object	O
is	O
a	O
vector	O
and	O
then	O
we	O
use	O
boldface	O
letters	O
x	O
and	O
the	O
ith	O
element	O
of	O
a	O
vector	O
x	O
is	O
denoted	O
by	O
xi	O
we	O
use	O
uppercase	O
letters	O
to	O
denote	O
matrices	O
sets	O
and	O
sequences	O
the	O
meaning	O
should	O
be	O
clear	O
from	O
the	O
context	O
as	O
we	O
will	O
see	O
momentarily	O
the	O
input	O
of	O
a	O
learning	O
algorithm	O
is	O
a	O
sequence	O
of	O
training	O
examples	O
we	O
denote	O
by	O
z	O
an	O
abstract	O
example	O
and	O
by	O
s	O
zm	O
a	O
sequence	O
of	O
m	O
examples	O
historically	O
s	O
is	O
often	O
referred	O
to	O
as	O
a	O
training	B
set	B
however	O
we	O
will	O
always	O
assume	O
that	O
s	O
is	O
a	O
sequence	O
rather	O
than	O
a	O
set	B
a	O
sequence	O
of	O
m	O
vectors	O
is	O
denoted	O
by	O
xm	O
the	O
ith	O
element	O
of	O
xt	O
is	O
denoted	O
by	O
xti	O
throughout	O
the	O
book	O
we	O
make	O
use	O
of	O
basic	O
notions	O
from	O
probability	O
we	O
denote	O
by	O
d	O
a	O
distribution	O
over	O
some	O
for	O
example	O
z	O
we	O
use	O
the	O
notation	O
z	O
d	O
to	O
denote	O
that	O
z	O
is	O
sampled	O
according	O
to	O
d	O
given	O
a	O
random	O
variable	O
f	O
z	O
r	O
its	O
expected	O
value	O
is	O
denoted	O
by	O
ez	O
df	O
we	O
sometimes	O
use	O
the	O
shorthand	O
ef	O
when	O
the	O
dependence	O
on	O
z	O
is	O
clear	O
from	O
the	O
context	O
for	O
f	O
z	O
false	O
we	O
also	O
use	O
pz	O
df	O
to	O
denote	O
dz	O
f	O
true	O
in	O
the	O
next	O
chapter	O
we	O
will	O
also	O
introduce	O
the	O
notation	O
dm	O
to	O
denote	O
the	O
probability	O
over	O
z	O
m	O
induced	O
by	O
sampling	O
zm	O
where	O
each	O
point	O
zi	O
is	O
sampled	O
from	O
d	O
independently	O
of	O
the	O
other	O
points	O
in	O
general	O
we	O
have	O
made	O
an	O
effort	O
to	O
avoid	O
asymptotic	O
notation	O
however	O
we	O
occasionally	O
use	O
it	O
to	O
clarify	O
the	O
main	O
results	O
in	O
particular	O
given	O
f	O
r	O
r	O
and	O
g	O
r	O
r	O
we	O
write	O
f	O
og	O
if	O
there	O
exist	O
r	O
such	O
that	O
for	O
all	O
x	O
we	O
have	O
f	O
gx	O
we	O
write	O
f	O
og	O
if	O
for	O
every	O
there	O
exists	O
to	O
be	O
mathematically	O
precise	O
d	O
should	O
be	O
defined	O
over	O
some	O
of	O
subsets	O
of	O
z	O
the	O
user	O
who	O
is	O
not	O
familiar	O
with	O
measure	O
theory	O
can	O
skip	O
the	O
few	O
footnotes	O
and	O
remarks	O
regarding	O
more	O
formal	O
measurability	O
definitions	O
and	O
assumptions	O
introduction	O
symbol	O
r	O
rd	O
r	O
n	O
o	O
o	O
o	O
expression	O
x	O
v	O
w	O
xi	O
vi	O
wi	O
or	O
a	O
rdk	O
aij	O
x	O
xm	O
xij	O
wt	O
wt	O
x	O
y	O
z	O
h	O
h	O
z	O
r	O
d	O
da	O
z	O
d	O
s	O
zm	O
s	O
dm	O
p	O
e	O
pz	O
df	O
ez	O
df	O
n	O
c	O
i	O
f	O
wi	O
f	O
f	O
minx	O
c	O
f	O
maxx	O
c	O
f	O
argminx	O
c	O
f	O
argmaxx	O
c	O
f	O
log	O
table	O
summary	O
of	O
notation	O
meaning	O
the	O
set	B
of	O
real	O
numbers	O
the	O
set	B
of	O
d-dimensional	O
vectors	O
over	O
r	O
the	O
set	B
of	O
non-negative	O
real	O
numbers	O
the	O
set	B
of	O
natural	O
numbers	O
asymptotic	O
notation	O
text	O
indicator	O
function	B
if	O
expression	O
is	O
true	O
and	O
o	O
w	O
a	O
the	O
set	B
n	O
n	O
n	O
vectors	O
the	O
ith	O
element	O
of	O
a	O
vector	O
norm	O
of	O
x	O
xivi	O
product	O
norm	O
of	O
x	O
maxi	O
norm	O
of	O
x	O
the	O
number	O
of	O
nonzero	O
elements	O
of	O
x	O
a	O
d	O
k	O
matrix	O
over	O
r	O
the	O
transpose	O
of	O
a	O
the	O
j	O
element	O
of	O
a	O
the	O
d	O
d	O
matrix	O
a	O
s	O
t	O
aij	O
xixj	O
x	O
rd	O
a	O
sequence	O
of	O
m	O
vectors	O
the	O
jth	O
element	O
of	O
the	O
ith	O
vector	O
in	O
the	O
sequence	O
the	O
values	O
of	O
a	O
vector	O
w	O
during	O
an	O
iterative	O
algorithm	O
the	O
ith	O
element	O
of	O
the	O
vector	O
wt	O
instances	O
domain	B
set	B
labels	O
domain	B
set	B
examples	O
domain	B
set	B
hypothesis	B
class	I
set	B
loss	B
function	B
a	O
distribution	O
over	O
some	O
set	B
over	O
z	O
or	O
over	O
x	O
the	O
probability	O
of	O
a	O
set	B
a	O
z	O
according	O
to	O
d	O
sampling	O
z	O
according	O
to	O
d	O
a	O
sequence	O
of	O
m	O
examples	O
sampling	O
s	O
zm	O
i	O
i	O
d	O
according	O
to	O
d	O
probability	O
and	O
expectation	O
of	O
a	O
random	O
variable	O
dz	O
f	O
true	O
for	O
f	O
z	O
false	O
expectation	O
of	O
the	O
random	O
variable	O
f	O
z	O
r	O
gaussian	O
distribution	O
with	O
expectation	O
and	O
covariance	O
c	O
the	O
derivative	O
of	O
a	O
function	B
f	O
r	O
r	O
at	O
x	O
the	O
second	O
derivative	O
of	O
a	O
function	B
f	O
r	O
r	O
at	O
x	O
the	O
partial	O
derivative	O
of	O
a	O
function	B
f	O
rd	O
r	O
at	O
w	O
w	O
r	O
t	O
wi	O
the	O
gradient	B
of	O
a	O
function	B
f	O
rd	O
r	O
at	O
w	O
the	O
differential	B
set	B
of	O
a	O
function	B
f	O
rd	O
r	O
at	O
w	O
minf	O
x	O
c	O
value	O
of	O
f	O
over	O
c	O
maxf	O
x	O
c	O
value	O
of	O
f	O
over	O
c	O
the	O
set	B
c	O
f	O
minz	O
c	O
f	O
the	O
set	B
c	O
f	O
maxz	O
c	O
f	O
the	O
natural	O
logarithm	O
notation	O
such	O
that	O
for	O
all	O
x	O
we	O
have	O
f	O
gx	O
we	O
write	O
f	O
if	O
there	O
exist	O
r	O
such	O
that	O
for	O
all	O
x	O
we	O
have	O
f	O
gx	O
the	O
notation	O
f	O
is	O
defined	O
analogously	O
the	O
notation	O
f	O
means	O
that	O
f	O
og	O
and	O
g	O
of	O
finally	O
the	O
notation	O
f	O
og	O
means	O
that	O
there	O
exists	O
k	O
n	O
such	O
that	O
f	O
ogx	O
logkgx	O
the	O
inner	O
product	O
between	O
vectors	O
x	O
and	O
w	O
is	O
denoted	O
by	O
whenever	O
we	O
do	O
not	O
specify	O
the	O
vector	O
space	O
we	O
assume	O
that	O
it	O
is	O
the	O
d-dimensional	O
euclidean	O
xiwi	O
the	O
euclidean	O
norm	O
of	O
a	O
vector	O
w	O
is	O
i	O
and	O
in	O
particular	O
space	O
and	O
then	O
we	O
omit	O
the	O
subscript	O
from	O
the	O
norm	O
when	O
it	O
is	O
clear	O
from	O
the	O
context	O
we	O
also	O
use	O
other	O
norms	O
i	O
and	O
maxi	O
we	O
use	O
the	O
notation	O
minx	O
c	O
f	O
to	O
denote	O
the	O
minimum	O
value	O
of	O
the	O
set	B
x	O
c	O
to	O
be	O
mathematically	O
more	O
precise	O
we	O
should	O
use	O
inf	O
x	O
c	O
f	O
whenever	O
the	O
minimum	O
is	O
not	O
achievable	O
however	O
in	O
the	O
context	O
of	O
this	O
book	O
the	O
distinction	O
between	O
infimum	O
and	O
minimum	O
is	O
often	O
of	O
little	O
interest	O
hence	O
to	O
simplify	O
the	O
presentation	O
we	O
sometimes	O
use	O
the	O
min	O
notation	O
even	O
when	O
inf	O
is	O
more	O
adequate	O
an	O
analogous	O
remark	O
applies	O
to	O
max	O
versus	O
sup	O
part	O
i	O
foundations	O
a	O
gentle	O
start	O
let	O
us	O
begin	O
our	O
mathematical	O
analysis	O
by	O
showing	O
how	O
successful	O
learning	O
can	O
be	O
achieved	O
in	O
a	O
relatively	O
simplified	O
setting	O
imagine	O
you	O
have	O
just	O
arrived	O
in	O
some	O
small	O
pacific	O
island	O
you	O
soon	O
find	O
out	O
that	O
papayas	O
are	O
a	O
significant	O
ingredient	O
in	O
the	O
local	O
diet	O
however	O
you	O
have	O
never	O
before	O
tasted	O
papayas	O
you	O
have	O
to	O
learn	O
how	O
to	O
predict	O
whether	O
a	O
papaya	O
you	O
see	O
in	O
the	O
market	O
is	O
tasty	O
or	O
not	O
first	O
you	O
need	O
to	O
decide	O
which	O
features	O
of	O
a	O
papaya	O
your	O
prediction	O
should	O
be	O
based	O
on	O
on	O
the	O
basis	O
of	O
your	O
previous	O
experience	O
with	O
other	O
fruits	O
you	O
decide	O
to	O
use	O
two	O
features	O
the	O
papaya	O
s	O
color	O
ranging	O
from	O
dark	O
green	O
through	O
orange	O
and	O
red	O
to	O
dark	O
brown	O
and	O
the	O
papaya	O
s	O
softness	O
ranging	O
from	O
rock	O
hard	O
to	O
mushy	O
your	O
input	O
for	O
figuring	O
out	O
your	O
prediction	O
rule	O
is	O
a	O
sample	O
of	O
papayas	O
that	O
you	O
have	O
examined	O
for	O
color	O
and	O
softness	O
and	O
then	O
tasted	O
and	O
found	O
out	O
whether	O
they	O
were	O
tasty	O
or	O
not	O
let	O
us	O
analyze	O
this	O
task	O
as	O
a	O
demonstration	O
of	O
the	O
considerations	O
involved	O
in	O
learning	O
problems	O
our	O
first	O
step	O
is	O
to	O
describe	O
a	O
formal	O
model	O
aimed	O
to	O
capture	O
such	O
learning	O
tasks	O
a	O
formal	O
model	O
the	O
statistical	O
learning	O
framework	O
the	O
learner	O
s	O
input	O
in	O
the	O
basic	O
statistical	O
learning	O
setting	O
the	O
learner	O
has	O
access	O
to	O
the	O
following	O
domain	B
set	B
an	O
arbitrary	O
set	B
x	O
this	O
is	O
the	O
set	B
of	O
objects	O
that	O
we	O
may	O
wish	O
to	O
label	B
for	O
example	O
in	O
the	O
papaya	O
learning	O
problem	O
mentioned	O
before	O
the	O
domain	B
set	B
will	O
be	O
the	O
set	B
of	O
all	O
papayas	O
usually	O
these	O
domain	B
points	O
will	O
be	O
represented	O
by	O
a	O
vector	O
of	O
features	O
the	O
papaya	O
s	O
color	O
and	O
softness	O
we	O
also	O
refer	O
to	O
domain	B
points	O
as	O
instances	O
and	O
to	O
x	O
as	O
instance	B
space	I
label	B
set	B
for	O
our	O
current	O
discussion	O
we	O
will	O
restrict	O
the	O
label	B
set	B
to	O
be	O
a	O
two-element	O
set	B
usually	O
or	O
let	O
y	O
denote	O
our	O
set	B
of	O
possible	O
labels	O
for	O
our	O
papayas	O
example	O
let	O
y	O
be	O
where	O
represents	O
being	O
tasty	O
and	O
stands	O
for	O
being	O
not-tasty	O
training	O
data	O
s	O
ym	O
is	O
a	O
finite	O
sequence	O
of	O
pairs	O
in	O
x	O
y	O
that	O
is	O
a	O
sequence	O
of	O
labeled	O
domain	B
points	O
this	O
is	O
the	O
input	O
that	O
the	O
learner	O
has	O
access	O
to	O
a	O
set	B
of	O
papayas	O
that	O
have	O
been	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
a	O
gentle	O
start	O
tasted	O
and	O
their	O
color	O
softness	O
and	O
tastiness	O
such	O
labeled	O
examples	O
are	O
often	O
called	O
training	O
examples	O
we	O
sometimes	O
also	O
refer	O
to	O
s	O
as	O
a	O
training	O
the	O
learner	O
s	O
output	O
the	O
learner	O
is	O
requested	O
to	O
output	O
a	O
prediction	O
rule	O
h	O
x	O
y	O
this	O
function	B
is	O
also	O
called	O
a	O
predictor	B
a	O
hypothesis	B
or	O
a	O
classifier	B
the	O
predictor	B
can	O
be	O
used	O
to	O
predict	O
the	O
label	B
of	O
new	O
domain	B
points	O
in	O
our	O
papayas	O
example	O
it	O
is	O
a	O
rule	O
that	O
our	O
learner	O
will	O
employ	O
to	O
predict	O
whether	O
future	O
papayas	O
he	O
examines	O
in	O
the	O
farmers	O
market	O
are	O
going	O
to	O
be	O
tasty	O
or	O
not	O
we	O
use	O
the	O
notation	O
as	O
to	O
denote	O
the	O
hypothesis	B
that	O
a	O
learning	O
algorithm	O
a	O
returns	O
upon	O
receiving	O
the	O
training	O
sequence	O
s	O
a	O
simple	O
data-generation	O
model	O
we	O
now	O
explain	O
how	O
the	O
training	O
data	O
is	O
generated	O
first	O
we	O
assume	O
that	O
the	O
instances	O
papayas	O
we	O
encounter	O
are	O
generated	O
by	O
some	O
probability	O
distribution	O
this	O
case	O
representing	O
the	O
environment	O
let	O
us	O
denote	O
that	O
probability	O
distribution	O
over	O
x	O
by	O
d	O
it	O
is	O
important	O
to	O
note	O
that	O
we	O
do	O
not	O
assume	O
that	O
the	O
learner	O
knows	O
anything	O
about	O
this	O
distribution	O
for	O
the	O
type	O
of	O
learning	O
tasks	O
we	O
discuss	O
this	O
could	O
be	O
any	O
arbitrary	O
probability	O
distribution	O
as	O
to	O
the	O
labels	O
in	O
the	O
current	O
discussion	O
we	O
assume	O
that	O
there	O
is	O
some	O
correct	O
labeling	O
function	B
f	O
x	O
y	O
and	O
that	O
yi	O
f	O
for	O
all	O
i	O
this	O
assumption	O
will	O
be	O
relaxed	O
in	O
the	O
next	O
chapter	O
the	O
labeling	O
function	B
is	O
unknown	O
to	O
the	O
learner	O
in	O
fact	O
this	O
is	O
just	O
what	O
the	O
learner	O
is	O
trying	O
to	O
figure	O
out	O
in	O
summary	O
each	O
pair	O
in	O
the	O
training	O
data	O
s	O
is	O
generated	O
by	O
first	O
sampling	O
a	O
point	O
xi	O
according	O
to	O
d	O
and	O
then	O
labeling	O
it	O
by	O
f	O
measures	O
of	O
success	O
we	O
define	O
the	O
error	O
of	O
a	O
classifier	B
to	O
be	O
the	O
probability	O
that	O
it	O
does	O
not	O
predict	O
the	O
correct	O
label	B
on	O
a	O
random	O
data	O
point	O
generated	O
by	O
the	O
aforementioned	O
underlying	O
distribution	O
that	O
is	O
the	O
error	O
of	O
h	O
is	O
the	O
probability	O
to	O
draw	O
a	O
random	O
instance	B
x	O
according	O
to	O
the	O
distribution	O
d	O
such	O
that	O
hx	O
does	O
not	O
equal	O
f	O
formally	O
given	O
a	O
domain	B
a	O
x	O
the	O
probability	O
distribution	O
d	O
assigns	O
a	O
number	O
da	O
which	O
determines	O
how	O
likely	O
it	O
is	O
to	O
observe	O
a	O
point	O
x	O
a	O
in	O
many	O
cases	O
we	O
refer	O
to	O
a	O
as	O
an	O
event	O
and	O
express	O
it	O
using	O
a	O
function	B
x	O
namely	O
a	O
x	O
in	O
that	O
case	O
we	O
also	O
use	O
the	O
notation	O
px	O
d	O
to	O
express	O
da	O
we	O
define	O
the	O
error	O
of	O
a	O
prediction	O
rule	O
h	O
x	O
y	O
to	O
be	O
x	O
dhx	O
f	O
def	O
dx	O
hx	O
f	O
ldf	O
def	O
p	O
that	O
is	O
the	O
error	O
of	O
such	O
h	O
is	O
the	O
probability	O
of	O
randomly	O
choosing	O
an	O
example	O
x	O
for	O
which	O
hx	O
f	O
the	O
subscript	O
f	O
indicates	O
that	O
the	O
error	O
is	O
measured	O
with	O
respect	O
to	O
the	O
probability	O
distribution	O
d	O
and	O
the	O
despite	O
the	O
set	B
notation	O
s	O
is	O
a	O
sequence	O
in	O
particular	O
the	O
same	O
example	O
may	O
appear	O
strictly	O
speaking	O
we	O
should	O
be	O
more	O
careful	O
and	O
require	O
that	O
a	O
is	O
a	O
member	O
of	O
some	O
twice	O
in	O
s	O
and	O
some	O
algorithms	O
can	O
take	O
into	O
account	O
the	O
order	O
of	O
examples	O
in	O
s	O
of	O
subsets	O
of	O
x	O
over	O
which	O
d	O
is	O
defined	O
we	O
will	O
formally	O
define	O
our	O
measurability	O
assumptions	O
in	O
the	O
next	O
chapter	O
empirical	B
risk	B
minimization	O
correct	O
labeling	O
function	B
f	O
we	O
omit	O
this	O
subscript	O
when	O
it	O
is	O
clear	O
from	O
the	O
context	O
ldf	O
has	O
several	O
synonymous	O
names	O
such	O
as	O
the	O
generalization	B
error	I
the	O
risk	B
or	O
the	O
true	B
error	I
of	O
h	O
and	O
we	O
will	O
use	O
these	O
names	O
interchangeably	O
throughout	O
the	O
book	O
we	O
use	O
the	O
letter	O
l	O
for	O
the	O
error	O
since	O
we	O
view	O
this	O
error	O
as	O
the	O
loss	B
of	O
the	O
learner	O
we	O
will	O
later	O
also	O
discuss	O
other	O
possible	O
formulations	O
of	O
such	O
loss	B
a	O
note	O
about	O
the	O
information	O
available	O
to	O
the	O
learner	O
the	O
learner	O
is	O
blind	O
to	O
the	O
underlying	O
distribution	O
d	O
over	O
the	O
world	O
and	O
to	O
the	O
labeling	O
function	B
f	O
in	O
our	O
papayas	O
example	O
we	O
have	O
just	O
arrived	O
in	O
a	O
new	O
island	O
and	O
we	O
have	O
no	O
clue	O
as	O
to	O
how	O
papayas	O
are	O
distributed	O
and	O
how	O
to	O
predict	O
their	O
tastiness	O
the	O
only	O
way	O
the	O
learner	O
can	O
interact	O
with	O
the	O
environment	O
is	O
through	O
observing	O
the	O
training	B
set	B
in	O
the	O
next	O
section	O
we	O
describe	O
a	O
simple	O
learning	O
paradigm	O
for	O
the	O
preceding	O
setup	O
and	O
analyze	O
its	O
performance	O
empirical	B
risk	B
minimization	O
as	O
mentioned	O
earlier	O
a	O
learning	O
algorithm	O
receives	O
as	O
input	O
a	O
training	B
set	B
s	O
sampled	O
from	O
an	O
unknown	O
distribution	O
d	O
and	O
labeled	O
by	O
some	O
target	O
function	B
f	O
and	O
should	O
output	O
a	O
predictor	B
hs	O
x	O
y	O
subscript	O
s	O
emphasizes	O
the	O
fact	O
that	O
the	O
output	O
predictor	B
depends	O
on	O
s	O
the	O
goal	O
of	O
the	O
algorithm	O
is	O
to	O
find	O
hs	O
that	O
minimizes	O
the	O
error	O
with	O
respect	O
to	O
the	O
unknown	O
d	O
and	O
f	O
since	O
the	O
learner	O
does	O
not	O
know	O
what	O
d	O
and	O
f	O
are	O
the	O
true	B
error	I
is	O
not	O
directly	O
available	O
to	O
the	O
learner	O
a	O
useful	O
notion	O
of	O
error	O
that	O
can	O
be	O
calculated	O
by	O
the	O
learner	O
is	O
the	O
training	B
error	I
the	O
error	O
the	O
classifier	B
incurs	O
over	O
the	O
training	O
sample	O
lsh	O
def	O
where	O
m	O
hxi	O
yi	O
m	O
the	O
terms	O
empirical	B
error	I
and	O
empirical	B
risk	B
are	O
often	O
used	O
interchangeably	O
for	O
this	O
error	O
since	O
the	O
training	O
sample	O
is	O
the	O
snapshot	O
of	O
the	O
world	O
that	O
is	O
available	O
to	O
the	O
learner	O
it	O
makes	O
sense	O
to	O
search	O
for	O
a	O
solution	O
that	O
works	O
well	O
on	O
that	O
data	O
this	O
learning	O
paradigm	O
coming	O
up	O
with	O
a	O
predictor	B
h	O
that	O
minimizes	O
lsh	O
is	O
called	O
empirical	B
risk	B
minimization	O
or	O
erm	B
for	O
short	O
something	O
may	O
go	O
wrong	O
overfitting	B
although	O
the	O
erm	B
rule	O
seems	O
very	O
natural	O
without	O
being	O
careful	O
this	O
approach	O
may	O
fail	O
miserably	O
to	O
demonstrate	O
such	O
a	O
failure	O
let	O
us	O
go	O
back	O
to	O
the	O
problem	O
of	O
learning	O
to	O
a	O
gentle	O
start	O
predict	O
the	O
taste	O
of	O
a	O
papaya	O
on	O
the	O
basis	O
of	O
its	O
softness	O
and	O
color	O
consider	O
a	O
sample	O
as	O
depicted	O
in	O
the	O
following	O
assume	O
that	O
the	O
probability	O
distribution	O
d	O
is	O
such	O
that	O
instances	O
are	O
distributed	O
uniformly	O
within	O
the	O
gray	O
square	O
and	O
the	O
labeling	O
function	B
f	O
determines	O
the	O
label	B
to	O
be	O
if	O
the	O
instance	B
is	O
within	O
the	O
inner	O
blue	O
square	O
and	O
otherwise	O
the	O
area	O
of	O
the	O
gray	O
square	O
in	O
the	O
picture	O
is	O
and	O
the	O
area	O
of	O
the	O
blue	O
square	O
is	O
consider	O
the	O
following	O
predictor	B
if	O
i	O
s	O
t	O
xi	O
x	O
otherwise	O
yi	O
hsx	O
while	O
this	O
predictor	B
might	O
seem	O
rather	O
artificial	O
in	O
exercise	O
we	O
show	O
a	O
natural	O
representation	O
of	O
it	O
using	O
polynomials	O
clearly	O
no	O
matter	O
what	O
the	O
sample	O
is	O
lshs	O
and	O
therefore	O
this	O
predictor	B
may	O
be	O
chosen	O
by	O
an	O
erm	B
algorithm	O
is	O
one	O
of	O
the	O
empirical-minimum-cost	O
hypotheses	O
no	O
classifier	B
can	O
have	O
smaller	O
error	O
on	O
the	O
other	O
hand	O
the	O
true	B
error	I
of	O
any	O
classifier	B
that	O
predicts	O
the	O
label	B
only	O
on	O
a	O
finite	O
number	O
of	O
instances	O
is	O
in	O
this	O
case	O
thus	O
ldhs	O
we	O
have	O
found	O
a	O
predictor	B
whose	O
performance	O
on	O
the	O
training	B
set	B
is	O
excellent	O
yet	O
its	O
performance	O
on	O
the	O
true	O
world	O
is	O
very	O
poor	O
this	O
phenomenon	O
is	O
called	O
overfitting	B
intuitively	O
overfitting	B
occurs	O
when	O
our	O
hypothesis	B
fits	O
the	O
training	O
data	O
too	O
well	O
like	O
the	O
everyday	O
experience	O
that	O
a	O
person	O
who	O
provides	O
a	O
perfect	O
detailed	O
explanation	O
for	O
each	O
of	O
his	O
single	O
actions	O
may	O
raise	O
suspicion	O
empirical	B
risk	B
minimization	O
with	O
inductive	O
bias	B
we	O
have	O
just	O
demonstrated	O
that	O
the	O
erm	B
rule	O
might	O
lead	O
to	O
overfitting	B
rather	O
than	O
giving	O
up	O
on	O
the	O
erm	B
paradigm	O
we	O
will	O
look	O
for	O
ways	O
to	O
rectify	O
it	O
we	O
will	O
search	O
for	O
conditions	O
under	O
which	O
there	O
is	O
a	O
guarantee	O
that	O
erm	B
does	O
not	O
overfit	O
namely	O
conditions	O
under	O
which	O
when	O
the	O
erm	B
predictor	B
has	O
good	O
performance	O
with	O
respect	O
to	O
the	O
training	O
data	O
it	O
is	O
also	O
highly	O
likely	O
to	O
perform	O
well	O
over	O
the	O
underlying	O
data	O
distribution	O
a	O
common	O
solution	O
is	O
to	O
apply	O
the	O
erm	B
learning	O
rule	O
over	O
a	O
restricted	O
search	O
space	O
formally	O
the	O
learner	O
should	O
choose	O
in	O
advance	O
seeing	O
the	O
data	O
a	O
set	B
of	O
predictors	O
this	O
set	B
is	O
called	O
a	O
hypothesis	B
class	I
and	O
is	O
denoted	O
by	O
h	O
each	O
h	O
h	O
is	O
a	O
function	B
mapping	O
from	O
x	O
to	O
y	O
for	O
a	O
given	O
class	O
h	O
and	O
a	O
training	O
sample	O
s	O
the	O
ermh	O
learner	O
uses	O
the	O
erm	B
rule	O
to	O
choose	O
a	O
predictor	B
h	O
h	O
empirical	B
risk	B
minimization	O
with	O
inductive	O
bias	B
with	O
the	O
lowest	O
possible	O
error	O
over	O
s	O
formally	O
ermhs	O
argmin	O
h	O
h	O
lsh	O
where	O
argmin	O
stands	O
for	O
the	O
set	B
of	O
hypotheses	O
in	O
h	O
that	O
achieve	O
the	O
minimum	O
value	O
of	O
lsh	O
over	O
h	O
by	O
restricting	O
the	O
learner	O
to	O
choosing	O
a	O
predictor	B
from	O
h	O
we	O
bias	B
it	O
toward	O
a	O
particular	O
set	B
of	O
predictors	O
such	O
restrictions	O
are	O
often	O
called	O
an	O
inductive	O
bias	B
since	O
the	O
choice	O
of	O
such	O
a	O
restriction	O
is	O
determined	O
before	O
the	O
learner	O
sees	O
the	O
training	O
data	O
it	O
should	O
ideally	O
be	O
based	O
on	O
some	O
prior	B
knowledge	I
about	O
the	O
problem	O
to	O
be	O
learned	O
for	O
example	O
for	O
the	O
papaya	O
taste	O
prediction	O
problem	O
we	O
may	O
choose	O
the	O
class	O
h	O
to	O
be	O
the	O
set	B
of	O
predictors	O
that	O
are	O
determined	O
by	O
axis	O
aligned	O
rectangles	O
the	O
space	O
determined	O
by	O
the	O
color	O
and	O
softness	O
coordinates	O
we	O
will	O
later	O
show	O
that	O
ermh	O
over	O
this	O
class	O
is	O
guaranteed	O
not	O
to	O
overfit	O
on	O
the	O
other	O
hand	O
the	O
example	O
of	O
overfitting	B
that	O
we	O
have	O
seen	O
previously	O
demonstrates	O
that	O
choosing	O
h	O
to	O
be	O
a	O
class	O
of	O
predictors	O
that	O
includes	O
all	O
functions	O
that	O
assign	O
the	O
value	O
to	O
a	O
finite	O
set	B
of	O
domain	B
points	O
does	O
not	O
suffice	O
to	O
guarantee	O
that	O
ermh	O
will	O
not	O
overfit	O
a	O
fundamental	O
question	O
in	O
learning	O
theory	O
is	O
over	O
which	O
hypothesis	B
classes	O
ermh	O
learning	O
will	O
not	O
result	O
in	O
overfitting	B
we	O
will	O
study	O
this	O
question	O
later	O
in	O
the	O
book	O
intuitively	O
choosing	O
a	O
more	O
restricted	O
hypothesis	B
class	I
better	O
protects	O
us	O
against	O
overfitting	B
but	O
at	O
the	O
same	O
time	O
might	O
cause	O
us	O
a	O
stronger	O
inductive	O
bias	B
we	O
will	O
get	O
back	O
to	O
this	O
fundamental	O
tradeoff	O
later	O
finite	O
hypothesis	B
classes	O
the	O
simplest	O
type	O
of	O
restriction	O
on	O
a	O
class	O
is	O
imposing	O
an	O
upper	O
bound	O
on	O
its	O
size	O
is	O
the	O
number	O
of	O
predictors	O
h	O
in	O
h	O
in	O
this	O
section	O
we	O
show	O
that	O
if	O
h	O
is	O
a	O
finite	O
class	O
then	O
ermh	O
will	O
not	O
overfit	O
provided	O
it	O
is	O
based	O
on	O
a	O
sufficiently	O
large	O
training	O
sample	O
size	O
requirement	O
will	O
depend	O
on	O
the	O
size	O
of	O
h	O
limiting	O
the	O
learner	O
to	O
prediction	O
rules	O
within	O
some	O
finite	O
hypothesis	B
class	I
may	O
be	O
considered	O
as	O
a	O
reasonably	O
mild	O
restriction	O
for	O
example	O
h	O
can	O
be	O
the	O
set	B
of	O
all	O
predictors	O
that	O
can	O
be	O
implemented	O
by	O
a	O
c	O
program	O
written	O
in	O
at	O
most	O
bits	O
of	O
code	O
in	O
our	O
papayas	O
example	O
we	O
mentioned	O
previously	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
while	O
this	O
is	O
an	O
infinite	O
class	O
if	O
we	O
discretize	O
the	O
representation	O
of	O
real	O
numbers	O
say	O
by	O
using	O
a	O
bits	O
floating-point	O
representation	O
the	O
hypothesis	B
class	I
becomes	O
a	O
finite	O
class	O
let	O
us	O
now	O
analyze	O
the	O
performance	O
of	O
the	O
ermh	O
learning	O
rule	O
assuming	O
that	O
h	O
is	O
a	O
finite	O
class	O
for	O
a	O
training	O
sample	O
s	O
labeled	O
according	O
to	O
some	O
f	O
x	O
y	O
let	O
hs	O
denote	O
a	O
result	O
of	O
applying	O
ermh	O
to	O
s	O
namely	O
hs	O
argmin	O
h	O
h	O
lsh	O
in	O
this	O
chapter	O
we	O
make	O
the	O
following	O
simplifying	O
assumption	O
will	O
be	O
relaxed	O
in	O
the	O
next	O
chapter	O
a	O
gentle	O
start	O
definition	O
realizability	B
assumption	O
there	O
exists	O
h	O
s	O
t	O
ldf	O
note	O
that	O
this	O
assumption	O
implies	O
that	O
with	O
probability	O
over	O
random	O
samples	O
s	O
where	O
the	O
instances	O
of	O
s	O
are	O
sampled	O
according	O
to	O
d	O
and	O
are	O
labeled	O
by	O
f	O
we	O
have	O
the	O
realizability	B
assumption	O
implies	O
that	O
for	O
every	O
erm	B
hypothesis	B
we	O
have	O
lshs	O
however	O
we	O
are	O
interested	O
in	O
the	O
true	O
risk	B
of	O
hs	O
ldf	O
rather	O
than	O
its	O
empirical	B
risk	B
clearly	O
any	O
guarantee	O
on	O
the	O
error	O
with	O
respect	O
to	O
the	O
underlying	O
distribution	O
d	O
for	O
an	O
algorithm	O
that	O
has	O
access	O
only	O
to	O
a	O
sample	O
s	O
should	O
depend	O
on	O
the	O
relationship	O
between	O
d	O
and	O
s	O
the	O
common	O
assumption	O
in	O
statistical	O
machine	O
learning	O
is	O
that	O
the	O
training	O
sample	O
s	O
is	O
generated	O
by	O
sampling	O
points	O
from	O
the	O
distribution	O
d	O
independently	O
of	O
each	O
other	O
formally	O
the	O
i	O
i	O
d	O
assumption	O
the	O
examples	O
in	O
the	O
training	B
set	B
are	O
independently	O
and	O
identically	O
distributed	O
according	O
to	O
the	O
distribution	O
d	O
that	O
is	O
every	O
xi	O
in	O
s	O
is	O
freshly	O
sampled	O
according	O
to	O
d	O
and	O
then	O
labeled	O
according	O
to	O
the	O
labeling	O
function	B
f	O
we	O
denote	O
this	O
assumption	O
by	O
s	O
dm	O
where	O
m	O
is	O
the	O
size	O
of	O
s	O
and	O
dm	O
denotes	O
the	O
probability	O
over	O
m-tuples	O
induced	O
by	O
applying	O
d	O
to	O
pick	O
each	O
element	O
of	O
the	O
tuple	O
independently	O
of	O
the	O
other	O
members	O
of	O
the	O
tuple	O
intuitively	O
the	O
training	B
set	B
s	O
is	O
a	O
window	O
through	O
which	O
the	O
learner	O
gets	O
partial	O
information	O
about	O
the	O
distribution	O
d	O
over	O
the	O
world	O
and	O
the	O
labeling	O
function	B
f	O
the	O
larger	O
the	O
sample	O
gets	O
the	O
more	O
likely	O
it	O
is	O
to	O
reflect	O
more	O
accurately	O
the	O
distribution	O
and	O
labeling	O
used	O
to	O
generate	O
it	O
since	O
ldf	O
depends	O
on	O
the	O
training	B
set	B
s	O
and	O
that	O
training	B
set	B
is	O
picked	O
by	O
a	O
random	O
process	O
there	O
is	O
randomness	O
in	O
the	O
choice	O
of	O
the	O
predictor	B
hs	O
and	O
consequently	O
in	O
the	O
risk	B
ldf	O
formally	O
we	O
say	O
that	O
it	O
is	O
a	O
random	O
variable	O
it	O
is	O
not	O
realistic	O
to	O
expect	O
that	O
with	O
full	O
certainty	O
s	O
will	O
suffice	O
to	O
direct	O
the	O
learner	O
toward	O
a	O
good	O
classifier	B
the	O
point	O
of	O
view	O
of	O
d	O
as	O
there	O
is	O
always	O
some	O
probability	O
that	O
the	O
sampled	O
training	O
data	O
happens	O
to	O
be	O
very	O
nonrepresentative	O
of	O
the	O
underlying	O
d	O
if	O
we	O
go	O
back	O
to	O
the	O
papaya	O
tasting	O
example	O
there	O
is	O
always	O
some	O
chance	O
that	O
all	O
the	O
papayas	O
we	O
have	O
happened	O
to	O
taste	O
were	O
not	O
tasty	O
in	O
spite	O
of	O
the	O
fact	O
that	O
say	O
of	O
the	O
papayas	O
in	O
our	O
island	O
are	O
tasty	O
in	O
such	O
a	O
case	O
ermhs	O
may	O
be	O
the	O
constant	O
function	B
that	O
labels	O
every	O
papaya	O
as	O
not	O
tasty	O
has	O
error	O
on	O
the	O
true	O
distribution	O
of	O
papapyas	O
in	O
the	O
island	O
we	O
will	O
therefore	O
address	O
the	O
probability	O
to	O
sample	O
a	O
training	B
set	B
for	O
which	O
ldf	O
is	O
not	O
too	O
large	O
usually	O
we	O
denote	O
the	O
probability	O
of	O
getting	O
a	O
nonrepresentative	O
sample	O
by	O
and	O
call	O
the	O
confidence	B
parameter	O
of	O
our	O
prediction	O
on	O
top	O
of	O
that	O
since	O
we	O
cannot	O
guarantee	O
perfect	O
label	B
prediction	O
we	O
introduce	O
another	O
parameter	O
for	O
the	O
quality	O
of	O
prediction	O
the	O
accuracy	B
parameter	O
mathematically	O
speaking	O
this	O
holds	O
with	O
probability	O
to	O
simplify	O
the	O
presentation	O
we	O
sometimes	O
omit	O
the	O
with	O
probability	O
specifier	O
empirical	B
risk	B
minimization	O
with	O
inductive	O
bias	B
commonly	O
denoted	O
by	O
we	O
interpret	O
the	O
event	O
ldf	O
as	O
a	O
failure	O
of	O
the	O
learner	O
while	O
if	O
ldf	O
we	O
view	O
the	O
output	O
of	O
the	O
algorithm	O
as	O
an	O
approximately	O
correct	O
predictor	B
therefore	O
some	O
labeling	O
function	B
f	O
x	O
y	O
we	O
are	O
interested	O
in	O
upper	O
bounding	O
the	O
probability	O
to	O
sample	O
m-tuple	O
of	O
instances	O
that	O
will	O
lead	O
to	O
failure	O
of	O
the	O
learner	O
formally	O
let	O
sx	O
xm	O
be	O
the	O
instances	O
of	O
the	O
training	B
set	B
we	O
would	O
like	O
to	O
upper	O
bound	O
dmsx	O
ldf	O
let	O
hb	O
be	O
the	O
set	B
of	O
bad	O
hypotheses	O
that	O
is	O
hb	O
h	O
ldf	O
in	O
addition	O
let	O
m	O
h	O
hb	O
lsh	O
be	O
the	O
set	B
of	O
misleading	O
samples	O
namely	O
for	O
every	O
sx	O
m	O
there	O
is	O
a	O
bad	O
hypothesis	B
h	O
hb	O
that	O
looks	O
like	O
a	O
good	O
hypothesis	B
on	O
sx	O
now	O
recall	B
that	O
we	O
would	O
like	O
to	O
bound	O
the	O
probability	O
of	O
the	O
event	O
ldf	O
but	O
since	O
the	O
realizability	B
assumption	O
implies	O
that	O
lshs	O
it	O
follows	O
that	O
the	O
event	O
ldf	O
can	O
only	O
happen	O
if	O
for	O
some	O
h	O
hb	O
we	O
have	O
lsh	O
in	O
other	O
words	O
this	O
event	O
will	O
only	O
happen	O
if	O
our	O
sample	O
is	O
in	O
the	O
set	B
of	O
misleading	O
samples	O
m	O
formally	O
we	O
have	O
shown	O
that	O
ldf	O
m	O
note	O
that	O
we	O
can	O
rewrite	O
m	O
as	O
h	O
hb	O
m	O
lsh	O
hence	O
dmsx	O
ldf	O
dmm	O
dm	O
h	O
hbsx	O
lsh	O
next	O
we	O
upper	O
bound	O
the	O
right-hand	O
side	O
of	O
the	O
preceding	O
equation	O
using	O
the	O
union	B
bound	I
a	O
basic	O
property	O
of	O
probabilities	O
lemma	O
bound	O
for	O
any	O
two	O
sets	O
a	O
b	O
and	O
a	O
distribution	O
d	O
we	O
have	O
da	O
b	O
da	O
db	O
applying	O
the	O
union	B
bound	I
to	O
the	O
right-hand	O
side	O
of	O
equation	O
yields	O
dmsx	O
ldf	O
h	O
hb	O
dmsx	O
lsh	O
next	O
let	O
us	O
bound	O
each	O
summand	O
of	O
the	O
right-hand	O
side	O
of	O
the	O
preceding	O
inequality	O
fix	O
some	O
bad	O
hypothesis	B
h	O
hb	O
the	O
event	O
lsh	O
is	O
equivalent	O
a	O
gentle	O
start	O
to	O
the	O
event	O
i	O
hxi	O
f	O
since	O
the	O
examples	O
in	O
the	O
training	B
set	B
are	O
sampled	O
i	O
i	O
d	O
we	O
get	O
that	O
dmsx	O
lsh	O
dmsx	O
i	O
hxi	O
f	O
dxi	O
hxi	O
f	O
for	O
each	O
individual	O
sampling	O
of	O
an	O
element	O
of	O
the	O
training	B
set	B
we	O
have	O
dxi	O
hxi	O
yi	O
ldf	O
where	O
the	O
last	O
inequality	O
follows	O
from	O
the	O
fact	O
that	O
h	O
hb	O
combining	O
the	O
previous	O
equation	O
with	O
equation	O
and	O
using	O
the	O
inequality	O
e	O
we	O
obtain	O
that	O
for	O
every	O
h	O
hb	O
dmsx	O
lsh	O
e	O
combining	O
this	O
equation	O
with	O
equation	O
we	O
conclude	O
that	O
dmsx	O
ldf	O
e	O
e	O
m	O
a	O
graphical	O
illustration	O
which	O
explains	O
how	O
we	O
used	O
the	O
union	B
bound	I
is	O
given	O
in	O
figure	O
figure	O
each	O
point	O
in	O
the	O
large	O
circle	O
represents	O
a	O
possible	O
m-tuple	O
of	O
instances	O
each	O
colored	O
oval	O
represents	O
the	O
set	B
of	O
misleading	O
m-tuple	O
of	O
instances	O
for	O
some	O
bad	O
predictor	B
h	O
hb	O
the	O
erm	B
can	O
potentially	O
overfit	O
whenever	O
it	O
gets	O
a	O
misleading	O
training	B
set	B
s	O
that	O
is	O
for	O
some	O
h	O
hb	O
we	O
have	O
lsh	O
equation	O
guarantees	O
that	O
for	O
each	O
individual	O
bad	O
hypothesis	B
h	O
hb	O
at	O
most	O
of	O
the	O
training	O
sets	O
would	O
be	O
misleading	O
in	O
particular	O
the	O
larger	O
m	O
is	O
the	O
smaller	O
each	O
of	O
these	O
colored	O
ovals	O
becomes	O
the	O
union	B
bound	I
formalizes	O
the	O
fact	O
that	O
the	O
area	O
representing	O
the	O
training	O
sets	O
that	O
are	O
misleading	O
with	O
respect	O
to	O
some	O
h	O
hb	O
is	O
the	O
training	O
sets	O
in	O
m	O
is	O
at	O
most	O
the	O
sum	O
of	O
the	O
areas	O
of	O
the	O
colored	O
ovals	O
therefore	O
it	O
is	O
bounded	O
by	O
times	O
the	O
maximum	O
size	O
of	O
a	O
colored	O
oval	O
any	O
sample	O
s	O
outside	O
the	O
colored	O
ovals	O
cannot	O
cause	O
the	O
erm	B
rule	O
to	O
overfit	O
corollary	O
let	O
h	O
be	O
a	O
finite	O
hypothesis	B
class	I
let	O
and	O
and	O
let	O
m	O
be	O
an	O
integer	O
that	O
satisfies	O
m	O
logh	O
exercises	O
then	O
for	O
any	O
labeling	O
function	B
f	O
and	O
for	O
any	O
distribution	O
d	O
for	O
which	O
the	O
realizability	B
assumption	O
holds	O
is	O
for	O
some	O
h	O
h	O
ldf	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
an	O
i	O
i	O
d	O
sample	O
s	O
of	O
size	O
m	O
we	O
have	O
that	O
for	O
every	O
erm	B
hypothesis	B
hs	O
it	O
holds	O
that	O
ldf	O
the	O
preceeding	O
corollary	O
tells	O
us	O
that	O
for	O
a	O
sufficiently	O
large	O
m	O
the	O
ermh	O
rule	O
over	O
a	O
finite	O
hypothesis	B
class	I
will	O
be	O
probably	O
confidence	B
approximately	O
to	O
an	O
error	O
of	O
correct	O
in	O
the	O
next	O
chapter	O
we	O
formally	O
define	O
the	O
model	O
of	O
probably	O
approximately	O
correct	O
learning	O
exercises	O
overfitting	B
of	O
polynomial	O
matching	O
we	O
have	O
shown	O
that	O
the	O
predictor	B
defined	O
in	O
equation	O
leads	O
to	O
overfitting	B
while	O
this	O
predictor	B
seems	O
to	O
be	O
very	O
unnatural	O
the	O
goal	O
of	O
this	O
exercise	O
is	O
to	O
show	O
that	O
it	O
can	O
be	O
described	O
as	O
a	O
thresholded	O
polynomial	O
that	O
is	O
show	O
that	O
given	O
a	O
training	B
set	B
s	O
f	O
there	O
exists	O
a	O
polynomial	O
ps	O
such	O
that	O
hsx	O
if	O
and	O
only	O
if	O
psx	O
where	O
hs	O
is	O
as	O
defined	O
in	O
equation	O
it	O
follows	O
that	O
learning	O
the	O
class	O
of	O
all	O
thresholded	O
polynomials	O
using	O
the	O
erm	B
rule	O
may	O
lead	O
to	O
overfitting	B
let	O
h	O
be	O
a	O
class	O
of	O
binary	O
classifiers	O
over	O
a	O
domain	B
x	O
let	O
d	O
be	O
an	O
unknown	O
distribution	O
over	O
x	O
and	O
let	O
f	O
be	O
the	O
target	O
hypothesis	B
in	O
h	O
fix	O
some	O
h	O
h	O
show	O
that	O
the	O
expected	O
value	O
of	O
lsh	O
over	O
the	O
choice	O
of	O
sx	O
equals	O
ldf	O
namely	O
e	O
sx	O
dm	O
ldf	O
axis	O
aligned	O
rectangles	O
an	O
axis	O
aligned	O
rectangle	O
classifier	B
in	O
the	O
plane	O
is	O
a	O
classifier	B
that	O
assigns	O
the	O
value	O
to	O
a	O
point	O
if	O
and	O
only	O
if	O
it	O
is	O
inside	O
a	O
certain	O
rectangle	O
formally	O
given	O
real	O
numbers	O
define	O
the	O
classifier	B
by	O
if	O
and	O
otherwise	O
the	O
class	O
of	O
all	O
axis	O
aligned	O
rectangles	O
in	O
the	O
plane	O
is	O
defined	O
as	O
rec	O
and	O
note	O
that	O
this	O
is	O
an	O
infinite	O
size	O
hypothesis	B
class	I
throughout	O
this	O
exercise	O
we	O
rely	O
on	O
the	O
realizability	B
assumption	O
a	O
gentle	O
start	O
then	O
with	O
proba	O
let	O
a	O
be	O
the	O
algorithm	O
that	O
returns	O
the	O
smallest	O
rectangle	O
enclosing	O
all	O
show	O
that	O
if	O
a	O
receives	O
a	O
training	B
set	B
of	O
size	O
positive	O
examples	O
in	O
the	O
training	B
set	B
show	O
that	O
a	O
is	O
an	O
erm	B
bility	O
of	O
at	O
least	O
it	O
returns	O
a	O
hypothesis	B
with	O
error	O
of	O
at	O
most	O
hint	O
fix	O
some	O
distribution	O
d	O
over	O
x	O
let	O
r	O
ra	O
be	O
the	O
rectangle	O
that	O
generates	O
the	O
labels	O
and	O
let	O
f	O
be	O
the	O
corresponding	O
hypothesis	B
let	O
a	O
be	O
a	O
number	O
such	O
that	O
the	O
probability	O
mass	O
respect	O
to	O
d	O
of	O
the	O
rectangle	O
ra	O
is	O
exactly	O
similarly	O
let	O
be	O
numbers	O
such	O
that	O
the	O
probability	O
masses	O
of	O
the	O
rectangles	O
b	O
ra	O
are	O
all	O
exactly	O
let	O
rs	O
be	O
the	O
rectangle	O
returned	O
by	O
a	O
see	O
illustration	O
in	O
figure	O
b	O
ra	O
a	O
b	O
a	O
b	O
a	O
b	O
a	O
b	O
b	O
b	O
r	O
rs	O
figure	O
axis	O
aligned	O
rectangles	O
show	O
that	O
rs	O
r	O
show	O
that	O
if	O
s	O
contains	O
examples	O
in	O
all	O
of	O
the	O
rectangles	O
then	O
the	O
hypothesis	B
returned	O
by	O
a	O
has	O
error	O
of	O
at	O
most	O
for	O
each	O
i	O
upper	O
bound	O
the	O
probability	O
that	O
s	O
does	O
not	O
use	O
the	O
union	B
bound	I
to	O
conclude	O
the	O
argument	O
contain	O
an	O
example	O
from	O
ri	O
repeat	O
the	O
previous	O
question	O
for	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rd	O
show	O
that	O
the	O
runtime	O
of	O
applying	O
the	O
algorithm	O
a	O
mentioned	O
earlier	O
is	O
polynomial	O
in	O
d	O
and	O
in	O
a	O
formal	O
learning	O
model	O
in	O
this	O
chapter	O
we	O
define	O
our	O
main	O
formal	O
learning	O
model	O
the	O
pac	B
learning	O
model	O
and	O
its	O
extensions	O
we	O
will	O
consider	O
other	O
notions	O
of	O
learnability	O
in	O
chapter	O
pac	B
learning	O
in	O
the	O
previous	O
chapter	O
we	O
have	O
shown	O
that	O
for	O
a	O
finite	O
hypothesis	B
class	I
if	O
the	O
erm	B
rule	O
with	O
respect	O
to	O
that	O
class	O
is	O
applied	O
on	O
a	O
sufficiently	O
large	O
training	O
sample	O
size	O
is	O
independent	O
of	O
the	O
underlying	O
distribution	O
or	O
labeling	O
function	B
then	O
the	O
output	O
hypothesis	B
will	O
be	O
probably	O
approximately	O
correct	O
more	O
generally	O
we	O
now	O
define	O
probably	O
approximately	O
correct	O
learning	O
definition	O
learnability	O
a	O
hypothesis	B
class	I
h	O
is	O
pac	B
learnable	O
if	O
there	O
exist	O
a	O
function	B
mh	O
n	O
and	O
a	O
learning	O
algorithm	O
with	O
the	O
following	O
property	O
for	O
every	O
for	O
every	O
distribution	O
d	O
over	O
x	O
and	O
for	O
every	O
labeling	O
function	B
f	O
x	O
if	O
the	O
realizable	O
assumption	O
holds	O
with	O
respect	O
to	O
hd	O
f	O
then	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
mh	O
i	O
i	O
d	O
examples	O
generated	O
by	O
d	O
and	O
labeled	O
by	O
f	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
h	O
such	O
that	O
with	O
probability	O
of	O
at	O
least	O
the	O
choice	O
of	O
the	O
examples	O
ldf	O
the	O
definition	O
of	O
probably	O
approximately	O
correct	O
learnability	O
contains	O
two	O
approximation	O
parameters	O
the	O
accuracy	B
parameter	O
determines	O
how	O
far	O
the	O
output	O
classifier	B
can	O
be	O
from	O
the	O
optimal	O
one	O
corresponds	O
to	O
the	O
approximately	O
correct	O
and	O
a	O
confidence	B
parameter	O
indicating	O
how	O
likely	O
the	O
classifier	B
is	O
to	O
meet	O
that	O
accuracy	B
requirement	O
to	O
the	O
probably	O
part	O
of	O
pac	B
under	O
the	O
data	O
access	O
model	O
that	O
we	O
are	O
investigating	O
these	O
approximations	O
are	O
inevitable	O
since	O
the	O
training	B
set	B
is	O
randomly	O
generated	O
there	O
may	O
always	O
be	O
a	O
small	O
chance	O
that	O
it	O
will	O
happen	O
to	O
be	O
noninformative	O
example	O
there	O
is	O
always	O
some	O
chance	O
that	O
the	O
training	B
set	B
will	O
contain	O
only	O
one	O
domain	B
point	O
sampled	O
over	O
and	O
over	O
again	O
furthermore	O
even	O
when	O
we	O
are	O
lucky	O
enough	O
to	O
get	O
a	O
training	O
sample	O
that	O
does	O
faithfully	O
represent	O
d	O
because	O
it	O
is	O
just	O
a	O
finite	O
sample	O
there	O
may	O
always	O
be	O
some	O
fine	O
details	O
of	O
d	O
that	O
it	O
fails	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
a	O
formal	O
learning	O
model	O
to	O
reflect	O
our	O
accuracy	B
parameter	O
allows	O
forgiving	O
the	O
learner	O
s	O
classifier	B
for	O
making	O
minor	O
errors	O
sample	B
complexity	I
the	O
function	B
mh	O
n	O
determines	O
the	O
sample	B
complexity	I
of	O
learning	O
h	O
that	O
is	O
how	O
many	O
examples	O
are	O
required	O
to	O
guarantee	O
a	O
probably	O
approximately	O
correct	O
solution	O
the	O
sample	B
complexity	I
is	O
a	O
function	B
of	O
the	O
accuracy	B
and	O
confidence	B
parameters	O
it	O
also	O
depends	O
on	O
properties	O
of	O
the	O
hypothesis	B
class	I
h	O
for	O
example	O
for	O
a	O
finite	O
class	O
we	O
showed	O
that	O
the	O
sample	B
complexity	I
depends	O
on	O
log	O
the	O
size	O
of	O
h	O
note	O
that	O
if	O
h	O
is	O
pac	B
learnable	O
there	O
are	O
many	O
functions	O
mh	O
that	O
satisfy	O
the	O
requirements	O
given	O
in	O
the	O
definition	O
of	O
pac	B
learnability	O
therefore	O
to	O
be	O
precise	O
we	O
will	O
define	O
the	O
sample	B
complexity	I
of	O
learning	O
h	O
to	O
be	O
the	O
minimal	O
function	B
in	O
the	O
sense	O
that	O
for	O
any	O
mh	O
is	O
the	O
minimal	O
integer	O
that	O
satisfies	O
the	O
requirements	O
of	O
pac	B
learning	O
with	O
accuracy	B
and	O
confidence	B
let	O
us	O
now	O
recall	B
the	O
conclusion	O
of	O
the	O
analysis	O
of	O
finite	O
hypothesis	B
classes	O
from	O
the	O
previous	O
chapter	O
it	O
can	O
be	O
rephrased	O
as	O
stating	O
corollary	O
every	O
finite	O
hypothesis	B
class	I
is	O
pac	B
learnable	O
with	O
sample	B
complexity	I
logh	O
mh	O
there	O
are	O
infinite	O
classes	O
that	O
are	O
learnable	O
as	O
well	O
for	O
example	O
exercise	O
later	O
on	O
we	O
will	O
show	O
that	O
what	O
determines	O
the	O
pac	B
learnability	O
of	O
a	O
class	O
is	O
not	O
its	O
finiteness	O
but	O
rather	O
a	O
combinatorial	O
measure	O
called	O
the	O
vc	B
dimension	I
a	O
more	O
general	O
learning	O
model	O
the	O
model	O
we	O
have	O
just	O
described	O
can	O
be	O
readily	O
generalized	O
so	O
that	O
it	O
can	O
be	O
made	O
relevant	O
to	O
a	O
wider	O
scope	O
of	O
learning	O
tasks	O
we	O
consider	O
generalizations	O
in	O
two	O
aspects	O
removing	O
the	O
realizability	B
assumption	O
we	O
have	O
required	O
that	O
the	O
learning	O
algorithm	O
succeeds	O
on	O
a	O
pair	O
of	O
data	O
distribution	O
d	O
and	O
labeling	O
function	B
f	O
provided	O
that	O
the	O
realizability	B
assumption	O
is	O
met	O
for	O
practical	O
learning	O
tasks	O
this	O
assumption	O
may	O
be	O
too	O
strong	O
we	O
really	O
guarantee	O
that	O
there	O
is	O
a	O
rectangle	O
in	O
the	O
color-hardness	O
space	O
that	O
fully	O
determines	O
which	O
papayas	O
are	O
tasty	O
in	O
the	O
next	O
subsection	O
we	O
will	O
describe	O
the	O
agnostic	B
pac	B
model	O
in	O
which	O
this	O
realizability	B
assumption	O
is	O
waived	O
a	O
more	O
general	O
learning	O
model	O
learning	O
problems	O
beyond	O
binary	O
classification	O
the	O
learning	O
task	O
that	O
we	O
have	O
been	O
discussing	O
so	O
far	O
has	O
to	O
do	O
with	O
predicting	O
a	O
binary	O
label	B
to	O
a	O
given	O
example	O
being	O
tasty	O
or	O
not	O
however	O
many	O
learning	O
tasks	O
take	O
a	O
different	O
form	O
for	O
example	O
one	O
may	O
wish	O
to	O
predict	O
a	O
real	O
valued	O
number	O
the	O
temperature	O
at	O
p	O
m	O
tomorrow	O
or	O
a	O
label	B
picked	O
from	O
a	O
finite	O
set	B
of	O
labels	O
the	O
topic	O
of	O
the	O
main	O
story	O
in	O
tomorrow	O
s	O
paper	O
it	O
turns	O
out	O
that	O
our	O
analysis	O
of	O
learning	O
can	O
be	O
readily	O
extended	O
to	O
such	O
and	O
many	O
other	O
scenarios	O
by	O
allowing	O
a	O
variety	O
of	O
loss	B
functions	O
we	O
shall	O
discuss	O
that	O
in	O
section	O
later	O
releasing	O
the	O
realizability	B
assumption	O
agnostic	B
pac	B
learning	O
a	O
more	O
realistic	O
model	O
for	O
the	O
data-generating	O
distribution	O
recall	B
that	O
the	O
realizability	B
assumption	O
requires	O
that	O
there	O
exists	O
h	O
such	O
that	O
px	O
f	O
in	O
many	O
practical	O
problems	O
this	O
assumption	O
does	O
not	O
hold	O
furthermore	O
it	O
is	O
maybe	O
more	O
realistic	O
not	O
to	O
assume	O
that	O
the	O
labels	O
are	O
fully	O
determined	O
by	O
the	O
features	O
we	O
measure	O
on	O
input	O
elements	O
the	O
case	O
of	O
the	O
papayas	O
it	O
is	O
plausible	O
that	O
two	O
papayas	O
of	O
the	O
same	O
color	O
and	O
softness	O
will	O
have	O
different	O
taste	O
in	O
the	O
following	O
we	O
relax	O
the	O
realizability	B
assumption	O
by	O
replacing	O
the	O
target	O
labeling	O
function	B
with	O
a	O
more	O
flexible	O
notion	O
a	O
data-labels	O
generating	O
distribution	O
formally	O
from	O
now	O
on	O
let	O
d	O
be	O
a	O
probability	O
distribution	O
over	O
x	O
y	O
where	O
as	O
before	O
x	O
is	O
our	O
domain	B
set	B
and	O
y	O
is	O
a	O
set	B
of	O
labels	O
we	O
will	O
consider	O
y	O
that	O
is	O
d	O
is	O
a	O
joint	O
distribution	O
over	O
domain	B
points	O
and	O
labels	O
one	O
can	O
view	O
such	O
a	O
distribution	O
as	O
being	O
composed	O
of	O
two	O
parts	O
a	O
distribution	O
dx	O
over	O
unlabeled	O
domain	B
points	O
called	O
the	O
marginal	O
distribution	O
and	O
a	O
conditional	O
probability	O
over	O
labels	O
for	O
each	O
domain	B
point	O
dx	O
yx	O
in	O
the	O
papaya	O
example	O
dx	O
determines	O
the	O
probability	O
of	O
encountering	O
a	O
papaya	O
whose	O
color	O
and	O
hardness	O
fall	O
in	O
some	O
color-hardness	O
values	O
domain	B
and	O
the	O
conditional	O
probability	O
is	O
the	O
probability	O
that	O
a	O
papaya	O
with	O
color	O
and	O
hardness	O
represented	O
by	O
x	O
is	O
tasty	O
indeed	O
such	O
modeling	O
allows	O
for	O
two	O
papayas	O
that	O
share	O
the	O
same	O
color	O
and	O
hardness	O
to	O
belong	O
to	O
different	O
taste	O
categories	O
the	O
empirical	O
and	O
the	O
true	B
error	I
revised	O
for	O
a	O
probability	O
distribution	O
d	O
over	O
x	O
y	O
one	O
can	O
measure	O
how	O
likely	O
h	O
is	O
to	O
make	O
an	O
error	O
when	O
labeled	O
points	O
are	O
randomly	O
drawn	O
according	O
to	O
d	O
we	O
redefine	O
the	O
true	B
error	I
risk	B
of	O
a	O
prediction	O
rule	O
h	O
to	O
be	O
ldh	O
def	O
p	O
dhx	O
y	O
def	O
dx	O
y	O
hx	O
y	O
we	O
would	O
like	O
to	O
find	O
a	O
predictor	B
h	O
for	O
which	O
that	O
error	O
will	O
be	O
minimized	O
however	O
the	O
learner	O
does	O
not	O
know	O
the	O
data	O
generating	O
d	O
what	O
the	O
learner	O
does	O
have	O
access	O
to	O
is	O
the	O
training	O
data	O
s	O
the	O
definition	O
of	O
the	O
empirical	B
risk	B
a	O
formal	O
learning	O
model	O
remains	O
the	O
same	O
as	O
before	O
namely	O
lsh	O
def	O
hxi	O
yi	O
m	O
given	O
s	O
a	O
learner	O
can	O
compute	O
lsh	O
for	O
any	O
function	B
h	O
x	O
note	O
that	O
lsh	O
lduniform	O
over	O
sh	O
the	O
goal	O
we	O
wish	O
to	O
find	O
some	O
hypothesis	B
h	O
x	O
y	O
that	O
approximately	O
minimizes	O
the	O
true	O
risk	B
ldh	O
the	O
bayes	B
optimal	I
predictor	B
given	O
any	O
probability	O
distribution	O
d	O
over	O
x	O
the	O
best	O
label	B
predicting	O
function	B
from	O
x	O
to	O
will	O
be	O
fdx	O
if	O
py	O
otherwise	O
it	O
is	O
easy	O
to	O
verify	O
exercise	O
that	O
for	O
every	O
probability	O
distribution	O
d	O
the	O
bayes	B
optimal	I
predictor	B
fd	O
is	O
optimal	O
in	O
the	O
sense	O
that	O
no	O
other	O
classifier	B
g	O
x	O
has	O
a	O
lower	O
error	O
that	O
is	O
for	O
every	O
classifier	B
g	O
ldfd	O
ldg	O
unfortunately	O
since	O
we	O
do	O
not	O
know	O
d	O
we	O
cannot	O
utilize	O
this	O
optimal	O
predictor	B
fd	O
what	O
the	O
learner	O
does	O
have	O
access	O
to	O
is	O
the	O
training	O
sample	O
we	O
can	O
now	O
present	O
the	O
formal	O
definition	O
of	O
agnostic	B
pac	B
learnability	O
which	O
is	O
a	O
natural	O
extension	O
of	O
the	O
definition	O
of	O
pac	B
learnability	O
to	O
the	O
more	O
realistic	O
nonrealizable	O
learning	O
setup	O
we	O
have	O
just	O
discussed	O
clearly	O
we	O
cannot	O
hope	O
that	O
the	O
learning	O
algorithm	O
will	O
find	O
a	O
hypothesis	B
whose	O
error	O
is	O
smaller	O
than	O
the	O
minimal	O
possible	O
error	O
that	O
of	O
the	O
bayes	O
predictor	B
furthermore	O
as	O
we	O
shall	O
prove	O
later	O
once	O
we	O
make	O
no	O
prior	O
assumptions	O
about	O
the	O
data-generating	O
distribution	O
no	O
algorithm	O
can	O
be	O
guaranteed	O
to	O
find	O
a	O
predictor	B
that	O
is	O
as	O
good	O
as	O
the	O
bayes	B
optimal	I
one	O
instead	O
we	O
require	O
that	O
the	O
learning	O
algorithm	O
will	O
find	O
a	O
predictor	B
whose	O
error	O
is	O
not	O
much	O
larger	O
than	O
the	O
best	O
possible	O
error	O
of	O
a	O
predictor	B
in	O
some	O
given	O
benchmark	O
hypothesis	B
class	I
of	O
course	O
the	O
strength	O
of	O
such	O
a	O
requirement	O
depends	O
on	O
the	O
choice	O
of	O
that	O
hypothesis	B
class	I
definition	O
pac	B
learnability	O
a	O
hypothesis	B
class	I
h	O
is	O
agnostic	B
pac	B
learnable	O
if	O
there	O
exist	O
a	O
function	B
mh	O
n	O
and	O
a	O
learning	O
algorithm	O
with	O
the	O
following	O
property	O
for	O
every	O
and	O
for	O
every	O
distribution	O
d	O
over	O
x	O
y	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
mh	O
i	O
i	O
d	O
examples	O
generated	O
by	O
d	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
h	O
such	O
that	O
with	O
probability	O
of	O
at	O
least	O
the	O
choice	O
of	O
the	O
m	O
training	O
examples	O
ldh	O
min	O
h	O
a	O
more	O
general	O
learning	O
model	O
clearly	O
if	O
the	O
realizability	B
assumption	O
holds	O
agnostic	B
pac	B
learning	O
provides	O
the	O
same	O
guarantee	O
as	O
pac	B
learning	O
in	O
that	O
sense	O
agnostic	B
pac	B
learning	O
generalizes	O
the	O
definition	O
of	O
pac	B
learning	O
when	O
the	O
realizability	B
assumption	O
does	O
not	O
hold	O
no	O
learner	O
can	O
guarantee	O
an	O
arbitrarily	O
small	O
error	O
nevertheless	O
under	O
the	O
definition	O
of	O
agnostic	B
pac	B
learning	O
a	O
learner	O
can	O
still	O
declare	O
success	O
if	O
its	O
error	O
is	O
not	O
much	O
larger	O
than	O
the	O
best	O
error	O
achievable	O
by	O
a	O
predictor	B
from	O
the	O
class	O
h	O
this	O
is	O
in	O
contrast	O
to	O
pac	B
learning	O
in	O
which	O
the	O
learner	O
is	O
required	O
to	O
achieve	O
a	O
small	O
error	O
in	O
absolute	O
terms	O
and	O
not	O
relative	O
to	O
the	O
best	O
error	O
achievable	O
by	O
the	O
hypothesis	B
class	I
the	O
scope	O
of	O
learning	O
problems	O
modeled	O
we	O
next	O
extend	O
our	O
model	O
so	O
that	O
it	O
can	O
be	O
applied	O
to	O
a	O
wide	O
variety	O
of	O
learning	O
tasks	O
let	O
us	O
consider	O
some	O
examples	O
of	O
different	O
learning	O
tasks	O
multiclass	B
classification	O
our	O
classification	O
does	O
not	O
have	O
to	O
be	O
binary	O
take	O
for	O
example	O
the	O
task	O
of	O
document	O
classification	O
we	O
wish	O
to	O
design	O
a	O
program	O
that	O
will	O
be	O
able	O
to	O
classify	O
given	O
documents	O
according	O
to	O
topics	O
news	O
sports	O
biology	O
medicine	O
a	O
learning	O
algorithm	O
for	O
such	O
a	O
task	O
will	O
have	O
access	O
to	O
examples	O
of	O
correctly	O
classified	O
documents	O
and	O
on	O
the	O
basis	O
of	O
these	O
examples	O
should	O
output	O
a	O
program	O
that	O
can	O
take	O
as	O
input	O
a	O
new	O
document	O
and	O
output	O
a	O
topic	O
classification	O
for	O
that	O
document	O
here	O
the	O
domain	B
set	B
is	O
the	O
set	B
of	O
all	O
potential	O
documents	O
once	O
again	O
we	O
would	O
usually	O
represent	O
documents	O
by	O
a	O
set	B
of	O
features	O
that	O
could	O
include	O
counts	O
of	O
different	O
key	O
words	O
in	O
the	O
document	O
as	O
well	O
as	O
other	O
possibly	O
relevant	O
features	O
like	O
the	O
size	O
of	O
the	O
document	O
or	O
its	O
origin	O
the	O
label	B
set	B
in	O
this	O
task	O
will	O
be	O
the	O
set	B
of	O
possible	O
document	O
topics	O
y	O
will	O
be	O
some	O
large	O
finite	O
set	B
once	O
we	O
determine	O
our	O
domain	B
and	O
label	B
sets	O
the	O
other	O
components	O
of	O
our	O
framework	O
look	O
exactly	O
the	O
same	O
as	O
in	O
the	O
papaya	O
tasting	O
example	O
our	O
training	O
sample	O
will	O
be	O
a	O
finite	O
sequence	O
of	O
vector	O
label	B
pairs	O
the	O
learner	O
s	O
output	O
will	O
be	O
a	O
function	B
from	O
the	O
domain	B
set	B
to	O
the	O
label	B
set	B
and	O
finally	O
for	O
our	O
measure	O
of	O
success	O
we	O
can	O
use	O
the	O
probability	O
over	O
topic	O
pairs	O
of	O
the	O
event	O
that	O
our	O
predictor	B
suggests	O
a	O
wrong	O
label	B
regression	B
in	O
this	O
task	O
one	O
wishes	O
to	O
find	O
some	O
simple	O
pattern	O
in	O
the	O
data	O
a	O
functional	O
relationship	O
between	O
the	O
x	O
and	O
y	O
components	O
of	O
the	O
data	O
for	O
example	O
one	O
wishes	O
to	O
find	O
a	O
linear	O
function	B
that	O
best	O
predicts	O
a	O
baby	O
s	O
birth	O
weight	O
on	O
the	O
basis	O
of	O
ultrasound	O
measures	O
of	O
his	O
head	O
circumference	O
abdominal	O
circumference	O
and	O
femur	O
length	O
here	O
our	O
domain	B
set	B
x	O
is	O
some	O
subset	O
of	O
three	O
ultrasound	O
measurements	O
and	O
the	O
set	B
of	O
labels	O
y	O
is	O
the	O
the	O
set	B
of	O
real	O
numbers	O
weight	O
in	O
grams	O
in	O
this	O
context	O
it	O
is	O
more	O
adequate	O
to	O
call	O
y	O
the	O
target	B
set	B
our	O
training	O
data	O
as	O
well	O
as	O
the	O
learner	O
s	O
output	O
are	O
as	O
before	O
finite	O
sequence	O
of	O
y	O
pairs	O
and	O
a	O
function	B
from	O
x	O
to	O
y	O
respectively	O
however	O
our	O
measure	O
of	O
success	O
is	O
a	O
formal	O
learning	O
model	O
different	O
we	O
may	O
evaluate	O
the	O
quality	O
of	O
a	O
hypothesis	B
function	B
h	O
x	O
y	O
by	O
the	O
expected	O
square	O
difference	O
between	O
the	O
true	O
labels	O
and	O
their	O
predicted	O
values	O
namely	O
ldh	O
def	O
dhx	O
e	O
to	O
accommodate	O
a	O
wide	O
range	O
of	O
learning	O
tasks	O
we	O
generalize	O
our	O
formalism	O
of	O
the	O
measure	O
of	O
success	O
as	O
follows	O
generalized	O
loss	B
functions	O
given	O
any	O
set	B
h	O
plays	O
the	O
role	O
of	O
our	O
hypotheses	O
or	O
models	O
and	O
some	O
domain	B
z	O
let	O
be	O
any	O
function	B
from	O
h	O
z	O
to	O
the	O
set	B
of	O
nonnegative	O
real	O
numbers	O
h	O
z	O
r	O
we	O
call	O
such	O
functions	O
loss	B
functions	O
note	O
that	O
for	O
prediction	O
problems	O
we	O
have	O
that	O
z	O
x	O
y	O
however	O
our	O
notion	O
of	O
the	O
loss	B
function	B
is	O
generalized	O
beyond	O
prediction	O
tasks	O
and	O
therefore	O
it	O
allows	O
z	O
to	O
be	O
any	O
domain	B
of	I
examples	I
instance	B
in	O
unsupervised	B
learning	I
tasks	O
such	O
as	O
the	O
one	O
described	O
in	O
chapter	O
z	O
is	O
not	O
a	O
product	O
of	O
an	O
instance	B
domain	B
and	O
a	O
label	B
domain	B
we	O
now	O
define	O
the	O
risk	B
function	B
to	O
be	O
the	O
expected	O
loss	B
of	O
a	O
classifier	B
h	O
h	O
with	O
respect	O
to	O
a	O
probability	O
distribution	O
d	O
over	O
z	O
namely	O
ldh	O
def	O
e	O
z	O
z	O
that	O
is	O
we	O
consider	O
the	O
expectation	O
of	O
the	O
loss	B
of	O
h	O
over	O
objects	O
z	O
picked	O
randomly	O
according	O
to	O
d	O
similarly	O
we	O
define	O
the	O
empirical	B
risk	B
to	O
be	O
the	O
expected	O
loss	B
over	O
a	O
given	O
sample	O
s	O
zm	O
z	O
m	O
namely	O
lsh	O
def	O
m	O
zi	O
the	O
loss	B
functions	O
used	O
in	O
the	O
preceding	O
examples	O
of	O
classification	O
and	O
regres	O
sion	O
tasks	O
are	O
as	O
follows	O
loss	B
here	O
our	O
random	O
variable	O
z	O
ranges	O
over	O
the	O
set	B
of	O
pairs	O
x	O
y	O
and	O
the	O
loss	B
function	B
is	O
y	O
def	O
if	O
hx	O
y	O
if	O
hx	O
y	O
this	O
loss	B
function	B
is	O
used	O
in	O
binary	O
or	O
multiclass	B
classification	O
problems	O
one	O
should	O
note	O
that	O
for	O
a	O
random	O
variable	O
taking	O
the	O
values	O
e	O
d	O
p	O
d	O
consequently	O
for	O
this	O
loss	B
function	B
the	O
definitions	O
of	O
ldh	O
given	O
in	O
equation	O
and	O
equation	O
coincide	O
square	B
loss	B
here	O
our	O
random	O
variable	O
z	O
ranges	O
over	O
the	O
set	B
of	O
pairs	O
x	O
y	O
and	O
the	O
loss	B
function	B
is	O
y	O
def	O
summary	O
this	O
loss	B
function	B
is	O
used	O
in	O
regression	B
problems	O
we	O
will	O
later	O
see	O
more	O
examples	O
of	O
useful	O
instantiations	O
of	O
loss	B
functions	O
to	O
summarize	O
we	O
formally	O
define	O
agnostic	B
pac	B
learnability	O
for	O
general	O
loss	B
functions	O
definition	O
pac	B
learnability	O
for	O
general	O
loss	B
functions	O
a	O
hypothesis	B
class	I
h	O
is	O
agnostic	B
pac	B
learnable	O
with	O
respect	O
to	O
a	O
set	B
z	O
and	O
a	O
loss	B
function	B
h	O
z	O
r	O
if	O
there	O
exist	O
a	O
function	B
mh	O
n	O
and	O
a	O
learning	O
algorithm	O
with	O
the	O
following	O
property	O
for	O
every	O
and	O
for	O
every	O
distribution	O
d	O
over	O
z	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
mh	O
i	O
i	O
d	O
examples	O
generated	O
by	O
d	O
the	O
algorithm	O
returns	O
h	O
h	O
such	O
that	O
with	O
probability	O
of	O
at	O
least	O
the	O
choice	O
of	O
the	O
m	O
training	O
examples	O
ldh	O
min	O
h	O
where	O
ldh	O
ez	O
z	O
remark	O
note	O
about	O
measurability	O
in	O
the	O
aforementioned	O
definition	O
for	O
every	O
h	O
h	O
we	O
view	O
the	O
function	B
z	O
r	O
as	O
a	O
random	O
variable	O
and	O
define	O
ldh	O
to	O
be	O
the	O
expected	O
value	O
of	O
this	O
random	O
variable	O
for	O
that	O
we	O
need	O
to	O
require	O
that	O
the	O
function	B
is	O
measurable	O
formally	O
we	O
assume	O
that	O
there	O
is	O
a	O
of	O
subsets	O
of	O
z	O
over	O
which	O
the	O
probability	O
d	O
is	O
defined	O
and	O
that	O
the	O
preimage	O
of	O
every	O
initial	O
segment	O
in	O
r	O
is	O
in	O
this	O
in	O
the	O
specific	O
case	O
of	O
binary	O
classification	O
with	O
the	O
loss	B
the	O
is	O
over	O
x	O
and	O
our	O
assumption	O
on	O
is	O
equivalent	O
to	O
the	O
assumption	O
that	O
for	O
every	O
h	O
the	O
set	B
hx	O
x	O
x	O
is	O
in	O
the	O
remark	O
versus	O
representation-independent	O
learning	O
in	O
the	O
preceding	O
definition	O
we	O
required	O
that	O
the	O
algorithm	O
will	O
return	O
a	O
hypothesis	B
from	O
h	O
in	O
some	O
situations	O
h	O
is	O
a	O
subset	O
of	O
a	O
set	B
and	O
the	O
loss	B
function	B
can	O
be	O
naturally	O
extended	O
to	O
be	O
a	O
function	B
from	O
z	O
to	O
the	O
reals	O
in	O
this	O
case	O
we	O
may	O
allow	O
the	O
algorithm	O
to	O
return	O
a	O
hypothesis	B
as	O
long	O
as	O
it	O
satisfies	O
the	O
requirement	O
minh	O
h	O
ldh	O
allowing	O
the	O
algorithm	O
to	O
output	O
a	O
hypothesis	B
from	O
is	O
called	O
representation	B
independent	I
learning	O
while	O
proper	B
learning	O
occurs	O
when	O
the	O
algorithm	O
must	O
output	O
a	O
hypothesis	B
from	O
h	O
representation	B
independent	I
learning	O
is	O
sometimes	O
called	O
improper	O
learning	O
although	O
there	O
is	O
nothing	O
improper	O
in	O
representation	B
independent	I
learning	O
summary	O
in	O
this	O
chapter	O
we	O
defined	O
our	O
main	O
formal	O
learning	O
model	O
pac	B
learning	O
the	O
basic	O
model	O
relies	O
on	O
the	O
realizability	B
assumption	O
while	O
the	O
agnostic	O
variant	O
does	O
a	O
formal	O
learning	O
model	O
not	O
impose	O
any	O
restrictions	O
on	O
the	O
underlying	O
distribution	O
over	O
the	O
examples	O
we	O
also	O
generalized	O
the	O
pac	B
model	O
to	O
arbitrary	O
loss	B
functions	O
we	O
will	O
sometimes	O
refer	O
to	O
the	O
most	O
general	O
model	O
simply	O
as	O
pac	B
learning	O
omitting	O
the	O
agnostic	O
prefix	O
and	O
letting	O
the	O
reader	O
infer	O
what	O
the	O
underlying	O
loss	B
function	B
is	O
from	O
the	O
context	O
when	O
we	O
would	O
like	O
to	O
emphasize	O
that	O
we	O
are	O
dealing	O
with	O
the	O
original	O
pac	B
setting	O
we	O
mention	O
that	O
the	O
realizability	B
assumption	O
holds	O
in	O
chapter	O
we	O
will	O
discuss	O
other	O
notions	O
of	O
learnability	O
bibliographic	O
remarks	O
our	O
most	O
general	O
definition	O
of	O
agnostic	B
pac	B
learning	O
with	O
general	O
loss	B
functions	O
follows	O
the	O
works	O
of	O
vladimir	O
vapnik	O
and	O
alexey	O
chervonenkis	O
chervonenkis	O
in	O
particular	O
we	O
follow	O
vapnik	O
s	O
general	O
setting	O
of	O
learning	O
vapnik	O
vapnik	O
vapnik	O
pac	B
learning	O
was	O
introduced	O
by	O
valiant	O
valiant	O
was	O
named	O
the	O
winner	O
of	O
the	O
turing	O
award	O
for	O
the	O
introduction	O
of	O
the	O
pac	B
model	O
valiant	O
s	O
definition	O
requires	O
that	O
the	O
sample	B
complexity	I
will	O
be	O
polynomial	O
in	O
and	O
in	O
as	O
well	O
as	O
in	O
the	O
representation	O
size	O
of	O
hypotheses	O
in	O
the	O
class	O
also	O
kearns	O
vazirani	O
as	O
we	O
will	O
see	O
in	O
chapter	O
if	O
a	O
problem	O
is	O
at	O
all	O
pac	B
learnable	O
then	O
the	O
sample	B
complexity	I
depends	O
polynomially	O
on	O
and	O
valiant	O
s	O
definition	O
also	O
requires	O
that	O
the	O
runtime	O
of	O
the	O
learning	O
algorithm	O
will	O
be	O
polynomial	O
in	O
these	O
quantities	O
in	O
contrast	O
we	O
chose	O
to	O
distinguish	O
between	O
the	O
statistical	O
aspect	O
of	O
learning	O
and	O
the	O
computational	O
aspect	O
of	O
learning	O
we	O
will	O
elaborate	O
on	O
the	O
computational	O
aspect	O
later	O
on	O
in	O
chapter	O
where	O
we	O
introduce	O
the	O
full	O
pac	B
learning	O
model	O
of	O
valiant	O
for	O
expository	O
reasons	O
we	O
use	O
the	O
term	O
pac	B
learning	O
even	O
when	O
we	O
ignore	O
the	O
runtime	O
aspect	O
of	O
learning	O
finally	O
the	O
formalization	O
of	O
agnostic	B
pac	B
learning	O
is	O
due	O
to	O
haussler	O
exercises	O
monotonicity	O
of	O
sample	B
complexity	I
let	O
h	O
be	O
a	O
hypothesis	B
class	I
for	O
a	O
binary	O
classification	O
task	O
suppose	O
that	O
h	O
is	O
pac	B
learnable	O
and	O
its	O
sample	B
complexity	I
is	O
given	O
by	O
mh	O
show	O
that	O
mh	O
is	O
monotonically	O
nonincreasing	O
in	O
each	O
of	O
its	O
parameters	O
that	O
is	O
show	O
that	O
given	O
and	O
given	O
we	O
have	O
that	O
similarly	O
show	O
that	O
given	O
and	O
given	O
we	O
have	O
that	O
mh	O
mh	O
let	O
x	O
be	O
a	O
discrete	O
domain	B
and	O
let	O
hsingleton	O
z	O
x	O
where	O
for	O
each	O
z	O
x	O
hz	O
is	O
the	O
function	B
defined	O
by	O
hzx	O
if	O
x	O
z	O
and	O
hzx	O
if	O
x	O
z	O
h	O
is	O
simply	O
the	O
all-negative	O
hypothesis	B
namely	O
x	O
x	O
h	O
the	O
realizability	B
assumption	O
here	O
implies	O
that	O
the	O
true	O
hypothesis	B
f	O
labels	O
negatively	O
all	O
examples	O
in	O
the	O
domain	B
perhaps	O
except	O
one	O
exercises	O
in	O
the	O
realizable	O
setup	O
describe	O
an	O
algorithm	O
that	O
implements	O
the	O
erm	B
rule	O
for	O
learning	O
hsingleton	O
show	O
that	O
hsingleton	O
is	O
pac	B
learnable	O
provide	O
an	O
upper	O
bound	O
on	O
the	O
let	O
x	O
y	O
and	O
let	O
h	O
be	O
the	O
class	O
of	O
concentric	O
circles	O
in	O
the	O
plane	O
that	O
is	O
h	O
r	O
r	O
where	O
hrx	O
r	O
prove	O
that	O
h	O
is	O
pac	B
learnable	O
realizability	B
and	O
its	O
sample	B
complexity	I
is	O
bounded	O
by	O
sample	B
complexity	I
mh	O
in	O
this	O
question	O
we	O
study	O
the	O
hypothesis	B
class	I
of	O
boolean	B
conjunctions	I
defined	O
as	O
follows	O
the	O
instance	B
space	I
is	O
x	O
and	O
the	O
label	B
set	B
is	O
y	O
a	O
literal	O
over	O
the	O
variables	O
xd	O
is	O
a	O
simple	O
boolean	O
function	B
that	O
takes	O
the	O
form	O
f	O
xi	O
for	O
some	O
i	O
or	O
f	O
xi	O
for	O
some	O
i	O
we	O
use	O
the	O
notation	O
xi	O
as	O
a	O
shorthand	O
for	O
xi	O
a	O
conjunction	O
is	O
any	O
product	O
of	O
literals	O
in	O
boolean	O
logic	O
the	O
product	O
is	O
denoted	O
using	O
the	O
sign	O
for	O
example	O
the	O
function	B
hx	O
is	O
written	O
as	O
we	O
consider	O
the	O
hypothesis	B
class	I
of	O
all	O
conjunctions	O
of	O
literals	O
over	O
the	O
d	O
variables	O
the	O
empty	O
conjunction	O
is	O
interpreted	O
as	O
the	O
all-positive	O
hypothesis	B
the	O
function	B
that	O
returns	O
hx	O
for	O
all	O
x	O
the	O
conjunction	O
similarly	O
any	O
conjunction	O
involving	O
a	O
literal	O
and	O
its	O
negation	O
is	O
allowed	O
and	O
interpreted	O
as	O
the	O
all-negative	O
hypothesis	B
the	O
conjunction	O
that	O
returns	O
hx	O
for	O
all	O
x	O
we	O
assume	O
realizability	B
namely	O
we	O
assume	O
that	O
there	O
exists	O
a	O
boolean	O
conjunction	O
that	O
generates	O
the	O
labels	O
thus	O
each	O
example	O
y	O
x	O
y	O
consists	O
of	O
an	O
assignment	O
to	O
the	O
d	O
boolean	O
variables	O
xd	O
and	O
its	O
truth	O
value	O
for	O
false	O
and	O
for	O
true	O
for	O
instance	B
let	O
d	O
and	O
suppose	O
that	O
the	O
true	O
conjunction	O
is	O
then	O
the	O
training	B
set	B
s	O
might	O
contain	O
the	O
following	O
instances	O
prove	O
that	O
the	O
hypothesis	B
class	I
of	O
all	O
conjunctions	O
over	O
d	O
variables	O
is	O
pac	B
learnable	O
and	O
bound	O
its	O
sample	B
complexity	I
propose	O
an	O
algorithm	O
that	O
implements	O
the	O
erm	B
rule	O
whose	O
runtime	O
is	O
polynomial	O
in	O
d	O
m	O
let	O
x	O
be	O
a	O
domain	B
and	O
let	O
be	O
a	O
sequence	O
of	O
distributions	O
over	O
x	O
let	O
h	O
be	O
a	O
finite	O
class	O
of	O
binary	O
classifiers	O
over	O
x	O
and	O
let	O
f	O
h	O
suppose	O
we	O
are	O
getting	O
a	O
sample	O
s	O
of	O
m	O
examples	O
such	O
that	O
the	O
instances	O
are	O
independent	O
but	O
are	O
not	O
identically	O
distributed	O
the	O
ith	O
instance	B
is	O
sampled	O
from	O
di	O
and	O
then	O
yi	O
is	O
set	B
to	O
be	O
f	O
let	O
dm	O
denote	O
the	O
average	O
that	O
is	O
dm	O
dmm	O
fix	O
an	O
accuracy	B
parameter	O
show	O
that	O
h	O
h	O
s	O
t	O
l	O
dmf	O
and	O
lsf	O
a	O
formal	O
learning	O
model	O
hint	O
use	O
the	O
geometric-arithmetic	O
mean	O
inequality	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
binary	O
classifiers	O
show	O
that	O
if	O
h	O
is	O
agnostic	B
pac	B
learnable	O
then	O
h	O
is	O
pac	B
learnable	O
as	O
well	O
furthermore	O
if	O
a	O
is	O
a	O
successful	O
agnostic	B
pac	B
learner	O
for	O
h	O
then	O
a	O
is	O
also	O
a	O
successful	O
pac	B
learner	O
for	O
h	O
the	O
bayes	B
optimal	I
predictor	B
show	O
that	O
for	O
every	O
probability	O
distribution	O
d	O
the	O
bayes	B
optimal	I
predictor	B
fd	O
is	O
optimal	O
in	O
the	O
sense	O
that	O
for	O
every	O
classifier	B
g	O
from	O
x	O
to	O
ldfd	O
ldg	O
probability	O
distribution	O
d	O
if	O
we	O
say	O
that	O
a	O
learning	O
algorithm	O
a	O
is	O
better	O
than	O
b	O
with	O
respect	O
to	O
some	O
ldas	O
ldbs	O
for	O
all	O
samples	O
s	O
we	O
say	O
that	O
a	O
learning	O
algorithm	O
a	O
is	O
better	O
than	O
b	O
if	O
it	O
is	O
better	O
than	O
b	O
with	O
respect	O
to	O
all	O
probability	O
distributions	O
d	O
over	O
x	O
a	O
probabilistic	O
label	B
predictor	B
is	O
a	O
function	B
that	O
assigns	O
to	O
every	O
domain	B
point	O
x	O
a	O
probability	O
value	O
hx	O
that	O
determines	O
the	O
probability	O
of	O
predicting	O
the	O
label	B
that	O
is	O
given	O
such	O
an	O
h	O
and	O
an	O
input	O
x	O
the	O
label	B
for	O
x	O
is	O
predicted	O
by	O
tossing	O
a	O
coin	O
with	O
bias	B
hx	O
toward	O
heads	O
and	O
predicting	O
iff	O
the	O
coin	O
comes	O
up	O
heads	O
formally	O
we	O
define	O
a	O
probabilistic	O
label	B
predictor	B
as	O
a	O
function	B
h	O
x	O
the	O
loss	B
of	O
such	O
h	O
on	O
an	O
example	O
y	O
is	O
defined	O
to	O
be	O
y	O
which	O
is	O
exactly	O
the	O
probability	O
that	O
the	O
prediction	O
of	O
h	O
will	O
not	O
be	O
equal	O
to	O
y	O
note	O
that	O
if	O
h	O
is	O
deterministic	O
that	O
is	O
returns	O
values	O
in	O
then	O
y	O
prove	O
that	O
for	O
every	O
data-generating	O
distribution	O
d	O
over	O
x	O
the	O
bayes	B
optimal	I
predictor	B
has	O
the	O
smallest	O
risk	B
the	O
loss	B
function	B
y	O
y	O
among	O
all	O
possible	O
label	B
predictors	O
including	O
probabilistic	O
ones	O
let	O
x	O
be	O
a	O
domain	B
and	O
be	O
a	O
set	B
of	O
labels	O
prove	O
that	O
for	O
every	O
distribution	O
d	O
over	O
x	O
there	O
exist	O
a	O
learning	O
algorithm	O
ad	O
that	O
is	O
better	O
than	O
any	O
other	O
learning	O
algorithm	O
with	O
respect	O
to	O
d	O
prove	O
that	O
for	O
every	O
learning	O
algorithm	O
a	O
there	O
exist	O
a	O
probability	O
distribution	O
d	O
and	O
a	O
learning	O
algorithm	O
b	O
such	O
that	O
a	O
is	O
not	O
better	O
than	O
b	O
w	O
r	O
t	O
d	O
consider	O
a	O
variant	O
of	O
the	O
pac	B
model	O
in	O
which	O
there	O
are	O
two	O
example	O
oracles	O
one	O
that	O
generates	O
positive	O
examples	O
and	O
one	O
that	O
generates	O
negative	O
examples	O
both	O
according	O
to	O
the	O
underlying	O
distribution	O
d	O
on	O
x	O
formally	O
given	O
a	O
target	O
function	B
f	O
x	O
let	O
d	O
be	O
the	O
distribution	O
over	O
x	O
x	O
f	O
defined	O
by	O
da	O
dadx	O
for	O
every	O
a	O
x	O
similarly	O
d	O
is	O
the	O
distribution	O
over	O
x	O
induced	O
by	O
d	O
the	O
definition	O
of	O
pac	B
learnability	O
in	O
the	O
two-oracle	O
model	O
is	O
the	O
same	O
as	O
the	O
standard	O
definition	O
of	O
pac	B
learnability	O
except	O
that	O
here	O
the	O
learner	O
has	O
access	O
to	O
mh	O
i	O
i	O
d	O
examples	O
from	O
d	O
and	O
m	O
i	O
i	O
d	O
examples	O
from	O
d	O
the	O
learner	O
s	O
goal	O
is	O
to	O
output	O
h	O
s	O
t	O
with	O
probability	O
at	O
least	O
the	O
choice	O
exercises	O
of	O
the	O
two	O
training	O
sets	O
and	O
possibly	O
over	O
the	O
nondeterministic	O
decisions	O
made	O
by	O
the	O
learning	O
algorithm	O
both	O
ldf	O
and	O
ld	O
show	O
that	O
if	O
h	O
is	O
pac	B
learnable	O
the	O
standard	O
one-oracle	O
model	O
then	O
h	O
is	O
pac	B
learnable	O
in	O
the	O
two-oracle	O
model	O
define	O
h	O
to	O
be	O
the	O
always-plus	O
hypothesis	B
and	O
h	O
to	O
be	O
the	O
alwaysminus	O
hypothesis	B
assume	O
that	O
h	O
h	O
h	O
show	O
that	O
if	O
h	O
is	O
pac	B
learnable	O
in	O
the	O
two-oracle	O
model	O
then	O
h	O
is	O
pac	B
learnable	O
in	O
the	O
standard	O
one-oracle	O
model	O
learning	O
via	O
uniform	B
convergence	I
the	O
first	O
formal	O
learning	O
model	O
that	O
we	O
have	O
discussed	O
was	O
the	O
pac	B
model	O
in	O
chapter	O
we	O
have	O
shown	O
that	O
under	O
the	O
realizability	B
assumption	O
any	O
finite	O
hypothesis	B
class	I
is	O
pac	B
learnable	O
in	O
this	O
chapter	O
we	O
will	O
develop	O
a	O
general	O
tool	O
uniform	B
convergence	I
and	O
apply	O
it	O
to	O
show	O
that	O
any	O
finite	O
class	O
is	O
learnable	O
in	O
the	O
agnostic	B
pac	B
model	O
with	O
general	O
loss	B
functions	O
as	O
long	O
as	O
the	O
range	O
loss	B
function	B
is	O
bounded	O
uniform	B
convergence	I
is	O
sufficient	O
for	O
learnability	O
the	O
idea	O
behind	O
the	O
learning	O
condition	O
discussed	O
in	O
this	O
chapter	O
is	O
very	O
simple	O
recall	B
that	O
given	O
a	O
hypothesis	B
class	I
h	O
the	O
erm	B
learning	O
paradigm	O
works	O
as	O
follows	O
upon	O
receiving	O
a	O
training	O
sample	O
s	O
the	O
learner	O
evaluates	O
the	O
risk	B
error	O
of	O
each	O
h	O
in	O
h	O
on	O
the	O
given	O
sample	O
and	O
outputs	O
a	O
member	O
of	O
h	O
that	O
minimizes	O
this	O
empirical	B
risk	B
the	O
hope	O
is	O
that	O
an	O
h	O
that	O
minimizes	O
the	O
empirical	B
risk	B
with	O
respect	O
to	O
s	O
is	O
a	O
risk	B
minimizer	O
has	O
risk	B
close	O
to	O
the	O
minimum	O
with	O
respect	O
to	O
the	O
true	O
data	O
probability	O
distribution	O
as	O
well	O
for	O
that	O
it	O
suffices	O
to	O
ensure	O
that	O
the	O
empirical	O
risks	O
of	O
all	O
members	O
of	O
h	O
are	O
good	O
approximations	O
of	O
their	O
true	O
risk	B
put	O
another	O
way	O
we	O
need	O
that	O
uniformly	O
over	O
all	O
hypotheses	O
in	O
the	O
hypothesis	B
class	I
the	O
empirical	B
risk	B
will	O
be	O
close	O
to	O
the	O
true	O
risk	B
as	O
formalized	O
in	O
the	O
following	O
definition	O
sample	O
a	O
training	B
set	B
s	O
is	O
called	O
domain	B
z	O
hypothesis	B
class	I
h	O
loss	B
function	B
and	O
distribution	O
d	O
if	O
h	O
h	O
ldh	O
the	O
next	O
simple	O
lemma	O
states	O
that	O
whenever	O
the	O
sample	O
is	O
the	O
erm	B
learning	O
rule	O
is	O
guaranteed	O
to	O
return	O
a	O
good	O
hypothesis	B
lemma	O
assume	O
that	O
a	O
training	B
set	B
s	O
is	O
domain	B
z	O
hypothesis	B
class	I
h	O
loss	B
function	B
and	O
distribution	O
d	O
then	O
any	O
output	O
of	O
ermhs	O
namely	O
any	O
hs	O
argminh	O
h	O
lsh	O
satisfies	O
ldhs	O
min	O
h	O
h	O
ldh	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
finite	O
classes	O
are	O
agnostic	B
pac	B
learnable	O
proof	O
for	O
every	O
h	O
h	O
ldhs	O
lshs	O
lsh	O
ldh	O
ldh	O
where	O
the	O
first	O
and	O
third	O
inequalities	O
are	O
due	O
to	O
the	O
assumption	O
that	O
s	O
is	O
representative	O
and	O
the	O
second	O
inequality	O
holds	O
since	O
hs	O
is	O
an	O
erm	B
predictor	B
the	O
preceding	O
lemma	O
implies	O
that	O
to	O
ensure	O
that	O
the	O
erm	B
rule	O
is	O
an	O
agnostic	B
pac	B
learner	O
it	O
suffices	O
to	O
show	O
that	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
random	O
choice	O
of	O
a	O
training	B
set	B
it	O
will	O
be	O
an	O
training	B
set	B
the	O
uniform	B
convergence	I
condition	O
formalizes	O
this	O
requirement	O
definition	O
convergence	O
we	O
say	O
that	O
a	O
hypothesis	B
class	I
h	O
has	O
the	O
uniform	B
convergence	I
property	O
a	O
domain	B
z	O
and	O
a	O
loss	B
function	B
if	O
there	O
exists	O
a	O
function	B
much	O
n	O
such	O
that	O
for	O
every	O
and	O
for	O
every	O
probability	O
distribution	O
d	O
over	O
z	O
if	O
s	O
is	O
a	O
sample	O
of	O
m	O
much	O
examples	O
drawn	O
i	O
i	O
d	O
according	O
to	O
d	O
then	O
with	O
probability	O
of	O
at	O
least	O
s	O
is	O
similar	O
to	O
the	O
definition	O
of	O
sample	B
complexity	I
for	O
pac	B
learning	O
the	O
function	B
much	O
measures	O
the	O
sample	B
complexity	I
of	O
obtaining	O
the	O
uniform	B
convergence	I
property	O
namely	O
how	O
many	O
examples	O
we	O
need	O
to	O
ensure	O
that	O
with	O
probability	O
of	O
at	O
least	O
the	O
sample	O
would	O
be	O
the	O
term	O
uniform	O
here	O
refers	O
to	O
having	O
a	O
fixed	O
sample	O
size	O
that	O
works	O
for	O
all	O
members	O
of	O
h	O
and	O
over	O
all	O
possible	O
probability	O
distributions	O
over	O
the	O
domain	B
the	O
following	O
corollary	O
follows	O
directly	O
from	O
lemma	O
and	O
the	O
definition	O
of	O
uniform	B
convergence	I
if	O
a	O
class	O
h	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
a	O
corollary	O
function	B
much	O
then	O
the	O
class	O
is	O
agnostically	O
pac	B
learnable	O
with	O
the	O
sample	B
complexity	I
mh	O
much	O
furthermore	O
in	O
that	O
case	O
the	O
ermh	O
paradigm	O
is	O
a	O
successful	O
agnostic	B
pac	B
learner	O
for	O
h	O
finite	O
classes	O
are	O
agnostic	B
pac	B
learnable	O
in	O
view	O
of	O
corollary	O
the	O
claim	O
that	O
every	O
finite	O
hypothesis	B
class	I
is	O
agnostic	B
pac	B
learnable	O
will	O
follow	O
once	O
we	O
establish	O
that	O
uniform	B
convergence	I
holds	O
for	O
a	O
finite	O
hypothesis	B
class	I
to	O
show	O
that	O
uniform	B
convergence	I
holds	O
we	O
follow	O
a	O
two	O
step	O
argument	O
similar	O
to	O
the	O
derivation	O
in	O
chapter	O
the	O
first	O
step	O
applies	O
the	O
union	B
bound	I
while	O
the	O
second	O
step	O
employs	O
a	O
measure	B
concentration	I
inequality	O
we	O
now	O
explain	O
these	O
two	O
steps	O
in	O
detail	O
fix	O
some	O
we	O
need	O
to	O
find	O
a	O
sample	O
size	O
m	O
that	O
guarantees	O
that	O
for	O
any	O
d	O
with	O
probability	O
of	O
at	O
least	O
of	O
the	O
choice	O
of	O
s	O
zm	O
sampled	O
learning	O
via	O
uniform	B
convergence	I
i	O
i	O
d	O
from	O
d	O
we	O
have	O
that	O
for	O
all	O
h	O
h	O
ldh	O
that	O
is	O
dms	O
h	O
hlsh	O
ldh	O
equivalently	O
we	O
need	O
to	O
show	O
that	O
dms	O
h	O
hlsh	O
ldh	O
writing	O
h	O
hlsh	O
ldh	O
h	O
hs	O
ldh	O
and	O
applying	O
the	O
union	B
bound	I
we	O
obtain	O
dms	O
h	O
hlsh	O
ldh	O
dms	O
ldh	O
h	O
h	O
our	O
second	O
step	O
will	O
be	O
to	O
argue	O
that	O
each	O
summand	O
of	O
the	O
right-hand	O
side	O
of	O
this	O
inequality	O
is	O
small	O
enough	O
a	O
sufficiently	O
large	O
m	O
that	O
is	O
we	O
will	O
show	O
that	O
for	O
any	O
fixed	O
hypothesis	B
h	O
is	O
chosen	O
in	O
advance	O
prior	O
to	O
the	O
sampling	O
of	O
the	O
training	B
set	B
the	O
gap	O
between	O
the	O
true	O
and	O
empirical	O
risks	O
ldh	O
is	O
likely	O
to	O
be	O
small	O
zi	O
since	O
each	O
zi	O
is	O
sampled	O
i	O
i	O
d	O
from	O
d	O
the	O
expected	O
value	O
of	O
the	O
random	O
variable	O
zi	O
is	O
ldh	O
by	O
the	O
linearity	O
of	O
expectation	O
it	O
follows	O
that	O
ldh	O
is	O
also	O
the	O
expected	O
value	O
of	O
lsh	O
hence	O
the	O
quantity	O
lsh	O
is	O
the	O
deviation	O
of	O
the	O
random	O
variable	O
lsh	O
from	O
its	O
expectation	O
we	O
therefore	O
need	O
to	O
show	O
that	O
the	O
measure	O
of	O
lsh	O
is	O
concentrated	O
around	O
its	O
expected	O
value	O
recall	B
that	O
ldh	O
ez	O
z	O
and	O
that	O
lsh	O
m	O
a	O
basic	O
statistical	O
fact	O
the	O
law	O
of	O
large	O
numbers	O
states	O
that	O
when	O
m	O
goes	O
to	O
infinity	O
empirical	O
averages	O
converge	O
to	O
their	O
true	O
expectation	O
this	O
is	O
true	O
for	O
lsh	O
since	O
it	O
is	O
the	O
empirical	O
average	O
of	O
m	O
i	O
i	O
d	O
random	O
variables	O
however	O
since	O
the	O
law	O
of	O
large	O
numbers	O
is	O
only	O
an	O
asymptotic	O
result	O
it	O
provides	O
no	O
information	O
about	O
the	O
gap	O
between	O
the	O
empirically	O
estimated	O
error	O
and	O
its	O
true	O
value	O
for	O
any	O
given	O
finite	O
sample	O
size	O
instead	O
we	O
will	O
use	O
a	O
measure	B
concentration	I
inequality	O
due	O
to	O
hoeffding	O
which	O
quantifies	O
the	O
gap	O
between	O
empirical	O
averages	O
and	O
their	O
expected	O
value	O
lemma	O
s	O
inequality	O
let	O
m	O
be	O
a	O
sequence	O
of	O
i	O
i	O
d	O
random	O
variables	O
and	O
assume	O
that	O
for	O
all	O
i	O
e	O
i	O
and	O
pa	O
i	O
b	O
then	O
for	O
any	O
m	O
p	O
i	O
m	O
the	O
proof	O
can	O
be	O
found	O
in	O
appendix	O
b	O
getting	O
back	O
to	O
our	O
problem	O
let	O
i	O
be	O
the	O
random	O
variable	O
zi	O
since	O
h	O
is	O
fixed	O
and	O
zm	O
are	O
sampled	O
i	O
i	O
d	O
it	O
follows	O
that	O
m	O
are	O
also	O
i	O
i	O
d	O
random	O
variables	O
furthermore	O
lsh	O
i	O
and	O
ldh	O
let	O
us	O
m	O
finite	O
classes	O
are	O
agnostic	B
pac	B
learnable	O
m	O
further	O
assume	O
that	O
the	O
range	O
of	O
is	O
and	O
therefore	O
i	O
we	O
therefore	O
obtain	O
that	O
dms	O
ldh	O
p	O
m	O
dms	O
h	O
hlsh	O
ldh	O
m	O
m	O
combining	O
this	O
with	O
equation	O
yields	O
i	O
h	O
h	O
finally	O
if	O
we	O
choose	O
m	O
then	O
dms	O
h	O
hlsh	O
ldh	O
corollary	O
let	O
h	O
be	O
a	O
finite	O
hypothesis	B
class	I
let	O
z	O
be	O
a	O
domain	B
and	O
let	O
h	O
z	O
be	O
a	O
loss	B
function	B
then	O
h	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
with	O
sample	B
complexity	I
much	O
furthermore	O
the	O
class	O
is	O
agnostically	O
pac	B
learnable	O
using	O
the	O
erm	B
algorithm	O
with	O
sample	B
complexity	I
mh	O
much	O
remark	O
discretization	B
trick	I
while	O
the	O
preceding	O
corollary	O
only	O
applies	O
to	O
finite	O
hypothesis	B
classes	O
there	O
is	O
a	O
simple	O
trick	O
that	O
allows	O
us	O
to	O
get	O
a	O
very	O
good	O
estimate	O
of	O
the	O
practical	O
sample	B
complexity	I
of	O
infinite	O
hypothesis	B
classes	O
consider	O
a	O
hypothesis	B
class	I
that	O
is	O
parameterized	O
by	O
d	O
parameters	O
for	O
example	O
let	O
x	O
r	O
y	O
and	O
the	O
hypothesis	B
class	I
h	O
be	O
all	O
functions	O
of	O
the	O
form	O
h	O
signx	O
that	O
is	O
each	O
hypothesis	B
is	O
parameterized	O
by	O
one	O
parameter	O
r	O
and	O
the	O
hypothesis	B
outputs	O
for	O
all	O
instances	O
larger	O
than	O
and	O
outputs	O
for	O
instances	O
smaller	O
than	O
this	O
is	O
a	O
hypothesis	B
class	I
of	O
an	O
infinite	O
size	O
however	O
if	O
we	O
are	O
going	O
to	O
learn	O
this	O
hypothesis	B
class	I
in	O
practice	O
using	O
a	O
computer	O
we	O
will	O
probably	O
maintain	O
real	O
numbers	O
using	O
floating	O
point	O
representation	O
say	O
of	O
bits	O
it	O
follows	O
that	O
in	O
practice	O
our	O
hypothesis	B
class	I
is	O
parameterized	O
by	O
the	O
set	B
of	O
scalars	O
that	O
can	O
be	O
represented	O
using	O
a	O
bits	O
floating	O
point	O
number	O
there	O
are	O
at	O
most	O
such	O
numbers	O
hence	O
the	O
actual	O
size	O
of	O
our	O
hypothesis	B
class	I
is	O
at	O
most	O
more	O
generally	O
if	O
our	O
hypothesis	B
class	I
is	O
parameterized	O
by	O
d	O
numbers	O
in	O
practice	O
we	O
learn	O
a	O
hypothesis	B
class	I
of	O
size	O
at	O
most	O
applying	O
corollary	O
we	O
obtain	O
that	O
the	O
sample	B
complexity	I
of	O
such	O
learning	O
via	O
uniform	B
convergence	I
classes	O
is	O
bounded	O
by	O
this	O
upper	O
bound	O
on	O
the	O
sample	B
complexity	I
has	O
the	O
deficiency	O
of	O
being	O
dependent	O
on	O
the	O
specific	O
representation	O
of	O
real	O
numbers	O
used	O
by	O
our	O
machine	O
in	O
chapter	O
we	O
will	O
introduce	O
a	O
rigorous	O
way	O
to	O
analyze	O
the	O
sample	B
complexity	I
of	O
infinite	O
size	O
hypothesis	B
classes	O
nevertheless	O
the	O
discretization	B
trick	I
can	O
be	O
used	O
to	O
get	O
a	O
rough	O
estimate	O
of	O
the	O
sample	B
complexity	I
in	O
many	O
practical	O
situations	O
summary	O
if	O
the	O
uniform	B
convergence	I
property	O
holds	O
for	O
a	O
hypothesis	B
class	I
h	O
then	O
in	O
most	O
cases	O
the	O
empirical	O
risks	O
of	O
hypotheses	O
in	O
h	O
will	O
faithfully	O
represent	O
their	O
true	O
risks	O
uniform	B
convergence	I
suffices	O
for	O
agnostic	B
pac	B
learnability	O
using	O
the	O
erm	B
rule	O
we	O
have	O
shown	O
that	O
finite	O
hypothesis	B
classes	O
enjoy	O
the	O
uniform	B
convergence	I
property	O
and	O
are	O
hence	O
agnostic	B
pac	B
learnable	O
bibliographic	O
remarks	O
classes	O
of	O
functions	O
for	O
which	O
the	O
uniform	B
convergence	I
property	O
holds	O
are	O
also	O
called	O
glivenko-cantelli	B
classes	O
named	O
after	O
valery	O
ivanovich	O
glivenko	O
and	O
francesco	O
paolo	O
cantelli	O
who	O
proved	O
the	O
first	O
uniform	B
convergence	I
result	O
in	O
the	O
see	O
gine	O
zinn	O
the	O
relation	O
between	O
uniform	B
convergence	I
and	O
learnability	O
was	O
thoroughly	O
studied	O
by	O
vapnik	O
see	O
vapnik	O
vapnik	O
in	O
fact	O
as	O
we	O
will	O
see	O
later	O
in	O
chapter	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
states	O
that	O
in	O
binary	O
classification	O
problems	O
uniform	B
convergence	I
is	O
not	O
only	O
a	O
sufficient	O
condition	O
for	O
learnability	O
but	O
is	O
also	O
a	O
necessary	O
condition	O
this	O
is	O
not	O
the	O
case	O
for	O
more	O
general	O
learning	O
problems	O
shamir	O
srebro	O
sridharan	O
exercises	O
in	O
this	O
exercise	O
we	O
show	O
that	O
the	O
requirement	O
on	O
the	O
convergence	O
of	O
errors	O
in	O
our	O
definitions	O
of	O
pac	B
learning	O
is	O
in	O
fact	O
quite	O
close	O
to	O
a	O
simpler	O
looking	O
requirement	O
about	O
averages	O
expectations	O
prove	O
that	O
the	O
following	O
two	O
statements	O
are	O
equivalent	O
any	O
learning	O
algorithm	O
a	O
any	O
probability	O
distribution	O
d	O
and	O
any	O
loss	B
function	B
whose	O
range	O
is	O
for	O
every	O
there	O
exists	O
m	O
such	O
that	O
m	O
m	O
p	O
s	O
dm	O
lim	O
m	O
e	O
s	O
dm	O
exercises	O
es	O
dm	O
denotes	O
the	O
expectation	O
over	O
samples	O
s	O
of	O
size	O
m	O
bounded	O
loss	B
functions	O
in	O
corollary	O
we	O
assumed	O
that	O
the	O
range	O
of	O
the	O
loss	B
function	B
is	O
prove	O
that	O
if	O
the	O
range	O
of	O
the	O
loss	B
function	B
is	O
b	O
then	O
the	O
sample	B
complexity	I
satisfies	O
mh	O
much	O
the	O
bias-complexity	B
tradeoff	I
in	O
chapter	O
we	O
saw	O
that	O
unless	O
one	O
is	O
careful	O
the	O
training	O
data	O
can	O
mislead	O
the	O
learner	O
and	O
result	O
in	O
overfitting	B
to	O
overcome	O
this	O
problem	O
we	O
restricted	O
the	O
search	O
space	O
to	O
some	O
hypothesis	B
class	I
h	O
such	O
a	O
hypothesis	B
class	I
can	O
be	O
viewed	O
as	O
reflecting	O
some	O
prior	B
knowledge	I
that	O
the	O
learner	O
has	O
about	O
the	O
task	O
a	O
belief	O
that	O
one	O
of	O
the	O
members	O
of	O
the	O
class	O
h	O
is	O
a	O
low-error	O
model	O
for	O
the	O
task	O
for	O
example	O
in	O
our	O
papayas	O
taste	O
problem	O
on	O
the	O
basis	O
of	O
our	O
previous	O
experience	O
with	O
other	O
fruits	O
we	O
may	O
assume	O
that	O
some	O
rectangle	O
in	O
the	O
color-hardness	O
plane	O
predicts	O
least	O
approximately	O
the	O
papaya	O
s	O
tastiness	O
is	O
such	O
prior	B
knowledge	I
really	O
necessary	O
for	O
the	O
success	O
of	O
learning	O
maybe	O
there	O
exists	O
some	O
kind	O
of	O
universal	O
learner	O
that	O
is	O
a	O
learner	O
who	O
has	O
no	O
prior	B
knowledge	I
about	O
a	O
certain	O
task	O
and	O
is	O
ready	O
to	O
be	O
challenged	O
by	O
any	O
task	O
let	O
us	O
elaborate	O
on	O
this	O
point	O
a	O
specific	O
learning	O
task	O
is	O
defined	O
by	O
an	O
unknown	O
distribution	O
d	O
over	O
x	O
y	O
where	O
the	O
goal	O
of	O
the	O
learner	O
is	O
to	O
find	O
a	O
predictor	B
h	O
x	O
y	O
whose	O
risk	B
ldh	O
is	O
small	O
enough	O
the	O
question	O
is	O
therefore	O
whether	O
there	O
exist	O
a	O
learning	O
algorithm	O
a	O
and	O
a	O
training	B
set	B
size	O
m	O
such	O
that	O
for	O
every	O
distribution	O
d	O
if	O
a	O
receives	O
m	O
i	O
i	O
d	O
examples	O
from	O
d	O
there	O
is	O
a	O
high	O
chance	O
it	O
outputs	O
a	O
predictor	B
h	O
that	O
has	O
a	O
low	O
risk	B
the	O
first	O
part	O
of	O
this	O
chapter	O
addresses	O
this	O
question	O
formally	O
the	O
no-freelunch	O
theorem	O
states	O
that	O
no	O
such	O
universal	O
learner	O
exists	O
to	O
be	O
more	O
precise	O
the	O
theorem	O
states	O
that	O
for	O
binary	O
classification	O
prediction	O
tasks	O
for	O
every	O
learner	O
there	O
exists	O
a	O
distribution	O
on	O
which	O
it	O
fails	O
we	O
say	O
that	O
the	O
learner	O
fails	O
if	O
upon	O
receiving	O
i	O
i	O
d	O
examples	O
from	O
that	O
distribution	O
its	O
output	O
hypothesis	B
is	O
likely	O
to	O
have	O
a	O
large	O
risk	B
say	O
whereas	O
for	O
the	O
same	O
distribution	O
there	O
exists	O
another	O
learner	O
that	O
will	O
output	O
a	O
hypothesis	B
with	O
a	O
small	O
risk	B
in	O
other	O
words	O
the	O
theorem	O
states	O
that	O
no	O
learner	O
can	O
succeed	O
on	O
all	O
learnable	O
tasks	O
every	O
learner	O
has	O
tasks	O
on	O
which	O
it	O
fails	O
while	O
other	O
learners	O
succeed	O
therefore	O
when	O
approaching	O
a	O
particular	O
learning	O
problem	O
defined	O
by	O
some	O
distribution	O
d	O
we	O
should	O
have	O
some	O
prior	B
knowledge	I
on	O
d	O
one	O
type	O
of	O
such	O
prior	B
knowledge	I
is	O
that	O
d	O
comes	O
from	O
some	O
specific	O
parametric	O
family	O
of	O
distributions	O
we	O
will	O
study	O
learning	O
under	O
such	O
assumptions	O
later	O
on	O
in	O
chapter	O
another	O
type	O
of	O
prior	B
knowledge	I
on	O
d	O
which	O
we	O
assumed	O
when	O
defining	O
the	O
pac	B
learning	O
model	O
is	O
that	O
there	O
exists	O
h	O
in	O
some	O
predefined	O
hypothesis	B
class	I
h	O
such	O
that	O
ldh	O
a	O
softer	O
type	O
of	O
prior	B
knowledge	I
on	O
d	O
is	O
assuming	O
that	O
minh	O
h	O
ldh	O
is	O
small	O
in	O
a	O
sense	O
this	O
weaker	O
assumption	O
on	O
d	O
is	O
a	O
prerequisite	O
for	O
using	O
the	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
the	O
no-free-lunch	B
theorem	O
agnostic	B
pac	B
model	O
in	O
which	O
we	O
require	O
that	O
the	O
risk	B
of	O
the	O
output	O
hypothesis	B
will	O
not	O
be	O
much	O
larger	O
than	O
minh	O
h	O
ldh	O
in	O
the	O
second	O
part	O
of	O
this	O
chapter	O
we	O
study	O
the	O
benefits	O
and	O
pitfalls	O
of	O
using	O
a	O
hypothesis	B
class	I
as	O
a	O
means	O
of	O
formalizing	O
prior	B
knowledge	I
we	O
decompose	O
the	O
error	O
of	O
an	O
erm	B
algorithm	O
over	O
a	O
class	O
h	O
into	O
two	O
components	O
the	O
first	O
component	O
reflects	O
the	O
quality	O
of	O
our	O
prior	B
knowledge	I
measured	O
by	O
the	O
minimal	O
risk	B
of	O
a	O
hypothesis	B
in	O
our	O
hypothesis	B
class	I
minh	O
h	O
ldh	O
this	O
component	O
is	O
also	O
called	O
the	O
approximation	B
error	I
or	O
the	O
bias	B
of	O
the	O
algorithm	O
toward	O
choosing	O
a	O
hypothesis	B
from	O
h	O
the	O
second	O
component	O
is	O
the	O
error	O
due	O
to	O
overfitting	B
which	O
depends	O
on	O
the	O
size	O
or	O
the	O
complexity	O
of	O
the	O
class	O
h	O
and	O
is	O
called	O
the	O
estimation	B
error	I
these	O
two	O
terms	O
imply	O
a	O
tradeoff	O
between	O
choosing	O
a	O
more	O
complex	O
h	O
can	O
decrease	O
the	O
bias	B
but	O
increases	O
the	O
risk	B
of	O
overfitting	B
or	O
a	O
less	O
complex	O
h	O
might	O
increase	O
the	O
bias	B
but	O
decreases	O
the	O
potential	O
overfitting	B
the	O
no-free-lunch	B
theorem	O
in	O
this	O
part	O
we	O
prove	O
that	O
there	O
is	O
no	O
universal	O
learner	O
we	O
do	O
this	O
by	O
showing	O
that	O
no	O
learner	O
can	O
succeed	O
on	O
all	O
learning	O
tasks	O
as	O
formalized	O
in	O
the	O
following	O
theorem	O
theorem	O
let	O
a	O
be	O
any	O
learning	O
algorithm	O
for	O
the	O
task	O
of	O
binary	O
classification	O
with	O
respect	O
to	O
the	O
loss	B
over	O
a	O
domain	B
x	O
let	O
m	O
be	O
any	O
number	O
smaller	O
than	O
representing	O
a	O
training	B
set	B
size	O
then	O
there	O
exists	O
a	O
distribution	O
d	O
over	O
x	O
such	O
that	O
there	O
exists	O
a	O
function	B
f	O
x	O
with	O
ldf	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
we	O
have	O
that	O
ldas	O
this	O
theorem	O
states	O
that	O
for	O
every	O
learner	O
there	O
exists	O
a	O
task	O
on	O
which	O
it	O
fails	O
even	O
though	O
that	O
task	O
can	O
be	O
successfully	O
learned	O
by	O
another	O
learner	O
indeed	O
a	O
trivial	O
successful	O
learner	O
in	O
this	O
case	O
would	O
be	O
an	O
erm	B
learner	O
with	O
the	O
hypothesis	B
class	I
h	O
or	O
more	O
generally	O
erm	B
with	O
respect	O
to	O
any	O
finite	O
hypothesis	B
class	I
that	O
contains	O
f	O
and	O
whose	O
size	O
satisfies	O
the	O
equation	O
m	O
corollary	O
proof	O
let	O
c	O
be	O
a	O
subset	O
of	O
x	O
of	O
size	O
the	O
intuition	O
of	O
the	O
proof	O
is	O
that	O
any	O
learning	O
algorithm	O
that	O
observes	O
only	O
half	O
of	O
the	O
instances	O
in	O
c	O
has	O
no	O
information	O
on	O
what	O
should	O
be	O
the	O
labels	O
of	O
the	O
rest	O
of	O
the	O
instances	O
in	O
c	O
therefore	O
there	O
exists	O
a	O
reality	O
that	O
is	O
some	O
target	O
function	B
f	O
that	O
would	O
contradict	O
the	O
labels	O
that	O
as	O
predicts	O
on	O
the	O
unobserved	O
instances	O
in	O
c	O
note	O
that	O
there	O
are	O
t	O
possible	O
functions	O
from	O
c	O
to	O
denote	O
these	O
functions	O
by	O
ft	O
for	O
each	O
such	O
function	B
let	O
di	O
be	O
a	O
distribution	O
over	O
the	O
bias-complexity	B
tradeoff	I
c	O
defined	O
by	O
dix	O
y	O
if	O
y	O
fix	O
otherwise	O
that	O
is	O
the	O
probability	O
to	O
choose	O
a	O
pair	O
y	O
is	O
if	O
the	O
label	B
y	O
is	O
indeed	O
the	O
true	O
label	B
according	O
to	O
fi	O
and	O
the	O
probability	O
is	O
if	O
y	O
fix	O
clearly	O
ldifi	O
we	O
will	O
show	O
that	O
for	O
every	O
algorithm	O
a	O
that	O
receives	O
a	O
training	B
set	B
of	O
m	O
examples	O
from	O
c	O
and	O
returns	O
a	O
function	B
as	O
c	O
it	O
holds	O
that	O
max	O
i	O
e	O
s	O
dm	O
i	O
clearly	O
this	O
means	O
that	O
for	O
every	O
algorithm	O
that	O
receives	O
a	O
training	B
set	B
of	O
m	O
examples	O
from	O
x	O
there	O
exist	O
a	O
function	B
f	O
x	O
and	O
a	O
distribution	O
d	O
over	O
x	O
such	O
that	O
ldf	O
and	O
e	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
preceding	O
suffices	O
for	O
showing	O
that	O
which	O
is	O
what	O
we	O
need	O
to	O
prove	O
exercise	O
s	O
dm	O
we	O
now	O
turn	O
to	O
proving	O
that	O
equation	O
holds	O
there	O
are	O
k	O
possible	O
sequences	O
of	O
m	O
examples	O
from	O
c	O
denote	O
these	O
sequences	O
by	O
sk	O
also	O
if	O
sj	O
xm	O
we	O
denote	O
by	O
si	O
j	O
the	O
sequence	O
containing	O
the	O
instances	O
in	O
sj	O
labeled	O
by	O
the	O
function	B
fi	O
namely	O
si	O
j	O
fixm	O
if	O
the	O
distribution	O
is	O
di	O
then	O
the	O
possible	O
training	O
sets	O
a	O
can	O
receive	O
are	O
si	O
si	O
k	O
and	O
all	O
these	O
training	O
sets	O
have	O
the	O
same	O
probability	O
of	O
being	O
sampled	O
therefore	O
e	O
s	O
dm	O
i	O
k	O
ldiasi	O
j	O
using	O
the	O
facts	O
that	O
maximum	O
is	O
larger	O
than	O
average	O
and	O
that	O
average	O
is	O
larger	O
than	O
minimum	O
we	O
have	O
k	O
t	O
min	O
j	O
t	O
max	O
i	O
k	O
ldiasi	O
j	O
t	O
k	O
ldiasi	O
j	O
ldiasi	O
j	O
ldiasi	O
j	O
next	O
fix	O
some	O
j	O
denote	O
sj	O
xm	O
and	O
let	O
vp	O
be	O
the	O
examples	O
in	O
c	O
that	O
do	O
not	O
appear	O
in	O
sj	O
clearly	O
p	O
m	O
therefore	O
for	O
every	O
the	O
no-free-lunch	B
theorem	O
function	B
h	O
c	O
and	O
every	O
i	O
we	O
have	O
ldih	O
x	O
c	O
hence	O
t	O
ldiasi	O
j	O
t	O
t	O
j	O
j	O
min	O
r	O
t	O
j	O
next	O
fix	O
some	O
r	O
we	O
can	O
partition	O
all	O
the	O
functions	O
in	O
ft	O
into	O
t	O
disjoint	O
pairs	O
where	O
for	O
a	O
pair	O
we	O
have	O
that	O
for	O
every	O
c	O
c	O
fic	O
if	O
and	O
only	O
if	O
c	O
vr	O
since	O
for	O
such	O
a	O
pair	O
we	O
must	O
have	O
si	O
j	O
it	O
follows	O
that	O
j	O
j	O
j	O
which	O
yields	O
t	O
j	O
combining	O
this	O
with	O
equation	O
equation	O
and	O
equation	O
we	O
obtain	O
that	O
equation	O
holds	O
which	O
concludes	O
our	O
proof	O
no-free-lunch	B
and	O
prior	B
knowledge	I
how	O
does	O
the	O
no-free-lunch	B
result	O
relate	O
to	O
the	O
need	O
for	O
prior	B
knowledge	I
let	O
us	O
consider	O
an	O
erm	B
predictor	B
over	O
the	O
hypothesis	B
class	I
h	O
of	O
all	O
the	O
functions	O
f	O
from	O
x	O
to	O
this	O
class	O
represents	O
lack	O
of	O
prior	B
knowledge	I
every	O
possible	O
function	B
from	O
the	O
domain	B
to	O
the	O
label	B
set	B
is	O
considered	O
a	O
good	O
candidate	O
according	O
to	O
the	O
no-free-lunch	B
theorem	O
any	O
algorithm	O
that	O
chooses	O
its	O
output	O
from	O
hypotheses	O
in	O
h	O
and	O
in	O
particular	O
the	O
erm	B
predictor	B
will	O
fail	O
on	O
some	O
learning	O
task	O
therefore	O
this	O
class	O
is	O
not	O
pac	B
learnable	O
as	O
formalized	O
in	O
the	O
following	O
corollary	O
corollary	O
let	O
x	O
be	O
an	O
infinite	O
domain	B
set	B
and	O
let	O
h	O
be	O
the	O
set	B
of	O
all	O
functions	O
from	O
x	O
to	O
then	O
h	O
is	O
not	O
pac	B
learnable	O
the	O
bias-complexity	B
tradeoff	I
proof	O
assume	O
by	O
way	O
of	O
contradiction	O
that	O
the	O
class	O
is	O
learnable	O
choose	O
some	O
and	O
by	O
the	O
definition	O
of	O
pac	B
learnability	O
there	O
must	O
be	O
some	O
learning	O
algorithm	O
a	O
and	O
an	O
integer	O
m	O
m	O
such	O
that	O
for	O
any	O
data-generating	O
distribution	O
over	O
x	O
if	O
for	O
some	O
function	B
f	O
x	O
ldf	O
then	O
with	O
probability	O
greater	O
than	O
when	O
a	O
is	O
applied	O
to	O
samples	O
s	O
of	O
size	O
m	O
generated	O
i	O
i	O
d	O
by	O
d	O
ldas	O
however	O
applying	O
the	O
no-free-lunch	B
theorem	O
since	O
for	O
every	O
learning	O
algorithm	O
in	O
particular	O
for	O
the	O
algorithm	O
a	O
there	O
exists	O
a	O
distribution	O
d	O
such	O
that	O
with	O
probability	O
greater	O
than	O
ldas	O
which	O
leads	O
to	O
the	O
desired	O
contradiction	O
how	O
can	O
we	O
prevent	O
such	O
failures	O
we	O
can	O
escape	O
the	O
hazards	O
foreseen	O
by	O
the	O
no-free-lunch	B
theorem	O
by	O
using	O
our	O
prior	B
knowledge	I
about	O
a	O
specific	O
learning	O
task	O
to	O
avoid	O
the	O
distributions	O
that	O
will	O
cause	O
us	O
to	O
fail	O
when	O
learning	O
that	O
task	O
such	O
prior	B
knowledge	I
can	O
be	O
expressed	O
by	O
restricting	O
our	O
hypothesis	B
class	I
but	O
how	O
should	O
we	O
choose	O
a	O
good	O
hypothesis	B
class	I
on	O
the	O
one	O
hand	O
we	O
want	O
to	O
believe	O
that	O
this	O
class	O
includes	O
the	O
hypothesis	B
that	O
has	O
no	O
error	O
at	O
all	O
the	O
pac	B
setting	O
or	O
at	O
least	O
that	O
the	O
smallest	O
error	O
achievable	O
by	O
a	O
hypothesis	B
from	O
this	O
class	O
is	O
indeed	O
rather	O
small	O
the	O
agnostic	O
setting	O
on	O
the	O
other	O
hand	O
we	O
have	O
just	O
seen	O
that	O
we	O
cannot	O
simply	O
choose	O
the	O
richest	O
class	O
the	O
class	O
of	O
all	O
functions	O
over	O
the	O
given	O
domain	B
this	O
tradeoff	O
is	O
discussed	O
in	O
the	O
following	O
section	O
error	B
decomposition	I
to	O
answer	O
this	O
question	O
we	O
decompose	O
the	O
error	O
of	O
an	O
ermh	O
predictor	B
into	O
two	O
components	O
as	O
follows	O
let	O
hs	O
be	O
an	O
ermh	O
hypothesis	B
then	O
we	O
can	O
write	O
ldhs	O
where	O
min	O
h	O
h	O
ldh	O
ldhs	O
the	O
approximation	B
error	I
the	O
minimum	O
risk	B
achievable	O
by	O
a	O
predictor	B
in	O
the	O
hypothesis	B
class	I
this	O
term	O
measures	O
how	O
much	O
risk	B
we	O
have	O
because	O
we	O
restrict	O
ourselves	O
to	O
a	O
specific	O
class	O
namely	O
how	O
much	O
inductive	O
bias	B
we	O
have	O
the	O
approximation	B
error	I
does	O
not	O
depend	O
on	O
the	O
sample	O
size	O
and	O
is	O
determined	O
by	O
the	O
hypothesis	B
class	I
chosen	O
enlarging	O
the	O
hypothesis	B
class	I
can	O
decrease	O
the	O
approximation	B
error	I
under	O
the	O
realizability	B
assumption	O
the	O
approximation	B
error	I
is	O
zero	O
in	O
the	O
agnostic	O
case	O
however	O
the	O
approximation	B
error	I
can	O
be	O
in	O
fact	O
it	O
always	O
includes	O
the	O
error	O
of	O
the	O
bayes	B
optimal	I
predictor	B
chapter	O
the	O
minimal	O
yet	O
inevitable	O
error	O
because	O
of	O
the	O
possible	O
nondeterminism	O
of	O
the	O
world	O
in	O
this	O
model	O
sometimes	O
in	O
the	O
literature	O
the	O
term	O
approximation	B
error	I
refers	O
not	O
to	O
minh	O
h	O
ldh	O
but	O
rather	O
to	O
the	O
excess	O
error	O
over	O
that	O
of	O
the	O
bayes	B
optimal	I
predictor	B
namely	O
minh	O
h	O
ldh	O
summary	O
the	O
estimation	B
error	I
the	O
difference	O
between	O
the	O
approximation	B
error	I
and	O
the	O
error	O
achieved	O
by	O
the	O
erm	B
predictor	B
the	O
estimation	B
error	I
results	O
because	O
the	O
empirical	B
risk	B
training	B
error	I
is	O
only	O
an	O
estimate	O
of	O
the	O
true	O
risk	B
and	O
so	O
the	O
predictor	B
minimizing	O
the	O
empirical	B
risk	B
is	O
only	O
an	O
estimate	O
of	O
the	O
predictor	B
minimizing	O
the	O
true	O
risk	B
the	O
quality	O
of	O
this	O
estimation	O
depends	O
on	O
the	O
training	B
set	B
size	O
and	O
on	O
the	O
size	O
or	O
complexity	O
of	O
the	O
hypothesis	B
class	I
as	O
we	O
have	O
shown	O
for	O
a	O
finite	O
hypothesis	B
class	I
increases	O
with	O
and	O
decreases	O
with	O
m	O
we	O
can	O
think	O
of	O
the	O
size	O
of	O
h	O
as	O
a	O
measure	O
of	O
its	O
complexity	O
in	O
future	O
chapters	O
we	O
will	O
define	O
other	O
complexity	O
measures	O
of	O
hypothesis	B
classes	O
since	O
our	O
goal	O
is	O
to	O
minimize	O
the	O
total	O
risk	B
we	O
face	O
a	O
tradeoff	O
called	O
the	O
biascomplexity	O
tradeoff	O
on	O
one	O
hand	O
choosing	O
h	O
to	O
be	O
a	O
very	O
rich	O
class	O
decreases	O
the	O
approximation	B
error	I
but	O
at	O
the	O
same	O
time	O
might	O
increase	O
the	O
estimation	B
error	I
as	O
a	O
rich	O
h	O
might	O
lead	O
to	O
overfitting	B
on	O
the	O
other	O
hand	O
choosing	O
h	O
to	O
be	O
a	O
very	O
small	O
set	B
reduces	O
the	O
estimation	B
error	I
but	O
might	O
increase	O
the	O
approximation	B
error	I
or	O
in	O
other	O
words	O
might	O
lead	O
to	O
underfitting	B
of	O
course	O
a	O
great	O
choice	O
for	O
h	O
is	O
the	O
class	O
that	O
contains	O
only	O
one	O
classifier	B
the	O
bayes	B
optimal	I
classifier	B
but	O
the	O
bayes	B
optimal	I
classifier	B
depends	O
on	O
the	O
underlying	O
distribution	O
d	O
which	O
we	O
do	O
not	O
know	O
learning	O
would	O
have	O
been	O
unnecessary	O
had	O
we	O
known	O
d	O
learning	O
theory	O
studies	O
how	O
rich	O
we	O
can	O
make	O
h	O
while	O
still	O
maintaining	O
reasonable	O
estimation	B
error	I
in	O
many	O
cases	O
empirical	O
research	O
focuses	O
on	O
designing	O
good	O
hypothesis	B
classes	O
for	O
a	O
certain	O
domain	B
here	O
good	O
means	O
classes	O
for	O
which	O
the	O
approximation	B
error	I
would	O
not	O
be	O
excessively	O
high	O
the	O
idea	O
is	O
that	O
although	O
we	O
are	O
not	O
experts	O
and	O
do	O
not	O
know	O
how	O
to	O
construct	O
the	O
optimal	O
classifier	B
we	O
still	O
have	O
some	O
prior	B
knowledge	I
of	O
the	O
specific	O
problem	O
at	O
hand	O
which	O
enables	O
us	O
to	O
design	O
hypothesis	B
classes	O
for	O
which	O
both	O
the	O
approximation	B
error	I
and	O
the	O
estimation	B
error	I
are	O
not	O
too	O
large	O
getting	O
back	O
to	O
our	O
papayas	O
example	O
we	O
do	O
not	O
know	O
how	O
exactly	O
the	O
color	O
and	O
hardness	O
of	O
a	O
papaya	O
predict	O
its	O
taste	O
but	O
we	O
do	O
know	O
that	O
papaya	O
is	O
a	O
fruit	O
and	O
on	O
the	O
basis	O
of	O
previous	O
experience	O
with	O
other	O
fruit	O
we	O
conjecture	O
that	O
a	O
rectangle	O
in	O
the	O
color-hardness	O
space	O
may	O
be	O
a	O
good	O
predictor	B
summary	O
the	O
no-free-lunch	B
theorem	O
states	O
that	O
there	O
is	O
no	O
universal	O
learner	O
every	O
learner	O
has	O
to	O
be	O
specified	O
to	O
some	O
task	O
and	O
use	O
some	O
prior	B
knowledge	I
about	O
that	O
task	O
in	O
order	O
to	O
succeed	O
so	O
far	O
we	O
have	O
modeled	O
our	O
prior	B
knowledge	I
by	O
restricting	O
our	O
output	O
hypothesis	B
to	O
be	O
a	O
member	O
of	O
a	O
chosen	O
hypothesis	B
class	I
when	O
choosing	O
this	O
hypothesis	B
class	I
we	O
face	O
a	O
tradeoff	O
between	O
a	O
larger	O
or	O
more	O
complex	O
class	O
that	O
is	O
more	O
likely	O
to	O
have	O
a	O
small	O
approximation	B
error	I
and	O
a	O
more	O
restricted	O
class	O
that	O
would	O
guarantee	O
that	O
the	O
estimation	B
error	I
will	O
the	O
bias-complexity	B
tradeoff	I
be	O
small	O
in	O
the	O
next	O
chapter	O
we	O
will	O
study	O
in	O
more	O
detail	O
the	O
behavior	O
of	O
the	O
estimation	B
error	I
in	O
chapter	O
we	O
will	O
discuss	O
alternative	O
ways	O
to	O
express	O
prior	B
knowledge	I
bibliographic	O
remarks	O
macready	O
proved	O
several	O
no-free-lunch	B
theorems	O
for	O
optimization	O
but	O
these	O
are	O
rather	O
different	O
from	O
the	O
theorem	O
we	O
prove	O
here	O
the	O
theorem	O
we	O
prove	O
here	O
is	O
closely	O
related	O
to	O
lower	O
bounds	O
in	O
vc	O
theory	O
as	O
we	O
will	O
study	O
in	O
the	O
next	O
chapter	O
exercises	O
prove	O
that	O
equation	O
suffices	O
for	O
showing	O
that	O
pldas	O
hint	O
let	O
be	O
a	O
random	O
variable	O
that	O
receives	O
values	O
in	O
and	O
whose	O
expectation	O
satisfies	O
e	O
use	O
lemma	O
to	O
show	O
that	O
p	O
assume	O
you	O
are	O
asked	O
to	O
design	O
a	O
learning	O
algorithm	O
to	O
predict	O
whether	O
patients	O
are	O
going	O
to	O
suffer	O
a	O
heart	O
attack	O
relevant	O
patient	O
features	O
the	O
algorithm	O
may	O
have	O
access	O
to	O
include	O
blood	O
pressure	O
body-mass	O
index	O
age	O
level	O
of	O
physical	O
activity	O
and	O
income	O
your	O
choice	O
you	O
have	O
to	O
choose	O
between	O
two	O
algorithms	O
the	O
first	O
picks	O
an	O
axis	O
aligned	O
rectangle	O
in	O
the	O
two	O
dimensional	O
space	O
spanned	O
by	O
the	O
features	O
bp	O
and	O
bmi	O
and	O
the	O
other	O
picks	O
an	O
axis	O
aligned	O
rectangle	O
in	O
the	O
five	O
dimensional	O
space	O
spanned	O
by	O
all	O
the	O
preceding	O
features	O
explain	O
the	O
pros	O
and	O
cons	O
of	O
each	O
choice	O
explain	O
how	O
the	O
number	O
of	O
available	O
labeled	O
training	O
samples	O
will	O
affect	O
prove	O
that	O
if	O
km	O
for	O
a	O
positive	O
integer	O
k	O
then	O
we	O
can	O
replace	O
the	O
lower	O
bound	O
of	O
in	O
the	O
no-free-lunch	B
theorem	O
with	O
k	O
namely	O
let	O
a	O
be	O
a	O
learning	O
algorithm	O
for	O
the	O
task	O
of	O
binary	O
classification	O
let	O
m	O
be	O
any	O
number	O
smaller	O
than	O
representing	O
a	O
training	B
set	B
size	O
then	O
there	O
exists	O
a	O
distribution	O
d	O
over	O
x	O
such	O
that	O
there	O
exists	O
a	O
function	B
f	O
x	O
with	O
ldf	O
es	O
dmldas	O
the	O
vc-dimension	O
in	O
the	O
previous	O
chapter	O
we	O
decomposed	O
the	O
error	O
of	O
the	O
ermh	O
rule	O
into	O
approximation	B
error	I
and	O
estimation	B
error	I
the	O
approximation	B
error	I
depends	O
on	O
the	O
fit	O
of	O
our	O
prior	B
knowledge	I
reflected	O
by	O
the	O
choice	O
of	O
the	O
hypothesis	B
class	I
h	O
to	O
the	O
underlying	O
unknown	O
distribution	O
in	O
contrast	O
the	O
definition	O
of	O
pac	B
learnability	O
requires	O
that	O
the	O
estimation	B
error	I
would	O
be	O
bounded	O
uniformly	O
over	O
all	O
distributions	O
our	O
current	O
goal	O
is	O
to	O
figure	O
out	O
which	O
classes	O
h	O
are	O
pac	B
learnable	O
and	O
to	O
characterize	O
exactly	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
given	O
hypothesis	B
class	I
so	O
far	O
we	O
have	O
seen	O
that	O
finite	O
classes	O
are	O
learnable	O
but	O
that	O
the	O
class	O
of	O
all	O
functions	O
an	O
infinite	O
size	O
domain	B
is	O
not	O
what	O
makes	O
one	O
class	O
learnable	O
and	O
the	O
other	O
unlearnable	O
can	O
infinite-size	O
classes	O
be	O
learnable	O
and	O
if	O
so	O
what	O
determines	O
their	O
sample	B
complexity	I
we	O
begin	O
the	O
chapter	O
by	O
showing	O
that	O
infinite	O
classes	O
can	O
indeed	O
be	O
learnable	O
and	O
thus	O
finiteness	O
of	O
the	O
hypothesis	B
class	I
is	O
not	O
a	O
necessary	O
condition	O
for	O
learnability	O
we	O
then	O
present	O
a	O
remarkably	O
crisp	O
characterization	O
of	O
the	O
family	O
of	O
learnable	O
classes	O
in	O
the	O
setup	O
of	O
binary	O
valued	O
classification	O
with	O
the	O
zero-one	O
loss	B
this	O
characterization	O
was	O
first	O
discovered	O
by	O
vladimir	O
vapnik	O
and	O
alexey	O
chervonenkis	O
in	O
and	O
relies	O
on	O
a	O
combinatorial	O
notion	O
called	O
the	O
vapnikchervonenkis	O
dimension	O
we	O
formally	O
define	O
the	O
vc-dimension	O
provide	O
several	O
examples	O
and	O
then	O
state	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
theory	O
which	O
integrates	O
the	O
concepts	O
of	O
learnability	O
vc-dimension	O
the	O
erm	B
rule	O
and	O
uniform	B
convergence	I
infinite-size	O
classes	O
can	O
be	O
learnable	O
in	O
chapter	O
we	O
saw	O
that	O
finite	O
classes	O
are	O
learnable	O
and	O
in	O
fact	O
the	O
sample	B
complexity	I
of	O
a	O
hypothesis	B
class	I
is	O
upper	O
bounded	O
by	O
the	O
log	O
of	O
its	O
size	O
to	O
show	O
that	O
the	O
size	O
of	O
the	O
hypothesis	B
class	I
is	O
not	O
the	O
right	O
characterization	O
of	O
its	O
sample	B
complexity	I
we	O
first	O
present	O
a	O
simple	O
example	O
of	O
an	O
infinite-size	O
hypothesis	B
class	I
that	O
is	O
learnable	O
example	O
let	O
h	O
be	O
the	O
set	B
of	O
threshold	O
functions	O
over	O
the	O
real	O
line	O
namely	O
h	O
a	O
r	O
where	O
ha	O
r	O
is	O
a	O
function	B
such	O
that	O
hax	O
to	O
remind	O
the	O
reader	O
is	O
if	O
x	O
a	O
and	O
otherwise	O
clearly	O
h	O
is	O
of	O
infinite	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
the	O
vc-dimension	O
size	O
nevertheless	O
the	O
following	O
lemma	O
shows	O
that	O
h	O
is	O
learnable	O
in	O
the	O
pac	B
model	O
using	O
the	O
erm	B
algorithm	O
lemma	O
let	O
h	O
be	O
the	O
class	O
of	O
thresholds	O
as	O
defined	O
earlier	O
then	O
h	O
is	O
pac	B
learnable	O
using	O
the	O
erm	B
rule	O
with	O
sample	B
complexity	I
of	O
mh	O
proof	O
let	O
be	O
a	O
threshold	O
such	O
that	O
the	O
hypothesis	B
achieves	O
let	O
dx	O
be	O
the	O
marginal	O
distribution	O
over	O
the	O
domain	B
x	O
and	O
let	O
be	O
such	O
that	O
p	O
x	O
dx	O
p	O
x	O
dx	O
mass	O
mass	O
dx	O
we	O
set	B
and	O
similarly	O
for	O
given	O
a	O
training	B
set	B
s	O
let	O
maxx	O
s	O
and	O
minx	O
s	O
no	O
example	O
in	O
s	O
is	O
positive	O
we	O
set	B
and	O
if	O
no	O
example	O
in	O
s	O
is	O
negative	O
we	O
set	B
let	O
bs	O
be	O
a	O
threshold	O
corresponding	O
to	O
an	O
erm	B
hypothesis	B
hs	O
which	O
implies	O
that	O
bs	O
therefore	O
a	O
sufficient	O
condition	O
for	O
ldhs	O
is	O
that	O
both	O
and	O
in	O
other	O
words	O
p	O
s	O
dm	O
p	O
s	O
dm	O
and	O
using	O
the	O
union	B
bound	I
we	O
can	O
bound	O
the	O
preceding	O
by	O
p	O
s	O
dm	O
p	O
s	O
dm	O
p	O
s	O
dm	O
the	O
event	O
happens	O
if	O
and	O
only	O
if	O
all	O
examples	O
in	O
s	O
are	O
not	O
in	O
the	O
interval	O
a	O
whose	O
probability	O
mass	O
is	O
defined	O
to	O
be	O
namely	O
p	O
s	O
dm	O
p	O
s	O
dm	O
y	O
s	O
x	O
e	O
m	O
since	O
we	O
assume	O
m	O
it	O
follows	O
that	O
the	O
equation	O
is	O
at	O
most	O
in	O
the	O
same	O
way	O
it	O
is	O
easy	O
to	O
see	O
that	O
ps	O
dm	O
combining	O
with	O
equation	O
we	O
conclude	O
our	O
proof	O
the	O
vc-dimension	O
we	O
see	O
therefore	O
that	O
while	O
finiteness	O
of	O
h	O
is	O
a	O
sufficient	O
condition	O
for	O
learnability	O
it	O
is	O
not	O
a	O
necessary	O
condition	O
as	O
we	O
will	O
show	O
a	O
property	O
called	O
the	O
vc-dimension	O
of	O
a	O
hypothesis	B
class	I
gives	O
the	O
correct	O
characterization	O
of	O
its	O
learnability	O
to	O
motivate	O
the	O
definition	O
of	O
the	O
vc-dimension	O
let	O
us	O
recall	B
the	O
no-freelunch	O
theorem	O
and	O
its	O
proof	O
there	O
we	O
have	O
shown	O
that	O
without	O
the	O
vc-dimension	O
restricting	O
the	O
hypothesis	B
class	I
for	O
any	O
learning	O
algorithm	O
an	O
adversary	O
can	O
construct	O
a	O
distribution	O
for	O
which	O
the	O
learning	O
algorithm	O
will	O
perform	O
poorly	O
while	O
there	O
is	O
another	O
learning	O
algorithm	O
that	O
will	O
succeed	O
on	O
the	O
same	O
distribution	O
to	O
do	O
so	O
the	O
adversary	O
used	O
a	O
finite	O
set	B
c	O
x	O
and	O
considered	O
a	O
family	O
of	O
distributions	O
that	O
are	O
concentrated	O
on	O
elements	O
of	O
c	O
each	O
distribution	O
was	O
derived	O
from	O
a	O
true	O
target	O
function	B
from	O
c	O
to	O
to	O
make	O
any	O
algorithm	O
fail	O
the	O
adversary	O
used	O
the	O
power	O
of	O
choosing	O
a	O
target	O
function	B
from	O
the	O
set	B
of	O
all	O
possible	O
functions	O
from	O
c	O
to	O
when	O
considering	O
pac	B
learnability	O
of	O
a	O
hypothesis	B
class	I
h	O
the	O
adversary	O
is	O
restricted	O
to	O
constructing	O
distributions	O
for	O
which	O
some	O
hypothesis	B
h	O
h	O
achieves	O
a	O
zero	O
risk	B
since	O
we	O
are	O
considering	O
distributions	O
that	O
are	O
concentrated	O
on	O
elements	O
of	O
c	O
we	O
should	O
study	O
how	O
h	O
behaves	O
on	O
c	O
which	O
leads	O
to	O
the	O
following	O
definition	O
definition	O
of	O
h	O
to	O
c	O
let	O
h	O
be	O
a	O
class	O
of	O
functions	O
from	O
x	O
to	O
and	O
let	O
c	O
cm	O
x	O
the	O
restriction	O
of	O
h	O
to	O
c	O
is	O
the	O
set	B
of	O
functions	O
from	O
c	O
to	O
that	O
can	O
be	O
derived	O
from	O
h	O
that	O
is	O
hc	O
hcm	O
h	O
h	O
where	O
we	O
represent	O
each	O
function	B
from	O
c	O
to	O
as	O
a	O
vector	O
in	O
if	O
the	O
restriction	O
of	O
h	O
to	O
c	O
is	O
the	O
set	B
of	O
all	O
functions	O
from	O
c	O
to	O
then	O
we	O
say	O
that	O
h	O
shatters	O
the	O
set	B
c	O
formally	O
definition	O
a	O
hypothesis	B
class	I
h	O
shatters	O
a	O
finite	O
set	B
c	O
x	O
if	O
the	O
restriction	O
of	O
h	O
to	O
c	O
is	O
the	O
set	B
of	O
all	O
functions	O
from	O
c	O
to	O
that	O
is	O
example	O
let	O
h	O
be	O
the	O
class	O
of	O
threshold	O
functions	O
over	O
r	O
take	O
a	O
set	B
c	O
now	O
if	O
we	O
take	O
a	O
then	O
we	O
have	O
and	O
if	O
we	O
take	O
a	O
then	O
we	O
have	O
therefore	O
hc	O
is	O
the	O
set	B
of	O
all	O
functions	O
from	O
c	O
to	O
and	O
h	O
shatters	O
c	O
now	O
take	O
a	O
set	B
c	O
where	O
no	O
h	O
h	O
can	O
account	O
for	O
the	O
labeling	O
because	O
any	O
threshold	O
that	O
assigns	O
the	O
label	B
to	O
must	O
assign	O
the	O
label	B
to	O
as	O
well	O
therefore	O
not	O
all	O
functions	O
from	O
c	O
to	O
are	O
included	O
in	O
hc	O
hence	O
c	O
is	O
not	O
shattered	O
by	O
h	O
getting	O
back	O
to	O
the	O
construction	O
of	O
an	O
adversarial	O
distribution	O
as	O
in	O
the	O
proof	O
of	O
the	O
no-free-lunch	B
theorem	O
we	O
see	O
that	O
whenever	O
some	O
set	B
c	O
is	O
shattered	O
by	O
h	O
the	O
adversary	O
is	O
not	O
restricted	O
by	O
h	O
as	O
they	O
can	O
construct	O
a	O
distribution	O
over	O
c	O
based	O
on	O
any	O
target	O
function	B
from	O
c	O
to	O
while	O
still	O
maintaining	O
the	O
realizability	B
assumption	O
this	O
immediately	O
yields	O
corollary	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
x	O
to	O
let	O
m	O
be	O
a	O
training	B
set	B
size	O
assume	O
that	O
there	O
exists	O
a	O
set	B
c	O
x	O
of	O
size	O
that	O
is	O
shattered	O
by	O
h	O
then	O
for	O
any	O
learning	O
algorithm	O
a	O
there	O
exist	O
a	O
distribution	O
d	O
over	O
x	O
and	O
a	O
predictor	B
h	O
h	O
such	O
that	O
ldh	O
but	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
we	O
have	O
that	O
ldas	O
the	O
vc-dimension	O
corollary	O
tells	O
us	O
that	O
if	O
h	O
shatters	O
some	O
set	B
c	O
of	O
size	O
then	O
we	O
cannot	O
learn	O
h	O
using	O
m	O
examples	O
intuitively	O
if	O
a	O
set	B
c	O
is	O
shattered	O
by	O
h	O
and	O
we	O
receive	O
a	O
sample	O
containing	O
half	O
the	O
instances	O
of	O
c	O
the	O
labels	O
of	O
these	O
instances	O
give	O
us	O
no	O
information	O
about	O
the	O
labels	O
of	O
the	O
rest	O
of	O
the	O
instances	O
in	O
c	O
every	O
possible	O
labeling	O
of	O
the	O
rest	O
of	O
the	O
instances	O
can	O
be	O
explained	O
by	O
some	O
hypothesis	B
in	O
h	O
philosophically	O
if	O
someone	O
can	O
explain	O
every	O
phenomenon	O
his	O
explanations	O
are	O
worthless	O
this	O
leads	O
us	O
directly	O
to	O
the	O
definition	O
of	O
the	O
vc	B
dimension	I
definition	O
the	O
vc-dimension	O
of	O
a	O
hypothesis	B
class	I
h	O
denoted	O
vcdimh	O
is	O
the	O
maximal	O
size	O
of	O
a	O
set	B
c	O
x	O
that	O
can	O
be	O
shattered	O
by	O
h	O
if	O
h	O
can	O
shatter	O
sets	O
of	O
arbitrarily	O
large	O
size	O
we	O
say	O
that	O
h	O
has	O
infinite	O
vc-dimension	O
a	O
direct	O
consequence	O
of	O
corollary	O
is	O
therefore	O
theorem	O
let	O
h	O
be	O
a	O
class	O
of	O
infinite	O
vc-dimension	O
then	O
h	O
is	O
not	O
pac	B
learnable	O
proof	O
since	O
h	O
has	O
an	O
infinite	O
vc-dimension	O
for	O
any	O
training	B
set	B
size	O
m	O
there	O
exists	O
a	O
shattered	O
set	B
of	O
size	O
and	O
the	O
claim	O
follows	O
by	O
corollary	O
we	O
shall	O
see	O
later	O
in	O
this	O
chapter	O
that	O
the	O
converse	O
is	O
also	O
true	O
a	O
finite	O
vcdimension	O
guarantees	O
learnability	O
hence	O
the	O
vc-dimension	O
characterizes	O
pac	B
learnability	O
but	O
before	O
delving	O
into	O
more	O
theory	O
we	O
first	O
show	O
several	O
examples	O
examples	O
in	O
this	O
section	O
we	O
calculate	O
the	O
vc-dimension	O
of	O
several	O
hypothesis	B
classes	O
to	O
show	O
that	O
vcdimh	O
d	O
we	O
need	O
to	O
show	O
that	O
there	O
exists	O
a	O
set	B
c	O
of	O
size	O
d	O
that	O
is	O
shattered	O
by	O
h	O
every	O
set	B
c	O
of	O
size	O
d	O
is	O
not	O
shattered	O
by	O
h	O
threshold	O
functions	O
let	O
h	O
be	O
the	O
class	O
of	O
threshold	O
functions	O
over	O
r	O
recall	B
example	O
where	O
we	O
have	O
shown	O
that	O
for	O
an	O
arbitrary	O
set	B
c	O
h	O
shatters	O
c	O
therefore	O
vcdimh	O
we	O
have	O
also	O
shown	O
that	O
for	O
an	O
arbitrary	O
set	B
c	O
where	O
h	O
does	O
not	O
shatter	O
c	O
we	O
therefore	O
conclude	O
that	O
vcdimh	O
examples	O
intervals	O
let	O
h	O
be	O
the	O
class	O
of	O
intervals	O
over	O
r	O
namely	O
h	O
a	O
b	O
r	O
a	O
b	O
where	O
hab	O
r	O
is	O
a	O
function	B
such	O
that	O
habx	O
take	O
the	O
set	B
c	O
then	O
h	O
shatters	O
c	O
sure	O
you	O
understand	O
why	O
and	O
therefore	O
vcdimh	O
now	O
take	O
an	O
arbitrary	O
set	B
c	O
and	O
assume	O
without	O
loss	B
of	O
generality	O
that	O
then	O
the	O
labeling	O
cannot	O
be	O
obtained	O
by	O
an	O
interval	O
and	O
therefore	O
h	O
does	O
not	O
shatter	O
c	O
we	O
therefore	O
conclude	O
that	O
vcdimh	O
axis	O
aligned	O
rectangles	O
let	O
h	O
be	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
formally	O
h	O
and	O
where	O
if	O
and	O
otherwise	O
we	O
shall	O
show	O
in	O
the	O
following	O
that	O
vcdimh	O
to	O
prove	O
this	O
we	O
need	O
to	O
find	O
a	O
set	B
of	O
points	O
that	O
are	O
shattered	O
by	O
h	O
and	O
show	O
that	O
no	O
set	B
of	O
points	O
can	O
be	O
shattered	O
by	O
h	O
finding	O
a	O
set	B
of	O
points	O
that	O
are	O
shattered	O
is	O
easy	O
figure	O
now	O
consider	O
any	O
set	B
c	O
of	O
points	O
in	O
c	O
take	O
a	O
leftmost	O
point	O
first	O
coordinate	O
is	O
the	O
smallest	O
in	O
c	O
a	O
rightmost	O
point	O
coordinate	O
is	O
the	O
largest	O
a	O
lowest	O
point	O
coordinate	O
is	O
the	O
smallest	O
and	O
a	O
highest	O
point	O
coordinate	O
is	O
the	O
largest	O
without	O
loss	B
of	O
generality	O
denote	O
c	O
and	O
let	O
be	O
the	O
point	O
that	O
was	O
not	O
selected	O
now	O
define	O
the	O
labeling	O
it	O
is	O
impossible	O
to	O
obtain	O
this	O
labeling	O
by	O
an	O
axis	O
aligned	O
rectangle	O
indeed	O
such	O
a	O
rectangle	O
must	O
contain	O
but	O
in	O
this	O
case	O
the	O
rectangle	O
contains	O
as	O
well	O
because	O
its	O
coordinates	O
are	O
within	O
the	O
intervals	O
defined	O
by	O
the	O
selected	O
points	O
so	O
c	O
is	O
not	O
shattered	O
by	O
h	O
and	O
therefore	O
vcdimh	O
figure	O
left	O
points	O
that	O
are	O
shattered	O
by	O
axis	O
aligned	O
rectangles	O
right	O
any	O
axis	O
aligned	O
rectangle	O
cannot	O
label	B
by	O
and	O
the	O
rest	O
of	O
the	O
points	O
by	O
the	O
vc-dimension	O
finite	O
classes	O
let	O
h	O
be	O
a	O
finite	O
class	O
then	O
clearly	O
for	O
any	O
set	B
c	O
we	O
have	O
and	O
thus	O
c	O
cannot	O
be	O
shattered	O
if	O
this	O
implies	O
that	O
vcdimh	O
this	O
shows	O
that	O
the	O
pac	B
learnability	O
of	O
finite	O
classes	O
follows	O
from	O
the	O
more	O
general	O
statement	O
of	O
pac	B
learnability	O
of	O
classes	O
with	O
finite	O
vc-dimension	O
which	O
we	O
shall	O
see	O
in	O
the	O
next	O
section	O
note	O
however	O
that	O
the	O
vc-dimension	O
of	O
a	O
finite	O
class	O
h	O
can	O
be	O
significantly	O
smaller	O
than	O
for	O
example	O
let	O
x	O
k	O
for	O
some	O
integer	O
k	O
and	O
consider	O
the	O
class	O
of	O
threshold	O
functions	O
defined	O
in	O
example	O
then	O
k	O
but	O
vcdimh	O
since	O
k	O
can	O
be	O
arbitrarily	O
large	O
the	O
gap	O
between	O
and	O
vcdimh	O
can	O
be	O
arbitrarily	O
large	O
vc-dimension	O
and	O
the	O
number	O
of	O
parameters	O
in	O
the	O
previous	O
examples	O
the	O
vc-dimension	O
happened	O
to	O
equal	O
the	O
number	O
of	O
parameters	O
defining	O
the	O
hypothesis	B
class	I
while	O
this	O
is	O
often	O
the	O
case	O
it	O
is	O
not	O
always	O
true	O
consider	O
for	O
example	O
the	O
domain	B
x	O
r	O
and	O
the	O
hypothesis	B
class	I
h	O
r	O
where	O
h	O
x	O
is	O
defined	O
by	O
h	O
sin	O
it	O
is	O
possible	O
to	O
prove	O
that	O
vcdimh	O
namely	O
for	O
every	O
d	O
one	O
can	O
find	O
d	O
points	O
that	O
are	O
shattered	O
by	O
h	O
exercise	O
the	O
fundamental	O
theorem	O
of	O
pac	B
learning	O
we	O
have	O
already	O
shown	O
that	O
a	O
class	O
of	O
infinite	O
vc-dimension	O
is	O
not	O
learnable	O
the	O
converse	O
statement	O
is	O
also	O
true	O
leading	O
to	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
theory	O
theorem	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
a	O
domain	B
x	O
to	O
and	O
let	O
the	O
loss	B
function	B
be	O
the	O
loss	B
then	O
the	O
following	O
are	O
equivalent	O
h	O
has	O
the	O
uniform	B
convergence	I
property	O
any	O
erm	B
rule	O
is	O
a	O
successful	O
agnostic	B
pac	B
learner	O
for	O
h	O
h	O
is	O
agnostic	B
pac	B
learnable	O
h	O
is	O
pac	B
learnable	O
any	O
erm	B
rule	O
is	O
a	O
successful	O
pac	B
learner	O
for	O
h	O
h	O
has	O
a	O
finite	O
vc-dimension	O
the	O
proof	O
of	O
the	O
theorem	O
is	O
given	O
in	O
the	O
next	O
section	O
not	O
only	O
does	O
the	O
vc-dimension	O
characterize	O
pac	B
learnability	O
it	O
even	O
deter	O
mines	O
the	O
sample	B
complexity	I
theorem	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
quantitative	O
version	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
a	O
domain	B
x	O
to	O
and	O
let	O
the	O
loss	B
function	B
be	O
the	O
loss	B
assume	O
that	O
vcdimh	O
d	O
then	O
there	O
are	O
absolute	O
constants	O
such	O
that	O
proof	O
of	O
theorem	O
h	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
sample	B
complexity	I
d	O
much	O
d	O
h	O
is	O
agnostic	B
pac	B
learnable	O
with	O
sample	B
complexity	I
d	O
mh	O
d	O
h	O
is	O
pac	B
learnable	O
with	O
sample	B
complexity	I
mh	O
d	O
d	O
the	O
proof	O
of	O
this	O
theorem	O
is	O
given	O
in	O
chapter	O
remark	O
we	O
stated	O
the	O
fundamental	O
theorem	O
for	O
binary	O
classification	O
tasks	O
a	O
similar	O
result	O
holds	O
for	O
some	O
other	O
learning	O
problems	O
such	O
as	O
regression	B
with	O
the	O
absolute	O
loss	B
or	O
the	O
squared	O
loss	B
however	O
the	O
theorem	O
does	O
not	O
hold	O
for	O
all	O
learning	O
tasks	O
in	O
particular	O
learnability	O
is	O
sometimes	O
possible	O
even	O
though	O
the	O
uniform	B
convergence	I
property	O
does	O
not	O
hold	O
will	O
see	O
an	O
example	O
in	O
chapter	O
exercise	O
furthermore	O
in	O
some	O
situations	O
the	O
erm	B
rule	O
fails	O
but	O
learnability	O
is	O
possible	O
with	O
other	O
learning	O
rules	O
proof	O
of	O
theorem	O
we	O
have	O
already	O
seen	O
that	O
in	O
chapter	O
the	O
implications	O
and	O
are	O
trivial	O
and	O
so	O
is	O
the	O
implications	O
and	O
follow	O
from	O
the	O
no-free-lunch	B
theorem	O
the	O
difficult	O
part	O
is	O
to	O
show	O
that	O
the	O
proof	O
is	O
based	O
on	O
two	O
main	O
claims	O
if	O
vcdimh	O
d	O
then	O
even	O
though	O
h	O
might	O
be	O
infinite	O
when	O
restricting	O
it	O
to	O
a	O
finite	O
set	B
c	O
x	O
its	O
effective	O
size	O
is	O
only	O
ocd	O
that	O
is	O
the	O
size	O
of	O
hc	O
grows	O
polynomially	O
rather	O
than	O
exponentially	O
with	O
this	O
claim	O
is	O
often	O
referred	O
to	O
as	O
sauer	O
s	O
lemma	O
but	O
it	O
has	O
also	O
been	O
stated	O
and	O
proved	O
independently	O
by	O
shelah	O
and	O
by	O
perles	O
the	O
formal	O
statement	O
is	O
given	O
in	O
section	O
later	O
in	O
section	O
we	O
have	O
shown	O
that	O
finite	O
hypothesis	B
classes	O
enjoy	O
the	O
uniform	B
convergence	I
property	O
in	O
section	O
later	O
we	O
generalize	O
this	O
result	O
and	O
show	O
that	O
uniform	B
convergence	I
holds	O
whenever	O
the	O
hypothesis	B
class	I
has	O
a	O
small	O
effective	O
size	O
by	O
small	O
effective	O
size	O
we	O
mean	O
classes	O
for	O
which	O
grows	O
polynomially	O
with	O
sauer	O
s	O
lemma	O
and	O
the	O
growth	B
function	B
we	O
defined	O
the	O
notion	O
of	O
shattering	B
by	O
considering	O
the	O
restriction	O
of	O
h	O
to	O
a	O
finite	O
set	B
of	O
instances	O
the	O
growth	B
function	B
measures	O
the	O
maximal	O
effective	O
size	O
of	O
h	O
on	O
a	O
set	B
of	O
m	O
examples	O
formally	O
the	O
vc-dimension	O
definition	O
function	B
let	O
h	O
be	O
a	O
hypothesis	B
class	I
then	O
the	O
growth	B
function	B
of	O
h	O
denoted	O
h	O
n	O
n	O
is	O
defined	O
as	O
hm	O
max	O
c	O
x	O
in	O
words	O
h	O
is	O
the	O
number	O
of	O
different	O
functions	O
from	O
a	O
set	B
c	O
of	O
size	O
m	O
to	O
that	O
can	O
be	O
obtained	O
by	O
restricting	O
h	O
to	O
c	O
obviously	O
if	O
vcdimh	O
d	O
then	O
for	O
any	O
m	O
d	O
we	O
have	O
hm	O
in	O
such	O
cases	O
h	O
induces	O
all	O
possible	O
functions	O
from	O
c	O
to	O
the	O
following	O
beautiful	O
lemma	O
proposed	O
independently	O
by	O
sauer	O
shelah	O
and	O
perles	O
shows	O
that	O
when	O
m	O
becomes	O
larger	O
than	O
the	O
vc-dimension	O
the	O
growth	B
function	B
increases	O
polynomially	O
rather	O
than	O
exponentially	O
with	O
m	O
lemma	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
with	O
vcdimh	O
hm	O
d	O
then	O
for	O
all	O
m	O
hm	O
in	O
particular	O
if	O
m	O
d	O
then	O
i	O
proof	O
of	O
sauer	O
s	O
lemma	O
to	O
prove	O
the	O
lemma	O
it	O
suffices	O
to	O
prove	O
the	O
following	O
stronger	O
claim	O
for	O
any	O
c	O
cm	O
we	O
have	O
h	O
c	O
h	O
shatters	O
b	O
the	O
reason	O
why	O
equation	O
is	O
sufficient	O
to	O
prove	O
the	O
lemma	O
is	O
that	O
if	O
vcdimh	O
d	O
then	O
no	O
set	B
whose	O
size	O
is	O
larger	O
than	O
d	O
is	O
shattered	O
by	O
h	O
and	O
therefore	O
c	O
h	O
shatters	O
b	O
i	O
when	O
m	O
d	O
the	O
right-hand	O
side	O
of	O
the	O
preceding	O
is	O
at	O
most	O
lemma	O
in	O
appendix	O
a	O
we	O
are	O
left	O
with	O
proving	O
equation	O
and	O
we	O
do	O
it	O
using	O
an	O
inductive	O
argument	O
for	O
m	O
no	O
matter	O
what	O
h	O
is	O
either	O
both	O
sides	O
of	O
equation	O
equal	O
or	O
both	O
sides	O
equal	O
empty	O
set	B
is	O
always	O
considered	O
to	O
be	O
shattered	O
by	O
h	O
assume	O
equation	O
holds	O
for	O
sets	O
of	O
size	O
k	O
m	O
and	O
let	O
us	O
prove	O
it	O
for	O
sets	O
of	O
size	O
m	O
fix	O
h	O
and	O
c	O
cm	O
denote	O
cm	O
and	O
in	O
addition	O
define	O
the	O
following	O
two	O
sets	O
ym	O
ym	O
hc	O
ym	O
hc	O
and	O
ym	O
ym	O
hc	O
ym	O
hc	O
it	O
is	O
easy	O
to	O
verify	O
that	O
additionally	O
since	O
using	O
the	O
induction	O
assumption	O
on	O
h	O
and	O
we	O
have	O
that	O
h	O
shatters	O
b	O
c	O
b	O
h	O
shatters	O
b	O
proof	O
of	O
theorem	O
next	O
define	O
h	O
to	O
be	O
h	O
h	O
s	O
t	O
hcm	O
namely	O
contains	O
pairs	O
of	O
hypotheses	O
that	O
agree	O
on	O
and	O
differ	O
on	O
using	O
this	O
definition	O
it	O
is	O
clear	O
that	O
if	O
shatters	O
a	O
set	B
b	O
then	O
it	O
also	O
shatters	O
the	O
set	B
b	O
and	O
vice	O
versa	O
combining	O
this	O
with	O
the	O
fact	O
that	O
and	O
using	O
the	O
inductive	O
assumption	O
applied	O
on	O
and	O
we	O
obtain	O
that	O
shatters	O
b	O
shatters	O
b	O
c	O
b	O
shatters	O
b	O
c	O
b	O
h	O
shatters	O
b	O
overall	O
we	O
have	O
shown	O
that	O
c	O
b	O
h	O
shatters	O
b	O
c	O
b	O
h	O
shatters	O
b	O
c	O
h	O
shatters	O
b	O
which	O
concludes	O
our	O
proof	O
uniform	B
convergence	I
for	O
classes	O
of	O
small	O
effective	O
size	O
in	O
this	O
section	O
we	O
prove	O
that	O
if	O
h	O
has	O
small	O
effective	O
size	O
then	O
it	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
formally	O
theorem	O
let	O
h	O
be	O
a	O
class	O
and	O
let	O
h	O
be	O
its	O
growth	B
function	B
then	O
for	O
every	O
d	O
and	O
every	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
we	O
have	O
lsh	O
before	O
proving	O
the	O
theorem	O
let	O
us	O
first	O
conclude	O
the	O
proof	O
of	O
theorem	O
proof	O
of	O
theorem	O
it	O
suffices	O
to	O
prove	O
that	O
if	O
the	O
vc-dimension	O
is	O
finite	O
then	O
the	O
uniform	B
convergence	I
property	O
holds	O
we	O
will	O
prove	O
that	O
much	O
log	O
d	O
from	O
sauer	O
s	O
lemma	O
we	O
have	O
that	O
for	O
m	O
d	O
combining	O
this	O
with	O
theorem	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
ldh	O
for	O
simplicity	O
assume	O
hence	O
ldh	O
m	O
the	O
vc-dimension	O
to	O
ensure	O
that	O
the	O
preceding	O
is	O
at	O
most	O
we	O
need	O
that	O
m	O
logm	O
d	O
standard	O
algebraic	O
manipulations	O
lemma	O
in	O
appendix	O
a	O
show	O
that	O
a	O
sufficient	O
condition	O
for	O
the	O
preceding	O
to	O
hold	O
is	O
that	O
m	O
log	O
d	O
remark	O
the	O
upper	O
bound	O
on	O
much	O
we	O
derived	O
in	O
the	O
proof	O
theorem	O
is	O
not	O
the	O
tightest	O
possible	O
a	O
tighter	O
analysis	O
that	O
yields	O
the	O
bounds	O
given	O
in	O
theorem	O
can	O
be	O
found	O
in	O
chapter	O
proof	O
of	O
theorem	O
we	O
will	O
start	O
by	O
showing	O
that	O
e	O
lsh	O
s	O
dm	O
sup	O
h	O
h	O
since	O
the	O
random	O
variable	O
suph	O
h	O
lsh	O
is	O
nonnegative	O
the	O
proof	O
of	O
the	O
theorem	O
follows	O
directly	O
from	O
the	O
preceding	O
using	O
markov	O
s	O
inequality	O
section	O
h	O
h	O
we	O
can	O
rewrite	O
ldh	O
dm	O
where	O
additional	O
i	O
i	O
d	O
sample	O
therefore	O
lsh	O
to	O
bound	O
the	O
left-hand	O
side	O
of	O
equation	O
we	O
first	O
note	O
that	O
for	O
every	O
m	O
is	O
an	O
lsh	O
e	O
e	O
dm	O
e	O
s	O
dm	O
sup	O
h	O
h	O
sup	O
h	O
h	O
a	O
generalization	O
of	O
the	O
triangle	O
inequality	O
yields	O
lsh	O
lsh	O
e	O
dm	O
s	O
dm	O
e	O
dm	O
and	O
the	O
fact	O
that	O
supermum	O
of	O
expectation	O
is	O
smaller	O
than	O
expectation	O
of	O
supremum	O
yields	O
e	O
dm	O
sup	O
h	O
h	O
lsh	O
e	O
dm	O
sup	O
h	O
h	O
lsh	O
formally	O
the	O
previous	O
two	O
inequalities	O
follow	O
from	O
jensen	O
s	O
inequality	O
combining	O
all	O
we	O
obtain	O
e	O
s	O
dm	O
sup	O
h	O
h	O
lsh	O
lsh	O
e	O
dm	O
e	O
dm	O
sup	O
h	O
h	O
m	O
sup	O
h	O
h	O
i	O
zi	O
m	O
m	O
sup	O
h	O
h	O
proof	O
of	O
theorem	O
the	O
expectation	O
on	O
the	O
right-hand	O
side	O
is	O
over	O
a	O
choice	O
of	O
two	O
i	O
i	O
d	O
samples	O
s	O
zm	O
and	O
m	O
since	O
all	O
of	O
these	O
vectors	O
are	O
chosen	O
i	O
i	O
d	O
nothing	O
will	O
change	O
if	O
we	O
replace	O
the	O
name	O
of	O
the	O
random	O
vector	O
zi	O
with	O
the	O
i	O
zi	O
name	O
of	O
the	O
random	O
vector	O
i	O
zi	O
it	O
follows	O
that	O
for	O
in	O
equation	O
we	O
will	O
have	O
the	O
term	O
every	O
we	O
have	O
that	O
equation	O
equals	O
i	O
if	O
we	O
do	O
it	O
instead	O
of	O
the	O
term	O
e	O
dm	O
m	O
sup	O
h	O
h	O
i	O
zi	O
since	O
this	O
holds	O
for	O
every	O
it	O
also	O
holds	O
if	O
we	O
sample	O
each	O
component	O
of	O
uniformly	O
at	O
random	O
from	O
the	O
uniform	O
distribution	O
over	O
denoted	O
u	O
hence	O
equation	O
also	O
equals	O
e	O
u	O
m	O
e	O
dm	O
sup	O
h	O
h	O
i	O
zi	O
and	O
by	O
the	O
linearity	O
of	O
expectation	O
it	O
also	O
equals	O
e	O
dm	O
e	O
u	O
m	O
i	O
zi	O
next	O
fix	O
s	O
and	O
and	O
let	O
c	O
be	O
the	O
instances	O
appearing	O
in	O
s	O
and	O
then	O
we	O
can	O
take	O
the	O
supremum	O
only	O
over	O
h	O
hc	O
therefore	O
i	O
zi	O
e	O
u	O
m	O
sup	O
h	O
h	O
m	O
u	O
m	O
max	O
h	O
hc	O
e	O
m	O
fix	O
some	O
h	O
hc	O
and	O
denote	O
h	O
i	O
zi	O
since	O
e	O
h	O
and	O
h	O
is	O
an	O
average	O
of	O
independent	O
variables	O
each	O
of	O
which	O
takes	O
values	O
in	O
we	O
have	O
by	O
hoeffding	O
s	O
inequality	O
that	O
for	O
every	O
i	O
zi	O
m	O
p	O
h	O
m	O
applying	O
the	O
union	B
bound	I
over	O
h	O
hc	O
we	O
obtain	O
that	O
for	O
any	O
finally	O
lemma	O
in	O
appendix	O
a	O
tells	O
us	O
that	O
the	O
preceding	O
implies	O
p	O
h	O
max	O
h	O
hc	O
e	O
h	O
max	O
h	O
hc	O
m	O
e	O
s	O
dm	O
sup	O
h	O
h	O
lsh	O
combining	O
all	O
with	O
the	O
definition	O
of	O
h	O
we	O
have	O
shown	O
that	O
the	O
vc-dimension	O
summary	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
characterizes	O
pac	B
learnability	O
of	O
classes	O
of	O
binary	O
classifiers	O
using	O
vc-dimension	O
the	O
vc-dimension	O
of	O
a	O
class	O
is	O
a	O
combinatorial	O
property	O
that	O
denotes	O
the	O
maximal	O
sample	O
size	O
that	O
can	O
be	O
shattered	O
by	O
the	O
class	O
the	O
fundamental	O
theorem	O
states	O
that	O
a	O
class	O
is	O
pac	B
learnable	O
if	O
and	O
only	O
if	O
its	O
vc-dimension	O
is	O
finite	O
and	O
specifies	O
the	O
sample	B
complexity	I
required	O
for	O
pac	B
learning	O
the	O
theorem	O
also	O
shows	O
that	O
if	O
a	O
problem	O
is	O
at	O
all	O
learnable	O
then	O
uniform	B
convergence	I
holds	O
and	O
therefore	O
the	O
problem	O
is	O
learnable	O
using	O
the	O
erm	B
rule	O
bibliographic	O
remarks	O
the	O
definition	O
of	O
vc-dimension	O
and	O
its	O
relation	O
to	O
learnability	O
and	O
to	O
uniform	B
convergence	I
is	O
due	O
to	O
the	O
seminal	O
work	O
of	O
vapnik	O
chervonenkis	O
the	O
relation	O
to	O
the	O
definition	O
of	O
pac	B
learnability	O
is	O
due	O
to	O
blumer	O
ehrenfeucht	O
haussler	O
warmuth	O
several	O
generalizations	O
of	O
the	O
vc-dimension	O
have	O
been	O
proposed	O
for	O
example	O
the	O
fat-shattering	O
dimension	O
characterizes	O
learnability	O
of	O
some	O
regression	B
problems	O
schapire	O
sellie	O
alon	O
ben-david	O
cesa-bianchi	O
haussler	O
bartlett	O
long	O
williamson	O
anthony	O
bartlet	O
and	O
the	O
natarajan	B
dimension	I
characterizes	O
learnability	O
of	O
some	O
multiclass	B
learning	O
problems	O
however	O
in	O
general	O
there	O
is	O
no	O
equivalence	O
between	O
learnability	O
and	O
uniform	B
convergence	I
see	O
shamir	O
srebro	O
sridharan	O
daniely	O
sabato	O
ben-david	O
shalev-shwartz	O
sauer	O
s	O
lemma	O
has	O
been	O
proved	O
by	O
sauer	O
in	O
response	O
to	O
a	O
problem	O
of	O
erdos	O
shelah	O
perles	O
proved	O
it	O
as	O
a	O
useful	O
lemma	O
for	O
shelah	O
s	O
theory	O
of	O
stable	O
models	O
gil	O
kalai	O
us	O
that	O
at	O
some	O
later	O
time	O
benjy	O
weiss	O
asked	O
perles	O
about	O
such	O
a	O
result	O
in	O
the	O
context	O
of	O
ergodic	O
theory	O
and	O
perles	O
who	O
forgot	O
that	O
he	O
had	O
proved	O
it	O
once	O
proved	O
it	O
again	O
vapnik	O
and	O
chervonenkis	O
proved	O
the	O
lemma	O
in	O
the	O
context	O
of	O
statistical	O
learning	O
theory	O
exercises	O
hypothesis	B
classes	O
if	O
h	O
then	O
vcdimh	O
show	O
the	O
following	O
monotonicity	O
property	O
of	O
vc-dimension	O
for	O
every	O
two	O
given	O
some	O
finite	O
domain	B
set	B
x	O
and	O
a	O
number	O
k	O
figure	O
out	O
the	O
vck	O
hx	O
k	O
that	O
is	O
the	O
set	B
of	O
all	O
functions	O
that	O
assign	O
the	O
value	O
to	O
exactly	O
k	O
elements	O
of	O
x	O
dimension	O
of	O
each	O
of	O
the	O
following	O
classes	O
prove	O
your	O
claims	O
hx	O
extremal-combinatorics-iii-some-basic-theorems	O
exercises	O
hat	O
most	O
k	O
hx	O
k	O
or	O
hx	O
k	O
let	O
x	O
be	O
the	O
boolean	O
hypercube	O
for	O
a	O
set	B
i	O
n	O
we	O
define	O
a	O
parity	O
function	B
hi	O
as	O
follows	O
on	O
a	O
binary	O
vector	O
x	O
xn	O
i	O
i	O
hi	O
xi	O
mod	O
is	O
hi	O
computes	O
parity	O
of	O
bits	O
in	O
i	O
what	O
is	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
all	O
such	O
parity	O
functions	O
hn-parity	O
i	O
n	O
we	O
proved	O
sauer	O
s	O
lemma	O
by	O
proving	O
that	O
for	O
every	O
class	O
h	O
of	O
finite	O
vc	B
dimension	I
d	O
and	O
every	O
subset	O
a	O
of	O
the	O
domain	B
a	O
h	O
shatters	O
b	O
i	O
show	O
that	O
there	O
are	O
cases	O
in	O
which	O
the	O
previous	O
two	O
inequalities	O
are	O
strict	O
the	O
can	O
be	O
replaced	O
by	O
and	O
cases	O
in	O
which	O
they	O
can	O
be	O
replaced	O
by	O
equalities	O
demonstrate	O
all	O
four	O
combinations	O
of	O
and	O
vc-dimension	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rd	O
let	O
hd	O
axis	O
aligned	O
rectangles	O
in	O
rd	O
we	O
have	O
already	O
seen	O
that	O
prove	O
that	O
in	O
general	O
vcdimhd	O
rec	O
be	O
the	O
class	O
of	O
rec	O
rec	O
vc-dimension	O
of	O
boolean	B
conjunctions	I
let	O
hd	O
con	O
con	O
be	O
the	O
class	O
of	O
boolean	B
conjunctions	I
over	O
the	O
variables	O
xd	O
we	O
already	O
know	O
that	O
this	O
class	O
is	O
finite	O
and	O
thus	O
pac	B
learnable	O
in	O
this	O
question	O
we	O
calculate	O
vcdimhd	O
show	O
that	O
conclude	O
that	O
vcdimh	O
d	O
log	O
show	O
that	O
hd	O
show	O
that	O
vcdimhd	O
con	O
shatters	O
the	O
set	B
of	O
unit	O
vectors	O
i	O
d	O
con	O
hint	O
assume	O
by	O
contradiction	O
that	O
there	O
exists	O
a	O
set	B
c	O
that	O
is	O
shattered	O
by	O
hd	O
con	O
that	O
satisfy	O
con	O
let	O
be	O
hypotheses	O
in	O
hd	O
con	O
d	O
i	O
j	O
hicj	O
i	O
j	O
otherwise	O
for	O
each	O
i	O
hi	O
more	O
accurately	O
the	O
conjunction	O
that	O
corresponds	O
to	O
hi	O
contains	O
some	O
literal	O
which	O
is	O
false	O
on	O
ci	O
and	O
true	O
on	O
cj	O
for	O
each	O
j	O
i	O
use	O
the	O
pigeonhole	O
principle	O
to	O
show	O
that	O
there	O
must	O
be	O
a	O
pair	O
i	O
j	O
d	O
such	O
that	O
and	O
use	O
the	O
same	O
xk	O
and	O
use	O
that	O
fact	O
to	O
derive	O
a	O
contradiction	O
to	O
the	O
requirements	O
from	O
the	O
conjunctions	O
hi	O
hj	O
mcon	O
of	O
monotone	O
boolean	B
conjunctions	I
over	O
monotonicity	O
here	O
means	O
that	O
the	O
conjunctions	O
do	O
not	O
contain	O
negations	O
consider	O
the	O
class	O
hd	O
the	O
vc-dimension	O
as	O
in	O
hd	O
pothesis	O
we	O
augment	O
hd	O
that	O
vcdimhd	O
mcon	O
d	O
con	O
the	O
empty	O
conjunction	O
is	O
interpreted	O
as	O
the	O
all-positive	O
hymcon	O
with	O
the	O
all-negative	O
hypothesis	B
h	O
show	O
we	O
have	O
shown	O
that	O
for	O
a	O
finite	O
hypothesis	B
class	I
h	O
vcdimh	O
however	O
this	O
is	O
just	O
an	O
upper	O
bound	O
the	O
vc-dimension	O
of	O
a	O
class	O
can	O
be	O
much	O
lower	O
than	O
that	O
find	O
an	O
example	O
of	O
a	O
class	O
h	O
of	O
functions	O
over	O
the	O
real	O
interval	O
x	O
give	O
an	O
example	O
of	O
a	O
finite	O
hypothesis	B
class	I
h	O
over	O
the	O
domain	B
x	O
such	O
that	O
h	O
is	O
infinite	O
while	O
vcdimh	O
where	O
vcdimh	O
it	O
is	O
often	O
the	O
case	O
that	O
the	O
vc-dimension	O
of	O
a	O
hypothesis	B
class	I
equals	O
can	O
be	O
bounded	O
above	O
by	O
the	O
number	O
of	O
parameters	O
one	O
needs	O
to	O
set	B
in	O
order	O
to	O
define	O
each	O
hypothesis	B
in	O
the	O
class	O
for	O
instance	B
if	O
h	O
is	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rd	O
then	O
vcdimh	O
which	O
is	O
equal	O
to	O
the	O
number	O
of	O
parameters	O
used	O
to	O
define	O
a	O
rectangle	O
in	O
rd	O
here	O
is	O
an	O
example	O
that	O
shows	O
that	O
this	O
is	O
not	O
always	O
the	O
case	O
we	O
will	O
see	O
that	O
a	O
hypothesis	B
class	I
might	O
be	O
very	O
complex	O
and	O
even	O
not	O
learnable	O
although	O
it	O
has	O
a	O
small	O
number	O
of	O
parameters	O
consider	O
the	O
domain	B
x	O
r	O
and	O
the	O
hypothesis	B
class	I
h	O
r	O
we	O
take	O
prove	O
that	O
vcdimh	O
hint	O
there	O
is	O
more	O
than	O
one	O
way	O
to	O
prove	O
the	O
required	O
result	O
one	O
option	O
is	O
by	O
applying	O
the	O
following	O
lemma	O
if	O
is	O
the	O
binary	O
expansion	O
of	O
x	O
then	O
for	O
any	O
natural	O
number	O
m	O
xm	O
provided	O
that	O
k	O
m	O
s	O
t	O
xk	O
h	O
a	O
b	O
s	O
where	O
let	O
h	O
be	O
the	O
class	O
of	O
signed	O
intervals	O
that	O
is	O
habsx	O
s	O
s	O
if	O
x	O
b	O
if	O
x	O
b	O
calculate	O
vcdimh	O
let	O
h	O
be	O
a	O
class	O
of	O
functions	O
from	O
x	O
to	O
prove	O
that	O
if	O
vcdimh	O
d	O
for	O
any	O
d	O
then	O
for	O
some	O
probability	O
distri	O
bution	O
d	O
over	O
x	O
for	O
every	O
sample	O
size	O
m	O
e	O
s	O
dm	O
min	O
h	O
h	O
ldh	O
d	O
m	O
hint	O
use	O
exercise	O
in	O
chapter	O
prove	O
that	O
for	O
every	O
h	O
that	O
is	O
pac	B
learnable	O
vcdimh	O
that	O
this	O
is	O
the	O
implication	O
in	O
theorem	O
vc	O
of	O
union	O
let	O
be	O
hypothesis	B
classes	O
over	O
some	O
fixed	O
domain	B
set	B
x	O
let	O
d	O
maxi	O
vcdimhi	O
and	O
assume	O
for	O
simplicity	O
that	O
d	O
exercises	O
prove	O
that	O
vcdim	O
r	O
logr	O
hint	O
take	O
a	O
set	B
of	O
k	O
examples	O
and	O
assume	O
that	O
they	O
are	O
shattered	O
by	O
the	O
union	O
class	O
therefore	O
the	O
union	O
class	O
can	O
produce	O
all	O
possible	O
labelings	O
on	O
these	O
examples	O
use	O
sauer	O
s	O
lemma	O
to	O
show	O
that	O
the	O
union	O
class	O
cannot	O
produce	O
more	O
than	O
rkd	O
labelings	O
therefore	O
rkd	O
now	O
use	O
lemma	O
prove	O
that	O
for	O
r	O
it	O
holds	O
that	O
vcdim	O
dudley	B
classes	I
in	O
this	O
question	O
we	O
discuss	O
an	O
algebraic	O
framework	O
for	O
defining	O
concept	O
classes	O
over	O
rn	O
and	O
show	O
a	O
connection	O
between	O
the	O
vc	B
dimension	I
of	O
such	O
classes	O
and	O
their	O
algebraic	O
properties	O
given	O
a	O
function	B
f	O
rn	O
r	O
we	O
define	O
the	O
corresponding	O
function	B
pos	O
for	O
a	O
class	O
f	O
of	O
real	O
valued	O
functions	O
we	O
define	O
a	O
corresponding	O
class	O
of	O
functions	O
pos	O
f	O
f	O
we	O
say	O
that	O
a	O
family	O
f	O
of	O
real	O
valued	O
functions	O
is	O
linearly	O
closed	O
if	O
for	O
all	O
f	O
g	O
f	O
and	O
r	O
r	O
rg	O
f	O
addition	O
and	O
scalar	O
multiplication	O
of	O
functions	O
are	O
defined	O
point	O
wise	O
namely	O
for	O
all	O
x	O
rn	O
rgx	O
f	O
rgx	O
note	O
that	O
if	O
a	O
family	O
of	O
functions	O
is	O
linearly	O
closed	O
then	O
we	O
can	O
view	O
it	O
as	O
a	O
vector	O
space	O
over	O
the	O
reals	O
for	O
a	O
function	B
g	O
rn	O
r	O
and	O
a	O
family	O
of	O
functions	O
f	O
let	O
f	O
def	O
f	O
f	O
hypothesis	B
classes	O
that	O
have	O
a	O
representation	O
as	O
pos	O
g	O
for	O
some	O
vector	O
space	O
of	O
functions	O
f	O
and	O
some	O
function	B
g	O
are	O
called	O
dudley	B
classes	I
show	O
that	O
for	O
every	O
g	O
rn	O
r	O
and	O
every	O
vector	O
space	O
of	O
functions	O
f	O
as	O
defined	O
earlier	O
vcdimpos	O
g	O
vcdimpos	O
for	O
every	O
linearly	O
closed	O
family	O
of	O
real	O
valued	O
functions	O
f	O
the	O
vcdimension	O
of	O
the	O
corresponding	O
class	O
pos	O
equals	O
the	O
linear	O
dimension	O
of	O
f	O
a	O
vector	O
space	O
hint	O
let	O
fd	O
be	O
a	O
basis	O
for	O
the	O
vector	O
space	O
f	O
consider	O
the	O
mapping	O
x	O
fdx	O
rn	O
to	O
rd	O
note	O
that	O
this	O
mapping	O
induces	O
a	O
matching	O
between	O
functions	O
over	O
rn	O
of	O
the	O
form	O
pos	O
and	O
homogeneous	O
linear	O
halfspaces	O
in	O
rd	O
vc-dimension	O
of	O
the	O
class	O
of	O
homogeneous	O
linear	O
halfspaces	O
is	O
analyzed	O
in	O
chapter	O
show	O
that	O
each	O
of	O
the	O
following	O
classes	O
can	O
be	O
represented	O
as	O
a	O
dudley	O
class	O
the	O
class	O
hsn	O
of	O
halfspaces	O
over	O
rn	O
chapter	O
the	O
class	O
hhsn	O
of	O
all	O
homogeneous	O
halfspaces	O
over	O
rn	O
chapter	O
the	O
class	O
bd	O
of	O
all	O
functions	O
defined	O
by	O
balls	O
in	O
rd	O
use	O
the	O
dudley	O
representation	O
to	O
figure	O
out	O
the	O
vc-dimension	O
of	O
this	O
class	O
let	O
p	O
d	O
n	O
denote	O
the	O
class	O
of	O
functions	O
defined	O
by	O
polynomial	O
inequalities	O
of	O
degree	O
d	O
namely	O
n	O
p	O
is	O
a	O
polynomial	O
of	O
degree	O
d	O
in	O
the	O
variables	O
xn	O
p	O
d	O
the	O
vc-dimension	O
where	O
for	O
x	O
xn	O
hpx	O
degree	O
of	O
a	O
multivariable	O
polynomial	O
is	O
the	O
maximal	O
sum	O
of	O
variable	O
exponents	O
over	O
all	O
of	O
its	O
terms	O
for	O
example	O
the	O
degree	O
of	O
px	O
use	O
the	O
dudley	O
representation	O
to	O
figure	O
out	O
the	O
vc-dimension	O
of	O
the	O
is	O
class	O
p	O
d	O
the	O
class	O
of	O
all	O
d-degree	O
polynomials	O
over	O
r	O
prove	O
that	O
the	O
class	O
of	O
all	O
polynomial	O
classifiers	O
over	O
r	O
has	O
infinite	O
vc-dimension	O
use	O
the	O
dudley	O
representation	O
to	O
figure	O
out	O
the	O
vc-dimension	O
of	O
the	O
class	O
p	O
d	O
n	O
a	O
function	B
of	O
d	O
and	O
n	O
nonuniform	O
learnability	O
the	O
notions	O
of	O
pac	B
learnability	O
discussed	O
so	O
far	O
in	O
the	O
book	O
allow	O
the	O
sample	O
sizes	O
to	O
depend	O
on	O
the	O
accuracy	B
and	O
confidence	B
parameters	O
but	O
they	O
are	O
uniform	O
with	O
respect	O
to	O
the	O
labeling	O
rule	O
and	O
the	O
underlying	O
data	O
distribution	O
consequently	O
classes	O
that	O
are	O
learnable	O
in	O
that	O
respect	O
are	O
limited	O
must	O
have	O
a	O
finite	O
vc-dimension	O
as	O
stated	O
by	O
theorem	O
in	O
this	O
chapter	O
we	O
consider	O
more	O
relaxed	O
weaker	O
notions	O
of	O
learnability	O
we	O
discuss	O
the	O
usefulness	O
of	O
such	O
notions	O
and	O
provide	O
characterization	O
of	O
the	O
concept	O
classes	O
that	O
are	O
learnable	O
using	O
these	O
definitions	O
we	O
begin	O
this	O
discussion	O
by	O
defining	O
a	O
notion	O
of	O
nonuniform	O
learnability	O
that	O
allows	O
the	O
sample	O
size	O
to	O
depend	O
on	O
the	O
hypothesis	B
to	O
which	O
the	O
learner	O
is	O
compared	O
we	O
then	O
provide	O
a	O
characterization	O
of	O
nonuniform	O
learnability	O
and	O
show	O
that	O
nonuniform	O
learnability	O
is	O
a	O
strict	O
relaxation	O
of	O
agnostic	B
pac	B
learnability	O
we	O
also	O
show	O
that	O
a	O
sufficient	O
condition	O
for	O
nonuniform	O
learnability	O
is	O
that	O
h	O
is	O
a	O
countable	O
union	O
of	O
hypothesis	B
classes	O
each	O
of	O
which	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
these	O
results	O
will	O
be	O
proved	O
in	O
section	O
by	O
introducing	O
a	O
new	O
learning	O
paradigm	O
which	O
is	O
called	O
structural	O
risk	B
minimization	O
in	O
section	O
we	O
specify	O
the	O
srm	B
paradigm	O
for	O
countable	O
hypothesis	B
classes	O
which	O
yields	O
the	O
minimum	O
description	O
length	O
paradigm	O
the	O
mdl	B
paradigm	O
gives	O
a	O
formal	O
justification	O
to	O
a	O
philosophical	O
principle	O
of	O
induction	O
called	O
occam	O
s	O
razor	O
next	O
in	O
section	O
we	O
introduce	O
consistency	B
as	O
an	O
even	O
weaker	O
notion	O
of	O
learnability	O
finally	O
we	O
discuss	O
the	O
significance	O
and	O
usefulness	O
of	O
the	O
different	O
notions	O
of	O
learnability	O
nonuniform	O
learnability	O
nonuniform	O
learnability	O
allows	O
the	O
sample	O
size	O
to	O
be	O
nonuniform	O
with	O
respect	O
to	O
the	O
different	O
hypotheses	O
with	O
which	O
the	O
learner	O
is	O
competing	O
we	O
say	O
that	O
a	O
hypothesis	B
h	O
is	O
with	O
another	O
hypothesis	B
if	O
with	O
probability	O
higher	O
than	O
ldh	O
in	O
pac	B
learnability	O
this	O
notion	O
of	O
competitiveness	O
is	O
not	O
very	O
useful	O
as	O
we	O
are	O
looking	O
for	O
a	O
hypothesis	B
with	O
an	O
absolute	O
low	O
risk	B
the	O
realizable	O
case	O
or	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
nonuniform	O
learnability	O
with	O
a	O
low	O
risk	B
compared	O
to	O
the	O
minimal	O
risk	B
achieved	O
by	O
hypotheses	O
in	O
our	O
class	O
the	O
agnostic	O
case	O
therefore	O
the	O
sample	O
size	O
depends	O
only	O
on	O
the	O
accuracy	B
and	O
confidence	B
parameters	O
in	O
nonuniform	O
learnability	O
however	O
we	O
allow	O
the	O
sample	O
size	O
to	O
be	O
of	O
the	O
form	O
mh	O
h	O
namely	O
it	O
depends	O
also	O
on	O
the	O
h	O
with	O
which	O
we	O
are	O
competing	O
formally	O
definition	O
a	O
hypothesis	B
class	I
h	O
is	O
nonuniformly	O
learnable	O
if	O
there	O
exist	O
a	O
learning	O
algorithm	O
a	O
and	O
a	O
function	B
mnulh	O
h	O
n	O
such	O
that	O
for	O
every	O
and	O
for	O
every	O
h	O
h	O
if	O
m	O
mnulh	O
h	O
then	O
for	O
every	O
distribution	O
d	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
it	O
holds	O
that	O
ldas	O
ldh	O
at	O
this	O
point	O
it	O
might	O
be	O
useful	O
to	O
recall	B
the	O
definition	O
of	O
agnostic	B
pac	B
learnability	O
a	O
hypothesis	B
class	I
h	O
is	O
agnostically	O
pac	B
learnable	O
if	O
there	O
exist	O
a	O
learning	O
algorithm	O
a	O
and	O
a	O
function	B
mh	O
n	O
such	O
that	O
for	O
every	O
and	O
for	O
every	O
distribution	O
d	O
if	O
m	O
mh	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
it	O
holds	O
that	O
ldas	O
min	O
note	O
that	O
this	O
implies	O
that	O
for	O
every	O
h	O
h	O
h	O
ldas	O
ldh	O
in	O
both	O
types	O
of	O
learnability	O
we	O
require	O
that	O
the	O
output	O
hypothesis	B
will	O
be	O
with	O
every	O
other	O
hypothesis	B
in	O
the	O
class	O
but	O
the	O
difference	O
between	O
these	O
two	O
notions	O
of	O
learnability	O
is	O
the	O
question	O
of	O
whether	O
the	O
sample	O
size	O
m	O
may	O
depend	O
on	O
the	O
hypothesis	B
h	O
to	O
which	O
the	O
error	O
of	O
as	O
is	O
compared	O
note	O
that	O
that	O
nonuniform	O
learnability	O
is	O
a	O
relaxation	O
of	O
agnostic	B
pac	B
learnability	O
that	O
is	O
if	O
a	O
class	O
is	O
agnostic	B
pac	B
learnable	O
then	O
it	O
is	O
also	O
nonuniformly	O
learnable	O
characterizing	O
nonuniform	O
learnability	O
our	O
goal	O
now	O
is	O
to	O
characterize	O
nonuniform	O
learnability	O
in	O
the	O
previous	O
chapter	O
we	O
have	O
found	O
a	O
crisp	O
characterization	O
of	O
pac	B
learnable	O
classes	O
by	O
showing	O
that	O
a	O
class	O
of	O
binary	O
classifiers	O
is	O
agnostic	B
pac	B
learnable	O
if	O
and	O
only	O
if	O
its	O
vcdimension	O
is	O
finite	O
in	O
the	O
following	O
theorem	O
we	O
find	O
a	O
different	O
characterization	O
for	O
nonuniform	O
learnable	O
classes	O
for	O
the	O
task	O
of	O
binary	O
classification	O
theorem	O
a	O
hypothesis	B
class	I
h	O
of	O
binary	O
classifiers	O
is	O
nonuniformly	O
learnable	O
if	O
and	O
only	O
if	O
it	O
is	O
a	O
countable	O
union	O
of	O
agnostic	B
pac	B
learnable	O
hypothesis	B
classes	O
the	O
proof	O
of	O
theorem	O
relies	O
on	O
the	O
following	O
result	O
of	O
independent	O
interest	O
structural	O
risk	B
minimization	O
union	O
of	O
hypothesis	B
classes	O
h	O
theorem	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
that	O
can	O
be	O
written	O
as	O
a	O
countable	O
n	O
n	O
hn	O
where	O
each	O
hn	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
then	O
h	O
is	O
nonuniformly	O
learnable	O
recall	B
that	O
in	O
chapter	O
we	O
have	O
shown	O
that	O
uniform	B
convergence	I
is	O
sufficient	O
for	O
agnostic	B
pac	B
learnability	O
theorem	O
generalizes	O
this	O
result	O
to	O
nonuniform	O
learnability	O
the	O
proof	O
of	O
this	O
theorem	O
will	O
be	O
given	O
in	O
the	O
next	O
section	O
by	O
introducing	O
a	O
new	O
learning	O
paradigm	O
we	O
now	O
turn	O
to	O
proving	O
theorem	O
proof	O
of	O
theorem	O
first	O
assume	O
that	O
h	O
n	O
n	O
hn	O
where	O
each	O
hn	O
is	O
agnostic	B
pac	B
learnable	O
using	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
it	O
follows	O
that	O
each	O
hn	O
has	O
the	O
uniform	B
convergence	I
property	O
therefore	O
using	O
theorem	O
we	O
obtain	O
that	O
h	O
is	O
nonuniform	O
learnable	O
for	O
the	O
other	O
direction	O
assume	O
that	O
h	O
is	O
nonuniform	O
learnable	O
using	O
some	O
algorithm	O
a	O
for	O
every	O
n	O
n	O
let	O
hn	O
h	O
mnulh	O
h	O
n	O
clearly	O
h	O
n	O
nhn	O
in	O
addition	O
using	O
the	O
definition	O
of	O
mnulh	O
we	O
know	O
that	O
for	O
any	O
distribution	O
d	O
that	O
satisfies	O
the	O
realizability	B
assumption	O
with	O
respect	O
to	O
hn	O
with	O
probability	O
of	O
at	O
least	O
over	O
s	O
dn	O
we	O
have	O
that	O
ldas	O
using	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
this	O
implies	O
that	O
the	O
vcdimension	O
of	O
hn	O
must	O
be	O
finite	O
and	O
therefore	O
hn	O
is	O
agnostic	B
pac	B
learnable	O
the	O
following	O
example	O
shows	O
that	O
nonuniform	O
learnability	O
is	O
a	O
strict	O
relaxation	O
of	O
agnostic	B
pac	B
learnability	O
namely	O
there	O
are	O
hypothesis	B
classes	O
that	O
are	O
nonuniform	O
learnable	O
but	O
are	O
not	O
agnostic	B
pac	B
learnable	O
where	O
p	O
r	O
r	O
is	O
a	O
polynomial	O
of	O
degree	O
n	O
let	O
h	O
example	O
consider	O
a	O
binary	O
classification	O
problem	O
with	O
the	O
instance	B
domain	B
being	O
x	O
r	O
for	O
every	O
n	O
n	O
let	O
hn	O
be	O
the	O
class	O
of	O
polynomial	O
classifiers	O
of	O
degree	O
n	O
namely	O
hn	O
is	O
the	O
set	B
of	O
all	O
classifiers	O
of	O
the	O
form	O
hx	O
signpx	O
n	O
n	O
hn	O
therefore	O
h	O
is	O
the	O
class	O
of	O
all	O
polynomial	O
classifiers	O
over	O
r	O
it	O
is	O
easy	O
to	O
verify	O
that	O
vcdimh	O
while	O
vcdimhn	O
n	O
exercise	O
hence	O
h	O
is	O
not	O
pac	B
learnable	O
while	O
on	O
the	O
basis	O
of	O
theorem	O
h	O
is	O
nonuniformly	O
learnable	O
structural	O
risk	B
minimization	O
we	O
do	O
so	O
by	O
first	O
assuming	O
that	O
h	O
can	O
be	O
written	O
as	O
h	O
so	O
far	O
we	O
have	O
encoded	O
our	O
prior	B
knowledge	I
by	O
specifying	O
a	O
hypothesis	B
class	I
h	O
which	O
we	O
believe	O
includes	O
a	O
good	O
predictor	B
for	O
the	O
learning	O
task	O
at	O
hand	O
yet	O
another	O
way	O
to	O
express	O
our	O
prior	B
knowledge	I
is	O
by	O
specifying	O
preferences	O
over	O
hypotheses	O
within	O
h	O
in	O
the	O
structural	O
risk	B
minimization	O
paradigm	O
n	O
n	O
hn	O
and	O
then	O
specifying	O
a	O
weight	O
function	B
w	O
n	O
which	O
assigns	O
a	O
weight	O
to	O
each	O
hypothesis	B
class	I
hn	O
such	O
that	O
a	O
higher	O
weight	O
reflects	O
a	O
stronger	O
preference	O
for	O
the	O
hypothesis	B
class	I
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
learn	O
with	O
such	O
prior	B
knowledge	I
in	O
the	O
next	O
section	O
we	O
describe	O
a	O
couple	O
of	O
important	O
weighting	O
schemes	O
including	O
minimum	O
description	O
length	O
nonuniform	O
learnability	O
concretely	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
that	O
can	O
be	O
written	O
as	O
h	O
n	O
n	O
hn	O
for	O
example	O
h	O
may	O
be	O
the	O
class	O
of	O
all	O
polynomial	O
classifiers	O
where	O
each	O
hn	O
is	O
the	O
class	O
of	O
polynomial	O
classifiers	O
of	O
degree	O
n	O
example	O
assume	O
that	O
for	O
each	O
n	O
the	O
class	O
hn	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
definition	O
let	O
us	O
also	O
define	O
in	O
chapter	O
with	O
a	O
sample	B
complexity	I
function	B
muchn	O
the	O
function	B
n	O
by	O
min	O
muchn	O
m	O
in	O
words	O
we	O
have	O
a	O
fixed	O
sample	O
size	O
m	O
and	O
we	O
are	O
interested	O
in	O
the	O
lowest	O
possible	O
upper	O
bound	O
on	O
the	O
gap	O
between	O
empirical	O
and	O
true	O
risks	O
achievable	O
by	O
using	O
a	O
sample	O
of	O
m	O
examples	O
from	O
the	O
definitions	O
of	O
uniform	B
convergence	I
and	O
it	O
follows	O
that	O
for	O
every	O
m	O
and	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
we	O
have	O
that	O
h	O
hn	O
lsh	O
let	O
w	O
n	O
be	O
a	O
function	B
such	O
wn	O
we	O
refer	O
to	O
w	O
as	O
a	O
weight	O
function	B
over	O
the	O
hypothesis	B
classes	O
such	O
a	O
weight	O
function	B
can	O
reflect	O
the	O
importance	O
that	O
the	O
learner	O
attributes	O
to	O
each	O
hypothesis	B
class	I
or	O
some	O
measure	O
of	O
the	O
complexity	O
of	O
different	O
hypothesis	B
classes	O
if	O
h	O
is	O
a	O
finite	O
union	O
of	O
n	O
hypothesis	B
classes	O
one	O
can	O
simply	O
assign	O
the	O
same	O
weight	O
of	O
to	O
all	O
hypothesis	B
classes	O
this	O
equal	O
weighting	O
corresponds	O
to	O
no	O
a	O
priori	O
preference	O
to	O
any	O
hypothesis	B
class	I
of	O
course	O
if	O
one	O
believes	O
prior	B
knowledge	I
that	O
a	O
certain	O
hypothesis	B
class	I
is	O
more	O
likely	O
to	O
contain	O
the	O
correct	O
target	O
function	B
then	O
it	O
should	O
be	O
assigned	O
a	O
larger	O
weight	O
reflecting	O
this	O
prior	B
knowledge	I
when	O
h	O
is	O
a	O
infinite	O
union	O
of	O
hypothesis	B
classes	O
a	O
uniform	O
weighting	O
is	O
not	O
possible	O
but	O
many	O
other	O
weighting	O
schemes	O
may	O
work	O
for	O
example	O
one	O
can	O
or	O
wn	O
n	O
later	O
in	O
this	O
chapter	O
we	O
will	O
provide	O
another	O
choose	O
wn	O
convenient	O
way	O
to	O
define	O
weighting	O
functions	O
using	O
description	O
languages	O
the	O
srm	B
rule	O
follows	O
a	O
bound	O
minimization	O
approach	O
this	O
means	O
that	O
the	O
goal	O
of	O
the	O
paradigm	O
is	O
to	O
find	O
a	O
hypothesis	B
that	O
minimizes	O
a	O
certain	O
upper	O
bound	O
on	O
the	O
true	O
risk	B
the	O
bound	O
that	O
the	O
srm	B
rule	O
wishes	O
to	O
minimize	O
is	O
given	O
in	O
the	O
following	O
theorem	O
theorem	O
let	O
w	O
n	O
be	O
a	O
function	B
such	O
that	O
h	O
be	O
a	O
hypothesis	B
class	I
that	O
can	O
be	O
written	O
as	O
h	O
wn	O
let	O
n	O
n	O
hn	O
where	O
for	O
each	O
n	O
hn	O
satisfies	O
the	O
uniform	B
convergence	I
property	O
with	O
a	O
sample	B
complexity	I
function	B
let	O
be	O
as	O
defined	O
in	O
equation	O
then	O
for	O
every	O
and	O
muchn	O
distribution	O
d	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
the	O
following	O
bound	O
holds	O
for	O
every	O
n	O
n	O
and	O
h	O
hn	O
lsh	O
wn	O
therefore	O
for	O
every	O
and	O
distribution	O
d	O
with	O
probability	O
of	O
at	O
least	O
structural	O
risk	B
minimization	O
it	O
holds	O
that	O
h	O
h	O
ldh	O
lsh	O
min	O
nh	O
hn	O
wn	O
proof	O
for	O
each	O
n	O
define	O
n	O
wn	O
applying	O
the	O
assumption	O
that	O
uniform	B
convergence	I
holds	O
for	O
all	O
n	O
with	O
the	O
rate	O
given	O
in	O
equation	O
we	O
obtain	O
that	O
if	O
we	O
fix	O
n	O
in	O
advance	O
then	O
with	O
probability	O
of	O
at	O
least	O
n	O
over	O
the	O
choice	O
of	O
s	O
dm	O
lsh	O
n	O
at	O
least	O
h	O
hn	O
n	O
n	O
applying	O
the	O
union	B
bound	I
over	O
n	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
n	O
wn	O
the	O
preceding	O
holds	O
for	O
all	O
n	O
which	O
concludes	O
our	O
proof	O
denote	O
nh	O
minn	O
h	O
hn	O
and	O
then	O
equation	O
implies	O
that	O
ldh	O
lsh	O
wnh	O
the	O
srm	B
paradigm	O
searches	O
for	O
h	O
that	O
minimizes	O
this	O
bound	O
as	O
formalized	O
in	O
the	O
following	O
pseudocode	O
prior	B
knowledge	I
structural	O
risk	B
minimization	O
n	O
hn	O
where	O
hn	O
has	O
uniform	B
convergence	I
with	O
muchn	O
h	O
w	O
n	O
output	O
h	O
argminh	O
wnh	O
define	O
as	O
in	O
equation	O
nh	O
as	O
in	O
equation	O
input	O
training	B
set	B
s	O
dm	O
confidence	B
n	O
wn	O
unlike	O
the	O
erm	B
paradigm	O
discussed	O
in	O
previous	O
chapters	O
we	O
no	O
longer	O
just	O
care	O
about	O
the	O
empirical	B
risk	B
lsh	O
but	O
we	O
are	O
willing	O
to	O
trade	O
some	O
of	O
our	O
bias	B
toward	O
low	O
empirical	B
risk	B
with	O
a	O
bias	B
toward	O
classes	O
for	O
which	O
wnh	O
is	O
smaller	O
for	O
the	O
sake	O
of	O
a	O
smaller	O
estimation	B
error	I
next	O
we	O
show	O
that	O
the	O
srm	B
paradigm	O
can	O
be	O
used	O
for	O
nonuniform	O
learning	O
of	O
every	O
class	O
which	O
is	O
a	O
countable	O
union	O
of	O
uniformly	O
converging	O
hypothesis	B
classes	O
theorem	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
such	O
that	O
h	O
each	O
hn	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
sample	B
complexity	I
muchn	O
w	O
n	O
be	O
such	O
that	O
wn	O
using	O
the	O
srm	B
rule	O
with	O
rate	O
n	O
n	O
hn	O
where	O
let	O
then	O
h	O
is	O
nonuniformly	O
learnable	O
mnulh	O
h	O
muchnh	O
nonuniform	O
learnability	O
proof	O
let	O
a	O
be	O
the	O
srm	B
algorithm	O
with	O
respect	O
to	O
the	O
weighting	O
function	B
w	O
wnh	O
using	O
the	O
fact	O
that	O
n	O
wn	O
we	O
can	O
apply	O
theorem	O
to	O
get	O
that	O
with	O
probability	O
of	O
at	O
least	O
for	O
every	O
h	O
h	O
and	O
let	O
m	O
muchnh	O
over	O
the	O
choice	O
of	O
s	O
dm	O
we	O
have	O
that	O
for	O
every	O
h	O
lsh	O
wnh	O
the	O
preceding	O
holds	O
in	O
particular	O
for	O
the	O
hypothesis	B
as	O
returned	O
by	O
the	O
srm	B
rule	O
by	O
the	O
definition	O
of	O
srm	B
we	O
obtain	O
that	O
ldas	O
min	O
finally	O
if	O
m	O
muchnh	O
wnh	O
then	O
clearly	O
wnh	O
in	O
addition	O
from	O
the	O
uniform	B
convergence	I
property	O
of	O
each	O
hn	O
we	O
have	O
that	O
with	O
probability	O
of	O
more	O
than	O
lsh	O
ldh	O
combining	O
all	O
the	O
preceding	O
we	O
obtain	O
that	O
ldas	O
ldh	O
which	O
concludes	O
our	O
proof	O
note	O
that	O
the	O
previous	O
theorem	O
also	O
proves	O
theorem	O
remark	O
for	O
nonuniform	O
learnability	O
we	O
have	O
shown	O
that	O
any	O
countable	O
union	O
of	O
classes	O
of	O
finite	O
vc-dimension	O
is	O
nonuniformly	O
learnable	O
it	O
turns	O
out	O
that	O
for	O
any	O
infinite	O
domain	B
set	B
x	O
the	O
class	O
of	O
all	O
binary	O
valued	O
functions	O
over	O
x	O
is	O
not	O
a	O
countable	O
union	O
of	O
classes	O
of	O
finite	O
vc-dimension	O
we	O
leave	O
the	O
proof	O
of	O
this	O
claim	O
as	O
a	O
exercise	O
exercise	O
it	O
follows	O
that	O
in	O
some	O
sense	O
the	O
no	O
free	O
lunch	O
theorem	O
holds	O
for	O
nonuniform	O
learning	O
as	O
well	O
namely	O
whenever	O
the	O
domain	B
is	O
not	O
finite	O
there	O
exists	O
no	O
nonuniform	O
learner	O
with	O
respect	O
to	O
the	O
class	O
of	O
all	O
deterministic	O
binary	O
classifiers	O
for	O
each	O
such	O
classifier	B
there	O
exists	O
a	O
trivial	O
algorithm	O
that	O
learns	O
it	O
erm	B
with	O
respect	O
to	O
the	O
hypothesis	B
class	I
that	O
contains	O
only	O
this	O
classifier	B
it	O
is	O
interesting	O
to	O
compare	O
the	O
nonuniform	O
learnability	O
result	O
given	O
in	O
theorem	O
to	O
the	O
task	O
of	O
agnostic	B
pac	B
learning	O
any	O
specific	O
hn	O
separately	O
the	O
prior	B
knowledge	I
or	O
bias	B
of	O
a	O
nonuniform	O
learner	O
for	O
h	O
is	O
weaker	O
it	O
is	O
searching	O
for	O
a	O
model	O
throughout	O
the	O
entire	O
class	O
h	O
rather	O
than	O
being	O
focused	O
on	O
one	O
specific	O
hn	O
the	O
cost	O
of	O
this	O
weakening	O
of	O
prior	B
knowledge	I
is	O
the	O
increase	O
in	O
sample	B
complexity	I
needed	O
to	O
compete	O
with	O
any	O
specific	O
h	O
hn	O
for	O
a	O
concrete	O
evaluation	O
of	O
this	O
gap	O
consider	O
the	O
task	O
of	O
binary	O
classification	O
with	O
the	O
zero-one	O
loss	B
assume	O
that	O
for	O
all	O
n	O
vcdimhn	O
n	O
since	O
muchn	O
c	O
is	O
the	O
contant	O
appearing	O
in	O
theorem	O
a	O
straightforward	O
calculation	O
shows	O
that	O
c	O
mnulh	O
h	O
muchn	O
that	O
is	O
the	O
cost	O
of	O
relaxing	O
the	O
learner	O
s	O
prior	B
knowledge	I
from	O
a	O
specific	O
hn	O
that	O
contains	O
the	O
target	O
h	O
to	O
a	O
countable	O
union	O
of	O
classes	O
depends	O
on	O
the	O
log	O
of	O
minimum	O
description	O
length	O
and	O
occam	O
s	O
razor	O
the	O
index	O
of	O
the	O
first	O
class	O
in	O
which	O
h	O
resides	O
that	O
cost	O
increases	O
with	O
the	O
index	O
of	O
the	O
class	O
which	O
can	O
be	O
interpreted	O
as	O
reflecting	O
the	O
value	O
of	O
knowing	O
a	O
good	O
priority	O
order	O
on	O
the	O
hypotheses	O
in	O
h	O
union	O
of	O
singleton	O
classes	O
namely	O
h	O
minimum	O
description	O
length	O
and	O
occam	O
s	O
razor	O
let	O
h	O
be	O
a	O
countable	O
hypothesis	B
class	I
then	O
we	O
can	O
write	O
h	O
as	O
a	O
countable	O
n	O
nhn	O
by	O
hoeffding	O
s	O
inequality	O
each	O
singleton	O
class	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
rate	O
muc	O
therefore	O
the	O
function	B
given	O
in	O
equation	O
becomes	O
argmin	O
hn	O
h	O
lsh	O
and	O
the	O
srm	B
rule	O
becomes	O
logwn	O
equivalently	O
we	O
can	O
think	O
of	O
w	O
as	O
a	O
function	B
from	O
h	O
to	O
and	O
then	O
the	O
srm	B
rule	O
becomes	O
logwh	O
argmin	O
h	O
h	O
lsh	O
it	O
follows	O
that	O
in	O
this	O
case	O
the	O
prior	B
knowledge	I
is	O
solely	O
determined	O
by	O
the	O
weight	O
we	O
assign	O
to	O
each	O
hypothesis	B
we	O
assign	O
higher	O
weights	O
to	O
hypotheses	O
that	O
we	O
believe	O
are	O
more	O
likely	O
to	O
be	O
the	O
correct	O
one	O
and	O
in	O
the	O
learning	O
algorithm	O
we	O
prefer	O
hypotheses	O
that	O
have	O
higher	O
weights	O
in	O
this	O
section	O
we	O
discuss	O
a	O
particular	O
convenient	O
way	O
to	O
define	O
a	O
weight	O
function	B
over	O
h	O
which	O
is	O
derived	O
from	O
the	O
length	O
of	O
descriptions	O
given	O
to	O
hypotheses	O
having	O
a	O
hypothesis	B
class	I
one	O
can	O
wonder	O
about	O
how	O
we	O
describe	O
or	O
represent	O
each	O
hypothesis	B
in	O
the	O
class	O
we	O
naturally	O
fix	O
some	O
description	O
language	O
this	O
can	O
be	O
english	O
or	O
a	O
programming	O
language	O
or	O
some	O
set	B
of	O
mathematical	O
formulas	O
in	O
any	O
of	O
these	O
languages	O
a	O
description	O
consists	O
of	O
finite	O
strings	O
of	O
symbols	O
characters	O
drawn	O
from	O
some	O
fixed	O
alphabet	O
we	O
shall	O
now	O
formalize	O
these	O
notions	O
let	O
h	O
be	O
the	O
hypothesis	B
class	I
we	O
wish	O
to	O
describe	O
fix	O
some	O
finite	O
set	B
of	O
symbols	O
characters	O
which	O
we	O
call	O
the	O
alphabet	O
for	O
concreteness	O
we	O
let	O
a	O
string	O
is	O
a	O
finite	O
sequence	O
of	O
symbols	O
from	O
for	O
example	O
is	O
a	O
string	O
of	O
length	O
we	O
denote	O
by	O
the	O
length	O
of	O
a	O
string	O
the	O
set	B
of	O
all	O
finite	O
length	O
strings	O
is	O
denoted	O
a	O
description	O
language	O
for	O
h	O
is	O
a	O
function	B
d	O
h	O
mapping	O
each	O
member	O
h	O
of	O
h	O
to	O
a	O
string	O
dh	O
dh	O
is	O
called	O
the	O
description	O
of	O
h	O
and	O
its	O
length	O
is	O
denoted	O
by	O
we	O
shall	O
require	O
that	O
description	O
languages	O
be	O
prefix-free	O
namely	O
for	O
every	O
distinct	O
h	O
dh	O
is	O
not	O
a	O
prefix	O
of	O
that	O
is	O
we	O
do	O
not	O
allow	O
that	O
any	O
string	O
dh	O
is	O
exactly	O
the	O
first	O
symbols	O
of	O
any	O
longer	O
string	O
prefix-free	O
collections	O
of	O
strings	O
enjoy	O
the	O
following	O
combinatorial	O
property	O
nonuniform	O
learnability	O
lemma	O
inequality	O
if	O
s	O
is	O
a	O
prefix-free	O
set	B
of	O
strings	O
then	O
s	O
proof	O
define	O
a	O
probability	O
distribution	O
over	O
the	O
members	O
of	O
s	O
as	O
follows	O
repeatedly	O
toss	O
an	O
unbiased	O
coin	O
with	O
faces	O
labeled	O
and	O
until	O
the	O
sequence	O
of	O
outcomes	O
is	O
a	O
member	O
of	O
s	O
at	O
that	O
point	O
stop	O
for	O
each	O
s	O
let	O
p	O
be	O
the	O
probability	O
that	O
this	O
process	O
generates	O
the	O
string	O
note	O
that	O
since	O
s	O
is	O
prefix-free	O
for	O
every	O
s	O
if	O
the	O
coin	O
toss	O
outcomes	O
follow	O
the	O
bits	O
of	O
then	O
we	O
will	O
stop	O
only	O
once	O
the	O
sequence	O
of	O
outcomes	O
equals	O
we	O
therefore	O
get	O
that	O
for	O
every	O
s	O
p	O
since	O
probabilities	O
add	O
up	O
to	O
at	O
most	O
our	O
proof	O
is	O
concluded	O
in	O
light	O
of	O
kraft	O
s	O
inequality	O
any	O
prefix-free	O
description	O
language	O
of	O
a	O
hypothesis	B
class	I
h	O
gives	O
rise	O
to	O
a	O
weighting	O
function	B
w	O
over	O
that	O
hypothesis	B
class	I
we	O
will	O
simply	O
set	B
wh	O
this	O
observation	O
immediately	O
yields	O
the	O
following	O
theorem	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
and	O
let	O
d	O
h	O
be	O
a	O
prefixfree	O
description	O
language	O
for	O
h	O
then	O
for	O
every	O
sample	O
size	O
m	O
every	O
confidence	B
parameter	O
and	O
every	O
probability	O
distribution	O
d	O
with	O
probability	O
greater	O
than	O
over	O
the	O
choice	O
of	O
s	O
dm	O
we	O
have	O
that	O
h	O
h	O
ldh	O
lsh	O
where	O
is	O
the	O
length	O
of	O
dh	O
proof	O
choose	O
wh	O
apply	O
theorem	O
with	O
note	O
that	O
and	O
as	O
was	O
the	O
case	O
with	O
theorem	O
this	O
result	O
suggests	O
a	O
learning	O
paradigm	O
for	O
h	O
given	O
a	O
training	B
set	B
s	O
search	O
for	O
a	O
hypothesis	B
h	O
h	O
that	O
minimizes	O
the	O
bound	O
lsh	O
in	O
particular	O
it	O
suggests	O
trading	O
off	O
empirical	B
risk	B
for	O
saving	O
description	O
length	O
this	O
yields	O
the	O
minimum	O
description	O
length	O
learning	O
paradigm	O
minimum	O
description	O
length	O
prior	B
knowledge	I
h	O
is	O
a	O
countable	O
hypothesis	B
class	I
h	O
is	O
described	O
by	O
a	O
prefix-free	O
language	O
over	O
for	O
every	O
h	O
h	O
is	O
the	O
length	O
of	O
the	O
representation	O
of	O
h	O
input	O
a	O
training	B
set	B
s	O
dm	O
confidence	B
output	O
h	O
argminh	O
h	O
lsh	O
example	O
let	O
h	O
be	O
the	O
class	O
of	O
all	O
predictors	O
that	O
can	O
be	O
implemented	O
using	O
some	O
programming	O
language	O
say	O
c	O
let	O
us	O
represent	O
each	O
program	O
using	O
the	O
minimum	O
description	O
length	O
and	O
occam	O
s	O
razor	O
binary	O
string	O
obtained	O
by	O
running	O
the	O
gzip	O
command	O
on	O
the	O
program	O
yields	O
a	O
prefix-free	O
description	O
language	O
over	O
the	O
alphabet	O
then	O
is	O
simply	O
the	O
length	O
bits	O
of	O
the	O
output	O
of	O
gzip	O
when	O
running	O
on	O
the	O
c	O
program	O
corresponding	O
to	O
h	O
occam	O
s	O
razor	O
theorem	O
suggests	O
that	O
having	O
two	O
hypotheses	O
sharing	O
the	O
same	O
empirical	B
risk	B
the	O
true	O
risk	B
of	O
the	O
one	O
that	O
has	O
shorter	O
description	O
can	O
be	O
bounded	O
by	O
a	O
lower	O
value	O
thus	O
this	O
result	O
can	O
be	O
viewed	O
as	O
conveying	O
a	O
philosophical	O
message	O
a	O
short	O
explanation	O
is	O
a	O
hypothesis	B
that	O
has	O
a	O
short	O
length	O
tends	O
to	O
be	O
more	O
valid	O
than	O
a	O
long	O
explanation	O
this	O
is	O
a	O
well	O
known	O
principle	O
called	O
occam	O
s	O
razor	O
after	O
william	O
of	O
ockham	O
a	O
english	O
logician	O
who	O
is	O
believed	O
to	O
have	O
been	O
the	O
first	O
to	O
phrase	O
it	O
explicitly	O
here	O
we	O
provide	O
one	O
possible	O
justification	O
to	O
this	O
principle	O
the	O
inequality	O
of	O
theorem	O
shows	O
that	O
the	O
more	O
complex	O
a	O
hypothesis	B
h	O
is	O
the	O
sense	O
of	O
having	O
a	O
longer	O
description	O
the	O
larger	O
the	O
sample	O
size	O
it	O
has	O
to	O
fit	O
to	O
guarantee	O
that	O
it	O
has	O
a	O
small	O
true	O
risk	B
ldh	O
at	O
a	O
second	O
glance	O
our	O
occam	O
razor	O
claim	O
might	O
seem	O
somewhat	O
problematic	O
in	O
the	O
context	O
in	O
which	O
the	O
occam	O
razor	O
principle	O
is	O
usually	O
invoked	O
in	O
science	O
the	O
language	O
according	O
to	O
which	O
complexity	O
is	O
measured	O
is	O
a	O
natural	O
language	O
whereas	O
here	O
we	O
may	O
consider	O
any	O
arbitrary	O
abstract	O
description	O
language	O
assume	O
that	O
we	O
have	O
two	O
hypotheses	O
such	O
that	O
is	O
much	O
smaller	O
than	O
by	O
the	O
preceding	O
result	O
if	O
both	O
have	O
the	O
same	O
error	O
on	O
a	O
given	O
training	B
set	B
s	O
then	O
the	O
true	B
error	I
of	O
h	O
may	O
be	O
much	O
higher	O
than	O
the	O
true	B
error	I
of	O
so	O
one	O
should	O
prefer	O
over	O
h	O
however	O
we	O
could	O
have	O
chosen	O
a	O
different	O
description	O
language	O
say	O
one	O
that	O
assigns	O
a	O
string	O
of	O
length	O
to	O
h	O
and	O
a	O
string	O
of	O
length	O
to	O
suddenly	O
it	O
looks	O
as	O
if	O
one	O
should	O
prefer	O
h	O
over	O
but	O
these	O
are	O
the	O
same	O
h	O
and	O
for	O
which	O
we	O
argued	O
two	O
sentences	O
ago	O
that	O
should	O
be	O
preferable	O
where	O
is	O
the	O
catch	O
here	O
indeed	O
there	O
is	O
no	O
inherent	O
generalizability	O
difference	O
between	O
hypotheses	O
the	O
crucial	O
aspect	O
here	O
is	O
the	O
dependency	O
order	O
between	O
the	O
initial	O
choice	O
of	O
language	O
preference	O
over	O
hypotheses	O
and	O
the	O
training	B
set	B
as	O
we	O
know	O
from	O
the	O
basic	O
hoeffding	O
s	O
bound	O
if	O
we	O
commit	O
to	O
any	O
hypothesis	B
before	O
seeing	O
the	O
data	O
then	O
we	O
are	O
guaranteed	O
a	O
rather	O
small	O
estimation	B
error	I
term	O
ldh	O
lsh	O
choosing	O
a	O
description	O
language	O
equivalently	O
some	O
weighting	O
of	O
hypotheses	O
is	O
a	O
weak	O
form	O
of	O
committing	O
to	O
a	O
hypothesis	B
rather	O
than	O
committing	O
to	O
a	O
single	O
hypothesis	B
we	O
spread	O
out	O
our	O
commitment	O
among	O
many	O
as	O
long	O
as	O
it	O
is	O
done	O
independently	O
of	O
the	O
training	O
sample	O
our	O
generalization	O
bound	O
holds	O
just	O
as	O
the	O
choice	O
of	O
a	O
single	O
hypothesis	B
to	O
be	O
evaluated	O
by	O
a	O
sample	O
can	O
be	O
arbitrary	O
so	O
is	O
the	O
choice	O
of	O
description	O
language	O
nonuniform	O
learnability	O
other	O
notions	O
of	O
learnability	O
consistency	B
the	O
notion	O
of	O
learnability	O
can	O
be	O
further	O
relaxed	O
by	O
allowing	O
the	O
needed	O
sample	O
sizes	O
to	O
depend	O
not	O
only	O
on	O
and	O
h	O
but	O
also	O
on	O
the	O
underlying	O
data-generating	O
probability	O
distribution	O
d	O
is	O
used	O
to	O
generate	O
the	O
training	O
sample	O
and	O
to	O
determine	O
the	O
risk	B
this	O
type	O
of	O
performance	O
guarantee	O
is	O
captured	O
by	O
the	O
notion	O
of	O
consistency	B
of	O
a	O
learning	O
rule	O
definition	O
let	O
z	O
be	O
a	O
domain	B
set	B
let	O
p	O
be	O
a	O
set	B
of	O
probability	O
distributions	O
over	O
z	O
and	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
a	O
learning	O
rule	O
a	O
is	O
consistent	B
with	O
respect	O
to	O
h	O
and	O
p	O
if	O
there	O
exists	O
a	O
function	B
mconh	O
h	O
p	O
n	O
such	O
that	O
for	O
every	O
every	O
h	O
h	O
and	O
every	O
d	O
p	O
if	O
m	O
mnulh	O
hd	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
it	O
holds	O
that	O
ldas	O
ldh	O
if	O
p	O
is	O
the	O
set	B
of	O
all	O
we	O
say	O
that	O
a	O
is	O
universally	O
consistent	B
with	O
respect	O
to	O
h	O
the	O
notion	O
of	O
consistency	B
is	O
of	O
course	O
a	O
relaxation	O
of	O
our	O
previous	O
notion	O
of	O
nonuniform	O
learnability	O
clearly	O
if	O
an	O
algorithm	O
nonuniformly	O
learns	O
a	O
class	O
h	O
it	O
is	O
also	O
universally	O
consistent	B
for	O
that	O
class	O
the	O
relaxation	O
is	O
strict	O
in	O
the	O
sense	O
that	O
there	O
are	O
consistent	B
learning	O
rules	O
that	O
are	O
not	O
successful	O
nonuniform	O
learners	O
for	O
example	O
the	O
algorithm	O
memorize	O
defined	O
in	O
example	O
later	O
is	O
universally	O
consistent	B
for	O
the	O
class	O
of	O
all	O
binary	O
classifiers	O
over	O
n	O
however	O
as	O
we	O
have	O
argued	O
before	O
this	O
class	O
is	O
not	O
nonuniformly	O
learnable	O
example	O
consider	O
the	O
classification	O
prediction	O
algorithm	O
memorize	O
defined	O
as	O
follows	O
the	O
algorithm	O
memorizes	O
the	O
training	O
examples	O
and	O
given	O
a	O
test	O
point	O
x	O
it	O
predicts	O
the	O
majority	O
label	B
among	O
all	O
labeled	O
instances	O
of	O
x	O
that	O
exist	O
in	O
the	O
training	O
sample	O
some	O
fixed	O
default	O
label	B
if	O
no	O
instance	B
of	O
x	O
appears	O
in	O
the	O
training	B
set	B
it	O
is	O
possible	O
to	O
show	O
exercise	O
that	O
the	O
memorize	O
algorithm	O
is	O
universally	O
consistent	B
for	O
every	O
countable	O
domain	B
x	O
and	O
a	O
finite	O
label	B
set	B
y	O
the	O
zero-one	O
loss	B
intuitively	O
it	O
is	O
not	O
obvious	O
that	O
the	O
memorize	O
algorithm	O
should	O
be	O
viewed	O
as	O
a	O
learner	O
since	O
it	O
lacks	O
the	O
aspect	O
of	O
generalization	O
namely	O
of	O
using	O
observed	O
data	O
to	O
predict	O
the	O
labels	O
of	O
unseen	O
examples	O
the	O
fact	O
that	O
memorize	O
is	O
a	O
consistent	B
algorithm	O
for	O
the	O
class	O
of	O
all	O
functions	O
over	O
any	O
countable	O
domain	B
set	B
therefore	O
raises	O
doubt	O
about	O
the	O
usefulness	O
of	O
consistency	B
guarantees	O
furthermore	O
the	O
sharp-eyed	O
reader	O
may	O
notice	O
that	O
the	O
bad	O
learner	O
we	O
introduced	O
in	O
chapter	O
in	O
the	O
literature	O
consistency	B
is	O
often	O
defined	O
using	O
the	O
notion	O
of	O
either	O
convergence	O
in	O
probability	O
to	O
weak	O
consistency	B
or	O
almost	O
sure	O
convergence	O
to	O
strong	O
consistency	B
formally	O
we	O
assume	O
that	O
z	O
is	O
endowed	O
with	O
some	O
sigma	O
algebra	O
of	O
subsets	O
and	O
by	O
all	O
distributions	O
we	O
mean	O
all	O
probability	O
distributions	O
that	O
have	O
contained	O
in	O
their	O
associated	O
family	O
of	O
measurable	O
subsets	O
discussing	O
the	O
different	O
notions	O
of	O
learnability	O
which	O
led	O
to	O
overfitting	B
is	O
in	O
fact	O
the	O
memorize	O
algorithm	O
in	O
the	O
next	O
section	O
we	O
discuss	O
the	O
significance	O
of	O
the	O
different	O
notions	O
of	O
learnability	O
and	O
revisit	O
the	O
no-free-lunch	B
theorem	O
in	O
light	O
of	O
the	O
different	O
definitions	O
of	O
learnability	O
discussing	O
the	O
different	O
notions	O
of	O
learnability	O
we	O
have	O
given	O
three	O
definitions	O
of	O
learnability	O
and	O
we	O
now	O
discuss	O
their	O
usefulness	O
as	O
is	O
usually	O
the	O
case	O
the	O
usefulness	O
of	O
a	O
mathematical	O
definition	O
depends	O
on	O
what	O
we	O
need	O
it	O
for	O
we	O
therefore	O
list	O
several	O
possible	O
goals	O
that	O
we	O
aim	O
to	O
achieve	O
by	O
defining	O
learnability	O
and	O
discuss	O
the	O
usefulness	O
of	O
the	O
different	O
definitions	O
in	O
light	O
of	O
these	O
goals	O
what	O
is	O
the	O
risk	B
of	O
the	O
learned	O
hypothesis	B
the	O
first	O
possible	O
goal	O
of	O
deriving	O
performance	O
guarantees	O
on	O
a	O
learning	O
algorithm	O
is	O
bounding	O
the	O
risk	B
of	O
the	O
output	O
predictor	B
here	O
both	O
pac	B
learning	O
and	O
nonuniform	O
learning	O
give	O
us	O
an	O
upper	O
bound	O
on	O
the	O
true	O
risk	B
of	O
the	O
learned	O
hypothesis	B
based	O
on	O
its	O
empirical	B
risk	B
consistency	B
guarantees	O
do	O
not	O
provide	O
such	O
a	O
bound	O
however	O
it	O
is	O
always	O
possible	O
to	O
estimate	O
the	O
risk	B
of	O
the	O
output	O
predictor	B
using	O
a	O
validation	B
set	B
will	O
be	O
described	O
in	O
chapter	O
how	O
many	O
examples	O
are	O
required	O
to	O
be	O
as	O
good	O
as	O
the	O
best	O
hypothesis	B
in	O
h	O
when	O
approaching	O
a	O
learning	O
problem	O
a	O
natural	O
question	O
is	O
how	O
many	O
examples	O
we	O
need	O
to	O
collect	O
in	O
order	O
to	O
learn	O
it	O
here	O
pac	B
learning	O
gives	O
a	O
crisp	O
answer	O
however	O
for	O
both	O
nonuniform	O
learning	O
and	O
consistency	B
we	O
do	O
not	O
know	O
in	O
advance	O
how	O
many	O
examples	O
are	O
required	O
to	O
learn	O
h	O
in	O
nonuniform	O
learning	O
this	O
number	O
depends	O
on	O
the	O
best	O
hypothesis	B
in	O
h	O
and	O
in	O
consistency	B
it	O
also	O
depends	O
on	O
the	O
underlying	O
distribution	O
in	O
this	O
sense	O
pac	B
learning	O
is	O
the	O
only	O
useful	O
definition	O
of	O
learnability	O
on	O
the	O
flip	O
side	O
one	O
should	O
keep	O
in	O
mind	O
that	O
even	O
if	O
the	O
estimation	B
error	I
of	O
the	O
predictor	B
we	O
learn	O
is	O
small	O
its	O
risk	B
may	O
still	O
be	O
large	O
if	O
h	O
has	O
a	O
large	O
approximation	B
error	I
so	O
for	O
the	O
question	O
how	O
many	O
examples	O
are	O
required	O
to	O
be	O
as	O
good	O
as	O
the	O
bayes	B
optimal	I
predictor	B
even	O
pac	B
guarantees	O
do	O
not	O
provide	O
us	O
with	O
a	O
crisp	O
answer	O
this	O
reflects	O
the	O
fact	O
that	O
the	O
usefulness	O
of	O
pac	B
learning	O
relies	O
on	O
the	O
quality	O
of	O
our	O
prior	B
knowledge	I
pac	B
guarantees	O
also	O
help	O
us	O
to	O
understand	O
what	O
we	O
should	O
do	O
next	O
if	O
our	O
learning	O
algorithm	O
returns	O
a	O
hypothesis	B
with	O
a	O
large	O
risk	B
since	O
we	O
can	O
bound	O
the	O
part	O
of	O
the	O
error	O
that	O
stems	O
from	O
estimation	B
error	I
and	O
therefore	O
know	O
how	O
much	O
of	O
the	O
error	O
is	O
attributed	O
to	O
approximation	B
error	I
if	O
the	O
approximation	B
error	I
is	O
large	O
we	O
know	O
that	O
we	O
should	O
use	O
a	O
different	O
hypothesis	B
class	I
similarly	O
if	O
a	O
nonuniform	O
algorithm	O
fails	O
we	O
can	O
consider	O
a	O
different	O
weighting	O
function	B
over	O
of	O
hypotheses	O
however	O
when	O
a	O
consistent	B
algorithm	O
fails	O
we	O
have	O
no	O
idea	O
whether	O
this	O
is	O
because	O
of	O
the	O
estimation	B
error	I
or	O
the	O
approximation	B
error	I
furthermore	O
even	O
if	O
we	O
are	O
sure	O
we	O
have	O
a	O
problem	O
with	O
the	O
estimation	O
nonuniform	O
learnability	O
error	O
term	O
we	O
do	O
not	O
know	O
how	O
many	O
more	O
examples	O
are	O
needed	O
to	O
make	O
the	O
estimation	B
error	I
small	O
how	O
to	O
learn	O
how	O
to	O
express	O
prior	B
knowledge	I
maybe	O
the	O
most	O
useful	O
aspect	O
of	O
the	O
theory	O
of	O
learning	O
is	O
in	O
providing	O
an	O
answer	O
to	O
the	O
question	O
of	O
how	O
to	O
learn	O
the	O
definition	O
of	O
pac	B
learning	O
yields	O
the	O
limitation	O
of	O
learning	O
the	O
no-free-lunch	B
theorem	O
and	O
the	O
necessity	O
of	O
prior	B
knowledge	I
it	O
gives	O
us	O
a	O
crisp	O
way	O
to	O
encode	O
prior	B
knowledge	I
by	O
choosing	O
a	O
hypothesis	B
class	I
and	O
once	O
this	O
choice	O
is	O
made	O
we	O
have	O
a	O
generic	O
learning	O
rule	O
erm	B
the	O
definition	O
of	O
nonuniform	O
learnability	O
also	O
yields	O
a	O
crisp	O
way	O
to	O
encode	O
prior	B
knowledge	I
by	O
specifying	O
weights	O
over	O
of	O
hypotheses	O
of	O
h	O
once	O
this	O
choice	O
is	O
made	O
we	O
again	O
have	O
a	O
generic	O
learning	O
rule	O
srm	B
the	O
srm	B
rule	O
is	O
also	O
advantageous	O
in	O
model	B
selection	I
tasks	O
where	O
prior	B
knowledge	I
is	O
partial	O
we	O
elaborate	O
on	O
model	B
selection	I
in	O
chapter	O
and	O
here	O
we	O
give	O
a	O
brief	O
example	O
consider	O
the	O
problem	O
of	O
fitting	O
a	O
one	O
dimensional	O
polynomial	O
to	O
data	O
namely	O
our	O
goal	O
is	O
to	O
learn	O
a	O
function	B
h	O
r	O
r	O
and	O
as	O
prior	B
knowledge	I
we	O
consider	O
the	O
hypothesis	B
class	I
of	O
polynomials	O
however	O
we	O
might	O
be	O
uncertain	O
regarding	O
which	O
degree	O
d	O
would	O
give	O
the	O
best	O
results	O
for	O
our	O
data	O
set	B
a	O
small	O
degree	O
might	O
not	O
fit	O
the	O
data	O
well	O
it	O
will	O
have	O
a	O
large	O
approximation	B
error	I
whereas	O
a	O
high	O
degree	O
might	O
lead	O
to	O
overfitting	B
it	O
will	O
have	O
a	O
large	O
estimation	B
error	I
in	O
the	O
following	O
we	O
depict	O
the	O
result	O
of	O
fitting	O
a	O
polynomial	O
of	O
degrees	O
and	O
to	O
the	O
same	O
training	B
set	B
degree	O
degree	O
degree	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
empirical	B
risk	B
decreases	O
as	O
we	O
enlarge	O
the	O
degree	O
therefore	O
if	O
we	O
choose	O
h	O
to	O
be	O
the	O
class	O
of	O
all	O
polynomials	O
up	O
to	O
degree	O
then	O
the	O
erm	B
rule	O
with	O
respect	O
to	O
this	O
class	O
would	O
output	O
a	O
degree	O
polynomial	O
and	O
would	O
overfit	O
on	O
the	O
other	O
hand	O
if	O
we	O
choose	O
too	O
small	O
a	O
hypothesis	B
class	I
say	O
polynomials	O
up	O
to	O
degree	O
then	O
the	O
erm	B
would	O
suffer	O
from	O
underfitting	B
a	O
large	O
approximation	B
error	I
in	O
contrast	O
we	O
can	O
use	O
the	O
srm	B
rule	O
on	O
the	O
set	B
of	O
all	O
polynomials	O
while	O
ordering	O
subsets	O
of	O
h	O
according	O
to	O
their	O
degree	O
and	O
this	O
will	O
yield	O
a	O
degree	O
polynomial	O
since	O
the	O
combination	O
of	O
its	O
empirical	B
risk	B
and	O
the	O
bound	O
on	O
its	O
estimation	B
error	I
is	O
the	O
smallest	O
in	O
other	O
words	O
the	O
srm	B
rule	O
enables	O
us	O
to	O
select	O
the	O
right	O
model	O
on	O
the	O
basis	O
of	O
the	O
data	O
itself	O
the	O
price	O
we	O
pay	O
for	O
this	O
flexibility	O
a	O
slight	O
increase	O
of	O
the	O
estimation	B
error	I
relative	O
to	O
pac	B
learning	O
w	O
r	O
t	O
the	O
optimal	O
degree	O
is	O
that	O
we	O
do	O
not	O
know	O
in	O
discussing	O
the	O
different	O
notions	O
of	O
learnability	O
advance	O
how	O
many	O
examples	O
are	O
needed	O
to	O
compete	O
with	O
the	O
best	O
hypothesis	B
in	O
h	O
unlike	O
the	O
notions	O
of	O
pac	B
learnability	O
and	O
nonuniform	O
learnability	O
the	O
definition	O
of	O
consistency	B
does	O
not	O
yield	O
a	O
natural	O
learning	O
paradigm	O
or	O
a	O
way	O
to	O
encode	O
prior	B
knowledge	I
in	O
fact	O
in	O
many	O
cases	O
there	O
is	O
no	O
need	O
for	O
prior	B
knowledge	I
at	O
all	O
for	O
example	O
we	O
saw	O
that	O
even	O
the	O
memorize	O
algorithm	O
which	O
intuitively	O
should	O
not	O
be	O
called	O
a	O
learning	O
algorithm	O
is	O
a	O
consistent	B
algorithm	O
for	O
any	O
class	O
defined	O
over	O
a	O
countable	O
domain	B
and	O
a	O
finite	O
label	B
set	B
this	O
hints	O
that	O
consistency	B
is	O
a	O
very	O
weak	O
requirement	O
which	O
learning	O
algorithm	O
should	O
we	O
prefer	O
one	O
may	O
argue	O
that	O
even	O
though	O
consistency	B
is	O
a	O
weak	O
requirement	O
it	O
is	O
desirable	O
that	O
a	O
learning	O
algorithm	O
will	O
be	O
consistent	B
with	O
respect	O
to	O
the	O
set	B
of	O
all	O
functions	O
from	O
x	O
to	O
y	O
which	O
gives	O
us	O
a	O
guarantee	O
that	O
for	O
enough	O
training	O
examples	O
we	O
will	O
always	O
be	O
as	O
good	O
as	O
the	O
bayes	B
optimal	I
predictor	B
therefore	O
if	O
we	O
have	O
two	O
algorithms	O
where	O
one	O
is	O
consistent	B
and	O
the	O
other	O
one	O
is	O
not	O
consistent	B
we	O
should	O
prefer	O
the	O
consistent	B
algorithm	O
however	O
this	O
argument	O
is	O
problematic	O
for	O
two	O
reasons	O
first	O
maybe	O
it	O
is	O
the	O
case	O
that	O
for	O
most	O
natural	O
distributions	O
we	O
will	O
observe	O
in	O
practice	O
that	O
the	O
sample	B
complexity	I
of	O
the	O
consistent	B
algorithm	O
will	O
be	O
so	O
large	O
so	O
that	O
in	O
every	O
practical	O
situation	O
we	O
will	O
not	O
obtain	O
enough	O
examples	O
to	O
enjoy	O
this	O
guarantee	O
second	O
it	O
is	O
not	O
very	O
hard	O
to	O
make	O
any	O
pac	B
or	O
nonuniform	O
learner	O
consistent	B
with	O
respect	O
to	O
the	O
class	O
of	O
all	O
functions	O
from	O
x	O
to	O
y	O
concretely	O
consider	O
a	O
countable	O
domain	B
x	O
a	O
finite	O
label	B
set	B
y	O
and	O
a	O
hypothesis	B
class	I
h	O
of	O
functions	O
from	O
x	O
to	O
y	O
we	O
can	O
make	O
any	O
nonuniform	O
learner	O
for	O
h	O
be	O
consistent	B
with	O
respect	O
to	O
the	O
class	O
of	O
all	O
classifiers	O
from	O
x	O
to	O
y	O
using	O
the	O
following	O
simple	O
trick	O
upon	O
receiving	O
a	O
training	B
set	B
we	O
will	O
first	O
run	O
the	O
nonuniform	O
learner	O
over	O
the	O
training	B
set	B
and	O
then	O
we	O
will	O
obtain	O
a	O
bound	O
on	O
the	O
true	O
risk	B
of	O
the	O
learned	O
predictor	B
if	O
this	O
bound	O
is	O
small	O
enough	O
we	O
are	O
done	O
otherwise	O
we	O
revert	O
to	O
the	O
memorize	O
algorithm	O
this	O
simple	O
modification	O
makes	O
the	O
algorithm	O
consistent	B
with	O
respect	O
to	O
all	O
functions	O
from	O
x	O
to	O
y	O
since	O
it	O
is	O
easy	O
to	O
make	O
any	O
algorithm	O
consistent	B
it	O
may	O
not	O
be	O
wise	O
to	O
prefer	O
one	O
algorithm	O
over	O
the	O
other	O
just	O
because	O
of	O
consistency	B
considerations	O
the	O
no-free-lunch	B
theorem	O
revisited	O
recall	B
that	O
the	O
no-free-lunch	B
theorem	O
from	O
chapter	O
implies	O
that	O
no	O
algorithm	O
can	O
learn	O
the	O
class	O
of	O
all	O
classifiers	O
over	O
an	O
infinite	O
domain	B
in	O
contrast	O
in	O
this	O
chapter	O
we	O
saw	O
that	O
the	O
memorize	O
algorithm	O
is	O
consistent	B
with	O
respect	O
to	O
the	O
class	O
of	O
all	O
classifiers	O
over	O
a	O
countable	O
infinite	O
domain	B
to	O
understand	O
why	O
these	O
two	O
statements	O
do	O
not	O
contradict	O
each	O
other	O
let	O
us	O
first	O
recall	B
the	O
formal	O
statement	O
of	O
the	O
no-free-lunch	B
theorem	O
let	O
x	O
be	O
a	O
countable	O
infinite	O
domain	B
and	O
let	O
y	O
the	O
no-free-lunch	B
theorem	O
implies	O
the	O
following	O
for	O
any	O
algorithm	O
a	O
and	O
a	O
training	B
set	B
size	O
m	O
there	O
exist	O
a	O
distribution	O
over	O
x	O
and	O
a	O
function	B
x	O
y	O
such	O
that	O
if	O
a	O
nonuniform	O
learnability	O
will	O
get	O
a	O
sample	O
of	O
m	O
i	O
i	O
d	O
training	O
examples	O
labeled	O
by	O
then	O
a	O
is	O
likely	O
to	O
return	O
a	O
classifier	B
with	O
a	O
larger	O
error	O
the	O
consistency	B
of	O
memorize	O
implies	O
the	O
following	O
for	O
every	O
distribution	O
over	O
x	O
and	O
a	O
labeling	O
function	B
x	O
y	O
there	O
exists	O
a	O
training	B
set	B
size	O
m	O
depends	O
on	O
the	O
distribution	O
and	O
on	O
such	O
that	O
if	O
memorize	O
receives	O
at	O
least	O
m	O
examples	O
it	O
is	O
likely	O
to	O
return	O
a	O
classifier	B
with	O
a	O
small	O
error	O
we	O
see	O
that	O
in	O
the	O
no-free-lunch	B
theorem	O
we	O
first	O
fix	O
the	O
training	B
set	B
size	O
and	O
then	O
find	O
a	O
distribution	O
and	O
a	O
labeling	O
function	B
that	O
are	O
bad	O
for	O
this	O
training	B
set	B
size	O
in	O
contrast	O
in	O
consistency	B
guarantees	O
we	O
first	O
fix	O
the	O
distribution	O
and	O
the	O
labeling	O
function	B
and	O
only	O
then	O
do	O
we	O
find	O
a	O
training	B
set	B
size	O
that	O
suffices	O
for	O
learning	O
this	O
particular	O
distribution	O
and	O
labeling	O
function	B
summary	O
we	O
introduced	O
nonuniform	O
learnability	O
as	O
a	O
relaxation	O
of	O
pac	B
learnability	O
and	O
consistency	B
as	O
a	O
relaxation	O
of	O
nonuniform	O
learnability	O
this	O
means	O
that	O
even	O
classes	O
of	O
infinite	O
vc-dimension	O
can	O
be	O
learnable	O
in	O
some	O
weaker	O
sense	O
of	O
learnability	O
we	O
discussed	O
the	O
usefulness	O
of	O
the	O
different	O
definitions	O
of	O
learnability	O
for	O
hypothesis	B
classes	O
that	O
are	O
countable	O
we	O
can	O
apply	O
the	O
minimum	O
description	O
length	O
scheme	O
where	O
hypotheses	O
with	O
shorter	O
descriptions	O
are	O
preferred	O
following	O
the	O
principle	O
of	O
occam	O
s	O
razor	O
an	O
interesting	O
example	O
is	O
the	O
hypothesis	B
class	I
of	O
all	O
predictors	O
we	O
can	O
implement	O
in	O
c	O
any	O
other	O
programming	O
language	O
which	O
we	O
can	O
learn	O
using	O
the	O
mdl	B
scheme	O
arguably	O
the	O
class	O
of	O
all	O
predictors	O
we	O
can	O
implement	O
in	O
c	O
is	O
a	O
powerful	O
class	O
of	O
functions	O
and	O
probably	O
contains	O
all	O
that	O
we	O
can	O
hope	O
to	O
learn	O
in	O
practice	O
the	O
ability	O
to	O
learn	O
this	O
class	O
is	O
impressive	O
and	O
seemingly	O
this	O
chapter	O
should	O
have	O
been	O
the	O
last	O
chapter	O
of	O
this	O
book	O
this	O
is	O
not	O
the	O
case	O
because	O
of	O
the	O
computational	O
aspect	O
of	O
learning	O
that	O
is	O
the	O
runtime	O
needed	O
to	O
apply	O
the	O
learning	O
rule	O
for	O
example	O
to	O
implement	O
the	O
mdl	B
paradigm	O
with	O
respect	O
to	O
all	O
c	O
programs	O
we	O
need	O
to	O
perform	O
an	O
exhaustive	O
search	O
over	O
all	O
c	O
programs	O
which	O
will	O
take	O
forever	O
even	O
the	O
implementation	O
of	O
the	O
erm	B
paradigm	O
with	O
respect	O
to	O
all	O
c	O
programs	O
of	O
description	O
length	O
at	O
most	O
bits	O
requires	O
an	O
exhaustive	O
search	O
over	O
hypotheses	O
while	O
the	O
sample	B
complexity	I
the	O
runtime	O
is	O
this	O
is	O
a	O
huge	O
of	O
learning	O
this	O
class	O
is	O
just	O
number	O
much	O
larger	O
than	O
the	O
number	O
of	O
atoms	O
in	O
the	O
visible	O
universe	O
in	O
the	O
next	O
chapter	O
we	O
formally	O
define	O
the	O
computational	B
complexity	I
of	O
learning	O
in	O
the	O
second	O
part	O
of	O
this	O
book	O
we	O
will	O
study	O
hypothesis	B
classes	O
for	O
which	O
the	O
erm	B
or	O
srm	B
schemes	O
can	O
be	O
implemented	O
efficiently	O
bibliographic	O
remarks	O
bibliographic	O
remarks	O
our	O
definition	O
of	O
nonuniform	O
learnability	O
is	O
related	O
to	O
the	O
definition	O
of	O
an	O
occamalgorithm	O
in	O
blumer	O
ehrenfeucht	O
haussler	O
warmuth	O
the	O
concept	O
of	O
srm	B
is	O
due	O
to	O
chervonenkis	O
vapnik	O
the	O
concept	O
of	O
mdl	B
is	O
due	O
to	O
rissanen	O
the	O
relation	O
between	O
srm	B
and	O
mdl	B
is	O
discussed	O
in	O
vapnik	O
these	O
notions	O
are	O
also	O
closely	O
related	O
to	O
the	O
notion	O
of	O
regularization	B
tikhonov	B
we	O
will	O
elaborate	O
on	O
regularization	B
in	O
the	O
second	O
part	O
of	O
this	O
book	O
the	O
notion	O
of	O
consistency	B
of	O
estimators	O
dates	O
back	O
to	O
fisher	O
our	O
presentation	O
of	O
consistency	B
follows	O
steinwart	O
christmann	O
who	O
also	O
derived	O
several	O
no-free-lunch	B
theorems	O
exercises	O
prove	O
that	O
for	O
any	O
finite	O
class	O
h	O
and	O
any	O
description	O
language	O
d	O
h	O
the	O
vc-dimension	O
of	O
h	O
is	O
at	O
most	O
supdh	O
h	O
h	O
the	O
maximum	O
description	O
length	O
of	O
a	O
predictor	B
in	O
h	O
furthermore	O
if	O
d	O
is	O
a	O
prefix-free	O
description	O
then	O
vcdimh	O
supdh	O
h	O
h	O
let	O
h	O
n	O
n	O
be	O
an	O
infinite	O
countable	O
hypothesis	B
class	I
for	O
binary	O
classification	O
show	O
that	O
it	O
is	O
impossible	O
to	O
assign	O
weights	O
to	O
the	O
hypotheses	O
in	O
h	O
such	O
that	O
h	O
could	O
be	O
learnt	O
nonuniformly	O
using	O
these	O
weights	O
that	O
is	O
the	O
weighting	O
h	O
h	O
wh	O
the	O
weights	O
would	O
be	O
monotonically	O
nondecreasing	O
that	O
is	O
if	O
i	O
j	O
then	O
function	B
w	O
h	O
should	O
satisfy	O
the	O
finite	O
find	O
a	O
weighting	O
function	B
w	O
h	O
such	O
hn	O
where	O
for	O
every	O
n	O
n	O
hn	O
is	O
h	O
h	O
wh	O
and	O
so	O
that	O
for	O
all	O
h	O
h	O
wh	O
is	O
determined	O
by	O
nh	O
minn	O
h	O
hn	O
and	O
by	O
consider	O
a	O
hypothesis	B
class	I
h	O
whi	O
whj	O
infinite	O
define	O
such	O
a	O
function	B
w	O
when	O
for	O
all	O
n	O
hn	O
is	O
countable	O
let	O
h	O
be	O
some	O
hypothesis	B
class	I
for	O
any	O
h	O
h	O
let	O
denote	O
the	O
description	O
length	O
of	O
h	O
according	O
to	O
some	O
fixed	O
description	O
language	O
consider	O
the	O
mdl	B
learning	O
paradigm	O
in	O
which	O
the	O
algorithm	O
returns	O
hs	O
arg	O
min	O
h	O
h	O
lsh	O
where	O
s	O
is	O
a	O
sample	O
of	O
size	O
m	O
for	O
any	O
b	O
let	O
hb	O
h	O
b	O
and	O
define	O
h	O
b	O
arg	O
min	O
h	O
hb	O
ldh	O
nonuniform	O
learnability	O
prove	O
a	O
bound	O
on	O
ldhs	O
ldh	O
b	O
in	O
terms	O
of	O
b	O
the	O
confidence	B
parameter	O
and	O
the	O
size	O
of	O
the	O
training	B
set	B
m	O
note	O
such	O
bounds	O
are	O
known	O
as	O
oracle	O
inequalities	O
in	O
the	O
literature	O
we	O
wish	O
to	O
estimate	O
how	O
good	O
we	O
are	O
compared	O
to	O
a	O
reference	O
classifier	B
oracle	O
h	O
b	O
in	O
this	O
question	O
we	O
wish	O
to	O
show	O
a	O
no-free-lunch	B
result	O
for	O
nonuniform	O
learnability	O
namely	O
that	O
over	O
any	O
infinite	O
domain	B
the	O
class	O
of	O
all	O
functions	O
is	O
not	O
learnable	O
even	O
under	O
the	O
relaxed	O
nonuniform	O
variation	O
of	O
learning	O
recall	B
that	O
an	O
algorithm	O
a	O
nonuniformly	O
learns	O
a	O
hypothesis	B
class	I
h	O
if	O
there	O
exists	O
a	O
function	B
mnulh	O
h	O
n	O
such	O
that	O
for	O
every	O
and	O
for	O
every	O
h	O
h	O
if	O
m	O
mnulh	O
h	O
then	O
for	O
every	O
distribution	O
d	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
it	O
holds	O
that	O
ldas	O
ldh	O
n	O
n	O
hn	O
and	O
for	O
every	O
n	O
n	O
vcdimhn	O
is	O
finite	O
so	O
that	O
h	O
of	O
classes	O
n	O
n	O
such	O
that	O
h	O
if	O
such	O
an	O
algorithm	O
exists	O
then	O
we	O
say	O
that	O
h	O
is	O
nonuniformly	O
learnable	O
let	O
a	O
be	O
a	O
nonuniform	O
learner	O
for	O
a	O
class	O
h	O
for	O
each	O
n	O
n	O
define	O
ha	O
n	O
h	O
h	O
n	O
prove	O
that	O
each	O
such	O
class	O
hn	O
has	O
a	O
finite	O
vc-dimension	O
prove	O
that	O
if	O
a	O
class	O
h	O
is	O
nonuniformly	O
learnable	O
then	O
there	O
are	O
classes	O
hn	O
let	O
h	O
be	O
a	O
class	O
that	O
shatters	O
an	O
infinite	O
set	B
then	O
for	O
every	O
sequence	O
n	O
n	O
hn	O
there	O
exists	O
some	O
n	O
for	O
which	O
vcdimhn	O
hint	O
given	O
a	O
class	O
h	O
that	O
shatters	O
some	O
infinite	O
set	B
k	O
and	O
a	O
sequence	O
of	O
classes	O
n	O
n	O
each	O
having	O
a	O
finite	O
vc-dimension	O
start	O
by	O
defining	O
subsets	O
kn	O
k	O
such	O
that	O
for	O
all	O
n	O
vcdimhn	O
and	O
for	O
any	O
n	O
m	O
kn	O
km	O
now	O
pick	O
for	O
each	O
such	O
kn	O
a	O
function	B
fn	O
kn	O
so	O
that	O
no	O
h	O
hn	O
agrees	O
with	O
fn	O
on	O
the	O
domain	B
kn	O
finally	O
define	O
n	O
n	O
hn	O
construct	O
a	O
class	O
of	O
functions	O
from	O
the	O
unit	O
interval	O
to	O
that	O
construct	O
a	O
class	O
of	O
functions	O
from	O
the	O
unit	O
interval	O
to	O
that	O
f	O
x	O
by	O
combining	O
these	O
fn	O
s	O
and	O
prove	O
that	O
f	O
is	O
nonuniformly	O
learnable	O
but	O
not	O
pac	B
learnable	O
is	O
not	O
nonuniformly	O
learnable	O
in	O
this	O
question	O
we	O
wish	O
to	O
show	O
that	O
the	O
algorithm	O
memorize	O
is	O
a	O
consistent	B
learner	O
for	O
every	O
class	O
of	O
functions	O
over	O
any	O
countable	O
domain	B
let	O
x	O
be	O
a	O
countable	O
domain	B
and	O
let	O
d	O
be	O
a	O
probability	O
distribution	O
over	O
x	O
let	O
i	O
n	O
be	O
an	O
enumeration	O
of	O
the	O
elements	O
of	O
x	O
so	O
that	O
for	O
all	O
i	O
j	O
dxi	O
dxj	O
prove	O
that	O
i	O
n	O
lim	O
n	O
dxi	O
given	O
any	O
prove	O
that	O
there	O
exists	O
such	O
that	O
dx	O
x	O
dx	O
exercises	O
for	O
every	O
m	O
n	O
p	O
s	O
dm	O
xi	O
and	O
xi	O
s	O
ne	O
m	O
prove	O
that	O
for	O
every	O
if	O
n	O
is	O
such	O
that	O
dxi	O
for	O
all	O
i	O
n	O
then	O
conclude	O
that	O
if	O
x	O
is	O
countable	O
then	O
for	O
every	O
probability	O
distribution	O
d	O
over	O
x	O
there	O
exists	O
a	O
function	B
md	O
n	O
such	O
that	O
for	O
every	O
if	O
m	O
md	O
then	O
p	O
s	O
dm	O
x	O
s	O
prove	O
that	O
memorize	O
is	O
a	O
consistent	B
learner	O
for	O
every	O
class	O
of	O
valued	O
functions	O
over	O
any	O
countable	O
domain	B
the	O
runtime	O
of	O
learning	O
so	O
far	O
in	O
the	O
book	O
we	O
have	O
studied	O
the	O
statistical	O
perspective	O
of	O
learning	O
namely	O
how	O
many	O
samples	O
are	O
needed	O
for	O
learning	O
in	O
other	O
words	O
we	O
focused	O
on	O
the	O
amount	O
of	O
information	O
learning	O
requires	O
however	O
when	O
considering	O
automated	O
learning	O
computational	O
resources	O
also	O
play	O
a	O
major	O
role	O
in	O
determining	O
the	O
complexity	O
of	O
a	O
task	O
that	O
is	O
how	O
much	O
computation	O
is	O
involved	O
in	O
carrying	O
out	O
a	O
learning	O
task	O
once	O
a	O
sufficient	O
training	O
sample	O
is	O
available	O
to	O
the	O
learner	O
there	O
is	O
some	O
computation	O
to	O
be	O
done	O
to	O
extract	O
a	O
hypothesis	B
or	O
figure	O
out	O
the	O
label	B
of	O
a	O
given	O
test	O
instance	B
these	O
computational	O
resources	O
are	O
crucial	O
in	O
any	O
practical	O
application	O
of	O
machine	O
learning	O
we	O
refer	O
to	O
these	O
two	O
types	O
of	O
resources	O
as	O
the	O
sample	B
complexity	I
and	O
the	O
computational	B
complexity	I
in	O
this	O
chapter	O
we	O
turn	O
our	O
attention	O
to	O
the	O
computational	B
complexity	I
of	O
learning	O
the	O
computational	B
complexity	I
of	O
learning	O
should	O
be	O
viewed	O
in	O
the	O
wider	O
context	O
of	O
the	O
computational	B
complexity	I
of	O
general	O
algorithmic	O
tasks	O
this	O
area	O
has	O
been	O
extensively	O
investigated	O
see	O
for	O
example	O
the	O
introductory	O
comments	O
that	O
follow	O
summarize	O
the	O
basic	O
ideas	O
of	O
that	O
general	O
theory	O
that	O
are	O
most	O
relevant	O
to	O
our	O
discussion	O
the	O
actual	O
runtime	O
seconds	O
of	O
an	O
algorithm	O
depends	O
on	O
the	O
specific	O
machine	O
the	O
algorithm	O
is	O
being	O
implemented	O
on	O
what	O
the	O
clock	O
rate	O
of	O
the	O
machine	O
s	O
cpu	O
is	O
to	O
avoid	O
dependence	O
on	O
the	O
specific	O
machine	O
it	O
is	O
common	O
to	O
analyze	O
the	O
runtime	O
of	O
algorithms	O
in	O
an	O
asymptotic	O
sense	O
for	O
example	O
we	O
say	O
that	O
the	O
computational	B
complexity	I
of	O
the	O
merge-sort	O
algorithm	O
which	O
sorts	O
a	O
list	O
of	O
n	O
items	O
is	O
on	O
logn	O
this	O
implies	O
that	O
we	O
can	O
implement	O
the	O
algorithm	O
on	O
any	O
machine	O
that	O
satisfies	O
the	O
requirements	O
of	O
some	O
accepted	O
abstract	O
model	O
of	O
computation	O
and	O
the	O
actual	O
runtime	O
in	O
seconds	O
will	O
satisfy	O
the	O
following	O
there	O
exist	O
constants	O
c	O
and	O
which	O
can	O
depend	O
on	O
the	O
actual	O
machine	O
such	O
that	O
for	O
any	O
value	O
of	O
n	O
the	O
runtime	O
in	O
seconds	O
of	O
sorting	O
any	O
n	O
items	O
will	O
be	O
at	O
most	O
c	O
n	O
logn	O
it	O
is	O
common	O
to	O
use	O
the	O
term	O
feasible	B
or	O
efficiently	O
computable	O
for	O
tasks	O
that	O
can	O
be	O
performed	O
by	O
an	O
algorithm	O
whose	O
running	O
time	O
is	O
opn	O
for	O
some	O
polynomial	O
function	B
p	O
one	O
should	O
note	O
that	O
this	O
type	O
of	O
analysis	O
depends	O
on	O
defining	O
what	O
is	O
the	O
input	O
size	O
n	O
of	O
any	O
instance	B
to	O
which	O
the	O
algorithm	O
is	O
expected	O
to	O
be	O
applied	O
for	O
purely	O
algorithmic	O
tasks	O
as	O
discussed	O
in	O
the	O
common	O
computational	B
complexity	I
literature	O
this	O
input	O
size	O
is	O
clearly	O
defined	O
the	O
algorithm	O
gets	O
an	O
input	O
instance	B
say	O
a	O
list	O
to	O
be	O
sorted	O
or	O
an	O
arithmetic	O
operation	O
to	O
be	O
calculated	O
which	O
has	O
a	O
well	O
defined	O
size	O
the	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
computational	B
complexity	I
of	O
learning	O
number	O
of	O
bits	O
in	O
its	O
representation	O
for	O
machine	O
learning	O
tasks	O
the	O
notion	O
of	O
an	O
input	O
size	O
is	O
not	O
so	O
clear	O
an	O
algorithm	O
aims	O
to	O
detect	O
some	O
pattern	O
in	O
a	O
data	O
set	B
and	O
can	O
only	O
access	O
random	O
samples	O
of	O
that	O
data	O
we	O
start	O
the	O
chapter	O
by	O
discussing	O
this	O
issue	O
and	O
define	O
the	O
computational	B
complexity	I
of	O
learning	O
for	O
advanced	O
students	O
we	O
also	O
provide	O
a	O
detailed	O
formal	O
definition	O
we	O
then	O
move	O
on	O
to	O
consider	O
the	O
computational	B
complexity	I
of	O
implementing	O
the	O
erm	B
rule	O
we	O
first	O
give	O
several	O
examples	O
of	O
hypothesis	B
classes	O
where	O
the	O
erm	B
rule	O
can	O
be	O
efficiently	O
implemented	O
and	O
then	O
consider	O
some	O
cases	O
where	O
although	O
the	O
class	O
is	O
indeed	O
efficiently	O
learnable	O
erm	B
implementation	O
is	O
computationally	O
hard	O
it	O
follows	O
that	O
hardness	O
of	O
implementing	O
erm	B
does	O
not	O
imply	O
hardness	O
of	O
learning	O
finally	O
we	O
briefly	O
discuss	O
how	O
one	O
can	O
show	O
hardness	O
of	O
a	O
given	O
learning	O
task	O
namely	O
that	O
no	O
learning	O
algorithm	O
can	O
solve	O
it	O
efficiently	O
computational	B
complexity	I
of	O
learning	O
recall	B
that	O
a	O
learning	O
algorithm	O
has	O
access	O
to	O
a	O
domain	B
of	I
examples	I
z	O
a	O
hypothesis	B
class	I
h	O
a	O
loss	B
function	B
and	O
a	O
training	B
set	B
of	O
examples	O
from	O
z	O
that	O
are	O
sampled	O
i	O
i	O
d	O
according	O
to	O
an	O
unknown	O
distribution	O
d	O
given	O
parameters	O
the	O
algorithm	O
should	O
output	O
a	O
hypothesis	B
h	O
such	O
that	O
with	O
probability	O
of	O
at	O
least	O
ldh	O
min	O
h	O
as	O
mentioned	O
before	O
the	O
actual	O
runtime	O
of	O
an	O
algorithm	O
in	O
seconds	O
depends	O
on	O
the	O
specific	O
machine	O
to	O
allow	O
machine	O
independent	O
analysis	O
we	O
use	O
the	O
standard	O
approach	O
in	O
computational	B
complexity	I
theory	O
first	O
we	O
rely	O
on	O
a	O
notion	O
of	O
an	O
abstract	O
machine	O
such	O
as	O
a	O
turing	O
machine	O
a	O
turing	O
machine	O
over	O
the	O
reals	O
shub	O
smale	O
second	O
we	O
analyze	O
the	O
runtime	O
in	O
an	O
asymptotic	O
sense	O
while	O
ignoring	O
constant	O
factors	O
thus	O
the	O
specific	O
machine	O
is	O
not	O
important	O
as	O
long	O
as	O
it	O
implements	O
the	O
abstract	O
machine	O
usually	O
the	O
asymptote	O
is	O
with	O
respect	O
to	O
the	O
size	O
of	O
the	O
input	O
to	O
the	O
algorithm	O
for	O
example	O
for	O
the	O
merge-sort	O
algorithm	O
mentioned	O
before	O
we	O
analyze	O
the	O
runtime	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
items	O
that	O
need	O
to	O
be	O
sorted	O
in	O
the	O
context	O
of	O
learning	O
algorithms	O
there	O
is	O
no	O
clear	O
notion	O
of	O
input	O
size	O
one	O
might	O
define	O
the	O
input	O
size	O
to	O
be	O
the	O
size	O
of	O
the	O
training	B
set	B
the	O
algorithm	O
receives	O
but	O
that	O
would	O
be	O
rather	O
pointless	O
if	O
we	O
give	O
the	O
algorithm	O
a	O
very	O
large	O
number	O
of	O
examples	O
much	O
larger	O
than	O
the	O
sample	B
complexity	I
of	O
the	O
learning	O
problem	O
the	O
algorithm	O
can	O
simply	O
ignore	O
the	O
extra	O
examples	O
therefore	O
a	O
larger	O
training	B
set	B
does	O
not	O
make	O
the	O
learning	O
problem	O
more	O
difficult	O
and	O
consequently	O
the	O
runtime	O
available	O
for	O
a	O
learning	O
algorithm	O
should	O
not	O
increase	O
as	O
we	O
increase	O
the	O
size	O
of	O
the	O
training	B
set	B
just	O
the	O
same	O
we	O
can	O
still	O
analyze	O
the	O
runtime	O
as	O
a	O
function	B
of	O
natural	O
parameters	O
of	O
the	O
problem	O
such	O
as	O
the	O
target	O
accuracy	B
the	O
confidence	B
of	O
achieving	O
that	O
accuracy	B
the	O
dimensionality	O
of	O
the	O
the	O
runtime	O
of	O
learning	O
domain	B
set	B
or	O
some	O
measures	O
of	O
the	O
complexity	O
of	O
the	O
hypothesis	B
class	I
with	O
which	O
the	O
algorithm	O
s	O
output	O
is	O
compared	O
to	O
illustrate	O
this	O
consider	O
a	O
learning	O
algorithm	O
for	O
the	O
task	O
of	O
learning	O
axis	O
aligned	O
rectangles	O
a	O
specific	O
problem	O
of	O
learning	O
axis	O
aligned	O
rectangles	O
is	O
derived	O
by	O
specifying	O
and	O
the	O
dimension	O
of	O
the	O
instance	B
space	I
we	O
can	O
define	O
a	O
sequence	O
of	O
problems	O
of	O
the	O
type	O
rectangles	O
learning	O
by	O
fixing	O
and	O
varying	O
the	O
dimension	O
to	O
be	O
d	O
we	O
can	O
also	O
define	O
another	O
sequence	O
of	O
rectangles	O
learning	O
problems	O
by	O
fixing	O
d	O
and	O
varying	O
the	O
target	O
accuracy	B
to	O
be	O
one	O
can	O
of	O
course	O
choose	O
other	O
sequences	O
of	O
such	O
problems	O
once	O
a	O
sequence	O
of	O
the	O
problems	O
is	O
fixed	O
one	O
can	O
analyze	O
the	O
asymptotic	O
runtime	O
as	O
a	O
function	B
of	O
variables	O
of	O
that	O
sequence	O
before	O
we	O
introduce	O
the	O
formal	O
definition	O
there	O
is	O
one	O
more	O
subtlety	O
we	O
need	O
to	O
tackle	O
on	O
the	O
basis	O
of	O
the	O
preceding	O
a	O
learning	O
algorithm	O
can	O
cheat	O
by	O
transferring	O
the	O
computational	O
burden	O
to	O
the	O
output	O
hypothesis	B
for	O
example	O
the	O
algorithm	O
can	O
simply	O
define	O
the	O
output	O
hypothesis	B
to	O
be	O
the	O
function	B
that	O
stores	O
the	O
training	B
set	B
in	O
its	O
memory	O
and	O
whenever	O
it	O
gets	O
a	O
test	O
example	O
x	O
it	O
calculates	O
the	O
erm	B
hypothesis	B
on	O
the	O
training	B
set	B
and	O
applies	O
it	O
on	O
x	O
note	O
that	O
in	O
this	O
case	O
our	O
algorithm	O
has	O
a	O
fixed	O
output	O
the	O
function	B
that	O
we	O
have	O
just	O
described	O
and	O
can	O
run	O
in	O
constant	O
time	O
however	O
learning	O
is	O
still	O
hard	O
the	O
hardness	O
is	O
now	O
in	O
implementing	O
the	O
output	O
classifier	B
to	O
obtain	O
a	O
label	B
prediction	O
to	O
prevent	O
this	O
cheating	O
we	O
shall	O
require	O
that	O
the	O
output	O
of	O
a	O
learning	O
algorithm	O
must	O
be	O
applied	O
to	O
predict	O
the	O
label	B
of	O
a	O
new	O
example	O
in	O
time	O
that	O
does	O
not	O
exceed	O
the	O
runtime	O
of	O
training	O
is	O
computing	O
the	O
output	O
classifier	B
from	O
the	O
input	O
training	O
sample	O
in	O
the	O
next	O
subsection	O
the	O
advanced	O
reader	O
may	O
find	O
a	O
formal	O
definition	O
of	O
the	O
computational	B
complexity	I
of	O
learning	O
formal	O
definition	O
the	O
definition	O
that	O
follows	O
relies	O
on	O
a	O
notion	O
of	O
an	O
underlying	O
abstract	O
machine	O
which	O
is	O
usually	O
either	O
a	O
turing	O
machine	O
or	O
a	O
turing	O
machine	O
over	O
the	O
reals	O
we	O
will	O
measure	O
the	O
computational	B
complexity	I
of	O
an	O
algorithm	O
using	O
the	O
number	O
of	O
operations	O
it	O
needs	O
to	O
perform	O
where	O
we	O
assume	O
that	O
for	O
any	O
machine	O
that	O
implements	O
the	O
underlying	O
abstract	O
machine	O
there	O
exists	O
a	O
constant	O
c	O
such	O
that	O
any	O
such	O
operation	O
can	O
be	O
performed	O
on	O
the	O
machine	O
using	O
c	O
seconds	O
definition	O
computational	B
complexity	I
of	O
a	O
learning	O
algorithm	O
we	O
define	O
the	O
complexity	O
of	O
learning	O
in	O
two	O
steps	O
first	O
we	O
consider	O
the	O
computational	B
complexity	I
of	O
a	O
fixed	O
learning	O
problem	O
by	O
a	O
triplet	O
a	O
domain	B
set	B
a	O
benchmark	O
hypothesis	B
class	I
and	O
a	O
loss	B
function	B
then	O
in	O
the	O
second	O
step	O
we	O
consider	O
the	O
rate	O
of	O
change	O
of	O
that	O
complexity	O
along	O
a	O
sequence	O
of	O
such	O
tasks	O
given	O
a	O
function	B
f	O
n	O
a	O
learning	O
task	O
and	O
a	O
learning	O
algorithm	O
a	O
we	O
say	O
that	O
a	O
solves	O
the	O
learning	O
task	O
in	O
time	O
of	O
if	O
there	O
exists	O
some	O
constant	O
number	O
c	O
such	O
that	O
for	O
every	O
probability	O
distribution	O
d	O
implementing	O
the	O
erm	B
rule	O
over	O
z	O
and	O
input	O
when	O
a	O
has	O
access	O
to	O
samples	O
generated	O
i	O
i	O
d	O
by	O
d	O
a	O
terminates	O
after	O
performing	O
at	O
most	O
cf	O
operations	O
the	O
output	O
of	O
a	O
denoted	O
ha	O
can	O
be	O
applied	O
to	O
predict	O
the	O
label	B
of	O
a	O
new	O
the	O
output	O
of	O
a	O
is	O
probably	O
approximately	O
correct	O
namely	O
with	O
probability	O
of	O
at	O
least	O
the	O
random	O
samples	O
a	O
receives	O
ldha	O
h	O
example	O
while	O
performing	O
at	O
most	O
cf	O
operations	O
consider	O
a	O
sequence	O
of	O
learning	O
problems	O
where	O
problem	O
n	O
is	O
defined	O
by	O
a	O
domain	B
zn	O
a	O
hypothesis	B
class	I
hn	O
and	O
a	O
loss	B
function	B
let	O
a	O
be	O
a	O
learning	O
algorithm	O
designed	O
for	O
solving	O
learning	O
problems	O
of	O
this	O
form	O
given	O
a	O
function	B
g	O
n	O
n	O
we	O
say	O
that	O
the	O
runtime	O
of	O
a	O
with	O
respect	O
to	O
the	O
preceding	O
sequence	O
is	O
og	O
if	O
for	O
all	O
n	O
a	O
solves	O
the	O
problem	O
in	O
time	O
ofn	O
where	O
fn	O
n	O
is	O
defined	O
by	O
fn	O
gn	O
we	O
say	O
that	O
a	O
is	O
an	O
efficient	O
algorithm	O
with	O
respect	O
to	O
a	O
sequence	O
if	O
its	O
runtime	O
is	O
opn	O
for	O
some	O
polynomial	O
p	O
from	O
this	O
definition	O
we	O
see	O
that	O
the	O
question	O
whether	O
a	O
general	O
learning	O
problem	O
can	O
be	O
solved	O
efficiently	O
depends	O
on	O
how	O
it	O
can	O
be	O
broken	O
into	O
a	O
sequence	O
of	O
specific	O
learning	O
problems	O
for	O
example	O
consider	O
the	O
problem	O
of	O
learning	O
a	O
finite	O
hypothesis	B
class	I
as	O
we	O
showed	O
in	O
previous	O
chapters	O
the	O
erm	B
rule	O
over	O
h	O
is	O
guaranteed	O
to	O
h	O
if	O
the	O
number	O
of	O
training	O
examples	O
is	O
order	O
of	O
mh	O
logh	O
assuming	O
that	O
the	O
evaluation	O
of	O
a	O
hypothesis	B
on	O
an	O
example	O
takes	O
a	O
constant	O
time	O
it	O
is	O
possible	O
to	O
implement	O
the	O
erm	B
rule	O
in	O
time	O
oh	O
mh	O
by	O
performing	O
an	O
exhaustive	O
search	O
over	O
h	O
with	O
a	O
training	B
set	B
of	O
size	O
mh	O
for	O
any	O
fixed	O
finite	O
h	O
the	O
exhaustive	O
search	O
algorithm	O
runs	O
in	O
polynomial	O
time	O
furthermore	O
if	O
we	O
define	O
a	O
sequence	O
of	O
problems	O
in	O
which	O
n	O
then	O
the	O
exhaustive	O
search	O
is	O
still	O
considered	O
to	O
be	O
efficient	O
however	O
if	O
we	O
define	O
a	O
sequence	O
of	O
problems	O
for	O
which	O
then	O
the	O
sample	B
complexity	I
is	O
still	O
polynomial	O
in	O
n	O
but	O
the	O
computational	B
complexity	I
of	O
the	O
exhaustive	O
search	O
algorithm	O
grows	O
exponentially	O
with	O
n	O
rendered	O
inefficient	O
implementing	O
the	O
erm	B
rule	O
given	O
a	O
hypothesis	B
class	I
h	O
the	O
ermh	O
rule	O
is	O
maybe	O
the	O
most	O
natural	O
learning	O
paradigm	O
furthermore	O
for	O
binary	O
classification	O
problems	O
we	O
saw	O
that	O
if	O
learning	O
is	O
at	O
all	O
possible	O
it	O
is	O
possible	O
with	O
the	O
erm	B
rule	O
in	O
this	O
section	O
we	O
discuss	O
the	O
computational	B
complexity	I
of	O
implementing	O
the	O
erm	B
rule	O
for	O
several	O
hypothesis	B
classes	O
given	O
a	O
hypothesis	B
class	I
h	O
a	O
domain	B
set	B
z	O
and	O
a	O
loss	B
function	B
the	O
corre	O
sponding	O
ermh	O
rule	O
can	O
be	O
defined	O
as	O
follows	O
the	O
runtime	O
of	O
learning	O
on	O
a	O
finite	O
input	O
sample	O
s	O
zm	O
output	O
some	O
h	O
h	O
that	O
minimizes	O
the	O
empirical	O
loss	B
lsh	O
z	O
s	O
z	O
this	O
section	O
studies	O
the	O
runtime	O
of	O
implementing	O
the	O
erm	B
rule	O
for	O
several	O
examples	O
of	O
learning	O
tasks	O
finite	O
classes	O
limiting	O
the	O
hypothesis	B
class	I
to	O
be	O
a	O
finite	O
class	O
may	O
be	O
considered	O
as	O
a	O
reasonably	O
mild	O
restriction	O
for	O
example	O
h	O
can	O
be	O
the	O
set	B
of	O
all	O
predictors	O
that	O
can	O
be	O
implemented	O
by	O
a	O
c	O
program	O
written	O
in	O
at	O
most	O
bits	O
of	O
code	O
other	O
examples	O
of	O
useful	O
finite	O
classes	O
are	O
any	O
hypothesis	B
class	I
that	O
can	O
be	O
parameterized	O
by	O
a	O
finite	O
number	O
of	O
parameters	O
where	O
we	O
are	O
satisfied	O
with	O
a	O
representation	O
of	O
each	O
of	O
the	O
parameters	O
using	O
a	O
finite	O
number	O
of	O
bits	O
for	O
example	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
the	O
euclidean	O
space	O
rd	O
when	O
the	O
parameters	O
defining	O
any	O
given	O
rectangle	O
are	O
specified	O
up	O
to	O
some	O
limited	O
precision	B
as	O
we	O
have	O
shown	O
in	O
previous	O
chapters	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
finite	O
class	O
is	O
upper	O
bounded	O
by	O
mh	O
c	O
logch	O
where	O
c	O
in	O
the	O
realizable	O
case	O
and	O
c	O
in	O
the	O
nonrealizable	O
case	O
therefore	O
the	O
sample	B
complexity	I
has	O
a	O
mild	O
dependence	O
on	O
the	O
size	O
of	O
h	O
in	O
the	O
example	O
of	O
c	O
programs	O
mentioned	O
before	O
the	O
number	O
of	O
hypotheses	O
is	O
but	O
the	O
sample	B
complexity	I
is	O
only	O
logc	O
a	O
straightforward	O
approach	O
for	O
implementing	O
the	O
erm	B
rule	O
over	O
a	O
finite	O
hypothesis	B
class	I
is	O
to	O
perform	O
an	O
exhaustive	O
search	O
that	O
is	O
for	O
each	O
h	O
h	O
we	O
calculate	O
the	O
empirical	B
risk	B
lsh	O
and	O
return	O
a	O
hypothesis	B
that	O
minimizes	O
the	O
empirical	B
risk	B
assuming	O
that	O
the	O
evaluation	O
of	O
z	O
on	O
a	O
single	O
example	O
takes	O
a	O
constant	O
amount	O
of	O
time	O
k	O
the	O
runtime	O
of	O
this	O
exhaustive	O
search	O
becomes	O
khm	O
where	O
m	O
is	O
the	O
size	O
of	O
the	O
training	B
set	B
if	O
we	O
let	O
m	O
to	O
be	O
the	O
upper	O
bound	O
on	O
the	O
sample	B
complexity	I
mentioned	O
then	O
the	O
runtime	O
becomes	O
khc	O
logch	O
the	O
linear	O
dependence	O
of	O
the	O
runtime	O
on	O
the	O
size	O
of	O
h	O
makes	O
this	O
approach	O
inefficient	O
unrealistic	O
for	O
large	O
classes	O
formally	O
if	O
we	O
define	O
a	O
sequence	O
of	O
such	O
that	O
loghn	O
n	O
then	O
the	O
exhaustive	O
search	O
problems	O
approach	O
yields	O
an	O
exponential	O
runtime	O
in	O
the	O
example	O
of	O
c	O
programs	O
if	O
hn	O
is	O
the	O
set	B
of	O
functions	O
that	O
can	O
be	O
implemented	O
by	O
a	O
c	O
program	O
written	O
in	O
at	O
most	O
n	O
bits	O
of	O
code	O
then	O
the	O
runtime	O
grows	O
exponentially	O
with	O
n	O
implying	O
that	O
the	O
exhaustive	O
search	O
approach	O
is	O
unrealistic	O
for	O
practical	O
use	O
in	O
fact	O
this	O
problem	O
is	O
one	O
of	O
the	O
reasons	O
we	O
are	O
dealing	O
with	O
other	O
hypothesis	B
classes	O
like	O
classes	O
of	O
linear	B
predictors	I
which	O
we	O
will	O
encounter	O
in	O
the	O
next	O
chapter	O
and	O
not	O
just	O
focusing	O
on	O
finite	O
classes	O
it	O
is	O
important	O
to	O
realize	O
that	O
the	O
inefficiency	O
of	O
one	O
algorithmic	O
approach	O
as	O
the	O
exhaustive	O
search	O
does	O
not	O
yet	O
imply	O
that	O
no	O
efficient	O
erm	B
implementation	O
exists	O
indeed	O
we	O
will	O
show	O
examples	O
in	O
which	O
the	O
erm	B
rule	O
can	O
be	O
implemented	O
efficiently	O
implementing	O
the	O
erm	B
rule	O
axis	O
aligned	O
rectangles	O
let	O
hn	O
be	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rn	O
namely	O
hn	O
i	O
ai	O
bi	O
where	O
y	O
if	O
i	O
xi	O
bi	O
otherwise	O
efficiently	O
learnable	O
in	O
the	O
realizable	O
case	O
consider	O
implementing	O
the	O
erm	B
rule	O
in	O
the	O
realizable	O
case	O
that	O
is	O
we	O
are	O
given	O
a	O
training	B
set	B
s	O
ym	O
of	O
examples	O
such	O
that	O
there	O
exists	O
an	O
axis	O
aligned	O
rectangle	O
h	O
hn	O
for	O
which	O
hxi	O
yi	O
for	O
all	O
i	O
our	O
goal	O
is	O
to	O
find	O
such	O
an	O
axis	O
aligned	O
rectangle	O
with	O
a	O
zero	O
training	B
error	I
namely	O
a	O
rectangle	O
that	O
is	O
consistent	B
with	O
all	O
the	O
labels	O
in	O
s	O
we	O
show	O
later	O
that	O
this	O
can	O
be	O
done	O
in	O
time	O
onm	O
indeed	O
for	O
each	O
i	O
set	B
ai	O
minxi	O
s	O
and	O
bi	O
maxxi	O
s	O
in	O
words	O
we	O
take	O
ai	O
to	O
be	O
the	O
minimal	O
value	O
of	O
the	O
i	O
th	O
coordinate	O
of	O
a	O
positive	O
example	O
in	O
s	O
and	O
bi	O
to	O
be	O
the	O
maximal	O
value	O
of	O
the	O
i	O
th	O
coordinate	O
of	O
a	O
positive	O
example	O
in	O
s	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
resulting	O
rectangle	O
has	O
zero	O
training	B
error	I
and	O
that	O
the	O
runtime	O
of	O
finding	O
each	O
ai	O
and	O
bi	O
is	O
om	O
hence	O
the	O
total	O
runtime	O
of	O
this	O
procedure	O
is	O
onm	O
not	O
efficiently	O
learnable	O
in	O
the	O
agnostic	O
case	O
in	O
the	O
agnostic	O
case	O
we	O
do	O
not	O
assume	O
that	O
some	O
hypothesis	B
h	O
perfectly	O
predicts	O
the	O
labels	O
of	O
all	O
the	O
examples	O
in	O
the	O
training	B
set	B
our	O
goal	O
is	O
therefore	O
to	O
find	O
h	O
that	O
minimizes	O
the	O
number	O
of	O
examples	O
for	O
which	O
yi	O
hxi	O
it	O
turns	O
out	O
that	O
for	O
many	O
common	O
hypothesis	B
classes	O
including	O
the	O
classes	O
of	O
axis	O
aligned	O
rectangles	O
we	O
consider	O
here	O
solving	O
the	O
erm	B
problem	O
in	O
the	O
agnostic	O
setting	O
is	O
np-hard	O
in	O
most	O
cases	O
it	O
is	O
even	O
np-hard	O
to	O
find	O
some	O
h	O
h	O
whose	O
error	O
is	O
no	O
more	O
than	O
some	O
constant	O
c	O
times	O
that	O
of	O
the	O
empirical	B
risk	B
minimizer	O
in	O
h	O
that	O
is	O
unless	O
p	O
np	O
there	O
is	O
no	O
algorithm	O
whose	O
running	O
time	O
is	O
polynomial	O
in	O
m	O
and	O
n	O
that	O
is	O
guaranteed	O
to	O
find	O
an	O
erm	B
hypothesis	B
for	O
these	O
problems	O
eiron	O
long	O
on	O
the	O
other	O
hand	O
it	O
is	O
worthwhile	O
noticing	O
that	O
if	O
we	O
fix	O
one	O
specific	O
hypothesis	B
class	I
say	O
axis	O
aligned	O
rectangles	O
in	O
some	O
fixed	O
dimension	O
n	O
then	O
there	O
exist	O
efficient	O
learning	O
algorithms	O
for	O
this	O
class	O
in	O
other	O
words	O
there	O
are	O
successful	O
agnostic	B
pac	B
learners	O
that	O
run	O
in	O
time	O
polynomial	O
in	O
and	O
their	O
dependence	O
on	O
the	O
dimension	O
n	O
is	O
not	O
polynomial	O
to	O
see	O
this	O
recall	B
the	O
implementation	O
of	O
the	O
erm	B
rule	O
we	O
presented	O
for	O
the	O
realizable	O
case	O
from	O
which	O
it	O
follows	O
that	O
an	O
axis	O
aligned	O
rectangle	O
is	O
determined	O
by	O
at	O
most	O
examples	O
therefore	O
given	O
a	O
training	B
set	B
of	O
size	O
m	O
we	O
can	O
perform	O
an	O
exhaustive	O
search	O
over	O
all	O
subsets	O
of	O
the	O
training	B
set	B
of	O
size	O
at	O
most	O
examples	O
and	O
construct	O
a	O
rectangle	O
from	O
each	O
such	O
subset	O
then	O
we	O
can	O
pick	O
the	O
runtime	O
of	O
learning	O
the	O
rectangle	O
with	O
the	O
minimal	O
training	B
error	I
this	O
procedure	O
is	O
guaranteed	O
to	O
find	O
an	O
erm	B
hypothesis	B
and	O
the	O
runtime	O
of	O
the	O
procedure	O
is	O
mon	O
it	O
follows	O
that	O
if	O
n	O
is	O
fixed	O
the	O
runtime	O
is	O
polynomial	O
in	O
the	O
sample	O
size	O
this	O
does	O
not	O
contradict	O
the	O
aforementioned	O
hardness	O
result	O
since	O
there	O
we	O
argued	O
that	O
unless	O
pnp	O
one	O
cannot	O
have	O
an	O
algorithm	O
whose	O
dependence	O
on	O
the	O
dimension	O
n	O
is	O
polynomial	O
as	O
well	O
boolean	B
conjunctions	I
a	O
boolean	O
conjunction	O
is	O
a	O
mapping	O
from	O
x	O
to	O
y	O
that	O
can	O
be	O
expressed	O
as	O
a	O
proposition	O
formula	O
of	O
the	O
form	O
xik	O
xjr	O
for	O
some	O
indices	O
ik	O
jr	O
the	O
function	B
that	O
such	O
a	O
proposition	O
formula	O
defines	O
is	O
hx	O
if	O
xik	O
and	O
xjr	O
otherwise	O
let	O
hn	O
c	O
be	O
the	O
class	O
of	O
all	O
boolean	B
conjunctions	I
over	O
the	O
size	O
of	O
hn	O
c	O
is	O
at	O
most	O
in	O
a	O
conjunction	O
formula	O
each	O
element	O
of	O
x	O
either	O
appears	O
or	O
appears	O
with	O
a	O
negation	O
sign	O
or	O
does	O
not	O
appear	O
at	O
all	O
and	O
we	O
also	O
have	O
the	O
all	O
negative	O
formula	O
hence	O
the	O
sample	B
complexity	I
of	O
learning	O
hn	O
c	O
using	O
the	O
erm	B
rule	O
is	O
at	O
most	O
n	O
efficiently	O
learnable	O
in	O
the	O
realizable	O
case	O
next	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
solve	O
the	O
erm	B
problem	O
for	O
hn	O
c	O
in	O
time	O
polynomial	O
in	O
n	O
and	O
m	O
the	O
idea	O
is	O
to	O
define	O
an	O
erm	B
conjunction	O
by	O
including	O
in	O
the	O
hypothesis	B
conjunction	O
all	O
the	O
literals	O
that	O
do	O
not	O
contradict	O
any	O
positively	O
labeled	O
example	O
let	O
vm	O
be	O
all	O
the	O
positively	O
labeled	O
instances	O
in	O
the	O
input	O
sample	O
s	O
we	O
define	O
by	O
induction	O
on	O
i	O
m	O
a	O
sequence	O
of	O
hypotheses	O
conjunctions	O
let	O
be	O
the	O
conjunction	O
of	O
all	O
possible	O
literals	O
that	O
is	O
xn	O
xn	O
note	O
that	O
assigns	O
the	O
label	B
to	O
all	O
the	O
elements	O
of	O
x	O
we	O
obtain	O
by	O
deleting	O
from	O
the	O
conjunction	O
hi	O
all	O
the	O
literals	O
that	O
are	O
not	O
satisfied	O
by	O
the	O
algorithm	O
outputs	O
the	O
hypothesis	B
hm	O
note	O
that	O
hm	O
labels	O
positively	O
all	O
the	O
positively	O
labeled	O
examples	O
in	O
s	O
furthermore	O
for	O
every	O
i	O
m	O
hi	O
is	O
the	O
most	O
restrictive	O
conjunction	O
that	O
labels	O
vi	O
positively	O
now	O
since	O
we	O
consider	O
learning	O
in	O
the	O
realizable	O
setup	O
there	O
exists	O
a	O
conjunction	O
hypothesis	B
f	O
hn	O
c	O
that	O
is	O
consistent	B
with	O
all	O
the	O
examples	O
in	O
s	O
since	O
hm	O
is	O
the	O
most	O
restrictive	O
conjunction	O
that	O
labels	O
positively	O
all	O
the	O
positively	O
labeled	O
members	O
of	O
s	O
any	O
instance	B
labeled	O
by	O
f	O
is	O
also	O
labeled	O
by	O
hm	O
it	O
follows	O
that	O
hm	O
has	O
zero	O
training	B
error	I
s	O
and	O
is	O
therefore	O
a	O
legal	O
erm	B
hypothesis	B
note	O
that	O
the	O
running	O
time	O
of	O
this	O
algorithm	O
is	O
omn	O
efficiently	O
learnable	O
but	O
not	O
by	O
a	O
proper	B
erm	B
not	O
efficiently	O
learnable	O
in	O
the	O
agnostic	O
case	O
as	O
in	O
the	O
case	O
of	O
axis	O
aligned	O
rectangles	O
unless	O
p	O
np	O
there	O
is	O
no	O
algorithm	O
whose	O
running	O
time	O
is	O
polynomial	O
in	O
m	O
and	O
n	O
that	O
guaranteed	O
to	O
find	O
an	O
erm	B
hypothesis	B
for	O
the	O
class	O
of	O
boolean	B
conjunctions	I
in	O
the	O
unrealizable	O
case	O
learning	O
dnf	O
we	O
next	O
show	O
that	O
a	O
slight	O
generalization	O
of	O
the	O
class	O
of	O
boolean	B
conjunctions	I
leads	O
to	O
intractability	O
of	O
solving	O
the	O
erm	B
problem	O
even	O
in	O
the	O
realizable	O
case	O
consider	O
the	O
class	O
of	O
disjunctive	O
normal	O
form	O
formulae	O
dnf	O
the	O
instance	B
space	I
is	O
x	O
and	O
each	O
hypothesis	B
is	O
represented	O
by	O
the	O
boolean	O
formula	O
of	O
the	O
form	O
hx	O
where	O
each	O
aix	O
is	O
a	O
boolean	O
conjunction	O
defined	O
in	O
the	O
previous	O
section	O
the	O
output	O
of	O
hx	O
is	O
if	O
either	O
or	O
or	O
outputs	O
the	O
label	B
if	O
all	O
three	O
conjunctions	O
output	O
the	O
label	B
then	O
hx	O
of	O
hn	O
the	O
erm	B
rule	O
is	O
at	O
most	O
let	O
hn	O
is	O
at	O
most	O
hence	O
the	O
sample	B
complexity	I
of	O
learning	O
hn	O
be	O
the	O
hypothesis	B
class	I
of	O
all	O
such	O
dnf	O
formulae	O
the	O
size	O
using	O
however	O
from	O
the	O
computational	O
perspective	O
this	O
learning	O
problem	O
is	O
hard	O
it	O
has	O
been	O
shown	O
valiant	O
kearns	O
et	O
al	O
that	O
unless	O
rp	O
np	O
there	O
is	O
no	O
polynomial	O
time	O
algorithm	O
that	O
properly	O
learns	O
a	O
sequence	O
of	O
dnf	O
learning	O
problems	O
in	O
which	O
the	O
dimension	O
of	O
the	O
n	O
th	O
problem	O
is	O
n	O
by	O
properly	O
we	O
mean	O
that	O
the	O
algorithm	O
should	O
output	O
a	O
hypothesis	B
that	O
is	O
a	O
dnf	O
formula	O
in	O
particular	O
since	O
ermhn	O
outputs	O
a	O
dnf	O
formula	O
it	O
is	O
a	O
proper	B
learner	O
and	O
therefore	O
it	O
is	O
hard	O
to	O
implement	O
it	O
the	O
proof	O
uses	O
a	O
reduction	O
of	O
the	O
graph	O
problem	O
to	O
the	O
problem	O
of	O
pac	B
learning	O
dnf	O
the	O
detailed	O
technique	O
is	O
given	O
in	O
exercise	O
see	O
also	O
vazirani	O
section	O
f	O
efficiently	O
learnable	O
but	O
not	O
by	O
a	O
proper	B
erm	B
in	O
the	O
previous	O
section	O
we	O
saw	O
that	O
it	O
is	O
impossible	O
to	O
implement	O
the	O
erm	B
rule	O
efficiently	O
for	O
the	O
class	O
hn	O
of	O
formulae	O
in	O
this	O
section	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
learn	O
this	O
class	O
efficiently	O
but	O
using	O
erm	B
with	O
respect	O
to	O
a	O
larger	O
class	O
representation	B
independent	I
learning	O
is	O
not	O
hard	O
next	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
learn	O
dnf	O
formulae	O
efficiently	O
there	O
is	O
no	O
contradiction	O
to	O
the	O
hardness	O
result	O
mentioned	O
in	O
the	O
previous	O
section	O
as	O
we	O
now	O
allow	O
representation	B
independent	I
learning	O
that	O
is	O
we	O
allow	O
the	O
learning	O
algorithm	O
to	O
output	O
a	O
hypothesis	B
that	O
is	O
not	O
a	O
dnf	O
formula	O
the	O
basic	O
idea	O
is	O
to	O
replace	O
the	O
original	O
hypothesis	B
class	I
of	O
dnf	O
formula	O
with	O
a	O
larger	O
hypothesis	B
class	I
so	O
that	O
the	O
new	O
class	O
is	O
easily	O
learnable	O
the	O
learning	O
the	O
runtime	O
of	O
learning	O
algorithm	O
might	O
return	O
a	O
hypothesis	B
that	O
does	O
not	O
belong	O
to	O
the	O
original	O
hypothesis	B
class	I
hence	O
the	O
name	O
representation	B
independent	I
learning	O
we	O
emphasize	O
that	O
in	O
most	O
situations	O
returning	O
a	O
hypothesis	B
with	O
good	O
predictive	O
ability	O
is	O
what	O
we	O
are	O
really	O
interested	O
in	O
doing	O
we	O
start	O
by	O
noting	O
that	O
because	O
distributes	O
over	O
each	O
dnf	O
formula	O
can	O
be	O
rewritten	O
as	O
v	O
w	O
u	O
next	O
let	O
us	O
define	O
such	O
that	O
for	O
each	O
triplet	O
of	O
literals	O
u	O
v	O
w	O
there	O
is	O
a	O
variable	O
in	O
the	O
range	O
of	O
indicating	O
if	O
u	O
v	O
w	O
is	O
true	O
or	O
false	O
so	O
for	O
each	O
formula	O
over	O
there	O
is	O
a	O
conjunction	O
over	O
with	O
the	O
same	O
truth	O
table	O
since	O
we	O
assume	O
that	O
the	O
data	O
is	O
realizable	O
we	O
can	O
solve	O
the	O
erm	B
problem	O
with	O
respect	O
to	O
the	O
class	O
of	O
conjunctions	O
over	O
furthermore	O
the	O
sample	B
complexity	I
of	O
learning	O
the	O
class	O
of	O
conjunctions	O
in	O
the	O
higher	O
dimensional	O
space	O
is	O
at	O
most	O
thus	O
the	O
overall	O
runtime	O
of	O
this	O
approach	O
is	O
polynomial	O
in	O
n	O
intuitively	O
the	O
idea	O
is	O
as	O
follows	O
we	O
started	O
with	O
a	O
hypothesis	B
class	I
for	O
which	O
learning	O
is	O
hard	O
we	O
switched	O
to	O
another	O
representation	O
where	O
the	O
hypothesis	B
class	I
is	O
larger	O
than	O
the	O
original	O
class	O
but	O
has	O
more	O
structure	O
which	O
allows	O
for	O
a	O
more	O
efficient	O
erm	B
search	O
in	O
the	O
new	O
representation	O
solving	O
the	O
erm	B
problem	O
is	O
easy	O
n	O
o	O
v	O
e	O
r	O
i	O
o	O
n	O
s	O
c	O
o	O
n	O
j	O
u	O
n	O
c	O
t	O
formulae	O
over	O
hardness	O
of	O
learning	O
we	O
have	O
just	O
demonstrated	O
that	O
the	O
computational	O
hardness	O
of	O
implementing	O
ermh	O
does	O
not	O
imply	O
that	O
such	O
a	O
class	O
h	O
is	O
not	O
learnable	O
how	O
can	O
we	O
prove	O
that	O
a	O
learning	O
problem	O
is	O
computationally	O
hard	O
one	O
approach	O
is	O
to	O
rely	O
on	O
cryptographic	O
assumptions	O
in	O
some	O
sense	O
cryptography	O
is	O
the	O
opposite	O
of	O
learning	O
in	O
learning	O
we	O
try	O
to	O
uncover	O
some	O
rule	O
underlying	O
the	O
examples	O
we	O
see	O
whereas	O
in	O
cryptography	O
the	O
goal	O
is	O
to	O
make	O
sure	O
that	O
nobody	O
will	O
be	O
able	O
to	O
discover	O
some	O
secret	O
in	O
spite	O
of	O
having	O
access	O
hardness	O
of	O
learning	O
to	O
some	O
partial	O
information	O
about	O
it	O
on	O
that	O
high	O
level	O
intuitive	O
sense	O
results	O
about	O
the	O
cryptographic	O
security	O
of	O
some	O
system	O
translate	O
into	O
results	O
about	O
the	O
unlearnability	O
of	O
some	O
corresponding	O
task	O
regrettably	O
currently	O
one	O
has	O
no	O
way	O
of	O
proving	O
that	O
a	O
cryptographic	O
protocol	O
is	O
not	O
breakable	O
even	O
the	O
common	O
assumption	O
of	O
p	O
np	O
does	O
not	O
suffice	O
for	O
that	O
it	O
can	O
be	O
shown	O
to	O
be	O
necessary	O
for	O
most	O
common	O
cryptographic	O
scenarios	O
the	O
common	O
approach	O
for	O
proving	O
that	O
cryptographic	O
protocols	O
are	O
secure	O
is	O
to	O
start	O
with	O
some	O
cryptographic	O
assumptions	O
the	O
more	O
these	O
are	O
used	O
as	O
a	O
basis	O
for	O
cryptography	O
the	O
stronger	O
is	O
our	O
belief	O
that	O
they	O
really	O
hold	O
at	O
least	O
that	O
algorithms	O
that	O
will	O
refute	O
them	O
are	O
hard	O
to	O
come	O
by	O
we	O
now	O
briefly	O
describe	O
the	O
basic	O
idea	O
of	O
how	O
to	O
deduce	O
hardness	O
of	O
learnability	O
from	O
cryptographic	O
assumptions	O
many	O
cryptographic	O
systems	O
rely	O
on	O
the	O
assumption	O
that	O
there	O
exists	O
a	O
one	O
way	O
function	B
roughly	O
speaking	O
a	O
one	O
way	O
function	B
is	O
a	O
function	B
f	O
formally	O
it	O
is	O
a	O
sequence	O
of	O
functions	O
one	O
for	O
each	O
dimension	O
n	O
that	O
is	O
easy	O
to	O
compute	O
but	O
is	O
hard	O
to	O
invert	O
more	O
formally	O
f	O
can	O
be	O
computed	O
in	O
time	O
polyn	O
but	O
for	O
any	O
randomized	O
polynomial	O
time	O
algorithm	O
a	O
and	O
for	O
every	O
polynomial	O
p	O
pf	O
f	O
pn	O
where	O
the	O
probability	O
is	O
taken	O
over	O
a	O
random	O
choice	O
of	O
x	O
according	O
to	O
the	O
uniform	O
distribution	O
over	O
and	O
the	O
randomness	O
of	O
a	O
a	O
one	O
way	O
function	B
f	O
is	O
called	O
trapdoor	O
one	O
way	O
function	B
if	O
for	O
some	O
polynomial	O
function	B
p	O
for	O
every	O
n	O
there	O
exists	O
a	O
bit-string	O
sn	O
a	O
secret	O
key	O
of	O
length	O
pn	O
such	O
that	O
there	O
is	O
a	O
polynomial	O
time	O
algorithm	O
that	O
for	O
every	O
n	O
and	O
every	O
x	O
on	O
input	O
sn	O
outputs	O
x	O
in	O
other	O
words	O
although	O
f	O
is	O
hard	O
to	O
invert	O
once	O
one	O
has	O
access	O
to	O
its	O
secret	O
key	O
inverting	O
f	O
becomes	O
feasible	B
such	O
functions	O
are	O
parameterized	O
by	O
their	O
secret	O
key	O
now	O
let	O
fn	O
be	O
a	O
family	O
of	O
trapdoor	O
functions	O
over	O
that	O
can	O
be	O
calculated	O
by	O
some	O
polynomial	O
time	O
algorithm	O
that	O
is	O
we	O
fix	O
an	O
algorithm	O
that	O
given	O
a	O
secret	O
key	O
one	O
function	B
in	O
fn	O
and	O
an	O
input	O
vector	O
it	O
calculates	O
the	O
value	O
of	O
the	O
function	B
corresponding	O
to	O
the	O
secret	O
key	O
on	O
the	O
input	O
vector	O
in	O
polynomial	O
time	O
consider	O
the	O
task	O
of	O
learning	O
the	O
class	O
of	O
the	O
corresponding	O
f	O
f	O
fn	O
since	O
each	O
function	B
in	O
this	O
class	O
can	O
be	O
inverted	O
inverses	O
h	O
n	O
by	O
some	O
secret	O
key	O
sn	O
of	O
size	O
polynomial	O
in	O
n	O
the	O
class	O
h	O
n	O
f	O
can	O
be	O
parameterized	O
by	O
these	O
keys	O
and	O
its	O
size	O
is	O
at	O
most	O
its	O
sample	B
complexity	I
is	O
therefore	O
polynomial	O
in	O
n	O
we	O
claim	O
that	O
there	O
can	O
be	O
no	O
efficient	O
learner	O
for	O
this	O
class	O
if	O
there	O
were	O
such	O
a	O
learner	O
l	O
then	O
by	O
sampling	O
uniformly	O
at	O
random	O
a	O
polynomial	O
number	O
of	O
strings	O
in	O
and	O
computing	O
f	O
over	O
them	O
we	O
could	O
generate	O
a	O
labeled	O
training	O
sample	O
of	O
pairs	O
x	O
which	O
should	O
suffice	O
for	O
our	O
learner	O
to	O
figure	O
out	O
an	O
approximation	O
of	O
f	O
the	O
uniform	O
distribution	O
over	O
the	O
range	O
of	O
f	O
which	O
would	O
violate	O
the	O
one	O
way	O
property	O
of	O
f	O
a	O
more	O
detailed	O
treatment	O
as	O
well	O
as	O
a	O
concrete	O
example	O
can	O
be	O
found	O
in	O
vazirani	O
chapter	O
using	O
reductions	B
they	O
also	O
show	O
that	O
the	O
runtime	O
of	O
learning	O
the	O
class	O
of	O
functions	O
that	O
can	O
be	O
calculated	O
by	O
small	O
boolean	O
circuits	O
is	O
not	O
efficiently	O
learnable	O
even	O
in	O
the	O
realizable	O
case	O
summary	O
the	O
runtime	O
of	O
learning	O
algorithms	O
is	O
asymptotically	O
analyzed	O
as	O
a	O
function	B
of	O
different	O
parameters	O
of	O
the	O
learning	O
problem	O
such	O
as	O
the	O
size	O
of	O
the	O
hypothesis	B
class	I
our	O
measure	O
of	O
accuracy	B
our	O
measure	O
of	O
confidence	B
or	O
the	O
size	O
of	O
the	O
domain	B
set	B
we	O
have	O
demonstrated	O
cases	O
in	O
which	O
the	O
erm	B
rule	O
can	O
be	O
implemented	O
efficiently	O
for	O
example	O
we	O
derived	O
efficient	O
algorithms	O
for	O
solving	O
the	O
erm	B
problem	O
for	O
the	O
class	O
of	O
boolean	B
conjunctions	I
and	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
under	O
the	O
realizability	B
assumption	O
however	O
implementing	O
erm	B
for	O
these	O
classes	O
in	O
the	O
agnostic	O
case	O
is	O
np-hard	O
recall	B
that	O
from	O
the	O
statistical	O
perspective	O
there	O
is	O
no	O
difference	O
between	O
the	O
realizable	O
and	O
agnostic	O
cases	O
a	O
class	O
is	O
learnable	O
in	O
both	O
cases	O
if	O
and	O
only	O
if	O
it	O
has	O
a	O
finite	O
vc-dimension	O
in	O
contrast	O
as	O
we	O
saw	O
from	O
the	O
computational	O
perspective	O
the	O
difference	O
is	O
immense	O
we	O
have	O
also	O
shown	O
another	O
example	O
the	O
class	O
of	O
dnf	O
where	O
implementing	O
erm	B
is	O
hard	O
even	O
in	O
the	O
realizable	O
case	O
yet	O
the	O
class	O
is	O
efficiently	O
learnable	O
by	O
another	O
algorithm	O
hardness	O
of	O
implementing	O
the	O
erm	B
rule	O
for	O
several	O
natural	O
hypothesis	B
classes	O
has	O
motivated	O
the	O
development	O
of	O
alternative	O
learning	O
methods	O
which	O
we	O
will	O
discuss	O
in	O
the	O
next	O
part	O
of	O
this	O
book	O
bibliographic	O
remarks	O
valiant	O
introduced	O
the	O
efficient	O
pac	B
learning	O
model	O
in	O
which	O
the	O
runtime	O
of	O
the	O
algorithm	O
is	O
required	O
to	O
be	O
polynomial	O
in	O
and	O
the	O
representation	O
size	O
of	O
hypotheses	O
in	O
the	O
class	O
a	O
detailed	O
discussion	O
and	O
thorough	O
bibliographic	O
notes	O
are	O
given	O
in	O
kearns	O
vazirani	O
exercises	O
let	O
h	O
be	O
the	O
class	O
of	O
intervals	O
on	O
the	O
line	O
equivalent	O
to	O
axis	O
aligned	O
rectangles	O
in	O
dimension	O
n	O
propose	O
an	O
implementation	O
of	O
the	O
ermh	O
learning	O
rule	O
the	O
agnostic	O
case	O
that	O
given	O
a	O
training	B
set	B
of	O
size	O
m	O
runs	O
in	O
time	O
hint	O
use	O
dynamic	O
programming	O
let	O
be	O
a	O
sequence	O
of	O
hypothesis	B
classes	O
for	O
binary	O
classification	O
assume	O
that	O
there	O
is	O
a	O
learning	O
algorithm	O
that	O
implements	O
the	O
erm	B
rule	O
in	O
the	O
realizable	O
case	O
such	O
that	O
the	O
output	O
hypothesis	B
of	O
the	O
algorithm	O
for	O
each	O
class	O
hn	O
only	O
depends	O
on	O
on	O
examples	O
out	O
of	O
the	O
training	B
set	B
furthermore	O
exercises	O
assume	O
that	O
such	O
a	O
hypothesis	B
can	O
be	O
calculated	O
given	O
these	O
on	O
examples	O
in	O
time	O
on	O
and	O
that	O
the	O
empirical	B
risk	B
of	O
each	O
such	O
hypothesis	B
can	O
be	O
evaluated	O
in	O
time	O
omn	O
for	O
example	O
if	O
hn	O
is	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rn	O
we	O
saw	O
that	O
it	O
is	O
possible	O
to	O
find	O
an	O
erm	B
hypothesis	B
in	O
the	O
realizable	O
case	O
that	O
is	O
defined	O
by	O
at	O
most	O
examples	O
prove	O
that	O
in	O
such	O
cases	O
it	O
is	O
possible	O
to	O
find	O
an	O
erm	B
hypothesis	B
for	O
hn	O
in	O
the	O
unrealizable	O
case	O
in	O
time	O
omn	O
mon	O
in	O
this	O
exercise	O
we	O
present	O
several	O
classes	O
for	O
which	O
finding	O
an	O
erm	B
classifier	B
is	O
computationally	O
hard	O
first	O
we	O
introduce	O
the	O
class	O
of	O
n-dimensional	O
halfspaces	O
hsn	O
for	O
a	O
domain	B
x	O
rn	O
this	O
is	O
the	O
class	O
of	O
all	O
functions	O
of	O
the	O
form	O
hwbx	O
b	O
where	O
w	O
x	O
rn	O
is	O
their	O
inner	O
product	O
and	O
b	O
r	O
see	O
a	O
detailed	O
description	O
in	O
chapter	O
show	O
that	O
ermh	O
over	O
the	O
class	O
h	O
hsn	O
of	O
linear	B
predictors	I
is	O
computationally	O
hard	O
more	O
precisely	O
we	O
consider	O
the	O
sequence	O
of	O
problems	O
in	O
which	O
the	O
dimension	O
n	O
grows	O
linearly	O
and	O
the	O
number	O
of	O
examples	O
m	O
is	O
set	B
to	O
be	O
some	O
constant	O
times	O
n	O
hint	O
you	O
can	O
prove	O
the	O
hardness	O
by	O
a	O
reduction	O
from	O
the	O
following	O
problem	O
max	O
fs	O
given	O
a	O
system	O
of	O
linear	O
inequalities	O
ax	O
b	O
with	O
a	O
rm	O
n	O
and	O
b	O
rm	O
is	O
a	O
system	O
of	O
m	O
linear	O
inequalities	O
in	O
n	O
variables	O
x	O
xn	O
find	O
a	O
subsystem	O
containing	O
as	O
many	O
inequalities	O
as	O
possible	O
that	O
has	O
a	O
solution	O
a	O
subsystem	O
is	O
called	O
feasible	B
it	O
has	O
been	O
shown	O
that	O
the	O
problem	O
max	O
fs	O
is	O
np-hard	O
show	O
that	O
any	O
algorithm	O
that	O
finds	O
an	O
ermhsn	O
hypothesis	B
for	O
any	O
training	O
sample	O
s	O
can	O
be	O
used	O
to	O
solve	O
the	O
max	O
fs	O
problem	O
of	O
size	O
m	O
n	O
hint	O
define	O
a	O
mapping	O
that	O
transforms	O
linear	O
inequalities	O
in	O
n	O
variables	O
into	O
labeled	O
points	O
in	O
rn	O
and	O
a	O
mapping	O
that	O
transforms	O
vectors	O
in	O
rn	O
to	O
halfspaces	O
such	O
that	O
a	O
vector	O
w	O
satisfies	O
an	O
inequality	O
q	O
if	O
and	O
only	O
if	O
the	O
labeled	O
point	O
that	O
corresponds	O
to	O
q	O
is	O
classified	O
correctly	O
by	O
the	O
halfspace	B
corresponding	O
to	O
w	O
conclude	O
that	O
the	O
problem	O
of	O
empirical	B
risk	B
minimization	O
for	O
halfspaces	O
in	O
also	O
np-hard	O
is	O
if	O
it	O
can	O
be	O
solved	O
in	O
time	O
polynomial	O
in	O
the	O
sample	O
size	O
m	O
and	O
the	O
euclidean	O
dimension	O
n	O
then	O
every	O
problem	O
in	O
the	O
class	O
np	O
can	O
be	O
solved	O
in	O
polynomial	O
time	O
let	O
x	O
rn	O
and	O
let	O
hn	O
k	O
be	O
the	O
class	O
of	O
all	O
intersections	O
of	O
k-many	O
linear	O
halfspaces	O
in	O
rn	O
in	O
this	O
exercise	O
we	O
wish	O
to	O
show	O
that	O
ermhn	O
is	O
computationally	O
hard	O
for	O
every	O
k	O
precisely	O
we	O
consider	O
a	O
sequence	O
of	O
problems	O
where	O
k	O
is	O
a	O
constant	O
and	O
n	O
grows	O
linearly	O
the	O
training	B
set	B
size	O
m	O
also	O
grows	O
linearly	O
with	O
n	O
towards	O
this	O
goal	O
consider	O
the	O
k-coloring	O
problem	O
for	O
graphs	O
defined	O
as	O
follows	O
given	O
a	O
graph	O
g	O
e	O
and	O
a	O
number	O
k	O
determine	O
whether	O
there	O
exists	O
a	O
function	B
f	O
v	O
k	O
so	O
that	O
for	O
every	O
v	O
e	O
f	O
f	O
the	O
k-coloring	O
problem	O
is	O
known	O
to	O
be	O
np-hard	O
for	O
every	O
k	O
k	O
the	O
runtime	O
of	O
learning	O
k	O
k	O
we	O
wish	O
to	O
reduce	O
the	O
k-coloring	O
problem	O
to	O
ermhn	O
that	O
is	O
to	O
prove	O
that	O
if	O
there	O
is	O
an	O
algorithm	O
that	O
solves	O
the	O
ermhn	O
problem	O
in	O
time	O
polynomial	O
in	O
k	O
n	O
and	O
the	O
sample	O
size	O
m	O
then	O
there	O
is	O
a	O
polynomial	O
time	O
algorithm	O
for	O
the	O
graph	O
k-coloring	O
problem	O
given	O
a	O
graph	O
g	O
e	O
let	O
vn	O
be	O
the	O
vertices	O
in	O
v	O
construct	O
a	O
sample	O
sg	O
where	O
m	O
as	O
follows	O
for	O
every	O
vi	O
v	O
construct	O
an	O
instance	B
ei	O
with	O
a	O
negative	O
label	B
for	O
every	O
edge	O
vj	O
e	O
construct	O
an	O
instance	B
with	O
a	O
prove	O
that	O
if	O
there	O
exists	O
some	O
h	O
hn	O
k	O
that	O
has	O
zero	O
error	O
over	O
sg	O
positive	O
label	B
hint	O
let	O
h	O
hj	O
be	O
an	O
erm	B
classifier	B
in	O
hn	O
then	O
g	O
is	O
k-colorable	O
k	O
over	O
s	O
define	O
a	O
coloring	O
of	O
v	O
by	O
setting	O
f	O
to	O
be	O
the	O
minimal	O
j	O
such	O
that	O
hjei	O
use	O
the	O
fact	O
that	O
halfspaces	O
are	O
convex	B
sets	O
to	O
show	O
that	O
it	O
cannot	O
be	O
true	O
that	O
two	O
vertices	O
that	O
are	O
connected	O
by	O
an	O
edge	O
have	O
the	O
same	O
color	O
prove	O
that	O
if	O
g	O
is	O
k-colorable	O
then	O
there	O
exists	O
some	O
h	O
h	O
n	O
k	O
that	O
has	O
zero	O
error	O
over	O
sg	O
hint	O
given	O
a	O
coloring	O
f	O
of	O
the	O
vertices	O
of	O
g	O
we	O
should	O
come	O
up	O
with	O
k	O
hyperplanes	O
hk	O
whose	O
intersection	O
is	O
a	O
perfect	O
classifier	B
for	O
sg	O
let	O
b	O
for	O
all	O
of	O
these	O
hyperplanes	O
and	O
for	O
t	O
k	O
let	O
the	O
i	O
th	O
weight	O
of	O
the	O
t	O
th	O
hyperplane	O
wti	O
be	O
if	O
f	O
t	O
and	O
otherwise	O
based	O
on	O
the	O
above	O
prove	O
that	O
for	O
any	O
k	O
the	O
ermhn	O
problem	O
is	O
k	O
np-hard	O
in	O
this	O
exercise	O
we	O
show	O
that	O
hardness	O
of	O
solving	O
the	O
erm	B
problem	O
is	O
equivalent	O
to	O
hardness	O
of	O
proper	B
pac	B
learning	O
recall	B
that	O
by	O
properness	O
of	O
the	O
algorithm	O
we	O
mean	O
that	O
it	O
must	O
output	O
a	O
hypothesis	B
from	O
the	O
hypothesis	B
class	I
to	O
formalize	O
this	O
statement	O
we	O
first	O
need	O
the	O
following	O
definition	O
definition	O
the	O
complexity	O
class	O
randomized	O
polynomial	O
time	O
is	O
the	O
class	O
of	O
all	O
decision	O
problems	O
is	O
problems	O
in	O
which	O
on	O
any	O
instance	B
one	O
has	O
to	O
find	O
out	O
whether	O
the	O
answer	O
is	O
yes	O
or	O
no	O
for	O
which	O
there	O
exists	O
a	O
probabilistic	O
algorithm	O
the	O
algorithm	O
is	O
allowed	O
to	O
flip	O
random	O
coins	O
while	O
it	O
is	O
running	O
with	O
these	O
properties	O
on	O
any	O
input	O
instance	B
the	O
algorithm	O
runs	O
in	O
polynomial	O
time	O
in	O
the	O
input	O
size	O
if	O
the	O
correct	O
answer	O
is	O
no	O
the	O
algorithm	O
must	O
return	O
no	O
if	O
the	O
correct	O
answer	O
is	O
yes	O
the	O
algorithm	O
returns	O
yes	O
with	O
probability	O
a	O
and	O
returns	O
no	O
with	O
probability	O
clearly	O
the	O
class	O
rp	O
contains	O
the	O
class	O
p	O
it	O
is	O
also	O
known	O
that	O
rp	O
is	O
contained	O
in	O
the	O
class	O
np	O
it	O
is	O
not	O
known	O
whether	O
any	O
equality	O
holds	O
among	O
these	O
three	O
complexity	O
classes	O
but	O
it	O
is	O
widely	O
believed	O
that	O
np	O
is	O
strictly	O
the	O
constant	O
in	O
the	O
definition	O
can	O
be	O
replaced	O
by	O
any	O
constant	O
in	O
exercises	O
larger	O
than	O
rp	O
in	O
particular	O
it	O
is	O
believed	O
that	O
np-hard	O
problems	O
cannot	O
be	O
solved	O
by	O
a	O
randomized	O
polynomial	O
time	O
algorithm	O
show	O
that	O
if	O
a	O
class	O
h	O
is	O
properly	O
pac	B
learnable	O
by	O
a	O
polynomial	O
time	O
algorithm	O
then	O
the	O
ermh	O
problem	O
is	O
in	O
the	O
class	O
rp	O
in	O
particular	O
this	O
implies	O
that	O
whenever	O
the	O
ermh	O
problem	O
is	O
np-hard	O
example	O
the	O
class	O
of	O
intersections	O
of	O
halfspaces	O
discussed	O
in	O
the	O
previous	O
exercise	O
then	O
unless	O
np	O
rp	O
there	O
exists	O
no	O
polynomial	O
time	O
proper	B
pac	B
learning	O
algorithm	O
for	O
h	O
hint	O
assume	O
you	O
have	O
an	O
algorithm	O
a	O
that	O
properly	O
pac	B
learns	O
a	O
class	O
h	O
in	O
time	O
polynomial	O
in	O
some	O
class	O
parameter	O
n	O
as	O
well	O
as	O
in	O
and	O
your	O
goal	O
is	O
to	O
use	O
that	O
algorithm	O
as	O
a	O
subroutine	O
to	O
contract	O
an	O
algorithm	O
b	O
for	O
solving	O
the	O
ermh	O
problem	O
in	O
random	O
polynomial	O
time	O
given	O
a	O
training	B
set	B
s	O
and	O
some	O
h	O
h	O
whose	O
error	O
on	O
s	O
is	O
zero	O
apply	O
the	O
pac	B
learning	O
algorithm	O
to	O
the	O
uniform	O
distribution	O
over	O
s	O
and	O
run	O
it	O
so	O
that	O
with	O
probability	O
it	O
finds	O
a	O
function	B
h	O
h	O
that	O
has	O
error	O
less	O
than	O
respect	O
to	O
that	O
uniform	O
distribution	O
show	O
that	O
the	O
algorithm	O
just	O
described	O
satisfies	O
the	O
requirements	O
for	O
being	O
a	O
rp	O
solver	O
for	O
ermh	O
part	O
ii	O
from	O
theory	O
to	O
algorithms	O
linear	B
predictors	I
in	O
this	O
chapter	O
we	O
will	O
study	O
the	O
family	O
of	O
linear	B
predictors	I
one	O
of	O
the	O
most	O
useful	O
families	O
of	O
hypothesis	B
classes	O
many	O
learning	O
algorithms	O
that	O
are	O
being	O
widely	O
used	O
in	O
practice	O
rely	O
on	O
linear	B
predictors	I
first	O
and	O
foremost	O
because	O
of	O
the	O
ability	O
to	O
learn	O
them	O
efficiently	O
in	O
many	O
cases	O
in	O
addition	O
linear	B
predictors	I
are	O
intuitive	O
are	O
easy	O
to	O
interpret	O
and	O
fit	O
the	O
data	O
reasonably	O
well	O
in	O
many	O
natural	O
learning	O
problems	O
we	O
will	O
introduce	O
several	O
hypothesis	B
classes	O
belonging	O
to	O
this	O
family	O
halfspaces	O
linear	B
regression	B
predictors	O
and	O
logistic	B
regression	B
predictors	O
and	O
present	O
relevant	O
learning	O
algorithms	O
linear	B
programming	I
and	O
the	O
perceptron	B
algorithm	O
for	O
the	O
class	O
of	O
halfspaces	O
and	O
the	O
least	B
squares	I
algorithm	O
for	O
linear	B
regression	B
this	O
chapter	O
is	O
focused	O
on	O
learning	O
linear	B
predictors	I
using	O
the	O
erm	B
approach	O
however	O
in	O
later	O
chapters	O
we	O
will	O
see	O
alternative	O
paradigms	O
for	O
learning	O
these	O
hypothesis	B
classes	O
first	O
we	O
define	O
the	O
class	O
of	O
affine	O
functions	O
as	O
where	O
hwbx	O
b	O
wixi	O
b	O
ld	O
w	O
rd	O
b	O
r	O
it	O
will	O
be	O
convenient	O
also	O
to	O
use	O
the	O
notation	O
ld	O
b	O
w	O
rd	O
b	O
r	O
which	O
reads	O
as	O
follows	O
ld	O
is	O
a	O
set	B
of	O
functions	O
where	O
each	O
function	B
is	O
parameterized	O
by	O
w	O
rd	O
and	O
b	O
r	O
and	O
each	O
such	O
function	B
takes	O
as	O
input	O
a	O
vector	O
x	O
and	O
returns	O
as	O
output	O
the	O
scalar	O
b	O
the	O
different	O
hypothesis	B
classes	O
of	O
linear	B
predictors	I
are	O
compositions	O
of	O
a	O
function	B
r	O
y	O
on	O
ld	O
for	O
example	O
in	O
binary	O
classification	O
we	O
can	O
choose	O
to	O
be	O
the	O
sign	O
function	B
and	O
for	O
regression	B
problems	O
where	O
y	O
r	O
is	O
simply	O
the	O
identity	O
function	B
it	O
may	O
be	O
more	O
convenient	O
to	O
incorporate	O
b	O
called	O
the	O
bias	B
into	O
w	O
as	O
an	O
extra	O
coordinate	O
and	O
add	O
an	O
extra	O
coordinate	O
with	O
a	O
value	O
of	O
to	O
all	O
x	O
x	O
namely	O
let	O
wd	O
and	O
let	O
xd	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
linear	B
predictors	I
therefore	O
hwbx	O
b	O
it	O
follows	O
that	O
each	O
affine	O
function	B
in	O
rd	O
can	O
be	O
rewritten	O
as	O
a	O
homogenous	B
linear	O
function	B
in	O
applied	O
over	O
the	O
transformation	O
that	O
appends	O
the	O
constant	O
to	O
each	O
input	O
vector	O
therefore	O
whenever	O
it	O
simplifies	O
the	O
presentation	O
we	O
will	O
omit	O
the	O
bias	B
term	O
and	O
refer	O
to	O
ld	O
as	O
the	O
class	O
of	O
homogenous	B
linear	O
functions	O
of	O
the	O
form	O
hwx	O
throughout	O
the	O
book	O
we	O
often	O
use	O
the	O
general	O
term	O
linear	O
functions	O
for	O
both	O
affine	O
functions	O
and	O
linear	O
functions	O
halfspaces	O
the	O
first	O
hypothesis	B
class	I
we	O
consider	O
is	O
the	O
class	O
of	O
halfspaces	O
designed	O
for	O
binary	O
classification	O
problems	O
namely	O
x	O
rd	O
and	O
y	O
the	O
class	O
of	O
halfspaces	O
is	O
defined	O
as	O
follows	O
hsd	O
sign	O
ld	O
signhwbx	O
hwb	O
ld	O
in	O
other	O
words	O
each	O
halfspace	B
hypothesis	B
in	O
hsd	O
is	O
parameterized	O
by	O
w	O
rd	O
and	O
b	O
r	O
and	O
upon	O
receiving	O
a	O
vector	O
x	O
the	O
hypothesis	B
returns	O
the	O
label	B
b	O
to	O
illustrate	O
this	O
hypothesis	B
class	I
geometrically	O
it	O
is	O
instructive	O
to	O
consider	O
the	O
case	O
d	O
each	O
hypothesis	B
forms	O
a	O
hyperplane	O
that	O
is	O
perpendicular	O
to	O
the	O
vector	O
w	O
and	O
intersects	O
the	O
vertical	O
axis	O
at	O
the	O
point	O
the	O
instances	O
that	O
are	O
above	O
the	O
hyperplane	O
that	O
is	O
share	O
an	O
acute	O
angle	O
with	O
w	O
are	O
labeled	O
positively	O
instances	O
that	O
are	O
below	O
the	O
hyperplane	O
that	O
is	O
share	O
an	O
obtuse	O
angle	O
with	O
w	O
are	O
labeled	O
negatively	O
w	O
in	O
section	O
we	O
will	O
show	O
that	O
vcdimhsd	O
d	O
it	O
follows	O
that	O
we	O
can	O
learn	O
halfspaces	O
using	O
the	O
erm	B
paradigm	O
as	O
long	O
as	O
the	O
sample	O
size	O
is	O
therefore	O
we	O
now	O
discuss	O
how	O
to	O
implement	O
an	O
erm	B
procedure	O
for	O
halfspaces	O
we	O
introduce	O
below	O
two	O
solutions	O
to	O
finding	O
an	O
erm	B
halfspace	B
in	O
the	O
realizable	O
case	O
in	O
the	O
context	O
of	O
halfspaces	O
the	O
realizable	O
case	O
is	O
often	O
referred	O
to	O
as	O
the	O
separable	B
case	O
since	O
it	O
is	O
possible	O
to	O
separate	O
with	O
a	O
hyperplane	O
all	O
the	O
positive	O
examples	O
from	O
all	O
the	O
negative	O
examples	O
implementing	O
the	O
erm	B
rule	O
halfspaces	O
in	O
the	O
nonseparable	O
case	O
the	O
agnostic	O
case	O
is	O
known	O
to	O
be	O
computationally	O
hard	O
simon	O
there	O
are	O
several	O
approaches	O
to	O
learning	O
nonseparable	O
data	O
the	O
most	O
popular	O
one	O
is	O
to	O
use	O
surrogate	B
loss	B
functions	O
namely	O
to	O
learn	O
a	O
halfspace	B
that	O
does	O
not	O
necessarily	O
minimize	O
the	O
empirical	B
risk	B
with	O
the	O
loss	B
but	O
rather	O
with	O
respect	O
to	O
a	O
diffferent	O
loss	B
function	B
for	O
example	O
in	O
section	O
we	O
will	O
describe	O
the	O
logistic	B
regression	B
approach	O
which	O
can	O
be	O
implemented	O
efficiently	O
even	O
in	O
the	O
nonseparable	O
case	O
we	O
will	O
study	O
surrogate	B
loss	B
functions	O
in	O
more	O
detail	O
later	O
on	O
in	O
chapter	O
linear	B
programming	I
for	O
the	O
class	O
of	O
halfspaces	O
linear	O
programs	O
are	O
problems	O
that	O
can	O
be	O
expressed	O
as	O
maximizing	O
a	O
linear	O
function	B
subject	O
to	O
linear	O
inequalities	O
that	O
is	O
max	O
w	O
rd	O
subject	O
to	O
aw	O
v	O
where	O
w	O
rd	O
is	O
the	O
vector	O
of	O
variables	O
we	O
wish	O
to	O
determine	O
a	O
is	O
an	O
m	O
d	O
matrix	O
and	O
v	O
rm	O
u	O
rd	O
are	O
vectors	O
linear	O
programs	O
can	O
be	O
solved	O
and	O
furthermore	O
there	O
are	O
publicly	O
available	O
implementations	O
of	O
lp	O
solvers	O
we	O
will	O
show	O
that	O
the	O
erm	B
problem	O
for	O
halfspaces	O
in	O
the	O
realizable	O
case	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
program	O
for	O
simplicity	O
we	O
assume	O
the	O
homogenous	B
case	O
let	O
s	O
yim	O
be	O
a	O
training	B
set	B
of	O
size	O
m	O
since	O
we	O
assume	O
the	O
realizable	O
case	O
an	O
erm	B
predictor	B
should	O
have	O
zero	O
errors	O
on	O
the	O
training	B
set	B
that	O
is	O
we	O
are	O
looking	O
for	O
some	O
vector	O
w	O
rd	O
for	O
which	O
i	O
m	O
yi	O
equivalently	O
we	O
are	O
looking	O
for	O
some	O
vector	O
w	O
for	O
which	O
i	O
m	O
let	O
w	O
be	O
a	O
vector	O
that	O
satisfies	O
this	O
condition	O
must	O
exist	O
since	O
we	O
assume	O
realizability	B
define	O
and	O
let	O
w	O
w	O
therefore	O
for	O
all	O
i	O
we	O
have	O
w	O
we	O
have	O
thus	O
shown	O
that	O
there	O
exists	O
a	O
vector	O
that	O
satisfies	O
i	O
m	O
and	O
clearly	O
such	O
a	O
vector	O
is	O
an	O
erm	B
predictor	B
to	O
find	O
a	O
vector	O
that	O
satisfies	O
equation	O
we	O
can	O
rely	O
on	O
an	O
lp	O
solver	O
as	O
follows	O
set	B
a	O
to	O
be	O
the	O
m	O
d	O
matrix	O
whose	O
rows	O
are	O
the	O
instances	O
multiplied	O
namely	O
in	O
time	O
polynomial	O
in	O
m	O
d	O
and	O
in	O
the	O
representation	O
size	O
of	O
real	O
numbers	O
linear	B
predictors	I
by	O
yi	O
that	O
is	O
aij	O
yi	O
xij	O
where	O
xij	O
is	O
the	O
j	O
th	O
element	O
of	O
the	O
vector	O
xi	O
let	O
v	O
be	O
the	O
vector	O
rm	O
then	O
equation	O
can	O
be	O
rewritten	O
as	O
aw	O
v	O
the	O
lp	O
form	O
requires	O
a	O
maximization	O
objective	O
yet	O
all	O
the	O
w	O
that	O
satisfy	O
the	O
constraints	O
are	O
equal	O
candidates	O
as	O
output	O
hypotheses	O
thus	O
we	O
set	B
a	O
dummy	O
objective	O
u	O
rd	O
perceptron	B
for	O
halfspaces	O
a	O
different	O
implementation	O
of	O
the	O
erm	B
rule	O
is	O
the	O
perceptron	B
algorithm	O
of	O
rosenblatt	O
the	O
perceptron	B
is	O
an	O
iterative	O
algorithm	O
that	O
constructs	O
a	O
sequence	O
of	O
vectors	O
initially	O
is	O
set	B
to	O
be	O
the	O
all-zeros	O
vector	O
at	O
iteration	O
t	O
the	O
perceptron	B
finds	O
an	O
example	O
i	O
that	O
is	O
mislabeled	O
by	O
wt	O
namely	O
an	O
example	O
for	O
which	O
yi	O
then	O
the	O
perceptron	B
updates	O
wt	O
by	O
adding	O
to	O
it	O
the	O
instance	B
xi	O
scaled	O
by	O
the	O
label	B
yi	O
that	O
is	O
wt	O
yixi	O
recall	B
that	O
our	O
goal	O
is	O
to	O
have	O
for	O
all	O
i	O
and	O
note	O
that	O
yixi	O
hence	O
the	O
update	O
of	O
the	O
perceptron	B
guides	O
the	O
solution	O
to	O
be	O
more	O
correct	O
on	O
the	O
i	O
th	O
example	O
batch	O
perceptron	B
input	O
a	O
training	B
set	B
ym	O
initialize	O
for	O
t	O
if	O
i	O
s	O
t	O
then	O
wt	O
yixi	O
else	O
output	O
wt	O
the	O
following	O
theorem	O
guarantees	O
that	O
in	O
the	O
realizable	O
case	O
the	O
algorithm	O
stops	O
with	O
all	O
sample	O
points	O
correctly	O
classified	O
theorem	O
assume	O
that	O
ym	O
is	O
separable	B
let	O
b	O
and	O
let	O
r	O
maxi	O
then	O
the	O
perceptron	B
al	O
i	O
gorithm	O
stops	O
after	O
at	O
most	O
iterations	O
and	O
when	O
it	O
stops	O
it	O
holds	O
that	O
i	O
proof	O
by	O
the	O
definition	O
of	O
the	O
stopping	O
condition	O
if	O
the	O
perceptron	B
stops	O
it	O
must	O
have	O
separated	O
all	O
the	O
examples	O
we	O
will	O
show	O
that	O
if	O
the	O
perceptron	B
runs	O
for	O
t	O
iterations	O
then	O
we	O
must	O
have	O
t	O
which	O
implies	O
the	O
perceptron	B
must	O
stop	O
after	O
at	O
most	O
iterations	O
let	O
be	O
a	O
vector	O
that	O
achieves	O
the	O
minimum	O
in	O
the	O
definition	O
of	O
b	O
that	O
is	O
halfspaces	O
for	O
all	O
i	O
and	O
among	O
all	O
vectors	O
that	O
satisfy	O
these	O
constraints	O
is	O
of	O
minimal	O
norm	O
the	O
idea	O
of	O
the	O
proof	O
is	O
to	O
show	O
that	O
after	O
performing	O
t	O
iterations	O
the	O
cosine	O
of	O
the	O
angle	O
between	O
and	O
wt	O
is	O
at	O
least	O
rb	O
that	O
is	O
we	O
will	O
show	O
that	O
t	O
wt	O
t	O
rb	O
by	O
the	O
cauchy-schwartz	O
inequality	O
the	O
left-hand	O
side	O
of	O
equation	O
is	O
at	O
most	O
therefore	O
equation	O
would	O
imply	O
that	O
t	O
rb	O
t	O
which	O
will	O
conclude	O
our	O
proof	O
to	O
show	O
that	O
equation	O
holds	O
we	O
first	O
show	O
that	O
wt	O
t	O
indeed	O
at	O
the	O
first	O
iteration	O
and	O
therefore	O
while	O
on	O
iteration	O
t	O
if	O
we	O
update	O
using	O
example	O
yi	O
we	O
have	O
that	O
therefore	O
after	O
performing	O
t	O
iterations	O
we	O
get	O
t	O
wt	O
as	O
required	O
next	O
we	O
upper	O
bound	O
for	O
each	O
iteration	O
t	O
we	O
have	O
that	O
i	O
where	O
the	O
last	O
inequality	O
is	O
due	O
to	O
the	O
fact	O
that	O
example	O
i	O
is	O
necessarily	O
such	O
that	O
and	O
the	O
norm	O
of	O
xi	O
is	O
at	O
most	O
r	O
now	O
since	O
if	O
we	O
use	O
equation	O
recursively	O
for	O
t	O
iterations	O
we	O
obtain	O
that	O
combining	O
equation	O
with	O
equation	O
and	O
using	O
the	O
fact	O
that	O
b	O
we	O
obtain	O
that	O
t	O
r	O
t	O
t	O
r	O
b	O
t	O
b	O
r	O
we	O
have	O
thus	O
shown	O
that	O
equation	O
holds	O
and	O
this	O
concludes	O
our	O
proof	O
linear	B
predictors	I
remark	O
the	O
perceptron	B
is	O
simple	O
to	O
implement	O
and	O
is	O
guaranteed	O
to	O
converge	O
however	O
the	O
convergence	O
rate	O
depends	O
on	O
the	O
parameter	O
b	O
which	O
in	O
some	O
situations	O
might	O
be	O
exponentially	O
large	O
in	O
d	O
in	O
such	O
cases	O
it	O
would	O
be	O
better	O
to	O
implement	O
the	O
erm	B
problem	O
by	O
solving	O
a	O
linear	O
program	O
as	O
described	O
in	O
the	O
previous	O
section	O
nevertheless	O
for	O
many	O
natural	O
data	O
sets	O
the	O
size	O
of	O
b	O
is	O
not	O
too	O
large	O
and	O
the	O
perceptron	B
converges	O
quite	O
fast	O
the	O
vc	B
dimension	I
of	O
halfspaces	O
to	O
compute	O
the	O
vc	B
dimension	I
of	O
halfspaces	O
we	O
start	O
with	O
the	O
homogenous	B
case	O
theorem	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
in	O
rd	O
is	O
d	O
proof	O
first	O
consider	O
the	O
set	B
of	O
vectors	O
ed	O
where	O
for	O
every	O
i	O
the	O
vector	O
ei	O
is	O
the	O
all	O
zeros	O
vector	O
except	O
in	O
the	O
i	O
th	O
coordinate	O
this	O
set	B
is	O
shattered	O
by	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
indeed	O
for	O
every	O
labeling	O
yd	O
set	B
w	O
yd	O
and	O
then	O
yi	O
for	O
all	O
i	O
real	O
numbers	O
not	O
all	O
of	O
them	O
are	O
zero	O
such	O
that	O
next	O
let	O
be	O
a	O
set	B
of	O
d	O
vectors	O
in	O
rd	O
then	O
there	O
must	O
exist	O
aixi	O
let	O
i	O
ai	O
and	O
j	O
aj	O
either	O
i	O
or	O
j	O
is	O
nonempty	O
let	O
us	O
first	O
assume	O
that	O
both	O
of	O
them	O
are	O
nonempty	O
then	O
aixi	O
now	O
suppose	O
that	O
are	O
shattered	O
by	O
the	O
class	O
of	O
homogenous	B
classes	O
then	O
there	O
must	O
exist	O
a	O
vector	O
w	O
such	O
that	O
for	O
all	O
i	O
i	O
while	O
for	O
every	O
j	O
j	O
it	O
follows	O
that	O
aixi	O
w	O
w	O
j	O
j	O
j	O
j	O
i	O
i	O
i	O
i	O
i	O
i	O
j	O
j	O
which	O
leads	O
to	O
a	O
contradiction	O
finally	O
if	O
j	O
i	O
is	O
empty	O
then	O
the	O
right-most	O
left-most	O
inequality	O
should	O
be	O
replaced	O
by	O
an	O
equality	O
which	O
still	O
leads	O
to	O
a	O
contradiction	O
theorem	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
nonhomogenous	O
halfspaces	O
in	O
rd	O
is	O
d	O
proof	O
first	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
set	B
of	O
vectors	O
ed	O
is	O
shattered	O
by	O
the	O
class	O
of	O
nonhomogenous	O
halfspaces	O
second	O
suppose	O
that	O
the	O
vectors	O
are	O
shattered	O
by	O
the	O
class	O
of	O
nonhomogenous	O
halfspaces	O
but	O
using	O
the	O
reduction	O
we	O
have	O
shown	O
in	O
the	O
beginning	O
of	O
this	O
chapter	O
it	O
follows	O
that	O
there	O
are	O
d	O
vectors	O
in	O
that	O
are	O
shattered	O
by	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
but	O
this	O
contradicts	O
theorem	O
linear	B
regression	B
r	O
r	O
r	O
r	O
r	O
r	O
r	O
rr	O
r	O
r	O
figure	O
linear	B
regression	B
for	O
d	O
for	O
instance	B
the	O
x-axis	O
may	O
denote	O
the	O
age	O
of	O
the	O
baby	O
and	O
the	O
y-axis	O
her	O
weight	O
linear	B
regression	B
linear	B
regression	B
is	O
a	O
common	O
statistical	O
tool	O
for	O
modeling	O
the	O
relationship	O
between	O
some	O
explanatory	O
variables	O
and	O
some	O
real	O
valued	O
outcome	O
cast	O
as	O
a	O
learning	O
problem	O
the	O
domain	B
set	B
x	O
is	O
a	O
subset	O
of	O
rd	O
for	O
some	O
d	O
and	O
the	O
label	B
set	B
y	O
is	O
the	O
set	B
of	O
real	O
numbers	O
we	O
would	O
like	O
to	O
learn	O
a	O
linear	O
function	B
h	O
rd	O
r	O
that	O
best	O
approximates	O
the	O
relationship	O
between	O
our	O
variables	O
for	O
example	O
predicting	O
the	O
weight	O
of	O
a	O
baby	O
as	O
a	O
function	B
of	O
her	O
age	O
and	O
weight	O
at	O
birth	O
figure	O
shows	O
an	O
example	O
of	O
a	O
linear	B
regression	B
predictor	B
for	O
d	O
the	O
hypothesis	B
class	I
of	O
linear	B
regression	B
predictors	O
is	O
simply	O
the	O
set	B
of	O
linear	O
functions	O
hreg	O
ld	O
b	O
w	O
rd	O
b	O
r	O
next	O
we	O
need	O
to	O
define	O
a	O
loss	B
function	B
for	O
regression	B
while	O
in	O
classification	O
the	O
definition	O
of	O
the	O
loss	B
is	O
straightforward	O
as	O
y	O
simply	O
indicates	O
whether	O
hx	O
correctly	O
predicts	O
y	O
or	O
not	O
in	O
regression	B
if	O
the	O
baby	O
s	O
weight	O
is	O
kg	O
both	O
the	O
predictions	O
kg	O
and	O
kg	O
are	O
wrong	O
but	O
we	O
would	O
clearly	O
prefer	O
the	O
former	O
over	O
the	O
latter	O
we	O
therefore	O
need	O
to	O
define	O
how	O
much	O
we	O
shall	O
be	O
penalized	O
for	O
the	O
discrepancy	O
between	O
hx	O
and	O
y	O
one	O
common	O
way	O
is	O
to	O
use	O
the	O
squared-loss	O
function	B
namely	O
y	O
for	O
this	O
loss	B
function	B
the	O
empirical	B
risk	B
function	B
is	O
called	O
the	O
mean	O
squared	O
error	O
namely	O
lsh	O
m	O
linear	B
predictors	I
in	O
the	O
next	O
subsection	O
we	O
will	O
see	O
how	O
to	O
implement	O
the	O
erm	B
rule	O
for	O
linear	B
regression	B
with	O
respect	O
to	O
the	O
squared	O
loss	B
of	O
course	O
there	O
are	O
a	O
variety	O
of	O
other	O
loss	B
functions	O
that	O
one	O
can	O
use	O
for	O
example	O
the	O
absolute	B
value	I
loss	B
function	B
y	O
y	O
the	O
erm	B
rule	O
for	O
the	O
absolute	B
value	I
loss	B
function	B
can	O
be	O
implemented	O
using	O
linear	B
programming	I
exercise	O
note	O
that	O
since	O
linear	B
regression	B
is	O
not	O
a	O
binary	O
prediction	O
task	O
we	O
cannot	O
analyze	O
its	O
sample	B
complexity	I
using	O
the	O
vc-dimension	O
one	O
possible	O
analysis	O
of	O
the	O
sample	B
complexity	I
of	O
linear	B
regression	B
is	O
by	O
relying	O
on	O
the	O
discretization	B
trick	I
remark	O
in	O
chapter	O
namely	O
if	O
we	O
are	O
happy	O
with	O
a	O
representation	O
of	O
each	O
element	O
of	O
the	O
vector	O
w	O
and	O
the	O
bias	B
b	O
using	O
a	O
finite	O
number	O
of	O
bits	O
a	O
bits	O
floating	O
point	O
representation	O
then	O
the	O
hypothesis	B
class	I
becomes	O
finite	O
and	O
its	O
size	O
is	O
at	O
most	O
we	O
can	O
now	O
rely	O
on	O
sample	B
complexity	I
bounds	O
for	O
finite	O
hypothesis	B
classes	O
as	O
described	O
in	O
chapter	O
note	O
however	O
that	O
to	O
apply	O
the	O
sample	B
complexity	I
bounds	O
from	O
chapter	O
we	O
also	O
need	O
that	O
the	O
loss	B
function	B
will	O
be	O
bounded	O
later	O
in	O
the	O
book	O
we	O
will	O
describe	O
more	O
rigorous	O
means	O
to	O
analyze	O
the	O
sample	B
complexity	I
of	O
regression	B
problems	O
least	B
squares	I
least	B
squares	I
is	O
the	O
algorithm	O
that	O
solves	O
the	O
erm	B
problem	O
for	O
the	O
hypothesis	B
class	I
of	O
linear	B
regression	B
predictors	O
with	O
respect	O
to	O
the	O
squared	O
loss	B
the	O
erm	B
problem	O
with	O
respect	O
to	O
this	O
class	O
given	O
a	O
training	B
set	B
s	O
and	O
using	O
the	O
homogenous	B
version	O
of	O
ld	O
is	O
to	O
find	O
argmin	O
w	O
lshw	O
argmin	O
w	O
m	O
to	O
solve	O
the	O
problem	O
we	O
calculate	O
the	O
gradient	B
of	O
the	O
objective	O
function	B
and	O
compare	O
it	O
to	O
zero	O
that	O
is	O
we	O
need	O
to	O
solve	O
m	O
yixi	O
we	O
can	O
rewrite	O
the	O
problem	O
as	O
the	O
problem	O
aw	O
b	O
where	O
a	O
xi	O
i	O
and	O
b	O
yixi	O
or	O
in	O
matrix	O
form	O
a	O
b	O
linear	B
regression	B
xm	O
xm	O
ym	O
xm	O
if	O
a	O
is	O
invertible	O
then	O
the	O
solution	O
to	O
the	O
erm	B
problem	O
is	O
w	O
a	O
b	O
the	O
case	O
in	O
which	O
a	O
is	O
not	O
invertible	O
requires	O
a	O
few	O
standard	O
tools	O
from	O
linear	O
algebra	O
which	O
are	O
available	O
in	O
appendix	O
c	O
it	O
can	O
be	O
easily	O
shown	O
that	O
if	O
the	O
training	O
instances	O
do	O
not	O
span	O
the	O
entire	O
space	O
of	O
rd	O
then	O
a	O
is	O
not	O
invertible	O
nevertheless	O
we	O
can	O
always	O
find	O
a	O
solution	O
to	O
the	O
system	O
aw	O
b	O
because	O
b	O
is	O
in	O
the	O
range	O
of	O
a	O
indeed	O
since	O
a	O
is	O
symmetric	O
we	O
can	O
write	O
it	O
using	O
its	O
eigenvalue	O
decomposition	O
as	O
a	O
v	O
dv	O
where	O
d	O
is	O
a	O
diagonal	O
matrix	O
and	O
v	O
is	O
an	O
orthonormal	O
matrix	O
is	O
v	O
is	O
the	O
identity	O
d	O
d	O
matrix	O
define	O
d	O
to	O
be	O
the	O
diagonal	O
matrix	O
such	O
that	O
d	O
ii	O
if	O
dii	O
and	O
otherwise	O
d	O
ii	O
now	O
define	O
a	O
v	O
dv	O
and	O
w	O
ab	O
let	O
vi	O
denote	O
the	O
i	O
th	O
column	O
of	O
v	O
then	O
we	O
have	O
a	O
w	O
aab	O
v	O
dv	O
dv	O
v	O
ddv	O
i	O
b	O
that	O
is	O
a	O
w	O
is	O
the	O
projection	B
of	O
b	O
onto	O
the	O
span	O
of	O
those	O
vectors	O
vi	O
for	O
which	O
dii	O
since	O
the	O
linear	O
span	O
of	O
xm	O
is	O
the	O
same	O
as	O
the	O
linear	O
span	O
of	O
those	O
vi	O
and	O
b	O
is	O
in	O
the	O
linear	O
span	O
of	O
the	O
xi	O
we	O
obtain	O
that	O
a	O
w	O
b	O
which	O
concludes	O
our	O
argument	O
linear	B
regression	B
for	O
polynomial	B
regression	B
tasks	O
some	O
learning	O
tasks	O
call	O
for	O
nonlinear	O
predictors	O
such	O
as	O
polynomial	O
predictors	O
take	O
for	O
instance	B
a	O
one	O
dimensional	O
polynomial	O
function	B
of	O
degree	O
n	O
that	O
is	O
px	O
anxn	O
where	O
an	O
is	O
a	O
vector	O
of	O
coefficients	O
of	O
size	O
n	O
in	O
the	O
following	O
we	O
depict	O
a	O
training	B
set	B
that	O
is	O
better	O
fitted	O
using	O
a	O
degree	O
polynomial	O
predictor	B
than	O
using	O
a	O
linear	B
predictor	B
linear	B
predictors	I
we	O
will	O
focus	O
here	O
on	O
the	O
class	O
of	O
one	O
dimensional	O
n-degree	O
polynomial	O
re	O
gression	O
predictors	O
namely	O
hn	O
poly	O
px	O
where	O
p	O
is	O
a	O
one	O
dimensional	O
polynomial	O
of	O
degree	O
n	O
parameterized	O
by	O
a	O
vector	O
of	O
coefficients	O
an	O
note	O
that	O
x	O
r	O
since	O
this	O
is	O
a	O
one	O
dimensional	O
polynomial	O
and	O
y	O
r	O
as	O
this	O
is	O
a	O
regression	B
problem	O
one	O
way	O
to	O
learn	O
this	O
class	O
is	O
by	O
reduction	O
to	O
the	O
problem	O
of	O
linear	B
regression	B
which	O
we	O
have	O
already	O
shown	O
how	O
to	O
solve	O
to	O
translate	O
a	O
polynomial	B
regression	B
problem	O
to	O
a	O
linear	B
regression	B
problem	O
we	O
define	O
the	O
mapping	O
r	O
such	O
that	O
x	O
xn	O
then	O
we	O
have	O
that	O
p	O
anxn	O
and	O
we	O
can	O
find	O
the	O
optimal	O
vector	O
of	O
coefficients	O
a	O
by	O
using	O
the	O
least	B
squares	I
algorithm	O
as	O
shown	O
earlier	O
logistic	B
regression	B
in	O
logistic	B
regression	B
we	O
learn	O
a	O
family	O
of	O
functions	O
h	O
from	O
rd	O
to	O
the	O
interval	O
however	O
logistic	B
regression	B
is	O
used	O
for	O
classification	O
tasks	O
we	O
can	O
interpret	O
hx	O
as	O
the	O
probability	O
that	O
the	O
label	B
of	O
x	O
is	O
the	O
hypothesis	B
class	I
associated	O
with	O
logistic	B
regression	B
is	O
the	O
composition	O
of	O
a	O
sigmoid	O
function	B
sig	O
r	O
over	O
the	O
class	O
of	O
linear	O
functions	O
ld	O
in	O
particular	O
the	O
sigmoid	O
function	B
used	O
in	O
logistic	B
regression	B
is	O
the	O
logistic	O
function	B
defined	O
as	O
sigz	O
exp	O
z	O
the	O
name	O
sigmoid	O
means	O
s-shaped	O
referring	O
to	O
the	O
plot	O
of	O
this	O
function	B
shown	O
in	O
the	O
figure	O
logistic	B
regression	B
the	O
hypothesis	B
class	I
is	O
therefore	O
for	O
simplicity	O
we	O
are	O
using	O
homogenous	B
linear	O
functions	O
hsig	O
sig	O
ld	O
w	O
rd	O
note	O
that	O
when	O
is	O
very	O
large	O
then	O
is	O
close	O
to	O
whereas	O
if	O
is	O
very	O
small	O
then	O
is	O
close	O
to	O
recall	B
that	O
the	O
prediction	O
of	O
the	O
halfspace	B
corresponding	O
to	O
a	O
vector	O
w	O
is	O
therefore	O
the	O
predictions	O
of	O
the	O
halfspace	B
hypothesis	B
and	O
the	O
logistic	O
hypothesis	B
are	O
very	O
similar	O
whenever	O
is	O
large	O
however	O
when	O
is	O
close	O
to	O
we	O
have	O
that	O
intuitively	O
the	O
logistic	O
hypothesis	B
is	O
not	O
sure	O
about	O
the	O
value	O
of	O
the	O
label	B
so	O
it	O
guesses	O
that	O
the	O
label	B
is	O
with	O
probability	O
slightly	O
larger	O
than	O
in	O
contrast	O
the	O
halfspace	B
hypothesis	B
always	O
outputs	O
a	O
deterministic	O
prediction	O
of	O
either	O
or	O
even	O
if	O
is	O
very	O
close	O
to	O
next	O
we	O
need	O
to	O
specify	O
a	O
loss	B
function	B
that	O
is	O
we	O
should	O
define	O
how	O
bad	O
it	O
is	O
to	O
predict	O
some	O
hwx	O
given	O
that	O
the	O
true	O
label	B
is	O
y	O
clearly	O
we	O
would	O
like	O
that	O
hwx	O
would	O
be	O
large	O
if	O
y	O
and	O
that	O
hwx	O
the	O
probability	O
of	O
predicting	O
would	O
be	O
large	O
if	O
y	O
note	O
that	O
hwx	O
exp	O
exp	O
exp	O
therefore	O
any	O
reasonable	O
loss	B
function	B
would	O
increase	O
monotonically	O
with	O
or	O
equivalently	O
would	O
increase	O
monotonically	O
with	O
exp	O
the	O
logistic	B
loss	B
function	B
used	O
in	O
logistic	B
regression	B
penalizes	O
hw	O
based	O
on	O
the	O
log	O
of	O
exp	O
that	O
log	O
is	O
a	O
monotonic	O
function	B
that	O
is	O
y	O
log	O
exp	O
therefore	O
given	O
a	O
training	B
set	B
s	O
ym	O
the	O
erm	B
problem	O
associated	O
with	O
logistic	B
regression	B
is	O
argmin	O
w	O
rd	O
m	O
log	O
exp	O
the	O
advantage	O
of	O
the	O
logistic	B
loss	B
function	B
is	O
that	O
it	O
is	O
a	O
convex	B
function	B
with	O
respect	O
to	O
w	O
hence	O
the	O
erm	B
problem	O
can	O
be	O
solved	O
efficiently	O
using	O
standard	O
methods	O
we	O
will	O
study	O
how	O
to	O
learn	O
with	O
convex	B
functions	O
and	O
in	O
particular	O
specify	O
a	O
simple	O
algorithm	O
for	O
minimizing	O
convex	B
functions	O
in	O
later	O
chapters	O
the	O
erm	B
problem	O
associated	O
with	O
logistic	B
regression	B
is	O
identical	O
to	O
the	O
problem	O
of	O
finding	O
a	O
maximum	B
likelihood	I
estimator	O
a	O
well-known	O
statistical	O
approach	O
for	O
finding	O
the	O
parameters	O
that	O
maximize	O
the	O
joint	O
probability	O
of	O
a	O
given	O
data	O
set	B
assuming	O
a	O
specific	O
parametric	O
probability	O
function	B
we	O
will	O
study	O
the	O
maximum	B
likelihood	I
approach	O
in	O
chapter	O
linear	B
predictors	I
summary	O
the	O
family	O
of	O
linear	B
predictors	I
is	O
one	O
of	O
the	O
most	O
useful	O
families	O
of	O
hypothesis	B
classes	O
and	O
many	O
learning	O
algorithms	O
that	O
are	O
being	O
widely	O
used	O
in	O
practice	O
rely	O
on	O
linear	B
predictors	I
we	O
have	O
shown	O
efficient	O
algorithms	O
for	O
learning	O
linear	B
predictors	I
with	O
respect	O
to	O
the	O
zero-one	O
loss	B
in	O
the	O
separable	B
case	O
and	O
with	O
respect	O
to	O
the	O
squared	O
and	O
logistic	O
losses	O
in	O
the	O
unrealizable	O
case	O
in	O
later	O
chapters	O
we	O
will	O
present	O
the	O
properties	O
of	O
the	O
loss	B
function	B
that	O
enable	O
efficient	O
learning	O
naturally	O
linear	B
predictors	I
are	O
effective	O
whenever	O
we	O
assume	O
as	O
prior	B
knowledge	I
that	O
some	O
linear	B
predictor	B
attains	O
low	O
risk	B
with	O
respect	O
to	O
the	O
underlying	O
distribution	O
in	O
the	O
next	O
chapter	O
we	O
show	O
how	O
to	O
construct	O
nonlinear	O
predictors	O
by	O
composing	O
linear	B
predictors	I
on	O
top	O
of	O
simple	O
classes	O
this	O
will	O
enable	O
us	O
to	O
employ	O
linear	B
predictors	I
for	O
a	O
variety	O
of	O
prior	B
knowledge	I
assumptions	O
bibliographic	O
remarks	O
the	O
perceptron	B
algorithm	O
dates	O
back	O
to	O
rosenblatt	O
the	O
proof	O
of	O
its	O
convergence	O
rate	O
is	O
due	O
to	O
novikoff	O
least	B
squares	I
regression	B
goes	O
back	O
to	O
gauss	O
legendre	O
and	O
adrain	O
exercises	O
show	O
how	O
to	O
cast	O
the	O
erm	B
problem	O
of	O
linear	B
regression	B
with	O
respect	O
to	O
the	O
absolute	B
value	I
loss	B
function	B
y	O
y	O
as	O
a	O
linear	O
program	O
namely	O
show	O
how	O
to	O
write	O
the	O
problem	O
yi	O
min	O
w	O
as	O
a	O
linear	O
program	O
hint	O
start	O
with	O
proving	O
that	O
for	O
any	O
c	O
r	O
min	O
a	O
a	O
s	O
t	O
c	O
a	O
and	O
c	O
a	O
show	O
that	O
the	O
matrix	O
a	O
defined	O
in	O
equation	O
is	O
invertible	O
if	O
and	O
only	O
if	O
xm	O
span	O
rd	O
show	O
that	O
theorem	O
is	O
tight	O
in	O
the	O
following	O
sense	O
for	O
any	O
positive	O
integer	O
m	O
there	O
exist	O
a	O
vector	O
w	O
rd	O
some	O
appropriate	O
d	O
and	O
a	O
sequence	O
of	O
examples	O
ym	O
such	O
that	O
the	O
following	O
hold	O
r	O
maxi	O
m	O
and	O
for	O
all	O
i	O
m	O
w	O
note	O
that	O
using	O
the	O
notation	O
in	O
theorem	O
we	O
therefore	O
get	O
b	O
i	O
m	O
exercises	O
thus	O
m	O
when	O
running	O
the	O
perceptron	B
on	O
this	O
sequence	O
of	O
examples	O
it	O
makes	O
m	O
updates	O
before	O
converging	O
hint	O
choose	O
d	O
m	O
and	O
for	O
every	O
i	O
choose	O
xi	O
ei	O
given	O
any	O
number	O
m	O
find	O
an	O
example	O
of	O
a	O
sequence	O
of	O
labeled	O
examples	O
ym	O
on	O
which	O
the	O
upper	O
bound	O
of	O
theorem	O
equals	O
m	O
and	O
the	O
perceptron	B
algorithm	O
is	O
bound	O
to	O
make	O
m	O
mistakes	O
hint	O
set	B
each	O
xi	O
to	O
be	O
a	O
third	O
dimensional	O
vector	O
of	O
the	O
form	O
b	O
yi	O
where	O
let	O
w	O
be	O
the	O
vector	O
now	O
go	O
over	O
the	O
proof	O
of	O
the	O
perceptron	B
s	O
upper	O
bound	O
see	O
where	O
we	O
used	O
inequalities	O
rather	O
than	O
equalities	O
and	O
figure	O
out	O
scenarios	O
where	O
the	O
inequality	O
actually	O
holds	O
with	O
equality	O
suppose	O
we	O
modify	O
the	O
perceptron	B
algorithm	O
as	O
follows	O
in	O
the	O
update	O
step	O
instead	O
of	O
performing	O
wt	O
yixi	O
whenever	O
we	O
make	O
a	O
mistake	O
we	O
perform	O
wt	O
yixi	O
for	O
some	O
prove	O
that	O
the	O
modified	O
perceptron	B
will	O
perform	O
the	O
same	O
number	O
of	O
iterations	O
as	O
the	O
vanilla	O
perceptron	B
and	O
will	O
converge	O
to	O
a	O
vector	O
that	O
points	O
to	O
the	O
same	O
direction	O
as	O
the	O
output	O
of	O
the	O
vanilla	O
perceptron	B
in	O
this	O
problem	O
we	O
will	O
get	O
bounds	O
on	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
balls	O
in	O
rd	O
that	O
is	O
bd	O
v	O
rd	O
r	O
where	O
bvrx	O
if	O
r	O
otherwise	O
consider	O
the	O
mapping	O
rd	O
defined	O
by	O
show	O
that	O
if	O
xm	O
are	O
shattered	O
by	O
bd	O
then	O
are	O
shattered	O
by	O
the	O
class	O
of	O
halfspaces	O
in	O
this	O
question	O
we	O
assume	O
that	O
what	O
does	O
this	O
tell	O
us	O
about	O
vcdimbd	O
find	O
a	O
set	B
of	O
d	O
points	O
in	O
rd	O
that	O
is	O
shattered	O
by	O
bd	O
conclude	O
that	O
d	O
vcdimbd	O
d	O
boosting	B
boosting	B
is	O
an	O
algorithmic	O
paradigm	O
that	O
grew	O
out	O
of	O
a	O
theoretical	O
question	O
and	O
became	O
a	O
very	O
practical	O
machine	O
learning	O
tool	O
the	O
boosting	B
approach	O
uses	O
a	O
generalization	O
of	O
linear	B
predictors	I
to	O
address	O
two	O
major	O
issues	O
that	O
have	O
been	O
raised	O
earlier	O
in	O
the	O
book	O
the	O
first	O
is	O
the	O
bias-complexity	B
tradeoff	I
we	O
have	O
seen	O
chapter	O
that	O
the	O
error	O
of	O
an	O
erm	B
learner	O
can	O
be	O
decomposed	O
into	O
a	O
sum	O
of	O
approximation	B
error	I
and	O
estimation	B
error	I
the	O
more	O
expressive	O
the	O
hypothesis	B
class	I
the	O
learner	O
is	O
searching	O
over	O
the	O
smaller	O
the	O
approximation	B
error	I
is	O
but	O
the	O
larger	O
the	O
estimation	B
error	I
becomes	O
a	O
learner	O
is	O
thus	O
faced	O
with	O
the	O
problem	O
of	O
picking	O
a	O
good	O
tradeoff	O
between	O
these	O
two	O
considerations	O
the	O
boosting	B
paradigm	O
allows	O
the	O
learner	O
to	O
have	O
smooth	O
control	O
over	O
this	O
tradeoff	O
the	O
learning	O
starts	O
with	O
a	O
basic	O
class	O
might	O
have	O
a	O
large	O
approximation	B
error	I
and	O
as	O
it	O
progresses	O
the	O
class	O
that	O
the	O
predictor	B
may	O
belong	O
to	O
grows	O
richer	O
the	O
second	O
issue	O
that	O
boosting	B
addresses	O
is	O
the	O
computational	B
complexity	I
of	O
learning	O
as	O
seen	O
in	O
chapter	O
for	O
many	O
interesting	O
concept	O
classes	O
the	O
task	O
of	O
finding	O
an	O
erm	B
hypothesis	B
may	O
be	O
computationally	O
infeasible	O
a	O
boosting	B
algorithm	O
amplifies	O
the	O
accuracy	B
of	O
weak	O
learners	O
intuitively	O
one	O
can	O
think	O
of	O
a	O
weak	O
learner	O
as	O
an	O
algorithm	O
that	O
uses	O
a	O
simple	O
rule	O
of	O
thumb	O
to	O
output	O
a	O
hypothesis	B
that	O
comes	O
from	O
an	O
easy-to-learn	O
hypothesis	B
class	I
and	O
performs	O
just	O
slightly	O
better	O
than	O
a	O
random	O
guess	O
when	O
a	O
weak	O
learner	O
can	O
be	O
implemented	O
efficiently	O
boosting	B
provides	O
a	O
tool	O
for	O
aggregating	O
such	O
weak	O
hypotheses	O
to	O
approximate	O
gradually	O
good	O
predictors	O
for	O
larger	O
and	O
harder	O
to	O
learn	O
classes	O
in	O
this	O
chapter	O
we	O
will	O
describe	O
and	O
analyze	O
a	O
practically	O
useful	O
boosting	B
algorithm	O
adaboost	B
shorthand	O
for	O
adaptive	O
boosting	B
the	O
adaboost	B
algorithm	O
outputs	O
a	O
hypothesis	B
that	O
is	O
a	O
linear	O
combination	O
of	O
simple	O
hypotheses	O
in	O
other	O
words	O
adaboost	B
relies	O
on	O
the	O
family	O
of	O
hypothesis	B
classes	O
obtained	O
by	O
composing	O
a	O
linear	B
predictor	B
on	O
top	O
of	O
simple	O
classes	O
we	O
will	O
show	O
that	O
adaboost	B
enables	O
us	O
to	O
control	O
the	O
tradeoff	O
between	O
the	O
approximation	O
and	O
estimation	O
errors	O
by	O
varying	O
a	O
single	O
parameter	O
adaboost	B
demonstrates	O
a	O
general	O
theme	O
that	O
will	O
recur	O
later	O
in	O
the	O
book	O
of	O
expanding	O
the	O
expressiveness	O
of	O
linear	B
predictors	I
by	O
composing	O
them	O
on	O
top	O
of	O
other	O
functions	O
this	O
will	O
be	O
elaborated	O
in	O
section	O
adaboost	B
stemmed	O
from	O
the	O
theoretical	O
question	O
of	O
whether	O
an	O
efficient	O
weak	O
learner	O
can	O
be	O
boosted	O
into	O
an	O
efficient	O
strong	O
learner	O
this	O
question	O
was	O
raised	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
weak	O
learnability	O
by	O
kearns	O
and	O
valiant	O
in	O
and	O
solved	O
in	O
by	O
robert	O
schapire	O
then	O
a	O
graduate	O
student	O
at	O
mit	O
however	O
the	O
proposed	O
mechanism	O
was	O
not	O
very	O
practical	O
in	O
robert	O
schapire	O
and	O
yoav	O
freund	O
proposed	O
the	O
adaboost	B
algorithm	O
which	O
was	O
the	O
first	O
truly	O
practical	O
implementation	O
of	O
boosting	B
this	O
simple	O
and	O
elegant	O
algorithm	O
became	O
hugely	O
popular	O
and	O
freund	O
and	O
schapire	O
s	O
work	O
has	O
been	O
recognized	O
by	O
numerous	O
awards	O
furthermore	O
boosting	B
is	O
a	O
great	O
example	O
for	O
the	O
practical	O
impact	O
of	O
learning	O
theory	O
while	O
boosting	B
originated	O
as	O
a	O
purely	O
theoretical	O
problem	O
it	O
has	O
led	O
to	O
popular	O
and	O
widely	O
used	O
algorithms	O
indeed	O
as	O
we	O
shall	O
demonstrate	O
later	O
in	O
this	O
chapter	O
adaboost	B
has	O
been	O
successfully	O
used	O
for	O
learning	O
to	O
detect	O
faces	O
in	O
images	O
weak	O
learnability	O
recall	B
the	O
definition	O
of	O
pac	B
learning	O
given	O
in	O
chapter	O
a	O
hypothesis	B
class	I
h	O
is	O
pac	B
learnable	O
if	O
there	O
exist	O
mh	O
n	O
and	O
a	O
learning	O
algorithm	O
with	O
the	O
following	O
property	O
for	O
every	O
for	O
every	O
distribution	O
d	O
over	O
x	O
and	O
for	O
every	O
labeling	O
function	B
f	O
x	O
if	O
the	O
realizable	O
assumption	O
holds	O
with	O
respect	O
to	O
hd	O
f	O
then	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
mh	O
i	O
i	O
d	O
examples	O
generated	O
by	O
d	O
and	O
labeled	O
by	O
f	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
h	O
such	O
that	O
with	O
probability	O
of	O
at	O
least	O
ldf	O
furthermore	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
in	O
chapter	O
characterizes	O
the	O
family	O
of	O
learnable	O
classes	O
and	O
states	O
that	O
every	O
pac	B
learnable	O
class	O
can	O
be	O
learned	O
using	O
any	O
erm	B
algorithm	O
however	O
the	O
definition	O
of	O
pac	B
learning	O
and	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
ignores	O
the	O
computational	O
aspect	O
of	O
learning	O
indeed	O
as	O
we	O
have	O
shown	O
in	O
chapter	O
there	O
are	O
cases	O
in	O
which	O
implementing	O
the	O
erm	B
rule	O
is	O
computationally	O
hard	O
in	O
the	O
realizable	O
case	O
however	O
perhaps	O
we	O
can	O
trade	O
computational	O
hardness	O
with	O
the	O
requirement	O
for	O
accuracy	B
given	O
a	O
distribution	O
d	O
and	O
a	O
target	O
labeling	O
function	B
f	O
maybe	O
there	O
exists	O
an	O
efficiently	O
computable	O
learning	O
algorithm	O
whose	O
error	O
is	O
just	O
slightly	O
better	O
than	O
a	O
random	O
guess	O
this	O
motivates	O
the	O
following	O
definition	O
definition	O
a	O
learning	O
algorithm	O
a	O
is	O
a	O
for	O
a	O
class	O
h	O
if	O
there	O
exists	O
a	O
function	B
mh	O
n	O
such	O
that	O
for	O
every	O
for	O
every	O
distribution	O
d	O
over	O
x	O
and	O
for	O
every	O
labeling	O
function	B
f	O
x	O
if	O
the	O
realizable	O
assumption	O
holds	O
with	O
respect	O
to	O
hd	O
f	O
then	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
mh	O
i	O
i	O
d	O
examples	O
generated	O
by	O
d	O
and	O
labeled	O
by	O
f	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
h	O
such	O
that	O
with	O
probability	O
of	O
at	O
least	O
ldf	O
a	O
hypothesis	B
class	I
h	O
is	O
if	O
there	O
exists	O
a	O
for	O
that	O
class	O
boosting	B
this	O
definition	O
is	O
almost	O
identical	O
to	O
the	O
definition	O
of	O
pac	B
learning	O
which	O
here	O
we	O
will	O
call	O
strong	B
learning	I
with	O
one	O
crucial	O
difference	O
strong	O
learnability	O
implies	O
the	O
ability	O
to	O
find	O
an	O
arbitrarily	O
good	O
classifier	B
error	O
rate	O
at	O
most	O
for	O
an	O
arbitrarily	O
small	O
in	O
weak	O
learnability	O
however	O
we	O
only	O
need	O
to	O
output	O
a	O
hypothesis	B
whose	O
error	O
rate	O
is	O
at	O
most	O
namely	O
whose	O
error	O
rate	O
is	O
slightly	O
better	O
than	O
what	O
a	O
random	O
labeling	O
would	O
give	O
us	O
the	O
hope	O
is	O
that	O
it	O
may	O
be	O
easier	O
to	O
come	O
up	O
with	O
efficient	O
weak	O
learners	O
than	O
with	O
efficient	O
pac	B
learners	O
the	O
fundamental	O
theorem	O
of	O
learning	O
states	O
that	O
if	O
a	O
hypothesis	B
class	I
h	O
has	O
a	O
vc	B
dimension	I
d	O
then	O
the	O
sample	B
complexity	I
of	O
pac	B
learning	O
h	O
satisfies	O
mh	O
where	O
is	O
a	O
constant	O
applying	O
this	O
with	O
we	O
immediately	O
obtain	O
that	O
if	O
d	O
then	O
h	O
is	O
not	O
this	O
implies	O
that	O
from	O
the	O
statistical	O
perspective	O
if	O
we	O
ignore	O
computational	B
complexity	I
weak	O
learnability	O
is	O
also	O
characterized	O
by	O
the	O
vc	B
dimension	I
of	O
h	O
and	O
therefore	O
is	O
just	O
as	O
hard	O
as	O
pac	B
learning	O
however	O
when	O
we	O
do	O
consider	O
computational	B
complexity	I
the	O
potential	O
advantage	O
of	O
weak	B
learning	I
is	O
that	O
maybe	O
there	O
is	O
an	O
algorithm	O
that	O
satisfies	O
the	O
requirements	O
of	O
weak	B
learning	I
and	O
can	O
be	O
implemented	O
efficiently	O
one	O
possible	O
approach	O
is	O
to	O
take	O
a	O
simple	O
hypothesis	B
class	I
denoted	O
b	O
and	O
to	O
apply	O
erm	B
with	O
respect	O
to	O
b	O
as	O
the	O
weak	B
learning	I
algorithm	O
for	O
this	O
to	O
work	O
we	O
need	O
that	O
b	O
will	O
satisfy	O
two	O
requirements	O
ermb	O
is	O
efficiently	O
implementable	O
for	O
every	O
sample	O
that	O
is	O
labeled	O
by	O
some	O
hypothesis	B
from	O
h	O
any	O
ermb	O
hypothesis	B
will	O
have	O
an	O
error	O
of	O
at	O
most	O
then	O
the	O
immediate	O
question	O
is	O
whether	O
we	O
can	O
boost	O
an	O
efficient	O
weak	O
learner	O
into	O
an	O
efficient	O
strong	O
learner	O
in	O
the	O
next	O
section	O
we	O
will	O
show	O
that	O
this	O
is	O
indeed	O
possible	O
but	O
before	O
that	O
let	O
us	O
show	O
an	O
example	O
in	O
which	O
efficient	O
weak	O
learnability	O
of	O
a	O
class	O
h	O
is	O
possible	O
using	O
a	O
base	B
hypothesis	B
class	I
b	O
example	O
learning	O
of	O
classifiers	O
using	O
decision	B
stumps	I
let	O
x	O
r	O
and	O
let	O
h	O
be	O
the	O
class	O
of	O
classifiers	O
namely	O
h	O
r	O
b	O
where	O
for	O
every	O
x	O
h	O
b	O
if	O
x	O
or	O
x	O
if	O
x	O
an	O
example	O
hypothesis	B
b	O
is	O
illustrated	O
as	O
follows	O
let	O
b	O
be	O
the	O
class	O
of	O
decision	B
stumps	I
that	O
is	O
b	O
signx	O
b	O
r	O
b	O
in	O
the	O
following	O
we	O
show	O
that	O
ermb	O
is	O
a	O
learner	O
for	O
h	O
for	O
weak	O
learnability	O
to	O
see	O
that	O
we	O
first	O
show	O
that	O
for	O
every	O
distribution	O
that	O
is	O
consistent	B
with	O
h	O
there	O
exists	O
a	O
decision	O
stump	O
with	O
ldh	O
indeed	O
just	O
note	O
that	O
every	O
classifier	B
in	O
h	O
consists	O
of	O
three	O
regions	O
unbounded	O
rays	O
and	O
a	O
center	O
interval	O
with	O
alternate	O
labels	O
for	O
any	O
pair	O
of	O
such	O
regions	O
there	O
exists	O
a	O
decision	O
stump	O
that	O
agrees	O
with	O
the	O
labeling	O
of	O
these	O
two	O
components	O
note	O
that	O
for	O
every	O
distribution	O
d	O
over	O
r	O
and	O
every	O
partitioning	O
of	O
the	O
line	O
into	O
three	O
such	O
regions	O
one	O
of	O
these	O
regions	O
must	O
have	O
d-weight	O
of	O
at	O
most	O
let	O
h	O
h	O
be	O
a	O
zero	O
error	O
hypothesis	B
a	O
decision	O
stump	O
that	O
disagrees	O
with	O
h	O
only	O
on	O
such	O
a	O
region	O
has	O
an	O
error	O
of	O
at	O
most	O
finally	O
since	O
the	O
vc-dimension	O
of	O
decision	B
stumps	I
is	O
if	O
the	O
sample	O
size	O
is	O
greater	O
than	O
then	O
with	O
probability	O
of	O
at	O
least	O
the	O
ermb	O
rule	O
returns	O
a	O
hypothesis	B
with	O
an	O
error	O
of	O
at	O
most	O
setting	O
we	O
obtain	O
that	O
the	O
error	O
of	O
ermb	O
is	O
at	O
most	O
we	O
see	O
that	O
ermb	O
is	O
a	O
learner	O
for	O
h	O
we	O
next	O
show	O
how	O
to	O
implement	O
the	O
erm	B
rule	O
efficiently	O
for	O
decision	B
stumps	I
efficient	O
implementation	O
of	O
erm	B
for	O
decision	B
stumps	I
let	O
x	O
rd	O
and	O
consider	O
the	O
base	B
hypothesis	B
class	I
of	O
decision	B
stumps	I
over	O
rd	O
namely	O
hds	O
sign	O
xi	O
b	O
r	O
i	O
b	O
for	O
simplicity	O
assume	O
that	O
b	O
that	O
is	O
we	O
focus	O
on	O
all	O
the	O
hypotheses	O
in	O
hds	O
of	O
the	O
form	O
sign	O
xi	O
let	O
s	O
ym	O
be	O
a	O
training	B
set	B
we	O
will	O
show	O
how	O
to	O
implement	O
an	O
erm	B
rule	O
namely	O
how	O
to	O
find	O
a	O
decision	O
stump	O
that	O
minimizes	O
lsh	O
furthermore	O
since	O
in	O
the	O
next	O
section	O
we	O
will	O
show	O
that	O
adaboost	B
requires	O
finding	O
a	O
hypothesis	B
with	O
a	O
small	O
risk	B
relative	O
to	O
some	O
distribution	O
over	O
s	O
we	O
will	O
show	O
here	O
how	O
to	O
minimize	O
such	O
risk	B
functions	O
concretely	O
let	O
d	O
be	O
a	O
probability	O
vector	O
in	O
rm	O
is	O
all	O
elements	O
of	O
d	O
are	O
i	O
di	O
the	O
weak	O
learner	O
we	O
describe	O
later	O
receives	O
d	O
and	O
s	O
and	O
outputs	O
a	O
decision	O
stump	O
h	O
x	O
y	O
that	O
minimizes	O
the	O
risk	B
w	O
r	O
t	O
d	O
nonnegative	O
ldh	O
note	O
that	O
if	O
d	O
then	O
ldh	O
lsh	O
recall	B
that	O
each	O
decision	O
stump	O
is	O
parameterized	O
by	O
an	O
index	O
j	O
and	O
a	O
threshold	O
therefore	O
minimizing	O
ldh	O
amounts	O
to	O
solving	O
the	O
problem	O
iyi	O
min	O
j	O
min	O
r	O
fix	O
j	O
and	O
let	O
us	O
sort	O
the	O
examples	O
so	O
that	O
xmj	O
define	O
i	O
note	O
that	O
for	O
any	O
r	O
j	O
xij	O
there	O
exists	O
j	O
that	O
yields	O
the	O
same	O
predictions	O
for	O
the	O
sample	O
s	O
as	O
the	O
boosting	B
threshold	O
therefore	O
instead	O
of	O
minimizing	O
over	O
r	O
we	O
can	O
minimize	O
over	O
j	O
this	O
already	O
gives	O
us	O
an	O
efficient	O
procedure	O
choose	O
j	O
and	O
j	O
that	O
minimize	O
the	O
objective	O
value	O
of	O
equation	O
for	O
every	O
j	O
and	O
j	O
we	O
have	O
to	O
calculate	O
a	O
sum	O
over	O
m	O
examples	O
therefore	O
the	O
runtime	O
of	O
this	O
approach	O
would	O
be	O
we	O
next	O
show	O
a	O
simple	O
trick	O
that	O
enables	O
us	O
to	O
minimize	O
the	O
objective	O
in	O
time	O
odm	O
the	O
observation	O
is	O
as	O
follows	O
suppose	O
we	O
have	O
calculated	O
the	O
objective	O
for	O
xij	O
let	O
f	O
be	O
the	O
value	O
of	O
the	O
objective	O
then	O
when	O
we	O
consider	O
we	O
have	O
that	O
f	O
f	O
f	O
yidi	O
therefore	O
we	O
can	O
calculate	O
the	O
objective	O
at	O
in	O
a	O
constant	O
time	O
given	O
the	O
objective	O
at	O
the	O
previous	O
threshold	O
it	O
follows	O
that	O
after	O
a	O
preprocessing	O
step	O
in	O
which	O
we	O
sort	O
the	O
examples	O
with	O
respect	O
to	O
each	O
coordinate	O
the	O
minimization	O
problem	O
can	O
be	O
performed	O
in	O
time	O
odm	O
this	O
yields	O
the	O
following	O
pseudocode	O
erm	B
for	O
decision	B
stumps	I
input	O
training	B
set	B
s	O
ym	O
distribution	O
vector	O
d	O
goal	O
find	O
that	O
solve	O
equation	O
initialize	O
f	O
for	O
j	O
d	O
sort	O
s	O
using	O
the	O
j	O
th	O
coordinate	O
and	O
denote	O
xmj	O
def	O
xmj	O
f	O
di	O
if	O
f	O
f	O
f	O
f	O
j	O
for	O
i	O
m	O
f	O
f	O
yidi	O
if	O
f	O
f	O
and	O
xij	O
f	O
f	O
j	O
output	O
adaboost	B
adaboost	B
for	O
adaptive	O
boosting	B
is	O
an	O
algorithm	O
that	O
has	O
access	O
to	O
a	O
weak	O
learner	O
and	O
finds	O
a	O
hypothesis	B
with	O
a	O
low	O
empirical	B
risk	B
the	O
adaboost	B
algorithm	O
receives	O
as	O
input	O
a	O
training	B
set	B
of	O
examples	O
s	O
ym	O
where	O
for	O
each	O
i	O
yi	O
f	O
for	O
some	O
labeling	O
function	B
f	O
the	O
boosting	B
process	O
proceeds	O
in	O
a	O
sequence	O
of	O
consecutive	O
rounds	O
at	O
round	O
t	O
the	O
booster	O
first	O
defines	O
adaboost	B
dt	O
a	O
distribution	O
over	O
the	O
examples	O
in	O
s	O
denoted	O
dt	O
that	O
is	O
dt	O
rm	O
and	O
i	O
then	O
the	O
booster	O
passes	O
the	O
distribution	O
dt	O
and	O
the	O
sample	O
s	O
to	O
the	O
weak	O
learner	O
way	O
the	O
weak	O
learner	O
can	O
construct	O
i	O
i	O
d	O
examples	O
according	O
to	O
dt	O
and	O
f	O
the	O
weak	O
learner	O
is	O
assumed	O
to	O
return	O
a	O
weak	O
hypothesis	B
ht	O
whose	O
error	O
def	O
ldt	O
def	O
dt	O
i	O
course	O
there	O
is	O
a	O
probability	O
of	O
at	O
most	O
that	O
the	O
weak	O
learner	O
is	O
at	O
most	O
fails	O
then	O
adaboost	B
assigns	O
a	O
weight	O
for	O
ht	O
as	O
follows	O
wt	O
that	O
is	O
the	O
weight	O
of	O
ht	O
is	O
inversely	O
proportional	O
to	O
the	O
error	O
of	O
ht	O
at	O
the	O
end	O
of	O
the	O
round	O
adaboost	B
updates	O
the	O
distribution	O
so	O
that	O
examples	O
on	O
which	O
ht	O
errs	O
will	O
get	O
a	O
higher	O
probability	O
mass	O
while	O
examples	O
on	O
which	O
ht	O
is	O
correct	O
will	O
get	O
a	O
lower	O
probability	O
mass	O
intuitively	O
this	O
will	O
force	O
the	O
weak	O
learner	O
to	O
focus	O
on	O
the	O
problematic	O
examples	O
in	O
the	O
next	O
round	O
the	O
output	O
of	O
the	O
adaboost	B
algorithm	O
is	O
a	O
strong	O
classifier	B
that	O
is	O
based	O
on	O
a	O
weighted	O
sum	O
of	O
all	O
the	O
weak	O
hypotheses	O
the	O
pseudocode	O
of	O
adaboost	B
is	O
presented	O
in	O
the	O
following	O
log	O
adaboost	B
input	O
training	B
set	B
s	O
ym	O
weak	O
learner	O
wl	O
number	O
of	O
rounds	O
t	O
invoke	O
weak	O
learner	O
ht	O
wldt	O
s	O
m	O
m	O
initialize	O
for	O
t	O
t	O
compute	O
dt	O
let	O
wt	O
log	O
update	O
i	O
i	O
i	O
dt	O
dt	O
j	O
exp	O
wtyihtxi	O
exp	O
wtyj	O
htxj	O
output	O
the	O
hypothesis	B
hsx	O
sign	O
wthtx	O
for	O
all	O
i	O
m	O
the	O
following	O
theorem	O
shows	O
that	O
the	O
training	B
error	I
of	O
the	O
output	O
hypothesis	B
decreases	O
exponentially	O
fast	O
with	O
the	O
number	O
of	O
boosting	B
rounds	O
theorem	O
let	O
s	O
be	O
a	O
training	B
set	B
and	O
assume	O
that	O
at	O
each	O
iteration	O
of	O
adaboost	B
the	O
weak	O
learner	O
returns	O
a	O
hypothesis	B
for	O
which	O
then	O
the	O
training	B
error	I
of	O
the	O
output	O
hypothesis	B
of	O
adaboost	B
is	O
at	O
most	O
proof	O
for	O
each	O
t	O
denote	O
ft	O
lshs	O
m	O
exp	O
t	O
p	O
t	O
wphp	O
therefore	O
the	O
output	O
of	O
adaboost	B
boosting	B
is	O
ft	O
in	O
addition	O
denote	O
zt	O
m	O
e	O
yiftxi	O
note	O
that	O
for	O
any	O
hypothesis	B
we	O
have	O
that	O
e	O
yhx	O
therefore	O
lsft	O
zt	O
so	O
it	O
suffices	O
to	O
show	O
that	O
zt	O
e	O
to	O
upper	O
bound	O
zt	O
we	O
rewrite	O
it	O
as	O
zt	O
where	O
we	O
used	O
the	O
fact	O
that	O
because	O
therefore	O
it	O
suffices	O
to	O
show	O
that	O
for	O
every	O
round	O
t	O
zt	O
zt	O
zt	O
zt	O
zt	O
e	O
zt	O
to	O
do	O
so	O
we	O
first	O
note	O
that	O
using	O
a	O
simple	O
inductive	O
argument	O
for	O
all	O
t	O
and	O
i	O
i	O
i	O
by	O
our	O
assumption	O
tonically	O
increasing	O
in	O
we	O
obtain	O
that	O
since	O
the	O
function	B
ga	O
a	O
is	O
mono	O
hence	O
zt	O
i	O
e	O
yiftxi	O
e	O
yj	O
ftxj	O
i	O
e	O
yj	O
ftxj	O
e	O
e	O
e	O
yj	O
ftxj	O
e	O
yiftxie	O
e	O
e	O
linear	O
combinations	O
of	O
base	O
hypotheses	O
finally	O
using	O
the	O
inequality	O
a	O
e	O
a	O
we	O
have	O
that	O
e	O
this	O
shows	O
that	O
equation	O
holds	O
and	O
thus	O
concludes	O
our	O
proof	O
e	O
each	O
iteration	O
of	O
adaboost	B
involves	O
om	O
operations	O
as	O
well	O
as	O
a	O
single	O
call	O
to	O
the	O
weak	O
learner	O
therefore	O
if	O
the	O
weak	O
learner	O
can	O
be	O
implemented	O
efficiently	O
happens	O
in	O
the	O
case	O
of	O
erm	B
with	O
respect	O
to	O
decision	B
stumps	I
then	O
the	O
total	O
training	O
process	O
will	O
be	O
efficient	O
remark	O
theorem	O
assumes	O
that	O
at	O
each	O
iteration	O
of	O
adaboost	B
the	O
weak	O
learner	O
returns	O
a	O
hypothesis	B
with	O
weighted	O
sample	O
error	O
of	O
at	O
most	O
according	O
to	O
the	O
definition	O
of	O
a	O
weak	O
learner	O
it	O
can	O
fail	O
with	O
probability	O
using	O
the	O
union	B
bound	I
the	O
probability	O
that	O
the	O
weak	O
learner	O
will	O
not	O
fail	O
at	O
all	O
of	O
the	O
iterations	O
is	O
at	O
least	O
t	O
as	O
we	O
show	O
in	O
exercise	O
the	O
dependence	O
of	O
the	O
sample	B
complexity	I
on	O
can	O
always	O
be	O
logarithmic	O
in	O
and	O
therefore	O
invoking	O
the	O
weak	O
learner	O
with	O
a	O
very	O
small	O
is	O
not	O
problematic	O
we	O
can	O
therefore	O
assume	O
that	O
t	O
is	O
also	O
small	O
furthermore	O
since	O
the	O
weak	O
learner	O
is	O
only	O
applied	O
with	O
distributions	O
over	O
the	O
training	B
set	B
in	O
many	O
cases	O
we	O
can	O
implement	O
the	O
weak	O
learner	O
so	O
that	O
it	O
will	O
have	O
a	O
zero	O
probability	O
of	O
failure	O
this	O
is	O
the	O
case	O
for	O
example	O
in	O
the	O
weak	O
learner	O
that	O
finds	O
the	O
minimum	O
value	O
of	O
ldh	O
for	O
decision	B
stumps	I
as	O
described	O
in	O
the	O
previous	O
section	O
theorem	O
tells	O
us	O
that	O
the	O
empirical	B
risk	B
of	O
the	O
hypothesis	B
constructed	O
by	O
adaboost	B
goes	O
to	O
zero	O
as	O
t	O
grows	O
however	O
what	O
we	O
really	O
care	O
about	O
is	O
the	O
true	O
risk	B
of	O
the	O
output	O
hypothesis	B
to	O
argue	O
about	O
the	O
true	O
risk	B
we	O
note	O
that	O
the	O
output	O
of	O
adaboost	B
is	O
in	O
fact	O
a	O
composition	O
of	O
a	O
halfspace	B
over	O
the	O
predictions	O
of	O
the	O
t	O
weak	O
hypotheses	O
constructed	O
by	O
the	O
weak	O
learner	O
in	O
the	O
next	O
section	O
we	O
show	O
that	O
if	O
the	O
weak	O
hypotheses	O
come	O
from	O
a	O
base	B
hypothesis	B
class	I
of	O
low	O
vc-dimension	O
then	O
the	O
estimation	B
error	I
of	O
adaboost	B
will	O
be	O
small	O
namely	O
the	O
true	O
risk	B
of	O
the	O
output	O
of	O
adaboost	B
would	O
not	O
be	O
very	O
far	O
from	O
its	O
empirical	B
risk	B
linear	O
combinations	O
of	O
base	O
hypotheses	O
as	O
mentioned	O
previously	O
a	O
popular	O
approach	O
for	O
constructing	O
a	O
weak	O
learner	O
is	O
to	O
apply	O
the	O
erm	B
rule	O
with	O
respect	O
to	O
a	O
base	B
hypothesis	B
class	I
erm	B
over	O
decision	B
stumps	I
we	O
have	O
also	O
seen	O
that	O
boosting	B
outputs	O
a	O
composition	O
of	O
a	O
halfspace	B
over	O
the	O
predictions	O
of	O
the	O
weak	O
hypotheses	O
therefore	O
given	O
a	O
base	B
hypothesis	B
class	I
b	O
decision	B
stumps	I
the	O
output	O
of	O
adaboost	B
will	O
be	O
a	O
member	O
of	O
the	O
following	O
class	O
lb	O
t	O
x	O
sign	O
wthtx	O
w	O
rt	O
t	O
ht	O
b	O
that	O
is	O
each	O
h	O
lb	O
t	O
is	O
parameterized	O
by	O
t	O
base	O
hypotheses	O
from	O
b	O
and	O
by	O
a	O
vector	O
w	O
rt	O
the	O
prediction	O
of	O
such	O
an	O
h	O
on	O
an	O
instance	B
x	O
is	O
obtained	O
by	O
first	O
applying	O
the	O
t	O
base	O
hypotheses	O
to	O
construct	O
the	O
vector	O
boosting	B
ht	O
rt	O
and	O
then	O
applying	O
the	O
halfspace	B
defined	O
by	O
w	O
on	O
in	O
this	O
section	O
we	O
analyze	O
the	O
estimation	B
error	I
of	O
lb	O
t	O
by	O
bounding	O
the	O
vc-dimension	O
of	O
lb	O
t	O
in	O
terms	O
of	O
the	O
vc-dimension	O
of	O
b	O
and	O
t	O
we	O
will	O
show	O
that	O
up	O
to	O
logarithmic	O
factors	O
the	O
vc-dimension	O
of	O
lb	O
t	O
is	O
bounded	O
by	O
t	O
times	O
the	O
vc-dimension	O
of	O
b	O
it	O
follows	O
that	O
the	O
estimation	B
error	I
of	O
adaboost	B
grows	O
linearly	O
with	O
t	O
on	O
the	O
other	O
hand	O
the	O
empirical	B
risk	B
of	O
adaboost	B
decreases	O
with	O
t	O
in	O
fact	O
as	O
we	O
demonstrate	O
later	O
t	O
can	O
be	O
used	O
to	O
decrease	O
the	O
approximation	B
error	I
of	O
lb	O
t	O
therefore	O
the	O
parameter	O
t	O
of	O
adaboost	B
enables	O
us	O
to	O
control	O
the	O
bias-complexity	B
tradeoff	I
the	O
simple	O
example	O
in	O
which	O
x	O
r	O
and	O
the	O
base	O
class	O
is	O
decision	B
stumps	I
to	O
demonstrate	O
how	O
the	O
expressive	O
power	O
of	O
lb	O
t	O
increases	O
with	O
t	O
consider	O
signx	O
b	O
r	O
b	O
note	O
that	O
in	O
this	O
one	O
dimensional	O
case	O
is	O
in	O
fact	O
equivalent	O
to	O
halfspaces	O
on	O
r	O
now	O
let	O
h	O
be	O
the	O
rather	O
complex	O
class	O
to	O
halfspaces	O
on	O
the	O
line	O
of	O
piece-wise	O
constant	O
functions	O
let	O
gr	O
be	O
a	O
piece-wise	O
constant	O
function	B
with	O
at	O
most	O
r	O
pieces	O
that	O
is	O
there	O
exist	O
thresholds	O
r	O
such	O
that	O
grx	O
i	O
i	O
i	O
i	O
denote	O
by	O
gr	O
the	O
class	O
of	O
all	O
such	O
piece-wise	O
constant	O
classifiers	O
with	O
at	O
most	O
r	O
pieces	O
in	O
the	O
following	O
we	O
show	O
that	O
gt	O
t	O
namely	O
the	O
class	O
of	O
halfspaces	O
over	O
t	O
decision	B
stumps	I
yields	O
all	O
the	O
piece-wise	O
constant	O
classifiers	O
with	O
at	O
most	O
t	O
pieces	O
indeed	O
without	O
loss	B
of	O
generality	O
consider	O
any	O
g	O
gt	O
with	O
t	O
this	O
implies	O
that	O
if	O
x	O
is	O
in	O
the	O
interval	O
t	O
t	O
then	O
gx	O
for	O
example	O
now	O
the	O
function	B
hx	O
sign	O
wt	O
signx	O
t	O
where	O
and	O
for	O
t	O
wt	O
is	O
in	O
t	O
and	O
is	O
equal	O
to	O
g	O
exercise	O
linear	O
combinations	O
of	O
base	O
hypotheses	O
from	O
this	O
example	O
we	O
obtain	O
that	O
t	O
can	O
shatter	O
any	O
set	B
of	O
t	O
instances	O
in	O
r	O
hence	O
the	O
vc-dimension	O
of	O
t	O
is	O
at	O
least	O
t	O
therefore	O
t	O
is	O
a	O
parameter	O
that	O
can	O
control	O
the	O
bias-complexity	B
tradeoff	I
enlarging	O
t	O
yields	O
a	O
more	O
expressive	O
hypothesis	B
class	I
but	O
on	O
the	O
other	O
hand	O
might	O
increase	O
the	O
estimation	B
error	I
in	O
the	O
next	O
subsection	O
we	O
formally	O
upper	O
bound	O
the	O
vcdimension	O
of	O
lb	O
t	O
for	O
any	O
base	O
class	O
b	O
the	O
vc-dimension	O
of	O
lb	O
t	O
the	O
following	O
lemma	O
tells	O
us	O
that	O
the	O
vc-dimension	O
of	O
lb	O
t	O
is	O
upper	O
bounded	O
by	O
ovcdimb	O
t	O
o	O
notation	O
ignores	O
constants	O
and	O
logarithmic	O
factors	O
lemma	O
let	O
b	O
be	O
a	O
base	O
class	O
and	O
let	O
lb	O
t	O
be	O
as	O
defined	O
in	O
equation	O
assume	O
that	O
both	O
t	O
and	O
vcdimb	O
are	O
at	O
least	O
then	O
vcdimlb	O
t	O
t	O
logt	O
proof	O
denote	O
d	O
vcdimb	O
let	O
c	O
xm	O
be	O
a	O
set	B
that	O
is	O
shattered	O
by	O
lb	O
t	O
each	O
labeling	O
of	O
c	O
by	O
h	O
lb	O
t	O
is	O
obtained	O
by	O
first	O
choosing	O
ht	O
b	O
and	O
then	O
applying	O
a	O
halfspace	B
hypothesis	B
over	O
the	O
vector	O
ht	O
by	O
sauer	O
s	O
lemma	O
there	O
are	O
at	O
most	O
different	O
dichotomies	O
labelings	O
induced	O
by	O
b	O
over	O
c	O
therefore	O
we	O
need	O
to	O
choose	O
t	O
hypotheses	O
out	O
of	O
at	O
most	O
different	O
hypotheses	O
there	O
are	O
at	O
most	O
ways	O
to	O
do	O
it	O
next	O
for	O
each	O
such	O
choice	O
we	O
apply	O
a	O
linear	B
predictor	B
which	O
yields	O
at	O
most	O
dichotomies	O
therefore	O
the	O
overall	O
number	O
of	O
dichotomies	O
we	O
can	O
construct	O
is	O
upper	O
bounded	O
by	O
where	O
we	O
used	O
the	O
assumption	O
that	O
both	O
d	O
and	O
t	O
are	O
at	O
least	O
since	O
we	O
assume	O
that	O
c	O
is	O
shattered	O
we	O
must	O
have	O
that	O
the	O
preceding	O
is	O
at	O
least	O
which	O
yields	O
therefore	O
m	O
logm	O
lemma	O
in	O
chapter	O
a	O
tells	O
us	O
that	O
a	O
necessary	O
condition	O
for	O
the	O
above	O
to	O
hold	O
is	O
that	O
m	O
log	O
logd	O
which	O
concludes	O
our	O
proof	O
in	O
exercise	O
we	O
show	O
that	O
for	O
some	O
base	O
classes	O
b	O
it	O
also	O
holds	O
that	O
vcdimlb	O
t	O
t	O
boosting	B
a	O
c	O
b	O
d	O
figure	O
the	O
four	O
types	O
of	O
functions	O
g	O
used	O
by	O
the	O
base	O
hypotheses	O
for	O
face	O
recognition	O
the	O
value	O
of	O
g	O
for	O
type	O
a	O
or	O
b	O
is	O
the	O
difference	O
between	O
the	O
sum	O
of	O
the	O
pixels	O
within	O
two	O
rectangular	O
regions	O
these	O
regions	O
have	O
the	O
same	O
size	O
and	O
shape	O
and	O
are	O
horizontally	O
or	O
vertically	O
adjacent	O
for	O
type	O
c	O
the	O
value	O
of	O
g	O
is	O
the	O
sum	O
within	O
two	O
outside	O
rectangles	O
subtracted	O
from	O
the	O
sum	O
in	O
a	O
center	O
rectangle	O
for	O
type	O
d	O
we	O
compute	O
the	O
difference	O
between	O
diagonal	O
pairs	O
of	O
rectangles	O
adaboost	B
for	O
face	O
recognition	O
we	O
now	O
turn	O
to	O
a	O
base	B
hypothesis	B
that	O
has	O
been	O
proposed	O
by	O
viola	O
and	O
jones	O
for	O
the	O
task	O
of	O
face	O
recognition	O
in	O
this	O
task	O
the	O
instance	B
space	I
is	O
images	O
represented	O
as	O
matrices	O
of	O
gray	O
level	O
values	O
of	O
pixels	O
to	O
be	O
concrete	O
let	O
us	O
take	O
images	O
of	O
size	O
pixels	O
and	O
therefore	O
our	O
instance	B
space	I
is	O
the	O
set	B
of	O
real	O
valued	O
matrices	O
of	O
size	O
the	O
goal	O
is	O
to	O
learn	O
a	O
classifier	B
h	O
x	O
that	O
given	O
an	O
image	O
as	O
input	O
should	O
output	O
whether	O
the	O
image	O
is	O
of	O
a	O
human	O
face	O
or	O
not	O
each	O
hypothesis	B
in	O
the	O
base	O
class	O
is	O
of	O
the	O
form	O
hx	O
f	O
where	O
f	O
is	O
a	O
decision	O
stump	O
hypothesis	B
and	O
g	O
r	O
is	O
a	O
function	B
that	O
maps	O
an	O
image	O
to	O
a	O
scalar	O
each	O
function	B
g	O
is	O
parameterized	O
by	O
an	O
axis	O
aligned	O
rectangle	O
r	O
since	O
each	O
image	O
is	O
of	O
size	O
there	O
are	O
at	O
most	O
axis	O
aligned	O
rectangles	O
a	O
type	O
t	O
b	O
c	O
d	O
each	O
type	O
corresponds	O
to	O
a	O
mask	O
as	O
depicted	O
in	O
figure	O
to	O
calculate	O
g	O
we	O
stretch	O
the	O
mask	O
t	O
to	O
fit	O
the	O
rectangle	O
r	O
and	O
then	O
calculate	O
the	O
sum	O
of	O
the	O
pixels	O
is	O
sum	O
of	O
their	O
gray	O
level	O
values	O
that	O
lie	O
within	O
the	O
red	O
rectangles	O
and	O
subtract	O
it	O
from	O
the	O
sum	O
of	O
pixels	O
in	O
the	O
blue	O
rectangles	O
since	O
the	O
number	O
of	O
such	O
functions	O
g	O
is	O
at	O
most	O
we	O
can	O
implement	O
a	O
weak	O
learner	O
for	O
the	O
base	B
hypothesis	B
class	I
by	O
first	O
calculating	O
all	O
the	O
possible	O
outputs	O
of	O
g	O
on	O
each	O
image	O
and	O
then	O
apply	O
the	O
weak	O
learner	O
of	O
decision	B
stumps	I
described	O
in	O
the	O
previous	O
subsection	O
it	O
is	O
possible	O
to	O
perform	O
the	O
first	O
step	O
very	O
summary	O
figure	O
the	O
first	O
and	O
second	O
features	O
selected	O
by	O
adaboost	B
as	O
implemented	O
by	O
viola	O
and	O
jones	O
the	O
two	O
features	O
are	O
shown	O
in	O
the	O
top	O
row	O
and	O
then	O
overlaid	O
on	O
a	O
typical	O
training	O
face	O
in	O
the	O
bottom	O
row	O
the	O
first	O
feature	B
measures	O
the	O
difference	O
in	O
intensity	O
between	O
the	O
region	O
of	O
the	O
eyes	O
and	O
a	O
region	O
across	O
the	O
upper	O
cheeks	O
the	O
feature	B
capitalizes	O
on	O
the	O
observation	O
that	O
the	O
eye	O
region	O
is	O
often	O
darker	O
than	O
the	O
cheeks	O
the	O
second	O
feature	B
compares	O
the	O
intensities	O
in	O
the	O
eye	O
regions	O
to	O
the	O
intensity	O
across	O
the	O
bridge	O
of	O
the	O
nose	O
efficiently	O
by	O
a	O
preprocessing	O
step	O
in	O
which	O
we	O
calculate	O
the	O
integral	B
image	I
of	O
each	O
image	O
in	O
the	O
training	B
set	B
see	O
exercise	O
for	O
details	O
in	O
figure	O
we	O
depict	O
the	O
first	O
two	O
features	O
selected	O
by	O
adaboost	B
when	O
running	O
it	O
with	O
the	O
base	O
features	O
proposed	O
by	O
viola	O
and	O
jones	O
summary	O
boosting	B
is	O
a	O
method	O
for	O
amplifying	O
the	O
accuracy	B
of	O
weak	O
learners	O
in	O
this	O
chapter	O
we	O
described	O
the	O
adaboost	B
algorithm	O
we	O
have	O
shown	O
that	O
after	O
t	O
iterations	O
of	O
adaboost	B
it	O
returns	O
a	O
hypothesis	B
from	O
the	O
class	O
lb	O
t	O
obtained	O
by	O
composing	O
a	O
linear	O
classifier	B
on	O
t	O
hypotheses	O
from	O
a	O
base	O
class	O
b	O
we	O
have	O
demonstrated	O
how	O
the	O
parameter	O
t	O
controls	O
the	O
tradeoff	O
between	O
approximation	O
and	O
estimation	O
errors	O
in	O
the	O
next	O
chapter	O
we	O
will	O
study	O
how	O
to	O
tune	O
parameters	O
such	O
as	O
t	O
based	O
on	O
the	O
data	O
bibliographic	O
remarks	O
as	O
mentioned	O
before	O
boosting	B
stemmed	O
from	O
the	O
theoretical	O
question	O
of	O
whether	O
an	O
efficient	O
weak	O
learner	O
can	O
be	O
boosted	O
into	O
an	O
efficient	O
strong	O
learner	O
valiant	O
and	O
solved	O
by	O
schapire	O
the	O
adaboost	B
algorithm	O
has	O
been	O
proposed	O
in	O
freund	O
schapire	O
boosting	B
can	O
be	O
viewed	O
from	O
many	O
perspectives	O
in	O
the	O
purely	O
theoretical	O
context	O
adaboost	B
can	O
be	O
interpreted	O
as	O
a	O
negative	O
result	O
if	O
strong	B
learning	I
of	O
a	O
hypothesis	B
class	I
is	O
computationally	O
hard	O
so	O
is	O
weak	B
learning	I
of	O
this	O
class	O
this	O
negative	O
result	O
can	O
be	O
useful	O
for	O
showing	O
hardness	O
of	O
agnostic	B
pac	B
learning	O
of	O
a	O
class	O
b	O
based	O
on	O
hardness	O
of	O
pac	B
learning	O
of	O
some	O
other	O
class	O
h	O
as	O
long	O
as	O
boosting	B
h	O
is	O
weakly	O
learnable	O
using	O
b	O
for	O
example	O
klivans	O
sherstov	O
have	O
shown	O
that	O
pac	B
learning	O
of	O
the	O
class	O
of	O
intersection	O
of	O
halfspaces	O
is	O
hard	O
in	O
the	O
realizable	O
case	O
this	O
hardness	O
result	O
can	O
be	O
used	O
to	O
show	O
that	O
agnostic	B
pac	B
learning	O
of	O
a	O
single	O
halfspace	B
is	O
also	O
computationally	O
hard	O
shamir	O
sridharan	O
the	O
idea	O
is	O
to	O
show	O
that	O
an	O
agnostic	B
pac	B
learner	O
for	O
a	O
single	O
halfspace	B
can	O
yield	O
a	O
weak	O
learner	O
for	O
the	O
class	O
of	O
intersection	O
of	O
halfspaces	O
and	O
since	O
such	O
a	O
weak	O
learner	O
can	O
be	O
boosted	O
we	O
will	O
obtain	O
a	O
strong	O
learner	O
for	O
the	O
class	O
of	O
intersection	O
of	O
halfspaces	O
adaboost	B
also	O
shows	O
an	O
equivalence	O
between	O
the	O
existence	O
of	O
a	O
weak	O
learner	O
and	O
separability	O
of	O
the	O
data	O
using	O
a	O
linear	O
classifier	B
over	O
the	O
predictions	O
of	O
base	O
hypotheses	O
this	O
result	O
is	O
closely	O
related	O
to	O
von	O
neumann	O
s	O
minimax	O
theorem	O
neumann	O
a	O
fundamental	O
result	O
in	O
game	O
theory	O
adaboost	B
is	O
also	O
related	O
to	O
the	O
concept	O
of	O
margin	B
which	O
we	O
will	O
study	O
later	O
on	O
in	O
chapter	O
it	O
can	O
also	O
be	O
viewed	O
as	O
a	O
forward	B
greedy	I
selection	I
algorithm	O
a	O
topic	O
that	O
will	O
be	O
presented	O
in	O
chapter	O
a	O
recent	O
book	O
by	O
schapire	O
freund	O
covers	O
boosting	B
from	O
all	O
points	O
of	O
view	O
and	O
gives	O
easy	O
access	O
to	O
the	O
wealth	O
of	O
research	O
that	O
this	O
field	O
has	O
produced	O
exercises	O
boosting	B
the	I
confidence	B
let	O
a	O
be	O
an	O
algorithm	O
that	O
guarantees	O
the	O
following	O
there	O
exist	O
some	O
constant	O
and	O
a	O
function	B
mh	O
n	O
such	O
that	O
for	O
every	O
if	O
m	O
mh	O
then	O
for	O
every	O
distribution	O
d	O
it	O
holds	O
that	O
with	O
probability	O
of	O
at	O
least	O
ldas	O
minh	O
h	O
ldh	O
suggest	O
a	O
procedure	O
that	O
relies	O
on	O
a	O
and	O
learns	O
h	O
in	O
the	O
usual	O
agnostic	B
pac	B
learning	O
model	O
and	O
has	O
a	O
sample	B
complexity	I
of	O
mh	O
k	O
mh	O
where	O
k	O
log	O
hint	O
divide	O
the	O
data	O
into	O
k	O
chunks	O
where	O
each	O
of	O
the	O
first	O
k	O
chunks	O
is	O
of	O
size	O
mh	O
examples	O
train	O
the	O
first	O
k	O
chunks	O
using	O
a	O
argue	O
that	O
the	O
probability	O
that	O
for	O
all	O
of	O
these	O
chunks	O
we	O
have	O
ldas	O
minh	O
h	O
ldh	O
finally	O
use	O
the	O
last	O
chunk	O
to	O
choose	O
from	O
the	O
k	O
hypotheses	O
is	O
at	O
most	O
k	O
that	O
a	O
generated	O
from	O
the	O
k	O
chunks	O
relying	O
on	O
corollary	O
prove	O
that	O
the	O
function	B
h	O
given	O
in	O
equation	O
equals	O
the	O
piece-wise	O
con	O
stant	O
function	B
defined	O
according	O
to	O
the	O
same	O
thresholds	O
as	O
h	O
we	O
have	O
informally	O
argued	O
that	O
the	O
adaboost	B
algorithm	O
uses	O
the	O
weighting	O
mechanism	O
to	O
force	O
the	O
weak	O
learner	O
to	O
focus	O
on	O
the	O
problematic	O
examples	O
in	O
the	O
next	O
iteration	O
in	O
this	O
question	O
we	O
will	O
find	O
some	O
rigorous	O
justification	O
for	O
this	O
argument	O
exercises	O
show	O
that	O
the	O
error	O
of	O
ht	O
w	O
r	O
t	O
the	O
distribution	O
is	O
exactly	O
that	O
is	O
show	O
that	O
for	O
every	O
t	O
i	O
in	O
this	O
exercise	O
we	O
discuss	O
the	O
vc-dimension	O
of	O
classes	O
of	O
the	O
form	O
lb	O
t	O
we	O
proved	O
an	O
upper	O
bound	O
of	O
odt	O
logdt	O
where	O
d	O
vcdimb	O
here	O
we	O
wish	O
to	O
prove	O
an	O
almost	O
matching	O
lower	O
bound	O
however	O
that	O
will	O
not	O
be	O
the	O
case	O
for	O
all	O
classes	O
b	O
note	O
that	O
for	O
every	O
class	O
b	O
and	O
every	O
number	O
t	O
vcdimb	O
vcdimlb	O
t	O
find	O
a	O
class	O
b	O
for	O
which	O
vcdimb	O
vcdimlb	O
t	O
for	O
every	O
t	O
hint	O
take	O
x	O
to	O
be	O
a	O
finite	O
set	B
vcdimbd	O
logd	O
hints	O
for	O
the	O
upper	O
bound	O
rely	O
on	O
exercise	O
for	O
the	O
lower	O
bound	O
assume	O
d	O
let	O
a	O
be	O
a	O
k	O
d	O
matrix	O
whose	O
columns	O
are	O
all	O
the	O
d	O
binary	O
vectors	O
in	O
the	O
rows	O
of	O
a	O
form	O
a	O
set	B
of	O
k	O
vectors	O
in	O
rd	O
show	O
that	O
this	O
set	B
is	O
shattered	O
by	O
decision	B
stumps	I
over	O
rd	O
let	O
bd	O
be	O
the	O
class	O
of	O
decision	B
stumps	I
over	O
rd	O
prove	O
that	O
logd	O
let	O
t	O
be	O
any	O
integer	O
prove	O
that	O
vcdimlbd	O
t	O
t	O
logd	O
hint	O
construct	O
a	O
set	B
of	O
t	O
from	O
the	O
previous	O
question	O
and	O
the	O
rows	O
of	O
the	O
matrices	O
t	O
show	O
that	O
the	O
resulting	O
set	B
is	O
shattered	O
by	O
lbd	O
t	O
k	O
instances	O
by	O
taking	O
the	O
rows	O
of	O
the	O
matrix	O
a	O
a	O
image	O
of	O
a	O
denoted	O
by	O
ia	O
is	O
the	O
matrix	O
b	O
such	O
that	O
bij	O
efficiently	O
calculating	O
the	O
viola	O
and	O
jones	O
features	O
using	O
an	O
integral	B
image	I
let	O
a	O
be	O
a	O
matrix	O
representing	O
an	O
image	O
the	O
integral	O
j	O
aij	O
show	O
that	O
ia	O
can	O
be	O
calculated	O
from	O
a	O
in	O
time	O
linear	O
in	O
the	O
size	O
of	O
a	O
show	O
how	O
every	O
viola	O
and	O
jones	O
feature	B
can	O
be	O
calculated	O
from	O
ia	O
in	O
a	O
constant	O
amount	O
of	O
time	O
is	O
the	O
runtime	O
does	O
not	O
depend	O
on	O
the	O
size	O
of	O
the	O
rectangle	O
defining	O
the	O
feature	B
model	B
selection	I
and	O
validation	B
in	O
the	O
previous	O
chapter	O
we	O
have	O
described	O
the	O
adaboost	B
algorithm	O
and	O
have	O
shown	O
how	O
the	O
parameter	O
t	O
of	O
adaboost	B
controls	O
the	O
bias-complexity	B
tradeoff	I
but	O
how	O
do	O
we	O
set	B
t	O
in	O
practice	O
more	O
generally	O
when	O
approaching	O
some	O
practical	O
problem	O
we	O
usually	O
can	O
think	O
of	O
several	O
algorithms	O
that	O
may	O
yield	O
a	O
good	O
solution	O
each	O
of	O
which	O
might	O
have	O
several	O
parameters	O
how	O
can	O
we	O
choose	O
the	O
best	O
algorithm	O
for	O
the	O
particular	O
problem	O
at	O
hand	O
and	O
how	O
do	O
we	O
set	B
the	O
algorithm	O
s	O
parameters	O
this	O
task	O
is	O
often	O
called	O
model	B
selection	I
to	O
illustrate	O
the	O
model	B
selection	I
task	O
consider	O
the	O
problem	O
of	O
learning	O
a	O
one	O
dimensional	O
regression	B
function	B
h	O
r	O
r	O
suppose	O
that	O
we	O
obtain	O
a	O
training	B
set	B
as	O
depicted	O
in	O
the	O
figure	O
we	O
can	O
consider	O
fitting	O
a	O
polynomial	O
to	O
the	O
data	O
as	O
described	O
in	O
chapter	O
however	O
we	O
might	O
be	O
uncertain	O
regarding	O
which	O
degree	O
d	O
would	O
give	O
the	O
best	O
results	O
for	O
our	O
data	O
set	B
a	O
small	O
degree	O
may	O
not	O
fit	O
the	O
data	O
well	O
it	O
will	O
have	O
a	O
large	O
approximation	B
error	I
whereas	O
a	O
high	O
degree	O
may	O
lead	O
to	O
overfitting	B
it	O
will	O
have	O
a	O
large	O
estimation	B
error	I
in	O
the	O
following	O
we	O
depict	O
the	O
result	O
of	O
fitting	O
a	O
polynomial	O
of	O
degrees	O
and	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
empirical	B
risk	B
decreases	O
as	O
we	O
enlarge	O
the	O
degree	O
however	O
looking	O
at	O
the	O
graphs	O
our	O
intuition	O
tells	O
us	O
that	O
setting	O
the	O
degree	O
to	O
may	O
be	O
better	O
than	O
setting	O
it	O
to	O
it	O
follows	O
that	O
the	O
empirical	B
risk	B
alone	O
is	O
not	O
enough	O
for	O
model	B
selection	I
degree	O
degree	O
degree	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
model	B
selection	I
using	O
srm	B
in	O
this	O
chapter	O
we	O
will	O
present	O
two	O
approaches	O
for	O
model	B
selection	I
the	O
first	O
approach	O
is	O
based	O
on	O
the	O
structural	O
risk	B
minimization	O
paradigm	O
we	O
have	O
described	O
and	O
analyzed	O
in	O
chapter	O
srm	B
is	O
particularly	O
useful	O
when	O
a	O
learning	O
algorithm	O
depends	O
on	O
a	O
parameter	O
that	O
controls	O
the	O
bias-complexity	B
tradeoff	I
as	O
the	O
degree	O
of	O
the	O
fitted	O
polynomial	O
in	O
the	O
preceding	O
example	O
or	O
the	O
parameter	O
t	O
in	O
adaboost	B
the	O
second	O
approach	O
relies	O
on	O
the	O
concept	O
of	O
validation	B
the	O
basic	O
idea	O
is	O
to	O
partition	O
the	O
training	B
set	B
into	O
two	O
sets	O
one	O
is	O
used	O
for	O
training	O
each	O
of	O
the	O
candidate	O
models	O
and	O
the	O
second	O
is	O
used	O
for	O
deciding	O
which	O
of	O
them	O
yields	O
the	O
best	O
results	O
in	O
model	B
selection	I
tasks	O
we	O
try	O
to	O
find	O
the	O
right	O
balance	O
between	O
approximation	O
and	O
estimation	O
errors	O
more	O
generally	O
if	O
our	O
learning	O
algorithm	O
fails	O
to	O
find	O
a	O
predictor	B
with	O
a	O
small	O
risk	B
it	O
is	O
important	O
to	O
understand	O
whether	O
we	O
suffer	O
from	O
overfitting	B
or	O
underfitting	B
in	O
section	O
we	O
discuss	O
how	O
this	O
can	O
be	O
achieved	O
model	B
selection	I
using	O
srm	B
the	O
srm	B
paradigm	O
has	O
been	O
described	O
and	O
analyzed	O
in	O
section	O
here	O
we	O
show	O
how	O
srm	B
can	O
be	O
used	O
for	O
tuning	O
the	O
tradeoff	O
between	O
bias	B
and	O
complexity	O
without	O
deciding	O
on	O
a	O
specific	O
hypothesis	B
class	I
in	O
advance	O
consider	O
a	O
countable	O
sequence	O
of	O
hypothesis	B
classes	O
for	O
example	O
in	O
the	O
problem	O
of	O
polynomial	B
regression	B
mentioned	O
we	O
can	O
take	O
hd	O
to	O
be	O
the	O
set	B
of	O
polynomials	O
of	O
degree	O
at	O
most	O
d	O
another	O
example	O
is	O
taking	O
hd	O
to	O
be	O
the	O
class	O
lb	O
d	O
used	O
by	O
adaboost	B
as	O
described	O
in	O
the	O
previous	O
chapter	O
we	O
assume	O
that	O
for	O
every	O
d	O
the	O
class	O
hd	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
definition	O
in	O
chapter	O
with	O
a	O
sample	B
complexity	I
function	B
of	O
the	O
form	O
gd	O
muchd	O
where	O
g	O
n	O
r	O
is	O
some	O
monotonically	O
increasing	O
function	B
for	O
example	O
in	O
the	O
case	O
of	O
binary	O
classification	O
problems	O
we	O
can	O
take	O
gd	O
to	O
be	O
the	O
vc-dimension	O
of	O
the	O
class	O
hd	O
multiplied	O
by	O
a	O
universal	O
constant	O
one	O
appearing	O
in	O
the	O
fundamental	O
theorem	O
of	O
learning	O
see	O
theorem	O
for	O
the	O
classes	O
lb	O
d	O
used	O
by	O
adaboost	B
the	O
function	B
g	O
will	O
simply	O
grow	O
with	O
d	O
recall	B
that	O
the	O
srm	B
rule	O
follows	O
a	O
bound	O
minimization	O
approach	O
where	O
in	O
our	O
case	O
the	O
bound	O
is	O
as	O
follows	O
with	O
probability	O
of	O
at	O
least	O
for	O
every	O
d	O
n	O
and	O
h	O
hd	O
ldh	O
lsh	O
logd	O
log	O
m	O
this	O
bound	O
which	O
follows	O
directly	O
from	O
theorem	O
shows	O
that	O
for	O
every	O
d	O
and	O
every	O
h	O
hd	O
the	O
true	O
risk	B
is	O
bounded	O
by	O
two	O
terms	O
the	O
empirical	B
risk	B
lsh	O
model	B
selection	I
and	O
validation	B
and	O
a	O
complexity	O
term	O
that	O
depends	O
on	O
d	O
the	O
srm	B
rule	O
will	O
search	O
for	O
d	O
and	O
h	O
hd	O
that	O
minimize	O
the	O
right-hand	O
side	O
of	O
equation	O
getting	O
back	O
to	O
the	O
example	O
of	O
polynomial	B
regression	B
described	O
earlier	O
even	O
though	O
the	O
empirical	B
risk	B
of	O
the	O
degree	O
polynomial	O
is	O
smaller	O
than	O
that	O
of	O
the	O
degree	O
polynomial	O
we	O
would	O
still	O
prefer	O
the	O
degree	O
polynomial	O
since	O
its	O
complexity	O
reflected	O
by	O
the	O
value	O
of	O
the	O
function	B
gd	O
is	O
much	O
smaller	O
while	O
the	O
srm	B
approach	O
can	O
be	O
useful	O
in	O
some	O
situations	O
in	O
many	O
practical	O
cases	O
the	O
upper	O
bound	O
given	O
in	O
equation	O
is	O
pessimistic	O
in	O
the	O
next	O
section	O
we	O
present	O
a	O
more	O
practical	O
approach	O
validation	B
we	O
would	O
often	O
like	O
to	O
get	O
a	O
better	O
estimation	O
of	O
the	O
true	O
risk	B
of	O
the	O
output	O
predictor	B
of	O
a	O
learning	O
algorithm	O
so	O
far	O
we	O
have	O
derived	O
bounds	O
on	O
the	O
estimation	B
error	I
of	O
a	O
hypothesis	B
class	I
which	O
tell	O
us	O
that	O
for	O
all	O
hypotheses	O
in	O
the	O
class	O
the	O
true	O
risk	B
is	O
not	O
very	O
far	O
from	O
the	O
empirical	B
risk	B
however	O
these	O
bounds	O
might	O
be	O
loose	O
and	O
pessimistic	O
as	O
they	O
hold	O
for	O
all	O
hypotheses	O
and	O
all	O
possible	O
data	O
distributions	O
a	O
more	O
accurate	O
estimation	O
of	O
the	O
true	O
risk	B
can	O
be	O
obtained	O
by	O
using	O
some	O
of	O
the	O
training	O
data	O
as	O
a	O
validation	B
set	B
over	O
which	O
one	O
can	O
evalutate	O
the	O
success	O
of	O
the	O
algorithm	O
s	O
output	O
predictor	B
this	O
procedure	O
is	O
called	O
validation	B
naturally	O
a	O
better	O
estimation	O
of	O
the	O
true	O
risk	B
is	O
useful	O
for	O
model	B
selection	I
as	O
we	O
will	O
describe	O
in	O
section	O
hold	B
out	I
set	B
the	O
simplest	O
way	O
to	O
estimate	O
the	O
true	B
error	I
of	O
a	O
predictor	B
h	O
is	O
by	O
sampling	O
an	O
additional	O
set	B
of	O
examples	O
independent	O
of	O
the	O
training	B
set	B
and	O
using	O
the	O
empirical	B
error	I
on	O
this	O
validation	B
set	B
as	O
our	O
estimator	O
formally	O
let	O
v	O
ymv	O
be	O
a	O
set	B
of	O
fresh	O
mv	O
examples	O
that	O
are	O
sampled	O
according	O
to	O
d	O
of	O
the	O
m	O
examples	O
of	O
the	O
training	B
set	B
s	O
using	O
hoeffding	O
s	O
inequality	O
lemma	O
we	O
have	O
the	O
following	O
theorem	O
let	O
h	O
be	O
some	O
predictor	B
and	O
assume	O
that	O
the	O
loss	B
function	B
is	O
in	O
then	O
for	O
every	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
a	O
validation	B
set	B
v	O
of	O
size	O
mv	O
we	O
have	O
ldh	O
mv	O
the	O
bound	O
in	O
theorem	O
does	O
not	O
depend	O
on	O
the	O
algorithm	O
or	O
the	O
training	B
set	B
used	O
to	O
construct	O
h	O
and	O
is	O
tighter	O
than	O
the	O
usual	O
bounds	O
that	O
we	O
have	O
seen	O
so	O
far	O
the	O
reason	O
for	O
the	O
tightness	O
of	O
this	O
bound	O
is	O
that	O
it	O
is	O
in	O
terms	O
of	O
an	O
estimate	O
on	O
a	O
fresh	O
validation	B
set	B
that	O
is	O
independent	O
of	O
the	O
way	O
h	O
was	O
generated	O
to	O
illustrate	O
this	O
point	O
suppose	O
that	O
h	O
was	O
obtained	O
by	O
applying	O
an	O
erm	B
predictor	B
validation	B
with	O
respect	O
to	O
a	O
hypothesis	B
class	I
of	O
vc-dimension	O
d	O
over	O
a	O
training	B
set	B
of	O
m	O
examples	O
then	O
from	O
the	O
fundamental	O
theorem	O
of	O
learning	O
we	O
obtain	O
the	O
bound	O
ldh	O
lsh	O
c	O
d	O
m	O
where	O
c	O
is	O
the	O
constant	O
appearing	O
in	O
theorem	O
in	O
contrast	O
from	O
theorem	O
we	O
obtain	O
the	O
bound	O
ldh	O
lv	O
therefore	O
taking	O
mv	O
to	O
be	O
order	O
of	O
m	O
we	O
obtain	O
an	O
estimate	O
that	O
is	O
more	O
accurate	O
by	O
a	O
factor	O
that	O
depends	O
on	O
the	O
vc-dimension	O
on	O
the	O
other	O
hand	O
the	O
price	O
we	O
pay	O
for	O
using	O
such	O
an	O
estimate	O
is	O
that	O
it	O
requires	O
an	O
additional	O
sample	O
on	O
top	O
of	O
the	O
sample	O
used	O
for	O
training	O
the	O
learner	O
sampling	O
a	O
training	B
set	B
and	O
then	O
sampling	O
an	O
independent	O
validation	B
set	B
is	O
equivalent	O
to	O
randomly	O
partitioning	O
our	O
random	O
set	B
of	O
examples	O
into	O
two	O
parts	O
using	O
one	O
part	O
for	O
training	O
and	O
the	O
other	O
one	O
for	O
validation	B
for	O
this	O
reason	O
the	O
validation	B
set	B
is	O
often	O
referred	O
to	O
as	O
a	O
hold	B
out	I
set	B
validation	B
for	O
model	B
selection	I
validation	B
can	O
be	O
naturally	O
used	O
for	O
model	B
selection	I
as	O
follows	O
we	O
first	O
train	O
different	O
algorithms	O
the	O
same	O
algorithm	O
with	O
different	O
parameters	O
on	O
the	O
given	O
training	B
set	B
let	O
h	O
hr	O
be	O
the	O
set	B
of	O
all	O
output	O
predictors	O
of	O
the	O
different	O
algorithms	O
for	O
example	O
in	O
the	O
case	O
of	O
training	O
polynomial	O
regressors	O
we	O
would	O
have	O
each	O
hr	O
be	O
the	O
output	O
of	O
polynomial	B
regression	B
of	O
degree	O
r	O
now	O
to	O
choose	O
a	O
single	O
predictor	B
from	O
h	O
we	O
sample	O
a	O
fresh	O
validation	B
set	B
and	O
choose	O
the	O
predictor	B
that	O
minimizes	O
the	O
error	O
over	O
the	O
validation	B
set	B
in	O
other	O
words	O
we	O
apply	O
ermh	O
over	O
the	O
validation	B
set	B
this	O
process	O
is	O
very	O
similar	O
to	O
learning	O
a	O
finite	O
hypothesis	B
class	I
the	O
only	O
difference	O
is	O
that	O
h	O
is	O
not	O
fixed	O
ahead	O
of	O
time	O
but	O
rather	O
depends	O
on	O
the	O
training	B
set	B
however	O
since	O
the	O
validation	B
set	B
is	O
independent	O
of	O
the	O
training	B
set	B
we	O
get	O
that	O
it	O
is	O
also	O
independent	O
of	O
h	O
and	O
therefore	O
the	O
same	O
technique	O
we	O
used	O
to	O
derive	O
bounds	O
for	O
finite	O
hypothesis	B
classes	O
holds	O
here	O
as	O
well	O
in	O
particular	O
combining	O
theorem	O
with	O
the	O
union	B
bound	I
we	O
obtain	O
theorem	O
let	O
h	O
hr	O
be	O
an	O
arbitrary	O
set	B
of	O
predictors	O
and	O
assume	O
that	O
the	O
loss	B
function	B
is	O
in	O
assume	O
that	O
a	O
validation	B
set	B
v	O
of	O
size	O
mv	O
is	O
sampled	O
independent	O
of	O
h	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
v	O
we	O
have	O
h	O
h	O
lv	O
mv	O
model	B
selection	I
and	O
validation	B
this	O
theorem	O
tells	O
us	O
that	O
the	O
error	O
on	O
the	O
validation	B
set	B
approximates	O
the	O
true	B
error	I
as	O
long	O
as	O
h	O
is	O
not	O
too	O
large	O
however	O
if	O
we	O
try	O
too	O
many	O
methods	O
in	O
that	O
is	O
large	O
relative	O
to	O
the	O
size	O
of	O
the	O
validation	B
set	B
then	O
we	O
re	O
in	O
danger	O
of	O
overfitting	B
to	O
illustrate	O
how	O
validation	B
is	O
useful	O
for	O
model	B
selection	I
consider	O
again	O
the	O
example	O
of	O
fitting	O
a	O
one	O
dimensional	O
polynomial	O
as	O
described	O
in	O
the	O
beginning	O
of	O
this	O
chapter	O
in	O
the	O
following	O
we	O
depict	O
the	O
same	O
training	B
set	B
with	O
erm	B
polynomials	O
of	O
degree	O
and	O
but	O
this	O
time	O
we	O
also	O
depict	O
an	O
additional	O
validation	B
set	B
as	O
red	O
unfilled	O
circles	O
the	O
polynomial	O
of	O
degree	O
has	O
minimal	O
training	B
error	I
yet	O
the	O
polynomial	O
of	O
degree	O
has	O
the	O
minimal	O
validation	B
error	O
and	O
hence	O
it	O
will	O
be	O
chosen	O
as	O
the	O
best	O
model	O
the	O
model-selection	O
curve	O
the	O
model	B
selection	I
curve	O
shows	O
the	O
training	B
error	I
and	O
validation	B
error	O
as	O
a	O
function	B
of	O
the	O
complexity	O
of	O
the	O
model	O
considered	O
for	O
example	O
for	O
the	O
polynomial	O
fitting	O
problem	O
mentioned	O
previously	O
the	O
curve	O
will	O
look	O
like	O
validation	B
train	O
validation	B
r	O
o	O
r	O
r	O
e	O
d	O
as	O
can	O
be	O
shown	O
the	O
training	B
error	I
is	O
monotonically	O
decreasing	O
as	O
we	O
increase	O
the	O
polynomial	O
degree	O
is	O
the	O
complexity	O
of	O
the	O
model	O
in	O
our	O
case	O
on	O
the	O
other	O
hand	O
the	O
validation	B
error	O
first	O
decreases	O
but	O
then	O
starts	O
to	O
increase	O
which	O
indicates	O
that	O
we	O
are	O
starting	O
to	O
suffer	O
from	O
overfitting	B
plotting	O
such	O
curves	O
can	O
help	O
us	O
understand	O
whether	O
we	O
are	O
searching	O
the	O
correct	O
regime	O
of	O
our	O
parameter	O
space	O
often	O
there	O
may	O
be	O
more	O
than	O
a	O
single	O
parameter	O
to	O
tune	O
and	O
the	O
possible	O
number	O
of	O
values	O
each	O
parameter	O
can	O
take	O
might	O
be	O
quite	O
large	O
for	O
example	O
in	O
chapter	O
we	O
describe	O
the	O
concept	O
of	O
regularization	B
in	O
which	O
the	O
parameter	O
of	O
the	O
learning	O
algorithm	O
is	O
a	O
real	O
number	O
in	O
such	O
cases	O
we	O
start	O
with	O
a	O
rough	O
grid	O
of	O
values	O
for	O
the	O
parameters	O
and	O
plot	O
the	O
corresponding	O
model-selection	O
curve	O
on	O
the	O
basis	O
of	O
the	O
curve	O
we	O
will	O
zoom	O
in	O
to	O
the	O
correct	O
regime	O
and	O
employ	O
a	O
finer	O
grid	O
to	O
search	O
over	O
it	O
is	O
important	O
to	O
verify	O
that	O
we	O
are	O
in	O
the	O
relevant	O
regime	O
for	O
example	O
in	O
the	O
polynomial	O
fitting	O
problem	O
described	O
if	O
we	O
start	O
searching	O
degrees	O
from	O
the	O
set	B
of	O
values	O
and	O
do	O
not	O
employ	O
a	O
finer	O
grid	O
based	O
on	O
the	O
resulting	O
curve	O
we	O
will	O
end	O
up	O
with	O
a	O
rather	O
poor	O
model	O
k-fold	O
cross	B
validation	B
the	O
validation	B
procedure	O
described	O
so	O
far	O
assumes	O
that	O
data	O
is	O
plentiful	O
and	O
that	O
we	O
have	O
the	O
ability	O
to	O
sample	O
a	O
fresh	O
validation	B
set	B
but	O
in	O
some	O
applications	O
data	O
is	O
scarce	O
and	O
we	O
do	O
not	O
want	O
to	O
waste	O
data	O
on	O
validation	B
the	O
k-fold	O
cross	B
validation	B
technique	O
is	O
designed	O
to	O
give	O
an	O
accurate	O
estimate	O
of	O
the	O
true	B
error	I
without	O
wasting	O
too	O
much	O
data	O
in	O
k-fold	O
cross	B
validation	B
the	O
original	O
training	B
set	B
is	O
partitioned	O
into	O
k	O
subsets	O
of	O
size	O
mk	O
simplicity	O
assume	O
that	O
mk	O
is	O
an	O
integer	O
for	O
each	O
fold	O
the	O
algorithm	O
is	O
trained	O
on	O
the	O
union	O
of	O
the	O
other	O
folds	O
and	O
then	O
the	O
error	O
of	O
its	O
output	O
is	O
estimated	O
using	O
the	O
fold	O
finally	O
the	O
average	O
of	O
all	O
these	O
errors	O
is	O
the	O
model	B
selection	I
and	O
validation	B
estimate	O
of	O
the	O
true	B
error	I
the	O
special	O
case	O
k	O
m	O
where	O
m	O
is	O
the	O
number	O
of	O
examples	O
is	O
called	O
leave-one-out	O
k-fold	O
cross	B
validation	B
is	O
often	O
used	O
for	O
model	B
selection	I
parameter	O
tuning	O
and	O
once	O
the	O
best	O
parameter	O
is	O
chosen	O
the	O
algorithm	O
is	O
retrained	O
using	O
this	O
parameter	O
on	O
the	O
entire	O
training	B
set	B
a	O
pseudocode	O
of	O
k-fold	O
cross	B
validation	B
for	O
model	B
selection	I
is	O
given	O
in	O
the	O
following	O
the	O
procedure	O
receives	O
as	O
input	O
a	O
training	B
set	B
s	O
a	O
set	B
of	O
possible	O
parameter	O
values	O
an	O
integer	O
k	O
representing	O
the	O
number	O
of	O
folds	O
and	O
a	O
learning	O
algorithm	O
a	O
which	O
receives	O
as	O
input	O
a	O
training	B
set	B
as	O
well	O
as	O
a	O
parameter	O
it	O
outputs	O
the	O
best	O
parameter	O
as	O
well	O
as	O
the	O
hypothesis	B
trained	O
by	O
this	O
parameter	O
on	O
the	O
entire	O
training	B
set	B
k-fold	O
cross	B
validation	B
for	O
model	B
selection	I
input	O
training	B
set	B
s	O
ym	O
set	B
of	O
parameter	O
values	O
learning	O
algorithm	O
a	O
integer	O
k	O
partition	O
s	O
into	O
sk	O
foreach	O
error	O
k	O
output	O
lsihi	O
for	O
i	O
k	O
hi	O
as	O
si	O
argmin	O
h	O
as	O
the	O
cross	B
validation	B
method	O
often	O
works	O
very	O
well	O
in	O
practice	O
however	O
it	O
might	O
sometime	O
fail	O
as	O
the	O
artificial	O
example	O
given	O
in	O
exercise	O
shows	O
rigorously	O
understanding	O
the	O
exact	O
behavior	O
of	O
cross	B
validation	B
is	O
still	O
an	O
open	O
problem	O
rogers	O
and	O
wagner	O
wagner	O
have	O
shown	O
that	O
for	O
k	O
local	O
rules	O
k	O
nearest	B
neighbor	I
see	O
chapter	O
the	O
cross	B
validation	B
procedure	O
gives	O
a	O
very	O
good	O
estimate	O
of	O
the	O
true	B
error	I
other	O
papers	O
show	O
that	O
cross	B
validation	B
works	O
for	O
stable	O
algorithms	O
will	O
study	O
stability	B
and	O
its	O
relation	O
to	O
learnability	O
in	O
chapter	O
train-validation-test	B
split	I
in	O
most	O
practical	O
applications	O
we	O
split	O
the	O
available	O
examples	O
into	O
three	O
sets	O
the	O
first	O
set	B
is	O
used	O
for	O
training	O
our	O
algorithm	O
and	O
the	O
second	O
is	O
used	O
as	O
a	O
validation	B
set	B
for	O
model	B
selection	I
after	O
we	O
select	O
the	O
best	O
model	O
we	O
test	O
the	O
performance	O
of	O
the	O
output	O
predictor	B
on	O
the	O
third	O
set	B
which	O
is	O
often	O
called	O
the	O
test	O
set	B
the	O
number	O
obtained	O
is	O
used	O
as	O
an	O
estimator	O
of	O
the	O
true	B
error	I
of	O
the	O
learned	O
predictor	B
what	O
to	O
do	O
if	O
learning	O
fails	O
what	O
to	O
do	O
if	O
learning	O
fails	O
consider	O
the	O
following	O
scenario	O
you	O
were	O
given	O
a	O
learning	O
task	O
and	O
have	O
approached	O
it	O
with	O
a	O
choice	O
of	O
a	O
hypothesis	B
class	I
a	O
learning	O
algorithm	O
and	O
parameters	O
you	O
used	O
a	O
validation	B
set	B
to	O
tune	O
the	O
parameters	O
and	O
tested	O
the	O
learned	O
predictor	B
on	O
a	O
test	O
set	B
the	O
test	O
results	O
unfortunately	O
turn	O
out	O
to	O
be	O
unsatisfactory	O
what	O
went	O
wrong	O
then	O
and	O
what	O
should	O
you	O
do	O
next	O
there	O
are	O
many	O
elements	O
that	O
can	O
be	O
fixed	O
the	O
main	O
approaches	O
are	O
listed	O
in	O
the	O
following	O
get	O
a	O
larger	O
sample	O
change	O
the	O
hypothesis	B
class	I
by	O
enlarging	O
it	O
reducing	O
it	O
completely	O
changing	O
it	O
changing	O
the	O
parameters	O
you	O
consider	O
change	O
the	O
feature	B
representation	O
of	O
the	O
data	O
change	O
the	O
optimization	O
algorithm	O
used	O
to	O
apply	O
your	O
learning	O
rule	O
in	O
order	O
to	O
find	O
the	O
best	O
remedy	O
it	O
is	O
essential	O
first	O
to	O
understand	O
the	O
cause	O
of	O
the	O
bad	O
performance	O
recall	B
that	O
in	O
chapter	O
we	O
decomposed	O
the	O
true	B
error	I
of	O
the	O
learned	O
predictor	B
into	O
approximation	B
error	I
and	O
estimation	B
error	I
the	O
approximation	B
error	I
is	O
defined	O
to	O
be	O
for	O
some	O
argminh	O
h	O
ldh	O
while	O
the	O
estimation	B
error	I
is	O
defined	O
to	O
be	O
ldhs	O
where	O
hs	O
is	O
the	O
learned	O
predictor	B
is	O
based	O
on	O
the	O
training	B
set	B
s	O
the	O
approximation	B
error	I
of	O
the	O
class	O
does	O
not	O
depend	O
on	O
the	O
sample	O
size	O
or	O
on	O
the	O
algorithm	O
being	O
used	O
it	O
only	O
depends	O
on	O
the	O
distribution	O
d	O
and	O
on	O
the	O
hypothesis	B
class	I
h	O
therefore	O
if	O
the	O
approximation	B
error	I
is	O
large	O
it	O
will	O
not	O
help	O
us	O
to	O
enlarge	O
the	O
training	B
set	B
size	O
and	O
it	O
also	O
does	O
not	O
make	O
sense	O
to	O
reduce	O
the	O
hypothesis	B
class	I
what	O
can	O
be	O
beneficial	O
in	O
this	O
case	O
is	O
to	O
enlarge	O
the	O
hypothesis	B
class	I
or	O
completely	O
change	O
it	O
we	O
have	O
some	O
alternative	O
prior	B
knowledge	I
in	O
the	O
form	O
of	O
a	O
different	O
hypothesis	B
class	I
we	O
can	O
also	O
consider	O
applying	O
the	O
same	O
hypothesis	B
class	I
but	O
on	O
a	O
different	O
feature	B
representation	O
of	O
the	O
data	O
chapter	O
the	O
estimation	B
error	I
of	O
the	O
class	O
does	O
depend	O
on	O
the	O
sample	O
size	O
therefore	O
if	O
we	O
have	O
a	O
large	O
estimation	B
error	I
we	O
can	O
make	O
an	O
effort	O
to	O
obtain	O
more	O
training	O
examples	O
we	O
can	O
also	O
consider	O
reducing	O
the	O
hypothesis	B
class	I
however	O
it	O
doesn	O
t	O
make	O
sense	O
to	O
enlarge	O
the	O
hypothesis	B
class	I
in	O
that	O
case	O
error	B
decomposition	I
using	O
validation	B
we	O
see	O
that	O
understanding	O
whether	O
our	O
problem	O
is	O
due	O
to	O
approximation	B
error	I
or	O
estimation	B
error	I
is	O
very	O
useful	O
for	O
finding	O
the	O
best	O
remedy	O
in	O
the	O
previous	O
section	O
we	O
saw	O
how	O
to	O
estimate	O
ldhs	O
using	O
the	O
empirical	B
risk	B
on	O
a	O
validation	B
set	B
however	O
it	O
is	O
more	O
difficult	O
to	O
estimate	O
the	O
approximation	B
error	I
of	O
the	O
class	O
model	B
selection	I
and	O
validation	B
instead	O
we	O
give	O
a	O
different	O
error	B
decomposition	I
one	O
that	O
can	O
be	O
estimated	O
from	O
the	O
train	O
and	O
validation	B
sets	O
ldhs	O
lv	O
lshs	O
lshs	O
the	O
first	O
term	O
lv	O
can	O
be	O
bounded	O
quite	O
tightly	O
using	O
theorem	O
intuitively	O
when	O
the	O
second	O
term	O
lshs	O
is	O
large	O
we	O
say	O
that	O
our	O
algorithm	O
suffers	O
from	O
overfitting	B
while	O
when	O
the	O
empirical	B
risk	B
term	O
lshs	O
is	O
large	O
we	O
say	O
that	O
our	O
algorithm	O
suffers	O
from	O
underfitting	B
note	O
that	O
these	O
two	O
terms	O
are	O
not	O
necessarily	O
good	O
estimates	O
of	O
the	O
estimation	O
and	O
approximation	O
errors	O
to	O
illustrate	O
this	O
consider	O
the	O
case	O
in	O
which	O
h	O
is	O
a	O
class	O
of	O
vc-dimension	O
d	O
and	O
d	O
is	O
a	O
distribution	O
such	O
that	O
the	O
approximation	B
error	I
of	O
h	O
with	O
respect	O
to	O
d	O
is	O
as	O
long	O
as	O
the	O
size	O
of	O
our	O
training	B
set	B
is	O
smaller	O
than	O
d	O
we	O
will	O
have	O
lshs	O
for	O
every	O
erm	B
hypothesis	B
therefore	O
the	O
training	O
risk	B
lshs	O
and	O
the	O
approximation	B
error	I
can	O
be	O
significantly	O
different	O
nevertheless	O
as	O
we	O
show	O
later	O
the	O
values	O
of	O
lshs	O
and	O
lshs	O
still	O
provide	O
us	O
useful	O
information	O
consider	O
first	O
the	O
case	O
in	O
which	O
lshs	O
is	O
large	O
we	O
can	O
write	O
lshs	O
when	O
hs	O
is	O
an	O
ermh	O
hypothesis	B
we	O
have	O
that	O
lshs	O
in	O
addition	O
since	O
does	O
not	O
depend	O
on	O
s	O
the	O
term	O
can	O
be	O
bounded	O
quite	O
tightly	O
in	O
theorem	O
the	O
last	O
term	O
is	O
the	O
approximation	B
error	I
it	O
follows	O
that	O
if	O
lshs	O
is	O
large	O
then	O
so	O
is	O
the	O
approximation	B
error	I
and	O
the	O
remedy	O
to	O
the	O
failure	O
of	O
our	O
algorithm	O
should	O
be	O
tailored	O
accordingly	O
discussed	O
previously	O
remark	O
it	O
is	O
possible	O
that	O
the	O
approximation	B
error	I
of	O
our	O
class	O
is	O
small	O
yet	O
the	O
value	O
of	O
lshs	O
is	O
large	O
for	O
example	O
maybe	O
we	O
had	O
a	O
bug	O
in	O
our	O
erm	B
implementation	O
and	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
hs	O
that	O
is	O
not	O
an	O
erm	B
it	O
may	O
also	O
be	O
the	O
case	O
that	O
finding	O
an	O
erm	B
hypothesis	B
is	O
computationally	O
hard	O
and	O
our	O
algorithm	O
applies	O
some	O
heuristic	O
trying	O
to	O
find	O
an	O
approximate	O
erm	B
in	O
some	O
cases	O
it	O
is	O
hard	O
to	O
know	O
how	O
good	O
hs	O
is	O
relative	O
to	O
an	O
erm	B
hypothesis	B
but	O
sometimes	O
it	O
is	O
possible	O
at	O
least	O
to	O
know	O
whether	O
there	O
are	O
better	O
hypotheses	O
for	O
example	O
in	O
the	O
next	O
chapter	O
we	O
will	O
study	O
convex	B
learning	O
problems	O
in	O
which	O
there	O
are	O
optimality	O
conditions	O
that	O
can	O
be	O
checked	O
to	O
verify	O
whether	O
our	O
optimization	O
algorithm	O
converged	O
to	O
an	O
erm	B
solution	O
in	O
other	O
cases	O
the	O
solution	O
may	O
depend	O
on	O
randomness	O
in	O
initializing	O
the	O
algorithm	O
so	O
we	O
can	O
try	O
different	O
randomly	O
selected	O
initial	O
points	O
to	O
see	O
whether	O
better	O
solutions	O
pop	O
out	O
next	O
consider	O
the	O
case	O
in	O
which	O
lshs	O
is	O
small	O
as	O
we	O
argued	O
before	O
this	O
does	O
not	O
necessarily	O
imply	O
that	O
the	O
approximation	B
error	I
is	O
small	O
indeed	O
consider	O
two	O
scenarios	O
in	O
both	O
of	O
which	O
we	O
are	O
trying	O
to	O
learn	O
a	O
hypothesis	B
class	I
of	O
vc-dimension	O
d	O
using	O
the	O
erm	B
learning	O
rule	O
in	O
the	O
first	O
scenario	O
we	O
have	O
a	O
training	B
set	B
of	O
m	O
d	O
examples	O
and	O
the	O
approximation	B
error	I
of	O
the	O
class	O
is	O
high	O
in	O
the	O
second	O
scenario	O
we	O
have	O
a	O
training	B
set	B
of	O
m	O
examples	O
and	O
the	O
what	O
to	O
do	O
if	O
learning	O
fails	O
error	O
error	O
validation	B
error	O
train	O
error	O
m	O
validationerror	O
train	O
error	O
m	O
figure	O
examples	O
of	O
learning	B
curves	I
left	O
this	O
learning	O
curve	O
corresponds	O
to	O
the	O
scenario	O
in	O
which	O
the	O
number	O
of	O
examples	O
is	O
always	O
smaller	O
than	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
right	O
this	O
learning	O
curve	O
corresponds	O
to	O
the	O
scenario	O
in	O
which	O
the	O
approximation	B
error	I
is	O
zero	O
and	O
the	O
number	O
of	O
examples	O
is	O
larger	O
than	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
approximation	B
error	I
of	O
the	O
class	O
is	O
zero	O
in	O
both	O
cases	O
lshs	O
how	O
can	O
we	O
distinguish	O
between	O
the	O
two	O
cases	O
learning	B
curves	I
one	O
possible	O
way	O
to	O
distinguish	O
between	O
the	O
two	O
cases	O
is	O
by	O
plotting	O
learning	B
curves	I
to	O
produce	O
a	O
learning	O
curve	O
we	O
train	O
the	O
algorithm	O
on	O
prefixes	O
of	O
the	O
data	O
of	O
increasing	O
sizes	O
for	O
example	O
we	O
can	O
first	O
train	O
the	O
algorithm	O
on	O
the	O
first	O
of	O
the	O
examples	O
then	O
on	O
of	O
them	O
and	O
so	O
on	O
for	O
each	O
prefix	O
we	O
calculate	O
the	O
training	B
error	I
the	O
prefix	O
the	O
algorithm	O
is	O
being	O
trained	O
on	O
and	O
the	O
validation	B
error	O
a	O
predefined	O
validation	B
set	B
such	O
learning	B
curves	I
can	O
help	O
us	O
distinguish	O
between	O
the	O
two	O
aforementioned	O
scenarios	O
in	O
the	O
first	O
scenario	O
we	O
expect	O
the	O
validation	B
error	O
to	O
be	O
approximately	O
for	O
all	O
prefixes	O
as	O
we	O
didn	O
t	O
really	O
learn	O
anything	O
in	O
the	O
second	O
scenario	O
the	O
validation	B
error	O
will	O
start	O
as	O
a	O
constant	O
but	O
then	O
should	O
start	O
decreasing	O
must	O
start	O
decreasing	O
once	O
the	O
training	B
set	B
size	O
is	O
larger	O
than	O
the	O
vc-dimension	O
an	O
illustration	O
of	O
the	O
two	O
cases	O
is	O
given	O
in	O
figure	O
in	O
general	O
as	O
long	O
as	O
the	O
approximation	B
error	I
is	O
greater	O
than	O
zero	O
we	O
expect	O
the	O
training	B
error	I
to	O
grow	O
with	O
the	O
sample	O
size	O
as	O
a	O
larger	O
amount	O
of	O
data	O
points	O
makes	O
it	O
harder	O
to	O
provide	O
an	O
explanation	O
for	O
all	O
of	O
them	O
on	O
the	O
other	O
hand	O
the	O
validation	B
error	O
tends	O
to	O
decrease	O
with	O
the	O
increase	O
in	O
sample	O
size	O
if	O
the	O
vc-dimension	O
is	O
finite	O
when	O
the	O
sample	O
size	O
goes	O
to	O
infinity	O
the	O
validation	B
and	O
train	O
errors	O
converge	O
to	O
the	O
approximation	B
error	I
therefore	O
by	O
extrapolating	O
the	O
training	O
and	O
validation	B
curves	O
we	O
can	O
try	O
to	O
guess	O
the	O
value	O
of	O
the	O
approximation	B
error	I
or	O
at	O
least	O
to	O
get	O
a	O
rough	O
estimate	O
on	O
an	O
interval	O
in	O
which	O
the	O
approximation	B
error	I
resides	O
getting	O
back	O
to	O
the	O
problem	O
of	O
finding	O
the	O
best	O
remedy	O
for	O
the	O
failure	O
of	O
our	O
algorithm	O
if	O
we	O
observe	O
that	O
lshs	O
is	O
small	O
while	O
the	O
validation	B
error	O
is	O
large	O
then	O
in	O
any	O
case	O
we	O
know	O
that	O
the	O
size	O
of	O
our	O
training	B
set	B
is	O
not	O
sufficient	O
for	O
learning	O
the	O
class	O
h	O
we	O
can	O
then	O
plot	O
a	O
learning	O
curve	O
if	O
we	O
see	O
that	O
the	O
model	B
selection	I
and	O
validation	B
validation	B
error	O
is	O
starting	O
to	O
decrease	O
then	O
the	O
best	O
solution	O
is	O
to	O
increase	O
the	O
number	O
of	O
examples	O
we	O
can	O
afford	O
to	O
enlarge	O
the	O
data	O
another	O
reasonable	O
solution	O
is	O
to	O
decrease	O
the	O
complexity	O
of	O
the	O
hypothesis	B
class	I
on	O
the	O
other	O
hand	O
if	O
we	O
see	O
that	O
the	O
validation	B
error	O
is	O
kept	O
around	O
then	O
we	O
have	O
no	O
evidence	O
that	O
the	O
approximation	B
error	I
of	O
h	O
is	O
good	O
it	O
may	O
be	O
the	O
case	O
that	O
increasing	O
the	O
training	B
set	B
size	O
will	O
not	O
help	O
us	O
at	O
all	O
obtaining	O
more	O
data	O
can	O
still	O
help	O
us	O
as	O
at	O
some	O
point	O
we	O
can	O
see	O
whether	O
the	O
validation	B
error	O
starts	O
to	O
decrease	O
or	O
whether	O
the	O
training	B
error	I
starts	O
to	O
increase	O
but	O
if	O
more	O
data	O
is	O
expensive	O
it	O
may	O
be	O
better	O
first	O
to	O
try	O
to	O
reduce	O
the	O
complexity	O
of	O
the	O
hypothesis	B
class	I
to	O
summarize	O
the	O
discussion	O
the	O
following	O
steps	O
should	O
be	O
applied	O
if	O
learning	O
involves	O
parameter	O
tuning	O
plot	O
the	O
model-selection	O
curve	O
to	O
make	O
sure	O
that	O
you	O
tuned	O
the	O
parameters	O
appropriately	O
section	O
if	O
the	O
training	B
error	I
is	O
excessively	O
large	O
consider	O
enlarging	O
the	O
hypothesis	B
class	I
completely	O
change	O
it	O
or	O
change	O
the	O
feature	B
representation	O
of	O
the	O
data	O
if	O
the	O
training	B
error	I
is	O
small	O
plot	O
learning	B
curves	I
and	O
try	O
to	O
deduce	O
from	O
them	O
whether	O
the	O
problem	O
is	O
estimation	B
error	I
or	O
approximation	B
error	I
if	O
the	O
approximation	B
error	I
seems	O
to	O
be	O
small	O
enough	O
try	O
to	O
obtain	O
more	O
data	O
if	O
this	O
is	O
not	O
possible	O
consider	O
reducing	O
the	O
complexity	O
of	O
the	O
hypothesis	B
class	I
if	O
the	O
approximation	B
error	I
seems	O
to	O
be	O
large	O
as	O
well	O
try	O
to	O
change	O
the	O
hy	O
pothesis	O
class	O
or	O
the	O
feature	B
representation	O
of	O
the	O
data	O
completely	O
summary	O
model	B
selection	I
is	O
the	O
task	O
of	O
selecting	O
an	O
appropriate	O
model	O
for	O
the	O
learning	O
task	O
based	O
on	O
the	O
data	O
itself	O
we	O
have	O
shown	O
how	O
this	O
can	O
be	O
done	O
using	O
the	O
srm	B
learning	O
paradigm	O
or	O
using	O
the	O
more	O
practical	O
approach	O
of	O
validation	B
if	O
our	O
learning	O
algorithm	O
fails	O
a	O
decomposition	O
of	O
the	O
algorithm	O
s	O
error	O
should	O
be	O
performed	O
using	O
learning	B
curves	I
so	O
as	O
to	O
find	O
the	O
best	O
remedy	O
exercises	O
failure	O
of	O
k-fold	O
cross	B
validation	B
consider	O
a	O
case	O
in	O
that	O
the	O
label	B
is	O
chosen	O
at	O
random	O
according	O
to	O
py	O
py	O
consider	O
a	O
learning	O
algorithm	O
that	O
outputs	O
the	O
constant	O
predictor	B
hx	O
if	O
the	O
parity	O
of	O
the	O
labels	O
on	O
the	O
training	B
set	B
is	O
and	O
otherwise	O
the	O
algorithm	O
outputs	O
the	O
constant	O
predictor	B
hx	O
prove	O
that	O
the	O
difference	O
between	O
the	O
leave-oneout	O
estimate	O
and	O
the	O
true	B
error	I
in	O
such	O
a	O
case	O
is	O
always	O
let	O
be	O
k	O
hypothesis	B
classes	O
suppose	O
you	O
are	O
given	O
m	O
i	O
i	O
d	O
training	O
consider	O
two	O
examples	O
and	O
you	O
would	O
like	O
to	O
learn	O
the	O
class	O
h	O
k	O
alternative	O
approaches	O
learn	O
h	O
on	O
the	O
m	O
examples	O
using	O
the	O
erm	B
rule	O
exercises	O
divide	O
the	O
m	O
examples	O
into	O
a	O
training	B
set	B
of	O
size	O
and	O
a	O
validation	B
set	B
of	O
size	O
m	O
for	O
some	O
then	O
apply	O
the	O
approach	O
of	O
model	B
selection	I
using	O
validation	B
that	O
is	O
first	O
train	O
each	O
class	O
hi	O
on	O
the	O
training	O
examples	O
using	O
the	O
erm	B
rule	O
with	O
respect	O
to	O
hi	O
and	O
let	O
hk	O
be	O
the	O
resulting	O
hypotheses	O
second	O
apply	O
the	O
erm	B
rule	O
with	O
respect	O
to	O
the	O
finite	O
class	O
hk	O
on	O
the	O
m	O
validation	B
examples	O
describe	O
scenarios	O
in	O
which	O
the	O
first	O
method	O
is	O
better	O
than	O
the	O
second	O
and	O
vice	O
versa	O
convex	B
learning	O
problems	O
in	O
this	O
chapter	O
we	O
introduce	O
convex	B
learning	O
problems	O
convex	B
learning	O
comprises	O
an	O
important	O
family	O
of	O
learning	O
problems	O
mainly	O
because	O
most	O
of	O
what	O
we	O
can	O
learn	O
efficiently	O
falls	O
into	O
it	O
we	O
have	O
already	O
encountered	O
linear	B
regression	B
with	O
the	O
squared	O
loss	B
and	O
logistic	B
regression	B
which	O
are	O
convex	B
problems	O
and	O
indeed	O
they	O
can	O
be	O
learned	O
efficiently	O
we	O
have	O
also	O
seen	O
nonconvex	O
problems	O
such	O
as	O
halfspaces	O
with	O
the	O
loss	B
which	O
is	O
known	O
to	O
be	O
computationally	O
hard	O
to	O
learn	O
in	O
the	O
unrealizable	O
case	O
in	O
general	O
a	O
convex	B
learning	O
problem	O
is	O
a	O
problem	O
whose	O
hypothesis	B
class	I
is	O
a	O
convex	B
set	B
and	O
whose	O
loss	B
function	B
is	O
a	O
convex	B
function	B
for	O
each	O
example	O
we	O
begin	O
the	O
chapter	O
with	O
some	O
required	O
definitions	O
of	O
convexity	O
besides	O
convexity	O
we	O
will	O
define	O
lipschitzness	B
and	O
smoothness	B
which	O
are	O
additional	O
properties	O
of	O
the	O
loss	B
function	B
that	O
facilitate	O
successful	O
learning	O
we	O
next	O
turn	O
to	O
defining	O
convex	B
learning	O
problems	O
and	O
demonstrate	O
the	O
necessity	O
for	O
further	O
constraints	O
such	O
as	O
boundedness	B
and	O
lipschitzness	B
or	O
smoothness	B
we	O
define	O
these	O
more	O
restricted	O
families	O
of	O
learning	O
problems	O
and	O
claim	O
that	O
convex-smoothlipschitz-bounded	O
problems	O
are	O
learnable	O
these	O
claims	O
will	O
be	O
proven	O
in	O
the	O
next	O
two	O
chapters	O
in	O
which	O
we	O
will	O
present	O
two	O
learning	O
paradigms	O
that	O
successfully	O
learn	O
all	O
problems	O
that	O
are	O
either	O
convex-lipschitz-bounded	O
or	O
convex-smooth-bounded	O
finally	O
in	O
section	O
we	O
show	O
how	O
one	O
can	O
handle	O
some	O
nonconvex	O
problems	O
by	O
minimizing	O
surrogate	B
loss	B
functions	O
that	O
are	O
convex	B
of	O
the	O
original	O
nonconvex	O
loss	B
function	B
surrogate	O
convex	B
loss	B
functions	O
give	O
rise	O
to	O
efficient	O
solutions	O
but	O
might	O
increase	O
the	O
risk	B
of	O
the	O
learned	O
predictor	B
convexity	O
lipschitzness	B
and	O
smoothness	B
convexity	O
definition	O
set	B
a	O
set	B
c	O
in	O
a	O
vector	O
space	O
is	O
convex	B
if	O
for	O
any	O
two	O
vectors	O
u	O
v	O
in	O
c	O
the	O
line	O
segment	O
between	O
u	O
and	O
v	O
is	O
contained	O
in	O
c	O
that	O
is	O
for	O
any	O
we	O
have	O
that	O
u	O
c	O
examples	O
of	O
convex	B
and	O
nonconvex	O
sets	O
in	O
are	O
given	O
in	O
the	O
following	O
for	O
the	O
nonconvex	O
sets	O
we	O
depict	O
two	O
points	O
in	O
the	O
set	B
such	O
that	O
the	O
line	O
between	O
the	O
two	O
points	O
is	O
not	O
contained	O
in	O
the	O
set	B
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
convexity	O
lipschitzness	B
and	O
smoothness	B
non-convex	O
convex	B
given	O
the	O
combination	O
u	O
of	O
the	O
points	O
u	O
v	O
is	O
called	O
a	O
convex	B
combination	O
definition	O
function	B
let	O
c	O
be	O
a	O
convex	B
set	B
a	O
function	B
f	O
c	O
r	O
is	O
convex	B
if	O
for	O
every	O
u	O
v	O
c	O
and	O
f	O
u	O
f	O
in	O
words	O
f	O
is	O
convex	B
if	O
for	O
any	O
u	O
v	O
the	O
graph	O
of	O
f	O
between	O
u	O
and	O
v	O
lies	O
below	O
the	O
line	O
segment	O
joining	O
f	O
and	O
f	O
an	O
illustration	O
of	O
a	O
convex	B
function	B
f	O
r	O
r	O
is	O
depicted	O
in	O
the	O
following	O
f	O
f	O
f	O
u	O
f	O
u	O
u	O
v	O
the	O
epigraph	B
of	O
a	O
function	B
f	O
is	O
the	O
set	B
epigraphf	O
f	O
it	O
is	O
easy	O
to	O
verify	O
that	O
a	O
function	B
f	O
is	O
convex	B
if	O
and	O
only	O
if	O
its	O
epigraph	B
is	O
a	O
convex	B
set	B
an	O
illustration	O
of	O
a	O
nonconvex	O
function	B
f	O
r	O
r	O
along	O
with	O
its	O
epigraph	B
is	O
given	O
in	O
the	O
following	O
convex	B
learning	O
problems	O
f	O
x	O
an	O
important	O
property	O
of	O
convex	B
functions	O
is	O
that	O
every	O
local	B
minimum	I
of	O
the	O
function	B
is	O
also	O
a	O
global	O
minimum	O
formally	O
let	O
bu	O
r	O
r	O
be	O
a	O
ball	O
of	O
radius	O
r	O
centered	O
around	O
u	O
we	O
say	O
that	O
f	O
is	O
a	O
local	B
minimum	I
of	O
f	O
at	O
u	O
if	O
there	O
exists	O
some	O
r	O
such	O
that	O
for	O
all	O
v	O
bu	O
r	O
we	O
have	O
f	O
f	O
it	O
follows	O
that	O
for	O
any	O
v	O
necessarily	O
in	O
b	O
there	O
is	O
a	O
small	O
enough	O
such	O
that	O
u	O
u	O
bu	O
r	O
and	O
therefore	O
f	O
f	O
u	O
if	O
f	O
is	O
convex	B
we	O
also	O
have	O
that	O
f	O
u	O
f	O
v	O
f	O
combining	O
these	O
two	O
equations	O
and	O
rearranging	O
terms	O
we	O
conclude	O
that	O
f	O
f	O
since	O
this	O
holds	O
for	O
every	O
v	O
it	O
follows	O
that	O
f	O
is	O
also	O
a	O
global	O
minimum	O
of	O
f	O
another	O
important	O
property	O
of	O
convex	B
functions	O
is	O
that	O
for	O
every	O
w	O
we	O
can	O
construct	O
a	O
tangent	O
to	O
f	O
at	O
w	O
that	O
lies	O
below	O
f	O
everywhere	O
if	O
f	O
is	O
differentiable	O
this	O
tangent	O
is	O
the	O
linear	O
function	B
lu	O
f	O
f	O
u	O
where	O
f	O
is	O
the	O
gradient	B
of	O
f	O
at	O
w	O
namely	O
the	O
vector	O
of	O
partial	O
derivatives	O
of	O
f	O
f	O
that	O
is	O
for	O
convex	B
differentiable	O
functions	O
u	O
f	O
f	O
f	O
u	O
f	O
f	O
wd	O
in	O
chapter	O
we	O
will	O
generalize	O
this	O
inequality	O
to	O
nondifferentiable	O
functions	O
an	O
illustration	O
of	O
equation	O
is	O
given	O
in	O
the	O
following	O
convexity	O
lipschitzness	B
and	O
smoothness	B
w	O
f	O
w	O
f	O
f	O
w	O
f	O
w	O
u	O
if	O
f	O
is	O
a	O
scalar	O
differentiable	O
function	B
there	O
is	O
an	O
easy	O
way	O
to	O
check	O
if	O
it	O
is	O
convex	B
lemma	O
let	O
f	O
r	O
r	O
be	O
a	O
scalar	O
twice	O
differential	O
function	B
and	O
let	O
be	O
its	O
first	O
and	O
second	O
derivatives	O
respectively	O
then	O
the	O
following	O
are	O
equivalent	O
f	O
is	O
convex	B
is	O
monotonically	O
nondecreasing	O
is	O
nonnegative	O
example	O
the	O
scalar	O
function	B
f	O
is	O
convex	B
to	O
see	O
this	O
note	O
that	O
and	O
the	O
scalar	O
function	B
f	O
expx	O
is	O
convex	B
to	O
see	O
this	O
observe	O
that	O
exp	O
this	O
is	O
a	O
monotonically	O
increasing	O
function	B
expx	O
since	O
the	O
exponent	O
function	B
is	O
a	O
monotonically	O
increasing	O
function	B
the	O
following	O
claim	O
shows	O
that	O
the	O
composition	O
of	O
a	O
convex	B
scalar	O
function	B
with	O
a	O
linear	O
function	B
yields	O
a	O
convex	B
vector-valued	O
function	B
claim	O
assume	O
that	O
f	O
rd	O
r	O
can	O
be	O
written	O
as	O
f	O
y	O
for	O
some	O
x	O
rd	O
y	O
r	O
and	O
g	O
r	O
r	O
then	O
convexity	O
of	O
g	O
implies	O
the	O
convexity	O
of	O
f	O
proof	O
let	O
rd	O
and	O
we	O
have	O
f	O
y	O
g	O
y	O
g	O
y	O
y	O
y	O
y	O
where	O
the	O
last	O
inequality	O
follows	O
from	O
the	O
convexity	O
of	O
g	O
example	O
convex	B
learning	O
problems	O
given	O
some	O
x	O
rd	O
and	O
y	O
r	O
let	O
f	O
rd	O
r	O
be	O
defined	O
as	O
f	O
then	O
f	O
is	O
a	O
composition	O
of	O
the	O
function	B
ga	O
onto	O
a	O
linear	O
function	B
and	O
hence	O
f	O
is	O
a	O
convex	B
function	B
given	O
some	O
x	O
rd	O
and	O
y	O
let	O
f	O
rd	O
r	O
be	O
defined	O
as	O
f	O
exp	O
then	O
f	O
is	O
a	O
composition	O
of	O
the	O
function	B
ga	O
expa	O
onto	O
a	O
linear	O
function	B
and	O
hence	O
f	O
is	O
a	O
convex	B
function	B
finally	O
the	O
following	O
lemma	O
shows	O
that	O
the	O
maximum	O
of	O
convex	B
functions	O
is	O
convex	B
and	O
that	O
a	O
weighted	O
sum	O
of	O
convex	B
functions	O
with	O
nonnegative	O
weights	O
is	O
also	O
convex	B
claim	O
for	O
i	O
r	O
let	O
fi	O
following	O
functions	O
from	O
rd	O
to	O
r	O
are	O
also	O
convex	B
gx	O
maxi	O
fix	O
wifix	O
where	O
for	O
all	O
i	O
wi	O
gx	O
rd	O
r	O
be	O
a	O
convex	B
function	B
the	O
proof	O
the	O
first	O
claim	O
follows	O
by	O
i	O
g	O
u	O
max	O
max	O
max	O
gu	O
fi	O
u	O
fiu	O
fiu	O
max	O
i	O
i	O
i	O
fiv	O
wifi	O
u	O
wi	O
fiu	O
wifiu	O
gu	O
i	O
wifiv	O
i	O
for	O
the	O
second	O
claim	O
g	O
u	O
i	O
i	O
example	O
the	O
function	B
gx	O
is	O
convex	B
to	O
see	O
this	O
note	O
that	O
gx	O
maxx	O
x	O
and	O
that	O
both	O
the	O
function	B
x	O
and	O
x	O
are	O
convex	B
lipschitzness	B
the	O
definition	O
of	O
lipschitzness	B
below	O
is	O
with	O
respect	O
to	O
the	O
euclidean	O
norm	O
over	O
rd	O
however	O
it	O
is	O
possible	O
to	O
define	O
lipschitzness	B
with	O
respect	O
to	O
any	O
norm	O
definition	O
let	O
c	O
rd	O
a	O
function	B
f	O
rd	O
rk	O
is	O
over	O
c	O
if	O
for	O
every	O
c	O
we	O
have	O
that	O
f	O
convexity	O
lipschitzness	B
and	O
smoothness	B
intuitively	O
a	O
lipschitz	O
function	B
cannot	O
change	O
too	O
fast	O
note	O
that	O
if	O
f	O
r	O
r	O
is	O
differentiable	O
then	O
by	O
the	O
mean	O
value	O
theorem	O
we	O
have	O
f	O
f	O
where	O
u	O
is	O
some	O
point	O
between	O
and	O
it	O
follows	O
that	O
if	O
the	O
derivative	O
of	O
f	O
is	O
everywhere	O
bounded	O
absolute	O
value	O
by	O
then	O
the	O
function	B
is	O
example	O
the	O
function	B
f	O
is	O
over	O
r	O
this	O
follows	O
from	O
the	O
triangle	O
inequality	O
for	O
every	O
since	O
this	O
holds	O
for	O
both	O
and	O
we	O
obtain	O
that	O
the	O
function	B
f	O
expx	O
is	O
over	O
r	O
to	O
see	O
this	O
observe	O
that	O
expx	O
expx	O
exp	O
x	O
the	O
function	B
f	O
is	O
not	O
over	O
r	O
for	O
any	O
to	O
see	O
this	O
take	O
and	O
then	O
f	O
f	O
however	O
this	O
function	B
is	O
over	O
the	O
set	B
c	O
indeed	O
for	O
any	O
c	O
we	O
have	O
the	O
linear	O
function	B
f	O
rd	O
r	O
defined	O
by	O
f	O
b	O
where	O
v	O
rd	O
is	O
indeed	O
using	O
cauchy-schwartz	O
inequality	O
f	O
the	O
following	O
claim	O
shows	O
that	O
composition	O
of	O
lipschitz	O
functions	O
preserves	O
lipschitzness	B
claim	O
let	O
f	O
where	O
is	O
and	O
is	O
then	O
f	O
is	O
in	O
particular	O
if	O
is	O
the	O
linear	O
function	B
b	O
for	O
some	O
v	O
rd	O
b	O
r	O
then	O
f	O
is	O
proof	O
f	O
convex	B
learning	O
problems	O
smoothness	B
the	O
definition	O
of	O
a	O
smooth	O
function	B
relies	O
on	O
the	O
notion	O
of	O
gradient	B
recall	B
that	O
the	O
gradient	B
of	O
a	O
differentiable	O
function	B
f	O
rd	O
r	O
at	O
w	O
denoted	O
f	O
is	O
the	O
vector	O
of	O
partial	O
derivatives	O
of	O
f	O
namely	O
f	O
definition	O
a	O
differentiable	O
function	B
f	O
rd	O
r	O
is	O
smooth	O
if	O
its	O
gradient	B
is	O
namely	O
for	O
all	O
v	O
w	O
we	O
have	O
f	O
f	O
f	O
wd	O
f	O
it	O
is	O
possible	O
to	O
show	O
that	O
smoothness	B
implies	O
that	O
for	O
all	O
v	O
w	O
we	O
have	O
f	O
f	O
f	O
v	O
recall	B
that	O
convexity	O
of	O
f	O
implies	O
that	O
f	O
f	O
f	O
v	O
therefore	O
when	O
a	O
function	B
is	O
both	O
convex	B
and	O
smooth	O
we	O
have	O
both	O
upper	O
and	O
lower	O
bounds	O
on	O
the	O
difference	O
between	O
the	O
function	B
and	O
its	O
first	O
order	O
approximation	O
setting	O
v	O
w	O
f	O
in	O
the	O
right-hand	O
side	O
of	O
equation	O
and	O
rear	O
ranging	O
terms	O
we	O
obtain	O
f	O
f	O
f	O
if	O
we	O
further	O
assume	O
that	O
f	O
for	O
all	O
v	O
we	O
conclude	O
that	O
smoothness	B
implies	O
the	O
following	O
f	O
f	O
a	O
function	B
that	O
satisfies	O
this	O
property	O
is	O
also	O
called	O
a	O
self-bounded	O
function	B
example	O
the	O
function	B
f	O
is	O
this	O
follows	O
directly	O
from	O
the	O
fact	O
that	O
note	O
that	O
for	O
this	O
particular	O
function	B
equation	O
and	O
equation	O
hold	O
with	O
equality	O
the	O
function	B
f	O
expx	O
is	O
indeed	O
since	O
x	O
we	O
have	O
that	O
exp	O
x	O
exp	O
exp	O
expx	O
hence	O
tion	O
holds	O
as	O
well	O
is	O
since	O
this	O
function	B
is	O
nonnegative	O
equa	O
the	O
following	O
claim	O
shows	O
that	O
a	O
composition	O
of	O
a	O
smooth	O
scalar	O
function	B
over	O
a	O
linear	O
function	B
preserves	O
smoothness	B
claim	O
let	O
f	O
b	O
where	O
g	O
r	O
r	O
is	O
a	O
function	B
x	O
rd	O
and	O
b	O
r	O
then	O
f	O
is	O
convex	B
learning	O
problems	O
proof	O
by	O
the	O
chain	O
rule	O
we	O
have	O
that	O
f	O
bx	O
where	O
is	O
the	O
derivative	O
of	O
g	O
using	O
the	O
smoothness	B
of	O
g	O
and	O
the	O
cauchy-schwartz	O
inequality	O
we	O
therefore	O
obtain	O
f	O
b	O
b	O
w	O
b	O
w	O
f	O
f	O
v	O
w	O
example	O
for	O
any	O
x	O
rd	O
and	O
y	O
r	O
let	O
f	O
then	O
f	O
is	O
for	O
any	O
x	O
rd	O
and	O
y	O
let	O
f	O
exp	O
then	O
f	O
is	O
smooth	O
convex	B
learning	O
problems	O
recall	B
that	O
in	O
our	O
general	O
definition	O
of	O
learning	O
in	O
chapter	O
we	O
have	O
a	O
hypothesis	B
class	I
h	O
a	O
set	B
of	O
examples	O
z	O
and	O
a	O
loss	B
function	B
h	O
z	O
r	O
so	O
far	O
in	O
the	O
book	O
we	O
have	O
mainly	O
thought	O
of	O
z	O
as	O
being	O
the	O
product	O
of	O
an	O
instance	B
space	I
and	O
a	O
target	O
space	O
z	O
x	O
y	O
and	O
h	O
being	O
a	O
set	B
of	O
functions	O
from	O
x	O
to	O
y	O
however	O
h	O
can	O
be	O
an	O
arbitrary	O
set	B
indeed	O
throughout	O
this	O
chapter	O
we	O
consider	O
hypothesis	B
classes	O
h	O
that	O
are	O
subsets	O
of	O
the	O
euclidean	O
space	O
rd	O
that	O
is	O
every	O
hypothesis	B
is	O
some	O
real-valued	O
vector	O
we	O
shall	O
therefore	O
denote	O
a	O
hypothesis	B
in	O
h	O
by	O
w	O
now	O
we	O
can	O
finally	O
define	O
convex	B
learning	O
problems	O
definition	O
learning	O
problem	O
a	O
learning	O
problem	O
z	O
is	O
called	O
convex	B
if	O
the	O
hypothesis	B
class	I
h	O
is	O
a	O
convex	B
set	B
and	O
for	O
all	O
z	O
z	O
the	O
loss	B
function	B
z	O
is	O
a	O
convex	B
function	B
for	O
any	O
z	O
z	O
denotes	O
the	O
function	B
f	O
h	O
r	O
defined	O
by	O
f	O
z	O
example	O
regression	B
with	O
the	O
squared	O
loss	B
recall	B
that	O
linear	B
regression	B
is	O
a	O
tool	O
for	O
modeling	O
the	O
relationship	O
between	O
some	O
explanatory	O
variables	O
and	O
some	O
real	O
valued	O
outcome	O
chapter	O
the	O
domain	B
set	B
x	O
is	O
a	O
subset	O
of	O
rd	O
for	O
some	O
d	O
and	O
the	O
label	B
set	B
y	O
is	O
the	O
set	B
of	O
real	O
numbers	O
we	O
would	O
like	O
to	O
learn	O
a	O
linear	O
function	B
h	O
rd	O
r	O
that	O
best	O
approximates	O
the	O
relationship	O
between	O
our	O
variables	O
in	O
chapter	O
we	O
defined	O
the	O
hypothesis	B
class	I
as	O
the	O
set	B
of	O
homogenous	B
linear	O
functions	O
h	O
w	O
rd	O
and	O
used	O
the	O
squared	O
loss	B
function	B
y	O
however	O
we	O
can	O
equivalently	O
model	O
the	O
learning	O
problem	O
as	O
a	O
convex	B
learning	O
problem	O
as	O
follows	O
convex	B
learning	O
problems	O
each	O
linear	O
function	B
is	O
parameterized	O
by	O
a	O
vector	O
w	O
rd	O
hence	O
we	O
can	O
define	O
h	O
to	O
be	O
the	O
set	B
of	O
all	O
such	O
parameters	O
namely	O
h	O
rd	O
the	O
set	B
of	O
examples	O
is	O
z	O
x	O
y	O
rd	O
r	O
and	O
the	O
loss	B
function	B
is	O
y	O
clearly	O
the	O
set	B
h	O
is	O
a	O
convex	B
set	B
the	O
loss	B
function	B
is	O
also	O
convex	B
with	O
respect	O
to	O
its	O
first	O
argument	O
example	O
if	O
is	O
a	O
convex	B
loss	B
function	B
and	O
the	O
class	O
h	O
is	O
convex	B
then	O
the	O
lemma	O
ermh	O
problem	O
of	O
minimizing	O
the	O
empirical	O
loss	B
over	O
h	O
is	O
a	O
convex	B
optimization	O
problem	O
is	O
a	O
problem	O
of	O
minimizing	O
a	O
convex	B
function	B
over	O
a	O
convex	B
set	B
proof	O
recall	B
that	O
the	O
ermh	O
problem	O
is	O
defined	O
by	O
ermhs	O
argmin	O
w	O
h	O
lsw	O
since	O
for	O
a	O
sample	O
s	O
zm	O
for	O
every	O
w	O
lsw	O
zi	O
m	O
claim	O
implies	O
that	O
lsw	O
is	O
a	O
convex	B
function	B
therefore	O
the	O
erm	B
rule	O
is	O
a	O
problem	O
of	O
minimizing	O
a	O
convex	B
function	B
subject	O
to	O
the	O
constraint	O
that	O
the	O
solution	O
should	O
be	O
in	O
a	O
convex	B
set	B
under	O
mild	O
conditions	O
such	O
problems	O
can	O
be	O
solved	O
efficiently	O
using	O
generic	O
optimization	O
algorithms	O
in	O
particular	O
in	O
chapter	O
we	O
will	O
present	O
a	O
very	O
simple	O
algorithm	O
for	O
minimizing	O
convex	B
functions	O
learnability	O
of	O
convex	B
learning	O
problems	O
we	O
have	O
argued	O
that	O
for	O
many	O
cases	O
implementing	O
the	O
erm	B
rule	O
for	O
convex	B
learning	O
problems	O
can	O
be	O
done	O
efficiently	O
but	O
is	O
convexity	O
a	O
sufficient	O
condition	O
for	O
the	O
learnability	O
of	O
a	O
problem	O
to	O
make	O
the	O
quesion	O
more	O
specific	O
in	O
vc	O
theory	O
we	O
saw	O
that	O
halfspaces	O
in	O
d-dimension	O
are	O
learnable	O
inefficiently	O
we	O
also	O
argued	O
in	O
chapter	O
using	O
the	O
discretization	B
trick	I
that	O
if	O
the	O
problem	O
is	O
of	O
d	O
parameters	O
it	O
is	O
learnable	O
with	O
a	O
sample	B
complexity	I
being	O
a	O
function	B
of	O
d	O
that	O
is	O
for	O
a	O
constant	O
d	O
the	O
problem	O
should	O
be	O
learnable	O
so	O
maybe	O
all	O
convex	B
learning	O
problems	O
over	O
rd	O
are	O
learnable	O
example	O
later	O
shows	O
that	O
the	O
answer	O
is	O
negative	O
even	O
when	O
d	O
is	O
low	O
not	O
all	O
convex	B
learning	O
problems	O
over	O
rd	O
are	O
learnable	O
there	O
is	O
no	O
contradiction	O
to	O
vc	O
theory	O
since	O
vc	O
theory	O
only	O
deals	O
with	O
binary	O
classification	O
while	O
here	O
we	O
consider	O
a	O
wide	O
family	O
of	O
problems	O
there	O
is	O
also	O
no	O
contradiction	O
to	O
the	O
discretization	B
trick	I
as	O
there	O
we	O
assumed	O
that	O
the	O
loss	B
function	B
is	O
bounded	O
and	O
also	O
assumed	O
that	O
a	O
representation	O
of	O
each	O
parameter	O
using	O
a	O
finite	O
number	O
of	O
bits	O
suffices	O
as	O
we	O
will	O
show	O
later	O
under	O
some	O
additional	O
restricting	O
conditions	O
that	O
hold	O
in	O
many	O
practical	O
scenarios	O
convex	B
problems	O
are	O
learnable	O
example	O
of	O
linear	B
regression	B
even	O
if	O
d	O
let	O
h	O
r	O
and	O
the	O
loss	B
be	O
the	O
squared	O
loss	B
y	O
re	O
referring	O
to	O
the	O
convex	B
learning	O
problems	O
choose	O
let	O
m	O
m	O
and	O
set	B
homogenous	B
case	O
let	O
a	O
be	O
any	O
deterministic	O
assume	O
by	O
way	O
of	O
contradiction	O
that	O
a	O
is	O
a	O
successful	O
pac	B
learner	O
for	O
this	O
problem	O
that	O
is	O
there	O
exists	O
a	O
function	B
m	O
such	O
that	O
for	O
every	O
distribution	O
d	O
and	O
for	O
every	O
if	O
a	O
receives	O
a	O
training	B
set	B
of	O
size	O
m	O
m	O
it	O
should	O
output	O
with	O
probability	O
of	O
at	O
least	O
a	O
hypothesis	B
w	O
as	O
such	O
that	O
ld	O
w	O
minw	O
ldw	O
we	O
will	O
define	O
two	O
distributions	O
and	O
will	O
show	O
that	O
a	O
is	O
likely	O
to	O
fail	O
on	O
at	O
least	O
one	O
of	O
them	O
the	O
first	O
distribution	O
is	O
supported	O
on	O
two	O
examples	O
and	O
where	O
the	O
probability	O
mass	O
of	O
the	O
first	O
example	O
is	O
while	O
the	O
probability	O
mass	O
of	O
the	O
second	O
example	O
is	O
the	O
second	O
distribution	O
is	O
supported	O
entirely	O
on	O
observe	O
that	O
for	O
both	O
distributions	O
the	O
probability	O
that	O
all	O
examples	O
of	O
the	O
training	B
set	B
will	O
be	O
of	O
the	O
second	O
type	O
is	O
at	O
least	O
this	O
is	O
trivially	O
true	O
for	O
whereas	O
for	O
the	O
probability	O
of	O
this	O
event	O
is	O
e	O
m	O
since	O
we	O
assume	O
that	O
a	O
is	O
a	O
deterministic	O
algorithm	O
upon	O
receiving	O
a	O
training	B
set	B
of	O
m	O
examples	O
each	O
of	O
which	O
is	O
the	O
algorithm	O
will	O
output	O
some	O
w	O
now	O
if	O
w	O
we	O
will	O
set	B
the	O
distribution	O
to	O
be	O
hence	O
w	O
min	O
w	O
however	O
it	O
follows	O
that	O
w	O
min	O
w	O
therefore	O
such	O
algorithm	O
a	O
fails	O
on	O
on	O
the	O
other	O
hand	O
if	O
w	O
then	O
we	O
ll	O
set	B
the	O
distribution	O
to	O
be	O
then	O
we	O
have	O
that	O
w	O
while	O
minw	O
so	O
a	O
fails	O
on	O
in	O
summary	O
we	O
have	O
shown	O
that	O
for	O
every	O
a	O
there	O
exists	O
a	O
distribution	O
on	O
which	O
a	O
fails	O
which	O
implies	O
that	O
the	O
problem	O
is	O
not	O
pac	B
learnable	O
a	O
possible	O
solution	O
to	O
this	O
problem	O
is	O
to	O
add	O
another	O
constraint	O
on	O
the	O
hypothesis	B
class	I
in	O
addition	O
to	O
the	O
convexity	O
requirement	O
we	O
require	O
that	O
h	O
will	O
be	O
bounded	O
namely	O
we	O
assume	O
that	O
for	O
some	O
predefined	O
scalar	O
b	O
every	O
hypothesis	B
w	O
h	O
satisfies	O
b	O
boundedness	B
and	O
convexity	O
alone	O
are	O
still	O
not	O
sufficient	O
for	O
ensuring	O
that	O
the	O
problem	O
is	O
learnable	O
as	O
the	O
following	O
example	O
demonstrates	O
example	O
as	O
in	O
example	O
consider	O
a	O
regression	B
problem	O
with	O
the	O
squared	O
loss	B
however	O
this	O
time	O
let	O
h	O
r	O
be	O
a	O
bounded	O
namely	O
given	O
s	O
the	O
output	O
of	O
a	O
is	O
determined	O
this	O
requirement	O
is	O
for	O
the	O
sake	O
of	O
simplicity	O
a	O
slightly	O
more	O
involved	O
argument	O
will	O
show	O
that	O
nondeterministic	O
algorithms	O
will	O
also	O
fail	O
to	O
learn	O
the	O
problem	O
convex	B
learning	O
problems	O
hypothesis	B
class	I
it	O
is	O
easy	O
to	O
verify	O
that	O
h	O
is	O
convex	B
the	O
argument	O
will	O
be	O
the	O
same	O
as	O
in	O
example	O
except	O
that	O
now	O
the	O
two	O
distributions	O
will	O
be	O
supported	O
on	O
and	O
if	O
the	O
algorithm	O
a	O
returns	O
w	O
upon	O
receiving	O
m	O
examples	O
of	O
the	O
second	O
type	O
then	O
we	O
will	O
set	B
the	O
distribution	O
to	O
be	O
and	O
have	O
that	O
w	O
min	O
w	O
similarly	O
if	O
w	O
we	O
will	O
set	B
the	O
distribution	O
to	O
be	O
and	O
have	O
that	O
w	O
w	O
min	O
w	O
this	O
example	O
shows	O
that	O
we	O
need	O
additional	O
assumptions	O
on	O
the	O
learning	O
problem	O
and	O
this	O
time	O
the	O
solution	O
is	O
in	O
lipschitzness	B
or	O
smoothness	B
of	O
the	O
loss	B
function	B
this	O
motivates	O
a	O
definition	O
of	O
two	O
families	O
of	O
learning	O
problems	O
convex-lipschitz-bounded	O
and	O
convex-smooth-bounded	O
which	O
are	O
defined	O
later	O
convex-lipschitzsmooth-bounded	O
learning	O
problems	O
definition	O
learning	O
problem	O
a	O
learning	O
problem	O
z	O
is	O
called	O
convex-lipschitz-bounded	O
with	O
parameters	O
b	O
if	O
the	O
following	O
holds	O
the	O
hypothesis	B
class	I
h	O
is	O
a	O
convex	B
set	B
and	O
for	O
all	O
w	O
h	O
we	O
have	O
b	O
for	O
all	O
z	O
z	O
the	O
loss	B
function	B
z	O
is	O
a	O
convex	B
and	O
function	B
example	O
let	O
x	O
rd	O
and	O
y	O
r	O
let	O
h	O
rd	O
b	O
and	O
let	O
the	O
loss	B
function	B
be	O
y	O
y	O
this	O
corresponds	O
to	O
a	O
regression	B
problem	O
with	O
the	O
absolute-value	O
loss	B
where	O
we	O
assume	O
that	O
the	O
instances	O
are	O
in	O
a	O
ball	O
of	O
radius	O
and	O
we	O
restrict	O
the	O
hypotheses	O
to	O
be	O
homogenous	B
linear	O
functions	O
defined	O
by	O
a	O
vector	O
w	O
whose	O
norm	O
is	O
bounded	O
by	O
b	O
then	O
the	O
resulting	O
problem	O
is	O
convex-lipschitz-bounded	O
with	O
parameters	O
b	O
definition	O
learning	O
problem	O
a	O
learning	O
problem	O
z	O
is	O
called	O
convex-smooth-bounded	O
with	O
parameters	O
b	O
if	O
the	O
following	O
holds	O
the	O
hypothesis	B
class	I
h	O
is	O
a	O
convex	B
set	B
and	O
for	O
all	O
w	O
h	O
we	O
have	O
b	O
for	O
all	O
z	O
z	O
the	O
loss	B
function	B
z	O
is	O
a	O
convex	B
nonnegative	O
and	O
function	B
note	O
that	O
we	O
also	O
required	O
that	O
the	O
loss	B
function	B
is	O
nonnegative	O
this	O
is	O
needed	O
to	O
ensure	O
that	O
the	O
loss	B
function	B
is	O
self-bounded	O
as	O
described	O
in	O
the	O
previous	O
section	O
surrogate	B
loss	B
functions	O
example	O
let	O
x	O
rd	O
and	O
y	O
r	O
let	O
h	O
rd	O
b	O
and	O
let	O
the	O
loss	B
function	B
be	O
y	O
this	O
corresponds	O
to	O
a	O
regression	B
problem	O
with	O
the	O
squared	O
loss	B
where	O
we	O
assume	O
that	O
the	O
instances	O
are	O
in	O
a	O
ball	O
of	O
radius	O
and	O
we	O
restrict	O
the	O
hypotheses	O
to	O
be	O
homogenous	B
linear	O
functions	O
defined	O
by	O
a	O
vector	O
w	O
whose	O
norm	O
is	O
bounded	O
by	O
b	O
then	O
the	O
resulting	O
problem	O
is	O
convex-smooth-bounded	O
with	O
parameters	O
b	O
we	O
claim	O
that	O
these	O
two	O
families	O
of	O
learning	O
problems	O
are	O
learnable	O
that	O
is	O
the	O
properties	O
of	O
convexity	O
boundedness	B
and	O
lipschitzness	B
or	O
smoothness	B
of	O
the	O
loss	B
function	B
are	O
sufficient	O
for	O
learnability	O
we	O
will	O
prove	O
this	O
claim	O
in	O
the	O
next	O
chapters	O
by	O
introducing	O
algorithms	O
that	O
learn	O
these	O
problems	O
successfully	O
surrogate	B
loss	B
functions	O
as	O
mentioned	O
and	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
chapters	O
convex	B
problems	O
can	O
be	O
learned	O
effficiently	O
however	O
in	O
many	O
cases	O
the	O
natural	O
loss	B
function	B
is	O
not	O
convex	B
and	O
in	O
particular	O
implementing	O
the	O
erm	B
rule	O
is	O
hard	O
as	O
an	O
example	O
consider	O
the	O
problem	O
of	O
learning	O
the	O
hypothesis	B
class	I
of	O
half	O
spaces	O
with	O
respect	O
to	O
the	O
loss	B
that	O
is	O
y	O
this	O
loss	B
function	B
is	O
not	O
convex	B
with	O
respect	O
to	O
w	O
and	O
indeed	O
when	O
trying	O
to	O
minimize	O
the	O
empirical	B
risk	B
with	O
respect	O
to	O
this	O
loss	B
function	B
we	O
might	O
encounter	O
local	O
minima	O
exercise	O
furthermore	O
as	O
discussed	O
in	O
chapter	O
solving	O
the	O
erm	B
problem	O
with	O
respect	O
to	O
the	O
loss	B
in	O
the	O
unrealizable	O
case	O
is	O
known	O
to	O
be	O
np-hard	O
to	O
circumvent	O
the	O
hardness	O
result	O
one	O
popular	O
approach	O
is	O
to	O
upper	O
bound	O
the	O
nonconvex	O
loss	B
function	B
by	O
a	O
convex	B
surrogate	B
loss	B
function	B
as	O
its	O
name	O
indicates	O
the	O
requirements	O
from	O
a	O
convex	B
surrogate	B
loss	B
are	O
as	O
follows	O
it	O
should	O
be	O
convex	B
it	O
should	O
upper	O
bound	O
the	O
original	O
loss	B
for	O
example	O
in	O
the	O
context	O
of	O
learning	O
halfspaces	O
we	O
can	O
define	O
the	O
so-called	O
hinge	B
loss	B
as	O
a	O
convex	B
surrogate	O
for	O
the	O
loss	B
as	O
follows	O
y	O
def	O
clearly	O
for	O
all	O
w	O
and	O
all	O
y	O
y	O
y	O
in	O
addition	O
the	O
convexity	O
of	O
the	O
hinge	B
loss	B
follows	O
directly	O
from	O
claim	O
hence	O
the	O
hinge	B
loss	B
satisfies	O
the	O
requirements	O
of	O
a	O
convex	B
surrogate	B
loss	B
function	B
for	O
the	O
zero-one	O
loss	B
an	O
illustration	O
of	O
the	O
functions	O
and	O
is	O
given	O
in	O
the	O
following	O
convex	B
learning	O
problems	O
once	O
we	O
have	O
defined	O
the	O
surrogate	O
convex	B
loss	B
we	O
can	O
learn	O
the	O
problem	O
with	O
respect	O
to	O
it	O
the	O
generalization	O
requirement	O
from	O
a	O
hinge	B
loss	B
learner	O
will	O
have	O
the	O
form	O
lhinged	O
min	O
w	O
h	O
lhinged	O
where	O
lhinged	O
can	O
lower	O
bound	O
the	O
left-hand	O
side	O
by	O
which	O
yields	O
exy	O
y	O
using	O
the	O
surrogate	O
property	O
we	O
w	O
h	O
lhinged	O
we	O
can	O
further	O
rewrite	O
the	O
upper	O
bound	O
as	O
follows	O
min	O
min	O
w	O
h	O
w	O
h	O
lhinged	O
min	O
w	O
h	O
min	O
that	O
is	O
the	O
error	O
of	O
the	O
learned	O
predictor	B
is	O
upper	O
bounded	O
by	O
three	O
terms	O
approximation	B
error	I
this	O
is	O
the	O
term	O
minw	O
h	O
which	O
measures	O
how	O
well	O
the	O
hypothesis	B
class	I
performs	O
on	O
the	O
distribution	O
we	O
already	O
elaborated	O
on	O
this	O
error	O
term	O
in	O
chapter	O
estimation	B
error	I
this	O
is	O
the	O
error	O
that	O
results	O
from	O
the	O
fact	O
that	O
we	O
only	O
receive	O
a	O
training	B
set	B
and	O
do	O
not	O
observe	O
the	O
distribution	O
d	O
we	O
already	O
elaborated	O
on	O
this	O
error	O
term	O
in	O
chapter	O
minw	O
h	O
that	O
measures	O
the	O
difference	O
between	O
the	O
approximation	B
error	I
with	O
respect	O
to	O
the	O
surrogate	B
loss	B
and	O
the	O
approximation	B
error	I
with	O
respect	O
to	O
the	O
original	O
loss	B
the	O
optimization	B
error	I
is	O
a	O
result	O
of	O
our	O
inability	O
to	O
minimize	O
the	O
training	O
loss	B
with	O
respect	O
to	O
the	O
original	O
loss	B
the	O
size	O
of	O
this	O
error	O
depends	O
on	O
the	O
specific	O
distribution	O
of	O
the	O
data	O
and	O
on	O
the	O
specific	O
surrogate	B
loss	B
we	O
are	O
using	O
optimization	B
error	I
this	O
is	O
the	O
term	O
minw	O
h	O
lhinged	O
summary	O
we	O
introduced	O
two	O
families	O
of	O
learning	O
problems	O
convex-lipschitz-bounded	O
and	O
convex-smooth-bounded	O
in	O
the	O
next	O
two	O
chapters	O
we	O
will	O
describe	O
two	O
generic	O
bibliographic	O
remarks	O
learning	O
algorithms	O
for	O
these	O
families	O
we	O
also	O
introduced	O
the	O
notion	O
of	O
convex	B
surrogate	B
loss	B
function	B
which	O
enables	O
us	O
also	O
to	O
utilize	O
the	O
convex	B
machinery	O
for	O
nonconvex	O
problems	O
bibliographic	O
remarks	O
there	O
are	O
several	O
excellent	O
books	O
on	O
convex	B
analysis	O
and	O
optimization	O
vandenberghe	O
borwein	O
lewis	O
bertsekas	O
hiriart-urruty	O
lemar	O
echal	O
regarding	O
learning	O
problems	O
the	O
family	O
of	O
convex-lipschitzbounded	O
problems	O
was	O
first	O
studied	O
by	O
zinkevich	O
in	O
the	O
context	O
of	O
online	B
learning	I
and	O
by	O
shalev-shwartz	O
shamir	O
sridharan	O
srebro	O
in	O
the	O
context	O
of	O
pac	B
learning	O
not	O
a	O
global	O
minimum	O
of	O
ls	O
exercises	O
construct	O
an	O
example	O
showing	O
that	O
the	O
loss	B
function	B
may	O
suffer	O
from	O
local	O
minima	O
namely	O
construct	O
a	O
training	O
sample	O
s	O
for	O
x	O
for	O
which	O
there	O
exist	O
a	O
vector	O
w	O
and	O
some	O
such	O
that	O
for	O
any	O
such	O
that	O
we	O
have	O
lsw	O
the	O
loss	B
here	O
is	O
the	O
loss	B
this	O
means	O
that	O
w	O
is	O
a	O
local	B
minimum	I
of	O
ls	O
there	O
exists	O
some	O
w	O
such	O
that	O
lsw	O
lsw	O
this	O
means	O
that	O
w	O
is	O
consider	O
the	O
learning	O
problem	O
of	O
logistic	B
regression	B
let	O
h	O
x	O
rd	O
b	O
for	O
some	O
scalar	O
b	O
let	O
y	O
and	O
let	O
the	O
loss	B
function	B
be	O
defined	O
as	O
y	O
exp	O
show	O
that	O
the	O
resulting	O
learning	O
problem	O
is	O
both	O
convex-lipschitz-bounded	O
and	O
convexsmooth-bounded	O
specify	O
the	O
parameters	O
of	O
lipschitzness	B
and	O
smoothness	B
consider	O
the	O
problem	O
of	O
learning	O
halfspaces	O
with	O
the	O
hinge	B
loss	B
we	O
limit	O
our	O
domain	B
to	O
the	O
euclidean	O
ball	O
with	O
radius	O
r	O
that	O
is	O
x	O
r	O
the	O
label	B
set	B
is	O
y	O
and	O
the	O
loss	B
function	B
is	O
defined	O
by	O
y	O
we	O
already	O
know	O
that	O
the	O
loss	B
function	B
is	O
convex	B
show	O
that	O
it	O
is	O
r-lipschitz	O
convex-lipschitz-boundedness	O
is	O
not	O
sufficient	O
for	O
computational	O
efficiency	O
in	O
the	O
next	O
chapter	O
we	O
show	O
that	O
from	O
the	O
statistical	O
perspective	O
all	O
convex-lipschitz-bounded	O
problems	O
are	O
learnable	O
the	O
agnostic	B
pac	B
model	O
however	O
our	O
main	O
motivation	O
to	O
learn	O
such	O
problems	O
resulted	O
from	O
the	O
computational	O
perspective	O
convex	B
optimization	O
is	O
often	O
efficiently	O
solvable	O
yet	O
the	O
goal	O
of	O
this	O
exercise	O
is	O
to	O
show	O
that	O
convexity	O
alone	O
is	O
not	O
sufficient	O
for	O
efficiency	O
we	O
show	O
that	O
even	O
for	O
the	O
case	O
d	O
there	O
is	O
a	O
convex-lipschitz-bounded	O
problem	O
which	O
cannot	O
be	O
learned	O
by	O
any	O
computable	O
learner	O
let	O
the	O
hypothesis	B
class	I
be	O
h	O
and	O
let	O
the	O
example	O
domain	B
z	O
be	O
convex	B
learning	O
problems	O
the	O
set	B
of	O
all	O
turing	O
machines	O
define	O
the	O
loss	B
function	B
as	O
follows	O
for	O
every	O
turing	O
machine	O
t	O
z	O
let	O
t	O
if	O
t	O
halts	O
on	O
the	O
input	O
and	O
t	O
if	O
t	O
doesn	O
t	O
halt	O
on	O
the	O
input	O
similarly	O
let	O
t	O
if	O
t	O
halts	O
on	O
the	O
input	O
and	O
t	O
if	O
t	O
doesn	O
t	O
halt	O
on	O
the	O
input	O
finally	O
for	O
h	O
let	O
t	O
t	O
t	O
show	O
that	O
the	O
resulting	O
learning	O
problem	O
is	O
convex-lipschitz-bounded	O
show	O
that	O
no	O
computable	O
algorithm	O
can	O
learn	O
the	O
problem	O
regularization	B
and	O
stability	B
in	O
the	O
previous	O
chapter	O
we	O
introduced	O
the	O
families	O
of	O
convex-lipschitz-bounded	O
and	O
convex-smooth-bounded	B
learning	I
problems	O
in	O
this	O
section	O
we	O
show	O
that	O
all	O
learning	O
problems	O
in	O
these	O
two	O
families	O
are	O
learnable	O
for	O
some	O
learning	O
problems	O
of	O
this	O
type	O
it	O
is	O
possible	O
to	O
show	O
that	O
uniform	B
convergence	I
holds	O
hence	O
they	O
are	O
learnable	O
using	O
the	O
erm	B
rule	O
however	O
this	O
is	O
not	O
true	O
for	O
all	O
learning	O
problems	O
of	O
this	O
type	O
yet	O
we	O
will	O
introduce	O
another	O
learning	O
rule	O
and	O
will	O
show	O
that	O
it	O
learns	O
all	O
convex-lipschitz-bounded	O
and	O
convex-smooth-bounded	B
learning	I
problems	O
the	O
new	O
learning	O
paradigm	O
we	O
introduce	O
in	O
this	O
chapter	O
is	O
called	O
regularized	O
loss	B
minimization	O
or	O
rlm	B
for	O
short	O
in	O
rlm	B
we	O
minimize	O
the	O
sum	O
of	O
the	O
empirical	B
risk	B
and	O
a	O
regularization	B
function	B
intuitively	O
the	O
regularization	B
function	B
measures	O
the	O
complexity	O
of	O
hypotheses	O
indeed	O
one	O
interpretation	O
of	O
the	O
regularization	B
function	B
is	O
the	O
structural	O
risk	B
minimization	O
paradigm	O
we	O
discussed	O
in	O
chapter	O
another	O
view	O
of	O
regularization	B
is	O
as	O
a	O
stabilizer	O
of	O
the	O
learning	O
algorithm	O
an	O
algorithm	O
is	O
considered	O
stable	O
if	O
a	O
slight	O
change	O
of	O
its	O
input	O
does	O
not	O
change	O
its	O
output	O
much	O
we	O
will	O
formally	O
define	O
the	O
notion	O
of	O
stability	B
we	O
mean	O
by	O
slight	O
change	O
of	O
input	O
and	O
by	O
does	O
not	O
change	O
much	O
the	O
output	O
and	O
prove	O
its	O
close	O
relation	O
to	O
learnability	O
finally	O
we	O
will	O
show	O
that	O
using	O
the	O
squared	O
norm	O
as	O
a	O
regularization	B
function	B
stabilizes	O
all	O
convex-lipschitz	O
or	O
convex-smooth	O
learning	O
problems	O
hence	O
rlm	B
can	O
be	O
used	O
as	O
a	O
general	O
learning	O
rule	O
for	O
these	O
families	O
of	O
learning	O
problems	O
regularized	O
loss	B
minimization	O
regularized	O
loss	B
minimization	O
is	O
a	O
learning	O
rule	O
in	O
which	O
we	O
jointly	O
minimize	O
the	O
empirical	B
risk	B
and	O
a	O
regularization	B
function	B
formally	O
a	O
regularization	B
function	B
is	O
a	O
mapping	O
r	O
rd	O
r	O
and	O
the	O
regularized	O
loss	B
minimization	O
rule	O
outputs	O
a	O
hypothesis	B
in	O
argmin	O
w	O
rw	O
regularized	O
loss	B
minimization	O
shares	O
similarities	O
with	O
minimum	O
description	O
length	O
algorithms	O
and	O
structural	O
risk	B
minimization	O
chapter	O
intuitively	O
the	O
complexity	O
of	O
hypotheses	O
is	O
measured	O
by	O
the	O
value	O
of	O
the	O
regularization	B
func	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
regularization	B
and	O
stability	B
tion	O
and	O
the	O
algorithm	O
balances	O
between	O
low	O
empirical	B
risk	B
and	O
simpler	O
or	O
less	O
complex	O
hypotheses	O
there	O
are	O
many	O
possible	O
regularization	B
functions	O
one	O
can	O
use	O
reflecting	O
some	O
prior	O
belief	O
about	O
the	O
problem	O
to	O
the	O
description	O
language	O
in	O
minimum	O
description	O
length	O
throughout	O
this	O
section	O
we	O
will	O
focus	O
on	O
one	O
of	O
the	O
most	O
simple	O
regularization	B
functions	O
rw	O
where	O
is	O
a	O
scalar	O
and	O
the	O
norm	O
is	O
the	O
norm	O
i	O
this	O
yields	O
the	O
learning	O
rule	O
as	O
argmin	O
w	O
this	O
type	O
of	O
regularization	B
function	B
is	O
often	O
called	O
tikhonov	B
regularization	B
as	O
mentioned	O
before	O
one	O
interpretation	O
of	O
equation	O
is	O
using	O
structural	O
risk	B
minimization	O
where	O
the	O
norm	O
of	O
w	O
is	O
a	O
measure	O
of	O
its	O
complexity	O
recall	B
that	O
in	O
the	O
previous	O
chapter	O
we	O
introduced	O
the	O
notion	O
of	O
bounded	O
hypothesis	B
classes	O
therefore	O
we	O
can	O
define	O
a	O
sequence	O
of	O
hypothesis	B
classes	O
where	O
hi	O
i	O
if	O
the	O
sample	B
complexity	I
of	O
each	O
hi	O
depends	O
on	O
i	O
then	O
the	O
rlm	B
rule	O
is	O
similar	O
to	O
the	O
srm	B
rule	O
for	O
this	O
sequence	O
of	O
nested	O
classes	O
a	O
different	O
interpretation	O
of	O
regularization	B
is	O
as	O
a	O
stabilizer	O
in	O
the	O
next	O
section	O
we	O
define	O
the	O
notion	O
of	O
stability	B
and	O
prove	O
that	O
stable	O
learning	O
rules	O
do	O
not	O
overfit	O
but	O
first	O
let	O
us	O
demonstrate	O
the	O
rlm	B
rule	O
for	O
linear	B
regression	B
with	O
the	O
squared	O
loss	B
ridge	B
regression	B
applying	O
the	O
rlm	B
rule	O
with	O
tikhonov	B
regularization	B
to	O
linear	B
regression	B
with	O
the	O
squared	O
loss	B
we	O
obtain	O
the	O
following	O
learning	O
rule	O
argmin	O
w	O
rd	O
m	O
performing	O
linear	B
regression	B
using	O
equation	O
is	O
called	O
ridge	B
regression	B
to	O
solve	O
equation	O
we	O
compare	O
the	O
gradient	B
of	O
the	O
objective	O
to	O
zero	O
and	O
obtain	O
the	O
set	B
of	O
linear	O
equations	O
mi	O
aw	O
b	O
a	O
xi	O
i	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
and	O
a	O
b	O
are	O
as	O
defined	O
in	O
equation	O
namely	O
and	O
b	O
yixi	O
since	O
a	O
is	O
a	O
positive	O
semidefinite	O
matrix	O
the	O
matrix	O
mi	O
a	O
has	O
all	O
its	O
eigenvalues	O
bounded	O
below	O
by	O
m	O
hence	O
this	O
matrix	O
is	O
invertible	O
and	O
the	O
solution	O
to	O
ridge	B
regression	B
becomes	O
w	O
mi	O
a	O
b	O
stable	O
rules	O
do	O
not	O
overfit	O
in	O
the	O
next	O
section	O
we	O
formally	O
show	O
how	O
regularization	B
stabilizes	O
the	O
algorithm	O
and	O
prevents	O
overfitting	B
in	O
particular	O
the	O
analysis	O
presented	O
in	O
the	O
next	O
sections	O
corollary	O
will	O
yield	O
theorem	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
where	O
x	O
rd	O
let	O
h	O
rd	O
b	O
for	O
any	O
let	O
m	O
then	O
applying	O
the	O
ridge	B
regression	B
algorithm	O
with	O
parameter	O
satisfies	O
e	O
s	O
dm	O
min	O
w	O
h	O
ldw	O
remark	O
the	O
preceding	O
theorem	O
tells	O
us	O
how	O
many	O
examples	O
are	O
needed	O
to	O
guarantee	O
that	O
the	O
expected	O
value	O
of	O
the	O
risk	B
of	O
the	O
learned	O
predictor	B
will	O
be	O
bounded	O
by	O
the	O
approximation	B
error	I
of	O
the	O
class	O
plus	O
in	O
the	O
usual	O
definition	O
of	O
agnostic	B
pac	B
learning	O
we	O
require	O
that	O
the	O
risk	B
of	O
the	O
learned	O
predictor	B
will	O
be	O
bounded	O
with	O
probability	O
of	O
at	O
least	O
in	O
exercise	O
we	O
show	O
how	O
an	O
algorithm	O
with	O
a	O
bounded	O
expected	O
risk	B
can	O
be	O
used	O
to	O
construct	O
an	O
agnostic	B
pac	B
learner	O
stable	O
rules	O
do	O
not	O
overfit	O
intuitively	O
a	O
learning	O
algorithm	O
is	O
stable	O
if	O
a	O
small	O
change	O
of	O
the	O
input	O
to	O
the	O
algorithm	O
does	O
not	O
change	O
the	O
output	O
of	O
the	O
algorithm	O
much	O
of	O
course	O
there	O
are	O
many	O
ways	O
to	O
define	O
what	O
we	O
mean	O
by	O
a	O
small	O
change	O
of	O
the	O
input	O
and	O
what	O
we	O
mean	O
by	O
does	O
not	O
change	O
the	O
output	O
much	O
in	O
this	O
section	O
we	O
define	O
a	O
specific	O
notion	O
of	O
stability	B
and	O
prove	O
that	O
under	O
this	O
definition	O
stable	O
rules	O
do	O
not	O
overfit	O
let	O
a	O
be	O
a	O
learning	O
algorithm	O
let	O
s	O
zm	O
be	O
a	O
training	B
set	B
of	O
m	O
examples	O
and	O
let	O
as	O
denote	O
the	O
output	O
of	O
a	O
the	O
algorithm	O
a	O
suffers	O
from	O
overfitting	B
if	O
the	O
difference	O
between	O
the	O
true	O
risk	B
of	O
its	O
output	O
ldas	O
and	O
the	O
empirical	B
risk	B
of	O
its	O
output	O
lsas	O
is	O
large	O
as	O
mentioned	O
in	O
remark	O
throughout	O
this	O
chapter	O
we	O
focus	O
on	O
the	O
expectation	O
respect	O
to	O
the	O
choice	O
of	O
s	O
of	O
this	O
quantity	O
namely	O
esldas	O
lsas	O
we	O
next	O
define	O
the	O
notion	O
of	O
stability	B
given	O
the	O
training	B
set	B
s	O
and	O
an	O
additional	O
example	O
let	O
si	O
be	O
the	O
training	B
set	B
obtained	O
by	O
replacing	O
the	O
i	O
th	O
example	O
of	O
s	O
with	O
namely	O
si	O
zi	O
zm	O
in	O
our	O
definition	O
of	O
stability	B
a	O
small	O
change	O
of	O
the	O
input	O
means	O
that	O
we	O
feed	O
a	O
with	O
si	O
instead	O
of	O
with	O
s	O
that	O
is	O
we	O
only	O
replace	O
one	O
training	O
example	O
we	O
measure	O
the	O
effect	O
of	O
this	O
small	O
change	O
of	O
the	O
input	O
on	O
the	O
output	O
of	O
a	O
by	O
comparing	O
the	O
loss	B
of	O
the	O
hypothesis	B
as	O
on	O
zi	O
to	O
the	O
loss	B
of	O
the	O
hypothesis	B
asi	O
on	O
zi	O
intuitively	O
a	O
good	O
learning	O
algorithm	O
will	O
have	O
zi	O
zi	O
since	O
in	O
the	O
first	O
term	O
the	O
learning	O
algorithm	O
does	O
not	O
observe	O
the	O
example	O
zi	O
while	O
in	O
the	O
second	O
term	O
zi	O
is	O
indeed	O
observed	O
if	O
the	O
preceding	O
difference	O
is	O
very	O
large	O
we	O
suspect	O
that	O
the	O
learning	O
algorithm	O
might	O
overfit	O
this	O
is	O
because	O
the	O
regularization	B
and	O
stability	B
learning	O
algorithm	O
drastically	O
changes	O
its	O
prediction	O
on	O
zi	O
if	O
it	O
observes	O
it	O
in	O
the	O
training	B
set	B
this	O
is	O
formalized	O
in	O
the	O
following	O
theorem	O
theorem	O
let	O
d	O
be	O
a	O
distribution	O
let	O
s	O
zm	O
be	O
an	O
i	O
i	O
d	O
sequence	O
of	O
examples	O
and	O
let	O
be	O
another	O
i	O
i	O
d	O
example	O
let	O
u	O
be	O
the	O
uniform	O
distribution	O
over	O
then	O
for	O
any	O
learning	O
algorithm	O
e	O
s	O
dm	O
lsas	O
e	O
u	O
zi	O
zi	O
proof	O
since	O
s	O
and	O
are	O
both	O
drawn	O
i	O
i	O
d	O
from	O
d	O
we	O
have	O
that	O
for	O
every	O
i	O
e	O
e	O
s	O
e	O
zi	O
on	O
the	O
other	O
hand	O
we	O
can	O
write	O
e	O
e	O
s	O
si	O
zi	O
combining	O
the	O
two	O
equations	O
we	O
conclude	O
our	O
proof	O
when	O
the	O
right-hand	O
side	O
of	O
equation	O
is	O
small	O
we	O
say	O
that	O
a	O
is	O
a	O
stable	O
algorithm	O
changing	O
a	O
single	O
example	O
in	O
the	O
training	B
set	B
does	O
not	O
lead	O
to	O
a	O
significant	O
change	O
formally	O
definition	O
let	O
n	O
r	O
be	O
a	O
monotonically	O
decreasing	O
function	B
we	O
say	O
that	O
a	O
learning	O
algorithm	O
a	O
is	O
on-averagereplace-one-stable	O
with	O
rate	O
if	O
for	O
every	O
distribution	O
d	O
e	O
u	O
zi	O
zi	O
theorem	O
tells	O
us	O
that	O
a	O
learning	O
algorithm	O
does	O
not	O
overfit	O
if	O
and	O
only	O
if	O
it	O
is	O
on-average-replace-one-stable	O
of	O
course	O
a	O
learning	O
algorithm	O
that	O
does	O
not	O
overfit	O
is	O
not	O
necessarily	O
a	O
good	O
learning	O
algorithm	O
take	O
for	O
example	O
an	O
algorithm	O
a	O
that	O
always	O
outputs	O
the	O
same	O
hypothesis	B
a	O
useful	O
algorithm	O
should	O
find	O
a	O
hypothesis	B
that	O
on	O
one	O
hand	O
fits	O
the	O
training	B
set	B
has	O
a	O
low	O
empirical	B
risk	B
and	O
on	O
the	O
other	O
hand	O
does	O
not	O
overfit	O
or	O
in	O
light	O
of	O
theorem	O
the	O
algorithm	O
should	O
both	O
fit	O
the	O
training	B
set	B
and	O
at	O
the	O
same	O
time	O
be	O
stable	O
as	O
we	O
shall	O
see	O
the	O
parameter	O
of	O
the	O
rlm	B
rule	O
balances	O
between	O
fitting	O
the	O
training	B
set	B
and	O
being	O
stable	O
tikhonov	B
regularization	B
as	O
a	O
stabilizer	O
in	O
the	O
previous	O
section	O
we	O
saw	O
that	O
stable	O
rules	O
do	O
not	O
overfit	O
in	O
this	O
section	O
we	O
show	O
that	O
applying	O
the	O
rlm	B
rule	O
with	O
tikhonov	B
regularization	B
leads	O
to	O
a	O
stable	O
algorithm	O
we	O
will	O
assume	O
that	O
the	O
loss	B
function	B
is	O
convex	B
and	O
that	O
it	O
is	O
either	O
lipschitz	O
or	O
smooth	O
the	O
main	O
property	O
of	O
the	O
tikhonov	B
regularization	B
that	O
we	O
rely	O
on	O
is	O
that	O
it	O
makes	O
the	O
objective	O
of	O
rlm	B
strongly	B
convex	B
as	O
defined	O
in	O
the	O
following	O
tikhonov	B
regularization	B
as	O
a	O
stabilizer	O
definition	O
convex	B
functions	O
a	O
function	B
f	O
is	O
convex	B
if	O
for	O
all	O
w	O
u	O
and	O
we	O
have	O
f	O
w	O
f	O
clearly	O
every	O
convex	B
function	B
is	O
convex	B
an	O
illustration	O
of	O
strong	O
convexity	O
is	O
given	O
in	O
the	O
following	O
figure	O
f	O
f	O
w	O
w	O
u	O
the	O
following	O
lemma	O
implies	O
that	O
the	O
objective	O
of	O
rlm	B
is	O
con	O
vex	O
in	O
addition	O
it	O
underscores	O
an	O
important	O
property	O
of	O
strong	O
convexity	O
lemma	O
the	O
function	B
f	O
is	O
convex	B
if	O
f	O
is	O
convex	B
and	O
g	O
is	O
convex	B
then	O
f	O
g	O
is	O
convex	B
if	O
f	O
is	O
convex	B
and	O
u	O
is	O
a	O
minimizer	O
of	O
f	O
then	O
for	O
any	O
w	O
f	O
f	O
proof	O
the	O
first	O
two	O
points	O
follow	O
directly	O
from	O
the	O
definition	O
to	O
prove	O
the	O
last	O
point	O
we	O
divide	O
the	O
definition	O
of	O
strong	O
convexity	O
by	O
and	O
rearrange	O
terms	O
to	O
get	O
that	O
f	O
u	O
f	O
f	O
f	O
taking	O
the	O
limit	O
we	O
obtain	O
that	O
the	O
right-hand	O
side	O
converges	O
to	O
f	O
f	O
on	O
the	O
other	O
hand	O
the	O
left-hand	O
side	O
becomes	O
the	O
derivative	O
of	O
the	O
function	B
g	O
f	O
u	O
at	O
since	O
u	O
is	O
a	O
minimizer	O
of	O
f	O
it	O
follows	O
that	O
is	O
a	O
minimizer	O
of	O
g	O
and	O
therefore	O
the	O
left-hand	O
side	O
of	O
the	O
preceding	O
goes	O
to	O
zero	O
in	O
the	O
limit	O
which	O
concludes	O
our	O
proof	O
we	O
now	O
turn	O
to	O
prove	O
that	O
rlm	B
is	O
stable	O
let	O
s	O
zm	O
be	O
a	O
training	B
set	B
let	O
be	O
an	O
additional	O
example	O
and	O
let	O
si	O
zi	O
zm	O
let	O
a	O
be	O
the	O
rlm	B
rule	O
namely	O
as	O
argmin	O
w	O
regularization	B
and	O
stability	B
denote	O
fsw	O
lsw	O
and	O
based	O
on	O
lemma	O
we	O
know	O
that	O
fs	O
is	O
convex	B
relying	O
on	O
part	O
of	O
the	O
lemma	O
it	O
follows	O
that	O
for	O
any	O
v	O
fsv	O
fsas	O
on	O
the	O
other	O
hand	O
for	O
any	O
v	O
and	O
u	O
and	O
for	O
all	O
i	O
we	O
have	O
fsv	O
fsu	O
lsv	O
lsiv	O
zi	O
zi	O
m	O
in	O
particular	O
choosing	O
v	O
asi	O
u	O
as	O
and	O
using	O
the	O
fact	O
that	O
v	O
minimizes	O
lsi	O
we	O
obtain	O
that	O
fsasi	O
fsas	O
zi	O
zi	O
m	O
m	O
m	O
combining	O
this	O
with	O
equation	O
we	O
obtain	O
that	O
zi	O
zi	O
m	O
m	O
the	O
two	O
subsections	O
that	O
follow	O
continue	O
the	O
stability	B
analysis	O
for	O
either	O
lipschitz	O
or	O
smooth	B
loss	B
functions	O
for	O
both	O
families	O
of	O
loss	B
functions	O
we	O
show	O
that	O
rlm	B
is	O
stable	O
and	O
therefore	O
it	O
does	O
not	O
overfit	O
lipschitz	B
loss	B
if	O
the	O
loss	B
function	B
zi	O
is	O
then	O
by	O
the	O
definition	O
of	O
lipschitzness	B
zi	O
zi	O
similarly	O
plugging	O
these	O
inequalities	O
into	O
equation	O
we	O
obtain	O
m	O
which	O
yields	O
m	O
plugging	O
the	O
preceding	O
back	O
into	O
equation	O
we	O
conclude	O
that	O
zi	O
zi	O
m	O
since	O
this	O
holds	O
for	O
any	O
s	O
i	O
we	O
immediately	O
obtain	O
tikhonov	B
regularization	B
as	O
a	O
stabilizer	O
corollary	O
assume	O
that	O
the	O
loss	B
function	B
is	O
convex	B
and	O
then	O
the	O
rlm	B
rule	O
with	O
the	O
regularizer	O
is	O
on-average-replace-one-stable	O
with	O
rate	O
m	O
it	O
follows	O
theorem	O
that	O
e	O
s	O
dm	O
lsas	O
m	O
smooth	O
and	O
nonnegative	O
loss	B
if	O
the	O
loss	B
is	O
and	O
nonnegative	O
then	O
it	O
is	O
also	O
self-bounded	O
section	O
we	O
further	O
assume	O
that	O
smoothness	B
assumption	O
we	O
have	O
that	O
f	O
f	O
m	O
or	O
in	O
other	O
words	O
that	O
by	O
the	O
zi	O
zi	O
zi	O
asi	O
using	O
the	O
cauchy-schwartz	O
inequality	O
and	O
equation	O
we	O
further	O
obtain	O
that	O
zi	O
zi	O
by	O
a	O
symmetric	O
argument	O
it	O
holds	O
that	O
plugging	O
these	O
inequalities	O
into	O
equation	O
and	O
rearranging	O
terms	O
we	O
obtain	O
that	O
zi	O
zi	O
m	O
m	O
combining	O
the	O
preceding	O
with	O
the	O
assumption	O
yields	O
regularization	B
and	O
stability	B
combining	O
the	O
preceding	O
with	O
equation	O
and	O
again	O
using	O
the	O
assumption	O
yield	O
zi	O
zi	O
zi	O
zi	O
zi	O
m	O
m	O
m	O
where	O
in	O
the	O
last	O
step	O
we	O
used	O
the	O
inequality	O
taking	O
expectation	O
with	O
respect	O
to	O
s	O
i	O
and	O
noting	O
that	O
zi	O
elsas	O
we	O
conclude	O
that	O
corollary	O
assume	O
that	O
the	O
loss	B
function	B
is	O
and	O
nonnegative	O
then	O
the	O
rlm	B
rule	O
with	O
the	O
regularizer	O
where	O
m	O
satisfies	O
zi	O
zi	O
elsas	O
note	O
that	O
if	O
for	O
all	O
z	O
we	O
have	O
z	O
c	O
for	O
some	O
scalar	O
c	O
then	O
for	O
m	O
every	O
s	O
lsas	O
lsas	O
c	O
hence	O
corollary	O
also	O
implies	O
that	O
zi	O
zi	O
c	O
m	O
controlling	O
the	O
fitting-stability	O
tradeoff	O
we	O
can	O
rewrite	O
the	O
expected	O
risk	B
of	O
a	O
learning	O
algorithm	O
as	O
e	O
e	O
s	O
s	O
e	O
lsas	O
s	O
the	O
first	O
term	O
reflects	O
how	O
well	O
as	O
fits	O
the	O
training	B
set	B
while	O
the	O
second	O
term	O
reflects	O
the	O
difference	O
between	O
the	O
true	O
and	O
empirical	O
risks	O
of	O
as	O
as	O
we	O
have	O
shown	O
in	O
theorem	O
the	O
second	O
term	O
is	O
equivalent	O
to	O
the	O
stability	B
of	O
a	O
since	O
our	O
goal	O
is	O
to	O
minimize	O
the	O
risk	B
of	O
the	O
algorithm	O
we	O
need	O
that	O
the	O
sum	O
of	O
both	O
terms	O
will	O
be	O
small	O
in	O
the	O
previous	O
section	O
we	O
have	O
bounded	O
the	O
stability	B
term	O
we	O
have	O
shown	O
that	O
the	O
stability	B
term	O
decreases	O
as	O
the	O
regularization	B
parameter	O
increases	O
on	O
the	O
other	O
hand	O
the	O
empirical	B
risk	B
increases	O
with	O
we	O
therefore	O
face	O
a	O
controlling	O
the	O
fitting-stability	O
tradeoff	O
tradeoff	O
between	O
fitting	O
and	O
overfitting	B
this	O
tradeoff	O
is	O
quite	O
similar	O
to	O
the	O
biascomplexity	O
tradeoff	O
we	O
discussed	O
previously	O
in	O
the	O
book	O
we	O
now	O
derive	O
bounds	O
on	O
the	O
empirical	B
risk	B
term	O
for	O
the	O
rlm	B
rule	O
recall	B
fix	O
some	O
that	O
the	O
rlm	B
rule	O
is	O
defined	O
as	O
as	O
argminw	O
arbitrary	O
vector	O
w	O
we	O
have	O
lsas	O
lsas	O
lsw	O
taking	O
expectation	O
of	O
both	O
sides	O
with	O
respect	O
to	O
s	O
and	O
noting	O
that	O
eslsw	O
ldw	O
we	O
obtain	O
that	O
ldw	O
e	O
s	O
plugging	O
this	O
into	O
equation	O
we	O
obtain	O
ldw	O
e	O
e	O
s	O
s	O
lsas	O
combining	O
the	O
preceding	O
with	O
corollary	O
we	O
conclude	O
corollary	O
assume	O
that	O
the	O
loss	B
function	B
is	O
convex	B
and	O
then	O
the	O
rlm	B
rule	O
with	O
the	O
regularization	B
function	B
satisfies	O
w	O
e	O
ldw	O
s	O
m	O
this	O
bound	O
is	O
often	O
called	O
an	O
oracle	B
inequality	I
if	O
we	O
think	O
of	O
w	O
as	O
a	O
hypothesis	B
with	O
low	O
risk	B
the	O
bound	O
tells	O
us	O
how	O
many	O
examples	O
are	O
needed	O
so	O
that	O
as	O
will	O
be	O
almost	O
as	O
good	O
as	O
w	O
had	O
we	O
known	O
the	O
norm	O
of	O
w	O
in	O
practice	O
however	O
we	O
usually	O
do	O
not	O
know	O
the	O
norm	O
of	O
w	O
we	O
therefore	O
usually	O
tune	O
on	O
the	O
basis	O
of	O
a	O
validation	B
set	B
as	O
described	O
in	O
chapter	O
we	O
can	O
also	O
easily	O
derive	O
a	O
pac-like	O
from	O
corollary	O
for	O
convex	B
lipschitz-bounded	O
learning	O
problems	O
corollary	O
let	O
z	O
be	O
a	O
convex-lipschitz-bounded	B
learning	I
problem	O
m	O
then	O
the	O
with	O
parameters	O
b	O
for	O
any	O
training	B
set	B
size	O
m	O
let	O
rlm	B
rule	O
with	O
the	O
regularization	B
function	B
satisfies	O
min	O
e	O
s	O
w	O
h	O
ldw	O
b	O
m	O
in	O
particular	O
for	O
every	O
if	O
m	O
esldas	O
minw	O
h	O
ldw	O
then	O
for	O
every	O
distribution	O
d	O
the	O
preceding	O
corollary	O
holds	O
for	O
lipschitz	B
loss	B
functions	O
if	O
instead	O
the	O
loss	B
function	B
is	O
smooth	O
and	O
nonnegative	O
then	O
we	O
can	O
combine	O
equation	O
with	O
corollary	O
to	O
get	O
again	O
the	O
bound	O
below	O
is	O
on	O
the	O
expected	O
risk	B
but	O
using	O
exercise	O
it	O
can	O
be	O
used	O
to	O
derive	O
an	O
agnostic	B
pac	B
learning	O
guarantee	O
regularization	B
and	O
stability	B
corollary	O
assume	O
that	O
the	O
loss	B
function	B
is	O
convex	B
and	O
nonnegative	O
then	O
the	O
rlm	B
rule	O
with	O
the	O
regularization	B
function	B
for	O
m	O
m	O
satisfies	O
the	O
following	O
for	O
all	O
w	O
e	O
s	O
e	O
s	O
m	O
for	O
example	O
if	O
we	O
choose	O
m	O
we	O
obtain	O
from	O
the	O
preceding	O
that	O
the	O
expected	O
true	O
risk	B
of	O
as	O
is	O
at	O
most	O
twice	O
the	O
expected	O
empirical	B
risk	B
of	O
as	O
furthermore	O
for	O
this	O
value	O
of	O
the	O
expected	O
empirical	B
risk	B
of	O
as	O
is	O
at	O
most	O
ldw	O
m	O
we	O
can	O
also	O
derive	O
a	O
learnability	O
guarantee	O
for	O
convex-smooth-bounded	O
learn	O
ing	O
problems	O
based	O
on	O
corollary	O
corollary	O
let	O
z	O
be	O
a	O
convex-smooth-bounded	B
learning	I
problem	O
with	O
parameters	O
b	O
assume	O
in	O
addition	O
that	O
z	O
for	O
all	O
z	O
z	O
for	O
any	O
and	O
set	B
then	O
for	O
every	O
distribution	O
d	O
let	O
m	O
min	O
e	O
s	O
w	O
h	O
ldw	O
summary	O
we	O
introduced	O
stability	B
and	O
showed	O
that	O
if	O
an	O
algorithm	O
is	O
stable	O
then	O
it	O
does	O
not	O
overfit	O
furthermore	O
for	O
convex-lipschitz-bounded	O
or	O
convex-smooth-bounded	O
problems	O
the	O
rlm	B
rule	O
with	O
tikhonov	B
regularization	B
leads	O
to	O
a	O
stable	O
learning	O
algorithm	O
we	O
discussed	O
how	O
the	O
regularization	B
parameter	O
controls	O
the	O
tradeoff	O
between	O
fitting	O
and	O
overfitting	B
finally	O
we	O
have	O
shown	O
that	O
all	O
learning	O
problems	O
that	O
are	O
from	O
the	O
families	O
of	O
convex-lipschitz-bounded	O
and	O
convex-smoothbounded	O
problems	O
are	O
learnable	O
using	O
the	O
rlm	B
rule	O
the	O
rlm	B
paradigm	O
is	O
the	O
basis	O
for	O
many	O
popular	O
learning	O
algorithms	O
including	O
ridge	B
regression	B
we	O
discussed	O
in	O
this	O
chapter	O
and	O
support	O
vector	O
machines	O
will	O
be	O
discussed	O
in	O
chapter	O
in	O
the	O
next	O
chapter	O
we	O
will	O
present	O
stochastic	O
gradient	B
descent	I
which	O
gives	O
us	O
a	O
very	O
practical	O
alternative	O
way	O
to	O
learn	O
convex-lipschitz-bounded	O
and	O
convexsmooth-bounded	O
problems	O
and	O
can	O
also	O
be	O
used	O
for	O
efficiently	O
implementing	O
the	O
rlm	B
rule	O
bibliographic	O
remarks	O
stability	B
is	O
widely	O
used	O
in	O
many	O
mathematical	O
contexts	O
for	O
example	O
the	O
necessity	O
of	O
stability	B
for	O
so-called	O
inverse	O
problems	O
to	O
be	O
well	O
posed	O
was	O
first	O
recognized	O
by	O
hadamard	O
the	O
idea	O
of	O
regularization	B
and	O
its	O
relation	O
to	O
stability	B
became	O
widely	O
known	O
through	O
the	O
works	O
of	O
tikhonov	B
and	O
phillips	O
exercises	O
in	O
the	O
context	O
of	O
modern	O
learning	O
theory	O
the	O
use	O
of	O
stability	B
can	O
be	O
traced	O
back	O
at	O
least	O
to	O
the	O
work	O
of	O
rogers	O
wagner	O
which	O
noted	O
that	O
the	O
sensitivity	B
of	O
a	O
learning	O
algorithm	O
with	O
regard	O
to	O
small	O
changes	O
in	O
the	O
sample	O
controls	O
the	O
variance	O
of	O
the	O
leave-one-out	O
estimate	O
the	O
authors	O
used	O
this	O
observation	O
to	O
obtain	O
generalization	B
bounds	I
for	O
the	O
k-nearest	O
neighbor	O
algorithm	O
chapter	O
these	O
results	O
were	O
later	O
extended	O
to	O
other	O
local	O
learning	O
algorithms	O
devroye	O
gy	O
orfi	O
lugosi	O
and	O
references	O
therein	O
in	O
addition	O
practical	O
methods	O
have	O
been	O
developed	O
to	O
introduce	O
stability	B
into	O
learning	O
algorithms	O
in	O
particular	O
the	O
bagging	O
technique	O
introduced	O
by	O
over	O
the	O
last	O
decade	O
stability	B
was	O
studied	O
as	O
a	O
generic	O
condition	O
for	O
learnability	O
see	O
ron	O
bousquet	O
elisseeff	O
kutin	O
niyogi	O
rakhlin	O
mukherjee	O
poggio	O
mukherjee	O
niyogi	O
poggio	O
rifkin	O
our	O
presentation	O
follows	O
the	O
work	O
of	O
shalev-shwartz	O
shamir	O
srebro	O
sridharan	O
who	O
showed	O
that	O
stability	B
is	O
sufficient	O
and	O
necessary	O
for	O
learning	O
they	O
have	O
also	O
shown	O
that	O
all	O
convex-lipschitz-bounded	B
learning	I
problems	O
are	O
learnable	O
using	O
rlm	B
even	O
though	O
for	O
some	O
convex-lipschitz-bounded	B
learning	I
problems	O
uniform	B
convergence	I
does	O
not	O
hold	O
in	O
a	O
strong	O
sense	O
exercises	O
from	O
bounded	O
expected	O
risk	B
to	O
agnostic	B
pac	B
learning	O
let	O
a	O
be	O
an	O
algorithm	O
that	O
guarantees	O
the	O
following	O
if	O
m	O
mh	O
then	O
for	O
every	O
distribution	O
d	O
it	O
holds	O
that	O
e	O
s	O
dm	O
min	O
h	O
h	O
ldh	O
show	O
that	O
for	O
every	O
if	O
m	O
mh	O
then	O
with	O
probability	O
of	O
at	O
least	O
it	O
holds	O
that	O
ldas	O
minh	O
h	O
ldh	O
hint	O
observe	O
that	O
the	O
random	O
variable	O
ldas	O
minh	O
h	O
ldh	O
is	O
nonnegative	O
and	O
rely	O
on	O
markov	O
s	O
inequality	O
for	O
every	O
let	O
mh	O
suggest	O
a	O
procedure	O
that	O
agnostic	B
pac	B
learns	O
the	O
problem	O
with	O
sample	B
complexity	I
of	O
mh	O
assuming	O
that	O
the	O
loss	B
function	B
is	O
bounded	O
by	O
hint	O
let	O
k	O
divide	O
the	O
data	O
into	O
k	O
chunks	O
where	O
each	O
of	O
the	O
first	O
k	O
chunks	O
is	O
of	O
size	O
examples	O
train	O
the	O
first	O
k	O
chunks	O
using	O
a	O
on	O
the	O
basis	O
of	O
the	O
previous	O
question	O
argue	O
that	O
the	O
probability	O
that	O
for	O
all	O
of	O
these	O
chunks	O
we	O
have	O
ldas	O
minh	O
h	O
ldh	O
is	O
at	O
most	O
k	O
finally	O
use	O
the	O
last	O
chunk	O
as	O
a	O
validation	B
set	B
learnability	O
without	O
uniform	B
convergence	I
let	O
b	O
be	O
the	O
unit	O
ball	O
of	O
regularization	B
and	O
stability	B
rd	O
let	O
h	O
b	O
let	O
z	O
b	O
and	O
let	O
z	O
h	O
r	O
be	O
defined	O
as	O
follows	O
ixi	O
this	O
problem	O
corresponds	O
to	O
an	O
unsupervised	B
learning	I
task	O
meaning	O
that	O
we	O
do	O
not	O
try	O
to	O
predict	O
the	O
label	B
of	O
x	O
instead	O
what	O
we	O
try	O
to	O
do	O
is	O
to	O
find	O
the	O
center	O
of	O
mass	O
of	O
the	O
distribution	O
over	O
b	O
however	O
there	O
is	O
a	O
twist	O
modeled	O
by	O
the	O
vectors	O
each	O
example	O
is	O
a	O
pair	O
where	O
x	O
is	O
the	O
instance	B
x	O
and	O
indicates	O
which	O
features	O
of	O
x	O
are	O
active	O
and	O
which	O
are	O
turned	O
off	O
a	O
hypothesis	B
is	O
a	O
vector	O
w	O
representing	O
the	O
center	O
of	O
mass	O
of	O
the	O
distribution	O
and	O
the	O
loss	B
function	B
is	O
the	O
squared	O
euclidean	O
distance	O
between	O
x	O
and	O
w	O
but	O
only	O
with	O
respect	O
to	O
the	O
active	O
elements	O
of	O
x	O
show	O
that	O
this	O
problem	O
is	O
learnable	O
using	O
the	O
rlm	B
rule	O
with	O
a	O
sample	B
complexity	I
that	O
does	O
not	O
depend	O
on	O
d	O
consider	O
a	O
distribution	O
d	O
over	O
z	O
as	O
follows	O
x	O
is	O
fixed	O
to	O
be	O
some	O
and	O
each	O
element	O
of	O
is	O
sampled	O
to	O
be	O
either	O
or	O
with	O
equal	O
probability	O
show	O
that	O
the	O
rate	O
of	O
uniform	B
convergence	I
of	O
this	O
problem	O
grows	O
with	O
d	O
hint	O
let	O
m	O
be	O
a	O
training	B
set	B
size	O
show	O
that	O
if	O
d	O
then	O
there	O
is	O
a	O
high	O
probability	O
of	O
sampling	O
a	O
set	B
of	O
examples	O
such	O
that	O
there	O
exists	O
some	O
j	O
for	O
which	O
j	O
for	O
all	O
the	O
examples	O
in	O
the	O
training	B
set	B
show	O
that	O
such	O
a	O
sample	O
cannot	O
be	O
conclude	O
that	O
the	O
sample	B
complexity	I
of	O
uniform	B
convergence	I
must	O
grow	O
with	O
logd	O
conclude	O
that	O
if	O
we	O
take	O
d	O
to	O
infinity	O
we	O
obtain	O
a	O
problem	O
that	O
is	O
learnable	O
but	O
for	O
which	O
the	O
uniform	B
convergence	I
property	O
does	O
not	O
hold	O
compare	O
to	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
stability	B
and	O
asymptotic	O
erm	B
are	O
sufficient	O
for	O
learnability	O
we	O
say	O
that	O
a	O
learning	O
rule	O
a	O
is	O
an	O
aerm	O
empirical	B
risk	B
minimizer	O
with	O
rate	O
if	O
for	O
every	O
distribution	O
d	O
it	O
holds	O
that	O
e	O
s	O
dm	O
lsas	O
min	O
h	O
h	O
lsh	O
we	O
say	O
that	O
a	O
learning	O
rule	O
a	O
learns	O
a	O
class	O
h	O
with	O
rate	O
if	O
for	O
every	O
distribution	O
d	O
it	O
holds	O
that	O
e	O
s	O
dm	O
prove	O
the	O
following	O
ldas	O
min	O
h	O
h	O
ldh	O
theorem	O
if	O
a	O
learning	O
algorithm	O
a	O
is	O
on-average-replace-one-stable	O
with	O
rate	O
and	O
is	O
an	O
aerm	O
with	O
rate	O
then	O
it	O
learns	O
h	O
with	O
rate	O
exercises	O
strong	O
convexity	O
with	O
respect	O
to	O
general	O
norms	O
throughout	O
the	O
section	O
we	O
used	O
the	O
norm	O
in	O
this	O
exercise	O
we	O
generalize	O
some	O
of	O
the	O
results	O
to	O
general	O
norms	O
let	O
be	O
some	O
arbitrary	O
norm	O
and	O
let	O
f	O
be	O
a	O
strongly	B
convex	B
function	B
with	O
respect	O
to	O
this	O
norm	O
definition	O
show	O
that	O
items	O
of	O
lemma	O
hold	O
for	O
every	O
norm	O
give	O
an	O
example	O
of	O
a	O
norm	O
for	O
which	O
item	O
of	O
lemma	O
does	O
not	O
let	O
rw	O
be	O
a	O
function	B
that	O
is	O
convex	B
with	O
respect	O
to	O
some	O
hold	O
norm	O
let	O
a	O
be	O
an	O
rlm	B
rule	O
with	O
respect	O
to	O
r	O
namely	O
as	O
argmin	O
rw	O
w	O
assume	O
that	O
for	O
every	O
z	O
the	O
loss	B
function	B
z	O
is	O
with	O
respect	O
to	O
the	O
same	O
norm	O
namely	O
z	O
w	O
v	O
z	O
z	O
prove	O
that	O
a	O
is	O
on-average-replace-one-stable	O
with	O
rate	O
m	O
let	O
q	O
and	O
consider	O
the	O
it	O
can	O
be	O
shown	O
for	O
example	O
shalev-shwartz	O
that	O
the	O
function	B
rw	O
q	O
is	O
convex	B
with	O
respect	O
to	O
show	O
that	O
if	O
q	O
logd	O
rw	O
is	O
convex	B
with	O
respect	O
to	O
the	O
norm	O
over	O
rd	O
logd	O
then	O
logd	O
stochastic	O
gradient	B
descent	I
recall	B
that	O
the	O
goal	O
of	O
learning	O
is	O
to	O
minimize	O
the	O
risk	B
function	B
ldh	O
ez	O
z	O
we	O
cannot	O
directly	O
minimize	O
the	O
risk	B
function	B
since	O
it	O
depends	O
on	O
the	O
unknown	O
distribution	O
d	O
so	O
far	O
in	O
the	O
book	O
we	O
have	O
discussed	O
learning	O
methods	O
that	O
depend	O
on	O
the	O
empirical	B
risk	B
that	O
is	O
we	O
first	O
sample	O
a	O
training	B
set	B
s	O
and	O
define	O
the	O
empirical	B
risk	B
function	B
lsh	O
then	O
the	O
learner	O
picks	O
a	O
hypothesis	B
based	O
on	O
the	O
value	O
of	O
lsh	O
for	O
example	O
the	O
erm	B
rule	O
tells	O
us	O
to	O
pick	O
the	O
hypothesis	B
that	O
minimizes	O
lsh	O
over	O
the	O
hypothesis	B
class	I
h	O
or	O
in	O
the	O
previous	O
chapter	O
we	O
discussed	O
regularized	O
risk	B
minimization	O
in	O
which	O
we	O
pick	O
a	O
hypothesis	B
that	O
jointly	O
minimizes	O
lsh	O
and	O
a	O
regularization	B
function	B
over	O
h	O
in	O
this	O
chapter	O
we	O
describe	O
and	O
analyze	O
a	O
rather	O
different	O
learning	O
approach	O
which	O
is	O
called	O
stochastic	O
gradient	B
descent	I
as	O
in	O
chapter	O
we	O
will	O
focus	O
on	O
the	O
important	O
family	O
of	O
convex	B
learning	O
problems	O
and	O
following	O
the	O
notation	O
in	O
that	O
chapter	O
we	O
will	O
refer	O
to	O
hypotheses	O
as	O
vectors	O
w	O
that	O
come	O
from	O
a	O
convex	B
hypothesis	B
class	I
h	O
in	O
sgd	B
we	O
try	O
to	O
minimize	O
the	O
risk	B
function	B
ldw	O
directly	O
using	O
a	O
gradient	B
descent	I
procedure	O
gradient	B
descent	I
is	O
an	O
iterative	O
optimization	O
procedure	O
in	O
which	O
at	O
each	O
step	O
we	O
improve	O
the	O
solution	O
by	O
taking	O
a	O
step	O
along	O
the	O
negative	O
of	O
the	O
gradient	B
of	O
the	O
function	B
to	O
be	O
minimized	O
at	O
the	O
current	O
point	O
of	O
course	O
in	O
our	O
case	O
we	O
are	O
minimizing	O
the	O
risk	B
function	B
and	O
since	O
we	O
do	O
not	O
know	O
d	O
we	O
also	O
do	O
not	O
know	O
the	O
gradient	B
of	O
ldw	O
sgd	B
circumvents	O
this	O
problem	O
by	O
allowing	O
the	O
optimization	O
procedure	O
to	O
take	O
a	O
step	O
along	O
a	O
random	O
direction	O
as	O
long	O
as	O
the	O
expected	O
value	O
of	O
the	O
direction	O
is	O
the	O
negative	O
of	O
the	O
gradient	B
and	O
as	O
we	O
shall	O
see	O
finding	O
a	O
random	O
direction	O
whose	O
expected	O
value	O
corresponds	O
to	O
the	O
gradient	B
is	O
rather	O
simple	O
even	O
though	O
we	O
do	O
not	O
know	O
the	O
underlying	O
distribution	O
d	O
the	O
advantage	O
of	O
sgd	B
in	O
the	O
context	O
of	O
convex	B
learning	O
problems	O
over	O
the	O
regularized	O
risk	B
minimization	O
learning	O
rule	O
is	O
that	O
sgd	B
is	O
an	O
efficient	O
algorithm	O
that	O
can	O
be	O
implemented	O
in	O
a	O
few	O
lines	O
of	O
code	O
yet	O
still	O
enjoys	O
the	O
same	O
sample	B
complexity	I
as	O
the	O
regularized	O
risk	B
minimization	O
rule	O
the	O
simplicity	O
of	O
sgd	B
also	O
allows	O
us	O
to	O
use	O
it	O
in	O
situations	O
when	O
it	O
is	O
not	O
possible	O
to	O
apply	O
methods	O
that	O
are	O
based	O
on	O
the	O
empirical	B
risk	B
but	O
this	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
we	O
start	O
this	O
chapter	O
with	O
the	O
basic	O
gradient	B
descent	I
algorithm	O
and	O
analyze	O
its	O
convergence	O
rate	O
for	O
convex-lipschitz	O
functions	O
next	O
we	O
introduce	O
the	O
notion	O
of	O
subgradient	O
and	O
show	O
that	O
gradient	B
descent	I
can	O
be	O
applied	O
for	O
nondifferentiable	O
functions	O
as	O
well	O
the	O
core	O
of	O
this	O
chapter	O
is	O
section	O
in	O
which	O
we	O
describe	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
gradient	B
descent	I
the	O
stochastic	O
gradient	B
descent	I
algorithm	O
along	O
with	O
several	O
useful	O
variants	O
we	O
show	O
that	O
sgd	B
enjoys	O
an	O
expected	O
convergence	O
rate	O
similar	O
to	O
the	O
rate	O
of	O
gradient	B
descent	I
finally	O
we	O
turn	O
to	O
the	O
applicability	O
of	O
sgd	B
to	O
learning	O
problems	O
gradient	B
descent	I
before	O
we	O
describe	O
the	O
stochastic	O
gradient	B
descent	I
method	O
we	O
would	O
like	O
to	O
describe	O
the	O
standard	O
gradient	B
descent	I
approach	O
for	O
minimizing	O
a	O
differentiable	O
convex	B
function	B
f	O
the	O
gradient	B
of	O
a	O
differentiable	O
function	B
f	O
rd	O
r	O
at	O
w	O
denoted	O
f	O
is	O
the	O
vector	O
of	O
partial	O
derivatives	O
of	O
f	O
namely	O
f	O
gradient	B
descent	I
is	O
an	O
iterative	O
algorithm	O
we	O
start	O
with	O
an	O
initial	O
value	O
of	O
w	O
then	O
at	O
each	O
iteration	O
we	O
take	O
a	O
step	O
in	O
the	O
direction	O
of	O
the	O
negative	O
of	O
the	O
gradient	B
at	O
the	O
current	O
point	O
that	O
is	O
the	O
update	O
step	O
is	O
f	O
f	O
wd	O
wt	O
f	O
where	O
is	O
a	O
parameter	O
to	O
be	O
discussed	O
later	O
intuitively	O
since	O
the	O
gradient	B
points	O
in	O
the	O
direction	O
of	O
the	O
greatest	O
rate	O
of	O
increase	O
of	O
f	O
around	O
wt	O
the	O
algorithm	O
makes	O
a	O
small	O
step	O
in	O
the	O
opposite	O
direction	O
thus	O
decreasing	O
the	O
value	O
of	O
the	O
function	B
eventually	O
after	O
t	O
iterations	O
the	O
algorithm	O
outputs	O
the	O
averaged	O
vector	O
w	O
wt	O
the	O
output	O
could	O
also	O
be	O
the	O
last	O
vector	O
t	O
wt	O
or	O
the	O
best	O
performing	O
vector	O
argmint	O
f	O
but	O
taking	O
the	O
average	O
turns	O
out	O
to	O
be	O
rather	O
useful	O
especially	O
when	O
we	O
generalize	O
gradient	B
descent	I
to	O
nondifferentiable	O
functions	O
and	O
to	O
the	O
stochastic	O
case	O
another	O
way	O
to	O
motivate	O
gradient	B
descent	I
is	O
by	O
relying	O
on	O
taylor	O
approximation	O
the	O
gradient	B
of	O
f	O
at	O
w	O
yields	O
the	O
first	O
order	O
taylor	O
approximation	O
of	O
f	O
around	O
w	O
by	O
f	O
f	O
w	O
f	O
when	O
f	O
is	O
convex	B
this	O
approximation	O
lower	O
bounds	O
f	O
that	O
is	O
f	O
f	O
w	O
f	O
therefore	O
for	O
w	O
close	O
to	O
wt	O
we	O
have	O
that	O
f	O
f	O
wt	O
f	O
hence	O
we	O
can	O
minimize	O
the	O
approximation	O
of	O
f	O
however	O
the	O
approximation	O
might	O
become	O
loose	O
for	O
w	O
which	O
is	O
far	O
away	O
from	O
wt	O
therefore	O
we	O
would	O
like	O
to	O
minimize	O
jointly	O
the	O
distance	O
between	O
w	O
and	O
wt	O
and	O
the	O
approximation	O
of	O
f	O
around	O
wt	O
if	O
the	O
parameter	O
controls	O
the	O
tradeoff	O
between	O
the	O
two	O
terms	O
we	O
obtain	O
the	O
update	O
rule	O
argmin	O
w	O
f	O
wt	O
f	O
solving	O
the	O
preceding	O
by	O
taking	O
the	O
derivative	O
with	O
respect	O
to	O
w	O
and	O
comparing	O
it	O
to	O
zero	O
yields	O
the	O
same	O
update	O
rule	O
as	O
in	O
equation	O
stochastic	O
gradient	B
descent	I
figure	O
an	O
illustration	O
of	O
the	O
gradient	B
descent	I
algorithm	O
the	O
function	B
to	O
be	O
minimized	O
is	O
analysis	O
of	O
gd	O
for	O
convex-lipschitz	O
functions	O
to	O
analyze	O
the	O
convergence	O
rate	O
of	O
the	O
gd	O
algorithm	O
we	O
limit	O
ourselves	O
to	O
the	O
case	O
of	O
convex-lipschitz	O
functions	O
we	O
have	O
seen	O
many	O
problems	O
lend	O
themselves	O
easily	O
to	O
this	O
setting	O
let	O
be	O
any	O
vector	O
and	O
let	O
b	O
be	O
an	O
upper	O
bound	O
on	O
it	O
is	O
convenient	O
to	O
think	O
of	O
as	O
the	O
minimizer	O
of	O
f	O
but	O
the	O
analysis	O
that	O
follows	O
holds	O
for	O
every	O
we	O
would	O
like	O
to	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
suboptimality	O
of	O
our	O
solution	O
wt	O
from	O
the	O
with	O
respect	O
to	O
namely	O
f	O
w	O
f	O
where	O
w	O
definition	O
of	O
w	O
and	O
using	O
jensen	O
s	O
inequality	O
we	O
have	O
that	O
t	O
f	O
t	O
t	O
t	O
f	O
w	O
f	O
f	O
wt	O
f	O
f	O
f	O
f	O
for	O
every	O
t	O
because	O
of	O
the	O
convexity	O
of	O
f	O
we	O
have	O
that	O
f	O
f	O
f	O
combining	O
the	O
preceding	O
we	O
obtain	O
f	O
w	O
f	O
t	O
f	O
to	O
bound	O
the	O
right-hand	O
side	O
we	O
rely	O
on	O
the	O
following	O
lemma	O
gradient	B
descent	I
lemma	O
let	O
vt	O
be	O
an	O
arbitrary	O
sequence	O
of	O
vectors	O
any	O
algorithm	O
with	O
an	O
initialization	O
and	O
an	O
update	O
rule	O
of	O
the	O
form	O
satisfies	O
wt	O
vt	O
t	O
then	O
for	O
every	O
with	O
b	O
we	O
have	O
in	O
particular	O
for	O
every	O
b	O
if	O
for	O
all	O
t	O
we	O
have	O
that	O
and	O
if	O
we	O
set	B
t	O
b	O
t	O
proof	O
using	O
algebraic	O
manipulations	O
the	O
square	O
we	O
obtain	O
where	O
the	O
last	O
equality	O
follows	O
from	O
the	O
definition	O
of	O
the	O
update	O
rule	O
summing	O
the	O
equality	O
over	O
t	O
we	O
have	O
the	O
first	O
sum	O
on	O
the	O
right-hand	O
side	O
is	O
a	O
telescopic	O
sum	O
that	O
collapses	O
to	O
plugging	O
this	O
in	O
equation	O
we	O
have	O
where	O
the	O
last	O
equality	O
is	O
due	O
to	O
the	O
definition	O
this	O
proves	O
the	O
first	O
part	O
of	O
the	O
lemma	O
the	O
second	O
part	O
follows	O
by	O
upper	O
bounding	O
by	O
b	O
by	O
dividing	O
by	O
t	O
and	O
plugging	O
in	O
the	O
value	O
of	O
stochastic	O
gradient	B
descent	I
lemma	O
applies	O
to	O
the	O
gd	O
algorithm	O
with	O
vt	O
f	O
as	O
we	O
will	O
show	O
later	O
in	O
lemma	O
if	O
f	O
is	O
then	O
f	O
we	O
therefore	O
satisfy	O
the	O
lemma	O
s	O
conditions	O
and	O
achieve	O
the	O
following	O
corollary	O
corollary	O
let	O
f	O
be	O
a	O
convex	B
function	B
and	O
let	O
b	O
f	O
if	O
we	O
run	O
the	O
gd	O
algorithm	O
on	O
f	O
for	O
t	O
steps	O
with	O
vector	O
w	O
satisfies	O
t	O
then	O
the	O
output	O
f	O
w	O
f	O
b	O
t	O
furthermore	O
for	O
every	O
to	O
achieve	O
f	O
w	O
f	O
it	O
suffices	O
to	O
run	O
the	O
gd	O
algorithm	O
for	O
a	O
number	O
of	O
iterations	O
that	O
satisfies	O
t	O
subgradients	O
the	O
gd	O
algorithm	O
requires	O
that	O
the	O
function	B
f	O
be	O
differentiable	O
we	O
now	O
generalize	O
the	O
discussion	O
beyond	O
differentiable	O
functions	O
we	O
will	O
show	O
that	O
the	O
gd	O
algorithm	O
can	O
be	O
applied	O
to	O
nondifferentiable	O
functions	O
by	O
using	O
a	O
so-called	O
subgradient	O
of	O
f	O
at	O
wt	O
instead	O
of	O
the	O
gradient	B
to	O
motivate	O
the	O
definition	O
of	O
subgradients	O
recall	B
that	O
for	O
a	O
convex	B
function	B
f	O
the	O
gradient	B
at	O
w	O
defines	O
the	O
slope	O
of	O
a	O
tangent	O
that	O
lies	O
below	O
f	O
that	O
is	O
u	O
f	O
f	O
w	O
f	O
an	O
illustration	O
is	O
given	O
on	O
the	O
left-hand	O
side	O
of	O
figure	O
the	O
existence	O
of	O
a	O
tangent	O
that	O
lies	O
below	O
f	O
is	O
an	O
important	O
property	O
of	O
convex	B
functions	O
which	O
is	O
in	O
fact	O
an	O
alternative	O
characterization	O
of	O
convexity	O
lemma	O
let	O
s	O
be	O
an	O
open	O
convex	B
set	B
a	O
function	B
f	O
s	O
r	O
is	O
convex	B
iff	O
for	O
every	O
w	O
s	O
there	O
exists	O
v	O
such	O
that	O
u	O
s	O
f	O
f	O
w	O
the	O
proof	O
of	O
this	O
lemma	O
can	O
be	O
found	O
in	O
many	O
convex	B
analysis	O
textbooks	O
lewis	O
the	O
preceding	O
inequality	O
leads	O
us	O
to	O
the	O
definition	O
of	O
subgradients	O
definition	O
a	O
vector	O
v	O
that	O
satisfies	O
equation	O
is	O
called	O
a	O
subgradient	O
of	O
f	O
at	O
w	O
the	O
set	B
of	O
subgradients	O
of	O
f	O
at	O
w	O
is	O
called	O
the	O
differential	B
set	B
and	O
denoted	O
f	O
an	O
illustration	O
of	O
subgradients	O
is	O
given	O
on	O
the	O
right-hand	O
side	O
of	O
figure	O
for	O
scalar	O
functions	O
a	O
subgradient	O
of	O
a	O
convex	B
function	B
f	O
at	O
w	O
is	O
a	O
slope	O
of	O
a	O
line	O
that	O
touches	O
f	O
at	O
w	O
and	O
is	O
not	O
above	O
f	O
elsewhere	O
subgradients	O
f	O
w	O
f	O
w	O
f	O
w	O
f	O
w	O
u	O
figure	O
left	O
the	O
right-hand	O
side	O
of	O
equation	O
is	O
the	O
tangent	O
of	O
f	O
at	O
w	O
for	O
a	O
convex	B
function	B
the	O
tangent	O
lower	O
bounds	O
f	O
right	O
illustration	O
of	O
several	O
subgradients	O
of	O
a	O
nondifferentiable	O
convex	B
function	B
calculating	O
subgradients	O
how	O
do	O
we	O
construct	O
subgradients	O
of	O
a	O
given	O
convex	B
function	B
if	O
a	O
function	B
is	O
differentiable	O
at	O
a	O
point	O
w	O
then	O
the	O
differential	B
set	B
is	O
trivial	O
as	O
the	O
following	O
claim	O
shows	O
claim	O
the	O
gradient	B
of	O
f	O
at	O
w	O
f	O
if	O
f	O
is	O
differentiable	O
at	O
w	O
then	O
f	O
contains	O
a	O
single	O
element	O
example	O
differential	B
set	B
of	O
the	O
absolute	O
function	B
consider	O
the	O
absolute	O
value	O
function	B
f	O
using	O
claim	O
we	O
can	O
easily	O
construct	O
the	O
differential	B
set	B
for	O
the	O
differentiable	O
parts	O
of	O
f	O
and	O
the	O
only	O
point	O
that	O
requires	O
special	O
attention	O
is	O
at	O
that	O
point	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
subdifferential	O
is	O
the	O
set	B
of	O
all	O
numbers	O
between	O
and	O
hence	O
f	O
if	O
x	O
if	O
x	O
if	O
x	O
for	O
many	O
practical	O
uses	O
we	O
do	O
not	O
need	O
to	O
calculate	O
the	O
whole	O
set	B
of	O
subgradients	O
at	O
a	O
given	O
point	O
as	O
one	O
member	O
of	O
this	O
set	B
would	O
suffice	O
the	O
following	O
claim	O
shows	O
how	O
to	O
construct	O
a	O
sub-gradient	B
for	O
pointwise	O
maximum	O
functions	O
claim	O
let	O
gw	O
maxi	O
giw	O
for	O
r	O
convex	B
differentiable	O
functions	O
gr	O
given	O
some	O
w	O
let	O
j	O
argmaxi	O
giw	O
then	O
gjw	O
gw	O
proof	O
since	O
gj	O
is	O
convex	B
we	O
have	O
that	O
for	O
all	O
u	O
gju	O
gjw	O
w	O
since	O
gw	O
gjw	O
and	O
gu	O
gju	O
we	O
obtain	O
that	O
gu	O
gw	O
w	O
which	O
concludes	O
our	O
proof	O
stochastic	O
gradient	B
descent	I
example	O
subgradient	O
of	O
the	O
hinge	B
loss	B
recall	B
the	O
hinge	B
loss	B
function	B
from	O
section	O
f	O
for	O
some	O
vector	O
x	O
and	O
scalar	O
y	O
to	O
calculate	O
a	O
subgradient	O
of	O
the	O
hinge	B
loss	B
at	O
some	O
w	O
we	O
rely	O
on	O
the	O
preceding	O
claim	O
and	O
obtain	O
that	O
the	O
vector	O
v	O
defined	O
in	O
the	O
following	O
is	O
a	O
subgradient	O
of	O
the	O
hinge	B
loss	B
at	O
w	O
v	O
if	O
yx	O
if	O
subgradients	O
of	O
lipschitz	O
functions	O
recall	B
that	O
a	O
function	B
f	O
a	O
r	O
is	O
if	O
for	O
all	O
u	O
v	O
a	O
f	O
the	O
following	O
lemma	O
gives	O
an	O
equivalent	O
definition	O
using	O
norms	O
of	O
subgradients	O
lemma	O
let	O
a	O
be	O
a	O
convex	B
open	O
set	B
and	O
let	O
f	O
a	O
r	O
be	O
a	O
convex	B
function	B
then	O
f	O
is	O
over	O
a	O
iff	O
for	O
all	O
w	O
a	O
and	O
v	O
f	O
we	O
have	O
that	O
proof	O
assume	O
that	O
for	O
all	O
v	O
f	O
we	O
have	O
that	O
since	O
v	O
f	O
we	O
have	O
f	O
f	O
w	O
bounding	O
the	O
right-hand	O
side	O
using	O
cauchy-schwartz	O
inequality	O
we	O
obtain	O
f	O
f	O
w	O
an	O
analogous	O
argument	O
can	O
show	O
that	O
f	O
f	O
hence	O
f	O
is	O
now	O
assume	O
that	O
f	O
is	O
choose	O
some	O
w	O
a	O
v	O
f	O
since	O
a	O
is	O
open	O
there	O
exists	O
such	O
that	O
u	O
w	O
belongs	O
to	O
a	O
therefore	O
w	O
and	O
from	O
the	O
definition	O
of	O
the	O
subgradient	O
f	O
f	O
u	O
on	O
the	O
other	O
hand	O
from	O
the	O
lipschitzness	B
of	O
f	O
we	O
have	O
f	O
f	O
combining	O
the	O
two	O
inequalities	O
we	O
conclude	O
that	O
subgradient	O
descent	O
the	O
gradient	B
descent	I
algorithm	O
can	O
be	O
generalized	O
to	O
nondifferentiable	O
functions	O
by	O
using	O
a	O
subgradient	O
of	O
f	O
at	O
wt	O
instead	O
of	O
the	O
gradient	B
the	O
analysis	O
of	O
the	O
convergence	O
rate	O
remains	O
unchanged	O
simply	O
note	O
that	O
equation	O
is	O
true	O
for	O
subgradients	O
as	O
well	O
stochastic	O
gradient	B
descent	I
figure	O
an	O
illustration	O
of	O
the	O
gradient	B
descent	I
algorithm	O
and	O
the	O
stochastic	O
gradient	B
descent	I
algorithm	O
the	O
function	B
to	O
be	O
minimized	O
is	O
for	O
the	O
stochastic	O
case	O
the	O
black	O
line	O
depicts	O
the	O
averaged	O
value	O
of	O
w	O
stochastic	O
gradient	B
descent	I
in	O
stochastic	O
gradient	B
descent	I
we	O
do	O
not	O
require	O
the	O
update	O
direction	O
to	O
be	O
based	O
exactly	O
on	O
the	O
gradient	B
instead	O
we	O
allow	O
the	O
direction	O
to	O
be	O
a	O
random	O
vector	O
and	O
only	O
require	O
that	O
its	O
expected	O
value	O
at	O
each	O
iteration	O
will	O
equal	O
the	O
gradient	B
direction	O
or	O
more	O
generally	O
we	O
require	O
that	O
the	O
expected	O
value	O
of	O
the	O
random	O
vector	O
will	O
be	O
a	O
subgradient	O
of	O
the	O
function	B
at	O
the	O
current	O
vector	O
stochastic	O
gradient	B
descent	I
for	O
minimizing	O
f	O
parameters	O
scalar	O
integer	O
t	O
initialize	O
for	O
t	O
t	O
choose	O
vt	O
at	O
random	O
from	O
a	O
distribution	O
such	O
that	O
evt	O
wt	O
f	O
update	O
wt	O
vt	O
output	O
w	O
t	O
wt	O
an	O
illustration	O
of	O
stochastic	O
gradient	B
descent	I
versus	O
gradient	B
descent	I
is	O
given	O
in	O
figure	O
as	O
we	O
will	O
see	O
in	O
section	O
in	O
the	O
context	O
of	O
learning	O
problems	O
it	O
is	O
easy	O
to	O
find	O
a	O
random	O
vector	O
whose	O
expectation	O
is	O
a	O
subgradient	O
of	O
the	O
risk	B
function	B
analysis	O
of	O
sgd	B
for	O
convex-lipschitz-bounded	O
functions	O
recall	B
the	O
bound	O
we	O
achieved	O
for	O
the	O
gd	O
algorithm	O
in	O
corollary	O
for	O
the	O
stochastic	O
case	O
in	O
which	O
only	O
the	O
expectation	O
of	O
vt	O
is	O
in	O
f	O
we	O
cannot	O
directly	O
apply	O
equation	O
however	O
since	O
the	O
expected	O
value	O
of	O
vt	O
is	O
a	O
stochastic	O
gradient	B
descent	I
subgradient	O
of	O
f	O
at	O
wt	O
we	O
can	O
still	O
derive	O
a	O
similar	O
bound	O
on	O
the	O
expected	O
output	O
of	O
stochastic	O
gradient	B
descent	I
this	O
is	O
formalized	O
in	O
the	O
following	O
theorem	O
theorem	O
let	O
b	O
let	O
f	O
be	O
a	O
convex	B
function	B
and	O
let	O
b	O
f	O
assume	O
that	O
sgd	B
is	O
run	O
for	O
t	O
iterations	O
with	O
all	O
t	O
with	O
probability	O
then	O
t	O
assume	O
also	O
that	O
for	O
e	O
w	O
f	O
b	O
t	O
therefore	O
for	O
any	O
to	O
achieve	O
ef	O
w	O
f	O
it	O
suffices	O
to	O
run	O
the	O
sgd	B
algorithm	O
for	O
a	O
number	O
of	O
iterations	O
that	O
satisfies	O
t	O
proof	O
let	O
us	O
introduce	O
the	O
notation	O
to	O
denote	O
the	O
sequence	O
vt	O
taking	O
expectation	O
of	O
equation	O
we	O
obtain	O
e	O
w	O
f	O
e	O
t	O
f	O
since	O
lemma	O
holds	O
for	O
any	O
sequence	O
it	O
applies	O
to	O
sgd	B
as	O
well	O
by	O
taking	O
expectation	O
of	O
the	O
bound	O
in	O
the	O
lemma	O
we	O
have	O
e	O
t	O
b	O
t	O
it	O
is	O
left	O
to	O
show	O
that	O
e	O
t	O
t	O
e	O
f	O
which	O
we	O
will	O
hereby	O
prove	O
using	O
the	O
linearity	O
of	O
the	O
expectation	O
we	O
have	O
e	O
t	O
t	O
e	O
next	O
we	O
recall	B
the	O
law	O
of	O
total	O
expectation	O
for	O
every	O
two	O
random	O
variables	O
and	O
a	O
function	B
g	O
e	O
e	O
e	O
setting	O
and	O
we	O
get	O
that	O
e	O
e	O
e	O
e	O
once	O
we	O
know	O
the	O
value	O
of	O
wt	O
is	O
not	O
random	O
any	O
more	O
and	O
therefore	O
e	O
e	O
e	O
e	O
vt	O
variants	O
since	O
wt	O
only	O
depends	O
on	O
and	O
sgd	B
requires	O
that	O
evtvt	O
wt	O
f	O
we	O
obtain	O
that	O
evtvt	O
f	O
thus	O
e	O
f	O
e	O
t	O
e	O
t	O
t	O
vt	O
overall	O
we	O
have	O
shown	O
that	O
e	O
e	O
e	O
f	O
f	O
summing	O
over	O
t	O
dividing	O
by	O
t	O
and	O
using	O
the	O
linearity	O
of	O
expectation	O
we	O
get	O
that	O
equation	O
holds	O
which	O
concludes	O
our	O
proof	O
variants	O
in	O
this	O
section	O
we	O
describe	O
several	O
variants	O
of	O
stochastic	O
gradient	B
descent	I
adding	O
a	O
projection	B
step	O
in	O
the	O
previous	O
analyses	O
of	O
the	O
gd	O
and	O
sgd	B
algorithms	O
we	O
required	O
that	O
the	O
norm	O
of	O
will	O
be	O
at	O
most	O
b	O
which	O
is	O
equivalent	O
to	O
requiring	O
that	O
is	O
in	O
the	O
set	B
h	O
b	O
in	O
terms	O
of	O
learning	O
this	O
means	O
restricting	O
ourselves	O
to	O
a	O
b-bounded	O
hypothesis	B
class	I
yet	O
any	O
step	O
we	O
take	O
in	O
the	O
opposite	O
direction	O
of	O
the	O
gradient	B
its	O
expected	O
direction	O
might	O
result	O
in	O
stepping	O
out	O
of	O
this	O
bound	O
and	O
there	O
is	O
even	O
no	O
guarantee	O
that	O
w	O
satisfies	O
it	O
we	O
show	O
in	O
the	O
following	O
how	O
to	O
overcome	O
this	O
problem	O
while	O
maintaining	O
the	O
same	O
convergence	O
rate	O
the	O
basic	O
idea	O
is	O
to	O
add	O
a	O
projection	B
step	O
namely	O
we	O
will	O
now	O
have	O
a	O
two-step	O
update	O
rule	O
where	O
we	O
first	O
subtract	O
a	O
subgradient	O
from	O
the	O
current	O
value	O
of	O
w	O
and	O
then	O
project	O
the	O
resulting	O
vector	O
onto	O
h	O
formally	O
wt	O
vt	O
wt	O
argminw	O
h	O
wt	O
the	O
projection	B
step	O
replaces	O
the	O
current	O
value	O
of	O
w	O
by	O
the	O
vector	O
in	O
h	O
closest	O
to	O
it	O
clearly	O
the	O
projection	B
step	O
guarantees	O
that	O
wt	O
h	O
for	O
all	O
t	O
since	O
h	O
is	O
convex	B
this	O
also	O
implies	O
that	O
w	O
h	O
as	O
required	O
we	O
next	O
show	O
that	O
the	O
analysis	O
of	O
sgd	B
with	O
projections	O
remains	O
the	O
same	O
this	O
is	O
based	O
on	O
the	O
following	O
lemma	O
lemma	O
lemma	O
let	O
h	O
be	O
a	O
closed	O
convex	B
set	B
and	O
let	O
v	O
be	O
the	O
projection	B
of	O
w	O
onto	O
h	O
namely	O
v	O
argmin	O
x	O
h	O
stochastic	O
gradient	B
descent	I
then	O
for	O
every	O
u	O
h	O
proof	O
by	O
the	O
convexity	O
of	O
h	O
for	O
every	O
we	O
have	O
that	O
v	O
v	O
h	O
therefore	O
from	O
the	O
optimality	O
of	O
v	O
we	O
obtain	O
v	O
w	O
u	O
rearranging	O
we	O
obtain	O
w	O
u	O
taking	O
the	O
limit	O
we	O
get	O
that	O
w	O
u	O
therefore	O
v	O
v	O
w	O
u	O
equipped	O
with	O
the	O
preceding	O
lemma	O
we	O
can	O
easily	O
adapt	O
the	O
analysis	O
of	O
sgd	B
to	O
the	O
case	O
in	O
which	O
we	O
add	O
projection	B
steps	O
on	O
a	O
closed	O
and	O
convex	B
set	B
simply	O
note	O
that	O
for	O
every	O
t	O
therefore	O
lemma	O
holds	O
when	O
we	O
add	O
projection	B
steps	O
and	O
hence	O
the	O
rest	O
of	O
the	O
analysis	O
follows	O
directly	O
variable	O
step	O
size	O
another	O
variant	O
of	O
sgd	B
is	O
decreasing	O
the	O
step	O
size	O
as	O
a	O
function	B
of	O
t	O
that	O
is	O
rather	O
than	O
updating	O
with	O
a	O
constant	O
we	O
use	O
t	O
for	O
instance	B
we	O
can	O
set	B
t	O
b	O
and	O
achieve	O
a	O
bound	O
similar	O
to	O
theorem	O
the	O
idea	O
is	O
that	O
when	O
we	O
are	O
closer	O
to	O
the	O
minimum	O
of	O
the	O
function	B
we	O
take	O
our	O
steps	O
more	O
carefully	O
so	O
as	O
not	O
to	O
overshoot	O
the	O
minimum	O
t	O
variants	O
other	O
averaging	O
techniques	O
we	O
have	O
set	B
the	O
output	O
vector	O
to	O
be	O
w	O
wt	O
there	O
are	O
alternative	O
approaches	O
such	O
as	O
outputting	O
wt	O
for	O
some	O
random	O
t	O
or	O
outputting	O
the	O
t	O
average	O
of	O
wt	O
over	O
the	O
last	O
t	O
iterations	O
for	O
some	O
one	O
can	O
also	O
take	O
a	O
weighted	O
average	O
of	O
the	O
last	O
few	O
iterates	O
these	O
more	O
sophisticated	O
averaging	O
schemes	O
can	O
improve	O
the	O
convergence	O
speed	O
in	O
some	O
situations	O
such	O
as	O
in	O
the	O
case	O
of	O
strongly	B
convex	B
functions	O
defined	O
in	O
the	O
following	O
strongly	B
convex	B
functions	O
in	O
this	O
section	O
we	O
show	O
a	O
variant	O
of	O
sgd	B
that	O
enjoys	O
a	O
faster	O
convergence	O
rate	O
for	O
problems	O
in	O
which	O
the	O
objective	O
function	B
is	O
strongly	B
convex	B
definition	O
of	O
strong	O
convexity	O
in	O
the	O
previous	O
chapter	O
we	O
rely	O
on	O
the	O
following	O
claim	O
which	O
generalizes	O
lemma	O
claim	O
have	O
if	O
f	O
is	O
convex	B
then	O
for	O
every	O
w	O
u	O
and	O
v	O
f	O
we	O
u	O
f	O
f	O
the	O
proof	O
is	O
similar	O
to	O
the	O
proof	O
of	O
lemma	O
and	O
is	O
left	O
as	O
an	O
exercise	O
sgd	B
for	O
minimizing	O
a	O
convex	B
function	B
goal	O
solve	O
minw	O
h	O
f	O
parameter	O
t	O
initialize	O
for	O
t	O
t	O
choose	O
a	O
random	O
vector	O
vt	O
s	O
t	O
evtwt	O
f	O
set	B
t	O
t	O
set	B
wt	O
set	B
arg	O
minw	O
h	O
wt	O
output	O
w	O
t	O
wt	O
tvt	O
wt	O
theorem	O
assume	O
that	O
f	O
is	O
convex	B
and	O
that	O
let	O
argminw	O
h	O
f	O
be	O
an	O
optimal	O
solution	O
then	O
ef	O
w	O
f	O
t	O
logt	O
proof	O
let	O
evtwt	O
since	O
f	O
is	O
strongly	B
convex	B
and	O
is	O
in	O
the	O
subgradient	O
set	B
of	O
f	O
at	O
wt	O
we	O
have	O
that	O
f	O
f	O
next	O
we	O
show	O
that	O
t	O
t	O
stochastic	O
gradient	B
descent	I
since	O
is	O
the	O
projection	B
of	O
wt	O
therefore	O
t	O
onto	O
h	O
and	O
h	O
we	O
have	O
that	O
taking	O
expectation	O
of	O
both	O
sides	O
rearranging	O
and	O
using	O
the	O
assumption	O
yield	O
equation	O
comparing	O
equation	O
and	O
equation	O
and	O
summing	O
over	O
t	O
we	O
obtain	O
e	O
f	O
t	O
t	O
next	O
we	O
use	O
the	O
definition	O
t	O
t	O
and	O
note	O
that	O
the	O
first	O
sum	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
equation	O
collapses	O
to	O
thus	O
f	O
t	O
logt	O
the	O
theorem	O
follows	O
from	O
the	O
preceding	O
by	O
dividing	O
by	O
t	O
and	O
using	O
jensen	O
s	O
inequality	O
remark	O
rakhlin	O
shamir	O
sridharan	O
derived	O
a	O
convergence	O
rate	O
in	O
which	O
the	O
logt	O
term	O
is	O
eliminated	O
for	O
a	O
variant	O
of	O
the	O
algorithm	O
in	O
which	O
we	O
output	O
the	O
average	O
of	O
the	O
last	O
t	O
iterates	O
w	O
tt	O
wt	O
shamir	O
t	O
zhang	O
have	O
shown	O
that	O
theorem	O
holds	O
even	O
if	O
we	O
output	O
w	O
wt	O
learning	O
with	O
sgd	B
we	O
have	O
so	O
far	O
introduced	O
and	O
analyzed	O
the	O
sgd	B
algorithm	O
for	O
general	O
convex	B
functions	O
now	O
we	O
shall	O
consider	O
its	O
applicability	O
to	O
learning	O
tasks	O
sgd	B
for	O
risk	B
minimization	O
recall	B
that	O
in	O
learning	O
we	O
face	O
the	O
problem	O
of	O
minimizing	O
the	O
risk	B
function	B
ldw	O
e	O
z	O
z	O
we	O
have	O
seen	O
the	O
method	O
of	O
empirical	B
risk	B
minimization	O
where	O
we	O
minimize	O
the	O
empirical	B
risk	B
lsw	O
as	O
an	O
estimate	O
to	O
minimizing	O
ldw	O
sgd	B
allows	O
us	O
to	O
take	O
a	O
different	O
approach	O
and	O
minimize	O
ldw	O
directly	O
since	O
we	O
do	O
not	O
know	O
d	O
we	O
cannot	O
simply	O
calculate	O
ldwt	O
and	O
minimize	O
it	O
with	O
the	O
gd	O
method	O
with	O
sgd	B
however	O
all	O
we	O
need	O
is	O
to	O
find	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	B
of	O
learning	O
with	O
sgd	B
ldw	O
that	O
is	O
a	O
random	O
vector	O
whose	O
conditional	O
expected	O
value	O
is	O
ldwt	O
we	O
shall	O
now	O
see	O
how	O
such	O
an	O
estimate	O
can	O
be	O
easily	O
constructed	O
for	O
simplicity	O
let	O
us	O
first	O
consider	O
the	O
case	O
of	O
differentiable	O
loss	B
functions	O
hence	O
the	O
risk	B
function	B
ld	O
is	O
also	O
differentiable	O
the	O
construction	O
of	O
the	O
random	O
vector	O
vt	O
will	O
be	O
as	O
follows	O
first	O
sample	O
z	O
d	O
then	O
define	O
vt	O
to	O
be	O
the	O
gradient	B
of	O
the	O
function	B
z	O
with	O
respect	O
to	O
w	O
at	O
the	O
point	O
wt	O
then	O
by	O
the	O
linearity	O
of	O
the	O
gradient	B
we	O
have	O
evtwt	O
e	O
z	O
d	O
z	O
e	O
z	O
z	O
ldwt	O
the	O
gradient	B
of	O
the	O
loss	B
function	B
z	O
at	O
wt	O
is	O
therefore	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	B
of	O
the	O
risk	B
function	B
ldwt	O
and	O
is	O
easily	O
constructed	O
by	O
sampling	O
a	O
single	O
fresh	O
example	O
z	O
d	O
at	O
each	O
iteration	O
t	O
the	O
same	O
argument	O
holds	O
for	O
nondifferentiable	O
loss	B
functions	O
we	O
simply	O
let	O
vt	O
be	O
a	O
subgradient	O
of	O
z	O
at	O
wt	O
then	O
for	O
every	O
u	O
we	O
have	O
z	O
z	O
wt	O
taking	O
expectation	O
on	O
both	O
sides	O
with	O
respect	O
to	O
z	O
d	O
and	O
conditioned	O
on	O
the	O
value	O
of	O
wt	O
we	O
obtain	O
ldu	O
ldwt	O
z	O
zwt	O
wt	O
wt	O
it	O
follows	O
that	O
evtwt	O
is	O
a	O
subgradient	O
of	O
ldw	O
at	O
wt	O
to	O
summarize	O
the	O
stochastic	O
gradient	B
descent	I
framework	O
for	O
minimizing	O
the	O
risk	B
is	O
as	O
follows	O
stochastic	O
gradient	B
descent	I
for	O
minimizing	O
ldw	O
parameters	O
scalar	O
integer	O
t	O
initialize	O
for	O
t	O
t	O
sample	O
z	O
d	O
pick	O
vt	O
z	O
update	O
wt	O
vt	O
output	O
w	O
t	O
wt	O
we	O
shall	O
now	O
use	O
our	O
analysis	O
of	O
sgd	B
to	O
obtain	O
a	O
sample	B
complexity	I
analysis	O
for	O
learning	O
convex-lipschitz-bounded	O
problems	O
theorem	O
yields	O
the	O
following	O
corollary	O
consider	O
a	O
convex-lipschitz-bounded	B
learning	I
problem	O
with	O
parameters	O
b	O
then	O
for	O
every	O
if	O
we	O
run	O
the	O
sgd	B
method	O
for	O
minimizing	O
stochastic	O
gradient	B
descent	I
ldw	O
with	O
a	O
number	O
of	O
iterations	O
number	O
of	O
examples	O
t	O
and	O
with	O
t	O
then	O
the	O
output	O
of	O
sgd	B
satisfies	O
w	O
h	O
ldw	O
e	O
w	O
min	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
required	O
sample	B
complexity	I
is	O
of	O
the	O
same	O
order	O
of	O
magnitude	O
as	O
the	O
sample	B
complexity	I
guarantee	O
we	O
derived	O
for	O
regularized	O
loss	B
minimization	O
in	O
fact	O
the	O
sample	B
complexity	I
of	O
sgd	B
is	O
even	O
better	O
than	O
what	O
we	O
have	O
derived	O
for	O
regularized	O
loss	B
minimization	O
by	O
a	O
factor	O
of	O
analyzing	O
sgd	B
for	O
convex-smooth	O
learning	O
problems	O
in	O
the	O
previous	O
chapter	O
we	O
saw	O
that	O
the	O
regularized	O
loss	B
minimization	O
rule	O
also	O
learns	O
the	O
class	O
of	O
convex-smooth-bounded	B
learning	I
problems	O
we	O
now	O
show	O
that	O
the	O
sgd	B
algorithm	O
can	O
be	O
also	O
used	O
for	O
such	O
problems	O
theorem	O
assume	O
that	O
for	O
all	O
z	O
the	O
loss	B
function	B
z	O
is	O
convex	B
smooth	O
and	O
nonnegative	O
then	O
if	O
we	O
run	O
the	O
sgd	B
algorithm	O
for	O
minimizing	O
ldw	O
we	O
have	O
that	O
for	O
every	O
t	O
eld	O
w	O
proof	O
recall	B
that	O
if	O
a	O
function	B
is	O
and	O
nonnegative	O
then	O
it	O
is	O
selfbounded	O
f	O
f	O
to	O
analyze	O
sgd	B
for	O
convex-smooth	O
problems	O
let	O
us	O
define	O
zt	O
the	O
random	O
samples	O
of	O
the	O
sgd	B
algorithm	O
let	O
ft	O
zt	O
and	O
note	O
that	O
vt	O
ftwt	O
for	O
all	O
t	O
ft	O
is	O
a	O
convex	B
function	B
and	O
therefore	O
ftwt	O
wt	O
summing	O
over	O
t	O
and	O
using	O
lemma	O
we	O
obtain	O
combining	O
the	O
preceding	O
with	O
the	O
self-boundedness	B
of	O
ft	O
yields	O
wt	O
ftwt	O
t	O
t	O
ftwt	O
t	O
dividing	O
by	O
t	O
and	O
rearranging	O
we	O
obtain	O
next	O
we	O
take	O
expectation	O
of	O
the	O
two	O
sides	O
of	O
the	O
preceding	O
equation	O
with	O
respect	O
learning	O
with	O
sgd	B
to	O
zt	O
clearly	O
in	O
addition	O
using	O
the	O
same	O
argument	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
we	O
have	O
that	O
e	O
t	O
t	O
ftwt	O
e	O
ldwt	O
eld	O
w	O
combining	O
all	O
we	O
conclude	O
our	O
proof	O
as	O
a	O
direct	O
corollary	O
we	O
obtain	O
corollary	O
consider	O
a	O
convex-smooth-bounded	B
learning	I
problem	O
with	O
parameters	O
b	O
assume	O
in	O
addition	O
that	O
z	O
for	O
all	O
z	O
z	O
for	O
every	O
set	B
then	O
running	O
sgd	B
with	O
t	O
yields	O
eld	O
w	O
min	O
w	O
h	O
ldw	O
sgd	B
for	O
regularized	O
loss	B
minimization	O
we	O
have	O
shown	O
that	O
sgd	B
enjoys	O
the	O
same	O
worst-case	O
sample	B
complexity	I
bound	O
as	O
regularized	O
loss	B
minimization	O
however	O
on	O
some	O
distributions	O
regularized	O
loss	B
minimization	O
may	O
yield	O
a	O
better	O
solution	O
therefore	O
in	O
some	O
cases	O
we	O
may	O
want	O
to	O
solve	O
the	O
optimization	O
problem	O
associated	O
with	O
regularized	O
loss	B
minimization	O
min	O
w	O
lsw	O
define	O
f	O
since	O
we	O
are	O
dealing	O
with	O
convex	B
learning	O
problems	O
in	O
which	O
the	O
loss	B
function	B
is	O
convex	B
the	O
preceding	O
problem	O
is	O
also	O
a	O
convex	B
optimization	O
problem	O
that	O
can	O
be	O
solved	O
using	O
sgd	B
as	O
well	O
as	O
we	O
shall	O
see	O
in	O
this	O
section	O
lsw	O
note	O
that	O
f	O
is	O
a	O
convex	B
function	B
therefore	O
we	O
can	O
apply	O
the	O
sgd	B
variant	O
given	O
in	O
section	O
h	O
rd	O
to	O
apply	O
this	O
algorithm	O
we	O
only	O
need	O
to	O
find	O
a	O
way	O
to	O
construct	O
an	O
unbiased	O
estimate	O
of	O
a	O
subgradient	O
of	O
f	O
at	O
wt	O
this	O
is	O
easily	O
done	O
by	O
noting	O
that	O
if	O
we	O
pick	O
z	O
uniformly	O
at	O
random	O
from	O
s	O
and	O
choose	O
vt	O
z	O
then	O
the	O
expected	O
value	O
of	O
wt	O
vt	O
is	O
a	O
subgradient	O
of	O
f	O
at	O
wt	O
to	O
analyze	O
the	O
resulting	O
algorithm	O
we	O
first	O
rewrite	O
the	O
update	O
rule	O
we	O
divided	O
by	O
for	O
convenience	O
stochastic	O
gradient	B
descent	I
that	O
h	O
rd	O
and	O
therefore	O
the	O
projection	B
step	O
does	O
not	O
matter	O
as	O
follows	O
wt	O
t	O
wt	O
vt	O
vt	O
vt	O
t	O
t	O
wt	O
t	O
t	O
t	O
wt	O
t	O
t	O
t	O
wt	O
t	O
t	O
vi	O
t	O
vt	O
vt	O
if	O
we	O
assume	O
that	O
the	O
loss	B
function	B
is	O
it	O
follows	O
that	O
for	O
all	O
t	O
we	O
have	O
and	O
therefore	O
which	O
yields	O
wt	O
theorem	O
therefore	O
tells	O
us	O
that	O
after	O
performing	O
t	O
iterations	O
we	O
have	O
that	O
ef	O
w	O
f	O
t	O
logt	O
summary	O
we	O
have	O
introduced	O
the	O
gradient	B
descent	I
and	O
stochastic	O
gradient	B
descent	I
algorithms	O
along	O
with	O
several	O
of	O
their	O
variants	O
we	O
have	O
analyzed	O
their	O
convergence	O
rate	O
and	O
calculated	O
the	O
number	O
of	O
iterations	O
that	O
would	O
guarantee	O
an	O
expected	O
objective	O
of	O
at	O
most	O
plus	O
the	O
optimal	O
objective	O
most	O
importantly	O
we	O
have	O
shown	O
that	O
by	O
using	O
sgd	B
we	O
can	O
directly	O
minimize	O
the	O
risk	B
function	B
we	O
do	O
so	O
by	O
sampling	O
a	O
point	O
i	O
i	O
d	O
from	O
d	O
and	O
using	O
a	O
subgradient	O
of	O
the	O
loss	B
of	O
the	O
current	O
hypothesis	B
wt	O
at	O
this	O
point	O
as	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	B
a	O
subgradient	O
of	O
the	O
risk	B
function	B
this	O
implies	O
that	O
a	O
bound	O
on	O
the	O
number	O
of	O
iterations	O
also	O
yields	O
a	O
sample	B
complexity	I
bound	O
finally	O
we	O
have	O
also	O
shown	O
how	O
to	O
apply	O
the	O
sgd	B
method	O
to	O
the	O
problem	O
of	O
regularized	O
risk	B
minimization	O
in	O
future	O
chapters	O
we	O
show	O
how	O
this	O
yields	O
extremely	O
simple	O
solvers	O
to	O
some	O
optimization	O
problems	O
associated	O
with	O
regularized	O
risk	B
minimization	O
bibliographic	O
remarks	O
sgd	B
dates	O
back	O
to	O
robbins	O
monro	O
it	O
is	O
especially	O
effective	O
in	O
large	O
scale	O
machine	O
learning	O
problems	O
see	O
for	O
example	O
le	O
cun	O
zhang	O
bottou	O
bousquet	O
shalev-shwartz	O
singer	O
srebro	O
shalev-shwartz	O
srebro	O
in	O
the	O
optimization	O
community	O
it	O
was	O
studied	O
exercises	O
in	O
the	O
context	O
of	O
stochastic	O
optimization	O
see	O
for	O
example	O
yudin	O
nesterov	O
nesterov	O
nesterov	O
nemirovski	O
juditsky	O
lan	O
shapiro	O
shapiro	O
dentcheva	O
ruszczy	O
nski	O
the	O
bound	O
we	O
have	O
derived	O
for	O
strongly	B
convex	B
function	B
is	O
due	O
to	O
hazan	O
agarwal	O
kale	O
as	O
mentioned	O
previously	O
improved	O
bounds	O
have	O
been	O
obtained	O
in	O
rakhlin	O
et	O
al	O
exercises	O
prove	O
claim	O
hint	O
extend	O
the	O
proof	O
of	O
lemma	O
prove	O
corollary	O
perceptron	B
as	O
a	O
subgradient	O
descent	O
algorithm	O
let	O
s	O
ym	O
assume	O
that	O
there	O
exists	O
w	O
rd	O
such	O
that	O
for	O
every	O
i	O
we	O
have	O
and	O
let	O
be	O
a	O
vector	O
that	O
has	O
the	O
minimal	O
norm	O
among	O
all	O
vectors	O
that	O
satisfy	O
the	O
preceding	O
requirement	O
let	O
r	O
maxi	O
define	O
a	O
function	B
f	O
max	O
i	O
yi	O
separates	O
the	O
examples	O
in	O
s	O
show	O
that	O
f	O
and	O
show	O
that	O
any	O
w	O
for	O
which	O
f	O
show	O
how	O
to	O
calculate	O
a	O
subgradient	O
of	O
f	O
describe	O
and	O
analyze	O
the	O
subgradient	O
descent	O
algorithm	O
for	O
this	O
case	O
compare	O
the	O
algorithm	O
and	O
the	O
analysis	O
to	O
the	O
batch	O
perceptron	B
algorithm	O
given	O
in	O
section	O
variable	O
step	O
size	O
prove	O
an	O
analog	O
of	O
theorem	O
for	O
sgd	B
with	O
a	O
variable	O
step	O
size	O
t	O
b	O
t	O
support	O
vector	O
machines	O
in	O
this	O
chapter	O
and	O
the	O
next	O
we	O
discuss	O
a	O
very	O
useful	O
machine	O
learning	O
tool	O
the	O
support	O
vector	O
machine	O
paradigm	O
for	O
learning	O
linear	B
predictors	I
in	O
high	O
dimensional	O
feature	B
spaces	O
the	O
high	O
dimensionality	O
of	O
the	O
feature	B
space	I
raises	O
both	O
sample	B
complexity	I
and	O
computational	B
complexity	I
challenges	O
the	O
svm	B
algorithmic	O
paradigm	O
tackles	O
the	O
sample	B
complexity	I
challenge	O
by	O
searching	O
for	O
large	O
margin	B
separators	O
roughly	O
speaking	O
a	O
halfspace	B
separates	O
a	O
training	B
set	B
with	O
a	O
large	O
margin	B
if	O
all	O
the	O
examples	O
are	O
not	O
only	O
on	O
the	O
correct	O
side	O
of	O
the	O
separating	O
hyperplane	O
but	O
also	O
far	O
away	O
from	O
it	O
restricting	O
the	O
algorithm	O
to	O
output	O
a	O
large	O
margin	B
separator	O
can	O
yield	O
a	O
small	O
sample	B
complexity	I
even	O
if	O
the	O
dimensionality	O
of	O
the	O
feature	B
space	I
is	O
high	O
even	O
infinite	O
we	O
introduce	O
the	O
concept	O
of	O
margin	B
and	O
relate	O
it	O
to	O
the	O
regularized	O
loss	B
minimization	O
paradigm	O
as	O
well	O
as	O
to	O
the	O
convergence	O
rate	O
of	O
the	O
perceptron	B
algorithm	O
in	O
the	O
next	O
chapter	O
we	O
will	O
tackle	O
the	O
computational	B
complexity	I
challenge	O
using	O
the	O
idea	O
of	O
kernels	B
margin	B
and	O
hard-svm	B
let	O
s	O
ym	O
be	O
a	O
training	B
set	B
of	O
examples	O
where	O
each	O
xi	O
rd	O
and	O
yi	O
we	O
say	O
that	O
this	O
training	B
set	B
is	O
linearly	O
separable	B
if	O
there	O
exists	O
a	O
halfspace	B
b	O
such	O
that	O
yi	O
b	O
for	O
all	O
i	O
alternatively	O
this	O
condition	O
can	O
be	O
rewritten	O
as	O
i	O
b	O
all	O
halfspaces	O
b	O
that	O
satisfy	O
this	O
condition	O
are	O
erm	B
hypotheses	O
error	O
is	O
zero	O
which	O
is	O
the	O
minimum	O
possible	O
error	O
for	O
any	O
separable	B
training	O
sample	O
there	O
are	O
many	O
erm	B
halfspaces	O
which	O
one	O
of	O
them	O
should	O
the	O
learner	O
pick	O
consider	O
for	O
example	O
the	O
training	B
set	B
described	O
in	O
the	O
picture	O
that	O
follows	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
margin	B
and	O
hard-svm	B
x	O
x	O
while	O
both	O
the	O
dashed-black	O
and	O
solid-green	O
hyperplanes	O
separate	O
the	O
four	O
examples	O
our	O
intuition	O
would	O
probably	O
lead	O
us	O
to	O
prefer	O
the	O
black	O
hyperplane	O
over	O
the	O
green	O
one	O
one	O
way	O
to	O
formalize	O
this	O
intuition	O
is	O
using	O
the	O
concept	O
of	O
margin	B
the	O
margin	B
of	O
a	O
hyperplane	O
with	O
respect	O
to	O
a	O
training	B
set	B
is	O
defined	O
to	O
be	O
the	O
minimal	O
distance	O
between	O
a	O
point	O
in	O
the	O
training	B
set	B
and	O
the	O
hyperplane	O
if	O
a	O
hyperplane	O
has	O
a	O
large	O
margin	B
then	O
it	O
will	O
still	O
separate	O
the	O
training	B
set	B
even	O
if	O
we	O
slightly	O
perturb	O
each	O
instance	B
we	O
will	O
see	O
later	O
on	O
that	O
the	O
true	B
error	I
of	O
a	O
halfspace	B
can	O
be	O
bounded	O
in	O
terms	O
of	O
the	O
margin	B
it	O
has	O
over	O
the	O
training	O
sample	O
larger	O
the	O
margin	B
the	O
smaller	O
the	O
error	O
regardless	O
of	O
the	O
euclidean	O
dimension	O
in	O
which	O
this	O
halfspace	B
resides	O
hard-svm	B
is	O
the	O
learning	O
rule	O
in	O
which	O
we	O
return	O
an	O
erm	B
hyperplane	O
that	O
separates	O
the	O
training	B
set	B
with	O
the	O
largest	O
possible	O
margin	B
to	O
define	O
hard-svm	B
formally	O
we	O
first	O
express	O
the	O
distance	O
between	O
a	O
point	O
x	O
to	O
a	O
hyperplane	O
using	O
the	O
parameters	O
defining	O
the	O
halfspace	B
claim	O
the	O
distance	O
between	O
a	O
point	O
x	O
and	O
the	O
hyperplane	O
defined	O
by	O
b	O
where	O
is	O
b	O
proof	O
the	O
distance	O
between	O
a	O
point	O
x	O
and	O
the	O
hyperplane	O
is	O
defined	O
as	O
b	O
taking	O
v	O
x	O
bw	O
we	O
have	O
that	O
b	O
b	O
and	O
b	O
hence	O
the	O
distance	O
is	O
at	O
most	O
b	O
next	O
take	O
any	O
other	O
point	O
u	O
on	O
the	O
hyperplane	O
thus	O
b	O
we	O
have	O
v	O
v	O
v	O
v	O
v	O
v	O
v	O
where	O
the	O
last	O
equality	O
is	O
because	O
b	O
hence	O
the	O
distance	O
support	O
vector	O
machines	O
between	O
x	O
and	O
u	O
is	O
at	O
least	O
the	O
distance	O
between	O
x	O
and	O
v	O
which	O
concludes	O
our	O
proof	O
on	O
the	O
basis	O
of	O
the	O
preceding	O
claim	O
the	O
closest	O
point	O
in	O
the	O
training	B
set	B
to	O
the	O
separating	O
hyperplane	O
is	O
mini	O
b	O
therefore	O
the	O
hard-svm	B
rule	O
is	O
argmax	O
min	O
i	O
b	O
s	O
t	O
i	O
b	O
whenever	O
there	O
is	O
a	O
solution	O
to	O
the	O
preceding	O
problem	O
we	O
are	O
in	O
the	O
separable	B
case	O
we	O
can	O
write	O
an	O
equivalent	O
problem	O
as	O
follows	O
exercise	O
argmax	O
min	O
i	O
b	O
next	O
we	O
give	O
another	O
equivalent	O
formulation	O
of	O
the	O
hard-svm	B
rule	O
as	O
a	O
quadratic	O
optimization	O
hard-svm	B
input	O
ym	O
solve	O
argmin	O
s	O
t	O
i	O
b	O
output	O
w	O
b	O
the	O
lemma	O
that	O
follows	O
shows	O
that	O
the	O
output	O
of	O
hard-svm	B
is	O
indeed	O
the	O
separating	O
hyperplane	O
with	O
the	O
largest	O
margin	B
intuitively	O
hard-svm	B
searches	O
for	O
w	O
of	O
minimal	O
norm	O
among	O
all	O
the	O
vectors	O
that	O
separate	O
the	O
data	O
and	O
for	O
which	O
b	O
for	O
all	O
i	O
in	O
other	O
words	O
we	O
enforce	O
the	O
margin	B
to	O
be	O
but	O
now	O
the	O
units	O
in	O
which	O
we	O
measure	O
the	O
margin	B
scale	O
with	O
the	O
norm	O
of	O
w	O
therefore	O
finding	O
the	O
largest	O
margin	B
halfspace	B
boils	O
down	O
to	O
finding	O
w	O
whose	O
norm	O
is	O
minimal	O
formally	O
lemma	O
the	O
output	O
of	O
hard-svm	B
is	O
a	O
solution	O
of	O
equation	O
proof	O
let	O
be	O
a	O
solution	O
of	O
equation	O
and	O
define	O
the	O
margin	B
achieved	O
by	O
to	O
be	O
mini	O
therefore	O
for	O
all	O
i	O
we	O
have	O
or	O
equivalently	O
hence	O
the	O
pair	O
satisfies	O
the	O
conditions	O
of	O
the	O
quadratic	O
optimization	O
a	O
quadratic	O
optimization	O
problem	O
is	O
an	O
optimization	O
problem	O
in	O
which	O
the	O
objective	O
is	O
a	O
convex	B
quadratic	O
function	B
and	O
the	O
constraints	O
are	O
linear	O
inequalities	O
margin	B
and	O
hard-svm	B
problem	O
given	O
in	O
equation	O
therefore	O
for	O
all	O
i	O
it	O
follows	O
that	O
w	O
b	O
since	O
we	O
obtain	O
that	O
w	O
b	O
is	O
an	O
optimal	O
solution	O
of	O
equation	O
the	O
homogenous	B
case	O
it	O
is	O
often	O
more	O
convenient	O
to	O
consider	O
homogenous	B
halfspaces	O
namely	O
halfspaces	O
that	O
pass	O
through	O
the	O
origin	O
and	O
are	O
thus	O
defined	O
by	O
where	O
the	O
bias	B
term	O
b	O
is	O
set	B
to	O
be	O
zero	O
hard-svm	B
for	O
homogenous	B
halfspaces	O
amounts	O
to	O
solving	O
s	O
t	O
i	O
min	O
w	O
as	O
we	O
discussed	O
in	O
chapter	O
we	O
can	O
reduce	O
the	O
problem	O
of	O
learning	O
nonhomogenous	O
halfspaces	O
to	O
the	O
problem	O
of	O
learning	O
homogenous	B
halfspaces	O
by	O
adding	O
one	O
more	O
feature	B
to	O
each	O
instance	B
of	O
xi	O
thus	O
increasing	O
the	O
dimension	O
to	O
d	O
note	O
however	O
that	O
the	O
optimization	O
problem	O
given	O
in	O
equation	O
does	O
not	O
regularize	O
the	O
bias	B
term	O
b	O
while	O
if	O
we	O
learn	O
a	O
homogenous	B
halfspace	B
in	O
using	O
equation	O
then	O
we	O
regularize	O
the	O
bias	B
term	O
the	O
d	O
component	O
of	O
the	O
weight	O
vector	O
as	O
well	O
however	O
regularizing	O
b	O
usually	O
does	O
not	O
make	O
a	O
significant	O
difference	O
to	O
the	O
sample	B
complexity	I
the	O
sample	B
complexity	I
of	O
hard-svm	B
recall	B
that	O
the	O
vc-dimension	O
of	O
halfspaces	O
in	O
rd	O
is	O
d	O
it	O
follows	O
that	O
the	O
sample	B
complexity	I
of	O
learning	O
halfspaces	O
grows	O
with	O
the	O
dimensionality	O
of	O
the	O
problem	O
furthermore	O
the	O
fundamental	O
theorem	O
of	O
learning	O
tells	O
us	O
that	O
if	O
the	O
number	O
of	O
examples	O
is	O
significantly	O
smaller	O
than	O
d	O
then	O
no	O
algorithm	O
can	O
learn	O
an	O
halfspace	B
this	O
is	O
problematic	O
when	O
d	O
is	O
very	O
large	O
to	O
overcome	O
this	O
problem	O
we	O
will	O
make	O
an	O
additional	O
assumption	O
on	O
the	O
underlying	O
data	O
distribution	O
in	O
particular	O
we	O
will	O
define	O
a	O
separability	O
with	O
margin	B
assumption	O
and	O
will	O
show	O
that	O
if	O
the	O
data	O
is	O
separable	B
with	O
margin	B
then	O
the	O
sample	B
complexity	I
is	O
bounded	O
from	O
above	O
by	O
a	O
function	B
of	O
it	O
follows	O
that	O
even	O
if	O
the	O
dimensionality	O
is	O
very	O
large	O
even	O
infinite	O
as	O
long	O
as	O
the	O
data	O
adheres	O
to	O
the	O
separability	O
with	O
margin	B
assumption	O
we	O
can	O
still	O
have	O
a	O
small	O
sample	B
complexity	I
there	O
is	O
no	O
contradiction	O
to	O
the	O
lower	O
bound	O
given	O
in	O
the	O
fundamental	O
theorem	O
of	O
learning	O
because	O
we	O
are	O
now	O
making	O
an	O
additional	O
assumption	O
on	O
the	O
underlying	O
data	O
distribution	O
before	O
we	O
formally	O
define	O
the	O
separability	O
with	O
margin	B
assumption	O
there	O
is	O
a	O
scaling	O
issue	O
we	O
need	O
to	O
resolve	O
suppose	O
that	O
a	O
training	B
set	B
s	O
ym	O
is	O
separable	B
with	O
a	O
margin	B
namely	O
the	O
maximal	O
objective	O
value	O
of	O
equation	O
is	O
at	O
least	O
then	O
for	O
any	O
positive	O
scalar	O
the	O
training	B
set	B
support	O
vector	O
machines	O
xm	O
ym	O
is	O
separable	B
with	O
a	O
margin	B
of	O
that	O
is	O
a	O
simple	O
scaling	O
of	O
the	O
data	O
can	O
make	O
it	O
separable	B
with	O
an	O
arbitrarily	O
large	O
margin	B
it	O
follows	O
that	O
in	O
order	O
to	O
give	O
a	O
meaningful	O
definition	O
of	O
margin	B
we	O
must	O
take	O
into	O
account	O
the	O
scale	O
of	O
the	O
examples	O
as	O
well	O
one	O
way	O
to	O
formalize	O
this	O
is	O
using	O
the	O
definition	O
that	O
follows	O
definition	O
let	O
d	O
be	O
a	O
distribution	O
over	O
rd	O
we	O
say	O
that	O
d	O
is	O
separable	B
with	O
a	O
if	O
there	O
exists	O
such	O
that	O
and	O
such	O
that	O
with	O
probability	O
over	O
the	O
choice	O
of	O
y	O
d	O
we	O
have	O
that	O
and	O
similarly	O
we	O
say	O
that	O
d	O
is	O
separable	B
with	O
a	O
using	O
a	O
homogenous	B
halfspace	B
if	O
the	O
preceding	O
holds	O
with	O
a	O
halfspace	B
of	O
the	O
form	O
in	O
the	O
advanced	O
part	O
of	O
the	O
book	O
we	O
will	O
prove	O
that	O
the	O
sample	B
complexity	I
of	O
hard-svm	B
depends	O
on	O
and	O
is	O
independent	O
of	O
the	O
dimension	O
d	O
in	O
particular	O
theorem	O
in	O
section	O
states	O
the	O
following	O
theorem	O
let	O
d	O
be	O
a	O
distribution	O
over	O
rd	O
that	O
satisfies	O
the	O
with	O
margin	B
assumption	O
using	O
a	O
homogenous	B
halfspace	B
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
a	O
training	B
set	B
of	O
size	O
m	O
the	O
error	O
of	O
the	O
output	O
of	O
hard-svm	B
is	O
at	O
most	O
m	O
m	O
remark	O
and	O
the	O
perceptron	B
in	O
section	O
we	O
have	O
described	O
and	O
analyzed	O
the	O
perceptron	B
algorithm	O
for	O
finding	O
an	O
erm	B
hypothesis	B
with	O
respect	O
to	O
the	O
class	O
of	O
halfspaces	O
in	O
particular	O
in	O
theorem	O
we	O
upper	O
bounded	O
the	O
number	O
of	O
updates	O
the	O
perceptron	B
might	O
make	O
on	O
a	O
given	O
training	B
set	B
it	O
can	O
be	O
shown	O
exercise	O
that	O
the	O
upper	O
bound	O
is	O
exactly	O
where	O
is	O
the	O
radius	O
of	O
examples	O
and	O
is	O
the	O
margin	B
soft-svm	B
and	O
norm	O
regularization	B
the	O
hard-svm	B
formulation	O
assumes	O
that	O
the	O
training	B
set	B
is	O
linearly	O
separable	B
which	O
is	O
a	O
rather	O
strong	O
assumption	O
soft-svm	B
can	O
be	O
viewed	O
as	O
a	O
relaxation	O
of	O
the	O
hard-svm	B
rule	O
that	O
can	O
be	O
applied	O
even	O
if	O
the	O
training	B
set	B
is	O
not	O
linearly	O
separable	B
the	O
optimization	O
problem	O
in	O
equation	O
enforces	O
the	O
hard	O
constraints	O
b	O
for	O
all	O
i	O
a	O
natural	O
relaxation	O
is	O
to	O
allow	O
the	O
constraint	O
to	O
be	O
violated	O
for	O
some	O
of	O
the	O
examples	O
in	O
the	O
training	B
set	B
this	O
can	O
be	O
modeled	O
by	O
introducing	O
nonnegative	O
slack	O
variables	O
m	O
and	O
replacing	O
each	O
constraint	O
b	O
by	O
the	O
constraint	O
b	O
i	O
that	O
is	O
i	O
measures	O
by	O
how	O
much	O
the	O
constraint	O
is	O
being	O
violated	O
soft-svm	B
jointly	O
minimizes	O
the	O
norm	O
of	O
w	O
to	O
the	O
margin	B
and	O
the	O
average	O
of	O
i	O
to	O
the	O
violations	O
of	O
the	O
constraints	O
the	O
tradeoff	O
between	O
the	O
two	O
soft-svm	B
and	O
norm	O
regularization	B
terms	O
is	O
controlled	O
by	O
a	O
parameter	O
this	O
leads	O
to	O
the	O
soft-svm	B
optimization	O
problem	O
soft-svm	B
input	O
ym	O
parameter	O
solve	O
m	O
min	O
wb	O
s	O
t	O
i	O
b	O
i	O
and	O
i	O
i	O
output	O
w	O
b	O
we	O
can	O
rewrite	O
equation	O
as	O
a	O
regularized	O
loss	B
minimization	O
problem	O
recall	B
the	O
definition	O
of	O
the	O
hinge	B
loss	B
b	O
y	O
b	O
given	O
b	O
and	O
a	O
training	B
set	B
s	O
the	O
averaged	O
hinge	B
loss	B
on	O
s	O
is	O
denoted	O
by	O
lhinge	O
b	O
now	O
consider	O
the	O
regularized	O
loss	B
minimization	O
problem	O
s	O
lhinge	O
s	O
b	O
min	O
wb	O
claim	O
equation	O
and	O
equation	O
are	O
equivalent	O
proof	O
fix	O
some	O
w	O
b	O
and	O
consider	O
the	O
minimization	O
over	O
in	O
equation	O
fix	O
some	O
i	O
since	O
i	O
must	O
be	O
nonnegative	O
the	O
best	O
assignment	O
to	O
i	O
would	O
be	O
if	O
b	O
and	O
would	O
be	O
b	O
otherwise	O
in	O
other	O
words	O
i	O
b	O
yi	O
for	O
all	O
i	O
and	O
the	O
claim	O
follows	O
we	O
therefore	O
see	O
that	O
soft-svm	B
falls	O
into	O
the	O
paradigm	O
of	O
regularized	O
loss	B
minimization	O
that	O
we	O
studied	O
in	O
the	O
previous	O
chapter	O
a	O
soft-svm	B
algorithm	O
that	O
is	O
a	O
solution	O
for	O
equation	O
has	O
a	O
bias	B
toward	O
low	O
norm	O
separators	O
the	O
objective	O
function	B
that	O
we	O
aim	O
to	O
minimize	O
in	O
equation	O
penalizes	O
not	O
only	O
for	O
training	O
errors	O
but	O
also	O
for	O
large	O
norm	O
it	O
is	O
often	O
more	O
convenient	O
to	O
consider	O
soft-svm	B
for	O
learning	O
a	O
homogenous	B
halfspace	B
where	O
the	O
bias	B
term	O
b	O
is	O
set	B
to	O
be	O
zero	O
which	O
yields	O
the	O
following	O
optimization	O
problem	O
lhinge	O
s	O
min	O
w	O
where	O
lhinge	O
s	O
m	O
support	O
vector	O
machines	O
the	O
sample	B
complexity	I
of	O
soft-svm	B
we	O
now	O
analyze	O
the	O
sample	B
complexity	I
of	O
soft-svm	B
for	O
the	O
case	O
of	O
homogenous	B
halfspaces	O
the	O
output	O
of	O
equation	O
in	O
corollary	O
we	O
derived	O
a	O
generalization	O
bound	O
for	O
the	O
regularized	O
loss	B
minimization	O
framework	O
assuming	O
that	O
the	O
loss	B
function	B
is	O
convex	B
and	O
lipschitz	O
we	O
have	O
already	O
shown	O
that	O
the	O
hinge	B
loss	B
is	O
convex	B
so	O
it	O
is	O
only	O
left	O
to	O
analyze	O
the	O
lipschitzness	B
of	O
the	O
hinge	B
loss	B
claim	O
let	O
f	O
then	O
f	O
is	O
it	O
is	O
easy	O
to	O
verify	O
that	O
any	O
subgradient	O
of	O
f	O
at	O
w	O
is	O
of	O
the	O
form	O
x	O
where	O
proof	O
the	O
claim	O
now	O
follows	O
from	O
lemma	O
corollary	O
therefore	O
yields	O
the	O
following	O
corollary	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
where	O
x	O
consider	O
running	O
soft-svm	B
on	O
a	O
training	B
set	B
s	O
dm	O
and	O
let	O
as	O
be	O
the	O
solution	O
of	O
soft-svm	B
then	O
for	O
every	O
u	O
e	O
s	O
dm	O
lhinged	O
m	O
furthermore	O
since	O
the	O
hinge	B
loss	B
upper	O
bounds	O
the	O
loss	B
we	O
also	O
have	O
e	O
s	O
dm	O
lhinged	O
m	O
last	O
for	O
every	O
b	O
if	O
we	O
set	B
then	O
e	O
s	O
dm	O
e	O
s	O
dm	O
min	O
b	O
lhinged	O
m	O
we	O
therefore	O
see	O
that	O
we	O
can	O
control	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
halfspace	B
as	O
a	O
function	B
of	O
the	O
norm	O
of	O
that	O
halfspace	B
independently	O
of	O
the	O
euclidean	O
dimension	O
of	O
the	O
space	O
over	O
which	O
the	O
halfspace	B
is	O
defined	O
this	O
becomes	O
highly	O
significant	O
when	O
we	O
learn	O
via	O
embeddings	O
into	O
high	O
dimensional	O
feature	B
spaces	O
as	O
we	O
will	O
consider	O
in	O
the	O
next	O
chapter	O
remark	O
the	O
condition	O
that	O
x	O
will	O
contain	O
vectors	O
with	O
a	O
bounded	O
norm	O
follows	O
from	O
the	O
requirement	O
that	O
the	O
loss	B
function	B
will	O
be	O
lipschitz	O
this	O
is	O
not	O
just	O
a	O
technicality	O
as	O
we	O
discussed	O
before	O
separation	O
with	O
large	O
margin	B
is	O
meaningless	O
without	O
imposing	O
a	O
restriction	O
on	O
the	O
scale	O
of	O
the	O
instances	O
indeed	O
without	O
a	O
constraint	O
on	O
the	O
scale	O
we	O
can	O
always	O
enlarge	O
the	O
margin	B
by	O
multiplying	O
all	O
instances	O
by	O
a	O
large	O
scalar	O
margin	B
and	O
norm-based	O
bounds	O
versus	O
dimension	O
the	O
bounds	O
we	O
have	O
derived	O
for	O
hard-svm	B
and	O
soft-svm	B
do	O
not	O
depend	O
on	O
the	O
dimension	O
of	O
the	O
instance	B
space	I
instead	O
the	O
bounds	O
depend	O
on	O
the	O
norm	O
of	O
the	O
soft-svm	B
and	O
norm	O
regularization	B
examples	O
the	O
norm	O
of	O
the	O
halfspace	B
b	O
equivalently	O
the	O
margin	B
parameter	O
and	O
in	O
the	O
nonseparable	O
case	O
the	O
bounds	O
also	O
depend	O
on	O
the	O
minimum	O
hinge	B
loss	B
of	O
all	O
halfspaces	O
of	O
norm	O
b	O
in	O
contrast	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
is	O
d	O
which	O
implies	O
that	O
the	O
error	O
of	O
an	O
erm	B
hypothesis	B
decreases	O
as	O
does	O
we	O
now	O
give	O
an	O
example	O
in	O
which	O
d	O
hence	O
the	O
bound	O
given	O
in	O
corollary	O
is	O
much	O
better	O
than	O
the	O
vc	O
bound	O
consider	O
the	O
problem	O
of	O
learning	O
to	O
classify	O
a	O
short	O
text	O
document	O
according	O
to	O
its	O
topic	O
say	O
whether	O
the	O
document	O
is	O
about	O
sports	O
or	O
not	O
we	O
first	O
need	O
to	O
represent	O
documents	O
as	O
vectors	O
one	O
simple	O
yet	O
effective	O
way	O
is	O
to	O
use	O
a	O
bagof-words	O
representation	O
that	O
is	O
we	O
define	O
a	O
dictionary	O
of	O
words	O
and	O
set	B
the	O
dimension	O
d	O
to	O
be	O
the	O
number	O
of	O
words	O
in	O
the	O
dictionary	O
given	O
a	O
document	O
we	O
represent	O
it	O
as	O
a	O
vector	O
x	O
where	O
xi	O
if	O
the	O
i	O
th	O
word	O
in	O
the	O
dictionary	O
appears	O
in	O
the	O
document	O
and	O
xi	O
otherwise	O
therefore	O
for	O
this	O
problem	O
the	O
value	O
of	O
will	O
be	O
the	O
maximal	O
number	O
of	O
distinct	O
words	O
in	O
a	O
given	O
document	O
a	O
halfspace	B
for	O
this	O
problem	O
assigns	O
weights	O
to	O
words	O
it	O
is	O
natural	O
to	O
assume	O
that	O
by	O
assigning	O
positive	O
and	O
negative	O
weights	O
to	O
a	O
few	O
dozen	O
words	O
we	O
will	O
be	O
able	O
to	O
determine	O
whether	O
a	O
given	O
document	O
is	O
about	O
sports	O
or	O
not	O
with	O
reasonable	O
accuracy	B
therefore	O
for	O
this	O
problem	O
the	O
value	O
of	O
can	O
be	O
set	B
to	O
be	O
less	O
than	O
overall	O
it	O
is	O
reasonable	O
to	O
say	O
that	O
the	O
value	O
of	O
is	O
smaller	O
than	O
on	O
the	O
other	O
hand	O
a	O
typical	O
size	O
of	O
a	O
dictionary	O
is	O
much	O
larger	O
than	O
for	O
example	O
there	O
are	O
more	O
than	O
distinct	O
words	O
in	O
english	O
we	O
have	O
therefore	O
shown	O
a	O
problem	O
in	O
which	O
there	O
can	O
be	O
an	O
order	O
of	O
magnitude	O
difference	O
between	O
learning	O
a	O
halfspace	B
with	O
the	O
svm	B
rule	O
and	O
learning	O
a	O
halfspace	B
using	O
the	O
vanilla	O
erm	B
rule	O
of	O
course	O
it	O
is	O
possible	O
to	O
construct	O
problems	O
in	O
which	O
the	O
svm	B
bound	O
will	O
be	O
worse	O
than	O
the	O
vc	O
bound	O
when	O
we	O
use	O
svm	B
we	O
in	O
fact	O
introduce	O
another	O
form	O
of	O
inductive	O
bias	B
we	O
prefer	O
large	O
margin	B
halfspaces	O
while	O
this	O
inductive	O
bias	B
can	O
significantly	O
decrease	O
our	O
estimation	B
error	I
it	O
can	O
also	O
enlarge	O
the	O
approximation	B
error	I
the	O
ramp	B
loss	B
the	O
margin-based	O
bounds	O
we	O
have	O
derived	O
in	O
corollary	O
rely	O
on	O
the	O
fact	O
that	O
we	O
minimize	O
the	O
hinge	B
loss	B
as	O
we	O
have	O
shown	O
in	O
the	O
previous	O
subsection	O
the	O
term	O
can	O
be	O
much	O
smaller	O
than	O
the	O
corresponding	O
term	O
in	O
the	O
vc	O
however	O
the	O
approximation	B
error	I
in	O
corollary	O
is	O
measured	O
with	O
respect	O
to	O
the	O
hinge	B
loss	B
while	O
the	O
approximation	B
error	I
in	O
vc	O
bounds	O
is	O
measured	O
with	O
respect	O
to	O
the	O
loss	B
since	O
the	O
hinge	B
loss	B
upper	O
bounds	O
the	O
loss	B
the	O
approximation	B
error	I
with	O
respect	O
to	O
the	O
loss	B
will	O
never	O
exceed	O
that	O
of	O
the	O
hinge	B
loss	B
for	O
the	O
loss	B
this	O
follows	O
from	O
the	O
fact	O
that	O
the	O
loss	B
is	O
scale	O
it	O
is	O
not	O
possible	O
to	O
derive	O
bounds	O
that	O
involve	O
the	O
estimation	B
error	I
term	O
support	O
vector	O
machines	O
insensitive	O
and	O
therefore	O
there	O
is	O
no	O
meaning	O
to	O
the	O
norm	O
of	O
w	O
or	O
its	O
margin	B
when	O
we	O
measure	O
error	O
with	O
the	O
loss	B
however	O
it	O
is	O
possible	O
to	O
define	O
a	O
loss	B
function	B
that	O
on	O
one	O
hand	O
it	O
is	O
scale	O
sensitive	O
and	O
thus	O
enjoys	O
the	O
estimation	O
while	O
on	O
the	O
other	O
hand	O
it	O
is	O
more	O
similar	O
to	O
the	O
loss	B
one	O
option	O
is	O
the	O
ramp	B
loss	B
defined	O
as	O
y	O
y	O
the	O
ramp	B
loss	B
penalizes	O
mistakes	O
in	O
the	O
same	O
way	O
as	O
the	O
loss	B
and	O
does	O
not	O
penalize	O
examples	O
that	O
are	O
separated	O
with	O
margin	B
the	O
difference	O
between	O
the	O
ramp	B
loss	B
and	O
the	O
loss	B
is	O
only	O
with	O
respect	O
to	O
examples	O
that	O
are	O
correctly	O
classified	O
but	O
not	O
with	O
a	O
significant	O
margin	B
generalization	B
bounds	I
for	O
the	O
ramp	B
loss	B
are	O
given	O
in	O
the	O
advanced	O
part	O
of	O
this	O
book	O
appendix	O
the	O
reason	O
svm	B
relies	O
on	O
the	O
hinge	B
loss	B
and	O
not	O
on	O
the	O
ramp	B
loss	B
is	O
that	O
the	O
hinge	B
loss	B
is	O
convex	B
and	O
therefore	O
from	O
the	O
computational	O
point	O
of	O
view	O
minimizing	O
the	O
hinge	B
loss	B
can	O
be	O
performed	O
efficiently	O
in	O
contrast	O
the	O
problem	O
of	O
minimizing	O
the	O
ramp	B
loss	B
is	O
computationally	O
intractable	O
optimality	O
conditions	O
and	O
support	B
vectors	I
the	O
name	O
support	O
vector	O
machine	O
stems	O
from	O
the	O
fact	O
that	O
the	O
solution	O
of	O
hard-svm	B
is	O
supported	O
by	O
is	O
in	O
the	O
linear	O
span	O
of	O
the	O
examples	O
that	O
are	O
exactly	O
at	O
distance	O
from	O
the	O
separating	O
hyperplane	O
these	O
vectors	O
are	O
therefore	O
called	O
support	B
vectors	I
to	O
see	O
this	O
we	O
rely	O
on	O
fritz	O
john	O
optimality	O
conditions	O
theorem	O
let	O
be	O
as	O
defined	O
in	O
equation	O
and	O
let	O
i	O
then	O
there	O
exist	O
coefficients	O
m	O
such	O
that	O
the	O
examples	O
i	O
i	O
are	O
called	O
support	B
vectors	I
the	O
proof	O
of	O
this	O
theorem	O
follows	O
by	O
applying	O
the	O
following	O
lemma	O
to	O
equa	O
tion	O
i	O
i	O
ixi	O
duality	B
lemma	O
john	O
suppose	O
that	O
argmin	O
f	O
w	O
s	O
t	O
i	O
giw	O
f	O
where	O
f	O
gm	O
are	O
differentiable	O
then	O
there	O
exists	O
rm	O
such	O
that	O
i	O
i	O
i	O
where	O
i	O
duality	B
historically	O
many	O
of	O
the	O
properties	O
of	O
svm	B
have	O
been	O
obtained	O
by	O
considering	O
the	O
dual	O
of	O
equation	O
our	O
presentation	O
of	O
svm	B
does	O
not	O
rely	O
on	O
duality	B
for	O
completeness	O
we	O
present	O
in	O
the	O
following	O
how	O
to	O
derive	O
the	O
dual	O
of	O
equation	O
we	O
start	O
by	O
rewriting	O
the	O
problem	O
in	O
an	O
equivalent	O
form	O
as	O
follows	O
consider	O
the	O
function	B
gw	O
max	O
rm	O
if	O
i	O
otherwise	O
we	O
can	O
therefore	O
rewrite	O
equation	O
as	O
min	O
w	O
rearranging	O
the	O
preceding	O
we	O
obtain	O
that	O
equation	O
can	O
be	O
rewritten	O
as	O
the	O
problem	O
min	O
w	O
max	O
rm	O
now	O
suppose	O
that	O
we	O
flip	O
the	O
order	O
of	O
min	O
and	O
max	O
in	O
the	O
above	O
equation	O
this	O
can	O
only	O
decrease	O
the	O
objective	O
value	O
exercise	O
and	O
we	O
have	O
min	O
w	O
max	O
rm	O
max	O
rm	O
min	O
w	O
the	O
preceding	O
inequality	O
is	O
called	O
weak	B
duality	B
it	O
turns	O
out	O
that	O
in	O
our	O
case	O
strong	B
duality	B
also	O
holds	O
namely	O
the	O
inequality	O
holds	O
with	O
equality	O
therefore	O
the	O
dual	O
problem	O
is	O
max	O
rm	O
min	O
w	O
we	O
can	O
simplify	O
the	O
dual	O
problem	O
by	O
noting	O
that	O
once	O
is	O
fixed	O
the	O
optimization	O
support	O
vector	O
machines	O
problem	O
with	O
respect	O
to	O
w	O
is	O
unconstrained	O
and	O
the	O
objective	O
is	O
differentiable	O
thus	O
at	O
the	O
optimum	O
the	O
gradient	B
equals	O
zero	O
iyixi	O
w	O
iyixi	O
this	O
shows	O
us	O
that	O
the	O
solution	O
must	O
be	O
in	O
the	O
linear	O
span	O
of	O
the	O
examples	O
a	O
fact	O
we	O
will	O
use	O
later	O
to	O
derive	O
svm	B
with	O
kernels	B
plugging	O
the	O
preceding	O
into	O
equation	O
we	O
obtain	O
that	O
the	O
dual	O
problem	O
can	O
be	O
rewritten	O
as	O
w	O
max	O
rm	O
iyixi	O
i	O
jyjxj	O
xi	O
rearranging	O
yields	O
the	O
dual	O
problem	O
j	O
yi	O
max	O
rm	O
i	O
i	O
note	O
that	O
the	O
dual	O
problem	O
only	O
involves	O
inner	O
products	O
between	O
instances	O
and	O
does	O
not	O
require	O
direct	O
access	O
to	O
specific	O
elements	O
within	O
an	O
instance	B
this	O
property	O
is	O
important	O
when	O
implementing	O
svm	B
with	O
kernels	B
as	O
we	O
will	O
discuss	O
in	O
the	O
next	O
chapter	O
implementing	O
soft-svm	B
using	O
sgd	B
in	O
this	O
section	O
we	O
describe	O
a	O
very	O
simple	O
algorithm	O
for	O
solving	O
the	O
optimization	O
problem	O
of	O
soft-svm	B
namely	O
min	O
w	O
m	O
we	O
rely	O
on	O
the	O
sgd	B
framework	O
for	O
solving	O
regularized	O
loss	B
minimization	O
problems	O
as	O
described	O
in	O
section	O
recall	B
that	O
on	O
the	O
basis	O
of	O
equation	O
we	O
can	O
rewrite	O
the	O
update	O
rule	O
of	O
sgd	B
as	O
t	O
vj	O
where	O
vj	O
is	O
a	O
subgradient	O
of	O
the	O
loss	B
function	B
at	O
wj	O
on	O
the	O
random	O
example	O
chosen	O
at	O
iteration	O
j	O
for	O
the	O
hinge	B
loss	B
given	O
an	O
example	O
y	O
we	O
can	O
choose	O
vj	O
to	O
be	O
if	O
and	O
vj	O
y	O
x	O
otherwise	O
example	O
denoting	O
jt	O
vj	O
we	O
obtain	O
the	O
following	O
procedure	O
summary	O
sgd	B
for	O
solving	O
soft-svm	B
goal	O
solve	O
equation	O
parameter	O
t	O
initialize	O
for	O
t	O
t	O
t	O
let	O
wt	O
choose	O
i	O
uniformly	O
at	O
random	O
from	O
if	O
set	B
yixi	O
else	O
set	B
wt	O
output	O
w	O
t	O
summary	O
svm	B
is	O
an	O
algorithm	O
for	O
learning	O
halfspaces	O
with	O
a	O
certain	O
type	O
of	O
prior	B
knowledge	I
namely	O
preference	O
for	O
large	O
margin	B
hard-svm	B
seeks	O
the	O
halfspace	B
that	O
separates	O
the	O
data	O
perfectly	O
with	O
the	O
largest	O
margin	B
whereas	O
soft-svm	B
does	O
not	O
assume	O
separability	O
of	O
the	O
data	O
and	O
allows	O
the	O
constraints	O
to	O
be	O
violated	O
to	O
some	O
extent	O
the	O
sample	B
complexity	I
for	O
both	O
types	O
of	O
svm	B
is	O
different	O
from	O
the	O
sample	B
complexity	I
of	O
straightforward	O
halfspace	B
learning	O
as	O
it	O
does	O
not	O
depend	O
on	O
the	O
dimension	O
of	O
the	O
domain	B
but	O
rather	O
on	O
parameters	O
such	O
as	O
the	O
maximal	O
norms	O
of	O
x	O
and	O
w	O
the	O
importance	O
of	O
dimension-independent	O
sample	B
complexity	I
will	O
be	O
realized	O
in	O
the	O
next	O
chapter	O
where	O
we	O
will	O
discuss	O
the	O
embedding	O
of	O
the	O
given	O
domain	B
into	O
some	O
high	O
dimensional	O
feature	B
space	I
as	O
means	O
for	O
enriching	O
our	O
hypothesis	B
class	I
such	O
a	O
procedure	O
raises	O
computational	O
and	O
sample	B
complexity	I
problems	O
the	O
latter	O
is	O
solved	O
by	O
using	O
svm	B
whereas	O
the	O
former	O
can	O
be	O
solved	O
by	O
using	O
svm	B
with	O
kernels	B
as	O
we	O
will	O
see	O
in	O
the	O
next	O
chapter	O
bibliographic	O
remarks	O
svms	O
have	O
been	O
introduced	O
in	O
vapnik	O
boser	O
guyon	O
vapnik	O
there	O
are	O
many	O
good	O
books	O
on	O
the	O
theoretical	O
and	O
practical	O
aspects	O
of	O
svms	O
for	O
example	O
cristianini	O
shawe-taylor	O
sch	O
olkopf	O
smola	O
hsu	O
chang	O
lin	O
steinwart	O
christmann	O
using	O
sgd	B
for	O
solving	O
soft-svm	B
has	O
been	O
proposed	O
in	O
shalev-shwartz	O
et	O
al	O
support	O
vector	O
machines	O
exercises	O
show	O
that	O
the	O
hard-svm	B
rule	O
namely	O
b	O
argmax	O
min	O
i	O
s	O
t	O
i	O
b	O
is	O
equivalent	O
to	O
the	O
following	O
formulation	O
argmax	O
min	O
i	O
b	O
hint	O
define	O
g	O
b	O
i	O
b	O
show	O
that	O
argmax	O
min	O
i	O
show	O
that	O
b	O
g	O
b	O
g	O
min	O
i	O
b	O
min	O
i	O
b	O
margin	B
and	O
the	O
perceptron	B
consider	O
a	O
training	B
set	B
that	O
is	O
linearly	O
separable	B
with	O
a	O
margin	B
and	O
such	O
that	O
all	O
the	O
instances	O
are	O
within	O
a	O
ball	O
of	O
radius	O
prove	O
that	O
the	O
maximal	O
number	O
of	O
updates	O
the	O
batch	O
perceptron	B
algorithm	O
given	O
in	O
section	O
will	O
make	O
when	O
running	O
on	O
this	O
training	B
set	B
is	O
hard	O
versus	O
soft	O
svm	B
prove	O
or	O
refute	O
the	O
following	O
claim	O
there	O
exists	O
such	O
that	O
for	O
every	O
sample	O
s	O
of	O
m	O
examples	O
which	O
is	O
separable	B
by	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
the	O
hard-svm	B
and	O
the	O
soft-svm	B
parameter	O
learning	O
rules	O
return	O
exactly	O
the	O
same	O
weight	O
vector	O
weak	B
duality	B
prove	O
that	O
for	O
any	O
function	B
f	O
of	O
two	O
vector	O
variables	O
x	O
x	O
y	O
y	O
it	O
holds	O
that	O
y	O
y	O
f	O
y	O
max	O
x	O
x	O
max	O
min	O
y	O
y	O
min	O
x	O
x	O
f	O
y	O
kernel	O
methods	O
in	O
the	O
previous	O
chapter	O
we	O
described	O
the	O
svm	B
paradigm	O
for	O
learning	O
halfspaces	O
in	O
high	O
dimensional	O
feature	B
spaces	O
this	O
enables	O
us	O
to	O
enrich	O
the	O
expressive	O
power	O
of	O
halfspaces	O
by	O
first	O
mapping	O
the	O
data	O
into	O
a	O
high	O
dimensional	O
feature	B
space	I
and	O
then	O
learning	O
a	O
linear	B
predictor	B
in	O
that	O
space	O
this	O
is	O
similar	O
to	O
the	O
adaboost	B
algorithm	O
which	O
learns	O
a	O
composition	O
of	O
a	O
halfspace	B
over	O
base	O
hypotheses	O
while	O
this	O
approach	O
greatly	O
extends	O
the	O
expressiveness	O
of	O
halfspace	B
predictors	O
it	O
raises	O
both	O
sample	B
complexity	I
and	O
computational	B
complexity	I
challenges	O
in	O
the	O
previous	O
chapter	O
we	O
tackled	O
the	O
sample	B
complexity	I
issue	O
using	O
the	O
concept	O
of	O
margin	B
in	O
this	O
chapter	O
we	O
tackle	O
the	O
computational	B
complexity	I
challenge	O
using	O
the	O
method	O
of	O
kernels	B
we	O
start	O
the	O
chapter	O
by	O
describing	O
the	O
idea	O
of	O
embedding	O
the	O
data	O
into	O
a	O
high	O
dimensional	O
feature	B
space	I
we	O
then	O
introduce	O
the	O
idea	O
of	O
kernels	B
a	O
kernel	O
is	O
a	O
type	O
of	O
a	O
similarity	O
measure	O
between	O
instances	O
the	O
special	O
property	O
of	O
kernel	O
similarities	O
is	O
that	O
they	O
can	O
be	O
viewed	O
as	O
inner	O
products	O
in	O
some	O
hilbert	B
space	I
euclidean	O
space	O
of	O
some	O
high	O
dimension	O
to	O
which	O
the	O
instance	B
space	I
is	O
virtually	O
embedded	O
we	O
introduce	O
the	O
kernel	B
trick	I
that	O
enables	O
computationally	O
efficient	O
implementation	O
of	O
learning	O
without	O
explicitly	O
handling	O
the	O
high	O
dimensional	O
representation	O
of	O
the	O
domain	B
instances	O
kernel	O
based	O
learning	O
algorithms	O
and	O
in	O
particular	O
kernel-svm	O
are	O
very	O
useful	O
and	O
popular	O
machine	O
learning	O
tools	O
their	O
success	O
may	O
be	O
attributed	O
both	O
to	O
being	O
flexible	O
for	O
accommodating	O
domain	B
specific	O
prior	B
knowledge	I
and	O
to	O
having	O
a	O
well	O
developed	O
set	B
of	O
efficient	O
implementation	O
algorithms	O
embeddings	O
into	O
feature	B
spaces	O
the	O
expressive	O
power	O
of	O
halfspaces	O
is	O
rather	O
restricted	O
for	O
example	O
the	O
following	O
training	B
set	B
is	O
not	O
separable	B
by	O
a	O
halfspace	B
let	O
the	O
domain	B
be	O
the	O
real	O
line	O
consider	O
the	O
domain	B
points	O
where	O
the	O
labels	O
are	O
for	O
all	O
x	O
such	O
that	O
and	O
otherwise	O
to	O
make	O
the	O
class	O
of	O
halfspaces	O
more	O
expressive	O
we	O
can	O
first	O
map	O
the	O
original	O
instance	B
space	I
into	O
another	O
space	O
of	O
a	O
higher	O
dimension	O
and	O
then	O
learn	O
a	O
halfspace	B
in	O
that	O
space	O
for	O
example	O
consider	O
the	O
example	O
mentioned	O
previously	O
instead	O
of	O
learning	O
a	O
halfspace	B
in	O
the	O
original	O
representation	O
let	O
us	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
kernel	O
methods	O
first	O
define	O
a	O
mapping	O
r	O
as	O
follows	O
we	O
use	O
the	O
term	O
feature	B
space	I
to	O
denote	O
the	O
range	O
of	O
after	O
applying	O
the	O
data	O
can	O
be	O
easily	O
explained	O
using	O
the	O
halfspace	B
hx	O
b	O
where	O
w	O
and	O
b	O
the	O
basic	O
paradigm	O
is	O
as	O
follows	O
given	O
some	O
domain	B
set	B
x	O
and	O
a	O
learning	O
task	O
choose	O
a	O
mapping	O
x	O
f	O
for	O
some	O
feature	B
space	I
f	O
that	O
will	O
usually	O
be	O
rn	O
for	O
some	O
n	O
the	O
range	O
of	O
such	O
a	O
mapping	O
can	O
be	O
any	O
hilbert	B
space	I
including	O
such	O
spaces	O
of	O
infinite	O
dimension	O
as	O
we	O
will	O
show	O
later	O
given	O
a	O
sequence	O
of	O
labeled	O
examples	O
s	O
ym	O
create	O
the	O
image	O
sequence	O
s	O
ym	O
train	O
a	O
linear	B
predictor	B
h	O
over	O
s	O
predict	O
the	O
label	B
of	O
a	O
test	O
point	O
x	O
to	O
be	O
h	O
note	O
that	O
for	O
every	O
probability	O
distribution	O
d	O
over	O
x	O
y	O
we	O
can	O
readily	O
define	O
its	O
image	O
probability	O
distribution	O
d	O
over	O
f	O
y	O
by	O
setting	O
for	O
every	O
subset	O
a	O
f	O
y	O
d	O
d	O
it	O
follows	O
that	O
for	O
every	O
predictor	B
h	O
over	O
the	O
feature	B
space	I
ld	O
ldh	O
where	O
h	O
is	O
the	O
composition	O
of	O
h	O
onto	O
the	O
success	O
of	O
this	O
learning	O
paradigm	O
depends	O
on	O
choosing	O
a	O
good	O
for	O
a	O
given	O
learning	O
task	O
that	O
is	O
a	O
that	O
will	O
make	O
the	O
image	O
of	O
the	O
data	O
distribution	O
to	O
being	O
linearly	O
separable	B
in	O
the	O
feature	B
space	I
thus	O
making	O
the	O
resulting	O
algorithm	O
a	O
good	O
learner	O
for	O
a	O
given	O
task	O
picking	O
such	O
an	O
embedding	O
requires	O
prior	B
knowledge	I
about	O
that	O
task	O
however	O
often	O
some	O
generic	O
mappings	O
that	O
enable	O
us	O
to	O
enrich	O
the	O
class	O
of	O
halfspaces	O
and	O
extend	O
its	O
expressiveness	O
are	O
used	O
one	O
notable	O
example	O
is	O
polynomial	O
mappings	O
which	O
are	O
a	O
generalization	O
of	O
the	O
we	O
have	O
seen	O
in	O
the	O
previous	O
example	O
recall	B
that	O
the	O
prediction	O
of	O
a	O
standard	O
halfspace	B
classifier	B
on	O
an	O
instance	B
x	O
is	O
based	O
on	O
the	O
linear	O
mapping	O
x	O
we	O
can	O
generalize	O
linear	O
mappings	O
to	O
a	O
polynomial	O
mapping	O
x	O
px	O
where	O
p	O
is	O
a	O
multivariate	O
polynomial	O
of	O
degree	O
k	O
for	O
simplicity	O
consider	O
first	O
the	O
case	O
in	O
which	O
x	O
is	O
dimensional	O
wjxj	O
where	O
w	O
is	O
the	O
vector	O
of	O
coefficients	O
of	O
the	O
polynomial	O
we	O
need	O
to	O
learn	O
we	O
can	O
rewrite	O
px	O
where	O
r	O
is	O
the	O
mapping	O
x	O
x	O
xk	O
it	O
follows	O
that	O
learning	O
a	O
k	O
degree	O
polynomial	O
over	O
r	O
can	O
be	O
done	O
by	O
learning	O
a	O
linear	O
mapping	O
in	O
the	O
dimensional	O
feature	B
space	I
in	O
that	O
case	O
px	O
more	O
generally	O
a	O
degree	O
k	O
multivariate	O
polynomial	O
from	O
rn	O
to	O
r	O
can	O
be	O
writ	O
ten	O
as	O
px	O
wj	O
xji	O
j	O
k	O
this	O
is	O
defined	O
for	O
every	O
a	O
such	O
that	O
is	O
measurable	O
with	O
respect	O
to	O
d	O
the	O
kernel	B
trick	I
as	O
before	O
we	O
can	O
rewrite	O
px	O
where	O
now	O
rn	O
rd	O
is	O
such	O
that	O
for	O
every	O
j	O
r	O
k	O
the	O
coordinate	O
of	O
associated	O
with	O
j	O
is	O
the	O
xji	O
naturally	O
polynomial-based	O
classifiers	O
yield	O
much	O
richer	O
hypothesis	B
classes	O
than	O
halfspaces	O
we	O
have	O
seen	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
an	O
example	O
in	O
which	O
the	O
training	B
set	B
in	O
its	O
original	O
domain	B
r	O
cannot	O
be	O
separable	B
by	O
a	O
halfspace	B
but	O
after	O
the	O
embedding	O
x	O
it	O
is	O
perfectly	O
separable	B
so	O
while	O
the	O
classifier	B
is	O
always	O
linear	O
in	O
the	O
feature	B
space	I
it	O
can	O
have	O
highly	O
nonlinear	O
behavior	O
on	O
the	O
original	O
space	O
from	O
which	O
instances	O
were	O
sampled	O
in	O
general	O
we	O
can	O
choose	O
any	O
feature	B
mapping	O
that	O
maps	O
the	O
original	O
instances	O
into	O
some	O
hilbert	O
the	O
euclidean	O
space	O
rd	O
is	O
a	O
hilbert	B
space	I
for	O
any	O
finite	O
d	O
but	O
there	O
are	O
also	O
infinite	O
dimensional	O
hilbert	O
spaces	O
we	O
shall	O
see	O
later	O
on	O
in	O
this	O
chapter	O
the	O
bottom	O
line	O
of	O
this	O
discussion	O
is	O
that	O
we	O
can	O
enrich	O
the	O
class	O
of	O
halfspaces	O
by	O
first	O
applying	O
a	O
nonlinear	O
mapping	O
that	O
maps	O
the	O
instance	B
space	I
into	O
some	O
feature	B
space	I
and	O
then	O
learning	O
a	O
halfspace	B
in	O
that	O
feature	B
space	I
however	O
if	O
the	O
range	O
of	O
is	O
a	O
high	O
dimensional	O
space	O
we	O
face	O
two	O
problems	O
first	O
the	O
vcdimension	O
of	O
halfspaces	O
in	O
rn	O
is	O
n	O
and	O
therefore	O
if	O
the	O
range	O
of	O
is	O
very	O
large	O
we	O
need	O
many	O
more	O
samples	O
in	O
order	O
to	O
learn	O
a	O
halfspace	B
in	O
the	O
range	O
of	O
second	O
from	O
the	O
computational	O
point	O
of	O
view	O
performing	O
calculations	O
in	O
the	O
high	O
dimensional	O
space	O
might	O
be	O
too	O
costly	O
in	O
fact	O
even	O
the	O
representation	O
of	O
the	O
vector	O
w	O
in	O
the	O
feature	B
space	I
can	O
be	O
unrealistic	O
the	O
first	O
issue	O
can	O
be	O
tackled	O
using	O
the	O
paradigm	O
of	O
large	O
margin	B
low	O
norm	O
predictors	O
as	O
we	O
already	O
discussed	O
in	O
the	O
previous	O
chapter	O
in	O
the	O
context	O
of	O
the	O
svm	B
algorithm	O
in	O
the	O
following	O
section	O
we	O
address	O
the	O
computational	O
issue	O
the	O
kernel	B
trick	I
we	O
have	O
seen	O
that	O
embedding	O
the	O
input	O
space	O
into	O
some	O
high	O
dimensional	O
feature	B
space	I
makes	O
halfspace	B
learning	O
more	O
expressive	O
however	O
the	O
computational	B
complexity	I
of	O
such	O
learning	O
may	O
still	O
pose	O
a	O
serious	O
hurdle	O
computing	O
linear	O
separators	O
over	O
very	O
high	O
dimensional	O
data	O
may	O
be	O
computationally	O
expensive	O
the	O
common	O
solution	O
to	O
this	O
concern	O
is	O
kernel	O
based	O
learning	O
the	O
term	O
kernels	B
is	O
used	O
in	O
this	O
context	O
to	O
describe	O
inner	O
products	O
in	O
the	O
feature	B
space	I
given	O
an	O
embedding	O
of	O
some	O
domain	B
space	O
x	O
into	O
some	O
hilbert	B
space	I
we	O
define	O
the	O
kernel	O
function	B
kx	O
one	O
can	O
think	O
of	O
k	O
as	O
specifying	O
similarity	O
between	O
instances	O
and	O
of	O
the	O
embedding	O
as	O
mapping	O
the	O
domain	B
set	B
a	O
hilbert	B
space	I
is	O
a	O
vector	O
space	O
with	O
an	O
inner	O
product	O
which	O
is	O
also	O
complete	O
a	O
space	O
is	O
in	O
our	O
case	O
the	O
norm	O
is	O
defined	O
by	O
the	O
inner	O
the	O
reason	O
we	O
require	O
complete	O
if	O
all	O
cauchy	O
sequences	O
in	O
the	O
space	O
converge	O
the	O
range	O
of	O
to	O
be	O
in	O
a	O
hilbert	B
space	I
is	O
that	O
projections	O
in	O
a	O
hilbert	B
space	I
are	O
well	O
defined	O
in	O
particular	O
if	O
m	O
is	O
a	O
linear	O
subspace	O
of	O
a	O
hilbert	B
space	I
then	O
every	O
x	O
in	O
the	O
hilbert	B
space	I
can	O
be	O
written	O
as	O
a	O
sum	O
x	O
u	O
v	O
where	O
u	O
m	O
and	O
for	O
all	O
w	O
m	O
we	O
use	O
this	O
fact	O
in	O
the	O
proof	O
of	O
the	O
representer	B
theorem	I
given	O
in	O
the	O
next	O
section	O
kernel	O
methods	O
x	O
into	O
a	O
space	O
where	O
these	O
similarities	O
are	O
realized	O
as	O
inner	O
products	O
it	O
turns	O
out	O
that	O
many	O
learning	O
algorithms	O
for	O
halfspaces	O
can	O
be	O
carried	O
out	O
just	O
on	O
the	O
basis	O
of	O
the	O
values	O
of	O
the	O
kernel	O
function	B
over	O
pairs	O
of	O
domain	B
points	O
the	O
main	O
advantage	O
of	O
such	O
algorithms	O
is	O
that	O
they	O
implement	O
linear	O
separators	O
in	O
high	O
dimensional	O
feature	B
spaces	O
without	O
having	O
to	O
specify	O
points	O
in	O
that	O
space	O
or	O
expressing	O
the	O
embedding	O
explicitly	O
the	O
remainder	O
of	O
this	O
section	O
is	O
devoted	O
to	O
constructing	O
such	O
algorithms	O
in	O
the	O
previous	O
chapter	O
we	O
saw	O
that	O
regularizing	O
the	O
norm	O
of	O
w	O
yields	O
a	O
small	O
sample	B
complexity	I
even	O
if	O
the	O
dimensionality	O
of	O
the	O
feature	B
space	I
is	O
high	O
interestingly	O
as	O
we	O
show	O
later	O
regularizing	O
the	O
norm	O
of	O
w	O
is	O
also	O
helpful	O
in	O
overcoming	O
the	O
computational	O
problem	O
to	O
do	O
so	O
first	O
note	O
that	O
all	O
versions	O
of	O
the	O
svm	B
optimization	O
problem	O
we	O
have	O
derived	O
in	O
the	O
previous	O
chapter	O
are	O
instances	O
of	O
the	O
following	O
general	O
problem	O
min	O
w	O
where	O
f	O
rm	O
r	O
is	O
an	O
arbitrary	O
function	B
and	O
r	O
r	O
r	O
is	O
a	O
cally	O
nondecreasing	O
function	B
for	O
example	O
soft-svm	B
for	O
homogenous	B
halfspaces	O
can	O
be	O
derived	O
from	O
equation	O
by	O
letting	O
ra	O
and	O
i	O
yiai	O
similarly	O
hard-svm	B
for	O
nonhomogenous	O
f	O
am	O
m	O
halfspaces	O
can	O
be	O
derived	O
from	O
equation	O
by	O
letting	O
ra	O
and	O
letting	O
f	O
am	O
be	O
if	O
there	O
exists	O
b	O
such	O
that	O
yiai	O
for	O
all	O
i	O
and	O
f	O
am	O
otherwise	O
tion	O
that	O
lies	O
in	O
the	O
span	O
of	O
theorem	O
theorem	O
assume	O
that	O
is	O
a	O
mapping	O
from	O
x	O
to	O
i	O
a	O
hilbert	B
space	I
then	O
there	O
exists	O
a	O
vector	O
rm	O
such	O
that	O
w	O
the	O
following	O
theorem	O
shows	O
that	O
there	O
exists	O
an	O
optimal	O
solution	O
of	O
equa	O
is	O
an	O
optimal	O
solution	O
of	O
equation	O
proof	O
let	O
be	O
an	O
optimal	O
solution	O
of	O
equation	O
because	O
is	O
an	O
element	O
of	O
a	O
hilbert	B
space	I
we	O
can	O
rewrite	O
as	O
i	O
u	O
where	O
for	O
all	O
i	O
set	B
w	O
u	O
clearly	O
thus	O
since	O
r	O
is	O
nondecreasing	O
we	O
obtain	O
that	O
additionally	O
for	O
all	O
i	O
we	O
have	O
that	O
u	O
hence	O
f	O
f	O
w	O
we	O
have	O
shown	O
that	O
the	O
objective	O
of	O
equation	O
at	O
w	O
cannot	O
be	O
larger	O
than	O
the	O
objective	O
at	O
and	O
therefore	O
w	O
is	O
also	O
an	O
optimal	O
solution	O
since	O
i	O
we	O
conclude	O
our	O
proof	O
min	O
rm	O
f	O
r	O
jkxj	O
jkxj	O
xm	O
the	O
kernel	B
trick	I
w	O
on	O
the	O
basis	O
of	O
the	O
representer	B
theorem	I
we	O
can	O
optimize	O
equation	O
with	O
respect	O
to	O
the	O
coefficients	O
instead	O
of	O
the	O
coefficients	O
w	O
as	O
follows	O
writing	O
j	O
we	O
have	O
that	O
for	O
all	O
i	O
j	O
similarly	O
j	O
j	O
j	O
j	O
j	O
i	O
let	O
kx	O
be	O
a	O
function	B
that	O
implements	O
the	O
kernel	O
function	B
with	O
respect	O
to	O
the	O
embedding	O
instead	O
of	O
solving	O
equation	O
we	O
can	O
solve	O
the	O
equivalent	O
problem	O
i	O
jkxj	O
xi	O
to	O
solve	O
the	O
optimization	O
problem	O
given	O
in	O
equation	O
we	O
do	O
not	O
need	O
any	O
direct	O
access	O
to	O
elements	O
in	O
the	O
feature	B
space	I
the	O
only	O
thing	O
we	O
should	O
know	O
is	O
how	O
to	O
calculate	O
inner	O
products	O
in	O
the	O
feature	B
space	I
or	O
equivalently	O
to	O
calculate	O
the	O
kernel	O
function	B
in	O
fact	O
to	O
solve	O
equation	O
we	O
solely	O
need	O
to	O
know	O
the	O
value	O
of	O
the	O
m	O
m	O
matrix	O
g	O
s	O
t	O
gij	O
kxi	O
xj	O
which	O
is	O
often	O
called	O
the	O
gram	B
matrix	I
in	O
particular	O
specifying	O
the	O
preceding	O
to	O
the	O
soft-svm	B
problem	O
given	O
in	O
equa	O
tion	O
we	O
can	O
rewrite	O
the	O
problem	O
as	O
min	O
rm	O
t	O
g	O
yig	O
m	O
where	O
is	O
the	O
i	O
th	O
element	O
of	O
the	O
vector	O
obtained	O
by	O
multiplying	O
the	O
gram	B
matrix	I
g	O
by	O
the	O
vector	O
note	O
that	O
equation	O
can	O
be	O
written	O
as	O
quadratic	O
programming	O
and	O
hence	O
can	O
be	O
solved	O
efficiently	O
in	O
the	O
next	O
section	O
we	O
describe	O
an	O
even	O
simpler	O
algorithm	O
for	O
solving	O
soft-svm	B
with	O
kernels	B
once	O
we	O
learn	O
the	O
coefficients	O
we	O
can	O
calculate	O
the	O
prediction	O
on	O
a	O
new	O
instance	B
by	O
jkxj	O
x	O
the	O
advantage	O
of	O
working	O
with	O
kernels	B
rather	O
than	O
directly	O
optimizing	O
w	O
in	O
the	O
feature	B
space	I
is	O
that	O
in	O
some	O
situations	O
the	O
dimension	O
of	O
the	O
feature	B
space	I
kernel	O
methods	O
is	O
extremely	O
large	O
while	O
implementing	O
the	O
kernel	O
function	B
is	O
very	O
simple	O
a	O
few	O
examples	O
are	O
given	O
in	O
the	O
following	O
example	O
kernels	B
the	O
k	O
degree	O
polynomial	B
kernel	I
is	O
defined	O
to	O
be	O
kx	O
now	O
we	O
will	O
show	O
that	O
this	O
is	O
indeed	O
a	O
kernel	O
function	B
that	O
is	O
we	O
will	O
show	O
that	O
there	O
exists	O
a	O
mapping	O
from	O
the	O
original	O
space	O
to	O
some	O
higher	O
dimensional	O
space	O
for	O
which	O
kx	O
for	O
simplicity	O
denote	O
then	O
we	O
have	O
kx	O
j	O
element	O
of	O
that	O
j	O
now	O
if	O
we	O
define	O
rn	O
j	O
xji	O
j	O
ji	O
ji	O
such	O
that	O
for	O
j	O
nk	O
there	O
is	O
an	O
xji	O
we	O
obtain	O
that	O
kx	O
since	O
contains	O
all	O
the	O
monomials	O
up	O
to	O
degree	O
k	O
a	O
halfspace	B
over	O
the	O
range	O
of	O
corresponds	O
to	O
a	O
polynomial	O
predictor	B
of	O
degree	O
k	O
over	O
the	O
original	O
space	O
hence	O
learning	O
a	O
halfspace	B
with	O
a	O
k	O
degree	O
polynomial	B
kernel	I
enables	O
us	O
to	O
learn	O
polynomial	O
predictors	O
of	O
degree	O
k	O
over	O
the	O
original	O
space	O
note	O
that	O
here	O
the	O
complexity	O
of	O
implementing	O
k	O
is	O
on	O
while	O
the	O
dimension	O
of	O
the	O
feature	B
space	I
is	O
on	O
the	O
order	O
of	O
nk	O
example	O
kernel	O
let	O
the	O
original	O
instance	B
space	I
be	O
r	O
and	O
consider	O
the	O
mapping	O
where	O
for	O
each	O
nonnegative	O
integer	O
n	O
there	O
exists	O
an	O
element	O
that	O
equals	O
xn	O
then	O
e	O
e	O
n	O
n	O
e	O
e	O
xn	O
e	O
n	O
n	O
here	O
the	O
feature	B
space	I
is	O
of	O
infinite	O
dimension	O
while	O
evaluating	O
the	O
kernel	O
is	O
very	O
the	O
kernel	B
trick	I
simple	O
more	O
generally	O
given	O
a	O
scalar	O
the	O
gaussian	B
kernel	I
is	O
defined	O
to	O
be	O
kx	O
e	O
intuitively	O
the	O
gaussian	B
kernel	I
sets	O
the	O
inner	O
product	O
in	O
the	O
feature	B
space	I
between	O
x	O
to	O
be	O
close	O
to	O
zero	O
if	O
the	O
instances	O
are	O
far	O
away	O
from	O
each	O
other	O
the	O
original	O
domain	B
and	O
close	O
to	O
if	O
they	O
are	O
close	O
is	O
a	O
parameter	O
that	O
controls	O
the	O
scale	O
determining	O
what	O
we	O
mean	O
by	O
close	O
it	O
is	O
easy	O
to	O
verify	O
that	O
k	O
implements	O
an	O
inner	O
product	O
in	O
a	O
space	O
in	O
which	O
for	O
any	O
n	O
and	O
any	O
monomial	O
of	O
order	O
k	O
there	O
exists	O
an	O
element	O
of	O
that	O
equals	O
xji	O
hence	O
we	O
can	O
learn	O
any	O
polynomial	O
predictor	B
over	O
the	O
original	O
space	O
by	O
using	O
a	O
gaussian	B
kernel	I
e	O
n	O
recall	B
that	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
all	O
polynomial	O
predictors	O
is	O
infinite	O
exercise	O
there	O
is	O
no	O
contradiction	O
because	O
the	O
sample	B
complexity	I
required	O
to	O
learn	O
with	O
gaussian	O
kernels	B
depends	O
on	O
the	O
margin	B
in	O
the	O
feature	B
space	I
which	O
will	O
be	O
large	O
if	O
we	O
are	O
lucky	O
but	O
can	O
in	O
general	O
be	O
arbitrarily	O
small	O
the	O
gaussian	B
kernel	I
is	O
also	O
called	O
the	O
rbf	B
kernel	I
for	O
radial	O
basis	O
func	O
tions	O
kernels	B
as	O
a	O
way	O
to	O
express	O
prior	B
knowledge	I
as	O
we	O
discussed	O
previously	O
a	O
feature	B
mapping	O
may	O
be	O
viewed	O
as	O
expanding	O
the	O
class	O
of	O
linear	O
classifiers	O
to	O
a	O
richer	O
class	O
to	O
linear	O
classifiers	O
over	O
the	O
feature	B
space	I
however	O
as	O
discussed	O
in	O
the	O
book	O
so	O
far	O
the	O
suitability	O
of	O
any	O
hypothesis	B
class	I
to	O
a	O
given	O
learning	O
task	O
depends	O
on	O
the	O
nature	O
of	O
that	O
task	O
one	O
can	O
therefore	O
think	O
of	O
an	O
embedding	O
as	O
a	O
way	O
to	O
express	O
and	O
utilize	O
prior	B
knowledge	I
about	O
the	O
problem	O
at	O
hand	O
for	O
example	O
if	O
we	O
believe	O
that	O
positive	O
examples	O
can	O
be	O
distinguished	O
by	O
some	O
ellipse	O
we	O
can	O
define	O
to	O
be	O
all	O
the	O
monomials	O
up	O
to	O
order	O
or	O
use	O
a	O
degree	O
polynomial	B
kernel	I
as	O
a	O
more	O
realistic	O
example	O
consider	O
the	O
task	O
of	O
learning	O
to	O
find	O
a	O
sequence	O
of	O
characters	O
signature	O
in	O
a	O
file	O
that	O
indicates	O
whether	O
it	O
contains	O
a	O
virus	O
or	O
not	O
formally	O
let	O
xd	O
be	O
the	O
set	B
of	O
all	O
strings	O
of	O
length	O
at	O
most	O
d	O
over	O
some	O
alphabet	O
set	B
the	O
hypothesis	B
class	I
that	O
one	O
wishes	O
to	O
learn	O
is	O
h	O
v	O
xd	O
where	O
for	O
a	O
string	O
x	O
xd	O
hvx	O
is	O
iff	O
v	O
is	O
a	O
substring	O
of	O
x	O
hvx	O
otherwise	O
let	O
us	O
show	O
how	O
using	O
an	O
appropriate	O
embedding	O
this	O
class	O
can	O
be	O
realized	O
by	O
linear	O
classifiers	O
over	O
the	O
resulting	O
feature	B
space	I
consider	O
a	O
mapping	O
to	O
a	O
space	O
rs	O
where	O
s	O
so	O
that	O
each	O
coordinate	O
of	O
corresponds	O
to	O
some	O
string	O
v	O
and	O
indicates	O
whether	O
v	O
is	O
a	O
substring	O
of	O
x	O
is	O
for	O
every	O
x	O
xd	O
is	O
a	O
vector	O
in	O
note	O
that	O
the	O
dimension	O
of	O
this	O
feature	B
space	I
is	O
exponential	O
in	O
d	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
every	O
member	O
of	O
the	O
class	O
h	O
can	O
be	O
realized	O
by	O
composing	O
a	O
linear	O
classifier	B
over	O
and	O
moreover	O
by	O
such	O
a	O
halfspace	B
whose	O
norm	O
is	O
and	O
that	O
attains	O
a	O
margin	B
of	O
exercise	O
furthermore	O
for	O
every	O
x	O
x	O
od	O
so	O
overall	O
it	O
is	O
learnable	O
using	O
svm	B
with	O
a	O
sample	O
kernel	O
methods	O
complexity	O
that	O
is	O
polynomial	O
in	O
d	O
however	O
the	O
dimension	O
of	O
the	O
feature	B
space	I
is	O
exponential	O
in	O
d	O
so	O
a	O
direct	O
implementation	O
of	O
svm	B
over	O
the	O
feature	B
space	I
is	O
problematic	O
luckily	O
it	O
is	O
easy	O
to	O
calculate	O
the	O
inner	O
product	O
in	O
the	O
feature	B
space	I
the	O
kernel	O
function	B
without	O
explicitly	O
mapping	O
instances	O
into	O
the	O
feature	B
space	I
indeed	O
kx	O
is	O
simply	O
the	O
number	O
of	O
common	O
substrings	O
of	O
x	O
and	O
which	O
can	O
be	O
easily	O
calculated	O
in	O
time	O
polynomial	O
in	O
d	O
this	O
example	O
also	O
demonstrates	O
how	O
feature	B
mapping	O
enables	O
us	O
to	O
use	O
halfspaces	O
for	O
nonvectorial	O
domains	O
characterizing	O
kernel	O
functions	O
as	O
we	O
have	O
discussed	O
in	O
the	O
previous	O
section	O
we	O
can	O
think	O
of	O
the	O
specification	O
of	O
the	O
kernel	O
matrix	O
as	O
a	O
way	O
to	O
express	O
prior	B
knowledge	I
consider	O
a	O
given	O
similarity	O
function	B
of	O
the	O
form	O
k	O
x	O
x	O
r	O
is	O
it	O
a	O
valid	O
kernel	O
function	B
that	O
is	O
does	O
it	O
represent	O
an	O
inner	O
product	O
between	O
and	O
for	O
some	O
feature	B
mapping	O
the	O
following	O
lemma	O
gives	O
a	O
sufficient	O
and	O
necessary	O
condition	O
lemma	O
a	O
symmetric	O
function	B
k	O
x	O
x	O
r	O
implements	O
an	O
inner	O
product	O
in	O
some	O
hilbert	B
space	I
if	O
and	O
only	O
if	O
it	O
is	O
positive	O
semidefinite	O
namely	O
for	O
all	O
xm	O
the	O
gram	B
matrix	I
gij	O
kxi	O
xj	O
is	O
a	O
positive	O
semidefinite	O
matrix	O
proof	O
it	O
is	O
trivial	O
to	O
see	O
that	O
if	O
k	O
implements	O
an	O
inner	O
product	O
in	O
some	O
hilbert	B
space	I
then	O
the	O
gram	B
matrix	I
is	O
positive	O
semidefinite	O
for	O
the	O
other	O
direction	O
define	O
the	O
space	O
of	O
functions	O
over	O
x	O
as	O
rx	O
x	O
r	O
for	O
each	O
x	O
x	O
let	O
be	O
the	O
function	B
x	O
k	O
x	O
define	O
a	O
vector	O
space	O
by	O
taking	O
all	O
linear	O
combinations	O
of	O
elements	O
of	O
the	O
form	O
k	O
x	O
define	O
an	O
inner	O
product	O
on	O
this	O
vector	O
space	O
to	O
ik	O
xi	O
jk	O
j	O
i	O
jkxi	O
j	O
i	O
j	O
ij	O
this	O
is	O
a	O
valid	O
inner	O
product	O
since	O
it	O
is	O
symmetric	O
k	O
is	O
symmetric	O
it	O
is	O
linear	O
and	O
it	O
is	O
positive	O
definite	O
is	O
easy	O
to	O
see	O
that	O
kx	O
x	O
with	O
equality	O
only	O
for	O
being	O
the	O
zero	O
function	B
clearly	O
x	O
k	O
kx	O
which	O
concludes	O
our	O
proof	O
implementing	O
soft-svm	B
with	O
kernels	B
next	O
we	O
turn	O
to	O
solving	O
soft-svm	B
with	O
kernels	B
while	O
we	O
could	O
have	O
designed	O
an	O
algorithm	O
for	O
solving	O
equation	O
there	O
is	O
an	O
even	O
simpler	O
approach	O
that	O
implementing	O
soft-svm	B
with	O
kernels	B
directly	O
tackles	O
the	O
soft-svm	B
optimization	O
problem	O
in	O
the	O
feature	B
space	I
min	O
w	O
m	O
while	O
only	O
using	O
kernel	O
evaluations	O
the	O
basic	O
observation	O
is	O
that	O
the	O
vector	O
wt	O
maintained	O
by	O
the	O
sgd	B
procedure	O
we	O
have	O
described	O
in	O
section	O
is	O
always	O
in	O
the	O
linear	O
span	O
of	O
therefore	O
rather	O
than	O
maintaining	O
wt	O
we	O
can	O
maintain	O
the	O
corresponding	O
coefficients	O
formally	O
let	O
k	O
be	O
the	O
kernel	O
function	B
namely	O
for	O
all	O
x	O
kx	O
we	O
shall	O
maintain	O
two	O
vectors	O
in	O
rm	O
corresponding	O
to	O
two	O
vectors	O
and	O
wt	O
defined	O
in	O
the	O
sgd	B
procedure	O
of	O
section	O
that	O
is	O
will	O
be	O
a	O
vector	O
such	O
that	O
and	O
be	O
such	O
that	O
wt	O
j	O
j	O
the	O
vectors	O
and	O
are	O
updated	O
according	O
to	O
the	O
following	O
procedure	O
sgd	B
for	O
solving	O
soft-svm	B
with	O
kernels	B
goal	O
solve	O
equation	O
parameter	O
t	O
initialize	O
for	O
t	O
t	O
let	O
choose	O
i	O
uniformly	O
at	O
random	O
from	O
for	O
all	O
j	O
i	O
set	B
if	O
j	O
kxj	O
xi	O
set	B
t	O
j	O
j	O
i	O
yi	O
i	O
else	O
output	O
w	O
set	B
i	O
i	O
j	O
where	O
t	O
the	O
following	O
lemma	O
shows	O
that	O
the	O
preceding	O
implementation	O
is	O
equivalent	O
to	O
running	O
the	O
sgd	B
procedure	O
described	O
in	O
section	O
on	O
the	O
feature	B
space	I
tion	O
when	O
applied	O
on	O
the	O
feature	B
space	I
and	O
let	O
w	O
lemma	O
let	O
w	O
be	O
the	O
output	O
of	O
the	O
sgd	B
procedure	O
described	O
in	O
j	O
be	O
the	O
output	O
of	O
applying	O
sgd	B
with	O
kernels	B
then	O
w	O
w	O
proof	O
we	O
will	O
show	O
that	O
for	O
every	O
t	O
equation	O
holds	O
where	O
is	O
the	O
result	O
of	O
running	O
the	O
sgd	B
procedure	O
described	O
in	O
section	O
in	O
the	O
feature	B
kernel	O
methods	O
t	O
this	O
claim	O
implies	O
space	O
by	O
the	O
definition	O
of	O
that	O
equation	O
also	O
holds	O
and	O
the	O
proof	O
of	O
our	O
lemma	O
will	O
follow	O
to	O
prove	O
that	O
equation	O
holds	O
we	O
use	O
a	O
simple	O
inductive	O
argument	O
for	O
t	O
the	O
claim	O
trivially	O
holds	O
assume	O
it	O
holds	O
for	O
t	O
then	O
t	O
and	O
wt	O
yi	O
wt	O
yi	O
j	O
yi	O
j	O
kxj	O
xi	O
j	O
hence	O
the	O
condition	O
in	O
the	O
two	O
algorithms	O
is	O
equivalent	O
and	O
if	O
we	O
update	O
we	O
have	O
j	O
yi	O
j	O
yi	O
which	O
concludes	O
our	O
proof	O
summary	O
mappings	O
from	O
the	O
given	O
domain	B
to	O
some	O
higher	O
dimensional	O
space	O
on	O
which	O
a	O
halfspace	B
predictor	B
is	O
used	O
can	O
be	O
highly	O
powerful	O
we	O
benefit	O
from	O
a	O
rich	O
and	O
complex	O
hypothesis	B
class	I
yet	O
need	O
to	O
solve	O
the	O
problems	O
of	O
high	O
sample	O
and	O
computational	O
complexities	O
in	O
chapter	O
we	O
discussed	O
the	O
adaboost	B
algorithm	O
which	O
faces	O
these	O
challenges	O
by	O
using	O
a	O
weak	O
learner	O
even	O
though	O
we	O
re	O
in	O
a	O
very	O
high	O
dimensional	O
space	O
we	O
have	O
an	O
oracle	O
that	O
bestows	O
on	O
us	O
a	O
single	O
good	O
coordinate	O
to	O
work	O
with	O
on	O
each	O
iteration	O
in	O
this	O
chapter	O
we	O
introduced	O
a	O
different	O
approach	O
the	O
kernel	B
trick	I
the	O
idea	O
is	O
that	O
in	O
order	O
to	O
find	O
a	O
halfspace	B
predictor	B
in	O
the	O
high	O
dimensional	O
space	O
we	O
do	O
not	O
need	O
to	O
know	O
the	O
representation	O
of	O
instances	O
in	O
that	O
space	O
but	O
rather	O
the	O
values	O
of	O
inner	O
products	O
between	O
the	O
mapped	O
instances	O
calculating	O
inner	O
products	O
between	O
instances	O
in	O
the	O
high	O
dimensional	O
space	O
without	O
using	O
their	O
representation	O
in	O
that	O
space	O
is	O
done	O
using	O
kernel	O
functions	O
we	O
have	O
also	O
shown	O
how	O
the	O
sgd	B
algorithm	O
can	O
be	O
implemented	O
using	O
kernels	B
the	O
ideas	O
of	O
feature	B
mapping	O
and	O
the	O
kernel	B
trick	I
allow	O
us	O
to	O
use	O
the	O
framework	O
of	O
halfspaces	O
and	O
linear	B
predictors	I
for	O
nonvectorial	O
data	O
we	O
demonstrated	O
how	O
kernels	B
can	O
be	O
used	O
to	O
learn	O
predictors	O
over	O
the	O
domain	B
of	O
strings	O
we	O
presented	O
the	O
applicability	O
of	O
the	O
kernel	B
trick	I
in	O
svm	B
however	O
the	O
kernel	B
trick	I
can	O
be	O
applied	O
in	O
many	O
other	O
algorithms	O
a	O
few	O
examples	O
are	O
given	O
as	O
exercises	O
this	O
chapter	O
ends	O
the	O
series	O
of	O
chapters	O
on	O
linear	B
predictors	I
and	O
convex	B
problems	O
the	O
next	O
two	O
chapters	O
deal	O
with	O
completely	O
different	O
types	O
of	O
hypothesis	B
classes	O
bibliographic	O
remarks	O
bibliographic	O
remarks	O
in	O
the	O
context	O
of	O
svm	B
the	O
kernel-trick	O
has	O
been	O
introduced	O
in	O
boser	O
et	O
al	O
see	O
also	O
aizerman	O
braverman	O
rozonoer	O
the	O
observation	O
that	O
the	O
kernel-trick	O
can	O
be	O
applied	O
whenever	O
an	O
algorithm	O
only	O
relies	O
on	O
inner	O
products	O
was	O
first	O
stated	O
by	O
sch	O
olkopf	O
smola	O
m	O
uller	O
the	O
proof	O
of	O
the	O
representer	B
theorem	I
is	O
given	O
in	O
olkopf	O
herbrich	O
smola	O
williamson	O
sch	O
olkopf	O
herbrich	O
smola	O
the	O
conditions	O
stated	O
in	O
lemma	O
are	O
simplification	O
of	O
conditions	O
due	O
to	O
mercer	O
many	O
useful	O
kernel	O
functions	O
have	O
been	O
introduced	O
in	O
the	O
literature	O
for	O
various	O
applications	O
we	O
refer	O
the	O
reader	O
to	O
sch	O
olkopf	O
smola	O
exercises	O
consider	O
the	O
task	O
of	O
finding	O
a	O
sequence	O
of	O
characters	O
in	O
a	O
file	O
as	O
described	O
in	O
section	O
show	O
that	O
every	O
member	O
of	O
the	O
class	O
h	O
can	O
be	O
realized	O
by	O
composing	O
a	O
linear	O
classifier	B
over	O
whose	O
norm	O
is	O
and	O
that	O
attains	O
a	O
margin	B
of	O
kernelized	B
perceptron	B
show	O
how	O
to	O
run	O
the	O
perceptron	B
algorithm	O
while	O
only	O
accessing	O
the	O
instances	O
via	O
the	O
kernel	O
function	B
hint	O
the	O
derivation	O
is	O
similar	O
to	O
the	O
derivation	O
of	O
implementing	O
sgd	B
with	O
kernels	B
kernel	B
ridge	B
regression	B
the	O
ridge	B
regression	B
problem	O
with	O
a	O
feature	B
mapping	O
is	O
the	O
problem	O
of	O
finding	O
a	O
vector	O
w	O
that	O
minimizes	O
the	O
function	B
f	O
and	O
then	O
returning	O
the	O
predictor	B
hx	O
such	O
show	O
how	O
to	O
implement	O
the	O
ridge	B
regression	B
algorithm	O
with	O
kernels	B
hint	O
the	O
representer	B
theorem	I
tells	O
us	O
that	O
there	O
exists	O
a	O
vector	O
rm	O
i	O
is	O
a	O
minimizer	O
of	O
equation	O
let	O
g	O
be	O
the	O
gram	B
matrix	I
with	O
regard	O
to	O
s	O
and	O
k	O
that	O
is	O
gij	O
kxi	O
xj	O
define	O
g	O
rm	O
r	O
by	O
g	O
t	O
g	O
tion	O
then	O
w	O
where	O
g	O
is	O
the	O
i	O
th	O
column	O
of	O
g	O
show	O
that	O
if	O
minimizes	O
equa	O
find	O
a	O
closed	O
form	O
expression	O
for	O
let	O
n	O
be	O
any	O
positive	O
integer	O
for	O
every	O
x	O
n	O
define	O
i	O
is	O
a	O
minimizer	O
of	O
f	O
g	O
kx	O
minx	O
kernel	O
methods	O
prove	O
that	O
k	O
is	O
a	O
valid	O
kernel	O
namely	O
find	O
a	O
mapping	O
n	O
h	O
where	O
h	O
is	O
some	O
hilbert	B
space	I
such	O
that	O
x	O
n	O
kx	O
a	O
supermarket	O
manager	O
would	O
like	O
to	O
learn	O
which	O
of	O
his	O
customers	O
have	O
babies	O
on	O
the	O
basis	O
of	O
their	O
shopping	O
carts	O
specifically	O
he	O
sampled	O
i	O
i	O
d	O
customers	O
where	O
for	O
customer	O
i	O
let	O
xi	O
d	O
denote	O
the	O
subset	O
of	O
items	O
the	O
customer	O
bought	O
and	O
let	O
yi	O
be	O
the	O
label	B
indicating	O
whether	O
this	O
customer	O
has	O
a	O
baby	O
as	O
prior	B
knowledge	I
the	O
manager	O
knows	O
that	O
there	O
are	O
k	O
items	O
such	O
that	O
the	O
label	B
is	O
determined	O
to	O
be	O
iff	O
the	O
customer	O
bought	O
at	O
least	O
one	O
of	O
these	O
k	O
items	O
of	O
course	O
the	O
identity	O
of	O
these	O
k	O
items	O
is	O
not	O
known	O
there	O
was	O
nothing	O
to	O
learn	O
in	O
addition	O
according	O
to	O
the	O
store	O
regulation	O
each	O
customer	O
can	O
buy	O
at	O
most	O
s	O
items	O
help	O
the	O
manager	O
to	O
design	O
a	O
learning	O
algorithm	O
such	O
that	O
both	O
its	O
time	O
complexity	O
and	O
its	O
sample	B
complexity	I
are	O
polynomial	O
in	O
s	O
k	O
and	O
let	O
x	O
be	O
an	O
instance	B
set	B
and	O
let	O
be	O
a	O
feature	B
mapping	O
of	O
x	O
into	O
some	O
hilbert	O
feature	B
space	I
v	O
let	O
k	O
x	O
x	O
r	O
be	O
a	O
kernel	O
function	B
that	O
implements	O
inner	O
products	O
in	O
the	O
feature	B
space	I
v	O
consider	O
the	O
binary	O
classification	O
algorithm	O
that	O
predicts	O
the	O
label	B
of	O
an	O
unseen	O
instance	B
according	O
to	O
the	O
class	O
with	O
the	O
closest	O
average	O
formally	O
given	O
a	O
training	O
sequence	O
s	O
ym	O
for	O
every	O
y	O
we	O
define	O
iyiy	O
cy	O
my	O
where	O
my	O
yi	O
y	O
we	O
assume	O
that	O
m	O
and	O
m	O
are	O
nonzero	O
then	O
the	O
algorithm	O
outputs	O
the	O
following	O
decision	O
rule	O
hx	O
c	O
otherwise	O
let	O
w	O
c	O
c	O
and	O
let	O
b	O
show	O
that	O
hx	O
b	O
show	O
how	O
to	O
express	O
hx	O
on	O
the	O
basis	O
of	O
the	O
kernel	O
function	B
and	O
without	O
accessing	O
individual	O
entries	O
of	O
or	O
w	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
multiclass	B
categorization	O
is	O
the	O
problem	O
of	O
classifying	O
instances	O
into	O
one	O
of	O
several	O
possible	O
target	O
classes	O
that	O
is	O
we	O
are	O
aiming	O
at	O
learning	O
a	O
predictor	B
h	O
x	O
y	O
where	O
y	O
is	O
a	O
finite	O
set	B
of	O
categories	O
applications	O
include	O
for	O
example	O
categorizing	O
documents	O
according	O
to	O
topic	O
is	O
the	O
set	B
of	O
documents	O
and	O
y	O
is	O
the	O
set	B
of	O
possible	O
topics	O
or	O
determining	O
which	O
object	O
appears	O
in	O
a	O
given	O
image	O
is	O
the	O
set	B
of	O
images	O
and	O
y	O
is	O
the	O
set	B
of	O
possible	O
objects	O
the	O
centrality	O
of	O
the	O
multiclass	B
learning	O
problem	O
has	O
spurred	O
the	O
development	O
of	O
various	O
approaches	O
for	O
tackling	O
the	O
task	O
perhaps	O
the	O
most	O
straightforward	O
approach	O
is	O
a	O
reduction	O
from	O
multiclass	B
classification	O
to	O
binary	O
classification	O
in	O
section	O
we	O
discuss	O
the	O
most	O
common	O
two	O
reductions	B
as	O
well	O
as	O
the	O
main	O
drawback	O
of	O
the	O
reduction	O
approach	O
we	O
then	O
turn	O
to	O
describe	O
a	O
family	O
of	O
linear	B
predictors	I
for	O
multiclass	B
problems	O
relying	O
on	O
the	O
rlm	B
and	O
sgd	B
frameworks	O
from	O
previous	O
chapters	O
we	O
describe	O
several	O
practical	O
algorithms	O
for	O
multiclass	B
prediction	O
in	O
section	O
we	O
show	O
how	O
to	O
use	O
the	O
multiclass	B
machinery	O
for	O
complex	O
prediction	O
problems	O
in	O
which	O
y	O
can	O
be	O
extremely	O
large	O
but	O
has	O
some	O
structure	O
on	O
it	O
this	O
task	O
is	O
often	O
called	O
structured	O
output	O
learning	O
in	O
particular	O
we	O
demonstrate	O
this	O
approach	O
for	O
the	O
task	O
of	O
recognizing	O
handwritten	O
words	O
in	O
which	O
y	O
is	O
the	O
set	B
of	O
all	O
possible	O
strings	O
of	O
some	O
bounded	O
length	O
the	O
size	O
of	O
y	O
is	O
exponential	O
in	O
the	O
maximal	O
length	O
of	O
a	O
word	O
finally	O
in	O
section	O
and	O
section	O
we	O
discuss	O
ranking	B
problems	O
in	O
which	O
the	O
learner	O
should	O
order	O
a	O
set	B
of	O
instances	O
according	O
to	O
their	O
relevance	O
a	O
typical	O
application	O
is	O
ordering	O
results	O
of	O
a	O
search	O
engine	O
according	O
to	O
their	O
relevance	O
to	O
the	O
query	O
we	O
describe	O
several	O
performance	O
measures	O
that	O
are	O
adequate	O
for	O
assessing	O
the	O
performance	O
of	O
ranking	B
predictors	O
and	O
describe	O
how	O
to	O
learn	O
linear	B
predictors	I
for	O
ranking	B
problems	O
efficiently	O
one-versus-all	O
and	O
all-pairs	B
the	O
simplest	O
approach	O
to	O
tackle	O
multiclass	B
prediction	O
problems	O
is	O
by	O
reduction	O
to	O
binary	O
classification	O
recall	B
that	O
in	O
multiclass	B
prediction	O
we	O
would	O
like	O
to	O
learn	O
a	O
function	B
h	O
x	O
y	O
without	O
loss	B
of	O
generality	O
let	O
us	O
denote	O
y	O
k	O
in	O
the	O
one-versus-all	O
method	O
one-versus-rest	O
we	O
train	O
k	O
binary	O
clas	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
sifiers	O
each	O
of	O
which	O
discriminates	O
between	O
one	O
class	O
and	O
the	O
rest	O
of	O
the	O
classes	O
that	O
is	O
given	O
a	O
training	B
set	B
s	O
ym	O
where	O
every	O
yi	O
is	O
in	O
y	O
we	O
construct	O
k	O
binary	O
training	O
sets	O
sk	O
where	O
si	O
in	O
words	O
si	O
is	O
the	O
set	B
of	O
instances	O
labeled	O
if	O
their	O
label	B
in	O
s	O
was	O
i	O
and	O
otherwise	O
for	O
every	O
i	O
we	O
train	O
a	O
binary	O
predictor	B
hi	O
x	O
based	O
on	O
si	O
hoping	O
that	O
hix	O
should	O
equal	O
if	O
and	O
only	O
if	O
x	O
belongs	O
to	O
class	O
i	O
then	O
given	O
hk	O
we	O
construct	O
a	O
multiclass	B
predictor	B
using	O
the	O
rule	O
hx	O
argmax	O
i	O
hix	O
when	O
more	O
than	O
one	O
binary	O
hypothesis	B
predicts	O
we	O
should	O
somehow	O
decide	O
which	O
class	O
to	O
predict	O
we	O
can	O
arbitrarily	O
decide	O
to	O
break	O
ties	O
by	O
taking	O
the	O
minimal	O
index	O
in	O
argmaxi	O
hix	O
a	O
better	O
approach	O
can	O
be	O
applied	O
whenever	O
each	O
hi	O
hides	O
additional	O
information	O
which	O
can	O
be	O
interpreted	O
as	O
the	O
confidence	B
in	O
the	O
prediction	O
y	O
i	O
for	O
example	O
this	O
is	O
the	O
case	O
in	O
halfspaces	O
where	O
the	O
actual	O
prediction	O
is	O
but	O
we	O
can	O
interpret	O
as	O
the	O
confidence	B
in	O
the	O
prediction	O
in	O
such	O
cases	O
we	O
can	O
apply	O
the	O
multiclass	B
rule	O
given	O
in	O
equation	O
on	O
the	O
real	O
valued	O
predictions	O
a	O
pseudocode	O
of	O
the	O
one-versus-all	O
approach	O
is	O
given	O
in	O
the	O
following	O
one-versus-all	O
input	O
training	B
set	B
s	O
ym	O
algorithm	O
for	O
binary	O
classification	O
a	O
foreach	O
i	O
y	O
let	O
si	O
let	O
hi	O
asi	O
output	O
the	O
multiclass	B
hypothesis	B
defined	O
by	O
hx	O
argmaxi	O
y	O
hix	O
another	O
popular	O
reduction	O
is	O
the	O
all-pairs	B
approach	O
in	O
which	O
all	O
pairs	O
of	O
classes	O
are	O
compared	O
to	O
each	O
other	O
formally	O
given	O
a	O
training	B
set	B
s	O
ym	O
where	O
every	O
yi	O
is	O
in	O
for	O
every	O
i	O
j	O
k	O
we	O
construct	O
a	O
binary	O
training	O
sequence	O
sij	O
containing	O
all	O
examples	O
from	O
s	O
whose	O
label	B
is	O
either	O
i	O
or	O
j	O
for	O
each	O
such	O
an	O
example	O
we	O
set	B
the	O
binary	O
label	B
in	O
sij	O
to	O
be	O
if	O
the	O
multiclass	B
label	B
in	O
s	O
is	O
i	O
and	O
if	O
the	O
multiclass	B
label	B
in	O
s	O
is	O
j	O
next	O
we	O
train	O
a	O
binary	O
classification	O
algorithm	O
based	O
on	O
every	O
sij	O
to	O
get	O
hij	O
finally	O
we	O
construct	O
a	O
multiclass	B
classifier	B
by	O
predicting	O
the	O
class	O
that	O
had	O
the	O
highest	O
number	O
of	O
wins	O
a	O
pseudocode	O
of	O
the	O
all-pairs	B
approach	O
is	O
given	O
in	O
the	O
following	O
one-versus-all	O
and	O
all-pairs	B
all-pairs	B
input	O
training	B
set	B
s	O
ym	O
algorithm	O
for	O
binary	O
classification	O
a	O
foreach	O
i	O
j	O
y	O
s	O
t	O
i	O
j	O
initialize	O
sij	O
to	O
be	O
the	O
empty	O
sequence	O
for	O
t	O
m	O
if	O
yt	O
i	O
add	O
to	O
sij	O
if	O
yt	O
j	O
add	O
to	O
sij	O
let	O
hij	O
asij	O
output	O
the	O
multiclass	B
hypothesis	B
defined	O
by	O
hx	O
argmaxi	O
y	O
j	O
y	O
signj	O
i	O
hijx	O
although	O
reduction	O
methods	O
such	O
as	O
the	O
one-versus-all	O
and	O
all-pairs	B
are	O
simple	O
and	O
easy	O
to	O
construct	O
from	O
existing	O
algorithms	O
their	O
simplicity	O
has	O
a	O
price	O
the	O
binary	O
learner	O
is	O
not	O
aware	O
of	O
the	O
fact	O
that	O
we	O
are	O
going	O
to	O
use	O
its	O
output	O
hypotheses	O
for	O
constructing	O
a	O
multiclass	B
predictor	B
and	O
this	O
might	O
lead	O
to	O
suboptimal	O
results	O
as	O
illustrated	O
in	O
the	O
following	O
example	O
example	O
consider	O
a	O
multiclass	B
categorization	O
problem	O
in	O
which	O
the	O
instance	B
space	I
is	O
x	O
and	O
the	O
label	B
set	B
is	O
y	O
suppose	O
that	O
instances	O
of	O
the	O
different	O
classes	O
are	O
located	O
in	O
nonintersecting	O
balls	O
as	O
depicted	O
in	O
the	O
following	O
suppose	O
that	O
the	O
probability	O
masses	O
of	O
classes	O
are	O
and	O
respectively	O
consider	O
the	O
application	O
of	O
one-versus-all	O
to	O
this	O
problem	O
and	O
assume	O
that	O
the	O
binary	O
classification	O
algorithm	O
used	O
by	O
one-versus-all	O
is	O
erm	B
with	O
respect	O
to	O
the	O
hypothesis	B
class	I
of	O
halfspaces	O
observe	O
that	O
for	O
the	O
problem	O
of	O
discriminating	O
between	O
class	O
and	O
the	O
rest	O
of	O
the	O
classes	O
the	O
optimal	O
halfspace	B
would	O
be	O
the	O
all	O
negative	O
classifier	B
therefore	O
the	O
multiclass	B
predictor	B
constructed	O
by	O
one-versus-all	O
might	O
err	O
on	O
all	O
the	O
examples	O
from	O
class	O
will	O
be	O
the	O
case	O
if	O
the	O
tie	O
in	O
the	O
definition	O
of	O
hx	O
is	O
broken	O
by	O
the	O
numerical	O
value	O
of	O
the	O
class	O
label	B
in	O
contrast	O
if	O
we	O
choose	O
hix	O
where	O
then	O
the	O
classifier	B
defined	O
by	O
hx	O
argmaxi	O
hix	O
perfectly	O
predicts	O
all	O
the	O
examples	O
we	O
see	O
and	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
that	O
even	O
though	O
the	O
approximation	B
error	I
of	O
the	O
class	O
of	O
predictors	O
of	O
the	O
form	O
hx	O
is	O
zero	O
the	O
one-versus-all	O
approach	O
might	O
fail	O
to	O
find	O
a	O
good	O
predictor	B
from	O
this	O
class	O
linear	O
multiclass	B
predictors	O
in	O
light	O
of	O
the	O
inadequacy	O
of	O
reduction	O
methods	O
in	O
this	O
section	O
we	O
study	O
a	O
more	O
direct	O
approach	O
for	O
learning	O
multiclass	B
predictors	O
we	O
describe	O
the	O
family	O
of	O
linear	O
multiclass	B
predictors	O
to	O
motivate	O
the	O
construction	O
of	O
this	O
family	O
recall	B
that	O
a	O
linear	B
predictor	B
for	O
binary	O
classification	O
a	O
halfspace	B
takes	O
the	O
form	O
hx	O
an	O
equivalent	O
way	O
to	O
express	O
the	O
prediction	O
is	O
as	O
follows	O
hx	O
argmax	O
y	O
where	O
yx	O
is	O
the	O
vector	O
obtained	O
by	O
multiplying	O
each	O
element	O
of	O
x	O
by	O
y	O
this	O
representation	O
leads	O
to	O
a	O
natural	O
generalization	O
of	O
halfspaces	O
to	O
multiclass	B
problems	O
as	O
follows	O
let	O
x	O
y	O
rd	O
be	O
a	O
class-sensitive	B
feature	B
mapping	I
that	O
is	O
takes	O
as	O
input	O
a	O
pair	O
y	O
and	O
maps	O
it	O
into	O
a	O
d	O
dimensional	O
feature	B
vector	O
intuitively	O
we	O
can	O
think	O
of	O
the	O
elements	O
of	O
y	O
as	O
score	O
functions	O
that	O
assess	O
how	O
well	O
the	O
label	B
y	O
fits	O
the	O
instance	B
x	O
we	O
will	O
elaborate	O
on	O
later	O
on	O
given	O
and	O
a	O
vector	O
w	O
rd	O
we	O
can	O
define	O
a	O
multiclass	B
predictor	B
h	O
x	O
y	O
as	O
follows	O
hx	O
argmax	O
y	O
y	O
that	O
is	O
the	O
prediction	O
of	O
h	O
for	O
the	O
input	O
x	O
is	O
the	O
label	B
that	O
achieves	O
the	O
highest	O
weighted	O
score	O
where	O
weighting	O
is	O
according	O
to	O
the	O
vector	O
w	O
let	O
w	O
be	O
some	O
set	B
of	O
vectors	O
in	O
rd	O
for	O
example	O
w	O
rd	O
b	O
for	O
some	O
scalar	O
b	O
each	O
pair	O
w	O
defines	O
a	O
hypothesis	B
class	I
of	O
multiclass	B
predictors	O
h	O
argmax	O
y	O
y	O
w	O
w	O
of	O
course	O
the	O
immediate	O
question	O
which	O
we	O
discuss	O
in	O
the	O
sequel	O
is	O
how	O
to	O
construct	O
a	O
good	O
note	O
that	O
if	O
y	O
and	O
we	O
set	B
y	O
yx	O
and	O
w	O
rd	O
then	O
h	O
becomes	O
the	O
hypothesis	B
class	I
of	O
homogeneous	O
halfspace	B
predictors	O
for	O
binary	O
classification	O
how	O
to	O
construct	O
as	O
mentioned	O
before	O
we	O
can	O
think	O
of	O
the	O
elements	O
of	O
y	O
as	O
score	O
functions	O
that	O
assess	O
how	O
well	O
the	O
label	B
y	O
fits	O
the	O
instance	B
x	O
naturally	O
designing	O
a	O
good	O
is	O
similar	O
to	O
the	O
problem	O
of	O
designing	O
a	O
good	O
feature	B
mapping	O
we	O
discussed	O
in	O
linear	O
multiclass	B
predictors	O
chapter	O
and	O
as	O
we	O
will	O
discuss	O
in	O
more	O
detail	O
in	O
chapter	O
two	O
examples	O
of	O
useful	O
constructions	O
are	O
given	O
in	O
the	O
following	O
the	O
multivector	O
construction	O
let	O
y	O
k	O
and	O
let	O
x	O
rn	O
we	O
define	O
x	O
y	O
rd	O
where	O
d	O
nk	O
as	O
follows	O
y	O
ry	O
xn	O
rn	O
rk	O
yn	O
that	O
is	O
y	O
is	O
composed	O
of	O
k	O
vectors	O
each	O
of	O
which	O
is	O
of	O
dimension	O
n	O
where	O
we	O
set	B
all	O
the	O
vectors	O
to	O
be	O
the	O
all	O
zeros	O
vector	O
except	O
the	O
y	O
th	O
vector	O
which	O
is	O
set	B
to	O
be	O
x	O
it	O
follows	O
that	O
we	O
can	O
think	O
of	O
w	O
rnk	O
as	O
being	O
composed	O
of	O
k	O
weight	O
vectors	O
in	O
rn	O
that	O
is	O
w	O
wk	O
hence	O
the	O
name	O
multivector	O
construction	O
by	O
the	O
construction	O
we	O
have	O
that	O
and	O
therefore	O
the	O
multiclass	B
prediction	O
becomes	O
hx	O
argmax	O
y	O
y	O
a	O
geometric	O
illustration	O
of	O
the	O
multiclass	B
prediction	O
over	O
x	O
is	O
given	O
in	O
the	O
following	O
tf-idf	B
the	O
previous	O
definition	O
of	O
y	O
does	O
not	O
incorporate	O
any	O
prior	B
knowledge	I
about	O
the	O
problem	O
we	O
next	O
describe	O
an	O
example	O
of	O
a	O
feature	B
function	B
that	O
does	O
incorporate	O
prior	B
knowledge	I
let	O
x	O
be	O
a	O
set	B
of	O
text	O
documents	O
and	O
y	O
be	O
a	O
set	B
of	O
possible	O
topics	O
let	O
d	O
be	O
a	O
size	O
of	O
a	O
dictionary	O
of	O
words	O
for	O
each	O
word	O
in	O
the	O
dictionary	O
whose	O
corresponding	O
index	O
is	O
j	O
let	O
t	O
f	O
x	O
be	O
the	O
number	O
of	O
times	O
the	O
word	O
corresponding	O
to	O
j	O
appears	O
in	O
the	O
document	O
x	O
this	O
quantity	O
is	O
called	O
term-frequency	B
additionally	O
let	O
df	O
y	O
be	O
the	O
number	O
of	O
times	O
the	O
word	O
corresponding	O
to	O
j	O
appears	O
in	O
documents	O
in	O
our	O
training	B
set	B
that	O
are	O
not	O
about	O
topic	O
y	O
this	O
quantity	O
is	O
called	O
document-frequency	O
and	O
measures	O
whether	O
word	O
j	O
is	O
frequent	O
in	O
other	O
topics	O
now	O
define	O
x	O
y	O
rd	O
to	O
be	O
such	O
that	O
m	O
jx	O
y	O
t	O
f	O
x	O
log	O
df	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
documents	O
in	O
our	O
training	B
set	B
the	O
preceding	O
quantity	O
is	O
called	O
term-frequency-inverse-document-frequency	O
or	O
tf-idf	B
for	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
short	O
intuitively	O
jx	O
y	O
should	O
be	O
large	O
if	O
the	O
word	O
corresponding	O
to	O
j	O
appears	O
a	O
lot	O
in	O
the	O
document	O
x	O
but	O
does	O
not	O
appear	O
at	O
all	O
in	O
documents	O
that	O
are	O
not	O
on	O
topic	O
y	O
if	O
this	O
is	O
the	O
case	O
we	O
tend	O
to	O
believe	O
that	O
the	O
document	O
x	O
is	O
on	O
topic	O
y	O
note	O
that	O
unlike	O
the	O
multivector	O
construction	O
described	O
previously	O
in	O
the	O
current	O
construction	O
the	O
dimension	O
of	O
does	O
not	O
depend	O
on	O
the	O
number	O
of	O
topics	O
the	O
size	O
of	O
y	O
cost-sensitive	B
classification	O
so	O
far	O
we	O
used	O
the	O
zero-one	O
loss	B
as	O
our	O
performance	O
measure	O
of	O
the	O
quality	O
of	O
hx	O
that	O
is	O
the	O
loss	B
of	O
a	O
hypothesis	B
h	O
on	O
an	O
example	O
y	O
is	O
if	O
hx	O
y	O
and	O
otherwise	O
in	O
some	O
situations	O
it	O
makes	O
more	O
sense	O
to	O
penalize	O
different	O
levels	O
of	O
loss	B
for	O
different	O
mistakes	O
for	O
example	O
in	O
object	O
recognition	O
tasks	O
it	O
is	O
less	O
severe	O
to	O
predict	O
that	O
an	O
image	O
of	O
a	O
tiger	O
contains	O
a	O
cat	O
than	O
predicting	O
that	O
the	O
image	O
contains	O
a	O
whale	O
this	O
can	O
be	O
modeled	O
by	O
specifying	O
a	O
loss	B
function	B
y	O
y	O
r	O
where	O
for	O
every	O
pair	O
of	O
labels	O
y	O
the	O
loss	B
of	O
predicting	O
the	O
label	B
when	O
the	O
correct	O
label	B
is	O
y	O
is	O
defined	O
to	O
be	O
y	O
we	O
assume	O
that	O
y	O
note	O
that	O
the	O
zero-one	O
loss	B
can	O
be	O
easily	O
modeled	O
by	O
setting	O
y	O
erm	B
we	O
have	O
defined	O
the	O
hypothesis	B
class	I
h	O
and	O
specified	O
a	O
loss	B
function	B
to	O
learn	O
the	O
class	O
with	O
respect	O
to	O
the	O
loss	B
function	B
we	O
can	O
apply	O
the	O
erm	B
rule	O
with	O
respect	O
to	O
this	O
class	O
that	O
is	O
we	O
search	O
for	O
a	O
multiclass	B
hypothesis	B
h	O
h	O
parameterized	O
by	O
a	O
vector	O
w	O
that	O
minimizes	O
the	O
empirical	B
risk	B
with	O
respect	O
to	O
lsh	O
m	O
yi	O
we	O
now	O
show	O
that	O
when	O
w	O
rd	O
and	O
we	O
are	O
in	O
the	O
realizable	O
case	O
then	O
it	O
is	O
possible	O
to	O
solve	O
the	O
erm	B
problem	O
efficiently	O
using	O
linear	B
programming	I
indeed	O
in	O
the	O
realizable	O
case	O
we	O
need	O
to	O
find	O
a	O
vector	O
w	O
rd	O
that	O
satisfies	O
i	O
yi	O
argmax	O
y	O
y	O
equivalently	O
we	O
need	O
that	O
w	O
will	O
satisfy	O
the	O
following	O
set	B
of	O
linear	O
inequalities	O
i	O
y	O
y	O
finding	O
w	O
that	O
satisfies	O
the	O
preceding	O
set	B
of	O
linear	O
equations	O
amounts	O
to	O
solving	O
a	O
linear	O
program	O
as	O
in	O
the	O
case	O
of	O
binary	O
classification	O
it	O
is	O
also	O
possible	O
to	O
use	O
a	O
generalization	O
of	O
the	O
perceptron	B
algorithm	O
for	O
solving	O
the	O
erm	B
problem	O
see	O
exercise	O
in	O
the	O
nonrealizable	O
case	O
solving	O
the	O
erm	B
problem	O
is	O
in	O
general	O
computationally	O
hard	O
we	O
tackle	O
this	O
difficulty	O
using	O
the	O
method	O
of	O
convex	B
surrogate	O
linear	O
multiclass	B
predictors	O
loss	B
functions	O
section	O
in	O
particular	O
we	O
generalize	O
the	O
hinge	B
loss	B
to	O
multiclass	B
problems	O
generalized	O
hinge	B
loss	B
recall	B
that	O
in	O
binary	O
classification	O
the	O
hinge	B
loss	B
is	O
defined	O
to	O
be	O
we	O
now	O
generalize	O
the	O
hinge	B
loss	B
to	O
multiclass	B
predictors	O
of	O
the	O
form	O
hwx	O
argmax	O
y	O
recall	B
that	O
a	O
surrogate	O
convex	B
loss	B
should	O
upper	O
bound	O
the	O
original	O
nonconvex	O
loss	B
which	O
in	O
our	O
case	O
is	O
y	O
to	O
derive	O
an	O
upper	O
bound	O
on	O
y	O
we	O
first	O
note	O
that	O
the	O
definition	O
of	O
hwx	O
implies	O
that	O
therefore	O
y	O
y	O
hwx	O
since	O
hwx	O
y	O
we	O
can	O
upper	O
bound	O
the	O
right-hand	O
side	O
of	O
the	O
preceding	O
by	O
y	O
y	O
max	O
def	O
y	O
we	O
use	O
the	O
term	O
generalized	O
hinge	B
loss	B
to	O
denote	O
the	O
preceding	O
expression	O
as	O
we	O
have	O
shown	O
y	O
y	O
furthermore	O
equality	O
holds	O
whenever	O
the	O
score	O
of	O
the	O
correct	O
label	B
is	O
larger	O
than	O
the	O
score	O
of	O
any	O
other	O
label	B
by	O
at	O
least	O
y	O
namely	O
y	O
y	O
it	O
is	O
also	O
immediate	O
to	O
see	O
that	O
y	O
is	O
a	O
convex	B
function	B
with	O
respect	O
to	O
w	O
since	O
it	O
is	O
a	O
maximum	O
over	O
linear	O
functions	O
of	O
w	O
claim	O
in	O
chapter	O
and	O
that	O
y	O
is	O
with	O
y	O
remark	O
we	O
use	O
the	O
name	O
generalized	O
hinge	B
loss	B
since	O
in	O
the	O
binary	O
case	O
when	O
y	O
if	O
we	O
set	B
y	O
yx	O
then	O
the	O
generalized	O
hinge	B
loss	B
becomes	O
the	O
vanilla	O
hinge	B
loss	B
for	O
binary	O
classification	O
y	O
geometric	O
intuition	O
the	O
feature	B
function	B
x	O
y	O
rd	O
maps	O
each	O
x	O
into	O
vectors	O
in	O
rd	O
the	O
value	O
of	O
y	O
will	O
be	O
zero	O
if	O
there	O
exists	O
a	O
direction	O
w	O
such	O
that	O
when	O
projecting	O
the	O
vectors	O
onto	O
this	O
direction	O
we	O
obtain	O
that	O
each	O
vector	O
is	O
represented	O
by	O
the	O
scalar	O
and	O
we	O
can	O
rank	O
the	O
different	O
points	O
on	O
the	O
basis	O
of	O
these	O
scalars	O
so	O
that	O
the	O
point	O
corresponding	O
to	O
the	O
correct	O
y	O
is	O
top-ranked	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
for	O
each	O
y	O
the	O
difference	O
between	O
and	O
is	O
larger	O
than	O
the	O
loss	B
of	O
predicting	O
instead	O
of	O
y	O
the	O
difference	O
is	O
also	O
referred	O
to	O
as	O
the	O
margin	B
section	O
this	O
is	O
illustrated	O
in	O
the	O
following	O
figure	O
w	O
y	O
y	O
multiclass	B
svm	B
and	O
sgd	B
once	O
we	O
have	O
defined	O
the	O
generalized	O
hinge	B
loss	B
we	O
obtain	O
a	O
convex-lipschitz	O
learning	O
problem	O
and	O
we	O
can	O
apply	O
our	O
general	O
techniques	O
for	O
solving	O
such	O
problems	O
in	O
particular	O
the	O
rlm	B
technique	O
we	O
have	O
studied	O
in	O
chapter	O
yields	O
the	O
multiclass	B
svm	B
rule	O
multiclass	B
svm	B
input	O
ym	O
parameters	O
regularization	B
parameter	O
loss	B
function	B
y	O
y	O
r	O
class-sensitive	B
feature	B
mapping	I
x	O
y	O
rd	O
solve	O
y	O
yi	O
max	O
min	O
w	O
rd	O
m	O
output	O
the	O
predictor	B
hwx	O
argmaxy	O
we	O
can	O
solve	O
the	O
optimization	O
problem	O
associated	O
with	O
multiclass	B
svm	B
using	O
generic	O
convex	B
optimization	O
algorithms	O
using	O
the	O
method	O
described	O
in	O
section	O
let	O
us	O
analyze	O
the	O
risk	B
of	O
the	O
resulting	O
hypothesis	B
the	O
analysis	O
seamlessly	O
follows	O
from	O
our	O
general	O
analysis	O
for	O
convex-lipschitz	O
problems	O
given	O
in	O
chapter	O
in	O
particular	O
applying	O
corollary	O
and	O
using	O
the	O
fact	O
that	O
the	O
generalized	O
hinge	B
loss	B
upper	O
bounds	O
the	O
loss	B
we	O
immediately	O
obtain	O
an	O
analog	O
of	O
corollary	O
corollary	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
y	O
let	O
x	O
y	O
rd	O
and	O
assume	O
that	O
for	O
all	O
x	O
x	O
and	O
y	O
y	O
we	O
have	O
let	O
b	O
linear	O
multiclass	B
predictors	O
on	O
a	O
training	B
set	B
s	O
dm	O
e	O
consider	O
running	O
multiclass	B
svm	B
with	O
and	O
let	O
hw	O
be	O
the	O
output	O
of	O
multiclass	B
svm	B
then	O
min	O
s	O
dm	O
b	O
where	O
l	O
dh	O
exy	O
d	O
y	O
and	O
lg	O
hinge	O
with	O
being	O
the	O
generalized	B
hinge-loss	I
as	O
defined	O
in	O
equation	O
dhw	O
lg	O
hinge	O
d	O
hinge	O
s	O
dm	O
e	O
d	O
d	O
m	O
exy	O
y	O
we	O
can	O
also	O
apply	O
the	O
sgd	B
learning	O
framework	O
for	O
minimizing	O
lg	O
hinge	O
as	O
described	O
in	O
chapter	O
recall	B
claim	O
which	O
dealt	O
with	O
subgradients	O
of	O
max	O
functions	O
in	O
light	O
of	O
this	O
claim	O
in	O
order	O
to	O
find	O
a	O
subgradient	O
of	O
the	O
generalized	O
hinge	B
loss	B
all	O
we	O
need	O
to	O
do	O
is	O
to	O
find	O
y	O
y	O
that	O
achieves	O
the	O
maximum	O
in	O
the	O
definition	O
of	O
the	O
generalized	O
hinge	B
loss	B
this	O
yields	O
the	O
following	O
algorithm	O
d	O
sgd	B
for	O
multiclass	B
learning	O
parameters	O
scalar	O
integer	O
t	O
loss	B
function	B
y	O
y	O
r	O
class-sensitive	B
feature	B
mapping	I
x	O
y	O
rd	O
initialize	O
rd	O
for	O
t	O
t	O
sample	O
y	O
d	O
set	B
vt	O
y	O
y	O
update	O
wt	O
vt	O
output	O
w	O
t	O
find	O
y	O
y	O
y	O
wt	O
our	O
general	O
analysis	O
of	O
sgd	B
given	O
in	O
corollary	O
immediately	O
implies	O
corollary	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
y	O
let	O
x	O
y	O
rd	O
and	O
assume	O
that	O
for	O
all	O
x	O
x	O
and	O
y	O
y	O
we	O
have	O
let	O
b	O
then	O
for	O
every	O
if	O
we	O
run	O
sgd	B
for	O
multiclass	B
learning	O
with	O
a	O
number	O
of	O
iterations	O
number	O
of	O
examples	O
t	O
and	O
with	O
t	O
then	O
the	O
output	O
of	O
sgd	B
satisfies	O
w	O
min	O
b	O
hinge	O
s	O
dm	O
dh	O
w	O
e	O
d	O
e	O
s	O
dm	O
lg	O
hinge	O
d	O
remark	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
risk	B
bounds	O
given	O
in	O
corollary	O
and	O
corollary	O
do	O
not	O
depend	O
explicitly	O
on	O
the	O
size	O
of	O
the	O
label	B
set	B
y	O
a	O
fact	O
we	O
will	O
rely	O
on	O
in	O
the	O
next	O
section	O
however	O
the	O
bounds	O
may	O
depend	O
implicitly	O
on	O
the	O
size	O
of	O
y	O
via	O
the	O
norm	O
of	O
y	O
and	O
the	O
fact	O
that	O
the	O
bounds	O
are	O
meaningful	O
only	O
when	O
there	O
exists	O
some	O
vector	O
u	O
b	O
for	O
which	O
lg	O
hinge	O
d	O
is	O
not	O
excessively	O
large	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
structured	B
output	I
prediction	I
structured	B
output	I
prediction	I
problems	O
are	O
multiclass	B
problems	O
in	O
which	O
y	O
is	O
very	O
large	O
but	O
is	O
endowed	O
with	O
a	O
predefined	O
structure	O
the	O
structure	O
plays	O
a	O
key	O
role	O
in	O
constructing	O
efficient	O
algorithms	O
to	O
motivate	O
structured	O
learning	O
problems	O
consider	O
the	O
problem	O
of	O
optical	O
character	O
recognition	O
suppose	O
we	O
receive	O
an	O
image	O
of	O
some	O
handwritten	O
word	O
and	O
would	O
like	O
to	O
predict	O
which	O
word	O
is	O
written	O
in	O
the	O
image	O
to	O
simplify	O
the	O
setting	O
suppose	O
we	O
know	O
how	O
to	O
segment	O
the	O
image	O
into	O
a	O
sequence	O
of	O
images	O
each	O
of	O
which	O
contains	O
a	O
patch	O
of	O
the	O
image	O
corresponding	O
to	O
a	O
single	O
letter	O
therefore	O
x	O
is	O
the	O
set	B
of	O
sequences	O
of	O
images	O
and	O
y	O
is	O
the	O
set	B
of	O
sequences	O
of	O
letters	O
note	O
that	O
the	O
size	O
of	O
y	O
grows	O
exponentially	O
with	O
the	O
maximal	O
length	O
of	O
a	O
word	O
an	O
example	O
of	O
an	O
image	O
x	O
corresponding	O
to	O
the	O
label	B
y	O
workable	O
is	O
given	O
in	O
the	O
following	O
to	O
tackle	O
structure	O
prediction	O
we	O
can	O
rely	O
on	O
the	O
family	O
of	O
linear	B
predictors	I
described	O
in	O
the	O
previous	O
section	O
in	O
particular	O
we	O
need	O
to	O
define	O
a	O
reasonable	O
loss	B
function	B
for	O
the	O
problem	O
as	O
well	O
as	O
a	O
good	O
class-sensitive	B
feature	B
mapping	I
by	O
good	O
we	O
mean	O
a	O
feature	B
mapping	O
that	O
will	O
lead	O
to	O
a	O
low	O
approximation	B
error	I
for	O
the	O
class	O
of	O
linear	B
predictors	I
with	O
respect	O
to	O
and	O
once	O
we	O
do	O
this	O
we	O
can	O
rely	O
for	O
example	O
on	O
the	O
sgd	B
learning	O
algorithm	O
defined	O
in	O
the	O
previous	O
section	O
however	O
the	O
huge	O
size	O
of	O
y	O
poses	O
several	O
challenges	O
to	O
apply	O
the	O
multiclass	B
prediction	O
we	O
need	O
to	O
solve	O
a	O
maximization	O
problem	O
over	O
y	O
how	O
can	O
we	O
predict	O
efficiently	O
when	O
y	O
is	O
so	O
large	O
how	O
do	O
we	O
train	O
w	O
efficiently	O
in	O
particular	O
to	O
apply	O
the	O
sgd	B
rule	O
we	O
again	O
need	O
to	O
solve	O
a	O
maximization	O
problem	O
over	O
y	O
how	O
can	O
we	O
avoid	O
overfitting	B
in	O
the	O
previous	O
section	O
we	O
have	O
already	O
shown	O
that	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
linear	O
multiclass	B
predictor	B
does	O
not	O
depend	O
explicitly	O
on	O
the	O
number	O
of	O
classes	O
we	O
just	O
need	O
to	O
make	O
sure	O
that	O
the	O
norm	O
of	O
the	O
range	O
of	O
is	O
not	O
too	O
large	O
this	O
will	O
take	O
care	O
of	O
the	O
overfitting	B
problem	O
to	O
tackle	O
the	O
computational	O
challenges	O
we	O
rely	O
on	O
the	O
structure	O
of	O
the	O
problem	O
and	O
define	O
the	O
functions	O
and	O
so	O
that	O
calculating	O
the	O
maximization	O
problems	O
in	O
the	O
definition	O
of	O
hw	O
and	O
in	O
the	O
sgd	B
algorithm	O
can	O
be	O
performed	O
efficiently	O
in	O
the	O
following	O
we	O
demonstrate	O
one	O
way	O
to	O
achieve	O
these	O
goals	O
for	O
the	O
ocr	O
task	O
mentioned	O
previously	O
to	O
simplify	O
the	O
presentation	O
let	O
us	O
assume	O
that	O
all	O
the	O
words	O
in	O
y	O
are	O
of	O
length	O
r	O
and	O
that	O
the	O
number	O
of	O
different	O
letters	O
in	O
our	O
alphabet	O
is	O
q	O
let	O
y	O
and	O
be	O
two	O
structured	B
output	I
prediction	I
words	O
sequences	O
of	O
letters	O
in	O
y	O
we	O
define	O
the	O
function	B
y	O
to	O
be	O
the	O
average	O
number	O
of	O
letters	O
that	O
are	O
different	O
in	O
and	O
y	O
namely	O
i	O
next	O
let	O
us	O
define	O
a	O
class-sensitive	B
feature	B
mapping	I
y	O
it	O
will	O
be	O
convenient	O
to	O
think	O
about	O
x	O
as	O
a	O
matrix	O
of	O
size	O
n	O
r	O
where	O
n	O
is	O
the	O
number	O
of	O
pixels	O
in	O
each	O
image	O
and	O
r	O
is	O
the	O
number	O
of	O
images	O
in	O
the	O
sequence	O
the	O
j	O
th	O
column	O
of	O
x	O
corresponds	O
to	O
the	O
j	O
th	O
image	O
in	O
the	O
sequence	O
as	O
a	O
vector	O
of	O
gray	O
level	O
values	O
of	O
pixels	O
the	O
dimension	O
of	O
the	O
range	O
of	O
is	O
set	B
to	O
be	O
d	O
n	O
q	O
r	O
the	O
first	O
nq	O
feature	B
functions	O
are	O
type	O
features	O
and	O
take	O
the	O
form	O
y	O
r	O
xit	O
that	O
is	O
we	O
sum	O
the	O
value	O
of	O
the	O
i	O
th	O
pixel	O
only	O
over	O
the	O
images	O
for	O
which	O
y	O
assigns	O
the	O
letter	O
j	O
the	O
triple	O
index	O
j	O
indicates	O
that	O
we	O
are	O
dealing	O
with	O
feature	B
j	O
of	O
type	O
intuitively	O
such	O
features	O
can	O
capture	O
pixels	O
in	O
the	O
image	O
whose	O
gray	O
level	O
values	O
are	O
indicative	O
of	O
a	O
certain	O
letter	O
the	O
second	O
type	O
of	O
features	O
take	O
the	O
form	O
y	O
r	O
that	O
is	O
we	O
sum	O
the	O
number	O
of	O
times	O
the	O
letter	O
i	O
follows	O
the	O
letter	O
j	O
intuitively	O
these	O
features	O
can	O
capture	O
rules	O
like	O
it	O
is	O
likely	O
to	O
see	O
the	O
pair	O
qu	O
in	O
a	O
word	O
or	O
it	O
is	O
unlikely	O
to	O
see	O
the	O
pair	O
rz	O
in	O
a	O
word	O
of	O
course	O
some	O
of	O
these	O
features	O
will	O
not	O
be	O
very	O
useful	O
so	O
the	O
goal	O
of	O
the	O
learning	O
process	O
is	O
to	O
assign	O
weights	O
to	O
features	O
by	O
learning	O
the	O
vector	O
w	O
so	O
that	O
the	O
weighted	O
score	O
will	O
give	O
us	O
a	O
good	O
prediction	O
via	O
hwx	O
argmax	O
y	O
y	O
it	O
is	O
left	O
to	O
show	O
how	O
to	O
solve	O
the	O
optimization	O
problem	O
in	O
the	O
definition	O
of	O
hwx	O
efficiently	O
as	O
well	O
as	O
how	O
to	O
solve	O
the	O
optimization	O
problem	O
in	O
the	O
definition	O
of	O
y	O
in	O
the	O
sgd	B
algorithm	O
we	O
can	O
do	O
this	O
by	O
applying	O
a	O
dynamic	O
programming	O
procedure	O
we	O
describe	O
the	O
procedure	O
for	O
solving	O
the	O
maximization	O
in	O
the	O
definition	O
of	O
hw	O
and	O
leave	O
as	O
an	O
exercise	O
the	O
maximization	O
problem	O
in	O
the	O
definition	O
of	O
y	O
in	O
the	O
sgd	B
algorithm	O
to	O
derive	O
the	O
dynamic	O
programming	O
procedure	O
let	O
us	O
first	O
observe	O
that	O
we	O
can	O
write	O
y	O
yt	O
yt	O
for	O
an	O
appropriate	O
x	O
rd	O
and	O
for	O
simplicity	O
we	O
assume	O
that	O
is	O
always	O
equal	O
to	O
indeed	O
each	O
feature	B
function	B
can	O
be	O
written	O
in	O
terms	O
of	O
yt	O
yt	O
xit	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
while	O
the	O
feature	B
function	B
can	O
be	O
written	O
in	O
terms	O
of	O
yt	O
yt	O
therefore	O
the	O
prediction	O
can	O
be	O
written	O
as	O
hwx	O
argmax	O
y	O
y	O
yt	O
yt	O
in	O
the	O
following	O
we	O
derive	O
a	O
dynamic	O
programming	O
procedure	O
that	O
solves	O
every	O
problem	O
of	O
the	O
form	O
given	O
in	O
equation	O
the	O
procedure	O
will	O
maintain	O
a	O
matrix	O
m	O
rqr	O
such	O
that	O
ms	O
max	O
yt	O
yt	O
clearly	O
the	O
maximum	O
of	O
equals	O
maxs	O
msr	O
furthermore	O
we	O
can	O
calculate	O
m	O
in	O
a	O
recursive	O
manner	O
ms	O
max	O
s	O
this	O
yields	O
the	O
following	O
procedure	O
dynamic	O
programming	O
for	O
calculating	O
hwx	O
as	O
given	O
in	O
equation	O
input	O
a	O
matrix	O
x	O
rnr	O
and	O
a	O
vector	O
w	O
initialize	O
foreach	O
s	O
s	O
for	O
r	O
foreach	O
s	O
set	B
ms	O
as	O
in	O
equation	O
set	B
is	O
to	O
be	O
the	O
that	O
maximizes	O
equation	O
set	B
yt	O
argmaxs	O
msr	O
for	O
r	O
r	O
set	B
y	O
iy	O
output	O
y	O
yr	O
ranking	B
ranking	B
is	O
the	O
problem	O
of	O
ordering	O
a	O
set	B
of	O
instances	O
according	O
to	O
their	O
relevance	O
a	O
typical	O
application	O
is	O
ordering	O
results	O
of	O
a	O
search	O
engine	O
according	O
to	O
their	O
relevance	O
to	O
the	O
query	O
another	O
example	O
is	O
a	O
system	O
that	O
monitors	O
electronic	O
transactions	O
and	O
should	O
alert	O
for	O
possible	O
fraudulent	O
transactions	O
such	O
a	O
system	O
should	O
order	O
transactions	O
according	O
to	O
how	O
suspicious	O
they	O
are	O
x	O
n	O
be	O
the	O
set	B
of	O
all	O
sequences	O
of	O
instances	O
from	O
formally	O
let	O
x	O
ranking	B
x	O
of	O
arbitrary	O
length	O
a	O
ranking	B
hypothesis	B
h	O
is	O
a	O
function	B
that	O
receives	O
a	O
sequence	O
of	O
instances	O
x	O
xr	O
x	O
and	O
returns	O
a	O
permutation	O
of	O
it	O
is	O
more	O
convenient	O
to	O
let	O
the	O
output	O
of	O
h	O
be	O
a	O
vector	O
y	O
rr	O
where	O
by	O
sorting	O
the	O
elements	O
of	O
y	O
we	O
obtain	O
the	O
permutation	O
over	O
we	O
denote	O
by	O
the	O
permutation	O
over	O
induced	O
by	O
y	O
for	O
example	O
for	O
r	O
the	O
vector	O
y	O
induces	O
the	O
permutation	O
that	O
is	O
if	O
we	O
sort	O
y	O
in	O
an	O
ascending	O
order	O
then	O
we	O
obtain	O
the	O
vector	O
now	O
is	O
the	O
position	O
of	O
yi	O
in	O
the	O
sorted	O
vector	O
this	O
notation	O
reflects	O
that	O
the	O
top-ranked	O
instances	O
are	O
those	O
that	O
achieve	O
the	O
highest	O
values	O
in	O
in	O
the	O
notation	O
of	O
our	O
pac	B
learning	O
model	O
the	O
examples	O
domain	B
is	O
z	O
r	O
rr	O
and	O
the	O
hypothesis	B
class	I
h	O
is	O
some	O
set	B
of	O
ranking	B
hypotheses	O
we	O
define	O
x	O
y	O
x	O
y	O
for	O
some	O
function	B
we	O
next	O
turn	O
to	O
describe	O
loss	B
functions	O
for	O
ranking	B
there	O
are	O
many	O
possible	O
ways	O
to	O
define	O
such	O
loss	B
functions	O
and	O
here	O
we	O
list	O
a	O
few	O
examples	O
in	O
all	O
the	O
examples	O
rr	O
r	O
ranking	B
loss	B
y	O
is	O
zero	O
if	O
y	O
and	O
induce	O
exactly	O
the	O
same	O
ranking	B
and	O
y	O
otherwise	O
that	O
is	O
y	O
such	O
a	O
loss	B
function	B
is	O
almost	O
never	O
used	O
in	O
practice	O
as	O
it	O
does	O
not	O
distinguish	O
between	O
the	O
case	O
in	O
which	O
is	O
almost	O
equal	O
to	O
and	O
the	O
case	O
in	O
which	O
is	O
completely	O
different	O
from	O
kendall-tau	O
loss	B
we	O
count	O
the	O
number	O
of	O
pairs	O
j	O
that	O
are	O
in	O
different	O
order	O
in	O
the	O
two	O
permutations	O
this	O
can	O
be	O
written	O
as	O
y	O
rr	O
i	O
j	O
yj	O
r	O
this	O
loss	B
function	B
is	O
more	O
useful	O
than	O
the	O
loss	B
as	O
it	O
reflects	O
the	O
level	O
of	O
similarity	O
between	O
the	O
two	O
rankings	O
normalized	O
discounted	O
cumulative	O
gain	B
this	O
measure	O
emphasizes	O
the	O
correctness	O
at	O
the	O
top	O
of	O
the	O
list	O
by	O
using	O
a	O
monotonically	O
nondecreasing	O
discount	O
function	B
d	O
n	O
r	O
we	O
first	O
define	O
a	O
discounted	O
cumulative	O
gain	B
measure	O
y	O
d	O
yi	O
in	O
words	O
if	O
we	O
interpret	O
yi	O
as	O
a	O
score	O
of	O
the	O
true	O
relevance	O
of	O
item	O
i	O
then	O
we	O
take	O
a	O
weighted	O
sum	O
of	O
the	O
relevance	O
of	O
the	O
elements	O
while	O
the	O
weight	O
of	O
yi	O
is	O
determined	O
on	O
the	O
basis	O
of	O
the	O
position	O
of	O
i	O
in	O
assuming	O
that	O
all	O
elements	O
of	O
y	O
are	O
nonnegative	O
it	O
is	O
easy	O
to	O
verify	O
that	O
y	O
gy	O
y	O
we	O
can	O
therefore	O
define	O
a	O
normalized	O
discounted	O
cumulative	O
gain	B
by	O
the	O
ratio	O
ygy	O
y	O
and	O
the	O
corresponding	O
loss	B
function	B
would	O
be	O
y	O
y	O
gy	O
y	O
gy	O
y	O
d	O
yi	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
we	O
can	O
easily	O
see	O
that	O
y	O
and	O
that	O
y	O
whenever	O
a	O
typical	O
way	O
to	O
define	O
the	O
discount	O
function	B
is	O
by	O
di	O
if	O
i	O
k	O
r	O
otherwise	O
where	O
k	O
r	O
is	O
a	O
parameter	O
this	O
means	O
that	O
we	O
care	O
more	O
about	O
elements	O
that	O
are	O
ranked	O
higher	O
and	O
we	O
completely	O
ignore	O
elements	O
that	O
are	O
not	O
at	O
the	O
top-k	O
ranked	O
elements	O
the	O
ndcg	B
measure	O
is	O
often	O
used	O
to	O
evaluate	O
the	O
performance	O
of	O
search	O
engines	O
since	O
in	O
such	O
applications	O
it	O
makes	O
sense	O
completely	O
to	O
ignore	O
elements	O
that	O
are	O
not	O
at	O
the	O
top	O
of	O
the	O
ranking	B
once	O
we	O
have	O
a	O
hypothesis	B
class	I
and	O
a	O
ranking	B
loss	B
function	B
we	O
can	O
learn	O
a	O
ranking	B
function	B
using	O
the	O
erm	B
rule	O
however	O
from	O
the	O
computational	O
point	O
of	O
view	O
the	O
resulting	O
optimization	O
problem	O
might	O
be	O
hard	O
to	O
solve	O
we	O
next	O
discuss	O
how	O
to	O
learn	O
linear	B
predictors	I
for	O
ranking	B
linear	B
predictors	I
for	O
ranking	B
a	O
natural	O
way	O
to	O
define	O
a	O
ranking	B
function	B
is	O
by	O
projecting	O
the	O
instances	O
onto	O
some	O
vector	O
w	O
and	O
then	O
outputting	O
the	O
resulting	O
scalars	O
as	O
our	O
representation	O
of	O
the	O
ranking	B
function	B
that	O
is	O
assuming	O
that	O
x	O
rd	O
for	O
every	O
w	O
rd	O
we	O
define	O
a	O
ranking	B
function	B
xr	O
as	O
we	O
discussed	O
in	O
chapter	O
we	O
can	O
also	O
apply	O
a	O
feature	B
mapping	O
that	O
maps	O
instances	O
into	O
some	O
feature	B
space	I
and	O
then	O
takes	O
the	O
inner	O
products	O
with	O
w	O
in	O
the	O
feature	B
space	I
for	O
simplicity	O
we	O
focus	O
on	O
the	O
simpler	O
form	O
as	O
in	O
equation	O
given	O
some	O
w	O
rd	O
we	O
can	O
now	O
define	O
the	O
hypothesis	B
class	I
hw	O
w	O
w	O
once	O
we	O
have	O
defined	O
this	O
hypothesis	B
class	I
and	O
have	O
chosen	O
a	O
ranking	B
loss	B
function	B
we	O
can	O
apply	O
the	O
erm	B
rule	O
as	O
follows	O
given	O
a	O
training	B
set	B
s	O
xm	O
ym	O
where	O
each	O
xi	O
yi	O
is	O
in	O
rri	O
for	O
some	O
ri	O
n	O
we	O
xi	O
yi	O
as	O
in	O
the	O
case	O
of	O
binary	O
classification	O
for	O
many	O
loss	B
functions	O
this	O
problem	O
is	O
computationally	O
hard	O
and	O
we	O
therefore	O
turn	O
to	O
describe	O
convex	B
surrogate	B
loss	B
functions	O
we	O
describe	O
the	O
surrogates	O
for	O
the	O
kendall	B
tau	I
loss	B
and	O
for	O
the	O
ndcg	B
loss	B
should	O
search	O
w	O
w	O
that	O
minimizes	O
the	O
empirical	O
loss	B
a	O
hinge	B
loss	B
for	O
the	O
kendall	B
tau	I
loss	B
function	B
we	O
can	O
think	O
of	O
the	O
kendall	B
tau	I
loss	B
as	O
an	O
average	O
of	O
losses	O
for	O
each	O
pair	O
in	O
particular	O
for	O
every	O
j	O
we	O
can	O
rewrite	O
i	O
j	O
yj	O
yj	O
i	O
j	O
ranking	B
i	O
in	O
our	O
case	O
bound	O
as	O
follows	O
j	O
xi	O
it	O
follows	O
that	O
we	O
can	O
use	O
the	O
hinge	B
loss	B
upper	O
yj	O
i	O
j	O
sign	O
xi	O
taking	O
the	O
average	O
over	O
the	O
pairs	O
we	O
obtain	O
the	O
following	O
surrogate	O
convex	B
loss	B
for	O
the	O
kendall	B
tau	I
loss	B
function	B
x	O
y	O
rr	O
signyi	O
xi	O
r	O
the	O
right-hand	O
side	O
is	O
convex	B
with	O
respect	O
to	O
w	O
and	O
upper	O
bounds	O
the	O
kendall	B
tau	I
loss	B
it	O
is	O
also	O
a	O
function	B
with	O
parameter	O
maxij	O
a	O
hinge	B
loss	B
for	O
the	O
ndcg	B
loss	B
function	B
the	O
ndcg	B
loss	B
function	B
depends	O
on	O
the	O
predicted	O
ranking	B
vector	O
rr	O
via	O
the	O
permutation	O
it	O
induces	O
to	O
derive	O
a	O
surrogate	B
loss	B
function	B
we	O
first	O
make	O
the	O
following	O
observation	O
let	O
v	O
be	O
the	O
set	B
of	O
all	O
permutations	O
of	O
encoded	O
as	O
vectors	O
namely	O
each	O
v	O
v	O
is	O
a	O
vector	O
in	O
such	O
that	O
for	O
all	O
i	O
j	O
we	O
have	O
vi	O
vj	O
then	O
exercise	O
let	O
us	O
denote	O
x	O
v	O
argmax	O
v	O
v	O
vi	O
i	O
vixi	O
it	O
follows	O
that	O
x	O
argmax	O
v	O
v	O
argmax	O
v	O
v	O
argmax	O
v	O
v	O
w	O
vixi	O
x	O
on	O
the	O
basis	O
of	O
this	O
observation	O
we	O
can	O
use	O
the	O
generalized	O
hinge	B
loss	B
for	O
costsensitive	O
multiclass	B
classification	O
as	O
a	O
surrogate	B
loss	B
function	B
for	O
the	O
ndcg	B
loss	B
as	O
follows	O
x	O
y	O
x	O
y	O
x	O
x	O
y	O
x	O
x	O
max	O
v	O
v	O
max	O
v	O
v	O
y	O
the	O
right-hand	O
side	O
is	O
a	O
convex	B
function	B
with	O
respect	O
to	O
w	O
we	O
can	O
now	O
solve	O
the	O
learning	O
problem	O
using	O
sgd	B
as	O
described	O
in	O
section	O
the	O
main	O
computational	O
bottleneck	O
is	O
calculating	O
a	O
subgradient	O
of	O
the	O
loss	B
function	B
which	O
is	O
equivalent	O
to	O
finding	O
v	O
that	O
achieves	O
the	O
maximum	O
in	O
equation	O
claim	O
using	O
the	O
definition	O
of	O
the	O
ndcg	B
loss	B
this	O
is	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
equivalent	O
to	O
solving	O
the	O
problem	O
argmin	O
v	O
v	O
ivi	O
i	O
dvi	O
where	O
i	O
and	O
i	O
yigy	O
y	O
we	O
can	O
think	O
of	O
this	O
problem	O
a	O
little	O
bit	O
differently	O
by	O
defining	O
a	O
matrix	O
a	O
rrr	O
where	O
aij	O
j	O
i	O
dj	O
i	O
now	O
let	O
us	O
think	O
about	O
each	O
j	O
as	O
a	O
worker	O
each	O
i	O
as	O
a	O
task	O
and	O
aij	O
as	O
the	O
cost	O
of	O
assigning	O
task	O
i	O
to	O
worker	O
j	O
with	O
this	O
view	O
the	O
problem	O
of	O
finding	O
v	O
becomes	O
the	O
problem	O
of	O
finding	O
an	O
assignment	O
of	O
the	O
tasks	O
to	O
workers	O
of	O
minimal	O
cost	O
this	O
problem	O
is	O
called	O
the	O
assignment	O
problem	O
and	O
can	O
be	O
solved	O
efficiently	O
one	O
particular	O
algorithm	O
is	O
the	O
hungarian	O
method	O
another	O
way	O
to	O
solve	O
the	O
assignment	O
problem	O
is	O
using	O
linear	B
programming	I
to	O
do	O
so	O
let	O
us	O
first	O
write	O
the	O
assignment	O
problem	O
as	O
argmin	O
b	O
rrr	O
aijbij	O
s	O
t	O
i	O
bij	O
j	O
i	O
j	O
bij	O
bij	O
a	O
matrix	O
b	O
that	O
satisfies	O
the	O
constraints	O
in	O
the	O
preceding	O
optimization	O
problem	O
is	O
called	O
a	O
permutation	B
matrix	I
this	O
is	O
because	O
the	O
constraints	O
guarantee	O
that	O
there	O
is	O
at	O
most	O
a	O
single	O
entry	O
of	O
each	O
row	O
that	O
equals	O
and	O
a	O
single	O
entry	O
of	O
each	O
column	O
that	O
equals	O
therefore	O
the	O
matrix	O
b	O
corresponds	O
to	O
the	O
permutation	O
v	O
v	O
defined	O
by	O
vi	O
j	O
for	O
the	O
single	O
index	O
j	O
that	O
satisfies	O
bij	O
the	O
preceding	O
optimization	O
is	O
still	O
not	O
a	O
linear	O
program	O
because	O
of	O
the	O
combinatorial	O
constraint	O
bij	O
however	O
as	O
it	O
turns	O
out	O
this	O
constraint	O
is	O
redundant	O
if	O
we	O
solve	O
the	O
optimization	O
problem	O
while	O
simply	O
omitting	O
the	O
combinatorial	O
constraint	O
then	O
we	O
are	O
still	O
guaranteed	O
that	O
there	O
is	O
an	O
optimal	O
solution	O
that	O
will	O
satisfy	O
this	O
constraint	O
this	O
is	O
formalized	O
later	O
denote	O
mizing	O
such	O
that	O
b	O
is	O
a	O
permutation	B
matrix	I
ij	O
aijbij	O
then	O
equation	O
is	O
the	O
problem	O
of	O
minia	O
matrix	O
b	O
rrr	O
is	O
called	O
doubly	O
stochastic	O
if	O
all	O
elements	O
of	O
b	O
are	O
nonnegative	O
the	O
sum	O
of	O
each	O
row	O
of	O
b	O
is	O
and	O
the	O
sum	O
of	O
each	O
column	O
of	O
b	O
is	O
therefore	O
solving	O
equation	O
without	O
the	O
constraints	O
bij	O
is	O
the	O
problem	O
s	O
t	O
b	O
is	O
a	O
doubly	B
stochastic	I
matrix	I
argmin	O
b	O
rrr	O
bipartite	B
ranking	B
and	O
multivariate	B
performance	I
measures	I
the	O
following	O
claim	O
states	O
that	O
every	O
doubly	B
stochastic	I
matrix	I
is	O
a	O
convex	B
combination	O
of	O
permutation	O
matrices	O
claim	O
von	O
neumann	O
the	O
set	B
of	O
doubly	O
stochastic	O
matrices	O
in	O
rrr	O
is	O
the	O
convex	B
hull	O
of	O
the	O
set	B
of	O
permutation	O
matrices	O
in	O
rrr	O
on	O
the	O
basis	O
of	O
the	O
claim	O
we	O
easily	O
obtain	O
the	O
following	O
lemma	O
there	O
exists	O
an	O
optimal	O
solution	O
of	O
equation	O
that	O
is	O
also	O
an	O
optimal	O
solution	O
of	O
equation	O
write	O
b	O
proof	O
let	O
b	O
be	O
a	O
solution	O
of	O
equation	O
then	O
by	O
claim	O
we	O
can	O
i	O
ici	O
where	O
each	O
ci	O
is	O
a	O
permutation	B
matrix	I
each	O
i	O
and	O
i	O
i	O
since	O
all	O
the	O
ci	O
are	O
also	O
doubly	O
stochastic	O
we	O
clearly	O
have	O
that	O
for	O
every	O
i	O
we	O
claim	O
that	O
there	O
is	O
some	O
i	O
for	O
which	O
this	O
must	O
be	O
true	O
since	O
otherwise	O
if	O
for	O
every	O
i	O
we	O
would	O
have	O
that	O
a	O
ici	O
i	O
i	O
i	O
which	O
cannot	O
hold	O
we	O
have	O
thus	O
shown	O
that	O
some	O
permutation	B
matrix	I
ci	O
satisfies	O
but	O
since	O
for	O
every	O
other	O
permutation	B
matrix	I
c	O
we	O
have	O
we	O
conclude	O
that	O
ci	O
is	O
an	O
optimal	O
solution	O
of	O
both	O
equation	O
and	O
equation	O
bipartite	B
ranking	B
and	O
multivariate	B
performance	I
measures	I
in	O
the	O
previous	O
section	O
we	O
described	O
the	O
problem	O
of	O
ranking	B
we	O
used	O
a	O
vector	O
y	O
rr	O
for	O
representing	O
an	O
order	O
over	O
the	O
elements	O
xr	O
if	O
all	O
elements	O
in	O
y	O
are	O
different	O
from	O
each	O
other	O
then	O
y	O
specifies	O
a	O
full	O
order	O
over	O
however	O
if	O
two	O
elements	O
of	O
y	O
attain	O
the	O
same	O
value	O
yi	O
yj	O
for	O
i	O
j	O
then	O
y	O
can	O
only	O
specify	O
a	O
partial	O
order	O
over	O
in	O
such	O
a	O
case	O
we	O
say	O
that	O
xi	O
and	O
xj	O
are	O
of	O
equal	O
relevance	O
according	O
to	O
y	O
in	O
the	O
extreme	O
case	O
y	O
which	O
means	O
that	O
each	O
xi	O
is	O
either	O
relevant	O
or	O
nonrelevant	O
this	O
setting	O
is	O
often	O
called	O
bipartite	B
ranking	B
for	O
example	O
in	O
the	O
fraud	O
detection	O
application	O
mentioned	O
in	O
the	O
previous	O
section	O
each	O
transaction	O
is	O
labeled	O
as	O
either	O
fraudulent	O
or	O
benign	O
seemingly	O
we	O
can	O
solve	O
the	O
bipartite	B
ranking	B
problem	O
by	O
learning	O
a	O
binary	O
classifier	B
applying	O
it	O
on	O
each	O
instance	B
and	O
putting	O
the	O
positive	O
ones	O
at	O
the	O
top	O
of	O
the	O
ranked	O
list	O
however	O
this	O
may	O
lead	O
to	O
poor	O
results	O
as	O
the	O
goal	O
of	O
a	O
binary	O
learner	O
is	O
usually	O
to	O
minimize	O
the	O
zero-one	O
loss	B
some	O
surrogate	O
of	O
it	O
while	O
the	O
goal	O
of	O
a	O
ranker	O
might	O
be	O
significantly	O
different	O
to	O
illustrate	O
this	O
consider	O
again	O
the	O
problem	O
of	O
fraud	O
detection	O
usually	O
most	O
of	O
the	O
transactions	O
are	O
benign	O
therefore	O
a	O
binary	O
classifier	B
that	O
predicts	O
benign	O
on	O
all	O
transactions	O
will	O
have	O
a	O
zero-one	O
error	O
of	O
while	O
this	O
is	O
a	O
very	O
small	O
number	O
the	O
resulting	O
predictor	B
is	O
meaningless	O
for	O
the	O
fraud	O
detection	O
application	O
the	O
crux	O
of	O
the	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
problem	O
stems	O
from	O
the	O
inadequacy	O
of	O
the	O
zero-one	O
loss	B
for	O
what	O
we	O
are	O
really	O
interested	O
in	O
a	O
more	O
adequate	O
performance	O
measure	O
should	O
take	O
into	O
account	O
the	O
predictions	O
over	O
the	O
entire	O
set	B
of	O
instances	O
for	O
example	O
in	O
the	O
previous	O
section	O
we	O
have	O
defined	O
the	O
ndcg	B
loss	B
which	O
emphasizes	O
the	O
correctness	O
of	O
the	O
top-ranked	O
items	O
in	O
this	O
section	O
we	O
describe	O
additional	O
loss	B
functions	O
that	O
are	O
specifically	O
adequate	O
for	O
bipartite	B
ranking	B
problems	O
as	O
in	O
the	O
previous	O
section	O
we	O
are	O
given	O
a	O
sequence	O
of	O
instances	O
x	O
xr	O
and	O
we	O
predict	O
a	O
ranking	B
vector	O
rr	O
the	O
feedback	O
vector	O
is	O
y	O
we	O
define	O
a	O
loss	B
that	O
depends	O
on	O
and	O
y	O
and	O
depends	O
on	O
a	O
threshold	O
r	O
this	O
threshold	O
transforms	O
the	O
vector	O
rr	O
into	O
the	O
vector	O
r	O
usually	O
the	O
value	O
of	O
is	O
set	B
to	O
be	O
however	O
as	O
we	O
will	O
see	O
we	O
sometimes	O
set	B
while	O
taking	O
into	O
account	O
additional	O
constraints	O
on	O
the	O
problem	O
the	O
loss	B
functions	O
we	O
define	O
in	O
the	O
following	O
depend	O
on	O
the	O
following	O
num	O
i	O
bers	O
true	O
positives	O
a	O
yi	O
false	O
positives	O
b	O
yi	O
false	O
negatives	O
c	O
yi	O
true	O
negatives	O
d	O
yi	O
i	O
i	O
i	O
i	O
the	O
recall	B
sensitivity	B
of	O
a	O
prediction	O
vector	O
is	O
the	O
fraction	O
of	O
true	O
a	O
ac	O
the	O
precision	B
is	O
the	O
fraction	O
of	O
correct	O
a	O
ab	O
the	O
specificity	B
positives	O
catches	O
namely	O
predictions	O
among	O
the	O
positive	O
labels	O
we	O
predict	O
namely	O
is	O
the	O
fraction	O
of	O
true	O
negatives	O
that	O
our	O
predictor	B
catches	O
namely	O
d	O
db	O
note	O
that	O
as	O
we	O
decrease	O
the	O
recall	B
increases	O
the	O
value	O
when	O
on	O
the	O
other	O
hand	O
the	O
precision	B
and	O
the	O
specificity	B
usually	O
decrease	O
as	O
we	O
decrease	O
therefore	O
there	O
is	O
a	O
tradeoff	O
between	O
precision	B
and	O
recall	B
and	O
we	O
can	O
control	O
it	O
by	O
changing	O
the	O
loss	B
functions	O
defined	O
in	O
the	O
following	O
use	O
various	O
techniques	O
for	O
combining	O
both	O
the	O
precision	B
and	O
recall	B
averaging	O
sensitivity	B
and	O
specificity	B
this	O
measure	O
is	O
the	O
average	O
of	O
the	O
sensitivity	B
and	O
specificity	B
namely	O
this	O
is	O
also	O
the	O
accuracy	B
on	O
positive	O
examples	O
averaged	O
with	O
the	O
accuracy	B
on	O
negative	O
examples	O
here	O
we	O
set	B
and	O
the	O
corresponding	O
loss	B
function	B
is	O
y	O
a	O
a	O
ac	O
d	O
recall	B
the	O
score	O
is	O
the	O
harmonic	O
mean	O
of	O
the	O
precision	B
and	O
recall	B
its	O
maximal	O
value	O
is	O
obtained	O
when	O
both	O
precision	B
precision	B
and	O
recall	B
are	O
and	O
its	O
minimal	O
value	O
is	O
obtained	O
whenever	O
one	O
of	O
them	O
is	O
if	O
the	O
other	O
one	O
is	O
the	O
score	O
can	O
be	O
written	O
using	O
the	O
numbers	O
a	O
b	O
c	O
as	O
follows	O
again	O
we	O
set	B
and	O
the	O
loss	B
function	B
becomes	O
y	O
f	O
it	O
is	O
like	O
score	O
but	O
we	O
attach	O
times	O
more	O
importance	O
to	O
it	O
can	O
also	O
be	O
written	O
as	O
recall	B
than	O
to	O
precision	B
that	O
is	O
ac	O
d	O
db	O
db	O
precision	B
recall	B
bipartite	B
ranking	B
and	O
multivariate	B
performance	I
measures	I
f	O
y	O
f	O
again	O
we	O
set	B
and	O
the	O
loss	B
function	B
becomes	O
recall	B
at	O
k	O
we	O
measure	O
the	O
recall	B
while	O
the	O
prediction	O
must	O
contain	O
at	O
most	O
k	O
positive	O
labels	O
that	O
is	O
we	O
should	O
set	B
so	O
that	O
a	O
b	O
k	O
this	O
is	O
convenient	O
for	O
example	O
in	O
the	O
application	O
of	O
a	O
fraud	O
detection	O
system	O
where	O
a	O
bank	O
employee	O
can	O
only	O
handle	O
a	O
small	O
number	O
of	O
suspicious	O
transactions	O
precision	B
at	O
k	O
we	O
measure	O
the	O
precision	B
while	O
the	O
prediction	O
must	O
contain	O
at	O
least	O
k	O
positive	O
labels	O
that	O
is	O
we	O
should	O
set	B
so	O
that	O
a	O
b	O
k	O
the	O
measures	O
defined	O
previously	O
are	O
often	O
referred	O
to	O
as	O
multivariate	B
performance	I
measures	I
note	O
that	O
these	O
measures	O
are	O
highly	O
different	O
from	O
the	O
average	O
abcd	O
in	O
the	O
aforemenzero-one	O
loss	B
which	O
in	O
the	O
preceding	O
notation	O
equals	O
tioned	O
example	O
of	O
fraud	O
detection	O
when	O
of	O
the	O
examples	O
are	O
negatively	O
labeled	O
the	O
zero-one	O
loss	B
of	O
predicting	O
that	O
all	O
the	O
examples	O
are	O
negatives	O
is	O
in	O
contrast	O
the	O
recall	B
of	O
such	O
prediction	O
is	O
and	O
hence	O
the	O
score	O
is	O
also	O
which	O
means	O
that	O
the	O
corresponding	O
loss	B
will	O
be	O
bd	O
linear	B
predictors	I
for	O
bipartite	B
ranking	B
we	O
next	O
describe	O
how	O
to	O
train	O
linear	B
predictors	I
for	O
bipartite	B
ranking	B
as	O
in	O
the	O
previous	O
section	O
a	O
linear	B
predictor	B
for	O
ranking	B
is	O
defined	O
to	O
be	O
hw	O
x	O
the	O
corresponding	O
loss	B
function	B
is	O
one	O
of	O
the	O
multivariate	B
performance	I
measures	I
described	O
before	O
the	O
loss	B
function	B
depends	O
on	O
hw	O
x	O
via	O
the	O
binary	O
vector	O
it	O
induces	O
which	O
we	O
denote	O
by	O
r	O
as	O
in	O
the	O
previous	O
section	O
to	O
facilitate	O
an	O
efficient	O
algorithm	O
we	O
derive	O
a	O
convex	B
surrogate	B
loss	B
function	B
on	O
the	O
derivation	O
is	O
similar	O
to	O
the	O
derivation	O
of	O
the	O
generalized	O
hinge	B
loss	B
for	O
the	O
ndcg	B
ranking	B
loss	B
as	O
described	O
in	O
the	O
previous	O
section	O
our	O
first	O
observation	O
is	O
that	O
for	O
all	O
the	O
values	O
of	O
defined	O
before	O
there	O
is	O
some	O
v	O
such	O
that	O
can	O
be	O
rewritten	O
as	O
argmax	O
v	O
v	O
i	O
this	O
is	O
clearly	O
true	O
for	O
the	O
case	O
if	O
we	O
choose	O
v	O
the	O
two	O
measures	O
for	O
which	O
is	O
not	O
taken	O
to	O
be	O
are	O
precision	B
at	O
k	O
and	O
recall	B
at	O
k	O
for	O
precision	B
at	O
k	O
we	O
can	O
take	O
v	O
to	O
be	O
the	O
set	B
v	O
k	O
containing	O
all	O
vectors	O
in	O
whose	O
number	O
of	O
ones	O
is	O
at	O
least	O
k	O
for	O
recall	B
at	O
k	O
we	O
can	O
take	O
v	O
to	O
be	O
v	O
k	O
which	O
is	O
defined	O
analogously	O
see	O
exercise	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
once	O
we	O
have	O
defined	O
b	O
as	O
in	O
equation	O
we	O
can	O
easily	O
derive	O
a	O
convex	B
surrogate	B
loss	B
as	O
follows	O
assuming	O
that	O
y	O
v	O
we	O
have	O
that	O
x	O
y	O
x	O
y	O
max	O
v	O
v	O
x	O
y	O
x	O
y	O
the	O
right-hand	O
side	O
is	O
a	O
convex	B
function	B
with	O
respect	O
to	O
w	O
we	O
can	O
now	O
solve	O
the	O
learning	O
problem	O
using	O
sgd	B
as	O
described	O
in	O
section	O
the	O
main	O
computational	O
bottleneck	O
is	O
calculating	O
a	O
subgradient	O
of	O
the	O
loss	B
function	B
which	O
is	O
equivalent	O
to	O
finding	O
v	O
that	O
achieves	O
the	O
maximum	O
in	O
equation	O
claim	O
in	O
the	O
following	O
we	O
describe	O
how	O
to	O
find	O
this	O
maximizer	O
efficiently	O
for	O
any	O
performance	O
measure	O
that	O
can	O
be	O
written	O
as	O
a	O
function	B
of	O
the	O
numbers	O
a	O
b	O
c	O
d	O
given	O
in	O
equation	O
and	O
for	O
which	O
the	O
set	B
v	O
contains	O
all	O
elements	O
in	O
for	O
which	O
the	O
values	O
of	O
a	O
b	O
satisfy	O
some	O
constraints	O
for	O
example	O
for	O
recall	B
at	O
k	O
the	O
set	B
v	O
is	O
all	O
vectors	O
for	O
which	O
a	O
b	O
k	O
the	O
idea	O
is	O
as	O
follows	O
for	O
any	O
a	O
b	O
let	O
yab	O
vi	O
yi	O
a	O
vi	O
yi	O
b	O
any	O
vector	O
v	O
v	O
falls	O
into	O
yab	O
for	O
some	O
a	O
b	O
furthermore	O
if	O
yab	O
v	O
is	O
not	O
empty	O
for	O
some	O
a	O
b	O
then	O
yab	O
v	O
yab	O
therefore	O
we	O
can	O
search	O
within	O
each	O
yab	O
that	O
has	O
a	O
nonempty	O
intersection	O
with	O
v	O
separately	O
and	O
then	O
take	O
the	O
optimal	O
value	O
the	O
key	O
observation	O
is	O
that	O
once	O
we	O
are	O
searching	O
only	O
within	O
yab	O
the	O
value	O
of	O
is	O
fixed	O
so	O
we	O
only	O
need	O
to	O
maximize	O
the	O
expression	O
max	O
v	O
yab	O
suppose	O
the	O
examples	O
are	O
sorted	O
so	O
that	O
then	O
it	O
is	O
easy	O
to	O
verify	O
that	O
we	O
would	O
like	O
to	O
set	B
vi	O
to	O
be	O
positive	O
for	O
the	O
smallest	O
indices	O
i	O
doing	O
this	O
with	O
the	O
constraint	O
on	O
a	O
b	O
amounts	O
to	O
setting	O
vi	O
for	O
the	O
a	O
top	O
ranked	O
positive	O
examples	O
and	O
for	O
the	O
b	O
top-ranked	O
negative	O
examples	O
this	O
yields	O
the	O
following	O
procedure	O
summary	O
solving	O
equation	O
input	O
xr	O
yr	O
w	O
v	O
assumptions	O
is	O
a	O
function	B
of	O
a	O
b	O
c	O
d	O
v	O
contains	O
all	O
vectors	O
for	O
which	O
f	O
b	O
for	O
some	O
function	B
f	O
initialize	O
p	O
yi	O
n	O
yi	O
sort	O
examples	O
so	O
that	O
r	O
let	O
ip	O
be	O
the	O
indices	O
of	O
the	O
positive	O
examples	O
let	O
jn	O
be	O
the	O
indices	O
of	O
the	O
negative	O
examples	O
for	O
a	O
p	O
c	O
p	O
a	O
for	O
b	O
n	O
such	O
that	O
f	O
b	O
d	O
n	O
b	O
calculate	O
using	O
a	O
b	O
c	O
d	O
set	B
vr	O
s	O
t	O
via	O
vjb	O
and	O
the	O
rest	O
of	O
the	O
elements	O
of	O
v	O
equal	O
if	O
v	O
set	B
vi	O
i	O
output	O
summary	O
many	O
real	O
world	O
supervised	O
learning	O
problems	O
can	O
be	O
cast	O
as	O
learning	O
a	O
multiclass	B
predictor	B
we	O
started	O
the	O
chapter	O
by	O
introducing	O
reductions	B
of	O
multiclass	B
learning	O
to	O
binary	O
learning	O
we	O
then	O
described	O
and	O
analyzed	O
the	O
family	O
of	O
linear	B
predictors	I
for	O
multiclass	B
learning	O
we	O
have	O
shown	O
how	O
this	O
family	O
can	O
be	O
used	O
even	O
if	O
the	O
number	O
of	O
classes	O
is	O
extremely	O
large	O
as	O
long	O
as	O
we	O
have	O
an	O
adequate	O
structure	O
on	O
the	O
problem	O
finally	O
we	O
have	O
described	O
ranking	B
problems	O
in	O
chapter	O
we	O
study	O
the	O
sample	B
complexity	I
of	O
multiclass	B
learning	O
in	O
more	O
detail	O
bibliographic	O
remarks	O
the	O
one-versus-all	O
and	O
all-pairs	B
approach	O
reductions	B
have	O
been	O
unified	O
under	O
the	O
framework	O
of	O
error	O
correction	O
output	O
codes	O
bakiri	O
allwein	O
schapire	O
singer	O
there	O
are	O
also	O
other	O
types	O
of	O
reductions	B
such	O
as	O
tree-based	O
classifiers	O
for	O
example	O
beygelzimer	O
langford	O
ravikumar	O
the	O
limitations	O
of	O
reduction	O
techniques	O
have	O
been	O
studied	O
multiclass	B
ranking	B
and	O
complex	O
prediction	O
problems	O
in	O
et	O
al	O
daniely	O
sabato	O
shwartz	O
see	O
also	O
chapter	O
in	O
which	O
we	O
analyze	O
the	O
sample	B
complexity	I
of	O
multiclass	B
learning	O
direct	O
approaches	O
to	O
multiclass	B
learning	O
with	O
linear	B
predictors	I
have	O
been	O
studied	O
in	O
weston	O
watkins	O
crammer	O
singer	O
in	O
particular	O
the	O
multivector	O
construction	O
is	O
due	O
to	O
crammer	O
singer	O
collins	O
has	O
shown	O
how	O
to	O
apply	O
the	O
perceptron	B
algorithm	O
for	O
structured	O
output	O
problems	O
see	O
also	O
collins	O
a	O
related	O
approach	O
is	O
discriminative	B
learning	O
of	O
conditional	O
random	O
fields	O
see	O
lafferty	O
mccallum	O
pereira	O
structured	O
output	O
svm	B
has	O
been	O
studied	O
in	O
chapelle	O
vapnik	O
elisseeff	O
sch	O
olkopf	O
taskar	O
guestrin	O
koller	O
tsochantaridis	O
hofmann	O
joachims	O
altun	O
the	O
dynamic	O
procedure	O
we	O
have	O
presented	O
for	O
calculating	O
the	O
prediction	O
hwx	O
in	O
the	O
structured	O
output	O
section	O
is	O
similar	O
to	O
the	O
forward-backward	O
variables	O
calculated	O
by	O
the	O
viterbi	O
procedure	O
in	O
hmms	O
for	O
instance	B
juang	O
more	O
generally	O
solving	O
the	O
maximization	O
problem	O
in	O
structured	O
output	O
is	O
closely	O
related	O
to	O
the	O
problem	O
of	O
inference	O
in	O
graphical	O
models	O
for	O
example	O
koller	O
friedman	O
chapelle	O
le	O
smola	O
proposed	O
to	O
learn	O
a	O
ranking	B
function	B
with	O
respect	O
to	O
the	O
ndcg	B
loss	B
using	O
ideas	O
from	O
structured	O
output	O
learning	O
they	O
also	O
observed	O
that	O
the	O
maximization	O
problem	O
in	O
the	O
definition	O
of	O
the	O
generalized	O
hinge	B
loss	B
is	O
equivalent	O
to	O
the	O
assignment	O
problem	O
agarwal	O
roth	O
analyzed	O
the	O
sample	B
complexity	I
of	O
bipartite	B
ranking	B
joachims	O
studied	O
the	O
applicability	O
of	O
structured	O
output	O
svm	B
to	O
bipartite	B
ranking	B
with	O
multivariate	B
performance	I
measures	I
exercises	O
consider	O
a	O
set	B
s	O
of	O
examples	O
in	O
rn	O
for	O
which	O
there	O
exist	O
vectors	O
k	O
such	O
that	O
every	O
example	O
y	O
s	O
falls	O
within	O
a	O
ball	O
centered	O
at	O
y	O
whose	O
radius	O
is	O
r	O
assume	O
also	O
that	O
for	O
every	O
i	O
j	O
i	O
consider	O
concatenating	O
each	O
instance	B
by	O
the	O
constant	O
and	O
then	O
applying	O
the	O
multivector	O
construction	O
namely	O
y	O
xn	O
ry	O
rk	O
show	O
that	O
there	O
exists	O
a	O
vector	O
w	O
such	O
that	O
y	O
for	O
every	O
y	O
s	O
hint	O
observe	O
that	O
for	O
every	O
example	O
y	O
s	O
we	O
can	O
write	O
x	O
y	O
v	O
for	O
some	O
r	O
now	O
take	O
w	O
wk	O
where	O
wi	O
i	O
multiclass	B
perceptron	B
consider	O
the	O
following	O
algorithm	O
exercises	O
multiclass	B
batch	O
perceptron	B
input	O
a	O
training	B
set	B
ym	O
a	O
class-sensitive	B
feature	B
mapping	I
x	O
y	O
rd	O
initialize	O
rd	O
for	O
t	O
if	O
i	O
and	O
y	O
yi	O
s	O
t	O
then	O
wt	O
yi	O
y	O
else	O
output	O
wt	O
prove	O
the	O
following	O
theorem	O
assume	O
that	O
there	O
exists	O
such	O
that	O
for	O
all	O
i	O
and	O
for	O
all	O
y	O
yi	O
it	O
holds	O
that	O
let	O
r	O
maxiy	O
yi	O
then	O
the	O
multiclass	B
perceptron	B
algorithm	O
stops	O
after	O
at	O
most	O
iterations	O
and	O
when	O
it	O
stops	O
it	O
holds	O
that	O
i	O
yi	O
argmaxy	O
dure	O
for	O
multiclass	B
prediction	O
you	O
can	O
assume	O
that	O
y	O
generalize	O
the	O
dynamic	O
programming	O
procedure	O
given	O
in	O
section	O
for	O
solving	O
the	O
maximization	O
problem	O
given	O
in	O
the	O
definition	O
of	O
h	O
in	O
the	O
sgd	B
procet	O
yt	O
for	O
some	O
arbitrary	O
function	B
prove	O
that	O
equation	O
holds	O
show	O
that	O
the	O
two	O
definitions	O
of	O
as	O
defined	O
in	O
equation	O
and	O
equation	O
are	O
indeed	O
equivalent	O
for	O
all	O
the	O
multivariate	B
performance	I
measures	I
decision	B
trees	I
a	O
decision	O
tree	O
is	O
a	O
predictor	B
h	O
x	O
y	O
that	O
predicts	O
the	O
label	B
associated	O
with	O
an	O
instance	B
x	O
by	O
traveling	O
from	O
a	O
root	O
node	O
of	O
a	O
tree	O
to	O
a	O
leaf	O
for	O
simplicity	O
we	O
focus	O
on	O
the	O
binary	O
classification	O
setting	O
namely	O
y	O
but	O
decision	B
trees	I
can	O
be	O
applied	O
for	O
other	O
prediction	O
problems	O
as	O
well	O
at	O
each	O
node	O
on	O
the	O
root-to-leaf	O
path	O
the	O
successor	O
child	O
is	O
chosen	O
on	O
the	O
basis	O
of	O
a	O
splitting	O
of	O
the	O
input	O
space	O
usually	O
the	O
splitting	O
is	O
based	O
on	O
one	O
of	O
the	O
features	O
of	O
x	O
or	O
on	O
a	O
predefined	O
set	B
of	O
splitting	O
rules	O
a	O
leaf	O
contains	O
a	O
specific	O
label	B
an	O
example	O
of	O
a	O
decision	O
tree	O
for	O
the	O
papayas	O
example	O
in	O
chapter	O
is	O
given	O
in	O
the	O
following	O
color	O
other	O
pale	O
green	O
to	O
pale	O
yellow	O
not-tasty	O
softness	O
other	O
gives	O
slightly	O
to	O
palm	O
pressure	O
not-tasty	O
tasty	O
to	O
check	O
if	O
a	O
given	O
papaya	O
is	O
tasty	O
or	O
not	O
the	O
decision	O
tree	O
first	O
examines	O
the	O
color	O
of	O
the	O
papaya	O
if	O
this	O
color	O
is	O
not	O
in	O
the	O
range	O
pale	O
green	O
to	O
pale	O
yellow	O
then	O
the	O
tree	O
immediately	O
predicts	O
that	O
the	O
papaya	O
is	O
not	O
tasty	O
without	O
additional	O
tests	O
otherwise	O
the	O
tree	O
turns	O
to	O
examine	O
the	O
softness	O
of	O
the	O
papaya	O
if	O
the	O
softness	O
level	O
of	O
the	O
papaya	O
is	O
such	O
that	O
it	O
gives	O
slightly	O
to	O
palm	O
pressure	O
the	O
decision	O
tree	O
predicts	O
that	O
the	O
papaya	O
is	O
tasty	O
otherwise	O
the	O
prediction	O
is	O
not-tasty	O
the	O
preceding	O
example	O
underscores	O
one	O
of	O
the	O
main	O
advantages	O
of	O
decision	B
trees	I
the	O
resulting	O
classifier	B
is	O
very	O
simple	O
to	O
understand	O
and	O
interpret	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
sample	B
complexity	I
sample	B
complexity	I
a	O
popular	O
splitting	O
rule	O
at	O
internal	O
nodes	O
of	O
the	O
tree	O
is	O
based	O
on	O
thresholding	O
the	O
value	O
of	O
a	O
single	O
feature	B
that	O
is	O
we	O
move	O
to	O
the	O
right	O
or	O
left	O
child	O
of	O
the	O
node	O
on	O
the	O
basis	O
of	O
where	O
i	O
is	O
the	O
index	O
of	O
the	O
relevant	O
feature	B
and	O
r	O
is	O
the	O
threshold	O
in	O
such	O
cases	O
we	O
can	O
think	O
of	O
a	O
decision	O
tree	O
as	O
a	O
splitting	O
of	O
the	O
instance	B
space	I
x	O
rd	O
into	O
cells	O
where	O
each	O
leaf	O
of	O
the	O
tree	O
corresponds	O
to	O
one	O
cell	O
it	O
follows	O
that	O
a	O
tree	O
with	O
k	O
leaves	O
can	O
shatter	O
a	O
set	B
of	O
k	O
instances	O
hence	O
if	O
we	O
allow	O
decision	B
trees	I
of	O
arbitrary	O
size	O
we	O
obtain	O
a	O
hypothesis	B
class	I
of	O
infinite	O
vc	B
dimension	I
such	O
an	O
approach	O
can	O
easily	O
lead	O
to	O
overfitting	B
to	O
avoid	O
overfitting	B
we	O
can	O
rely	O
on	O
the	O
minimum	O
description	O
length	O
principle	O
described	O
in	O
chapter	O
and	O
aim	O
at	O
learning	O
a	O
decision	O
tree	O
that	O
on	O
one	O
hand	O
fits	O
the	O
data	O
well	O
while	O
on	O
the	O
other	O
hand	O
is	O
not	O
too	O
large	O
for	O
simplicity	O
we	O
will	O
assume	O
that	O
x	O
in	O
other	O
words	O
each	O
instance	B
is	O
a	O
vector	O
of	O
d	O
bits	O
in	O
that	O
case	O
thresholding	O
the	O
value	O
of	O
a	O
single	O
feature	B
corresponds	O
to	O
a	O
splitting	O
rule	O
of	O
the	O
form	O
for	O
some	O
i	O
for	O
instance	B
we	O
can	O
model	O
the	O
papaya	O
decision	O
tree	O
earlier	O
by	O
assuming	O
that	O
a	O
papaya	O
is	O
parameterized	O
by	O
a	O
two-dimensional	O
bit	O
vector	O
x	O
where	O
the	O
bit	O
represents	O
whether	O
the	O
color	O
is	O
pale	O
green	O
to	O
pale	O
yellow	O
or	O
not	O
and	O
the	O
bit	O
represents	O
whether	O
the	O
softness	O
is	O
gives	O
slightly	O
to	O
palm	O
pressure	O
or	O
not	O
with	O
this	O
representation	O
the	O
node	O
color	O
can	O
be	O
replaced	O
with	O
and	O
the	O
node	O
softness	O
can	O
be	O
replaced	O
with	O
while	O
this	O
is	O
a	O
big	O
simplification	O
the	O
algorithms	O
and	O
analysis	O
we	O
provide	O
in	O
the	O
following	O
can	O
be	O
extended	O
to	O
more	O
general	O
cases	O
with	O
the	O
aforementioned	O
simplifying	O
assumption	O
the	O
hypothesis	B
class	I
becomes	O
finite	O
but	O
is	O
still	O
very	O
large	O
in	O
particular	O
any	O
classifier	B
from	O
to	O
can	O
be	O
represented	O
by	O
a	O
decision	O
tree	O
with	O
leaves	O
and	O
depth	O
of	O
d	O
exercise	O
therefore	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
is	O
which	O
means	O
that	O
the	O
number	O
of	O
examples	O
we	O
need	O
to	O
pac	B
learn	O
the	O
hypothesis	B
class	I
grows	O
with	O
unless	O
d	O
is	O
very	O
small	O
this	O
is	O
a	O
huge	O
number	O
of	O
examples	O
to	O
overcome	O
this	O
obstacle	O
we	O
rely	O
on	O
the	O
mdl	B
scheme	O
described	O
in	O
chapter	O
the	O
underlying	O
prior	B
knowledge	I
is	O
that	O
we	O
should	O
prefer	O
smaller	O
trees	O
over	O
larger	O
trees	O
to	O
formalize	O
this	O
intuition	O
we	O
first	O
need	O
to	O
define	O
a	O
description	O
language	O
for	O
decision	B
trees	I
which	O
is	O
prefix	O
free	O
and	O
requires	O
fewer	O
bits	O
for	O
smaller	O
decision	B
trees	I
here	O
is	O
one	O
possible	O
way	O
a	O
tree	O
with	O
n	O
nodes	O
will	O
be	O
described	O
in	O
n	O
blocks	O
each	O
of	O
size	O
bits	O
the	O
first	O
n	O
blocks	O
encode	O
the	O
nodes	O
of	O
the	O
tree	O
in	O
a	O
depth-first	O
order	O
and	O
the	O
last	O
block	O
marks	O
the	O
end	O
of	O
the	O
code	O
each	O
block	O
indicates	O
whether	O
the	O
current	O
node	O
is	O
an	O
internal	O
node	O
of	O
the	O
form	O
for	O
some	O
i	O
a	O
leaf	O
whose	O
value	O
is	O
a	O
leaf	O
whose	O
value	O
is	O
end	O
of	O
the	O
code	O
decision	B
trees	I
overall	O
there	O
are	O
d	O
options	O
hence	O
we	O
need	O
bits	O
to	O
describe	O
each	O
block	O
assuming	O
each	O
internal	O
node	O
has	O
two	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
this	O
is	O
a	O
prefix-free	O
encoding	O
of	O
the	O
tree	O
and	O
that	O
the	O
description	O
length	O
of	O
a	O
tree	O
with	O
n	O
nodes	O
is	O
by	O
theorem	O
we	O
have	O
that	O
with	O
probability	O
of	O
at	O
least	O
over	O
a	O
sample	O
of	O
size	O
m	O
for	O
every	O
n	O
and	O
every	O
decision	O
tree	O
h	O
h	O
with	O
n	O
nodes	O
it	O
holds	O
that	O
ldh	O
lsh	O
this	O
bound	O
performs	O
a	O
tradeoff	O
on	O
the	O
one	O
hand	O
we	O
expect	O
larger	O
more	O
complex	O
decision	B
trees	I
to	O
have	O
a	O
smaller	O
training	O
risk	B
lsh	O
but	O
the	O
respective	O
value	O
of	O
n	O
will	O
be	O
larger	O
on	O
the	O
other	O
hand	O
smaller	O
decision	B
trees	I
will	O
have	O
a	O
smaller	O
value	O
of	O
n	O
but	O
lsh	O
might	O
be	O
larger	O
our	O
hope	O
prior	B
knowledge	I
is	O
that	O
we	O
can	O
find	O
a	O
decision	O
tree	O
with	O
both	O
low	O
empirical	B
risk	B
lsh	O
and	O
a	O
number	O
of	O
nodes	O
n	O
not	O
too	O
high	O
our	O
bound	O
indicates	O
that	O
such	O
a	O
tree	O
will	O
have	O
low	O
true	O
risk	B
ldh	O
decision	O
tree	O
algorithms	O
the	O
bound	O
on	O
ldh	O
given	O
in	O
equation	O
suggests	O
a	O
learning	O
rule	O
for	O
decision	B
trees	I
search	O
for	O
a	O
tree	O
that	O
minimizes	O
the	O
right-hand	O
side	O
of	O
equation	O
unfortunately	O
it	O
turns	O
out	O
that	O
solving	O
this	O
problem	O
is	O
computationally	O
consequently	O
practical	O
decision	O
tree	O
learning	O
algorithms	O
are	O
based	O
on	O
heuristics	O
such	O
as	O
a	O
greedy	O
approach	O
where	O
the	O
tree	O
is	O
constructed	O
gradually	O
and	O
locally	O
optimal	O
decisions	O
are	O
made	O
at	O
the	O
construction	O
of	O
each	O
node	O
such	O
algorithms	O
cannot	O
guarantee	O
to	O
return	O
the	O
globally	O
optimal	O
decision	O
tree	O
but	O
tend	O
to	O
work	O
reasonably	O
well	O
in	O
practice	O
a	O
general	O
framework	O
for	O
growing	O
a	O
decision	O
tree	O
is	O
as	O
follows	O
we	O
start	O
with	O
a	O
tree	O
with	O
a	O
single	O
leaf	O
root	O
and	O
assign	O
this	O
leaf	O
a	O
label	B
according	O
to	O
a	O
majority	O
vote	O
among	O
all	O
labels	O
over	O
the	O
training	B
set	B
we	O
now	O
perform	O
a	O
series	O
of	O
iterations	O
on	O
each	O
iteration	O
we	O
examine	O
the	O
effect	O
of	O
splitting	O
a	O
single	O
leaf	O
we	O
define	O
some	O
gain	B
measure	O
that	O
quantifies	O
the	O
improvement	O
due	O
to	O
this	O
split	O
then	O
among	O
all	O
possible	O
splits	O
we	O
either	O
choose	O
the	O
one	O
that	O
maximizes	O
the	O
gain	B
and	O
perform	O
it	O
or	O
choose	O
not	O
to	O
split	O
the	O
leaf	O
at	O
all	O
in	O
the	O
following	O
we	O
provide	O
a	O
possible	O
implementation	O
it	O
is	O
based	O
on	O
a	O
popular	O
decision	O
tree	O
algorithm	O
known	O
as	O
for	O
iterative	O
dichotomizer	O
we	O
describe	O
the	O
algorithm	O
for	O
the	O
case	O
of	O
binary	O
features	O
namely	O
x	O
we	O
may	O
assume	O
this	O
without	O
loss	B
of	O
generality	O
because	O
if	O
a	O
decision	O
node	O
has	O
only	O
one	O
child	O
we	O
can	O
replace	O
the	O
node	O
by	O
its	O
child	O
without	O
affecting	O
the	O
predictions	O
of	O
the	O
decision	O
tree	O
more	O
precisely	O
if	O
then	O
no	O
algorithm	O
can	O
solve	O
equation	O
in	O
time	O
polynomial	O
in	O
n	O
d	O
and	O
m	O
decision	O
tree	O
algorithms	O
and	O
therefore	O
all	O
splitting	O
rules	O
are	O
of	O
the	O
form	O
for	O
some	O
feature	B
i	O
we	O
discuss	O
the	O
case	O
of	O
real	O
valued	O
features	O
in	O
section	O
the	O
algorithm	O
works	O
by	O
recursive	O
calls	O
with	O
the	O
initial	O
call	O
being	O
and	O
returns	O
a	O
decision	O
tree	O
in	O
the	O
pseudocode	O
that	O
follows	O
we	O
use	O
a	O
call	O
to	O
a	O
procedure	O
gains	O
i	O
which	O
receives	O
a	O
training	B
set	B
s	O
and	O
an	O
index	O
i	O
and	O
evaluates	O
the	O
gain	B
of	O
a	O
split	O
of	O
the	O
tree	O
according	O
to	O
the	O
ith	O
feature	B
we	O
describe	O
several	O
gain	B
measures	O
in	O
section	O
a	O
input	O
training	B
set	B
s	O
feature	B
subset	O
a	O
if	O
all	O
examples	O
in	O
s	O
are	O
labeled	O
by	O
return	O
a	O
leaf	O
if	O
all	O
examples	O
in	O
s	O
are	O
labeled	O
by	O
return	O
a	O
leaf	O
if	O
a	O
return	O
a	O
leaf	O
whose	O
value	O
majority	O
of	O
labels	O
in	O
s	O
else	O
let	O
j	O
argmaxi	O
a	O
gains	O
i	O
if	O
all	O
examples	O
in	O
s	O
have	O
the	O
same	O
label	B
return	O
a	O
leaf	O
whose	O
value	O
majority	O
of	O
labels	O
in	O
s	O
else	O
let	O
be	O
the	O
tree	O
returned	O
by	O
y	O
s	O
xj	O
a	O
let	O
be	O
the	O
tree	O
returned	O
by	O
y	O
s	O
xj	O
a	O
return	O
the	O
tree	O
xj	O
implementations	O
of	O
the	O
gain	B
measure	O
different	O
algorithms	O
use	O
different	O
implementations	O
of	O
gains	O
i	O
here	O
we	O
present	O
three	O
we	O
use	O
the	O
notation	O
psf	O
to	O
denote	O
the	O
probability	O
that	O
an	O
event	O
holds	O
with	O
respect	O
to	O
the	O
uniform	O
distribution	O
over	O
s	O
train	O
error	O
the	O
simplest	O
definition	O
of	O
gain	B
is	O
the	O
decrease	O
in	O
training	B
error	I
formally	O
let	O
ca	O
mina	O
a	O
note	O
that	O
the	O
training	B
error	I
before	O
splitting	O
on	O
feature	B
i	O
is	O
cpsy	O
since	O
we	O
took	O
a	O
majority	O
vote	O
among	O
labels	O
similarly	O
the	O
error	O
after	O
splitting	O
on	O
feature	B
i	O
is	O
p	O
s	O
cp	O
p	O
s	O
s	O
s	O
therefore	O
we	O
can	O
define	O
gain	B
to	O
be	O
the	O
difference	O
between	O
the	O
two	O
namely	O
gains	O
i	O
cp	O
s	O
s	O
cp	O
s	O
p	O
s	O
s	O
decision	B
trees	I
information	B
gain	B
another	O
popular	O
gain	B
measure	O
that	O
is	O
used	O
in	O
the	O
and	O
algorithms	O
of	O
quinlan	O
is	O
the	O
information	B
gain	B
the	O
information	B
gain	B
is	O
the	O
difference	O
between	O
the	O
entropy	B
of	O
the	O
label	B
before	O
and	O
after	O
the	O
split	O
and	O
is	O
achieved	O
by	O
replacing	O
the	O
function	B
c	O
in	O
the	O
previous	O
expression	O
by	O
the	O
entropy	B
function	B
ca	O
a	O
loga	O
a	O
a	O
gini	B
index	I
yet	O
another	O
definition	O
of	O
a	O
gain	B
which	O
is	O
used	O
by	O
the	O
cart	B
algorithm	O
of	O
breiman	O
friedman	O
olshen	O
stone	O
is	O
the	O
gini	B
index	I
ca	O
a	O
both	O
the	O
information	B
gain	B
and	O
the	O
gini	B
index	I
are	O
smooth	O
and	O
concave	O
upper	O
bounds	O
of	O
the	O
train	O
error	O
these	O
properties	O
can	O
be	O
advantageous	O
in	O
some	O
situations	O
for	O
example	O
kearns	O
mansour	O
pruning	B
the	O
algorithm	O
described	O
previously	O
still	O
suffers	O
from	O
a	O
big	O
problem	O
the	O
returned	O
tree	O
will	O
usually	O
be	O
very	O
large	O
such	O
trees	O
may	O
have	O
low	O
empirical	B
risk	B
but	O
their	O
true	O
risk	B
will	O
tend	O
to	O
be	O
high	O
both	O
according	O
to	O
our	O
theoretical	O
analysis	O
and	O
in	O
practice	O
one	O
solution	O
is	O
to	O
limit	O
the	O
number	O
of	O
iterations	O
of	O
leading	O
to	O
a	O
tree	O
with	O
a	O
bounded	O
number	O
of	O
nodes	O
another	O
common	O
solution	O
is	O
to	O
prune	O
the	O
tree	O
after	O
it	O
is	O
built	O
hoping	O
to	O
reduce	O
it	O
to	O
a	O
much	O
smaller	O
tree	O
but	O
still	O
with	O
a	O
similar	O
empirical	B
error	I
theoretically	O
according	O
to	O
the	O
bound	O
in	O
equation	O
if	O
we	O
can	O
make	O
n	O
much	O
smaller	O
without	O
increasing	O
lsh	O
by	O
much	O
we	O
are	O
likely	O
to	O
get	O
a	O
decision	O
tree	O
with	O
a	O
smaller	O
true	O
risk	B
usually	O
the	O
pruning	B
is	O
performed	O
by	O
a	O
bottom-up	O
walk	O
on	O
the	O
tree	O
each	O
node	O
might	O
be	O
replaced	O
with	O
one	O
of	O
its	O
subtrees	O
or	O
with	O
a	O
leaf	O
based	O
on	O
some	O
bound	O
or	O
estimate	O
of	O
ldh	O
example	O
the	O
bound	O
in	O
equation	O
a	O
pseudocode	O
of	O
a	O
common	O
template	O
is	O
given	O
in	O
the	O
following	O
generic	O
tree	O
pruning	B
procedure	O
input	O
function	B
f	O
m	O
for	O
the	O
generalization	B
error	I
of	O
a	O
decision	O
tree	O
t	O
based	O
on	O
a	O
sample	O
of	O
size	O
m	O
tree	O
t	O
foreach	O
node	O
j	O
in	O
a	O
bottom-up	O
walk	O
on	O
t	O
leaves	O
to	O
root	O
find	O
t	O
which	O
minimizes	O
f	O
m	O
where	O
t	O
is	O
any	O
of	O
the	O
following	O
the	O
current	O
tree	O
after	O
replacing	O
node	O
j	O
with	O
a	O
leaf	O
the	O
current	O
tree	O
after	O
replacing	O
node	O
j	O
with	O
a	O
leaf	O
the	O
current	O
tree	O
after	O
replacing	O
node	O
j	O
with	O
its	O
left	O
subtree	O
the	O
current	O
tree	O
after	O
replacing	O
node	O
j	O
with	O
its	O
right	O
subtree	O
the	O
current	O
tree	O
let	O
t	O
t	O
random	B
forests	I
threshold-based	O
splitting	O
rules	O
for	O
real-valued	O
features	O
in	O
the	O
previous	O
section	O
we	O
have	O
described	O
an	O
algorithm	O
for	O
growing	O
a	O
decision	O
tree	O
assuming	O
that	O
the	O
features	O
are	O
binary	O
and	O
the	O
splitting	O
rules	O
are	O
of	O
the	O
form	O
we	O
now	O
extend	O
this	O
result	O
to	O
the	O
case	O
of	O
real-valued	O
features	O
and	O
threshold-based	O
splitting	O
rules	O
namely	O
such	O
splitting	O
rules	O
yield	O
decision	B
stumps	I
and	O
we	O
have	O
studied	O
them	O
in	O
chapter	O
the	O
basic	O
idea	O
is	O
to	O
reduce	O
the	O
problem	O
to	O
the	O
case	O
of	O
binary	O
features	O
as	O
follows	O
let	O
xm	O
be	O
the	O
instances	O
of	O
the	O
training	B
set	B
for	O
each	O
real-valued	O
feature	B
i	O
sort	O
the	O
instances	O
so	O
that	O
xmi	O
define	O
a	O
set	B
of	O
thresholds	O
such	O
that	O
ji	O
we	O
use	O
the	O
convention	O
and	O
finally	O
for	O
each	O
i	O
and	O
j	O
we	O
define	O
the	O
binary	O
feature	B
ji	O
once	O
we	O
have	O
constructed	O
these	O
binary	O
features	O
we	O
can	O
run	O
the	O
procedure	O
described	O
in	O
the	O
previous	O
section	O
it	O
is	O
easy	O
to	O
verify	O
that	O
for	O
any	O
decision	O
tree	O
with	O
threshold-based	O
splitting	O
rules	O
over	O
the	O
original	O
real-valued	O
features	O
there	O
exists	O
a	O
decision	O
tree	O
over	O
the	O
constructed	O
binary	O
features	O
with	O
the	O
same	O
training	B
error	I
and	O
the	O
same	O
number	O
of	O
nodes	O
if	O
the	O
original	O
number	O
of	O
real-valued	O
features	O
is	O
d	O
and	O
the	O
number	O
of	O
examples	O
is	O
m	O
then	O
the	O
number	O
of	O
constructed	O
binary	O
features	O
becomes	O
dm	O
calculating	O
the	O
gain	B
of	O
each	O
feature	B
might	O
therefore	O
take	O
operations	O
however	O
using	O
a	O
more	O
clever	O
implementation	O
the	O
runtime	O
can	O
be	O
reduced	O
to	O
odm	O
logm	O
the	O
idea	O
is	O
similar	O
to	O
the	O
implementation	O
of	O
erm	B
for	O
decision	B
stumps	I
as	O
described	O
in	O
section	O
random	B
forests	I
as	O
mentioned	O
before	O
the	O
class	O
of	O
decision	B
trees	I
of	O
arbitrary	O
size	O
has	O
infinite	O
vc	B
dimension	I
we	O
therefore	O
restricted	O
the	O
size	O
of	O
the	O
decision	O
tree	O
another	O
way	O
to	O
reduce	O
the	O
danger	O
of	O
overfitting	B
is	O
by	O
constructing	O
an	O
ensemble	O
of	O
trees	O
in	O
particular	O
in	O
the	O
following	O
we	O
describe	O
the	O
method	O
of	O
random	B
forests	I
introduced	O
by	O
breiman	O
a	O
random	O
forest	O
is	O
a	O
classifier	B
consisting	O
of	O
a	O
collection	O
of	O
decision	B
trees	I
where	O
each	O
tree	O
is	O
constructed	O
by	O
applying	O
an	O
algorithm	O
a	O
on	O
the	O
training	B
set	B
s	O
and	O
an	O
additional	O
random	O
vector	O
where	O
is	O
sampled	O
i	O
i	O
d	O
from	O
some	O
distribution	O
the	O
prediction	O
of	O
the	O
random	O
forest	O
is	O
obtained	O
by	O
a	O
majority	O
vote	O
over	O
the	O
predictions	O
of	O
the	O
individual	O
trees	O
to	O
specify	O
a	O
particular	O
random	O
forest	O
we	O
need	O
to	O
define	O
the	O
algorithm	O
a	O
and	O
the	O
distribution	O
over	O
there	O
are	O
many	O
ways	O
to	O
do	O
this	O
and	O
here	O
we	O
describe	O
one	O
particular	O
option	O
we	O
generate	O
as	O
follows	O
first	O
we	O
take	O
a	O
random	O
subsample	O
from	O
s	O
with	O
replacements	O
namely	O
we	O
sample	O
a	O
new	O
training	B
set	B
of	O
size	O
using	O
the	O
uniform	O
distribution	O
over	O
s	O
second	O
we	O
construct	O
a	O
sequence	O
where	O
each	O
it	O
is	O
a	O
subset	O
of	O
of	O
size	O
k	O
which	O
is	O
generated	O
by	O
sampling	O
uniformly	O
at	O
random	O
elements	O
from	O
all	O
these	O
random	O
variables	O
form	O
the	O
vector	O
then	O
decision	B
trees	I
the	O
algorithm	O
a	O
grows	O
a	O
decision	O
tree	O
using	O
the	O
algorithm	O
based	O
on	O
the	O
sample	O
where	O
at	O
each	O
splitting	O
stage	O
of	O
the	O
algorithm	O
the	O
algorithm	O
is	O
restricted	O
to	O
choosing	O
a	O
feature	B
that	O
maximizes	O
gain	B
from	O
the	O
set	B
it	O
intuitively	O
if	O
k	O
is	O
small	O
this	O
restriction	O
may	O
prevent	O
overfitting	B
summary	O
decision	B
trees	I
are	O
very	O
intuitive	O
predictors	O
typically	O
if	O
a	O
human	O
programmer	O
creates	O
a	O
predictor	B
it	O
will	O
look	O
like	O
a	O
decision	O
tree	O
we	O
have	O
shown	O
that	O
the	O
vc	B
dimension	I
of	O
decision	B
trees	I
with	O
k	O
leaves	O
is	O
k	O
and	O
proposed	O
the	O
mdl	B
paradigm	O
for	O
learning	O
decision	B
trees	I
the	O
main	O
problem	O
with	O
decision	B
trees	I
is	O
that	O
they	O
are	O
computationally	O
hard	O
to	O
learn	O
therefore	O
we	O
described	O
several	O
heuristic	O
procedures	O
for	O
training	O
them	O
bibliographic	O
remarks	O
many	O
algorithms	O
for	O
learning	O
decision	B
trees	I
as	O
and	O
have	O
been	O
derived	O
by	O
quinlan	O
the	O
cart	B
algorithm	O
is	O
due	O
to	O
breiman	O
et	O
al	O
random	B
forests	I
were	O
introduced	O
by	O
breiman	O
for	O
additional	O
reading	O
we	O
refer	O
the	O
reader	O
to	O
tibshirani	O
friedman	O
rokach	O
the	O
proof	O
of	O
the	O
hardness	O
of	O
training	O
decision	B
trees	I
is	O
given	O
in	O
hyafil	O
rivest	O
exercises	O
show	O
that	O
any	O
binary	O
classifier	B
h	O
can	O
be	O
implemented	O
as	O
a	O
decision	O
tree	O
of	O
height	O
at	O
most	O
d	O
with	O
internal	O
nodes	O
of	O
the	O
form	O
for	O
some	O
i	O
d	O
domain	B
is	O
conclude	O
that	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
decision	B
trees	I
over	O
the	O
of	O
consider	O
the	O
following	O
training	B
set	B
where	O
x	O
and	O
y	O
suppose	O
we	O
wish	O
to	O
use	O
this	O
training	B
set	B
in	O
order	O
to	O
build	O
a	O
decision	O
tree	O
of	O
depth	O
for	O
each	O
input	O
we	O
are	O
allowed	O
to	O
ask	O
two	O
questions	O
of	O
the	O
form	O
before	O
deciding	O
on	O
the	O
label	B
exercises	O
suppose	O
we	O
run	O
the	O
algorithm	O
up	O
to	O
depth	O
we	O
pick	O
the	O
root	O
node	O
and	O
its	O
children	O
according	O
to	O
the	O
algorithm	O
but	O
instead	O
of	O
keeping	O
on	O
with	O
the	O
recursion	O
we	O
stop	O
and	O
pick	O
leaves	O
according	O
to	O
the	O
majority	O
label	B
in	O
each	O
subtree	O
assume	O
that	O
the	O
subroutine	O
used	O
to	O
measure	O
the	O
quality	O
of	O
each	O
feature	B
is	O
based	O
on	O
the	O
entropy	B
function	B
we	O
measure	O
the	O
information	B
gain	B
and	O
that	O
if	O
two	O
features	O
get	O
the	O
same	O
score	O
one	O
of	O
them	O
is	O
picked	O
arbitrarily	O
show	O
that	O
the	O
training	B
error	I
of	O
the	O
resulting	O
decision	O
tree	O
is	O
at	O
least	O
find	O
a	O
decision	O
tree	O
of	O
depth	O
that	O
attains	O
zero	O
training	B
error	I
nearest	B
neighbor	I
nearest	B
neighbor	I
algorithms	O
are	O
among	O
the	O
simplest	O
of	O
all	O
machine	O
learning	O
algorithms	O
the	O
idea	O
is	O
to	O
memorize	O
the	O
training	B
set	B
and	O
then	O
to	O
predict	O
the	O
label	B
of	O
any	O
new	O
instance	B
on	O
the	O
basis	O
of	O
the	O
labels	O
of	O
its	O
closest	O
neighbors	O
in	O
the	O
training	B
set	B
the	O
rationale	O
behind	O
such	O
a	O
method	O
is	O
based	O
on	O
the	O
assumption	O
that	O
the	O
features	O
that	O
are	O
used	O
to	O
describe	O
the	O
domain	B
points	O
are	O
relevant	O
to	O
their	O
labelings	O
in	O
a	O
way	O
that	O
makes	O
close-by	O
points	O
likely	O
to	O
have	O
the	O
same	O
label	B
furthermore	O
in	O
some	O
situations	O
even	O
when	O
the	O
training	B
set	B
is	O
immense	O
finding	O
a	O
nearest	B
neighbor	I
can	O
be	O
done	O
extremely	O
fast	O
example	O
when	O
the	O
training	B
set	B
is	O
the	O
entire	O
web	O
and	O
distances	O
are	O
based	O
on	O
links	O
note	O
that	O
in	O
contrast	O
with	O
the	O
algorithmic	O
paradigms	O
that	O
we	O
have	O
discussed	O
so	O
far	O
like	O
erm	B
srm	B
mdl	B
or	O
rlm	B
that	O
are	O
determined	O
by	O
some	O
hypothesis	B
class	I
h	O
the	O
nearest	B
neighbor	I
method	O
figures	O
out	O
a	O
label	B
on	O
any	O
test	O
point	O
without	O
searching	O
for	O
a	O
predictor	B
within	O
some	O
predefined	O
class	O
of	O
functions	O
in	O
this	O
chapter	O
we	O
describe	O
nearest	B
neighbor	I
methods	O
for	O
classification	O
and	O
regression	B
problems	O
we	O
analyze	O
their	O
performance	O
for	O
the	O
simple	O
case	O
of	O
binary	O
classification	O
and	O
discuss	O
the	O
efficiency	O
of	O
implementing	O
these	O
methods	O
k	O
nearest	O
neighbors	O
throughout	O
the	O
entire	O
chapter	O
we	O
assume	O
that	O
our	O
instance	B
domain	B
x	O
is	O
endowed	O
with	O
a	O
metric	O
function	B
that	O
is	O
x	O
x	O
r	O
is	O
a	O
function	B
that	O
returns	O
the	O
distance	O
between	O
any	O
two	O
elements	O
of	O
x	O
for	O
example	O
if	O
x	O
rd	O
then	O
can	O
be	O
the	O
euclidean	O
distance	O
let	O
s	O
ym	O
be	O
a	O
sequence	O
of	O
training	O
examples	O
for	O
each	O
x	O
x	O
let	O
mx	O
be	O
a	O
reordering	O
of	O
m	O
according	O
to	O
their	O
distance	O
to	O
x	O
xi	O
that	O
is	O
for	O
all	O
i	O
m	O
x	O
ix	O
x	O
for	O
a	O
number	O
k	O
the	O
k-nn	B
rule	O
for	O
binary	O
classification	O
is	O
defined	O
as	O
follows	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
analysis	O
figure	O
an	O
illustration	O
of	O
the	O
decision	O
boundaries	O
of	O
the	O
rule	O
the	O
points	O
depicted	O
are	O
the	O
sample	O
points	O
and	O
the	O
predicted	O
label	B
of	O
any	O
new	O
point	O
will	O
be	O
the	O
label	B
of	O
the	O
sample	O
point	O
in	O
the	O
center	O
of	O
the	O
cell	O
it	O
belongs	O
to	O
these	O
cells	O
are	O
called	O
a	O
voronoi	O
tessellation	O
of	O
the	O
space	O
k-nn	B
input	O
a	O
training	O
sample	O
s	O
ym	O
output	O
for	O
every	O
point	O
x	O
x	O
return	O
the	O
majority	O
label	B
among	O
ix	O
i	O
k	O
when	O
k	O
we	O
have	O
the	O
rule	O
hsx	O
y	O
a	O
geometric	O
illustration	O
of	O
the	O
rule	O
is	O
given	O
in	O
figure	O
for	O
regression	B
problems	O
namely	O
y	O
r	O
one	O
can	O
define	O
the	O
prediction	O
to	O
be	O
the	O
average	O
target	O
of	O
the	O
k	O
nearest	O
neighbors	O
that	O
is	O
hsx	O
y	O
ix	O
more	O
generally	O
for	O
some	O
function	B
yk	O
y	O
the	O
k-nn	B
rule	O
with	O
respect	O
k	O
to	O
is	O
hsx	O
y	O
kx	O
y	O
it	O
is	O
easy	O
to	O
verify	O
that	O
we	O
can	O
cast	O
the	O
prediction	O
by	O
majority	O
of	O
labels	O
classification	O
or	O
by	O
the	O
averaged	O
target	O
regression	B
as	O
in	O
equation	O
by	O
an	O
appropriate	O
choice	O
of	O
the	O
generality	O
can	O
lead	O
to	O
other	O
rules	O
for	O
example	O
if	O
y	O
r	O
we	O
can	O
take	O
a	O
weighted	O
average	O
of	O
the	O
targets	O
according	O
to	O
the	O
distance	O
from	O
x	O
hsx	O
x	O
ix	O
x	O
j	O
y	O
ix	O
analysis	O
since	O
the	O
nn	O
rules	O
are	O
such	O
natural	O
learning	O
methods	O
their	O
generalization	O
properties	O
have	O
been	O
extensively	O
studied	O
most	O
previous	O
results	O
are	O
asymptotic	O
consistency	B
results	O
analyzing	O
the	O
performance	O
of	O
nn	O
rules	O
when	O
the	O
sample	O
size	O
m	O
nearest	B
neighbor	I
goes	O
to	O
infinity	O
and	O
the	O
rate	O
of	O
convergence	O
depends	O
on	O
the	O
underlying	O
distribution	O
as	O
we	O
have	O
argued	O
in	O
section	O
this	O
type	O
of	O
analysis	O
is	O
not	O
satisfactory	O
one	O
would	O
like	O
to	O
learn	O
from	O
finite	O
training	O
samples	O
and	O
to	O
understand	O
the	O
generalization	O
performance	O
as	O
a	O
function	B
of	O
the	O
size	O
of	O
such	O
finite	O
training	O
sets	O
and	O
clear	O
prior	O
assumptions	O
on	O
the	O
data	O
distribution	O
we	O
therefore	O
provide	O
a	O
finitesample	O
analysis	O
of	O
the	O
rule	O
showing	O
how	O
the	O
error	O
decreases	O
as	O
a	O
function	B
of	O
m	O
and	O
how	O
it	O
depends	O
on	O
properties	O
of	O
the	O
distribution	O
we	O
will	O
also	O
explain	O
how	O
the	O
analysis	O
can	O
be	O
generalized	O
to	O
k-nn	B
rules	O
for	O
arbitrary	O
values	O
of	O
k	O
in	O
particular	O
the	O
analysis	O
specifies	O
the	O
number	O
of	O
examples	O
required	O
to	O
achieve	O
a	O
true	B
error	I
of	O
where	O
is	O
the	O
bayes	B
optimal	I
hypothesis	B
assuming	O
that	O
the	O
labeling	O
rule	O
is	O
well	O
behaved	O
a	O
sense	O
we	O
will	O
define	O
later	O
a	O
generalization	O
bound	O
for	O
the	O
rule	O
we	O
now	O
analyze	O
the	O
true	B
error	I
of	O
the	O
rule	O
for	O
binary	O
classification	O
with	O
the	O
loss	B
namely	O
y	O
and	O
y	O
we	O
also	O
assume	O
throughout	O
the	O
analysis	O
that	O
x	O
and	O
is	O
the	O
euclidean	O
distance	O
we	O
start	O
by	O
introducing	O
some	O
notation	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
y	O
let	O
dx	O
denote	O
the	O
induced	O
marginal	O
distribution	O
over	O
x	O
and	O
let	O
rd	O
r	O
be	O
the	O
conditional	O
over	O
the	O
labels	O
that	O
is	O
py	O
recall	B
that	O
the	O
bayes	B
optimal	I
rule	O
is	O
the	O
hypothesis	B
that	O
minimizes	O
ldh	O
over	O
all	O
functions	O
is	O
we	O
assume	O
that	O
the	O
conditional	O
probability	O
function	B
is	O
c-lipschitz	O
for	O
some	O
in	O
other	O
words	O
this	O
c	O
namely	O
for	O
all	O
x	O
x	O
assumption	O
means	O
that	O
if	O
two	O
vectors	O
are	O
close	O
to	O
each	O
other	O
then	O
their	O
labels	O
are	O
likely	O
to	O
be	O
the	O
same	O
the	O
following	O
lemma	O
applies	O
the	O
lipschitzness	B
of	O
the	O
conditional	O
probability	O
function	B
to	O
upper	O
bound	O
the	O
true	B
error	I
of	O
the	O
rule	O
as	O
a	O
function	B
of	O
the	O
expected	O
distance	O
between	O
each	O
test	O
instance	B
and	O
its	O
nearest	B
neighbor	I
in	O
the	O
training	B
set	B
lemma	O
let	O
x	O
and	O
d	O
be	O
a	O
distribution	O
over	O
x	O
y	O
for	O
which	O
the	O
conditional	O
probability	O
function	B
is	O
a	O
c-lipschitz	O
function	B
let	O
s	O
ym	O
be	O
an	O
i	O
i	O
d	O
sample	O
and	O
let	O
hs	O
be	O
its	O
corresponding	O
hypothesis	B
let	O
be	O
the	O
bayes	B
optimal	I
rule	O
for	O
then	O
e	O
s	O
dm	O
c	O
s	O
dmx	O
x	O
e	O
formally	O
py	O
lim	O
centered	O
around	O
x	O
bx	O
bx	O
y	O
where	O
bx	O
is	O
a	O
ball	O
of	O
radius	O
analysis	O
proof	O
since	O
ldhs	O
exy	O
we	O
obtain	O
that	O
esldhs	O
is	O
the	O
probability	O
to	O
sample	O
a	O
training	B
set	B
s	O
and	O
an	O
additional	O
example	O
y	O
such	O
that	O
the	O
label	B
of	O
is	O
different	O
from	O
y	O
in	O
other	O
words	O
we	O
can	O
first	O
sample	O
m	O
unlabeled	O
examples	O
sx	O
xm	O
according	O
to	O
dx	O
and	O
an	O
additional	O
unlabeled	O
example	O
x	O
dx	O
then	O
find	O
to	O
be	O
the	O
nearest	B
neighbor	I
of	O
x	O
in	O
sx	O
and	O
finally	O
sample	O
y	O
and	O
y	O
it	O
follows	O
that	O
sx	O
dmx	O
dx	O
e	O
s	O
e	O
e	O
sx	O
dmx	O
dx	O
y	O
p	O
we	O
next	O
upper	O
bound	O
py	O
for	O
any	O
two	O
domain	B
points	O
x	O
p	O
y	O
using	O
and	O
the	O
assumption	O
that	O
is	O
c-lipschitz	O
we	O
obtain	O
that	O
the	O
probability	O
is	O
at	O
most	O
p	O
y	O
plugging	O
this	O
into	O
equation	O
we	O
conclude	O
that	O
c	O
e	O
e	O
e	O
s	O
x	O
sx	O
x	O
finally	O
the	O
error	O
of	O
the	O
bayes	B
optimal	I
classifier	B
is	O
e	O
e	O
x	O
x	O
combining	O
the	O
preceding	O
two	O
inequalities	O
concludes	O
our	O
proof	O
the	O
next	O
step	O
is	O
to	O
bound	O
the	O
expected	O
distance	O
between	O
a	O
random	O
x	O
and	O
its	O
closest	O
element	O
in	O
s	O
we	O
first	O
need	O
the	O
following	O
general	O
probability	O
lemma	O
the	O
lemma	O
bounds	O
the	O
probability	O
weight	O
of	O
subsets	O
that	O
are	O
not	O
hit	O
by	O
a	O
random	O
sample	O
as	O
a	O
function	B
of	O
the	O
size	O
of	O
that	O
sample	O
lemma	O
let	O
cr	O
be	O
a	O
collection	O
of	O
subsets	O
of	O
some	O
domain	B
set	B
x	O
let	O
s	O
be	O
a	O
sequence	O
of	O
m	O
points	O
sampled	O
i	O
i	O
d	O
according	O
to	O
some	O
probability	O
distribution	O
d	O
over	O
x	O
then	O
ici	O
s	O
r	O
m	O
e	O
pci	O
e	O
s	O
dm	O
nearest	B
neighbor	I
proof	O
from	O
the	O
linearity	O
of	O
expectation	O
we	O
can	O
rewrite	O
s	O
pci	O
e	O
s	O
next	O
for	O
each	O
i	O
we	O
have	O
pci	O
e	O
s	O
ici	O
s	O
p	O
s	O
pci	O
e	O
s	O
s	O
ici	O
s	O
e	O
s	O
s	O
pcim	O
e	O
pci	O
m	O
combining	O
the	O
preceding	O
two	O
equations	O
we	O
get	O
pci	O
e	O
pci	O
m	O
r	O
max	O
i	O
pci	O
e	O
pci	O
m	O
finally	O
by	O
a	O
standard	O
calculus	O
maxa	O
ae	O
ma	O
me	O
and	O
this	O
concludes	O
the	O
proof	O
equipped	O
with	O
the	O
preceding	O
lemmas	O
we	O
are	O
now	O
ready	O
to	O
state	O
and	O
prove	O
the	O
main	O
result	O
of	O
this	O
section	O
an	O
upper	O
bound	O
on	O
the	O
expected	O
error	O
of	O
the	O
learning	O
rule	O
theorem	O
let	O
x	O
and	O
d	O
be	O
a	O
distribution	O
over	O
x	O
y	O
for	O
which	O
the	O
conditional	O
probability	O
function	B
is	O
a	O
c-lipschitz	O
function	B
let	O
hs	O
denote	O
the	O
result	O
of	O
applying	O
the	O
rule	O
to	O
a	O
sample	O
s	O
dm	O
then	O
e	O
s	O
dm	O
c	O
d	O
m	O
proof	O
fix	O
some	O
for	O
some	O
integer	O
t	O
let	O
r	O
t	O
d	O
and	O
let	O
cr	O
be	O
the	O
cover	O
of	O
the	O
set	B
x	O
using	O
boxes	O
of	O
length	O
namely	O
for	O
every	O
d	O
there	O
exists	O
a	O
set	B
ci	O
of	O
the	O
form	O
j	O
xj	O
j	O
jt	O
an	O
illustration	O
for	O
d	O
t	O
and	O
the	O
set	B
corresponding	O
to	O
is	O
given	O
in	O
the	O
following	O
therefore	O
p	O
for	O
each	O
x	O
in	O
the	O
same	O
box	O
we	O
have	O
and	O
by	O
combining	O
lemma	O
with	O
the	O
trivial	O
bound	O
r	O
me	O
x	O
e	O
x	O
ici	O
s	O
d	O
p	O
get	O
that	O
e	O
xs	O
ci	O
s	O
e	O
xs	O
ici	O
d	O
otherwise	O
d	O
d	O
ci	O
ici	O
ci	O
we	O
analysis	O
since	O
the	O
number	O
of	O
boxes	O
is	O
r	O
we	O
get	O
that	O
x	O
e	O
sx	O
d	O
d	O
d	O
m	O
e	O
m	O
e	O
combining	O
the	O
preceding	O
with	O
lemma	O
we	O
obtain	O
that	O
d	O
finally	O
setting	O
m	O
and	O
noting	O
that	O
c	O
e	O
s	O
d	O
m	O
e	O
we	O
conclude	O
our	O
proof	O
d	O
m	O
e	O
m	O
m	O
the	O
theorem	O
implies	O
that	O
if	O
we	O
first	O
fix	O
the	O
data-generating	O
distribution	O
and	O
then	O
let	O
m	O
go	O
to	O
infinity	O
then	O
the	O
error	O
of	O
the	O
rule	O
converges	O
to	O
twice	O
the	O
bayes	O
error	O
the	O
analysis	O
can	O
be	O
generalized	O
to	O
larger	O
values	O
of	O
k	O
showing	O
that	O
the	O
expected	O
error	O
of	O
the	O
k-nn	B
rule	O
converges	O
to	O
times	O
the	O
error	O
of	O
the	O
bayes	O
classifier	B
this	O
is	O
formalized	O
in	O
theorem	O
whose	O
proof	O
is	O
left	O
as	O
a	O
guided	O
exercise	O
the	O
curse	B
of	I
dimensionality	I
the	O
upper	O
bound	O
given	O
in	O
theorem	O
grows	O
with	O
c	O
lipschitz	O
coefficient	O
of	O
and	O
with	O
d	O
the	O
euclidean	O
dimension	O
of	O
the	O
domain	B
set	B
x	O
in	O
fact	O
it	O
is	O
easy	O
to	O
see	O
that	O
a	O
necessary	O
condition	O
for	O
the	O
last	O
term	O
in	O
theorem	O
to	O
be	O
smaller	O
than	O
is	O
that	O
m	O
c	O
that	O
is	O
the	O
size	O
of	O
the	O
training	B
set	B
should	O
increase	O
exponentially	O
with	O
the	O
dimension	O
the	O
following	O
theorem	O
tells	O
us	O
that	O
this	O
is	O
not	O
just	O
an	O
artifact	O
of	O
our	O
upper	O
bound	O
but	O
for	O
some	O
distributions	O
this	O
amount	O
of	O
examples	O
is	O
indeed	O
necessary	O
for	O
learning	O
with	O
the	O
nn	O
rule	O
theorem	O
for	O
any	O
c	O
and	O
every	O
learning	O
rule	O
l	O
there	O
exists	O
a	O
distribution	O
over	O
such	O
that	O
is	O
c-lipschitz	O
the	O
bayes	O
error	O
of	O
the	O
distribution	O
is	O
but	O
for	O
sample	O
sizes	O
m	O
the	O
true	B
error	I
of	O
the	O
rule	O
l	O
is	O
greater	O
than	O
proof	O
fix	O
any	O
values	O
of	O
c	O
and	O
d	O
let	O
gd	O
c	O
be	O
the	O
grid	O
on	O
with	O
distance	O
of	O
between	O
points	O
on	O
the	O
grid	O
that	O
is	O
each	O
point	O
on	O
the	O
grid	O
is	O
of	O
the	O
form	O
adc	O
where	O
ai	O
is	O
in	O
c	O
c	O
note	O
that	O
since	O
any	O
two	O
distinct	O
c	O
is	O
a	O
points	O
on	O
this	O
grid	O
are	O
at	O
least	O
apart	O
any	O
function	B
gd	O
c-lipschitz	O
function	B
it	O
follows	O
that	O
the	O
set	B
of	O
all	O
c-lipschitz	O
functions	O
over	O
gd	O
c	O
contains	O
the	O
set	B
of	O
all	O
binary	O
valued	O
functions	O
over	O
that	O
domain	B
we	O
can	O
therefore	O
invoke	O
the	O
no-free-lunch	B
result	O
to	O
obtain	O
a	O
lower	O
bound	O
on	O
the	O
needed	O
sample	O
sizes	O
for	O
learning	O
that	O
class	O
the	O
number	O
of	O
points	O
on	O
the	O
grid	O
is	O
hence	O
if	O
m	O
theorem	O
implies	O
the	O
lower	O
bound	O
we	O
are	O
after	O
nearest	B
neighbor	I
the	O
exponential	O
dependence	O
on	O
the	O
dimension	O
is	O
known	O
as	O
the	O
curse	B
of	I
dimensionality	I
as	O
we	O
saw	O
the	O
rule	O
might	O
fail	O
if	O
the	O
number	O
of	O
examples	O
is	O
smaller	O
than	O
therefore	O
while	O
the	O
rule	O
does	O
not	O
restrict	O
itself	O
to	O
a	O
predefined	O
set	B
of	O
hypotheses	O
it	O
still	O
relies	O
on	O
some	O
prior	B
knowledge	I
its	O
success	O
depends	O
on	O
the	O
assumption	O
that	O
the	O
dimension	O
and	O
the	O
lipschitz	O
constant	O
of	O
the	O
underlying	O
distribution	O
are	O
not	O
too	O
high	O
efficient	O
implementation	O
nearest	B
neighbor	I
is	O
a	O
learning-by-memorization	O
type	O
of	O
rule	O
it	O
requires	O
the	O
entire	O
training	O
data	O
set	B
to	O
be	O
stored	O
and	O
at	O
test	O
time	O
we	O
need	O
to	O
scan	O
the	O
entire	O
data	O
set	B
in	O
order	O
to	O
find	O
the	O
neighbors	O
the	O
time	O
of	O
applying	O
the	O
nn	O
rule	O
is	O
therefore	O
m	O
this	O
leads	O
to	O
expensive	O
computation	O
at	O
test	O
time	O
when	O
d	O
is	O
small	O
several	O
results	O
from	O
the	O
field	O
of	O
computational	O
geometry	O
have	O
proposed	O
data	O
structures	O
that	O
enable	O
to	O
apply	O
the	O
nn	O
rule	O
in	O
time	O
logm	O
however	O
the	O
space	O
required	O
by	O
these	O
data	O
structures	O
is	O
roughly	O
mod	O
which	O
makes	O
these	O
methods	O
impractical	O
for	O
larger	O
values	O
of	O
d	O
to	O
overcome	O
this	O
problem	O
it	O
was	O
suggested	O
to	O
improve	O
the	O
search	O
method	O
by	O
allowing	O
an	O
approximate	O
search	O
formally	O
an	O
r-approximate	O
search	O
procedure	O
is	O
guaranteed	O
to	O
retrieve	O
a	O
point	O
within	O
distance	O
of	O
at	O
most	O
r	O
times	O
the	O
distance	O
to	O
the	O
nearest	B
neighbor	I
three	O
popular	O
approximate	O
algorithms	O
for	O
nn	O
are	O
the	O
kd-tree	O
balltrees	O
and	O
locality-sensitive	O
hashing	O
we	O
refer	O
the	O
reader	O
for	O
example	O
to	O
darrell	O
indyk	O
summary	O
the	O
k-nn	B
rule	O
is	O
a	O
very	O
simple	O
learning	O
algorithm	O
that	O
relies	O
on	O
the	O
assumption	O
that	O
things	O
that	O
look	O
alike	O
must	O
be	O
alike	O
we	O
formalized	O
this	O
intuition	O
using	O
the	O
lipschitzness	B
of	O
the	O
conditional	O
probability	O
we	O
have	O
shown	O
that	O
with	O
a	O
sufficiently	O
large	O
training	B
set	B
the	O
risk	B
of	O
the	O
is	O
upper	O
bounded	O
by	O
twice	O
the	O
risk	B
of	O
the	O
bayes	B
optimal	I
rule	O
we	O
have	O
also	O
derived	O
a	O
lower	O
bound	O
that	O
shows	O
the	O
curse	B
of	I
dimensionality	I
the	O
required	O
sample	O
size	O
might	O
increase	O
exponentially	O
with	O
the	O
dimension	O
as	O
a	O
result	O
nn	O
is	O
usually	O
performed	O
in	O
practice	O
after	O
a	O
dimensionality	B
reduction	I
preprocessing	O
step	O
we	O
discuss	O
dimensionality	B
reduction	I
techniques	O
later	O
on	O
in	O
chapter	O
bibliographic	O
remarks	O
cover	O
hart	O
gave	O
the	O
first	O
analysis	O
of	O
showing	O
that	O
its	O
risk	B
converges	O
to	O
twice	O
the	O
bayes	B
optimal	I
error	O
under	O
mild	O
conditions	O
following	O
a	O
lemma	O
due	O
to	O
stone	O
devroye	O
gy	O
orfi	O
have	O
shown	O
that	O
the	O
k-nn	B
rule	O
exercises	O
is	O
consistent	B
respect	O
to	O
the	O
hypothesis	B
class	I
of	O
all	O
functions	O
from	O
rd	O
to	O
a	O
good	O
presentation	O
of	O
the	O
analysis	O
is	O
given	O
in	O
the	O
book	O
of	O
devroye	O
et	O
al	O
here	O
we	O
give	O
a	O
finite	O
sample	O
guarantee	O
that	O
explicitly	O
underscores	O
the	O
prior	O
assumption	O
on	O
the	O
distribution	O
see	O
section	O
for	O
a	O
discussion	O
on	O
consistency	B
results	O
finally	O
gottlieb	O
kontorovich	O
krauthgamer	O
derived	O
another	O
finite	O
sample	O
bound	O
for	O
nn	O
that	O
is	O
more	O
similar	O
to	O
vc	O
bounds	O
exercises	O
in	O
this	O
exercise	O
we	O
will	O
prove	O
the	O
following	O
theorem	O
for	O
the	O
k-nn	B
rule	O
theorem	O
let	O
x	O
and	O
d	O
be	O
a	O
distribution	O
over	O
x	O
y	O
for	O
which	O
the	O
conditional	O
probability	O
function	B
is	O
a	O
c-lipschitz	O
function	B
let	O
hs	O
denote	O
the	O
result	O
of	O
applying	O
the	O
k-nn	B
rule	O
to	O
a	O
sample	O
s	O
dm	O
where	O
k	O
let	O
be	O
the	O
bayes	B
optimal	I
hypothesis	B
then	O
k	O
e	O
s	O
c	O
d	O
k	O
m	O
prove	O
the	O
following	O
lemma	O
lemma	O
let	O
cr	O
be	O
a	O
collection	O
of	O
subsets	O
of	O
some	O
domain	B
set	B
x	O
let	O
s	O
be	O
a	O
sequence	O
of	O
m	O
points	O
sampled	O
i	O
i	O
d	O
according	O
to	O
some	O
probability	O
distribution	O
d	O
over	O
x	O
then	O
for	O
every	O
k	O
e	O
s	O
dm	O
ici	O
sk	O
pci	O
pci	O
pci	O
p	O
m	O
s	O
hints	O
show	O
that	O
e	O
s	O
ici	O
sk	O
s	O
k	O
fix	O
some	O
i	O
and	O
suppose	O
that	O
k	O
pci	O
use	O
chernoff	O
s	O
bound	O
to	O
show	O
that	O
p	O
s	O
s	O
k	O
p	O
s	O
e	O
pci	O
s	O
use	O
the	O
inequality	O
maxa	O
ae	O
ma	O
me	O
to	O
show	O
that	O
for	O
such	O
i	O
we	O
have	O
pci	O
p	O
s	O
s	O
k	O
pcie	O
pci	O
me	O
conclude	O
the	O
proof	O
by	O
using	O
the	O
fact	O
that	O
for	O
the	O
case	O
k	O
pci	O
we	O
clearly	O
have	O
pci	O
p	O
s	O
s	O
k	O
pci	O
m	O
nearest	B
neighbor	I
we	O
use	O
the	O
notation	O
y	O
p	O
as	O
a	O
shorthand	O
for	O
y	O
is	O
a	O
bernoulli	O
random	O
variable	O
with	O
expected	O
value	O
p	O
prove	O
the	O
following	O
lemma	O
lemma	O
let	O
k	O
and	O
let	O
zk	O
be	O
independent	O
bernoulli	O
random	O
variables	O
with	O
pzi	O
pi	O
denote	O
p	O
zi	O
show	O
that	O
i	O
pi	O
and	O
k	O
k	O
e	O
p	O
y	O
p	O
k	O
p	O
y	O
p	O
hints	O
w	O
l	O
o	O
g	O
assume	O
that	O
p	O
then	O
py	O
py	O
p	O
let	O
show	O
that	O
e	O
p	O
y	O
p	O
p	O
p	O
use	O
chernoff	O
s	O
bound	O
to	O
show	O
that	O
where	O
e	O
k	O
p	O
h	O
ha	O
a	O
a	O
a	O
to	O
conclude	O
the	O
proof	O
of	O
the	O
lemma	O
you	O
can	O
rely	O
on	O
the	O
following	O
inequality	O
proving	O
it	O
for	O
every	O
p	O
and	O
k	O
e	O
k	O
p	O
k	O
fix	O
some	O
p	O
and	O
show	O
that	O
p	O
y	O
p	O
p	O
y	O
p	O
k	O
conclude	O
the	O
proof	O
of	O
the	O
theorem	O
according	O
to	O
the	O
following	O
steps	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
six	O
some	O
and	O
let	O
cr	O
be	O
the	O
cover	O
of	O
the	O
set	B
x	O
using	O
boxes	O
of	O
length	O
for	O
each	O
x	O
in	O
the	O
same	O
box	O
we	O
have	O
d	O
show	O
that	O
d	O
otherwise	O
pci	O
e	O
e	O
s	O
s	O
max	O
i	O
p	O
sxy	O
ici	O
sk	O
hsx	O
y	O
j	O
x	O
j	O
d	O
bound	O
the	O
first	O
summand	O
using	O
lemma	O
to	O
bound	O
the	O
second	O
summand	O
let	O
us	O
fix	O
sx	O
and	O
x	O
such	O
that	O
all	O
the	O
k	O
neighbors	O
of	O
x	O
in	O
sx	O
are	O
at	O
distance	O
of	O
at	O
most	O
d	O
from	O
x	O
w	O
l	O
o	O
g	O
assume	O
that	O
the	O
k	O
nn	O
are	O
xk	O
denote	O
pi	O
and	O
let	O
p	O
k	O
i	O
pi	O
use	O
exercise	O
to	O
show	O
that	O
y	O
e	O
e	O
p	O
y	O
p	O
y	O
p	O
y	O
exercises	O
w	O
l	O
o	O
g	O
assume	O
that	O
p	O
now	O
use	O
lemma	O
to	O
show	O
that	O
k	O
y	O
p	O
y	O
p	O
p	O
p	O
y	O
p	O
y	O
show	O
that	O
p	O
y	O
p	O
y	O
p	O
minp	O
p	O
min	O
combine	O
all	O
the	O
preceding	O
to	O
obtain	O
that	O
the	O
second	O
summand	O
in	O
equa	O
tion	O
is	O
bounded	O
c	O
d	O
k	O
m	O
k	O
d	O
k	O
m	O
d	O
k	O
m	O
use	O
r	O
to	O
obtain	O
that	O
e	O
s	O
c	O
set	B
and	O
use	O
e	O
c	O
m	O
d	O
to	O
conclude	O
the	O
proof	O
neural	B
networks	I
an	O
artificial	O
neural	O
network	O
is	O
a	O
model	O
of	O
computation	O
inspired	O
by	O
the	O
structure	O
of	O
neural	B
networks	I
in	O
the	O
brain	O
in	O
simplified	O
models	O
of	O
the	O
brain	O
it	O
consists	O
of	O
a	O
large	O
number	O
of	O
basic	O
computing	O
devices	O
that	O
are	O
connected	O
to	O
each	O
other	O
in	O
a	O
complex	O
communication	O
network	O
through	O
which	O
the	O
brain	O
is	O
able	O
to	O
carry	O
out	O
highly	O
complex	O
computations	O
artificial	O
neural	B
networks	I
are	O
formal	O
computation	O
constructs	O
that	O
are	O
modeled	O
after	O
this	O
computation	O
paradigm	O
learning	O
with	O
neural	B
networks	I
was	O
proposed	O
in	O
the	O
century	O
it	O
yields	O
an	O
effective	O
learning	O
paradigm	O
and	O
has	O
recently	O
been	O
shown	O
to	O
achieve	O
cuttingedge	O
performance	O
on	O
several	O
learning	O
tasks	O
a	O
neural	O
network	O
can	O
be	O
described	O
as	O
a	O
directed	O
graph	O
whose	O
nodes	O
correspond	O
to	O
neurons	O
and	O
edges	O
correspond	O
to	O
links	O
between	O
them	O
each	O
neuron	O
receives	O
as	O
input	O
a	O
weighted	O
sum	O
of	O
the	O
outputs	O
of	O
the	O
neurons	O
connected	O
to	O
its	O
incoming	O
edges	O
we	O
focus	O
on	O
feedforward	B
networks	I
in	O
which	O
the	O
underlying	O
graph	O
does	O
not	O
contain	O
cycles	O
in	O
the	O
context	O
of	O
learning	O
we	O
can	O
define	O
a	O
hypothesis	B
class	I
consisting	O
of	O
neural	O
network	O
predictors	O
where	O
all	O
the	O
hypotheses	O
share	O
the	O
underlying	O
graph	O
structure	O
of	O
the	O
network	O
and	O
differ	O
in	O
the	O
weights	O
over	O
edges	O
as	O
we	O
will	O
show	O
in	O
section	O
every	O
predictor	B
over	O
n	O
variables	O
that	O
can	O
be	O
implemented	O
in	O
time	O
t	O
can	O
also	O
be	O
expressed	O
as	O
a	O
neural	O
network	O
predictor	B
of	O
size	O
ot	O
where	O
the	O
size	O
of	O
the	O
network	O
is	O
the	O
number	O
of	O
nodes	O
in	O
it	O
it	O
follows	O
that	O
the	O
family	O
of	O
hypothesis	B
classes	O
of	O
neural	B
networks	I
of	O
polynomial	O
size	O
can	O
suffice	O
for	O
all	O
practical	O
learning	O
tasks	O
in	O
which	O
our	O
goal	O
is	O
to	O
learn	O
predictors	O
which	O
can	O
be	O
implemented	O
efficiently	O
furthermore	O
in	O
section	O
we	O
will	O
show	O
that	O
the	O
sample	B
complexity	I
of	O
learning	O
such	O
hypothesis	B
classes	O
is	O
also	O
bounded	O
in	O
terms	O
of	O
the	O
size	O
of	O
the	O
network	O
hence	O
it	O
seems	O
that	O
this	O
is	O
the	O
ultimate	O
learning	O
paradigm	O
we	O
would	O
want	O
to	O
adapt	O
in	O
the	O
sense	O
that	O
it	O
both	O
has	O
a	O
polynomial	O
sample	B
complexity	I
and	O
has	O
the	O
minimal	O
approximation	B
error	I
among	O
all	O
hypothesis	B
classes	O
consisting	O
of	O
efficiently	O
implementable	O
predictors	O
the	O
caveat	O
is	O
that	O
the	O
problem	O
of	O
training	O
such	O
hypothesis	B
classes	O
of	O
neural	O
network	O
predictors	O
is	O
computationally	O
hard	O
this	O
will	O
be	O
formalized	O
in	O
section	O
a	O
widely	O
used	O
heuristic	O
for	O
training	O
neural	B
networks	I
relies	O
on	O
the	O
sgd	B
framework	O
we	O
studied	O
in	O
chapter	O
there	O
we	O
have	O
shown	O
that	O
sgd	B
is	O
a	O
successful	O
learner	O
if	O
the	O
loss	B
function	B
is	O
convex	B
in	O
neural	B
networks	I
the	O
loss	B
function	B
is	O
highly	O
nonconvex	O
nevertheless	O
we	O
can	O
still	O
implement	O
the	O
sgd	B
algorithm	O
and	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
feedforward	O
neural	B
networks	I
hope	O
it	O
will	O
find	O
a	O
reasonable	O
solution	O
happens	O
to	O
be	O
the	O
case	O
in	O
several	O
practical	O
tasks	O
in	O
section	O
we	O
describe	O
how	O
to	O
implement	O
sgd	B
for	O
neural	B
networks	I
in	O
particular	O
the	O
most	O
complicated	O
operation	O
is	O
the	O
calculation	O
of	O
the	O
gradient	B
of	O
the	O
loss	B
function	B
with	O
respect	O
to	O
the	O
parameters	O
of	O
the	O
network	O
we	O
present	O
the	O
backpropagation	B
algorithm	O
that	O
efficiently	O
calculates	O
the	O
gradient	B
feedforward	O
neural	B
networks	I
the	O
idea	O
behind	O
neural	B
networks	I
is	O
that	O
many	O
neurons	O
can	O
be	O
joined	O
together	O
by	O
communication	O
links	O
to	O
carry	O
out	O
complex	O
computations	O
it	O
is	O
common	O
to	O
describe	O
the	O
structure	O
of	O
a	O
neural	O
network	O
as	O
a	O
graph	O
whose	O
nodes	O
are	O
the	O
neurons	O
and	O
each	O
edge	O
in	O
the	O
graph	O
links	O
the	O
output	O
of	O
some	O
neuron	O
to	O
the	O
input	O
of	O
another	O
neuron	O
we	O
will	O
restrict	O
our	O
attention	O
to	O
feedforward	O
network	O
structures	O
in	O
which	O
the	O
underlying	O
graph	O
does	O
not	O
contain	O
cycles	O
a	O
feedforward	O
neural	O
network	O
is	O
described	O
by	O
a	O
directed	O
acyclic	O
graph	O
g	O
e	O
and	O
a	O
weight	O
function	B
over	O
the	O
edges	O
w	O
e	O
r	O
nodes	O
of	O
the	O
graph	O
correspond	O
to	O
neurons	O
each	O
single	O
neuron	O
is	O
modeled	O
as	O
a	O
simple	O
scalar	O
function	B
r	O
r	O
we	O
will	O
focus	O
on	O
three	O
possible	O
functions	O
for	O
the	O
sign	O
function	B
signa	O
the	O
threshold	O
function	B
and	O
the	O
sigmoid	O
function	B
exp	O
a	O
which	O
is	O
a	O
smooth	O
approximation	O
to	O
the	O
threshold	O
function	B
we	O
call	O
the	O
activation	B
function	B
of	O
the	O
neuron	O
each	O
edge	O
in	O
the	O
graph	O
links	O
the	O
output	O
of	O
some	O
neuron	O
to	O
the	O
input	O
of	O
another	O
neuron	O
the	O
input	O
of	O
a	O
neuron	O
is	O
obtained	O
by	O
taking	O
a	O
weighted	O
sum	O
of	O
the	O
outputs	O
of	O
all	O
the	O
neurons	O
connected	O
to	O
it	O
where	O
the	O
weighting	O
is	O
according	O
to	O
w	O
to	O
simplify	O
the	O
description	O
of	O
the	O
calculation	O
performed	O
by	O
the	O
network	O
we	O
further	O
assume	O
that	O
the	O
network	O
is	O
organized	O
in	O
layers	O
that	O
is	O
the	O
set	B
of	O
nodes	O
can	O
be	O
decomposed	O
into	O
a	O
union	O
of	O
disjoint	O
subsets	O
v	O
t	O
such	O
that	O
every	O
edge	O
in	O
e	O
connects	O
some	O
node	O
in	O
vt	O
to	O
some	O
node	O
in	O
vt	O
for	O
some	O
t	O
the	O
bottom	O
layer	O
is	O
called	O
the	O
input	O
layer	O
it	O
contains	O
n	O
neurons	O
where	O
n	O
is	O
the	O
dimensionality	O
of	O
the	O
input	O
space	O
for	O
every	O
i	O
the	O
output	O
of	O
neuron	O
i	O
in	O
is	O
simply	O
xi	O
the	O
last	O
neuron	O
in	O
is	O
the	O
constant	O
neuron	O
which	O
always	O
outputs	O
we	O
denote	O
by	O
vti	O
the	O
ith	O
neuron	O
of	O
the	O
tth	O
layer	O
and	O
by	O
otix	O
the	O
output	O
of	O
vti	O
when	O
the	O
network	O
is	O
fed	O
with	O
the	O
input	O
vector	O
x	O
therefore	O
for	O
i	O
we	O
have	O
xi	O
and	O
for	O
i	O
n	O
we	O
have	O
we	O
now	O
proceed	O
with	O
the	O
calculation	O
in	O
a	O
layer	O
by	O
layer	O
manner	O
suppose	O
we	O
have	O
calculated	O
the	O
outputs	O
of	O
the	O
neurons	O
at	O
layer	O
t	O
then	O
we	O
can	O
calculate	O
the	O
outputs	O
of	O
the	O
neurons	O
at	O
layer	O
t	O
as	O
follows	O
fix	O
some	O
let	O
denote	O
the	O
input	O
to	O
when	O
the	O
network	O
is	O
fed	O
with	O
the	O
input	O
vector	O
x	O
then	O
wvtr	O
otrx	O
r	O
e	O
neural	B
networks	I
and	O
that	O
is	O
the	O
input	O
to	O
is	O
a	O
weighted	O
sum	O
of	O
the	O
outputs	O
of	O
the	O
neurons	O
in	O
vt	O
that	O
are	O
connected	O
to	O
where	O
weighting	O
is	O
according	O
to	O
w	O
and	O
the	O
output	O
of	O
is	O
simply	O
the	O
application	O
of	O
the	O
activation	B
function	B
on	O
its	O
input	O
layers	O
vt	O
are	O
often	O
called	O
hidden	B
layers	I
the	O
top	O
layer	O
vt	O
is	O
called	O
the	O
output	O
layer	O
in	O
simple	O
prediction	O
problems	O
the	O
output	O
layer	O
contains	O
a	O
single	O
neuron	O
whose	O
output	O
is	O
the	O
output	O
of	O
the	O
network	O
we	O
refer	O
to	O
t	O
as	O
the	O
number	O
of	O
layers	O
in	O
the	O
network	O
or	O
the	O
depth	O
of	O
the	O
network	O
the	O
size	O
of	O
the	O
network	O
is	O
the	O
width	O
of	O
the	O
network	O
is	O
maxt	O
an	O
illustration	O
of	O
a	O
layered	O
feedforward	O
neural	O
network	O
of	O
depth	O
size	O
and	O
width	O
is	O
given	O
in	O
the	O
following	O
note	O
that	O
there	O
is	O
a	O
neuron	O
in	O
the	O
hidden	O
layer	O
that	O
has	O
no	O
incoming	O
edges	O
this	O
neuron	O
will	O
output	O
the	O
constant	O
input	O
layer	O
hidden	O
layer	O
output	O
layer	O
constant	O
output	O
learning	O
neural	B
networks	I
once	O
we	O
have	O
specified	O
a	O
neural	O
network	O
by	O
e	O
w	O
we	O
obtain	O
a	O
function	B
hve	O
rvt	O
any	O
set	B
of	O
such	O
functions	O
can	O
serve	O
as	O
a	O
hypothesis	B
class	I
for	O
learning	O
usually	O
we	O
define	O
a	O
hypothesis	B
class	I
of	O
neural	O
network	O
predictors	O
by	O
fixing	O
the	O
graph	O
e	O
as	O
well	O
as	O
the	O
activation	B
function	B
and	O
letting	O
the	O
hypothesis	B
class	I
be	O
all	O
functions	O
of	O
the	O
form	O
hve	O
for	O
some	O
w	O
e	O
r	O
the	O
triplet	O
e	O
is	O
often	O
called	O
the	O
architecture	O
of	O
the	O
network	O
we	O
denote	O
the	O
hypothesis	B
class	I
by	O
hve	O
w	O
is	O
a	O
mapping	O
from	O
e	O
to	O
r	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
that	O
is	O
the	O
parameters	O
specifying	O
a	O
hypothesis	B
in	O
the	O
hypothesis	B
class	I
are	O
the	O
weights	O
over	O
the	O
edges	O
of	O
the	O
network	O
we	O
can	O
now	O
study	O
the	O
approximation	B
error	I
estimation	B
error	I
and	O
optimization	B
error	I
of	O
such	O
hypothesis	B
classes	O
in	O
section	O
we	O
study	O
the	O
approximation	B
error	I
of	O
hve	O
by	O
studying	O
what	O
type	O
of	O
functions	O
hypotheses	O
in	O
hve	O
can	O
implement	O
in	O
terms	O
of	O
the	O
size	O
of	O
the	O
underlying	O
graph	O
in	O
section	O
we	O
study	O
the	O
estimation	B
error	I
of	O
hve	O
for	O
the	O
case	O
of	O
binary	O
classification	O
vt	O
and	O
is	O
the	O
sign	O
function	B
by	O
analyzing	O
its	O
vc	B
dimension	I
finally	O
in	O
section	O
we	O
show	O
that	O
it	O
is	O
computationally	O
hard	O
to	O
learn	O
the	O
class	O
hve	O
even	O
if	O
the	O
underlying	O
graph	O
is	O
small	O
and	O
in	O
section	O
we	O
present	O
the	O
most	O
commonly	O
used	O
heuristic	O
for	O
training	O
hve	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
in	O
this	O
section	O
we	O
study	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
namely	O
what	O
type	O
of	O
functions	O
can	O
be	O
implemented	O
using	O
a	O
neural	O
network	O
more	O
concretely	O
we	O
will	O
fix	O
some	O
architecture	O
v	O
e	O
and	O
will	O
study	O
what	O
functions	O
hypotheses	O
in	O
hve	O
can	O
implement	O
as	O
a	O
function	B
of	O
the	O
size	O
of	O
v	O
we	O
start	O
the	O
discussion	O
with	O
studying	O
which	O
type	O
of	O
boolean	O
functions	O
functions	O
from	O
to	O
can	O
be	O
implemented	O
by	O
hvesign	O
observe	O
that	O
for	O
every	O
computer	O
in	O
which	O
real	O
numbers	O
are	O
stored	O
using	O
b	O
bits	O
whenever	O
we	O
calculate	O
a	O
function	B
f	O
rn	O
r	O
on	O
such	O
a	O
computer	O
we	O
in	O
fact	O
calculate	O
a	O
function	B
g	O
therefore	O
studying	O
which	O
boolean	O
functions	O
can	O
be	O
implemented	O
by	O
hvesign	O
can	O
tell	O
us	O
which	O
functions	O
can	O
be	O
implemented	O
on	O
a	O
computer	O
that	O
stores	O
real	O
numbers	O
using	O
b	O
bits	O
we	O
begin	O
with	O
a	O
simple	O
claim	O
showing	O
that	O
without	O
restricting	O
the	O
size	O
of	O
the	O
network	O
every	O
boolean	O
function	B
can	O
be	O
implemented	O
using	O
a	O
neural	O
network	O
of	O
depth	O
claim	O
for	O
every	O
n	O
there	O
exists	O
a	O
graph	O
e	O
of	O
depth	O
such	O
that	O
hvesign	O
contains	O
all	O
functions	O
from	O
to	O
proof	O
we	O
construct	O
a	O
graph	O
with	O
n	O
and	O
let	O
e	O
be	O
all	O
possible	O
edges	O
between	O
adjacent	O
layers	O
now	O
let	O
f	O
be	O
some	O
boolean	O
function	B
we	O
need	O
to	O
show	O
that	O
we	O
can	O
adjust	O
the	O
weights	O
so	O
that	O
the	O
network	O
will	O
implement	O
f	O
let	O
uk	O
be	O
all	O
vectors	O
in	O
on	O
which	O
f	O
outputs	O
observe	O
that	O
for	O
every	O
i	O
and	O
every	O
x	O
if	O
x	O
ui	O
then	O
n	O
and	O
if	O
x	O
ui	O
then	O
n	O
it	O
follows	O
that	O
the	O
function	B
gix	O
n	O
equals	O
if	O
and	O
only	O
if	O
x	O
ui	O
it	O
follows	O
that	O
we	O
can	O
adapt	O
the	O
weights	O
between	O
and	O
so	O
that	O
for	O
every	O
i	O
the	O
neuron	O
implements	O
the	O
function	B
gix	O
next	O
we	O
observe	O
that	O
f	O
is	O
the	O
disjunction	O
of	O
neural	B
networks	I
the	O
functions	O
gix	O
and	O
therefore	O
can	O
be	O
written	O
as	O
f	O
sign	O
gix	O
k	O
which	O
concludes	O
our	O
proof	O
the	O
preceding	O
claim	O
shows	O
that	O
neural	B
networks	I
can	O
implement	O
any	O
boolean	O
function	B
however	O
this	O
is	O
a	O
very	O
weak	O
property	O
as	O
the	O
size	O
of	O
the	O
resulting	O
network	O
might	O
be	O
exponentially	O
large	O
in	O
the	O
construction	O
given	O
at	O
the	O
proof	O
of	O
claim	O
the	O
number	O
of	O
nodes	O
in	O
the	O
hidden	O
layer	O
is	O
exponentially	O
large	O
this	O
is	O
not	O
an	O
artifact	O
of	O
our	O
proof	O
as	O
stated	O
in	O
the	O
following	O
theorem	O
theorem	O
for	O
every	O
n	O
let	O
sn	O
be	O
the	O
minimal	O
integer	O
such	O
that	O
there	O
exists	O
a	O
graph	O
e	O
with	O
sn	O
such	O
that	O
the	O
hypothesis	B
class	I
hvesign	O
contains	O
all	O
the	O
functions	O
from	O
to	O
then	O
sn	O
is	O
exponential	O
in	O
n	O
similar	O
results	O
hold	O
for	O
hve	O
where	O
is	O
the	O
sigmoid	O
function	B
proof	O
suppose	O
that	O
for	O
some	O
e	O
we	O
have	O
that	O
hvesign	O
contains	O
all	O
functions	O
from	O
to	O
it	O
follows	O
that	O
it	O
can	O
shatter	O
the	O
set	B
of	O
m	O
vectors	O
in	O
and	O
hence	O
the	O
vc	B
dimension	I
of	O
hvesign	O
is	O
on	O
the	O
other	O
hand	O
the	O
vc	B
dimension	I
of	O
hvesign	O
is	O
bounded	O
by	O
oe	O
loge	O
ov	O
as	O
we	O
will	O
show	O
in	O
the	O
next	O
section	O
this	O
implies	O
that	O
which	O
concludes	O
our	O
proof	O
for	O
the	O
case	O
of	O
networks	O
with	O
the	O
sign	O
activation	B
function	B
the	O
proof	O
for	O
the	O
sigmoid	O
case	O
is	O
analogous	O
it	O
is	O
possible	O
to	O
derive	O
a	O
similar	O
theorem	O
for	O
hve	O
for	O
any	O
as	O
remark	O
long	O
as	O
we	O
restrict	O
the	O
weights	O
so	O
that	O
it	O
is	O
possible	O
to	O
express	O
every	O
weight	O
using	O
a	O
number	O
of	O
bits	O
which	O
is	O
bounded	O
by	O
a	O
universal	O
constant	O
we	O
can	O
even	O
consider	O
hypothesis	B
classes	O
where	O
different	O
neurons	O
can	O
employ	O
different	O
activation	O
functions	O
as	O
long	O
as	O
the	O
number	O
of	O
allowed	O
activation	O
functions	O
is	O
also	O
finite	O
which	O
functions	O
can	O
we	O
express	O
using	O
a	O
network	O
of	O
polynomial	O
size	O
the	O
preceding	O
claim	O
tells	O
us	O
that	O
it	O
is	O
impossible	O
to	O
express	O
all	O
boolean	O
functions	O
using	O
a	O
network	O
of	O
polynomial	O
size	O
on	O
the	O
positive	O
side	O
in	O
the	O
following	O
we	O
show	O
that	O
all	O
boolean	O
functions	O
that	O
can	O
be	O
calculated	O
in	O
time	O
ot	O
can	O
also	O
be	O
expressed	O
by	O
a	O
network	O
of	O
size	O
ot	O
theorem	O
let	O
t	O
n	O
n	O
and	O
for	O
every	O
n	O
let	O
fn	O
be	O
the	O
set	B
of	O
functions	O
that	O
can	O
be	O
implemented	O
using	O
a	O
turing	O
machine	O
using	O
runtime	O
of	O
at	O
most	O
t	O
then	O
there	O
exist	O
constants	O
b	O
c	O
r	O
such	O
that	O
for	O
every	O
n	O
there	O
is	O
a	O
graph	O
en	O
of	O
size	O
at	O
most	O
c	O
t	O
b	O
such	O
that	O
hvnensign	O
contains	O
fn	O
the	O
proof	O
of	O
this	O
theorem	O
relies	O
on	O
the	O
relation	O
between	O
the	O
time	O
complexity	O
of	O
programs	O
and	O
their	O
circuit	O
complexity	O
for	O
example	O
sipser	O
in	O
a	O
nutshell	O
a	O
boolean	O
circuit	O
is	O
a	O
type	O
of	O
network	O
in	O
which	O
the	O
individual	O
neurons	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
implement	O
conjunctions	O
disjunctions	O
and	O
negation	O
of	O
their	O
inputs	O
circuit	O
complexity	O
measures	O
the	O
size	O
of	O
boolean	O
circuits	O
required	O
to	O
calculate	O
functions	O
the	O
relation	O
between	O
time	O
complexity	O
and	O
circuit	O
complexity	O
can	O
be	O
seen	O
intuitively	O
as	O
follows	O
we	O
can	O
model	O
each	O
step	O
of	O
the	O
execution	O
of	O
a	O
computer	O
program	O
as	O
a	O
simple	O
operation	O
on	O
its	O
memory	O
state	O
therefore	O
the	O
neurons	O
at	O
each	O
layer	O
of	O
the	O
network	O
will	O
reflect	O
the	O
memory	O
state	O
of	O
the	O
computer	O
at	O
the	O
corresponding	O
time	O
and	O
the	O
translation	O
to	O
the	O
next	O
layer	O
of	O
the	O
network	O
involves	O
a	O
simple	O
calculation	O
that	O
can	O
be	O
carried	O
out	O
by	O
the	O
network	O
to	O
relate	O
boolean	O
circuits	O
to	O
networks	O
with	O
the	O
sign	O
activation	B
function	B
we	O
need	O
to	O
show	O
that	O
we	O
can	O
implement	O
the	O
operations	O
of	O
conjunction	O
disjunction	O
and	O
negation	O
using	O
the	O
sign	O
activation	B
function	B
clearly	O
we	O
can	O
implement	O
the	O
negation	O
operator	O
using	O
the	O
sign	O
activation	B
function	B
the	O
following	O
lemma	O
shows	O
that	O
the	O
sign	O
activation	B
function	B
can	O
also	O
implement	O
conjunctions	O
and	O
disjunctions	O
of	O
its	O
inputs	O
lemma	O
suppose	O
that	O
a	O
neuron	O
v	O
that	O
implements	O
the	O
sign	O
activation	B
function	B
has	O
k	O
incoming	O
edges	O
connecting	O
it	O
to	O
neurons	O
whose	O
outputs	O
are	O
in	O
then	O
by	O
adding	O
one	O
more	O
edge	O
linking	O
a	O
constant	O
neuron	O
to	O
v	O
and	O
by	O
adjusting	O
the	O
weights	O
on	O
the	O
edges	O
to	O
v	O
the	O
output	O
of	O
v	O
can	O
implement	O
the	O
conjunction	O
or	O
the	O
disjunction	O
of	O
its	O
inputs	O
proof	O
simply	O
observe	O
that	O
if	O
f	O
is	O
the	O
conjunction	O
function	B
f	O
ixi	O
then	O
it	O
can	O
be	O
written	O
as	O
f	O
sign	O
similarly	O
the	O
disjunction	O
function	B
f	O
ixi	O
can	O
be	O
written	O
as	O
f	O
sign	O
k	O
k	O
xi	O
xi	O
so	O
far	O
we	O
have	O
discussed	O
boolean	O
functions	O
in	O
exercise	O
we	O
show	O
that	O
neural	B
networks	I
are	O
universal	O
approximators	O
that	O
is	O
for	O
every	O
fixed	O
precision	B
parameter	O
and	O
every	O
lipschitz	O
function	B
f	O
it	O
is	O
possible	O
to	O
construct	O
a	O
network	O
such	O
that	O
for	O
every	O
input	O
x	O
the	O
network	O
outputs	O
a	O
number	O
between	O
f	O
and	O
f	O
however	O
as	O
in	O
the	O
case	O
of	O
boolean	O
functions	O
the	O
size	O
of	O
the	O
network	O
here	O
again	O
cannot	O
be	O
polynomial	O
in	O
n	O
this	O
is	O
formalized	O
in	O
the	O
following	O
theorem	O
whose	O
proof	O
is	O
a	O
direct	O
corollary	O
of	O
theorem	O
and	O
is	O
left	O
as	O
an	O
exercise	O
theorem	O
fix	O
some	O
for	O
every	O
n	O
let	O
sn	O
be	O
the	O
minimal	O
integer	O
such	O
that	O
there	O
exists	O
a	O
graph	O
e	O
with	O
sn	O
such	O
that	O
the	O
hypothesis	B
class	I
hve	O
with	O
being	O
the	O
sigmoid	O
function	B
can	O
approximate	O
to	O
within	O
precision	B
of	O
every	O
function	B
f	O
then	O
sn	O
is	O
exponential	O
in	O
n	O
geometric	O
intuition	O
we	O
next	O
provide	O
several	O
geometric	O
illustrations	O
of	O
functions	O
f	O
and	O
show	O
how	O
to	O
express	O
them	O
using	O
a	O
neural	O
network	O
with	O
the	O
sign	O
activation	B
function	B
neural	B
networks	I
let	O
us	O
start	O
with	O
a	O
depth	O
network	O
namely	O
a	O
network	O
with	O
a	O
single	O
hidden	O
layer	O
each	O
neuron	O
in	O
the	O
hidden	O
layer	O
implements	O
a	O
halfspace	B
predictor	B
then	O
the	O
single	O
neuron	O
at	O
the	O
output	O
layer	O
applies	O
a	O
halfspace	B
on	O
top	O
of	O
the	O
binary	O
outputs	O
of	O
the	O
neurons	O
in	O
the	O
hidden	O
layer	O
as	O
we	O
have	O
shown	O
before	O
a	O
halfspace	B
can	O
implement	O
the	O
conjunction	O
function	B
therefore	O
such	O
networks	O
contain	O
all	O
hypotheses	O
which	O
are	O
an	O
intersection	O
of	O
k	O
halfspaces	O
where	O
k	O
is	O
the	O
number	O
of	O
neurons	O
in	O
the	O
hidden	O
layer	O
namely	O
they	O
can	O
express	O
all	O
convex	B
polytopes	O
with	O
k	O
faces	O
an	O
example	O
of	O
an	O
intersection	O
of	O
halfspaces	O
is	O
given	O
in	O
the	O
following	O
we	O
have	O
shown	O
that	O
a	O
neuron	O
in	O
layer	O
can	O
implement	O
a	O
function	B
that	O
indicates	O
whether	O
x	O
is	O
in	O
some	O
convex	B
polytope	O
by	O
adding	O
one	O
more	O
layer	O
and	O
letting	O
the	O
neuron	O
in	O
the	O
output	O
layer	O
implement	O
the	O
disjunction	O
of	O
its	O
inputs	O
we	O
get	O
a	O
network	O
that	O
computes	O
the	O
union	O
of	O
polytopes	O
an	O
illustration	O
of	O
such	O
a	O
function	B
is	O
given	O
in	O
the	O
following	O
the	O
sample	B
complexity	I
of	O
neural	B
networks	I
next	O
we	O
discuss	O
the	O
sample	B
complexity	I
of	O
learning	O
the	O
class	O
hve	O
recall	B
that	O
the	O
fundamental	O
theorem	O
of	O
learning	O
tells	O
us	O
that	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
hypothesis	B
class	I
of	O
binary	O
classifiers	O
depends	O
on	O
its	O
vc	B
dimension	I
therefore	O
we	O
focus	O
on	O
calculating	O
the	O
vc	B
dimension	I
of	O
hypothesis	B
classes	O
of	O
the	O
form	O
hve	O
where	O
the	O
output	O
layer	O
of	O
the	O
graph	O
contains	O
a	O
single	O
neuron	O
we	O
start	O
with	O
the	O
sign	O
activation	B
function	B
namely	O
with	O
hvesign	O
what	O
is	O
the	O
vc	B
dimension	I
of	O
this	O
class	O
intuitively	O
since	O
we	O
learn	O
parameters	O
the	O
vc	B
dimension	I
should	O
be	O
order	O
of	O
this	O
is	O
indeed	O
the	O
case	O
as	O
formalized	O
by	O
the	O
following	O
theorem	O
theorem	O
the	O
vc	B
dimension	I
of	O
hvesign	O
is	O
oe	O
loge	O
the	O
sample	B
complexity	I
of	O
neural	B
networks	I
proof	O
to	O
simplify	O
the	O
notation	O
throughout	O
the	O
proof	O
let	O
us	O
denote	O
the	O
hypothesis	B
class	I
by	O
h	O
recall	B
the	O
definition	O
of	O
the	O
growth	B
function	B
hm	O
from	O
section	O
this	O
function	B
measures	O
maxc	O
x	O
where	O
hc	O
is	O
the	O
restriction	O
of	O
h	O
to	O
functions	O
from	O
c	O
to	O
we	O
can	O
naturally	O
extend	O
the	O
definition	O
for	O
a	O
set	B
of	O
functions	O
from	O
x	O
to	O
some	O
finite	O
set	B
y	O
by	O
letting	O
hc	O
be	O
the	O
restriction	O
of	O
h	O
to	O
functions	O
from	O
c	O
to	O
y	O
and	O
keeping	O
the	O
definition	O
of	O
hm	O
intact	O
our	O
neural	O
network	O
is	O
defined	O
by	O
a	O
layered	O
graph	O
let	O
vt	O
be	O
the	O
layers	O
of	O
the	O
graph	O
fix	O
some	O
t	O
by	O
assigning	O
different	O
weights	O
on	O
the	O
edges	O
between	O
vt	O
and	O
vt	O
we	O
obtain	O
different	O
functions	O
from	O
rvt	O
let	O
ht	O
be	O
the	O
class	O
of	O
all	O
possible	O
such	O
mappings	O
from	O
rvt	O
then	O
h	O
can	O
be	O
written	O
as	O
a	O
composition	O
h	O
ht	O
in	O
exercise	O
we	O
show	O
that	O
the	O
growth	B
function	B
of	O
a	O
composition	O
of	O
hypothesis	B
classes	O
is	O
bounded	O
by	O
the	O
products	O
of	O
the	O
growth	O
functions	O
of	O
the	O
individual	O
classes	O
therefore	O
hm	O
ht	O
in	O
addition	O
each	O
ht	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
function	B
classes	O
ht	O
htvt	O
where	O
each	O
htj	O
is	O
all	O
functions	O
from	O
layer	O
t	O
to	O
that	O
the	O
jth	O
neuron	O
of	O
layer	O
t	O
can	O
implement	O
in	O
exercise	O
we	O
bound	O
product	O
classes	O
and	O
this	O
yields	O
ht	O
htim	O
let	O
dti	O
be	O
the	O
number	O
of	O
edges	O
that	O
are	O
headed	O
to	O
the	O
ith	O
neuron	O
of	O
layer	O
t	O
since	O
the	O
neuron	O
is	O
a	O
homogenous	B
halfspace	B
hypothesis	B
and	O
the	O
vc	B
dimension	I
of	O
homogenous	B
halfspaces	O
is	O
the	O
dimension	O
of	O
their	O
input	O
we	O
have	O
by	O
sauer	O
s	O
lemma	O
that	O
htim	O
em	B
hm	O
dti	O
ti	O
dti	O
overall	O
we	O
obtained	O
that	O
now	O
assume	O
that	O
there	O
are	O
m	O
shattered	O
points	O
then	O
we	O
must	O
have	O
hm	O
from	O
which	O
we	O
obtain	O
m	O
logem	O
the	O
claim	O
follows	O
by	O
lemma	O
next	O
we	O
consider	O
hve	O
where	O
is	O
the	O
sigmoid	O
function	B
surprisingly	O
it	O
turns	O
out	O
that	O
the	O
vc	B
dimension	I
of	O
hve	O
is	O
lower	O
bounded	O
by	O
exercise	O
that	O
is	O
the	O
vc	B
dimension	I
is	O
the	O
number	O
of	O
tunable	O
parameters	O
squared	O
it	O
is	O
also	O
possible	O
to	O
upper	O
bound	O
the	O
vc	B
dimension	I
by	O
ov	O
but	O
the	O
proof	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
in	O
any	O
case	O
since	O
in	O
practice	O
neural	B
networks	I
we	O
only	O
consider	O
networks	O
in	O
which	O
the	O
weights	O
have	O
a	O
short	O
representation	O
as	O
floating	O
point	O
numbers	O
with	O
bits	O
by	O
using	O
the	O
discretization	B
trick	I
we	O
easily	O
obtain	O
that	O
such	O
networks	O
have	O
a	O
vc	B
dimension	I
of	O
oe	O
even	O
if	O
we	O
use	O
the	O
sigmoid	O
activation	B
function	B
the	O
runtime	O
of	O
learning	O
neural	B
networks	I
in	O
the	O
previous	O
sections	O
we	O
have	O
shown	O
that	O
the	O
class	O
of	O
neural	B
networks	I
with	O
an	O
underlying	O
graph	O
of	O
polynomial	O
size	O
can	O
express	O
all	O
functions	O
that	O
can	O
be	O
implemented	O
efficiently	O
and	O
that	O
the	O
sample	B
complexity	I
has	O
a	O
favorable	O
dependence	O
on	O
the	O
size	O
of	O
the	O
network	O
in	O
this	O
section	O
we	O
turn	O
to	O
the	O
analysis	O
of	O
the	O
time	O
complexity	O
of	O
training	O
neural	B
networks	I
we	O
first	O
show	O
that	O
it	O
is	O
np	O
hard	O
to	O
implement	O
the	O
erm	B
rule	O
with	O
respect	O
to	O
hvesign	O
even	O
for	O
networks	O
with	O
a	O
single	O
hidden	O
layer	O
that	O
contain	O
just	O
neurons	O
in	O
the	O
hidden	O
layer	O
theorem	O
let	O
k	O
for	O
every	O
n	O
let	O
e	O
be	O
a	O
layered	O
graph	O
with	O
n	O
input	O
nodes	O
k	O
nodes	O
at	O
the	O
hidden	O
layer	O
where	O
one	O
of	O
them	O
is	O
the	O
constant	O
neuron	O
and	O
a	O
single	O
output	O
node	O
then	O
it	O
is	O
np	O
hard	O
to	O
implement	O
the	O
erm	B
rule	O
with	O
respect	O
to	O
hvesign	O
the	O
proof	O
relies	O
on	O
a	O
reduction	O
from	O
the	O
k-coloring	O
problem	O
and	O
is	O
left	O
as	O
exercise	O
one	O
way	O
around	O
the	O
preceding	O
hardness	O
result	O
could	O
be	O
that	O
for	O
the	O
purpose	O
of	O
learning	O
it	O
may	O
suffice	O
to	O
find	O
a	O
predictor	B
h	O
h	O
with	O
low	O
empirical	B
error	I
not	O
necessarily	O
an	O
exact	O
erm	B
however	O
it	O
turns	O
out	O
that	O
even	O
the	O
task	O
of	O
finding	O
weights	O
that	O
result	O
in	O
close-to-minimal	O
empirical	B
error	I
is	O
computationally	O
infeasible	O
ben-david	O
one	O
may	O
also	O
wonder	O
whether	O
it	O
may	O
be	O
possible	O
to	O
change	O
the	O
architecture	O
of	O
the	O
network	O
so	O
as	O
to	O
circumvent	O
the	O
hardness	O
result	O
that	O
is	O
maybe	O
erm	B
with	O
respect	O
to	O
the	O
original	O
network	O
structure	O
is	O
computationally	O
hard	O
but	O
erm	B
with	O
respect	O
to	O
some	O
other	O
larger	O
network	O
may	O
be	O
implemented	O
efficiently	O
chapter	O
for	O
examples	O
of	O
such	O
cases	O
another	O
possibility	O
is	O
to	O
use	O
other	O
activation	O
functions	O
as	O
sigmoids	O
or	O
any	O
other	O
type	O
of	O
efficiently	O
computable	O
activation	O
functions	O
there	O
is	O
a	O
strong	O
indication	O
that	O
all	O
of	O
such	O
approaches	O
are	O
doomed	O
to	O
fail	O
indeed	O
under	O
some	O
cryptographic	O
assumption	O
the	O
problem	O
of	O
learning	O
intersections	O
of	O
halfspaces	O
is	O
known	O
to	O
be	O
hard	O
even	O
in	O
the	O
representation	B
independent	I
model	O
of	O
learning	O
klivans	O
sherstov	O
this	O
implies	O
that	O
under	O
the	O
same	O
cryptographic	O
assumption	O
any	O
hypothesis	B
class	I
which	O
contains	O
intersections	O
of	O
halfspaces	O
cannot	O
be	O
learned	O
efficiently	O
a	O
widely	O
used	O
heuristic	O
for	O
training	O
neural	B
networks	I
relies	O
on	O
the	O
sgd	B
framework	O
we	O
studied	O
in	O
chapter	O
there	O
we	O
have	O
shown	O
that	O
sgd	B
is	O
a	O
successful	O
learner	O
if	O
the	O
loss	B
function	B
is	O
convex	B
in	O
neural	B
networks	I
the	O
loss	B
function	B
is	O
highly	O
nonconvex	O
nevertheless	O
we	O
can	O
still	O
implement	O
the	O
sgd	B
algorithm	O
and	O
sgd	B
and	O
backpropagation	B
hope	O
it	O
will	O
find	O
a	O
reasonable	O
solution	O
happens	O
to	O
be	O
the	O
case	O
in	O
several	O
practical	O
tasks	O
sgd	B
and	O
backpropagation	B
the	O
problem	O
of	O
finding	O
a	O
hypothesis	B
in	O
hve	O
with	O
a	O
low	O
risk	B
amounts	O
to	O
the	O
problem	O
of	O
tuning	O
the	O
weights	O
over	O
the	O
edges	O
in	O
this	O
section	O
we	O
show	O
how	O
to	O
apply	O
a	O
heuristic	O
search	O
for	O
good	O
weights	O
using	O
the	O
sgd	B
algorithm	O
throughout	O
this	O
section	O
we	O
assume	O
that	O
is	O
the	O
sigmoid	O
function	B
e	O
a	O
but	O
the	O
derivation	O
holds	O
for	O
any	O
differentiable	O
scalar	O
function	B
since	O
e	O
is	O
a	O
finite	O
set	B
we	O
can	O
think	O
of	O
the	O
weight	O
function	B
as	O
a	O
vector	O
w	O
re	O
suppose	O
the	O
network	O
has	O
n	O
input	O
neurons	O
and	O
k	O
output	O
neurons	O
and	O
denote	O
by	O
hw	O
rn	O
rk	O
the	O
function	B
calculated	O
by	O
the	O
network	O
if	O
the	O
weight	O
function	B
is	O
defined	O
by	O
w	O
let	O
us	O
denote	O
by	O
y	O
the	O
loss	B
of	O
predicting	O
hwx	O
when	O
the	O
target	O
is	O
y	O
y	O
for	O
concreteness	O
we	O
will	O
take	O
to	O
be	O
the	O
squared	O
loss	B
however	O
similar	O
derivation	O
can	O
be	O
obtained	O
for	O
y	O
every	O
differentiable	O
function	B
finally	O
given	O
a	O
distribution	O
d	O
over	O
the	O
examples	O
domain	B
rn	O
rk	O
let	O
ldw	O
be	O
the	O
risk	B
of	O
the	O
network	O
namely	O
ldw	O
e	O
d	O
y	O
recall	B
the	O
sgd	B
algorithm	O
for	O
minimizing	O
the	O
risk	B
function	B
ldw	O
we	O
repeat	O
the	O
pseudocode	O
from	O
chapter	O
with	O
a	O
few	O
modifications	O
which	O
are	O
relevant	O
to	O
the	O
neural	O
network	O
application	O
because	O
of	O
the	O
nonconvexity	O
of	O
the	O
objective	O
function	B
first	O
while	O
in	O
chapter	O
we	O
initialized	O
w	O
to	O
be	O
the	O
zero	O
vector	O
here	O
we	O
initialize	O
w	O
to	O
be	O
a	O
randomly	O
chosen	O
vector	O
with	O
values	O
close	O
to	O
zero	O
this	O
is	O
because	O
an	O
initialization	O
with	O
the	O
zero	O
vector	O
will	O
lead	O
all	O
hidden	O
neurons	O
to	O
have	O
the	O
same	O
weights	O
the	O
network	O
is	O
a	O
full	O
layered	O
network	O
in	O
addition	O
the	O
hope	O
is	O
that	O
if	O
we	O
repeat	O
the	O
sgd	B
procedure	O
several	O
times	O
where	O
each	O
time	O
we	O
initialize	O
the	O
process	O
with	O
a	O
new	O
random	O
vector	O
one	O
of	O
the	O
runs	O
will	O
lead	O
to	O
a	O
good	O
local	B
minimum	I
second	O
while	O
a	O
fixed	O
step	O
size	O
is	O
guaranteed	O
to	O
be	O
good	O
enough	O
for	O
convex	B
problems	O
here	O
we	O
utilize	O
a	O
variable	O
step	O
size	O
t	O
as	O
defined	O
in	O
section	O
because	O
of	O
the	O
nonconvexity	O
of	O
the	O
loss	B
function	B
the	O
choice	O
of	O
the	O
sequence	O
t	O
is	O
more	O
significant	O
and	O
it	O
is	O
tuned	O
in	O
practice	O
by	O
a	O
trial	O
and	O
error	O
manner	O
third	O
we	O
output	O
the	O
best	O
performing	O
vector	O
on	O
a	O
validation	B
set	B
in	O
addition	O
it	O
is	O
sometimes	O
helpful	O
to	O
add	O
regularization	B
on	O
the	O
weights	O
with	O
parameter	O
that	O
is	O
we	O
try	O
to	O
minimize	O
ldw	O
gradient	B
does	O
not	O
have	O
a	O
closed	O
form	O
solution	O
instead	O
it	O
is	O
implemented	O
using	O
the	O
backpropagation	B
algorithm	O
which	O
will	O
be	O
described	O
in	O
the	O
sequel	O
finally	O
the	O
neural	B
networks	I
sgd	B
for	O
neural	B
networks	I
parameters	O
number	O
of	O
iterations	O
step	O
size	O
sequence	O
regularization	B
parameter	O
input	O
layered	O
graph	O
e	O
differentiable	O
activation	B
function	B
r	O
r	O
initialize	O
choose	O
re	O
at	O
random	O
a	O
distribution	O
s	O
t	O
is	O
close	O
enough	O
to	O
for	O
i	O
sample	O
y	O
d	O
calculate	O
gradient	B
vi	O
backpropagationx	O
y	O
w	O
e	O
update	O
wi	O
ivi	O
wi	O
output	O
w	O
is	O
the	O
best	O
performing	O
wi	O
on	O
a	O
validation	B
set	B
backpropagation	B
input	O
example	O
y	O
weight	O
vector	O
w	O
layered	O
graph	O
e	O
activation	B
function	B
r	O
r	O
initialize	O
denote	O
layers	O
of	O
the	O
graph	O
vt	O
where	O
vt	O
vtkt	O
define	O
wtij	O
as	O
the	O
weight	O
of	O
we	O
set	B
wtij	O
if	O
e	O
forward	O
set	B
x	O
for	O
t	O
t	O
for	O
i	O
kt	O
set	B
ati	O
wt	O
ot	O
set	B
oti	O
backward	O
set	B
t	O
ot	O
y	O
for	O
t	O
t	O
t	O
for	O
i	O
kt	O
ti	O
wtji	O
output	O
foreach	O
edge	O
vti	O
e	O
set	B
the	O
partial	O
derivative	O
to	O
ti	O
ot	O
sgd	B
and	O
backpropagation	B
explaining	O
how	O
backpropagation	B
calculates	O
the	O
gradient	B
we	O
next	O
explain	O
how	O
the	O
backpropagation	B
algorithm	O
calculates	O
the	O
gradient	B
of	O
the	O
loss	B
function	B
on	O
an	O
example	O
y	O
with	O
respect	O
to	O
the	O
vector	O
w	O
let	O
us	O
first	O
recall	B
a	O
few	O
definitions	O
from	O
vector	O
calculus	O
each	O
element	O
of	O
the	O
gradient	B
is	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
the	O
variable	O
in	O
w	O
corresponding	O
to	O
one	O
of	O
the	O
edges	O
of	O
the	O
network	O
recall	B
the	O
definition	O
of	O
a	O
partial	O
derivative	O
given	O
a	O
function	B
f	O
rn	O
r	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
the	O
ith	O
variable	O
at	O
w	O
is	O
obtained	O
by	O
fixing	O
the	O
values	O
of	O
wi	O
wn	O
which	O
yields	O
the	O
scalar	O
function	B
g	O
r	O
r	O
defined	O
by	O
ga	O
f	O
wi	O
wi	O
a	O
wn	O
and	O
then	O
taking	O
the	O
derivative	O
of	O
g	O
at	O
for	O
a	O
function	B
with	O
multiple	O
outputs	O
f	O
rn	O
rm	O
the	O
jacobian	O
of	O
f	O
at	O
w	O
rn	O
denoted	O
jwf	O
is	O
the	O
m	O
n	O
matrix	O
whose	O
i	O
j	O
element	O
is	O
the	O
partial	O
derivative	O
of	O
fi	O
rn	O
r	O
w	O
r	O
t	O
its	O
jth	O
variable	O
at	O
w	O
note	O
that	O
if	O
m	O
then	O
the	O
jacobian	O
matrix	O
is	O
the	O
gradient	B
of	O
the	O
function	B
as	O
a	O
row	O
vector	O
two	O
examples	O
of	O
jacobian	O
calculations	O
which	O
we	O
will	O
later	O
use	O
are	O
as	O
follows	O
let	O
f	O
aw	O
for	O
a	O
rmn	O
then	O
jwf	O
a	O
for	O
every	O
n	O
we	O
use	O
the	O
notation	O
to	O
denote	O
the	O
function	B
from	O
rn	O
to	O
rn	O
which	O
applies	O
the	O
sigmoid	O
function	B
element-wise	O
that	O
is	O
means	O
i	O
it	O
is	O
easy	O
to	O
verify	O
that	O
for	O
every	O
i	O
we	O
have	O
i	O
i	O
that	O
j	O
is	O
a	O
diagonal	O
matrix	O
whose	O
i	O
entry	O
is	O
i	O
where	O
is	O
the	O
derivative	O
function	B
of	O
the	O
sigmoid	O
function	B
namely	O
i	O
i	O
we	O
also	O
use	O
the	O
notation	O
diag	O
to	O
denote	O
this	O
matrix	O
the	O
chain	O
rule	O
for	O
taking	O
the	O
derivative	O
of	O
a	O
composition	O
of	O
functions	O
can	O
be	O
written	O
in	O
terms	O
of	O
the	O
jacobian	O
as	O
follows	O
given	O
two	O
functions	O
f	O
rn	O
rm	O
and	O
g	O
rk	O
rn	O
we	O
have	O
that	O
the	O
jacobian	O
of	O
the	O
composition	O
function	B
g	O
rk	O
rm	O
at	O
w	O
is	O
jwf	O
g	O
jgwf	O
for	O
example	O
for	O
gw	O
aw	O
where	O
a	O
rnk	O
we	O
have	O
that	O
jw	O
g	O
diag	O
a	O
to	O
describe	O
the	O
backpropagation	B
algorithm	O
let	O
us	O
first	O
decompose	O
v	O
into	O
the	O
layers	O
of	O
the	O
graph	O
v	O
t	O
for	O
every	O
t	O
let	O
us	O
write	O
vt	O
vtkt	O
where	O
kt	O
in	O
addition	O
for	O
every	O
t	O
denote	O
wt	O
a	O
matrix	O
which	O
gives	O
a	O
weight	O
to	O
every	O
potential	O
edge	O
between	O
vt	O
and	O
if	O
the	O
edge	O
exists	O
in	O
e	O
then	O
we	O
set	B
wtij	O
to	O
be	O
the	O
weight	O
according	O
to	O
w	O
of	O
the	O
edge	O
otherwise	O
we	O
add	O
a	O
phantom	O
edge	O
and	O
set	B
its	O
weight	O
to	O
be	O
zero	O
wtij	O
since	O
when	O
calculating	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
the	O
weight	O
of	O
some	O
edge	O
we	O
fix	O
all	O
other	O
weights	O
these	O
additional	O
phantom	O
edges	O
have	O
no	O
effect	O
on	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
existing	O
edges	O
it	O
follows	O
that	O
we	O
can	O
assume	O
without	O
loss	B
of	O
generality	O
that	O
all	O
edges	O
exist	O
that	O
is	O
e	O
tvt	O
neural	B
networks	I
next	O
we	O
discuss	O
how	O
to	O
calculate	O
the	O
partial	O
derivatives	O
with	O
respect	O
to	O
the	O
edges	O
from	O
vt	O
to	O
vt	O
namely	O
with	O
respect	O
to	O
the	O
elements	O
in	O
wt	O
since	O
we	O
fix	O
all	O
other	O
weights	O
of	O
the	O
network	O
it	O
follows	O
that	O
the	O
outputs	O
of	O
all	O
the	O
neurons	O
in	O
vt	O
are	O
fixed	O
numbers	O
which	O
do	O
not	O
depend	O
on	O
the	O
weights	O
in	O
wt	O
denote	O
the	O
corresponding	O
vector	O
by	O
ot	O
in	O
addition	O
let	O
us	O
denote	O
by	O
rkt	O
r	O
the	O
loss	B
function	B
of	O
the	O
subnetwork	O
defined	O
by	O
layers	O
vt	O
vt	O
as	O
a	O
function	B
of	O
the	O
outputs	O
of	O
the	O
neurons	O
in	O
vt	O
the	O
input	O
to	O
the	O
neurons	O
of	O
vt	O
can	O
be	O
written	O
as	O
at	O
wt	O
and	O
the	O
output	O
of	O
the	O
neurons	O
of	O
vt	O
is	O
ot	O
that	O
is	O
for	O
every	O
j	O
we	O
have	O
otj	O
we	O
obtain	O
that	O
the	O
loss	B
as	O
a	O
function	B
of	O
wt	O
can	O
be	O
written	O
as	O
gtwt	O
it	O
would	O
be	O
convenient	O
to	O
rewrite	O
this	O
as	O
follows	O
let	O
wt	O
rkt	O
be	O
the	O
column	O
vector	O
obtained	O
by	O
concatenating	O
the	O
rows	O
of	O
wt	O
and	O
then	O
taking	O
the	O
transpose	O
of	O
the	O
resulting	O
long	O
vector	O
define	O
by	O
ot	O
the	O
kt	O
matrix	O
t	O
t	O
t	O
ot	O
then	O
wt	O
ot	O
so	O
we	O
can	O
also	O
write	O
gtwt	O
wt	O
therefore	O
applying	O
the	O
chain	O
rule	O
we	O
obtain	O
that	O
jwt	O
j	O
diag	O
ot	O
using	O
our	O
notation	O
we	O
have	O
ot	O
and	O
at	O
ot	O
which	O
yields	O
jwt	O
diag	O
ot	O
let	O
us	O
also	O
denote	O
t	O
then	O
we	O
can	O
further	O
rewrite	O
the	O
preceding	O
as	O
t	O
tkt	O
t	O
jwt	O
it	O
is	O
left	O
to	O
calculate	O
the	O
vector	O
t	O
for	O
every	O
t	O
this	O
is	O
the	O
gradient	B
of	O
at	O
ot	O
we	O
calculate	O
this	O
in	O
a	O
recursive	O
manner	O
first	O
observe	O
that	O
for	O
the	O
last	O
layer	O
we	O
have	O
that	O
y	O
where	O
is	O
the	O
loss	B
function	B
since	O
we	O
we	O
obtain	O
that	O
y	O
in	O
particular	O
assume	O
that	O
y	O
t	O
jot	O
y	O
next	O
note	O
that	O
therefore	O
by	O
the	O
chain	O
rule	O
j	O
summary	O
in	O
particular	O
t	O
j	O
diag	O
in	O
summary	O
we	O
can	O
first	O
calculate	O
the	O
vectors	O
ot	O
from	O
the	O
bottom	O
of	O
the	O
network	O
to	O
its	O
top	O
then	O
we	O
calculate	O
the	O
vectors	O
t	O
from	O
the	O
top	O
of	O
the	O
network	O
back	O
to	O
its	O
bottom	O
once	O
we	O
have	O
all	O
of	O
these	O
vectors	O
the	O
partial	O
derivatives	O
are	O
easily	O
obtained	O
using	O
equation	O
we	O
have	O
thus	O
shown	O
that	O
the	O
pseudocode	O
of	O
backpropagation	B
indeed	O
calculates	O
the	O
gradient	B
summary	O
classes	O
of	O
all	O
predictors	O
that	O
can	O
be	O
implemented	O
in	O
runtime	O
of	O
we	O
neural	B
networks	I
over	O
graphs	O
of	O
size	O
sn	O
can	O
be	O
used	O
to	O
describe	O
hypothesis	B
have	O
also	O
shown	O
that	O
their	O
sample	B
complexity	I
depends	O
polynomially	O
on	O
sn	O
it	O
depends	O
on	O
the	O
number	O
of	O
edges	O
in	O
the	O
network	O
therefore	O
classes	O
of	O
neural	O
network	O
hypotheses	O
seem	O
to	O
be	O
an	O
excellent	O
choice	O
regrettably	O
the	O
problem	O
of	O
training	O
the	O
network	O
on	O
the	O
basis	O
of	O
training	O
data	O
is	O
computationally	O
hard	O
we	O
have	O
presented	O
the	O
sgd	B
framework	O
as	O
a	O
heuristic	O
approach	O
for	O
training	O
neural	B
networks	I
and	O
described	O
the	O
backpropagation	B
algorithm	O
which	O
efficiently	O
calculates	O
the	O
gradient	B
of	O
the	O
loss	B
function	B
with	O
respect	O
to	O
the	O
weights	O
over	O
the	O
edges	O
bibliographic	O
remarks	O
neural	B
networks	I
were	O
extensively	O
studied	O
in	O
the	O
and	O
early	O
but	O
with	O
mixed	O
empirical	O
success	O
in	O
recent	O
years	O
a	O
combination	O
of	O
algorithmic	O
advancements	O
as	O
well	O
as	O
increasing	O
computational	O
power	O
and	O
data	O
size	O
has	O
led	O
to	O
a	O
breakthrough	O
in	O
the	O
effectiveness	O
of	O
neural	B
networks	I
in	O
particular	O
deep	O
networks	O
networks	O
of	O
more	O
than	O
layers	O
have	O
shown	O
very	O
impressive	O
practical	O
performance	O
on	O
a	O
variety	O
of	O
domains	O
a	O
few	O
examples	O
include	O
convolutional	O
networks	O
bengio	O
restricted	O
boltzmann	O
machines	O
osindero	O
teh	O
auto-encoders	B
huang	O
boureau	O
lecun	O
bengio	O
lecun	O
collobert	O
weston	O
lee	O
grosse	O
ranganath	O
ng	O
le	O
ranzato	O
monga	O
devin	O
corrado	O
chen	O
dean	O
ng	O
and	O
sum-product	O
networks	O
shalev-shwartz	O
shamir	O
poon	O
domingos	O
see	O
also	O
and	O
the	O
references	O
therein	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
and	O
the	O
relation	O
to	O
circuit	O
complexity	O
have	O
been	O
extensively	O
studied	O
in	O
for	O
the	O
analysis	O
of	O
the	O
sample	B
complexity	I
of	O
neural	B
networks	I
we	O
refer	O
the	O
reader	O
to	O
bartlet	O
our	O
proof	O
technique	O
of	O
theorem	O
is	O
due	O
to	O
kakade	O
and	O
tewari	O
lecture	O
notes	O
neural	B
networks	I
klivans	O
sherstov	O
have	O
shown	O
that	O
for	O
any	O
c	O
intersections	O
of	O
nc	O
halfspaces	O
over	O
are	O
not	O
efficiently	O
pac	B
learnable	O
even	O
if	O
we	O
allow	O
representation	B
independent	I
learning	O
this	O
hardness	O
result	O
relies	O
on	O
the	O
cryptographic	O
assumption	O
that	O
there	O
is	O
no	O
polynomial	O
time	O
solution	O
to	O
the	O
unique-shortestvector	O
problem	O
as	O
we	O
have	O
argued	O
this	O
implies	O
that	O
there	O
cannot	O
be	O
an	O
efficient	O
algorithm	O
for	O
training	O
neural	B
networks	I
even	O
if	O
we	O
allow	O
larger	O
networks	O
or	O
other	O
activation	O
functions	O
that	O
can	O
be	O
implemented	O
efficiently	O
the	O
backpropagation	B
algorithm	O
has	O
been	O
introduced	O
in	O
rumelhart	O
hinton	O
williams	O
exercises	O
prove	O
theorem	O
neural	B
networks	I
are	O
universal	O
approximators	O
let	O
f	O
be	O
a	O
function	B
fix	O
some	O
construct	O
a	O
neural	O
network	O
n	O
with	O
the	O
sigmoid	O
activation	B
function	B
such	O
that	O
for	O
every	O
x	O
it	O
holds	O
that	O
n	O
hint	O
similarly	O
to	O
the	O
proof	O
of	O
theorem	O
partition	O
into	O
small	O
boxes	O
use	O
the	O
lipschitzness	B
of	O
f	O
to	O
show	O
that	O
it	O
is	O
approximately	O
constant	O
at	O
each	O
box	O
finally	O
show	O
that	O
a	O
neural	O
network	O
can	O
first	O
decide	O
which	O
box	O
the	O
input	O
vector	O
belongs	O
to	O
and	O
then	O
predict	O
the	O
averaged	O
value	O
of	O
f	O
at	O
that	O
box	O
hint	O
for	O
every	O
f	O
construct	O
a	O
function	B
g	O
such	O
that	O
if	O
you	O
can	O
approximate	O
g	O
then	O
you	O
can	O
express	O
f	O
growth	B
function	B
of	O
product	O
for	O
i	O
let	O
fi	O
be	O
a	O
set	B
of	O
functions	O
from	O
x	O
to	O
yi	O
define	O
h	O
to	O
be	O
the	O
cartesian	O
product	O
class	O
that	O
is	O
for	O
every	O
and	O
there	O
exists	O
h	O
h	O
such	O
that	O
hx	O
prove	O
that	O
hm	O
growth	B
function	B
of	O
composition	O
let	O
be	O
a	O
set	B
of	O
functions	O
from	O
x	O
to	O
z	O
and	O
let	O
be	O
a	O
set	B
of	O
functions	O
from	O
z	O
to	O
y	O
let	O
h	O
be	O
the	O
composition	O
class	O
that	O
is	O
for	O
every	O
and	O
there	O
exists	O
h	O
h	O
such	O
that	O
hx	O
prove	O
that	O
hm	O
vc	O
of	O
sigmoidal	O
networks	O
in	O
this	O
exercise	O
we	O
show	O
that	O
there	O
is	O
a	O
graph	O
e	O
such	O
that	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
neural	B
networks	I
over	O
these	O
graphs	O
with	O
the	O
sigmoid	O
activation	B
function	B
is	O
note	O
that	O
for	O
every	O
function	B
the	O
sigmoid	O
activation	B
function	B
can	O
approximate	O
the	O
threshold	O
activation	O
i	O
xi	O
up	O
to	O
accuracy	B
to	O
simplify	O
the	O
presentation	O
throughout	O
the	O
exercise	O
we	O
assume	O
that	O
we	O
can	O
exactly	O
implement	O
the	O
activation	B
function	B
i	O
using	O
a	O
sigmoid	O
activation	B
function	B
fix	O
some	O
n	O
construct	O
a	O
network	O
with	O
on	O
weights	O
which	O
implements	O
a	O
function	B
from	O
r	O
to	O
and	O
satisfies	O
the	O
following	O
property	O
for	O
every	O
x	O
exercises	O
if	O
we	O
feed	O
the	O
network	O
with	O
the	O
real	O
number	O
xn	O
then	O
the	O
output	O
of	O
the	O
network	O
will	O
be	O
x	O
hint	O
denote	O
xn	O
and	O
observe	O
that	O
is	O
at	O
least	O
if	O
xk	O
and	O
is	O
at	O
most	O
if	O
xk	O
construct	O
a	O
network	O
with	O
on	O
weights	O
which	O
implements	O
a	O
function	B
from	O
to	O
such	O
that	O
ei	O
for	O
all	O
i	O
that	O
is	O
upon	O
receiving	O
the	O
input	O
i	O
the	O
network	O
outputs	O
the	O
vector	O
of	O
all	O
zeros	O
except	O
at	O
the	O
i	O
th	O
neuron	O
let	O
n	O
be	O
n	O
real	O
numbers	O
such	O
that	O
every	O
i	O
is	O
of	O
the	O
form	O
ai	O
j	O
construct	O
a	O
network	O
with	O
on	O
weights	O
which	O
imwith	O
ai	O
plements	O
a	O
function	B
from	O
to	O
r	O
and	O
satisfies	O
i	O
for	O
every	O
i	O
combine	O
to	O
obtain	O
a	O
network	O
that	O
receives	O
i	O
and	O
output	O
ai	O
construct	O
a	O
network	O
that	O
receives	O
j	O
and	O
outputs	O
ai	O
j	O
hint	O
observe	O
that	O
the	O
and	O
function	B
over	O
can	O
be	O
calculated	O
using	O
weights	O
ai	O
n	O
conclude	O
that	O
there	O
is	O
a	O
graph	O
with	O
on	O
weights	O
such	O
that	O
the	O
vc	O
di	O
mension	O
of	O
the	O
resulting	O
hypothesis	B
class	I
is	O
prove	O
theorem	O
hint	O
the	O
proof	O
is	O
similar	O
to	O
the	O
hardness	O
of	O
learning	O
intersections	O
of	O
halfspaces	O
see	O
exercise	O
in	O
chapter	O
part	O
iii	O
additional	O
learning	O
models	O
online	B
learning	I
in	O
this	O
chapter	O
we	O
describe	O
a	O
different	O
model	O
of	O
learning	O
which	O
is	O
called	O
online	B
learning	I
previously	O
we	O
studied	O
the	O
pac	B
learning	O
model	O
in	O
which	O
the	O
learner	O
first	O
receives	O
a	O
batch	O
of	O
training	O
examples	O
uses	O
the	O
training	B
set	B
to	O
learn	O
a	O
hypothesis	B
and	O
only	O
when	O
learning	O
is	O
completed	O
uses	O
the	O
learned	O
hypothesis	B
for	O
predicting	O
the	O
label	B
of	O
new	O
examples	O
in	O
our	O
papayas	O
learning	O
problem	O
this	O
means	O
that	O
we	O
should	O
first	O
buy	O
a	O
bunch	O
of	O
papayas	O
and	O
taste	O
them	O
all	O
then	O
we	O
use	O
all	O
of	O
this	O
information	O
to	O
learn	O
a	O
prediction	O
rule	O
that	O
determines	O
the	O
taste	O
of	O
new	O
papayas	O
in	O
contrast	O
in	O
online	B
learning	I
there	O
is	O
no	O
separation	O
between	O
a	O
training	O
phase	O
and	O
a	O
prediction	O
phase	O
instead	O
each	O
time	O
we	O
buy	O
a	O
papaya	O
it	O
is	O
first	O
considered	O
a	O
test	O
example	O
since	O
we	O
should	O
predict	O
whether	O
it	O
is	O
going	O
to	O
taste	O
good	O
then	O
after	O
taking	O
a	O
bite	O
from	O
the	O
papaya	O
we	O
know	O
the	O
true	O
label	B
and	O
the	O
same	O
papaya	O
can	O
be	O
used	O
as	O
a	O
training	O
example	O
that	O
can	O
help	O
us	O
improve	O
our	O
prediction	O
mechanism	O
for	O
future	O
papayas	O
concretely	O
online	B
learning	I
takes	O
place	O
in	O
a	O
sequence	O
of	O
consecutive	O
rounds	O
on	O
each	O
online	B
round	O
the	O
learner	O
first	O
receives	O
an	O
instance	B
learner	O
buys	O
a	O
papaya	O
and	O
knows	O
its	O
shape	O
and	O
color	O
which	O
form	O
the	O
instance	B
then	O
the	O
learner	O
is	O
required	O
to	O
predict	O
a	O
label	B
the	O
papaya	O
tasty	O
at	O
the	O
end	O
of	O
the	O
round	O
the	O
learner	O
obtains	O
the	O
correct	O
label	B
tastes	O
the	O
papaya	O
and	O
then	O
knows	O
whether	O
it	O
is	O
tasty	O
or	O
not	O
finally	O
the	O
learner	O
uses	O
this	O
information	O
to	O
improve	O
his	O
future	O
predictions	O
to	O
analyze	O
online	B
learning	I
we	O
follow	O
a	O
similar	O
route	O
to	O
our	O
study	O
of	O
pac	B
learning	O
we	O
start	O
with	O
online	B
binary	O
classification	O
problems	O
we	O
consider	O
both	O
the	O
realizable	O
case	O
in	O
which	O
we	O
assume	O
as	O
prior	B
knowledge	I
that	O
all	O
the	O
labels	O
are	O
generated	O
by	O
some	O
hypothesis	B
from	O
a	O
given	O
hypothesis	B
class	I
and	O
the	O
unrealizable	O
case	O
which	O
corresponds	O
to	O
the	O
agnostic	B
pac	B
learning	O
model	O
in	O
particular	O
we	O
present	O
an	O
important	O
algorithm	O
called	O
weighted-majority	B
next	O
we	O
study	O
online	B
learning	I
problems	O
in	O
which	O
the	O
loss	B
function	B
is	O
convex	B
finally	O
we	O
present	O
the	O
perceptron	B
algorithm	O
as	O
an	O
example	O
of	O
the	O
use	O
of	O
surrogate	O
convex	B
loss	B
functions	O
in	O
the	O
online	B
learning	I
model	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
online	B
learning	I
online	B
classification	O
in	O
the	O
realizable	O
case	O
online	B
learning	I
is	O
performed	O
in	O
a	O
sequence	O
of	O
consecutive	O
rounds	O
where	O
at	O
round	O
t	O
the	O
learner	O
is	O
given	O
an	O
instance	B
xt	O
taken	O
from	O
an	O
instance	B
domain	B
x	O
and	O
is	O
required	O
to	O
provide	O
its	O
label	B
we	O
denote	O
the	O
predicted	O
label	B
by	O
pt	O
after	O
predicting	O
the	O
label	B
the	O
correct	O
label	B
yt	O
is	O
revealed	O
to	O
the	O
learner	O
the	O
learner	O
s	O
goal	O
is	O
to	O
make	O
as	O
few	O
prediction	O
mistakes	O
as	O
possible	O
during	O
this	O
process	O
the	O
learner	O
tries	O
to	O
deduce	O
information	O
from	O
previous	O
rounds	O
so	O
as	O
to	O
improve	O
its	O
predictions	O
on	O
future	O
rounds	O
clearly	O
learning	O
is	O
hopeless	O
if	O
there	O
is	O
no	O
correlation	O
between	O
past	O
and	O
present	O
rounds	O
previously	O
in	O
the	O
book	O
we	O
studied	O
the	O
pac	B
model	O
in	O
which	O
we	O
assume	O
that	O
past	O
and	O
present	O
examples	O
are	O
sampled	O
i	O
i	O
d	O
from	O
the	O
same	O
distribution	O
source	O
in	O
the	O
online	B
learning	I
model	O
we	O
make	O
no	O
statistical	O
assumptions	O
regarding	O
the	O
origin	O
of	O
the	O
sequence	O
of	O
examples	O
the	O
sequence	O
is	O
allowed	O
to	O
be	O
deterministic	O
stochastic	O
or	O
even	O
adversarially	O
adaptive	O
to	O
the	O
learner	O
s	O
own	O
behavior	O
in	O
the	O
case	O
of	O
spam	O
e-mail	O
filtering	O
naturally	O
an	O
adversary	O
can	O
make	O
the	O
number	O
of	O
prediction	O
mistakes	O
of	O
our	O
online	B
learning	I
algorithm	O
arbitrarily	O
large	O
for	O
example	O
the	O
adversary	O
can	O
present	O
the	O
same	O
instance	B
on	O
each	O
online	B
round	O
wait	O
for	O
the	O
learner	O
s	O
prediction	O
and	O
provide	O
the	O
opposite	O
label	B
as	O
the	O
correct	O
label	B
to	O
make	O
nontrivial	O
statements	O
we	O
must	O
further	O
restrict	O
the	O
problem	O
the	O
realizability	B
assumption	O
is	O
one	O
possible	O
natural	O
restriction	O
in	O
the	O
realizable	O
case	O
we	O
assume	O
that	O
all	O
the	O
labels	O
are	O
generated	O
by	O
some	O
hypothesis	B
x	O
y	O
furthermore	O
is	O
taken	O
from	O
a	O
hypothesis	B
class	I
h	O
which	O
is	O
known	O
to	O
the	O
learner	O
this	O
is	O
analogous	O
to	O
the	O
pac	B
learning	O
model	O
we	O
studied	O
in	O
chapter	O
with	O
this	O
restriction	O
on	O
the	O
sequence	O
the	O
learner	O
should	O
make	O
as	O
few	O
mistakes	O
as	O
possible	O
assuming	O
that	O
both	O
and	O
the	O
sequence	O
of	O
instances	O
can	O
be	O
chosen	O
by	O
an	O
adversary	O
for	O
an	O
online	B
learning	I
algorithm	O
a	O
we	O
denote	O
by	O
mah	O
the	O
maximal	O
number	O
of	O
mistakes	O
a	O
might	O
make	O
on	O
a	O
sequence	O
of	O
examples	O
which	O
is	O
labeled	O
by	O
some	O
h	O
we	O
emphasize	O
again	O
that	O
both	O
and	O
the	O
sequence	O
of	O
instances	O
can	O
be	O
chosen	O
by	O
an	O
adversary	O
a	O
bound	O
on	O
mah	O
is	O
called	O
a	O
mistake-bound	O
and	O
we	O
will	O
study	O
how	O
to	O
design	O
algorithms	O
for	O
which	O
mah	O
is	O
minimal	O
formally	O
definition	O
bounds	O
online	B
learnability	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
and	O
let	O
a	O
be	O
an	O
online	B
learning	I
algorithm	O
given	O
any	O
sequence	O
s	O
where	O
t	O
is	O
any	O
integer	O
and	O
h	O
let	O
mas	O
be	O
the	O
number	O
of	O
mistakes	O
a	O
makes	O
on	O
the	O
sequence	O
s	O
we	O
denote	O
by	O
mah	O
the	O
supremum	O
of	O
mas	O
over	O
all	O
sequences	O
of	O
the	O
above	O
form	O
a	O
bound	O
of	O
the	O
form	O
mah	O
b	O
is	O
called	O
a	O
mistake	B
bound	I
we	O
say	O
that	O
a	O
hypothesis	B
class	I
h	O
is	O
online	B
learnable	O
if	O
there	O
exists	O
an	O
algorithm	O
a	O
for	O
which	O
mah	O
b	O
our	O
goal	O
is	O
to	O
study	O
which	O
hypothesis	B
classes	O
are	O
learnable	O
in	O
the	O
online	B
model	O
and	O
in	O
particular	O
to	O
find	O
good	O
learning	O
algorithms	O
for	O
a	O
given	O
hypothesis	B
class	I
remark	O
throughout	O
this	O
section	O
and	O
the	O
next	O
we	O
ignore	O
the	O
computa	O
online	B
classification	O
in	O
the	O
realizable	O
case	O
tional	O
aspect	O
of	O
learning	O
and	O
do	O
not	O
restrict	O
the	O
algorithms	O
to	O
be	O
efficient	O
in	O
section	O
and	O
section	O
we	O
study	O
efficient	O
online	B
learning	I
algorithms	O
to	O
simplify	O
the	O
presentation	O
we	O
start	O
with	O
the	O
case	O
of	O
a	O
finite	O
hypothesis	B
class	I
namely	O
in	O
pac	B
learning	O
we	O
identified	O
erm	B
as	O
a	O
good	O
learning	O
algorithm	O
in	O
the	O
sense	O
that	O
if	O
h	O
is	O
learnable	O
then	O
it	O
is	O
learnable	O
by	O
the	O
rule	O
ermh	O
a	O
natural	O
learning	O
rule	O
for	O
online	B
learning	I
is	O
to	O
use	O
any	O
online	B
round	O
any	O
erm	B
hypothesis	B
namely	O
any	O
hypothesis	B
which	O
is	O
consistent	B
with	O
all	O
past	O
examples	O
consistent	B
input	O
a	O
finite	O
hypothesis	B
class	I
h	O
initialize	O
h	O
for	O
t	O
receive	O
xt	O
choose	O
any	O
h	O
vt	O
predict	O
pt	O
hxt	O
receive	O
true	O
label	B
yt	O
update	O
vt	O
hxt	O
yt	O
the	O
consistent	B
algorithm	O
maintains	O
a	O
set	B
vt	O
of	O
all	O
the	O
hypotheses	O
which	O
are	O
consistent	B
with	O
yt	O
this	O
set	B
is	O
often	O
called	O
the	O
version	B
space	I
it	O
then	O
picks	O
any	O
hypothesis	B
from	O
vt	O
and	O
predicts	O
according	O
to	O
this	O
hypothesis	B
obviously	O
whenever	O
consistent	B
makes	O
a	O
prediction	O
mistake	O
at	O
least	O
one	O
hypothesis	B
is	O
removed	O
from	O
vt	O
therefore	O
after	O
making	O
m	O
mistakes	O
we	O
have	O
m	O
since	O
vt	O
is	O
always	O
nonempty	O
the	O
realizability	B
assumption	O
it	O
contains	O
we	O
have	O
m	O
rearranging	O
we	O
obtain	O
the	O
following	O
corollary	O
let	O
h	O
be	O
a	O
finite	O
hypothesis	B
class	I
the	O
consistent	B
algorithm	O
enjoys	O
the	O
mistake	B
bound	I
mconsistenth	O
it	O
is	O
rather	O
easy	O
to	O
construct	O
a	O
hypothesis	B
class	I
and	O
a	O
sequence	O
of	O
examples	O
on	O
which	O
consistent	B
will	O
indeed	O
make	O
mistakes	O
exercise	O
therefore	O
we	O
present	O
a	O
better	O
algorithm	O
in	O
which	O
we	O
choose	O
h	O
vt	O
in	O
a	O
smarter	O
way	O
we	O
shall	O
see	O
that	O
this	O
algorithm	O
is	O
guaranteed	O
to	O
make	O
exponentially	O
fewer	O
mistakes	O
halving	B
input	O
a	O
finite	O
hypothesis	B
class	I
h	O
initialize	O
h	O
for	O
t	O
receive	O
xt	O
predict	O
pt	O
argmaxr	O
vt	O
hxt	O
r	O
case	O
of	O
a	O
tie	O
predict	O
pt	O
receive	O
true	O
label	B
yt	O
update	O
vt	O
hxt	O
yt	O
online	B
learning	I
theorem	O
let	O
h	O
be	O
a	O
finite	O
hypothesis	B
class	I
the	O
halving	B
algorithm	O
enjoys	O
the	O
mistake	B
bound	I
mhalvingh	O
proof	O
we	O
simply	O
note	O
that	O
whenever	O
the	O
algorithm	O
errs	O
we	O
have	O
the	O
name	O
halving	B
therefore	O
if	O
m	O
is	O
the	O
total	O
number	O
of	O
mistakes	O
we	O
have	O
m	O
rearranging	O
this	O
inequality	O
we	O
conclude	O
our	O
proof	O
of	O
course	O
halving	B
s	O
mistake	B
bound	I
is	O
much	O
better	O
than	O
consistent	B
s	O
mistake	B
bound	I
we	O
already	O
see	O
that	O
online	B
learning	I
is	O
different	O
from	O
pac	B
learning	O
while	O
in	O
pac	B
any	O
erm	B
hypothesis	B
is	O
good	O
in	O
online	B
learning	I
choosing	O
an	O
arbitrary	O
erm	B
hypothesis	B
is	O
far	O
from	O
being	O
optimal	O
online	B
learnability	O
we	O
next	O
take	O
a	O
more	O
general	O
approach	O
and	O
aim	O
at	O
characterizing	O
online	B
learnability	O
in	O
particular	O
we	O
target	O
the	O
following	O
question	O
what	O
is	O
the	O
optimal	O
online	B
learning	I
algorithm	O
for	O
a	O
given	O
hypothesis	B
class	I
h	O
we	O
present	O
a	O
dimension	O
of	O
hypothesis	B
classes	O
that	O
characterizes	O
the	O
best	O
achievable	O
mistake	B
bound	I
this	O
measure	O
was	O
proposed	O
by	O
nick	O
littlestone	O
and	O
we	O
therefore	O
refer	O
to	O
it	O
as	O
ldimh	O
to	O
motivate	O
the	O
definition	O
of	O
ldim	B
it	O
is	O
convenient	O
to	O
view	O
the	O
online	B
learning	I
process	O
as	O
a	O
game	O
between	O
two	O
players	O
the	O
learner	O
versus	O
the	O
environment	O
on	O
round	O
t	O
of	O
the	O
game	O
the	O
environment	O
picks	O
an	O
instance	B
xt	O
the	O
learner	O
predicts	O
a	O
label	B
pt	O
and	O
finally	O
the	O
environment	O
outputs	O
the	O
true	O
label	B
yt	O
suppose	O
that	O
the	O
environment	O
wants	O
to	O
make	O
the	O
learner	O
err	O
on	O
the	O
first	O
t	O
rounds	O
of	O
the	O
game	O
then	O
it	O
must	O
output	O
yt	O
pt	O
and	O
the	O
only	O
question	O
is	O
how	O
it	O
should	O
choose	O
the	O
instances	O
xt	O
in	O
such	O
a	O
way	O
that	O
ensures	O
that	O
for	O
some	O
h	O
we	O
have	O
yt	O
for	O
all	O
t	O
a	O
strategy	O
for	O
an	O
adversarial	O
environment	O
can	O
be	O
formally	O
described	O
as	O
a	O
binary	O
tree	O
as	O
follows	O
each	O
node	O
of	O
the	O
tree	O
is	O
associated	O
with	O
an	O
instance	B
from	O
x	O
initially	O
the	O
environment	O
presents	O
to	O
the	O
learner	O
the	O
instance	B
associated	O
with	O
the	O
root	O
of	O
the	O
tree	O
then	O
if	O
the	O
learner	O
predicts	O
pt	O
the	O
environment	O
will	O
declare	O
that	O
this	O
is	O
a	O
wrong	O
prediction	O
yt	O
and	O
will	O
traverse	O
to	O
the	O
right	O
child	O
of	O
the	O
current	O
node	O
if	O
the	O
learner	O
predicts	O
pt	O
then	O
the	O
environment	O
will	O
set	B
yt	O
and	O
will	O
traverse	O
to	O
the	O
left	O
child	O
this	O
process	O
will	O
continue	O
and	O
at	O
each	O
round	O
the	O
environment	O
will	O
present	O
the	O
instance	B
associated	O
with	O
the	O
current	O
node	O
formally	O
consider	O
a	O
complete	O
binary	O
tree	O
of	O
depth	O
t	O
define	O
the	O
depth	O
of	O
the	O
tree	O
as	O
the	O
number	O
of	O
edges	O
in	O
a	O
path	O
from	O
the	O
root	O
to	O
a	O
leaf	O
we	O
have	O
nodes	O
in	O
such	O
a	O
tree	O
and	O
we	O
attach	O
an	O
instance	B
to	O
each	O
node	O
let	O
be	O
these	O
instances	O
we	O
start	O
from	O
the	O
root	O
of	O
the	O
tree	O
and	O
set	B
at	O
round	O
t	O
we	O
set	B
xt	O
vit	O
where	O
it	O
is	O
the	O
current	O
node	O
at	O
the	O
end	O
of	O
online	B
classification	O
in	O
the	O
realizable	O
case	O
figure	O
an	O
illustration	O
of	O
a	O
shattered	O
tree	O
of	O
depth	O
the	O
dashed	O
path	O
corresponds	O
to	O
the	O
sequence	O
of	O
examples	O
the	O
tree	O
is	O
shattered	O
by	O
h	O
where	O
the	O
predictions	O
of	O
each	O
hypothesis	B
in	O
h	O
on	O
the	O
instances	O
is	O
given	O
in	O
the	O
table	O
mark	O
means	O
that	O
hjvi	O
can	O
be	O
either	O
or	O
is	O
unraveling	O
the	O
recursion	O
we	O
obtain	O
it	O
round	O
t	O
we	O
go	O
to	O
the	O
left	O
child	O
of	O
it	O
if	O
yt	O
or	O
to	O
the	O
right	O
child	O
if	O
yt	O
that	O
yj	O
j	O
the	O
preceding	O
strategy	O
for	O
the	O
environment	O
succeeds	O
only	O
if	O
for	O
every	O
yt	O
there	O
exists	O
h	O
h	O
such	O
that	O
yt	O
hxt	O
for	O
all	O
t	O
this	O
leads	O
to	O
the	O
following	O
definition	O
definition	O
shattered	O
tree	O
a	O
shattered	O
tree	O
of	O
depth	O
d	O
is	O
a	O
sequence	O
of	O
instances	O
in	O
x	O
such	O
that	O
for	O
every	O
labeling	O
yd	O
there	O
exists	O
h	O
h	O
such	O
that	O
for	O
all	O
t	O
we	O
have	O
hvit	O
yt	O
where	O
it	O
yj	O
j	O
an	O
illustration	O
of	O
a	O
shattered	O
tree	O
of	O
depth	O
is	O
given	O
in	O
figure	O
definition	O
s	O
dimension	O
ldimh	O
is	O
the	O
maximal	O
integer	O
t	O
such	O
that	O
there	O
exists	O
a	O
shattered	O
tree	O
of	O
depth	O
t	O
which	O
is	O
shattered	O
by	O
h	O
the	O
definition	O
of	O
ldim	B
and	O
the	O
discussion	O
above	O
immediately	O
imply	O
the	O
fol	O
lowing	O
lemma	O
no	O
algorithm	O
can	O
have	O
a	O
mistake	B
bound	I
strictly	O
smaller	O
than	O
ldimh	O
namely	O
for	O
every	O
algorithm	O
a	O
we	O
have	O
mah	O
ldimh	O
proof	O
let	O
t	O
ldimh	O
and	O
let	O
be	O
a	O
sequence	O
that	O
satisfies	O
the	O
requirements	O
in	O
the	O
definition	O
of	O
ldim	B
if	O
the	O
environment	O
sets	O
xt	O
vit	O
and	O
yt	O
pt	O
for	O
all	O
t	O
then	O
the	O
learner	O
makes	O
t	O
mistakes	O
while	O
the	O
definition	O
of	O
ldim	B
implies	O
that	O
there	O
exists	O
a	O
hypothesis	B
h	O
h	O
such	O
that	O
yt	O
hxt	O
for	O
all	O
t	O
let	O
us	O
now	O
give	O
several	O
examples	O
example	O
let	O
h	O
be	O
a	O
finite	O
hypothesis	B
class	I
clearly	O
any	O
tree	O
that	O
is	O
shattered	O
by	O
h	O
has	O
depth	O
of	O
at	O
most	O
therefore	O
ldimh	O
another	O
way	O
to	O
conclude	O
this	O
inequality	O
is	O
by	O
combining	O
lemma	O
with	O
theorem	O
example	O
let	O
x	O
d	O
and	O
h	O
hd	O
where	O
hjx	O
iff	O
online	B
learning	I
x	O
j	O
then	O
it	O
is	O
easy	O
to	O
show	O
that	O
ldimh	O
while	O
d	O
can	O
be	O
arbitrarily	O
large	O
therefore	O
this	O
example	O
shows	O
that	O
ldimh	O
can	O
be	O
significantly	O
smaller	O
than	O
example	O
let	O
x	O
and	O
h	O
a	O
namely	O
h	O
is	O
the	O
class	O
of	O
thresholds	O
on	O
the	O
interval	O
then	O
ldimh	O
to	O
see	O
this	O
consider	O
the	O
tree	O
this	O
tree	O
is	O
shattered	O
by	O
h	O
and	O
because	O
of	O
the	O
density	O
of	O
the	O
reals	O
this	O
tree	O
can	O
be	O
made	O
arbitrarily	O
deep	O
lemma	O
states	O
that	O
ldimh	O
lower	O
bounds	O
the	O
mistake	B
bound	I
of	O
any	O
algorithm	O
interestingly	O
there	O
is	O
a	O
standard	O
algorithm	O
whose	O
mistake	B
bound	I
matches	O
this	O
lower	O
bound	O
the	O
algorithm	O
is	O
similar	O
to	O
the	O
halving	B
algorithm	O
recall	B
that	O
the	O
prediction	O
of	O
halving	B
is	O
made	O
according	O
to	O
a	O
majority	O
vote	O
of	O
the	O
hypotheses	O
which	O
are	O
consistent	B
with	O
previous	O
examples	O
we	O
denoted	O
this	O
t	O
vt	O
set	B
by	O
vt	O
put	O
another	O
way	O
halving	B
partitions	O
vt	O
into	O
two	O
sets	O
v	O
hxt	O
and	O
v	O
t	O
vt	O
hxt	O
it	O
then	O
predicts	O
according	O
to	O
the	O
larger	O
of	O
the	O
two	O
groups	O
the	O
rationale	O
behind	O
this	O
prediction	O
is	O
that	O
whenever	O
halving	B
makes	O
a	O
mistake	O
it	O
ends	O
up	O
with	O
the	O
optimal	O
algorithm	O
we	O
present	O
in	O
the	O
following	O
uses	O
the	O
same	O
idea	O
but	O
instead	O
of	O
predicting	O
according	O
to	O
the	O
larger	O
class	O
it	O
predicts	O
according	O
to	O
the	O
class	O
with	O
larger	O
ldim	B
standard	O
optimal	O
algorithm	O
input	O
a	O
hypothesis	B
class	I
h	O
initialize	O
h	O
for	O
t	O
receive	O
xt	O
for	O
r	O
let	O
v	O
predict	O
pt	O
argmaxr	O
ldimv	O
t	O
vt	O
hxt	O
r	O
t	O
case	O
of	O
a	O
tie	O
predict	O
pt	O
receive	O
true	O
label	B
yt	O
update	O
vt	O
hxt	O
yt	O
the	O
following	O
lemma	O
formally	O
establishes	O
the	O
optimality	O
of	O
the	O
preceding	O
al	O
gorithm	O
online	B
classification	O
in	O
the	O
realizable	O
case	O
lemma	O
soa	B
enjoys	O
the	O
mistake	B
bound	I
msoah	O
ldimh	O
proof	O
it	O
suffices	O
to	O
prove	O
that	O
whenever	O
the	O
algorithm	O
makes	O
a	O
prediction	O
mistake	O
we	O
have	O
ldimvt	O
we	O
prove	O
this	O
claim	O
by	O
assuming	O
the	O
contrary	O
that	O
is	O
ldimvt	O
if	O
this	O
holds	O
true	O
then	O
the	O
definition	O
of	O
pt	O
implies	O
that	O
ldimv	O
ldimvt	O
for	O
both	O
r	O
and	O
r	O
but	O
then	O
we	O
can	O
construct	O
a	O
shaterred	O
tree	O
of	O
depth	O
ldimvt	O
for	O
the	O
class	O
vt	O
which	O
leads	O
to	O
the	O
desired	O
contradiction	O
t	O
combining	O
lemma	O
and	O
lemma	O
we	O
obtain	O
corollary	O
let	O
h	O
be	O
any	O
hypothesis	B
class	I
then	O
the	O
standard	O
optimal	O
algorithm	O
enjoys	O
the	O
mistake	B
bound	I
msoah	O
ldimh	O
and	O
no	O
other	O
algorithm	O
can	O
have	O
mah	O
ldimh	O
comparison	O
to	O
vc	B
dimension	I
in	O
the	O
pac	B
learning	O
model	O
learnability	O
is	O
characterized	O
by	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
h	O
recall	B
that	O
the	O
vc	B
dimension	I
of	O
a	O
class	O
h	O
is	O
the	O
maximal	O
number	O
d	O
such	O
that	O
there	O
are	O
instances	O
xd	O
that	O
are	O
shattered	O
by	O
h	O
that	O
is	O
for	O
any	O
sequence	O
of	O
labels	O
yd	O
there	O
exists	O
a	O
hypothesis	B
h	O
h	O
that	O
gives	O
exactly	O
this	O
sequence	O
of	O
labels	O
the	O
following	O
theorem	O
relates	O
the	O
vc	B
dimension	I
to	O
the	O
littlestone	O
dimension	O
theorem	O
for	O
any	O
class	O
h	O
vcdimh	O
ldimh	O
and	O
there	O
are	O
classes	O
for	O
which	O
strict	O
inequality	O
holds	O
furthermore	O
the	O
gap	O
can	O
be	O
arbitrarily	O
larger	O
proof	O
we	O
first	O
prove	O
that	O
vcdimh	O
ldimh	O
suppose	O
vcdimh	O
d	O
and	O
let	O
xd	O
be	O
a	O
shattered	O
set	B
we	O
now	O
construct	O
a	O
complete	O
binary	O
tree	O
of	O
instances	O
where	O
all	O
nodes	O
at	O
depth	O
i	O
are	O
set	B
to	O
be	O
xi	O
see	O
the	O
following	O
illustration	O
now	O
the	O
definition	O
of	O
a	O
shattered	O
set	B
clearly	O
implies	O
that	O
we	O
got	O
a	O
valid	O
shattered	O
tree	O
of	O
depth	O
d	O
and	O
we	O
conclude	O
that	O
vcdimh	O
ldimh	O
to	O
show	O
that	O
the	O
gap	O
can	O
be	O
arbitrarily	O
large	O
simply	O
note	O
that	O
the	O
class	O
given	O
in	O
example	O
has	O
vc	B
dimension	I
of	O
whereas	O
its	O
littlestone	O
dimension	O
is	O
infinite	O
online	B
learning	I
online	B
classification	O
in	O
the	O
unrealizable	O
case	O
yt	O
yt	O
in	O
the	O
previous	O
section	O
we	O
studied	O
online	B
learnability	O
in	O
the	O
realizable	O
case	O
we	O
now	O
consider	O
the	O
unrealizable	O
case	O
similarly	O
to	O
the	O
agnostic	B
pac	B
model	O
we	O
no	O
longer	O
assume	O
that	O
all	O
labels	O
are	O
generated	O
by	O
some	O
h	O
but	O
we	O
require	O
the	O
learner	O
to	O
be	O
competitive	O
with	O
the	O
best	O
fixed	O
predictor	B
from	O
h	O
this	O
is	O
captured	O
by	O
the	O
regret	O
of	O
the	O
algorithm	O
which	O
measures	O
how	O
sorry	O
the	O
learner	O
is	O
in	O
retrospect	O
not	O
to	O
have	O
followed	O
the	O
predictions	O
of	O
some	O
hypothesis	B
h	O
h	O
formally	O
the	O
regret	O
of	O
an	O
algorithm	O
a	O
relative	O
to	O
h	O
when	O
running	O
on	O
a	O
sequence	O
of	O
t	O
examples	O
is	O
defined	O
as	O
regretah	O
t	O
sup	O
and	O
the	O
regret	O
of	O
the	O
algorithm	O
relative	O
to	O
a	O
hypothesis	B
class	I
h	O
is	O
regretah	O
t	O
sup	O
h	O
h	O
regretah	O
t	O
we	O
restate	O
the	O
learner	O
s	O
goal	O
as	O
having	O
the	O
lowest	O
possible	O
regret	O
relative	O
to	O
h	O
an	O
interesting	O
question	O
is	O
whether	O
we	O
can	O
derive	O
an	O
algorithm	O
with	O
low	O
regret	O
meaning	O
that	O
regretah	O
t	O
grows	O
sublinearly	O
with	O
the	O
number	O
of	O
rounds	O
t	O
which	O
implies	O
that	O
the	O
difference	O
between	O
the	O
error	O
rate	O
of	O
the	O
learner	O
and	O
the	O
best	O
hypothesis	B
in	O
h	O
tends	O
to	O
zero	O
as	O
t	O
goes	O
to	O
infinity	O
we	O
first	O
show	O
that	O
this	O
is	O
an	O
impossible	O
mission	O
no	O
algorithm	O
can	O
obtain	O
a	O
sublinear	O
regret	O
bound	O
even	O
if	O
indeed	O
consider	O
h	O
where	O
is	O
the	O
function	B
that	O
always	O
returns	O
and	O
is	O
the	O
function	B
that	O
always	O
returns	O
an	O
adversary	O
can	O
make	O
the	O
number	O
of	O
mistakes	O
of	O
any	O
online	B
algorithm	O
be	O
equal	O
to	O
t	O
by	O
simply	O
waiting	O
for	O
the	O
learner	O
s	O
prediction	O
and	O
then	O
providing	O
the	O
opposite	O
label	B
as	O
the	O
true	O
label	B
in	O
contrast	O
for	O
any	O
sequence	O
of	O
true	O
labels	O
yt	O
let	O
b	O
be	O
the	O
majority	O
of	O
labels	O
in	O
yt	O
then	O
the	O
number	O
of	O
mistakes	O
of	O
hb	O
is	O
at	O
most	O
t	O
therefore	O
the	O
regret	O
of	O
any	O
online	B
algorithm	O
might	O
be	O
at	O
least	O
t	O
t	O
t	O
which	O
is	O
not	O
sublinear	O
in	O
t	O
this	O
impossibility	O
result	O
is	O
attributed	O
to	O
cover	O
to	O
sidestep	O
cover	O
s	O
impossibility	O
result	O
we	O
must	O
further	O
restrict	O
the	O
power	O
of	O
the	O
adversarial	O
environment	O
we	O
do	O
so	O
by	O
allowing	O
the	O
learner	O
to	O
randomize	O
his	O
predictions	O
of	O
course	O
this	O
by	O
itself	O
does	O
not	O
circumvent	O
cover	O
s	O
impossibility	O
result	O
since	O
in	O
deriving	O
this	O
result	O
we	O
assumed	O
nothing	O
about	O
the	O
learner	O
s	O
strategy	O
to	O
make	O
the	O
randomization	O
meaningful	O
we	O
force	O
the	O
adversarial	O
environment	O
to	O
decide	O
on	O
yt	O
without	O
knowing	O
the	O
random	O
coins	O
flipped	O
by	O
the	O
learner	O
on	O
round	O
t	O
the	O
adversary	O
can	O
still	O
know	O
the	O
learner	O
s	O
forecasting	O
strategy	O
and	O
even	O
the	O
random	O
coin	O
flips	O
of	O
previous	O
rounds	O
but	O
it	O
does	O
not	O
know	O
the	O
actual	O
value	O
of	O
the	O
random	O
coin	O
flips	O
used	O
by	O
the	O
learner	O
on	O
round	O
t	O
with	O
this	O
change	O
of	O
game	O
we	O
analyze	O
the	O
expected	O
number	O
of	O
mistakes	O
of	O
the	O
algorithm	O
where	O
the	O
expectation	O
is	O
with	O
respect	O
to	O
the	O
learner	O
s	O
own	O
randomization	O
that	O
is	O
if	O
the	O
learner	O
outputs	O
yt	O
where	O
p	O
yt	O
pt	O
then	O
the	O
expected	O
loss	B
he	O
pays	O
online	B
classification	O
in	O
the	O
unrealizable	O
case	O
on	O
round	O
t	O
is	O
p	O
yt	O
yt	O
yt	O
put	O
another	O
way	O
instead	O
of	O
having	O
the	O
predictions	O
of	O
the	O
learner	O
being	O
in	O
we	O
allow	O
them	O
to	O
be	O
in	O
and	O
interpret	O
pt	O
as	O
the	O
probability	O
to	O
predict	O
the	O
label	B
on	O
round	O
t	O
with	O
this	O
assumption	O
it	O
is	O
possible	O
to	O
derive	O
a	O
low	O
regret	O
algorithm	O
in	O
partic	O
ular	O
we	O
will	O
prove	O
the	O
following	O
theorem	O
theorem	O
for	O
every	O
hypothesis	B
class	I
h	O
there	O
exists	O
an	O
algorithm	O
for	O
online	B
classification	O
whose	O
predictions	O
come	O
from	O
that	O
enjoys	O
the	O
regret	O
bound	O
yt	O
minlogh	O
ldimh	O
loget	O
t	O
h	O
h	O
t	O
yt	O
furthermore	O
no	O
algorithm	O
can	O
achieve	O
an	O
expected	O
regret	O
bound	O
smaller	O
than	O
we	O
will	O
provide	O
a	O
constructive	O
proof	O
of	O
the	O
upper	O
bound	O
part	O
of	O
the	O
preceding	O
theorem	O
the	O
proof	O
of	O
the	O
lower	O
bound	O
part	O
can	O
be	O
found	O
in	O
pal	O
shalev-shwartz	O
the	O
proof	O
of	O
theorem	O
relies	O
on	O
the	O
weighted-majority	B
algorithm	O
for	O
learning	O
with	O
expert	O
advice	O
this	O
algorithm	O
is	O
important	O
by	O
itself	O
and	O
we	O
dedicate	O
the	O
next	O
subsection	O
to	O
it	O
weighted-majority	B
i	O
wt	O
with	O
i	O
and	O
choosing	O
the	O
ith	O
expert	O
with	O
probability	O
wt	O
weighted-majority	B
is	O
an	O
algorithm	O
for	O
the	O
problem	O
of	O
prediction	O
with	O
expert	O
advice	O
in	O
this	O
online	B
learning	I
problem	O
on	O
round	O
t	O
the	O
learner	O
has	O
to	O
choose	O
the	O
advice	O
of	O
d	O
given	O
experts	O
we	O
also	O
allow	O
the	O
learner	O
to	O
randomize	O
his	O
choice	O
by	O
defining	O
a	O
distribution	O
over	O
the	O
d	O
experts	O
that	O
is	O
picking	O
a	O
vector	O
wt	O
after	O
the	O
learner	O
chooses	O
an	O
expert	O
it	O
receives	O
a	O
vector	O
of	O
costs	O
vt	O
where	O
vti	O
is	O
the	O
cost	O
of	O
following	O
the	O
advice	O
of	O
the	O
ith	O
expert	O
if	O
the	O
learner	O
s	O
predictions	O
are	O
randomized	O
then	O
its	O
loss	B
is	O
defined	O
to	O
be	O
the	O
averaged	O
cost	O
namely	O
i	O
vti	O
the	O
algorithm	O
assumes	O
that	O
the	O
number	O
of	O
rounds	O
t	O
is	O
given	O
in	O
exercise	O
we	O
show	O
how	O
to	O
get	O
rid	O
of	O
this	O
dependence	O
using	O
the	O
doubling	O
trick	O
i	O
wt	O
i	O
online	B
learning	I
input	O
number	O
of	O
experts	O
d	O
number	O
of	O
rounds	O
t	O
weighted-majority	B
parameter	O
logdt	O
set	B
wt	O
wtzt	O
where	O
zt	O
initialize	O
for	O
t	O
i	O
wt	O
choose	O
expert	O
i	O
at	O
random	O
according	O
to	O
pi	O
wt	O
receive	O
costs	O
of	O
all	O
experts	O
vt	O
pay	O
cost	O
update	O
rule	O
i	O
wt	O
i	O
e	O
vti	O
i	O
i	O
i	O
the	O
following	O
theorem	O
is	O
key	O
for	O
analyzing	O
the	O
regret	O
bound	O
of	O
weighted	O
majority	O
theorem	O
assuming	O
that	O
t	O
logd	O
the	O
weighted-majority	B
algorithm	O
enjoys	O
the	O
bound	O
proof	O
we	O
have	O
fact	O
using	O
the	O
inequality	O
e	O
a	O
a	O
which	O
holds	O
for	O
all	O
a	O
and	O
the	O
min	O
i	O
vti	O
logd	O
t	O
i	O
wt	O
i	O
zt	O
e	O
vti	O
log	O
i	O
i	O
e	O
vti	O
wt	O
log	O
zt	O
log	O
i	O
wt	O
i	O
we	O
obtain	O
log	O
log	O
zt	O
wt	O
i	O
i	O
i	O
vti	O
vti	O
wt	O
i	O
def	O
b	O
next	O
note	O
that	O
b	O
therefore	O
taking	O
log	O
of	O
the	O
two	O
sides	O
of	O
the	O
inequality	O
b	O
e	O
b	O
we	O
obtain	O
the	O
inequality	O
b	O
b	O
which	O
holds	O
for	O
all	O
b	O
and	O
obtain	O
log	O
zt	O
vti	O
wt	O
wt	O
i	O
i	O
i	O
i	O
online	B
classification	O
in	O
the	O
unrealizable	O
case	O
summing	O
this	O
inequality	O
over	O
t	O
we	O
get	O
logzt	O
log	O
zt	O
next	O
we	O
lower	O
bound	O
zt	O
for	O
each	O
i	O
we	O
can	O
rewrite	O
wt	O
we	O
get	O
that	O
i	O
i	O
e	O
e	O
log	O
zt	O
log	O
t	O
vti	O
log	O
max	O
i	O
t	O
vti	O
min	O
vti	O
combining	O
the	O
preceding	O
with	O
equation	O
and	O
using	O
the	O
fact	O
that	O
logd	O
we	O
get	O
that	O
t	O
vti	O
and	O
t	O
e	O
i	O
t	O
min	O
i	O
vti	O
logd	O
t	O
t	O
which	O
can	O
be	O
rearranged	O
as	O
follows	O
min	O
i	O
t	O
vti	O
logd	O
t	O
plugging	O
the	O
value	O
of	O
into	O
the	O
equation	O
concludes	O
our	O
proof	O
proof	O
of	O
theorem	O
equipped	O
with	O
the	O
weighted-majority	B
algorithm	O
and	O
theorem	O
we	O
are	O
ready	O
to	O
prove	O
theorem	O
we	O
start	O
with	O
the	O
simpler	O
case	O
in	O
which	O
h	O
is	O
a	O
finite	O
class	O
and	O
let	O
us	O
write	O
h	O
hd	O
in	O
this	O
case	O
we	O
can	O
refer	O
to	O
each	O
hypothesis	B
hi	O
as	O
an	O
expert	O
whose	O
advice	O
is	O
to	O
predict	O
hixt	O
and	O
whose	O
cost	O
is	O
vti	O
yt	O
the	O
prediction	O
of	O
the	O
algorithm	O
will	O
therefore	O
be	O
i	O
hixt	O
and	O
the	O
loss	B
is	O
i	O
wt	O
pt	O
yt	O
i	O
hixt	O
yt	O
wt	O
i	O
yt	O
wt	O
i	O
wt	O
now	O
if	O
yt	O
then	O
for	O
all	O
i	O
hixt	O
yt	O
therefore	O
the	O
above	O
equals	O
to	O
yt	O
if	O
yt	O
then	O
for	O
all	O
i	O
hixt	O
yt	O
and	O
the	O
above	O
also	O
i	O
wt	O
yt	O
all	O
in	O
all	O
we	O
have	O
shown	O
that	O
i	O
i	O
yt	O
furthermore	O
for	O
each	O
yt	O
wt	O
i	O
makes	O
applying	O
theorem	O
we	O
obtain	O
t	O
vti	O
is	O
exactly	O
the	O
number	O
of	O
mistakes	O
hypothesis	B
hi	O
online	B
learning	I
corollary	O
let	O
h	O
be	O
a	O
finite	O
hypothesis	B
class	I
there	O
exists	O
an	O
algorithm	O
for	O
online	B
classification	O
whose	O
predictions	O
come	O
from	O
that	O
enjoys	O
the	O
regret	O
bound	O
yt	O
logh	O
t	O
yt	O
min	O
h	O
h	O
next	O
we	O
consider	O
the	O
case	O
of	O
a	O
general	O
hypothesis	B
class	I
previously	O
we	O
constructed	O
an	O
expert	O
for	O
each	O
individual	O
hypothesis	B
however	O
if	O
h	O
is	O
infinite	O
this	O
leads	O
to	O
a	O
vacuous	O
bound	O
the	O
main	O
idea	O
is	O
to	O
construct	O
a	O
set	B
of	O
experts	O
in	O
a	O
more	O
sophisticated	O
way	O
the	O
challenge	O
is	O
how	O
to	O
define	O
a	O
set	B
of	O
experts	O
that	O
on	O
one	O
hand	O
is	O
not	O
excessively	O
large	O
and	O
on	O
the	O
other	O
hand	O
contains	O
experts	O
that	O
give	O
accurate	O
predictions	O
we	O
construct	O
the	O
set	B
of	O
experts	O
so	O
that	O
for	O
each	O
hypothesis	B
h	O
h	O
and	O
every	O
sequence	O
of	O
instances	O
xt	O
there	O
exists	O
at	O
least	O
one	O
expert	O
in	O
the	O
set	B
which	O
behaves	O
exactly	O
as	O
h	O
on	O
these	O
instances	O
for	O
each	O
l	O
ldimh	O
and	O
each	O
sequence	O
il	O
t	O
we	O
define	O
an	O
expert	O
the	O
expert	O
simulates	O
the	O
game	O
between	O
soa	B
in	O
the	O
previous	O
section	O
and	O
the	O
environment	O
on	O
the	O
sequence	O
of	O
instances	O
xt	O
assuming	O
that	O
soa	B
makes	O
a	O
mistake	O
precisely	O
in	O
rounds	O
il	O
the	O
expert	O
is	O
defined	O
by	O
the	O
following	O
algorithm	O
il	O
input	O
a	O
hypothesis	B
class	I
h	O
indices	O
il	O
initialize	O
h	O
for	O
t	O
t	O
t	O
vt	O
hxt	O
r	O
v	O
t	O
receive	O
xt	O
for	O
r	O
let	O
v	O
define	O
yt	O
argmaxr	O
ldim	B
if	O
case	O
of	O
a	O
tie	O
set	B
yt	O
t	O
il	O
predict	O
yt	O
yt	O
else	O
predict	O
yt	O
yt	O
update	O
v	O
yt	O
t	O
note	O
that	O
each	O
such	O
expert	O
can	O
give	O
us	O
predictions	O
at	O
every	O
round	O
t	O
while	O
only	O
observing	O
the	O
instances	O
xt	O
our	O
generic	O
online	B
learning	I
algorithm	O
is	O
now	O
an	O
application	O
of	O
the	O
weighted-majority	B
algorithm	O
with	O
these	O
experts	O
to	O
analyze	O
the	O
algorithm	O
we	O
first	O
note	O
that	O
the	O
number	O
of	O
experts	O
is	O
l	O
d	O
it	O
can	O
be	O
shown	O
that	O
when	O
t	O
ldimh	O
the	O
right-hand	O
side	O
of	O
the	O
equation	O
is	O
bounded	O
by	O
proof	O
can	O
be	O
found	O
in	O
lemma	O
online	B
classification	O
in	O
the	O
unrealizable	O
case	O
is	O
at	O
most	O
the	O
number	O
of	O
mistakes	O
of	O
the	O
best	O
expert	O
logd	O
t	O
we	O
will	O
theorem	O
tells	O
us	O
that	O
the	O
expected	O
number	O
of	O
mistakes	O
of	O
weighted-majority	B
next	O
show	O
that	O
the	O
number	O
of	O
mistakes	O
of	O
the	O
best	O
expert	O
is	O
at	O
most	O
the	O
number	O
of	O
mistakes	O
of	O
the	O
best	O
hypothesis	B
in	O
h	O
the	O
following	O
key	O
lemma	O
shows	O
that	O
on	O
any	O
sequence	O
of	O
instances	O
for	O
each	O
hypothesis	B
h	O
h	O
there	O
exists	O
an	O
expert	O
with	O
the	O
same	O
behavior	O
lemma	O
let	O
h	O
be	O
any	O
hypothesis	B
class	I
with	O
ldimh	O
let	O
xt	O
be	O
any	O
sequence	O
of	O
instances	O
for	O
any	O
h	O
h	O
there	O
exists	O
l	O
ldimh	O
and	O
indices	O
il	O
t	O
such	O
that	O
when	O
running	O
il	O
on	O
the	O
sequence	O
xt	O
the	O
expert	O
predicts	O
hxt	O
on	O
each	O
online	B
round	O
t	O
t	O
proof	O
fix	O
h	O
h	O
and	O
the	O
sequence	O
xt	O
we	O
must	O
construct	O
l	O
and	O
the	O
indices	O
il	O
consider	O
running	O
soa	B
on	O
the	O
input	O
hxt	O
soa	B
makes	O
at	O
most	O
ldimh	O
mistakes	O
on	O
such	O
input	O
we	O
define	O
l	O
to	O
be	O
the	O
number	O
of	O
mistakes	O
made	O
by	O
soa	B
and	O
we	O
define	O
il	O
to	O
be	O
the	O
set	B
of	O
rounds	O
in	O
which	O
soa	B
made	O
the	O
mistakes	O
now	O
consider	O
the	O
il	O
running	O
on	O
the	O
sequence	O
xt	O
by	O
construction	O
the	O
set	B
vt	O
maintained	O
by	O
il	O
equals	O
the	O
set	B
vt	O
maintained	O
by	O
soa	B
when	O
running	O
on	O
the	O
sequence	O
hxt	O
the	O
predictions	O
of	O
soa	B
differ	O
from	O
the	O
predictions	O
of	O
h	O
if	O
and	O
only	O
if	O
the	O
round	O
is	O
in	O
il	O
since	O
il	O
predicts	O
exactly	O
like	O
soa	B
if	O
t	O
is	O
not	O
in	O
il	O
and	O
the	O
opposite	O
of	O
soas	O
predictions	O
if	O
t	O
is	O
in	O
il	O
we	O
conclude	O
that	O
the	O
predictions	O
of	O
the	O
expert	O
are	O
always	O
the	O
same	O
as	O
the	O
predictions	O
of	O
h	O
the	O
previous	O
lemma	O
holds	O
in	O
particular	O
for	O
the	O
hypothesis	B
in	O
h	O
that	O
makes	O
the	O
least	O
number	O
of	O
mistakes	O
on	O
the	O
sequence	O
of	O
examples	O
and	O
we	O
therefore	O
obtain	O
the	O
following	O
corollary	O
let	O
yt	O
be	O
a	O
sequence	O
of	O
examples	O
and	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
with	O
ldimh	O
there	O
exists	O
l	O
ldimh	O
and	O
indices	O
il	O
t	O
such	O
that	O
il	O
makes	O
at	O
most	O
as	O
many	O
mistakes	O
as	O
the	O
best	O
h	O
h	O
does	O
namely	O
yt	O
min	O
h	O
h	O
mistakes	O
on	O
the	O
sequence	O
of	O
examples	O
together	O
with	O
theorem	O
the	O
upper	O
bound	O
part	O
of	O
theorem	O
is	O
proven	O
online	B
learning	I
online	B
convex	B
optimization	I
in	O
chapter	O
we	O
studied	O
convex	B
learning	O
problems	O
and	O
showed	O
learnability	O
results	O
for	O
these	O
problems	O
in	O
the	O
agnostic	B
pac	B
learning	O
framework	O
in	O
this	O
section	O
we	O
show	O
that	O
similar	O
learnability	O
results	O
hold	O
for	O
convex	B
problems	O
in	O
the	O
online	B
learning	I
framework	O
in	O
particular	O
we	O
consider	O
the	O
following	O
problem	O
online	B
convex	B
optimization	I
loss	B
function	B
h	O
z	O
r	O
definitions	O
hypothesis	B
class	I
h	O
domain	B
z	O
assumptions	O
h	O
is	O
convex	B
z	O
z	O
z	O
is	O
a	O
convex	B
function	B
for	O
t	O
t	O
learner	O
predicts	O
a	O
vector	O
wt	O
h	O
environment	O
responds	O
with	O
zt	O
z	O
learner	O
suffers	O
loss	B
zt	O
as	O
in	O
the	O
online	B
classification	O
problem	O
we	O
analyze	O
the	O
regret	O
of	O
the	O
algorithm	O
recall	B
that	O
the	O
regret	O
of	O
an	O
online	B
algorithm	O
with	O
respect	O
to	O
a	O
competing	O
hypothesis	B
which	O
here	O
will	O
be	O
some	O
vector	O
h	O
is	O
defined	O
as	O
zt	O
t	O
zt	O
as	O
before	O
the	O
regret	O
of	O
the	O
algorithm	O
relative	O
to	O
a	O
set	B
of	O
competing	O
vectors	O
h	O
is	O
defined	O
as	O
regretah	O
t	O
sup	O
h	O
t	O
in	O
chapter	O
we	O
have	O
shown	O
that	O
stochastic	O
gradient	B
descent	I
solves	O
convex	B
learning	O
problems	O
in	O
the	O
agnostic	B
pac	B
model	O
we	O
now	O
show	O
that	O
a	O
very	O
similar	O
algorithm	O
online	B
gradient	B
descent	I
solves	O
online	B
convex	B
learning	O
problems	O
online	B
gradient	B
descent	I
parameter	O
initialize	O
for	O
t	O
t	O
predict	O
wt	O
receive	O
zt	O
and	O
let	O
ft	O
zt	O
choose	O
vt	O
ftwt	O
update	O
wt	O
argminw	O
h	O
wt	O
wt	O
vt	O
the	O
online	B
perceptron	B
algorithm	O
theorem	O
the	O
online	B
gradient	B
descent	I
algorithm	O
enjoys	O
the	O
following	O
regret	O
bound	O
for	O
every	O
h	O
t	O
if	O
we	O
further	O
assume	O
that	O
ft	O
is	O
for	O
all	O
t	O
then	O
setting	O
t	O
t	O
if	O
we	O
further	O
assume	O
that	O
h	O
is	O
b-bounded	O
and	O
we	O
set	B
b	O
t	O
then	O
regretah	O
t	O
b	O
t	O
t	O
yields	O
proof	O
the	O
analysis	O
is	O
similar	O
to	O
the	O
analysis	O
of	O
stochastic	O
gradient	B
descent	I
with	O
projections	O
using	O
the	O
projection	B
lemma	I
the	O
definition	O
of	O
wt	O
and	O
the	O
definition	O
of	O
subgradients	O
we	O
have	O
that	O
for	O
every	O
t	O
vt	O
summing	O
over	O
t	O
and	O
observing	O
that	O
the	O
left-hand	O
side	O
is	O
a	O
telescopic	O
sum	O
we	O
obtain	O
that	O
rearranging	O
the	O
inequality	O
and	O
using	O
the	O
fact	O
that	O
we	O
get	O
that	O
this	O
proves	O
the	O
first	O
bound	O
in	O
the	O
theorem	O
the	O
second	O
bound	O
follows	O
from	O
the	O
assumption	O
that	O
ft	O
is	O
which	O
implies	O
that	O
the	O
online	B
perceptron	B
algorithm	O
the	O
perceptron	B
is	O
a	O
classic	O
online	B
learning	I
algorithm	O
for	O
binary	O
classification	O
with	O
the	O
hypothesis	B
class	I
of	O
homogenous	B
halfspaces	O
namely	O
h	O
online	B
learning	I
w	O
rd	O
in	O
section	O
we	O
have	O
presented	O
the	O
batch	O
version	O
of	O
the	O
perceptron	B
which	O
aims	O
to	O
solve	O
the	O
erm	B
problem	O
with	O
respect	O
to	O
h	O
we	O
now	O
present	O
an	O
online	B
version	O
of	O
the	O
perceptron	B
algorithm	O
let	O
x	O
rd	O
y	O
on	O
round	O
t	O
the	O
learner	O
receives	O
a	O
vector	O
xt	O
rd	O
the	O
learner	O
maintains	O
a	O
weight	O
vector	O
wt	O
rd	O
and	O
predicts	O
pt	O
then	O
it	O
receives	O
yt	O
y	O
and	O
pays	O
if	O
pt	O
yt	O
and	O
otherwise	O
the	O
goal	O
of	O
the	O
learner	O
is	O
to	O
make	O
as	O
few	O
prediction	O
mistakes	O
as	O
possible	O
in	O
section	O
we	O
characterized	O
the	O
optimal	O
algorithm	O
and	O
showed	O
that	O
the	O
best	O
achievable	O
mistake	B
bound	I
depends	O
on	O
the	O
littlestone	O
dimension	O
of	O
the	O
class	O
we	O
show	O
later	O
that	O
if	O
d	O
then	O
ldimh	O
which	O
implies	O
that	O
we	O
have	O
no	O
hope	O
of	O
making	O
few	O
prediction	O
mistakes	O
indeed	O
consider	O
the	O
tree	O
for	O
which	O
etc	O
because	O
of	O
the	O
density	O
of	O
the	O
reals	O
this	O
tree	O
is	O
shattered	O
by	O
the	O
subset	O
of	O
h	O
which	O
contains	O
all	O
hypotheses	O
that	O
are	O
parametrized	O
by	O
w	O
of	O
the	O
form	O
w	O
a	O
for	O
a	O
we	O
conclude	O
that	O
indeed	O
ldimh	O
to	O
sidestep	O
this	O
impossibility	O
result	O
the	O
perceptron	B
algorithm	O
relies	O
on	O
the	O
technique	O
of	O
surrogate	O
convex	B
losses	O
section	O
this	O
is	O
also	O
closely	O
related	O
to	O
the	O
notion	O
of	O
margin	B
we	O
studied	O
in	O
chapter	O
a	O
weight	O
vector	O
w	O
makes	O
a	O
mistake	O
on	O
an	O
example	O
y	O
whenever	O
the	O
sign	O
of	O
does	O
not	O
equal	O
y	O
therefore	O
we	O
can	O
write	O
the	O
loss	B
function	B
as	O
follows	O
y	O
on	O
rounds	O
on	O
which	O
the	O
algorithm	O
makes	O
a	O
prediction	O
mistake	O
we	O
shall	O
use	O
the	O
hinge-loss	O
as	O
a	O
surrogate	O
convex	B
loss	B
function	B
ftw	O
the	O
hinge-loss	O
satisfies	O
the	O
two	O
conditions	O
ft	O
is	O
a	O
convex	B
function	B
for	O
all	O
w	O
ftw	O
yt	O
in	O
particular	O
this	O
holds	O
for	O
wt	O
on	O
rounds	O
on	O
which	O
the	O
algorithm	O
is	O
correct	O
we	O
shall	O
define	O
ftw	O
clearly	O
ft	O
is	O
convex	B
in	O
this	O
case	O
as	O
well	O
furthermore	O
ftwt	O
yt	O
remark	O
in	O
section	O
we	O
used	O
the	O
same	O
surrogate	B
loss	B
function	B
for	O
all	O
the	O
examples	O
in	O
the	O
online	B
model	O
we	O
allow	O
the	O
surrogate	O
to	O
depend	O
on	O
the	O
specific	O
round	O
it	O
can	O
even	O
depend	O
on	O
wt	O
our	O
ability	O
to	O
use	O
a	O
round	O
specific	O
surrogate	O
stems	O
from	O
the	O
worst-case	O
type	O
of	O
analysis	O
we	O
employ	O
in	O
online	B
learning	I
let	O
us	O
now	O
run	O
the	O
online	B
gradient	B
descent	I
algorithm	O
on	O
the	O
sequence	O
of	O
functions	O
ft	O
with	O
the	O
hypothesis	B
class	I
being	O
all	O
vectors	O
in	O
rd	O
the	O
projection	B
step	O
is	O
vacuous	O
recall	B
that	O
the	O
algorithm	O
initializes	O
and	O
its	O
update	O
rule	O
is	O
wt	O
vt	O
for	O
some	O
vt	O
ftwt	O
in	O
our	O
case	O
if	O
then	O
ft	O
is	O
the	O
zero	O
the	O
online	B
perceptron	B
algorithm	O
function	B
and	O
we	O
can	O
take	O
vt	O
otherwise	O
it	O
is	O
easy	O
to	O
verify	O
that	O
vt	O
ytxt	O
is	O
in	O
ftwt	O
we	O
therefore	O
obtain	O
the	O
update	O
rule	O
wt	O
wt	O
ytxt	O
if	O
otherwise	O
denote	O
by	O
m	O
the	O
set	B
of	O
rounds	O
in	O
which	O
yt	O
note	O
that	O
on	O
round	O
t	O
the	O
prediction	O
of	O
the	O
perceptron	B
can	O
be	O
rewritten	O
as	O
pt	O
sign	O
yi	O
i	O
mit	O
this	O
form	O
implies	O
that	O
the	O
predictions	O
of	O
the	O
perceptron	B
algorithm	O
and	O
the	O
set	B
m	O
do	O
not	O
depend	O
on	O
the	O
actual	O
value	O
of	O
as	O
long	O
as	O
we	O
have	O
therefore	O
obtained	O
the	O
perceptron	B
algorithm	O
perceptron	B
initialize	O
for	O
t	O
t	O
receive	O
xt	O
predict	O
pt	O
if	O
wt	O
ytxt	O
else	O
wt	O
to	O
analyze	O
the	O
perceptron	B
we	O
rely	O
on	O
the	O
analysis	O
of	O
online	B
gradient	B
descent	I
given	O
in	O
the	O
previous	O
section	O
in	O
our	O
case	O
the	O
subgradient	O
of	O
ft	O
we	O
use	O
in	O
the	O
perceptron	B
is	O
vt	O
yt	O
xt	O
indeed	O
the	O
perceptron	B
s	O
update	O
is	O
wt	O
vt	O
and	O
as	O
discussed	O
before	O
this	O
is	O
equivalent	O
to	O
wt	O
vt	O
for	O
every	O
therefore	O
theorem	O
tells	O
us	O
that	O
ftwt	O
since	O
ftwt	O
is	O
a	O
surrogate	O
for	O
the	O
loss	B
we	O
know	O
ftwt	O
denote	O
r	O
maxt	O
then	O
we	O
obtain	O
setting	O
r	O
and	O
rearranging	O
we	O
obtain	O
this	O
inequality	O
implies	O
online	B
learning	I
theorem	O
suppose	O
that	O
the	O
perceptron	B
algorithm	O
runs	O
on	O
a	O
sequence	O
yt	O
and	O
let	O
r	O
maxt	O
let	O
m	O
be	O
the	O
rounds	O
on	O
which	O
the	O
perceptron	B
errs	O
and	O
let	O
ftw	O
m	O
then	O
for	O
every	O
r	O
t	O
t	O
in	O
particular	O
if	O
there	O
exists	O
such	O
that	O
for	O
all	O
t	O
then	O
proof	O
the	O
theorem	O
follows	O
from	O
equation	O
and	O
the	O
following	O
claim	O
given	O
x	O
b	O
c	O
r	O
the	O
inequality	O
x	O
b	O
c	O
the	O
last	O
claim	O
can	O
be	O
easily	O
derived	O
by	O
analyzing	O
the	O
roots	O
of	O
the	O
convex	B
parabola	O
qy	O
by	O
c	O
x	O
c	O
implies	O
that	O
x	O
c	O
b	O
the	O
last	O
assumption	O
of	O
theorem	O
is	O
called	O
separability	O
with	O
large	O
margin	B
chapter	O
that	O
is	O
there	O
exists	O
that	O
not	O
only	O
satisfies	O
that	O
the	O
point	O
xt	O
lies	O
on	O
the	O
correct	O
side	O
of	O
the	O
halfspace	B
it	O
also	O
guarantees	O
that	O
xt	O
is	O
not	O
too	O
close	O
to	O
the	O
decision	O
boundary	O
more	O
specifically	O
the	O
distance	O
from	O
xt	O
to	O
the	O
decision	O
boundary	O
is	O
at	O
least	O
and	O
the	O
bound	O
becomes	O
when	O
the	O
separability	O
assumption	O
does	O
not	O
hold	O
the	O
bound	O
involves	O
the	O
term	O
which	O
measures	O
how	O
much	O
the	O
separability	O
with	O
margin	B
requirement	O
is	O
violated	O
as	O
a	O
last	O
remark	O
we	O
note	O
that	O
there	O
can	O
be	O
cases	O
in	O
which	O
there	O
exists	O
some	O
that	O
makes	O
zero	O
errors	O
on	O
the	O
sequence	O
but	O
the	O
perceptron	B
will	O
make	O
many	O
errors	O
indeed	O
this	O
is	O
a	O
direct	O
consequence	O
of	O
the	O
fact	O
that	O
ldimh	O
the	O
way	O
we	O
sidestep	O
this	O
impossibility	O
result	O
is	O
by	O
assuming	O
more	O
on	O
the	O
sequence	O
of	O
examples	O
the	O
bound	O
in	O
theorem	O
will	O
be	O
meaningful	O
only	O
if	O
the	O
cumulative	O
surrogate	O
t	O
is	O
not	O
excessively	O
large	O
summary	O
in	O
this	O
chapter	O
we	O
have	O
studied	O
the	O
online	B
learning	I
model	O
many	O
of	O
the	O
results	O
we	O
derived	O
for	O
the	O
pac	B
learning	O
model	O
have	O
an	O
analog	O
in	O
the	O
online	B
model	O
first	O
we	O
have	O
shown	O
that	O
a	O
combinatorial	O
dimension	O
the	O
littlestone	O
dimension	O
characterizes	O
online	B
learnability	O
to	O
show	O
this	O
we	O
introduced	O
the	O
soa	B
algorithm	O
the	O
realizable	O
case	O
and	O
the	O
weighted-majority	B
algorithm	O
the	O
unrealizable	O
case	O
we	O
have	O
also	O
studied	O
online	B
convex	B
optimization	I
and	O
have	O
shown	O
that	O
online	B
gradient	B
descent	I
is	O
a	O
successful	O
online	B
learner	O
whenever	O
the	O
loss	B
function	B
is	O
convex	B
and	O
lipschitz	O
finally	O
we	O
presented	O
the	O
online	B
perceptron	B
algorithm	O
as	O
a	O
combination	O
of	O
online	B
gradient	B
descent	I
and	O
the	O
concept	O
of	O
surrogate	O
convex	B
loss	B
functions	O
bibliographic	O
remarks	O
bibliographic	O
remarks	O
the	O
standard	O
optimal	O
algorithm	O
was	O
derived	O
by	O
the	O
seminal	O
work	O
of	O
littlestone	O
a	O
generalization	O
to	O
the	O
nonrealizable	O
case	O
as	O
well	O
as	O
other	O
variants	O
like	O
margin-based	O
littlestone	O
s	O
dimension	O
were	O
derived	O
in	O
et	O
al	O
characterizations	O
of	O
online	B
learnability	O
beyond	O
classification	O
have	O
been	O
obtained	O
in	O
bartlett	O
rakhlin	O
tewari	O
rakhlin	O
sridharan	O
tewari	O
daniely	O
et	O
al	O
the	O
weighted-majority	B
algorithm	O
is	O
due	O
to	O
warmuth	O
and	O
the	O
term	O
online	B
convex	B
programming	O
was	O
introduced	O
by	O
zinkevich	O
but	O
this	O
setting	O
was	O
introduced	O
some	O
years	O
earlier	O
by	O
gordon	O
the	O
perceptron	B
dates	O
back	O
to	O
rosenblatt	O
an	O
analysis	O
for	O
the	O
realizable	O
case	O
margin	B
assumptions	O
appears	O
in	O
minsky	O
papert	O
freund	O
and	O
schapire	O
schapire	O
presented	O
an	O
analysis	O
for	O
the	O
unrealizable	O
case	O
with	O
a	O
squared-hinge-loss	O
based	O
on	O
a	O
reduction	O
to	O
the	O
realizable	O
case	O
a	O
direct	O
analysis	O
for	O
the	O
unrealizable	O
case	O
with	O
the	O
hinge-loss	O
was	O
given	O
by	O
gentile	O
for	O
additional	O
information	O
we	O
refer	O
the	O
reader	O
to	O
cesa-bianchi	O
lugosi	O
and	O
shalev-shwartz	O
exercises	O
find	O
a	O
hypothesis	B
class	I
h	O
and	O
a	O
sequence	O
of	O
examples	O
on	O
which	O
consistent	B
makes	O
mistakes	O
find	O
a	O
hypothesis	B
class	I
h	O
and	O
a	O
sequence	O
of	O
examples	O
on	O
which	O
the	O
mistake	B
bound	I
of	O
the	O
halving	B
algorithm	O
is	O
tight	O
let	O
d	O
x	O
d	O
and	O
let	O
h	O
j	O
where	O
hjx	O
calculate	O
mhalvingh	O
derive	O
lower	O
and	O
upper	O
bounds	O
on	O
mhalvingh	O
and	O
prove	O
that	O
they	O
are	O
equal	O
the	O
doubling	O
trick	O
in	O
theorem	O
the	O
parameter	O
depends	O
on	O
the	O
time	O
horizon	O
t	O
in	O
this	O
exercise	O
we	O
show	O
how	O
to	O
get	O
rid	O
of	O
this	O
dependence	O
by	O
a	O
simple	O
trick	O
consider	O
an	O
algorithm	O
that	O
enjoys	O
a	O
regret	O
bound	O
of	O
the	O
form	O
t	O
but	O
its	O
parameters	O
require	O
the	O
knowledge	O
of	O
t	O
the	O
doubling	O
trick	O
described	O
in	O
the	O
following	O
enables	O
us	O
to	O
convert	O
such	O
an	O
algorithm	O
into	O
an	O
algorithm	O
that	O
does	O
not	O
need	O
to	O
know	O
the	O
time	O
horizon	O
the	O
idea	O
is	O
to	O
divide	O
the	O
time	O
into	O
periods	O
of	O
increasing	O
size	O
and	O
run	O
the	O
original	O
algorithm	O
on	O
each	O
period	O
the	O
doubling	O
trick	O
input	O
algorithm	O
a	O
whose	O
parameters	O
depend	O
on	O
the	O
time	O
horizon	O
for	O
m	O
run	O
a	O
on	O
the	O
rounds	O
t	O
online	B
learning	I
show	O
that	O
if	O
the	O
regret	O
of	O
a	O
on	O
each	O
period	O
of	O
rounds	O
is	O
at	O
most	O
then	O
the	O
total	O
regret	O
is	O
at	O
most	O
t	O
online-to-batch	O
conversions	O
in	O
this	O
exercise	O
we	O
demonstrate	O
how	O
a	O
successful	O
online	B
learning	I
algorithm	O
can	O
be	O
used	O
to	O
derive	O
a	O
successful	O
pac	B
learner	O
as	O
well	O
consider	O
a	O
pac	B
learning	O
problem	O
for	O
binary	O
classification	O
parameterized	O
by	O
an	O
instance	B
domain	B
x	O
and	O
a	O
hypothesis	B
class	I
h	O
suppose	O
that	O
there	O
exists	O
an	O
online	B
learning	I
algorithm	O
a	O
which	O
enjoys	O
a	O
mistake	B
bound	I
mah	O
consider	O
running	O
this	O
algorithm	O
on	O
a	O
sequence	O
of	O
t	O
examples	O
which	O
are	O
sampled	O
i	O
i	O
d	O
from	O
a	O
distribution	O
d	O
over	O
the	O
instance	B
space	I
x	O
and	O
are	O
labeled	O
by	O
some	O
h	O
suppose	O
that	O
for	O
every	O
round	O
t	O
the	O
prediction	O
of	O
the	O
algorithm	O
is	O
based	O
on	O
a	O
hypothesis	B
ht	O
x	O
show	O
that	O
eldhr	O
mah	O
t	O
where	O
the	O
expectation	O
is	O
over	O
the	O
random	O
choice	O
of	O
the	O
instances	O
as	O
well	O
as	O
a	O
random	O
choice	O
of	O
r	O
according	O
to	O
the	O
uniform	O
distribution	O
over	O
hint	O
use	O
similar	O
arguments	O
to	O
the	O
ones	O
appearing	O
in	O
the	O
proof	O
of	O
theorem	O
clustering	B
clustering	B
is	O
one	O
of	O
the	O
most	O
widely	O
used	O
techniques	O
for	O
exploratory	O
data	O
analysis	O
across	O
all	O
disciplines	O
from	O
social	O
sciences	O
to	O
biology	O
to	O
computer	O
science	O
people	O
try	O
to	O
get	O
a	O
first	O
intuition	O
about	O
their	O
data	O
by	O
identifying	O
meaningful	O
groups	O
among	O
the	O
data	O
points	O
for	O
example	O
computational	O
biologists	O
cluster	O
genes	O
on	O
the	O
basis	O
of	O
similarities	O
in	O
their	O
expression	O
in	O
different	O
experiments	O
retailers	O
cluster	O
customers	O
on	O
the	O
basis	O
of	O
their	O
customer	O
profiles	O
for	O
the	O
purpose	O
of	O
targeted	O
marketing	O
and	O
astronomers	O
cluster	O
stars	O
on	O
the	O
basis	O
of	O
their	O
spacial	O
proximity	O
the	O
first	O
point	O
that	O
one	O
should	O
clarify	O
is	O
naturally	O
what	O
is	O
clustering	B
intuitively	O
clustering	B
is	O
the	O
task	O
of	O
grouping	O
a	O
set	B
of	O
objects	O
such	O
that	O
similar	O
objects	O
end	O
up	O
in	O
the	O
same	O
group	O
and	O
dissimilar	O
objects	O
are	O
separated	O
into	O
different	O
groups	O
clearly	O
this	O
description	O
is	O
quite	O
imprecise	O
and	O
possibly	O
ambiguous	O
quite	O
surprisingly	O
it	O
is	O
not	O
at	O
all	O
clear	O
how	O
to	O
come	O
up	O
with	O
a	O
more	O
rigorous	O
definition	O
there	O
are	O
several	O
sources	O
for	O
this	O
difficulty	O
one	O
basic	O
problem	O
is	O
that	O
the	O
two	O
objectives	O
mentioned	O
in	O
the	O
earlier	O
statement	O
may	O
in	O
many	O
cases	O
contradict	O
each	O
other	O
mathematically	O
speaking	O
similarity	O
proximity	O
is	O
not	O
a	O
transitive	O
relation	O
while	O
cluster	O
sharing	O
is	O
an	O
equivalence	O
relation	O
and	O
in	O
particular	O
it	O
is	O
a	O
transitive	O
relation	O
more	O
concretely	O
it	O
may	O
be	O
the	O
case	O
that	O
there	O
is	O
a	O
long	O
sequence	O
of	O
objects	O
xm	O
such	O
that	O
each	O
xi	O
is	O
very	O
similar	O
to	O
its	O
two	O
neighbors	O
xi	O
and	O
but	O
and	O
xm	O
are	O
very	O
dissimilar	O
if	O
we	O
wish	O
to	O
make	O
sure	O
that	O
whenever	O
two	O
elements	O
are	O
similar	O
they	O
share	O
the	O
same	O
cluster	O
then	O
we	O
must	O
put	O
all	O
of	O
the	O
elements	O
of	O
the	O
sequence	O
in	O
the	O
same	O
cluster	O
however	O
in	O
that	O
case	O
we	O
end	O
up	O
with	O
dissimilar	O
elements	O
and	O
xm	O
sharing	O
a	O
cluster	O
thus	O
violating	O
the	O
second	O
requirement	O
to	O
illustrate	O
this	O
point	O
further	O
suppose	O
that	O
we	O
would	O
like	O
to	O
cluster	O
the	O
points	O
in	O
the	O
following	O
picture	O
into	O
two	O
clusters	O
a	O
clustering	B
algorithm	O
that	O
emphasizes	O
not	O
separating	O
close-by	O
points	O
the	O
single	B
linkage	B
algorithm	O
that	O
will	O
be	O
described	O
in	O
section	O
will	O
cluster	O
this	O
input	O
by	O
separating	O
it	O
horizontally	O
according	O
to	O
the	O
two	O
lines	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
clustering	B
in	O
contrast	O
a	O
clustering	B
method	O
that	O
emphasizes	O
not	O
having	O
far-away	O
points	O
share	O
the	O
same	O
cluster	O
the	O
algorithm	O
that	O
will	O
be	O
described	O
in	O
section	O
will	O
cluster	O
the	O
same	O
input	O
by	O
dividing	O
it	O
vertically	O
into	O
the	O
righthand	O
half	O
and	O
the	O
left-hand	O
half	O
another	O
basic	O
problem	O
is	O
the	O
lack	O
of	O
ground	O
truth	O
for	O
clustering	B
which	O
is	O
a	O
common	O
problem	O
in	O
unsupervised	B
learning	I
so	O
far	O
in	O
the	O
book	O
we	O
have	O
mainly	O
dealt	O
with	O
supervised	O
learning	O
the	O
problem	O
of	O
learning	O
a	O
classifier	B
from	O
labeled	O
training	O
data	O
the	O
goal	O
of	O
supervised	O
learning	O
is	O
clear	O
we	O
wish	O
to	O
learn	O
a	O
classifier	B
which	O
will	O
predict	O
the	O
labels	O
of	O
future	O
examples	O
as	O
accurately	O
as	O
possible	O
furthermore	O
a	O
supervised	O
learner	O
can	O
estimate	O
the	O
success	O
or	O
the	O
risk	B
of	O
its	O
hypotheses	O
using	O
the	O
labeled	O
training	O
data	O
by	O
computing	O
the	O
empirical	O
loss	B
in	O
contrast	O
clustering	B
is	O
an	O
unsupervised	B
learning	I
problem	O
namely	O
there	O
are	O
no	O
labels	O
that	O
we	O
try	O
to	O
predict	O
instead	O
we	O
wish	O
to	O
organize	O
the	O
data	O
in	O
some	O
meaningful	O
way	O
as	O
a	O
result	O
there	O
is	O
no	O
clear	O
success	O
evaluation	O
procedure	O
for	O
clustering	B
in	O
fact	O
even	O
on	O
the	O
basis	O
of	O
full	O
knowledge	O
of	O
the	O
underlying	O
data	O
distribution	O
it	O
is	O
not	O
clear	O
what	O
is	O
the	O
correct	O
clustering	B
for	O
that	O
data	O
or	O
how	O
to	O
evaluate	O
a	O
proposed	O
clustering	B
consider	O
for	O
example	O
the	O
following	O
set	B
of	O
points	O
in	O
and	O
suppose	O
we	O
are	O
required	O
to	O
cluster	O
them	O
into	O
two	O
clusters	O
we	O
have	O
two	O
highly	O
justifiable	O
solutions	O
clustering	B
this	O
phenomenon	O
is	O
not	O
just	O
artificial	O
but	O
occurs	O
in	O
real	O
applications	O
a	O
given	O
set	B
of	O
objects	O
can	O
be	O
clustered	O
in	O
various	O
different	O
meaningful	O
ways	O
this	O
may	O
be	O
due	O
to	O
having	O
different	O
implicit	O
notions	O
of	O
distance	O
similarity	O
between	O
objects	O
for	O
example	O
clustering	B
recordings	O
of	O
speech	O
by	O
the	O
accent	O
of	O
the	O
speaker	O
versus	O
clustering	B
them	O
by	O
content	O
clustering	B
movie	O
reviews	O
by	O
movie	O
topic	O
versus	O
clustering	B
them	O
by	O
the	O
review	O
sentiment	O
clustering	B
paintings	O
by	O
topic	O
versus	O
clustering	B
them	O
by	O
style	O
and	O
so	O
on	O
to	O
summarize	O
there	O
may	O
be	O
several	O
very	O
different	O
conceivable	O
clustering	B
solutions	O
for	O
a	O
given	O
data	O
set	B
as	O
a	O
result	O
there	O
is	O
a	O
wide	O
variety	O
of	O
clustering	B
algorithms	O
that	O
on	O
some	O
input	O
data	O
will	O
output	O
very	O
different	O
clusterings	O
a	O
clustering	B
model	O
clustering	B
tasks	O
can	O
vary	O
in	O
terms	O
of	O
both	O
the	O
type	O
of	O
input	O
they	O
have	O
and	O
the	O
type	O
of	O
outcome	O
they	O
are	O
expected	O
to	O
compute	O
for	O
concreteness	O
we	O
shall	O
focus	O
on	O
the	O
following	O
common	O
setup	O
input	O
a	O
set	B
of	O
elements	O
x	O
and	O
a	O
distance	O
function	B
over	O
it	O
that	O
is	O
a	O
function	B
d	O
x	O
x	O
r	O
that	O
is	O
symmetric	O
satisfies	O
dx	O
x	O
for	O
all	O
x	O
x	O
and	O
often	O
also	O
satisfies	O
the	O
triangle	O
inequality	O
alternatively	O
the	O
function	B
could	O
be	O
a	O
similarity	O
function	B
s	O
x	O
x	O
that	O
is	O
symmetric	O
and	O
satisfies	O
sx	O
x	O
for	O
all	O
x	O
x	O
additionally	O
some	O
clustering	B
algorithms	O
also	O
require	O
an	O
input	O
parameter	O
k	O
the	O
number	O
of	O
required	O
clusters	O
output	O
a	O
partition	O
of	O
the	O
domain	B
set	B
x	O
into	O
subsets	O
that	O
is	O
c	O
ck	O
ci	O
x	O
and	O
for	O
all	O
i	O
j	O
ci	O
cj	O
in	O
some	O
situations	O
the	O
clustering	B
is	O
soft	O
namely	O
the	O
partition	O
of	O
x	O
into	O
the	O
different	O
clusters	O
is	O
probabilistic	O
where	O
the	O
output	O
is	O
a	O
function	B
assigning	O
to	O
each	O
domain	B
point	O
x	O
x	O
a	O
vector	O
pkx	O
where	O
pix	O
px	O
ci	O
is	O
the	O
probability	O
that	O
x	O
belongs	O
to	O
cluster	O
ci	O
another	O
possible	O
output	O
is	O
a	O
clustering	B
dendrogram	B
greek	O
dendron	O
tree	O
gramma	O
drawing	O
which	O
is	O
a	O
hierarchical	O
tree	O
of	O
domain	B
subsets	O
having	O
the	O
singleton	O
sets	O
in	O
its	O
leaves	O
and	O
the	O
full	O
domain	B
as	O
its	O
root	O
we	O
shall	O
discuss	O
this	O
formulation	O
in	O
more	O
detail	O
in	O
the	O
following	O
clustering	B
in	O
the	O
following	O
we	O
survey	O
some	O
of	O
the	O
most	O
popular	O
clustering	B
methods	O
in	O
the	O
last	O
section	O
of	O
this	O
chapter	O
we	O
return	O
to	O
the	O
high	O
level	O
discussion	O
of	O
what	O
is	O
clustering	B
linkage-based	O
clustering	B
algorithms	O
linkage-based	O
clustering	B
is	O
probably	O
the	O
simplest	O
and	O
most	O
straightforward	O
paradigm	O
of	O
clustering	B
these	O
algorithms	O
proceed	O
in	O
a	O
sequence	O
of	O
rounds	O
they	O
start	O
from	O
the	O
trivial	O
clustering	B
that	O
has	O
each	O
data	O
point	O
as	O
a	O
single-point	O
cluster	O
then	O
repeatedly	O
these	O
algorithms	O
merge	O
the	O
closest	O
clusters	O
of	O
the	O
previous	O
clustering	B
consequently	O
the	O
number	O
of	O
clusters	O
decreases	O
with	O
each	O
such	O
round	O
if	O
kept	O
going	O
such	O
algorithms	O
would	O
eventually	O
result	O
in	O
the	O
trivial	O
clustering	B
in	O
which	O
all	O
of	O
the	O
domain	B
points	O
share	O
one	O
large	O
cluster	O
two	O
parameters	O
then	O
need	O
to	O
be	O
determined	O
to	O
define	O
such	O
an	O
algorithm	O
clearly	O
first	O
we	O
have	O
to	O
decide	O
how	O
to	O
measure	O
define	O
the	O
distance	O
between	O
clusters	O
and	O
second	O
we	O
have	O
to	O
determine	O
when	O
to	O
stop	O
merging	O
recall	B
that	O
the	O
input	O
to	O
a	O
clustering	B
algorithm	O
is	O
a	O
between-points	O
distance	O
function	B
d	O
there	O
are	O
many	O
ways	O
of	O
extending	O
d	O
to	O
a	O
measure	O
of	O
distance	O
between	O
domain	B
subsets	O
clusters	O
the	O
most	O
common	O
ways	O
are	O
single	B
linkage	B
clustering	B
in	O
which	O
the	O
between-clusters	O
distance	O
is	O
defined	O
by	O
the	O
minimum	O
distance	O
between	O
members	O
of	O
the	O
two	O
clusters	O
namely	O
da	O
b	O
def	O
mindx	O
y	O
x	O
a	O
y	O
b	O
average	O
linkage	B
clustering	B
in	O
which	O
the	O
distance	O
between	O
two	O
clusters	O
is	O
defined	O
to	O
be	O
the	O
average	O
distance	O
between	O
a	O
point	O
in	O
one	O
of	O
the	O
clusters	O
and	O
a	O
point	O
in	O
the	O
other	O
namely	O
da	O
b	O
def	O
dx	O
y	O
x	O
a	O
y	O
b	O
max	B
linkage	B
clustering	B
in	O
which	O
the	O
distance	O
between	O
two	O
clusters	O
is	O
defined	O
as	O
the	O
maximum	O
distance	O
between	O
their	O
elements	O
namely	O
da	O
b	O
def	O
maxdx	O
y	O
x	O
a	O
y	O
b	O
the	O
linkage-based	O
clustering	B
algorithms	O
are	O
agglomerative	O
in	O
the	O
sense	O
that	O
they	O
start	O
from	O
data	O
that	O
is	O
completely	O
fragmented	O
and	O
keep	O
building	O
larger	O
and	O
larger	O
clusters	O
as	O
they	O
proceed	O
without	O
employing	O
a	O
stopping	O
rule	O
the	O
outcome	O
of	O
such	O
an	O
algorithm	O
can	O
be	O
described	O
by	O
a	O
clustering	B
dendrogram	B
that	O
is	O
a	O
tree	O
of	O
domain	B
subsets	O
having	O
the	O
singleton	O
sets	O
in	O
its	O
leaves	O
and	O
the	O
full	O
domain	B
as	O
its	O
root	O
for	O
example	O
if	O
the	O
input	O
is	O
the	O
elements	O
x	O
b	O
c	O
d	O
e	O
with	O
the	O
euclidean	O
distance	O
as	O
depicted	O
on	O
the	O
left	O
then	O
the	O
resulting	O
dendrogram	B
is	O
the	O
one	O
depicted	O
on	O
the	O
right	O
k-means	B
and	O
other	O
cost	O
minimization	O
clusterings	O
a	O
e	O
d	O
c	O
b	O
b	O
c	O
d	O
e	O
c	O
d	O
e	O
c	O
e	O
the	O
single	B
linkage	B
algorithm	O
is	O
closely	O
related	O
to	O
kruskal	O
s	O
algorithm	O
for	O
finding	O
a	O
minimal	O
spanning	O
tree	O
on	O
a	O
weighted	O
graph	O
indeed	O
consider	O
the	O
full	O
graph	O
whose	O
vertices	O
are	O
elements	O
of	O
x	O
and	O
the	O
weight	O
of	O
an	O
edge	O
y	O
is	O
the	O
distance	O
dx	O
y	O
each	O
merge	O
of	O
two	O
clusters	O
performed	O
by	O
the	O
single	B
linkage	B
algorithm	O
corresponds	O
to	O
a	O
choice	O
of	O
an	O
edge	O
in	O
the	O
aforementioned	O
graph	O
it	O
is	O
also	O
possible	O
to	O
show	O
that	O
the	O
set	B
of	O
edges	O
the	O
single	B
linkage	B
algorithm	O
chooses	O
along	O
its	O
run	O
forms	O
a	O
minimal	O
spanning	O
tree	O
if	O
one	O
wishes	O
to	O
turn	O
a	O
dendrogram	B
into	O
a	O
partition	O
of	O
the	O
space	O
clustering	B
one	O
needs	O
to	O
employ	O
a	O
stopping	O
criterion	O
common	O
stopping	O
criteria	O
include	O
fixed	O
number	O
of	O
clusters	O
fix	O
some	O
parameter	O
k	O
and	O
stop	O
merging	O
clusters	O
as	O
soon	O
as	O
the	O
number	O
of	O
clusters	O
is	O
k	O
distance	O
upper	O
bound	O
fix	O
some	O
r	O
r	O
stop	O
merging	O
as	O
soon	O
as	O
all	O
the	O
between-clusters	O
distances	O
are	O
larger	O
than	O
r	O
we	O
can	O
also	O
set	B
r	O
to	O
be	O
maxdx	O
y	O
x	O
y	O
x	O
for	O
some	O
in	O
that	O
case	O
the	O
stopping	O
criterion	O
is	O
called	O
scaled	O
distance	O
upper	O
bound	O
k-means	B
and	O
other	O
cost	O
minimization	O
clusterings	O
another	O
popular	O
approach	O
to	O
clustering	B
starts	O
by	O
defining	O
a	O
cost	O
function	B
over	O
a	O
parameterized	O
set	B
of	O
possible	O
clusterings	O
and	O
the	O
goal	O
of	O
the	O
clustering	B
algorithm	O
is	O
to	O
find	O
a	O
partitioning	O
of	O
minimal	O
cost	O
under	O
this	O
paradigm	O
the	O
clustering	B
task	O
is	O
turned	O
into	O
an	O
optimization	O
problem	O
the	O
objective	O
function	B
is	O
a	O
function	B
from	O
pairs	O
of	O
an	O
input	O
d	O
and	O
a	O
proposed	O
clustering	B
solution	O
c	O
ck	O
to	O
positive	O
real	O
numbers	O
given	O
such	O
an	O
objective	O
function	B
which	O
we	O
denote	O
by	O
g	O
the	O
goal	O
of	O
a	O
clustering	B
algorithm	O
is	O
defined	O
as	O
finding	O
for	O
a	O
given	O
input	O
d	O
a	O
clustering	B
c	O
so	O
that	O
gx	O
d	O
c	O
is	O
minimized	O
in	O
order	O
to	O
reach	O
that	O
goal	O
one	O
has	O
to	O
apply	O
some	O
appropriate	O
search	O
algorithm	O
as	O
it	O
turns	O
out	O
most	O
of	O
the	O
resulting	O
optimization	O
problems	O
are	O
np-hard	O
and	O
some	O
are	O
even	O
np-hard	O
to	O
approximate	O
consequently	O
when	O
people	O
talk	O
about	O
say	O
k-means	B
clustering	B
they	O
often	O
refer	O
to	O
some	O
particular	O
common	O
approximation	O
algorithm	O
rather	O
than	O
the	O
cost	O
function	B
or	O
the	O
corresponding	O
exact	O
solution	O
of	O
the	O
minimization	O
problem	O
many	O
common	O
objective	O
functions	O
require	O
the	O
number	O
of	O
clusters	O
k	O
as	O
a	O
clustering	B
parameter	O
in	O
practice	O
it	O
is	O
often	O
up	O
to	O
the	O
user	O
of	O
the	O
clustering	B
algorithm	O
to	O
choose	O
the	O
parameter	O
k	O
that	O
is	O
most	O
suitable	O
for	O
the	O
given	O
clustering	B
problem	O
in	O
the	O
following	O
we	O
describe	O
some	O
of	O
the	O
most	O
common	O
objective	O
functions	O
the	O
k-means	B
objective	O
function	B
is	O
one	O
of	O
the	O
most	O
popular	O
clustering	B
objectives	O
in	O
k-means	B
the	O
data	O
is	O
partitioned	O
into	O
disjoint	O
sets	O
ck	O
where	O
each	O
ci	O
is	O
represented	O
by	O
a	O
centroid	O
i	O
it	O
is	O
assumed	O
that	O
the	O
input	O
set	B
x	O
is	O
embedded	O
in	O
some	O
larger	O
metric	O
space	O
d	O
that	O
x	O
x	O
and	O
centroids	O
are	O
members	O
of	O
x	O
the	O
k-means	B
objective	O
function	B
measures	O
the	O
squared	O
distance	O
between	O
each	O
point	O
in	O
x	O
to	O
the	O
centroid	O
of	O
its	O
cluster	O
the	O
centroid	O
of	O
ci	O
is	O
defined	O
to	O
be	O
x	O
ci	O
ici	O
argmin	O
x	O
dx	O
then	O
the	O
k-means	B
objective	O
is	O
gk	O
meansx	O
d	O
ck	O
dx	O
this	O
can	O
also	O
be	O
rewritten	O
as	O
gk	O
meansx	O
d	O
ck	O
min	O
k	O
x	O
dx	O
x	O
ci	O
x	O
ci	O
the	O
k-means	B
objective	O
function	B
is	O
relevant	O
for	O
example	O
in	O
digital	O
communication	O
tasks	O
where	O
the	O
members	O
of	O
x	O
may	O
be	O
viewed	O
as	O
a	O
collection	O
of	O
signals	O
that	O
have	O
to	O
be	O
transmitted	O
while	O
x	O
may	O
be	O
a	O
very	O
large	O
set	B
of	O
real	O
valued	O
vectors	O
digital	O
transmission	O
allows	O
transmitting	O
of	O
only	O
a	O
finite	O
number	O
of	O
bits	O
for	O
each	O
signal	O
one	O
way	O
to	O
achieve	O
good	O
transmission	O
under	O
such	O
constraints	O
is	O
to	O
represent	O
each	O
member	O
of	O
x	O
by	O
a	O
close	O
member	O
of	O
some	O
finite	O
set	B
k	O
and	O
replace	O
the	O
transmission	O
of	O
any	O
x	O
x	O
by	O
transmitting	O
the	O
index	O
of	O
the	O
closest	O
i	O
the	O
k-means	B
objective	O
can	O
be	O
viewed	O
as	O
a	O
measure	O
of	O
the	O
distortion	O
created	O
by	O
such	O
a	O
transmission	O
representation	O
scheme	O
the	O
k-medoids	B
objective	O
function	B
is	O
similar	O
to	O
the	O
k-means	B
objective	O
except	O
that	O
it	O
requires	O
the	O
cluster	O
centroids	O
to	O
be	O
members	O
of	O
the	O
input	O
set	B
the	O
objective	O
function	B
is	O
defined	O
by	O
gk	O
medoidx	O
d	O
ck	O
min	O
k	O
x	O
dx	O
the	O
k-median	B
objective	O
function	B
is	O
quite	O
similar	O
to	O
the	O
k-medoids	B
objective	O
except	O
that	O
the	O
distortion	O
between	O
a	O
data	O
point	O
and	O
the	O
centroid	O
of	O
its	O
cluster	O
is	O
measured	O
by	O
distance	O
rather	O
than	O
by	O
the	O
square	O
of	O
the	O
distance	O
gk	O
medianx	O
d	O
ck	O
min	O
k	O
x	O
dx	O
i	O
x	O
ci	O
x	O
ci	O
k-means	B
and	O
other	O
cost	O
minimization	O
clusterings	O
an	O
example	O
where	O
such	O
an	O
objective	O
makes	O
sense	O
is	O
the	O
facility	O
location	O
problem	O
consider	O
the	O
task	O
of	O
locating	O
k	O
fire	O
stations	O
in	O
a	O
city	O
one	O
can	O
model	O
houses	O
as	O
data	O
points	O
and	O
aim	O
to	O
place	O
the	O
stations	O
so	O
as	O
to	O
minimize	O
the	O
average	O
distance	O
between	O
a	O
house	O
and	O
its	O
closest	O
fire	O
station	O
the	O
previous	O
examples	O
can	O
all	O
be	O
viewed	O
as	O
center-based	O
objectives	O
the	O
solution	O
to	O
such	O
a	O
clustering	B
problem	O
is	O
determined	O
by	O
a	O
set	B
of	O
cluster	O
centers	O
and	O
the	O
clustering	B
assigns	O
each	O
instance	B
to	O
the	O
center	O
closest	O
to	O
it	O
more	O
generally	O
center-based	O
objective	O
is	O
determined	O
by	O
choosing	O
some	O
monotonic	O
function	B
f	O
r	O
r	O
and	O
then	O
defining	O
some	O
objective	O
functions	O
are	O
not	O
center	O
based	O
for	O
example	O
the	O
sum	O
of	O
in	O
gf	O
d	O
ck	O
min	O
k	O
x	O
where	O
x	O
is	O
either	O
x	O
or	O
some	O
superset	O
of	O
x	O
cluster	O
distances	O
gsodx	O
d	O
ck	O
x	O
ci	O
f	O
i	O
xy	O
ci	O
dx	O
y	O
and	O
the	O
mincut	O
objective	O
that	O
we	O
shall	O
discuss	O
in	O
section	O
are	O
not	O
centerbased	O
objectives	O
the	O
k-means	B
algorithm	O
the	O
k-means	B
objective	O
function	B
is	O
quite	O
popular	O
in	O
practical	O
applications	O
of	O
clustering	B
however	O
it	O
turns	O
out	O
that	O
finding	O
the	O
optimal	O
k-means	B
solution	O
is	O
often	O
computationally	O
infeasible	O
problem	O
is	O
np-hard	O
and	O
even	O
np-hard	O
to	O
approximate	O
to	O
within	O
some	O
constant	O
as	O
an	O
alternative	O
the	O
following	O
simple	O
iterative	O
algorithm	O
is	O
often	O
used	O
so	O
often	O
that	O
in	O
many	O
cases	O
the	O
term	O
k-means	B
clustering	B
refers	O
to	O
the	O
outcome	O
of	O
this	O
algorithm	O
rather	O
than	O
to	O
the	O
clustering	B
that	O
minimizes	O
the	O
k-means	B
objective	O
cost	O
we	O
describe	O
the	O
algorithm	O
with	O
respect	O
to	O
the	O
euclidean	O
distance	O
function	B
dx	O
y	O
k-means	B
input	O
x	O
rn	O
number	O
of	O
clusters	O
k	O
initialize	O
randomly	O
choose	O
initial	O
centroids	O
k	O
repeat	O
until	O
convergence	O
i	O
set	B
ci	O
x	O
i	O
argminj	O
ties	O
in	O
some	O
arbitrary	O
manner	O
i	O
update	O
i	O
x	O
ci	O
x	O
lemma	O
each	O
iteration	O
of	O
the	O
k-means	B
algorithm	O
does	O
not	O
increase	O
the	O
k-means	B
objective	O
function	B
given	O
in	O
equation	O
clustering	B
proof	O
to	O
simplify	O
the	O
notation	O
let	O
us	O
use	O
the	O
shorthand	O
ck	O
for	O
the	O
k-means	B
objective	O
namely	O
ck	O
min	O
k	O
rn	O
it	O
is	O
convenient	O
to	O
define	O
therefore	O
we	O
can	O
rewrite	O
the	O
k-means	B
objective	O
as	O
x	O
ci	O
x	O
and	O
note	O
that	O
argmin	O
rn	O
x	O
ci	O
x	O
ci	O
x	O
ci	O
ck	O
x	O
ct	O
i	O
k	O
x	O
ci	O
c	O
consider	O
the	O
update	O
at	O
iteration	O
t	O
of	O
the	O
k-means	B
algorithm	O
let	O
c	O
be	O
the	O
previous	O
partition	O
let	O
k	O
be	O
the	O
new	O
partition	O
assigned	O
at	O
iteration	O
t	O
using	O
the	O
definition	O
of	O
the	O
objective	O
as	O
given	O
in	O
equation	O
we	O
clearly	O
have	O
that	O
and	O
let	O
c	O
c	O
k	O
i	O
i	O
gc	O
c	O
i	O
in	O
addition	O
the	O
definition	O
of	O
the	O
new	O
partition	O
minimizes	O
the	O
i	O
c	O
k	O
implies	O
that	O
it	O
over	O
all	O
possible	O
partitions	O
ck	O
hence	O
x	O
ct	O
i	O
i	O
x	O
ct	O
i	O
i	O
c	O
using	O
equation	O
we	O
have	O
that	O
the	O
right-hand	O
side	O
of	O
equation	O
equals	O
gc	O
combining	O
this	O
with	O
equation	O
and	O
equation	O
we	O
obtain	O
that	O
gc	O
c	O
which	O
concludes	O
our	O
proof	O
k	O
gc	O
c	O
k	O
k	O
while	O
the	O
preceding	O
lemma	O
tells	O
us	O
that	O
the	O
k-means	B
objective	O
is	O
monotonically	O
nonincreasing	O
there	O
is	O
no	O
guarantee	O
on	O
the	O
number	O
of	O
iterations	O
the	O
k-means	B
algorithm	O
needs	O
in	O
order	O
to	O
reach	O
convergence	O
furthermore	O
there	O
is	O
no	O
nontrivial	O
lower	O
bound	O
on	O
the	O
gap	O
between	O
the	O
value	O
of	O
the	O
k-means	B
objective	O
of	O
the	O
algorithm	O
s	O
output	O
and	O
the	O
minimum	O
possible	O
value	O
of	O
that	O
objective	O
function	B
in	O
fact	O
k-means	B
might	O
converge	O
to	O
a	O
point	O
which	O
is	O
not	O
even	O
a	O
local	B
minimum	I
exercise	O
to	O
improve	O
the	O
results	O
of	O
k-means	B
it	O
is	O
often	O
recommended	O
to	O
repeat	O
the	O
procedure	O
several	O
times	O
with	O
different	O
randomly	O
chosen	O
initial	O
centroids	O
we	O
can	O
choose	O
the	O
initial	O
centroids	O
to	O
be	O
random	O
points	O
from	O
the	O
data	O
spectral	B
clustering	B
spectral	B
clustering	B
often	O
a	O
convenient	O
way	O
to	O
represent	O
the	O
relationships	O
between	O
points	O
in	O
a	O
data	O
set	B
x	O
xm	O
is	O
by	O
a	O
similarity	O
graph	O
each	O
vertex	O
represents	O
a	O
data	O
point	O
xi	O
and	O
every	O
two	O
vertices	O
are	O
connected	O
by	O
an	O
edge	O
whose	O
weight	O
is	O
their	O
similarity	O
wij	O
sxi	O
xj	O
where	O
w	O
rmm	O
for	O
example	O
we	O
can	O
set	B
wij	O
exp	O
dxi	O
where	O
d	O
is	O
a	O
distance	O
function	B
and	O
is	O
a	O
parameter	O
the	O
clustering	B
problem	O
can	O
now	O
be	O
formulated	O
as	O
follows	O
we	O
want	O
to	O
find	O
a	O
partition	O
of	O
the	O
graph	O
such	O
that	O
the	O
edges	O
between	O
different	O
groups	O
have	O
low	O
weights	O
and	O
the	O
edges	O
within	O
a	O
group	O
have	O
high	O
weights	O
in	O
the	O
clustering	B
objectives	O
described	O
previously	O
the	O
focus	O
was	O
on	O
one	O
side	O
of	O
our	O
intuitive	O
definition	O
of	O
clustering	B
making	O
sure	O
that	O
points	O
in	O
the	O
same	O
cluster	O
are	O
similar	O
we	O
now	O
present	O
objectives	O
that	O
focus	O
on	O
the	O
other	O
requirement	O
points	O
separated	O
into	O
different	O
clusters	O
should	O
be	O
nonsimilar	O
graph	O
cut	O
given	O
a	O
graph	O
represented	O
by	O
a	O
similarity	O
matrix	O
w	O
the	O
simplest	O
and	O
most	O
direct	O
way	O
to	O
construct	O
a	O
partition	O
of	O
the	O
graph	O
is	O
to	O
solve	O
the	O
mincut	O
problem	O
which	O
chooses	O
a	O
partition	O
ck	O
that	O
minimizes	O
the	O
objective	O
ck	O
wrs	O
r	O
cis	O
ci	O
for	O
k	O
the	O
mincut	O
problem	O
can	O
be	O
solved	O
efficiently	O
however	O
in	O
practice	O
it	O
often	O
does	O
not	O
lead	O
to	O
satisfactory	O
partitions	O
the	O
problem	O
is	O
that	O
in	O
many	O
cases	O
the	O
solution	O
of	O
mincut	O
simply	O
separates	O
one	O
individual	O
vertex	O
from	O
the	O
rest	O
of	O
the	O
graph	O
of	O
course	O
this	O
is	O
not	O
what	O
we	O
want	O
to	O
achieve	O
in	O
clustering	B
as	O
clusters	O
should	O
be	O
reasonably	O
large	O
groups	O
of	O
points	O
several	O
solutions	O
to	O
this	O
problem	O
have	O
been	O
suggested	O
the	O
simplest	O
solution	O
is	O
to	O
normalize	O
the	O
cut	O
and	O
define	O
the	O
normalized	O
mincut	O
objective	O
as	O
follows	O
ck	O
wrs	O
r	O
cis	O
ci	O
the	O
preceding	O
objective	O
assumes	O
smaller	O
values	O
if	O
the	O
clusters	O
are	O
not	O
too	O
small	O
unfortunately	O
introducing	O
this	O
balancing	O
makes	O
the	O
problem	O
computationally	O
hard	O
to	O
solve	O
spectral	B
clustering	B
is	O
a	O
way	O
to	O
relax	O
the	O
problem	O
of	O
minimizing	O
ratiocut	O
graph	O
laplacian	O
and	O
relaxed	O
graph	O
cuts	O
the	O
main	O
mathematical	O
object	O
for	O
spectral	B
clustering	B
is	O
the	O
graph	O
laplacian	O
matrix	O
there	O
are	O
several	O
different	O
definitions	O
of	O
graph	O
laplacian	O
in	O
the	O
literature	O
and	O
in	O
the	O
following	O
we	O
describe	O
one	O
particular	O
definition	O
clustering	B
dii	O
definition	O
graph	O
laplacian	O
the	O
unnormalized	O
graph	O
laplacian	O
is	O
the	O
m	O
m	O
matrix	O
l	O
d	O
w	O
where	O
d	O
is	O
a	O
diagonal	O
matrix	O
with	O
wij	O
the	O
matrix	O
d	O
is	O
called	O
the	O
degree	O
matrix	O
the	O
following	O
lemma	O
underscores	O
the	O
relation	O
between	O
ratiocut	O
and	O
the	O
lapla	O
cian	O
matrix	O
lemma	O
let	O
ck	O
be	O
a	O
clustering	B
and	O
let	O
h	O
rmk	O
be	O
the	O
matrix	O
such	O
that	O
hij	O
cj	O
then	O
the	O
columns	O
of	O
h	O
are	O
orthonormal	O
to	O
each	O
other	O
and	O
ck	O
l	O
h	O
proof	O
let	O
hk	O
be	O
the	O
columns	O
of	O
h	O
the	O
fact	O
that	O
these	O
vectors	O
are	O
orthonormal	O
is	O
immediate	O
from	O
the	O
definition	O
next	O
by	O
standard	O
algebraic	O
mai	O
lhi	O
and	O
that	O
for	O
nipulations	O
it	O
can	O
be	O
shown	O
that	O
l	O
h	O
any	O
vector	O
v	O
we	O
have	O
r	O
vrvswrs	O
s	O
r	O
rs	O
s	O
wrsvr	O
applying	O
this	O
with	O
v	O
hi	O
and	O
noting	O
that	O
is	O
nonzero	O
only	O
if	O
r	O
ci	O
s	O
ci	O
or	O
the	O
other	O
way	O
around	O
we	O
obtain	O
that	O
rs	O
i	O
lhi	O
wrs	O
r	O
cis	O
ci	O
are	O
orthonormal	O
and	O
such	O
that	O
each	O
hij	O
is	O
either	O
or	O
unfortunately	O
therefore	O
to	O
minimize	O
ratiocut	O
we	O
can	O
search	O
for	O
a	O
matrix	O
h	O
whose	O
columns	O
this	O
is	O
an	O
integer	O
programming	O
problem	O
which	O
we	O
cannot	O
solve	O
efficiently	O
instead	O
we	O
relax	O
the	O
latter	O
requirement	O
and	O
simply	O
search	O
an	O
orthonormal	O
matrix	O
h	O
rmk	O
that	O
minimizes	O
l	O
h	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
chapter	O
about	O
pca	B
the	O
proof	O
of	O
theorem	O
the	O
solution	O
to	O
this	O
problem	O
is	O
to	O
set	B
u	O
to	O
be	O
the	O
matrix	O
whose	O
columns	O
are	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
k	O
minimal	O
eigenvalues	O
of	O
l	O
the	O
resulting	O
algorithm	O
is	O
called	O
unnormalized	O
spectral	B
clustering	B
information	B
bottleneck	I
unnormalized	O
spectral	B
clustering	B
unnormalized	O
spectral	B
clustering	B
input	O
w	O
rmm	O
number	O
of	O
clusters	O
k	O
initialize	O
compute	O
the	O
unnormalized	O
graph	O
laplacian	O
l	O
let	O
u	O
rmk	O
be	O
the	O
matrix	O
whose	O
columns	O
are	O
the	O
eigenvectors	O
of	O
l	O
corresponding	O
to	O
the	O
k	O
smallest	O
eigenvalues	O
let	O
vm	O
be	O
the	O
rows	O
of	O
u	O
cluster	O
the	O
points	O
vm	O
using	O
k-means	B
output	O
clusters	O
ck	O
of	O
the	O
k-means	B
algorithm	O
the	O
spectral	B
clustering	B
algorithm	O
starts	O
with	O
finding	O
the	O
matrix	O
h	O
of	O
the	O
k	O
eigenvectors	O
corresponding	O
to	O
the	O
smallest	O
eigenvalues	O
of	O
the	O
graph	O
laplacian	O
matrix	O
it	O
then	O
represents	O
points	O
according	O
to	O
the	O
rows	O
of	O
h	O
it	O
is	O
due	O
to	O
the	O
properties	O
of	O
the	O
graph	O
laplacians	O
that	O
this	O
change	O
of	O
representation	O
is	O
useful	O
in	O
many	O
situations	O
this	O
change	O
of	O
representation	O
enables	O
the	O
simple	O
k-means	B
algorithm	O
to	O
detect	O
the	O
clusters	O
seamlessly	O
intuitively	O
if	O
h	O
is	O
as	O
defined	O
in	O
lemma	O
then	O
each	O
point	O
in	O
the	O
new	O
representation	O
is	O
an	O
indicator	O
vector	O
whose	O
value	O
is	O
nonzero	O
only	O
on	O
the	O
element	O
corresponding	O
to	O
the	O
cluster	O
it	O
belongs	O
to	O
information	B
bottleneck	I
the	O
information	B
bottleneck	I
method	O
is	O
a	O
clustering	B
technique	O
introduced	O
by	O
tishby	O
pereira	O
and	O
bialek	O
it	O
relies	O
on	O
notions	O
from	O
information	O
theory	O
to	O
illustrate	O
the	O
method	O
consider	O
the	O
problem	O
of	O
clustering	B
text	O
documents	O
where	O
each	O
document	O
is	O
represented	O
as	O
a	O
bag-of-words	B
namely	O
each	O
document	O
is	O
a	O
vector	O
x	O
where	O
n	O
is	O
the	O
size	O
of	O
the	O
dictionary	O
and	O
xi	O
iff	O
the	O
word	O
corresponding	O
to	O
index	O
i	O
appears	O
in	O
the	O
document	O
given	O
a	O
set	B
of	O
m	O
documents	O
we	O
can	O
interpret	O
the	O
bag-of-words	B
representation	O
of	O
the	O
m	O
documents	O
as	O
a	O
joint	O
probability	O
over	O
a	O
random	O
variable	O
x	O
indicating	O
the	O
identity	O
of	O
a	O
document	O
taking	O
values	O
in	O
and	O
a	O
random	O
variable	O
y	O
indicating	O
the	O
identity	O
of	O
a	O
word	O
in	O
the	O
dictionary	O
taking	O
values	O
in	O
with	O
this	O
interpretation	O
the	O
information	B
bottleneck	I
refers	O
to	O
the	O
identity	O
of	O
a	O
clustering	B
as	O
another	O
random	O
variable	O
denoted	O
c	O
that	O
takes	O
values	O
in	O
k	O
will	O
be	O
set	B
by	O
the	O
method	O
as	O
well	O
once	O
we	O
have	O
formulated	O
x	O
y	O
c	O
as	O
random	O
variables	O
we	O
can	O
use	O
tools	O
from	O
information	O
theory	O
to	O
express	O
a	O
clustering	B
objective	O
in	O
particular	O
the	O
information	B
bottleneck	I
objective	O
is	O
ix	O
c	O
ic	O
y	O
min	O
pcx	O
where	O
i	O
is	O
the	O
mutual	O
information	O
between	O
two	O
random	O
is	O
a	O
that	O
is	O
given	O
a	O
probability	O
function	B
p	O
over	O
the	O
pairs	O
c	O
clustering	B
parameter	O
and	O
the	O
minimization	O
is	O
over	O
all	O
possible	O
probabilistic	O
assignments	O
of	O
points	O
to	O
clusters	O
intuitively	O
we	O
would	O
like	O
to	O
achieve	O
two	O
contradictory	O
goals	O
on	O
one	O
hand	O
we	O
would	O
like	O
the	O
mutual	O
information	O
between	O
the	O
identity	O
of	O
the	O
document	O
and	O
the	O
identity	O
of	O
the	O
cluster	O
to	O
be	O
as	O
small	O
as	O
possible	O
this	O
reflects	O
the	O
fact	O
that	O
we	O
would	O
like	O
a	O
strong	O
compression	O
of	O
the	O
original	O
data	O
on	O
the	O
other	O
hand	O
we	O
would	O
like	O
high	O
mutual	O
information	O
between	O
the	O
clustering	B
variable	O
and	O
the	O
identity	O
of	O
the	O
words	O
which	O
reflects	O
the	O
goal	O
that	O
the	O
relevant	O
information	O
about	O
the	O
document	O
reflected	O
by	O
the	O
words	O
that	O
appear	O
in	O
the	O
document	O
is	O
retained	O
this	O
generalizes	O
the	O
classical	O
notion	O
of	O
minimal	O
sufficient	O
used	O
in	O
parametric	O
statistics	O
to	O
arbitrary	O
distributions	O
solving	O
the	O
optimization	O
problem	O
associated	O
with	O
the	O
information	B
bottleneck	I
principle	O
is	O
hard	O
in	O
the	O
general	O
case	O
some	O
of	O
the	O
proposed	O
methods	O
are	O
similar	O
to	O
the	O
em	B
principle	O
which	O
we	O
will	O
discuss	O
in	O
chapter	O
a	O
high	O
level	O
view	O
of	O
clustering	B
so	O
far	O
we	O
have	O
mainly	O
listed	O
various	O
useful	O
clustering	B
tools	O
however	O
some	O
fundamental	O
questions	O
remain	O
unaddressed	O
first	O
and	O
foremost	O
what	O
is	O
clustering	B
what	O
is	O
it	O
that	O
distinguishes	O
a	O
clustering	B
algorithm	O
from	O
any	O
arbitrary	O
function	B
that	O
takes	O
an	O
input	O
space	O
and	O
outputs	O
a	O
partition	O
of	O
that	O
space	O
are	O
there	O
any	O
basic	O
properties	O
of	O
clustering	B
that	O
are	O
independent	O
of	O
any	O
specific	O
algorithm	O
or	O
task	O
one	O
method	O
for	O
addressing	O
such	O
questions	O
is	O
via	O
an	O
axiomatic	O
approach	O
there	O
have	O
been	O
several	O
attempts	O
to	O
provide	O
an	O
axiomatic	O
definition	O
of	O
clustering	B
let	O
us	O
demonstrate	O
this	O
approach	O
by	O
presenting	O
the	O
attempt	O
made	O
by	O
kleinberg	O
consider	O
a	O
clustering	B
function	B
f	O
that	O
takes	O
as	O
input	O
any	O
finite	O
domain	B
x	O
with	O
a	O
dissimilarity	O
function	B
d	O
over	O
its	O
pairs	O
and	O
returns	O
a	O
partition	O
of	O
x	O
consider	O
the	O
following	O
three	O
properties	O
of	O
such	O
a	O
function	B
scale	O
invariance	O
for	O
any	O
domain	B
set	B
x	O
dissimilarity	O
function	B
d	O
and	O
any	O
the	O
following	O
should	O
hold	O
f	O
d	O
f	O
d	O
dx	O
y	O
def	O
dx	O
y	O
richness	O
for	O
any	O
finite	O
x	O
and	O
every	O
partition	O
c	O
ck	O
of	O
x	O
nonempty	O
subsets	O
there	O
exists	O
some	O
dissimilarity	O
function	B
d	O
over	O
x	O
such	O
that	O
f	O
d	O
c	O
ix	O
c	O
where	O
the	O
sum	O
is	O
over	O
all	O
values	O
x	O
can	O
take	O
and	O
all	O
pab	O
a	O
b	O
pa	O
b	O
log	O
papb	O
values	O
c	O
can	O
take	O
a	O
sufficient	O
statistic	O
is	O
a	O
function	B
of	O
the	O
data	O
which	O
has	O
the	O
property	O
of	O
sufficiency	O
with	O
respect	O
to	O
a	O
statistical	O
model	O
and	O
its	O
associated	O
unknown	O
parameter	O
meaning	O
that	O
no	O
other	O
statistic	O
which	O
can	O
be	O
calculated	O
from	O
the	O
same	O
sample	O
provides	O
any	O
additional	O
information	O
as	O
to	O
the	O
value	O
of	O
the	O
parameter	O
for	O
example	O
if	O
we	O
assume	O
that	O
a	O
variable	O
is	O
distributed	O
normally	O
with	O
a	O
unit	O
variance	O
and	O
an	O
unknown	O
expectation	O
then	O
the	O
average	O
function	B
is	O
a	O
sufficient	O
statistic	O
a	O
high	O
level	O
view	O
of	O
clustering	B
consistency	B
if	O
d	O
and	O
are	O
dissimilarity	O
functions	O
over	O
x	O
such	O
that	O
for	O
every	O
x	O
y	O
x	O
if	O
x	O
y	O
belong	O
to	O
the	O
same	O
cluster	O
in	O
f	O
d	O
then	O
y	O
dx	O
y	O
and	O
if	O
x	O
y	O
belong	O
to	O
different	O
clusters	O
in	O
f	O
d	O
then	O
y	O
dx	O
y	O
then	O
f	O
d	O
f	O
a	O
moment	O
of	O
reflection	O
reveals	O
that	O
the	O
scale	O
invariance	O
is	O
a	O
very	O
natural	O
requirement	O
it	O
would	O
be	O
odd	O
to	O
have	O
the	O
result	O
of	O
a	O
clustering	B
function	B
depend	O
on	O
the	O
units	O
used	O
to	O
measure	O
between-point	O
distances	O
the	O
richness	O
requirement	O
basically	O
states	O
that	O
the	O
outcome	O
of	O
the	O
clustering	B
function	B
is	O
fully	O
controlled	O
by	O
the	O
function	B
d	O
which	O
is	O
also	O
a	O
very	O
intuitive	O
feature	B
the	O
third	O
requirement	O
consistency	B
is	O
the	O
only	O
requirement	O
that	O
refers	O
to	O
the	O
basic	O
definition	O
of	O
clustering	B
we	O
wish	O
that	O
similar	O
points	O
will	O
be	O
clustered	O
together	O
and	O
that	O
dissimilar	O
points	O
will	O
be	O
separated	O
to	O
different	O
clusters	O
and	O
therefore	O
if	O
points	O
that	O
already	O
share	O
a	O
cluster	O
become	O
more	O
similar	O
and	O
points	O
that	O
are	O
already	O
separated	O
become	O
even	O
less	O
similar	O
to	O
each	O
other	O
the	O
clustering	B
function	B
should	O
have	O
even	O
stronger	O
support	O
of	O
its	O
previous	O
clustering	B
decisions	O
however	O
kleinberg	O
has	O
shown	O
the	O
following	O
impossibility	O
result	O
theorem	O
there	O
exists	O
no	O
function	B
f	O
that	O
satisfies	O
all	O
the	O
three	O
properties	O
scale	O
invariance	O
richness	O
and	O
consistency	B
proof	O
assume	O
by	O
way	O
of	O
contradiction	O
that	O
some	O
f	O
does	O
satisfy	O
all	O
three	O
properties	O
pick	O
some	O
domain	B
set	B
x	O
with	O
at	O
least	O
three	O
points	O
by	O
richness	O
there	O
must	O
be	O
some	O
such	O
that	O
f	O
x	O
x	O
and	O
there	O
also	O
exists	O
some	O
such	O
that	O
f	O
f	O
let	O
r	O
be	O
such	O
that	O
for	O
every	O
x	O
y	O
x	O
y	O
y	O
let	O
consider	O
f	O
by	O
the	O
scale	O
invariance	O
property	O
of	O
f	O
we	O
should	O
have	O
f	O
f	O
on	O
the	O
other	O
hand	O
since	O
all	O
distinct	O
x	O
y	O
x	O
reside	O
in	O
different	O
clusters	O
w	O
r	O
t	O
f	O
and	O
y	O
y	O
the	O
consistency	B
of	O
f	O
implies	O
that	O
f	O
f	O
this	O
is	O
a	O
contradiction	O
since	O
we	O
chose	O
so	O
that	O
f	O
f	O
it	O
is	O
important	O
to	O
note	O
that	O
there	O
is	O
no	O
single	O
bad	O
property	O
among	O
the	O
three	O
properties	O
for	O
every	O
pair	O
of	O
the	O
the	O
three	O
axioms	O
there	O
exist	O
natural	O
clustering	B
functions	O
that	O
satisfy	O
the	O
two	O
properties	O
in	O
that	O
pair	O
can	O
even	O
construct	O
such	O
examples	O
just	O
by	O
varying	O
the	O
stopping	O
criteria	O
for	O
the	O
single	B
linkage	B
clustering	B
function	B
on	O
the	O
other	O
hand	O
kleinberg	O
shows	O
that	O
any	O
clustering	B
algorithm	O
that	O
minimizes	O
any	O
center-based	O
objective	O
function	B
inevitably	O
fails	O
the	O
consistency	B
property	O
the	O
k-sum-of-in-cluster-distances	O
minimization	O
clustering	B
does	O
satisfy	O
consistency	B
the	O
kleinberg	O
impossibility	O
result	O
can	O
be	O
easily	O
circumvented	O
by	O
varying	O
the	O
properties	O
for	O
example	O
if	O
one	O
wishes	O
to	O
discuss	O
clustering	B
functions	O
that	O
have	O
a	O
fixed	O
number-of-clusters	O
parameter	O
then	O
it	O
is	O
natural	O
to	O
replace	O
richness	O
by	O
k-richness	O
the	O
requirement	O
that	O
every	O
partition	O
of	O
the	O
domain	B
into	O
k	O
subsets	O
is	O
attainable	O
by	O
the	O
clustering	B
function	B
k-richness	O
scale	O
invariance	O
and	O
consistency	B
all	O
hold	O
for	O
the	O
k-means	B
clustering	B
and	O
are	O
therefore	O
consistent	B
clustering	B
j	O
or	O
alternatively	O
one	O
can	O
relax	O
the	O
consistency	B
property	O
for	O
example	O
say	O
that	O
two	O
clusterings	O
c	O
ck	O
and	O
l	O
are	O
compatible	O
if	O
for	O
every	O
clusters	O
ci	O
c	O
and	O
j	O
either	O
ci	O
j	O
ci	O
or	O
ci	O
j	O
is	O
worthwhile	O
noting	O
that	O
for	O
every	O
dendrogram	B
every	O
two	O
clusterings	O
that	O
are	O
obtained	O
by	O
trimming	O
that	O
dendrogram	B
are	O
compatible	O
refinement	O
consistency	B
is	O
the	O
requirement	O
that	O
under	O
the	O
assumptions	O
of	O
the	O
consistency	B
property	O
the	O
new	O
clustering	B
f	O
is	O
compatible	O
with	O
the	O
old	O
clustering	B
f	O
d	O
many	O
common	O
clustering	B
functions	O
satisfy	O
this	O
requirement	O
as	O
well	O
as	O
scale	O
invariance	O
and	O
richness	O
furthermore	O
one	O
can	O
come	O
up	O
with	O
many	O
other	O
different	O
properties	O
of	O
clustering	B
functions	O
that	O
sound	O
intuitive	O
and	O
desirable	O
and	O
are	O
satisfied	O
by	O
some	O
common	O
clustering	B
functions	O
there	O
are	O
many	O
ways	O
to	O
interpret	O
these	O
results	O
we	O
suggest	O
to	O
view	O
it	O
as	O
indicating	O
that	O
there	O
is	O
no	O
ideal	O
clustering	B
function	B
every	O
clustering	B
function	B
will	O
inevitably	O
have	O
some	O
undesirable	O
properties	O
the	O
choice	O
of	O
a	O
clustering	B
function	B
for	O
any	O
given	O
task	O
must	O
therefore	O
take	O
into	O
account	O
the	O
specific	O
properties	O
of	O
that	O
task	O
there	O
is	O
no	O
generic	O
clustering	B
solution	O
just	O
as	O
there	O
is	O
no	O
classification	O
algorithm	O
that	O
will	O
learn	O
every	O
learnable	O
task	O
the	O
no-free-lunch	B
theorem	O
shows	O
clustering	B
just	O
like	O
classification	O
prediction	O
must	O
take	O
into	O
account	O
some	O
prior	B
knowledge	I
about	O
the	O
specific	O
task	O
at	O
hand	O
summary	O
clustering	B
is	O
an	O
unsupervised	B
learning	I
problem	O
in	O
which	O
we	O
wish	O
to	O
partition	O
a	O
set	B
of	O
points	O
into	O
meaningful	O
subsets	O
we	O
presented	O
several	O
clustering	B
approaches	O
including	O
linkage-based	O
algorithms	O
the	O
k-means	B
family	O
spectral	B
clustering	B
and	O
the	O
information	B
bottleneck	I
we	O
discussed	O
the	O
difficulty	O
of	O
formalizing	O
the	O
intuitive	O
meaning	O
of	O
clustering	B
bibliographic	O
remarks	O
the	O
k-means	B
algorithm	O
is	O
sometimes	O
named	O
lloyd	O
s	O
algorithm	O
after	O
stuart	O
lloyd	O
who	O
proposed	O
the	O
method	O
in	O
for	O
a	O
more	O
complete	O
overview	O
of	O
spectral	B
clustering	B
we	O
refer	O
the	O
reader	O
to	O
the	O
excellent	O
tutorial	O
by	O
von	O
luxburg	O
the	O
information	B
bottleneck	I
method	O
was	O
introduced	O
by	O
tishby	O
pereira	O
bialek	O
for	O
an	O
additional	O
discussion	O
on	O
the	O
axiomatic	O
approach	O
see	O
ackerman	O
ben-david	O
exercises	O
suboptimality	O
of	O
k-means	B
for	O
every	O
parameter	O
t	O
show	O
that	O
there	O
exists	O
an	O
instance	B
of	O
the	O
k-means	B
problem	O
for	O
which	O
the	O
k-means	B
algorithm	O
exercises	O
find	O
a	O
solution	O
whose	O
k-means	B
objective	O
is	O
at	O
least	O
t	O
opt	O
where	O
opt	O
is	O
the	O
minimum	O
k-means	B
objective	O
k-means	B
might	O
not	O
necessarily	O
converge	O
to	O
a	O
local	B
minimum	I
show	O
that	O
the	O
k-means	B
algorithm	O
might	O
converge	O
to	O
a	O
point	O
which	O
is	O
not	O
a	O
local	B
minimum	I
hint	O
suppose	O
that	O
k	O
and	O
the	O
sample	O
points	O
are	O
r	O
suppose	O
we	O
initialize	O
the	O
k-means	B
with	O
the	O
centers	O
and	O
suppose	O
we	O
break	O
ties	O
in	O
the	O
definition	O
of	O
ci	O
by	O
assigning	O
i	O
to	O
be	O
the	O
smallest	O
value	O
in	O
argminj	O
given	O
a	O
metric	O
space	O
d	O
where	O
and	O
k	O
n	O
we	O
would	O
like	O
to	O
find	O
a	O
partition	O
of	O
x	O
into	O
ck	O
which	O
minimizes	O
the	O
expression	O
gk	O
diamx	O
d	O
ck	O
max	O
j	O
diamcj	O
where	O
diamcj	O
cj	O
dx	O
use	O
the	O
convention	O
diamcj	O
if	O
similarly	O
to	O
the	O
k-means	B
objective	O
it	O
is	O
np-hard	O
to	O
minimize	O
the	O
kdiam	O
objective	O
fortunately	O
we	O
have	O
a	O
very	O
simple	O
approximation	O
algorithm	O
initially	O
we	O
pick	O
some	O
x	O
x	O
and	O
set	B
x	O
then	O
the	O
algorithm	O
iteratively	O
sets	O
j	O
k	O
j	O
argmax	O
x	O
x	O
min	O
i	O
dx	O
i	O
finally	O
we	O
set	B
i	O
ci	O
x	O
i	O
argmin	O
j	O
dx	O
j	O
prove	O
that	O
the	O
algorithm	O
described	O
is	O
a	O
algorithm	O
that	O
is	O
if	O
we	O
denote	O
its	O
output	O
by	O
ck	O
and	O
denote	O
the	O
optimal	O
solution	O
by	O
c	O
c	O
k	O
then	O
gk	O
diamx	O
d	O
ck	O
gk	O
diamx	O
d	O
c	O
k	O
hint	O
consider	O
the	O
point	O
other	O
words	O
the	O
next	O
center	O
we	O
would	O
have	O
chosen	O
if	O
we	O
wanted	O
k	O
clusters	O
let	O
r	O
minj	O
d	O
j	O
prove	O
the	O
following	O
inequalities	O
gk	O
diamx	O
d	O
ck	O
k	O
r	O
gk	O
diamx	O
d	O
c	O
recall	B
that	O
a	O
clustering	B
function	B
f	O
is	O
called	O
center-based	O
clustering	B
if	O
for	O
some	O
monotonic	O
function	B
f	O
r	O
r	O
on	O
every	O
given	O
input	O
d	O
f	O
d	O
is	O
a	O
clustering	B
that	O
minimizes	O
the	O
objective	O
gf	O
d	O
ck	O
min	O
k	O
x	O
where	O
x	O
is	O
either	O
x	O
or	O
some	O
superset	O
of	O
x	O
f	O
i	O
x	O
ci	O
clustering	B
prove	O
that	O
for	O
every	O
k	O
the	O
k-diam	O
clustering	B
function	B
defined	O
in	O
the	O
previous	O
exercise	O
is	O
not	O
a	O
center-based	O
clustering	B
function	B
hint	O
given	O
a	O
clustering	B
input	O
d	O
with	O
consider	O
the	O
effect	O
of	O
adding	O
many	O
close-by	O
points	O
to	O
some	O
not	O
all	O
of	O
the	O
members	O
of	O
x	O
on	O
either	O
the	O
k-diam	O
clustering	B
or	O
any	O
given	O
center-based	O
clustering	B
recall	B
that	O
we	O
discussed	O
three	O
clustering	B
properties	O
scale	O
invariance	O
rich	O
ness	O
and	O
consistency	B
consider	O
the	O
single	B
linkage	B
clustering	B
algorithm	O
find	O
which	O
of	O
the	O
three	O
properties	O
is	O
satisfied	O
by	O
single	B
linkage	B
with	O
the	O
fixed	O
number	O
of	O
clusters	O
fixed	O
nonzero	O
number	O
stopping	O
rule	O
find	O
which	O
of	O
the	O
three	O
properties	O
is	O
satisfied	O
by	O
single	B
linkage	B
with	O
the	O
distance	O
upper	O
bound	O
fixed	O
nonzero	O
upper	O
bound	O
stopping	O
rule	O
show	O
that	O
for	O
any	O
pair	O
of	O
these	O
properties	O
there	O
exists	O
a	O
stopping	O
criterion	O
for	O
single	B
linkage	B
clustering	B
under	O
which	O
these	O
two	O
axioms	O
are	O
satisfied	O
given	O
some	O
number	O
k	O
let	O
k-richness	O
be	O
the	O
following	O
requirement	O
for	O
any	O
finite	O
x	O
and	O
every	O
partition	O
c	O
ck	O
of	O
x	O
nonempty	O
subsets	O
there	O
exists	O
some	O
dissimilarity	O
function	B
d	O
over	O
x	O
such	O
that	O
f	O
d	O
c	O
prove	O
that	O
for	O
every	O
number	O
k	O
there	O
exists	O
a	O
clustering	B
function	B
that	O
satisfies	O
the	O
three	O
properties	O
scale	O
invariance	O
k-richness	O
and	O
consistency	B
dimensionality	B
reduction	I
dimensionality	B
reduction	I
is	O
the	O
process	O
of	O
taking	O
data	O
in	O
a	O
high	O
dimensional	O
space	O
and	O
mapping	O
it	O
into	O
a	O
new	O
space	O
whose	O
dimensionality	O
is	O
much	O
smaller	O
this	O
process	O
is	O
closely	O
related	O
to	O
the	O
concept	O
of	O
compression	O
in	O
information	O
theory	O
there	O
are	O
several	O
reasons	O
to	O
reduce	O
the	O
dimensionality	O
of	O
the	O
data	O
first	O
high	O
dimensional	O
data	O
impose	O
computational	O
challenges	O
moreover	O
in	O
some	O
situations	O
high	O
dimensionality	O
might	O
lead	O
to	O
poor	O
generalization	O
abilities	O
of	O
the	O
learning	O
algorithm	O
example	O
in	O
nearest	B
neighbor	I
classifiers	O
the	O
sample	B
complexity	I
increases	O
exponentially	O
with	O
the	O
dimension	O
see	O
chapter	O
finally	O
dimensionality	B
reduction	I
can	O
be	O
used	O
for	O
interpretability	O
of	O
the	O
data	O
for	O
finding	O
meaningful	O
structure	O
of	O
the	O
data	O
and	O
for	O
illustration	O
purposes	O
in	O
this	O
chapter	O
we	O
describe	O
popular	O
methods	O
for	O
dimensionality	B
reduction	I
in	O
those	O
methods	O
the	O
reduction	O
is	O
performed	O
by	O
applying	O
a	O
linear	O
transformation	O
to	O
the	O
original	O
data	O
that	O
is	O
if	O
the	O
original	O
data	O
is	O
in	O
rd	O
and	O
we	O
want	O
to	O
embed	O
it	O
into	O
rn	O
d	O
then	O
we	O
would	O
like	O
to	O
find	O
a	O
matrix	O
w	O
rnd	O
that	O
induces	O
the	O
mapping	O
x	O
w	O
x	O
a	O
natural	O
criterion	O
for	O
choosing	O
w	O
is	O
in	O
a	O
way	O
that	O
will	O
enable	O
a	O
reasonable	O
recovery	O
of	O
the	O
original	O
x	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
in	O
general	O
exact	O
recovery	O
of	O
x	O
from	O
w	O
x	O
is	O
impossible	O
exercise	O
the	O
first	O
method	O
we	O
describe	O
is	O
called	O
principal	O
component	O
analysis	O
in	O
pca	B
both	O
the	O
compression	O
and	O
the	O
recovery	O
are	O
performed	O
by	O
linear	O
transformations	O
and	O
the	O
method	O
finds	O
the	O
linear	O
transformations	O
for	O
which	O
the	O
differences	O
between	O
the	O
recovered	O
vectors	O
and	O
the	O
original	O
vectors	O
are	O
minimal	O
in	O
the	O
least	O
squared	O
sense	O
next	O
we	O
describe	O
dimensionality	B
reduction	I
using	O
random	O
matrices	O
w	O
we	O
derive	O
an	O
important	O
lemma	O
often	O
called	O
the	O
johnson-lindenstrauss	B
lemma	I
which	O
analyzes	O
the	O
distortion	O
caused	O
by	O
such	O
a	O
random	O
dimensionality	B
reduction	I
technique	O
last	O
we	O
show	O
how	O
one	O
can	O
reduce	O
the	O
dimension	O
of	O
all	O
sparse	O
vectors	O
using	O
again	O
a	O
random	O
matrix	O
this	O
process	O
is	O
known	O
as	O
compressed	B
sensing	I
in	O
this	O
case	O
the	O
recovery	O
process	O
is	O
nonlinear	O
but	O
can	O
still	O
be	O
implemented	O
efficiently	O
using	O
linear	B
programming	I
we	O
conclude	O
by	O
underscoring	O
the	O
underlying	O
prior	O
assumptions	O
behind	O
pca	B
and	O
compressed	B
sensing	I
which	O
can	O
help	O
us	O
understand	O
the	O
merits	O
and	O
pitfalls	O
of	O
the	O
two	O
methods	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
dimensionality	B
reduction	I
principal	O
component	O
analysis	O
let	O
xm	O
be	O
m	O
vectors	O
in	O
rd	O
we	O
would	O
like	O
to	O
reduce	O
the	O
dimensionality	O
of	O
these	O
vectors	O
using	O
a	O
linear	O
transformation	O
a	O
matrix	O
w	O
rnd	O
where	O
n	O
d	O
induces	O
a	O
mapping	O
x	O
w	O
x	O
where	O
w	O
x	O
rn	O
is	O
the	O
lower	O
dimensionality	O
representation	O
of	O
x	O
then	O
a	O
second	O
matrix	O
u	O
rdn	O
can	O
be	O
used	O
to	O
recover	O
each	O
original	O
vector	O
x	O
from	O
its	O
compressed	O
version	O
that	O
is	O
for	O
a	O
compressed	O
vector	O
y	O
w	O
x	O
where	O
y	O
is	O
in	O
the	O
low	O
dimensional	O
space	O
rn	O
we	O
can	O
construct	O
x	O
u	O
y	O
so	O
that	O
x	O
is	O
the	O
recovered	O
version	O
of	O
x	O
and	O
resides	O
in	O
the	O
original	O
high	O
dimensional	O
space	O
rd	O
in	O
pca	B
we	O
find	O
the	O
compression	O
matrix	O
w	O
and	O
the	O
recovering	O
matrix	O
u	O
so	O
that	O
the	O
total	O
squared	O
distance	O
between	O
the	O
original	O
and	O
recovered	O
vectors	O
is	O
minimal	O
namely	O
we	O
aim	O
at	O
solving	O
the	O
problem	O
argmin	O
w	O
rndu	O
rdn	O
u	O
w	O
to	O
solve	O
this	O
problem	O
we	O
first	O
show	O
that	O
the	O
optimal	O
solution	O
takes	O
a	O
specific	O
form	O
lemma	O
let	O
w	O
be	O
a	O
solution	O
to	O
equation	O
then	O
the	O
columns	O
of	O
u	O
are	O
orthonormal	O
is	O
the	O
identity	O
matrix	O
of	O
rn	O
and	O
w	O
proof	O
fix	O
any	O
u	O
w	O
and	O
consider	O
the	O
mapping	O
x	O
u	O
w	O
x	O
the	O
range	O
of	O
this	O
mapping	O
r	O
w	O
x	O
x	O
rd	O
is	O
an	O
n	O
dimensional	O
linear	O
subspace	O
of	O
rd	O
let	O
v	O
rdn	O
be	O
a	O
matrix	O
whose	O
columns	O
form	O
an	O
orthonormal	O
basis	O
of	O
this	O
subspace	O
namely	O
the	O
range	O
of	O
v	O
is	O
r	O
and	O
v	O
i	O
therefore	O
each	O
vector	O
in	O
r	O
can	O
be	O
written	O
as	O
v	O
y	O
where	O
y	O
rn	O
for	O
every	O
x	O
rd	O
and	O
y	O
rn	O
we	O
have	O
v	O
y	O
where	O
we	O
used	O
the	O
fact	O
that	O
v	O
is	O
the	O
identity	O
matrix	O
of	O
rn	O
minimizing	O
the	O
preceding	O
expression	O
with	O
respect	O
to	O
y	O
by	O
comparing	O
the	O
gradient	B
with	O
respect	O
to	O
y	O
to	O
zero	O
gives	O
that	O
y	O
v	O
therefore	O
for	O
each	O
x	O
we	O
have	O
that	O
v	O
v	O
argmin	O
x	O
r	O
in	O
particular	O
this	O
holds	O
for	O
xm	O
and	O
therefore	O
we	O
can	O
replace	O
u	O
w	O
by	O
v	O
v	O
and	O
by	O
that	O
do	O
not	O
increase	O
the	O
objective	O
u	O
w	O
v	O
v	O
since	O
this	O
holds	O
for	O
every	O
u	O
w	O
the	O
proof	O
of	O
the	O
lemma	O
follows	O
on	O
the	O
basis	O
of	O
the	O
preceding	O
lemma	O
we	O
can	O
rewrite	O
the	O
optimization	O
problem	O
given	O
in	O
equation	O
as	O
follows	O
argmin	O
u	O
u	O
principal	O
component	O
analysis	O
we	O
further	O
simplify	O
the	O
optimization	O
problem	O
by	O
using	O
the	O
following	O
elementary	O
algebraic	O
manipulations	O
for	O
every	O
x	O
rd	O
and	O
a	O
matrix	O
u	O
rdn	O
such	O
that	O
i	O
we	O
have	O
uu	O
where	O
the	O
trace	O
of	O
a	O
matrix	O
is	O
the	O
sum	O
of	O
its	O
diagonal	O
entries	O
since	O
the	O
trace	O
is	O
a	O
linear	O
operator	O
this	O
allows	O
us	O
to	O
rewrite	O
equation	O
as	O
follows	O
i	O
u	O
let	O
a	O
argmax	O
u	O
trace	O
i	O
the	O
matrix	O
a	O
is	O
symmetric	O
and	O
therefore	O
it	O
can	O
be	O
written	O
using	O
its	O
spectral	B
decomposition	O
as	O
a	O
vdv	O
where	O
d	O
is	O
diagonal	O
and	O
v	O
vv	O
i	O
here	O
the	O
elements	O
on	O
the	O
diagonal	O
of	O
d	O
are	O
the	O
eigenvalues	O
of	O
a	O
and	O
the	O
columns	O
of	O
v	O
are	O
the	O
corresponding	O
eigenvectors	O
we	O
assume	O
without	O
loss	B
of	O
generality	O
that	O
ddd	O
since	O
a	O
is	O
positive	O
semidefinite	O
it	O
also	O
holds	O
that	O
ddd	O
we	O
claim	O
that	O
the	O
solution	O
to	O
equation	O
is	O
the	O
matrix	O
u	O
whose	O
columns	O
are	O
the	O
n	O
eigenvectors	O
of	O
a	O
corresponding	O
to	O
the	O
largest	O
n	O
eigenvalues	O
theorem	O
let	O
xm	O
be	O
arbitrary	O
vectors	O
in	O
rd	O
let	O
a	O
i	O
and	O
let	O
un	O
be	O
n	O
eigenvectors	O
of	O
the	O
matrix	O
a	O
corresponding	O
to	O
the	O
largest	O
n	O
eigenvalues	O
of	O
a	O
then	O
the	O
solution	O
to	O
the	O
pca	B
optimization	O
problem	O
given	O
in	O
equation	O
is	O
to	O
set	B
u	O
to	O
be	O
the	O
matrix	O
whose	O
columns	O
are	O
un	O
and	O
to	O
set	B
w	O
proof	O
let	O
vdv	O
be	O
the	O
spectral	B
decomposition	O
of	O
a	O
fix	O
some	O
matrix	O
u	O
rdn	O
with	O
orthonormal	O
columns	O
and	O
let	O
b	O
v	O
then	O
vb	O
vv	O
u	O
it	O
follows	O
that	O
and	O
therefore	O
also	O
orthonormal	O
which	O
implies	O
addition	O
b	O
i	O
then	O
for	O
every	O
j	O
we	O
note	O
that	O
i	O
therefore	O
the	O
columns	O
of	O
b	O
are	O
ji	O
n	O
in	O
addition	O
let	O
b	O
rdd	O
be	O
a	O
matrix	O
such	O
that	O
its	O
first	O
n	O
columns	O
are	O
the	O
columns	O
of	O
b	O
and	O
in	O
ji	O
which	O
implies	O
that	O
ji	O
it	O
follows	O
that	O
djj	O
ji	O
max	O
n	O
djj	O
j	O
dimensionality	B
reduction	I
thonormal	O
columns	O
it	O
holds	O
that	O
it	O
is	O
not	O
hard	O
to	O
verify	O
exercise	O
that	O
the	O
right-hand	O
side	O
equals	O
to	O
djj	O
we	O
have	O
therefore	O
shown	O
that	O
for	O
every	O
matrix	O
u	O
rdn	O
with	O
orwe	O
obtain	O
that	O
djj	O
on	O
the	O
other	O
hand	O
if	O
we	O
set	B
u	O
to	O
be	O
the	O
matrix	O
whose	O
columns	O
are	O
the	O
n	O
leading	O
eigenvectors	O
of	O
a	O
objective	O
of	O
equation	O
and	O
noting	O
objective	O
value	O
of	O
equation	O
remark	O
the	O
proof	O
of	O
theorem	O
also	O
tells	O
us	O
that	O
the	O
value	O
of	O
the	O
dii	O
combining	O
this	O
with	O
equation	O
dii	O
we	O
obtain	O
that	O
the	O
optimal	O
djj	O
and	O
this	O
concludes	O
our	O
proof	O
dii	O
tracea	O
remark	O
it	O
is	O
a	O
common	O
practice	O
to	O
center	O
the	O
examples	O
before	O
applying	O
pca	B
that	O
is	O
we	O
first	O
calculate	O
xi	O
and	O
then	O
apply	O
pca	B
on	O
the	O
vectors	O
this	O
is	O
also	O
related	O
to	O
the	O
interpretation	O
of	O
pca	B
m	O
as	O
variance	O
maximization	O
exercise	O
a	O
more	O
efficient	O
solution	O
for	O
the	O
case	O
d	O
m	O
in	O
some	O
situations	O
the	O
original	O
dimensionality	O
of	O
the	O
data	O
is	O
much	O
larger	O
than	O
the	O
number	O
of	O
examples	O
m	O
the	O
computational	B
complexity	I
of	O
calculating	O
the	O
pca	B
solution	O
as	O
described	O
previously	O
is	O
calculating	O
eigenvalues	O
of	O
a	O
plus	O
constructing	O
the	O
matrix	O
a	O
we	O
now	O
show	O
a	O
simple	O
trick	O
that	O
enables	O
us	O
to	O
calculate	O
the	O
pca	B
solution	O
more	O
efficiently	O
when	O
d	O
m	O
recall	B
that	O
the	O
matrix	O
a	O
is	O
defined	O
to	O
i	O
it	O
is	O
convenient	O
to	O
rewrite	O
a	O
where	O
x	O
rmd	O
is	O
a	O
matrix	O
whose	O
ith	O
row	O
is	O
i	O
consider	O
the	O
matrix	O
b	O
that	O
is	O
b	O
rmm	O
is	O
the	O
matrix	O
whose	O
i	O
j	O
element	O
equals	O
suppose	O
that	O
u	O
is	O
an	O
eigenvector	O
of	O
b	O
that	O
is	O
bu	O
u	O
for	O
some	O
r	O
multiplying	O
the	O
equality	O
by	O
and	O
using	O
the	O
definition	O
of	O
b	O
we	O
obtain	O
but	O
using	O
the	O
definition	O
of	O
a	O
we	O
get	O
that	O
thus	O
is	O
an	O
eigenvector	O
of	O
a	O
with	O
eigenvalue	O
of	O
we	O
can	O
therefore	O
calculate	O
the	O
pca	B
solution	O
by	O
calculating	O
the	O
eigenvalues	O
of	O
b	O
instead	O
of	O
a	O
the	O
complexity	O
is	O
calculating	O
eigenvalues	O
of	O
b	O
and	O
constructing	O
the	O
matrix	O
b	O
remark	O
the	O
previous	O
discussion	O
also	O
implies	O
that	O
to	O
calculate	O
the	O
pca	B
solution	O
we	O
only	O
need	O
to	O
know	O
how	O
to	O
calculate	O
inner	O
products	O
between	O
vectors	O
this	O
enables	O
us	O
to	O
calculate	O
pca	B
implicitly	O
even	O
when	O
d	O
is	O
very	O
large	O
even	O
infinite	O
using	O
kernels	B
which	O
yields	O
the	O
kernel	B
pca	B
algorithm	O
implementation	O
and	O
demonstration	O
a	O
pseudocode	O
of	O
pca	B
is	O
given	O
in	O
the	O
following	O
principal	O
component	O
analysis	O
figure	O
a	O
set	B
of	O
vectors	O
in	O
x	O
s	O
and	O
their	O
reconstruction	O
after	O
dimensionality	B
reduction	I
to	O
using	O
pca	B
circles	O
pca	B
input	O
a	O
matrix	O
of	O
m	O
examples	O
x	O
rmd	O
number	O
of	O
components	O
n	O
if	O
d	O
a	O
let	O
un	O
be	O
the	O
eigenvectors	O
of	O
a	O
with	O
largest	O
eigenvalues	O
else	O
b	O
let	O
vn	O
be	O
the	O
eigenvectors	O
of	O
b	O
with	O
largest	O
eigenvalues	O
for	O
i	O
n	O
set	B
ui	O
output	O
un	O
to	O
illustrate	O
how	O
pca	B
works	O
let	O
us	O
generate	O
vectors	O
in	O
that	O
approximately	O
reside	O
on	O
a	O
line	O
namely	O
on	O
a	O
one	O
dimensional	O
subspace	O
of	O
for	O
example	O
suppose	O
that	O
each	O
example	O
is	O
of	O
the	O
form	O
x	O
y	O
where	O
x	O
is	O
chosen	O
uniformly	O
at	O
random	O
from	O
and	O
y	O
is	O
sampled	O
from	O
a	O
gaussian	O
distribution	O
with	O
mean	O
and	O
standard	O
deviation	O
of	O
suppose	O
we	O
apply	O
pca	B
on	O
this	O
data	O
then	O
the	O
eigenvector	O
corresponding	O
to	O
the	O
largest	O
eigenvalue	O
will	O
be	O
close	O
to	O
the	O
vector	O
when	O
projecting	O
a	O
point	O
x	O
y	O
on	O
this	O
principal	O
component	O
we	O
will	O
obtain	O
the	O
scalar	O
the	O
reconstruction	O
of	O
the	O
original	O
vector	O
will	O
be	O
in	O
figure	O
we	O
depict	O
the	O
original	O
versus	O
reconstructed	O
data	O
next	O
we	O
demonstrate	O
the	O
effectiveness	O
of	O
pca	B
on	O
a	O
data	O
set	B
of	O
faces	O
we	O
extracted	O
images	O
of	O
faces	O
from	O
the	O
yale	O
data	O
set	B
belhumeur	O
kriegman	O
each	O
image	O
contains	O
pixels	O
therefore	O
the	O
original	O
dimensionality	O
is	O
very	O
high	O
dimensionality	B
reduction	I
o	O
o	O
oo	O
o	O
o	O
o	O
x	O
x	O
x	O
x	O
xx	O
x	O
figure	O
images	O
of	O
faces	O
extracted	O
from	O
the	O
yale	O
data	O
set	B
top-left	O
the	O
original	O
images	O
in	O
top-right	O
the	O
images	O
after	O
dimensionality	B
reduction	I
to	O
and	O
reconstruction	O
middle	O
row	O
an	O
enlarged	O
version	O
of	O
one	O
of	O
the	O
images	O
before	O
and	O
after	O
pca	B
bottom	O
the	O
images	O
after	O
dimensionality	B
reduction	I
to	O
the	O
different	O
marks	O
indicate	O
different	O
individuals	O
some	O
images	O
of	O
faces	O
are	O
depicted	O
on	O
the	O
top-left	O
side	O
of	O
figure	O
using	O
pca	B
we	O
reduced	O
the	O
dimensionality	O
to	O
and	O
reconstructed	O
back	O
to	O
the	O
original	O
dimension	O
which	O
is	O
the	O
resulting	O
reconstructed	O
images	O
are	O
depicted	O
on	O
the	O
top-right	O
side	O
of	O
figure	O
finally	O
on	O
the	O
bottom	O
of	O
figure	O
we	O
depict	O
a	O
dimensional	O
representation	O
of	O
the	O
images	O
as	O
can	O
be	O
seen	O
even	O
from	O
a	O
dimensional	O
representation	O
of	O
the	O
images	O
we	O
can	O
still	O
roughly	O
separate	O
different	O
individuals	O
random	B
projections	I
random	B
projections	I
in	O
this	O
section	O
we	O
show	O
that	O
reducing	O
the	O
dimension	O
by	O
using	O
a	O
random	O
linear	O
transformation	O
leads	O
to	O
a	O
simple	O
compression	B
scheme	I
with	O
a	O
surprisingly	O
low	O
distortion	O
the	O
transformation	O
x	O
w	O
x	O
when	O
w	O
is	O
a	O
random	O
matrix	O
is	O
often	O
referred	O
to	O
as	O
a	O
random	O
projection	B
in	O
particular	O
we	O
provide	O
a	O
variant	O
of	O
a	O
famous	O
lemma	O
due	O
to	O
johnson	O
and	O
lindenstrauss	O
showing	O
that	O
random	B
projections	I
do	O
not	O
distort	O
euclidean	O
distances	O
too	O
much	O
let	O
be	O
two	O
vectors	O
in	O
rd	O
a	O
matrix	O
w	O
does	O
not	O
distort	O
too	O
much	O
the	O
distance	O
between	O
and	O
if	O
the	O
ratio	O
w	O
is	O
close	O
to	O
in	O
other	O
words	O
the	O
distances	O
between	O
and	O
before	O
and	O
after	O
the	O
transformation	O
are	O
almost	O
the	O
same	O
to	O
show	O
that	O
w	O
is	O
not	O
too	O
far	O
away	O
from	O
it	O
suffices	O
to	O
show	O
that	O
w	O
does	O
not	O
distort	O
the	O
norm	O
of	O
the	O
difference	O
vector	O
x	O
therefore	O
from	O
now	O
on	O
we	O
focus	O
on	O
the	O
ratio	O
we	O
start	O
with	O
analyzing	O
the	O
distortion	O
caused	O
by	O
applying	O
a	O
random	O
projection	B
to	O
a	O
single	O
vector	O
lemma	O
fix	O
some	O
x	O
rd	O
let	O
w	O
rnd	O
be	O
a	O
random	O
matrix	O
such	O
that	O
each	O
wij	O
is	O
an	O
independent	O
normal	O
random	O
variable	O
then	O
for	O
every	O
we	O
have	O
p	O
nw	O
e	O
proof	O
without	O
loss	B
of	O
generality	O
we	O
can	O
assume	O
that	O
therefore	O
an	O
equivalent	O
inequality	O
is	O
with	O
zero	O
mean	O
and	O
variance	O
able	O
let	O
wi	O
be	O
the	O
ith	O
row	O
of	O
w	O
the	O
random	O
variable	O
is	O
a	O
weighted	O
sum	O
of	O
d	O
independent	O
normal	O
random	O
variables	O
and	O
therefore	O
it	O
is	O
normally	O
distributed	O
j	O
therefore	O
the	O
random	O
varin	O
distribution	O
the	O
claim	O
now	O
follows	O
directly	O
from	O
a	O
measure	B
concentration	I
property	O
of	O
random	O
variables	O
stated	O
in	O
lemma	O
given	O
in	O
section	O
has	O
a	O
j	O
the	O
johnson-lindenstrauss	B
lemma	I
follows	O
from	O
this	O
using	O
a	O
simple	O
union	B
bound	I
argument	O
lemma	O
lemma	O
let	O
q	O
be	O
a	O
finite	O
set	B
of	O
vectors	O
in	O
rd	O
let	O
and	O
n	O
be	O
an	O
integer	O
such	O
that	O
n	O
dimensionality	B
reduction	I
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
a	O
choice	O
of	O
a	O
random	O
matrix	O
w	O
rnd	O
such	O
that	O
each	O
element	O
of	O
w	O
is	O
distributed	O
normally	O
with	O
zero	O
mean	O
and	O
variance	O
of	O
we	O
have	O
n	O
p	O
sup	O
x	O
q	O
sup	O
x	O
q	O
proof	O
combining	O
lemma	O
and	O
the	O
union	B
bound	I
we	O
have	O
that	O
for	O
every	O
e	O
let	O
denote	O
the	O
right-hand	O
side	O
of	O
the	O
inequality	O
thus	O
we	O
obtain	O
that	O
interestingly	O
the	O
bound	O
given	O
in	O
lemma	O
does	O
not	O
depend	O
on	O
the	O
original	O
dimension	O
of	O
x	O
in	O
fact	O
the	O
bound	O
holds	O
even	O
if	O
x	O
is	O
in	O
an	O
infinite	O
dimensional	O
hilbert	B
space	I
compressed	B
sensing	I
compressed	B
sensing	I
is	O
a	O
dimensionality	B
reduction	I
technique	O
which	O
utilizes	O
a	O
prior	O
assumption	O
that	O
the	O
original	O
vector	O
is	O
sparse	O
in	O
some	O
basis	O
to	O
motivate	O
compressed	B
sensing	I
consider	O
a	O
vector	O
x	O
rd	O
that	O
has	O
at	O
most	O
s	O
nonzero	O
elements	O
that	O
is	O
def	O
xi	O
s	O
clearly	O
we	O
can	O
compress	O
x	O
by	O
representing	O
it	O
using	O
s	O
pairs	O
furthermore	O
this	O
compression	O
is	O
lossless	O
we	O
can	O
reconstruct	O
x	O
exactly	O
from	O
the	O
s	O
pairs	O
now	O
lets	O
take	O
one	O
step	O
forward	O
and	O
assume	O
that	O
x	O
u	O
where	O
is	O
a	O
sparse	O
vector	O
s	O
and	O
u	O
is	O
a	O
fixed	O
orthonormal	O
matrix	O
that	O
is	O
x	O
has	O
a	O
sparse	O
representation	O
in	O
another	O
basis	O
it	O
turns	O
out	O
that	O
many	O
natural	O
vectors	O
are	O
least	O
approximately	O
sparse	O
in	O
some	O
representation	O
in	O
fact	O
this	O
assumption	O
underlies	O
many	O
modern	O
compression	O
schemes	O
for	O
example	O
the	O
format	O
for	O
image	O
compression	O
relies	O
on	O
the	O
fact	O
that	O
natural	O
images	O
are	O
approximately	O
sparse	O
in	O
a	O
wavelet	O
basis	O
can	O
we	O
still	O
compress	O
x	O
into	O
roughly	O
s	O
numbers	O
well	O
one	O
simple	O
way	O
to	O
do	O
this	O
is	O
to	O
multiply	O
x	O
by	O
which	O
yields	O
the	O
sparse	O
vector	O
and	O
then	O
represent	O
by	O
its	O
s	O
pairs	O
however	O
this	O
requires	O
us	O
first	O
to	O
sense	O
x	O
to	O
store	O
it	O
and	O
then	O
to	O
multiply	O
it	O
by	O
this	O
raises	O
a	O
very	O
natural	O
question	O
why	O
go	O
to	O
so	O
much	O
effort	O
to	O
acquire	O
all	O
the	O
data	O
when	O
most	O
of	O
what	O
we	O
get	O
will	O
be	O
thrown	O
away	O
cannot	O
we	O
just	O
directly	O
measure	O
the	O
part	O
that	O
will	O
not	O
end	O
up	O
being	O
thrown	O
away	O
compressed	B
sensing	I
compressed	B
sensing	I
is	O
a	O
technique	O
that	O
simultaneously	O
acquires	O
and	O
compresses	O
the	O
data	O
the	O
key	O
result	O
is	O
that	O
a	O
random	O
linear	O
transformation	O
can	O
compress	O
x	O
without	O
losing	O
information	O
the	O
number	O
of	O
measurements	O
needed	O
is	O
order	O
of	O
s	O
logd	O
that	O
is	O
we	O
roughly	O
acquire	O
only	O
the	O
important	O
information	O
about	O
the	O
signal	O
as	O
we	O
will	O
see	O
later	O
the	O
price	O
we	O
pay	O
is	O
a	O
slower	O
reconstruction	O
phase	O
in	O
some	O
situations	O
it	O
makes	O
sense	O
to	O
save	O
time	O
in	O
compression	O
even	O
at	O
the	O
price	O
of	O
a	O
slower	O
reconstruction	O
for	O
example	O
a	O
security	O
camera	O
should	O
sense	O
and	O
compress	O
a	O
large	O
amount	O
of	O
images	O
while	O
most	O
of	O
the	O
time	O
we	O
do	O
not	O
need	O
to	O
decode	O
the	O
compressed	O
data	O
at	O
all	O
furthermore	O
in	O
many	O
practical	O
applications	O
compression	O
by	O
a	O
linear	O
transformation	O
is	O
advantageous	O
because	O
it	O
can	O
be	O
performed	O
efficiently	O
in	O
hardware	O
for	O
example	O
a	O
team	O
led	O
by	O
baraniuk	O
and	O
kelly	O
has	O
proposed	O
a	O
camera	O
architecture	O
that	O
employs	O
a	O
digital	O
micromirror	O
array	O
to	O
perform	O
optical	O
calculations	O
of	O
a	O
linear	O
transformation	O
of	O
an	O
image	O
in	O
this	O
case	O
obtaining	O
each	O
compressed	O
measurement	O
is	O
as	O
easy	O
as	O
obtaining	O
a	O
single	O
raw	O
measurement	O
another	O
important	O
application	O
of	O
compressed	B
sensing	I
is	O
medical	O
imaging	O
in	O
which	O
requiring	O
fewer	O
measurements	O
translates	O
to	O
less	O
radiation	O
for	O
the	O
patient	O
informally	O
the	O
main	O
premise	O
of	O
compressed	B
sensing	I
is	O
the	O
following	O
three	O
sur	O
prising	O
results	O
it	O
is	O
possible	O
to	O
reconstruct	O
any	O
sparse	O
signal	O
fully	O
if	O
it	O
was	O
compressed	O
by	O
x	O
w	O
x	O
where	O
w	O
is	O
a	O
matrix	O
which	O
satisfies	O
a	O
condition	O
called	O
the	O
restricted	O
isoperimetric	O
property	O
a	O
matrix	O
that	O
satisfies	O
this	O
property	O
is	O
guaranteed	O
to	O
have	O
a	O
low	O
distortion	O
of	O
the	O
norm	O
of	O
any	O
sparse	O
representable	O
vector	O
the	O
reconstruction	O
can	O
be	O
calculated	O
in	O
polynomial	O
time	O
by	O
solving	O
a	O
linear	O
program	O
a	O
random	O
n	O
d	O
matrix	O
is	O
likely	O
to	O
satisfy	O
the	O
rip	B
condition	O
provided	O
that	O
n	O
is	O
greater	O
than	O
an	O
order	O
of	O
s	O
logd	O
formally	O
definition	O
a	O
matrix	O
w	O
rnd	O
is	O
s-rip	O
if	O
for	O
all	O
x	O
s	O
t	O
s	O
we	O
have	O
the	O
first	O
theorem	O
establishes	O
that	O
rip	B
matrices	O
yield	O
a	O
lossless	O
compression	B
scheme	I
for	O
sparse	O
vectors	O
it	O
also	O
provides	O
a	O
reconstruction	O
scheme	O
theorem	O
let	O
and	O
let	O
w	O
be	O
a	O
matrix	O
let	O
x	O
be	O
a	O
vector	O
s	O
t	O
s	O
let	O
y	O
w	O
x	O
be	O
the	O
compression	O
of	O
x	O
and	O
let	O
x	O
argmin	O
vw	O
vy	O
be	O
a	O
reconstructed	O
vector	O
then	O
x	O
x	O
dimensionality	B
reduction	I
proof	O
we	O
assume	O
by	O
way	O
of	O
contradiction	O
that	O
x	O
x	O
since	O
x	O
satisfies	O
the	O
constraints	O
in	O
the	O
optimization	O
problem	O
that	O
defines	O
x	O
we	O
clearly	O
have	O
that	O
s	O
therefore	O
and	O
we	O
can	O
apply	O
the	O
rip	B
inequality	O
on	O
the	O
vector	O
x	O
x	O
but	O
since	O
w	O
x	O
we	O
get	O
that	O
which	O
leads	O
to	O
a	O
contradiction	O
the	O
reconstruction	O
scheme	O
given	O
in	O
theorem	O
seems	O
to	O
be	O
nonefficient	O
because	O
we	O
need	O
to	O
minimize	O
a	O
combinatorial	O
objective	O
sparsity	O
of	O
v	O
quite	O
surprisingly	O
it	O
turns	O
out	O
that	O
we	O
can	O
replace	O
the	O
combinatorial	O
objective	O
with	O
a	O
convex	B
objective	O
which	O
leads	O
to	O
a	O
linear	B
programming	I
problem	O
that	O
can	O
be	O
solved	O
efficiently	O
this	O
is	O
stated	O
formally	O
in	O
the	O
following	O
theorem	O
theorem	O
assume	O
that	O
the	O
conditions	O
of	O
theorem	O
holds	O
and	O
that	O
then	O
x	O
argmin	O
vw	O
vy	O
argmin	O
vw	O
vy	O
in	O
fact	O
we	O
will	O
prove	O
a	O
stronger	O
result	O
which	O
holds	O
even	O
if	O
x	O
is	O
not	O
a	O
sparse	O
vector	O
theorem	O
let	O
arbitrary	O
vector	O
and	O
denote	O
and	O
let	O
w	O
be	O
a	O
matrix	O
let	O
x	O
be	O
an	O
xs	O
argmin	O
s	O
that	O
is	O
xs	O
is	O
the	O
vector	O
which	O
equals	O
x	O
on	O
the	O
s	O
largest	O
elements	O
of	O
x	O
and	O
equals	O
elsewhere	O
let	O
y	O
w	O
x	O
be	O
the	O
compression	O
of	O
x	O
and	O
let	O
argmin	O
vw	O
vy	O
be	O
the	O
reconstructed	O
vector	O
then	O
s	O
where	O
note	O
that	O
in	O
the	O
special	O
case	O
that	O
x	O
xs	O
we	O
get	O
an	O
exact	O
recovery	O
x	O
so	O
theorem	O
is	O
a	O
special	O
case	O
of	O
theorem	O
the	O
proof	O
of	O
theorem	O
is	O
given	O
in	O
section	O
finally	O
the	O
third	O
result	O
tells	O
us	O
that	O
random	O
matrices	O
with	O
n	O
logd	O
are	O
likely	O
to	O
be	O
rip	B
in	O
fact	O
the	O
theorem	O
shows	O
that	O
multiplying	O
a	O
random	O
matrix	O
by	O
an	O
orthonormal	O
matrix	O
also	O
provides	O
an	O
rip	B
matrix	O
this	O
is	O
important	O
for	O
compressing	O
signals	O
of	O
the	O
form	O
x	O
u	O
where	O
x	O
is	O
not	O
sparse	O
but	O
is	O
sparse	O
in	O
that	O
case	O
if	O
w	O
is	O
a	O
random	O
matrix	O
and	O
we	O
compress	O
using	O
y	O
w	O
x	O
then	O
this	O
is	O
the	O
same	O
as	O
compressing	O
by	O
y	O
u	O
and	O
since	O
w	O
u	O
is	O
also	O
rip	B
we	O
can	O
reconstruct	O
thus	O
also	O
x	O
from	O
y	O
compressed	B
sensing	I
theorem	O
let	O
u	O
be	O
an	O
arbitrary	O
fixed	O
d	O
d	O
orthonormal	O
matrix	O
let	O
be	O
scalars	O
in	O
let	O
s	O
be	O
an	O
integer	O
in	O
and	O
let	O
n	O
be	O
an	O
integer	O
that	O
satisfies	O
n	O
s	O
let	O
w	O
rnd	O
be	O
a	O
matrix	O
s	O
t	O
each	O
element	O
of	O
w	O
is	O
distributed	O
normally	O
with	O
zero	O
mean	O
and	O
variance	O
of	O
then	O
with	O
proabability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
w	O
the	O
matrix	O
w	O
u	O
is	O
s-rip	O
proofs	O
proof	O
of	O
theorem	O
we	O
follow	O
a	O
proof	O
due	O
to	O
candes	O
let	O
h	O
x	O
given	O
a	O
vector	O
v	O
and	O
a	O
set	B
of	O
indices	O
i	O
we	O
denote	O
by	O
vi	O
the	O
vector	O
whose	O
ith	O
element	O
is	O
vi	O
if	O
i	O
i	O
and	O
otherwise	O
the	O
first	O
trick	O
we	O
use	O
is	O
to	O
partition	O
the	O
set	B
of	O
indices	O
d	O
into	O
disjoint	O
sets	O
of	O
size	O
s	O
that	O
is	O
we	O
will	O
write	O
tds	O
where	O
for	O
all	O
i	O
s	O
and	O
we	O
assume	O
for	O
simplicity	O
that	O
ds	O
is	O
an	O
integer	O
we	O
define	O
the	O
partition	O
as	O
follows	O
in	O
we	O
put	O
the	O
s	O
indices	O
corresponding	O
to	O
the	O
s	O
largest	O
elements	O
in	O
absolute	O
values	O
of	O
x	O
are	O
broken	O
arbitrarily	O
let	O
t	O
c	O
next	O
will	O
be	O
the	O
s	O
indices	O
corresponding	O
to	O
the	O
s	O
largest	O
elements	O
in	O
absolute	O
next	O
will	O
correspond	O
to	O
value	O
of	O
ht	O
c	O
the	O
s	O
largest	O
elements	O
in	O
absolute	O
value	O
of	O
ht	O
c	O
and	O
we	O
will	O
construct	O
in	O
the	O
same	O
way	O
let	O
and	O
t	O
c	O
to	O
prove	O
the	O
theorem	O
we	O
first	O
need	O
the	O
following	O
lemma	O
which	O
shows	O
that	O
rip	B
also	O
implies	O
approximate	O
orthogonality	O
lemma	O
let	O
w	O
be	O
an	O
matrix	O
then	O
for	O
any	O
two	O
disjoint	O
sets	O
i	O
j	O
both	O
of	O
size	O
at	O
most	O
s	O
and	O
for	O
any	O
vector	O
u	O
we	O
have	O
that	O
ui	O
w	O
proof	O
w	O
l	O
o	O
g	O
assume	O
ui	O
w	O
ui	O
w	O
ui	O
w	O
but	O
since	O
i	O
we	O
get	O
from	O
the	O
rip	B
condition	O
that	O
ui	O
w	O
and	O
that	O
ui	O
w	O
which	O
concludes	O
our	O
proof	O
we	O
are	O
now	O
ready	O
to	O
prove	O
the	O
theorem	O
clearly	O
c	O
to	O
prove	O
the	O
theorem	O
we	O
will	O
show	O
the	O
following	O
two	O
claims	O
ht	O
c	O
claim	O
c	O
claim	O
s	O
dimensionality	B
reduction	I
combining	O
these	O
two	O
claims	O
with	O
equation	O
we	O
get	O
that	O
c	O
s	O
s	O
and	O
this	O
will	O
conclude	O
our	O
proof	O
proving	O
claim	O
to	O
prove	O
this	O
claim	O
we	O
do	O
not	O
use	O
the	O
rip	B
condition	O
at	O
all	O
but	O
only	O
use	O
the	O
fact	O
that	O
minimizes	O
the	O
norm	O
take	O
j	O
for	O
each	O
i	O
tj	O
and	O
tj	O
we	O
have	O
that	O
therefore	O
thus	O
s	O
j	O
summing	O
this	O
over	O
j	O
and	O
using	O
the	O
triangle	O
inequality	O
we	O
obtain	O
that	O
c	O
s	O
c	O
next	O
we	O
show	O
that	O
c	O
cannot	O
be	O
large	O
indeed	O
from	O
the	O
definition	O
of	O
we	O
have	O
that	O
thus	O
using	O
the	O
triangle	O
inequality	O
we	O
obtain	O
that	O
c	O
c	O
i	O
i	O
t	O
c	O
and	O
since	O
c	O
we	O
get	O
that	O
c	O
c	O
combining	O
this	O
with	O
equation	O
we	O
get	O
that	O
s	O
c	O
c	O
c	O
which	O
concludes	O
the	O
proof	O
of	O
claim	O
proving	O
claim	O
for	O
the	O
second	O
claim	O
we	O
use	O
the	O
rip	B
condition	O
to	O
get	O
that	O
since	O
w	O
w	O
h	O
j	O
w	O
htj	O
w	O
j	O
j	O
j	O
w	O
htj	O
we	O
have	O
that	O
w	O
w	O
from	O
the	O
rip	B
condition	O
on	O
inner	O
products	O
we	O
obtain	O
that	O
for	O
all	O
i	O
and	O
j	O
we	O
have	O
hti	O
w	O
compressed	B
sensing	I
since	O
we	O
therefore	O
get	O
that	O
j	O
combining	O
this	O
with	O
equation	O
and	O
equation	O
we	O
obtain	O
c	O
s	O
c	O
rearranging	O
the	O
inequality	O
gives	O
finally	O
using	O
equation	O
we	O
get	O
that	O
s	O
c	O
but	O
since	O
this	O
implies	O
s	O
c	O
s	O
c	O
which	O
concludes	O
the	O
proof	O
of	O
the	O
second	O
claim	O
proof	O
of	O
theorem	O
to	O
prove	O
the	O
theorem	O
we	O
follow	O
an	O
approach	O
due	O
to	O
davenport	O
devore	O
wakin	O
the	O
idea	O
is	O
to	O
combine	O
the	O
johnson-lindenstrauss	B
lemma	I
with	O
a	O
simple	O
covering	O
argument	O
we	O
start	O
with	O
a	O
covering	O
property	O
of	O
the	O
unit	O
ball	O
lemma	O
let	O
there	O
exists	O
a	O
finite	O
set	B
q	O
rd	O
of	O
size	O
such	O
that	O
sup	O
min	O
v	O
q	O
proof	O
let	O
k	O
be	O
an	O
integer	O
and	O
let	O
rd	O
j	O
i	O
k	O
k	O
k	O
s	O
t	O
xj	O
i	O
k	O
clearly	O
we	O
shall	O
set	B
q	O
where	O
is	O
the	O
unit	O
ball	O
of	O
rd	O
since	O
the	O
points	O
in	O
are	O
distributed	O
evenly	O
on	O
the	O
unit	O
ball	O
the	O
size	O
of	O
q	O
is	O
the	O
size	O
of	O
times	O
the	O
ratio	O
between	O
the	O
volumes	O
of	O
the	O
unit	O
and	O
balls	O
the	O
volume	O
of	O
the	O
ball	O
is	O
and	O
the	O
volume	O
of	O
is	O
for	O
simplicity	O
assume	O
that	O
d	O
is	O
even	O
and	O
therefore	O
e	O
dimensionality	B
reduction	I
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
stirling	O
s	O
approximation	O
overall	O
we	O
obtained	O
that	O
d	O
now	O
lets	O
specify	O
k	O
for	O
each	O
x	O
let	O
v	O
q	O
be	O
the	O
vector	O
whose	O
ith	O
element	O
is	O
then	O
for	O
each	O
element	O
we	O
have	O
that	O
vi	O
and	O
thus	O
d	O
k	O
to	O
ensure	O
that	O
the	O
right-hand	O
side	O
will	O
be	O
at	O
most	O
we	O
shall	O
set	B
k	O
plugging	O
this	O
value	O
into	O
equation	O
we	O
conclude	O
that	O
let	O
x	O
be	O
a	O
vector	O
that	O
can	O
be	O
written	O
as	O
x	O
u	O
with	O
u	O
being	O
some	O
orthonormal	O
matrix	O
and	O
s	O
combining	O
the	O
earlier	O
covering	O
property	O
and	O
the	O
jl	O
lemma	O
enables	O
us	O
to	O
show	O
that	O
a	O
random	O
w	O
will	O
not	O
distort	O
any	O
such	O
x	O
lemma	O
let	O
u	O
be	O
an	O
orthonormal	O
d	O
d	O
matrix	O
and	O
let	O
i	O
be	O
a	O
set	B
of	O
indices	O
of	O
size	O
s	O
let	O
s	O
be	O
the	O
span	O
of	O
i	O
i	O
where	O
ui	O
is	O
the	O
ith	O
column	O
of	O
u	O
let	O
and	O
n	O
n	O
such	O
that	O
n	O
s	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
a	O
choice	O
of	O
a	O
random	O
matrix	O
w	O
rnd	O
such	O
that	O
each	O
element	O
of	O
w	O
is	O
independently	O
distributed	O
according	O
to	O
n	O
we	O
have	O
sup	O
x	O
s	O
it	O
suffices	O
to	O
prove	O
the	O
lemma	O
for	O
all	O
x	O
s	O
with	O
we	O
can	O
write	O
proof	O
x	O
ui	O
where	O
rs	O
and	O
ui	O
is	O
the	O
matrix	O
whose	O
columns	O
are	O
i	O
i	O
using	O
lemma	O
we	O
know	O
that	O
there	O
exists	O
a	O
set	B
q	O
of	O
size	O
such	O
that	O
sup	O
min	O
v	O
q	O
but	O
since	O
u	O
is	O
orthogonal	O
we	O
also	O
have	O
that	O
sup	O
min	O
v	O
q	O
ui	O
applying	O
lemma	O
on	O
the	O
set	B
v	O
v	O
q	O
we	O
obtain	O
that	O
for	O
n	O
satisfying	O
compressed	B
sensing	I
the	O
condition	O
given	O
in	O
the	O
lemma	O
the	O
following	O
holds	O
with	O
probability	O
of	O
at	O
least	O
sup	O
v	O
q	O
this	O
also	O
implies	O
that	O
ui	O
ui	O
sup	O
v	O
q	O
let	O
a	O
be	O
the	O
smallest	O
number	O
such	O
that	O
x	O
s	O
a	O
clearly	O
a	O
our	O
goal	O
is	O
to	O
show	O
that	O
a	O
this	O
follows	O
from	O
the	O
fact	O
that	O
for	O
any	O
x	O
s	O
of	O
unit	O
norm	O
there	O
exists	O
v	O
q	O
such	O
that	O
ui	O
and	O
therefore	O
ui	O
ui	O
thus	O
x	O
s	O
but	O
the	O
definition	O
of	O
a	O
implies	O
that	O
a	O
a	O
this	O
proves	O
that	O
for	O
all	O
x	O
s	O
we	O
have	O
this	O
as	O
well	O
since	O
the	O
other	O
side	O
follows	O
from	O
ui	O
ui	O
the	O
preceding	O
lemma	O
tells	O
us	O
that	O
for	O
x	O
s	O
of	O
unit	O
norm	O
we	O
have	O
which	O
implies	O
that	O
the	O
proof	O
of	O
theorem	O
follows	O
from	O
this	O
by	O
a	O
union	B
bound	I
over	O
all	O
choices	O
of	O
i	O
dimensionality	B
reduction	I
pca	B
or	O
compressed	B
sensing	I
suppose	O
we	O
would	O
like	O
to	O
apply	O
a	O
dimensionality	B
reduction	I
technique	O
to	O
a	O
given	O
set	B
of	O
examples	O
which	O
method	O
should	O
we	O
use	O
pca	B
or	O
compressed	B
sensing	I
in	O
this	O
section	O
we	O
tackle	O
this	O
question	O
by	O
underscoring	O
the	O
underlying	O
assumptions	O
behind	O
the	O
two	O
methods	O
it	O
is	O
helpful	O
first	O
to	O
understand	O
when	O
each	O
of	O
the	O
methods	O
can	O
guarantee	O
perfect	O
recovery	O
pca	B
guarantees	O
perfect	O
recovery	O
whenever	O
the	O
set	B
of	O
examples	O
is	O
contained	O
in	O
an	O
n	O
dimensional	O
subspace	O
of	O
rd	O
compressed	B
sensing	I
guarantees	O
perfect	O
recovery	O
whenever	O
the	O
set	B
of	O
examples	O
is	O
sparse	O
some	O
basis	O
on	O
the	O
basis	O
of	O
these	O
observations	O
we	O
can	O
describe	O
cases	O
in	O
which	O
pca	B
will	O
be	O
better	O
than	O
compressed	B
sensing	I
and	O
vice	O
versa	O
as	O
a	O
first	O
example	O
suppose	O
that	O
the	O
examples	O
are	O
the	O
vectors	O
of	O
the	O
standard	O
basis	O
of	O
rd	O
namely	O
ed	O
where	O
each	O
ei	O
is	O
the	O
all	O
zeros	O
vector	O
except	O
in	O
the	O
ith	O
coordinate	O
in	O
this	O
case	O
the	O
examples	O
are	O
hence	O
compressed	B
sensing	I
will	O
yield	O
a	O
perfect	O
recovery	O
whenever	O
n	O
on	O
the	O
other	O
hand	O
pca	B
will	O
lead	O
to	O
poor	O
performance	O
since	O
the	O
data	O
is	O
far	O
from	O
being	O
in	O
an	O
n	O
dimensional	O
subspace	O
as	O
long	O
as	O
n	O
d	O
indeed	O
it	O
is	O
easy	O
ro	O
verify	O
that	O
in	O
such	O
a	O
case	O
the	O
averaged	O
recovery	O
error	O
of	O
pca	B
the	O
objective	O
of	O
equation	O
divided	O
by	O
m	O
will	O
be	O
nd	O
which	O
is	O
larger	O
than	O
whenever	O
n	O
we	O
next	O
show	O
a	O
case	O
where	O
pca	B
is	O
better	O
than	O
compressed	B
sensing	I
consider	O
m	O
examples	O
that	O
are	O
exactly	O
on	O
an	O
n	O
dimensional	O
subspace	O
clearly	O
in	O
such	O
a	O
case	O
pca	B
will	O
lead	O
to	O
perfect	O
recovery	O
as	O
to	O
compressed	B
sensing	I
note	O
that	O
the	O
examples	O
are	O
n-sparse	O
in	O
any	O
orthonormal	O
basis	O
whose	O
first	O
n	O
vectors	O
span	O
the	O
subspace	O
therefore	O
compressed	B
sensing	I
would	O
also	O
work	O
if	O
we	O
will	O
reduce	O
the	O
dimension	O
to	O
logd	O
however	O
with	O
exactly	O
n	O
dimensions	O
compressed	B
sensing	I
might	O
fail	O
pca	B
has	O
also	O
better	O
resilience	O
to	O
certain	O
types	O
of	O
noise	O
see	O
weiss	O
freeman	O
for	O
a	O
discussion	O
summary	O
we	O
introduced	O
two	O
methods	O
for	O
dimensionality	B
reduction	I
using	O
linear	O
transformations	O
pca	B
and	O
random	B
projections	I
we	O
have	O
shown	O
that	O
pca	B
is	O
optimal	O
in	O
the	O
sense	O
of	O
averaged	O
squared	O
reconstruction	O
error	O
if	O
we	O
restrict	O
the	O
reconstruction	O
procedure	O
to	O
be	O
linear	O
as	O
well	O
however	O
if	O
we	O
allow	O
nonlinear	O
reconstruction	O
pca	B
is	O
not	O
necessarily	O
the	O
optimal	O
procedure	O
in	O
particular	O
for	O
sparse	O
data	O
random	B
projections	I
can	O
significantly	O
outperform	O
pca	B
this	O
fact	O
is	O
at	O
the	O
heart	O
of	O
the	O
compressed	B
sensing	I
method	O
bibliographic	O
remarks	O
bibliographic	O
remarks	O
pca	B
is	O
equivalent	O
to	O
best	O
subspace	O
approximation	O
using	O
singular	O
value	O
decomposition	O
the	O
svd	B
method	O
is	O
described	O
in	O
appendix	O
c	O
svd	B
dates	O
back	O
to	O
eugenio	O
beltrami	O
and	O
camille	O
jordan	O
it	O
has	O
been	O
rediscovered	O
many	O
times	O
in	O
the	O
statistical	O
literature	O
it	O
was	O
introduced	O
by	O
pearson	O
besides	O
pca	B
and	O
svd	B
there	O
are	O
additional	O
names	O
that	O
refer	O
to	O
the	O
same	O
idea	O
and	O
are	O
being	O
used	O
in	O
different	O
scientific	O
communities	O
a	O
few	O
examples	O
are	O
the	O
eckartyoung	O
theorem	O
carl	O
eckart	O
and	O
gale	O
young	O
who	O
analyzed	O
the	O
method	O
in	O
the	O
schmidt-mirsky	O
theorem	O
factor	O
analysis	O
and	O
the	O
hotelling	O
transform	O
compressed	B
sensing	I
was	O
introduced	O
in	O
donoho	O
and	O
in	O
tao	O
see	O
also	O
candes	O
exercises	O
in	O
this	O
exercise	O
we	O
show	O
that	O
in	O
the	O
general	O
case	O
exact	O
recovery	O
of	O
a	O
linear	O
compression	B
scheme	I
is	O
impossible	O
let	O
a	O
rnd	O
be	O
an	O
arbitrary	O
compression	O
matrix	O
where	O
n	O
d	O
show	O
that	O
there	O
exists	O
u	O
v	O
rn	O
u	O
v	O
such	O
that	O
au	O
av	O
conclude	O
that	O
exact	O
recovery	O
of	O
a	O
linear	O
compression	B
scheme	I
is	O
impossible	O
let	O
rd	O
such	O
that	O
d	O
show	O
that	O
max	O
n	O
j	O
j	O
j	O
hint	O
take	O
every	O
vector	O
such	O
that	O
n	O
let	O
i	O
be	O
the	O
minimal	O
index	O
for	O
which	O
i	O
if	O
i	O
n	O
we	O
are	O
done	O
otherwise	O
show	O
that	O
we	O
can	O
increase	O
i	O
while	O
possibly	O
decreasing	O
j	O
for	O
some	O
j	O
i	O
and	O
obtain	O
a	O
better	O
solution	O
this	O
will	O
imply	O
that	O
the	O
optimal	O
solution	O
is	O
to	O
set	B
i	O
for	O
i	O
n	O
and	O
i	O
for	O
i	O
n	O
kernel	B
pca	B
in	O
this	O
exercise	O
we	O
show	O
how	O
pca	B
can	O
be	O
used	O
for	O
constructing	O
nonlinear	O
dimensionality	B
reduction	I
on	O
the	O
basis	O
of	O
the	O
kernel	B
trick	I
chapter	O
let	O
x	O
be	O
some	O
instance	B
space	I
and	O
let	O
s	O
xm	O
be	O
a	O
set	B
of	O
points	O
in	O
x	O
consider	O
a	O
feature	B
mapping	O
x	O
v	O
where	O
v	O
is	O
some	O
hilbert	B
space	I
of	O
infinite	O
dimension	O
let	O
k	O
x	O
x	O
be	O
a	O
kernel	O
function	B
that	O
is	O
kx	O
kernel	B
pca	B
is	O
the	O
process	O
of	O
mapping	O
the	O
elements	O
in	O
s	O
into	O
v	O
using	O
and	O
then	O
applying	O
pca	B
over	O
into	O
rn	O
the	O
output	O
of	O
this	O
process	O
is	O
the	O
set	B
of	O
reduced	O
elements	O
show	O
how	O
this	O
process	O
can	O
be	O
done	O
in	O
polynomial	O
time	O
in	O
terms	O
of	O
m	O
and	O
n	O
assuming	O
that	O
each	O
evaluation	O
of	O
k	O
can	O
be	O
calculated	O
in	O
a	O
constant	O
time	O
in	O
particular	O
if	O
your	O
implementation	O
requires	O
multiplication	O
of	O
two	O
matrices	O
a	O
and	O
b	O
verify	O
that	O
their	O
product	O
can	O
be	O
computed	O
similarly	O
dimensionality	B
reduction	I
if	O
an	O
eigenvalue	O
decomposition	O
of	O
some	O
matrix	O
c	O
is	O
required	O
verify	O
that	O
this	O
decomposition	O
can	O
be	O
computed	O
an	O
interpretation	O
of	O
pca	B
as	O
variance	O
maximization	O
let	O
xm	O
be	O
m	O
vectors	O
in	O
rd	O
and	O
let	O
x	O
be	O
a	O
random	O
vector	O
distributed	O
according	O
to	O
the	O
uniform	O
distribution	O
over	O
xm	O
assume	O
that	O
ex	O
consider	O
the	O
problem	O
of	O
finding	O
a	O
unit	O
vector	O
w	O
rd	O
such	O
that	O
the	O
random	O
variable	O
has	O
maximal	O
variance	O
that	O
is	O
we	O
would	O
like	O
to	O
solve	O
the	O
problem	O
argmax	O
argmax	O
m	O
show	O
that	O
the	O
solution	O
of	O
the	O
problem	O
is	O
to	O
set	B
w	O
to	O
be	O
the	O
first	O
principle	O
vector	O
of	O
xm	O
let	O
be	O
the	O
first	O
principal	O
component	O
as	O
in	O
the	O
previous	O
question	O
now	O
suppose	O
we	O
would	O
like	O
to	O
find	O
a	O
second	O
unit	O
vector	O
rd	O
that	O
maximizes	O
the	O
variance	O
of	O
but	O
is	O
also	O
uncorrelated	O
to	O
that	O
is	O
we	O
would	O
like	O
to	O
solve	O
argmax	O
show	O
that	O
the	O
solution	O
to	O
this	O
problem	O
is	O
to	O
set	B
w	O
to	O
be	O
the	O
second	O
principal	O
component	O
of	O
xm	O
hint	O
note	O
that	O
aw	O
where	O
a	O
i	O
constraint	O
is	O
equivalent	O
to	O
the	O
constraint	O
i	O
since	O
w	O
is	O
an	O
eigenvector	O
of	O
a	O
we	O
have	O
that	O
the	O
the	O
relation	O
between	O
svd	B
and	O
pca	B
use	O
the	O
svd	B
theorem	O
lary	O
for	O
providing	O
an	O
alternative	O
proof	O
of	O
theorem	O
random	B
projections	I
preserve	O
inner	O
products	O
the	O
johnson-lindenstrauss	B
lemma	I
tells	O
us	O
that	O
a	O
random	O
projection	B
preserves	O
distances	O
between	O
a	O
finite	O
set	B
of	O
vectors	O
in	O
this	O
exercise	O
you	O
need	O
to	O
prove	O
that	O
if	O
the	O
set	B
of	O
vectors	O
are	O
within	O
the	O
unit	O
ball	O
then	O
not	O
only	O
are	O
the	O
distances	O
between	O
any	O
two	O
vectors	O
preserved	O
but	O
the	O
inner	O
product	O
is	O
also	O
preserved	O
let	O
q	O
be	O
a	O
finite	O
set	B
of	O
vectors	O
in	O
rd	O
and	O
assume	O
that	O
for	O
every	O
x	O
q	O
we	O
have	O
let	O
and	O
n	O
be	O
an	O
integer	O
such	O
that	O
n	O
prove	O
that	O
with	O
probability	O
of	O
at	O
least	O
over	O
a	O
choice	O
of	O
a	O
random	O
exercises	O
matrix	O
w	O
rnd	O
where	O
each	O
element	O
of	O
w	O
is	O
independently	O
distributed	O
according	O
to	O
n	O
we	O
have	O
u	O
w	O
for	O
every	O
u	O
v	O
q	O
hint	O
use	O
jl	O
to	O
bound	O
both	O
and	O
let	O
xm	O
be	O
a	O
set	B
of	O
vectors	O
in	O
rd	O
of	O
norm	O
at	O
most	O
and	O
assume	O
that	O
these	O
vectors	O
are	O
linearly	O
separable	B
with	O
margin	B
of	O
assume	O
that	O
d	O
show	O
that	O
there	O
exists	O
a	O
constant	O
c	O
such	O
that	O
if	O
we	O
randomly	O
project	O
these	O
vectors	O
into	O
rn	O
for	O
n	O
c	O
then	O
with	O
probability	O
of	O
at	O
least	O
it	O
holds	O
that	O
the	O
projected	O
vectors	O
are	O
linearly	O
separable	B
with	O
margin	B
generative	B
models	I
we	O
started	O
this	O
book	O
with	O
a	O
distribution	B
free	I
learning	O
framework	O
namely	O
we	O
did	O
not	O
impose	O
any	O
assumptions	O
on	O
the	O
underlying	O
distribution	O
over	O
the	O
data	O
furthermore	O
we	O
followed	O
a	O
discriminative	B
approach	O
in	O
which	O
our	O
goal	O
is	O
not	O
to	O
learn	O
the	O
underlying	O
distribution	O
but	O
rather	O
to	O
learn	O
an	O
accurate	O
predictor	B
in	O
this	O
chapter	O
we	O
describe	O
a	O
generative	O
approach	O
in	O
which	O
it	O
is	O
assumed	O
that	O
the	O
underlying	O
distribution	O
over	O
the	O
data	O
has	O
a	O
specific	O
parametric	O
form	O
and	O
our	O
goal	O
is	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
model	O
this	O
task	O
is	O
called	O
parametric	B
density	I
estimation	I
the	O
discriminative	B
approach	O
has	O
the	O
advantage	O
of	O
directly	O
optimizing	O
the	O
quantity	O
of	O
interest	O
prediction	O
accuracy	B
instead	O
of	O
learning	O
the	O
underlying	O
distribution	O
this	O
was	O
phrased	O
as	O
follows	O
by	O
vladimir	O
vapnik	O
in	O
his	O
principle	O
for	O
solving	O
problems	O
using	O
a	O
restricted	O
amount	O
of	O
information	O
when	O
solving	O
a	O
given	O
problem	O
try	O
to	O
avoid	O
a	O
more	O
general	O
problem	O
as	O
an	O
intermediate	O
step	O
of	O
course	O
if	O
we	O
succeed	O
in	O
learning	O
the	O
underlying	O
distribution	O
accurately	O
we	O
are	O
considered	O
to	O
be	O
experts	O
in	O
the	O
sense	O
that	O
we	O
can	O
predict	O
by	O
using	O
the	O
bayes	B
optimal	I
classifier	B
the	O
problem	O
is	O
that	O
it	O
is	O
usually	O
more	O
difficult	O
to	O
learn	O
the	O
underlying	O
distribution	O
than	O
to	O
learn	O
an	O
accurate	O
predictor	B
however	O
in	O
some	O
situations	O
it	O
is	O
reasonable	O
to	O
adopt	O
the	O
generative	O
learning	O
approach	O
for	O
example	O
sometimes	O
it	O
is	O
easier	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
model	O
than	O
to	O
learn	O
a	O
discriminative	B
predictor	B
additionally	O
in	O
some	O
cases	O
we	O
do	O
not	O
have	O
a	O
specific	O
task	O
at	O
hand	O
but	O
rather	O
would	O
like	O
to	O
model	O
the	O
data	O
either	O
for	O
making	O
predictions	O
at	O
a	O
later	O
time	O
without	O
having	O
to	O
retrain	O
a	O
predictor	B
or	O
for	O
the	O
sake	O
of	O
interpretability	O
of	O
the	O
data	O
we	O
start	O
with	O
a	O
popular	O
statistical	O
method	O
for	O
estimating	O
the	O
parameters	O
of	O
the	O
data	O
which	O
is	O
called	O
the	O
maximum	B
likelihood	I
principle	O
next	O
we	O
describe	O
two	O
generative	O
assumptions	O
which	O
greatly	O
simplify	O
the	O
learning	O
process	O
we	O
also	O
describe	O
the	O
em	B
algorithm	O
for	O
calculating	O
the	O
maximum	B
likelihood	I
in	O
the	O
presence	O
of	O
latent	B
variables	I
we	O
conclude	O
with	O
a	O
brief	O
description	O
of	O
bayesian	B
reasoning	I
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
maximum	B
likelihood	I
estimator	O
maximum	B
likelihood	I
estimator	O
let	O
us	O
start	O
with	O
a	O
simple	O
example	O
a	O
drug	O
company	O
developed	O
a	O
new	O
drug	O
to	O
treat	O
some	O
deadly	O
disease	O
we	O
would	O
like	O
to	O
estimate	O
the	O
probability	O
of	O
survival	O
when	O
using	O
the	O
drug	O
to	O
do	O
so	O
the	O
drug	O
company	O
sampled	O
a	O
training	B
set	B
of	O
m	O
people	O
and	O
gave	O
them	O
the	O
drug	O
let	O
s	O
xm	O
denote	O
the	O
training	B
set	B
where	O
for	O
each	O
i	O
xi	O
if	O
the	O
ith	O
person	O
survived	O
and	O
xi	O
otherwise	O
we	O
can	O
model	O
the	O
underlying	O
distribution	O
using	O
a	O
single	O
parameter	O
indicating	O
the	O
probability	O
of	O
survival	O
we	O
now	O
would	O
like	O
to	O
estimate	O
the	O
parameter	O
on	O
the	O
basis	O
of	O
the	O
training	B
set	B
s	O
a	O
natural	O
idea	O
is	O
to	O
use	O
the	O
average	O
number	O
of	O
s	O
in	O
s	O
as	O
an	O
estimator	O
that	O
is	O
m	O
xi	O
clearly	O
es	O
that	O
is	O
is	O
an	O
unbiased	O
estimator	O
of	O
furthermore	O
since	O
is	O
the	O
average	O
of	O
m	O
i	O
i	O
d	O
binary	O
random	O
variables	O
we	O
can	O
use	O
hoeffding	O
s	O
inequality	O
to	O
get	O
that	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
we	O
have	O
that	O
m	O
another	O
interpretation	O
of	O
is	O
as	O
the	O
maximum	B
likelihood	I
estimator	O
as	O
we	O
formally	O
explain	O
now	O
we	O
first	O
write	O
the	O
probability	O
of	O
generating	O
the	O
sample	O
s	O
ps	O
xm	O
xi	O
xi	O
i	O
xi	O
xi	O
we	O
define	O
the	O
log	O
likelihood	O
of	O
s	O
given	O
the	O
parameter	O
as	O
the	O
log	O
of	O
the	O
preceding	O
expression	O
ls	O
log	O
xm	O
log	O
xi	O
xi	O
i	O
i	O
the	O
maximum	B
likelihood	I
estimator	O
is	O
the	O
parameter	O
that	O
maximizes	O
the	O
likelihood	O
argmax	O
ls	O
next	O
we	O
show	O
that	O
in	O
our	O
case	O
equation	O
is	O
a	O
maximum	B
likelihood	I
estimator	O
to	O
see	O
this	O
we	O
take	O
the	O
derivative	O
of	O
ls	O
with	O
respect	O
to	O
and	O
equate	O
it	O
to	O
zero	O
i	O
xi	O
xi	O
solving	O
the	O
equation	O
for	O
we	O
obtain	O
the	O
estimator	O
given	O
in	O
equation	O
generative	B
models	I
maximum	B
likelihood	I
estimation	O
for	O
continuous	O
random	O
variables	O
let	O
x	O
be	O
a	O
continuous	O
random	O
variable	O
then	O
for	O
most	O
x	O
r	O
we	O
have	O
px	O
x	O
and	O
therefore	O
the	O
definition	O
of	O
likelihood	O
as	O
given	O
before	O
is	O
trivialized	O
to	O
overcome	O
this	O
technical	O
problem	O
we	O
define	O
the	O
likelihood	O
as	O
log	O
of	O
the	O
density	O
of	O
the	O
probability	O
of	O
x	O
at	O
x	O
that	O
is	O
given	O
an	O
i	O
i	O
d	O
training	B
set	B
s	O
xm	O
sampled	O
according	O
to	O
a	O
density	O
distribution	O
p	O
we	O
define	O
the	O
likelihood	O
of	O
s	O
given	O
as	O
ls	O
log	O
p	O
logp	O
as	O
before	O
the	O
maximum	B
likelihood	I
estimator	O
is	O
a	O
maximizer	O
of	O
ls	O
with	O
respect	O
to	O
as	O
an	O
example	O
consider	O
a	O
gaussian	O
random	O
variable	O
for	O
which	O
the	O
density	O
function	B
of	O
x	O
is	O
parameterized	O
by	O
and	O
is	O
defined	O
as	O
follows	O
p	O
exp	O
we	O
can	O
rewrite	O
the	O
likelihood	O
as	O
ls	O
m	O
log	O
to	O
find	O
a	O
parameter	O
that	O
optimizes	O
this	O
we	O
take	O
the	O
derivative	O
of	O
the	O
likelihood	O
w	O
r	O
t	O
and	O
w	O
r	O
t	O
and	O
compare	O
it	O
to	O
we	O
obtain	O
the	O
following	O
two	O
equations	O
d	O
d	O
d	O
d	O
ls	O
ls	O
m	O
m	O
xi	O
and	O
m	O
solving	O
the	O
preceding	O
equations	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
estimates	O
note	O
that	O
the	O
maximum	B
likelihood	I
estimate	O
is	O
not	O
always	O
an	O
unbiased	O
estimator	O
for	O
example	O
while	O
is	O
unbiased	O
it	O
is	O
possible	O
to	O
show	O
that	O
the	O
estimate	O
of	O
the	O
variance	O
is	O
biased	O
simplifying	O
notation	O
to	O
simplify	O
our	O
notation	O
we	O
use	O
px	O
x	O
in	O
this	O
chapter	O
to	O
describe	O
both	O
the	O
probability	O
that	O
x	O
x	O
discrete	O
random	O
variables	O
and	O
the	O
density	O
of	O
the	O
distribution	O
at	O
x	O
continuous	O
variables	O
maximum	B
likelihood	I
estimator	O
maximum	B
likelihood	I
and	O
empirical	B
risk	B
minimization	O
the	O
maximum	B
likelihood	I
estimator	O
shares	O
some	O
similarity	O
with	O
the	O
empirical	B
risk	B
minimization	O
principle	O
which	O
we	O
studied	O
extensively	O
in	O
previous	O
chapters	O
recall	B
that	O
in	O
the	O
erm	B
principle	O
we	O
have	O
a	O
hypothesis	B
class	I
h	O
and	O
we	O
use	O
the	O
training	B
set	B
for	O
choosing	O
a	O
hypothesis	B
h	O
h	O
that	O
minimizes	O
the	O
empirical	B
risk	B
we	O
now	O
show	O
that	O
the	O
maximum	B
likelihood	I
estimator	O
is	O
an	O
erm	B
for	O
a	O
particular	O
loss	B
function	B
given	O
a	O
parameter	O
and	O
an	O
observation	O
x	O
we	O
define	O
the	O
loss	B
of	O
on	O
x	O
as	O
x	O
logp	O
that	O
is	O
x	O
is	O
the	O
negation	O
of	O
the	O
log-likelihood	O
of	O
the	O
observation	O
x	O
assuming	O
the	O
data	O
is	O
distributed	O
according	O
to	O
p	O
this	O
loss	B
function	B
is	O
often	O
referred	O
to	O
as	O
the	O
log-loss	B
on	O
the	O
basis	O
of	O
this	O
definition	O
it	O
is	O
immediate	O
that	O
the	O
maximum	B
likelihood	I
principle	O
is	O
equivalent	O
to	O
minimizing	O
the	O
empirical	B
risk	B
with	O
respect	O
to	O
the	O
loss	B
function	B
given	O
in	O
equation	O
that	O
is	O
argmin	O
logp	O
argmax	O
logp	O
assuming	O
that	O
the	O
data	O
is	O
distributed	O
according	O
to	O
a	O
distribution	O
p	O
necessarily	O
of	O
the	O
parametric	O
form	O
we	O
employ	O
the	O
true	O
risk	B
of	O
a	O
parameter	O
becomes	O
e	O
x	O
x	O
x	O
x	O
px	O
logp	O
px	O
p	O
x	O
px	O
px	O
log	O
hp	O
px	O
log	O
drepp	O
where	O
dre	O
is	O
called	O
the	O
relative	B
entropy	B
and	O
h	O
is	O
called	O
the	O
entropy	B
function	B
the	O
relative	B
entropy	B
is	O
a	O
divergence	O
measure	O
between	O
two	O
probabilities	O
for	O
discrete	O
variables	O
it	O
is	O
always	O
nonnegative	O
and	O
is	O
equal	O
to	O
only	O
if	O
the	O
two	O
distributions	O
are	O
the	O
same	O
it	O
follows	O
that	O
the	O
true	O
risk	B
is	O
minimal	O
when	O
p	O
p	O
the	O
expression	O
given	O
in	O
equation	O
underscores	O
how	O
our	O
generative	O
assumption	O
affects	O
our	O
density	O
estimation	O
even	O
in	O
the	O
limit	O
of	O
infinite	O
data	O
it	O
shows	O
that	O
if	O
the	O
underlying	O
distribution	O
is	O
indeed	O
of	O
a	O
parametric	O
form	O
then	O
by	O
choosing	O
the	O
correct	O
parameter	O
we	O
can	O
make	O
the	O
risk	B
be	O
the	O
entropy	B
of	O
the	O
distribution	O
however	O
if	O
the	O
distribution	O
is	O
not	O
of	O
the	O
assumed	O
parametric	O
form	O
even	O
the	O
best	O
parameter	O
leads	O
to	O
an	O
inferior	O
model	O
and	O
the	O
suboptimality	O
is	O
measured	O
by	O
the	O
relative	B
entropy	B
divergence	O
generalization	O
analysis	O
how	O
good	O
is	O
the	O
maximum	B
likelihood	I
estimator	O
when	O
we	O
learn	O
from	O
a	O
finite	O
training	B
set	B
generative	B
models	I
to	O
answer	O
this	O
question	O
we	O
need	O
to	O
define	O
how	O
we	O
assess	O
the	O
quality	O
of	O
an	O
approximated	O
solution	O
of	O
the	O
density	O
estimation	O
problem	O
unlike	O
discriminative	B
learning	O
where	O
there	O
is	O
a	O
clear	O
notion	O
of	O
loss	B
in	O
generative	O
learning	O
there	O
are	O
various	O
ways	O
to	O
define	O
the	O
loss	B
of	O
a	O
model	O
on	O
the	O
basis	O
of	O
the	O
previous	O
subsection	O
one	O
natural	O
candidate	O
is	O
the	O
expected	O
log-loss	B
as	O
given	O
in	O
equation	O
in	O
some	O
situations	O
it	O
is	O
easy	O
to	O
prove	O
that	O
the	O
maximum	B
likelihood	I
principle	O
guarantees	O
low	O
true	O
risk	B
as	O
well	O
for	O
example	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
mean	O
of	O
a	O
gaussian	O
variable	O
of	O
unit	O
variance	O
we	O
saw	O
previously	O
that	O
the	O
maximum	B
likelihood	I
estimator	O
is	O
the	O
average	O
i	O
xi	O
let	O
be	O
the	O
optimal	O
m	O
parameter	O
then	O
e	O
x	O
n	O
x	O
x	O
e	O
x	O
n	O
log	O
p	O
e	O
x	O
n	O
e	O
x	O
n	O
next	O
we	O
note	O
that	O
is	O
the	O
average	O
of	O
m	O
gaussian	O
variables	O
and	O
therefore	O
it	O
is	O
also	O
distributed	O
normally	O
with	O
mean	O
and	O
variance	O
from	O
this	O
fact	O
we	O
can	O
derive	O
bounds	O
of	O
the	O
form	O
with	O
probability	O
of	O
at	O
least	O
we	O
have	O
that	O
where	O
depends	O
on	O
and	O
on	O
in	O
some	O
situations	O
the	O
maximum	B
likelihood	I
estimator	O
clearly	O
overfits	O
for	O
example	O
consider	O
a	O
bernoulli	O
random	O
variable	O
x	O
and	O
let	O
px	O
as	O
we	O
saw	O
previously	O
using	O
hoeffding	O
s	O
inequality	O
we	O
can	O
easily	O
derive	O
a	O
guarantee	O
on	O
that	O
holds	O
with	O
high	O
probability	O
equation	O
however	O
if	O
our	O
goal	O
is	O
to	O
obtain	O
a	O
small	O
value	O
of	O
the	O
expected	O
log-loss	B
function	B
as	O
defined	O
in	O
equation	O
we	O
might	O
fail	O
for	O
example	O
assume	O
that	O
is	O
nonzero	O
but	O
very	O
small	O
then	O
the	O
probability	O
that	O
no	O
element	O
of	O
a	O
sample	O
of	O
size	O
m	O
will	O
be	O
is	O
which	O
is	O
greater	O
than	O
e	O
m	O
it	O
follows	O
that	O
whenever	O
m	O
the	O
probability	O
that	O
the	O
sample	O
is	O
all	O
zeros	O
is	O
at	O
least	O
and	O
in	O
that	O
case	O
the	O
maximum	B
likelihood	I
rule	O
will	O
set	B
but	O
the	O
true	O
risk	B
of	O
the	O
estimate	O
is	O
e	O
x	O
x	O
this	O
simple	O
example	O
shows	O
that	O
we	O
should	O
be	O
careful	O
in	O
applying	O
the	O
maximum	B
likelihood	I
principle	O
to	O
overcome	O
overfitting	B
we	O
can	O
use	O
the	O
variety	O
of	O
tools	O
we	O
encountered	O
pre	O
naive	B
bayes	I
viously	O
in	O
the	O
book	O
a	O
simple	O
regularization	B
technique	O
is	O
outlined	O
in	O
exercise	O
naive	B
bayes	I
the	O
naive	B
bayes	I
classifier	B
is	O
a	O
classical	O
demonstration	O
of	O
how	O
generative	O
assumptions	O
and	O
parameter	O
estimations	O
simplify	O
the	O
learning	O
process	O
consider	O
the	O
problem	O
of	O
predicting	O
a	O
label	B
y	O
on	O
the	O
basis	O
of	O
a	O
vector	O
of	O
features	O
x	O
xd	O
where	O
we	O
assume	O
that	O
each	O
xi	O
is	O
in	O
recall	B
that	O
the	O
bayes	B
optimal	I
classifier	B
is	O
hbayesx	O
argmax	O
y	O
py	O
yx	O
x	O
to	O
describe	O
the	O
probability	O
function	B
py	O
yx	O
x	O
we	O
need	O
parameters	O
each	O
of	O
which	O
corresponds	O
to	O
py	O
x	O
for	O
a	O
certain	O
value	O
of	O
x	O
this	O
implies	O
that	O
the	O
number	O
of	O
examples	O
we	O
need	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
features	O
in	O
the	O
naive	B
bayes	I
approach	O
we	O
make	O
the	O
naive	O
generative	O
assumption	O
that	O
given	O
the	O
label	B
the	O
features	O
are	O
independent	O
of	O
each	O
other	O
that	O
is	O
px	O
xy	O
y	O
pxi	O
xiy	O
y	O
with	O
this	O
assumption	O
and	O
using	O
bayes	B
rule	I
the	O
bayes	B
optimal	I
classifier	B
can	O
be	O
further	O
simplified	O
hbayesx	O
argmax	O
y	O
argmax	O
y	O
py	O
yx	O
x	O
py	O
ypx	O
xy	O
y	O
argmax	O
y	O
py	O
y	O
pxi	O
xiy	O
y	O
that	O
is	O
now	O
the	O
number	O
of	O
parameters	O
we	O
need	O
to	O
estimate	O
is	O
only	O
here	O
the	O
generative	O
assumption	O
we	O
made	O
reduced	O
significantly	O
the	O
number	O
of	O
parameters	O
we	O
need	O
to	O
learn	O
when	O
we	O
also	O
estimate	O
the	O
parameters	O
using	O
the	O
maximum	B
likelihood	I
princi	O
ple	O
the	O
resulting	O
classifier	B
is	O
called	O
the	O
naive	B
bayes	I
classifier	B
linear	O
discriminant	O
analysis	O
linear	O
discriminant	O
analysis	O
is	O
another	O
demonstration	O
of	O
how	O
generative	O
assumptions	O
simplify	O
the	O
learning	O
process	O
as	O
in	O
the	O
naive	B
bayes	I
classifier	B
we	O
consider	O
again	O
the	O
problem	O
of	O
predicting	O
a	O
label	B
y	O
on	O
the	O
basis	O
of	O
a	O
generative	B
models	I
vector	O
of	O
features	O
x	O
xd	O
but	O
now	O
the	O
generative	O
assumption	O
is	O
as	O
follows	O
first	O
we	O
assume	O
that	O
py	O
py	O
second	O
we	O
assume	O
that	O
the	O
conditional	O
probability	O
of	O
x	O
given	O
y	O
is	O
a	O
gaussian	O
distribution	O
finally	O
the	O
covariance	O
matrix	O
of	O
the	O
gaussian	O
distribution	O
is	O
the	O
same	O
for	O
both	O
values	O
of	O
the	O
label	B
formally	O
let	O
rd	O
and	O
let	O
be	O
a	O
covariance	O
matrix	O
then	O
the	O
density	O
distribution	O
is	O
given	O
by	O
px	O
xy	O
y	O
exp	O
yt	O
y	O
as	O
we	O
have	O
shown	O
in	O
the	O
previous	O
section	O
using	O
bayes	B
rule	I
we	O
can	O
write	O
hbayesx	O
argmax	O
y	O
py	O
ypx	O
xy	O
y	O
this	O
means	O
that	O
we	O
will	O
predict	O
hbayesx	O
iff	O
xy	O
py	O
xy	O
log	O
this	O
ratio	O
is	O
often	O
called	O
the	O
log-likelihood	O
ratio	O
in	O
our	O
case	O
the	O
log-likelihood	O
ratio	O
becomes	O
we	O
can	O
rewrite	O
this	O
as	O
b	O
where	O
w	O
and	O
b	O
t	O
t	O
as	O
a	O
result	O
of	O
the	O
preceding	O
derivation	O
we	O
obtain	O
that	O
under	O
the	O
aforementioned	O
generative	O
assumptions	O
the	O
bayes	B
optimal	I
classifier	B
is	O
a	O
linear	O
classifier	B
additionally	O
one	O
may	O
train	O
the	O
classifier	B
by	O
estimating	O
the	O
parameter	O
and	O
from	O
the	O
data	O
using	O
for	O
example	O
the	O
maximum	B
likelihood	I
estimator	O
with	O
those	O
estimators	O
at	O
hand	O
the	O
values	O
of	O
w	O
and	O
b	O
can	O
be	O
calculated	O
as	O
in	O
equation	O
latent	B
variables	I
and	O
the	O
em	B
algorithm	O
in	O
generative	B
models	I
we	O
assume	O
that	O
the	O
data	O
is	O
generated	O
by	O
sampling	O
from	O
a	O
specific	O
parametric	O
distribution	O
over	O
our	O
instance	B
space	I
x	O
sometimes	O
it	O
is	O
convenient	O
to	O
express	O
this	O
distribution	O
using	O
latent	O
random	O
variables	O
a	O
natural	O
example	O
is	O
a	O
mixture	O
of	O
k	O
gaussian	O
distributions	O
that	O
is	O
x	O
rd	O
and	O
we	O
assume	O
that	O
each	O
x	O
is	O
generated	O
as	O
follows	O
first	O
we	O
choose	O
a	O
random	O
number	O
in	O
k	O
let	O
y	O
be	O
a	O
random	O
variable	O
corresponding	O
to	O
this	O
choice	O
and	O
denote	O
py	O
y	O
cy	O
second	O
we	O
choose	O
x	O
on	O
the	O
basis	O
of	O
the	O
value	O
of	O
y	O
according	O
to	O
a	O
gaussian	O
distribution	O
y	O
y	O
px	O
xy	O
y	O
exp	O
yt	O
latent	B
variables	I
and	O
the	O
em	B
algorithm	O
therefore	O
the	O
density	O
of	O
x	O
can	O
be	O
written	O
as	O
px	O
x	O
py	O
ypx	O
xy	O
y	O
cy	O
exp	O
yt	O
y	O
y	O
note	O
that	O
y	O
is	O
a	O
hidden	O
variable	O
that	O
we	O
do	O
not	O
observe	O
in	O
our	O
data	O
nevertheless	O
we	O
introduce	O
y	O
since	O
it	O
helps	O
us	O
describe	O
a	O
simple	O
parametric	O
form	O
of	O
the	O
probability	O
of	O
x	O
more	O
generally	O
let	O
be	O
the	O
parameters	O
of	O
the	O
joint	O
distribution	O
of	O
x	O
and	O
y	O
in	O
the	O
preceding	O
example	O
consists	O
of	O
cy	O
y	O
and	O
y	O
for	O
all	O
y	O
k	O
then	O
the	O
log-likelihood	O
of	O
an	O
observation	O
x	O
can	O
be	O
written	O
as	O
log	O
x	O
log	O
p	O
x	O
y	O
y	O
given	O
an	O
i	O
i	O
d	O
sample	O
s	O
xm	O
we	O
would	O
like	O
to	O
find	O
that	O
maxi	O
mizes	O
the	O
log-likelihood	O
of	O
s	O
l	O
log	O
p	O
xi	O
log	O
p	O
xi	O
log	O
p	O
xi	O
y	O
y	O
the	O
maximum-likelihood	O
estimator	O
is	O
therefore	O
the	O
solution	O
of	O
the	O
maximization	O
problem	O
argmax	O
l	O
argmax	O
log	O
p	O
xi	O
y	O
y	O
in	O
many	O
situations	O
the	O
summation	O
inside	O
the	O
log	O
makes	O
the	O
preceding	O
optimization	O
problem	O
computationally	O
hard	O
the	O
expectation-maximization	O
algorithm	O
due	O
to	O
dempster	O
laird	O
and	O
rubin	O
is	O
an	O
iterative	O
procedure	O
for	O
searching	O
a	O
maximum	O
of	O
l	O
while	O
em	B
is	O
not	O
guaranteed	O
to	O
find	O
the	O
global	O
maximum	O
it	O
often	O
works	O
reasonably	O
well	O
in	O
practice	O
em	B
is	O
designed	O
for	O
those	O
cases	O
in	O
which	O
had	O
we	O
known	O
the	O
values	O
of	O
the	O
latent	B
variables	I
y	O
then	O
the	O
maximum	B
likelihood	I
optimization	O
problem	O
would	O
have	O
been	O
tractable	O
more	O
precisely	O
define	O
the	O
following	O
function	B
over	O
m	O
k	O
matrices	O
and	O
the	O
set	B
of	O
parameters	O
f	O
qiy	O
log	O
xi	O
y	O
y	O
generative	B
models	I
if	O
each	O
row	O
of	O
q	O
defines	O
a	O
probability	O
over	O
the	O
ith	O
latent	O
variable	O
given	O
x	O
xi	O
then	O
we	O
can	O
interpret	O
f	O
as	O
the	O
expected	O
log-likelihood	O
of	O
a	O
training	B
set	B
ym	O
where	O
the	O
expectation	O
is	O
with	O
respect	O
to	O
the	O
choice	O
of	O
each	O
yi	O
on	O
the	O
basis	O
of	O
the	O
ith	O
row	O
of	O
q	O
in	O
the	O
definition	O
of	O
f	O
the	O
summation	O
is	O
outside	O
the	O
log	O
and	O
we	O
assume	O
that	O
this	O
makes	O
the	O
optimization	O
problem	O
with	O
respect	O
to	O
tractable	O
assumption	O
for	O
any	O
matrix	O
q	O
such	O
that	O
each	O
row	O
of	O
q	O
sums	O
to	O
the	O
optimization	O
problem	O
is	O
tractable	O
argmax	O
f	O
the	O
intuitive	O
idea	O
of	O
em	B
is	O
that	O
we	O
have	O
a	O
chicken	O
and	O
egg	O
problem	O
on	O
one	O
hand	O
had	O
we	O
known	O
q	O
then	O
by	O
our	O
assumption	O
the	O
optimization	O
problem	O
of	O
finding	O
the	O
best	O
is	O
tractable	O
on	O
the	O
other	O
hand	O
had	O
we	O
known	O
the	O
parameters	O
we	O
could	O
have	O
set	B
qiy	O
to	O
be	O
the	O
probability	O
of	O
y	O
y	O
given	O
that	O
x	O
xi	O
the	O
em	B
algorithm	O
therefore	O
alternates	O
between	O
finding	O
given	O
q	O
and	O
finding	O
q	O
given	O
formally	O
em	B
finds	O
a	O
sequence	O
of	O
solutions	O
where	O
at	O
iteration	O
t	O
we	O
construct	O
by	O
performing	O
two	O
steps	O
expectation	O
step	O
set	B
iy	O
p	O
yx	O
xi	O
this	O
step	O
is	O
called	O
the	O
expectation	O
step	O
because	O
it	O
yields	O
a	O
new	O
probability	O
over	O
the	O
latent	B
variables	I
which	O
defines	O
a	O
new	O
expected	O
log-likelihood	O
function	B
over	O
maximization	O
step	O
set	B
to	O
be	O
the	O
maximizer	O
of	O
the	O
expected	O
log	O
likelihood	O
where	O
the	O
expectation	O
is	O
according	O
to	O
argmax	O
f	O
by	O
our	O
assumption	O
it	O
is	O
possible	O
to	O
solve	O
this	O
optimization	O
problem	O
efficiently	O
the	O
initial	O
values	O
of	O
and	O
are	O
usually	O
chosen	O
at	O
random	O
and	O
the	O
procedure	O
terminates	O
after	O
the	O
improvement	O
in	O
the	O
likelihood	O
value	O
stops	O
being	O
significant	O
em	B
as	O
an	O
alternate	O
maximization	O
algorithm	O
to	O
analyze	O
the	O
em	B
algorithm	O
we	O
first	O
view	O
it	O
as	O
an	O
alternate	O
maximization	O
algorithm	O
define	O
the	O
following	O
objective	O
function	B
gq	O
f	O
qiy	O
logqiy	O
latent	B
variables	I
and	O
the	O
em	B
algorithm	O
the	O
second	O
term	O
is	O
the	O
sum	O
of	O
the	O
entropies	O
of	O
the	O
rows	O
of	O
q	O
let	O
q	O
q	O
i	O
qiy	O
be	O
the	O
set	B
of	O
matrices	O
whose	O
rows	O
define	O
probabilities	O
over	O
the	O
following	O
lemma	O
shows	O
that	O
em	B
performs	O
alternate	O
maximization	O
iterations	O
for	O
maximizing	O
g	O
lemma	O
the	O
em	B
procedure	O
can	O
be	O
rewritten	O
as	O
argmax	O
q	O
q	O
argmax	O
gq	O
furthermore	O
l	O
proof	O
given	O
we	O
clearly	O
have	O
that	O
argmax	O
argmax	O
f	O
therefore	O
we	O
only	O
need	O
to	O
show	O
that	O
for	O
any	O
the	O
solution	O
of	O
argmaxq	O
q	O
gq	O
is	O
to	O
set	B
qiy	O
p	O
yx	O
xi	O
indeed	O
by	O
jensen	O
s	O
inequality	O
for	O
any	O
q	O
q	O
we	O
have	O
that	O
gq	O
qiy	O
log	O
xi	O
y	O
y	O
p	O
xi	O
y	O
y	O
qiy	O
qiy	O
qiy	O
log	O
log	O
p	O
xi	O
y	O
y	O
log	O
xi	O
l	O
generative	B
models	I
while	O
for	O
qiy	O
p	O
yx	O
xi	O
we	O
have	O
p	O
yx	O
xi	O
log	O
xi	O
y	O
y	O
p	O
yx	O
xi	O
gq	O
p	O
yx	O
xi	O
log	O
xi	O
log	O
xi	O
log	O
xi	O
l	O
p	O
yx	O
xi	O
this	O
shows	O
that	O
setting	O
qiy	O
p	O
yx	O
xi	O
maximizes	O
gq	O
over	O
q	O
q	O
and	O
shows	O
that	O
l	O
the	O
preceding	O
lemma	O
immediately	O
implies	O
theorem	O
the	O
em	B
procedure	O
never	O
decreases	O
the	O
log-likelihood	O
namely	O
for	O
all	O
t	O
l	O
l	O
proof	O
by	O
the	O
lemma	O
we	O
have	O
l	O
l	O
em	B
for	O
mixture	B
of	I
gaussians	I
k-means	B
consider	O
the	O
case	O
of	O
a	O
mixture	O
of	O
k	O
gaussians	O
in	O
which	O
is	O
a	O
triplet	O
k	O
k	O
where	O
p	O
y	O
cy	O
and	O
p	O
xy	O
y	O
is	O
as	O
given	O
in	O
equation	O
for	O
simplicity	O
we	O
assume	O
that	O
k	O
i	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
specifying	O
the	O
em	B
algorithm	O
for	O
this	O
case	O
we	O
obtain	O
the	O
following	O
expectation	O
step	O
for	O
each	O
i	O
and	O
y	O
we	O
have	O
that	O
p	O
yx	O
xi	O
p	O
yp	O
xiy	O
y	O
where	O
zi	O
is	O
a	O
normalization	O
factor	O
which	O
ensures	O
y	O
y	O
p	O
yx	O
maximization	O
step	O
we	O
need	O
to	O
set	B
to	O
be	O
a	O
maximizer	O
of	O
equation	O
xi	O
sums	O
to	O
ct	O
y	O
exp	O
zi	O
zi	O
bayesian	B
reasoning	I
which	O
in	O
our	O
case	O
amounts	O
to	O
maximizing	O
the	O
following	O
expression	O
w	O
r	O
t	O
c	O
and	O
p	O
yx	O
xi	O
logcy	O
comparing	O
the	O
derivative	O
of	O
equation	O
w	O
r	O
t	O
y	O
to	O
zero	O
and	O
rearranging	O
terms	O
we	O
obtain	O
p	O
yx	O
xi	O
xi	O
p	O
yx	O
xi	O
y	O
that	O
is	O
y	O
is	O
a	O
weighted	O
average	O
of	O
the	O
xi	O
where	O
the	O
weights	O
are	O
according	O
to	O
the	O
probabilities	O
calculated	O
in	O
the	O
e	O
step	O
to	O
find	O
the	O
optimal	O
c	O
we	O
need	O
to	O
be	O
more	O
careful	O
since	O
we	O
must	O
ensure	O
that	O
c	O
is	O
a	O
probability	O
vector	O
in	O
exercise	O
we	O
show	O
that	O
the	O
solution	O
is	O
p	O
yx	O
xi	O
p	O
xi	O
cy	O
it	O
is	O
interesting	O
to	O
compare	O
the	O
preceding	O
algorithm	O
to	O
the	O
k-means	B
algorithm	O
described	O
in	O
chapter	O
in	O
the	O
k-means	B
algorithm	O
we	O
first	O
assign	O
each	O
example	O
to	O
a	O
cluster	O
according	O
to	O
the	O
distance	O
then	O
we	O
update	O
each	O
center	O
y	O
according	O
to	O
the	O
average	O
of	O
the	O
examples	O
assigned	O
to	O
this	O
cluster	O
in	O
the	O
em	B
approach	O
however	O
we	O
determine	O
the	O
probability	O
that	O
each	O
example	O
belongs	O
to	O
each	O
cluster	O
then	O
we	O
update	O
the	O
centers	O
on	O
the	O
basis	O
of	O
a	O
weighted	O
sum	O
over	O
the	O
entire	O
sample	O
for	O
this	O
reason	O
the	O
em	B
approach	O
for	O
k-means	B
is	O
sometimes	O
called	O
soft	B
k-means	B
bayesian	B
reasoning	I
the	O
maximum	B
likelihood	I
estimator	O
follows	O
a	O
frequentist	B
approach	O
this	O
means	O
that	O
we	O
refer	O
to	O
the	O
parameter	O
as	O
a	O
fixed	O
parameter	O
and	O
the	O
only	O
problem	O
is	O
that	O
we	O
do	O
not	O
know	O
its	O
value	O
a	O
different	O
approach	O
to	O
parameter	O
estimation	O
is	O
called	O
bayesian	B
reasoning	I
in	O
the	O
bayesian	O
approach	O
our	O
uncertainty	O
about	O
is	O
also	O
modeled	O
using	O
probability	O
theory	O
that	O
is	O
we	O
think	O
of	O
as	O
a	O
random	O
variable	O
as	O
well	O
and	O
refer	O
to	O
the	O
distribution	O
p	O
as	O
a	O
prior	O
distribution	O
as	O
its	O
name	O
indicates	O
the	O
prior	O
distribution	O
should	O
be	O
defined	O
by	O
the	O
learner	O
prior	O
to	O
observing	O
the	O
data	O
as	O
an	O
example	O
let	O
us	O
consider	O
again	O
the	O
drug	O
company	O
which	O
developed	O
a	O
new	O
drug	O
on	O
the	O
basis	O
of	O
past	O
experience	O
the	O
statisticians	O
at	O
the	O
drug	O
company	O
believe	O
that	O
whenever	O
a	O
drug	O
has	O
reached	O
the	O
level	O
of	O
clinic	O
experiments	O
on	O
people	O
it	O
is	O
likely	O
to	O
be	O
effective	O
they	O
model	O
this	O
prior	O
belief	O
by	O
defining	O
a	O
density	O
distribution	O
on	O
such	O
that	O
p	O
if	O
if	O
generative	B
models	I
as	O
before	O
given	O
a	O
specific	O
value	O
of	O
it	O
is	O
assumed	O
that	O
the	O
conditional	O
probability	O
px	O
x	O
is	O
known	O
in	O
the	O
drug	O
company	O
example	O
x	O
takes	O
values	O
in	O
and	O
px	O
x	O
x	O
once	O
the	O
prior	O
distribution	O
over	O
and	O
the	O
conditional	O
distribution	O
over	O
x	O
given	O
are	O
defined	O
we	O
again	O
have	O
complete	O
knowledge	O
of	O
the	O
distribution	O
over	O
x	O
this	O
is	O
because	O
we	O
can	O
write	O
the	O
probability	O
over	O
x	O
as	O
a	O
marginal	O
probability	O
px	O
x	O
px	O
x	O
p	O
x	O
where	O
the	O
last	O
equality	O
follows	O
from	O
the	O
definition	O
of	O
conditional	O
probability	O
if	O
is	O
continuous	O
we	O
replace	O
p	O
with	O
the	O
density	O
function	B
and	O
the	O
sum	O
becomes	O
an	O
integral	O
px	O
x	O
p	O
x	O
d	O
seemingly	O
once	O
we	O
know	O
px	O
x	O
a	O
training	B
set	B
s	O
xm	O
tells	O
us	O
nothing	O
as	O
we	O
are	O
already	O
experts	O
who	O
know	O
the	O
distribution	O
over	O
a	O
new	O
point	O
x	O
however	O
the	O
bayesian	O
view	O
introduces	O
dependency	O
between	O
s	O
and	O
x	O
this	O
is	O
because	O
we	O
now	O
refer	O
to	O
as	O
a	O
random	O
variable	O
a	O
new	O
point	O
x	O
and	O
the	O
previous	O
points	O
in	O
s	O
are	O
independent	O
only	O
conditioned	O
on	O
this	O
is	O
different	O
from	O
the	O
frequentist	B
philosophy	O
in	O
which	O
is	O
a	O
parameter	O
that	O
we	O
might	O
not	O
know	O
but	O
since	O
it	O
is	O
just	O
a	O
parameter	O
of	O
the	O
distribution	O
a	O
new	O
point	O
x	O
and	O
previous	O
points	O
s	O
are	O
always	O
independent	O
in	O
the	O
bayesian	O
framework	O
since	O
x	O
and	O
s	O
are	O
not	O
independent	O
anymore	O
what	O
we	O
would	O
like	O
to	O
calculate	O
is	O
the	O
probability	O
of	O
x	O
given	O
s	O
which	O
by	O
the	O
chain	O
rule	O
can	O
be	O
written	O
as	O
follows	O
px	O
xs	O
px	O
x	O
sp	O
px	O
x	O
the	O
second	O
inequality	O
follows	O
from	O
the	O
assumption	O
that	O
x	O
and	O
s	O
are	O
independent	O
when	O
we	O
condition	O
on	O
using	O
bayes	B
rule	I
we	O
have	O
ps	O
p	O
ps	O
and	O
together	O
with	O
the	O
assumption	O
that	O
points	O
are	O
independent	O
conditioned	O
on	O
we	O
can	O
write	O
p	O
ps	O
ps	O
ps	O
px	O
xi	O
we	O
therefore	O
obtain	O
the	O
following	O
expression	O
for	O
bayesian	O
prediction	O
px	O
xs	O
ps	O
px	O
x	O
px	O
xi	O
getting	O
back	O
to	O
our	O
drug	O
company	O
example	O
we	O
can	O
rewrite	O
px	O
xs	O
as	O
i	O
xi	O
p	O
d	O
px	O
xs	O
p	O
summary	O
it	O
is	O
interesting	O
to	O
note	O
that	O
when	O
p	O
is	O
uniform	O
we	O
obtain	O
that	O
px	O
xs	O
xi	O
d	O
solving	O
the	O
preceding	O
integral	O
integration	O
by	O
parts	O
we	O
obtain	O
i	O
i	O
xi	O
m	O
px	O
recall	B
that	O
the	O
prediction	O
according	O
to	O
the	O
maximum	B
likelihood	I
principle	O
in	O
this	O
case	O
is	O
px	O
i	O
xi	O
m	O
the	O
bayesian	O
prediction	O
with	O
uniform	O
prior	O
is	O
rather	O
similar	O
to	O
the	O
maximum	B
likelihood	I
prediction	O
except	O
it	O
adds	O
pseudoexamples	O
to	O
the	O
training	B
set	B
thus	O
biasing	O
the	O
prediction	O
toward	O
the	O
uniform	O
prior	O
maximum	O
a	O
posteriori	O
in	O
many	O
situations	O
it	O
is	O
difficult	O
to	O
find	O
a	O
closed	O
form	O
solution	O
to	O
the	O
integral	O
given	O
in	O
equation	O
several	O
numerical	O
methods	O
can	O
be	O
used	O
to	O
approximate	O
this	O
integral	O
another	O
popular	O
solution	O
is	O
to	O
find	O
a	O
single	O
which	O
maximizes	O
p	O
the	O
value	O
of	O
which	O
maximizes	O
p	O
is	O
called	O
the	O
maximum	O
a	O
posteriori	O
estimator	O
once	O
this	O
value	O
is	O
found	O
we	O
can	O
calculate	O
the	O
probability	O
that	O
x	O
x	O
given	O
the	O
maximum	O
a	O
posteriori	O
estimator	O
and	O
independently	O
on	O
s	O
summary	O
in	O
the	O
generative	O
approach	O
to	O
machine	O
learning	O
we	O
aim	O
at	O
modeling	O
the	O
distribution	O
over	O
the	O
data	O
in	O
particular	O
in	O
parametric	B
density	I
estimation	I
we	O
further	O
assume	O
that	O
the	O
underlying	O
distribution	O
over	O
the	O
data	O
has	O
a	O
specific	O
parametric	O
form	O
and	O
our	O
goal	O
is	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
model	O
we	O
have	O
described	O
several	O
principles	O
for	O
parameter	O
estimation	O
including	O
maximum	B
likelihood	I
bayesian	O
estimation	O
and	O
maximum	O
a	O
posteriori	O
we	O
have	O
also	O
described	O
several	O
specific	O
algorithms	O
for	O
implementing	O
the	O
maximum	B
likelihood	I
under	O
different	O
assumptions	O
on	O
the	O
underlying	O
data	O
distribution	O
in	O
particular	O
naive	B
bayes	I
lda	B
and	O
em	B
bibliographic	O
remarks	O
the	O
maximum	B
likelihood	I
principle	O
was	O
studied	O
by	O
ronald	O
fisher	O
in	O
the	O
beginning	O
of	O
the	O
century	O
bayesian	O
statistics	O
follow	O
bayes	B
rule	I
which	O
is	O
named	O
after	O
the	O
century	O
english	O
mathematician	O
thomas	O
bayes	O
there	O
are	O
many	O
excellent	O
books	O
on	O
the	O
generative	O
and	O
bayesian	O
approaches	O
to	O
machine	O
learning	O
see	O
for	O
example	O
koller	O
friedman	O
mackay	O
murphy	O
barber	O
generative	B
models	I
exercises	O
prove	O
that	O
the	O
maximum	B
likelihood	I
estimator	O
of	O
the	O
variance	O
of	O
a	O
gaussian	O
variable	O
is	O
biased	O
regularization	B
for	O
maximum	B
likelihood	I
consider	O
the	O
following	O
regularized	O
loss	B
minimization	O
m	O
m	O
show	O
that	O
the	O
preceding	O
objective	O
is	O
equivalent	O
to	O
the	O
usual	O
empirical	B
error	I
had	O
we	O
added	O
two	O
pseudoexamples	O
to	O
the	O
training	B
set	B
conclude	O
that	O
the	O
regularized	O
maximum	B
likelihood	I
estimator	O
would	O
be	O
m	O
xi	O
derive	O
a	O
high	O
probability	O
bound	O
on	O
hint	O
rewrite	O
this	O
as	O
e	O
e	O
and	O
then	O
use	O
the	O
triangle	O
inequality	O
and	O
hoeffding	O
inequality	O
use	O
this	O
to	O
bound	O
the	O
true	O
risk	B
hint	O
use	O
the	O
fact	O
that	O
now	O
to	O
relate	O
to	O
the	O
relative	B
entropy	B
consider	O
a	O
general	O
optimization	O
problem	O
of	O
the	O
form	O
y	O
logcy	O
s	O
t	O
cy	O
cy	O
y	O
is	O
a	O
vector	O
of	O
nonnegative	O
weights	O
verify	O
that	O
the	O
m	O
step	O
where	O
rk	O
of	O
soft	B
k-means	B
involves	O
solving	O
such	O
an	O
optimization	O
problem	O
let	O
show	O
that	O
the	O
optimization	O
problem	O
is	O
equivalent	O
to	O
the	O
problem	O
show	O
that	O
is	O
a	O
probability	O
vector	O
y	O
y	O
min	O
c	O
s	O
t	O
cy	O
cy	O
using	O
properties	O
of	O
the	O
relative	B
entropy	B
conclude	O
that	O
is	O
the	O
solution	O
to	O
the	O
optimization	O
problem	O
max	O
c	O
y	O
feature	B
selection	I
and	O
generation	O
in	O
the	O
beginning	O
of	O
the	O
book	O
we	O
discussed	O
the	O
abstract	O
model	O
of	O
learning	O
in	O
which	O
the	O
prior	B
knowledge	I
utilized	O
by	O
the	O
learner	O
is	O
fully	O
encoded	O
by	O
the	O
choice	O
of	O
the	O
hypothesis	B
class	I
however	O
there	O
is	O
another	O
modeling	O
choice	O
which	O
we	O
have	O
so	O
far	O
ignored	O
how	O
do	O
we	O
represent	O
the	O
instance	B
space	I
x	O
for	O
example	O
in	O
the	O
papayas	O
learning	O
problem	O
we	O
proposed	O
the	O
hypothesis	B
class	I
of	O
rectangles	O
in	O
the	O
softness-color	O
two	O
dimensional	O
plane	O
that	O
is	O
our	O
first	O
modeling	O
choice	O
was	O
to	O
represent	O
a	O
papaya	O
as	O
a	O
two	O
dimensional	O
point	O
corresponding	O
to	O
its	O
softness	O
and	O
color	O
only	O
after	O
that	O
did	O
we	O
choose	O
the	O
hypothesis	B
class	I
of	O
rectangles	O
as	O
a	O
class	O
of	O
mappings	O
from	O
the	O
plane	O
into	O
the	O
label	B
set	B
the	O
transformation	O
from	O
the	O
real	O
world	O
object	O
papaya	O
into	O
the	O
scalar	O
representing	O
its	O
softness	O
or	O
its	O
color	O
is	O
called	O
a	O
feature	B
function	B
or	O
a	O
feature	B
for	O
short	O
namely	O
any	O
measurement	O
of	O
the	O
real	O
world	O
object	O
can	O
be	O
regarded	O
as	O
a	O
feature	B
if	O
x	O
is	O
a	O
subset	O
of	O
a	O
vector	O
space	O
each	O
x	O
x	O
is	O
sometimes	O
referred	O
to	O
as	O
a	O
feature	B
vector	O
it	O
is	O
important	O
to	O
understand	O
that	O
the	O
way	O
we	O
encode	O
real	O
world	O
objects	O
as	O
an	O
instance	B
space	I
x	O
is	O
by	O
itself	O
prior	B
knowledge	I
about	O
the	O
problem	O
furthermore	O
even	O
when	O
we	O
already	O
have	O
an	O
instance	B
space	I
x	O
which	O
is	O
represented	O
as	O
a	O
subset	O
of	O
a	O
vector	O
space	O
we	O
might	O
still	O
want	O
to	O
change	O
it	O
into	O
a	O
different	O
representation	O
and	O
apply	O
a	O
hypothesis	B
class	I
on	O
top	O
of	O
it	O
that	O
is	O
we	O
may	O
define	O
a	O
hypothesis	B
class	I
on	O
x	O
by	O
composing	O
some	O
class	O
h	O
on	O
top	O
of	O
a	O
feature	B
function	B
which	O
maps	O
x	O
into	O
some	O
other	O
vector	O
space	O
x	O
we	O
have	O
already	O
encountered	O
examples	O
of	O
such	O
compositions	O
in	O
chapter	O
we	O
saw	O
that	O
kernel-based	O
svm	B
learns	O
a	O
composition	O
of	O
the	O
class	O
of	O
halfspaces	O
over	O
a	O
feature	B
mapping	O
that	O
maps	O
each	O
original	O
instance	B
in	O
x	O
into	O
some	O
hilbert	B
space	I
and	O
indeed	O
the	O
choice	O
of	O
is	O
another	O
form	O
of	O
prior	B
knowledge	I
we	O
impose	O
on	O
the	O
problem	O
in	O
this	O
chapter	O
we	O
study	O
several	O
methods	O
for	O
constructing	O
a	O
good	O
feature	B
set	B
we	O
start	O
with	O
the	O
problem	O
of	O
feature	B
selection	I
in	O
which	O
we	O
have	O
a	O
large	O
pool	O
of	O
features	O
and	O
our	O
goal	O
is	O
to	O
select	O
a	O
small	O
number	O
of	O
features	O
that	O
will	O
be	O
used	O
by	O
our	O
predictor	B
next	O
we	O
discuss	O
feature	B
manipulations	O
and	O
normalization	O
these	O
include	O
simple	O
transformations	O
that	O
we	O
apply	O
on	O
our	O
original	O
features	O
such	O
transformations	O
may	O
decrease	O
the	O
sample	B
complexity	I
of	O
our	O
learning	O
algorithm	O
its	O
bias	B
or	O
its	O
computational	B
complexity	I
last	O
we	O
discuss	O
several	O
approaches	O
for	O
feature	B
learning	I
in	O
these	O
methods	O
we	O
try	O
to	O
automate	O
the	O
process	O
of	O
feature	B
construction	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
feature	B
selection	I
and	O
generation	O
we	O
emphasize	O
that	O
while	O
there	O
are	O
some	O
common	O
techniques	O
for	O
feature	B
learning	I
one	O
may	O
want	O
to	O
try	O
the	O
no-free-lunch	B
theorem	O
implies	O
that	O
there	O
is	O
no	O
ultimate	O
feature	B
learner	O
any	O
feature	B
learning	I
algorithm	O
might	O
fail	O
on	O
some	O
problem	O
in	O
other	O
words	O
the	O
success	O
of	O
each	O
feature	B
learner	O
relies	O
implicitly	O
on	O
some	O
form	O
of	O
prior	O
assumption	O
on	O
the	O
data	O
distribution	O
furthermore	O
the	O
relative	O
quality	O
of	O
features	O
highly	O
depends	O
on	O
the	O
learning	O
algorithm	O
we	O
are	O
later	O
going	O
to	O
apply	O
using	O
these	O
features	O
this	O
is	O
illustrated	O
in	O
the	O
following	O
example	O
example	O
consider	O
a	O
regression	B
problem	O
in	O
which	O
x	O
y	O
r	O
and	O
the	O
loss	B
function	B
is	O
the	O
squared	O
loss	B
suppose	O
that	O
the	O
underlying	O
distribution	O
is	O
such	O
that	O
an	O
example	O
y	O
is	O
generated	O
as	O
follows	O
first	O
we	O
sample	O
from	O
the	O
uniform	O
distribution	O
over	O
then	O
we	O
deterministically	O
set	B
y	O
finally	O
the	O
second	O
feature	B
is	O
set	B
to	O
be	O
y	O
z	O
where	O
z	O
is	O
sampled	O
from	O
the	O
uniform	O
distribution	O
over	O
suppose	O
we	O
would	O
like	O
to	O
choose	O
a	O
single	O
feature	B
intuitively	O
the	O
first	O
feature	B
should	O
be	O
preferred	O
over	O
the	O
second	O
feature	B
as	O
the	O
target	O
can	O
be	O
perfectly	O
predicted	O
based	O
on	O
the	O
first	O
feature	B
alone	O
while	O
it	O
cannot	O
be	O
perfectly	O
predicted	O
based	O
on	O
the	O
second	O
feature	B
indeed	O
choosing	O
the	O
first	O
feature	B
would	O
be	O
the	O
right	O
choice	O
if	O
we	O
are	O
later	O
going	O
to	O
apply	O
polynomial	B
regression	B
of	O
degree	O
at	O
least	O
however	O
if	O
the	O
learner	O
is	O
going	O
to	O
be	O
a	O
linear	O
regressor	O
then	O
we	O
should	O
prefer	O
the	O
second	O
feature	B
over	O
the	O
first	O
one	O
since	O
the	O
optimal	O
linear	B
predictor	B
based	O
on	O
the	O
first	O
feature	B
will	O
have	O
a	O
larger	O
risk	B
than	O
the	O
optimal	O
linear	B
predictor	B
based	O
on	O
the	O
second	O
feature	B
feature	B
selection	I
throughout	O
this	O
section	O
we	O
assume	O
that	O
x	O
rd	O
that	O
is	O
each	O
instance	B
is	O
represented	O
as	O
a	O
vector	O
of	O
d	O
features	O
our	O
goal	O
is	O
to	O
learn	O
a	O
predictor	B
that	O
only	O
relies	O
on	O
k	O
d	O
features	O
predictors	O
that	O
use	O
only	O
a	O
small	O
subset	O
of	O
features	O
require	O
a	O
smaller	O
memory	O
footprint	O
and	O
can	O
be	O
applied	O
faster	O
furthermore	O
in	O
applications	O
such	O
as	O
medical	O
diagnostics	O
obtaining	O
each	O
possible	O
feature	B
test	O
result	O
can	O
be	O
costly	O
therefore	O
a	O
predictor	B
that	O
uses	O
only	O
a	O
small	O
number	O
of	O
features	O
is	O
desirable	O
even	O
at	O
the	O
cost	O
of	O
a	O
small	O
degradation	O
in	O
performance	O
relative	O
to	O
a	O
predictor	B
that	O
uses	O
more	O
features	O
finally	O
constraining	O
the	O
hypothesis	B
class	I
to	O
use	O
a	O
small	O
subset	O
of	O
features	O
can	O
reduce	O
its	O
estimation	B
error	I
and	O
thus	O
prevent	O
overfitting	B
ideally	O
we	O
could	O
have	O
tried	O
all	O
subsets	O
of	O
k	O
out	O
of	O
d	O
features	O
and	O
choose	O
the	O
subset	O
which	O
leads	O
to	O
the	O
best	O
performing	O
predictor	B
however	O
such	O
an	O
exhaustive	O
search	O
is	O
usually	O
computationally	O
intractable	O
in	O
the	O
following	O
we	O
describe	O
three	O
computationally	O
feasible	B
approaches	O
for	O
feature	B
selection	I
while	O
these	O
methods	O
cannot	O
guarantee	O
finding	O
the	O
optimal	O
subset	O
they	O
often	O
work	O
reasonably	O
well	O
in	O
practice	O
some	O
of	O
the	O
methods	O
come	O
with	O
formal	O
guarantees	O
on	O
the	O
quality	O
of	O
the	O
selected	O
subsets	O
under	O
certain	O
assumptions	O
we	O
do	O
not	O
discuss	O
these	O
guarantees	O
here	O
feature	B
selection	I
filters	O
maybe	O
the	O
simplest	O
approach	O
for	O
feature	B
selection	I
is	O
the	O
filter	O
method	O
in	O
which	O
we	O
assess	O
individual	O
features	O
independently	O
of	O
other	O
features	O
according	O
to	O
some	O
quality	O
measure	O
we	O
can	O
then	O
select	O
the	O
k	O
features	O
that	O
achieve	O
the	O
highest	O
score	O
decide	O
also	O
on	O
the	O
number	O
of	O
features	O
to	O
select	O
according	O
to	O
the	O
value	O
of	O
their	O
scores	O
many	O
quality	O
measures	O
for	O
features	O
have	O
been	O
proposed	O
in	O
the	O
literature	O
maybe	O
the	O
most	O
straightforward	O
approach	O
is	O
to	O
set	B
the	O
score	O
of	O
a	O
feature	B
according	O
to	O
the	O
error	O
rate	O
of	O
a	O
predictor	B
that	O
is	O
trained	O
solely	O
by	O
that	O
feature	B
to	O
illustrate	O
this	O
consider	O
a	O
linear	B
regression	B
problem	O
with	O
the	O
squared	O
loss	B
let	O
v	O
xmj	O
rm	O
be	O
a	O
vector	O
designating	O
the	O
values	O
of	O
the	O
jth	O
feature	B
on	O
a	O
training	B
set	B
of	O
m	O
examples	O
and	O
let	O
y	O
ym	O
rm	O
be	O
the	O
values	O
of	O
the	O
target	O
on	O
the	O
same	O
m	O
examples	O
the	O
empirical	O
squared	O
loss	B
of	O
an	O
erm	B
linear	B
predictor	B
that	O
uses	O
only	O
the	O
jth	O
feature	B
would	O
be	O
min	O
ab	O
r	O
m	O
b	O
where	O
the	O
meaning	O
of	O
adding	O
a	O
scalar	O
b	O
to	O
a	O
vector	O
v	O
is	O
adding	O
b	O
to	O
all	O
coordinates	O
of	O
v	O
to	O
solve	O
this	O
problem	O
let	O
v	O
vi	O
be	O
the	O
averaged	O
value	O
of	O
the	O
m	O
feature	B
and	O
let	O
y	O
yi	O
be	O
the	O
averaged	O
value	O
of	O
the	O
target	O
clearly	O
m	O
exercise	O
min	O
ab	O
r	O
m	O
b	O
min	O
ab	O
r	O
m	O
v	O
b	O
taking	O
the	O
derivative	O
of	O
the	O
right-hand	O
side	O
objective	O
with	O
respect	O
to	O
b	O
and	O
comparing	O
it	O
to	O
zero	O
we	O
obtain	O
that	O
b	O
similarly	O
solving	O
for	O
a	O
we	O
know	O
that	O
b	O
yields	O
a	O
v	O
y	O
plugging	O
this	O
value	O
back	O
into	O
the	O
objective	O
we	O
obtain	O
the	O
value	O
v	O
y	O
ranking	B
the	O
features	O
according	O
to	O
the	O
minimal	O
loss	B
they	O
achieve	O
is	O
equivalent	O
to	O
ranking	B
them	O
according	O
to	O
the	O
absolute	O
value	O
of	O
the	O
following	O
score	O
now	O
a	O
higher	O
score	O
yields	O
a	O
better	O
feature	B
v	O
y	O
m	O
v	O
y	O
the	O
preceding	O
expression	O
is	O
known	O
as	O
pearson	O
s	O
correlation	O
coefficient	O
the	O
numerator	O
is	O
the	O
empirical	O
estimate	O
of	O
the	O
covariance	O
of	O
the	O
jth	O
feature	B
and	O
the	O
target	O
value	O
ev	O
e	O
vy	O
e	O
y	O
while	O
the	O
denominator	O
is	O
the	O
squared	O
root	O
of	O
the	O
empirical	O
estimate	O
for	O
the	O
variance	O
of	O
the	O
jth	O
feature	B
ev	O
e	O
times	O
the	O
variance	O
of	O
the	O
target	O
pearson	O
s	O
coefficient	O
ranges	O
from	O
to	O
where	O
if	O
the	O
pearson	O
s	O
coefficient	O
is	O
either	O
or	O
there	O
is	O
a	O
linear	O
mapping	O
from	O
v	O
to	O
y	O
with	O
zero	O
empirical	B
risk	B
feature	B
selection	I
and	O
generation	O
if	O
pearson	O
s	O
coefficient	O
equals	O
zero	O
it	O
means	O
that	O
the	O
optimal	O
linear	O
function	B
from	O
v	O
to	O
y	O
is	O
the	O
all-zeros	O
function	B
which	O
means	O
that	O
v	O
alone	O
is	O
useless	O
for	O
predicting	O
y	O
however	O
this	O
does	O
not	O
mean	O
that	O
v	O
is	O
a	O
bad	O
feature	B
as	O
it	O
might	O
be	O
the	O
case	O
that	O
together	O
with	O
other	O
features	O
v	O
can	O
perfectly	O
predict	O
y	O
indeed	O
consider	O
a	O
simple	O
example	O
in	O
which	O
the	O
target	O
is	O
generated	O
by	O
the	O
function	B
y	O
assume	O
also	O
that	O
is	O
generated	O
from	O
the	O
uniform	O
distribution	O
over	O
and	O
z	O
where	O
z	O
is	O
also	O
generated	O
i	O
i	O
d	O
from	O
the	O
uniform	O
distribution	O
over	O
then	O
ey	O
and	O
we	O
also	O
have	O
therefore	O
for	O
a	O
large	O
enough	O
training	B
set	B
the	O
first	O
feature	B
is	O
likely	O
to	O
have	O
a	O
pearson	O
s	O
correlation	O
coefficient	O
that	O
is	O
close	O
to	O
zero	O
and	O
hence	O
it	O
will	O
most	O
probably	O
not	O
be	O
selected	O
however	O
no	O
function	B
can	O
predict	O
the	O
target	O
value	O
well	O
without	O
knowing	O
the	O
first	O
feature	B
there	O
are	O
many	O
other	O
score	O
functions	O
that	O
can	O
be	O
used	O
by	O
a	O
filter	O
method	O
notable	O
examples	O
are	O
estimators	O
of	O
the	O
mutual	O
information	O
or	O
the	O
area	O
under	O
the	O
receiver	O
operating	O
characteristic	O
curve	O
all	O
of	O
these	O
score	O
functions	O
suffer	O
from	O
similar	O
problems	O
to	O
the	O
one	O
illustrated	O
previously	O
we	O
refer	O
the	O
reader	O
to	O
guyon	O
elisseeff	O
greedy	O
selection	O
approaches	O
greedy	O
selection	O
is	O
another	O
popular	O
approach	O
for	O
feature	B
selection	I
unlike	O
filter	O
methods	O
greedy	O
selection	O
approaches	O
are	O
coupled	O
with	O
the	O
underlying	O
learning	O
algorithm	O
the	O
simplest	O
instance	B
of	O
greedy	O
selection	O
is	O
forward	B
greedy	I
selection	I
we	O
start	O
with	O
an	O
empty	O
set	B
of	O
features	O
and	O
then	O
we	O
gradually	O
add	O
one	O
feature	B
at	O
a	O
time	O
to	O
the	O
set	B
of	O
selected	O
features	O
given	O
that	O
our	O
current	O
set	B
of	O
selected	O
features	O
is	O
i	O
we	O
go	O
over	O
all	O
i	O
i	O
and	O
apply	O
the	O
learning	O
algorithm	O
on	O
the	O
set	B
of	O
features	O
i	O
each	O
such	O
application	O
yields	O
a	O
different	O
predictor	B
and	O
we	O
choose	O
to	O
add	O
the	O
feature	B
that	O
yields	O
the	O
predictor	B
with	O
the	O
smallest	O
risk	B
the	O
training	B
set	B
or	O
on	O
a	O
validation	B
set	B
this	O
process	O
continues	O
until	O
we	O
either	O
select	O
k	O
features	O
where	O
k	O
is	O
a	O
predefined	O
budget	O
of	O
allowed	O
features	O
or	O
achieve	O
an	O
accurate	O
enough	O
predictor	B
example	O
matching	O
pursuit	O
to	O
illustrate	O
the	O
forward	B
greedy	I
selection	I
approach	O
we	O
specify	O
it	O
to	O
the	O
problem	O
of	O
linear	B
regression	B
with	O
the	O
squared	O
loss	B
let	O
x	O
rmd	O
be	O
a	O
matrix	O
whose	O
rows	O
are	O
the	O
m	O
training	O
instances	O
let	O
y	O
rm	O
be	O
the	O
vector	O
of	O
the	O
m	O
labels	O
for	O
every	O
i	O
let	O
xi	O
be	O
the	O
ith	O
column	O
of	O
x	O
given	O
a	O
set	B
i	O
we	O
denote	O
by	O
xi	O
the	O
matrix	O
whose	O
columns	O
are	O
i	O
i	O
the	O
forward	B
greedy	I
selection	I
method	O
starts	O
with	O
at	O
iteration	O
t	O
we	O
look	O
for	O
the	O
feature	B
index	O
jt	O
which	O
is	O
in	O
argmin	O
j	O
min	O
w	O
rt	O
feature	B
selection	I
then	O
we	O
update	O
it	O
it	O
we	O
now	O
describe	O
a	O
more	O
efficient	O
implementation	O
of	O
the	O
forward	B
greedy	I
selection	I
approach	O
for	O
linear	B
regression	B
which	O
is	O
called	O
orthogonal	O
matching	O
pursuit	O
the	O
idea	O
is	O
to	O
keep	O
an	O
orthogonal	O
basis	O
of	O
the	O
features	O
aggregated	O
so	O
far	O
let	O
vt	O
be	O
a	O
matrix	O
whose	O
columns	O
form	O
an	O
orthonormal	O
basis	O
of	O
the	O
columns	O
of	O
xit	O
clearly	O
min	O
rt	O
min	O
w	O
we	O
will	O
maintain	O
a	O
vector	O
t	O
which	O
minimizes	O
the	O
right-hand	O
side	O
of	O
the	O
equation	O
initially	O
we	O
set	B
and	O
to	O
be	O
the	O
empty	O
vector	O
at	O
round	O
t	O
for	O
every	O
j	O
we	O
decompose	O
xj	O
vj	O
uj	O
where	O
vj	O
vt	O
t	O
is	O
the	O
projection	B
of	O
xj	O
onto	O
the	O
subspace	O
spanned	O
by	O
vt	O
and	O
uj	O
is	O
the	O
part	O
of	O
xj	O
orthogonal	O
to	O
vt	O
appendix	O
c	O
then	O
min	O
min	O
min	O
uj	O
vt	O
min	O
t	O
min	O
t	O
min	O
it	O
follows	O
that	O
we	O
should	O
select	O
the	O
feature	B
jt	O
argmax	O
j	O
the	O
rest	O
of	O
the	O
update	O
is	O
to	O
set	B
vt	O
vt	O
ujt	O
t	O
t	O
the	O
omp	B
procedure	O
maintains	O
an	O
orthonormal	O
basis	O
of	O
the	O
selected	O
features	O
where	O
in	O
the	O
preceding	O
description	O
the	O
orthonormalization	O
property	O
is	O
obtained	O
by	O
a	O
procedure	O
similar	O
to	O
gram-schmidt	O
orthonormalization	O
in	O
practice	O
the	O
gram-schmidt	O
procedure	O
is	O
often	O
numerically	O
unstable	O
in	O
the	O
pseudocode	O
that	O
follows	O
we	O
use	O
svd	B
section	O
at	O
the	O
end	O
of	O
each	O
round	O
to	O
obtain	O
an	O
orthonormal	O
basis	O
in	O
a	O
numerically	O
stable	O
manner	O
feature	B
selection	I
and	O
generation	O
orthogonal	O
matching	O
pursuit	O
input	O
data	O
matrix	O
x	O
rmd	O
labels	O
vector	O
y	O
rm	O
budget	O
of	O
features	O
t	O
initialize	O
for	O
t	O
t	O
use	O
svd	B
to	O
find	O
an	O
orthonormal	O
basis	O
v	O
rmt	O
of	O
xit	O
t	O
set	B
v	O
to	O
be	O
the	O
all	O
zeros	O
matrix	O
foreach	O
j	O
it	O
let	O
uj	O
xj	O
v	O
v	O
let	O
jt	O
argmaxj	O
update	O
it	O
output	O
it	O
more	O
efficient	O
greedy	O
selection	O
criteria	O
let	O
rw	O
be	O
the	O
empirical	B
risk	B
of	O
a	O
vector	O
w	O
at	O
each	O
round	O
of	O
the	O
forward	B
greedy	I
selection	I
method	O
and	O
for	O
every	O
possible	O
j	O
we	O
should	O
minimize	O
rw	O
over	O
the	O
vectors	O
w	O
whose	O
support	O
is	O
it	O
this	O
might	O
be	O
time	O
consuming	O
a	O
simpler	O
approach	O
is	O
to	O
choose	O
jt	O
that	O
minimizes	O
argmin	O
j	O
r	O
rwt	O
ej	O
min	O
where	O
ej	O
is	O
the	O
all	O
zeros	O
vector	O
except	O
in	O
the	O
jth	O
element	O
that	O
is	O
we	O
keep	O
the	O
weights	O
of	O
the	O
previously	O
chosen	O
coordinates	O
intact	O
and	O
only	O
optimize	O
over	O
the	O
new	O
variable	O
therefore	O
for	O
each	O
j	O
we	O
need	O
to	O
solve	O
an	O
optimization	O
problem	O
over	O
a	O
single	O
variable	O
which	O
is	O
a	O
much	O
easier	O
task	O
than	O
optimizing	O
over	O
t	O
an	O
even	O
simpler	O
approach	O
is	O
to	O
upper	O
bound	O
rw	O
using	O
a	O
simple	O
function	B
and	O
then	O
choose	O
the	O
feature	B
which	O
leads	O
to	O
the	O
largest	O
decrease	O
in	O
this	O
upper	O
bound	O
for	O
example	O
if	O
r	O
is	O
a	O
function	B
equation	O
in	O
chapter	O
then	O
rw	O
ej	O
rw	O
rw	O
wj	O
minimizing	O
the	O
right-hand	O
side	O
over	O
yields	O
rw	O
value	O
into	O
the	O
above	O
yields	O
wj	O
rw	O
and	O
plugging	O
this	O
rw	O
ej	O
rw	O
wj	O
this	O
value	O
is	O
minimized	O
if	O
the	O
partial	O
derivative	O
of	O
rw	O
with	O
respect	O
to	O
wj	O
is	O
maximal	O
we	O
can	O
therefore	O
choose	O
jt	O
to	O
be	O
the	O
index	O
of	O
the	O
largest	O
coordinate	O
of	O
the	O
gradient	B
of	O
rw	O
at	O
w	O
remark	O
as	O
a	O
forward	B
greedy	I
selection	I
procedure	O
it	O
is	O
possible	O
to	O
interpret	O
the	O
adaboost	B
algorithm	O
from	O
chapter	O
as	O
a	O
forward	O
greedy	O
feature	B
selection	I
selection	O
procedure	O
with	O
respect	O
to	O
the	O
function	B
exp	O
yi	O
wjhjxi	O
rw	O
log	O
see	O
exercise	O
backward	B
elimination	I
another	O
popular	O
greedy	O
selection	O
approach	O
is	O
backward	B
elimination	I
here	O
we	O
start	O
with	O
the	O
full	O
set	B
of	O
features	O
and	O
then	O
we	O
gradually	O
remove	O
one	O
feature	B
at	O
a	O
time	O
from	O
the	O
set	B
of	O
features	O
given	O
that	O
our	O
current	O
set	B
of	O
selected	O
features	O
is	O
i	O
we	O
go	O
over	O
all	O
i	O
i	O
and	O
apply	O
the	O
learning	O
algorithm	O
on	O
the	O
set	B
of	O
features	O
ii	O
each	O
such	O
application	O
yields	O
a	O
different	O
predictor	B
and	O
we	O
choose	O
to	O
remove	O
the	O
feature	B
i	O
for	O
which	O
the	O
predictor	B
obtained	O
from	O
i	O
has	O
the	O
smallest	O
risk	B
the	O
training	B
set	B
or	O
on	O
a	O
validation	B
set	B
naturally	O
there	O
are	O
many	O
possible	O
variants	O
of	O
the	O
backward	B
elimination	I
idea	O
it	O
is	O
also	O
possible	O
to	O
combine	O
forward	O
and	O
backward	O
greedy	O
steps	O
sparsity-inducing	B
norms	I
the	O
problem	O
of	O
minimizing	O
the	O
empirical	B
risk	B
subject	O
to	O
a	O
budget	O
of	O
k	O
features	O
can	O
be	O
written	O
as	O
min	O
w	O
lsw	O
s	O
t	O
k	O
wi	O
in	O
other	O
words	O
we	O
want	O
w	O
to	O
be	O
sparse	O
which	O
implies	O
that	O
we	O
only	O
need	O
to	O
measure	O
the	O
features	O
corresponding	O
to	O
nonzero	O
elements	O
of	O
w	O
convex	B
function	B
with	O
the	O
norm	O
solving	O
this	O
optimization	O
problem	O
is	O
computationally	O
hard	O
davis	O
mallat	O
avellaneda	O
a	O
possible	O
relaxation	O
is	O
to	O
replace	O
the	O
and	O
to	O
solve	O
the	O
problem	O
min	O
w	O
lsw	O
s	O
t	O
where	O
is	O
a	O
parameter	O
since	O
the	O
norm	O
is	O
a	O
convex	B
function	B
this	O
problem	O
can	O
be	O
solved	O
efficiently	O
as	O
long	O
as	O
the	O
loss	B
function	B
is	O
convex	B
a	O
related	O
problem	O
is	O
minimizing	O
the	O
sum	O
of	O
lsw	O
plus	O
an	O
norm	O
regularization	B
term	O
min	O
w	O
where	O
is	O
a	O
regularization	B
parameter	O
since	O
for	O
any	O
there	O
exists	O
a	O
such	O
that	O
the	O
function	B
is	O
often	O
referred	O
to	O
as	O
the	O
norm	O
despite	O
the	O
use	O
of	O
the	O
norm	O
notation	O
is	O
not	O
really	O
a	O
norm	O
for	O
example	O
it	O
does	O
not	O
satisfy	O
the	O
positive	O
homogeneity	O
property	O
of	O
norms	O
feature	B
selection	I
and	O
generation	O
equation	O
and	O
equation	O
lead	O
to	O
the	O
same	O
solution	O
the	O
two	O
problems	O
are	O
in	O
some	O
sense	O
equivalent	O
the	O
regularization	B
often	O
induces	O
sparse	O
solutions	O
to	O
illustrate	O
this	O
let	O
us	O
start	O
with	O
the	O
simple	O
optimization	O
problem	O
min	O
w	O
r	O
xw	O
it	O
is	O
easy	O
to	O
verify	O
exercise	O
that	O
the	O
solution	O
to	O
this	O
problem	O
is	O
the	O
soft	O
thresholding	O
operator	O
w	O
signx	O
def	O
maxa	O
that	O
is	O
as	O
long	O
as	O
the	O
absolute	O
value	O
of	O
x	O
is	O
smaller	O
where	O
than	O
the	O
optimal	O
solution	O
will	O
be	O
zero	O
next	O
consider	O
a	O
one	O
dimensional	O
regression	B
problem	O
with	O
respect	O
to	O
the	O
squared	O
loss	B
argmin	O
w	O
rm	O
we	O
can	O
rewrite	O
the	O
problem	O
as	O
i	O
m	O
i	O
argmin	O
w	O
rm	O
for	O
simplicity	O
let	O
us	O
assume	O
that	O
m	O
then	O
the	O
optimal	O
solution	O
is	O
i	O
and	O
denote	O
w	O
xiyi	O
m	O
i	O
xiyi	O
w	O
that	O
is	O
the	O
solution	O
will	O
be	O
zero	O
unless	O
the	O
correlation	O
between	O
the	O
feature	B
x	O
and	O
the	O
labels	O
vector	O
y	O
is	O
larger	O
than	O
remark	O
unlike	O
the	O
norm	O
the	O
norm	O
does	O
not	O
induce	O
sparse	O
solutions	O
indeed	O
consider	O
the	O
problem	O
above	O
with	O
an	O
regularization	B
namely	O
argmin	O
w	O
rm	O
then	O
the	O
optimal	O
solution	O
is	O
w	O
this	O
solution	O
will	O
be	O
nonzero	O
even	O
if	O
the	O
correlation	O
between	O
x	O
and	O
y	O
is	O
very	O
small	O
in	O
contrast	O
as	O
we	O
have	O
shown	O
before	O
when	O
using	O
regularization	B
w	O
will	O
be	O
nonzero	O
only	O
if	O
the	O
correlation	O
between	O
x	O
and	O
y	O
is	O
larger	O
than	O
the	O
regularization	B
parameter	O
feature	B
manipulation	O
and	O
normalization	O
adding	O
regularization	B
to	O
a	O
linear	B
regression	B
problem	O
with	O
the	O
squared	O
loss	B
yields	O
the	O
lasso	B
algorithm	O
defined	O
as	O
argmin	O
w	O
under	O
some	O
assumptions	O
on	O
the	O
distribution	O
and	O
the	O
regularization	B
parameter	O
the	O
lasso	B
will	O
find	O
sparse	O
solutions	O
for	O
example	O
yu	O
and	O
the	O
references	O
therein	O
another	O
advantage	O
of	O
the	O
norm	O
is	O
that	O
a	O
vector	O
with	O
low	O
norm	O
can	O
be	O
sparsified	O
for	O
example	O
zhang	O
srebro	O
and	O
the	O
references	O
therein	O
feature	B
manipulation	O
and	O
normalization	O
feature	B
manipulations	O
or	O
normalization	O
include	O
simple	O
transformations	O
that	O
we	O
apply	O
on	O
each	O
of	O
our	O
original	O
features	O
such	O
transformations	O
may	O
decrease	O
the	O
approximation	O
or	O
estimation	O
errors	O
of	O
our	O
hypothesis	B
class	I
or	O
can	O
yield	O
a	O
faster	O
algorithm	O
similarly	O
to	O
the	O
problem	O
of	O
feature	B
selection	I
here	O
again	O
there	O
are	O
no	O
absolute	O
good	O
and	O
bad	O
transformations	O
but	O
rather	O
each	O
transformation	O
that	O
we	O
apply	O
should	O
be	O
related	O
to	O
the	O
learning	O
algorithm	O
we	O
are	O
going	O
to	O
apply	O
on	O
the	O
resulting	O
feature	B
vector	O
as	O
well	O
as	O
to	O
our	O
prior	O
assumptions	O
on	O
the	O
problem	O
to	O
motivate	O
normalization	O
consider	O
a	O
linear	B
regression	B
problem	O
with	O
the	O
squared	O
loss	B
let	O
x	O
rmd	O
be	O
a	O
matrix	O
whose	O
rows	O
are	O
the	O
instance	B
vectors	O
and	O
let	O
y	O
rm	O
be	O
a	O
vector	O
of	O
target	O
values	O
recall	B
that	O
ridge	B
regression	B
returns	O
the	O
vector	O
mi	O
m	O
argmin	O
w	O
suppose	O
that	O
d	O
and	O
the	O
underlying	O
data	O
distribution	O
is	O
as	O
follows	O
first	O
we	O
sample	O
y	O
uniformly	O
at	O
random	O
from	O
then	O
we	O
set	B
to	O
be	O
y	O
where	O
is	O
sampled	O
uniformly	O
at	O
random	O
from	O
and	O
we	O
set	B
to	O
be	O
note	O
that	O
the	O
optimal	O
weight	O
vector	O
is	O
and	O
however	O
the	O
objective	O
of	O
ridge	B
regression	B
at	O
is	O
in	O
contrast	O
the	O
objective	O
of	O
ridge	B
regression	B
at	O
w	O
is	O
likely	O
to	O
be	O
close	O
to	O
it	O
follows	O
that	O
whenever	O
the	O
objective	O
of	O
ridge	B
regression	B
is	O
smaller	O
at	O
the	O
suboptimal	O
solution	O
w	O
since	O
typically	O
should	O
be	O
at	O
least	O
the	O
analysis	O
in	O
chapter	O
it	O
follows	O
that	O
in	O
the	O
aforementioned	O
example	O
if	O
the	O
number	O
of	O
examples	O
is	O
smaller	O
than	O
then	O
we	O
are	O
likely	O
to	O
output	O
a	O
suboptimal	O
solution	O
the	O
crux	O
of	O
the	O
preceding	O
example	O
is	O
that	O
the	O
two	O
features	O
have	O
completely	O
different	O
scales	O
feature	B
normalization	I
can	O
overcome	O
this	O
problem	O
there	O
are	O
many	O
ways	O
to	O
perform	O
feature	B
normalization	I
and	O
one	O
of	O
the	O
simplest	O
approaches	O
is	O
simply	O
to	O
make	O
sure	O
that	O
each	O
feature	B
receives	O
values	O
between	O
and	O
in	O
the	O
preceding	O
example	O
if	O
we	O
divide	O
each	O
feature	B
by	O
the	O
maximal	O
value	O
it	O
attains	O
feature	B
selection	I
and	O
generation	O
we	O
will	O
obtain	O
that	O
ridge	B
regression	B
is	O
quite	O
close	O
to	O
and	O
y	O
then	O
for	O
the	O
solution	O
of	O
moreover	O
the	O
generalization	B
bounds	I
we	O
have	O
derived	O
in	O
chapter	O
for	O
regularized	O
loss	B
minimization	O
depend	O
on	O
the	O
norm	O
of	O
the	O
optimal	O
vector	O
and	O
on	O
the	O
maximal	O
norm	O
of	O
the	O
instance	B
therefore	O
in	O
the	O
aforementioned	O
example	O
before	O
we	O
normalize	O
the	O
features	O
we	O
have	O
that	O
while	O
after	O
we	O
normalize	O
the	O
features	O
we	O
have	O
that	O
the	O
maximal	O
norm	O
of	O
the	O
instance	B
vector	O
remains	O
roughly	O
the	O
same	O
hence	O
the	O
normalization	O
greatly	O
improves	O
the	O
estimation	B
error	I
feature	B
normalization	I
can	O
also	O
improve	O
the	O
runtime	O
of	O
the	O
learning	O
algorithm	O
for	O
example	O
in	O
section	O
we	O
have	O
shown	O
how	O
to	O
use	O
the	O
stochastic	O
gradient	B
descent	I
optimization	O
algorithm	O
for	O
solving	O
the	O
regularized	O
loss	B
minimization	O
problem	O
the	O
number	O
of	O
iterations	O
required	O
by	O
sgd	B
to	O
converge	O
also	O
depends	O
on	O
the	O
norm	O
of	O
and	O
on	O
the	O
maximal	O
norm	O
of	O
therefore	O
as	O
before	O
using	O
normalization	O
can	O
greatly	O
decrease	O
the	O
runtime	O
of	O
sgd	B
next	O
we	O
demonstrate	O
in	O
the	O
following	O
how	O
a	O
simple	O
transformation	O
on	O
features	O
such	O
as	O
clipping	O
can	O
sometime	O
decrease	O
the	O
approximation	B
error	I
of	O
our	O
hypothesis	B
class	I
consider	O
again	O
linear	B
regression	B
with	O
the	O
squared	O
loss	B
let	O
a	O
be	O
a	O
large	O
number	O
suppose	O
that	O
the	O
target	O
y	O
is	O
chosen	O
uniformly	O
at	O
random	O
from	O
and	O
then	O
the	O
single	O
feature	B
x	O
is	O
set	B
to	O
be	O
y	O
with	O
probability	O
and	O
set	B
to	O
be	O
ay	O
with	O
probability	O
that	O
is	O
most	O
of	O
the	O
time	O
our	O
feature	B
is	O
bounded	O
but	O
with	O
a	O
very	O
small	O
probability	O
it	O
gets	O
a	O
very	O
high	O
value	O
then	O
for	O
any	O
w	O
the	O
expected	O
squared	O
loss	B
of	O
w	O
is	O
ldw	O
e	O
a	O
a	O
solving	O
for	O
w	O
we	O
obtain	O
that	O
which	O
goes	O
to	O
zero	O
as	O
a	O
goes	O
to	O
infinity	O
therefore	O
the	O
objective	O
at	O
goes	O
to	O
as	O
a	O
goes	O
to	O
infinity	O
for	O
example	O
for	O
a	O
we	O
will	O
obtain	O
next	O
suppose	O
we	O
apply	O
a	O
clipping	O
transformation	O
that	O
is	O
we	O
use	O
the	O
transformation	O
x	O
signx	O
then	O
following	O
this	O
transformation	O
becomes	O
and	O
this	O
simple	O
example	O
shows	O
that	O
a	O
simple	O
transformation	O
can	O
have	O
a	O
significant	O
influence	O
on	O
the	O
approximation	B
error	I
of	O
course	O
it	O
is	O
not	O
hard	O
to	O
think	O
of	O
examples	O
in	O
which	O
the	O
same	O
feature	B
transformation	O
actually	O
hurts	O
performance	O
and	O
increases	O
the	O
approximation	B
error	I
this	O
is	O
not	O
surprising	O
as	O
we	O
have	O
already	O
argued	O
that	O
feature	B
transformations	I
more	O
precisely	O
the	O
bounds	O
we	O
derived	O
in	O
chapter	O
for	O
regularized	O
loss	B
minimization	O
depend	O
on	O
and	O
on	O
either	O
the	O
lipschitzness	B
or	O
the	O
smoothness	B
of	O
the	O
loss	B
function	B
for	O
linear	B
predictors	I
and	O
loss	B
functions	O
of	O
the	O
form	O
y	O
y	O
where	O
is	O
convex	B
and	O
either	O
or	O
with	O
respect	O
to	O
its	O
first	O
argument	O
we	O
have	O
that	O
is	O
either	O
or	O
for	O
example	O
for	O
the	O
squared	O
loss	B
y	O
first	O
argument	O
is	O
with	O
respect	O
to	O
its	O
and	O
y	O
feature	B
manipulation	O
and	O
normalization	O
should	O
rely	O
on	O
our	O
prior	O
assumptions	O
on	O
the	O
problem	O
in	O
the	O
aforementioned	O
example	O
a	O
prior	O
assumption	O
that	O
may	O
lead	O
us	O
to	O
use	O
the	O
clipping	O
transformation	O
is	O
that	O
features	O
that	O
get	O
values	O
larger	O
than	O
a	O
predefined	O
threshold	O
value	O
give	O
us	O
no	O
additional	O
useful	O
information	O
and	O
therefore	O
we	O
can	O
clip	O
them	O
to	O
the	O
predefined	O
threshold	O
examples	O
of	O
feature	B
transformations	I
we	O
now	O
list	O
several	O
common	O
techniques	O
for	O
feature	B
transformations	I
usually	O
it	O
is	O
helpful	O
to	O
combine	O
some	O
of	O
these	O
transformations	O
centering	O
scaling	O
in	O
the	O
following	O
we	O
denote	O
by	O
f	O
fm	O
rm	O
the	O
value	O
of	O
the	O
feature	B
f	O
over	O
the	O
m	O
training	O
examples	O
also	O
we	O
denote	O
by	O
f	O
fi	O
the	O
empirical	O
m	O
mean	O
of	O
the	O
feature	B
over	O
all	O
examples	O
centering	O
this	O
transformation	O
makes	O
the	O
feature	B
have	O
zero	O
mean	O
by	O
setting	O
fi	O
fi	O
f	O
unit	O
range	O
this	O
transformation	O
makes	O
the	O
range	O
of	O
each	O
feature	B
be	O
formally	O
let	O
fmax	O
maxi	O
fi	O
and	O
fmin	O
mini	O
fi	O
then	O
we	O
set	B
fi	O
fi	O
fmin	O
similarly	O
fmax	O
fmin	O
we	O
can	O
make	O
the	O
range	O
of	O
each	O
feature	B
be	O
by	O
the	O
transformation	O
fi	O
of	O
course	O
it	O
is	O
easy	O
to	O
make	O
the	O
range	O
b	O
or	O
b	O
b	O
where	O
b	O
is	O
fi	O
fmin	O
fmax	O
fmin	O
a	O
user-specified	O
parameter	O
standardization	O
this	O
transformation	O
makes	O
all	O
features	O
have	O
a	O
zero	O
mean	O
and	O
unit	O
variance	O
f	O
be	O
the	O
empirical	O
variance	O
of	O
the	O
feature	B
formally	O
let	O
m	O
then	O
we	O
set	B
fi	O
fi	O
f	O
clipping	O
this	O
transformation	O
clips	O
high	O
or	O
low	O
values	O
of	O
the	O
feature	B
for	O
example	O
fi	O
signfi	O
maxbfi	O
where	O
b	O
is	O
a	O
user-specified	O
parameter	O
sigmoidal	O
transformation	O
as	O
its	O
name	O
indicates	O
this	O
transformation	O
applies	O
a	O
sigmoid	O
function	B
on	O
the	O
feature	B
for	O
example	O
fi	O
fi	O
where	O
b	O
is	O
a	O
user-specified	O
parameter	O
this	O
transformation	O
can	O
be	O
thought	O
of	O
as	O
a	O
soft	O
version	O
of	O
clipping	O
it	O
has	O
a	O
small	O
effect	O
on	O
values	O
close	O
to	O
zero	O
and	O
behaves	O
similarly	O
to	O
clipping	O
on	O
values	O
far	O
away	O
from	O
zero	O
feature	B
selection	I
and	O
generation	O
logarithmic	O
transformation	O
the	O
transformation	O
is	O
fi	O
logbfi	O
where	O
b	O
is	O
a	O
user-specified	O
parameter	O
this	O
is	O
widely	O
used	O
when	O
the	O
feature	B
is	O
a	O
counting	O
feature	B
for	O
example	O
suppose	O
that	O
the	O
feature	B
represents	O
the	O
number	O
of	O
appearances	O
of	O
a	O
certain	O
word	O
in	O
a	O
text	O
document	O
then	O
the	O
difference	O
between	O
zero	O
occurrences	O
of	O
the	O
word	O
and	O
a	O
single	O
occurrence	O
is	O
much	O
more	O
important	O
than	O
the	O
difference	O
between	O
occurrences	O
and	O
occurrences	O
remark	O
in	O
the	O
aforementioned	O
transformations	O
each	O
feature	B
is	O
transformed	O
on	O
the	O
basis	O
of	O
the	O
values	O
it	O
obtains	O
on	O
the	O
training	B
set	B
independently	O
of	O
other	O
features	O
values	O
in	O
some	O
situations	O
we	O
would	O
like	O
to	O
set	B
the	O
parameter	O
of	O
the	O
transformation	O
on	O
the	O
basis	O
of	O
other	O
features	O
as	O
well	O
a	O
notable	O
example	O
is	O
a	O
transformation	O
in	O
which	O
one	O
applies	O
a	O
scaling	O
to	O
the	O
features	O
so	O
that	O
the	O
empirical	O
average	O
of	O
some	O
norm	O
of	O
the	O
instances	O
becomes	O
feature	B
learning	I
so	O
far	O
we	O
have	O
discussed	O
feature	B
selection	I
and	O
manipulations	O
in	O
these	O
cases	O
we	O
start	O
with	O
a	O
predefined	O
vector	O
space	O
rd	O
representing	O
our	O
features	O
then	O
we	O
select	O
a	O
subset	O
of	O
features	O
selection	O
or	O
transform	O
individual	O
features	O
transformation	O
in	O
this	O
section	O
we	O
describe	O
feature	B
learning	I
in	O
which	O
we	O
start	O
with	O
some	O
instance	B
space	I
x	O
and	O
would	O
like	O
to	O
learn	O
a	O
function	B
x	O
rd	O
which	O
maps	O
instances	O
in	O
x	O
into	O
a	O
representation	O
as	O
d-dimensional	O
feature	B
vectors	O
the	O
idea	O
of	O
feature	B
learning	I
is	O
to	O
automate	O
the	O
process	O
of	O
finding	O
a	O
good	O
representation	O
of	O
the	O
input	O
space	O
as	O
mentioned	O
before	O
the	O
no-free-lunch	B
theorem	O
tells	O
us	O
that	O
we	O
must	O
incorporate	O
some	O
prior	B
knowledge	I
on	O
the	O
data	O
distribution	O
in	O
order	O
to	O
build	O
a	O
good	O
feature	B
representation	O
in	O
this	O
section	O
we	O
present	O
a	O
few	O
feature	B
learning	I
approaches	O
and	O
demonstrate	O
conditions	O
on	O
the	O
underlying	O
data	O
distribution	O
in	O
which	O
these	O
methods	O
can	O
be	O
useful	O
throughout	O
the	O
book	O
we	O
have	O
already	O
seen	O
several	O
useful	O
feature	B
constructions	O
for	O
example	O
in	O
the	O
context	O
of	O
polynomial	B
regression	B
we	O
have	O
mapped	O
the	O
original	O
instances	O
into	O
the	O
vector	O
space	O
of	O
all	O
their	O
monomials	O
section	O
in	O
chapter	O
after	O
performing	O
this	O
mapping	O
we	O
trained	O
a	O
linear	B
predictor	B
on	O
top	O
of	O
the	O
constructed	O
features	O
automation	O
of	O
this	O
process	O
would	O
be	O
to	O
learn	O
a	O
transformation	O
x	O
rd	O
such	O
that	O
the	O
composition	O
of	O
the	O
class	O
of	O
linear	B
predictors	I
on	O
top	O
of	O
yields	O
a	O
good	O
hypothesis	B
class	I
for	O
the	O
task	O
at	O
hand	O
in	O
the	O
following	O
we	O
describe	O
a	O
technique	O
of	O
feature	B
construction	O
called	O
dictio	O
nary	O
learning	O
dictionary	B
learning	I
using	O
auto-encoders	B
the	O
motivation	O
of	O
dictionary	B
learning	I
stems	O
from	O
a	O
commonly	O
used	O
representation	O
of	O
documents	O
as	O
a	O
bag-of-words	B
given	O
a	O
dictionary	O
of	O
words	O
d	O
wk	O
where	O
each	O
wi	O
is	O
a	O
string	O
representing	O
a	O
word	O
in	O
the	O
dictionary	O
feature	B
learning	I
and	O
given	O
a	O
document	O
pd	O
where	O
each	O
pi	O
is	O
a	O
word	O
in	O
the	O
document	O
we	O
represent	O
the	O
document	O
as	O
a	O
vector	O
x	O
where	O
xi	O
is	O
if	O
wi	O
pj	O
for	O
some	O
j	O
and	O
xi	O
otherwise	O
it	O
was	O
empirically	O
observed	O
in	O
many	O
text	O
processing	O
tasks	O
that	O
linear	B
predictors	I
are	O
quite	O
powerful	O
when	O
applied	O
on	O
this	O
representation	O
intuitively	O
we	O
can	O
think	O
of	O
each	O
word	O
as	O
a	O
feature	B
that	O
measures	O
some	O
aspect	O
of	O
the	O
document	O
given	O
labeled	O
examples	O
topics	O
of	O
the	O
documents	O
a	O
learning	O
algorithm	O
searches	O
for	O
a	O
linear	B
predictor	B
that	O
weights	O
these	O
features	O
so	O
that	O
a	O
right	O
combination	O
of	O
appearances	O
of	O
words	O
is	O
indicative	O
of	O
the	O
label	B
while	O
in	O
text	O
processing	O
there	O
is	O
a	O
natural	O
meaning	O
to	O
words	O
and	O
to	O
the	O
dictionary	O
in	O
other	O
applications	O
we	O
do	O
not	O
have	O
such	O
an	O
intuitive	O
representation	O
of	O
an	O
instance	B
for	O
example	O
consider	O
the	O
computer	O
vision	O
application	O
of	O
object	O
recognition	O
here	O
the	O
instance	B
is	O
an	O
image	O
and	O
the	O
goal	O
is	O
to	O
recognize	O
which	O
object	O
appears	O
in	O
the	O
image	O
applying	O
a	O
linear	B
predictor	B
on	O
the	O
pixel-based	O
representation	O
of	O
the	O
image	O
does	O
not	O
yield	O
a	O
good	O
classifier	B
what	O
we	O
would	O
like	O
to	O
have	O
is	O
a	O
mapping	O
that	O
would	O
take	O
the	O
pixel-based	O
representation	O
of	O
the	O
image	O
and	O
would	O
output	O
a	O
bag	O
of	O
visual	O
words	O
representing	O
the	O
content	O
of	O
the	O
image	O
for	O
example	O
a	O
visual	O
word	O
can	O
be	O
there	O
is	O
an	O
eye	O
in	O
the	O
image	O
if	O
we	O
had	O
such	O
representation	O
we	O
could	O
have	O
applied	O
a	O
linear	B
predictor	B
on	O
top	O
of	O
this	O
representation	O
to	O
train	O
a	O
classifier	B
for	O
say	O
face	O
recognition	O
our	O
question	O
is	O
therefore	O
how	O
can	O
we	O
learn	O
a	O
dictionary	O
of	O
visual	O
words	O
such	O
that	O
a	O
bag-ofwords	O
representation	O
of	O
an	O
image	O
would	O
be	O
helpful	O
for	O
predicting	O
which	O
object	O
appears	O
in	O
the	O
image	O
a	O
first	O
naive	O
approach	O
for	O
dictionary	B
learning	I
relies	O
on	O
a	O
clustering	B
algorithm	O
chapter	O
suppose	O
that	O
we	O
learn	O
a	O
function	B
c	O
x	O
k	O
where	O
cx	O
is	O
the	O
cluster	O
to	O
which	O
x	O
belongs	O
then	O
we	O
can	O
think	O
of	O
the	O
clusters	O
as	O
words	O
and	O
of	O
instances	O
as	O
documents	O
where	O
a	O
document	O
x	O
is	O
mapped	O
to	O
the	O
vector	O
where	O
is	O
if	O
and	O
only	O
if	O
x	O
belongs	O
to	O
the	O
ith	O
cluster	O
now	O
it	O
is	O
straightforward	O
to	O
see	O
that	O
applying	O
a	O
linear	B
predictor	B
on	O
is	O
equivalent	O
to	O
assigning	O
the	O
same	O
target	O
value	O
to	O
all	O
instances	O
that	O
belong	O
to	O
the	O
same	O
cluster	O
furthermore	O
if	O
the	O
clustering	B
is	O
based	O
on	O
distances	O
from	O
a	O
class	O
center	O
k-means	B
then	O
a	O
linear	B
predictor	B
on	O
yields	O
a	O
piece-wise	O
constant	O
predictor	B
on	O
x	O
find	O
a	O
pair	O
of	O
functions	O
such	O
that	O
the	O
reconstruction	O
both	O
the	O
k-means	B
and	O
pca	B
approaches	O
can	O
be	O
regarded	O
as	O
special	O
cases	O
of	O
a	O
more	O
general	O
approach	O
for	O
dictionary	B
learning	I
which	O
is	O
called	O
auto-encoders	B
in	O
an	O
auto-encoder	O
we	O
learn	O
a	O
pair	O
of	O
functions	O
an	O
encoder	O
function	B
rd	O
rk	O
and	O
a	O
decoder	O
function	B
rk	O
rd	O
the	O
goal	O
of	O
the	O
learning	O
process	O
is	O
to	O
i	O
is	O
small	O
of	O
course	O
we	O
can	O
trivially	O
set	B
k	O
d	O
and	O
both	O
to	O
be	O
the	O
identity	O
mapping	O
which	O
yields	O
a	O
perfect	O
reconstruction	O
we	O
therefore	O
must	O
restrict	O
and	O
in	O
some	O
way	O
in	O
pca	B
we	O
constrain	O
k	O
d	O
and	O
further	O
restrict	O
and	O
to	O
be	O
linear	O
functions	O
in	O
k-means	B
k	O
is	O
not	O
restricted	O
to	O
be	O
smaller	O
than	O
d	O
but	O
now	O
and	O
rely	O
on	O
k	O
centroids	O
k	O
and	O
returns	O
an	O
indicator	O
vector	O
feature	B
selection	I
and	O
generation	O
in	O
that	O
indicates	O
the	O
closest	O
centroid	O
to	O
x	O
while	O
takes	O
as	O
input	O
an	O
indicator	O
vector	O
and	O
returns	O
the	O
centroid	O
representing	O
this	O
vector	O
an	O
important	O
property	O
of	O
the	O
k-means	B
construction	O
which	O
is	O
key	O
in	O
allowing	O
k	O
to	O
be	O
larger	O
than	O
d	O
is	O
that	O
maps	O
instances	O
into	O
sparse	O
vectors	O
in	O
fact	O
in	O
k-means	B
only	O
a	O
single	O
coordinate	O
of	O
is	O
nonzero	O
an	O
immediate	O
extension	O
of	O
the	O
k-means	B
construction	O
is	O
therefore	O
to	O
restrict	O
the	O
range	O
of	O
to	O
be	O
vectors	O
with	O
at	O
most	O
s	O
nonzero	O
elements	O
where	O
s	O
is	O
a	O
small	O
integer	O
in	O
particular	O
let	O
and	O
be	O
functions	O
that	O
depend	O
on	O
k	O
the	O
function	B
maps	O
an	O
instance	B
vector	O
x	O
to	O
a	O
vector	O
rk	O
where	O
should	O
have	O
at	O
most	O
s	O
nonzero	O
elements	O
vi	O
i	O
as	O
before	O
our	O
goal	O
is	O
to	O
have	O
a	O
the	O
function	B
is	O
defined	O
to	O
be	O
small	O
reconstruction	O
error	O
and	O
therefore	O
we	O
can	O
define	O
argmin	O
v	O
s	O
t	O
s	O
where	O
vj	O
note	O
that	O
when	O
s	O
and	O
we	O
further	O
restrict	O
then	O
we	O
obtain	O
the	O
k-means	B
encoding	O
function	B
that	O
is	O
is	O
the	O
indicator	O
vector	O
of	O
the	O
centroid	O
closest	O
to	O
x	O
for	O
larger	O
values	O
of	O
s	O
the	O
optimization	O
problem	O
in	O
the	O
preceding	O
definition	O
of	O
becomes	O
computationally	O
difficult	O
therefore	O
in	O
practice	O
we	O
sometime	O
use	O
regularization	B
instead	O
of	O
the	O
sparsity	O
constraint	O
and	O
define	O
to	O
be	O
argmin	O
v	O
ror	O
where	O
is	O
a	O
regularization	B
parameter	O
anyway	O
the	O
dictionary	B
learning	I
problem	O
is	O
now	O
to	O
find	O
the	O
vectors	O
k	O
such	O
that	O
the	O
reconstruction	O
is	O
as	O
small	O
as	O
possible	O
even	O
if	O
is	O
defined	O
using	O
the	O
regularization	B
this	O
is	O
still	O
a	O
computationally	O
hard	O
problem	O
to	O
the	O
k-means	B
problem	O
however	O
several	O
heuristic	O
search	O
algorithms	O
may	O
give	O
reasonably	O
good	O
solutions	O
these	O
algorithms	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
summary	O
many	O
machine	O
learning	O
algorithms	O
take	O
the	O
feature	B
representation	O
of	O
instances	O
for	O
granted	O
yet	O
the	O
choice	O
of	O
representation	O
requires	O
careful	O
attention	O
we	O
discussed	O
approaches	O
for	O
feature	B
selection	I
introducing	O
filters	O
greedy	O
selection	O
algorithms	O
and	O
sparsity-inducing	B
norms	I
next	O
we	O
presented	O
several	O
examples	O
for	O
feature	B
transformations	I
and	O
demonstrated	O
their	O
usefulness	O
last	O
we	O
discussed	O
feature	B
learning	I
and	O
in	O
particular	O
dictionary	B
learning	I
we	O
have	O
shown	O
that	O
feature	B
selection	I
manipulation	O
and	O
learning	O
all	O
depend	O
on	O
some	O
prior	B
knowledge	I
on	O
the	O
data	O
bibliographic	O
remarks	O
bibliographic	O
remarks	O
guyon	O
elisseeff	O
surveyed	O
several	O
feature	B
selection	I
procedures	O
including	O
many	O
types	O
of	O
filters	O
forward	B
greedy	I
selection	I
procedures	O
for	O
minimizing	O
a	O
convex	B
objective	O
subject	O
to	O
a	O
polyhedron	O
constraint	O
date	O
back	O
to	O
the	O
frank-wolfe	O
algorithm	O
wolfe	O
the	O
relation	O
to	O
boosting	B
has	O
been	O
studied	O
by	O
several	O
authors	O
including	O
liao	O
ratsch	O
warmuth	O
glocer	O
vishwanathan	O
shalev-shwartz	O
singer	O
matching	O
pursuit	O
has	O
been	O
studied	O
in	O
the	O
signal	O
processing	O
community	O
zhang	O
several	O
papers	O
analyzed	O
greedy	O
selection	O
methods	O
under	O
various	O
conditions	O
see	O
for	O
example	O
shalevshwartz	O
zhang	O
srebro	O
and	O
the	O
references	O
therein	O
the	O
use	O
of	O
the	O
as	O
a	O
surrogate	O
for	O
sparsity	O
has	O
a	O
long	O
history	O
tibshirani	O
and	O
the	O
references	O
therein	O
and	O
much	O
work	O
has	O
been	O
done	O
on	O
understanding	O
the	O
relationship	O
between	O
the	O
and	O
sparsity	O
it	O
is	O
also	O
closely	O
related	O
to	O
compressed	B
sensing	I
chapter	O
the	O
ability	O
to	O
sparsify	O
low	O
norm	O
predictors	O
dates	O
back	O
to	O
maurey	O
in	O
section	O
we	O
also	O
show	O
that	O
low	O
norm	O
can	O
be	O
used	O
to	O
bound	O
the	O
estimation	B
error	I
of	O
our	O
predictor	B
feature	B
learning	I
and	O
dictionary	B
learning	I
have	O
been	O
extensively	O
studied	O
recently	O
in	O
the	O
context	O
of	O
deep	O
neural	B
networks	I
see	O
for	O
example	O
bengio	O
hinton	O
et	O
al	O
ranzato	O
et	O
al	O
collobert	O
weston	O
lee	O
et	O
al	O
le	O
et	O
al	O
bengio	O
and	O
the	O
references	O
therein	O
exercises	O
prove	O
the	O
equality	O
given	O
in	O
equation	O
hint	O
let	O
a	O
b	O
be	O
minimizers	O
of	O
the	O
left-hand	O
side	O
find	O
a	O
b	O
such	O
that	O
the	O
objective	O
value	O
of	O
the	O
right-hand	O
side	O
is	O
smaller	O
than	O
that	O
of	O
the	O
left-hand	O
side	O
do	O
the	O
same	O
for	O
the	O
other	O
direction	O
show	O
that	O
equation	O
is	O
the	O
solution	O
of	O
equation	O
adaboost	B
as	O
a	O
forward	B
greedy	I
selection	I
algorithm	O
recall	B
the	O
adaboost	B
algorithm	O
from	O
chapter	O
in	O
this	O
section	O
we	O
give	O
another	O
interpretation	O
of	O
adaboost	B
as	O
a	O
forward	B
greedy	I
selection	I
algorithm	O
given	O
a	O
set	B
of	O
m	O
instances	O
xm	O
and	O
a	O
hypothesis	B
class	I
h	O
of	O
finite	O
vc	B
dimension	I
show	O
that	O
there	O
exist	O
d	O
and	O
hd	O
such	O
that	O
for	O
every	O
h	O
h	O
there	O
exists	O
i	O
with	O
hixj	O
hxj	O
for	O
every	O
j	O
let	O
rw	O
be	O
as	O
defined	O
in	O
equation	O
given	O
some	O
w	O
define	O
fw	O
to	O
be	O
the	O
function	B
fw	O
wihi	O
feature	B
selection	I
and	O
generation	O
let	O
d	O
be	O
the	O
distribution	O
over	O
defined	O
by	O
exp	O
yifwxi	O
di	O
z	O
where	O
z	O
is	O
a	O
normalization	O
factor	O
that	O
ensures	O
that	O
d	O
is	O
a	O
probability	O
vector	O
show	O
that	O
diyihjxi	O
rw	O
furthermore	O
denoting	O
wj	O
show	O
that	O
rw	O
conclude	O
that	O
if	O
then	O
hint	O
use	O
the	O
proof	O
of	O
theorem	O
rw	O
wj	O
wj	O
show	O
that	O
the	O
update	O
of	O
adaboost	B
guarantees	O
rwt	O
part	O
iv	O
advanced	O
theory	O
rademacher	O
complexities	O
in	O
chapter	O
we	O
have	O
shown	O
that	O
uniform	B
convergence	I
is	O
a	O
sufficient	O
condition	O
for	O
learnability	O
in	O
this	O
chapter	O
we	O
study	O
the	O
rademacher	B
complexity	I
which	O
measures	O
the	O
rate	O
of	O
uniform	B
convergence	I
we	O
will	O
provide	O
generalization	B
bounds	I
based	O
on	O
this	O
measure	O
the	O
rademacher	B
complexity	I
recall	B
the	O
definition	O
of	O
an	O
sample	O
from	O
chapter	O
repeated	O
here	O
for	O
convenience	O
definition	O
sample	O
a	O
training	B
set	B
s	O
is	O
called	O
domain	B
z	O
hypothesis	B
class	I
h	O
loss	B
function	B
and	O
distribution	O
d	O
if	O
lsh	O
sup	O
h	O
h	O
we	O
have	O
shown	O
that	O
if	O
s	O
is	O
an	O
representative	B
sample	I
then	O
the	O
erm	B
rule	O
is	O
namely	O
ldermhs	O
minh	O
h	O
ldh	O
to	O
simplify	O
our	O
notation	O
let	O
us	O
denote	O
f	O
def	O
h	O
def	O
z	O
h	O
h	O
and	O
given	O
f	O
f	O
we	O
define	O
ldf	O
e	O
z	O
df	O
lsf	O
m	O
f	O
we	O
define	O
the	O
representativeness	O
of	O
s	O
with	O
respect	O
to	O
f	O
as	O
the	O
largest	O
gap	O
between	O
the	O
true	B
error	I
of	O
a	O
function	B
f	O
and	O
its	O
empirical	B
error	I
namely	O
repdf	O
s	O
def	O
sup	O
f	O
f	O
lsf	O
now	O
suppose	O
we	O
would	O
like	O
to	O
estimate	O
the	O
representativeness	O
of	O
s	O
using	O
the	O
sample	O
s	O
only	O
one	O
simple	O
idea	O
is	O
to	O
split	O
s	O
into	O
two	O
disjoint	O
sets	O
s	O
refer	O
to	O
as	O
a	O
validation	B
set	B
and	O
to	O
as	O
a	O
training	B
set	B
we	O
can	O
then	O
estimate	O
the	O
representativeness	O
of	O
s	O
by	O
sup	O
f	O
f	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
rademacher	O
complexities	O
this	O
can	O
be	O
written	O
more	O
compactly	O
by	O
defining	O
m	O
to	O
be	O
a	O
vector	O
such	O
that	O
i	O
and	O
i	O
then	O
if	O
we	O
further	O
assume	O
that	O
then	O
equation	O
can	O
be	O
rewritten	O
as	O
m	O
sup	O
f	O
f	O
if	O
the	O
rademacher	B
complexity	I
measure	O
captures	O
this	O
idea	O
by	O
considering	O
the	O
expectation	O
of	O
the	O
above	O
with	O
respect	O
to	O
a	O
random	O
choice	O
of	O
formally	O
let	O
f	O
s	O
be	O
the	O
set	B
of	O
all	O
possible	O
evaluations	O
a	O
function	B
f	O
f	O
can	O
achieve	O
on	O
a	O
sample	O
s	O
namely	O
f	O
s	O
f	O
f	O
f	O
let	O
the	O
variables	O
in	O
be	O
distributed	O
i	O
i	O
d	O
according	O
to	O
p	O
i	O
p	O
i	O
then	O
the	O
rademacher	B
complexity	I
of	O
f	O
with	O
respect	O
to	O
s	O
is	O
defined	O
as	O
follows	O
rf	O
s	O
def	O
m	O
e	O
if	O
more	O
generally	O
given	O
a	O
set	B
of	O
vectors	O
a	O
rm	O
we	O
define	O
sup	O
f	O
f	O
ra	O
def	O
m	O
e	O
sup	O
a	O
a	O
iai	O
the	O
following	O
lemma	O
bounds	O
the	O
expected	O
value	O
of	O
the	O
representativeness	O
of	O
s	O
by	O
twice	O
the	O
expected	O
rademacher	B
complexity	I
lemma	O
e	O
s	O
dm	O
proof	O
let	O
ldf	O
therefore	O
for	O
every	O
f	O
f	O
we	O
have	O
repdf	O
s	O
e	O
s	O
dm	O
rf	O
s	O
m	O
be	O
another	O
i	O
i	O
d	O
sample	O
clearly	O
for	O
all	O
f	O
f	O
ldf	O
lsf	O
e	O
lsf	O
e	O
lsf	O
taking	O
supremum	O
over	O
f	O
f	O
of	O
both	O
sides	O
and	O
using	O
the	O
fact	O
that	O
the	O
supremum	O
of	O
expectation	O
is	O
smaller	O
than	O
expectation	O
of	O
the	O
supremum	O
we	O
obtain	O
sup	O
f	O
f	O
lsf	O
sup	O
f	O
f	O
e	O
taking	O
expectation	O
over	O
s	O
on	O
both	O
sides	O
we	O
obtain	O
e	O
s	O
sup	O
f	O
f	O
lsf	O
sup	O
f	O
f	O
lsf	O
e	O
lsf	O
lsf	O
i	O
f	O
sup	O
f	O
f	O
sup	O
f	O
f	O
e	O
m	O
e	O
the	O
rademacher	B
complexity	I
next	O
we	O
note	O
that	O
for	O
each	O
j	O
zj	O
and	O
replace	O
them	O
without	O
affecting	O
the	O
expectation	O
j	O
are	O
i	O
i	O
d	O
variables	O
therefore	O
we	O
can	O
j	O
f	O
i	O
f	O
j	O
i	O
f	O
let	O
j	O
be	O
a	O
random	O
variable	O
such	O
that	O
p	O
j	O
p	O
j	O
from	O
equation	O
we	O
obtain	O
that	O
f	O
jf	O
e	O
e	O
e	O
j	O
e	O
f	O
f	O
f	O
f	O
sup	O
sup	O
sup	O
sup	O
f	O
f	O
f	O
f	O
sup	O
f	O
f	O
sup	O
f	O
f	O
i	O
e	O
of	O
equation	O
of	O
equation	O
j	O
f	O
i	O
f	O
j	O
f	O
e	O
i	O
f	O
if	O
sup	O
f	O
f	O
i	O
f	O
if	O
repeating	O
this	O
for	O
all	O
j	O
we	O
obtain	O
that	O
i	O
f	O
e	O
finally	O
if	O
i	O
f	O
sup	O
f	O
f	O
if	O
i	O
sup	O
f	O
f	O
i	O
i	O
and	O
since	O
the	O
probability	O
of	O
is	O
the	O
same	O
as	O
the	O
probability	O
of	O
the	O
right-hand	O
side	O
of	O
equation	O
can	O
be	O
bounded	O
by	O
i	O
sup	O
f	O
f	O
m	O
e	O
if	O
s	O
i	O
i	O
sup	O
f	O
f	O
m	O
e	O
if	O
s	O
e	O
s	O
s	O
the	O
lemma	O
immediately	O
yields	O
that	O
in	O
expectation	O
the	O
erm	B
rule	O
finds	O
a	O
hypothesis	B
which	O
is	O
close	O
to	O
the	O
optimal	O
hypothesis	B
in	O
h	O
theorem	O
we	O
have	O
e	O
s	O
dm	O
lsermhs	O
e	O
s	O
dm	O
furthermore	O
for	O
any	O
h	O
h	O
s	O
e	O
s	O
dm	O
e	O
s	O
dm	O
h	O
s	O
rademacher	O
complexities	O
furthermore	O
if	O
argminh	O
ldh	O
then	O
for	O
each	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
we	O
have	O
ldermhs	O
dm	O
h	O
proof	O
the	O
first	O
inequality	O
follows	O
directly	O
from	O
lemma	O
the	O
second	O
inequality	O
follows	O
because	O
for	O
any	O
fixed	O
e	O
e	O
s	O
s	O
the	O
third	O
inequality	O
follows	O
from	O
the	O
previous	O
inequality	O
by	O
relying	O
on	O
markov	O
s	O
inequality	O
that	O
the	O
random	O
variable	O
ldermhs	O
is	O
nonnegative	O
next	O
we	O
derive	O
bounds	O
similar	O
to	O
the	O
bounds	O
in	O
theorem	O
with	O
a	O
better	O
dependence	O
on	O
the	O
confidence	B
parameter	O
to	O
do	O
so	O
we	O
first	O
introduce	O
the	O
following	O
bounded	O
differences	O
concentration	O
inequality	O
lemma	O
s	O
inequality	O
let	O
v	O
be	O
some	O
set	B
and	O
let	O
f	O
v	O
m	O
r	O
be	O
a	O
function	B
of	O
m	O
variables	O
such	O
that	O
for	O
some	O
c	O
for	O
all	O
i	O
and	O
for	O
all	O
xm	O
i	O
v	O
we	O
have	O
xm	O
f	O
xi	O
i	O
xm	O
c	O
let	O
xm	O
be	O
m	O
independent	O
random	O
variables	O
taking	O
values	O
in	O
v	O
then	O
with	O
probability	O
of	O
at	O
least	O
we	O
have	O
xm	O
ef	O
xm	O
c	O
on	O
the	O
basis	O
of	O
the	O
mcdiarmid	O
inequality	O
we	O
can	O
derive	O
generalization	B
bounds	I
with	O
a	O
better	O
dependence	O
on	O
the	O
confidence	B
parameter	O
theorem	O
assume	O
that	O
for	O
all	O
z	O
and	O
h	O
h	O
we	O
have	O
that	O
z	O
c	O
then	O
with	O
probability	O
of	O
at	O
least	O
for	O
all	O
h	O
h	O
ldh	O
lsh	O
e	O
dm	O
h	O
c	O
m	O
in	O
particular	O
this	O
holds	O
for	O
h	O
ermhs	O
with	O
probability	O
of	O
at	O
least	O
for	O
all	O
h	O
h	O
ldh	O
lsh	O
h	O
s	O
c	O
m	O
in	O
particular	O
this	O
holds	O
for	O
h	O
ermhs	O
for	O
any	O
with	O
probability	O
of	O
at	O
least	O
ldermhs	O
h	O
s	O
c	O
ln	O
m	O
the	O
rademacher	B
complexity	I
proof	O
first	O
note	O
that	O
the	O
random	O
variable	O
repdf	O
s	O
suph	O
h	O
lsh	O
satisfies	O
the	O
bounded	O
differences	O
condition	O
of	O
lemma	O
with	O
a	O
constant	O
combining	O
the	O
bounds	O
in	O
lemma	O
with	O
lemma	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
e	O
m	O
h	O
c	O
repdf	O
s	O
e	O
repdf	O
s	O
c	O
the	O
first	O
inequality	O
of	O
the	O
theorem	O
follows	O
from	O
the	O
definition	O
of	O
repdf	O
s	O
for	O
the	O
second	O
inequality	O
we	O
note	O
that	O
the	O
random	O
variable	O
h	O
s	O
also	O
satisfies	O
the	O
bounded	O
differences	O
condition	O
of	O
lemma	O
with	O
a	O
constant	O
therefore	O
the	O
second	O
inequality	O
follows	O
from	O
the	O
first	O
inequality	O
lemma	O
and	O
the	O
union	B
bound	I
finally	O
for	O
the	O
last	O
inequality	O
denote	O
hs	O
ermhs	O
and	O
note	O
that	O
m	O
ldhs	O
ldhs	O
lshs	O
lshs	O
lshs	O
the	O
first	O
summand	O
on	O
the	O
right-hand	O
side	O
is	O
bounded	O
by	O
the	O
second	O
inequality	O
of	O
the	O
theorem	O
for	O
the	O
second	O
summand	O
we	O
use	O
the	O
fact	O
that	O
does	O
not	O
depend	O
on	O
s	O
hence	O
by	O
using	O
hoeffding	O
s	O
inequality	O
we	O
obtain	O
that	O
with	O
probaility	O
of	O
at	O
least	O
c	O
combining	O
this	O
with	O
the	O
union	B
bound	I
we	O
conclude	O
our	O
proof	O
the	O
preceding	O
theorem	O
tells	O
us	O
that	O
if	O
the	O
quantity	O
h	O
s	O
is	O
small	O
then	O
it	O
is	O
possible	O
to	O
learn	O
the	O
class	O
h	O
using	O
the	O
erm	B
rule	O
it	O
is	O
important	O
to	O
emphasize	O
that	O
the	O
last	O
two	O
bounds	O
given	O
in	O
the	O
theorem	O
depend	O
on	O
the	O
specific	O
training	B
set	B
s	O
that	O
is	O
we	O
use	O
s	O
both	O
for	O
learning	O
a	O
hypothesis	B
from	O
h	O
as	O
well	O
as	O
for	O
estimating	O
the	O
quality	O
of	O
it	O
this	O
type	O
of	O
bound	O
is	O
called	O
a	O
data-dependent	O
bound	O
rademacher	O
calculus	O
let	O
us	O
now	O
discuss	O
some	O
properties	O
of	O
the	O
rademacher	B
complexity	I
measure	O
these	O
properties	O
will	O
help	O
us	O
in	O
deriving	O
some	O
simple	O
bounds	O
on	O
h	O
s	O
for	O
specific	O
cases	O
of	O
interest	O
the	O
following	O
lemma	O
is	O
immediate	O
from	O
the	O
definition	O
lemma	O
for	O
any	O
a	O
rm	O
scalar	O
c	O
r	O
and	O
vector	O
rm	O
we	O
have	O
rc	O
a	O
a	O
a	O
ra	O
the	O
following	O
lemma	O
tells	O
us	O
that	O
the	O
convex	B
hull	O
of	O
a	O
has	O
the	O
same	O
complexity	O
as	O
a	O
rademacher	O
complexities	O
lemma	O
let	O
a	O
be	O
a	O
subset	O
of	O
rm	O
and	O
let	O
n	O
j	O
aj	O
a	O
j	O
then	O
ra	O
proof	O
the	O
main	O
idea	O
follows	O
from	O
the	O
fact	O
that	O
for	O
any	O
vector	O
v	O
we	O
have	O
jaj	O
n	O
sup	O
therefore	O
jvj	O
max	O
j	O
vj	O
jaj	O
i	O
i	O
iaj	O
i	O
m	O
e	O
sup	O
sup	O
e	O
j	O
sup	O
aj	O
sup	O
iai	O
e	O
sup	O
a	O
a	O
m	O
ra	O
and	O
we	O
conclude	O
our	O
proof	O
the	O
next	O
lemma	O
due	O
to	O
massart	O
states	O
that	O
the	O
rademacher	B
complexity	I
of	O
a	O
finite	O
set	B
grows	O
logarithmically	O
with	O
the	O
size	O
of	O
the	O
set	B
lemma	O
lemma	O
let	O
a	O
an	O
be	O
a	O
finite	O
set	B
of	O
vectors	O
in	O
rm	O
define	O
a	O
n	O
ai	O
then	O
ra	O
max	O
a	O
a	O
logn	O
m	O
proof	O
based	O
on	O
lemma	O
we	O
can	O
assume	O
without	O
loss	B
of	O
generality	O
that	O
a	O
let	O
and	O
let	O
an	O
we	O
upper	O
bound	O
the	O
rademacher	B
complexity	I
as	O
follows	O
a	O
max	O
e	O
log	O
e	O
e	O
max	O
a	O
a	O
a	O
e	O
log	O
a	O
e	O
i	O
iai	O
log	O
log	O
jensen	O
s	O
inequality	O
where	O
the	O
last	O
equality	O
occurs	O
because	O
the	O
rademacher	O
variables	O
are	O
independent	O
next	O
using	O
lemma	O
we	O
have	O
that	O
for	O
all	O
ai	O
r	O
expai	O
exp	O
ai	O
e	O
iai	O
i	O
e	O
i	O
the	O
rademacher	B
complexity	I
and	O
therefore	O
log	O
a	O
exp	O
i	O
a	O
max	O
log	O
a	O
since	O
ra	O
max	O
a	O
log	O
we	O
obtain	O
from	O
the	O
equation	O
that	O
ra	O
loga	O
maxa	O
setting	O
loga	O
maxa	O
a	O
and	O
rearranging	O
terms	O
we	O
conclude	O
our	O
m	O
proof	O
the	O
following	O
lemma	O
shows	O
that	O
composing	O
a	O
with	O
a	O
lipschitz	O
function	B
does	O
not	O
blow	O
up	O
the	O
rademacher	B
complexity	I
the	O
proof	O
is	O
due	O
to	O
kakade	O
and	O
tewari	O
lemma	O
lemma	O
for	O
each	O
i	O
let	O
i	O
r	O
r	O
be	O
a	O
lipschitz	O
function	B
namely	O
for	O
all	O
r	O
we	O
have	O
i	O
i	O
for	O
a	O
rm	O
let	O
denote	O
the	O
vector	O
mym	O
let	O
a	O
a	O
a	O
then	O
r	O
a	O
ra	O
proof	O
for	O
simplicity	O
we	O
prove	O
the	O
lemma	O
for	O
the	O
case	O
the	O
case	O
will	O
follow	O
by	O
defining	O
and	O
then	O
using	O
lemma	O
let	O
ai	O
ai	O
iai	O
am	O
a	O
a	O
clearly	O
it	O
suffices	O
to	O
prove	O
that	O
for	O
any	O
set	B
a	O
and	O
all	O
i	O
we	O
have	O
rai	O
ra	O
without	O
loss	B
of	O
generality	O
we	O
will	O
prove	O
the	O
latter	O
claim	O
for	O
i	O
and	O
to	O
simplify	O
notation	O
we	O
omit	O
the	O
subscript	O
from	O
we	O
have	O
iai	O
iai	O
iai	O
e	O
e	O
sup	O
a	O
sup	O
a	O
a	O
e	O
m	O
e	O
m	O
e	O
m	O
sup	O
a	O
a	O
sup	O
a	O
sup	O
a	O
iai	O
sup	O
a	O
a	O
i	O
iai	O
iai	O
i	O
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
the	O
assumption	O
that	O
is	O
lipschitz	O
next	O
we	O
note	O
that	O
the	O
absolute	O
value	O
on	O
in	O
the	O
preceding	O
expression	O
can	O
rademacher	O
complexities	O
be	O
omitted	O
since	O
both	O
a	O
and	O
are	O
from	O
the	O
same	O
set	B
a	O
and	O
the	O
rest	O
of	O
the	O
expression	O
in	O
the	O
supremum	O
is	O
not	O
affected	O
by	O
replacing	O
a	O
and	O
therefore	O
i	O
e	O
m	O
sup	O
a	O
iai	O
but	O
using	O
the	O
same	O
equalities	O
as	O
in	O
equation	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
right-hand	O
side	O
of	O
equation	O
exactly	O
equals	O
m	O
ra	O
which	O
concludes	O
our	O
proof	O
rademacher	B
complexity	I
of	O
linear	O
classes	O
in	O
this	O
section	O
we	O
analyze	O
the	O
rademacher	B
complexity	I
of	O
linear	O
classes	O
to	O
simplify	O
the	O
derivation	O
we	O
first	O
define	O
the	O
following	O
two	O
classes	O
the	O
following	O
lemma	O
bounds	O
the	O
rademacher	B
complexity	I
of	O
we	O
allow	O
the	O
xi	O
to	O
be	O
vectors	O
in	O
any	O
hilbert	B
space	I
infinite	O
dimensional	O
and	O
the	O
bound	O
does	O
not	O
depend	O
on	O
the	O
dimensionality	O
of	O
the	O
hilbert	B
space	I
this	O
property	O
becomes	O
useful	O
when	O
analyzing	O
kernel	O
methods	O
lemma	O
let	O
s	O
xm	O
be	O
vectors	O
in	O
a	O
hilbert	B
space	I
define	O
s	O
then	O
s	O
maxi	O
m	O
proof	O
using	O
cauchy-schwartz	O
inequality	O
we	O
know	O
that	O
for	O
any	O
vectors	O
w	O
v	O
we	O
have	O
therefore	O
s	O
e	O
e	O
e	O
e	O
sup	O
a	O
s	O
iai	O
sup	O
sup	O
e	O
ixi	O
next	O
using	O
jensen	O
s	O
inequality	O
we	O
have	O
that	O
e	O
ixi	O
e	O
ixi	O
generalization	B
bounds	I
for	O
svm	B
finally	O
since	O
the	O
variables	O
m	O
are	O
independent	O
we	O
have	O
e	O
i	O
e	O
ij	O
i	O
e	O
i	O
j	O
e	O
m	O
max	O
i	O
combining	O
this	O
with	O
equation	O
and	O
equation	O
we	O
conclude	O
our	O
proof	O
next	O
we	O
bound	O
the	O
rademacher	B
complexity	I
of	O
s	O
lemma	O
let	O
s	O
xm	O
be	O
vectors	O
in	O
rn	O
then	O
proof	O
using	O
holder	O
s	O
inequality	O
we	O
know	O
that	O
for	O
any	O
vectors	O
w	O
v	O
we	O
have	O
therefore	O
s	O
max	O
i	O
m	O
s	O
e	O
e	O
e	O
sup	O
a	O
s	O
iai	O
sup	O
sup	O
e	O
for	O
each	O
j	O
let	O
vj	O
xmj	O
rm	O
note	O
that	O
let	O
v	O
vn	O
vn	O
the	O
right-hand	O
side	O
of	O
equation	O
is	O
m	O
rv	O
using	O
massart	B
lemma	I
we	O
have	O
that	O
m	O
maxi	O
rv	O
max	O
i	O
which	O
concludes	O
our	O
proof	O
generalization	B
bounds	I
for	O
svm	B
in	O
this	O
section	O
we	O
use	O
rademacher	B
complexity	I
to	O
derive	O
generalization	B
bounds	I
for	O
generalized	O
linear	B
predictors	I
with	O
euclidean	O
norm	O
constraint	O
we	O
will	O
show	O
how	O
this	O
leads	O
to	O
generalization	B
bounds	I
for	O
hard-svm	B
and	O
soft-svm	B
rademacher	O
complexities	O
we	O
shall	O
consider	O
the	O
following	O
general	O
constraint-based	O
formulation	O
let	O
h	O
b	O
be	O
our	O
hypothesis	B
class	I
and	O
let	O
z	O
x	O
y	O
be	O
the	O
examples	O
domain	B
assume	O
that	O
the	O
loss	B
function	B
h	O
z	O
r	O
is	O
of	O
the	O
form	O
y	O
y	O
where	O
r	O
y	O
r	O
is	O
such	O
that	O
for	O
all	O
y	O
y	O
the	O
scalar	O
function	B
a	O
y	O
is	O
for	O
example	O
the	O
hinge-loss	O
function	B
y	O
can	O
be	O
written	O
as	O
in	O
equation	O
using	O
y	O
ya	O
and	O
note	O
that	O
is	O
for	O
all	O
y	O
another	O
example	O
is	O
the	O
absolute	O
loss	B
function	B
y	O
y	O
which	O
can	O
be	O
written	O
as	O
in	O
equation	O
using	O
y	O
y	O
which	O
is	O
also	O
for	O
all	O
y	O
r	O
the	O
following	O
theorem	O
bounds	O
the	O
generalization	B
error	I
of	O
all	O
predictors	O
in	O
h	O
using	O
their	O
empirical	B
error	I
theorem	O
suppose	O
that	O
d	O
is	O
a	O
distribution	O
over	O
x	O
y	O
such	O
that	O
with	O
probability	O
we	O
have	O
that	O
r	O
let	O
h	O
b	O
and	O
let	O
h	O
z	O
r	O
be	O
a	O
loss	B
function	B
of	O
the	O
form	O
given	O
in	O
equation	O
such	O
that	O
for	O
all	O
y	O
y	O
a	O
y	O
is	O
a	O
function	B
and	O
such	O
that	O
maxa	O
brbr	O
y	O
c	O
then	O
for	O
any	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
an	O
i	O
i	O
d	O
sample	O
of	O
size	O
m	O
w	O
h	O
ldw	O
lsw	O
br	O
m	O
c	O
m	O
proof	O
let	O
f	O
y	O
y	O
w	O
h	O
we	O
will	O
show	O
that	O
with	O
probability	O
rf	O
s	O
br	O
m	O
and	O
then	O
the	O
theorem	O
will	O
follow	O
from	O
theorem	O
indeed	O
the	O
set	B
f	O
s	O
can	O
be	O
written	O
as	O
f	O
s	O
ym	O
w	O
h	O
and	O
the	O
bound	O
on	O
rf	O
s	O
follows	O
directly	O
by	O
combining	O
lemma	O
lemma	O
and	O
the	O
assumption	O
that	O
r	O
with	O
probability	O
we	O
next	O
derive	O
a	O
generalization	O
bound	O
for	O
hard-svm	B
based	O
on	O
the	O
previous	O
theorem	O
for	O
simplicity	O
we	O
do	O
not	O
allow	O
a	O
bias	B
term	O
and	O
consider	O
the	O
hard-svm	B
problem	O
s	O
t	O
i	O
argmin	O
w	O
theorem	O
consider	O
a	O
distribution	O
d	O
over	O
x	O
such	O
that	O
there	O
exists	O
some	O
vector	O
with	O
pxy	O
and	O
such	O
that	O
r	O
with	O
probability	O
let	O
ws	O
be	O
the	O
output	O
of	O
equation	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
we	O
have	O
that	O
dy	O
r	O
p	O
m	O
r	O
m	O
generalization	B
bounds	I
for	O
svm	B
proof	O
throughout	O
the	O
proof	O
let	O
the	O
loss	B
function	B
be	O
the	O
ramp	B
loss	B
section	O
note	O
that	O
the	O
range	O
of	O
the	O
ramp	B
loss	B
is	O
and	O
that	O
it	O
is	O
a	O
function	B
since	O
the	O
ramp	B
loss	B
upper	O
bounds	O
the	O
zero-one	O
loss	B
we	O
have	O
that	O
p	O
dy	O
ldws	O
let	O
b	O
and	O
consider	O
the	O
set	B
h	O
b	O
by	O
the	O
definition	O
of	O
hard-svm	B
and	O
our	O
assumption	O
on	O
the	O
distribution	O
we	O
have	O
that	O
ws	O
h	O
with	O
probability	O
and	O
that	O
lsws	O
therefore	O
using	O
theorem	O
we	O
have	O
that	O
ldws	O
lsws	O
m	O
m	O
remark	O
theorem	O
implies	O
that	O
the	O
sample	B
complexity	I
of	O
hard-svm	B
grows	O
like	O
using	O
a	O
more	O
delicate	O
analysis	O
and	O
the	O
separability	O
assumption	O
it	O
is	O
possible	O
to	O
improve	O
the	O
bound	O
to	O
an	O
order	O
of	O
the	O
bound	O
in	O
the	O
preceding	O
theorem	O
depends	O
on	O
which	O
is	O
unknown	O
in	O
the	O
following	O
we	O
derive	O
a	O
bound	O
that	O
depends	O
on	O
the	O
norm	O
of	O
the	O
output	O
of	O
svm	B
hence	O
it	O
can	O
be	O
calculated	O
from	O
the	O
training	B
set	B
itself	O
the	O
proof	O
is	O
similar	O
to	O
the	O
derivation	O
of	O
bounds	O
for	O
structure	O
risk	B
minimization	O
theorem	O
assume	O
that	O
the	O
conditions	O
of	O
theorem	O
hold	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
dm	O
we	O
have	O
that	O
dy	O
p	O
m	O
ln	O
m	O
proof	O
for	O
any	O
integer	O
i	O
let	O
bi	O
hi	O
bi	O
and	O
let	O
i	O
fix	O
i	O
then	O
using	O
theorem	O
we	O
have	O
that	O
with	O
probability	O
of	O
at	O
least	O
i	O
m	O
i	O
m	O
w	O
hi	O
ldw	O
lsw	O
applying	O
the	O
union	B
bound	I
and	O
i	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
this	O
holds	O
for	O
all	O
i	O
therefore	O
for	O
all	O
w	O
if	O
we	O
let	O
i	O
then	O
w	O
hi	O
bi	O
and	O
ldw	O
lsw	O
m	O
therefore	O
i	O
i	O
lsw	O
m	O
m	O
m	O
in	O
particular	O
it	O
holds	O
for	O
ws	O
which	O
concludes	O
our	O
proof	O
rademacher	O
complexities	O
remark	O
note	O
that	O
all	O
the	O
bounds	O
we	O
have	O
derived	O
do	O
not	O
depend	O
on	O
the	O
dimension	O
of	O
w	O
this	O
property	O
is	O
utilized	O
when	O
learning	O
svm	B
with	O
kernels	B
where	O
the	O
dimension	O
of	O
w	O
can	O
be	O
extremely	O
large	O
generalization	B
bounds	I
for	O
predictors	O
with	O
low	O
norm	O
in	O
the	O
previous	O
section	O
we	O
derived	O
generalization	B
bounds	I
for	O
linear	B
predictors	I
with	O
an	O
constraint	O
in	O
this	O
section	O
we	O
consider	O
the	O
following	O
general	O
constraint	O
formulation	O
let	O
h	O
b	O
be	O
our	O
hypothesis	B
class	I
and	O
let	O
z	O
x	O
y	O
be	O
the	O
examples	O
domain	B
assume	O
that	O
the	O
loss	B
function	B
h	O
z	O
r	O
is	O
of	O
the	O
same	O
form	O
as	O
in	O
equation	O
with	O
r	O
y	O
r	O
being	O
w	O
r	O
t	O
its	O
first	O
argument	O
the	O
following	O
theorem	O
bounds	O
the	O
generalization	B
error	I
of	O
all	O
predictors	O
in	O
h	O
using	O
their	O
empirical	B
error	I
theorem	O
suppose	O
that	O
d	O
is	O
a	O
distribution	O
over	O
x	O
y	O
such	O
that	O
with	O
probability	O
we	O
have	O
that	O
r	O
let	O
h	O
rd	O
b	O
and	O
let	O
h	O
z	O
r	O
be	O
a	O
loss	B
function	B
of	O
the	O
form	O
given	O
in	O
equation	O
such	O
that	O
for	O
all	O
y	O
y	O
a	O
y	O
is	O
an	O
function	B
and	O
such	O
that	O
maxa	O
brbr	O
y	O
c	O
then	O
for	O
any	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
an	O
i	O
i	O
d	O
sample	O
of	O
size	O
m	O
m	O
w	O
h	O
ldw	O
lsw	O
br	O
m	O
c	O
proof	O
the	O
proof	O
is	O
identical	O
to	O
the	O
proof	O
of	O
theorem	O
while	O
relying	O
on	O
lemma	O
instead	O
of	O
relying	O
on	O
lemma	O
it	O
is	O
interesting	O
to	O
compare	O
the	O
two	O
bounds	O
given	O
in	O
theorem	O
and	O
theorem	O
apart	O
from	O
the	O
extra	O
logd	O
factor	O
that	O
appears	O
in	O
theorem	O
both	O
bounds	O
look	O
similar	O
however	O
the	O
parameters	O
b	O
r	O
have	O
different	O
meanings	O
in	O
the	O
two	O
bounds	O
in	O
theorem	O
the	O
parameter	O
b	O
imposes	O
an	O
constraint	O
on	O
w	O
and	O
the	O
parameter	O
r	O
captures	O
a	O
low	O
assumption	O
on	O
the	O
instances	O
in	O
contrast	O
in	O
theorem	O
the	O
parameter	O
b	O
imposes	O
an	O
constraint	O
on	O
w	O
is	O
stronger	O
than	O
an	O
constraint	O
while	O
the	O
parameter	O
r	O
captures	O
a	O
low	O
assumption	O
on	O
the	O
instance	B
is	O
weaker	O
than	O
a	O
low	O
assumption	O
therefore	O
the	O
choice	O
of	O
the	O
constraint	O
should	O
depend	O
on	O
our	O
prior	B
knowledge	I
of	O
the	O
set	B
of	O
instances	O
and	O
on	O
prior	O
assumptions	O
on	O
good	O
predictors	O
bibliographic	O
remarks	O
the	O
use	O
of	O
rademacher	B
complexity	I
for	O
bounding	O
the	O
uniform	B
convergence	I
is	O
due	O
to	O
panchenko	O
bartlett	O
mendelson	O
bartlett	O
mendelson	O
for	O
additional	O
reading	O
see	O
for	O
example	O
boucheron	O
bousquet	O
lugosi	O
bartlett	O
bousquet	O
mendelson	O
bibliographic	O
remarks	O
our	O
proof	O
of	O
the	O
concentration	O
lemma	O
is	O
due	O
to	O
kakade	O
and	O
tewari	O
lecture	O
notes	O
kakade	O
sridharan	O
tewari	O
gave	O
a	O
unified	O
framework	O
for	O
deriving	O
bounds	O
on	O
the	O
rademacher	B
complexity	I
of	O
linear	O
classes	O
with	O
respect	O
to	O
different	O
assumptions	O
on	O
the	O
norms	O
covering	B
numbers	I
in	O
this	O
chapter	O
we	O
describe	O
another	O
way	O
to	O
measure	O
the	O
complexity	O
of	O
sets	O
which	O
is	O
called	O
covering	B
numbers	I
covering	O
definition	O
let	O
a	O
rm	O
be	O
a	O
set	B
of	O
vectors	O
we	O
say	O
that	O
a	O
is	O
r-covered	O
by	O
a	O
set	B
with	O
respect	O
to	O
the	O
euclidean	O
metric	O
if	O
for	O
all	O
a	O
a	O
there	O
exists	O
with	O
r	O
we	O
define	O
by	O
n	O
a	O
the	O
cardinality	O
of	O
the	O
smallest	O
that	O
r-covers	O
a	O
example	O
suppose	O
that	O
a	O
rm	O
let	O
c	O
maxa	O
a	O
and	O
as	O
sume	O
that	O
a	O
lies	O
in	O
a	O
d-dimensional	O
subspace	O
of	O
rm	O
then	O
n	O
a	O
drd	O
to	O
see	O
this	O
let	O
vd	O
be	O
an	O
orthonormal	O
basis	O
of	O
the	O
subspace	O
then	O
any	O
ivi	O
with	O
c	O
let	O
r	O
and	O
consider	O
the	O
set	B
a	O
a	O
can	O
be	O
written	O
as	O
a	O
i	O
c	O
c	O
c	O
c	O
given	O
a	O
a	O
s	O
t	O
a	O
ivi	O
i	O
i	O
d	O
then	O
r	O
and	O
therefore	O
is	O
an	O
r-cover	O
of	O
a	O
hence	O
ivi	O
with	O
c	O
there	O
exists	O
such	O
that	O
d	O
i	O
i	O
r	O
d	O
n	O
a	O
choose	O
r	O
properties	O
the	O
following	O
lemma	O
is	O
immediate	O
from	O
the	O
definition	O
lemma	O
for	O
any	O
a	O
rm	O
scalar	O
c	O
and	O
vector	O
rm	O
we	O
have	O
r	O
n	O
a	O
a	O
a	O
n	O
a	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
from	O
covering	O
to	O
rademacher	B
complexity	I
via	O
chaining	B
next	O
we	O
derive	O
a	O
contraction	O
principle	O
lemma	O
for	O
each	O
i	O
let	O
i	O
r	O
r	O
be	O
a	O
function	B
namely	O
for	O
all	O
r	O
we	O
have	O
i	O
i	O
for	O
a	O
rm	O
let	O
denote	O
the	O
vector	O
mam	O
let	O
a	O
a	O
a	O
then	O
n	O
r	O
a	O
n	O
a	O
proof	O
define	O
b	O
a	O
let	O
be	O
an	O
r-cover	O
of	O
a	O
and	O
define	O
then	O
for	O
all	O
a	O
a	O
there	O
exists	O
with	O
r	O
so	O
iai	O
hence	O
is	O
an	O
r-cover	O
of	O
b	O
i	O
i	O
from	O
covering	O
to	O
rademacher	B
complexity	I
via	O
chaining	B
the	O
following	O
lemma	O
bounds	O
the	O
rademacher	B
complexity	I
of	O
a	O
based	O
on	O
the	O
covering	B
numbers	I
n	O
a	O
this	O
technique	O
is	O
called	O
chaining	B
and	O
is	O
attributed	O
to	O
dudley	O
lemma	O
let	O
c	O
min	O
a	O
maxa	O
a	O
then	O
for	O
any	O
integer	O
m	O
ra	O
c	O
m	O
m	O
c	O
m	O
logn	O
k	O
a	O
proof	O
let	O
a	O
be	O
a	O
minimizer	O
of	O
the	O
objective	O
function	B
given	O
in	O
the	O
definition	O
of	O
c	O
on	O
the	O
basis	O
of	O
lemma	O
we	O
can	O
analyze	O
the	O
rademacher	B
complexity	I
assuming	O
that	O
a	O
consider	O
the	O
set	B
and	O
note	O
that	O
it	O
is	O
a	O
c-cover	O
of	O
a	O
let	O
bm	O
be	O
sets	O
such	O
that	O
each	O
bk	O
corresponds	O
to	O
a	O
minimal	O
k-cover	O
of	O
a	O
let	O
a	O
argmaxa	O
if	O
there	O
is	O
more	O
than	O
one	O
maximizer	O
choose	O
one	O
in	O
an	O
arbitrary	O
way	O
and	O
if	O
a	O
maximizer	O
does	O
not	O
exist	O
choose	O
a	O
such	O
that	O
a	O
is	O
close	O
enough	O
to	O
the	O
supremum	O
note	O
that	O
a	O
is	O
a	O
function	B
of	O
for	O
each	O
k	O
let	O
bk	O
be	O
the	O
nearest	B
neighbor	I
of	O
a	O
in	O
bk	O
bk	O
is	O
also	O
a	O
function	B
of	O
using	O
the	O
triangle	O
inequality	O
bk	O
a	O
bk	O
c	O
k	O
c	O
k	O
for	O
each	O
k	O
define	O
the	O
set	B
bk	O
a	O
bk	O
bk	O
c	O
k	O
covering	B
numbers	I
we	O
can	O
now	O
write	O
ra	O
m	O
m	O
e	O
a	O
bm	O
a	O
bm	O
bk	O
bk	O
e	O
m	O
sup	O
a	O
bk	O
m	O
m	O
and	O
bm	O
c	O
m	O
the	O
first	O
summand	O
is	O
at	O
most	O
since	O
m	O
m	O
additionally	O
by	O
massart	B
lemma	I
c	O
m	O
c	O
k	O
m	O
e	O
sup	O
a	O
bk	O
therefore	O
logn	O
k	O
m	O
ra	O
c	O
m	O
m	O
k	O
a	O
m	O
c	O
k	O
logn	O
k	O
a	O
as	O
a	O
corollary	O
we	O
obtain	O
the	O
following	O
lemma	O
assume	O
that	O
there	O
are	O
such	O
that	O
for	O
any	O
k	O
we	O
have	O
logn	O
k	O
a	O
k	O
then	O
ra	O
m	O
k	O
proof	O
the	O
bound	O
follows	O
from	O
lemma	O
by	O
taking	O
m	O
and	O
noting	O
that	O
k	O
and	O
such	O
that	O
c	O
maxa	O
a	O
we	O
have	O
shown	O
that	O
n	O
a	O
fore	O
for	O
any	O
k	O
example	O
consider	O
a	O
set	B
a	O
which	O
lies	O
in	O
a	O
d	O
dimensional	O
subspace	O
of	O
rm	O
there	O
d	O
r	O
logn	O
k	O
a	O
d	O
log	O
d	O
d	O
d	O
k	O
d	O
d	O
d	O
k	O
d	O
logd	O
m	O
hence	O
lemma	O
yields	O
ra	O
m	O
d	O
d	O
d	O
o	O
bibliographic	O
remarks	O
bibliographic	O
remarks	O
the	O
chaining	B
technique	O
is	O
due	O
to	O
dudley	O
for	O
an	O
extensive	O
study	O
of	O
covering	B
numbers	I
as	O
well	O
as	O
other	O
complexity	O
measures	O
that	O
can	O
be	O
used	O
to	O
bound	O
the	O
rate	O
of	O
uniform	B
convergence	I
we	O
refer	O
the	O
reader	O
to	O
bartlet	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
in	O
this	O
chapter	O
we	O
prove	O
theorem	O
from	O
chapter	O
we	O
remind	O
the	O
reader	O
the	O
conditions	O
of	O
the	O
theorem	O
which	O
will	O
hold	O
throughout	O
this	O
chapter	O
h	O
is	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
a	O
domain	B
x	O
to	O
the	O
loss	B
function	B
is	O
the	O
loss	B
and	O
vcdimh	O
d	O
we	O
shall	O
prove	O
the	O
upper	O
bound	O
for	O
both	O
the	O
realizable	O
and	O
agnostic	O
cases	O
and	O
shall	O
prove	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
the	O
lower	O
bound	O
for	O
the	O
realizable	O
case	O
is	O
left	O
as	O
an	O
exercise	O
the	O
upper	O
bound	O
for	O
the	O
agnostic	O
case	O
for	O
the	O
upper	O
bound	O
we	O
need	O
to	O
prove	O
that	O
there	O
exists	O
c	O
such	O
that	O
h	O
is	O
agnostic	B
pac	B
learnable	O
with	O
sample	B
complexity	I
mh	O
c	O
d	O
we	O
will	O
prove	O
the	O
slightly	O
looser	O
bound	O
mh	O
c	O
d	O
logd	O
the	O
tighter	O
bound	O
in	O
the	O
theorem	O
statement	O
requires	O
a	O
more	O
involved	O
proof	O
in	O
which	O
a	O
more	O
careful	O
analysis	O
of	O
the	O
rademacher	B
complexity	I
using	O
a	O
technique	O
called	O
chaining	B
should	O
be	O
used	O
this	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
to	O
prove	O
equation	O
it	O
suffices	O
to	O
show	O
that	O
applying	O
the	O
erm	B
with	O
a	O
sample	O
size	O
m	O
loged	O
yields	O
an	O
for	O
h	O
we	O
prove	O
this	O
result	O
on	O
the	O
basis	O
of	O
theorem	O
let	O
ym	O
be	O
a	O
classification	O
training	B
set	B
recall	B
that	O
the	O
sauershelah	O
lemma	O
tells	O
us	O
that	O
if	O
vcdimh	O
d	O
then	O
log	O
hxm	O
h	O
h	O
e	O
m	O
d	O
denote	O
a	O
h	O
h	O
this	O
clearly	O
implies	O
that	O
e	O
m	O
d	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
combining	O
this	O
with	O
lemma	O
we	O
obtain	O
the	O
following	O
bound	O
on	O
the	O
rademacher	B
complexity	I
ra	O
logemd	O
m	O
using	O
theorem	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
for	O
every	O
h	O
h	O
we	O
have	O
that	O
ldh	O
lsh	O
logemd	O
m	O
m	O
repeating	O
the	O
previous	O
argument	O
for	O
minus	O
the	O
zero-one	O
loss	B
and	O
applying	O
the	O
union	B
bound	I
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
for	O
every	O
h	O
h	O
it	O
holds	O
that	O
lsh	O
logemd	O
m	O
m	O
logemd	O
m	O
to	O
ensure	O
that	O
this	O
is	O
smaller	O
than	O
we	O
need	O
m	O
logm	O
loged	O
using	O
lemma	O
a	O
sufficient	O
condition	O
for	O
the	O
inequality	O
to	O
hold	O
is	O
that	O
m	O
log	O
loged	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
here	O
we	O
prove	O
that	O
there	O
exists	O
c	O
such	O
that	O
h	O
is	O
agnostic	B
pac	B
learnable	O
with	O
sample	B
complexity	I
mh	O
c	O
d	O
we	O
will	O
prove	O
the	O
lower	O
bound	O
in	O
two	O
parts	O
first	O
we	O
will	O
show	O
that	O
m	O
and	O
second	O
we	O
will	O
show	O
that	O
for	O
every	O
we	O
have	O
that	O
m	O
these	O
two	O
bounds	O
will	O
conclude	O
the	O
proof	O
showing	O
that	O
m	O
and	O
any	O
we	O
have	O
that	O
m	O
we	O
first	O
show	O
that	O
for	O
any	O
to	O
do	O
so	O
we	O
show	O
that	O
for	O
m	O
h	O
is	O
not	O
learnable	O
choose	O
one	O
example	O
that	O
is	O
shattered	O
by	O
h	O
that	O
is	O
let	O
c	O
be	O
an	O
example	O
such	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
that	O
there	O
are	O
h	O
h	O
h	O
for	O
which	O
hc	O
and	O
h	O
define	O
two	O
distributions	O
d	O
and	O
d	O
such	O
that	O
for	O
b	O
we	O
have	O
dbx	O
y	O
if	O
x	O
c	O
otherwise	O
that	O
is	O
all	O
the	O
distribution	O
mass	O
is	O
concentrated	O
on	O
two	O
examples	O
and	O
where	O
the	O
probability	O
of	O
b	O
is	O
and	O
the	O
probability	O
of	O
b	O
is	O
b	O
let	O
a	O
be	O
an	O
arbitrary	O
algorithm	O
any	O
training	B
set	B
sampled	O
from	O
db	O
has	O
the	O
form	O
s	O
ym	O
therefore	O
it	O
is	O
fully	O
characterized	O
by	O
the	O
vector	O
y	O
ym	O
upon	O
receiving	O
a	O
training	B
set	B
s	O
the	O
algorithm	O
a	O
returns	O
a	O
hypothesis	B
h	O
x	O
since	O
the	O
error	O
of	O
a	O
w	O
r	O
t	O
db	O
only	O
depends	O
on	O
hc	O
we	O
can	O
think	O
of	O
a	O
as	O
a	O
mapping	O
from	O
into	O
therefore	O
we	O
denote	O
by	O
ay	O
the	O
value	O
in	O
corresponding	O
to	O
the	O
prediction	O
of	O
hc	O
where	O
h	O
is	O
the	O
hypothesis	B
that	O
a	O
outputs	O
upon	O
receiving	O
the	O
training	B
set	B
s	O
ym	O
note	O
that	O
for	O
any	O
hypothesis	B
h	O
we	O
have	O
ldb	O
hcb	O
in	O
particular	O
the	O
bayes	B
optimal	I
hypothesis	B
is	O
hb	O
and	O
ldb	O
ldb	O
ayb	O
if	O
ay	O
b	O
otherwise	O
fix	O
a	O
for	O
b	O
let	O
y	O
b	O
ay	O
b	O
the	O
distribution	O
db	O
induces	O
a	O
probability	O
pb	O
over	O
hence	O
p	O
ldb	O
dby	O
b	O
y	O
denote	O
n	O
yi	O
and	O
n	O
n	O
note	O
that	O
for	O
any	O
y	O
n	O
we	O
have	O
py	O
p	O
and	O
for	O
any	O
y	O
n	O
we	O
have	O
p	O
py	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
therefore	O
p	O
p	O
max	O
b	O
max	O
b	O
y	O
y	O
y	O
y	O
n	O
p	O
ldb	O
p	O
y	O
n	O
p	O
exp	O
py	O
y	O
n	O
y	O
n	O
next	O
note	O
that	O
y	O
n	O
p	O
y	O
n	O
y	O
n	O
p	O
y	O
n	O
py	O
and	O
both	O
values	O
are	O
the	O
probability	O
that	O
a	O
binomial	O
random	O
variable	O
will	O
have	O
value	O
greater	O
than	O
using	O
lemma	O
this	O
probability	O
is	O
lower	O
bounded	O
by	O
exp	O
where	O
we	O
used	O
the	O
assumption	O
that	O
it	O
follows	O
that	O
if	O
m	O
then	O
there	O
exists	O
b	O
such	O
that	O
p	O
ldb	O
where	O
the	O
last	O
inequality	O
follows	O
by	O
standard	O
algebraic	O
manipulations	O
this	O
concludes	O
our	O
proof	O
let	O
and	O
note	O
that	O
showing	O
that	O
m	O
we	O
shall	O
now	O
prove	O
that	O
for	O
every	O
we	O
will	O
construct	O
a	O
family	O
of	O
distributions	O
as	O
follows	O
first	O
let	O
c	O
cd	O
be	O
a	O
set	B
of	O
d	O
instances	O
which	O
are	O
shattered	O
by	O
h	O
second	O
for	O
each	O
vector	O
bd	O
define	O
a	O
distribution	O
db	O
such	O
that	O
we	O
have	O
that	O
m	O
d	O
if	O
i	O
x	O
ci	O
otherwise	O
dbx	O
y	O
that	O
is	O
to	O
sample	O
an	O
example	O
according	O
to	O
db	O
we	O
first	O
sample	O
an	O
element	O
ci	O
c	O
uniformly	O
at	O
random	O
and	O
then	O
set	B
the	O
label	B
to	O
be	O
bi	O
with	O
probability	O
or	O
bi	O
with	O
probability	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
bayes	B
optimal	I
predictor	B
for	O
db	O
is	O
the	O
hypothesis	B
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
h	O
h	O
such	O
that	O
hci	O
bi	O
for	O
all	O
i	O
and	O
its	O
error	O
is	O
any	O
other	O
function	B
f	O
x	O
it	O
is	O
easy	O
to	O
verify	O
that	O
in	O
addition	O
for	O
ldb	O
f	O
bi	O
d	O
f	O
bi	O
d	O
therefore	O
ldb	O
min	O
h	O
h	O
ldb	O
f	O
bi	O
d	O
next	O
fix	O
some	O
learning	O
algorithm	O
a	O
as	O
in	O
the	O
proof	O
of	O
the	O
no-free-lunch	B
theorem	O
we	O
have	O
that	O
max	O
dbb	O
e	O
s	O
dm	O
b	O
ldb	O
min	O
h	O
h	O
ldb	O
ldb	O
min	O
asci	O
bi	O
h	O
h	O
ldb	O
d	O
e	O
dbb	O
u	O
e	O
s	O
dm	O
b	O
e	O
dbb	O
u	O
e	O
s	O
dm	O
b	O
d	O
e	O
dbb	O
u	O
e	O
s	O
dm	O
b	O
d	O
where	O
the	O
first	O
equality	O
follows	O
from	O
equation	O
in	O
addition	O
using	O
the	O
definition	O
of	O
db	O
to	O
sample	O
s	O
db	O
we	O
can	O
first	O
sample	O
jm	O
u	O
set	B
xr	O
cji	O
and	O
finally	O
sample	O
yr	O
such	O
that	O
pyr	O
bji	O
let	O
us	O
simplify	O
the	O
notation	O
and	O
use	O
y	O
b	O
to	O
denote	O
sampling	O
according	O
to	O
py	O
b	O
therefore	O
the	O
right-hand	O
side	O
of	O
equation	O
equals	O
e	O
j	O
u	O
e	O
b	O
u	O
e	O
ryr	O
bjr	O
we	O
now	O
proceed	O
in	O
two	O
steps	O
first	O
we	O
show	O
that	O
among	O
all	O
learning	O
algorithms	O
a	O
the	O
one	O
which	O
minimizes	O
equation	O
hence	O
also	O
equation	O
is	O
the	O
maximum-likelihood	O
learning	O
rule	O
denoted	O
am	O
l	O
formally	O
for	O
each	O
i	O
am	O
lsci	O
is	O
the	O
majority	O
vote	O
among	O
the	O
set	B
r	O
xr	O
ci	O
second	O
we	O
lower	O
bound	O
equation	O
for	O
am	O
l	O
lemma	O
among	O
all	O
algorithms	O
equation	O
is	O
minimized	O
for	O
a	O
being	O
the	O
maximum-likelihood	O
algorithm	O
am	O
l	O
defined	O
as	O
i	O
am	O
lsci	O
sign	O
yr	O
proof	O
fix	O
some	O
j	O
note	O
that	O
given	O
j	O
and	O
y	O
the	O
training	B
set	B
s	O
is	O
fully	O
determined	O
therefore	O
we	O
can	O
write	O
aj	O
y	O
instead	O
of	O
as	O
let	O
us	O
also	O
fix	O
i	O
denote	O
b	O
i	O
the	O
sequence	O
bi	O
bm	O
also	O
for	O
any	O
rxrci	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
y	O
let	O
yi	O
denote	O
the	O
elements	O
of	O
y	O
corresponding	O
to	O
indices	O
for	O
which	O
jr	O
i	O
and	O
let	O
y	O
i	O
be	O
the	O
rest	O
of	O
the	O
elements	O
of	O
y	O
we	O
have	O
b	O
u	O
ryr	O
bjr	O
e	O
e	O
b	O
i	O
u	O
bi	O
y	O
e	O
y	O
i	O
p	O
i	O
yi	O
bi	O
e	O
b	O
i	O
u	O
p	O
ib	O
i	O
p	O
the	O
sum	O
within	O
the	O
parentheses	O
is	O
minimized	O
when	O
aj	O
yci	O
is	O
the	O
maximizer	O
of	O
p	O
over	O
bi	O
which	O
is	O
exactly	O
the	O
maximum-likelihood	O
rule	O
repeating	O
the	O
same	O
argument	O
for	O
all	O
i	O
we	O
conclude	O
our	O
proof	O
fix	O
i	O
for	O
every	O
j	O
let	O
nij	O
jt	O
i	O
be	O
the	O
number	O
of	O
instances	O
in	O
which	O
the	O
instance	B
is	O
ci	O
for	O
the	O
maximum-likelihood	O
rule	O
we	O
have	O
that	O
the	O
quantity	O
e	O
b	O
u	O
e	O
ryr	O
bjr	O
is	O
exactly	O
the	O
probability	O
that	O
a	O
binomial	O
random	O
variable	O
will	O
be	O
larger	O
than	O
using	O
lemma	O
and	O
the	O
assumption	O
we	O
have	O
that	O
e	O
p	O
we	O
have	O
thus	O
shown	O
that	O
d	O
e	O
j	O
u	O
b	O
u	O
e	O
e	O
ryr	O
bjr	O
e	O
e	O
j	O
u	O
e	O
j	O
u	O
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
the	O
inequality	O
e	O
a	O
a	O
since	O
the	O
square	O
root	O
function	B
is	O
concave	O
we	O
can	O
apply	O
jensen	O
s	O
inequality	O
to	O
obtain	O
that	O
the	O
above	O
is	O
lower	O
bounded	O
by	O
j	O
u	O
nij	O
e	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
as	O
long	O
as	O
m	O
d	O
this	O
term	O
would	O
be	O
larger	O
than	O
in	O
summary	O
we	O
have	O
shown	O
that	O
if	O
m	O
d	O
then	O
for	O
any	O
algorithm	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
e	O
ldas	O
min	O
h	O
h	O
ldh	O
s	O
dm	O
minh	O
h	O
ldh	O
and	O
note	O
that	O
finally	O
let	O
equation	O
therefore	O
using	O
lemma	O
we	O
get	O
that	O
pldas	O
min	O
h	O
h	O
ldh	O
p	O
e	O
choosing	O
we	O
conclude	O
that	O
if	O
m	O
d	O
we	O
will	O
have	O
ldas	O
minh	O
h	O
ldh	O
then	O
with	O
probability	O
of	O
at	O
least	O
the	O
upper	O
bound	O
for	O
the	O
realizable	O
case	O
here	O
we	O
prove	O
that	O
there	O
exists	O
c	O
such	O
that	O
h	O
is	O
pac	B
learnable	O
with	O
sample	B
complexity	I
mh	O
c	O
d	O
we	O
do	O
so	O
by	O
showing	O
that	O
for	O
m	O
c	O
d	O
erm	B
rule	O
we	O
prove	O
this	O
claim	O
based	O
on	O
the	O
notion	O
of	O
definition	O
let	O
x	O
be	O
a	O
domain	B
s	O
x	O
is	O
an	O
for	O
h	O
with	O
respect	O
to	O
a	O
distribution	O
d	O
over	O
x	O
if	O
h	O
is	O
learnable	O
using	O
the	O
h	O
h	O
dh	O
h	O
s	O
theorem	O
let	O
h	O
with	O
vcdimh	O
d	O
fix	O
and	O
let	O
m	O
log	O
log	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
a	O
choice	O
of	O
s	O
dm	O
we	O
have	O
that	O
s	O
is	O
an	O
for	O
h	O
proof	O
let	O
b	O
x	O
m	O
h	O
hdh	O
h	O
s	O
be	O
the	O
set	B
of	O
sets	O
which	O
are	O
not	O
we	O
need	O
to	O
bound	O
ps	O
b	O
define	O
t	O
x	O
m	O
h	O
hdh	O
h	O
s	O
h	O
the	O
upper	O
bound	O
for	O
the	O
realizable	O
case	O
claim	O
ps	O
b	O
ps	O
t	O
proof	O
of	O
claim	O
since	O
s	O
and	O
t	O
are	O
chosen	O
independently	O
we	O
can	O
write	O
e	O
e	O
ps	O
t	O
e	O
s	O
dm	O
t	O
dm	O
note	O
that	O
t	O
implies	O
s	O
b	O
and	O
therefore	O
b	O
which	O
gives	O
ps	O
t	O
e	O
s	O
dm	O
e	O
s	O
dm	O
e	O
t	O
dm	O
b	O
e	O
t	O
dm	O
b	O
fix	O
some	O
s	O
then	O
either	O
b	O
or	O
s	O
b	O
and	O
then	O
hs	O
such	O
that	O
dhs	O
and	O
s	O
it	O
follows	O
that	O
a	O
sufficient	O
condition	O
for	O
t	O
is	O
that	O
hs	O
therefore	O
whenever	O
s	O
b	O
we	O
have	O
e	O
t	O
dm	O
p	O
t	O
dm	O
hs	O
but	O
since	O
we	O
now	O
assume	O
s	O
b	O
we	O
know	O
that	O
dhs	O
therefore	O
hs	O
is	O
a	O
binomial	O
random	O
variable	O
with	O
parameters	O
of	O
success	O
for	O
a	O
single	O
try	O
and	O
m	O
of	O
tries	O
chernoff	O
s	O
inequality	O
implies	O
pt	O
hs	O
m	O
thus	O
pt	O
hs	O
combining	O
all	O
the	O
preceding	O
we	O
conclude	O
the	O
proof	O
of	O
claim	O
pt	O
hs	O
m	O
pt	O
hs	O
m	O
m	O
e	O
e	O
m	O
e	O
e	O
d	O
claim	O
ps	O
t	O
e	O
proof	O
of	O
claim	O
to	O
simplify	O
notation	O
let	O
and	O
for	O
a	O
sequence	O
a	O
let	O
xm	O
using	O
the	O
definition	O
of	O
we	O
get	O
that	O
pa	O
e	O
e	O
a	O
a	O
max	O
h	O
h	O
max	O
h	O
h	O
a	O
a	O
now	O
let	O
us	O
define	O
by	O
ha	O
the	O
effective	O
number	O
of	O
different	O
hypotheses	O
on	O
a	O
namely	O
ha	O
a	O
h	O
h	O
it	O
follows	O
that	O
pa	O
e	O
e	O
a	O
a	O
max	O
h	O
ha	O
h	O
ha	O
a	O
a	O
let	O
j	O
m	O
for	O
any	O
j	O
j	O
and	O
a	O
define	O
aj	O
xjm	O
since	O
the	O
elements	O
of	O
a	O
are	O
chosen	O
i	O
i	O
d	O
we	O
have	O
that	O
for	O
any	O
j	O
j	O
and	O
any	O
function	B
f	O
it	O
holds	O
that	O
ea	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
ea	O
aj	O
since	O
this	O
holds	O
for	O
any	O
j	O
it	O
also	O
holds	O
for	O
the	O
expectation	O
of	O
j	O
chosen	O
at	O
random	O
from	O
j	O
in	O
particular	O
it	O
holds	O
for	O
the	O
function	B
f	O
h	O
ha	O
a	O
we	O
therefore	O
obtain	O
that	O
pa	O
e	O
a	O
e	O
a	O
e	O
j	O
j	O
h	O
ha	O
a	O
h	O
ha	O
a	O
e	O
j	O
j	O
now	O
fix	O
some	O
a	O
s	O
t	O
a	O
then	O
ej	O
is	O
the	O
probability	O
that	O
when	O
choosing	O
m	O
balls	O
from	O
a	O
bag	O
with	O
at	O
least	O
red	O
balls	O
we	O
will	O
never	O
choose	O
a	O
red	O
ball	O
this	O
probability	O
is	O
at	O
most	O
e	O
we	O
therefore	O
get	O
that	O
pa	O
e	O
a	O
h	O
ha	O
e	O
e	O
e	O
a	O
using	O
the	O
definition	O
of	O
the	O
growth	B
function	B
we	O
conclude	O
the	O
proof	O
of	O
claim	O
completing	O
the	O
proof	O
by	O
sauer	O
s	O
lemma	O
we	O
know	O
that	O
combining	O
this	O
with	O
the	O
two	O
claims	O
we	O
obtain	O
that	O
ps	O
b	O
e	O
we	O
would	O
like	O
the	O
right-hand	O
side	O
of	O
the	O
inequality	O
to	O
be	O
at	O
most	O
that	O
is	O
e	O
rearranging	O
we	O
obtain	O
the	O
requirement	O
m	O
logm	O
using	O
lemma	O
a	O
sufficient	O
condition	O
for	O
the	O
preceding	O
to	O
hold	O
is	O
that	O
a	O
sufficient	O
condition	O
for	O
this	O
is	O
that	O
d	O
m	O
log	O
log	O
log	O
m	O
log	O
log	O
and	O
this	O
concludes	O
our	O
proof	O
the	O
upper	O
bound	O
for	O
the	O
realizable	O
case	O
from	O
to	O
pac	B
learnability	O
theorem	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
over	O
x	O
with	O
vcdimh	O
d	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
and	O
let	O
c	O
h	O
be	O
a	O
target	O
hypothesis	B
fix	O
and	O
let	O
m	O
be	O
as	O
defined	O
in	O
theorem	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
a	O
choice	O
of	O
m	O
i	O
i	O
d	O
instances	O
from	O
x	O
with	O
labels	O
according	O
to	O
c	O
we	O
have	O
that	O
any	O
erm	B
hypothesis	B
has	O
a	O
true	B
error	I
of	O
at	O
most	O
proof	O
define	O
the	O
class	O
hc	O
h	O
h	O
h	O
where	O
h	O
c	O
h	O
it	O
is	O
note	O
that	O
ldh	O
c	O
therefore	O
for	O
any	O
h	O
h	O
with	O
ldh	O
we	O
have	O
that	O
c	O
s	O
which	O
implies	O
that	O
h	O
cannot	O
be	O
an	O
erm	B
hypothesis	B
which	O
easy	O
to	O
verify	O
that	O
if	O
some	O
a	O
x	O
is	O
shattered	O
by	O
h	O
then	O
it	O
is	O
also	O
shattered	O
by	O
hc	O
and	O
vice	O
versa	O
hence	O
vcdimh	O
vcdimhc	O
therefore	O
using	O
theorem	O
we	O
know	O
that	O
with	O
probability	O
of	O
at	O
least	O
the	O
sample	O
s	O
is	O
an	O
for	O
hc	O
concludes	O
our	O
proof	O
multiclass	B
learnability	O
in	O
chapter	O
we	O
have	O
introduced	O
the	O
problem	O
of	O
multiclass	B
categorization	O
in	O
which	O
the	O
goal	O
is	O
to	O
learn	O
a	O
predictor	B
h	O
x	O
in	O
this	O
chapter	O
we	O
address	O
pac	B
learnability	O
of	O
multiclass	B
predictors	O
with	O
respect	O
to	O
the	O
loss	B
as	O
in	O
chapter	O
the	O
main	O
goal	O
of	O
this	O
chapter	O
is	O
to	O
characterize	O
which	O
classes	O
of	O
multiclass	B
hypotheses	O
are	O
learnable	O
in	O
the	O
ticlass	O
pac	B
model	O
quantify	O
the	O
sample	B
complexity	I
of	O
such	O
hypothesis	B
classes	O
in	O
view	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
it	O
is	O
natural	O
to	O
seek	O
a	O
generalization	O
of	O
the	O
vc	B
dimension	I
to	O
multiclass	B
hypothesis	B
classes	O
in	O
section	O
we	O
show	O
such	O
a	O
generalization	O
called	O
the	O
natarajan	B
dimension	I
and	O
state	O
a	O
generalization	O
of	O
the	O
fundamental	O
theorem	O
based	O
on	O
the	O
natarajan	B
dimension	I
then	O
we	O
demonstrate	O
how	O
to	O
calculate	O
the	O
natarajan	B
dimension	I
of	O
several	O
important	O
hypothesis	B
classes	O
recall	B
that	O
the	O
main	O
message	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
is	O
that	O
a	O
hypothesis	B
class	I
of	O
binary	O
classifiers	O
is	O
learnable	O
respect	O
to	O
the	O
loss	B
if	O
and	O
only	O
if	O
it	O
has	O
the	O
uniform	B
convergence	I
property	O
and	O
then	O
it	O
is	O
learnable	O
by	O
any	O
erm	B
learner	O
in	O
chapter	O
exercise	O
we	O
have	O
shown	O
that	O
this	O
equivalence	O
breaks	O
down	O
for	O
a	O
certain	O
convex	B
learning	O
problem	O
the	O
last	O
section	O
of	O
this	O
chapter	O
is	O
devoted	O
to	O
showing	O
that	O
the	O
equivalence	O
between	O
learnability	O
and	O
uniform	B
convergence	I
breaks	O
down	O
even	O
in	O
multiclass	B
problems	O
with	O
the	O
loss	B
which	O
are	O
very	O
similar	O
to	O
binary	O
classification	O
indeed	O
we	O
construct	O
a	O
hypothesis	B
class	I
which	O
is	O
learnable	O
by	O
a	O
specific	O
erm	B
learner	O
but	O
for	O
which	O
other	O
erm	B
learners	O
might	O
fail	O
and	O
the	O
uniform	B
convergence	I
property	O
does	O
not	O
hold	O
the	O
natarajan	B
dimension	I
in	O
this	O
section	O
we	O
define	O
the	O
natarajan	B
dimension	I
which	O
is	O
a	O
generalization	O
of	O
the	O
vc	B
dimension	I
to	O
classes	O
of	O
multiclass	B
predictors	O
throughout	O
this	O
section	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
multiclass	B
predictors	O
namely	O
each	O
h	O
h	O
is	O
a	O
function	B
from	O
x	O
to	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
the	O
multiclass	B
fundamental	O
theorem	O
to	O
define	O
the	O
natarajan	B
dimension	I
we	O
first	O
generalize	O
the	O
definition	O
of	O
shat	O
tering	O
definition	O
version	O
we	O
say	O
that	O
a	O
set	B
c	O
x	O
is	O
shattered	O
by	O
h	O
if	O
there	O
exist	O
two	O
functions	O
c	O
such	O
that	O
for	O
every	O
x	O
c	O
for	O
every	O
b	O
c	O
there	O
exists	O
a	O
function	B
h	O
h	O
such	O
that	O
x	O
b	O
hx	O
and	O
x	O
c	O
b	O
hx	O
definition	O
dimension	O
the	O
natarajan	B
dimension	I
of	O
h	O
denoted	O
ndimh	O
is	O
the	O
maximal	O
size	O
of	O
a	O
shattered	O
set	B
c	O
x	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
in	O
the	O
case	O
that	O
there	O
are	O
exactly	O
two	O
classes	O
ndimh	O
vcdimh	O
therefore	O
the	O
natarajan	B
dimension	I
generalizes	O
the	O
vc	B
dimension	I
we	O
next	O
show	O
that	O
the	O
natarajan	B
dimension	I
allows	O
us	O
to	O
generalize	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
from	O
binary	O
classification	O
to	O
multiclass	B
classification	O
the	O
multiclass	B
fundamental	O
theorem	O
theorem	O
multiclass	B
fundamental	O
theorem	O
there	O
exist	O
absolute	O
constants	O
such	O
that	O
the	O
following	O
holds	O
for	O
every	O
hypothesis	B
class	I
h	O
of	O
functions	O
from	O
x	O
to	O
such	O
that	O
the	O
natarajan	B
dimension	I
of	O
h	O
is	O
d	O
we	O
have	O
h	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
sample	B
complexity	I
d	O
log	O
d	O
much	O
h	O
is	O
agnostic	B
pac	B
learnable	O
with	O
sample	B
complexity	I
d	O
mh	O
d	O
log	O
d	O
kd	O
h	O
is	O
pac	B
learnable	O
realizability	B
with	O
sample	B
complexity	I
d	O
mh	O
on	O
the	O
proof	O
of	O
theorem	O
the	O
lower	O
bounds	O
in	O
theorem	O
can	O
be	O
deduced	O
by	O
a	O
reduction	O
from	O
the	O
binary	O
fundamental	O
theorem	O
exercise	O
the	O
upper	O
bounds	O
in	O
theorem	O
can	O
be	O
proved	O
along	O
the	O
same	O
lines	O
of	O
the	O
proof	O
of	O
the	O
fundamental	O
theorem	O
for	O
binary	O
classification	O
given	O
in	O
chapter	O
exercise	O
the	O
sole	O
ingredient	O
of	O
that	O
proof	O
that	O
should	O
be	O
modified	O
in	O
a	O
nonstraightforward	O
manner	O
is	O
sauer	O
s	O
lemma	O
it	O
applies	O
only	O
to	O
binary	O
classes	O
and	O
therefore	O
must	O
be	O
replaced	O
an	O
appropriate	O
substitute	O
is	O
natarajan	O
s	O
lemma	O
multiclass	B
learnability	O
lemma	O
the	O
proof	O
of	O
natarajan	O
s	O
lemma	O
shares	O
the	O
same	O
spirit	O
of	O
the	O
proof	O
of	O
sauer	O
s	O
lemma	O
and	O
is	O
left	O
as	O
an	O
exercise	O
exercise	O
calculating	O
the	O
natarajan	B
dimension	I
in	O
this	O
section	O
we	O
show	O
how	O
to	O
calculate	O
estimate	O
the	O
natarajan	B
dimension	I
of	O
several	O
popular	O
classes	O
some	O
of	O
which	O
were	O
studied	O
in	O
chapter	O
as	O
these	O
calculations	O
indicate	O
the	O
natarajan	B
dimension	I
is	O
often	O
proportional	O
to	O
the	O
number	O
of	O
parameters	O
required	O
to	O
define	O
a	O
hypothesis	B
one-versus-all	O
based	O
classes	O
in	O
chapter	O
we	O
have	O
seen	O
two	O
reductions	B
of	O
multiclass	B
categorization	O
to	O
binary	O
classification	O
one-versus-all	O
and	O
all-pairs	B
in	O
this	O
section	O
we	O
calculate	O
the	O
natarajan	B
dimension	I
of	O
the	O
one-versus-all	O
method	O
recall	B
that	O
in	O
one-versus-all	O
we	O
train	O
for	O
each	O
label	B
a	O
binary	O
classifier	B
that	O
distinguishes	O
between	O
that	O
label	B
and	O
the	O
rest	O
of	O
the	O
labels	O
this	O
naturally	O
suggests	O
considering	O
multiclass	B
hypothesis	B
classes	O
of	O
the	O
following	O
form	O
let	O
hbin	O
be	O
a	O
binary	O
hypothesis	B
class	I
for	O
every	O
h	O
hk	O
define	O
t	O
h	O
x	O
by	O
t	O
hx	O
argmax	O
i	O
hix	O
if	O
there	O
are	O
two	O
labels	O
that	O
maximize	O
hix	O
we	O
choose	O
the	O
smaller	O
one	O
also	O
let	O
hovak	O
bin	O
h	O
h	O
what	O
should	O
be	O
the	O
natarajan	B
dimension	I
of	O
hovak	O
intuitively	O
to	O
specify	O
a	O
hypothesis	B
in	O
hbin	O
we	O
need	O
d	O
vcdimhbin	O
parameters	O
to	O
specify	O
a	O
hypothesis	B
in	O
hovak	O
we	O
need	O
to	O
specify	O
k	O
hypotheses	O
in	O
hbin	O
therefore	O
kd	O
parameters	O
should	O
suffice	O
the	O
following	O
lemma	O
establishes	O
this	O
intuition	O
bin	O
bin	O
lemma	O
if	O
d	O
vcdimhbin	O
then	O
ndimhovak	O
bin	O
log	O
proof	O
let	O
c	O
x	O
be	O
a	O
shattered	O
set	B
by	O
the	O
definition	O
of	O
shattering	B
multiclass	B
hypotheses	O
is	O
determined	O
by	O
using	O
k	O
hypothe	O
ses	O
from	O
hbin	O
on	O
the	O
other	O
hand	O
each	O
hypothesis	B
in	O
hovak	O
bin	O
bin	O
c	O
bin	O
c	O
calculating	O
the	O
natarajan	B
dimension	I
by	O
sauer	O
s	O
lemma	O
we	O
conclude	O
that	O
bin	O
c	O
the	O
proof	O
follows	O
by	O
taking	O
the	O
logarithm	O
and	O
applying	O
lemma	O
how	O
tight	O
is	O
lemma	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
for	O
some	O
classes	O
ndimhovak	O
bin	O
can	O
be	O
much	O
smaller	O
than	O
dk	O
exercise	O
however	O
there	O
are	O
several	O
natural	O
binary	O
classes	O
hbin	O
halfspaces	O
for	O
which	O
ndimhovak	O
exercise	O
bin	O
general	O
multiclass-to-binary	O
reductions	B
the	O
same	O
reasoning	O
used	O
to	O
establish	O
lemma	O
can	O
be	O
used	O
to	O
upper	O
bound	O
the	O
natarajan	B
dimension	I
of	O
more	O
general	O
multiclass-to-binary	O
reductions	B
these	O
reductions	B
train	O
several	O
binary	O
classifiers	O
on	O
the	O
data	O
then	O
given	O
a	O
new	O
instance	B
they	O
predict	O
its	O
label	B
by	O
using	O
some	O
rule	O
that	O
takes	O
into	O
account	O
the	O
labels	O
predicted	O
by	O
the	O
binary	O
classifiers	O
these	O
reductions	B
include	O
one-versusall	O
and	O
all-pairs	B
suppose	O
that	O
such	O
a	O
method	O
trains	O
l	O
binary	O
classifiers	O
from	O
a	O
binary	O
class	O
hbin	O
and	O
r	O
is	O
the	O
rule	O
that	O
determines	O
the	O
label	B
according	O
to	O
the	O
predictions	O
of	O
the	O
binary	O
classifiers	O
the	O
hypothesis	B
class	I
corresponding	O
to	O
this	O
method	O
can	O
be	O
defined	O
as	O
follows	O
for	O
every	O
h	O
hl	O
define	O
r	O
h	O
x	O
by	O
finally	O
let	O
r	O
hx	O
hlx	O
hr	O
bin	O
h	O
h	O
similarly	O
to	O
lemma	O
it	O
can	O
be	O
proven	O
that	O
lemma	O
if	O
d	O
vcdimhbin	O
then	O
ndimhr	O
bin	O
l	O
d	O
log	O
d	O
the	O
proof	O
is	O
left	O
as	O
exercise	O
linear	O
multiclass	B
predictors	O
next	O
we	O
consider	O
the	O
class	O
of	O
linear	O
multiclass	B
predictors	O
section	O
let	O
x	O
rd	O
be	O
some	O
class-sensitive	B
feature	B
mapping	I
and	O
let	O
h	O
x	O
argmax	O
i	O
w	O
rd	O
each	O
hypothesis	B
in	O
h	O
is	O
determined	O
by	O
d	O
parameters	O
namely	O
a	O
vector	O
w	O
rd	O
therefore	O
we	O
would	O
expect	O
that	O
the	O
natarajan	B
dimension	I
would	O
be	O
upper	O
bounded	O
by	O
d	O
indeed	O
multiclass	B
learnability	O
theorem	O
ndimh	O
d	O
proof	O
let	O
c	O
x	O
be	O
a	O
shattered	O
set	B
and	O
let	O
c	O
be	O
the	O
two	O
functions	O
that	O
witness	O
the	O
shattering	B
we	O
need	O
to	O
show	O
that	O
d	O
for	O
every	O
x	O
c	O
let	O
we	O
claim	O
that	O
the	O
set	B
def	O
x	O
c	O
consists	O
of	O
elements	O
is	O
one	O
to	O
one	O
and	O
is	O
shattered	O
by	O
the	O
binary	O
hypothesis	B
class	I
of	O
homogeneous	O
linear	O
separators	O
on	O
rd	O
h	O
w	O
rd	O
since	O
vcdimh	O
d	O
it	O
will	O
follow	O
that	O
d	O
as	O
required	O
to	O
establish	O
our	O
claim	O
it	O
is	O
enough	O
to	O
show	O
that	O
indeed	O
given	O
a	O
subset	O
b	O
c	O
by	O
the	O
definition	O
of	O
shattering	B
there	O
exists	O
hb	O
h	O
for	O
which	O
x	O
b	O
hbx	O
and	O
x	O
c	O
b	O
hbx	O
let	O
wb	O
rd	O
be	O
a	O
vector	O
that	O
defines	O
hb	O
we	O
have	O
that	O
for	O
every	O
x	O
b	O
similarly	O
for	O
every	O
x	O
c	O
b	O
it	O
follows	O
that	O
the	O
hypothesis	B
gb	O
h	O
defined	O
by	O
the	O
same	O
w	O
rd	O
label	B
the	O
points	O
in	O
by	O
and	O
the	O
points	O
in	O
b	O
by	O
since	O
this	O
holds	O
for	O
every	O
b	O
c	O
we	O
obtain	O
that	O
and	O
which	O
concludes	O
our	O
proof	O
the	O
theorem	O
is	O
tight	O
in	O
the	O
sense	O
that	O
there	O
are	O
mappings	O
for	O
which	O
ndimh	O
for	O
example	O
this	O
is	O
true	O
for	O
the	O
multivector	O
construction	O
section	O
and	O
the	O
bibliographic	O
remarks	O
at	O
the	O
end	O
of	O
this	O
chapter	O
we	O
therefore	O
conclude	O
corollary	O
let	O
x	O
rn	O
and	O
let	O
x	O
rnk	O
be	O
the	O
class	O
sensitive	O
feature	B
mapping	O
for	O
the	O
multi-vector	B
construction	O
y	O
ry	O
rk	O
yn	O
rn	O
xn	O
let	O
h	O
be	O
as	O
defined	O
in	O
equation	O
then	O
the	O
natarajan	B
dimension	I
of	O
h	O
satisfies	O
ndimh	O
kn	O
on	O
good	O
and	O
bad	O
erms	O
in	O
this	O
section	O
we	O
present	O
an	O
example	O
of	O
a	O
hypothesis	B
class	I
with	O
the	O
property	O
that	O
not	O
all	O
erms	O
for	O
the	O
class	O
are	O
equally	O
successful	O
furthermore	O
if	O
we	O
allow	O
an	O
infinite	O
number	O
of	O
labels	O
we	O
will	O
also	O
obtain	O
an	O
example	O
of	O
a	O
class	O
that	O
is	O
on	O
good	O
and	O
bad	O
erms	O
learnable	O
by	O
some	O
erm	B
but	O
other	O
erms	O
will	O
fail	O
to	O
learn	O
it	O
clearly	O
this	O
also	O
implies	O
that	O
the	O
class	O
is	O
learnable	O
but	O
it	O
does	O
not	O
have	O
the	O
uniform	B
convergence	I
property	O
for	O
simplicity	O
we	O
consider	O
only	O
the	O
realizable	O
case	O
the	O
class	O
we	O
consider	O
is	O
defined	O
as	O
follows	O
the	O
instance	B
space	I
x	O
will	O
be	O
any	O
finite	O
or	O
countable	O
set	B
let	O
pf	O
be	O
the	O
collection	O
of	O
all	O
finite	O
and	O
cofinite	O
subsets	O
of	O
x	O
is	O
for	O
each	O
a	O
pf	O
either	O
a	O
or	O
x	O
a	O
must	O
be	O
finite	O
instead	O
of	O
the	O
label	B
set	B
is	O
y	O
pf	O
where	O
is	O
some	O
special	O
label	B
for	O
every	O
a	O
pf	O
define	O
ha	O
x	O
y	O
by	O
hax	O
a	O
x	O
a	O
x	O
a	O
finally	O
the	O
hypothesis	B
class	I
we	O
take	O
is	O
h	O
a	O
pf	O
let	O
a	O
be	O
some	O
erm	B
algorithm	O
for	O
h	O
assume	O
that	O
a	O
operates	O
on	O
a	O
sample	O
labeled	O
by	O
ha	O
h	O
since	O
ha	O
is	O
the	O
only	O
hypothesis	B
in	O
h	O
that	O
might	O
return	O
the	O
label	B
a	O
if	O
a	O
observes	O
the	O
label	B
a	O
it	O
knows	O
that	O
the	O
learned	O
hypothesis	B
is	O
ha	O
and	O
as	O
an	O
erm	B
must	O
return	O
it	O
that	O
in	O
this	O
case	O
the	O
error	O
of	O
the	O
returned	O
hypothesis	B
is	O
therefore	O
to	O
specify	O
an	O
erm	B
we	O
should	O
only	O
specify	O
the	O
hypothesis	B
it	O
returns	O
upon	O
receiving	O
a	O
sample	O
of	O
the	O
form	O
s	O
we	O
consider	O
two	O
erms	O
the	O
first	O
agood	O
is	O
defined	O
by	O
agoods	O
h	O
that	O
is	O
it	O
outputs	O
the	O
hypothesis	B
which	O
predicts	O
for	O
every	O
x	O
x	O
the	O
second	O
erm	B
abad	O
is	O
defined	O
by	O
abads	O
the	O
following	O
claim	O
shows	O
that	O
the	O
sample	B
complexity	I
of	O
abad	O
is	O
about	O
larger	O
than	O
the	O
sample	B
complexity	I
of	O
agood	O
this	O
establishes	O
a	O
gap	O
between	O
different	O
erms	O
if	O
x	O
is	O
infinite	O
we	O
even	O
obtain	O
a	O
learnable	O
class	O
that	O
is	O
not	O
learnable	O
by	O
every	O
erm	B
examples	O
sampled	O
according	O
to	O
d	O
and	O
labeled	O
by	O
claim	O
let	O
d	O
a	O
distribution	O
over	O
x	O
and	O
ha	O
h	O
let	O
s	O
be	O
an	O
i	O
i	O
d	O
sample	O
consisting	O
of	O
m	O
ha	O
then	O
with	O
probability	O
of	O
at	O
least	O
the	O
hypothesis	B
returned	O
by	O
agood	O
will	O
have	O
an	O
error	O
of	O
at	O
most	O
there	O
exists	O
a	O
constant	O
a	O
such	O
that	O
for	O
every	O
a	O
there	O
exists	O
a	O
distribution	O
d	O
over	O
x	O
and	O
ha	O
h	O
such	O
that	O
the	O
following	O
holds	O
the	O
hypothesis	B
returned	O
by	O
abad	O
upon	O
receiving	O
a	O
sample	O
of	O
size	O
m	O
sampled	O
according	O
to	O
d	O
and	O
labeled	O
by	O
ha	O
will	O
have	O
error	O
with	O
probability	O
e	O
multiclass	B
learnability	O
proof	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
and	O
suppose	O
that	O
the	O
correct	O
labeling	O
is	O
ha	O
for	O
any	O
sample	O
agood	O
returns	O
either	O
h	O
or	O
ha	O
if	O
it	O
returns	O
ha	O
then	O
its	O
true	B
error	I
is	O
zero	O
thus	O
it	O
returns	O
a	O
hypothesis	B
with	O
error	O
only	O
if	O
all	O
the	O
m	O
examples	O
in	O
the	O
sample	O
are	O
from	O
x	O
a	O
while	O
the	O
error	O
of	O
h	O
ldh	O
pda	O
is	O
assume	O
m	O
then	O
the	O
probability	O
of	O
the	O
latter	O
event	O
is	O
no	O
more	O
than	O
e	O
this	O
establishes	O
item	O
next	O
we	O
prove	O
item	O
we	O
restrict	O
the	O
proof	O
to	O
the	O
case	O
that	O
d	O
log	O
the	O
proof	O
for	O
infinite	O
x	O
is	O
similar	O
suppose	O
that	O
x	O
xd	O
let	O
a	O
be	O
small	O
enough	O
such	O
that	O
e	O
for	O
every	O
a	O
and	O
fix	O
some	O
a	O
define	O
a	O
distribution	O
on	O
x	O
by	O
setting	O
and	O
for	O
all	O
i	O
d	O
pxi	O
d	O
suppose	O
that	O
the	O
correct	O
hypothesis	B
is	O
h	O
and	O
let	O
the	O
sample	O
size	O
be	O
m	O
clearly	O
the	O
hypothesis	B
returned	O
by	O
abad	O
will	O
err	O
on	O
all	O
the	O
examples	O
from	O
x	O
which	O
are	O
not	O
in	O
the	O
sample	O
by	O
chernoff	O
s	O
bound	O
if	O
m	O
d	O
then	O
with	O
probability	O
e	O
examples	O
from	O
x	O
thus	O
the	O
returned	O
hypothesis	B
will	O
have	O
error	O
the	O
sample	O
will	O
include	O
no	O
more	O
than	O
d	O
the	O
conclusion	O
of	O
the	O
example	O
presented	O
is	O
that	O
in	O
multiclass	B
classification	O
the	O
sample	B
complexity	I
of	O
different	O
erms	O
may	O
differ	O
are	O
there	O
good	O
erms	O
for	O
every	O
hypothesis	B
class	I
the	O
following	O
conjecture	O
asserts	O
that	O
the	O
answer	O
is	O
yes	O
conjecture	O
the	O
realizable	O
sample	B
complexity	I
of	O
every	O
hypothesis	B
class	I
h	O
is	O
x	O
ndimh	O
mh	O
o	O
we	O
emphasize	O
that	O
the	O
o	O
notation	O
may	O
hide	O
only	O
poly-log	O
factors	O
of	O
and	O
ndimh	O
but	O
no	O
factor	O
of	O
k	O
bibliographic	O
remarks	O
the	O
natarajan	B
dimension	I
is	O
due	O
to	O
natarajan	O
that	O
paper	O
also	O
established	O
the	O
natarajan	O
lemma	O
and	O
the	O
generalization	O
of	O
the	O
fundamental	O
theorem	O
generalizations	O
and	O
sharper	O
versions	O
of	O
the	O
natarajan	O
lemma	O
are	O
studied	O
in	O
haussler	O
long	O
ben-david	O
cesa-bianchi	O
haussler	O
long	O
defined	O
a	O
large	O
family	O
of	O
notions	O
of	O
dimensions	O
all	O
of	O
which	O
generalize	O
the	O
vc	B
dimension	I
and	O
may	O
be	O
used	O
to	O
estimate	O
the	O
sample	B
complexity	I
of	O
multiclass	B
classification	O
the	O
calculation	O
of	O
the	O
natarajan	B
dimension	I
presented	O
here	O
together	O
with	O
calculation	O
of	O
other	O
classes	O
can	O
be	O
found	O
in	O
daniely	O
et	O
al	O
the	O
example	O
of	O
good	O
and	O
bad	O
erms	O
as	O
well	O
as	O
conjecture	O
are	O
from	O
daniely	O
et	O
al	O
exercises	O
exercises	O
let	O
d	O
k	O
show	O
that	O
there	O
exists	O
a	O
binary	O
hypothesis	B
hbin	O
of	O
vc	B
dimension	I
d	O
such	O
that	O
ndimhovak	O
d	O
bin	O
prove	O
lemma	O
prove	O
natarajan	O
s	O
lemma	O
hint	O
fix	O
some	O
x	O
for	O
i	O
j	O
denote	O
by	O
hij	O
all	O
the	O
functions	O
f	O
x	O
that	O
can	O
be	O
extended	O
to	O
a	O
function	B
in	O
h	O
both	O
by	O
defining	O
f	O
i	O
and	O
by	O
defining	O
f	O
j	O
show	O
that	O
and	O
use	O
induction	O
adapt	O
the	O
proof	O
of	O
the	O
binary	O
fundamental	O
theorem	O
and	O
natarajan	O
s	O
lemma	O
to	O
prove	O
that	O
for	O
some	O
universal	O
constant	O
c	O
and	O
for	O
every	O
hypothesis	B
class	I
of	O
natarajan	B
dimension	I
d	O
the	O
agnostic	O
sample	B
complexity	I
of	O
h	O
is	O
d	O
kd	O
mh	O
c	O
prove	O
that	O
for	O
some	O
universal	O
constant	O
c	O
and	O
for	O
every	O
hypothesis	B
class	I
of	O
natarajan	B
dimension	I
d	O
the	O
agnostic	O
sample	B
complexity	I
of	O
h	O
is	O
mh	O
c	O
d	O
hint	O
deduce	O
it	O
from	O
the	O
binary	O
fundamental	O
theorem	O
let	O
h	O
be	O
the	O
binary	O
hypothesis	B
class	I
of	O
halfspaces	O
in	O
rd	O
the	O
goal	O
of	O
this	O
exercise	O
is	O
to	O
prove	O
that	O
ndimhovak	O
let	O
hdiscrete	O
be	O
the	O
class	O
of	O
all	O
functions	O
f	O
for	O
which	O
there	O
exists	O
some	O
such	O
that	O
for	O
every	O
j	O
i	O
f	O
j	O
while	O
i	O
f	O
j	O
show	O
that	O
hdiscrete	O
can	O
be	O
realized	O
by	O
h	O
that	O
is	O
show	O
that	O
there	O
exists	O
show	O
that	O
ndimhovak	O
discrete	O
a	O
mapping	O
rd	O
such	O
that	O
hdiscrete	O
h	O
h	O
hint	O
you	O
can	O
take	O
j	O
to	O
be	O
the	O
vector	O
whose	O
jth	O
coordinate	O
is	O
whose	O
last	O
coordinate	O
is	O
i	O
and	O
the	O
rest	O
are	O
zeros	O
conclude	O
that	O
ndimhovak	O
compression	B
bounds	I
throughout	O
the	O
book	O
we	O
have	O
tried	O
to	O
characterize	O
the	O
notion	O
of	O
learnability	O
using	O
different	O
approaches	O
at	O
first	O
we	O
have	O
shown	O
that	O
the	O
uniform	B
convergence	I
property	O
of	O
a	O
hypothesis	B
class	I
guarantees	O
successful	O
learning	O
later	O
on	O
we	O
introduced	O
the	O
notion	O
of	O
stability	B
and	O
have	O
shown	O
that	O
stable	O
algorithms	O
are	O
guaranteed	O
to	O
be	O
good	O
learners	O
yet	O
there	O
are	O
other	O
properties	O
which	O
may	O
be	O
sufficient	O
for	O
learning	O
and	O
in	O
this	O
chapter	O
and	O
its	O
sequel	O
we	O
will	O
introduce	O
two	O
approaches	O
to	O
this	O
issue	O
compression	B
bounds	I
and	O
the	O
pac-bayes	B
approach	O
in	O
this	O
chapter	O
we	O
study	O
compression	B
bounds	I
roughly	O
speaking	O
we	O
shall	O
see	O
that	O
if	O
a	O
learning	O
algorithm	O
can	O
express	O
the	O
output	O
hypothesis	B
using	O
a	O
small	O
subset	O
of	O
the	O
training	B
set	B
then	O
the	O
error	O
of	O
the	O
hypothesis	B
on	O
the	O
rest	O
of	O
the	O
examples	O
estimates	O
its	O
true	B
error	I
in	O
other	O
words	O
an	O
algorithm	O
that	O
can	O
compress	O
its	O
output	O
is	O
a	O
good	O
learner	O
compression	B
bounds	I
to	O
motivate	O
the	O
results	O
let	O
us	O
first	O
consider	O
the	O
following	O
learning	O
protocol	O
first	O
we	O
sample	O
a	O
sequence	O
of	O
k	O
examples	O
denoted	O
t	O
on	O
the	O
basis	O
of	O
these	O
examples	O
we	O
construct	O
a	O
hypothesis	B
denoted	O
ht	O
now	O
we	O
would	O
like	O
to	O
estimate	O
the	O
performance	O
of	O
ht	O
so	O
we	O
sample	O
a	O
fresh	O
sequence	O
of	O
m	O
k	O
examples	O
denoted	O
v	O
and	O
calculate	O
the	O
error	O
of	O
ht	O
on	O
v	O
since	O
v	O
and	O
t	O
are	O
independent	O
we	O
immediately	O
get	O
the	O
following	O
from	O
bernstein	O
s	O
inequality	O
lemma	O
lemma	O
assume	O
that	O
the	O
range	O
of	O
the	O
loss	B
function	B
is	O
then	O
p	O
ldht	O
lv	O
to	O
derive	O
this	O
bound	O
all	O
we	O
needed	O
was	O
independence	O
between	O
t	O
and	O
v	O
therefore	O
we	O
can	O
redefine	O
the	O
protocol	O
as	O
follows	O
first	O
we	O
agree	O
on	O
a	O
sequence	O
of	O
k	O
indices	O
i	O
ik	O
then	O
we	O
sample	O
a	O
sequence	O
of	O
m	O
examples	O
s	O
zm	O
now	O
define	O
t	O
si	O
zik	O
and	O
define	O
v	O
to	O
be	O
the	O
rest	O
of	O
the	O
examples	O
in	O
s	O
note	O
that	O
this	O
protocol	O
is	O
equivalent	O
to	O
the	O
protocol	O
we	O
defined	O
before	O
hence	O
lemma	O
still	O
holds	O
applying	O
a	O
union	B
bound	I
over	O
the	O
choice	O
of	O
the	O
sequence	O
of	O
indices	O
we	O
obtain	O
the	O
following	O
theorem	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
compression	B
bounds	I
theorem	O
let	O
k	O
be	O
an	O
integer	O
and	O
let	O
b	O
z	O
k	O
h	O
be	O
a	O
mapping	O
from	O
sequences	O
of	O
k	O
examples	O
to	O
the	O
hypothesis	B
class	I
let	O
m	O
be	O
a	O
training	B
set	B
size	O
and	O
let	O
a	O
z	O
m	O
h	O
be	O
a	O
learning	O
rule	O
that	O
receives	O
a	O
training	O
sequence	O
s	O
of	O
size	O
m	O
and	O
returns	O
a	O
hypothesis	B
such	O
that	O
as	O
zik	O
for	O
some	O
ik	O
let	O
v	O
j	O
ik	O
be	O
the	O
set	B
of	O
examples	O
which	O
were	O
not	O
selected	O
for	O
defining	O
as	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
we	O
have	O
ldas	O
lv	O
lv	O
logm	O
m	O
logm	O
m	O
p	O
proof	O
for	O
any	O
i	O
let	O
hi	O
zik	O
let	O
n	O
m	O
k	O
combining	O
lemma	O
with	O
the	O
union	B
bound	I
we	O
have	O
i	O
s	O
t	O
ldhi	O
lv	O
p	O
ldhi	O
lv	O
n	O
n	O
n	O
n	O
i	O
mk	O
denote	O
mk	O
using	O
the	O
assumption	O
k	O
which	O
implies	O
that	O
n	O
m	O
k	O
the	O
above	O
implies	O
that	O
with	O
probability	O
of	O
at	O
least	O
we	O
have	O
that	O
ldas	O
lv	O
lv	O
logm	O
m	O
logm	O
m	O
which	O
concludes	O
our	O
proof	O
as	O
a	O
direct	O
corollary	O
we	O
obtain	O
corollary	O
assuming	O
the	O
conditions	O
of	O
theorem	O
and	O
further	O
assuming	O
that	O
lv	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
s	O
we	O
have	O
ldas	O
logm	O
m	O
these	O
results	O
motivate	O
the	O
following	O
definition	O
scheme	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
definition	O
functions	O
from	O
x	O
to	O
y	O
and	O
let	O
k	O
be	O
an	O
integer	O
we	O
say	O
that	O
h	O
has	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
if	O
the	O
following	O
holds	O
for	O
all	O
m	O
there	O
exists	O
a	O
z	O
m	O
and	O
b	O
z	O
k	O
h	O
such	O
that	O
for	O
all	O
h	O
h	O
if	O
we	O
feed	O
any	O
training	B
set	B
of	O
the	O
form	O
hxm	O
into	O
a	O
and	O
then	O
feed	O
hxik	O
into	O
b	O
where	O
ik	O
is	O
the	O
output	O
of	O
a	O
then	O
the	O
output	O
of	O
b	O
denoted	O
satisfies	O
it	O
is	O
possible	O
to	O
generalize	O
the	O
definition	O
for	O
unrealizable	O
sequences	O
as	O
follows	O
compression	B
bounds	I
definition	O
scheme	O
for	O
unrealizable	O
sequences	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
x	O
to	O
y	O
and	O
let	O
k	O
be	O
an	O
integer	O
we	O
say	O
that	O
h	O
has	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
if	O
the	O
following	O
holds	O
for	O
all	O
m	O
there	O
exists	O
a	O
z	O
m	O
and	O
b	O
z	O
k	O
h	O
such	O
that	O
for	O
all	O
h	O
h	O
if	O
we	O
feed	O
any	O
training	B
set	B
of	O
the	O
form	O
ym	O
into	O
a	O
and	O
then	O
feed	O
yik	O
into	O
b	O
where	O
ik	O
is	O
the	O
output	O
of	O
a	O
then	O
the	O
output	O
of	O
b	O
denoted	O
satisfies	O
lsh	O
the	O
following	O
lemma	O
shows	O
that	O
the	O
existence	O
of	O
a	O
compression	B
scheme	I
for	O
the	O
realizable	O
case	O
also	O
implies	O
the	O
existence	O
of	O
a	O
compression	B
scheme	I
for	O
the	O
unrealizable	O
case	O
lemma	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
for	O
binary	O
classification	O
and	O
assume	O
it	O
has	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
in	O
the	O
realizable	O
case	O
then	O
it	O
has	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
for	O
the	O
unrealizable	O
case	O
as	O
well	O
proof	O
consider	O
the	O
following	O
scheme	O
first	O
find	O
an	O
erm	B
hypothesis	B
and	O
denote	O
it	O
by	O
h	O
then	O
discard	O
all	O
the	O
examples	O
on	O
which	O
h	O
errs	O
now	O
apply	O
the	O
realizable	O
compression	B
scheme	I
on	O
the	O
examples	O
that	O
have	O
not	O
been	O
removed	O
the	O
output	O
of	O
the	O
realizable	O
compression	B
scheme	I
denoted	O
must	O
be	O
correct	O
on	O
the	O
examples	O
that	O
have	O
not	O
been	O
removed	O
since	O
h	O
errs	O
on	O
the	O
removed	O
examples	O
it	O
follows	O
that	O
the	O
error	O
of	O
cannot	O
be	O
larger	O
than	O
the	O
error	O
of	O
h	O
hence	O
is	O
also	O
an	O
erm	B
hypothesis	B
examples	O
in	O
the	O
examples	O
that	O
follows	O
we	O
present	O
compression	O
schemes	O
for	O
several	O
hypothesis	B
classes	O
for	O
binary	O
classification	O
in	O
light	O
of	O
lemma	O
we	O
focus	O
on	O
the	O
realizable	O
case	O
therefore	O
to	O
show	O
that	O
a	O
certain	O
hypothesis	B
class	I
has	O
a	O
compression	B
scheme	I
it	O
is	O
necessary	O
to	O
show	O
that	O
there	O
exist	O
a	O
b	O
and	O
k	O
for	O
which	O
axis	O
aligned	O
rectangles	O
note	O
that	O
this	O
is	O
an	O
uncountable	O
infinite	O
class	O
we	O
show	O
that	O
there	O
is	O
a	O
simple	O
compression	B
scheme	I
consider	O
the	O
algorithm	O
a	O
that	O
works	O
as	O
follows	O
for	O
each	O
dimension	O
choose	O
the	O
two	O
positive	O
examples	O
with	O
extremal	O
values	O
at	O
this	O
dimension	O
define	O
b	O
to	O
be	O
the	O
function	B
that	O
returns	O
the	O
minimal	O
enclosing	O
rectangle	O
then	O
for	O
k	O
we	O
have	O
that	O
in	O
the	O
realizable	O
case	O
lsbas	O
halfspaces	O
let	O
x	O
rd	O
and	O
consider	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
w	O
rd	O
examples	O
a	O
compression	B
scheme	I
w	O
l	O
o	O
g	O
assume	O
all	O
labels	O
are	O
positive	O
replace	O
xi	O
by	O
yixi	O
the	O
compression	B
scheme	I
we	O
propose	O
is	O
as	O
follows	O
first	O
a	O
finds	O
the	O
vector	O
w	O
which	O
is	O
in	O
the	O
convex	B
hull	O
of	O
xm	O
and	O
has	O
minimal	O
norm	O
then	O
it	O
represents	O
it	O
as	O
a	O
convex	B
combination	O
of	O
d	O
points	O
in	O
the	O
sample	O
will	O
be	O
shown	O
later	O
that	O
this	O
is	O
always	O
possible	O
the	O
output	O
of	O
a	O
are	O
these	O
d	O
points	O
the	O
algorithm	O
b	O
receives	O
these	O
d	O
points	O
and	O
set	B
w	O
to	O
be	O
the	O
point	O
in	O
their	O
convex	B
hull	O
of	O
minimal	O
norm	O
next	O
we	O
prove	O
that	O
this	O
indeed	O
is	O
a	O
compression	O
sceme	O
since	O
the	O
data	O
is	O
linearly	O
separable	B
the	O
convex	B
hull	O
of	O
xm	O
does	O
not	O
contain	O
the	O
origin	O
consider	O
the	O
point	O
w	O
in	O
this	O
convex	B
hull	O
closest	O
to	O
the	O
origin	O
is	O
a	O
unique	O
point	O
which	O
is	O
the	O
euclidean	O
projection	B
of	O
the	O
origin	O
onto	O
this	O
convex	B
hull	O
we	O
claim	O
that	O
w	O
separates	O
the	O
to	O
see	O
this	O
assume	O
by	O
contradiction	O
that	O
for	O
some	O
i	O
take	O
xi	O
for	O
then	O
is	O
also	O
in	O
the	O
convex	B
hull	O
and	O
which	O
leads	O
to	O
a	O
contradiction	O
we	O
have	O
thus	O
shown	O
that	O
w	O
is	O
also	O
an	O
erm	B
finally	O
since	O
w	O
is	O
in	O
the	O
convex	B
hull	O
of	O
the	O
examples	O
we	O
can	O
apply	O
caratheodory	O
s	O
theorem	O
to	O
obtain	O
that	O
w	O
is	O
also	O
in	O
the	O
convex	B
hull	O
of	O
a	O
subset	O
of	O
d	O
points	O
of	O
the	O
polygon	O
furthermore	O
the	O
minimality	O
of	O
w	O
implies	O
that	O
w	O
must	O
be	O
on	O
a	O
face	O
of	O
the	O
polygon	O
and	O
this	O
implies	O
it	O
can	O
be	O
represented	O
as	O
a	O
convex	B
combination	O
of	O
d	O
points	O
it	O
remains	O
to	O
show	O
that	O
w	O
is	O
also	O
the	O
projection	B
onto	O
the	O
polygon	O
defined	O
by	O
the	O
d	O
points	O
but	O
this	O
must	O
be	O
true	O
on	O
one	O
hand	O
the	O
smaller	O
polygon	O
is	O
a	O
subset	O
of	O
the	O
larger	O
one	O
hence	O
the	O
projection	B
onto	O
the	O
smaller	O
cannot	O
be	O
smaller	O
in	O
norm	O
on	O
the	O
other	O
hand	O
w	O
itself	O
is	O
a	O
valid	O
solution	O
the	O
uniqueness	O
of	O
projection	B
concludes	O
our	O
proof	O
separating	O
polynomials	O
let	O
x	O
rd	O
and	O
consider	O
the	O
class	O
x	O
signpx	O
where	O
p	O
is	O
a	O
degree	O
r	O
polynomial	O
it	O
can	O
be	O
shown	O
that	O
w	O
is	O
the	O
direction	O
of	O
the	O
max-margin	O
solution	O
compression	B
bounds	I
note	O
that	O
px	O
can	O
be	O
rewritten	O
as	O
where	O
the	O
elements	O
of	O
are	O
all	O
the	O
monomials	O
of	O
x	O
up	O
to	O
degree	O
r	O
therefore	O
the	O
problem	O
of	O
constructing	O
a	O
compression	B
scheme	I
for	O
px	O
reduces	O
to	O
the	O
problem	O
of	O
constructing	O
a	O
compression	B
scheme	I
for	O
halfspaces	O
in	O
where	O
odr	O
separation	O
with	O
margin	B
suppose	O
that	O
a	O
training	B
set	B
is	O
separated	O
with	O
margin	B
the	O
perceptron	B
algorithm	O
guarantees	O
to	O
make	O
at	O
most	O
updates	O
before	O
converging	O
to	O
a	O
solution	O
that	O
makes	O
no	O
mistakes	O
on	O
the	O
entire	O
training	B
set	B
hence	O
we	O
have	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
bibliographic	O
remarks	O
compression	O
schemes	O
and	O
their	O
relation	O
to	O
learning	O
were	O
introduced	O
by	O
littlestone	O
warmuth	O
as	O
we	O
have	O
shown	O
if	O
a	O
class	O
has	O
a	O
compression	B
scheme	I
then	O
it	O
is	O
learnable	O
for	O
binary	O
classification	O
problems	O
it	O
follows	O
from	O
the	O
fundamental	O
theorem	O
of	O
learning	O
that	O
the	O
class	O
has	O
a	O
finite	O
vc	B
dimension	I
the	O
other	O
direction	O
namely	O
whether	O
every	O
hypothesis	B
class	I
of	O
finite	O
vc	B
dimension	I
has	O
a	O
compression	B
scheme	I
of	O
finite	O
size	O
is	O
an	O
open	O
problem	O
posed	O
by	O
manfred	O
warmuth	O
and	O
is	O
still	O
open	O
also	O
floyd	O
warmuth	O
ben-david	O
litman	O
livni	O
simon	O
pac-bayes	B
the	O
minimum	O
description	O
length	O
and	O
occam	O
s	O
razor	O
principles	O
allow	O
a	O
potentially	O
very	O
large	O
hypothesis	B
class	I
but	O
define	O
a	O
hierarchy	O
over	O
hypotheses	O
and	O
prefer	O
to	O
choose	O
hypotheses	O
that	O
appear	O
higher	O
in	O
the	O
hierarchy	O
in	O
this	O
chapter	O
we	O
describe	O
the	O
pac-bayesian	O
approach	O
that	O
further	O
generalizes	O
this	O
idea	O
in	O
the	O
pac-bayesian	O
approach	O
one	O
expresses	O
the	O
prior	B
knowledge	I
by	O
defining	O
prior	O
distribution	O
over	O
the	O
hypothesis	B
class	I
pac-bayes	B
bounds	O
as	O
in	O
the	O
mdl	B
paradigm	O
we	O
define	O
a	O
hierarchy	O
over	O
hypotheses	O
in	O
our	O
class	O
h	O
now	O
the	O
hierarchy	O
takes	O
the	O
form	O
of	O
a	O
prior	O
distribution	O
over	O
h	O
that	O
is	O
we	O
assign	O
a	O
probability	O
density	O
if	O
h	O
is	O
continuous	O
p	O
for	O
each	O
h	O
h	O
and	O
refer	O
to	O
p	O
as	O
the	O
prior	O
score	O
of	O
h	O
following	O
the	O
bayesian	B
reasoning	I
approach	O
the	O
output	O
of	O
the	O
learning	O
algorithm	O
is	O
not	O
necessarily	O
a	O
single	O
hypothesis	B
instead	O
the	O
learning	O
process	O
defines	O
a	O
posterior	O
probability	O
over	O
h	O
which	O
we	O
denote	O
by	O
q	O
in	O
the	O
context	O
of	O
a	O
supervised	O
learning	O
problem	O
where	O
h	O
contains	O
functions	O
from	O
x	O
to	O
y	O
one	O
can	O
think	O
of	O
q	O
as	O
defining	O
a	O
randomized	O
prediction	O
rule	O
as	O
follows	O
whenever	O
we	O
get	O
a	O
new	O
instance	B
x	O
we	O
randomly	O
pick	O
a	O
hypothesis	B
h	O
h	O
according	O
to	O
q	O
and	O
predict	O
hx	O
we	O
define	O
the	O
loss	B
of	O
q	O
on	O
an	O
example	O
z	O
to	O
be	O
z	O
def	O
e	O
h	O
q	O
z	O
by	O
the	O
linearity	O
of	O
expectation	O
the	O
generalization	O
loss	B
and	O
training	O
loss	B
of	O
q	O
can	O
be	O
written	O
as	O
ldq	O
def	O
e	O
h	O
q	O
and	O
lsq	O
def	O
e	O
h	O
q	O
the	O
following	O
theorem	O
tells	O
us	O
that	O
the	O
difference	O
between	O
the	O
generalization	O
loss	B
and	O
the	O
empirical	O
loss	B
of	O
a	O
posterior	O
q	O
is	O
bounded	O
by	O
an	O
expression	O
that	O
depends	O
on	O
the	O
kullback-leibler	O
divergence	O
between	O
q	O
and	O
the	O
prior	O
distribution	O
p	O
the	O
kullback-leibler	O
is	O
a	O
natural	O
measure	O
of	O
the	O
distance	O
between	O
two	O
distributions	O
the	O
theorem	O
suggests	O
that	O
if	O
we	O
would	O
like	O
to	O
minimize	O
the	O
generalization	O
loss	B
of	O
q	O
we	O
should	O
jointly	O
minimize	O
both	O
the	O
empirical	O
loss	B
of	O
q	O
and	O
the	O
kullback-leibler	O
distance	O
between	O
q	O
and	O
the	O
prior	O
distribution	O
we	O
will	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
pac-bayes	B
later	O
show	O
how	O
in	O
some	O
cases	O
this	O
idea	O
leads	O
to	O
the	O
regularized	O
risk	B
minimization	O
principle	O
theorem	O
let	O
d	O
be	O
an	O
arbitrary	O
distribution	O
over	O
an	O
example	O
domain	B
z	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
and	O
let	O
h	O
z	O
be	O
a	O
loss	B
function	B
let	O
p	O
be	O
a	O
prior	O
distribution	O
over	O
h	O
and	O
let	O
then	O
with	O
probability	O
of	O
at	O
least	O
over	O
the	O
choice	O
of	O
an	O
i	O
i	O
d	O
training	B
set	B
s	O
zm	O
sampled	O
according	O
to	O
d	O
for	O
all	O
distributions	O
q	O
over	O
h	O
such	O
that	O
depend	O
on	O
s	O
we	O
have	O
ldq	O
lsq	O
dqp	O
ln	O
m	O
where	O
dqp	O
def	O
e	O
h	O
q	O
is	O
the	O
kullback-leibler	O
divergence	O
proof	O
for	O
any	O
function	B
f	O
using	O
markov	O
s	O
inequality	O
p	O
e	O
esef	O
s	O
e	O
p	O
s	O
let	O
ldh	O
lsh	O
we	O
will	O
apply	O
equation	O
with	O
the	O
function	B
f	O
sup	O
q	O
e	O
h	O
q	O
dqp	O
we	O
now	O
turn	O
to	O
bound	O
esef	O
the	O
main	O
trick	O
is	O
to	O
upper	O
bound	O
f	O
by	O
using	O
an	O
expression	O
that	O
does	O
not	O
depend	O
on	O
q	O
but	O
rather	O
depends	O
on	O
the	O
prior	O
probability	O
p	O
to	O
do	O
so	O
fix	O
some	O
s	O
and	O
note	O
that	O
from	O
the	O
definition	O
of	O
dqp	O
we	O
get	O
that	O
for	O
all	O
q	O
e	O
h	O
q	O
dqp	O
e	O
h	O
q	O
ln	O
e	O
h	O
q	O
ln	O
e	O
h	O
p	O
p	O
p	O
where	O
the	O
inequality	O
follows	O
from	O
jensen	O
s	O
inequality	O
and	O
the	O
concavity	O
of	O
the	O
log	O
function	B
therefore	O
e	O
e	O
s	O
s	O
e	O
h	O
p	O
the	O
advantage	O
of	O
the	O
expression	O
on	O
the	O
right-hand	O
side	O
stems	O
from	O
the	O
fact	O
that	O
we	O
can	O
switch	O
the	O
order	O
of	O
expectations	O
p	O
is	O
a	O
prior	O
that	O
does	O
not	O
depend	O
on	O
s	O
which	O
yields	O
e	O
e	O
h	O
p	O
s	O
e	O
s	O
bibliographic	O
remarks	O
next	O
we	O
claim	O
that	O
for	O
all	O
h	O
we	O
have	O
hoeffding	O
s	O
inequality	O
tells	O
us	O
that	O
m	O
to	O
do	O
so	O
recall	B
that	O
e	O
p	O
s	O
this	O
implies	O
that	O
equation	O
and	O
plugging	O
into	O
equation	O
we	O
get	O
m	O
exercise	O
combining	O
this	O
with	O
p	O
s	O
m	O
e	O
denote	O
the	O
right-hand	O
side	O
of	O
the	O
above	O
thus	O
lnm	O
and	O
we	O
therefore	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
we	O
have	O
that	O
for	O
all	O
q	O
e	O
h	O
q	O
dqp	O
lnm	O
rearranging	O
the	O
inequality	O
and	O
using	O
jensen	O
s	O
inequality	O
again	O
function	B
is	O
convex	B
we	O
conclude	O
that	O
e	O
h	O
q	O
e	O
h	O
q	O
lnm	O
dqp	O
remark	O
the	O
pac-bayes	B
bound	O
leads	O
to	O
the	O
following	O
learning	O
rule	O
given	O
a	O
prior	O
p	O
return	O
a	O
posterior	O
q	O
that	O
minimizes	O
the	O
function	B
lsq	O
dqp	O
ln	O
m	O
this	O
rule	O
is	O
similar	O
to	O
the	O
regularized	O
risk	B
minimization	O
principle	O
that	O
is	O
we	O
jointly	O
minimize	O
the	O
empirical	O
loss	B
of	O
q	O
on	O
the	O
sample	O
and	O
the	O
kullback-leibler	O
distance	O
between	O
q	O
and	O
p	O
bibliographic	O
remarks	O
pac-bayes	B
bounds	O
were	O
first	O
introduced	O
by	O
mcallester	O
see	O
also	O
mcallester	O
seeger	O
langford	O
shawe-taylor	O
langford	O
exercises	O
let	O
x	O
be	O
a	O
random	O
variable	O
that	O
satisfies	O
px	O
e	O
m	O
prove	O
that	O
pac-bayes	B
suppose	O
that	O
h	O
is	O
a	O
finite	O
hypothesis	B
class	I
set	B
the	O
prior	O
to	O
be	O
uniform	O
over	O
h	O
and	O
set	B
the	O
posterior	O
to	O
be	O
qhs	O
for	O
some	O
hs	O
and	O
qh	O
for	O
all	O
other	O
h	O
h	O
show	O
that	O
ldhs	O
lsh	O
lnh	O
lnm	O
compare	O
to	O
the	O
bounds	O
we	O
derived	O
using	O
uniform	B
convergence	I
derive	O
a	O
bound	O
similar	O
to	O
the	O
occam	O
bound	O
given	O
in	O
chapter	O
using	O
the	O
pac-bayes	B
bound	O
appendix	O
a	O
technical	O
lemmas	O
lemma	O
let	O
a	O
then	O
x	O
loga	O
x	O
a	O
logx	O
it	O
follows	O
that	O
a	O
necessary	O
condition	O
for	O
the	O
inequality	O
x	O
a	O
logx	O
to	O
hold	O
is	O
that	O
x	O
loga	O
e	O
the	O
inequality	O
x	O
a	O
logx	O
holds	O
unconproof	O
first	O
note	O
that	O
for	O
a	O
e	O
ditionally	O
and	O
therefore	O
the	O
claim	O
is	O
trivial	O
from	O
now	O
on	O
assume	O
that	O
a	O
consider	O
the	O
function	B
f	O
x	O
a	O
logx	O
the	O
derivative	O
is	O
ax	O
thus	O
for	O
x	O
a	O
the	O
derivative	O
is	O
positive	O
and	O
the	O
function	B
increases	O
in	O
addition	O
f	O
loga	O
loga	O
a	O
loga	O
loga	O
a	O
loga	O
a	O
loga	O
a	O
loga	O
a	O
loga	O
since	O
a	O
loga	O
for	O
all	O
a	O
the	O
proof	O
follows	O
lemma	O
let	O
a	O
and	O
b	O
then	O
x	O
x	O
a	O
logxb	O
it	O
suffices	O
to	O
prove	O
that	O
x	O
implies	O
that	O
both	O
x	O
proof	O
logx	O
and	O
x	O
since	O
we	O
assume	O
a	O
we	O
clearly	O
have	O
that	O
x	O
in	O
addition	O
since	O
b	O
we	O
have	O
that	O
x	O
which	O
using	O
lemma	O
implies	O
that	O
x	O
logx	O
this	O
concludes	O
our	O
proof	O
lemma	O
let	O
x	O
be	O
a	O
random	O
variable	O
and	O
r	O
be	O
a	O
scalar	O
and	O
assume	O
that	O
there	O
exists	O
a	O
such	O
that	O
for	O
all	O
t	O
we	O
have	O
px	O
t	O
then	O
ex	O
a	O
we	O
have	O
that	O
ex	O
is	O
at	O
with	O
the	O
assumption	O
in	O
the	O
lemma	O
we	O
get	O
that	O
ex	O
proof	O
for	O
all	O
i	O
denote	O
ti	O
a	O
i	O
since	O
ti	O
is	O
monotonically	O
increasing	O
ti	O
px	O
ti	O
combining	O
this	O
ie	O
the	O
proof	O
now	O
follows	O
from	O
the	O
inequalities	O
ie	O
ie	O
xe	O
dx	O
lemma	O
let	O
x	O
be	O
a	O
random	O
variable	O
and	O
r	O
be	O
a	O
scalar	O
and	O
assume	O
that	O
there	O
exists	O
a	O
and	O
b	O
e	O
such	O
that	O
for	O
all	O
t	O
we	O
have	O
px	O
t	O
e	O
then	O
ex	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
technical	O
lemmas	O
proof	O
for	O
all	O
i	O
denote	O
ti	O
a	O
since	O
ti	O
is	O
monotonically	O
increasing	O
we	O
have	O
that	O
ex	O
ti	O
px	O
ti	O
using	O
the	O
assumption	O
in	O
the	O
lemma	O
we	O
have	O
ti	O
px	O
ti	O
a	O
b	O
logb	O
e	O
logb	O
logb	O
a	O
b	O
a	O
b	O
a	O
b	O
a	O
b	O
xe	O
dx	O
dy	O
ye	O
dy	O
logb	O
a	O
bb	O
a	O
combining	O
the	O
preceding	O
inequalities	O
we	O
conclude	O
our	O
proof	O
lemma	O
let	O
m	O
d	O
be	O
two	O
positive	O
integers	O
such	O
that	O
d	O
m	O
then	O
k	O
e	O
m	O
d	O
proof	O
we	O
prove	O
the	O
claim	O
by	O
induction	O
for	O
d	O
the	O
left-hand	O
side	O
equals	O
m	O
while	O
the	O
right-hand	O
side	O
equals	O
em	B
hence	O
the	O
claim	O
is	O
true	O
assume	O
that	O
the	O
claim	O
holds	O
for	O
d	O
and	O
let	O
us	O
prove	O
it	O
for	O
d	O
by	O
the	O
induction	O
assumption	O
we	O
have	O
k	O
d	O
e	O
m	O
e	O
m	O
em	B
d	O
d	O
d	O
m	O
mm	O
d	O
d	O
d	O
d	O
e	O
m	O
e	O
technical	O
lemmas	O
d	O
dded	O
d	O
using	O
stirling	O
s	O
approximation	O
we	O
further	O
have	O
that	O
d	O
d	O
d	O
e	O
m	O
e	O
m	O
e	O
m	O
e	O
m	O
e	O
m	O
e	O
m	O
e	O
m	O
d	O
d	O
d	O
d	O
d	O
d	O
e	O
d	O
m	O
d	O
dd	O
d	O
d	O
d	O
m	O
e	O
m	O
e	O
m	O
e	O
m	O
e	O
m	O
em	B
em	B
em	B
m	O
d	O
d	O
d	O
d	O
d	O
d	O
d	O
d	O
d	O
d	O
d	O
d	O
e	O
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
the	O
assumption	O
that	O
d	O
m	O
on	O
the	O
other	O
hand	O
which	O
proves	O
our	O
inductive	O
argument	O
lemma	O
for	O
all	O
a	O
r	O
we	O
have	O
ea	O
e	O
a	O
proof	O
observe	O
that	O
therefore	O
and	O
ea	O
ea	O
e	O
a	O
an	O
n	O
n	O
observing	O
that	O
n	O
for	O
every	O
n	O
we	O
conclude	O
our	O
proof	O
appendix	O
b	O
measure	B
concentration	I
let	O
zm	O
be	O
an	O
i	O
i	O
d	O
sequence	O
of	O
random	O
variables	O
and	O
let	O
be	O
their	O
mean	O
the	O
strong	O
law	O
of	O
large	O
numbers	O
states	O
that	O
when	O
m	O
tends	O
to	O
infinity	O
the	O
empirical	O
average	O
zi	O
converges	O
to	O
the	O
expected	O
value	O
with	O
probability	O
m	O
measure	B
concentration	I
inequalities	O
quantify	O
the	O
deviation	O
of	O
the	O
empirical	O
average	O
from	O
the	O
expectation	O
when	O
m	O
is	O
finite	O
markov	O
s	O
inequality	O
a	O
we	O
start	O
with	O
an	O
inequality	O
which	O
is	O
called	O
markov	O
s	O
inequality	O
let	O
z	O
be	O
a	O
nonnegative	O
random	O
variable	O
the	O
expectation	O
of	O
z	O
can	O
be	O
written	O
as	O
follows	O
ez	O
pz	O
xdx	O
a	O
since	O
pz	O
x	O
is	O
monotonically	O
nonincreasing	O
we	O
obtain	O
a	O
ez	O
pz	O
xdx	O
pz	O
adx	O
a	O
pz	O
a	O
rearranging	O
the	O
inequality	O
yields	O
markov	O
s	O
inequality	O
a	O
pz	O
a	O
ez	O
a	O
for	O
random	O
variables	O
that	O
take	O
value	O
in	O
we	O
can	O
derive	O
from	O
markov	O
s	O
inequality	O
the	O
following	O
lemma	O
let	O
z	O
be	O
a	O
random	O
variable	O
that	O
takes	O
values	O
in	O
assume	O
that	O
ez	O
then	O
for	O
any	O
a	O
pz	O
a	O
a	O
a	O
this	O
also	O
implies	O
that	O
for	O
every	O
a	O
pz	O
a	O
a	O
a	O
a	O
proof	O
let	O
y	O
z	O
then	O
y	O
is	O
a	O
nonnegative	O
random	O
variable	O
with	O
ey	O
ez	O
applying	O
markov	O
s	O
inequality	O
on	O
y	O
we	O
obtain	O
pz	O
a	O
z	O
a	O
py	O
a	O
ey	O
a	O
a	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
chebyshev	O
s	O
inequality	O
therefore	O
pz	O
a	O
a	O
a	O
a	O
chebyshev	O
s	O
inequality	O
applying	O
markov	O
s	O
inequality	O
on	O
the	O
random	O
variable	O
we	O
obtain	O
chebyshev	O
s	O
inequality	O
a	O
pz	O
ez	O
a	O
pz	O
varz	O
where	O
varz	O
ez	O
is	O
the	O
variance	O
of	O
z	O
consider	O
the	O
random	O
variable	O
m	O
zi	O
since	O
zm	O
are	O
i	O
i	O
d	O
it	O
is	O
easy	O
to	O
verify	O
that	O
var	O
zi	O
m	O
m	O
applying	O
chebyshev	O
s	O
inequality	O
we	O
obtain	O
the	O
following	O
lemma	O
let	O
zm	O
be	O
a	O
sequence	O
of	O
i	O
i	O
d	O
random	O
variables	O
and	O
assume	O
that	O
and	O
then	O
for	O
any	O
with	O
probability	O
of	O
at	O
least	O
we	O
have	O
m	O
m	O
zi	O
a	O
m	O
p	O
zi	O
m	O
m	O
proof	O
applying	O
chebyshev	O
s	O
inequality	O
we	O
obtain	O
that	O
for	O
all	O
a	O
the	O
proof	O
follows	O
by	O
denoting	O
the	O
right-hand	O
side	O
and	O
solving	O
for	O
a	O
the	O
deviation	O
between	O
the	O
empirical	O
average	O
and	O
the	O
mean	O
given	O
previously	O
decreases	O
polynomially	O
with	O
m	O
it	O
is	O
possible	O
to	O
obtain	O
a	O
significantly	O
faster	O
decrease	O
in	O
the	O
sections	O
that	O
follow	O
we	O
derive	O
bounds	O
that	O
decrease	O
exponentially	O
fast	O
chernoff	O
s	O
bounds	O
pi	O
and	O
pzi	O
pi	O
let	O
p	O
let	O
zm	O
be	O
independent	O
bernoulli	O
variables	O
where	O
for	O
every	O
i	O
pzi	O
zi	O
using	O
the	O
pi	O
and	O
let	O
z	O
measure	B
concentration	I
monotonicity	O
of	O
the	O
exponent	O
function	B
and	O
markov	O
s	O
inequality	O
we	O
have	O
that	O
for	O
every	O
t	O
pz	O
petz	O
eetz	O
next	O
etzi	O
i	O
eetzi	O
i	O
zi	O
e	O
piet	O
eetz	O
i	O
piet	O
epiet	O
i	O
i	O
i	O
i	O
e	O
eet	O
by	O
independence	O
using	O
x	O
ex	O
combining	O
the	O
above	O
with	O
equation	O
and	O
choosing	O
t	O
we	O
obtain	O
i	O
pzi	O
pi	O
and	O
pzi	O
pi	O
let	O
p	O
lemma	O
let	O
zm	O
be	O
independent	O
bernoulli	O
variables	O
where	O
for	O
every	O
zi	O
pi	O
and	O
let	O
z	O
then	O
for	O
any	O
pz	O
e	O
h	O
p	O
where	O
h	O
using	O
the	O
inequality	O
ha	O
we	O
obtain	O
lemma	O
using	O
the	O
notation	O
of	O
lemma	O
we	O
also	O
have	O
pz	O
e	O
p	O
for	O
the	O
other	O
direction	O
we	O
apply	O
similar	O
calculations	O
pz	O
p	O
z	O
pe	O
tz	O
e	O
ee	O
tz	O
e	O
hoeffding	O
s	O
inequality	O
and	O
e	O
tzi	O
ee	O
tz	O
ee	O
ee	O
tzi	O
i	O
zi	O
e	O
pie	O
t	O
i	O
i	O
i	O
by	O
independence	O
epie	O
t	O
using	O
x	O
ex	O
i	O
ee	O
t	O
setting	O
t	O
yields	O
pz	O
it	O
is	O
easy	O
to	O
verify	O
that	O
h	O
h	O
and	O
hence	O
e	O
p	O
p	O
e	O
ph	O
lemma	O
using	O
the	O
notation	O
of	O
lemma	O
we	O
also	O
have	O
pz	O
e	O
ph	O
e	O
ph	O
e	O
p	O
hoeffding	O
s	O
inequality	O
lemma	O
s	O
inequality	O
let	O
zm	O
be	O
a	O
sequence	O
of	O
i	O
i	O
d	O
zi	O
assume	O
that	O
e	O
z	O
and	O
pa	O
random	O
variables	O
and	O
let	O
z	O
zi	O
b	O
for	O
every	O
i	O
then	O
for	O
any	O
m	O
m	O
zi	O
p	O
m	O
proof	O
denote	O
xi	O
zi	O
ezi	O
and	O
x	O
i	O
xi	O
using	O
the	O
monotonicity	O
of	O
the	O
exponent	O
function	B
and	O
markov	O
s	O
inequality	O
we	O
have	O
that	O
for	O
every	O
and	O
m	O
p	O
x	O
pe	O
x	O
e	O
e	O
ee	O
x	O
using	O
the	O
independence	O
assumption	O
we	O
also	O
have	O
ee	O
x	O
e	O
e	O
xim	O
ee	O
xim	O
by	O
hoeffding	O
s	O
lemma	O
later	O
for	O
every	O
i	O
we	O
have	O
i	O
i	O
ee	O
xim	O
e	O
measure	B
concentration	I
therefore	O
p	O
x	O
e	O
i	O
setting	O
we	O
obtain	O
e	O
e	O
p	O
x	O
e	O
applying	O
the	O
same	O
arguments	O
on	O
the	O
variable	O
x	O
we	O
obtain	O
that	O
p	O
x	O
e	O
the	O
theorem	O
follows	O
by	O
applying	O
the	O
union	B
bound	I
on	O
the	O
two	O
cases	O
lemma	O
s	O
lemma	O
let	O
x	O
be	O
a	O
random	O
variable	O
that	O
takes	O
values	O
in	O
the	O
interval	O
b	O
and	O
such	O
that	O
ex	O
then	O
for	O
every	O
ee	O
x	O
e	O
proof	O
since	O
f	O
e	O
x	O
is	O
a	O
convex	B
function	B
we	O
have	O
that	O
for	O
every	O
and	O
x	O
b	O
f	O
f	O
setting	O
b	O
x	O
b	O
a	O
yields	O
e	O
x	O
b	O
x	O
b	O
a	O
taking	O
the	O
expectation	O
we	O
obtain	O
that	O
e	O
a	O
x	O
a	O
b	O
a	O
e	O
b	O
ee	O
x	O
b	O
ex	O
b	O
a	O
e	O
a	O
ex	O
a	O
b	O
a	O
e	O
b	O
b	O
b	O
a	O
e	O
a	O
a	O
b	O
a	O
e	O
b	O
where	O
we	O
used	O
the	O
fact	O
that	O
ex	O
denote	O
h	O
a	O
p	O
a	O
b	O
a	O
and	O
lh	O
hp	O
p	O
peh	O
then	O
the	O
expression	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
above	O
can	O
be	O
rewritten	O
as	O
elh	O
therefore	O
to	O
conclude	O
our	O
proof	O
it	O
suffices	O
to	O
show	O
that	O
lh	O
this	O
follows	O
from	O
taylor	O
s	O
theorem	O
using	O
the	O
facts	O
and	O
for	O
all	O
h	O
bennet	O
s	O
and	O
bernstein	O
s	O
inequalities	O
bennet	O
s	O
and	O
bernsein	O
s	O
inequalities	O
are	O
similar	O
to	O
chernoff	O
s	O
bounds	O
but	O
they	O
hold	O
for	O
any	O
sequence	O
of	O
independent	O
random	O
variables	O
we	O
state	O
the	O
inequalities	O
without	O
proof	O
which	O
can	O
be	O
found	O
for	O
example	O
in	O
cesa-bianchi	O
lugosi	O
lemma	O
s	O
inequality	O
let	O
zm	O
be	O
independent	O
random	O
variables	O
with	O
zero	O
mean	O
and	O
assume	O
that	O
zi	O
with	O
probability	O
let	O
m	O
ez	O
i	O
bennet	O
s	O
and	O
bernstein	O
s	O
inequalities	O
then	O
for	O
all	O
where	O
zi	O
p	O
e	O
m	O
m	O
ha	O
a	O
a	O
a	O
by	O
using	O
the	O
inequality	O
ha	O
it	O
is	O
possible	O
to	O
derive	O
the	O
following	O
lemma	O
s	O
inequality	O
let	O
zm	O
be	O
i	O
i	O
d	O
random	O
variables	O
with	O
a	O
zero	O
mean	O
if	O
for	O
all	O
i	O
pzi	O
m	O
then	O
for	O
all	O
t	O
p	O
zi	O
t	O
exp	O
e	O
z	O
j	O
m	O
application	O
bernstein	O
s	O
inequality	O
can	O
be	O
used	O
to	O
interpolate	O
between	O
the	O
rate	O
we	O
derived	O
for	O
pac	B
learning	O
in	O
the	O
realizable	O
case	O
chapter	O
and	O
the	O
rate	O
we	O
derived	O
for	O
the	O
unrealizable	O
case	O
chapter	O
lemma	O
let	O
h	O
z	O
be	O
a	O
loss	B
function	B
let	O
d	O
be	O
an	O
arbitrary	O
distribution	O
over	O
z	O
fix	O
some	O
h	O
then	O
for	O
any	O
we	O
have	O
p	O
s	O
dm	O
p	O
s	O
dm	O
lsh	O
ldh	O
ldh	O
lsh	O
m	O
m	O
m	O
m	O
proof	O
define	O
random	O
variables	O
m	O
s	O
t	O
i	O
zi	O
ldh	O
note	O
that	O
e	O
i	O
and	O
that	O
e	O
i	O
zi	O
zi	O
ldh	O
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
the	O
fact	O
that	O
zi	O
and	O
thus	O
zi	O
applying	O
bernsein	O
s	O
inequality	O
over	O
the	O
i	O
s	O
yields	O
p	O
i	O
t	O
exp	O
exp	O
e	O
j	O
m	O
ldh	O
def	O
measure	B
concentration	I
solving	O
for	O
t	O
yields	O
m	O
ldh	O
t	O
m	O
ldh	O
m	O
ldh	O
i	O
i	O
lsh	O
ldh	O
it	O
follows	O
that	O
with	O
probability	O
of	O
at	O
least	O
since	O
m	O
t	O
m	O
ldh	O
lsh	O
ldh	O
ldh	O
m	O
which	O
proves	O
the	O
first	O
inequality	O
the	O
second	O
part	O
of	O
the	O
lemma	O
follows	O
in	O
a	O
similar	O
way	O
slud	O
s	O
inequality	O
let	O
x	O
be	O
a	O
p	O
binomial	O
variable	O
that	O
is	O
x	O
bility	O
that	O
a	O
normal	O
variable	O
will	O
be	O
greater	O
than	O
or	O
equal	O
the	O
zi	O
where	O
each	O
zi	O
is	O
with	O
probability	O
p	O
and	O
with	O
probability	O
p	O
assume	O
that	O
p	O
slud	O
s	O
inequality	O
tells	O
us	O
that	O
px	O
is	O
lower	O
bounded	O
by	O
the	O
proba	O
following	O
lemma	O
follows	O
by	O
standard	O
tail	O
bounds	O
for	O
the	O
normal	O
distribution	O
lemma	O
let	O
x	O
be	O
a	O
p	O
binomial	O
variable	O
and	O
assume	O
that	O
p	O
then	O
exp	O
px	O
concentration	O
of	O
variables	O
let	O
xk	O
be	O
k	O
independent	O
normally	O
distributed	O
random	O
variables	O
that	O
is	O
for	O
all	O
i	O
xi	O
n	O
the	O
distribution	O
of	O
the	O
random	O
variable	O
x	O
is	O
called	O
x	O
square	O
and	O
the	O
distribution	O
of	O
the	O
random	O
variable	O
z	O
x	O
k	O
k	O
square	O
with	O
k	O
degrees	O
of	O
freedom	O
clearly	O
ex	O
is	O
called	O
i	O
and	O
ez	O
k	O
the	O
following	O
lemma	O
states	O
that	O
x	O
k	O
is	O
concentrated	O
around	O
its	O
mean	O
lemma	O
let	O
z	O
i	O
k	O
then	O
for	O
all	O
we	O
have	O
pz	O
e	O
and	O
for	O
all	O
we	O
have	O
pz	O
e	O
concentration	O
of	O
variables	O
finally	O
for	O
all	O
proof	O
let	O
us	O
write	O
z	O
p	O
z	O
i	O
where	O
xi	O
n	O
to	O
prove	O
both	O
bounds	O
we	O
use	O
chernoff	O
s	O
bounding	O
method	O
for	O
the	O
first	O
inequality	O
we	O
first	O
bound	O
for	O
all	O
a	O
ee	O
x	O
we	O
have	O
that	O
where	O
will	O
be	O
specified	O
later	O
since	O
e	O
a	O
a	O
x	O
ee	O
x	O
ex	O
ex	O
and	O
ex	O
and	O
the	O
fact	O
that	O
using	O
the	O
well	O
known	O
equalities	O
ex	O
a	O
e	O
a	O
we	O
obtain	O
that	O
ee	O
x	O
p	O
z	O
now	O
applying	O
chernoff	O
s	O
bounding	O
method	O
we	O
get	O
that	O
e	O
e	O
z	O
e	O
e	O
x	O
e	O
k	O
e	O
k	O
choose	O
we	O
obtain	O
the	O
first	O
inequality	O
stated	O
in	O
the	O
lemma	O
for	O
the	O
second	O
inequality	O
we	O
use	O
a	O
known	O
closed	O
form	O
expression	O
for	O
the	O
moment	O
generating	O
function	B
of	O
a	O
k	O
distributed	O
random	O
variable	O
e	O
e	O
z	O
pz	O
e	O
on	O
the	O
basis	O
of	O
the	O
equation	O
and	O
using	O
chernoff	O
s	O
bounding	O
method	O
we	O
have	O
e	O
e	O
ek	O
e	O
where	O
the	O
last	O
inequality	O
occurs	O
because	O
a	O
e	O
a	O
setting	O
is	O
in	O
by	O
our	O
assumption	O
we	O
obtain	O
the	O
second	O
inequality	O
stated	O
in	O
the	O
lemma	O
finally	O
the	O
last	O
inequality	O
follows	O
from	O
the	O
first	O
two	O
inequalities	O
and	O
the	O
union	B
bound	I
appendix	O
c	O
linear	O
algebra	O
basic	O
definitions	O
in	O
this	O
chapter	O
we	O
only	O
deal	O
with	O
linear	O
algebra	O
over	O
finite	O
dimensional	O
euclidean	O
spaces	O
we	O
refer	O
to	O
vectors	O
as	O
column	O
vectors	O
given	O
two	O
d	O
dimensional	O
vectors	O
u	O
v	O
rd	O
their	O
inner	O
product	O
is	O
uivi	O
the	O
euclidean	O
norm	O
the	O
norm	O
is	O
we	O
also	O
use	O
the	O
norm	O
and	O
the	O
norm	O
maxi	O
a	O
subspace	O
of	O
rd	O
is	O
a	O
subset	O
of	O
rd	O
which	O
is	O
closed	O
under	O
addition	O
and	O
scalar	O
multiplication	O
the	O
span	O
of	O
a	O
set	B
of	O
vectors	O
uk	O
is	O
the	O
subspace	O
containing	O
all	O
vectors	O
of	O
the	O
form	O
iui	O
where	O
for	O
all	O
i	O
i	O
r	O
a	O
set	B
of	O
vectors	O
u	O
uk	O
is	O
independent	O
if	O
for	O
every	O
i	O
ui	O
is	O
not	O
in	O
the	O
span	O
of	O
ui	O
uk	O
we	O
say	O
that	O
u	O
spans	O
a	O
subspace	O
v	O
if	O
v	O
is	O
the	O
span	O
of	O
the	O
vectors	O
in	O
u	O
we	O
say	O
that	O
u	O
is	O
a	O
basis	O
of	O
v	O
if	O
it	O
is	O
both	O
independent	O
and	O
spans	O
v	O
the	O
dimension	O
of	O
v	O
is	O
the	O
size	O
of	O
a	O
basis	O
of	O
v	O
it	O
can	O
be	O
verified	O
that	O
all	O
bases	O
of	O
v	O
have	O
the	O
same	O
size	O
we	O
say	O
that	O
u	O
is	O
an	O
orthogonal	O
set	B
if	O
for	O
all	O
i	O
j	O
we	O
say	O
that	O
u	O
is	O
an	O
orthonormal	O
set	B
if	O
it	O
is	O
orthogonal	O
and	O
if	O
for	O
every	O
i	O
given	O
a	O
matrix	O
a	O
rnd	O
the	O
range	O
of	O
a	O
is	O
the	O
span	O
of	O
its	O
columns	O
and	O
the	O
null	O
space	O
of	O
a	O
is	O
the	O
subspace	O
of	O
all	O
vectors	O
that	O
satisfy	O
au	O
the	O
rank	O
of	O
a	O
is	O
the	O
dimension	O
of	O
its	O
range	O
the	O
transpose	O
of	O
a	O
matrix	O
a	O
denoted	O
is	O
the	O
matrix	O
whose	O
j	O
entry	O
equals	O
the	O
i	O
entry	O
of	O
a	O
we	O
say	O
that	O
a	O
is	O
symmetric	O
if	O
a	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
eigenvalues	O
and	O
eigenvectors	O
eigenvalues	O
and	O
eigenvectors	O
let	O
a	O
rdd	O
be	O
a	O
matrix	O
a	O
non-zero	O
vector	O
u	O
is	O
an	O
eigenvector	O
of	O
a	O
with	O
a	O
corresponding	O
eigenvalue	O
if	O
au	O
u	O
ui	O
is	O
an	O
eigenvector	O
of	O
a	O
furthermore	O
a	O
can	O
be	O
written	O
as	O
a	O
if	O
a	O
rdd	O
is	O
a	O
symmetric	O
matrix	O
of	O
theorem	O
decomposition	O
rank	O
k	O
then	O
there	O
exists	O
an	O
orthonormal	O
basis	O
of	O
rd	O
ud	O
such	O
that	O
each	O
i	O
where	O
each	O
i	O
is	O
the	O
eigenvalue	O
corresponding	O
to	O
the	O
eigenvector	O
ui	O
this	O
can	O
be	O
written	O
equivalently	O
as	O
a	O
u	O
where	O
the	O
columns	O
of	O
u	O
are	O
the	O
vectors	O
ud	O
and	O
d	O
is	O
a	O
diagonal	O
matrix	O
with	O
dii	O
i	O
and	O
for	O
i	O
j	O
dij	O
finally	O
the	O
number	O
of	O
i	O
which	O
are	O
nonzero	O
is	O
the	O
rank	O
of	O
the	O
matrix	O
the	O
eigenvectors	O
which	O
correspond	O
to	O
the	O
nonzero	O
eigenvalues	O
span	O
the	O
range	O
of	O
a	O
and	O
the	O
eigenvectors	O
which	O
correspond	O
to	O
zero	O
eigenvalues	O
span	O
the	O
null	O
space	O
of	O
a	O
positive	O
definite	O
matrices	O
a	O
symmetric	O
matrix	O
a	O
rdd	O
is	O
positive	O
definite	O
if	O
all	O
its	O
eigenvalues	O
are	O
positive	O
a	O
is	O
positive	O
semidefinite	O
if	O
all	O
its	O
eigenvalues	O
are	O
nonnegative	O
theorem	O
let	O
a	O
rdd	O
be	O
a	O
symmetric	O
matrix	O
then	O
the	O
following	O
are	O
equivalent	O
definitions	O
of	O
positive	O
semidefiniteness	O
of	O
a	O
all	O
the	O
eigenvalues	O
of	O
a	O
are	O
nonnegative	O
for	O
every	O
vector	O
u	O
there	O
exists	O
a	O
matrix	O
b	O
such	O
that	O
a	O
singular	O
value	O
decomposition	O
let	O
a	O
rmn	O
be	O
a	O
matrix	O
of	O
rank	O
r	O
when	O
m	O
n	O
the	O
eigenvalue	O
decomposition	O
given	O
in	O
theorem	O
cannot	O
be	O
applied	O
we	O
will	O
describe	O
another	O
decomposition	O
of	O
a	O
which	O
is	O
called	O
singular	O
value	O
decomposition	O
or	O
svd	B
for	O
short	O
unit	O
vectors	O
v	O
rn	O
and	O
u	O
rm	O
are	O
called	O
right	O
and	O
left	O
singular	O
vectors	O
of	O
a	O
with	O
corresponding	O
singular	O
value	O
if	O
av	O
u	O
and	O
v	O
we	O
first	O
show	O
that	O
if	O
we	O
can	O
find	O
r	O
orthonormal	O
singular	O
vectors	O
with	O
positive	O
singular	O
values	O
then	O
we	O
can	O
decompose	O
a	O
u	O
dv	O
with	O
the	O
columns	O
of	O
u	O
and	O
v	O
containing	O
the	O
left	O
and	O
right	O
singular	O
vectors	O
and	O
d	O
being	O
a	O
diagonal	O
r	O
r	O
matrix	O
with	O
the	O
singular	O
values	O
on	O
its	O
diagonal	O
linear	O
algebra	O
lemma	O
let	O
a	O
rmn	O
be	O
a	O
matrix	O
of	O
rank	O
r	O
assume	O
that	O
vr	O
is	O
an	O
orthonormal	O
set	B
of	O
right	O
singular	O
vectors	O
of	O
a	O
ur	O
is	O
an	O
orthonormal	O
set	B
of	O
corresponding	O
left	O
singular	O
vectors	O
of	O
a	O
and	O
r	O
are	O
the	O
corresponding	O
singular	O
values	O
then	O
a	O
i	O
it	O
follows	O
that	O
if	O
u	O
is	O
a	O
matrix	O
whose	O
columns	O
are	O
the	O
ui	O
s	O
v	O
is	O
a	O
matrix	O
whose	O
columns	O
are	O
the	O
vi	O
s	O
and	O
d	O
is	O
a	O
diagonal	O
matrix	O
with	O
dii	O
i	O
then	O
a	O
u	O
dv	O
adding	O
the	O
vectors	O
vn	O
define	O
b	O
proof	O
any	O
right	O
singular	O
vector	O
of	O
a	O
must	O
be	O
in	O
the	O
range	O
of	O
the	O
singular	O
value	O
will	O
have	O
to	O
be	O
zero	O
therefore	O
vr	O
is	O
an	O
orthonormal	O
basis	O
of	O
the	O
range	O
of	O
a	O
let	O
us	O
complete	O
it	O
to	O
an	O
orthonormal	O
basis	O
of	O
rn	O
by	O
i	O
it	O
suffices	O
to	O
prove	O
that	O
for	O
all	O
i	O
avi	O
bvi	O
clearly	O
if	O
i	O
r	O
then	O
avi	O
and	O
bvi	O
as	O
well	O
for	O
i	O
r	O
we	O
have	O
bvi	O
j	O
vi	O
iui	O
avi	O
where	O
the	O
last	O
equality	O
follows	O
from	O
the	O
definition	O
the	O
next	O
lemma	O
relates	O
the	O
singular	O
values	O
of	O
a	O
to	O
the	O
eigenvalues	O
of	O
and	O
lemma	O
v	O
u	O
are	O
right	O
and	O
left	O
singular	O
vectors	O
of	O
a	O
with	O
singular	O
value	O
iff	O
v	O
is	O
an	O
eigenvector	O
of	O
with	O
corresponding	O
eigenvalue	O
and	O
u	O
is	O
an	O
eigenvector	O
of	O
with	O
corresponding	O
eigenvalue	O
proof	O
suppose	O
that	O
is	O
a	O
singular	O
value	O
of	O
a	O
with	O
v	O
rn	O
being	O
the	O
corresponding	O
right	O
singular	O
vector	O
then	O
similarly	O
av	O
for	O
the	O
other	O
direction	O
if	O
is	O
an	O
eigenvalue	O
of	O
with	O
v	O
being	O
the	O
corresponding	O
eigenvector	O
then	O
because	O
is	O
positive	O
semidefinite	O
let	O
u	O
then	O
u	O
and	O
av	O
av	O
v	O
v	O
singular	O
value	O
decomposition	O
finally	O
we	O
show	O
that	O
if	O
a	O
has	O
rank	O
r	O
then	O
it	O
has	O
r	O
orthonormal	O
singular	O
vectors	O
lemma	O
let	O
a	O
rmn	O
with	O
rank	O
r	O
define	O
the	O
following	O
vectors	O
argmax	O
v	O
argmax	O
v	O
vr	O
argmax	O
v	O
ir	O
then	O
vr	O
is	O
an	O
orthonormal	O
set	B
of	O
right	O
singular	O
vectors	O
of	O
a	O
proof	O
first	O
note	O
that	O
since	O
the	O
rank	O
of	O
a	O
is	O
r	O
the	O
range	O
of	O
a	O
is	O
a	O
subspace	O
of	O
dimension	O
r	O
and	O
therefore	O
it	O
is	O
easy	O
to	O
verify	O
that	O
for	O
all	O
i	O
r	O
let	O
w	O
rnn	O
be	O
an	O
orthonormal	O
matrix	O
obtained	O
by	O
the	O
eigenvalue	O
decomposition	O
of	O
namely	O
w	O
dw	O
with	O
d	O
being	O
a	O
diagonal	O
matrix	O
with	O
we	O
will	O
show	O
that	O
vr	O
are	O
eigenvectors	O
of	O
that	O
correspond	O
to	O
nonzero	O
eigenvalues	O
and	O
hence	O
using	O
lemma	O
it	O
follows	O
that	O
these	O
are	O
also	O
right	O
singular	O
vectors	O
of	O
a	O
the	O
proof	O
is	O
by	O
induction	O
for	O
the	O
basis	O
of	O
the	O
induction	O
note	O
that	O
any	O
unit	O
vector	O
v	O
can	O
be	O
written	O
as	O
v	O
w	O
x	O
for	O
x	O
w	O
and	O
note	O
that	O
therefore	O
dw	O
iixi	O
therefore	O
max	O
max	O
iixi	O
the	O
solution	O
of	O
the	O
right-hand	O
side	O
is	O
to	O
set	B
x	O
which	O
implies	O
that	O
is	O
the	O
first	O
eigenvector	O
of	O
since	O
it	O
follows	O
that	O
as	O
required	O
for	O
the	O
induction	O
step	O
assume	O
that	O
the	O
claim	O
holds	O
for	O
some	O
t	O
r	O
then	O
any	O
v	O
which	O
is	O
orthogonal	O
to	O
vt	O
can	O
be	O
written	O
as	O
v	O
w	O
x	O
with	O
all	O
the	O
first	O
t	O
elements	O
of	O
x	O
being	O
zero	O
it	O
follows	O
that	O
max	O
i	O
max	O
iixi	O
the	O
solution	O
of	O
the	O
right-hand	O
side	O
is	O
the	O
all	O
zeros	O
vector	O
except	O
this	O
implies	O
that	O
is	O
the	O
column	O
of	O
w	O
finally	O
since	O
it	O
follows	O
that	O
as	O
required	O
this	O
concludes	O
our	O
proof	O
linear	O
algebra	O
corollary	O
svd	B
theorem	O
let	O
a	O
rmn	O
with	O
rank	O
r	O
then	O
a	O
u	O
dv	O
where	O
d	O
is	O
an	O
r	O
r	O
matrix	O
with	O
nonzero	O
singular	O
values	O
of	O
a	O
and	O
the	O
columns	O
of	O
u	O
v	O
are	O
orthonormal	O
left	O
and	O
right	O
singular	O
vectors	O
of	O
a	O
furtherii	O
is	O
an	O
eigenvalue	O
of	O
the	O
ith	O
column	O
of	O
v	O
is	O
the	O
cormore	O
for	O
all	O
i	O
responding	O
eigenvector	O
of	O
and	O
the	O
ith	O
column	O
of	O
u	O
is	O
the	O
corresponding	O
eigenvector	O
of	O
notes	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
references	O
abernethy	O
j	O
bartlett	O
p	O
l	O
rakhlin	O
a	O
tewari	O
a	O
optimal	O
strategies	O
and	O
minimax	O
lower	O
bounds	O
for	O
online	B
convex	B
games	O
in	O
proceedings	O
of	O
the	O
nineteenth	O
annual	O
conference	O
on	O
computational	O
learning	O
theory	O
ackerman	O
m	O
ben-david	O
s	O
measures	O
of	O
clustering	B
quality	O
a	O
working	O
set	B
of	O
axioms	O
for	O
clustering	B
in	O
proceedings	O
of	O
neural	O
information	O
processing	O
systems	O
pp	O
agarwal	O
s	O
roth	O
d	O
learnability	O
of	O
bipartite	B
ranking	B
functions	O
in	O
pro	O
ceedings	O
of	O
the	O
annual	O
conference	O
on	O
learning	O
theory	O
pp	O
agmon	O
s	O
the	O
relaxation	O
method	O
for	O
linear	O
inequalities	O
canadian	O
journal	O
of	O
mathematics	O
aizerman	O
m	O
a	O
braverman	O
e	O
m	O
rozonoer	O
l	O
i	O
theoretical	O
foundations	O
of	O
the	O
potential	O
function	B
method	O
in	O
pattern	O
recognition	O
learning	O
automation	O
and	O
remote	O
control	O
allwein	O
e	O
l	O
schapire	O
r	O
singer	O
y	O
reducing	O
multiclass	B
to	O
binary	O
a	O
unifying	O
approach	O
for	O
margin	B
classifiers	O
journal	O
of	O
machine	O
learning	O
research	O
alon	O
n	O
ben-david	O
s	O
cesa-bianchi	O
n	O
haussler	O
d	O
scale-sensitive	O
dimen	O
sions	O
uniform	B
convergence	I
and	O
learnability	O
journal	O
of	O
the	O
acm	O
anthony	O
m	O
bartlet	O
p	O
neural	O
network	O
learning	O
theoretical	O
foundations	O
cambridge	O
university	O
press	O
baraniuk	O
r	O
davenport	O
m	O
devore	O
r	O
wakin	O
m	O
a	O
simple	O
proof	O
of	O
the	O
restricted	O
isometry	O
property	O
for	O
random	O
matrices	O
constructive	O
approximation	O
barber	O
d	O
bayesian	B
reasoning	I
and	O
machine	O
learning	O
cambridge	O
university	O
press	O
bartlett	O
p	O
bousquet	O
o	O
mendelson	O
s	O
local	O
rademacher	O
complexities	O
annals	O
of	O
statistics	O
bartlett	O
p	O
l	O
ben-david	O
s	O
hardness	O
results	O
for	O
neural	O
network	O
approxi	O
mation	O
problems	O
theor	O
comput	O
sci	O
bartlett	O
p	O
l	O
long	O
p	O
m	O
williamson	O
r	O
c	O
fat-shattering	O
and	O
the	O
learnability	O
of	O
real-valued	O
functions	O
in	O
proceedings	O
of	O
the	O
seventh	O
annual	O
conference	O
on	O
computational	O
learning	O
theory	O
acm	O
pp	O
bartlett	O
p	O
l	O
mendelson	O
s	O
rademacher	O
and	O
gaussian	O
complexities	O
risk	B
bounds	O
and	O
structural	O
results	O
in	O
annual	O
conference	O
on	O
computational	O
learning	O
theory	O
colt	O
vol	O
springer	O
berlin	O
pp	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
references	O
bartlett	O
p	O
l	O
mendelson	O
s	O
rademacher	O
and	O
gaussian	O
complexities	O
risk	B
bounds	O
and	O
structural	O
results	O
journal	O
of	O
machine	O
learning	O
research	O
ben-david	O
s	O
cesa-bianchi	O
n	O
haussler	O
d	O
long	O
p	O
characterizations	O
of	O
learnability	O
for	O
classes	O
of	O
n-valued	O
functions	O
journal	O
of	O
computer	O
and	O
system	O
sciences	O
ben-david	O
s	O
eiron	O
n	O
long	O
p	O
on	O
the	O
difficulty	O
of	O
approximately	O
maxi	O
mizing	O
agreements	O
journal	O
of	O
computer	O
and	O
system	O
sciences	O
ben-david	O
s	O
litman	O
a	O
combinatorial	O
variability	O
of	O
vapnik-chervonenkis	O
classes	O
with	O
applications	O
to	O
sample	O
compression	O
schemes	O
discrete	O
applied	O
mathematics	O
ben-david	O
s	O
pal	O
d	O
shalev-shwartz	O
s	O
agnostic	O
online	B
learning	I
in	O
con	O
ference	O
on	O
learning	O
theory	O
ben-david	O
s	O
simon	O
h	O
efficient	O
learning	O
of	O
linear	O
perceptrons	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pp	O
bengio	O
y	O
learning	O
deep	O
architectures	O
for	O
ai	O
foundations	O
and	O
trends	O
in	O
machine	O
learning	O
bengio	O
y	O
lecun	O
y	O
scaling	O
learning	O
algorithms	O
towards	O
ai	O
large-scale	O
kernel	O
machines	O
bertsekas	O
d	O
nonlinear	O
programming	O
athena	O
scientific	O
beygelzimer	O
a	O
langford	O
j	O
ravikumar	O
p	O
multiclass	B
classification	O
with	O
filter	O
trees	O
preprint	O
june	O
birkhoff	O
g	O
three	O
observations	O
on	O
linear	O
algebra	O
revi	O
univ	O
nac	O
tucuman	O
ser	O
a	O
bishop	O
c	O
m	O
pattern	O
recognition	O
and	O
machine	O
learning	O
vol	O
springer	O
new	O
york	O
blum	O
l	O
shub	O
m	O
smale	O
s	O
on	O
a	O
theory	O
of	O
computation	O
and	O
complexity	O
over	O
the	O
real	O
numbers	O
np-completeness	O
recursive	O
functions	O
and	O
universal	O
machines	O
am	O
math	O
soc	O
blumer	O
a	O
ehrenfeucht	O
a	O
haussler	O
d	O
warmuth	O
m	O
k	O
occam	O
s	O
razor	O
information	O
processing	O
letters	O
blumer	O
a	O
ehrenfeucht	O
a	O
haussler	O
d	O
warmuth	O
m	O
k	O
learnability	O
and	O
the	O
vapnik-chervonenkis	O
dimension	O
journal	O
of	O
the	O
association	O
for	O
computing	O
machinery	O
borwein	O
j	O
lewis	O
a	O
convex	B
analysis	O
and	O
nonlinear	O
optimization	O
springer	O
boser	O
b	O
e	O
guyon	O
i	O
m	O
vapnik	O
v	O
n	O
a	O
training	O
algorithm	O
for	O
optimal	O
margin	B
classifiers	O
in	O
conference	O
on	O
learning	O
theory	O
pp	O
bottou	O
l	O
bousquet	O
o	O
the	O
tradeoffs	O
of	O
large	O
scale	O
learning	O
in	O
nips	O
pp	O
boucheron	O
s	O
bousquet	O
o	O
lugosi	O
g	O
theory	O
of	O
classification	O
a	O
survey	O
of	O
recent	O
advances	O
esaim	O
probability	O
and	O
statistics	O
bousquet	O
o	O
concentration	O
inequalities	O
and	O
empirical	O
processes	O
theory	O
ap	O
plied	O
to	O
the	O
analysis	O
of	O
learning	O
algorithms	O
phd	O
thesis	O
ecole	O
polytechnique	O
bousquet	O
o	O
elisseeff	O
a	O
stability	B
and	O
generalization	O
journal	O
of	O
machine	O
learning	O
research	O
boyd	O
s	O
vandenberghe	O
l	O
convex	B
optimization	O
cambridge	O
university	O
press	O
references	O
breiman	O
l	O
bias	B
variance	O
and	O
arcing	O
classifiers	O
technical	O
report	O
statis	O
tics	O
department	O
university	O
of	O
california	O
at	O
berkeley	O
breiman	O
l	O
random	B
forests	I
machine	O
learning	O
breiman	O
l	O
friedman	O
j	O
h	O
olshen	O
r	O
a	O
stone	O
c	O
j	O
classification	O
and	O
regression	B
trees	O
wadsworth	O
brooks	O
candes	O
e	O
the	O
restricted	O
isometry	O
property	O
and	O
its	O
implications	O
for	O
com	O
pressed	O
sensing	O
comptes	O
rendus	O
mathematique	O
candes	O
e	O
j	O
compressive	O
sampling	O
in	O
proc	O
of	O
the	O
int	O
congress	O
of	O
math	O
madrid	O
spain	O
candes	O
e	O
tao	O
t	O
decoding	O
by	O
linear	B
programming	I
ieee	O
trans	O
on	O
information	O
theory	O
cesa-bianchi	O
n	O
lugosi	O
g	O
prediction	O
learning	O
and	O
games	O
cambridge	O
university	O
press	O
chang	O
h	O
s	O
weiss	O
y	O
freeman	O
w	O
t	O
informative	O
sensing	O
arxiv	O
preprint	O
chapelle	O
o	O
le	O
q	O
smola	O
a	O
large	O
margin	B
optimization	O
of	O
ranking	B
mea	O
sures	O
in	O
nips	O
workshop	O
machine	O
learning	O
for	O
web	O
search	O
collins	O
m	O
discriminative	B
reranking	O
for	O
natural	O
language	O
parsing	O
in	O
machine	O
learning	O
collins	O
m	O
discriminative	B
training	O
methods	O
for	O
hidden	O
markov	O
models	O
theory	O
and	O
experiments	O
with	O
perceptron	B
algorithms	O
in	O
conference	O
on	O
empirical	O
methods	O
in	O
natural	O
language	O
processing	O
collobert	O
r	O
weston	O
j	O
a	O
unified	O
architecture	O
for	O
natural	O
language	O
processing	O
deep	O
neural	B
networks	I
with	O
multitask	O
learning	O
in	O
international	O
conference	O
on	O
machine	O
learning	O
cortes	O
c	O
vapnik	O
v	O
support-vector	O
networks	O
machine	O
learning	O
cover	O
t	O
behavior	O
of	O
sequential	O
predictors	O
of	O
binary	O
sequences	O
trans	O
prague	O
conf	O
information	O
theory	O
statistical	O
decision	O
functions	O
random	O
processes	O
pp	O
cover	O
t	O
hart	O
p	O
nearest	B
neighbor	I
pattern	O
classification	O
information	O
theory	O
ieee	O
transactions	O
on	O
crammer	O
k	O
singer	O
y	O
on	O
the	O
algorithmic	O
implementation	O
of	O
multiclass	B
kernel-based	O
vector	O
machines	O
journal	O
of	O
machine	O
learning	O
research	O
cristianini	O
n	O
shawe-taylor	O
j	O
an	O
introduction	O
to	O
support	O
vector	O
machines	O
cambridge	O
university	O
press	O
daniely	O
a	O
sabato	O
s	O
ben-david	O
s	O
shalev-shwartz	O
s	O
multiclass	B
learn	O
ability	O
and	O
the	O
erm	B
principle	O
in	O
conference	O
on	O
learning	O
theory	O
daniely	O
a	O
sabato	O
s	O
shwartz	O
s	O
s	O
multiclass	B
learning	O
approaches	O
a	O
theoretical	O
comparison	O
with	O
implications	O
in	O
nips	O
davis	O
g	O
mallat	O
s	O
avellaneda	O
m	O
greedy	O
adaptive	O
approximation	O
jour	O
nal	O
of	O
constructive	O
approximation	O
devroye	O
l	O
gy	O
orfi	O
l	O
nonparametric	O
density	O
estimation	O
the	O
l	O
s	O
view	O
wiley	O
devroye	O
l	O
gy	O
orfi	O
l	O
lugosi	O
g	O
a	O
probabilistic	O
theory	O
of	O
pattern	O
recog	O
nition	O
springer	O
references	O
dietterich	O
t	O
g	O
bakiri	O
g	O
solving	O
multiclass	B
learning	O
problems	O
via	O
error	O
correcting	O
output	O
codes	O
journal	O
of	O
artificial	O
intelligence	O
research	O
donoho	O
d	O
l	O
compressed	B
sensing	I
information	O
theory	O
ieee	O
transactions	O
on	O
dudley	O
r	O
gine	O
e	O
zinn	O
j	O
uniform	O
and	O
universal	O
glivenko-cantelli	B
classes	O
journal	O
of	O
theoretical	O
probability	O
dudley	O
r	O
m	O
universal	O
donsker	O
classes	O
and	O
metric	O
entropy	B
annals	O
of	O
prob	O
ability	O
fisher	O
r	O
a	O
on	O
the	O
mathematical	O
foundations	O
of	O
theoretical	O
statistics	O
philosophical	O
transactions	O
of	O
the	O
royal	O
society	O
of	O
london	O
series	O
a	O
containing	O
papers	O
of	O
a	O
mathematical	O
or	O
physical	O
character	O
floyd	O
s	O
space-bounded	O
learning	O
and	O
the	O
vapnik-chervonenkis	O
dimension	O
in	O
conference	O
on	O
learning	O
theory	O
pp	O
floyd	O
s	O
warmuth	O
m	O
sample	O
compression	O
learnability	O
and	O
the	O
vapnik	O
chervonenkis	O
dimension	O
machine	O
learning	O
frank	O
m	O
wolfe	O
p	O
an	O
algorithm	O
for	O
quadratic	O
programming	O
naval	O
res	O
logist	O
quart	O
freund	O
y	O
schapire	O
r	O
a	O
decision-theoretic	O
generalization	O
of	O
on-line	O
learning	O
and	O
an	O
application	O
to	O
boosting	B
in	O
european	O
conference	O
on	O
computational	O
learning	O
theory	O
springer-verlag	O
pp	O
freund	O
y	O
schapire	O
r	O
e	O
large	O
margin	B
classification	O
using	O
the	O
perceptron	B
algorithm	O
machine	O
learning	O
garcia	O
j	O
koelling	O
r	O
relation	O
of	O
cue	O
to	O
consequence	O
in	O
avoidance	O
learning	O
foundations	O
of	O
animal	O
behavior	O
classic	O
papers	O
with	O
commentaries	O
gentile	O
c	O
the	O
robustness	O
of	O
the	O
p-norm	O
algorithms	O
machine	O
learning	O
georghiades	O
a	O
belhumeur	O
p	O
kriegman	O
d	O
from	O
few	O
to	O
many	O
illumination	O
cone	O
models	O
for	O
face	O
recognition	O
under	O
variable	O
lighting	O
and	O
pose	O
ieee	O
trans	O
pattern	O
anal	O
mach	O
intelligence	O
gordon	O
g	O
regret	O
bounds	O
for	O
prediction	O
problems	O
in	O
conference	O
on	O
learning	O
theory	O
gottlieb	O
l	O
-a	O
kontorovich	O
l	O
krauthgamer	O
r	O
efficient	O
classification	O
for	O
metric	O
data	O
in	O
conference	O
on	O
learning	O
theory	O
pp	O
guyon	O
i	O
elisseeff	O
a	O
an	O
introduction	O
to	O
variable	O
and	O
feature	B
selection	I
journal	O
of	O
machine	O
learning	O
research	O
special	O
issue	O
on	O
variable	O
and	O
feature	B
selection	I
hadamard	O
j	O
sur	O
les	O
problemes	O
aux	O
d	O
eriv	O
ees	O
partielles	O
et	O
leur	O
signification	O
physique	O
princeton	O
university	O
bulletin	O
hastie	O
t	O
tibshirani	O
r	O
friedman	O
j	O
the	O
elements	O
of	O
statistical	O
learning	O
springer	O
haussler	O
d	O
decision	O
theoretic	O
generalizations	O
of	O
the	O
pac	B
model	O
for	O
neural	O
net	O
and	O
other	O
learning	O
applications	O
information	O
and	O
computation	O
haussler	O
d	O
long	O
p	O
m	O
a	O
generalization	O
of	O
sauer	O
s	O
lemma	O
journal	O
of	O
combinatorial	O
theory	O
series	O
a	O
hazan	O
e	O
agarwal	O
a	O
kale	O
s	O
logarithmic	O
regret	O
algorithms	O
for	O
online	B
convex	B
optimization	I
machine	O
learning	O
references	O
hinton	O
g	O
e	O
osindero	O
s	O
teh	O
y	O
-w	O
a	O
fast	O
learning	O
algorithm	O
for	O
deep	O
belief	O
nets	O
neural	O
computation	O
hiriart-urruty	O
j	O
-b	O
lemar	O
echal	O
c	O
convex	B
analysis	O
and	O
minimization	O
al	O
gorithms	O
part	O
fundamentals	O
vol	O
springer	O
hsu	O
c	O
-w	O
chang	O
c	O
-c	O
lin	O
c	O
-j	O
a	O
practical	O
guide	O
to	O
support	O
vector	O
classification	O
hyafil	O
l	O
rivest	O
r	O
l	O
constructing	O
optimal	O
binary	O
decision	B
trees	I
is	O
np	O
complete	O
information	O
processing	O
letters	O
joachims	O
t	O
a	O
support	O
vector	O
method	O
for	O
multivariate	B
performance	I
measures	I
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
kakade	O
s	O
sridharan	O
k	O
tewari	O
a	O
on	O
the	O
complexity	O
of	O
linear	O
prediction	O
risk	B
bounds	O
margin	B
bounds	O
and	O
regularization	B
in	O
nips	O
karp	O
r	O
m	O
reducibility	O
among	O
combinatorial	O
problems	O
springer	O
kearns	O
m	O
j	O
schapire	O
r	O
e	O
sellie	O
l	O
m	O
toward	O
efficient	O
agnostic	O
learn	O
ing	O
machine	O
learning	O
kearns	O
m	O
mansour	O
y	O
on	O
the	O
boosting	B
ability	O
of	O
top-down	O
decision	O
tree	O
learning	O
algorithms	O
in	O
acm	O
symposium	O
on	O
the	O
theory	O
of	O
computing	O
kearns	O
m	O
ron	O
d	O
algorithmic	O
stability	B
and	O
sanity-check	O
bounds	O
for	O
leave	O
one-out	O
cross-validation	O
neural	O
computation	O
kearns	O
m	O
valiant	O
l	O
g	O
learning	O
boolean	O
formulae	O
or	O
finite	O
automata	O
is	O
as	O
hard	O
as	O
factoring	O
technical	O
report	O
harvard	O
university	O
aiken	O
computation	O
laboratory	O
kearns	O
m	O
vazirani	O
u	O
an	O
introduction	O
to	O
computational	O
learning	O
theory	O
mit	O
press	O
kleinberg	O
j	O
an	O
impossibility	O
theorem	O
for	O
clustering	B
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pp	O
klivans	O
a	O
r	O
sherstov	O
a	O
a	O
cryptographic	O
hardness	O
for	O
learning	O
intersec	O
tions	O
of	O
halfspaces	O
in	O
focs	O
koller	O
d	O
friedman	O
n	O
probabilistic	O
graphical	O
models	O
principles	O
and	O
tech	O
niques	O
mit	O
press	O
koltchinskii	O
v	O
panchenko	O
d	O
rademacher	O
processes	O
and	O
bounding	O
the	O
risk	B
of	O
function	B
learning	O
in	O
high	O
dimensional	O
probability	O
ii	O
springer	O
pp	O
kuhn	O
h	O
w	O
the	O
hungarian	O
method	O
for	O
the	O
assignment	O
problem	O
naval	O
re	O
search	O
logistics	O
quarterly	O
kutin	O
s	O
niyogi	O
p	O
almost-everywhere	O
algorithmic	O
stability	B
and	O
generalization	B
error	I
in	O
proceedings	O
of	O
the	O
conference	O
in	O
uncertainty	O
in	O
artificial	O
intelligence	O
pp	O
lafferty	O
j	O
mccallum	O
a	O
pereira	O
f	O
conditional	O
random	O
fields	O
probabilistic	O
models	O
for	O
segmenting	O
and	O
labeling	O
sequence	O
data	O
in	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
langford	O
j	O
tutorial	O
on	O
practical	O
prediction	O
theory	O
for	O
classification	O
journal	O
of	O
machine	O
learning	O
research	O
langford	O
j	O
shawe-taylor	O
j	O
pac-bayes	B
margins	O
in	O
nips	O
pp	O
le	O
cun	O
l	O
large	O
scale	O
online	B
learning	I
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
proceedings	O
of	O
the	O
conference	O
vol	O
mit	O
press	O
p	O
references	O
le	O
q	O
v	O
ranzato	O
m	O
-a	O
monga	O
r	O
devin	O
m	O
corrado	O
g	O
chen	O
k	O
dean	O
j	O
ng	O
a	O
y	O
building	O
high-level	O
features	O
using	O
large	O
scale	O
unsupervised	B
learning	I
in	O
international	O
conference	O
on	O
machine	O
learning	O
lecun	O
y	O
bengio	O
y	O
convolutional	O
networks	O
for	O
images	O
speech	O
and	O
time	O
series	O
the	O
mit	O
press	O
pp	O
lee	O
h	O
grosse	O
r	O
ranganath	O
r	O
ng	O
a	O
convolutional	O
deep	O
belief	O
networks	O
for	O
scalable	O
unsupervised	B
learning	I
of	O
hierarchical	O
representations	O
in	O
international	O
conference	O
on	O
machine	O
learning	O
littlestone	O
n	O
learning	O
quickly	O
when	O
irrelevant	O
attributes	O
abound	O
a	O
new	O
linear-threshold	O
algorithm	O
machine	O
learning	O
littlestone	O
n	O
warmuth	O
m	O
relating	O
data	O
compression	O
and	O
learnability	O
unpublished	O
manuscript	O
littlestone	O
n	O
warmuth	O
m	O
k	O
the	O
weighted	O
majority	O
algorithm	O
infor	O
mation	O
and	O
computation	O
livni	O
r	O
shalev-shwartz	O
s	O
shamir	O
o	O
a	O
provably	O
efficient	O
algorithm	O
for	O
training	O
deep	O
networks	O
arxiv	O
preprint	O
livni	O
r	O
simon	O
p	O
honest	O
compressions	O
and	O
their	O
application	O
to	O
compression	O
schemes	O
in	O
conference	O
on	O
learning	O
theory	O
mackay	O
d	O
j	O
information	O
theory	O
inference	O
and	O
learning	O
algorithms	O
cambridge	O
university	O
press	O
mallat	O
s	O
zhang	O
z	O
matching	O
pursuits	O
with	O
time-frequency	O
dictionaries	O
ieee	O
transactions	O
on	O
signal	O
processing	O
mcallester	O
d	O
a	O
some	O
pac-bayesian	O
theorems	O
in	O
conference	O
on	O
learning	O
theory	O
mcallester	O
d	O
a	O
pac-bayesian	O
model	O
averaging	O
in	O
conference	O
on	O
learning	O
theory	O
pp	O
mcallester	O
d	O
a	O
simplified	O
pac-bayesian	O
margin	B
bounds	O
in	O
conference	O
on	O
learning	O
theory	O
pp	O
minsky	O
m	O
papert	O
s	O
perceptrons	O
an	O
introduction	O
to	O
computational	O
ge	O
ometry	O
the	O
mit	O
press	O
mukherjee	O
s	O
niyogi	O
p	O
poggio	O
t	O
rifkin	O
r	O
learning	O
theory	O
stability	B
is	O
sufficient	O
for	O
generalization	O
and	O
necessary	O
and	O
sufficient	O
for	O
consistency	B
of	O
empirical	B
risk	B
minimization	O
advances	O
in	O
computational	O
mathematics	O
murata	O
n	O
a	O
statistical	O
study	O
of	O
on-line	O
learning	O
online	B
learning	I
and	O
neural	B
networks	I
cambridge	O
university	O
press	O
cambridge	O
uk	O
murphy	O
k	O
p	O
machine	O
learning	O
a	O
probabilistic	O
perspective	O
the	O
mit	O
press	O
natarajan	O
b	O
sparse	O
approximate	O
solutions	O
to	O
linear	O
systems	O
siam	O
j	O
com	O
puting	O
natarajan	O
b	O
k	O
on	O
learning	O
sets	O
and	O
functions	O
mach	O
learn	O
nemirovski	O
a	O
juditsky	O
a	O
lan	O
g	O
shapiro	O
a	O
robust	O
stochastic	O
approximation	O
approach	O
to	O
stochastic	O
programming	O
siam	O
journal	O
on	O
optimization	O
nemirovski	O
a	O
yudin	O
d	O
problem	O
complexity	O
and	O
method	O
efficiency	O
in	O
opti	O
mization	O
nauka	O
publishers	O
moscow	O
nesterov	O
y	O
primal-dual	O
subgradient	O
methods	O
for	O
convex	B
problems	O
technical	O
report	O
center	O
for	O
operations	O
research	O
and	O
econometrics	O
catholic	O
university	O
of	O
louvain	O
references	O
nesterov	O
y	O
nesterov	O
i	O
introductory	O
lectures	O
on	O
convex	B
optimization	O
a	O
basic	O
course	O
vol	O
springer	O
netherlands	O
novikoff	O
a	O
b	O
j	O
on	O
convergence	O
proofs	O
on	O
perceptrons	O
in	O
proceedings	O
of	O
the	O
symposium	O
on	O
the	O
mathematical	O
theory	O
of	O
automata	O
vol	O
xii	O
pp	O
parberry	O
i	O
circuit	O
complexity	O
and	O
neural	B
networks	I
the	O
mit	O
press	O
pearson	O
k	O
on	O
lines	O
and	O
planes	O
of	O
closest	O
fit	O
to	O
systems	O
of	O
points	O
in	O
space	O
the	O
london	O
edinburgh	O
and	O
dublin	O
philosophical	O
magazine	O
and	O
journal	O
of	O
science	O
phillips	O
d	O
l	O
a	O
technique	O
for	O
the	O
numerical	O
solution	O
of	O
certain	O
integral	O
equa	O
tions	O
of	O
the	O
first	O
kind	O
journal	O
of	O
the	O
acm	O
pisier	O
g	O
remarques	O
sur	O
un	O
r	O
esultat	O
non	O
publi	O
e	O
de	O
b	O
maurey	O
pitt	O
l	O
valiant	O
l	O
computational	O
limitations	O
on	O
learning	O
from	O
examples	O
journal	O
of	O
the	O
association	O
for	O
computing	O
machinery	O
poon	O
h	O
domingos	O
p	O
sum-product	O
networks	O
a	O
new	O
deep	O
architecture	O
in	O
conference	O
on	O
uncertainty	O
in	O
artificial	O
intelligence	O
quinlan	O
j	O
r	O
induction	O
of	O
decision	B
trees	I
machine	O
learning	O
quinlan	O
j	O
r	O
programs	O
for	O
machine	O
learning	O
morgan	O
kaufmann	O
rabiner	O
l	O
juang	O
b	O
an	O
introduction	O
to	O
hidden	O
markov	O
models	O
ieee	O
assp	O
magazine	O
rakhlin	O
a	O
shamir	O
o	O
sridharan	O
k	O
making	O
gradient	B
descent	I
optimal	O
for	O
strongly	B
convex	B
stochastic	O
optimization	O
in	O
international	O
conference	O
on	O
machine	O
learning	O
rakhlin	O
a	O
sridharan	O
k	O
tewari	O
a	O
online	B
learning	I
random	O
averages	O
combinatorial	O
parameters	O
and	O
learnability	O
in	O
nips	O
rakhlin	O
s	O
mukherjee	O
s	O
poggio	O
t	O
stability	B
results	O
in	O
learning	O
theory	O
analysis	O
and	O
applications	O
ranzato	O
m	O
huang	O
f	O
boureau	O
y	O
lecun	O
y	O
unsupervised	B
learning	I
of	O
invariant	O
feature	B
hierarchies	O
with	O
applications	O
to	O
object	O
recognition	O
in	O
computer	O
vision	O
and	O
pattern	O
recognition	O
cvpr	O
ieee	O
conference	O
on	O
ieee	O
pp	O
rissanen	O
j	O
modeling	O
by	O
shortest	O
data	O
description	O
automatica	O
rissanen	O
j	O
a	O
universal	O
prior	O
for	O
integers	O
and	O
estimation	O
by	O
minimum	O
descrip	O
tion	O
length	O
the	O
annals	O
of	O
statistics	O
robbins	O
h	O
monro	O
s	O
a	O
stochastic	O
approximation	O
method	O
the	O
annals	O
of	O
mathematical	O
statistics	O
pp	O
rogers	O
w	O
wagner	O
t	O
a	O
finite	O
sample	O
distribution-free	O
performance	O
bound	O
for	O
local	O
discrimination	O
rules	O
the	O
annals	O
of	O
statistics	O
rokach	O
l	O
data	O
mining	O
with	O
decision	B
trees	I
theory	O
and	O
applications	O
vol	O
world	O
scientific	O
rosenblatt	O
f	O
the	O
perceptron	B
a	O
probabilistic	O
model	O
for	O
information	O
storage	O
in	O
and	O
organization	O
in	O
the	O
brain	O
psychological	O
review	O
neurocomputing	O
press	O
rumelhart	O
d	O
e	O
hinton	O
g	O
e	O
williams	O
r	O
j	O
learning	O
internal	O
representations	O
by	O
error	O
propagation	O
in	O
d	O
e	O
rumelhart	O
j	O
l	O
mcclelland	O
eds	O
parallel	O
distributed	O
processing	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
mit	O
press	O
chapter	O
pp	O
references	O
sankaran	O
j	O
k	O
a	O
note	O
on	O
resolving	O
infeasibility	O
in	O
linear	O
programs	O
by	O
con	O
straint	O
relaxation	O
operations	O
research	O
letters	O
sauer	O
n	O
on	O
the	O
density	O
of	O
families	O
of	O
sets	O
journal	O
of	O
combinatorial	O
theory	O
series	O
a	O
schapire	O
r	O
the	O
strength	O
of	O
weak	O
learnability	O
machine	O
learning	O
schapire	O
r	O
e	O
freund	O
y	O
boosting	B
foundations	O
and	O
algorithms	O
mit	O
press	O
sch	O
olkopf	O
b	O
herbrich	O
r	O
smola	O
a	O
a	O
generalized	O
representer	B
theorem	I
in	O
computational	O
learning	O
theory	O
pp	O
sch	O
olkopf	O
b	O
herbrich	O
r	O
smola	O
a	O
williamson	O
r	O
a	O
generalized	O
repre	O
senter	O
theorem	O
in	O
neurocolt	O
sch	O
olkopf	O
b	O
smola	O
a	O
j	O
learning	O
with	O
kernels	B
support	O
vector	O
machines	O
regularization	B
optimization	O
and	O
beyond	O
mit	O
press	O
sch	O
olkopf	O
b	O
smola	O
a	O
m	O
uller	O
k	O
-r	O
nonlinear	O
component	O
analysis	O
as	O
a	O
kernel	O
eigenvalue	O
problem	O
neural	O
computation	O
seeger	O
m	O
pac-bayesian	O
generalisation	O
error	O
bounds	O
for	O
gaussian	O
process	O
clas	O
sification	O
the	O
journal	O
of	O
machine	O
learning	O
research	O
shakhnarovich	O
g	O
darrell	O
t	O
indyk	O
p	O
nearest-neighbor	O
methods	O
in	O
learning	O
and	O
vision	O
theory	O
and	O
practice	O
mit	O
press	O
shalev-shwartz	O
s	O
online	B
learning	I
theory	O
algorithms	O
and	O
applications	O
phd	O
thesis	O
the	O
hebrew	O
university	O
shalev-shwartz	O
s	O
online	B
learning	I
and	O
online	B
convex	B
optimization	I
founda	O
tions	O
and	O
trends	O
in	O
machine	O
learning	O
shalev-shwartz	O
s	O
shamir	O
o	O
srebro	O
n	O
sridharan	O
k	O
learnability	O
stability	B
and	O
uniform	B
convergence	I
the	O
journal	O
of	O
machine	O
learning	O
research	O
shalev-shwartz	O
s	O
shamir	O
o	O
sridharan	O
k	O
learning	O
kernel-based	O
halfs	O
paces	O
with	O
the	O
zero-one	O
loss	B
in	O
conference	O
on	O
learning	O
theory	O
shalev-shwartz	O
s	O
shamir	O
o	O
sridharan	O
k	O
srebro	O
n	O
stochastic	O
convex	B
optimization	O
in	O
conference	O
on	O
learning	O
theory	O
shalev-shwartz	O
s	O
singer	O
y	O
on	O
the	O
equivalence	O
of	O
weak	O
learnability	O
and	O
linear	O
separability	O
new	O
relaxations	O
and	O
efficient	O
boosting	B
algorithms	O
in	O
proceedings	O
of	O
the	O
nineteenth	O
annual	O
conference	O
on	O
computational	O
learning	O
theory	O
shalev-shwartz	O
s	O
singer	O
y	O
srebro	O
n	O
pegasos	O
primal	O
estimated	O
subgradient	O
solver	O
for	O
svm	B
in	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
shalev-shwartz	O
s	O
srebro	O
n	O
svm	B
optimization	O
inverse	O
dependence	O
on	O
training	B
set	B
size	O
in	O
international	O
conference	O
on	O
machine	O
learning	O
pp	O
shalev-shwartz	O
s	O
zhang	O
t	O
srebro	O
n	O
trading	O
accuracy	B
for	O
sparsity	O
in	O
optimization	O
problems	O
with	O
sparsity	O
constraints	O
siam	O
journal	O
on	O
optimization	O
shamir	O
o	O
zhang	O
t	O
stochastic	O
gradient	B
descent	I
for	O
non-smooth	O
optimization	O
convergence	O
results	O
and	O
optimal	O
averaging	O
schemes	O
in	O
international	O
conference	O
on	O
machine	O
learning	O
shapiro	O
a	O
dentcheva	O
d	O
ruszczy	O
nski	O
a	O
lectures	O
on	O
stochastic	O
programming	O
modeling	O
and	O
theory	O
vol	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
references	O
shelah	O
s	O
a	O
combinatorial	O
problem	O
stability	B
and	O
order	O
for	O
models	O
and	O
theories	O
in	O
infinitary	O
languages	O
pac	B
j	O
math	O
sipser	O
m	O
introduction	O
to	O
the	O
theory	O
of	O
computation	O
thomson	O
course	O
tech	O
nology	O
slud	O
e	O
v	O
distribution	O
inequalities	O
for	O
the	O
binomial	O
law	O
the	O
annals	O
of	O
probability	O
steinwart	O
i	O
christmann	O
a	O
support	O
vector	O
machines	O
springerverlag	O
new	O
york	O
stone	O
c	O
consistent	B
nonparametric	O
regression	B
the	O
annals	O
of	O
statistics	O
taskar	O
b	O
guestrin	O
c	O
koller	O
d	O
max-margin	O
markov	O
networks	O
in	O
nips	O
tibshirani	O
r	O
regression	B
shrinkage	O
and	O
selection	O
via	O
the	O
lasso	B
j	O
royal	O
statist	O
soc	O
b	O
tikhonov	B
a	O
n	O
on	O
the	O
stability	B
of	O
inverse	O
problems	O
dolk	O
akad	O
nauk	O
sssr	O
tishby	O
n	O
pereira	O
f	O
bialek	O
w	O
the	O
information	B
bottleneck	I
method	O
in	O
the	O
th	O
allerton	O
conference	O
on	O
communication	O
control	O
and	O
computing	O
tsochantaridis	O
i	O
hofmann	O
t	O
joachims	O
t	O
altun	O
y	O
support	O
vector	O
machine	O
learning	O
for	O
interdependent	O
and	O
structured	O
output	O
spaces	O
in	O
proceedings	O
of	O
the	O
twenty-first	O
international	O
conference	O
on	O
machine	O
learning	O
valiant	O
l	O
g	O
a	O
theory	O
of	O
the	O
learnable	O
communications	O
of	O
the	O
acm	O
vapnik	O
v	O
principles	O
of	O
risk	B
minimization	O
for	O
learning	O
theory	O
in	O
j	O
e	O
moody	O
s	O
j	O
hanson	O
r	O
p	O
lippmann	O
eds	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
morgan	O
kaufmann	O
pp	O
vapnik	O
v	O
the	O
nature	O
of	O
statistical	O
learning	O
theory	O
springer	O
vapnik	O
v	O
n	O
estimation	O
of	O
dependences	O
based	O
on	O
empirical	O
data	O
springer	O
verlag	O
vapnik	O
v	O
n	O
statistical	O
learning	O
theory	O
wiley	O
vapnik	O
v	O
n	O
chervonenkis	O
a	O
y	O
on	O
the	O
uniform	B
convergence	I
of	O
relative	O
frequencies	O
of	O
events	O
to	O
their	O
probabilities	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
vapnik	O
v	O
n	O
chervonenkis	O
a	O
y	O
theory	O
of	O
pattern	O
recognition	O
nauka	O
moscow	O
russian	O
von	O
luxburg	O
u	O
a	O
tutorial	O
on	O
spectral	B
clustering	B
statistics	O
and	O
computing	O
von	O
neumann	O
j	O
zur	O
theorie	O
der	O
gesellschaftsspiele	O
the	O
theory	O
of	O
parlor	O
games	O
math	O
ann	O
von	O
neumann	O
j	O
a	O
certain	O
zero-sum	O
two-person	O
game	O
equivalent	O
to	O
the	O
opti	O
mal	O
assignment	O
problem	O
contributions	O
to	O
the	O
theory	O
of	O
games	O
vovk	O
v	O
g	O
aggregating	O
strategies	O
in	O
conference	O
on	O
learning	O
theory	O
pp	O
warmuth	O
m	O
glocer	O
k	O
vishwanathan	O
s	O
entropy	B
regularized	O
lpboost	O
in	O
algorithmic	O
learning	O
theory	O
warmuth	O
m	O
liao	O
j	O
ratsch	O
g	O
totally	O
corrective	O
boosting	B
algorithms	O
that	O
maximize	O
the	O
margin	B
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
references	O
weston	O
j	O
chapelle	O
o	O
vapnik	O
v	O
elisseeff	O
a	O
sch	O
olkopf	O
b	O
kernel	O
dependency	O
estimation	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pp	O
weston	O
j	O
watkins	O
c	O
support	O
vector	O
machines	O
for	O
multi-class	O
pattern	O
recognition	O
in	O
proceedings	O
of	O
the	O
seventh	O
european	O
symposium	O
on	O
artificial	O
neural	B
networks	I
wolpert	O
d	O
h	O
macready	O
w	O
g	O
no	O
free	O
lunch	O
theorems	O
for	O
optimization	O
evolutionary	O
computation	O
ieee	O
transactions	O
on	O
zhang	O
t	O
solving	O
large	O
scale	O
linear	O
prediction	O
problems	O
using	O
stochastic	O
gradient	B
descent	I
algorithms	O
in	O
proceedings	O
of	O
the	O
twenty-first	O
international	O
conference	O
on	O
machine	O
learning	O
zhao	O
p	O
yu	O
b	O
on	O
model	B
selection	I
consistency	B
of	O
lasso	B
journal	O
of	O
machine	O
learning	O
research	O
zinkevich	O
m	O
online	B
convex	B
programming	O
and	O
generalized	O
infinitesimal	O
gradi	O
ent	O
ascent	O
in	O
international	O
conference	O
on	O
machine	O
learning	O
index	O
dnf	O
norm	O
accuracy	B
activation	B
function	B
adaboost	B
all-pairs	B
approximation	B
error	I
auto-encoders	B
backpropagation	B
backward	B
elimination	I
bag-of-words	B
base	B
hypothesis	B
bayes	B
optimal	I
bayes	B
rule	I
bayesian	B
reasoning	I
bennet	O
s	O
inequality	O
bernstein	O
s	O
inequality	O
bias	B
bias-complexity	B
tradeoff	I
boolean	B
conjunctions	I
boosting	B
boosting	B
the	I
confidence	B
boundedness	B
cart	B
chaining	B
chebyshev	O
s	O
inequality	O
chernoff	B
bounds	I
class-sensitive	B
feature	B
mapping	I
classifier	B
clustering	B
spectral	B
compressed	B
sensing	I
compression	B
bounds	I
compression	B
scheme	I
computational	B
complexity	I
confidence	B
consistency	B
consistent	B
contraction	B
lemma	I
convex	B
function	B
set	B
strongly	B
convex	B
convex-lipschitz-bounded	B
learning	I
convex-smooth-bounded	B
learning	I
covering	B
numbers	I
curse	B
of	I
dimensionality	I
decision	B
stumps	I
decision	B
trees	I
dendrogram	B
dictionary	B
learning	I
differential	B
set	B
dimensionality	B
reduction	I
discretization	B
trick	I
discriminative	B
distribution	B
free	I
domain	B
domain	B
of	I
examples	I
doubly	B
stochastic	I
matrix	I
duality	B
strong	B
duality	B
weak	B
duality	B
dudley	B
classes	I
efficient	O
computable	O
em	B
empirical	B
error	I
empirical	B
risk	B
empirical	B
risk	B
minimization	O
see	O
erm	B
entropy	B
relative	B
entropy	B
epigraph	B
erm	B
error	B
decomposition	I
estimation	B
error	I
expectation-maximization	O
see	O
em	B
face	O
recognition	O
see	O
viola-jones	B
feasible	B
feature	B
feature	B
learning	I
feature	B
normalization	I
feature	B
selection	I
feature	B
space	I
feature	B
transformations	I
filters	O
understanding	O
machine	O
learning	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
by	O
cambridge	O
university	O
press	O
personal	O
use	O
only	O
not	O
for	O
distribution	O
do	O
not	O
post	O
please	O
link	O
to	O
httpwww	O
cs	O
huji	O
ac	O
ilshaisunderstandingmachinelearning	O
index	O
forward	B
greedy	I
selection	I
frequentist	B
gain	B
gd	O
see	O
gradient	B
descent	I
generalization	B
error	I
generative	B
models	I
gini	B
index	I
glivenko-cantelli	B
gradient	B
gradient	B
descent	I
gram	B
matrix	I
growth	B
function	B
halfspace	B
homogenous	B
non-separable	B
separable	B
halving	B
hidden	B
layers	I
hilbert	B
space	I
hoeffding	O
s	O
inequality	O
hold	B
out	I
hypothesis	B
hypothesis	B
class	I
i	O
i	O
d	O
improper	O
see	O
representation	B
independent	I
inductive	O
bias	B
see	O
bias	B
information	B
bottleneck	I
information	B
gain	B
instance	B
instance	B
space	I
integral	B
image	I
johnson-lindenstrauss	B
lemma	I
k-means	B
soft	B
k-means	B
k-median	B
k-medoids	B
kendall	B
tau	I
kernel	B
pca	B
kernels	B
gaussian	B
kernel	I
kernel	B
trick	I
polynomial	B
kernel	I
rbf	B
kernel	I
label	B
lasso	B
generalization	B
bounds	I
latent	B
variables	I
lda	B
ldim	B
learning	B
curves	I
least	B
squares	I
likelihood	B
ratio	I
linear	O
discriminant	O
analysis	O
see	O
lda	B
linear	B
predictor	B
homogenous	B
linear	B
programming	I
linear	B
regression	B
linkage	B
lipschitzness	B
sub-gradient	B
littlestone	O
dimension	O
see	O
ldim	B
local	B
minimum	I
logistic	B
regression	B
loss	B
loss	B
function	B
loss	B
absolute	B
value	I
loss	B
convex	B
loss	B
generalized	B
hinge-loss	I
hinge	B
loss	B
lipschitz	B
loss	B
log-loss	B
logistic	B
loss	B
ramp	B
loss	B
smooth	B
loss	B
square	B
loss	B
surrogate	B
loss	B
margin	B
markov	O
s	O
inequality	O
massart	B
lemma	I
max	B
linkage	B
maximum	B
a-posteriori	I
maximum	B
likelihood	I
mcdiarmid	O
s	O
inequality	O
mdl	B
measure	B
concentration	I
minimum	O
description	O
length	O
see	O
mdl	B
mistake	B
bound	I
mixture	B
of	I
gaussians	I
model	B
selection	I
multiclass	B
cost-sensitive	B
linear	B
predictors	I
multi-vector	B
perceptron	B
reductions	B
sgd	B
svm	B
multivariate	B
performance	I
measures	I
naive	B
bayes	I
natarajan	B
dimension	I
ndcg	B
nearest	B
neighbor	I
k-nn	B
neural	B
networks	I
feedforward	B
networks	I
layered	B
networks	I
sgd	B
no-free-lunch	B
non-uniform	B
learning	I
index	O
sample	B
complexity	I
sauer	O
s	O
lemma	O
self-boundedness	B
sensitivity	B
sgd	B
shattering	B
single	B
linkage	B
singular	O
value	O
decomposition	O
see	O
svd	B
slud	O
s	O
inequality	O
smoothness	B
soa	B
sparsity-inducing	B
norms	I
specificity	B
spectral	B
clustering	B
srm	B
stability	B
stochastic	O
gradient	B
descent	I
see	O
sgd	B
strong	B
learning	I
structural	O
risk	B
minimization	O
see	O
srm	B
structured	B
output	I
prediction	I
sub-gradient	B
support	O
vector	O
machines	O
see	O
svm	B
svd	B
svm	B
duality	B
generalization	B
bounds	I
hard-svm	B
homogenous	B
kernel	B
trick	I
soft-svm	B
support	B
vectors	I
target	B
set	B
term-frequency	B
tf-idf	B
training	B
error	I
training	B
set	B
true	B
error	I
underfitting	B
uniform	B
convergence	I
union	B
bound	I
unsupervised	B
learning	I
validation	B
cross	B
validation	B
train-validation-test	B
split	I
vapnik-chervonenkis	O
dimension	O
see	O
vc	B
dimension	I
vc	B
dimension	I
version	B
space	I
viola-jones	B
weak	B
learning	I
weighted-majority	B
normalized	O
discounted	O
cumulative	O
gain	B
see	O
ndcg	B
occam	O
s	O
razor	O
omp	B
one-vs-all	B
one-vs-rest	O
see	O
one-vs-all	B
one-vs	O
-all	O
online	B
convex	B
optimization	I
online	B
gradient	B
descent	I
online	B
learning	I
optimization	B
error	I
oracle	B
inequality	I
orthogonal	O
matching	O
pursuit	O
see	O
omp	B
overfitting	B
pac	B
agnostic	B
pac	B
agnostic	B
pac	B
for	I
general	I
loss	B
pac-bayes	B
parametric	B
density	I
estimation	I
pca	B
pearson	O
s	O
correlation	O
coefficient	O
perceptron	B
kernelized	B
perceptron	B
multiclass	B
online	B
permutation	B
matrix	I
polynomial	B
regression	B
precision	B
predictor	B
prefix	B
free	I
language	I
principal	O
component	O
analysis	O
see	O
pca	B
prior	B
knowledge	I
probably	O
approximately	O
correct	O
see	O
pac	B
projection	B
projection	B
lemma	I
proper	B
pruning	B
rademacher	B
complexity	I
random	B
forests	I
random	B
projections	I
ranking	B
bipartite	B
realizability	B
recall	B
regression	B
regularization	B
tikhonov	B
regularized	O
loss	B
minimization	O
see	O
rlm	B
representation	B
independent	I
representative	B
sample	I
representer	B
theorem	I
ridge	B
regression	B
kernel	B
ridge	B
regression	B
rip	B
risk	B
rlm	B
