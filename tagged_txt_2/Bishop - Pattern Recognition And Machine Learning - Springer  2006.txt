information	O
science	O
and	O
statistics	O
series	O
editors	O
m	O
jordan	O
j	O
kleinberg	O
b	O
scho	O
lkopf	O
information	O
science	O
and	O
statistics	O
akaike	O
and	O
kitagawa	O
the	O
practice	O
of	O
time	O
series	O
analysis	O
bishop	O
pattern	O
recognition	O
and	O
machine	O
learning	B
cowell	O
dawid	O
lauritzen	O
and	O
spiegelhalter	O
probabilistic	O
networks	O
and	O
expert	O
systems	O
doucet	O
de	O
freitas	O
and	O
gordon	O
sequential	O
monte	O
carlo	O
methods	O
in	O
practice	O
fine	O
feedforward	O
neural	B
network	I
methodology	O
hawkins	O
and	O
olwell	O
cumulative	O
sum	O
charts	O
and	O
charting	O
for	O
quality	O
improvement	O
jensen	O
bayesian	B
networks	O
and	O
decision	O
graphs	O
marchette	O
computer	O
intrusion	O
detection	O
and	O
network	O
monitoring	O
a	O
statistical	O
viewpoint	O
rubinstein	O
and	O
kroese	O
the	O
cross-entropy	O
method	O
a	O
unified	O
approach	O
to	O
combinatorial	O
optimization	O
monte	O
carlo	O
simulation	O
and	O
machine	O
learning	B
studen	O
probabilistic	O
conditional	B
independence	I
structures	O
vapnik	O
the	O
nature	O
of	O
statistical	O
learning	B
theory	B
second	O
edition	O
wallace	O
statistical	O
and	O
inductive	O
inference	B
by	O
minimum	O
massage	O
length	O
christopher	O
m	O
bishop	O
pattern	O
recognition	O
and	O
machine	O
learning	B
christopher	O
m	O
bishop	O
f	O
r	O
eng	O
assistant	O
director	O
microsoft	O
research	O
ltd	O
cambridge	O
u	O
k	O
cmbishopmicrosoft	O
com	O
httpresearch	O
microsoft	O
com	O
cmbishop	O
series	O
editors	O
michael	O
jordan	O
department	O
of	O
computer	O
science	O
and	O
department	O
of	O
statistics	O
university	O
of	O
california	O
berkeley	O
berkeley	O
ca	O
usa	O
professor	O
jon	O
kleinberg	O
department	O
of	O
computer	O
science	O
cornell	O
university	O
ithaca	O
ny	O
usa	O
bernhard	O
scho	O
lkopf	O
max	O
planck	O
institute	O
for	O
biological	O
cybernetics	O
spemannstrasse	O
tu	O
bingen	O
germany	O
library	O
of	O
congress	O
control	O
number	O
printed	O
on	O
acid-free	O
paper	O
springer	O
sciencebusiness	O
media	O
llc	O
all	O
rights	O
reserved	O
this	O
work	O
may	O
not	O
be	O
translated	O
or	O
copied	O
in	O
whole	O
or	O
in	O
part	O
without	O
the	O
written	O
permission	O
of	O
the	O
publisher	O
sciencebusiness	O
media	O
llc	O
spring	O
street	O
new	O
york	O
ny	O
usa	O
except	O
for	O
brief	O
excerpts	O
in	O
connection	O
with	O
reviews	O
or	O
scholarly	O
analysis	O
use	O
in	O
connection	O
with	O
any	O
form	O
of	O
information	O
storage	O
and	O
retrieval	O
electronic	O
adaptation	O
computer	O
software	O
or	O
by	O
similar	O
or	O
dissimilar	O
methodology	O
now	O
known	O
or	O
hereafter	O
developed	O
is	O
forbidden	O
the	O
use	O
in	O
this	O
publication	O
of	O
trade	O
names	O
trademarks	O
service	O
marks	O
and	O
similar	O
terms	O
even	O
if	O
they	O
are	O
not	O
identified	O
as	O
such	O
is	O
not	O
to	O
be	O
taken	O
as	O
an	O
expression	O
of	O
opinion	O
as	O
to	O
whether	O
or	O
not	O
they	O
are	O
subject	O
to	O
proprietary	O
rights	O
printed	O
in	O
singapore	O
springer	O
com	O
this	O
book	O
is	O
dedicated	O
to	O
my	O
family	O
jenna	O
mark	O
and	O
hugh	O
total	O
eclipse	O
of	O
the	O
sun	O
antalya	O
turkey	O
march	O
preface	O
pattern	O
recognition	O
has	O
its	O
origins	O
in	O
engineering	O
whereas	O
machine	O
learning	B
grew	O
out	O
of	O
computer	O
science	O
however	O
these	O
activities	O
can	O
be	O
viewed	O
as	O
two	O
facets	O
of	O
the	O
same	O
field	O
and	O
together	O
they	O
have	O
undergone	O
substantial	O
development	O
over	O
the	O
past	O
ten	O
years	O
in	O
particular	O
bayesian	B
methods	O
have	O
grown	O
from	O
a	O
specialist	O
niche	O
to	O
become	O
mainstream	O
while	O
graphical	O
models	O
have	O
emerged	O
as	O
a	O
general	O
framework	O
for	O
describing	O
and	O
applying	O
probabilistic	O
models	O
also	O
the	O
practical	O
applicability	O
of	O
bayesian	B
methods	O
has	O
been	O
greatly	O
enhanced	O
through	O
the	O
development	O
of	O
a	O
range	O
of	O
approximate	O
inference	B
algorithms	O
such	O
as	O
variational	B
bayes	B
and	O
expectation	B
propagation	I
similarly	O
new	O
models	O
based	O
on	O
kernels	O
have	O
had	O
significant	O
impact	O
on	O
both	O
algorithms	O
and	O
applications	O
this	O
new	O
textbook	O
reflects	O
these	O
recent	O
developments	O
while	O
providing	O
a	O
comprehensive	O
introduction	O
to	O
the	O
fields	O
of	O
pattern	O
recognition	O
and	O
machine	O
learning	B
it	O
is	O
aimed	O
at	O
advanced	O
undergraduates	O
or	O
first	O
year	O
phd	O
students	O
as	O
well	O
as	O
researchers	O
and	O
practitioners	O
and	O
assumes	O
no	O
previous	O
knowledge	O
of	O
pattern	O
recognition	O
or	O
machine	O
learning	B
concepts	O
knowledge	O
of	O
multivariate	O
calculus	O
and	O
basic	O
linear	O
algebra	O
is	O
required	O
and	O
some	O
familiarity	O
with	O
probabilities	O
would	O
be	O
helpful	O
though	O
not	O
essential	O
as	O
the	O
book	O
includes	O
a	O
self-contained	O
introduction	O
to	O
basic	O
probability	B
theory	B
because	O
this	O
book	O
has	O
broad	O
scope	O
it	O
is	O
impossible	O
to	O
provide	O
a	O
complete	O
list	O
of	O
references	O
and	O
in	O
particular	O
no	O
attempt	O
has	O
been	O
made	O
to	O
provide	O
accurate	O
historical	O
attribution	O
of	O
ideas	O
instead	O
the	O
aim	O
has	O
been	O
to	O
give	O
references	O
that	O
offer	O
greater	O
detail	O
than	O
is	O
possible	O
here	O
and	O
that	O
hopefully	O
provide	O
entry	O
points	O
into	O
what	O
in	O
some	O
cases	O
is	O
a	O
very	O
extensive	O
literature	O
for	O
this	O
reason	O
the	O
references	O
are	O
often	O
to	O
more	O
recent	O
textbooks	O
and	O
review	O
articles	O
rather	O
than	O
to	O
original	O
sources	O
the	O
book	O
is	O
supported	O
by	O
a	O
great	O
deal	O
of	O
additional	O
material	O
including	O
lecture	O
slides	O
as	O
well	O
as	O
the	O
complete	O
set	O
of	O
figures	O
used	O
in	O
the	O
book	O
and	O
the	O
reader	O
is	O
encouraged	O
to	O
visit	O
the	O
book	O
web	O
site	O
for	O
the	O
latest	O
information	O
httpresearch	O
microsoft	O
com	O
cmbishopprml	O
vii	O
viii	O
preface	O
exercises	O
the	O
exercises	O
that	O
appear	O
at	O
the	O
end	O
of	O
every	O
chapter	O
form	O
an	O
important	O
component	O
of	O
the	O
book	O
each	O
exercise	O
has	O
been	O
carefully	O
chosen	O
to	O
reinforce	O
concepts	O
explained	O
in	O
the	O
text	O
or	O
to	O
develop	O
and	O
generalize	O
them	O
in	O
significant	O
ways	O
and	O
each	O
is	O
graded	O
according	O
to	O
difficulty	O
ranging	O
from	O
which	O
denotes	O
a	O
simple	O
exercise	O
taking	O
a	O
few	O
minutes	O
to	O
complete	O
through	O
to	O
which	O
denotes	O
a	O
significantly	O
more	O
complex	O
exercise	O
it	O
has	O
been	O
difficult	O
to	O
know	O
to	O
what	O
extent	O
these	O
solutions	O
should	O
be	O
made	O
widely	O
available	O
those	O
engaged	O
in	O
self	O
study	O
will	O
find	O
worked	O
solutions	O
very	O
beneficial	O
whereas	O
many	O
course	O
tutors	O
request	O
that	O
solutions	O
be	O
available	O
only	O
via	O
the	O
publisher	O
so	O
that	O
the	O
exercises	O
may	O
be	O
used	O
in	O
class	O
in	O
order	O
to	O
try	O
to	O
meet	O
these	O
conflicting	O
requirements	O
those	O
exercises	O
that	O
help	O
amplify	O
key	O
points	O
in	O
the	O
text	O
or	O
that	O
fill	O
in	O
important	O
details	O
have	O
solutions	O
that	O
are	O
available	O
as	O
a	O
pdf	O
file	O
from	O
the	O
book	O
web	O
site	O
such	O
exercises	O
are	O
denoted	O
by	O
www	O
solutions	O
for	O
the	O
remaining	O
exercises	O
are	O
available	O
to	O
course	O
tutors	O
by	O
contacting	O
the	O
publisher	O
details	O
are	O
given	O
on	O
the	O
book	O
web	O
site	O
readers	O
are	O
strongly	O
encouraged	O
to	O
work	O
through	O
the	O
exercises	O
unaided	O
and	O
to	O
turn	O
to	O
the	O
solutions	O
only	O
as	O
required	O
although	O
this	O
book	O
focuses	O
on	O
concepts	O
and	O
principles	O
in	O
a	O
taught	O
course	O
the	O
students	O
should	O
ideally	O
have	O
the	O
opportunity	O
to	O
experiment	O
with	O
some	O
of	O
the	O
key	O
algorithms	O
using	O
appropriate	O
data	O
sets	O
a	O
companion	O
volume	O
and	O
nabney	O
will	O
deal	O
with	O
practical	O
aspects	O
of	O
pattern	O
recognition	O
and	O
machine	O
learning	B
and	O
will	O
be	O
accompanied	O
by	O
matlab	O
software	O
implementing	O
most	O
of	O
the	O
algorithms	O
discussed	O
in	O
this	O
book	O
acknowledgements	O
first	O
of	O
all	O
i	O
would	O
like	O
to	O
express	O
my	O
sincere	O
thanks	O
to	O
markus	O
svens	O
en	O
who	O
has	O
provided	O
immense	O
help	O
with	O
preparation	O
of	O
figures	O
and	O
with	O
the	O
typesetting	O
of	O
the	O
book	O
in	O
latex	O
his	O
assistance	O
has	O
been	O
invaluable	O
i	O
am	O
very	O
grateful	O
to	O
microsoft	O
research	O
for	O
providing	O
a	O
highly	O
stimulating	O
research	O
environment	O
and	O
for	O
giving	O
me	O
the	O
freedom	O
to	O
write	O
this	O
book	O
views	O
and	O
opinions	O
expressed	O
in	O
this	O
book	O
however	O
are	O
my	O
own	O
and	O
are	O
therefore	O
not	O
necessarily	O
the	O
same	O
as	O
those	O
of	O
microsoft	O
or	O
its	O
affiliates	O
springer	O
has	O
provided	O
excellent	O
support	O
throughout	O
the	O
final	O
stages	O
of	O
preparation	O
of	O
this	O
book	O
and	O
i	O
would	O
like	O
to	O
thank	O
my	O
commissioning	O
editor	O
john	O
kimmel	O
for	O
his	O
support	O
and	O
professionalism	O
as	O
well	O
as	O
joseph	O
piliero	O
for	O
his	O
help	O
in	O
designing	O
the	O
cover	O
and	O
the	O
text	O
format	O
and	O
maryann	O
brickner	O
for	O
her	O
numerous	O
contributions	O
during	O
the	O
production	O
phase	O
the	O
inspiration	O
for	O
the	O
cover	O
design	O
came	O
from	O
a	O
discussion	O
with	O
antonio	O
criminisi	O
i	O
also	O
wish	O
to	O
thank	O
oxford	O
university	O
press	O
for	O
permission	O
to	O
reproduce	O
excerpts	O
from	O
an	O
earlier	O
textbook	O
neural	O
networks	O
for	O
pattern	O
recognition	O
the	O
images	O
of	O
the	O
mark	O
perceptron	B
and	O
of	O
frank	O
rosenblatt	B
are	O
reproduced	O
with	O
the	O
permission	O
of	O
arvin	O
calspan	O
advanced	O
technology	O
center	O
i	O
would	O
also	O
like	O
to	O
thank	O
asela	O
gunawardana	O
for	O
plotting	O
the	O
spectrogram	B
in	O
figure	O
and	O
bernhard	O
sch	O
olkopf	O
for	O
permission	O
to	O
use	O
his	O
kernel	B
pca	I
code	O
to	O
plot	O
figure	O
preface	O
ix	O
many	O
people	O
have	O
helped	O
by	O
proofreading	O
draft	O
material	O
and	O
providing	O
comments	O
and	O
suggestions	O
including	O
shivani	O
agarwal	O
c	O
edric	O
archambeau	O
arik	O
azran	O
andrew	O
blake	O
hakan	O
cevikalp	O
michael	O
fourman	O
brendan	O
frey	O
zoubin	O
ghahramani	O
thore	O
graepel	O
katherine	O
heller	O
ralf	O
herbrich	O
geoffrey	O
hinton	O
adam	O
johansen	O
matthew	O
johnson	O
michael	O
jordan	O
eva	O
kalyvianaki	O
anitha	O
kannan	O
julia	O
lasserre	O
david	O
liu	O
tom	O
minka	O
ian	O
nabney	O
tonatiuh	O
pena	O
yuan	O
qi	O
sam	O
roweis	O
balaji	O
sanjiya	O
toby	O
sharp	O
ana	O
costa	O
e	O
silva	O
david	O
spiegelhalter	O
jay	O
stokes	O
tara	O
symeonides	O
martin	O
szummer	O
marshall	O
tappen	O
ilkay	O
ulusoy	O
chris	O
williams	O
john	O
winn	O
and	O
andrew	O
zisserman	O
finally	O
i	O
would	O
like	O
to	O
thank	O
my	O
wife	O
jenna	O
who	O
has	O
been	O
hugely	O
supportive	O
throughout	O
the	O
several	O
years	O
it	O
has	O
taken	O
to	O
write	O
this	O
book	O
chris	O
bishop	O
cambridge	O
february	O
mathematical	O
notation	O
i	O
have	O
tried	O
to	O
keep	O
the	O
mathematical	O
content	O
of	O
the	O
book	O
to	O
the	O
minimum	O
necessary	O
to	O
achieve	O
a	O
proper	O
understanding	O
of	O
the	O
field	O
however	O
this	O
minimum	O
level	O
is	O
nonzero	O
and	O
it	O
should	O
be	O
emphasized	O
that	O
a	O
good	O
grasp	O
of	O
calculus	O
linear	O
algebra	O
and	O
probability	B
theory	B
is	O
essential	O
for	O
a	O
clear	O
understanding	O
of	O
modern	O
pattern	O
recognition	O
and	O
machine	O
learning	B
techniques	O
nevertheless	O
the	O
emphasis	O
in	O
this	O
book	O
is	O
on	O
conveying	O
the	O
underlying	O
concepts	O
rather	O
than	O
on	O
mathematical	O
rigour	O
i	O
have	O
tried	O
to	O
use	O
a	O
consistent	B
notation	O
throughout	O
the	O
book	O
although	O
at	O
times	O
this	O
means	O
departing	O
from	O
some	O
of	O
the	O
conventions	O
used	O
in	O
the	O
corresponding	O
research	O
literature	O
vectors	O
are	O
denoted	O
by	O
lower	O
case	O
bold	O
roman	O
letters	O
such	O
as	O
x	O
and	O
all	O
vectors	O
are	O
assumed	O
to	O
be	O
column	O
vectors	O
a	O
superscript	O
t	O
denotes	O
the	O
transpose	O
of	O
a	O
matrix	O
or	O
vector	O
so	O
that	O
xt	O
will	O
be	O
a	O
row	O
vector	O
uppercase	O
bold	O
roman	O
letters	O
such	O
as	O
m	O
denote	O
matrices	O
the	O
notation	O
wm	O
denotes	O
a	O
row	O
vector	O
with	O
m	O
elements	O
while	O
the	O
corresponding	O
column	O
vector	O
is	O
written	O
as	O
w	O
wm	O
the	O
notation	O
b	O
is	O
used	O
to	O
denote	O
the	O
closed	O
interval	O
from	O
a	O
to	O
b	O
that	O
is	O
the	O
interval	O
including	O
the	O
values	O
a	O
and	O
b	O
themselves	O
while	O
b	O
denotes	O
the	O
corresponding	O
open	O
interval	O
that	O
is	O
the	O
interval	O
excluding	O
a	O
and	O
b	O
similarly	O
b	O
denotes	O
an	O
interval	O
that	O
includes	O
a	O
but	O
excludes	O
b	O
for	O
the	O
most	O
part	O
however	O
there	O
will	O
be	O
little	O
need	O
to	O
dwell	O
on	O
such	O
refinements	O
as	O
whether	O
the	O
end	O
points	O
of	O
an	O
interval	O
are	O
included	O
or	O
not	O
the	O
m	O
m	O
identity	O
matrix	O
known	O
as	O
the	O
unit	O
matrix	O
is	O
denoted	O
im	O
which	O
will	O
be	O
abbreviated	O
to	O
i	O
where	O
there	O
is	O
no	O
ambiguity	O
about	O
it	O
dimensionality	O
it	O
has	O
elements	O
iij	O
that	O
equal	O
if	O
i	O
j	O
and	O
if	O
i	O
j	O
functional	B
is	O
discussed	O
in	O
appendix	O
d	O
a	O
functional	B
is	O
denoted	O
fy	O
where	O
yx	O
is	O
some	O
function	O
the	O
concept	O
of	O
a	O
the	O
notation	O
gx	O
ofx	O
denotes	O
that	O
is	O
bounded	O
as	O
x	O
for	O
instance	O
if	O
gx	O
then	O
gx	O
the	O
expectation	B
of	O
a	O
function	O
fx	O
y	O
with	O
respect	O
to	O
a	O
random	O
variable	O
x	O
is	O
denoted	O
by	O
exfx	O
y	O
in	O
situations	O
where	O
there	O
is	O
no	O
ambiguity	O
as	O
to	O
which	O
variable	O
is	O
being	O
averaged	O
over	O
this	O
will	O
be	O
simplified	O
by	O
omitting	O
the	O
suffix	O
for	O
instance	O
xi	O
xii	O
mathematical	O
notation	O
ex	O
if	O
the	O
distribution	O
of	O
x	O
is	O
conditioned	O
on	O
another	O
variable	O
z	O
then	O
the	O
corresponding	O
conditional	B
expectation	B
will	O
be	O
written	O
exfxz	O
similarly	O
the	O
variance	B
is	O
denoted	O
varfx	O
and	O
for	O
vector	O
variables	O
the	O
covariance	B
is	O
written	O
covx	O
y	O
we	O
shall	O
also	O
use	O
covx	O
as	O
a	O
shorthand	O
notation	O
for	O
covx	O
x	O
the	O
concepts	O
of	O
expectations	O
and	O
covariances	O
are	O
introduced	O
in	O
section	O
if	O
we	O
have	O
n	O
values	O
xn	O
of	O
a	O
d-dimensional	O
vector	O
x	O
xdt	O
we	O
can	O
combine	O
the	O
observations	O
into	O
a	O
data	O
matrix	O
x	O
in	O
which	O
the	O
nth	O
row	O
of	O
x	O
corresponds	O
to	O
the	O
row	O
vector	O
xt	O
n	O
thus	O
the	O
n	O
i	O
element	O
of	O
x	O
corresponds	O
to	O
the	O
ith	O
element	O
of	O
the	O
nth	O
observation	O
xn	O
for	O
the	O
case	O
of	O
one-dimensional	O
variables	O
we	O
shall	O
denote	O
such	O
a	O
matrix	O
by	O
x	O
which	O
is	O
a	O
column	O
vector	O
whose	O
nth	O
element	O
is	O
xn	O
note	O
that	O
x	O
has	O
dimensionality	O
n	O
uses	O
a	O
different	O
typeface	O
to	O
distinguish	O
it	O
from	O
x	O
has	O
dimensionality	O
d	O
contents	O
preface	O
mathematical	O
notation	O
contents	O
introduction	O
vii	O
xi	O
xiii	O
xiii	O
example	O
polynomial	B
curve	B
fitting	I
probability	B
theory	B
probability	B
densities	O
expectations	O
and	O
covariances	O
bayesian	B
probabilities	O
the	O
gaussian	B
distribution	O
curve	B
fitting	I
re-visited	O
bayesian	B
curve	B
fitting	I
model	B
selection	I
decision	B
theory	B
the	O
curse	B
of	I
dimensionality	I
minimizing	O
the	O
misclassification	O
rate	O
minimizing	O
the	O
expected	O
loss	O
the	O
reject	B
option	I
inference	B
and	O
decision	O
loss	O
functions	O
for	B
regression	B
information	B
theory	B
relative	B
entropy	B
and	O
mutual	B
information	I
exercises	O
xiv	O
contents	O
probability	B
distributions	O
binary	O
variables	O
the	O
beta	B
distribution	I
multinomial	O
variables	O
sequential	B
estimation	I
the	O
dirichlet	B
distribution	I
the	O
gaussian	B
distribution	O
conditional	B
gaussian	B
distributions	O
marginal	B
gaussian	B
distributions	O
bayes	B
theorem	O
for	O
gaussian	B
variables	O
maximum	B
likelihood	I
for	O
the	O
gaussian	B
bayesian	B
inference	B
for	O
the	O
gaussian	B
student	O
s	O
t-distribution	O
periodic	O
variables	O
mixtures	O
of	O
gaussians	O
the	O
exponential	B
family	I
maximum	B
likelihood	I
and	O
sufficient	B
statistics	I
conjugate	B
priors	O
noninformative	B
priors	O
nonparametric	B
methods	I
kernel	O
density	B
estimators	O
nearest-neighbour	B
methods	I
exercises	O
linear	O
models	O
for	B
regression	B
linear	O
basis	B
function	I
models	O
maximum	B
likelihood	I
and	O
least	O
squares	O
geometry	O
of	O
least	O
squares	O
regularized	B
least	I
squares	I
multiple	O
outputs	O
the	O
bias-variance	O
decomposition	O
sequential	B
learning	B
equivalent	B
kernel	I
bayesian	B
linear	B
regression	B
parameter	O
distribution	O
predictive	B
distribution	I
bayesian	B
model	B
comparison	I
the	O
evidence	B
approximation	I
evaluation	O
of	O
the	O
evidence	B
function	I
maximizing	O
the	O
evidence	B
function	I
effective	B
number	I
of	I
parameters	I
limitations	O
of	O
fixed	O
basis	O
functions	O
exercises	O
fisher	B
s	O
linear	B
discriminant	I
fisher	B
s	O
discriminant	O
for	O
multiple	O
classes	O
linear	O
models	O
for	O
classification	B
discriminant	O
functions	O
two	O
classes	O
multiple	O
classes	O
least	O
squares	O
for	O
classification	B
relation	O
to	O
least	O
squares	O
the	O
perceptron	B
algorithm	O
probabilistic	O
generative	O
models	O
continuous	O
inputs	O
maximum	B
likelihood	I
solution	O
discrete	O
features	O
exponential	B
family	I
probabilistic	O
discriminative	O
models	O
fixed	O
basis	O
functions	O
logistic	B
regression	B
multiclass	B
logistic	B
regression	B
canonical	O
link	B
functions	O
the	O
laplace	B
approximation	I
model	B
comparison	I
and	O
bic	O
iterative	B
reweighted	I
least	I
squares	I
probit	B
regression	B
bayesian	B
logistic	B
regression	B
laplace	B
approximation	I
predictive	B
distribution	I
exercises	O
neural	O
networks	O
feed-forward	O
network	O
functions	O
weight-space	O
symmetries	B
network	O
training	B
parameter	O
optimization	O
local	B
quadratic	O
approximation	O
use	O
of	O
gradient	O
information	O
gradient	B
descent	I
optimization	O
error	B
backpropagation	B
evaluation	O
of	O
error-function	O
derivatives	O
a	O
simple	O
example	O
efficiency	O
of	O
backpropagation	B
the	O
jacobian	B
matrix	I
the	O
hessian	B
matrix	I
diagonal	B
approximation	I
outer	B
product	I
approximation	I
inverse	B
hessian	O
contents	O
xv	O
xvi	O
contents	O
finite	O
differences	O
exact	B
evaluation	I
of	O
the	O
hessian	O
fast	B
multiplication	I
by	O
the	O
hessian	O
regularization	B
in	O
neural	O
networks	O
invariances	O
consistent	B
gaussian	B
priors	O
early	B
stopping	I
tangent	B
propagation	I
training	B
with	O
transformed	O
data	O
convolutional	B
networks	O
soft	B
weight	B
sharing	I
mixture	B
density	B
networks	O
bayesian	B
neural	O
networks	O
posterior	O
parameter	O
distribution	O
hyperparameter	B
optimization	O
bayesian	B
neural	O
networks	O
for	O
classification	B
exercises	O
kernel	O
methods	O
dual	O
representations	O
constructing	O
kernels	O
radial	B
basis	B
function	I
networks	O
nadaraya-watson	O
model	O
gaussian	B
processes	O
linear	B
regression	B
revisited	O
gaussian	B
processes	O
for	B
regression	B
learning	B
the	O
hyperparameters	O
automatic	B
relevance	I
determination	I
gaussian	B
processes	O
for	O
classification	B
laplace	B
approximation	I
connection	O
to	O
neural	O
networks	O
exercises	O
sparse	O
kernel	O
machines	O
maximum	O
margin	B
classifiers	O
overlapping	O
class	O
distributions	O
relation	O
to	O
logistic	B
regression	B
multiclass	B
svms	O
svms	O
for	B
regression	B
computational	B
learning	B
theory	B
relevance	B
vector	I
machines	O
rvm	O
for	B
regression	B
analysis	O
of	O
sparsity	B
rvm	O
for	O
classification	B
exercises	O
contents	O
xvii	O
graphical	O
models	O
bayesian	B
networks	O
example	O
polynomial	O
regression	B
generative	O
models	O
discrete	O
variables	O
linear-gaussian	O
models	O
conditional	B
independence	I
three	O
example	O
graphs	O
d-separation	B
markov	O
random	O
fields	O
inference	B
on	O
a	O
chain	O
conditional	B
independence	I
properties	O
factorization	B
properties	O
illustration	O
image	B
de-noising	I
relation	O
to	O
directed	B
graphs	O
inference	B
in	O
graphical	O
models	O
trees	O
the	O
sum-product	B
algorithm	I
the	O
max-sum	B
algorithm	I
exact	O
inference	B
in	O
general	O
graphs	O
loopy	B
belief	B
propagation	I
learning	B
the	O
graph	O
structure	O
factor	O
graphs	O
exercises	O
mixture	B
models	O
and	O
em	B
k-means	O
clustering	B
image	O
segmentation	O
and	O
compression	O
mixtures	O
of	O
gaussians	O
maximum	B
likelihood	I
em	B
for	O
gaussian	B
mixtures	O
an	O
alternative	O
view	O
of	O
em	B
gaussian	B
mixtures	O
revisited	O
relation	O
to	O
k-means	O
mixtures	O
of	O
bernoulli	B
distributions	O
em	B
for	O
bayesian	B
linear	B
regression	B
the	O
em	B
algorithm	I
in	O
general	O
exercises	O
approximate	O
inference	B
variational	B
inference	B
factorized	O
distributions	O
properties	O
of	O
factorized	O
approximations	O
example	O
the	O
univariate	O
gaussian	B
model	B
comparison	I
illustration	O
variational	B
mixture	B
of	I
gaussians	I
xviii	O
contents	O
variational	B
distribution	O
variational	B
lower	B
bound	I
predictive	O
density	B
determining	O
the	O
number	O
of	O
components	O
induced	O
factorizations	O
variational	B
linear	B
regression	B
variational	B
distribution	O
predictive	B
distribution	I
lower	B
bound	I
exponential	B
family	I
distributions	O
variational	B
message	B
passing	I
local	B
variational	B
methods	O
variational	B
logistic	B
regression	B
variational	B
posterior	O
distribution	O
optimizing	O
the	O
variational	B
parameters	O
inference	B
of	O
hyperparameters	O
expectation	B
propagation	I
example	O
the	O
clutter	B
problem	I
expectation	B
propagation	I
on	O
graphs	O
exercises	O
sampling	B
methods	I
basic	O
sampling	O
algorithms	O
standard	O
distributions	O
rejection	B
sampling	I
adaptive	B
rejection	B
sampling	I
importance	B
sampling	I
sampling-importance-resampling	B
sampling	O
and	O
the	O
em	B
algorithm	I
markov	B
chain	I
monte	I
carlo	I
markov	O
chains	O
the	O
metropolis-hastings	B
algorithm	I
gibbs	B
sampling	I
slice	B
sampling	I
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
dynamical	O
systems	O
hybrid	B
monte	I
carlo	I
estimating	O
the	O
partition	B
function	I
exercises	O
continuous	O
latent	O
variables	O
principal	B
component	I
analysis	I
maximum	O
variance	B
formulation	O
minimum-error	O
formulation	O
applications	O
of	O
pca	O
pca	O
for	O
high-dimensional	O
data	O
contents	O
xix	O
probabilistic	B
pca	I
maximum	B
likelihood	I
pca	O
em	B
algorithm	I
for	O
pca	O
bayesian	B
pca	O
factor	B
analysis	I
kernel	B
pca	I
nonlinear	O
latent	B
variable	I
models	O
independent	B
component	I
analysis	I
autoassociative	O
neural	O
networks	O
modelling	O
nonlinear	O
manifolds	O
exercises	O
sequential	B
data	I
markov	O
models	O
hidden	O
markov	O
models	O
maximum	B
likelihood	I
for	O
the	O
hmm	O
the	O
forward-backward	B
algorithm	I
the	O
sum-product	B
algorithm	I
for	O
the	O
hmm	O
scaling	O
factors	O
the	O
viterbi	B
algorithm	I
extensions	O
of	O
the	O
hidden	B
markov	B
model	I
linear	O
dynamical	O
systems	O
inference	B
in	O
lds	O
learning	B
in	O
lds	O
extensions	O
of	O
lds	O
particle	O
filters	O
exercises	O
combining	B
models	I
bayesian	B
model	B
averaging	I
committees	O
boosting	B
minimizing	O
exponential	O
error	B
error	B
functions	O
for	O
boosting	B
tree-based	O
models	O
conditional	B
mixture	B
models	O
mixtures	O
of	O
linear	B
regression	B
models	O
mixtures	O
of	O
logistic	O
models	O
mixtures	O
of	O
experts	O
exercises	O
appendix	O
a	O
data	O
sets	O
appendix	O
b	O
probability	B
distributions	O
appendix	O
c	O
properties	O
of	O
matrices	O
xx	O
contents	O
appendix	O
d	O
calculus	B
of	I
variations	I
appendix	O
e	O
lagrange	B
multipliers	O
references	O
index	O
introduction	O
the	O
problem	O
of	O
searching	O
for	O
patterns	O
in	O
data	O
is	O
a	O
fundamental	O
one	O
and	O
has	O
a	O
long	O
and	O
successful	O
history	O
for	O
instance	O
the	O
extensive	O
astronomical	O
observations	O
of	O
tycho	O
brahe	O
in	O
the	O
century	O
allowed	O
johannes	O
kepler	O
to	O
discover	O
the	O
empirical	O
laws	O
of	O
planetary	O
motion	O
which	O
in	O
turn	O
provided	O
a	O
springboard	O
for	O
the	O
development	O
of	O
classical	B
mechanics	O
similarly	O
the	O
discovery	O
of	O
regularities	O
in	O
atomic	O
spectra	O
played	O
a	O
key	O
role	O
in	O
the	O
development	O
and	O
verification	O
of	O
quantum	O
physics	O
in	O
the	O
early	O
twentieth	O
century	O
the	O
field	O
of	O
pattern	O
recognition	O
is	O
concerned	O
with	O
the	O
automatic	O
discovery	O
of	O
regularities	O
in	O
data	O
through	O
the	O
use	O
of	O
computer	O
algorithms	O
and	O
with	O
the	O
use	O
of	O
these	O
regularities	O
to	O
take	O
actions	O
such	O
as	O
classifying	O
the	O
data	O
into	O
different	O
categories	O
consider	O
the	O
example	O
of	O
recognizing	O
handwritten	O
digits	O
illustrated	O
in	O
figure	O
each	O
digit	O
corresponds	O
to	O
a	O
pixel	O
image	O
and	O
so	O
can	O
be	O
represented	O
by	O
a	O
vector	O
x	O
comprising	O
real	O
numbers	O
the	O
goal	O
is	O
to	O
build	O
a	O
machine	O
that	O
will	O
take	O
such	O
a	O
vector	O
x	O
as	O
input	O
and	O
that	O
will	O
produce	O
the	O
identity	O
of	O
the	O
digit	O
as	O
the	O
output	O
this	O
is	O
a	O
nontrivial	O
problem	O
due	O
to	O
the	O
wide	O
variability	O
of	O
handwriting	O
it	O
could	O
be	O
introduction	O
figure	O
examples	O
of	O
hand-written	O
dig	O
its	O
taken	O
from	O
us	O
zip	O
codes	O
tackled	O
using	O
handcrafted	O
rules	O
or	O
heuristics	O
for	O
distinguishing	O
the	O
digits	O
based	O
on	O
the	O
shapes	O
of	O
the	O
strokes	O
but	O
in	O
practice	O
such	O
an	O
approach	O
leads	O
to	O
a	O
proliferation	O
of	O
rules	O
and	O
of	O
exceptions	O
to	O
the	O
rules	O
and	O
so	O
on	O
and	O
invariably	O
gives	O
poor	O
results	O
far	O
better	O
results	O
can	O
be	O
obtained	O
by	O
adopting	O
a	O
machine	O
learning	B
approach	O
in	O
which	O
a	O
large	O
set	O
of	O
n	O
digits	O
xn	O
called	O
a	O
training	B
set	I
is	O
used	O
to	O
tune	O
the	O
parameters	O
of	O
an	O
adaptive	O
model	O
the	O
categories	O
of	O
the	O
digits	O
in	O
the	O
training	B
set	I
are	O
known	O
in	O
advance	O
typically	O
by	O
inspecting	O
them	O
individually	O
and	O
hand-labelling	O
them	O
we	O
can	O
express	O
the	O
category	O
of	O
a	O
digit	O
using	O
target	B
vector	I
t	O
which	O
represents	O
the	O
identity	O
of	O
the	O
corresponding	O
digit	O
suitable	O
techniques	O
for	O
representing	O
categories	O
in	O
terms	O
of	O
vectors	O
will	O
be	O
discussed	O
later	O
note	O
that	O
there	O
is	O
one	O
such	O
target	B
vector	I
t	O
for	O
each	O
digit	O
image	O
x	O
the	O
result	O
of	O
running	O
the	O
machine	O
learning	B
algorithm	O
can	O
be	O
expressed	O
as	O
a	O
function	O
yx	O
which	O
takes	O
a	O
new	O
digit	O
image	O
x	O
as	O
input	O
and	O
that	O
generates	O
an	O
output	O
vector	O
y	O
encoded	O
in	O
the	O
same	O
way	O
as	O
the	O
target	O
vectors	O
the	O
precise	O
form	O
of	O
the	O
function	O
yx	O
is	O
determined	O
during	O
the	O
training	B
phase	O
also	O
known	O
as	O
the	O
learning	B
phase	O
on	O
the	O
basis	O
of	O
the	O
training	B
data	O
once	O
the	O
model	O
is	O
trained	O
it	O
can	O
then	O
determine	O
the	O
identity	O
of	O
new	O
digit	O
images	O
which	O
are	O
said	O
to	O
comprise	O
a	O
test	B
set	I
the	O
ability	O
to	O
categorize	O
correctly	O
new	O
examples	O
that	O
differ	O
from	O
those	O
used	O
for	O
training	B
is	O
known	O
as	O
generalization	B
in	O
practical	O
applications	O
the	O
variability	O
of	O
the	O
input	O
vectors	O
will	O
be	O
such	O
that	O
the	O
training	B
data	O
can	O
comprise	O
only	O
a	O
tiny	O
fraction	O
of	O
all	O
possible	O
input	O
vectors	O
and	O
so	O
generalization	B
is	O
a	O
central	O
goal	O
in	O
pattern	O
recognition	O
for	O
most	O
practical	O
applications	O
the	O
original	O
input	O
variables	O
are	O
typically	O
preprocessed	O
to	O
transform	O
them	O
into	O
some	O
new	O
space	O
of	O
variables	O
where	O
it	O
is	O
hoped	O
the	O
pattern	O
recognition	O
problem	O
will	O
be	O
easier	O
to	O
solve	O
for	O
instance	O
in	O
the	O
digit	O
recognition	O
problem	O
the	O
images	O
of	O
the	O
digits	O
are	O
typically	O
translated	O
and	O
scaled	O
so	O
that	O
each	O
digit	O
is	O
contained	O
within	O
a	O
box	O
of	O
a	O
fixed	O
size	O
this	O
greatly	O
reduces	O
the	O
variability	O
within	O
each	O
digit	O
class	O
because	O
the	O
location	O
and	O
scale	O
of	O
all	O
the	O
digits	O
are	O
now	O
the	O
same	O
which	O
makes	O
it	O
much	O
easier	O
for	O
a	O
subsequent	O
pattern	O
recognition	O
algorithm	O
to	O
distinguish	O
between	O
the	O
different	O
classes	O
this	O
pre-processing	O
stage	O
is	O
sometimes	O
also	O
called	O
feature	B
extraction	I
note	O
that	O
new	O
test	O
data	O
must	O
be	O
pre-processed	O
using	O
the	O
same	O
steps	O
as	O
the	O
training	B
data	O
pre-processing	O
might	O
also	O
be	O
performed	O
in	O
order	O
to	O
speed	O
up	O
computation	O
for	O
example	O
if	O
the	O
goal	O
is	O
real-time	O
face	B
detection	I
in	O
a	O
high-resolution	O
video	O
stream	O
the	O
computer	O
must	O
handle	O
huge	O
numbers	O
of	O
pixels	O
per	O
second	O
and	O
presenting	O
these	O
directly	O
to	O
a	O
complex	O
pattern	O
recognition	O
algorithm	O
may	O
be	O
computationally	O
infeasible	O
instead	O
the	O
aim	O
is	O
to	O
find	O
useful	O
features	O
that	O
are	O
fast	O
to	O
compute	O
and	O
yet	O
that	O
introduction	O
also	O
preserve	O
useful	O
discriminatory	O
information	O
enabling	O
faces	O
to	O
be	O
distinguished	O
from	O
non-faces	O
these	O
features	O
are	O
then	O
used	O
as	O
the	O
inputs	O
to	O
the	O
pattern	O
recognition	O
algorithm	O
for	O
instance	O
the	O
average	O
value	O
of	O
the	O
image	O
intensity	O
over	O
a	O
rectangular	O
subregion	O
can	O
be	O
evaluated	O
extremely	O
efficiently	O
and	O
jones	O
and	O
a	O
set	O
of	O
such	O
features	O
can	O
prove	O
very	O
effective	O
in	O
fast	O
face	B
detection	I
because	O
the	O
number	O
of	O
such	O
features	O
is	O
smaller	O
than	O
the	O
number	O
of	O
pixels	O
this	O
kind	O
of	O
pre-processing	O
represents	O
a	O
form	O
of	O
dimensionality	O
reduction	O
care	O
must	O
be	O
taken	O
during	O
pre-processing	O
because	O
often	O
information	O
is	O
discarded	O
and	O
if	O
this	O
information	O
is	O
important	O
to	O
the	O
solution	O
of	O
the	O
problem	O
then	O
the	O
overall	O
accuracy	O
of	O
the	O
system	O
can	O
suffer	O
applications	O
in	O
which	O
the	O
training	B
data	O
comprises	O
examples	O
of	O
the	O
input	O
vectors	O
along	O
with	O
their	O
corresponding	O
target	O
vectors	O
are	O
known	O
as	O
supervised	B
learning	B
problems	O
cases	O
such	O
as	O
the	O
digit	O
recognition	O
example	O
in	O
which	O
the	O
aim	O
is	O
to	O
assign	O
each	O
input	O
vector	O
to	O
one	O
of	O
a	O
finite	O
number	O
of	O
discrete	O
categories	O
are	O
called	O
classification	B
problems	O
if	O
the	O
desired	O
output	O
consists	O
of	O
one	O
or	O
more	O
continuous	O
variables	O
then	O
the	O
task	O
is	O
called	O
regression	B
an	O
example	O
of	O
a	O
regression	B
problem	O
would	O
be	O
the	O
prediction	O
of	O
the	O
yield	O
in	O
a	O
chemical	O
manufacturing	O
process	O
in	O
which	O
the	O
inputs	O
consist	O
of	O
the	O
concentrations	O
of	O
reactants	O
the	O
temperature	O
and	O
the	O
pressure	O
in	O
other	O
pattern	O
recognition	O
problems	O
the	O
training	B
data	O
consists	O
of	O
a	O
set	O
of	O
input	O
vectors	O
x	O
without	O
any	O
corresponding	O
target	O
values	O
the	O
goal	O
in	O
such	O
unsupervised	B
learning	B
problems	O
may	O
be	O
to	O
discover	O
groups	O
of	O
similar	O
examples	O
within	O
the	O
data	O
where	O
it	O
is	O
called	O
clustering	B
or	O
to	O
determine	O
the	O
distribution	O
of	O
data	O
within	O
the	O
input	O
space	O
known	O
as	O
density	B
estimation	I
or	O
to	O
project	O
the	O
data	O
from	O
a	O
high-dimensional	O
space	O
down	O
to	O
two	O
or	O
three	O
dimensions	O
for	O
the	O
purpose	O
of	O
visualization	B
finally	O
the	O
technique	O
of	O
reinforcement	B
learning	B
and	O
barto	O
is	O
concerned	O
with	O
the	O
problem	O
of	O
finding	O
suitable	O
actions	O
to	O
take	O
in	O
a	O
given	O
situation	O
in	O
order	O
to	O
maximize	O
a	O
reward	O
here	O
the	O
learning	B
algorithm	O
is	O
not	O
given	O
examples	O
of	O
optimal	O
outputs	O
in	O
contrast	O
to	O
supervised	B
learning	B
but	O
must	O
instead	O
discover	O
them	O
by	O
a	O
process	O
of	O
trial	O
and	O
error	B
typically	O
there	O
is	O
a	O
sequence	O
of	O
states	O
and	O
actions	O
in	O
which	O
the	O
learning	B
algorithm	O
is	O
interacting	O
with	O
its	O
environment	O
in	O
many	O
cases	O
the	O
current	O
action	O
not	O
only	O
affects	O
the	O
immediate	O
reward	O
but	O
also	O
has	O
an	O
impact	O
on	O
the	O
reward	O
at	O
all	O
subsequent	O
time	O
steps	O
for	O
example	O
by	O
using	O
appropriate	O
reinforcement	B
learning	B
techniques	O
a	O
neural	B
network	I
can	O
learn	O
to	O
play	O
the	O
game	O
of	O
backgammon	B
to	O
a	O
high	O
standard	O
here	O
the	O
network	O
must	O
learn	O
to	O
take	O
a	O
board	O
position	O
as	O
input	O
along	O
with	O
the	O
result	O
of	O
a	O
dice	O
throw	O
and	O
produce	O
a	O
strong	O
move	O
as	O
the	O
output	O
this	O
is	O
done	O
by	O
having	O
the	O
network	O
play	O
against	O
a	O
copy	O
of	O
itself	O
for	O
perhaps	O
a	O
million	O
games	O
a	O
major	O
challenge	O
is	O
that	O
a	O
game	O
of	O
backgammon	B
can	O
involve	O
dozens	O
of	O
moves	O
and	O
yet	O
it	O
is	O
only	O
at	O
the	O
end	O
of	O
the	O
game	O
that	O
the	O
reward	O
in	O
the	O
form	O
of	O
victory	O
is	O
achieved	O
the	O
reward	O
must	O
then	O
be	O
attributed	O
appropriately	O
to	O
all	O
of	O
the	O
moves	O
that	O
led	O
to	O
it	O
even	O
though	O
some	O
moves	O
will	O
have	O
been	O
good	O
ones	O
and	O
others	O
less	O
so	O
this	O
is	O
an	O
example	O
of	O
a	O
credit	B
assignment	I
problem	O
a	O
general	O
feature	O
of	O
reinforcement	B
learning	B
is	O
the	O
trade-off	O
between	O
exploration	B
in	O
which	O
the	O
system	O
tries	O
out	O
new	O
kinds	O
of	O
actions	O
to	O
see	O
how	O
effective	O
they	O
are	O
and	O
exploitation	B
in	O
which	O
the	O
system	O
makes	O
use	O
of	O
actions	O
that	O
are	O
known	O
to	O
yield	O
a	O
high	O
reward	O
too	O
strong	O
a	O
focus	O
on	O
either	O
exploration	B
or	O
exploitation	B
will	O
yield	O
poor	O
results	O
reinforcement	B
learning	B
continues	O
to	O
be	O
an	O
active	O
area	O
of	O
machine	O
learning	B
research	O
however	O
a	O
introduction	O
figure	O
plot	O
of	O
a	O
training	B
data	O
set	O
of	O
n	O
points	O
shown	O
as	O
blue	O
circles	O
each	O
comprising	O
an	O
observation	O
of	O
the	O
input	O
variable	O
x	O
along	O
with	O
the	O
corresponding	O
target	O
variable	O
t	O
the	O
green	O
curve	O
shows	O
the	O
function	O
x	O
used	O
to	O
generate	O
the	O
data	O
our	O
goal	O
is	O
to	O
predict	O
the	O
value	O
of	O
t	O
for	O
some	O
new	O
value	O
of	O
x	O
without	O
knowledge	O
of	O
the	O
green	O
curve	O
t	O
x	O
detailed	O
treatment	O
lies	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
although	O
each	O
of	O
these	O
tasks	O
needs	O
its	O
own	O
tools	O
and	O
techniques	O
many	O
of	O
the	O
key	O
ideas	O
that	O
underpin	O
them	O
are	O
common	O
to	O
all	O
such	O
problems	O
one	O
of	O
the	O
main	O
goals	O
of	O
this	O
chapter	O
is	O
to	O
introduce	O
in	O
a	O
relatively	O
informal	O
way	O
several	O
of	O
the	O
most	O
important	O
of	O
these	O
concepts	O
and	O
to	O
illustrate	O
them	O
using	O
simple	O
examples	O
later	O
in	O
the	O
book	O
we	O
shall	O
see	O
these	O
same	O
ideas	O
re-emerge	O
in	O
the	O
context	O
of	O
more	O
sophisticated	O
models	O
that	O
are	O
applicable	O
to	O
real-world	O
pattern	O
recognition	O
applications	O
this	O
chapter	O
also	O
provides	O
a	O
self-contained	O
introduction	O
to	O
three	O
important	O
tools	O
that	O
will	O
be	O
used	O
throughout	O
the	O
book	O
namely	O
probability	B
theory	B
decision	B
theory	B
and	O
information	B
theory	B
although	O
these	O
might	O
sound	O
like	O
daunting	O
topics	O
they	O
are	O
in	O
fact	O
straightforward	O
and	O
a	O
clear	O
understanding	O
of	O
them	O
is	O
essential	O
if	O
machine	O
learning	B
techniques	O
are	O
to	O
be	O
used	O
to	O
best	O
effect	O
in	O
practical	O
applications	O
example	O
polynomial	B
curve	B
fitting	I
we	O
begin	O
by	O
introducing	O
a	O
simple	O
regression	B
problem	O
which	O
we	O
shall	O
use	O
as	O
a	O
running	O
example	O
throughout	O
this	O
chapter	O
to	O
motivate	O
a	O
number	O
of	O
key	O
concepts	O
suppose	O
we	O
observe	O
a	O
real-valued	O
input	O
variable	O
x	O
and	O
we	O
wish	O
to	O
use	O
this	O
observation	O
to	O
predict	O
the	O
value	O
of	O
a	O
real-valued	O
target	O
variable	O
t	O
for	O
the	O
present	O
purposes	O
it	O
is	O
instructive	O
to	O
consider	O
an	O
artificial	O
example	O
using	O
synthetically	O
generated	O
data	O
because	O
we	O
then	O
know	O
the	O
precise	O
process	O
that	O
generated	O
the	O
data	O
for	O
comparison	O
against	O
any	O
learned	O
model	O
the	O
data	O
for	O
this	O
example	O
is	O
generated	O
from	O
the	O
function	O
x	O
with	O
random	O
noise	O
included	O
in	O
the	O
target	O
values	O
as	O
described	O
in	O
detail	O
in	O
appendix	O
a	O
now	O
suppose	O
that	O
we	O
are	O
given	O
a	O
training	B
set	I
comprising	O
n	O
observations	O
of	O
x	O
written	O
x	O
xn	O
together	O
with	O
corresponding	O
observations	O
of	O
the	O
values	O
of	O
t	O
denoted	O
t	O
tnt	O
figure	O
shows	O
a	O
plot	O
of	O
a	O
training	B
set	I
comprising	O
n	O
data	O
points	O
the	O
input	O
data	O
set	O
x	O
in	O
figure	O
was	O
generated	O
by	O
choosing	O
values	O
of	O
xn	O
for	O
n	O
n	O
spaced	O
uniformly	O
in	O
range	O
and	O
the	O
target	O
data	O
set	O
t	O
was	O
obtained	O
by	O
first	O
computing	O
the	O
corresponding	O
values	O
of	O
the	O
function	O
example	O
polynomial	B
curve	B
fitting	I
x	O
and	O
then	O
adding	O
a	O
small	O
level	O
of	O
random	O
noise	O
having	O
a	O
gaussian	B
distribution	O
gaussian	B
distribution	O
is	O
discussed	O
in	O
section	O
to	O
each	O
such	O
point	O
in	O
order	O
to	O
obtain	O
the	O
corresponding	O
value	O
tn	O
by	O
generating	O
data	O
in	O
this	O
way	O
we	O
are	O
capturing	O
a	O
property	O
of	O
many	O
real	O
data	O
sets	O
namely	O
that	O
they	O
possess	O
an	O
underlying	O
regularity	O
which	O
we	O
wish	O
to	O
learn	O
but	O
that	O
individual	O
observations	O
are	O
corrupted	O
by	O
random	O
noise	O
this	O
noise	O
might	O
arise	O
from	O
intrinsically	O
stochastic	B
random	O
processes	O
such	O
as	O
radioactive	O
decay	O
but	O
more	O
typically	O
is	O
due	O
to	O
there	O
being	O
sources	O
of	O
variability	O
that	O
are	O
themselves	O
unobserved	O
of	O
the	O
target	O
variable	O
for	O
some	O
new	O
of	O
the	O
input	O
variable	O
as	O
we	O
shall	O
see	O
set	O
furthermore	O
the	O
observed	O
data	O
are	O
corrupted	O
with	O
noise	O
and	O
so	O
for	O
a	O
there	O
is	O
uncertainty	O
as	O
to	O
the	O
appropriate	O
value	O
probability	B
theory	B
discussed	O
later	O
this	O
involves	O
implicitly	O
trying	O
to	O
discover	O
the	O
underlying	O
function	O
x	O
this	O
is	O
intrinsically	O
a	O
difficult	O
problem	O
as	O
we	O
have	O
to	O
generalize	O
from	O
a	O
finite	O
data	O
our	O
goal	O
is	O
to	O
exploit	O
this	O
training	B
set	I
in	O
order	O
to	O
make	O
predictions	O
of	O
the	O
value	O
in	O
section	O
provides	O
a	O
framework	O
for	O
expressing	O
such	O
uncertainty	O
in	O
a	O
precise	O
and	O
quantitative	O
manner	O
and	O
decision	B
theory	B
discussed	O
in	O
section	O
allows	O
us	O
to	O
exploit	O
this	O
probabilistic	O
representation	O
in	O
order	O
to	O
make	O
predictions	O
that	O
are	O
optimal	O
according	O
to	O
appropriate	O
criteria	O
for	O
the	O
moment	O
however	O
we	O
shall	O
proceed	O
rather	O
informally	O
and	O
consider	O
a	O
simple	O
approach	O
based	O
on	O
curve	B
fitting	I
in	O
particular	O
we	O
shall	O
fit	O
the	O
data	O
using	O
a	O
polynomial	O
function	O
of	O
the	O
form	O
yx	O
w	O
wm	O
xm	O
wjxj	O
where	O
m	O
is	O
the	O
order	O
of	O
the	O
polynomial	O
and	O
xj	O
denotes	O
x	O
raised	O
to	O
the	O
power	O
of	O
j	O
the	O
polynomial	O
coefficients	O
wm	O
are	O
collectively	O
denoted	O
by	O
the	O
vector	O
w	O
note	O
that	O
although	O
the	O
polynomial	O
function	O
yx	O
w	O
is	O
a	O
nonlinear	O
function	O
of	O
x	O
it	O
is	O
a	O
linear	O
function	O
of	O
the	O
coefficients	O
w	O
functions	O
such	O
as	O
the	O
polynomial	O
which	O
are	O
linear	O
in	O
the	O
unknown	O
parameters	O
have	O
important	O
properties	O
and	O
are	O
called	O
linear	O
models	O
and	O
will	O
be	O
discussed	O
extensively	O
in	O
chapters	O
and	O
the	O
values	O
of	O
the	O
coefficients	O
will	O
be	O
determined	O
by	O
fitting	O
the	O
polynomial	O
to	O
the	O
training	B
data	O
this	O
can	O
be	O
done	O
by	O
minimizing	O
an	O
error	B
function	I
that	O
measures	O
the	O
misfit	O
between	O
the	O
function	O
yx	O
w	O
for	O
any	O
given	O
value	O
of	O
w	O
and	O
the	O
training	B
set	I
data	O
points	O
one	O
simple	O
choice	O
of	O
error	B
function	I
which	O
is	O
widely	O
used	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
errors	O
between	O
the	O
predictions	O
yxn	O
w	O
for	O
each	O
data	O
point	O
xn	O
and	O
the	O
corresponding	O
target	O
values	O
tn	O
so	O
that	O
we	O
minimize	O
ew	O
w	O
where	O
the	O
factor	O
of	O
is	O
included	O
for	O
later	O
convenience	O
we	O
shall	O
discuss	O
the	O
motivation	O
for	O
this	O
choice	O
of	O
error	B
function	I
later	O
in	O
this	O
chapter	O
for	O
the	O
moment	O
we	O
simply	O
note	O
that	O
it	O
is	O
a	O
nonnegative	O
quantity	O
that	O
would	O
be	O
zero	O
if	O
and	O
only	O
if	O
the	O
introduction	O
figure	O
the	O
error	B
function	I
corresponds	O
to	O
half	O
of	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
displacements	O
by	O
the	O
vertical	O
green	O
bars	O
of	O
each	O
data	O
point	O
from	O
the	O
function	O
yx	O
w	O
t	O
tn	O
yxn	O
w	O
xn	O
x	O
exercise	O
function	O
yx	O
w	O
were	O
to	O
pass	O
exactly	O
through	O
each	O
training	B
data	O
point	O
the	O
geometrical	O
interpretation	O
of	O
the	O
sum-of-squares	B
error	B
function	I
is	O
illustrated	O
in	O
figure	O
we	O
can	O
solve	O
the	O
curve	B
fitting	I
problem	O
by	O
choosing	O
the	O
value	O
of	O
w	O
for	O
which	O
ew	O
is	O
as	O
small	O
as	O
possible	O
because	O
the	O
error	B
function	I
is	O
a	O
quadratic	O
function	O
of	O
the	O
coefficients	O
w	O
its	O
derivatives	O
with	O
respect	O
to	O
the	O
coefficients	O
will	O
be	O
linear	O
in	O
the	O
elements	O
of	O
w	O
and	O
so	O
the	O
minimization	O
of	O
the	O
error	B
function	I
has	O
a	O
unique	O
solution	O
denoted	O
by	O
which	O
can	O
be	O
found	O
in	O
closed	O
form	O
the	O
resulting	O
polynomial	O
is	O
given	O
by	O
the	O
function	O
yx	O
there	O
remains	O
the	O
problem	O
of	O
choosing	O
the	O
order	O
m	O
of	O
the	O
polynomial	O
and	O
as	O
we	O
shall	O
see	O
this	O
will	O
turn	O
out	O
to	O
be	O
an	O
example	O
of	O
an	O
important	O
concept	O
called	O
model	B
comparison	I
or	O
model	B
selection	I
in	O
figure	O
we	O
show	O
four	O
examples	O
of	O
the	O
results	O
of	O
fitting	O
polynomials	O
having	O
orders	O
m	O
and	O
to	O
the	O
data	O
set	O
shown	O
in	O
figure	O
we	O
notice	O
that	O
the	O
constant	O
and	O
first	O
order	O
polynomials	O
give	O
rather	O
poor	O
fits	O
to	O
the	O
data	O
and	O
consequently	O
rather	O
poor	O
representations	O
of	O
the	O
function	O
x	O
the	O
third	O
order	O
polynomial	O
seems	O
to	O
give	O
the	O
best	O
fit	O
to	O
the	O
function	O
x	O
of	O
the	O
examples	O
shown	O
in	O
figure	O
when	O
we	O
go	O
to	O
a	O
much	O
higher	O
order	O
polynomial	O
we	O
obtain	O
an	O
excellent	O
fit	O
to	O
the	O
training	B
data	O
in	O
fact	O
the	O
polynomial	O
passes	O
exactly	O
through	O
each	O
data	O
point	O
and	O
however	O
the	O
fitted	O
curve	O
oscillates	O
wildly	O
and	O
gives	O
a	O
very	O
poor	O
representation	O
of	O
the	O
function	O
x	O
this	O
latter	O
behaviour	O
is	O
known	O
as	O
over-fitting	B
as	O
we	O
have	O
noted	O
earlier	O
the	O
goal	O
is	O
to	O
achieve	O
good	O
generalization	B
by	O
making	O
accurate	O
predictions	O
for	O
new	O
data	O
we	O
can	O
obtain	O
some	O
quantitative	O
insight	O
into	O
the	O
dependence	O
of	O
the	O
generalization	B
performance	O
on	O
m	O
by	O
considering	O
a	O
separate	O
test	B
set	I
comprising	O
data	O
points	O
generated	O
using	O
exactly	O
the	O
same	O
procedure	O
used	O
to	O
generate	O
the	O
training	B
set	I
points	O
but	O
with	O
new	O
choices	O
for	O
the	O
random	O
noise	O
values	O
included	O
in	O
the	O
target	O
values	O
for	O
each	O
choice	O
of	O
m	O
we	O
can	O
then	O
evaluate	O
the	O
residual	O
value	O
of	O
given	O
by	O
for	O
the	O
training	B
data	O
and	O
we	O
can	O
also	O
evaluate	O
for	O
the	O
test	O
data	O
set	O
it	O
is	O
sometimes	O
more	O
convenient	O
to	O
use	O
the	O
root-mean-square	O
t	O
t	O
example	O
polynomial	B
curve	B
fitting	I
m	O
x	O
m	O
m	O
t	O
x	O
m	O
t	O
x	O
x	O
figure	O
plots	O
of	O
polynomials	O
having	O
various	O
orders	O
m	O
shown	O
as	O
red	O
curves	O
fitted	O
to	O
the	O
data	O
set	O
shown	O
in	O
figure	O
error	B
defined	O
by	O
erms	O
in	O
which	O
the	O
division	O
by	O
n	O
allows	O
us	O
to	O
compare	O
different	O
sizes	O
of	O
data	O
sets	O
on	O
an	O
equal	O
footing	O
and	O
the	O
square	O
root	O
ensures	O
that	O
erms	O
is	O
measured	O
on	O
the	O
same	O
scale	O
in	O
the	O
same	O
units	O
as	O
the	O
target	O
variable	O
t	O
graphs	O
of	O
the	O
training	B
and	O
test	B
set	I
rms	O
errors	O
are	O
shown	O
for	O
various	O
values	O
of	O
m	O
in	O
figure	O
the	O
test	B
set	I
error	B
is	O
a	O
measure	O
of	O
how	O
well	O
we	O
are	O
doing	O
in	O
predicting	O
the	O
values	O
of	O
t	O
for	O
new	O
data	O
observations	O
of	O
x	O
we	O
note	O
from	O
figure	O
that	O
small	O
values	O
of	O
m	O
give	O
relatively	O
large	O
values	O
of	O
the	O
test	B
set	I
error	B
and	O
this	O
can	O
be	O
attributed	O
to	O
the	O
fact	O
that	O
the	O
corresponding	O
polynomials	O
are	O
rather	O
inflexible	O
and	O
are	O
incapable	O
of	O
capturing	O
the	O
oscillations	O
in	O
the	O
function	O
x	O
values	O
of	O
m	O
in	O
the	O
range	O
m	O
give	O
small	O
values	O
for	O
the	O
test	B
set	I
error	B
and	O
these	O
also	O
give	O
reasonable	O
representations	O
of	O
the	O
generating	O
function	O
x	O
as	O
can	O
be	O
seen	O
for	O
the	O
case	O
of	O
m	O
from	O
figure	O
introduction	O
figure	O
graphs	O
of	O
the	O
root-mean-square	B
error	B
defined	O
by	O
evaluated	O
on	O
the	O
training	B
set	I
and	O
on	O
an	O
independent	B
test	B
set	I
for	O
various	O
values	O
of	O
m	O
training	B
test	O
s	O
m	O
r	O
e	O
m	O
for	O
m	O
the	O
training	B
set	I
error	B
goes	O
to	O
zero	O
as	O
we	O
might	O
expect	O
because	O
this	O
polynomial	O
contains	O
degrees	B
of	I
freedom	I
corresponding	O
to	O
the	O
coefficients	O
and	O
so	O
can	O
be	O
tuned	O
exactly	O
to	O
the	O
data	O
points	O
in	O
the	O
training	B
set	I
however	O
the	O
test	B
set	I
error	B
has	O
become	O
very	O
large	O
and	O
as	O
we	O
saw	O
in	O
figure	O
the	O
corresponding	O
function	O
yx	O
exhibits	O
wild	O
oscillations	O
this	O
may	O
seem	O
paradoxical	O
because	O
a	O
polynomial	O
of	O
given	O
order	O
contains	O
all	O
lower	O
order	O
polynomials	O
as	O
special	O
cases	O
the	O
m	O
polynomial	O
is	O
therefore	O
capable	O
of	O
generating	O
results	O
at	O
least	O
as	O
good	O
as	O
the	O
m	O
polynomial	O
furthermore	O
we	O
might	O
suppose	O
that	O
the	O
best	O
predictor	O
of	O
new	O
data	O
would	O
be	O
the	O
function	O
x	O
from	O
which	O
the	O
data	O
was	O
generated	O
we	O
shall	O
see	O
later	O
that	O
this	O
is	O
indeed	O
the	O
case	O
we	O
know	O
that	O
a	O
power	O
series	O
expansion	O
of	O
the	O
function	O
x	O
contains	O
terms	O
of	O
all	O
orders	O
so	O
we	O
might	O
expect	O
that	O
results	O
should	O
improve	O
monotonically	O
as	O
we	O
increase	O
m	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
problem	O
by	O
examining	O
the	O
values	O
of	O
the	O
coefficients	O
obtained	O
from	O
polynomials	O
of	O
various	O
order	O
as	O
shown	O
in	O
table	O
we	O
see	O
that	O
as	O
m	O
increases	O
the	O
magnitude	O
of	O
the	O
coefficients	O
typically	O
gets	O
larger	O
in	O
particular	O
for	O
the	O
m	O
polynomial	O
the	O
coefficients	O
have	O
become	O
finely	O
tuned	O
to	O
the	O
data	O
by	O
developing	O
large	O
positive	O
and	O
negative	O
values	O
so	O
that	O
the	O
correspond	O
table	O
table	O
of	O
the	O
coefficients	O
for	O
polynomials	O
of	O
various	O
order	O
observe	O
how	O
the	O
typical	O
magnitude	O
of	O
the	O
coefficients	O
increases	O
dramatically	O
as	O
the	O
order	O
of	O
the	O
polynomial	O
increases	O
m	O
m	O
m	O
m	O
example	O
polynomial	B
curve	B
fitting	I
t	O
n	O
t	O
n	O
x	O
x	O
figure	O
plots	O
of	O
the	O
solutions	O
obtained	O
by	O
minimizing	O
the	O
sum-of-squares	B
error	B
function	I
using	O
the	O
m	O
polynomial	O
for	O
n	O
data	O
points	O
plot	O
and	O
n	O
data	O
points	O
plot	O
we	O
see	O
that	O
increasing	O
the	O
size	O
of	O
the	O
data	O
set	O
reduces	O
the	O
over-fitting	B
problem	O
ing	O
polynomial	O
function	O
matches	O
each	O
of	O
the	O
data	O
points	O
exactly	O
but	O
between	O
data	O
points	O
near	O
the	O
ends	O
of	O
the	O
range	O
the	O
function	O
exhibits	O
the	O
large	O
oscillations	O
observed	O
in	O
figure	O
intuitively	O
what	O
is	O
happening	O
is	O
that	O
the	O
more	O
flexible	O
polynomials	O
with	O
larger	O
values	O
of	O
m	O
are	O
becoming	O
increasingly	O
tuned	O
to	O
the	O
random	O
noise	O
on	O
the	O
target	O
values	O
it	O
is	O
also	O
interesting	O
to	O
examine	O
the	O
behaviour	O
of	O
a	O
given	O
model	O
as	O
the	O
size	O
of	O
the	O
data	O
set	O
is	O
varied	O
as	O
shown	O
in	O
figure	O
we	O
see	O
that	O
for	O
a	O
given	O
model	O
complexity	O
the	O
over-fitting	B
problem	O
become	O
less	O
severe	O
as	O
the	O
size	O
of	O
the	O
data	O
set	O
increases	O
another	O
way	O
to	O
say	O
this	O
is	O
that	O
the	O
larger	O
the	O
data	O
set	O
the	O
more	O
complex	O
other	O
words	O
more	O
flexible	O
the	O
model	O
that	O
we	O
can	O
afford	O
to	O
fit	O
to	O
the	O
data	O
one	O
rough	O
heuristic	O
that	O
is	O
sometimes	O
advocated	O
is	O
that	O
the	O
number	O
of	O
data	O
points	O
should	O
be	O
no	O
less	O
than	O
some	O
multiple	O
or	O
of	O
the	O
number	O
of	O
adaptive	O
parameters	O
in	O
the	O
model	O
however	O
as	O
we	O
shall	O
see	O
in	O
chapter	O
the	O
number	O
of	O
parameters	O
is	O
not	O
necessarily	O
the	O
most	O
appropriate	O
measure	O
of	O
model	O
complexity	O
also	O
there	O
is	O
something	O
rather	O
unsatisfying	O
about	O
having	O
to	O
limit	O
the	O
number	O
of	O
parameters	O
in	O
a	O
model	O
according	O
to	O
the	O
size	O
of	O
the	O
available	O
training	B
set	I
it	O
would	O
seem	O
more	O
reasonable	O
to	O
choose	O
the	O
complexity	O
of	O
the	O
model	O
according	O
to	O
the	O
complexity	O
of	O
the	O
problem	O
being	O
solved	O
we	O
shall	O
see	O
that	O
the	O
least	O
squares	O
approach	O
to	O
finding	O
the	O
model	O
parameters	O
represents	O
a	O
specific	O
case	O
of	O
maximum	B
likelihood	I
in	O
section	O
and	O
that	O
the	O
over-fitting	B
problem	O
can	O
be	O
understood	O
as	O
a	O
general	O
property	O
of	O
maximum	B
likelihood	I
by	O
adopting	O
a	O
bayesian	B
approach	O
the	O
over-fitting	B
problem	O
can	O
be	O
avoided	O
we	O
shall	O
see	O
that	O
there	O
is	O
no	O
difficulty	O
from	O
a	O
bayesian	B
perspective	O
in	O
employing	O
models	O
for	O
which	O
the	O
number	O
of	O
parameters	O
greatly	O
exceeds	O
the	O
number	O
of	O
data	O
points	O
indeed	O
in	O
a	O
bayesian	B
model	O
the	O
effective	B
number	I
of	I
parameters	I
adapts	O
automatically	O
to	O
the	O
size	O
of	O
the	O
data	O
set	O
for	O
the	O
moment	O
however	O
it	O
is	O
instructive	O
to	O
continue	O
with	O
the	O
current	O
approach	O
and	O
to	O
consider	O
how	O
in	O
practice	O
we	O
can	O
apply	O
it	O
to	O
data	O
sets	O
of	O
limited	O
size	O
where	O
we	O
section	O
introduction	O
t	O
ln	O
t	O
ln	O
x	O
x	O
figure	O
plots	O
of	O
m	O
polynomials	O
fitted	O
to	O
the	O
data	O
set	O
shown	O
in	O
figure	O
using	O
the	O
regularized	O
error	B
function	I
for	O
two	O
values	O
of	O
the	O
regularization	B
parameter	O
corresponding	O
to	O
ln	O
and	O
ln	O
the	O
case	O
of	O
no	O
regularizer	O
i	O
e	O
corresponding	O
to	O
ln	O
is	O
shown	O
at	O
the	O
bottom	O
right	O
of	O
figure	O
may	O
wish	O
to	O
use	O
relatively	O
complex	O
and	O
flexible	O
models	O
one	O
technique	O
that	O
is	O
often	O
used	O
to	O
control	O
the	O
over-fitting	B
phenomenon	O
in	O
such	O
cases	O
is	O
that	O
of	O
regularization	B
which	O
involves	O
adding	O
a	O
penalty	O
term	O
to	O
the	O
error	B
function	I
in	O
order	O
to	O
discourage	O
the	O
coefficients	O
from	O
reaching	O
large	O
values	O
the	O
simplest	O
such	O
penalty	O
term	O
takes	O
the	O
form	O
of	O
a	O
sum	O
of	O
squares	O
of	O
all	O
of	O
the	O
coefficients	O
leading	O
to	O
a	O
modified	O
error	B
function	I
of	O
the	O
form	O
w	O
where	O
wtw	O
m	O
and	O
the	O
coefficient	O
governs	O
the	O
relative	B
importance	O
of	O
the	O
regularization	B
term	O
compared	O
with	O
the	O
sum-of-squares	B
error	B
term	O
note	O
that	O
often	O
the	O
coefficient	O
is	O
omitted	O
from	O
the	O
regularizer	O
because	O
its	O
inclusion	O
causes	O
the	O
results	O
to	O
depend	O
on	O
the	O
choice	O
of	O
origin	O
for	O
the	O
target	O
variable	O
et	O
al	O
or	O
it	O
may	O
be	O
included	O
but	O
with	O
its	O
own	O
regularization	B
coefficient	O
shall	O
discuss	O
this	O
topic	O
in	O
more	O
detail	O
in	O
section	O
again	O
the	O
error	B
function	I
in	O
can	O
be	O
minimized	O
exactly	O
in	O
closed	O
form	O
techniques	O
such	O
as	O
this	O
are	O
known	O
in	O
the	O
statistics	O
literature	O
as	O
shrinkage	B
methods	O
because	O
they	O
reduce	O
the	O
value	O
of	O
the	O
coefficients	O
the	O
particular	O
case	O
of	O
a	O
quadratic	O
regularizer	O
is	O
called	O
ridge	B
regression	B
and	O
kennard	O
in	O
the	O
context	O
of	O
neural	O
networks	O
this	O
approach	O
is	O
known	O
as	O
weight	B
decay	I
figure	O
shows	O
the	O
results	O
of	O
fitting	O
the	O
polynomial	O
of	O
order	O
m	O
to	O
the	O
same	O
data	O
set	O
as	O
before	O
but	O
now	O
using	O
the	O
regularized	O
error	B
function	I
given	O
by	O
we	O
see	O
that	O
for	O
a	O
value	O
of	O
ln	O
the	O
over-fitting	B
has	O
been	O
suppressed	O
and	O
we	O
now	O
obtain	O
a	O
much	O
closer	O
representation	O
of	O
the	O
underlying	O
function	O
x	O
if	O
however	O
we	O
use	O
too	O
large	O
a	O
value	O
for	O
then	O
we	O
again	O
obtain	O
a	O
poor	O
fit	O
as	O
shown	O
in	O
figure	O
for	O
ln	O
the	O
corresponding	O
coefficients	O
from	O
the	O
fitted	O
polynomials	O
are	O
given	O
in	O
table	O
showing	O
that	O
regularization	B
has	O
the	O
desired	O
effect	O
of	O
reducing	O
exercise	O
example	O
polynomial	B
curve	B
fitting	I
table	O
table	O
of	O
the	O
coefficients	O
for	O
m	O
polynomials	O
with	O
various	O
values	O
for	O
the	O
regularization	B
parameter	O
note	O
that	O
ln	O
corresponds	O
to	O
a	O
model	O
with	O
no	O
regularization	B
i	O
e	O
to	O
the	O
graph	O
at	O
the	O
bottom	O
right	O
in	O
figure	O
we	O
see	O
that	O
as	O
the	O
value	O
of	O
increases	O
the	O
typical	O
magnitude	O
of	O
the	O
coefficients	O
gets	O
smaller	O
ln	O
ln	O
ln	O
the	O
magnitude	O
of	O
the	O
coefficients	O
the	O
impact	O
of	O
the	O
regularization	B
term	O
on	O
the	O
generalization	B
error	B
can	O
be	O
seen	O
by	O
plotting	O
the	O
value	O
of	O
the	O
rms	O
error	B
for	O
both	O
training	B
and	O
test	O
sets	O
against	O
ln	O
as	O
shown	O
in	O
figure	O
we	O
see	O
that	O
in	O
effect	O
now	O
controls	O
the	O
effective	O
complexity	O
of	O
the	O
model	O
and	O
hence	O
determines	O
the	O
degree	O
of	O
over-fitting	B
the	O
issue	O
of	O
model	O
complexity	O
is	O
an	O
important	O
one	O
and	O
will	O
be	O
discussed	O
at	O
length	O
in	O
section	O
here	O
we	O
simply	O
note	O
that	O
if	O
we	O
were	O
trying	O
to	O
solve	O
a	O
practical	O
application	O
using	O
this	O
approach	O
of	O
minimizing	O
an	O
error	B
function	I
we	O
would	O
have	O
to	O
find	O
a	O
way	O
to	O
determine	O
a	O
suitable	O
value	O
for	O
the	O
model	O
complexity	O
the	O
results	O
above	O
suggest	O
a	O
simple	O
way	O
of	O
achieving	O
this	O
namely	O
by	O
taking	O
the	O
available	O
data	O
and	O
partitioning	O
it	O
into	O
a	O
training	B
set	I
used	O
to	O
determine	O
the	O
coefficients	O
w	O
and	O
a	O
separate	O
validation	B
set	I
also	O
called	O
a	O
hold-out	B
set	I
used	O
to	O
optimize	O
the	O
model	O
complexity	O
m	O
or	O
in	O
many	O
cases	O
however	O
this	O
will	O
prove	O
to	O
be	O
too	O
wasteful	O
of	O
valuable	O
training	B
data	O
and	O
we	O
have	O
to	O
seek	O
more	O
sophisticated	O
approaches	O
so	O
far	O
our	O
discussion	O
of	O
polynomial	B
curve	B
fitting	I
has	O
appealed	O
largely	O
to	O
intuition	O
we	O
now	O
seek	O
a	O
more	O
principled	O
approach	O
to	O
solving	O
problems	O
in	O
pattern	O
recognition	O
by	O
turning	O
to	O
a	O
discussion	O
of	O
probability	B
theory	B
as	O
well	O
as	O
providing	O
the	O
foundation	O
for	O
nearly	O
all	O
of	O
the	O
subsequent	O
developments	O
in	O
this	O
book	O
it	O
will	O
also	O
section	O
figure	O
graph	O
of	O
the	O
root-mean-square	B
error	B
versus	O
ln	O
for	O
the	O
m	O
polynomial	O
training	B
test	O
s	O
m	O
r	O
e	O
ln	O
introduction	O
give	O
us	O
some	O
important	O
insights	O
into	O
the	O
concepts	O
we	O
have	O
introduced	O
in	O
the	O
context	O
of	O
polynomial	B
curve	B
fitting	I
and	O
will	O
allow	O
us	O
to	O
extend	O
these	O
to	O
more	O
complex	O
situations	O
probability	B
theory	B
a	O
key	O
concept	O
in	O
the	O
field	O
of	O
pattern	O
recognition	O
is	O
that	O
of	O
uncertainty	O
it	O
arises	O
both	O
through	O
noise	O
on	O
measurements	O
as	O
well	O
as	O
through	O
the	O
finite	O
size	O
of	O
data	O
sets	O
probability	B
theory	B
provides	O
a	O
consistent	B
framework	O
for	O
the	O
quantification	O
and	O
manipulation	O
of	O
uncertainty	O
and	O
forms	O
one	O
of	O
the	O
central	O
foundations	O
for	O
pattern	O
recognition	O
when	O
combined	O
with	O
decision	B
theory	B
discussed	O
in	O
section	O
it	O
allows	O
us	O
to	O
make	O
optimal	O
predictions	O
given	O
all	O
the	O
information	O
available	O
to	O
us	O
even	O
though	O
that	O
information	O
may	O
be	O
incomplete	O
or	O
ambiguous	O
we	O
will	O
introduce	O
the	O
basic	O
concepts	O
of	O
probability	B
theory	B
by	O
considering	O
a	O
simple	O
example	O
imagine	O
we	O
have	O
two	O
boxes	O
one	O
red	O
and	O
one	O
blue	O
and	O
in	O
the	O
red	O
box	O
we	O
have	O
apples	O
and	O
oranges	O
and	O
in	O
the	O
blue	O
box	O
we	O
have	O
apples	O
and	O
orange	O
this	O
is	O
illustrated	O
in	O
figure	O
now	O
suppose	O
we	O
randomly	O
pick	O
one	O
of	O
the	O
boxes	O
and	O
from	O
that	O
box	O
we	O
randomly	O
select	O
an	O
item	O
of	O
fruit	O
and	O
having	O
observed	O
which	O
sort	O
of	O
fruit	O
it	O
is	O
we	O
replace	O
it	O
in	O
the	O
box	O
from	O
which	O
it	O
came	O
we	O
could	O
imagine	O
repeating	O
this	O
process	O
many	O
times	O
let	O
us	O
suppose	O
that	O
in	O
so	O
doing	O
we	O
pick	O
the	O
red	O
box	O
of	O
the	O
time	O
and	O
we	O
pick	O
the	O
blue	O
box	O
of	O
the	O
time	O
and	O
that	O
when	O
we	O
remove	O
an	O
item	O
of	O
fruit	O
from	O
a	O
box	O
we	O
are	O
equally	O
likely	O
to	O
select	O
any	O
of	O
the	O
pieces	O
of	O
fruit	O
in	O
the	O
box	O
in	O
this	O
example	O
the	O
identity	O
of	O
the	O
box	O
that	O
will	O
be	O
chosen	O
is	O
a	O
random	O
variable	O
which	O
we	O
shall	O
denote	O
by	O
b	O
this	O
random	O
variable	O
can	O
take	O
one	O
of	O
two	O
possible	O
values	O
namely	O
r	O
to	O
the	O
red	O
box	O
or	O
b	O
to	O
the	O
blue	O
box	O
similarly	O
the	O
identity	O
of	O
the	O
fruit	O
is	O
also	O
a	O
random	O
variable	O
and	O
will	O
be	O
denoted	O
by	O
f	O
it	O
can	O
take	O
either	O
of	O
the	O
values	O
a	O
apple	O
or	O
o	O
orange	O
to	O
begin	O
with	O
we	O
shall	O
define	O
the	O
probability	B
of	O
an	O
event	O
to	O
be	O
the	O
fraction	O
of	O
times	O
that	O
event	O
occurs	O
out	O
of	O
the	O
total	O
number	O
of	O
trials	O
in	O
the	O
limit	O
that	O
the	O
total	O
number	O
of	O
trials	O
goes	O
to	O
infinity	O
thus	O
the	O
probability	B
of	O
selecting	O
the	O
red	O
box	O
is	O
figure	O
we	O
use	O
a	O
simple	O
example	O
of	O
two	O
coloured	O
boxes	O
each	O
containing	O
fruit	O
shown	O
in	O
green	O
and	O
oranges	O
shown	O
in	O
orange	O
to	O
introduce	O
the	O
basic	O
ideas	O
of	O
probability	B
probability	B
theory	B
figure	O
we	O
can	O
derive	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
by	O
considering	O
two	O
random	O
variables	O
x	O
which	O
takes	O
the	O
values	O
where	O
i	O
m	O
and	O
y	O
which	O
takes	O
the	O
values	O
where	O
j	O
l	O
in	O
this	O
illustration	O
we	O
have	O
m	O
and	O
l	O
if	O
we	O
consider	O
a	O
total	O
number	O
n	O
of	O
instances	O
of	O
these	O
variables	O
then	O
we	O
denote	O
the	O
number	O
of	O
instances	O
where	O
x	O
xi	O
and	O
y	O
yj	O
by	O
nij	O
which	O
is	O
the	O
number	O
of	O
points	O
in	O
the	O
corresponding	O
cell	O
of	O
the	O
array	O
the	O
number	O
of	O
points	O
in	O
column	O
i	O
corresponding	O
to	O
x	O
xi	O
is	O
denoted	O
by	O
ci	O
and	O
the	O
number	O
of	O
points	O
in	O
row	O
j	O
corresponding	O
to	O
y	O
yj	O
is	O
denoted	O
by	O
rj	O
yj	O
ci	O
nij	O
xi	O
rj	O
and	O
the	O
probability	B
of	O
selecting	O
the	O
blue	O
box	O
is	O
we	O
write	O
these	O
probabilities	O
as	O
pb	O
r	O
and	O
pb	O
b	O
note	O
that	O
by	O
definition	O
probabilities	O
must	O
lie	O
in	O
the	O
interval	O
also	O
if	O
the	O
events	O
are	O
mutually	O
exclusive	O
and	O
if	O
they	O
include	O
all	O
possible	O
outcomes	O
instance	O
in	O
this	O
example	O
the	O
box	O
must	O
be	O
either	O
red	O
or	O
blue	O
then	O
we	O
see	O
that	O
the	O
probabilities	O
for	O
those	O
events	O
must	O
sum	O
to	O
one	O
we	O
can	O
now	O
ask	O
questions	O
such	O
as	O
what	O
is	O
the	O
overall	O
probability	B
that	O
the	O
selection	O
procedure	O
will	O
pick	O
an	O
apple	O
or	O
given	O
that	O
we	O
have	O
chosen	O
an	O
orange	O
what	O
is	O
the	O
probability	B
that	O
the	O
box	O
we	O
chose	O
was	O
the	O
blue	O
one	O
we	O
can	O
answer	O
questions	O
such	O
as	O
these	O
and	O
indeed	O
much	O
more	O
complex	O
questions	O
associated	O
with	O
problems	O
in	O
pattern	O
recognition	O
once	O
we	O
have	O
equipped	O
ourselves	O
with	O
the	O
two	O
elementary	O
rules	O
of	O
probability	B
known	O
as	O
the	O
sum	B
rule	I
and	O
the	O
product	B
rule	I
having	O
obtained	O
these	O
rules	O
we	O
shall	O
then	O
return	O
to	O
our	O
boxes	O
of	O
fruit	O
example	O
in	O
order	O
to	O
derive	O
the	O
rules	O
of	O
probability	B
consider	O
the	O
slightly	O
more	O
general	O
example	O
shown	O
in	O
figure	O
involving	O
two	O
random	O
variables	O
x	O
and	O
y	O
could	O
for	O
instance	O
be	O
the	O
box	O
and	O
fruit	O
variables	O
considered	O
above	O
we	O
shall	O
suppose	O
that	O
x	O
can	O
take	O
any	O
of	O
the	O
values	O
xi	O
where	O
i	O
m	O
and	O
y	O
can	O
take	O
the	O
values	O
yj	O
where	O
j	O
l	O
consider	O
a	O
total	O
of	O
n	O
trials	O
in	O
which	O
we	O
sample	O
both	O
of	O
the	O
variables	O
x	O
and	O
y	O
and	O
let	O
the	O
number	O
of	O
such	O
trials	O
in	O
which	O
x	O
xi	O
and	O
y	O
yj	O
be	O
nij	O
also	O
let	O
the	O
number	O
of	O
trials	O
in	O
which	O
x	O
takes	O
the	O
value	O
xi	O
of	O
the	O
value	O
that	O
y	O
takes	O
be	O
denoted	O
by	O
ci	O
and	O
similarly	O
let	O
the	O
number	O
of	O
trials	O
in	O
which	O
y	O
takes	O
the	O
value	O
yj	O
be	O
denoted	O
by	O
rj	O
the	O
probability	B
that	O
x	O
will	O
take	O
the	O
value	O
xi	O
and	O
y	O
will	O
take	O
the	O
value	O
yj	O
is	O
written	O
px	O
xi	O
y	O
yj	O
and	O
is	O
called	O
the	O
joint	O
probability	B
of	O
x	O
xi	O
and	O
y	O
yj	O
it	O
is	O
given	O
by	O
the	O
number	O
of	O
points	O
falling	O
in	O
the	O
cell	O
ij	O
as	O
a	O
fraction	O
of	O
the	O
total	O
number	O
of	O
points	O
and	O
hence	O
px	O
xi	O
y	O
yj	O
nij	O
n	O
here	O
we	O
are	O
implicitly	O
considering	O
the	O
limit	O
n	O
similarly	O
the	O
probability	B
that	O
x	O
takes	O
the	O
value	O
xi	O
irrespective	O
of	O
the	O
value	O
of	O
y	O
is	O
written	O
as	O
px	O
xi	O
and	O
is	O
given	O
by	O
the	O
fraction	O
of	O
the	O
total	O
number	O
of	O
points	O
that	O
fall	O
in	O
column	O
i	O
so	O
that	O
px	O
xi	O
ci	O
n	O
because	O
the	O
number	O
of	O
instances	O
in	O
column	O
i	O
in	O
figure	O
is	O
just	O
the	O
sum	O
of	O
the	O
number	O
of	O
instances	O
in	O
each	O
cell	O
of	O
that	O
column	O
we	O
have	O
ci	O
j	O
nij	O
and	O
therefore	O
introduction	O
from	O
and	O
we	O
have	O
px	O
xi	O
px	O
xi	O
y	O
yj	O
which	O
is	O
the	O
sum	B
rule	I
of	I
probability	B
note	O
that	O
px	O
xi	O
is	O
sometimes	O
called	O
the	O
marginal	B
probability	B
because	O
it	O
is	O
obtained	O
by	O
marginalizing	O
or	O
summing	O
out	O
the	O
other	O
variables	O
this	O
case	O
y	O
if	O
we	O
consider	O
only	O
those	O
instances	O
for	O
which	O
x	O
xi	O
then	O
the	O
fraction	O
of	O
such	O
instances	O
for	O
which	O
y	O
yj	O
is	O
written	O
py	O
yjx	O
xi	O
and	O
is	O
called	O
the	O
conditional	B
probability	B
of	O
y	O
yj	O
given	O
x	O
xi	O
it	O
is	O
obtained	O
by	O
finding	O
the	O
fraction	O
of	O
those	O
points	O
in	O
column	O
i	O
that	O
fall	O
in	O
cell	O
ij	O
and	O
hence	O
is	O
given	O
by	O
py	O
yjx	O
xi	O
nij	O
ci	O
from	O
and	O
we	O
can	O
then	O
derive	O
the	O
following	O
relationship	O
px	O
xi	O
y	O
yj	O
nij	O
n	O
nij	O
ci	O
ci	O
n	O
py	O
yjx	O
xipx	O
xi	O
which	O
is	O
the	O
product	B
rule	I
of	I
probability	B
so	O
far	O
we	O
have	O
been	O
quite	O
careful	O
to	O
make	O
a	O
distinction	O
between	O
a	O
random	O
variable	O
such	O
as	O
the	O
box	O
b	O
in	O
the	O
fruit	O
example	O
and	O
the	O
values	O
that	O
the	O
random	O
variable	O
can	O
take	O
for	O
example	O
r	O
if	O
the	O
box	O
were	O
the	O
red	O
one	O
thus	O
the	O
probability	B
that	O
b	O
takes	O
the	O
value	O
r	O
is	O
denoted	O
pb	O
r	O
although	O
this	O
helps	O
to	O
avoid	O
ambiguity	O
it	O
leads	O
to	O
a	O
rather	O
cumbersome	O
notation	O
and	O
in	O
many	O
cases	O
there	O
will	O
be	O
no	O
need	O
for	O
such	O
pedantry	O
instead	O
we	O
may	O
simply	O
write	O
pb	O
to	O
denote	O
a	O
distribution	O
over	O
the	O
random	O
variable	O
b	O
or	O
pr	O
to	O
denote	O
the	O
distribution	O
evaluated	O
for	O
the	O
particular	O
value	O
r	O
provided	O
that	O
the	O
interpretation	O
is	O
clear	O
from	O
the	O
context	O
with	O
this	O
more	O
compact	O
notation	O
we	O
can	O
write	O
the	O
two	O
fundamental	O
rules	O
of	O
probability	B
theory	B
in	O
the	O
following	O
form	O
the	O
rules	O
of	O
probability	B
sum	B
rule	I
px	O
y	O
px	O
y	O
product	B
rule	I
px	O
y	O
py	O
here	O
px	O
y	O
is	O
a	O
joint	O
probability	B
and	O
is	O
verbalized	O
as	O
the	O
probability	B
of	O
x	O
and	O
y	O
similarly	O
the	O
quantity	O
py	O
is	O
a	O
conditional	B
probability	B
and	O
is	O
verbalized	O
as	O
the	O
probability	B
of	O
y	O
given	O
x	O
whereas	O
the	O
quantity	O
px	O
is	O
a	O
marginal	B
probability	B
probability	B
theory	B
and	O
is	O
simply	O
the	O
probability	B
of	O
x	O
these	O
two	O
simple	O
rules	O
form	O
the	O
basis	O
for	O
all	O
of	O
the	O
probabilistic	O
machinery	O
that	O
we	O
use	O
throughout	O
this	O
book	O
from	O
the	O
product	B
rule	I
together	O
with	O
the	O
symmetry	O
property	O
px	O
y	O
py	O
x	O
we	O
immediately	O
obtain	O
the	O
following	O
relationship	O
between	O
conditional	B
probabilities	O
py	O
pxy	O
px	O
which	O
is	O
called	O
bayes	B
theorem	O
and	O
which	O
plays	O
a	O
central	O
role	O
in	O
pattern	O
recognition	O
and	O
machine	O
learning	B
using	O
the	O
sum	B
rule	I
the	O
denominator	O
in	O
bayes	B
theorem	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
quantities	O
appearing	O
in	O
the	O
numerator	O
px	O
pxy	O
y	O
we	O
can	O
view	O
the	O
denominator	O
in	O
bayes	B
theorem	O
as	O
being	O
the	O
normalization	O
constant	O
required	O
to	O
ensure	O
that	O
the	O
sum	O
of	O
the	O
conditional	B
probability	B
on	O
the	O
left-hand	O
side	O
of	O
over	O
all	O
values	O
of	O
y	O
equals	O
one	O
in	O
figure	O
we	O
show	O
a	O
simple	O
example	O
involving	O
a	O
joint	O
distribution	O
over	O
two	O
variables	O
to	O
illustrate	O
the	O
concept	O
of	O
marginal	B
and	O
conditional	B
distributions	O
here	O
a	O
finite	O
sample	O
of	O
n	O
data	O
points	O
has	O
been	O
drawn	O
from	O
the	O
joint	O
distribution	O
and	O
is	O
shown	O
in	O
the	O
top	O
left	O
in	O
the	O
top	O
right	O
is	O
a	O
histogram	O
of	O
the	O
fractions	O
of	O
data	O
points	O
having	O
each	O
of	O
the	O
two	O
values	O
of	O
y	O
from	O
the	O
definition	O
of	O
probability	B
these	O
fractions	O
would	O
equal	O
the	O
corresponding	O
probabilities	O
py	O
in	O
the	O
limit	O
n	O
we	O
can	O
view	O
the	O
histogram	O
as	O
a	O
simple	O
way	O
to	O
model	O
a	O
probability	B
distribution	O
given	O
only	O
a	O
finite	O
number	O
of	O
points	O
drawn	O
from	O
that	O
distribution	O
modelling	O
distributions	O
from	O
data	O
lies	O
at	O
the	O
heart	O
of	O
statistical	O
pattern	O
recognition	O
and	O
will	O
be	O
explored	O
in	O
great	O
detail	O
in	O
this	O
book	O
the	O
remaining	O
two	O
plots	O
in	O
figure	O
show	O
the	O
corresponding	O
histogram	O
estimates	O
of	O
px	O
and	O
pxy	O
let	O
us	O
now	O
return	O
to	O
our	O
example	O
involving	O
boxes	O
of	O
fruit	O
for	O
the	O
moment	O
we	O
shall	O
once	O
again	O
be	O
explicit	O
about	O
distinguishing	O
between	O
the	O
random	O
variables	O
and	O
their	O
instantiations	O
we	O
have	O
seen	O
that	O
the	O
probabilities	O
of	O
selecting	O
either	O
the	O
red	O
or	O
the	O
blue	O
boxes	O
are	O
given	O
by	O
pb	O
r	O
pb	O
b	O
respectively	O
note	O
that	O
these	O
satisfy	O
pb	O
r	O
pb	O
b	O
now	O
suppose	O
that	O
we	O
pick	O
a	O
box	O
at	O
random	O
and	O
it	O
turns	O
out	O
to	O
be	O
the	O
blue	O
box	O
then	O
the	O
probability	B
of	O
selecting	O
an	O
apple	O
is	O
just	O
the	O
fraction	O
of	O
apples	O
in	O
the	O
blue	O
box	O
which	O
is	O
and	O
so	O
pf	O
ab	O
b	O
in	O
fact	O
we	O
can	O
write	O
out	O
all	O
four	O
conditional	B
probabilities	O
for	O
the	O
type	O
of	O
fruit	O
given	O
the	O
selected	O
box	O
pf	O
ab	O
r	O
pf	O
ob	O
r	O
pf	O
ab	O
b	O
pf	O
ob	O
b	O
introduction	O
px	O
y	O
py	O
y	O
y	O
x	O
px	O
pxy	O
x	O
x	O
figure	O
an	O
illustration	O
of	O
a	O
distribution	O
over	O
two	O
variables	O
x	O
which	O
takes	O
possible	O
values	O
and	O
y	O
which	O
takes	O
two	O
possible	O
values	O
the	O
top	O
left	O
figure	O
shows	O
a	O
sample	O
of	O
points	O
drawn	O
from	O
a	O
joint	O
probability	B
distribution	O
over	O
these	O
variables	O
the	O
remaining	O
figures	O
show	O
histogram	O
estimates	O
of	O
the	O
marginal	B
distributions	O
px	O
and	O
py	O
as	O
well	O
as	O
the	O
conditional	B
distribution	O
pxy	O
corresponding	O
to	O
the	O
bottom	O
row	O
in	O
the	O
top	O
left	O
figure	O
again	O
note	O
that	O
these	O
probabilities	O
are	O
normalized	O
so	O
that	O
pf	O
ab	O
r	O
pf	O
ob	O
r	O
and	O
similarly	O
pf	O
ab	O
b	O
pf	O
ob	O
b	O
we	O
can	O
now	O
use	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
to	O
evaluate	O
the	O
overall	O
probability	B
of	O
choosing	O
an	O
apple	O
pf	O
a	O
pf	O
ab	O
rpb	O
r	O
pf	O
ab	O
bpb	O
b	O
from	O
which	O
it	O
follows	O
using	O
the	O
sum	B
rule	I
that	O
pf	O
o	O
probability	B
theory	B
suppose	O
instead	O
we	O
are	O
told	O
that	O
a	O
piece	O
of	O
fruit	O
has	O
been	O
selected	O
and	O
it	O
is	O
an	O
orange	O
and	O
we	O
would	O
like	O
to	O
know	O
which	O
box	O
it	O
came	O
from	O
this	O
requires	O
that	O
we	O
evaluate	O
the	O
probability	B
distribution	O
over	O
boxes	O
conditioned	O
on	O
the	O
identity	O
of	O
the	O
fruit	O
whereas	O
the	O
probabilities	O
in	O
give	O
the	O
probability	B
distribution	O
over	O
the	O
fruit	O
conditioned	O
on	O
the	O
identity	O
of	O
the	O
box	O
we	O
can	O
solve	O
the	O
problem	O
of	O
reversing	O
the	O
conditional	B
probability	B
by	O
using	O
bayes	B
theorem	O
to	O
give	O
from	O
the	O
sum	B
rule	I
it	O
then	O
follows	O
that	O
pb	O
bf	O
o	O
pb	O
rf	O
o	O
pf	O
ob	O
rpb	O
r	O
pf	O
o	O
we	O
can	O
provide	O
an	O
important	O
interpretation	O
of	O
bayes	B
theorem	O
as	O
follows	O
if	O
we	O
had	O
been	O
asked	O
which	O
box	O
had	O
been	O
chosen	O
before	O
being	O
told	O
the	O
identity	O
of	O
the	O
selected	O
item	O
of	O
fruit	O
then	O
the	O
most	O
complete	O
information	O
we	O
have	O
available	O
is	O
provided	O
by	O
the	O
probability	B
pb	O
we	O
call	O
this	O
the	O
prior	B
probability	B
because	O
it	O
is	O
the	O
probability	B
available	O
before	O
we	O
observe	O
the	O
identity	O
of	O
the	O
fruit	O
once	O
we	O
are	O
told	O
that	O
the	O
fruit	O
is	O
an	O
orange	O
we	O
can	O
then	O
use	O
bayes	B
theorem	O
to	O
compute	O
the	O
probability	B
pbf	O
which	O
we	O
shall	O
call	O
the	O
posterior	B
probability	B
because	O
it	O
is	O
the	O
probability	B
obtained	O
after	O
we	O
have	O
observed	O
f	O
note	O
that	O
in	O
this	O
example	O
the	O
prior	B
probability	B
of	O
selecting	O
the	O
red	O
box	O
was	O
so	O
that	O
we	O
were	O
more	O
likely	O
to	O
select	O
the	O
blue	O
box	O
than	O
the	O
red	O
one	O
however	O
once	O
we	O
have	O
observed	O
that	O
the	O
piece	O
of	O
selected	O
fruit	O
is	O
an	O
orange	O
we	O
find	O
that	O
the	O
posterior	B
probability	B
of	O
the	O
red	O
box	O
is	O
now	O
so	O
that	O
it	O
is	O
now	O
more	O
likely	O
that	O
the	O
box	O
we	O
selected	O
was	O
in	O
fact	O
the	O
red	O
one	O
this	O
result	O
accords	O
with	O
our	O
intuition	O
as	O
the	O
proportion	O
of	O
oranges	O
is	O
much	O
higher	O
in	O
the	O
red	O
box	O
than	O
it	O
is	O
in	O
the	O
blue	O
box	O
and	O
so	O
the	O
observation	O
that	O
the	O
fruit	O
was	O
an	O
orange	O
provides	O
significant	O
evidence	O
favouring	O
the	O
red	O
box	O
in	O
fact	O
the	O
evidence	O
is	O
sufficiently	O
strong	O
that	O
it	O
outweighs	O
the	O
prior	B
and	O
makes	O
it	O
more	O
likely	O
that	O
the	O
red	O
box	O
was	O
chosen	O
rather	O
than	O
the	O
blue	O
one	O
finally	O
we	O
note	O
that	O
if	O
the	O
joint	O
distribution	O
of	O
two	O
variables	O
factorizes	O
into	O
the	O
product	O
of	O
the	O
marginals	O
so	O
that	O
px	O
y	O
pxpy	O
then	O
x	O
and	O
y	O
are	O
said	O
to	O
be	O
independent	B
from	O
the	O
product	B
rule	I
we	O
see	O
that	O
py	O
py	O
and	O
so	O
the	O
conditional	B
distribution	O
of	O
y	O
given	O
x	O
is	O
indeed	O
independent	B
of	O
the	O
value	O
of	O
x	O
for	O
instance	O
in	O
our	O
boxes	O
of	O
fruit	O
example	O
if	O
each	O
box	O
contained	O
the	O
same	O
fraction	O
of	O
apples	O
and	O
oranges	O
then	O
pfb	O
p	O
so	O
that	O
the	O
probability	B
of	O
selecting	O
say	O
an	O
apple	O
is	O
independent	B
of	O
which	O
box	O
is	O
chosen	O
probability	B
densities	O
as	O
well	O
as	O
considering	O
probabilities	O
defined	O
over	O
discrete	O
sets	O
of	O
events	O
we	O
also	O
wish	O
to	O
consider	O
probabilities	O
with	O
respect	O
to	O
continuous	O
variables	O
we	O
shall	O
limit	O
ourselves	O
to	O
a	O
relatively	O
informal	O
discussion	O
if	O
the	O
probability	B
of	O
a	O
real-valued	O
variable	O
x	O
falling	O
in	O
the	O
interval	O
x	O
x	O
is	O
given	O
by	O
px	O
x	O
for	O
x	O
then	O
px	O
is	O
called	O
the	O
probability	B
density	B
over	O
x	O
this	O
is	O
illustrated	O
in	O
figure	O
the	O
probability	B
that	O
x	O
will	O
lie	O
in	O
an	O
interval	O
b	O
is	O
then	O
given	O
by	O
b	O
px	O
b	O
px	O
dx	O
a	O
introduction	O
figure	O
the	O
concept	O
of	O
probability	B
for	O
discrete	O
variables	O
can	O
be	O
extended	B
to	O
that	O
of	O
a	O
probability	B
density	B
px	O
over	O
a	O
continuous	O
variable	O
x	O
and	O
is	O
such	O
that	O
the	O
probability	B
of	O
x	O
lying	O
in	O
the	O
interval	O
x	O
x	O
is	O
given	O
by	O
px	O
x	O
for	O
x	O
the	O
probability	B
density	B
can	O
be	O
expressed	O
as	O
the	O
derivative	B
of	O
a	O
cumulative	B
distribution	I
function	I
p	O
px	O
p	O
x	O
x	O
because	O
probabilities	O
are	O
nonnegative	O
and	O
because	O
the	O
value	O
of	O
x	O
must	O
lie	O
somewhere	O
on	O
the	O
real	O
axis	O
the	O
probability	B
density	B
px	O
must	O
satisfy	O
the	O
two	O
conditions	O
px	O
px	O
dx	O
under	O
a	O
nonlinear	O
change	O
of	O
variable	O
a	O
probability	B
density	B
transforms	O
differently	O
from	O
a	O
simple	O
function	O
due	O
to	O
the	O
jacobian	O
factor	O
for	O
instance	O
if	O
we	O
consider	O
a	O
change	O
of	O
variables	O
x	O
gy	O
then	O
a	O
function	O
fx	O
fgy	O
now	O
consider	O
a	O
probability	B
density	B
pxx	O
that	O
corresponds	O
to	O
a	O
density	B
pyy	O
with	O
respect	O
to	O
the	O
new	O
variable	O
y	O
where	O
the	O
suffices	O
denote	O
the	O
fact	O
that	O
pxx	O
and	O
pyy	O
are	O
different	O
densities	O
observations	O
falling	O
in	O
the	O
range	O
x	O
x	O
will	O
for	O
small	O
values	O
of	O
x	O
be	O
transformed	O
into	O
the	O
range	O
y	O
y	O
where	O
pxx	O
x	O
pyy	O
y	O
and	O
hence	O
dx	O
pyy	O
pxx	O
dy	O
pxgyg	O
exercise	O
one	O
consequence	O
of	O
this	O
property	O
is	O
that	O
the	O
concept	O
of	O
the	O
maximum	O
of	O
a	O
probability	B
density	B
is	O
dependent	O
on	O
the	O
choice	O
of	O
variable	O
the	O
probability	B
that	O
x	O
lies	O
in	O
the	O
interval	O
z	O
is	O
given	O
by	O
the	O
cumulative	B
distribution	I
function	I
defined	O
by	O
z	O
p	O
px	O
dx	O
which	O
satisfies	O
p	O
px	O
as	O
shown	O
in	O
figure	O
if	O
we	O
have	O
several	O
continuous	O
variables	O
xd	O
denoted	O
collectively	O
by	O
the	O
vector	O
x	O
then	O
we	O
can	O
define	O
a	O
joint	O
probability	B
density	B
px	O
xd	O
such	O
probability	B
theory	B
that	O
the	O
probability	B
of	O
x	O
falling	O
in	O
an	O
infinitesimal	O
volume	O
x	O
containing	O
the	O
point	O
x	O
is	O
given	O
by	O
px	O
x	O
this	O
multivariate	O
probability	B
density	B
must	O
satisfy	O
px	O
px	O
dx	O
in	O
which	O
the	O
integral	O
is	O
taken	O
over	O
the	O
whole	O
of	O
x	O
space	O
we	O
can	O
also	O
consider	O
joint	O
probability	B
distributions	O
over	O
a	O
combination	O
of	O
discrete	O
and	O
continuous	O
variables	O
note	O
that	O
if	O
x	O
is	O
a	O
discrete	O
variable	O
then	O
px	O
is	O
sometimes	O
called	O
a	O
probability	B
mass	B
function	I
because	O
it	O
can	O
be	O
regarded	O
as	O
a	O
set	O
of	O
probability	B
masses	O
concentrated	O
at	O
the	O
allowed	O
values	O
of	O
x	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
as	O
well	O
as	O
bayes	B
theorem	O
apply	O
equally	O
to	O
the	O
case	O
of	O
probability	B
densities	O
or	O
to	O
combinations	O
of	O
discrete	O
and	O
continuous	O
variables	O
for	O
instance	O
if	O
x	O
and	O
y	O
are	O
two	O
real	O
variables	O
then	O
the	O
sum	O
and	O
product	O
rules	O
take	O
the	O
form	O
px	O
px	O
y	O
dy	O
px	O
y	O
pyxpx	O
a	O
formal	O
justification	O
of	O
the	O
sum	O
and	O
product	O
rules	O
for	O
continuous	O
variables	O
requires	O
a	O
branch	O
of	O
mathematics	O
called	O
measure	B
theory	B
and	O
lies	O
outside	O
the	O
scope	O
of	O
this	O
book	O
its	O
validity	O
can	O
be	O
seen	O
informally	O
however	O
by	O
dividing	O
each	O
real	O
variable	O
into	O
intervals	O
of	O
width	O
and	O
considering	O
the	O
discrete	O
probability	B
distribution	O
over	O
these	O
intervals	O
taking	O
the	O
limit	O
then	O
turns	O
sums	O
into	O
integrals	O
and	O
gives	O
the	O
desired	O
result	O
expectations	O
and	O
covariances	O
one	O
of	O
the	O
most	O
important	O
operations	O
involving	O
probabilities	O
is	O
that	O
of	O
finding	O
weighted	O
averages	O
of	O
functions	O
the	O
average	O
value	O
of	O
some	O
function	O
fx	O
under	O
a	O
probability	B
distribution	O
px	O
is	O
called	O
the	O
expectation	B
of	O
fx	O
and	O
will	O
be	O
denoted	O
by	O
ef	O
for	O
a	O
discrete	O
distribution	O
it	O
is	O
given	O
by	O
ef	O
pxfx	O
x	O
so	O
that	O
the	O
average	O
is	O
weighted	O
by	O
the	O
relative	B
probabilities	O
of	O
the	O
different	O
values	O
of	O
x	O
in	O
the	O
case	O
of	O
continuous	O
variables	O
expectations	O
are	O
expressed	O
in	O
terms	O
of	O
an	O
integration	O
with	O
respect	O
to	O
the	O
corresponding	O
probability	B
density	B
ef	O
pxfx	O
dx	O
in	O
either	O
case	O
if	O
we	O
are	O
given	O
a	O
finite	O
number	O
n	O
of	O
points	O
drawn	O
from	O
the	O
probability	B
distribution	O
or	O
probability	B
density	B
then	O
the	O
expectation	B
can	O
be	O
approximated	O
as	O
a	O
introduction	O
finite	O
sum	O
over	O
these	O
points	O
ef	O
n	O
fxn	O
we	O
shall	O
make	O
extensive	O
use	O
of	O
this	O
result	O
when	O
we	O
discuss	O
sampling	B
methods	I
in	O
chapter	O
the	O
approximation	O
in	O
becomes	O
exact	O
in	O
the	O
limit	O
n	O
sometimes	O
we	O
will	O
be	O
considering	O
expectations	O
of	O
functions	O
of	O
several	O
variables	O
in	O
which	O
case	O
we	O
can	O
use	O
a	O
subscript	O
to	O
indicate	O
which	O
variable	O
is	O
being	O
averaged	O
over	O
so	O
that	O
for	O
instance	O
denotes	O
the	O
average	O
of	O
the	O
function	O
fx	O
y	O
with	O
respect	O
to	O
the	O
distribution	O
of	O
x	O
note	O
that	O
exfx	O
y	O
will	O
be	O
a	O
function	O
of	O
y	O
exfx	O
y	O
we	O
can	O
also	O
consider	O
a	O
conditional	B
expectation	B
with	O
respect	O
to	O
a	O
conditional	B
distribution	O
so	O
that	O
exfy	O
pxyfx	O
x	O
with	O
an	O
analogous	O
definition	O
for	O
continuous	O
variables	O
the	O
variance	B
of	O
fx	O
is	O
defined	O
by	O
varf	O
e	O
exercise	O
exercise	O
and	O
provides	O
a	O
measure	O
of	O
how	O
much	O
variability	O
there	O
is	O
in	O
fx	O
around	O
its	O
mean	B
value	O
efx	O
expanding	O
out	O
the	O
square	O
we	O
see	O
that	O
the	O
variance	B
can	O
also	O
be	O
written	O
in	O
terms	O
of	O
the	O
expectations	O
of	O
fx	O
and	O
varf	O
in	O
particular	O
we	O
can	O
consider	O
the	O
variance	B
of	O
the	O
variable	O
x	O
itself	O
which	O
is	O
given	O
by	O
varx	O
for	O
two	O
random	O
variables	O
x	O
and	O
y	O
the	O
covariance	B
is	O
defined	O
by	O
covx	O
y	O
exy	O
exy	O
ey	O
exyxy	O
exey	O
which	O
expresses	O
the	O
extent	O
to	O
which	O
x	O
and	O
y	O
vary	O
together	O
if	O
x	O
and	O
y	O
are	O
independent	B
then	O
their	O
covariance	B
vanishes	O
in	O
the	O
case	O
of	O
two	O
vectors	O
of	O
random	O
variables	O
x	O
and	O
y	O
the	O
covariance	B
is	O
a	O
matrix	O
exyt	O
covx	O
y	O
exy	O
exyxyt	O
exeyt	O
if	O
we	O
consider	O
the	O
covariance	B
of	O
the	O
components	O
of	O
a	O
vector	O
x	O
with	O
each	O
other	O
then	O
we	O
use	O
a	O
slightly	O
simpler	O
notation	O
covx	O
covx	O
x	O
probability	B
theory	B
bayesian	B
probabilities	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
viewed	O
probabilities	O
in	O
terms	O
of	O
the	O
frequencies	O
of	O
random	O
repeatable	O
events	O
we	O
shall	O
refer	O
to	O
this	O
as	O
the	O
classical	B
or	O
frequentist	B
interpretation	O
of	O
probability	B
now	O
we	O
turn	O
to	O
the	O
more	O
general	O
bayesian	B
view	O
in	O
which	O
probabilities	O
provide	O
a	O
quantification	O
of	O
uncertainty	O
consider	O
an	O
uncertain	O
event	O
for	O
example	O
whether	O
the	O
moon	O
was	O
once	O
in	O
its	O
own	O
orbit	O
around	O
the	O
sun	O
or	O
whether	O
the	O
arctic	O
ice	O
cap	O
will	O
have	O
disappeared	O
by	O
the	O
end	O
of	O
the	O
century	O
these	O
are	O
not	O
events	O
that	O
can	O
be	O
repeated	O
numerous	O
times	O
in	O
order	O
to	O
define	O
a	O
notion	O
of	O
probability	B
as	O
we	O
did	O
earlier	O
in	O
the	O
context	O
of	O
boxes	O
of	O
fruit	O
nevertheless	O
we	O
will	O
generally	O
have	O
some	O
idea	O
for	O
example	O
of	O
how	O
quickly	O
we	O
think	O
the	O
polar	O
ice	O
is	O
melting	O
if	O
we	O
now	O
obtain	O
fresh	O
evidence	O
for	O
instance	O
from	O
a	O
new	O
earth	O
observation	O
satellite	O
gathering	O
novel	O
forms	O
of	O
diagnostic	O
information	O
we	O
may	O
revise	O
our	O
opinion	O
on	O
the	O
rate	O
of	O
ice	O
loss	O
our	O
assessment	O
of	O
such	O
matters	O
will	O
affect	O
the	O
actions	O
we	O
take	O
for	O
instance	O
the	O
extent	O
to	O
which	O
we	O
endeavour	O
to	O
reduce	O
the	O
emission	O
of	O
greenhouse	O
gasses	O
in	O
such	O
circumstances	O
we	O
would	O
like	O
to	O
be	O
able	O
to	O
quantify	O
our	O
expression	O
of	O
uncertainty	O
and	O
make	O
precise	O
revisions	O
of	O
uncertainty	O
in	O
the	O
light	O
of	O
new	O
evidence	O
as	O
well	O
as	O
subsequently	O
to	O
be	O
able	O
to	O
take	O
optimal	O
actions	O
or	O
decisions	O
as	O
a	O
consequence	O
this	O
can	O
all	O
be	O
achieved	O
through	O
the	O
elegant	O
and	O
very	O
general	O
bayesian	B
interpretation	O
of	O
probability	B
the	O
use	O
of	O
probability	B
to	O
represent	O
uncertainty	O
however	O
is	O
not	O
an	O
ad-hoc	O
choice	O
but	O
is	O
inevitable	O
if	O
we	O
are	O
to	O
respect	O
common	O
sense	O
while	O
making	O
rational	O
coherent	O
inferences	O
for	O
instance	O
cox	O
showed	O
that	O
if	O
numerical	O
values	O
are	O
used	O
to	O
represent	O
degrees	O
of	O
belief	O
then	O
a	O
simple	O
set	O
of	O
axioms	O
encoding	O
common	O
sense	O
properties	O
of	O
such	O
beliefs	O
leads	O
uniquely	O
to	O
a	O
set	O
of	O
rules	O
for	O
manipulating	O
degrees	O
of	O
belief	O
that	O
are	O
equivalent	O
to	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
this	O
provided	O
the	O
first	O
rigorous	O
proof	O
that	O
probability	B
theory	B
could	O
be	O
regarded	O
as	O
an	O
extension	O
of	O
boolean	B
logic	I
to	O
situations	O
involving	O
uncertainty	O
numerous	O
other	O
authors	O
have	O
proposed	O
different	O
sets	O
of	O
properties	O
or	O
axioms	O
that	O
such	O
measures	O
of	O
uncertainty	O
should	O
satisfy	O
good	O
savage	O
definetti	O
lindley	O
in	O
each	O
case	O
the	O
resulting	O
numerical	O
quantities	O
behave	O
precisely	O
according	O
to	O
the	O
rules	O
of	O
probability	B
it	O
is	O
therefore	O
natural	O
to	O
refer	O
to	O
these	O
quantities	O
as	O
probabilities	O
in	O
the	O
field	O
of	O
pattern	O
recognition	O
too	O
it	O
is	O
helpful	O
to	O
have	O
a	O
more	O
general	O
no	O
thomas	O
bayes	B
thomas	O
bayes	B
was	O
born	O
in	O
tunbridge	O
wells	O
and	O
was	O
a	O
clergyman	O
as	O
well	O
as	O
an	O
amateur	O
scientist	O
and	O
a	O
mathematician	O
he	O
studied	O
logic	O
and	O
theology	O
at	O
edinburgh	O
university	O
and	O
was	O
elected	O
fellow	O
of	O
the	O
royal	O
society	O
in	O
during	O
the	O
century	O
issues	O
regarding	O
probability	B
arose	O
in	O
connection	O
with	O
gambling	O
and	O
with	O
the	O
new	O
concept	O
of	O
insurance	O
one	O
particularly	O
important	O
problem	O
concerned	O
so-called	O
inverse	B
probability	B
a	O
solution	O
was	O
proposed	O
by	O
thomas	O
bayes	B
in	O
his	O
paper	O
essay	O
towards	O
solving	O
a	O
problem	O
in	O
the	O
doctrine	O
of	O
chances	O
which	O
was	O
published	O
in	O
some	O
three	O
years	O
after	O
his	O
death	O
in	O
the	O
philosophical	O
transactions	O
of	O
the	O
royal	O
society	O
in	O
fact	O
bayes	B
only	O
formulated	O
his	O
theory	B
for	O
the	O
case	O
of	O
a	O
uniform	O
prior	B
and	O
it	O
was	O
pierre-simon	O
laplace	B
who	O
independently	O
rediscovered	O
the	O
theory	B
in	O
general	O
form	O
and	O
who	O
demonstrated	O
its	O
broad	O
applicability	O
introduction	O
tion	O
of	O
probability	B
consider	O
the	O
example	O
of	O
polynomial	B
curve	B
fitting	I
discussed	O
in	O
section	O
it	O
seems	O
reasonable	O
to	O
apply	O
the	O
frequentist	B
notion	O
of	O
probability	B
to	O
the	O
random	O
values	O
of	O
the	O
observed	O
variables	O
tn	O
however	O
we	O
would	O
like	O
to	O
address	O
and	O
quantify	O
the	O
uncertainty	O
that	O
surrounds	O
the	O
appropriate	O
choice	O
for	O
the	O
model	O
parameters	O
w	O
we	O
shall	O
see	O
that	O
from	O
a	O
bayesian	B
perspective	O
we	O
can	O
use	O
the	O
machinery	O
of	O
probability	B
theory	B
to	O
describe	O
the	O
uncertainty	O
in	O
model	O
parameters	O
such	O
as	O
w	O
or	O
indeed	O
in	O
the	O
choice	O
of	O
model	O
itself	O
bayes	B
theorem	O
now	O
acquires	O
a	O
new	O
significance	O
recall	O
that	O
in	O
the	O
boxes	O
of	O
fruit	O
example	O
the	O
observation	O
of	O
the	O
identity	O
of	O
the	O
fruit	O
provided	O
relevant	O
information	O
that	O
altered	O
the	O
probability	B
that	O
the	O
chosen	O
box	O
was	O
the	O
red	O
one	O
in	O
that	O
example	O
bayes	B
theorem	O
was	O
used	O
to	O
convert	O
a	O
prior	B
probability	B
into	O
a	O
posterior	B
probability	B
by	O
incorporating	O
the	O
evidence	O
provided	O
by	O
the	O
observed	O
data	O
as	O
we	O
shall	O
see	O
in	O
detail	O
later	O
we	O
can	O
adopt	O
a	O
similar	O
approach	O
when	O
making	O
inferences	O
about	O
quantities	O
such	O
as	O
the	O
parameters	O
w	O
in	O
the	O
polynomial	B
curve	B
fitting	I
example	O
we	O
capture	O
our	O
assumptions	O
about	O
w	O
before	O
observing	O
the	O
data	O
in	O
the	O
form	O
of	O
a	O
prior	B
probability	B
distribution	O
pw	O
the	O
effect	O
of	O
the	O
observed	O
data	O
d	O
tn	O
is	O
expressed	O
through	O
the	O
conditional	B
probability	B
pdw	O
and	O
we	O
shall	O
see	O
later	O
in	O
section	O
how	O
this	O
can	O
be	O
represented	O
explicitly	O
bayes	B
theorem	O
which	O
takes	O
the	O
form	O
pwd	O
pdwpw	O
pd	O
then	O
allows	O
us	O
to	O
evaluate	O
the	O
uncertainty	O
in	O
w	O
after	O
we	O
have	O
observed	O
d	O
in	O
the	O
form	O
of	O
the	O
posterior	B
probability	B
pwd	O
the	O
quantity	O
pdw	O
on	O
the	O
right-hand	O
side	O
of	O
bayes	B
theorem	O
is	O
evaluated	O
for	O
the	O
observed	O
data	O
set	O
d	O
and	O
can	O
be	O
viewed	O
as	O
a	O
function	O
of	O
the	O
parameter	O
vector	O
w	O
in	O
which	O
case	O
it	O
is	O
called	O
the	O
likelihood	B
function	I
it	O
expresses	O
how	O
probable	O
the	O
observed	O
data	O
set	O
is	O
for	O
different	O
settings	O
of	O
the	O
parameter	O
vector	O
w	O
note	O
that	O
the	O
likelihood	O
is	O
not	O
a	O
probability	B
distribution	O
over	O
w	O
and	O
its	O
integral	O
with	O
respect	O
to	O
w	O
does	O
not	O
equal	O
one	O
given	O
this	O
definition	O
of	O
likelihood	O
we	O
can	O
state	O
bayes	B
theorem	O
in	O
words	O
posterior	O
likelihood	O
prior	B
where	O
all	O
of	O
these	O
quantities	O
are	O
viewed	O
as	O
functions	O
of	O
w	O
the	O
denominator	O
in	O
is	O
the	O
normalization	O
constant	O
which	O
ensures	O
that	O
the	O
posterior	O
distribution	O
on	O
the	O
left-hand	O
side	O
is	O
a	O
valid	O
probability	B
density	B
and	O
integrates	O
to	O
one	O
indeed	O
integrating	O
both	O
sides	O
of	O
with	O
respect	O
to	O
w	O
we	O
can	O
express	O
the	O
denominator	O
in	O
bayes	B
theorem	O
in	O
terms	O
of	O
the	O
prior	B
distribution	O
and	O
the	O
likelihood	B
function	I
pd	O
pdwpw	O
dw	O
in	O
both	O
the	O
bayesian	B
and	O
frequentist	B
paradigms	O
the	O
likelihood	B
function	I
pdw	O
plays	O
a	O
central	O
role	O
however	O
the	O
manner	O
in	O
which	O
it	O
is	O
used	O
is	O
fundamentally	O
different	O
in	O
the	O
two	O
approaches	O
in	O
a	O
frequentist	B
setting	O
w	O
is	O
considered	O
to	O
be	O
a	O
fixed	O
parameter	O
whose	O
value	O
is	O
determined	O
by	O
some	O
form	O
of	O
estimator	O
and	O
error	B
bars	O
probability	B
theory	B
on	O
this	O
estimate	O
are	O
obtained	O
by	O
considering	O
the	O
distribution	O
of	O
possible	O
data	O
sets	O
d	O
by	O
contrast	O
from	O
the	O
bayesian	B
viewpoint	O
there	O
is	O
only	O
a	O
single	O
data	O
set	O
d	O
the	O
one	O
that	O
is	O
actually	O
observed	O
and	O
the	O
uncertainty	O
in	O
the	O
parameters	O
is	O
expressed	O
through	O
a	O
probability	B
distribution	O
over	O
w	O
a	O
widely	O
used	O
frequentist	B
estimator	O
is	O
maximum	B
likelihood	I
in	O
which	O
w	O
is	O
set	O
to	O
the	O
value	O
that	O
maximizes	O
the	O
likelihood	B
function	I
pdw	O
this	O
corresponds	O
to	O
choosing	O
the	O
value	O
of	O
w	O
for	O
which	O
the	O
probability	B
of	O
the	O
observed	O
data	O
set	O
is	O
maximized	O
in	O
the	O
machine	O
learning	B
literature	O
the	O
negative	O
log	O
of	O
the	O
likelihood	B
function	I
is	O
called	O
an	O
error	B
function	I
because	O
the	O
negative	O
logarithm	O
is	O
a	O
monotonically	O
decreasing	O
function	O
maximizing	O
the	O
likelihood	O
is	O
equivalent	O
to	O
minimizing	O
the	O
error	B
one	O
approach	O
to	O
determining	O
frequentist	B
error	B
bars	O
is	O
the	O
bootstrap	B
hastie	O
et	O
al	O
in	O
which	O
multiple	O
data	O
sets	O
are	O
created	O
as	O
follows	O
suppose	O
our	O
original	O
data	O
set	O
consists	O
of	O
n	O
data	O
points	O
x	O
xn	O
we	O
can	O
create	O
a	O
new	O
data	O
set	O
xb	O
by	O
drawing	O
n	O
points	O
at	O
random	O
from	O
x	O
with	O
replacement	O
so	O
that	O
some	O
points	O
in	O
x	O
may	O
be	O
replicated	O
in	O
xb	O
whereas	O
other	O
points	O
in	O
x	O
may	O
be	O
absent	O
from	O
xb	O
this	O
process	O
can	O
be	O
repeated	O
l	O
times	O
to	O
generate	O
l	O
data	O
sets	O
each	O
of	O
size	O
n	O
and	O
each	O
obtained	O
by	O
sampling	O
from	O
the	O
original	O
data	O
set	O
x	O
the	O
statistical	O
accuracy	O
of	O
parameter	O
estimates	O
can	O
then	O
be	O
evaluated	O
by	O
looking	O
at	O
the	O
variability	O
of	O
predictions	O
between	O
the	O
different	O
bootstrap	B
data	O
sets	O
one	O
advantage	O
of	O
the	O
bayesian	B
viewpoint	O
is	O
that	O
the	O
inclusion	O
of	O
prior	B
knowledge	O
arises	O
naturally	O
suppose	O
for	O
instance	O
that	O
a	O
fair-looking	O
coin	O
is	O
tossed	O
three	O
times	O
and	O
lands	O
heads	O
each	O
time	O
a	O
classical	B
maximum	B
likelihood	I
estimate	O
of	O
the	O
probability	B
of	O
landing	O
heads	O
would	O
give	O
implying	O
that	O
all	O
future	O
tosses	O
will	O
land	O
heads	O
by	O
contrast	O
a	O
bayesian	B
approach	O
with	O
any	O
reasonable	O
prior	B
will	O
lead	O
to	O
a	O
much	O
less	O
extreme	O
conclusion	O
there	O
has	O
been	O
much	O
controversy	O
and	O
debate	O
associated	O
with	O
the	O
relative	B
merits	O
of	O
the	O
frequentist	B
and	O
bayesian	B
paradigms	O
which	O
have	O
not	O
been	O
helped	O
by	O
the	O
fact	O
that	O
there	O
is	O
no	O
unique	O
frequentist	B
or	O
even	O
bayesian	B
viewpoint	O
for	O
instance	O
one	O
common	O
criticism	O
of	O
the	O
bayesian	B
approach	O
is	O
that	O
the	O
prior	B
distribution	O
is	O
often	O
selected	O
on	O
the	O
basis	O
of	O
mathematical	O
convenience	O
rather	O
than	O
as	O
a	O
reflection	O
of	O
any	O
prior	B
beliefs	O
even	O
the	O
subjective	O
nature	O
of	O
the	O
conclusions	O
through	O
their	O
dependence	O
on	O
the	O
choice	O
of	O
prior	B
is	O
seen	O
by	O
some	O
as	O
a	O
source	O
of	O
difficulty	O
reducing	O
the	O
dependence	O
on	O
the	O
prior	B
is	O
one	O
motivation	O
for	O
so-called	O
noninformative	B
priors	O
however	O
these	O
lead	O
to	O
difficulties	O
when	O
comparing	O
different	O
models	O
and	O
indeed	O
bayesian	B
methods	O
based	O
on	O
poor	O
choices	O
of	O
prior	B
can	O
give	O
poor	O
results	O
with	O
high	O
confidence	O
frequentist	B
evaluation	O
methods	O
offer	O
some	O
protection	O
from	O
such	O
problems	O
and	O
techniques	O
such	O
as	O
cross-validation	B
remain	O
useful	O
in	O
areas	O
such	O
as	O
model	B
comparison	I
this	O
book	O
places	O
a	O
strong	O
emphasis	O
on	O
the	O
bayesian	B
viewpoint	O
reflecting	O
the	O
huge	O
growth	O
in	O
the	O
practical	O
importance	O
of	O
bayesian	B
methods	O
in	O
the	O
past	O
few	O
years	O
while	O
also	O
discussing	O
useful	O
frequentist	B
concepts	O
as	O
required	O
although	O
the	O
bayesian	B
framework	O
has	O
its	O
origins	O
in	O
the	O
century	O
the	O
practical	O
application	O
of	O
bayesian	B
methods	O
was	O
for	O
a	O
long	O
time	O
severely	O
limited	O
by	O
the	O
difficulties	O
in	O
carrying	O
through	O
the	O
full	O
bayesian	B
procedure	O
particularly	O
the	O
need	O
to	O
marginalize	O
or	O
integrate	O
over	O
the	O
whole	O
of	O
parameter	O
space	O
which	O
as	O
we	O
shall	O
section	O
section	O
section	O
introduction	O
see	O
is	O
required	O
in	O
order	O
to	O
make	O
predictions	O
or	O
to	O
compare	O
different	O
models	O
the	O
development	O
of	O
sampling	B
methods	I
such	O
as	O
markov	B
chain	I
monte	I
carlo	I
in	O
chapter	O
along	O
with	O
dramatic	O
improvements	O
in	O
the	O
speed	O
and	O
memory	O
capacity	O
of	O
computers	O
opened	O
the	O
door	O
to	O
the	O
practical	O
use	O
of	O
bayesian	B
techniques	O
in	O
an	O
impressive	O
range	O
of	O
problem	O
domains	O
monte	O
carlo	O
methods	O
are	O
very	O
flexible	O
and	O
can	O
be	O
applied	O
to	O
a	O
wide	O
range	O
of	O
models	O
however	O
they	O
are	O
computationally	O
intensive	O
and	O
have	O
mainly	O
been	O
used	O
for	O
small-scale	O
problems	O
more	O
recently	O
highly	O
efficient	O
deterministic	O
approximation	O
schemes	O
such	O
as	O
variational	B
bayes	B
and	O
expectation	B
propagation	I
in	O
chapter	O
have	O
been	O
developed	O
these	O
offer	O
a	O
complementary	O
alternative	O
to	O
sampling	B
methods	I
and	O
have	O
allowed	O
bayesian	B
techniques	O
to	O
be	O
used	O
in	O
large-scale	O
applications	O
et	O
al	O
the	O
gaussian	B
distribution	O
we	O
shall	O
devote	O
the	O
whole	O
of	O
chapter	O
to	O
a	O
study	O
of	O
various	O
probability	B
distributions	O
and	O
their	O
key	O
properties	O
it	O
is	O
convenient	O
however	O
to	O
introduce	O
here	O
one	O
of	O
the	O
most	O
important	O
probability	B
distributions	O
for	O
continuous	O
variables	O
called	O
the	O
normal	O
or	O
gaussian	B
distribution	O
we	O
shall	O
make	O
extensive	O
use	O
of	O
this	O
distribution	O
in	O
the	O
remainder	O
of	O
this	O
chapter	O
and	O
indeed	O
throughout	O
much	O
of	O
the	O
book	O
for	O
the	O
case	O
of	O
a	O
single	O
real-valued	O
variable	O
x	O
the	O
gaussian	B
distribution	O
is	O
de	O
fined	O
by	O
x	O
exp	O
which	O
is	O
governed	O
by	O
two	O
parameters	O
called	O
the	O
mean	B
and	O
called	O
the	O
variance	B
the	O
square	O
root	O
of	O
the	O
variance	B
given	O
by	O
is	O
called	O
the	O
standard	B
deviation	I
and	O
the	O
reciprocal	O
of	O
the	O
variance	B
written	O
as	O
is	O
called	O
the	O
precision	O
we	O
shall	O
see	O
the	O
motivation	O
for	O
these	O
terms	O
shortly	O
figure	O
shows	O
a	O
plot	O
of	O
the	O
gaussian	B
distribution	O
from	O
the	O
form	O
of	O
we	O
see	O
that	O
the	O
gaussian	B
distribution	O
satisfies	O
n	O
exercise	O
also	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
gaussian	B
is	O
normalized	O
so	O
that	O
pierre-simon	O
laplace	B
it	O
is	O
said	O
that	O
laplace	B
was	O
seriously	O
lacking	O
in	O
modesty	O
and	O
at	O
one	O
point	O
declared	O
himself	O
to	O
be	O
the	O
best	O
mathematician	O
in	O
france	O
at	O
the	O
time	O
a	O
claim	O
that	O
was	O
arguably	O
true	O
as	O
well	O
as	O
being	O
prolific	O
in	O
mathematics	O
he	O
also	O
made	O
numerous	O
contributions	O
to	O
astronomy	O
including	O
the	O
nebular	O
hypothesis	O
by	O
which	O
the	O
earth	O
is	O
thought	O
to	O
have	O
formed	O
from	O
the	O
condensation	O
and	O
cooling	O
of	O
a	O
large	O
rotating	O
disk	O
of	O
gas	O
and	O
dust	O
in	O
he	O
published	O
the	O
first	O
edition	O
of	O
th	O
eorie	O
analytique	O
des	O
probabilit	O
es	O
in	O
which	O
laplace	B
states	O
that	O
probability	B
theory	B
is	O
nothing	O
but	O
common	O
sense	O
reduced	O
to	O
calculation	O
this	O
work	O
included	O
a	O
discussion	O
of	O
the	O
inverse	B
probability	B
calculation	O
termed	O
bayes	B
theorem	O
by	O
poincar	O
e	O
which	O
he	O
used	O
to	O
solve	O
problems	O
in	O
life	O
expectancy	O
jurisprudence	O
planetary	O
masses	O
triangulation	O
and	O
error	B
estimation	O
figure	O
plot	O
of	O
the	O
univariate	O
gaussian	B
showing	O
the	O
mean	B
and	O
the	O
standard	B
deviation	I
n	O
probability	B
theory	B
dx	O
x	O
x	O
thus	O
satisfies	O
the	O
two	O
requirements	O
for	O
a	O
valid	O
probability	B
density	B
we	O
can	O
readily	O
find	O
expectations	O
of	O
functions	O
of	O
x	O
under	O
the	O
gaussian	B
distribu	O
tion	O
in	O
particular	O
the	O
average	O
value	O
of	O
x	O
is	O
given	O
by	O
x	O
x	O
exercise	O
exercise	O
ex	O
x	O
dx	O
because	O
the	O
parameter	O
represents	O
the	O
average	O
value	O
of	O
x	O
under	O
the	O
distribution	O
it	O
is	O
referred	O
to	O
as	O
the	O
mean	B
similarly	O
for	O
the	O
second	B
order	I
moment	O
dx	O
from	O
and	O
it	O
follows	O
that	O
the	O
variance	B
of	O
x	O
is	O
given	O
by	O
varx	O
and	O
hence	O
is	O
referred	O
to	O
as	O
the	O
variance	B
parameter	O
the	O
maximum	O
of	O
a	O
distribution	O
is	O
known	O
as	O
its	O
mode	O
for	O
a	O
gaussian	B
the	O
mode	O
coincides	O
with	O
the	O
mean	B
we	O
are	O
also	O
interested	O
in	O
the	O
gaussian	B
distribution	O
defined	O
over	O
a	O
d-dimensional	O
vector	O
x	O
of	O
continuous	O
variables	O
which	O
is	O
given	O
by	O
n	O
exp	O
where	O
the	O
d-dimensional	O
vector	O
is	O
called	O
the	O
mean	B
the	O
d	O
d	O
matrix	O
is	O
called	O
the	O
covariance	B
and	O
denotes	O
the	O
determinant	O
of	O
we	O
shall	O
make	O
use	O
of	O
the	O
multivariate	O
gaussian	B
distribution	O
briefly	O
in	O
this	O
chapter	O
although	O
its	O
properties	O
will	O
be	O
studied	O
in	O
detail	O
in	O
section	O
introduction	O
figure	O
illustration	O
of	O
the	O
likelihood	B
function	I
for	O
a	O
gaussian	B
distribution	O
shown	O
by	O
the	O
red	O
curve	O
here	O
the	O
black	O
points	O
denote	O
a	O
data	O
set	O
of	O
values	O
and	O
the	O
likelihood	B
function	I
given	O
by	O
corresponds	O
to	O
the	O
product	O
of	O
the	O
blue	O
values	O
maximizing	O
the	O
likelihood	O
involves	O
adjusting	O
the	O
mean	B
and	O
variance	B
of	O
the	O
gaussian	B
so	O
as	O
to	O
maximize	O
this	O
product	O
px	O
n	O
xn	O
x	O
now	O
suppose	O
that	O
we	O
have	O
a	O
data	O
set	O
of	O
observations	O
x	O
xn	O
representing	O
n	O
observations	O
of	O
the	O
scalar	O
variable	O
x	O
note	O
that	O
we	O
are	O
using	O
the	O
typeface	O
x	O
to	O
distinguish	O
this	O
from	O
a	O
single	O
observation	O
of	O
the	O
vector-valued	O
variable	O
xdt	O
which	O
we	O
denote	O
by	O
x	O
we	O
shall	O
suppose	O
that	O
the	O
observations	O
are	O
drawn	O
independently	O
from	O
a	O
gaussian	B
distribution	O
whose	O
mean	B
and	O
variance	B
are	O
unknown	O
and	O
we	O
would	O
like	O
to	O
determine	O
these	O
parameters	O
from	O
the	O
data	O
set	O
data	O
points	O
that	O
are	O
drawn	O
independently	O
from	O
the	O
same	O
distribution	O
are	O
said	O
to	O
be	O
independent	B
and	O
identically	O
distributed	O
which	O
is	O
often	O
abbreviated	O
to	O
i	O
i	O
d	O
we	O
have	O
seen	O
that	O
the	O
joint	O
probability	B
of	O
two	O
independent	B
events	O
is	O
given	O
by	O
the	O
product	O
of	O
the	O
marginal	B
probabilities	O
for	O
each	O
event	O
separately	O
because	O
our	O
data	O
set	O
x	O
is	O
i	O
i	O
d	O
we	O
can	O
therefore	O
write	O
the	O
probability	B
of	O
the	O
data	O
set	O
given	O
and	O
in	O
the	O
form	O
px	O
xn	O
section	O
when	O
viewed	O
as	O
a	O
function	O
of	O
and	O
this	O
is	O
the	O
likelihood	B
function	I
for	O
the	O
gaussian	B
and	O
is	O
interpreted	O
diagrammatically	O
in	O
figure	O
one	O
common	O
criterion	O
for	O
determining	O
the	O
parameters	O
in	O
a	O
probability	B
distribution	O
using	O
an	O
observed	O
data	O
set	O
is	O
to	O
find	O
the	O
parameter	O
values	O
that	O
maximize	O
the	O
likelihood	B
function	I
this	O
might	O
seem	O
like	O
a	O
strange	O
criterion	O
because	O
from	O
our	O
foregoing	O
discussion	O
of	O
probability	B
theory	B
it	O
would	O
seem	O
more	O
natural	O
to	O
maximize	O
the	O
probability	B
of	O
the	O
parameters	O
given	O
the	O
data	O
not	O
the	O
probability	B
of	O
the	O
data	O
given	O
the	O
parameters	O
in	O
fact	O
these	O
two	O
criteria	O
are	O
related	O
as	O
we	O
shall	O
discuss	O
in	O
the	O
context	O
of	O
curve	B
fitting	I
for	O
the	O
moment	O
however	O
we	O
shall	O
determine	O
values	O
for	O
the	O
unknown	O
parameters	O
and	O
in	O
the	O
gaussian	B
by	O
maximizing	O
the	O
likelihood	B
function	I
in	O
practice	O
it	O
is	O
more	O
convenient	O
to	O
maximize	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
because	O
the	O
logarithm	O
is	O
a	O
monotonically	O
increasing	O
function	O
of	O
its	O
argument	O
maximization	O
of	O
the	O
log	O
of	O
a	O
function	O
is	O
equivalent	O
to	O
maximization	O
of	O
the	O
function	O
itself	O
taking	O
the	O
log	O
not	O
only	O
simplifies	O
the	O
subsequent	O
mathematical	O
analysis	O
but	O
it	O
also	O
helps	O
numerically	O
because	O
the	O
product	O
of	O
a	O
large	O
number	O
of	O
small	O
probabilities	O
can	O
easily	O
underflow	O
the	O
numerical	O
precision	O
of	O
the	O
computer	O
and	O
this	O
is	O
resolved	O
by	O
computing	O
instead	O
the	O
sum	O
of	O
the	O
log	O
probabilities	O
from	O
and	O
the	O
log	O
likelihood	O
exercise	O
section	O
exercise	O
function	O
can	O
be	O
written	O
in	O
the	O
form	O
x	O
ln	O
p	O
probability	B
theory	B
n	O
ln	O
n	O
maximizing	O
with	O
respect	O
to	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
solution	O
given	O
by	O
ml	O
which	O
is	O
the	O
sample	B
mean	B
i	O
e	O
the	O
mean	B
of	O
the	O
observed	O
values	O
similarly	O
maximizing	O
with	O
respect	O
to	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
variance	B
in	O
the	O
form	O
xn	O
ml	O
n	O
n	O
which	O
is	O
the	O
sample	B
variance	B
measured	O
with	O
respect	O
to	O
the	O
sample	B
mean	B
ml	O
note	O
that	O
we	O
are	O
performing	O
a	O
joint	O
maximization	O
of	O
with	O
respect	O
to	O
and	O
but	O
in	O
the	O
case	O
of	O
the	O
gaussian	B
distribution	O
the	O
solution	O
for	O
decouples	O
from	O
that	O
for	O
so	O
that	O
we	O
can	O
first	O
evaluate	O
and	O
then	O
subsequently	O
use	O
this	O
result	O
to	O
evaluate	O
later	O
in	O
this	O
chapter	O
and	O
also	O
in	O
subsequent	O
chapters	O
we	O
shall	O
highlight	O
the	O
significant	O
limitations	O
of	O
the	O
maximum	B
likelihood	I
approach	O
here	O
we	O
give	O
an	O
indication	O
of	O
the	O
problem	O
in	O
the	O
context	O
of	O
our	O
solutions	O
for	O
the	O
maximum	B
likelihood	I
parameter	O
settings	O
for	O
the	O
univariate	O
gaussian	B
distribution	O
in	O
particular	O
we	O
shall	O
show	O
that	O
the	O
maximum	B
likelihood	I
approach	O
systematically	O
underestimates	O
the	O
variance	B
of	O
the	O
distribution	O
this	O
is	O
an	O
example	O
of	O
a	O
phenomenon	O
called	O
bias	B
and	O
is	O
related	O
to	O
the	O
problem	O
of	O
over-fitting	B
encountered	O
in	O
the	O
context	O
of	O
polynomial	B
curve	B
fitting	I
we	O
first	O
note	O
that	O
the	O
maximum	B
likelihood	I
solutions	O
ml	O
and	O
ml	O
are	O
functions	O
of	O
the	O
data	O
set	O
values	O
xn	O
consider	O
the	O
expectations	O
of	O
these	O
quantities	O
with	O
respect	O
to	O
the	O
data	O
set	O
values	O
which	O
themselves	O
come	O
from	O
a	O
gaussian	B
distribution	O
with	O
parameters	O
and	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
e	O
ml	O
e	O
ml	O
n	O
n	O
so	O
that	O
on	O
average	O
the	O
maximum	B
likelihood	I
estimate	O
will	O
obtain	O
the	O
correct	O
mean	B
but	O
will	O
underestimate	O
the	O
true	O
variance	B
by	O
a	O
factor	O
the	O
intuition	O
behind	O
this	O
result	O
is	O
given	O
by	O
figure	O
from	O
it	O
follows	O
that	O
the	O
following	O
estimate	O
for	O
the	O
variance	B
parameter	O
is	O
unbiased	O
n	O
n	O
ml	O
n	O
introduction	O
figure	O
illustration	O
of	O
how	O
bias	B
arises	O
in	O
using	O
maximum	B
likelihood	I
to	O
determine	O
the	O
variance	B
of	O
a	O
gaussian	B
the	O
green	O
curve	O
shows	O
the	O
true	O
gaussian	B
distribution	O
from	O
which	O
data	O
is	O
generated	O
and	O
the	O
three	O
red	O
curves	O
show	O
the	O
gaussian	B
distributions	O
obtained	O
by	O
fitting	O
to	O
three	O
data	O
sets	O
each	O
consisting	O
of	O
two	O
data	O
points	O
shown	O
in	O
blue	O
using	O
the	O
maximum	B
likelihood	I
results	O
and	O
averaged	O
across	O
the	O
three	O
data	O
sets	O
the	O
mean	B
is	O
correct	O
but	O
the	O
variance	B
is	O
systematically	O
under-estimated	O
because	O
it	O
is	O
measured	O
relative	B
to	O
the	O
sample	B
mean	B
and	O
not	O
relative	B
to	O
the	O
true	O
mean	B
in	O
section	O
we	O
shall	O
see	O
how	O
this	O
result	O
arises	O
automatically	O
when	O
we	O
adopt	O
a	O
bayesian	B
approach	O
note	O
that	O
the	O
bias	B
of	O
the	O
maximum	B
likelihood	I
solution	O
becomes	O
less	O
significant	O
as	O
the	O
number	O
n	O
of	O
data	O
points	O
increases	O
and	O
in	O
the	O
limit	O
n	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
variance	B
equals	O
the	O
true	O
variance	B
of	O
the	O
distribution	O
that	O
generated	O
the	O
data	O
in	O
practice	O
for	O
anything	O
other	O
than	O
small	O
n	O
this	O
bias	B
will	O
not	O
prove	O
to	O
be	O
a	O
serious	O
problem	O
however	O
throughout	O
this	O
book	O
we	O
shall	O
be	O
interested	O
in	O
more	O
complex	O
models	O
with	O
many	O
parameters	O
for	O
which	O
the	O
bias	B
problems	O
associated	O
with	O
maximum	B
likelihood	I
will	O
be	O
much	O
more	O
severe	O
in	O
fact	O
as	O
we	O
shall	O
see	O
the	O
issue	O
of	O
bias	B
in	O
maximum	B
likelihood	I
lies	O
at	O
the	O
root	O
of	O
the	O
over-fitting	B
problem	O
that	O
we	O
encountered	O
earlier	O
in	O
the	O
context	O
of	O
polynomial	B
curve	B
fitting	I
curve	B
fitting	I
re-visited	O
we	O
have	O
seen	O
how	O
the	O
problem	O
of	O
polynomial	B
curve	B
fitting	I
can	O
be	O
expressed	O
in	O
terms	O
of	O
error	B
minimization	O
here	O
we	O
return	O
to	O
the	O
curve	B
fitting	I
example	O
and	O
view	O
it	O
from	O
a	O
probabilistic	O
perspective	O
thereby	O
gaining	O
some	O
insights	O
into	O
error	B
functions	O
and	O
regularization	B
as	O
well	O
as	O
taking	O
us	O
towards	O
a	O
full	O
bayesian	B
treatment	O
section	O
the	O
goal	O
in	O
the	O
curve	B
fitting	I
problem	O
is	O
to	O
be	O
able	O
to	O
make	O
predictions	O
for	O
the	O
target	O
variable	O
t	O
given	O
some	O
new	O
value	O
of	O
the	O
input	O
variable	O
x	O
on	O
the	O
basis	O
of	O
a	O
set	O
of	O
training	B
data	O
comprising	O
n	O
input	O
values	O
x	O
xn	O
and	O
their	O
corresponding	O
target	O
values	O
t	O
tn	O
we	O
can	O
express	O
our	O
uncertainty	O
over	O
the	O
value	O
of	O
the	O
target	O
variable	O
using	O
a	O
probability	B
distribution	O
for	O
this	O
purpose	O
we	O
shall	O
assume	O
that	O
given	O
the	O
value	O
of	O
x	O
the	O
corresponding	O
value	O
of	O
t	O
has	O
a	O
gaussian	B
distribution	O
with	O
a	O
mean	B
equal	O
to	O
the	O
value	O
yx	O
w	O
of	O
the	O
polynomial	O
curve	O
given	O
by	O
thus	O
we	O
have	O
ptx	O
w	O
tyx	O
w	O
where	O
for	O
consistency	O
with	O
the	O
notation	O
in	O
later	O
chapters	O
we	O
have	O
defined	O
a	O
precision	B
parameter	I
corresponding	O
to	O
the	O
inverse	B
variance	B
of	O
the	O
distribution	O
this	O
is	O
illustrated	O
schematically	O
in	O
figure	O
figure	O
schematic	O
illustration	O
of	O
a	O
gaussian	B
conditional	B
distribution	O
for	O
t	O
given	O
x	O
given	O
by	O
in	O
which	O
the	O
mean	B
is	O
given	O
by	O
the	O
polynomial	O
function	O
yx	O
w	O
and	O
the	O
precision	O
is	O
given	O
by	O
the	O
parameter	O
which	O
is	O
related	O
to	O
the	O
variance	B
by	O
t	O
w	O
probability	B
theory	B
yx	O
w	O
w	O
x	O
we	O
now	O
use	O
the	O
training	B
data	O
t	O
to	O
determine	O
the	O
values	O
of	O
the	O
unknown	O
parameters	O
w	O
and	O
by	O
maximum	B
likelihood	I
if	O
the	O
data	O
are	O
assumed	O
to	O
be	O
drawn	O
independently	O
from	O
the	O
distribution	O
then	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
ptx	O
w	O
tnyxn	O
w	O
as	O
we	O
did	O
in	O
the	O
case	O
of	O
the	O
simple	O
gaussian	B
distribution	O
earlier	O
it	O
is	O
convenient	O
to	O
maximize	O
the	O
logarithm	O
of	O
the	O
likelihood	B
function	I
substituting	O
for	O
the	O
form	O
of	O
the	O
gaussian	B
distribution	O
given	O
by	O
we	O
obtain	O
the	O
log	O
likelihood	B
function	I
in	O
the	O
form	O
ln	O
ptx	O
w	O
w	O
n	O
ln	O
n	O
consider	O
first	O
the	O
determination	O
of	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
polynomial	O
coefficients	O
which	O
will	O
be	O
denoted	O
by	O
wml	O
these	O
are	O
determined	O
by	O
maximizing	O
with	O
respect	O
to	O
w	O
for	O
this	O
purpose	O
we	O
can	O
omit	O
the	O
last	O
two	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
because	O
they	O
do	O
not	O
depend	O
on	O
w	O
also	O
we	O
note	O
that	O
scaling	O
the	O
log	O
likelihood	O
by	O
a	O
positive	O
constant	O
coefficient	O
does	O
not	O
alter	O
the	O
location	O
of	O
the	O
maximum	O
with	O
respect	O
to	O
w	O
and	O
so	O
we	O
can	O
replace	O
the	O
coefficient	O
with	O
finally	O
instead	O
of	O
maximizing	O
the	O
log	O
likelihood	O
we	O
can	O
equivalently	O
minimize	O
the	O
negative	O
log	O
likelihood	O
we	O
therefore	O
see	O
that	O
maximizing	O
likelihood	O
is	O
equivalent	O
so	O
far	O
as	O
determining	O
w	O
is	O
concerned	O
to	O
minimizing	O
the	O
sum-of-squares	B
error	B
function	I
defined	O
by	O
thus	O
the	O
sum-of-squares	B
error	B
function	I
has	O
arisen	O
as	O
a	O
consequence	O
of	O
maximizing	O
likelihood	O
under	O
the	O
assumption	O
of	O
a	O
gaussian	B
noise	O
distribution	O
we	O
can	O
also	O
use	O
maximum	B
likelihood	I
to	O
determine	O
the	O
precision	B
parameter	I
of	O
the	O
gaussian	B
conditional	B
distribution	O
maximizing	O
with	O
respect	O
to	O
gives	O
wml	O
ml	O
n	O
introduction	O
section	O
w	O
wtw	O
again	O
we	O
can	O
first	O
determine	O
the	O
parameter	O
vector	O
wml	O
governing	O
the	O
mean	B
and	O
subsequently	O
use	O
this	O
to	O
find	O
the	O
precision	O
ml	O
as	O
was	O
the	O
case	O
for	O
the	O
simple	O
gaussian	B
distribution	O
having	O
determined	O
the	O
parameters	O
w	O
and	O
we	O
can	O
now	O
make	O
predictions	O
for	O
new	O
values	O
of	O
x	O
because	O
we	O
now	O
have	O
a	O
probabilistic	O
model	O
these	O
are	O
expressed	O
in	O
terms	O
of	O
the	O
predictive	B
distribution	I
that	O
gives	O
the	O
probability	B
distribution	O
over	O
t	O
rather	O
than	O
simply	O
a	O
point	O
estimate	O
and	O
is	O
obtained	O
by	O
substituting	O
the	O
maximum	B
likelihood	I
parameters	O
into	O
to	O
give	O
tyx	O
wml	O
ml	O
ptx	O
wml	O
ml	O
now	O
let	O
us	O
take	O
a	O
step	O
towards	O
a	O
more	O
bayesian	B
approach	O
and	O
introduce	O
a	O
prior	B
distribution	O
over	O
the	O
polynomial	O
coefficients	O
w	O
for	O
simplicity	O
let	O
us	O
consider	O
a	O
gaussian	B
distribution	O
of	O
the	O
form	O
exp	O
wtw	O
pw	O
n	O
where	O
is	O
the	O
precision	O
of	O
the	O
distribution	O
and	O
m	O
is	O
the	O
total	O
number	O
of	O
elements	O
in	O
the	O
vector	O
w	O
for	O
an	O
m	O
th	O
order	O
polynomial	O
variables	O
such	O
as	O
which	O
control	O
the	O
distribution	O
of	O
model	O
parameters	O
are	O
called	O
hyperparameters	O
using	O
bayes	B
theorem	O
the	O
posterior	O
distribution	O
for	O
w	O
is	O
proportional	O
to	O
the	O
product	O
of	O
the	O
prior	B
distribution	O
and	O
the	O
likelihood	B
function	I
pwx	O
t	O
ptx	O
w	O
we	O
can	O
now	O
determine	O
w	O
by	O
finding	O
the	O
most	O
probable	O
value	O
of	O
w	O
given	O
the	O
data	O
in	O
other	O
words	O
by	O
maximizing	O
the	O
posterior	O
distribution	O
this	O
technique	O
is	O
called	O
maximum	B
posterior	I
or	O
simply	O
map	O
taking	O
the	O
negative	O
logarithm	O
of	O
and	O
combining	O
with	O
and	O
we	O
find	O
that	O
the	O
maximum	O
of	O
the	O
posterior	O
is	O
given	O
by	O
the	O
minimum	O
of	O
thus	O
we	O
see	O
that	O
maximizing	O
the	O
posterior	O
distribution	O
is	O
equivalent	O
to	O
minimizing	O
the	O
regularized	O
sum-of-squares	B
error	B
function	I
encountered	O
earlier	O
in	O
the	O
form	O
with	O
a	O
regularization	B
parameter	O
given	O
by	O
bayesian	B
curve	B
fitting	I
although	O
we	O
have	O
included	O
a	O
prior	B
distribution	O
pw	O
we	O
are	O
so	O
far	O
still	O
making	O
a	O
point	O
estimate	O
of	O
w	O
and	O
so	O
this	O
does	O
not	O
yet	O
amount	O
to	O
a	O
bayesian	B
treatment	O
in	O
a	O
fully	O
bayesian	B
approach	O
we	O
should	O
consistently	O
apply	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
which	O
requires	O
as	O
we	O
shall	O
see	O
shortly	O
that	O
we	O
integrate	O
over	O
all	O
values	O
of	O
w	O
such	O
marginalizations	O
lie	O
at	O
the	O
heart	O
of	O
bayesian	B
methods	O
for	O
pattern	O
recognition	O
probability	B
theory	B
in	O
the	O
curve	B
fitting	I
problem	O
we	O
are	O
given	O
the	O
training	B
data	O
x	O
and	O
t	O
along	O
with	O
a	O
new	O
test	O
point	O
x	O
and	O
our	O
goal	O
is	O
to	O
predict	O
the	O
value	O
of	O
t	O
we	O
therefore	O
wish	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
ptx	O
x	O
t	O
here	O
we	O
shall	O
assume	O
that	O
the	O
parameters	O
and	O
are	O
fixed	O
and	O
known	O
in	O
advance	O
later	O
chapters	O
we	O
shall	O
discuss	O
how	O
such	O
parameters	O
can	O
be	O
inferred	O
from	O
data	O
in	O
a	O
bayesian	B
setting	O
a	O
bayesian	B
treatment	O
simply	O
corresponds	O
to	O
a	O
consistent	B
application	O
of	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
which	O
allow	O
the	O
predictive	B
distribution	I
to	O
be	O
written	O
in	O
the	O
form	O
ptx	O
x	O
t	O
ptx	O
wpwx	O
t	O
dw	O
here	O
ptx	O
w	O
is	O
given	O
by	O
and	O
we	O
have	O
omitted	O
the	O
dependence	O
on	O
and	O
to	O
simplify	O
the	O
notation	O
here	O
pwx	O
t	O
is	O
the	O
posterior	O
distribution	O
over	O
parameters	O
and	O
can	O
be	O
found	O
by	O
normalizing	O
the	O
right-hand	O
side	O
of	O
we	O
shall	O
see	O
in	O
section	O
that	O
for	O
problems	O
such	O
as	O
the	O
curve-fitting	O
example	O
this	O
posterior	O
distribution	O
is	O
a	O
gaussian	B
and	O
can	O
be	O
evaluated	O
analytically	O
similarly	O
the	O
integration	O
in	O
can	O
also	O
be	O
performed	O
analytically	O
with	O
the	O
result	O
that	O
the	O
predictive	B
distribution	I
is	O
given	O
by	O
a	O
gaussian	B
of	O
the	O
form	O
ptx	O
x	O
t	O
tmx	O
where	O
the	O
mean	B
and	O
variance	B
are	O
given	O
by	O
here	O
the	O
matrix	O
s	O
is	O
given	O
by	O
mx	O
s	O
i	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
and	O
we	O
have	O
defined	O
the	O
vector	O
with	O
elements	O
ix	O
xi	O
for	O
i	O
m	O
we	O
see	O
that	O
the	O
variance	B
as	O
well	O
as	O
the	O
mean	B
of	O
the	O
predictive	B
distribution	I
in	O
is	O
dependent	O
on	O
x	O
the	O
first	O
term	O
in	O
represents	O
the	O
uncertainty	O
in	O
the	O
predicted	O
value	O
of	O
t	O
due	O
to	O
the	O
noise	O
on	O
the	O
target	O
variables	O
and	O
was	O
expressed	O
already	O
in	O
the	O
maximum	B
likelihood	I
predictive	B
distribution	I
through	O
ml	O
however	O
the	O
second	O
term	O
arises	O
from	O
the	O
uncertainty	O
in	O
the	O
parameters	O
w	O
and	O
is	O
a	O
consequence	O
of	O
the	O
bayesian	B
treatment	O
the	O
predictive	B
distribution	I
for	O
the	O
synthetic	O
sinusoidal	O
regression	B
problem	O
is	O
illustrated	O
in	O
figure	O
introduction	O
figure	O
the	O
predictive	B
distribution	I
resulting	O
from	O
a	O
bayesian	B
treatment	O
of	O
polynomial	B
curve	B
fitting	I
using	O
an	O
m	O
polynomial	O
with	O
the	O
fixed	O
parameters	O
and	O
to	O
the	O
known	O
noise	O
variance	B
in	O
which	O
the	O
red	O
curve	O
denotes	O
the	O
mean	B
of	O
the	O
predictive	B
distribution	I
and	O
the	O
red	O
region	O
corresponds	O
to	O
standard	B
deviation	I
around	O
the	O
mean	B
t	O
x	O
model	B
selection	I
in	O
our	O
example	O
of	O
polynomial	B
curve	B
fitting	I
using	O
least	O
squares	O
we	O
saw	O
that	O
there	O
was	O
an	O
optimal	O
order	O
of	O
polynomial	O
that	O
gave	O
the	O
best	O
generalization	B
the	O
order	O
of	O
the	O
polynomial	O
controls	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
model	O
and	O
thereby	O
governs	O
the	O
model	O
complexity	O
with	O
regularized	B
least	I
squares	I
the	O
regularization	B
coefficient	O
also	O
controls	O
the	O
effective	O
complexity	O
of	O
the	O
model	O
whereas	O
for	O
more	O
complex	O
models	O
such	O
as	O
mixture	B
distributions	O
or	O
neural	O
networks	O
there	O
may	O
be	O
multiple	O
parameters	O
governing	O
complexity	O
in	O
a	O
practical	O
application	O
we	O
need	O
to	O
determine	O
the	O
values	O
of	O
such	O
parameters	O
and	O
the	O
principal	O
objective	O
in	O
doing	O
so	O
is	O
usually	O
to	O
achieve	O
the	O
best	O
predictive	O
performance	O
on	O
new	O
data	O
furthermore	O
as	O
well	O
as	O
finding	O
the	O
appropriate	O
values	O
for	O
complexity	O
parameters	O
within	O
a	O
given	O
model	O
we	O
may	O
wish	O
to	O
consider	O
a	O
range	O
of	O
different	O
types	O
of	O
model	O
in	O
order	O
to	O
find	O
the	O
best	O
one	O
for	O
our	O
particular	O
application	O
we	O
have	O
already	O
seen	O
that	O
in	O
the	O
maximum	B
likelihood	I
approach	O
the	O
performance	O
on	O
the	O
training	B
set	I
is	O
not	O
a	O
good	O
indicator	O
of	O
predictive	O
performance	O
on	O
unseen	O
data	O
due	O
to	O
the	O
problem	O
of	O
over-fitting	B
if	O
data	O
is	O
plentiful	O
then	O
one	O
approach	O
is	O
simply	O
to	O
use	O
some	O
of	O
the	O
available	O
data	O
to	O
train	O
a	O
range	O
of	O
models	O
or	O
a	O
given	O
model	O
with	O
a	O
range	O
of	O
values	O
for	O
its	O
complexity	O
parameters	O
and	O
then	O
to	O
compare	O
them	O
on	O
independent	B
data	O
sometimes	O
called	O
a	O
validation	B
set	I
and	O
select	O
the	O
one	O
having	O
the	O
best	O
predictive	O
performance	O
if	O
the	O
model	O
design	O
is	O
iterated	O
many	O
times	O
using	O
a	O
limited	O
size	O
data	O
set	O
then	O
some	O
over-fitting	B
to	O
the	O
validation	O
data	O
can	O
occur	O
and	O
so	O
it	O
may	O
be	O
necessary	O
to	O
keep	O
aside	O
a	O
third	O
test	B
set	I
on	O
which	O
the	O
performance	O
of	O
the	O
selected	O
model	O
is	O
finally	O
evaluated	O
in	O
many	O
applications	O
however	O
the	O
supply	O
of	O
data	O
for	O
training	B
and	O
testing	O
will	O
be	O
limited	O
and	O
in	O
order	O
to	O
build	O
good	O
models	O
we	O
wish	O
to	O
use	O
as	O
much	O
of	O
the	O
available	O
data	O
as	O
possible	O
for	O
training	B
however	O
if	O
the	O
validation	B
set	I
is	O
small	O
it	O
will	O
give	O
a	O
relatively	O
noisy	O
estimate	O
of	O
predictive	O
performance	O
one	O
solution	O
to	O
this	O
dilemma	O
is	O
to	O
use	O
cross-validation	B
which	O
is	O
illustrated	O
in	O
figure	O
this	O
allows	O
a	O
proportion	O
of	O
the	O
available	O
data	O
to	O
be	O
used	O
for	O
training	B
while	O
making	O
use	O
of	O
all	O
of	O
the	O
the	O
curse	B
of	I
dimensionality	I
figure	O
the	O
technique	O
of	O
s-fold	O
cross-validation	B
illustrated	O
here	O
for	O
the	O
case	O
of	O
s	O
involves	O
taking	O
the	O
available	O
data	O
and	O
partitioning	O
it	O
into	O
s	O
groups	O
the	O
simplest	O
case	O
these	O
are	O
of	O
equal	O
size	O
then	O
s	O
of	O
the	O
groups	O
are	O
used	O
to	O
train	O
a	O
set	O
of	O
models	O
that	O
are	O
then	O
evaluated	O
on	O
the	O
remaining	O
group	O
this	O
procedure	O
is	O
then	O
repeated	O
for	O
all	O
s	O
possible	O
choices	O
for	O
the	O
held-out	O
group	O
indicated	O
here	O
by	O
the	O
red	O
blocks	O
and	O
the	O
performance	O
scores	O
from	O
the	O
s	O
runs	O
are	O
then	O
averaged	O
run	O
run	O
run	O
run	O
data	O
to	O
assess	O
performance	O
when	O
data	O
is	O
particularly	O
scarce	O
it	O
may	O
be	O
appropriate	O
to	O
consider	O
the	O
case	O
s	O
n	O
where	O
n	O
is	O
the	O
total	O
number	O
of	O
data	O
points	O
which	O
gives	O
the	O
leave-one-out	B
technique	O
one	O
major	O
drawback	O
of	O
cross-validation	B
is	O
that	O
the	O
number	O
of	O
training	B
runs	O
that	O
must	O
be	O
performed	O
is	O
increased	O
by	O
a	O
factor	O
of	O
s	O
and	O
this	O
can	O
prove	O
problematic	O
for	O
models	O
in	O
which	O
the	O
training	B
is	O
itself	O
computationally	O
expensive	O
a	O
further	O
problem	O
with	O
techniques	O
such	O
as	O
cross-validation	B
that	O
use	O
separate	O
data	O
to	O
assess	O
performance	O
is	O
that	O
we	O
might	O
have	O
multiple	O
complexity	O
parameters	O
for	O
a	O
single	O
model	O
instance	O
there	O
might	O
be	O
several	O
regularization	B
parameters	O
exploring	O
combinations	O
of	O
settings	O
for	O
such	O
parameters	O
could	O
in	O
the	O
worst	O
case	O
require	O
a	O
number	O
of	O
training	B
runs	O
that	O
is	O
exponential	O
in	O
the	O
number	O
of	O
parameters	O
clearly	O
we	O
need	O
a	O
better	O
approach	O
ideally	O
this	O
should	O
rely	O
only	O
on	O
the	O
training	B
data	O
and	O
should	O
allow	O
multiple	O
hyperparameters	O
and	O
model	O
types	O
to	O
be	O
compared	O
in	O
a	O
single	O
training	B
run	O
we	O
therefore	O
need	O
to	O
find	O
a	O
measure	O
of	O
performance	O
which	O
depends	O
only	O
on	O
the	O
training	B
data	O
and	O
which	O
does	O
not	O
suffer	O
from	O
bias	B
due	O
to	O
over-fitting	B
historically	O
various	O
information	O
criteria	O
have	O
been	O
proposed	O
that	O
attempt	O
to	O
correct	O
for	O
the	O
bias	B
of	O
maximum	B
likelihood	I
by	O
the	O
addition	O
of	O
a	O
penalty	O
term	O
to	O
compensate	O
for	O
the	O
over-fitting	B
of	O
more	O
complex	O
models	O
for	O
example	O
the	O
akaike	B
information	B
criterion	I
or	O
aic	O
chooses	O
the	O
model	O
for	O
which	O
the	O
quantity	O
is	O
largest	O
here	O
pdwml	O
is	O
the	O
best-fit	O
log	O
likelihood	O
and	O
m	O
is	O
the	O
number	O
of	O
adjustable	O
parameters	O
in	O
the	O
model	O
a	O
variant	O
of	O
this	O
quantity	O
called	O
the	O
bayesian	B
information	B
criterion	I
or	O
bic	O
will	O
be	O
discussed	O
in	O
section	O
such	O
criteria	O
do	O
not	O
take	O
account	O
of	O
the	O
uncertainty	O
in	O
the	O
model	O
parameters	O
however	O
and	O
in	O
practice	O
they	O
tend	O
to	O
favour	O
overly	O
simple	O
models	O
we	O
therefore	O
turn	O
in	O
section	O
to	O
a	O
fully	O
bayesian	B
approach	O
where	O
we	O
shall	O
see	O
how	O
complexity	O
penalties	O
arise	O
in	O
a	O
natural	O
and	O
principled	O
way	O
ln	O
pdwml	O
m	O
the	O
curse	B
of	I
dimensionality	I
in	O
the	O
polynomial	B
curve	B
fitting	I
example	O
we	O
had	O
just	O
one	O
input	O
variable	O
x	O
for	O
practical	O
applications	O
of	O
pattern	O
recognition	O
however	O
we	O
will	O
have	O
to	O
deal	O
with	O
spaces	O
introduction	O
figure	O
scatter	O
plot	O
of	O
the	O
oil	O
flow	O
data	O
for	O
input	O
variables	O
and	O
in	O
which	O
red	O
denotes	O
the	O
homogenous	O
class	O
green	O
denotes	O
the	O
annular	O
class	O
and	O
blue	O
denotes	O
the	O
laminar	O
class	O
our	O
goal	O
is	O
to	O
classify	O
the	O
new	O
test	O
point	O
denoted	O
by	O
of	O
high	O
dimensionality	O
comprising	O
many	O
input	O
variables	O
as	O
we	O
now	O
discuss	O
this	O
poses	O
some	O
serious	O
challenges	O
and	O
is	O
an	O
important	O
factor	O
influencing	O
the	O
design	O
of	O
pattern	O
recognition	O
techniques	O
in	O
order	O
to	O
illustrate	O
the	O
problem	O
we	O
consider	O
a	O
synthetically	O
generated	O
data	O
set	O
representing	O
measurements	O
taken	O
from	O
a	O
pipeline	O
containing	O
a	O
mixture	B
of	O
oil	O
water	O
and	O
gas	O
and	O
james	O
these	O
three	O
materials	O
can	O
be	O
present	O
in	O
one	O
of	O
three	O
different	O
geometrical	O
configurations	O
known	O
as	O
homogenous	O
annular	O
and	O
laminar	O
and	O
the	O
fractions	O
of	O
the	O
three	O
materials	O
can	O
also	O
vary	O
each	O
data	O
point	O
comprises	O
a	O
input	O
vector	O
consisting	O
of	O
measurements	O
taken	O
with	O
gamma	O
ray	O
densitometers	O
that	O
measure	O
the	O
attenuation	O
of	O
gamma	O
rays	O
passing	O
along	O
narrow	O
beams	O
through	O
the	O
pipe	O
this	O
data	O
set	O
is	O
described	O
in	O
detail	O
in	O
appendix	O
a	O
figure	O
shows	O
points	O
from	O
this	O
data	O
set	O
on	O
a	O
plot	O
showing	O
two	O
of	O
the	O
measurements	O
and	O
remaining	O
ten	O
input	O
values	O
are	O
ignored	O
for	O
the	O
purposes	O
of	O
this	O
illustration	O
each	O
data	O
point	O
is	O
labelled	O
according	O
to	O
which	O
of	O
the	O
three	O
geometrical	O
classes	O
it	O
belongs	O
to	O
and	O
our	O
goal	O
is	O
to	O
use	O
this	O
data	O
as	O
a	O
training	B
set	I
in	O
order	O
to	O
be	O
able	O
to	O
classify	O
a	O
new	O
observation	O
such	O
as	O
the	O
one	O
denoted	O
by	O
the	O
cross	O
in	O
figure	O
we	O
observe	O
that	O
the	O
cross	O
is	O
surrounded	O
by	O
numerous	O
red	O
points	O
and	O
so	O
we	O
might	O
suppose	O
that	O
it	O
belongs	O
to	O
the	O
red	O
class	O
however	O
there	O
are	O
also	O
plenty	O
of	O
green	O
points	O
nearby	O
so	O
we	O
might	O
think	O
that	O
it	O
could	O
instead	O
belong	O
to	O
the	O
green	O
class	O
it	O
seems	O
unlikely	O
that	O
it	O
belongs	O
to	O
the	O
blue	O
class	O
the	O
intuition	O
here	O
is	O
that	O
the	O
identity	O
of	O
the	O
cross	O
should	O
be	O
determined	O
more	O
strongly	O
by	O
nearby	O
points	O
from	O
the	O
training	B
set	I
and	O
less	O
strongly	O
by	O
more	O
distant	O
points	O
in	O
fact	O
this	O
intuition	O
turns	O
out	O
to	O
be	O
reasonable	O
and	O
will	O
be	O
discussed	O
more	O
fully	O
in	O
later	O
chapters	O
how	O
can	O
we	O
turn	O
this	O
intuition	O
into	O
a	O
learning	B
algorithm	O
one	O
very	O
simple	O
approach	O
would	O
be	O
to	O
divide	O
the	O
input	O
space	O
into	O
regular	O
cells	O
as	O
indicated	O
in	O
figure	O
when	O
we	O
are	O
given	O
a	O
test	O
point	O
and	O
we	O
wish	O
to	O
predict	O
its	O
class	O
we	O
first	O
decide	O
which	O
cell	O
it	O
belongs	O
to	O
and	O
we	O
then	O
find	O
all	O
of	O
the	O
training	B
data	O
points	O
that	O
the	O
curse	B
of	I
dimensionality	I
figure	O
illustration	O
of	O
a	O
simple	O
approach	O
to	O
the	O
solution	O
of	O
a	O
classification	B
problem	O
in	O
which	O
the	O
input	O
space	O
is	O
divided	O
into	O
cells	O
and	O
any	O
new	O
test	O
point	O
is	O
assigned	O
to	O
the	O
class	O
that	O
has	O
a	O
majority	O
number	O
of	O
representatives	O
in	O
the	O
same	O
cell	O
as	O
the	O
test	O
point	O
as	O
we	O
shall	O
see	O
shortly	O
this	O
simplistic	O
approach	O
has	O
some	O
severe	O
shortcomings	O
fall	O
in	O
the	O
same	O
cell	O
the	O
identity	O
of	O
the	O
test	O
point	O
is	O
predicted	O
as	O
being	O
the	O
same	O
as	O
the	O
class	O
having	O
the	O
largest	O
number	O
of	O
training	B
points	O
in	O
the	O
same	O
cell	O
as	O
the	O
test	O
point	O
ties	O
being	O
broken	O
at	O
random	O
there	O
are	O
numerous	O
problems	O
with	O
this	O
naive	O
approach	O
but	O
one	O
of	O
the	O
most	O
severe	O
becomes	O
apparent	O
when	O
we	O
consider	O
its	O
extension	O
to	O
problems	O
having	O
larger	O
numbers	O
of	O
input	O
variables	O
corresponding	O
to	O
input	O
spaces	O
of	O
higher	O
dimensionality	O
the	O
origin	O
of	O
the	O
problem	O
is	O
illustrated	O
in	O
figure	O
which	O
shows	O
that	O
if	O
we	O
divide	O
a	O
region	O
of	O
a	O
space	O
into	O
regular	O
cells	O
then	O
the	O
number	O
of	O
such	O
cells	O
grows	O
exponentially	O
with	O
the	O
dimensionality	O
of	O
the	O
space	O
the	O
problem	O
with	O
an	O
exponentially	O
large	O
number	O
of	O
cells	O
is	O
that	O
we	O
would	O
need	O
an	O
exponentially	O
large	O
quantity	O
of	O
training	B
data	O
in	O
order	O
to	O
ensure	O
that	O
the	O
cells	O
are	O
not	O
empty	O
clearly	O
we	O
have	O
no	O
hope	O
of	O
applying	O
such	O
a	O
technique	O
in	O
a	O
space	O
of	O
more	O
than	O
a	O
few	O
variables	O
and	O
so	O
we	O
need	O
to	O
find	O
a	O
more	O
sophisticated	O
approach	O
we	O
can	O
gain	O
further	O
insight	O
into	O
the	O
problems	O
of	O
high-dimensional	O
spaces	O
by	O
returning	O
to	O
the	O
example	O
of	O
polynomial	B
curve	B
fitting	I
and	O
considering	O
how	O
we	O
would	O
section	O
of	O
figure	O
illustration	O
the	O
curse	B
of	I
dimensionality	I
showing	O
how	O
the	O
number	O
of	O
regions	O
of	O
a	O
regular	O
grid	O
grows	O
exponentially	O
with	O
the	O
dimensionality	O
d	O
of	O
the	O
space	O
for	O
clarity	O
only	O
a	O
subset	O
of	O
the	O
cubical	O
regions	O
are	O
shown	O
for	O
d	O
d	O
d	O
d	O
introduction	O
extend	O
this	O
approach	O
to	O
deal	O
with	O
input	O
spaces	O
having	O
several	O
variables	O
if	O
we	O
have	O
d	O
input	O
variables	O
then	O
a	O
general	O
polynomial	O
with	O
coefficients	O
up	O
to	O
order	O
would	O
take	O
the	O
form	O
yx	O
w	O
wixi	O
wijxixj	O
wijkxixjxk	O
exercise	O
as	O
d	O
increases	O
so	O
the	O
number	O
of	O
independent	B
coefficients	O
all	O
of	O
the	O
coefficients	O
are	O
independent	B
due	O
to	O
interchange	O
symmetries	B
amongst	O
the	O
x	O
variables	O
grows	O
proportionally	O
to	O
in	O
practice	O
to	O
capture	O
complex	O
dependencies	O
in	O
the	O
data	O
we	O
may	O
need	O
to	O
use	O
a	O
higher-order	O
polynomial	O
for	O
a	O
polynomial	O
of	O
order	O
m	O
the	O
growth	O
in	O
the	O
number	O
of	O
coefficients	O
is	O
like	O
dm	O
although	O
this	O
is	O
now	O
a	O
power	O
law	O
growth	O
rather	O
than	O
an	O
exponential	O
growth	O
it	O
still	O
points	O
to	O
the	O
method	O
becoming	O
rapidly	O
unwieldy	O
and	O
of	O
limited	O
practical	O
utility	O
our	O
geometrical	O
intuitions	O
formed	O
through	O
a	O
life	O
spent	O
in	O
a	O
space	O
of	O
three	O
dimensions	O
can	O
fail	O
badly	O
when	O
we	O
consider	O
spaces	O
of	O
higher	O
dimensionality	O
as	O
a	O
simple	O
example	O
consider	O
a	O
sphere	O
of	O
radius	O
r	O
in	O
a	O
space	O
of	O
d	O
dimensions	O
and	O
ask	O
what	O
is	O
the	O
fraction	O
of	O
the	O
volume	O
of	O
the	O
sphere	O
that	O
lies	O
between	O
radius	O
r	O
and	O
r	O
we	O
can	O
evaluate	O
this	O
fraction	O
by	O
noting	O
that	O
the	O
volume	O
of	O
a	O
sphere	O
of	O
radius	O
r	O
in	O
d	O
dimensions	O
must	O
scale	O
as	O
rd	O
and	O
so	O
we	O
write	O
vdr	O
kdrd	O
exercise	O
where	O
the	O
constant	O
kd	O
depends	O
only	O
on	O
d	O
thus	O
the	O
required	O
fraction	O
is	O
given	O
by	O
exercise	O
which	O
is	O
plotted	O
as	O
a	O
function	O
of	O
for	O
various	O
values	O
of	O
d	O
in	O
figure	O
we	O
see	O
that	O
for	O
large	O
d	O
this	O
fraction	O
tends	O
to	O
even	O
for	O
small	O
values	O
of	O
thus	O
in	O
spaces	O
of	O
high	O
dimensionality	O
most	O
of	O
the	O
volume	O
of	O
a	O
sphere	O
is	O
concentrated	O
in	O
a	O
thin	O
shell	O
near	O
the	O
surface	O
as	O
a	O
further	O
example	O
of	O
direct	O
relevance	O
to	O
pattern	O
recognition	O
consider	O
the	O
behaviour	O
of	O
a	O
gaussian	B
distribution	O
in	O
a	O
high-dimensional	O
space	O
if	O
we	O
transform	O
from	O
cartesian	O
to	O
polar	O
coordinates	O
and	O
then	O
integrate	O
out	O
the	O
directional	O
variables	O
we	O
obtain	O
an	O
expression	O
for	O
the	O
density	B
pr	O
as	O
a	O
function	O
of	O
radius	O
r	O
from	O
the	O
origin	O
thus	O
pr	O
r	O
is	O
the	O
probability	B
mass	O
inside	O
a	O
thin	O
shell	O
of	O
thickness	O
r	O
located	O
at	O
radius	O
r	O
this	O
distribution	O
is	O
plotted	O
for	O
various	O
values	O
of	O
d	O
in	O
figure	O
and	O
we	O
see	O
that	O
for	O
large	O
d	O
the	O
probability	B
mass	O
of	O
the	O
gaussian	B
is	O
concentrated	O
in	O
a	O
thin	O
shell	O
the	O
severe	O
difficulty	O
that	O
can	O
arise	O
in	O
spaces	O
of	O
many	O
dimensions	O
is	O
sometimes	O
called	O
the	O
curse	B
of	I
dimensionality	I
in	O
this	O
book	O
we	O
shall	O
make	O
extensive	O
use	O
of	O
illustrative	O
examples	O
involving	O
input	O
spaces	O
of	O
one	O
or	O
two	O
dimensions	O
because	O
this	O
makes	O
it	O
particularly	O
easy	O
to	O
illustrate	O
the	O
techniques	O
graphically	O
the	O
reader	O
should	O
be	O
warned	O
however	O
that	O
not	O
all	O
intuitions	O
developed	O
in	O
spaces	O
of	O
low	O
dimensionality	O
will	O
generalize	O
to	O
spaces	O
of	O
many	O
dimensions	O
figure	O
plot	O
of	O
the	O
fraction	O
of	O
the	O
volume	O
of	O
a	O
sphere	O
lying	O
in	O
the	O
range	O
r	O
to	O
r	O
for	O
various	O
values	O
of	O
the	O
dimensionality	O
d	O
the	O
curse	B
of	I
dimensionality	I
n	O
o	O
i	O
t	O
c	O
a	O
r	O
f	O
e	O
m	O
u	O
o	O
v	O
l	O
d	O
d	O
d	O
d	O
although	O
the	O
curse	B
of	I
dimensionality	I
certainly	O
raises	O
important	O
issues	O
for	O
pattern	O
recognition	O
applications	O
it	O
does	O
not	O
prevent	O
us	O
from	O
finding	O
effective	O
techniques	O
applicable	O
to	O
high-dimensional	O
spaces	O
the	O
reasons	O
for	O
this	O
are	O
twofold	O
first	O
real	O
data	O
will	O
often	O
be	O
confined	O
to	O
a	O
region	O
of	O
the	O
space	O
having	O
lower	O
effective	O
dimensionality	O
and	O
in	O
particular	O
the	O
directions	O
over	O
which	O
important	O
variations	O
in	O
the	O
target	O
variables	O
occur	O
may	O
be	O
so	O
confined	O
second	O
real	O
data	O
will	O
typically	O
exhibit	O
some	O
smoothness	O
properties	O
least	O
locally	O
so	O
that	O
for	O
the	O
most	O
part	O
small	O
changes	O
in	O
the	O
input	O
variables	O
will	O
produce	O
small	O
changes	O
in	O
the	O
target	O
variables	O
and	O
so	O
we	O
can	O
exploit	O
local	B
interpolation-like	O
techniques	O
to	O
allow	O
us	O
to	O
make	O
predictions	O
of	O
the	O
target	O
variables	O
for	O
new	O
values	O
of	O
the	O
input	O
variables	O
successful	O
pattern	O
recognition	O
techniques	O
exploit	O
one	O
or	O
both	O
of	O
these	O
properties	O
consider	O
for	O
example	O
an	O
application	O
in	O
manufacturing	O
in	O
which	O
images	O
are	O
captured	O
of	O
identical	O
planar	O
objects	O
on	O
a	O
conveyor	O
belt	O
in	O
which	O
the	O
goal	O
is	O
to	O
determine	O
their	O
orientation	O
each	O
image	O
is	O
a	O
point	O
figure	O
plot	O
of	O
the	O
probability	B
density	B
with	O
to	O
radius	O
r	O
of	O
a	O
gausrespect	O
sian	O
distribution	O
for	O
various	O
values	O
of	O
in	O
a	O
high-dimensional	O
space	O
most	O
of	O
the	O
probability	B
mass	O
of	O
a	O
gaussian	B
is	O
located	O
within	O
a	O
thin	O
shell	O
at	O
a	O
specific	O
radius	O
the	O
dimensionality	O
d	O
r	O
p	O
d	O
d	O
d	O
r	O
introduction	O
in	O
a	O
high-dimensional	O
space	O
whose	O
dimensionality	O
is	O
determined	O
by	O
the	O
number	O
of	O
pixels	O
because	O
the	O
objects	O
can	O
occur	O
at	O
different	O
positions	O
within	O
the	O
image	O
and	O
in	O
different	O
orientations	O
there	O
are	O
three	O
degrees	B
of	I
freedom	I
of	O
variability	O
between	O
images	O
and	O
a	O
set	O
of	O
images	O
will	O
live	O
on	O
a	O
three	O
dimensional	O
manifold	B
embedded	O
within	O
the	O
high-dimensional	O
space	O
due	O
to	O
the	O
complex	O
relationships	O
between	O
the	O
object	O
position	O
or	O
orientation	O
and	O
the	O
pixel	O
intensities	O
this	O
manifold	B
will	O
be	O
highly	O
nonlinear	O
if	O
the	O
goal	O
is	O
to	O
learn	O
a	O
model	O
that	O
can	O
take	O
an	O
input	O
image	O
and	O
output	O
the	O
orientation	O
of	O
the	O
object	O
irrespective	O
of	O
its	O
position	O
then	O
there	O
is	O
only	O
one	O
degree	O
of	O
freedom	O
of	O
variability	O
within	O
the	O
manifold	B
that	O
is	O
significant	O
decision	B
theory	B
we	O
have	O
seen	O
in	O
section	O
how	O
probability	B
theory	B
provides	O
us	O
with	O
a	O
consistent	B
mathematical	O
framework	O
for	O
quantifying	O
and	O
manipulating	O
uncertainty	O
here	O
we	O
turn	O
to	O
a	O
discussion	O
of	O
decision	B
theory	B
that	O
when	O
combined	O
with	O
probability	B
theory	B
allows	O
us	O
to	O
make	O
optimal	O
decisions	O
in	O
situations	O
involving	O
uncertainty	O
such	O
as	O
those	O
encountered	O
in	O
pattern	O
recognition	O
suppose	O
we	O
have	O
an	O
input	O
vector	O
x	O
together	O
with	O
a	O
corresponding	O
vector	O
t	O
of	O
target	O
variables	O
and	O
our	O
goal	O
is	O
to	O
predict	O
t	O
given	O
a	O
new	O
value	O
for	O
x	O
for	B
regression	B
problems	O
t	O
will	O
comprise	O
continuous	O
variables	O
whereas	O
for	O
classification	B
problems	O
t	O
will	O
represent	O
class	O
labels	O
the	O
joint	O
probability	B
distribution	O
px	O
t	O
provides	O
a	O
complete	O
summary	O
of	O
the	O
uncertainty	O
associated	O
with	O
these	O
variables	O
determination	O
of	O
px	O
t	O
from	O
a	O
set	O
of	O
training	B
data	O
is	O
an	O
example	O
of	O
inference	B
and	O
is	O
typically	O
a	O
very	O
difficult	O
problem	O
whose	O
solution	O
forms	O
the	O
subject	O
of	O
much	O
of	O
this	O
book	O
in	O
a	O
practical	O
application	O
however	O
we	O
must	O
often	O
make	O
a	O
specific	O
prediction	O
for	O
the	O
value	O
of	O
t	O
or	O
more	O
generally	O
take	O
a	O
specific	O
action	O
based	O
on	O
our	O
understanding	O
of	O
the	O
values	O
t	O
is	O
likely	O
to	O
take	O
and	O
this	O
aspect	O
is	O
the	O
subject	O
of	O
decision	B
theory	B
consider	O
for	O
example	O
a	O
medical	O
diagnosis	O
problem	O
in	O
which	O
we	O
have	O
taken	O
an	O
x-ray	O
image	O
of	O
a	O
patient	O
and	O
we	O
wish	O
to	O
determine	O
whether	O
the	O
patient	O
has	O
cancer	O
or	O
not	O
in	O
this	O
case	O
the	O
input	O
vector	O
x	O
is	O
the	O
set	O
of	O
pixel	O
intensities	O
in	O
the	O
image	O
and	O
output	O
variable	O
t	O
will	O
represent	O
the	O
presence	O
of	O
cancer	O
which	O
we	O
denote	O
by	O
the	O
class	O
or	O
the	O
absence	O
of	O
cancer	O
which	O
we	O
denote	O
by	O
the	O
class	O
we	O
might	O
for	O
instance	O
choose	O
t	O
to	O
be	O
a	O
binary	O
variable	O
such	O
that	O
t	O
corresponds	O
to	O
class	O
and	O
t	O
corresponds	O
to	O
class	O
we	O
shall	O
see	O
later	O
that	O
this	O
choice	O
of	O
label	O
values	O
is	O
particularly	O
convenient	O
for	O
probabilistic	O
models	O
the	O
general	O
inference	B
problem	O
then	O
involves	O
determining	O
the	O
joint	O
distribution	O
pxck	O
or	O
equivalently	O
px	O
t	O
which	O
gives	O
us	O
the	O
most	O
complete	O
probabilistic	O
description	O
of	O
the	O
situation	O
although	O
this	O
can	O
be	O
a	O
very	O
useful	O
and	O
informative	O
quantity	O
in	O
the	O
end	O
we	O
must	O
decide	O
either	O
to	O
give	O
treatment	O
to	O
the	O
patient	O
or	O
not	O
and	O
we	O
would	O
like	O
this	O
choice	O
to	O
be	O
optimal	O
in	O
some	O
appropriate	O
sense	O
and	O
hart	O
this	O
is	O
the	O
decision	O
step	O
and	O
it	O
is	O
the	O
subject	O
of	O
decision	B
theory	B
to	O
tell	O
us	O
how	O
to	O
make	O
optimal	O
decisions	O
given	O
the	O
appropriate	O
probabilities	O
we	O
shall	O
see	O
that	O
the	O
decision	O
stage	O
is	O
generally	O
very	O
simple	O
even	O
trivial	O
once	O
we	O
have	O
solved	O
the	O
inference	B
problem	O
here	O
we	O
give	O
an	O
introduction	O
to	O
the	O
key	O
ideas	O
of	O
decision	B
theory	B
as	O
required	O
for	O
decision	B
theory	B
the	O
rest	O
of	O
the	O
book	O
further	O
background	O
as	O
well	O
as	O
more	O
detailed	O
accounts	O
can	O
be	O
found	O
in	O
berger	O
and	O
bather	O
before	O
giving	O
a	O
more	O
detailed	O
analysis	O
let	O
us	O
first	O
consider	O
informally	O
how	O
we	O
might	O
expect	O
probabilities	O
to	O
play	O
a	O
role	O
in	O
making	O
decisions	O
when	O
we	O
obtain	O
the	O
x-ray	O
image	O
x	O
for	O
a	O
new	O
patient	O
our	O
goal	O
is	O
to	O
decide	O
which	O
of	O
the	O
two	O
classes	O
to	O
assign	O
to	O
the	O
image	O
we	O
are	O
interested	O
in	O
the	O
probabilities	O
of	O
the	O
two	O
classes	O
given	O
the	O
image	O
which	O
are	O
given	O
by	O
pckx	O
using	O
bayes	B
theorem	O
these	O
probabilities	O
can	O
be	O
expressed	O
in	O
the	O
form	O
pckx	O
pxckpck	O
px	O
note	O
that	O
any	O
of	O
the	O
quantities	O
appearing	O
in	O
bayes	B
theorem	O
can	O
be	O
obtained	O
from	O
the	O
joint	O
distribution	O
pxck	O
by	O
either	O
marginalizing	O
or	O
conditioning	O
with	O
respect	O
to	O
the	O
appropriate	O
variables	O
we	O
can	O
now	O
interpret	O
pck	O
as	O
the	O
prior	B
probability	B
for	O
the	O
class	O
ck	O
and	O
pckx	O
as	O
the	O
corresponding	O
posterior	B
probability	B
thus	O
represents	O
the	O
probability	B
that	O
a	O
person	O
has	O
cancer	O
before	O
we	O
take	O
the	O
x-ray	O
measurement	O
similarly	O
is	O
the	O
corresponding	O
probability	B
revised	O
using	O
bayes	B
theorem	O
in	O
light	O
of	O
the	O
information	O
contained	O
in	O
the	O
x-ray	O
if	O
our	O
aim	O
is	O
to	O
minimize	O
the	O
chance	O
of	O
assigning	O
x	O
to	O
the	O
wrong	O
class	O
then	O
intuitively	O
we	O
would	O
choose	O
the	O
class	O
having	O
the	O
higher	O
posterior	B
probability	B
we	O
now	O
show	O
that	O
this	O
intuition	O
is	O
correct	O
and	O
we	O
also	O
discuss	O
more	O
general	O
criteria	O
for	O
making	O
decisions	O
minimizing	O
the	O
misclassification	O
rate	O
suppose	O
that	O
our	O
goal	O
is	O
simply	O
to	O
make	O
as	O
few	O
misclassifications	O
as	O
possible	O
we	O
need	O
a	O
rule	O
that	O
assigns	O
each	O
value	O
of	O
x	O
to	O
one	O
of	O
the	O
available	O
classes	O
such	O
a	O
rule	O
will	O
divide	O
the	O
input	O
space	O
into	O
regions	O
rk	O
called	O
decision	O
regions	O
one	O
for	O
each	O
class	O
such	O
that	O
all	O
points	O
in	O
rk	O
are	O
assigned	O
to	O
class	O
ck	O
the	O
boundaries	O
between	O
decision	O
regions	O
are	O
called	O
decision	O
boundaries	O
or	O
decision	O
surfaces	O
note	O
that	O
each	O
decision	B
region	I
need	O
not	O
be	O
contiguous	O
but	O
could	O
comprise	O
some	O
number	O
of	O
disjoint	O
regions	O
we	O
shall	O
encounter	O
examples	O
of	O
decision	O
boundaries	O
and	O
decision	O
regions	O
in	O
later	O
chapters	O
in	O
order	O
to	O
find	O
the	O
optimal	O
decision	O
rule	O
consider	O
first	O
of	O
all	O
the	O
case	O
of	O
two	O
classes	O
as	O
in	O
the	O
cancer	O
problem	O
for	O
instance	O
a	O
mistake	O
occurs	O
when	O
an	O
input	O
vector	O
belonging	O
to	O
class	O
is	O
assigned	O
to	O
class	O
or	O
vice	O
versa	O
the	O
probability	B
of	O
this	O
occurring	O
is	O
given	O
by	O
pmistake	O
px	O
px	O
dx	O
dx	O
we	O
are	O
free	O
to	O
choose	O
the	O
decision	O
rule	O
that	O
assigns	O
each	O
point	O
x	O
to	O
one	O
of	O
the	O
two	O
classes	O
clearly	O
to	O
minimize	O
pmistake	O
we	O
should	O
arrange	O
that	O
each	O
x	O
is	O
assigned	O
to	O
whichever	O
class	O
has	O
the	O
smaller	O
value	O
of	O
the	O
integrand	O
in	O
thus	O
if	O
for	O
a	O
given	O
value	O
of	O
x	O
then	O
we	O
should	O
assign	O
that	O
x	O
to	O
class	O
from	O
the	O
product	B
rule	I
of	I
probability	B
we	O
have	O
pxck	O
pckxpx	O
because	O
the	O
factor	O
px	O
is	O
common	O
to	O
both	O
terms	O
we	O
can	O
restate	O
this	O
result	O
as	O
saying	O
that	O
the	O
minimum	O
introduction	O
x	O
figure	O
schematic	O
illustration	O
of	O
the	O
joint	O
probabilities	O
pxck	O
for	O
each	O
of	O
two	O
classes	O
plotted	O
against	O
x	O
together	O
with	O
the	O
decision	B
boundary	I
x	O
bx	O
values	O
of	O
x	O
bx	O
are	O
classified	O
as	O
class	O
and	O
hence	O
belong	O
to	O
decision	B
region	I
whereas	O
points	O
x	O
bx	O
are	O
classified	O
as	O
and	O
belong	O
to	O
errors	O
arise	O
from	O
the	O
blue	O
green	O
and	O
red	O
regions	O
so	O
that	O
for	O
x	O
bx	O
the	O
errors	O
are	O
due	O
to	O
points	O
from	O
class	O
being	O
misclassified	O
as	O
by	O
the	O
sum	O
of	O
the	O
red	O
and	O
green	O
regions	O
and	O
conversely	O
for	O
points	O
in	O
the	O
region	O
x	O
bx	O
the	O
errors	O
are	O
due	O
to	O
points	O
from	O
class	O
being	O
misclassified	O
as	O
by	O
the	O
blue	O
region	O
as	O
we	O
vary	O
the	O
location	O
bx	O
of	O
the	O
decision	B
boundary	I
the	O
combined	O
areas	O
of	O
the	O
blue	O
and	O
green	O
regions	O
remains	O
constant	O
whereas	O
the	O
size	O
of	O
the	O
red	O
region	O
varies	O
the	O
optimal	O
choice	O
for	O
bx	O
is	O
where	O
the	O
curves	O
for	O
and	O
cross	O
corresponding	O
to	O
bx	O
because	O
in	O
this	O
case	O
the	O
red	O
region	O
disappears	O
this	O
is	O
equivalent	O
to	O
the	O
minimum	O
misclassification	O
rate	O
decision	O
rule	O
which	O
assigns	O
each	O
value	O
of	O
x	O
to	O
the	O
class	O
having	O
the	O
higher	O
posterior	B
probability	B
pckx	O
probability	B
of	O
making	O
a	O
mistake	O
is	O
obtained	O
if	O
each	O
value	O
of	O
x	O
is	O
assigned	O
to	O
the	O
class	O
for	O
which	O
the	O
posterior	B
probability	B
pckx	O
is	O
largest	O
this	O
result	O
is	O
illustrated	O
for	O
two	O
classes	O
and	O
a	O
single	O
input	O
variable	O
x	O
in	O
figure	O
for	O
the	O
more	O
general	O
case	O
of	O
k	O
classes	O
it	O
is	O
slightly	O
easier	O
to	O
maximize	O
the	O
probability	B
of	O
being	O
correct	O
which	O
is	O
given	O
by	O
rk	O
px	O
rkck	O
pcorrect	O
pxck	O
dx	O
which	O
is	O
maximized	O
when	O
the	O
regions	O
rk	O
are	O
chosen	O
such	O
that	O
each	O
x	O
is	O
assigned	O
to	O
the	O
class	O
for	O
which	O
pxck	O
is	O
largest	O
again	O
using	O
the	O
product	B
rule	I
pxck	O
pckxpx	O
and	O
noting	O
that	O
the	O
factor	O
of	O
px	O
is	O
common	O
to	O
all	O
terms	O
we	O
see	O
that	O
each	O
x	O
should	O
be	O
assigned	O
to	O
the	O
class	O
having	O
the	O
largest	O
posterior	B
probability	B
pckx	O
figure	O
an	O
example	O
of	O
a	O
loss	B
matrix	I
with	O
elements	O
lkj	O
for	O
the	O
cancer	O
treatment	O
problem	O
the	O
rows	O
correspond	O
to	O
the	O
true	O
class	O
whereas	O
the	O
columns	O
correspond	O
to	O
the	O
assignment	O
of	O
class	O
made	O
by	O
our	O
decision	O
criterion	O
cancer	O
normal	O
decision	B
theory	B
cancer	O
normal	O
minimizing	O
the	O
expected	O
loss	O
for	O
many	O
applications	O
our	O
objective	O
will	O
be	O
more	O
complex	O
than	O
simply	O
minimizing	O
the	O
number	O
of	O
misclassifications	O
let	O
us	O
consider	O
again	O
the	O
medical	O
diagnosis	O
problem	O
we	O
note	O
that	O
if	O
a	O
patient	O
who	O
does	O
not	O
have	O
cancer	O
is	O
incorrectly	O
diagnosed	O
as	O
having	O
cancer	O
the	O
consequences	O
may	O
be	O
some	O
patient	O
distress	O
plus	O
the	O
need	O
for	O
further	O
investigations	O
conversely	O
if	O
a	O
patient	O
with	O
cancer	O
is	O
diagnosed	O
as	O
healthy	O
the	O
result	O
may	O
be	O
premature	O
death	O
due	O
to	O
lack	O
of	O
treatment	O
thus	O
the	O
consequences	O
of	O
these	O
two	O
types	O
of	O
mistake	O
can	O
be	O
dramatically	O
different	O
it	O
would	O
clearly	O
be	O
better	O
to	O
make	O
fewer	O
mistakes	O
of	O
the	O
second	O
kind	O
even	O
if	O
this	O
was	O
at	O
the	O
expense	O
of	O
making	O
more	O
mistakes	O
of	O
the	O
first	O
kind	O
we	O
can	O
formalize	O
such	O
issues	O
through	O
the	O
introduction	O
of	O
a	O
loss	B
function	I
also	O
called	O
a	O
cost	B
function	I
which	O
is	O
a	O
single	O
overall	O
measure	O
of	O
loss	O
incurred	O
in	O
taking	O
any	O
of	O
the	O
available	O
decisions	O
or	O
actions	O
our	O
goal	O
is	O
then	O
to	O
minimize	O
the	O
total	O
loss	O
incurred	O
note	O
that	O
some	O
authors	O
consider	O
instead	O
a	O
utility	B
function	I
whose	O
value	O
they	O
aim	O
to	O
maximize	O
these	O
are	O
equivalent	O
concepts	O
if	O
we	O
take	O
the	O
utility	O
to	O
be	O
simply	O
the	O
negative	O
of	O
the	O
loss	O
and	O
throughout	O
this	O
text	O
we	O
shall	O
use	O
the	O
loss	B
function	I
convention	O
suppose	O
that	O
for	O
a	O
new	O
value	O
of	O
x	O
the	O
true	O
class	O
is	O
ck	O
and	O
that	O
we	O
assign	O
x	O
to	O
class	O
cj	O
j	O
may	O
or	O
may	O
not	O
be	O
equal	O
to	O
k	O
in	O
so	O
doing	O
we	O
incur	O
some	O
level	O
of	O
loss	O
that	O
we	O
denote	O
by	O
lkj	O
which	O
we	O
can	O
view	O
as	O
the	O
k	O
j	O
element	O
of	O
a	O
loss	B
matrix	I
for	O
instance	O
in	O
our	O
cancer	O
example	O
we	O
might	O
have	O
a	O
loss	B
matrix	I
of	O
the	O
form	O
shown	O
in	O
figure	O
this	O
particular	O
loss	B
matrix	I
says	O
that	O
there	O
is	O
no	O
loss	O
incurred	O
if	O
the	O
correct	O
decision	O
is	O
made	O
there	O
is	O
a	O
loss	O
of	O
if	O
a	O
healthy	O
patient	O
is	O
diagnosed	O
as	O
having	O
cancer	O
whereas	O
there	O
is	O
a	O
loss	O
of	O
if	O
a	O
patient	O
having	O
cancer	O
is	O
diagnosed	O
as	O
healthy	O
the	O
optimal	O
solution	O
is	O
the	O
one	O
which	O
minimizes	O
the	O
loss	B
function	I
however	O
the	O
loss	B
function	I
depends	O
on	O
the	O
true	O
class	O
which	O
is	O
unknown	O
for	O
a	O
given	O
input	O
vector	O
x	O
our	O
uncertainty	O
in	O
the	O
true	O
class	O
is	O
expressed	O
through	O
the	O
joint	O
probability	B
distribution	O
pxck	O
and	O
so	O
we	O
seek	O
instead	O
to	O
minimize	O
the	O
average	O
loss	O
where	O
the	O
average	O
is	O
computed	O
with	O
respect	O
to	O
this	O
distribution	O
which	O
is	O
given	O
by	O
el	O
k	O
j	O
rj	O
lkjpxck	O
dx	O
each	O
x	O
can	O
be	O
assigned	O
independently	O
to	O
one	O
of	O
the	O
decision	O
regions	O
rj	O
our	O
goal	O
is	O
to	O
choose	O
the	O
regions	O
rj	O
in	O
order	O
to	O
minimize	O
the	O
expected	O
loss	O
which	O
k	O
lkjpxck	O
as	O
before	O
we	O
can	O
use	O
implies	O
that	O
for	O
each	O
x	O
we	O
should	O
minimize	O
the	O
product	B
rule	I
pxck	O
pckxpx	O
to	O
eliminate	O
the	O
common	O
factor	O
of	O
px	O
thus	O
the	O
decision	O
rule	O
that	O
minimizes	O
the	O
expected	O
loss	O
is	O
the	O
one	O
that	O
assigns	O
each	O
introduction	O
figure	O
illustration	O
of	O
the	O
reject	B
option	I
inputs	O
x	O
such	O
that	O
the	O
larger	O
of	O
the	O
two	O
posterior	O
probabilities	O
is	O
less	O
than	O
or	O
equal	O
to	O
some	O
threshold	O
will	O
be	O
rejected	O
reject	O
region	O
x	O
new	O
x	O
to	O
the	O
class	O
j	O
for	O
which	O
the	O
lkjpckx	O
k	O
is	O
a	O
minimum	O
this	O
is	O
clearly	O
trivial	O
to	O
do	O
once	O
we	O
know	O
the	O
posterior	O
class	O
probabilities	O
pckx	O
the	O
reject	B
option	I
we	O
have	O
seen	O
that	O
classification	B
errors	O
arise	O
from	O
the	O
regions	O
of	O
input	O
space	O
where	O
the	O
largest	O
of	O
the	O
posterior	O
probabilities	O
pckx	O
is	O
significantly	O
less	O
than	O
unity	O
or	O
equivalently	O
where	O
the	O
joint	O
distributions	O
pxck	O
have	O
comparable	O
values	O
these	O
are	O
the	O
regions	O
where	O
we	O
are	O
relatively	O
uncertain	O
about	O
class	O
membership	O
in	O
some	O
applications	O
it	O
will	O
be	O
appropriate	O
to	O
avoid	O
making	O
decisions	O
on	O
the	O
difficult	O
cases	O
in	O
anticipation	O
of	O
a	O
lower	O
error	B
rate	O
on	O
those	O
examples	O
for	O
which	O
a	O
classification	B
decision	O
is	O
made	O
this	O
is	O
known	O
as	O
the	O
reject	B
option	I
for	O
example	O
in	O
our	O
hypothetical	O
medical	O
illustration	O
it	O
may	O
be	O
appropriate	O
to	O
use	O
an	O
automatic	O
system	O
to	O
classify	O
those	O
x-ray	O
images	O
for	O
which	O
there	O
is	O
little	O
doubt	O
as	O
to	O
the	O
correct	O
class	O
while	O
leaving	O
a	O
human	O
expert	O
to	O
classify	O
the	O
more	O
ambiguous	O
cases	O
we	O
can	O
achieve	O
this	O
by	O
introducing	O
a	O
threshold	O
and	O
rejecting	O
those	O
inputs	O
x	O
for	O
which	O
the	O
largest	O
of	O
the	O
posterior	O
probabilities	O
pckx	O
is	O
less	O
than	O
or	O
equal	O
to	O
this	O
is	O
illustrated	O
for	O
the	O
case	O
of	O
two	O
classes	O
and	O
a	O
single	O
continuous	O
input	O
variable	O
x	O
in	O
figure	O
note	O
that	O
setting	O
will	O
ensure	O
that	O
all	O
examples	O
are	O
rejected	O
whereas	O
if	O
there	O
are	O
k	O
classes	O
then	O
setting	O
will	O
ensure	O
that	O
no	O
examples	O
are	O
rejected	O
thus	O
the	O
fraction	O
of	O
examples	O
that	O
get	O
rejected	O
is	O
controlled	O
by	O
the	O
value	O
of	O
we	O
can	O
easily	O
extend	O
the	O
reject	O
criterion	O
to	O
minimize	O
the	O
expected	O
loss	O
when	O
a	O
loss	B
matrix	I
is	O
given	O
taking	O
account	O
of	O
the	O
loss	O
incurred	O
when	O
a	O
reject	O
decision	O
is	O
made	O
inference	B
and	O
decision	O
we	O
have	O
broken	O
the	O
classification	B
problem	O
down	O
into	O
two	O
separate	O
stages	O
the	O
inference	B
stage	O
in	O
which	O
we	O
use	O
training	B
data	O
to	O
learn	O
a	O
model	O
for	O
pckx	O
and	O
the	O
exercise	O
decision	B
theory	B
subsequent	O
decision	O
stage	O
in	O
which	O
we	O
use	O
these	O
posterior	O
probabilities	O
to	O
make	O
optimal	O
class	O
assignments	O
an	O
alternative	O
possibility	O
would	O
be	O
to	O
solve	O
both	O
problems	O
together	O
and	O
simply	O
learn	O
a	O
function	O
that	O
maps	O
inputs	O
x	O
directly	O
into	O
decisions	O
such	O
a	O
function	O
is	O
called	O
a	O
discriminant	B
function	I
in	O
fact	O
we	O
can	O
identify	O
three	O
distinct	O
approaches	O
to	O
solving	O
decision	O
problems	O
all	O
of	O
which	O
have	O
been	O
used	O
in	O
practical	O
applications	O
these	O
are	O
given	O
in	O
decreasing	O
order	O
of	O
complexity	O
by	O
first	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
class-conditional	O
densities	O
pxck	O
for	O
each	O
class	O
ck	O
individually	O
also	O
separately	O
infer	O
the	O
prior	B
class	O
probabilities	O
pck	O
then	O
use	O
bayes	B
theorem	O
in	O
the	O
form	O
pckx	O
pxckpck	O
to	O
find	O
the	O
posterior	O
class	O
probabilities	O
pckx	O
as	O
usual	O
the	O
denominator	O
in	O
bayes	B
theorem	O
can	O
be	O
found	O
in	O
terms	O
of	O
the	O
quantities	O
appearing	O
in	O
the	O
numerator	O
because	O
px	O
pxckpck	O
px	O
k	O
equivalently	O
we	O
can	O
model	O
the	O
joint	O
distribution	O
pxck	O
directly	O
and	O
then	O
normalize	O
to	O
obtain	O
the	O
posterior	O
probabilities	O
having	O
found	O
the	O
posterior	O
probabilities	O
we	O
use	O
decision	B
theory	B
to	O
determine	O
class	O
membership	O
for	O
each	O
new	O
input	O
x	O
approaches	O
that	O
explicitly	O
or	O
implicitly	O
model	O
the	O
distribution	O
of	O
inputs	O
as	O
well	O
as	O
outputs	O
are	O
known	O
as	O
generative	O
models	O
because	O
by	O
sampling	O
from	O
them	O
it	O
is	O
possible	O
to	O
generate	O
synthetic	O
data	O
points	O
in	O
the	O
input	O
space	O
first	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
posterior	O
class	O
probabilities	O
pckx	O
and	O
then	O
subsequently	O
use	O
decision	B
theory	B
to	O
assign	O
each	O
new	O
x	O
to	O
one	O
of	O
the	O
classes	O
approaches	O
that	O
model	O
the	O
posterior	O
probabilities	O
directly	O
are	O
called	O
discriminative	O
models	O
find	O
a	O
function	O
fx	O
called	O
a	O
discriminant	B
function	I
which	O
maps	O
each	O
input	O
x	O
directly	O
onto	O
a	O
class	O
label	O
for	O
instance	O
in	O
the	O
case	O
of	O
two-class	O
problems	O
f	O
might	O
be	O
binary	O
valued	O
and	O
such	O
that	O
f	O
represents	O
class	O
and	O
f	O
represents	O
class	O
in	O
this	O
case	O
probabilities	O
play	O
no	O
role	O
let	O
us	O
consider	O
the	O
relative	B
merits	O
of	O
these	O
three	O
alternatives	O
approach	O
is	O
the	O
most	O
demanding	O
because	O
it	O
involves	O
finding	O
the	O
joint	O
distribution	O
over	O
both	O
x	O
and	O
ck	O
for	O
many	O
applications	O
x	O
will	O
have	O
high	O
dimensionality	O
and	O
consequently	O
we	O
may	O
need	O
a	O
large	O
training	B
set	I
in	O
order	O
to	O
be	O
able	O
to	O
determine	O
the	O
class-conditional	O
densities	O
to	O
reasonable	O
accuracy	O
note	O
that	O
the	O
class	O
priors	O
pck	O
can	O
often	O
be	O
estimated	O
simply	O
from	O
the	O
fractions	O
of	O
the	O
training	B
set	I
data	O
points	O
in	O
each	O
of	O
the	O
classes	O
one	O
advantage	O
of	O
approach	O
however	O
is	O
that	O
it	O
also	O
allows	O
the	O
marginal	B
density	B
of	O
data	O
px	O
to	O
be	O
determined	O
from	O
this	O
can	O
be	O
useful	O
for	O
detecting	O
new	O
data	O
points	O
that	O
have	O
low	O
probability	B
under	O
the	O
model	O
and	O
for	O
which	O
the	O
predictions	O
may	O
introduction	O
s	O
e	O
i	O
t	O
i	O
s	O
n	O
e	O
d	O
s	O
s	O
a	O
c	O
l	O
x	O
x	O
figure	O
example	O
of	O
the	O
class-conditional	O
densities	O
for	O
two	O
classes	O
having	O
a	O
single	O
input	O
variable	O
x	O
plot	O
together	O
with	O
the	O
corresponding	O
posterior	O
probabilities	O
plot	O
note	O
that	O
the	O
left-hand	O
mode	O
of	O
the	O
class-conditional	O
density	B
shown	O
in	O
blue	O
on	O
the	O
left	O
plot	O
has	O
no	O
effect	O
on	O
the	O
posterior	O
probabilities	O
the	O
vertical	O
green	O
line	O
in	O
the	O
right	O
plot	O
shows	O
the	O
decision	B
boundary	I
in	O
x	O
that	O
gives	O
the	O
minimum	O
misclassification	O
rate	O
be	O
of	O
low	O
accuracy	O
which	O
is	O
known	O
as	O
outlier	B
detection	O
or	O
novelty	B
detection	I
tarassenko	O
however	O
if	O
we	O
only	O
wish	O
to	O
make	O
classification	B
decisions	O
then	O
it	O
can	O
be	O
wasteful	O
of	O
computational	O
resources	O
and	O
excessively	O
demanding	O
of	O
data	O
to	O
find	O
the	O
joint	O
distribution	O
pxck	O
when	O
in	O
fact	O
we	O
only	O
really	O
need	O
the	O
posterior	O
probabilities	O
pckx	O
which	O
can	O
be	O
obtained	O
directly	O
through	O
approach	O
indeed	O
the	O
classconditional	O
densities	O
may	O
contain	O
a	O
lot	O
of	O
structure	O
that	O
has	O
little	O
effect	O
on	O
the	O
posterior	O
probabilities	O
as	O
illustrated	O
in	O
figure	O
there	O
has	O
been	O
much	O
interest	O
in	O
exploring	O
the	O
relative	B
merits	O
of	O
generative	O
and	O
discriminative	O
approaches	O
to	O
machine	O
learning	B
and	O
in	O
finding	O
ways	O
to	O
combine	O
them	O
lasserre	O
et	O
al	O
an	O
even	O
simpler	O
approach	O
is	O
in	O
which	O
we	O
use	O
the	O
training	B
data	O
to	O
find	O
a	O
discriminant	B
function	I
fx	O
that	O
maps	O
each	O
x	O
directly	O
onto	O
a	O
class	O
label	O
thereby	O
combining	O
the	O
inference	B
and	O
decision	O
stages	O
into	O
a	O
single	O
learning	B
problem	O
in	O
the	O
example	O
of	O
figure	O
this	O
would	O
correspond	O
to	O
finding	O
the	O
value	O
of	O
x	O
shown	O
by	O
the	O
vertical	O
green	O
line	O
because	O
this	O
is	O
the	O
decision	B
boundary	I
giving	O
the	O
minimum	O
probability	B
of	O
misclassification	O
with	O
option	O
however	O
we	O
no	O
longer	O
have	O
access	O
to	O
the	O
posterior	O
probabilities	O
pckx	O
there	O
are	O
many	O
powerful	O
reasons	O
for	O
wanting	O
to	O
compute	O
the	O
posterior	O
probabilities	O
even	O
if	O
we	O
subsequently	O
use	O
them	O
to	O
make	O
decisions	O
these	O
include	O
minimizing	O
risk	O
consider	O
a	O
problem	O
in	O
which	O
the	O
elements	O
of	O
the	O
loss	B
matrix	I
are	O
subjected	O
to	O
revision	O
from	O
time	O
to	O
time	O
as	O
might	O
occur	O
in	O
a	O
financial	O
decision	B
theory	B
application	O
if	O
we	O
know	O
the	O
posterior	O
probabilities	O
we	O
can	O
trivially	O
revise	O
the	O
minimum	B
risk	I
decision	O
criterion	O
by	O
modifying	O
appropriately	O
if	O
we	O
have	O
only	O
a	O
discriminant	B
function	I
then	O
any	O
change	O
to	O
the	O
loss	B
matrix	I
would	O
require	O
that	O
we	O
return	O
to	O
the	O
training	B
data	O
and	O
solve	O
the	O
classification	B
problem	O
afresh	O
reject	B
option	I
posterior	O
probabilities	O
allow	O
us	O
to	O
determine	O
a	O
rejection	O
criterion	O
that	O
will	O
minimize	O
the	O
misclassification	O
rate	O
or	O
more	O
generally	O
the	O
expected	O
loss	O
for	O
a	O
given	O
fraction	O
of	O
rejected	O
data	O
points	O
compensating	O
for	O
class	O
priors	O
consider	O
our	O
medical	O
x-ray	O
problem	O
again	O
and	O
suppose	O
that	O
we	O
have	O
collected	O
a	O
large	O
number	O
of	O
x-ray	O
images	O
from	O
the	O
general	O
population	O
for	O
use	O
as	O
training	B
data	O
in	O
order	O
to	O
build	O
an	O
automated	O
screening	O
system	O
because	O
cancer	O
is	O
rare	O
amongst	O
the	O
general	O
population	O
we	O
might	O
find	O
that	O
say	O
only	O
in	O
every	O
examples	O
corresponds	O
to	O
the	O
presence	O
of	O
cancer	O
if	O
we	O
used	O
such	O
a	O
data	O
set	O
to	O
train	O
an	O
adaptive	O
model	O
we	O
could	O
run	O
into	O
severe	O
difficulties	O
due	O
to	O
the	O
small	O
proportion	O
of	O
the	O
cancer	O
class	O
for	O
instance	O
a	O
classifier	O
that	O
assigned	O
every	O
point	O
to	O
the	O
normal	O
class	O
would	O
already	O
achieve	O
accuracy	O
and	O
it	O
would	O
be	O
difficult	O
to	O
avoid	O
this	O
trivial	O
solution	O
also	O
even	O
a	O
large	O
data	O
set	O
will	O
contain	O
very	O
few	O
examples	O
of	O
x-ray	O
images	O
corresponding	O
to	O
cancer	O
and	O
so	O
the	O
learning	B
algorithm	O
will	O
not	O
be	O
exposed	O
to	O
a	O
broad	O
range	O
of	O
examples	O
of	O
such	O
images	O
and	O
hence	O
is	O
not	O
likely	O
to	O
generalize	O
well	O
a	O
balanced	O
data	O
set	O
in	O
which	O
we	O
have	O
selected	O
equal	O
numbers	O
of	O
examples	O
from	O
each	O
of	O
the	O
classes	O
would	O
allow	O
us	O
to	O
find	O
a	O
more	O
accurate	O
model	O
however	O
we	O
then	O
have	O
to	O
compensate	O
for	O
the	O
effects	O
of	O
our	O
modifications	O
to	O
the	O
training	B
data	O
suppose	O
we	O
have	O
used	O
such	O
a	O
modified	O
data	O
set	O
and	O
found	O
models	O
for	O
the	O
posterior	O
probabilities	O
from	O
bayes	B
theorem	O
we	O
see	O
that	O
the	O
posterior	O
probabilities	O
are	O
proportional	O
to	O
the	O
prior	B
probabilities	O
which	O
we	O
can	O
interpret	O
as	O
the	O
fractions	O
of	O
points	O
in	O
each	O
class	O
we	O
can	O
therefore	O
simply	O
take	O
the	O
posterior	O
probabilities	O
obtained	O
from	O
our	O
artificially	O
balanced	O
data	O
set	O
and	O
first	O
divide	O
by	O
the	O
class	O
fractions	O
in	O
that	O
data	O
set	O
and	O
then	O
multiply	O
by	O
the	O
class	O
fractions	O
in	O
the	O
population	O
to	O
which	O
we	O
wish	O
to	O
apply	O
the	O
model	O
finally	O
we	O
need	O
to	O
normalize	O
to	O
ensure	O
that	O
the	O
new	O
posterior	O
probabilities	O
sum	O
to	O
one	O
note	O
that	O
this	O
procedure	O
cannot	O
be	O
applied	O
if	O
we	O
have	O
learned	O
a	O
discriminant	B
function	I
directly	O
instead	O
of	O
determining	O
posterior	O
probabilities	O
combining	B
models	I
for	O
complex	O
applications	O
we	O
may	O
wish	O
to	O
break	O
the	O
problem	O
into	O
a	O
number	O
of	O
smaller	O
subproblems	O
each	O
of	O
which	O
can	O
be	O
tackled	O
by	O
a	O
separate	O
module	O
for	O
example	O
in	O
our	O
hypothetical	O
medical	O
diagnosis	O
problem	O
we	O
may	O
have	O
information	O
available	O
from	O
say	O
blood	O
tests	O
as	O
well	O
as	O
x-ray	O
images	O
rather	O
than	O
combine	O
all	O
of	O
this	O
heterogeneous	O
information	O
into	O
one	O
huge	O
input	O
space	O
it	O
may	O
be	O
more	O
effective	O
to	O
build	O
one	O
system	O
to	O
interpret	O
the	O
xray	O
images	O
and	O
a	O
different	O
one	O
to	O
interpret	O
the	O
blood	O
data	O
as	O
long	O
as	O
each	O
of	O
the	O
two	O
models	O
gives	O
posterior	O
probabilities	O
for	O
the	O
classes	O
we	O
can	O
combine	O
the	O
outputs	O
systematically	O
using	O
the	O
rules	O
of	O
probability	B
one	O
simple	O
way	O
to	O
do	O
this	O
is	O
to	O
assume	O
that	O
for	O
each	O
class	O
separately	O
the	O
distributions	O
of	O
inputs	O
for	O
the	O
x-ray	O
images	O
denoted	O
by	O
xi	O
and	O
the	O
blood	O
data	O
denoted	O
by	O
xb	O
are	O
introduction	O
independent	B
so	O
that	O
pxi	O
xbck	O
pxickpxbck	O
section	O
section	O
section	O
appendix	O
d	O
this	O
is	O
an	O
example	O
of	O
conditional	B
independence	I
property	O
because	O
the	O
independence	O
holds	O
when	O
the	O
distribution	O
is	O
conditioned	O
on	O
the	O
class	O
ck	O
the	O
posterior	B
probability	B
given	O
both	O
the	O
x-ray	O
and	O
blood	O
data	O
is	O
then	O
given	O
by	O
pckxi	O
xb	O
pxi	O
xbckpck	O
pxickpxbckpck	O
pckxipckxb	O
pck	O
thus	O
we	O
need	O
the	O
class	O
prior	B
probabilities	O
pck	O
which	O
we	O
can	O
easily	O
estimate	O
from	O
the	O
fractions	O
of	O
data	O
points	O
in	O
each	O
class	O
and	O
then	O
we	O
need	O
to	O
normalize	O
the	O
resulting	O
posterior	O
probabilities	O
so	O
they	O
sum	O
to	O
one	O
the	O
particular	O
conditional	B
independence	I
assumption	O
is	O
an	O
example	O
of	O
the	O
naive	B
bayes	B
model	I
note	O
that	O
the	O
joint	O
marginal	B
distribution	O
pxi	O
xb	O
will	O
typically	O
not	O
factorize	O
under	O
this	O
model	O
we	O
shall	O
see	O
in	O
later	O
chapters	O
how	O
to	O
construct	O
models	O
for	O
combining	O
data	O
that	O
do	O
not	O
require	O
the	O
conditional	B
independence	I
assumption	O
loss	O
functions	O
for	B
regression	B
so	O
far	O
we	O
have	O
discussed	O
decision	B
theory	B
in	O
the	O
context	O
of	O
classification	B
problems	O
we	O
now	O
turn	O
to	O
the	O
case	O
of	O
regression	B
problems	O
such	O
as	O
the	O
curve	B
fitting	I
example	O
discussed	O
earlier	O
the	O
decision	O
stage	O
consists	O
of	O
choosing	O
a	O
specific	O
estimate	O
yx	O
of	O
the	O
value	O
of	O
t	O
for	O
each	O
input	O
x	O
suppose	O
that	O
in	O
doing	O
so	O
we	O
incur	O
a	O
loss	O
lt	O
yx	O
the	O
average	O
or	O
expected	O
loss	O
is	O
then	O
given	O
by	O
el	O
lt	O
yxpx	O
t	O
dx	O
dt	O
a	O
common	O
choice	O
of	O
loss	B
function	I
in	O
regression	B
problems	O
is	O
the	O
squared	O
loss	O
given	O
by	O
lt	O
yx	O
in	O
this	O
case	O
the	O
expected	O
loss	O
can	O
be	O
written	O
el	O
t	O
dx	O
dt	O
our	O
goal	O
is	O
to	O
choose	O
yx	O
so	O
as	O
to	O
minimize	O
el	O
if	O
we	O
assume	O
a	O
completely	O
flexible	O
function	O
yx	O
we	O
can	O
do	O
this	O
formally	O
using	O
the	O
calculus	B
of	I
variations	I
to	O
give	O
tpx	O
t	O
dt	O
solving	O
for	O
yx	O
and	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
we	O
obtain	O
el	O
yx	O
yx	O
tpx	O
t	O
dt	O
px	O
tptx	O
dt	O
ettx	O
figure	O
the	O
regression	B
function	I
yx	O
which	O
minimizes	O
the	O
expected	O
squared	O
loss	O
is	O
given	O
by	O
the	O
mean	B
of	O
the	O
conditional	B
distribution	O
ptx	O
t	O
decision	B
theory	B
yx	O
x	O
which	O
is	O
the	O
conditional	B
average	O
of	O
t	O
conditioned	O
on	O
x	O
and	O
is	O
known	O
as	O
the	O
regression	B
function	I
this	O
result	O
is	O
illustrated	O
in	O
figure	O
it	O
can	O
readily	O
be	O
extended	B
to	O
multiple	O
target	O
variables	O
represented	O
by	O
the	O
vector	O
t	O
in	O
which	O
case	O
the	O
optimal	O
solution	O
is	O
the	O
conditional	B
average	O
yx	O
ettx	O
exercise	O
we	O
can	O
also	O
derive	O
this	O
result	O
in	O
a	O
slightly	O
different	O
way	O
which	O
will	O
also	O
shed	O
light	O
on	O
the	O
nature	O
of	O
the	O
regression	B
problem	O
armed	O
with	O
the	O
knowledge	O
that	O
the	O
optimal	O
solution	O
is	O
the	O
conditional	B
expectation	B
we	O
can	O
expand	O
the	O
square	O
term	O
as	O
follows	O
etx	O
etx	O
etxetx	O
t	O
where	O
to	O
keep	O
the	O
notation	O
uncluttered	O
we	O
use	O
etx	O
to	O
denote	O
ettx	O
substituting	O
into	O
the	O
loss	B
function	I
and	O
performing	O
the	O
integral	O
over	O
t	O
we	O
see	O
that	O
the	O
cross-term	O
vanishes	O
and	O
we	O
obtain	O
an	O
expression	O
for	O
the	O
loss	B
function	I
in	O
the	O
form	O
dx	O
px	O
dx	O
el	O
the	O
function	O
yx	O
we	O
seek	O
to	O
determine	O
enters	O
only	O
in	O
the	O
first	O
term	O
which	O
will	O
be	O
minimized	O
when	O
yx	O
is	O
equal	O
to	O
etx	O
in	O
which	O
case	O
this	O
term	O
will	O
vanish	O
this	O
is	O
simply	O
the	O
result	O
that	O
we	O
derived	O
previously	O
and	O
that	O
shows	O
that	O
the	O
optimal	O
least	O
squares	O
predictor	O
is	O
given	O
by	O
the	O
conditional	B
mean	B
the	O
second	O
term	O
is	O
the	O
variance	B
of	O
the	O
distribution	O
of	O
t	O
averaged	O
over	O
x	O
it	O
represents	O
the	O
intrinsic	O
variability	O
of	O
the	O
target	O
data	O
and	O
can	O
be	O
regarded	O
as	O
noise	O
because	O
it	O
is	O
independent	B
of	O
yx	O
it	O
represents	O
the	O
irreducible	O
minimum	O
value	O
of	O
the	O
loss	B
function	I
as	O
with	O
the	O
classification	B
problem	O
we	O
can	O
either	O
determine	O
the	O
appropriate	O
probabilities	O
and	O
then	O
use	O
these	O
to	O
make	O
optimal	O
decisions	O
or	O
we	O
can	O
build	O
models	O
that	O
make	O
decisions	O
directly	O
indeed	O
we	O
can	O
identify	O
three	O
distinct	O
approaches	O
to	O
solving	O
regression	B
problems	O
given	O
in	O
order	O
of	O
decreasing	O
complexity	O
by	O
first	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
joint	O
density	B
px	O
t	O
then	O
normalize	O
to	O
find	O
the	O
conditional	B
density	B
ptx	O
and	O
finally	O
marginalize	O
to	O
find	O
the	O
conditional	B
mean	B
given	O
by	O
introduction	O
first	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
conditional	B
density	B
ptx	O
and	O
then	O
subsequently	O
marginalize	O
to	O
find	O
the	O
conditional	B
mean	B
given	O
by	O
find	O
a	O
regression	B
function	I
yx	O
directly	O
from	O
the	O
training	B
data	O
the	O
relative	B
merits	O
of	O
these	O
three	O
approaches	O
follow	O
the	O
same	O
lines	O
as	O
for	O
classification	B
problems	O
above	O
the	O
squared	O
loss	O
is	O
not	O
the	O
only	O
possible	O
choice	O
of	O
loss	B
function	I
for	B
regression	B
indeed	O
there	O
are	O
situations	O
in	O
which	O
squared	O
loss	O
can	O
lead	O
to	O
very	O
poor	O
results	O
and	O
where	O
we	O
need	O
to	O
develop	O
more	O
sophisticated	O
approaches	O
an	O
important	O
example	O
concerns	O
situations	O
in	O
which	O
the	O
conditional	B
distribution	O
ptx	O
is	O
multimodal	O
as	O
often	O
arises	O
in	O
the	O
solution	O
of	O
inverse	B
problems	O
here	O
we	O
consider	O
briefly	O
one	O
simple	O
generalization	B
of	O
the	O
squared	O
loss	O
called	O
the	O
minkowski	B
loss	I
whose	O
expectation	B
is	O
given	O
by	O
tqpx	O
t	O
dx	O
dt	O
elq	O
which	O
reduces	O
to	O
the	O
expected	O
squared	O
loss	O
for	O
q	O
the	O
function	O
tq	O
is	O
plotted	O
against	O
y	O
t	O
for	O
various	O
values	O
of	O
q	O
in	O
figure	O
the	O
minimum	O
of	O
elq	O
is	O
given	O
by	O
the	O
conditional	B
mean	B
for	O
q	O
the	O
conditional	B
median	O
for	O
q	O
and	O
the	O
conditional	B
mode	O
for	O
q	O
section	O
exercise	O
information	B
theory	B
in	O
this	O
chapter	O
we	O
have	O
discussed	O
a	O
variety	O
of	O
concepts	O
from	O
probability	B
theory	B
and	O
decision	B
theory	B
that	O
will	O
form	O
the	O
foundations	O
for	O
much	O
of	O
the	O
subsequent	O
discussion	O
in	O
this	O
book	O
we	O
close	O
this	O
chapter	O
by	O
introducing	O
some	O
additional	O
concepts	O
from	O
the	O
field	O
of	O
information	B
theory	B
which	O
will	O
also	O
prove	O
useful	O
in	O
our	O
development	O
of	O
pattern	O
recognition	O
and	O
machine	O
learning	B
techniques	O
again	O
we	O
shall	O
focus	O
only	O
on	O
the	O
key	O
concepts	O
and	O
we	O
refer	O
the	O
reader	O
elsewhere	O
for	O
more	O
detailed	O
discussions	O
and	O
omura	O
cover	O
and	O
thomas	O
mackay	O
we	O
begin	O
by	O
considering	O
a	O
discrete	O
random	O
variable	O
x	O
and	O
we	O
ask	O
how	O
much	O
information	O
is	O
received	O
when	O
we	O
observe	O
a	O
specific	O
value	O
for	O
this	O
variable	O
the	O
amount	O
of	O
information	O
can	O
be	O
viewed	O
as	O
the	O
degree	O
of	O
surprise	O
on	O
learning	B
the	O
value	O
of	O
x	O
if	O
we	O
are	O
told	O
that	O
a	O
highly	O
improbable	O
event	O
has	O
just	O
occurred	O
we	O
will	O
have	O
received	O
more	O
information	O
than	O
if	O
we	O
were	O
told	O
that	O
some	O
very	O
likely	O
event	O
has	O
just	O
occurred	O
and	O
if	O
we	O
knew	O
that	O
the	O
event	O
was	O
certain	O
to	O
happen	O
we	O
would	O
receive	O
no	O
information	O
our	O
measure	O
of	O
information	O
content	O
will	O
therefore	O
depend	O
on	O
the	O
probability	B
distribution	O
px	O
and	O
we	O
therefore	O
look	O
for	O
a	O
quantity	O
hx	O
that	O
is	O
a	O
monotonic	O
function	O
of	O
the	O
probability	B
px	O
and	O
that	O
expresses	O
the	O
information	O
content	O
the	O
form	O
of	O
h	O
can	O
be	O
found	O
by	O
noting	O
that	O
if	O
we	O
have	O
two	O
events	O
x	O
and	O
y	O
that	O
are	O
unrelated	O
then	O
the	O
information	O
gain	O
from	O
observing	O
both	O
of	O
them	O
should	O
be	O
the	O
sum	O
of	O
the	O
information	O
gained	O
from	O
each	O
of	O
them	O
separately	O
so	O
that	O
hx	O
y	O
hx	O
hy	O
two	O
unrelated	O
events	O
will	O
be	O
statistically	O
independent	B
and	O
so	O
px	O
y	O
pxpy	O
from	O
these	O
two	O
relationships	O
it	O
is	O
easily	O
shown	O
that	O
hx	O
must	O
be	O
given	O
by	O
the	O
logarithm	O
of	O
px	O
and	O
so	O
we	O
have	O
exercise	O
q	O
t	O
y	O
q	O
t	O
y	O
q	O
y	O
t	O
q	O
y	O
t	O
q	O
t	O
y	O
q	O
t	O
y	O
information	B
theory	B
q	O
y	O
t	O
q	O
y	O
t	O
figure	O
plots	O
of	O
the	O
quantity	O
lq	O
tq	O
for	O
various	O
values	O
of	O
q	O
hx	O
px	O
where	O
the	O
negative	O
sign	O
ensures	O
that	O
information	O
is	O
positive	O
or	O
zero	O
note	O
that	O
low	O
probability	B
events	O
x	O
correspond	O
to	O
high	O
information	O
content	O
the	O
choice	O
of	O
basis	O
for	O
the	O
logarithm	O
is	O
arbitrary	O
and	O
for	O
the	O
moment	O
we	O
shall	O
adopt	O
the	O
convention	O
prevalent	O
in	O
information	B
theory	B
of	O
using	O
logarithms	O
to	O
the	O
base	O
of	O
in	O
this	O
case	O
as	O
we	O
shall	O
see	O
shortly	O
the	O
units	O
of	O
hx	O
are	O
bits	B
binary	O
digits	O
now	O
suppose	O
that	O
a	O
sender	O
wishes	O
to	O
transmit	O
the	O
value	O
of	O
a	O
random	O
variable	O
to	O
a	O
receiver	O
the	O
average	O
amount	O
of	O
information	O
that	O
they	O
transmit	O
in	O
the	O
process	O
is	O
obtained	O
by	O
taking	O
the	O
expectation	B
of	O
with	O
respect	O
to	O
the	O
distribution	O
px	O
and	O
is	O
given	O
by	O
hx	O
px	O
px	O
x	O
this	O
important	O
quantity	O
is	O
called	O
the	O
entropy	B
of	O
the	O
random	O
variable	O
x	O
note	O
that	O
limp	O
p	O
ln	O
p	O
and	O
so	O
we	O
shall	O
take	O
px	O
ln	O
px	O
whenever	O
we	O
encounter	O
a	O
value	O
for	O
x	O
such	O
that	O
px	O
so	O
far	O
we	O
have	O
given	O
a	O
rather	O
heuristic	O
motivation	O
for	O
the	O
definition	O
of	O
informa	O
introduction	O
tion	O
and	O
the	O
corresponding	O
entropy	B
we	O
now	O
show	O
that	O
these	O
definitions	O
indeed	O
possess	O
useful	O
properties	O
consider	O
a	O
random	O
variable	O
x	O
having	O
possible	O
states	O
each	O
of	O
which	O
is	O
equally	O
likely	O
in	O
order	O
to	O
communicate	O
the	O
value	O
of	O
x	O
to	O
a	O
receiver	O
we	O
would	O
need	O
to	O
transmit	O
a	O
message	O
of	O
length	O
bits	B
notice	O
that	O
the	O
entropy	B
of	O
this	O
variable	O
is	O
given	O
by	O
hx	O
bits	B
now	O
consider	O
an	O
example	O
and	O
thomas	O
of	O
a	O
variable	O
having	O
possible	O
states	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
for	O
which	O
the	O
respective	O
probabilities	O
are	O
given	O
by	O
hx	O
the	O
entropy	B
in	O
this	O
case	O
is	O
given	O
by	O
bits	B
we	O
see	O
that	O
the	O
nonuniform	O
distribution	O
has	O
a	O
smaller	O
entropy	B
than	O
the	O
uniform	O
one	O
and	O
we	O
shall	O
gain	O
some	O
insight	O
into	O
this	O
shortly	O
when	O
we	O
discuss	O
the	O
interpretation	O
of	O
entropy	B
in	O
terms	O
of	O
disorder	O
for	O
the	O
moment	O
let	O
us	O
consider	O
how	O
we	O
would	O
transmit	O
the	O
identity	O
of	O
the	O
variable	O
s	O
state	O
to	O
a	O
receiver	O
we	O
could	O
do	O
this	O
as	O
before	O
using	O
a	O
number	O
however	O
we	O
can	O
take	O
advantage	O
of	O
the	O
nonuniform	O
distribution	O
by	O
using	O
shorter	O
codes	O
for	O
the	O
more	O
probable	O
events	O
at	O
the	O
expense	O
of	O
longer	O
codes	O
for	O
the	O
less	O
probable	O
events	O
in	O
the	O
hope	O
of	O
getting	O
a	O
shorter	O
average	O
code	O
length	O
this	O
can	O
be	O
done	O
by	O
representing	O
the	O
states	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
using	O
for	O
instance	O
the	O
following	O
set	O
of	O
code	O
strings	O
the	O
average	O
length	O
of	O
the	O
code	O
that	O
has	O
to	O
be	O
transmitted	O
is	O
then	O
average	O
code	O
length	O
bits	B
which	O
again	O
is	O
the	O
same	O
as	O
the	O
entropy	B
of	O
the	O
random	O
variable	O
note	O
that	O
shorter	O
code	O
strings	O
cannot	O
be	O
used	O
because	O
it	O
must	O
be	O
possible	O
to	O
disambiguate	O
a	O
concatenation	O
of	O
such	O
strings	O
into	O
its	O
component	O
parts	O
for	O
instance	O
decodes	O
uniquely	O
into	O
the	O
state	O
sequence	O
c	O
a	O
d	O
this	O
relation	O
between	O
entropy	B
and	O
shortest	O
coding	O
length	O
is	O
a	O
general	O
one	O
the	O
noiseless	B
coding	I
theorem	I
states	O
that	O
the	O
entropy	B
is	O
a	O
lower	B
bound	I
on	O
the	O
number	O
of	O
bits	B
needed	O
to	O
transmit	O
the	O
state	O
of	O
a	O
random	O
variable	O
from	O
now	O
on	O
we	O
shall	O
switch	O
to	O
the	O
use	O
of	O
natural	O
logarithms	O
in	O
defining	O
entropy	B
as	O
this	O
will	O
provide	O
a	O
more	O
convenient	O
link	B
with	O
ideas	O
elsewhere	O
in	O
this	O
book	O
in	O
this	O
case	O
the	O
entropy	B
is	O
measured	O
in	O
units	O
of	O
nats	B
instead	O
of	O
bits	B
which	O
differ	O
simply	O
by	O
a	O
factor	O
of	O
ln	O
we	O
have	O
introduced	O
the	O
concept	O
of	O
entropy	B
in	O
terms	O
of	O
the	O
average	O
amount	O
of	O
information	O
needed	O
to	O
specify	O
the	O
state	O
of	O
a	O
random	O
variable	O
in	O
fact	O
the	O
concept	O
of	O
entropy	B
has	O
much	O
earlier	O
origins	O
in	O
physics	O
where	O
it	O
was	O
introduced	O
in	O
the	O
context	O
of	O
equilibrium	O
thermodynamics	O
and	O
later	O
given	O
a	O
deeper	O
interpretation	O
as	O
a	O
measure	O
of	O
disorder	O
through	O
developments	O
in	O
statistical	O
mechanics	O
we	O
can	O
understand	O
this	O
alternative	O
view	O
of	O
entropy	B
by	O
considering	O
a	O
set	O
of	O
n	O
identical	O
objects	O
that	O
are	O
to	O
be	O
divided	O
amongst	O
a	O
set	O
of	O
bins	O
such	O
that	O
there	O
are	O
ni	O
objects	O
in	O
the	O
ith	O
bin	O
consider	O
information	B
theory	B
the	O
number	O
of	O
different	O
ways	O
of	O
allocating	O
the	O
objects	O
to	O
the	O
bins	O
there	O
are	O
n	O
ways	O
to	O
choose	O
the	O
first	O
object	O
ways	O
to	O
choose	O
the	O
second	O
object	O
and	O
so	O
on	O
leading	O
to	O
a	O
total	O
of	O
n	O
ways	O
to	O
allocate	O
all	O
n	O
objects	O
to	O
the	O
bins	O
where	O
n	O
factorial	B
n	O
denotes	O
the	O
product	O
n	O
however	O
we	O
don	O
t	O
wish	O
to	O
distinguish	O
between	O
rearrangements	O
of	O
objects	O
within	O
each	O
bin	O
in	O
the	O
ith	O
bin	O
there	O
are	O
ni	O
ways	O
of	O
reordering	O
the	O
objects	O
and	O
so	O
the	O
total	O
number	O
of	O
ways	O
of	O
allocating	O
the	O
n	O
objects	O
to	O
the	O
bins	O
is	O
given	O
by	O
w	O
i	O
ni	O
which	O
is	O
called	O
the	O
multiplicity	B
the	O
entropy	B
is	O
then	O
defined	O
as	O
the	O
logarithm	O
of	O
the	O
multiplicity	B
scaled	O
by	O
an	O
appropriate	O
constant	O
h	O
n	O
ln	O
w	O
n	O
ln	O
n	O
n	O
ln	O
ni	O
we	O
now	O
consider	O
the	O
limit	O
n	O
in	O
which	O
the	O
fractions	O
nin	O
are	O
held	O
fixed	O
and	O
apply	O
stirling	O
s	O
approximation	O
i	O
which	O
gives	O
h	O
lim	O
n	O
i	O
ln	O
n	O
n	O
ln	O
n	O
n	O
ni	O
n	O
ln	O
ni	O
n	O
i	O
pi	O
ln	O
pi	O
i	O
ni	O
n	O
here	O
pi	O
limn	O
is	O
the	O
probability	B
where	O
we	O
have	O
used	O
of	O
an	O
object	O
being	O
assigned	O
to	O
the	O
ith	O
bin	O
in	O
physics	O
terminology	O
the	O
specific	O
arrangements	O
of	O
objects	O
in	O
the	O
bins	O
is	O
called	O
a	O
microstate	B
and	O
the	O
overall	O
distribution	O
of	O
occupation	O
numbers	O
expressed	O
through	O
the	O
ratios	O
nin	O
is	O
called	O
a	O
macrostate	B
the	O
multiplicity	B
w	O
is	O
also	O
known	O
as	O
the	O
weight	O
of	O
the	O
macrostate	B
we	O
can	O
interpret	O
the	O
bins	O
as	O
the	O
states	O
xi	O
of	O
a	O
discrete	O
random	O
variable	O
x	O
where	O
px	O
xi	O
pi	O
the	O
entropy	B
of	O
the	O
random	O
variable	O
x	O
is	O
then	O
hp	O
pxi	O
ln	O
pxi	O
i	O
appendix	O
e	O
distributions	O
pxi	O
that	O
are	O
sharply	O
peaked	O
around	O
a	O
few	O
values	O
will	O
have	O
a	O
relatively	O
low	O
entropy	B
whereas	O
those	O
that	O
are	O
spread	O
more	O
evenly	O
across	O
many	O
values	O
will	O
have	O
higher	O
entropy	B
as	O
illustrated	O
in	O
figure	O
because	O
pi	O
the	O
entropy	B
is	O
nonnegative	O
and	O
it	O
will	O
equal	O
its	O
minimum	O
value	O
of	O
when	O
one	O
of	O
the	O
pi	O
and	O
all	O
other	O
the	O
maximum	O
entropy	B
configuration	O
can	O
be	O
found	O
by	O
maximizing	O
h	O
using	O
a	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
normalization	O
constraint	O
on	O
the	O
probabilities	O
thus	O
we	O
maximize	O
pxi	O
ln	O
pxi	O
i	O
i	O
pxi	O
introduction	O
s	O
e	O
i	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
h	O
s	O
e	O
i	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
h	O
figure	O
histograms	O
of	O
two	O
probability	B
distributions	O
over	O
bins	O
illustrating	O
the	O
higher	O
value	O
of	O
the	O
entropy	B
h	O
for	O
the	O
broader	O
distribution	O
the	O
largest	O
entropy	B
would	O
arise	O
from	O
a	O
uniform	B
distribution	I
that	O
would	O
give	O
h	O
exercise	O
from	O
which	O
we	O
find	O
that	O
all	O
of	O
the	O
pxi	O
are	O
equal	O
and	O
are	O
given	O
by	O
pxi	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
states	O
xi	O
the	O
corresponding	O
value	O
of	O
the	O
entropy	B
is	O
then	O
h	O
ln	O
m	O
this	O
result	O
can	O
also	O
be	O
derived	O
from	O
jensen	O
s	O
inequality	O
be	O
discussed	O
shortly	O
to	O
verify	O
that	O
the	O
stationary	B
point	O
is	O
indeed	O
a	O
maximum	O
we	O
can	O
evaluate	O
the	O
second	O
derivative	B
of	O
the	O
entropy	B
which	O
gives	O
pxi	O
pxj	O
iij	O
pi	O
where	O
iij	O
are	O
the	O
elements	O
of	O
the	O
identity	O
matrix	O
we	O
can	O
extend	O
the	O
definition	O
of	O
entropy	B
to	O
include	O
distributions	O
px	O
over	O
continuous	O
variables	O
x	O
as	O
follows	O
first	O
divide	O
x	O
into	O
bins	O
of	O
width	O
then	O
assuming	O
px	O
is	O
continuous	O
the	O
mean	B
value	I
theorem	I
tells	O
us	O
that	O
for	O
each	O
such	O
bin	O
there	O
must	O
exist	O
a	O
value	O
xi	O
such	O
that	O
px	O
dx	O
pxi	O
i	O
we	O
can	O
now	O
quantize	O
the	O
continuous	O
variable	O
x	O
by	O
assigning	O
any	O
value	O
x	O
to	O
the	O
value	O
xi	O
whenever	O
x	O
falls	O
in	O
the	O
ith	O
bin	O
the	O
probability	B
of	O
observing	O
the	O
value	O
xi	O
is	O
then	O
pxi	O
this	O
gives	O
a	O
discrete	O
distribution	O
for	O
which	O
the	O
entropy	B
takes	O
the	O
form	O
h	O
pxi	O
ln	O
pxi	O
ln	O
pxi	O
ln	O
i	O
i	O
i	O
pxi	O
which	O
follows	O
from	O
we	O
now	O
omit	O
where	O
we	O
have	O
used	O
the	O
second	O
term	O
ln	O
on	O
the	O
right-hand	O
side	O
of	O
and	O
then	O
consider	O
the	O
limit	O
information	B
theory	B
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
will	O
approach	O
the	O
integral	O
of	O
px	O
ln	O
px	O
in	O
this	O
limit	O
so	O
that	O
i	O
lim	O
pxi	O
ln	O
pxi	O
px	O
ln	O
px	O
dx	O
where	O
the	O
quantity	O
on	O
the	O
right-hand	O
side	O
is	O
called	O
the	O
differential	B
entropy	B
we	O
see	O
that	O
the	O
discrete	O
and	O
continuous	O
forms	O
of	O
the	O
entropy	B
differ	O
by	O
a	O
quantity	O
ln	O
which	O
diverges	O
in	O
the	O
limit	O
this	O
reflects	O
the	O
fact	O
that	O
to	O
specify	O
a	O
continuous	O
variable	O
very	O
precisely	O
requires	O
a	O
large	O
number	O
of	O
bits	B
for	O
a	O
density	B
defined	O
over	O
multiple	O
continuous	O
variables	O
denoted	O
collectively	O
by	O
the	O
vector	O
x	O
the	O
differential	B
entropy	B
is	O
given	O
by	O
hx	O
px	O
ln	O
px	O
dx	O
in	O
the	O
case	O
of	O
discrete	O
distributions	O
we	O
saw	O
that	O
the	O
maximum	O
entropy	B
configuration	O
corresponded	O
to	O
an	O
equal	O
distribution	O
of	O
probabilities	O
across	O
the	O
possible	O
states	O
of	O
the	O
variable	O
let	O
us	O
now	O
consider	O
the	O
maximum	O
entropy	B
configuration	O
for	O
a	O
continuous	O
variable	O
in	O
order	O
for	O
this	O
maximum	O
to	O
be	O
well	O
defined	O
it	O
will	O
be	O
necessary	O
to	O
constrain	O
the	O
first	O
and	O
second	O
moments	O
of	O
px	O
as	O
well	O
as	O
preserving	O
the	O
normalization	O
constraint	O
we	O
therefore	O
maximize	O
the	O
differential	B
entropy	B
with	O
the	O
ludwig	O
boltzmann	B
ludwig	O
eduard	O
boltzmann	B
was	O
an	O
austrian	O
physicist	O
who	O
created	O
the	O
field	O
of	O
statistical	O
mechanics	O
prior	B
to	O
boltzmann	B
the	O
concept	O
of	O
entropy	B
was	O
already	O
known	O
from	O
classical	B
thermodynamics	O
where	O
it	O
quantifies	O
the	O
fact	O
that	O
when	O
we	O
take	O
energy	O
from	O
a	O
system	O
not	O
all	O
of	O
that	O
energy	O
is	O
typically	O
available	O
to	O
do	O
useful	O
work	O
boltzmann	B
showed	O
that	O
the	O
thermodynamic	O
entropy	B
s	O
a	O
macroscopic	O
quantity	O
could	O
be	O
related	O
to	O
the	O
statistical	O
properties	O
at	O
the	O
microscopic	O
level	O
this	O
is	O
expressed	O
through	O
the	O
famous	O
equation	O
s	O
k	O
ln	O
w	O
in	O
which	O
w	O
represents	O
the	O
number	O
of	O
possible	O
microstates	O
in	O
a	O
macrostate	B
and	O
k	O
units	O
of	O
joules	O
per	O
kelvin	O
is	O
known	O
as	O
boltzmann	B
s	O
constant	O
boltzmann	B
s	O
ideas	O
were	O
disputed	O
by	O
many	O
scientists	O
of	O
they	O
day	O
one	O
difficulty	O
they	O
saw	O
arose	O
from	O
the	O
second	O
law	O
of	O
thermo	O
dynamics	O
which	O
states	O
that	O
the	O
entropy	B
of	O
a	O
closed	O
system	O
tends	O
to	O
increase	O
with	O
time	O
by	O
contrast	O
at	O
the	O
microscopic	O
level	O
the	O
classical	B
newtonian	O
equations	O
of	O
physics	O
are	O
reversible	O
and	O
so	O
they	O
found	O
it	O
difficult	O
to	O
see	O
how	O
the	O
latter	O
could	O
explain	O
the	O
former	O
they	O
didn	O
t	O
fully	O
appreciate	O
boltzmann	B
s	O
arguments	O
which	O
were	O
statistical	O
in	O
nature	O
and	O
which	O
concluded	O
not	O
that	O
entropy	B
could	O
never	O
decrease	O
over	O
time	O
but	O
simply	O
that	O
with	O
overwhelming	O
probability	B
it	O
would	O
generally	O
increase	O
boltzmann	B
even	O
had	O
a	O
longrunning	O
dispute	O
with	O
the	O
editor	O
of	O
the	O
leading	O
german	O
physics	O
journal	O
who	O
refused	O
to	O
let	O
him	O
refer	O
to	O
atoms	O
and	O
molecules	O
as	O
anything	O
other	O
than	O
convenient	O
theoretical	O
constructs	O
the	O
continued	O
attacks	O
on	O
his	O
work	O
lead	O
to	O
bouts	O
of	O
depression	O
and	O
eventually	O
he	O
committed	O
suicide	O
shortly	O
after	O
boltzmann	B
s	O
death	O
new	O
experiments	O
by	O
perrin	O
on	O
colloidal	O
suspensions	O
verified	O
his	O
theories	O
and	O
confirmed	O
the	O
value	O
of	O
the	O
boltzmann	B
constant	O
the	O
equation	O
s	O
k	O
ln	O
w	O
is	O
carved	O
on	O
boltzmann	B
s	O
tombstone	O
introduction	O
three	O
constraints	O
px	O
dx	O
xpx	O
dx	O
dx	O
appendix	O
e	O
appendix	O
d	O
exercise	O
exercise	O
the	O
constrained	O
maximization	O
can	O
be	O
performed	O
using	O
lagrange	B
multipliers	O
so	O
that	O
we	O
maximize	O
the	O
following	O
functional	B
with	O
respect	O
to	O
px	O
px	O
dx	O
px	O
ln	O
px	O
dx	O
xpx	O
dx	O
dx	O
using	O
the	O
calculus	B
of	I
variations	I
we	O
set	O
the	O
derivative	B
of	O
this	O
functional	B
to	O
zero	O
giving	O
px	O
exp	O
exp	O
the	O
lagrange	B
multipliers	O
can	O
be	O
found	O
by	O
back	O
substitution	O
of	O
this	O
result	O
into	O
the	O
three	O
constraint	O
equations	O
leading	O
finally	O
to	O
the	O
result	O
px	O
and	O
so	O
the	O
distribution	O
that	O
maximizes	O
the	O
differential	B
entropy	B
is	O
the	O
gaussian	B
note	O
that	O
we	O
did	O
not	O
constrain	O
the	O
distribution	O
to	O
be	O
nonnegative	O
when	O
we	O
maximized	O
the	O
entropy	B
however	O
because	O
the	O
resulting	O
distribution	O
is	O
indeed	O
nonnegative	O
we	O
see	O
with	O
hindsight	O
that	O
such	O
a	O
constraint	O
is	O
not	O
necessary	O
if	O
we	O
evaluate	O
the	O
differential	B
entropy	B
of	O
the	O
gaussian	B
we	O
obtain	O
hx	O
thus	O
we	O
see	O
again	O
that	O
the	O
entropy	B
increases	O
as	O
the	O
distribution	O
becomes	O
broader	O
i	O
e	O
as	O
increases	O
this	O
result	O
also	O
shows	O
that	O
the	O
differential	B
entropy	B
unlike	O
the	O
discrete	O
entropy	B
can	O
be	O
negative	O
because	O
hx	O
in	O
for	O
e	O
suppose	O
we	O
have	O
a	O
joint	O
distribution	O
px	O
y	O
from	O
which	O
we	O
draw	O
pairs	O
of	O
values	O
of	O
x	O
and	O
y	O
if	O
a	O
value	O
of	O
x	O
is	O
already	O
known	O
then	O
the	O
additional	O
information	O
needed	O
to	O
specify	O
the	O
corresponding	O
value	O
of	O
y	O
is	O
given	O
by	O
ln	O
pyx	O
thus	O
the	O
average	O
additional	O
information	O
needed	O
to	O
specify	O
y	O
can	O
be	O
written	O
as	O
hyx	O
py	O
x	O
ln	O
pyx	O
dy	O
dx	O
information	B
theory	B
exercise	O
which	O
is	O
called	O
the	O
conditional	B
entropy	B
of	O
y	O
given	O
x	O
it	O
is	O
easily	O
seen	O
using	O
the	O
product	B
rule	I
that	O
the	O
conditional	B
entropy	B
satisfies	O
the	O
relation	O
hx	O
y	O
hyx	O
hx	O
where	O
hx	O
y	O
is	O
the	O
differential	B
entropy	B
of	O
px	O
y	O
and	O
hx	O
is	O
the	O
differential	B
entropy	B
of	O
the	O
marginal	B
distribution	O
px	O
thus	O
the	O
information	O
needed	O
to	O
describe	O
x	O
and	O
y	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
information	O
needed	O
to	O
describe	O
x	O
alone	O
plus	O
the	O
additional	O
information	O
required	O
to	O
specify	O
y	O
given	O
x	O
relative	B
entropy	B
and	O
mutual	B
information	I
so	O
far	O
in	O
this	O
section	O
we	O
have	O
introduced	O
a	O
number	O
of	O
concepts	O
from	O
information	B
theory	B
including	O
the	O
key	O
notion	O
of	O
entropy	B
we	O
now	O
start	O
to	O
relate	O
these	O
ideas	O
to	O
pattern	O
recognition	O
consider	O
some	O
unknown	O
distribution	O
px	O
and	O
suppose	O
that	O
we	O
have	O
modelled	O
this	O
using	O
an	O
approximating	O
distribution	O
qx	O
if	O
we	O
use	O
qx	O
to	O
construct	O
a	O
coding	O
scheme	O
for	O
the	O
purpose	O
of	O
transmitting	O
values	O
of	O
x	O
to	O
a	O
receiver	O
then	O
the	O
average	O
additional	O
amount	O
of	O
information	O
nats	B
required	O
to	O
specify	O
the	O
value	O
of	O
x	O
we	O
choose	O
an	O
efficient	O
coding	O
scheme	O
as	O
a	O
result	O
of	O
using	O
qx	O
instead	O
of	O
the	O
true	O
distribution	O
px	O
is	O
given	O
by	O
px	O
ln	O
qx	O
dx	O
px	O
ln	O
px	O
dx	O
px	O
ln	O
qx	O
px	O
dx	O
this	O
is	O
known	O
as	O
the	O
relative	B
entropy	B
or	O
kullback-leibler	B
divergence	I
or	O
kl	O
divergence	O
and	O
leibler	O
between	O
the	O
distributions	O
px	O
and	O
qx	O
note	O
that	O
it	O
is	O
not	O
a	O
symmetrical	O
quantity	O
that	O
is	O
to	O
say	O
we	O
now	O
show	O
that	O
the	O
kullback-leibler	B
divergence	I
satisfies	O
with	O
equality	O
if	O
and	O
only	O
if	O
px	O
qx	O
to	O
do	O
this	O
we	O
first	O
introduce	O
the	O
concept	O
of	O
convex	O
functions	O
a	O
function	O
fx	O
is	O
said	O
to	O
be	O
convex	O
if	O
it	O
has	O
the	O
property	O
that	O
every	O
chord	O
lies	O
on	O
or	O
above	O
the	O
function	O
as	O
shown	O
in	O
figure	O
any	O
value	O
of	O
x	O
in	O
the	O
interval	O
from	O
x	O
a	O
to	O
x	O
b	O
can	O
be	O
written	O
in	O
the	O
form	O
a	O
where	O
the	O
corresponding	O
point	O
on	O
the	O
chord	O
is	O
given	O
by	O
fa	O
claude	O
shannon	B
after	O
graduating	O
from	O
michigan	O
and	O
mit	O
shannon	B
joined	O
the	O
att	O
bell	O
telephone	O
laboratories	O
in	O
his	O
paper	O
a	O
mathematical	O
theory	B
of	O
communication	O
published	O
in	O
the	O
bell	O
system	O
technical	O
journal	O
in	O
laid	O
the	O
foundations	O
for	O
modern	O
information	O
the	O
ory	O
this	O
paper	O
introduced	O
the	O
word	O
bit	O
and	O
his	O
concept	O
that	O
information	O
could	O
be	O
sent	O
as	O
a	O
stream	O
of	O
and	O
paved	O
the	O
way	O
for	O
the	O
communications	O
revolution	O
it	O
is	O
said	O
that	O
von	O
neumann	O
recommended	O
to	O
shannon	B
that	O
he	O
use	O
the	O
term	O
entropy	B
not	O
only	O
because	O
of	O
its	O
similarity	O
to	O
the	O
quantity	O
used	O
in	O
physics	O
but	O
also	O
because	O
nobody	O
knows	O
what	O
entropy	B
really	O
is	O
so	O
in	O
any	O
discussion	O
you	O
will	O
always	O
have	O
an	O
advantage	O
introduction	O
figure	O
a	O
convex	B
function	I
f	O
is	O
one	O
for	O
which	O
every	O
chord	O
in	O
blue	O
lies	O
on	O
or	O
above	O
the	O
function	O
in	O
red	O
fx	O
chord	O
a	O
x	O
x	O
b	O
x	O
and	O
the	O
corresponding	O
value	O
of	O
the	O
function	O
is	O
f	O
a	O
convexity	O
then	O
implies	O
f	O
a	O
fa	O
exercise	O
exercise	O
this	O
is	O
equivalent	O
to	O
the	O
requirement	O
that	O
the	O
second	O
derivative	B
of	O
the	O
function	O
be	O
everywhere	O
positive	O
examples	O
of	O
convex	O
functions	O
are	O
x	O
ln	O
x	O
x	O
and	O
a	O
function	O
is	O
called	O
strictly	O
convex	O
if	O
the	O
equality	O
is	O
satisfied	O
only	O
for	O
and	O
if	O
a	O
function	O
has	O
the	O
opposite	O
property	O
namely	O
that	O
every	O
chord	O
lies	O
on	O
or	O
below	O
the	O
function	O
it	O
is	O
called	O
concave	O
with	O
a	O
corresponding	O
definition	O
for	O
strictly	O
concave	O
if	O
a	O
function	O
fx	O
is	O
convex	O
then	O
fx	O
will	O
be	O
concave	O
using	O
the	O
technique	O
of	O
proof	O
by	O
induction	O
we	O
can	O
show	O
from	O
that	O
a	O
convex	B
function	I
fx	O
satisfies	O
f	O
ixi	O
i	O
i	O
for	O
any	O
set	O
of	O
points	O
the	O
result	O
is	O
where	O
i	O
and	O
known	O
as	O
jensen	O
s	O
inequality	O
if	O
we	O
interpret	O
the	O
i	O
as	O
the	O
probability	B
distribution	O
over	O
a	O
discrete	O
variable	O
x	O
taking	O
the	O
values	O
then	O
can	O
be	O
written	O
ifxi	O
where	O
e	O
denotes	O
the	O
expectation	B
for	O
continuous	O
variables	O
jensen	O
s	O
inequality	O
takes	O
the	O
form	O
f	O
efx	O
f	O
xpx	O
dx	O
fxpx	O
dx	O
we	O
can	O
apply	O
jensen	O
s	O
inequality	O
in	O
the	O
form	O
to	O
the	O
kullback-leibler	B
divergence	I
to	O
give	O
px	O
ln	O
qx	O
px	O
dx	O
ln	O
qx	O
dx	O
information	B
theory	B
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
ln	O
x	O
is	O
a	O
convex	B
function	I
together	O
with	O
the	O
norqx	O
dx	O
in	O
fact	O
ln	O
x	O
is	O
a	O
strictly	O
convex	B
function	I
malization	O
condition	O
so	O
the	O
equality	O
will	O
hold	O
if	O
and	O
only	O
if	O
qx	O
px	O
for	O
all	O
x	O
thus	O
we	O
can	O
interpret	O
the	O
kullback-leibler	B
divergence	I
as	O
a	O
measure	O
of	O
the	O
dissimilarity	O
of	O
the	O
two	O
distributions	O
px	O
and	O
qx	O
we	O
see	O
that	O
there	O
is	O
an	O
intimate	O
relationship	O
between	O
data	B
compression	I
and	O
density	B
estimation	I
the	O
problem	O
of	O
modelling	O
an	O
unknown	O
probability	B
distribution	O
because	O
the	O
most	O
efficient	O
compression	O
is	O
achieved	O
when	O
we	O
know	O
the	O
true	O
distribution	O
if	O
we	O
use	O
a	O
distribution	O
that	O
is	O
different	O
from	O
the	O
true	O
one	O
then	O
we	O
must	O
necessarily	O
have	O
a	O
less	O
efficient	O
coding	O
and	O
on	O
average	O
the	O
additional	O
information	O
that	O
must	O
be	O
transmitted	O
is	O
least	O
equal	O
to	O
the	O
kullback-leibler	B
divergence	I
between	O
the	O
two	O
distributions	O
suppose	O
that	O
data	O
is	O
being	O
generated	O
from	O
an	O
unknown	O
distribution	O
px	O
that	O
we	O
wish	O
to	O
model	O
we	O
can	O
try	O
to	O
approximate	O
this	O
distribution	O
using	O
some	O
parametric	O
distribution	O
qx	O
governed	O
by	O
a	O
set	O
of	O
adjustable	O
parameters	O
for	O
example	O
a	O
multivariate	O
gaussian	B
one	O
way	O
to	O
determine	O
is	O
to	O
minimize	O
the	O
kullback-leibler	B
divergence	I
between	O
px	O
and	O
qx	O
with	O
respect	O
to	O
we	O
cannot	O
do	O
this	O
directly	O
because	O
we	O
don	O
t	O
know	O
px	O
suppose	O
however	O
that	O
we	O
have	O
observed	O
a	O
finite	O
set	O
of	O
training	B
points	O
xn	O
for	O
n	O
n	O
drawn	O
from	O
px	O
then	O
the	O
expectation	B
with	O
respect	O
to	O
px	O
can	O
be	O
approximated	O
by	O
a	O
finite	O
sum	O
over	O
these	O
points	O
using	O
so	O
that	O
ln	O
qxn	O
ln	O
pxn	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
is	O
independent	B
of	O
and	O
the	O
first	O
term	O
is	O
the	O
negative	O
log	O
likelihood	B
function	I
for	O
under	O
the	O
distribution	O
qx	O
evaluated	O
using	O
the	O
training	B
set	I
thus	O
we	O
see	O
that	O
minimizing	O
this	O
kullback-leibler	B
divergence	I
is	O
equivalent	O
to	O
maximizing	O
the	O
likelihood	B
function	I
now	O
consider	O
the	O
joint	O
distribution	O
between	O
two	O
sets	O
of	O
variables	O
x	O
and	O
y	O
given	O
by	O
px	O
y	O
if	O
the	O
sets	O
of	O
variables	O
are	O
independent	B
then	O
their	O
joint	O
distribution	O
will	O
factorize	O
into	O
the	O
product	O
of	O
their	O
marginals	O
px	O
y	O
pxpy	O
if	O
the	O
variables	O
are	O
not	O
independent	B
we	O
can	O
gain	O
some	O
idea	O
of	O
whether	O
they	O
are	O
close	O
to	O
being	O
independent	B
by	O
considering	O
the	O
kullback-leibler	B
divergence	I
between	O
the	O
joint	O
distribution	O
and	O
the	O
product	O
of	O
the	O
marginals	O
given	O
by	O
ix	O
y	O
klpx	O
px	O
y	O
ln	O
pxpy	O
px	O
y	O
dx	O
dy	O
which	O
is	O
called	O
the	O
mutual	B
information	I
between	O
the	O
variables	O
x	O
and	O
y	O
from	O
the	O
properties	O
of	O
the	O
kullback-leibler	B
divergence	I
we	O
see	O
that	O
ix	O
y	O
with	O
equality	O
if	O
and	O
only	O
if	O
x	O
and	O
y	O
are	O
independent	B
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
we	O
see	O
that	O
the	O
mutual	B
information	I
is	O
related	O
to	O
the	O
conditional	B
entropy	B
through	O
ix	O
y	O
hx	O
hxy	O
hy	O
hyx	O
exercise	O
introduction	O
thus	O
we	O
can	O
view	O
the	O
mutual	B
information	I
as	O
the	O
reduction	O
in	O
the	O
uncertainty	O
about	O
x	O
by	O
virtue	O
of	O
being	O
told	O
the	O
value	O
of	O
y	O
vice	O
versa	O
from	O
a	O
bayesian	B
perspective	O
we	O
can	O
view	O
px	O
as	O
the	O
prior	B
distribution	O
for	O
x	O
and	O
pxy	O
as	O
the	O
posterior	O
distribution	O
after	O
we	O
have	O
observed	O
new	O
data	O
y	O
the	O
mutual	B
information	I
therefore	O
represents	O
the	O
reduction	O
in	O
uncertainty	O
about	O
x	O
as	O
a	O
consequence	O
of	O
the	O
new	O
observation	O
y	O
exercises	O
www	O
consider	O
the	O
sum-of-squares	B
error	B
function	I
given	O
by	O
in	O
which	O
the	O
function	O
yx	O
w	O
is	O
given	O
by	O
the	O
polynomial	O
show	O
that	O
the	O
coefficients	O
w	O
that	O
minimize	O
this	O
error	B
function	I
are	O
given	O
by	O
the	O
solution	O
to	O
the	O
following	O
set	O
of	O
linear	O
equations	O
where	O
aij	O
aijwj	O
ti	O
ti	O
here	O
a	O
suffix	O
i	O
or	O
j	O
denotes	O
the	O
index	O
of	O
a	O
component	O
whereas	O
denotes	O
x	O
raised	O
to	O
the	O
power	O
of	O
i	O
write	O
down	O
the	O
set	O
of	O
coupled	O
linear	O
equations	O
analogous	O
to	O
satisfied	O
by	O
the	O
coefficients	O
wi	O
which	O
minimize	O
the	O
regularized	O
sum-of-squares	B
error	B
function	I
given	O
by	O
suppose	O
that	O
we	O
have	O
three	O
coloured	O
boxes	O
r	O
b	O
and	O
g	O
box	O
r	O
contains	O
apples	O
oranges	O
and	O
limes	O
box	O
b	O
contains	O
apple	O
orange	O
and	O
limes	O
and	O
box	O
g	O
contains	O
apples	O
oranges	O
and	O
limes	O
if	O
a	O
box	O
is	O
chosen	O
at	O
random	O
with	O
probabilities	O
pr	O
pb	O
pg	O
and	O
a	O
piece	O
of	O
fruit	O
is	O
removed	O
from	O
the	O
box	O
equal	O
probability	B
of	O
selecting	O
any	O
of	O
the	O
items	O
in	O
the	O
box	O
then	O
what	O
is	O
the	O
probability	B
of	O
selecting	O
an	O
apple	O
if	O
we	O
observe	O
that	O
the	O
selected	O
fruit	O
is	O
in	O
fact	O
an	O
orange	O
what	O
is	O
the	O
probability	B
that	O
it	O
came	O
from	O
the	O
green	O
box	O
www	O
consider	O
a	O
probability	B
density	B
pxx	O
defined	O
over	O
a	O
continuous	O
variable	O
x	O
and	O
suppose	O
that	O
we	O
make	O
a	O
nonlinear	O
change	O
of	O
variable	O
using	O
x	O
gy	O
so	O
that	O
the	O
density	B
transforms	O
according	O
to	O
by	O
differentiating	O
show	O
that	O
the	O
of	O
the	O
maximum	O
of	O
the	O
density	B
in	O
y	O
is	O
not	O
in	O
general	O
related	O
to	O
the	O
of	O
the	O
maximum	O
of	O
the	O
density	B
over	O
x	O
by	O
the	O
simple	O
functional	B
relation	O
as	O
a	O
consequence	O
of	O
the	O
jacobian	O
factor	O
this	O
shows	O
that	O
the	O
maximum	O
of	O
a	O
probability	B
density	B
contrast	O
to	O
a	O
simple	O
function	O
is	O
dependent	O
on	O
the	O
choice	O
of	O
variable	O
verify	O
that	O
in	O
the	O
case	O
of	O
a	O
linear	O
transformation	O
the	O
location	O
of	O
the	O
maximum	O
transforms	O
in	O
the	O
same	O
way	O
as	O
the	O
variable	O
itself	O
using	O
the	O
definition	O
show	O
that	O
varfx	O
satisfies	O
which	O
we	O
can	O
evaluate	O
by	O
first	O
writing	O
its	O
square	O
in	O
the	O
form	O
dx	O
i	O
exp	O
i	O
exp	O
dx	O
dy	O
show	O
that	O
if	O
two	O
variables	O
x	O
and	O
y	O
are	O
independent	B
then	O
their	O
covariance	B
is	O
zero	O
exercises	O
www	O
in	O
this	O
exercise	O
we	O
prove	O
the	O
normalization	O
condition	O
for	O
the	O
univariate	O
gaussian	B
to	O
do	O
this	O
consider	O
the	O
integral	O
now	O
make	O
the	O
transformation	O
from	O
cartesian	O
coordinates	O
y	O
to	O
polar	O
coordinates	O
and	O
then	O
substitute	O
u	O
show	O
that	O
by	O
performing	O
the	O
integrals	O
over	O
and	O
u	O
and	O
then	O
taking	O
the	O
square	O
root	O
of	O
both	O
sides	O
we	O
obtain	O
finally	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
gaussian	B
distribution	O
n	O
is	O
normalized	O
i	O
www	O
by	O
using	O
a	O
change	O
of	O
variables	O
verify	O
that	O
the	O
univariate	O
gaussian	B
distribution	O
given	O
by	O
satisfies	O
next	O
by	O
differentiating	O
both	O
sides	O
of	O
the	O
normalization	O
condition	O
x	O
dx	O
with	O
respect	O
to	O
verify	O
that	O
the	O
gaussian	B
satisfies	O
finally	O
show	O
that	O
holds	O
www	O
show	O
that	O
the	O
mode	O
the	O
maximum	O
of	O
the	O
gaussian	B
distribution	O
is	O
given	O
by	O
similarly	O
show	O
that	O
the	O
mode	O
of	O
the	O
multivariate	O
gaussian	B
is	O
given	O
by	O
www	O
suppose	O
that	O
the	O
two	O
variables	O
x	O
and	O
z	O
are	O
statistically	O
independent	B
show	O
that	O
the	O
mean	B
and	O
variance	B
of	O
their	O
sum	O
satisfies	O
ex	O
z	O
ex	O
ez	O
varx	O
z	O
varx	O
varz	O
by	O
setting	O
the	O
derivatives	O
of	O
the	O
log	O
likelihood	B
function	I
with	O
respect	O
to	O
and	O
equal	O
to	O
zero	O
verify	O
the	O
results	O
and	O
introduction	O
www	O
using	O
the	O
results	O
and	O
show	O
that	O
exnxm	O
inm	O
where	O
xn	O
and	O
xm	O
denote	O
data	O
points	O
sampled	O
from	O
a	O
gaussian	B
distribution	O
with	O
mean	B
and	O
variance	B
and	O
inm	O
satisfies	O
inm	O
if	O
n	O
m	O
and	O
inm	O
otherwise	O
hence	O
prove	O
the	O
results	O
and	O
suppose	O
that	O
the	O
variance	B
of	O
a	O
gaussian	B
is	O
estimated	O
using	O
the	O
result	O
but	O
with	O
the	O
maximum	B
likelihood	I
estimate	O
ml	O
replaced	O
with	O
the	O
true	O
value	O
of	O
the	O
mean	B
show	O
that	O
this	O
estimator	O
has	O
the	O
property	O
that	O
its	O
expectation	B
is	O
given	O
by	O
the	O
true	O
variance	B
show	O
that	O
an	O
arbitrary	O
square	O
matrix	O
with	O
elements	O
wij	O
can	O
be	O
written	O
in	O
the	O
form	O
wij	O
ws	O
ij	O
are	O
symmetric	O
and	O
anti-symmetric	O
matrices	O
respectively	O
satisfying	O
ws	O
ji	O
for	O
all	O
i	O
and	O
j	O
now	O
consider	O
the	O
second	B
order	I
term	O
in	O
a	O
higher	O
order	O
polynomial	O
in	O
d	O
dimensions	O
given	O
by	O
ij	O
wa	O
ij	O
and	O
wa	O
ij	O
wa	O
ij	O
where	O
ws	O
ij	O
ws	O
ji	O
and	O
wa	O
show	O
that	O
wijxixj	O
wijxixj	O
ws	O
ijxixj	O
so	O
that	O
the	O
contribution	O
from	O
the	O
anti-symmetric	O
matrix	O
vanishes	O
we	O
therefore	O
see	O
that	O
without	O
loss	O
of	O
generality	O
the	O
matrix	O
of	O
coefficients	O
wij	O
can	O
be	O
chosen	O
to	O
be	O
symmetric	O
and	O
so	O
not	O
all	O
of	O
the	O
elements	O
of	O
this	O
matrix	O
can	O
be	O
chosen	O
independently	O
show	O
that	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
matrix	O
ws	O
ij	O
is	O
given	O
by	O
dd	O
www	O
in	O
this	O
exercise	O
and	O
the	O
next	O
we	O
explore	O
how	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
a	O
polynomial	O
grows	O
with	O
the	O
order	O
m	O
of	O
the	O
polynomial	O
and	O
with	O
the	O
dimensionality	O
d	O
of	O
the	O
input	O
space	O
we	O
start	O
by	O
writing	O
down	O
the	O
m	O
th	O
order	O
term	O
for	O
a	O
polynomial	O
in	O
d	O
dimensions	O
in	O
the	O
form	O
im	O
im	O
xim	O
the	O
coefficients	O
im	O
comprise	O
dm	O
elements	O
but	O
the	O
number	O
of	O
independent	B
parameters	O
is	O
significantly	O
fewer	O
due	O
to	O
the	O
many	O
interchange	O
symmetries	B
of	O
the	O
factor	O
xim	O
begin	O
by	O
showing	O
that	O
the	O
redundancy	O
in	O
the	O
coefficients	O
can	O
be	O
removed	O
by	O
rewriting	O
this	O
m	O
th	O
order	O
term	O
in	O
the	O
form	O
im	O
im	O
im	O
xim	O
note	O
that	O
the	O
precise	O
relationship	O
between	O
coefficients	O
and	O
w	O
coefficients	O
need	O
exercises	O
not	O
be	O
made	O
explicit	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
number	O
of	O
independent	B
parameters	O
nd	O
m	O
which	O
appear	O
at	O
order	O
m	O
satisfies	O
the	O
following	O
recursion	O
relation	O
next	O
use	O
proof	O
by	O
induction	O
to	O
show	O
that	O
the	O
following	O
result	O
holds	O
nd	O
m	O
ni	O
m	O
m	O
m	O
m	O
which	O
can	O
be	O
done	O
by	O
first	O
proving	O
the	O
result	O
for	O
d	O
and	O
arbitrary	O
m	O
by	O
making	O
use	O
of	O
the	O
result	O
then	O
assuming	O
it	O
is	O
correct	O
for	O
dimension	O
d	O
and	O
verifying	O
that	O
it	O
is	O
correct	O
for	O
dimension	O
d	O
finally	O
use	O
the	O
two	O
previous	O
results	O
together	O
with	O
proof	O
by	O
induction	O
to	O
show	O
m	O
m	O
nd	O
m	O
to	O
do	O
this	O
first	O
show	O
that	O
the	O
result	O
is	O
true	O
for	O
m	O
and	O
any	O
value	O
of	O
d	O
by	O
comparison	O
with	O
the	O
result	O
of	O
exercise	O
then	O
make	O
use	O
of	O
together	O
with	O
to	O
show	O
that	O
if	O
the	O
result	O
holds	O
at	O
order	O
m	O
then	O
it	O
will	O
also	O
hold	O
at	O
order	O
m	O
in	O
exercise	O
we	O
proved	O
the	O
result	O
for	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
m	O
th	O
order	O
term	O
of	O
a	O
d-dimensional	O
polynomial	O
we	O
now	O
find	O
an	O
expression	O
for	O
the	O
total	O
number	O
nd	O
m	O
of	O
independent	B
parameters	O
in	O
all	O
of	O
the	O
terms	O
up	O
to	O
and	O
including	O
the	O
order	O
first	O
show	O
that	O
nd	O
m	O
satisfies	O
nd	O
m	O
nd	O
m	O
where	O
nd	O
m	O
is	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
term	O
of	O
order	O
m	O
now	O
make	O
use	O
of	O
the	O
result	O
together	O
with	O
proof	O
by	O
induction	O
to	O
show	O
that	O
nd	O
m	O
m	O
d	O
m	O
this	O
can	O
be	O
done	O
by	O
first	O
proving	O
that	O
the	O
result	O
holds	O
for	O
m	O
and	O
arbitrary	O
d	O
then	O
assuming	O
that	O
it	O
holds	O
at	O
order	O
m	O
and	O
hence	O
showing	O
that	O
it	O
holds	O
at	O
order	O
m	O
finally	O
make	O
use	O
of	O
stirling	O
s	O
approximation	O
in	O
the	O
form	O
n	O
nne	O
n	O
for	O
large	O
n	O
to	O
show	O
that	O
for	O
d	O
m	O
the	O
quantity	O
nd	O
m	O
grows	O
like	O
dm	O
and	O
for	O
m	O
d	O
it	O
grows	O
like	O
m	O
d	O
consider	O
a	O
cubic	O
polynomial	O
in	O
d	O
dimensions	O
and	O
evaluate	O
numerically	O
the	O
total	O
number	O
of	O
independent	B
parameters	O
for	O
d	O
and	O
d	O
which	O
correspond	O
to	O
typical	O
small-scale	O
and	O
medium-scale	O
machine	O
learning	B
applications	O
introduction	O
www	O
the	O
gamma	B
function	I
is	O
defined	O
by	O
ux	O
u	O
du	O
using	O
integration	O
by	O
parts	O
prove	O
the	O
relation	O
x	O
show	O
also	O
that	O
and	O
hence	O
that	O
x	O
when	O
x	O
is	O
an	O
integer	O
www	O
we	O
can	O
use	O
the	O
result	O
to	O
derive	O
an	O
expression	O
for	O
the	O
surface	O
area	O
sd	O
and	O
the	O
volume	O
vd	O
of	O
a	O
sphere	O
of	O
unit	O
radius	O
in	O
d	O
dimensions	O
to	O
do	O
this	O
consider	O
the	O
following	O
result	O
which	O
is	O
obtained	O
by	O
transforming	O
from	O
cartesian	O
to	O
polar	O
coordinates	O
e	O
i	O
dxi	O
sd	O
e	O
rd	O
dr	O
using	O
the	O
definition	O
of	O
the	O
gamma	B
function	I
together	O
with	O
evaluate	O
both	O
sides	O
of	O
this	O
equation	O
and	O
hence	O
show	O
that	O
sd	O
next	O
by	O
integrating	O
with	O
respect	O
to	O
radius	O
from	O
to	O
show	O
that	O
the	O
volume	O
of	O
the	O
unit	O
sphere	O
in	O
d	O
dimensions	O
is	O
given	O
by	O
finally	O
use	O
the	O
results	O
and	O
reduce	O
to	O
the	O
usual	O
expressions	O
for	O
d	O
and	O
d	O
vd	O
sd	O
d	O
to	O
show	O
that	O
and	O
consider	O
a	O
sphere	O
of	O
radius	O
a	O
in	O
d-dimensions	O
together	O
with	O
the	O
concentric	O
hypercube	O
of	O
side	O
so	O
that	O
the	O
sphere	O
touches	O
the	O
hypercube	O
at	O
the	O
centres	O
of	O
each	O
of	O
its	O
sides	O
by	O
using	O
the	O
results	O
of	O
exercise	O
show	O
that	O
the	O
ratio	O
of	O
the	O
volume	O
of	O
the	O
sphere	O
to	O
the	O
volume	O
of	O
the	O
cube	O
is	O
given	O
by	O
volume	O
of	O
sphere	O
volume	O
of	O
cube	O
now	O
make	O
use	O
of	O
stirling	O
s	O
formula	O
in	O
the	O
form	O
which	O
is	O
valid	O
for	O
x	O
to	O
show	O
that	O
as	O
d	O
the	O
ratio	O
goes	O
to	O
zero	O
show	O
also	O
that	O
the	O
ratio	O
of	O
the	O
distance	O
from	O
the	O
centre	O
of	O
the	O
hypercube	O
to	O
one	O
of	O
d	O
which	O
the	O
corners	O
divided	O
by	O
the	O
perpendicular	O
distance	O
to	O
one	O
of	O
the	O
sides	O
is	O
therefore	O
goes	O
to	O
as	O
d	O
from	O
these	O
results	O
we	O
see	O
that	O
in	O
a	O
space	O
of	O
high	O
dimensionality	O
most	O
of	O
the	O
volume	O
of	O
a	O
cube	O
is	O
concentrated	O
in	O
the	O
large	O
number	O
of	O
corners	O
which	O
themselves	O
become	O
very	O
long	O
spikes	O
exercises	O
www	O
in	O
this	O
exercise	O
we	O
explore	O
the	O
behaviour	O
of	O
the	O
gaussian	B
distribution	O
in	O
high-dimensional	O
spaces	O
consider	O
a	O
gaussian	B
distribution	O
in	O
d	O
dimensions	O
given	O
by	O
px	O
exp	O
exp	O
where	O
sd	O
is	O
the	O
surface	O
area	O
of	O
a	O
unit	O
sphere	O
in	O
d	O
dimensions	O
show	O
that	O
the	O
function	O
d	O
by	O
considering	O
we	O
wish	O
to	O
find	O
the	O
density	B
with	O
respect	O
to	O
radius	O
in	O
polar	O
coordinates	O
in	O
which	O
the	O
direction	O
variables	O
have	O
been	O
integrated	O
out	O
to	O
do	O
this	O
show	O
that	O
the	O
integral	O
of	O
the	O
probability	B
density	B
over	O
a	O
thin	O
shell	O
of	O
radius	O
r	O
and	O
thickness	O
where	O
is	O
given	O
by	O
pr	O
where	O
pr	O
sdrd	O
pr	O
has	O
a	O
single	O
stationary	B
point	O
located	O
for	O
large	O
d	O
where	O
show	O
that	O
for	O
large	O
d	O
exp	O
which	O
shows	O
is	O
a	O
maximum	O
of	O
the	O
radial	O
probability	B
density	B
and	O
also	O
that	O
pr	O
decays	O
exponentially	O
away	O
from	O
its	O
maximum	O
with	O
length	O
scale	O
we	O
have	O
already	O
seen	O
that	O
for	O
large	O
d	O
and	O
so	O
we	O
see	O
that	O
most	O
of	O
the	O
probability	B
density	B
px	O
is	O
larger	O
at	O
the	O
origin	O
than	O
at	O
the	O
by	O
a	O
factor	O
of	O
mass	O
is	O
concentrated	O
in	O
a	O
thin	O
shell	O
at	O
large	O
radius	O
finally	O
show	O
that	O
the	O
probability	B
we	O
therefore	O
see	O
that	O
most	O
of	O
the	O
probability	B
mass	O
in	O
a	O
high-dimensional	O
gaussian	B
distribution	O
is	O
located	O
at	O
a	O
different	O
radius	O
from	O
the	O
region	O
of	O
high	O
probability	B
density	B
this	O
property	O
of	O
distributions	O
in	O
spaces	O
of	O
high	O
dimensionality	O
will	O
have	O
important	O
consequences	O
when	O
we	O
consider	O
bayesian	B
inference	B
of	O
model	O
parameters	O
in	O
later	O
chapters	O
consider	O
two	O
nonnegative	O
numbers	O
a	O
and	O
b	O
and	O
show	O
that	O
if	O
a	O
b	O
then	O
a	O
use	O
this	O
result	O
to	O
show	O
that	O
if	O
the	O
decision	O
regions	O
of	O
a	O
two-class	O
classification	B
problem	O
are	O
chosen	O
to	O
minimize	O
the	O
probability	B
of	O
misclassification	O
this	O
probability	B
will	O
satisfy	O
pmistake	O
dx	O
www	O
given	O
a	O
loss	B
matrix	I
with	O
elements	O
lkj	O
the	O
expected	O
risk	O
is	O
minimized	O
if	O
for	O
each	O
x	O
we	O
choose	O
the	O
class	O
that	O
minimizes	O
verify	O
that	O
when	O
the	O
loss	B
matrix	I
is	O
given	O
by	O
lkj	O
ikj	O
where	O
ikj	O
are	O
the	O
elements	O
of	O
the	O
identity	O
matrix	O
this	O
reduces	O
to	O
the	O
criterion	O
of	O
choosing	O
the	O
class	O
having	O
the	O
largest	O
posterior	B
probability	B
what	O
is	O
the	O
interpretation	O
of	O
this	O
form	O
of	O
loss	B
matrix	I
derive	O
the	O
criterion	O
for	O
minimizing	O
the	O
expected	O
loss	O
when	O
there	O
is	O
a	O
general	O
loss	B
matrix	I
and	O
general	O
prior	B
probabilities	O
for	O
the	O
classes	O
introduction	O
www	O
consider	O
a	O
classification	B
problem	O
in	O
which	O
the	O
loss	O
incurred	O
when	O
an	O
input	O
vector	O
from	O
class	O
ck	O
is	O
classified	O
as	O
belonging	O
to	O
class	O
cj	O
is	O
given	O
by	O
the	O
loss	B
matrix	I
lkj	O
and	O
for	O
which	O
the	O
loss	O
incurred	O
in	O
selecting	O
the	O
reject	B
option	I
is	O
find	O
the	O
decision	O
criterion	O
that	O
will	O
give	O
the	O
minimum	O
expected	O
loss	O
verify	O
that	O
this	O
reduces	O
to	O
the	O
reject	O
criterion	O
discussed	O
in	O
section	O
when	O
the	O
loss	B
matrix	I
is	O
given	O
by	O
lkj	O
ikj	O
what	O
is	O
the	O
relationship	O
between	O
and	O
the	O
rejection	O
threshold	O
www	O
consider	O
the	O
generalization	B
of	O
the	O
squared	O
loss	B
function	I
for	O
a	O
single	O
target	O
variable	O
t	O
to	O
the	O
case	O
of	O
multiple	O
target	O
variables	O
described	O
by	O
the	O
vector	O
t	O
given	O
by	O
elt	O
yx	O
t	O
dx	O
dt	O
using	O
the	O
calculus	B
of	I
variations	I
show	O
that	O
the	O
function	O
yx	O
for	O
which	O
this	O
expected	O
loss	O
is	O
minimized	O
is	O
given	O
by	O
yx	O
ettx	O
show	O
that	O
this	O
result	O
reduces	O
to	O
for	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
t	O
by	O
expansion	O
of	O
the	O
square	O
in	O
derive	O
a	O
result	O
analogous	O
to	O
and	O
hence	O
show	O
that	O
the	O
function	O
yx	O
that	O
minimizes	O
the	O
expected	O
squared	O
loss	O
for	O
the	O
case	O
of	O
a	O
vector	O
t	O
of	O
target	O
variables	O
is	O
again	O
given	O
by	O
the	O
conditional	B
expectation	B
of	O
t	O
www	O
consider	O
the	O
expected	O
loss	O
for	B
regression	B
problems	O
under	O
the	O
lq	O
loss	B
function	I
given	O
by	O
write	O
down	O
the	O
condition	O
that	O
yx	O
must	O
satisfy	O
in	O
order	O
to	O
minimize	O
elq	O
show	O
that	O
for	O
q	O
this	O
solution	O
represents	O
the	O
conditional	B
median	O
i	O
e	O
the	O
function	O
yx	O
such	O
that	O
the	O
probability	B
mass	O
for	O
t	O
yx	O
is	O
the	O
same	O
as	O
for	O
t	O
yx	O
also	O
show	O
that	O
the	O
minimum	O
expected	O
lq	O
loss	O
for	O
q	O
is	O
given	O
by	O
the	O
conditional	B
mode	O
i	O
e	O
by	O
the	O
function	O
yx	O
equal	O
to	O
the	O
value	O
of	O
t	O
that	O
maximizes	O
ptx	O
for	O
each	O
x	O
in	O
section	O
we	O
introduced	O
the	O
idea	O
of	O
entropy	B
hx	O
as	O
the	O
information	O
gained	O
on	O
observing	O
the	O
value	O
of	O
a	O
random	O
variable	O
x	O
having	O
distribution	O
px	O
we	O
saw	O
that	O
for	O
independent	B
variables	I
x	O
and	O
y	O
for	O
which	O
px	O
y	O
pxpy	O
the	O
entropy	B
functions	O
are	O
additive	O
so	O
that	O
hx	O
y	O
hx	O
hy	O
in	O
this	O
exercise	O
we	O
derive	O
the	O
relation	O
between	O
h	O
and	O
p	O
in	O
the	O
form	O
of	O
a	O
function	O
hp	O
first	O
show	O
that	O
and	O
hence	O
by	O
induction	O
that	O
hpn	O
nhp	O
where	O
n	O
is	O
a	O
positive	O
integer	O
hence	O
show	O
that	O
hpnm	O
where	O
m	O
is	O
also	O
a	O
positive	O
integer	O
this	O
implies	O
that	O
hpx	O
xhp	O
where	O
x	O
is	O
a	O
positive	O
rational	O
number	O
and	O
hence	O
by	O
continuity	O
when	O
it	O
is	O
a	O
positive	O
real	O
number	O
finally	O
show	O
that	O
this	O
implies	O
hp	O
must	O
take	O
the	O
form	O
hp	O
ln	O
p	O
www	O
consider	O
an	O
m-state	O
discrete	O
random	O
variable	O
x	O
and	O
use	O
jensen	O
s	O
inequality	O
in	O
the	O
form	O
to	O
show	O
that	O
the	O
entropy	B
of	O
its	O
distribution	O
px	O
satisfies	O
hx	O
ln	O
m	O
evaluate	O
the	O
kullback-leibler	B
divergence	I
between	O
two	O
gaussians	O
px	O
n	O
and	O
qx	O
n	O
table	O
the	O
joint	O
distribution	O
px	O
y	O
for	O
two	O
binary	O
variables	O
x	O
and	O
y	O
used	O
in	O
exercise	O
exercises	O
y	O
x	O
www	O
consider	O
two	O
variables	O
x	O
and	O
y	O
having	O
joint	O
distribution	O
px	O
y	O
show	O
that	O
the	O
differential	B
entropy	B
of	O
this	O
pair	O
of	O
variables	O
satisfies	O
hx	O
y	O
hx	O
hy	O
with	O
equality	O
if	O
and	O
only	O
if	O
x	O
and	O
y	O
are	O
statistically	O
independent	B
consider	O
a	O
vector	O
x	O
of	O
continuous	O
variables	O
with	O
distribution	O
px	O
and	O
corresponding	O
entropy	B
hx	O
suppose	O
that	O
we	O
make	O
a	O
nonsingular	O
linear	O
transformation	O
of	O
x	O
to	O
obtain	O
a	O
new	O
variable	O
y	O
ax	O
show	O
that	O
the	O
corresponding	O
entropy	B
is	O
given	O
by	O
hy	O
hx	O
lna	O
where	O
denotes	O
the	O
determinant	O
of	O
a	O
suppose	O
that	O
the	O
conditional	B
entropy	B
hyx	O
between	O
two	O
discrete	O
random	O
variables	O
x	O
and	O
y	O
is	O
zero	O
show	O
that	O
for	O
all	O
values	O
of	O
x	O
such	O
that	O
px	O
the	O
variable	O
y	O
must	O
be	O
a	O
function	O
of	O
x	O
in	O
other	O
words	O
for	O
each	O
x	O
there	O
is	O
only	O
one	O
value	O
of	O
y	O
such	O
that	O
pyx	O
www	O
use	O
the	O
calculus	B
of	I
variations	I
to	O
show	O
that	O
the	O
stationary	B
point	O
of	O
the	O
functional	B
is	O
given	O
by	O
then	O
use	O
the	O
constraints	O
and	O
to	O
eliminate	O
the	O
lagrange	B
multipliers	O
and	O
hence	O
show	O
that	O
the	O
maximum	O
entropy	B
solution	O
is	O
given	O
by	O
the	O
gaussian	B
www	O
use	O
the	O
results	O
and	O
to	O
show	O
that	O
the	O
entropy	B
of	O
the	O
univariate	O
gaussian	B
is	O
given	O
by	O
a	O
strictly	O
convex	B
function	I
is	O
defined	O
as	O
one	O
for	O
which	O
every	O
chord	O
lies	O
above	O
the	O
function	O
show	O
that	O
this	O
is	O
equivalent	O
to	O
the	O
condition	O
that	O
the	O
second	O
derivative	B
of	O
the	O
function	O
be	O
positive	O
using	O
the	O
definition	O
together	O
with	O
the	O
product	B
rule	I
of	I
probability	B
prove	O
the	O
result	O
www	O
using	O
proof	O
by	O
induction	O
show	O
that	O
the	O
inequality	O
for	O
convex	O
functions	O
implies	O
the	O
result	O
consider	O
two	O
binary	O
variables	O
x	O
and	O
y	O
having	O
the	O
joint	O
distribution	O
given	O
in	O
table	O
evaluate	O
the	O
following	O
quantities	O
hx	O
hy	O
hyx	O
hxy	O
hx	O
y	O
ix	O
y	O
draw	O
a	O
diagram	O
to	O
show	O
the	O
relationship	O
between	O
these	O
various	O
quantities	O
introduction	O
by	O
applying	O
jensen	O
s	O
inequality	O
with	O
fx	O
ln	O
x	O
show	O
that	O
the	O
arith	O
metic	O
mean	B
of	O
a	O
set	O
of	O
real	O
numbers	O
is	O
never	O
less	O
than	O
their	O
geometrical	O
mean	B
www	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
show	O
that	O
the	O
mutual	B
information	I
ix	O
y	O
satisfies	O
the	O
relation	O
probability	B
distributions	O
in	O
chapter	O
we	O
emphasized	O
the	O
central	O
role	O
played	O
by	O
probability	B
theory	B
in	O
the	O
solution	O
of	O
pattern	O
recognition	O
problems	O
we	O
turn	O
now	O
to	O
an	O
exploration	B
of	O
some	O
particular	O
examples	O
of	O
probability	B
distributions	O
and	O
their	O
properties	O
as	O
well	O
as	O
being	O
of	O
great	O
interest	O
in	O
their	O
own	O
right	O
these	O
distributions	O
can	O
form	O
building	O
blocks	O
for	O
more	O
complex	O
models	O
and	O
will	O
be	O
used	O
extensively	O
throughout	O
the	O
book	O
the	O
distributions	O
introduced	O
in	O
this	O
chapter	O
will	O
also	O
serve	O
another	O
important	O
purpose	O
namely	O
to	O
provide	O
us	O
with	O
the	O
opportunity	O
to	O
discuss	O
some	O
key	O
statistical	O
concepts	O
such	O
as	O
bayesian	B
inference	B
in	O
the	O
context	O
of	O
simple	O
models	O
before	O
we	O
encounter	O
them	O
in	O
more	O
complex	O
situations	O
in	O
later	O
chapters	O
one	O
role	O
for	O
the	O
distributions	O
discussed	O
in	O
this	O
chapter	O
is	O
to	O
model	O
the	O
probability	B
distribution	O
px	O
of	O
a	O
random	O
variable	O
x	O
given	O
a	O
finite	O
set	O
xn	O
of	O
observations	O
this	O
problem	O
is	O
known	O
as	O
density	B
estimation	I
for	O
the	O
purposes	O
of	O
this	O
chapter	O
we	O
shall	O
assume	O
that	O
the	O
data	O
points	O
are	O
independent	B
and	O
identically	O
distributed	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
problem	O
of	O
density	B
estimation	I
is	O
fun	O
probability	B
distributions	O
damentally	O
ill-posed	O
because	O
there	O
are	O
infinitely	O
many	O
probability	B
distributions	O
that	O
could	O
have	O
given	O
rise	O
to	O
the	O
observed	O
finite	O
data	O
set	O
indeed	O
any	O
distribution	O
px	O
that	O
is	O
nonzero	O
at	O
each	O
of	O
the	O
data	O
points	O
xn	O
is	O
a	O
potential	O
candidate	O
the	O
issue	O
of	O
choosing	O
an	O
appropriate	O
distribution	O
relates	O
to	O
the	O
problem	O
of	O
model	B
selection	I
that	O
has	O
already	O
been	O
encountered	O
in	O
the	O
context	O
of	O
polynomial	B
curve	B
fitting	I
in	O
chapter	O
and	O
that	O
is	O
a	O
central	O
issue	O
in	O
pattern	O
recognition	O
we	O
begin	O
by	O
considering	O
the	O
binomial	O
and	O
multinomial	O
distributions	O
for	O
discrete	O
random	O
variables	O
and	O
the	O
gaussian	B
distribution	O
for	O
continuous	O
random	O
variables	O
these	O
are	O
specific	O
examples	O
of	O
parametric	O
distributions	O
so-called	O
because	O
they	O
are	O
governed	O
by	O
a	O
small	O
number	O
of	O
adaptive	O
parameters	O
such	O
as	O
the	O
mean	B
and	O
variance	B
in	O
the	O
case	O
of	O
a	O
gaussian	B
for	O
example	O
to	O
apply	O
such	O
models	O
to	O
the	O
problem	O
of	O
density	B
estimation	I
we	O
need	O
a	O
procedure	O
for	O
determining	O
suitable	O
values	O
for	O
the	O
parameters	O
given	O
an	O
observed	O
data	O
set	O
in	O
a	O
frequentist	B
treatment	O
we	O
choose	O
specific	O
values	O
for	O
the	O
parameters	O
by	O
optimizing	O
some	O
criterion	O
such	O
as	O
the	O
likelihood	B
function	I
by	O
contrast	O
in	O
a	O
bayesian	B
treatment	O
we	O
introduce	O
prior	B
distributions	O
over	O
the	O
parameters	O
and	O
then	O
use	O
bayes	B
theorem	O
to	O
compute	O
the	O
corresponding	O
posterior	O
distribution	O
given	O
the	O
observed	O
data	O
we	O
shall	O
see	O
that	O
an	O
important	O
role	O
is	O
played	O
by	O
conjugate	B
priors	O
that	O
lead	O
to	O
posterior	O
distributions	O
having	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
and	O
that	O
therefore	O
lead	O
to	O
a	O
greatly	O
simplified	O
bayesian	B
analysis	I
for	O
example	O
the	O
conjugate	B
prior	B
for	O
the	O
parameters	O
of	O
the	O
multinomial	B
distribution	I
is	O
called	O
the	O
dirichlet	B
distribution	I
while	O
the	O
conjugate	B
prior	B
for	O
the	O
mean	B
of	O
a	O
gaussian	B
is	O
another	O
gaussian	B
all	O
of	O
these	O
distributions	O
are	O
examples	O
of	O
the	O
exponential	B
family	I
of	O
distributions	O
which	O
possess	O
a	O
number	O
of	O
important	O
properties	O
and	O
which	O
will	O
be	O
discussed	O
in	O
some	O
detail	O
one	O
limitation	O
of	O
the	O
parametric	O
approach	O
is	O
that	O
it	O
assumes	O
a	O
specific	O
functional	B
form	O
for	O
the	O
distribution	O
which	O
may	O
turn	O
out	O
to	O
be	O
inappropriate	O
for	O
a	O
particular	O
application	O
an	O
alternative	O
approach	O
is	O
given	O
by	O
nonparametric	O
density	B
estimation	I
methods	O
in	O
which	O
the	O
form	O
of	O
the	O
distribution	O
typically	O
depends	O
on	O
the	O
size	O
of	O
the	O
data	O
set	O
such	O
models	O
still	O
contain	O
parameters	O
but	O
these	O
control	O
the	O
model	O
complexity	O
rather	O
than	O
the	O
form	O
of	O
the	O
distribution	O
we	O
end	O
this	O
chapter	O
by	O
considering	O
three	O
nonparametric	B
methods	I
based	O
respectively	O
on	O
histograms	O
nearest-neighbours	O
and	O
kernels	O
binary	O
variables	O
we	O
begin	O
by	O
considering	O
a	O
single	O
binary	O
random	O
variable	O
x	O
for	O
example	O
x	O
might	O
describe	O
the	O
outcome	O
of	O
flipping	O
a	O
coin	O
with	O
x	O
representing	O
heads	O
and	O
x	O
representing	O
tails	O
we	O
can	O
imagine	O
that	O
this	O
is	O
a	O
damaged	O
coin	O
so	O
that	O
the	O
probability	B
of	O
landing	O
heads	O
is	O
not	O
necessarily	O
the	O
same	O
as	O
that	O
of	O
landing	O
tails	O
the	O
probability	B
of	O
x	O
will	O
be	O
denoted	O
by	O
the	O
parameter	O
so	O
that	O
px	O
binary	O
variables	O
where	O
from	O
which	O
it	O
follows	O
that	O
px	O
the	O
probability	B
distribution	O
over	O
x	O
can	O
therefore	O
be	O
written	O
in	O
the	O
form	O
bernx	O
x	O
exercise	O
which	O
is	O
known	O
as	O
the	O
bernoulli	B
distribution	I
it	O
is	O
easily	O
verified	O
that	O
this	O
distribution	O
is	O
normalized	O
and	O
that	O
it	O
has	O
mean	B
and	O
variance	B
given	O
by	O
ex	O
varx	O
now	O
suppose	O
we	O
have	O
a	O
data	O
set	O
d	O
xn	O
of	O
observed	O
values	O
of	O
x	O
we	O
can	O
construct	O
the	O
likelihood	B
function	I
which	O
is	O
a	O
function	O
of	O
on	O
the	O
assumption	O
that	O
the	O
observations	O
are	O
drawn	O
independently	O
from	O
px	O
so	O
that	O
pd	O
pxn	O
xn	O
in	O
a	O
frequentist	B
setting	O
we	O
can	O
estimate	O
a	O
value	O
for	O
by	O
maximizing	O
the	O
likelihood	B
function	I
or	O
equivalently	O
by	O
maximizing	O
the	O
logarithm	O
of	O
the	O
likelihood	O
in	O
the	O
case	O
of	O
the	O
bernoulli	B
distribution	I
the	O
log	O
likelihood	B
function	I
is	O
given	O
by	O
ln	O
pd	O
ln	O
pxn	O
ln	O
xn	O
section	O
at	O
this	O
point	O
it	O
is	O
worth	O
noting	O
that	O
the	O
log	O
likelihood	B
function	I
depends	O
on	O
the	O
n	O
observations	O
xn	O
only	O
through	O
their	O
sum	O
n	O
xn	O
this	O
sum	O
provides	O
an	O
example	O
of	O
a	O
sufficient	O
statistic	O
for	O
the	O
data	O
under	O
this	O
distribution	O
and	O
we	O
shall	O
study	O
the	O
important	O
role	O
of	O
sufficient	B
statistics	I
in	O
some	O
detail	O
if	O
we	O
set	O
the	O
derivative	B
of	O
ln	O
pd	O
with	O
respect	O
to	O
equal	O
to	O
zero	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
estimator	O
ml	O
n	O
xn	O
jacob	O
bernoulli	B
jacob	O
bernoulli	B
also	O
known	O
as	O
jacques	O
or	O
james	O
bernoulli	B
was	O
a	O
swiss	O
mathematician	O
and	O
was	O
the	O
first	O
of	O
many	O
in	O
the	O
bernoulli	B
family	O
to	O
pursue	O
a	O
career	O
in	O
science	O
and	O
mathematics	O
although	O
compelled	O
to	O
study	O
philosophy	O
and	O
theology	O
against	O
his	O
will	O
by	O
his	O
parents	O
he	O
travelled	O
extensively	O
after	O
graduating	O
in	O
order	O
to	O
meet	O
with	O
many	O
of	O
the	O
leading	O
scientists	O
of	O
his	O
time	O
including	O
boyle	O
and	O
hooke	O
in	O
england	O
when	O
he	O
returned	O
to	O
switzerland	O
he	O
taught	O
mechanics	O
and	O
became	O
professor	O
of	O
mathematics	O
at	O
basel	O
in	O
unfortunately	O
rivalry	O
between	O
jacob	O
and	O
his	O
younger	O
brother	O
johann	O
turned	O
an	O
initially	O
productive	O
collaboration	O
into	O
a	O
bitter	O
and	O
public	O
dispute	O
jacob	O
s	O
most	O
significant	O
contributions	O
to	O
mathematics	O
appeared	O
in	O
the	O
artofconjecture	O
published	O
in	O
eight	O
years	O
after	O
his	O
death	O
which	O
deals	O
with	O
topics	O
in	O
probability	B
theory	B
including	O
what	O
has	O
become	O
known	O
as	O
the	O
bernoulli	B
distribution	I
probability	B
distributions	O
figure	O
histogram	O
plot	O
of	O
the	O
binomial	B
distribution	I
as	O
a	O
function	O
of	O
m	O
for	O
n	O
and	O
m	O
which	O
is	O
also	O
known	O
as	O
the	O
sample	B
mean	B
if	O
we	O
denote	O
the	O
number	O
of	O
observations	O
of	O
x	O
within	O
this	O
data	O
set	O
by	O
m	O
then	O
we	O
can	O
write	O
in	O
the	O
form	O
ml	O
m	O
n	O
so	O
that	O
the	O
probability	B
of	O
landing	O
heads	O
is	O
given	O
in	O
this	O
maximum	B
likelihood	I
framework	O
by	O
the	O
fraction	O
of	O
observations	O
of	O
heads	O
in	O
the	O
data	O
set	O
now	O
suppose	O
we	O
flip	O
a	O
coin	O
say	O
times	O
and	O
happen	O
to	O
observe	O
heads	O
then	O
n	O
m	O
and	O
ml	O
in	O
this	O
case	O
the	O
maximum	B
likelihood	I
result	O
would	O
predict	O
that	O
all	O
future	O
observations	O
should	O
give	O
heads	O
common	O
sense	O
tells	O
us	O
that	O
this	O
is	O
unreasonable	O
and	O
in	O
fact	O
this	O
is	O
an	O
extreme	O
example	O
of	O
the	O
over-fitting	B
associated	O
with	O
maximum	B
likelihood	I
we	O
shall	O
see	O
shortly	O
how	O
to	O
arrive	O
at	O
more	O
sensible	O
conclusions	O
through	O
the	O
introduction	O
of	O
a	O
prior	B
distribution	O
over	O
we	O
can	O
also	O
work	O
out	O
the	O
distribution	O
of	O
the	O
number	O
m	O
of	O
observations	O
of	O
x	O
given	O
that	O
the	O
data	O
set	O
has	O
size	O
n	O
this	O
is	O
called	O
the	O
binomial	B
distribution	I
and	O
from	O
we	O
see	O
that	O
it	O
is	O
proportional	O
to	O
m	O
in	O
order	O
to	O
obtain	O
the	O
normalization	O
coefficient	O
we	O
note	O
that	O
out	O
of	O
n	O
coin	O
flips	O
we	O
have	O
to	O
add	O
up	O
all	O
of	O
the	O
possible	O
ways	O
of	O
obtaining	O
m	O
heads	O
so	O
that	O
the	O
binomial	B
distribution	I
can	O
be	O
written	O
binmn	O
m	O
where	O
n	O
m	O
n	O
m	O
n	O
m	O
m	O
exercise	O
is	O
the	O
number	O
of	O
ways	O
of	O
choosing	O
m	O
objects	O
out	O
of	O
a	O
total	O
of	O
n	O
identical	O
objects	O
figure	O
shows	O
a	O
plot	O
of	O
the	O
binomial	B
distribution	I
for	O
n	O
and	O
the	O
mean	B
and	O
variance	B
of	O
the	O
binomial	B
distribution	I
can	O
be	O
found	O
by	O
using	O
the	O
result	O
of	O
exercise	O
which	O
shows	O
that	O
for	O
independent	B
events	O
the	O
mean	B
of	O
the	O
sum	O
is	O
the	O
sum	O
of	O
the	O
means	O
and	O
the	O
variance	B
of	O
the	O
sum	O
is	O
the	O
sum	O
of	O
the	O
variances	O
because	O
m	O
xn	O
and	O
for	O
each	O
observation	O
the	O
mean	B
and	O
variance	B
are	O
binary	O
variables	O
given	O
by	O
and	O
respectively	O
we	O
have	O
em	B
varm	O
mbinmn	O
n	O
binmn	O
n	O
exercise	O
these	O
results	O
can	O
also	O
be	O
proved	O
directly	O
using	O
calculus	O
the	O
beta	B
distribution	I
we	O
have	O
seen	O
in	O
that	O
the	O
maximum	B
likelihood	I
setting	O
for	O
the	O
parameter	O
in	O
the	O
bernoulli	B
distribution	I
and	O
hence	O
in	O
the	O
binomial	B
distribution	I
is	O
given	O
by	O
the	O
fraction	O
of	O
the	O
observations	O
in	O
the	O
data	O
set	O
having	O
x	O
as	O
we	O
have	O
already	O
noted	O
this	O
can	O
give	O
severely	O
over-fitted	O
results	O
for	O
small	O
data	O
sets	O
in	O
order	O
to	O
develop	O
a	O
bayesian	B
treatment	O
for	O
this	O
problem	O
we	O
need	O
to	O
introduce	O
a	O
prior	B
distribution	O
p	O
over	O
the	O
parameter	O
here	O
we	O
consider	O
a	O
form	O
of	O
prior	B
distribution	O
that	O
has	O
a	O
simple	O
interpretation	O
as	O
well	O
as	O
some	O
useful	O
analytical	O
properties	O
to	O
motivate	O
this	O
prior	B
we	O
note	O
that	O
the	O
likelihood	B
function	I
takes	O
the	O
form	O
of	O
the	O
product	O
of	O
factors	O
of	O
the	O
form	O
x	O
if	O
we	O
choose	O
a	O
prior	B
to	O
be	O
proportional	O
to	O
powers	O
of	O
and	O
then	O
the	O
posterior	O
distribution	O
which	O
is	O
proportional	O
to	O
the	O
product	O
of	O
the	O
prior	B
and	O
the	O
likelihood	B
function	I
will	O
have	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
this	O
property	O
is	O
called	O
conjugacy	O
and	O
we	O
will	O
see	O
several	O
examples	O
of	O
it	O
later	O
in	O
this	O
chapter	O
we	O
therefore	O
choose	O
a	O
prior	B
called	O
the	O
beta	B
distribution	I
given	O
by	O
exercise	O
beta	O
b	O
b	O
a	O
where	O
is	O
the	O
gamma	B
function	I
defined	O
by	O
and	O
the	O
coefficient	O
in	O
ensures	O
that	O
the	O
beta	B
distribution	I
is	O
normalized	O
so	O
that	O
beta	O
b	O
d	O
exercise	O
the	O
mean	B
and	O
variance	B
of	O
the	O
beta	B
distribution	I
are	O
given	O
by	O
e	O
var	O
a	O
a	O
b	O
b	O
ab	O
the	O
parameters	O
a	O
and	O
b	O
are	O
often	O
called	O
hyperparameters	O
because	O
they	O
control	O
the	O
distribution	O
of	O
the	O
parameter	O
figure	O
shows	O
plots	O
of	O
the	O
beta	B
distribution	I
for	O
various	O
values	O
of	O
the	O
hyperparameters	O
the	O
posterior	O
distribution	O
of	O
is	O
now	O
obtained	O
by	O
multiplying	O
the	O
beta	O
prior	B
by	O
the	O
binomial	O
likelihood	B
function	I
and	O
normalizing	O
keeping	O
only	O
the	O
factors	O
that	O
depend	O
on	O
we	O
see	O
that	O
this	O
posterior	O
distribution	O
has	O
the	O
form	O
p	O
l	O
a	O
b	O
ma	O
probability	B
distributions	O
a	O
b	O
a	O
b	O
a	O
b	O
a	O
b	O
figure	O
plots	O
of	O
the	O
beta	B
distribution	I
beta	O
b	O
given	O
by	O
as	O
a	O
function	O
of	O
for	O
various	O
values	O
of	O
the	O
hyperparameters	O
a	O
and	O
b	O
where	O
l	O
n	O
m	O
and	O
therefore	O
corresponds	O
to	O
the	O
number	O
of	O
tails	O
in	O
the	O
coin	O
example	O
we	O
see	O
that	O
has	O
the	O
same	O
functional	B
dependence	O
on	O
as	O
the	O
prior	B
distribution	O
reflecting	O
the	O
conjugacy	O
properties	O
of	O
the	O
prior	B
with	O
respect	O
to	O
the	O
likelihood	B
function	I
indeed	O
it	O
is	O
simply	O
another	O
beta	B
distribution	I
and	O
its	O
normalization	O
coefficient	O
can	O
therefore	O
be	O
obtained	O
by	O
comparison	O
with	O
to	O
give	O
p	O
l	O
a	O
b	O
a	O
l	O
b	O
a	O
b	O
ma	O
we	O
see	O
that	O
the	O
effect	O
of	O
observing	O
a	O
data	O
set	O
of	O
m	O
observations	O
of	O
x	O
and	O
l	O
observations	O
of	O
x	O
has	O
been	O
to	O
increase	O
the	O
value	O
of	O
a	O
by	O
m	O
and	O
the	O
value	O
of	O
b	O
by	O
l	O
in	O
going	O
from	O
the	O
prior	B
distribution	O
to	O
the	O
posterior	O
distribution	O
this	O
allows	O
us	O
to	O
provide	O
a	O
simple	O
interpretation	O
of	O
the	O
hyperparameters	O
a	O
and	O
b	O
in	O
the	O
prior	B
as	O
an	O
effective	B
number	I
of	I
observations	I
of	O
x	O
and	O
x	O
respectively	O
note	O
that	O
a	O
and	O
b	O
need	O
not	O
be	O
integers	O
furthermore	O
the	O
posterior	O
distribution	O
can	O
act	O
as	O
the	O
prior	B
if	O
we	O
subsequently	O
observe	O
additional	O
data	O
to	O
see	O
this	O
we	O
can	O
imagine	O
taking	O
observations	O
one	O
at	O
a	O
time	O
and	O
after	O
each	O
observation	O
updating	O
the	O
current	O
posterior	O
prior	B
likelihood	B
function	I
binary	O
variables	O
posterior	O
figure	O
illustration	O
of	O
one	O
step	O
of	O
sequential	O
bayesian	B
inference	B
the	O
prior	B
is	O
given	O
by	O
a	O
beta	B
distribution	I
with	O
parameters	O
a	O
b	O
and	O
the	O
likelihood	B
function	I
given	O
by	O
with	O
n	O
m	O
corresponds	O
to	O
a	O
single	O
observation	O
of	O
x	O
so	O
that	O
the	O
posterior	O
is	O
given	O
by	O
a	O
beta	B
distribution	I
with	O
parameters	O
a	O
b	O
distribution	O
by	O
multiplying	O
by	O
the	O
likelihood	B
function	I
for	O
the	O
new	O
observation	O
and	O
then	O
normalizing	O
to	O
obtain	O
the	O
new	O
revised	O
posterior	O
distribution	O
at	O
each	O
stage	O
the	O
posterior	O
is	O
a	O
beta	B
distribution	I
with	O
some	O
total	O
number	O
of	O
and	O
actual	O
observed	O
values	O
for	O
x	O
and	O
x	O
given	O
by	O
the	O
parameters	O
a	O
and	O
b	O
incorporation	O
of	O
an	O
additional	O
observation	O
of	O
x	O
simply	O
corresponds	O
to	O
incrementing	O
the	O
value	O
of	O
a	O
by	O
whereas	O
for	O
an	O
observation	O
of	O
x	O
we	O
increment	O
b	O
by	O
figure	O
illustrates	O
one	O
step	O
in	O
this	O
process	O
section	O
we	O
see	O
that	O
this	O
sequential	O
approach	O
to	O
learning	B
arises	O
naturally	O
when	O
we	O
adopt	O
a	O
bayesian	B
viewpoint	O
it	O
is	O
independent	B
of	O
the	O
choice	O
of	O
prior	B
and	O
of	O
the	O
likelihood	B
function	I
and	O
depends	O
only	O
on	O
the	O
assumption	O
of	O
i	O
i	O
d	O
data	O
sequential	O
methods	O
make	O
use	O
of	O
observations	O
one	O
at	O
a	O
time	O
or	O
in	O
small	O
batches	O
and	O
then	O
discard	O
them	O
before	O
the	O
next	O
observations	O
are	O
used	O
they	O
can	O
be	O
used	O
for	O
example	O
in	O
real-time	O
learning	B
scenarios	O
where	O
a	O
steady	O
stream	O
of	O
data	O
is	O
arriving	O
and	O
predictions	O
must	O
be	O
made	O
before	O
all	O
of	O
the	O
data	O
is	O
seen	O
because	O
they	O
do	O
not	O
require	O
the	O
whole	O
data	O
set	O
to	O
be	O
stored	O
or	O
loaded	O
into	O
memory	O
sequential	O
methods	O
are	O
also	O
useful	O
for	O
large	O
data	O
sets	O
maximum	B
likelihood	I
methods	O
can	O
also	O
be	O
cast	O
into	O
a	O
sequential	O
framework	O
if	O
our	O
goal	O
is	O
to	O
predict	O
as	O
best	O
we	O
can	O
the	O
outcome	O
of	O
the	O
next	O
trial	O
then	O
we	O
must	O
evaluate	O
the	O
predictive	B
distribution	I
of	O
x	O
given	O
the	O
observed	O
data	O
set	O
d	O
from	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
this	O
takes	O
the	O
form	O
px	O
px	O
d	O
p	O
d	O
e	O
using	O
the	O
result	O
for	O
the	O
posterior	O
distribution	O
p	O
together	O
with	O
the	O
result	O
for	O
the	O
mean	B
of	O
the	O
beta	B
distribution	I
we	O
obtain	O
m	O
a	O
px	O
m	O
a	O
l	O
b	O
which	O
has	O
a	O
simple	O
interpretation	O
as	O
the	O
total	O
fraction	O
of	O
observations	O
real	O
observations	O
and	O
fictitious	O
prior	B
observations	O
that	O
correspond	O
to	O
x	O
note	O
that	O
in	O
the	O
limit	O
of	O
an	O
infinitely	O
large	O
data	O
set	O
m	O
l	O
the	O
result	O
reduces	O
to	O
the	O
maximum	B
likelihood	I
result	O
as	O
we	O
shall	O
see	O
it	O
is	O
a	O
very	O
general	O
property	O
that	O
the	O
bayesian	B
and	O
maximum	B
likelihood	I
results	O
will	O
agree	O
in	O
the	O
limit	O
of	O
an	O
infinitely	O
probability	B
distributions	O
exercise	O
exercise	O
large	O
data	O
set	O
for	O
a	O
finite	O
data	O
set	O
the	O
posterior	O
mean	B
for	O
always	O
lies	O
between	O
the	O
prior	B
mean	B
and	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
corresponding	O
to	O
the	O
relative	B
frequencies	O
of	O
events	O
given	O
by	O
from	O
figure	O
we	O
see	O
that	O
as	O
the	O
number	O
of	O
observations	O
increases	O
so	O
the	O
posterior	O
distribution	O
becomes	O
more	O
sharply	O
peaked	O
this	O
can	O
also	O
be	O
seen	O
from	O
the	O
result	O
for	O
the	O
variance	B
of	O
the	O
beta	B
distribution	I
in	O
which	O
we	O
see	O
that	O
the	O
variance	B
goes	O
to	O
zero	O
for	O
a	O
or	O
b	O
in	O
fact	O
we	O
might	O
wonder	O
whether	O
it	O
is	O
a	O
general	O
property	O
of	O
bayesian	B
learning	B
that	O
as	O
we	O
observe	O
more	O
and	O
more	O
data	O
the	O
uncertainty	O
represented	O
by	O
the	O
posterior	O
distribution	O
will	O
steadily	O
decrease	O
to	O
address	O
this	O
we	O
can	O
take	O
a	O
frequentist	B
view	O
of	O
bayesian	B
learning	B
and	O
show	O
that	O
on	O
average	O
such	O
a	O
property	O
does	O
indeed	O
hold	O
consider	O
a	O
general	O
bayesian	B
inference	B
problem	O
for	O
a	O
parameter	O
for	O
which	O
we	O
have	O
observed	O
a	O
data	O
set	O
d	O
described	O
by	O
the	O
joint	O
distribution	O
p	O
the	O
following	O
result	O
where	O
e	O
ede	O
pd	O
dd	O
p	O
d	O
e	O
ed	O
p	O
d	O
says	O
that	O
the	O
posterior	O
mean	B
of	O
averaged	O
over	O
the	O
distribution	O
generating	O
the	O
data	O
is	O
equal	O
to	O
the	O
prior	B
mean	B
of	O
similarly	O
we	O
can	O
show	O
that	O
var	O
ed	O
vard	O
the	O
term	O
on	O
the	O
left-hand	O
side	O
of	O
is	O
the	O
prior	B
variance	B
of	O
on	O
the	O
righthand	O
side	O
the	O
first	O
term	O
is	O
the	O
average	O
posterior	O
variance	B
of	O
and	O
the	O
second	O
term	O
measures	O
the	O
variance	B
in	O
the	O
posterior	O
mean	B
of	O
because	O
this	O
variance	B
is	O
a	O
positive	O
quantity	O
this	O
result	O
shows	O
that	O
on	O
average	O
the	O
posterior	O
variance	B
of	O
is	O
smaller	O
than	O
the	O
prior	B
variance	B
the	O
reduction	O
in	O
variance	B
is	O
greater	O
if	O
the	O
variance	B
in	O
the	O
posterior	O
mean	B
is	O
greater	O
note	O
however	O
that	O
this	O
result	O
only	O
holds	O
on	O
average	O
and	O
that	O
for	O
a	O
particular	O
observed	O
data	O
set	O
it	O
is	O
possible	O
for	O
the	O
posterior	O
variance	B
to	O
be	O
larger	O
than	O
the	O
prior	B
variance	B
multinomial	O
variables	O
binary	O
variables	O
can	O
be	O
used	O
to	O
describe	O
quantities	O
that	O
can	O
take	O
one	O
of	O
two	O
possible	O
values	O
often	O
however	O
we	O
encounter	O
discrete	O
variables	O
that	O
can	O
take	O
on	O
one	O
of	O
k	O
possible	O
mutually	O
exclusive	O
states	O
although	O
there	O
are	O
various	O
alternative	O
ways	O
to	O
express	O
such	O
variables	O
we	O
shall	O
see	O
shortly	O
that	O
a	O
particularly	O
convenient	O
representation	O
is	O
the	O
scheme	O
in	O
which	O
the	O
variable	O
is	O
represented	O
by	O
a	O
k-dimensional	O
vector	O
x	O
in	O
which	O
one	O
of	O
the	O
elements	O
xk	O
equals	O
and	O
all	O
remaining	O
elements	O
equal	O
multinomial	O
variables	O
x	O
so	O
for	O
instance	O
if	O
we	O
have	O
a	O
variable	O
that	O
can	O
take	O
k	O
states	O
and	O
a	O
particular	O
observation	O
of	O
the	O
variable	O
happens	O
to	O
correspond	O
to	O
the	O
state	O
where	O
then	O
x	O
will	O
be	O
represented	O
by	O
note	O
that	O
such	O
vectors	O
satisfy	O
by	O
the	O
parameter	O
k	O
then	O
the	O
distribution	O
of	O
x	O
is	O
given	O
x	O
xk	O
if	O
we	O
denote	O
the	O
probability	B
of	O
xk	O
px	O
xk	O
k	O
where	O
kt	O
and	O
the	O
parameters	O
k	O
are	O
constrained	O
to	O
satisfy	O
k	O
k	O
k	O
because	O
they	O
represent	O
probabilities	O
the	O
distribution	O
can	O
be	O
and	O
regarded	O
as	O
a	O
generalization	B
of	O
the	O
bernoulli	B
distribution	I
to	O
more	O
than	O
two	O
outcomes	O
it	O
is	O
easily	O
seen	O
that	O
the	O
distribution	O
is	O
normalized	O
px	O
k	O
and	O
that	O
ex	O
now	O
consider	O
a	O
data	O
set	O
d	O
of	O
n	O
independent	B
observations	O
xn	O
the	O
x	O
px	O
m	O
corresponding	O
likelihood	B
function	I
takes	O
the	O
form	O
pd	O
xnk	O
k	O
p	O
k	O
n	O
xnk	O
mk	O
k	O
we	O
see	O
that	O
the	O
likelihood	B
function	I
depends	O
on	O
the	O
n	O
data	O
points	O
only	O
through	O
the	O
k	O
quantities	O
mk	O
xnk	O
n	O
which	O
represent	O
the	O
number	O
of	O
observations	O
of	O
xk	O
these	O
are	O
called	O
the	O
sufficient	B
statistics	I
for	O
this	O
distribution	O
in	O
order	O
to	O
find	O
the	O
maximum	B
likelihood	I
solution	O
for	O
we	O
need	O
to	O
maximize	O
ln	O
pd	O
with	O
respect	O
to	O
k	O
taking	O
account	O
of	O
the	O
constraint	O
that	O
the	O
k	O
must	O
sum	O
to	O
one	O
this	O
can	O
be	O
achieved	O
using	O
a	O
lagrange	B
multiplier	I
and	O
maximizing	O
mk	O
ln	O
k	O
k	O
setting	O
the	O
derivative	B
of	O
with	O
respect	O
to	O
k	O
to	O
zero	O
we	O
obtain	O
k	O
mk	O
section	O
appendix	O
e	O
probability	B
distributions	O
we	O
can	O
solve	O
for	O
the	O
lagrange	B
multiplier	I
by	O
substituting	O
into	O
the	O
constraint	O
k	O
k	O
to	O
give	O
n	O
thus	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
solution	O
in	O
the	O
form	O
k	O
mk	O
ml	O
n	O
which	O
is	O
the	O
fraction	O
of	O
the	O
n	O
observations	O
for	O
which	O
xk	O
we	O
can	O
consider	O
the	O
joint	O
distribution	O
of	O
the	O
quantities	O
mk	O
conditioned	O
on	O
the	O
parameters	O
and	O
on	O
the	O
total	O
number	O
n	O
of	O
observations	O
from	O
this	O
takes	O
the	O
form	O
mk	O
n	O
n	O
mk	O
mk	O
k	O
which	O
is	O
known	O
as	O
the	O
multinomial	B
distribution	I
the	O
normalization	O
coefficient	O
is	O
the	O
number	O
of	O
ways	O
of	O
partitioning	O
n	O
objects	O
into	O
k	O
groups	O
of	O
size	O
mk	O
and	O
is	O
given	O
by	O
n	O
mk	O
n	O
mk	O
note	O
that	O
the	O
variables	O
mk	O
are	O
subject	O
to	O
the	O
constraint	O
mk	O
n	O
the	O
dirichlet	B
distribution	I
we	O
now	O
introduce	O
a	O
family	O
of	O
prior	B
distributions	O
for	O
the	O
parameters	O
k	O
of	O
the	O
multinomial	B
distribution	I
by	O
inspection	O
of	O
the	O
form	O
of	O
the	O
multinomial	B
distribution	I
we	O
see	O
that	O
the	O
conjugate	B
prior	B
is	O
given	O
by	O
k	O
k	O
p	O
where	O
k	O
and	O
k	O
k	O
here	O
k	O
are	O
the	O
parameters	O
of	O
the	O
distribution	O
and	O
denotes	O
kt	O
note	O
that	O
because	O
of	O
the	O
summation	O
constraint	O
the	O
distribution	O
over	O
the	O
space	O
of	O
the	O
k	O
is	O
confined	O
to	O
a	O
simplex	B
of	O
dimensionality	O
k	O
as	O
illustrated	O
for	O
k	O
in	O
figure	O
exercise	O
the	O
normalized	O
form	O
for	O
this	O
distribution	O
is	O
by	O
dir	O
k	O
k	O
k	O
which	O
is	O
called	O
the	O
dirichlet	B
distribution	I
here	O
is	O
the	O
gamma	B
function	I
defined	O
by	O
while	O
k	O
multinomial	O
variables	O
figure	O
the	O
dirichlet	B
distribution	I
over	O
three	O
variables	O
is	O
confined	O
to	O
a	O
simplex	B
bounded	O
linear	O
manifold	B
of	O
the	O
form	O
shown	O
as	O
a	O
consequence	O
of	O
the	O
constraints	O
k	O
and	O
k	O
k	O
p	O
plots	O
of	O
the	O
dirichlet	B
distribution	I
over	O
the	O
simplex	B
for	O
various	O
settings	O
of	O
the	O
parameters	O
k	O
are	O
shown	O
in	O
figure	O
posterior	O
distribution	O
for	O
the	O
parameters	O
k	O
in	O
the	O
form	O
multiplying	O
the	O
prior	B
by	O
the	O
likelihood	B
function	I
we	O
obtain	O
the	O
p	O
pd	O
kmk	O
k	O
we	O
see	O
that	O
the	O
posterior	O
distribution	O
again	O
takes	O
the	O
form	O
of	O
a	O
dirichlet	B
distribution	I
confirming	O
that	O
the	O
dirichlet	B
is	O
indeed	O
a	O
conjugate	B
prior	B
for	O
the	O
multinomial	O
this	O
allows	O
us	O
to	O
determine	O
the	O
normalization	O
coefficient	O
by	O
comparison	O
with	O
so	O
that	O
p	O
dir	O
m	O
n	O
k	O
mk	O
kmk	O
k	O
where	O
we	O
have	O
denoted	O
m	O
mkt	O
as	O
for	O
the	O
case	O
of	O
the	O
binomial	B
distribution	I
with	O
its	O
beta	O
prior	B
we	O
can	O
interpret	O
the	O
parameters	O
k	O
of	O
the	O
dirichlet	B
prior	B
as	O
an	O
effective	B
number	I
of	I
observations	I
of	O
xk	O
note	O
that	O
two-state	O
quantities	O
can	O
either	O
be	O
represented	O
as	O
binary	O
variables	O
and	O
lejeune	O
dirichlet	B
johann	O
peter	O
gustav	O
lejeune	O
dirichlet	B
was	O
a	O
modest	O
and	O
reserved	O
mathematician	O
who	O
made	O
contributions	O
in	O
number	O
theory	B
mechanics	O
and	O
astronomy	O
and	O
who	O
gave	O
the	O
first	O
rigorous	O
analysis	O
of	O
fourier	O
series	O
his	O
family	O
originated	O
from	O
richelet	O
in	O
belgium	O
and	O
the	O
name	O
lejeune	O
dirichlet	B
comes	O
from	O
le	O
jeune	O
de	O
richelet	O
young	O
person	O
from	O
richelet	O
dirichlet	B
s	O
first	O
paper	O
which	O
was	O
published	O
in	O
brought	O
him	O
instant	O
fame	O
it	O
concerned	O
fermat	O
s	O
last	O
theorem	O
which	O
claims	O
that	O
there	O
are	O
no	O
positive	O
integer	O
solutions	O
to	O
xn	O
yn	O
zn	O
for	O
n	O
dirichlet	B
gave	O
a	O
partial	O
proof	O
for	O
the	O
case	O
n	O
which	O
was	O
sent	O
to	O
legendre	O
for	O
review	O
and	O
who	O
in	O
turn	O
completed	O
the	O
proof	O
later	O
dirichlet	B
gave	O
a	O
complete	O
proof	O
for	O
n	O
although	O
a	O
full	O
proof	O
of	O
fermat	O
s	O
last	O
theorem	O
for	O
arbitrary	O
n	O
had	O
to	O
wait	O
until	O
the	O
work	O
of	O
andrew	O
wiles	O
in	O
the	O
closing	O
years	O
of	O
the	O
century	O
probability	B
distributions	O
figure	O
plots	O
of	O
the	O
dirichlet	B
distribution	I
over	O
three	O
variables	O
where	O
the	O
two	O
horizontal	O
axes	O
are	O
coordinates	O
in	O
the	O
plane	O
of	O
the	O
simplex	B
and	O
the	O
vertical	O
axis	O
corresponds	O
to	O
the	O
value	O
of	O
the	O
density	B
here	O
k	O
on	O
the	O
left	O
plot	O
k	O
in	O
the	O
centre	O
plot	O
and	O
k	O
in	O
the	O
right	O
plot	O
modelled	O
using	O
the	O
binomial	B
distribution	I
or	O
as	O
variables	O
and	O
modelled	O
using	O
the	O
multinomial	B
distribution	I
with	O
k	O
the	O
gaussian	B
distribution	O
the	O
gaussian	B
also	O
known	O
as	O
the	O
normal	O
distribution	O
is	O
a	O
widely	O
used	O
model	O
for	O
the	O
distribution	O
of	O
continuous	O
variables	O
in	O
the	O
case	O
of	O
a	O
single	O
variable	O
x	O
the	O
gaussian	B
distribution	O
can	O
be	O
written	O
in	O
the	O
form	O
n	O
exp	O
n	O
where	O
is	O
the	O
mean	B
and	O
is	O
the	O
variance	B
for	O
a	O
d-dimensional	O
vector	O
x	O
the	O
multivariate	O
gaussian	B
distribution	O
takes	O
the	O
form	O
where	O
is	O
a	O
d-dimensional	O
mean	B
vector	O
is	O
a	O
d	O
d	O
covariance	B
matrix	O
and	O
denotes	O
the	O
determinant	O
of	O
exp	O
section	O
exercise	O
the	O
gaussian	B
distribution	O
arises	O
in	O
many	O
different	O
contexts	O
and	O
can	O
be	O
motivated	O
from	O
a	O
variety	O
of	O
different	O
perspectives	O
for	O
example	O
we	O
have	O
already	O
seen	O
that	O
for	O
a	O
single	O
real	O
variable	O
the	O
distribution	O
that	O
maximizes	O
the	O
entropy	B
is	O
the	O
gaussian	B
this	O
property	O
applies	O
also	O
to	O
the	O
multivariate	O
gaussian	B
another	O
situation	O
in	O
which	O
the	O
gaussian	B
distribution	O
arises	O
is	O
when	O
we	O
consider	O
the	O
sum	O
of	O
multiple	O
random	O
variables	O
the	O
central	B
limit	I
theorem	I
to	O
laplace	B
tells	O
us	O
that	O
subject	O
to	O
certain	O
mild	O
conditions	O
the	O
sum	O
of	O
a	O
set	O
of	O
random	O
variables	O
which	O
is	O
of	O
course	O
itself	O
a	O
random	O
variable	O
has	O
a	O
distribution	O
that	O
becomes	O
increasingly	O
gaussian	B
as	O
the	O
number	O
of	O
terms	O
in	O
the	O
sum	O
increases	O
we	O
can	O
the	O
gaussian	B
distribution	O
n	O
n	O
n	O
figure	O
histogram	O
plots	O
of	O
the	O
mean	B
of	O
n	O
uniformly	O
distributed	O
numbers	O
for	O
various	O
values	O
of	O
n	O
we	O
observe	O
that	O
as	O
n	O
increases	O
the	O
distribution	O
tends	O
towards	O
a	O
gaussian	B
illustrate	O
this	O
by	O
considering	O
n	O
variables	O
xn	O
each	O
of	O
which	O
has	O
a	O
uniform	B
distribution	I
over	O
the	O
interval	O
and	O
then	O
considering	O
the	O
distribution	O
of	O
the	O
mean	B
xn	O
for	O
large	O
n	O
this	O
distribution	O
tends	O
to	O
a	O
gaussian	B
as	O
illustrated	O
in	O
practice	O
the	O
convergence	O
to	O
a	O
gaussian	B
as	O
n	O
increases	O
can	O
be	O
in	O
figure	O
very	O
rapid	O
one	O
consequence	O
of	O
this	O
result	O
is	O
that	O
the	O
binomial	B
distribution	I
which	O
is	O
a	O
distribution	O
over	O
m	O
defined	O
by	O
the	O
sum	O
of	O
n	O
observations	O
of	O
the	O
random	O
binary	O
variable	O
x	O
will	O
tend	O
to	O
a	O
gaussian	B
as	O
n	O
figure	O
for	O
the	O
case	O
of	O
n	O
the	O
gaussian	B
distribution	O
has	O
many	O
important	O
analytical	O
properties	O
and	O
we	O
shall	O
consider	O
several	O
of	O
these	O
in	O
detail	O
as	O
a	O
result	O
this	O
section	O
will	O
be	O
rather	O
more	O
technically	O
involved	O
than	O
some	O
of	O
the	O
earlier	O
sections	O
and	O
will	O
require	O
familiarity	O
with	O
various	O
matrix	O
identities	O
however	O
we	O
strongly	O
encourage	O
the	O
reader	O
to	O
become	O
proficient	O
in	O
manipulating	O
gaussian	B
distributions	O
using	O
the	O
techniques	O
presented	O
here	O
as	O
this	O
will	O
prove	O
invaluable	O
in	O
understanding	O
the	O
more	O
complex	O
models	O
presented	O
in	O
later	O
chapters	O
we	O
begin	O
by	O
considering	O
the	O
geometrical	O
form	O
of	O
the	O
gaussian	B
distribution	O
the	O
appendix	O
c	O
carl	O
friedrich	O
gauss	B
it	O
is	O
said	O
that	O
when	O
gauss	B
went	O
to	O
elementary	O
school	O
at	O
age	O
his	O
teacher	O
b	O
uttner	O
trying	O
to	O
keep	O
the	O
class	O
occupied	O
asked	O
the	O
pupils	O
to	O
sum	O
the	O
integers	O
from	O
to	O
to	O
the	O
teacher	O
s	O
amazement	O
gauss	B
arrived	O
at	O
the	O
answer	O
in	O
a	O
matter	O
of	O
moments	O
by	O
noting	O
that	O
the	O
sum	O
can	O
be	O
represented	O
as	O
pairs	O
etc	O
each	O
of	O
which	O
added	O
to	O
giving	O
the	O
answer	O
it	O
is	O
now	O
believed	O
that	O
the	O
problem	O
which	O
was	O
actually	O
set	O
was	O
of	O
the	O
same	O
form	O
but	O
somewhat	O
harder	O
in	O
that	O
the	O
sequence	O
had	O
a	O
larger	O
starting	O
value	O
and	O
a	O
larger	O
increment	O
gauss	B
was	O
a	O
german	O
math	O
ematician	O
and	O
scientist	O
with	O
a	O
reputation	O
for	O
being	O
a	O
hard-working	O
perfectionist	O
one	O
of	O
his	O
many	O
contributions	O
was	O
to	O
show	O
that	O
least	O
squares	O
can	O
be	O
derived	O
under	O
the	O
assumption	O
of	O
normally	O
distributed	O
errors	O
he	O
also	O
created	O
an	O
early	O
formulation	O
of	O
non-euclidean	O
geometry	O
self-consistent	O
geometrical	O
theory	B
that	O
violates	O
the	O
axioms	O
of	O
euclid	O
but	O
was	O
reluctant	O
to	O
discuss	O
it	O
openly	O
for	O
fear	O
that	O
his	O
reputation	O
might	O
suffer	O
if	O
it	O
were	O
seen	O
that	O
he	O
believed	O
in	O
such	O
a	O
geometry	O
at	O
one	O
point	O
gauss	B
was	O
asked	O
to	O
conduct	O
a	O
geodetic	O
survey	O
of	O
the	O
state	O
of	O
hanover	O
which	O
led	O
to	O
his	O
formulation	O
of	O
the	O
normal	O
distribution	O
now	O
also	O
known	O
as	O
the	O
gaussian	B
after	O
his	O
death	O
a	O
study	O
of	O
his	O
diaries	O
revealed	O
that	O
he	O
had	O
discovered	O
several	O
important	O
mathematical	O
results	O
years	O
or	O
even	O
decades	O
before	O
they	O
were	O
published	O
by	O
others	O
probability	B
distributions	O
functional	B
dependence	O
of	O
the	O
gaussian	B
on	O
x	O
is	O
through	O
the	O
quadratic	O
form	O
which	O
appears	O
in	O
the	O
exponent	O
the	O
quantity	O
is	O
called	O
the	O
mahalanobis	B
distance	I
from	O
to	O
x	O
and	O
reduces	O
to	O
the	O
euclidean	O
distance	O
when	O
is	O
the	O
identity	O
matrix	O
the	O
gaussian	B
distribution	O
will	O
be	O
constant	O
on	O
surfaces	O
in	O
x-space	O
for	O
which	O
this	O
quadratic	O
form	O
is	O
constant	O
first	O
of	O
all	O
we	O
note	O
that	O
the	O
matrix	O
can	O
be	O
taken	O
to	O
be	O
symmetric	O
without	O
loss	O
of	O
generality	O
because	O
any	O
antisymmetric	O
component	O
would	O
disappear	O
from	O
the	O
exponent	O
now	O
consider	O
the	O
eigenvector	O
equation	O
for	O
the	O
covariance	B
matrix	O
ui	O
iui	O
exercise	O
exercise	O
where	O
i	O
d	O
because	O
is	O
a	O
real	O
symmetric	O
matrix	O
its	O
eigenvalues	O
will	O
be	O
real	O
and	O
its	O
eigenvectors	O
can	O
be	O
chosen	O
to	O
form	O
an	O
orthonormal	O
set	O
so	O
that	O
where	O
iij	O
is	O
the	O
i	O
j	O
element	O
of	O
the	O
identity	O
matrix	O
and	O
satisfies	O
ut	O
i	O
uj	O
iij	O
iij	O
if	O
i	O
j	O
otherwise	O
exercise	O
the	O
covariance	B
matrix	O
can	O
be	O
expressed	O
as	O
an	O
expansion	O
in	O
terms	O
of	O
its	O
eigenvectors	O
in	O
the	O
form	O
iuiut	O
i	O
i	O
uiut	O
i	O
and	O
similarly	O
the	O
inverse	B
covariance	B
matrix	O
can	O
be	O
expressed	O
as	O
substituting	O
into	O
the	O
quadratic	O
form	O
becomes	O
i	O
i	O
where	O
we	O
have	O
defined	O
we	O
can	O
interpret	O
as	O
a	O
new	O
coordinate	O
system	O
defined	O
by	O
the	O
orthonormal	O
vectors	O
ui	O
that	O
are	O
shifted	O
and	O
rotated	O
with	O
respect	O
to	O
the	O
original	O
xi	O
coordinates	O
forming	O
the	O
vector	O
y	O
ydt	O
we	O
have	O
i	O
yi	O
ut	O
y	O
ux	O
the	O
gaussian	B
distribution	O
figure	O
the	O
red	O
curve	O
shows	O
the	O
elliptical	O
surface	O
of	O
constant	O
probability	B
density	B
for	O
a	O
gaussian	B
in	O
a	O
two-dimensional	O
space	O
x	O
on	O
which	O
the	O
density	B
is	O
exp	O
of	O
its	O
value	O
at	O
x	O
the	O
major	O
axes	O
of	O
the	O
ellipse	O
are	O
defined	O
by	O
the	O
eigenvectors	O
ui	O
of	O
the	O
covariance	B
matrix	O
with	O
corresponding	O
eigenvalues	O
i	O
appendix	O
c	O
where	O
u	O
is	O
a	O
matrix	O
whose	O
rows	O
are	O
given	O
by	O
ut	O
i	O
from	O
it	O
follows	O
that	O
u	O
is	O
an	O
orthogonal	O
matrix	O
i	O
e	O
it	O
satisfies	O
uut	O
i	O
and	O
hence	O
also	O
utu	O
i	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
the	O
quadratic	O
form	O
and	O
hence	O
the	O
gaussian	B
density	B
will	O
be	O
constant	O
on	O
surfaces	O
if	O
all	O
of	O
the	O
eigenvalues	O
i	O
are	O
positive	O
then	O
these	O
for	O
which	O
is	O
constant	O
surfaces	O
represent	O
ellipsoids	O
with	O
their	O
centres	O
at	O
and	O
their	O
axes	O
oriented	O
along	O
ui	O
and	O
with	O
scaling	O
factors	O
in	O
the	O
directions	O
of	O
the	O
axes	O
given	O
by	O
as	O
illustrated	O
in	O
figure	O
i	O
for	O
the	O
gaussian	B
distribution	O
to	O
be	O
well	O
defined	O
it	O
is	O
necessary	O
for	O
all	O
of	O
the	O
eigenvalues	O
i	O
of	O
the	O
covariance	B
matrix	O
to	O
be	O
strictly	O
positive	O
otherwise	O
the	O
distribution	O
cannot	O
be	O
properly	O
normalized	O
a	O
matrix	O
whose	O
eigenvalues	O
are	O
strictly	O
positive	O
is	O
said	O
to	O
be	O
positive	B
definite	I
in	O
chapter	O
we	O
will	O
encounter	O
gaussian	B
distributions	O
for	O
which	O
one	O
or	O
more	O
of	O
the	O
eigenvalues	O
are	O
zero	O
in	O
which	O
case	O
the	O
distribution	O
is	O
singular	O
and	O
is	O
confined	O
to	O
a	O
subspace	O
of	O
lower	O
dimensionality	O
if	O
all	O
of	O
the	O
eigenvalues	O
are	O
nonnegative	O
then	O
the	O
covariance	B
matrix	O
is	O
said	O
to	O
be	O
positive	O
semidefinite	O
now	O
consider	O
the	O
form	O
of	O
the	O
gaussian	B
distribution	O
in	O
the	O
new	O
coordinate	O
system	O
defined	O
by	O
the	O
yi	O
in	O
going	O
from	O
the	O
x	O
to	O
the	O
y	O
coordinate	O
system	O
we	O
have	O
a	O
jacobian	B
matrix	I
j	O
with	O
elements	O
given	O
by	O
jij	O
xi	O
yj	O
uji	O
where	O
uji	O
are	O
the	O
elements	O
of	O
the	O
matrix	O
ut	O
using	O
the	O
orthonormality	O
property	O
of	O
the	O
matrix	O
u	O
we	O
see	O
that	O
the	O
square	O
of	O
the	O
determinant	O
of	O
the	O
jacobian	B
matrix	I
is	O
and	O
hence	O
also	O
the	O
determinant	O
of	O
the	O
covariance	B
matrix	O
can	O
be	O
written	O
probability	B
distributions	O
as	O
the	O
product	O
of	O
its	O
eigenvalues	O
and	O
hence	O
j	O
thus	O
in	O
the	O
yj	O
coordinate	O
system	O
the	O
gaussian	B
distribution	O
takes	O
the	O
form	O
py	O
pxj	O
exp	O
j	O
j	O
ex	O
which	O
is	O
the	O
product	O
of	O
d	O
independent	B
univariate	O
gaussian	B
distributions	O
the	O
eigenvectors	O
therefore	O
define	O
a	O
new	O
set	O
of	O
shifted	O
and	O
rotated	O
coordinates	O
with	O
respect	O
to	O
which	O
the	O
joint	O
probability	B
distribution	O
factorizes	O
into	O
a	O
product	O
of	O
independent	B
distributions	O
the	O
integral	O
of	O
the	O
distribution	O
in	O
the	O
y	O
coordinate	O
system	O
is	O
then	O
py	O
dy	O
exp	O
j	O
j	O
dyj	O
where	O
we	O
have	O
used	O
the	O
result	O
for	O
the	O
normalization	O
of	O
the	O
univariate	O
gaussian	B
this	O
confirms	O
that	O
the	O
multivariate	O
gaussian	B
is	O
indeed	O
normalized	O
we	O
now	O
look	O
at	O
the	O
moments	O
of	O
the	O
gaussian	B
distribution	O
and	O
thereby	O
provide	O
an	O
interpretation	O
of	O
the	O
parameters	O
and	O
the	O
expectation	B
of	O
x	O
under	O
the	O
gaussian	B
distribution	O
is	O
given	O
by	O
exp	O
x	O
dx	O
exp	O
where	O
we	O
have	O
changed	O
variables	O
using	O
z	O
x	O
we	O
now	O
note	O
that	O
the	O
exponent	O
is	O
an	O
even	O
function	O
of	O
the	O
components	O
of	O
z	O
and	O
because	O
the	O
integrals	O
over	O
these	O
are	O
taken	O
over	O
the	O
range	O
the	O
term	O
in	O
z	O
in	O
the	O
factor	O
will	O
vanish	O
by	O
symmetry	O
thus	O
dz	O
zt	O
ex	O
and	O
so	O
we	O
refer	O
to	O
as	O
the	O
mean	B
of	O
the	O
gaussian	B
distribution	O
we	O
now	O
consider	O
second	B
order	I
moments	O
of	O
the	O
gaussian	B
in	O
the	O
univariate	O
case	O
we	O
considered	O
the	O
second	B
order	I
moment	O
given	O
by	O
for	O
the	O
multivariate	O
gaussian	B
there	O
are	O
second	B
order	I
moments	O
given	O
by	O
exixj	O
which	O
we	O
can	O
group	O
together	O
to	O
form	O
the	O
matrix	O
exxt	O
this	O
matrix	O
can	O
be	O
written	O
as	O
exxt	O
exp	O
exp	O
zt	O
xxt	O
dx	O
dz	O
the	O
gaussian	B
distribution	O
where	O
again	O
we	O
have	O
changed	O
variables	O
using	O
z	O
x	O
note	O
that	O
the	O
cross-terms	O
involving	O
zt	O
and	O
tz	O
will	O
again	O
vanish	O
by	O
symmetry	O
the	O
term	O
t	O
is	O
constant	O
and	O
can	O
be	O
taken	O
outside	O
the	O
integral	O
which	O
itself	O
is	O
unity	O
because	O
the	O
gaussian	B
distribution	O
is	O
normalized	O
consider	O
the	O
term	O
involving	O
zzt	O
again	O
we	O
can	O
make	O
use	O
of	O
the	O
eigenvector	O
expansion	O
of	O
the	O
covariance	B
matrix	O
given	O
by	O
together	O
with	O
the	O
completeness	O
of	O
the	O
set	O
of	O
eigenvectors	O
to	O
write	O
z	O
yjuj	O
exp	O
zt	O
zzt	O
dz	O
uiut	O
j	O
exp	O
k	O
k	O
yiyj	O
dy	O
where	O
yj	O
ut	O
j	O
z	O
which	O
gives	O
uiut	O
i	O
i	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
eigenvector	O
equation	O
together	O
with	O
the	O
fact	O
that	O
the	O
integral	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
middle	O
line	O
vanishes	O
by	O
symmetry	O
unless	O
i	O
j	O
and	O
in	O
the	O
final	O
line	O
we	O
have	O
made	O
use	O
of	O
the	O
results	O
and	O
together	O
with	O
thus	O
we	O
have	O
exxt	O
t	O
for	O
single	O
random	O
variables	O
we	O
subtracted	O
the	O
mean	B
before	O
taking	O
second	O
moments	O
in	O
order	O
to	O
define	O
a	O
variance	B
similarly	O
in	O
the	O
multivariate	O
case	O
it	O
is	O
again	O
convenient	O
to	O
subtract	O
off	O
the	O
mean	B
giving	O
rise	O
to	O
the	O
covariance	B
of	O
a	O
random	O
vector	O
x	O
defined	O
by	O
covx	O
e	O
for	O
the	O
specific	O
case	O
of	O
a	O
gaussian	B
distribution	O
we	O
can	O
make	O
use	O
of	O
ex	O
together	O
with	O
the	O
result	O
to	O
give	O
exx	O
ext	O
covx	O
because	O
the	O
parameter	O
matrix	O
governs	O
the	O
covariance	B
of	O
x	O
under	O
the	O
gaussian	B
distribution	O
it	O
is	O
called	O
the	O
covariance	B
matrix	O
although	O
the	O
gaussian	B
distribution	O
is	O
widely	O
used	O
as	O
a	O
density	B
model	O
it	O
suffers	O
from	O
some	O
significant	O
limitations	O
consider	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
distribution	O
a	O
general	O
symmetric	O
covariance	B
matrix	O
will	O
have	O
dd	O
independent	B
parameters	O
and	O
there	O
are	O
another	O
d	O
independent	B
parameters	O
in	O
giving	O
dd	O
parameters	O
in	O
total	O
for	O
large	O
d	O
the	O
total	O
number	O
of	O
parameters	O
exercise	O
probability	B
distributions	O
figure	O
contours	O
of	O
constant	O
probability	B
density	B
for	O
a	O
gaussian	B
distribution	O
in	O
two	O
dimensions	O
in	O
which	O
the	O
covariance	B
matrix	O
is	O
of	O
general	O
form	O
diagonal	B
in	O
which	O
the	O
elliptical	O
contours	O
are	O
aligned	O
with	O
the	O
coordinate	O
axes	O
and	O
proportional	O
to	O
the	O
identity	O
matrix	O
in	O
which	O
the	O
contours	O
are	O
concentric	O
circles	O
therefore	O
grows	O
quadratically	O
with	O
d	O
and	O
the	O
computational	O
task	O
of	O
manipulating	O
and	O
inverting	O
large	O
matrices	O
can	O
become	O
prohibitive	O
one	O
way	O
to	O
address	O
this	O
problem	O
is	O
to	O
use	O
restricted	O
forms	O
of	O
the	O
covariance	B
matrix	O
if	O
we	O
consider	O
covariance	B
matrices	O
that	O
are	O
diagonal	B
so	O
that	O
diag	O
i	O
we	O
then	O
have	O
a	O
total	O
of	O
independent	B
parameters	O
in	O
the	O
density	B
model	O
the	O
corresponding	O
contours	O
of	O
constant	O
density	B
are	O
given	O
by	O
axis-aligned	O
ellipsoids	O
we	O
could	O
further	O
restrict	O
the	O
covariance	B
matrix	O
to	O
be	O
proportional	O
to	O
the	O
identity	O
matrix	O
known	O
as	O
an	O
isotropic	B
covariance	B
giving	O
d	O
independent	B
parameters	O
in	O
the	O
model	O
and	O
spherical	O
surfaces	O
of	O
constant	O
density	B
the	O
three	O
possibilities	O
of	O
general	O
diagonal	B
and	O
isotropic	B
covariance	B
matrices	O
are	O
illustrated	O
in	O
figure	O
unfortunately	O
whereas	O
such	O
approaches	O
limit	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
distribution	O
and	O
make	O
inversion	O
of	O
the	O
covariance	B
matrix	O
a	O
much	O
faster	O
operation	O
they	O
also	O
greatly	O
restrict	O
the	O
form	O
of	O
the	O
probability	B
density	B
and	O
limit	O
its	O
ability	O
to	O
capture	O
interesting	O
correlations	O
in	O
the	O
data	O
a	O
further	O
limitation	O
of	O
the	O
gaussian	B
distribution	O
is	O
that	O
it	O
is	O
intrinsically	O
unimodal	O
has	O
a	O
single	O
maximum	O
and	O
so	O
is	O
unable	O
to	O
provide	O
a	O
good	O
approximation	O
to	O
multimodal	O
distributions	O
thus	O
the	O
gaussian	B
distribution	O
can	O
be	O
both	O
too	O
flexible	O
in	O
the	O
sense	O
of	O
having	O
too	O
many	O
parameters	O
while	O
also	O
being	O
too	O
limited	O
in	O
the	O
range	O
of	O
distributions	O
that	O
it	O
can	O
adequately	O
represent	O
we	O
will	O
see	O
later	O
that	O
the	O
introduction	O
of	O
latent	O
variables	O
also	O
called	O
hidden	O
variables	O
or	O
unobserved	O
variables	O
allows	O
both	O
of	O
these	O
problems	O
to	O
be	O
addressed	O
in	O
particular	O
a	O
rich	O
family	O
of	O
multimodal	O
distributions	O
is	O
obtained	O
by	O
introducing	O
discrete	O
latent	O
variables	O
leading	O
to	O
mixtures	O
of	O
gaussians	O
as	O
discussed	O
in	O
section	O
similarly	O
the	O
introduction	O
of	O
continuous	O
latent	O
variables	O
as	O
described	O
in	O
chapter	O
leads	O
to	O
models	O
in	O
which	O
the	O
number	O
of	O
free	O
parameters	O
can	O
be	O
controlled	O
independently	O
of	O
the	O
dimensionality	O
d	O
of	O
the	O
data	O
space	O
while	O
still	O
allowing	O
the	O
model	O
to	O
capture	O
the	O
dominant	O
correlations	O
in	O
the	O
data	O
set	O
indeed	O
these	O
two	O
approaches	O
can	O
be	O
combined	O
and	O
further	O
extended	B
to	O
derive	O
a	O
very	O
rich	O
set	O
of	O
hierarchical	B
models	O
that	O
can	O
be	O
adapted	O
to	O
a	O
broad	O
range	O
of	O
practical	O
applications	O
for	O
instance	O
the	O
gaussian	B
version	O
of	O
the	O
markov	B
random	I
field	I
which	O
is	O
widely	O
used	O
as	O
a	O
probabilistic	O
model	O
of	O
images	O
is	O
a	O
gaussian	B
distribution	O
over	O
the	O
joint	O
space	O
of	O
pixel	O
intensities	O
but	O
rendered	O
tractable	O
through	O
the	O
imposition	O
of	O
considerable	O
structure	O
reflecting	O
the	O
spatial	O
organization	O
of	O
the	O
pixels	O
similarly	O
the	O
linear	B
dynamical	B
system	I
used	O
to	O
model	O
time	O
series	O
data	O
for	O
applications	O
such	O
as	O
tracking	O
is	O
also	O
a	O
joint	O
gaussian	B
distribution	O
over	O
a	O
potentially	O
large	O
number	O
of	O
observed	O
and	O
latent	O
variables	O
and	O
again	O
is	O
tractable	O
due	O
to	O
the	O
structure	O
imposed	O
on	O
the	O
distribution	O
a	O
powerful	O
framework	O
for	O
expressing	O
the	O
form	O
and	O
properties	O
of	O
section	O
section	O
the	O
gaussian	B
distribution	O
such	O
complex	O
distributions	O
is	O
that	O
of	O
probabilistic	O
graphical	O
models	O
which	O
will	O
form	O
the	O
subject	O
of	O
chapter	O
conditional	B
gaussian	B
distributions	O
an	O
important	O
property	O
of	O
the	O
multivariate	O
gaussian	B
distribution	O
is	O
that	O
if	O
two	O
sets	O
of	O
variables	O
are	O
jointly	O
gaussian	B
then	O
the	O
conditional	B
distribution	O
of	O
one	O
set	O
conditioned	O
on	O
the	O
other	O
is	O
again	O
gaussian	B
similarly	O
the	O
marginal	B
distribution	O
of	O
either	O
set	O
is	O
also	O
gaussian	B
consider	O
first	O
the	O
case	O
of	O
conditional	B
distributions	O
suppose	O
x	O
is	O
a	O
d-dimensional	O
vector	O
with	O
gaussian	B
distribution	O
n	O
and	O
that	O
we	O
partition	O
x	O
into	O
two	O
disjoint	O
subsets	O
xa	O
and	O
xb	O
without	O
loss	O
of	O
generality	O
we	O
can	O
take	O
xa	O
to	O
form	O
the	O
first	O
m	O
components	O
of	O
x	O
with	O
xb	O
comprising	O
the	O
remaining	O
d	O
m	O
components	O
so	O
that	O
we	O
also	O
define	O
corresponding	O
partitions	O
of	O
the	O
mean	B
vector	O
given	O
by	O
and	O
of	O
the	O
covariance	B
matrix	O
given	O
by	O
note	O
that	O
the	O
symmetry	O
t	O
of	O
the	O
covariance	B
matrix	O
implies	O
that	O
aa	O
and	O
bb	O
are	O
symmetric	O
while	O
ba	O
t	O
ab	O
in	O
many	O
situations	O
it	O
will	O
be	O
convenient	O
to	O
work	O
with	O
the	O
inverse	B
of	O
the	O
covari	O
ance	O
matrix	O
which	O
is	O
known	O
as	O
the	O
precision	B
matrix	I
in	O
fact	O
we	O
shall	O
see	O
that	O
some	O
properties	O
of	O
gaussian	B
distributions	O
are	O
most	O
naturally	O
expressed	O
in	O
terms	O
of	O
the	O
covariance	B
whereas	O
others	O
take	O
a	O
simpler	O
form	O
when	O
viewed	O
in	O
terms	O
of	O
the	O
precision	O
we	O
therefore	O
also	O
introduce	O
the	O
partitioned	B
form	O
of	O
the	O
precision	B
matrix	I
aa	O
ab	O
ba	O
bb	O
exercise	O
corresponding	O
to	O
the	O
partitioning	O
of	O
the	O
vector	O
x	O
because	O
the	O
inverse	B
of	O
a	O
symmetric	O
matrix	O
is	O
also	O
symmetric	O
we	O
see	O
that	O
aa	O
and	O
bb	O
are	O
symmetric	O
while	O
t	O
ab	O
ba	O
it	O
should	O
be	O
stressed	O
at	O
this	O
point	O
that	O
for	O
instance	O
aa	O
is	O
not	O
simply	O
given	O
by	O
the	O
inverse	B
of	O
aa	O
in	O
fact	O
we	O
shall	O
shortly	O
examine	O
the	O
relation	O
between	O
the	O
inverse	B
of	O
a	O
partitioned	B
matrix	O
and	O
the	O
inverses	O
of	O
its	O
partitions	O
let	O
us	O
begin	O
by	O
finding	O
an	O
expression	O
for	O
the	O
conditional	B
distribution	O
pxaxb	O
from	O
the	O
product	B
rule	I
of	I
probability	B
we	O
see	O
that	O
this	O
conditional	B
distribution	O
can	O
be	O
x	O
xa	O
xb	O
a	O
b	O
aa	O
ab	O
ba	O
bb	O
probability	B
distributions	O
evaluated	O
from	O
the	O
joint	O
distribution	O
px	O
pxa	O
xb	O
simply	O
by	O
fixing	O
xb	O
to	O
the	O
observed	O
value	O
and	O
normalizing	O
the	O
resulting	O
expression	O
to	O
obtain	O
a	O
valid	O
probability	B
distribution	O
over	O
xa	O
instead	O
of	O
performing	O
this	O
normalization	O
explicitly	O
we	O
can	O
obtain	O
the	O
solution	O
more	O
efficiently	O
by	O
considering	O
the	O
quadratic	O
form	O
in	O
the	O
exponent	O
of	O
the	O
gaussian	B
distribution	O
given	O
by	O
and	O
then	O
reinstating	O
the	O
normalization	O
coefficient	O
at	O
the	O
end	O
of	O
the	O
calculation	O
if	O
we	O
make	O
use	O
of	O
the	O
partitioning	O
and	O
we	O
obtain	O
at	O
abxb	O
b	O
at	O
aaxa	O
a	O
bt	O
bbxb	O
b	O
bt	O
baxa	O
a	O
we	O
see	O
that	O
as	O
a	O
function	O
of	O
xa	O
this	O
is	O
again	O
a	O
quadratic	O
form	O
and	O
hence	O
the	O
corresponding	O
conditional	B
distribution	O
pxaxb	O
will	O
be	O
gaussian	B
because	O
this	O
distribution	O
is	O
completely	O
characterized	O
by	O
its	O
mean	B
and	O
its	O
covariance	B
our	O
goal	O
will	O
be	O
to	O
identify	O
expressions	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
pxaxb	O
by	O
inspection	O
of	O
this	O
is	O
an	O
example	O
of	O
a	O
rather	O
common	O
operation	O
associated	O
with	O
gaussian	B
distributions	O
sometimes	O
called	O
completing	B
the	I
square	I
in	O
which	O
we	O
are	O
given	O
a	O
quadratic	O
form	O
defining	O
the	O
exponent	O
terms	O
in	O
a	O
gaussian	B
distribution	O
and	O
we	O
need	O
to	O
determine	O
the	O
corresponding	O
mean	B
and	O
covariance	B
such	O
problems	O
can	O
be	O
solved	O
straightforwardly	O
by	O
noting	O
that	O
the	O
exponent	O
in	O
a	O
general	O
gaussian	B
distribution	O
n	O
can	O
be	O
written	O
xt	O
xt	O
const	O
and	O
the	O
coefficient	O
of	O
the	O
linear	O
term	O
in	O
x	O
to	O
where	O
const	O
denotes	O
terms	O
which	O
are	O
independent	B
of	O
x	O
and	O
we	O
have	O
made	O
use	O
of	O
the	O
symmetry	O
of	O
thus	O
if	O
we	O
take	O
our	O
general	O
quadratic	O
form	O
and	O
express	O
it	O
in	O
the	O
form	O
given	O
by	O
the	O
right-hand	O
side	O
of	O
then	O
we	O
can	O
immediately	O
equate	O
the	O
matrix	O
of	O
coefficients	O
entering	O
the	O
second	B
order	I
term	O
in	O
x	O
to	O
the	O
inverse	B
covariance	B
from	O
which	O
we	O
can	O
matrix	O
obtain	O
now	O
let	O
us	O
apply	O
this	O
procedure	O
to	O
the	O
conditional	B
gaussian	B
distribution	O
pxaxb	O
for	O
which	O
the	O
quadratic	O
form	O
in	O
the	O
exponent	O
is	O
given	O
by	O
we	O
will	O
denote	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
distribution	O
by	O
ab	O
and	O
ab	O
respectively	O
consider	O
the	O
functional	B
dependence	O
of	O
on	O
xa	O
in	O
which	O
xb	O
is	O
regarded	O
as	O
a	O
constant	O
if	O
we	O
pick	O
out	O
all	O
terms	O
that	O
are	O
second	B
order	I
in	O
xa	O
we	O
have	O
xt	O
a	O
aaxa	O
from	O
which	O
we	O
can	O
immediately	O
conclude	O
that	O
the	O
covariance	B
precision	O
of	O
pxaxb	O
is	O
given	O
by	O
ab	O
aa	O
the	O
gaussian	B
distribution	O
now	O
consider	O
all	O
of	O
the	O
terms	O
in	O
that	O
are	O
linear	O
in	O
xa	O
a	O
aa	O
a	O
abxb	O
b	O
xt	O
where	O
we	O
have	O
used	O
t	O
the	O
coefficient	O
of	O
xa	O
in	O
this	O
expression	O
must	O
equal	O
ba	O
ab	O
from	O
our	O
discussion	O
of	O
the	O
general	O
form	O
ab	O
ab	O
and	O
hence	O
ab	O
ab	O
aa	O
a	O
abxb	O
b	O
a	O
aa	O
abxb	O
b	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
results	O
and	O
are	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
precision	B
matrix	I
of	O
the	O
original	O
joint	O
distribution	O
pxa	O
xb	O
we	O
can	O
also	O
express	O
these	O
results	O
in	O
terms	O
of	O
the	O
corresponding	O
partitioned	B
covariance	B
matrix	O
to	O
do	O
this	O
we	O
make	O
use	O
of	O
the	O
following	O
identity	O
for	O
the	O
inverse	B
of	O
a	O
partitioned	B
matrix	O
mbd	O
m	O
d	O
d	O
d	O
a	O
b	O
c	O
d	O
exercise	O
where	O
we	O
have	O
defined	O
the	O
quantity	O
m	O
is	O
known	O
as	O
the	O
schur	O
complement	O
of	O
the	O
matrix	O
on	O
the	O
left-hand	O
side	O
of	O
with	O
respect	O
to	O
the	O
submatrix	O
d	O
using	O
the	O
definition	O
m	O
bd	O
aa	O
ab	O
ba	O
bb	O
aa	O
ab	O
ba	O
bb	O
and	O
making	O
use	O
of	O
we	O
have	O
aa	O
aa	O
ab	O
bb	O
ba	O
ab	O
aa	O
ab	O
bb	O
ba	O
ab	O
bb	O
from	O
these	O
we	O
obtain	O
the	O
following	O
expressions	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
conditional	B
distribution	O
pxaxb	O
bb	O
b	O
ab	O
a	O
ab	O
ab	O
aa	O
ab	O
bb	O
ba	O
comparing	O
and	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
pxaxb	O
takes	O
a	O
simpler	O
form	O
when	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
precision	B
matrix	I
than	O
when	O
it	O
is	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
covariance	B
matrix	O
note	O
that	O
the	O
mean	B
of	O
the	O
conditional	B
distribution	O
pxaxb	O
given	O
by	O
is	O
a	O
linear	O
function	O
of	O
xb	O
and	O
that	O
the	O
covariance	B
given	O
by	O
is	O
independent	B
of	O
xa	O
this	O
represents	O
an	O
example	O
of	O
a	O
linear-gaussian	B
model	I
section	O
probability	B
distributions	O
marginal	B
gaussian	B
distributions	O
we	O
have	O
seen	O
that	O
if	O
a	O
joint	O
distribution	O
pxa	O
xb	O
is	O
gaussian	B
then	O
the	O
conditional	B
distribution	O
pxaxb	O
will	O
again	O
be	O
gaussian	B
now	O
we	O
turn	O
to	O
a	O
discussion	O
of	O
the	O
marginal	B
distribution	O
given	O
by	O
pxa	O
pxa	O
xb	O
dxb	O
which	O
as	O
we	O
shall	O
see	O
is	O
also	O
gaussian	B
once	O
again	O
our	O
strategy	O
for	O
evaluating	O
this	O
distribution	O
efficiently	O
will	O
be	O
to	O
focus	O
on	O
the	O
quadratic	O
form	O
in	O
the	O
exponent	O
of	O
the	O
joint	O
distribution	O
and	O
thereby	O
to	O
identify	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
marginal	B
distribution	O
pxa	O
the	O
quadratic	O
form	O
for	O
the	O
joint	O
distribution	O
can	O
be	O
expressed	O
using	O
the	O
partitioned	B
precision	B
matrix	I
in	O
the	O
form	O
because	O
our	O
goal	O
is	O
to	O
integrate	O
out	O
xb	O
this	O
is	O
most	O
easily	O
achieved	O
by	O
first	O
considering	O
the	O
terms	O
involving	O
xb	O
and	O
then	O
completing	B
the	I
square	I
in	O
order	O
to	O
facilitate	O
integration	O
picking	O
out	O
just	O
those	O
terms	O
that	O
involve	O
xb	O
we	O
have	O
bb	O
mt	O
bbxb	O
b	O
m	O
bb	O
m	O
b	O
bbxbxt	O
xt	O
bb	O
m	O
mt	O
where	O
we	O
have	O
defined	O
m	O
bb	O
b	O
baxa	O
a	O
we	O
see	O
that	O
the	O
dependence	O
on	O
xb	O
has	O
been	O
cast	O
into	O
the	O
standard	O
quadratic	O
form	O
of	O
a	O
gaussian	B
distribution	O
corresponding	O
to	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
plus	O
a	O
term	O
that	O
does	O
not	O
depend	O
on	O
xb	O
that	O
does	O
depend	O
on	O
xa	O
thus	O
when	O
we	O
take	O
the	O
exponential	O
of	O
this	O
quadratic	O
form	O
we	O
see	O
that	O
the	O
integration	O
over	O
xb	O
required	O
by	O
will	O
take	O
the	O
form	O
bb	O
mt	O
bbxb	O
bb	O
m	O
dxb	O
exp	O
this	O
integration	O
is	O
easily	O
performed	O
by	O
noting	O
that	O
it	O
is	O
the	O
integral	O
over	O
an	O
unnormalized	O
gaussian	B
and	O
so	O
the	O
result	O
will	O
be	O
the	O
reciprocal	O
of	O
the	O
normalization	O
coefficient	O
we	O
know	O
from	O
the	O
form	O
of	O
the	O
normalized	O
gaussian	B
given	O
by	O
that	O
this	O
coefficient	O
is	O
independent	B
of	O
the	O
mean	B
and	O
depends	O
only	O
on	O
the	O
determinant	O
of	O
the	O
covariance	B
matrix	O
thus	O
by	O
completing	B
the	I
square	I
with	O
respect	O
to	O
xb	O
we	O
can	O
integrate	O
out	O
xb	O
and	O
the	O
only	O
term	O
remaining	O
from	O
the	O
contributions	O
on	O
the	O
left-hand	O
side	O
of	O
that	O
depends	O
on	O
xa	O
is	O
the	O
last	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
in	O
which	O
m	O
is	O
given	O
by	O
combining	O
this	O
term	O
with	O
the	O
remaining	O
terms	O
from	O
the	O
gaussian	B
distribution	O
that	O
depend	O
on	O
xa	O
we	O
obtain	O
bb	O
b	O
baxa	O
at	O
bb	O
bb	O
b	O
baxa	O
a	O
a	O
aa	O
a	O
ab	O
b	O
const	O
xt	O
a	O
aaxa	O
xt	O
a	O
aa	O
ab	O
xt	O
bb	O
baxa	O
a	O
aa	O
ab	O
bb	O
ba	O
a	O
const	O
where	O
const	O
denotes	O
quantities	O
independent	B
of	O
xa	O
again	O
by	O
comparison	O
with	O
we	O
see	O
that	O
the	O
covariance	B
of	O
the	O
marginal	B
distribution	O
of	O
pxa	O
is	O
given	O
by	O
a	O
aa	O
ab	O
bb	O
ba	O
similarly	O
the	O
mean	B
is	O
given	O
by	O
a	O
aa	O
ab	O
bb	O
ba	O
a	O
a	O
where	O
we	O
have	O
used	O
the	O
covariance	B
in	O
is	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
precision	B
matrix	I
given	O
by	O
we	O
can	O
rewrite	O
this	O
in	O
terms	O
of	O
the	O
corresponding	O
partitioning	O
of	O
the	O
covariance	B
matrix	O
given	O
by	O
as	O
we	O
did	O
for	O
the	O
conditional	B
distribution	O
these	O
partitioned	B
matrices	O
are	O
related	O
by	O
aa	O
ab	O
ba	O
bb	O
aa	O
ab	O
ba	O
bb	O
aa	O
making	O
use	O
of	O
we	O
then	O
have	O
aa	O
ab	O
bb	O
ba	O
thus	O
we	O
obtain	O
the	O
intuitively	O
satisfying	O
result	O
that	O
the	O
marginal	B
distribution	O
pxa	O
has	O
mean	B
and	O
covariance	B
given	O
by	O
exa	O
a	O
covxa	O
aa	O
we	O
see	O
that	O
for	O
a	O
marginal	B
distribution	O
the	O
mean	B
and	O
covariance	B
are	O
most	O
simply	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
covariance	B
matrix	O
in	O
contrast	O
to	O
the	O
conditional	B
distribution	O
for	O
which	O
the	O
partitioned	B
precision	B
matrix	I
gives	O
rise	O
to	O
simpler	O
expressions	O
our	O
results	O
for	O
the	O
marginal	B
and	O
conditional	B
distributions	O
of	O
a	O
partitioned	B
gaus	O
sian	O
are	O
summarized	O
below	O
partitioned	B
gaussians	O
given	O
a	O
joint	O
gaussian	B
distribution	O
n	O
with	O
and	O
xa	O
xb	O
a	O
b	O
x	O
probability	B
distributions	O
xb	O
xb	O
pxaxb	O
pxa	O
xb	O
pxa	O
xa	O
xa	O
figure	O
the	O
plot	O
on	O
the	O
left	O
shows	O
the	O
contours	O
of	O
a	O
gaussian	B
distribution	O
pxa	O
xb	O
over	O
two	O
variables	O
and	O
the	O
plot	O
on	O
the	O
right	O
shows	O
the	O
marginal	B
distribution	O
pxa	O
curve	O
and	O
the	O
conditional	B
distribution	O
pxaxb	O
for	O
xb	O
curve	O
aa	O
ab	O
ba	O
bb	O
aa	O
ab	O
ba	O
bb	O
conditional	B
distribution	O
pxaxb	O
n	O
ab	O
aa	O
ab	O
a	O
aa	O
abxb	O
b	O
marginal	B
distribution	O
pxa	O
n	O
a	O
aa	O
we	O
illustrate	O
the	O
idea	O
of	O
conditional	B
and	O
marginal	B
distributions	O
associated	O
with	O
a	O
multivariate	O
gaussian	B
using	O
an	O
example	O
involving	O
two	O
variables	O
in	O
figure	O
bayes	B
theorem	O
for	O
gaussian	B
variables	O
in	O
sections	O
and	O
we	O
considered	O
a	O
gaussian	B
px	O
in	O
which	O
we	O
partitioned	B
the	O
vector	O
x	O
into	O
two	O
subvectors	O
x	O
xb	O
and	O
then	O
found	O
expressions	O
for	O
the	O
conditional	B
distribution	O
pxaxb	O
and	O
the	O
marginal	B
distribution	O
pxa	O
we	O
noted	O
that	O
the	O
mean	B
of	O
the	O
conditional	B
distribution	O
pxaxb	O
was	O
a	O
linear	O
function	O
of	O
xb	O
here	O
we	O
shall	O
suppose	O
that	O
we	O
are	O
given	O
a	O
gaussian	B
marginal	B
distribution	O
px	O
and	O
a	O
gaussian	B
conditional	B
distribution	O
pyx	O
in	O
which	O
pyx	O
has	O
a	O
mean	B
that	O
is	O
a	O
linear	O
function	O
of	O
x	O
and	O
a	O
covariance	B
which	O
is	O
independent	B
of	O
x	O
this	O
is	O
an	O
example	O
of	O
the	O
gaussian	B
distribution	O
a	O
linear	O
gaussian	B
model	O
and	O
ghahramani	O
which	O
we	O
shall	O
study	O
in	O
greater	O
generality	O
in	O
section	O
we	O
wish	O
to	O
find	O
the	O
marginal	B
distribution	O
py	O
and	O
the	O
conditional	B
distribution	O
pxy	O
this	O
is	O
a	O
problem	O
that	O
will	O
arise	O
frequently	O
in	O
subsequent	O
chapters	O
and	O
it	O
will	O
prove	O
convenient	O
to	O
derive	O
the	O
general	O
results	O
here	O
we	O
shall	O
take	O
the	O
marginal	B
and	O
conditional	B
distributions	O
to	O
be	O
px	O
pyx	O
x	O
yax	O
b	O
l	O
where	O
a	O
and	O
b	O
are	O
parameters	O
governing	O
the	O
means	O
and	O
and	O
l	O
are	O
precision	O
matrices	O
if	O
x	O
has	O
dimensionality	O
m	O
and	O
y	O
has	O
dimensionality	O
d	O
then	O
the	O
matrix	O
a	O
has	O
size	O
d	O
m	O
first	O
we	O
find	O
an	O
expression	O
for	O
the	O
joint	O
distribution	O
over	O
x	O
and	O
y	O
to	O
do	O
this	O
we	O
define	O
z	O
x	O
y	O
and	O
then	O
consider	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
ln	O
pz	O
ln	O
px	O
ln	O
pyx	O
ax	O
btly	O
ax	O
b	O
const	O
where	O
const	O
denotes	O
terms	O
independent	B
of	O
x	O
and	O
y	O
as	O
before	O
we	O
see	O
that	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
the	O
components	O
of	O
z	O
and	O
hence	O
pz	O
is	O
gaussian	B
distribution	O
to	O
find	O
the	O
precision	O
of	O
this	O
gaussian	B
we	O
consider	O
the	O
second	B
order	I
terms	O
in	O
which	O
can	O
be	O
written	O
as	O
xt	O
atlax	O
la	O
x	O
y	O
ytly	O
ytlax	O
atla	O
atl	O
l	O
xtatly	O
ztrz	O
and	O
so	O
the	O
gaussian	B
distribution	O
over	O
z	O
has	O
precision	O
covariance	B
matrix	O
given	O
by	O
r	O
atla	O
atl	O
la	O
l	O
x	O
y	O
exercise	O
the	O
covariance	B
matrix	O
is	O
found	O
by	O
taking	O
the	O
inverse	B
of	O
the	O
precision	O
which	O
can	O
be	O
done	O
using	O
the	O
matrix	O
inversion	O
formula	O
to	O
give	O
covz	O
r	O
l	O
a	O
a	O
probability	B
distributions	O
similarly	O
we	O
can	O
find	O
the	O
mean	B
of	O
the	O
gaussian	B
distribution	O
over	O
z	O
by	O
identify	O
ing	O
the	O
linear	O
terms	O
in	O
which	O
are	O
given	O
by	O
xt	O
xtatlb	O
ytlb	O
atlb	O
lb	O
x	O
y	O
atlb	O
lb	O
using	O
our	O
earlier	O
result	O
obtained	O
by	O
completing	B
the	I
square	I
over	O
the	O
quadratic	O
form	O
of	O
a	O
multivariate	O
gaussian	B
we	O
find	O
that	O
the	O
mean	B
of	O
z	O
is	O
given	O
by	O
ez	O
r	O
exercise	O
making	O
use	O
of	O
we	O
then	O
obtain	O
ez	O
a	O
b	O
section	O
section	O
next	O
we	O
find	O
an	O
expression	O
for	O
the	O
marginal	B
distribution	O
py	O
in	O
which	O
we	O
have	O
marginalized	O
over	O
x	O
recall	O
that	O
the	O
marginal	B
distribution	O
over	O
a	O
subset	O
of	O
the	O
components	O
of	O
a	O
gaussian	B
random	O
vector	O
takes	O
a	O
particularly	O
simple	O
form	O
when	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
covariance	B
matrix	O
specifically	O
its	O
mean	B
and	O
covariance	B
are	O
given	O
by	O
and	O
respectively	O
making	O
use	O
of	O
and	O
we	O
see	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
marginal	B
distribution	O
py	O
are	O
given	O
by	O
ey	O
a	O
b	O
covy	O
l	O
a	O
a	O
special	O
case	O
of	O
this	O
result	O
is	O
when	O
a	O
i	O
in	O
which	O
case	O
it	O
reduces	O
to	O
the	O
convolution	O
of	O
two	O
gaussians	O
for	O
which	O
we	O
see	O
that	O
the	O
mean	B
of	O
the	O
convolution	O
is	O
the	O
sum	O
of	O
the	O
mean	B
of	O
the	O
two	O
gaussians	O
and	O
the	O
covariance	B
of	O
the	O
convolution	O
is	O
the	O
sum	O
of	O
their	O
covariances	O
finally	O
we	O
seek	O
an	O
expression	O
for	O
the	O
conditional	B
pxy	O
recall	O
that	O
the	O
results	O
for	O
the	O
conditional	B
distribution	O
are	O
most	O
easily	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
precision	B
matrix	I
using	O
and	O
applying	O
these	O
results	O
to	O
and	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
pxy	O
has	O
mean	B
and	O
covariance	B
given	O
by	O
exy	O
atla	O
covxy	O
atla	O
atly	O
b	O
the	O
evaluation	O
of	O
this	O
conditional	B
can	O
be	O
seen	O
as	O
an	O
example	O
of	O
bayes	B
theorem	O
we	O
can	O
interpret	O
the	O
distribution	O
px	O
as	O
a	O
prior	B
distribution	O
over	O
x	O
if	O
the	O
variable	O
y	O
is	O
observed	O
then	O
the	O
conditional	B
distribution	O
pxy	O
represents	O
the	O
corresponding	O
posterior	O
distribution	O
over	O
x	O
having	O
found	O
the	O
marginal	B
and	O
conditional	B
distributions	O
we	O
effectively	O
expressed	O
the	O
joint	O
distribution	O
pz	O
pxpyx	O
in	O
the	O
form	O
pxypy	O
these	O
results	O
are	O
summarized	O
below	O
the	O
gaussian	B
distribution	O
marginal	B
and	O
conditional	B
gaussians	O
given	O
a	O
marginal	B
gaussian	B
distribution	O
for	O
x	O
and	O
a	O
conditional	B
gaussian	B
distribution	O
for	O
y	O
given	O
x	O
in	O
the	O
form	O
px	O
n	O
pyx	O
n	O
b	O
l	O
the	O
marginal	B
distribution	O
of	O
y	O
and	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
y	O
are	O
given	O
by	O
py	O
n	O
b	O
l	O
a	O
pxy	O
n	O
b	O
where	O
atla	O
maximum	B
likelihood	I
for	O
the	O
gaussian	B
given	O
a	O
data	O
set	O
x	O
xn	O
in	O
which	O
the	O
observations	O
are	O
assumed	O
to	O
be	O
drawn	O
independently	O
from	O
a	O
multivariate	O
gaussian	B
distribution	O
we	O
can	O
estimate	O
the	O
parameters	O
of	O
the	O
distribution	O
by	O
maximum	B
likelihood	I
the	O
log	O
likelihood	B
function	I
is	O
given	O
by	O
ln	O
px	O
n	O
d	O
n	O
ln	O
by	O
simple	O
rearrangement	O
we	O
see	O
that	O
the	O
likelihood	B
function	I
depends	O
on	O
the	O
data	O
set	O
only	O
through	O
the	O
two	O
quantities	O
xn	O
xnxt	O
n	O
appendix	O
c	O
these	O
are	O
known	O
as	O
the	O
sufficient	B
statistics	I
for	O
the	O
gaussian	B
distribution	O
using	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	O
with	O
respect	O
to	O
is	O
given	O
by	O
ln	O
px	O
and	O
setting	O
this	O
derivative	B
to	O
zero	O
we	O
obtain	O
the	O
solution	O
for	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
the	O
mean	B
given	O
by	O
ml	O
n	O
xn	O
probability	B
distributions	O
exercise	O
which	O
is	O
the	O
mean	B
of	O
the	O
observed	O
set	O
of	O
data	O
points	O
the	O
maximization	O
of	O
with	O
respect	O
to	O
is	O
rather	O
more	O
involved	O
the	O
simplest	O
approach	O
is	O
to	O
ignore	O
the	O
symmetry	O
constraint	O
and	O
show	O
that	O
the	O
resulting	O
solution	O
is	O
symmetric	O
as	O
required	O
alternative	O
derivations	O
of	O
this	O
result	O
which	O
impose	O
the	O
symmetry	O
and	O
positive	O
definiteness	O
constraints	O
explicitly	O
can	O
be	O
found	O
in	O
magnus	O
and	O
neudecker	O
the	O
result	O
is	O
as	O
expected	O
and	O
takes	O
the	O
form	O
ml	O
n	O
mlxn	O
mlt	O
which	O
involves	O
ml	O
because	O
this	O
is	O
the	O
result	O
of	O
a	O
joint	O
maximization	O
with	O
respect	O
to	O
and	O
note	O
that	O
the	O
solution	O
for	O
ml	O
does	O
not	O
depend	O
on	O
ml	O
and	O
so	O
we	O
can	O
first	O
evaluate	O
ml	O
and	O
then	O
use	O
this	O
to	O
evaluate	O
ml	O
if	O
we	O
evaluate	O
the	O
expectations	O
of	O
the	O
maximum	B
likelihood	I
solutions	O
under	O
the	O
exercise	O
true	O
distribution	O
we	O
obtain	O
the	O
following	O
results	O
e	O
ml	O
e	O
ml	O
n	O
this	O
bias	B
by	O
defining	O
a	O
different	O
given	O
by	O
we	O
see	O
that	O
the	O
expectation	B
of	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
the	O
mean	B
is	O
equal	O
to	O
the	O
true	O
mean	B
however	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
the	O
covariance	B
has	O
an	O
expectation	B
that	O
is	O
less	O
than	O
the	O
true	O
value	O
and	O
hence	O
it	O
is	O
biased	O
we	O
can	O
correct	O
n	O
clearly	O
from	O
and	O
the	O
expectation	B
is	O
equal	O
to	O
mlxn	O
mlt	O
n	O
sequential	B
estimation	I
our	O
discussion	O
of	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
parameters	O
of	O
a	O
gaussian	B
distribution	O
provides	O
a	O
convenient	O
opportunity	O
to	O
give	O
a	O
more	O
general	O
discussion	O
of	O
the	O
topic	O
of	O
sequential	B
estimation	I
for	O
maximum	B
likelihood	I
sequential	O
methods	O
allow	O
data	O
points	O
to	O
be	O
processed	O
one	O
at	O
a	O
time	O
and	O
then	O
discarded	O
and	O
are	O
important	O
for	O
on-line	O
applications	O
and	O
also	O
where	O
large	O
data	O
sets	O
are	O
involved	O
so	O
that	O
batch	O
processing	O
of	O
all	O
data	O
points	O
at	O
once	O
is	O
infeasible	O
consider	O
the	O
result	O
for	O
the	O
maximum	B
likelihood	I
estimator	O
of	O
the	O
mean	B
ml	O
when	O
it	O
is	O
based	O
on	O
n	O
observations	O
if	O
we	O
ml	O
which	O
we	O
will	O
denote	O
by	O
the	O
gaussian	B
distribution	O
figure	O
a	O
schematic	O
illustration	O
of	O
two	O
correlated	O
random	O
variables	O
z	O
and	O
together	O
with	O
the	O
regression	B
function	I
f	O
given	O
by	O
the	O
conditional	B
expectation	B
ez	O
the	O
robbinsmonro	O
algorithm	O
provides	O
a	O
general	O
sequential	O
procedure	O
for	O
finding	O
the	O
root	O
of	O
such	O
functions	O
z	O
f	O
dissect	O
out	O
the	O
contribution	O
from	O
the	O
final	O
data	O
point	O
xn	O
we	O
obtain	O
ml	O
n	O
xn	O
n	O
xn	O
n	O
n	O
xn	O
n	O
n	O
ml	O
xn	O
ml	O
n	O
n	O
this	O
result	O
has	O
a	O
nice	O
interpretation	O
as	O
follows	O
after	O
observing	O
n	O
data	O
points	O
we	O
have	O
estimated	O
by	O
we	O
now	O
observe	O
data	O
point	O
xn	O
and	O
we	O
obtain	O
our	O
revised	O
estimate	O
ml	O
by	O
moving	O
the	O
old	O
estimate	O
a	O
small	O
amount	O
proportional	O
to	O
in	O
the	O
direction	O
of	O
the	O
error	B
signal	O
note	O
that	O
as	O
n	O
increases	O
so	O
the	O
contribution	O
from	O
successive	O
data	O
points	O
gets	O
smaller	O
ml	O
ml	O
ml	O
the	O
result	O
will	O
clearly	O
give	O
the	O
same	O
answer	O
as	O
the	O
batch	O
result	O
because	O
the	O
two	O
formulae	O
are	O
equivalent	O
however	O
we	O
will	O
not	O
always	O
be	O
able	O
to	O
derive	O
a	O
sequential	O
algorithm	O
by	O
this	O
route	O
and	O
so	O
we	O
seek	O
a	O
more	O
general	O
formulation	O
of	O
sequential	B
learning	B
which	O
leads	O
us	O
to	O
the	O
robbins-monro	B
algorithm	I
consider	O
a	O
pair	O
of	O
random	O
variables	O
and	O
z	O
governed	O
by	O
a	O
joint	O
distribution	O
pz	O
the	O
conditional	B
expectation	B
of	O
z	O
given	O
defines	O
a	O
deterministic	O
function	O
f	O
that	O
is	O
given	O
by	O
f	O
ez	O
zpz	O
dz	O
and	O
is	O
illustrated	O
schematically	O
in	O
figure	O
functions	O
defined	O
in	O
this	O
way	O
are	O
called	O
regression	B
functions	O
our	O
goal	O
is	O
to	O
find	O
the	O
root	O
at	O
which	O
f	O
if	O
we	O
had	O
a	O
large	O
data	O
set	O
of	O
observations	O
of	O
z	O
and	O
then	O
we	O
could	O
model	O
the	O
regression	B
function	I
directly	O
and	O
then	O
obtain	O
an	O
estimate	O
of	O
its	O
root	O
suppose	O
however	O
that	O
we	O
observe	O
values	O
of	O
z	O
one	O
at	O
a	O
time	O
and	O
we	O
wish	O
to	O
find	O
a	O
corresponding	O
sequential	B
estimation	I
scheme	O
for	O
the	O
following	O
general	O
procedure	O
for	O
solving	O
such	O
problems	O
was	O
given	O
by	O
probability	B
distributions	O
robbins	O
and	O
monro	O
we	O
shall	O
assume	O
that	O
the	O
conditional	B
variance	B
of	O
z	O
is	O
finite	O
so	O
that	O
and	O
we	O
shall	O
also	O
without	O
loss	O
of	O
generality	O
consider	O
the	O
case	O
where	O
f	O
for	O
and	O
f	O
for	O
as	O
is	O
the	O
case	O
in	O
figure	O
the	O
robbins-monro	O
procedure	O
then	O
defines	O
a	O
sequence	O
of	O
successive	O
estimates	O
of	O
the	O
root	O
given	O
by	O
e	O
an	O
where	O
z	O
is	O
an	O
observed	O
value	O
of	O
z	O
when	O
takes	O
the	O
value	O
the	O
coefficients	O
represent	O
a	O
sequence	O
of	O
positive	O
numbers	O
that	O
satisfy	O
the	O
conditions	O
n	O
an	O
lim	O
an	O
n	O
n	O
n	O
it	O
can	O
then	O
be	O
shown	O
and	O
monro	O
fukunaga	O
that	O
the	O
sequence	O
of	O
estimates	O
given	O
by	O
does	O
indeed	O
converge	O
to	O
the	O
root	O
with	O
probability	B
one	O
note	O
that	O
the	O
first	O
condition	O
ensures	O
that	O
the	O
successive	O
corrections	O
decrease	O
in	O
magnitude	O
so	O
that	O
the	O
process	O
can	O
converge	O
to	O
a	O
limiting	O
value	O
the	O
second	O
condition	O
is	O
required	O
to	O
ensure	O
that	O
the	O
algorithm	O
does	O
not	O
converge	O
short	O
of	O
the	O
root	O
and	O
the	O
third	O
condition	O
is	O
needed	O
to	O
ensure	O
that	O
the	O
accumulated	O
noise	O
has	O
finite	O
variance	B
and	O
hence	O
does	O
not	O
spoil	O
convergence	O
now	O
let	O
us	O
consider	O
how	O
a	O
general	O
maximum	B
likelihood	I
problem	O
can	O
be	O
solved	O
sequentially	O
using	O
the	O
robbins-monro	B
algorithm	I
by	O
definition	O
the	O
maximum	B
likelihood	I
solution	O
ml	O
is	O
a	O
stationary	B
point	O
of	O
the	O
log	O
likelihood	B
function	I
and	O
hence	O
satisfies	O
exchanging	O
the	O
derivative	B
and	O
the	O
summation	O
and	O
taking	O
the	O
limit	O
n	O
we	O
have	O
ml	O
n	O
ln	O
pxn	O
lim	O
n	O
n	O
ln	O
pxn	O
ex	O
ln	O
px	O
and	O
so	O
we	O
see	O
that	O
finding	O
the	O
maximum	B
likelihood	I
solution	O
corresponds	O
to	O
finding	O
the	O
root	O
of	O
a	O
regression	B
function	I
we	O
can	O
therefore	O
apply	O
the	O
robbins-monro	O
procedure	O
which	O
now	O
takes	O
the	O
form	O
an	O
ln	O
pxn	O
the	O
gaussian	B
distribution	O
z	O
figure	O
in	O
the	O
case	O
of	O
a	O
gaussian	B
distribution	O
with	O
corresponding	O
to	O
the	O
mean	B
the	O
regression	B
function	I
illustrated	O
in	O
figure	O
takes	O
the	O
form	O
of	O
a	O
straight	O
line	O
as	O
shown	O
in	O
red	O
in	O
this	O
case	O
the	O
random	O
variable	O
z	O
corresponds	O
to	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	B
function	I
and	O
is	O
given	O
by	O
ml	O
and	O
its	O
expectation	B
that	O
defines	O
the	O
regression	B
function	I
is	O
a	O
straight	O
line	O
given	O
by	O
ml	O
the	O
root	O
of	O
the	O
regression	B
function	I
corresponds	O
to	O
the	O
maximum	B
likelihood	I
estimator	O
ml	O
ml	O
pz	O
as	O
a	O
specific	O
example	O
we	O
consider	O
once	O
again	O
the	O
sequential	B
estimation	I
of	O
the	O
mean	B
of	O
a	O
gaussian	B
distribution	O
in	O
which	O
case	O
the	O
parameter	O
is	O
the	O
estimate	O
ml	O
of	O
the	O
mean	B
of	O
the	O
gaussian	B
and	O
the	O
random	O
variable	O
z	O
is	O
given	O
by	O
ml	O
z	O
ln	O
px	O
ml	O
ml	O
thus	O
the	O
distribution	O
of	O
z	O
is	O
gaussian	B
with	O
mean	B
ml	O
as	O
illustrated	O
in	O
figure	O
substituting	O
into	O
we	O
obtain	O
the	O
univariate	O
form	O
of	O
provided	O
we	O
choose	O
the	O
coefficients	O
an	O
to	O
have	O
the	O
form	O
an	O
note	O
that	O
although	O
we	O
have	O
focussed	O
on	O
the	O
case	O
of	O
a	O
single	O
variable	O
the	O
same	O
technique	O
together	O
with	O
the	O
same	O
restrictions	O
on	O
the	O
coefficients	O
an	O
apply	O
equally	O
to	O
the	O
multivariate	O
case	O
bayesian	B
inference	B
for	O
the	O
gaussian	B
the	O
maximum	B
likelihood	I
framework	O
gave	O
point	O
estimates	O
for	O
the	O
parameters	O
and	O
now	O
we	O
develop	O
a	O
bayesian	B
treatment	O
by	O
introducing	O
prior	B
distributions	O
over	O
these	O
parameters	O
let	O
us	O
begin	O
with	O
a	O
simple	O
example	O
in	O
which	O
we	O
consider	O
a	O
single	O
gaussian	B
random	O
variable	O
x	O
we	O
shall	O
suppose	O
that	O
the	O
variance	B
is	O
known	O
and	O
we	O
consider	O
the	O
task	O
of	O
inferring	O
the	O
mean	B
given	O
a	O
set	O
of	O
n	O
observations	O
x	O
xn	O
the	O
likelihood	B
function	I
that	O
is	O
the	O
probability	B
of	O
the	O
observed	O
data	O
given	O
viewed	O
as	O
a	O
function	O
of	O
is	O
given	O
by	O
px	O
pxn	O
again	O
we	O
emphasize	O
that	O
the	O
likelihood	B
function	I
px	O
is	O
not	O
a	O
probability	B
distribution	O
over	O
and	O
is	O
not	O
normalized	O
exp	O
we	O
see	O
that	O
the	O
likelihood	B
function	I
takes	O
the	O
form	O
of	O
the	O
exponential	O
of	O
a	O
quadratic	O
form	O
in	O
thus	O
if	O
we	O
choose	O
a	O
prior	B
p	O
given	O
by	O
a	O
gaussian	B
it	O
will	O
be	O
a	O
probability	B
distributions	O
conjugate	B
distribution	O
for	O
this	O
likelihood	B
function	I
because	O
the	O
corresponding	O
posterior	O
will	O
be	O
a	O
product	O
of	O
two	O
exponentials	O
of	O
quadratic	O
functions	O
of	O
and	O
hence	O
will	O
also	O
be	O
gaussian	B
we	O
therefore	O
take	O
our	O
prior	B
distribution	O
to	O
be	O
p	O
and	O
the	O
posterior	O
distribution	O
is	O
given	O
by	O
p	O
px	O
exercise	O
simple	O
manipulation	O
involving	O
completing	B
the	I
square	I
in	O
the	O
exponent	O
shows	O
that	O
the	O
posterior	O
distribution	O
is	O
given	O
by	O
p	O
n	O
n	O
where	O
n	O
n	O
n	O
n	O
n	O
n	O
ml	O
in	O
which	O
ml	O
is	O
the	O
maximum	B
likelihood	I
solution	O
for	O
given	O
by	O
the	O
sample	B
mean	B
ml	O
n	O
xn	O
it	O
is	O
worth	O
spending	O
a	O
moment	O
studying	O
the	O
form	O
of	O
the	O
posterior	O
mean	B
and	O
variance	B
first	O
of	O
all	O
we	O
note	O
that	O
the	O
mean	B
of	O
the	O
posterior	O
distribution	O
given	O
by	O
is	O
a	O
compromise	O
between	O
the	O
prior	B
mean	B
and	O
the	O
maximum	B
likelihood	I
solution	O
ml	O
if	O
the	O
number	O
of	O
observed	O
data	O
points	O
n	O
then	O
reduces	O
to	O
the	O
prior	B
mean	B
as	O
expected	O
for	O
n	O
the	O
posterior	O
mean	B
is	O
given	O
by	O
the	O
maximum	B
likelihood	I
solution	O
similarly	O
consider	O
the	O
result	O
for	O
the	O
variance	B
of	O
the	O
posterior	O
distribution	O
we	O
see	O
that	O
this	O
is	O
most	O
naturally	O
expressed	O
in	O
terms	O
of	O
the	O
inverse	B
variance	B
which	O
is	O
called	O
the	O
precision	O
furthermore	O
the	O
precisions	O
are	O
additive	O
so	O
that	O
the	O
precision	O
of	O
the	O
posterior	O
is	O
given	O
by	O
the	O
precision	O
of	O
the	O
prior	B
plus	O
one	O
contribution	O
of	O
the	O
data	O
precision	O
from	O
each	O
of	O
the	O
observed	O
data	O
points	O
as	O
we	O
increase	O
the	O
number	O
of	O
observed	O
data	O
points	O
the	O
precision	O
steadily	O
increases	O
corresponding	O
to	O
a	O
posterior	O
distribution	O
with	O
steadily	O
decreasing	O
variance	B
with	O
no	O
observed	O
data	O
points	O
we	O
have	O
the	O
prior	B
variance	B
whereas	O
if	O
the	O
number	O
of	O
data	O
points	O
n	O
the	O
variance	B
n	O
goes	O
to	O
zero	O
and	O
the	O
posterior	O
distribution	O
becomes	O
infinitely	O
peaked	O
around	O
the	O
maximum	B
likelihood	I
solution	O
we	O
therefore	O
see	O
that	O
the	O
maximum	B
likelihood	I
result	O
of	O
a	O
point	O
estimate	O
for	O
given	O
by	O
is	O
recovered	O
precisely	O
from	O
the	O
bayesian	B
formalism	O
in	O
the	O
limit	O
of	O
an	O
infinite	O
number	O
in	O
which	O
the	O
of	O
observations	O
note	O
also	O
that	O
for	O
finite	O
n	O
if	O
we	O
take	O
the	O
limit	O
prior	B
has	O
infinite	O
variance	B
then	O
the	O
posterior	O
mean	B
reduces	O
to	O
the	O
maximum	O
n	O
likelihood	O
result	O
while	O
from	O
the	O
posterior	O
variance	B
is	O
given	O
by	O
the	O
gaussian	B
distribution	O
figure	O
illustration	O
of	O
bayesian	B
inference	B
for	O
the	O
mean	B
of	O
a	O
gaussian	B
distribution	O
in	O
which	O
the	O
variance	B
is	O
assumed	O
to	O
be	O
known	O
the	O
curves	O
show	O
the	O
prior	B
distribution	O
over	O
curve	O
labelled	O
n	O
which	O
in	O
this	O
case	O
is	O
itself	O
gaussian	B
along	O
with	O
the	O
posterior	O
distribution	O
given	O
by	O
for	O
increasing	O
numbers	O
n	O
of	O
data	O
points	O
the	O
data	O
points	O
are	O
generated	O
from	O
a	O
gaussian	B
of	O
mean	B
and	O
variance	B
and	O
the	O
prior	B
is	O
chosen	O
to	O
have	O
mean	B
in	O
both	O
the	O
prior	B
and	O
the	O
likelihood	B
function	I
the	O
variance	B
is	O
set	O
to	O
the	O
true	O
value	O
n	O
n	O
n	O
n	O
exercise	O
section	O
we	O
illustrate	O
our	O
analysis	O
of	O
bayesian	B
inference	B
for	O
the	O
mean	B
of	O
a	O
gaussian	B
distribution	O
in	O
figure	O
the	O
generalization	B
of	O
this	O
result	O
to	O
the	O
case	O
of	O
a	O
ddimensional	O
gaussian	B
random	O
variable	O
x	O
with	O
known	O
covariance	B
and	O
unknown	O
mean	B
is	O
straightforward	O
we	O
have	O
already	O
seen	O
how	O
the	O
maximum	B
likelihood	I
expression	O
for	O
the	O
mean	B
of	O
a	O
gaussian	B
can	O
be	O
re-cast	O
as	O
a	O
sequential	O
update	O
formula	O
in	O
which	O
the	O
mean	B
after	O
observing	O
n	O
data	O
points	O
was	O
expressed	O
in	O
terms	O
of	O
the	O
mean	B
after	O
observing	O
n	O
data	O
points	O
together	O
with	O
the	O
contribution	O
from	O
data	O
point	O
xn	O
in	O
fact	O
the	O
bayesian	B
paradigm	O
leads	O
very	O
naturally	O
to	O
a	O
sequential	O
view	O
of	O
the	O
inference	B
problem	O
to	O
see	O
this	O
in	O
the	O
context	O
of	O
the	O
inference	B
of	O
the	O
mean	B
of	O
a	O
gaussian	B
we	O
write	O
the	O
posterior	O
distribution	O
with	O
the	O
contribution	O
from	O
the	O
final	O
data	O
point	O
xn	O
separated	O
out	O
so	O
that	O
n	O
p	O
p	O
pxn	O
pxn	O
the	O
term	O
in	O
square	O
brackets	O
is	O
to	O
a	O
normalization	O
coefficient	O
just	O
the	O
posterior	O
distribution	O
after	O
observing	O
n	O
data	O
points	O
we	O
see	O
that	O
this	O
can	O
be	O
viewed	O
as	O
a	O
prior	B
distribution	O
which	O
is	O
combined	O
using	O
bayes	B
theorem	O
with	O
the	O
likelihood	B
function	I
associated	O
with	O
data	O
point	O
xn	O
to	O
arrive	O
at	O
the	O
posterior	O
distribution	O
after	O
observing	O
n	O
data	O
points	O
this	O
sequential	O
view	O
of	O
bayesian	B
inference	B
is	O
very	O
general	O
and	O
applies	O
to	O
any	O
problem	O
in	O
which	O
the	O
observed	O
data	O
are	O
assumed	O
to	O
be	O
independent	B
and	O
identically	O
distributed	O
so	O
far	O
we	O
have	O
assumed	O
that	O
the	O
variance	B
of	O
the	O
gaussian	B
distribution	O
over	O
the	O
data	O
is	O
known	O
and	O
our	O
goal	O
is	O
to	O
infer	O
the	O
mean	B
now	O
let	O
us	O
suppose	O
that	O
the	O
mean	B
is	O
known	O
and	O
we	O
wish	O
to	O
infer	O
the	O
variance	B
again	O
our	O
calculations	O
will	O
be	O
greatly	O
simplified	O
if	O
we	O
choose	O
a	O
conjugate	B
form	O
for	O
the	O
prior	B
distribution	O
it	O
turns	O
out	O
to	O
be	O
most	O
convenient	O
to	O
work	O
with	O
the	O
precision	O
the	O
likelihood	B
function	I
for	O
takes	O
the	O
form	O
px	O
exp	O
n	O
probability	B
distributions	O
a	O
b	O
a	O
b	O
a	O
b	O
figure	O
plot	O
of	O
the	O
gamma	B
distribution	I
gam	O
b	O
defined	O
by	O
for	O
various	O
values	O
of	O
the	O
parameters	O
a	O
and	O
b	O
the	O
corresponding	O
conjugate	B
prior	B
should	O
therefore	O
be	O
proportional	O
to	O
the	O
product	O
of	O
a	O
power	O
of	O
and	O
the	O
exponential	O
of	O
a	O
linear	O
function	O
of	O
this	O
corresponds	O
to	O
the	O
gamma	B
distribution	I
which	O
is	O
defined	O
by	O
gam	O
b	O
ba	O
a	O
exp	O
b	O
exercise	O
exercise	O
here	O
is	O
the	O
gamma	B
function	I
that	O
is	O
defined	O
by	O
and	O
that	O
ensures	O
that	O
is	O
correctly	O
normalized	O
the	O
gamma	B
distribution	I
has	O
a	O
finite	O
integral	O
if	O
a	O
and	O
the	O
distribution	O
itself	O
is	O
finite	O
if	O
a	O
it	O
is	O
plotted	O
for	O
various	O
values	O
of	O
a	O
and	O
b	O
in	O
figure	O
the	O
mean	B
and	O
variance	B
of	O
the	O
gamma	B
distribution	I
are	O
given	O
by	O
e	O
a	O
b	O
var	O
a	O
consider	O
a	O
prior	B
distribution	O
gam	O
if	O
we	O
multiply	O
by	O
the	O
likelihood	B
function	I
then	O
we	O
obtain	O
a	O
posterior	O
distribution	O
p	O
exp	O
which	O
we	O
recognize	O
as	O
a	O
gamma	B
distribution	I
of	O
the	O
form	O
gam	O
bn	O
where	O
an	O
n	O
bn	O
n	O
ml	O
where	O
ml	O
is	O
the	O
maximum	B
likelihood	I
estimator	O
of	O
the	O
variance	B
note	O
that	O
in	O
there	O
is	O
no	O
need	O
to	O
keep	O
track	O
of	O
the	O
normalization	O
constants	O
in	O
the	O
prior	B
and	O
the	O
likelihood	B
function	I
because	O
if	O
required	O
the	O
correct	O
coefficient	O
can	O
be	O
found	O
at	O
the	O
end	O
using	O
the	O
normalized	O
form	O
for	O
the	O
gamma	B
distribution	I
the	O
gaussian	B
distribution	O
from	O
we	O
see	O
that	O
the	O
effect	O
of	O
observing	O
n	O
data	O
points	O
is	O
to	O
increase	O
the	O
value	O
of	O
the	O
coefficient	O
a	O
by	O
thus	O
we	O
can	O
interpret	O
the	O
parameter	O
in	O
the	O
prior	B
in	O
terms	O
of	O
effective	O
prior	B
observations	O
similarly	O
from	O
we	O
see	O
that	O
the	O
n	O
data	O
points	O
contribute	O
n	O
ml	O
is	O
the	O
variance	B
and	O
so	O
we	O
can	O
interpret	O
the	O
parameter	O
in	O
the	O
prior	B
as	O
arising	O
from	O
the	O
effective	O
prior	B
observations	O
having	O
variance	B
recall	O
that	O
we	O
made	O
an	O
analogous	O
interpretation	O
for	O
the	O
dirichlet	B
prior	B
these	O
distributions	O
are	O
examples	O
of	O
the	O
exponential	B
family	I
and	O
we	O
shall	O
see	O
that	O
the	O
interpretation	O
of	O
a	O
conjugate	B
prior	B
in	O
terms	O
of	O
effective	O
fictitious	O
data	O
points	O
is	O
a	O
general	O
one	O
for	O
the	O
exponential	B
family	I
of	O
distributions	O
to	O
the	O
parameter	O
b	O
where	O
section	O
instead	O
of	O
working	O
with	O
the	O
precision	O
we	O
can	O
consider	O
the	O
variance	B
itself	O
the	O
conjugate	B
prior	B
in	O
this	O
case	O
is	O
called	O
the	O
inverse	B
gamma	B
distribution	I
although	O
we	O
shall	O
not	O
discuss	O
this	O
further	O
because	O
we	O
will	O
find	O
it	O
more	O
convenient	O
to	O
work	O
with	O
the	O
precision	O
now	O
suppose	O
that	O
both	O
the	O
mean	B
and	O
the	O
precision	O
are	O
unknown	O
to	O
find	O
a	O
conjugate	B
prior	B
we	O
consider	O
the	O
dependence	O
of	O
the	O
likelihood	B
function	I
on	O
and	O
exp	O
px	O
exp	O
exp	O
xn	O
n	O
we	O
now	O
wish	O
to	O
identify	O
a	O
prior	B
distribution	O
p	O
that	O
has	O
the	O
same	O
functional	B
dependence	O
on	O
and	O
as	O
the	O
likelihood	B
function	I
and	O
that	O
should	O
therefore	O
take	O
the	O
form	O
p	O
exp	O
c	O
expc	O
d	O
exp	O
exp	O
where	O
c	O
d	O
and	O
are	O
constants	O
since	O
we	O
can	O
always	O
write	O
p	O
p	O
we	O
can	O
find	O
p	O
and	O
p	O
by	O
inspection	O
in	O
particular	O
we	O
see	O
that	O
p	O
is	O
a	O
gaussian	B
whose	O
precision	O
is	O
a	O
linear	O
function	O
of	O
and	O
that	O
p	O
is	O
a	O
gamma	B
distribution	I
so	O
that	O
the	O
normalized	O
prior	B
takes	O
the	O
form	O
d	O
p	O
n	O
b	O
where	O
we	O
have	O
defined	O
new	O
constants	O
given	O
by	O
c	O
a	O
b	O
d	O
the	O
distribution	O
is	O
called	O
the	O
normal-gamma	O
or	O
gaussian-gamma	B
distribution	I
and	O
is	O
plotted	O
in	O
figure	O
note	O
that	O
this	O
is	O
not	O
simply	O
the	O
product	O
of	O
an	O
independent	B
gaussian	B
prior	B
over	O
and	O
a	O
gamma	O
prior	B
over	O
because	O
the	O
precision	O
of	O
is	O
a	O
linear	O
function	O
of	O
even	O
if	O
we	O
chose	O
a	O
prior	B
in	O
which	O
and	O
were	O
independent	B
the	O
posterior	O
distribution	O
would	O
exhibit	O
a	O
coupling	O
between	O
the	O
precision	O
of	O
and	O
the	O
value	O
of	O
probability	B
distributions	O
figure	O
contour	O
plot	O
of	O
the	O
normal-gamma	B
distribution	I
for	O
parameter	O
values	O
a	O
and	O
b	O
exercise	O
in	O
the	O
case	O
of	O
the	O
multivariate	O
gaussian	B
distribution	O
x	O
for	O
a	O
ddimensional	O
variable	O
x	O
the	O
conjugate	B
prior	B
distribution	O
for	O
the	O
mean	B
assuming	O
the	O
precision	O
is	O
known	O
is	O
again	O
a	O
gaussian	B
for	O
known	O
mean	B
and	O
unknown	O
precision	B
matrix	I
the	O
conjugate	B
prior	B
is	O
the	O
wishart	B
distribution	I
given	O
by	O
w	O
b	O
d	O
exp	O
where	O
is	O
called	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
of	O
the	O
distribution	O
w	O
is	O
a	O
d	O
d	O
scale	O
matrix	O
and	O
tr	O
denotes	O
the	O
trace	O
the	O
normalization	O
constant	O
b	O
is	O
given	O
by	O
trw	O
i	O
bw	O
dd	O
again	O
it	O
is	O
also	O
possible	O
to	O
define	O
a	O
conjugate	B
prior	B
over	O
the	O
covariance	B
matrix	O
itself	O
rather	O
than	O
over	O
the	O
precision	B
matrix	I
which	O
leads	O
to	O
the	O
inverse	B
wishart	B
distribution	I
although	O
we	O
shall	O
not	O
discuss	O
this	O
further	O
if	O
both	O
the	O
mean	B
and	O
the	O
precision	O
are	O
unknown	O
then	O
following	O
a	O
similar	O
line	O
of	O
reasoning	O
to	O
the	O
univariate	O
case	O
the	O
conjugate	B
prior	B
is	O
given	O
by	O
p	O
w	O
n	O
which	O
is	O
known	O
as	O
the	O
normal-wishart	O
or	O
gaussian-wishart	B
distribution	I
student	O
s	O
t-distribution	O
we	O
have	O
seen	O
that	O
the	O
conjugate	B
prior	B
for	O
the	O
precision	O
of	O
a	O
gaussian	B
is	O
given	O
by	O
a	O
gamma	B
distribution	I
if	O
we	O
have	O
a	O
univariate	O
gaussian	B
n	O
together	O
with	O
a	O
gamma	O
prior	B
gam	O
b	O
and	O
we	O
integrate	O
out	O
the	O
precision	O
we	O
obtain	O
the	O
marginal	B
distribution	O
of	O
x	O
in	O
the	O
form	O
section	O
exercise	O
figure	O
plot	O
of	O
student	O
s	O
t-distribution	O
for	O
and	O
for	O
various	O
values	O
of	O
the	O
limit	O
corresponds	O
to	O
a	O
gaussian	B
distribution	O
with	O
mean	B
and	O
precision	O
the	O
gaussian	B
distribution	O
ba	O
px	O
a	O
b	O
n	O
bae	O
b	O
a	O
b	O
d	O
exp	O
b	O
d	O
a	O
where	O
we	O
have	O
made	O
the	O
change	O
of	O
variable	O
z	O
by	O
convention	O
we	O
define	O
new	O
parameters	O
given	O
by	O
and	O
ab	O
in	O
terms	O
of	O
which	O
the	O
distribution	O
px	O
a	O
b	O
takes	O
the	O
form	O
stx	O
which	O
is	O
known	O
as	O
student	O
s	O
t-distribution	O
the	O
parameter	O
is	O
sometimes	O
called	O
the	O
precision	O
of	O
the	O
t-distribution	O
even	O
though	O
it	O
is	O
not	O
in	O
general	O
equal	O
to	O
the	O
inverse	B
of	O
the	O
variance	B
the	O
parameter	O
is	O
called	O
the	O
degrees	B
of	I
freedom	I
and	O
its	O
effect	O
is	O
illustrated	O
in	O
figure	O
for	O
the	O
particular	O
case	O
of	O
the	O
t-distribution	O
reduces	O
to	O
the	O
cauchy	B
distribution	I
while	O
in	O
the	O
limit	O
the	O
t-distribution	O
stx	O
becomes	O
a	O
gaussian	B
n	O
with	O
mean	B
and	O
precision	O
from	O
we	O
see	O
that	O
student	O
s	O
t-distribution	O
is	O
obtained	O
by	O
adding	O
up	O
an	O
infinite	O
number	O
of	O
gaussian	B
distributions	O
having	O
the	O
same	O
mean	B
but	O
different	O
precisions	O
this	O
can	O
be	O
interpreted	O
as	O
an	O
infinite	O
mixture	B
of	I
gaussians	I
mixtures	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
section	O
the	O
result	O
is	O
a	O
distribution	O
that	O
in	O
general	O
has	O
longer	O
tails	O
than	O
a	O
gaussian	B
as	O
was	O
seen	O
in	O
figure	O
this	O
gives	O
the	O
tdistribution	O
an	O
important	O
property	O
called	O
robustness	B
which	O
means	O
that	O
it	O
is	O
much	O
less	O
sensitive	O
than	O
the	O
gaussian	B
to	O
the	O
presence	O
of	O
a	O
few	O
data	O
points	O
which	O
are	O
outliers	B
the	O
robustness	B
of	O
the	O
t-distribution	O
is	O
illustrated	O
in	O
figure	O
which	O
compares	O
the	O
maximum	B
likelihood	I
solutions	O
for	O
a	O
gaussian	B
and	O
a	O
t-distribution	O
note	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
t-distribution	O
can	O
be	O
found	O
using	O
the	O
expectationmaximization	O
algorithm	O
here	O
we	O
see	O
that	O
the	O
effect	O
of	O
a	O
small	O
number	O
of	O
exercise	O
exercise	O
probability	B
distributions	O
figure	O
illustration	O
of	O
the	O
robustness	B
of	O
student	O
s	O
t-distribution	O
compared	O
to	O
a	O
gaussian	B
histogram	O
distribution	O
of	O
data	O
points	O
drawn	O
from	O
a	O
gaussian	B
distribution	O
together	O
with	O
the	O
maximum	B
likelihood	I
fit	O
obtained	O
from	O
a	O
t-distribution	O
curve	O
and	O
a	O
gaussian	B
curve	O
largely	O
hidden	O
by	O
the	O
red	O
curve	O
because	O
the	O
t-distribution	O
contains	O
the	O
gaussian	B
as	O
a	O
special	O
case	O
it	O
gives	O
almost	O
the	O
same	O
solution	O
as	O
the	O
gaussian	B
the	O
same	O
data	O
set	O
but	O
with	O
three	O
additional	O
outlying	O
data	O
points	O
showing	O
how	O
the	O
gaussian	B
curve	O
is	O
strongly	O
distorted	O
by	O
the	O
outliers	B
whereas	O
the	O
t-distribution	O
curve	O
is	O
relatively	O
unaffected	O
outliers	B
is	O
much	O
less	O
significant	O
for	O
the	O
t-distribution	O
than	O
for	O
the	O
gaussian	B
outliers	B
can	O
arise	O
in	O
practical	O
applications	O
either	O
because	O
the	O
process	O
that	O
generates	O
the	O
data	O
corresponds	O
to	O
a	O
distribution	O
having	O
a	O
heavy	O
tail	O
or	O
simply	O
through	O
mislabelled	O
data	O
robustness	B
is	O
also	O
an	O
important	O
property	O
for	B
regression	B
problems	O
unsurprisingly	O
the	O
least	O
squares	O
approach	O
to	O
regression	B
does	O
not	O
exhibit	O
robustness	B
because	O
it	O
corresponds	O
to	O
maximum	B
likelihood	I
under	O
a	O
gaussian	B
distribution	O
by	O
basing	O
a	O
regression	B
model	O
on	O
a	O
heavy-tailed	O
distribution	O
such	O
as	O
a	O
t-distribution	O
we	O
obtain	O
a	O
more	O
robust	O
model	O
if	O
we	O
go	O
back	O
to	O
and	O
substitute	O
the	O
alternative	O
parameters	O
ab	O
and	O
ba	O
we	O
see	O
that	O
the	O
t-distribution	O
can	O
be	O
written	O
in	O
the	O
form	O
stx	O
x	O
gam	O
d	O
we	O
can	O
then	O
generalize	O
this	O
to	O
a	O
multivariate	O
gaussian	B
n	O
to	O
obtain	O
the	O
corresponding	O
multivariate	O
student	O
s	O
t-distribution	O
in	O
the	O
form	O
stx	O
n	O
d	O
exercise	O
using	O
the	O
same	O
technique	O
as	O
for	O
the	O
univariate	O
case	O
we	O
can	O
evaluate	O
this	O
integral	O
to	O
give	O
the	O
gaussian	B
distribution	O
stx	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
and	O
is	O
the	O
squared	O
mahalanobis	B
distance	I
defined	O
by	O
exercise	O
this	O
is	O
the	O
multivariate	O
form	O
of	O
student	O
s	O
t-distribution	O
and	O
satisfies	O
the	O
following	O
properties	O
ex	O
covx	O
modex	O
if	O
if	O
with	O
corresponding	O
results	O
for	O
the	O
univariate	O
case	O
periodic	O
variables	O
although	O
gaussian	B
distributions	O
are	O
of	O
great	O
practical	O
significance	O
both	O
in	O
their	O
own	O
right	O
and	O
as	O
building	O
blocks	O
for	O
more	O
complex	O
probabilistic	O
models	O
there	O
are	O
situations	O
in	O
which	O
they	O
are	O
inappropriate	O
as	O
density	B
models	O
for	O
continuous	O
variables	O
one	O
important	O
case	O
which	O
arises	O
in	O
practical	O
applications	O
is	O
that	O
of	O
periodic	O
variables	O
an	O
example	O
of	O
a	O
periodic	B
variable	I
would	O
be	O
the	O
wind	O
direction	O
at	O
a	O
particular	O
geographical	O
location	O
we	O
might	O
for	O
instance	O
measure	O
values	O
of	O
wind	O
direction	O
on	O
a	O
number	O
of	O
days	O
and	O
wish	O
to	O
summarize	O
this	O
using	O
a	O
parametric	O
distribution	O
another	O
example	O
is	O
calendar	O
time	O
where	O
we	O
may	O
be	O
interested	O
in	O
modelling	O
quantities	O
that	O
are	O
believed	O
to	O
be	O
periodic	O
over	O
hours	O
or	O
over	O
an	O
annual	O
cycle	O
such	O
quantities	O
can	O
conveniently	O
be	O
represented	O
using	O
an	O
angular	O
coordinate	O
and	O
we	O
might	O
be	O
tempted	O
to	O
treat	O
periodic	O
variables	O
by	O
choosing	O
some	O
direction	O
as	O
the	O
origin	O
and	O
then	O
applying	O
a	O
conventional	O
distribution	O
such	O
as	O
the	O
gaussian	B
such	O
an	O
approach	O
however	O
would	O
give	O
results	O
that	O
were	O
strongly	O
dependent	O
on	O
the	O
arbitrary	O
choice	O
of	O
origin	O
suppose	O
for	O
instance	O
that	O
we	O
have	O
two	O
observations	O
at	O
and	O
we	O
model	O
them	O
using	O
a	O
standard	O
univariate	O
gaussian	B
distribution	O
if	O
we	O
choose	O
the	O
origin	O
at	O
then	O
the	O
sample	B
mean	B
of	O
this	O
data	O
set	O
will	O
be	O
with	O
standard	B
deviation	I
whereas	O
if	O
we	O
choose	O
the	O
origin	O
at	O
then	O
the	O
mean	B
will	O
be	O
we	O
clearly	O
need	O
to	O
develop	O
a	O
special	O
approach	O
for	O
the	O
treatment	O
of	O
periodic	O
variables	O
let	O
us	O
consider	O
the	O
problem	O
of	O
evaluating	O
the	O
mean	B
of	O
a	O
set	O
of	O
observations	O
d	O
n	O
of	O
a	O
periodic	B
variable	I
from	O
now	O
on	O
we	O
shall	O
assume	O
that	O
is	O
measured	O
in	O
radians	O
we	O
have	O
already	O
seen	O
that	O
the	O
simple	O
average	O
n	O
will	O
be	O
strongly	O
coordinate	O
dependent	O
to	O
find	O
an	O
invariant	O
measure	O
of	O
the	O
mean	B
we	O
note	O
that	O
the	O
observations	O
can	O
be	O
viewed	O
as	O
points	O
on	O
the	O
unit	O
circle	O
and	O
can	O
therefore	O
be	O
described	O
instead	O
by	O
two-dimensional	O
unit	O
vectors	O
xn	O
where	O
for	O
n	O
n	O
as	O
illustrated	O
in	O
figure	O
we	O
can	O
average	O
the	O
vectors	O
and	O
the	O
standard	B
deviation	I
will	O
be	O
probability	B
distributions	O
figure	O
illustration	O
of	O
the	O
representation	O
of	O
values	O
n	O
of	O
a	O
periodic	B
variable	I
as	O
twodimensional	O
vectors	O
xn	O
living	O
on	O
the	O
unit	O
circle	O
also	O
shown	O
is	O
the	O
average	O
x	O
of	O
those	O
vectors	O
x	O
r	O
instead	O
to	O
give	O
xn	O
x	O
n	O
and	O
then	O
find	O
the	O
corresponding	O
angle	O
of	O
this	O
average	O
clearly	O
this	O
definition	O
will	O
ensure	O
that	O
the	O
location	O
of	O
the	O
mean	B
is	O
independent	B
of	O
the	O
origin	O
of	O
the	O
angular	O
coordinate	O
note	O
that	O
x	O
will	O
typically	O
lie	O
inside	O
the	O
unit	O
circle	O
the	O
cartesian	O
coordinates	O
of	O
the	O
observations	O
are	O
given	O
by	O
xn	O
n	O
sin	O
n	O
and	O
we	O
can	O
write	O
the	O
cartesian	O
coordinates	O
of	O
the	O
sample	B
mean	B
in	O
the	O
form	O
x	O
cos	O
r	O
sin	O
substituting	O
into	O
and	O
equating	O
the	O
and	O
components	O
then	O
gives	O
r	O
cos	O
n	O
cos	O
n	O
r	O
sin	O
n	O
sin	O
n	O
taking	O
the	O
ratio	O
and	O
using	O
the	O
identity	O
tan	O
sin	O
cos	O
we	O
can	O
solve	O
for	O
to	O
give	O
tan	O
n	O
sin	O
n	O
n	O
cos	O
n	O
shortly	O
we	O
shall	O
see	O
how	O
this	O
result	O
arises	O
naturally	O
as	O
the	O
maximum	B
likelihood	I
estimator	O
for	O
an	O
appropriately	O
defined	O
distribution	O
over	O
a	O
periodic	B
variable	I
we	O
now	O
consider	O
a	O
periodic	O
generalization	B
of	O
the	O
gaussian	B
called	O
the	O
von	B
mises	I
distribution	I
here	O
we	O
shall	O
limit	O
our	O
attention	O
to	O
univariate	O
distributions	O
although	O
periodic	O
distributions	O
can	O
also	O
be	O
found	O
over	O
hyperspheres	O
of	O
arbitrary	O
dimension	O
for	O
an	O
extensive	O
discussion	O
of	O
periodic	O
distributions	O
see	O
mardia	O
and	O
jupp	O
by	O
convention	O
we	O
will	O
consider	O
distributions	O
p	O
that	O
have	O
period	O
any	O
probability	B
density	B
p	O
defined	O
over	O
must	O
not	O
only	O
be	O
nonnegative	O
and	O
integrate	O
the	O
gaussian	B
distribution	O
figure	O
the	O
von	B
mises	I
distribution	I
can	O
be	O
derived	O
by	O
considering	O
a	O
two-dimensional	O
gaussian	B
of	O
the	O
form	O
whose	O
density	B
contours	O
are	O
shown	O
in	O
blue	O
and	O
conditioning	O
on	O
the	O
unit	O
circle	O
shown	O
in	O
red	O
px	O
r	O
to	O
one	O
but	O
it	O
must	O
also	O
be	O
periodic	O
thus	O
p	O
must	O
satisfy	O
the	O
three	O
conditions	O
p	O
p	O
d	O
p	O
p	O
from	O
it	O
follows	O
that	O
p	O
p	O
for	O
any	O
integer	O
m	O
we	O
can	O
easily	O
obtain	O
a	O
gaussian-like	O
distribution	O
that	O
satisfies	O
these	O
three	O
properties	O
as	O
follows	O
consider	O
a	O
gaussian	B
distribution	O
over	O
two	O
variables	O
x	O
having	O
mean	B
and	O
a	O
covariance	B
matrix	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
so	O
that	O
exp	O
the	O
contours	O
of	O
constant	O
px	O
are	O
circles	O
as	O
illustrated	O
in	O
figure	O
now	O
suppose	O
we	O
consider	O
the	O
value	O
of	O
this	O
distribution	O
along	O
a	O
circle	O
of	O
fixed	O
radius	O
then	O
by	O
construction	O
this	O
distribution	O
will	O
be	O
periodic	O
although	O
it	O
will	O
not	O
be	O
normalized	O
we	O
can	O
determine	O
the	O
form	O
of	O
this	O
distribution	O
by	O
transforming	O
from	O
cartesian	O
coordinates	O
to	O
polar	O
coordinates	O
so	O
that	O
r	O
sin	O
we	O
also	O
map	O
the	O
mean	B
into	O
polar	O
coordinates	O
by	O
writing	O
r	O
cos	O
cos	O
sin	O
next	O
we	O
substitute	O
these	O
transformations	O
into	O
the	O
two-dimensional	O
gaussian	B
distribution	O
and	O
then	O
condition	O
on	O
the	O
unit	O
circle	O
r	O
noting	O
that	O
we	O
are	O
interested	O
only	O
in	O
the	O
dependence	O
on	O
focussing	O
on	O
the	O
exponent	O
in	O
the	O
gaussian	B
distribution	O
we	O
have	O
cos	O
cos	O
sin	O
sin	O
cos	O
cos	O
sin	O
sin	O
cos	O
const	O
probability	B
distributions	O
m	O
m	O
m	O
m	O
figure	O
the	O
von	B
mises	I
distribution	I
plotted	O
for	O
two	O
different	O
parameter	O
values	O
shown	O
as	O
a	O
cartesian	O
plot	O
on	O
the	O
left	O
and	O
as	O
the	O
corresponding	O
polar	O
plot	O
on	O
the	O
right	O
exercise	O
where	O
const	O
denotes	O
terms	O
independent	B
of	O
and	O
we	O
have	O
made	O
use	O
of	O
the	O
following	O
trigonometrical	O
identities	O
a	O
a	O
if	O
we	O
now	O
define	O
m	O
we	O
obtain	O
our	O
final	O
expression	O
for	O
the	O
distribution	O
of	O
p	O
along	O
the	O
unit	O
circle	O
r	O
in	O
the	O
form	O
cos	O
a	O
cos	O
b	O
sin	O
a	O
sin	O
b	O
cosa	O
b	O
p	O
m	O
expm	O
cos	O
which	O
is	O
called	O
the	O
von	B
mises	I
distribution	I
or	O
the	O
circular	O
normal	O
here	O
the	O
parameter	O
corresponds	O
to	O
the	O
mean	B
of	O
the	O
distribution	O
while	O
m	O
which	O
is	O
known	O
as	O
the	O
concentration	B
parameter	I
is	O
analogous	O
to	O
the	O
inverse	B
variance	B
for	O
the	O
gaussian	B
the	O
normalization	O
coefficient	O
in	O
is	O
expressed	O
in	O
terms	O
of	O
which	O
is	O
the	O
zeroth-order	O
bessel	O
function	O
of	O
the	O
first	O
kind	O
and	O
stegun	O
and	O
is	O
defined	O
by	O
expm	O
cos	O
d	O
exercise	O
for	O
large	O
m	O
the	O
distribution	O
becomes	O
approximately	O
gaussian	B
the	O
von	B
mises	I
distribution	I
is	O
plotted	O
in	O
figure	O
and	O
the	O
function	O
is	O
plotted	O
in	O
figure	O
now	O
consider	O
the	O
maximum	B
likelihood	I
estimators	O
for	O
the	O
parameters	O
and	O
m	O
for	O
the	O
von	B
mises	I
distribution	I
the	O
log	O
likelihood	B
function	I
is	O
given	O
by	O
cos	O
n	O
ln	O
pd	O
m	O
n	O
n	O
ln	O
m	O
the	O
gaussian	B
distribution	O
am	O
m	O
m	O
figure	O
plot	O
of	O
the	O
bessel	O
function	O
defined	O
by	O
together	O
with	O
the	O
function	O
am	O
defined	O
by	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
equal	O
to	O
zero	O
gives	O
sin	O
n	O
to	O
solve	O
for	O
we	O
make	O
use	O
of	O
the	O
trigonometric	O
identity	O
sina	O
b	O
cos	O
b	O
sin	O
a	O
cos	O
a	O
sin	O
b	O
n	O
sin	O
n	O
n	O
cos	O
n	O
exercise	O
from	O
which	O
we	O
obtain	O
tan	O
ml	O
which	O
we	O
recognize	O
as	O
the	O
result	O
obtained	O
earlier	O
for	O
the	O
mean	B
of	O
the	O
observations	O
viewed	O
in	O
a	O
two-dimensional	O
cartesian	O
space	O
similarly	O
maximizing	O
with	O
respect	O
to	O
m	O
and	O
making	O
use	O
of	O
i	O
and	O
stegun	O
we	O
have	O
am	O
n	O
cos	O
n	O
ml	O
where	O
we	O
have	O
substituted	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
ml	O
that	O
we	O
are	O
performing	O
a	O
joint	O
optimization	O
over	O
and	O
m	O
and	O
we	O
have	O
defined	O
am	O
the	O
function	O
am	O
is	O
plotted	O
in	O
figure	O
making	O
use	O
of	O
the	O
trigonometric	O
identity	O
we	O
can	O
write	O
in	O
the	O
form	O
amml	O
cos	O
ml	O
sin	O
n	O
sin	O
ml	O
cos	O
n	O
n	O
n	O
probability	B
distributions	O
on	O
the	O
left	O
figure	O
plots	O
of	O
the	O
old	B
faithful	I
data	I
in	O
which	O
the	O
blue	O
curves	O
show	O
contours	O
of	O
constant	O
probability	B
density	B
is	O
a	O
single	O
gaussian	B
distribution	O
which	O
has	O
been	O
fitted	O
to	O
the	O
data	O
using	O
maximum	B
likelihood	I
note	O
that	O
this	O
distribution	O
fails	O
to	O
capture	O
the	O
two	O
clumps	O
in	O
the	O
data	O
and	O
indeed	O
places	O
much	O
of	O
its	O
probability	B
mass	O
in	O
the	O
central	O
region	O
between	O
the	O
clumps	O
where	O
the	O
data	O
are	O
relatively	O
sparse	O
on	O
the	O
right	O
the	O
distribution	O
is	O
given	O
by	O
a	O
linear	O
combination	O
of	O
two	O
gaussians	O
which	O
has	O
been	O
fitted	O
to	O
the	O
data	O
by	O
maximum	B
likelihood	I
using	O
techniques	O
discussed	O
chapter	O
and	O
which	O
gives	O
a	O
better	O
representation	O
of	O
the	O
data	O
the	O
right-hand	O
side	O
of	O
is	O
easily	O
evaluated	O
and	O
the	O
function	O
am	O
can	O
be	O
inverted	O
numerically	O
for	O
completeness	O
we	O
mention	O
briefly	O
some	O
alternative	O
techniques	O
for	O
the	O
construction	O
of	O
periodic	O
distributions	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
a	O
histogram	O
of	O
observations	O
in	O
which	O
the	O
angular	O
coordinate	O
is	O
divided	O
into	O
fixed	O
bins	O
this	O
has	O
the	O
virtue	O
of	O
simplicity	O
and	O
flexibility	O
but	O
also	O
suffers	O
from	O
significant	O
limitations	O
as	O
we	O
shall	O
see	O
when	O
we	O
discuss	O
histogram	O
methods	O
in	O
more	O
detail	O
in	O
section	O
another	O
approach	O
starts	O
like	O
the	O
von	B
mises	I
distribution	I
from	O
a	O
gaussian	B
distribution	O
over	O
a	O
euclidean	O
space	O
but	O
now	O
marginalizes	O
onto	O
the	O
unit	O
circle	O
rather	O
than	O
conditioning	O
and	O
jupp	O
however	O
this	O
leads	O
to	O
more	O
complex	O
forms	O
of	O
distribution	O
and	O
will	O
not	O
be	O
discussed	O
further	O
finally	O
any	O
valid	O
distribution	O
over	O
the	O
real	O
axis	O
as	O
a	O
gaussian	B
can	O
be	O
turned	O
into	O
a	O
periodic	O
distribution	O
by	O
mapping	O
successive	O
intervals	O
of	O
width	O
onto	O
the	O
periodic	B
variable	I
which	O
corresponds	O
to	O
wrapping	O
the	O
real	O
axis	O
around	O
unit	O
circle	O
again	O
the	O
resulting	O
distribution	O
is	O
more	O
complex	O
to	O
handle	O
than	O
the	O
von	B
mises	I
distribution	I
one	O
limitation	O
of	O
the	O
von	B
mises	I
distribution	I
is	O
that	O
it	O
is	O
unimodal	O
by	O
forming	O
mixtures	O
of	O
von	O
mises	O
distributions	O
we	O
obtain	O
a	O
flexible	O
framework	O
for	O
modelling	O
periodic	O
variables	O
that	O
can	O
handle	O
multimodality	B
for	O
an	O
example	O
of	O
a	O
machine	O
learning	B
application	O
that	O
makes	O
use	O
of	O
von	O
mises	O
distributions	O
see	O
lawrence	O
et	O
al	O
and	O
for	O
extensions	O
to	O
modelling	O
conditional	B
densities	O
for	B
regression	B
problems	O
see	O
bishop	O
and	O
nabney	O
mixtures	O
of	O
gaussians	O
while	O
the	O
gaussian	B
distribution	O
has	O
some	O
important	O
analytical	O
properties	O
it	O
suffers	O
from	O
significant	O
limitations	O
when	O
it	O
comes	O
to	O
modelling	O
real	O
data	O
sets	O
consider	O
the	O
example	O
shown	O
in	O
figure	O
this	O
is	O
known	O
as	O
the	O
old	B
faithful	I
data	I
set	O
and	O
comprises	O
measurements	O
of	O
the	O
eruption	O
of	O
the	O
old	O
faithful	O
geyser	O
at	O
yellowstone	B
national	I
park	I
in	O
the	O
usa	O
each	O
measurement	O
comprises	O
the	O
duration	O
of	O
appendix	O
a	O
the	O
gaussian	B
distribution	O
figure	O
example	O
of	O
a	O
gaussian	B
mixture	B
distribution	I
in	O
one	O
dimension	O
showing	O
three	O
gaussians	O
scaled	O
by	O
a	O
coefficient	O
in	O
blue	O
and	O
their	O
sum	O
in	O
red	O
px	O
the	O
eruption	O
in	O
minutes	O
axis	O
and	O
the	O
time	O
in	O
minutes	O
to	O
the	O
next	O
eruption	O
axis	O
we	O
see	O
that	O
the	O
data	O
set	O
forms	O
two	O
dominant	O
clumps	O
and	O
that	O
a	O
simple	O
gaussian	B
distribution	O
is	O
unable	O
to	O
capture	O
this	O
structure	O
whereas	O
a	O
linear	O
superposition	O
of	O
two	O
gaussians	O
gives	O
a	O
better	O
characterization	O
of	O
the	O
data	O
set	O
x	O
such	O
superpositions	O
formed	O
by	O
taking	O
linear	O
combinations	O
of	O
more	O
basic	O
distributions	O
such	O
as	O
gaussians	O
can	O
be	O
formulated	O
as	O
probabilistic	O
models	O
known	O
as	O
mixture	B
distributions	O
and	O
basford	O
mclachlan	O
and	O
peel	O
in	O
figure	O
we	O
see	O
that	O
a	O
linear	O
combination	O
of	O
gaussians	O
can	O
give	O
rise	O
to	O
very	O
complex	O
densities	O
by	O
using	O
a	O
sufficient	O
number	O
of	O
gaussians	O
and	O
by	O
adjusting	O
their	O
means	O
and	O
covariances	O
as	O
well	O
as	O
the	O
coefficients	O
in	O
the	O
linear	O
combination	O
almost	O
any	O
continuous	O
density	B
can	O
be	O
approximated	O
to	O
arbitrary	O
accuracy	O
we	O
therefore	O
consider	O
a	O
superposition	O
of	O
k	O
gaussian	B
densities	O
of	O
the	O
form	O
px	O
kn	O
k	O
k	O
which	O
is	O
called	O
a	O
mixture	B
of	I
gaussians	I
each	O
gaussian	B
density	B
n	O
k	O
k	O
is	O
called	O
a	O
component	O
of	O
the	O
mixture	B
and	O
has	O
its	O
own	O
mean	B
k	O
and	O
covariance	B
k	O
contour	O
and	O
surface	O
plots	O
for	O
a	O
gaussian	B
mixture	B
having	O
components	O
are	O
shown	O
in	O
figure	O
in	O
this	O
section	O
we	O
shall	O
consider	O
gaussian	B
components	O
to	O
illustrate	O
the	O
framework	O
of	O
mixture	B
models	O
more	O
generally	O
mixture	B
models	O
can	O
comprise	O
linear	O
combinations	O
of	O
other	O
distributions	O
for	O
instance	O
in	O
section	O
we	O
shall	O
consider	O
mixtures	O
of	O
bernoulli	B
distributions	O
as	O
an	O
example	O
of	O
a	O
mixture	B
model	I
for	O
discrete	O
variables	O
section	O
the	O
parameters	O
k	O
in	O
are	O
called	O
mixing	O
coefficients	O
if	O
we	O
integrate	O
both	O
sides	O
of	O
with	O
respect	O
to	O
x	O
and	O
note	O
that	O
both	O
px	O
and	O
the	O
individual	O
gaussian	B
components	O
are	O
normalized	O
we	O
obtain	O
k	O
also	O
the	O
requirement	O
that	O
px	O
together	O
with	O
n	O
k	O
k	O
implies	O
k	O
for	O
all	O
k	O
combining	O
this	O
with	O
the	O
condition	O
we	O
obtain	O
k	O
probability	B
distributions	O
figure	O
illustration	O
of	O
a	O
mixture	B
of	I
gaussians	I
in	O
a	O
two-dimensional	O
space	O
contours	O
of	O
constant	O
density	B
for	O
each	O
of	O
the	O
mixture	B
components	O
in	O
which	O
the	O
components	O
are	O
denoted	O
red	O
blue	O
and	O
green	O
and	O
the	O
values	O
of	O
the	O
mixing	O
coefficients	O
are	O
shown	O
below	O
each	O
component	O
contours	O
of	O
the	O
marginal	B
probability	B
density	B
px	O
of	O
the	O
mixture	B
distribution	I
a	O
surface	O
plot	O
of	O
the	O
distribution	O
px	O
we	O
therefore	O
see	O
that	O
the	O
mixing	O
coefficients	O
satisfy	O
the	O
requirements	O
to	O
be	O
probabilities	O
from	O
the	O
sum	O
and	O
product	O
rules	O
the	O
marginal	B
density	B
is	O
given	O
by	O
px	O
pkpxk	O
which	O
is	O
equivalent	O
to	O
in	O
which	O
we	O
can	O
view	O
k	O
pk	O
as	O
the	O
prior	B
probability	B
of	O
picking	O
the	O
kth	O
component	O
and	O
the	O
density	B
n	O
k	O
k	O
pxk	O
as	O
the	O
probability	B
of	O
x	O
conditioned	O
on	O
k	O
as	O
we	O
shall	O
see	O
in	O
later	O
chapters	O
an	O
important	O
role	O
is	O
played	O
by	O
the	O
posterior	O
probabilities	O
pkx	O
which	O
are	O
also	O
known	O
as	O
responsibilities	O
from	O
bayes	B
theorem	O
these	O
are	O
given	O
by	O
kx	O
pkx	O
pkpxk	O
l	O
plpxl	O
kn	O
k	O
k	O
l	O
ln	O
l	O
l	O
we	O
shall	O
discuss	O
the	O
probabilistic	O
interpretation	O
of	O
the	O
mixture	B
distribution	I
in	O
greater	O
detail	O
in	O
chapter	O
the	O
form	O
of	O
the	O
gaussian	B
mixture	B
distribution	I
is	O
governed	O
by	O
the	O
parameters	O
and	O
where	O
we	O
have	O
used	O
the	O
notation	O
k	O
k	O
and	O
k	O
one	O
way	O
to	O
set	O
the	O
values	O
of	O
these	O
parameters	O
is	O
to	O
use	O
maximum	B
likelihood	I
from	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
kn	O
k	O
k	O
ln	O
px	O
ln	O
the	O
exponential	B
family	I
where	O
x	O
xn	O
we	O
immediately	O
see	O
that	O
the	O
situation	O
is	O
now	O
much	O
more	O
complex	O
than	O
with	O
a	O
single	O
gaussian	B
due	O
to	O
the	O
presence	O
of	O
the	O
summation	O
over	O
k	O
inside	O
the	O
logarithm	O
as	O
a	O
result	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
parameters	O
no	O
longer	O
has	O
a	O
closed-form	O
analytical	O
solution	O
one	O
approach	O
to	O
maximizing	O
the	O
likelihood	B
function	I
is	O
to	O
use	O
iterative	O
numerical	O
optimization	O
techniques	O
nocedal	O
and	O
wright	O
bishop	O
and	O
nabney	O
alternatively	O
we	O
can	O
employ	O
a	O
powerful	O
framework	O
called	O
expectation	B
maximization	I
which	O
will	O
be	O
discussed	O
at	O
length	O
in	O
chapter	O
the	O
exponential	B
family	I
the	O
probability	B
distributions	O
that	O
we	O
have	O
studied	O
so	O
far	O
in	O
this	O
chapter	O
the	O
exception	O
of	O
the	O
gaussian	B
mixture	B
are	O
specific	O
examples	O
of	O
a	O
broad	O
class	O
of	O
distributions	O
called	O
the	O
exponential	B
family	I
and	O
hart	O
bernardo	O
and	O
smith	O
members	O
of	O
the	O
exponential	B
family	I
have	O
many	O
important	O
properties	O
in	O
common	O
and	O
it	O
is	O
illuminating	O
to	O
discuss	O
these	O
properties	O
in	O
some	O
generality	O
the	O
exponential	B
family	I
of	O
distributions	O
over	O
x	O
given	O
parameters	O
is	O
defined	O
to	O
be	O
the	O
set	O
of	O
distributions	O
of	O
the	O
form	O
px	O
hxg	O
exp	O
tux	O
where	O
x	O
may	O
be	O
scalar	O
or	O
vector	O
and	O
may	O
be	O
discrete	O
or	O
continuous	O
here	O
are	O
called	O
the	O
natural	B
parameters	I
of	O
the	O
distribution	O
and	O
ux	O
is	O
some	O
function	O
of	O
x	O
the	O
function	O
g	O
can	O
be	O
interpreted	O
as	O
the	O
coefficient	O
that	O
ensures	O
that	O
the	O
distribution	O
is	O
normalized	O
and	O
therefore	O
satisfies	O
g	O
hx	O
exp	O
tux	O
dx	O
where	O
the	O
integration	O
is	O
replaced	O
by	O
summation	O
if	O
x	O
is	O
a	O
discrete	O
variable	O
we	O
begin	O
by	O
taking	O
some	O
examples	O
of	O
the	O
distributions	O
introduced	O
earlier	O
in	O
the	O
chapter	O
and	O
showing	O
that	O
they	O
are	O
indeed	O
members	O
of	O
the	O
exponential	B
family	I
consider	O
first	O
the	O
bernoulli	B
distribution	I
px	O
bernx	O
x	O
expressing	O
the	O
right-hand	O
side	O
as	O
the	O
exponential	O
of	O
the	O
logarithm	O
we	O
have	O
px	O
expx	O
ln	O
x	O
exp	O
ln	O
ln	O
x	O
comparison	O
with	O
allows	O
us	O
to	O
identify	O
probability	B
distributions	O
which	O
we	O
can	O
solve	O
for	O
to	O
give	O
where	O
exp	O
is	O
called	O
the	O
logistic	B
sigmoid	I
function	O
thus	O
we	O
can	O
write	O
the	O
bernoulli	B
distribution	I
using	O
the	O
standard	O
representation	O
in	O
the	O
form	O
px	O
exp	O
x	O
where	O
we	O
have	O
used	O
which	O
is	O
easily	O
proved	O
from	O
comparison	O
with	O
shows	O
that	O
next	O
consider	O
the	O
multinomial	B
distribution	I
that	O
for	O
a	O
single	O
observation	O
x	O
takes	O
the	O
form	O
px	O
ux	O
x	O
hx	O
g	O
xk	O
k	O
exp	O
xk	O
ln	O
k	O
where	O
x	O
xn	O
again	O
we	O
can	O
write	O
this	O
in	O
the	O
standard	O
representation	O
so	O
that	O
where	O
k	O
ln	O
k	O
and	O
we	O
have	O
defined	O
m	O
again	O
comparing	O
with	O
we	O
have	O
px	O
exp	O
tx	O
ux	O
x	O
hx	O
g	O
note	O
that	O
the	O
parameters	O
k	O
are	O
not	O
independent	B
because	O
the	O
parameters	O
k	O
are	O
subject	O
to	O
the	O
constraint	O
k	O
so	O
that	O
given	O
any	O
m	O
of	O
the	O
parameters	O
k	O
the	O
value	O
of	O
the	O
remaining	O
parameter	O
is	O
fixed	O
in	O
some	O
circumstances	O
it	O
will	O
be	O
convenient	O
to	O
remove	O
this	O
constraint	O
by	O
expressing	O
the	O
distribution	O
in	O
terms	O
of	O
only	O
m	O
parameters	O
this	O
can	O
be	O
achieved	O
by	O
using	O
the	O
relationship	O
to	O
eliminate	O
m	O
by	O
expressing	O
it	O
in	O
terms	O
of	O
the	O
remaining	O
k	O
where	O
k	O
m	O
thereby	O
leaving	O
m	O
parameters	O
note	O
that	O
these	O
remaining	O
parameters	O
are	O
still	O
subject	O
to	O
the	O
constraints	O
k	O
k	O
m	O
exp	O
exp	O
exp	O
xk	O
ln	O
k	O
m	O
m	O
we	O
now	O
identify	O
ln	O
ln	O
m	O
m	O
k	O
k	O
k	O
xk	O
ln	O
k	O
xk	O
ln	O
ln	O
xk	O
k	O
j	O
m	O
j	O
j	O
k	O
k	O
exp	O
k	O
j	O
exp	O
j	O
m	O
the	O
exponential	B
family	I
making	O
use	O
of	O
the	O
constraint	O
the	O
multinomial	B
distribution	I
in	O
this	O
representation	O
then	O
becomes	O
which	O
we	O
can	O
solve	O
for	O
k	O
by	O
first	O
summing	O
both	O
sides	O
over	O
k	O
and	O
then	O
rearranging	O
and	O
back-substituting	O
to	O
give	O
this	O
is	O
called	O
the	O
softmax	B
function	I
or	O
the	O
normalized	O
exponential	O
in	O
this	O
representation	O
the	O
multinomial	B
distribution	I
therefore	O
takes	O
the	O
form	O
px	O
exp	O
k	O
exp	O
tx	O
this	O
is	O
the	O
standard	O
form	O
of	O
the	O
exponential	B
family	I
with	O
parameter	O
vector	O
m	O
in	O
which	O
ux	O
x	O
hx	O
g	O
m	O
exp	O
k	O
finally	O
let	O
us	O
consider	O
the	O
gaussian	B
distribution	O
for	O
the	O
univariate	O
gaussian	B
we	O
have	O
px	O
exp	O
exp	O
x	O
probability	B
distributions	O
exercise	O
exercise	O
which	O
after	O
some	O
simple	O
rearrangement	O
can	O
be	O
cast	O
in	O
the	O
standard	O
exponential	B
family	I
form	O
with	O
x	O
ux	O
hx	O
g	O
exp	O
maximum	B
likelihood	I
and	O
sufficient	B
statistics	I
let	O
us	O
now	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
parameter	O
vector	O
in	O
the	O
general	O
exponential	B
family	I
distribution	O
using	O
the	O
technique	O
of	O
maximum	B
likelihood	I
taking	O
the	O
gradient	O
of	O
both	O
sides	O
of	O
with	O
respect	O
to	O
we	O
have	O
g	O
hx	O
exp	O
tux	O
dx	O
g	O
hx	O
exp	O
tux	O
ux	O
dx	O
rearranging	O
and	O
making	O
use	O
again	O
of	O
then	O
gives	O
g	O
g	O
g	O
hx	O
exp	O
tux	O
ux	O
dx	O
eux	O
where	O
we	O
have	O
used	O
we	O
therefore	O
obtain	O
the	O
result	O
ln	O
g	O
eux	O
note	O
that	O
the	O
covariance	B
of	O
ux	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
second	O
derivatives	O
of	O
g	O
and	O
similarly	O
for	O
higher	O
order	O
moments	O
thus	O
provided	O
we	O
can	O
normalize	O
a	O
distribution	O
from	O
the	O
exponential	B
family	I
we	O
can	O
always	O
find	O
its	O
moments	O
by	O
simple	O
differentiation	O
xn	O
for	O
which	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
now	O
consider	O
a	O
set	O
of	O
independent	B
identically	I
distributed	I
data	O
denoted	O
by	O
x	O
hxn	O
g	O
exp	O
t	O
uxn	O
px	O
setting	O
the	O
gradient	O
of	O
ln	O
px	O
with	O
respect	O
to	O
to	O
zero	O
we	O
get	O
the	O
following	O
condition	O
to	O
be	O
satisfied	O
by	O
the	O
maximum	B
likelihood	I
estimator	O
ml	O
ln	O
g	O
ml	O
n	O
uxn	O
which	O
can	O
in	O
principle	O
be	O
solved	O
to	O
obtain	O
ml	O
we	O
see	O
that	O
the	O
solution	O
for	O
the	O
n	O
uxn	O
which	O
maximum	B
likelihood	I
estimator	O
depends	O
on	O
the	O
data	O
only	O
through	O
is	O
therefore	O
called	O
the	O
sufficient	O
statistic	O
of	O
the	O
distribution	O
we	O
do	O
not	O
need	O
to	O
store	O
the	O
entire	O
data	O
set	O
itself	O
but	O
only	O
the	O
value	O
of	O
the	O
sufficient	O
statistic	O
for	O
the	O
bernoulli	B
distribution	I
for	O
example	O
the	O
function	O
ux	O
is	O
given	O
just	O
by	O
x	O
and	O
so	O
we	O
need	O
only	O
keep	O
the	O
sum	O
of	O
the	O
data	O
points	O
whereas	O
for	O
the	O
gaussian	B
ux	O
and	O
so	O
we	O
should	O
keep	O
both	O
the	O
sum	O
of	O
and	O
the	O
sum	O
of	O
n	O
if	O
we	O
consider	O
the	O
limit	O
n	O
then	O
the	O
right-hand	O
side	O
of	O
becomes	O
eux	O
and	O
so	O
by	O
comparing	O
with	O
we	O
see	O
that	O
in	O
this	O
limit	O
ml	O
will	O
equal	O
the	O
true	O
value	O
in	O
fact	O
this	O
sufficiency	O
property	O
holds	O
also	O
for	O
bayesian	B
inference	B
although	O
we	O
shall	O
defer	O
discussion	O
of	O
this	O
until	O
chapter	O
when	O
we	O
have	O
equipped	O
ourselves	O
with	O
the	O
tools	O
of	O
graphical	O
models	O
and	O
can	O
thereby	O
gain	O
a	O
deeper	O
insight	O
into	O
these	O
important	O
concepts	O
conjugate	B
priors	O
we	O
have	O
already	O
encountered	O
the	O
concept	O
of	O
a	O
conjugate	B
prior	B
several	O
times	O
for	O
example	O
in	O
the	O
context	O
of	O
the	O
bernoulli	B
distribution	I
which	O
the	O
conjugate	B
prior	B
is	O
the	O
beta	B
distribution	I
or	O
the	O
gaussian	B
the	O
conjugate	B
prior	B
for	O
the	O
mean	B
is	O
a	O
gaussian	B
and	O
the	O
conjugate	B
prior	B
for	O
the	O
precision	O
is	O
the	O
wishart	B
distribution	I
in	O
general	O
for	O
a	O
given	O
probability	B
distribution	O
px	O
we	O
can	O
seek	O
a	O
prior	B
p	O
that	O
is	O
conjugate	B
to	O
the	O
likelihood	B
function	I
so	O
that	O
the	O
posterior	O
distribution	O
has	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
for	O
any	O
member	O
of	O
the	O
exponential	B
family	I
there	O
exists	O
a	O
conjugate	B
prior	B
that	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
f	O
exp	O
t	O
where	O
f	O
is	O
a	O
normalization	O
coefficient	O
and	O
g	O
is	O
the	O
same	O
function	O
as	O
appears	O
in	O
to	O
see	O
that	O
this	O
is	O
indeed	O
conjugate	B
let	O
us	O
multiply	O
the	O
prior	B
by	O
the	O
likelihood	B
function	I
to	O
obtain	O
the	O
posterior	O
distribution	O
up	O
to	O
a	O
normalization	O
coefficient	O
in	O
the	O
form	O
the	O
exponential	B
family	I
p	O
g	O
exp	O
t	O
uxn	O
this	O
again	O
takes	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
confirming	O
conjugacy	O
furthermore	O
we	O
see	O
that	O
the	O
parameter	O
can	O
be	O
interpreted	O
as	O
a	O
effective	O
number	O
of	O
pseudo-observations	O
in	O
the	O
prior	B
each	O
of	O
which	O
has	O
a	O
value	O
for	O
the	O
sufficient	O
statistic	O
ux	O
given	O
by	O
noninformative	B
priors	O
in	O
some	O
applications	O
of	O
probabilistic	O
inference	B
we	O
may	O
have	O
prior	B
knowledge	O
that	O
can	O
be	O
conveniently	O
expressed	O
through	O
the	O
prior	B
distribution	O
for	O
example	O
if	O
the	O
prior	B
assigns	O
zero	O
probability	B
to	O
some	O
value	O
of	O
variable	O
then	O
the	O
posterior	O
distribution	O
will	O
necessarily	O
also	O
assign	O
zero	O
probability	B
to	O
that	O
value	O
irrespective	O
of	O
probability	B
distributions	O
any	O
subsequent	O
observations	O
of	O
data	O
in	O
many	O
cases	O
however	O
we	O
may	O
have	O
little	O
idea	O
of	O
what	O
form	O
the	O
distribution	O
should	O
take	O
we	O
may	O
then	O
seek	O
a	O
form	O
of	O
prior	B
distribution	O
called	O
a	O
noninformative	B
prior	B
which	O
is	O
intended	O
to	O
have	O
as	O
little	O
influence	O
on	O
the	O
posterior	O
distribution	O
as	O
possible	O
box	O
and	O
tao	O
bernardo	O
and	O
smith	O
this	O
is	O
sometimes	O
referred	O
to	O
as	O
letting	O
the	O
data	O
speak	O
for	O
themselves	O
if	O
we	O
have	O
a	O
distribution	O
px	O
governed	O
by	O
a	O
parameter	O
we	O
might	O
be	O
tempted	O
to	O
propose	O
a	O
prior	B
distribution	O
p	O
const	O
as	O
a	O
suitable	O
prior	B
if	O
is	O
a	O
discrete	O
variable	O
with	O
k	O
states	O
this	O
simply	O
amounts	O
to	O
setting	O
the	O
prior	B
probability	B
of	O
each	O
state	O
to	O
in	O
the	O
case	O
of	O
continuous	O
parameters	O
however	O
there	O
are	O
two	O
potential	O
difficulties	O
with	O
this	O
approach	O
the	O
first	O
is	O
that	O
if	O
the	O
domain	O
of	O
is	O
unbounded	O
this	O
prior	B
distribution	O
cannot	O
be	O
correctly	O
normalized	O
because	O
the	O
integral	O
over	O
diverges	O
such	O
priors	O
are	O
called	O
improper	B
in	O
practice	O
improper	B
priors	O
can	O
often	O
be	O
used	O
provided	O
the	O
corresponding	O
posterior	O
distribution	O
is	O
proper	O
i	O
e	O
that	O
it	O
can	O
be	O
correctly	O
normalized	O
for	O
instance	O
if	O
we	O
put	O
a	O
uniform	O
prior	B
distribution	O
over	O
the	O
mean	B
of	O
a	O
gaussian	B
then	O
the	O
posterior	O
distribution	O
for	O
the	O
mean	B
once	O
we	O
have	O
observed	O
at	O
least	O
one	O
data	O
point	O
will	O
be	O
proper	O
a	O
second	O
difficulty	O
arises	O
from	O
the	O
transformation	O
behaviour	O
of	O
a	O
probability	B
density	B
under	O
a	O
nonlinear	O
change	O
of	O
variables	O
given	O
by	O
if	O
a	O
function	O
h	O
is	O
constant	O
and	O
we	O
change	O
variables	O
to	O
h	O
will	O
also	O
be	O
constant	O
however	O
if	O
we	O
choose	O
the	O
density	B
p	O
to	O
be	O
constant	O
then	O
the	O
density	B
of	O
will	O
be	O
given	O
from	O
by	O
d	O
d	O
p	O
p	O
p	O
and	O
so	O
the	O
density	B
over	O
will	O
not	O
be	O
constant	O
this	O
issue	O
does	O
not	O
arise	O
when	O
we	O
use	O
maximum	B
likelihood	I
because	O
the	O
likelihood	B
function	I
px	O
is	O
a	O
simple	O
function	O
of	O
and	O
so	O
we	O
are	O
free	O
to	O
use	O
any	O
convenient	O
parameterization	O
if	O
however	O
we	O
are	O
to	O
choose	O
a	O
prior	B
distribution	O
that	O
is	O
constant	O
we	O
must	O
take	O
care	O
to	O
use	O
an	O
appropriate	O
representation	O
for	O
the	O
parameters	O
here	O
we	O
consider	O
two	O
simple	O
examples	O
of	O
noninformative	B
priors	O
first	O
of	O
all	O
if	O
a	O
density	B
takes	O
the	O
form	O
px	O
fx	O
then	O
the	O
parameter	O
is	O
known	O
as	O
a	O
location	B
parameter	I
this	O
family	O
of	O
densities	O
exhibits	O
translation	B
invariance	B
because	O
if	O
we	O
shift	O
x	O
by	O
a	O
constant	O
to	O
x	O
c	O
where	O
we	O
have	O
c	O
thus	O
the	O
density	B
takes	O
the	O
same	O
form	O
in	O
the	O
then	O
new	O
variable	O
as	O
in	O
the	O
original	O
one	O
and	O
so	O
the	O
density	B
is	O
independent	B
of	O
the	O
choice	O
of	O
origin	O
we	O
would	O
like	O
to	O
choose	O
a	O
prior	B
distribution	O
that	O
reflects	O
this	O
translation	B
invariance	B
property	O
and	O
so	O
we	O
choose	O
a	O
prior	B
that	O
assigns	O
equal	O
probability	B
mass	O
to	O
the	O
exponential	B
family	I
an	O
interval	O
a	O
b	O
as	O
to	O
the	O
shifted	O
interval	O
a	O
c	O
b	O
c	O
this	O
implies	O
b	O
b	O
c	O
b	O
p	O
d	O
a	O
a	O
c	O
p	O
d	O
a	O
p	O
c	O
d	O
and	O
because	O
this	O
must	O
hold	O
for	O
all	O
choices	O
of	O
a	O
and	O
b	O
we	O
have	O
p	O
c	O
p	O
which	O
implies	O
that	O
p	O
is	O
constant	O
an	O
example	O
of	O
a	O
location	B
parameter	I
would	O
be	O
the	O
mean	B
of	O
a	O
gaussian	B
distribution	O
as	O
we	O
have	O
seen	O
the	O
conjugate	B
prior	B
distribution	O
for	O
in	O
this	O
case	O
is	O
a	O
gaussian	B
p	O
and	O
we	O
obtain	O
a	O
indeed	O
from	O
and	O
noninformative	B
prior	B
by	O
taking	O
the	O
limit	O
we	O
see	O
that	O
this	O
gives	O
a	O
posterior	O
distribution	O
over	O
in	O
which	O
the	O
contributions	O
from	O
the	O
prior	B
vanish	O
n	O
as	O
a	O
second	O
example	O
consider	O
a	O
density	B
of	O
the	O
form	O
px	O
f	O
x	O
exercise	O
where	O
note	O
that	O
this	O
will	O
be	O
a	O
normalized	O
density	B
provided	O
fx	O
is	O
correctly	O
normalized	O
the	O
parameter	O
is	O
known	O
as	O
a	O
scale	B
parameter	I
and	O
the	O
density	B
exhibits	O
scale	B
invariance	B
because	O
if	O
we	O
scale	O
x	O
by	O
a	O
constant	O
to	O
cx	O
then	O
where	O
we	O
have	O
c	O
this	O
transformation	O
corresponds	O
to	O
a	O
change	O
of	O
f	O
scale	O
for	O
example	O
from	O
meters	O
to	O
kilometers	O
if	O
x	O
is	O
a	O
length	O
and	O
we	O
would	O
like	O
to	O
choose	O
a	O
prior	B
distribution	O
that	O
reflects	O
this	O
scale	B
invariance	B
if	O
we	O
consider	O
an	O
interval	O
a	O
b	O
and	O
a	O
scaled	O
interval	O
ac	O
bc	O
then	O
the	O
prior	B
should	O
assign	O
equal	O
probability	B
mass	O
to	O
these	O
two	O
intervals	O
thus	O
we	O
have	O
b	O
bc	O
p	O
d	O
p	O
d	O
a	O
ac	O
c	O
c	O
d	O
b	O
p	O
a	O
and	O
because	O
this	O
must	O
hold	O
for	O
choices	O
of	O
a	O
and	O
b	O
we	O
have	O
c	O
c	O
p	O
p	O
and	O
hence	O
p	O
note	O
that	O
again	O
this	O
is	O
an	O
improper	B
prior	B
because	O
the	O
integral	O
of	O
the	O
distribution	O
over	O
is	O
divergent	O
it	O
is	O
sometimes	O
also	O
convenient	O
to	O
think	O
of	O
the	O
prior	B
distribution	O
for	O
a	O
scale	B
parameter	I
in	O
terms	O
of	O
the	O
density	B
of	O
the	O
log	O
of	O
the	O
parameter	O
using	O
the	O
transformation	O
rule	O
for	O
densities	O
we	O
see	O
that	O
pln	O
const	O
thus	O
for	O
this	O
prior	B
there	O
is	O
the	O
same	O
probability	B
mass	O
in	O
the	O
range	O
as	O
in	O
the	O
range	O
and	O
in	O
probability	B
distributions	O
an	O
example	O
of	O
a	O
scale	B
parameter	I
would	O
be	O
the	O
standard	B
deviation	I
of	O
a	O
gaussian	B
distribution	O
after	O
we	O
have	O
taken	O
account	O
of	O
the	O
location	B
parameter	I
because	O
x	O
as	O
discussed	O
earlier	O
it	O
is	O
often	O
more	O
convenient	O
to	O
work	O
in	O
terms	O
n	O
exp	O
of	O
the	O
precision	O
rather	O
than	O
itself	O
using	O
the	O
transformation	O
rule	O
for	O
densities	O
we	O
see	O
that	O
a	O
distribution	O
p	O
corresponds	O
to	O
a	O
distribution	O
over	O
of	O
the	O
form	O
p	O
we	O
have	O
seen	O
that	O
the	O
conjugate	B
prior	B
for	O
was	O
the	O
gamma	B
distribution	I
gam	O
given	O
by	O
the	O
noninformative	B
prior	B
is	O
obtained	O
as	O
the	O
special	O
case	O
again	O
if	O
we	O
examine	O
the	O
results	O
and	O
for	O
the	O
posterior	O
distribution	O
of	O
we	O
see	O
that	O
for	O
the	O
posterior	O
depends	O
only	O
on	O
terms	O
arising	O
from	O
the	O
data	O
and	O
not	O
from	O
the	O
prior	B
section	O
nonparametric	B
methods	I
throughout	O
this	O
chapter	O
we	O
have	O
focussed	O
on	O
the	O
use	O
of	O
probability	B
distributions	O
having	O
specific	O
functional	B
forms	O
governed	O
by	O
a	O
small	O
number	O
of	O
parameters	O
whose	O
values	O
are	O
to	O
be	O
determined	O
from	O
a	O
data	O
set	O
this	O
is	O
called	O
the	O
parametric	O
approach	O
to	O
density	B
modelling	O
an	O
important	O
limitation	O
of	O
this	O
approach	O
is	O
that	O
the	O
chosen	O
density	B
might	O
be	O
a	O
poor	O
model	O
of	O
the	O
distribution	O
that	O
generates	O
the	O
data	O
which	O
can	O
result	O
in	O
poor	O
predictive	O
performance	O
for	O
instance	O
if	O
the	O
process	O
that	O
generates	O
the	O
data	O
is	O
multimodal	O
then	O
this	O
aspect	O
of	O
the	O
distribution	O
can	O
never	O
be	O
captured	O
by	O
a	O
gaussian	B
which	O
is	O
necessarily	O
unimodal	O
in	O
this	O
final	O
section	O
we	O
consider	O
some	O
nonparametric	O
approaches	O
to	O
density	B
estimation	I
that	O
make	O
few	O
assumptions	O
about	O
the	O
form	O
of	O
the	O
distribution	O
here	O
we	O
shall	O
focus	O
mainly	O
on	O
simple	O
frequentist	B
methods	O
the	O
reader	O
should	O
be	O
aware	O
however	O
that	O
nonparametric	O
bayesian	B
methods	O
are	O
attracting	O
increasing	O
interest	O
et	O
al	O
neal	O
m	O
uller	O
and	O
quintana	O
teh	O
et	O
al	O
let	O
us	O
start	O
with	O
a	O
discussion	O
of	O
histogram	O
methods	O
for	O
density	B
estimation	I
which	O
we	O
have	O
already	O
encountered	O
in	O
the	O
context	O
of	O
marginal	B
and	O
conditional	B
distributions	O
in	O
figure	O
and	O
in	O
the	O
context	O
of	O
the	O
central	B
limit	I
theorem	I
in	O
figure	O
here	O
we	O
explore	O
the	O
properties	O
of	O
histogram	O
density	B
models	O
in	O
more	O
detail	O
focussing	O
on	O
the	O
case	O
of	O
a	O
single	O
continuous	O
variable	O
x	O
standard	O
histograms	O
simply	O
partition	O
x	O
into	O
distinct	O
bins	O
of	O
width	O
i	O
and	O
then	O
count	O
the	O
number	O
ni	O
of	O
observations	O
of	O
x	O
falling	O
in	O
bin	O
i	O
in	O
order	O
to	O
turn	O
this	O
count	O
into	O
a	O
normalized	O
probability	B
density	B
we	O
simply	O
divide	O
by	O
the	O
total	O
number	O
n	O
of	O
observations	O
and	O
by	O
the	O
width	O
i	O
of	O
the	O
bins	O
to	O
obtain	O
probability	B
values	O
for	O
each	O
bin	O
given	O
by	O
pi	O
ni	O
n	O
i	O
px	O
dx	O
this	O
gives	O
a	O
model	O
for	O
the	O
density	B
for	O
which	O
it	O
is	O
easily	O
seen	O
that	O
px	O
that	O
is	O
constant	O
over	O
the	O
width	O
of	O
each	O
bin	O
and	O
often	O
the	O
bins	O
are	O
chosen	O
to	O
have	O
the	O
same	O
width	O
i	O
figure	O
an	O
illustration	O
of	O
the	O
histogram	O
approach	O
to	O
density	B
estimation	I
in	O
which	O
a	O
data	O
set	O
of	O
data	O
points	O
is	O
generated	O
from	O
the	O
distribution	O
shown	O
by	O
the	O
green	O
curve	O
histogram	O
density	B
estimates	O
based	O
on	O
with	O
a	O
common	O
bin	O
width	O
are	O
shown	O
for	O
various	O
values	O
of	O
nonparametric	B
methods	I
in	O
figure	O
we	O
show	O
an	O
example	O
of	O
histogram	B
density	B
estimation	I
here	O
the	O
data	O
is	O
drawn	O
from	O
the	O
distribution	O
corresponding	O
to	O
the	O
green	O
curve	O
which	O
is	O
formed	O
from	O
a	O
mixture	B
of	O
two	O
gaussians	O
also	O
shown	O
are	O
three	O
examples	O
of	O
histogram	O
density	B
estimates	O
corresponding	O
to	O
three	O
different	O
choices	O
for	O
the	O
bin	O
width	O
we	O
see	O
that	O
when	O
is	O
very	O
small	O
figure	O
the	O
resulting	O
density	B
model	O
is	O
very	O
spiky	O
with	O
a	O
lot	O
of	O
structure	O
that	O
is	O
not	O
present	O
in	O
the	O
underlying	O
distribution	O
that	O
generated	O
the	O
data	O
set	O
conversely	O
if	O
is	O
too	O
large	O
figure	O
then	O
the	O
result	O
is	O
a	O
model	O
that	O
is	O
too	O
smooth	O
and	O
that	O
consequently	O
fails	O
to	O
capture	O
the	O
bimodal	O
property	O
of	O
the	O
green	O
curve	O
the	O
best	O
results	O
are	O
obtained	O
for	O
some	O
intermediate	O
value	O
of	O
figure	O
in	O
principle	O
a	O
histogram	O
density	B
model	O
is	O
also	O
dependent	O
on	O
the	O
choice	O
of	O
edge	B
location	O
for	O
the	O
bins	O
though	O
this	O
is	O
typically	O
much	O
less	O
significant	O
than	O
the	O
value	O
of	O
note	O
that	O
the	O
histogram	O
method	O
has	O
the	O
property	O
the	O
methods	O
to	O
be	O
discussed	O
shortly	O
that	O
once	O
the	O
histogram	O
has	O
been	O
computed	O
the	O
data	O
set	O
itself	O
can	O
be	O
discarded	O
which	O
can	O
be	O
advantageous	O
if	O
the	O
data	O
set	O
is	O
large	O
also	O
the	O
histogram	O
approach	O
is	O
easily	O
applied	O
if	O
the	O
data	O
points	O
are	O
arriving	O
sequentially	O
in	O
practice	O
the	O
histogram	O
technique	O
can	O
be	O
useful	O
for	O
obtaining	O
a	O
quick	O
visualization	B
of	O
data	O
in	O
one	O
or	O
two	O
dimensions	O
but	O
is	O
unsuited	O
to	O
most	O
density	B
estimation	I
applications	O
one	O
obvious	O
problem	O
is	O
that	O
the	O
estimated	O
density	B
has	O
discontinuities	O
that	O
are	O
due	O
to	O
the	O
bin	O
edges	O
rather	O
than	O
any	O
property	O
of	O
the	O
underlying	O
distribution	O
that	O
generated	O
the	O
data	O
another	O
major	O
limitation	O
of	O
the	O
histogram	O
approach	O
is	O
its	O
scaling	O
with	O
dimensionality	O
if	O
we	O
divide	O
each	O
variable	O
in	O
a	O
d-dimensional	O
space	O
into	O
m	O
bins	O
then	O
the	O
total	O
number	O
of	O
bins	O
will	O
be	O
m	O
d	O
this	O
exponential	O
scaling	O
with	O
d	O
is	O
an	O
example	O
of	O
the	O
curse	B
of	I
dimensionality	I
in	O
a	O
space	O
of	O
high	O
dimensionality	O
the	O
quantity	O
of	O
data	O
needed	O
to	O
provide	O
meaningful	O
estimates	O
of	O
local	B
probability	B
density	B
would	O
be	O
prohibitive	O
the	O
histogram	O
approach	O
to	O
density	B
estimation	I
does	O
however	O
teach	O
us	O
two	O
important	O
lessons	O
first	O
to	O
estimate	O
the	O
probability	B
density	B
at	O
a	O
particular	O
location	O
we	O
should	O
consider	O
the	O
data	O
points	O
that	O
lie	O
within	O
some	O
local	B
neighbourhood	O
of	O
that	O
point	O
note	O
that	O
the	O
concept	O
of	O
locality	O
requires	O
that	O
we	O
assume	O
some	O
form	O
of	O
distance	O
measure	O
and	O
here	O
we	O
have	O
been	O
assuming	O
euclidean	O
distance	O
for	O
histograms	O
section	O
probability	B
distributions	O
this	O
neighbourhood	O
property	O
was	O
defined	O
by	O
the	O
bins	O
and	O
there	O
is	O
a	O
natural	O
smoothing	B
parameter	I
describing	O
the	O
spatial	O
extent	O
of	O
the	O
local	B
region	O
in	O
this	O
case	O
the	O
bin	O
width	O
second	O
the	O
value	O
of	O
the	O
smoothing	B
parameter	I
should	O
be	O
neither	O
too	O
large	O
nor	O
too	O
small	O
in	O
order	O
to	O
obtain	O
good	O
results	O
this	O
is	O
reminiscent	O
of	O
the	O
choice	O
of	O
model	O
complexity	O
in	O
polynomial	B
curve	B
fitting	I
discussed	O
in	O
chapter	O
where	O
the	O
degree	O
m	O
of	O
the	O
polynomial	O
or	O
alternatively	O
the	O
value	O
of	O
the	O
regularization	B
parameter	O
was	O
optimal	O
for	O
some	O
intermediate	O
value	O
neither	O
too	O
large	O
nor	O
too	O
small	O
armed	O
with	O
these	O
insights	O
we	O
turn	O
now	O
to	O
a	O
discussion	O
of	O
two	O
widely	O
used	O
nonparametric	O
techniques	O
for	O
density	B
estimation	I
kernel	O
estimators	O
and	O
nearest	O
neighbours	O
which	O
have	O
better	O
scaling	O
with	O
dimensionality	O
than	O
the	O
simple	O
histogram	O
model	O
kernel	O
density	B
estimators	O
let	O
us	O
suppose	O
that	O
observations	O
are	O
being	O
drawn	O
from	O
some	O
unknown	O
probability	B
density	B
px	O
in	O
some	O
d-dimensional	O
space	O
which	O
we	O
shall	O
take	O
to	O
be	O
euclidean	O
and	O
we	O
wish	O
to	O
estimate	O
the	O
value	O
of	O
px	O
from	O
our	O
earlier	O
discussion	O
of	O
locality	O
let	O
us	O
consider	O
some	O
small	O
region	O
r	O
containing	O
x	O
the	O
probability	B
mass	O
associated	O
with	O
this	O
region	O
is	O
given	O
by	O
p	O
r	O
px	O
dx	O
section	O
now	O
suppose	O
that	O
we	O
have	O
collected	O
a	O
data	O
set	O
comprising	O
n	O
observations	O
drawn	O
from	O
px	O
because	O
each	O
data	O
point	O
has	O
a	O
probability	B
p	O
of	O
falling	O
within	O
r	O
the	O
total	O
number	O
k	O
of	O
points	O
that	O
lie	O
inside	O
r	O
will	O
be	O
distributed	O
according	O
to	O
the	O
binomial	B
distribution	I
binkn	O
p	O
n	O
k	O
n	O
k	O
p	O
p	O
k	O
using	O
we	O
see	O
that	O
the	O
mean	B
fraction	O
of	O
points	O
falling	O
inside	O
the	O
region	O
is	O
ekn	O
p	O
and	O
similarly	O
using	O
we	O
see	O
that	O
the	O
variance	B
around	O
this	O
mean	B
is	O
varkn	O
p	O
p	O
for	O
large	O
n	O
this	O
distribution	O
will	O
be	O
sharply	O
peaked	O
around	O
the	O
mean	B
and	O
so	O
if	O
however	O
we	O
also	O
assume	O
that	O
the	O
region	O
r	O
is	O
sufficiently	O
small	O
that	O
the	O
probability	B
density	B
px	O
is	O
roughly	O
constant	O
over	O
the	O
region	O
then	O
we	O
have	O
k	O
n	O
p	O
where	O
v	O
is	O
the	O
volume	O
of	O
r	O
combining	O
and	O
we	O
obtain	O
our	O
density	B
estimate	O
in	O
the	O
form	O
p	O
pxv	O
px	O
k	O
n	O
v	O
note	O
that	O
the	O
validity	O
of	O
depends	O
on	O
two	O
contradictory	O
assumptions	O
namely	O
that	O
the	O
region	O
r	O
be	O
sufficiently	O
small	O
that	O
the	O
density	B
is	O
approximately	O
constant	O
over	O
the	O
region	O
and	O
yet	O
sufficiently	O
large	O
relation	O
to	O
the	O
value	O
of	O
that	O
density	B
that	O
the	O
number	O
k	O
of	O
points	O
falling	O
inside	O
the	O
region	O
is	O
sufficient	O
for	O
the	O
binomial	B
distribution	I
to	O
be	O
sharply	O
peaked	O
nonparametric	B
methods	I
we	O
can	O
exploit	O
the	O
result	O
in	O
two	O
different	O
ways	O
either	O
we	O
can	O
fix	O
k	O
and	O
determine	O
the	O
value	O
of	O
v	O
from	O
the	O
data	O
which	O
gives	O
rise	O
to	O
the	O
k-nearest-neighbour	O
technique	O
discussed	O
shortly	O
or	O
we	O
can	O
fix	O
v	O
and	O
determine	O
k	O
from	O
the	O
data	O
giving	O
rise	O
to	O
the	O
kernel	O
approach	O
it	O
can	O
be	O
shown	O
that	O
both	O
the	O
k-nearest-neighbour	O
density	B
estimator	O
and	O
the	O
kernel	B
density	B
estimator	I
converge	O
to	O
the	O
true	O
probability	B
density	B
in	O
the	O
limit	O
n	O
provided	O
v	O
shrinks	O
suitably	O
with	O
n	O
and	O
k	O
grows	O
with	O
n	O
and	O
hart	O
we	O
begin	O
by	O
discussing	O
the	O
kernel	O
method	O
in	O
detail	O
and	O
to	O
start	O
with	O
we	O
take	O
the	O
region	O
r	O
to	O
be	O
a	O
small	O
hypercube	O
centred	O
on	O
the	O
point	O
x	O
at	O
which	O
we	O
wish	O
to	O
determine	O
the	O
probability	B
density	B
in	O
order	O
to	O
count	O
the	O
number	O
k	O
of	O
points	O
falling	O
within	O
this	O
region	O
it	O
is	O
convenient	O
to	O
define	O
the	O
following	O
function	O
ku	O
otherwise	O
i	O
d	O
which	O
represents	O
a	O
unit	O
cube	O
centred	O
on	O
the	O
origin	O
the	O
function	O
ku	O
is	O
an	O
example	O
of	O
a	O
kernel	B
function	I
and	O
in	O
this	O
context	O
is	O
also	O
called	O
a	O
parzen	B
window	I
from	O
the	O
quantity	O
kx	O
xnh	O
will	O
be	O
one	O
if	O
the	O
data	O
point	O
xn	O
lies	O
inside	O
a	O
cube	O
of	O
side	O
h	O
centred	O
on	O
x	O
and	O
zero	O
otherwise	O
the	O
total	O
number	O
of	O
data	O
points	O
lying	O
inside	O
this	O
cube	O
will	O
therefore	O
be	O
x	O
xn	O
k	O
k	O
substituting	O
this	O
expression	O
into	O
then	O
gives	O
the	O
following	O
result	O
for	O
the	O
estimated	O
density	B
at	O
x	O
h	O
px	O
n	O
hd	O
k	O
x	O
xn	O
h	O
where	O
we	O
have	O
used	O
v	O
hd	O
for	O
the	O
volume	O
of	O
a	O
hypercube	O
of	O
side	O
h	O
in	O
d	O
dimensions	O
using	O
the	O
symmetry	O
of	O
the	O
function	O
ku	O
we	O
can	O
now	O
re-interpret	O
this	O
equation	O
not	O
as	O
a	O
single	O
cube	O
centred	O
on	O
x	O
but	O
as	O
the	O
sum	O
over	O
n	O
cubes	O
centred	O
on	O
the	O
n	O
data	O
points	O
xn	O
as	O
it	O
stands	O
the	O
kernel	B
density	B
estimator	I
will	O
suffer	O
from	O
one	O
of	O
the	O
same	O
problems	O
that	O
the	O
histogram	O
method	O
suffered	O
from	O
namely	O
the	O
presence	O
of	O
artificial	O
discontinuities	O
in	O
this	O
case	O
at	O
the	O
boundaries	O
of	O
the	O
cubes	O
we	O
can	O
obtain	O
a	O
smoother	O
density	B
model	O
if	O
we	O
choose	O
a	O
smoother	O
kernel	B
function	I
and	O
a	O
common	O
choice	O
is	O
the	O
gaussian	B
which	O
gives	O
rise	O
to	O
the	O
following	O
kernel	O
density	B
model	O
px	O
n	O
exp	O
where	O
h	O
represents	O
the	O
standard	B
deviation	I
of	O
the	O
gaussian	B
components	O
thus	O
our	O
density	B
model	O
is	O
obtained	O
by	O
placing	O
a	O
gaussian	B
over	O
each	O
data	O
point	O
and	O
then	O
adding	O
up	O
the	O
contributions	O
over	O
the	O
whole	O
data	O
set	O
and	O
then	O
dividing	O
by	O
n	O
so	O
that	O
the	O
density	B
is	O
correctly	O
normalized	O
in	O
figure	O
we	O
apply	O
the	O
model	O
to	O
the	O
data	O
probability	B
distributions	O
figure	O
illustration	O
of	O
the	O
kernel	O
density	B
model	O
applied	O
to	O
the	O
same	O
data	O
set	O
used	O
to	O
demonstrate	O
the	O
histogram	O
approach	O
in	O
figure	O
we	O
see	O
that	O
h	O
acts	O
as	O
a	O
smoothing	B
parameter	I
and	O
that	O
if	O
it	O
is	O
set	O
too	O
small	O
panel	O
the	O
result	O
is	O
a	O
very	O
noisy	O
density	B
model	O
whereas	O
if	O
it	O
is	O
set	O
too	O
large	O
panel	O
then	O
the	O
bimodal	O
nature	O
of	O
the	O
underlying	O
distribution	O
from	O
which	O
the	O
data	O
is	O
generated	O
by	O
the	O
green	O
curve	O
is	O
washed	O
out	O
the	O
best	O
density	B
model	O
is	O
obtained	O
for	O
some	O
intermediate	O
value	O
of	O
h	O
panel	O
h	O
h	O
h	O
set	O
used	O
earlier	O
to	O
demonstrate	O
the	O
histogram	O
technique	O
we	O
see	O
that	O
as	O
expected	O
the	O
parameter	O
h	O
plays	O
the	O
role	O
of	O
a	O
smoothing	B
parameter	I
and	O
there	O
is	O
a	O
trade-off	O
between	O
sensitivity	O
to	O
noise	O
at	O
small	O
h	O
and	O
over-smoothing	O
at	O
large	O
h	O
again	O
the	O
optimization	O
of	O
h	O
is	O
a	O
problem	O
in	O
model	O
complexity	O
analogous	O
to	O
the	O
choice	O
of	O
bin	O
width	O
in	O
histogram	B
density	B
estimation	I
or	O
the	O
degree	O
of	O
the	O
polynomial	O
used	O
in	O
curve	B
fitting	I
we	O
can	O
choose	O
any	O
other	O
kernel	B
function	I
ku	O
in	O
subject	O
to	O
the	O
condi	O
tions	O
ku	O
ku	O
du	O
which	O
ensure	O
that	O
the	O
resulting	O
probability	B
distribution	O
is	O
nonnegative	O
everywhere	O
and	O
integrates	O
to	O
one	O
the	O
class	O
of	O
density	B
model	O
given	O
by	O
is	O
called	O
a	O
kernel	B
density	B
estimator	I
or	O
parzen	O
estimator	O
it	O
has	O
a	O
great	O
merit	O
that	O
there	O
is	O
no	O
computation	O
involved	O
in	O
the	O
training	B
phase	O
because	O
this	O
simply	O
requires	O
storage	O
of	O
the	O
training	B
set	I
however	O
this	O
is	O
also	O
one	O
of	O
its	O
great	O
weaknesses	O
because	O
the	O
computational	O
cost	O
of	O
evaluating	O
the	O
density	B
grows	O
linearly	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
nearest-neighbour	B
methods	I
one	O
of	O
the	O
difficulties	O
with	O
the	O
kernel	O
approach	O
to	O
density	B
estimation	I
is	O
that	O
the	O
parameter	O
h	O
governing	O
the	O
kernel	O
width	O
is	O
fixed	O
for	O
all	O
kernels	O
in	O
regions	O
of	O
high	O
data	O
density	B
a	O
large	O
value	O
of	O
h	O
may	O
lead	O
to	O
over-smoothing	O
and	O
a	O
washing	O
out	O
of	O
structure	O
that	O
might	O
otherwise	O
be	O
extracted	O
from	O
the	O
data	O
however	O
reducing	O
h	O
may	O
lead	O
to	O
noisy	O
estimates	O
elsewhere	O
in	O
data	O
space	O
where	O
the	O
density	B
is	O
smaller	O
thus	O
the	O
optimal	O
choice	O
for	O
h	O
may	O
be	O
dependent	O
on	O
location	O
within	O
the	O
data	O
space	O
this	O
issue	O
is	O
addressed	O
by	O
nearest-neighbour	B
methods	I
for	O
density	B
estimation	I
we	O
therefore	O
return	O
to	O
our	O
general	O
result	O
for	O
local	B
density	B
estimation	I
and	O
instead	O
of	O
fixing	O
v	O
and	O
determining	O
the	O
value	O
of	O
k	O
from	O
the	O
data	O
we	O
consider	O
a	O
fixed	O
value	O
of	O
k	O
and	O
use	O
the	O
data	O
to	O
find	O
an	O
appropriate	O
value	O
for	O
v	O
to	O
do	O
this	O
we	O
consider	O
a	O
small	O
sphere	O
centred	O
on	O
the	O
point	O
x	O
at	O
which	O
we	O
wish	O
to	O
estimate	O
the	O
nonparametric	B
methods	I
figure	O
illustration	O
of	O
k-nearest-neighbour	O
density	B
estimation	I
using	O
the	O
same	O
data	O
set	O
as	O
in	O
figures	O
and	O
we	O
see	O
that	O
the	O
parameter	O
k	O
governs	O
the	O
degree	O
of	O
smoothing	O
so	O
that	O
a	O
small	O
value	O
of	O
k	O
leads	O
to	O
a	O
very	O
noisy	O
density	B
model	O
panel	O
whereas	O
a	O
large	O
value	O
panel	O
smoothes	O
out	O
the	O
bimodal	O
nature	O
of	O
the	O
true	O
distribution	O
by	O
the	O
green	O
curve	O
from	O
which	O
the	O
data	O
set	O
was	O
generated	O
k	O
k	O
k	O
exercise	O
density	B
px	O
and	O
we	O
allow	O
the	O
radius	O
of	O
the	O
sphere	O
to	O
grow	O
until	O
it	O
contains	O
precisely	O
k	O
data	O
points	O
the	O
estimate	O
of	O
the	O
density	B
px	O
is	O
then	O
given	O
by	O
with	O
v	O
set	O
to	O
the	O
volume	O
of	O
the	O
resulting	O
sphere	O
this	O
technique	O
is	O
known	O
as	O
k	B
nearest	I
neighbours	I
and	O
is	O
illustrated	O
in	O
figure	O
for	O
various	O
choices	O
of	O
the	O
parameter	O
k	O
using	O
the	O
same	O
data	O
set	O
as	O
used	O
in	O
figure	O
and	O
figure	O
we	O
see	O
that	O
the	O
value	O
of	O
k	O
now	O
governs	O
the	O
degree	O
of	O
smoothing	O
and	O
that	O
again	O
there	O
is	O
an	O
optimum	O
choice	O
for	O
k	O
that	O
is	O
neither	O
too	O
large	O
nor	O
too	O
small	O
note	O
that	O
the	O
model	O
produced	O
by	O
k	B
nearest	I
neighbours	I
is	O
not	O
a	O
true	O
density	B
model	O
because	O
the	O
integral	O
over	O
all	O
space	O
diverges	O
we	O
close	O
this	O
chapter	O
by	O
showing	O
how	O
the	O
k-nearest-neighbour	O
technique	O
for	O
density	B
estimation	I
can	O
be	O
extended	B
to	O
the	O
problem	O
of	O
classification	B
to	O
do	O
this	O
we	O
apply	O
the	O
k-nearest-neighbour	O
density	B
estimation	I
technique	O
to	O
each	O
class	O
separately	O
and	O
then	O
make	O
use	O
of	O
bayes	B
theorem	O
let	O
us	O
suppose	O
that	O
we	O
have	O
a	O
data	O
set	O
comprising	O
nk	O
points	O
in	O
class	O
ck	O
with	O
n	O
points	O
in	O
total	O
so	O
that	O
k	O
nk	O
n	O
if	O
we	O
wish	O
to	O
classify	O
a	O
new	O
point	O
x	O
we	O
draw	O
a	O
sphere	O
centred	O
on	O
x	O
containing	O
precisely	O
k	O
points	O
irrespective	O
of	O
their	O
class	O
suppose	O
this	O
sphere	O
has	O
volume	O
v	O
and	O
contains	O
kk	O
points	O
from	O
class	O
ck	O
then	O
provides	O
an	O
estimate	O
of	O
the	O
density	B
associated	O
with	O
each	O
class	O
pxck	O
kk	O
nkv	O
similarly	O
the	O
unconditional	O
density	B
is	O
given	O
by	O
px	O
k	O
n	O
v	O
while	O
the	O
class	O
priors	O
are	O
given	O
by	O
pck	O
nk	O
n	O
we	O
can	O
now	O
combine	O
and	O
using	O
bayes	B
theorem	O
to	O
obtain	O
the	O
posterior	B
probability	B
of	O
class	O
membership	O
pckx	O
pxckpck	O
px	O
kk	O
k	O
probability	B
distributions	O
figure	O
in	O
the	O
k-nearestneighbour	O
classifier	O
a	O
new	O
point	O
shown	O
by	O
the	O
black	O
diamond	O
is	O
classified	O
according	O
to	O
the	O
majority	O
class	O
membership	O
of	O
the	O
k	O
closest	O
training	B
data	O
points	O
in	O
this	O
case	O
k	O
in	O
the	O
nearest-neighbour	O
approach	O
to	O
classification	B
the	O
resulting	O
decision	B
boundary	I
is	O
composed	O
of	O
hyperplanes	O
that	O
form	O
perpendicular	O
bisectors	O
of	O
pairs	O
of	O
points	O
from	O
different	O
classes	O
if	O
we	O
wish	O
to	O
minimize	O
the	O
probability	B
of	O
misclassification	O
this	O
is	O
done	O
by	O
assigning	O
the	O
test	O
point	O
x	O
to	O
the	O
class	O
having	O
the	O
largest	O
posterior	B
probability	B
corresponding	O
to	O
the	O
largest	O
value	O
of	O
kkk	O
thus	O
to	O
classify	O
a	O
new	O
point	O
we	O
identify	O
the	O
k	O
nearest	O
points	O
from	O
the	O
training	B
data	O
set	O
and	O
then	O
assign	O
the	O
new	O
point	O
to	O
the	O
class	O
having	O
the	O
largest	O
number	O
of	O
representatives	O
amongst	O
this	O
set	O
ties	O
can	O
be	O
broken	O
at	O
random	O
the	O
particular	O
case	O
of	O
k	O
is	O
called	O
the	O
nearest-neighbour	O
rule	O
because	O
a	O
test	O
point	O
is	O
simply	O
assigned	O
to	O
the	O
same	O
class	O
as	O
the	O
nearest	O
point	O
from	O
the	O
training	B
set	I
these	O
concepts	O
are	O
illustrated	O
in	O
figure	O
in	O
figure	O
we	O
show	O
the	O
results	O
of	O
applying	O
the	O
k-nearest-neighbour	O
algorithm	O
to	O
the	O
oil	O
flow	O
data	O
introduced	O
in	O
chapter	O
for	O
various	O
values	O
of	O
k	O
as	O
expected	O
we	O
see	O
that	O
k	O
controls	O
the	O
degree	O
of	O
smoothing	O
so	O
that	O
small	O
k	O
produces	O
many	O
small	O
regions	O
of	O
each	O
class	O
whereas	O
large	O
k	O
leads	O
to	O
fewer	O
larger	O
regions	O
k	O
k	O
k	O
figure	O
plot	O
of	O
data	O
points	O
from	O
the	O
oil	O
data	O
set	O
showing	O
values	O
of	O
plotted	O
against	O
where	O
the	O
red	O
green	O
and	O
blue	O
points	O
correspond	O
to	O
the	O
laminar	O
annular	O
and	O
homogeneous	B
classes	O
respectively	O
also	O
shown	O
are	O
the	O
classifications	O
of	O
the	O
input	O
space	O
given	O
by	O
the	O
k-nearest-neighbour	O
algorithm	O
for	O
various	O
values	O
of	O
k	O
exercises	O
an	O
interesting	O
property	O
of	O
the	O
nearest-neighbour	O
classifier	O
is	O
that	O
in	O
the	O
limit	O
n	O
the	O
error	B
rate	O
is	O
never	O
more	O
than	O
twice	O
the	O
minimum	O
achievable	O
error	B
rate	O
of	O
an	O
optimal	O
classifier	O
i	O
e	O
one	O
that	O
uses	O
the	O
true	O
class	O
distributions	O
and	O
hart	O
as	O
discussed	O
so	O
far	O
both	O
the	O
k-nearest-neighbour	O
method	O
and	O
the	O
kernel	B
density	B
estimator	I
require	O
the	O
entire	O
training	B
data	O
set	O
to	O
be	O
stored	O
leading	O
to	O
expensive	O
computation	O
if	O
the	O
data	O
set	O
is	O
large	O
this	O
effect	O
can	O
be	O
offset	O
at	O
the	O
expense	O
of	O
some	O
additional	O
one-off	O
computation	O
by	O
constructing	O
tree-based	O
search	O
structures	O
to	O
allow	O
near	O
neighbours	O
to	O
be	O
found	O
efficiently	O
without	O
doing	O
an	O
exhaustive	O
search	O
of	O
the	O
data	O
set	O
nevertheless	O
these	O
nonparametric	B
methods	I
are	O
still	O
severely	O
limited	O
on	O
the	O
other	O
hand	O
we	O
have	O
seen	O
that	O
simple	O
parametric	O
models	O
are	O
very	O
restricted	O
in	O
terms	O
of	O
the	O
forms	O
of	O
distribution	O
that	O
they	O
can	O
represent	O
we	O
therefore	O
need	O
to	O
find	O
density	B
models	O
that	O
are	O
very	O
flexible	O
and	O
yet	O
for	O
which	O
the	O
complexity	O
of	O
the	O
models	O
can	O
be	O
controlled	O
independently	O
of	O
the	O
size	O
of	O
the	O
training	B
set	I
and	O
we	O
shall	O
see	O
in	O
subsequent	O
chapters	O
how	O
to	O
achieve	O
this	O
exercises	O
erties	O
www	O
verify	O
that	O
the	O
bernoulli	B
distribution	I
satisfies	O
the	O
following	O
prop	O
px	O
ex	O
varx	O
show	O
that	O
the	O
entropy	B
hx	O
of	O
a	O
bernoulli	B
distributed	O
random	O
binary	O
variable	O
x	O
is	O
given	O
by	O
hx	O
ln	O
the	O
form	O
of	O
the	O
bernoulli	B
distribution	I
given	O
by	O
is	O
not	O
symmetric	O
between	O
the	O
two	O
values	O
of	O
x	O
in	O
some	O
situations	O
it	O
will	O
be	O
more	O
convenient	O
to	O
use	O
an	O
equivalent	O
formulation	O
for	O
which	O
x	O
in	O
which	O
case	O
the	O
distribution	O
can	O
be	O
written	O
where	O
show	O
that	O
the	O
distribution	O
is	O
normalized	O
and	O
evaluate	O
its	O
mean	B
variance	B
and	O
entropy	B
px	O
www	O
in	O
this	O
exercise	O
we	O
prove	O
that	O
the	O
binomial	B
distribution	I
is	O
normalized	O
first	O
use	O
the	O
definition	O
of	O
the	O
number	O
of	O
combinations	O
of	O
m	O
identical	O
objects	O
chosen	O
from	O
a	O
total	O
of	O
n	O
to	O
show	O
that	O
n	O
m	O
n	O
m	O
n	O
m	O
probability	B
distributions	O
use	O
this	O
result	O
to	O
prove	O
by	O
induction	O
the	O
following	O
result	O
n	O
m	O
xn	O
xm	O
which	O
is	O
known	O
as	O
the	O
binomial	O
theorem	O
and	O
which	O
is	O
valid	O
for	O
all	O
real	O
values	O
of	O
x	O
finally	O
show	O
that	O
the	O
binomial	B
distribution	I
is	O
normalized	O
so	O
that	O
m	O
n	O
m	O
which	O
can	O
be	O
done	O
by	O
first	O
pulling	O
out	O
a	O
factor	O
out	O
of	O
the	O
summation	O
and	O
then	O
making	O
use	O
of	O
the	O
binomial	O
theorem	O
show	O
that	O
the	O
mean	B
of	O
the	O
binomial	B
distribution	I
is	O
given	O
by	O
to	O
do	O
this	O
differentiate	O
both	O
sides	O
of	O
the	O
normalization	O
condition	O
with	O
respect	O
to	O
and	O
then	O
rearrange	O
to	O
obtain	O
an	O
expression	O
for	O
the	O
mean	B
of	O
n	O
similarly	O
by	O
differentiating	O
twice	O
with	O
respect	O
to	O
and	O
making	O
use	O
of	O
the	O
result	O
for	O
the	O
mean	B
of	O
the	O
binomial	B
distribution	I
prove	O
the	O
result	O
for	O
the	O
variance	B
of	O
the	O
binomial	O
www	O
in	O
this	O
exercise	O
we	O
prove	O
that	O
the	O
beta	B
distribution	I
given	O
by	O
is	O
correctly	O
normalized	O
so	O
that	O
holds	O
this	O
is	O
equivalent	O
to	O
showing	O
that	O
a	O
d	O
b	O
from	O
the	O
definition	O
of	O
the	O
gamma	B
function	I
we	O
have	O
exp	O
xxa	O
dx	O
exp	O
yyb	O
dy	O
use	O
this	O
expression	O
to	O
prove	O
as	O
follows	O
first	O
bring	O
the	O
integral	O
over	O
y	O
inside	O
the	O
integrand	O
of	O
the	O
integral	O
over	O
x	O
next	O
make	O
the	O
change	O
of	O
variable	O
t	O
y	O
x	O
where	O
x	O
is	O
fixed	O
then	O
interchange	O
the	O
order	O
of	O
the	O
x	O
and	O
t	O
integrations	O
and	O
finally	O
make	O
the	O
change	O
of	O
variable	O
x	O
t	O
where	O
t	O
is	O
fixed	O
make	O
use	O
of	O
the	O
result	O
to	O
show	O
that	O
the	O
mean	B
variance	B
and	O
mode	O
of	O
the	O
beta	B
distribution	I
are	O
given	O
respectively	O
by	O
e	O
var	O
mode	O
a	O
a	O
b	O
ab	O
b	O
a	O
a	O
b	O
exercises	O
consider	O
a	O
binomial	O
random	O
variable	O
x	O
given	O
by	O
with	O
prior	B
distribution	O
for	O
given	O
by	O
the	O
beta	B
distribution	I
and	O
suppose	O
we	O
have	O
observed	O
m	O
occurrences	O
of	O
x	O
and	O
l	O
occurrences	O
of	O
x	O
show	O
that	O
the	O
posterior	O
mean	B
value	O
of	O
x	O
lies	O
between	O
the	O
prior	B
mean	B
and	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
to	O
do	O
this	O
show	O
that	O
the	O
posterior	O
mean	B
can	O
be	O
written	O
as	O
times	O
the	O
prior	B
mean	B
plus	O
times	O
the	O
maximum	B
likelihood	I
estimate	O
where	O
this	O
illustrates	O
the	O
concept	O
of	O
the	O
posterior	O
distribution	O
being	O
a	O
compromise	O
between	O
the	O
prior	B
distribution	O
and	O
the	O
maximum	B
likelihood	I
solution	O
consider	O
two	O
variables	O
x	O
and	O
y	O
with	O
joint	O
distribution	O
px	O
y	O
prove	O
the	O
follow	O
ing	O
two	O
results	O
ex	O
ey	O
varx	O
ey	O
vary	O
here	O
exxy	O
denotes	O
the	O
expectation	B
of	O
x	O
under	O
the	O
conditional	B
distribution	O
pxy	O
with	O
a	O
similar	O
notation	O
for	O
the	O
conditional	B
variance	B
www	O
in	O
this	O
exercise	O
we	O
prove	O
the	O
normalization	O
of	O
the	O
dirichlet	B
distribution	I
using	O
induction	O
we	O
have	O
already	O
shown	O
in	O
exercise	O
that	O
the	O
beta	B
distribution	I
which	O
is	O
a	O
special	O
case	O
of	O
the	O
dirichlet	B
for	O
m	O
is	O
normalized	O
we	O
now	O
assume	O
that	O
the	O
dirichlet	B
distribution	I
is	O
normalized	O
for	O
m	O
variables	O
and	O
prove	O
that	O
it	O
is	O
normalized	O
for	O
m	O
variables	O
to	O
do	O
this	O
consider	O
the	O
dirichlet	B
k	O
by	O
distribution	O
over	O
m	O
variables	O
and	O
take	O
account	O
of	O
the	O
constraint	O
eliminating	O
m	O
so	O
that	O
the	O
dirichlet	B
is	O
written	O
pm	O
m	O
cm	O
j	O
and	O
our	O
goal	O
is	O
to	O
find	O
an	O
expression	O
for	O
cm	O
to	O
do	O
this	O
integrate	O
over	O
m	O
taking	O
care	O
over	O
the	O
limits	O
of	O
integration	O
and	O
then	O
make	O
a	O
change	O
of	O
variable	O
so	O
that	O
this	O
integral	O
has	O
limits	O
and	O
by	O
assuming	O
the	O
correct	O
result	O
for	O
cm	O
and	O
making	O
use	O
of	O
derive	O
the	O
expression	O
for	O
cm	O
using	O
the	O
property	O
x	O
of	O
the	O
gamma	B
function	I
derive	O
the	O
following	O
results	O
for	O
the	O
mean	B
variance	B
and	O
covariance	B
of	O
the	O
dirichlet	B
distribution	I
given	O
by	O
e	O
j	O
j	O
var	O
j	O
j	O
j	O
cov	O
j	O
l	O
j	O
l	O
j	O
l	O
where	O
is	O
defined	O
by	O
m	O
m	O
k	O
k	O
m	O
probability	B
distributions	O
www	O
by	O
expressing	O
the	O
expectation	B
of	O
ln	O
j	O
under	O
the	O
dirichlet	B
distribution	I
as	O
a	O
derivative	B
with	O
respect	O
to	O
j	O
show	O
that	O
eln	O
j	O
j	O
where	O
is	O
given	O
by	O
and	O
d	O
da	O
ln	O
is	O
the	O
digamma	B
function	I
the	O
uniform	B
distribution	I
for	O
a	O
continuous	O
variable	O
x	O
is	O
defined	O
by	O
uxa	O
b	O
b	O
a	O
a	O
x	O
b	O
verify	O
that	O
this	O
distribution	O
is	O
normalized	O
and	O
find	O
expressions	O
for	O
its	O
mean	B
and	O
variance	B
evaluate	O
the	O
kullback-leibler	B
divergence	I
between	O
two	O
gaussians	O
px	O
n	O
and	O
qx	O
n	O
l	O
www	O
this	O
exercise	O
demonstrates	O
that	O
the	O
multivariate	O
distribution	O
with	O
maximum	O
entropy	B
for	O
a	O
given	O
covariance	B
is	O
a	O
gaussian	B
the	O
entropy	B
of	O
a	O
distribution	O
px	O
is	O
given	O
by	O
hx	O
px	O
ln	O
px	O
dx	O
we	O
wish	O
to	O
maximize	O
hx	O
over	O
all	O
distributions	O
px	O
subject	O
to	O
the	O
constraints	O
that	O
px	O
be	O
normalized	O
and	O
that	O
it	O
have	O
a	O
specific	O
mean	B
and	O
covariance	B
so	O
that	O
px	O
dx	O
pxx	O
dx	O
pxx	O
dx	O
by	O
performing	O
a	O
variational	B
maximization	O
of	O
and	O
using	O
lagrange	B
multipliers	O
to	O
enforce	O
the	O
constraints	O
and	O
show	O
that	O
the	O
maximum	B
likelihood	I
distribution	O
is	O
given	O
by	O
the	O
gaussian	B
show	O
that	O
the	O
entropy	B
of	O
the	O
multivariate	O
gaussian	B
n	O
is	O
given	O
by	O
hx	O
ln	O
d	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
exercises	O
www	O
consider	O
two	O
random	O
variables	O
and	O
having	O
gaussian	B
distributions	O
with	O
means	O
and	O
precisions	O
respectively	O
derive	O
an	O
expression	O
for	O
the	O
differential	B
entropy	B
of	O
the	O
variable	O
x	O
to	O
do	O
this	O
first	O
find	O
the	O
distribution	O
of	O
x	O
by	O
using	O
the	O
relation	O
px	O
and	O
completing	B
the	I
square	I
in	O
the	O
exponent	O
then	O
observe	O
that	O
this	O
represents	O
the	O
convolution	O
of	O
two	O
gaussian	B
distributions	O
which	O
itself	O
will	O
be	O
gaussian	B
and	O
finally	O
make	O
use	O
of	O
the	O
result	O
for	O
the	O
entropy	B
of	O
the	O
univariate	O
gaussian	B
www	O
consider	O
the	O
multivariate	O
gaussian	B
distribution	O
given	O
by	O
by	O
as	O
the	O
sum	O
of	O
a	O
symwriting	O
the	O
precision	B
matrix	I
covariance	B
matrix	O
metric	O
and	O
an	O
anti-symmetric	O
matrix	O
show	O
that	O
the	O
anti-symmetric	O
term	O
does	O
not	O
appear	O
in	O
the	O
exponent	O
of	O
the	O
gaussian	B
and	O
hence	O
that	O
the	O
precision	B
matrix	I
may	O
be	O
taken	O
to	O
be	O
symmetric	O
without	O
loss	O
of	O
generality	O
because	O
the	O
inverse	B
of	O
a	O
symmetric	O
matrix	O
is	O
also	O
symmetric	O
exercise	O
it	O
follows	O
that	O
the	O
covariance	B
matrix	O
may	O
also	O
be	O
chosen	O
to	O
be	O
symmetric	O
without	O
loss	O
of	O
generality	O
consider	O
a	O
real	O
symmetric	O
matrix	O
whose	O
eigenvalue	O
equation	O
is	O
given	O
by	O
by	O
taking	O
the	O
complex	O
conjugate	B
of	O
this	O
equation	O
and	O
subtracting	O
the	O
original	O
equation	O
and	O
then	O
forming	O
the	O
inner	O
product	O
with	O
eigenvector	O
ui	O
show	O
that	O
the	O
eigenvalues	O
i	O
are	O
real	O
similarly	O
use	O
the	O
symmetry	O
property	O
of	O
to	O
show	O
that	O
two	O
eigenvectors	O
ui	O
and	O
uj	O
will	O
be	O
orthogonal	O
provided	O
j	O
i	O
finally	O
show	O
that	O
without	O
loss	O
of	O
generality	O
the	O
set	O
of	O
eigenvectors	O
can	O
be	O
chosen	O
to	O
be	O
orthonormal	O
so	O
that	O
they	O
satisfy	O
even	O
if	O
some	O
of	O
the	O
eigenvalues	O
are	O
zero	O
show	O
that	O
a	O
real	O
symmetric	O
matrix	O
having	O
the	O
eigenvector	O
equation	O
can	O
be	O
expressed	O
as	O
an	O
expansion	O
in	O
the	O
eigenvectors	O
with	O
coefficients	O
given	O
by	O
the	O
has	O
a	O
eigenvalues	O
of	O
the	O
form	O
similarly	O
show	O
that	O
the	O
inverse	B
matrix	O
representation	O
of	O
the	O
form	O
www	O
a	O
positive	B
definite	I
matrix	I
can	O
be	O
defined	O
as	O
one	O
for	O
which	O
the	O
quadratic	O
form	O
is	O
positive	O
for	O
any	O
real	O
value	O
of	O
the	O
vector	O
a	O
show	O
that	O
a	O
necessary	O
and	O
sufficient	O
condition	O
for	O
to	O
be	O
positive	B
definite	I
is	O
that	O
all	O
of	O
the	O
eigenvalues	O
i	O
of	O
defined	O
by	O
are	O
positive	O
at	O
a	O
show	O
that	O
a	O
real	O
symmetric	O
matrix	O
of	O
size	O
d	O
d	O
has	O
dd	O
independent	B
parameters	O
www	O
show	O
that	O
the	O
inverse	B
of	O
a	O
symmetric	O
matrix	O
is	O
itself	O
symmetric	O
by	O
diagonalizing	O
the	O
coordinate	O
system	O
using	O
the	O
eigenvector	O
expansion	O
show	O
that	O
the	O
volume	O
contained	O
within	O
the	O
hyperellipsoid	O
corresponding	O
to	O
a	O
constant	O
probability	B
distributions	O
mahalanobis	B
distance	I
is	O
given	O
by	O
vd	O
d	O
where	O
vd	O
is	O
the	O
volume	O
of	O
the	O
unit	O
sphere	O
in	O
d	O
dimensions	O
and	O
the	O
mahalanobis	B
distance	I
is	O
defined	O
by	O
www	O
prove	O
the	O
identity	O
by	O
multiplying	O
both	O
sides	O
by	O
the	O
matrix	O
a	O
b	O
c	O
d	O
and	O
making	O
use	O
of	O
the	O
definition	O
in	O
sections	O
and	O
we	O
considered	O
the	O
conditional	B
and	O
marginal	B
distributions	O
for	O
a	O
multivariate	O
gaussian	B
more	O
generally	O
we	O
can	O
consider	O
a	O
partitioning	O
of	O
the	O
components	O
of	O
x	O
into	O
three	O
groups	O
xa	O
xb	O
and	O
xc	O
with	O
a	O
corresponding	O
partitioning	O
of	O
the	O
mean	B
vector	O
and	O
of	O
the	O
covariance	B
matrix	O
in	O
the	O
form	O
a	O
b	O
c	O
aa	O
ab	O
ac	O
ba	O
bb	O
bc	O
ca	O
cb	O
cc	O
by	O
making	O
use	O
of	O
the	O
results	O
of	O
section	O
find	O
an	O
expression	O
for	O
the	O
conditional	B
distribution	O
pxaxb	O
in	O
which	O
xc	O
has	O
been	O
marginalized	O
out	O
a	O
very	O
useful	O
result	O
from	O
linear	O
algebra	O
is	O
the	O
woodbury	O
matrix	O
inversion	O
formula	O
given	O
by	O
bcd	O
a	O
a	O
da	O
by	O
multiplying	O
both	O
sides	O
by	O
bcd	O
prove	O
the	O
correctness	O
of	O
this	O
result	O
let	O
x	O
and	O
z	O
be	O
two	O
independent	B
random	O
vectors	O
so	O
that	O
px	O
z	O
pxpz	O
show	O
that	O
the	O
mean	B
of	O
their	O
sum	O
y	O
x	O
z	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
means	O
of	O
each	O
of	O
the	O
variable	O
separately	O
similarly	O
show	O
that	O
the	O
covariance	B
matrix	O
of	O
y	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
covariance	B
matrices	O
of	O
x	O
and	O
z	O
confirm	O
that	O
this	O
result	O
agrees	O
with	O
that	O
of	O
exercise	O
www	O
consider	O
a	O
joint	O
distribution	O
over	O
the	O
variable	O
x	O
y	O
z	O
whose	O
mean	B
and	O
covariance	B
are	O
given	O
by	O
and	O
respectively	O
by	O
making	O
use	O
of	O
the	O
results	O
and	O
show	O
that	O
the	O
marginal	B
distribution	O
px	O
is	O
given	O
similarly	O
by	O
making	O
use	O
of	O
the	O
results	O
and	O
show	O
that	O
the	O
conditional	B
distribution	O
pyx	O
is	O
given	O
by	O
exercises	O
using	O
the	O
partitioned	B
matrix	O
inversion	O
formula	O
show	O
that	O
the	O
inverse	B
of	O
the	O
precision	B
matrix	I
is	O
given	O
by	O
the	O
covariance	B
matrix	O
by	O
starting	O
from	O
and	O
making	O
use	O
of	O
the	O
result	O
verify	O
the	O
result	O
consider	O
two	O
multidimensional	O
random	O
vectors	O
x	O
and	O
z	O
having	O
gaussian	B
distributions	O
px	O
n	O
x	O
x	O
and	O
pz	O
n	O
z	O
z	O
respectively	O
together	O
with	O
their	O
sum	O
y	O
xz	O
use	O
the	O
results	O
and	O
to	O
find	O
an	O
expression	O
for	O
the	O
marginal	B
distribution	O
py	O
by	O
considering	O
the	O
linear-gaussian	B
model	I
comprising	O
the	O
product	O
of	O
the	O
marginal	B
distribution	O
px	O
and	O
the	O
conditional	B
distribution	O
pyx	O
www	O
this	O
exercise	O
and	O
the	O
next	O
provide	O
practice	O
at	O
manipulating	O
the	O
quadratic	O
forms	O
that	O
arise	O
in	O
linear-gaussian	O
models	O
as	O
well	O
as	O
giving	O
an	O
independent	B
check	O
of	O
results	O
derived	O
in	O
the	O
main	O
text	O
consider	O
a	O
joint	O
distribution	O
px	O
y	O
defined	O
by	O
the	O
marginal	B
and	O
conditional	B
distributions	O
given	O
by	O
and	O
by	O
examining	O
the	O
quadratic	O
form	O
in	O
the	O
exponent	O
of	O
the	O
joint	O
distribution	O
and	O
using	O
the	O
technique	O
of	O
completing	B
the	I
square	I
discussed	O
in	O
section	O
find	O
expressions	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
marginal	B
distribution	O
py	O
in	O
which	O
the	O
variable	O
x	O
has	O
been	O
integrated	O
out	O
to	O
do	O
this	O
make	O
use	O
of	O
the	O
woodbury	O
matrix	O
inversion	O
formula	O
verify	O
that	O
these	O
results	O
agree	O
with	O
and	O
obtained	O
using	O
the	O
results	O
of	O
chapter	O
consider	O
the	O
same	O
joint	O
distribution	O
as	O
in	O
exercise	O
but	O
now	O
use	O
the	O
technique	O
of	O
completing	B
the	I
square	I
to	O
find	O
expressions	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
conditional	B
distribution	O
pxy	O
again	O
verify	O
that	O
these	O
agree	O
with	O
the	O
corresponding	O
expressions	O
and	O
www	O
to	O
find	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
covariance	B
matrix	O
of	O
a	O
multivariate	O
gaussian	B
we	O
need	O
to	O
maximize	O
the	O
log	O
likelihood	B
function	I
with	O
respect	O
to	O
noting	O
that	O
the	O
covariance	B
matrix	O
must	O
be	O
symmetric	O
and	O
positive	B
definite	I
here	O
we	O
proceed	O
by	O
ignoring	O
these	O
constraints	O
and	O
doing	O
a	O
straightforward	O
maximization	O
using	O
the	O
results	O
and	O
from	O
appendix	O
c	O
show	O
that	O
the	O
covariance	B
matrix	O
that	O
maximizes	O
the	O
log	O
likelihood	B
function	I
is	O
given	O
by	O
the	O
sample	O
covariance	B
we	O
note	O
that	O
the	O
final	O
result	O
is	O
necessarily	O
symmetric	O
and	O
positive	B
definite	I
the	O
sample	O
covariance	B
is	O
nonsingular	O
use	O
the	O
result	O
to	O
prove	O
now	O
using	O
the	O
results	O
and	O
show	O
that	O
exnxm	O
t	O
inm	O
where	O
xn	O
denotes	O
a	O
data	O
point	O
sampled	O
from	O
a	O
gaussian	B
distribution	O
with	O
mean	B
and	O
covariance	B
and	O
inm	O
denotes	O
the	O
m	O
element	O
of	O
the	O
identity	O
matrix	O
hence	O
prove	O
the	O
result	O
www	O
using	O
an	O
analogous	O
procedure	O
to	O
that	O
used	O
to	O
obtain	O
derive	O
an	O
expression	O
for	O
the	O
sequential	B
estimation	I
of	O
the	O
variance	B
of	O
a	O
univariate	O
gaussian	B
probability	B
distributions	O
distribution	O
by	O
starting	O
with	O
the	O
maximum	B
likelihood	I
expression	O
ml	O
n	O
verify	O
that	O
substituting	O
the	O
expression	O
for	O
a	O
gaussian	B
distribution	O
into	O
the	O
robbinsmonro	O
sequential	B
estimation	I
formula	O
gives	O
a	O
result	O
of	O
the	O
same	O
form	O
and	O
hence	O
obtain	O
an	O
expression	O
for	O
the	O
corresponding	O
coefficients	O
an	O
using	O
an	O
analogous	O
procedure	O
to	O
that	O
used	O
to	O
obtain	O
derive	O
an	O
expression	O
for	O
the	O
sequential	B
estimation	I
of	O
the	O
covariance	B
of	O
a	O
multivariate	O
gaussian	B
distribution	O
by	O
starting	O
with	O
the	O
maximum	B
likelihood	I
expression	O
verify	O
that	O
substituting	O
the	O
expression	O
for	O
a	O
gaussian	B
distribution	O
into	O
the	O
robbins-monro	O
sequential	B
estimation	I
formula	O
gives	O
a	O
result	O
of	O
the	O
same	O
form	O
and	O
hence	O
obtain	O
an	O
expression	O
for	O
the	O
corresponding	O
coefficients	O
an	O
use	O
the	O
technique	O
of	O
completing	B
the	I
square	I
for	O
the	O
quadratic	O
form	O
in	O
the	O
expo	O
nent	O
to	O
derive	O
the	O
results	O
and	O
starting	O
from	O
the	O
results	O
and	O
for	O
the	O
posterior	O
distribution	O
of	O
the	O
mean	B
of	O
a	O
gaussian	B
random	O
variable	O
dissect	O
out	O
the	O
contributions	O
from	O
the	O
first	O
n	O
data	O
points	O
and	O
hence	O
obtain	O
expressions	O
for	O
the	O
sequential	O
update	O
of	O
n	O
and	O
n	O
now	O
derive	O
the	O
same	O
results	O
starting	O
from	O
the	O
posterior	O
distribution	O
p	O
xn	O
n	O
n	O
n	O
and	O
multiplying	O
by	O
the	O
likelihood	B
function	I
pxn	O
n	O
and	O
then	O
completing	B
the	I
square	I
and	O
normalizing	O
to	O
obtain	O
the	O
posterior	O
distribution	O
after	O
n	O
observations	O
www	O
consider	O
a	O
d-dimensional	O
gaussian	B
random	O
variable	O
x	O
with	O
distribution	O
n	O
in	O
which	O
the	O
covariance	B
is	O
known	O
and	O
for	O
which	O
we	O
wish	O
to	O
infer	O
the	O
mean	B
from	O
a	O
set	O
of	O
observations	O
x	O
xn	O
given	O
a	O
prior	B
distribution	O
p	O
n	O
find	O
the	O
corresponding	O
posterior	O
distribution	O
p	O
use	O
the	O
definition	O
of	O
the	O
gamma	B
function	I
to	O
show	O
that	O
the	O
gamma	O
dis	O
tribution	O
is	O
normalized	O
evaluate	O
the	O
mean	B
variance	B
and	O
mode	O
of	O
the	O
gamma	B
distribution	I
the	O
following	O
distribution	O
px	O
q	O
q	O
exp	O
px	O
q	O
dx	O
is	O
a	O
generalization	B
of	O
the	O
univariate	O
gaussian	B
distribution	O
show	O
that	O
this	O
distribution	O
is	O
normalized	O
so	O
that	O
and	O
that	O
it	O
reduces	O
to	O
the	O
gaussian	B
when	O
q	O
consider	O
a	O
regression	B
model	O
in	O
which	O
the	O
target	O
variable	O
is	O
given	O
by	O
t	O
yx	O
w	O
and	O
is	O
a	O
random	O
noise	O
exercises	O
variable	O
drawn	O
from	O
the	O
distribution	O
show	O
that	O
the	O
log	O
likelihood	B
function	I
over	O
w	O
and	O
for	O
an	O
observed	O
data	O
set	O
of	O
input	O
vectors	O
x	O
xn	O
and	O
corresponding	O
target	O
variables	O
t	O
tnt	O
is	O
given	O
by	O
ln	O
ptx	O
w	O
w	O
tnq	O
n	O
q	O
const	O
where	O
const	O
denotes	O
terms	O
independent	B
of	O
both	O
w	O
and	O
note	O
that	O
as	O
a	O
function	O
of	O
w	O
this	O
is	O
the	O
lq	O
error	B
function	I
considered	O
in	O
section	O
consider	O
a	O
univariate	O
gaussian	B
distribution	O
n	O
having	O
conjugate	B
gaussian-gamma	O
prior	B
given	O
by	O
and	O
a	O
data	O
set	O
x	O
xn	O
of	O
i	O
i	O
d	O
observations	O
show	O
that	O
the	O
posterior	O
distribution	O
is	O
also	O
a	O
gaussian-gamma	B
distribution	I
of	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
and	O
write	O
down	O
expressions	O
for	O
the	O
parameters	O
of	O
this	O
posterior	O
distribution	O
verify	O
that	O
the	O
wishart	B
distribution	I
defined	O
by	O
is	O
indeed	O
a	O
conjugate	B
prior	B
for	O
the	O
precision	B
matrix	I
of	O
a	O
multivariate	O
gaussian	B
www	O
verify	O
that	O
evaluating	O
the	O
integral	O
in	O
leads	O
to	O
the	O
result	O
www	O
show	O
that	O
in	O
the	O
limit	O
the	O
t-distribution	O
becomes	O
a	O
gaussian	B
hint	O
ignore	O
the	O
normalization	O
coefficient	O
and	O
simply	O
look	O
at	O
the	O
dependence	O
on	O
x	O
by	O
following	O
analogous	O
steps	O
to	O
those	O
used	O
to	O
derive	O
the	O
univariate	O
student	O
s	O
t-distribution	O
verify	O
the	O
result	O
for	O
the	O
multivariate	O
form	O
of	O
the	O
student	O
s	O
t-distribution	O
by	O
marginalizing	O
over	O
the	O
variable	O
in	O
using	O
the	O
definition	O
show	O
by	O
exchanging	O
integration	O
variables	O
that	O
the	O
multivariate	O
t-distribution	O
is	O
correctly	O
normalized	O
by	O
using	O
the	O
definition	O
of	O
the	O
multivariate	O
student	O
s	O
t-distribution	O
as	O
a	O
convolution	O
of	O
a	O
gaussian	B
with	O
a	O
gamma	B
distribution	I
verify	O
the	O
properties	O
and	O
for	O
the	O
multivariate	O
t-distribution	O
defined	O
by	O
show	O
that	O
in	O
the	O
limit	O
the	O
multivariate	O
student	O
s	O
t-distribution	O
reduces	O
to	O
a	O
gaussian	B
with	O
mean	B
and	O
precision	O
www	O
the	O
various	O
trigonometric	O
identities	O
used	O
in	O
the	O
discussion	O
of	O
periodic	O
variables	O
in	O
this	O
chapter	O
can	O
be	O
proven	O
easily	O
from	O
the	O
relation	O
expia	O
cos	O
a	O
i	O
sin	O
a	O
in	O
which	O
i	O
is	O
the	O
square	O
root	O
of	O
minus	O
one	O
by	O
considering	O
the	O
identity	O
expia	O
exp	O
ia	O
prove	O
the	O
result	O
similarly	O
using	O
the	O
identity	O
cosa	O
b	O
expia	O
b	O
probability	B
distributions	O
where	O
denotes	O
the	O
real	O
part	O
prove	O
finally	O
by	O
using	O
sina	O
b	O
expia	O
b	O
where	O
denotes	O
the	O
imaginary	O
part	O
prove	O
the	O
result	O
for	O
large	O
m	O
the	O
von	B
mises	I
distribution	I
becomes	O
sharply	O
peaked	O
around	O
the	O
mode	O
by	O
defining	O
and	O
making	O
the	O
taylor	O
expansion	O
of	O
the	O
cosine	O
function	O
given	O
by	O
show	O
that	O
as	O
m	O
the	O
von	B
mises	I
distribution	I
tends	O
to	O
a	O
gaussian	B
cos	O
o	O
using	O
the	O
trigonometric	O
identity	O
show	O
that	O
solution	O
of	O
for	O
is	O
given	O
by	O
by	O
computing	O
first	O
and	O
second	O
derivatives	O
of	O
the	O
von	B
mises	I
distribution	I
and	O
using	O
for	O
m	O
show	O
that	O
the	O
maximum	O
of	O
the	O
distribution	O
occurs	O
when	O
and	O
that	O
the	O
minimum	O
occurs	O
when	O
by	O
making	O
use	O
of	O
the	O
result	O
together	O
with	O
and	O
the	O
trigonometric	O
identity	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
mml	O
for	O
the	O
concentration	O
of	O
the	O
von	B
mises	I
distribution	I
satisfies	O
amml	O
r	O
where	O
r	O
is	O
the	O
radius	O
of	O
the	O
mean	B
of	O
the	O
observations	O
viewed	O
as	O
unit	O
vectors	O
in	O
the	O
two-dimensional	O
euclidean	O
plane	O
as	O
illustrated	O
in	O
figure	O
www	O
express	O
the	O
beta	B
distribution	I
the	O
gamma	B
distribution	I
and	O
the	O
von	B
mises	I
distribution	I
as	O
members	O
of	O
the	O
exponential	B
family	I
and	O
thereby	O
identify	O
their	O
natural	B
parameters	I
verify	O
that	O
the	O
multivariate	O
gaussian	B
distribution	O
can	O
be	O
cast	O
in	O
exponential	B
family	I
form	O
and	O
derive	O
expressions	O
for	O
ux	O
hx	O
and	O
g	O
analogous	O
to	O
the	O
result	O
showed	O
that	O
the	O
negative	O
gradient	O
of	O
ln	O
g	O
for	O
the	O
exponential	B
family	I
is	O
given	O
by	O
the	O
expectation	B
of	O
ux	O
by	O
taking	O
the	O
second	O
derivatives	O
of	O
show	O
that	O
ln	O
g	O
euxuxt	O
euxeuxt	O
covux	O
by	O
changing	O
variables	O
using	O
y	O
x	O
show	O
that	O
the	O
density	B
will	O
be	O
correctly	O
normalized	O
provided	O
fx	O
is	O
correctly	O
normalized	O
www	O
consider	O
a	O
histogram-like	O
density	B
model	O
in	O
which	O
the	O
space	O
x	O
is	O
divided	O
into	O
fixed	O
regions	O
for	O
which	O
the	O
density	B
px	O
takes	O
the	O
constant	O
value	O
hi	O
over	O
the	O
ith	O
region	O
and	O
that	O
the	O
volume	O
of	O
region	O
i	O
is	O
denoted	O
i	O
suppose	O
we	O
have	O
a	O
set	O
of	O
n	O
observations	O
of	O
x	O
such	O
that	O
ni	O
of	O
these	O
observations	O
fall	O
in	O
region	O
i	O
using	O
a	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
normalization	O
constraint	O
on	O
the	O
density	B
derive	O
an	O
expression	O
for	O
the	O
maximum	B
likelihood	I
estimator	O
for	O
the	O
show	O
that	O
the	O
k-nearest-neighbour	O
density	B
model	O
defines	O
an	O
improper	B
distribu	O
tion	O
whose	O
integral	O
over	O
all	O
space	O
is	O
divergent	O
linear	O
models	O
for	B
regression	B
the	O
focus	O
so	O
far	O
in	O
this	O
book	O
has	O
been	O
on	O
unsupervised	B
learning	B
including	O
topics	O
such	O
as	O
density	B
estimation	I
and	O
data	O
clustering	B
we	O
turn	O
now	O
to	O
a	O
discussion	O
of	O
supervised	B
learning	B
starting	O
with	O
regression	B
the	O
goal	O
of	O
regression	B
is	O
to	O
predict	O
the	O
value	O
of	O
one	O
or	O
more	O
continuous	O
target	O
variables	O
t	O
given	O
the	O
value	O
of	O
a	O
d-dimensional	O
vector	O
x	O
of	O
input	O
variables	O
we	O
have	O
already	O
encountered	O
an	O
example	O
of	O
a	O
regression	B
problem	O
when	O
we	O
considered	O
polynomial	B
curve	B
fitting	I
in	O
chapter	O
the	O
polynomial	O
is	O
a	O
specific	O
example	O
of	O
a	O
broad	O
class	O
of	O
functions	O
called	O
linear	B
regression	B
models	O
which	O
share	O
the	O
property	O
of	O
being	O
linear	O
functions	O
of	O
the	O
adjustable	O
parameters	O
and	O
which	O
will	O
form	O
the	O
focus	O
of	O
this	O
chapter	O
the	O
simplest	O
form	O
of	O
linear	B
regression	B
models	O
are	O
also	O
linear	O
functions	O
of	O
the	O
input	O
variables	O
however	O
we	O
can	O
obtain	O
a	O
much	O
more	O
useful	O
class	O
of	O
functions	O
by	O
taking	O
linear	O
combinations	O
of	O
a	O
fixed	O
set	O
of	O
nonlinear	O
functions	O
of	O
the	O
input	O
variables	O
known	O
as	O
basis	O
functions	O
such	O
models	O
are	O
linear	O
functions	O
of	O
the	O
parameters	O
which	O
gives	O
them	O
simple	O
analytical	O
properties	O
and	O
yet	O
can	O
be	O
nonlinear	O
with	O
respect	O
to	O
the	O
input	O
variables	O
linear	O
models	O
for	B
regression	B
given	O
a	O
training	B
data	O
set	O
comprising	O
n	O
observations	O
where	O
n	O
n	O
together	O
with	O
corresponding	O
target	O
values	O
the	O
goal	O
is	O
to	O
predict	O
the	O
value	O
of	O
t	O
for	O
a	O
new	O
value	O
of	O
x	O
in	O
the	O
simplest	O
approach	O
this	O
can	O
be	O
done	O
by	O
directly	O
constructing	O
an	O
appropriate	O
function	O
yx	O
whose	O
values	O
for	O
new	O
inputs	O
x	O
constitute	O
the	O
predictions	O
for	O
the	O
corresponding	O
values	O
of	O
t	O
more	O
generally	O
from	O
a	O
probabilistic	O
perspective	O
we	O
aim	O
to	O
model	O
the	O
predictive	B
distribution	I
ptx	O
because	O
this	O
expresses	O
our	O
uncertainty	O
about	O
the	O
value	O
of	O
t	O
for	O
each	O
value	O
of	O
x	O
from	O
this	O
conditional	B
distribution	O
we	O
can	O
make	O
predictions	O
of	O
t	O
for	O
any	O
new	O
value	O
of	O
x	O
in	O
such	O
a	O
way	O
as	O
to	O
minimize	O
the	O
expected	O
value	O
of	O
a	O
suitably	O
chosen	O
loss	B
function	I
as	O
discussed	O
in	O
section	O
a	O
common	O
choice	O
of	O
loss	B
function	I
for	O
real-valued	O
variables	O
is	O
the	O
squared	O
loss	O
for	O
which	O
the	O
optimal	O
solution	O
is	O
given	O
by	O
the	O
conditional	B
expectation	B
of	O
t	O
although	O
linear	O
models	O
have	O
significant	O
limitations	O
as	O
practical	O
techniques	O
for	O
pattern	O
recognition	O
particularly	O
for	O
problems	O
involving	O
input	O
spaces	O
of	O
high	O
dimensionality	O
they	O
have	O
nice	O
analytical	O
properties	O
and	O
form	O
the	O
foundation	O
for	O
more	O
sophisticated	O
models	O
to	O
be	O
discussed	O
in	O
later	O
chapters	O
linear	O
basis	B
function	I
models	O
the	O
simplest	O
linear	O
model	O
for	B
regression	B
is	O
one	O
that	O
involves	O
a	O
linear	O
combination	O
of	O
the	O
input	O
variables	O
yx	O
w	O
wdxd	O
where	O
x	O
xdt	O
this	O
is	O
often	O
simply	O
known	O
as	O
linear	B
regression	B
the	O
key	O
property	O
of	O
this	O
model	O
is	O
that	O
it	O
is	O
a	O
linear	O
function	O
of	O
the	O
parameters	O
wd	O
it	O
is	O
also	O
however	O
a	O
linear	O
function	O
of	O
the	O
input	O
variables	O
xi	O
and	O
this	O
imposes	O
significant	O
limitations	O
on	O
the	O
model	O
we	O
therefore	O
extend	O
the	O
class	O
of	O
models	O
by	O
considering	O
linear	O
combinations	O
of	O
fixed	O
nonlinear	O
functions	O
of	O
the	O
input	O
variables	O
of	O
the	O
form	O
m	O
yx	O
w	O
wj	O
jx	O
where	O
jx	O
are	O
known	O
as	O
basis	O
functions	O
by	O
denoting	O
the	O
maximum	O
value	O
of	O
the	O
index	O
j	O
by	O
m	O
the	O
total	O
number	O
of	O
parameters	O
in	O
this	O
model	O
will	O
be	O
m	O
the	O
parameter	O
allows	O
for	O
any	O
fixed	O
offset	O
in	O
the	O
data	O
and	O
is	O
sometimes	O
called	O
a	O
bias	B
parameter	I
to	O
be	O
confused	O
with	O
bias	B
in	O
a	O
statistical	O
sense	O
it	O
is	O
often	O
convenient	O
to	O
define	O
an	O
additional	O
dummy	O
basis	B
function	I
so	O
that	O
m	O
yx	O
w	O
wj	O
jx	O
wt	O
where	O
w	O
wm	O
and	O
m	O
in	O
many	O
practical	O
applications	O
of	O
pattern	O
recognition	O
we	O
will	O
apply	O
some	O
form	O
of	O
fixed	O
pre-processing	O
linear	O
basis	B
function	I
models	O
or	O
feature	B
extraction	I
to	O
the	O
original	O
data	O
variables	O
if	O
the	O
original	O
variables	O
comprise	O
the	O
vector	O
x	O
then	O
the	O
features	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
basis	O
functions	O
jx	O
by	O
using	O
nonlinear	O
basis	O
functions	O
we	O
allow	O
the	O
function	O
yx	O
w	O
to	O
be	O
a	O
nonlinear	O
function	O
of	O
the	O
input	O
vector	O
x	O
functions	O
of	O
the	O
form	O
are	O
called	O
linear	O
models	O
however	O
because	O
this	O
function	O
is	O
linear	O
in	O
w	O
it	O
is	O
this	O
linearity	O
in	O
the	O
parameters	O
that	O
will	O
greatly	O
simplify	O
the	O
analysis	O
of	O
this	O
class	O
of	O
models	O
however	O
it	O
also	O
leads	O
to	O
some	O
significant	O
limitations	O
as	O
we	O
discuss	O
in	O
section	O
the	O
example	O
of	O
polynomial	O
regression	B
considered	O
in	O
chapter	O
is	O
a	O
particular	O
example	O
of	O
this	O
model	O
in	O
which	O
there	O
is	O
a	O
single	O
input	O
variable	O
x	O
and	O
the	O
basis	O
functions	O
take	O
the	O
form	O
of	O
powers	O
of	O
x	O
so	O
that	O
jx	O
xj	O
one	O
limitation	O
of	O
polynomial	O
basis	O
functions	O
is	O
that	O
they	O
are	O
global	O
functions	O
of	O
the	O
input	O
variable	O
so	O
that	O
changes	O
in	O
one	O
region	O
of	O
input	O
space	O
affect	O
all	O
other	O
regions	O
this	O
can	O
be	O
resolved	O
by	O
dividing	O
the	O
input	O
space	O
up	O
into	O
regions	O
and	O
fit	O
a	O
different	O
polynomial	O
in	O
each	O
region	O
leading	O
to	O
spline	B
functions	I
et	O
al	O
there	O
are	O
many	O
other	O
possible	O
choices	O
for	O
the	O
basis	O
functions	O
for	O
example	O
jx	O
exp	O
where	O
the	O
j	O
govern	O
the	O
locations	O
of	O
the	O
basis	O
functions	O
in	O
input	O
space	O
and	O
the	O
parameter	O
s	O
governs	O
their	O
spatial	O
scale	O
these	O
are	O
usually	O
referred	O
to	O
as	O
gaussian	B
basis	O
functions	O
although	O
it	O
should	O
be	O
noted	O
that	O
they	O
are	O
not	O
required	O
to	O
have	O
a	O
probabilistic	O
interpretation	O
and	O
in	O
particular	O
the	O
normalization	O
coefficient	O
is	O
unimportant	O
because	O
these	O
basis	O
functions	O
will	O
be	O
multiplied	O
by	O
adaptive	O
parameters	O
wj	O
another	O
possibility	O
is	O
the	O
sigmoidal	O
basis	B
function	I
of	O
the	O
form	O
x	O
j	O
s	O
jx	O
where	O
is	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
exp	O
a	O
equivalently	O
we	O
can	O
use	O
the	O
tanh	O
function	O
because	O
this	O
is	O
related	O
to	O
the	O
logistic	B
sigmoid	I
by	O
tanha	O
and	O
so	O
a	O
general	O
linear	O
combination	O
of	O
logistic	B
sigmoid	I
functions	O
is	O
equivalent	O
to	O
a	O
general	O
linear	O
combination	O
of	O
tanh	O
functions	O
these	O
various	O
choices	O
of	O
basis	B
function	I
are	O
illustrated	O
in	O
figure	O
yet	O
another	O
possible	O
choice	O
of	O
basis	B
function	I
is	O
the	O
fourier	O
basis	O
which	O
leads	O
to	O
an	O
expansion	O
in	O
sinusoidal	O
functions	O
each	O
basis	B
function	I
represents	O
a	O
specific	O
frequency	O
and	O
has	O
infinite	O
spatial	O
extent	O
by	O
contrast	O
basis	O
functions	O
that	O
are	O
localized	O
to	O
finite	O
regions	O
of	O
input	O
space	O
necessarily	O
comprise	O
a	O
spectrum	O
of	O
different	O
spatial	O
frequencies	O
in	O
many	O
signal	O
processing	O
applications	O
it	O
is	O
of	O
interest	O
to	O
consider	O
basis	O
functions	O
that	O
are	O
localized	O
in	O
both	O
space	O
and	O
frequency	O
leading	O
to	O
a	O
class	O
of	O
functions	O
known	O
as	O
wavelets	B
these	O
are	O
also	O
defined	O
to	O
be	O
mutually	O
orthogonal	O
to	O
simplify	O
their	O
application	O
wavelets	B
are	O
most	O
applicable	O
when	O
the	O
input	O
values	O
live	O
linear	O
models	O
for	B
regression	B
figure	O
examples	O
of	O
basis	O
functions	O
showing	O
polynomials	O
on	O
the	O
left	O
gaussians	O
of	O
the	O
form	O
in	O
the	O
centre	O
and	O
sigmoidal	O
of	O
the	O
form	O
on	O
the	O
right	O
on	O
a	O
regular	O
lattice	O
such	O
as	O
the	O
successive	O
time	O
points	O
in	O
a	O
temporal	O
sequence	O
or	O
the	O
pixels	O
in	O
an	O
image	O
useful	O
texts	O
on	O
wavelets	B
include	O
ogden	O
mallat	O
and	O
vidakovic	O
most	O
of	O
the	O
discussion	O
in	O
this	O
chapter	O
however	O
is	O
independent	B
of	O
the	O
particular	O
choice	O
of	O
basis	B
function	I
set	O
and	O
so	O
for	O
most	O
of	O
our	O
discussion	O
we	O
shall	O
not	O
specify	O
the	O
particular	O
form	O
of	O
the	O
basis	O
functions	O
except	O
for	O
the	O
purposes	O
of	O
numerical	O
illustration	O
indeed	O
much	O
of	O
our	O
discussion	O
will	O
be	O
equally	O
applicable	O
to	O
the	O
situation	O
in	O
which	O
the	O
vector	O
of	O
basis	O
functions	O
is	O
simply	O
the	O
identity	O
x	O
furthermore	O
in	O
order	O
to	O
keep	O
the	O
notation	O
simple	O
we	O
shall	O
focus	O
on	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
t	O
however	O
in	O
section	O
we	O
consider	O
briefly	O
the	O
modifications	O
needed	O
to	O
deal	O
with	O
multiple	O
target	O
variables	O
maximum	B
likelihood	I
and	O
least	O
squares	O
in	O
chapter	O
we	O
fitted	O
polynomial	O
functions	O
to	O
data	O
sets	O
by	O
minimizing	O
a	O
sumof-squares	O
error	B
function	I
we	O
also	O
showed	O
that	O
this	O
error	B
function	I
could	O
be	O
motivated	O
as	O
the	O
maximum	B
likelihood	I
solution	O
under	O
an	O
assumed	O
gaussian	B
noise	O
model	O
let	O
us	O
return	O
to	O
this	O
discussion	O
and	O
consider	O
the	O
least	O
squares	O
approach	O
and	O
its	O
relation	O
to	O
maximum	B
likelihood	I
in	O
more	O
detail	O
as	O
before	O
we	O
assume	O
that	O
the	O
target	O
variable	O
t	O
is	O
given	O
by	O
a	O
deterministic	O
func	O
tion	O
yx	O
w	O
with	O
additive	O
gaussian	B
noise	O
so	O
that	O
t	O
yx	O
w	O
where	O
is	O
a	O
zero	O
mean	B
gaussian	B
random	O
variable	O
with	O
precision	O
variance	B
thus	O
we	O
can	O
write	O
ptx	O
w	O
n	O
w	O
section	O
recall	O
that	O
if	O
we	O
assume	O
a	O
squared	O
loss	B
function	I
then	O
the	O
optimal	O
prediction	O
for	O
a	O
new	O
value	O
of	O
x	O
will	O
be	O
given	O
by	O
the	O
conditional	B
mean	B
of	O
the	O
target	O
variable	O
in	O
the	O
case	O
of	O
a	O
gaussian	B
conditional	B
distribution	O
of	O
the	O
form	O
the	O
conditional	B
mean	B
will	O
be	O
simply	O
etx	O
tptx	O
dt	O
yx	O
w	O
note	O
that	O
the	O
gaussian	B
noise	O
assumption	O
implies	O
that	O
the	O
conditional	B
distribution	O
of	O
t	O
given	O
x	O
is	O
unimodal	O
which	O
may	O
be	O
inappropriate	O
for	O
some	O
applications	O
an	O
extension	O
to	O
mixtures	O
of	O
conditional	B
gaussian	B
distributions	O
which	O
permit	O
multimodal	O
conditional	B
distributions	O
will	O
be	O
discussed	O
in	O
section	O
now	O
consider	O
a	O
data	O
set	O
of	O
inputs	O
x	O
xn	O
with	O
corresponding	O
target	O
values	O
tn	O
we	O
group	O
the	O
target	O
variables	O
into	O
a	O
column	O
vector	O
that	O
we	O
denote	O
by	O
t	O
where	O
the	O
typeface	O
is	O
chosen	O
to	O
distinguish	O
it	O
from	O
a	O
single	O
observation	O
of	O
a	O
multivariate	O
target	O
which	O
would	O
be	O
denoted	O
t	O
making	O
the	O
assumption	O
that	O
these	O
data	O
points	O
are	O
drawn	O
independently	O
from	O
the	O
distribution	O
we	O
obtain	O
the	O
following	O
expression	O
for	O
the	O
likelihood	B
function	I
which	O
is	O
a	O
function	O
of	O
the	O
adjustable	O
parameters	O
w	O
and	O
in	O
the	O
form	O
ptx	O
w	O
n	O
where	O
we	O
have	O
used	O
note	O
that	O
in	O
supervised	B
learning	B
problems	O
such	O
as	O
regression	B
classification	B
we	O
are	O
not	O
seeking	O
to	O
model	O
the	O
distribution	O
of	O
the	O
input	O
variables	O
thus	O
x	O
will	O
always	O
appear	O
in	O
the	O
set	O
of	O
conditioning	O
variables	O
and	O
so	O
from	O
now	O
on	O
we	O
will	O
drop	O
the	O
explicit	O
x	O
from	O
expressions	O
such	O
as	O
ptx	O
w	O
in	O
order	O
to	O
keep	O
the	O
notation	O
uncluttered	O
taking	O
the	O
logarithm	O
of	O
the	O
likelihood	B
function	I
and	O
making	O
use	O
of	O
the	O
standard	O
form	O
for	O
the	O
univariate	O
gaussian	B
we	O
have	O
ln	O
ptw	O
lnn	O
n	O
ln	O
n	O
edw	O
where	O
the	O
sum-of-squares	B
error	B
function	I
is	O
defined	O
by	O
edw	O
wt	O
having	O
written	O
down	O
the	O
likelihood	B
function	I
we	O
can	O
use	O
maximum	B
likelihood	I
to	O
determine	O
w	O
and	O
consider	O
first	O
the	O
maximization	O
with	O
respect	O
to	O
w	O
as	O
observed	O
already	O
in	O
section	O
we	O
see	O
that	O
maximization	O
of	O
the	O
likelihood	B
function	I
under	O
a	O
conditional	B
gaussian	B
noise	O
distribution	O
for	O
a	O
linear	O
model	O
is	O
equivalent	O
to	O
minimizing	O
a	O
sum-of-squares	B
error	B
function	I
given	O
by	O
edw	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
function	I
takes	O
the	O
form	O
ln	O
ptw	O
tn	O
wt	O
linear	O
basis	B
function	I
models	O
linear	O
models	O
for	B
regression	B
setting	O
this	O
gradient	O
to	O
zero	O
gives	O
solving	O
for	O
w	O
we	O
obtain	O
tt	O
tn	O
wt	O
the	O
quantity	O
wml	O
which	O
are	O
known	O
as	O
the	O
normal	B
equations	I
for	O
the	O
least	O
squares	O
problem	O
here	O
is	O
an	O
n	O
m	O
matrix	O
called	O
the	O
design	B
matrix	I
whose	O
elements	O
are	O
given	O
by	O
nj	O
jxn	O
so	O
that	O
t	O
m	O
m	O
m	O
t	O
t	O
is	O
known	O
as	O
the	O
moore-penrose	O
pseudo-inverse	B
of	O
the	O
matrix	O
and	O
mitra	O
golub	O
and	O
van	O
loan	O
it	O
can	O
be	O
regarded	O
as	O
a	O
generalization	B
of	O
the	O
notion	O
of	O
matrix	O
inverse	B
to	O
nonsquare	O
matrices	O
indeed	O
if	O
is	O
square	O
and	O
invertible	O
then	O
using	O
the	O
property	O
b	O
we	O
see	O
that	O
at	O
this	O
point	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
role	O
of	O
the	O
bias	B
parameter	I
if	O
we	O
make	O
the	O
bias	B
parameter	I
explicit	O
then	O
the	O
error	B
function	I
becomes	O
m	O
t	O
m	O
wj	O
j	O
edw	O
wj	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
equal	O
to	O
zero	O
and	O
solving	O
for	O
we	O
obtain	O
where	O
we	O
have	O
defined	O
t	O
n	O
tn	O
j	O
n	O
jxn	O
thus	O
the	O
bias	B
compensates	O
for	O
the	O
difference	O
between	O
the	O
averages	O
the	O
training	B
set	I
of	O
the	O
target	O
values	O
and	O
the	O
weighted	O
sum	O
of	O
the	O
averages	O
of	O
the	O
basis	B
function	I
values	O
we	O
can	O
also	O
maximize	O
the	O
log	O
likelihood	B
function	I
with	O
respect	O
to	O
the	O
noise	O
precision	B
parameter	I
giving	O
ml	O
n	O
wt	O
ml	O
linear	O
basis	B
function	I
models	O
figure	O
geometrical	O
interpretation	O
of	O
the	O
least-squares	O
solution	O
in	O
an	O
n-dimensional	O
space	O
whose	O
axes	O
are	O
the	O
values	O
of	O
tn	O
the	O
least-squares	O
regression	B
function	I
is	O
obtained	O
by	O
finding	O
the	O
orthogonal	O
projection	O
of	O
the	O
data	O
vector	O
t	O
onto	O
the	O
subspace	O
spanned	O
by	O
the	O
basis	O
functions	O
jx	O
in	O
which	O
each	O
basis	B
function	I
is	O
viewed	O
as	O
a	O
vector	O
j	O
of	O
length	O
n	O
with	O
elements	O
jxn	O
s	O
t	O
y	O
and	O
so	O
we	O
see	O
that	O
the	O
inverse	B
of	O
the	O
noise	O
precision	O
is	O
given	O
by	O
the	O
residual	O
variance	B
of	O
the	O
target	O
values	O
around	O
the	O
regression	B
function	I
geometry	O
of	O
least	O
squares	O
at	O
this	O
point	O
it	O
is	O
instructive	O
to	O
consider	O
the	O
geometrical	O
interpretation	O
of	O
the	O
least-squares	O
solution	O
to	O
do	O
this	O
we	O
consider	O
an	O
n-dimensional	O
space	O
whose	O
axes	O
are	O
given	O
by	O
the	O
tn	O
so	O
that	O
t	O
tnt	O
is	O
a	O
vector	O
in	O
this	O
space	O
each	O
basis	B
function	I
jxn	O
evaluated	O
at	O
the	O
n	O
data	O
points	O
can	O
also	O
be	O
represented	O
as	O
a	O
vector	O
in	O
the	O
same	O
space	O
denoted	O
by	O
j	O
as	O
illustrated	O
in	O
figure	O
note	O
that	O
j	O
corresponds	O
to	O
the	O
jth	O
column	O
of	O
whereas	O
corresponds	O
to	O
the	O
nth	O
row	O
of	O
if	O
the	O
number	O
m	O
of	O
basis	O
functions	O
is	O
smaller	O
than	O
the	O
number	O
n	O
of	O
data	O
points	O
then	O
the	O
m	O
vectors	O
jxn	O
will	O
span	O
a	O
linear	O
subspace	O
s	O
of	O
dimensionality	O
m	O
we	O
define	O
y	O
to	O
be	O
an	O
n-dimensional	O
vector	O
whose	O
nth	O
element	O
is	O
given	O
by	O
yxn	O
w	O
where	O
n	O
n	O
because	O
y	O
is	O
an	O
arbitrary	O
linear	O
combination	O
of	O
the	O
vectors	O
j	O
it	O
can	O
live	O
anywhere	O
in	O
the	O
m-dimensional	O
subspace	O
the	O
sum-of-squares	B
error	B
is	O
then	O
equal	O
to	O
a	O
factor	O
of	O
to	O
the	O
squared	O
euclidean	O
distance	O
between	O
y	O
and	O
t	O
thus	O
the	O
least-squares	O
solution	O
for	O
w	O
corresponds	O
to	O
that	O
choice	O
of	O
y	O
that	O
lies	O
in	O
subspace	O
s	O
and	O
that	O
is	O
closest	O
to	O
t	O
intuitively	O
from	O
figure	O
we	O
anticipate	O
that	O
this	O
solution	O
corresponds	O
to	O
the	O
orthogonal	O
projection	O
of	O
t	O
onto	O
the	O
subspace	O
s	O
this	O
is	O
indeed	O
the	O
case	O
as	O
can	O
easily	O
be	O
verified	O
by	O
noting	O
that	O
the	O
solution	O
for	O
y	O
is	O
given	O
by	O
wml	O
and	O
then	O
confirming	O
that	O
this	O
takes	O
the	O
form	O
of	O
an	O
orthogonal	O
projection	O
in	O
practice	O
a	O
direct	O
solution	O
of	O
the	O
normal	B
equations	I
can	O
lead	O
to	O
numerical	O
difficulties	O
when	O
t	O
is	O
close	O
to	O
singular	O
in	O
particular	O
when	O
two	O
or	O
more	O
of	O
the	O
basis	O
vectors	O
j	O
are	O
co-linear	O
or	O
nearly	O
so	O
the	O
resulting	O
parameter	O
values	O
can	O
have	O
large	O
magnitudes	O
such	O
near	O
degeneracies	O
will	O
not	O
be	O
uncommon	O
when	O
dealing	O
with	O
real	O
data	O
sets	O
the	O
resulting	O
numerical	O
difficulties	O
can	O
be	O
addressed	O
using	O
the	O
technique	O
of	O
singular	B
value	I
decomposition	I
or	O
svd	O
et	O
al	O
bishop	O
and	O
nabney	O
note	O
that	O
the	O
addition	O
of	O
a	O
regularization	B
term	O
ensures	O
that	O
the	O
matrix	O
is	O
nonsingular	O
even	O
in	O
the	O
presence	O
of	O
degeneracies	O
sequential	B
learning	B
batch	O
techniques	O
such	O
as	O
the	O
maximum	B
likelihood	I
solution	O
which	O
involve	O
processing	O
the	O
entire	O
training	B
set	I
in	O
one	O
go	O
can	O
be	O
computationally	O
costly	O
for	O
large	O
data	O
sets	O
as	O
we	O
have	O
discussed	O
in	O
chapter	O
if	O
the	O
data	O
set	O
is	O
sufficiently	O
large	O
it	O
may	O
be	O
worthwhile	O
to	O
use	O
sequential	O
algorithms	O
also	O
known	O
as	O
on-line	O
algorithms	O
exercise	O
linear	O
models	O
for	B
regression	B
in	O
which	O
the	O
data	O
points	O
are	O
considered	O
one	O
at	O
a	O
time	O
and	O
the	O
model	O
parameters	O
updated	O
after	O
each	O
such	O
presentation	O
sequential	B
learning	B
is	O
also	O
appropriate	O
for	O
realtime	O
applications	O
in	O
which	O
the	O
data	O
observations	O
are	O
arriving	O
in	O
a	O
continuous	O
stream	O
and	O
predictions	O
must	O
be	O
made	O
before	O
all	O
of	O
the	O
data	O
points	O
are	O
seen	O
we	O
can	O
obtain	O
a	O
sequential	B
learning	B
algorithm	O
by	O
applying	O
the	O
technique	O
of	O
stochastic	B
gradient	B
descent	I
also	O
known	O
as	O
sequential	B
gradient	B
descent	I
as	O
follows	O
if	O
the	O
error	B
function	I
comprises	O
a	O
sum	O
over	O
data	O
points	O
e	O
n	O
en	O
then	O
after	O
presentation	O
of	O
pattern	O
n	O
the	O
stochastic	B
gradient	B
descent	I
algorithm	O
updates	O
the	O
parameter	O
vector	O
w	O
using	O
where	O
denotes	O
the	O
iteration	O
number	O
and	O
is	O
a	O
learning	B
rate	I
parameter	I
we	O
shall	O
discuss	O
the	O
choice	O
of	O
value	O
for	O
shortly	O
the	O
value	O
of	O
w	O
is	O
initialized	O
to	O
some	O
starting	O
vector	O
for	O
the	O
case	O
of	O
the	O
sum-of-squares	B
error	B
function	I
this	O
gives	O
w	O
w	O
w	O
n	O
n	O
where	O
n	O
this	O
is	O
known	O
as	O
least-mean-squares	O
or	O
the	O
lms	O
algorithm	O
the	O
value	O
of	O
needs	O
to	O
be	O
chosen	O
with	O
care	O
to	O
ensure	O
that	O
the	O
algorithm	O
converges	O
and	O
nabney	O
w	O
w	O
en	O
regularized	B
least	I
squares	I
in	O
section	O
we	O
introduced	O
the	O
idea	O
of	O
adding	O
a	O
regularization	B
term	O
to	O
an	O
error	B
function	I
in	O
order	O
to	O
control	O
over-fitting	B
so	O
that	O
the	O
total	O
error	B
function	I
to	O
be	O
minimized	O
takes	O
the	O
form	O
edw	O
ew	O
where	O
is	O
the	O
regularization	B
coefficient	O
that	O
controls	O
the	O
relative	B
importance	O
of	O
the	O
data-dependent	O
error	B
edw	O
and	O
the	O
regularization	B
term	O
ew	O
one	O
of	O
the	O
simplest	O
forms	O
of	O
regularizer	O
is	O
given	O
by	O
the	O
sum-of-squares	O
of	O
the	O
weight	B
vector	I
elements	O
if	O
we	O
also	O
consider	O
the	O
sum-of-squares	B
error	B
function	I
given	O
by	O
ew	O
wtw	O
ew	O
wt	O
then	O
the	O
total	O
error	B
function	I
becomes	O
wt	O
wtw	O
this	O
particular	O
choice	O
of	O
regularizer	O
is	O
known	O
in	O
the	O
machine	O
learning	B
literature	O
as	O
weight	B
decay	I
because	O
in	O
sequential	B
learning	B
algorithms	O
it	O
encourages	O
weight	O
values	O
to	O
decay	O
towards	O
zero	O
unless	O
supported	O
by	O
the	O
data	O
in	O
statistics	O
it	O
provides	O
an	O
example	O
of	O
a	O
parameter	B
shrinkage	B
method	O
because	O
it	O
shrinks	O
parameter	O
values	O
towards	O
linear	O
basis	B
function	I
models	O
q	O
q	O
q	O
q	O
figure	O
contours	O
of	O
the	O
regularization	B
term	O
in	O
for	O
various	O
values	O
of	O
the	O
parameter	O
q	O
zero	O
it	O
has	O
the	O
advantage	O
that	O
the	O
error	B
function	I
remains	O
a	O
quadratic	O
function	O
of	O
w	O
and	O
so	O
its	O
exact	O
minimizer	O
can	O
be	O
found	O
in	O
closed	O
form	O
specifically	O
setting	O
the	O
gradient	O
of	O
with	O
respect	O
to	O
w	O
to	O
zero	O
and	O
solving	O
for	O
w	O
as	O
before	O
we	O
obtain	O
w	O
i	O
t	O
this	O
represents	O
a	O
simple	O
extension	O
of	O
the	O
least-squares	O
solution	O
a	O
more	O
general	O
regularizer	O
is	O
sometimes	O
used	O
for	O
which	O
the	O
regularized	O
error	B
tt	O
exercise	O
appendix	O
e	O
takes	O
the	O
form	O
wt	O
where	O
q	O
corresponds	O
to	O
the	O
quadratic	O
regularizer	O
figure	O
shows	O
contours	O
of	O
the	O
regularization	B
function	O
for	O
different	O
values	O
of	O
q	O
the	O
case	O
of	O
q	O
is	O
know	O
as	O
the	O
lasso	B
in	O
the	O
statistics	O
literature	O
it	O
has	O
the	O
property	O
that	O
if	O
is	O
sufficiently	O
large	O
some	O
of	O
the	O
coefficients	O
wj	O
are	O
driven	O
to	O
zero	O
leading	O
to	O
a	O
sparse	O
model	O
in	O
which	O
the	O
corresponding	O
basis	O
functions	O
play	O
no	O
role	O
to	O
see	O
this	O
we	O
first	O
note	O
that	O
minimizing	O
is	O
equivalent	O
to	O
minimizing	O
the	O
unregularized	O
sum-of-squares	B
error	B
subject	O
to	O
the	O
constraint	O
for	O
an	O
appropriate	O
value	O
of	O
the	O
parameter	O
where	O
the	O
two	O
approaches	O
can	O
be	O
related	O
using	O
lagrange	B
multipliers	O
the	O
origin	O
of	O
the	O
sparsity	B
can	O
be	O
seen	O
from	O
figure	O
which	O
shows	O
that	O
the	O
minimum	O
of	O
the	O
error	B
function	I
subject	O
to	O
the	O
constraint	O
as	O
is	O
increased	O
so	O
an	O
increasing	O
number	O
of	O
parameters	O
are	O
driven	O
to	O
zero	O
regularization	B
allows	O
complex	O
models	O
to	O
be	O
trained	O
on	O
data	O
sets	O
of	O
limited	O
size	O
without	O
severe	O
over-fitting	B
essentially	O
by	O
limiting	O
the	O
effective	O
model	O
complexity	O
however	O
the	O
problem	O
of	O
determining	O
the	O
optimal	O
model	O
complexity	O
is	O
then	O
shifted	O
from	O
one	O
of	O
finding	O
the	O
appropriate	O
number	O
of	O
basis	O
functions	O
to	O
one	O
of	O
determining	O
a	O
suitable	O
value	O
of	O
the	O
regularization	B
coefficient	O
we	O
shall	O
return	O
to	O
the	O
issue	O
of	O
model	O
complexity	O
later	O
in	O
this	O
chapter	O
linear	O
models	O
for	B
regression	B
figure	O
plot	O
of	O
the	O
contours	O
of	O
the	O
unregularized	O
error	B
function	I
along	O
with	O
the	O
constraint	O
region	O
for	O
the	O
quadratic	O
regularizer	O
q	O
on	O
the	O
left	O
and	O
the	O
lasso	B
regularizer	O
q	O
on	O
the	O
right	O
in	O
which	O
the	O
optimum	O
value	O
for	O
the	O
parameter	O
vector	O
w	O
is	O
denoted	O
by	O
the	O
lasso	B
gives	O
a	O
sparse	O
solution	O
in	O
which	O
for	O
the	O
remainder	O
of	O
this	O
chapter	O
we	O
shall	O
focus	O
on	O
the	O
quadratic	O
regularizer	O
both	O
for	O
its	O
practical	O
importance	O
and	O
its	O
analytical	O
tractability	O
multiple	O
outputs	O
so	O
far	O
we	O
have	O
considered	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
t	O
in	O
some	O
applications	O
we	O
may	O
wish	O
to	O
predict	O
k	O
target	O
variables	O
which	O
we	O
denote	O
collectively	O
by	O
the	O
target	B
vector	I
t	O
this	O
could	O
be	O
done	O
by	O
introducing	O
a	O
different	O
set	O
of	O
basis	O
functions	O
for	O
each	O
component	O
of	O
t	O
leading	O
to	O
multiple	O
independent	B
regression	B
problems	O
however	O
a	O
more	O
interesting	O
and	O
more	O
common	O
approach	O
is	O
to	O
use	O
the	O
same	O
set	O
of	O
basis	O
functions	O
to	O
model	O
all	O
of	O
the	O
components	O
of	O
the	O
target	B
vector	I
so	O
that	O
yx	O
w	O
wt	O
where	O
y	O
is	O
a	O
k-dimensional	O
column	O
vector	O
w	O
is	O
an	O
m	O
k	O
matrix	O
of	O
parameters	O
and	O
is	O
an	O
m-dimensional	O
column	O
vector	O
with	O
elements	O
jx	O
with	O
as	O
before	O
suppose	O
we	O
take	O
the	O
conditional	B
distribution	O
of	O
the	O
target	B
vector	I
to	O
be	O
an	O
isotropic	B
gaussian	B
of	O
the	O
form	O
ptx	O
w	O
n	O
if	O
we	O
have	O
a	O
set	O
of	O
observations	O
tn	O
we	O
can	O
combine	O
these	O
into	O
a	O
matrix	O
t	O
of	O
size	O
n	O
k	O
such	O
that	O
the	O
nth	O
row	O
is	O
given	O
by	O
tt	O
n	O
similarly	O
we	O
can	O
combine	O
the	O
input	O
vectors	O
xn	O
into	O
a	O
matrix	O
x	O
the	O
log	O
likelihood	B
function	I
is	O
then	O
given	O
by	O
ln	O
ptx	O
w	O
lnn	O
n	O
k	O
ln	O
tn	O
wt	O
as	O
before	O
we	O
can	O
maximize	O
this	O
function	O
with	O
respect	O
to	O
w	O
giving	O
if	O
we	O
examine	O
this	O
result	O
for	O
each	O
target	O
variable	O
tk	O
we	O
have	O
wml	O
wk	O
t	O
the	O
bias-variance	O
decomposition	O
t	O
tt	O
ttk	O
tk	O
exercise	O
where	O
tk	O
is	O
an	O
n-dimensional	O
column	O
vector	O
with	O
components	O
tnk	O
for	O
n	O
n	O
thus	O
the	O
solution	O
to	O
the	O
regression	B
problem	O
decouples	O
between	O
the	O
different	O
target	O
variables	O
and	O
we	O
need	O
only	O
compute	O
a	O
single	O
pseudo-inverse	B
matrix	O
which	O
is	O
shared	O
by	O
all	O
of	O
the	O
vectors	O
wk	O
the	O
extension	O
to	O
general	O
gaussian	B
noise	O
distributions	O
having	O
arbitrary	O
covariance	B
matrices	O
is	O
straightforward	O
again	O
this	O
leads	O
to	O
a	O
decoupling	O
into	O
k	O
independent	B
regression	B
problems	O
this	O
result	O
is	O
unsurprising	O
because	O
the	O
parameters	O
w	O
define	O
only	O
the	O
mean	B
of	O
the	O
gaussian	B
noise	O
distribution	O
and	O
we	O
know	O
from	O
section	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
mean	B
of	O
a	O
multivariate	O
gaussian	B
is	O
independent	B
of	O
the	O
covariance	B
from	O
now	O
on	O
we	O
shall	O
therefore	O
consider	O
a	O
single	O
target	O
variable	O
t	O
for	O
simplicity	O
the	O
bias-variance	O
decomposition	O
so	O
far	O
in	O
our	O
discussion	O
of	O
linear	O
models	O
for	B
regression	B
we	O
have	O
assumed	O
that	O
the	O
form	O
and	O
number	O
of	O
basis	O
functions	O
are	O
both	O
fixed	O
as	O
we	O
have	O
seen	O
in	O
chapter	O
the	O
use	O
of	O
maximum	B
likelihood	I
or	O
equivalently	O
least	O
squares	O
can	O
lead	O
to	O
severe	O
over-fitting	B
if	O
complex	O
models	O
are	O
trained	O
using	O
data	O
sets	O
of	O
limited	O
size	O
however	O
limiting	O
the	O
number	O
of	O
basis	O
functions	O
in	O
order	O
to	O
avoid	O
over-fitting	B
has	O
the	O
side	O
effect	O
of	O
limiting	O
the	O
flexibility	O
of	O
the	O
model	O
to	O
capture	O
interesting	O
and	O
important	O
trends	O
in	O
the	O
data	O
although	O
the	O
introduction	O
of	O
regularization	B
terms	O
can	O
control	O
over-fitting	B
for	O
models	O
with	O
many	O
parameters	O
this	O
raises	O
the	O
question	O
of	O
how	O
to	O
determine	O
a	O
suitable	O
value	O
for	O
the	O
regularization	B
coefficient	O
seeking	O
the	O
solution	O
that	O
minimizes	O
the	O
regularized	O
error	B
function	I
with	O
respect	O
to	O
both	O
the	O
weight	B
vector	I
w	O
and	O
the	O
regularization	B
coefficient	O
is	O
clearly	O
not	O
the	O
right	O
approach	O
since	O
this	O
leads	O
to	O
the	O
unregularized	O
solution	O
with	O
as	O
we	O
have	O
seen	O
in	O
earlier	O
chapters	O
the	O
phenomenon	O
of	O
over-fitting	B
is	O
really	O
an	O
unfortunate	O
property	O
of	O
maximum	B
likelihood	I
and	O
does	O
not	O
arise	O
when	O
we	O
marginalize	O
over	O
parameters	O
in	O
a	O
bayesian	B
setting	O
in	O
this	O
chapter	O
we	O
shall	O
consider	O
the	O
bayesian	B
view	O
of	O
model	O
complexity	O
in	O
some	O
depth	O
before	O
doing	O
so	O
however	O
it	O
is	O
instructive	O
to	O
consider	O
a	O
frequentist	B
viewpoint	O
of	O
the	O
model	O
complexity	O
issue	O
known	O
as	O
the	O
biasvariance	O
trade-off	O
although	O
we	O
shall	O
introduce	O
this	O
concept	O
in	O
the	O
context	O
of	O
linear	O
basis	B
function	I
models	O
where	O
it	O
is	O
easy	O
to	O
illustrate	O
the	O
ideas	O
using	O
simple	O
examples	O
the	O
discussion	O
has	O
more	O
general	O
applicability	O
in	O
section	O
when	O
we	O
discussed	O
decision	B
theory	B
for	B
regression	B
problems	O
we	O
considered	O
various	O
loss	O
functions	O
each	O
of	O
which	O
leads	O
to	O
a	O
corresponding	O
optimal	O
prediction	O
once	O
we	O
are	O
given	O
the	O
conditional	B
distribution	O
ptx	O
a	O
popular	O
choice	O
is	O
linear	O
models	O
for	B
regression	B
the	O
squared	O
loss	B
function	I
for	O
which	O
the	O
optimal	O
prediction	O
is	O
given	O
by	O
the	O
conditional	B
expectation	B
which	O
we	O
denote	O
by	O
hx	O
and	O
which	O
is	O
given	O
by	O
hx	O
etx	O
tptx	O
dt	O
at	O
this	O
point	O
it	O
is	O
worth	O
distinguishing	O
between	O
the	O
squared	O
loss	B
function	I
arising	O
from	O
decision	B
theory	B
and	O
the	O
sum-of-squares	B
error	B
function	I
that	O
arose	O
in	O
the	O
maximum	B
likelihood	I
estimation	O
of	O
model	O
parameters	O
we	O
might	O
use	O
more	O
sophisticated	O
techniques	O
than	O
least	O
squares	O
for	O
example	O
regularization	B
or	O
a	O
fully	O
bayesian	B
approach	O
to	O
determine	O
the	O
conditional	B
distribution	O
ptx	O
these	O
can	O
all	O
be	O
combined	O
with	O
the	O
squared	O
loss	B
function	I
for	O
the	O
purpose	O
of	O
making	O
predictions	O
we	O
showed	O
in	O
section	O
that	O
the	O
expected	O
squared	O
loss	O
can	O
be	O
written	O
in	O
the	O
form	O
el	O
px	O
dx	O
t	O
dx	O
dt	O
recall	O
that	O
the	O
second	O
term	O
which	O
is	O
independent	B
of	O
yx	O
arises	O
from	O
the	O
intrinsic	O
noise	O
on	O
the	O
data	O
and	O
represents	O
the	O
minimum	O
achievable	O
value	O
of	O
the	O
expected	O
loss	O
the	O
first	O
term	O
depends	O
on	O
our	O
choice	O
for	O
the	O
function	O
yx	O
and	O
we	O
will	O
seek	O
a	O
solution	O
for	O
yx	O
which	O
makes	O
this	O
term	O
a	O
minimum	O
because	O
it	O
is	O
nonnegative	O
the	O
smallest	O
that	O
we	O
can	O
hope	O
to	O
make	O
this	O
term	O
is	O
zero	O
if	O
we	O
had	O
an	O
unlimited	O
supply	O
of	O
data	O
unlimited	O
computational	O
resources	O
we	O
could	O
in	O
principle	O
find	O
the	O
regression	B
function	I
hx	O
to	O
any	O
desired	O
degree	O
of	O
accuracy	O
and	O
this	O
would	O
represent	O
the	O
optimal	O
choice	O
for	O
yx	O
however	O
in	O
practice	O
we	O
have	O
a	O
data	O
set	O
d	O
containing	O
only	O
a	O
finite	O
number	O
n	O
of	O
data	O
points	O
and	O
consequently	O
we	O
do	O
not	O
know	O
the	O
regression	B
function	I
hx	O
exactly	O
if	O
we	O
model	O
the	O
hx	O
using	O
a	O
parametric	O
function	O
yx	O
w	O
governed	O
by	O
a	O
parameter	O
vector	O
w	O
then	O
from	O
a	O
bayesian	B
perspective	O
the	O
uncertainty	O
in	O
our	O
model	O
is	O
expressed	O
through	O
a	O
posterior	O
distribution	O
over	O
w	O
a	O
frequentist	B
treatment	O
however	O
involves	O
making	O
a	O
point	O
estimate	O
of	O
w	O
based	O
on	O
the	O
data	O
set	O
d	O
and	O
tries	O
instead	O
to	O
interpret	O
the	O
uncertainty	O
of	O
this	O
estimate	O
through	O
the	O
following	O
thought	O
experiment	O
suppose	O
we	O
had	O
a	O
large	O
number	O
of	O
data	O
sets	O
each	O
of	O
size	O
n	O
and	O
each	O
drawn	O
independently	O
from	O
the	O
distribution	O
pt	O
x	O
for	O
any	O
given	O
data	O
set	O
d	O
we	O
can	O
run	O
our	O
learning	B
algorithm	O
and	O
obtain	O
a	O
prediction	O
function	O
yxd	O
different	O
data	O
sets	O
from	O
the	O
ensemble	O
will	O
give	O
different	O
functions	O
and	O
consequently	O
different	O
values	O
of	O
the	O
squared	O
loss	O
the	O
performance	O
of	O
a	O
particular	O
learning	B
algorithm	O
is	O
then	O
assessed	O
by	O
taking	O
the	O
average	O
over	O
this	O
ensemble	O
of	O
data	O
sets	O
d	O
takes	O
the	O
form	O
because	O
this	O
quantity	O
will	O
be	O
dependent	O
on	O
the	O
particular	O
data	O
set	O
d	O
we	O
take	O
its	O
average	O
over	O
the	O
ensemble	O
of	O
data	O
sets	O
if	O
we	O
add	O
and	O
subtract	O
the	O
quantity	O
edyxd	O
consider	O
the	O
integrand	O
of	O
the	O
first	O
term	O
in	O
which	O
for	O
a	O
particular	O
data	O
set	O
the	O
bias-variance	O
decomposition	O
inside	O
the	O
braces	O
and	O
then	O
expand	O
we	O
obtain	O
edyxd	O
edyxd	O
edyxdedyxd	O
hx	O
we	O
now	O
take	O
the	O
expectation	B
of	O
this	O
expression	O
with	O
respect	O
to	O
d	O
and	O
note	O
that	O
the	O
final	O
term	O
will	O
vanish	O
giving	O
ed	O
ed	O
variance	B
we	O
see	O
that	O
the	O
expected	O
squared	O
difference	O
between	O
yxd	O
and	O
the	O
regression	B
function	I
hx	O
can	O
be	O
expressed	O
as	O
the	O
sum	O
of	O
two	O
terms	O
the	O
first	O
term	O
called	O
the	O
squared	O
bias	B
represents	O
the	O
extent	O
to	O
which	O
the	O
average	O
prediction	O
over	O
all	O
data	O
sets	O
differs	O
from	O
the	O
desired	O
regression	B
function	I
the	O
second	O
term	O
called	O
the	O
variance	B
measures	O
the	O
extent	O
to	O
which	O
the	O
solutions	O
for	O
individual	O
data	O
sets	O
vary	O
around	O
their	O
average	O
and	O
hence	O
this	O
measures	O
the	O
extent	O
to	O
which	O
the	O
function	O
yxd	O
is	O
sensitive	O
to	O
the	O
particular	O
choice	O
of	O
data	O
set	O
we	O
shall	O
provide	O
some	O
intuition	O
to	O
support	O
these	O
definitions	O
shortly	O
when	O
we	O
consider	O
a	O
simple	O
example	O
so	O
far	O
we	O
have	O
considered	O
a	O
single	O
input	O
value	O
x	O
if	O
we	O
substitute	O
this	O
expansion	O
back	O
into	O
we	O
obtain	O
the	O
following	O
decomposition	O
of	O
the	O
expected	O
squared	O
loss	O
expected	O
loss	O
variance	B
noise	O
where	O
variance	B
noise	O
dx	O
ed	O
t	O
dx	O
dt	O
px	O
dx	O
and	O
the	O
bias	B
and	O
variance	B
terms	O
now	O
refer	O
to	O
integrated	O
quantities	O
our	O
goal	O
is	O
to	O
minimize	O
the	O
expected	O
loss	O
which	O
we	O
have	O
decomposed	O
into	O
the	O
sum	O
of	O
a	O
bias	B
a	O
variance	B
and	O
a	O
constant	O
noise	O
term	O
as	O
we	O
shall	O
see	O
there	O
is	O
a	O
trade-off	O
between	O
bias	B
and	O
variance	B
with	O
very	O
flexible	O
models	O
having	O
low	O
bias	B
and	O
high	O
variance	B
and	O
relatively	O
rigid	O
models	O
having	O
high	O
bias	B
and	O
low	O
variance	B
the	O
model	O
with	O
the	O
optimal	O
predictive	O
capability	O
is	O
the	O
one	O
that	O
leads	O
to	O
the	O
best	O
balance	O
between	O
bias	B
and	O
variance	B
this	O
is	O
illustrated	O
by	O
considering	O
the	O
sinusoidal	B
data	I
set	O
from	O
chapter	O
here	O
we	O
generate	O
data	O
sets	O
each	O
containing	O
n	O
data	O
points	O
independently	O
from	O
the	O
sinusoidal	O
curve	O
hx	O
x	O
the	O
data	O
sets	O
are	O
indexed	O
by	O
l	O
l	O
where	O
l	O
and	O
for	O
each	O
data	O
set	O
dl	O
we	O
appendix	O
a	O
linear	O
models	O
for	B
regression	B
t	O
t	O
t	O
ln	O
t	O
x	O
x	O
ln	O
t	O
x	O
x	O
ln	O
t	O
x	O
x	O
figure	O
illustration	O
of	O
the	O
dependence	O
of	O
bias	B
and	O
variance	B
on	O
model	O
complexity	O
governed	O
by	O
a	O
regularization	B
parameter	O
using	O
the	O
sinusoidal	B
data	I
set	O
from	O
chapter	O
there	O
are	O
l	O
data	O
sets	O
each	O
having	O
n	O
data	O
points	O
and	O
there	O
are	O
gaussian	B
basis	O
functions	O
in	O
the	O
model	O
so	O
that	O
the	O
total	O
number	O
of	O
parameters	O
is	O
m	O
including	O
the	O
bias	B
parameter	I
the	O
left	O
column	O
shows	O
the	O
result	O
of	O
fitting	O
the	O
model	O
to	O
the	O
data	O
sets	O
for	O
various	O
values	O
of	O
ln	O
clarity	O
only	O
of	O
the	O
fits	O
are	O
shown	O
the	O
right	O
column	O
shows	O
the	O
corresponding	O
average	O
of	O
the	O
fits	O
along	O
with	O
the	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
sets	O
were	O
generated	O
the	O
bias-variance	O
decomposition	O
figure	O
plot	O
of	O
squared	O
bias	B
and	O
variance	B
together	O
with	O
their	O
sum	O
corresponding	O
to	O
the	O
results	O
shown	O
in	O
figure	O
also	O
shown	O
is	O
the	O
average	O
test	B
set	I
error	B
for	O
a	O
test	O
data	O
set	O
size	O
of	O
points	O
the	O
minimum	O
value	O
of	O
variance	B
occurs	O
around	O
ln	O
which	O
is	O
close	O
to	O
the	O
value	O
that	O
gives	O
the	O
minimum	O
error	B
on	O
the	O
test	O
data	O
variance	B
variance	B
test	O
error	B
ln	O
fit	O
a	O
model	O
with	O
gaussian	B
basis	O
functions	O
by	O
minimizing	O
the	O
regularized	O
error	B
function	I
to	O
give	O
a	O
prediction	O
function	O
ylx	O
as	O
shown	O
in	O
figure	O
the	O
top	O
row	O
corresponds	O
to	O
a	O
large	O
value	O
of	O
the	O
regularization	B
coefficient	O
that	O
gives	O
low	O
variance	B
the	O
red	O
curves	O
in	O
the	O
left	O
plot	O
look	O
similar	O
but	O
high	O
bias	B
the	O
two	O
curves	O
in	O
the	O
right	O
plot	O
are	O
very	O
different	O
conversely	O
on	O
the	O
bottom	O
row	O
for	O
which	O
is	O
small	O
there	O
is	O
large	O
variance	B
by	O
the	O
high	O
variability	O
between	O
the	O
red	O
curves	O
in	O
the	O
left	O
plot	O
but	O
low	O
bias	B
by	O
the	O
good	O
fit	O
between	O
the	O
average	O
model	O
fit	O
and	O
the	O
original	O
sinusoidal	O
function	O
note	O
that	O
the	O
result	O
of	O
averaging	O
many	O
solutions	O
for	O
the	O
complex	O
model	O
with	O
m	O
is	O
a	O
very	O
good	O
fit	O
to	O
the	O
regression	B
function	I
which	O
suggests	O
that	O
averaging	O
may	O
be	O
a	O
beneficial	O
procedure	O
indeed	O
a	O
weighted	O
averaging	O
of	O
multiple	O
solutions	O
lies	O
at	O
the	O
heart	O
of	O
a	O
bayesian	B
approach	O
although	O
the	O
averaging	O
is	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
parameters	O
not	O
with	O
respect	O
to	O
multiple	O
data	O
sets	O
we	O
can	O
also	O
examine	O
the	O
bias-variance	B
trade-off	I
quantitatively	O
for	O
this	O
example	O
the	O
average	O
prediction	O
is	O
estimated	O
from	O
yx	O
l	O
ylx	O
and	O
the	O
integrated	O
squared	O
bias	B
and	O
integrated	O
variance	B
are	O
then	O
given	O
by	O
l	O
variance	B
n	O
n	O
ylxn	O
yxn	O
where	O
the	O
integral	O
over	O
x	O
weighted	O
by	O
the	O
distribution	O
px	O
is	O
approximated	O
by	O
a	O
finite	O
sum	O
over	O
data	O
points	O
drawn	O
from	O
that	O
distribution	O
these	O
quantities	O
along	O
with	O
their	O
sum	O
are	O
plotted	O
as	O
a	O
function	O
of	O
ln	O
in	O
figure	O
we	O
see	O
that	O
small	O
values	O
of	O
allow	O
the	O
model	O
to	O
become	O
finely	O
tuned	O
to	O
the	O
noise	O
on	O
each	O
individual	O
linear	O
models	O
for	B
regression	B
data	O
set	O
leading	O
to	O
large	O
variance	B
conversely	O
a	O
large	O
value	O
of	O
pulls	O
the	O
weight	O
parameters	O
towards	O
zero	O
leading	O
to	O
large	O
bias	B
although	O
the	O
bias-variance	O
decomposition	O
may	O
provide	O
some	O
interesting	O
insights	O
into	O
the	O
model	O
complexity	O
issue	O
from	O
a	O
frequentist	B
perspective	O
it	O
is	O
of	O
limited	O
practical	O
value	O
because	O
the	O
bias-variance	O
decomposition	O
is	O
based	O
on	O
averages	O
with	O
respect	O
to	O
ensembles	O
of	O
data	O
sets	O
whereas	O
in	O
practice	O
we	O
have	O
only	O
the	O
single	O
observed	O
data	O
set	O
if	O
we	O
had	O
a	O
large	O
number	O
of	O
independent	B
training	B
sets	O
of	O
a	O
given	O
size	O
we	O
would	O
be	O
better	O
off	O
combining	O
them	O
into	O
a	O
single	O
large	O
training	B
set	I
which	O
of	O
course	O
would	O
reduce	O
the	O
level	O
of	O
over-fitting	B
for	O
a	O
given	O
model	O
complexity	O
given	O
these	O
limitations	O
we	O
turn	O
in	O
the	O
next	O
section	O
to	O
a	O
bayesian	B
treatment	O
of	O
linear	O
basis	B
function	I
models	O
which	O
not	O
only	O
provides	O
powerful	O
insights	O
into	O
the	O
issues	O
of	O
over-fitting	B
but	O
which	O
also	O
leads	O
to	O
practical	O
techniques	O
for	O
addressing	O
the	O
question	O
model	O
complexity	O
bayesian	B
linear	B
regression	B
in	O
our	O
discussion	O
of	O
maximum	B
likelihood	I
for	O
setting	O
the	O
parameters	O
of	O
a	O
linear	B
regression	B
model	O
we	O
have	O
seen	O
that	O
the	O
effective	O
model	O
complexity	O
governed	O
by	O
the	O
number	O
of	O
basis	O
functions	O
needs	O
to	O
be	O
controlled	O
according	O
to	O
the	O
size	O
of	O
the	O
data	O
set	O
adding	O
a	O
regularization	B
term	O
to	O
the	O
log	O
likelihood	B
function	I
means	O
the	O
effective	O
model	O
complexity	O
can	O
then	O
be	O
controlled	O
by	O
the	O
value	O
of	O
the	O
regularization	B
coefficient	O
although	O
the	O
choice	O
of	O
the	O
number	O
and	O
form	O
of	O
the	O
basis	O
functions	O
is	O
of	O
course	O
still	O
important	O
in	O
determining	O
the	O
overall	O
behaviour	O
of	O
the	O
model	O
this	O
leaves	O
the	O
issue	O
of	O
deciding	O
the	O
appropriate	O
model	O
complexity	O
for	O
the	O
particular	O
problem	O
which	O
cannot	O
be	O
decided	O
simply	O
by	O
maximizing	O
the	O
likelihood	B
function	I
because	O
this	O
always	O
leads	O
to	O
excessively	O
complex	O
models	O
and	O
over-fitting	B
independent	B
hold-out	O
data	O
can	O
be	O
used	O
to	O
determine	O
model	O
complexity	O
as	O
discussed	O
in	O
section	O
but	O
this	O
can	O
be	O
both	O
computationally	O
expensive	O
and	O
wasteful	O
of	O
valuable	O
data	O
we	O
therefore	O
turn	O
to	O
a	O
bayesian	B
treatment	O
of	O
linear	B
regression	B
which	O
will	O
avoid	O
the	O
over-fitting	B
problem	O
of	O
maximum	B
likelihood	I
and	O
which	O
will	O
also	O
lead	O
to	O
automatic	O
methods	O
of	O
determining	O
model	O
complexity	O
using	O
the	O
training	B
data	O
alone	O
again	O
for	O
simplicity	O
we	O
will	O
focus	O
on	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
t	O
extension	O
to	O
multiple	O
target	O
variables	O
is	O
straightforward	O
and	O
follows	O
the	O
discussion	O
of	O
section	O
parameter	O
distribution	O
we	O
begin	O
our	O
discussion	O
of	O
the	O
bayesian	B
treatment	O
of	O
linear	B
regression	B
by	O
introducing	O
a	O
prior	B
probability	B
distribution	O
over	O
the	O
model	O
parameters	O
w	O
for	O
the	O
moment	O
we	O
shall	O
treat	O
the	O
noise	O
precision	B
parameter	I
as	O
a	O
known	O
constant	O
first	O
note	O
that	O
the	O
likelihood	B
function	I
ptw	O
defined	O
by	O
is	O
the	O
exponential	O
of	O
a	O
quadratic	O
function	O
of	O
w	O
the	O
corresponding	O
conjugate	B
prior	B
is	O
therefore	O
given	O
by	O
a	O
gaussian	B
distribution	O
of	O
the	O
form	O
pw	O
n	O
having	O
mean	B
and	O
covariance	B
exercise	O
exercise	O
bayesian	B
linear	B
regression	B
next	O
we	O
compute	O
the	O
posterior	O
distribution	O
which	O
is	O
proportional	O
to	O
the	O
product	O
of	O
the	O
likelihood	B
function	I
and	O
the	O
prior	B
due	O
to	O
the	O
choice	O
of	O
a	O
conjugate	B
gaussian	B
prior	B
distribution	O
the	O
posterior	O
will	O
also	O
be	O
gaussian	B
we	O
can	O
evaluate	O
this	O
distribution	O
by	O
the	O
usual	O
procedure	O
of	O
completing	B
the	I
square	I
in	O
the	O
exponential	O
and	O
then	O
finding	O
the	O
normalization	O
coefficient	O
using	O
the	O
standard	O
result	O
for	O
a	O
normalized	O
gaussian	B
however	O
we	O
have	O
already	O
done	O
the	O
necessary	O
work	O
in	O
deriving	O
the	O
general	O
result	O
which	O
allows	O
us	O
to	O
write	O
down	O
the	O
posterior	O
distribution	O
directly	O
in	O
the	O
form	O
pwt	O
n	O
sn	O
mn	O
sn	O
t	O
n	O
s	O
s	O
s	O
tt	O
where	O
note	O
that	O
because	O
the	O
posterior	O
distribution	O
is	O
gaussian	B
its	O
mode	O
coincides	O
with	O
its	O
mean	B
thus	O
the	O
maximum	B
posterior	I
weight	B
vector	I
is	O
simply	O
given	O
by	O
wmap	O
mn	O
with	O
the	O
mean	B
mn	O
if	O
we	O
consider	O
an	O
infinitely	O
broad	O
prior	B
of	O
the	O
posterior	O
distribution	O
reduces	O
to	O
the	O
maximum	B
likelihood	I
value	O
wml	O
given	O
by	O
similarly	O
if	O
n	O
then	O
the	O
posterior	O
distribution	O
reverts	O
to	O
the	O
prior	B
furthermore	O
if	O
data	O
points	O
arrive	O
sequentially	O
then	O
the	O
posterior	O
distribution	O
at	O
any	O
stage	O
acts	O
as	O
the	O
prior	B
distribution	O
for	O
the	O
subsequent	O
data	O
point	O
such	O
that	O
the	O
new	O
posterior	O
distribution	O
is	O
again	O
given	O
by	O
for	O
the	O
remainder	O
of	O
this	O
chapter	O
we	O
shall	O
consider	O
a	O
particular	O
form	O
of	O
gaussian	B
prior	B
in	O
order	O
to	O
simplify	O
the	O
treatment	O
specifically	O
we	O
consider	O
a	O
zero-mean	O
isotropic	B
gaussian	B
governed	O
by	O
a	O
single	O
precision	B
parameter	I
so	O
that	O
pw	O
n	O
and	O
the	O
corresponding	O
posterior	O
distribution	O
over	O
w	O
is	O
then	O
given	O
by	O
with	O
mn	O
sn	O
tt	O
n	O
i	O
t	O
s	O
the	O
log	O
of	O
the	O
posterior	O
distribution	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
log	O
likelihood	O
and	O
the	O
log	O
of	O
the	O
prior	B
and	O
as	O
a	O
function	O
of	O
w	O
takes	O
the	O
form	O
ln	O
pwt	O
wt	O
wtw	O
const	O
maximization	O
of	O
this	O
posterior	O
distribution	O
with	O
respect	O
to	O
w	O
is	O
therefore	O
equivalent	O
to	O
the	O
minimization	O
of	O
the	O
sum-of-squares	B
error	B
function	I
with	O
the	O
addition	O
of	O
a	O
quadratic	O
regularization	B
term	O
corresponding	O
to	O
with	O
we	O
can	O
illustrate	O
bayesian	B
learning	B
in	O
a	O
linear	O
basis	B
function	I
model	O
as	O
well	O
as	O
the	O
sequential	O
update	O
of	O
a	O
posterior	O
distribution	O
using	O
a	O
simple	O
example	O
involving	O
straight-line	O
fitting	O
consider	O
a	O
single	O
input	O
variable	O
x	O
a	O
single	O
target	O
variable	O
t	O
and	O
linear	O
models	O
for	B
regression	B
a	O
linear	O
model	O
of	O
the	O
form	O
yx	O
w	O
because	O
this	O
has	O
just	O
two	O
adaptive	O
parameters	O
we	O
can	O
plot	O
the	O
prior	B
and	O
posterior	O
distributions	O
directly	O
in	O
parameter	O
space	O
we	O
generate	O
synthetic	O
data	O
from	O
the	O
function	O
fx	O
a	O
with	O
parameter	O
values	O
and	O
by	O
first	O
choosing	O
values	O
of	O
xn	O
from	O
the	O
uniform	B
distribution	I
ux	O
then	O
evaluating	O
fxn	O
a	O
and	O
finally	O
adding	O
gaussian	B
noise	O
with	O
standard	B
deviation	I
of	O
to	O
obtain	O
the	O
target	O
values	O
tn	O
our	O
goal	O
is	O
to	O
recover	O
the	O
values	O
of	O
and	O
from	O
such	O
data	O
and	O
we	O
will	O
explore	O
the	O
dependence	O
on	O
the	O
size	O
of	O
the	O
data	O
set	O
we	O
assume	O
here	O
that	O
the	O
noise	O
variance	B
is	O
known	O
and	O
hence	O
we	O
set	O
the	O
precision	B
parameter	I
to	O
its	O
true	O
value	O
similarly	O
we	O
fix	O
the	O
parameter	O
to	O
we	O
shall	O
shortly	O
discuss	O
strategies	O
for	O
determining	O
and	O
from	O
the	O
training	B
data	O
figure	O
shows	O
the	O
results	O
of	O
bayesian	B
learning	B
in	O
this	O
model	O
as	O
the	O
size	O
of	O
the	O
data	O
set	O
is	O
increased	O
and	O
demonstrates	O
the	O
sequential	O
nature	O
of	O
bayesian	B
learning	B
in	O
which	O
the	O
current	O
posterior	O
distribution	O
forms	O
the	O
prior	B
when	O
a	O
new	O
data	O
point	O
is	O
observed	O
it	O
is	O
worth	O
taking	O
time	O
to	O
study	O
this	O
figure	O
in	O
detail	O
as	O
it	O
illustrates	O
several	O
important	O
aspects	O
of	O
bayesian	B
inference	B
the	O
first	O
row	O
of	O
this	O
figure	O
corresponds	O
to	O
the	O
situation	O
before	O
any	O
data	O
points	O
are	O
observed	O
and	O
shows	O
a	O
plot	O
of	O
the	O
prior	B
distribution	O
in	O
w	O
space	O
together	O
with	O
six	O
samples	O
of	O
the	O
function	O
yx	O
w	O
in	O
which	O
the	O
values	O
of	O
w	O
are	O
drawn	O
from	O
the	O
prior	B
in	O
the	O
second	O
row	O
we	O
see	O
the	O
situation	O
after	O
observing	O
a	O
single	O
data	O
point	O
the	O
location	O
t	O
of	O
the	O
data	O
point	O
is	O
shown	O
by	O
a	O
blue	O
circle	O
in	O
the	O
right-hand	O
column	O
in	O
the	O
left-hand	O
column	O
is	O
a	O
plot	O
of	O
the	O
likelihood	B
function	I
ptx	O
w	O
for	O
this	O
data	O
point	O
as	O
a	O
function	O
of	O
w	O
note	O
that	O
the	O
likelihood	B
function	I
provides	O
a	O
soft	B
constraint	O
that	O
the	O
line	O
must	O
pass	O
close	O
to	O
the	O
data	O
point	O
where	O
close	O
is	O
determined	O
by	O
the	O
noise	O
precision	O
for	O
comparison	O
the	O
true	O
parameter	O
values	O
and	O
used	O
to	O
generate	O
the	O
data	O
set	O
are	O
shown	O
by	O
a	O
white	O
cross	O
in	O
the	O
plots	O
in	O
the	O
left	O
column	O
of	O
figure	O
when	O
we	O
multiply	O
this	O
likelihood	B
function	I
by	O
the	O
prior	B
from	O
the	O
top	O
row	O
and	O
normalize	O
we	O
obtain	O
the	O
posterior	O
distribution	O
shown	O
in	O
the	O
middle	O
plot	O
on	O
the	O
second	O
row	O
samples	O
of	O
the	O
regression	B
function	I
yx	O
w	O
obtained	O
by	O
drawing	O
samples	O
of	O
w	O
from	O
this	O
posterior	O
distribution	O
are	O
shown	O
in	O
the	O
right-hand	O
plot	O
note	O
that	O
these	O
sample	O
lines	O
all	O
pass	O
close	O
to	O
the	O
data	O
point	O
the	O
third	O
row	O
of	O
this	O
figure	O
shows	O
the	O
effect	O
of	O
observing	O
a	O
second	O
data	O
point	O
again	O
shown	O
by	O
a	O
blue	O
circle	O
in	O
the	O
plot	O
in	O
the	O
right-hand	O
column	O
the	O
corresponding	O
likelihood	B
function	I
for	O
this	O
second	O
data	O
point	O
alone	O
is	O
shown	O
in	O
the	O
left	O
plot	O
when	O
we	O
multiply	O
this	O
likelihood	B
function	I
by	O
the	O
posterior	O
distribution	O
from	O
the	O
second	O
row	O
we	O
obtain	O
the	O
posterior	O
distribution	O
shown	O
in	O
the	O
middle	O
plot	O
of	O
the	O
third	O
row	O
note	O
that	O
this	O
is	O
exactly	O
the	O
same	O
posterior	O
distribution	O
as	O
would	O
be	O
obtained	O
by	O
combining	O
the	O
original	O
prior	B
with	O
the	O
likelihood	B
function	I
for	O
the	O
two	O
data	O
points	O
this	O
posterior	O
has	O
now	O
been	O
influenced	O
by	O
two	O
data	O
points	O
and	O
because	O
two	O
points	O
are	O
sufficient	O
to	O
define	O
a	O
line	O
this	O
already	O
gives	O
a	O
relatively	O
compact	O
posterior	O
distribution	O
samples	O
from	O
this	O
posterior	O
distribution	O
give	O
rise	O
to	O
the	O
functions	O
shown	O
in	O
red	O
in	O
the	O
third	O
column	O
and	O
we	O
see	O
that	O
these	O
functions	O
pass	O
close	O
to	O
both	O
of	O
the	O
data	O
points	O
the	O
fourth	O
row	O
shows	O
the	O
effect	O
of	O
observing	O
a	O
total	O
of	O
data	O
points	O
the	O
left-hand	O
plot	O
shows	O
the	O
likelihood	B
function	I
for	O
the	O
data	O
point	O
alone	O
and	O
the	O
middle	O
plot	O
shows	O
the	O
resulting	O
posterior	O
distribution	O
that	O
has	O
now	O
absorbed	O
information	O
from	O
all	O
observations	O
note	O
how	O
the	O
posterior	O
is	O
much	O
sharper	O
than	O
in	O
the	O
third	O
row	O
in	O
the	O
limit	O
of	O
an	O
infinite	O
number	O
of	O
data	O
points	O
the	O
bayesian	B
linear	B
regression	B
figure	O
illustration	O
of	O
sequential	O
bayesian	B
learning	B
for	O
a	O
simple	O
linear	O
model	O
of	O
the	O
form	O
yx	O
w	O
a	O
detailed	O
description	O
of	O
this	O
figure	O
is	O
given	O
in	O
the	O
text	O
linear	O
models	O
for	B
regression	B
posterior	O
distribution	O
would	O
become	O
a	O
delta	O
function	O
centred	O
on	O
the	O
true	O
parameter	O
values	O
shown	O
by	O
the	O
white	O
cross	O
other	O
forms	O
of	O
prior	B
over	O
the	O
parameters	O
can	O
be	O
considered	O
for	O
instance	O
we	O
can	O
generalize	O
the	O
gaussian	B
prior	B
to	O
give	O
pw	O
q	O
exp	O
in	O
which	O
q	O
corresponds	O
to	O
the	O
gaussian	B
distribution	O
and	O
only	O
in	O
this	O
case	O
is	O
the	O
prior	B
conjugate	B
to	O
the	O
likelihood	B
function	I
finding	O
the	O
maximum	O
of	O
the	O
posterior	O
distribution	O
over	O
w	O
corresponds	O
to	O
minimization	O
of	O
the	O
regularized	O
error	B
function	I
in	O
the	O
case	O
of	O
the	O
gaussian	B
prior	B
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
was	O
equal	O
to	O
the	O
mean	B
although	O
this	O
will	O
no	O
longer	O
hold	O
if	O
q	O
predictive	B
distribution	I
in	O
practice	O
we	O
are	O
not	O
usually	O
interested	O
in	O
the	O
value	O
of	O
w	O
itself	O
but	O
rather	O
in	O
making	O
predictions	O
of	O
t	O
for	O
new	O
values	O
of	O
x	O
this	O
requires	O
that	O
we	O
evaluate	O
the	O
predictive	B
distribution	I
defined	O
by	O
ptt	O
ptw	O
dw	O
exercise	O
exercise	O
in	O
which	O
t	O
is	O
the	O
vector	O
of	O
target	O
values	O
from	O
the	O
training	B
set	I
and	O
we	O
have	O
omitted	O
the	O
corresponding	O
input	O
vectors	O
from	O
the	O
right-hand	O
side	O
of	O
the	O
conditioning	O
statements	O
to	O
simplify	O
the	O
notation	O
the	O
conditional	B
distribution	O
ptx	O
w	O
of	O
the	O
target	O
variable	O
is	O
given	O
by	O
and	O
the	O
posterior	O
weight	O
distribution	O
is	O
given	O
by	O
we	O
see	O
that	O
involves	O
the	O
convolution	O
of	O
two	O
gaussian	B
distributions	O
and	O
so	O
making	O
use	O
of	O
the	O
result	O
from	O
section	O
we	O
see	O
that	O
the	O
predictive	B
distribution	I
takes	O
the	O
form	O
where	O
the	O
variance	B
ptx	O
t	O
n	O
n	O
of	O
the	O
predictive	B
distribution	I
is	O
given	O
by	O
n	O
n	O
n	O
the	O
first	O
term	O
in	O
represents	O
the	O
noise	O
on	O
the	O
data	O
whereas	O
the	O
second	O
term	O
reflects	O
the	O
uncertainty	O
associated	O
with	O
the	O
parameters	O
w	O
because	O
the	O
noise	O
process	O
and	O
the	O
distribution	O
of	O
w	O
are	O
independent	B
gaussians	O
their	O
variances	O
are	O
additive	O
note	O
that	O
as	O
additional	O
data	O
points	O
are	O
observed	O
the	O
posterior	O
distribution	O
becomes	O
n	O
narrower	O
as	O
a	O
consequence	O
it	O
can	O
be	O
shown	O
et	O
al	O
that	O
n	O
in	O
the	O
limit	O
n	O
the	O
second	O
term	O
in	O
goes	O
to	O
zero	O
and	O
the	O
variance	B
of	O
the	O
predictive	B
distribution	I
arises	O
solely	O
from	O
the	O
additive	O
noise	O
governed	O
by	O
the	O
parameter	O
as	O
an	O
illustration	O
of	O
the	O
predictive	B
distribution	I
for	O
bayesian	B
linear	B
regression	B
models	O
let	O
us	O
return	O
to	O
the	O
synthetic	O
sinusoidal	B
data	I
set	O
of	O
section	O
in	O
figure	O
t	O
t	O
bayesian	B
linear	B
regression	B
t	O
x	O
x	O
t	O
x	O
x	O
figure	O
examples	O
of	O
the	O
predictive	B
distribution	I
for	O
a	O
model	O
consisting	O
of	O
gaussian	B
basis	O
functions	O
of	O
the	O
form	O
using	O
the	O
synthetic	O
sinusoidal	B
data	I
set	O
of	O
section	O
see	O
the	O
text	O
for	O
a	O
detailed	O
discussion	O
we	O
fit	O
a	O
model	O
comprising	O
a	O
linear	O
combination	O
of	O
gaussian	B
basis	O
functions	O
to	O
data	O
sets	O
of	O
various	O
sizes	O
and	O
then	O
look	O
at	O
the	O
corresponding	O
posterior	O
distributions	O
here	O
the	O
green	O
curves	O
correspond	O
to	O
the	O
function	O
x	O
from	O
which	O
the	O
data	O
points	O
were	O
generated	O
the	O
addition	O
of	O
gaussian	B
noise	O
data	O
sets	O
of	O
size	O
n	O
n	O
n	O
and	O
n	O
are	O
shown	O
in	O
the	O
four	O
plots	O
by	O
the	O
blue	O
circles	O
for	O
each	O
plot	O
the	O
red	O
curve	O
shows	O
the	O
mean	B
of	O
the	O
corresponding	O
gaussian	B
predictive	B
distribution	I
and	O
the	O
red	O
shaded	O
region	O
spans	O
one	O
standard	B
deviation	I
either	O
side	O
of	O
the	O
mean	B
note	O
that	O
the	O
predictive	O
uncertainty	O
depends	O
on	O
x	O
and	O
is	O
smallest	O
in	O
the	O
neighbourhood	O
of	O
the	O
data	O
points	O
also	O
note	O
that	O
the	O
level	O
of	O
uncertainty	O
decreases	O
as	O
more	O
data	O
points	O
are	O
observed	O
the	O
plots	O
in	O
figure	O
only	O
show	O
the	O
point-wise	O
predictive	O
variance	B
as	O
a	O
function	O
of	O
x	O
in	O
order	O
to	O
gain	O
insight	O
into	O
the	O
covariance	B
between	O
the	O
predictions	O
at	O
different	O
values	O
of	O
x	O
we	O
can	O
draw	O
samples	O
from	O
the	O
posterior	O
distribution	O
over	O
w	O
and	O
then	O
plot	O
the	O
corresponding	O
functions	O
yx	O
w	O
as	O
shown	O
in	O
figure	O
linear	O
models	O
for	B
regression	B
t	O
t	O
t	O
x	O
x	O
t	O
x	O
x	O
figure	O
plots	O
of	O
the	O
function	O
yx	O
w	O
using	O
samples	O
from	O
the	O
posterior	O
distributions	O
over	O
w	O
corresponding	O
to	O
the	O
plots	O
in	O
figure	O
if	O
we	O
used	O
localized	O
basis	O
functions	O
such	O
as	O
gaussians	O
then	O
in	O
regions	O
away	O
from	O
the	O
basis	B
function	I
centres	O
the	O
contribution	O
from	O
the	O
second	O
term	O
in	O
the	O
predic	O
thus	O
tive	O
variance	B
will	O
go	O
to	O
zero	O
leaving	O
only	O
the	O
noise	O
contribution	O
the	O
model	O
becomes	O
very	O
confident	O
in	O
its	O
predictions	O
when	O
extrapolating	O
outside	O
the	O
region	O
occupied	O
by	O
the	O
basis	O
functions	O
which	O
is	O
generally	O
an	O
undesirable	O
behaviour	O
this	O
problem	O
can	O
be	O
avoided	O
by	O
adopting	O
an	O
alternative	O
bayesian	B
approach	O
to	O
regression	B
known	O
as	O
a	O
gaussian	B
process	I
note	O
that	O
if	O
both	O
w	O
and	O
are	O
treated	O
as	O
unknown	O
then	O
we	O
can	O
introduce	O
a	O
conjugate	B
prior	B
distribution	O
pw	O
that	O
from	O
the	O
discussion	O
in	O
section	O
will	O
be	O
given	O
by	O
a	O
gaussian-gamma	B
distribution	I
et	O
al	O
in	O
this	O
case	O
the	O
predictive	B
distribution	I
is	O
a	O
student	O
s	O
t-distribution	O
section	O
exercise	O
exercise	O
bayesian	B
linear	B
regression	B
figure	O
the	O
equivalent	B
kernel	I
kx	O
for	O
the	O
gaussian	B
basis	O
functions	O
in	O
figure	O
shown	O
as	O
a	O
plot	O
of	O
x	O
versus	O
together	O
with	O
three	O
slices	O
through	O
this	O
matrix	O
corresponding	O
to	O
three	O
different	O
values	O
of	O
x	O
the	O
data	O
set	O
used	O
to	O
generate	O
this	O
kernel	O
comprised	O
values	O
of	O
x	O
equally	O
spaced	O
over	O
the	O
interval	O
equivalent	B
kernel	I
the	O
posterior	O
mean	B
solution	O
for	O
the	O
linear	O
basis	B
function	I
model	O
has	O
an	O
interesting	O
interpretation	O
that	O
will	O
set	O
the	O
stage	O
for	O
kernel	O
methods	O
including	O
gaussian	B
processes	O
if	O
we	O
substitute	O
into	O
the	O
expression	O
we	O
see	O
that	O
the	O
predictive	O
mean	B
can	O
be	O
written	O
in	O
the	O
form	O
chapter	O
yx	O
mn	O
mt	O
n	O
tt	O
where	O
sn	O
is	O
defined	O
by	O
thus	O
the	O
mean	B
of	O
the	O
predictive	B
distribution	I
at	O
a	O
point	O
x	O
is	O
given	O
by	O
a	O
linear	O
combination	O
of	O
the	O
training	B
set	I
target	O
variables	O
tn	O
so	O
that	O
we	O
can	O
write	O
yx	O
mn	O
kx	O
xntn	O
where	O
the	O
function	O
kx	O
is	O
known	O
as	O
the	O
smoother	B
matrix	I
or	O
the	O
equivalent	B
kernel	B
regression	B
functions	O
such	O
as	O
this	O
which	O
make	O
predictions	O
by	O
taking	O
linear	O
combinations	O
of	O
the	O
training	B
set	I
target	O
values	O
are	O
known	O
as	O
linear	O
smoothers	O
note	O
that	O
the	O
equivalent	B
kernel	I
depends	O
on	O
the	O
input	O
values	O
xn	O
from	O
the	O
data	O
set	O
because	O
these	O
appear	O
in	O
the	O
definition	O
of	O
sn	O
the	O
equivalent	B
kernel	I
is	O
illustrated	O
for	O
the	O
case	O
of	O
gaussian	B
basis	O
functions	O
in	O
have	O
been	O
plotted	O
as	O
a	O
function	O
of	O
figure	O
in	O
which	O
the	O
kernel	O
functions	O
kx	O
x	O
for	O
three	O
different	O
values	O
of	O
x	O
we	O
see	O
that	O
they	O
are	O
localized	O
around	O
x	O
and	O
so	O
the	O
x	O
mean	B
of	O
the	O
predictive	B
distribution	I
at	O
x	O
given	O
by	O
yx	O
mn	O
is	O
obtained	O
by	O
forming	O
a	O
weighted	O
combination	O
of	O
the	O
target	O
values	O
in	O
which	O
data	O
points	O
close	O
to	O
x	O
are	O
given	O
higher	O
weight	O
than	O
points	O
further	O
removed	O
from	O
x	O
intuitively	O
it	O
seems	O
reasonable	O
that	O
we	O
should	O
weight	O
local	B
evidence	O
more	O
strongly	O
than	O
distant	O
evidence	O
note	O
that	O
this	O
localization	O
property	O
holds	O
not	O
only	O
for	O
the	O
localized	O
gaussian	B
basis	O
functions	O
but	O
also	O
for	O
the	O
nonlocal	O
polynomial	O
and	O
sigmoidal	O
basis	O
functions	O
as	O
illustrated	O
in	O
figure	O
linear	O
models	O
for	B
regression	B
figure	O
examples	O
of	O
equivalent	O
kernels	O
kx	O
for	O
x	O
plotted	O
as	O
a	O
function	O
of	O
corresponding	O
to	O
the	O
polynomial	O
basis	O
functions	O
and	O
to	O
the	O
sigmoidal	O
basis	O
functions	O
shown	O
in	O
figure	O
note	O
that	O
these	O
are	O
localized	O
functions	O
of	O
even	O
though	O
the	O
corresponding	O
basis	O
functions	O
are	O
nonlocal	O
further	O
insight	O
into	O
the	O
role	O
of	O
the	O
equivalent	B
kernel	I
can	O
be	O
obtained	O
by	O
consid	O
ering	O
the	O
covariance	B
between	O
yx	O
and	O
which	O
is	O
given	O
by	O
covyx	O
cov	O
wt	O
where	O
we	O
have	O
made	O
use	O
of	O
and	O
from	O
the	O
form	O
of	O
the	O
equivalent	B
kernel	I
we	O
see	O
that	O
the	O
predictive	O
mean	B
at	O
nearby	O
points	O
will	O
be	O
highly	O
correlated	O
whereas	O
for	O
more	O
distant	O
pairs	O
of	O
points	O
the	O
correlation	O
will	O
be	O
smaller	O
the	O
predictive	B
distribution	I
shown	O
in	O
figure	O
allows	O
us	O
to	O
visualize	O
the	O
pointwise	O
uncertainty	O
in	O
the	O
predictions	O
governed	O
by	O
however	O
by	O
drawing	O
samples	O
from	O
the	O
posterior	O
distribution	O
over	O
w	O
and	O
plotting	O
the	O
corresponding	O
model	O
functions	O
yx	O
w	O
as	O
in	O
figure	O
we	O
are	O
visualizing	O
the	O
joint	O
uncertainty	O
in	O
the	O
posterior	O
distribution	O
between	O
the	O
y	O
values	O
at	O
two	O
more	O
x	O
values	O
as	O
governed	O
by	O
the	O
equivalent	B
kernel	I
the	O
formulation	O
of	O
linear	B
regression	B
in	O
terms	O
of	O
a	O
kernel	B
function	I
suggests	O
an	O
alternative	O
approach	O
to	O
regression	B
as	O
follows	O
instead	O
of	O
introducing	O
a	O
set	O
of	O
basis	O
functions	O
which	O
implicitly	O
determines	O
an	O
equivalent	B
kernel	I
we	O
can	O
instead	O
define	O
a	O
localized	O
kernel	O
directly	O
and	O
use	O
this	O
to	O
make	O
predictions	O
for	O
new	O
input	O
vectors	O
x	O
given	O
the	O
observed	O
training	B
set	I
this	O
leads	O
to	O
a	O
practical	O
framework	O
for	B
regression	B
classification	B
called	O
gaussian	B
processes	O
which	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
section	O
we	O
have	O
seen	O
that	O
the	O
effective	O
kernel	O
defines	O
the	O
weights	O
by	O
which	O
the	O
training	B
set	I
target	O
values	O
are	O
combined	O
in	O
order	O
to	O
make	O
a	O
prediction	O
at	O
a	O
new	O
value	O
of	O
x	O
and	O
it	O
can	O
be	O
shown	O
that	O
these	O
weights	O
sum	O
to	O
one	O
in	O
other	O
words	O
kx	O
xn	O
exercise	O
by	O
noting	O
that	O
the	O
summation	O
is	O
equivalent	O
to	O
considering	O
the	O
predictive	O
for	O
all	O
values	O
of	O
x	O
this	O
intuitively	O
pleasing	O
result	O
can	O
easily	O
be	O
proven	O
informally	O
for	O
a	O
set	O
of	O
target	O
data	O
in	O
which	O
tn	O
for	O
all	O
n	O
provided	O
the	O
basis	O
functions	O
are	O
linearly	O
independent	B
that	O
there	O
are	O
more	O
data	O
points	O
than	O
basis	O
functions	O
and	O
that	O
one	O
of	O
the	O
basis	O
functions	O
is	O
constant	O
to	O
the	O
bias	B
parameter	I
then	O
it	O
is	O
clear	O
that	O
we	O
can	O
fit	O
the	O
training	B
data	O
exactly	O
and	O
hence	O
that	O
the	O
predictive	O
mean	B
will	O
be	O
from	O
which	O
we	O
obtain	O
note	O
that	O
the	O
kernel	B
function	I
can	O
bayesian	B
model	B
comparison	I
chapter	O
be	O
negative	O
as	O
well	O
as	O
positive	O
so	O
although	O
it	O
satisfies	O
a	O
summation	O
constraint	O
the	O
corresponding	O
predictions	O
are	O
not	O
necessarily	O
convex	O
combinations	O
of	O
the	O
training	B
set	I
target	O
variables	O
finally	O
we	O
note	O
that	O
the	O
equivalent	B
kernel	I
satisfies	O
an	O
important	O
property	O
shared	O
by	O
kernel	O
functions	O
in	O
general	O
namely	O
that	O
it	O
can	O
be	O
expressed	O
in	O
the	O
form	O
an	O
inner	O
product	O
with	O
respect	O
to	O
a	O
vector	O
of	O
nonlinear	O
functions	O
so	O
that	O
kx	O
z	O
where	O
n	O
bayesian	B
model	B
comparison	I
in	O
chapter	O
we	O
highlighted	O
the	O
problem	O
of	O
over-fitting	B
as	O
well	O
as	O
the	O
use	O
of	O
crossvalidation	O
as	O
a	O
technique	O
for	O
setting	O
the	O
values	O
of	O
regularization	B
parameters	O
or	O
for	O
choosing	O
between	O
alternative	O
models	O
here	O
we	O
consider	O
the	O
problem	O
of	O
model	B
selection	I
from	O
a	O
bayesian	B
perspective	O
in	O
this	O
section	O
our	O
discussion	O
will	O
be	O
very	O
general	O
and	O
then	O
in	O
section	O
we	O
shall	O
see	O
how	O
these	O
ideas	O
can	O
be	O
applied	O
to	O
the	O
determination	O
of	O
regularization	B
parameters	O
in	O
linear	B
regression	B
as	O
we	O
shall	O
see	O
the	O
over-fitting	B
associated	O
with	O
maximum	B
likelihood	I
can	O
be	O
avoided	O
by	O
marginalizing	O
or	O
integrating	O
over	O
the	O
model	O
parameters	O
instead	O
of	O
making	O
point	O
estimates	O
of	O
their	O
values	O
models	O
can	O
then	O
be	O
compared	O
directly	O
on	O
the	O
training	B
data	O
without	O
the	O
need	O
for	O
a	O
validation	B
set	I
this	O
allows	O
all	O
available	O
data	O
to	O
be	O
used	O
for	O
training	B
and	O
avoids	O
the	O
multiple	O
training	B
runs	O
for	O
each	O
model	O
associated	O
with	O
cross-validation	B
it	O
also	O
allows	O
multiple	O
complexity	O
parameters	O
to	O
be	O
determined	O
simultaneously	O
as	O
part	O
of	O
the	O
training	B
process	O
for	O
example	O
in	O
chapter	O
we	O
shall	O
introduce	O
the	O
relevance	B
vector	I
machine	I
which	O
is	O
a	O
bayesian	B
model	O
having	O
one	O
complexity	O
parameter	O
for	O
every	O
training	B
data	O
point	O
section	O
the	O
bayesian	B
view	O
of	O
model	B
comparison	I
simply	O
involves	O
the	O
use	O
of	O
probabilities	O
to	O
represent	O
uncertainty	O
in	O
the	O
choice	O
of	O
model	O
along	O
with	O
a	O
consistent	B
application	O
of	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
suppose	O
we	O
wish	O
to	O
compare	O
a	O
set	O
of	O
l	O
models	O
where	O
i	O
l	O
here	O
a	O
model	O
refers	O
to	O
a	O
probability	B
distribution	O
over	O
the	O
observed	O
data	O
d	O
in	O
the	O
case	O
of	O
the	O
polynomial	O
curve-fitting	O
problem	O
the	O
distribution	O
is	O
defined	O
over	O
the	O
set	O
of	O
target	O
values	O
t	O
while	O
the	O
set	O
of	O
input	O
values	O
x	O
is	O
assumed	O
to	O
be	O
known	O
other	O
types	O
of	O
model	O
define	O
a	O
joint	O
distributions	O
over	O
x	O
and	O
t	O
we	O
shall	O
suppose	O
that	O
the	O
data	O
is	O
generated	O
from	O
one	O
of	O
these	O
models	O
but	O
we	O
are	O
uncertain	O
which	O
one	O
our	O
uncertainty	O
is	O
expressed	O
through	O
a	O
prior	B
probability	B
distribution	O
pmi	O
given	O
a	O
training	B
set	I
d	O
we	O
then	O
wish	O
to	O
evaluate	O
the	O
posterior	O
distribution	O
the	O
prior	B
allows	O
us	O
to	O
express	O
a	O
preference	O
for	O
different	O
models	O
let	O
us	O
simply	O
assume	O
that	O
all	O
models	O
are	O
given	O
equal	O
prior	B
probability	B
the	O
interesting	O
term	O
is	O
the	O
model	B
evidence	I
pdmi	O
which	O
expresses	O
the	O
preference	O
shown	O
by	O
the	O
data	O
for	O
pmid	O
pmipdmi	O
linear	O
models	O
for	B
regression	B
different	O
models	O
and	O
we	O
shall	O
examine	O
this	O
term	O
in	O
more	O
detail	O
shortly	O
the	O
model	B
evidence	I
is	O
sometimes	O
also	O
called	O
the	O
marginal	B
likelihood	I
because	O
it	O
can	O
be	O
viewed	O
as	O
a	O
likelihood	B
function	I
over	O
the	O
space	O
of	O
models	O
in	O
which	O
the	O
parameters	O
have	O
been	O
marginalized	O
out	O
the	O
ratio	O
of	O
model	O
evidences	O
pdmipdmj	O
for	O
two	O
models	O
is	O
known	O
as	O
a	O
bayes	B
factor	O
and	O
raftery	O
once	O
we	O
know	O
the	O
posterior	O
distribution	O
over	O
models	O
the	O
predictive	B
distribution	I
is	O
given	O
from	O
the	O
sum	O
and	O
product	O
rules	O
by	O
ptxd	O
ptxmidpmid	O
this	O
is	O
an	O
example	O
of	O
a	O
mixture	B
distribution	I
in	O
which	O
the	O
overall	O
predictive	B
distribution	I
is	O
obtained	O
by	O
averaging	O
the	O
predictive	O
distributions	O
ptxmid	O
of	O
individual	O
models	O
weighted	O
by	O
the	O
posterior	O
probabilities	O
pmid	O
of	O
those	O
models	O
for	O
instance	O
if	O
we	O
have	O
two	O
models	O
that	O
are	O
a-posteriori	O
equally	O
likely	O
and	O
one	O
predicts	O
a	O
narrow	O
distribution	O
around	O
t	O
a	O
while	O
the	O
other	O
predicts	O
a	O
narrow	O
distribution	O
around	O
t	O
b	O
the	O
overall	O
predictive	B
distribution	I
will	O
be	O
a	O
bimodal	O
distribution	O
with	O
modes	O
at	O
t	O
a	O
and	O
t	O
b	O
not	O
a	O
single	O
model	O
at	O
t	O
a	O
simple	O
approximation	O
to	O
model	B
averaging	I
is	O
to	O
use	O
the	O
single	O
most	O
probable	O
model	O
alone	O
to	O
make	O
predictions	O
this	O
is	O
known	O
as	O
model	B
selection	I
for	O
a	O
model	O
governed	O
by	O
a	O
set	O
of	O
parameters	O
w	O
the	O
model	B
evidence	I
is	O
given	O
from	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
by	O
pdmi	O
pdwmipwmi	O
dw	O
chapter	O
from	O
a	O
sampling	O
perspective	O
the	O
marginal	B
likelihood	I
can	O
be	O
viewed	O
as	O
the	O
probability	B
of	O
generating	O
the	O
data	O
set	O
d	O
from	O
a	O
model	O
whose	O
parameters	O
are	O
sampled	O
at	O
random	O
from	O
the	O
prior	B
it	O
is	O
also	O
interesting	O
to	O
note	O
that	O
the	O
evidence	O
is	O
precisely	O
the	O
normalizing	O
term	O
that	O
appears	O
in	O
the	O
denominator	O
in	O
bayes	B
theorem	O
when	O
evaluating	O
the	O
posterior	O
distribution	O
over	O
parameters	O
because	O
pwdmi	O
pdwmipwmi	O
pdmi	O
we	O
can	O
obtain	O
some	O
insight	O
into	O
the	O
model	B
evidence	I
by	O
making	O
a	O
simple	O
approximation	O
to	O
the	O
integral	O
over	O
parameters	O
consider	O
first	O
the	O
case	O
of	O
a	O
model	O
having	O
a	O
single	O
parameter	O
w	O
the	O
posterior	O
distribution	O
over	O
parameters	O
is	O
proportional	O
to	O
pdwpw	O
where	O
we	O
omit	O
the	O
dependence	O
on	O
the	O
model	O
mi	O
to	O
keep	O
the	O
notation	O
uncluttered	O
if	O
we	O
assume	O
that	O
the	O
posterior	O
distribution	O
is	O
sharply	O
peaked	O
around	O
the	O
most	O
probable	O
value	O
wmap	O
with	O
width	O
wposterior	O
then	O
we	O
can	O
approximate	O
the	O
integral	O
by	O
the	O
value	O
of	O
the	O
integrand	O
at	O
its	O
maximum	O
times	O
the	O
width	O
of	O
the	O
peak	O
if	O
we	O
further	O
assume	O
that	O
the	O
prior	B
is	O
flat	O
with	O
width	O
wprior	O
so	O
that	O
pw	O
wprior	O
then	O
we	O
have	O
pd	O
pdwpw	O
dw	O
pdwmap	O
wposterior	O
wprior	O
bayesian	B
model	B
comparison	I
figure	O
we	O
can	O
obtain	O
a	O
rough	O
approximation	O
to	O
the	O
model	B
evidence	I
if	O
we	O
assume	O
that	O
the	O
posterior	O
distribution	O
over	O
parameters	O
is	O
sharply	O
peaked	O
around	O
its	O
mode	O
wmap	O
wposterior	O
and	O
so	O
taking	O
logs	O
we	O
obtain	O
ln	O
pd	O
ln	O
pdwmap	O
ln	O
wmap	O
wprior	O
w	O
wposterior	O
wprior	O
this	O
approximation	O
is	O
illustrated	O
in	O
figure	O
the	O
first	O
term	O
represents	O
the	O
fit	O
to	O
the	O
data	O
given	O
by	O
the	O
most	O
probable	O
parameter	O
values	O
and	O
for	O
a	O
flat	O
prior	B
this	O
would	O
correspond	O
to	O
the	O
log	O
likelihood	O
the	O
second	O
term	O
penalizes	O
the	O
model	O
according	O
to	O
its	O
complexity	O
because	O
wposterior	O
wprior	O
this	O
term	O
is	O
negative	O
and	O
it	O
increases	O
in	O
magnitude	O
as	O
the	O
ratio	O
wposterior	O
wprior	O
gets	O
smaller	O
thus	O
if	O
parameters	O
are	O
finely	O
tuned	O
to	O
the	O
data	O
in	O
the	O
posterior	O
distribution	O
then	O
the	O
penalty	O
term	O
is	O
large	O
for	O
a	O
model	O
having	O
a	O
set	O
of	O
m	O
parameters	O
we	O
can	O
make	O
a	O
similar	O
approximation	O
for	O
each	O
parameter	O
in	O
turn	O
assuming	O
that	O
all	O
parameters	O
have	O
the	O
same	O
ratio	O
of	O
wposterior	O
wprior	O
we	O
obtain	O
ln	O
pd	O
ln	O
pdwmap	O
m	O
ln	O
wposterior	O
wprior	O
section	O
thus	O
in	O
this	O
very	O
simple	O
approximation	O
the	O
size	O
of	O
the	O
complexity	O
penalty	O
increases	O
linearly	O
with	O
the	O
number	O
m	O
of	O
adaptive	O
parameters	O
in	O
the	O
model	O
as	O
we	O
increase	O
the	O
complexity	O
of	O
the	O
model	O
the	O
first	O
term	O
will	O
typically	O
decrease	O
because	O
a	O
more	O
complex	O
model	O
is	O
better	O
able	O
to	O
fit	O
the	O
data	O
whereas	O
the	O
second	O
term	O
will	O
increase	O
due	O
to	O
the	O
dependence	O
on	O
m	O
the	O
optimal	O
model	O
complexity	O
as	O
determined	O
by	O
the	O
maximum	O
evidence	O
will	O
be	O
given	O
by	O
a	O
trade-off	O
between	O
these	O
two	O
competing	O
terms	O
we	O
shall	O
later	O
develop	O
a	O
more	O
refined	O
version	O
of	O
this	O
approximation	O
based	O
on	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
we	O
can	O
gain	O
further	O
insight	O
into	O
bayesian	B
model	B
comparison	I
and	O
understand	O
how	O
the	O
marginal	B
likelihood	I
can	O
favour	O
models	O
of	O
intermediate	O
complexity	O
by	O
considering	O
figure	O
here	O
the	O
horizontal	O
axis	O
is	O
a	O
one-dimensional	O
representation	O
of	O
the	O
space	O
of	O
possible	O
data	O
sets	O
so	O
that	O
each	O
point	O
on	O
this	O
axis	O
corresponds	O
to	O
a	O
specific	O
data	O
set	O
we	O
now	O
consider	O
three	O
models	O
and	O
of	O
successively	O
increasing	O
complexity	O
imagine	O
running	O
these	O
models	O
generatively	O
to	O
produce	O
example	O
data	O
sets	O
and	O
then	O
looking	O
at	O
the	O
distribution	O
of	O
data	O
sets	O
that	O
result	O
any	O
given	O
linear	O
models	O
for	B
regression	B
pd	O
figure	O
schematic	O
illustration	O
of	O
the	O
distribution	O
of	O
data	O
sets	O
for	O
three	O
models	O
of	O
different	O
comin	O
which	O
is	O
the	O
plexity	O
simplest	O
and	O
is	O
the	O
most	O
complex	O
note	O
that	O
the	O
distributions	O
are	O
normalized	O
in	O
for	O
the	O
particthis	O
example	O
ular	O
observed	O
data	O
set	O
the	O
model	O
with	O
intermediate	O
complexity	O
has	O
the	O
largest	O
evidence	O
d	O
model	O
can	O
generate	O
a	O
variety	O
of	O
different	O
data	O
sets	O
since	O
the	O
parameters	O
are	O
governed	O
by	O
a	O
prior	B
probability	B
distribution	O
and	O
for	O
any	O
choice	O
of	O
the	O
parameters	O
there	O
may	O
be	O
random	O
noise	O
on	O
the	O
target	O
variables	O
to	O
generate	O
a	O
particular	O
data	O
set	O
from	O
a	O
specific	O
model	O
we	O
first	O
choose	O
the	O
values	O
of	O
the	O
parameters	O
from	O
their	O
prior	B
distribution	O
pw	O
and	O
then	O
for	O
these	O
parameter	O
values	O
we	O
sample	O
the	O
data	O
from	O
pdw	O
a	O
simple	O
model	O
example	O
based	O
on	O
a	O
first	O
order	O
polynomial	O
has	O
little	O
variability	O
and	O
so	O
will	O
generate	O
data	O
sets	O
that	O
are	O
fairly	O
similar	O
to	O
each	O
other	O
its	O
distribution	O
pd	O
is	O
therefore	O
confined	O
to	O
a	O
relatively	O
small	O
region	O
of	O
the	O
horizontal	O
axis	O
by	O
contrast	O
a	O
complex	O
model	O
as	O
a	O
ninth	O
order	O
polynomial	O
can	O
generate	O
a	O
great	O
variety	O
of	O
different	O
data	O
sets	O
and	O
so	O
its	O
distribution	O
pd	O
is	O
spread	O
over	O
a	O
large	O
region	O
of	O
the	O
space	O
of	O
data	O
sets	O
because	O
the	O
distributions	O
pdmi	O
are	O
normalized	O
we	O
see	O
that	O
the	O
particular	O
data	O
set	O
can	O
have	O
the	O
highest	O
value	O
of	O
the	O
evidence	O
for	O
the	O
model	O
of	O
intermediate	O
complexity	O
essentially	O
the	O
simpler	O
model	O
cannot	O
fit	O
the	O
data	O
well	O
whereas	O
the	O
more	O
complex	O
model	O
spreads	O
its	O
predictive	O
probability	B
over	O
too	O
broad	O
a	O
range	O
of	O
data	O
sets	O
and	O
so	O
assigns	O
relatively	O
small	O
probability	B
to	O
any	O
one	O
of	O
them	O
implicit	O
in	O
the	O
bayesian	B
model	B
comparison	I
framework	O
is	O
the	O
assumption	O
that	O
the	O
true	O
distribution	O
from	O
which	O
the	O
data	O
are	O
generated	O
is	O
contained	O
within	O
the	O
set	O
of	O
models	O
under	O
consideration	O
provided	O
this	O
is	O
so	O
we	O
can	O
show	O
that	O
bayesian	B
model	B
comparison	I
will	O
on	O
average	O
favour	O
the	O
correct	O
model	O
to	O
see	O
this	O
consider	O
two	O
models	O
and	O
in	O
which	O
the	O
truth	O
corresponds	O
to	O
for	O
a	O
given	O
finite	O
data	O
set	O
it	O
is	O
possible	O
for	O
the	O
bayes	B
factor	O
to	O
be	O
larger	O
for	O
the	O
incorrect	O
model	O
however	O
if	O
bayes	B
factor	O
in	O
the	O
form	O
we	O
average	O
the	O
bayes	B
factor	O
over	O
the	O
distribution	O
of	O
data	O
sets	O
we	O
obtain	O
the	O
expected	O
ln	O
dd	O
section	O
where	O
the	O
average	O
has	O
been	O
taken	O
with	O
respect	O
to	O
the	O
true	O
distribution	O
of	O
the	O
data	O
this	O
quantity	O
is	O
an	O
example	O
of	O
the	O
kullback-leibler	B
divergence	I
and	O
satisfies	O
the	O
property	O
of	O
always	O
being	O
positive	O
unless	O
the	O
two	O
distributions	O
are	O
equal	O
in	O
which	O
case	O
it	O
is	O
zero	O
thus	O
on	O
average	O
the	O
bayes	B
factor	O
will	O
always	O
favour	O
the	O
correct	O
model	O
we	O
have	O
seen	O
that	O
the	O
bayesian	B
framework	O
avoids	O
the	O
problem	O
of	O
over-fitting	B
and	O
allows	O
models	O
to	O
be	O
compared	O
on	O
the	O
basis	O
of	O
the	O
training	B
data	O
alone	O
however	O
the	O
evidence	B
approximation	I
a	O
bayesian	B
approach	O
like	O
any	O
approach	O
to	O
pattern	O
recognition	O
needs	O
to	O
make	O
assumptions	O
about	O
the	O
form	O
of	O
the	O
model	O
and	O
if	O
these	O
are	O
invalid	O
then	O
the	O
results	O
can	O
be	O
misleading	O
in	O
particular	O
we	O
see	O
from	O
figure	O
that	O
the	O
model	B
evidence	I
can	O
be	O
sensitive	O
to	O
many	O
aspects	O
of	O
the	O
prior	B
such	O
as	O
the	O
behaviour	O
in	O
the	O
tails	O
indeed	O
the	O
evidence	O
is	O
not	O
defined	O
if	O
the	O
prior	B
is	O
improper	B
as	O
can	O
be	O
seen	O
by	O
noting	O
that	O
an	O
improper	B
prior	B
has	O
an	O
arbitrary	O
scaling	B
factor	I
other	O
words	O
the	O
normalization	O
coefficient	O
is	O
not	O
defined	O
because	O
the	O
distribution	O
cannot	O
be	O
normalized	O
if	O
we	O
consider	O
a	O
proper	O
prior	B
and	O
then	O
take	O
a	O
suitable	O
limit	O
in	O
order	O
to	O
obtain	O
an	O
improper	B
prior	B
example	O
a	O
gaussian	B
prior	B
in	O
which	O
we	O
take	O
the	O
limit	O
of	O
infinite	O
variance	B
then	O
the	O
evidence	O
will	O
go	O
to	O
zero	O
as	O
can	O
be	O
seen	O
from	O
and	O
figure	O
it	O
may	O
however	O
be	O
possible	O
to	O
consider	O
the	O
evidence	O
ratio	O
between	O
two	O
models	O
first	O
and	O
then	O
take	O
a	O
limit	O
to	O
obtain	O
a	O
meaningful	O
answer	O
in	O
a	O
practical	O
application	O
therefore	O
it	O
will	O
be	O
wise	O
to	O
keep	O
aside	O
an	O
independent	B
test	B
set	I
of	O
data	O
on	O
which	O
to	O
evaluate	O
the	O
overall	O
performance	O
of	O
the	O
final	O
system	O
the	O
evidence	B
approximation	I
in	O
a	O
fully	O
bayesian	B
treatment	O
of	O
the	O
linear	O
basis	B
function	I
model	O
we	O
would	O
introduce	O
prior	B
distributions	O
over	O
the	O
hyperparameters	O
and	O
and	O
make	O
predictions	O
by	O
marginalizing	O
with	O
respect	O
to	O
these	O
hyperparameters	O
as	O
well	O
as	O
with	O
respect	O
to	O
the	O
parameters	O
w	O
however	O
although	O
we	O
can	O
integrate	O
analytically	O
over	O
either	O
w	O
or	O
over	O
the	O
hyperparameters	O
the	O
complete	O
marginalization	O
over	O
all	O
of	O
these	O
variables	O
is	O
analytically	O
intractable	O
here	O
we	O
discuss	O
an	O
approximation	O
in	O
which	O
we	O
set	O
the	O
hyperparameters	O
to	O
specific	O
values	O
determined	O
by	O
maximizing	O
the	O
marginal	B
likelihood	B
function	I
obtained	O
by	O
first	O
integrating	O
over	O
the	O
parameters	O
w	O
this	O
framework	O
is	O
known	O
in	O
the	O
statistics	O
literature	O
as	O
empirical	O
bayes	B
and	O
smith	O
gelman	O
et	O
al	O
or	O
type	O
maximum	B
likelihood	I
or	O
generalized	B
maximum	B
likelihood	I
and	O
in	O
the	O
machine	O
learning	B
literature	O
is	O
also	O
called	O
the	O
evidence	B
approximation	I
mackay	O
if	O
we	O
introduce	O
hyperpriors	O
over	O
and	O
the	O
predictive	B
distribution	I
is	O
obtained	O
by	O
marginalizing	O
over	O
w	O
and	O
so	O
that	O
ptt	O
ptw	O
dw	O
d	O
d	O
where	O
ptw	O
is	O
given	O
by	O
and	O
pwt	O
is	O
given	O
by	O
with	O
mn	O
and	O
sn	O
defined	O
by	O
and	O
respectively	O
here	O
we	O
have	O
omitted	O
the	O
dependence	O
on	O
the	O
input	O
variable	O
x	O
to	O
keep	O
the	O
notation	O
uncluttered	O
if	O
the	O
posterior	O
distribution	O
p	O
is	O
sharply	O
peaked	O
around	O
then	O
the	O
predictive	B
distribution	I
is	O
obtained	O
simply	O
by	O
marginalizing	O
over	O
w	O
in	O
which	O
and	O
are	O
fixed	O
to	O
the	O
so	O
that	O
ptt	O
dw	O
linear	O
models	O
for	B
regression	B
from	O
bayes	B
theorem	O
the	O
posterior	O
distribution	O
for	O
and	O
is	O
given	O
by	O
if	O
the	O
prior	B
is	O
relatively	O
flat	O
then	O
in	O
the	O
evidence	O
framework	O
the	O
values	O
of	O
and	O
are	O
obtained	O
by	O
maximizing	O
the	O
marginal	B
likelihood	B
function	I
pt	O
we	O
shall	O
p	O
pt	O
proceed	O
by	O
evaluating	O
the	O
marginal	B
likelihood	I
for	O
the	O
linear	O
basis	B
function	I
model	O
and	O
then	O
finding	O
its	O
maxima	O
this	O
will	O
allow	O
us	O
to	O
determine	O
values	O
for	O
these	O
hyperparameters	O
from	O
the	O
training	B
data	O
alone	O
without	O
recourse	O
to	O
cross-validation	B
recall	O
that	O
the	O
ratio	O
is	O
analogous	O
to	O
a	O
regularization	B
parameter	O
as	O
an	O
aside	O
it	O
is	O
worth	O
noting	O
that	O
if	O
we	O
define	O
conjugate	B
prior	B
distributions	O
over	O
and	O
then	O
the	O
marginalization	O
over	O
these	O
hyperparameters	O
in	O
can	O
be	O
performed	O
analytically	O
to	O
give	O
a	O
student	O
s	O
t-distribution	O
over	O
w	O
section	O
although	O
the	O
resulting	O
integral	O
over	O
w	O
is	O
no	O
longer	O
analytically	O
tractable	O
it	O
might	O
be	O
thought	O
that	O
approximating	O
this	O
integral	O
for	O
example	O
using	O
the	O
laplace	B
approximation	I
discussed	O
which	O
is	O
based	O
on	O
a	O
local	B
gaussian	B
approximation	O
centred	O
on	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
might	O
provide	O
a	O
practical	O
alternative	O
to	O
the	O
evidence	O
framework	O
and	O
weigend	O
however	O
the	O
integrand	O
as	O
a	O
function	O
of	O
w	O
typically	O
has	O
a	O
strongly	O
skewed	O
mode	O
so	O
that	O
the	O
laplace	B
approximation	I
fails	O
to	O
capture	O
the	O
bulk	O
of	O
the	O
probability	B
mass	O
leading	O
to	O
poorer	O
results	O
than	O
those	O
obtained	O
by	O
maximizing	O
the	O
evidence	O
returning	O
to	O
the	O
evidence	O
framework	O
we	O
note	O
that	O
there	O
are	O
two	O
approaches	O
that	O
we	O
can	O
take	O
to	O
the	O
maximization	O
of	O
the	O
log	O
evidence	O
we	O
can	O
evaluate	O
the	O
evidence	B
function	I
analytically	O
and	O
then	O
set	O
its	O
derivative	B
equal	O
to	O
zero	O
to	O
obtain	O
re-estimation	O
equations	O
for	O
and	O
which	O
we	O
shall	O
do	O
in	O
section	O
alternatively	O
we	O
use	O
a	O
technique	O
called	O
the	O
expectation	B
maximization	I
algorithm	O
which	O
will	O
be	O
discussed	O
in	O
section	O
where	O
we	O
shall	O
also	O
show	O
that	O
these	O
two	O
approaches	O
converge	O
to	O
the	O
same	O
solution	O
evaluation	O
of	O
the	O
evidence	B
function	I
the	O
marginal	B
likelihood	B
function	I
pt	O
is	O
obtained	O
by	O
integrating	O
over	O
the	O
weight	O
parameters	O
w	O
so	O
that	O
pt	O
ptw	O
dw	O
exercise	O
exercise	O
one	O
way	O
to	O
evaluate	O
this	O
integral	O
is	O
to	O
make	O
use	O
once	O
again	O
of	O
the	O
result	O
for	O
the	O
conditional	B
distribution	O
in	O
a	O
linear-gaussian	B
model	I
here	O
we	O
shall	O
evaluate	O
the	O
integral	O
instead	O
by	O
completing	B
the	I
square	I
in	O
the	O
exponent	O
and	O
making	O
use	O
of	O
the	O
standard	O
form	O
for	O
the	O
normalization	O
coefficient	O
of	O
a	O
gaussian	B
from	O
and	O
we	O
can	O
write	O
the	O
evidence	B
function	I
in	O
the	O
form	O
pt	O
exp	O
ew	O
dw	O
exercise	O
exercise	O
the	O
evidence	B
approximation	I
where	O
m	O
is	O
the	O
dimensionality	O
of	O
w	O
and	O
we	O
have	O
defined	O
ew	O
edw	O
ew	O
wtw	O
we	O
recognize	O
as	O
being	O
equal	O
up	O
to	O
a	O
constant	O
of	O
proportionality	O
to	O
the	O
regularized	O
sum-of-squares	B
error	B
function	I
we	O
now	O
complete	O
the	O
square	O
over	O
w	O
giving	O
ew	O
emn	O
mn	O
mn	O
where	O
we	O
have	O
introduced	O
a	O
i	O
t	O
together	O
with	O
emn	O
mt	O
n	O
mn	O
note	O
that	O
a	O
corresponds	O
to	O
the	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
error	B
function	I
a	O
ew	O
and	O
is	O
known	O
as	O
the	O
hessian	B
matrix	I
here	O
we	O
have	O
also	O
defined	O
mn	O
given	O
by	O
mn	O
a	O
tt	O
n	O
and	O
hence	O
is	O
equivalent	O
to	O
the	O
previous	O
using	O
we	O
see	O
that	O
a	O
s	O
definition	O
and	O
therefore	O
represents	O
the	O
mean	B
of	O
the	O
posterior	O
distribution	O
the	O
integral	O
over	O
w	O
can	O
now	O
be	O
evaluated	O
simply	O
by	O
appealing	O
to	O
the	O
standard	O
result	O
for	O
the	O
normalization	O
coefficient	O
of	O
a	O
multivariate	O
gaussian	B
giving	O
exp	O
ew	O
dw	O
exp	O
emn	O
exp	O
emn	O
exp	O
mntaw	O
mn	O
dw	O
using	O
we	O
can	O
then	O
write	O
the	O
log	O
of	O
the	O
marginal	B
likelihood	I
in	O
the	O
form	O
ln	O
pt	O
m	O
ln	O
n	O
ln	O
emn	O
lna	O
n	O
which	O
is	O
the	O
required	O
expression	O
for	O
the	O
evidence	B
function	I
returning	O
to	O
the	O
polynomial	O
regression	B
problem	O
we	O
can	O
plot	O
the	O
model	B
evidence	I
against	O
the	O
order	O
of	O
the	O
polynomial	O
as	O
shown	O
in	O
figure	O
here	O
we	O
have	O
assumed	O
a	O
prior	B
of	O
the	O
form	O
with	O
the	O
parameter	O
fixed	O
at	O
the	O
form	O
of	O
this	O
plot	O
is	O
very	O
instructive	O
referring	O
back	O
to	O
figure	O
we	O
see	O
that	O
the	O
m	O
polynomial	O
has	O
very	O
poor	O
fit	O
to	O
the	O
data	O
and	O
consequently	O
gives	O
a	O
relatively	O
low	O
value	O
linear	O
models	O
for	B
regression	B
figure	O
plot	O
of	O
the	O
model	B
evidence	I
versus	O
the	O
order	O
m	O
for	O
the	O
polynomial	O
regression	B
model	O
showing	O
that	O
the	O
evidence	O
favours	O
the	O
model	O
with	O
m	O
m	O
for	O
the	O
evidence	O
going	O
to	O
the	O
m	O
polynomial	O
greatly	O
improves	O
the	O
data	O
fit	O
and	O
hence	O
the	O
evidence	O
is	O
significantly	O
higher	O
however	O
in	O
going	O
to	O
m	O
the	O
data	O
fit	O
is	O
improved	O
only	O
very	O
marginally	O
due	O
to	O
the	O
fact	O
that	O
the	O
underlying	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
is	O
generated	O
is	O
an	O
odd	O
function	O
and	O
so	O
has	O
no	O
even	O
terms	O
in	O
a	O
polynomial	O
expansion	O
indeed	O
figure	O
shows	O
that	O
the	O
residual	O
data	O
error	B
is	O
reduced	O
only	O
slightly	O
in	O
going	O
from	O
m	O
to	O
m	O
because	O
this	O
richer	O
model	O
suffers	O
a	O
greater	O
complexity	O
penalty	O
the	O
evidence	O
actually	O
falls	O
in	O
going	O
from	O
m	O
to	O
m	O
when	O
we	O
go	O
to	O
m	O
we	O
obtain	O
a	O
significant	O
further	O
improvement	O
in	O
data	O
fit	O
as	O
seen	O
in	O
figure	O
and	O
so	O
the	O
evidence	O
is	O
increased	O
again	O
giving	O
the	O
highest	O
overall	O
evidence	O
for	O
any	O
of	O
the	O
polynomials	O
further	O
increases	O
in	O
the	O
value	O
of	O
m	O
produce	O
only	O
small	O
improvements	O
in	O
the	O
fit	O
to	O
the	O
data	O
but	O
suffer	O
increasing	O
complexity	O
penalty	O
leading	O
overall	O
to	O
a	O
decrease	O
in	O
the	O
evidence	O
values	O
looking	O
again	O
at	O
figure	O
we	O
see	O
that	O
the	O
generalization	B
error	B
is	O
roughly	O
constant	O
between	O
m	O
and	O
m	O
and	O
it	O
would	O
be	O
difficult	O
to	O
choose	O
between	O
these	O
models	O
on	O
the	O
basis	O
of	O
this	O
plot	O
alone	O
the	O
evidence	O
values	O
however	O
show	O
a	O
clear	O
preference	O
for	O
m	O
since	O
this	O
is	O
the	O
simplest	O
model	O
which	O
gives	O
a	O
good	O
explanation	O
for	O
the	O
observed	O
data	O
maximizing	O
the	O
evidence	B
function	I
let	O
us	O
first	O
consider	O
the	O
maximization	O
of	O
pt	O
with	O
respect	O
to	O
this	O
can	O
be	O
done	O
by	O
first	O
defining	O
the	O
following	O
eigenvector	O
equation	O
from	O
it	O
then	O
follows	O
that	O
a	O
has	O
eigenvalues	O
i	O
now	O
consider	O
the	O
derivative	B
of	O
the	O
term	O
involving	O
lna	O
in	O
with	O
respect	O
to	O
we	O
have	O
t	O
ui	O
iui	O
d	O
d	O
lna	O
d	O
d	O
ln	O
i	O
d	O
d	O
i	O
i	O
ln	O
i	O
i	O
i	O
thus	O
the	O
stationary	B
points	O
of	O
with	O
respect	O
to	O
satisfy	O
m	O
n	O
mn	O
mt	O
i	O
i	O
multiplying	O
through	O
by	O
and	O
rearranging	O
we	O
obtain	O
the	O
evidence	B
approximation	I
i	O
i	O
mt	O
n	O
mn	O
m	O
i	O
i	O
i	O
since	O
there	O
are	O
m	O
terms	O
in	O
the	O
sum	O
over	O
i	O
the	O
quantity	O
can	O
be	O
written	O
exercise	O
the	O
interpretation	O
of	O
the	O
quantity	O
will	O
be	O
discussed	O
shortly	O
from	O
we	O
see	O
that	O
the	O
value	O
of	O
that	O
maximizes	O
the	O
marginal	B
likelihood	I
satisfies	O
n	O
mn	O
mt	O
note	O
that	O
this	O
is	O
an	O
implicit	O
solution	O
for	O
not	O
only	O
because	O
depends	O
on	O
but	O
also	O
because	O
the	O
mode	O
mn	O
of	O
the	O
posterior	O
distribution	O
itself	O
depends	O
on	O
the	O
choice	O
of	O
we	O
therefore	O
adopt	O
an	O
iterative	O
procedure	O
in	O
which	O
we	O
make	O
an	O
initial	O
choice	O
for	O
and	O
use	O
this	O
to	O
find	O
mn	O
which	O
is	O
given	O
by	O
and	O
also	O
to	O
evaluate	O
which	O
is	O
given	O
by	O
these	O
values	O
are	O
then	O
used	O
to	O
re-estimate	O
using	O
and	O
the	O
process	O
repeated	O
until	O
convergence	O
note	O
that	O
because	O
the	O
matrix	O
t	O
is	O
fixed	O
we	O
can	O
compute	O
its	O
eigenvalues	O
once	O
at	O
the	O
start	O
and	O
then	O
simply	O
multiply	O
these	O
by	O
to	O
obtain	O
the	O
i	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
value	O
of	O
has	O
been	O
determined	O
purely	O
by	O
looking	O
at	O
the	O
training	B
data	O
in	O
contrast	O
to	O
maximum	B
likelihood	I
methods	O
no	O
independent	B
data	O
set	O
is	O
required	O
in	O
order	O
to	O
optimize	O
the	O
model	O
complexity	O
we	O
can	O
similarly	O
maximize	O
the	O
log	O
marginal	B
likelihood	I
with	O
respect	O
to	O
to	O
do	O
this	O
we	O
note	O
that	O
the	O
eigenvalues	O
i	O
defined	O
by	O
are	O
proportional	O
to	O
and	O
hence	O
d	O
id	O
i	O
giving	O
d	O
d	O
lna	O
d	O
d	O
ln	O
i	O
the	O
stationary	B
point	O
of	O
the	O
marginal	B
likelihood	I
therefore	O
satisfies	O
n	O
exercise	O
and	O
rearranging	O
we	O
obtain	O
tn	O
mt	O
n	O
n	O
tn	O
mt	O
n	O
i	O
i	O
i	O
i	O
again	O
this	O
is	O
an	O
implicit	O
solution	O
for	O
and	O
can	O
be	O
solved	O
by	O
choosing	O
an	O
initial	O
value	O
for	O
and	O
then	O
using	O
this	O
to	O
calculate	O
mn	O
and	O
and	O
then	O
re-estimate	O
using	O
repeating	O
until	O
convergence	O
if	O
both	O
and	O
are	O
to	O
be	O
determined	O
from	O
the	O
data	O
then	O
their	O
values	O
can	O
be	O
re-estimated	O
together	O
after	O
each	O
update	O
of	O
linear	O
models	O
for	B
regression	B
figure	O
contours	O
of	O
the	O
likelihood	B
function	I
and	O
the	O
prior	B
in	O
which	O
the	O
axes	O
in	O
parameter	O
space	O
have	O
been	O
rotated	O
to	O
align	O
with	O
the	O
eigenvectors	O
ui	O
of	O
the	O
hessian	O
for	O
the	O
mode	O
of	O
the	O
posterior	O
is	O
given	O
by	O
the	O
maximum	B
likelihood	I
solution	O
wml	O
whereas	O
for	O
nonzero	O
the	O
mode	O
is	O
at	O
wmap	O
mn	O
in	O
the	O
direction	O
the	O
eigenvalue	O
defined	O
by	O
is	O
small	O
compared	O
with	O
and	O
so	O
the	O
quantity	O
is	O
close	O
to	O
zero	O
and	O
the	O
corresponding	O
map	O
value	O
of	O
is	O
also	O
close	O
to	O
zero	O
by	O
contrast	O
in	O
the	O
direction	O
the	O
eigenvalue	O
is	O
large	O
compared	O
with	O
and	O
so	O
the	O
quantity	O
is	O
close	O
to	O
unity	O
and	O
the	O
map	O
value	O
of	O
is	O
close	O
to	O
its	O
maximum	B
likelihood	I
value	O
wml	O
wmap	O
effective	B
number	I
of	I
parameters	I
the	O
result	O
has	O
an	O
elegant	O
interpretation	O
which	O
provides	O
insight	O
into	O
the	O
bayesian	B
solution	O
for	O
to	O
see	O
this	O
consider	O
the	O
contours	O
of	O
the	O
likelihood	B
function	I
and	O
the	O
prior	B
as	O
illustrated	O
in	O
figure	O
here	O
we	O
have	O
implicitly	O
transformed	O
to	O
a	O
rotated	O
set	O
of	O
axes	O
in	O
parameter	O
space	O
aligned	O
with	O
the	O
eigenvectors	O
ui	O
defined	O
in	O
contours	O
of	O
the	O
likelihood	B
function	I
are	O
then	O
axis-aligned	O
ellipses	O
the	O
eigenvalues	O
i	O
measure	O
the	O
curvature	O
of	O
the	O
likelihood	B
function	I
and	O
so	O
in	O
figure	O
the	O
eigenvalue	O
is	O
small	O
compared	O
with	O
a	O
smaller	O
curvature	O
corresponds	O
to	O
a	O
greater	O
elongation	O
of	O
the	O
contours	O
of	O
the	O
likelihood	B
function	I
because	O
t	O
is	O
a	O
positive	B
definite	I
matrix	I
it	O
will	O
have	O
positive	O
eigenvalues	O
and	O
so	O
the	O
ratio	O
i	O
i	O
will	O
lie	O
between	O
and	O
consequently	O
the	O
quantity	O
defined	O
by	O
will	O
lie	O
in	O
the	O
range	O
m	O
for	O
directions	O
in	O
which	O
i	O
the	O
corresponding	O
parameter	O
wi	O
will	O
be	O
close	O
to	O
its	O
maximum	B
likelihood	I
value	O
and	O
the	O
ratio	O
i	O
i	O
will	O
be	O
close	O
to	O
such	O
parameters	O
are	O
called	O
well	O
determined	O
because	O
their	O
values	O
are	O
tightly	O
constrained	O
by	O
the	O
data	O
conversely	O
for	O
directions	O
in	O
which	O
i	O
the	O
corresponding	O
parameters	O
wi	O
will	O
be	O
close	O
to	O
zero	O
as	O
will	O
the	O
ratios	O
i	O
i	O
these	O
are	O
directions	O
in	O
which	O
the	O
likelihood	B
function	I
is	O
relatively	O
insensitive	O
to	O
the	O
parameter	O
value	O
and	O
so	O
the	O
parameter	O
has	O
been	O
set	O
to	O
a	O
small	O
value	O
by	O
the	O
prior	B
the	O
quantity	O
defined	O
by	O
therefore	O
measures	O
the	O
effective	O
total	O
number	O
of	O
well	O
determined	O
parameters	O
we	O
can	O
obtain	O
some	O
insight	O
into	O
the	O
result	O
for	O
re-estimating	O
by	O
comparing	O
it	O
with	O
the	O
corresponding	O
maximum	B
likelihood	I
result	O
given	O
by	O
both	O
of	O
these	O
formulae	O
express	O
the	O
variance	B
inverse	B
precision	O
as	O
an	O
average	O
of	O
the	O
squared	O
differences	O
between	O
the	O
targets	O
and	O
the	O
model	O
predictions	O
however	O
they	O
differ	O
in	O
that	O
the	O
number	O
of	O
data	O
points	O
n	O
in	O
the	O
denominator	O
of	O
the	O
maximum	B
likelihood	I
result	O
is	O
replaced	O
by	O
n	O
in	O
the	O
bayesian	B
result	O
we	O
recall	O
from	O
that	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
the	O
variance	B
for	O
a	O
gaussian	B
distribution	O
over	O
a	O
the	O
evidence	B
approximation	I
single	O
variable	O
x	O
is	O
given	O
by	O
ml	O
n	O
and	O
that	O
this	O
estimate	O
is	O
biased	O
because	O
the	O
maximum	B
likelihood	I
solution	O
ml	O
for	O
the	O
mean	B
has	O
fitted	O
some	O
of	O
the	O
noise	O
on	O
the	O
data	O
in	O
effect	O
this	O
has	O
used	O
up	O
one	O
degree	O
of	O
freedom	O
in	O
the	O
model	O
the	O
corresponding	O
unbiased	O
estimate	O
is	O
given	O
by	O
and	O
takes	O
the	O
form	O
map	O
n	O
we	O
shall	O
see	O
in	O
section	O
that	O
this	O
result	O
can	O
be	O
obtained	O
from	O
a	O
bayesian	B
treatment	O
in	O
which	O
we	O
marginalize	O
over	O
the	O
unknown	O
mean	B
the	O
factor	O
of	O
n	O
in	O
the	O
denominator	O
of	O
the	O
bayesian	B
result	O
takes	O
account	O
of	O
the	O
fact	O
that	O
one	O
degree	O
of	O
freedom	O
has	O
been	O
used	O
in	O
fitting	O
the	O
mean	B
and	O
removes	O
the	O
bias	B
of	O
maximum	B
likelihood	I
now	O
consider	O
the	O
corresponding	O
results	O
for	O
the	O
linear	B
regression	B
model	O
the	O
mean	B
of	O
the	O
target	O
distribution	O
is	O
now	O
given	O
by	O
the	O
function	O
wt	O
which	O
contains	O
m	O
parameters	O
however	O
not	O
all	O
of	O
these	O
parameters	O
are	O
tuned	O
to	O
the	O
data	O
the	O
effective	B
number	I
of	I
parameters	I
that	O
are	O
determined	O
by	O
the	O
data	O
is	O
with	O
the	O
remaining	O
m	O
parameters	O
set	O
to	O
small	O
values	O
by	O
the	O
prior	B
this	O
is	O
reflected	O
in	O
the	O
bayesian	B
result	O
for	O
the	O
variance	B
that	O
has	O
a	O
factor	O
n	O
in	O
the	O
denominator	O
thereby	O
correcting	O
for	O
the	O
bias	B
of	O
the	O
maximum	B
likelihood	I
result	O
we	O
can	O
illustrate	O
the	O
evidence	O
framework	O
for	O
setting	O
hyperparameters	O
using	O
the	O
sinusoidal	O
synthetic	O
data	O
set	O
from	O
section	O
together	O
with	O
the	O
gaussian	B
basis	B
function	I
model	O
comprising	O
basis	O
functions	O
so	O
that	O
the	O
total	O
number	O
of	O
parameters	O
in	O
the	O
model	O
is	O
given	O
by	O
m	O
including	O
the	O
bias	B
here	O
for	O
simplicity	O
of	O
illustration	O
we	O
have	O
set	O
to	O
its	O
true	O
value	O
of	O
and	O
then	O
used	O
the	O
evidence	O
framework	O
to	O
determine	O
as	O
shown	O
in	O
figure	O
we	O
can	O
also	O
see	O
how	O
the	O
parameter	O
controls	O
the	O
magnitude	O
of	O
the	O
parameters	O
by	O
plotting	O
the	O
individual	O
parameters	O
versus	O
the	O
effective	B
number	I
of	I
parameters	I
as	O
shown	O
in	O
figure	O
if	O
we	O
consider	O
the	O
limit	O
n	O
m	O
in	O
which	O
the	O
number	O
of	O
data	O
points	O
is	O
large	O
in	O
relation	O
to	O
the	O
number	O
of	O
parameters	O
then	O
from	O
all	O
of	O
the	O
parameters	O
will	O
be	O
well	O
determined	O
by	O
the	O
data	O
because	O
t	O
involves	O
an	O
implicit	O
sum	O
over	O
data	O
points	O
and	O
so	O
the	O
eigenvalues	O
i	O
increase	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
in	O
this	O
case	O
m	O
and	O
the	O
re-estimation	O
equations	O
for	O
and	O
become	O
m	O
n	O
where	O
ew	O
and	O
ed	O
are	O
defined	O
by	O
and	O
respectively	O
these	O
results	O
can	O
be	O
used	O
as	O
an	O
easy-to-compute	O
approximation	O
to	O
the	O
full	O
evidence	O
re-estimation	O
linear	O
models	O
for	B
regression	B
ln	O
ln	O
figure	O
the	O
left	O
plot	O
shows	O
curve	O
and	O
ew	O
curve	O
versus	O
ln	O
for	O
the	O
sinusoidal	O
synthetic	O
data	O
set	O
it	O
is	O
the	O
intersection	O
of	O
these	O
two	O
curves	O
that	O
defines	O
the	O
optimum	O
value	O
for	O
given	O
by	O
the	O
evidence	O
procedure	O
the	O
right	O
plot	O
shows	O
the	O
corresponding	O
graph	O
of	O
log	O
evidence	O
ln	O
pt	O
versus	O
ln	O
curve	O
showing	O
that	O
the	O
peak	O
coincides	O
with	O
the	O
crossing	O
point	O
of	O
the	O
curves	O
in	O
the	O
left	O
plot	O
also	O
shown	O
is	O
the	O
test	B
set	I
error	B
curve	O
showing	O
that	O
the	O
evidence	O
maximum	O
occurs	O
close	O
to	O
the	O
point	O
of	O
best	O
generalization	B
formulae	O
because	O
they	O
do	O
not	O
require	O
evaluation	O
of	O
the	O
eigenvalue	O
spectrum	O
of	O
the	O
hessian	O
figure	O
plot	O
of	O
the	O
parameters	O
wi	O
from	O
the	O
gaussian	B
basis	B
function	I
model	O
versus	O
the	O
effective	B
number	I
of	I
parameters	I
in	O
which	O
the	O
hyperparameter	B
is	O
varied	O
in	O
the	O
range	O
causing	O
to	O
vary	O
in	O
the	O
range	O
m	O
wi	O
limitations	O
of	O
fixed	O
basis	O
functions	O
throughout	O
this	O
chapter	O
we	O
have	O
focussed	O
on	O
models	O
comprising	O
a	O
linear	O
combination	O
of	O
fixed	O
nonlinear	O
basis	O
functions	O
we	O
have	O
seen	O
that	O
the	O
assumption	O
of	O
linearity	O
in	O
the	O
parameters	O
led	O
to	O
a	O
range	O
of	O
useful	O
properties	O
including	O
closed-form	O
solutions	O
to	O
the	O
least-squares	O
problem	O
as	O
well	O
as	O
a	O
tractable	O
bayesian	B
treatment	O
furthermore	O
for	O
a	O
suitable	O
choice	O
of	O
basis	O
functions	O
we	O
can	O
model	O
arbitrary	O
nonlinearities	O
in	O
the	O
exercises	O
mapping	O
from	O
input	O
variables	O
to	O
targets	O
in	O
the	O
next	O
chapter	O
we	O
shall	O
study	O
an	O
analogous	O
class	O
of	O
models	O
for	O
classification	B
it	O
might	O
appear	O
therefore	O
that	O
such	O
linear	O
models	O
constitute	O
a	O
general	O
purpose	O
framework	O
for	O
solving	O
problems	O
in	O
pattern	O
recognition	O
unfortunately	O
there	O
are	O
some	O
significant	O
shortcomings	O
with	O
linear	O
models	O
which	O
will	O
cause	O
us	O
to	O
turn	O
in	O
later	O
chapters	O
to	O
more	O
complex	O
models	O
such	O
as	O
support	B
vector	I
machines	O
and	O
neural	O
networks	O
the	O
difficulty	O
stems	O
from	O
the	O
assumption	O
that	O
the	O
basis	O
functions	O
jx	O
are	O
fixed	O
before	O
the	O
training	B
data	O
set	O
is	O
observed	O
and	O
is	O
a	O
manifestation	O
of	O
the	O
curse	B
of	I
dimensionality	I
discussed	O
in	O
section	O
as	O
a	O
consequence	O
the	O
number	O
of	O
basis	O
functions	O
needs	O
to	O
grow	O
rapidly	O
often	O
exponentially	O
with	O
the	O
dimensionality	O
d	O
of	O
the	O
input	O
space	O
fortunately	O
there	O
are	O
two	O
properties	O
of	O
real	O
data	O
sets	O
that	O
we	O
can	O
exploit	O
to	O
help	O
alleviate	O
this	O
problem	O
first	O
of	O
all	O
the	O
data	O
vectors	O
typically	O
lie	O
close	O
to	O
a	O
nonlinear	O
manifold	B
whose	O
intrinsic	B
dimensionality	I
is	O
smaller	O
than	O
that	O
of	O
the	O
input	O
space	O
as	O
a	O
result	O
of	O
strong	O
correlations	O
between	O
the	O
input	O
variables	O
we	O
will	O
see	O
an	O
example	O
of	O
this	O
when	O
we	O
consider	O
images	O
of	O
handwritten	O
digits	O
in	O
chapter	O
if	O
we	O
are	O
using	O
localized	O
basis	O
functions	O
we	O
can	O
arrange	O
that	O
they	O
are	O
scattered	O
in	O
input	O
space	O
only	O
in	O
regions	O
containing	O
data	O
this	O
approach	O
is	O
used	O
in	O
radial	B
basis	B
function	I
networks	O
and	O
also	O
in	O
support	B
vector	I
and	O
relevance	B
vector	I
machines	O
neural	B
network	I
models	O
which	O
use	O
adaptive	O
basis	O
functions	O
having	O
sigmoidal	O
nonlinearities	O
can	O
adapt	O
the	O
parameters	O
so	O
that	O
the	O
regions	O
of	O
input	O
space	O
over	O
which	O
the	O
basis	O
functions	O
vary	O
corresponds	O
to	O
the	O
data	O
manifold	B
the	O
second	O
property	O
is	O
that	O
target	O
variables	O
may	O
have	O
significant	O
dependence	O
on	O
only	O
a	O
small	O
number	O
of	O
possible	O
directions	O
within	O
the	O
data	O
manifold	B
neural	O
networks	O
can	O
exploit	O
this	O
property	O
by	O
choosing	O
the	O
directions	O
in	O
input	O
space	O
to	O
which	O
the	O
basis	O
functions	O
respond	O
exercises	O
www	O
show	O
that	O
the	O
tanh	O
function	O
and	O
the	O
logistic	B
sigmoid	I
function	O
are	O
related	O
by	O
tanha	O
hence	O
show	O
that	O
a	O
general	O
linear	O
combination	O
of	O
logistic	B
sigmoid	I
functions	O
of	O
the	O
form	O
x	O
j	O
s	O
x	O
j	O
s	O
is	O
equivalent	O
to	O
a	O
linear	O
combination	O
of	O
tanh	O
functions	O
of	O
the	O
form	O
yx	O
w	O
wj	O
yx	O
u	O
uj	O
tanh	O
and	O
find	O
expressions	O
to	O
relate	O
the	O
new	O
parameters	O
um	O
to	O
the	O
original	O
parameters	O
wm	O
linear	O
models	O
for	B
regression	B
show	O
that	O
the	O
matrix	O
t	O
t	O
takes	O
any	O
vector	O
v	O
and	O
projects	O
it	O
onto	O
the	O
space	O
spanned	O
by	O
the	O
columns	O
of	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
least-squares	O
solution	O
corresponds	O
to	O
an	O
orthogonal	O
projection	O
of	O
the	O
vector	O
t	O
onto	O
the	O
manifold	B
s	O
as	O
shown	O
in	O
figure	O
consider	O
a	O
data	O
set	O
in	O
which	O
each	O
data	O
point	O
tn	O
is	O
associated	O
with	O
a	O
weighting	O
factor	O
rn	O
so	O
that	O
the	O
sum-of-squares	B
error	B
function	I
becomes	O
rn	O
edw	O
tn	O
wt	O
find	O
an	O
expression	O
for	O
the	O
solution	O
that	O
minimizes	O
this	O
error	B
function	I
give	O
two	O
alternative	O
interpretations	O
of	O
the	O
weighted	O
sum-of-squares	B
error	B
function	I
in	O
terms	O
of	O
data	O
dependent	O
noise	O
variance	B
and	O
replicated	O
data	O
points	O
www	O
consider	O
a	O
linear	O
model	O
of	O
the	O
form	O
yx	O
w	O
wixi	O
together	O
with	O
a	O
sum-of-squares	B
error	B
function	I
of	O
the	O
form	O
edw	O
w	O
now	O
suppose	O
that	O
gaussian	B
noise	O
with	O
zero	O
mean	B
and	O
variance	B
is	O
added	O
independently	O
to	O
each	O
of	O
the	O
input	O
variables	O
xi	O
by	O
making	O
use	O
of	O
ei	O
and	O
eij	O
ij	O
show	O
that	O
minimizing	O
ed	O
averaged	O
over	O
the	O
noise	O
distribution	O
is	O
equivalent	O
to	O
minimizing	O
the	O
sum-of-squares	B
error	B
for	O
noise-free	O
input	O
variables	O
with	O
the	O
addition	O
of	O
a	O
weight-decay	O
regularization	B
term	O
in	O
which	O
the	O
bias	B
parameter	I
is	O
omitted	O
from	O
the	O
regularizer	O
www	O
using	O
the	O
technique	O
of	O
lagrange	B
multipliers	O
discussed	O
in	O
appendix	O
e	O
show	O
that	O
minimization	O
of	O
the	O
regularized	O
error	B
function	I
is	O
equivalent	O
to	O
minimizing	O
the	O
unregularized	O
sum-of-squares	B
error	B
subject	O
to	O
the	O
constraint	O
discuss	O
the	O
relationship	O
between	O
the	O
parameters	O
and	O
www	O
consider	O
a	O
linear	O
basis	B
function	I
regression	B
model	O
for	O
a	O
multivariate	O
target	O
variable	O
t	O
having	O
a	O
gaussian	B
distribution	O
of	O
the	O
form	O
ptw	O
n	O
w	O
where	O
yx	O
w	O
wt	O
exercises	O
together	O
with	O
a	O
training	B
data	O
set	O
comprising	O
input	O
basis	O
vectors	O
and	O
corresponding	O
target	O
vectors	O
tn	O
with	O
n	O
n	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
wml	O
for	O
the	O
parameter	O
matrix	O
w	O
has	O
the	O
property	O
that	O
each	O
column	O
is	O
given	O
by	O
an	O
expression	O
of	O
the	O
form	O
which	O
was	O
the	O
solution	O
for	O
an	O
isotropic	B
noise	O
distribution	O
note	O
that	O
this	O
is	O
independent	B
of	O
the	O
covariance	B
matrix	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
is	O
given	O
by	O
n	O
tn	O
wt	O
ml	O
tn	O
wt	O
ml	O
by	O
using	O
the	O
technique	O
of	O
completing	B
the	I
square	I
verify	O
the	O
result	O
for	O
the	O
posterior	O
distribution	O
of	O
the	O
parameters	O
w	O
in	O
the	O
linear	O
basis	B
function	I
model	O
in	O
which	O
mn	O
and	O
sn	O
are	O
defined	O
by	O
and	O
respectively	O
www	O
consider	O
the	O
linear	O
basis	B
function	I
model	O
in	O
section	O
and	O
suppose	O
that	O
we	O
have	O
already	O
observed	O
n	O
data	O
points	O
so	O
that	O
the	O
posterior	O
distribution	O
over	O
w	O
is	O
given	O
by	O
this	O
posterior	O
can	O
be	O
regarded	O
as	O
the	O
prior	B
for	O
the	O
next	O
observation	O
by	O
considering	O
an	O
additional	O
data	O
point	O
tn	O
and	O
by	O
completing	B
the	I
square	I
in	O
the	O
exponential	O
show	O
that	O
the	O
resulting	O
posterior	O
distribution	O
is	O
again	O
given	O
by	O
but	O
with	O
sn	O
replaced	O
by	O
sn	O
and	O
mn	O
replaced	O
by	O
mn	O
repeat	O
the	O
previous	O
exercise	O
but	O
instead	O
of	O
completing	B
the	I
square	I
by	O
hand	O
make	O
use	O
of	O
the	O
general	O
result	O
for	O
linear-gaussian	O
models	O
given	O
by	O
www	O
by	O
making	O
use	O
of	O
the	O
result	O
to	O
evaluate	O
the	O
integral	O
in	O
verify	O
that	O
the	O
predictive	B
distribution	I
for	O
the	O
bayesian	B
linear	B
regression	B
model	O
is	O
given	O
by	O
in	O
which	O
the	O
input-dependent	O
variance	B
is	O
given	O
by	O
we	O
have	O
seen	O
that	O
as	O
the	O
size	O
of	O
a	O
data	O
set	O
increases	O
the	O
uncertainty	O
associated	O
with	O
the	O
posterior	O
distribution	O
over	O
model	O
parameters	O
decreases	O
make	O
use	O
of	O
the	O
matrix	O
identity	O
c	O
m	O
vvt	O
m	O
vtm	O
vtm	O
to	O
show	O
that	O
the	O
uncertainty	O
given	O
by	O
satisfies	O
n	O
associated	O
with	O
the	O
linear	B
regression	B
function	I
n	O
n	O
we	O
saw	O
in	O
section	O
that	O
the	O
conjugate	B
prior	B
for	O
a	O
gaussian	B
distribution	O
with	O
unknown	O
mean	B
and	O
unknown	O
precision	O
variance	B
is	O
a	O
normal-gamma	B
distribution	I
this	O
property	O
also	O
holds	O
for	O
the	O
case	O
of	O
the	O
conditional	B
gaussian	B
distribution	O
ptx	O
w	O
of	O
the	O
linear	B
regression	B
model	O
if	O
we	O
consider	O
the	O
likelihood	B
function	I
then	O
the	O
conjugate	B
prior	B
for	O
w	O
and	O
is	O
given	O
by	O
pw	O
n	O
linear	O
models	O
for	B
regression	B
show	O
that	O
the	O
corresponding	O
posterior	O
distribution	O
takes	O
the	O
same	O
functional	B
form	O
so	O
that	O
pw	O
n	O
bn	O
and	O
find	O
expressions	O
for	O
the	O
posterior	O
parameters	O
mn	O
sn	O
an	O
and	O
bn	O
show	O
that	O
the	O
predictive	B
distribution	I
ptx	O
t	O
for	O
the	O
model	O
discussed	O
in	O
ex	O
ercise	O
is	O
given	O
by	O
a	O
student	O
s	O
t-distribution	O
of	O
the	O
form	O
ptx	O
t	O
stt	O
and	O
obtain	O
expressions	O
for	O
and	O
in	O
this	O
exercise	O
we	O
explore	O
in	O
more	O
detail	O
the	O
properties	O
of	O
the	O
equivalent	B
kernel	I
defined	O
by	O
where	O
sn	O
is	O
defined	O
by	O
suppose	O
that	O
the	O
basis	O
functions	O
jx	O
are	O
linearly	O
independent	B
and	O
that	O
the	O
number	O
n	O
of	O
data	O
points	O
is	O
greater	O
than	O
the	O
number	O
m	O
of	O
basis	O
functions	O
furthermore	O
let	O
one	O
of	O
the	O
basis	O
functions	O
be	O
constant	O
say	O
by	O
taking	O
suitable	O
linear	O
combinations	O
of	O
these	O
basis	O
functions	O
we	O
can	O
construct	O
a	O
new	O
basis	O
set	O
jx	O
spanning	O
the	O
same	O
space	O
but	O
that	O
are	O
orthonormal	O
so	O
that	O
jxn	O
kxn	O
ijk	O
where	O
ijk	O
is	O
defined	O
to	O
be	O
if	O
j	O
k	O
and	O
otherwise	O
and	O
we	O
take	O
show	O
that	O
for	O
the	O
equivalent	B
kernel	I
can	O
be	O
written	O
as	O
kx	O
where	O
m	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
kernel	O
satisfies	O
the	O
summation	O
constraint	O
kx	O
xn	O
www	O
consider	O
a	O
linear	O
basis	B
function	I
model	O
for	B
regression	B
in	O
which	O
the	O
parameters	O
and	O
are	O
set	O
using	O
the	O
evidence	O
framework	O
show	O
that	O
the	O
function	O
emn	O
defined	O
by	O
satisfies	O
the	O
relation	O
n	O
derive	O
the	O
result	O
for	O
the	O
log	O
evidence	B
function	I
pt	O
of	O
the	O
linear	B
regression	B
model	O
by	O
making	O
use	O
of	O
to	O
evaluate	O
the	O
integral	O
directly	O
show	O
that	O
the	O
evidence	B
function	I
for	O
the	O
bayesian	B
linear	B
regression	B
model	O
can	O
be	O
written	O
in	O
the	O
form	O
in	O
which	O
ew	O
is	O
defined	O
by	O
www	O
by	O
completing	B
the	I
square	I
over	O
w	O
show	O
that	O
the	O
error	B
function	I
in	O
bayesian	B
linear	B
regression	B
can	O
be	O
written	O
in	O
the	O
form	O
show	O
that	O
the	O
integration	O
over	O
w	O
in	O
the	O
bayesian	B
linear	B
regression	B
model	O
gives	O
the	O
result	O
hence	O
show	O
that	O
the	O
log	O
marginal	B
likelihood	I
is	O
given	O
by	O
www	O
starting	O
from	O
verify	O
all	O
of	O
the	O
steps	O
needed	O
to	O
show	O
that	O
maximization	O
of	O
the	O
log	O
marginal	B
likelihood	B
function	I
with	O
respect	O
to	O
leads	O
to	O
the	O
re-estimation	O
equation	O
exercises	O
an	O
alternative	O
way	O
to	O
derive	O
the	O
result	O
for	O
the	O
optimal	O
value	O
of	O
in	O
the	O
evidence	O
framework	O
is	O
to	O
make	O
use	O
of	O
the	O
identity	O
lna	O
tr	O
d	O
d	O
a	O
d	O
d	O
a	O
prove	O
this	O
identity	O
by	O
considering	O
the	O
eigenvalue	O
expansion	O
of	O
a	O
real	O
symmetric	O
matrix	O
a	O
and	O
making	O
use	O
of	O
the	O
standard	O
results	O
for	O
the	O
determinant	O
and	O
trace	O
of	O
a	O
expressed	O
in	O
terms	O
of	O
its	O
eigenvalues	O
c	O
then	O
make	O
use	O
of	O
to	O
derive	O
starting	O
from	O
starting	O
from	O
verify	O
all	O
of	O
the	O
steps	O
needed	O
to	O
show	O
that	O
maximization	O
of	O
the	O
log	O
marginal	B
likelihood	B
function	I
with	O
respect	O
to	O
leads	O
to	O
the	O
re-estimation	O
equation	O
www	O
show	O
that	O
the	O
marginal	B
probability	B
of	O
the	O
data	O
in	O
other	O
words	O
the	O
model	B
evidence	I
for	O
the	O
model	O
described	O
in	O
exercise	O
is	O
given	O
by	O
pt	O
ban	O
n	O
by	O
first	O
marginalizing	O
with	O
respect	O
to	O
w	O
and	O
then	O
with	O
respect	O
to	O
repeat	O
the	O
previous	O
exercise	O
but	O
now	O
use	O
bayes	B
theorem	O
in	O
the	O
form	O
pt	O
ptw	O
pw	O
and	O
then	O
substitute	O
for	O
the	O
prior	B
and	O
posterior	O
distributions	O
and	O
the	O
likelihood	B
function	I
in	O
order	O
to	O
derive	O
the	O
result	O
linear	O
models	O
for	O
classification	B
in	O
the	O
previous	O
chapter	O
we	O
explored	O
a	O
class	O
of	O
regression	B
models	O
having	O
particularly	O
simple	O
analytical	O
and	O
computational	O
properties	O
we	O
now	O
discuss	O
an	O
analogous	O
class	O
of	O
models	O
for	O
solving	O
classification	B
problems	O
the	O
goal	O
in	O
classification	B
is	O
to	O
take	O
an	O
input	O
vector	O
x	O
and	O
to	O
assign	O
it	O
to	O
one	O
of	O
k	O
discrete	O
classes	O
ck	O
where	O
k	O
k	O
in	O
the	O
most	O
common	O
scenario	O
the	O
classes	O
are	O
taken	O
to	O
be	O
disjoint	O
so	O
that	O
each	O
input	O
is	O
assigned	O
to	O
one	O
and	O
only	O
one	O
class	O
the	O
input	O
space	O
is	O
thereby	O
divided	O
into	O
decision	O
regions	O
whose	O
boundaries	O
are	O
called	O
decision	O
boundaries	O
or	O
decision	O
surfaces	O
in	O
this	O
chapter	O
we	O
consider	O
linear	O
models	O
for	O
classification	B
by	O
which	O
we	O
mean	B
that	O
the	O
decision	O
surfaces	O
are	O
linear	O
functions	O
of	O
the	O
input	O
vector	O
x	O
and	O
hence	O
are	O
defined	O
by	O
hyperplanes	O
within	O
the	O
d-dimensional	O
input	O
space	O
data	O
sets	O
whose	O
classes	O
can	O
be	O
separated	O
exactly	O
by	O
linear	O
decision	O
surfaces	O
are	O
said	O
to	O
be	O
linearly	B
separable	I
for	B
regression	B
problems	O
the	O
target	O
variable	O
t	O
was	O
simply	O
the	O
vector	O
of	O
real	O
numbers	O
whose	O
values	O
we	O
wish	O
to	O
predict	O
in	O
the	O
case	O
of	O
classification	B
there	O
are	O
various	O
linear	O
models	O
for	O
classification	B
ways	O
of	O
using	O
target	O
values	O
to	O
represent	O
class	O
labels	O
for	O
probabilistic	O
models	O
the	O
most	O
convenient	O
in	O
the	O
case	O
of	O
two-class	O
problems	O
is	O
the	O
binary	O
representation	O
in	O
which	O
there	O
is	O
a	O
single	O
target	O
variable	O
t	O
such	O
that	O
t	O
represents	O
class	O
and	O
t	O
represents	O
class	O
we	O
can	O
interpret	O
the	O
value	O
of	O
t	O
as	O
the	O
probability	B
that	O
the	O
class	O
is	O
with	O
the	O
values	O
of	O
probability	B
taking	O
only	O
the	O
extreme	O
values	O
of	O
and	O
for	O
k	O
classes	O
it	O
is	O
convenient	O
to	O
use	O
a	O
coding	O
scheme	O
in	O
which	O
t	O
is	O
a	O
vector	O
of	O
length	O
k	O
such	O
that	O
if	O
the	O
class	O
is	O
cj	O
then	O
all	O
elements	O
tk	O
of	O
t	O
are	O
zero	O
except	O
element	O
tj	O
which	O
takes	O
the	O
value	O
for	O
instance	O
if	O
we	O
have	O
k	O
classes	O
then	O
a	O
pattern	O
from	O
class	O
would	O
be	O
given	O
the	O
target	B
vector	I
t	O
again	O
we	O
can	O
interpret	O
the	O
value	O
of	O
tk	O
as	O
the	O
probability	B
that	O
the	O
class	O
is	O
ck	O
for	O
nonprobabilistic	O
models	O
alternative	O
choices	O
of	O
target	O
variable	O
representation	O
will	O
sometimes	O
prove	O
convenient	O
in	O
chapter	O
we	O
identified	O
three	O
distinct	O
approaches	O
to	O
the	O
classification	B
problem	O
the	O
simplest	O
involves	O
constructing	O
a	O
discriminant	B
function	I
that	O
directly	O
assigns	O
each	O
vector	O
x	O
to	O
a	O
specific	O
class	O
a	O
more	O
powerful	O
approach	O
however	O
models	O
the	O
conditional	B
probability	B
distribution	O
pckx	O
in	O
an	O
inference	B
stage	O
and	O
then	O
subsequently	O
uses	O
this	O
distribution	O
to	O
make	O
optimal	O
decisions	O
by	O
separating	O
inference	B
and	O
decision	O
we	O
gain	O
numerous	O
benefits	O
as	O
discussed	O
in	O
section	O
there	O
are	O
two	O
different	O
approaches	O
to	O
determining	O
the	O
conditional	B
probabilities	O
pckx	O
one	O
technique	O
is	O
to	O
model	O
them	O
directly	O
for	O
example	O
by	O
representing	O
them	O
as	O
parametric	O
models	O
and	O
then	O
optimizing	O
the	O
parameters	O
using	O
a	O
training	B
set	I
alternatively	O
we	O
can	O
adopt	O
a	O
generative	O
approach	O
in	O
which	O
we	O
model	O
the	O
class-conditional	O
densities	O
given	O
by	O
pxck	O
together	O
with	O
the	O
prior	B
probabilities	O
pck	O
for	O
the	O
classes	O
and	O
then	O
we	O
compute	O
the	O
required	O
posterior	O
probabilities	O
using	O
bayes	B
theorem	O
pckx	O
pxckpck	O
px	O
we	O
shall	O
discuss	O
examples	O
of	O
all	O
three	O
approaches	O
in	O
this	O
chapter	O
in	O
the	O
linear	B
regression	B
models	O
considered	O
in	O
chapter	O
the	O
model	O
prediction	O
yx	O
w	O
was	O
given	O
by	O
a	O
linear	O
function	O
of	O
the	O
parameters	O
w	O
in	O
the	O
simplest	O
case	O
the	O
model	O
is	O
also	O
linear	O
in	O
the	O
input	O
variables	O
and	O
therefore	O
takes	O
the	O
form	O
yx	O
wtx	O
so	O
that	O
y	O
is	O
a	O
real	O
number	O
for	O
classification	B
problems	O
however	O
we	O
wish	O
to	O
predict	O
discrete	O
class	O
labels	O
or	O
more	O
generally	O
posterior	O
probabilities	O
that	O
lie	O
in	O
the	O
range	O
to	O
achieve	O
this	O
we	O
consider	O
a	O
generalization	B
of	O
this	O
model	O
in	O
which	O
we	O
transform	O
the	O
linear	O
function	O
of	O
w	O
using	O
a	O
nonlinear	O
function	O
f	O
so	O
that	O
yx	O
f	O
wtx	O
in	O
the	O
machine	O
learning	B
literature	O
f	O
is	O
known	O
as	O
an	O
activation	B
function	I
whereas	O
its	O
inverse	B
is	O
called	O
a	O
link	B
function	I
in	O
the	O
statistics	O
literature	O
the	O
decision	O
surfaces	O
correspond	O
to	O
yx	O
constant	O
so	O
that	O
wtx	O
constant	O
and	O
hence	O
the	O
decision	O
surfaces	O
are	O
linear	O
functions	O
of	O
x	O
even	O
if	O
the	O
function	O
f	O
is	O
nonlinear	O
for	O
this	O
reason	O
the	O
class	O
of	O
models	O
described	O
by	O
are	O
called	O
generalized	B
linear	O
models	O
discriminant	O
functions	O
and	O
nelder	O
note	O
however	O
that	O
in	O
contrast	O
to	O
the	O
models	O
used	O
for	B
regression	B
they	O
are	O
no	O
longer	O
linear	O
in	O
the	O
parameters	O
due	O
to	O
the	O
presence	O
of	O
the	O
nonlinear	O
function	O
f	O
this	O
will	O
lead	O
to	O
more	O
complex	O
analytical	O
and	O
computational	O
properties	O
than	O
for	O
linear	B
regression	B
models	O
nevertheless	O
these	O
models	O
are	O
still	O
relatively	O
simple	O
compared	O
to	O
the	O
more	O
general	O
nonlinear	O
models	O
that	O
will	O
be	O
studied	O
in	O
subsequent	O
chapters	O
the	O
algorithms	O
discussed	O
in	O
this	O
chapter	O
will	O
be	O
equally	O
applicable	O
if	O
we	O
first	O
make	O
a	O
fixed	O
nonlinear	O
transformation	O
of	O
the	O
input	O
variables	O
using	O
a	O
vector	O
of	O
basis	O
functions	O
as	O
we	O
did	O
for	B
regression	B
models	O
in	O
chapter	O
we	O
begin	O
by	O
considering	O
classification	B
directly	O
in	O
the	O
original	O
input	O
space	O
x	O
while	O
in	O
section	O
we	O
shall	O
find	O
it	O
convenient	O
to	O
switch	O
to	O
a	O
notation	O
involving	O
basis	O
functions	O
for	O
consistency	O
with	O
later	O
chapters	O
discriminant	O
functions	O
a	O
discriminant	O
is	O
a	O
function	O
that	O
takes	O
an	O
input	O
vector	O
x	O
and	O
assigns	O
it	O
to	O
one	O
of	O
k	O
classes	O
denoted	O
ck	O
in	O
this	O
chapter	O
we	O
shall	O
restrict	O
attention	O
to	O
linear	O
discriminants	O
namely	O
those	O
for	O
which	O
the	O
decision	O
surfaces	O
are	O
hyperplanes	O
to	O
simplify	O
the	O
discussion	O
we	O
consider	O
first	O
the	O
case	O
of	O
two	O
classes	O
and	O
then	O
investigate	O
the	O
extension	O
to	O
k	O
classes	O
two	O
classes	O
the	O
simplest	O
representation	O
of	O
a	O
linear	B
discriminant	B
function	I
is	O
obtained	O
by	O
tak	O
ing	O
a	O
linear	O
function	O
of	O
the	O
input	O
vector	O
so	O
that	O
yx	O
wtx	O
where	O
w	O
is	O
called	O
a	O
weight	B
vector	I
and	O
is	O
a	O
bias	B
to	O
be	O
confused	O
with	O
bias	B
in	O
the	O
statistical	O
sense	O
the	O
negative	O
of	O
the	O
bias	B
is	O
sometimes	O
called	O
a	O
threshold	O
an	O
input	O
vector	O
x	O
is	O
assigned	O
to	O
class	O
if	O
yx	O
and	O
to	O
class	O
otherwise	O
the	O
corresponding	O
decision	B
boundary	I
is	O
therefore	O
defined	O
by	O
the	O
relation	O
yx	O
which	O
corresponds	O
to	O
a	O
hyperplane	O
within	O
the	O
d-dimensional	O
input	O
space	O
consider	O
two	O
points	O
xa	O
and	O
xb	O
both	O
of	O
which	O
lie	O
on	O
the	O
decision	O
surface	O
because	O
yxa	O
yxb	O
we	O
have	O
wtxa	O
xb	O
and	O
hence	O
the	O
vector	O
w	O
is	O
orthogonal	O
to	O
every	O
vector	O
lying	O
within	O
the	O
decision	O
surface	O
and	O
so	O
w	O
determines	O
the	O
orientation	O
of	O
the	O
decision	O
surface	O
similarly	O
if	O
x	O
is	O
a	O
point	O
on	O
the	O
decision	O
surface	O
then	O
yx	O
and	O
so	O
the	O
normal	O
distance	O
from	O
the	O
origin	O
to	O
the	O
decision	O
surface	O
is	O
given	O
by	O
wtx	O
we	O
therefore	O
see	O
that	O
the	O
bias	B
parameter	I
determines	O
the	O
location	O
of	O
the	O
decision	O
surface	O
these	O
properties	O
are	O
illustrated	O
for	O
the	O
case	O
of	O
d	O
in	O
figure	O
furthermore	O
we	O
note	O
that	O
the	O
value	O
of	O
yx	O
gives	O
a	O
signed	O
measure	O
of	O
the	O
perpendicular	O
distance	O
r	O
of	O
the	O
point	O
x	O
from	O
the	O
decision	O
surface	O
to	O
see	O
this	O
consider	O
linear	O
models	O
for	O
classification	B
figure	O
illustration	O
of	O
the	O
geometry	O
of	O
a	O
linear	B
discriminant	B
function	I
in	O
two	O
dimensions	O
the	O
decision	O
surface	O
shown	O
in	O
red	O
is	O
perpendicular	O
to	O
w	O
and	O
its	O
displacement	O
from	O
the	O
origin	O
is	O
controlled	O
by	O
the	O
bias	B
parameter	I
also	O
the	O
signed	O
orthogonal	O
distance	O
of	O
a	O
general	O
point	O
x	O
from	O
the	O
decision	O
surface	O
is	O
given	O
by	O
y	O
y	O
y	O
w	O
x	O
x	O
an	O
arbitrary	O
point	O
x	O
and	O
let	O
x	O
be	O
its	O
orthogonal	O
projection	O
onto	O
the	O
decision	O
surface	O
so	O
that	O
x	O
x	O
r	O
w	O
multiplying	O
both	O
sides	O
of	O
this	O
result	O
by	O
wt	O
and	O
adding	O
and	O
making	O
use	O
of	O
yx	O
wtx	O
and	O
yx	O
wtx	O
we	O
have	O
r	O
yx	O
this	O
result	O
is	O
illustrated	O
in	O
figure	O
as	O
with	O
the	O
linear	B
regression	B
models	O
in	O
chapter	O
it	O
is	O
sometimes	O
convenient	O
to	O
use	O
a	O
more	O
compact	O
notation	O
in	O
which	O
we	O
introduce	O
an	O
additional	O
dummy	O
input	O
value	O
and	O
then	O
w	O
x	O
so	O
that	O
yx	O
in	O
this	O
case	O
the	O
decision	O
surfaces	O
are	O
d-dimensional	O
hyperplanes	O
passing	O
through	O
the	O
origin	O
of	O
the	O
d	O
expanded	O
input	O
space	O
multiple	O
classes	O
now	O
consider	O
the	O
extension	O
of	O
linear	O
discriminants	O
to	O
k	O
classes	O
we	O
might	O
be	O
tempted	O
be	O
to	O
build	O
a	O
k-class	O
discriminant	O
by	O
combining	O
a	O
number	O
of	O
two-class	O
discriminant	O
functions	O
however	O
this	O
leads	O
to	O
some	O
serious	O
difficulties	O
and	O
hart	O
as	O
we	O
now	O
show	O
consider	O
the	O
use	O
of	O
k	O
classifiers	O
each	O
of	O
which	O
solves	O
a	O
two-class	O
problem	O
of	O
separating	O
points	O
in	O
a	O
particular	O
class	O
ck	O
from	O
points	O
not	O
in	O
that	O
class	O
this	O
is	O
known	O
as	O
a	O
one-versus-the-rest	B
classifier	I
the	O
left-hand	O
example	O
in	O
figure	O
shows	O
an	O
discriminant	O
functions	O
not	O
not	O
figure	O
attempting	O
to	O
construct	O
a	O
k	O
class	O
discriminant	O
from	O
a	O
set	O
of	O
two	O
class	O
discriminants	O
leads	O
to	O
ambiguous	O
regions	O
shown	O
in	O
green	O
on	O
the	O
left	O
is	O
an	O
example	O
involving	O
the	O
use	O
of	O
two	O
discriminants	O
designed	O
to	O
distinguish	O
points	O
in	O
class	O
ck	O
from	O
points	O
not	O
in	O
class	O
ck	O
on	O
the	O
right	O
is	O
an	O
example	O
involving	O
three	O
discriminant	O
functions	O
each	O
of	O
which	O
is	O
used	O
to	O
separate	O
a	O
pair	O
of	O
classes	O
ck	O
and	O
cj	O
example	O
involving	O
three	O
classes	O
where	O
this	O
approach	O
leads	O
to	O
regions	O
of	O
input	O
space	O
that	O
are	O
ambiguously	O
classified	O
an	O
alternative	O
is	O
to	O
introduce	O
kk	O
binary	O
discriminant	O
functions	O
one	O
for	O
every	O
possible	O
pair	O
of	O
classes	O
this	O
is	O
known	O
as	O
a	O
one-versus-one	B
classifier	I
each	O
point	O
is	O
then	O
classified	O
according	O
to	O
a	O
majority	O
vote	O
amongst	O
the	O
discriminant	O
functions	O
however	O
this	O
too	O
runs	O
into	O
the	O
problem	O
of	O
ambiguous	O
regions	O
as	O
illustrated	O
in	O
the	O
right-hand	O
diagram	O
of	O
figure	O
we	O
can	O
avoid	O
these	O
difficulties	O
by	O
considering	O
a	O
single	O
k-class	O
discriminant	O
comprising	O
k	O
linear	O
functions	O
of	O
the	O
form	O
ykx	O
wt	O
and	O
then	O
assigning	O
a	O
point	O
x	O
to	O
class	O
ck	O
if	O
ykx	O
yjx	O
for	O
all	O
j	O
k	O
the	O
decision	B
boundary	I
between	O
class	O
ck	O
and	O
class	O
cj	O
is	O
therefore	O
given	O
by	O
ykx	O
yjx	O
and	O
hence	O
corresponds	O
to	O
a	O
hyperplane	O
defined	O
by	O
k	O
x	O
wjtx	O
this	O
has	O
the	O
same	O
form	O
as	O
the	O
decision	B
boundary	I
for	O
the	O
two-class	O
case	O
discussed	O
in	O
section	O
and	O
so	O
analogous	O
geometrical	O
properties	O
apply	O
the	O
decision	O
regions	O
of	O
such	O
a	O
discriminant	O
are	O
always	O
singly	O
connected	O
and	O
convex	O
to	O
see	O
this	O
consider	O
two	O
points	O
xa	O
and	O
xb	O
both	O
of	O
which	O
lie	O
inside	O
decision	B
region	I
rk	O
as	O
illustrated	O
in	O
figure	O
any	O
that	O
lies	O
on	O
the	O
line	O
connecting	O
xa	O
and	O
xb	O
can	O
be	O
expressed	O
in	O
the	O
form	O
xa	O
linear	O
models	O
for	O
classification	B
figure	O
illustration	O
of	O
the	O
decision	O
regions	O
for	O
a	O
multiclass	B
linear	B
discriminant	I
with	O
the	O
decision	O
if	O
two	O
points	O
xa	O
boundaries	O
shown	O
in	O
red	O
and	O
xb	O
both	O
lie	O
inside	O
the	O
same	O
decision	B
region	I
rk	O
then	O
any	O
point	O
bx	O
that	O
lies	O
on	O
the	O
line	O
connecting	O
these	O
two	O
points	O
must	O
also	O
lie	O
in	O
rk	O
and	O
hence	O
the	O
decision	B
region	I
must	O
be	O
singly	O
connected	O
and	O
convex	O
ri	O
xa	O
rj	O
rk	O
x	O
xb	O
where	O
from	O
the	O
linearity	O
of	O
the	O
discriminant	O
functions	O
it	O
follows	O
that	O
ykxa	O
ykxb	O
yjxb	O
for	O
all	O
j	O
k	O
and	O
hence	O
and	O
also	O
lies	O
because	O
both	O
xa	O
and	O
xb	O
lie	O
inside	O
rk	O
it	O
follows	O
that	O
ykxa	O
yjxa	O
and	O
inside	O
rk	O
thus	O
rk	O
is	O
singly	O
connected	O
and	O
convex	O
note	O
that	O
for	O
two	O
classes	O
we	O
can	O
either	O
employ	O
the	O
formalism	O
discussed	O
here	O
based	O
on	O
two	O
discriminant	O
functions	O
and	O
or	O
else	O
use	O
the	O
simpler	O
but	O
equivalent	O
formulation	O
described	O
in	O
section	O
based	O
on	O
a	O
single	O
discriminant	B
function	I
yx	O
we	O
now	O
explore	O
three	O
approaches	O
to	O
learning	B
the	O
parameters	O
of	O
linear	B
discriminant	I
functions	O
based	O
on	O
least	O
squares	O
fisher	B
s	O
linear	B
discriminant	I
and	O
the	O
perceptron	B
algorithm	O
least	O
squares	O
for	O
classification	B
in	O
chapter	O
we	O
considered	O
models	O
that	O
were	O
linear	O
functions	O
of	O
the	O
parameters	O
and	O
we	O
saw	O
that	O
the	O
minimization	O
of	O
a	O
sum-of-squares	B
error	B
function	I
led	O
to	O
a	O
simple	O
closed-form	O
solution	O
for	O
the	O
parameter	O
values	O
it	O
is	O
therefore	O
tempting	O
to	O
see	O
if	O
we	O
can	O
apply	O
the	O
same	O
formalism	O
to	O
classification	B
problems	O
consider	O
a	O
general	O
classification	B
problem	O
with	O
k	O
classes	O
with	O
a	O
binary	O
coding	O
scheme	O
for	O
the	O
target	B
vector	I
t	O
one	O
justification	O
for	O
using	O
least	O
squares	O
in	O
such	O
a	O
context	O
is	O
that	O
it	O
approximates	O
the	O
conditional	B
expectation	B
etx	O
of	O
the	O
target	O
values	O
given	O
the	O
input	O
vector	O
for	O
the	O
binary	O
coding	O
scheme	O
this	O
conditional	B
expectation	B
is	O
given	O
by	O
the	O
vector	O
of	O
posterior	O
class	O
probabilities	O
unfortunately	O
however	O
these	O
probabilities	O
are	O
typically	O
approximated	O
rather	O
poorly	O
indeed	O
the	O
approximations	O
can	O
have	O
values	O
outside	O
the	O
range	O
due	O
to	O
the	O
limited	O
flexibility	O
of	O
a	O
linear	O
model	O
as	O
we	O
shall	O
see	O
shortly	O
each	O
class	O
ck	O
is	O
described	O
by	O
its	O
own	O
linear	O
model	O
so	O
that	O
ykx	O
wt	O
k	O
x	O
where	O
k	O
k	O
we	O
can	O
conveniently	O
group	O
these	O
together	O
using	O
vector	O
notation	O
so	O
that	O
yx	O
can	O
then	O
be	O
written	O
as	O
discriminant	O
functions	O
a	O
dummy	O
input	O
this	O
representation	O
was	O
discussed	O
in	O
detail	O
in	O
section	O
a	O
error	B
function	I
as	O
we	O
did	O
for	B
regression	B
in	O
chapter	O
consider	O
a	O
training	B
data	O
set	O
tn	O
where	O
n	O
n	O
and	O
define	O
a	O
matrix	O
t	O
whose	O
nth	O
row	O
is	O
the	O
vector	O
tt	O
n	O
n	O
the	O
sum-of-squares	B
error	B
function	I
where	O
is	O
a	O
matrix	O
whose	O
kth	O
column	O
comprises	O
the	O
d	O
vector	O
wt	O
k	O
is	O
the	O
corresponding	O
augmented	O
input	O
vector	O
xtt	O
with	O
new	O
input	O
x	O
is	O
then	O
assigned	O
to	O
the	O
class	O
for	O
which	O
the	O
output	O
yk	O
is	O
largest	O
we	O
now	O
determine	O
the	O
parameter	O
matrix	O
by	O
minimizing	O
a	O
sum-of-squares	O
together	O
with	O
a	O
matrix	O
whose	O
nth	O
row	O
t	O
setting	O
the	O
derivative	B
with	O
respect	O
tow	O
to	O
zero	O
and	O
rearranging	O
we	O
then	O
obtain	O
the	O
solution	O
forw	O
in	O
the	O
formw	O
t	O
is	O
the	O
pseudo-inverse	B
of	O
the	O
matrix	O
as	O
discussed	O
in	O
section	O
we	O
where	O
yx	O
tt	O
then	O
obtain	O
the	O
discriminant	B
function	I
in	O
the	O
form	O
edw	O
tr	O
exercise	O
section	O
an	O
interesting	O
property	O
of	O
least-squares	O
solutions	O
with	O
multiple	O
target	O
variables	O
is	O
that	O
if	O
every	O
target	B
vector	I
in	O
the	O
training	B
set	I
satisfies	O
some	O
linear	O
constraint	O
attn	O
b	O
for	O
some	O
constants	O
a	O
and	O
b	O
then	O
the	O
model	O
prediction	O
for	O
any	O
value	O
of	O
x	O
will	O
satisfy	O
the	O
same	O
constraint	O
so	O
that	O
atyx	O
b	O
thus	O
if	O
we	O
use	O
a	O
coding	O
scheme	O
for	O
k	O
classes	O
then	O
the	O
predictions	O
made	O
by	O
the	O
model	O
will	O
have	O
the	O
property	O
that	O
the	O
elements	O
of	O
yx	O
will	O
sum	O
to	O
for	O
any	O
value	O
of	O
x	O
however	O
this	O
summation	O
constraint	O
alone	O
is	O
not	O
sufficient	O
to	O
allow	O
the	O
model	O
outputs	O
to	O
be	O
interpreted	O
as	O
probabilities	O
because	O
they	O
are	O
not	O
constrained	O
to	O
lie	O
within	O
the	O
interval	O
the	O
least-squares	O
approach	O
gives	O
an	O
exact	O
closed-form	O
solution	O
for	O
the	O
discriminant	B
function	I
parameters	O
however	O
even	O
as	O
a	O
discriminant	B
function	I
we	O
use	O
it	O
to	O
make	O
decisions	O
directly	O
and	O
dispense	O
with	O
any	O
probabilistic	O
interpretation	O
it	O
suffers	O
from	O
some	O
severe	O
problems	O
we	O
have	O
already	O
seen	O
that	O
least-squares	O
solutions	O
lack	O
robustness	B
to	O
outliers	B
and	O
this	O
applies	O
equally	O
to	O
the	O
classification	B
application	O
as	O
illustrated	O
in	O
figure	O
here	O
we	O
see	O
that	O
the	O
additional	O
data	O
points	O
in	O
the	O
righthand	O
figure	O
produce	O
a	O
significant	O
change	O
in	O
the	O
location	O
of	O
the	O
decision	B
boundary	I
even	O
though	O
these	O
point	O
would	O
be	O
correctly	O
classified	O
by	O
the	O
original	O
decision	B
boundary	I
in	O
the	O
left-hand	O
figure	O
the	O
sum-of-squares	B
error	B
function	I
penalizes	O
predictions	O
that	O
are	O
too	O
correct	O
in	O
that	O
they	O
lie	O
a	O
long	O
way	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	O
linear	O
models	O
for	O
classification	B
figure	O
the	O
left	O
plot	O
shows	O
data	O
from	O
two	O
classes	O
denoted	O
by	O
red	O
crosses	O
and	O
blue	O
circles	O
together	O
with	O
the	O
decision	B
boundary	I
found	O
by	O
least	O
squares	O
curve	O
and	O
also	O
by	O
the	O
logistic	B
regression	B
model	O
curve	O
which	O
is	O
discussed	O
later	O
in	O
section	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
results	O
obtained	O
when	O
extra	O
data	O
points	O
are	O
added	O
at	O
the	O
bottom	O
left	O
of	O
the	O
diagram	O
showing	O
that	O
least	O
squares	O
is	O
highly	O
sensitive	O
to	O
outliers	B
unlike	O
logistic	B
regression	B
boundary	O
in	O
section	O
we	O
shall	O
consider	O
several	O
alternative	O
error	B
functions	O
for	O
classification	B
and	O
we	O
shall	O
see	O
that	O
they	O
do	O
not	O
suffer	O
from	O
this	O
difficulty	O
however	O
problems	O
with	O
least	O
squares	O
can	O
be	O
more	O
severe	O
than	O
simply	O
lack	O
of	O
robustness	B
as	O
illustrated	O
in	O
figure	O
this	O
shows	O
a	O
synthetic	O
data	O
set	O
drawn	O
from	O
three	O
classes	O
in	O
a	O
two-dimensional	O
input	O
space	O
having	O
the	O
property	O
that	O
linear	O
decision	O
boundaries	O
can	O
give	O
excellent	O
separation	O
between	O
the	O
classes	O
indeed	O
the	O
technique	O
of	O
logistic	B
regression	B
described	O
later	O
in	O
this	O
chapter	O
gives	O
a	O
satisfactory	O
solution	O
as	O
seen	O
in	O
the	O
right-hand	O
plot	O
however	O
the	O
least-squares	O
solution	O
gives	O
poor	O
results	O
with	O
only	O
a	O
small	O
region	O
of	O
the	O
input	O
space	O
assigned	O
to	O
the	O
green	O
class	O
the	O
failure	O
of	O
least	O
squares	O
should	O
not	O
surprise	O
us	O
when	O
we	O
recall	O
that	O
it	O
corresponds	O
to	O
maximum	B
likelihood	I
under	O
the	O
assumption	O
of	O
a	O
gaussian	B
conditional	B
distribution	O
whereas	O
binary	O
target	O
vectors	O
clearly	O
have	O
a	O
distribution	O
that	O
is	O
far	O
from	O
gaussian	B
by	O
adopting	O
more	O
appropriate	O
probabilistic	O
models	O
we	O
shall	O
obtain	O
classification	B
techniques	O
with	O
much	O
better	O
properties	O
than	O
least	O
squares	O
for	O
the	O
moment	O
however	O
we	O
continue	O
to	O
explore	O
alternative	O
nonprobabilistic	O
methods	O
for	O
setting	O
the	O
parameters	O
in	O
the	O
linear	O
classification	B
models	O
fisher	B
s	O
linear	B
discriminant	I
one	O
way	O
to	O
view	O
a	O
linear	O
classification	B
model	O
is	O
in	O
terms	O
of	O
dimensionality	O
reduction	O
consider	O
first	O
the	O
case	O
of	O
two	O
classes	O
and	O
suppose	O
we	O
take	O
the	O
d	O
discriminant	O
functions	O
figure	O
example	O
of	O
a	O
synthetic	O
data	O
set	O
comprising	O
three	O
classes	O
with	O
training	B
data	O
points	O
denoted	O
in	O
red	O
green	O
and	O
blue	O
lines	O
denote	O
the	O
decision	O
boundaries	O
and	O
the	O
background	O
colours	O
denote	O
the	O
respective	O
classes	O
of	O
the	O
decision	O
regions	O
on	O
the	O
left	O
is	O
the	O
result	O
of	O
using	O
a	O
least-squares	O
discriminant	O
we	O
see	O
that	O
the	O
region	O
of	O
input	O
space	O
assigned	O
to	O
the	O
green	O
class	O
is	O
too	O
small	O
and	O
so	O
most	O
of	O
the	O
points	O
from	O
this	O
class	O
are	O
misclassified	O
on	O
the	O
right	O
is	O
the	O
result	O
of	O
using	O
logistic	O
regressions	O
as	O
described	O
in	O
section	O
showing	O
correct	O
classification	B
of	O
the	O
training	B
data	O
dimensional	O
input	O
vector	O
x	O
and	O
project	O
it	O
down	O
to	O
one	O
dimension	O
using	O
y	O
wtx	O
if	O
we	O
place	O
a	O
threshold	O
on	O
y	O
and	O
classify	O
y	O
as	O
class	O
and	O
otherwise	O
class	O
then	O
we	O
obtain	O
our	O
standard	O
linear	O
classifier	O
discussed	O
in	O
the	O
previous	O
section	O
in	O
general	O
the	O
projection	O
onto	O
one	O
dimension	O
leads	O
to	O
a	O
considerable	O
loss	O
of	O
information	O
and	O
classes	O
that	O
are	O
well	O
separated	O
in	O
the	O
original	O
d-dimensional	O
space	O
may	O
become	O
strongly	O
overlapping	O
in	O
one	O
dimension	O
however	O
by	O
adjusting	O
the	O
components	O
of	O
the	O
weight	B
vector	I
w	O
we	O
can	O
select	O
a	O
projection	O
that	O
maximizes	O
the	O
class	O
separation	O
to	O
begin	O
with	O
consider	O
a	O
two-class	O
problem	O
in	O
which	O
there	O
are	O
points	O
of	O
class	O
and	O
points	O
of	O
class	O
so	O
that	O
the	O
mean	B
vectors	O
of	O
the	O
two	O
classes	O
are	O
given	O
by	O
xn	O
xn	O
n	O
n	O
the	O
simplest	O
measure	O
of	O
the	O
separation	O
of	O
the	O
classes	O
when	O
projected	O
onto	O
w	O
is	O
the	O
separation	O
of	O
the	O
projected	O
class	O
means	O
this	O
suggests	O
that	O
we	O
might	O
choose	O
w	O
so	O
as	O
to	O
maximize	O
where	O
mk	O
wtmk	O
linear	O
models	O
for	O
classification	B
figure	O
the	O
left	O
plot	O
shows	O
samples	O
from	O
two	O
classes	O
in	O
red	O
and	O
blue	O
along	O
with	O
the	O
histograms	O
resulting	O
from	O
projection	O
onto	O
the	O
line	O
joining	O
the	O
class	O
means	O
note	O
that	O
there	O
is	O
considerable	O
class	O
overlap	O
in	O
the	O
projected	O
space	O
the	O
right	O
plot	O
shows	O
the	O
corresponding	O
projection	O
based	O
on	O
the	O
fisher	B
linear	B
discriminant	I
showing	O
the	O
greatly	O
improved	O
class	O
separation	O
appendix	O
e	O
exercise	O
i	O
is	O
the	O
mean	B
of	O
the	O
projected	O
data	O
from	O
class	O
ck	O
however	O
this	O
expression	O
can	O
be	O
made	O
arbitrarily	O
large	O
simply	O
by	O
increasing	O
the	O
magnitude	O
of	O
w	O
to	O
solve	O
this	O
i	O
using	O
problem	O
we	O
could	O
constrain	O
w	O
to	O
have	O
unit	O
length	O
so	O
that	O
a	O
lagrange	B
multiplier	I
to	O
perform	O
the	O
constrained	O
maximization	O
we	O
then	O
find	O
that	O
w	O
there	O
is	O
still	O
a	O
problem	O
with	O
this	O
approach	O
however	O
as	O
illustrated	O
in	O
figure	O
this	O
shows	O
two	O
classes	O
that	O
are	O
well	O
separated	O
in	O
the	O
original	O
twodimensional	O
space	O
but	O
that	O
have	O
considerable	O
overlap	O
when	O
projected	O
onto	O
the	O
line	O
joining	O
their	O
means	O
this	O
difficulty	O
arises	O
from	O
the	O
strongly	O
nondiagonal	O
covariances	O
of	O
the	O
class	O
distributions	O
the	O
idea	O
proposed	O
by	O
fisher	B
is	O
to	O
maximize	O
a	O
function	O
that	O
will	O
give	O
a	O
large	O
separation	O
between	O
the	O
projected	O
class	O
means	O
while	O
also	O
giving	O
a	O
small	O
variance	B
within	O
each	O
class	O
thereby	O
minimizing	O
the	O
class	O
overlap	O
the	O
projection	O
formula	O
transforms	O
the	O
set	O
of	O
labelled	O
data	O
points	O
in	O
x	O
into	O
a	O
labelled	O
set	O
in	O
the	O
one-dimensional	O
space	O
y	O
the	O
within-class	B
variance	B
of	O
the	O
transformed	O
data	O
from	O
class	O
ck	O
is	O
therefore	O
given	O
by	O
k	O
n	O
ck	O
where	O
yn	O
wtxn	O
we	O
can	O
define	O
the	O
total	O
within-class	B
variance	B
for	O
the	O
whole	O
data	O
set	O
to	O
be	O
simply	O
the	O
fisher	B
criterion	O
is	O
defined	O
to	O
be	O
the	O
ratio	O
of	O
the	O
between-class	B
variance	B
to	O
the	O
within-class	B
variance	B
and	O
is	O
given	O
by	O
jw	O
exercise	O
we	O
can	O
make	O
the	O
dependence	O
on	O
w	O
explicit	O
by	O
using	O
and	O
to	O
rewrite	O
the	O
fisher	B
criterion	O
in	O
the	O
form	O
discriminant	O
functions	O
jw	O
wtsbw	O
wtsww	O
where	O
sb	O
is	O
the	O
between-class	B
covariance	B
matrix	O
and	O
is	O
given	O
by	O
sb	O
and	O
sw	O
is	O
the	O
total	O
within-class	B
covariance	B
matrix	O
given	O
by	O
sw	O
n	O
n	O
differentiating	O
with	O
respect	O
to	O
w	O
we	O
find	O
that	O
jw	O
is	O
maximized	O
when	O
from	O
we	O
see	O
that	O
sbw	O
is	O
always	O
in	O
the	O
direction	O
of	O
furthermore	O
we	O
do	O
not	O
care	O
about	O
the	O
magnitude	O
of	O
w	O
only	O
its	O
direction	O
and	O
so	O
we	O
can	O
drop	O
the	O
scalar	O
factors	O
and	O
multiplying	O
both	O
sides	O
of	O
by	O
s	O
w	O
we	O
then	O
obtain	O
note	O
that	O
if	O
the	O
within-class	B
covariance	B
is	O
isotropic	B
so	O
that	O
sw	O
is	O
proportional	O
to	O
the	O
unit	O
matrix	O
we	O
find	O
that	O
w	O
is	O
proportional	O
to	O
the	O
difference	O
of	O
the	O
class	O
means	O
as	O
discussed	O
above	O
w	O
s	O
w	O
the	O
result	O
is	O
known	O
as	O
fisher	B
s	O
linear	B
discriminant	I
although	O
strictly	O
it	O
is	O
not	O
a	O
discriminant	O
but	O
rather	O
a	O
specific	O
choice	O
of	O
direction	O
for	O
projection	O
of	O
the	O
data	O
down	O
to	O
one	O
dimension	O
however	O
the	O
projected	O
data	O
can	O
subsequently	O
be	O
used	O
to	O
construct	O
a	O
discriminant	O
by	O
choosing	O
a	O
threshold	O
so	O
that	O
we	O
classify	O
a	O
new	O
point	O
as	O
belonging	O
to	O
if	O
yx	O
and	O
classify	O
it	O
as	O
belonging	O
to	O
otherwise	O
for	O
example	O
we	O
can	O
model	O
the	O
class-conditional	O
densities	O
pyck	O
using	O
gaussian	B
distributions	O
and	O
then	O
use	O
the	O
techniques	O
of	O
section	O
to	O
find	O
the	O
parameters	O
of	O
the	O
gaussian	B
distributions	O
by	O
maximum	B
likelihood	I
having	O
found	O
gaussian	B
approximations	O
to	O
the	O
projected	O
classes	O
the	O
formalism	O
of	O
section	O
then	O
gives	O
an	O
expression	O
for	O
the	O
optimal	O
threshold	O
some	O
justification	O
for	O
the	O
gaussian	B
assumption	O
comes	O
from	O
the	O
central	B
limit	I
theorem	I
by	O
noting	O
that	O
y	O
wtx	O
is	O
the	O
sum	O
of	O
a	O
set	O
of	O
random	O
variables	O
relation	O
to	O
least	O
squares	O
the	O
least-squares	O
approach	O
to	O
the	O
determination	O
of	O
a	O
linear	B
discriminant	I
was	O
based	O
on	O
the	O
goal	O
of	O
making	O
the	O
model	O
predictions	O
as	O
close	O
as	O
possible	O
to	O
a	O
set	O
of	O
target	O
values	O
by	O
contrast	O
the	O
fisher	B
criterion	O
was	O
derived	O
by	O
requiring	O
maximum	O
class	O
separation	O
in	O
the	O
output	O
space	O
it	O
is	O
interesting	O
to	O
see	O
the	O
relationship	O
between	O
these	O
two	O
approaches	O
in	O
particular	O
we	O
shall	O
show	O
that	O
for	O
the	O
two-class	O
problem	O
the	O
fisher	B
criterion	O
can	O
be	O
obtained	O
as	O
a	O
special	O
case	O
of	O
least	O
squares	O
so	O
far	O
we	O
have	O
considered	O
coding	O
for	O
the	O
target	O
values	O
if	O
however	O
we	O
adopt	O
a	O
slightly	O
different	O
target	O
coding	O
scheme	O
then	O
the	O
least-squares	O
solution	O
for	O
linear	O
models	O
for	O
classification	B
the	O
weights	O
becomes	O
equivalent	O
to	O
the	O
fisher	B
solution	O
and	O
hart	O
in	O
particular	O
we	O
shall	O
take	O
the	O
targets	O
for	O
class	O
to	O
be	O
where	O
is	O
the	O
number	O
of	O
patterns	O
in	O
class	O
and	O
n	O
is	O
the	O
total	O
number	O
of	O
patterns	O
this	O
target	O
value	O
approximates	O
the	O
reciprocal	O
of	O
the	O
prior	B
probability	B
for	O
class	O
for	O
class	O
we	O
shall	O
take	O
the	O
targets	O
to	O
be	O
where	O
is	O
the	O
number	O
of	O
patterns	O
in	O
class	O
setting	O
the	O
derivatives	O
of	O
e	O
with	O
respect	O
to	O
and	O
w	O
to	O
zero	O
we	O
obtain	O
respectively	O
the	O
sum-of-squares	B
error	B
function	I
can	O
be	O
written	O
wtxn	O
tn	O
e	O
wtxn	O
tn	O
wtxn	O
tn	O
xn	O
from	O
and	O
making	O
use	O
of	O
our	O
choice	O
of	O
target	O
coding	O
scheme	O
for	O
the	O
tn	O
we	O
obtain	O
an	O
expression	O
for	O
the	O
bias	B
in	O
the	O
form	O
where	O
we	O
have	O
used	O
wtm	O
tn	O
n	O
n	O
and	O
where	O
m	O
is	O
the	O
mean	B
of	O
the	O
total	O
data	O
set	O
and	O
is	O
given	O
by	O
m	O
n	O
xn	O
n	O
exercise	O
after	O
some	O
straightforward	O
algebra	O
and	O
again	O
making	O
use	O
of	O
the	O
choice	O
of	O
tn	O
the	O
second	O
equation	O
becomes	O
sw	O
n	O
sb	O
w	O
where	O
sw	O
is	O
defined	O
by	O
sb	O
is	O
defined	O
by	O
and	O
we	O
have	O
substituted	O
for	O
the	O
bias	B
using	O
using	O
we	O
note	O
that	O
sbw	O
is	O
always	O
in	O
the	O
direction	O
of	O
thus	O
we	O
can	O
write	O
w	O
s	O
w	O
where	O
we	O
have	O
ignored	O
irrelevant	O
scale	O
factors	O
thus	O
the	O
weight	B
vector	I
coincides	O
with	O
that	O
found	O
from	O
the	O
fisher	B
criterion	O
in	O
addition	O
we	O
have	O
also	O
found	O
an	O
expression	O
for	O
the	O
bias	B
value	O
given	O
by	O
this	O
tells	O
us	O
that	O
a	O
new	O
vector	O
x	O
should	O
be	O
classified	O
as	O
belonging	O
to	O
class	O
if	O
yx	O
wtx	O
m	O
and	O
class	O
otherwise	O
discriminant	O
functions	O
fisher	B
s	O
discriminant	O
for	O
multiple	O
classes	O
we	O
now	O
consider	O
the	O
generalization	B
of	O
the	O
fisher	B
discriminant	O
to	O
k	O
classes	O
and	O
we	O
shall	O
assume	O
that	O
the	O
dimensionality	O
d	O
of	O
the	O
input	O
space	O
is	O
greater	O
than	O
the	O
k	O
x	O
where	O
number	O
k	O
of	O
classes	O
next	O
we	O
introduce	O
d	O
k	O
d	O
these	O
feature	O
values	O
can	O
conveniently	O
be	O
grouped	O
together	O
to	O
form	O
a	O
vector	O
y	O
similarly	O
the	O
weight	O
vectors	O
can	O
be	O
considered	O
to	O
be	O
the	O
columns	O
of	O
a	O
matrix	O
w	O
so	O
that	O
linear	O
features	O
yk	O
wt	O
note	O
that	O
again	O
we	O
are	O
not	O
including	O
any	O
bias	B
parameters	O
in	O
the	O
definition	O
of	O
y	O
the	O
generalization	B
of	O
the	O
within-class	B
covariance	B
matrix	O
to	O
the	O
case	O
of	O
k	O
classes	O
follows	O
from	O
to	O
give	O
y	O
wtx	O
sw	O
sk	O
xn	O
n	O
ck	O
nk	O
n	O
ck	O
mkxn	O
mkt	O
where	O
sk	O
mk	O
and	O
nk	O
is	O
the	O
number	O
of	O
patterns	O
in	O
class	O
ck	O
in	O
order	O
to	O
find	O
a	O
generalization	B
of	O
the	O
between-class	B
covariance	B
matrix	O
we	O
follow	O
duda	O
and	O
hart	O
and	O
consider	O
first	O
the	O
total	O
covariance	B
matrix	O
where	O
m	O
is	O
the	O
mean	B
of	O
the	O
total	O
data	O
set	O
st	O
mxn	O
mt	O
m	O
n	O
xn	O
n	O
nkmk	O
and	O
n	O
k	O
nk	O
is	O
the	O
total	O
number	O
of	O
data	O
points	O
the	O
total	O
covariance	B
matrix	O
can	O
be	O
decomposed	O
into	O
the	O
sum	O
of	O
the	O
within-class	B
covariance	B
matrix	O
given	O
by	O
and	O
plus	O
an	O
additional	O
matrix	O
sb	O
which	O
we	O
identify	O
as	O
a	O
measure	O
of	O
the	O
between-class	B
covariance	B
st	O
sw	O
sb	O
where	O
sb	O
nkmk	O
mmk	O
mt	O
n	O
ck	O
n	O
ck	O
linear	O
models	O
for	O
classification	B
these	O
covariance	B
matrices	O
have	O
been	O
defined	O
in	O
the	O
original	O
x-space	O
we	O
can	O
now	O
define	O
similar	O
matrices	O
in	O
the	O
projected	O
d	O
y-space	O
sw	O
kyn	O
kt	O
and	O
where	O
sb	O
k	O
nk	O
nk	O
k	O
k	O
yn	O
nk	O
k	O
n	O
jw	O
tr	O
s	O
w	O
sb	O
again	O
we	O
wish	O
to	O
construct	O
a	O
scalar	O
that	O
is	O
large	O
when	O
the	O
between-class	B
covariance	B
is	O
large	O
and	O
when	O
the	O
within-class	B
covariance	B
is	O
small	O
there	O
are	O
now	O
many	O
possible	O
choices	O
of	O
criterion	O
one	O
example	O
is	O
given	O
by	O
this	O
criterion	O
can	O
then	O
be	O
rewritten	O
as	O
an	O
explicit	O
function	O
of	O
the	O
projection	O
matrix	O
w	O
in	O
the	O
form	O
jw	O
tr	O
maximization	O
of	O
such	O
criteria	O
is	O
straightforward	O
though	O
somewhat	O
involved	O
and	O
is	O
discussed	O
at	O
length	O
in	O
fukunaga	O
the	O
weight	O
values	O
are	O
determined	O
by	O
those	O
eigenvectors	O
of	O
s	O
w	O
sb	O
that	O
correspond	O
to	O
the	O
d	O
largest	O
eigenvalues	O
there	O
is	O
one	O
important	O
result	O
that	O
is	O
common	O
to	O
all	O
such	O
criteria	O
which	O
is	O
worth	O
emphasizing	O
we	O
first	O
note	O
from	O
that	O
sb	O
is	O
composed	O
of	O
the	O
sum	O
of	O
k	O
matrices	O
each	O
of	O
which	O
is	O
an	O
outer	O
product	O
of	O
two	O
vectors	O
and	O
therefore	O
of	O
rank	O
in	O
addition	O
only	O
of	O
these	O
matrices	O
are	O
independent	B
as	O
a	O
result	O
of	O
the	O
constraint	O
thus	O
sb	O
has	O
rank	O
at	O
most	O
equal	O
to	O
and	O
so	O
there	O
are	O
at	O
most	O
nonzero	O
eigenvalues	O
this	O
shows	O
that	O
the	O
projection	O
onto	O
the	O
subspace	O
spanned	O
by	O
the	O
eigenvectors	O
of	O
sb	O
does	O
not	O
alter	O
the	O
value	O
of	O
jw	O
and	O
so	O
we	O
are	O
therefore	O
unable	O
to	O
find	O
more	O
than	O
linear	O
features	O
by	O
this	O
means	O
the	O
perceptron	B
algorithm	O
another	O
example	O
of	O
a	O
linear	B
discriminant	I
model	O
is	O
the	O
perceptron	B
of	O
rosenblatt	B
which	O
occupies	O
an	O
important	O
place	O
in	O
the	O
history	O
of	O
pattern	O
recognition	O
algorithms	O
it	O
corresponds	O
to	O
a	O
two-class	O
model	O
in	O
which	O
the	O
input	O
vector	O
x	O
is	O
first	O
transformed	O
using	O
a	O
fixed	O
nonlinear	O
transformation	O
to	O
give	O
a	O
feature	O
vector	O
and	O
this	O
is	O
then	O
used	O
to	O
construct	O
a	O
generalized	B
linear	I
model	I
of	O
the	O
form	O
yx	O
f	O
wt	O
discriminant	O
functions	O
where	O
the	O
nonlinear	O
activation	B
function	I
f	O
is	O
given	O
by	O
a	O
step	O
function	O
of	O
the	O
form	O
fa	O
a	O
a	O
the	O
vector	O
will	O
typically	O
include	O
a	O
bias	B
component	O
in	O
earlier	O
discussions	O
of	O
two-class	O
classification	B
problems	O
we	O
have	O
focussed	O
on	O
a	O
target	O
coding	O
scheme	O
in	O
which	O
t	O
which	O
is	O
appropriate	O
in	O
the	O
context	O
of	O
probabilistic	O
models	O
for	O
the	O
perceptron	B
however	O
it	O
is	O
more	O
convenient	O
to	O
use	O
target	O
values	O
t	O
for	O
class	O
and	O
t	O
for	O
class	O
which	O
matches	O
the	O
choice	O
of	O
activation	B
function	I
the	O
algorithm	O
used	O
to	O
determine	O
the	O
parameters	O
w	O
of	O
the	O
perceptron	B
can	O
most	O
easily	O
be	O
motivated	O
by	O
error	B
function	I
minimization	O
a	O
natural	O
choice	O
of	O
error	B
function	I
would	O
be	O
the	O
total	O
number	O
of	O
misclassified	O
patterns	O
however	O
this	O
does	O
not	O
lead	O
to	O
a	O
simple	O
learning	B
algorithm	O
because	O
the	O
error	B
is	O
a	O
piecewise	O
constant	O
function	O
of	O
w	O
with	O
discontinuities	O
wherever	O
a	O
change	O
in	O
w	O
causes	O
the	O
decision	B
boundary	I
to	O
move	O
across	O
one	O
of	O
the	O
data	O
points	O
methods	O
based	O
on	O
changing	O
w	O
using	O
the	O
gradient	O
of	O
the	O
error	B
function	I
cannot	O
then	O
be	O
applied	O
because	O
the	O
gradient	O
is	O
zero	O
almost	O
everywhere	O
we	O
therefore	O
consider	O
an	O
alternative	O
error	B
function	I
known	O
as	O
the	O
perceptron	B
criterion	I
to	O
derive	O
this	O
we	O
note	O
that	O
we	O
are	O
seeking	O
a	O
weight	B
vector	I
w	O
such	O
that	O
patterns	O
xn	O
in	O
class	O
will	O
have	O
wt	O
whereas	O
patterns	O
xn	O
in	O
class	O
have	O
wt	O
using	O
the	O
t	O
target	O
coding	O
scheme	O
it	O
follows	O
that	O
we	O
would	O
like	O
all	O
patterns	O
to	O
satisfy	O
wt	O
the	O
perceptron	B
criterion	I
associates	O
zero	O
error	B
with	O
any	O
pattern	O
that	O
is	O
correctly	O
classified	O
whereas	O
for	O
a	O
misclassified	O
pattern	O
xn	O
it	O
tries	O
to	O
minimize	O
the	O
quantity	O
wt	O
the	O
perceptron	B
criterion	I
is	O
therefore	O
given	O
by	O
epw	O
wt	O
ntn	O
n	O
m	O
frank	O
rosenblatt	B
rosenblatt	B
s	O
perceptron	B
played	O
an	O
important	O
role	O
in	O
the	O
history	O
of	O
machine	O
learning	B
initially	O
rosenblatt	B
simulated	O
the	O
perceptron	B
on	O
an	O
ibm	O
computer	O
at	O
cornell	O
in	O
but	O
by	O
the	O
early	O
he	O
had	O
built	O
special-purpose	O
hardware	B
that	O
provided	O
a	O
direct	O
parallel	O
implementation	O
of	O
perceptron	B
learning	B
many	O
of	O
his	O
ideas	O
were	O
encapsulated	O
in	O
principles	O
of	O
neurodynamics	O
perceptrons	O
and	O
the	O
theory	B
of	O
brain	O
mechanisms	O
published	O
in	O
rosenblatt	B
s	O
work	O
was	O
criticized	O
by	O
marvin	O
minksy	O
whose	O
objections	O
were	O
published	O
in	O
the	O
book	O
perceptrons	O
co-authored	O
with	O
seymour	O
papert	O
this	O
book	O
was	O
widely	O
misinterpreted	O
at	O
the	O
time	O
as	O
showing	O
that	O
neural	O
networks	O
were	O
fatally	O
flawed	O
and	O
could	O
only	O
learn	O
solutions	O
for	O
linearly	B
separable	I
problems	O
in	O
fact	O
it	O
only	O
proved	O
such	O
limitations	O
in	O
the	O
case	O
of	O
single-layer	O
networks	O
such	O
as	O
the	O
perceptron	B
and	O
merely	O
conjectured	O
that	O
they	O
applied	O
to	O
more	O
general	O
network	O
models	O
unfortunately	O
however	O
this	O
book	O
contributed	O
to	O
the	O
substantial	O
decline	O
in	O
research	O
funding	O
for	O
neural	O
computing	O
a	O
situation	O
that	O
was	O
not	O
reversed	O
until	O
the	O
today	O
there	O
are	O
many	O
hundreds	O
if	O
not	O
thousands	O
of	O
applications	O
of	O
neural	O
networks	O
in	O
widespread	O
use	O
with	O
examples	O
in	O
areas	O
such	O
as	O
handwriting	B
recognition	I
and	O
information	O
retrieval	O
being	O
used	O
routinely	O
by	O
millions	O
of	O
people	O
linear	O
models	O
for	O
classification	B
section	O
where	O
m	O
denotes	O
the	O
set	O
of	O
all	O
misclassified	O
patterns	O
the	O
contribution	O
to	O
the	O
error	B
associated	O
with	O
a	O
particular	O
misclassified	O
pattern	O
is	O
a	O
linear	O
function	O
of	O
w	O
in	O
regions	O
of	O
w	O
space	O
where	O
the	O
pattern	O
is	O
misclassified	O
and	O
zero	O
in	O
regions	O
where	O
it	O
is	O
correctly	O
classified	O
the	O
total	O
error	B
function	I
is	O
therefore	O
piecewise	O
linear	O
we	O
now	O
apply	O
the	O
stochastic	B
gradient	B
descent	I
algorithm	O
to	O
this	O
error	B
function	I
the	O
change	O
in	O
the	O
weight	B
vector	I
w	O
is	O
then	O
given	O
by	O
w	O
w	O
epw	O
w	O
ntn	O
where	O
is	O
the	O
learning	B
rate	I
parameter	I
and	O
is	O
an	O
integer	O
that	O
indexes	O
the	O
steps	O
of	O
the	O
algorithm	O
because	O
the	O
perceptron	B
function	O
yx	O
w	O
is	O
unchanged	O
if	O
we	O
multiply	O
w	O
by	O
a	O
constant	O
we	O
can	O
set	O
the	O
learning	B
rate	I
parameter	I
equal	O
to	O
without	O
of	O
generality	O
note	O
that	O
as	O
the	O
weight	B
vector	I
evolves	O
during	O
training	B
the	O
set	O
of	O
patterns	O
that	O
are	O
misclassified	O
will	O
change	O
the	O
perceptron	B
learning	B
algorithm	O
has	O
a	O
simple	O
interpretation	O
as	O
follows	O
we	O
cycle	O
through	O
the	O
training	B
patterns	O
in	O
turn	O
and	O
for	O
each	O
pattern	O
xn	O
we	O
evaluate	O
the	O
perceptron	B
function	O
if	O
the	O
pattern	O
is	O
correctly	O
classified	O
then	O
the	O
weight	B
vector	I
remains	O
unchanged	O
whereas	O
if	O
it	O
is	O
incorrectly	O
classified	O
then	O
for	O
class	O
we	O
add	O
the	O
vector	O
onto	O
the	O
current	O
estimate	O
of	O
weight	B
vector	I
w	O
while	O
for	O
class	O
we	O
subtract	O
the	O
vector	O
from	O
w	O
the	O
perceptron	B
learning	B
algorithm	O
is	O
illustrated	O
in	O
figure	O
if	O
we	O
consider	O
the	O
effect	O
of	O
a	O
single	O
update	O
in	O
the	O
perceptron	B
learning	B
algorithm	O
we	O
see	O
that	O
the	O
contribution	O
to	O
the	O
error	B
from	O
a	O
misclassified	O
pattern	O
will	O
be	O
reduced	O
because	O
from	O
we	O
have	O
w	O
ntn	O
w	O
ntn	O
ntnt	O
ntn	O
w	O
ntn	O
where	O
we	O
have	O
set	O
and	O
made	O
use	O
of	O
of	O
course	O
this	O
does	O
not	O
imply	O
that	O
the	O
contribution	O
to	O
the	O
error	B
function	I
from	O
the	O
other	O
misclassified	O
patterns	O
will	O
have	O
been	O
reduced	O
furthermore	O
the	O
change	O
in	O
weight	B
vector	I
may	O
have	O
caused	O
some	O
previously	O
correctly	O
classified	O
patterns	O
to	O
become	O
misclassified	O
thus	O
the	O
perceptron	B
learning	B
rule	O
is	O
not	O
guaranteed	O
to	O
reduce	O
the	O
total	O
error	B
function	I
at	O
each	O
stage	O
however	O
the	O
perceptron	B
convergence	B
theorem	I
states	O
that	O
if	O
there	O
exists	O
an	O
exact	O
solution	O
other	O
words	O
if	O
the	O
training	B
data	O
set	O
is	O
linearly	B
separable	I
then	O
the	O
perceptron	B
learning	B
algorithm	O
is	O
guaranteed	O
to	O
find	O
an	O
exact	O
solution	O
in	O
a	O
finite	O
number	O
of	O
steps	O
proofs	O
of	O
this	O
theorem	O
can	O
be	O
found	O
for	O
example	O
in	O
rosenblatt	B
block	O
nilsson	O
minsky	O
and	O
papert	O
hertz	O
et	O
al	O
and	O
bishop	O
note	O
however	O
that	O
the	O
number	O
of	O
steps	O
required	O
to	O
achieve	O
convergence	O
could	O
still	O
be	O
substantial	O
and	O
in	O
practice	O
until	O
convergence	O
is	O
achieved	O
we	O
will	O
not	O
be	O
able	O
to	O
distinguish	O
between	O
a	O
nonseparable	O
problem	O
and	O
one	O
that	O
is	O
simply	O
slow	O
to	O
converge	O
even	O
when	O
the	O
data	O
set	O
is	O
linearly	B
separable	I
there	O
may	O
be	O
many	O
solutions	O
and	O
which	O
one	O
is	O
found	O
will	O
depend	O
on	O
the	O
initialization	O
of	O
the	O
parameters	O
and	O
on	O
the	O
order	O
of	O
presentation	O
of	O
the	O
data	O
points	O
furthermore	O
for	O
data	O
sets	O
that	O
are	O
not	O
linearly	B
separable	I
the	O
perceptron	B
learning	B
algorithm	O
will	O
never	O
converge	O
discriminant	O
functions	O
figure	O
illustration	O
of	O
the	O
convergence	O
of	O
the	O
perceptron	B
learning	B
algorithm	O
showing	O
data	O
points	O
from	O
two	O
classes	O
and	O
blue	O
in	O
a	O
two-dimensional	O
feature	B
space	I
the	O
top	O
left	O
plot	O
shows	O
the	O
initial	O
parameter	O
vector	O
w	O
shown	O
as	O
a	O
black	O
arrow	O
together	O
with	O
the	O
corresponding	O
decision	B
boundary	I
line	O
in	O
which	O
the	O
arrow	O
points	O
towards	O
the	O
decision	B
region	I
which	O
classified	O
as	O
belonging	O
to	O
the	O
red	O
class	O
the	O
data	O
point	O
circled	O
in	O
green	O
is	O
misclassified	O
and	O
so	O
its	O
feature	O
vector	O
is	O
added	O
to	O
the	O
current	O
weight	B
vector	I
giving	O
the	O
new	O
decision	B
boundary	I
shown	O
in	O
the	O
top	O
right	O
plot	O
the	O
bottom	O
left	O
plot	O
shows	O
the	O
next	O
misclassified	O
point	O
to	O
be	O
considered	O
indicated	O
by	O
the	O
green	O
circle	O
and	O
its	O
feature	O
vector	O
is	O
again	O
added	O
to	O
the	O
weight	B
vector	I
giving	O
the	O
decision	B
boundary	I
shown	O
in	O
the	O
bottom	O
right	O
plot	O
for	O
which	O
all	O
data	O
points	O
are	O
correctly	O
classified	O
linear	O
models	O
for	O
classification	B
figure	O
illustration	O
of	O
the	O
mark	O
perceptron	B
hardware	B
the	O
photograph	O
on	O
the	O
left	O
shows	O
how	O
the	O
inputs	O
were	O
obtained	O
using	O
a	O
simple	O
camera	O
system	O
in	O
which	O
an	O
input	O
scene	O
in	O
this	O
case	O
a	O
printed	O
character	O
was	O
illuminated	O
by	O
powerful	O
lights	O
and	O
an	O
image	O
focussed	O
onto	O
a	O
array	O
of	O
cadmium	O
sulphide	O
photocells	O
giving	O
a	O
primitive	O
pixel	O
image	O
the	O
perceptron	B
also	O
had	O
a	O
patch	O
board	O
shown	O
in	O
the	O
middle	O
photograph	O
which	O
allowed	O
different	O
configurations	O
of	O
input	O
features	O
to	O
be	O
tried	O
often	O
these	O
were	O
wired	O
up	O
at	O
random	O
to	O
demonstrate	O
the	O
ability	O
of	O
the	O
perceptron	B
to	O
learn	O
without	O
the	O
need	O
for	O
precise	O
wiring	O
in	O
contrast	O
to	O
a	O
modern	O
digital	O
computer	O
the	O
photograph	O
on	O
the	O
right	O
shows	O
one	O
of	O
the	O
racks	O
of	O
adaptive	O
weights	O
each	O
weight	O
was	O
implemented	O
using	O
a	O
rotary	O
variable	O
resistor	O
also	O
called	O
a	O
potentiometer	O
driven	O
by	O
an	O
electric	O
motor	O
thereby	O
allowing	O
the	O
value	O
of	O
the	O
weight	O
to	O
be	O
adjusted	O
automatically	O
by	O
the	O
learning	B
algorithm	O
aside	O
from	O
difficulties	O
with	O
the	O
learning	B
algorithm	O
the	O
perceptron	B
does	O
not	O
provide	O
probabilistic	O
outputs	O
nor	O
does	O
it	O
generalize	O
readily	O
to	O
k	O
classes	O
the	O
most	O
important	O
limitation	O
however	O
arises	O
from	O
the	O
fact	O
that	O
common	O
with	O
all	O
of	O
the	O
models	O
discussed	O
in	O
this	O
chapter	O
and	O
the	O
previous	O
one	O
it	O
is	O
based	O
on	O
linear	O
combinations	O
of	O
fixed	O
basis	O
functions	O
more	O
detailed	O
discussions	O
of	O
the	O
limitations	O
of	O
perceptrons	O
can	O
be	O
found	O
in	O
minsky	O
and	O
papert	O
and	O
bishop	O
analogue	O
hardware	B
implementations	O
of	O
the	O
perceptron	B
were	O
built	O
by	O
rosenblatt	B
based	O
on	O
motor-driven	O
variable	O
resistors	O
to	O
implement	O
the	O
adaptive	O
parameters	O
wj	O
these	O
are	O
illustrated	O
in	O
figure	O
the	O
inputs	O
were	O
obtained	O
from	O
a	O
simple	O
camera	O
system	O
based	O
on	O
an	O
array	O
of	O
photo-sensors	O
while	O
the	O
basis	O
functions	O
could	O
be	O
chosen	O
in	O
a	O
variety	O
of	O
ways	O
for	O
example	O
based	O
on	O
simple	O
fixed	O
functions	O
of	O
randomly	O
chosen	O
subsets	O
of	O
pixels	O
from	O
the	O
input	O
image	O
typical	O
applications	O
involved	O
learning	B
to	O
discriminate	O
simple	O
shapes	O
or	O
characters	O
at	O
the	O
same	O
time	O
that	O
the	O
perceptron	B
was	O
being	O
developed	O
a	O
closely	O
related	O
system	O
called	O
the	O
adaline	B
which	O
is	O
short	O
for	O
adaptive	O
linear	O
element	O
was	O
being	O
explored	O
by	O
widrow	O
and	O
co-workers	O
the	O
functional	B
form	O
of	O
the	O
model	O
was	O
the	O
same	O
as	O
for	O
the	O
perceptron	B
but	O
a	O
different	O
approach	O
to	O
training	B
was	O
adopted	O
and	O
hoff	O
widrow	O
and	O
lehr	O
probabilistic	O
generative	O
models	O
we	O
turn	O
next	O
to	O
a	O
probabilistic	O
view	O
of	O
classification	B
and	O
show	O
how	O
models	O
with	O
linear	O
decision	O
boundaries	O
arise	O
from	O
simple	O
assumptions	O
about	O
the	O
distribution	O
of	O
the	O
data	O
in	O
section	O
we	O
discussed	O
the	O
distinction	O
between	O
the	O
discriminative	O
and	O
the	O
generative	O
approaches	O
to	O
classification	B
here	O
we	O
shall	O
adopt	O
a	O
generative	O
probabilistic	O
generative	O
models	O
figure	O
plot	O
of	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
shown	O
in	O
red	O
together	O
with	O
the	O
scaled	O
probit	B
function	I
a	O
for	O
shown	O
in	O
dashed	O
blue	O
where	O
is	O
defined	O
by	O
the	O
scaling	B
factor	I
is	O
chosen	O
so	O
that	O
the	O
derivatives	O
of	O
the	O
two	O
curves	O
are	O
equal	O
for	O
a	O
approach	O
in	O
which	O
we	O
model	O
the	O
class-conditional	O
densities	O
pxck	O
as	O
well	O
as	O
the	O
class	O
priors	O
pck	O
and	O
then	O
use	O
these	O
to	O
compute	O
posterior	O
probabilities	O
pckx	O
through	O
bayes	B
theorem	O
can	O
be	O
written	O
as	O
consider	O
first	O
of	O
all	O
the	O
case	O
of	O
two	O
classes	O
the	O
posterior	B
probability	B
for	O
class	O
exp	O
a	O
a	O
ln	O
and	O
is	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
where	O
we	O
have	O
defined	O
exp	O
a	O
which	O
is	O
plotted	O
in	O
figure	O
the	O
term	O
sigmoid	O
means	O
s-shaped	O
this	O
type	O
of	O
function	O
is	O
sometimes	O
also	O
called	O
a	O
squashing	O
function	O
because	O
it	O
maps	O
the	O
whole	O
real	O
axis	O
into	O
a	O
finite	O
interval	O
the	O
logistic	B
sigmoid	I
has	O
been	O
encountered	O
already	O
in	O
earlier	O
chapters	O
and	O
plays	O
an	O
important	O
role	O
in	O
many	O
classification	B
algorithms	O
it	O
satisfies	O
the	O
following	O
symmetry	O
property	O
as	O
is	O
easily	O
verified	O
the	O
inverse	B
of	O
the	O
logistic	B
sigmoid	I
is	O
given	O
by	O
a	O
a	O
ln	O
and	O
is	O
known	O
as	O
the	O
logit	B
function	I
it	O
represents	O
the	O
log	O
of	O
the	O
ratio	O
of	O
probabilities	O
ln	O
for	O
the	O
two	O
classes	O
also	O
known	O
as	O
the	O
log	B
odds	I
linear	O
models	O
for	O
classification	B
note	O
that	O
in	O
we	O
have	O
simply	O
rewritten	O
the	O
posterior	O
probabilities	O
in	O
an	O
equivalent	O
form	O
and	O
so	O
the	O
appearance	O
of	O
the	O
logistic	B
sigmoid	I
may	O
seem	O
rather	O
vacuous	O
however	O
it	O
will	O
have	O
significance	O
provided	O
ax	O
takes	O
a	O
simple	O
functional	B
form	O
we	O
shall	O
shortly	O
consider	O
situations	O
in	O
which	O
ax	O
is	O
a	O
linear	O
function	O
of	O
x	O
in	O
which	O
case	O
the	O
posterior	B
probability	B
is	O
governed	O
by	O
a	O
generalized	B
linear	I
model	I
for	O
the	O
case	O
of	O
k	O
classes	O
we	O
have	O
pckx	O
pxckpck	O
j	O
pxcjpcj	O
expak	O
j	O
expaj	O
which	O
is	O
known	O
as	O
the	O
normalized	O
exponential	O
and	O
can	O
be	O
regarded	O
as	O
a	O
multiclass	B
generalization	B
of	O
the	O
logistic	B
sigmoid	I
here	O
the	O
quantities	O
ak	O
are	O
defined	O
by	O
ak	O
ln	O
pxckpck	O
the	O
normalized	O
exponential	O
is	O
also	O
known	O
as	O
the	O
softmax	B
function	I
as	O
it	O
represents	O
a	O
smoothed	O
version	O
of	O
the	O
max	O
function	O
because	O
if	O
ak	O
aj	O
for	O
all	O
j	O
k	O
then	O
pckx	O
and	O
pcjx	O
we	O
now	O
investigate	O
the	O
consequences	O
of	O
choosing	O
specific	O
forms	O
for	O
the	O
classconditional	O
densities	O
looking	O
first	O
at	O
continuous	O
input	O
variables	O
x	O
and	O
then	O
discussing	O
briefly	O
the	O
case	O
of	O
discrete	O
inputs	O
continuous	O
inputs	O
let	O
us	O
assume	O
that	O
the	O
class-conditional	O
densities	O
are	O
gaussian	B
and	O
then	O
explore	O
the	O
resulting	O
form	O
for	O
the	O
posterior	O
probabilities	O
to	O
start	O
with	O
we	O
shall	O
assume	O
that	O
all	O
classes	O
share	O
the	O
same	O
covariance	B
matrix	O
thus	O
the	O
density	B
for	O
class	O
ck	O
is	O
given	O
by	O
k	O
pxck	O
exp	O
kt	O
consider	O
first	O
the	O
case	O
of	O
two	O
classes	O
from	O
and	O
we	O
have	O
where	O
we	O
have	O
defined	O
w	O
t	O
ln	O
t	O
we	O
see	O
that	O
the	O
quadratic	O
terms	O
in	O
x	O
from	O
the	O
exponents	O
of	O
the	O
gaussian	B
densities	O
have	O
cancelled	O
to	O
the	O
assumption	O
of	O
common	O
covariance	B
matrices	O
leading	O
to	O
a	O
linear	O
function	O
of	O
x	O
in	O
the	O
argument	O
of	O
the	O
logistic	B
sigmoid	I
this	O
result	O
is	O
illustrated	O
for	O
the	O
case	O
of	O
a	O
two-dimensional	O
input	O
space	O
x	O
in	O
figure	O
the	O
resulting	O
probabilistic	O
generative	O
models	O
figure	O
the	O
left-hand	O
plot	O
shows	O
the	O
class-conditional	O
densities	O
for	O
two	O
classes	O
denoted	O
red	O
and	O
blue	O
on	O
the	O
right	O
is	O
the	O
corresponding	O
posterior	B
probability	B
which	O
is	O
given	O
by	O
a	O
logistic	B
sigmoid	I
of	O
a	O
linear	O
function	O
of	O
x	O
the	O
surface	O
in	O
the	O
right-hand	O
plot	O
is	O
coloured	O
using	O
a	O
proportion	O
of	O
red	O
ink	O
given	O
by	O
and	O
a	O
proportion	O
of	O
blue	O
ink	O
given	O
by	O
decision	O
boundaries	O
correspond	O
to	O
surfaces	O
along	O
which	O
the	O
posterior	O
probabilities	O
pckx	O
are	O
constant	O
and	O
so	O
will	O
be	O
given	O
by	O
linear	O
functions	O
of	O
x	O
and	O
therefore	O
the	O
decision	O
boundaries	O
are	O
linear	O
in	O
input	O
space	O
the	O
prior	B
probabilities	O
pck	O
enter	O
only	O
through	O
the	O
bias	B
parameter	I
so	O
that	O
changes	O
in	O
the	O
priors	O
have	O
the	O
effect	O
of	O
making	O
parallel	O
shifts	O
of	O
the	O
decision	B
boundary	I
and	O
more	O
generally	O
of	O
the	O
parallel	O
contours	O
of	O
constant	O
posterior	B
probability	B
for	O
the	O
general	O
case	O
of	O
k	O
classes	O
we	O
have	O
from	O
and	O
akx	O
wt	O
k	O
x	O
where	O
we	O
have	O
defined	O
wk	O
k	O
t	O
k	O
k	O
ln	O
pck	O
we	O
see	O
that	O
the	O
akx	O
are	O
again	O
linear	O
functions	O
of	O
x	O
as	O
a	O
consequence	O
of	O
the	O
cancellation	O
of	O
the	O
quadratic	O
terms	O
due	O
to	O
the	O
shared	O
covariances	O
the	O
resulting	O
decision	O
boundaries	O
corresponding	O
to	O
the	O
minimum	O
misclassification	O
rate	O
will	O
occur	O
when	O
two	O
of	O
the	O
posterior	O
probabilities	O
two	O
largest	O
are	O
equal	O
and	O
so	O
will	O
be	O
defined	O
by	O
linear	O
functions	O
of	O
x	O
and	O
so	O
again	O
we	O
have	O
a	O
generalized	B
linear	I
model	I
if	O
we	O
relax	O
the	O
assumption	O
of	O
a	O
shared	O
covariance	B
matrix	O
and	O
allow	O
each	O
classconditional	O
density	B
pxck	O
to	O
have	O
its	O
own	O
covariance	B
matrix	O
k	O
then	O
the	O
earlier	O
cancellations	O
will	O
no	O
longer	O
occur	O
and	O
we	O
will	O
obtain	O
quadratic	O
functions	O
of	O
x	O
giving	O
rise	O
to	O
a	O
quadratic	B
discriminant	I
the	O
linear	O
and	O
quadratic	O
decision	O
boundaries	O
are	O
illustrated	O
in	O
figure	O
linear	O
models	O
for	O
classification	B
figure	O
the	O
left-hand	O
plot	O
shows	O
the	O
class-conditional	O
densities	O
for	O
three	O
classes	O
each	O
having	O
a	O
gaussian	B
distribution	O
coloured	O
red	O
green	O
and	O
blue	O
in	O
which	O
the	O
red	O
and	O
green	O
classes	O
have	O
the	O
same	O
covariance	B
matrix	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
posterior	O
probabilities	O
in	O
which	O
the	O
rgb	O
colour	O
vector	O
represents	O
the	O
posterior	O
probabilities	O
for	O
the	O
respective	O
three	O
classes	O
the	O
decision	O
boundaries	O
are	O
also	O
shown	O
notice	O
that	O
the	O
boundary	O
between	O
the	O
red	O
and	O
green	O
classes	O
which	O
have	O
the	O
same	O
covariance	B
matrix	O
is	O
linear	O
whereas	O
those	O
between	O
the	O
other	O
pairs	O
of	O
classes	O
are	O
quadratic	O
maximum	B
likelihood	I
solution	O
once	O
we	O
have	O
specified	O
a	O
parametric	O
functional	B
form	O
for	O
the	O
class-conditional	O
densities	O
pxck	O
we	O
can	O
then	O
determine	O
the	O
values	O
of	O
the	O
parameters	O
together	O
with	O
the	O
prior	B
class	O
probabilities	O
pck	O
using	O
maximum	B
likelihood	I
this	O
requires	O
a	O
data	O
set	O
comprising	O
observations	O
of	O
x	O
along	O
with	O
their	O
corresponding	O
class	O
labels	O
consider	O
first	O
the	O
case	O
of	O
two	O
classes	O
each	O
having	O
a	O
gaussian	B
class-conditional	O
density	B
with	O
a	O
shared	O
covariance	B
matrix	O
and	O
suppose	O
we	O
have	O
a	O
data	O
set	O
tn	O
where	O
n	O
n	O
here	O
tn	O
denotes	O
class	O
and	O
tn	O
denotes	O
class	O
we	O
denote	O
the	O
prior	B
class	O
probability	B
so	O
that	O
for	O
a	O
data	O
point	O
xn	O
from	O
class	O
we	O
have	O
tn	O
and	O
hence	O
n	O
similarly	O
for	O
class	O
we	O
have	O
tn	O
and	O
hence	O
thus	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
pt	O
n	O
tn	O
where	O
t	O
tnt	O
as	O
usual	O
it	O
is	O
convenient	O
to	O
maximize	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
consider	O
first	O
the	O
maximization	O
with	O
respect	O
to	O
the	O
terms	O
in	O
exercise	O
probabilistic	O
generative	O
models	O
the	O
log	O
likelihood	B
function	I
that	O
depend	O
on	O
are	O
ln	O
tn	O
n	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
equal	O
to	O
zero	O
and	O
rearranging	O
we	O
obtain	O
tn	O
n	O
where	O
denotes	O
the	O
total	O
number	O
of	O
data	O
points	O
in	O
class	O
and	O
denotes	O
the	O
total	O
number	O
of	O
data	O
points	O
in	O
class	O
thus	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
is	O
simply	O
the	O
fraction	O
of	O
points	O
in	O
class	O
as	O
expected	O
this	O
result	O
is	O
easily	O
generalized	B
to	O
the	O
multiclass	B
case	O
where	O
again	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
the	O
prior	B
probability	B
associated	O
with	O
class	O
ck	O
is	O
given	O
by	O
the	O
fraction	O
of	O
the	O
training	B
set	I
points	O
assigned	O
to	O
that	O
class	O
now	O
consider	O
the	O
maximization	O
with	O
respect	O
to	O
again	O
we	O
can	O
pick	O
out	O
of	O
the	O
log	O
likelihood	B
function	I
those	O
terms	O
that	O
depend	O
on	O
giving	O
tnxn	O
const	O
tn	O
lnn	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
to	O
zero	O
and	O
rearranging	O
we	O
obtain	O
which	O
is	O
simply	O
the	O
mean	B
of	O
all	O
the	O
input	O
vectors	O
xn	O
assigned	O
to	O
class	O
by	O
a	O
similar	O
argument	O
the	O
corresponding	O
result	O
for	O
is	O
given	O
by	O
tnxn	O
tnxn	O
which	O
again	O
is	O
the	O
mean	B
of	O
all	O
the	O
input	O
vectors	O
xn	O
assigned	O
to	O
class	O
finally	O
consider	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
shared	O
covariance	B
matrix	O
picking	O
out	O
the	O
terms	O
in	O
the	O
log	O
likelihood	B
function	I
that	O
depend	O
on	O
we	O
have	O
tn	O
ln	O
n	O
tn	O
ln	O
ln	O
n	O
tr	O
tnxn	O
tnxn	O
linear	O
models	O
for	O
classification	B
where	O
we	O
have	O
defined	O
s	O
n	O
n	O
n	O
n	O
using	O
the	O
standard	O
result	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
a	O
gaussian	B
distribution	O
we	O
see	O
that	O
s	O
which	O
represents	O
a	O
weighted	O
average	O
of	O
the	O
covariance	B
matrices	O
associated	O
with	O
each	O
of	O
the	O
two	O
classes	O
separately	O
this	O
result	O
is	O
easily	O
extended	B
to	O
the	O
k	O
class	O
problem	O
to	O
obtain	O
the	O
corresponding	O
maximum	B
likelihood	I
solutions	O
for	O
the	O
parameters	O
in	O
which	O
each	O
class-conditional	O
density	B
is	O
gaussian	B
with	O
a	O
shared	O
covariance	B
matrix	O
note	O
that	O
the	O
approach	O
of	O
fitting	O
gaussian	B
distributions	O
to	O
the	O
classes	O
is	O
not	O
robust	O
to	O
outliers	B
because	O
the	O
maximum	B
likelihood	I
estimation	O
of	O
a	O
gaussian	B
is	O
not	O
robust	O
discrete	O
features	O
let	O
us	O
now	O
consider	O
the	O
case	O
of	O
discrete	O
feature	O
values	O
xi	O
for	O
simplicity	O
we	O
begin	O
by	O
looking	O
at	O
binary	O
feature	O
values	O
xi	O
and	O
discuss	O
the	O
extension	O
to	O
more	O
general	O
discrete	O
features	O
shortly	O
if	O
there	O
are	O
d	O
inputs	O
then	O
a	O
general	O
distribution	O
would	O
correspond	O
to	O
a	O
table	O
of	O
numbers	O
for	O
each	O
class	O
containing	O
independent	B
variables	I
to	O
the	O
summation	O
constraint	O
because	O
this	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
features	O
we	O
might	O
seek	O
a	O
more	O
restricted	O
representation	O
here	O
we	O
will	O
make	O
the	O
naive	O
bayes	B
assumption	O
in	O
which	O
the	O
feature	O
values	O
are	O
treated	O
as	O
independent	B
conditioned	O
on	O
the	O
class	O
ck	O
thus	O
we	O
have	O
class-conditional	O
distributions	O
of	O
the	O
form	O
pxck	O
xi	O
xi	O
exercise	O
section	O
section	O
which	O
contain	O
d	O
independent	B
parameters	O
for	O
each	O
class	O
substituting	O
into	O
then	O
gives	O
akx	O
ln	O
ki	O
xi	O
ki	O
ln	O
pck	O
exercise	O
which	O
again	O
are	O
linear	O
functions	O
of	O
the	O
input	O
values	O
xi	O
for	O
the	O
case	O
of	O
k	O
classes	O
we	O
can	O
alternatively	O
consider	O
the	O
logistic	B
sigmoid	I
formulation	O
given	O
by	O
analogous	O
results	O
are	O
obtained	O
for	O
discrete	O
variables	O
each	O
of	O
which	O
can	O
take	O
m	O
states	O
exponential	B
family	I
as	O
we	O
have	O
seen	O
for	O
both	O
gaussian	B
distributed	O
and	O
discrete	O
inputs	O
the	O
posterior	O
class	O
probabilities	O
are	O
given	O
by	O
generalized	B
linear	O
models	O
with	O
logistic	B
sigmoid	I
probabilistic	O
discriminative	O
models	O
classes	O
or	O
softmax	O
classes	O
activation	O
functions	O
these	O
are	O
particular	O
cases	O
of	O
a	O
more	O
general	O
result	O
obtained	O
by	O
assuming	O
that	O
the	O
class-conditional	O
densities	O
pxck	O
are	O
members	O
of	O
the	O
exponential	B
family	I
of	O
distributions	O
using	O
the	O
form	O
for	O
members	O
of	O
the	O
exponential	B
family	I
we	O
see	O
that	O
the	O
distribution	O
of	O
x	O
can	O
be	O
written	O
in	O
the	O
form	O
px	O
k	O
hxg	O
k	O
exp	O
t	O
k	O
ux	O
we	O
now	O
restrict	O
attention	O
to	O
the	O
subclass	O
of	O
such	O
distributions	O
for	O
which	O
ux	O
x	O
then	O
we	O
make	O
use	O
of	O
to	O
introduce	O
a	O
scaling	O
parameter	O
s	O
so	O
that	O
we	O
obtain	O
the	O
restricted	O
set	O
of	O
exponential	B
family	I
class-conditional	O
densities	O
of	O
the	O
form	O
s	O
px	O
k	O
s	O
s	O
h	O
s	O
x	O
g	O
k	O
exp	O
t	O
k	O
x	O
note	O
that	O
we	O
are	O
allowing	O
each	O
class	O
to	O
have	O
its	O
own	O
parameter	O
vector	O
k	O
but	O
we	O
are	O
assuming	O
that	O
the	O
classes	O
share	O
the	O
same	O
scale	B
parameter	I
s	O
for	O
the	O
two-class	O
problem	O
we	O
substitute	O
this	O
expression	O
for	O
the	O
class-conditional	O
densities	O
into	O
and	O
we	O
see	O
that	O
the	O
posterior	O
class	O
probability	B
is	O
again	O
given	O
by	O
a	O
logistic	B
sigmoid	I
acting	O
on	O
a	O
linear	O
function	O
ax	O
which	O
is	O
given	O
by	O
ax	O
ln	O
g	O
ln	O
g	O
ln	O
ln	O
similarly	O
for	O
the	O
k-class	O
problem	O
we	O
substitute	O
the	O
class-conditional	O
density	B
expression	O
into	O
to	O
give	O
akx	O
t	O
k	O
x	O
ln	O
g	O
k	O
ln	O
pck	O
and	O
so	O
again	O
is	O
a	O
linear	O
function	O
of	O
x	O
probabilistic	O
discriminative	O
models	O
for	O
the	O
two-class	O
classification	B
problem	O
we	O
have	O
seen	O
that	O
the	O
posterior	B
probability	B
of	O
class	O
can	O
be	O
written	O
as	O
a	O
logistic	B
sigmoid	I
acting	O
on	O
a	O
linear	O
function	O
of	O
x	O
for	O
a	O
wide	O
choice	O
of	O
class-conditional	O
distributions	O
pxck	O
similarly	O
for	O
the	O
multiclass	B
case	O
the	O
posterior	B
probability	B
of	O
class	O
ck	O
is	O
given	O
by	O
a	O
softmax	O
transformation	O
of	O
a	O
linear	O
function	O
of	O
x	O
for	O
specific	O
choices	O
of	O
the	O
class-conditional	O
densities	O
pxck	O
we	O
have	O
used	O
maximum	B
likelihood	I
to	O
determine	O
the	O
parameters	O
of	O
the	O
densities	O
as	O
well	O
as	O
the	O
class	O
priors	O
pck	O
and	O
then	O
used	O
bayes	B
theorem	O
to	O
find	O
the	O
posterior	O
class	O
probabilities	O
however	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
the	O
functional	B
form	O
of	O
the	O
generalized	B
linear	I
model	I
explicitly	O
and	O
to	O
determine	O
its	O
parameters	O
directly	O
by	O
using	O
maximum	B
likelihood	I
we	O
shall	O
see	O
that	O
there	O
is	O
an	O
efficient	O
algorithm	O
finding	O
such	O
solutions	O
known	O
as	O
iterative	B
reweighted	I
least	I
squares	I
or	O
irls	O
the	O
indirect	O
approach	O
to	O
finding	O
the	O
parameters	O
of	O
a	O
generalized	B
linear	I
model	I
by	O
fitting	O
class-conditional	O
densities	O
and	O
class	O
priors	O
separately	O
and	O
then	O
applying	O
linear	O
models	O
for	O
classification	B
figure	O
illustration	O
of	O
the	O
role	O
of	O
nonlinear	O
basis	O
functions	O
in	O
linear	O
classification	B
models	O
the	O
left	O
plot	O
shows	O
the	O
original	O
input	O
space	O
together	O
with	O
data	O
points	O
from	O
two	O
classes	O
labelled	O
red	O
and	O
blue	O
two	O
gaussian	B
basis	O
functions	O
and	O
are	O
defined	O
in	O
this	O
space	O
with	O
centres	O
shown	O
by	O
the	O
green	O
crosses	O
and	O
with	O
contours	O
shown	O
by	O
the	O
green	O
circles	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
feature	B
space	I
together	O
with	O
the	O
linear	O
decision	B
boundary	I
obtained	O
given	O
by	O
a	O
logistic	B
regression	B
model	O
of	O
the	O
form	O
discussed	O
in	O
section	O
this	O
corresponds	O
to	O
a	O
nonlinear	O
decision	B
boundary	I
in	O
the	O
original	O
input	O
space	O
shown	O
by	O
the	O
black	O
curve	O
in	O
the	O
left-hand	O
plot	O
bayes	B
theorem	O
represents	O
an	O
example	O
of	O
generative	O
modelling	O
because	O
we	O
could	O
take	O
such	O
a	O
model	O
and	O
generate	O
synthetic	O
data	O
by	O
drawing	O
values	O
of	O
x	O
from	O
the	O
marginal	B
distribution	O
px	O
in	O
the	O
direct	O
approach	O
we	O
are	O
maximizing	O
a	O
likelihood	B
function	I
defined	O
through	O
the	O
conditional	B
distribution	O
pckx	O
which	O
represents	O
a	O
form	O
of	O
discriminative	O
training	B
one	O
advantage	O
of	O
the	O
discriminative	O
approach	O
is	O
that	O
there	O
will	O
typically	O
be	O
fewer	O
adaptive	O
parameters	O
to	O
be	O
determined	O
as	O
we	O
shall	O
see	O
shortly	O
it	O
may	O
also	O
lead	O
to	O
improved	O
predictive	O
performance	O
particularly	O
when	O
the	O
class-conditional	O
density	B
assumptions	O
give	O
a	O
poor	O
approximation	O
to	O
the	O
true	O
distributions	O
fixed	O
basis	O
functions	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
considered	O
classification	B
models	O
that	O
work	O
directly	O
with	O
the	O
original	O
input	O
vector	O
x	O
however	O
all	O
of	O
the	O
algorithms	O
are	O
equally	O
applicable	O
if	O
we	O
first	O
make	O
a	O
fixed	O
nonlinear	O
transformation	O
of	O
the	O
inputs	O
using	O
a	O
vector	O
of	O
basis	O
functions	O
the	O
resulting	O
decision	O
boundaries	O
will	O
be	O
linear	O
in	O
the	O
feature	B
space	I
and	O
these	O
correspond	O
to	O
nonlinear	O
decision	O
boundaries	O
in	O
the	O
original	O
x	O
space	O
as	O
illustrated	O
in	O
figure	O
classes	O
that	O
are	O
linearly	B
separable	I
in	O
the	O
feature	B
space	I
need	O
not	O
be	O
linearly	B
separable	I
in	O
the	O
original	O
observation	O
space	O
x	O
note	O
that	O
as	O
in	O
our	O
discussion	O
of	O
linear	O
models	O
for	B
regression	B
one	O
of	O
the	O
probabilistic	O
discriminative	O
models	O
basis	O
functions	O
is	O
typically	O
set	O
to	O
a	O
constant	O
say	O
so	O
that	O
the	O
corresponding	O
parameter	O
plays	O
the	O
role	O
of	O
a	O
bias	B
for	O
the	O
remainder	O
of	O
this	O
chapter	O
we	O
shall	O
include	O
a	O
fixed	O
basis	B
function	I
transformation	O
as	O
this	O
will	O
highlight	O
some	O
useful	O
similarities	O
to	O
the	O
regression	B
models	O
discussed	O
in	O
chapter	O
for	O
many	O
problems	O
of	O
practical	O
interest	O
there	O
is	O
significant	O
overlap	O
between	O
the	O
class-conditional	O
densities	O
pxck	O
this	O
corresponds	O
to	O
posterior	O
probabilities	O
pckx	O
which	O
for	O
at	O
least	O
some	O
values	O
of	O
x	O
are	O
not	O
or	O
in	O
such	O
cases	O
the	O
optimal	O
solution	O
is	O
obtained	O
by	O
modelling	O
the	O
posterior	O
probabilities	O
accurately	O
and	O
then	O
applying	O
standard	O
decision	B
theory	B
as	O
discussed	O
in	O
chapter	O
note	O
that	O
nonlinear	O
transformations	O
cannot	O
remove	O
such	O
class	O
overlap	O
indeed	O
they	O
can	O
increase	O
the	O
level	O
of	O
overlap	O
or	O
create	O
overlap	O
where	O
none	O
existed	O
in	O
the	O
original	O
observation	O
space	O
however	O
suitable	O
choices	O
of	O
nonlinearity	O
can	O
make	O
the	O
process	O
of	O
modelling	O
the	O
posterior	O
probabilities	O
easier	O
such	O
fixed	O
basis	B
function	I
models	O
have	O
important	O
limitations	O
and	O
these	O
will	O
be	O
resolved	O
in	O
later	O
chapters	O
by	O
allowing	O
the	O
basis	O
functions	O
themselves	O
to	O
adapt	O
to	O
the	O
data	O
notwithstanding	O
these	O
limitations	O
models	O
with	O
fixed	O
nonlinear	O
basis	O
functions	O
play	O
an	O
important	O
role	O
in	O
applications	O
and	O
a	O
discussion	O
of	O
such	O
models	O
will	O
introduce	O
many	O
of	O
the	O
key	O
concepts	O
needed	O
for	O
an	O
understanding	O
of	O
their	O
more	O
complex	O
counterparts	O
logistic	B
regression	B
we	O
begin	O
our	O
treatment	O
of	O
generalized	B
linear	O
models	O
by	O
considering	O
the	O
problem	O
of	O
two-class	O
classification	B
in	O
our	O
discussion	O
of	O
generative	O
approaches	O
in	O
section	O
we	O
saw	O
that	O
under	O
rather	O
general	O
assumptions	O
the	O
posterior	B
probability	B
of	O
class	O
can	O
be	O
written	O
as	O
a	O
logistic	B
sigmoid	I
acting	O
on	O
a	O
linear	O
function	O
of	O
the	O
feature	O
vector	O
so	O
that	O
with	O
here	O
is	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
in	O
the	O
terminology	O
of	O
statistics	O
this	O
model	O
is	O
known	O
as	O
logistic	B
regression	B
although	O
it	O
should	O
be	O
emphasized	O
that	O
this	O
is	O
a	O
model	O
for	O
classification	B
rather	O
than	O
regression	B
y	O
wt	O
for	O
an	O
m-dimensional	O
feature	B
space	I
this	O
model	O
has	O
m	O
adjustable	O
parameters	O
by	O
contrast	O
if	O
we	O
had	O
fitted	O
gaussian	B
class	O
conditional	B
densities	O
using	O
maximum	B
likelihood	I
we	O
would	O
have	O
used	O
parameters	O
for	O
the	O
means	O
and	O
mm	O
parameters	O
for	O
the	O
covariance	B
matrix	O
together	O
with	O
the	O
class	O
prior	B
this	O
gives	O
a	O
total	O
of	O
mm	O
parameters	O
which	O
grows	O
quadratically	O
with	O
m	O
in	O
contrast	O
to	O
the	O
linear	O
dependence	O
on	O
m	O
of	O
the	O
number	O
of	O
parameters	O
in	O
logistic	B
regression	B
for	O
large	O
values	O
of	O
m	O
there	O
is	O
a	O
clear	O
advantage	O
in	O
working	O
with	O
the	O
logistic	B
regression	B
model	O
directly	O
we	O
now	O
use	O
maximum	B
likelihood	I
to	O
determine	O
the	O
parameters	O
of	O
the	O
logistic	B
regression	B
model	O
to	O
do	O
this	O
we	O
shall	O
make	O
use	O
of	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
function	O
which	O
can	O
conveniently	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
sigmoid	O
function	O
itself	O
d	O
da	O
section	O
exercise	O
linear	O
models	O
for	O
classification	B
for	O
a	O
data	O
set	O
n	O
tn	O
where	O
tn	O
and	O
n	O
with	O
n	O
n	O
the	O
likelihood	B
function	I
can	O
be	O
written	O
ptw	O
n	O
tn	O
ytn	O
where	O
t	O
tn	O
and	O
yn	O
n	O
as	O
usual	O
we	O
can	O
define	O
an	O
error	B
function	I
by	O
taking	O
the	O
negative	O
logarithm	O
of	O
the	O
likelihood	O
which	O
gives	O
the	O
crossentropy	O
error	B
function	I
in	O
the	O
form	O
ew	O
ln	O
ptw	O
exercise	O
section	O
exercise	O
ln	O
yn	O
tn	O
yn	O
where	O
yn	O
and	O
an	O
wt	O
n	O
taking	O
the	O
gradient	O
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
w	O
we	O
obtain	O
ew	O
tn	O
n	O
where	O
we	O
have	O
made	O
use	O
of	O
we	O
see	O
that	O
the	O
factor	O
involving	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
has	O
cancelled	O
leading	O
to	O
a	O
simplified	O
form	O
for	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
in	O
particular	O
the	O
contribution	O
to	O
the	O
gradient	O
from	O
data	O
point	O
n	O
is	O
given	O
by	O
the	O
error	B
yn	O
tn	O
between	O
the	O
target	O
value	O
and	O
the	O
prediction	O
of	O
the	O
model	O
times	O
the	O
basis	B
function	I
vector	O
n	O
furthermore	O
comparison	O
with	O
shows	O
that	O
this	O
takes	O
precisely	O
the	O
same	O
form	O
as	O
the	O
gradient	O
of	O
the	O
sum-of-squares	B
error	B
function	I
for	O
the	O
linear	B
regression	B
model	O
if	O
desired	O
we	O
could	O
make	O
use	O
of	O
the	O
result	O
to	O
give	O
a	O
sequential	O
algorithm	O
in	O
which	O
patterns	O
are	O
presented	O
one	O
at	O
a	O
time	O
in	O
which	O
each	O
of	O
the	O
weight	O
vectors	O
is	O
updated	O
using	O
in	O
which	O
en	O
is	O
the	O
nth	O
term	O
in	O
it	O
is	O
worth	O
noting	O
that	O
maximum	B
likelihood	I
can	O
exhibit	O
severe	O
over-fitting	B
for	O
data	O
sets	O
that	O
are	O
linearly	B
separable	I
this	O
arises	O
because	O
the	O
maximum	B
likelihood	I
solution	O
occurs	O
when	O
the	O
hyperplane	O
corresponding	O
to	O
equivalent	O
to	O
wt	O
separates	O
the	O
two	O
classes	O
and	O
the	O
magnitude	O
of	O
w	O
goes	O
to	O
infinity	O
in	O
this	O
case	O
the	O
logistic	B
sigmoid	I
function	O
becomes	O
infinitely	O
steep	O
in	O
feature	B
space	I
corresponding	O
to	O
a	O
heaviside	B
step	I
function	I
so	O
that	O
every	O
training	B
point	O
from	O
each	O
class	O
k	O
is	O
assigned	O
a	O
posterior	B
probability	B
pckx	O
furthermore	O
there	O
is	O
typically	O
a	O
continuum	O
of	O
such	O
solutions	O
because	O
any	O
separating	O
hyperplane	O
will	O
give	O
rise	O
to	O
the	O
same	O
posterior	O
probabilities	O
at	O
the	O
training	B
data	O
points	O
as	O
will	O
be	O
seen	O
later	O
in	O
figure	O
maximum	B
likelihood	I
provides	O
no	O
way	O
to	O
favour	O
one	O
such	O
solution	O
over	O
another	O
and	O
which	O
solution	O
is	O
found	O
in	O
practice	O
will	O
depend	O
on	O
the	O
choice	O
of	O
optimization	O
algorithm	O
and	O
on	O
the	O
parameter	O
initialization	O
note	O
that	O
the	O
problem	O
will	O
arise	O
even	O
if	O
the	O
number	O
of	O
data	O
points	O
is	O
large	O
compared	O
with	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	O
so	O
long	O
as	O
the	O
training	B
data	O
set	O
is	O
linearly	B
separable	I
the	O
singularity	O
can	O
be	O
avoided	O
by	O
inclusion	O
of	O
a	O
prior	B
and	O
finding	O
a	O
map	O
solution	O
for	O
w	O
or	O
equivalently	O
by	O
adding	O
a	O
regularization	B
term	O
to	O
the	O
error	B
function	I
probabilistic	O
discriminative	O
models	O
iterative	B
reweighted	I
least	I
squares	I
in	O
the	O
case	O
of	O
the	O
linear	B
regression	B
models	O
discussed	O
in	O
chapter	O
the	O
maximum	B
likelihood	I
solution	O
on	O
the	O
assumption	O
of	O
a	O
gaussian	B
noise	O
model	O
leads	O
to	O
a	O
closed-form	O
solution	O
this	O
was	O
a	O
consequence	O
of	O
the	O
quadratic	O
dependence	O
of	O
the	O
log	O
likelihood	B
function	I
on	O
the	O
parameter	O
vector	O
w	O
for	O
logistic	B
regression	B
there	O
is	O
no	O
longer	O
a	O
closed-form	O
solution	O
due	O
to	O
the	O
nonlinearity	O
of	O
the	O
logistic	B
sigmoid	I
function	O
however	O
the	O
departure	O
from	O
a	O
quadratic	O
form	O
is	O
not	O
substantial	O
to	O
be	O
precise	O
the	O
error	B
function	I
is	O
concave	O
as	O
we	O
shall	O
see	O
shortly	O
and	O
hence	O
has	O
a	O
unique	O
minimum	O
furthermore	O
the	O
error	B
function	I
can	O
be	O
minimized	O
by	O
an	O
efficient	O
iterative	O
technique	O
based	O
on	O
the	O
newton-raphson	B
iterative	O
optimization	O
scheme	O
which	O
uses	O
a	O
local	B
quadratic	O
approximation	O
to	O
the	O
log	O
likelihood	B
function	I
the	O
newton-raphson	B
update	O
for	O
minimizing	O
a	O
function	O
ew	O
takes	O
the	O
form	O
bishop	O
and	O
nabney	O
where	O
h	O
is	O
the	O
hessian	B
matrix	I
whose	O
elements	O
comprise	O
the	O
second	O
derivatives	O
of	O
ew	O
with	O
respect	O
to	O
the	O
components	O
of	O
w	O
let	O
us	O
first	O
of	O
all	O
apply	O
the	O
newton-raphson	B
method	O
to	O
the	O
linear	B
regression	B
model	O
with	O
the	O
sum-of-squares	B
error	B
function	I
the	O
gradient	O
and	O
hessian	O
of	O
this	O
error	B
function	I
are	O
given	O
by	O
wnew	O
wold	O
h	O
ew	O
ew	O
h	O
ew	O
n	O
tn	O
n	O
t	O
w	O
tt	O
n	O
t	O
n	O
t	O
section	O
where	O
is	O
the	O
n	O
m	O
design	B
matrix	I
whose	O
nth	O
row	O
is	O
given	O
by	O
t	O
raphson	O
update	O
then	O
takes	O
the	O
form	O
t	O
wold	O
tt	O
wnew	O
wold	O
t	O
n	O
the	O
newton	O
t	O
tt	O
which	O
we	O
recognize	O
as	O
the	O
standard	O
least-squares	O
solution	O
note	O
that	O
the	O
error	B
function	I
in	O
this	O
case	O
is	O
quadratic	O
and	O
hence	O
the	O
newton-raphson	B
formula	O
gives	O
the	O
exact	O
solution	O
in	O
one	O
step	O
now	O
let	O
us	O
apply	O
the	O
newton-raphson	B
update	O
to	O
the	O
cross-entropy	B
error	B
function	I
for	O
the	O
logistic	B
regression	B
model	O
from	O
we	O
see	O
that	O
the	O
gradient	O
and	O
hessian	O
of	O
this	O
error	B
function	I
are	O
given	O
by	O
ew	O
tn	O
n	O
ty	O
t	O
h	O
ew	O
yn	O
n	O
t	O
n	O
tr	O
linear	O
models	O
for	O
classification	B
rnn	O
yn	O
where	O
we	O
have	O
made	O
use	O
of	O
also	O
we	O
have	O
introduced	O
the	O
n	O
n	O
diagonal	B
matrix	O
r	O
with	O
elements	O
we	O
see	O
that	O
the	O
hessian	O
is	O
no	O
longer	O
constant	O
but	O
depends	O
on	O
w	O
through	O
the	O
weighting	O
matrix	O
r	O
corresponding	O
to	O
the	O
fact	O
that	O
the	O
error	B
function	I
is	O
no	O
longer	O
quadratic	O
using	O
the	O
property	O
yn	O
which	O
follows	O
from	O
the	O
form	O
of	O
the	O
logistic	B
sigmoid	I
function	O
we	O
see	O
that	O
uthu	O
for	O
an	O
arbitrary	O
vector	O
u	O
and	O
so	O
the	O
hessian	B
matrix	I
h	O
is	O
positive	B
definite	I
it	O
follows	O
that	O
the	O
error	B
function	I
is	O
a	O
concave	B
function	I
of	O
w	O
and	O
hence	O
has	O
a	O
unique	O
minimum	O
exercise	O
the	O
newton-raphson	B
update	O
formula	O
for	O
the	O
logistic	B
regression	B
model	O
then	O
be	O
comes	O
wnew	O
wold	O
tr	O
ty	O
t	O
tr	O
wold	O
ty	O
t	O
tr	O
tr	O
trz	O
where	O
z	O
is	O
an	O
n-dimensional	O
vector	O
with	O
elements	O
z	O
wold	O
r	O
t	O
we	O
see	O
that	O
the	O
update	O
formula	O
takes	O
the	O
form	O
of	O
a	O
set	O
of	O
normal	B
equations	I
for	O
a	O
weighted	O
least-squares	O
problem	O
because	O
the	O
weighing	O
matrix	O
r	O
is	O
not	O
constant	O
but	O
depends	O
on	O
the	O
parameter	O
vector	O
w	O
we	O
must	O
apply	O
the	O
normal	B
equations	I
iteratively	O
each	O
time	O
using	O
the	O
new	O
weight	B
vector	I
w	O
to	O
compute	O
a	O
revised	O
weighing	O
matrix	O
r	O
for	O
this	O
reason	O
the	O
algorithm	O
is	O
known	O
as	O
iterative	B
reweighted	I
least	I
squares	I
or	O
irls	O
as	O
in	O
the	O
weighted	O
least-squares	O
problem	O
the	O
elements	O
of	O
the	O
diagonal	B
weighting	O
matrix	O
r	O
can	O
be	O
interpreted	O
as	O
variances	O
because	O
the	O
mean	B
and	O
variance	B
of	O
t	O
in	O
the	O
logistic	B
regression	B
model	O
are	O
given	O
by	O
et	O
y	O
vart	O
y	O
where	O
we	O
have	O
used	O
the	O
property	O
t	O
for	O
t	O
in	O
fact	O
we	O
can	O
interpret	O
irls	O
as	O
the	O
solution	O
to	O
a	O
linearized	O
problem	O
in	O
the	O
space	O
of	O
the	O
variable	O
a	O
wt	O
the	O
quantity	O
zn	O
which	O
corresponds	O
to	O
the	O
nth	O
element	O
of	O
z	O
can	O
then	O
be	O
given	O
a	O
simple	O
interpretation	O
as	O
an	O
effective	O
target	O
value	O
in	O
this	O
space	O
obtained	O
by	O
making	O
a	O
local	B
linear	O
approximation	O
to	O
the	O
logistic	B
sigmoid	I
function	O
around	O
the	O
current	O
operating	O
point	O
wold	O
anw	O
anwold	O
dan	O
dyn	O
nwold	O
tn	O
yn	O
wold	O
yn	O
zn	O
t	O
probabilistic	O
discriminative	O
models	O
section	O
multiclass	B
logistic	B
regression	B
in	O
our	O
discussion	O
of	O
generative	O
models	O
for	O
multiclass	B
classification	B
we	O
have	O
seen	O
that	O
for	O
a	O
large	O
class	O
of	O
distributions	O
the	O
posterior	O
probabilities	O
are	O
given	O
by	O
a	O
softmax	O
transformation	O
of	O
linear	O
functions	O
of	O
the	O
feature	O
variables	O
so	O
that	O
pck	O
yk	O
expak	O
j	O
expaj	O
where	O
the	O
activations	O
ak	O
are	O
given	O
by	O
ak	O
wt	O
k	O
there	O
we	O
used	O
maximum	B
likelihood	I
to	O
determine	O
separately	O
the	O
class-conditional	O
densities	O
and	O
the	O
class	O
priors	O
and	O
then	O
found	O
the	O
corresponding	O
posterior	O
probabilities	O
using	O
bayes	B
theorem	O
thereby	O
implicitly	O
determining	O
the	O
parameters	O
here	O
we	O
consider	O
the	O
use	O
of	O
maximum	B
likelihood	I
to	O
determine	O
the	O
parameters	O
of	O
this	O
model	O
directly	O
to	O
do	O
this	O
we	O
will	O
require	O
the	O
derivatives	O
of	O
yk	O
with	O
respect	O
to	O
all	O
of	O
the	O
activations	O
aj	O
these	O
are	O
given	O
by	O
exercise	O
ykikj	O
yj	O
yk	O
aj	O
where	O
ikj	O
are	O
the	O
elements	O
of	O
the	O
identity	O
matrix	O
next	O
we	O
write	O
down	O
the	O
likelihood	B
function	I
this	O
is	O
most	O
easily	O
done	O
using	O
the	O
coding	O
scheme	O
in	O
which	O
the	O
target	B
vector	I
tn	O
for	O
a	O
feature	O
vector	O
n	O
belonging	O
to	O
class	O
ck	O
is	O
a	O
binary	O
vector	O
with	O
all	O
elements	O
zero	O
except	O
for	O
element	O
k	O
which	O
equals	O
one	O
the	O
likelihood	B
function	I
is	O
then	O
given	O
by	O
wk	O
pck	O
ntnk	O
ytnk	O
nk	O
where	O
ynk	O
yk	O
n	O
and	O
t	O
is	O
an	O
n	O
k	O
matrix	O
of	O
target	O
variables	O
with	O
elements	O
tnk	O
taking	O
the	O
negative	O
logarithm	O
then	O
gives	O
wk	O
ln	O
wk	O
tnk	O
ln	O
ynk	O
which	O
is	O
known	O
as	O
the	O
cross-entropy	B
error	B
function	I
for	O
the	O
multiclass	B
classification	B
problem	O
we	O
now	O
take	O
the	O
gradient	O
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
one	O
of	O
the	O
parameter	O
vectors	O
wj	O
making	O
use	O
of	O
the	O
result	O
for	O
the	O
derivatives	O
of	O
the	O
softmax	B
function	I
we	O
obtain	O
wj	O
wk	O
tnj	O
n	O
exercise	O
linear	O
models	O
for	O
classification	B
k	O
tnk	O
once	O
again	O
we	O
see	O
the	O
same	O
form	O
arising	O
where	O
we	O
have	O
made	O
use	O
of	O
for	O
the	O
gradient	O
as	O
was	O
found	O
for	O
the	O
sum-of-squares	B
error	B
function	I
with	O
the	O
linear	O
model	O
and	O
the	O
cross-entropy	O
error	B
for	O
the	O
logistic	B
regression	B
model	O
namely	O
the	O
product	O
of	O
the	O
error	B
tnj	O
times	O
the	O
basis	B
function	I
n	O
again	O
we	O
could	O
use	O
this	O
to	O
formulate	O
a	O
sequential	O
algorithm	O
in	O
which	O
patterns	O
are	O
presented	O
one	O
at	O
a	O
time	O
in	O
which	O
each	O
of	O
the	O
weight	O
vectors	O
is	O
updated	O
using	O
we	O
have	O
seen	O
that	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	B
function	I
for	O
a	O
linear	B
regression	B
model	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
w	O
for	O
a	O
data	O
point	O
n	O
took	O
the	O
form	O
of	O
the	O
error	B
yn	O
tn	O
times	O
the	O
feature	O
vector	O
n	O
similarly	O
for	O
the	O
combination	O
of	O
logistic	B
sigmoid	I
activation	B
function	I
and	O
cross-entropy	B
error	B
function	I
and	O
for	O
the	O
softmax	O
activation	B
function	I
with	O
the	O
multiclass	B
cross-entropy	B
error	B
function	I
we	O
again	O
obtain	O
this	O
same	O
simple	O
form	O
this	O
is	O
an	O
example	O
of	O
a	O
more	O
general	O
result	O
as	O
we	O
shall	O
see	O
in	O
section	O
to	O
find	O
a	O
batch	O
algorithm	O
we	O
again	O
appeal	O
to	O
the	O
newton-raphson	B
update	O
to	O
obtain	O
the	O
corresponding	O
irls	O
algorithm	O
for	O
the	O
multiclass	B
problem	O
this	O
requires	O
evaluation	O
of	O
the	O
hessian	B
matrix	I
that	O
comprises	O
blocks	O
of	O
size	O
m	O
m	O
in	O
which	O
block	O
j	O
k	O
is	O
given	O
by	O
wj	O
wk	O
wk	O
ynkikj	O
ynj	O
n	O
t	O
n	O
exercise	O
as	O
with	O
the	O
two-class	O
problem	O
the	O
hessian	B
matrix	I
for	O
the	O
multiclass	B
logistic	B
regression	B
model	O
is	O
positive	B
definite	I
and	O
so	O
the	O
error	B
function	I
again	O
has	O
a	O
unique	O
minimum	O
practical	O
details	O
of	O
irls	O
for	O
the	O
multiclass	B
case	O
can	O
be	O
found	O
in	O
bishop	O
and	O
nabney	O
probit	B
regression	B
we	O
have	O
seen	O
that	O
for	O
a	O
broad	O
range	O
of	O
class-conditional	O
distributions	O
described	O
by	O
the	O
exponential	B
family	I
the	O
resulting	O
posterior	O
class	O
probabilities	O
are	O
given	O
by	O
a	O
logistic	O
softmax	O
transformation	O
acting	O
on	O
a	O
linear	O
function	O
of	O
the	O
feature	O
variables	O
however	O
not	O
all	O
choices	O
of	O
class-conditional	O
density	B
give	O
rise	O
to	O
such	O
a	O
simple	O
form	O
for	O
the	O
posterior	O
probabilities	O
instance	O
if	O
the	O
class-conditional	O
densities	O
are	O
modelled	O
using	O
gaussian	B
mixtures	O
this	O
suggests	O
that	O
it	O
might	O
be	O
worth	O
exploring	O
other	O
types	O
of	O
discriminative	O
probabilistic	O
model	O
for	O
the	O
purposes	O
of	O
this	O
chapter	O
however	O
we	O
shall	O
return	O
to	O
the	O
two-class	O
case	O
and	O
again	O
remain	O
within	O
the	O
framework	O
of	O
generalized	B
linear	O
models	O
so	O
that	O
pt	O
fa	O
where	O
a	O
wt	O
and	O
f	O
is	O
the	O
activation	B
function	I
one	O
way	O
to	O
motivate	O
an	O
alternative	O
choice	O
for	O
the	O
link	B
function	I
is	O
to	O
consider	O
a	O
noisy	O
threshold	O
model	O
as	O
follows	O
for	O
each	O
input	O
n	O
we	O
evaluate	O
an	O
wt	O
n	O
and	O
then	O
we	O
set	O
the	O
target	O
value	O
according	O
to	O
tn	O
if	O
an	O
tn	O
otherwise	O
probabilistic	O
discriminative	O
models	O
figure	O
schematic	O
example	O
of	O
a	O
probability	B
density	B
p	O
shown	O
by	O
the	O
blue	O
curve	O
given	O
in	O
this	O
example	O
by	O
a	O
mixture	B
of	O
two	O
gaussians	O
along	O
with	O
its	O
cumulative	B
distribution	I
function	I
f	O
shown	O
by	O
the	O
red	O
curve	O
note	O
that	O
the	O
value	O
of	O
the	O
blue	O
curve	O
at	O
any	O
point	O
such	O
as	O
that	O
indicated	O
by	O
the	O
vertical	O
green	O
line	O
corresponds	O
to	O
the	O
slope	O
of	O
the	O
red	O
curve	O
at	O
the	O
same	O
point	O
conversely	O
the	O
value	O
of	O
the	O
red	O
curve	O
at	O
this	O
point	O
corresponds	O
to	O
the	O
area	O
under	O
the	O
blue	O
curve	O
indicated	O
by	O
the	O
shaded	O
green	O
region	O
in	O
the	O
stochastic	B
threshold	O
model	O
the	O
class	O
label	O
takes	O
the	O
value	O
t	O
if	O
the	O
value	O
of	O
a	O
wt	O
exceeds	O
a	O
threshold	O
otherwise	O
it	O
takes	O
the	O
value	O
t	O
this	O
is	O
equivalent	O
to	O
an	O
activation	B
function	I
given	O
by	O
the	O
cumulative	B
distribution	I
function	I
f	O
if	O
the	O
value	O
of	O
is	O
drawn	O
from	O
a	O
probability	B
density	B
p	O
then	O
the	O
corresponding	O
activation	B
function	I
will	O
be	O
given	O
by	O
the	O
cumulative	B
distribution	I
function	I
a	O
fa	O
p	O
d	O
as	O
illustrated	O
in	O
figure	O
as	O
a	O
specific	O
example	O
suppose	O
that	O
the	O
density	B
p	O
is	O
given	O
by	O
a	O
zero	O
mean	B
unit	O
variance	B
gaussian	B
the	O
corresponding	O
cumulative	B
distribution	I
function	I
is	O
given	O
by	O
a	O
n	O
d	O
which	O
is	O
known	O
as	O
the	O
probit	B
function	I
it	O
has	O
a	O
sigmoidal	O
shape	O
and	O
is	O
compared	O
with	O
the	O
logistic	B
sigmoid	I
function	O
in	O
figure	O
note	O
that	O
the	O
use	O
of	O
a	O
more	O
general	O
gaussian	B
distribution	O
does	O
not	O
change	O
the	O
model	O
because	O
this	O
is	O
equivalent	O
to	O
a	O
re-scaling	O
of	O
the	O
linear	O
coefficients	O
w	O
many	O
numerical	O
packages	O
provide	O
for	O
the	O
evaluation	O
of	O
a	O
closely	O
related	O
function	O
defined	O
by	O
erfa	O
exp	O
d	O
a	O
exercise	O
and	O
known	O
as	O
the	O
erf	B
function	I
or	O
error	B
function	I
to	O
be	O
confused	O
with	O
the	O
error	B
function	I
of	O
a	O
machine	O
learning	B
model	O
it	O
is	O
related	O
to	O
the	O
probit	B
function	I
by	O
erfa	O
the	O
generalized	B
linear	I
model	I
based	O
on	O
a	O
probit	O
activation	B
function	I
is	O
known	O
as	O
probit	B
regression	B
we	O
can	O
determine	O
the	O
parameters	O
of	O
this	O
model	O
using	O
maximum	B
likelihood	I
by	O
a	O
straightforward	O
extension	O
of	O
the	O
ideas	O
discussed	O
earlier	O
in	O
practice	O
the	O
results	O
found	O
using	O
probit	B
regression	B
tend	O
to	O
be	O
similar	O
to	O
those	O
of	O
logistic	B
regression	B
we	O
shall	O
linear	O
models	O
for	O
classification	B
however	O
find	O
another	O
use	O
for	O
the	O
probit	O
model	O
when	O
we	O
discuss	O
bayesian	B
treatments	O
of	O
logistic	B
regression	B
in	O
section	O
one	O
issue	O
that	O
can	O
occur	O
in	O
practical	O
applications	O
is	O
that	O
of	O
outliers	B
which	O
can	O
arise	O
for	O
instance	O
through	O
errors	O
in	O
measuring	O
the	O
input	O
vector	O
x	O
or	O
through	O
mislabelling	O
of	O
the	O
target	O
value	O
t	O
because	O
such	O
points	O
can	O
lie	O
a	O
long	O
way	O
to	O
the	O
wrong	O
side	O
of	O
the	O
ideal	O
decision	B
boundary	I
they	O
can	O
seriously	O
distort	O
the	O
classifier	O
note	O
that	O
the	O
logistic	O
and	O
probit	B
regression	B
models	O
behave	O
differently	O
in	O
this	O
respect	O
because	O
the	O
tails	O
of	O
the	O
logistic	B
sigmoid	I
decay	O
asymptotically	O
like	O
exp	O
x	O
for	O
x	O
whereas	O
for	O
the	O
probit	O
activation	B
function	I
they	O
decay	O
like	O
exp	O
and	O
so	O
the	O
probit	O
model	O
can	O
be	O
significantly	O
more	O
sensitive	O
to	O
outliers	B
however	O
both	O
the	O
logistic	O
and	O
the	O
probit	O
models	O
assume	O
the	O
data	O
is	O
correctly	O
labelled	O
the	O
effect	O
of	O
mislabelling	O
is	O
easily	O
incorporated	O
into	O
a	O
probabilistic	O
model	O
by	O
introducing	O
a	O
probability	B
that	O
the	O
target	O
value	O
t	O
has	O
been	O
flipped	O
to	O
the	O
wrong	O
value	O
and	O
winther	O
leading	O
to	O
a	O
target	O
value	O
distribution	O
for	O
data	O
point	O
x	O
of	O
the	O
form	O
ptx	O
where	O
is	O
the	O
activation	B
function	I
with	O
input	O
vector	O
x	O
here	O
may	O
be	O
set	O
in	O
advance	O
or	O
it	O
may	O
be	O
treated	O
as	O
a	O
hyperparameter	B
whose	O
value	O
is	O
inferred	O
from	O
the	O
data	O
canonical	O
link	B
functions	O
for	O
the	O
linear	B
regression	B
model	O
with	O
a	O
gaussian	B
noise	O
distribution	O
the	O
error	B
function	I
corresponding	O
to	O
the	O
negative	O
log	O
likelihood	O
is	O
given	O
by	O
if	O
we	O
take	O
the	O
derivative	B
with	O
respect	O
to	O
the	O
parameter	O
vector	O
w	O
of	O
the	O
contribution	O
to	O
the	O
error	B
function	I
from	O
a	O
data	O
point	O
n	O
this	O
takes	O
the	O
form	O
of	O
the	O
error	B
yn	O
tn	O
times	O
the	O
feature	O
vector	O
n	O
where	O
yn	O
wt	O
n	O
similarly	O
for	O
the	O
combination	O
of	O
the	O
logistic	B
sigmoid	I
activation	B
function	I
and	O
the	O
cross-entropy	B
error	B
function	I
and	O
for	O
the	O
softmax	O
activation	B
function	I
with	O
the	O
multiclass	B
cross-entropy	B
error	B
function	I
we	O
again	O
obtain	O
this	O
same	O
simple	O
form	O
we	O
now	O
show	O
that	O
this	O
is	O
a	O
general	O
result	O
of	O
assuming	O
a	O
conditional	B
distribution	O
for	O
the	O
target	O
variable	O
from	O
the	O
exponential	B
family	I
along	O
with	O
a	O
corresponding	O
choice	O
for	O
the	O
activation	B
function	I
known	O
as	O
the	O
canonical	B
link	B
function	I
we	O
again	O
make	O
use	O
of	O
the	O
restricted	O
form	O
of	O
exponential	B
family	I
distributions	O
note	O
that	O
here	O
we	O
are	O
applying	O
the	O
assumption	O
of	O
exponential	B
family	I
distribution	O
to	O
the	O
target	O
variable	O
t	O
in	O
contrast	O
to	O
section	O
where	O
we	O
applied	O
it	O
to	O
the	O
input	O
vector	O
x	O
we	O
therefore	O
consider	O
conditional	B
distributions	O
of	O
the	O
target	O
variable	O
of	O
the	O
form	O
pt	O
s	O
s	O
h	O
t	O
s	O
t	O
s	O
g	O
exp	O
using	O
the	O
same	O
line	O
of	O
argument	O
as	O
led	O
to	O
the	O
derivation	O
of	O
the	O
result	O
we	O
see	O
that	O
the	O
conditional	B
mean	B
of	O
t	O
which	O
we	O
denote	O
by	O
y	O
is	O
given	O
by	O
y	O
et	O
s	O
d	O
d	O
ln	O
g	O
the	O
laplace	B
approximation	I
thus	O
y	O
and	O
must	O
related	O
and	O
we	O
denote	O
this	O
relation	O
through	O
following	O
nelder	O
and	O
wedderburn	O
we	O
define	O
a	O
generalized	B
linear	I
model	I
to	O
be	O
one	O
for	O
which	O
y	O
is	O
a	O
nonlinear	O
function	O
of	O
a	O
linear	O
combination	O
of	O
the	O
input	O
feature	O
variables	O
so	O
that	O
y	O
fwt	O
where	O
f	O
is	O
known	O
as	O
the	O
activation	B
function	I
in	O
the	O
machine	O
learning	B
literature	O
and	O
is	O
known	O
as	O
the	O
link	B
function	I
in	O
statistics	O
f	O
now	O
consider	O
the	O
log	O
likelihood	B
function	I
for	O
this	O
model	O
which	O
as	O
a	O
function	O
of	O
is	O
given	O
by	O
ln	O
pt	O
s	O
ln	O
ptn	O
s	O
ln	O
g	O
n	O
ntn	O
s	O
const	O
where	O
we	O
are	O
assuming	O
that	O
all	O
observations	O
share	O
a	O
common	O
scale	B
parameter	I
corresponds	O
to	O
the	O
noise	O
variance	B
for	O
a	O
gaussian	B
distribution	O
for	O
instance	O
and	O
so	O
s	O
is	O
independent	B
of	O
n	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
model	O
parameters	O
w	O
is	O
then	O
given	O
by	O
w	O
ln	O
pt	O
s	O
d	O
d	O
n	O
ln	O
g	O
n	O
tn	O
s	O
d	O
n	O
dyn	O
dyn	O
dan	O
an	O
yn	O
n	O
s	O
where	O
an	O
wt	O
n	O
and	O
we	O
have	O
used	O
yn	O
fan	O
together	O
with	O
the	O
result	O
for	O
et	O
we	O
now	O
see	O
that	O
there	O
is	O
a	O
considerable	O
simplification	O
if	O
we	O
choose	O
a	O
particular	O
form	O
for	O
the	O
link	B
function	I
f	O
given	O
by	O
which	O
gives	O
f	O
y	O
and	O
hence	O
f	O
we	O
have	O
a	O
and	O
hence	O
f	O
function	O
reduces	O
to	O
f	O
in	O
this	O
case	O
the	O
gradient	O
of	O
the	O
error	B
also	O
because	O
a	O
f	O
ln	O
ew	O
s	O
tn	O
n	O
for	O
the	O
gaussian	B
s	O
whereas	O
for	O
the	O
logistic	O
model	O
s	O
the	O
laplace	B
approximation	I
in	O
section	O
we	O
shall	O
discuss	O
the	O
bayesian	B
treatment	O
of	O
logistic	B
regression	B
as	O
we	O
shall	O
see	O
this	O
is	O
more	O
complex	O
than	O
the	O
bayesian	B
treatment	O
of	O
linear	B
regression	B
models	O
discussed	O
in	O
sections	O
and	O
in	O
particular	O
we	O
cannot	O
integrate	O
exactly	O
linear	O
models	O
for	O
classification	B
chapter	O
chapter	O
over	O
the	O
parameter	O
vector	O
w	O
since	O
the	O
posterior	O
distribution	O
is	O
no	O
longer	O
gaussian	B
it	O
is	O
therefore	O
necessary	O
to	O
introduce	O
some	O
form	O
of	O
approximation	O
later	O
in	O
the	O
book	O
we	O
shall	O
consider	O
a	O
range	O
of	O
techniques	O
based	O
on	O
analytical	O
approximations	O
and	O
numerical	O
sampling	O
here	O
we	O
introduce	O
a	O
simple	O
but	O
widely	O
used	O
framework	O
called	O
the	O
laplace	B
approximation	I
that	O
aims	O
to	O
find	O
a	O
gaussian	B
approximation	O
to	O
a	O
probability	B
density	B
defined	O
over	O
a	O
set	O
of	O
continuous	O
variables	O
consider	O
first	O
the	O
case	O
of	O
a	O
single	O
continuous	O
variable	O
z	O
and	O
suppose	O
the	O
distribution	O
pz	O
is	O
defined	O
by	O
pz	O
z	O
fz	O
fz	O
dz	O
is	O
the	O
normalization	O
coefficient	O
we	O
shall	O
suppose	O
that	O
the	O
where	O
z	O
value	O
of	O
z	O
is	O
unknown	O
in	O
the	O
laplace	B
method	O
the	O
goal	O
is	O
to	O
find	O
a	O
gaussian	B
approximation	O
qz	O
which	O
is	O
centred	O
on	O
a	O
mode	O
of	O
the	O
distribution	O
pz	O
the	O
first	O
step	O
is	O
to	O
or	O
equivalently	O
find	O
a	O
mode	O
of	O
pz	O
in	O
other	O
words	O
a	O
point	O
such	O
that	O
p	O
a	O
gaussian	B
distribution	O
has	O
the	O
property	O
that	O
its	O
logarithm	O
is	O
a	O
quadratic	O
function	O
of	O
the	O
variables	O
we	O
therefore	O
consider	O
a	O
taylor	O
expansion	O
of	O
ln	O
fz	O
centred	O
on	O
the	O
mode	O
so	O
that	O
dfz	O
dz	O
where	O
ln	O
fz	O
ln	O
az	O
a	O
ln	O
fz	O
note	O
that	O
the	O
first-order	O
term	O
in	O
the	O
taylor	O
expansion	O
does	O
not	O
appear	O
since	O
is	O
a	O
local	B
maximum	O
of	O
the	O
distribution	O
taking	O
the	O
exponential	O
we	O
obtain	O
fz	O
exp	O
a	O
we	O
can	O
then	O
obtain	O
a	O
normalized	O
distribution	O
qz	O
by	O
making	O
use	O
of	O
the	O
standard	O
result	O
for	O
the	O
normalization	O
of	O
a	O
gaussian	B
so	O
that	O
qz	O
a	O
exp	O
a	O
the	O
laplace	B
approximation	I
is	O
illustrated	O
in	O
figure	O
note	O
that	O
the	O
gaussian	B
approximation	O
will	O
only	O
be	O
well	O
defined	O
if	O
its	O
precision	O
a	O
in	O
other	O
words	O
the	O
stationary	B
point	O
must	O
be	O
a	O
local	B
maximum	O
so	O
that	O
the	O
second	O
derivative	B
of	O
fz	O
at	O
the	O
point	O
is	O
negative	O
the	O
laplace	B
approximation	I
figure	O
illustration	O
of	O
the	O
laplace	B
approximation	I
applied	O
to	O
the	O
distribution	O
pz	O
exp	O
where	O
is	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
e	O
z	O
the	O
left	O
plot	O
shows	O
the	O
normalized	O
distribution	O
pz	O
in	O
yellow	O
together	O
with	O
the	O
laplace	B
approximation	I
centred	O
on	O
the	O
mode	O
of	O
pz	O
in	O
red	O
the	O
right	O
plot	O
shows	O
the	O
negative	O
logarithms	O
of	O
the	O
corresponding	O
curves	O
we	O
can	O
extend	O
the	O
laplace	B
method	O
to	O
approximate	O
a	O
distribution	O
pz	O
fzz	O
defined	O
over	O
an	O
m-dimensional	O
space	O
z	O
at	O
a	O
stationary	B
point	O
the	O
gradient	O
fz	O
will	O
vanish	O
expanding	O
around	O
this	O
stationary	B
point	O
we	O
have	O
ln	O
fz	O
ln	O
where	O
the	O
m	O
m	O
hessian	B
matrix	I
a	O
is	O
defined	O
by	O
a	O
ln	O
and	O
is	O
the	O
gradient	O
operator	O
taking	O
the	O
exponential	O
of	O
both	O
sides	O
we	O
obtain	O
fz	O
exp	O
qz	O
exp	O
the	O
distribution	O
qz	O
is	O
proportional	O
to	O
fz	O
and	O
the	O
appropriate	O
normalization	O
coefficient	O
can	O
be	O
found	O
by	O
inspection	O
using	O
the	O
standard	O
result	O
for	O
a	O
normalized	O
multivariate	O
gaussian	B
giving	O
where	O
denotes	O
the	O
determinant	O
of	O
a	O
this	O
gaussian	B
distribution	O
will	O
be	O
well	O
defined	O
provided	O
its	O
precision	B
matrix	I
given	O
by	O
a	O
is	O
positive	B
definite	I
which	O
implies	O
that	O
the	O
stationary	B
point	O
must	O
be	O
a	O
local	B
maximum	O
not	O
a	O
minimum	O
or	O
a	O
saddle	O
point	O
n	O
a	O
in	O
order	O
to	O
apply	O
the	O
laplace	B
approximation	I
we	O
first	O
need	O
to	O
find	O
the	O
mode	O
and	O
then	O
evaluate	O
the	O
hessian	B
matrix	I
at	O
that	O
mode	O
in	O
practice	O
a	O
mode	O
will	O
typically	O
be	O
found	O
by	O
running	O
some	O
form	O
of	O
numerical	O
optimization	O
algorithm	O
linear	O
models	O
for	O
classification	B
and	O
nabney	O
many	O
of	O
the	O
distributions	O
encountered	O
in	O
practice	O
will	O
be	O
multimodal	O
and	O
so	O
there	O
will	O
be	O
different	O
laplace	B
approximations	O
according	O
to	O
which	O
mode	O
is	O
being	O
considered	O
note	O
that	O
the	O
normalization	O
constant	O
z	O
of	O
the	O
true	O
distribution	O
does	O
not	O
need	O
to	O
be	O
known	O
in	O
order	O
to	O
apply	O
the	O
laplace	B
method	O
as	O
a	O
result	O
of	O
the	O
central	B
limit	I
theorem	I
the	O
posterior	O
distribution	O
for	O
a	O
model	O
is	O
expected	O
to	O
become	O
increasingly	O
better	O
approximated	O
by	O
a	O
gaussian	B
as	O
the	O
number	O
of	O
observed	O
data	O
points	O
is	O
increased	O
and	O
so	O
we	O
would	O
expect	O
the	O
laplace	B
approximation	I
to	O
be	O
most	O
useful	O
in	O
situations	O
where	O
the	O
number	O
of	O
data	O
points	O
is	O
relatively	O
large	O
one	O
major	O
weakness	O
of	O
the	O
laplace	B
approximation	I
is	O
that	O
since	O
it	O
is	O
based	O
on	O
a	O
gaussian	B
distribution	O
it	O
is	O
only	O
directly	O
applicable	O
to	O
real	O
variables	O
in	O
other	O
cases	O
it	O
may	O
be	O
possible	O
to	O
apply	O
the	O
laplace	B
approximation	I
to	O
a	O
transformation	O
of	O
the	O
variable	O
for	O
instance	O
if	O
then	O
we	O
can	O
consider	O
a	O
laplace	B
approximation	I
of	O
ln	O
the	O
most	O
serious	O
limitation	O
of	O
the	O
laplace	B
framework	O
however	O
is	O
that	O
it	O
is	O
based	O
purely	O
on	O
the	O
aspects	O
of	O
the	O
true	O
distribution	O
at	O
a	O
specific	O
value	O
of	O
the	O
variable	O
and	O
so	O
can	O
fail	O
to	O
capture	O
important	O
global	O
properties	O
in	O
chapter	O
we	O
shall	O
consider	O
alternative	O
approaches	O
which	O
adopt	O
a	O
more	O
global	O
perspective	O
model	B
comparison	I
and	O
bic	O
as	O
well	O
as	O
approximating	O
the	O
distribution	O
pz	O
we	O
can	O
also	O
obtain	O
an	O
approxi	O
mation	O
to	O
the	O
normalization	O
constant	O
z	O
using	O
the	O
approximation	O
we	O
have	O
z	O
fz	O
dz	O
exp	O
dz	O
where	O
we	O
have	O
noted	O
that	O
the	O
integrand	O
is	O
gaussian	B
and	O
made	O
use	O
of	O
the	O
standard	O
result	O
for	O
a	O
normalized	O
gaussian	B
distribution	O
we	O
can	O
use	O
the	O
result	O
to	O
obtain	O
an	O
approximation	O
to	O
the	O
model	B
evidence	I
which	O
as	O
discussed	O
in	O
section	O
plays	O
a	O
central	O
role	O
in	O
bayesian	B
model	B
comparison	I
consider	O
a	O
data	O
set	O
d	O
and	O
a	O
set	O
of	O
models	O
having	O
parameters	O
i	O
for	O
each	O
model	O
we	O
define	O
a	O
likelihood	B
function	I
pd	O
imi	O
if	O
we	O
introduce	O
a	O
prior	B
p	O
imi	O
over	O
the	O
parameters	O
then	O
we	O
are	O
interested	O
in	O
computing	O
the	O
model	B
evidence	I
pdmi	O
for	O
the	O
various	O
models	O
from	O
now	O
on	O
we	O
omit	O
the	O
conditioning	O
on	O
mi	O
to	O
keep	O
the	O
notation	O
uncluttered	O
from	O
bayes	B
theorem	O
the	O
model	B
evidence	I
is	O
given	O
by	O
pd	O
pd	O
d	O
identifying	O
f	O
pd	O
and	O
z	O
pd	O
and	O
applying	O
the	O
result	O
we	O
obtain	O
ln	O
pd	O
ln	O
pd	O
map	O
ln	O
p	O
map	O
m	O
lna	O
occam	B
factor	I
exercise	O
exercise	O
section	O
bayesian	B
logistic	B
regression	B
where	O
map	O
is	O
the	O
value	O
of	O
at	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
and	O
a	O
is	O
the	O
hessian	B
matrix	I
of	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
posterior	O
a	O
ln	O
pd	O
mapp	O
map	O
ln	O
p	O
mapd	O
the	O
first	O
term	O
on	O
the	O
right	O
hand	O
side	O
of	O
represents	O
the	O
log	O
likelihood	O
evaluated	O
using	O
the	O
optimized	O
parameters	O
while	O
the	O
remaining	O
three	O
terms	O
comprise	O
the	O
occam	B
factor	I
which	O
penalizes	O
model	O
complexity	O
if	O
we	O
assume	O
that	O
the	O
gaussian	B
prior	B
distribution	O
over	O
parameters	O
is	O
broad	O
and	O
that	O
the	O
hessian	O
has	O
full	O
rank	O
then	O
we	O
can	O
approximate	O
very	O
roughly	O
using	O
ln	O
pd	O
ln	O
pd	O
map	O
m	O
ln	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
data	O
points	O
m	O
is	O
the	O
number	O
of	O
parameters	O
in	O
and	O
we	O
have	O
omitted	O
additive	O
constants	O
this	O
is	O
known	O
as	O
the	O
bayesian	B
information	B
criterion	I
or	O
the	O
schwarz	O
criterion	O
note	O
that	O
compared	O
to	O
aic	O
given	O
by	O
this	O
penalizes	O
model	O
complexity	O
more	O
heavily	O
complexity	O
measures	O
such	O
as	O
aic	O
and	O
bic	O
have	O
the	O
virtue	O
of	O
being	O
easy	O
to	O
evaluate	O
but	O
can	O
also	O
give	O
misleading	O
results	O
in	O
particular	O
the	O
assumption	O
that	O
the	O
hessian	B
matrix	I
has	O
full	O
rank	O
is	O
often	O
not	O
valid	O
since	O
many	O
of	O
the	O
parameters	O
are	O
not	O
well-determined	O
we	O
can	O
use	O
the	O
result	O
to	O
obtain	O
a	O
more	O
accurate	O
estimate	O
of	O
the	O
model	B
evidence	I
starting	O
from	O
the	O
laplace	B
approximation	I
as	O
we	O
illustrate	O
in	O
the	O
context	O
of	O
neural	O
networks	O
in	O
section	O
bayesian	B
logistic	B
regression	B
we	O
now	O
turn	O
to	O
a	O
bayesian	B
treatment	O
of	O
logistic	B
regression	B
exact	O
bayesian	B
inference	B
for	O
logistic	B
regression	B
is	O
intractable	O
in	O
particular	O
evaluation	O
of	O
the	O
posterior	O
distribution	O
would	O
require	O
normalization	O
of	O
the	O
product	O
of	O
a	O
prior	B
distribution	O
and	O
a	O
likelihood	B
function	I
that	O
itself	O
comprises	O
a	O
product	O
of	O
logistic	B
sigmoid	I
functions	O
one	O
for	O
every	O
data	O
point	O
evaluation	O
of	O
the	O
predictive	B
distribution	I
is	O
similarly	O
intractable	O
here	O
we	O
consider	O
the	O
application	O
of	O
the	O
laplace	B
approximation	I
to	O
the	O
problem	O
of	O
bayesian	B
logistic	B
regression	B
and	O
lauritzen	O
mackay	O
laplace	B
approximation	I
recall	O
from	O
section	O
that	O
the	O
laplace	B
approximation	I
is	O
obtained	O
by	O
finding	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
and	O
then	O
fitting	O
a	O
gaussian	B
centred	O
at	O
that	O
mode	O
this	O
requires	O
evaluation	O
of	O
the	O
second	O
derivatives	O
of	O
the	O
log	O
posterior	O
which	O
is	O
equivalent	O
to	O
finding	O
the	O
hessian	B
matrix	I
because	O
we	O
seek	O
a	O
gaussian	B
representation	O
for	O
the	O
posterior	O
distribution	O
it	O
is	O
natural	O
to	O
begin	O
with	O
a	O
gaussian	B
prior	B
which	O
we	O
write	O
in	O
the	O
general	O
form	O
pw	O
n	O
linear	O
models	O
for	O
classification	B
where	O
and	O
are	O
fixed	O
hyperparameters	O
the	O
posterior	O
distribution	O
over	O
w	O
is	O
given	O
by	O
where	O
t	O
tnt	O
taking	O
the	O
log	O
of	O
both	O
sides	O
and	O
substituting	O
for	O
the	O
prior	B
distribution	O
using	O
and	O
for	O
the	O
likelihood	B
function	I
using	O
we	O
obtain	O
pwt	O
pwptw	O
ln	O
pwt	O
ln	O
yn	O
tn	O
yn	O
const	O
where	O
yn	O
n	O
to	O
obtain	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
we	O
first	O
maximize	O
the	O
posterior	O
distribution	O
to	O
give	O
the	O
map	O
posterior	O
solution	O
wmap	O
which	O
defines	O
the	O
mean	B
of	O
the	O
gaussian	B
the	O
covariance	B
is	O
then	O
given	O
by	O
the	O
inverse	B
of	O
the	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
likelihood	O
which	O
takes	O
the	O
form	O
sn	O
ln	O
pwt	O
s	O
yn	O
n	O
t	O
n	O
the	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
therefore	O
takes	O
the	O
form	O
qw	O
n	O
sn	O
having	O
obtained	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
there	O
remains	O
the	O
task	O
of	O
marginalizing	O
with	O
respect	O
to	O
this	O
distribution	O
in	O
order	O
to	O
make	O
predictions	O
predictive	B
distribution	I
the	O
predictive	B
distribution	I
for	O
class	O
given	O
a	O
new	O
feature	O
vector	O
is	O
obtained	O
by	O
marginalizing	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
pwt	O
which	O
is	O
itself	O
approximated	O
by	O
a	O
gaussian	B
distribution	O
qw	O
so	O
that	O
t	O
wpwt	O
dw	O
with	O
the	O
corresponding	O
probability	B
for	O
class	O
given	O
by	O
t	O
t	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
we	O
first	O
note	O
that	O
the	O
function	O
depends	O
on	O
w	O
only	O
through	O
its	O
projection	O
onto	O
denoting	O
a	O
wt	O
we	O
have	O
dw	O
wt	O
da	O
where	O
is	O
the	O
dirac	O
delta	O
function	O
from	O
this	O
we	O
obtain	O
dw	O
da	O
where	O
pa	O
bayesian	B
logistic	B
regression	B
wt	O
dw	O
we	O
can	O
evaluate	O
pa	O
by	O
noting	O
that	O
the	O
delta	O
function	O
imposes	O
a	O
linear	O
constraint	O
on	O
w	O
and	O
so	O
forms	O
a	O
marginal	B
distribution	O
from	O
the	O
joint	O
distribution	O
qw	O
by	O
integrating	O
out	O
all	O
directions	O
orthogonal	O
to	O
because	O
qw	O
is	O
gaussian	B
we	O
know	O
from	O
section	O
that	O
the	O
marginal	B
distribution	O
will	O
also	O
be	O
gaussian	B
we	O
can	O
evaluate	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
distribution	O
by	O
taking	O
moments	O
and	O
interchanging	O
the	O
order	O
of	O
integration	O
over	O
a	O
and	O
w	O
so	O
that	O
a	O
ea	O
paa	O
da	O
qwwt	O
dw	O
wt	O
map	O
where	O
we	O
have	O
used	O
the	O
result	O
for	O
the	O
variational	B
posterior	O
distribution	O
qw	O
similarly	O
a	O
vara	O
da	O
pa	O
qw	O
n	O
dw	O
tsn	O
note	O
that	O
the	O
distribution	O
of	O
a	O
takes	O
the	O
same	O
form	O
as	O
the	O
predictive	B
distribution	I
for	O
the	O
linear	B
regression	B
model	O
with	O
the	O
noise	O
variance	B
set	O
to	O
zero	O
thus	O
our	O
variational	B
approximation	O
to	O
the	O
predictive	B
distribution	I
becomes	O
a	O
da	O
a	O
da	O
this	O
result	O
can	O
also	O
be	O
derived	O
directly	O
by	O
making	O
use	O
of	O
the	O
results	O
for	O
the	O
marginal	B
of	O
a	O
gaussian	B
distribution	O
given	O
in	O
section	O
the	O
integral	O
over	O
a	O
represents	O
the	O
convolution	O
of	O
a	O
gaussian	B
with	O
a	O
logistic	B
sigmoid	I
and	O
cannot	O
be	O
evaluated	O
analytically	O
we	O
can	O
however	O
obtain	O
a	O
good	O
approximation	O
and	O
lauritzen	O
mackay	O
barber	O
and	O
bishop	O
by	O
making	O
use	O
of	O
the	O
close	O
similarity	O
between	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
and	O
the	O
probit	B
function	I
defined	O
by	O
in	O
order	O
to	O
obtain	O
the	O
best	O
approximation	O
to	O
the	O
logistic	O
function	O
we	O
need	O
to	O
re-scale	O
the	O
horizontal	O
axis	O
so	O
that	O
we	O
approximate	O
by	O
a	O
we	O
can	O
find	O
a	O
suitable	O
value	O
of	O
by	O
requiring	O
that	O
the	O
two	O
functions	O
have	O
the	O
same	O
slope	O
at	O
the	O
origin	O
which	O
gives	O
the	O
similarity	O
of	O
the	O
logistic	B
sigmoid	I
and	O
the	O
probit	B
function	I
for	O
this	O
choice	O
of	O
is	O
illustrated	O
in	O
figure	O
the	O
advantage	O
of	O
using	O
a	O
probit	B
function	I
is	O
that	O
its	O
convolution	O
with	O
a	O
gaussian	B
can	O
be	O
expressed	O
analytically	O
in	O
terms	O
of	O
another	O
probit	B
function	I
specifically	O
we	O
can	O
show	O
that	O
an	O
da	O
exercise	O
exercise	O
exercise	O
linear	O
models	O
for	O
classification	B
we	O
now	O
apply	O
the	O
approximation	O
a	O
to	O
the	O
probit	O
functions	O
appearing	O
on	O
both	O
sides	O
of	O
this	O
equation	O
leading	O
to	O
the	O
following	O
approximation	O
for	O
the	O
convolution	O
of	O
a	O
logistic	B
sigmoid	I
with	O
a	O
gaussian	B
da	O
exercises	O
where	O
we	O
have	O
defined	O
applying	O
this	O
result	O
to	O
we	O
obtain	O
the	O
approximate	O
predictive	B
distribution	I
in	O
the	O
form	O
t	O
a	O
a	O
where	O
a	O
and	O
fined	O
by	O
a	O
are	O
defined	O
by	O
and	O
respectively	O
and	O
a	O
is	O
denote	O
that	O
the	O
decision	B
boundary	I
corresponding	O
to	O
t	O
is	O
given	O
by	O
a	O
which	O
is	O
the	O
same	O
as	O
the	O
decision	B
boundary	I
obtained	O
by	O
using	O
the	O
map	O
value	O
for	O
w	O
thus	O
if	O
the	O
decision	O
criterion	O
is	O
based	O
on	O
minimizing	O
misclassification	O
rate	O
with	O
equal	O
prior	B
probabilities	O
then	O
the	O
marginalization	O
over	O
w	O
has	O
no	O
effect	O
however	O
for	O
more	O
complex	O
decision	O
criteria	O
it	O
will	O
play	O
an	O
important	O
role	O
marginalization	O
of	O
the	O
logistic	B
sigmoid	I
model	O
under	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
will	O
be	O
illustrated	O
in	O
the	O
context	O
of	O
variational	B
inference	B
in	O
figure	O
given	O
a	O
set	O
of	O
data	O
points	O
we	O
can	O
define	O
the	O
convex	O
hull	O
to	O
be	O
the	O
set	O
of	O
all	O
points	O
x	O
given	O
by	O
x	O
n	O
n	O
consider	O
a	O
second	O
set	O
of	O
points	O
together	O
with	O
separable	O
if	O
there	O
exists	O
a	O
and	O
a	O
scalar	O
such	O
for	O
all	O
where	O
n	O
and	O
xn	O
for	O
all	O
yn	O
show	O
that	O
if	O
their	O
convex	O
hulls	O
intersect	O
the	O
two	O
their	O
corresponding	O
convex	O
hull	O
by	O
definition	O
the	O
two	O
sets	O
of	O
points	O
will	O
be	O
linearly	O
nxn	O
n	O
sets	O
of	O
points	O
cannot	O
be	O
linearly	B
separable	I
and	O
conversely	O
that	O
if	O
they	O
are	O
linearly	B
separable	I
their	O
convex	O
hulls	O
do	O
not	O
intersect	O
www	O
consider	O
the	O
minimization	O
of	O
a	O
sum-of-squares	B
error	B
function	I
and	O
suppose	O
that	O
all	O
of	O
the	O
target	O
vectors	O
in	O
the	O
training	B
set	I
satisfy	O
a	O
linear	O
constraint	O
attn	O
b	O
where	O
tn	O
corresponds	O
to	O
the	O
nth	O
row	O
of	O
the	O
matrix	O
t	O
in	O
show	O
that	O
as	O
a	O
consequence	O
of	O
this	O
constraint	O
the	O
elements	O
of	O
the	O
model	O
prediction	O
yx	O
given	O
by	O
the	O
least-squares	O
solution	O
also	O
satisfy	O
this	O
constraint	O
so	O
that	O
atyx	O
b	O
exercises	O
to	O
do	O
so	O
assume	O
that	O
one	O
of	O
the	O
basis	O
functions	O
so	O
that	O
the	O
corresponding	O
parameter	O
plays	O
the	O
role	O
of	O
a	O
bias	B
extend	O
the	O
result	O
of	O
exercise	O
to	O
show	O
that	O
if	O
multiple	O
linear	O
constraints	O
are	O
satisfied	O
simultaneously	O
by	O
the	O
target	O
vectors	O
then	O
the	O
same	O
constraints	O
will	O
also	O
be	O
satisfied	O
by	O
the	O
least-squares	O
prediction	O
of	O
a	O
linear	O
model	O
www	O
show	O
that	O
maximization	O
of	O
the	O
class	O
separation	O
criterion	O
given	O
by	O
with	O
respect	O
to	O
w	O
using	O
a	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
constraint	O
wtw	O
leads	O
to	O
the	O
result	O
that	O
w	O
by	O
making	O
use	O
of	O
and	O
show	O
that	O
the	O
fisher	B
criterion	O
can	O
be	O
written	O
in	O
the	O
form	O
using	O
the	O
definitions	O
of	O
the	O
between-class	B
and	O
within-class	B
covariance	B
matrices	O
given	O
by	O
and	O
respectively	O
together	O
with	O
and	O
and	O
the	O
choice	O
of	O
target	O
values	O
described	O
in	O
section	O
show	O
that	O
the	O
expression	O
that	O
minimizes	O
the	O
sum-of-squares	B
error	B
function	I
can	O
be	O
written	O
in	O
the	O
form	O
www	O
show	O
that	O
the	O
logistic	B
sigmoid	I
function	O
satisfies	O
the	O
property	O
a	O
and	O
that	O
its	O
inverse	B
is	O
given	O
by	O
y	O
using	O
and	O
derive	O
the	O
result	O
for	O
the	O
posterior	O
class	O
probability	B
in	O
the	O
two-class	O
generative	B
model	I
with	O
gaussian	B
densities	O
and	O
verify	O
the	O
results	O
and	O
for	O
the	O
parameters	O
w	O
and	O
www	O
consider	O
a	O
generative	O
classification	B
model	O
for	O
k	O
classes	O
defined	O
by	O
prior	B
class	O
probabilities	O
pck	O
k	O
and	O
general	O
class-conditional	O
densities	O
p	O
where	O
is	O
the	O
input	O
feature	O
vector	O
suppose	O
we	O
are	O
given	O
a	O
training	B
data	O
set	O
n	O
tn	O
where	O
n	O
n	O
and	O
tn	O
is	O
a	O
binary	O
target	B
vector	I
of	O
length	O
k	O
that	O
uses	O
the	O
coding	O
scheme	O
so	O
that	O
it	O
has	O
components	O
tnj	O
ijk	O
if	O
pattern	O
n	O
is	O
from	O
class	O
ck	O
assuming	O
that	O
the	O
data	O
points	O
are	O
drawn	O
independently	O
from	O
this	O
model	O
show	O
that	O
the	O
maximum-likelihood	O
solution	O
for	O
the	O
prior	B
probabilities	O
is	O
given	O
by	O
where	O
nk	O
is	O
the	O
number	O
of	O
data	O
points	O
assigned	O
to	O
class	O
ck	O
k	O
nk	O
n	O
consider	O
the	O
classification	B
model	O
of	O
exercise	O
and	O
now	O
suppose	O
that	O
the	O
class-conditional	O
densities	O
are	O
given	O
by	O
gaussian	B
distributions	O
with	O
a	O
shared	O
covariance	B
matrix	O
so	O
that	O
p	O
n	O
k	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
mean	B
of	O
the	O
gaussian	B
distribution	O
for	O
class	O
ck	O
is	O
given	O
by	O
tnk	O
n	O
k	O
nk	O
linear	O
models	O
for	O
classification	B
which	O
represents	O
the	O
mean	B
of	O
those	O
feature	O
vectors	O
assigned	O
to	O
class	O
ck	O
similarly	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
shared	O
covariance	B
matrix	O
is	O
given	O
by	O
nk	O
n	O
sk	O
where	O
sk	O
nk	O
tnk	O
n	O
k	O
n	O
kt	O
thus	O
is	O
given	O
by	O
a	O
weighted	O
average	O
of	O
the	O
covariances	O
of	O
the	O
data	O
associated	O
with	O
each	O
class	O
in	O
which	O
the	O
weighting	O
coefficients	O
are	O
given	O
by	O
the	O
prior	B
probabilities	O
of	O
the	O
classes	O
consider	O
a	O
classification	B
problem	O
with	O
k	O
classes	O
for	O
which	O
the	O
feature	O
vector	O
has	O
m	O
components	O
each	O
of	O
which	O
can	O
take	O
l	O
discrete	O
states	O
let	O
the	O
values	O
of	O
the	O
components	O
be	O
represented	O
by	O
a	O
binary	O
coding	O
scheme	O
further	O
suppose	O
that	O
conditioned	O
on	O
the	O
class	O
ck	O
the	O
m	O
components	O
of	O
are	O
independent	B
so	O
that	O
the	O
class-conditional	O
density	B
factorizes	O
with	O
respect	O
to	O
the	O
feature	O
vector	O
components	O
show	O
that	O
the	O
quantities	O
ak	O
given	O
by	O
which	O
appear	O
in	O
the	O
argument	O
to	O
the	O
softmax	B
function	I
describing	O
the	O
posterior	O
class	O
probabilities	O
are	O
linear	O
functions	O
of	O
the	O
components	O
of	O
note	O
that	O
this	O
represents	O
an	O
example	O
of	O
the	O
naive	B
bayes	B
model	I
which	O
is	O
discussed	O
in	O
section	O
www	O
verify	O
the	O
relation	O
for	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
func	O
tion	O
defined	O
by	O
www	O
by	O
making	O
use	O
of	O
the	O
result	O
for	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
show	O
that	O
the	O
derivative	B
of	O
the	O
error	B
function	I
for	O
the	O
logistic	B
regression	B
model	O
is	O
given	O
by	O
show	O
that	O
for	O
a	O
linearly	B
separable	I
data	O
set	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
logistic	B
regression	B
model	O
is	O
obtained	O
by	O
finding	O
a	O
vector	O
w	O
whose	O
decision	B
boundary	I
wt	O
separates	O
the	O
classes	O
and	O
then	O
taking	O
the	O
magnitude	O
of	O
w	O
to	O
infinity	O
show	O
that	O
the	O
hessian	B
matrix	I
h	O
for	O
the	O
logistic	B
regression	B
model	O
given	O
by	O
is	O
positive	B
definite	I
here	O
r	O
is	O
a	O
diagonal	B
matrix	O
with	O
elements	O
yn	O
and	O
yn	O
is	O
the	O
output	O
of	O
the	O
logistic	B
regression	B
model	O
for	O
input	O
vector	O
xn	O
hence	O
show	O
that	O
the	O
error	B
function	I
is	O
a	O
concave	B
function	I
of	O
w	O
and	O
that	O
it	O
has	O
a	O
unique	O
minimum	O
consider	O
a	O
binary	O
classification	B
problem	O
in	O
which	O
each	O
observation	O
xn	O
is	O
known	O
to	O
belong	O
to	O
one	O
of	O
two	O
classes	O
corresponding	O
to	O
t	O
and	O
t	O
and	O
suppose	O
that	O
the	O
procedure	O
for	O
collecting	O
training	B
data	O
is	O
imperfect	O
so	O
that	O
training	B
points	O
are	O
sometimes	O
mislabelled	O
for	O
every	O
data	O
point	O
xn	O
instead	O
of	O
having	O
a	O
value	O
t	O
for	O
the	O
class	O
label	O
we	O
have	O
instead	O
a	O
value	O
n	O
representing	O
the	O
probability	B
that	O
tn	O
given	O
a	O
probabilistic	O
model	O
pt	O
write	O
down	O
the	O
log	O
likelihood	B
function	I
appropriate	O
to	O
such	O
a	O
data	O
set	O
exercises	O
www	O
show	O
that	O
the	O
derivatives	O
of	O
the	O
softmax	O
activation	B
function	I
where	O
the	O
ak	O
are	O
defined	O
by	O
are	O
given	O
by	O
using	O
the	O
result	O
for	O
the	O
derivatives	O
of	O
the	O
softmax	O
activation	B
function	I
show	O
that	O
the	O
gradients	O
of	O
the	O
cross-entropy	O
error	B
are	O
given	O
by	O
www	O
write	O
down	O
expressions	O
for	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
as	O
well	O
as	O
the	O
corresponding	O
hessian	B
matrix	I
for	O
the	O
probit	B
regression	B
model	O
defined	O
in	O
section	O
these	O
are	O
the	O
quantities	O
that	O
would	O
be	O
required	O
to	O
train	O
such	O
a	O
model	O
using	O
irls	O
show	O
that	O
the	O
hessian	B
matrix	I
for	O
the	O
multiclass	B
logistic	B
regression	B
problem	O
defined	O
by	O
is	O
positive	O
semidefinite	O
note	O
that	O
the	O
full	O
hessian	B
matrix	I
for	O
this	O
problem	O
is	O
of	O
size	O
m	O
k	O
m	O
k	O
where	O
m	O
is	O
the	O
number	O
of	O
parameters	O
and	O
k	O
is	O
the	O
number	O
of	O
classes	O
to	O
prove	O
the	O
positive	O
semidefinite	O
property	O
consider	O
the	O
product	O
uthu	O
where	O
u	O
is	O
an	O
arbitrary	O
vector	O
of	O
length	O
m	O
k	O
and	O
then	O
apply	O
jensen	O
s	O
inequality	O
show	O
that	O
the	O
probit	B
function	I
and	O
the	O
erf	B
function	I
are	O
related	O
by	O
using	O
the	O
result	O
derive	O
the	O
expression	O
for	O
the	O
log	O
model	O
evi	O
dence	O
under	O
the	O
laplace	B
approximation	I
www	O
in	O
this	O
exercise	O
we	O
derive	O
the	O
bic	O
result	O
starting	O
from	O
the	O
laplace	B
approximation	I
to	O
the	O
model	B
evidence	I
given	O
by	O
show	O
that	O
if	O
the	O
prior	B
over	O
parameters	O
is	O
gaussian	B
of	O
the	O
form	O
p	O
n	O
the	O
log	O
model	B
evidence	I
under	O
the	O
laplace	B
approximation	I
takes	O
the	O
form	O
ln	O
pd	O
ln	O
pd	O
map	O
lnh	O
const	O
where	O
h	O
is	O
the	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
log	O
likelihood	O
ln	O
pd	O
evaluated	O
at	O
map	O
now	O
assume	O
that	O
the	O
prior	B
is	O
broad	O
so	O
that	O
v	O
is	O
small	O
and	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
above	O
can	O
be	O
neglected	O
furthermore	O
consider	O
the	O
case	O
of	O
independent	B
identically	I
distributed	I
data	O
so	O
that	O
h	O
is	O
the	O
sum	O
of	O
terms	O
one	O
for	O
each	O
data	O
point	O
show	O
that	O
the	O
log	O
model	B
evidence	I
can	O
then	O
be	O
written	O
approximately	O
in	O
the	O
form	O
of	O
the	O
bic	O
expression	O
map	O
m	O
map	O
mtv	O
use	O
the	O
results	O
from	O
section	O
to	O
derive	O
the	O
result	O
for	O
the	O
marginalization	O
of	O
the	O
logistic	B
regression	B
model	O
with	O
respect	O
to	O
a	O
gaussian	B
posterior	O
distribution	O
over	O
the	O
parameters	O
w	O
suppose	O
we	O
wish	O
to	O
approximate	O
the	O
logistic	B
sigmoid	I
defined	O
by	O
by	O
a	O
scaled	O
probit	B
function	I
a	O
where	O
is	O
defined	O
by	O
show	O
that	O
if	O
is	O
chosen	O
so	O
that	O
the	O
derivatives	O
of	O
the	O
two	O
functions	O
are	O
equal	O
at	O
a	O
then	O
linear	O
models	O
for	O
classification	B
in	O
this	O
exercise	O
we	O
prove	O
the	O
relation	O
for	O
the	O
convolution	O
of	O
a	O
probit	B
function	I
with	O
a	O
gaussian	B
distribution	O
to	O
do	O
this	O
show	O
that	O
the	O
derivative	B
of	O
the	O
lefthand	O
side	O
with	O
respect	O
to	O
is	O
equal	O
to	O
the	O
derivative	B
of	O
the	O
right-hand	O
side	O
and	O
then	O
integrate	O
both	O
sides	O
with	O
respect	O
to	O
and	O
then	O
show	O
that	O
the	O
constant	O
of	O
integration	O
vanishes	O
note	O
that	O
before	O
differentiating	O
the	O
left-hand	O
side	O
it	O
is	O
convenient	O
first	O
to	O
introduce	O
a	O
change	O
of	O
variable	O
given	O
by	O
a	O
z	O
so	O
that	O
the	O
integral	O
over	O
a	O
is	O
replaced	O
by	O
an	O
integral	O
over	O
z	O
when	O
we	O
differentiate	O
the	O
left-hand	O
side	O
of	O
the	O
relation	O
we	O
will	O
then	O
obtain	O
a	O
gaussian	B
integral	O
over	O
z	O
that	O
can	O
be	O
evaluated	O
analytically	O
neural	O
networks	O
in	O
chapters	O
and	O
we	O
considered	O
models	O
for	B
regression	B
and	O
classification	B
that	O
comprised	O
linear	O
combinations	O
of	O
fixed	O
basis	O
functions	O
we	O
saw	O
that	O
such	O
models	O
have	O
useful	O
analytical	O
and	O
computational	O
properties	O
but	O
that	O
their	O
practical	O
applicability	O
was	O
limited	O
by	O
the	O
curse	B
of	I
dimensionality	I
in	O
order	O
to	O
apply	O
such	O
models	O
to	O
largescale	O
problems	O
it	O
is	O
necessary	O
to	O
adapt	O
the	O
basis	O
functions	O
to	O
the	O
data	O
support	B
vector	I
machines	O
discussed	O
in	O
chapter	O
address	O
this	O
by	O
first	O
defining	O
basis	O
functions	O
that	O
are	O
centred	O
on	O
the	O
training	B
data	O
points	O
and	O
then	O
selecting	O
a	O
subset	O
of	O
these	O
during	O
training	B
one	O
advantage	O
of	O
svms	O
is	O
that	O
although	O
the	O
training	B
involves	O
nonlinear	O
optimization	O
the	O
objective	O
function	O
is	O
convex	O
and	O
so	O
the	O
solution	O
of	O
the	O
optimization	O
problem	O
is	O
relatively	O
straightforward	O
the	O
number	O
of	O
basis	O
functions	O
in	O
the	O
resulting	O
models	O
is	O
generally	O
much	O
smaller	O
than	O
the	O
number	O
of	O
training	B
points	O
although	O
it	O
is	O
often	O
still	O
relatively	O
large	O
and	O
typically	O
increases	O
with	O
the	O
size	O
of	O
the	O
training	B
set	I
the	O
relevance	B
vector	I
machine	I
discussed	O
in	O
section	O
also	O
chooses	O
a	O
subset	O
from	O
a	O
fixed	O
set	O
of	O
basis	O
functions	O
and	O
typically	O
results	O
in	O
much	O
neural	O
networks	O
sparser	O
models	O
unlike	O
the	O
svm	O
it	O
also	O
produces	O
probabilistic	O
outputs	O
although	O
this	O
is	O
at	O
the	O
expense	O
of	O
a	O
nonconvex	O
optimization	O
during	O
training	B
an	O
alternative	O
approach	O
is	O
to	O
fix	O
the	O
number	O
of	O
basis	O
functions	O
in	O
advance	O
but	O
allow	O
them	O
to	O
be	O
adaptive	O
in	O
other	O
words	O
to	O
use	O
parametric	O
forms	O
for	O
the	O
basis	O
functions	O
in	O
which	O
the	O
parameter	O
values	O
are	O
adapted	O
during	O
training	B
the	O
most	O
successful	O
model	O
of	O
this	O
type	O
in	O
the	O
context	O
of	O
pattern	O
recognition	O
is	O
the	O
feed-forward	O
neural	B
network	I
also	O
known	O
as	O
the	O
multilayer	B
perceptron	B
discussed	O
in	O
this	O
chapter	O
in	O
fact	O
multilayer	B
perceptron	B
is	O
really	O
a	O
misnomer	O
because	O
the	O
model	O
comprises	O
multiple	O
layers	O
of	O
logistic	B
regression	B
models	O
continuous	O
nonlinearities	O
rather	O
than	O
multiple	O
perceptrons	O
discontinuous	O
nonlinearities	O
for	O
many	O
applications	O
the	O
resulting	O
model	O
can	O
be	O
significantly	O
more	O
compact	O
and	O
hence	O
faster	O
to	O
evaluate	O
than	O
a	O
support	B
vector	I
machine	I
having	O
the	O
same	O
generalization	B
performance	O
the	O
price	O
to	O
be	O
paid	O
for	O
this	O
compactness	O
as	O
with	O
the	O
relevance	B
vector	I
machine	I
is	O
that	O
the	O
likelihood	B
function	I
which	O
forms	O
the	O
basis	O
for	O
network	O
training	B
is	O
no	O
longer	O
a	O
convex	B
function	I
of	O
the	O
model	O
parameters	O
in	O
practice	O
however	O
it	O
is	O
often	O
worth	O
investing	O
substantial	O
computational	O
resources	O
during	O
the	O
training	B
phase	O
in	O
order	O
to	O
obtain	O
a	O
compact	O
model	O
that	O
is	O
fast	O
at	O
processing	O
new	O
data	O
the	O
term	O
neural	B
network	I
has	O
its	O
origins	O
in	O
attempts	O
to	O
find	O
mathematical	O
representations	O
of	O
information	O
processing	O
in	O
biological	O
systems	O
and	O
pitts	O
widrow	O
and	O
hoff	O
rosenblatt	B
rumelhart	O
et	O
al	O
indeed	O
it	O
has	O
been	O
used	O
very	O
broadly	O
to	O
cover	O
a	O
wide	O
range	O
of	O
different	O
models	O
many	O
of	O
which	O
have	O
been	O
the	O
subject	O
of	O
exaggerated	O
claims	O
regarding	O
their	O
biological	O
plausibility	O
from	O
the	O
perspective	O
of	O
practical	O
applications	O
of	O
pattern	O
recognition	O
however	O
biological	O
realism	O
would	O
impose	O
entirely	O
unnecessary	O
constraints	O
our	O
focus	O
in	O
this	O
chapter	O
is	O
therefore	O
on	O
neural	O
networks	O
as	O
efficient	O
models	O
for	O
statistical	O
pattern	O
recognition	O
in	O
particular	O
we	O
shall	O
restrict	O
our	O
attention	O
to	O
the	O
specific	O
class	O
of	O
neural	O
networks	O
that	O
have	O
proven	O
to	O
be	O
of	O
greatest	O
practical	O
value	O
namely	O
the	O
multilayer	B
perceptron	B
we	O
begin	O
by	O
considering	O
the	O
functional	B
form	O
of	O
the	O
network	O
model	O
including	O
the	O
specific	O
parameterization	O
of	O
the	O
basis	O
functions	O
and	O
we	O
then	O
discuss	O
the	O
problem	O
of	O
determining	O
the	O
network	O
parameters	O
within	O
a	O
maximum	B
likelihood	I
framework	O
which	O
involves	O
the	O
solution	O
of	O
a	O
nonlinear	O
optimization	O
problem	O
this	O
requires	O
the	O
evaluation	O
of	O
derivatives	O
of	O
the	O
log	O
likelihood	B
function	I
with	O
respect	O
to	O
the	O
network	O
parameters	O
and	O
we	O
shall	O
see	O
how	O
these	O
can	O
be	O
obtained	O
efficiently	O
using	O
the	O
technique	O
of	O
error	B
backpropagation	B
we	O
shall	O
also	O
show	O
how	O
the	O
backpropagation	B
framework	O
can	O
be	O
extended	B
to	O
allow	O
other	O
derivatives	O
to	O
be	O
evaluated	O
such	O
as	O
the	O
jacobian	O
and	O
hessian	O
matrices	O
next	O
we	O
discuss	O
various	O
approaches	O
to	O
regularization	B
of	O
neural	B
network	I
training	B
and	O
the	O
relationships	O
between	O
them	O
we	O
also	O
consider	O
some	O
extensions	O
to	O
the	O
neural	B
network	I
model	O
and	O
in	O
particular	O
we	O
describe	O
a	O
general	O
framework	O
for	O
modelling	O
conditional	B
probability	B
distributions	O
known	O
as	O
mixture	B
density	B
networks	O
finally	O
we	O
discuss	O
the	O
use	O
of	O
bayesian	B
treatments	O
of	O
neural	O
networks	O
additional	O
background	O
on	O
neural	B
network	I
models	O
can	O
be	O
found	O
in	O
bishop	O
feed-forward	O
network	O
functions	O
feed-forward	O
network	O
functions	O
the	O
linear	O
models	O
for	B
regression	B
and	O
classification	B
discussed	O
in	O
chapters	O
and	O
respectively	O
are	O
based	O
on	O
linear	O
combinations	O
of	O
fixed	O
nonlinear	O
basis	O
functions	O
jx	O
and	O
take	O
the	O
form	O
wj	O
jx	O
yx	O
w	O
f	O
where	O
f	O
is	O
a	O
nonlinear	O
activation	B
function	I
in	O
the	O
case	O
of	O
classification	B
and	O
is	O
the	O
identity	O
in	O
the	O
case	O
of	O
regression	B
our	O
goal	O
is	O
to	O
extend	O
this	O
model	O
by	O
making	O
the	O
basis	O
functions	O
jx	O
depend	O
on	O
parameters	O
and	O
then	O
to	O
allow	O
these	O
parameters	O
to	O
be	O
adjusted	O
along	O
with	O
the	O
coefficients	O
during	O
training	B
there	O
are	O
of	O
course	O
many	O
ways	O
to	O
construct	O
parametric	O
nonlinear	O
basis	O
functions	O
neural	O
networks	O
use	O
basis	O
functions	O
that	O
follow	O
the	O
same	O
form	O
as	O
so	O
that	O
each	O
basis	B
function	I
is	O
itself	O
a	O
nonlinear	O
function	O
of	O
a	O
linear	O
combination	O
of	O
the	O
inputs	O
where	O
the	O
coefficients	O
in	O
the	O
linear	O
combination	O
are	O
adaptive	O
parameters	O
this	O
leads	O
to	O
the	O
basic	O
neural	B
network	I
model	O
which	O
can	O
be	O
described	O
a	O
series	O
of	O
functional	B
transformations	O
first	O
we	O
construct	O
m	O
linear	O
combinations	O
of	O
the	O
input	O
variables	O
xd	O
in	O
the	O
form	O
aj	O
ji	O
xi	O
w	O
w	O
where	O
j	O
m	O
and	O
the	O
superscript	O
indicates	O
that	O
the	O
corresponding	O
eters	O
are	O
in	O
the	O
first	O
layer	O
of	O
the	O
network	O
we	O
shall	O
refer	O
to	O
the	O
parameters	O
w	O
ji	O
as	O
as	O
biases	O
following	O
the	O
nomenclature	O
of	O
chapter	O
weights	O
and	O
the	O
parameters	O
w	O
the	O
quantities	O
aj	O
are	O
known	O
as	O
activations	O
each	O
of	O
them	O
is	O
then	O
transformed	O
using	O
a	O
differentiable	O
nonlinear	O
activation	B
function	I
h	O
to	O
give	O
zj	O
haj	O
these	O
quantities	O
correspond	O
to	O
the	O
outputs	O
of	O
the	O
basis	O
functions	O
in	O
that	O
in	O
the	O
context	O
of	O
neural	O
networks	O
are	O
called	O
hidden	O
units	O
the	O
nonlinear	O
functions	O
h	O
are	O
generally	O
chosen	O
to	O
be	O
sigmoidal	O
functions	O
such	O
as	O
the	O
logistic	B
sigmoid	I
or	O
the	O
tanh	O
function	O
following	O
these	O
values	O
are	O
again	O
linearly	O
combined	O
to	O
give	O
output	O
unit	O
activations	O
ak	O
w	O
kj	O
zj	O
w	O
where	O
k	O
k	O
and	O
k	O
is	O
the	O
total	O
number	O
of	O
outputs	O
this	O
transformation	O
responds	O
to	O
the	O
second	O
layer	O
of	O
the	O
network	O
and	O
again	O
the	O
w	O
are	O
bias	B
parameters	O
finally	O
the	O
output	O
unit	O
activations	O
are	O
transformed	O
using	O
an	O
appropriate	O
activation	B
function	I
to	O
give	O
a	O
set	O
of	O
network	O
outputs	O
yk	O
the	O
choice	O
of	O
activation	B
function	I
is	O
determined	O
by	O
the	O
nature	O
of	O
the	O
data	O
and	O
the	O
assumed	O
distribution	O
of	O
target	O
variables	O
exercise	O
neural	O
networks	O
figure	O
network	O
diagram	O
for	O
the	O
twolayer	O
neural	B
network	I
corresponding	O
to	O
the	O
input	O
hidden	O
and	O
output	O
variables	O
are	O
represented	O
by	O
nodes	O
and	O
the	O
weight	O
parameters	O
are	O
represented	O
by	O
links	O
between	O
the	O
nodes	O
in	O
which	O
the	O
bias	B
parameters	O
are	O
denoted	O
by	O
links	O
coming	O
from	O
additional	O
input	O
and	O
hidden	O
variables	O
and	O
arrows	O
denote	O
the	O
direction	O
of	O
information	O
flow	O
through	O
the	O
network	O
during	O
forward	B
propagation	I
xd	O
inputs	O
hidden	O
units	O
zm	O
m	O
d	O
w	O
km	O
w	O
yk	O
outputs	O
w	O
and	O
follows	O
the	O
same	O
considerations	O
as	O
for	O
linear	O
models	O
discussed	O
in	O
chapters	O
and	O
thus	O
for	O
standard	O
regression	B
problems	O
the	O
activation	B
function	I
is	O
the	O
identity	O
so	O
that	O
yk	O
ak	O
similarly	O
for	O
multiple	O
binary	O
classification	B
problems	O
each	O
output	O
unit	O
activation	O
is	O
transformed	O
using	O
a	O
logistic	B
sigmoid	I
function	O
so	O
that	O
where	O
yk	O
exp	O
a	O
finally	O
for	O
multiclass	B
problems	O
a	O
softmax	O
activation	B
function	I
of	O
the	O
form	O
is	O
used	O
the	O
choice	O
of	O
output	O
unit	O
activation	B
function	I
is	O
discussed	O
in	O
detail	O
in	O
section	O
we	O
can	O
combine	O
these	O
various	O
stages	O
to	O
give	O
the	O
overall	O
network	O
function	O
that	O
for	O
sigmoidal	O
output	O
unit	O
activation	O
functions	O
takes	O
the	O
form	O
ykx	O
w	O
w	O
kj	O
h	O
ji	O
xi	O
w	O
w	O
w	O
where	O
the	O
set	O
of	O
all	O
weight	O
and	O
bias	B
parameters	O
have	O
been	O
grouped	O
together	O
into	O
a	O
vector	O
w	O
thus	O
the	O
neural	B
network	I
model	O
is	O
simply	O
a	O
nonlinear	O
function	O
from	O
a	O
set	O
of	O
input	O
variables	O
to	O
a	O
set	O
of	O
output	O
variables	O
controlled	O
by	O
a	O
vector	O
w	O
of	O
adjustable	O
parameters	O
this	O
function	O
can	O
be	O
represented	O
in	O
the	O
form	O
of	O
a	O
network	O
diagram	O
as	O
shown	O
in	O
figure	O
the	O
process	O
of	O
evaluating	O
can	O
then	O
be	O
interpreted	O
as	O
a	O
forward	B
propagation	I
of	O
information	O
through	O
the	O
network	O
it	O
should	O
be	O
emphasized	O
that	O
these	O
diagrams	O
do	O
not	O
represent	O
probabilistic	O
graphical	O
models	O
of	O
the	O
kind	O
to	O
be	O
considered	O
in	O
chapter	O
because	O
the	O
internal	O
nodes	O
represent	O
deterministic	O
variables	O
rather	O
than	O
stochastic	B
ones	O
for	O
this	O
reason	O
we	O
have	O
adopted	O
a	O
slightly	O
different	O
graphical	O
feed-forward	O
network	O
functions	O
notation	O
for	O
the	O
two	O
kinds	O
of	O
model	O
we	O
shall	O
see	O
later	O
how	O
to	O
give	O
a	O
probabilistic	O
interpretation	O
to	O
a	O
neural	B
network	I
as	O
discussed	O
in	O
section	O
the	O
bias	B
parameters	O
in	O
can	O
be	O
absorbed	O
into	O
the	O
set	O
of	O
weight	O
parameters	O
by	O
defining	O
an	O
additional	O
input	O
variable	O
whose	O
value	O
is	O
clamped	O
at	O
so	O
that	O
takes	O
the	O
form	O
aj	O
ji	O
xi	O
w	O
we	O
can	O
similarly	O
absorb	O
the	O
second-layer	O
biases	O
into	O
the	O
second-layer	O
weights	O
so	O
that	O
the	O
overall	O
network	O
function	O
becomes	O
ykx	O
w	O
w	O
kj	O
h	O
ji	O
xi	O
w	O
as	O
can	O
be	O
seen	O
from	O
figure	O
the	O
neural	B
network	I
model	O
comprises	O
two	O
stages	O
of	O
processing	O
each	O
of	O
which	O
resembles	O
the	O
perceptron	B
model	O
of	O
section	O
and	O
for	O
this	O
reason	O
the	O
neural	B
network	I
is	O
also	O
known	O
as	O
the	O
multilayer	B
perceptron	B
or	O
mlp	O
a	O
key	O
difference	O
compared	O
to	O
the	O
perceptron	B
however	O
is	O
that	O
the	O
neural	B
network	I
uses	O
continuous	O
sigmoidal	O
nonlinearities	O
in	O
the	O
hidden	O
units	O
whereas	O
the	O
perceptron	B
uses	O
step-function	O
nonlinearities	O
this	O
means	O
that	O
the	O
neural	B
network	I
function	O
is	O
differentiable	O
with	O
respect	O
to	O
the	O
network	O
parameters	O
and	O
this	O
property	O
will	O
play	O
a	O
central	O
role	O
in	O
network	O
training	B
if	O
the	O
activation	O
functions	O
of	O
all	O
the	O
hidden	O
units	O
in	O
a	O
network	O
are	O
taken	O
to	O
be	O
linear	O
then	O
for	O
any	O
such	O
network	O
we	O
can	O
always	O
find	O
an	O
equivalent	O
network	O
without	O
hidden	O
units	O
this	O
follows	O
from	O
the	O
fact	O
that	O
the	O
composition	O
of	O
successive	O
linear	O
transformations	O
is	O
itself	O
a	O
linear	O
transformation	O
however	O
if	O
the	O
number	O
of	O
hidden	O
units	O
is	O
smaller	O
than	O
either	O
the	O
number	O
of	O
input	O
or	O
output	O
units	O
then	O
the	O
transformations	O
that	O
the	O
network	O
can	O
generate	O
are	O
not	O
the	O
most	O
general	O
possible	O
linear	O
transformations	O
from	O
inputs	O
to	O
outputs	O
because	O
information	O
is	O
lost	O
in	O
the	O
dimensionality	O
reduction	O
at	O
the	O
hidden	O
units	O
in	O
section	O
we	O
show	O
that	O
networks	O
of	O
linear	O
units	O
give	O
rise	O
to	O
principal	B
component	I
analysis	I
in	O
general	O
however	O
there	O
is	O
little	O
interest	O
in	O
multilayer	O
networks	O
of	O
linear	O
units	O
the	O
network	O
architecture	O
shown	O
in	O
figure	O
is	O
the	O
most	O
commonly	O
used	O
one	O
in	O
practice	O
however	O
it	O
is	O
easily	O
generalized	B
for	O
instance	O
by	O
considering	O
additional	O
layers	O
of	O
processing	O
each	O
consisting	O
of	O
a	O
weighted	O
linear	O
combination	O
of	O
the	O
form	O
followed	O
by	O
an	O
element-wise	O
transformation	O
using	O
a	O
nonlinear	O
activation	B
function	I
note	O
that	O
there	O
is	O
some	O
confusion	O
in	O
the	O
literature	O
regarding	O
the	O
terminology	O
for	O
counting	O
the	O
number	O
of	O
layers	O
in	O
such	O
networks	O
thus	O
the	O
network	O
in	O
figure	O
may	O
be	O
described	O
as	O
a	O
network	O
counts	O
the	O
number	O
of	O
layers	O
of	O
units	O
and	O
treats	O
the	O
inputs	O
as	O
units	O
or	O
sometimes	O
as	O
a	O
single-hidden-layer	O
network	O
counts	O
the	O
number	O
of	O
layers	O
of	O
hidden	O
units	O
we	O
recommend	O
a	O
terminology	O
in	O
which	O
figure	O
is	O
called	O
a	O
two-layer	O
network	O
because	O
it	O
is	O
the	O
number	O
of	O
layers	O
of	O
adaptive	O
weights	O
that	O
is	O
important	O
for	O
determining	O
the	O
network	O
properties	O
another	O
generalization	B
of	O
the	O
network	O
architecture	O
is	O
to	O
include	O
skip-layer	O
connections	O
each	O
of	O
which	O
is	O
associated	O
with	O
a	O
corresponding	O
adaptive	O
parameter	O
for	O
neural	O
networks	O
figure	O
example	O
of	O
a	O
neural	B
network	I
having	O
a	O
general	O
feed-forward	O
topology	O
note	O
that	O
each	O
hidden	O
and	O
output	O
unit	O
has	O
an	O
associated	O
bias	B
parameter	I
for	O
clarity	O
inputs	O
outputs	O
instance	O
in	O
a	O
two-layer	O
network	O
these	O
would	O
go	O
directly	O
from	O
inputs	O
to	O
outputs	O
in	O
principle	O
a	O
network	O
with	O
sigmoidal	O
hidden	O
units	O
can	O
always	O
mimic	O
skip	O
layer	O
connections	O
bounded	O
input	O
values	O
by	O
using	O
a	O
sufficiently	O
small	O
first-layer	O
weight	O
that	O
over	O
its	O
operating	O
range	O
the	O
hidden	B
unit	I
is	O
effectively	O
linear	O
and	O
then	O
compensating	O
with	O
a	O
large	O
weight	O
value	O
from	O
the	O
hidden	B
unit	I
to	O
the	O
output	O
in	O
practice	O
however	O
it	O
may	O
be	O
advantageous	O
to	O
include	O
skip-layer	O
connections	O
explicitly	O
furthermore	O
the	O
network	O
can	O
be	O
sparse	O
with	O
not	O
all	O
possible	O
connections	O
within	O
a	O
layer	O
being	O
present	O
we	O
shall	O
see	O
an	O
example	O
of	O
a	O
sparse	O
network	O
architecture	O
when	O
we	O
consider	O
convolutional	B
neural	O
networks	O
in	O
section	O
because	O
there	O
is	O
a	O
direct	O
correspondence	O
between	O
a	O
network	O
diagram	O
and	O
its	O
mathematical	O
function	O
we	O
can	O
develop	O
more	O
general	O
network	O
mappings	O
by	O
considering	O
more	O
complex	O
network	O
diagrams	O
however	O
these	O
must	O
be	O
restricted	O
to	O
a	O
feed-forward	O
architecture	O
in	O
other	O
words	O
to	O
one	O
having	O
no	O
closed	O
directed	B
cycles	O
to	O
ensure	O
that	O
the	O
outputs	O
are	O
deterministic	O
functions	O
of	O
the	O
inputs	O
this	O
is	O
illustrated	O
with	O
a	O
simple	O
example	O
in	O
figure	O
each	O
or	O
output	O
unit	O
in	O
such	O
a	O
network	O
computes	O
a	O
function	O
given	O
by	O
zk	O
h	O
wkjzj	O
j	O
where	O
the	O
sum	O
runs	O
over	O
all	O
units	O
that	O
send	O
connections	O
to	O
unit	O
k	O
a	O
bias	B
parameter	I
is	O
included	O
in	O
the	O
summation	O
for	O
a	O
given	O
set	O
of	O
values	O
applied	O
to	O
the	O
inputs	O
of	O
the	O
network	O
successive	O
application	O
of	O
allows	O
the	O
activations	O
of	O
all	O
units	O
in	O
the	O
network	O
to	O
be	O
evaluated	O
including	O
those	O
of	O
the	O
output	O
units	O
the	O
approximation	O
properties	O
of	O
feed-forward	O
networks	O
have	O
been	O
widely	O
studied	O
cybenko	O
hornik	O
et	O
al	O
stinchecombe	O
and	O
white	O
cotter	O
ito	O
hornik	O
kreinovich	O
ripley	O
and	O
found	O
to	O
be	O
very	O
general	O
neural	O
networks	O
are	O
therefore	O
said	O
to	O
be	O
universal	O
approximators	O
for	O
example	O
a	O
two-layer	O
network	O
with	O
linear	O
outputs	O
can	O
uniformly	O
approximate	O
any	O
continuous	O
function	O
on	O
a	O
compact	O
input	O
domain	O
to	O
arbitrary	O
accuracy	O
provided	O
the	O
network	O
has	O
a	O
sufficiently	O
large	O
number	O
of	O
hidden	O
units	O
this	O
result	O
holds	O
for	O
a	O
wide	O
range	O
of	O
hidden	B
unit	I
activation	O
functions	O
but	O
excluding	O
polynomials	O
although	O
such	O
theorems	O
are	O
reassuring	O
the	O
key	O
problem	O
is	O
how	O
to	O
find	O
suitable	O
parameter	O
values	O
given	O
a	O
set	O
of	O
training	B
data	O
and	O
in	O
later	O
sections	O
of	O
this	O
chapter	O
we	O
figure	O
illustration	O
of	O
the	O
capability	O
of	O
a	O
multilayer	B
perceptron	B
to	O
approximate	O
four	O
different	O
functions	O
comprising	O
f	O
f	O
sinx	O
f	O
and	O
f	O
hx	O
where	O
hx	O
is	O
the	O
heaviside	B
step	I
function	I
in	O
each	O
case	O
n	O
data	O
points	O
shown	O
as	O
blue	O
dots	O
have	O
been	O
sampled	O
uniformly	O
in	O
x	O
over	O
the	O
interval	O
and	O
the	O
corresponding	O
values	O
of	O
f	O
evaluated	O
these	O
data	O
points	O
are	O
then	O
used	O
to	O
train	O
a	O
twolayer	O
network	O
having	O
hidden	O
units	O
with	O
tanh	O
activation	O
functions	O
and	O
linear	O
output	O
units	O
the	O
resulting	O
network	O
functions	O
are	O
shown	O
by	O
the	O
red	O
curves	O
and	O
the	O
outputs	O
of	O
the	O
three	O
hidden	O
units	O
are	O
shown	O
by	O
the	O
three	O
dashed	O
curves	O
feed-forward	O
network	O
functions	O
will	O
show	O
that	O
there	O
exist	O
effective	O
solutions	O
to	O
this	O
problem	O
based	O
on	O
both	O
maximum	B
likelihood	I
and	O
bayesian	B
approaches	O
the	O
capability	O
of	O
a	O
two-layer	O
network	O
to	O
model	O
a	O
broad	O
range	O
of	O
functions	O
is	O
illustrated	O
in	O
figure	O
this	O
figure	O
also	O
shows	O
how	O
individual	O
hidden	O
units	O
work	O
collaboratively	O
to	O
approximate	O
the	O
final	O
function	O
the	O
role	O
of	O
hidden	O
units	O
in	O
a	O
simple	O
classification	B
problem	O
is	O
illustrated	O
in	O
figure	O
using	O
the	O
synthetic	O
classification	B
data	O
set	O
described	O
in	O
appendix	O
a	O
weight-space	O
symmetries	B
one	O
property	O
of	O
feed-forward	O
networks	O
which	O
will	O
play	O
a	O
role	O
when	O
we	O
consider	O
bayesian	B
model	B
comparison	I
is	O
that	O
multiple	O
distinct	O
choices	O
for	O
the	O
weight	B
vector	I
w	O
can	O
all	O
give	O
rise	O
to	O
the	O
same	O
mapping	O
function	O
from	O
inputs	O
to	O
outputs	O
et	O
al	O
consider	O
a	O
two-layer	O
network	O
of	O
the	O
form	O
shown	O
in	O
figure	O
with	O
m	O
hidden	O
units	O
having	O
tanh	O
activation	O
functions	O
and	O
full	O
connectivity	O
in	O
both	O
layers	O
if	O
we	O
change	O
the	O
sign	O
of	O
all	O
of	O
the	O
weights	O
and	O
the	O
bias	B
feeding	O
into	O
a	O
particular	O
hidden	B
unit	I
then	O
for	O
a	O
given	O
input	O
pattern	O
the	O
sign	O
of	O
the	O
activation	O
of	O
the	O
hidden	B
unit	I
will	O
be	O
reversed	O
because	O
tanh	O
is	O
an	O
odd	O
function	O
so	O
that	O
tanh	O
a	O
tanha	O
this	O
transformation	O
can	O
be	O
exactly	O
compensated	O
by	O
changing	O
the	O
sign	O
of	O
all	O
of	O
the	O
weights	O
leading	O
out	O
of	O
that	O
hidden	B
unit	I
thus	O
by	O
changing	O
the	O
signs	O
of	O
a	O
particular	O
group	O
of	O
weights	O
a	O
bias	B
the	O
input	O
output	O
mapping	O
function	O
represented	O
by	O
the	O
network	O
is	O
unchanged	O
and	O
so	O
we	O
have	O
found	O
two	O
different	O
weight	O
vectors	O
that	O
give	O
rise	O
to	O
the	O
same	O
mapping	O
function	O
for	O
m	O
hidden	O
units	O
there	O
will	O
be	O
m	O
such	O
sign-flip	O
neural	O
networks	O
figure	O
example	O
of	O
the	O
solution	O
of	O
a	O
simple	O
twoclass	O
classification	B
problem	O
involving	O
synthetic	O
data	O
using	O
a	O
neural	B
network	I
having	O
two	O
inputs	O
two	O
hidden	O
units	O
with	O
tanh	O
activation	O
functions	O
and	O
a	O
single	O
output	O
having	O
a	O
logistic	B
sigmoid	I
activation	B
function	I
the	O
dashed	O
blue	O
lines	O
show	O
the	O
z	O
contours	O
for	O
each	O
of	O
the	O
hidden	O
units	O
and	O
the	O
red	O
line	O
shows	O
the	O
y	O
decision	O
surface	O
for	O
the	O
network	O
for	O
comparison	O
the	O
green	O
line	O
denotes	O
the	O
optimal	O
decision	B
boundary	I
computed	O
from	O
the	O
distributions	O
used	O
to	O
generate	O
the	O
data	O
symmetries	B
and	O
thus	O
any	O
given	O
weight	B
vector	I
will	O
be	O
one	O
of	O
a	O
set	O
equivalent	O
weight	O
vectors	O
similarly	O
imagine	O
that	O
we	O
interchange	O
the	O
values	O
of	O
all	O
of	O
the	O
weights	O
the	O
bias	B
leading	O
both	O
into	O
and	O
out	O
of	O
a	O
particular	O
hidden	B
unit	I
with	O
the	O
corresponding	O
values	O
of	O
the	O
weights	O
bias	B
associated	O
with	O
a	O
different	O
hidden	B
unit	I
again	O
this	O
clearly	O
leaves	O
the	O
network	O
input	O
output	O
mapping	O
function	O
unchanged	O
but	O
it	O
corresponds	O
to	O
a	O
different	O
choice	O
of	O
weight	B
vector	I
for	O
m	O
hidden	O
units	O
any	O
given	O
weight	B
vector	I
will	O
belong	O
to	O
a	O
set	O
of	O
m	O
equivalent	O
weight	O
vectors	O
associated	O
with	O
this	O
interchange	O
symmetry	O
corresponding	O
to	O
the	O
m	O
different	O
orderings	O
of	O
the	O
hidden	O
units	O
the	O
network	O
will	O
therefore	O
have	O
an	O
overall	O
weight-space	B
symmetry	I
factor	O
of	O
for	O
networks	O
with	O
more	O
than	O
two	O
layers	O
of	O
weights	O
the	O
total	O
level	O
of	O
symmetry	O
will	O
be	O
given	O
by	O
the	O
product	O
of	O
such	O
factors	O
one	O
for	O
each	O
layer	O
of	O
hidden	O
units	O
it	O
turns	O
out	O
that	O
these	O
factors	O
account	O
for	O
all	O
of	O
the	O
symmetries	B
in	O
weight	O
space	O
for	O
possible	O
accidental	O
symmetries	B
due	O
to	O
specific	O
choices	O
for	O
the	O
weight	O
values	O
furthermore	O
the	O
existence	O
of	O
these	O
symmetries	B
is	O
not	O
a	O
particular	O
property	O
of	O
the	O
tanh	O
function	O
but	O
applies	O
to	O
a	O
wide	O
range	O
of	O
activation	O
functions	O
urkov	O
a	O
and	O
kainen	O
in	O
many	O
cases	O
these	O
symmetries	B
in	O
weight	O
space	O
are	O
of	O
little	O
practical	O
consequence	O
although	O
in	O
section	O
we	O
shall	O
encounter	O
a	O
situation	O
in	O
which	O
we	O
need	O
to	O
take	O
them	O
into	O
account	O
network	O
training	B
so	O
far	O
we	O
have	O
viewed	O
neural	O
networks	O
as	O
a	O
general	O
class	O
of	O
parametric	O
nonlinear	O
functions	O
from	O
a	O
vector	O
x	O
of	O
input	O
variables	O
to	O
a	O
vector	O
y	O
of	O
output	O
variables	O
a	O
simple	O
approach	O
to	O
the	O
problem	O
of	O
determining	O
the	O
network	O
parameters	O
is	O
to	O
make	O
an	O
analogy	O
with	O
the	O
discussion	O
of	O
polynomial	B
curve	B
fitting	I
in	O
section	O
and	O
therefore	O
to	O
minimize	O
a	O
sum-of-squares	B
error	B
function	I
given	O
a	O
training	B
set	I
comprising	O
a	O
set	O
of	O
input	O
vectors	O
where	O
n	O
n	O
together	O
with	O
a	O
corresponding	O
set	O
of	O
network	O
training	B
target	O
vectors	O
we	O
minimize	O
the	O
error	B
function	I
ew	O
w	O
however	O
we	O
can	O
provide	O
a	O
much	O
more	O
general	O
view	O
of	O
network	O
training	B
by	O
first	O
giving	O
a	O
probabilistic	O
interpretation	O
to	O
the	O
network	O
outputs	O
we	O
have	O
already	O
seen	O
many	O
advantages	O
of	O
using	O
probabilistic	O
predictions	O
in	O
section	O
here	O
it	O
will	O
also	O
provide	O
us	O
with	O
a	O
clearer	O
motivation	O
both	O
for	O
the	O
choice	O
of	O
output	O
unit	O
nonlinearity	O
and	O
the	O
choice	O
of	O
error	B
function	I
we	O
start	O
by	O
discussing	O
regression	B
problems	O
and	O
for	O
the	O
moment	O
we	O
consider	O
a	O
single	O
target	O
variable	O
t	O
that	O
can	O
take	O
any	O
real	O
value	O
following	O
the	O
discussions	O
in	O
section	O
and	O
we	O
assume	O
that	O
t	O
has	O
a	O
gaussian	B
distribution	O
with	O
an	O
xdependent	O
mean	B
which	O
is	O
given	O
by	O
the	O
output	O
of	O
the	O
neural	B
network	I
so	O
that	O
ptx	O
w	O
tyx	O
w	O
where	O
is	O
the	O
precision	O
variance	B
of	O
the	O
gaussian	B
noise	O
of	O
course	O
this	O
is	O
a	O
somewhat	O
restrictive	O
assumption	O
and	O
in	O
section	O
we	O
shall	O
see	O
how	O
to	O
extend	O
this	O
approach	O
to	O
allow	O
for	O
more	O
general	O
conditional	B
distributions	O
for	O
the	O
conditional	B
distribution	O
given	O
by	O
it	O
is	O
sufficient	O
to	O
take	O
the	O
output	O
unit	O
activation	B
function	I
to	O
be	O
the	O
identity	O
because	O
such	O
a	O
network	O
can	O
approximate	O
any	O
continuous	O
function	O
from	O
x	O
to	O
y	O
given	O
a	O
data	O
set	O
of	O
n	O
independent	B
identically	I
distributed	I
observations	O
x	O
xn	O
along	O
with	O
corresponding	O
target	O
values	O
t	O
tn	O
we	O
can	O
construct	O
the	O
corresponding	O
likelihood	B
function	I
ptnxn	O
w	O
ptx	O
w	O
taking	O
the	O
negative	O
logarithm	O
we	O
obtain	O
the	O
error	B
function	I
w	O
n	O
ln	O
n	O
which	O
can	O
be	O
used	O
to	O
learn	O
the	O
parameters	O
w	O
and	O
in	O
section	O
we	O
shall	O
discuss	O
the	O
bayesian	B
treatment	O
of	O
neural	O
networks	O
while	O
here	O
we	O
consider	O
a	O
maximum	B
likelihood	I
approach	O
note	O
that	O
in	O
the	O
neural	O
networks	O
literature	O
it	O
is	O
usual	O
to	O
consider	O
the	O
minimization	O
of	O
an	O
error	B
function	I
rather	O
than	O
the	O
maximization	O
of	O
the	O
likelihood	O
and	O
so	O
here	O
we	O
shall	O
follow	O
this	O
convention	O
consider	O
first	O
the	O
determination	O
of	O
w	O
maximizing	O
the	O
likelihood	B
function	I
is	O
equivalent	O
to	O
minimizing	O
the	O
sum-of-squares	B
error	B
function	I
given	O
by	O
ew	O
w	O
neural	O
networks	O
where	O
we	O
have	O
discarded	O
additive	O
and	O
multiplicative	O
constants	O
the	O
value	O
of	O
w	O
found	O
by	O
minimizing	O
ew	O
will	O
be	O
denoted	O
wml	O
because	O
it	O
corresponds	O
to	O
the	O
maximum	B
likelihood	I
solution	O
in	O
practice	O
the	O
nonlinearity	O
of	O
the	O
network	O
function	O
yxn	O
w	O
causes	O
the	O
error	B
ew	O
to	O
be	O
nonconvex	O
and	O
so	O
in	O
practice	O
local	B
maxima	O
of	O
the	O
likelihood	O
may	O
be	O
found	O
corresponding	O
to	O
local	B
minima	O
of	O
the	O
error	B
function	I
as	O
discussed	O
in	O
section	O
having	O
found	O
wml	O
the	O
value	O
of	O
can	O
be	O
found	O
by	O
minimizing	O
the	O
negative	O
log	O
likelihood	O
to	O
give	O
ml	O
n	O
wml	O
note	O
that	O
this	O
can	O
be	O
evaluated	O
once	O
the	O
iterative	O
optimization	O
required	O
to	O
find	O
wml	O
is	O
completed	O
if	O
we	O
have	O
multiple	O
target	O
variables	O
and	O
we	O
assume	O
that	O
they	O
are	O
independent	B
conditional	B
on	O
x	O
and	O
w	O
with	O
shared	O
noise	O
precision	O
then	O
the	O
conditional	B
distribution	O
of	O
the	O
target	O
values	O
is	O
given	O
by	O
tyx	O
w	O
exercise	O
following	O
the	O
same	O
argument	O
as	O
for	O
a	O
single	O
target	O
variable	O
we	O
see	O
that	O
the	O
maximum	B
likelihood	I
weights	O
are	O
determined	O
by	O
minimizing	O
the	O
sum-of-squares	B
error	B
function	I
the	O
noise	O
precision	O
is	O
then	O
given	O
by	O
ptx	O
w	O
ml	O
n	O
k	O
wml	O
exercise	O
where	O
k	O
is	O
the	O
number	O
of	O
target	O
variables	O
the	O
assumption	O
of	O
independence	O
can	O
be	O
dropped	O
at	O
the	O
expense	O
of	O
a	O
slightly	O
more	O
complex	O
optimization	O
problem	O
recall	O
from	O
section	O
that	O
there	O
is	O
a	O
natural	O
pairing	O
of	O
the	O
error	B
function	I
by	O
the	O
negative	O
log	O
likelihood	O
and	O
the	O
output	O
unit	O
activation	B
function	I
in	O
the	O
regression	B
case	O
we	O
can	O
view	O
the	O
network	O
as	O
having	O
an	O
output	O
activation	B
function	I
that	O
is	O
the	O
identity	O
so	O
that	O
yk	O
ak	O
the	O
corresponding	O
sum-of-squares	B
error	B
function	I
has	O
the	O
property	O
yk	O
tk	O
e	O
ak	O
which	O
we	O
shall	O
make	O
use	O
of	O
when	O
discussing	O
error	B
backpropagation	B
in	O
section	O
now	O
consider	O
the	O
case	O
of	O
binary	O
classification	B
in	O
which	O
we	O
have	O
a	O
single	O
target	O
variable	O
t	O
such	O
that	O
t	O
denotes	O
class	O
and	O
t	O
denotes	O
class	O
following	O
the	O
discussion	O
of	O
canonical	O
link	B
functions	O
in	O
section	O
we	O
consider	O
a	O
network	O
having	O
a	O
single	O
output	O
whose	O
activation	B
function	I
is	O
a	O
logistic	B
sigmoid	I
y	O
so	O
that	O
yx	O
w	O
we	O
can	O
interpret	O
yx	O
w	O
as	O
the	O
conditional	B
probability	B
with	O
given	O
by	O
yx	O
w	O
the	O
conditional	B
distribution	O
of	O
targets	O
given	O
inputs	O
is	O
then	O
a	O
bernoulli	B
distribution	I
of	O
the	O
form	O
exp	O
a	O
ptx	O
w	O
yx	O
wt	O
yx	O
t	O
network	O
training	B
if	O
we	O
consider	O
a	O
training	B
set	I
of	O
independent	B
observations	O
then	O
the	O
error	B
function	I
which	O
is	O
given	O
by	O
the	O
negative	O
log	O
likelihood	O
is	O
then	O
a	O
cross-entropy	B
error	B
function	I
of	O
the	O
form	O
ln	O
yn	O
tn	O
yn	O
ew	O
exercise	O
where	O
yn	O
denotes	O
yxn	O
w	O
note	O
that	O
there	O
is	O
no	O
analogue	O
of	O
the	O
noise	O
precision	O
because	O
the	O
target	O
values	O
are	O
assumed	O
to	O
be	O
correctly	O
labelled	O
however	O
the	O
model	O
is	O
easily	O
extended	B
to	O
allow	O
for	O
labelling	O
errors	O
simard	O
et	O
al	O
found	O
that	O
using	O
the	O
cross-entropy	B
error	B
function	I
instead	O
of	O
the	O
sum-of-squares	O
for	O
a	O
classification	B
problem	O
leads	O
to	O
faster	O
training	B
as	O
well	O
as	O
improved	O
generalization	B
if	O
we	O
have	O
k	O
separate	O
binary	O
classifications	O
to	O
perform	O
then	O
we	O
can	O
use	O
a	O
network	O
having	O
k	O
outputs	O
each	O
of	O
which	O
has	O
a	O
logistic	B
sigmoid	I
activation	B
function	I
associated	O
with	O
each	O
output	O
is	O
a	O
binary	O
class	O
label	O
tk	O
where	O
k	O
k	O
if	O
we	O
assume	O
that	O
the	O
class	O
labels	O
are	O
independent	B
given	O
the	O
input	O
vector	O
then	O
the	O
conditional	B
distribution	O
of	O
the	O
targets	O
is	O
ptx	O
w	O
ykx	O
wtk	O
ykx	O
tk	O
exercise	O
exercise	O
ew	O
taking	O
the	O
negative	O
logarithm	O
of	O
the	O
corresponding	O
likelihood	B
function	I
then	O
gives	O
the	O
following	O
error	B
function	I
ln	O
ynk	O
tnk	O
ynk	O
where	O
ynk	O
denotes	O
ykxn	O
w	O
again	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
activation	O
for	O
a	O
particular	O
output	O
unit	O
takes	O
the	O
form	O
just	O
as	O
in	O
the	O
regression	B
case	O
it	O
is	O
interesting	O
to	O
contrast	O
the	O
neural	B
network	I
solution	O
to	O
this	O
problem	O
with	O
the	O
corresponding	O
approach	O
based	O
on	O
a	O
linear	O
classification	B
model	O
of	O
the	O
kind	O
discussed	O
in	O
chapter	O
suppose	O
that	O
we	O
are	O
using	O
a	O
standard	O
two-layer	O
network	O
of	O
the	O
kind	O
shown	O
in	O
figure	O
we	O
see	O
that	O
the	O
weight	O
parameters	O
in	O
the	O
first	O
layer	O
of	O
the	O
network	O
are	O
shared	O
between	O
the	O
various	O
outputs	O
whereas	O
in	O
the	O
linear	O
model	O
each	O
classification	B
problem	O
is	O
solved	O
independently	O
the	O
first	O
layer	O
of	O
the	O
network	O
can	O
be	O
viewed	O
as	O
performing	O
a	O
nonlinear	O
feature	B
extraction	I
and	O
the	O
sharing	O
of	O
features	O
between	O
the	O
different	O
outputs	O
can	O
save	O
on	O
computation	O
and	O
can	O
also	O
lead	O
to	O
improved	O
generalization	B
finally	O
we	O
consider	O
the	O
standard	O
multiclass	B
classification	B
problem	O
in	O
which	O
each	O
input	O
is	O
assigned	O
to	O
one	O
of	O
k	O
mutually	O
exclusive	O
classes	O
the	O
binary	O
target	O
variables	O
tk	O
have	O
a	O
coding	O
scheme	O
indicating	O
the	O
class	O
and	O
the	O
network	O
outputs	O
are	O
interpreted	O
as	O
ykx	O
w	O
ptk	O
leading	O
to	O
the	O
following	O
error	B
function	I
ew	O
tkn	O
ln	O
ykxn	O
w	O
neural	O
networks	O
figure	O
geometrical	O
view	O
of	O
the	O
error	B
function	I
ew	O
as	O
a	O
surface	O
sitting	O
over	O
weight	O
space	O
point	O
wa	O
is	O
a	O
local	B
minimum	I
and	O
wb	O
is	O
the	O
global	B
minimum	I
at	O
any	O
point	O
wc	O
the	O
local	B
gradient	O
of	O
the	O
error	B
surface	O
is	O
given	O
by	O
the	O
vector	O
e	O
ew	O
wa	O
wb	O
wc	O
e	O
following	O
the	O
discussion	O
of	O
section	O
we	O
see	O
that	O
the	O
output	O
unit	O
activation	B
function	I
which	O
corresponds	O
to	O
the	O
canonical	O
link	B
is	O
given	O
by	O
the	O
softmax	B
function	I
j	O
expakx	O
w	O
expajx	O
w	O
ykx	O
w	O
which	O
satisfies	O
yk	O
and	O
k	O
yk	O
note	O
that	O
the	O
ykx	O
w	O
are	O
unchanged	O
if	O
a	O
constant	O
is	O
added	O
to	O
all	O
of	O
the	O
akx	O
w	O
causing	O
the	O
error	B
function	I
to	O
be	O
constant	O
for	O
some	O
directions	O
in	O
weight	O
space	O
this	O
degeneracy	O
is	O
removed	O
if	O
an	O
appropriate	O
regularization	B
term	O
is	O
added	O
to	O
the	O
error	B
function	I
exercise	O
a	O
particular	O
output	O
unit	O
takes	O
the	O
familiar	O
form	O
once	O
again	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
activation	O
for	O
in	O
summary	O
there	O
is	O
a	O
natural	O
choice	O
of	O
both	O
output	O
unit	O
activation	B
function	I
and	O
matching	O
error	B
function	I
according	O
to	O
the	O
type	O
of	O
problem	O
being	O
solved	O
for	B
regression	B
we	O
use	O
linear	O
outputs	O
and	O
a	O
sum-of-squares	B
error	B
for	O
independent	B
binary	O
classifications	O
we	O
use	O
logistic	B
sigmoid	I
outputs	O
and	O
a	O
cross-entropy	B
error	B
function	I
and	O
for	O
multiclass	B
classification	B
we	O
use	O
softmax	O
outputs	O
with	O
the	O
corresponding	O
multiclass	B
cross-entropy	B
error	B
function	I
for	O
classification	B
problems	O
involving	O
two	O
classes	O
we	O
can	O
use	O
a	O
single	O
logistic	B
sigmoid	I
output	O
or	O
alternatively	O
we	O
can	O
use	O
a	O
network	O
with	O
two	O
outputs	O
having	O
a	O
softmax	O
output	O
activation	B
function	I
parameter	O
optimization	O
we	O
turn	O
next	O
to	O
the	O
task	O
of	O
finding	O
a	O
weight	B
vector	I
w	O
which	O
minimizes	O
the	O
chosen	O
function	O
ew	O
at	O
this	O
point	O
it	O
is	O
useful	O
to	O
have	O
a	O
geometrical	O
picture	O
of	O
the	O
error	B
function	I
which	O
we	O
can	O
view	O
as	O
a	O
surface	O
sitting	O
over	O
weight	O
space	O
as	O
shown	O
in	O
figure	O
first	O
note	O
that	O
if	O
we	O
make	O
a	O
small	O
step	O
in	O
weight	O
space	O
from	O
w	O
to	O
w	O
w	O
then	O
the	O
change	O
in	O
the	O
error	B
function	I
is	O
e	O
wt	O
ew	O
where	O
the	O
vector	O
ew	O
points	O
in	O
the	O
direction	O
of	O
greatest	O
rate	O
of	O
increase	O
of	O
the	O
error	B
function	I
because	O
the	O
error	B
ew	O
is	O
a	O
smooth	O
continuous	O
function	O
of	O
w	O
its	O
smallest	O
value	O
will	O
occur	O
at	O
a	O
section	O
point	O
in	O
weight	O
space	O
such	O
that	O
the	O
gradient	O
of	O
the	O
error	B
function	I
vanishes	O
so	O
that	O
network	O
training	B
ew	O
as	O
otherwise	O
we	O
could	O
make	O
a	O
small	O
step	O
in	O
the	O
direction	O
of	O
ew	O
and	O
thereby	O
further	O
reduce	O
the	O
error	B
points	O
at	O
which	O
the	O
gradient	O
vanishes	O
are	O
called	O
stationary	B
points	O
and	O
may	O
be	O
further	O
classified	O
into	O
minima	O
maxima	O
and	O
saddle	O
points	O
our	O
goal	O
is	O
to	O
find	O
a	O
vector	O
w	O
such	O
that	O
ew	O
takes	O
its	O
smallest	O
value	O
however	O
the	O
error	B
function	I
typically	O
has	O
a	O
highly	O
nonlinear	O
dependence	O
on	O
the	O
weights	O
and	O
bias	B
parameters	O
and	O
so	O
there	O
will	O
be	O
many	O
points	O
in	O
weight	O
space	O
at	O
which	O
the	O
gradient	O
vanishes	O
is	O
numerically	O
very	O
small	O
indeed	O
from	O
the	O
discussion	O
in	O
section	O
we	O
see	O
that	O
for	O
any	O
point	O
w	O
that	O
is	O
a	O
local	B
minimum	I
there	O
will	O
be	O
other	O
points	O
in	O
weight	O
space	O
that	O
are	O
equivalent	O
minima	O
for	O
instance	O
in	O
a	O
two-layer	O
network	O
of	O
the	O
kind	O
shown	O
in	O
figure	O
with	O
m	O
hidden	O
units	O
each	O
point	O
in	O
weight	O
space	O
is	O
a	O
member	O
of	O
a	O
family	O
of	O
equivalent	O
points	O
furthermore	O
there	O
will	O
typically	O
be	O
multiple	O
inequivalent	O
stationary	B
points	O
and	O
in	O
particular	O
multiple	O
inequivalent	O
minima	O
a	O
minimum	O
that	O
corresponds	O
to	O
the	O
smallest	O
value	O
of	O
the	O
error	B
function	I
for	O
any	O
weight	B
vector	I
is	O
said	O
to	O
be	O
a	O
global	B
minimum	I
any	O
other	O
minima	O
corresponding	O
to	O
higher	O
values	O
of	O
the	O
error	B
function	I
are	O
said	O
to	O
be	O
local	B
minima	O
for	O
a	O
successful	O
application	O
of	O
neural	O
networks	O
it	O
may	O
not	O
be	O
necessary	O
to	O
find	O
the	O
global	B
minimum	I
in	O
general	O
it	O
will	O
not	O
be	O
known	O
whether	O
the	O
global	B
minimum	I
has	O
been	O
found	O
but	O
it	O
may	O
be	O
necessary	O
to	O
compare	O
several	O
local	B
minima	O
in	O
order	O
to	O
find	O
a	O
sufficiently	O
good	O
solution	O
because	O
there	O
is	O
clearly	O
no	O
hope	O
of	O
finding	O
an	O
analytical	O
solution	O
to	O
the	O
equation	O
ew	O
we	O
resort	O
to	O
iterative	O
numerical	O
procedures	O
the	O
optimization	O
of	O
continuous	O
nonlinear	O
functions	O
is	O
a	O
widely	O
studied	O
problem	O
and	O
there	O
exists	O
an	O
extensive	O
literature	O
on	O
how	O
to	O
solve	O
it	O
efficiently	O
most	O
techniques	O
involve	O
choosing	O
some	O
initial	O
value	O
for	O
the	O
weight	B
vector	I
and	O
then	O
moving	O
through	O
weight	O
space	O
in	O
a	O
succession	O
of	O
steps	O
of	O
the	O
form	O
w	O
w	O
w	O
where	O
labels	O
the	O
iteration	O
step	O
different	O
algorithms	O
involve	O
different	O
choices	O
for	O
the	O
weight	B
vector	I
update	O
w	O
many	O
algorithms	O
make	O
use	O
of	O
gradient	O
information	O
and	O
therefore	O
require	O
that	O
after	O
each	O
update	O
the	O
value	O
of	O
ew	O
is	O
evaluated	O
at	O
the	O
new	O
weight	B
vector	I
w	O
in	O
order	O
to	O
understand	O
the	O
importance	O
of	O
gradient	O
information	O
it	O
is	O
useful	O
to	O
consider	O
a	O
local	B
approximation	O
to	O
the	O
error	B
function	I
based	O
on	O
a	O
taylor	O
expansion	O
local	B
quadratic	O
approximation	O
insight	O
into	O
the	O
optimization	O
problem	O
and	O
into	O
the	O
various	O
techniques	O
for	O
solving	O
it	O
can	O
be	O
obtained	O
by	O
considering	O
a	O
local	B
quadratic	O
approximation	O
to	O
the	O
error	B
function	I
consider	O
the	O
taylor	O
expansion	O
of	O
ew	O
around	O
some	O
in	O
weight	O
space	O
ew	O
neural	O
networks	O
where	O
cubic	O
and	O
higher	O
terms	O
have	O
been	O
omitted	O
here	O
b	O
is	O
defined	O
to	O
be	O
the	O
gradient	O
of	O
e	O
evaluated	O
b	O
ewbw	O
and	O
the	O
hessian	B
matrix	I
h	O
e	O
has	O
elements	O
e	O
wi	O
wj	O
e	O
b	O
hw	O
wbw	O
from	O
the	O
corresponding	O
local	B
approximation	O
to	O
the	O
gradient	O
is	O
given	O
by	O
for	O
points	O
w	O
that	O
are	O
sufficiently	O
close	O
to	O
these	O
expressions	O
will	O
give	O
reasonable	O
approximations	O
for	O
the	O
error	B
and	O
its	O
gradient	O
consider	O
the	O
particular	O
case	O
of	O
a	O
local	B
quadratic	O
approximation	O
around	O
a	O
point	O
that	O
is	O
a	O
minimum	O
of	O
the	O
error	B
function	I
in	O
this	O
case	O
there	O
is	O
no	O
linear	O
term	O
because	O
e	O
at	O
and	O
becomes	O
ew	O
where	O
the	O
hessian	O
h	O
is	O
evaluated	O
at	O
in	O
order	O
to	O
interpret	O
this	O
geometrically	O
consider	O
the	O
eigenvalue	O
equation	O
for	O
the	O
hessian	B
matrix	I
hui	O
iui	O
where	O
the	O
eigenvectors	O
ui	O
form	O
a	O
complete	O
orthonormal	O
set	O
c	O
so	O
that	O
we	O
now	O
expand	O
as	O
a	O
linear	O
combination	O
of	O
the	O
eigenvectors	O
in	O
the	O
form	O
ut	O
i	O
uj	O
ij	O
w	O
iui	O
i	O
this	O
can	O
be	O
regarded	O
as	O
a	O
transformation	O
of	O
the	O
coordinate	O
system	O
in	O
which	O
the	O
origin	O
is	O
translated	O
to	O
the	O
point	O
and	O
the	O
axes	O
are	O
rotated	O
to	O
align	O
with	O
the	O
eigenvectors	O
the	O
orthogonal	O
matrix	O
whose	O
columns	O
are	O
the	O
ui	O
and	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
appendix	O
c	O
substituting	O
into	O
and	O
using	O
and	O
allows	O
the	O
error	B
function	I
to	O
be	O
written	O
in	O
the	O
form	O
ew	O
i	O
i	O
i	O
a	O
matrix	O
h	O
is	O
said	O
to	O
be	O
positive	B
definite	I
if	O
and	O
only	O
if	O
vthv	O
for	O
all	O
v	O
the	O
error	B
figure	O
in	O
the	O
neighbourhood	O
of	O
a	O
minimum	O
function	O
can	O
be	O
approximated	O
by	O
a	O
quadratic	O
contours	O
of	O
constant	O
error	B
are	O
then	O
ellipses	O
whose	O
axes	O
are	O
aligned	O
with	O
the	O
eigenvectors	O
ui	O
of	O
the	O
hessian	B
matrix	I
with	O
lengths	O
that	O
are	O
inversely	O
proportional	O
to	O
the	O
square	O
roots	O
of	O
the	O
corresponding	O
eigenvectors	O
i	O
network	O
training	B
because	O
the	O
eigenvectors	O
form	O
a	O
complete	O
set	O
an	O
arbitrary	O
vector	O
v	O
can	O
be	O
written	O
in	O
the	O
form	O
from	O
and	O
we	O
then	O
have	O
v	O
ciui	O
i	O
i	O
vthv	O
i	O
i	O
exercise	O
exercise	O
and	O
so	O
h	O
will	O
be	O
positive	B
definite	I
if	O
and	O
only	O
if	O
all	O
of	O
its	O
eigenvalues	O
are	O
positive	O
in	O
the	O
new	O
coordinate	O
system	O
whose	O
basis	O
vectors	O
are	O
given	O
by	O
the	O
eigenvectors	O
the	O
contours	O
of	O
constant	O
e	O
are	O
ellipses	O
centred	O
on	O
the	O
origin	O
as	O
illustrated	O
in	O
figure	O
for	O
a	O
one-dimensional	O
weight	O
space	O
a	O
stationary	B
point	O
will	O
be	O
a	O
minimum	O
if	O
exercise	O
the	O
corresponding	O
result	O
in	O
d-dimensions	O
is	O
that	O
the	O
hessian	B
matrix	I
evaluated	O
at	O
should	O
be	O
positive	B
definite	I
use	O
of	O
gradient	O
information	O
as	O
we	O
shall	O
see	O
in	O
section	O
it	O
is	O
possible	O
to	O
evaluate	O
the	O
gradient	O
of	O
an	O
error	B
function	I
efficiently	O
by	O
means	O
of	O
the	O
backpropagation	B
procedure	O
the	O
use	O
of	O
this	O
gradient	O
information	O
can	O
lead	O
to	O
significant	O
improvements	O
in	O
the	O
speed	O
with	O
which	O
the	O
minima	O
of	O
the	O
error	B
function	I
can	O
be	O
located	O
we	O
can	O
see	O
why	O
this	O
is	O
so	O
as	O
follows	O
in	O
the	O
quadratic	O
approximation	O
to	O
the	O
error	B
function	I
given	O
in	O
the	O
error	B
surface	O
is	O
specified	O
by	O
the	O
quantities	O
b	O
and	O
h	O
which	O
contain	O
a	O
total	O
of	O
w	O
independent	B
elements	O
the	O
matrix	O
h	O
is	O
symmetric	O
where	O
w	O
is	O
the	O
dimensionality	O
of	O
w	O
the	O
total	O
number	O
of	O
adaptive	O
parameters	O
in	O
the	O
network	O
the	O
location	O
of	O
the	O
minimum	O
of	O
this	O
quadratic	O
approximation	O
therefore	O
depends	O
on	O
ow	O
parameters	O
and	O
we	O
should	O
not	O
expect	O
to	O
be	O
able	O
to	O
locate	O
the	O
minimum	O
until	O
we	O
have	O
gathered	O
ow	O
independent	B
pieces	O
of	O
information	O
if	O
we	O
do	O
not	O
make	O
use	O
of	O
gradient	O
information	O
we	O
would	O
expect	O
to	O
have	O
to	O
perform	O
ow	O
function	O
exercise	O
neural	O
networks	O
evaluations	O
each	O
of	O
which	O
would	O
require	O
ow	O
steps	O
thus	O
the	O
computational	O
effort	O
needed	O
to	O
find	O
the	O
minimum	O
using	O
such	O
an	O
approach	O
would	O
be	O
ow	O
now	O
compare	O
this	O
with	O
an	O
algorithm	O
that	O
makes	O
use	O
of	O
the	O
gradient	O
information	O
because	O
each	O
evaluation	O
of	O
e	O
brings	O
w	O
items	O
of	O
information	O
we	O
might	O
hope	O
to	O
find	O
the	O
minimum	O
of	O
the	O
function	O
in	O
ow	O
gradient	O
evaluations	O
as	O
we	O
shall	O
see	O
by	O
using	O
error	B
backpropagation	B
each	O
such	O
evaluation	O
takes	O
only	O
ow	O
steps	O
and	O
so	O
the	O
minimum	O
can	O
now	O
be	O
found	O
in	O
ow	O
steps	O
for	O
this	O
reason	O
the	O
use	O
of	O
gradient	O
information	O
forms	O
the	O
basis	O
of	O
practical	O
algorithms	O
for	O
training	B
neural	O
networks	O
w	O
w	O
ew	O
gradient	B
descent	I
optimization	O
the	O
simplest	O
approach	O
to	O
using	O
gradient	O
information	O
is	O
to	O
choose	O
the	O
weight	O
update	O
in	O
to	O
comprise	O
a	O
small	O
step	O
in	O
the	O
direction	O
of	O
the	O
negative	O
gradient	O
so	O
that	O
where	O
the	O
parameter	O
is	O
known	O
as	O
the	O
learning	B
rate	O
after	O
each	O
such	O
update	O
the	O
gradient	O
is	O
re-evaluated	O
for	O
the	O
new	O
weight	B
vector	I
and	O
the	O
process	O
repeated	O
note	O
that	O
the	O
error	B
function	I
is	O
defined	O
with	O
respect	O
to	O
a	O
training	B
set	I
and	O
so	O
each	O
step	O
requires	O
that	O
the	O
entire	O
training	B
set	I
be	O
processed	O
in	O
order	O
to	O
evaluate	O
e	O
techniques	O
that	O
use	O
the	O
whole	O
data	O
set	O
at	O
once	O
are	O
called	O
batch	O
methods	O
at	O
each	O
step	O
the	O
weight	B
vector	I
is	O
moved	O
in	O
the	O
direction	O
of	O
the	O
greatest	O
rate	O
of	O
decrease	O
of	O
the	O
error	B
function	I
and	O
so	O
this	O
approach	O
is	O
known	O
as	O
gradient	B
descent	I
or	O
steepest	B
descent	I
although	O
such	O
an	O
approach	O
might	O
intuitively	O
seem	O
reasonable	O
in	O
fact	O
it	O
turns	O
out	O
to	O
be	O
a	O
poor	O
algorithm	O
for	O
reasons	O
discussed	O
in	O
bishop	O
and	O
nabney	O
for	O
batch	O
optimization	O
there	O
are	O
more	O
efficient	O
methods	O
such	O
as	O
conjugate	B
gradients	O
and	O
quasi-newton	O
methods	O
which	O
are	O
much	O
more	O
robust	O
and	O
much	O
faster	O
than	O
simple	O
gradient	B
descent	I
et	O
al	O
fletcher	O
nocedal	O
and	O
wright	O
unlike	O
gradient	B
descent	I
these	O
algorithms	O
have	O
the	O
property	O
that	O
the	O
error	B
function	I
always	O
decreases	O
at	O
each	O
iteration	O
unless	O
the	O
weight	B
vector	I
has	O
arrived	O
at	O
a	O
local	B
or	O
global	B
minimum	I
in	O
order	O
to	O
find	O
a	O
sufficiently	O
good	O
minimum	O
it	O
may	O
be	O
necessary	O
to	O
run	O
a	O
gradient-based	O
algorithm	O
multiple	O
times	O
each	O
time	O
using	O
a	O
different	O
randomly	O
chosen	O
starting	O
point	O
and	O
comparing	O
the	O
resulting	O
performance	O
on	O
an	O
independent	B
validation	B
set	I
there	O
is	O
however	O
an	O
on-line	O
version	O
of	O
gradient	B
descent	I
that	O
has	O
proved	O
useful	O
in	O
practice	O
for	O
training	B
neural	O
networks	O
on	O
large	O
data	O
sets	O
cun	O
et	O
al	O
error	B
functions	O
based	O
on	O
maximum	B
likelihood	I
for	O
a	O
set	O
of	O
independent	B
observations	O
comprise	O
a	O
sum	O
of	O
terms	O
one	O
for	O
each	O
data	O
point	O
ew	O
enw	O
on-line	O
gradient	B
descent	I
also	O
known	O
as	O
sequential	B
gradient	B
descent	I
or	O
stochastic	B
gradient	B
descent	I
makes	O
an	O
update	O
to	O
the	O
weight	B
vector	I
based	O
on	O
one	O
data	O
point	O
at	O
a	O
time	O
so	O
that	O
w	O
w	O
enw	O
error	B
backpropagation	B
this	O
update	O
is	O
repeated	O
by	O
cycling	O
through	O
the	O
data	O
either	O
in	O
sequence	O
or	O
by	O
selecting	O
points	O
at	O
random	O
with	O
replacement	O
there	O
are	O
of	O
course	O
intermediate	O
scenarios	O
in	O
which	O
the	O
updates	O
are	O
based	O
on	O
batches	O
of	O
data	O
points	O
one	O
advantage	O
of	O
on-line	O
methods	O
compared	O
to	O
batch	O
methods	O
is	O
that	O
the	O
former	O
handle	O
redundancy	O
in	O
the	O
data	O
much	O
more	O
efficiently	O
to	O
see	O
this	O
consider	O
an	O
extreme	O
example	O
in	O
which	O
we	O
take	O
a	O
data	O
set	O
and	O
double	O
its	O
size	O
by	O
duplicating	O
every	O
data	O
point	O
note	O
that	O
this	O
simply	O
multiplies	O
the	O
error	B
function	I
by	O
a	O
factor	O
of	O
and	O
so	O
is	O
equivalent	O
to	O
using	O
the	O
original	O
error	B
function	I
batch	O
methods	O
will	O
require	O
double	O
the	O
computational	O
effort	O
to	O
evaluate	O
the	O
batch	O
error	B
function	I
gradient	O
whereas	O
online	O
methods	O
will	O
be	O
unaffected	O
another	O
property	O
of	O
on-line	O
gradient	B
descent	I
is	O
the	O
possibility	O
of	O
escaping	O
from	O
local	B
minima	O
since	O
a	O
stationary	B
point	O
with	O
respect	O
to	O
the	O
error	B
function	I
for	O
the	O
whole	O
data	O
set	O
will	O
generally	O
not	O
be	O
a	O
stationary	B
point	O
for	O
each	O
data	O
point	O
individually	O
nonlinear	O
optimization	O
algorithms	O
and	O
their	O
practical	O
application	O
to	O
neural	O
net	O
work	O
training	B
are	O
discussed	O
in	O
detail	O
in	O
bishop	O
and	O
nabney	O
error	B
backpropagation	B
our	O
goal	O
in	O
this	O
section	O
is	O
to	O
find	O
an	O
efficient	O
technique	O
for	O
evaluating	O
the	O
gradient	O
of	O
an	O
error	B
function	I
ew	O
for	O
a	O
feed-forward	O
neural	B
network	I
we	O
shall	O
see	O
that	O
this	O
can	O
be	O
achieved	O
using	O
a	O
local	B
message	B
passing	I
scheme	O
in	O
which	O
information	O
is	O
sent	O
alternately	O
forwards	O
and	O
backwards	O
through	O
the	O
network	O
and	O
is	O
known	O
as	O
error	B
backpropagation	B
or	O
sometimes	O
simply	O
as	O
backprop	O
it	O
should	O
be	O
noted	O
that	O
the	O
term	O
backpropagation	B
is	O
used	O
in	O
the	O
neural	O
computing	O
literature	O
to	O
mean	B
a	O
variety	O
of	O
different	O
things	O
for	O
instance	O
the	O
multilayer	B
perceptron	B
architecture	O
is	O
sometimes	O
called	O
a	O
backpropagation	B
network	O
the	O
term	O
backpropagation	B
is	O
also	O
used	O
to	O
describe	O
the	O
training	B
of	O
a	O
multilayer	B
perceptron	B
using	O
gradient	B
descent	I
applied	O
to	O
a	O
sum-of-squares	B
error	B
function	I
in	O
order	O
to	O
clarify	O
the	O
terminology	O
it	O
is	O
useful	O
to	O
consider	O
the	O
nature	O
of	O
the	O
training	B
process	O
more	O
carefully	O
most	O
training	B
algorithms	O
involve	O
an	O
iterative	O
procedure	O
for	O
minimization	O
of	O
an	O
error	B
function	I
with	O
adjustments	O
to	O
the	O
weights	O
being	O
made	O
in	O
a	O
sequence	O
of	O
steps	O
at	O
each	O
such	O
step	O
we	O
can	O
distinguish	O
between	O
two	O
distinct	O
stages	O
in	O
the	O
first	O
stage	O
the	O
derivatives	O
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
must	O
be	O
evaluated	O
as	O
we	O
shall	O
see	O
the	O
important	O
contribution	O
of	O
the	O
backpropagation	B
technique	O
is	O
in	O
providing	O
a	O
computationally	O
efficient	O
method	O
for	O
evaluating	O
such	O
derivatives	O
because	O
it	O
is	O
at	O
this	O
stage	O
that	O
errors	O
are	O
propagated	O
backwards	O
through	O
the	O
network	O
we	O
shall	O
use	O
the	O
term	O
backpropagation	B
specifically	O
to	O
describe	O
the	O
evaluation	O
of	O
derivatives	O
in	O
the	O
second	O
stage	O
the	O
derivatives	O
are	O
then	O
used	O
to	O
compute	O
the	O
adjustments	O
to	O
be	O
made	O
to	O
the	O
weights	O
the	O
simplest	O
such	O
technique	O
and	O
the	O
one	O
originally	O
considered	O
by	O
rumelhart	O
et	O
al	O
involves	O
gradient	B
descent	I
it	O
is	O
important	O
to	O
recognize	O
that	O
the	O
two	O
stages	O
are	O
distinct	O
thus	O
the	O
first	O
stage	O
namely	O
the	O
propagation	O
of	O
errors	O
backwards	O
through	O
the	O
network	O
in	O
order	O
to	O
evaluate	O
derivatives	O
can	O
be	O
applied	O
to	O
many	O
other	O
kinds	O
of	O
network	O
and	O
not	O
just	O
the	O
multilayer	B
perceptron	B
it	O
can	O
also	O
be	O
applied	O
to	O
error	B
functions	O
other	O
that	O
just	O
the	O
simple	O
sum-of-squares	O
and	O
to	O
the	O
eval	O
neural	O
networks	O
uation	O
of	O
other	O
derivatives	O
such	O
as	O
the	O
jacobian	O
and	O
hessian	O
matrices	O
as	O
we	O
shall	O
see	O
later	O
in	O
this	O
chapter	O
similarly	O
the	O
second	O
stage	O
of	O
weight	O
adjustment	O
using	O
the	O
calculated	O
derivatives	O
can	O
be	O
tackled	O
using	O
a	O
variety	O
of	O
optimization	O
schemes	O
many	O
of	O
which	O
are	O
substantially	O
more	O
powerful	O
than	O
simple	O
gradient	B
descent	I
evaluation	O
of	O
error-function	O
derivatives	O
we	O
now	O
derive	O
the	O
backpropagation	B
algorithm	O
for	O
a	O
general	O
network	O
having	O
arbitrary	O
feed-forward	O
topology	O
arbitrary	O
differentiable	O
nonlinear	O
activation	O
functions	O
and	O
a	O
broad	O
class	O
of	O
error	B
function	I
the	O
resulting	O
formulae	O
will	O
then	O
be	O
illustrated	O
using	O
a	O
simple	O
layered	O
network	O
structure	O
having	O
a	O
single	O
layer	O
of	O
sigmoidal	O
hidden	O
units	O
together	O
with	O
a	O
sum-of-squares	B
error	B
many	O
error	B
functions	O
of	O
practical	O
interest	O
for	O
instance	O
those	O
defined	O
by	O
maximum	B
likelihood	I
for	O
a	O
set	O
of	O
i	O
i	O
d	O
data	O
comprise	O
a	O
sum	O
of	O
terms	O
one	O
for	O
each	O
data	O
point	O
in	O
the	O
training	B
set	I
so	O
that	O
i	O
ew	O
here	O
we	O
shall	O
consider	O
the	O
problem	O
of	O
evaluating	O
enw	O
for	O
one	O
such	O
term	O
in	O
the	O
error	B
function	I
this	O
may	O
be	O
used	O
directly	O
for	O
sequential	O
optimization	O
or	O
the	O
results	O
can	O
be	O
accumulated	O
over	O
the	O
training	B
set	I
in	O
the	O
case	O
of	O
batch	O
methods	O
enw	O
consider	O
first	O
a	O
simple	O
linear	O
model	O
in	O
which	O
the	O
outputs	O
yk	O
are	O
linear	O
combina	O
tions	O
of	O
the	O
input	O
variables	O
xi	O
so	O
that	O
yk	O
wkixi	O
together	O
with	O
an	O
error	B
function	I
that	O
for	O
a	O
particular	O
input	O
pattern	O
n	O
takes	O
the	O
form	O
en	O
k	O
tnjxni	O
en	O
wji	O
where	O
ynk	O
ykxn	O
w	O
the	O
gradient	O
of	O
this	O
error	B
function	I
with	O
respect	O
to	O
a	O
weight	O
wji	O
is	O
given	O
by	O
which	O
can	O
be	O
interpreted	O
as	O
a	O
local	B
computation	O
involving	O
the	O
product	O
of	O
an	O
error	B
signal	O
ynj	O
tnj	O
associated	O
with	O
the	O
output	O
end	O
of	O
the	O
link	B
wji	O
and	O
the	O
variable	O
xni	O
associated	O
with	O
the	O
input	O
end	O
of	O
the	O
link	B
in	O
section	O
we	O
saw	O
how	O
a	O
similar	O
formula	O
arises	O
with	O
the	O
logistic	B
sigmoid	I
activation	B
function	I
together	O
with	O
the	O
cross	O
entropy	B
error	B
function	I
and	O
similarly	O
for	O
the	O
softmax	O
activation	B
function	I
together	O
with	O
its	O
matching	O
cross-entropy	B
error	B
function	I
we	O
shall	O
now	O
see	O
how	O
this	O
simple	O
result	O
extends	O
to	O
the	O
more	O
complex	O
setting	O
of	O
multilayer	O
feed-forward	O
networks	O
in	O
a	O
general	O
feed-forward	O
network	O
each	O
unit	O
computes	O
a	O
weighted	O
sum	O
of	O
its	O
inputs	O
of	O
the	O
form	O
aj	O
wjizi	O
i	O
error	B
backpropagation	B
where	O
zi	O
is	O
the	O
activation	O
of	O
a	O
unit	O
or	O
input	O
that	O
sends	O
a	O
connection	O
to	O
unit	O
j	O
and	O
wji	O
is	O
the	O
weight	O
associated	O
with	O
that	O
connection	O
in	O
section	O
we	O
saw	O
that	O
biases	O
can	O
be	O
included	O
in	O
this	O
sum	O
by	O
introducing	O
an	O
extra	O
unit	O
or	O
input	O
with	O
activation	O
fixed	O
at	O
we	O
therefore	O
do	O
not	O
need	O
to	O
deal	O
with	O
biases	O
explicitly	O
the	O
sum	O
in	O
is	O
transformed	O
by	O
a	O
nonlinear	O
activation	B
function	I
h	O
to	O
give	O
the	O
activation	O
zj	O
of	O
unit	O
j	O
in	O
the	O
form	O
zj	O
haj	O
note	O
that	O
one	O
or	O
more	O
of	O
the	O
variables	O
zi	O
in	O
the	O
sum	O
in	O
could	O
be	O
an	O
input	O
and	O
similarly	O
the	O
unit	O
j	O
in	O
could	O
be	O
an	O
output	O
for	O
each	O
pattern	O
in	O
the	O
training	B
set	I
we	O
shall	O
suppose	O
that	O
we	O
have	O
supplied	O
the	O
corresponding	O
input	O
vector	O
to	O
the	O
network	O
and	O
calculated	O
the	O
activations	O
of	O
all	O
of	O
the	O
hidden	O
and	O
output	O
units	O
in	O
the	O
network	O
by	O
successive	O
application	O
of	O
and	O
this	O
process	O
is	O
often	O
called	O
forward	B
propagation	I
because	O
it	O
can	O
be	O
regarded	O
as	O
a	O
forward	O
flow	O
of	O
information	O
through	O
the	O
network	O
now	O
consider	O
the	O
evaluation	O
of	O
the	O
derivative	B
of	O
en	O
with	O
respect	O
to	O
a	O
weight	O
wji	O
the	O
outputs	O
of	O
the	O
various	O
units	O
will	O
depend	O
on	O
the	O
particular	O
input	O
pattern	O
n	O
however	O
in	O
order	O
to	O
keep	O
the	O
notation	O
uncluttered	O
we	O
shall	O
omit	O
the	O
subscript	O
n	O
from	O
the	O
network	O
variables	O
first	O
we	O
note	O
that	O
en	O
depends	O
on	O
the	O
weight	O
wji	O
only	O
via	O
the	O
summed	O
input	O
aj	O
to	O
unit	O
j	O
we	O
can	O
therefore	O
apply	O
the	O
chain	O
rule	O
for	O
partial	O
derivatives	O
to	O
give	O
we	O
now	O
introduce	O
a	O
useful	O
notation	O
j	O
en	O
aj	O
en	O
wji	O
en	O
aj	O
aj	O
wji	O
where	O
the	O
s	O
are	O
often	O
referred	O
to	O
as	O
errors	O
for	O
reasons	O
we	O
shall	O
see	O
shortly	O
using	O
we	O
can	O
write	O
aj	O
wji	O
zi	O
substituting	O
and	O
into	O
we	O
then	O
obtain	O
en	O
wji	O
jzi	O
equation	O
tells	O
us	O
that	O
the	O
required	O
derivative	B
is	O
obtained	O
simply	O
by	O
multiplying	O
the	O
value	O
of	O
for	O
the	O
unit	O
at	O
the	O
output	O
end	O
of	O
the	O
weight	O
by	O
the	O
value	O
of	O
z	O
for	O
the	O
unit	O
at	O
the	O
input	O
end	O
of	O
the	O
weight	O
z	O
in	O
the	O
case	O
of	O
a	O
bias	B
note	O
that	O
this	O
takes	O
the	O
same	O
form	O
as	O
for	O
the	O
simple	O
linear	O
model	O
considered	O
at	O
the	O
start	O
of	O
this	O
section	O
thus	O
in	O
order	O
to	O
evaluate	O
the	O
derivatives	O
we	O
need	O
only	O
to	O
calculate	O
the	O
value	O
of	O
j	O
for	O
each	O
hidden	O
and	O
output	O
unit	O
in	O
the	O
network	O
and	O
then	O
apply	O
as	O
we	O
have	O
seen	O
already	O
for	O
the	O
output	O
units	O
we	O
have	O
k	O
yk	O
tk	O
neural	O
networks	O
figure	O
illustration	O
of	O
the	O
calculation	O
of	O
j	O
for	O
hidden	B
unit	I
j	O
by	O
backpropagation	B
of	O
the	O
s	O
from	O
those	O
units	O
k	O
to	O
which	O
unit	O
j	O
sends	O
connections	O
the	O
blue	O
arrow	O
denotes	O
the	O
direction	O
of	O
information	O
flow	O
during	O
forward	B
propagation	I
and	O
the	O
red	O
arrows	O
indicate	O
the	O
backward	O
propagation	O
of	O
error	B
information	O
zi	O
wji	O
j	O
zj	O
wkj	O
k	O
provided	O
we	O
are	O
using	O
the	O
canonical	O
link	B
as	O
the	O
output-unit	O
activation	B
function	I
to	O
evaluate	O
the	O
s	O
for	O
hidden	O
units	O
we	O
again	O
make	O
use	O
of	O
the	O
chain	O
rule	O
for	O
partial	O
derivatives	O
j	O
en	O
aj	O
en	O
ak	O
ak	O
aj	O
k	O
where	O
the	O
sum	O
runs	O
over	O
all	O
units	O
k	O
to	O
which	O
unit	O
j	O
sends	O
connections	O
the	O
arrangement	O
of	O
units	O
and	O
weights	O
is	O
illustrated	O
in	O
figure	O
note	O
that	O
the	O
units	O
labelled	O
k	O
could	O
include	O
other	O
hidden	O
units	O
andor	O
output	O
units	O
in	O
writing	O
down	O
we	O
are	O
making	O
use	O
of	O
the	O
fact	O
that	O
variations	O
in	O
aj	O
give	O
rise	O
to	O
variations	O
in	O
the	O
error	B
function	I
only	O
through	O
variations	O
in	O
the	O
variables	O
ak	O
if	O
we	O
now	O
substitute	O
the	O
definition	O
of	O
given	O
by	O
into	O
and	O
make	O
use	O
of	O
and	O
we	O
obtain	O
the	O
following	O
backpropagation	B
formula	O
j	O
h	O
wkj	O
k	O
k	O
which	O
tells	O
us	O
that	O
the	O
value	O
of	O
for	O
a	O
particular	O
hidden	B
unit	I
can	O
be	O
obtained	O
by	O
propagating	O
the	O
s	O
backwards	O
from	O
units	O
higher	O
up	O
in	O
the	O
network	O
as	O
illustrated	O
in	O
figure	O
note	O
that	O
the	O
summation	O
in	O
is	O
taken	O
over	O
the	O
first	O
index	O
on	O
wkj	O
to	O
backward	O
propagation	O
of	O
information	O
through	O
the	O
network	O
whereas	O
in	O
the	O
forward	B
propagation	I
equation	O
it	O
is	O
taken	O
over	O
the	O
second	O
index	O
because	O
we	O
already	O
know	O
the	O
values	O
of	O
the	O
s	O
for	O
the	O
output	O
units	O
it	O
follows	O
that	O
by	O
recursively	O
applying	O
we	O
can	O
evaluate	O
the	O
s	O
for	O
all	O
of	O
the	O
hidden	O
units	O
in	O
a	O
feed-forward	O
network	O
regardless	O
of	O
its	O
topology	O
the	O
backpropagation	B
procedure	O
can	O
therefore	O
be	O
summarized	O
as	O
follows	O
error	B
backpropagation	B
apply	O
an	O
input	O
vector	O
xn	O
to	O
the	O
network	O
and	O
forward	O
propagate	O
through	O
the	O
network	O
using	O
and	O
to	O
find	O
the	O
activations	O
of	O
all	O
the	O
hidden	O
and	O
output	O
units	O
evaluate	O
the	O
k	O
for	O
all	O
the	O
output	O
units	O
using	O
backpropagate	O
the	O
s	O
using	O
to	O
obtain	O
j	O
for	O
each	O
hidden	B
unit	I
in	O
the	O
network	O
use	O
to	O
evaluate	O
the	O
required	O
derivatives	O
error	B
backpropagation	B
for	O
batch	O
methods	O
the	O
derivative	B
of	O
the	O
total	O
error	B
e	O
can	O
then	O
be	O
obtained	O
by	O
repeating	O
the	O
above	O
steps	O
for	O
each	O
pattern	O
in	O
the	O
training	B
set	I
and	O
then	O
summing	O
over	O
all	O
patterns	O
e	O
wji	O
en	O
wji	O
n	O
in	O
the	O
above	O
derivation	O
we	O
have	O
implicitly	O
assumed	O
that	O
each	O
hidden	O
or	O
output	O
unit	O
in	O
the	O
network	O
has	O
the	O
same	O
activation	B
function	I
h	O
the	O
derivation	O
is	O
easily	O
generalized	B
however	O
to	O
allow	O
different	O
units	O
to	O
have	O
individual	O
activation	O
functions	O
simply	O
by	O
keeping	O
track	O
of	O
which	O
form	O
of	O
h	O
goes	O
with	O
which	O
unit	O
a	O
simple	O
example	O
the	O
above	O
derivation	O
of	O
the	O
backpropagation	B
procedure	O
allowed	O
for	O
general	O
forms	O
for	O
the	O
error	B
function	I
the	O
activation	O
functions	O
and	O
the	O
network	O
topology	O
in	O
order	O
to	O
illustrate	O
the	O
application	O
of	O
this	O
algorithm	O
we	O
shall	O
consider	O
a	O
particular	O
example	O
this	O
is	O
chosen	O
both	O
for	O
its	O
simplicity	O
and	O
for	O
its	O
practical	O
importance	O
because	O
many	O
applications	O
of	O
neural	O
networks	O
reported	O
in	O
the	O
literature	O
make	O
use	O
of	O
this	O
type	O
of	O
network	O
specifically	O
we	O
shall	O
consider	O
a	O
two-layer	O
network	O
of	O
the	O
form	O
illustrated	O
in	O
figure	O
together	O
with	O
a	O
sum-of-squares	B
error	B
in	O
which	O
the	O
output	O
units	O
have	O
linear	O
activation	O
functions	O
so	O
that	O
yk	O
ak	O
while	O
the	O
hidden	O
units	O
have	O
logistic	B
sigmoid	I
activation	O
functions	O
given	O
by	O
where	O
a	O
useful	O
feature	O
of	O
this	O
function	O
is	O
that	O
its	O
derivative	B
can	O
be	O
expressed	O
in	O
a	O
par	O
ticularly	O
simple	O
form	O
we	O
also	O
consider	O
a	O
standard	O
sum-of-squares	B
error	B
function	I
so	O
that	O
for	O
pattern	O
n	O
the	O
error	B
is	O
given	O
by	O
h	O
where	O
yk	O
is	O
the	O
activation	O
of	O
output	O
unit	O
k	O
and	O
tk	O
is	O
the	O
corresponding	O
target	O
for	O
a	O
particular	O
input	O
pattern	O
xn	O
for	O
each	O
pattern	O
in	O
the	O
training	B
set	I
in	O
turn	O
we	O
first	O
perform	O
a	O
forward	B
propagation	I
using	O
aj	O
ha	O
tanha	O
tanha	O
ea	O
e	O
a	O
ea	O
e	O
a	O
en	O
ji	O
xi	O
w	O
zj	O
tanhaj	O
yk	O
w	O
kj	O
zj	O
neural	O
networks	O
next	O
we	O
compute	O
the	O
s	O
for	O
each	O
output	O
unit	O
using	O
k	O
yk	O
tk	O
then	O
we	O
backpropagate	O
these	O
to	O
obtain	O
s	O
for	O
the	O
hidden	O
units	O
using	O
j	O
j	O
wkj	O
k	O
finally	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
first-layer	O
and	O
second-layer	O
weights	O
are	O
given	O
by	O
en	O
ji	O
w	O
jxi	O
en	O
kj	O
w	O
kzj	O
efficiency	O
of	O
backpropagation	B
one	O
of	O
the	O
most	O
important	O
aspects	O
of	O
backpropagation	B
is	O
its	O
computational	O
efficiency	O
to	O
understand	O
this	O
let	O
us	O
examine	O
how	O
the	O
number	O
of	O
computer	O
operations	O
required	O
to	O
evaluate	O
the	O
derivatives	O
of	O
the	O
error	B
function	I
scales	O
with	O
the	O
total	O
number	O
w	O
of	O
weights	O
and	O
biases	O
in	O
the	O
network	O
a	O
single	O
evaluation	O
of	O
the	O
error	B
function	I
a	O
given	O
input	O
pattern	O
would	O
require	O
ow	O
operations	O
for	O
sufficiently	O
large	O
w	O
this	O
follows	O
from	O
the	O
fact	O
that	O
except	O
for	O
a	O
network	O
with	O
very	O
sparse	O
connections	O
the	O
number	O
of	O
weights	O
is	O
typically	O
much	O
greater	O
than	O
the	O
number	O
of	O
units	O
and	O
so	O
the	O
bulk	O
of	O
the	O
computational	O
effort	O
in	O
forward	B
propagation	I
is	O
concerned	O
with	O
evaluating	O
the	O
sums	O
in	O
with	O
the	O
evaluation	O
of	O
the	O
activation	O
functions	O
representing	O
a	O
small	O
overhead	O
each	O
term	O
in	O
the	O
sum	O
in	O
requires	O
one	O
multiplication	O
and	O
one	O
addition	O
leading	O
to	O
an	O
overall	O
computational	O
cost	O
that	O
is	O
ow	O
an	O
alternative	O
approach	O
to	O
backpropagation	B
for	O
computing	O
the	O
derivatives	O
of	O
the	O
error	B
function	I
is	O
to	O
use	O
finite	O
differences	O
this	O
can	O
be	O
done	O
by	O
perturbing	O
each	O
weight	O
in	O
turn	O
and	O
approximating	O
the	O
derivatives	O
by	O
the	O
expression	O
enwji	O
enwji	O
en	O
wji	O
where	O
in	O
a	O
software	O
simulation	O
the	O
accuracy	O
of	O
the	O
approximation	O
to	O
the	O
derivatives	O
can	O
be	O
improved	O
by	O
making	O
smaller	O
until	O
numerical	O
roundoff	O
problems	O
arise	O
the	O
accuracy	O
of	O
the	O
finite	O
differences	O
method	O
can	O
be	O
improved	O
significantly	O
by	O
using	O
symmetrical	O
central	B
differences	I
of	O
the	O
form	O
o	O
enwji	O
enwji	O
en	O
wji	O
exercise	O
in	O
this	O
case	O
the	O
o	O
corrections	O
cancel	O
as	O
can	O
be	O
verified	O
by	O
taylor	O
expansion	O
on	O
the	O
right-hand	O
side	O
of	O
and	O
so	O
the	O
residual	O
corrections	O
are	O
the	O
number	O
of	O
computational	O
steps	O
is	O
however	O
roughly	O
doubled	O
compared	O
with	O
the	O
main	O
problem	O
with	O
numerical	O
differentiation	O
is	O
that	O
the	O
highly	O
desirable	O
ow	O
scaling	O
has	O
been	O
lost	O
each	O
forward	B
propagation	I
requires	O
ow	O
steps	O
and	O
error	B
backpropagation	B
figure	O
illustration	O
of	O
a	O
modular	O
pattern	O
recognition	O
system	O
in	O
which	O
the	O
jacobian	B
matrix	I
can	O
be	O
used	O
to	O
backpropagate	O
error	B
signals	O
from	O
the	O
outputs	O
through	O
to	O
earlier	O
modules	O
in	O
the	O
system	O
u	O
x	O
v	O
z	O
w	O
y	O
there	O
are	O
w	O
weights	O
in	O
the	O
network	O
each	O
of	O
which	O
must	O
be	O
perturbed	O
individually	O
so	O
that	O
the	O
overall	O
scaling	O
is	O
ow	O
however	O
numerical	O
differentiation	O
plays	O
an	O
important	O
role	O
in	O
practice	O
because	O
a	O
comparison	O
of	O
the	O
derivatives	O
calculated	O
by	O
backpropagation	B
with	O
those	O
obtained	O
using	O
central	B
differences	I
provides	O
a	O
powerful	O
check	O
on	O
the	O
correctness	O
of	O
any	O
software	O
implementation	O
of	O
the	O
backpropagation	B
algorithm	O
when	O
training	B
networks	O
in	O
practice	O
derivatives	O
should	O
be	O
evaluated	O
using	O
backpropagation	B
because	O
this	O
gives	O
the	O
greatest	O
accuracy	O
and	O
numerical	O
efficiency	O
however	O
the	O
results	O
should	O
be	O
compared	O
with	O
numerical	O
differentiation	O
using	O
for	O
some	O
test	O
cases	O
in	O
order	O
to	O
check	O
the	O
correctness	O
of	O
the	O
implementation	O
the	O
jacobian	B
matrix	I
we	O
have	O
seen	O
how	O
the	O
derivatives	O
of	O
an	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
can	O
be	O
obtained	O
by	O
the	O
propagation	O
of	O
errors	O
backwards	O
through	O
the	O
network	O
the	O
technique	O
of	O
backpropagation	B
can	O
also	O
be	O
applied	O
to	O
the	O
calculation	O
of	O
other	O
derivatives	O
here	O
we	O
consider	O
the	O
evaluation	O
of	O
the	O
jacobian	B
matrix	I
whose	O
elements	O
are	O
given	O
by	O
the	O
derivatives	O
of	O
the	O
network	O
outputs	O
with	O
respect	O
to	O
the	O
inputs	O
jki	O
yk	O
xi	O
where	O
each	O
such	O
derivative	B
is	O
evaluated	O
with	O
all	O
other	O
inputs	O
held	O
fixed	O
jacobian	O
matrices	O
play	O
a	O
useful	O
role	O
in	O
systems	O
built	O
from	O
a	O
number	O
of	O
distinct	O
modules	O
as	O
illustrated	O
in	O
figure	O
each	O
module	O
can	O
comprise	O
a	O
fixed	O
or	O
adaptive	O
function	O
which	O
can	O
be	O
linear	O
or	O
nonlinear	O
so	O
long	O
as	O
it	O
is	O
differentiable	O
suppose	O
we	O
wish	O
to	O
minimize	O
an	O
error	B
function	I
e	O
with	O
respect	O
to	O
the	O
parameter	O
w	O
in	O
figure	O
the	O
derivative	B
of	O
the	O
error	B
function	I
is	O
given	O
by	O
kj	O
e	O
w	O
e	O
yk	O
yk	O
zj	O
zj	O
w	O
in	O
which	O
the	O
jacobian	B
matrix	I
for	O
the	O
red	O
module	O
in	O
figure	O
appears	O
in	O
the	O
middle	O
term	O
because	O
the	O
jacobian	B
matrix	I
provides	O
a	O
measure	O
of	O
the	O
local	B
sensitivity	O
of	O
the	O
outputs	O
to	O
changes	O
in	O
each	O
of	O
the	O
input	O
variables	O
it	O
also	O
allows	O
any	O
known	O
errors	O
xi	O
neural	O
networks	O
associated	O
with	O
the	O
inputs	O
to	O
be	O
propagated	O
through	O
the	O
trained	O
network	O
in	O
order	O
to	O
estimate	O
their	O
contribution	O
yk	O
to	O
the	O
errors	O
at	O
the	O
outputs	O
through	O
the	O
relation	O
yk	O
yk	O
xi	O
xi	O
which	O
is	O
valid	O
provided	O
the	O
xi	O
are	O
small	O
in	O
general	O
the	O
network	O
mapping	O
represented	O
by	O
a	O
trained	O
neural	B
network	I
will	O
be	O
nonlinear	O
and	O
so	O
the	O
elements	O
of	O
the	O
jacobian	B
matrix	I
will	O
not	O
be	O
constants	O
but	O
will	O
depend	O
on	O
the	O
particular	O
input	O
vector	O
used	O
thus	O
is	O
valid	O
only	O
for	O
small	O
perturbations	O
of	O
the	O
inputs	O
and	O
the	O
jacobian	O
itself	O
must	O
be	O
re-evaluated	O
for	O
each	O
new	O
input	O
vector	O
i	O
the	O
jacobian	B
matrix	I
can	O
be	O
evaluated	O
using	O
a	O
backpropagation	B
procedure	O
that	O
is	O
similar	O
to	O
the	O
one	O
derived	O
earlier	O
for	O
evaluating	O
the	O
derivatives	O
of	O
an	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
we	O
start	O
by	O
writing	O
the	O
element	O
jki	O
in	O
the	O
form	O
j	O
j	O
jki	O
yk	O
xi	O
yk	O
aj	O
aj	O
xi	O
wji	O
yk	O
aj	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
sum	O
in	O
runs	O
over	O
all	O
units	O
j	O
to	O
which	O
the	O
input	O
unit	O
i	O
sends	O
connections	O
example	O
over	O
all	O
units	O
in	O
the	O
first	O
hidden	O
layer	O
in	O
the	O
layered	O
topology	O
considered	O
earlier	O
we	O
now	O
write	O
down	O
a	O
recursive	O
backpropagation	B
formula	O
to	O
determine	O
the	O
derivatives	O
yk	O
aj	O
yk	O
aj	O
al	O
aj	O
yk	O
al	O
l	O
l	O
h	O
wlj	O
yk	O
al	O
where	O
the	O
sum	O
runs	O
over	O
all	O
units	O
l	O
to	O
which	O
unit	O
j	O
sends	O
connections	O
to	O
the	O
first	O
index	O
of	O
wlj	O
again	O
we	O
have	O
made	O
use	O
of	O
and	O
this	O
backpropagation	B
starts	O
at	O
the	O
output	O
units	O
for	O
which	O
the	O
required	O
derivatives	O
can	O
be	O
found	O
directly	O
from	O
the	O
functional	B
form	O
of	O
the	O
output-unit	O
activation	B
function	I
for	O
instance	O
if	O
we	O
have	O
individual	O
sigmoidal	O
activation	O
functions	O
at	O
each	O
output	O
unit	O
then	O
yk	O
aj	O
whereas	O
for	O
softmax	O
outputs	O
we	O
have	O
kj	O
kjyk	O
ykyj	O
yk	O
aj	O
we	O
can	O
summarize	O
the	O
procedure	O
for	O
evaluating	O
the	O
jacobian	B
matrix	I
as	O
follows	O
apply	O
the	O
input	O
vector	O
corresponding	O
to	O
the	O
point	O
in	O
input	O
space	O
at	O
which	O
the	O
jacobian	B
matrix	I
is	O
to	O
be	O
found	O
and	O
forward	O
propagate	O
in	O
the	O
usual	O
way	O
to	O
obtain	O
the	O
the	O
hessian	B
matrix	I
activations	O
of	O
all	O
of	O
the	O
hidden	O
and	O
output	O
units	O
in	O
the	O
network	O
next	O
for	O
each	O
row	O
k	O
of	O
the	O
jacobian	B
matrix	I
corresponding	O
to	O
the	O
output	O
unit	O
k	O
backpropagate	O
using	O
the	O
recursive	O
relation	O
starting	O
with	O
or	O
for	O
all	O
of	O
the	O
hidden	O
units	O
in	O
the	O
network	O
finally	O
use	O
to	O
do	O
the	O
backpropagation	B
to	O
the	O
inputs	O
the	O
jacobian	O
can	O
also	O
be	O
evaluated	O
using	O
an	O
alternative	O
forward	B
propagation	I
formalism	O
which	O
can	O
be	O
derived	O
in	O
an	O
analogous	O
way	O
to	O
the	O
backpropagation	B
approach	O
given	O
here	O
again	O
the	O
implementation	O
of	O
such	O
algorithms	O
can	O
be	O
checked	O
by	O
using	O
numeri	O
exercise	O
cal	O
differentiation	O
in	O
the	O
form	O
ykxi	O
ykxi	O
yk	O
xi	O
which	O
involves	O
forward	O
propagations	O
for	O
a	O
network	O
having	O
d	O
inputs	O
the	O
hessian	B
matrix	I
we	O
have	O
shown	O
how	O
the	O
technique	O
of	O
backpropagation	B
can	O
be	O
used	O
to	O
obtain	O
the	O
first	O
derivatives	O
of	O
an	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
in	O
the	O
network	O
backpropagation	B
can	O
also	O
be	O
used	O
to	O
evaluate	O
the	O
second	O
derivatives	O
of	O
the	O
error	B
given	O
by	O
wji	O
wlk	O
note	O
that	O
it	O
is	O
sometimes	O
convenient	O
to	O
consider	O
all	O
of	O
the	O
weight	O
and	O
bias	B
parameters	O
as	O
elements	O
wi	O
of	O
a	O
single	O
vector	O
denoted	O
w	O
in	O
which	O
case	O
the	O
second	O
derivatives	O
form	O
the	O
elements	O
hij	O
of	O
the	O
hessian	B
matrix	I
h	O
where	O
i	O
j	O
w	O
and	O
w	O
is	O
the	O
total	O
number	O
of	O
weights	O
and	O
biases	O
the	O
hessian	O
plays	O
an	O
important	O
role	O
in	O
many	O
aspects	O
of	O
neural	O
computing	O
including	O
the	O
following	O
several	O
nonlinear	O
optimization	O
algorithms	O
used	O
for	O
training	B
neural	O
networks	O
are	O
based	O
on	O
considerations	O
of	O
the	O
second-order	O
properties	O
of	O
the	O
error	B
surface	O
which	O
are	O
controlled	O
by	O
the	O
hessian	B
matrix	I
and	O
nabney	O
the	O
hessian	O
forms	O
the	O
basis	O
of	O
a	O
fast	O
procedure	O
for	O
re-training	O
a	O
feed-forward	O
network	O
following	O
a	O
small	O
change	O
in	O
the	O
training	B
data	O
the	O
inverse	B
of	O
the	O
hessian	O
has	O
been	O
used	O
to	O
identify	O
the	O
least	O
significant	O
weights	O
in	O
a	O
network	O
as	O
part	O
of	O
network	O
pruning	O
algorithms	O
cun	O
et	O
al	O
the	O
hessian	O
plays	O
a	O
central	O
role	O
in	O
the	O
laplace	B
approximation	I
for	O
a	O
bayesian	B
neural	B
network	I
section	O
its	O
inverse	B
is	O
used	O
to	O
determine	O
the	O
predictive	B
distribution	I
for	O
a	O
trained	O
network	O
its	O
eigenvalues	O
determine	O
the	O
values	O
of	O
hyperparameters	O
and	O
its	O
determinant	O
is	O
used	O
to	O
evaluate	O
the	O
model	B
evidence	I
various	O
approximation	O
schemes	O
have	O
been	O
used	O
to	O
evaluate	O
the	O
hessian	B
matrix	I
for	O
a	O
neural	B
network	I
however	O
the	O
hessian	O
can	O
also	O
be	O
calculated	O
exactly	O
using	O
an	O
extension	O
of	O
the	O
backpropagation	B
technique	O
neural	O
networks	O
an	O
important	O
consideration	O
for	O
many	O
applications	O
of	O
the	O
hessian	O
is	O
the	O
efficiency	O
with	O
which	O
it	O
can	O
be	O
evaluated	O
if	O
there	O
are	O
w	O
parameters	O
and	O
biases	O
in	O
the	O
network	O
then	O
the	O
hessian	B
matrix	I
has	O
dimensions	O
w	O
w	O
and	O
so	O
the	O
computational	O
effort	O
needed	O
to	O
evaluate	O
the	O
hessian	O
will	O
scale	O
like	O
ow	O
for	O
each	O
pattern	O
in	O
the	O
data	O
set	O
as	O
we	O
shall	O
see	O
there	O
are	O
efficient	O
methods	O
for	O
evaluating	O
the	O
hessian	O
whose	O
scaling	O
is	O
indeed	O
ow	O
diagonal	B
approximation	I
some	O
of	O
the	O
applications	O
for	O
the	O
hessian	B
matrix	I
discussed	O
above	O
require	O
the	O
inverse	B
of	O
the	O
hessian	O
rather	O
than	O
the	O
hessian	O
itself	O
for	O
this	O
reason	O
there	O
has	O
been	O
some	O
interest	O
in	O
using	O
a	O
diagonal	B
approximation	I
to	O
the	O
hessian	O
in	O
other	O
words	O
one	O
that	O
simply	O
replaces	O
the	O
off-diagonal	O
elements	O
with	O
zeros	O
because	O
its	O
inverse	B
is	O
trivial	O
to	O
evaluate	O
again	O
we	O
shall	O
consider	O
an	O
error	B
function	I
that	O
consists	O
of	O
a	O
sum	O
of	O
terms	O
one	O
for	O
each	O
pattern	O
in	O
the	O
data	O
set	O
so	O
that	O
e	O
n	O
en	O
the	O
hessian	O
can	O
then	O
be	O
obtained	O
by	O
considering	O
one	O
pattern	O
at	O
a	O
time	O
and	O
then	O
summing	O
the	O
results	O
over	O
all	O
patterns	O
from	O
the	O
diagonal	B
elements	O
of	O
the	O
hessian	O
for	O
pattern	O
n	O
can	O
be	O
written	O
ji	O
j	O
i	O
using	O
and	O
the	O
second	O
derivatives	O
on	O
the	O
right-hand	O
side	O
of	O
can	O
be	O
found	O
recursively	O
using	O
the	O
chain	O
rule	O
of	O
differential	B
calculus	O
to	O
give	O
a	O
backpropagation	B
equation	O
of	O
the	O
form	O
j	O
h	O
ak	O
h	O
wkj	O
en	O
ak	O
if	O
we	O
now	O
neglect	O
off-diagonal	O
elements	O
in	O
the	O
second-derivative	O
terms	O
we	O
obtain	O
and	O
le	O
cun	O
le	O
cun	O
et	O
al	O
k	O
k	O
k	O
k	O
j	O
h	O
kj	O
k	O
h	O
wkj	O
en	O
ak	O
note	O
that	O
the	O
number	O
of	O
computational	O
steps	O
required	O
to	O
evaluate	O
this	O
approximation	O
is	O
ow	O
where	O
w	O
is	O
the	O
total	O
number	O
of	O
weight	O
and	O
bias	B
parameters	O
in	O
the	O
network	O
compared	O
with	O
ow	O
for	O
the	O
full	O
hessian	O
ricotti	O
et	O
al	O
also	O
used	O
the	O
diagonal	B
approximation	I
to	O
the	O
hessian	O
but	O
they	O
retained	O
all	O
terms	O
in	O
the	O
evaluation	O
of	O
j	O
and	O
so	O
obtained	O
exact	O
expressions	O
for	O
the	O
diagonal	B
terms	O
note	O
that	O
this	O
no	O
longer	O
has	O
ow	O
scaling	O
the	O
major	O
problem	O
with	O
diagonal	B
approximations	O
however	O
is	O
that	O
in	O
practice	O
the	O
hessian	O
is	O
typically	O
found	O
to	O
be	O
strongly	O
nondiagonal	O
and	O
so	O
these	O
approximations	O
which	O
are	O
driven	O
mainly	O
be	O
computational	O
convenience	O
must	O
be	O
treated	O
with	O
care	O
the	O
hessian	B
matrix	I
outer	B
product	I
approximation	I
when	O
neural	O
networks	O
are	O
applied	O
to	O
regression	B
problems	O
it	O
is	O
common	O
to	O
use	O
a	O
sum-of-squares	B
error	B
function	I
of	O
the	O
form	O
e	O
exercise	O
an	O
analogous	O
result	O
can	O
be	O
obtained	O
for	O
multiclass	B
networks	O
having	O
softmax	O
outputunit	O
activation	O
functions	O
ynbnbt	O
n	O
exercise	O
exercise	O
exercise	O
where	O
we	O
have	O
considered	O
the	O
case	O
of	O
a	O
single	O
output	O
in	O
order	O
to	O
keep	O
the	O
notation	O
simple	O
extension	O
to	O
several	O
outputs	O
is	O
straightforward	O
we	O
can	O
then	O
write	O
the	O
hessian	B
matrix	I
in	O
the	O
form	O
h	O
e	O
yn	O
yn	O
tn	O
yn	O
if	O
the	O
network	O
has	O
been	O
trained	O
on	O
the	O
data	O
set	O
and	O
its	O
outputs	O
yn	O
happen	O
to	O
be	O
very	O
close	O
to	O
the	O
target	O
values	O
tn	O
then	O
the	O
second	O
term	O
in	O
will	O
be	O
small	O
and	O
can	O
be	O
neglected	O
more	O
generally	O
however	O
it	O
may	O
be	O
appropriate	O
to	O
neglect	O
this	O
term	O
by	O
the	O
following	O
argument	O
recall	O
from	O
section	O
that	O
the	O
optimal	O
function	O
that	O
minimizes	O
a	O
sum-of-squares	O
loss	O
is	O
the	O
conditional	B
average	O
of	O
the	O
target	O
data	O
the	O
quantity	O
tn	O
is	O
then	O
a	O
random	O
variable	O
with	O
zero	O
mean	B
if	O
we	O
assume	O
that	O
its	O
value	O
is	O
uncorrelated	O
with	O
the	O
value	O
of	O
the	O
second	O
derivative	B
term	O
on	O
the	O
right-hand	O
side	O
of	O
then	O
the	O
whole	O
term	O
will	O
average	O
to	O
zero	O
in	O
the	O
summation	O
over	O
n	O
by	O
neglecting	O
the	O
second	O
term	O
in	O
we	O
arrive	O
at	O
the	O
levenberg	O
marquardt	O
approximation	O
or	O
outer	B
product	I
approximation	I
the	O
hessian	B
matrix	I
is	O
built	O
up	O
from	O
a	O
sum	O
of	O
outer	O
products	O
of	O
vectors	O
given	O
by	O
bnbt	O
n	O
where	O
bn	O
yn	O
an	O
because	O
the	O
activation	B
function	I
for	O
the	O
output	O
units	O
is	O
simply	O
the	O
identity	O
evaluation	O
of	O
the	O
outer	B
product	I
approximation	I
for	O
the	O
hessian	O
is	O
straightforward	O
as	O
it	O
only	O
involves	O
first	O
derivatives	O
of	O
the	O
error	B
function	I
which	O
can	O
be	O
evaluated	O
efficiently	O
in	O
ow	O
steps	O
using	O
standard	O
backpropagation	B
the	O
elements	O
of	O
the	O
matrix	O
can	O
then	O
be	O
found	O
in	O
ow	O
steps	O
by	O
simple	O
multiplication	O
it	O
is	O
important	O
to	O
emphasize	O
that	O
this	O
approximation	O
is	O
only	O
likely	O
to	O
be	O
valid	O
for	O
a	O
network	O
that	O
has	O
been	O
trained	O
appropriately	O
and	O
that	O
for	O
a	O
general	O
network	O
mapping	O
the	O
second	O
derivative	B
terms	O
on	O
the	O
right-hand	O
side	O
of	O
will	O
typically	O
not	O
be	O
negligible	O
in	O
the	O
case	O
of	O
the	O
cross-entropy	B
error	B
function	I
for	O
a	O
network	O
with	O
logistic	B
sigmoid	I
output-unit	O
activation	O
functions	O
the	O
corresponding	O
approximation	O
is	O
given	O
by	O
h	O
h	O
neural	O
networks	O
inverse	B
hessian	O
we	O
can	O
use	O
the	O
outer-product	O
approximation	O
to	O
develop	O
a	O
computationally	O
efficient	O
procedure	O
for	O
approximating	O
the	O
inverse	B
of	O
the	O
hessian	O
and	O
stork	O
first	O
we	O
write	O
the	O
outer-product	O
approximation	O
in	O
matrix	O
notation	O
as	O
bnbt	O
n	O
hn	O
where	O
bn	O
wan	O
is	O
the	O
contribution	O
to	O
the	O
gradient	O
of	O
the	O
output	O
unit	O
activation	O
arising	O
from	O
data	O
point	O
n	O
we	O
now	O
derive	O
a	O
sequential	O
procedure	O
for	O
building	O
up	O
the	O
hessian	O
by	O
including	O
data	O
points	O
one	O
at	O
a	O
time	O
suppose	O
we	O
have	O
already	O
obtained	O
the	O
inverse	B
hessian	O
using	O
the	O
first	O
l	O
data	O
points	O
by	O
separating	O
off	O
the	O
contribution	O
from	O
data	O
point	O
l	O
we	O
obtain	O
in	O
order	O
to	O
evaluate	O
the	O
inverse	B
of	O
the	O
hessian	O
we	O
now	O
consider	O
the	O
matrix	O
identity	O
m	O
vvt	O
hl	O
m	O
vtm	O
vtm	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
which	O
is	O
simply	O
a	O
special	O
case	O
of	O
the	O
woodbury	B
identity	I
if	O
we	O
now	O
identify	O
hl	O
with	O
m	O
and	O
with	O
v	O
we	O
obtain	O
h	O
h	O
l	O
l	O
h	O
l	O
l	O
bt	O
exercise	O
in	O
this	O
way	O
data	O
points	O
are	O
sequentially	O
absorbed	O
until	O
n	O
and	O
the	O
whole	O
data	O
set	O
has	O
been	O
processed	O
this	O
result	O
therefore	O
represents	O
a	O
procedure	O
for	O
evaluating	O
the	O
inverse	B
of	O
the	O
hessian	O
using	O
a	O
single	O
pass	O
through	O
the	O
data	O
set	O
the	O
initial	O
matrix	O
is	O
chosen	O
to	O
be	O
i	O
where	O
is	O
a	O
small	O
quantity	O
so	O
that	O
the	O
algorithm	O
actually	O
finds	O
the	O
inverse	B
of	O
h	O
i	O
the	O
results	O
are	O
not	O
particularly	O
sensitive	O
to	O
the	O
precise	O
value	O
of	O
extension	O
of	O
this	O
algorithm	O
to	O
networks	O
having	O
more	O
than	O
one	O
output	O
is	O
straightforward	O
we	O
note	O
here	O
that	O
the	O
hessian	B
matrix	I
can	O
sometimes	O
be	O
calculated	O
indirectly	O
as	O
part	O
of	O
the	O
network	O
training	B
algorithm	O
in	O
particular	O
quasi-newton	O
nonlinear	O
optimization	O
algorithms	O
gradually	O
build	O
up	O
an	O
approximation	O
to	O
the	O
inverse	B
of	O
the	O
hessian	O
during	O
training	B
such	O
algorithms	O
are	O
discussed	O
in	O
detail	O
in	O
bishop	O
and	O
nabney	O
finite	O
differences	O
as	O
in	O
the	O
case	O
of	O
the	O
first	O
derivatives	O
of	O
the	O
error	B
function	I
we	O
can	O
find	O
the	O
second	O
derivatives	O
by	O
using	O
finite	O
differences	O
with	O
accuracy	O
limited	O
by	O
numerical	O
precision	O
if	O
we	O
perturb	O
each	O
possible	O
pair	O
of	O
weights	O
in	O
turn	O
we	O
obtain	O
wji	O
wlk	O
wlk	O
ewji	O
wlk	O
ewji	O
wlk	O
ewji	O
wlk	O
the	O
hessian	B
matrix	I
again	O
by	O
using	O
a	O
symmetrical	O
central	B
differences	I
formulation	O
we	O
ensure	O
that	O
the	O
residual	O
errors	O
are	O
rather	O
than	O
o	O
because	O
there	O
are	O
w	O
elements	O
in	O
the	O
hessian	B
matrix	I
and	O
because	O
the	O
evaluation	O
of	O
each	O
element	O
requires	O
four	O
forward	O
propagations	O
each	O
needing	O
ow	O
operations	O
pattern	O
we	O
see	O
that	O
this	O
approach	O
will	O
require	O
ow	O
operations	O
to	O
evaluate	O
the	O
complete	O
hessian	O
it	O
therefore	O
has	O
poor	O
scaling	O
properties	O
although	O
in	O
practice	O
it	O
is	O
very	O
useful	O
as	O
a	O
check	O
on	O
the	O
software	O
implementation	O
of	O
backpropagation	B
methods	O
a	O
more	O
efficient	O
version	O
of	O
numerical	O
differentiation	O
can	O
be	O
found	O
by	O
applying	O
central	B
differences	I
to	O
the	O
first	O
derivatives	O
of	O
the	O
error	B
function	I
which	O
are	O
themselves	O
calculated	O
using	O
backpropagation	B
this	O
gives	O
wji	O
wlk	O
e	O
wji	O
e	O
wji	O
because	O
there	O
are	O
now	O
only	O
w	O
weights	O
to	O
be	O
perturbed	O
and	O
because	O
the	O
gradients	O
can	O
be	O
evaluated	O
in	O
ow	O
steps	O
we	O
see	O
that	O
this	O
method	O
gives	O
the	O
hessian	O
in	O
ow	O
operations	O
exact	B
evaluation	I
of	O
the	O
hessian	O
so	O
far	O
we	O
have	O
considered	O
various	O
approximation	O
schemes	O
for	O
evaluating	O
the	O
hessian	B
matrix	I
or	O
its	O
inverse	B
the	O
hessian	O
can	O
also	O
be	O
evaluated	O
exactly	O
for	O
a	O
network	O
of	O
arbitrary	O
feed-forward	O
topology	O
using	O
extension	O
of	O
the	O
technique	O
of	O
backpropagation	B
used	O
to	O
evaluate	O
first	O
derivatives	O
which	O
shares	O
many	O
of	O
its	O
desirable	O
features	O
including	O
computational	O
efficiency	O
bishop	O
it	O
can	O
be	O
applied	O
to	O
any	O
differentiable	O
error	B
function	I
that	O
can	O
be	O
expressed	O
as	O
a	O
function	O
of	O
the	O
network	O
outputs	O
and	O
to	O
networks	O
having	O
arbitrary	O
differentiable	O
activation	O
functions	O
the	O
number	O
of	O
computational	O
steps	O
needed	O
to	O
evaluate	O
the	O
hessian	O
scales	O
like	O
ow	O
similar	O
algorithms	O
have	O
also	O
been	O
considered	O
by	O
buntine	O
and	O
weigend	O
here	O
we	O
consider	O
the	O
specific	O
case	O
of	O
a	O
network	O
having	O
two	O
layers	O
of	O
weights	O
for	O
which	O
the	O
required	O
equations	O
are	O
easily	O
derived	O
we	O
shall	O
use	O
indices	O
i	O
and	O
i	O
to	O
to	O
denote	O
inputs	O
indices	O
j	O
and	O
j	O
denote	O
outputs	O
we	O
first	O
define	O
to	O
denoted	O
hidden	O
units	O
and	O
indices	O
k	O
and	O
k	O
k	O
en	O
ak	O
ak	O
where	O
en	O
is	O
the	O
contribution	O
to	O
the	O
error	B
from	O
data	O
point	O
n	O
the	O
hessian	B
matrix	I
for	O
this	O
network	O
can	O
then	O
be	O
considered	O
in	O
three	O
separate	O
blocks	O
as	O
follows	O
both	O
weights	O
in	O
the	O
second	O
layer	O
exercise	O
kj	O
w	O
w	O
neural	O
networks	O
both	O
weights	O
in	O
the	O
first	O
layer	O
w	O
ji	O
w	O
one	O
weight	O
in	O
each	O
layer	O
ji	O
w	O
w	O
xih	O
zj	O
w	O
k	O
k	O
k	O
w	O
kj	O
w	O
exercise	O
here	O
is	O
the	O
j	O
j	O
element	O
of	O
the	O
identity	O
matrix	O
if	O
one	O
or	O
both	O
of	O
the	O
weights	O
is	O
a	O
bias	B
term	O
then	O
the	O
corresponding	O
expressions	O
are	O
obtained	O
simply	O
by	O
setting	O
the	O
appropriate	O
activations	O
to	O
inclusion	O
of	O
skip-layer	O
connections	O
is	O
straightforward	O
fast	B
multiplication	I
by	O
the	O
hessian	O
for	O
many	O
applications	O
of	O
the	O
hessian	O
the	O
quantity	O
of	O
interest	O
is	O
not	O
the	O
hessian	B
matrix	I
h	O
itself	O
but	O
the	O
product	O
of	O
h	O
with	O
some	O
vector	O
v	O
we	O
have	O
seen	O
that	O
the	O
evaluation	O
of	O
the	O
hessian	O
takes	O
ow	O
operations	O
and	O
it	O
also	O
requires	O
storage	O
that	O
is	O
ow	O
the	O
vector	O
vth	O
that	O
we	O
wish	O
to	O
calculate	O
however	O
has	O
only	O
w	O
elements	O
so	O
instead	O
of	O
computing	O
the	O
hessian	O
as	O
an	O
intermediate	O
step	O
we	O
can	O
instead	O
try	O
to	O
find	O
an	O
efficient	O
approach	O
to	O
evaluating	O
vth	O
directly	O
in	O
a	O
way	O
that	O
requires	O
only	O
ow	O
operations	O
to	O
do	O
this	O
we	O
first	O
note	O
that	O
vth	O
vt	O
e	O
where	O
denotes	O
the	O
gradient	O
operator	O
in	O
weight	O
space	O
we	O
can	O
then	O
write	O
down	O
the	O
standard	O
forward-propagation	O
and	O
backpropagation	B
equations	O
for	O
the	O
evaluation	O
of	O
e	O
and	O
apply	O
to	O
these	O
equations	O
to	O
give	O
a	O
set	O
of	O
forward-propagation	O
and	O
backpropagation	B
equations	O
for	O
the	O
evaluation	O
of	O
vth	O
ller	O
pearlmutter	O
this	O
corresponds	O
to	O
acting	O
on	O
the	O
original	O
forward-propagation	O
and	O
backpropagation	B
equations	O
with	O
a	O
differential	B
operator	O
vt	O
pearlmutter	O
used	O
the	O
notation	O
r	O
to	O
denote	O
the	O
operator	O
vt	O
and	O
we	O
shall	O
follow	O
this	O
convention	O
the	O
analysis	O
is	O
straightforward	O
and	O
makes	O
use	O
of	O
the	O
usual	O
rules	O
of	O
differential	B
calculus	O
together	O
with	O
the	O
result	O
rw	O
v	O
the	O
technique	O
is	O
best	O
illustrated	O
with	O
a	O
simple	O
example	O
and	O
again	O
we	O
choose	O
a	O
two-layer	O
network	O
of	O
the	O
form	O
shown	O
in	O
figure	O
with	O
linear	O
output	O
units	O
and	O
a	O
sum-of-squares	B
error	B
function	I
as	O
before	O
we	O
consider	O
the	O
contribution	O
to	O
the	O
error	B
function	I
from	O
one	O
pattern	O
in	O
the	O
data	O
set	O
the	O
required	O
vector	O
is	O
then	O
obtained	O
as	O
the	O
hessian	B
matrix	I
usual	O
by	O
summing	O
over	O
the	O
contributions	O
from	O
each	O
of	O
the	O
patterns	O
separately	O
for	O
the	O
two-layer	O
network	O
the	O
forward-propagation	O
equations	O
are	O
given	O
by	O
i	O
j	O
aj	O
wjixi	O
zj	O
haj	O
yk	O
wkjzj	O
we	O
now	O
act	O
on	O
these	O
equations	O
using	O
the	O
r	O
operator	O
to	O
obtain	O
a	O
set	O
of	O
forward	B
propagation	I
equations	O
in	O
the	O
form	O
raj	O
rzj	O
h	O
ryk	O
wkjrzj	O
vkjzj	O
vjixi	O
i	O
j	O
j	O
where	O
vji	O
is	O
the	O
element	O
of	O
the	O
vector	O
v	O
that	O
corresponds	O
to	O
the	O
weight	O
wji	O
quantities	O
of	O
the	O
form	O
rzj	O
raj	O
and	O
ryk	O
are	O
to	O
be	O
regarded	O
as	O
new	O
variables	O
whose	O
values	O
are	O
found	O
using	O
the	O
above	O
equations	O
because	O
we	O
are	O
considering	O
a	O
sum-of-squares	B
error	B
function	I
we	O
have	O
the	O
fol	O
lowing	O
standard	O
backpropagation	B
expressions	O
k	O
yk	O
tk	O
j	O
h	O
wkj	O
k	O
again	O
we	O
act	O
on	O
these	O
equations	O
with	O
the	O
r	O
operator	O
to	O
obtain	O
a	O
set	O
of	O
backpropagation	B
equations	O
in	O
the	O
form	O
k	O
r	O
k	O
ryk	O
r	O
j	O
h	O
wkj	O
k	O
k	O
vkj	O
k	O
h	O
h	O
k	O
wkjr	O
k	O
k	O
finally	O
we	O
have	O
the	O
usual	O
equations	O
for	O
the	O
first	O
derivatives	O
of	O
the	O
error	B
e	O
wkj	O
e	O
wji	O
kzj	O
jxi	O
neural	O
networks	O
and	O
acting	O
on	O
these	O
with	O
the	O
r	O
operator	O
we	O
obtain	O
expressions	O
for	O
the	O
elements	O
of	O
the	O
vector	O
vth	O
r	O
kzj	O
krzj	O
xir	O
j	O
r	O
r	O
e	O
wkj	O
e	O
wji	O
the	O
implementation	O
of	O
this	O
algorithm	O
involves	O
the	O
introduction	O
of	O
additional	O
variables	O
raj	O
rzj	O
and	O
r	O
j	O
for	O
the	O
hidden	O
units	O
and	O
r	O
k	O
and	O
ryk	O
for	O
the	O
output	O
units	O
for	O
each	O
input	O
pattern	O
the	O
values	O
of	O
these	O
quantities	O
can	O
be	O
found	O
using	O
the	O
above	O
results	O
and	O
the	O
elements	O
of	O
vth	O
are	O
then	O
given	O
by	O
and	O
an	O
elegant	O
aspect	O
of	O
this	O
technique	O
is	O
that	O
the	O
equations	O
for	O
evaluating	O
vth	O
mirror	O
closely	O
those	O
for	O
standard	O
forward	O
and	O
backward	O
propagation	O
and	O
so	O
the	O
extension	O
of	O
existing	O
software	O
to	O
compute	O
this	O
product	O
is	O
typically	O
straightforward	O
if	O
desired	O
the	O
technique	O
can	O
be	O
used	O
to	O
evaluate	O
the	O
full	O
hessian	B
matrix	I
by	O
choosing	O
the	O
vector	O
v	O
to	O
be	O
given	O
successively	O
by	O
a	O
series	O
of	O
unit	O
vectors	O
of	O
the	O
form	O
each	O
of	O
which	O
picks	O
out	O
one	O
column	O
of	O
the	O
hessian	O
this	O
leads	O
to	O
a	O
formalism	O
that	O
is	O
analytically	O
equivalent	O
to	O
the	O
backpropagation	B
procedure	O
of	O
bishop	O
as	O
described	O
in	O
section	O
though	O
with	O
some	O
loss	O
of	O
efficiency	O
due	O
to	O
redundant	O
calculations	O
regularization	B
in	O
neural	O
networks	O
the	O
number	O
of	O
input	O
and	O
outputs	O
units	O
in	O
a	O
neural	B
network	I
is	O
generally	O
determined	O
by	O
the	O
dimensionality	O
of	O
the	O
data	O
set	O
whereas	O
the	O
number	O
m	O
of	O
hidden	O
units	O
is	O
a	O
free	O
parameter	O
that	O
can	O
be	O
adjusted	O
to	O
give	O
the	O
best	O
predictive	O
performance	O
note	O
that	O
m	O
controls	O
the	O
number	O
of	O
parameters	O
and	O
biases	O
in	O
the	O
network	O
and	O
so	O
we	O
might	O
expect	O
that	O
in	O
a	O
maximum	B
likelihood	I
setting	O
there	O
will	O
be	O
an	O
optimum	O
value	O
of	O
m	O
that	O
gives	O
the	O
best	O
generalization	B
performance	O
corresponding	O
to	O
the	O
optimum	O
balance	O
between	O
under-fitting	O
and	O
over-fitting	B
figure	O
shows	O
an	O
example	O
of	O
the	O
effect	O
of	O
different	O
values	O
of	O
m	O
for	O
the	O
sinusoidal	O
regression	B
problem	O
the	O
generalization	B
error	B
however	O
is	O
not	O
a	O
simple	O
function	O
of	O
m	O
due	O
to	O
the	O
presence	O
of	O
local	B
minima	O
in	O
the	O
error	B
function	I
as	O
illustrated	O
in	O
figure	O
here	O
we	O
see	O
the	O
effect	O
of	O
choosing	O
multiple	O
random	O
initializations	O
for	O
the	O
weight	B
vector	I
for	O
a	O
range	O
of	O
values	O
of	O
m	O
the	O
overall	O
best	O
validation	B
set	I
performance	O
in	O
this	O
case	O
occurred	O
for	O
a	O
particular	O
solution	O
having	O
m	O
in	O
practice	O
one	O
approach	O
to	O
choosing	O
m	O
is	O
in	O
fact	O
to	O
plot	O
a	O
graph	O
of	O
the	O
kind	O
shown	O
in	O
figure	O
and	O
then	O
to	O
choose	O
the	O
specific	O
solution	O
having	O
the	O
smallest	O
validation	B
set	I
error	B
there	O
are	O
however	O
other	O
ways	O
to	O
control	O
the	O
complexity	O
of	O
a	O
neural	B
network	I
model	O
in	O
order	O
to	O
avoid	O
over-fitting	B
from	O
our	O
discussion	O
of	O
polynomial	B
curve	B
fitting	I
in	O
chapter	O
we	O
see	O
that	O
an	O
alternative	O
approach	O
is	O
to	O
choose	O
a	O
relatively	O
large	O
value	O
for	O
m	O
and	O
then	O
to	O
control	O
complexity	O
by	O
the	O
addition	O
of	O
a	O
regularization	B
term	O
to	O
the	O
error	B
function	I
the	O
simplest	O
regularizer	O
is	O
the	O
quadratic	O
giving	O
a	O
regularized	O
error	B
regularization	B
in	O
neural	O
networks	O
m	O
m	O
m	O
figure	O
examples	O
of	O
two-layer	O
networks	O
trained	O
on	O
data	O
points	O
drawn	O
from	O
the	O
sinusoidal	B
data	I
set	O
the	O
graphs	O
show	O
the	O
result	O
of	O
fitting	O
networks	O
having	O
m	O
and	O
hidden	O
units	O
respectively	O
by	O
minimizing	O
a	O
sum-of-squares	B
error	B
function	I
using	O
a	O
scaled	O
conjugate-gradient	O
algorithm	O
of	O
the	O
form	O
ew	O
wtw	O
this	O
regularizer	O
is	O
also	O
known	O
as	O
weight	B
decay	I
and	O
has	O
been	O
discussed	O
at	O
length	O
in	O
chapter	O
the	O
effective	O
model	O
complexity	O
is	O
then	O
determined	O
by	O
the	O
choice	O
of	O
the	O
regularization	B
coefficient	O
as	O
we	O
have	O
seen	O
previously	O
this	O
regularizer	O
can	O
be	O
interpreted	O
as	O
the	O
negative	O
logarithm	O
of	O
a	O
zero-mean	O
gaussian	B
prior	B
distribution	O
over	O
the	O
weight	B
vector	I
w	O
consistent	B
gaussian	B
priors	O
one	O
of	O
the	O
limitations	O
of	O
simple	O
weight	B
decay	I
in	O
the	O
form	O
is	O
that	O
is	O
inconsistent	O
with	O
certain	O
scaling	O
properties	O
of	O
network	O
mappings	O
to	O
illustrate	O
this	O
consider	O
a	O
multilayer	B
perceptron	B
network	O
having	O
two	O
layers	O
of	O
weights	O
and	O
linear	O
output	O
units	O
which	O
performs	O
a	O
mapping	O
from	O
a	O
set	O
of	O
input	O
variables	O
to	O
a	O
set	O
of	O
output	O
variables	O
the	O
activations	O
of	O
the	O
hidden	O
units	O
in	O
the	O
first	O
hidden	O
layer	O
figure	O
plot	O
of	O
the	O
sum-of-squares	O
test-set	O
error	B
for	O
the	O
polynomial	O
data	O
set	O
versus	O
the	O
number	O
of	O
hidden	O
units	O
in	O
the	O
network	O
with	O
random	O
starts	O
for	O
each	O
network	O
size	O
showing	O
the	O
effect	O
of	O
local	B
minima	O
for	O
each	O
new	O
start	O
the	O
weight	B
vector	I
was	O
initialized	O
by	O
sampling	O
from	O
an	O
isotropic	B
gaussian	B
distribution	O
having	O
a	O
mean	B
of	O
zero	O
and	O
a	O
variance	B
of	O
neural	O
networks	O
take	O
the	O
form	O
zj	O
h	O
wjixi	O
while	O
the	O
activations	O
of	O
the	O
output	O
units	O
are	O
given	O
by	O
i	O
yk	O
wkjzj	O
suppose	O
we	O
perform	O
a	O
linear	O
transformation	O
of	O
the	O
input	O
data	O
of	O
the	O
form	O
j	O
xi	O
axi	O
b	O
exercise	O
then	O
we	O
can	O
arrange	O
for	O
the	O
mapping	O
performed	O
by	O
the	O
network	O
to	O
be	O
unchanged	O
by	O
making	O
a	O
corresponding	O
linear	O
transformation	O
of	O
the	O
weights	O
and	O
biases	O
from	O
the	O
inputs	O
to	O
the	O
units	O
in	O
the	O
hidden	O
layer	O
of	O
the	O
form	O
wji	O
a	O
a	O
wji	O
wji	O
b	O
yk	O
cyk	O
d	O
wkj	O
cwkj	O
d	O
i	O
similarly	O
a	O
linear	O
transformation	O
of	O
the	O
output	O
variables	O
of	O
the	O
network	O
of	O
the	O
form	O
can	O
be	O
achieved	O
by	O
making	O
a	O
transformation	O
of	O
the	O
second-layer	O
weights	O
and	O
biases	O
using	O
if	O
we	O
train	O
one	O
network	O
using	O
the	O
original	O
data	O
and	O
one	O
network	O
using	O
data	O
for	O
which	O
the	O
input	O
andor	O
target	O
variables	O
are	O
transformed	O
by	O
one	O
of	O
the	O
above	O
linear	O
transformations	O
then	O
consistency	O
requires	O
that	O
we	O
should	O
obtain	O
equivalent	O
networks	O
that	O
differ	O
only	O
by	O
the	O
linear	O
transformation	O
of	O
the	O
weights	O
as	O
given	O
any	O
regularizer	O
should	O
be	O
consistent	B
with	O
this	O
property	O
otherwise	O
it	O
arbitrarily	O
favours	O
one	O
solution	O
over	O
another	O
equivalent	O
one	O
clearly	O
simple	O
weight	B
decay	I
that	O
treats	O
all	O
weights	O
and	O
biases	O
on	O
an	O
equal	O
footing	O
does	O
not	O
satisfy	O
this	O
property	O
we	O
therefore	O
look	O
for	O
a	O
regularizer	O
which	O
is	O
invariant	O
under	O
the	O
linear	O
transformations	O
and	O
these	O
require	O
that	O
the	O
regularizer	O
should	O
be	O
invariant	O
to	O
re-scaling	O
of	O
the	O
weights	O
and	O
to	O
shifts	O
of	O
the	O
biases	O
such	O
a	O
regularizer	O
is	O
given	O
by	O
where	O
denotes	O
the	O
set	O
of	O
weights	O
in	O
the	O
first	O
layer	O
denotes	O
the	O
set	O
of	O
weights	O
in	O
the	O
second	O
layer	O
and	O
biases	O
are	O
excluded	O
from	O
the	O
summations	O
this	O
regularizer	O
w	O
w	O
pw	O
exp	O
k	O
k	O
j	O
j	O
wk	O
regularization	B
in	O
neural	O
networks	O
will	O
remain	O
unchanged	O
under	O
the	O
weight	O
transformations	O
provided	O
the	O
regularization	B
parameters	O
are	O
re-scaled	O
using	O
and	O
c	O
the	O
regularizer	O
corresponds	O
to	O
a	O
prior	B
of	O
the	O
form	O
pw	O
exp	O
w	O
w	O
note	O
that	O
priors	O
of	O
this	O
form	O
are	O
improper	B
cannot	O
be	O
normalized	O
because	O
the	O
bias	B
parameters	O
are	O
unconstrained	O
the	O
use	O
of	O
improper	B
priors	O
can	O
lead	O
to	O
difficulties	O
in	O
selecting	O
regularization	B
coefficients	O
and	O
in	O
model	B
comparison	I
within	O
the	O
bayesian	B
framework	O
because	O
the	O
corresponding	O
evidence	O
is	O
zero	O
it	O
is	O
therefore	O
common	O
to	O
include	O
separate	O
priors	O
for	O
the	O
biases	O
then	O
break	O
shift	O
invariance	B
having	O
their	O
own	O
hyperparameters	O
we	O
can	O
illustrate	O
the	O
effect	O
of	O
the	O
resulting	O
four	O
hyperparameters	O
by	O
drawing	O
samples	O
from	O
the	O
prior	B
and	O
plotting	O
the	O
corresponding	O
network	O
functions	O
as	O
shown	O
in	O
figure	O
any	O
number	O
of	O
groups	O
wk	O
so	O
that	O
more	O
generally	O
we	O
can	O
consider	O
priors	O
in	O
which	O
the	O
weights	O
are	O
divided	O
into	O
where	O
k	O
as	O
a	O
special	O
case	O
of	O
this	O
prior	B
if	O
we	O
choose	O
the	O
groups	O
to	O
correspond	O
to	O
the	O
sets	O
of	O
weights	O
associated	O
with	O
each	O
of	O
the	O
input	O
units	O
and	O
we	O
optimize	O
the	O
marginal	B
likelihood	I
with	O
respect	O
to	O
the	O
corresponding	O
parameters	O
k	O
we	O
obtain	O
automatic	B
relevance	I
determination	I
as	O
discussed	O
in	O
section	O
early	B
stopping	I
an	O
alternative	O
to	O
regularization	B
as	O
a	O
way	O
of	O
controlling	O
the	O
effective	O
complexity	O
of	O
a	O
network	O
is	O
the	O
procedure	O
of	O
early	B
stopping	I
the	O
training	B
of	O
nonlinear	O
network	O
models	O
corresponds	O
to	O
an	O
iterative	O
reduction	O
of	O
the	O
error	B
function	I
defined	O
with	O
respect	O
to	O
a	O
set	O
of	O
training	B
data	O
for	O
many	O
of	O
the	O
optimization	O
algorithms	O
used	O
for	O
network	O
training	B
such	O
as	O
conjugate	B
gradients	O
the	O
error	B
is	O
a	O
nonincreasing	O
function	O
of	O
the	O
iteration	O
index	O
however	O
the	O
error	B
measured	O
with	O
respect	O
to	O
independent	B
data	O
generally	O
called	O
a	O
validation	B
set	I
often	O
shows	O
a	O
decrease	O
at	O
first	O
followed	O
by	O
an	O
increase	O
as	O
the	O
network	O
starts	O
to	O
over-fit	O
training	B
can	O
therefore	O
be	O
stopped	O
at	O
the	O
point	O
of	O
smallest	O
error	B
with	O
respect	O
to	O
the	O
validation	O
data	O
set	O
as	O
indicated	O
in	O
figure	O
in	O
order	O
to	O
obtain	O
a	O
network	O
having	O
good	O
generalization	B
performance	O
the	O
behaviour	O
of	O
the	O
network	O
in	O
this	O
case	O
is	O
sometimes	O
explained	O
qualitatively	O
in	O
terms	O
of	O
the	O
effective	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
network	O
in	O
which	O
this	O
number	O
starts	O
out	O
small	O
and	O
then	O
to	O
grows	O
during	O
the	O
training	B
process	O
corresponding	O
to	O
a	O
steady	O
increase	O
in	O
the	O
effective	O
complexity	O
of	O
the	O
model	O
halting	O
training	B
before	O
neural	O
networks	O
w	O
b	O
w	O
b	O
w	O
b	O
w	O
b	O
w	O
b	O
w	O
b	O
w	O
b	O
w	O
b	O
figure	O
illustration	O
of	O
the	O
effect	O
of	O
the	O
hyperparameters	O
governing	O
the	O
prior	B
distribution	O
over	O
weights	O
and	O
biases	O
in	O
a	O
two-layer	O
network	O
having	O
a	O
single	O
input	O
a	O
single	O
linear	O
output	O
and	O
hidden	O
units	O
having	O
tanh	O
activation	O
functions	O
the	O
priors	O
are	O
governed	O
by	O
four	O
hyperparameters	O
b	O
which	O
represent	O
the	O
precisions	O
of	O
the	O
gaussian	B
distributions	O
of	O
the	O
first-layer	O
biases	O
first-layer	O
weights	O
second-layer	O
biases	O
and	O
second-layer	O
weights	O
respectively	O
we	O
see	O
that	O
the	O
parameter	O
w	O
governs	O
the	O
vertical	O
scale	O
of	O
functions	O
the	O
different	O
vertical	O
axis	O
ranges	O
on	O
the	O
top	O
two	O
diagrams	O
w	O
governs	O
the	O
horizontal	O
scale	O
of	O
variations	O
in	O
the	O
function	O
values	O
and	O
b	O
whose	O
effect	O
is	O
not	O
illustrated	O
here	O
governs	O
the	O
range	O
of	O
vertical	O
offsets	O
of	O
the	O
functions	O
governs	O
the	O
horizontal	O
range	O
over	O
which	O
variations	O
occur	O
the	O
parameter	O
b	O
and	O
w	O
w	O
b	O
a	O
minimum	O
of	O
the	O
training	B
error	B
has	O
been	O
reached	O
then	O
represents	O
a	O
way	O
of	O
limiting	O
the	O
effective	O
network	O
complexity	O
in	O
the	O
case	O
of	O
a	O
quadratic	O
error	B
function	I
we	O
can	O
verify	O
this	O
insight	O
and	O
show	O
that	O
early	B
stopping	I
should	O
exhibit	O
similar	O
behaviour	O
to	O
regularization	B
using	O
a	O
simple	O
weight-decay	O
term	O
this	O
can	O
be	O
understood	O
from	O
figure	O
in	O
which	O
the	O
axes	O
in	O
weight	O
space	O
have	O
been	O
rotated	O
to	O
be	O
parallel	O
to	O
the	O
eigenvectors	O
of	O
the	O
hessian	B
matrix	I
if	O
in	O
the	O
absence	O
of	O
weight	B
decay	I
the	O
weight	B
vector	I
starts	O
at	O
the	O
origin	O
and	O
proceeds	O
during	O
training	B
along	O
a	O
path	O
that	O
follows	O
the	O
local	B
negative	O
gradient	O
vector	O
then	O
the	O
weight	B
vector	I
will	O
move	O
initially	O
parallel	O
to	O
the	O
axis	O
through	O
a	O
point	O
corresponding	O
roughly	O
to	O
and	O
then	O
move	O
towards	O
the	O
minimum	O
of	O
the	O
error	B
funceigenvalues	O
of	O
the	O
hessian	O
stopping	O
at	O
a	O
point	O
is	O
therefore	O
similar	O
to	O
weight	O
tion	O
wml	O
this	O
follows	O
from	O
the	O
shape	O
of	O
the	O
error	B
surface	O
and	O
the	O
widely	O
differing	O
exercise	O
decay	O
the	O
relationship	O
between	O
early	B
stopping	I
and	O
weight	B
decay	I
can	O
be	O
made	O
quantitative	O
thereby	O
showing	O
that	O
the	O
quantity	O
is	O
the	O
iteration	O
index	O
and	O
is	O
the	O
learning	B
rate	I
parameter	I
plays	O
the	O
role	O
of	O
the	O
reciprocal	O
of	O
the	O
regularization	B
regularization	B
in	O
neural	O
networks	O
figure	O
an	O
illustration	O
of	O
the	O
behaviour	O
of	O
training	B
set	I
error	B
and	O
validation	B
set	I
error	B
during	O
a	O
typical	O
training	B
session	O
as	O
a	O
function	O
of	O
the	O
iteration	O
step	O
for	O
the	O
sinusoidal	B
data	I
set	O
the	O
goal	O
of	O
achieving	O
the	O
best	O
generalization	B
performance	O
suggests	O
that	O
training	B
should	O
be	O
stopped	O
at	O
the	O
point	O
shown	O
by	O
the	O
vertical	O
dashed	O
lines	O
corresponding	O
to	O
the	O
minimum	O
of	O
the	O
validation	B
set	I
error	B
parameter	O
the	O
effective	B
number	I
of	I
parameters	I
in	O
the	O
network	O
therefore	O
grows	O
during	O
the	O
course	O
of	O
training	B
invariances	O
in	O
many	O
applications	O
of	O
pattern	O
recognition	O
it	O
is	O
known	O
that	O
predictions	O
should	O
be	O
unchanged	O
or	O
invariant	O
under	O
one	O
or	O
more	O
transformations	O
of	O
the	O
input	O
variables	O
for	O
example	O
in	O
the	O
classification	B
of	O
objects	O
in	O
two-dimensional	O
images	O
such	O
as	O
handwritten	O
digits	O
a	O
particular	O
object	O
should	O
be	O
assigned	O
the	O
same	O
classification	B
irrespective	O
of	O
its	O
position	O
within	O
the	O
image	O
invariance	B
or	O
of	O
its	O
size	O
invariance	B
such	O
transformations	O
produce	O
significant	O
changes	O
in	O
the	O
raw	O
data	O
expressed	O
in	O
terms	O
of	O
the	O
intensities	O
at	O
each	O
of	O
the	O
pixels	O
in	O
the	O
image	O
and	O
yet	O
should	O
give	O
rise	O
to	O
the	O
same	O
output	O
from	O
the	O
classification	B
system	O
similarly	O
in	O
speech	B
recognition	I
small	O
levels	O
of	O
nonlinear	O
warping	O
along	O
the	O
time	O
axis	O
which	O
preserve	O
temporal	O
ordering	O
should	O
not	O
change	O
the	O
interpretation	O
of	O
the	O
signal	O
if	O
sufficiently	O
large	O
numbers	O
of	O
training	B
patterns	O
are	O
available	O
then	O
an	O
adaptive	O
model	O
such	O
as	O
a	O
neural	B
network	I
can	O
learn	O
the	O
invariance	B
at	O
least	O
approximately	O
this	O
involves	O
including	O
within	O
the	O
training	B
set	I
a	O
sufficiently	O
large	O
number	O
of	O
examples	O
of	O
the	O
effects	O
of	O
the	O
various	O
transformations	O
thus	O
for	O
translation	B
invariance	B
in	O
an	O
image	O
the	O
training	B
set	I
should	O
include	O
examples	O
of	O
objects	O
at	O
many	O
different	O
positions	O
this	O
approach	O
may	O
be	O
impractical	O
however	O
if	O
the	O
number	O
of	O
training	B
examples	O
is	O
limited	O
or	O
if	O
there	O
are	O
several	O
invariants	O
the	O
number	O
of	O
combinations	O
of	O
transformations	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
such	O
transformations	O
we	O
therefore	O
seek	O
alternative	O
approaches	O
for	O
encouraging	O
an	O
adaptive	O
model	O
to	O
exhibit	O
the	O
required	O
invariances	O
these	O
can	O
broadly	O
be	O
divided	O
into	O
four	O
categories	O
the	O
training	B
set	I
is	O
augmented	O
using	O
replicas	O
of	O
the	O
training	B
patterns	O
transformed	O
according	O
to	O
the	O
desired	O
invariances	O
for	O
instance	O
in	O
our	O
digit	O
recognition	O
example	O
we	O
could	O
make	O
multiple	O
copies	O
of	O
each	O
example	O
in	O
which	O
the	O
neural	O
networks	O
figure	O
a	O
schematic	O
illustration	O
of	O
why	O
early	B
stopping	I
can	O
give	O
similar	O
results	O
to	O
weight	B
decay	I
in	O
the	O
case	O
of	O
a	O
quadratic	O
error	B
function	I
the	O
ellipse	O
shows	O
a	O
contour	O
of	O
constant	O
error	B
and	O
wml	O
denotes	O
the	O
minimum	O
of	O
the	O
error	B
function	I
if	O
the	O
weight	B
vector	I
starts	O
at	O
the	O
origin	O
and	O
moves	O
according	O
to	O
the	O
local	B
negative	O
gradient	O
direction	O
then	O
it	O
will	O
follow	O
the	O
path	O
shown	O
by	O
the	O
curve	O
by	O
stopping	O
training	B
early	O
a	O
weight	B
vector	I
ew	O
is	O
found	O
that	O
is	O
qualitatively	O
similar	O
to	O
that	O
obtained	O
with	O
a	O
simple	O
weight-decay	O
regularizer	O
and	O
training	B
to	O
the	O
minimum	O
of	O
the	O
regularized	O
error	B
as	O
can	O
be	O
seen	O
by	O
comparing	O
with	O
figure	O
wml	O
digit	O
is	O
shifted	O
to	O
a	O
different	O
position	O
in	O
each	O
image	O
a	O
regularization	B
term	O
is	O
added	O
to	O
the	O
error	B
function	I
that	O
penalizes	O
changes	O
in	O
the	O
model	O
output	O
when	O
the	O
input	O
is	O
transformed	O
this	O
leads	O
to	O
the	O
technique	O
of	O
tangent	B
propagation	I
discussed	O
in	O
section	O
invariance	B
is	O
built	O
into	O
the	O
pre-processing	O
by	O
extracting	O
features	O
that	O
are	O
invariant	O
under	O
the	O
required	O
transformations	O
any	O
subsequent	O
regression	B
or	O
classification	B
system	O
that	O
uses	O
such	O
features	O
as	O
inputs	O
will	O
necessarily	O
also	O
respect	O
these	O
invariances	O
the	O
final	O
option	O
is	O
to	O
build	O
the	O
invariance	B
properties	O
into	O
the	O
structure	O
of	O
a	O
neural	B
network	I
into	O
the	O
definition	O
of	O
a	O
kernel	B
function	I
in	O
the	O
case	O
of	O
techniques	O
such	O
as	O
the	O
relevance	B
vector	I
machine	I
one	O
way	O
to	O
achieve	O
this	O
is	O
through	O
the	O
use	O
of	O
local	B
receptive	O
fields	O
and	O
shared	O
weights	O
as	O
discussed	O
in	O
the	O
context	O
of	O
convolutional	B
neural	O
networks	O
in	O
section	O
approach	O
is	O
often	O
relatively	O
easy	O
to	O
implement	O
and	O
can	O
be	O
used	O
to	O
encourage	O
complex	O
invariances	O
such	O
as	O
those	O
illustrated	O
in	O
figure	O
for	O
sequential	O
training	B
algorithms	O
this	O
can	O
be	O
done	O
by	O
transforming	O
each	O
input	O
pattern	O
before	O
it	O
is	O
presented	O
to	O
the	O
model	O
so	O
that	O
if	O
the	O
patterns	O
are	O
being	O
recycled	O
a	O
different	O
transformation	O
from	O
an	O
appropriate	O
distribution	O
is	O
added	O
each	O
time	O
for	O
batch	O
methods	O
a	O
similar	O
effect	O
can	O
be	O
achieved	O
by	O
replicating	O
each	O
data	O
point	O
a	O
number	O
of	O
times	O
and	O
transforming	O
each	O
copy	O
independently	O
the	O
use	O
of	O
such	O
augmented	O
data	O
can	O
lead	O
to	O
significant	O
improvements	O
in	O
generalization	B
et	O
al	O
although	O
it	O
can	O
also	O
be	O
computationally	O
costly	O
approach	O
leaves	O
the	O
data	O
set	O
unchanged	O
but	O
modifies	O
the	O
error	B
function	I
through	O
the	O
addition	O
of	O
a	O
regularizer	O
in	O
section	O
we	O
shall	O
show	O
that	O
this	O
approach	O
is	O
closely	O
related	O
to	O
approach	O
regularization	B
in	O
neural	O
networks	O
figure	O
illustration	O
of	O
the	O
synthetic	O
warping	O
of	O
a	O
handwritten	B
digit	I
the	O
original	O
image	O
is	O
shown	O
on	O
the	O
left	O
on	O
the	O
right	O
the	O
top	O
row	O
shows	O
three	O
examples	O
of	O
warped	O
digits	O
with	O
the	O
corresponding	O
displacement	O
fields	O
shown	O
on	O
the	O
bottom	O
row	O
these	O
displacement	O
fields	O
are	O
generated	O
by	O
sampling	O
random	O
displacements	O
x	O
y	O
at	O
each	O
pixel	O
and	O
then	O
smoothing	O
by	O
convolution	O
with	O
gaussians	O
of	O
width	O
and	O
respectively	O
one	O
advantage	O
of	O
approach	O
is	O
that	O
it	O
can	O
correctly	O
extrapolate	O
well	O
beyond	O
the	O
range	O
of	O
transformations	O
included	O
in	O
the	O
training	B
set	I
however	O
it	O
can	O
be	O
difficult	O
to	O
find	O
hand-crafted	O
features	O
with	O
the	O
required	O
invariances	O
that	O
do	O
not	O
also	O
discard	O
information	O
that	O
can	O
be	O
useful	O
for	O
discrimination	O
tangent	B
propagation	I
we	O
can	O
use	O
regularization	B
to	O
encourage	O
models	O
to	O
be	O
invariant	O
to	O
transformations	O
of	O
the	O
input	O
through	O
the	O
technique	O
of	O
tangent	B
propagation	I
et	O
al	O
consider	O
the	O
effect	O
of	O
a	O
transformation	O
on	O
a	O
particular	O
input	O
vector	O
xn	O
provided	O
the	O
transformation	O
is	O
continuous	O
as	O
translation	O
or	O
rotation	O
but	O
not	O
mirror	O
reflection	O
for	O
instance	O
then	O
the	O
transformed	O
pattern	O
will	O
sweep	O
out	O
a	O
manifold	B
m	O
within	O
the	O
d-dimensional	O
input	O
space	O
this	O
is	O
illustrated	O
in	O
figure	O
for	O
the	O
case	O
of	O
d	O
for	O
simplicity	O
suppose	O
the	O
transformation	O
is	O
governed	O
by	O
a	O
single	O
parameter	O
might	O
be	O
rotation	O
angle	O
for	O
instance	O
then	O
the	O
subspace	O
m	O
swept	O
out	O
by	O
xn	O
figure	O
illustration	O
of	O
a	O
two-dimensional	O
input	O
space	O
showing	O
the	O
effect	O
of	O
a	O
continuous	O
transformation	O
on	O
a	O
particular	O
input	O
vector	O
xn	O
a	O
onedimensional	O
transformation	O
parameterized	O
by	O
the	O
continuous	O
variable	O
applied	O
to	O
xn	O
causes	O
it	O
to	O
sweep	O
out	O
a	O
one-dimensional	O
manifold	B
m	O
locally	O
the	O
effect	O
of	O
the	O
transformation	O
can	O
be	O
approximated	O
by	O
the	O
tangent	O
vector	O
n	O
n	O
xn	O
m	O
neural	O
networks	O
will	O
be	O
one-dimensional	O
and	O
will	O
be	O
parameterized	O
by	O
let	O
the	O
vector	O
that	O
results	O
from	O
acting	O
on	O
xn	O
by	O
this	O
transformation	O
be	O
denoted	O
by	O
sxn	O
which	O
is	O
defined	O
so	O
that	O
sx	O
x	O
then	O
the	O
tangent	O
to	O
the	O
curve	O
m	O
is	O
given	O
by	O
the	O
directional	O
derivative	B
s	O
and	O
the	O
tangent	O
vector	O
at	O
the	O
point	O
xn	O
is	O
given	O
by	O
n	O
sxn	O
under	O
a	O
transformation	O
of	O
the	O
input	O
vector	O
the	O
network	O
output	O
vector	O
will	O
in	O
general	O
change	O
the	O
derivative	B
of	O
output	O
k	O
with	O
respect	O
to	O
is	O
given	O
by	O
yk	O
yk	O
xi	O
xi	O
jki	O
i	O
where	O
jki	O
is	O
the	O
i	O
element	O
of	O
the	O
jacobian	B
matrix	I
j	O
as	O
discussed	O
in	O
section	O
the	O
result	O
can	O
be	O
used	O
to	O
modify	O
the	O
standard	O
error	B
function	I
so	O
as	O
to	O
encourage	O
local	B
invariance	B
in	O
the	O
neighbourhood	O
of	O
the	O
data	O
points	O
by	O
the	O
addition	O
to	O
the	O
original	O
error	B
function	I
e	O
of	O
a	O
regularization	B
function	O
to	O
give	O
a	O
total	O
error	B
function	I
of	O
the	O
form	O
e	O
jnki	O
ni	O
n	O
k	O
where	O
is	O
a	O
regularization	B
coefficient	O
and	O
n	O
k	O
ynk	O
exercise	O
the	O
regularization	B
function	O
will	O
be	O
zero	O
when	O
the	O
network	O
mapping	O
function	O
is	O
invariant	O
under	O
the	O
transformation	O
in	O
the	O
neighbourhood	O
of	O
each	O
pattern	O
vector	O
and	O
the	O
value	O
of	O
the	O
parameter	O
determines	O
the	O
balance	O
between	O
fitting	O
the	O
training	B
data	O
and	O
learning	B
the	O
invariance	B
property	O
in	O
a	O
practical	O
implementation	O
the	O
tangent	O
vector	O
n	O
can	O
be	O
approximated	O
using	O
finite	O
differences	O
by	O
subtracting	O
the	O
original	O
vector	O
xn	O
from	O
the	O
corresponding	O
vector	O
after	O
transformation	O
using	O
a	O
small	O
value	O
of	O
and	O
then	O
dividing	O
by	O
this	O
is	O
illustrated	O
in	O
figure	O
the	O
regularization	B
function	O
depends	O
on	O
the	O
network	O
weights	O
through	O
the	O
jacobian	O
j	O
a	O
backpropagation	B
formalism	O
for	O
computing	O
the	O
derivatives	O
of	O
the	O
regularizer	O
with	O
respect	O
to	O
the	O
network	O
weights	O
is	O
easily	O
obtained	O
by	O
extension	O
of	O
the	O
techniques	O
introduced	O
in	O
section	O
if	O
the	O
transformation	O
is	O
governed	O
by	O
l	O
parameters	O
l	O
for	O
the	O
case	O
of	O
translations	O
combined	O
with	O
in-plane	O
rotations	O
in	O
a	O
two-dimensional	O
image	O
then	O
the	O
manifold	B
m	O
will	O
have	O
dimensionality	O
l	O
and	O
the	O
corresponding	O
regularizer	O
is	O
given	O
by	O
the	O
sum	O
of	O
terms	O
of	O
the	O
form	O
one	O
for	O
each	O
transformation	O
if	O
several	O
transformations	O
are	O
considered	O
at	O
the	O
same	O
time	O
and	O
the	O
network	O
mapping	O
is	O
made	O
invariant	O
to	O
each	O
separately	O
then	O
it	O
will	O
be	O
invariant	O
to	O
combinations	O
of	O
the	O
transformations	O
et	O
al	O
regularization	B
in	O
neural	O
networks	O
figure	O
illustration	O
showing	O
the	O
original	O
image	O
x	O
of	O
a	O
handwritten	B
digit	I
the	O
tangent	O
vector	O
corresponding	O
to	O
an	O
infinitesimal	O
clockwise	O
rotation	O
the	O
result	O
of	O
adding	O
a	O
small	O
contribution	O
from	O
the	O
tangent	O
vector	O
to	O
the	O
original	O
image	O
giving	O
x	O
with	O
degrees	O
and	O
the	O
true	O
image	O
rotated	O
for	O
comparison	O
a	O
related	O
technique	O
called	O
tangent	B
distance	I
can	O
be	O
used	O
to	O
build	O
invariance	B
properties	O
into	O
distance-based	O
methods	O
such	O
as	O
nearest-neighbour	O
classifiers	O
et	O
al	O
training	B
with	O
transformed	O
data	O
we	O
have	O
seen	O
that	O
one	O
way	O
to	O
encourage	O
invariance	B
of	O
a	O
model	O
to	O
a	O
set	O
of	O
transformations	O
is	O
to	O
expand	O
the	O
training	B
set	I
using	O
transformed	O
versions	O
of	O
the	O
original	O
input	O
patterns	O
here	O
we	O
show	O
that	O
this	O
approach	O
is	O
closely	O
related	O
to	O
the	O
technique	O
of	O
tangent	B
propagation	I
leen	O
as	O
in	O
section	O
we	O
shall	O
consider	O
a	O
transformation	O
governed	O
by	O
a	O
single	O
parameter	O
and	O
described	O
by	O
the	O
function	O
sx	O
with	O
sx	O
x	O
we	O
shall	O
also	O
consider	O
a	O
sum-of-squares	B
error	B
function	I
the	O
error	B
function	I
for	O
untransformed	O
inputs	O
can	O
be	O
written	O
the	O
infinite	O
data	O
set	O
limit	O
in	O
the	O
form	O
dx	O
dt	O
e	O
as	O
discussed	O
in	O
section	O
here	O
we	O
have	O
considered	O
a	O
network	O
having	O
a	O
single	O
output	O
in	O
order	O
to	O
keep	O
the	O
notation	O
uncluttered	O
if	O
we	O
now	O
consider	O
an	O
infinite	O
number	O
of	O
copies	O
of	O
each	O
data	O
point	O
each	O
of	O
which	O
is	O
perturbed	O
by	O
the	O
transformation	O
neural	O
networks	O
in	O
which	O
the	O
parameter	O
is	O
drawn	O
from	O
a	O
distribution	O
p	O
then	O
the	O
error	B
function	I
defined	O
over	O
this	O
expanded	O
data	O
set	O
can	O
be	O
written	O
as	O
dx	O
dt	O
d	O
we	O
now	O
assume	O
that	O
the	O
distribution	O
p	O
has	O
zero	O
mean	B
with	O
small	O
variance	B
so	O
that	O
we	O
are	O
only	O
considering	O
small	O
transformations	O
of	O
the	O
original	O
input	O
vectors	O
we	O
can	O
then	O
expand	O
the	O
transformation	O
function	O
as	O
a	O
taylor	O
series	O
in	O
powers	O
of	O
to	O
give	O
sx	O
sx	O
sx	O
sx	O
o	O
denotes	O
the	O
second	O
derivative	B
of	O
sx	O
with	O
respect	O
to	O
evaluated	O
at	O
x	O
o	O
where	O
this	O
allows	O
us	O
to	O
expand	O
the	O
model	O
function	O
to	O
give	O
ysx	O
yx	O
t	O
yx	O
yx	O
t	O
yx	O
o	O
substituting	O
into	O
the	O
mean	B
error	B
function	I
and	O
expanding	O
we	O
then	O
have	O
dx	O
dt	O
e	O
t	O
t	O
yxptxpx	O
dx	O
dt	O
e	O
t	O
yx	O
ptxpx	O
dx	O
dt	O
o	O
yx	O
t	O
yx	O
because	O
the	O
distribution	O
of	O
transformations	O
has	O
zero	O
mean	B
we	O
have	O
e	O
also	O
we	O
shall	O
denote	O
e	O
by	O
omitting	O
terms	O
of	O
o	O
the	O
average	O
error	B
function	I
then	O
becomes	O
where	O
e	O
is	O
the	O
original	O
sum-of-squares	B
error	B
and	O
the	O
regularization	B
term	O
takes	O
the	O
form	O
e	O
yx	O
t	O
yx	O
t	O
yx	O
px	O
dx	O
in	O
which	O
we	O
have	O
performed	O
the	O
integration	O
over	O
t	O
regularization	B
in	O
neural	O
networks	O
we	O
can	O
further	O
simplify	O
this	O
regularization	B
term	O
as	O
follows	O
in	O
section	O
we	O
saw	O
that	O
the	O
function	O
that	O
minimizes	O
the	O
sum-of-squares	B
error	B
is	O
given	O
by	O
the	O
conditional	B
average	O
etx	O
of	O
the	O
target	O
values	O
t	O
from	O
we	O
see	O
that	O
the	O
regularized	O
error	B
will	O
equal	O
the	O
unregularized	O
sum-of-squares	O
plus	O
terms	O
which	O
are	O
o	O
and	O
so	O
the	O
network	O
function	O
that	O
minimizes	O
the	O
total	O
error	B
will	O
have	O
the	O
form	O
yx	O
etx	O
o	O
thus	O
to	O
leading	O
order	O
in	O
the	O
first	O
term	O
in	O
the	O
regularizer	O
vanishes	O
and	O
we	O
are	O
left	O
with	O
px	O
dx	O
t	O
yx	O
exercise	O
which	O
is	O
equivalent	O
to	O
the	O
tangent	B
propagation	I
regularizer	O
if	O
we	O
consider	O
the	O
special	O
case	O
in	O
which	O
the	O
transformation	O
of	O
the	O
inputs	O
simply	O
consists	O
of	O
the	O
addition	O
of	O
random	O
noise	O
so	O
that	O
x	O
x	O
then	O
the	O
regularizer	O
takes	O
the	O
form	O
px	O
dx	O
which	O
is	O
known	O
as	O
tikhonov	B
regularization	B
and	O
arsenin	O
bishop	O
derivatives	O
of	O
this	O
regularizer	O
with	O
respect	O
to	O
the	O
network	O
weights	O
can	O
be	O
found	O
using	O
an	O
extended	B
backpropagation	B
algorithm	O
we	O
see	O
that	O
for	O
small	O
noise	O
amplitudes	O
tikhonov	B
regularization	B
is	O
related	O
to	O
the	O
addition	O
of	O
random	O
noise	O
to	O
the	O
inputs	O
which	O
has	O
been	O
shown	O
to	O
improve	O
generalization	B
in	O
appropriate	O
circumstances	O
and	O
dow	O
convolutional	B
networks	O
another	O
approach	O
to	O
creating	O
models	O
that	O
are	O
invariant	O
to	O
certain	O
transformation	O
of	O
the	O
inputs	O
is	O
to	O
build	O
the	O
invariance	B
properties	O
into	O
the	O
structure	O
of	O
a	O
neural	B
network	I
this	O
is	O
the	O
basis	O
for	O
the	O
convolutional	B
neural	B
network	I
cun	O
et	O
al	O
lecun	O
et	O
al	O
which	O
has	O
been	O
widely	O
applied	O
to	O
image	O
data	O
consider	O
the	O
specific	O
task	O
of	O
recognizing	O
handwritten	O
digits	O
each	O
input	O
image	O
comprises	O
a	O
set	O
of	O
pixel	O
intensity	O
values	O
and	O
the	O
desired	O
output	O
is	O
a	O
posterior	B
probability	B
distribution	O
over	O
the	O
ten	O
digit	O
classes	O
we	O
know	O
that	O
the	O
identity	O
of	O
the	O
digit	O
is	O
invariant	O
under	O
translations	O
and	O
scaling	O
as	O
well	O
as	O
rotations	O
furthermore	O
the	O
network	O
must	O
also	O
exhibit	O
invariance	B
to	O
more	O
subtle	O
transformations	O
such	O
as	O
elastic	O
deformations	O
of	O
the	O
kind	O
illustrated	O
in	O
figure	O
one	O
simple	O
approach	O
would	O
be	O
to	O
treat	O
the	O
image	O
as	O
the	O
input	O
to	O
a	O
fully	B
connected	I
network	O
such	O
as	O
the	O
kind	O
shown	O
in	O
figure	O
given	O
a	O
sufficiently	O
large	O
training	B
set	I
such	O
a	O
network	O
could	O
in	O
principle	O
yield	O
a	O
good	O
solution	O
to	O
this	O
problem	O
and	O
would	O
learn	O
the	O
appropriate	O
invariances	O
by	O
example	O
however	O
this	O
approach	O
ignores	O
a	O
key	O
property	O
of	O
images	O
which	O
is	O
that	O
nearby	O
pixels	O
are	O
more	O
strongly	O
correlated	O
than	O
more	O
distant	O
pixels	O
many	O
of	O
the	O
modern	O
approaches	O
to	O
computer	O
vision	O
exploit	O
this	O
property	O
by	O
extracting	O
local	B
features	O
that	O
depend	O
only	O
on	O
small	O
subregions	O
of	O
the	O
image	O
information	O
from	O
such	O
features	O
can	O
then	O
be	O
merged	O
in	O
later	O
stages	O
of	O
processing	O
in	O
order	O
to	O
detect	O
higher-order	O
features	O
neural	O
networks	O
input	O
image	O
convolutional	B
layer	O
sub-sampling	O
layer	O
figure	O
diagram	O
illustrating	O
part	O
of	O
a	O
convolutional	B
neural	B
network	I
showing	O
a	O
layer	O
of	O
convolutional	B
units	O
followed	O
by	O
a	O
layer	O
of	O
subsampling	B
units	O
several	O
successive	O
pairs	O
of	O
such	O
layers	O
may	O
be	O
used	O
and	O
ultimately	O
to	O
yield	O
information	O
about	O
the	O
image	O
as	O
whole	O
also	O
local	B
features	O
that	O
are	O
useful	O
in	O
one	O
region	O
of	O
the	O
image	O
are	O
likely	O
to	O
be	O
useful	O
in	O
other	O
regions	O
of	O
the	O
image	O
for	O
instance	O
if	O
the	O
object	O
of	O
interest	O
is	O
translated	O
these	O
notions	O
are	O
incorporated	O
into	O
convolutional	B
neural	O
networks	O
through	O
three	O
mechanisms	O
local	B
receptive	O
fields	O
weight	B
sharing	I
and	O
subsampling	B
the	O
structure	O
of	O
a	O
convolutional	B
network	O
is	O
illustrated	O
in	O
figure	O
in	O
the	O
convolutional	B
layer	O
the	O
units	O
are	O
organized	O
into	O
planes	O
each	O
of	O
which	O
is	O
called	O
a	O
feature	B
map	I
units	O
in	O
a	O
feature	B
map	I
each	O
take	O
inputs	O
only	O
from	O
a	O
small	O
subregion	O
of	O
the	O
image	O
and	O
all	O
of	O
the	O
units	O
in	O
a	O
feature	B
map	I
are	O
constrained	O
to	O
share	O
the	O
same	O
weight	O
values	O
for	O
instance	O
a	O
feature	B
map	I
might	O
consist	O
of	O
units	O
arranged	O
in	O
a	O
grid	O
with	O
each	O
unit	O
taking	O
inputs	O
from	O
a	O
pixel	O
patch	O
of	O
the	O
image	O
the	O
whole	O
feature	B
map	I
therefore	O
has	O
adjustable	O
weight	O
parameters	O
plus	O
one	O
adjustable	O
bias	B
parameter	I
input	O
values	O
from	O
a	O
patch	O
are	O
linearly	O
combined	O
using	O
the	O
weights	O
and	O
the	O
bias	B
and	O
the	O
result	O
transformed	O
by	O
a	O
sigmoidal	O
nonlinearity	O
using	O
if	O
we	O
think	O
of	O
the	O
units	O
as	O
feature	O
detectors	O
then	O
all	O
of	O
the	O
units	O
in	O
a	O
feature	B
map	I
detect	O
the	O
same	O
pattern	O
but	O
at	O
different	O
locations	O
in	O
the	O
input	O
image	O
due	O
to	O
the	O
weight	B
sharing	I
the	O
evaluation	O
of	O
the	O
activations	O
of	O
these	O
units	O
is	O
equivalent	O
to	O
a	O
convolution	O
of	O
the	O
image	O
pixel	O
intensities	O
with	O
a	O
kernel	O
comprising	O
the	O
weight	O
parameters	O
if	O
the	O
input	O
image	O
is	O
shifted	O
the	O
activations	O
of	O
the	O
feature	B
map	I
will	O
be	O
shifted	O
by	O
the	O
same	O
amount	O
but	O
will	O
otherwise	O
be	O
unchanged	O
this	O
provides	O
the	O
basis	O
for	O
the	O
invariance	B
of	O
regularization	B
in	O
neural	O
networks	O
the	O
network	O
outputs	O
to	O
translations	O
and	O
distortions	O
of	O
the	O
input	O
image	O
because	O
we	O
will	O
typically	O
need	O
to	O
detect	O
multiple	O
features	O
in	O
order	O
to	O
build	O
an	O
effective	O
model	O
there	O
will	O
generally	O
be	O
multiple	O
feature	O
maps	O
in	O
the	O
convolutional	B
layer	O
each	O
having	O
its	O
own	O
set	O
of	O
weight	O
and	O
bias	B
parameters	O
the	O
outputs	O
of	O
the	O
convolutional	B
units	O
form	O
the	O
inputs	O
to	O
the	O
subsampling	B
layer	O
of	O
the	O
network	O
for	O
each	O
feature	B
map	I
in	O
the	O
convolutional	B
layer	O
there	O
is	O
a	O
plane	O
of	O
units	O
in	O
the	O
subsampling	B
layer	O
and	O
each	O
unit	O
takes	O
inputs	O
from	O
a	O
small	O
receptive	O
field	O
in	O
the	O
corresponding	O
feature	B
map	I
of	O
the	O
convolutional	B
layer	O
these	O
units	O
perform	O
subsampling	B
for	O
instance	O
each	O
subsampling	B
unit	O
might	O
take	O
inputs	O
from	O
a	O
unit	O
region	O
in	O
the	O
corresponding	O
feature	B
map	I
and	O
would	O
compute	O
the	O
average	O
of	O
those	O
inputs	O
multiplied	O
by	O
an	O
adaptive	O
weight	O
with	O
the	O
addition	O
of	O
an	O
adaptive	O
bias	B
parameter	I
and	O
then	O
transformed	O
using	O
a	O
sigmoidal	O
nonlinear	O
activation	B
function	I
the	O
receptive	O
fields	O
are	O
chosen	O
to	O
be	O
contiguous	O
and	O
nonoverlapping	O
so	O
that	O
there	O
are	O
half	O
the	O
number	O
of	O
rows	O
and	O
columns	O
in	O
the	O
subsampling	B
layer	O
compared	O
with	O
the	O
convolutional	B
layer	O
in	O
this	O
way	O
the	O
response	O
of	O
a	O
unit	O
in	O
the	O
subsampling	B
layer	O
will	O
be	O
relatively	O
insensitive	O
to	O
small	O
shifts	O
of	O
the	O
image	O
in	O
the	O
corresponding	O
regions	O
of	O
the	O
input	O
space	O
in	O
a	O
practical	O
architecture	O
there	O
may	O
be	O
several	O
pairs	O
of	O
convolutional	B
and	O
subsampling	B
layers	O
at	O
each	O
stage	O
there	O
is	O
a	O
larger	O
degree	O
of	O
invariance	B
to	O
input	O
transformations	O
compared	O
to	O
the	O
previous	O
layer	O
there	O
may	O
be	O
several	O
feature	O
maps	O
in	O
a	O
given	O
convolutional	B
layer	O
for	O
each	O
plane	O
of	O
units	O
in	O
the	O
previous	O
subsampling	B
layer	O
so	O
that	O
the	O
gradual	O
reduction	O
in	O
spatial	O
resolution	O
is	O
then	O
compensated	O
by	O
an	O
increasing	O
number	O
of	O
features	O
the	O
final	O
layer	O
of	O
the	O
network	O
would	O
typically	O
be	O
a	O
fully	B
connected	I
fully	O
adaptive	O
layer	O
with	O
a	O
softmax	O
output	O
nonlinearity	O
in	O
the	O
case	O
of	O
multiclass	B
classification	B
the	O
whole	O
network	O
can	O
be	O
trained	O
by	O
error	B
minimization	O
using	O
backpropagation	B
to	O
evaluate	O
the	O
gradient	O
of	O
the	O
error	B
function	I
this	O
involves	O
a	O
slight	O
modification	O
of	O
the	O
usual	O
backpropagation	B
algorithm	O
to	O
ensure	O
that	O
the	O
shared-weight	O
constraints	O
are	O
satisfied	O
due	O
to	O
the	O
use	O
of	O
local	B
receptive	O
fields	O
the	O
number	O
of	O
weights	O
in	O
the	O
network	O
is	O
smaller	O
than	O
if	O
the	O
network	O
were	O
fully	B
connected	I
furthermore	O
the	O
number	O
of	O
independent	B
parameters	O
to	O
be	O
learned	O
from	O
the	O
data	O
is	O
much	O
smaller	O
still	O
due	O
to	O
the	O
substantial	O
numbers	O
of	O
constraints	O
on	O
the	O
weights	O
soft	B
weight	B
sharing	I
one	O
way	O
to	O
reduce	O
the	O
effective	O
complexity	O
of	O
a	O
network	O
with	O
a	O
large	O
number	O
of	O
weights	O
is	O
to	O
constrain	O
weights	O
within	O
certain	O
groups	O
to	O
be	O
equal	O
this	O
is	O
the	O
technique	O
of	O
weight	B
sharing	I
that	O
was	O
discussed	O
in	O
section	O
as	O
a	O
way	O
of	O
building	O
translation	B
invariance	B
into	O
networks	O
used	O
for	O
image	O
interpretation	O
it	O
is	O
only	O
applicable	O
however	O
to	O
particular	O
problems	O
in	O
which	O
the	O
form	O
of	O
the	O
constraints	O
can	O
be	O
specified	O
in	O
advance	O
here	O
we	O
consider	O
a	O
form	O
of	O
soft	B
weight	B
sharing	I
and	O
hinton	O
in	O
which	O
the	O
hard	O
constraint	O
of	O
equal	O
weights	O
is	O
replaced	O
by	O
a	O
form	O
of	O
regularization	B
in	O
which	O
groups	O
of	O
weights	O
are	O
encouraged	O
to	O
have	O
similar	O
values	O
furthermore	O
the	O
division	O
of	O
weights	O
into	O
groups	O
the	O
mean	B
weight	O
value	O
for	O
each	O
group	O
and	O
the	O
spread	O
of	O
values	O
within	O
the	O
groups	O
are	O
all	O
determined	O
as	O
part	O
of	O
the	O
learning	B
process	O
exercise	O
neural	O
networks	O
section	O
exercise	O
recall	O
that	O
the	O
simple	O
weight	B
decay	I
regularizer	O
given	O
in	O
can	O
be	O
viewed	O
as	O
the	O
negative	O
log	O
of	O
a	O
gaussian	B
prior	B
distribution	O
over	O
the	O
weights	O
we	O
can	O
encourage	O
the	O
weight	O
values	O
to	O
form	O
several	O
groups	O
rather	O
than	O
just	O
one	O
group	O
by	O
considering	O
instead	O
a	O
probability	B
distribution	O
that	O
is	O
a	O
mixture	B
of	I
gaussians	I
the	O
centres	O
and	O
variances	O
of	O
the	O
gaussian	B
components	O
as	O
well	O
as	O
the	O
mixing	O
coefficients	O
will	O
be	O
considered	O
as	O
adjustable	O
parameters	O
to	O
be	O
determined	O
as	O
part	O
of	O
the	O
learning	B
process	O
thus	O
we	O
have	O
a	O
probability	B
density	B
of	O
the	O
form	O
pw	O
pwi	O
where	O
i	O
jn	O
j	O
j	O
pwi	O
and	O
j	O
are	O
the	O
mixing	O
coefficients	O
taking	O
the	O
negative	O
logarithm	O
then	O
leads	O
to	O
a	O
regularization	B
function	O
of	O
the	O
form	O
ln	O
jn	O
j	O
j	O
i	O
the	O
total	O
error	B
function	I
is	O
then	O
given	O
by	O
ew	O
where	O
is	O
the	O
regularization	B
coefficient	O
this	O
error	B
is	O
minimized	O
both	O
with	O
respect	O
to	O
the	O
weights	O
wi	O
and	O
with	O
respect	O
to	O
the	O
parameters	O
j	O
j	O
j	O
of	O
the	O
mixture	B
model	I
if	O
the	O
weights	O
were	O
constant	O
then	O
the	O
parameters	O
of	O
the	O
mixture	B
model	I
could	O
be	O
determined	O
by	O
using	O
the	O
em	B
algorithm	I
discussed	O
in	O
chapter	O
however	O
the	O
distribution	O
of	O
weights	O
is	O
itself	O
evolving	O
during	O
the	O
learning	B
process	O
and	O
so	O
to	O
avoid	O
numerical	O
instability	O
a	O
joint	O
optimization	O
is	O
performed	O
simultaneously	O
over	O
the	O
weights	O
and	O
the	O
mixture-model	O
parameters	O
this	O
can	O
be	O
done	O
using	O
a	O
standard	O
optimization	O
algorithm	O
such	O
as	O
conjugate	B
gradients	O
or	O
quasi-newton	O
methods	O
in	O
order	O
to	O
minimize	O
the	O
total	O
error	B
function	I
it	O
is	O
necessary	O
to	O
be	O
able	O
to	O
evaluate	O
its	O
derivatives	O
with	O
respect	O
to	O
the	O
various	O
adjustable	O
parameters	O
to	O
do	O
this	O
it	O
is	O
convenient	O
to	O
regard	O
the	O
j	O
as	O
prior	B
probabilities	O
and	O
to	O
introduce	O
the	O
corresponding	O
posterior	O
probabilities	O
which	O
following	O
are	O
given	O
by	O
bayes	B
theorem	O
in	O
the	O
form	O
jw	O
jn	O
j	O
j	O
k	O
kn	O
k	O
k	O
j	O
j	O
e	O
wi	O
jwi	O
j	O
wi	O
the	O
derivatives	O
of	O
the	O
total	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
are	O
then	O
given	O
by	O
regularization	B
in	O
neural	O
networks	O
the	O
effect	O
of	O
the	O
regularization	B
term	O
is	O
therefore	O
to	O
pull	O
each	O
weight	O
towards	O
the	O
centre	O
of	O
the	O
jth	O
gaussian	B
with	O
a	O
force	O
proportional	O
to	O
the	O
posterior	B
probability	B
of	O
that	O
gaussian	B
for	O
the	O
given	O
weight	O
this	O
is	O
precisely	O
the	O
kind	O
of	O
effect	O
that	O
we	O
are	O
seeking	O
derivatives	O
of	O
the	O
error	B
with	O
respect	O
to	O
the	O
centres	O
of	O
the	O
gaussians	O
are	O
also	O
j	O
i	O
jwi	O
i	O
wj	O
j	O
exercise	O
easily	O
computed	O
to	O
give	O
which	O
has	O
a	O
simple	O
intuitive	O
interpretation	O
because	O
it	O
pushes	O
j	O
towards	O
an	O
average	O
of	O
the	O
weight	O
values	O
weighted	O
by	O
the	O
posterior	O
probabilities	O
that	O
the	O
respective	O
weight	O
parameters	O
were	O
generated	O
by	O
component	O
j	O
similarly	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
variances	O
are	O
given	O
by	O
exercise	O
j	O
i	O
jwi	O
j	O
j	O
which	O
drives	O
j	O
towards	O
the	O
weighted	O
average	O
of	O
the	O
squared	O
deviations	O
of	O
the	O
weights	O
around	O
the	O
corresponding	O
centre	O
j	O
where	O
the	O
weighting	O
coefficients	O
are	O
again	O
given	O
by	O
the	O
posterior	B
probability	B
that	O
each	O
weight	O
is	O
generated	O
by	O
component	O
j	O
note	O
that	O
in	O
a	O
practical	O
implementation	O
new	O
variables	O
j	O
defined	O
by	O
j	O
exp	O
j	O
are	O
introduced	O
and	O
the	O
minimization	O
is	O
performed	O
with	O
respect	O
to	O
the	O
j	O
this	O
ensures	O
that	O
the	O
parameters	O
j	O
remain	O
positive	O
it	O
also	O
has	O
the	O
effect	O
of	O
discouraging	O
pathological	O
solutions	O
in	O
which	O
one	O
or	O
more	O
of	O
the	O
j	O
goes	O
to	O
zero	O
corresponding	O
to	O
a	O
gaussian	B
component	O
collapsing	O
onto	O
one	O
of	O
the	O
weight	B
parameter	I
values	O
such	O
solutions	O
are	O
discussed	O
in	O
more	O
detail	O
in	O
the	O
context	O
of	O
gaussian	B
mixture	B
models	O
in	O
section	O
for	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
mixing	O
coefficients	O
j	O
we	O
need	O
to	O
take	O
account	O
of	O
the	O
constraints	O
j	O
i	O
j	O
which	O
follow	O
from	O
the	O
interpretation	O
of	O
the	O
j	O
as	O
prior	B
probabilities	O
this	O
can	O
be	O
done	O
by	O
expressing	O
the	O
mixing	O
coefficients	O
in	O
terms	O
of	O
a	O
set	O
of	O
auxiliary	O
variables	O
j	O
using	O
the	O
softmax	B
function	I
given	O
by	O
j	O
exp	O
j	O
exp	O
k	O
exercise	O
the	O
derivatives	O
of	O
the	O
regularized	O
error	B
function	I
with	O
respect	O
to	O
the	O
j	O
then	O
take	O
the	O
form	O
neural	O
networks	O
figure	O
the	O
left	O
figure	O
shows	O
a	O
two-link	O
robot	B
arm	I
in	O
which	O
the	O
cartesian	O
coordinates	O
of	O
the	O
end	O
effector	O
are	O
determined	O
uniquely	O
by	O
the	O
two	O
joint	O
angles	O
and	O
and	O
the	O
lengths	O
and	O
of	O
the	O
arms	O
this	O
is	O
know	O
as	O
the	O
forward	B
kinematics	I
of	O
the	O
arm	O
in	O
practice	O
we	O
have	O
to	O
find	O
the	O
joint	O
angles	O
that	O
will	O
give	O
rise	O
to	O
a	O
desired	O
end	O
effector	O
position	O
and	O
as	O
shown	O
in	O
the	O
right	O
figure	O
this	O
inversekinematicshas	O
two	O
solutions	O
corresponding	O
to	O
elbow	O
up	O
and	O
elbow	O
down	O
j	O
elbow	O
up	O
j	O
jwi	O
i	O
elbow	O
down	O
we	O
see	O
that	O
j	O
is	O
therefore	O
driven	O
towards	O
the	O
average	O
posterior	B
probability	B
for	O
component	O
j	O
mixture	B
density	B
networks	O
exercise	O
the	O
goal	O
of	O
supervised	B
learning	B
is	O
to	O
model	O
a	O
conditional	B
distribution	O
ptx	O
which	O
for	O
many	O
simple	O
regression	B
problems	O
is	O
chosen	O
to	O
be	O
gaussian	B
however	O
practical	O
machine	O
learning	B
problems	O
can	O
often	O
have	O
significantly	O
non-gaussian	O
distributions	O
these	O
can	O
arise	O
for	O
example	O
with	O
inverse	B
problems	O
in	O
which	O
the	O
distribution	O
can	O
be	O
multimodal	O
in	O
which	O
case	O
the	O
gaussian	B
assumption	O
can	O
lead	O
to	O
very	O
poor	O
predictions	O
as	O
a	O
simple	O
example	O
of	O
an	O
inverse	B
problem	I
consider	O
the	O
kinematics	O
of	O
a	O
robot	B
arm	I
as	O
illustrated	O
in	O
figure	O
the	O
forward	B
problem	I
involves	O
finding	O
the	O
end	O
effector	O
position	O
given	O
the	O
joint	O
angles	O
and	O
has	O
a	O
unique	O
solution	O
however	O
in	O
practice	O
we	O
wish	O
to	O
move	O
the	O
end	O
effector	O
of	O
the	O
robot	O
to	O
a	O
specific	O
position	O
and	O
to	O
do	O
this	O
we	O
must	O
set	O
appropriate	O
joint	O
angles	O
we	O
therefore	O
need	O
to	O
solve	O
the	O
inverse	B
problem	I
which	O
has	O
two	O
solutions	O
as	O
seen	O
in	O
figure	O
forward	O
problems	O
often	O
corresponds	O
to	O
causality	B
in	O
a	O
physical	O
system	O
and	O
generally	O
have	O
a	O
unique	O
solution	O
for	O
instance	O
a	O
specific	O
pattern	O
of	O
symptoms	O
in	O
the	O
human	O
body	O
may	O
be	O
caused	O
by	O
the	O
presence	O
of	O
a	O
particular	O
disease	O
in	O
pattern	O
recognition	O
however	O
we	O
typically	O
have	O
to	O
solve	O
an	O
inverse	B
problem	I
such	O
as	O
trying	O
to	O
predict	O
the	O
presence	O
of	O
a	O
disease	O
given	O
a	O
set	O
of	O
symptoms	O
if	O
the	O
forward	B
problem	I
involves	O
a	O
many-to-one	O
mapping	O
then	O
the	O
inverse	B
problem	I
will	O
have	O
multiple	O
solutions	O
for	O
instance	O
several	O
different	O
diseases	O
may	O
result	O
in	O
the	O
same	O
symptoms	O
in	O
the	O
robotics	O
example	O
the	O
kinematics	O
is	O
defined	O
by	O
geometrical	O
equations	O
and	O
the	O
multimodality	B
is	O
readily	O
apparent	O
however	O
in	O
many	O
machine	O
learning	B
problems	O
the	O
presence	O
of	O
multimodality	B
particularly	O
in	O
problems	O
involving	O
spaces	O
of	O
high	O
dimensionality	O
can	O
be	O
less	O
obvious	O
for	O
tutorial	O
purposes	O
however	O
we	O
shall	O
consider	O
a	O
simple	O
toy	O
problem	O
for	O
which	O
we	O
can	O
easily	O
visualize	O
the	O
multimodality	B
data	O
for	O
this	O
problem	O
is	O
generated	O
by	O
sampling	O
a	O
variable	O
x	O
uniformly	O
over	O
the	O
interval	O
to	O
give	O
a	O
set	O
of	O
values	O
and	O
the	O
corresponding	O
target	O
values	O
tn	O
are	O
obtained	O
figure	O
on	O
the	O
left	O
is	O
the	O
data	O
set	O
for	O
a	O
simple	O
forward	B
problem	I
in	O
which	O
the	O
red	O
curve	O
shows	O
the	O
result	O
of	O
fitting	O
a	O
two-layer	O
neural	B
network	I
by	O
minimizing	O
the	O
sum-of-squares	B
error	B
function	I
the	O
corresponding	O
inverse	B
problem	I
shown	O
on	O
the	O
right	O
is	O
obtained	O
by	O
exchanging	O
the	O
roles	O
of	O
x	O
and	O
t	O
here	O
the	O
same	O
network	O
trained	O
again	O
by	O
minimizing	O
the	O
sum-of-squares	B
error	B
function	I
gives	O
a	O
very	O
poor	O
fit	O
to	O
the	O
data	O
due	O
to	O
the	O
multimodality	B
of	O
the	O
data	O
set	O
mixture	B
density	B
networks	O
by	O
computing	O
the	O
function	O
xn	O
xn	O
and	O
then	O
adding	O
uniform	O
noise	O
over	O
the	O
interval	O
the	O
inverse	B
problem	I
is	O
then	O
obtained	O
by	O
keeping	O
the	O
same	O
data	O
points	O
but	O
exchanging	O
the	O
roles	O
of	O
x	O
and	O
t	O
figure	O
shows	O
the	O
data	O
sets	O
for	O
the	O
forward	O
and	O
inverse	B
problems	O
along	O
with	O
the	O
results	O
of	O
fitting	O
two-layer	O
neural	O
networks	O
having	O
hidden	O
units	O
and	O
a	O
single	O
linear	O
output	O
unit	O
by	O
minimizing	O
a	O
sumof-squares	O
error	B
function	I
least	O
squares	O
corresponds	O
to	O
maximum	B
likelihood	I
under	O
a	O
gaussian	B
assumption	O
we	O
see	O
that	O
this	O
leads	O
to	O
a	O
very	O
poor	O
model	O
for	O
the	O
highly	O
non-gaussian	O
inverse	B
problem	I
we	O
therefore	O
seek	O
a	O
general	O
framework	O
for	O
modelling	O
conditional	B
probability	B
distributions	O
this	O
can	O
be	O
achieved	O
by	O
using	O
a	O
mixture	B
model	I
for	O
ptx	O
in	O
which	O
both	O
the	O
mixing	O
coefficients	O
as	O
well	O
as	O
the	O
component	O
densities	O
are	O
flexible	O
functions	O
of	O
the	O
input	O
vector	O
x	O
giving	O
rise	O
to	O
the	O
mixture	B
density	B
network	I
for	O
any	O
given	O
value	O
of	O
x	O
the	O
mixture	B
model	I
provides	O
a	O
general	O
formalism	O
for	O
modelling	O
an	O
arbitrary	O
conditional	B
density	B
function	O
ptx	O
provided	O
we	O
consider	O
a	O
sufficiently	O
flexible	O
network	O
we	O
then	O
have	O
a	O
framework	O
for	O
approximating	O
arbitrary	O
conditional	B
distributions	O
here	O
we	O
shall	O
develop	O
the	O
model	O
explicitly	O
for	O
gaussian	B
components	O
so	O
that	O
ptx	O
t	O
kx	O
kx	O
this	O
is	O
an	O
example	O
of	O
a	O
heteroscedastic	B
model	O
since	O
the	O
noise	O
variance	B
on	O
the	O
data	O
is	O
a	O
function	O
of	O
the	O
input	O
vector	O
x	O
instead	O
of	O
gaussians	O
we	O
can	O
use	O
other	O
distributions	O
for	O
the	O
components	O
such	O
as	O
bernoulli	B
distributions	O
if	O
the	O
target	O
variables	O
are	O
binary	O
rather	O
than	O
continuous	O
we	O
have	O
also	O
specialized	O
to	O
the	O
case	O
of	O
isotropic	B
covariances	O
for	O
the	O
components	O
although	O
the	O
mixture	B
density	B
network	I
can	O
readily	O
be	O
extended	B
to	O
allow	O
for	O
general	O
covariance	B
matrices	O
by	O
representing	O
the	O
covariances	O
using	O
a	O
cholesky	O
factorization	B
even	O
with	O
isotropic	B
components	O
the	O
conditional	B
distribution	O
ptx	O
does	O
not	O
assume	O
factorization	B
with	O
respect	O
to	O
the	O
components	O
of	O
t	O
contrast	O
to	O
the	O
standard	O
sum-of-squares	O
regression	B
model	O
as	O
a	O
consequence	O
of	O
the	O
mixture	B
distribution	I
we	O
now	O
take	O
the	O
various	O
parameters	O
of	O
the	O
mixture	B
model	I
namely	O
the	O
mixing	O
kx	O
to	O
be	O
governed	O
by	O
coefficients	O
kx	O
the	O
means	O
kx	O
and	O
the	O
variances	O
neural	O
networks	O
xd	O
ptx	O
m	O
figure	O
the	O
mixturedensitynetwork	O
can	O
represent	O
general	O
conditional	B
probability	B
densities	O
ptx	O
by	O
considering	O
a	O
parametric	O
mixture	B
model	I
for	O
the	O
distribution	O
of	O
t	O
whose	O
parameters	O
are	O
determined	O
by	O
the	O
outputs	O
of	O
a	O
neural	B
network	I
that	O
takes	O
x	O
as	O
its	O
input	O
vector	O
t	O
the	O
outputs	O
of	O
a	O
conventional	O
neural	B
network	I
that	O
takes	O
x	O
as	O
its	O
input	O
the	O
structure	O
of	O
this	O
mixture	B
density	B
network	I
is	O
illustrated	O
in	O
figure	O
the	O
mixture	B
density	B
network	I
is	O
closely	O
related	O
to	O
the	O
mixture	B
of	I
experts	I
discussed	O
in	O
section	O
the	O
principle	O
difference	O
is	O
that	O
in	O
the	O
mixture	B
density	B
network	I
the	O
same	O
function	O
is	O
used	O
to	O
predict	O
the	O
parameters	O
of	O
all	O
of	O
the	O
component	O
densities	O
as	O
well	O
as	O
the	O
mixing	O
coefficients	O
and	O
so	O
the	O
nonlinear	O
hidden	O
units	O
are	O
shared	O
amongst	O
the	O
input-dependent	O
functions	O
the	O
neural	B
network	I
in	O
figure	O
can	O
for	O
example	O
be	O
a	O
two-layer	O
network	O
having	O
sigmoidal	O
tanh	O
hidden	O
units	O
if	O
there	O
are	O
l	O
components	O
in	O
the	O
mixture	B
model	I
and	O
if	O
t	O
has	O
k	O
components	O
then	O
the	O
network	O
will	O
have	O
l	O
output	O
unit	O
activations	O
denoted	O
by	O
a	O
k	O
that	O
determine	O
the	O
mixing	O
coefficients	O
kx	O
k	O
outputs	O
k	O
that	O
determine	O
the	O
kernel	O
widths	O
kx	O
and	O
l	O
k	O
outputs	O
denoted	O
denoted	O
by	O
a	O
kj	O
that	O
determine	O
the	O
components	O
kjx	O
of	O
the	O
kernel	O
centres	O
kx	O
the	O
total	O
by	O
a	O
number	O
of	O
network	O
outputs	O
is	O
given	O
by	O
as	O
compared	O
with	O
the	O
usual	O
k	O
outputs	O
for	O
a	O
network	O
which	O
simply	O
predicts	O
the	O
conditional	B
means	O
of	O
the	O
target	O
variables	O
the	O
mixing	O
coefficients	O
must	O
satisfy	O
the	O
constraints	O
kx	O
kx	O
which	O
can	O
be	O
achieved	O
using	O
a	O
set	O
of	O
softmax	O
outputs	O
expa	O
k	O
expa	O
l	O
kx	O
similarly	O
the	O
variances	O
must	O
satisfy	O
of	O
the	O
exponentials	O
of	O
the	O
corresponding	O
network	O
activations	O
using	O
kx	O
and	O
so	O
can	O
be	O
represented	O
in	O
terms	O
kx	O
expa	O
k	O
finally	O
because	O
the	O
means	O
kx	O
have	O
real	O
components	O
they	O
can	O
be	O
represented	O
mixture	B
density	B
networks	O
directly	O
by	O
the	O
network	O
output	O
activations	O
kjx	O
a	O
kj	O
the	O
adaptive	O
parameters	O
of	O
the	O
mixture	B
density	B
network	I
comprise	O
the	O
vector	O
w	O
of	O
weights	O
and	O
biases	O
in	O
the	O
neural	B
network	I
that	O
can	O
be	O
set	O
by	O
maximum	B
likelihood	I
or	O
equivalently	O
by	O
minimizing	O
an	O
error	B
function	I
defined	O
to	O
be	O
the	O
negative	O
logarithm	O
of	O
the	O
likelihood	O
for	O
independent	B
data	O
this	O
error	B
function	I
takes	O
the	O
form	O
ew	O
ln	O
kxn	O
tn	O
kxn	O
w	O
kxn	O
w	O
where	O
we	O
have	O
made	O
the	O
dependencies	O
on	O
w	O
explicit	O
in	O
order	O
to	O
minimize	O
the	O
error	B
function	I
we	O
need	O
to	O
calculate	O
the	O
derivatives	O
of	O
the	O
error	B
ew	O
with	O
respect	O
to	O
the	O
components	O
of	O
w	O
these	O
can	O
be	O
evaluated	O
by	O
using	O
the	O
standard	O
backpropagation	B
procedure	O
provided	O
we	O
obtain	O
suitable	O
expressions	O
for	O
the	O
derivatives	O
of	O
the	O
error	B
with	O
respect	O
to	O
the	O
output-unit	O
activations	O
these	O
represent	O
error	B
signals	O
for	O
each	O
pattern	O
and	O
for	O
each	O
output	O
unit	O
and	O
can	O
be	O
backpropagated	O
to	O
the	O
hidden	O
units	O
and	O
the	O
error	B
function	I
derivatives	O
evaluated	O
in	O
the	O
usual	O
way	O
because	O
the	O
error	B
function	I
is	O
composed	O
of	O
a	O
sum	O
of	O
terms	O
one	O
for	O
each	O
training	B
data	O
point	O
we	O
can	O
consider	O
the	O
derivatives	O
for	O
a	O
particular	O
pattern	O
n	O
and	O
then	O
find	O
the	O
derivatives	O
of	O
e	O
by	O
summing	O
over	O
all	O
patterns	O
because	O
we	O
are	O
dealing	O
with	O
mixture	B
distributions	O
it	O
is	O
convenient	O
to	O
view	O
the	O
mixing	O
coefficients	O
kx	O
as	O
x-dependent	O
prior	B
probabilities	O
and	O
to	O
introduce	O
the	O
corresponding	O
posterior	O
probabilities	O
given	O
by	O
ktx	O
knnk	O
lnnl	O
where	O
nnk	O
denotes	O
n	O
kxn	O
kxn	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
network	O
output	O
activations	O
governing	O
the	O
mix	O
exercise	O
ing	O
coefficients	O
are	O
given	O
by	O
en	O
a	O
k	O
k	O
k	O
similarly	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
output	O
activations	O
controlling	O
the	O
component	O
means	O
are	O
given	O
by	O
exercise	O
exercise	O
en	O
a	O
kl	O
k	O
kl	O
tl	O
k	O
k	O
k	O
k	O
en	O
a	O
k	O
finally	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
output	O
activations	O
controlling	O
the	O
component	O
variances	O
are	O
given	O
by	O
neural	O
networks	O
figure	O
plot	O
of	O
the	O
mixing	O
coefficients	O
kx	O
as	O
a	O
function	O
of	O
x	O
for	O
the	O
three	O
kernel	O
functions	O
in	O
a	O
mixture	B
density	B
network	I
trained	O
on	O
the	O
data	O
shown	O
in	O
figure	O
the	O
model	O
has	O
three	O
gaussian	B
components	O
and	O
uses	O
a	O
two-layer	O
multilayer	B
perceptron	B
with	O
five	O
tanh	O
sigmoidal	O
units	O
in	O
the	O
hidden	O
layer	O
and	O
nine	O
outputs	O
to	O
the	O
means	O
and	O
variances	O
of	O
the	O
gaussian	B
components	O
and	O
the	O
mixing	O
coefficients	O
at	O
both	O
small	O
and	O
large	O
values	O
of	O
x	O
where	O
the	O
conditional	B
probability	B
density	B
of	O
the	O
target	O
data	O
is	O
unimodal	O
only	O
one	O
of	O
the	O
kernels	O
has	O
a	O
high	O
value	O
for	O
its	O
prior	B
probability	B
while	O
at	O
intermediate	O
values	O
of	O
x	O
where	O
the	O
conditional	B
density	B
is	O
trimodal	O
the	O
three	O
mixing	O
coefficients	O
have	O
comparable	O
values	O
plots	O
of	O
the	O
means	O
kx	O
using	O
the	O
same	O
colour	O
coding	O
as	O
for	O
the	O
mixing	O
coefficients	O
plot	O
of	O
the	O
contours	O
of	O
the	O
corresponding	O
conditional	B
probability	B
density	B
of	O
the	O
target	O
data	O
for	O
the	O
same	O
mixture	B
density	B
network	I
the	O
approximate	O
conditional	B
mode	O
shown	O
by	O
the	O
red	O
points	O
of	O
the	O
conditional	B
density	B
plot	O
of	O
we	O
illustrate	O
the	O
use	O
of	O
a	O
mixture	B
density	B
network	I
by	O
returning	O
to	O
the	O
toy	O
example	O
of	O
an	O
inverse	B
problem	I
shown	O
in	O
figure	O
plots	O
of	O
the	O
mixing	O
coefficients	O
kx	O
the	O
means	O
kx	O
and	O
the	O
conditional	B
density	B
contours	O
corresponding	O
to	O
ptx	O
are	O
shown	O
in	O
figure	O
the	O
outputs	O
of	O
the	O
neural	B
network	I
and	O
hence	O
the	O
parameters	O
in	O
the	O
mixture	B
model	I
are	O
necessarily	O
continuous	O
single-valued	O
functions	O
of	O
the	O
input	O
variables	O
however	O
we	O
see	O
from	O
figure	O
that	O
the	O
model	O
is	O
able	O
to	O
produce	O
a	O
conditional	B
density	B
that	O
is	O
unimodal	O
for	O
some	O
values	O
of	O
x	O
and	O
trimodal	O
for	O
other	O
values	O
by	O
modulating	O
the	O
amplitudes	O
of	O
the	O
mixing	O
components	O
kx	O
once	O
a	O
mixture	B
density	B
network	I
has	O
been	O
trained	O
it	O
can	O
predict	O
the	O
conditional	B
density	B
function	O
of	O
the	O
target	O
data	O
for	O
any	O
given	O
value	O
of	O
the	O
input	O
vector	O
this	O
conditional	B
density	B
represents	O
a	O
complete	O
description	O
of	O
the	O
generator	O
of	O
the	O
data	O
so	O
far	O
as	O
the	O
problem	O
of	O
predicting	O
the	O
value	O
of	O
the	O
output	O
vector	O
is	O
concerned	O
from	O
this	O
density	B
function	O
we	O
can	O
calculate	O
more	O
specific	O
quantities	O
that	O
may	O
be	O
of	O
interest	O
in	O
different	O
applications	O
one	O
of	O
the	O
simplest	O
of	O
these	O
is	O
the	O
mean	B
corresponding	O
to	O
the	O
conditional	B
average	O
of	O
the	O
target	O
data	O
and	O
is	O
given	O
by	O
e	O
tptx	O
dt	O
kx	O
kx	O
bayesian	B
neural	O
networks	O
where	O
we	O
have	O
used	O
because	O
a	O
standard	O
network	O
trained	O
by	O
least	O
squares	O
is	O
approximating	O
the	O
conditional	B
mean	B
we	O
see	O
that	O
a	O
mixture	B
density	B
network	I
can	O
reproduce	O
the	O
conventional	O
least-squares	O
result	O
as	O
a	O
special	O
case	O
of	O
course	O
as	O
we	O
have	O
already	O
noted	O
for	O
a	O
multimodal	O
distribution	O
the	O
conditional	B
mean	B
is	O
of	O
limited	O
value	O
we	O
can	O
similarly	O
evaluate	O
the	O
variance	B
of	O
the	O
density	B
function	O
about	O
the	O
condi	O
kx	O
kx	O
kx	O
lx	O
lx	O
exercise	O
tional	O
average	O
to	O
give	O
e	O
where	O
we	O
have	O
used	O
and	O
this	O
is	O
more	O
general	O
than	O
the	O
corresponding	O
least-squares	O
result	O
because	O
the	O
variance	B
is	O
a	O
function	O
of	O
x	O
we	O
have	O
seen	O
that	O
for	O
multimodal	O
distributions	O
the	O
conditional	B
mean	B
can	O
give	O
a	O
poor	O
representation	O
of	O
the	O
data	O
for	O
instance	O
in	O
controlling	O
the	O
simple	O
robot	B
arm	I
shown	O
in	O
figure	O
we	O
need	O
to	O
pick	O
one	O
of	O
the	O
two	O
possible	O
joint	O
angle	O
settings	O
in	O
order	O
to	O
achieve	O
the	O
desired	O
end-effector	O
location	O
whereas	O
the	O
average	O
of	O
the	O
two	O
solutions	O
is	O
not	O
itself	O
a	O
solution	O
in	O
such	O
cases	O
the	O
conditional	B
mode	O
may	O
be	O
of	O
more	O
value	O
because	O
the	O
conditional	B
mode	O
for	O
the	O
mixture	B
density	B
network	I
does	O
not	O
have	O
a	O
simple	O
analytical	O
solution	O
this	O
would	O
require	O
numerical	O
iteration	O
a	O
simple	O
alternative	O
is	O
to	O
take	O
the	O
mean	B
of	O
the	O
most	O
probable	O
component	O
the	O
one	O
with	O
the	O
largest	O
mixing	B
coefficient	I
at	O
each	O
value	O
of	O
x	O
this	O
is	O
shown	O
for	O
the	O
toy	O
data	O
set	O
in	O
figure	O
bayesian	B
neural	O
networks	O
so	O
far	O
our	O
discussion	O
of	O
neural	O
networks	O
has	O
focussed	O
on	O
the	O
use	O
of	O
maximum	B
likelihood	I
to	O
determine	O
the	O
network	O
parameters	O
and	O
biases	O
regularized	O
maximum	B
likelihood	I
can	O
be	O
interpreted	O
as	O
a	O
map	O
posterior	O
approach	O
in	O
which	O
the	O
regularizer	O
can	O
be	O
viewed	O
as	O
the	O
logarithm	O
of	O
a	O
prior	B
parameter	O
distribution	O
however	O
in	O
a	O
bayesian	B
treatment	O
we	O
need	O
to	O
marginalize	O
over	O
the	O
distribution	O
of	O
parameters	O
in	O
order	O
to	O
make	O
predictions	O
in	O
section	O
we	O
developed	O
a	O
bayesian	B
solution	O
for	O
a	O
simple	O
linear	B
regression	B
model	O
under	O
the	O
assumption	O
of	O
gaussian	B
noise	O
we	O
saw	O
that	O
the	O
posterior	O
distribution	O
which	O
is	O
gaussian	B
could	O
be	O
evaluated	O
exactly	O
and	O
that	O
the	O
predictive	B
distribution	I
could	O
also	O
be	O
found	O
in	O
closed	O
form	O
in	O
the	O
case	O
of	O
a	O
multilayered	O
network	O
the	O
highly	O
nonlinear	O
dependence	O
of	O
the	O
network	O
function	O
on	O
the	O
parameter	O
values	O
means	O
that	O
an	O
exact	O
bayesian	B
treatment	O
can	O
no	O
longer	O
be	O
found	O
in	O
fact	O
the	O
log	O
of	O
the	O
posterior	O
distribution	O
will	O
be	O
nonconvex	O
corresponding	O
to	O
the	O
multiple	O
local	B
minima	O
in	O
the	O
error	B
function	I
the	O
technique	O
of	O
variational	B
inference	B
to	O
be	O
discussed	O
in	O
chapter	O
has	O
been	O
applied	O
to	O
bayesian	B
neural	O
networks	O
using	O
a	O
factorized	O
gaussian	B
approximation	O
neural	O
networks	O
to	O
the	O
posterior	O
distribution	O
and	O
van	O
camp	O
and	O
also	O
using	O
a	O
fullcovariance	O
gaussian	B
and	O
bishop	O
barber	O
and	O
bishop	O
the	O
most	O
complete	O
treatment	O
however	O
has	O
been	O
based	O
on	O
the	O
laplace	B
approximation	I
mackay	O
and	O
forms	O
the	O
basis	O
for	O
the	O
discussion	O
given	O
here	O
we	O
will	O
approximate	O
the	O
posterior	O
distribution	O
by	O
a	O
gaussian	B
centred	O
at	O
a	O
mode	O
of	O
the	O
true	O
posterior	O
furthermore	O
we	O
shall	O
assume	O
that	O
the	O
covariance	B
of	O
this	O
gaussian	B
is	O
small	O
so	O
that	O
the	O
network	O
function	O
is	O
approximately	O
linear	O
with	O
respect	O
to	O
the	O
parameters	O
over	O
the	O
region	O
of	O
parameter	O
space	O
for	O
which	O
the	O
posterior	B
probability	B
is	O
significantly	O
nonzero	O
with	O
these	O
two	O
approximations	O
we	O
will	O
obtain	O
models	O
that	O
are	O
analogous	O
to	O
the	O
linear	B
regression	B
and	O
classification	B
models	O
discussed	O
in	O
earlier	O
chapters	O
and	O
so	O
we	O
can	O
exploit	O
the	O
results	O
obtained	O
there	O
we	O
can	O
then	O
make	O
use	O
of	O
the	O
evidence	O
framework	O
to	O
provide	O
point	O
estimates	O
for	O
the	O
hyperparameters	O
and	O
to	O
compare	O
alternative	O
models	O
example	O
networks	O
having	O
different	O
numbers	O
of	O
hidden	O
units	O
to	O
start	O
with	O
we	O
shall	O
discuss	O
the	O
regression	B
case	O
and	O
then	O
later	O
consider	O
the	O
modifications	O
needed	O
for	O
solving	O
classification	B
tasks	O
posterior	O
parameter	O
distribution	O
consider	O
the	O
problem	O
of	O
predicting	O
a	O
single	O
continuous	O
target	O
variable	O
t	O
from	O
a	O
vector	O
x	O
of	O
inputs	O
extension	O
to	O
multiple	O
targets	O
is	O
straightforward	O
we	O
shall	O
suppose	O
that	O
the	O
conditional	B
distribution	O
ptx	O
is	O
gaussian	B
with	O
an	O
x-dependent	O
mean	B
given	O
by	O
the	O
output	O
of	O
a	O
neural	B
network	I
model	O
yx	O
w	O
and	O
with	O
precision	O
variance	B
ptx	O
w	O
n	O
w	O
similarly	O
we	O
shall	O
choose	O
a	O
prior	B
distribution	O
over	O
the	O
weights	O
w	O
that	O
is	O
gaussian	B
of	O
the	O
form	O
for	O
an	O
i	O
i	O
d	O
data	O
set	O
of	O
n	O
observations	O
xn	O
with	O
a	O
corresponding	O
set	O
of	O
target	O
values	O
d	O
tn	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
pw	O
n	O
pdw	O
n	O
w	O
and	O
so	O
the	O
resulting	O
posterior	O
distribution	O
is	O
then	O
pwd	O
pw	O
which	O
as	O
a	O
consequence	O
of	O
the	O
nonlinear	O
dependence	O
of	O
yx	O
w	O
on	O
w	O
will	O
be	O
nongaussian	O
we	O
can	O
find	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
by	O
using	O
the	O
laplace	B
approximation	I
to	O
do	O
this	O
we	O
must	O
first	O
find	O
a	O
maximum	O
of	O
the	O
posterior	O
and	O
this	O
must	O
be	O
done	O
using	O
iterative	O
numerical	O
optimization	O
as	O
usual	O
it	O
is	O
convenient	O
to	O
maximize	O
the	O
logarithm	O
of	O
the	O
posterior	O
which	O
can	O
be	O
written	O
in	O
the	O
bayesian	B
neural	O
networks	O
form	O
ln	O
pwd	O
wtw	O
w	O
const	O
which	O
corresponds	O
to	O
a	O
regularized	O
sum-of-squares	B
error	B
function	I
assuming	O
for	O
the	O
moment	O
that	O
and	O
are	O
fixed	O
we	O
can	O
find	O
a	O
maximum	O
of	O
the	O
posterior	O
which	O
we	O
denote	O
wmap	O
by	O
standard	O
nonlinear	O
optimization	O
algorithms	O
such	O
as	O
conjugate	B
gradients	O
using	O
error	B
backpropagation	B
to	O
evaluate	O
the	O
required	O
derivatives	O
having	O
found	O
a	O
mode	O
wmap	O
we	O
can	O
then	O
build	O
a	O
local	B
gaussian	B
approximation	O
by	O
evaluating	O
the	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
posterior	O
distribution	O
from	O
this	O
is	O
given	O
by	O
a	O
ln	O
pwd	O
i	O
h	O
where	O
h	O
is	O
the	O
hessian	B
matrix	I
comprising	O
the	O
second	O
derivatives	O
of	O
the	O
sum-ofsquares	O
error	B
function	I
with	O
respect	O
to	O
the	O
components	O
of	O
w	O
algorithms	O
for	O
computing	O
and	O
approximating	O
the	O
hessian	O
were	O
discussed	O
in	O
section	O
the	O
corresponding	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
is	O
then	O
given	O
from	O
by	O
qwd	O
n	O
a	O
similarly	O
the	O
predictive	B
distribution	I
is	O
obtained	O
by	O
marginalizing	O
with	O
respect	O
to	O
this	O
posterior	O
distribution	O
ptxd	O
ptx	O
wqwd	O
dw	O
however	O
even	O
with	O
the	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
this	O
integration	O
is	O
still	O
analytically	O
intractable	O
due	O
to	O
the	O
nonlinearity	O
of	O
the	O
network	O
function	O
yx	O
w	O
as	O
a	O
function	O
of	O
w	O
to	O
make	O
progress	O
we	O
now	O
assume	O
that	O
the	O
posterior	O
distribution	O
has	O
small	O
variance	B
compared	O
with	O
the	O
characteristic	O
scales	O
of	O
w	O
over	O
which	O
yx	O
w	O
is	O
varying	O
this	O
allows	O
us	O
to	O
make	O
a	O
taylor	O
series	O
expansion	O
of	O
the	O
network	O
function	O
around	O
wmap	O
and	O
retain	O
only	O
the	O
linear	O
terms	O
yx	O
w	O
yx	O
wmap	O
gtw	O
wmap	O
where	O
we	O
have	O
defined	O
g	O
wyx	O
wwwmap	O
with	O
this	O
approximation	O
we	O
now	O
have	O
a	O
linear-gaussian	B
model	I
with	O
a	O
gaussian	B
distribution	O
for	O
pw	O
and	O
a	O
gaussian	B
for	O
ptw	O
whose	O
mean	B
is	O
a	O
linear	O
function	O
of	O
w	O
of	O
the	O
form	O
tyx	O
wmap	O
gtw	O
wmap	O
ptx	O
w	O
exercise	O
we	O
can	O
therefore	O
make	O
use	O
of	O
the	O
general	O
result	O
for	O
the	O
marginal	B
pt	O
to	O
give	O
ptxd	O
tyx	O
wmap	O
neural	O
networks	O
where	O
the	O
input-dependent	O
variance	B
is	O
given	O
by	O
gta	O
we	O
see	O
that	O
the	O
predictive	B
distribution	I
ptxd	O
is	O
a	O
gaussian	B
whose	O
mean	B
is	O
given	O
by	O
the	O
network	O
function	O
yx	O
wmap	O
with	O
the	O
parameter	O
set	O
to	O
their	O
map	O
value	O
the	O
variance	B
has	O
two	O
terms	O
the	O
first	O
of	O
which	O
arises	O
from	O
the	O
intrinsic	O
noise	O
on	O
the	O
target	O
variable	O
whereas	O
the	O
second	O
is	O
an	O
x-dependent	O
term	O
that	O
expresses	O
the	O
uncertainty	O
in	O
the	O
interpolant	O
due	O
to	O
the	O
uncertainty	O
in	O
the	O
model	O
parameters	O
w	O
this	O
should	O
be	O
compared	O
with	O
the	O
corresponding	O
predictive	B
distribution	I
for	O
the	O
linear	B
regression	B
model	O
given	O
by	O
and	O
hyperparameter	B
optimization	O
so	O
far	O
we	O
have	O
assumed	O
that	O
the	O
hyperparameters	O
and	O
are	O
fixed	O
and	O
known	O
we	O
can	O
make	O
use	O
of	O
the	O
evidence	O
framework	O
discussed	O
in	O
section	O
together	O
with	O
the	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
obtained	O
using	O
the	O
laplace	B
approximation	I
to	O
obtain	O
a	O
practical	O
procedure	O
for	O
choosing	O
the	O
values	O
of	O
such	O
hyperparameters	O
the	O
marginal	B
likelihood	I
or	O
evidence	O
for	O
the	O
hyperparameters	O
is	O
obtained	O
by	O
integrating	O
over	O
the	O
network	O
weights	O
pd	O
pdw	O
dw	O
exercise	O
this	O
is	O
easily	O
evaluated	O
by	O
making	O
use	O
of	O
the	O
laplace	B
approximation	I
result	O
taking	O
logarithms	O
then	O
gives	O
ln	O
pd	O
ewmap	O
lna	O
w	O
ln	O
n	O
ln	O
n	O
where	O
w	O
is	O
the	O
total	O
number	O
of	O
parameters	O
in	O
w	O
and	O
the	O
regularized	O
error	B
function	I
is	O
defined	O
by	O
ewmap	O
wmap	O
wt	O
mapwmap	O
we	O
see	O
that	O
this	O
takes	O
the	O
same	O
form	O
as	O
the	O
corresponding	O
result	O
for	O
the	O
linear	B
regression	B
model	O
in	O
the	O
evidence	O
framework	O
we	O
make	O
point	O
estimates	O
for	O
and	O
by	O
maximizing	O
ln	O
pd	O
consider	O
first	O
the	O
maximization	O
with	O
respect	O
to	O
which	O
can	O
be	O
done	O
by	O
analogy	O
with	O
the	O
linear	B
regression	B
case	O
discussed	O
in	O
section	O
we	O
first	O
define	O
the	O
eigenvalue	O
equation	O
where	O
h	O
is	O
the	O
hessian	B
matrix	I
comprising	O
the	O
second	O
derivatives	O
of	O
the	O
sum-ofsquares	O
error	B
function	I
evaluated	O
at	O
w	O
wmap	O
by	O
analogy	O
with	O
we	O
obtain	O
hui	O
iui	O
wt	O
mapwmap	O
section	O
section	O
bayesian	B
neural	O
networks	O
where	O
represents	O
the	O
effective	B
number	I
of	I
parameters	I
and	O
is	O
defined	O
by	O
i	O
i	O
note	O
that	O
this	O
result	O
was	O
exact	O
for	O
the	O
linear	B
regression	B
case	O
for	O
the	O
nonlinear	O
neural	B
network	I
however	O
it	O
ignores	O
the	O
fact	O
that	O
changes	O
in	O
will	O
cause	O
changes	O
in	O
the	O
hessian	O
h	O
which	O
in	O
turn	O
will	O
change	O
the	O
eigenvalues	O
we	O
have	O
therefore	O
implicitly	O
ignored	O
terms	O
involving	O
the	O
derivatives	O
of	O
i	O
with	O
respect	O
to	O
similarly	O
from	O
we	O
see	O
that	O
maximizing	O
the	O
evidence	O
with	O
respect	O
to	O
gives	O
the	O
re-estimation	O
formula	O
n	O
wmap	O
as	O
with	O
the	O
linear	O
model	O
we	O
need	O
to	O
alternate	O
between	O
re-estimation	O
of	O
the	O
hyperparameters	O
and	O
and	O
updating	O
of	O
the	O
posterior	O
distribution	O
the	O
situation	O
with	O
a	O
neural	B
network	I
model	O
is	O
more	O
complex	O
however	O
due	O
to	O
the	O
multimodality	B
of	O
the	O
posterior	O
distribution	O
as	O
a	O
consequence	O
the	O
solution	O
for	O
wmap	O
found	O
by	O
maximizing	O
the	O
log	O
posterior	O
will	O
depend	O
on	O
the	O
initialization	O
of	O
w	O
solutions	O
that	O
differ	O
only	O
as	O
a	O
consequence	O
of	O
the	O
interchange	O
and	O
sign	O
reversal	O
symmetries	B
in	O
the	O
hidden	O
units	O
are	O
identical	O
so	O
far	O
as	O
predictions	O
are	O
concerned	O
and	O
it	O
is	O
irrelevant	O
which	O
of	O
the	O
equivalent	O
solutions	O
is	O
found	O
however	O
there	O
may	O
be	O
inequivalent	O
solutions	O
as	O
well	O
and	O
these	O
will	O
generally	O
yield	O
different	O
values	O
for	O
the	O
optimized	O
hyperparameters	O
in	O
order	O
to	O
compare	O
different	O
models	O
for	O
example	O
neural	O
networks	O
having	O
different	O
numbers	O
of	O
hidden	O
units	O
we	O
need	O
to	O
evaluate	O
the	O
model	B
evidence	I
pd	O
this	O
can	O
be	O
approximated	O
by	O
taking	O
and	O
substituting	O
the	O
values	O
of	O
and	O
obtained	O
from	O
the	O
iterative	O
optimization	O
of	O
these	O
hyperparameters	O
a	O
more	O
careful	O
evaluation	O
is	O
obtained	O
by	O
marginalizing	O
over	O
and	O
again	O
by	O
making	O
a	O
gaussian	B
approximation	O
bishop	O
in	O
either	O
case	O
it	O
is	O
necessary	O
to	O
evaluate	O
the	O
determinant	O
of	O
the	O
hessian	B
matrix	I
this	O
can	O
be	O
problematic	O
in	O
practice	O
because	O
the	O
determinant	O
unlike	O
the	O
trace	O
is	O
sensitive	O
to	O
the	O
small	O
eigenvalues	O
that	O
are	O
often	O
difficult	O
to	O
determine	O
accurately	O
the	O
laplace	B
approximation	I
is	O
based	O
on	O
a	O
local	B
quadratic	O
expansion	O
around	O
a	O
mode	O
of	O
the	O
posterior	O
distribution	O
over	O
weights	O
we	O
have	O
seen	O
in	O
section	O
that	O
any	O
given	O
mode	O
in	O
a	O
two-layer	O
network	O
is	O
a	O
member	O
of	O
a	O
set	O
of	O
equivalent	O
modes	O
that	O
differ	O
by	O
interchange	O
and	O
sign-change	O
symmetries	B
where	O
m	O
is	O
the	O
number	O
of	O
hidden	O
units	O
when	O
comparing	O
networks	O
having	O
different	O
numbers	O
of	O
hidden	O
units	O
this	O
can	O
be	O
taken	O
into	O
account	O
by	O
multiplying	O
the	O
evidence	O
by	O
a	O
factor	O
of	O
bayesian	B
neural	O
networks	O
for	O
classification	B
so	O
far	O
we	O
have	O
used	O
the	O
laplace	B
approximation	I
to	O
develop	O
a	O
bayesian	B
treatment	O
of	O
neural	B
network	I
regression	B
models	O
we	O
now	O
discuss	O
the	O
modifications	O
to	O
neural	O
networks	O
exercise	O
exercise	O
this	O
framework	O
that	O
arise	O
when	O
it	O
is	O
applied	O
to	O
classification	B
here	O
we	O
shall	O
consider	O
a	O
network	O
having	O
a	O
single	O
logistic	B
sigmoid	I
output	O
corresponding	O
to	O
a	O
two-class	O
classification	B
problem	O
the	O
extension	O
to	O
networks	O
with	O
multiclass	B
softmax	O
outputs	O
is	O
straightforward	O
we	O
shall	O
build	O
extensively	O
on	O
the	O
analogous	O
results	O
for	O
linear	O
classification	B
models	O
discussed	O
in	O
section	O
and	O
so	O
we	O
encourage	O
the	O
reader	O
to	O
familiarize	O
themselves	O
with	O
that	O
material	O
before	O
studying	O
this	O
section	O
the	O
log	O
likelihood	B
function	I
for	O
this	O
model	O
is	O
given	O
by	O
ln	O
pdw	O
ln	O
yn	O
tn	O
yn	O
n	O
where	O
tn	O
are	O
the	O
target	O
values	O
and	O
yn	O
yxn	O
w	O
note	O
that	O
there	O
is	O
no	O
hyperparameter	B
because	O
the	O
data	O
points	O
are	O
assumed	O
to	O
be	O
correctly	O
labelled	O
as	O
before	O
the	O
prior	B
is	O
taken	O
to	O
be	O
an	O
isotropic	B
gaussian	B
of	O
the	O
form	O
the	O
first	O
stage	O
in	O
applying	O
the	O
laplace	B
framework	O
to	O
this	O
model	O
is	O
to	O
initialize	O
the	O
hyperparameter	B
and	O
then	O
to	O
determine	O
the	O
parameter	O
vector	O
w	O
by	O
maximizing	O
the	O
log	O
posterior	O
distribution	O
this	O
is	O
equivalent	O
to	O
minimizing	O
the	O
regularized	O
error	B
function	I
ew	O
ln	O
pdw	O
wtw	O
and	O
can	O
be	O
achieved	O
using	O
error	B
backpropagation	B
combined	O
with	O
standard	O
optimization	O
algorithms	O
as	O
discussed	O
in	O
section	O
having	O
found	O
a	O
solution	O
wmap	O
for	O
the	O
weight	B
vector	I
the	O
next	O
step	O
is	O
to	O
evaluate	O
the	O
hessian	B
matrix	I
h	O
comprising	O
the	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
likelihood	B
function	I
this	O
can	O
be	O
done	O
for	O
instance	O
using	O
the	O
exact	O
method	O
of	O
section	O
or	O
using	O
the	O
outer	B
product	I
approximation	I
given	O
by	O
the	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
posterior	O
can	O
again	O
be	O
written	O
in	O
the	O
form	O
and	O
the	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
is	O
then	O
given	O
by	O
to	O
optimize	O
the	O
hyperparameter	B
we	O
again	O
maximize	O
the	O
marginal	B
likelihood	I
which	O
is	O
easily	O
shown	O
to	O
take	O
the	O
form	O
ln	O
pd	O
ewmap	O
where	O
the	O
regularized	O
error	B
function	I
is	O
defined	O
by	O
ewmap	O
lna	O
w	O
ln	O
const	O
ln	O
yn	O
tn	O
yn	O
wt	O
mapwmap	O
in	O
which	O
yn	O
yxn	O
wmap	O
maximizing	O
this	O
evidence	B
function	I
with	O
respect	O
to	O
again	O
leads	O
to	O
the	O
re-estimation	O
equation	O
given	O
by	O
the	O
use	O
of	O
the	O
evidence	O
procedure	O
to	O
determine	O
is	O
illustrated	O
in	O
figure	O
for	O
the	O
synthetic	O
two-dimensional	O
data	O
discussed	O
in	O
appendix	O
a	O
finally	O
we	O
need	O
the	O
predictive	B
distribution	I
which	O
is	O
defined	O
by	O
again	O
this	O
integration	O
is	O
intractable	O
due	O
to	O
the	O
nonlinearity	O
of	O
the	O
network	O
function	O
the	O
bayesian	B
neural	O
networks	O
figure	O
illustration	O
of	O
the	O
evidence	O
framework	O
applied	O
to	O
a	O
synthetic	O
two-class	O
data	O
set	O
the	O
green	O
curve	O
shows	O
the	O
optimal	O
decision	B
boundary	I
the	O
black	O
curve	O
shows	O
the	O
result	O
of	O
fitting	O
a	O
two-layer	O
network	O
with	O
hidden	O
units	O
by	O
maximum	B
likelihood	I
and	O
the	O
red	O
curve	O
shows	O
the	O
result	O
of	O
including	O
a	O
regularizer	O
in	O
which	O
is	O
optimized	O
using	O
the	O
evidence	O
procedure	O
starting	O
from	O
the	O
initial	O
value	O
note	O
that	O
the	O
evidence	O
procedure	O
greatly	O
reduces	O
the	O
over-fitting	B
of	O
the	O
network	O
simplest	O
approximation	O
is	O
to	O
assume	O
that	O
the	O
posterior	O
distribution	O
is	O
very	O
narrow	O
and	O
hence	O
make	O
the	O
approximation	O
ptxd	O
ptx	O
wmap	O
we	O
can	O
improve	O
on	O
this	O
however	O
by	O
taking	O
account	O
of	O
the	O
variance	B
of	O
the	O
posterior	O
distribution	O
in	O
this	O
case	O
a	O
linear	O
approximation	O
for	O
the	O
network	O
outputs	O
as	O
was	O
used	O
in	O
the	O
case	O
of	O
regression	B
would	O
be	O
inappropriate	O
due	O
to	O
the	O
logistic	B
sigmoid	I
outputunit	O
activation	B
function	I
that	O
constrains	O
the	O
output	O
to	O
lie	O
in	O
the	O
range	O
instead	O
we	O
make	O
a	O
linear	O
approximation	O
for	O
the	O
output	O
unit	O
activation	O
in	O
the	O
form	O
ax	O
w	O
amapx	O
btw	O
wmap	O
where	O
amapx	O
ax	O
wmap	O
and	O
the	O
vector	O
b	O
ax	O
wmap	O
can	O
be	O
found	O
by	O
backpropagation	B
because	O
we	O
now	O
have	O
a	O
gaussian	B
approximation	O
for	O
the	O
posterior	O
distribution	O
over	O
w	O
and	O
a	O
model	O
for	O
a	O
that	O
is	O
a	O
linear	O
function	O
of	O
w	O
we	O
can	O
now	O
appeal	O
to	O
the	O
results	O
of	O
section	O
the	O
distribution	O
of	O
output	O
unit	O
activation	O
values	O
induced	O
by	O
the	O
distribution	O
over	O
network	O
weights	O
is	O
given	O
by	O
paxd	O
a	O
amapx	O
btxw	O
wmap	O
qwd	O
dw	O
where	O
qwd	O
is	O
the	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
given	O
by	O
from	O
section	O
we	O
see	O
that	O
this	O
distribution	O
is	O
gaussian	B
with	O
mean	B
amap	O
ax	O
wmap	O
and	O
variance	B
ax	O
btxa	O
finally	O
to	O
obtain	O
the	O
predictive	B
distribution	I
we	O
must	O
marginalize	O
over	O
a	O
using	O
pt	O
da	O
neural	O
networks	O
figure	O
an	O
illustration	O
of	O
the	O
laplace	B
approximation	I
for	O
a	O
bayesian	B
neural	B
network	I
having	O
hidden	O
units	O
with	O
tanh	O
activation	O
functions	O
and	O
a	O
single	O
logistic-sigmoid	O
output	O
unit	O
the	O
weight	O
parameters	O
were	O
found	O
using	O
scaled	O
conjugate	B
gradients	O
and	O
the	O
hyperparameter	B
was	O
optimized	O
using	O
the	O
evidence	O
framework	O
on	O
the	O
left	O
is	O
the	O
result	O
of	O
using	O
the	O
simple	O
approximation	O
based	O
on	O
a	O
point	O
estimate	O
wmap	O
of	O
the	O
parameters	O
in	O
which	O
the	O
green	O
curve	O
shows	O
the	O
y	O
decision	B
boundary	I
and	O
the	O
other	O
contours	O
correspond	O
to	O
output	O
probabilities	O
of	O
y	O
and	O
on	O
the	O
right	O
is	O
the	O
corresponding	O
result	O
obtained	O
using	O
note	O
that	O
the	O
effect	O
of	O
marginalization	O
is	O
to	O
spread	O
out	O
the	O
contours	O
and	O
to	O
make	O
the	O
predictions	O
less	O
confident	O
so	O
that	O
at	O
each	O
input	O
point	O
x	O
the	O
posterior	O
probabilities	O
are	O
shifted	O
towards	O
while	O
the	O
y	O
contour	O
itself	O
is	O
unaffected	O
the	O
convolution	O
of	O
a	O
gaussian	B
with	O
a	O
logistic	B
sigmoid	I
is	O
intractable	O
we	O
therefore	O
apply	O
the	O
approximation	O
to	O
giving	O
pt	O
abtwmap	O
where	O
is	O
defined	O
by	O
recall	O
that	O
both	O
a	O
and	O
b	O
are	O
functions	O
of	O
x	O
figure	O
shows	O
an	O
example	O
of	O
this	O
framework	O
applied	O
to	O
the	O
synthetic	O
classi	O
fication	O
data	O
set	O
described	O
in	O
appendix	O
a	O
exercises	O
consider	O
a	O
two-layer	O
network	O
function	O
of	O
the	O
form	O
in	O
which	O
the	O
hiddenunit	O
nonlinear	O
activation	O
functions	O
g	O
are	O
given	O
by	O
logistic	B
sigmoid	I
functions	O
of	O
the	O
form	O
exp	O
a	O
show	O
that	O
there	O
exists	O
an	O
equivalent	O
network	O
which	O
computes	O
exactly	O
the	O
same	O
function	O
but	O
with	O
hidden	B
unit	I
activation	O
functions	O
given	O
by	O
tanha	O
where	O
the	O
tanh	O
function	O
is	O
defined	O
by	O
hint	O
first	O
find	O
the	O
relation	O
between	O
and	O
tanha	O
and	O
then	O
show	O
that	O
the	O
parameters	O
of	O
the	O
two	O
networks	O
differ	O
by	O
linear	O
transformations	O
www	O
show	O
that	O
maximizing	O
the	O
likelihood	B
function	I
under	O
the	O
conditional	B
distribution	O
for	O
a	O
multioutput	O
neural	B
network	I
is	O
equivalent	O
to	O
minimizing	O
the	O
sum-of-squares	B
error	B
function	I
exercises	O
consider	O
a	O
regression	B
problem	O
involving	O
multiple	O
target	O
variables	O
in	O
which	O
it	O
is	O
assumed	O
that	O
the	O
distribution	O
of	O
the	O
targets	O
conditioned	O
on	O
the	O
input	O
vector	O
x	O
is	O
a	O
gaussian	B
of	O
the	O
form	O
ptx	O
w	O
n	O
w	O
where	O
yx	O
w	O
is	O
the	O
output	O
of	O
a	O
neural	B
network	I
with	O
input	O
vector	O
x	O
and	O
weight	B
vector	I
w	O
and	O
is	O
the	O
covariance	B
of	O
the	O
assumed	O
gaussian	B
noise	O
on	O
the	O
targets	O
given	O
a	O
set	O
of	O
independent	B
observations	O
of	O
x	O
and	O
t	O
write	O
down	O
the	O
error	B
function	I
that	O
must	O
be	O
minimized	O
in	O
order	O
to	O
find	O
the	O
maximum	B
likelihood	I
solution	O
for	O
w	O
if	O
we	O
assume	O
that	O
is	O
fixed	O
and	O
known	O
now	O
assume	O
that	O
is	O
also	O
to	O
be	O
determined	O
from	O
the	O
data	O
and	O
write	O
down	O
an	O
expression	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
note	O
that	O
the	O
optimizations	O
of	O
w	O
and	O
are	O
now	O
coupled	O
in	O
contrast	O
to	O
the	O
case	O
of	O
independent	B
target	O
variables	O
discussed	O
in	O
section	O
consider	O
a	O
binary	O
classification	B
problem	O
in	O
which	O
the	O
target	O
values	O
are	O
t	O
with	O
a	O
network	O
output	O
yx	O
w	O
that	O
represents	O
pt	O
and	O
suppose	O
that	O
there	O
is	O
a	O
probability	B
that	O
the	O
class	O
label	O
on	O
a	O
training	B
data	O
point	O
has	O
been	O
incorrectly	O
set	O
assuming	O
independent	B
and	O
identically	O
distributed	O
data	O
write	O
down	O
the	O
error	B
function	I
corresponding	O
to	O
the	O
negative	O
log	O
likelihood	O
verify	O
that	O
the	O
error	B
function	I
is	O
obtained	O
when	O
note	O
that	O
this	O
error	B
function	I
makes	O
the	O
model	O
robust	O
to	O
incorrectly	O
labelled	O
data	O
in	O
contrast	O
to	O
the	O
usual	O
error	B
function	I
www	O
show	O
that	O
maximizing	O
likelihood	O
for	O
a	O
multiclass	B
neural	B
network	I
model	O
in	O
which	O
the	O
network	O
outputs	O
have	O
the	O
interpretation	O
ykx	O
w	O
ptk	O
is	O
equivalent	O
to	O
the	O
minimization	O
of	O
the	O
cross-entropy	B
error	B
function	I
www	O
show	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
activation	O
ak	O
for	O
an	O
output	O
unit	O
having	O
a	O
logistic	B
sigmoid	I
activation	B
function	I
satisfies	O
show	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
activation	O
ak	O
for	O
output	O
units	O
having	O
a	O
softmax	O
activation	B
function	I
satisfies	O
we	O
saw	O
in	O
that	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
activation	B
function	I
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
function	O
value	O
itself	O
derive	O
the	O
corresponding	O
result	O
for	O
the	O
tanh	O
activation	B
function	I
defined	O
by	O
www	O
the	O
error	B
function	I
for	O
binary	O
classification	B
problems	O
was	O
derived	O
for	O
a	O
network	O
having	O
a	O
logistic-sigmoid	O
output	O
activation	B
function	I
so	O
that	O
yx	O
w	O
and	O
data	O
having	O
target	O
values	O
t	O
derive	O
the	O
corresponding	O
error	B
function	I
if	O
we	O
consider	O
a	O
network	O
having	O
an	O
output	O
yx	O
w	O
and	O
target	O
values	O
t	O
for	O
class	O
and	O
t	O
for	O
class	O
what	O
would	O
be	O
the	O
appropriate	O
choice	O
of	O
output	O
unit	O
activation	B
function	I
www	O
consider	O
a	O
hessian	B
matrix	I
h	O
with	O
eigenvector	O
equation	O
by	O
setting	O
the	O
vector	O
v	O
in	O
equal	O
to	O
each	O
of	O
the	O
eigenvectors	O
ui	O
in	O
turn	O
show	O
that	O
h	O
is	O
positive	B
definite	I
if	O
and	O
only	O
if	O
all	O
of	O
its	O
eigenvalues	O
are	O
positive	O
neural	O
networks	O
www	O
consider	O
a	O
quadratic	O
error	B
function	I
defined	O
by	O
in	O
which	O
the	O
hessian	B
matrix	I
h	O
has	O
an	O
eigenvalue	O
equation	O
given	O
by	O
show	O
that	O
the	O
contours	O
of	O
constant	O
error	B
are	O
ellipses	O
whose	O
axes	O
are	O
aligned	O
with	O
the	O
eigenvectors	O
ui	O
with	O
lengths	O
that	O
are	O
inversely	O
proportional	O
to	O
the	O
square	O
root	O
of	O
the	O
corresponding	O
eigenvalues	O
i	O
www	O
by	O
considering	O
the	O
local	B
taylor	O
expansion	O
of	O
an	O
error	B
function	I
about	O
a	O
stationary	B
point	O
show	O
that	O
the	O
necessary	O
and	O
sufficient	O
condition	O
for	O
the	O
stationary	B
point	O
to	O
be	O
a	O
local	B
minimum	I
of	O
the	O
error	B
function	I
is	O
that	O
the	O
hessian	B
matrix	I
h	O
defined	O
by	O
be	O
positive	B
definite	I
show	O
that	O
as	O
a	O
consequence	O
of	O
the	O
symmetry	O
of	O
the	O
hessian	B
matrix	I
h	O
the	O
number	O
of	O
independent	B
elements	O
in	O
the	O
quadratic	O
error	B
function	I
is	O
given	O
by	O
w	O
by	O
making	O
a	O
taylor	O
expansion	O
verify	O
that	O
the	O
terms	O
that	O
are	O
o	O
cancel	O
on	O
the	O
right-hand	O
side	O
of	O
in	O
section	O
we	O
derived	O
a	O
procedure	O
for	O
evaluating	O
the	O
jacobian	B
matrix	I
of	O
a	O
neural	B
network	I
using	O
a	O
backpropagation	B
procedure	O
derive	O
an	O
alternative	O
formalism	O
for	O
finding	O
the	O
jacobian	O
based	O
on	O
forward	B
propagation	I
equations	O
the	O
outer	B
product	I
approximation	I
to	O
the	O
hessian	B
matrix	I
for	O
a	O
neural	B
network	I
using	O
a	O
sum-of-squares	B
error	B
function	I
is	O
given	O
by	O
extend	O
this	O
result	O
to	O
the	O
case	O
of	O
multiple	O
outputs	O
consider	O
a	O
squared	O
loss	B
function	I
of	O
the	O
form	O
w	O
e	O
px	O
t	O
dx	O
dt	O
where	O
yx	O
w	O
is	O
a	O
parametric	O
function	O
such	O
as	O
a	O
neural	B
network	I
the	O
result	O
shows	O
that	O
the	O
function	O
yx	O
w	O
that	O
minimizes	O
this	O
error	B
is	O
given	O
by	O
the	O
conditional	B
expectation	B
of	O
t	O
given	O
x	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
second	O
derivative	B
of	O
e	O
with	O
respect	O
to	O
two	O
elements	O
wr	O
and	O
ws	O
of	O
the	O
vector	O
w	O
is	O
given	O
by	O
wr	O
ws	O
y	O
wr	O
y	O
ws	O
px	O
dx	O
note	O
that	O
for	O
a	O
finite	O
sample	O
from	O
px	O
we	O
obtain	O
consider	O
a	O
two-layer	O
network	O
of	O
the	O
form	O
shown	O
in	O
figure	O
with	O
the	O
addition	O
of	O
extra	O
parameters	O
corresponding	O
to	O
skip-layer	O
connections	O
that	O
go	O
directly	O
from	O
the	O
inputs	O
to	O
the	O
outputs	O
by	O
extending	O
the	O
discussion	O
of	O
section	O
write	O
down	O
the	O
equations	O
for	O
the	O
derivatives	O
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
these	O
additional	O
parameters	O
www	O
derive	O
the	O
expression	O
for	O
the	O
outer	B
product	I
approximation	I
to	O
the	O
hessian	B
matrix	I
for	O
a	O
network	O
having	O
a	O
single	O
output	O
with	O
a	O
logistic	B
sigmoid	I
output-unit	O
activation	B
function	I
and	O
a	O
cross-entropy	B
error	B
function	I
corresponding	O
to	O
the	O
result	O
for	O
the	O
sum-of-squares	B
error	B
function	I
exercises	O
derive	O
an	O
expression	O
for	O
the	O
outer	B
product	I
approximation	I
to	O
the	O
hessian	B
matrix	I
for	O
a	O
network	O
having	O
k	O
outputs	O
with	O
a	O
softmax	O
output-unit	O
activation	B
function	I
and	O
a	O
cross-entropy	B
error	B
function	I
corresponding	O
to	O
the	O
result	O
for	O
the	O
sum-ofsquares	O
error	B
function	I
extend	O
the	O
expression	O
for	O
the	O
outer	B
product	I
approximation	I
of	O
the	O
hessian	B
matrix	I
to	O
the	O
case	O
of	O
k	O
output	O
units	O
hence	O
derive	O
a	O
recursive	O
expression	O
analogous	O
to	O
for	O
incrementing	O
the	O
number	O
n	O
of	O
patterns	O
and	O
a	O
similar	O
expression	O
for	O
incrementing	O
the	O
number	O
k	O
of	O
outputs	O
use	O
these	O
results	O
together	O
with	O
the	O
identity	O
to	O
find	O
sequential	O
update	O
expressions	O
analogous	O
to	O
for	O
finding	O
the	O
inverse	B
of	O
the	O
hessian	O
by	O
incrementally	O
including	O
both	O
extra	O
patterns	O
and	O
extra	O
outputs	O
derive	O
the	O
results	O
and	O
for	O
the	O
elements	O
of	O
the	O
hessian	B
matrix	I
of	O
a	O
two-layer	O
feed-forward	O
network	O
by	O
application	O
of	O
the	O
chain	O
rule	O
of	O
calculus	O
extend	O
the	O
results	O
of	O
section	O
for	O
the	O
exact	O
hessian	O
of	O
a	O
two-layer	O
network	O
to	O
include	O
skip-layer	O
connections	O
that	O
go	O
directly	O
from	O
inputs	O
to	O
outputs	O
verify	O
that	O
the	O
network	O
function	O
defined	O
by	O
and	O
is	O
invariant	O
under	O
the	O
transformation	O
applied	O
to	O
the	O
inputs	O
provided	O
the	O
weights	O
and	O
biases	O
are	O
simultaneously	O
transformed	O
using	O
and	O
similarly	O
show	O
that	O
the	O
network	O
outputs	O
can	O
be	O
transformed	O
according	O
by	O
applying	O
the	O
transformation	O
and	O
to	O
the	O
second-layer	O
weights	O
and	O
biases	O
www	O
consider	O
a	O
quadratic	O
error	B
function	I
of	O
the	O
form	O
e	O
where	O
represents	O
the	O
minimum	O
and	O
the	O
hessian	B
matrix	I
h	O
is	O
positive	B
definite	I
and	O
constant	O
suppose	O
the	O
initial	O
weight	B
vector	I
is	O
chosen	O
to	O
be	O
at	O
the	O
origin	O
and	O
is	O
updated	O
using	O
simple	O
gradient	B
descent	I
w	O
w	O
e	O
where	O
denotes	O
the	O
step	O
number	O
and	O
is	O
the	O
learning	B
rate	O
is	O
assumed	O
to	O
be	O
small	O
show	O
that	O
after	O
steps	O
the	O
components	O
of	O
the	O
weight	B
vector	I
parallel	O
to	O
the	O
eigenvectors	O
of	O
h	O
can	O
be	O
written	O
j	O
j	O
w	O
j	O
where	O
wj	O
wtuj	O
and	O
uj	O
and	O
j	O
are	O
the	O
eigenvectors	O
and	O
eigenvalues	O
respectively	O
of	O
h	O
so	O
that	O
show	O
that	O
as	O
this	O
gives	O
w	O
as	O
expected	O
provided	O
j	O
now	O
suppose	O
that	O
training	B
is	O
halted	O
after	O
a	O
finite	O
number	O
of	O
steps	O
show	O
that	O
the	O
huj	O
juj	O
neural	O
networks	O
components	O
of	O
the	O
weight	B
vector	I
parallel	O
to	O
the	O
eigenvectors	O
of	O
the	O
hessian	O
satisfy	O
j	O
j	O
when	O
j	O
j	O
when	O
j	O
w	O
j	O
compare	O
this	O
result	O
with	O
the	O
discussion	O
in	O
section	O
of	O
regularization	B
with	O
simple	O
weight	B
decay	I
and	O
hence	O
show	O
that	O
is	O
analogous	O
to	O
the	O
regularization	B
parameter	O
the	O
above	O
results	O
also	O
show	O
that	O
the	O
effective	B
number	I
of	I
parameters	I
in	O
the	O
network	O
as	O
defined	O
by	O
grows	O
as	O
the	O
training	B
progresses	O
consider	O
a	O
multilayer	B
perceptron	B
with	O
arbitrary	O
feed-forward	O
topology	O
which	O
is	O
to	O
be	O
trained	O
by	O
minimizing	O
the	O
tangent	B
propagation	I
error	B
function	I
in	O
which	O
the	O
regularizing	O
function	O
is	O
given	O
by	O
show	O
that	O
the	O
regularization	B
term	O
can	O
be	O
written	O
as	O
a	O
sum	O
over	O
patterns	O
of	O
terms	O
of	O
the	O
form	O
where	O
g	O
is	O
a	O
differential	B
operator	O
defined	O
by	O
n	O
k	O
g	O
xi	O
i	O
i	O
by	O
acting	O
on	O
the	O
forward	B
propagation	I
equations	O
zj	O
haj	O
with	O
the	O
operator	O
g	O
show	O
that	O
n	O
can	O
be	O
evaluated	O
by	O
forward	B
propagation	I
using	O
the	O
following	O
equations	O
wjizi	O
aj	O
i	O
j	O
h	O
j	O
j	O
wji	O
i	O
i	O
where	O
we	O
have	O
defined	O
the	O
new	O
variables	O
j	O
gzj	O
j	O
gaj	O
now	O
show	O
that	O
the	O
derivatives	O
of	O
n	O
with	O
respect	O
to	O
a	O
weight	O
wrs	O
in	O
the	O
network	O
can	O
be	O
written	O
in	O
the	O
form	O
k	O
n	O
wrs	O
where	O
we	O
have	O
defined	O
k	O
krzs	O
kr	O
s	O
kr	O
yk	O
ar	O
kr	O
g	O
kr	O
write	O
down	O
the	O
backpropagation	B
equations	O
for	O
kr	O
and	O
hence	O
derive	O
a	O
set	O
of	O
backpropagation	B
equations	O
for	O
the	O
evaluation	O
of	O
the	O
kr	O
exercises	O
www	O
consider	O
the	O
framework	O
for	O
training	B
with	O
transformed	O
data	O
in	O
the	O
special	O
case	O
in	O
which	O
the	O
transformation	O
consists	O
simply	O
of	O
the	O
addition	O
of	O
random	O
noise	O
x	O
x	O
where	O
has	O
a	O
gaussian	B
distribution	O
with	O
zero	O
mean	B
and	O
unit	O
covariance	B
by	O
following	O
an	O
argument	O
analogous	O
to	O
that	O
of	O
section	O
show	O
that	O
the	O
resulting	O
regularizer	O
reduces	O
to	O
the	O
tikhonov	B
form	O
www	O
consider	O
a	O
neural	B
network	I
such	O
as	O
the	O
convolutional	B
network	O
discussed	O
in	O
section	O
in	O
which	O
multiple	O
weights	O
are	O
constrained	O
to	O
have	O
the	O
same	O
value	O
discuss	O
how	O
the	O
standard	O
backpropagation	B
algorithm	O
must	O
be	O
modified	O
in	O
order	O
to	O
ensure	O
that	O
such	O
constraints	O
are	O
satisfied	O
when	O
evaluating	O
the	O
derivatives	O
of	O
an	O
error	B
function	I
with	O
respect	O
to	O
the	O
adjustable	O
parameters	O
in	O
the	O
network	O
www	O
verify	O
the	O
result	O
verify	O
the	O
result	O
verify	O
the	O
result	O
show	O
that	O
the	O
derivatives	O
of	O
the	O
mixing	O
coefficients	O
k	O
defined	O
by	O
with	O
respect	O
to	O
the	O
auxiliary	O
parameters	O
j	O
are	O
given	O
by	O
jk	O
j	O
j	O
k	O
k	O
j	O
hence	O
by	O
making	O
use	O
of	O
the	O
constraint	O
k	O
k	O
derive	O
the	O
result	O
write	O
down	O
a	O
pair	O
of	O
equations	O
that	O
express	O
the	O
cartesian	O
coordinates	O
for	O
the	O
robot	B
arm	I
shown	O
in	O
figure	O
in	O
terms	O
of	O
the	O
joint	O
angles	O
and	O
and	O
the	O
lengths	O
and	O
of	O
the	O
links	O
assume	O
the	O
origin	O
of	O
the	O
coordinate	O
system	O
is	O
given	O
by	O
the	O
attachment	O
point	O
of	O
the	O
lower	O
arm	O
these	O
equations	O
define	O
the	O
forward	B
kinematics	I
of	O
the	O
robot	B
arm	I
www	O
derive	O
the	O
result	O
for	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
network	O
output	O
activations	O
controlling	O
the	O
mixing	O
coefficients	O
in	O
the	O
mixture	B
density	B
network	I
derive	O
the	O
result	O
for	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
network	O
output	O
activations	O
controlling	O
the	O
component	O
means	O
in	O
the	O
mixture	B
density	B
network	I
derive	O
the	O
result	O
for	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
network	O
output	O
activations	O
controlling	O
the	O
component	O
variances	O
in	O
the	O
mixture	B
density	B
network	I
verify	O
the	O
results	O
and	O
for	O
the	O
conditional	B
mean	B
and	O
variance	B
of	O
the	O
mixture	B
density	B
network	I
model	O
using	O
the	O
general	O
result	O
derive	O
the	O
predictive	B
distribution	I
for	O
the	O
laplace	B
approximation	I
to	O
the	O
bayesian	B
neural	B
network	I
model	O
neural	O
networks	O
www	O
make	O
use	O
of	O
the	O
laplace	B
approximation	I
result	O
to	O
show	O
that	O
the	O
evidence	B
function	I
for	O
the	O
hyperparameters	O
and	O
in	O
the	O
bayesian	B
neural	B
network	I
model	O
can	O
be	O
approximated	O
by	O
www	O
outline	O
the	O
modifications	O
needed	O
to	O
the	O
framework	O
for	O
bayesian	B
neural	O
networks	O
discussed	O
in	O
section	O
to	O
handle	O
multiclass	B
problems	O
using	O
networks	O
having	O
softmax	O
output-unit	O
activation	O
functions	O
by	O
following	O
analogous	O
steps	O
to	O
those	O
given	O
in	O
section	O
for	B
regression	B
networks	O
derive	O
the	O
result	O
for	O
the	O
marginal	B
likelihood	I
in	O
the	O
case	O
of	O
a	O
network	O
having	O
a	O
cross-entropy	B
error	B
function	I
and	O
logistic-sigmoid	O
output-unit	O
activation	B
function	I
kernel	O
methods	O
in	O
chapters	O
and	O
we	O
considered	O
linear	O
parametric	O
models	O
for	B
regression	B
and	O
classification	B
in	O
which	O
the	O
form	O
of	O
the	O
mapping	O
yx	O
w	O
from	O
input	O
x	O
to	O
output	O
y	O
is	O
governed	O
by	O
a	O
vector	O
w	O
of	O
adaptive	O
parameters	O
during	O
the	O
learning	B
phase	O
a	O
set	O
of	O
training	B
data	O
is	O
used	O
either	O
to	O
obtain	O
a	O
point	O
estimate	O
of	O
the	O
parameter	O
vector	O
or	O
to	O
determine	O
a	O
posterior	O
distribution	O
over	O
this	O
vector	O
the	O
training	B
data	O
is	O
then	O
discarded	O
and	O
predictions	O
for	O
new	O
inputs	O
are	O
based	O
purely	O
on	O
the	O
learned	O
parameter	O
vector	O
w	O
this	O
approach	O
is	O
also	O
used	O
in	O
nonlinear	O
parametric	O
models	O
such	O
as	O
neural	O
networks	O
however	O
there	O
is	O
a	O
class	O
of	O
pattern	O
recognition	O
techniques	O
in	O
which	O
the	O
training	B
data	O
points	O
or	O
a	O
subset	O
of	O
them	O
are	O
kept	O
and	O
used	O
also	O
during	O
the	O
prediction	O
phase	O
for	O
instance	O
the	O
parzen	O
probability	B
density	B
model	O
comprised	O
a	O
linear	O
combination	O
of	O
kernel	O
functions	O
each	O
one	O
centred	O
on	O
one	O
of	O
the	O
training	B
data	O
points	O
similarly	O
in	O
section	O
we	O
introduced	O
a	O
simple	O
technique	O
for	O
classification	B
called	O
nearest	O
neighbours	O
which	O
involved	O
assigning	O
to	O
each	O
new	O
test	O
vector	O
the	O
same	O
label	O
as	O
the	O
chapter	O
section	O
kernel	O
methods	O
closest	O
example	O
from	O
the	O
training	B
set	I
these	O
are	O
examples	O
of	O
memory-based	B
methods	I
that	O
involve	O
storing	O
the	O
entire	O
training	B
set	I
in	O
order	O
to	O
make	O
predictions	O
for	O
future	O
data	O
points	O
they	O
typically	O
require	O
a	O
metric	O
to	O
be	O
defined	O
that	O
measures	O
the	O
similarity	O
of	O
any	O
two	O
vectors	O
in	O
input	O
space	O
and	O
are	O
generally	O
fast	O
to	O
train	O
but	O
slow	O
at	O
making	O
predictions	O
for	O
test	O
data	O
points	O
many	O
linear	O
parametric	O
models	O
can	O
be	O
re-cast	O
into	O
an	O
equivalent	O
dual	B
representation	I
in	O
which	O
the	O
predictions	O
are	O
also	O
based	O
on	O
linear	O
combinations	O
of	O
a	O
kernel	B
function	I
evaluated	O
at	O
the	O
training	B
data	O
points	O
as	O
we	O
shall	O
see	O
for	O
models	O
which	O
are	O
based	O
on	O
a	O
fixed	O
nonlinear	O
feature	B
space	I
mapping	O
the	O
kernel	B
function	I
is	O
given	O
by	O
the	O
relation	O
kx	O
from	O
this	O
definition	O
we	O
see	O
that	O
the	O
kernel	O
is	O
a	O
symmetric	O
function	O
of	O
its	O
arguments	O
so	O
that	O
kx	O
x	O
the	O
kernel	O
concept	O
was	O
introduced	O
into	O
the	O
field	O
of	O
pattern	O
recognition	O
by	O
aizerman	O
et	O
al	O
in	O
the	O
context	O
of	O
the	O
method	O
of	O
potential	O
functions	O
so-called	O
because	O
of	O
an	O
analogy	O
with	O
electrostatics	O
although	O
neglected	O
for	O
many	O
years	O
it	O
was	O
re-introduced	O
into	O
machine	O
learning	B
in	O
the	O
context	O
of	O
largemargin	O
classifiers	O
by	O
boser	O
et	O
al	O
giving	O
rise	O
to	O
the	O
technique	O
of	O
support	B
vector	I
machines	O
since	O
then	O
there	O
has	O
been	O
considerable	O
interest	O
in	O
this	O
topic	O
both	O
in	O
terms	O
of	O
theory	B
and	O
applications	O
one	O
of	O
the	O
most	O
significant	O
developments	O
has	O
been	O
the	O
extension	O
of	O
kernels	O
to	O
handle	O
symbolic	O
objects	O
thereby	O
greatly	O
expanding	O
the	O
range	O
of	O
problems	O
that	O
can	O
be	O
addressed	O
the	O
simplest	O
example	O
of	O
a	O
kernel	B
function	I
is	O
obtained	O
by	O
considering	O
the	O
identity	O
mapping	O
for	O
the	O
feature	B
space	I
in	O
so	O
that	O
x	O
in	O
which	O
case	O
kx	O
we	O
shall	O
refer	O
to	O
this	O
as	O
the	O
linear	O
kernel	O
the	O
concept	O
of	O
a	O
kernel	O
formulated	O
as	O
an	O
inner	O
product	O
in	O
a	O
feature	B
space	I
allows	O
us	O
to	O
build	O
interesting	O
extensions	O
of	O
many	O
well-known	O
algorithms	O
by	O
making	O
use	O
of	O
the	O
kernel	B
trick	I
also	O
known	O
as	O
kernel	B
substitution	I
the	O
general	O
idea	O
is	O
that	O
if	O
we	O
have	O
an	O
algorithm	O
formulated	O
in	O
such	O
a	O
way	O
that	O
the	O
input	O
vector	O
x	O
enters	O
only	O
in	O
the	O
form	O
of	O
scalar	O
products	O
then	O
we	O
can	O
replace	O
that	O
scalar	O
product	O
with	O
some	O
other	O
choice	O
of	O
kernel	O
for	O
instance	O
the	O
technique	O
of	O
kernel	B
substitution	I
can	O
be	O
applied	O
to	O
principal	B
component	I
analysis	I
in	O
order	O
to	O
develop	O
a	O
nonlinear	O
variant	O
of	O
pca	O
olkopf	O
et	O
al	O
other	O
examples	O
of	O
kernel	B
substitution	I
include	O
nearest-neighbour	O
classifiers	O
and	O
the	O
kernel	O
fisher	B
discriminant	O
et	O
al	O
roth	O
and	O
steinhage	O
baudat	O
and	O
anouar	O
there	O
are	O
numerous	O
forms	O
of	O
kernel	O
functions	O
in	O
common	O
use	O
and	O
we	O
shall	O
encounter	O
several	O
examples	O
in	O
this	O
chapter	O
many	O
have	O
the	O
property	O
of	O
being	O
a	O
function	O
only	O
of	O
the	O
difference	O
between	O
the	O
arguments	O
so	O
that	O
kx	O
kx	O
which	O
are	O
known	O
as	O
stationary	B
kernels	O
because	O
they	O
are	O
invariant	O
to	O
translations	O
in	O
input	O
space	O
a	O
further	O
specialization	O
involves	O
homogeneous	B
kernels	O
also	O
known	O
as	O
radial	O
basis	O
functions	O
which	O
depend	O
only	O
on	O
the	O
magnitude	O
of	O
the	O
distance	O
euclidean	O
between	O
the	O
arguments	O
so	O
that	O
kx	O
for	O
recent	O
textbooks	O
on	O
kernel	O
methods	O
see	O
sch	O
olkopf	O
and	O
smola	O
her	O
brich	O
and	O
shawe-taylor	O
and	O
cristianini	O
chapter	O
section	O
section	O
dual	O
representations	O
dual	O
representations	O
many	O
linear	O
models	O
for	B
regression	B
and	O
classification	B
can	O
be	O
reformulated	O
in	O
terms	O
of	O
a	O
dual	B
representation	I
in	O
which	O
the	O
kernel	B
function	I
arises	O
naturally	O
this	O
concept	O
will	O
play	O
an	O
important	O
role	O
when	O
we	O
consider	O
support	B
vector	I
machines	O
in	O
the	O
next	O
chapter	O
here	O
we	O
consider	O
a	O
linear	B
regression	B
model	O
whose	O
parameters	O
are	O
determined	O
by	O
minimizing	O
a	O
regularized	O
sum-of-squares	B
error	B
function	I
given	O
by	O
jw	O
wt	O
tn	O
wtw	O
where	O
if	O
we	O
set	O
the	O
gradient	O
of	O
jw	O
with	O
respect	O
to	O
w	O
equal	O
to	O
zero	O
we	O
see	O
that	O
the	O
solution	O
for	O
w	O
takes	O
the	O
form	O
of	O
a	O
linear	O
combination	O
of	O
the	O
vectors	O
with	O
coefficients	O
that	O
are	O
functions	O
of	O
w	O
of	O
the	O
form	O
w	O
wt	O
tn	O
an	O
ta	O
where	O
is	O
the	O
design	B
matrix	I
whose	O
nth	O
row	O
is	O
given	O
by	O
here	O
the	O
vector	O
a	O
an	O
and	O
we	O
have	O
defined	O
an	O
wt	O
tn	O
instead	O
of	O
working	O
with	O
the	O
parameter	O
vector	O
w	O
we	O
can	O
now	O
reformulate	O
the	O
leastsquares	O
algorithm	O
in	O
terms	O
of	O
the	O
parameter	O
vector	O
a	O
giving	O
rise	O
to	O
a	O
dual	B
representation	I
if	O
we	O
substitute	O
w	O
ta	O
into	O
jw	O
we	O
obtain	O
ja	O
at	O
t	O
ta	O
at	O
tt	O
ttt	O
at	O
ta	O
where	O
t	O
tn	O
we	O
now	O
define	O
the	O
gram	B
matrix	I
k	O
t	O
which	O
is	O
an	O
n	O
n	O
symmetric	O
matrix	O
with	O
elements	O
knm	O
kxn	O
xm	O
where	O
we	O
have	O
introduced	O
the	O
kernel	B
function	I
kx	O
defined	O
by	O
in	O
terms	O
of	O
the	O
gram	B
matrix	I
the	O
sum-of-squares	B
error	B
function	I
can	O
be	O
written	O
as	O
ja	O
atkka	O
atkt	O
ttt	O
atka	O
setting	O
the	O
gradient	O
of	O
ja	O
with	O
respect	O
to	O
a	O
to	O
zero	O
we	O
obtain	O
the	O
following	O
solution	O
a	O
in	O
t	O
kernel	O
methods	O
if	O
we	O
substitute	O
this	O
back	O
into	O
the	O
linear	B
regression	B
model	O
we	O
obtain	O
the	O
following	O
prediction	O
for	O
a	O
new	O
input	O
x	O
yx	O
wt	O
at	O
kxt	O
in	O
t	O
where	O
we	O
have	O
defined	O
the	O
vector	O
kx	O
with	O
elements	O
knx	O
kxn	O
x	O
thus	O
we	O
see	O
that	O
the	O
dual	O
formulation	O
allows	O
the	O
solution	O
to	O
the	O
least-squares	O
problem	O
to	O
be	O
expressed	O
entirely	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
kx	O
this	O
is	O
known	O
as	O
a	O
dual	O
formulation	O
because	O
by	O
noting	O
that	O
the	O
solution	O
for	O
a	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
combination	O
of	O
the	O
elements	O
of	O
we	O
recover	O
the	O
original	O
formulation	O
in	O
terms	O
of	O
the	O
parameter	O
vector	O
w	O
note	O
that	O
the	O
prediction	O
at	O
x	O
is	O
given	O
by	O
a	O
linear	O
combination	O
of	O
the	O
target	O
values	O
from	O
the	O
training	B
set	I
in	O
fact	O
we	O
have	O
already	O
obtained	O
this	O
result	O
using	O
a	O
slightly	O
different	O
notation	O
in	O
section	O
in	O
the	O
dual	O
formulation	O
we	O
determine	O
the	O
parameter	O
vector	O
a	O
by	O
inverting	O
an	O
n	O
n	O
matrix	O
whereas	O
in	O
the	O
original	O
parameter	O
space	O
formulation	O
we	O
had	O
to	O
invert	O
an	O
m	O
m	O
matrix	O
in	O
order	O
to	O
determine	O
w	O
because	O
n	O
is	O
typically	O
much	O
larger	O
than	O
m	O
the	O
dual	O
formulation	O
does	O
not	O
seem	O
to	O
be	O
particularly	O
useful	O
however	O
the	O
advantage	O
of	O
the	O
dual	O
formulation	O
as	O
we	O
shall	O
see	O
is	O
that	O
it	O
is	O
expressed	O
entirely	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
kx	O
we	O
can	O
therefore	O
work	O
directly	O
in	O
terms	O
of	O
kernels	O
and	O
avoid	O
the	O
explicit	O
introduction	O
of	O
the	O
feature	O
vector	O
which	O
allows	O
us	O
implicitly	O
to	O
use	O
feature	O
spaces	O
of	O
high	O
even	O
infinite	O
dimensionality	O
the	O
existence	O
of	O
a	O
dual	B
representation	I
based	O
on	O
the	O
gram	B
matrix	I
is	O
a	O
property	O
of	O
many	O
linear	O
models	O
including	O
the	O
perceptron	B
in	O
section	O
we	O
will	O
develop	O
a	O
duality	O
between	O
probabilistic	O
linear	O
models	O
for	B
regression	B
and	O
the	O
technique	O
of	O
gaussian	B
processes	O
duality	O
will	O
also	O
play	O
an	O
important	O
role	O
when	O
we	O
discuss	O
support	B
vector	I
machines	O
in	O
chapter	O
exercise	O
exercise	O
constructing	O
kernels	O
in	O
order	O
to	O
exploit	O
kernel	B
substitution	I
we	O
need	O
to	O
be	O
able	O
to	O
construct	O
valid	O
kernel	O
functions	O
one	O
approach	O
is	O
to	O
choose	O
a	O
feature	B
space	I
mapping	O
and	O
then	O
use	O
this	O
to	O
find	O
the	O
corresponding	O
kernel	O
as	O
is	O
illustrated	O
in	O
figure	O
here	O
the	O
kernel	B
function	I
is	O
defined	O
for	O
a	O
one-dimensional	O
input	O
space	O
by	O
kx	O
x	O
ix	O
ix	O
where	O
ix	O
are	O
the	O
basis	O
functions	O
an	O
alternative	O
approach	O
is	O
to	O
construct	O
kernel	O
functions	O
directly	O
in	O
this	O
case	O
we	O
must	O
ensure	O
that	O
the	O
function	O
we	O
choose	O
is	O
a	O
valid	O
kernel	O
in	O
other	O
words	O
that	O
it	O
corresponds	O
to	O
a	O
scalar	O
product	O
in	O
some	O
infinite	O
dimensional	O
feature	B
space	I
as	O
a	O
simple	O
example	O
consider	O
a	O
kernel	B
function	I
given	O
by	O
kx	O
z	O
xtz	O
constructing	O
kernels	O
figure	O
illustration	O
of	O
the	O
construction	O
of	O
kernel	O
functions	O
starting	O
from	O
a	O
corresponding	O
set	O
of	O
basis	O
functions	O
in	O
each	O
column	O
the	O
lower	O
plot	O
shows	O
the	O
kernel	B
function	I
kx	O
defined	O
by	O
plotted	O
as	O
a	O
function	O
of	O
x	O
for	O
while	O
the	O
upper	O
plot	O
shows	O
the	O
corresponding	O
basis	O
functions	O
given	O
by	O
polynomials	O
column	O
gaussians	O
column	O
and	O
logistic	O
sigmoids	O
column	O
if	O
we	O
take	O
the	O
particular	O
case	O
of	O
a	O
two-dimensional	O
input	O
space	O
x	O
we	O
can	O
expand	O
out	O
the	O
terms	O
and	O
thereby	O
identify	O
the	O
corresponding	O
nonlinear	O
feature	O
mapping	O
kx	O
z	O
xtz	O
and	O
we	O
see	O
that	O
the	O
feature	O
mapping	O
takes	O
the	O
form	O
therefore	O
comprises	O
all	O
possible	O
second	B
order	I
terms	O
with	O
a	O
specific	O
weighting	O
between	O
them	O
more	O
generally	O
however	O
we	O
need	O
a	O
simple	O
way	O
to	O
test	O
whether	O
a	O
function	O
constitutes	O
a	O
valid	O
kernel	O
without	O
having	O
to	O
construct	O
the	O
function	O
explicitly	O
a	O
necessary	O
and	O
sufficient	O
condition	O
for	O
a	O
function	O
kx	O
to	O
be	O
a	O
valid	O
kernel	O
and	O
cristianini	O
is	O
that	O
the	O
gram	B
matrix	I
k	O
whose	O
elements	O
are	O
given	O
by	O
kxn	O
xm	O
should	O
be	O
positive	O
semidefinite	O
for	O
all	O
possible	O
choices	O
of	O
the	O
set	O
note	O
that	O
a	O
positive	B
semidefinite	I
matrix	I
is	O
not	O
the	O
same	O
thing	O
as	O
a	O
matrix	O
whose	O
elements	O
are	O
nonnegative	O
one	O
powerful	O
technique	O
for	O
constructing	O
new	O
kernels	O
is	O
to	O
build	O
them	O
out	O
of	O
simpler	O
kernels	O
as	O
building	O
blocks	O
this	O
can	O
be	O
done	O
using	O
the	O
following	O
properties	O
appendix	O
c	O
kernel	O
methods	O
techniques	O
for	O
constructing	O
new	O
kernels	O
given	O
valid	O
kernels	O
and	O
the	O
following	O
new	O
kernels	O
will	O
also	O
be	O
valid	O
kx	O
kx	O
kx	O
q	O
kx	O
exp	O
kx	O
kx	O
kx	O
kx	O
kx	O
kaxa	O
kx	O
kaxa	O
where	O
c	O
is	O
a	O
constant	O
f	O
is	O
any	O
function	O
q	O
is	O
a	O
polynomial	O
with	O
nonnegm	O
is	O
a	O
valid	O
kernel	O
in	O
ative	O
coefficients	O
is	O
a	O
function	O
from	O
x	O
to	O
r	O
m	O
a	O
is	O
a	O
symmetric	O
positive	B
semidefinite	I
matrix	I
xa	O
and	O
xb	O
are	O
variables	O
r	O
necessarily	O
disjoint	O
with	O
x	O
xb	O
and	O
ka	O
and	O
kb	O
are	O
valid	O
kernel	O
functions	O
over	O
their	O
respective	O
spaces	O
a	O
kbxb	O
b	O
akbxb	O
b	O
equipped	O
with	O
these	O
properties	O
we	O
can	O
now	O
embark	O
on	O
the	O
construction	O
of	O
more	O
complex	O
kernels	O
appropriate	O
to	O
specific	O
applications	O
we	O
require	O
that	O
the	O
kernel	O
kx	O
be	O
symmetric	O
and	O
positive	O
semidefinite	O
and	O
that	O
it	O
expresses	O
the	O
appropriate	O
form	O
of	O
similarity	O
between	O
x	O
and	O
according	O
to	O
the	O
intended	O
application	O
here	O
we	O
consider	O
a	O
few	O
common	O
examples	O
of	O
kernel	O
functions	O
for	O
a	O
more	O
extensive	O
discussion	O
of	O
kernel	O
engineering	O
see	O
shawe-taylor	O
and	O
cristianini	O
we	O
saw	O
that	O
the	O
simple	O
polynomial	O
kernel	O
kx	O
contains	O
only	O
if	O
we	O
consider	O
the	O
slightly	O
generalized	B
kernel	O
kx	O
with	O
c	O
then	O
the	O
corresponding	O
feature	O
mapping	O
contains	O
con	O
terms	O
of	O
degree	O
two	O
c	O
stant	O
and	O
linear	O
terms	O
as	O
well	O
as	O
terms	O
of	O
order	O
two	O
similarly	O
kx	O
contains	O
all	O
monomials	O
of	O
order	O
m	O
for	O
instance	O
if	O
x	O
and	O
are	O
two	O
images	O
then	O
the	O
kernel	O
represents	O
a	O
particular	O
weighted	O
sum	O
of	O
all	O
possible	O
products	O
of	O
m	O
pixels	O
in	O
the	O
first	O
image	O
with	O
m	O
pixels	O
in	O
the	O
second	O
image	O
this	O
can	O
similarly	O
be	O
generalized	B
to	O
include	O
all	O
terms	O
up	O
to	O
degree	O
m	O
by	O
considering	O
kx	O
with	O
c	O
using	O
the	O
results	O
and	O
for	O
combining	O
kernels	O
we	O
see	O
that	O
these	O
will	O
all	O
be	O
valid	O
kernel	O
functions	O
c	O
another	O
commonly	O
used	O
kernel	O
takes	O
the	O
form	O
kx	O
exp	O
and	O
is	O
often	O
called	O
a	O
gaussian	B
kernel	I
note	O
however	O
that	O
in	O
this	O
context	O
it	O
is	O
not	O
interpreted	O
as	O
a	O
probability	B
density	B
and	O
hence	O
the	O
normalization	O
coefficient	O
is	O
constructing	O
kernels	O
omitted	O
we	O
can	O
see	O
that	O
this	O
is	O
a	O
valid	O
kernel	O
by	O
expanding	O
the	O
square	O
to	O
give	O
kx	O
exp	O
xtx	O
exp	O
exp	O
exercise	O
exercise	O
and	O
then	O
making	O
use	O
of	O
and	O
together	O
with	O
the	O
validity	O
of	O
the	O
linear	O
kernel	O
kx	O
note	O
that	O
the	O
feature	O
vector	O
that	O
corresponds	O
to	O
the	O
gaussian	B
kernel	I
has	O
infinite	O
dimensionality	O
kernel	B
substitution	I
in	O
to	O
replace	O
obtain	O
the	O
gaussian	B
kernel	I
is	O
not	O
restricted	O
to	O
the	O
use	O
of	O
euclidean	O
distance	O
if	O
we	O
use	O
with	O
a	O
nonlinear	O
kernel	O
we	O
kx	O
exp	O
x	O
an	O
important	O
contribution	O
to	O
arise	O
from	O
the	O
kernel	O
viewpoint	O
has	O
been	O
the	O
extension	O
to	O
inputs	O
that	O
are	O
symbolic	O
rather	O
than	O
simply	O
vectors	O
of	O
real	O
numbers	O
kernel	O
functions	O
can	O
be	O
defined	O
over	O
objects	O
as	O
diverse	O
as	O
graphs	O
sets	O
strings	O
and	O
text	O
documents	O
consider	O
for	O
instance	O
a	O
fixed	O
set	O
and	O
define	O
a	O
nonvectorial	O
space	O
consisting	O
of	O
all	O
possible	O
subsets	O
of	O
this	O
set	O
if	O
and	O
are	O
two	O
such	O
subsets	O
then	O
one	O
simple	O
choice	O
of	O
kernel	O
would	O
be	O
where	O
denotes	O
the	O
intersection	O
of	O
sets	O
and	O
and	O
denotes	O
the	O
number	O
of	O
subsets	O
in	O
a	O
this	O
is	O
a	O
valid	O
kernel	B
function	I
because	O
it	O
can	O
be	O
shown	O
to	O
correspond	O
to	O
an	O
inner	O
product	O
in	O
a	O
feature	B
space	I
one	O
powerful	O
approach	O
to	O
the	O
construction	O
of	O
kernels	O
starts	O
from	O
a	O
probabilistic	O
generative	B
model	I
which	O
allows	O
us	O
to	O
apply	O
generative	O
models	O
in	O
a	O
discriminative	O
setting	O
generative	O
models	O
can	O
deal	O
naturally	O
with	O
missing	B
data	I
and	O
in	O
the	O
case	O
of	O
hidden	O
markov	O
models	O
can	O
handle	O
sequences	O
of	O
varying	O
length	O
by	O
contrast	O
discriminative	O
models	O
generally	O
give	O
better	O
performance	O
on	O
discriminative	O
tasks	O
than	O
generative	O
models	O
it	O
is	O
therefore	O
of	O
some	O
interest	O
to	O
combine	O
these	O
two	O
approaches	O
et	O
al	O
one	O
way	O
to	O
combine	O
them	O
is	O
to	O
use	O
a	O
generative	B
model	I
to	O
define	O
a	O
kernel	O
and	O
then	O
use	O
this	O
kernel	O
in	O
a	O
discriminative	O
approach	O
given	O
a	O
generative	B
model	I
px	O
we	O
can	O
define	O
a	O
kernel	O
by	O
kx	O
this	O
is	O
clearly	O
a	O
valid	O
kernel	B
function	I
because	O
we	O
can	O
interpret	O
it	O
as	O
an	O
inner	O
product	O
in	O
the	O
one-dimensional	O
feature	B
space	I
defined	O
by	O
the	O
mapping	O
px	O
it	O
says	O
that	O
two	O
inputs	O
x	O
and	O
are	O
similar	O
if	O
they	O
both	O
have	O
high	O
probabilities	O
we	O
can	O
use	O
and	O
to	O
extend	O
this	O
class	O
of	O
kernels	O
by	O
considering	O
sums	O
over	O
products	O
of	O
different	O
probability	B
distributions	O
with	O
positive	O
weighting	O
coefficients	O
pi	O
of	O
the	O
form	O
kx	O
i	O
kernel	O
methods	O
section	O
section	O
exercise	O
this	O
is	O
equivalent	O
up	O
to	O
an	O
overall	O
multiplicative	O
constant	O
to	O
a	O
mixture	B
distribution	I
in	O
which	O
the	O
components	O
factorize	O
with	O
the	O
index	O
i	O
playing	O
the	O
role	O
of	O
a	O
latent	B
variable	I
two	O
inputs	O
x	O
and	O
will	O
give	O
a	O
large	O
value	O
for	O
the	O
kernel	B
function	I
and	O
hence	O
appear	O
similar	O
if	O
they	O
have	O
significant	O
probability	B
under	O
a	O
range	O
of	O
different	O
components	O
taking	O
the	O
limit	O
of	O
an	O
infinite	O
sum	O
we	O
can	O
also	O
consider	O
kernels	O
of	O
the	O
form	O
kx	O
dz	O
where	O
z	O
is	O
a	O
continuous	O
latent	B
variable	I
now	O
suppose	O
that	O
our	O
data	O
consists	O
of	O
ordered	O
sequences	O
of	O
length	O
l	O
so	O
that	O
an	O
observation	O
is	O
given	O
by	O
x	O
xl	O
a	O
popular	O
generative	B
model	I
for	O
sequences	O
is	O
the	O
hidden	B
markov	B
model	I
which	O
expresses	O
the	O
distribution	O
px	O
as	O
a	O
marginalization	O
over	O
a	O
corresponding	O
sequence	O
of	O
hidden	O
states	O
z	O
zl	O
we	O
can	O
use	O
this	O
approach	O
to	O
define	O
a	O
kernel	B
function	I
measuring	O
the	O
similarity	O
of	O
two	O
sequences	O
x	O
and	O
by	O
extending	O
the	O
mixture	B
representation	O
to	O
give	O
kx	O
z	O
so	O
that	O
both	O
observed	O
sequences	O
are	O
generated	O
by	O
the	O
same	O
hidden	O
sequence	O
z	O
this	O
model	O
can	O
easily	O
be	O
extended	B
to	O
allow	O
sequences	O
of	O
differing	O
length	O
to	O
be	O
compared	O
an	O
alternative	O
technique	O
for	O
using	O
generative	O
models	O
to	O
define	O
kernel	O
functions	O
is	O
known	O
as	O
the	O
fisher	B
kernel	I
and	O
haussler	O
consider	O
a	O
parametric	O
generative	B
model	I
px	O
where	O
denotes	O
the	O
vector	O
of	O
parameters	O
the	O
goal	O
is	O
to	O
find	O
a	O
kernel	O
that	O
measures	O
the	O
similarity	O
of	O
two	O
input	O
vectors	O
x	O
and	O
induced	O
by	O
the	O
generative	B
model	I
jaakkola	O
and	O
haussler	O
consider	O
the	O
gradient	O
with	O
respect	O
to	O
which	O
defines	O
a	O
vector	O
in	O
a	O
feature	B
space	I
having	O
the	O
same	O
dimensionality	O
as	O
in	O
particular	O
they	O
consider	O
the	O
fisher	B
score	O
g	O
x	O
ln	O
px	O
from	O
which	O
the	O
fisher	B
kernel	I
is	O
defined	O
by	O
kx	O
g	O
xtf	O
here	O
f	O
is	O
the	O
fisher	B
information	I
matrix	I
given	O
by	O
f	O
ex	O
g	O
xg	O
xt	O
where	O
the	O
expectation	B
is	O
with	O
respect	O
to	O
x	O
under	O
the	O
distribution	O
px	O
this	O
can	O
be	O
motivated	O
from	O
the	O
perspective	O
of	O
information	B
geometry	I
which	O
considers	O
the	O
differential	B
geometry	O
of	O
the	O
space	O
of	O
model	O
parameters	O
here	O
we	O
simply	O
note	O
that	O
the	O
presence	O
of	O
the	O
fisher	B
information	I
matrix	I
causes	O
this	O
kernel	O
to	O
be	O
invariant	O
under	O
a	O
nonlinear	O
re-parameterization	O
of	O
the	O
density	B
model	O
in	O
practice	O
it	O
is	O
often	O
infeasible	O
to	O
evaluate	O
the	O
fisher	B
information	I
matrix	I
one	O
approach	O
is	O
simply	O
to	O
replace	O
the	O
expectation	B
in	O
the	O
definition	O
of	O
the	O
fisher	B
information	O
with	O
the	O
sample	O
average	O
giving	O
g	O
xng	O
xnt	O
f	O
n	O
radial	B
basis	B
function	I
networks	O
section	O
this	O
is	O
the	O
covariance	B
matrix	O
of	O
the	O
fisher	B
scores	O
and	O
so	O
the	O
fisher	B
kernel	I
corresponds	O
to	O
a	O
whitening	B
of	O
these	O
scores	O
more	O
simply	O
we	O
can	O
just	O
omit	O
the	O
fisher	B
information	I
matrix	I
altogether	O
and	O
use	O
the	O
noninvariant	O
kernel	O
kx	O
g	O
xtg	O
an	O
application	O
of	O
fisher	B
kernels	O
to	O
document	B
retrieval	I
is	O
given	O
by	O
hofmann	O
a	O
final	O
example	O
of	O
a	O
kernel	B
function	I
is	O
the	O
sigmoidal	O
kernel	O
given	O
by	O
kx	O
tanh	O
b	O
whose	O
gram	B
matrix	I
in	O
general	O
is	O
not	O
positive	O
semidefinite	O
this	O
form	O
of	O
kernel	O
has	O
however	O
been	O
used	O
in	O
practice	O
possibly	O
because	O
it	O
gives	O
kernel	O
expansions	O
such	O
as	O
the	O
support	B
vector	I
machine	I
a	O
superficial	O
resemblance	O
to	O
neural	B
network	I
models	O
as	O
we	O
shall	O
see	O
in	O
the	O
limit	O
of	O
an	O
infinite	O
number	O
of	O
basis	O
functions	O
a	O
bayesian	B
neural	B
network	I
with	O
an	O
appropriate	O
prior	B
reduces	O
to	O
a	O
gaussian	B
process	I
thereby	O
providing	O
a	O
deeper	O
link	B
between	O
neural	O
networks	O
and	O
kernel	O
methods	O
section	O
radial	B
basis	B
function	I
networks	O
in	O
chapter	O
we	O
discussed	O
regression	B
models	O
based	O
on	O
linear	O
combinations	O
of	O
fixed	O
basis	O
functions	O
although	O
we	O
did	O
not	O
discuss	O
in	O
detail	O
what	O
form	O
those	O
basis	O
functions	O
might	O
take	O
one	O
choice	O
that	O
has	O
been	O
widely	O
used	O
is	O
that	O
of	O
radial	O
basis	O
functions	O
which	O
have	O
the	O
property	O
that	O
each	O
basis	B
function	I
depends	O
only	O
on	O
the	O
radial	O
distance	O
euclidean	O
from	O
a	O
centre	O
j	O
so	O
that	O
jx	O
historically	O
radial	O
basis	O
functions	O
were	O
introduced	O
for	O
the	O
purpose	O
of	O
exact	O
function	B
interpolation	I
given	O
a	O
set	O
of	O
input	O
vectors	O
xn	O
along	O
with	O
corresponding	O
target	O
values	O
tn	O
the	O
goal	O
is	O
to	O
find	O
a	O
smooth	O
function	O
fx	O
that	O
fits	O
every	O
target	O
value	O
exactly	O
so	O
that	O
fxn	O
tn	O
for	O
n	O
n	O
this	O
is	O
achieved	O
by	O
expressing	O
fx	O
as	O
a	O
linear	O
combination	O
of	O
radial	O
basis	O
functions	O
one	O
centred	O
on	O
every	O
data	O
point	O
fx	O
the	O
values	O
of	O
the	O
coefficients	O
are	O
found	O
by	O
least	O
squares	O
and	O
because	O
there	O
are	O
the	O
same	O
number	O
of	O
coefficients	O
as	O
there	O
are	O
constraints	O
the	O
result	O
is	O
a	O
function	O
that	O
fits	O
every	O
target	O
value	O
exactly	O
in	O
pattern	O
recognition	O
applications	O
however	O
the	O
target	O
values	O
are	O
generally	O
noisy	O
and	O
exact	O
interpolation	O
is	O
undesirable	O
because	O
this	O
corresponds	O
to	O
an	O
over-fitted	O
solution	O
expansions	O
in	O
radial	O
basis	O
functions	O
also	O
arise	O
from	O
regularization	B
theory	B
and	O
girosi	O
bishop	O
for	O
a	O
sum-of-squares	B
error	B
function	I
with	O
a	O
regularizer	O
defined	O
in	O
terms	O
of	O
a	O
differential	B
operator	O
the	O
optimal	O
solution	O
is	O
given	O
by	O
an	O
expansion	O
in	O
the	O
green	O
s	O
functions	O
of	O
the	O
operator	O
are	O
analogous	O
to	O
the	O
eigenvectors	O
of	O
a	O
discrete	O
matrix	O
again	O
with	O
one	O
basis	B
function	I
centred	O
on	O
each	O
data	O
kernel	O
methods	O
point	O
if	O
the	O
differential	B
operator	O
is	O
isotropic	B
then	O
the	O
green	O
s	O
functions	O
depend	O
only	O
on	O
the	O
radial	O
distance	O
from	O
the	O
corresponding	O
data	O
point	O
due	O
to	O
the	O
presence	O
of	O
the	O
regularizer	O
the	O
solution	O
no	O
longer	O
interpolates	O
the	O
training	B
data	O
exactly	O
another	O
motivation	O
for	O
radial	O
basis	O
functions	O
comes	O
from	O
a	O
consideration	O
of	O
the	O
interpolation	O
problem	O
when	O
the	O
input	O
than	O
the	O
target	O
variables	O
are	O
noisy	O
if	O
the	O
noise	O
on	O
the	O
input	O
variable	O
x	O
is	O
described	O
bishop	O
by	O
a	O
variable	O
having	O
a	O
distribution	O
then	O
the	O
sum-of-squares	B
error	B
function	I
becomes	O
d	O
e	O
appendix	O
d	O
exercise	O
using	O
the	O
calculus	B
of	I
variations	I
we	O
can	O
optimize	O
with	O
respect	O
to	O
the	O
function	O
fx	O
to	O
give	O
yxn	O
tnhx	O
xn	O
where	O
the	O
basis	O
functions	O
are	O
given	O
by	O
hx	O
xn	O
xn	O
xn	O
we	O
see	O
that	O
there	O
is	O
one	O
basis	B
function	I
centred	O
on	O
every	O
data	O
point	O
this	O
is	O
known	O
as	O
the	O
nadaraya-watson	O
model	O
and	O
will	O
be	O
derived	O
again	O
from	O
a	O
different	O
perspective	O
in	O
section	O
if	O
the	O
noise	O
distribution	O
is	O
isotropic	B
so	O
that	O
it	O
is	O
a	O
function	O
only	O
of	O
then	O
the	O
basis	O
functions	O
will	O
be	O
radial	O
n	O
hx	O
xn	O
for	O
any	O
value	O
of	O
x	O
the	O
effect	O
of	O
such	O
normalization	O
is	O
shown	O
in	O
figure	O
normalization	O
is	O
sometimes	O
used	O
in	O
practice	O
as	O
it	O
avoids	O
having	O
regions	O
of	O
input	O
space	O
where	O
all	O
of	O
the	O
basis	O
functions	O
take	O
small	O
values	O
which	O
would	O
necessarily	O
lead	O
to	O
predictions	O
in	O
such	O
regions	O
that	O
are	O
either	O
small	O
or	O
controlled	O
purely	O
by	O
the	O
bias	B
parameter	I
note	O
that	O
the	O
basis	O
functions	O
are	O
normalized	O
so	O
that	O
another	O
situation	O
in	O
which	O
expansions	O
in	O
normalized	O
radial	O
basis	O
functions	O
arise	O
is	O
in	O
the	O
application	O
of	O
kernel	O
density	B
estimation	I
to	O
the	O
problem	O
of	O
regression	B
as	O
we	O
shall	O
discuss	O
in	O
section	O
because	O
there	O
is	O
one	O
basis	B
function	I
associated	O
with	O
every	O
data	O
point	O
the	O
corresponding	O
model	O
can	O
be	O
computationally	O
costly	O
to	O
evaluate	O
when	O
making	O
predictions	O
for	O
new	O
data	O
points	O
models	O
have	O
therefore	O
been	O
proposed	O
and	O
lowe	O
moody	O
and	O
darken	O
poggio	O
and	O
girosi	O
which	O
retain	O
the	O
expansion	O
in	O
radial	O
basis	O
functions	O
but	O
where	O
the	O
number	O
m	O
of	O
basis	O
functions	O
is	O
smaller	O
than	O
the	O
number	O
n	O
of	O
data	O
points	O
typically	O
the	O
number	O
of	O
basis	O
functions	O
and	O
the	O
locations	O
i	O
of	O
their	O
centres	O
are	O
determined	O
based	O
on	O
the	O
input	O
data	O
alone	O
the	O
basis	O
functions	O
are	O
then	O
kept	O
fixed	O
and	O
the	O
coefficients	O
are	O
determined	O
by	O
least	O
squares	O
by	O
solving	O
the	O
usual	O
set	O
of	O
linear	O
equations	O
as	O
discussed	O
in	O
section	O
radial	B
basis	B
function	I
networks	O
figure	O
plot	O
of	O
a	O
set	O
of	O
gaussian	B
basis	O
functions	O
on	O
the	O
left	O
together	O
with	O
the	O
corresponding	O
normalized	O
basis	O
functions	O
on	O
the	O
right	O
one	O
of	O
the	O
simplest	O
ways	O
of	O
choosing	O
basis	B
function	I
centres	O
is	O
to	O
use	O
a	O
randomly	O
chosen	O
subset	O
of	O
the	O
data	O
points	O
a	O
more	O
systematic	O
approach	O
is	O
called	O
orthogonal	B
least	I
squares	I
et	O
al	O
this	O
is	O
a	O
sequential	O
selection	O
process	O
in	O
which	O
at	O
each	O
step	O
the	O
next	O
data	O
point	O
to	O
be	O
chosen	O
as	O
a	O
basis	B
function	I
centre	O
corresponds	O
to	O
the	O
one	O
that	O
gives	O
the	O
greatest	O
reduction	O
in	O
the	O
sum-of-squares	B
error	B
values	O
for	O
the	O
expansion	O
coefficients	O
are	O
determined	O
as	O
part	O
of	O
the	O
algorithm	O
clustering	B
algorithms	O
such	O
as	O
k-means	O
have	O
also	O
been	O
used	O
which	O
give	O
a	O
set	O
of	O
basis	B
function	I
centres	O
that	O
no	O
longer	O
coincide	O
with	O
training	B
data	O
points	O
nadaraya-watson	O
model	O
in	O
section	O
we	O
saw	O
that	O
the	O
prediction	O
of	O
a	O
linear	B
regression	B
model	O
for	O
a	O
new	O
input	O
x	O
takes	O
the	O
form	O
of	O
a	O
linear	O
combination	O
of	O
the	O
training	B
set	I
target	O
values	O
with	O
coefficients	O
given	O
by	O
the	O
equivalent	B
kernel	I
where	O
the	O
equivalent	B
kernel	I
satisfies	O
the	O
summation	O
constraint	O
we	O
can	O
motivate	O
the	O
kernel	B
regression	B
model	O
from	O
a	O
different	O
perspective	O
starting	O
with	O
kernel	O
density	B
estimation	I
suppose	O
we	O
have	O
a	O
training	B
set	I
tn	O
and	O
we	O
use	O
a	O
parzen	O
density	B
estimator	O
to	O
model	O
the	O
joint	O
distribution	O
px	O
t	O
so	O
that	O
section	O
section	O
px	O
t	O
n	O
fx	O
xn	O
t	O
tn	O
where	O
fx	O
t	O
is	O
the	O
component	O
density	B
function	O
and	O
there	O
is	O
one	O
such	O
component	O
centred	O
on	O
each	O
data	O
point	O
we	O
now	O
find	O
an	O
expression	O
for	O
the	O
regression	B
function	I
yx	O
corresponding	O
to	O
the	O
conditional	B
average	O
of	O
the	O
target	O
variable	O
conditioned	O
on	O
kernel	O
methods	O
the	O
input	O
variable	O
which	O
is	O
given	O
by	O
yx	O
etx	O
tptx	O
dt	O
tpx	O
t	O
dt	O
px	O
t	O
dt	O
m	O
n	O
gx	O
xntn	O
gx	O
xm	O
kx	O
xntn	O
yx	O
kx	O
xn	O
gx	O
xn	O
gx	O
xm	O
m	O
n	O
n	O
m	O
tfx	O
xn	O
t	O
tn	O
dt	O
fx	O
xm	O
t	O
tm	O
dt	O
we	O
now	O
assume	O
for	O
simplicity	O
that	O
the	O
component	O
density	B
functions	O
have	O
zero	O
mean	B
so	O
that	O
fx	O
tt	O
dt	O
for	O
all	O
values	O
of	O
x	O
using	O
a	O
simple	O
change	O
of	O
variable	O
we	O
then	O
obtain	O
where	O
n	O
m	O
n	O
and	O
the	O
kernel	B
function	I
kx	O
xn	O
is	O
given	O
by	O
and	O
we	O
have	O
defined	O
gx	O
fx	O
t	O
dt	O
the	O
result	O
is	O
known	O
as	O
the	O
nadaraya-watson	O
model	O
or	O
kernel	B
regression	B
watson	O
for	O
a	O
localized	O
kernel	B
function	I
it	O
has	O
the	O
property	O
of	O
giving	O
more	O
weight	O
to	O
the	O
data	O
points	O
xn	O
that	O
are	O
close	O
to	O
x	O
note	O
that	O
the	O
kernel	O
satisfies	O
the	O
summation	O
constraint	O
kx	O
xn	O
gaussian	B
processes	O
figure	O
illustration	O
of	O
the	O
nadaraya-watson	O
kernel	B
regression	B
model	O
using	O
isotropic	B
gaussian	B
kernels	O
for	O
the	O
sinusoidal	B
data	I
set	O
the	O
original	O
sine	O
function	O
is	O
shown	O
by	O
the	O
green	O
curve	O
the	O
data	O
points	O
are	O
shown	O
in	O
blue	O
and	O
each	O
is	O
the	O
centre	O
of	O
an	O
isotropic	B
gaussian	B
kernel	I
the	O
resulting	O
regression	B
function	I
given	O
by	O
the	O
conditional	B
mean	B
is	O
shown	O
by	O
the	O
red	O
line	O
along	O
with	O
the	O
twostandard-deviation	O
region	O
for	O
the	O
conditional	B
distribution	O
ptx	O
shown	O
by	O
the	O
red	O
shading	O
the	O
blue	O
ellipse	O
around	O
each	O
data	O
point	O
shows	O
one	O
standard	B
deviation	I
contour	O
for	O
the	O
corresponding	O
kernel	O
these	O
appear	O
noncircular	O
due	O
to	O
the	O
different	O
scales	O
on	O
the	O
horizontal	O
and	O
vertical	O
axes	O
in	O
fact	O
this	O
model	O
defines	O
not	O
only	O
a	O
conditional	B
expectation	B
but	O
also	O
a	O
full	O
conditional	B
distribution	O
given	O
by	O
ptx	O
pt	O
x	O
pt	O
x	O
dt	O
n	O
fx	O
xn	O
t	O
tn	O
fx	O
xm	O
t	O
tm	O
dt	O
exercise	O
m	O
from	O
which	O
other	O
expectations	O
can	O
be	O
evaluated	O
as	O
an	O
illustration	O
we	O
consider	O
the	O
case	O
of	O
a	O
single	O
input	O
variable	O
x	O
in	O
which	O
fx	O
t	O
is	O
given	O
by	O
a	O
zero-mean	O
isotropic	B
gaussian	B
over	O
the	O
variable	O
z	O
t	O
with	O
variance	B
the	O
corresponding	O
conditional	B
distribution	O
is	O
given	O
by	O
a	O
gaussian	B
mixture	B
and	O
is	O
shown	O
together	O
with	O
the	O
conditional	B
mean	B
for	O
the	O
sinusoidal	O
synthetic	O
data	O
set	O
in	O
figure	O
an	O
obvious	O
extension	O
of	O
this	O
model	O
is	O
to	O
allow	O
for	O
more	O
flexible	O
forms	O
of	O
gaussian	B
components	O
for	O
instance	O
having	O
different	O
variance	B
parameters	O
for	O
the	O
input	O
and	O
target	O
variables	O
more	O
generally	O
we	O
could	O
model	O
the	O
joint	O
distribution	O
pt	O
x	O
using	O
a	O
gaussian	B
mixture	B
model	I
trained	O
using	O
techniques	O
discussed	O
in	O
chapter	O
and	O
jordan	O
and	O
then	O
find	O
the	O
corresponding	O
conditional	B
distribution	O
ptx	O
in	O
this	O
latter	O
case	O
we	O
no	O
longer	O
have	O
a	O
representation	O
in	O
terms	O
of	O
kernel	O
functions	O
evaluated	O
at	O
the	O
training	B
set	I
data	O
points	O
however	O
the	O
number	O
of	O
components	O
in	O
the	O
mixture	B
model	I
can	O
be	O
smaller	O
than	O
the	O
number	O
of	O
training	B
set	I
points	O
resulting	O
in	O
a	O
model	O
that	O
is	O
faster	O
to	O
evaluate	O
for	O
test	O
data	O
points	O
we	O
have	O
thereby	O
accepted	O
an	O
increased	O
computational	O
cost	O
during	O
the	O
training	B
phase	O
in	O
order	O
to	O
have	O
a	O
model	O
that	O
is	O
faster	O
at	O
making	O
predictions	O
gaussian	B
processes	O
in	O
section	O
we	O
introduced	O
kernels	O
by	O
applying	O
the	O
concept	O
of	O
duality	O
to	O
a	O
nonprobabilistic	O
model	O
for	B
regression	B
here	O
we	O
extend	O
the	O
role	O
of	O
kernels	O
to	O
probabilis	O
kernel	O
methods	O
tic	O
discriminative	O
models	O
leading	O
to	O
the	O
framework	O
of	O
gaussian	B
processes	O
we	O
shall	O
thereby	O
see	O
how	O
kernels	O
arise	O
naturally	O
in	O
a	O
bayesian	B
setting	O
in	O
chapter	O
we	O
considered	O
linear	B
regression	B
models	O
of	O
the	O
form	O
yx	O
w	O
wt	O
in	O
which	O
w	O
is	O
a	O
vector	O
of	O
parameters	O
and	O
is	O
a	O
vector	O
of	O
fixed	O
nonlinear	O
basis	O
functions	O
that	O
depend	O
on	O
the	O
input	O
vector	O
x	O
we	O
showed	O
that	O
a	O
prior	B
distribution	O
over	O
w	O
induced	O
a	O
corresponding	O
prior	B
distribution	O
over	O
functions	O
yx	O
w	O
given	O
a	O
training	B
data	O
set	O
we	O
then	O
evaluated	O
the	O
posterior	O
distribution	O
over	O
w	O
and	O
thereby	O
obtained	O
the	O
corresponding	O
posterior	O
distribution	O
over	O
regression	B
functions	O
which	O
in	O
turn	O
the	O
addition	O
of	O
noise	O
implies	O
a	O
predictive	B
distribution	I
ptx	O
for	O
new	O
input	O
vectors	O
x	O
in	O
the	O
gaussian	B
process	I
viewpoint	O
we	O
dispense	O
with	O
the	O
parametric	O
model	O
and	O
instead	O
define	O
a	O
prior	B
probability	B
distribution	O
over	O
functions	O
directly	O
at	O
first	O
sight	O
it	O
might	O
seem	O
difficult	O
to	O
work	O
with	O
a	O
distribution	O
over	O
the	O
uncountably	O
infinite	O
space	O
of	O
functions	O
however	O
as	O
we	O
shall	O
see	O
for	O
a	O
finite	O
training	B
set	I
we	O
only	O
need	O
to	O
consider	O
the	O
values	O
of	O
the	O
function	O
at	O
the	O
discrete	O
set	O
of	O
input	O
values	O
xn	O
corresponding	O
to	O
the	O
training	B
set	I
and	O
test	B
set	I
data	O
points	O
and	O
so	O
in	O
practice	O
we	O
can	O
work	O
in	O
a	O
finite	O
space	O
models	O
equivalent	O
to	O
gaussian	B
processes	O
have	O
been	O
widely	O
studied	O
in	O
many	O
different	O
fields	O
for	O
instance	O
in	O
the	O
geostatistics	O
literature	O
gaussian	B
process	I
regression	B
is	O
known	O
as	O
kriging	O
similarly	O
arma	O
moving	O
average	O
models	O
kalman	O
filters	O
and	O
radial	B
basis	B
function	I
networks	O
can	O
all	O
be	O
viewed	O
as	O
forms	O
of	O
gaussian	B
process	I
models	O
reviews	O
of	O
gaussian	B
processes	O
from	O
a	O
machine	O
learning	B
perspective	O
can	O
be	O
found	O
in	O
mackay	O
williams	O
and	O
mackay	O
and	O
a	O
comparison	O
of	O
gaussian	B
process	I
models	O
with	O
alternative	O
approaches	O
is	O
given	O
in	O
rasmussen	O
see	O
also	O
rasmussen	O
and	O
williams	O
for	O
a	O
recent	O
textbook	O
on	O
gaussian	B
processes	O
linear	B
regression	B
revisited	O
in	O
order	O
to	O
motivate	O
the	O
gaussian	B
process	I
viewpoint	O
let	O
us	O
return	O
to	O
the	O
linear	B
regression	B
example	O
and	O
re-derive	O
the	O
predictive	B
distribution	I
by	O
working	O
in	O
terms	O
of	O
distributions	O
over	O
functions	O
yx	O
w	O
this	O
will	O
provide	O
a	O
specific	O
example	O
of	O
a	O
gaussian	B
process	I
consider	O
a	O
model	O
defined	O
in	O
terms	O
of	O
a	O
linear	O
combination	O
of	O
m	O
fixed	O
basis	O
functions	O
given	O
by	O
the	O
elements	O
of	O
the	O
vector	O
so	O
that	O
yx	O
wt	O
where	O
x	O
is	O
the	O
input	O
vector	O
and	O
w	O
is	O
the	O
m-dimensional	O
weight	B
vector	I
now	O
consider	O
a	O
prior	B
distribution	O
over	O
w	O
given	O
by	O
an	O
isotropic	B
gaussian	B
of	O
the	O
form	O
pw	O
n	O
governed	O
by	O
the	O
hyperparameter	B
which	O
represents	O
the	O
precision	O
variance	B
of	O
the	O
distribution	O
for	O
any	O
given	O
value	O
of	O
w	O
the	O
definition	O
defines	O
a	O
particular	O
function	O
of	O
x	O
the	O
probability	B
distribution	O
over	O
w	O
defined	O
by	O
therefore	O
induces	O
a	O
probability	B
distribution	O
over	O
functions	O
yx	O
in	O
practice	O
we	O
wish	O
to	O
evaluate	O
this	O
function	O
at	O
specific	O
values	O
of	O
x	O
for	O
example	O
at	O
the	O
training	B
data	O
points	O
gaussian	B
processes	O
xn	O
we	O
are	O
therefore	O
interested	O
in	O
the	O
joint	O
distribution	O
of	O
the	O
function	O
values	O
yxn	O
which	O
we	O
denote	O
by	O
the	O
vector	O
y	O
with	O
elements	O
yn	O
yxn	O
for	O
n	O
n	O
from	O
this	O
vector	O
is	O
given	O
by	O
y	O
w	O
where	O
is	O
the	O
design	B
matrix	I
with	O
elements	O
nk	O
kxn	O
we	O
can	O
find	O
the	O
probability	B
distribution	O
of	O
y	O
as	O
follows	O
first	O
of	O
all	O
we	O
note	O
that	O
y	O
is	O
a	O
linear	O
combination	O
of	O
gaussian	B
distributed	O
variables	O
given	O
by	O
the	O
elements	O
of	O
w	O
and	O
hence	O
is	O
itself	O
gaussian	B
we	O
therefore	O
need	O
only	O
to	O
find	O
its	O
mean	B
and	O
covariance	B
which	O
are	O
given	O
from	O
by	O
ey	O
ew	O
exercise	O
covy	O
e	O
yyt	O
e	O
wwt	O
t	O
t	O
k	O
where	O
k	O
is	O
the	O
gram	B
matrix	I
with	O
elements	O
knm	O
kxn	O
xm	O
and	O
kx	O
is	O
the	O
kernel	B
function	I
this	O
model	O
provides	O
us	O
with	O
a	O
particular	O
example	O
of	O
a	O
gaussian	B
process	I
in	O
general	O
a	O
gaussian	B
process	I
is	O
defined	O
as	O
a	O
probability	B
distribution	O
over	O
functions	O
yx	O
such	O
that	O
the	O
set	O
of	O
values	O
of	O
yx	O
evaluated	O
at	O
an	O
arbitrary	O
set	O
of	O
points	O
xn	O
jointly	O
have	O
a	O
gaussian	B
distribution	O
in	O
cases	O
where	O
the	O
input	O
vector	O
x	O
is	O
two	O
dimensional	O
this	O
may	O
also	O
be	O
known	O
as	O
a	O
gaussian	B
random	I
field	I
more	O
generally	O
a	O
stochastic	B
process	I
yx	O
is	O
specified	O
by	O
giving	O
the	O
joint	O
probability	B
distribution	O
for	O
any	O
finite	O
set	O
of	O
values	O
yxn	O
in	O
a	O
consistent	B
manner	O
a	O
key	O
point	O
about	O
gaussian	B
stochastic	B
processes	O
is	O
that	O
the	O
joint	O
distribution	O
over	O
n	O
variables	O
yn	O
is	O
specified	O
completely	O
by	O
the	O
second-order	O
statistics	O
namely	O
the	O
mean	B
and	O
the	O
covariance	B
in	O
most	O
applications	O
we	O
will	O
not	O
have	O
any	O
prior	B
knowledge	O
about	O
the	O
mean	B
of	O
yx	O
and	O
so	O
by	O
symmetry	O
we	O
take	O
it	O
to	O
be	O
zero	O
this	O
is	O
equivalent	O
to	O
choosing	O
the	O
mean	B
of	O
the	O
prior	B
over	O
weight	O
values	O
pw	O
to	O
be	O
zero	O
in	O
the	O
basis	B
function	I
viewpoint	O
the	O
specification	O
of	O
the	O
gaussian	B
process	I
is	O
then	O
completed	O
by	O
giving	O
the	O
covariance	B
of	O
yx	O
evaluated	O
at	O
any	O
two	O
values	O
of	O
x	O
which	O
is	O
given	O
by	O
the	O
kernel	B
function	I
e	O
kxn	O
xm	O
for	O
the	O
specific	O
case	O
of	O
a	O
gaussian	B
process	I
defined	O
by	O
the	O
linear	B
regression	B
model	O
with	O
a	O
weight	O
prior	B
the	O
kernel	B
function	I
is	O
given	O
by	O
we	O
can	O
also	O
define	O
the	O
kernel	B
function	I
directly	O
rather	O
than	O
indirectly	O
through	O
a	O
choice	O
of	O
basis	B
function	I
figure	O
shows	O
samples	O
of	O
functions	O
drawn	O
from	O
gaussian	B
processes	O
for	O
two	O
different	O
choices	O
of	O
kernel	B
function	I
the	O
first	O
of	O
these	O
is	O
a	O
gaussian	B
kernel	I
of	O
the	O
form	O
and	O
the	O
second	O
is	O
the	O
exponential	O
kernel	O
given	O
by	O
which	O
corresponds	O
to	O
the	O
ornstein-uhlenbeck	B
process	I
originally	O
introduced	O
by	O
uhlenbeck	O
and	O
ornstein	O
to	O
describe	O
brownian	O
motion	O
exp	O
x	O
kx	O
x	O
kernel	O
methods	O
figure	O
samples	O
from	O
gaussian	B
processes	O
for	O
a	O
gaussian	B
kernel	I
and	O
an	O
exponential	O
kernel	O
gaussian	B
processes	O
for	B
regression	B
in	O
order	O
to	O
apply	O
gaussian	B
process	I
models	O
to	O
the	O
problem	O
of	O
regression	B
we	O
need	O
to	O
take	O
account	O
of	O
the	O
noise	O
on	O
the	O
observed	O
target	O
values	O
which	O
are	O
given	O
by	O
tn	O
yn	O
where	O
yn	O
yxn	O
and	O
is	O
a	O
random	O
noise	O
variable	O
whose	O
value	O
is	O
chosen	O
independently	O
for	O
each	O
observation	O
n	O
here	O
we	O
shall	O
consider	O
noise	O
processes	O
that	O
have	O
a	O
gaussian	B
distribution	O
so	O
that	O
ptnyn	O
n	O
where	O
is	O
a	O
hyperparameter	B
representing	O
the	O
precision	O
of	O
the	O
noise	O
because	O
the	O
noise	O
is	O
independent	B
for	O
each	O
data	O
point	O
the	O
joint	O
distribution	O
of	O
the	O
target	O
values	O
t	O
tnt	O
conditioned	O
on	O
the	O
values	O
of	O
y	O
ynt	O
is	O
given	O
by	O
an	O
isotropic	B
gaussian	B
of	O
the	O
form	O
pty	O
n	O
where	O
in	O
denotes	O
the	O
n	O
n	O
unit	O
matrix	O
from	O
the	O
definition	O
of	O
a	O
gaussian	B
process	I
the	O
marginal	B
distribution	O
py	O
is	O
given	O
by	O
a	O
gaussian	B
whose	O
mean	B
is	O
zero	O
and	O
whose	O
covariance	B
is	O
defined	O
by	O
a	O
gram	B
matrix	I
k	O
so	O
that	O
py	O
n	O
k	O
the	O
kernel	B
function	I
that	O
determines	O
k	O
is	O
typically	O
chosen	O
to	O
express	O
the	O
property	O
that	O
for	O
points	O
xn	O
and	O
xm	O
that	O
are	O
similar	O
the	O
corresponding	O
values	O
yxn	O
and	O
yxm	O
will	O
be	O
more	O
strongly	O
correlated	O
than	O
for	O
dissimilar	O
points	O
here	O
the	O
notion	O
of	O
similarity	O
will	O
depend	O
on	O
the	O
application	O
in	O
order	O
to	O
find	O
the	O
marginal	B
distribution	O
pt	O
conditioned	O
on	O
the	O
input	O
values	O
xn	O
we	O
need	O
to	O
integrate	O
over	O
y	O
this	O
can	O
be	O
done	O
by	O
making	O
use	O
of	O
the	O
results	O
from	O
section	O
for	O
the	O
linear-gaussian	B
model	I
using	O
we	O
see	O
that	O
the	O
marginal	B
distribution	O
of	O
t	O
is	O
given	O
by	O
pt	O
ptypy	O
dy	O
n	O
c	O
gaussian	B
processes	O
where	O
the	O
covariance	B
matrix	O
c	O
has	O
elements	O
cxn	O
xm	O
kxn	O
xm	O
nm	O
this	O
result	O
reflects	O
the	O
fact	O
that	O
the	O
two	O
gaussian	B
sources	O
of	O
randomness	O
namely	O
that	O
associated	O
with	O
yx	O
and	O
that	O
associated	O
with	O
are	O
independent	B
and	O
so	O
their	O
covariances	O
simply	O
add	O
one	O
widely	O
used	O
kernel	B
function	I
for	O
gaussian	B
process	I
regression	B
is	O
given	O
by	O
the	O
exponential	O
of	O
a	O
quadratic	O
form	O
with	O
the	O
addition	O
of	O
constant	O
and	O
linear	O
terms	O
to	O
give	O
kxn	O
xm	O
exp	O
nxm	O
note	O
that	O
the	O
term	O
involving	O
corresponds	O
to	O
a	O
parametric	O
model	O
that	O
is	O
a	O
linear	O
function	O
of	O
the	O
input	O
variables	O
samples	O
from	O
this	O
prior	B
are	O
plotted	O
for	O
various	O
values	O
of	O
the	O
parameters	O
in	O
figure	O
and	O
figure	O
shows	O
a	O
set	O
of	O
points	O
sampled	O
from	O
the	O
joint	O
distribution	O
along	O
with	O
the	O
corresponding	O
values	O
defined	O
by	O
so	O
far	O
we	O
have	O
used	O
the	O
gaussian	B
process	I
viewpoint	O
to	O
build	O
a	O
model	O
of	O
the	O
joint	O
distribution	O
over	O
sets	O
of	O
data	O
points	O
our	O
goal	O
in	O
regression	B
however	O
is	O
to	O
make	O
predictions	O
of	O
the	O
target	O
variables	O
for	O
new	O
inputs	O
given	O
a	O
set	O
of	O
training	B
data	O
let	O
us	O
suppose	O
that	O
tn	O
tnt	O
corresponding	O
to	O
input	O
values	O
xn	O
comprise	O
the	O
observed	O
training	B
set	I
and	O
our	O
goal	O
is	O
to	O
predict	O
the	O
target	O
variable	O
tn	O
for	O
a	O
new	O
input	O
vector	O
xn	O
this	O
requires	O
that	O
we	O
evaluate	O
the	O
predictive	B
distribution	I
ptn	O
note	O
that	O
this	O
distribution	O
is	O
conditioned	O
also	O
on	O
the	O
variables	O
xn	O
and	O
xn	O
however	O
to	O
keep	O
the	O
notation	O
simple	O
we	O
will	O
not	O
show	O
these	O
conditioning	O
variables	O
explicitly	O
to	O
find	O
the	O
conditional	B
distribution	O
ptn	O
we	O
begin	O
by	O
writing	O
down	O
the	O
joint	O
distribution	O
ptn	O
where	O
tn	O
denotes	O
the	O
vector	O
tn	O
tn	O
we	O
then	O
apply	O
the	O
results	O
from	O
section	O
to	O
obtain	O
the	O
required	O
conditional	B
distribution	O
as	O
illustrated	O
in	O
figure	O
from	O
the	O
joint	O
distribution	O
over	O
tn	O
will	O
be	O
given	O
by	O
ptn	O
n	O
cn	O
where	O
cn	O
is	O
an	O
covariance	B
matrix	O
with	O
elements	O
given	O
by	O
because	O
this	O
joint	O
distribution	O
is	O
gaussian	B
we	O
can	O
apply	O
the	O
results	O
from	O
section	O
to	O
find	O
the	O
conditional	B
gaussian	B
distribution	O
to	O
do	O
this	O
we	O
partition	O
the	O
covariance	B
matrix	O
as	O
follows	O
cn	O
where	O
cn	O
is	O
the	O
n	O
n	O
covariance	B
matrix	O
with	O
elements	O
given	O
by	O
for	O
n	O
m	O
n	O
the	O
vector	O
k	O
has	O
elements	O
kxn	O
xn	O
for	O
n	O
n	O
and	O
the	O
scalar	O
cn	O
k	O
kt	O
c	O
kernel	O
methods	O
figure	O
samples	O
from	O
a	O
gaussian	B
process	I
prior	B
defined	O
by	O
the	O
covariance	B
function	O
the	O
title	O
above	O
each	O
plot	O
denotes	O
using	O
the	O
results	O
and	O
we	O
see	O
that	O
the	O
conc	O
kxn	O
xn	O
ditional	O
distribution	O
ptn	O
is	O
a	O
gaussian	B
distribution	O
with	O
mean	B
and	O
covariance	B
given	O
by	O
mxn	O
ktc	O
n	O
t	O
c	O
ktc	O
n	O
k	O
these	O
are	O
the	O
key	O
results	O
that	O
define	O
gaussian	B
process	I
regression	B
because	O
the	O
vector	O
k	O
is	O
a	O
function	O
of	O
the	O
test	O
point	O
input	O
value	O
xn	O
we	O
see	O
that	O
the	O
predictive	B
distribution	I
is	O
a	O
gaussian	B
whose	O
mean	B
and	O
variance	B
both	O
depend	O
on	O
xn	O
an	O
example	O
of	O
gaussian	B
process	I
regression	B
is	O
shown	O
in	O
figure	O
the	O
only	O
restriction	O
on	O
the	O
kernel	B
function	I
is	O
that	O
the	O
covariance	B
matrix	O
given	O
by	O
must	O
be	O
positive	B
definite	I
if	O
i	O
is	O
an	O
eigenvalue	O
of	O
k	O
then	O
the	O
corresponding	O
it	O
is	O
therefore	O
sufficient	O
that	O
the	O
kernel	O
matrix	O
eigenvalue	O
of	O
c	O
will	O
be	O
i	O
kxn	O
xm	O
be	O
positive	O
semidefinite	O
for	O
any	O
pair	O
of	O
points	O
xn	O
and	O
xm	O
so	O
that	O
i	O
because	O
any	O
eigenvalue	O
i	O
that	O
is	O
zero	O
will	O
still	O
give	O
rise	O
to	O
a	O
positive	O
eigenvalue	O
for	O
c	O
because	O
this	O
is	O
the	O
same	O
restriction	O
on	O
the	O
kernel	B
function	I
discussed	O
earlier	O
and	O
so	O
we	O
can	O
again	O
exploit	O
all	O
of	O
the	O
techniques	O
in	O
section	O
to	O
construct	O
figure	O
illustration	O
of	O
the	O
sampling	O
of	O
data	O
points	O
from	O
a	O
gaussian	B
process	I
the	O
blue	O
curve	O
shows	O
a	O
sample	O
function	O
from	O
the	O
gaussian	B
process	I
prior	B
over	O
functions	O
and	O
the	O
red	O
points	O
show	O
the	O
values	O
of	O
yn	O
obtained	O
by	O
evaluating	O
the	O
function	O
at	O
a	O
set	O
of	O
input	O
values	O
the	O
corresponding	O
values	O
of	O
shown	O
in	O
green	O
are	O
obtained	O
by	O
adding	O
independent	B
gaussian	B
noise	O
to	O
each	O
of	O
the	O
t	O
suitable	O
kernels	O
gaussian	B
processes	O
x	O
note	O
that	O
the	O
mean	B
of	O
the	O
predictive	B
distribution	I
can	O
be	O
written	O
as	O
a	O
func	O
tion	O
of	O
xn	O
in	O
the	O
form	O
mxn	O
ankxn	O
xn	O
exercise	O
n	O
t	O
thus	O
if	O
the	O
kernel	B
function	I
kxn	O
xm	O
where	O
an	O
is	O
the	O
nth	O
component	O
of	O
c	O
depends	O
only	O
on	O
the	O
distance	O
then	O
we	O
obtain	O
an	O
expansion	O
in	O
radial	O
basis	O
functions	O
the	O
results	O
and	O
define	O
the	O
predictive	B
distribution	I
for	O
gaussian	B
process	I
regression	B
with	O
an	O
arbitrary	O
kernel	B
function	I
kxn	O
xm	O
in	O
the	O
particular	O
case	O
in	O
which	O
the	O
kernel	B
function	I
kx	O
is	O
defined	O
in	O
terms	O
of	O
a	O
finite	O
set	O
of	O
basis	O
functions	O
we	O
can	O
derive	O
the	O
results	O
obtained	O
previously	O
in	O
section	O
for	O
linear	B
regression	B
starting	O
from	O
the	O
gaussian	B
process	I
viewpoint	O
for	O
such	O
models	O
we	O
can	O
therefore	O
obtain	O
the	O
predictive	B
distribution	I
either	O
by	O
taking	O
a	O
parameter	O
space	O
viewpoint	O
and	O
using	O
the	O
linear	B
regression	B
result	O
or	O
by	O
taking	O
a	O
function	O
space	O
viewpoint	O
and	O
using	O
the	O
gaussian	B
process	I
result	O
the	O
central	O
computational	O
operation	O
in	O
using	O
gaussian	B
processes	O
will	O
involve	O
the	O
inversion	O
of	O
a	O
matrix	O
of	O
size	O
n	O
n	O
for	O
which	O
standard	O
methods	O
require	O
on	O
computations	O
by	O
contrast	O
in	O
the	O
basis	B
function	I
model	O
we	O
have	O
to	O
invert	O
a	O
matrix	O
sn	O
of	O
size	O
m	O
m	O
which	O
has	O
om	O
computational	O
complexity	O
note	O
that	O
for	O
both	O
viewpoints	O
the	O
matrix	O
inversion	O
must	O
be	O
performed	O
once	O
for	O
the	O
given	O
training	B
set	I
for	O
each	O
new	O
test	O
point	O
both	O
methods	O
require	O
a	O
vector-matrix	O
multiply	O
which	O
has	O
cost	O
on	O
in	O
the	O
gaussian	B
process	I
case	O
and	O
om	O
for	O
the	O
linear	O
basis	B
function	I
model	O
if	O
the	O
number	O
m	O
of	O
basis	O
functions	O
is	O
smaller	O
than	O
the	O
number	O
n	O
of	O
data	O
points	O
it	O
will	O
be	O
computationally	O
more	O
efficient	O
to	O
work	O
in	O
the	O
basis	B
function	I
kernel	O
methods	O
figure	O
illustration	O
of	O
the	O
mechanism	O
of	O
gaussian	B
process	I
regression	B
for	O
the	O
case	O
of	O
one	O
training	B
point	O
and	O
one	O
test	O
point	O
in	O
which	O
the	O
red	O
ellipses	O
show	O
contours	O
of	O
the	O
joint	O
distribution	O
here	O
is	O
the	O
training	B
data	O
point	O
and	O
conditioning	O
on	O
the	O
value	O
of	O
corresponding	O
to	O
the	O
vertical	O
blue	O
line	O
we	O
obtain	O
shown	O
as	O
a	O
function	O
of	O
by	O
the	O
green	O
curve	O
framework	O
however	O
an	O
advantage	O
of	O
a	O
gaussian	B
processes	O
viewpoint	O
is	O
that	O
we	O
can	O
consider	O
covariance	B
functions	O
that	O
can	O
only	O
be	O
expressed	O
in	O
terms	O
of	O
an	O
infinite	O
number	O
of	O
basis	O
functions	O
for	O
large	O
training	B
data	O
sets	O
however	O
the	O
direct	O
application	O
of	O
gaussian	B
process	I
methods	O
can	O
become	O
infeasible	O
and	O
so	O
a	O
range	O
of	O
approximation	O
schemes	O
have	O
been	O
developed	O
that	O
have	O
better	O
scaling	O
with	O
training	B
set	I
size	O
than	O
the	O
exact	O
approach	O
tresp	O
smola	O
and	O
bartlett	O
williams	O
and	O
seeger	O
csat	O
o	O
and	O
opper	O
seeger	O
et	O
al	O
practical	O
issues	O
in	O
the	O
application	O
of	O
gaussian	B
processes	O
are	O
discussed	O
in	O
bishop	O
and	O
nabney	O
we	O
have	O
introduced	O
gaussian	B
process	I
regression	B
for	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
the	O
extension	O
of	O
this	O
formalism	O
to	O
multiple	O
target	O
variables	O
known	O
as	O
co-kriging	O
is	O
straightforward	O
various	O
other	O
extensions	O
of	O
gaus	O
exercise	O
figure	O
illustration	O
of	O
gaussian	B
process	I
regression	B
applied	O
to	O
the	O
sinusoidal	B
data	I
set	O
in	O
figure	O
in	O
which	O
the	O
three	O
right-most	O
data	O
points	O
have	O
been	O
omitted	O
the	O
green	O
curve	O
shows	O
the	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
points	O
shown	O
in	O
blue	O
are	O
obtained	O
by	O
sampling	O
and	O
addition	O
of	O
gaussian	B
noise	O
the	O
red	O
line	O
shows	O
the	O
mean	B
of	O
the	O
gaussian	B
process	I
predictive	B
distribution	I
and	O
the	O
shaded	O
region	O
corresponds	O
to	O
plus	O
and	O
minus	O
two	O
standard	O
deviations	O
notice	O
how	O
the	O
uncertainty	O
increases	O
in	O
the	O
region	O
to	O
the	O
right	O
of	O
the	O
data	O
points	O
gaussian	B
processes	O
sian	O
process	O
regression	B
have	O
also	O
been	O
considered	O
for	O
purposes	O
such	O
as	O
modelling	O
the	O
distribution	O
over	O
low-dimensional	O
manifolds	O
for	O
unsupervised	B
learning	B
et	O
al	O
and	O
the	O
solution	O
of	O
stochastic	B
differential	B
equations	O
learning	B
the	O
hyperparameters	O
the	O
predictions	O
of	O
a	O
gaussian	B
process	I
model	O
will	O
depend	O
in	O
part	O
on	O
the	O
choice	O
of	O
covariance	B
function	O
in	O
practice	O
rather	O
than	O
fixing	O
the	O
covariance	B
function	O
we	O
may	O
prefer	O
to	O
use	O
a	O
parametric	O
family	O
of	O
functions	O
and	O
then	O
infer	O
the	O
parameter	O
values	O
from	O
the	O
data	O
these	O
parameters	O
govern	O
such	O
things	O
as	O
the	O
length	O
scale	O
of	O
the	O
correlations	O
and	O
the	O
precision	O
of	O
the	O
noise	O
and	O
correspond	O
to	O
the	O
hyperparameters	O
in	O
a	O
standard	O
parametric	O
model	O
techniques	O
for	O
learning	B
the	O
hyperparameters	O
are	O
based	O
on	O
the	O
evaluation	O
of	O
the	O
likelihood	B
function	I
pt	O
where	O
denotes	O
the	O
hyperparameters	O
of	O
the	O
gaussian	B
process	I
model	O
the	O
simplest	O
approach	O
is	O
to	O
make	O
a	O
point	O
estimate	O
of	O
by	O
maximizing	O
the	O
log	O
likelihood	B
function	I
because	O
represents	O
a	O
set	O
of	O
hyperparameters	O
for	O
the	O
regression	B
problem	O
this	O
can	O
be	O
viewed	O
as	O
analogous	O
to	O
the	O
type	O
maximum	B
likelihood	I
procedure	O
for	O
linear	B
regression	B
models	O
maximization	O
of	O
the	O
log	O
likelihood	O
can	O
be	O
done	O
using	O
efficient	O
gradient-based	O
optimization	O
algorithms	O
such	O
as	O
conjugate	B
gradients	O
nocedal	O
and	O
wright	O
bishop	O
and	O
nabney	O
the	O
log	O
likelihood	B
function	I
for	O
a	O
gaussian	B
process	I
regression	B
model	O
is	O
easily	O
evaluated	O
using	O
the	O
standard	O
form	O
for	O
a	O
multivariate	O
gaussian	B
distribution	O
giving	O
ln	O
pt	O
lncn	O
ttc	O
n	O
t	O
n	O
section	O
for	O
nonlinear	O
optimization	O
we	O
also	O
need	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
function	I
with	O
respect	O
to	O
the	O
parameter	O
vector	O
we	O
shall	O
assume	O
that	O
evaluation	O
of	O
the	O
derivatives	O
of	O
cn	O
is	O
straightforward	O
as	O
would	O
be	O
the	O
case	O
for	O
the	O
covariance	B
functions	O
considered	O
in	O
this	O
chapter	O
making	O
use	O
of	O
the	O
result	O
for	O
the	O
derivative	B
of	O
n	O
together	O
with	O
the	O
result	O
for	O
the	O
derivative	B
of	O
lncn	O
we	O
obtain	O
c	O
n	O
c	O
cn	O
i	O
ttc	O
n	O
cn	O
i	O
n	O
t	O
c	O
ln	O
pt	O
i	O
because	O
ln	O
pt	O
will	O
in	O
general	O
be	O
a	O
nonconvex	O
function	O
it	O
can	O
have	O
multiple	O
maxima	O
tr	O
it	O
is	O
straightforward	O
to	O
introduce	O
a	O
prior	B
over	O
and	O
to	O
maximize	O
the	O
log	O
posterior	O
using	O
gradient-based	O
methods	O
in	O
a	O
fully	O
bayesian	B
treatment	O
we	O
need	O
to	O
evaluate	O
marginals	O
over	O
weighted	O
by	O
the	O
product	O
of	O
the	O
prior	B
p	O
and	O
the	O
likelihood	B
function	I
pt	O
in	O
general	O
however	O
exact	O
marginalization	O
will	O
be	O
intractable	O
and	O
we	O
must	O
resort	O
to	O
approximations	O
the	O
gaussian	B
process	I
regression	B
model	O
gives	O
a	O
predictive	B
distribution	I
whose	O
mean	B
and	O
variance	B
are	O
functions	O
of	O
the	O
input	O
vector	O
x	O
however	O
we	O
have	O
assumed	O
that	O
the	O
contribution	O
to	O
the	O
predictive	O
variance	B
arising	O
from	O
the	O
additive	O
noise	O
governed	O
by	O
the	O
parameter	O
is	O
a	O
constant	O
for	O
some	O
problems	O
known	O
as	O
heteroscedastic	B
the	O
noise	O
variance	B
itself	O
will	O
also	O
depend	O
on	O
x	O
to	O
model	O
this	O
we	O
can	O
extend	O
the	O
kernel	O
methods	O
for	O
gaussian	B
processes	O
figure	O
samples	O
from	O
the	O
ard	O
prior	B
in	O
which	O
the	O
kernel	B
function	I
is	O
given	O
by	O
the	O
left	O
plot	O
corresponds	O
to	O
and	O
the	O
right	O
plot	O
corresponds	O
to	O
gaussian	B
process	I
framework	O
by	O
introducing	O
a	O
second	O
gaussian	B
process	I
to	O
represent	O
the	O
dependence	O
of	O
on	O
the	O
input	O
x	O
et	O
al	O
because	O
is	O
a	O
variance	B
and	O
hence	O
nonnegative	O
we	O
use	O
the	O
gaussian	B
process	I
to	O
model	O
ln	O
automatic	B
relevance	I
determination	I
in	O
the	O
previous	O
section	O
we	O
saw	O
how	O
maximum	B
likelihood	I
could	O
be	O
used	O
to	O
determine	O
a	O
value	O
for	O
the	O
correlation	O
length-scale	O
parameter	O
in	O
a	O
gaussian	B
process	I
this	O
technique	O
can	O
usefully	O
be	O
extended	B
by	O
incorporating	O
a	O
separate	O
parameter	O
for	O
each	O
input	O
variable	O
and	O
williams	O
the	O
result	O
as	O
we	O
shall	O
see	O
is	O
that	O
the	O
optimization	O
of	O
these	O
parameters	O
by	O
maximum	B
likelihood	I
allows	O
the	O
relative	B
importance	O
of	O
different	O
inputs	O
to	O
be	O
inferred	O
from	O
the	O
data	O
this	O
represents	O
an	O
example	O
in	O
the	O
gaussian	B
process	I
context	O
of	O
automatic	B
relevance	I
determination	I
or	O
ard	O
which	O
was	O
originally	O
formulated	O
in	O
the	O
framework	O
of	O
neural	O
networks	O
neal	O
the	O
mechanism	O
by	O
which	O
appropriate	O
inputs	O
are	O
preferred	O
is	O
discussed	O
in	O
section	O
consider	O
a	O
gaussian	B
process	I
with	O
a	O
two-dimensional	O
input	O
space	O
x	O
having	O
a	O
kernel	B
function	I
of	O
the	O
form	O
kx	O
exp	O
ixi	O
x	O
samples	O
from	O
the	O
resulting	O
prior	B
over	O
functions	O
yx	O
are	O
shown	O
for	O
two	O
different	O
settings	O
of	O
the	O
precision	O
parameters	O
i	O
in	O
figure	O
we	O
see	O
that	O
as	O
a	O
particular	O
parameter	O
i	O
becomes	O
small	O
the	O
function	O
becomes	O
relatively	O
insensitive	O
to	O
the	O
corresponding	O
input	O
variable	O
xi	O
by	O
adapting	O
these	O
parameters	O
to	O
a	O
data	O
set	O
using	O
maximum	B
likelihood	I
it	O
becomes	O
possible	O
to	O
detect	O
input	O
variables	O
that	O
have	O
little	O
effect	O
on	O
the	O
predictive	B
distribution	I
because	O
the	O
corresponding	O
values	O
of	O
i	O
will	O
be	O
small	O
this	O
can	O
be	O
useful	O
in	O
practice	O
because	O
it	O
allows	O
such	O
inputs	O
to	O
be	O
discarded	O
ard	O
is	O
illustrated	O
using	O
a	O
simple	O
synthetic	O
data	O
set	O
having	O
three	O
inputs	O
and	O
in	O
figure	O
the	O
target	O
variable	O
t	O
is	O
generated	O
by	O
sampling	O
values	O
of	O
from	O
a	O
gaussian	B
evaluating	O
the	O
function	O
and	O
then	O
adding	O
gaussian	B
processes	O
figure	O
illustration	O
of	O
automatic	B
relevance	I
determination	I
in	O
a	O
gaussian	B
process	I
for	O
a	O
synthetic	O
problem	O
having	O
three	O
inputs	O
and	O
for	O
which	O
the	O
curves	O
show	O
the	O
corresponding	O
values	O
of	O
the	O
hyperparameters	O
and	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
iterations	O
when	O
optimizing	O
the	O
marginal	B
likelihood	I
details	O
are	O
given	O
in	O
the	O
text	O
note	O
the	O
logarithmic	O
scale	O
on	O
the	O
vertical	O
axis	O
gaussian	B
noise	O
values	O
of	O
are	O
given	O
by	O
copying	O
the	O
corresponding	O
values	O
of	O
and	O
adding	O
noise	O
and	O
values	O
of	O
are	O
sampled	O
from	O
an	O
independent	B
gaussian	B
distribution	O
thus	O
is	O
a	O
good	O
predictor	O
of	O
t	O
is	O
a	O
more	O
noisy	O
predictor	O
of	O
t	O
and	O
has	O
only	O
chance	O
correlations	O
with	O
t	O
the	O
marginal	B
likelihood	I
for	O
a	O
gaussian	B
process	I
with	O
ard	O
parameters	O
is	O
optimized	O
using	O
the	O
scaled	O
conjugate	B
gradients	O
algorithm	O
we	O
see	O
from	O
figure	O
that	O
converges	O
to	O
a	O
relatively	O
large	O
value	O
converges	O
to	O
a	O
much	O
smaller	O
value	O
and	O
becomes	O
very	O
small	O
indicating	O
that	O
is	O
irrelevant	O
for	O
predicting	O
t	O
the	O
ard	O
framework	O
is	O
easily	O
incorporated	O
into	O
the	O
exponential-quadratic	O
kernel	O
to	O
give	O
the	O
following	O
form	O
of	O
kernel	B
function	I
which	O
has	O
been	O
found	O
useful	O
for	O
applications	O
of	O
gaussian	B
processes	O
to	O
a	O
range	O
of	O
regression	B
problems	O
kxn	O
xm	O
exp	O
ixni	O
xnixmi	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
the	O
input	O
space	O
gaussian	B
processes	O
for	O
classification	B
in	O
a	O
probabilistic	O
approach	O
to	O
classification	B
our	O
goal	O
is	O
to	O
model	O
the	O
posterior	O
probabilities	O
of	O
the	O
target	O
variable	O
for	O
a	O
new	O
input	O
vector	O
given	O
a	O
set	O
of	O
training	B
data	O
these	O
probabilities	O
must	O
lie	O
in	O
the	O
interval	O
whereas	O
a	O
gaussian	B
process	I
model	O
makes	O
predictions	O
that	O
lie	O
on	O
the	O
entire	O
real	O
axis	O
however	O
we	O
can	O
easily	O
adapt	O
gaussian	B
processes	O
to	O
classification	B
problems	O
by	O
transforming	O
the	O
output	O
of	O
the	O
gaussian	B
process	I
using	O
an	O
appropriate	O
nonlinear	O
activation	B
function	I
consider	O
first	O
the	O
two-class	O
problem	O
with	O
a	O
target	O
variable	O
t	O
if	O
we	O
define	O
a	O
gaussian	B
process	I
over	O
a	O
function	O
ax	O
and	O
then	O
transform	O
the	O
function	O
using	O
a	O
logistic	B
sigmoid	I
y	O
given	O
by	O
then	O
we	O
will	O
obtain	O
a	O
non-gaussian	O
stochastic	B
process	I
over	O
functions	O
yx	O
where	O
y	O
this	O
is	O
illustrated	O
for	O
the	O
case	O
of	O
a	O
one-dimensional	O
input	O
space	O
in	O
figure	O
in	O
which	O
the	O
probability	B
distri	O
kernel	O
methods	O
figure	O
the	O
left	O
plot	O
shows	O
a	O
sample	O
from	O
a	O
gaussian	B
process	I
prior	B
over	O
functions	O
ax	O
and	O
the	O
right	O
plot	O
shows	O
the	O
result	O
of	O
transforming	O
this	O
sample	O
using	O
a	O
logistic	B
sigmoid	I
function	O
bution	O
over	O
the	O
target	O
variable	O
t	O
is	O
then	O
given	O
by	O
the	O
bernoulli	B
distribution	I
pta	O
t	O
as	O
usual	O
we	O
denote	O
the	O
training	B
set	I
inputs	O
by	O
xn	O
with	O
corresponding	O
observed	O
target	O
variables	O
t	O
tnt	O
we	O
also	O
consider	O
a	O
single	O
test	O
point	O
xn	O
with	O
target	O
value	O
tn	O
our	O
goal	O
is	O
to	O
determine	O
the	O
predictive	B
distribution	I
ptn	O
where	O
we	O
have	O
left	O
the	O
conditioning	O
on	O
the	O
input	O
variables	O
implicit	O
to	O
do	O
this	O
we	O
introduce	O
a	O
gaussian	B
process	I
prior	B
over	O
the	O
vector	O
an	O
which	O
has	O
components	O
axn	O
this	O
in	O
turn	O
defines	O
a	O
non-gaussian	O
process	O
over	O
tn	O
and	O
by	O
conditioning	O
on	O
the	O
training	B
data	O
tn	O
we	O
obtain	O
the	O
required	O
predictive	B
distribution	I
the	O
gaussian	B
process	I
prior	B
for	O
an	O
takes	O
the	O
form	O
pan	O
n	O
cn	O
unlike	O
the	O
regression	B
case	O
the	O
covariance	B
matrix	O
no	O
longer	O
includes	O
a	O
noise	O
term	O
because	O
we	O
assume	O
that	O
all	O
of	O
the	O
training	B
data	O
points	O
are	O
correctly	O
labelled	O
however	O
for	O
numerical	O
reasons	O
it	O
is	O
convenient	O
to	O
introduce	O
a	O
noise-like	O
term	O
governed	O
by	O
a	O
parameter	O
that	O
ensures	O
that	O
the	O
covariance	B
matrix	O
is	O
positive	B
definite	I
thus	O
the	O
covariance	B
matrix	O
cn	O
has	O
elements	O
given	O
by	O
cxn	O
xm	O
kxn	O
xm	O
nm	O
where	O
kxn	O
xm	O
is	O
any	O
positive	O
semidefinite	O
kernel	B
function	I
of	O
the	O
kind	O
considered	O
in	O
section	O
and	O
the	O
value	O
of	O
is	O
typically	O
fixed	O
in	O
advance	O
we	O
shall	O
assume	O
that	O
the	O
kernel	B
function	I
kx	O
is	O
governed	O
by	O
a	O
vector	O
of	O
parameters	O
and	O
we	O
shall	O
later	O
discuss	O
how	O
may	O
be	O
learned	O
from	O
the	O
training	B
data	O
for	O
two-class	O
problems	O
it	O
is	O
sufficient	O
to	O
predict	O
ptn	O
because	O
the	O
value	O
of	O
ptn	O
is	O
then	O
given	O
by	O
ptn	O
the	O
required	O
gaussian	B
processes	O
predictive	B
distribution	I
is	O
given	O
by	O
ptn	O
ptn	O
dan	O
where	O
ptn	O
this	O
integral	O
is	O
analytically	O
intractable	O
and	O
so	O
may	O
be	O
approximated	O
using	O
sampling	B
methods	I
alternatively	O
we	O
can	O
consider	O
techniques	O
based	O
on	O
an	O
analytical	O
approximation	O
in	O
section	O
we	O
derived	O
the	O
approximate	O
formula	O
for	O
the	O
convolution	O
of	O
a	O
logistic	B
sigmoid	I
with	O
a	O
gaussian	B
distribution	O
we	O
can	O
use	O
this	O
result	O
to	O
evaluate	O
the	O
integral	O
in	O
provided	O
we	O
have	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
pan	O
the	O
usual	O
justification	O
for	O
a	O
gaussian	B
approximation	O
to	O
a	O
posterior	O
distribution	O
is	O
that	O
the	O
true	O
posterior	O
will	O
tend	O
to	O
a	O
gaussian	B
as	O
the	O
number	O
of	O
data	O
points	O
increases	O
as	O
a	O
consequence	O
of	O
the	O
central	B
limit	I
theorem	I
in	O
the	O
case	O
of	O
gaussian	B
processes	O
the	O
number	O
of	O
variables	O
grows	O
with	O
the	O
number	O
of	O
data	O
points	O
and	O
so	O
this	O
argument	O
does	O
not	O
apply	O
directly	O
however	O
if	O
we	O
consider	O
increasing	O
the	O
number	O
of	O
data	O
points	O
falling	O
in	O
a	O
fixed	O
region	O
of	O
x	O
space	O
then	O
the	O
corresponding	O
uncertainty	O
in	O
the	O
function	O
ax	O
will	O
decrease	O
again	O
leading	O
asymptotically	O
to	O
a	O
gaussian	B
and	O
barber	O
three	O
different	O
approaches	O
to	O
obtaining	O
a	O
gaussian	B
approximation	O
have	O
been	O
considered	O
one	O
technique	O
is	O
based	O
on	O
variational	B
inference	B
and	O
mackay	O
and	O
makes	O
use	O
of	O
the	O
local	B
variational	B
bound	O
on	O
the	O
logistic	B
sigmoid	I
this	O
allows	O
the	O
product	O
of	O
sigmoid	O
functions	O
to	O
be	O
approximated	O
by	O
a	O
product	O
of	O
gaussians	O
thereby	O
allowing	O
the	O
marginalization	O
over	O
an	O
to	O
be	O
performed	O
analytically	O
the	O
approach	O
also	O
yields	O
a	O
lower	B
bound	I
on	O
the	O
likelihood	B
function	I
ptn	O
the	O
variational	B
framework	O
for	O
gaussian	B
process	I
classification	B
can	O
also	O
be	O
extended	B
to	O
multiclass	B
problems	O
by	O
using	O
a	O
gaussian	B
approximation	O
to	O
the	O
softmax	B
function	I
a	O
second	O
approach	O
uses	O
expectation	B
propagation	I
and	O
winther	O
minka	O
seeger	O
because	O
the	O
true	O
posterior	O
distribution	O
is	O
unimodal	O
as	O
we	O
shall	O
see	O
shortly	O
the	O
expectation	B
propagation	I
approach	O
can	O
give	O
good	O
results	O
laplace	B
approximation	I
the	O
third	O
approach	O
to	O
gaussian	B
process	I
classification	B
is	O
based	O
on	O
the	O
laplace	B
approximation	I
which	O
we	O
now	O
consider	O
in	O
detail	O
in	O
order	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
we	O
seek	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
over	O
an	O
which	O
using	O
bayes	B
theorem	O
is	O
given	O
by	O
pan	O
antn	O
dan	O
pan	O
pan	O
anptnan	O
an	O
dan	O
pan	O
dan	O
ptn	O
ptn	O
pan	O
dan	O
section	O
section	O
section	O
section	O
kernel	O
methods	O
where	O
we	O
have	O
used	O
ptnan	O
an	O
ptnan	O
the	O
conditional	B
distribution	O
pan	O
is	O
obtained	O
by	O
invoking	O
the	O
results	O
and	O
for	O
gaussian	B
process	I
regression	B
to	O
give	O
pan	O
n	O
n	O
an	O
c	O
ktc	O
n	O
k	O
we	O
can	O
therefore	O
evaluate	O
the	O
integral	O
in	O
by	O
finding	O
a	O
laplace	B
approximation	I
for	O
the	O
posterior	O
distribution	O
pantn	O
and	O
then	O
using	O
the	O
standard	O
result	O
for	O
the	O
convolution	O
of	O
two	O
gaussian	B
distributions	O
the	O
prior	B
pan	O
is	O
given	O
by	O
a	O
zero-mean	O
gaussian	B
process	I
with	O
covariance	B
ma	O
trix	O
cn	O
and	O
the	O
data	O
term	O
independence	O
of	O
the	O
data	O
points	O
is	O
given	O
by	O
ptnan	O
tn	O
eantn	O
an	O
we	O
then	O
obtain	O
the	O
laplace	B
approximation	I
by	O
taylor	O
expanding	O
the	O
logarithm	O
of	O
pantn	O
which	O
up	O
to	O
an	O
additive	O
normalization	O
constant	O
is	O
given	O
by	O
the	O
quantity	O
ln	O
pan	O
ln	O
ptnan	O
at	O
n	O
c	O
n	O
an	O
n	O
lncn	O
tt	O
n	O
an	O
ean	O
const	O
first	O
we	O
need	O
to	O
find	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
and	O
this	O
requires	O
that	O
we	O
evaluate	O
the	O
gradient	O
of	O
which	O
is	O
given	O
by	O
tn	O
n	O
c	O
n	O
an	O
where	O
n	O
is	O
a	O
vector	O
with	O
elements	O
we	O
cannot	O
simply	O
find	O
the	O
mode	O
by	O
setting	O
this	O
gradient	O
to	O
zero	O
because	O
n	O
depends	O
nonlinearly	O
on	O
an	O
and	O
so	O
we	O
resort	O
to	O
an	O
iterative	O
scheme	O
based	O
on	O
the	O
newton-raphson	B
method	O
which	O
gives	O
rise	O
to	O
an	O
iterative	B
reweighted	I
least	I
squares	I
algorithm	O
this	O
requires	O
the	O
second	O
derivatives	O
of	O
which	O
we	O
also	O
require	O
for	O
the	O
laplace	B
approximation	I
anyway	O
and	O
which	O
are	O
given	O
by	O
wn	O
c	O
n	O
where	O
wn	O
is	O
a	O
diagonal	B
matrix	O
with	O
elements	O
and	O
we	O
have	O
used	O
the	O
result	O
for	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
function	O
note	O
that	O
these	O
diagonal	B
elements	O
lie	O
in	O
the	O
range	O
and	O
hence	O
wn	O
is	O
a	O
positive	B
definite	I
matrix	I
because	O
cn	O
hence	O
its	O
inverse	B
is	O
positive	B
definite	I
by	O
construction	O
and	O
because	O
the	O
sum	O
of	O
two	O
positive	B
definite	I
matrices	O
is	O
also	O
positive	B
definite	I
we	O
see	O
that	O
the	O
hessian	B
matrix	I
a	O
is	O
positive	B
definite	I
and	O
so	O
the	O
posterior	O
distribution	O
pantn	O
is	O
log	O
convex	O
and	O
therefore	O
has	O
a	O
single	O
mode	O
that	O
is	O
the	O
global	O
section	O
exercise	O
gaussian	B
processes	O
maximum	O
the	O
posterior	O
distribution	O
is	O
not	O
gaussian	B
however	O
because	O
the	O
hessian	O
is	O
a	O
function	O
of	O
an	O
using	O
the	O
newton-raphson	B
formula	O
the	O
iterative	O
update	O
equation	O
for	O
an	O
exercise	O
is	O
given	O
by	O
n	O
cn	O
wn	O
cn	O
n	O
wn	O
an	O
anew	O
these	O
equations	O
are	O
iterated	O
until	O
they	O
converge	O
to	O
the	O
mode	O
which	O
we	O
denote	O
by	O
n	O
at	O
the	O
mode	O
the	O
gradient	O
will	O
vanish	O
and	O
hence	O
n	O
will	O
satisfy	O
n	O
cn	O
n	O
once	O
we	O
have	O
found	O
the	O
mode	O
n	O
of	O
the	O
posterior	O
we	O
can	O
evaluate	O
the	O
hessian	B
matrix	I
given	O
by	O
h	O
wn	O
c	O
n	O
where	O
the	O
elements	O
of	O
wn	O
are	O
evaluated	O
using	O
proximation	O
to	O
the	O
posterior	O
distribution	O
pantn	O
given	O
by	O
n	O
this	O
defines	O
our	O
gaussian	B
ap	O
qan	O
n	O
n	O
h	O
exercise	O
we	O
can	O
now	O
combine	O
this	O
with	O
and	O
hence	O
evaluate	O
the	O
integral	O
because	O
this	O
corresponds	O
to	O
a	O
linear-gaussian	B
model	I
we	O
can	O
use	O
the	O
general	O
result	O
to	O
give	O
ean	O
kttn	O
n	O
varan	O
c	O
ktw	O
n	O
cn	O
now	O
that	O
we	O
have	O
a	O
gaussian	B
distribution	O
for	O
pan	O
we	O
can	O
approximate	O
the	O
integral	O
using	O
the	O
result	O
as	O
with	O
the	O
bayesian	B
logistic	B
regression	B
model	O
of	O
section	O
if	O
we	O
are	O
only	O
interested	O
in	O
the	O
decision	B
boundary	I
corresponding	O
to	O
ptn	O
then	O
we	O
need	O
only	O
consider	O
the	O
mean	B
and	O
we	O
can	O
ignore	O
the	O
effect	O
of	O
the	O
variance	B
we	O
also	O
need	O
to	O
determine	O
the	O
parameters	O
of	O
the	O
covariance	B
function	O
one	O
approach	O
is	O
to	O
maximize	O
the	O
likelihood	B
function	I
given	O
by	O
ptn	O
for	O
which	O
we	O
need	O
expressions	O
for	O
the	O
log	O
likelihood	O
and	O
its	O
gradient	O
if	O
desired	O
suitable	O
regularization	B
terms	O
can	O
also	O
be	O
added	O
leading	O
to	O
a	O
penalized	O
maximum	B
likelihood	I
solution	O
the	O
likelihood	B
function	I
is	O
defined	O
by	O
ptn	O
ptnan	O
dan	O
this	O
integral	O
is	O
analytically	O
intractable	O
so	O
again	O
we	O
make	O
use	O
of	O
the	O
laplace	B
approximation	I
using	O
the	O
result	O
we	O
obtain	O
the	O
following	O
approximation	O
for	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
ln	O
ptn	O
n	O
lnwn	O
c	O
n	O
n	O
kernel	O
methods	O
n	O
ln	O
n	O
ln	O
where	O
n	O
we	O
also	O
need	O
to	O
evaluate	O
the	O
gradient	O
of	O
ln	O
ptn	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
note	O
that	O
changes	O
in	O
will	O
cause	O
changes	O
in	O
n	O
leading	O
to	O
additional	O
terms	O
in	O
the	O
gradient	O
thus	O
when	O
we	O
differentiate	O
with	O
respect	O
to	O
we	O
obtain	O
two	O
sets	O
of	O
terms	O
the	O
first	O
arising	O
from	O
the	O
dependence	O
of	O
the	O
covariance	B
matrix	O
cn	O
on	O
and	O
the	O
rest	O
arising	O
from	O
dependence	O
of	O
n	O
on	O
the	O
terms	O
arising	O
from	O
the	O
explicit	O
dependence	O
on	O
can	O
be	O
found	O
by	O
using	O
together	O
with	O
the	O
results	O
and	O
and	O
are	O
given	O
by	O
ln	O
ptn	O
j	O
n	O
c	O
n	O
cn	O
j	O
n	O
n	O
c	O
tr	O
cn	O
wn	O
cn	O
j	O
n	O
and	O
so	O
to	O
compute	O
the	O
terms	O
arising	O
from	O
the	O
dependence	O
of	O
n	O
on	O
we	O
note	O
that	O
the	O
laplace	B
approximation	I
has	O
been	O
constructed	O
such	O
that	O
has	O
zero	O
gradient	O
at	O
an	O
n	O
gives	O
no	O
contribution	O
to	O
the	O
gradient	O
as	O
a	O
result	O
of	O
its	O
dependence	O
on	O
n	O
this	O
leaves	O
the	O
following	O
contribution	O
to	O
the	O
derivative	B
with	O
respect	O
to	O
a	O
component	O
j	O
of	O
n	O
lnwn	O
c	O
n	O
n	O
j	O
cn	O
wn	O
nn	O
n	O
n	O
j	O
n	O
where	O
definition	O
of	O
wn	O
we	O
can	O
evaluate	O
the	O
derivative	B
of	O
entiating	O
the	O
relation	O
with	O
respect	O
to	O
j	O
to	O
give	O
n	O
and	O
again	O
we	O
have	O
used	O
the	O
result	O
together	O
with	O
the	O
n	O
with	O
respect	O
to	O
j	O
by	O
differ	O
n	O
j	O
cn	O
j	O
n	O
cn	O
wn	O
n	O
j	O
rearranging	O
then	O
gives	O
n	O
j	O
wn	O
cn	O
cn	O
j	O
n	O
combining	O
and	O
we	O
can	O
evaluate	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
function	I
which	O
can	O
be	O
used	O
with	O
standard	O
nonlinear	O
optimization	O
algorithms	O
in	O
order	O
to	O
determine	O
a	O
value	O
for	O
we	O
can	O
illustrate	O
the	O
application	O
of	O
the	O
laplace	B
approximation	I
for	O
gaussian	B
processes	O
using	O
the	O
synthetic	O
two-class	O
data	O
set	O
shown	O
in	O
figure	O
extension	O
of	O
the	O
laplace	B
approximation	I
to	O
gaussian	B
processes	O
involving	O
k	O
classes	O
using	O
the	O
softmax	O
activation	B
function	I
is	O
straightforward	O
and	O
barber	O
appendix	O
a	O
gaussian	B
processes	O
figure	O
illustration	O
of	O
the	O
use	O
of	O
a	O
gaussian	B
process	I
for	O
classification	B
showing	O
the	O
data	O
on	O
the	O
left	O
together	O
with	O
the	O
optimal	O
decision	B
boundary	I
from	O
the	O
true	O
distribution	O
in	O
green	O
and	O
the	O
decision	B
boundary	I
from	O
the	O
gaussian	B
process	I
classifier	O
in	O
black	O
on	O
the	O
right	O
is	O
the	O
predicted	O
posterior	B
probability	B
for	O
the	O
blue	O
and	O
red	O
classes	O
together	O
with	O
the	O
gaussian	B
process	I
decision	B
boundary	I
connection	O
to	O
neural	O
networks	O
we	O
have	O
seen	O
that	O
the	O
range	O
of	O
functions	O
which	O
can	O
be	O
represented	O
by	O
a	O
neural	B
network	I
is	O
governed	O
by	O
the	O
number	O
m	O
of	O
hidden	O
units	O
and	O
that	O
for	O
sufficiently	O
large	O
m	O
a	O
two-layer	O
network	O
can	O
approximate	O
any	O
given	O
function	O
with	O
arbitrary	O
accuracy	O
in	O
the	O
framework	O
of	O
maximum	B
likelihood	I
the	O
number	O
of	O
hidden	O
units	O
needs	O
to	O
be	O
limited	O
a	O
level	O
dependent	O
on	O
the	O
size	O
of	O
the	O
training	B
set	I
in	O
order	O
to	O
avoid	O
over-fitting	B
however	O
from	O
a	O
bayesian	B
perspective	O
it	O
makes	O
little	O
sense	O
to	O
limit	O
the	O
number	O
of	O
parameters	O
in	O
the	O
network	O
according	O
to	O
the	O
size	O
of	O
the	O
training	B
set	I
in	O
a	O
bayesian	B
neural	B
network	I
the	O
prior	B
distribution	O
over	O
the	O
parameter	O
vector	O
w	O
in	O
conjunction	O
with	O
the	O
network	O
function	O
fx	O
w	O
produces	O
a	O
prior	B
distribution	O
over	O
functions	O
from	O
yx	O
where	O
y	O
is	O
the	O
vector	O
of	O
network	O
outputs	O
neal	O
has	O
shown	O
that	O
for	O
a	O
broad	O
class	O
of	O
prior	B
distributions	O
over	O
w	O
the	O
distribution	O
of	O
functions	O
generated	O
by	O
a	O
neural	B
network	I
will	O
tend	O
to	O
a	O
gaussian	B
process	I
in	O
the	O
limit	O
m	O
it	O
should	O
be	O
noted	O
however	O
that	O
in	O
this	O
limit	O
the	O
output	O
variables	O
of	O
the	O
neural	B
network	I
become	O
independent	B
one	O
of	O
the	O
great	O
merits	O
of	O
neural	O
networks	O
is	O
that	O
the	O
outputs	O
share	O
the	O
hidden	O
units	O
and	O
so	O
they	O
can	O
borrow	O
statistical	O
strength	O
from	O
each	O
other	O
that	O
is	O
the	O
weights	O
associated	O
with	O
each	O
hidden	B
unit	I
are	O
influenced	O
by	O
all	O
of	O
the	O
output	O
variables	O
not	O
just	O
by	O
one	O
of	O
them	O
this	O
property	O
is	O
therefore	O
lost	O
in	O
the	O
gaussian	B
process	I
limit	O
we	O
have	O
seen	O
that	O
a	O
gaussian	B
process	I
is	O
determined	O
by	O
its	O
covariance	B
function	O
williams	O
has	O
given	O
explicit	O
forms	O
for	O
the	O
covariance	B
in	O
the	O
case	O
of	O
two	O
specific	O
choices	O
for	O
the	O
hidden	B
unit	I
activation	B
function	I
and	O
gaussian	B
these	O
kernel	O
functions	O
kx	O
are	O
nonstationary	O
i	O
e	O
they	O
cannot	O
be	O
expressed	O
as	O
a	O
function	O
of	O
the	O
difference	O
x	O
as	O
a	O
consequence	O
of	O
the	O
gaussian	B
weight	O
prior	B
being	O
centred	O
on	O
zero	O
which	O
breaks	O
translation	B
invariance	B
in	O
weight	O
space	O
kernel	O
methods	O
exercises	O
by	O
working	O
directly	O
with	O
the	O
covariance	B
function	O
we	O
have	O
implicitly	O
marginalized	O
over	O
the	O
distribution	O
of	O
weights	O
if	O
the	O
weight	O
prior	B
is	O
governed	O
by	O
hyperparameters	O
then	O
their	O
values	O
will	O
determine	O
the	O
length	O
scales	O
of	O
the	O
distribution	O
over	O
functions	O
as	O
can	O
be	O
understood	O
by	O
studying	O
the	O
examples	O
in	O
figure	O
for	O
the	O
case	O
of	O
a	O
finite	O
number	O
of	O
hidden	O
units	O
note	O
that	O
we	O
cannot	O
marginalize	O
out	O
the	O
hyperparameters	O
analytically	O
and	O
must	O
instead	O
resort	O
to	O
techniques	O
of	O
the	O
kind	O
discussed	O
in	O
section	O
www	O
consider	O
the	O
dual	O
formulation	O
of	O
the	O
least	O
squares	O
linear	B
regression	B
problem	O
given	O
in	O
section	O
show	O
that	O
the	O
solution	O
for	O
the	O
components	O
an	O
of	O
the	O
vector	O
a	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
combination	O
of	O
the	O
elements	O
of	O
the	O
vector	O
denoting	O
these	O
coefficients	O
by	O
the	O
vector	O
w	O
show	O
that	O
the	O
dual	O
of	O
the	O
dual	O
formulation	O
is	O
given	O
by	O
the	O
original	O
representation	O
in	O
terms	O
of	O
the	O
parameter	O
vector	O
w	O
in	O
this	O
exercise	O
we	O
develop	O
a	O
dual	O
formulation	O
of	O
the	O
perceptron	B
learning	B
algorithm	O
using	O
the	O
perceptron	B
learning	B
rule	O
show	O
that	O
the	O
learned	O
weight	B
vector	I
w	O
can	O
be	O
written	O
as	O
a	O
linear	O
combination	O
of	O
the	O
vectors	O
tn	O
where	O
tn	O
denote	O
the	O
coefficients	O
of	O
this	O
linear	O
combination	O
by	O
n	O
and	O
derive	O
a	O
formulation	O
of	O
the	O
perceptron	B
learning	B
algorithm	O
and	O
the	O
predictive	O
function	O
for	O
the	O
perceptron	B
in	O
terms	O
of	O
the	O
n	O
show	O
that	O
the	O
feature	O
vector	O
enters	O
only	O
in	O
the	O
form	O
of	O
the	O
kernel	B
function	I
kx	O
the	O
nearest-neighbour	O
classifier	O
assigns	O
a	O
new	O
input	O
vector	O
x	O
to	O
the	O
same	O
class	O
as	O
that	O
of	O
the	O
nearest	O
input	O
vector	O
xn	O
from	O
the	O
training	B
set	I
where	O
in	O
the	O
simplest	O
case	O
the	O
distance	O
is	O
defined	O
by	O
the	O
euclidean	O
metric	O
by	O
expressing	O
this	O
rule	O
in	O
terms	O
of	O
scalar	O
products	O
and	O
then	O
making	O
use	O
of	O
kernel	B
substitution	I
formulate	O
the	O
nearest-neighbour	O
classifier	O
for	O
a	O
general	O
nonlinear	O
kernel	O
in	O
appendix	O
c	O
we	O
give	O
an	O
example	O
of	O
a	O
matrix	O
that	O
has	O
positive	O
elements	O
but	O
that	O
has	O
a	O
negative	O
eigenvalue	O
and	O
hence	O
that	O
is	O
not	O
positive	B
definite	I
find	O
an	O
example	O
of	O
the	O
converse	O
property	O
namely	O
a	O
matrix	O
with	O
positive	O
eigenvalues	O
yet	O
that	O
has	O
at	O
least	O
one	O
negative	O
element	O
www	O
verify	O
the	O
results	O
and	O
for	O
constructing	O
valid	O
kernels	O
verify	O
the	O
results	O
and	O
for	O
constructing	O
valid	O
kernels	O
www	O
verify	O
the	O
results	O
and	O
for	O
constructing	O
valid	O
kernels	O
verify	O
the	O
results	O
and	O
for	O
constructing	O
valid	O
kernels	O
verify	O
the	O
results	O
and	O
for	O
constructing	O
valid	O
kernels	O
show	O
that	O
an	O
excellent	O
choice	O
of	O
kernel	O
for	O
learning	B
a	O
function	O
fx	O
is	O
given	O
by	O
kx	O
by	O
showing	O
that	O
a	O
linear	O
learning	B
machine	O
based	O
on	O
this	O
kernel	O
will	O
always	O
find	O
a	O
solution	O
proportional	O
to	O
fx	O
by	O
making	O
use	O
of	O
the	O
expansion	O
and	O
then	O
expanding	O
the	O
middle	O
factor	O
as	O
a	O
power	O
series	O
show	O
that	O
the	O
gaussian	B
kernel	I
can	O
be	O
expressed	O
as	O
the	O
inner	O
product	O
of	O
an	O
infinite-dimensional	O
feature	O
vector	O
exercises	O
www	O
consider	O
the	O
space	O
of	O
all	O
possible	O
subsets	O
a	O
of	O
a	O
given	O
fixed	O
set	O
d	O
show	O
that	O
the	O
kernel	B
function	I
corresponds	O
to	O
an	O
inner	O
product	O
in	O
a	O
feature	B
space	I
of	O
dimensionality	O
defined	O
by	O
the	O
mapping	O
where	O
a	O
is	O
a	O
subset	O
of	O
d	O
and	O
the	O
element	O
u	O
indexed	O
by	O
the	O
subset	O
u	O
is	O
given	O
by	O
u	O
if	O
u	O
a	O
otherwise	O
here	O
u	O
a	O
denotes	O
that	O
u	O
is	O
either	O
a	O
subset	O
of	O
a	O
or	O
is	O
equal	O
to	O
a	O
show	O
that	O
the	O
fisher	B
kernel	I
defined	O
by	O
remains	O
invariant	O
if	O
we	O
make	O
a	O
nonlinear	O
transformation	O
of	O
the	O
parameter	O
vector	O
where	O
the	O
function	O
is	O
invertible	O
and	O
differentiable	O
www	O
write	O
down	O
the	O
form	O
of	O
the	O
fisher	B
kernel	I
defined	O
by	O
for	O
the	O
case	O
of	O
a	O
distribution	O
px	O
n	O
s	O
that	O
is	O
gaussian	B
with	O
mean	B
and	O
fixed	O
covariance	B
s	O
by	O
considering	O
the	O
determinant	O
of	O
a	O
gram	B
matrix	I
show	O
that	O
a	O
positive	B
definite	I
kernel	B
function	I
kx	O
x	O
satisfies	O
the	O
cauchy-schwartz	O
inequality	O
consider	O
a	O
parametric	O
model	O
governed	O
by	O
the	O
parameter	O
vector	O
w	O
together	O
with	O
a	O
data	O
set	O
of	O
input	O
values	O
xn	O
and	O
a	O
nonlinear	O
feature	O
mapping	O
suppose	O
that	O
the	O
dependence	O
of	O
the	O
error	B
function	I
on	O
w	O
takes	O
the	O
form	O
jw	O
fwt	O
wt	O
gwtw	O
where	O
g	O
is	O
a	O
monotonically	O
increasing	O
function	O
by	O
writing	O
w	O
in	O
the	O
form	O
w	O
n	O
w	O
show	O
that	O
the	O
value	O
of	O
w	O
that	O
minimizes	O
jw	O
takes	O
the	O
form	O
of	O
a	O
linear	O
combination	O
of	O
the	O
basis	O
functions	O
for	O
n	O
n	O
www	O
consider	O
the	O
sum-of-squares	B
error	B
function	I
for	O
data	O
having	O
noisy	O
inputs	O
where	O
is	O
the	O
distribution	O
of	O
the	O
noise	O
use	O
the	O
calculus	B
of	I
variations	I
to	O
minimize	O
this	O
error	B
function	I
with	O
respect	O
to	O
the	O
function	O
yx	O
and	O
hence	O
show	O
that	O
the	O
optimal	O
solution	O
is	O
given	O
by	O
an	O
expansion	O
of	O
the	O
form	O
in	O
which	O
the	O
basis	O
functions	O
are	O
given	O
by	O
kernel	O
methods	O
consider	O
a	O
nadaraya-watson	O
model	O
with	O
one	O
input	O
variable	O
x	O
and	O
one	O
target	O
variable	O
t	O
having	O
gaussian	B
components	O
with	O
isotropic	B
covariances	O
so	O
that	O
the	O
covariance	B
matrix	O
is	O
given	O
by	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
write	O
down	O
expressions	O
for	O
the	O
conditional	B
density	B
ptx	O
and	O
for	O
the	O
conditional	B
mean	B
etx	O
and	O
variance	B
vartx	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
kx	O
xn	O
another	O
viewpoint	O
on	O
kernel	B
regression	B
comes	O
from	O
a	O
consideration	O
of	O
regression	B
problems	O
in	O
which	O
the	O
input	O
variables	O
as	O
well	O
as	O
the	O
target	O
variables	O
are	O
corrupted	O
with	O
additive	O
noise	O
suppose	O
each	O
target	O
value	O
tn	O
is	O
generated	O
as	O
usual	O
by	O
taking	O
a	O
function	O
yzn	O
evaluated	O
at	O
a	O
point	O
zn	O
and	O
adding	O
gaussian	B
noise	O
the	O
value	O
of	O
zn	O
is	O
not	O
directly	O
observed	O
however	O
but	O
only	O
a	O
noise	O
corrupted	O
version	O
xn	O
zn	O
n	O
where	O
the	O
random	O
variable	O
is	O
governed	O
by	O
some	O
distribution	O
g	O
consider	O
a	O
set	O
of	O
observations	O
tn	O
where	O
n	O
n	O
together	O
with	O
a	O
corresponding	O
sum-of-squares	B
error	B
function	I
defined	O
by	O
averaging	O
over	O
the	O
distribution	O
of	O
input	O
noise	O
to	O
give	O
e	O
n	O
g	O
n	O
d	O
n	O
by	O
minimizing	O
e	O
with	O
respect	O
to	O
the	O
function	O
yz	O
using	O
the	O
calculus	B
of	I
variations	I
d	O
show	O
that	O
optimal	O
solution	O
for	O
yx	O
is	O
given	O
by	O
a	O
nadaraya-watson	O
kernel	B
regression	B
solution	O
of	O
the	O
form	O
with	O
a	O
kernel	O
of	O
the	O
form	O
www	O
verify	O
the	O
results	O
and	O
www	O
consider	O
a	O
gaussian	B
process	I
regression	B
model	O
in	O
which	O
the	O
kernel	B
function	I
is	O
defined	O
in	O
terms	O
of	O
a	O
fixed	O
set	O
of	O
nonlinear	O
basis	O
functions	O
show	O
that	O
the	O
predictive	B
distribution	I
is	O
identical	O
to	O
the	O
result	O
obtained	O
in	O
section	O
for	O
the	O
bayesian	B
linear	B
regression	B
model	O
to	O
do	O
this	O
note	O
that	O
both	O
models	O
have	O
gaussian	B
predictive	O
distributions	O
and	O
so	O
it	O
is	O
only	O
necessary	O
to	O
show	O
that	O
the	O
conditional	B
mean	B
and	O
variance	B
are	O
the	O
same	O
for	O
the	O
mean	B
make	O
use	O
of	O
the	O
matrix	O
identity	O
and	O
for	O
the	O
variance	B
make	O
use	O
of	O
the	O
matrix	O
identity	O
consider	O
a	O
regression	B
problem	O
with	O
n	O
training	B
set	I
input	O
vectors	O
xn	O
and	O
l	O
test	B
set	I
input	O
vectors	O
xn	O
xn	O
and	O
suppose	O
we	O
define	O
a	O
gaussian	B
process	I
prior	B
over	O
functions	O
tx	O
derive	O
an	O
expression	O
for	O
the	O
joint	O
predictive	B
distribution	I
for	O
txn	O
txn	O
given	O
the	O
values	O
of	O
txn	O
show	O
the	O
marginal	B
of	O
this	O
distribution	O
for	O
one	O
of	O
the	O
test	O
observations	O
tj	O
where	O
n	O
j	O
n	O
l	O
is	O
given	O
by	O
the	O
usual	O
gaussian	B
process	I
regression	B
result	O
and	O
www	O
consider	O
a	O
gaussian	B
process	I
regression	B
model	O
in	O
which	O
the	O
target	O
variable	O
t	O
has	O
dimensionality	O
d	O
write	O
down	O
the	O
conditional	B
distribution	O
of	O
tn	O
for	O
a	O
test	O
input	O
vector	O
xn	O
given	O
a	O
training	B
set	I
of	O
input	O
vectors	O
xn	O
and	O
corresponding	O
target	O
observations	O
tn	O
show	O
that	O
a	O
diagonal	B
matrix	O
w	O
whose	O
elements	O
satisfy	O
wii	O
is	O
positive	B
definite	I
show	O
that	O
the	O
sum	O
of	O
two	O
positive	B
definite	I
matrices	O
is	O
itself	O
positive	B
definite	I
exercises	O
www	O
using	O
the	O
newton-raphson	B
formula	O
derive	O
the	O
iterative	O
update	O
n	O
of	O
the	O
posterior	O
distribution	O
in	O
the	O
gaussian	B
formula	O
for	O
finding	O
the	O
mode	O
process	O
classification	B
model	O
using	O
the	O
result	O
derive	O
the	O
expressions	O
and	O
for	O
the	O
mean	B
and	O
variance	B
of	O
the	O
posterior	O
distribution	O
pan	O
in	O
the	O
gaussian	B
process	I
classification	B
model	O
derive	O
the	O
result	O
for	O
the	O
log	O
likelihood	B
function	I
in	O
the	O
laplace	B
approximation	I
framework	O
for	O
gaussian	B
process	I
classification	B
similarly	O
derive	O
the	O
results	O
and	O
for	O
the	O
terms	O
in	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
sparse	O
kernel	O
machines	O
in	O
the	O
previous	O
chapter	O
we	O
explored	O
a	O
variety	O
of	O
learning	B
algorithms	O
based	O
on	O
nonlinear	O
kernels	O
one	O
of	O
the	O
significant	O
limitations	O
of	O
many	O
such	O
algorithms	O
is	O
that	O
the	O
kernel	B
function	I
kxn	O
xm	O
must	O
be	O
evaluated	O
for	O
all	O
possible	O
pairs	O
xn	O
and	O
xm	O
of	O
training	B
points	O
which	O
can	O
be	O
computationally	O
infeasible	O
during	O
training	B
and	O
can	O
lead	O
to	O
excessive	O
computation	O
times	O
when	O
making	O
predictions	O
for	O
new	O
data	O
points	O
in	O
this	O
chapter	O
we	O
shall	O
look	O
at	O
kernel-based	O
algorithms	O
that	O
have	O
sparse	O
solutions	O
so	O
that	O
predictions	O
for	O
new	O
inputs	O
depend	O
only	O
on	O
the	O
kernel	B
function	I
evaluated	O
at	O
a	O
subset	O
of	O
the	O
training	B
data	O
points	O
we	O
begin	O
by	O
looking	O
in	O
some	O
detail	O
at	O
the	O
support	B
vector	I
machine	I
which	O
became	O
popular	O
in	O
some	O
years	O
ago	O
for	O
solving	O
problems	O
in	O
classification	B
regression	B
and	O
novelty	B
detection	I
an	O
important	O
property	O
of	O
support	B
vector	I
machines	O
is	O
that	O
the	O
determination	O
of	O
the	O
model	O
parameters	O
corresponds	O
to	O
a	O
convex	O
optimization	O
problem	O
and	O
so	O
any	O
local	B
solution	O
is	O
also	O
a	O
global	O
optimum	O
because	O
the	O
discussion	O
of	O
support	B
vector	I
machines	O
makes	O
extensive	O
use	O
of	O
lagrange	B
multipliers	O
the	O
reader	O
is	O
sparse	O
kernel	O
machines	O
encouraged	O
to	O
review	O
the	O
key	O
concepts	O
covered	O
in	O
appendix	O
e	O
additional	O
information	O
on	O
support	B
vector	I
machines	O
can	O
be	O
found	O
in	O
vapnik	O
burges	O
cristianini	O
and	O
shawe-taylor	O
m	O
uller	O
et	O
al	O
sch	O
olkopf	O
and	O
smola	O
and	O
herbrich	O
the	O
svm	O
is	O
a	O
decision	O
machine	O
and	O
so	O
does	O
not	O
provide	O
posterior	O
probabilities	O
we	O
have	O
already	O
discussed	O
some	O
of	O
the	O
benefits	O
of	O
determining	O
probabilities	O
in	O
section	O
an	O
alternative	O
sparse	O
kernel	O
technique	O
known	O
as	O
the	O
relevance	B
vector	I
machine	I
is	O
based	O
on	O
a	O
bayesian	B
formulation	O
and	O
provides	O
posterior	O
probabilistic	O
outputs	O
as	O
well	O
as	O
having	O
typically	O
much	O
sparser	O
solutions	O
than	O
the	O
svm	O
section	O
maximum	O
margin	B
classifiers	O
we	O
begin	O
our	O
discussion	O
of	O
support	B
vector	I
machines	O
by	O
returning	O
to	O
the	O
two-class	O
classification	B
problem	O
using	O
linear	O
models	O
of	O
the	O
form	O
yx	O
wt	O
b	O
where	O
denotes	O
a	O
fixed	O
feature-space	O
transformation	O
and	O
we	O
have	O
made	O
the	O
bias	B
parameter	I
b	O
explicit	O
note	O
that	O
we	O
shall	O
shortly	O
introduce	O
a	O
dual	B
representation	I
expressed	O
in	O
terms	O
of	O
kernel	O
functions	O
which	O
avoids	O
having	O
to	O
work	O
explicitly	O
in	O
feature	B
space	I
the	O
training	B
data	O
set	O
comprises	O
n	O
input	O
vectors	O
xn	O
with	O
corresponding	O
target	O
values	O
tn	O
where	O
tn	O
and	O
new	O
data	O
points	O
x	O
are	O
classified	O
according	O
to	O
the	O
sign	O
of	O
yx	O
we	O
shall	O
assume	O
for	O
the	O
moment	O
that	O
the	O
training	B
data	O
set	O
is	O
linearly	B
separable	I
in	O
feature	B
space	I
so	O
that	O
by	O
definition	O
there	O
exists	O
at	O
least	O
one	O
choice	O
of	O
the	O
parameters	O
w	O
and	O
b	O
such	O
that	O
a	O
function	O
of	O
the	O
form	O
satisfies	O
yxn	O
for	O
points	O
having	O
tn	O
and	O
yxn	O
for	O
points	O
having	O
tn	O
so	O
that	O
tnyxn	O
for	O
all	O
training	B
data	O
points	O
there	O
may	O
of	O
course	O
exist	O
many	O
such	O
solutions	O
that	O
separate	O
the	O
classes	O
exactly	O
in	O
section	O
we	O
described	O
the	O
perceptron	B
algorithm	O
that	O
is	O
guaranteed	O
to	O
find	O
a	O
solution	O
in	O
a	O
finite	O
number	O
of	O
steps	O
the	O
solution	O
that	O
it	O
finds	O
however	O
will	O
be	O
dependent	O
on	O
the	O
initial	O
values	O
chosen	O
for	O
w	O
and	O
b	O
as	O
well	O
as	O
on	O
the	O
order	O
in	O
which	O
the	O
data	O
points	O
are	O
presented	O
if	O
there	O
are	O
multiple	O
solutions	O
all	O
of	O
which	O
classify	O
the	O
training	B
data	O
set	O
exactly	O
then	O
we	O
should	O
try	O
to	O
find	O
the	O
one	O
that	O
will	O
give	O
the	O
smallest	O
generalization	B
error	B
the	O
support	B
vector	I
machine	I
approaches	O
this	O
problem	O
through	O
the	O
concept	O
of	O
the	O
margin	B
which	O
is	O
defined	O
to	O
be	O
the	O
smallest	O
distance	O
between	O
the	O
decision	B
boundary	I
and	O
any	O
of	O
the	O
samples	O
as	O
illustrated	O
in	O
figure	O
in	O
support	B
vector	I
machines	O
the	O
decision	B
boundary	I
is	O
chosen	O
to	O
be	O
the	O
one	O
for	O
which	O
the	O
margin	B
is	O
maximized	O
the	O
maximum	O
margin	B
solution	O
can	O
be	O
motivated	O
using	O
computational	B
learning	B
theory	B
also	O
known	O
as	O
statistical	O
learning	B
theory	B
however	O
a	O
simple	O
insight	O
into	O
the	O
origins	O
of	O
maximum	O
margin	B
has	O
been	O
given	O
by	O
tong	O
and	O
koller	O
who	O
consider	O
a	O
framework	O
for	O
classification	B
based	O
on	O
a	O
hybrid	O
of	O
generative	O
and	O
discriminative	O
approaches	O
they	O
first	O
model	O
the	O
distribution	O
over	O
input	O
vectors	O
x	O
for	O
each	O
class	O
using	O
a	O
parzen	O
density	B
estimator	O
with	O
gaussian	B
kernels	O
section	O
y	O
y	O
y	O
maximum	O
margin	B
classifiers	O
y	O
y	O
y	O
margin	B
figure	O
the	O
margin	B
is	O
defined	O
as	O
the	O
perpendicular	O
distance	O
between	O
the	O
decision	B
boundary	I
and	O
the	O
closest	O
of	O
the	O
data	O
points	O
as	O
shown	O
on	O
the	O
left	O
figure	O
maximizing	O
the	O
margin	B
leads	O
to	O
a	O
particular	O
choice	O
of	O
decision	B
boundary	I
as	O
shown	O
on	O
the	O
right	O
the	O
location	O
of	O
this	O
boundary	O
is	O
determined	O
by	O
a	O
subset	O
of	O
the	O
data	O
points	O
known	O
as	O
support	O
vectors	O
which	O
are	O
indicated	O
by	O
the	O
circles	O
having	O
a	O
common	O
parameter	O
together	O
with	O
the	O
class	O
priors	O
this	O
defines	O
an	O
optimal	O
misclassification-rate	O
decision	B
boundary	I
however	O
instead	O
of	O
using	O
this	O
optimal	O
boundary	O
they	O
determine	O
the	O
best	O
hyperplane	O
by	O
minimizing	O
the	O
probability	B
of	O
error	B
relative	B
to	O
the	O
learned	O
density	B
model	O
in	O
the	O
limit	O
the	O
optimal	O
hyperplane	O
is	O
shown	O
to	O
be	O
the	O
one	O
having	O
maximum	O
margin	B
the	O
intuition	O
behind	O
this	O
result	O
is	O
that	O
as	O
is	O
reduced	O
the	O
hyperplane	O
is	O
increasingly	O
dominated	O
by	O
nearby	O
data	O
points	O
relative	B
to	O
more	O
distant	O
ones	O
in	O
the	O
limit	O
the	O
hyperplane	O
becomes	O
independent	B
of	O
data	O
points	O
that	O
are	O
not	O
support	O
vectors	O
we	O
shall	O
see	O
in	O
figure	O
that	O
marginalization	O
with	O
respect	O
to	O
the	O
prior	B
distribution	O
of	O
the	O
parameters	O
in	O
a	O
bayesian	B
approach	O
for	O
a	O
simple	O
linearly	B
separable	I
data	O
set	O
leads	O
to	O
a	O
decision	B
boundary	I
that	O
lies	O
in	O
the	O
middle	O
of	O
the	O
region	O
separating	O
the	O
data	O
points	O
the	O
large	O
margin	B
solution	O
has	O
similar	O
behaviour	O
recall	O
from	O
figure	O
that	O
the	O
perpendicular	O
distance	O
of	O
a	O
point	O
x	O
from	O
a	O
hyperplane	O
defined	O
by	O
yx	O
where	O
yx	O
takes	O
the	O
form	O
is	O
given	O
by	O
furthermore	O
we	O
are	O
only	O
interested	O
in	O
solutions	O
for	O
which	O
all	O
data	O
points	O
are	O
correctly	O
classified	O
so	O
that	O
tnyxn	O
for	O
all	O
n	O
thus	O
the	O
distance	O
of	O
a	O
point	O
xn	O
to	O
the	O
decision	O
surface	O
is	O
given	O
by	O
tnyxn	O
tnwt	O
b	O
the	O
margin	B
is	O
given	O
by	O
the	O
perpendicular	O
distance	O
to	O
the	O
closest	O
point	O
xn	O
from	O
the	O
data	O
set	O
and	O
we	O
wish	O
to	O
optimize	O
the	O
parameters	O
w	O
and	O
b	O
in	O
order	O
to	O
maximize	O
this	O
distance	O
thus	O
the	O
maximum	O
margin	B
solution	O
is	O
found	O
by	O
solving	O
arg	O
max	O
where	O
we	O
have	O
taken	O
the	O
factor	O
outside	O
the	O
optimization	O
over	O
n	O
because	O
w	O
wt	O
b	O
tn	O
wb	O
min	O
n	O
sparse	O
kernel	O
machines	O
does	O
not	O
depend	O
on	O
n	O
direct	O
solution	O
of	O
this	O
optimization	O
problem	O
would	O
be	O
very	O
complex	O
and	O
so	O
we	O
shall	O
convert	O
it	O
into	O
an	O
equivalent	O
problem	O
that	O
is	O
much	O
easier	O
to	O
solve	O
to	O
do	O
this	O
we	O
note	O
that	O
if	O
we	O
make	O
the	O
rescaling	O
w	O
w	O
and	O
b	O
b	O
then	O
the	O
distance	O
from	O
any	O
point	O
xn	O
to	O
the	O
decision	O
surface	O
given	O
by	O
is	O
unchanged	O
we	O
can	O
use	O
this	O
freedom	O
to	O
set	O
tn	O
wt	O
b	O
tn	O
for	O
the	O
point	O
that	O
is	O
closest	O
to	O
the	O
surface	O
in	O
this	O
case	O
all	O
data	O
points	O
will	O
satisfy	O
the	O
constraints	O
wt	O
b	O
n	O
n	O
this	O
is	O
known	O
as	O
the	O
canonical	O
representation	O
of	O
the	O
decision	O
hyperplane	O
in	O
the	O
case	O
of	O
data	O
points	O
for	O
which	O
the	O
equality	O
holds	O
the	O
constraints	O
are	O
said	O
to	O
be	O
active	O
whereas	O
for	O
the	O
remainder	O
they	O
are	O
said	O
to	O
be	O
inactive	O
by	O
definition	O
there	O
will	O
always	O
be	O
at	O
least	O
one	O
active	B
constraint	I
because	O
there	O
will	O
always	O
be	O
a	O
closest	O
point	O
and	O
once	O
the	O
margin	B
has	O
been	O
maximized	O
there	O
will	O
be	O
at	O
least	O
two	O
active	O
constraints	O
the	O
optimization	O
problem	O
then	O
simply	O
requires	O
that	O
we	O
maximize	O
which	O
is	O
equivalent	O
to	O
minimizing	O
and	O
so	O
we	O
have	O
to	O
solve	O
the	O
optimization	O
problem	O
arg	O
min	O
wb	O
subject	O
to	O
the	O
constraints	O
given	O
by	O
the	O
factor	O
of	O
in	O
is	O
included	O
for	O
later	O
convenience	O
this	O
is	O
an	O
example	O
of	O
a	O
quadratic	O
programming	O
problem	O
in	O
which	O
we	O
are	O
trying	O
to	O
minimize	O
a	O
quadratic	O
function	O
subject	O
to	O
a	O
set	O
of	O
linear	O
inequality	O
constraints	O
it	O
appears	O
that	O
the	O
bias	B
parameter	I
b	O
has	O
disappeared	O
from	O
the	O
optimization	O
however	O
it	O
is	O
determined	O
implicitly	O
via	O
the	O
constraints	O
because	O
these	O
require	O
that	O
changes	O
to	O
be	O
compensated	O
by	O
changes	O
to	O
b	O
we	O
shall	O
see	O
how	O
this	O
works	O
shortly	O
in	O
order	O
to	O
solve	O
this	O
constrained	O
optimization	O
problem	O
we	O
introduce	O
lagrange	B
multipliers	O
an	O
with	O
one	O
multiplier	O
an	O
for	O
each	O
of	O
the	O
constraints	O
in	O
giving	O
the	O
lagrangian	B
function	O
lw	O
b	O
a	O
tnwt	O
b	O
an	O
appendix	O
e	O
where	O
a	O
an	O
note	O
the	O
minus	O
sign	O
in	O
front	O
of	O
the	O
lagrange	B
multiplier	I
term	O
because	O
we	O
are	O
minimizing	O
with	O
respect	O
to	O
w	O
and	O
b	O
and	O
maximizing	O
with	O
respect	O
to	O
a	O
setting	O
the	O
derivatives	O
of	O
lw	O
b	O
a	O
with	O
respect	O
to	O
w	O
and	O
b	O
equal	O
to	O
zero	O
we	O
obtain	O
the	O
following	O
two	O
conditions	O
antn	O
antn	O
w	O
maximum	O
margin	B
classifiers	O
eliminating	O
w	O
and	O
b	O
from	O
lw	O
b	O
a	O
using	O
these	O
conditions	O
then	O
gives	O
the	O
dual	B
representation	I
of	O
the	O
maximum	O
margin	B
problem	O
in	O
which	O
we	O
maximize	O
an	O
with	O
respect	O
to	O
a	O
subject	O
to	O
the	O
constraints	O
anamtntmkxn	O
xm	O
an	O
n	O
n	O
antn	O
here	O
the	O
kernel	B
function	I
is	O
defined	O
by	O
kx	O
again	O
this	O
takes	O
the	O
form	O
of	O
a	O
quadratic	O
programming	O
problem	O
in	O
which	O
we	O
optimize	O
a	O
quadratic	O
function	O
of	O
a	O
subject	O
to	O
a	O
set	O
of	O
inequality	O
constraints	O
we	O
shall	O
discuss	O
techniques	O
for	O
solving	O
such	O
quadratic	O
programming	O
problems	O
in	O
section	O
the	O
solution	O
to	O
a	O
quadratic	O
programming	O
problem	O
in	O
m	O
variables	O
in	O
general	O
has	O
computational	O
complexity	O
that	O
is	O
om	O
in	O
going	O
to	O
the	O
dual	O
formulation	O
we	O
have	O
turned	O
the	O
original	O
optimization	O
problem	O
which	O
involved	O
minimizing	O
over	O
m	O
variables	O
into	O
the	O
dual	O
problem	O
which	O
has	O
n	O
variables	O
for	O
a	O
fixed	O
set	O
of	O
basis	O
functions	O
whose	O
number	O
m	O
is	O
smaller	O
than	O
the	O
number	O
n	O
of	O
data	O
points	O
the	O
move	O
to	O
the	O
dual	O
problem	O
appears	O
disadvantageous	O
however	O
it	O
allows	O
the	O
model	O
to	O
be	O
reformulated	O
using	O
kernels	O
and	O
so	O
the	O
maximum	O
margin	B
classifier	O
can	O
be	O
applied	O
efficiently	O
to	O
feature	O
spaces	O
whose	O
dimensionality	O
exceeds	O
the	O
number	O
of	O
data	O
points	O
including	O
infinite	O
feature	O
spaces	O
the	O
kernel	O
formulation	O
also	O
makes	O
clear	O
the	O
role	O
of	O
the	O
constraint	O
that	O
the	O
kernel	B
function	I
kx	O
be	O
positive	B
definite	I
because	O
this	O
ensures	O
that	O
the	O
lagrangian	B
is	O
bounded	O
below	O
giving	O
rise	O
to	O
a	O
well	O
defined	O
optimization	O
problem	O
in	O
order	O
to	O
classify	O
new	O
data	O
points	O
using	O
the	O
trained	O
model	O
we	O
evaluate	O
the	O
sign	O
of	O
yx	O
defined	O
by	O
this	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
parameters	O
and	O
the	O
kernel	B
function	I
by	O
substituting	O
for	O
w	O
using	O
to	O
give	O
yx	O
antnkx	O
xn	O
b	O
joseph-louis	O
lagrange	B
although	O
widely	O
considered	O
to	O
be	O
a	O
french	O
mathematician	O
lagrange	B
was	O
born	O
in	O
turin	O
in	O
italy	O
by	O
the	O
age	O
of	O
nineteen	O
he	O
had	O
already	O
made	O
important	O
contributions	O
mathematics	O
and	O
had	O
been	O
appointed	O
as	O
professor	O
at	O
the	O
royal	O
artillery	O
school	O
in	O
turin	O
for	O
many	O
years	O
euler	B
worked	O
hard	O
to	O
persuade	O
lagrange	B
to	O
move	O
to	O
berlin	O
which	O
he	O
eventually	O
did	O
in	O
where	O
he	O
succeeded	O
euler	B
as	O
director	O
of	O
mathematics	O
at	O
the	O
berlin	O
academy	O
later	O
he	O
moved	O
to	O
paris	O
narrowly	O
escaping	O
with	O
his	O
life	O
during	O
the	O
french	O
revolution	O
thanks	O
to	O
the	O
personal	O
intervention	O
of	O
lavoisier	O
french	O
chemist	O
who	O
discovered	O
oxygen	O
who	O
himself	O
was	O
later	O
executed	O
at	O
the	O
guillotine	O
lagrange	B
made	O
key	O
contributions	O
to	O
the	O
calculus	B
of	I
variations	I
and	O
the	O
foundations	O
of	O
dynamics	O
sparse	O
kernel	O
machines	O
in	O
appendix	O
e	O
we	O
show	O
that	O
a	O
constrained	O
optimization	O
of	O
this	O
form	O
satisfies	O
the	O
karush-kuhn-tucker	B
conditions	I
which	O
in	O
this	O
case	O
require	O
that	O
the	O
following	O
three	O
properties	O
hold	O
an	O
tnyxn	O
an	O
thus	O
for	O
every	O
data	O
point	O
either	O
an	O
or	O
tnyxn	O
any	O
data	O
point	O
for	O
which	O
an	O
will	O
not	O
appear	O
in	O
the	O
sum	O
in	O
and	O
hence	O
plays	O
no	O
role	O
in	O
making	O
predictions	O
for	O
new	O
data	O
points	O
the	O
remaining	O
data	O
points	O
are	O
called	O
support	O
vectors	O
and	O
because	O
they	O
satisfy	O
tnyxn	O
they	O
correspond	O
to	O
points	O
that	O
lie	O
on	O
the	O
maximum	O
margin	B
hyperplanes	O
in	O
feature	B
space	I
as	O
illustrated	O
in	O
figure	O
this	O
property	O
is	O
central	O
to	O
the	O
practical	O
applicability	O
of	O
support	B
vector	I
machines	O
once	O
the	O
model	O
is	O
trained	O
a	O
significant	O
proportion	O
of	O
the	O
data	O
points	O
can	O
be	O
discarded	O
and	O
only	O
the	O
support	O
vectors	O
retained	O
having	O
solved	O
the	O
quadratic	O
programming	O
problem	O
and	O
found	O
a	O
value	O
for	O
a	O
we	O
can	O
then	O
determine	O
the	O
value	O
of	O
the	O
threshold	B
parameter	I
b	O
by	O
noting	O
that	O
any	O
support	B
vector	I
xn	O
satisfies	O
tnyxn	O
using	O
this	O
gives	O
amtmkxn	O
xm	O
b	O
tn	O
m	O
s	O
where	O
s	O
denotes	O
the	O
set	O
of	O
indices	O
of	O
the	O
support	O
vectors	O
although	O
we	O
can	O
solve	O
this	O
equation	O
for	O
b	O
using	O
an	O
arbitrarily	O
chosen	O
support	B
vector	I
xn	O
a	O
numerically	O
more	O
n	O
stable	O
solution	O
is	O
obtained	O
by	O
first	O
multiplying	O
through	O
by	O
tn	O
making	O
use	O
of	O
and	O
then	O
averaging	O
these	O
equations	O
over	O
all	O
support	O
vectors	O
and	O
solving	O
for	O
b	O
to	O
give	O
n	O
s	O
m	O
s	O
b	O
ns	O
tn	O
amtmkxn	O
xm	O
where	O
ns	O
is	O
the	O
total	O
number	O
of	O
support	O
vectors	O
for	O
later	O
comparison	O
with	O
alternative	O
models	O
we	O
can	O
express	O
the	O
maximummargin	O
classifier	O
in	O
terms	O
of	O
the	O
minimization	O
of	O
an	O
error	B
function	I
with	O
a	O
simple	O
quadratic	O
regularizer	O
in	O
the	O
form	O
e	O
where	O
e	O
is	O
a	O
function	O
that	O
is	O
zero	O
if	O
z	O
and	O
otherwise	O
and	O
ensures	O
that	O
the	O
constraints	O
are	O
satisfied	O
note	O
that	O
as	O
long	O
as	O
the	O
regularization	B
parameter	O
satisfies	O
its	O
precise	O
value	O
plays	O
no	O
role	O
figure	O
shows	O
an	O
example	O
of	O
the	O
classification	B
resulting	O
from	O
training	B
a	O
support	B
vector	I
machine	I
on	O
a	O
simple	O
synthetic	O
data	O
set	O
using	O
a	O
gaussian	B
kernel	I
of	O
the	O
maximum	O
margin	B
classifiers	O
figure	O
example	O
of	O
synthetic	O
data	O
from	O
two	O
classes	O
in	O
two	O
dimensions	O
showing	O
contours	O
of	O
constant	O
yx	O
obtained	O
from	O
a	O
support	B
vector	I
machine	I
having	O
a	O
gaussian	B
kernel	B
function	I
also	O
shown	O
are	O
the	O
decision	B
boundary	I
the	O
margin	B
boundaries	O
and	O
the	O
support	O
vectors	O
form	O
although	O
the	O
data	O
set	O
is	O
not	O
linearly	B
separable	I
in	O
the	O
two-dimensional	O
data	O
space	O
x	O
it	O
is	O
linearly	B
separable	I
in	O
the	O
nonlinear	O
feature	B
space	I
defined	O
implicitly	O
by	O
the	O
nonlinear	O
kernel	B
function	I
thus	O
the	O
training	B
data	O
points	O
are	O
perfectly	O
separated	O
in	O
the	O
original	O
data	O
space	O
this	O
example	O
also	O
provides	O
a	O
geometrical	O
insight	O
into	O
the	O
origin	O
of	O
sparsity	B
in	O
the	O
svm	O
the	O
maximum	O
margin	B
hyperplane	O
is	O
defined	O
by	O
the	O
location	O
of	O
the	O
support	O
vectors	O
other	O
data	O
points	O
can	O
be	O
moved	O
around	O
freely	O
long	O
as	O
they	O
remain	O
outside	O
the	O
margin	B
region	O
without	O
changing	O
the	O
decision	B
boundary	I
and	O
so	O
the	O
solution	O
will	O
be	O
independent	B
of	O
such	O
data	O
points	O
overlapping	O
class	O
distributions	O
so	O
far	O
we	O
have	O
assumed	O
that	O
the	O
training	B
data	O
points	O
are	O
linearly	B
separable	I
in	O
the	O
feature	B
space	I
the	O
resulting	O
support	B
vector	I
machine	I
will	O
give	O
exact	O
separation	O
of	O
the	O
training	B
data	O
in	O
the	O
original	O
input	O
space	O
x	O
although	O
the	O
corresponding	O
decision	B
boundary	I
will	O
be	O
nonlinear	O
in	O
practice	O
however	O
the	O
class-conditional	O
distributions	O
may	O
overlap	O
in	O
which	O
case	O
exact	O
separation	O
of	O
the	O
training	B
data	O
can	O
lead	O
to	O
poor	O
generalization	B
we	O
therefore	O
need	O
a	O
way	O
to	O
modify	O
the	O
support	B
vector	I
machine	I
so	O
as	O
to	O
allow	O
some	O
of	O
the	O
training	B
points	O
to	O
be	O
misclassified	O
from	O
we	O
see	O
that	O
in	O
the	O
case	O
of	O
separable	O
classes	O
we	O
implicitly	O
used	O
an	O
error	B
function	I
that	O
gave	O
infinite	O
error	B
if	O
a	O
data	O
point	O
was	O
misclassified	O
and	O
zero	O
error	B
if	O
it	O
was	O
classified	O
correctly	O
and	O
then	O
optimized	O
the	O
model	O
parameters	O
to	O
maximize	O
the	O
margin	B
we	O
now	O
modify	O
this	O
approach	O
so	O
that	O
data	O
points	O
are	O
allowed	O
to	O
be	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
boundary	O
but	O
with	O
a	O
penalty	O
that	O
increases	O
with	O
the	O
distance	O
from	O
that	O
boundary	O
for	O
the	O
subsequent	O
optimization	O
problem	O
it	O
is	O
convenient	O
to	O
make	O
this	O
penalty	O
a	O
linear	O
function	O
of	O
this	O
distance	O
to	O
do	O
this	O
we	O
introduce	O
slack	O
variables	O
n	O
where	O
n	O
n	O
with	O
one	O
slack	B
variable	I
for	O
each	O
training	B
data	O
point	O
cortes	O
and	O
vapnik	O
these	O
are	O
defined	O
by	O
n	O
for	O
data	O
points	O
that	O
are	O
on	O
or	O
inside	O
the	O
correct	O
margin	B
boundary	O
and	O
n	O
yxn	O
for	O
other	O
points	O
thus	O
a	O
data	O
point	O
that	O
is	O
on	O
the	O
decision	B
boundary	I
yxn	O
will	O
have	O
n	O
and	O
points	O
sparse	O
kernel	O
machines	O
figure	O
illustration	O
of	O
the	O
slack	O
variables	O
n	O
data	O
points	O
with	O
circles	O
around	O
them	O
are	O
support	O
vectors	O
y	O
y	O
y	O
tnyxn	O
n	O
with	O
n	O
will	O
be	O
misclassified	O
the	O
exact	O
classification	B
constraints	O
are	O
then	O
replaced	O
with	O
n	O
n	O
in	O
which	O
the	O
slack	O
variables	O
are	O
constrained	O
to	O
satisfy	O
n	O
data	O
points	O
for	O
which	O
n	O
are	O
correctly	O
classified	O
and	O
are	O
either	O
on	O
the	O
margin	B
or	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
points	O
for	O
which	O
n	O
lie	O
inside	O
the	O
margin	B
but	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
and	O
those	O
data	O
points	O
for	O
which	O
n	O
lie	O
on	O
the	O
wrong	O
side	O
of	O
the	O
decision	B
boundary	I
and	O
are	O
misclassified	O
as	O
illustrated	O
in	O
figure	O
this	O
is	O
sometimes	O
described	O
as	O
relaxing	O
the	O
hard	O
margin	B
constraint	O
to	O
give	O
a	O
soft	B
margin	B
and	O
allows	O
some	O
of	O
the	O
training	B
set	I
data	O
points	O
to	O
be	O
misclassified	O
note	O
that	O
while	O
slack	O
variables	O
allow	O
for	O
overlapping	O
class	O
distributions	O
this	O
framework	O
is	O
still	O
sensitive	O
to	O
outliers	B
because	O
the	O
penalty	O
for	O
misclassification	O
increases	O
linearly	O
with	O
our	O
goal	O
is	O
now	O
to	O
maximize	O
the	O
margin	B
while	O
softly	O
penalizing	O
points	O
that	O
lie	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
boundary	O
we	O
therefore	O
minimize	O
c	O
n	O
where	O
the	O
parameter	O
c	O
controls	O
the	O
trade-off	O
between	O
the	O
slack	B
variable	I
penalty	O
and	O
the	O
margin	B
because	O
any	O
point	O
that	O
is	O
misclassified	O
has	O
n	O
it	O
follows	O
that	O
n	O
n	O
is	O
an	O
upper	O
bound	O
on	O
the	O
number	O
of	O
misclassified	O
points	O
the	O
parameter	O
c	O
is	O
therefore	O
analogous	O
to	O
inverse	B
of	O
a	O
regularization	B
coefficient	O
because	O
it	O
controls	O
the	O
trade-off	O
between	O
minimizing	O
training	B
errors	O
and	O
controlling	O
model	O
complexity	O
in	O
the	O
limit	O
c	O
we	O
will	O
recover	O
the	O
earlier	O
support	B
vector	I
machine	I
for	O
separable	O
data	O
n	O
the	O
corresponding	O
lagrangian	B
is	O
given	O
by	O
we	O
now	O
wish	O
to	O
minimize	O
subject	O
to	O
the	O
constraints	O
together	O
with	O
n	O
an	O
n	O
n	O
n	O
lw	O
b	O
a	O
c	O
appendix	O
e	O
where	O
and	O
n	O
are	O
lagrange	B
multipliers	O
the	O
corresponding	O
set	O
of	O
kkt	O
conditions	O
are	O
given	O
by	O
maximum	O
margin	B
classifiers	O
an	O
tnyxn	O
n	O
an	O
n	O
n	O
n	O
n	O
n	O
where	O
n	O
n	O
we	O
now	O
optimize	O
out	O
w	O
b	O
and	O
n	O
making	O
use	O
of	O
the	O
definition	O
of	O
yx	O
an	O
c	O
antn	O
for	O
n	O
n	O
where	O
are	O
known	O
as	O
box	B
constraints	I
this	O
again	O
represents	O
a	O
quadratic	O
programming	O
problem	O
if	O
we	O
substitute	O
into	O
we	O
see	O
that	O
predictions	O
for	O
new	O
data	O
points	O
are	O
again	O
made	O
by	O
using	O
we	O
can	O
now	O
interpret	O
the	O
resulting	O
solution	O
as	O
before	O
a	O
subset	O
of	O
the	O
data	O
points	O
may	O
have	O
an	O
in	O
which	O
case	O
they	O
do	O
not	O
contribute	O
to	O
the	O
predictive	O
to	O
give	O
l	O
w	O
l	O
b	O
antn	O
w	O
antn	O
an	O
c	O
n	O
using	O
these	O
results	O
to	O
eliminate	O
w	O
b	O
and	O
n	O
from	O
the	O
lagrangian	B
we	O
obtain	O
the	O
dual	O
lagrangian	B
in	O
the	O
form	O
l	O
n	O
an	O
anamtntmkxn	O
xm	O
which	O
is	O
identical	O
to	O
the	O
separable	O
case	O
except	O
that	O
the	O
constraints	O
are	O
somewhat	O
different	O
to	O
see	O
what	O
these	O
constraints	O
are	O
we	O
note	O
that	O
an	O
is	O
required	O
because	O
these	O
are	O
lagrange	B
multipliers	O
furthermore	O
together	O
with	O
n	O
implies	O
an	O
c	O
we	O
therefore	O
have	O
to	O
minimize	O
with	O
respect	O
to	O
the	O
dual	O
variables	O
subject	O
to	O
sparse	O
kernel	O
machines	O
model	O
the	O
remaining	O
data	O
points	O
constitute	O
the	O
support	O
vectors	O
these	O
have	O
an	O
and	O
hence	O
from	O
must	O
satisfy	O
tnyxn	O
n	O
if	O
an	O
c	O
then	O
implies	O
that	O
n	O
which	O
from	O
requires	O
n	O
and	O
hence	O
such	O
points	O
lie	O
on	O
the	O
margin	B
points	O
with	O
an	O
c	O
can	O
lie	O
inside	O
the	O
margin	B
and	O
can	O
either	O
be	O
correctly	O
classified	O
if	O
n	O
or	O
misclassified	O
if	O
n	O
to	O
determine	O
the	O
parameter	O
b	O
in	O
we	O
note	O
that	O
those	O
support	O
vectors	O
for	O
which	O
an	O
c	O
have	O
n	O
so	O
that	O
tnyxn	O
and	O
hence	O
will	O
satisfy	O
tn	O
amtmkxn	O
xm	O
b	O
again	O
a	O
numerically	O
stable	O
solution	O
is	O
obtained	O
by	O
averaging	O
to	O
give	O
m	O
s	O
m	O
s	O
b	O
nm	O
n	O
m	O
tn	O
amtmkxn	O
xm	O
where	O
m	O
denotes	O
the	O
set	O
of	O
indices	O
of	O
data	O
points	O
having	O
an	O
c	O
an	O
alternative	O
equivalent	O
formulation	O
of	O
the	O
support	B
vector	I
machine	I
known	O
as	O
the	O
has	O
been	O
proposed	O
by	O
sch	O
olkopf	O
et	O
al	O
this	O
involves	O
maximizing	O
anamtntmkxn	O
xm	O
subject	O
to	O
the	O
constraints	O
an	O
antn	O
an	O
this	O
approach	O
has	O
the	O
advantage	O
that	O
the	O
parameter	O
which	O
replaces	O
c	O
can	O
be	O
interpreted	O
as	O
both	O
an	O
upper	O
bound	O
on	O
the	O
fraction	O
of	O
margin	B
errors	O
for	O
which	O
n	O
and	O
hence	O
which	O
lie	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
boundary	O
and	O
which	O
may	O
or	O
may	O
not	O
be	O
misclassified	O
and	O
a	O
lower	B
bound	I
on	O
the	O
fraction	O
of	O
support	O
vectors	O
an	O
example	O
of	O
the	O
applied	O
to	O
a	O
synthetic	O
data	O
set	O
is	O
shown	O
in	O
figure	O
here	O
gaussian	B
kernels	O
of	O
the	O
form	O
exp	O
have	O
been	O
used	O
with	O
although	O
predictions	O
for	O
new	O
inputs	O
are	O
made	O
using	O
only	O
the	O
support	O
vectors	O
the	O
training	B
phase	O
the	O
determination	O
of	O
the	O
parameters	O
a	O
and	O
b	O
makes	O
use	O
of	O
the	O
whole	O
data	O
set	O
and	O
so	O
it	O
is	O
important	O
to	O
have	O
efficient	O
algorithms	O
for	O
solving	O
maximum	O
margin	B
classifiers	O
figure	O
illustration	O
of	O
the	O
applied	O
to	O
a	O
nonseparable	O
data	O
set	O
in	O
two	O
dimensions	O
the	O
support	O
vectors	O
are	O
indicated	O
by	O
circles	O
the	O
quadratic	O
programming	O
problem	O
we	O
first	O
note	O
that	O
the	O
objective	O
given	O
by	O
or	O
is	O
quadratic	O
and	O
so	O
any	O
local	B
optimum	O
will	O
also	O
be	O
a	O
global	O
optimum	O
provided	O
the	O
constraints	O
define	O
a	O
convex	O
region	O
they	O
do	O
as	O
a	O
consequence	O
of	O
being	O
linear	O
direct	O
solution	O
of	O
the	O
quadratic	O
programming	O
problem	O
using	O
traditional	O
techniques	O
is	O
often	O
infeasible	O
due	O
to	O
the	O
demanding	O
computation	O
and	O
memory	O
requirements	O
and	O
so	O
more	O
practical	O
approaches	O
need	O
to	O
be	O
found	O
the	O
technique	O
of	O
chunking	B
exploits	O
the	O
fact	O
that	O
the	O
value	O
of	O
the	O
lagrangian	B
is	O
unchanged	O
if	O
we	O
remove	O
the	O
rows	O
and	O
columns	O
of	O
the	O
kernel	O
matrix	O
corresponding	O
to	O
lagrange	B
multipliers	O
that	O
have	O
value	O
zero	O
this	O
allows	O
the	O
full	O
quadratic	O
programming	O
problem	O
to	O
be	O
broken	O
down	O
into	O
a	O
series	O
of	O
smaller	O
ones	O
whose	O
goal	O
is	O
eventually	O
to	O
identify	O
all	O
of	O
the	O
nonzero	O
lagrange	B
multipliers	O
and	O
discard	O
the	O
others	O
chunking	B
can	O
be	O
implemented	O
using	O
protected	B
conjugate	B
gradients	I
although	O
chunking	B
reduces	O
the	O
size	O
of	O
the	O
matrix	O
in	O
the	O
quadratic	O
function	O
from	O
the	O
number	O
of	O
data	O
points	O
squared	O
to	O
approximately	O
the	O
number	O
of	O
nonzero	O
lagrange	B
multipliers	O
squared	O
even	O
this	O
may	O
be	O
too	O
big	O
to	O
fit	O
in	O
memory	O
for	O
large-scale	O
applications	O
decomposition	B
methods	I
et	O
al	O
also	O
solve	O
a	O
series	O
of	O
smaller	O
quadratic	O
programming	O
problems	O
but	O
are	O
designed	O
so	O
that	O
each	O
of	O
these	O
is	O
of	O
a	O
fixed	O
size	O
and	O
so	O
the	O
technique	O
can	O
be	O
applied	O
to	O
arbitrarily	O
large	O
data	O
sets	O
however	O
it	O
still	O
involves	O
numerical	O
solution	O
of	O
quadratic	O
programming	O
subproblems	O
and	O
these	O
can	O
be	O
problematic	O
and	O
expensive	O
one	O
of	O
the	O
most	O
popular	O
approaches	O
to	O
training	B
support	B
vector	I
machines	O
is	O
called	O
sequential	B
minimal	I
optimization	I
or	O
smo	O
it	O
takes	O
the	O
concept	O
of	O
chunking	B
to	O
the	O
extreme	O
limit	O
and	O
considers	O
just	O
two	O
lagrange	B
multipliers	O
at	O
a	O
time	O
in	O
this	O
case	O
the	O
subproblem	O
can	O
be	O
solved	O
analytically	O
thereby	O
avoiding	O
numerical	O
quadratic	O
programming	O
altogether	O
heuristics	O
are	O
given	O
for	O
choosing	O
the	O
pair	O
of	O
lagrange	B
multipliers	O
to	O
be	O
considered	O
at	O
each	O
step	O
in	O
practice	O
smo	O
is	O
found	O
to	O
have	O
a	O
scaling	O
with	O
the	O
number	O
of	O
data	O
points	O
that	O
is	O
somewhere	O
between	O
linear	O
and	O
quadratic	O
depending	O
on	O
the	O
particular	O
application	O
we	O
have	O
seen	O
that	O
kernel	O
functions	O
correspond	O
to	O
inner	O
products	O
in	O
feature	O
spaces	O
that	O
can	O
have	O
high	O
or	O
even	O
infinite	O
dimensionality	O
by	O
working	O
directly	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
without	O
introducing	O
the	O
feature	B
space	I
explicitly	O
it	O
might	O
therefore	O
seem	O
that	O
support	B
vector	I
machines	O
somehow	O
manage	O
to	O
avoid	O
the	O
curse	O
of	O
di	O
sparse	O
kernel	O
machines	O
section	O
section	O
mensionality	O
this	O
is	O
not	O
the	O
case	O
however	O
because	O
there	O
are	O
constraints	O
amongst	O
the	O
feature	O
values	O
that	O
restrict	O
the	O
effective	O
dimensionality	O
of	O
feature	B
space	I
to	O
see	O
this	O
consider	O
a	O
simple	O
second-order	O
polynomial	O
kernel	O
that	O
we	O
can	O
expand	O
in	O
terms	O
of	O
its	O
components	O
kx	O
z	O
xtz	O
this	O
kernel	B
function	I
therefore	O
represents	O
an	O
inner	O
product	O
in	O
a	O
feature	B
space	I
having	O
six	O
dimensions	O
in	O
which	O
the	O
mapping	O
from	O
input	O
space	O
to	O
feature	B
space	I
is	O
described	O
by	O
the	O
vector	O
function	O
however	O
the	O
coefficients	O
weighting	O
these	O
different	O
features	O
are	O
constrained	O
to	O
have	O
specific	O
forms	O
thus	O
any	O
set	O
of	O
points	O
in	O
the	O
original	O
two-dimensional	O
space	O
x	O
would	O
be	O
constrained	O
to	O
lie	O
exactly	O
on	O
a	O
two-dimensional	O
nonlinear	O
manifold	B
embedded	O
in	O
the	O
six-dimensional	O
feature	B
space	I
we	O
have	O
already	O
highlighted	O
the	O
fact	O
that	O
the	O
support	B
vector	I
machine	I
does	O
not	O
provide	O
probabilistic	O
outputs	O
but	O
instead	O
makes	O
classification	B
decisions	O
for	O
new	O
input	O
vectors	O
veropoulos	O
et	O
al	O
discuss	O
modifications	O
to	O
the	O
svm	O
to	O
allow	O
the	O
trade-off	O
between	O
false	O
positive	O
and	O
false	O
negative	O
errors	O
to	O
be	O
controlled	O
however	O
if	O
we	O
wish	O
to	O
use	O
the	O
svm	O
as	O
a	O
module	O
in	O
a	O
larger	O
probabilistic	O
system	O
then	O
probabilistic	O
predictions	O
of	O
the	O
class	O
label	O
t	O
for	O
new	O
inputs	O
x	O
are	O
required	O
to	O
address	O
this	O
issue	O
platt	O
has	O
proposed	O
fitting	O
a	O
logistic	B
sigmoid	I
to	O
the	O
outputs	O
of	O
a	O
previously	O
trained	O
support	B
vector	I
machine	I
specifically	O
the	O
required	O
conditional	B
probability	B
is	O
assumed	O
to	O
be	O
of	O
the	O
form	O
pt	O
b	O
where	O
yx	O
is	O
defined	O
by	O
values	O
for	O
the	O
parameters	O
a	O
and	O
b	O
are	O
found	O
by	O
minimizing	O
the	O
cross-entropy	B
error	B
function	I
defined	O
by	O
a	O
training	B
set	I
consisting	O
of	O
pairs	O
of	O
values	O
yxn	O
and	O
tn	O
the	O
data	O
used	O
to	O
fit	O
the	O
sigmoid	O
needs	O
to	O
be	O
independent	B
of	O
that	O
used	O
to	O
train	O
the	O
original	O
svm	O
in	O
order	O
to	O
avoid	O
severe	O
over-fitting	B
this	O
twostage	O
approach	O
is	O
equivalent	O
to	O
assuming	O
that	O
the	O
output	O
yx	O
of	O
the	O
support	B
vector	I
machine	I
represents	O
the	O
log-odds	O
of	O
x	O
belonging	O
to	O
class	O
t	O
because	O
the	O
svm	O
training	B
procedure	O
is	O
not	O
specifically	O
intended	O
to	O
encourage	O
this	O
the	O
svm	O
can	O
give	O
a	O
poor	O
approximation	O
to	O
the	O
posterior	O
probabilities	O
relation	O
to	O
logistic	B
regression	B
as	O
with	O
the	O
separable	O
case	O
we	O
can	O
re-cast	O
the	O
svm	O
for	O
nonseparable	O
distributions	O
in	O
terms	O
of	O
the	O
minimization	O
of	O
a	O
regularized	O
error	B
function	I
this	O
will	O
also	O
allow	O
us	O
to	O
highlight	O
similarities	O
and	O
differences	O
compared	O
to	O
the	O
logistic	B
regression	B
model	O
we	O
have	O
seen	O
that	O
for	O
data	O
points	O
that	O
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
boundary	O
and	O
which	O
therefore	O
satisfy	O
yntn	O
we	O
have	O
n	O
and	O
for	O
the	O
maximum	O
margin	B
classifiers	O
ez	O
figure	O
plot	O
of	O
the	O
hinge	B
error	B
function	I
used	O
in	O
support	B
vector	I
machines	O
shown	O
in	O
blue	O
along	O
with	O
the	O
error	B
function	I
for	O
logistic	B
regression	B
rescaled	O
by	O
a	O
factor	O
of	O
so	O
that	O
it	O
passes	O
through	O
the	O
point	O
shown	O
in	O
red	O
also	O
shown	O
are	O
the	O
misclassification	O
error	B
in	O
black	O
and	O
the	O
squared	O
error	B
in	O
green	O
z	O
remaining	O
points	O
we	O
have	O
n	O
yntn	O
thus	O
the	O
objective	O
function	O
can	O
be	O
written	O
to	O
an	O
overall	O
multiplicative	O
constant	O
in	O
the	O
form	O
esvyntn	O
where	O
and	O
esv	O
is	O
the	O
hinge	B
error	B
function	I
defined	O
by	O
esvyntn	O
yntn	O
where	O
denotes	O
the	O
positive	O
part	O
the	O
hinge	B
error	B
function	I
so-called	O
because	O
of	O
its	O
shape	O
is	O
plotted	O
in	O
figure	O
it	O
can	O
be	O
viewed	O
as	O
an	O
approximation	O
to	O
the	O
misclassification	O
error	B
i	O
e	O
the	O
error	B
function	I
that	O
ideally	O
we	O
would	O
like	O
to	O
minimize	O
which	O
is	O
also	O
shown	O
in	O
figure	O
when	O
we	O
considered	O
the	O
logistic	B
regression	B
model	O
in	O
section	O
we	O
found	O
it	O
convenient	O
to	O
work	O
with	O
target	O
variable	O
t	O
for	O
comparison	O
with	O
the	O
support	B
vector	I
machine	I
we	O
first	O
reformulate	O
maximum	B
likelihood	I
logistic	B
regression	B
using	O
the	O
target	O
variable	O
t	O
to	O
do	O
this	O
we	O
note	O
that	O
pt	O
where	O
yx	O
is	O
given	O
by	O
and	O
is	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
it	O
follows	O
that	O
pt	O
y	O
where	O
we	O
have	O
used	O
the	O
properties	O
of	O
the	O
logistic	B
sigmoid	I
function	O
and	O
so	O
we	O
can	O
write	O
pty	O
exercise	O
from	O
this	O
we	O
can	O
construct	O
an	O
error	B
function	I
by	O
taking	O
the	O
negative	O
logarithm	O
of	O
the	O
likelihood	B
function	I
that	O
with	O
a	O
quadratic	O
regularizer	O
takes	O
the	O
form	O
elryntn	O
where	O
elryt	O
ln	O
exp	O
yt	O
sparse	O
kernel	O
machines	O
for	O
comparison	O
with	O
other	O
error	B
functions	O
we	O
can	O
divide	O
by	O
so	O
that	O
the	O
error	B
function	I
passes	O
through	O
the	O
point	O
this	O
rescaled	O
error	B
function	I
is	O
also	O
plotted	O
in	O
figure	O
and	O
we	O
see	O
that	O
it	O
has	O
a	O
similar	O
form	O
to	O
the	O
support	B
vector	I
error	B
function	I
the	O
key	O
difference	O
is	O
that	O
the	O
flat	O
region	O
in	O
esvyt	O
leads	O
to	O
sparse	O
solutions	O
both	O
the	O
logistic	O
error	B
and	O
the	O
hinge	O
loss	O
can	O
be	O
viewed	O
as	O
continuous	O
approximations	O
to	O
the	O
misclassification	O
error	B
another	O
continuous	O
error	B
function	I
that	O
has	O
sometimes	O
been	O
used	O
to	O
solve	O
classification	B
problems	O
is	O
the	O
squared	O
error	B
which	O
is	O
again	O
plotted	O
in	O
figure	O
it	O
has	O
the	O
property	O
however	O
of	O
placing	O
increasing	O
emphasis	O
on	O
data	O
points	O
that	O
are	O
correctly	O
classified	O
but	O
that	O
are	O
a	O
long	O
way	O
from	O
the	O
decision	B
boundary	I
on	O
the	O
correct	O
side	O
such	O
points	O
will	O
be	O
strongly	O
weighted	O
at	O
the	O
expense	O
of	O
misclassified	O
points	O
and	O
so	O
if	O
the	O
objective	O
is	O
to	O
minimize	O
the	O
misclassification	O
rate	O
then	O
a	O
monotonically	O
decreasing	O
error	B
function	I
would	O
be	O
a	O
better	O
choice	O
multiclass	B
svms	O
the	O
support	B
vector	I
machine	I
is	O
fundamentally	O
a	O
two-class	O
classifier	O
in	O
practice	O
however	O
we	O
often	O
have	O
to	O
tackle	O
problems	O
involving	O
k	O
classes	O
various	O
methods	O
have	O
therefore	O
been	O
proposed	O
for	O
combining	O
multiple	O
two-class	O
svms	O
in	O
order	O
to	O
build	O
a	O
multiclass	B
classifier	O
one	O
commonly	O
used	O
approach	O
is	O
to	O
construct	O
k	O
separate	O
svms	O
in	O
which	O
the	O
kth	O
model	O
ykx	O
is	O
trained	O
using	O
the	O
data	O
from	O
class	O
ck	O
as	O
the	O
positive	O
examples	O
and	O
the	O
data	O
from	O
the	O
remaining	O
k	O
classes	O
as	O
the	O
negative	O
examples	O
this	O
is	O
known	O
as	O
the	O
one-versus-the-rest	O
approach	O
however	O
in	O
figure	O
we	O
saw	O
that	O
using	O
the	O
decisions	O
of	O
the	O
individual	O
classifiers	O
can	O
lead	O
to	O
inconsistent	O
results	O
in	O
which	O
an	O
input	O
is	O
assigned	O
to	O
multiple	O
classes	O
simultaneously	O
this	O
problem	O
is	O
sometimes	O
addressed	O
by	O
making	O
predictions	O
for	O
new	O
inputs	O
x	O
using	O
yx	O
max	O
k	O
ykx	O
unfortunately	O
this	O
heuristic	O
approach	O
suffers	O
from	O
the	O
problem	O
that	O
the	O
different	O
classifiers	O
were	O
trained	O
on	O
different	O
tasks	O
and	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
realvalued	O
quantities	O
ykx	O
for	O
different	O
classifiers	O
will	O
have	O
appropriate	O
scales	O
another	O
problem	O
with	O
the	O
one-versus-the-rest	O
approach	O
is	O
that	O
the	O
training	B
sets	O
are	O
imbalanced	O
for	O
instance	O
if	O
we	O
have	O
ten	O
classes	O
each	O
with	O
equal	O
numbers	O
of	O
training	B
data	O
points	O
then	O
the	O
individual	O
classifiers	O
are	O
trained	O
on	O
data	O
sets	O
comprising	O
negative	O
examples	O
and	O
only	O
positive	O
examples	O
and	O
the	O
symmetry	O
of	O
the	O
original	O
problem	O
is	O
lost	O
a	O
variant	O
of	O
the	O
one-versus-the-rest	O
scheme	O
was	O
proposed	O
by	O
lee	O
et	O
al	O
who	O
modify	O
the	O
target	O
values	O
so	O
that	O
the	O
positive	O
class	O
has	O
target	O
and	O
the	O
negative	O
class	O
has	O
target	O
weston	O
and	O
watkins	O
define	O
a	O
single	O
objective	O
function	O
for	O
training	B
all	O
k	O
svms	O
simultaneously	O
based	O
on	O
maximizing	O
the	O
margin	B
from	O
each	O
to	O
remaining	O
classes	O
however	O
this	O
can	O
result	O
in	O
much	O
slower	O
training	B
because	O
instead	O
of	O
solving	O
k	O
separate	O
optimization	O
problems	O
each	O
over	O
n	O
data	O
points	O
with	O
an	O
overall	O
cost	O
of	O
okn	O
a	O
single	O
optimization	O
problem	O
of	O
size	O
must	O
be	O
solved	O
giving	O
an	O
overall	O
cost	O
of	O
ok	O
maximum	O
margin	B
classifiers	O
another	O
approach	O
is	O
to	O
train	O
kk	O
different	O
svms	O
on	O
all	O
possible	O
pairs	O
of	O
classes	O
and	O
then	O
to	O
classify	O
test	O
points	O
according	O
to	O
which	O
class	O
has	O
the	O
highest	O
number	O
of	O
votes	O
an	O
approach	O
that	O
is	O
sometimes	O
called	O
one-versus-one	O
again	O
we	O
saw	O
in	O
figure	O
that	O
this	O
can	O
lead	O
to	O
ambiguities	O
in	O
the	O
resulting	O
classification	B
also	O
for	O
large	O
k	O
this	O
approach	O
requires	O
significantly	O
more	O
training	B
time	O
than	O
the	O
one-versus-the-rest	O
approach	O
similarly	O
to	O
evaluate	O
test	O
points	O
significantly	O
more	O
computation	O
is	O
required	O
the	O
latter	O
problem	O
can	O
be	O
alleviated	O
by	O
organizing	O
the	O
pairwise	O
classifiers	O
into	O
a	O
directed	B
acyclic	I
graph	I
to	O
be	O
confused	O
with	O
a	O
probabilistic	O
graphical	B
model	I
leading	O
to	O
the	O
dagsvm	B
et	O
al	O
for	O
k	O
classes	O
the	O
dagsvm	B
has	O
a	O
total	O
of	O
kk	O
classifiers	O
and	O
to	O
classify	O
a	O
new	O
test	O
point	O
only	O
k	O
pairwise	O
classifiers	O
need	O
to	O
be	O
evaluated	O
with	O
the	O
particular	O
classifiers	O
used	O
depending	O
on	O
which	O
path	O
through	O
the	O
graph	O
is	O
traversed	O
a	O
different	O
approach	O
to	O
multiclass	B
classification	B
based	O
on	O
error-correcting	B
output	I
codes	I
was	O
developed	O
by	O
dietterich	O
and	O
bakiri	O
and	O
applied	O
to	O
support	B
vector	I
machines	O
by	O
allwein	O
et	O
al	O
this	O
can	O
be	O
viewed	O
as	O
a	O
generalization	B
of	O
the	O
voting	O
scheme	O
of	O
the	O
one-versus-one	O
approach	O
in	O
which	O
more	O
general	O
partitions	O
of	O
the	O
classes	O
are	O
used	O
to	O
train	O
the	O
individual	O
classifiers	O
the	O
k	O
classes	O
themselves	O
are	O
represented	O
as	O
particular	O
sets	O
of	O
responses	O
from	O
the	O
two-class	O
classifiers	O
chosen	O
and	O
together	O
with	O
a	O
suitable	O
decoding	O
scheme	O
this	O
gives	O
robustness	B
to	O
errors	O
and	O
to	O
ambiguity	O
in	O
the	O
outputs	O
of	O
the	O
individual	O
classifiers	O
although	O
the	O
application	O
of	O
svms	O
to	O
multiclass	B
classification	B
problems	O
remains	O
an	O
open	O
issue	O
in	O
practice	O
the	O
one-versus-the-rest	O
approach	O
is	O
the	O
most	O
widely	O
used	O
in	O
spite	O
of	O
its	O
ad-hoc	O
formulation	O
and	O
its	O
practical	O
limitations	O
there	O
are	O
also	O
single-class	O
support	B
vector	I
machines	O
which	O
solve	O
an	O
unsupervised	B
learning	B
problem	O
related	O
to	O
probability	B
density	B
estimation	I
instead	O
of	O
modelling	O
the	O
density	B
of	O
data	O
however	O
these	O
methods	O
aim	O
to	O
find	O
a	O
smooth	O
boundary	O
enclosing	O
a	O
region	O
of	O
high	O
density	B
the	O
boundary	O
is	O
chosen	O
to	O
represent	O
a	O
quantile	O
of	O
the	O
density	B
that	O
is	O
the	O
probability	B
that	O
a	O
data	O
point	O
drawn	O
from	O
the	O
distribution	O
will	O
land	O
inside	O
that	O
region	O
is	O
given	O
by	O
a	O
fixed	O
number	O
between	O
and	O
that	O
is	O
specified	O
in	O
advance	O
this	O
is	O
a	O
more	O
restricted	O
problem	O
than	O
estimating	O
the	O
full	O
density	B
but	O
may	O
be	O
sufficient	O
in	O
specific	O
applications	O
two	O
approaches	O
to	O
this	O
problem	O
using	O
support	B
vector	I
machines	O
have	O
been	O
proposed	O
the	O
algorithm	O
of	O
sch	O
olkopf	O
et	O
al	O
tries	O
to	O
find	O
a	O
hyperplane	O
that	O
separates	O
all	O
but	O
a	O
fixed	O
fraction	O
of	O
the	O
training	B
data	O
from	O
the	O
origin	O
while	O
at	O
the	O
same	O
time	O
maximizing	O
the	O
distance	O
of	O
the	O
hyperplane	O
from	O
the	O
origin	O
while	O
tax	O
and	O
duin	O
look	O
for	O
the	O
smallest	O
sphere	O
in	O
feature	B
space	I
that	O
contains	O
all	O
but	O
a	O
fraction	O
of	O
the	O
data	O
points	O
for	O
kernels	O
kx	O
that	O
are	O
functions	O
only	O
of	O
x	O
the	O
two	O
algorithms	O
are	O
equivalent	O
svms	O
for	B
regression	B
we	O
now	O
extend	O
support	B
vector	I
machines	O
to	O
regression	B
problems	O
while	O
at	O
the	O
same	O
time	O
preserving	O
the	O
property	O
of	O
sparseness	O
in	O
simple	O
linear	B
regression	B
we	O
section	O
sparse	O
kernel	O
machines	O
figure	O
plot	O
of	O
an	O
error	B
function	I
red	O
in	O
which	O
the	O
error	B
increases	O
linearly	O
with	O
distance	O
beyond	O
the	O
insensitive	O
region	O
also	O
shown	O
for	O
comparison	O
is	O
the	O
quadratic	O
error	B
function	I
green	O
ez	O
z	O
minimize	O
a	O
regularized	O
error	B
function	I
given	O
by	O
to	O
obtain	O
sparse	O
solutions	O
the	O
quadratic	O
error	B
function	I
is	O
replaced	O
by	O
an	O
error	B
function	I
which	O
gives	O
zero	O
error	B
if	O
the	O
absolute	O
difference	O
between	O
the	O
prediction	O
yx	O
and	O
the	O
target	O
t	O
is	O
less	O
than	O
where	O
a	O
simple	O
example	O
of	O
an	O
error	B
function	I
having	O
a	O
linear	O
cost	O
associated	O
with	O
errors	O
outside	O
the	O
insensitive	O
region	O
is	O
given	O
by	O
eyx	O
t	O
t	O
otherwise	O
if	O
t	O
c	O
and	O
is	O
illustrated	O
in	O
figure	O
we	O
therefore	O
minimize	O
a	O
regularized	O
error	B
function	I
given	O
by	O
eyxn	O
tn	O
where	O
yx	O
is	O
given	O
by	O
by	O
convention	O
the	O
regularization	B
parameter	O
denoted	O
c	O
appears	O
in	O
front	O
of	O
the	O
error	B
term	O
as	O
before	O
we	O
can	O
re-express	O
the	O
optimization	O
problem	O
by	O
introducing	O
slack	O
variables	O
for	O
each	O
data	O
point	O
xn	O
we	O
now	O
need	O
two	O
slack	O
variables	O
n	O
and	O
corresponds	O
to	O
a	O
point	O
for	O
which	O
tn	O
yxn	O
as	O
illustrated	O
in	O
figure	O
n	O
where	O
n	O
corresponds	O
to	O
a	O
point	O
for	O
which	O
tn	O
yxn	O
n	O
the	O
condition	O
for	O
a	O
target	O
point	O
to	O
lie	O
inside	O
the	O
is	O
that	O
yn	O
tn	O
yn	O
where	O
yn	O
yxn	O
introducing	O
the	O
slack	O
variables	O
allows	O
points	O
to	O
lie	O
outside	O
the	O
tube	O
provided	O
the	O
slack	O
variables	O
are	O
nonzero	O
and	O
the	O
corresponding	O
conditions	O
are	O
tn	O
yxn	O
n	O
tn	O
yxn	O
n	O
maximum	O
margin	B
classifiers	O
figure	O
illustration	O
of	O
svm	O
regression	B
showing	O
the	O
regression	B
curve	O
together	O
with	O
the	O
tube	O
also	O
shown	O
are	O
examples	O
of	O
the	O
slack	O
variables	O
and	O
b	O
points	O
above	O
the	O
have	O
and	O
b	O
points	O
below	O
the	O
have	O
and	O
b	O
and	O
points	O
inside	O
the	O
have	O
b	O
yx	O
y	O
y	O
y	O
x	O
the	O
error	B
function	I
for	O
support	B
vector	I
regression	B
can	O
then	O
be	O
written	O
as	O
c	O
n	O
n	O
and	O
this	O
can	O
be	O
achieved	O
by	O
introducing	O
lagrange	B
multipliers	O
an	O
which	O
must	O
be	O
minimized	O
subject	O
to	O
the	O
constraints	O
n	O
n	O
as	O
well	O
as	O
n	O
n	O
and	O
optimizing	O
the	O
lagrangian	B
n	O
n	O
n	O
n	O
yn	O
tn	O
n	O
n	O
an	O
n	O
yn	O
tn	O
grangian	O
with	O
respect	O
to	O
w	O
b	O
n	O
n	O
to	O
zero	O
giving	O
we	O
now	O
substitute	O
for	O
yx	O
using	O
and	O
then	O
set	O
the	O
derivatives	O
of	O
the	O
la	O
n	O
l	O
c	O
w	O
n	O
c	O
an	O
n	O
c	O
l	O
w	O
l	O
b	O
l	O
n	O
l	O
n	O
exercise	O
using	O
these	O
results	O
to	O
eliminate	O
the	O
corresponding	O
variables	O
from	O
the	O
lagrangian	B
we	O
see	O
that	O
the	O
dual	O
problem	O
involves	O
maximizing	O
sparse	O
kernel	O
machines	O
xm	O
with	O
respect	O
to	O
and	O
where	O
we	O
have	O
introduced	O
the	O
kernel	O
kx	O
we	O
note	O
that	O
an	O
are	O
both	O
required	O
because	O
these	O
are	O
lagrange	B
multipliers	O
also	O
n	O
and	O
n	O
together	O
with	O
and	O
require	O
an	O
c	O
c	O
and	O
so	O
again	O
we	O
have	O
the	O
box	B
constraints	I
again	O
this	O
is	O
a	O
constrained	O
maximization	O
and	O
to	O
find	O
the	O
constraints	O
together	O
with	O
the	O
condition	O
substituting	O
into	O
we	O
see	O
that	O
predictions	O
for	O
new	O
inputs	O
can	O
be	O
made	O
using	O
yx	O
xn	O
b	O
which	O
is	O
again	O
expressed	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
the	O
corresponding	O
karush-kuhn-tucker	B
conditions	I
which	O
state	O
that	O
at	O
the	O
solution	O
the	O
product	O
of	O
the	O
dual	O
variables	O
and	O
the	O
constraints	O
must	O
vanish	O
are	O
given	O
by	O
and	O
such	O
points	O
must	O
lie	O
either	O
on	O
or	O
below	O
the	O
lower	O
boundary	O
of	O
the	O
from	O
these	O
we	O
can	O
obtain	O
several	O
useful	O
results	O
first	O
of	O
all	O
we	O
note	O
that	O
a	O
coefficient	O
an	O
can	O
only	O
be	O
nonzero	O
if	O
n	O
yn	O
tn	O
which	O
implies	O
that	O
the	O
data	O
point	O
either	O
lies	O
on	O
the	O
upper	O
boundary	O
of	O
the	O
n	O
or	O
lies	O
above	O
the	O
upper	O
boundary	O
n	O
similarly	O
a	O
nonzero	O
value	O
implies	O
n	O
yn	O
tn	O
furthermore	O
the	O
two	O
constraints	O
n	O
yn	O
tn	O
and	O
n	O
yn	O
tn	O
n	O
are	O
nonnegative	O
while	O
is	O
strictly	O
positive	O
and	O
so	O
for	O
every	O
data	O
point	O
xn	O
either	O
an	O
both	O
must	O
be	O
zero	O
in	O
other	O
words	O
those	O
for	O
which	O
either	O
an	O
these	O
are	O
points	O
that	O
are	O
incompatible	O
as	O
is	O
easily	O
seen	O
by	O
adding	O
them	O
together	O
and	O
noting	O
that	O
n	O
and	O
the	O
support	O
vectors	O
are	O
those	O
data	O
points	O
that	O
contribute	O
to	O
predictions	O
given	O
by	O
lie	O
on	O
the	O
boundary	O
of	O
the	O
or	O
outside	O
the	O
tube	O
all	O
points	O
within	O
the	O
tube	O
have	O
an	O
c	O
c	O
an	O
n	O
yn	O
tn	O
n	O
yn	O
tn	O
n	O
an	O
n	O
an	O
we	O
again	O
have	O
a	O
sparse	O
solution	O
and	O
the	O
only	O
terms	O
that	O
have	O
to	O
be	O
maximum	O
margin	B
classifiers	O
evaluated	O
in	O
the	O
predictive	O
model	O
are	O
those	O
that	O
involve	O
the	O
support	O
vectors	O
the	O
parameter	O
b	O
can	O
be	O
found	O
by	O
considering	O
a	O
data	O
point	O
for	O
which	O
an	O
c	O
which	O
from	O
must	O
have	O
n	O
and	O
from	O
must	O
therefore	O
satisfy	O
yn	O
tn	O
using	O
and	O
solving	O
for	O
b	O
we	O
obtain	O
b	O
tn	O
wt	O
tn	O
xm	O
for	O
which	O
c	O
in	O
practice	O
it	O
is	O
better	O
to	O
average	O
over	O
all	O
such	O
estimates	O
of	O
where	O
we	O
have	O
used	O
we	O
can	O
obtain	O
an	O
analogous	O
result	O
by	O
considering	O
a	O
point	O
b	O
as	O
with	O
the	O
classification	B
case	O
there	O
is	O
an	O
alternative	O
formulation	O
of	O
the	O
svm	O
for	B
regression	B
in	O
which	O
the	O
parameter	O
governing	O
complexity	O
has	O
a	O
more	O
intuitive	O
interpretation	O
olkopf	O
et	O
al	O
in	O
particular	O
instead	O
of	O
fixing	O
the	O
width	O
of	O
the	O
insensitive	O
region	O
we	O
fix	O
instead	O
a	O
parameter	O
that	O
bounds	O
the	O
fraction	O
of	O
points	O
lying	O
outside	O
the	O
tube	O
this	O
involves	O
maximizing	O
xm	O
subject	O
to	O
the	O
constraints	O
an	O
cn	O
cn	O
c	O
it	O
can	O
be	O
shown	O
that	O
there	O
are	O
at	O
most	O
n	O
data	O
points	O
falling	O
outside	O
the	O
insensitive	O
tube	O
while	O
at	O
least	O
n	O
data	O
points	O
are	O
support	O
vectors	O
and	O
so	O
lie	O
either	O
on	O
the	O
tube	O
or	O
outside	O
it	O
the	O
use	O
of	O
a	O
support	B
vector	I
machine	I
to	O
solve	O
a	O
regression	B
problem	O
is	O
illustrated	O
using	O
the	O
sinusoidal	B
data	I
set	O
in	O
figure	O
here	O
the	O
parameters	O
and	O
c	O
have	O
been	O
chosen	O
by	O
hand	O
in	O
practice	O
their	O
values	O
would	O
typically	O
be	O
determined	O
by	O
crossvalidation	O
appendix	O
a	O
sparse	O
kernel	O
machines	O
figure	O
illustration	O
of	O
the	O
for	B
regression	B
applied	O
to	O
the	O
sinusoidal	O
synthetic	O
data	O
set	O
using	O
gaussian	B
kernels	O
the	O
predicted	O
regression	B
curve	O
is	O
shown	O
by	O
the	O
red	O
line	O
and	O
the	O
tube	O
corresponds	O
to	O
the	O
shaded	O
region	O
also	O
the	O
data	O
points	O
are	O
shown	O
in	O
green	O
and	O
those	O
with	O
support	O
vectors	O
are	O
indicated	O
by	O
blue	O
circles	O
t	O
x	O
computational	B
learning	B
theory	B
historically	O
support	B
vector	I
machines	O
have	O
largely	O
been	O
motivated	O
and	O
analysed	O
using	O
a	O
theoretical	O
framework	O
known	O
as	O
computational	B
learning	B
theory	B
also	O
sometimes	O
called	O
statistical	O
learning	B
theory	B
and	O
biggs	O
kearns	O
and	O
vazirani	O
vapnik	O
vapnik	O
this	O
has	O
its	O
origins	O
with	O
valiant	O
who	O
formulated	O
the	O
probably	B
approximately	I
correct	I
or	O
pac	O
learning	B
framework	O
the	O
goal	O
of	O
the	O
pac	O
framework	O
is	O
to	O
understand	O
how	O
large	O
a	O
data	O
set	O
needs	O
to	O
be	O
in	O
order	O
to	O
give	O
good	O
generalization	B
it	O
also	O
gives	O
bounds	O
for	O
the	O
computational	O
cost	O
of	O
learning	B
although	O
we	O
do	O
not	O
consider	O
these	O
here	O
suppose	O
that	O
a	O
data	O
set	O
d	O
of	O
size	O
n	O
is	O
drawn	O
from	O
some	O
joint	O
distribution	O
px	O
t	O
where	O
x	O
is	O
the	O
input	O
variable	O
and	O
t	O
represents	O
the	O
class	O
label	O
and	O
that	O
we	O
restrict	O
attention	O
to	O
noise	O
free	O
situations	O
in	O
which	O
the	O
class	O
labels	O
are	O
determined	O
by	O
some	O
deterministic	O
function	O
t	O
gx	O
in	O
pac	O
learning	B
we	O
say	O
that	O
a	O
function	O
fxd	O
drawn	O
from	O
a	O
space	O
f	O
of	O
such	O
functions	O
on	O
the	O
basis	O
of	O
the	O
training	B
set	I
d	O
has	O
good	O
generalization	B
if	O
its	O
expected	O
error	B
rate	O
is	O
below	O
some	O
pre-specified	O
threshold	O
so	O
that	O
where	O
i	O
is	O
the	O
indicator	O
function	O
and	O
the	O
expectation	B
is	O
with	O
respect	O
to	O
the	O
distribution	O
px	O
t	O
the	O
quantity	O
on	O
the	O
left-hand	O
side	O
is	O
a	O
random	O
variable	O
because	O
it	O
depends	O
on	O
the	O
training	B
set	I
d	O
and	O
the	O
pac	O
framework	O
requires	O
that	O
holds	O
with	O
probability	B
greater	O
than	O
for	O
a	O
data	O
set	O
d	O
drawn	O
randomly	O
from	O
px	O
t	O
here	O
is	O
another	O
pre-specified	O
parameter	O
and	O
the	O
terminology	O
probably	B
approximately	I
correct	I
comes	O
from	O
the	O
requirement	O
that	O
with	O
high	O
probability	B
than	O
the	O
error	B
rate	O
be	O
small	O
than	O
for	O
a	O
given	O
choice	O
of	O
model	O
space	O
f	O
and	O
for	O
given	O
parameters	O
and	O
pac	O
learning	B
aims	O
to	O
provide	O
bounds	O
on	O
the	O
minimum	O
size	O
n	O
of	O
data	O
set	O
needed	O
to	O
meet	O
this	O
criterion	O
a	O
key	O
quantity	O
in	O
pac	O
learning	B
is	O
the	O
vapnik-chervonenkis	B
dimension	I
or	O
vc	O
dimension	O
which	O
provides	O
a	O
measure	O
of	O
the	O
complexity	O
of	O
a	O
space	O
of	O
functions	O
and	O
which	O
allows	O
the	O
pac	O
framework	O
to	O
be	O
extended	B
to	O
spaces	O
containing	O
an	O
infinite	O
number	O
of	O
functions	O
ext	O
t	O
the	O
bounds	O
derived	O
within	O
the	O
pac	O
framework	O
are	O
often	O
described	O
as	O
worst	O
relevance	B
vector	I
machines	O
case	O
because	O
they	O
apply	O
to	O
any	O
choice	O
for	O
the	O
distribution	O
px	O
t	O
so	O
long	O
as	O
both	O
the	O
training	B
and	O
the	O
test	O
examples	O
are	O
drawn	O
from	O
the	O
same	O
distribution	O
and	O
for	O
any	O
choice	O
for	O
the	O
function	O
fx	O
so	O
long	O
as	O
it	O
belongs	O
to	O
f	O
in	O
real-world	O
applications	O
of	O
machine	O
learning	B
we	O
deal	O
with	O
distributions	O
that	O
have	O
significant	O
regularity	O
for	O
example	O
in	O
which	O
large	O
regions	O
of	O
input	O
space	O
carry	O
the	O
same	O
class	O
label	O
as	O
a	O
consequence	O
of	O
the	O
lack	O
of	O
any	O
assumptions	O
about	O
the	O
form	O
of	O
the	O
distribution	O
the	O
pac	O
bounds	O
are	O
very	O
conservative	O
in	O
other	O
words	O
they	O
strongly	O
over-estimate	O
the	O
size	O
of	O
data	O
sets	O
required	O
to	O
achieve	O
a	O
given	O
generalization	B
performance	O
for	O
this	O
reason	O
pac	O
bounds	O
have	O
found	O
few	O
if	O
any	O
practical	O
applications	O
one	O
attempt	O
to	O
improve	O
the	O
tightness	O
of	O
the	O
pac	O
bounds	O
is	O
the	O
pac-bayesian	B
framework	I
which	O
considers	O
a	O
distribution	O
over	O
the	O
space	O
f	O
of	O
functions	O
somewhat	O
analogous	O
to	O
the	O
prior	B
in	O
a	O
bayesian	B
treatment	O
this	O
still	O
considers	O
any	O
possible	O
choice	O
for	O
px	O
t	O
and	O
so	O
although	O
the	O
bounds	O
are	O
tighter	O
they	O
are	O
still	O
very	O
conservative	O
relevance	B
vector	I
machines	O
support	B
vector	I
machines	O
have	O
been	O
used	O
in	O
a	O
variety	O
of	O
classification	B
and	O
regression	B
applications	O
nevertheless	O
they	O
suffer	O
from	O
a	O
number	O
of	O
limitations	O
several	O
of	O
which	O
have	O
been	O
highlighted	O
already	O
in	O
this	O
chapter	O
in	O
particular	O
the	O
outputs	O
of	O
an	O
svm	O
represent	O
decisions	O
rather	O
than	O
posterior	O
probabilities	O
also	O
the	O
svm	O
was	O
originally	O
formulated	O
for	O
two	O
classes	O
and	O
the	O
extension	O
to	O
k	O
classes	O
is	O
problematic	O
there	O
is	O
a	O
complexity	O
parameter	O
c	O
or	O
well	O
as	O
a	O
parameter	O
in	O
the	O
case	O
of	O
regression	B
that	O
must	O
be	O
found	O
using	O
a	O
hold-out	O
method	O
such	O
as	O
cross-validation	B
finally	O
predictions	O
are	O
expressed	O
as	O
linear	O
combinations	O
of	O
kernel	O
functions	O
that	O
are	O
centred	O
on	O
training	B
data	O
points	O
and	O
that	O
are	O
required	O
to	O
be	O
positive	B
definite	I
the	O
relevance	B
vector	I
machine	I
or	O
rvm	O
is	O
a	O
bayesian	B
sparse	O
kernel	O
technique	O
for	B
regression	B
and	O
classification	B
that	O
shares	O
many	O
of	O
the	O
characteristics	O
of	O
the	O
svm	O
whilst	O
avoiding	O
its	O
principal	O
limitations	O
additionally	O
it	O
typically	O
leads	O
to	O
much	O
sparser	O
models	O
resulting	O
in	O
correspondingly	O
faster	O
performance	O
on	O
test	O
data	O
whilst	O
maintaining	O
comparable	O
generalization	B
error	B
in	O
contrast	O
to	O
the	O
svm	O
we	O
shall	O
find	O
it	O
more	O
convenient	O
to	O
introduce	O
the	O
regres	O
sion	O
form	O
of	O
the	O
rvm	O
first	O
and	O
then	O
consider	O
the	O
extension	O
to	O
classification	B
tasks	O
rvm	O
for	B
regression	B
the	O
relevance	B
vector	I
machine	I
for	B
regression	B
is	O
a	O
linear	O
model	O
of	O
the	O
form	O
studied	O
in	O
chapter	O
but	O
with	O
a	O
modified	O
prior	B
that	O
results	O
in	O
sparse	O
solutions	O
the	O
model	O
defines	O
a	O
conditional	B
distribution	O
for	O
a	O
real-valued	O
target	O
variable	O
t	O
given	O
an	O
input	O
vector	O
x	O
which	O
takes	O
the	O
form	O
ptx	O
w	O
n	O
sparse	O
kernel	O
machines	O
where	O
by	O
a	O
linear	O
model	O
of	O
the	O
form	O
is	O
the	O
noise	O
precision	O
noise	O
variance	B
and	O
the	O
mean	B
is	O
given	O
yx	O
wi	O
ix	O
wt	O
with	O
fixed	O
nonlinear	O
basis	O
functions	O
ix	O
which	O
will	O
typically	O
include	O
a	O
constant	O
term	O
so	O
that	O
the	O
corresponding	O
weight	B
parameter	I
represents	O
a	O
bias	B
the	O
relevance	B
vector	I
machine	I
is	O
a	O
specific	O
instance	O
of	O
this	O
model	O
which	O
is	O
intended	O
to	O
mirror	O
the	O
structure	O
of	O
the	O
support	B
vector	I
machine	I
in	O
particular	O
the	O
basis	O
functions	O
are	O
given	O
by	O
kernels	O
with	O
one	O
kernel	O
associated	O
with	O
each	O
of	O
the	O
data	O
points	O
from	O
the	O
training	B
set	I
the	O
general	O
expression	O
then	O
takes	O
the	O
svm-like	O
form	O
yx	O
wnkx	O
xn	O
b	O
where	O
b	O
is	O
a	O
bias	B
parameter	I
the	O
number	O
of	O
parameters	O
in	O
this	O
case	O
is	O
m	O
n	O
and	O
yx	O
has	O
the	O
same	O
form	O
as	O
the	O
predictive	O
model	O
for	O
the	O
svm	O
except	O
that	O
the	O
coefficients	O
an	O
are	O
here	O
denoted	O
wn	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
subsequent	O
analysis	O
is	O
valid	O
for	O
arbitrary	O
choices	O
of	O
basis	B
function	I
and	O
for	O
generality	O
we	O
shall	O
work	O
with	O
the	O
form	O
in	O
contrast	O
to	O
the	O
svm	O
there	O
is	O
no	O
restriction	O
to	O
positivedefinite	O
kernels	O
nor	O
are	O
the	O
basis	O
functions	O
tied	O
in	O
either	O
number	O
or	O
location	O
to	O
the	O
training	B
data	O
points	O
suppose	O
we	O
are	O
given	O
a	O
set	O
of	O
n	O
observations	O
of	O
the	O
input	O
vector	O
x	O
which	O
we	O
n	O
with	O
n	O
n	O
the	O
denote	O
collectively	O
by	O
a	O
data	O
matrix	O
x	O
whose	O
nth	O
row	O
is	O
xt	O
corresponding	O
target	O
values	O
are	O
given	O
by	O
t	O
tnt	O
thus	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
ptx	O
w	O
ptnxn	O
w	O
next	O
we	O
introduce	O
a	O
prior	B
distribution	O
over	O
the	O
parameter	O
vector	O
w	O
and	O
as	O
in	O
chapter	O
we	O
shall	O
consider	O
a	O
zero-mean	O
gaussian	B
prior	B
however	O
the	O
key	O
difference	O
in	O
the	O
rvm	O
is	O
that	O
we	O
introduce	O
a	O
separate	O
hyperparameter	B
i	O
for	O
each	O
of	O
the	O
weight	O
parameters	O
wi	O
instead	O
of	O
a	O
single	O
shared	O
hyperparameter	B
thus	O
the	O
weight	O
prior	B
takes	O
the	O
form	O
pw	O
n	O
i	O
where	O
i	O
represents	O
the	O
precision	O
of	O
the	O
corresponding	O
parameter	O
wi	O
and	O
denotes	O
m	O
we	O
shall	O
see	O
that	O
when	O
we	O
maximize	O
the	O
evidence	O
with	O
respect	O
to	O
these	O
hyperparameters	O
a	O
significant	O
proportion	O
of	O
them	O
go	O
to	O
infinity	O
and	O
the	O
corresponding	O
weight	O
parameters	O
have	O
posterior	O
distributions	O
that	O
are	O
concentrated	O
at	O
zero	O
the	O
basis	O
functions	O
associated	O
with	O
these	O
parameters	O
therefore	O
play	O
no	O
role	O
relevance	B
vector	I
machines	O
in	O
the	O
predictions	O
made	O
by	O
the	O
model	O
and	O
so	O
are	O
effectively	O
pruned	O
out	O
resulting	O
in	O
a	O
sparse	O
model	O
using	O
the	O
result	O
for	O
linear	B
regression	B
models	O
we	O
see	O
that	O
the	O
posterior	O
distribution	O
for	O
the	O
weights	O
is	O
again	O
gaussian	B
and	O
takes	O
the	O
form	O
pwt	O
x	O
n	O
where	O
the	O
mean	B
and	O
covariance	B
are	O
given	O
by	O
m	O
tt	O
where	O
is	O
the	O
n	O
m	O
design	B
matrix	I
with	O
elements	O
ni	O
ixn	O
and	O
a	O
diag	O
i	O
note	O
that	O
in	O
the	O
specific	O
case	O
of	O
the	O
model	O
we	O
have	O
k	O
where	O
k	O
is	O
the	O
symmetric	O
kernel	O
matrix	O
with	O
elements	O
kxn	O
xm	O
a	O
t	O
the	O
values	O
of	O
and	O
are	O
determined	O
using	O
maximum	B
likelihood	I
also	O
known	O
as	O
the	O
evidence	B
approximation	I
in	O
which	O
we	O
maximize	O
the	O
marginal	B
likelihood	B
function	I
obtained	O
by	O
integrating	O
out	O
the	O
weight	O
parameters	O
ptx	O
w	O
dw	O
ptx	O
because	O
this	O
represents	O
the	O
convolution	O
of	O
two	O
gaussians	O
it	O
is	O
readily	O
evaluated	O
to	O
give	O
the	O
log	O
marginal	B
likelihood	I
in	O
the	O
form	O
ln	O
ptx	O
lnn	O
c	O
n	O
lnc	O
ttc	O
where	O
t	O
tnt	O
and	O
we	O
have	O
defined	O
the	O
n	O
n	O
matrix	O
c	O
given	O
by	O
c	O
a	O
t	O
section	O
exercise	O
our	O
goal	O
is	O
now	O
to	O
maximize	O
with	O
respect	O
to	O
the	O
hyperparameters	O
and	O
this	O
requires	O
only	O
a	O
small	O
modification	O
to	O
the	O
results	O
obtained	O
in	O
section	O
for	O
the	O
evidence	B
approximation	I
in	O
the	O
linear	B
regression	B
model	O
again	O
we	O
can	O
identify	O
two	O
approaches	O
in	O
the	O
first	O
we	O
simply	O
set	O
the	O
required	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
to	O
zero	O
and	O
obtain	O
the	O
following	O
re-estimation	O
equations	O
exercise	O
new	O
i	O
new	O
i	O
i	O
i	O
i	O
n	O
section	O
where	O
mi	O
is	O
the	O
ith	O
component	O
of	O
the	O
posterior	O
mean	B
m	O
defined	O
by	O
the	O
quantity	O
i	O
measures	O
how	O
well	O
the	O
corresponding	O
parameter	O
wi	O
is	O
determined	O
by	O
the	O
data	O
and	O
is	O
defined	O
by	O
sparse	O
kernel	O
machines	O
i	O
i	O
ii	O
in	O
which	O
ii	O
is	O
the	O
ith	O
diagonal	B
component	O
of	O
the	O
posterior	O
covariance	B
given	O
by	O
learning	B
therefore	O
proceeds	O
by	O
choosing	O
initial	O
values	O
for	O
and	O
evaluating	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
posterior	O
using	O
and	O
respectively	O
and	O
then	O
alternately	O
re-estimating	O
the	O
hyperparameters	O
using	O
and	O
and	O
re-estimating	O
the	O
posterior	O
mean	B
and	O
covariance	B
using	O
and	O
until	O
a	O
suitable	O
convergence	O
criterion	O
is	O
satisfied	O
exercise	O
section	O
exercise	O
section	O
the	O
second	O
approach	O
is	O
to	O
use	O
the	O
em	B
algorithm	I
and	O
is	O
discussed	O
in	O
section	O
these	O
two	O
approaches	O
to	O
finding	O
the	O
values	O
of	O
the	O
hyperparameters	O
that	O
maximize	O
the	O
evidence	O
are	O
formally	O
equivalent	O
numerically	O
however	O
it	O
is	O
found	O
that	O
the	O
direct	O
optimization	O
approach	O
corresponding	O
to	O
and	O
gives	O
somewhat	O
faster	O
convergence	O
as	O
a	O
result	O
of	O
the	O
optimization	O
we	O
find	O
that	O
a	O
proportion	O
of	O
the	O
hyperparameters	O
i	O
are	O
driven	O
to	O
large	O
principle	O
infinite	O
values	O
and	O
so	O
the	O
weight	O
parameters	O
wi	O
corresponding	O
to	O
these	O
hyperparameters	O
have	O
posterior	O
distributions	O
with	O
mean	B
and	O
variance	B
both	O
zero	O
thus	O
those	O
parameters	O
and	O
the	O
corresponding	O
basis	O
functions	O
ix	O
are	O
removed	O
from	O
the	O
model	O
and	O
play	O
no	O
role	O
in	O
making	O
predictions	O
for	O
new	O
inputs	O
in	O
the	O
case	O
of	O
models	O
of	O
the	O
form	O
the	O
inputs	O
xn	O
corresponding	O
to	O
the	O
remaining	O
nonzero	O
weights	O
are	O
called	O
relevance	O
vectors	O
because	O
they	O
are	O
identified	O
through	O
the	O
mechanism	O
of	O
automatic	B
relevance	I
determination	I
and	O
are	O
analogous	O
to	O
the	O
support	O
vectors	O
of	O
an	O
svm	O
it	O
is	O
worth	O
emphasizing	O
however	O
that	O
this	O
mechanism	O
for	O
achieving	O
sparsity	B
in	O
probabilistic	O
models	O
through	O
automatic	B
relevance	I
determination	I
is	O
quite	O
general	O
and	O
can	O
be	O
applied	O
to	O
any	O
model	O
expressed	O
as	O
an	O
adaptive	O
linear	O
combination	O
of	O
basis	O
functions	O
having	O
found	O
values	O
and	O
for	O
the	O
hyperparameters	O
that	O
maximize	O
the	O
marginal	B
likelihood	I
we	O
can	O
evaluate	O
the	O
predictive	B
distribution	I
over	O
t	O
for	O
a	O
new	O
input	O
x	O
using	O
and	O
this	O
is	O
given	O
by	O
ptx	O
x	O
t	O
ptx	O
w	O
t	O
dw	O
tmt	O
thus	O
the	O
predictive	O
mean	B
is	O
given	O
by	O
with	O
w	O
set	O
equal	O
to	O
the	O
posterior	O
mean	B
m	O
and	O
the	O
variance	B
of	O
the	O
predictive	B
distribution	I
is	O
given	O
by	O
where	O
is	O
given	O
by	O
in	O
which	O
and	O
are	O
set	O
to	O
their	O
optimized	O
values	O
and	O
this	O
is	O
just	O
the	O
familiar	O
result	O
obtained	O
in	O
the	O
context	O
of	O
linear	B
regression	B
recall	O
that	O
for	O
localized	O
basis	O
functions	O
the	O
predictive	O
variance	B
for	O
linear	B
regression	B
models	O
becomes	O
small	O
in	O
regions	O
of	O
input	O
space	O
where	O
there	O
are	O
no	O
basis	O
functions	O
in	O
the	O
case	O
of	O
an	O
rvm	O
with	O
the	O
basis	O
functions	O
centred	O
on	O
data	O
points	O
the	O
model	O
will	O
therefore	O
become	O
increasingly	O
certain	O
of	O
its	O
predictions	O
when	O
extrapolating	O
outside	O
the	O
domain	O
of	O
the	O
data	O
and	O
qui	O
nonero-candela	O
which	O
of	O
course	O
is	O
undesirable	O
the	O
predictive	B
distribution	I
in	O
gaussian	B
process	I
regression	B
does	O
not	O
relevance	B
vector	I
machines	O
figure	O
illustration	O
of	O
rvm	O
regression	B
using	O
the	O
same	O
data	O
set	O
and	O
the	O
same	O
gaussian	B
kernel	I
functions	O
as	O
used	O
in	O
figure	O
for	O
the	O
regression	B
model	O
the	O
mean	B
of	O
the	O
predictive	B
distribution	I
for	O
the	O
rvm	O
is	O
shown	O
by	O
the	O
red	O
line	O
and	O
the	O
one	O
standarddeviation	O
predictive	B
distribution	I
is	O
shown	O
by	O
the	O
shaded	O
region	O
also	O
the	O
data	O
points	O
are	O
shown	O
in	O
green	O
and	O
the	O
relevance	O
vectors	O
are	O
indicated	O
by	O
blue	O
circles	O
note	O
that	O
there	O
are	O
only	O
relevance	O
vectors	O
compared	O
to	O
support	O
vectors	O
for	O
the	O
in	O
figure	O
t	O
x	O
suffer	O
from	O
this	O
problem	O
however	O
the	O
computational	O
cost	O
of	O
making	O
predictions	O
with	O
a	O
gaussian	B
processes	O
is	O
typically	O
much	O
higher	O
than	O
with	O
an	O
rvm	O
figure	O
shows	O
an	O
example	O
of	O
the	O
rvm	O
applied	O
to	O
the	O
sinusoidal	O
regression	B
data	O
set	O
here	O
the	O
noise	O
precision	B
parameter	I
is	O
also	O
determined	O
through	O
evidence	O
maximization	O
we	O
see	O
that	O
the	O
number	O
of	O
relevance	O
vectors	O
in	O
the	O
rvm	O
is	O
significantly	O
smaller	O
than	O
the	O
number	O
of	O
support	O
vectors	O
used	O
by	O
the	O
svm	O
for	O
a	O
wide	O
range	O
of	O
regression	B
and	O
classification	B
tasks	O
the	O
rvm	O
is	O
found	O
to	O
give	O
models	O
that	O
are	O
typically	O
an	O
order	O
of	O
magnitude	O
more	O
compact	O
than	O
the	O
corresponding	O
support	B
vector	I
machine	I
resulting	O
in	O
a	O
significant	O
improvement	O
in	O
the	O
speed	O
of	O
processing	O
on	O
test	O
data	O
remarkably	O
this	O
greater	O
sparsity	B
is	O
achieved	O
with	O
little	O
or	O
no	O
reduction	O
in	O
generalization	B
error	B
compared	O
with	O
the	O
corresponding	O
svm	O
the	O
principal	O
disadvantage	O
of	O
the	O
rvm	O
compared	O
to	O
the	O
svm	O
is	O
that	O
training	B
involves	O
optimizing	O
a	O
nonconvex	O
function	O
and	O
training	B
times	O
can	O
be	O
longer	O
than	O
for	O
a	O
comparable	O
svm	O
for	O
a	O
model	O
with	O
m	O
basis	O
functions	O
the	O
rvm	O
requires	O
inversion	O
of	O
a	O
matrix	O
of	O
size	O
m	O
m	O
which	O
in	O
general	O
requires	O
om	O
computation	O
in	O
the	O
specific	O
case	O
of	O
the	O
svm-like	O
model	O
we	O
have	O
m	O
n	O
as	O
we	O
have	O
noted	O
there	O
are	O
techniques	O
for	O
training	B
svms	O
whose	O
cost	O
is	O
roughly	O
quadratic	O
in	O
n	O
of	O
course	O
in	O
the	O
case	O
of	O
the	O
rvm	O
we	O
always	O
have	O
the	O
option	O
of	O
starting	O
with	O
a	O
smaller	O
number	O
of	O
basis	O
functions	O
than	O
n	O
more	O
significantly	O
in	O
the	O
relevance	B
vector	I
machine	I
the	O
parameters	O
governing	O
complexity	O
and	O
noise	O
variance	B
are	O
determined	O
automatically	O
from	O
a	O
single	O
training	B
run	O
whereas	O
in	O
the	O
support	B
vector	I
machine	I
the	O
parameters	O
c	O
and	O
are	O
generally	O
found	O
using	O
cross-validation	B
which	O
involves	O
multiple	O
training	B
runs	O
furthermore	O
in	O
the	O
next	O
section	O
we	O
shall	O
derive	O
an	O
alternative	O
procedure	O
for	O
training	B
the	O
relevance	B
vector	I
machine	I
that	O
improves	O
training	B
speed	O
significantly	O
analysis	O
of	O
sparsity	B
we	O
have	O
noted	O
earlier	O
that	O
the	O
mechanism	O
of	O
automatic	B
relevance	I
determination	I
causes	O
a	O
subset	O
of	O
parameters	O
to	O
be	O
driven	O
to	O
zero	O
we	O
now	O
examine	O
in	O
more	O
detail	O
sparse	O
kernel	O
machines	O
c	O
t	O
c	O
t	O
figure	O
illustration	O
of	O
the	O
mechanism	O
for	O
sparsity	B
in	O
a	O
bayesian	B
linear	B
regression	B
model	O
showing	O
a	O
training	B
set	I
vector	O
of	O
target	O
values	O
given	O
by	O
t	O
indicated	O
by	O
the	O
cross	O
for	O
a	O
model	O
with	O
one	O
basis	O
vector	O
which	O
is	O
poorly	O
aligned	O
with	O
the	O
target	O
data	O
vector	O
t	O
on	O
the	O
left	O
we	O
see	O
a	O
model	O
having	O
only	O
isotropic	B
noise	O
so	O
that	O
c	O
corresponding	O
to	O
with	O
set	O
to	O
its	O
most	O
probable	O
value	O
on	O
the	O
right	O
we	O
see	O
the	O
same	O
model	O
but	O
with	O
a	O
finite	O
value	O
of	O
in	O
each	O
case	O
the	O
red	O
ellipse	O
corresponds	O
to	O
unit	O
mahalanobis	B
distance	I
with	O
taking	O
the	O
same	O
value	O
for	O
both	O
plots	O
while	O
the	O
dashed	O
green	O
circle	O
shows	O
the	O
contrition	O
arising	O
from	O
the	O
noise	O
term	O
we	O
see	O
that	O
any	O
finite	O
value	O
of	O
reduces	O
the	O
probability	B
of	O
the	O
observed	O
data	O
and	O
so	O
for	O
the	O
most	O
probable	O
solution	O
the	O
basis	O
vector	O
is	O
removed	O
the	O
mechanism	O
of	O
sparsity	B
in	O
the	O
context	O
of	O
the	O
relevance	B
vector	I
machine	I
in	O
the	O
process	O
we	O
will	O
arrive	O
at	O
a	O
significantly	O
faster	O
procedure	O
for	O
optimizing	O
the	O
hyperparameters	O
compared	O
to	O
the	O
direct	O
techniques	O
given	O
above	O
before	O
proceeding	O
with	O
a	O
mathematical	O
analysis	O
we	O
first	O
give	O
some	O
informal	O
insight	O
into	O
the	O
origin	O
of	O
sparsity	B
in	O
bayesian	B
linear	O
models	O
consider	O
a	O
data	O
set	O
comprising	O
n	O
observations	O
and	O
together	O
with	O
a	O
model	O
having	O
a	O
single	O
basis	B
function	I
with	O
hyperparameter	B
along	O
with	O
isotropic	B
noise	O
having	O
precision	O
from	O
the	O
marginal	B
likelihood	I
is	O
given	O
by	O
pt	O
n	O
c	O
in	O
which	O
the	O
covariance	B
matrix	O
takes	O
the	O
form	O
c	O
i	O
t	O
where	O
denotes	O
the	O
n-dimensional	O
vector	O
and	O
similarly	O
t	O
notice	O
that	O
this	O
is	O
just	O
a	O
zero-mean	O
gaussian	B
process	I
model	O
over	O
t	O
with	O
covariance	B
c	O
given	O
a	O
particular	O
observation	O
for	O
t	O
our	O
goal	O
is	O
to	O
find	O
and	O
by	O
maximizing	O
the	O
marginal	B
likelihood	I
we	O
see	O
from	O
figure	O
that	O
if	O
there	O
is	O
a	O
poor	O
alignment	O
between	O
the	O
direction	O
of	O
and	O
that	O
of	O
the	O
training	B
data	O
vector	O
t	O
then	O
the	O
corresponding	O
hyperparameter	B
will	O
be	O
driven	O
to	O
and	O
the	O
basis	O
vector	O
will	O
be	O
pruned	O
from	O
the	O
model	O
this	O
arises	O
because	O
any	O
finite	O
value	O
for	O
will	O
always	O
assign	O
a	O
lower	O
probability	B
to	O
the	O
data	O
thereby	O
decreasing	O
the	O
value	O
of	O
the	O
density	B
at	O
t	O
provided	O
that	O
is	O
set	O
to	O
its	O
optimal	O
value	O
we	O
see	O
that	O
any	O
finite	O
value	O
for	O
would	O
cause	O
the	O
distribution	O
to	O
be	O
elongated	O
in	O
a	O
direction	O
away	O
from	O
the	O
data	O
thereby	O
increasing	O
the	O
probability	B
mass	O
in	O
regions	O
away	O
from	O
the	O
observed	O
data	O
and	O
hence	O
reducing	O
the	O
value	O
of	O
the	O
density	B
at	O
the	O
target	O
data	O
vector	O
itself	O
for	O
the	O
more	O
general	O
case	O
of	O
m	O
relevance	B
vector	I
machines	O
basis	O
vectors	O
m	O
a	O
similar	O
intuition	O
holds	O
namely	O
that	O
if	O
a	O
particular	O
basis	O
vector	O
is	O
poorly	O
aligned	O
with	O
the	O
data	O
vector	O
t	O
then	O
it	O
is	O
likely	O
to	O
be	O
pruned	O
from	O
the	O
model	O
we	O
now	O
investigate	O
the	O
mechanism	O
for	O
sparsity	B
from	O
a	O
more	O
mathematical	O
perspective	O
for	O
a	O
general	O
case	O
involving	O
m	O
basis	O
functions	O
to	O
motivate	O
this	O
analysis	O
we	O
first	O
note	O
that	O
in	O
the	O
result	O
for	O
re-estimating	O
the	O
parameter	O
i	O
the	O
terms	O
on	O
the	O
right-hand	O
side	O
are	O
themselves	O
also	O
functions	O
of	O
i	O
these	O
results	O
therefore	O
represent	O
implicit	O
solutions	O
and	O
iteration	O
would	O
be	O
required	O
even	O
to	O
determine	O
a	O
single	O
i	O
with	O
all	O
other	O
j	O
for	O
j	O
i	O
fixed	O
this	O
suggests	O
a	O
different	O
approach	O
to	O
solving	O
the	O
optimization	O
problem	O
for	O
the	O
rvm	O
in	O
which	O
we	O
make	O
explicit	O
all	O
of	O
the	O
dependence	O
of	O
the	O
marginal	B
likelihood	I
on	O
a	O
particular	O
i	O
and	O
then	O
determine	O
its	O
stationary	B
points	O
explicitly	O
and	O
tipping	O
tipping	O
and	O
faul	O
to	O
do	O
this	O
we	O
first	O
pull	O
out	O
the	O
contribution	O
from	O
i	O
in	O
the	O
matrix	O
c	O
defined	O
by	O
to	O
give	O
c	O
j	O
j	O
t	O
j	O
c	O
i	O
i	O
i	O
t	O
i	O
i	O
i	O
t	O
i	O
where	O
i	O
denotes	O
the	O
ith	O
column	O
of	O
in	O
other	O
words	O
the	O
n-dimensional	O
vector	O
with	O
elements	O
ixn	O
in	O
contrast	O
to	O
n	O
which	O
denotes	O
the	O
nth	O
row	O
of	O
the	O
matrix	O
c	O
i	O
represents	O
the	O
matrix	O
c	O
with	O
the	O
contribution	O
from	O
basis	B
function	I
i	O
removed	O
using	O
the	O
matrix	O
identities	O
and	O
the	O
determinant	O
and	O
inverse	B
of	O
c	O
can	O
then	O
be	O
written	O
i	O
c	O
c	O
c	O
i	O
i	O
i	O
t	O
i	O
c	O
i	O
i	O
t	O
i	O
i	O
c	O
i	O
i	O
i	O
t	O
i	O
c	O
using	O
these	O
results	O
we	O
can	O
then	O
write	O
the	O
log	O
marginal	B
likelihood	B
function	I
in	O
the	O
form	O
where	O
l	O
i	O
is	O
simply	O
the	O
log	O
marginal	B
likelihood	I
with	O
basis	B
function	I
i	O
omitted	O
and	O
the	O
quantity	O
i	O
is	O
defined	O
by	O
l	O
l	O
i	O
i	O
i	O
ln	O
i	O
ln	O
i	O
si	O
i	O
i	O
si	O
and	O
contains	O
all	O
of	O
the	O
dependence	O
on	O
i	O
here	O
we	O
have	O
introduced	O
the	O
two	O
quantities	O
si	O
t	O
qi	O
t	O
i	O
c	O
i	O
c	O
i	O
i	O
i	O
t	O
here	O
si	O
is	O
called	O
the	O
sparsity	B
and	O
qi	O
is	O
known	O
as	O
the	O
quality	O
of	O
i	O
and	O
as	O
we	O
shall	O
see	O
a	O
large	O
value	O
of	O
si	O
relative	B
to	O
the	O
value	O
of	O
qi	O
means	O
that	O
the	O
basis	B
function	I
i	O
exercise	O
sparse	O
kernel	O
machines	O
of	O
the	O
figure	O
plots	O
log	O
likelihood	O
i	O
versus	O
marginal	B
ln	O
i	O
showing	O
on	O
the	O
left	O
the	O
single	O
maximum	O
at	O
a	O
finite	O
i	O
for	O
i	O
and	O
si	O
that	O
i	O
si	O
and	O
on	O
the	O
right	O
the	O
maximum	O
at	O
i	O
for	O
i	O
and	O
si	O
that	O
i	O
si	O
is	O
more	O
likely	O
to	O
be	O
pruned	O
from	O
the	O
model	O
the	O
sparsity	B
measures	O
the	O
extent	O
to	O
which	O
basis	B
function	I
i	O
overlaps	O
with	O
the	O
other	O
basis	O
vectors	O
in	O
the	O
model	O
and	O
the	O
quality	O
represents	O
a	O
measure	O
of	O
the	O
alignment	O
of	O
the	O
basis	O
vector	O
n	O
with	O
the	O
error	B
between	O
the	O
training	B
set	I
values	O
t	O
tnt	O
and	O
the	O
vector	O
y	O
i	O
of	O
predictions	O
that	O
would	O
result	O
from	O
the	O
model	O
with	O
the	O
vector	O
i	O
excluded	O
and	O
faul	O
the	O
stationary	B
points	O
of	O
the	O
marginal	B
likelihood	I
with	O
respect	O
to	O
i	O
occur	O
when	O
the	O
derivative	B
d	O
i	O
i	O
i	O
i	O
i	O
si	O
is	O
equal	O
to	O
zero	O
there	O
are	O
two	O
possible	O
forms	O
for	O
the	O
solution	O
recalling	O
that	O
i	O
we	O
see	O
that	O
if	O
i	O
si	O
we	O
can	O
solve	O
for	O
i	O
to	O
obtain	O
i	O
si	O
then	O
i	O
provides	O
a	O
solution	O
conversely	O
if	O
d	O
i	O
i	O
i	O
i	O
si	O
exercise	O
these	O
two	O
solutions	O
are	O
illustrated	O
in	O
figure	O
we	O
see	O
that	O
the	O
relative	B
size	O
of	O
the	O
quality	O
and	O
sparsity	B
terms	O
determines	O
whether	O
a	O
particular	O
basis	O
vector	O
will	O
be	O
pruned	O
from	O
the	O
model	O
or	O
not	O
a	O
more	O
complete	O
analysis	O
and	O
tipping	O
based	O
on	O
the	O
second	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
confirms	O
these	O
solutions	O
are	O
indeed	O
the	O
unique	O
maxima	O
of	O
i	O
note	O
that	O
this	O
approach	O
has	O
yielded	O
a	O
closed-form	O
solution	O
for	O
i	O
for	O
given	O
values	O
of	O
the	O
other	O
hyperparameters	O
as	O
well	O
as	O
providing	O
insight	O
into	O
the	O
origin	O
of	O
sparsity	B
in	O
the	O
rvm	O
this	O
analysis	O
also	O
leads	O
to	O
a	O
practical	O
algorithm	O
for	O
optimizing	O
the	O
hyperparameters	O
that	O
has	O
significant	O
speed	O
advantages	O
this	O
uses	O
a	O
fixed	O
set	O
of	O
candidate	O
basis	O
vectors	O
and	O
then	O
cycles	O
through	O
them	O
in	O
turn	O
to	O
decide	O
whether	O
each	O
vector	O
should	O
be	O
included	O
in	O
the	O
model	O
or	O
not	O
the	O
resulting	O
sequential	O
sparse	O
bayesian	B
learning	B
algorithm	O
is	O
described	O
below	O
sequential	O
sparse	O
bayesian	B
learning	B
algorithm	O
if	O
solving	O
a	O
regression	B
problem	O
initialize	O
initialize	O
using	O
one	O
basis	B
function	I
with	O
hyperparameter	B
set	O
using	O
with	O
the	O
remaining	O
hyperparameters	O
j	O
for	O
j	O
i	O
initialized	O
to	O
infinity	O
so	O
that	O
only	O
is	O
included	O
in	O
the	O
model	O
relevance	B
vector	I
machines	O
evaluate	O
and	O
m	O
along	O
with	O
qi	O
and	O
si	O
for	O
all	O
basis	O
functions	O
select	O
a	O
candidate	O
basis	B
function	I
i	O
if	O
i	O
si	O
and	O
i	O
so	O
that	O
the	O
basis	O
vector	O
i	O
is	O
already	O
included	O
in	O
i	O
si	O
and	O
i	O
then	O
add	O
i	O
to	O
the	O
model	O
and	O
evaluate	O
hyperpai	O
si	O
and	O
i	O
then	O
remove	O
basis	B
function	I
i	O
from	O
the	O
model	O
the	O
model	O
then	O
update	O
i	O
using	O
if	O
if	O
rameter	O
i	O
using	O
and	O
set	O
i	O
if	O
solving	O
a	O
regression	B
problem	O
update	O
if	O
converged	O
terminate	O
otherwise	O
go	O
to	O
note	O
that	O
if	O
from	O
the	O
model	O
and	O
no	O
action	O
is	O
required	O
i	O
si	O
and	O
i	O
then	O
the	O
basis	B
function	I
i	O
is	O
already	O
excluded	O
in	O
practice	O
it	O
is	O
convenient	O
to	O
evaluate	O
the	O
quantities	O
qi	O
t	O
si	O
t	O
i	O
c	O
i	O
c	O
i	O
the	O
quality	O
and	O
sparseness	O
variables	O
can	O
then	O
be	O
expressed	O
in	O
the	O
form	O
qi	O
iqi	O
i	O
si	O
isi	O
i	O
si	O
note	O
that	O
when	O
i	O
we	O
have	O
qi	O
qi	O
and	O
si	O
si	O
using	O
we	O
can	O
write	O
si	O
qi	O
t	O
si	O
t	O
i	O
t	O
t	O
i	O
i	O
t	O
i	O
tt	O
i	O
t	O
i	O
where	O
and	O
involve	O
only	O
those	O
basis	O
vectors	O
that	O
correspond	O
to	O
finite	O
hyperparameters	O
i	O
at	O
each	O
stage	O
the	O
required	O
computations	O
therefore	O
scale	O
like	O
om	O
where	O
m	O
is	O
the	O
number	O
of	O
active	O
basis	O
vectors	O
in	O
the	O
model	O
and	O
is	O
typically	O
much	O
smaller	O
than	O
the	O
number	O
n	O
of	O
training	B
patterns	O
rvm	O
for	O
classification	B
we	O
can	O
extend	O
the	O
relevance	B
vector	I
machine	I
framework	O
to	O
classification	B
problems	O
by	O
applying	O
the	O
ard	O
prior	B
over	O
weights	O
to	O
a	O
probabilistic	O
linear	O
classification	B
model	O
of	O
the	O
kind	O
studied	O
in	O
chapter	O
to	O
start	O
with	O
we	O
consider	O
two-class	O
problems	O
with	O
a	O
binary	O
target	O
variable	O
t	O
the	O
model	O
now	O
takes	O
the	O
form	O
of	O
a	O
linear	O
combination	O
of	O
basis	O
functions	O
transformed	O
by	O
a	O
logistic	B
sigmoid	I
function	O
yx	O
w	O
wt	O
exercise	O
sparse	O
kernel	O
machines	O
section	O
exercise	O
where	O
is	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
if	O
we	O
introduce	O
a	O
gaussian	B
prior	B
over	O
the	O
weight	B
vector	I
w	O
then	O
we	O
obtain	O
the	O
model	O
that	O
has	O
been	O
considered	O
already	O
in	O
chapter	O
the	O
difference	O
here	O
is	O
that	O
in	O
the	O
rvm	O
this	O
model	O
uses	O
the	O
ard	O
prior	B
in	O
which	O
there	O
is	O
a	O
separate	O
precision	O
hyperparameter	B
associated	O
with	O
each	O
weight	B
parameter	I
in	O
contrast	O
to	O
the	O
regression	B
model	O
we	O
can	O
no	O
longer	O
integrate	O
analytically	O
over	O
the	O
parameter	O
vector	O
w	O
here	O
we	O
follow	O
tipping	O
and	O
use	O
the	O
laplace	B
approximation	I
which	O
was	O
applied	O
to	O
the	O
closely	O
related	O
problem	O
of	O
bayesian	B
logistic	B
regression	B
in	O
section	O
we	O
begin	O
by	O
initializing	O
the	O
hyperparameter	B
vector	O
for	O
this	O
given	O
value	O
of	O
we	O
then	O
build	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
and	O
thereby	O
obtain	O
an	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
maximization	O
of	O
this	O
approximate	O
marginal	B
likelihood	I
then	O
leads	O
to	O
a	O
re-estimated	O
value	O
for	O
and	O
the	O
process	O
is	O
repeated	O
until	O
convergence	O
let	O
us	O
consider	O
the	O
laplace	B
approximation	I
for	O
this	O
model	O
in	O
more	O
detail	O
for	O
a	O
fixed	O
value	O
of	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
over	O
w	O
is	O
obtained	O
by	O
maximizing	O
ln	O
pwt	O
lnptwpw	O
ln	O
pt	O
ln	O
yn	O
tn	O
yn	O
wtaw	O
const	O
where	O
a	O
diag	O
i	O
this	O
can	O
be	O
done	O
using	O
iterative	B
reweighted	I
least	I
squares	I
as	O
discussed	O
in	O
section	O
for	O
this	O
we	O
need	O
the	O
gradient	O
vector	O
and	O
hessian	B
matrix	I
of	O
the	O
log	O
posterior	O
distribution	O
which	O
from	O
are	O
given	O
by	O
ln	O
pwt	O
ln	O
pwt	O
tt	O
y	O
aw	O
tb	O
a	O
where	O
b	O
is	O
an	O
n	O
n	O
diagonal	B
matrix	O
with	O
elements	O
bn	O
yn	O
the	O
vector	O
y	O
ynt	O
and	O
is	O
the	O
design	B
matrix	I
with	O
elements	O
ni	O
ixn	O
here	O
we	O
have	O
used	O
the	O
property	O
for	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
function	O
at	O
convergence	O
of	O
the	O
irls	O
algorithm	O
the	O
negative	O
hessian	O
represents	O
the	O
inverse	B
covariance	B
matrix	O
for	O
the	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
the	O
mode	O
of	O
the	O
resulting	O
approximation	O
to	O
the	O
posterior	O
distribution	O
corresponding	O
to	O
the	O
mean	B
of	O
the	O
gaussian	B
approximation	O
is	O
obtained	O
setting	O
to	O
zero	O
giving	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
laplace	B
approximation	I
in	O
the	O
form	O
a	O
tt	O
y	O
tb	O
a	O
we	O
can	O
now	O
use	O
this	O
laplace	B
approximation	I
to	O
evaluate	O
the	O
marginal	B
likelihood	I
using	O
the	O
general	O
result	O
for	O
an	O
integral	O
evaluated	O
using	O
the	O
laplace	B
approxi	O
relevance	B
vector	I
machines	O
mation	O
we	O
have	O
ptwpw	O
dw	O
pt	O
if	O
we	O
substitute	O
for	O
and	O
and	O
then	O
set	O
the	O
derivative	B
of	O
the	O
marginal	B
likelihood	I
with	O
respect	O
to	O
i	O
equal	O
to	O
zero	O
we	O
obtain	O
exercise	O
i	O
i	O
ii	O
defining	O
i	O
i	O
ii	O
and	O
rearranging	O
then	O
gives	O
i	O
i	O
new	O
i	O
which	O
is	O
identical	O
to	O
the	O
re-estimation	O
formula	O
obtained	O
for	O
the	O
regression	B
rvm	O
if	O
we	O
define	O
we	O
can	O
write	O
the	O
approximate	O
log	O
marginal	B
likelihood	I
in	O
the	O
form	O
b	O
y	O
n	O
lnc	O
ln	O
pt	O
where	O
c	O
b	O
a	O
t	O
appendix	O
a	O
section	O
this	O
takes	O
the	O
same	O
form	O
as	O
in	O
the	O
regression	B
case	O
and	O
so	O
we	O
can	O
apply	O
the	O
same	O
analysis	O
of	O
sparsity	B
and	O
obtain	O
the	O
same	O
fast	O
learning	B
algorithm	O
in	O
which	O
we	O
fully	O
optimize	O
a	O
single	O
hyperparameter	B
i	O
at	O
each	O
step	O
figure	O
shows	O
the	O
relevance	B
vector	I
machine	I
applied	O
to	O
a	O
synthetic	O
classification	B
data	O
set	O
we	O
see	O
that	O
the	O
relevance	O
vectors	O
tend	O
not	O
to	O
lie	O
in	O
the	O
region	O
of	O
the	O
decision	B
boundary	I
in	O
contrast	O
to	O
the	O
support	B
vector	I
machine	I
this	O
is	O
consistent	B
with	O
our	O
earlier	O
discussion	O
of	O
sparsity	B
in	O
the	O
rvm	O
because	O
a	O
basis	B
function	I
ix	O
centred	O
on	O
a	O
data	O
point	O
near	O
the	O
boundary	O
will	O
have	O
a	O
vector	O
i	O
that	O
is	O
poorly	O
aligned	O
with	O
the	O
training	B
data	O
vector	O
t	O
one	O
of	O
the	O
potential	O
advantages	O
of	O
the	O
relevance	B
vector	I
machine	I
compared	O
with	O
the	O
svm	O
is	O
that	O
it	O
makes	O
probabilistic	O
predictions	O
for	O
example	O
this	O
allows	O
the	O
rvm	O
to	O
be	O
used	O
to	O
help	O
construct	O
an	O
emission	O
density	B
in	O
a	O
nonlinear	O
extension	O
of	O
the	O
linear	B
dynamical	B
system	I
for	O
tracking	O
faces	O
in	O
video	O
sequences	O
et	O
al	O
so	O
far	O
we	O
have	O
considered	O
the	O
rvm	O
for	O
binary	O
classification	B
problems	O
for	O
k	O
classes	O
we	O
again	O
make	O
use	O
of	O
the	O
probabilistic	O
approach	O
in	O
section	O
in	O
which	O
there	O
are	O
k	O
linear	O
models	O
of	O
the	O
form	O
ak	O
wt	O
k	O
x	O
sparse	O
kernel	O
machines	O
figure	O
example	O
of	O
the	O
relevance	B
vector	I
machine	I
applied	O
to	O
a	O
synthetic	O
data	O
set	O
in	O
which	O
the	O
left-hand	O
plot	O
shows	O
the	O
decision	B
boundary	I
and	O
the	O
data	O
points	O
with	O
the	O
relevance	O
vectors	O
indicated	O
by	O
circles	O
comparison	O
with	O
the	O
results	O
shown	O
in	O
figure	O
for	O
the	O
corresponding	O
support	B
vector	I
machine	I
shows	O
that	O
the	O
rvm	O
gives	O
a	O
much	O
sparser	O
model	O
the	O
right-hand	O
plot	O
shows	O
the	O
posterior	B
probability	B
given	O
by	O
the	O
rvm	O
output	O
in	O
which	O
the	O
proportion	O
of	O
red	O
ink	O
indicates	O
the	O
probability	B
of	O
that	O
point	O
belonging	O
to	O
the	O
red	O
class	O
which	O
are	O
combined	O
using	O
a	O
softmax	B
function	I
to	O
give	O
outputs	O
expaj	O
ykx	O
the	O
log	O
likelihood	B
function	I
is	O
then	O
given	O
by	O
j	O
ln	O
wk	O
ytnk	O
nk	O
where	O
the	O
target	O
values	O
tnk	O
have	O
a	O
coding	O
for	O
each	O
data	O
point	O
n	O
and	O
t	O
is	O
a	O
matrix	O
with	O
elements	O
tnk	O
again	O
the	O
laplace	B
approximation	I
can	O
be	O
used	O
to	O
optimize	O
the	O
hyperparameters	O
in	O
which	O
the	O
model	O
and	O
its	O
hessian	O
are	O
found	O
using	O
irls	O
this	O
gives	O
a	O
more	O
principled	O
approach	O
to	O
multiclass	B
classification	B
than	O
the	O
pairwise	O
method	O
used	O
in	O
the	O
support	B
vector	I
machine	I
and	O
also	O
provides	O
probabilistic	O
predictions	O
for	O
new	O
data	O
points	O
the	O
principal	O
disadvantage	O
is	O
that	O
the	O
hessian	B
matrix	I
has	O
size	O
m	O
k	O
m	O
k	O
where	O
m	O
is	O
the	O
number	O
of	O
active	O
basis	O
functions	O
which	O
gives	O
an	O
additional	O
factor	O
of	O
k	O
in	O
the	O
computational	O
cost	O
of	O
training	B
compared	O
with	O
the	O
two-class	O
rvm	O
the	O
principal	O
disadvantage	O
of	O
the	O
relevance	B
vector	I
machine	I
is	O
the	O
relatively	O
long	O
training	B
times	O
compared	O
with	O
the	O
svm	O
this	O
is	O
offset	O
however	O
by	O
the	O
avoidance	O
of	O
cross-validation	B
runs	O
to	O
set	O
the	O
model	O
complexity	O
parameters	O
furthermore	O
because	O
it	O
yields	O
sparser	O
models	O
the	O
computation	O
time	O
on	O
test	O
points	O
which	O
is	O
usually	O
the	O
more	O
important	O
consideration	O
in	O
practice	O
is	O
typically	O
much	O
less	O
exercises	O
exercises	O
www	O
suppose	O
we	O
have	O
a	O
data	O
set	O
of	O
input	O
vectors	O
with	O
corresponding	O
target	O
values	O
tn	O
and	O
suppose	O
that	O
we	O
model	O
the	O
density	B
of	O
input	O
vectors	O
within	O
each	O
class	O
separately	O
using	O
a	O
parzen	O
kernel	B
density	B
estimator	I
section	O
with	O
a	O
kernel	O
kx	O
write	O
down	O
the	O
minimum	O
misclassification-rate	O
decision	O
rule	O
assuming	O
the	O
two	O
classes	O
have	O
equal	O
prior	B
probability	B
show	O
also	O
that	O
if	O
the	O
kernel	O
is	O
chosen	O
to	O
be	O
kx	O
then	O
the	O
classification	B
rule	O
reduces	O
to	O
simply	O
assigning	O
a	O
new	O
input	O
vector	O
to	O
the	O
class	O
having	O
the	O
closest	O
mean	B
finally	O
show	O
that	O
if	O
the	O
kernel	O
takes	O
the	O
form	O
kx	O
that	O
the	O
classification	B
is	O
based	O
on	O
the	O
closest	O
mean	B
in	O
the	O
feature	B
space	I
show	O
that	O
if	O
the	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
constraint	O
is	O
replaced	O
by	O
some	O
arbitrary	O
constant	O
the	O
solution	O
for	O
the	O
maximum	O
margin	B
hyperplane	O
is	O
unchanged	O
show	O
that	O
irrespective	O
of	O
the	O
dimensionality	O
of	O
the	O
data	O
space	O
a	O
data	O
set	O
consisting	O
of	O
just	O
two	O
data	O
points	O
one	O
from	O
each	O
class	O
is	O
sufficient	O
to	O
determine	O
the	O
location	O
of	O
the	O
maximum-margin	O
hyperplane	O
www	O
show	O
that	O
the	O
value	O
of	O
the	O
margin	B
for	O
the	O
maximum-margin	O
hyper	O
plane	O
is	O
given	O
by	O
where	O
are	O
given	O
by	O
maximizing	O
subject	O
to	O
the	O
constraints	O
and	O
an	O
show	O
that	O
the	O
values	O
of	O
and	O
in	O
the	O
previous	O
exercise	O
also	O
satisfy	O
is	O
defined	O
by	O
similarly	O
show	O
that	O
consider	O
the	O
logistic	B
regression	B
model	O
with	O
a	O
target	O
variable	O
t	O
if	O
we	O
define	O
pt	O
where	O
yx	O
is	O
given	O
by	O
show	O
that	O
the	O
negative	O
log	O
likelihood	O
with	O
the	O
addition	O
of	O
a	O
quadratic	O
regularization	B
term	O
takes	O
the	O
form	O
consider	O
the	O
lagrangian	B
for	O
the	O
regression	B
support	B
vector	I
machine	I
by	O
setting	O
the	O
derivatives	O
of	O
the	O
lagrangian	B
with	O
respect	O
to	O
w	O
b	O
n	O
n	O
to	O
zero	O
and	O
then	O
back	O
substituting	O
to	O
eliminate	O
the	O
corresponding	O
variables	O
show	O
that	O
the	O
dual	O
lagrangian	B
is	O
given	O
by	O
sparse	O
kernel	O
machines	O
www	O
for	O
the	O
regression	B
support	B
vector	I
machine	I
considered	O
in	O
section	O
show	O
that	O
all	O
training	B
data	O
points	O
for	O
which	O
n	O
will	O
have	O
an	O
c	O
and	O
similarly	O
all	O
points	O
for	O
n	O
will	O
c	O
verify	O
the	O
results	O
and	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
posterior	O
distribution	O
over	O
weights	O
in	O
the	O
regression	B
rvm	O
www	O
derive	O
the	O
result	O
for	O
the	O
marginal	B
likelihood	B
function	I
in	O
the	O
regression	B
rvm	O
by	O
performing	O
the	O
gaussian	B
integral	O
over	O
w	O
in	O
using	O
the	O
technique	O
of	O
completing	B
the	I
square	I
in	O
the	O
exponential	O
repeat	O
the	O
above	O
exercise	O
but	O
this	O
time	O
make	O
use	O
of	O
the	O
general	O
result	O
www	O
show	O
that	O
direct	O
maximization	O
of	O
the	O
log	O
marginal	B
likelihood	I
for	O
the	O
regression	B
relevance	B
vector	I
machine	I
leads	O
to	O
the	O
re-estimation	O
equations	O
and	O
where	O
i	O
is	O
defined	O
by	O
in	O
the	O
evidence	O
framework	O
for	O
rvm	O
regression	B
we	O
obtained	O
the	O
re-estimation	O
formulae	O
and	O
by	O
maximizing	O
the	O
marginal	B
likelihood	I
given	O
by	O
extend	O
this	O
approach	O
by	O
inclusion	O
of	O
hyperpriors	O
given	O
by	O
gamma	O
distributions	O
of	O
the	O
form	O
and	O
obtain	O
the	O
corresponding	O
re-estimation	O
formulae	O
for	O
and	O
by	O
maximizing	O
the	O
corresponding	O
posterior	B
probability	B
pt	O
with	O
respect	O
to	O
and	O
derive	O
the	O
result	O
for	O
the	O
predictive	B
distribution	I
in	O
the	O
relevance	B
vector	I
machine	I
for	B
regression	B
show	O
that	O
the	O
predictive	O
variance	B
is	O
given	O
by	O
www	O
using	O
the	O
results	O
and	O
show	O
that	O
the	O
marginal	B
likelihood	I
can	O
be	O
written	O
in	O
the	O
form	O
where	O
n	O
is	O
defined	O
by	O
and	O
the	O
sparsity	B
and	O
quality	O
factors	O
are	O
defined	O
by	O
and	O
respectively	O
by	O
taking	O
the	O
second	O
derivative	B
of	O
the	O
log	O
marginal	B
likelihood	I
for	O
the	O
regression	B
rvm	O
with	O
respect	O
to	O
the	O
hyperparameter	B
i	O
show	O
that	O
the	O
stationary	B
point	O
given	O
by	O
is	O
a	O
maximum	O
of	O
the	O
marginal	B
likelihood	I
using	O
and	O
together	O
with	O
the	O
matrix	O
identity	O
show	O
that	O
the	O
quantities	O
sn	O
and	O
qn	O
defined	O
by	O
and	O
can	O
be	O
written	O
in	O
the	O
form	O
and	O
www	O
show	O
that	O
the	O
gradient	O
vector	O
and	O
hessian	B
matrix	I
of	O
the	O
log	O
posterior	O
distribution	O
for	O
the	O
classification	B
relevance	B
vector	I
machine	I
are	O
given	O
by	O
and	O
verify	O
that	O
maximization	O
of	O
the	O
approximate	O
log	O
marginal	B
likelihood	B
function	I
for	O
the	O
classification	B
relevance	B
vector	I
machine	I
leads	O
to	O
the	O
result	O
for	O
re-estimation	O
of	O
the	O
hyperparameters	O
graphical	O
models	O
probabilities	O
play	O
a	O
central	O
role	O
in	O
modern	O
pattern	O
recognition	O
we	O
have	O
seen	O
in	O
chapter	O
that	O
probability	B
theory	B
can	O
be	O
expressed	O
in	O
terms	O
of	O
two	O
simple	O
equations	O
corresponding	O
to	O
the	O
sum	B
rule	I
and	O
the	O
product	B
rule	I
all	O
of	O
the	O
probabilistic	O
inference	B
and	O
learning	B
manipulations	O
discussed	O
in	O
this	O
book	O
no	O
matter	O
how	O
complex	O
amount	O
to	O
repeated	O
application	O
of	O
these	O
two	O
equations	O
we	O
could	O
therefore	O
proceed	O
to	O
formulate	O
and	O
solve	O
complicated	O
probabilistic	O
models	O
purely	O
by	O
algebraic	O
manipulation	O
however	O
we	O
shall	O
find	O
it	O
highly	O
advantageous	O
to	O
augment	O
the	O
analysis	O
using	O
diagrammatic	O
representations	O
of	O
probability	B
distributions	O
called	O
probabilistic	O
graphical	O
models	O
these	O
offer	O
several	O
useful	O
properties	O
they	O
provide	O
a	O
simple	O
way	O
to	O
visualize	O
the	O
structure	O
of	O
a	O
probabilistic	O
model	O
and	O
can	O
be	O
used	O
to	O
design	O
and	O
motivate	O
new	O
models	O
insights	O
into	O
the	O
properties	O
of	O
the	O
model	O
including	O
conditional	B
independence	I
properties	O
can	O
be	O
obtained	O
by	O
inspection	O
of	O
the	O
graph	O
graphical	O
models	O
complex	O
computations	O
required	O
to	O
perform	O
inference	B
and	O
learning	B
in	O
sophisticated	O
models	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
graphical	O
manipulations	O
in	O
which	O
underlying	O
mathematical	O
expressions	O
are	O
carried	O
along	O
implicitly	O
a	O
graph	O
comprises	O
nodes	O
called	O
vertices	O
connected	O
by	O
links	O
known	O
as	O
edges	O
or	O
arcs	O
in	O
a	O
probabilistic	O
graphical	B
model	I
each	O
node	B
represents	O
a	O
random	O
variable	O
group	O
of	O
random	O
variables	O
and	O
the	O
links	O
express	O
probabilistic	O
relationships	O
between	O
these	O
variables	O
the	O
graph	O
then	O
captures	O
the	O
way	O
in	O
which	O
the	O
joint	O
distribution	O
over	O
all	O
of	O
the	O
random	O
variables	O
can	O
be	O
decomposed	O
into	O
a	O
product	O
of	O
factors	O
each	O
depending	O
only	O
on	O
a	O
subset	O
of	O
the	O
variables	O
we	O
shall	O
begin	O
by	O
discussing	O
bayesian	B
networks	O
also	O
known	O
as	O
directed	B
graphical	O
models	O
in	O
which	O
the	O
links	O
of	O
the	O
graphs	O
have	O
a	O
particular	O
directionality	O
indicated	O
by	O
arrows	O
the	O
other	O
major	O
class	O
of	O
graphical	O
models	O
are	O
markov	O
random	O
fields	O
also	O
known	O
as	O
undirected	B
graphical	O
models	O
in	O
which	O
the	O
links	O
do	O
not	O
carry	O
arrows	O
and	O
have	O
no	O
directional	O
significance	O
directed	B
graphs	O
are	O
useful	O
for	O
expressing	O
causal	O
relationships	O
between	O
random	O
variables	O
whereas	O
undirected	B
graphs	O
are	O
better	O
suited	O
to	O
expressing	O
soft	B
constraints	O
between	O
random	O
variables	O
for	O
the	O
purposes	O
of	O
solving	O
inference	B
problems	O
it	O
is	O
often	O
convenient	O
to	O
convert	O
both	O
directed	B
and	O
undirected	B
graphs	O
into	O
a	O
different	O
representation	O
called	O
a	O
factor	B
graph	I
in	O
this	O
chapter	O
we	O
shall	O
focus	O
on	O
the	O
key	O
aspects	O
of	O
graphical	O
models	O
as	O
needed	O
for	O
applications	O
in	O
pattern	O
recognition	O
and	O
machine	O
learning	B
more	O
general	O
treatments	O
of	O
graphical	O
models	O
can	O
be	O
found	O
in	O
the	O
books	O
by	O
whittaker	O
lauritzen	O
jensen	O
castillo	O
et	O
al	O
jordan	O
cowell	O
et	O
al	O
and	O
jordan	O
bayesian	B
networks	O
in	O
order	O
to	O
motivate	O
the	O
use	O
of	O
directed	B
graphs	O
to	O
describe	O
probability	B
distributions	O
consider	O
first	O
an	O
arbitrary	O
joint	O
distribution	O
pa	O
b	O
c	O
over	O
three	O
variables	O
a	O
b	O
and	O
c	O
note	O
that	O
at	O
this	O
stage	O
we	O
do	O
not	O
need	O
to	O
specify	O
anything	O
further	O
about	O
these	O
variables	O
such	O
as	O
whether	O
they	O
are	O
discrete	O
or	O
continuous	O
indeed	O
one	O
of	O
the	O
powerful	O
aspects	O
of	O
graphical	O
models	O
is	O
that	O
a	O
specific	O
graph	O
can	O
make	O
probabilistic	O
statements	O
for	O
a	O
broad	O
class	O
of	O
distributions	O
by	O
application	O
of	O
the	O
product	B
rule	I
of	I
probability	B
we	O
can	O
write	O
the	O
joint	O
distribution	O
in	O
the	O
form	O
pa	O
b	O
c	O
pca	O
bpa	O
b	O
a	O
second	O
application	O
of	O
the	O
product	B
rule	I
this	O
time	O
to	O
the	O
second	O
term	O
on	O
the	O
righthand	O
side	O
of	O
gives	O
pa	O
b	O
c	O
pca	O
bpbapa	O
note	O
that	O
this	O
decomposition	O
holds	O
for	O
any	O
choice	O
of	O
the	O
joint	O
distribution	O
we	O
now	O
represent	O
the	O
right-hand	O
side	O
of	O
in	O
terms	O
of	O
a	O
simple	O
graphical	B
model	I
as	O
follows	O
first	O
we	O
introduce	O
a	O
node	B
for	O
each	O
of	O
the	O
random	O
variables	O
a	O
b	O
and	O
c	O
and	O
associate	O
each	O
node	B
with	O
the	O
corresponding	O
conditional	B
distribution	O
on	O
the	O
right-hand	O
side	O
of	O
figure	O
a	O
directed	B
graphical	B
model	I
representing	O
the	O
joint	O
probability	B
distribution	O
over	O
three	O
variables	O
a	O
b	O
and	O
c	O
corresponding	O
to	O
the	O
decomposition	O
on	O
the	O
right-hand	O
side	O
of	O
a	O
b	O
bayesian	B
networks	O
c	O
then	O
for	O
each	O
conditional	B
distribution	O
we	O
add	O
directed	B
links	O
to	O
the	O
graph	O
from	O
the	O
nodes	O
corresponding	O
to	O
the	O
variables	O
on	O
which	O
the	O
distribution	O
is	O
conditioned	O
thus	O
for	O
the	O
factor	O
pca	O
b	O
there	O
will	O
be	O
links	O
from	O
nodes	O
a	O
and	O
b	O
to	O
node	B
c	O
whereas	O
for	O
the	O
factor	O
pa	O
there	O
will	O
be	O
no	O
incoming	O
links	O
the	O
result	O
is	O
the	O
graph	O
shown	O
in	O
figure	O
if	O
there	O
is	O
a	O
link	B
going	O
from	O
a	O
node	B
a	O
to	O
a	O
node	B
b	O
then	O
we	O
say	O
that	O
node	B
a	O
is	O
the	O
parent	O
of	O
node	B
b	O
and	O
we	O
say	O
that	O
node	B
b	O
is	O
the	O
child	O
of	O
node	B
a	O
note	O
that	O
we	O
shall	O
not	O
make	O
any	O
formal	O
distinction	O
between	O
a	O
node	B
and	O
the	O
variable	O
to	O
which	O
it	O
corresponds	O
but	O
will	O
simply	O
use	O
the	O
same	O
symbol	O
to	O
refer	O
to	O
both	O
an	O
interesting	O
point	O
to	O
note	O
about	O
is	O
that	O
the	O
left-hand	O
side	O
is	O
symmetrical	O
with	O
respect	O
to	O
the	O
three	O
variables	O
a	O
b	O
and	O
c	O
whereas	O
the	O
right-hand	O
side	O
is	O
not	O
indeed	O
in	O
making	O
the	O
decomposition	O
in	O
we	O
have	O
implicitly	O
chosen	O
a	O
particular	O
ordering	O
namely	O
a	O
b	O
c	O
and	O
had	O
we	O
chosen	O
a	O
different	O
ordering	O
we	O
would	O
have	O
obtained	O
a	O
different	O
decomposition	O
and	O
hence	O
a	O
different	O
graphical	O
representation	O
we	O
shall	O
return	O
to	O
this	O
point	O
later	O
for	O
the	O
moment	O
let	O
us	O
extend	O
the	O
example	O
of	O
figure	O
by	O
considering	O
the	O
joint	O
distribution	O
over	O
k	O
variables	O
given	O
by	O
xk	O
by	O
repeated	O
application	O
of	O
the	O
product	B
rule	I
of	I
probability	B
this	O
joint	O
distribution	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
conditional	B
distributions	O
one	O
for	O
each	O
of	O
the	O
variables	O
xk	O
xk	O
for	O
a	O
given	O
choice	O
of	O
k	O
we	O
can	O
again	O
represent	O
this	O
as	O
a	O
directed	B
graph	O
having	O
k	O
nodes	O
one	O
for	O
each	O
conditional	B
distribution	O
on	O
the	O
right-hand	O
side	O
of	O
with	O
each	O
node	B
having	O
incoming	O
links	O
from	O
all	O
lower	O
numbered	O
nodes	O
we	O
say	O
that	O
this	O
graph	O
is	O
fully	B
connected	I
because	O
there	O
is	O
a	O
link	B
between	O
every	O
pair	O
of	O
nodes	O
so	O
far	O
we	O
have	O
worked	O
with	O
completely	O
general	O
joint	O
distributions	O
so	O
that	O
the	O
decompositions	O
and	O
their	O
representations	O
as	O
fully	B
connected	I
graphs	O
will	O
be	O
applicable	O
to	O
any	O
choice	O
of	O
distribution	O
as	O
we	O
shall	O
see	O
shortly	O
it	O
is	O
the	O
absence	O
of	O
links	O
in	O
the	O
graph	O
that	O
conveys	O
interesting	O
information	O
about	O
the	O
properties	O
of	O
the	O
class	O
of	O
distributions	O
that	O
the	O
graph	O
represents	O
consider	O
the	O
graph	O
shown	O
in	O
figure	O
this	O
is	O
not	O
a	O
fully	B
connected	I
graph	O
because	O
for	O
instance	O
there	O
is	O
no	O
link	B
from	O
to	O
or	O
from	O
to	O
we	O
shall	O
now	O
go	O
from	O
this	O
graph	O
to	O
the	O
corresponding	O
representation	O
of	O
the	O
joint	O
probability	B
distribution	O
written	O
in	O
terms	O
of	O
the	O
product	O
of	O
a	O
set	O
of	O
conditional	B
distributions	O
one	O
for	O
each	O
node	B
in	O
the	O
graph	O
each	O
such	O
conditional	B
distribution	O
will	O
be	O
conditioned	O
only	O
on	O
the	O
parents	O
of	O
the	O
corresponding	O
node	B
in	O
the	O
graph	O
for	O
instance	O
will	O
be	O
conditioned	O
on	O
and	O
the	O
joint	O
distribution	O
of	O
all	O
variables	O
graphical	O
models	O
figure	O
example	O
of	O
a	O
directed	B
acyclic	I
graph	I
describing	O
the	O
joint	O
distribution	O
over	O
variables	O
the	O
corresponding	O
decomposition	O
of	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
is	O
therefore	O
given	O
by	O
the	O
reader	O
should	O
take	O
a	O
moment	O
to	O
study	O
carefully	O
the	O
correspondence	O
between	O
and	O
figure	O
px	O
pxkpak	O
we	O
can	O
now	O
state	O
in	O
general	O
terms	O
the	O
relationship	O
between	O
a	O
given	O
directed	B
graph	O
and	O
the	O
corresponding	O
distribution	O
over	O
the	O
variables	O
the	O
joint	O
distribution	O
defined	O
by	O
a	O
graph	O
is	O
given	O
by	O
the	O
product	O
over	O
all	O
of	O
the	O
nodes	O
of	O
the	O
graph	O
of	O
a	O
conditional	B
distribution	O
for	O
each	O
node	B
conditioned	O
on	O
the	O
variables	O
corresponding	O
to	O
the	O
parents	O
of	O
that	O
node	B
in	O
the	O
graph	O
thus	O
for	O
a	O
graph	O
with	O
k	O
nodes	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
where	O
pak	O
denotes	O
the	O
set	O
of	O
parents	O
of	O
xk	O
and	O
x	O
xk	O
this	O
key	O
equation	O
expresses	O
the	O
factorization	B
properties	O
of	O
the	O
joint	O
distribution	O
for	O
a	O
directed	B
graphical	B
model	I
although	O
we	O
have	O
considered	O
each	O
node	B
to	O
correspond	O
to	O
a	O
single	O
variable	O
we	O
can	O
equally	O
well	O
associate	O
sets	O
of	O
variables	O
and	O
vector-valued	O
variables	O
with	O
the	O
nodes	O
of	O
a	O
graph	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
representation	O
on	O
the	O
righthand	O
side	O
of	O
is	O
always	O
correctly	O
normalized	O
provided	O
the	O
individual	O
conditional	B
distributions	O
are	O
normalized	O
the	O
directed	B
graphs	O
that	O
we	O
are	O
considering	O
are	O
subject	O
to	O
an	O
important	O
restriction	O
namely	O
that	O
there	O
must	O
be	O
no	O
directed	B
cycles	O
in	O
other	O
words	O
there	O
are	O
no	O
closed	O
paths	O
within	O
the	O
graph	O
such	O
that	O
we	O
can	O
move	O
from	O
node	B
to	O
node	B
along	O
links	O
following	O
the	O
direction	O
of	O
the	O
arrows	O
and	O
end	O
up	O
back	O
at	O
the	O
starting	O
node	B
such	O
graphs	O
are	O
also	O
called	O
directed	B
acyclic	O
graphs	O
or	O
dags	O
this	O
is	O
equivalent	O
to	O
the	O
statement	O
that	O
there	O
exists	O
an	O
ordering	O
of	O
the	O
nodes	O
such	O
that	O
there	O
are	O
no	O
links	O
that	O
go	O
from	O
any	O
node	B
to	O
any	O
lower	O
numbered	O
node	B
example	O
polynomial	O
regression	B
as	O
an	O
illustration	O
of	O
the	O
use	O
of	O
directed	B
graphs	O
to	O
describe	O
probability	B
distributions	O
we	O
consider	O
the	O
bayesian	B
polynomial	O
regression	B
model	O
introduced	O
in	O
sec	O
exercise	O
exercise	O
bayesian	B
networks	O
figure	O
directed	B
graphical	B
model	I
representing	O
the	O
joint	O
distribution	O
corresponding	O
to	O
the	O
bayesian	B
polynomial	O
regression	B
model	O
introduced	O
in	O
section	O
w	O
tn	O
tion	O
the	O
random	O
variables	O
in	O
this	O
model	O
are	O
the	O
vector	O
of	O
polynomial	O
coefficients	O
w	O
and	O
the	O
observed	O
data	O
t	O
tnt	O
in	O
addition	O
this	O
model	O
contains	O
the	O
input	O
data	O
x	O
xn	O
the	O
noise	O
variance	B
and	O
the	O
hyperparameter	B
representing	O
the	O
precision	O
of	O
the	O
gaussian	B
prior	B
over	O
w	O
all	O
of	O
which	O
are	O
parameters	O
of	O
the	O
model	O
rather	O
than	O
random	O
variables	O
focussing	O
just	O
on	O
the	O
random	O
variables	O
for	O
the	O
moment	O
we	O
see	O
that	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
the	O
product	O
of	O
the	O
prior	B
pw	O
and	O
n	O
conditional	B
distributions	O
ptnw	O
for	O
n	O
n	O
so	O
that	O
pt	O
w	O
pw	O
ptnw	O
this	O
joint	O
distribution	O
can	O
be	O
represented	O
by	O
a	O
graphical	B
model	I
shown	O
in	O
figure	O
when	O
we	O
start	O
to	O
deal	O
with	O
more	O
complex	O
models	O
later	O
in	O
the	O
book	O
we	O
shall	O
find	O
it	O
inconvenient	O
to	O
have	O
to	O
write	O
out	O
multiple	O
nodes	O
of	O
the	O
form	O
tn	O
explicitly	O
as	O
in	O
figure	O
we	O
therefore	O
introduce	O
a	O
graphical	O
notation	O
that	O
allows	O
such	O
multiple	O
nodes	O
to	O
be	O
expressed	O
more	O
compactly	O
in	O
which	O
we	O
draw	O
a	O
single	O
representative	O
node	B
tn	O
and	O
then	O
surround	O
this	O
with	O
a	O
box	O
called	O
a	O
plate	B
labelled	O
with	O
n	O
indicating	O
that	O
there	O
are	O
n	O
nodes	O
of	O
this	O
kind	O
re-writing	O
the	O
graph	O
of	O
figure	O
in	O
this	O
way	O
we	O
obtain	O
the	O
graph	O
shown	O
in	O
figure	O
we	O
shall	O
sometimes	O
find	O
it	O
helpful	O
to	O
make	O
the	O
parameters	O
of	O
a	O
model	O
as	O
well	O
as	O
its	O
stochastic	B
variables	O
explicit	O
in	O
this	O
case	O
becomes	O
pt	O
wx	O
pw	O
ptnw	O
xn	O
correspondingly	O
we	O
can	O
make	O
x	O
and	O
explicit	O
in	O
the	O
graphical	O
representation	O
to	O
do	O
this	O
we	O
shall	O
adopt	O
the	O
convention	O
that	O
random	O
variables	O
will	O
be	O
denoted	O
by	O
open	O
circles	O
and	O
deterministic	O
parameters	O
will	O
be	O
denoted	O
by	O
smaller	O
solid	O
circles	O
if	O
we	O
take	O
the	O
graph	O
of	O
figure	O
and	O
include	O
the	O
deterministic	O
parameters	O
we	O
obtain	O
the	O
graph	O
shown	O
in	O
figure	O
when	O
we	O
apply	O
a	O
graphical	B
model	I
to	O
a	O
problem	O
in	O
machine	O
learning	B
or	O
pattern	O
recognition	O
we	O
will	O
typically	O
set	O
some	O
of	O
the	O
random	O
variables	O
to	O
specific	O
observed	O
figure	O
an	O
alternative	O
more	O
compact	O
representation	O
of	O
the	O
graph	O
shown	O
in	O
figure	O
in	O
which	O
we	O
have	O
introduced	O
a	O
plate	B
box	O
labelled	O
n	O
that	O
represents	O
n	O
nodes	O
of	O
which	O
only	O
a	O
single	O
example	O
tn	O
is	O
shown	O
explicitly	O
tn	O
n	O
w	O
graphical	O
models	O
figure	O
this	O
shows	O
the	O
same	O
model	O
as	O
in	O
figure	O
but	O
with	O
the	O
deterministic	O
parameters	O
shown	O
explicitly	O
by	O
the	O
smaller	O
solid	O
nodes	O
xn	O
tn	O
n	O
w	O
values	O
for	O
example	O
the	O
variables	O
from	O
the	O
training	B
set	I
in	O
the	O
case	O
of	O
polynomial	B
curve	B
fitting	I
in	O
a	O
graphical	B
model	I
we	O
will	O
denote	O
such	O
observed	O
variables	O
by	O
shading	O
the	O
corresponding	O
nodes	O
thus	O
the	O
graph	O
corresponding	O
to	O
figure	O
in	O
which	O
the	O
variables	O
are	O
observed	O
is	O
shown	O
in	O
figure	O
note	O
that	O
the	O
value	O
of	O
w	O
is	O
not	O
observed	O
and	O
so	O
w	O
is	O
an	O
example	O
of	O
a	O
latent	B
variable	I
also	O
known	O
as	O
a	O
hidden	B
variable	I
such	O
variables	O
play	O
a	O
crucial	O
role	O
in	O
many	O
probabilistic	O
models	O
and	O
will	O
form	O
the	O
focus	O
of	O
chapters	O
and	O
having	O
observed	O
the	O
values	O
we	O
can	O
if	O
desired	O
evaluate	O
the	O
posterior	O
distribution	O
of	O
the	O
polynomial	O
coefficients	O
w	O
as	O
discussed	O
in	O
section	O
for	O
the	O
moment	O
we	O
note	O
that	O
this	O
involves	O
a	O
straightforward	O
application	O
of	O
bayes	B
theorem	O
pwt	O
pw	O
ptnw	O
where	O
again	O
we	O
have	O
omitted	O
the	O
deterministic	O
parameters	O
in	O
order	O
to	O
keep	O
the	O
notation	O
uncluttered	O
in	O
general	O
model	O
parameters	O
such	O
as	O
w	O
are	O
of	O
little	O
direct	O
interest	O
in	O
themselves	O
because	O
our	O
ultimate	O
goal	O
is	O
to	O
make	O
predictions	O
for	O
new	O
input	O
values	O
suppose	O
we	O
are	O
given	O
a	O
new	O
input	O
and	O
we	O
wish	O
to	O
find	O
the	O
corresponding	O
probability	B
distribution	O
conditioned	O
on	O
the	O
observed	O
data	O
the	O
graphical	B
model	I
that	O
describes	O
this	O
problem	O
is	O
shown	O
in	O
figure	O
and	O
the	O
corresponding	O
joint	O
distribution	O
of	O
all	O
of	O
the	O
random	O
variables	O
in	O
this	O
model	O
conditioned	O
on	O
the	O
deterministic	O
parameters	O
is	O
then	O
given	O
by	O
t	O
x	O
ptnxn	O
w	O
pw	O
w	O
figure	O
as	O
in	O
figure	O
but	O
with	O
the	O
nodes	O
shaded	O
to	O
indicate	O
that	O
the	O
corresponding	O
random	O
variables	O
have	O
been	O
set	O
to	O
their	O
observed	O
set	O
values	O
xn	O
tn	O
n	O
w	O
bayesian	B
networks	O
figure	O
the	O
polynomial	O
regression	B
model	O
corresponding	O
to	O
figure	O
showing	O
also	O
a	O
new	O
input	O
value	O
bx	O
together	O
with	O
the	O
corresponding	O
model	O
prediction	O
bt	O
xn	O
w	O
tn	O
n	O
the	O
required	O
predictive	B
distribution	I
is	O
then	O
obtained	O
from	O
the	O
sum	B
rule	I
of	O
t	O
x	O
probability	B
by	O
integrating	O
out	O
the	O
model	O
parameters	O
w	O
so	O
that	O
x	O
t	O
t	O
x	O
dw	O
where	O
we	O
are	O
implicitly	O
setting	O
the	O
random	O
variables	O
in	O
t	O
to	O
the	O
specific	O
values	O
observed	O
in	O
the	O
data	O
set	O
the	O
details	O
of	O
this	O
calculation	O
were	O
discussed	O
in	O
chapter	O
generative	O
models	O
there	O
are	O
many	O
situations	O
in	O
which	O
we	O
wish	O
to	O
draw	O
samples	O
from	O
a	O
given	O
probability	B
distribution	O
although	O
we	O
shall	O
devote	O
the	O
whole	O
of	O
chapter	O
to	O
a	O
detailed	O
discussion	O
of	O
sampling	B
methods	I
it	O
is	O
instructive	O
to	O
outline	O
here	O
one	O
technique	O
called	O
ancestral	B
sampling	I
which	O
is	O
particularly	O
relevant	O
to	O
graphical	O
models	O
consider	O
a	O
joint	O
distribution	O
xk	O
over	O
k	O
variables	O
that	O
factorizes	O
according	O
to	O
corresponding	O
to	O
a	O
directed	B
acyclic	I
graph	I
we	O
shall	O
suppose	O
that	O
the	O
variables	O
have	O
been	O
ordered	O
such	O
that	O
there	O
are	O
no	O
links	O
from	O
any	O
node	B
to	O
any	O
lower	O
numbered	O
node	B
in	O
other	O
words	O
each	O
node	B
has	O
a	O
higher	O
number	O
than	O
any	O
of	O
its	O
parents	O
our	O
goal	O
is	O
to	O
draw	O
a	O
from	O
the	O
joint	O
distribution	O
distribution	O
which	O
we	O
we	O
then	O
work	O
through	O
each	O
of	O
the	O
nodes	O
in	O
or	O
to	O
do	O
this	O
we	O
start	O
with	O
the	O
lowest-numbered	O
node	B
and	O
draw	O
a	O
sample	O
from	O
the	O
der	O
so	O
that	O
for	O
node	B
n	O
we	O
draw	O
a	O
sample	O
from	O
the	O
conditional	B
distribution	O
pxnpan	O
in	O
which	O
the	O
parent	O
variables	O
have	O
been	O
set	O
to	O
their	O
sampled	O
values	O
note	O
that	O
at	O
each	O
stage	O
these	O
parent	O
values	O
will	O
always	O
be	O
available	O
because	O
they	O
correspond	O
to	O
lowernumbered	O
nodes	O
that	O
have	O
already	O
been	O
sampled	O
techniques	O
for	O
sampling	O
from	O
specific	O
distributions	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
chapter	O
once	O
we	O
have	O
sampled	O
from	O
the	O
final	O
variable	O
xk	O
we	O
will	O
have	O
achieved	O
our	O
objective	O
of	O
obtaining	O
a	O
sample	O
from	O
the	O
joint	O
distribution	O
to	O
obtain	O
a	O
sample	O
from	O
some	O
marginal	B
distribution	O
corresponding	O
to	O
a	O
subset	O
of	O
the	O
variables	O
we	O
simply	O
take	O
the	O
sampled	O
values	O
for	O
the	O
required	O
nodes	O
and	O
ignore	O
the	O
sampled	O
values	O
for	O
the	O
remaining	O
nodes	O
for	O
example	O
to	O
draw	O
a	O
sample	O
from	O
the	O
distribution	O
we	O
simply	O
sample	O
from	O
the	O
full	O
joint	O
distribution	O
and	O
then	O
retain	O
the	O
and	O
discard	O
the	O
remaining	O
values	O
graphical	O
models	O
figure	O
a	O
graphical	B
model	I
representing	O
the	O
process	O
by	O
which	O
images	O
of	O
objects	O
are	O
created	O
in	O
which	O
the	O
identity	O
of	O
an	O
object	O
discrete	O
variable	O
and	O
the	O
position	O
and	O
orientation	O
of	O
that	O
object	O
variables	O
have	O
independent	B
prior	B
probabilities	O
the	O
image	O
vector	O
of	O
pixel	O
intensities	O
has	O
a	O
probability	B
distribution	O
that	O
is	O
dependent	O
on	O
the	O
identity	O
of	O
the	O
object	O
as	O
well	O
as	O
on	O
its	O
position	O
and	O
orientation	O
object	O
position	O
orientation	O
image	O
for	O
practical	O
applications	O
of	O
probabilistic	O
models	O
it	O
will	O
typically	O
be	O
the	O
highernumbered	O
variables	O
corresponding	O
to	O
terminal	O
nodes	O
of	O
the	O
graph	O
that	O
represent	O
the	O
observations	O
with	O
lower-numbered	O
nodes	O
corresponding	O
to	O
latent	O
variables	O
the	O
primary	O
role	O
of	O
the	O
latent	O
variables	O
is	O
to	O
allow	O
a	O
complicated	O
distribution	O
over	O
the	O
observed	O
variables	O
to	O
be	O
represented	O
in	O
terms	O
of	O
a	O
model	O
constructed	O
from	O
simpler	O
exponential	B
family	I
conditional	B
distributions	O
we	O
can	O
interpret	O
such	O
models	O
as	O
expressing	O
the	O
processes	O
by	O
which	O
the	O
observed	O
data	O
arose	O
for	O
instance	O
consider	O
an	O
object	B
recognition	I
task	O
in	O
which	O
each	O
observed	O
data	O
point	O
corresponds	O
to	O
an	O
image	O
a	O
vector	O
of	O
pixel	O
intensities	O
of	O
one	O
of	O
the	O
objects	O
in	O
this	O
case	O
the	O
latent	O
variables	O
might	O
have	O
an	O
interpretation	O
as	O
the	O
position	O
and	O
orientation	O
of	O
the	O
object	O
given	O
a	O
particular	O
observed	O
image	O
our	O
goal	O
is	O
to	O
find	O
the	O
posterior	O
distribution	O
over	O
objects	O
in	O
which	O
we	O
integrate	O
over	O
all	O
possible	O
positions	O
and	O
orientations	O
we	O
can	O
represent	O
this	O
problem	O
using	O
a	O
graphical	B
model	I
of	O
the	O
form	O
show	O
in	O
figure	O
the	O
graphical	B
model	I
captures	O
the	O
causal	O
process	O
by	O
which	O
the	O
observed	O
data	O
was	O
generated	O
for	O
this	O
reason	O
such	O
models	O
are	O
often	O
called	O
generative	O
models	O
by	O
contrast	O
the	O
polynomial	O
regression	B
model	O
described	O
by	O
figure	O
is	O
not	O
generative	O
because	O
there	O
is	O
no	O
probability	B
distribution	O
associated	O
with	O
the	O
input	O
variable	O
x	O
and	O
so	O
it	O
is	O
not	O
possible	O
to	O
generate	O
synthetic	O
data	O
points	O
from	O
this	O
model	O
we	O
could	O
make	O
it	O
generative	O
by	O
introducing	O
a	O
suitable	O
prior	B
distribution	O
px	O
at	O
the	O
expense	O
of	O
a	O
more	O
complex	O
model	O
the	O
hidden	O
variables	O
in	O
a	O
probabilistic	O
model	O
need	O
not	O
however	O
have	O
any	O
explicit	O
physical	O
interpretation	O
but	O
may	O
be	O
introduced	O
simply	O
to	O
allow	O
a	O
more	O
complex	O
joint	O
distribution	O
to	O
be	O
constructed	O
from	O
simpler	O
components	O
in	O
either	O
case	O
the	O
technique	O
of	O
ancestral	B
sampling	I
applied	O
to	O
a	O
generative	B
model	I
mimics	O
the	O
creation	O
of	O
the	O
observed	O
data	O
and	O
would	O
therefore	O
give	O
rise	O
to	O
fantasy	O
data	O
whose	O
probability	B
distribution	O
the	O
model	O
were	O
a	O
perfect	O
representation	O
of	O
reality	O
would	O
be	O
the	O
same	O
as	O
that	O
of	O
the	O
observed	O
data	O
in	O
practice	O
producing	O
synthetic	O
observations	O
from	O
a	O
generative	B
model	I
can	O
prove	O
informative	O
in	O
understanding	O
the	O
form	O
of	O
the	O
probability	B
distribution	O
represented	O
by	O
that	O
model	O
discrete	O
variables	O
we	O
have	O
discussed	O
the	O
importance	O
of	O
probability	B
distributions	O
that	O
are	O
members	O
of	O
the	O
exponential	B
family	I
and	O
we	O
have	O
seen	O
that	O
this	O
family	O
includes	O
many	O
wellknown	O
distributions	O
as	O
particular	O
cases	O
although	O
such	O
distributions	O
are	O
relatively	O
simple	O
they	O
form	O
useful	O
building	O
blocks	O
for	O
constructing	O
more	O
complex	O
probability	B
section	O
bayesian	B
networks	O
figure	O
this	O
fully-connected	O
graph	O
describes	O
a	O
general	O
distribution	O
over	O
two	O
k-state	O
discrete	O
variables	O
having	O
a	O
total	O
of	O
k	O
parameters	O
by	O
dropping	O
the	O
link	B
between	O
the	O
nodes	O
the	O
number	O
of	O
parameters	O
is	O
reduced	O
to	O
distributions	O
and	O
the	O
framework	O
of	O
graphical	O
models	O
is	O
very	O
useful	O
in	O
expressing	O
the	O
way	O
in	O
which	O
these	O
building	O
blocks	O
are	O
linked	O
together	O
such	O
models	O
have	O
particularly	O
nice	O
properties	O
if	O
we	O
choose	O
the	O
relationship	O
between	O
each	O
parent-child	O
pair	O
in	O
a	O
directed	B
graph	O
to	O
be	O
conjugate	B
and	O
we	O
shall	O
explore	O
several	O
examples	O
of	O
this	O
shortly	O
two	O
cases	O
are	O
particularly	O
worthy	O
of	O
note	O
namely	O
when	O
the	O
parent	O
and	O
child	B
node	B
each	O
correspond	O
to	O
discrete	O
variables	O
and	O
when	O
they	O
each	O
correspond	O
to	O
gaussian	B
variables	O
because	O
in	O
these	O
two	O
cases	O
the	O
relationship	O
can	O
be	O
extended	B
hierarchically	O
to	O
construct	O
arbitrarily	O
complex	O
directed	B
acyclic	O
graphs	O
we	O
begin	O
by	O
examining	O
the	O
discrete	O
case	O
the	O
probability	B
distribution	O
px	O
for	O
a	O
single	O
discrete	O
variable	O
x	O
having	O
k	O
possible	O
states	O
the	O
representation	O
is	O
given	O
by	O
px	O
xk	O
k	O
and	O
is	O
governed	O
by	O
the	O
parameters	O
kt	O
due	O
to	O
the	O
constraint	O
k	O
k	O
only	O
k	O
values	O
for	O
k	O
need	O
to	O
be	O
specified	O
in	O
order	O
to	O
define	O
the	O
now	O
suppose	O
that	O
we	O
have	O
two	O
discrete	O
variables	O
and	O
each	O
of	O
which	O
has	O
k	O
states	O
and	O
we	O
wish	O
to	O
model	O
their	O
joint	O
distribution	O
we	O
denote	O
the	O
probability	B
of	O
observing	O
both	O
and	O
by	O
the	O
parameter	O
kl	O
where	O
denotes	O
the	O
kth	O
component	O
of	O
and	O
similarly	O
for	O
the	O
joint	O
distribution	O
can	O
be	O
written	O
distribution	O
kl	O
k	O
l	O
kl	O
this	O
distribecause	O
the	O
parameters	O
kl	O
are	O
subject	O
to	O
the	O
constraint	O
bution	O
is	O
governed	O
by	O
k	O
parameters	O
it	O
is	O
easily	O
seen	O
that	O
the	O
total	O
number	O
of	O
parameters	O
that	O
must	O
be	O
specified	O
for	O
an	O
arbitrary	O
joint	O
distribution	O
over	O
m	O
variables	O
is	O
km	O
and	O
therefore	O
grows	O
exponentially	O
with	O
the	O
number	O
m	O
of	O
variables	O
using	O
the	O
product	B
rule	I
we	O
can	O
factor	O
the	O
joint	O
distribution	O
in	O
the	O
form	O
which	O
corresponds	O
to	O
a	O
two-node	O
graph	O
with	O
a	O
link	B
going	O
from	O
the	O
node	B
to	O
the	O
node	B
as	O
shown	O
in	O
figure	O
the	O
marginal	B
distribution	O
is	O
governed	O
by	O
k	O
parameters	O
as	O
before	O
similarly	O
the	O
conditional	B
distribution	O
requires	O
the	O
specification	O
of	O
k	O
parameters	O
for	O
each	O
of	O
the	O
k	O
possible	O
values	O
of	O
the	O
total	O
number	O
of	O
parameters	O
that	O
must	O
be	O
specified	O
in	O
the	O
joint	O
distribution	O
is	O
therefore	O
kk	O
k	O
as	O
before	O
now	O
suppose	O
that	O
the	O
variables	O
and	O
were	O
independent	B
corresponding	O
to	O
the	O
graphical	B
model	I
shown	O
in	O
figure	O
each	O
variable	O
is	O
then	O
described	O
by	O
graphical	O
models	O
figure	O
this	O
chain	O
of	O
m	O
discrete	O
nodes	O
each	O
having	O
k	O
states	O
requires	O
the	O
specification	O
of	O
k	O
parameters	O
which	O
grows	O
linearly	O
with	O
the	O
length	O
m	O
of	O
the	O
chain	O
in	O
contrast	O
a	O
fully	B
connected	I
graph	O
of	O
m	O
nodes	O
would	O
have	O
km	O
parameters	O
which	O
grows	O
exponentially	O
with	O
m	O
xm	O
a	O
separate	O
multinomial	B
distribution	I
and	O
the	O
total	O
number	O
of	O
parameters	O
would	O
be	O
for	O
a	O
distribution	O
over	O
m	O
independent	B
discrete	O
variables	O
each	O
having	O
k	O
states	O
the	O
total	O
number	O
of	O
parameters	O
would	O
be	O
mk	O
which	O
therefore	O
grows	O
linearly	O
with	O
the	O
number	O
of	O
variables	O
from	O
a	O
graphical	O
perspective	O
we	O
have	O
reduced	O
the	O
number	O
of	O
parameters	O
by	O
dropping	O
links	O
in	O
the	O
graph	O
at	O
the	O
expense	O
of	O
having	O
a	O
restricted	O
class	O
of	O
distributions	O
more	O
generally	O
if	O
we	O
have	O
m	O
discrete	O
variables	O
xm	O
we	O
can	O
model	O
the	O
joint	O
distribution	O
using	O
a	O
directed	B
graph	O
with	O
one	O
variable	O
corresponding	O
to	O
each	O
node	B
the	O
conditional	B
distribution	O
at	O
each	O
node	B
is	O
given	O
by	O
a	O
set	O
of	O
nonnegative	O
parameters	O
subject	O
to	O
the	O
usual	O
normalization	O
constraint	O
if	O
the	O
graph	O
is	O
fully	B
connected	I
then	O
we	O
have	O
a	O
completely	O
general	O
distribution	O
having	O
km	O
parameters	O
whereas	O
if	O
there	O
are	O
no	O
links	O
in	O
the	O
graph	O
the	O
joint	O
distribution	O
factorizes	O
into	O
the	O
product	O
of	O
the	O
marginals	O
and	O
the	O
total	O
number	O
of	O
parameters	O
is	O
mk	O
graphs	O
having	O
intermediate	O
levels	O
of	O
connectivity	O
allow	O
for	O
more	O
general	O
distributions	O
than	O
the	O
fully	O
factorized	O
one	O
while	O
requiring	O
fewer	O
parameters	O
than	O
the	O
general	O
joint	O
distribution	O
as	O
an	O
illustration	O
consider	O
the	O
chain	O
of	O
nodes	O
shown	O
in	O
figure	O
the	O
marginal	B
distribution	O
requires	O
k	O
parameters	O
whereas	O
each	O
of	O
the	O
m	O
conditional	B
distributions	O
pxixi	O
for	O
i	O
m	O
requires	O
kk	O
parameters	O
this	O
gives	O
a	O
total	O
parameter	O
count	O
of	O
k	O
which	O
is	O
quadratic	O
in	O
k	O
and	O
which	O
grows	O
linearly	O
than	O
exponentially	O
with	O
the	O
length	O
m	O
of	O
the	O
chain	O
an	O
alternative	O
way	O
to	O
reduce	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
a	O
model	O
is	O
by	O
sharing	O
parameters	O
known	O
as	O
tying	O
of	O
parameters	O
for	O
instance	O
in	O
the	O
chain	O
example	O
of	O
figure	O
we	O
can	O
arrange	O
that	O
all	O
of	O
the	O
conditional	B
distributions	O
pxixi	O
for	O
i	O
m	O
are	O
governed	O
by	O
the	O
same	O
set	O
of	O
kk	O
parameters	O
together	O
with	O
the	O
k	O
parameters	O
governing	O
the	O
distribution	O
of	O
this	O
gives	O
a	O
total	O
of	O
k	O
parameters	O
that	O
must	O
be	O
specified	O
in	O
order	O
to	O
define	O
the	O
joint	O
distribution	O
we	O
can	O
turn	O
a	O
graph	O
over	O
discrete	O
variables	O
into	O
a	O
bayesian	B
model	O
by	O
introducing	O
dirichlet	B
priors	O
for	O
the	O
parameters	O
from	O
a	O
graphical	O
point	O
of	O
view	O
each	O
node	B
then	O
acquires	O
an	O
additional	O
parent	O
representing	O
the	O
dirichlet	B
distribution	I
over	O
the	O
parameters	O
associated	O
with	O
the	O
corresponding	O
discrete	O
node	B
this	O
is	O
illustrated	O
for	O
the	O
chain	O
model	O
in	O
figure	O
the	O
corresponding	O
model	O
in	O
which	O
we	O
tie	O
the	O
parameters	O
governing	O
the	O
conditional	B
distributions	O
pxixi	O
for	O
i	O
m	O
is	O
shown	O
in	O
figure	O
another	O
way	O
of	O
controlling	O
the	O
exponential	O
growth	O
in	O
the	O
number	O
of	O
parameters	O
in	O
models	O
of	O
discrete	O
variables	O
is	O
to	O
use	O
parameterized	O
models	O
for	O
the	O
conditional	B
distributions	O
instead	O
of	O
complete	O
tables	O
of	O
conditional	B
probability	B
values	O
to	O
illustrate	O
this	O
idea	O
consider	O
the	O
graph	O
in	O
figure	O
in	O
which	O
all	O
of	O
the	O
nodes	O
represent	O
binary	O
variables	O
each	O
of	O
the	O
parent	O
variables	O
xi	O
is	O
governed	O
by	O
a	O
single	O
parame	O
bayesian	B
networks	O
figure	O
an	O
extension	O
of	O
the	O
model	O
of	O
figure	O
to	O
include	O
dirichlet	B
priors	O
over	O
the	O
parameters	O
governing	O
the	O
discrete	O
distributions	O
figure	O
as	O
in	O
figure	O
but	O
with	O
a	O
single	O
set	O
of	O
parameters	O
shared	O
amongst	O
all	O
of	O
the	O
conditional	B
distributions	O
pxixi	O
m	O
xm	O
xm	O
ter	O
i	O
representing	O
the	O
probability	B
pxi	O
giving	O
m	O
parameters	O
in	O
total	O
for	O
the	O
parent	O
nodes	O
the	O
conditional	B
distribution	O
xm	O
however	O
would	O
require	O
parameters	O
representing	O
the	O
probability	B
py	O
for	O
each	O
of	O
the	O
possible	O
settings	O
of	O
the	O
parent	O
variables	O
thus	O
in	O
general	O
the	O
number	O
of	O
parameters	O
required	O
to	O
specify	O
this	O
conditional	B
distribution	O
will	O
grow	O
exponentially	O
with	O
m	O
we	O
can	O
obtain	O
a	O
more	O
parsimonious	O
form	O
for	O
the	O
conditional	B
distribution	O
by	O
using	O
a	O
logistic	B
sigmoid	I
function	O
acting	O
on	O
a	O
linear	O
combination	O
of	O
the	O
parent	O
variables	O
giving	O
section	O
py	O
xm	O
wixi	O
where	O
a	O
is	O
the	O
logistic	B
sigmoid	I
x	O
xm	O
is	O
an	O
vector	O
of	O
parent	O
states	O
augmented	O
with	O
an	O
additional	O
variable	O
whose	O
value	O
is	O
clamped	O
to	O
and	O
w	O
wm	O
is	O
a	O
vector	O
of	O
m	O
parameters	O
this	O
is	O
a	O
more	O
restricted	O
form	O
of	O
conditional	B
distribution	O
than	O
the	O
general	O
case	O
but	O
is	O
now	O
governed	O
by	O
a	O
number	O
of	O
parameters	O
that	O
grows	O
linearly	O
with	O
m	O
in	O
this	O
sense	O
it	O
is	O
analogous	O
to	O
the	O
choice	O
of	O
a	O
restrictive	O
form	O
of	O
covariance	B
matrix	O
example	O
a	O
diagonal	B
matrix	O
in	O
a	O
multivariate	O
gaussian	B
distribution	O
the	O
motivation	O
for	O
the	O
logistic	B
sigmoid	I
representation	O
was	O
discussed	O
in	O
section	O
figure	O
a	O
graph	O
comprising	O
m	O
parents	O
xm	O
and	O
a	O
single	O
child	O
y	O
used	O
to	O
illustrate	O
the	O
idea	O
of	O
parameterized	O
conditional	B
distributions	O
for	O
discrete	O
variables	O
xm	O
y	O
graphical	O
models	O
linear-gaussian	O
models	O
in	O
the	O
previous	O
section	O
we	O
saw	O
how	O
to	O
construct	O
joint	O
probability	B
distributions	O
over	O
a	O
set	O
of	O
discrete	O
variables	O
by	O
expressing	O
the	O
variables	O
as	O
nodes	O
in	O
a	O
directed	B
acyclic	I
graph	I
here	O
we	O
show	O
how	O
a	O
multivariate	O
gaussian	B
can	O
be	O
expressed	O
as	O
a	O
directed	B
graph	O
corresponding	O
to	O
a	O
linear-gaussian	B
model	I
over	O
the	O
component	O
variables	O
this	O
allows	O
us	O
to	O
impose	O
interesting	O
structure	O
on	O
the	O
distribution	O
with	O
the	O
general	O
gaussian	B
and	O
the	O
diagonal	B
covariance	B
gaussian	B
representing	O
opposite	O
extremes	O
several	O
widely	O
used	O
techniques	O
are	O
examples	O
of	O
linear-gaussian	O
models	O
such	O
as	O
probabilistic	O
principal	B
component	I
analysis	I
factor	B
analysis	I
and	O
linear	O
dynamical	O
systems	O
and	O
ghahramani	O
we	O
shall	O
make	O
extensive	O
use	O
of	O
the	O
results	O
of	O
this	O
section	O
in	O
later	O
chapters	O
when	O
we	O
consider	O
some	O
of	O
these	O
techniques	O
in	O
detail	O
consider	O
an	O
arbitrary	O
directed	B
acyclic	I
graph	I
over	O
d	O
variables	O
in	O
which	O
node	B
i	O
represents	O
a	O
single	O
continuous	O
random	O
variable	O
xi	O
having	O
a	O
gaussian	B
distribution	O
the	O
mean	B
of	O
this	O
distribution	O
is	O
taken	O
to	O
be	O
a	O
linear	O
combination	O
of	O
the	O
states	O
of	O
its	O
parent	O
nodes	O
pai	O
of	O
node	B
i	O
xi	O
j	O
pai	O
pxipai	O
n	O
wijxj	O
bi	O
vi	O
where	O
wij	O
and	O
bi	O
are	O
parameters	O
governing	O
the	O
mean	B
and	O
vi	O
is	O
the	O
variance	B
of	O
the	O
conditional	B
distribution	O
for	O
xi	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
is	O
then	O
the	O
log	O
of	O
the	O
product	O
of	O
these	O
conditionals	O
over	O
all	O
nodes	O
in	O
the	O
graph	O
and	O
hence	O
takes	O
the	O
form	O
ln	O
px	O
ln	O
pxipai	O
xi	O
wijxj	O
bi	O
j	O
pai	O
const	O
where	O
x	O
xdt	O
and	O
const	O
denotes	O
terms	O
independent	B
of	O
x	O
we	O
see	O
that	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
the	O
components	O
of	O
x	O
and	O
hence	O
the	O
joint	O
distribution	O
px	O
is	O
a	O
multivariate	O
gaussian	B
we	O
can	O
determine	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
joint	O
distribution	O
recursively	O
as	O
follows	O
each	O
variable	O
xi	O
has	O
on	O
the	O
states	O
of	O
its	O
parents	O
a	O
gaussian	B
distribution	O
of	O
the	O
form	O
and	O
so	O
xi	O
wijxj	O
bi	O
vii	O
where	O
is	O
a	O
zero	O
mean	B
unit	O
variance	B
gaussian	B
random	O
variable	O
satisfying	O
ei	O
and	O
eij	O
iij	O
where	O
iij	O
is	O
the	O
i	O
j	O
element	O
of	O
the	O
identity	O
matrix	O
taking	O
the	O
expectation	B
of	O
we	O
have	O
exi	O
wij	O
exj	O
bi	O
j	O
pai	O
j	O
pai	O
figure	O
a	O
directed	B
graph	O
over	O
three	O
gaussian	B
variables	O
with	O
one	O
missing	O
link	B
bayesian	B
networks	O
thus	O
we	O
can	O
find	O
the	O
components	O
of	O
ex	O
exdt	O
by	O
starting	O
at	O
the	O
lowest	O
numbered	O
node	B
and	O
working	O
recursively	O
through	O
the	O
graph	O
we	O
again	O
assume	O
that	O
the	O
nodes	O
are	O
numbered	O
such	O
that	O
each	O
node	B
has	O
a	O
higher	O
number	O
than	O
its	O
parents	O
similarly	O
we	O
can	O
use	O
and	O
to	O
obtain	O
the	O
i	O
j	O
element	O
of	O
the	O
covariance	B
matrix	O
for	O
px	O
in	O
the	O
form	O
of	O
a	O
recursion	O
relation	O
covxi	O
xj	O
e	O
exixj	O
exj	O
exi	O
k	O
paj	O
e	O
wjkcovxi	O
xk	O
iijvj	O
k	O
paj	O
wjkxk	O
exk	O
vjj	O
and	O
so	O
the	O
covariance	B
can	O
similarly	O
be	O
evaluated	O
recursively	O
starting	O
from	O
the	O
lowest	O
numbered	O
node	B
let	O
us	O
consider	O
two	O
extreme	O
cases	O
first	O
of	O
all	O
suppose	O
that	O
there	O
are	O
no	O
links	O
in	O
the	O
graph	O
which	O
therefore	O
comprises	O
d	O
isolated	O
nodes	O
in	O
this	O
case	O
there	O
are	O
no	O
parameters	O
wij	O
and	O
so	O
there	O
are	O
just	O
d	O
parameters	O
bi	O
and	O
d	O
parameters	O
vi	O
from	O
the	O
recursion	O
relations	O
and	O
we	O
see	O
that	O
the	O
mean	B
of	O
px	O
is	O
given	O
by	O
bdt	O
and	O
the	O
covariance	B
matrix	O
is	O
diagonal	B
of	O
the	O
form	O
vd	O
the	O
joint	O
distribution	O
has	O
a	O
total	O
of	O
parameters	O
and	O
represents	O
a	O
set	O
of	O
d	O
independent	B
univariate	O
gaussian	B
distributions	O
now	O
consider	O
a	O
fully	B
connected	I
graph	O
in	O
which	O
each	O
node	B
has	O
all	O
lower	O
numbered	O
nodes	O
as	O
parents	O
the	O
matrix	O
wij	O
then	O
has	O
i	O
entries	O
on	O
the	O
ith	O
row	O
and	O
hence	O
is	O
a	O
lower	O
triangular	O
matrix	O
no	O
entries	O
on	O
the	O
leading	O
diagonal	B
then	O
the	O
total	O
number	O
of	O
parameters	O
wij	O
is	O
obtained	O
by	O
taking	O
the	O
number	O
of	O
elements	O
in	O
a	O
d	O
d	O
matrix	O
subtracting	O
d	O
to	O
account	O
for	O
the	O
absence	O
of	O
elements	O
on	O
the	O
leading	O
diagonal	B
and	O
then	O
dividing	O
by	O
because	O
the	O
matrix	O
has	O
elements	O
only	O
below	O
the	O
diagonal	B
giving	O
a	O
total	O
of	O
dd	O
the	O
total	O
number	O
of	O
independent	B
parameters	O
and	O
in	O
the	O
covariance	B
matrix	O
is	O
therefore	O
dd	O
corresponding	O
to	O
a	O
general	O
symmetric	O
covariance	B
matrix	O
graphs	O
having	O
some	O
intermediate	O
level	O
of	O
complexity	O
correspond	O
to	O
joint	O
gaussian	B
distributions	O
with	O
partially	O
constrained	O
covariance	B
matrices	O
consider	O
for	O
example	O
the	O
graph	O
shown	O
in	O
figure	O
which	O
has	O
a	O
link	B
missing	O
between	O
variables	O
and	O
using	O
the	O
recursion	O
relations	O
and	O
we	O
see	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
joint	O
distribution	O
are	O
given	O
by	O
section	O
exercise	O
graphical	O
models	O
we	O
can	O
readily	O
extend	O
the	O
linear-gaussian	O
graphical	B
model	I
to	O
the	O
case	O
in	O
which	O
the	O
nodes	O
of	O
the	O
graph	O
represent	O
multivariate	O
gaussian	B
variables	O
in	O
this	O
case	O
we	O
can	O
write	O
the	O
conditional	B
distribution	O
for	O
node	B
i	O
in	O
the	O
form	O
pxipai	O
n	O
wijxj	O
bi	O
i	O
xi	O
j	O
pai	O
section	O
where	O
now	O
wij	O
is	O
a	O
matrix	O
is	O
nonsquare	O
if	O
xi	O
and	O
xj	O
have	O
different	O
dimensionalities	O
again	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
joint	O
distribution	O
over	O
all	O
variables	O
is	O
gaussian	B
note	O
that	O
we	O
have	O
already	O
encountered	O
a	O
specific	O
example	O
of	O
the	O
linear-gaussian	O
relationship	O
when	O
we	O
saw	O
that	O
the	O
conjugate	B
prior	B
for	O
the	O
mean	B
of	O
a	O
gaussian	B
variable	O
x	O
is	O
itself	O
a	O
gaussian	B
distribution	O
over	O
the	O
joint	O
distribution	O
over	O
x	O
and	O
is	O
therefore	O
gaussian	B
this	O
corresponds	O
to	O
a	O
simple	O
two-node	O
graph	O
in	O
which	O
the	O
node	B
representing	O
is	O
the	O
parent	O
of	O
the	O
node	B
representing	O
x	O
the	O
mean	B
of	O
the	O
distribution	O
over	O
is	O
a	O
parameter	O
controlling	O
a	O
prior	B
and	O
so	O
it	O
can	O
be	O
viewed	O
as	O
a	O
hyperparameter	B
because	O
the	O
value	O
of	O
this	O
hyperparameter	B
may	O
itself	O
be	O
unknown	O
we	O
can	O
again	O
treat	O
it	O
from	O
a	O
bayesian	B
perspective	O
by	O
introducing	O
a	O
prior	B
over	O
the	O
hyperparameter	B
sometimes	O
called	O
a	O
hyperprior	B
which	O
is	O
again	O
given	O
by	O
a	O
gaussian	B
distribution	O
this	O
type	O
of	O
construction	O
can	O
be	O
extended	B
in	O
principle	O
to	O
any	O
level	O
and	O
is	O
an	O
illustration	O
of	O
a	O
hierarchical	B
bayesian	B
model	I
of	O
which	O
we	O
shall	O
encounter	O
further	O
examples	O
in	O
later	O
chapters	O
conditional	B
independence	I
an	O
important	O
concept	O
for	O
probability	B
distributions	O
over	O
multiple	O
variables	O
is	O
that	O
of	O
conditional	B
independence	I
consider	O
three	O
variables	O
a	O
b	O
and	O
c	O
and	O
suppose	O
that	O
the	O
conditional	B
distribution	O
of	O
a	O
given	O
b	O
and	O
c	O
is	O
such	O
that	O
it	O
does	O
not	O
depend	O
on	O
the	O
value	O
of	O
b	O
so	O
that	O
pab	O
c	O
pac	O
we	O
say	O
that	O
a	O
is	O
conditionally	O
independent	B
of	O
b	O
given	O
c	O
this	O
can	O
be	O
expressed	O
in	O
a	O
slightly	O
different	O
way	O
if	O
we	O
consider	O
the	O
joint	O
distribution	O
of	O
a	O
and	O
b	O
conditioned	O
on	O
c	O
which	O
we	O
can	O
write	O
in	O
the	O
form	O
pa	O
bc	O
pab	O
cpbc	O
pacpbc	O
where	O
we	O
have	O
used	O
the	O
product	B
rule	I
of	I
probability	B
together	O
with	O
thus	O
we	O
see	O
that	O
conditioned	O
on	O
c	O
the	O
joint	O
distribution	O
of	O
a	O
and	O
b	O
factorizes	O
into	O
the	O
product	O
of	O
the	O
marginal	B
distribution	O
of	O
a	O
and	O
the	O
marginal	B
distribution	O
of	O
b	O
both	O
conditioned	O
on	O
c	O
this	O
says	O
that	O
the	O
variables	O
a	O
and	O
b	O
are	O
statistically	O
independent	B
given	O
c	O
note	O
that	O
our	O
definition	O
of	O
conditional	B
independence	I
will	O
require	O
that	O
conditional	B
independence	I
figure	O
the	O
first	O
of	O
three	O
examples	O
of	O
graphs	O
over	O
three	O
variables	O
a	O
b	O
and	O
c	O
used	O
to	O
discuss	O
conditional	B
independence	I
properties	O
of	O
directed	B
graphical	O
models	O
c	O
a	O
b	O
or	O
equivalently	O
must	O
hold	O
for	O
every	O
possible	O
value	O
of	O
c	O
and	O
not	O
just	O
for	O
some	O
values	O
we	O
shall	O
sometimes	O
use	O
a	O
shorthand	O
notation	O
for	O
conditional	B
independence	I
in	O
which	O
a	O
b	O
c	O
denotes	O
that	O
a	O
is	O
conditionally	O
independent	B
of	O
b	O
given	O
c	O
and	O
is	O
equivalent	O
to	O
conditional	B
independence	I
properties	O
play	O
an	O
important	O
role	O
in	O
using	O
probabilistic	O
models	O
for	O
pattern	O
recognition	O
by	O
simplifying	O
both	O
the	O
structure	O
of	O
a	O
model	O
and	O
the	O
computations	O
needed	O
to	O
perform	O
inference	B
and	O
learning	B
under	O
that	O
model	O
we	O
shall	O
see	O
examples	O
of	O
this	O
shortly	O
if	O
we	O
are	O
given	O
an	O
expression	O
for	O
the	O
joint	O
distribution	O
over	O
a	O
set	O
of	O
variables	O
in	O
terms	O
of	O
a	O
product	O
of	O
conditional	B
distributions	O
the	O
mathematical	O
representation	O
underlying	O
a	O
directed	B
graph	O
then	O
we	O
could	O
in	O
principle	O
test	O
whether	O
any	O
potential	O
conditional	B
independence	I
property	O
holds	O
by	O
repeated	O
application	O
of	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
in	O
practice	O
such	O
an	O
approach	O
would	O
be	O
very	O
time	O
consuming	O
an	O
important	O
and	O
elegant	O
feature	O
of	O
graphical	O
models	O
is	O
that	O
conditional	B
independence	I
properties	O
of	O
the	O
joint	O
distribution	O
can	O
be	O
read	O
directly	O
from	O
the	O
graph	O
without	O
having	O
to	O
perform	O
any	O
analytical	O
manipulations	O
the	O
general	O
framework	O
for	O
achieving	O
this	O
is	O
called	O
d-separation	B
where	O
the	O
d	O
stands	O
for	O
directed	B
here	O
we	O
shall	O
motivate	O
the	O
concept	O
of	O
d-separation	B
and	O
give	O
a	O
general	O
statement	O
of	O
the	O
d-separation	B
criterion	O
a	O
formal	O
proof	O
can	O
be	O
found	O
in	O
lauritzen	O
three	O
example	O
graphs	O
we	O
begin	O
our	O
discussion	O
of	O
the	O
conditional	B
independence	I
properties	O
of	O
directed	B
graphs	O
by	O
considering	O
three	O
simple	O
examples	O
each	O
involving	O
graphs	O
having	O
just	O
three	O
nodes	O
together	O
these	O
will	O
motivate	O
and	O
illustrate	O
the	O
key	O
concepts	O
of	O
d-separation	B
the	O
first	O
of	O
the	O
three	O
examples	O
is	O
shown	O
in	O
figure	O
and	O
the	O
joint	O
distribution	O
corresponding	O
to	O
this	O
graph	O
is	O
easily	O
written	O
down	O
using	O
the	O
general	O
result	O
to	O
give	O
if	O
none	O
of	O
the	O
variables	O
are	O
observed	O
then	O
we	O
can	O
investigate	O
whether	O
a	O
and	O
b	O
are	O
independent	B
by	O
marginalizing	O
both	O
sides	O
of	O
with	O
respect	O
to	O
c	O
to	O
give	O
pa	O
b	O
c	O
pacpbcpc	O
pa	O
b	O
pacpbcpc	O
c	O
in	O
general	O
this	O
does	O
not	O
factorize	O
into	O
the	O
product	O
papb	O
and	O
so	O
a	O
b	O
graphical	O
models	O
figure	O
as	O
in	O
figure	O
but	O
where	O
we	O
have	O
conditioned	O
on	O
the	O
value	O
of	O
variable	O
c	O
c	O
a	O
b	O
where	O
denotes	O
the	O
empty	O
set	O
and	O
the	O
symbol	O
means	O
that	O
the	O
conditional	B
independence	I
property	O
does	O
not	O
hold	O
in	O
general	O
of	O
course	O
it	O
may	O
hold	O
for	O
a	O
particular	O
distribution	O
by	O
virtue	O
of	O
the	O
specific	O
numerical	O
values	O
associated	O
with	O
the	O
various	O
conditional	B
probabilities	O
but	O
it	O
does	O
not	O
follow	O
in	O
general	O
from	O
the	O
structure	O
of	O
the	O
graph	O
now	O
suppose	O
we	O
condition	O
on	O
the	O
variable	O
c	O
as	O
represented	O
by	O
the	O
graph	O
of	O
figure	O
from	O
we	O
can	O
easily	O
write	O
down	O
the	O
conditional	B
distribution	O
of	O
a	O
and	O
b	O
given	O
c	O
in	O
the	O
form	O
pa	O
bc	O
pa	O
b	O
c	O
pc	O
pacpbc	O
and	O
so	O
we	O
obtain	O
the	O
conditional	B
independence	I
property	O
a	O
b	O
c	O
we	O
can	O
provide	O
a	O
simple	O
graphical	O
interpretation	O
of	O
this	O
result	O
by	O
considering	O
the	O
path	O
from	O
node	B
a	O
to	O
node	B
b	O
via	O
c	O
the	O
node	B
c	O
is	O
said	O
to	O
be	O
tail-to-tail	O
with	O
respect	O
to	O
this	O
path	O
because	O
the	O
node	B
is	O
connected	O
to	O
the	O
tails	O
of	O
the	O
two	O
arrows	O
and	O
the	O
presence	O
of	O
such	O
a	O
path	O
connecting	O
nodes	O
a	O
and	O
b	O
causes	O
these	O
nodes	O
to	O
be	O
dependent	O
however	O
when	O
we	O
condition	O
on	O
node	B
c	O
as	O
in	O
figure	O
the	O
conditioned	O
node	B
blocks	O
the	O
path	O
from	O
a	O
to	O
b	O
and	O
causes	O
a	O
and	O
b	O
to	O
become	O
independent	B
we	O
can	O
similarly	O
consider	O
the	O
graph	O
shown	O
in	O
figure	O
the	O
joint	O
distribution	O
corresponding	O
to	O
this	O
graph	O
is	O
again	O
obtained	O
from	O
our	O
general	O
formula	O
to	O
give	O
pa	O
b	O
c	O
papcapbc	O
first	O
of	O
all	O
suppose	O
that	O
none	O
of	O
the	O
variables	O
are	O
observed	O
again	O
we	O
can	O
test	O
to	O
see	O
if	O
a	O
and	O
b	O
are	O
independent	B
by	O
marginalizing	O
over	O
c	O
to	O
give	O
pa	O
b	O
pa	O
pcapbc	O
papba	O
c	O
figure	O
the	O
second	O
of	O
our	O
three	O
examples	O
of	O
graphs	O
used	O
to	O
motivate	O
the	O
conditional	B
independence	I
framework	O
for	O
directed	B
graphical	O
models	O
a	O
c	O
b	O
figure	O
as	O
in	O
figure	O
but	O
now	O
conditioning	O
on	O
node	B
c	O
a	O
c	O
conditional	B
independence	I
which	O
in	O
general	O
does	O
not	O
factorize	O
into	O
papb	O
and	O
so	O
a	O
b	O
b	O
as	O
before	O
now	O
suppose	O
we	O
condition	O
on	O
node	B
c	O
as	O
shown	O
in	O
figure	O
using	O
bayes	B
theorem	O
together	O
with	O
we	O
obtain	O
pa	O
bc	O
pa	O
b	O
c	O
pc	O
papcapbc	O
pc	O
pacpbc	O
and	O
so	O
again	O
we	O
obtain	O
the	O
conditional	B
independence	I
property	O
a	O
b	O
c	O
as	O
before	O
we	O
can	O
interpret	O
these	O
results	O
graphically	O
the	O
node	B
c	O
is	O
said	O
to	O
be	O
head-to-tail	O
with	O
respect	O
to	O
the	O
path	O
from	O
node	B
a	O
to	O
node	B
b	O
such	O
a	O
path	O
connects	O
nodes	O
a	O
and	O
b	O
and	O
renders	O
them	O
dependent	O
if	O
we	O
now	O
observe	O
c	O
as	O
in	O
figure	O
then	O
this	O
observation	O
blocks	O
the	O
path	O
from	O
a	O
to	O
b	O
and	O
so	O
we	O
obtain	O
the	O
conditional	B
independence	I
property	O
a	O
b	O
c	O
finally	O
we	O
consider	O
the	O
third	O
of	O
our	O
examples	O
shown	O
by	O
the	O
graph	O
in	O
figure	O
as	O
we	O
shall	O
see	O
this	O
has	O
a	O
more	O
subtle	O
behaviour	O
than	O
the	O
two	O
previous	O
graphs	O
the	O
joint	O
distribution	O
can	O
again	O
be	O
written	O
down	O
using	O
our	O
general	O
result	O
to	O
give	O
pa	O
b	O
c	O
papbpca	O
b	O
consider	O
first	O
the	O
case	O
where	O
none	O
of	O
the	O
variables	O
are	O
observed	O
marginalizing	O
both	O
sides	O
of	O
over	O
c	O
we	O
obtain	O
pa	O
b	O
papb	O
figure	O
the	O
last	O
of	O
our	O
three	O
examples	O
of	O
graphs	O
used	O
to	O
explore	O
conditional	B
independence	I
properties	O
in	O
graphical	O
models	O
this	O
graph	O
has	O
rather	O
different	O
properties	O
from	O
the	O
two	O
previous	O
examples	O
a	O
b	O
c	O
graphical	O
models	O
figure	O
as	O
in	O
figure	O
but	O
conditioning	O
on	O
the	O
value	O
of	O
node	B
c	O
in	O
this	O
graph	O
the	O
act	O
of	O
conditioning	O
induces	O
a	O
dependence	O
between	O
a	O
and	O
b	O
a	O
b	O
c	O
and	O
so	O
a	O
and	O
b	O
are	O
independent	B
with	O
no	O
variables	O
observed	O
in	O
contrast	O
to	O
the	O
two	O
previous	O
examples	O
we	O
can	O
write	O
this	O
result	O
as	O
a	O
b	O
now	O
suppose	O
we	O
condition	O
on	O
c	O
as	O
indicated	O
in	O
figure	O
the	O
conditional	B
distribution	O
of	O
a	O
and	O
b	O
is	O
then	O
given	O
by	O
pa	O
bc	O
pa	O
b	O
c	O
pc	O
papbpca	O
b	O
pc	O
exercise	O
which	O
in	O
general	O
does	O
not	O
factorize	O
into	O
the	O
product	O
papb	O
and	O
so	O
a	O
b	O
c	O
thus	O
our	O
third	O
example	O
has	O
the	O
opposite	O
behaviour	O
from	O
the	O
first	O
two	O
graphically	O
we	O
say	O
that	O
node	B
c	O
is	O
head-to-head	O
with	O
respect	O
to	O
the	O
path	O
from	O
a	O
to	O
b	O
because	O
it	O
connects	O
to	O
the	O
heads	O
of	O
the	O
two	O
arrows	O
when	O
node	B
c	O
is	O
unobserved	O
it	O
blocks	O
the	O
path	O
and	O
the	O
variables	O
a	O
and	O
b	O
are	O
independent	B
however	O
conditioning	O
on	O
c	O
unblocks	O
the	O
path	O
and	O
renders	O
a	O
and	O
b	O
dependent	O
there	O
is	O
one	O
more	O
subtlety	O
associated	O
with	O
this	O
third	O
example	O
that	O
we	O
need	O
to	O
consider	O
first	O
we	O
introduce	O
some	O
more	O
terminology	O
we	O
say	O
that	O
node	B
y	O
is	O
a	O
descendant	O
of	O
node	B
x	O
if	O
there	O
is	O
a	O
path	O
from	O
x	O
to	O
y	O
in	O
which	O
each	O
step	O
of	O
the	O
path	O
follows	O
the	O
directions	O
of	O
the	O
arrows	O
then	O
it	O
can	O
be	O
shown	O
that	O
a	O
head-to-head	B
path	I
will	O
become	O
unblocked	O
if	O
either	O
the	O
node	B
or	O
any	O
of	O
its	O
descendants	O
is	O
observed	O
in	O
summary	O
a	O
tail-to-tail	O
node	B
or	O
a	O
head-to-tail	O
node	B
leaves	O
a	O
path	O
unblocked	O
unless	O
it	O
is	O
observed	O
in	O
which	O
case	O
it	O
blocks	O
the	O
path	O
by	O
contrast	O
a	O
head-to-head	O
node	B
blocks	O
a	O
path	O
if	O
it	O
is	O
unobserved	O
but	O
once	O
the	O
node	B
andor	O
at	O
least	O
one	O
of	O
its	O
descendants	O
is	O
observed	O
the	O
path	O
becomes	O
unblocked	O
it	O
is	O
worth	O
spending	O
a	O
moment	O
to	O
understand	O
further	O
the	O
unusual	O
behaviour	O
of	O
the	O
graph	O
of	O
figure	O
consider	O
a	O
particular	O
instance	O
of	O
such	O
a	O
graph	O
corresponding	O
to	O
a	O
problem	O
with	O
three	O
binary	O
random	O
variables	O
relating	O
to	O
the	O
fuel	B
system	I
on	O
a	O
car	O
as	O
shown	O
in	O
figure	O
the	O
variables	O
are	O
called	O
b	O
representing	O
the	O
state	O
of	O
a	O
battery	O
that	O
is	O
either	O
charged	O
or	O
flat	O
f	O
representing	O
the	O
state	O
of	O
the	O
fuel	O
tank	O
that	O
is	O
either	O
full	O
of	O
fuel	O
or	O
empty	O
and	O
g	O
which	O
is	O
the	O
state	O
of	O
an	O
electric	O
fuel	O
gauge	O
and	O
which	O
indicates	O
either	O
full	O
or	O
empty	O
b	O
f	O
b	O
f	O
b	O
f	O
conditional	B
independence	I
g	O
g	O
g	O
figure	O
an	O
example	O
of	O
a	O
graph	O
used	O
to	O
illustrate	O
the	O
phenomenon	O
of	O
explaining	B
away	I
the	O
three	O
nodes	O
represent	O
the	O
state	O
of	O
the	O
battery	O
the	O
state	O
of	O
the	O
fuel	O
tank	O
and	O
the	O
reading	O
on	O
the	O
electric	O
fuel	O
gauge	O
see	O
the	O
text	O
for	O
details	O
the	O
battery	O
is	O
either	O
charged	O
or	O
flat	O
and	O
independently	O
the	O
fuel	O
tank	O
is	O
either	O
full	O
or	O
empty	O
with	O
prior	B
probabilities	O
pb	O
pf	O
given	O
the	O
state	O
of	O
the	O
fuel	O
tank	O
and	O
the	O
battery	O
the	O
fuel	O
gauge	O
reads	O
full	O
with	O
probabilities	O
given	O
by	O
pg	O
f	O
pg	O
f	O
pg	O
f	O
pg	O
f	O
so	O
this	O
is	O
a	O
rather	O
unreliable	O
fuel	O
gauge	O
all	O
remaining	O
probabilities	O
are	O
determined	O
by	O
the	O
requirement	O
that	O
probabilities	O
sum	O
to	O
one	O
and	O
so	O
we	O
have	O
a	O
complete	O
specification	O
of	O
the	O
probabilistic	O
model	O
before	O
we	O
observe	O
any	O
data	O
the	O
prior	B
probability	B
of	O
the	O
fuel	O
tank	O
being	O
empty	O
is	O
pf	O
now	O
suppose	O
that	O
we	O
observe	O
the	O
fuel	O
gauge	O
and	O
discover	O
that	O
it	O
reads	O
empty	O
i	O
e	O
g	O
corresponding	O
to	O
the	O
middle	O
graph	O
in	O
figure	O
we	O
can	O
use	O
bayes	B
theorem	O
to	O
evaluate	O
the	O
posterior	B
probability	B
of	O
the	O
fuel	O
tank	O
being	O
empty	O
first	O
we	O
evaluate	O
the	O
denominator	O
for	O
bayes	B
theorem	O
given	O
by	O
pg	O
f	O
pg	O
b	O
f	O
b	O
and	O
similarly	O
we	O
evaluate	O
pg	O
pg	O
f	O
and	O
using	O
these	O
results	O
we	O
have	O
pf	O
pg	O
pg	O
graphical	O
models	O
and	O
so	O
pf	O
pf	O
thus	O
observing	O
that	O
the	O
gauge	O
reads	O
empty	O
makes	O
it	O
more	O
likely	O
that	O
the	O
tank	O
is	O
indeed	O
empty	O
as	O
we	O
would	O
intuitively	O
expect	O
next	O
suppose	O
that	O
we	O
also	O
check	O
the	O
state	O
of	O
the	O
battery	O
and	O
find	O
that	O
it	O
is	O
flat	O
i	O
e	O
b	O
we	O
have	O
now	O
observed	O
the	O
states	O
of	O
both	O
the	O
fuel	O
gauge	O
and	O
the	O
battery	O
as	O
shown	O
by	O
the	O
right-hand	O
graph	O
in	O
figure	O
the	O
posterior	B
probability	B
that	O
the	O
fuel	O
tank	O
is	O
empty	O
given	O
the	O
observations	O
of	O
both	O
the	O
fuel	O
gauge	O
and	O
the	O
battery	O
state	O
is	O
then	O
given	O
by	O
pf	O
b	O
pg	O
f	O
f	O
pg	O
f	O
where	O
the	O
prior	B
probability	B
pb	O
has	O
cancelled	O
between	O
numerator	O
and	O
denominator	O
thus	O
the	O
probability	B
that	O
the	O
tank	O
is	O
empty	O
has	O
decreased	O
to	O
as	O
a	O
result	O
of	O
the	O
observation	O
of	O
the	O
state	O
of	O
the	O
battery	O
this	O
accords	O
with	O
our	O
intuition	O
that	O
finding	O
out	O
that	O
the	O
battery	O
is	O
flat	O
explains	O
away	O
the	O
observation	O
that	O
the	O
fuel	O
gauge	O
reads	O
empty	O
we	O
see	O
that	O
the	O
state	O
of	O
the	O
fuel	O
tank	O
and	O
that	O
of	O
the	O
battery	O
have	O
indeed	O
become	O
dependent	O
on	O
each	O
other	O
as	O
a	O
result	O
of	O
observing	O
the	O
reading	O
on	O
the	O
fuel	O
gauge	O
in	O
fact	O
this	O
would	O
also	O
be	O
the	O
case	O
if	O
instead	O
of	O
observing	O
the	O
fuel	O
gauge	O
directly	O
we	O
observed	O
the	O
state	O
of	O
some	O
descendant	O
of	O
g	O
note	O
that	O
the	O
probability	B
pf	O
b	O
is	O
greater	O
than	O
the	O
prior	B
probability	B
pf	O
because	O
the	O
observation	O
that	O
the	O
fuel	O
gauge	O
reads	O
zero	O
still	O
provides	O
some	O
evidence	O
in	O
favour	O
of	O
an	O
empty	O
fuel	O
tank	O
d-separation	B
we	O
now	O
give	O
a	O
general	O
statement	O
of	O
the	O
d-separation	B
property	O
for	O
directed	B
graphs	O
consider	O
a	O
general	O
directed	B
graph	O
in	O
which	O
a	O
b	O
and	O
c	O
are	O
arbitrary	O
nonintersecting	O
sets	O
of	O
nodes	O
union	O
may	O
be	O
smaller	O
than	O
the	O
complete	O
set	O
of	O
nodes	O
in	O
the	O
graph	O
we	O
wish	O
to	O
ascertain	O
whether	O
a	O
particular	O
conditional	B
independence	I
statement	O
a	O
b	O
c	O
is	O
implied	O
by	O
a	O
given	O
directed	B
acyclic	I
graph	I
to	O
do	O
so	O
we	O
consider	O
all	O
possible	O
paths	O
from	O
any	O
node	B
in	O
a	O
to	O
any	O
node	B
in	O
b	O
any	O
such	O
path	O
is	O
said	O
to	O
be	O
blocked	O
if	O
it	O
includes	O
a	O
node	B
such	O
that	O
either	O
the	O
arrows	O
on	O
the	O
path	O
meet	O
either	O
head-to-tail	O
or	O
tail-to-tail	O
at	O
the	O
node	B
and	O
the	O
node	B
is	O
in	O
the	O
set	O
c	O
or	O
the	O
arrows	O
meet	O
head-to-head	O
at	O
the	O
node	B
and	O
neither	O
the	O
node	B
nor	O
any	O
of	O
its	O
descendants	O
is	O
in	O
the	O
set	O
c	O
if	O
all	O
paths	O
are	O
blocked	O
then	O
a	O
is	O
said	O
to	O
be	O
d-separated	O
from	O
b	O
by	O
c	O
and	O
the	O
joint	O
distribution	O
over	O
all	O
of	O
the	O
variables	O
in	O
the	O
graph	O
will	O
satisfy	O
a	O
b	O
c	O
the	O
concept	O
of	O
d-separation	B
is	O
illustrated	O
in	O
figure	O
in	O
graph	O
the	O
path	O
from	O
a	O
to	O
b	O
is	O
not	O
blocked	O
by	O
node	B
f	O
because	O
it	O
is	O
a	O
tail-to-tail	O
node	B
for	O
this	O
path	O
and	O
is	O
not	O
observed	O
nor	O
is	O
it	O
blocked	O
by	O
node	B
e	O
because	O
although	O
the	O
latter	O
is	O
a	O
head-to-head	O
node	B
it	O
has	O
a	O
descendant	O
c	O
because	O
is	O
in	O
the	O
conditioning	O
set	O
thus	O
the	O
conditional	B
independence	I
statement	O
a	O
b	O
c	O
does	O
not	O
follow	O
from	O
this	O
graph	O
in	O
graph	O
the	O
path	O
from	O
a	O
to	O
b	O
is	O
blocked	O
by	O
node	B
f	O
because	O
this	O
is	O
a	O
tail-to-tail	O
node	B
that	O
is	O
observed	O
and	O
so	O
the	O
conditional	B
independence	I
property	O
a	O
b	O
f	O
will	O
figure	O
illustration	O
of	O
the	O
concept	O
of	O
d-separation	B
see	O
the	O
text	O
for	O
details	O
a	O
f	O
a	O
f	O
conditional	B
independence	I
b	O
e	O
c	O
b	O
e	O
c	O
be	O
satisfied	O
by	O
any	O
distribution	O
that	O
factorizes	O
according	O
to	O
this	O
graph	O
note	O
that	O
this	O
path	O
is	O
also	O
blocked	O
by	O
node	B
e	O
because	O
e	O
is	O
a	O
head-to-head	O
node	B
and	O
neither	O
it	O
nor	O
its	O
descendant	O
are	O
in	O
the	O
conditioning	O
set	O
for	O
the	O
purposes	O
of	O
d-separation	B
parameters	O
such	O
as	O
and	O
in	O
figure	O
indicated	O
by	O
small	O
filled	O
circles	O
behave	O
in	O
the	O
same	O
was	O
as	O
observed	O
nodes	O
however	O
there	O
are	O
no	O
marginal	B
distributions	O
associated	O
with	O
such	O
nodes	O
consequently	O
parameter	O
nodes	O
never	O
themselves	O
have	O
parents	O
and	O
so	O
all	O
paths	O
through	O
these	O
nodes	O
will	O
always	O
be	O
tail-to-tail	O
and	O
hence	O
blocked	O
consequently	O
they	O
play	O
no	O
role	O
in	O
d-separation	B
section	O
another	O
example	O
of	O
conditional	B
independence	I
and	O
d-separation	B
is	O
provided	O
by	O
the	O
concept	O
of	O
i	O
i	O
d	O
identically	O
distributed	O
data	O
introduced	O
in	O
section	O
consider	O
the	O
problem	O
of	O
finding	O
the	O
posterior	O
distribution	O
for	O
the	O
mean	B
of	O
a	O
univariate	O
gaussian	B
distribution	O
this	O
can	O
be	O
represented	O
by	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
in	O
which	O
the	O
joint	O
distribution	O
is	O
defined	O
by	O
a	O
prior	B
p	O
together	O
with	O
a	O
set	O
of	O
conditional	B
distributions	O
pxn	O
for	O
n	O
n	O
in	O
practice	O
we	O
observe	O
d	O
xn	O
and	O
our	O
goal	O
is	O
to	O
infer	O
suppose	O
for	O
a	O
moment	O
that	O
we	O
condition	O
on	O
and	O
consider	O
the	O
joint	O
distribution	O
of	O
the	O
observations	O
using	O
d-separation	B
we	O
note	O
that	O
there	O
is	O
a	O
unique	O
path	O
from	O
any	O
xi	O
to	O
any	O
other	O
and	O
that	O
this	O
path	O
is	O
tail-to-tail	O
with	O
respect	O
to	O
the	O
observed	O
node	B
every	O
such	O
path	O
is	O
blocked	O
and	O
so	O
the	O
observations	O
d	O
xn	O
are	O
independent	B
given	O
so	O
that	O
figure	O
directed	B
graph	O
corresponding	O
to	O
the	O
problem	O
of	O
inferring	O
the	O
mean	B
of	O
a	O
univariate	O
gaussian	B
distribution	O
from	O
observations	O
xn	O
the	O
same	O
graph	O
drawn	O
using	O
the	O
plate	B
notation	O
pd	O
pxn	O
xn	O
xn	O
n	O
n	O
graphical	O
models	O
figure	O
a	O
graphical	O
representation	O
of	O
the	O
naive	O
bayes	B
conditioned	O
on	O
the	O
model	O
class	O
label	O
z	O
the	O
components	O
of	O
the	O
observed	O
vector	O
x	O
xdt	O
are	O
assumed	O
to	O
be	O
independent	B
for	O
classification	B
z	O
xd	O
however	O
if	O
we	O
integrate	O
over	O
the	O
observations	O
are	O
in	O
general	O
no	O
longer	O
independent	B
pd	O
pd	O
d	O
pxn	O
section	O
independence	O
property	O
here	O
is	O
a	O
latent	B
variable	I
because	O
its	O
value	O
is	O
not	O
observed	O
another	O
example	O
of	O
a	O
model	O
representing	O
i	O
i	O
d	O
data	O
is	O
the	O
graph	O
in	O
figure	O
corresponding	O
to	O
bayesian	B
polynomial	O
regression	B
here	O
the	O
stochastic	B
nodes	O
corre	O
spond	O
to	O
w	O
we	O
see	O
that	O
the	O
node	B
for	O
w	O
is	O
tail-to-tail	O
with	O
respect	O
to	O
the	O
path	O
to	O
any	O
one	O
of	O
the	O
nodes	O
tn	O
and	O
so	O
we	O
have	O
the	O
following	O
conditional	B
tn	O
w	O
is	O
independent	B
of	O
the	O
training	B
data	O
tn	O
we	O
can	O
therefore	O
first	O
use	O
the	O
predictions	O
for	O
new	O
input	O
thus	O
conditioned	O
on	O
the	O
polynomial	O
coefficients	O
w	O
the	O
predictive	B
distribution	I
for	O
training	B
data	O
to	O
determine	O
the	O
posterior	O
distribution	O
over	O
the	O
coefficients	O
w	O
and	O
then	O
we	O
can	O
discard	O
the	O
training	B
data	O
and	O
use	O
the	O
posterior	O
distribution	O
for	O
w	O
to	O
make	O
a	O
related	O
graphical	O
structure	O
arises	O
in	O
an	O
approach	O
to	O
classification	B
called	O
the	O
naive	B
bayes	B
model	I
in	O
which	O
we	O
use	O
conditional	B
independence	I
assumptions	O
to	O
simplify	O
the	O
model	O
structure	O
suppose	O
our	O
observed	B
variable	I
consists	O
of	O
a	O
d-dimensional	O
vector	O
x	O
xdt	O
and	O
we	O
wish	O
to	O
assign	O
observed	O
values	O
of	O
x	O
to	O
one	O
of	O
k	O
classes	O
using	O
the	O
encoding	O
scheme	O
we	O
can	O
represent	O
these	O
classes	O
by	O
a	O
kdimensional	O
binary	O
vector	O
z	O
we	O
can	O
then	O
define	O
a	O
generative	B
model	I
by	O
introducing	O
a	O
multinomial	O
prior	B
pz	O
over	O
the	O
class	O
labels	O
where	O
the	O
kth	O
component	O
k	O
of	O
is	O
the	O
prior	B
probability	B
of	O
class	O
ck	O
together	O
with	O
a	O
conditional	B
distribution	O
pxz	O
for	O
the	O
observed	O
vector	O
x	O
the	O
key	O
assumption	O
of	O
the	O
naive	B
bayes	B
model	I
is	O
that	O
conditioned	O
on	O
the	O
class	O
z	O
the	O
distributions	O
of	O
the	O
input	O
variables	O
xd	O
are	O
independent	B
the	O
graphical	O
representation	O
of	O
this	O
model	O
is	O
shown	O
in	O
figure	O
we	O
see	O
that	O
observation	O
of	O
z	O
blocks	O
the	O
path	O
between	O
xi	O
and	O
xj	O
for	O
j	O
i	O
such	O
paths	O
are	O
tail-to-tail	O
at	O
the	O
node	B
z	O
and	O
so	O
xi	O
and	O
xj	O
are	O
conditionally	O
independent	B
given	O
z	O
if	O
however	O
we	O
marginalize	O
out	O
z	O
that	O
z	O
is	O
unobserved	O
the	O
tail-to-tail	B
path	I
from	O
xi	O
to	O
xj	O
is	O
no	O
longer	O
blocked	O
this	O
tells	O
us	O
that	O
in	O
general	O
the	O
marginal	B
density	B
px	O
will	O
not	O
factorize	O
with	O
respect	O
to	O
the	O
components	O
of	O
x	O
we	O
encountered	O
a	O
simple	O
application	O
of	O
the	O
naive	B
bayes	B
model	I
in	O
the	O
context	O
of	O
fusing	O
data	O
from	O
different	O
sources	O
for	O
medical	O
diagnosis	O
in	O
section	O
if	O
we	O
are	O
given	O
a	O
labelled	O
training	B
set	I
comprising	O
inputs	O
xn	O
together	O
with	O
their	O
class	O
labels	O
then	O
we	O
can	O
fit	O
the	O
naive	B
bayes	B
model	I
to	O
the	O
training	B
data	O
conditional	B
independence	I
using	O
maximum	B
likelihood	I
assuming	O
that	O
the	O
data	O
are	O
drawn	O
independently	O
from	O
the	O
model	O
the	O
solution	O
is	O
obtained	O
by	O
fitting	O
the	O
model	O
for	O
each	O
class	O
separately	O
using	O
the	O
correspondingly	O
labelled	O
data	O
as	O
an	O
example	O
suppose	O
that	O
the	O
probability	B
density	B
within	O
each	O
class	O
is	O
chosen	O
to	O
be	O
gaussian	B
in	O
this	O
case	O
the	O
naive	O
bayes	B
assumption	O
then	O
implies	O
that	O
the	O
covariance	B
matrix	O
for	O
each	O
gaussian	B
is	O
diagonal	B
and	O
the	O
contours	O
of	O
constant	O
density	B
within	O
each	O
class	O
will	O
be	O
axis-aligned	O
ellipsoids	O
the	O
marginal	B
density	B
however	O
is	O
given	O
by	O
a	O
superposition	O
of	O
diagonal	B
gaussians	O
weighting	O
coefficients	O
given	O
by	O
the	O
class	O
priors	O
and	O
so	O
will	O
no	O
longer	O
factorize	O
with	O
respect	O
to	O
its	O
components	O
the	O
naive	O
bayes	B
assumption	O
is	O
helpful	O
when	O
the	O
dimensionality	O
d	O
of	O
the	O
input	O
space	O
is	O
high	O
making	O
density	B
estimation	I
in	O
the	O
full	O
d-dimensional	O
space	O
more	O
challenging	O
it	O
is	O
also	O
useful	O
if	O
the	O
input	O
vector	O
contains	O
both	O
discrete	O
and	O
continuous	O
variables	O
since	O
each	O
can	O
be	O
represented	O
separately	O
using	O
appropriate	O
models	O
bernoulli	B
distributions	O
for	O
binary	O
observations	O
or	O
gaussians	O
for	O
real-valued	O
variables	O
the	O
conditional	B
independence	I
assumption	O
of	O
this	O
model	O
is	O
clearly	O
a	O
strong	O
one	O
that	O
may	O
lead	O
to	O
rather	O
poor	O
representations	O
of	O
the	O
class-conditional	O
densities	O
nevertheless	O
even	O
if	O
this	O
assumption	O
is	O
not	O
precisely	O
satisfied	O
the	O
model	O
may	O
still	O
give	O
good	O
classification	B
performance	O
in	O
practice	O
because	O
the	O
decision	O
boundaries	O
can	O
be	O
insensitive	O
to	O
some	O
of	O
the	O
details	O
in	O
the	O
class-conditional	O
densities	O
as	O
illustrated	O
in	O
figure	O
we	O
have	O
seen	O
that	O
a	O
particular	O
directed	B
graph	O
represents	O
a	O
specific	O
decomposition	O
of	O
a	O
joint	O
probability	B
distribution	O
into	O
a	O
product	O
of	O
conditional	B
probabilities	O
the	O
graph	O
also	O
expresses	O
a	O
set	O
of	O
conditional	B
independence	I
statements	O
obtained	O
through	O
the	O
d-separation	B
criterion	O
and	O
the	O
d-separation	B
theorem	O
is	O
really	O
an	O
expression	O
of	O
the	O
equivalence	O
of	O
these	O
two	O
properties	O
in	O
order	O
to	O
make	O
this	O
clear	O
it	O
is	O
helpful	O
to	O
think	O
of	O
a	O
directed	B
graph	O
as	O
a	O
filter	O
suppose	O
we	O
consider	O
a	O
particular	O
joint	O
probability	B
distribution	O
px	O
over	O
the	O
variables	O
x	O
corresponding	O
to	O
the	O
nodes	O
of	O
the	O
graph	O
the	O
filter	O
will	O
allow	O
this	O
distribution	O
to	O
pass	O
through	O
if	O
and	O
only	O
if	O
it	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
factorization	B
implied	O
by	O
the	O
graph	O
if	O
we	O
present	O
to	O
the	O
filter	O
the	O
set	O
of	O
all	O
possible	O
distributions	O
px	O
over	O
the	O
set	O
of	O
variables	O
x	O
then	O
the	O
subset	O
of	O
distributions	O
that	O
are	O
passed	O
by	O
the	O
filter	O
will	O
be	O
denoted	O
df	O
for	O
directed	B
factorization	B
this	O
is	O
illustrated	O
in	O
figure	O
alternatively	O
we	O
can	O
use	O
the	O
graph	O
as	O
a	O
different	O
kind	O
of	O
filter	O
by	O
first	O
listing	O
all	O
of	O
the	O
conditional	B
independence	I
properties	O
obtained	O
by	O
applying	O
the	O
d-separation	B
criterion	O
to	O
the	O
graph	O
and	O
then	O
allowing	O
a	O
distribution	O
to	O
pass	O
only	O
if	O
it	O
satisfies	O
all	O
of	O
these	O
properties	O
if	O
we	O
present	O
all	O
possible	O
distributions	O
px	O
to	O
this	O
second	O
kind	O
of	O
filter	O
then	O
the	O
d-separation	B
theorem	O
tells	O
us	O
that	O
the	O
set	O
of	O
distributions	O
that	O
will	O
be	O
allowed	O
through	O
is	O
precisely	O
the	O
set	O
df	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
conditional	B
independence	I
properties	O
obtained	O
from	O
d-separation	B
apply	O
to	O
any	O
probabilistic	O
model	O
described	O
by	O
that	O
particular	O
directed	B
graph	O
this	O
will	O
be	O
true	O
for	O
instance	O
whether	O
the	O
variables	O
are	O
discrete	O
or	O
continuous	O
or	O
a	O
combination	O
of	O
these	O
again	O
we	O
see	O
that	O
a	O
particular	O
graph	O
is	O
describing	O
a	O
whole	O
family	O
of	O
probability	B
distributions	O
at	O
one	O
extreme	O
we	O
have	O
a	O
fully	B
connected	I
graph	O
that	O
exhibits	O
no	O
conditional	B
independence	I
properties	O
at	O
all	O
and	O
which	O
can	O
represent	O
any	O
possible	O
joint	O
probability	B
distribution	O
over	O
the	O
given	O
variables	O
the	O
set	O
df	O
will	O
contain	O
all	O
possible	O
distribu	O
graphical	O
models	O
px	O
df	O
figure	O
we	O
can	O
view	O
a	O
graphical	B
model	I
this	O
case	O
a	O
directed	B
graph	O
as	O
a	O
filter	O
in	O
which	O
a	O
probability	B
distribution	O
px	O
is	O
allowed	O
through	O
the	O
filter	O
if	O
and	O
only	O
if	O
it	O
satisfies	O
the	O
directed	B
factorization	B
property	O
the	O
set	O
of	O
all	O
possible	O
probability	B
distributions	O
px	O
that	O
pass	O
through	O
the	O
filter	O
is	O
denoted	O
df	O
we	O
can	O
alternatively	O
use	O
the	O
graph	O
to	O
filter	O
distributions	O
according	O
to	O
whether	O
they	O
respect	O
all	O
of	O
the	O
conditional	B
independencies	O
implied	O
by	O
the	O
d-separation	B
properties	O
of	O
the	O
graph	O
the	O
d-separation	B
theorem	O
says	O
that	O
it	O
is	O
the	O
same	O
set	O
of	O
distributions	O
df	O
that	O
will	O
be	O
allowed	O
through	O
this	O
second	O
kind	O
of	O
filter	O
tions	O
px	O
at	O
the	O
other	O
extreme	O
we	O
have	O
the	O
fully	O
disconnected	O
graph	O
i	O
e	O
one	O
having	O
no	O
links	O
at	O
all	O
this	O
corresponds	O
to	O
joint	O
distributions	O
which	O
factorize	O
into	O
the	O
product	O
of	O
the	O
marginal	B
distributions	O
over	O
the	O
variables	O
comprising	O
the	O
nodes	O
of	O
the	O
graph	O
note	O
that	O
for	O
any	O
given	O
graph	O
the	O
set	O
of	O
distributions	O
df	O
will	O
include	O
any	O
distributions	O
that	O
have	O
additional	O
independence	O
properties	O
beyond	O
those	O
described	O
by	O
the	O
graph	O
for	O
instance	O
a	O
fully	O
factorized	B
distribution	I
will	O
always	O
be	O
passed	O
through	O
the	O
filter	O
implied	O
by	O
any	O
graph	O
over	O
the	O
corresponding	O
set	O
of	O
variables	O
we	O
end	O
our	O
discussion	O
of	O
conditional	B
independence	I
properties	O
by	O
exploring	O
the	O
concept	O
of	O
a	O
markov	B
blanket	I
or	O
markov	O
boundary	O
consider	O
a	O
joint	O
distribution	O
xd	O
represented	O
by	O
a	O
directed	B
graph	O
having	O
d	O
nodes	O
and	O
consider	O
the	O
conditional	B
distribution	O
of	O
a	O
particular	O
node	B
with	O
variables	O
xi	O
conditioned	O
on	O
all	O
of	O
the	O
remaining	O
variables	O
using	O
the	O
factorization	B
property	O
we	O
can	O
express	O
this	O
conditional	B
distribution	O
in	O
the	O
form	O
xd	O
k	O
xd	O
dxi	O
pxkpak	O
pxkpak	O
dxi	O
k	O
in	O
which	O
the	O
integral	O
is	O
replaced	O
by	O
a	O
summation	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
we	O
now	O
observe	O
that	O
any	O
factor	O
pxkpak	O
that	O
does	O
not	O
have	O
any	O
functional	B
dependence	O
on	O
xi	O
can	O
be	O
taken	O
outside	O
the	O
integral	O
over	O
xi	O
and	O
will	O
therefore	O
cancel	O
between	O
numerator	O
and	O
denominator	O
the	O
only	O
factors	O
that	O
remain	O
will	O
be	O
the	O
conditional	B
distribution	O
pxipai	O
for	O
node	B
xi	O
itself	O
together	O
with	O
the	O
conditional	B
distributions	O
for	O
any	O
nodes	O
xk	O
such	O
that	O
node	B
xi	O
is	O
in	O
the	O
conditioning	O
set	O
of	O
pxkpak	O
in	O
other	O
words	O
for	O
which	O
xi	O
is	O
a	O
parent	O
of	O
xk	O
the	O
conditional	B
pxipai	O
will	O
depend	O
on	O
the	O
parents	O
of	O
node	B
xi	O
whereas	O
the	O
conditionals	O
pxkpak	O
will	O
depend	O
on	O
the	O
children	O
markov	O
random	O
fields	O
figure	O
the	O
markov	B
blanket	I
of	O
a	O
node	B
xi	O
comprises	O
the	O
set	O
of	O
parents	O
children	O
and	O
co-parents	B
of	O
the	O
node	B
it	O
has	O
the	O
property	O
that	O
the	O
conditional	B
distribution	O
of	O
xi	O
conditioned	O
on	O
all	O
the	O
remaining	O
variables	O
in	O
the	O
graph	O
is	O
dependent	O
only	O
on	O
the	O
variables	O
in	O
the	O
markov	B
blanket	I
xi	O
of	O
xi	O
as	O
well	O
as	O
on	O
the	O
co-parents	B
in	O
other	O
words	O
variables	O
corresponding	O
to	O
parents	O
of	O
node	B
xk	O
other	O
than	O
node	B
xi	O
the	O
set	O
of	O
nodes	O
comprising	O
the	O
parents	O
the	O
children	O
and	O
the	O
co-parents	B
is	O
called	O
the	O
markov	B
blanket	I
and	O
is	O
illustrated	O
in	O
figure	O
we	O
can	O
think	O
of	O
the	O
markov	B
blanket	I
of	O
a	O
node	B
xi	O
as	O
being	O
the	O
minimal	O
set	O
of	O
nodes	O
that	O
isolates	O
xi	O
from	O
the	O
rest	O
of	O
the	O
graph	O
note	O
that	O
it	O
is	O
not	O
sufficient	O
to	O
include	O
only	O
the	O
parents	O
and	O
children	O
of	O
node	B
xi	O
because	O
the	O
phenomenon	O
of	O
explaining	B
away	I
means	O
that	O
observations	O
of	O
the	O
child	O
nodes	O
will	O
not	O
block	O
paths	O
to	O
the	O
co-parents	B
we	O
must	O
therefore	O
observe	O
the	O
co-parent	O
nodes	O
also	O
markov	O
random	O
fields	O
we	O
have	O
seen	O
that	O
directed	B
graphical	O
models	O
specify	O
a	O
factorization	B
of	O
the	O
joint	O
distribution	O
over	O
a	O
set	O
of	O
variables	O
into	O
a	O
product	O
of	O
local	B
conditional	B
distributions	O
they	O
also	O
define	O
a	O
set	O
of	O
conditional	B
independence	I
properties	O
that	O
must	O
be	O
satisfied	O
by	O
any	O
distribution	O
that	O
factorizes	O
according	O
to	O
the	O
graph	O
we	O
turn	O
now	O
to	O
the	O
second	O
major	O
class	O
of	O
graphical	O
models	O
that	O
are	O
described	O
by	O
undirected	B
graphs	O
and	O
that	O
again	O
specify	O
both	O
a	O
factorization	B
and	O
a	O
set	O
of	O
conditional	B
independence	I
relations	O
a	O
markov	B
random	I
field	I
also	O
known	O
as	O
a	O
markov	O
network	O
or	O
an	O
undirected	B
graphical	B
model	I
and	O
snell	O
has	O
a	O
set	O
of	O
nodes	O
each	O
of	O
which	O
corresponds	O
to	O
a	O
variable	O
or	O
group	O
of	O
variables	O
as	O
well	O
as	O
a	O
set	O
of	O
links	O
each	O
of	O
which	O
connects	O
a	O
pair	O
of	O
nodes	O
the	O
links	O
are	O
undirected	B
that	O
is	O
they	O
do	O
not	O
carry	O
arrows	O
in	O
the	O
case	O
of	O
undirected	B
graphs	O
it	O
is	O
convenient	O
to	O
begin	O
with	O
a	O
discussion	O
of	O
conditional	B
independence	I
properties	O
conditional	B
independence	I
properties	O
in	O
the	O
case	O
of	O
directed	B
graphs	O
we	O
saw	O
that	O
it	O
was	O
possible	O
to	O
test	O
whether	O
a	O
particular	O
conditional	B
independence	I
property	O
holds	O
by	O
applying	O
a	O
graphical	O
test	O
called	O
d-separation	B
this	O
involved	O
testing	O
whether	O
or	O
not	O
the	O
paths	O
connecting	O
two	O
sets	O
of	O
nodes	O
were	O
blocked	O
the	O
definition	O
of	O
blocked	O
however	O
was	O
somewhat	O
subtle	O
due	O
to	O
the	O
presence	O
of	O
paths	O
having	O
head-to-head	O
nodes	O
we	O
might	O
ask	O
whether	O
it	O
is	O
possible	O
to	O
define	O
an	O
alternative	O
graphical	O
semantics	O
for	O
probability	B
distributions	O
such	O
that	O
conditional	B
independence	I
is	O
determined	O
by	O
simple	O
graph	O
separation	O
this	O
is	O
indeed	O
the	O
case	O
and	O
corresponds	O
to	O
undirected	B
graphical	O
models	O
by	O
removing	O
the	O
section	O
graphical	O
models	O
figure	O
an	O
example	O
of	O
an	O
undirected	B
graph	O
in	O
which	O
every	O
path	O
from	O
any	O
node	B
in	O
set	O
a	O
to	O
any	O
node	B
in	O
set	O
b	O
passes	O
through	O
at	O
least	O
one	O
node	B
in	O
set	O
c	O
consequently	O
the	O
conditional	B
independence	I
property	O
a	O
b	O
c	O
holds	O
for	O
any	O
probability	B
distribution	O
described	O
by	O
this	O
graph	O
c	O
b	O
a	O
directionality	O
from	O
the	O
links	O
of	O
the	O
graph	O
the	O
asymmetry	O
between	O
parent	O
and	O
child	O
nodes	O
is	O
removed	O
and	O
so	O
the	O
subtleties	O
associated	O
with	O
head-to-head	O
nodes	O
no	O
longer	O
arise	O
suppose	O
that	O
in	O
an	O
undirected	B
graph	O
we	O
identify	O
three	O
sets	O
of	O
nodes	O
denoted	O
a	O
b	O
and	O
c	O
and	O
that	O
we	O
consider	O
the	O
conditional	B
independence	I
property	O
a	O
b	O
c	O
to	O
test	O
whether	O
this	O
property	O
is	O
satisfied	O
by	O
a	O
probability	B
distribution	O
defined	O
by	O
a	O
graph	O
we	O
consider	O
all	O
possible	O
paths	O
that	O
connect	O
nodes	O
in	O
set	O
a	O
to	O
nodes	O
in	O
set	O
b	O
if	O
all	O
such	O
paths	O
pass	O
through	O
one	O
or	O
more	O
nodes	O
in	O
set	O
c	O
then	O
all	O
such	O
paths	O
are	O
blocked	O
and	O
so	O
the	O
conditional	B
independence	I
property	O
holds	O
however	O
if	O
there	O
is	O
at	O
least	O
one	O
such	O
path	O
that	O
is	O
not	O
blocked	O
then	O
the	O
property	O
does	O
not	O
necessarily	O
hold	O
or	O
more	O
precisely	O
there	O
will	O
exist	O
at	O
least	O
some	O
distributions	O
corresponding	O
to	O
the	O
graph	O
that	O
do	O
not	O
satisfy	O
this	O
conditional	B
independence	I
relation	O
this	O
is	O
illustrated	O
with	O
an	O
example	O
in	O
figure	O
note	O
that	O
this	O
is	O
exactly	O
the	O
same	O
as	O
the	O
d-separation	B
criterion	O
except	O
that	O
there	O
is	O
no	O
explaining	B
away	I
phenomenon	O
testing	O
for	O
conditional	B
independence	I
in	O
undirected	B
graphs	O
is	O
therefore	O
simpler	O
than	O
in	O
directed	B
graphs	O
an	O
alternative	O
way	O
to	O
view	O
the	O
conditional	B
independence	I
test	O
is	O
to	O
imagine	O
removing	O
all	O
nodes	O
in	O
set	O
c	O
from	O
the	O
graph	O
together	O
with	O
any	O
links	O
that	O
connect	O
to	O
those	O
nodes	O
we	O
then	O
ask	O
if	O
there	O
exists	O
a	O
path	O
that	O
connects	O
any	O
node	B
in	O
a	O
to	O
any	O
node	B
in	O
b	O
if	O
there	O
are	O
no	O
such	O
paths	O
then	O
the	O
conditional	B
independence	I
property	O
must	O
hold	O
the	O
markov	B
blanket	I
for	O
an	O
undirected	B
graph	O
takes	O
a	O
particularly	O
simple	O
form	O
because	O
a	O
node	B
will	O
be	O
conditionally	O
independent	B
of	O
all	O
other	O
nodes	O
conditioned	O
only	O
on	O
the	O
neighbouring	O
nodes	O
as	O
illustrated	O
in	O
figure	O
factorization	B
properties	O
we	O
now	O
seek	O
a	O
factorization	B
rule	O
for	O
undirected	B
graphs	O
that	O
will	O
correspond	O
to	O
the	O
above	O
conditional	B
independence	I
test	O
again	O
this	O
will	O
involve	O
expressing	O
the	O
joint	O
distribution	O
px	O
as	O
a	O
product	O
of	O
functions	O
defined	O
over	O
sets	O
of	O
variables	O
that	O
are	O
local	B
to	O
the	O
graph	O
we	O
therefore	O
need	O
to	O
decide	O
what	O
is	O
the	O
appropriate	O
notion	O
of	O
locality	O
in	O
this	O
case	O
markov	O
random	O
fields	O
figure	O
for	O
an	O
undirected	B
graph	O
the	O
markov	B
blanket	I
of	O
a	O
node	B
xi	O
consists	O
of	O
the	O
set	O
of	O
neighbouring	O
nodes	O
it	O
has	O
the	O
property	O
that	O
the	O
conditional	B
distribution	O
of	O
xi	O
conditioned	O
on	O
all	O
the	O
remaining	O
variables	O
in	O
the	O
graph	O
is	O
dependent	O
only	O
on	O
the	O
variables	O
in	O
the	O
markov	B
blanket	I
if	O
we	O
consider	O
two	O
nodes	O
xi	O
and	O
xj	O
that	O
are	O
not	O
connected	O
by	O
a	O
link	B
then	O
these	O
variables	O
must	O
be	O
conditionally	O
independent	B
given	O
all	O
other	O
nodes	O
in	O
the	O
graph	O
this	O
follows	O
from	O
the	O
fact	O
that	O
there	O
is	O
no	O
direct	O
path	O
between	O
the	O
two	O
nodes	O
and	O
all	O
other	O
paths	O
pass	O
through	O
nodes	O
that	O
are	O
observed	O
and	O
hence	O
those	O
paths	O
are	O
blocked	O
this	O
conditional	B
independence	I
property	O
can	O
be	O
expressed	O
as	O
pxi	O
xjxij	O
pxixijpxjxij	O
where	O
xij	O
denotes	O
the	O
set	O
x	O
of	O
all	O
variables	O
with	O
xi	O
and	O
xj	O
removed	O
the	O
factorization	B
of	O
the	O
joint	O
distribution	O
must	O
therefore	O
be	O
such	O
that	O
xi	O
and	O
xj	O
do	O
not	O
appear	O
in	O
the	O
same	O
factor	O
in	O
order	O
for	O
the	O
conditional	B
independence	I
property	O
to	O
hold	O
for	O
all	O
possible	O
distributions	O
belonging	O
to	O
the	O
graph	O
this	O
leads	O
us	O
to	O
consider	O
a	O
graphical	O
concept	O
called	O
a	O
clique	B
which	O
is	O
defined	O
as	O
a	O
subset	O
of	O
the	O
nodes	O
in	O
a	O
graph	O
such	O
that	O
there	O
exists	O
a	O
link	B
between	O
all	O
pairs	O
of	O
nodes	O
in	O
the	O
subset	O
in	O
other	O
words	O
the	O
set	O
of	O
nodes	O
in	O
a	O
clique	B
is	O
fully	B
connected	I
furthermore	O
a	O
maximal	B
clique	B
is	O
a	O
clique	B
such	O
that	O
it	O
is	O
not	O
possible	O
to	O
include	O
any	O
other	O
nodes	O
from	O
the	O
graph	O
in	O
the	O
set	O
without	O
it	O
ceasing	O
to	O
be	O
a	O
clique	B
these	O
concepts	O
are	O
illustrated	O
by	O
the	O
undirected	B
graph	O
over	O
four	O
variables	O
shown	O
in	O
figure	O
this	O
graph	O
has	O
five	O
cliques	O
of	O
two	O
nodes	O
given	O
by	O
and	O
as	O
well	O
as	O
two	O
maximal	O
cliques	O
given	O
by	O
and	O
the	O
set	O
is	O
not	O
a	O
clique	B
because	O
of	O
the	O
missing	O
link	B
from	O
to	O
we	O
can	O
therefore	O
define	O
the	O
factors	O
in	O
the	O
decomposition	O
of	O
the	O
joint	O
distribution	O
to	O
be	O
functions	O
of	O
the	O
variables	O
in	O
the	O
cliques	O
in	O
fact	O
we	O
can	O
consider	O
functions	O
of	O
the	O
maximal	O
cliques	O
without	O
loss	O
of	O
generality	O
because	O
other	O
cliques	O
must	O
be	O
subsets	O
of	O
maximal	O
cliques	O
thus	O
if	O
is	O
a	O
maximal	B
clique	B
and	O
we	O
define	O
an	O
arbitrary	O
function	O
over	O
this	O
clique	B
then	O
including	O
another	O
factor	O
defined	O
over	O
a	O
subset	O
of	O
these	O
variables	O
would	O
be	O
redundant	O
let	O
us	O
denote	O
a	O
clique	B
by	O
c	O
and	O
the	O
set	O
of	O
variables	O
in	O
that	O
clique	B
by	O
xc	O
then	O
figure	O
a	O
four-node	O
undirected	B
graph	O
showing	O
a	O
clique	B
in	O
green	O
and	O
a	O
maximal	B
clique	B
in	O
blue	O
graphical	O
models	O
the	O
joint	O
distribution	O
is	O
written	O
as	O
a	O
product	O
of	O
potential	O
functions	O
cxc	O
over	O
the	O
maximal	O
cliques	O
of	O
the	O
graph	O
px	O
cxc	O
c	O
z	O
here	O
the	O
quantity	O
z	O
sometimes	O
called	O
the	O
partition	B
function	I
is	O
a	O
normalization	O
constant	O
and	O
is	O
given	O
by	O
z	O
cxc	O
x	O
c	O
which	O
ensures	O
that	O
the	O
distribution	O
px	O
given	O
by	O
is	O
correctly	O
normalized	O
by	O
considering	O
only	O
potential	O
functions	O
which	O
satisfy	O
cxc	O
we	O
ensure	O
that	O
px	O
in	O
we	O
have	O
assumed	O
that	O
x	O
comprises	O
discrete	O
variables	O
but	O
the	O
framework	O
is	O
equally	O
applicable	O
to	O
continuous	O
variables	O
or	O
a	O
combination	O
of	O
the	O
two	O
in	O
which	O
the	O
summation	O
is	O
replaced	O
by	O
the	O
appropriate	O
combination	O
of	O
summation	O
and	O
integration	O
note	O
that	O
we	O
do	O
not	O
restrict	O
the	O
choice	O
of	O
potential	O
functions	O
to	O
those	O
that	O
have	O
a	O
specific	O
probabilistic	O
interpretation	O
as	O
marginal	B
or	O
conditional	B
distributions	O
this	O
is	O
in	O
contrast	O
to	O
directed	B
graphs	O
in	O
which	O
each	O
factor	O
represents	O
the	O
conditional	B
distribution	O
of	O
the	O
corresponding	O
variable	O
conditioned	O
on	O
the	O
state	O
of	O
its	O
parents	O
however	O
in	O
special	O
cases	O
for	O
instance	O
where	O
the	O
undirected	B
graph	O
is	O
constructed	O
by	O
starting	O
with	O
a	O
directed	B
graph	O
the	O
potential	O
functions	O
may	O
indeed	O
have	O
such	O
an	O
interpretation	O
as	O
we	O
shall	O
see	O
shortly	O
one	O
consequence	O
of	O
the	O
generality	O
of	O
the	O
potential	O
functions	O
cxc	O
is	O
that	O
their	O
product	O
will	O
in	O
general	O
not	O
be	O
correctly	O
normalized	O
we	O
therefore	O
have	O
to	O
introduce	O
an	O
explicit	O
normalization	O
factor	O
given	O
by	O
recall	O
that	O
for	O
directed	B
graphs	O
the	O
joint	O
distribution	O
was	O
automatically	O
normalized	O
as	O
a	O
consequence	O
of	O
the	O
normalization	O
of	O
each	O
of	O
the	O
conditional	B
distributions	O
in	O
the	O
factorization	B
the	O
presence	O
of	O
this	O
normalization	O
constant	O
is	O
one	O
of	O
the	O
major	O
limitations	O
of	O
undirected	B
graphs	O
if	O
we	O
have	O
a	O
model	O
with	O
m	O
discrete	O
nodes	O
each	O
having	O
k	O
states	O
then	O
the	O
evaluation	O
of	O
the	O
normalization	O
term	O
involves	O
summing	O
over	O
km	O
states	O
and	O
so	O
the	O
worst	O
case	O
is	O
exponential	O
in	O
the	O
size	O
of	O
the	O
model	O
the	O
partition	B
function	I
is	O
needed	O
for	O
parameter	O
learning	B
because	O
it	O
will	O
be	O
a	O
function	O
of	O
any	O
parameters	O
that	O
govern	O
the	O
potential	O
functions	O
cxc	O
however	O
for	O
evaluation	O
of	O
local	B
conditional	B
distributions	O
the	O
partition	B
function	I
is	O
not	O
needed	O
because	O
a	O
conditional	B
is	O
the	O
ratio	O
of	O
two	O
marginals	O
and	O
the	O
partition	B
function	I
cancels	O
between	O
numerator	O
and	O
denominator	O
when	O
evaluating	O
this	O
ratio	O
similarly	O
for	O
evaluating	O
local	B
marginal	B
probabilities	O
we	O
can	O
work	O
with	O
the	O
unnormalized	O
joint	O
distribution	O
and	O
then	O
normalize	O
the	O
marginals	O
explicitly	O
at	O
the	O
end	O
provided	O
the	O
marginals	O
only	O
involves	O
a	O
small	O
number	O
of	O
variables	O
the	O
evaluation	O
of	O
their	O
normalization	O
coefficient	O
will	O
be	O
feasible	O
so	O
far	O
we	O
have	O
discussed	O
the	O
notion	O
of	O
conditional	B
independence	I
based	O
on	O
simple	O
graph	O
separation	O
and	O
we	O
have	O
proposed	O
a	O
factorization	B
of	O
the	O
joint	O
distribution	O
that	O
is	O
intended	O
to	O
correspond	O
to	O
this	O
conditional	B
independence	I
structure	O
however	O
we	O
have	O
not	O
made	O
any	O
formal	O
connection	O
between	O
conditional	B
independence	I
and	O
factorization	B
for	O
undirected	B
graphs	O
to	O
do	O
so	O
we	O
need	O
to	O
restrict	O
attention	O
to	O
potential	O
functions	O
cxc	O
that	O
are	O
strictly	O
positive	O
never	O
zero	O
or	O
negative	O
for	O
any	O
markov	O
random	O
fields	O
choice	O
of	O
xc	O
given	O
this	O
restriction	O
we	O
can	O
make	O
a	O
precise	O
relationship	O
between	O
factorization	B
and	O
conditional	B
independence	I
to	O
do	O
this	O
we	O
again	O
return	O
to	O
the	O
concept	O
of	O
a	O
graphical	B
model	I
as	O
a	O
filter	O
corresponding	O
to	O
figure	O
consider	O
the	O
set	O
of	O
all	O
possible	O
distributions	O
defined	O
over	O
a	O
fixed	O
set	O
of	O
variables	O
corresponding	O
to	O
the	O
nodes	O
of	O
a	O
particular	O
undirected	B
graph	O
we	O
can	O
define	O
ui	O
to	O
be	O
the	O
set	O
of	O
such	O
distributions	O
that	O
are	O
consistent	B
with	O
the	O
set	O
of	O
conditional	B
independence	I
statements	O
that	O
can	O
be	O
read	O
from	O
the	O
graph	O
using	O
graph	O
separation	O
similarly	O
we	O
can	O
define	O
uf	O
to	O
be	O
the	O
set	O
of	O
such	O
distributions	O
that	O
can	O
be	O
expressed	O
as	O
a	O
factorization	B
of	O
the	O
form	O
with	O
respect	O
to	O
the	O
maximal	O
cliques	O
of	O
the	O
graph	O
the	O
hammersley-clifford	B
theorem	I
states	O
that	O
the	O
sets	O
ui	O
and	O
uf	O
are	O
identical	O
because	O
we	O
are	O
restricted	O
to	O
potential	O
functions	O
which	O
are	O
strictly	O
positive	O
it	O
is	O
convenient	O
to	O
express	O
them	O
as	O
exponentials	O
so	O
that	O
cxc	O
exp	O
exc	O
where	O
exc	O
is	O
called	O
an	O
energy	B
function	I
and	O
the	O
exponential	O
representation	O
is	O
called	O
the	O
boltzmann	B
distribution	I
the	O
joint	O
distribution	O
is	O
defined	O
as	O
the	O
product	O
of	O
potentials	O
and	O
so	O
the	O
total	O
energy	O
is	O
obtained	O
by	O
adding	O
the	O
energies	O
of	O
each	O
of	O
the	O
maximal	O
cliques	O
in	O
contrast	O
to	O
the	O
factors	O
in	O
the	O
joint	O
distribution	O
for	O
a	O
directed	B
graph	O
the	O
potentials	O
in	O
an	O
undirected	B
graph	O
do	O
not	O
have	O
a	O
specific	O
probabilistic	O
interpretation	O
although	O
this	O
gives	O
greater	O
flexibility	O
in	O
choosing	O
the	O
potential	O
functions	O
because	O
there	O
is	O
no	O
normalization	O
constraint	O
it	O
does	O
raise	O
the	O
question	O
of	O
how	O
to	O
motivate	O
a	O
choice	O
of	O
potential	B
function	I
for	O
a	O
particular	O
application	O
this	O
can	O
be	O
done	O
by	O
viewing	O
the	O
potential	B
function	I
as	O
expressing	O
which	O
configurations	O
of	O
the	O
local	B
variables	O
are	O
preferred	O
to	O
others	O
global	O
configurations	O
that	O
have	O
a	O
relatively	O
high	O
probability	B
are	O
those	O
that	O
find	O
a	O
good	O
balance	O
in	O
satisfying	O
the	O
conflicting	O
influences	O
of	O
the	O
clique	B
potentials	O
we	O
turn	O
now	O
to	O
a	O
specific	O
example	O
to	O
illustrate	O
the	O
use	O
of	O
undirected	B
graphs	O
illustration	O
image	B
de-noising	I
we	O
can	O
illustrate	O
the	O
application	O
of	O
undirected	B
graphs	O
using	O
an	O
example	O
of	O
noise	O
removal	O
from	O
a	O
binary	O
image	O
geman	O
and	O
geman	O
besag	O
although	O
a	O
very	O
simple	O
example	O
this	O
is	O
typical	O
of	O
more	O
sophisticated	O
applications	O
let	O
the	O
observed	O
noisy	O
image	O
be	O
described	O
by	O
an	O
array	O
of	O
binary	O
pixel	O
values	O
yi	O
where	O
the	O
index	O
i	O
d	O
runs	O
over	O
all	O
pixels	O
we	O
shall	O
suppose	O
that	O
the	O
image	O
is	O
obtained	O
by	O
taking	O
an	O
unknown	O
noise-free	O
image	O
described	O
by	O
binary	O
pixel	O
values	O
xi	O
and	O
randomly	O
flipping	O
the	O
sign	O
of	O
pixels	O
with	O
some	O
small	O
probability	B
an	O
example	O
binary	O
image	O
together	O
with	O
a	O
noise	O
corrupted	O
image	O
obtained	O
by	O
flipping	O
the	O
sign	O
of	O
the	O
pixels	O
with	O
probability	B
is	O
shown	O
in	O
figure	O
given	O
the	O
noisy	O
image	O
our	O
goal	O
is	O
to	O
recover	O
the	O
original	O
noise-free	O
image	O
because	O
the	O
noise	O
level	O
is	O
small	O
we	O
know	O
that	O
there	O
will	O
be	O
a	O
strong	O
correlation	O
between	O
xi	O
and	O
yi	O
we	O
also	O
know	O
that	O
neighbouring	O
pixels	O
xi	O
and	O
xj	O
in	O
an	O
image	O
are	O
strongly	O
correlated	O
this	O
prior	B
knowledge	O
can	O
be	O
captured	O
using	O
the	O
markov	O
graphical	O
models	O
figure	O
illustration	O
of	O
image	B
de-noising	I
using	O
a	O
markov	B
random	I
field	I
the	O
top	O
row	O
shows	O
the	O
original	O
binary	O
image	O
on	O
the	O
left	O
and	O
the	O
corrupted	O
image	O
after	O
randomly	O
changing	O
of	O
the	O
pixels	O
on	O
the	O
right	O
the	O
bottom	O
row	O
shows	O
the	O
restored	O
images	O
obtained	O
using	O
iterated	O
conditional	B
models	O
on	O
the	O
left	O
and	O
using	O
the	O
graph-cut	B
algorithm	I
on	O
the	O
right	O
icm	O
produces	O
an	O
image	O
where	O
of	O
the	O
pixels	O
agree	O
with	O
the	O
original	O
image	O
whereas	O
the	O
corresponding	O
number	O
for	O
graph-cut	O
is	O
random	O
field	O
model	O
whose	O
undirected	B
graph	O
is	O
shown	O
in	O
figure	O
this	O
graph	O
has	O
two	O
types	O
of	O
cliques	O
each	O
of	O
which	O
contains	O
two	O
variables	O
the	O
cliques	O
of	O
the	O
form	O
yi	O
have	O
an	O
associated	O
energy	B
function	I
that	O
expresses	O
the	O
correlation	O
between	O
these	O
variables	O
we	O
choose	O
a	O
very	O
simple	O
energy	B
function	I
for	O
these	O
cliques	O
of	O
the	O
form	O
xiyi	O
where	O
is	O
a	O
positive	O
constant	O
this	O
has	O
the	O
desired	O
effect	O
of	O
giving	O
a	O
lower	O
energy	O
encouraging	O
a	O
higher	O
probability	B
when	O
xi	O
and	O
yi	O
have	O
the	O
same	O
sign	O
and	O
a	O
higher	O
energy	O
when	O
they	O
have	O
the	O
opposite	O
sign	O
the	O
remaining	O
cliques	O
comprise	O
pairs	O
of	O
variables	O
xj	O
where	O
i	O
and	O
j	O
are	O
indices	O
of	O
neighbouring	O
pixels	O
again	O
we	O
want	O
the	O
energy	O
to	O
be	O
lower	O
when	O
the	O
pixels	O
have	O
the	O
same	O
sign	O
than	O
when	O
they	O
have	O
the	O
opposite	O
sign	O
and	O
so	O
we	O
choose	O
an	O
energy	O
given	O
by	O
xixj	O
where	O
is	O
a	O
positive	O
constant	O
because	O
a	O
potential	B
function	I
is	O
an	O
arbitrary	O
nonnegative	O
function	O
over	O
a	O
maximal	B
clique	B
we	O
can	O
multiply	O
it	O
by	O
any	O
nonnegative	O
functions	O
of	O
subsets	O
of	O
the	O
clique	B
or	O
markov	O
random	O
fields	O
figure	O
an	O
undirected	B
graphical	B
model	I
representing	O
a	O
markov	B
random	I
field	I
for	O
image	B
de-noising	I
in	O
which	O
xi	O
is	O
a	O
binary	O
variable	O
denoting	O
the	O
state	O
of	O
pixel	O
i	O
in	O
the	O
unknown	O
noise-free	O
image	O
and	O
yi	O
denotes	O
the	O
corresponding	O
value	O
of	O
pixel	O
i	O
in	O
the	O
observed	O
noisy	O
image	O
yi	O
xi	O
equivalently	O
we	O
can	O
add	O
the	O
corresponding	O
energies	O
in	O
this	O
example	O
this	O
allows	O
us	O
to	O
add	O
an	O
extra	O
term	O
hxi	O
for	O
each	O
pixel	O
i	O
in	O
the	O
noise-free	O
image	O
such	O
a	O
term	O
has	O
the	O
effect	O
of	O
biasing	O
the	O
model	O
towards	O
pixel	O
values	O
that	O
have	O
one	O
particular	O
sign	O
in	O
preference	O
to	O
the	O
other	O
the	O
complete	O
energy	B
function	I
for	O
the	O
model	O
then	O
takes	O
the	O
form	O
i	O
ex	O
y	O
h	O
xi	O
xixj	O
which	O
defines	O
a	O
joint	O
distribution	O
over	O
x	O
and	O
y	O
given	O
by	O
exp	O
ex	O
y	O
px	O
y	O
z	O
xiyi	O
i	O
we	O
now	O
fix	O
the	O
elements	O
of	O
y	O
to	O
the	O
observed	O
values	O
given	O
by	O
the	O
pixels	O
of	O
the	O
noisy	O
image	O
which	O
implicitly	O
defines	O
a	O
conditional	B
distribution	O
pxy	O
over	O
noisefree	O
images	O
this	O
is	O
an	O
example	O
of	O
the	O
ising	B
model	I
which	O
has	O
been	O
widely	O
studied	O
in	O
statistical	O
physics	O
for	O
the	O
purposes	O
of	O
image	O
restoration	O
we	O
wish	O
to	O
find	O
an	O
image	O
x	O
having	O
a	O
high	O
probability	B
the	O
maximum	O
probability	B
to	O
do	O
this	O
we	O
shall	O
use	O
a	O
simple	O
iterative	O
technique	O
called	O
iterated	B
conditional	B
modes	I
or	O
icm	O
and	O
f	O
oglein	O
which	O
is	O
simply	O
an	O
application	O
of	O
coordinate-wise	O
gradient	O
ascent	O
the	O
idea	O
is	O
first	O
to	O
initialize	O
the	O
variables	O
which	O
we	O
do	O
by	O
simply	O
setting	O
xi	O
yi	O
for	O
all	O
i	O
then	O
we	O
take	O
one	O
node	B
xj	O
at	O
a	O
time	O
and	O
we	O
evaluate	O
the	O
total	O
energy	O
for	O
the	O
two	O
possible	O
states	O
xj	O
and	O
xj	O
keeping	O
all	O
other	O
node	B
variables	O
fixed	O
and	O
set	O
xj	O
to	O
whichever	O
state	O
has	O
the	O
lower	O
energy	O
this	O
will	O
either	O
leave	O
the	O
probability	B
unchanged	O
if	O
xj	O
is	O
unchanged	O
or	O
will	O
increase	O
it	O
because	O
only	O
one	O
variable	O
is	O
changed	O
this	O
is	O
a	O
simple	O
local	B
computation	O
that	O
can	O
be	O
performed	O
efficiently	O
we	O
then	O
repeat	O
the	O
update	O
for	O
another	O
site	O
and	O
so	O
on	O
until	O
some	O
suitable	O
stopping	O
criterion	O
is	O
satisfied	O
the	O
nodes	O
may	O
be	O
updated	O
in	O
a	O
systematic	O
way	O
for	O
instance	O
by	O
repeatedly	O
raster	O
scanning	O
through	O
the	O
image	O
or	O
by	O
choosing	O
nodes	O
at	O
random	O
if	O
we	O
have	O
a	O
sequence	O
of	O
updates	O
in	O
which	O
every	O
site	O
is	O
visited	O
at	O
least	O
once	O
and	O
in	O
which	O
no	O
changes	O
to	O
the	O
variables	O
are	O
made	O
then	O
by	O
definition	O
the	O
algorithm	O
exercise	O
graphical	O
models	O
figure	O
example	O
of	O
a	O
directed	B
graph	O
the	O
equivalent	O
undirected	B
graph	O
xn	O
xn	O
xn	O
xn	O
will	O
have	O
converged	O
to	O
a	O
local	B
maximum	O
of	O
the	O
probability	B
this	O
need	O
not	O
however	O
correspond	O
to	O
the	O
global	O
maximum	O
for	O
the	O
purposes	O
of	O
this	O
simple	O
illustration	O
we	O
have	O
fixed	O
the	O
parameters	O
to	O
be	O
and	O
h	O
note	O
that	O
leaving	O
h	O
simply	O
means	O
that	O
the	O
prior	B
probabilities	O
of	O
the	O
two	O
states	O
of	O
xi	O
are	O
equal	O
starting	O
with	O
the	O
observed	O
noisy	O
image	O
as	O
the	O
initial	O
configuration	O
we	O
run	O
icm	O
until	O
convergence	O
leading	O
to	O
the	O
de-noised	O
image	O
shown	O
in	O
the	O
lower	O
left	O
panel	O
of	O
figure	O
note	O
that	O
if	O
we	O
set	O
which	O
effectively	O
removes	O
the	O
links	O
between	O
neighbouring	O
pixels	O
then	O
the	O
global	O
most	O
probable	O
solution	O
is	O
given	O
by	O
xi	O
yi	O
for	O
all	O
i	O
corresponding	O
to	O
the	O
observed	O
noisy	O
image	O
later	O
we	O
shall	O
discuss	O
a	O
more	O
effective	O
algorithm	O
for	O
finding	O
high	O
probability	B
solutions	O
called	O
the	O
max-product	O
algorithm	O
which	O
typically	O
leads	O
to	O
better	O
solutions	O
although	O
this	O
is	O
still	O
not	O
guaranteed	O
to	O
find	O
the	O
global	O
maximum	O
of	O
the	O
posterior	O
distribution	O
however	O
for	O
certain	O
classes	O
of	O
model	O
including	O
the	O
one	O
given	O
by	O
there	O
exist	O
efficient	O
algorithms	O
based	O
on	O
graph	O
cuts	O
that	O
are	O
guaranteed	O
to	O
find	O
the	O
global	O
maximum	O
et	O
al	O
boykov	O
et	O
al	O
kolmogorov	O
and	O
zabih	O
the	O
lower	O
right	O
panel	O
of	O
figure	O
shows	O
the	O
result	O
of	O
applying	O
a	O
graph-cut	B
algorithm	I
to	O
the	O
de-noising	O
problem	O
exercise	O
section	O
relation	O
to	O
directed	B
graphs	O
we	O
have	O
introduced	O
two	O
graphical	O
frameworks	O
for	O
representing	O
probability	B
distributions	O
corresponding	O
to	O
directed	B
and	O
undirected	B
graphs	O
and	O
it	O
is	O
instructive	O
to	O
discuss	O
the	O
relation	O
between	O
these	O
consider	O
first	O
the	O
problem	O
of	O
taking	O
a	O
model	O
that	O
is	O
specified	O
using	O
a	O
directed	B
graph	O
and	O
trying	O
to	O
convert	O
it	O
to	O
an	O
undirected	B
graph	O
in	O
some	O
cases	O
this	O
is	O
straightforward	O
as	O
in	O
the	O
simple	O
example	O
in	O
figure	O
here	O
the	O
joint	O
distribution	O
for	O
the	O
directed	B
graph	O
is	O
given	O
as	O
a	O
product	O
of	O
conditionals	O
in	O
the	O
form	O
px	O
pxnxn	O
now	O
let	O
us	O
convert	O
this	O
to	O
an	O
undirected	B
graph	O
representation	O
as	O
shown	O
in	O
figure	O
in	O
the	O
undirected	B
graph	O
the	O
maximal	O
cliques	O
are	O
simply	O
the	O
pairs	O
of	O
neighbouring	O
nodes	O
and	O
so	O
from	O
we	O
wish	O
to	O
write	O
the	O
joint	O
distribution	O
in	O
the	O
form	O
px	O
z	O
n	O
xn	O
figure	O
example	O
of	O
a	O
simple	O
directed	B
graph	O
and	O
the	O
corresponding	O
moral	O
graph	O
markov	O
random	O
fields	O
this	O
is	O
easily	O
done	O
by	O
identifying	O
n	O
xn	O
pxnxn	O
where	O
we	O
have	O
absorbed	O
the	O
marginal	B
for	O
the	O
first	O
node	B
into	O
the	O
first	O
potential	B
function	I
note	O
that	O
in	O
this	O
case	O
the	O
partition	B
function	I
z	O
let	O
us	O
consider	O
how	O
to	O
generalize	O
this	O
construction	O
so	O
that	O
we	O
can	O
convert	O
any	O
distribution	O
specified	O
by	O
a	O
factorization	B
over	O
a	O
directed	B
graph	O
into	O
one	O
specified	O
by	O
a	O
factorization	B
over	O
an	O
undirected	B
graph	O
this	O
can	O
be	O
achieved	O
if	O
the	O
clique	B
potentials	O
of	O
the	O
undirected	B
graph	O
are	O
given	O
by	O
the	O
conditional	B
distributions	O
of	O
the	O
directed	B
graph	O
in	O
order	O
for	O
this	O
to	O
be	O
valid	O
we	O
must	O
ensure	O
that	O
the	O
set	O
of	O
variables	O
that	O
appears	O
in	O
each	O
of	O
the	O
conditional	B
distributions	O
is	O
a	O
member	O
of	O
at	O
least	O
one	O
clique	B
of	O
the	O
undirected	B
graph	O
for	O
nodes	O
on	O
the	O
directed	B
graph	O
having	O
just	O
one	O
parent	O
this	O
is	O
achieved	O
simply	O
by	O
replacing	O
the	O
directed	B
link	B
with	O
an	O
undirected	B
link	B
however	O
for	O
nodes	O
in	O
the	O
directed	B
graph	O
having	O
more	O
than	O
one	O
parent	O
this	O
is	O
not	O
sufficient	O
these	O
are	O
nodes	O
that	O
have	O
head-to-head	O
paths	O
encountered	O
in	O
our	O
discussion	O
of	O
conditional	B
independence	I
consider	O
a	O
simple	O
directed	B
graph	O
over	O
nodes	O
shown	O
in	O
figure	O
the	O
joint	O
distribution	O
for	O
the	O
directed	B
graph	O
takes	O
the	O
form	O
px	O
we	O
see	O
that	O
the	O
factor	O
involves	O
the	O
four	O
variables	O
and	O
and	O
so	O
these	O
must	O
all	O
belong	O
to	O
a	O
single	O
clique	B
if	O
this	O
conditional	B
distribution	O
is	O
to	O
be	O
absorbed	O
into	O
a	O
clique	B
potential	O
to	O
ensure	O
this	O
we	O
add	O
extra	O
links	O
between	O
all	O
pairs	O
of	O
parents	O
of	O
the	O
node	B
anachronistically	O
this	O
process	O
of	O
marrying	O
the	O
parents	O
has	O
become	O
known	O
as	O
moralization	B
and	O
the	O
resulting	O
undirected	B
graph	O
after	O
dropping	O
the	O
arrows	O
is	O
called	O
the	O
moral	O
graph	O
it	O
is	O
important	O
to	O
observe	O
that	O
the	O
moral	O
graph	O
in	O
this	O
example	O
is	O
fully	B
connected	I
and	O
so	O
exhibits	O
no	O
conditional	B
independence	I
properties	O
in	O
contrast	O
to	O
the	O
original	O
directed	B
graph	O
thus	O
in	O
general	O
to	O
convert	O
a	O
directed	B
graph	O
into	O
an	O
undirected	B
graph	O
we	O
first	O
add	O
additional	O
undirected	B
links	O
between	O
all	O
pairs	O
of	O
parents	O
for	O
each	O
node	B
in	O
the	O
graph	O
and	O
graphical	O
models	O
section	O
section	O
then	O
drop	O
the	O
arrows	O
on	O
the	O
original	O
links	O
to	O
give	O
the	O
moral	O
graph	O
then	O
we	O
initialize	O
all	O
of	O
the	O
clique	B
potentials	O
of	O
the	O
moral	O
graph	O
to	O
we	O
then	O
take	O
each	O
conditional	B
distribution	O
factor	O
in	O
the	O
original	O
directed	B
graph	O
and	O
multiply	O
it	O
into	O
one	O
of	O
the	O
clique	B
potentials	O
there	O
will	O
always	O
exist	O
at	O
least	O
one	O
maximal	B
clique	B
that	O
contains	O
all	O
of	O
the	O
variables	O
in	O
the	O
factor	O
as	O
a	O
result	O
of	O
the	O
moralization	B
step	O
note	O
that	O
in	O
all	O
cases	O
the	O
partition	B
function	I
is	O
given	O
by	O
z	O
the	O
process	O
of	O
converting	O
a	O
directed	B
graph	O
into	O
an	O
undirected	B
graph	O
plays	O
an	O
important	O
role	O
in	O
exact	O
inference	B
techniques	O
such	O
as	O
the	O
junction	B
tree	B
algorithm	I
converting	O
from	O
an	O
undirected	B
to	O
a	O
directed	B
representation	O
is	O
much	O
less	O
common	O
and	O
in	O
general	O
presents	O
problems	O
due	O
to	O
the	O
normalization	O
constraints	O
we	O
saw	O
that	O
in	O
going	O
from	O
a	O
directed	B
to	O
an	O
undirected	B
representation	O
we	O
had	O
to	O
discard	O
some	O
conditional	B
independence	I
properties	O
from	O
the	O
graph	O
of	O
course	O
we	O
could	O
always	O
trivially	O
convert	O
any	O
distribution	O
over	O
a	O
directed	B
graph	O
into	O
one	O
over	O
an	O
undirected	B
graph	O
by	O
simply	O
using	O
a	O
fully	B
connected	I
undirected	B
graph	O
this	O
would	O
however	O
discard	O
all	O
conditional	B
independence	I
properties	O
and	O
so	O
would	O
be	O
vacuous	O
the	O
process	O
of	O
moralization	B
adds	O
the	O
fewest	O
extra	O
links	O
and	O
so	O
retains	O
the	O
maximum	O
number	O
of	O
independence	O
properties	O
we	O
have	O
seen	O
that	O
the	O
procedure	O
for	O
determining	O
the	O
conditional	B
independence	I
properties	O
is	O
different	O
between	O
directed	B
and	O
undirected	B
graphs	O
it	O
turns	O
out	O
that	O
the	O
two	O
types	O
of	O
graph	O
can	O
express	O
different	O
conditional	B
independence	I
properties	O
and	O
it	O
is	O
worth	O
exploring	O
this	O
issue	O
in	O
more	O
detail	O
to	O
do	O
so	O
we	O
return	O
to	O
the	O
view	O
of	O
a	O
specific	O
or	O
undirected	B
graph	O
as	O
a	O
filter	O
so	O
that	O
the	O
set	O
of	O
all	O
possible	O
distributions	O
over	O
the	O
given	O
variables	O
could	O
be	O
reduced	O
to	O
a	O
subset	O
that	O
respects	O
the	O
conditional	B
independencies	O
implied	O
by	O
the	O
graph	O
a	O
graph	O
is	O
said	O
to	O
be	O
a	O
d	O
map	O
dependency	B
map	I
of	O
a	O
distribution	O
if	O
every	O
conditional	B
independence	I
statement	O
satisfied	O
by	O
the	O
distribution	O
is	O
reflected	O
in	O
the	O
graph	O
thus	O
a	O
completely	O
disconnected	O
graph	O
links	O
will	O
be	O
a	O
trivial	O
d	O
map	O
for	O
any	O
distribution	O
alternatively	O
we	O
can	O
consider	O
a	O
specific	O
distribution	O
and	O
ask	O
which	O
graphs	O
have	O
the	O
appropriate	O
conditional	B
independence	I
properties	O
if	O
every	O
conditional	B
independence	I
statement	O
implied	O
by	O
a	O
graph	O
is	O
satisfied	O
by	O
a	O
specific	O
distribution	O
then	O
the	O
graph	O
is	O
said	O
to	O
be	O
an	O
i	O
map	O
independence	B
map	I
of	O
that	O
distribution	O
clearly	O
a	O
fully	B
connected	I
graph	O
will	O
be	O
a	O
trivial	O
i	O
map	O
for	O
any	O
distribution	O
if	O
it	O
is	O
the	O
case	O
that	O
every	O
conditional	B
independence	I
property	O
of	O
the	O
distribution	O
is	O
reflected	O
in	O
the	O
graph	O
and	O
vice	O
versa	O
then	O
the	O
graph	O
is	O
said	O
to	O
be	O
a	O
perfect	B
map	I
for	O
figure	O
venn	O
diagram	O
illustrating	O
the	O
set	O
of	O
all	O
distributions	O
p	O
over	O
a	O
given	O
set	O
of	O
variables	O
together	O
with	O
the	O
set	O
of	O
distributions	O
d	O
that	O
can	O
be	O
represented	O
as	O
a	O
perfect	B
map	I
using	O
a	O
directed	B
graph	O
and	O
the	O
set	O
u	O
that	O
can	O
be	O
represented	O
as	O
a	O
perfect	B
map	I
using	O
an	O
undirected	B
graph	O
d	O
u	O
p	O
inference	B
in	O
graphical	O
models	O
figure	O
a	O
directed	B
graph	O
whose	O
conditional	B
independence	I
properties	O
cannot	O
be	O
expressed	O
using	O
an	O
undirected	B
graph	O
over	O
the	O
same	O
three	O
variables	O
a	O
b	O
c	O
that	O
distribution	O
a	O
perfect	B
map	I
is	O
therefore	O
both	O
an	O
i	O
map	O
and	O
a	O
d	O
map	O
consider	O
the	O
set	O
of	O
distributions	O
such	O
that	O
for	O
each	O
distribution	O
there	O
exists	O
a	O
directed	B
graph	O
that	O
is	O
a	O
perfect	B
map	I
this	O
set	O
is	O
distinct	O
from	O
the	O
set	O
of	O
distributions	O
such	O
that	O
for	O
each	O
distribution	O
there	O
exists	O
an	O
undirected	B
graph	O
that	O
is	O
a	O
perfect	B
map	I
in	O
addition	O
there	O
are	O
distributions	O
for	O
which	O
neither	O
directed	B
nor	O
undirected	B
graphs	O
offer	O
a	O
perfect	B
map	I
this	O
is	O
illustrated	O
as	O
a	O
venn	O
diagram	O
in	O
figure	O
figure	O
shows	O
an	O
example	O
of	O
a	O
directed	B
graph	O
that	O
is	O
a	O
perfect	B
map	I
for	O
a	O
distribution	O
satisfying	O
the	O
conditional	B
independence	I
properties	O
a	O
b	O
and	O
a	O
b	O
c	O
there	O
is	O
no	O
corresponding	O
undirected	B
graph	O
over	O
the	O
same	O
three	O
variables	O
that	O
is	O
a	O
perfect	B
map	I
conversely	O
consider	O
the	O
undirected	B
graph	O
over	O
four	O
variables	O
shown	O
in	O
figure	O
this	O
graph	O
exhibits	O
the	O
properties	O
a	O
b	O
c	O
d	O
a	O
b	O
and	O
a	O
b	O
c	O
d	O
there	O
is	O
no	O
directed	B
graph	O
over	O
four	O
variables	O
that	O
implies	O
the	O
same	O
set	O
of	O
conditional	B
independence	I
properties	O
the	O
graphical	O
framework	O
can	O
be	O
extended	B
in	O
a	O
consistent	B
way	O
to	O
graphs	O
that	O
include	O
both	O
directed	B
and	O
undirected	B
links	O
these	O
are	O
called	O
chain	O
graphs	O
and	O
wermuth	O
frydenberg	O
and	O
contain	O
the	O
directed	B
and	O
undirected	B
graphs	O
considered	O
so	O
far	O
as	O
special	O
cases	O
although	O
such	O
graphs	O
can	O
represent	O
a	O
broader	O
class	O
of	O
distributions	O
than	O
either	O
directed	B
or	O
undirected	B
alone	O
there	O
remain	O
distributions	O
for	O
which	O
even	O
a	O
chain	B
graph	I
cannot	O
provide	O
a	O
perfect	B
map	I
chain	O
graphs	O
are	O
not	O
discussed	O
further	O
in	O
this	O
book	O
figure	O
an	O
undirected	B
graph	O
whose	O
conditional	B
independence	I
properties	O
cannot	O
be	O
expressed	O
in	O
terms	O
of	O
a	O
directed	B
graph	O
over	O
the	O
same	O
variables	O
a	O
b	O
c	O
d	O
inference	B
in	O
graphical	O
models	O
we	O
turn	O
now	O
to	O
the	O
problem	O
of	O
inference	B
in	O
graphical	O
models	O
in	O
which	O
some	O
of	O
the	O
nodes	O
in	O
a	O
graph	O
are	O
clamped	O
to	O
observed	O
values	O
and	O
we	O
wish	O
to	O
compute	O
the	O
posterior	O
distributions	O
of	O
one	O
or	O
more	O
subsets	O
of	O
other	O
nodes	O
as	O
we	O
shall	O
see	O
we	O
can	O
exploit	O
the	O
graphical	O
structure	O
both	O
to	O
find	O
efficient	O
algorithms	O
for	O
inference	B
and	O
graphical	O
models	O
figure	O
a	O
graphical	O
representation	O
of	O
bayes	B
see	O
the	O
text	O
for	O
details	O
theorem	O
x	O
y	O
x	O
y	O
x	O
y	O
to	O
make	O
the	O
structure	O
of	O
those	O
algorithms	O
transparent	O
specifically	O
we	O
shall	O
see	O
that	O
many	O
algorithms	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
propagation	O
of	O
local	B
messages	O
around	O
the	O
graph	O
in	O
this	O
section	O
we	O
shall	O
focus	O
primarily	O
on	O
techniques	O
for	O
exact	O
inference	B
and	O
in	O
chapter	O
we	O
shall	O
consider	O
a	O
number	O
of	O
approximate	O
inference	B
algorithms	O
to	O
start	O
with	O
let	O
us	O
consider	O
the	O
graphical	O
interpretation	O
of	O
bayes	B
theorem	O
suppose	O
we	O
decompose	O
the	O
joint	O
distribution	O
px	O
y	O
over	O
two	O
variables	O
x	O
and	O
y	O
into	O
a	O
product	O
of	O
factors	O
in	O
the	O
form	O
px	O
y	O
pxpyx	O
this	O
can	O
be	O
represented	O
by	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
now	O
suppose	O
we	O
observe	O
the	O
value	O
of	O
y	O
as	O
indicated	O
by	O
the	O
shaded	O
node	B
in	O
figure	O
we	O
can	O
view	O
the	O
marginal	B
distribution	O
px	O
as	O
a	O
prior	B
over	O
the	O
latent	B
variable	I
x	O
and	O
our	O
goal	O
is	O
to	O
infer	O
the	O
corresponding	O
posterior	O
distribution	O
over	O
x	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
we	O
can	O
evaluate	O
py	O
pyx	O
which	O
can	O
then	O
be	O
used	O
in	O
bayes	B
theorem	O
to	O
calculate	O
pxy	O
pyxpx	O
thus	O
the	O
joint	O
distribution	O
is	O
now	O
expressed	O
in	O
terms	O
of	O
py	O
and	O
pxy	O
from	O
a	O
graphical	O
perspective	O
the	O
joint	O
distribution	O
px	O
y	O
is	O
now	O
represented	O
by	O
the	O
graph	O
shown	O
in	O
figure	O
in	O
which	O
the	O
direction	O
of	O
the	O
arrow	O
is	O
reversed	O
this	O
is	O
the	O
simplest	O
example	O
of	O
an	O
inference	B
problem	O
for	O
a	O
graphical	B
model	I
py	O
inference	B
on	O
a	O
chain	O
now	O
consider	O
a	O
more	O
complex	O
problem	O
involving	O
the	O
chain	O
of	O
nodes	O
of	O
the	O
form	O
shown	O
in	O
figure	O
this	O
example	O
will	O
lay	O
the	O
foundation	O
for	O
a	O
discussion	O
of	O
exact	O
inference	B
in	O
more	O
general	O
graphs	O
later	O
in	O
this	O
section	O
specifically	O
we	O
shall	O
consider	O
the	O
undirected	B
graph	O
in	O
figure	O
we	O
have	O
already	O
seen	O
that	O
the	O
directed	B
chain	O
can	O
be	O
transformed	O
into	O
an	O
equivalent	O
undirected	B
chain	O
because	O
the	O
directed	B
graph	O
does	O
not	O
have	O
any	O
nodes	O
with	O
more	O
than	O
one	O
parent	O
this	O
does	O
not	O
require	O
the	O
addition	O
of	O
any	O
extra	O
links	O
and	O
the	O
directed	B
and	O
undirected	B
versions	O
of	O
this	O
graph	O
express	O
exactly	O
the	O
same	O
set	O
of	O
conditional	B
independence	I
statements	O
inference	B
in	O
graphical	O
models	O
the	O
joint	O
distribution	O
for	O
this	O
graph	O
takes	O
the	O
form	O
px	O
z	O
n	O
xn	O
we	O
shall	O
consider	O
the	O
specific	O
case	O
in	O
which	O
the	O
n	O
nodes	O
represent	O
discrete	O
variables	O
each	O
having	O
k	O
states	O
in	O
which	O
case	O
each	O
potential	B
function	I
n	O
xn	O
comprises	O
an	O
k	O
k	O
table	O
and	O
so	O
the	O
joint	O
distribution	O
has	O
parameters	O
let	O
us	O
consider	O
the	O
inference	B
problem	O
of	O
finding	O
the	O
marginal	B
distribution	O
pxn	O
for	O
a	O
specific	O
node	B
xn	O
that	O
is	O
part	O
way	O
along	O
the	O
chain	O
note	O
that	O
for	O
the	O
moment	O
there	O
are	O
no	O
observed	O
nodes	O
by	O
definition	O
the	O
required	O
marginal	B
is	O
obtained	O
by	O
summing	O
the	O
joint	O
distribution	O
over	O
all	O
variables	O
except	O
xn	O
so	O
that	O
pxn	O
px	O
xn	O
xn	O
in	O
a	O
naive	O
implementation	O
we	O
would	O
first	O
evaluate	O
the	O
joint	O
distribution	O
and	O
then	O
perform	O
the	O
summations	O
explicitly	O
the	O
joint	O
distribution	O
can	O
be	O
represented	O
as	O
a	O
set	O
of	O
numbers	O
one	O
for	O
each	O
possible	O
value	O
for	O
x	O
because	O
there	O
are	O
n	O
variables	O
each	O
with	O
k	O
states	O
there	O
are	O
k	O
n	O
values	O
for	O
x	O
and	O
so	O
evaluation	O
and	O
storage	O
of	O
the	O
joint	O
distribution	O
as	O
well	O
as	O
marginalization	O
to	O
obtain	O
pxn	O
all	O
involve	O
storage	O
and	O
computation	O
that	O
scale	O
exponentially	O
with	O
the	O
length	O
n	O
of	O
the	O
chain	O
we	O
can	O
however	O
obtain	O
a	O
much	O
more	O
efficient	O
algorithm	O
by	O
exploiting	O
the	O
conditional	B
independence	I
properties	O
of	O
the	O
graphical	B
model	I
if	O
we	O
substitute	O
the	O
factorized	O
expression	O
for	O
the	O
joint	O
distribution	O
into	O
then	O
we	O
can	O
rearrange	O
the	O
order	O
of	O
the	O
summations	O
and	O
the	O
multiplications	O
to	O
allow	O
the	O
required	O
marginal	B
to	O
be	O
evaluated	O
much	O
more	O
efficiently	O
consider	O
for	O
instance	O
the	O
summation	O
over	O
xn	O
the	O
potential	O
n	O
xn	O
is	O
the	O
only	O
one	O
that	O
depends	O
on	O
xn	O
and	O
so	O
we	O
can	O
perform	O
the	O
summation	O
n	O
xn	O
xn	O
first	O
to	O
give	O
a	O
function	O
of	O
xn	O
we	O
can	O
then	O
use	O
this	O
to	O
perform	O
the	O
summation	O
over	O
xn	O
which	O
will	O
involve	O
only	O
this	O
new	O
function	O
together	O
with	O
the	O
potential	O
n	O
xn	O
because	O
this	O
is	O
the	O
only	O
other	O
place	O
that	O
xn	O
appears	O
similarly	O
the	O
summation	O
over	O
involves	O
only	O
the	O
potential	O
and	O
so	O
can	O
be	O
performed	O
separately	O
to	O
give	O
a	O
function	O
of	O
and	O
so	O
on	O
because	O
each	O
summation	O
effectively	O
removes	O
a	O
variable	O
from	O
the	O
distribution	O
this	O
can	O
be	O
viewed	O
as	O
the	O
removal	O
of	O
a	O
node	B
from	O
the	O
graph	O
if	O
we	O
group	O
the	O
potentials	O
and	O
summations	O
together	O
in	O
this	O
way	O
we	O
can	O
express	O
graphical	O
models	O
the	O
desired	O
marginal	B
in	O
the	O
form	O
n	O
xn	O
pxn	O
xn	O
z	O
xn	O
n	O
xn	O
the	O
reader	O
is	O
encouraged	O
to	O
study	O
this	O
re-ordering	O
carefully	O
as	O
the	O
underlying	O
idea	O
forms	O
the	O
basis	O
for	O
the	O
later	O
discussion	O
of	O
the	O
general	O
sum-product	B
algorithm	I
here	O
the	O
key	O
concept	O
that	O
we	O
are	O
exploiting	O
is	O
that	O
multiplication	O
is	O
distributive	O
over	O
addition	O
so	O
that	O
ab	O
ac	O
ab	O
c	O
in	O
which	O
the	O
left-hand	O
side	O
involves	O
three	O
arithmetic	O
operations	O
whereas	O
the	O
righthand	O
side	O
reduces	O
this	O
to	O
two	O
operations	O
let	O
us	O
work	O
out	O
the	O
computational	O
cost	O
of	O
evaluating	O
the	O
required	O
marginal	B
using	O
this	O
re-ordered	O
expression	O
we	O
have	O
to	O
perform	O
n	O
summations	O
each	O
of	O
which	O
is	O
over	O
k	O
states	O
and	O
each	O
of	O
which	O
involves	O
a	O
function	O
of	O
two	O
variables	O
for	O
instance	O
the	O
summation	O
over	O
involves	O
only	O
the	O
function	O
which	O
is	O
a	O
table	O
of	O
k	O
k	O
numbers	O
we	O
have	O
to	O
sum	O
this	O
table	O
over	O
for	O
each	O
value	O
of	O
and	O
so	O
this	O
has	O
ok	O
cost	O
the	O
resulting	O
vector	O
of	O
k	O
numbers	O
is	O
multiplied	O
by	O
the	O
matrix	O
of	O
numbers	O
and	O
so	O
is	O
again	O
ok	O
because	O
there	O
are	O
n	O
summations	O
and	O
multiplications	O
of	O
this	O
kind	O
the	O
total	O
cost	O
of	O
evaluating	O
the	O
marginal	B
pxn	O
is	O
on	O
k	O
this	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
chain	O
in	O
contrast	O
to	O
the	O
exponential	O
cost	O
of	O
a	O
naive	O
approach	O
we	O
have	O
therefore	O
been	O
able	O
to	O
exploit	O
the	O
many	O
conditional	B
independence	I
properties	O
of	O
this	O
simple	O
graph	O
in	O
order	O
to	O
obtain	O
an	O
efficient	O
calculation	O
if	O
the	O
graph	O
had	O
been	O
fully	B
connected	I
there	O
would	O
have	O
been	O
no	O
conditional	B
independence	I
properties	O
and	O
we	O
would	O
have	O
been	O
forced	O
to	O
work	O
directly	O
with	O
the	O
full	O
joint	O
distribution	O
we	O
now	O
give	O
a	O
powerful	O
interpretation	O
of	O
this	O
calculation	O
in	O
terms	O
of	O
the	O
passing	O
of	O
local	B
messages	O
around	O
on	O
the	O
graph	O
from	O
we	O
see	O
that	O
the	O
expression	O
for	O
the	O
marginal	B
pxn	O
decomposes	O
into	O
the	O
product	O
of	O
two	O
factors	O
times	O
the	O
normalization	O
constant	O
pxn	O
z	O
we	O
shall	O
interpret	O
as	O
a	O
message	O
passed	O
forwards	O
along	O
the	O
chain	O
from	O
node	B
xn	O
to	O
node	B
xn	O
similarly	O
can	O
be	O
viewed	O
as	O
a	O
message	O
passed	O
backwards	O
inference	B
in	O
graphical	O
models	O
xn	O
xn	O
xn	O
figure	O
the	O
marginal	B
distribution	O
pxn	O
for	O
a	O
node	B
xn	O
along	O
the	O
chain	O
is	O
obtained	O
by	O
multiplying	O
the	O
two	O
messages	O
and	O
and	O
then	O
normalizing	O
these	O
messages	O
can	O
themselves	O
be	O
evaluated	O
recursively	O
by	O
passing	O
messages	O
from	O
both	O
ends	O
of	O
the	O
chain	O
towards	O
node	B
xn	O
xn	O
along	O
the	O
chain	O
to	O
node	B
xn	O
from	O
node	B
note	O
that	O
each	O
of	O
the	O
messages	O
comprises	O
a	O
set	O
of	O
k	O
values	O
one	O
for	O
each	O
choice	O
of	O
xn	O
and	O
so	O
the	O
product	O
of	O
two	O
messages	O
should	O
be	O
interpreted	O
as	O
the	O
point-wise	O
multiplication	O
of	O
the	O
elements	O
of	O
the	O
two	O
messages	O
to	O
give	O
another	O
set	O
of	O
k	O
values	O
the	O
message	O
can	O
be	O
evaluated	O
recursively	O
because	O
n	O
xn	O
xn	O
n	O
xn	O
we	O
therefore	O
first	O
evaluate	O
xn	O
and	O
then	O
apply	O
repeatedly	O
until	O
we	O
reach	O
the	O
desired	O
node	B
note	O
carefully	O
the	O
structure	O
of	O
the	O
message	B
passing	I
equation	O
the	O
outgoing	O
message	O
in	O
is	O
obtained	O
by	O
multiplying	O
the	O
incoming	O
message	O
by	O
the	O
local	B
potential	O
involving	O
the	O
node	B
variable	O
and	O
the	O
outgoing	O
variable	O
and	O
then	O
summing	O
over	O
the	O
node	B
variable	O
similarly	O
the	O
message	O
can	O
be	O
evaluated	O
recursively	O
by	O
starting	O
with	O
node	B
xn	O
and	O
using	O
xn	O
xn	O
this	O
recursive	O
message	B
passing	I
is	O
illustrated	O
in	O
figure	O
the	O
normalization	O
constant	O
z	O
is	O
easily	O
evaluated	O
by	O
summing	O
the	O
right-hand	O
side	O
of	O
over	O
all	O
states	O
of	O
xn	O
an	O
operation	O
that	O
requires	O
only	O
ok	O
computation	O
graphs	O
of	O
the	O
form	O
shown	O
in	O
figure	O
are	O
called	O
markov	O
chains	O
and	O
the	O
corresponding	O
message	B
passing	I
equations	O
represent	O
an	O
example	O
of	O
the	O
chapmankolmogorov	O
equations	O
for	O
markov	O
processes	O
graphical	O
models	O
now	O
suppose	O
we	O
wish	O
to	O
evaluate	O
the	O
marginals	O
pxn	O
for	O
every	O
node	B
n	O
n	O
in	O
the	O
chain	O
simply	O
applying	O
the	O
above	O
procedure	O
separately	O
for	O
each	O
node	B
will	O
have	O
computational	O
cost	O
that	O
is	O
on	O
however	O
such	O
an	O
approach	O
would	O
be	O
very	O
wasteful	O
of	O
computation	O
for	O
instance	O
to	O
find	O
we	O
need	O
to	O
propagate	O
a	O
message	O
from	O
node	B
xn	O
back	O
to	O
node	B
similarly	O
to	O
evaluate	O
we	O
need	O
to	O
propagate	O
a	O
messages	O
from	O
node	B
xn	O
back	O
to	O
node	B
this	O
will	O
involve	O
much	O
duplicated	O
computation	O
because	O
most	O
of	O
the	O
messages	O
will	O
be	O
identical	O
in	O
the	O
two	O
cases	O
suppose	O
instead	O
we	O
first	O
launch	O
a	O
message	O
starting	O
from	O
node	B
xn	O
and	O
propagate	O
corresponding	O
messages	O
all	O
the	O
way	O
back	O
to	O
node	B
and	O
suppose	O
we	O
similarly	O
launch	O
a	O
message	O
starting	O
from	O
node	B
and	O
propagate	O
the	O
corresponding	O
messages	O
all	O
the	O
way	O
forward	O
to	O
node	B
xn	O
provided	O
we	O
store	O
all	O
of	O
the	O
intermediate	O
messages	O
along	O
the	O
way	O
then	O
any	O
node	B
can	O
evaluate	O
its	O
marginal	B
simply	O
by	O
applying	O
the	O
computational	O
cost	O
is	O
only	O
twice	O
that	O
for	O
finding	O
the	O
marginal	B
of	O
a	O
single	O
node	B
rather	O
than	O
n	O
times	O
as	O
much	O
observe	O
that	O
a	O
message	O
has	O
passed	O
once	O
in	O
each	O
direction	O
across	O
each	O
link	B
in	O
the	O
graph	O
note	O
also	O
that	O
the	O
normalization	O
constant	O
z	O
need	O
be	O
evaluated	O
only	O
once	O
using	O
any	O
convenient	O
node	B
if	O
some	O
of	O
the	O
nodes	O
in	O
the	O
graph	O
are	O
observed	O
then	O
the	O
corresponding	O
variables	O
are	O
simply	O
clamped	O
to	O
their	O
observed	O
values	O
and	O
there	O
is	O
no	O
summation	O
to	O
see	O
this	O
note	O
that	O
the	O
effect	O
of	O
clamping	O
a	O
variable	O
xn	O
to	O
an	O
observed	O
value	O
can	O
additional	O
function	O
which	O
takes	O
the	O
value	O
when	O
xn	O
and	O
the	O
value	O
contain	O
xn	O
summations	O
over	O
xn	O
then	O
contain	O
only	O
one	O
term	O
in	O
which	O
xn	O
otherwise	O
one	O
such	O
function	O
can	O
then	O
be	O
absorbed	O
into	O
each	O
of	O
the	O
potentials	O
that	O
be	O
expressed	O
by	O
multiplying	O
the	O
joint	O
distribution	O
by	O
or	O
more	O
copies	O
of	O
an	O
now	O
suppose	O
we	O
wish	O
to	O
calculate	O
the	O
joint	O
distribution	O
pxn	O
xn	O
for	O
two	O
neighbouring	O
nodes	O
on	O
the	O
chain	O
this	O
is	O
similar	O
to	O
the	O
evaluation	O
of	O
the	O
marginal	B
for	O
a	O
single	O
node	B
except	O
that	O
there	O
are	O
now	O
two	O
variables	O
that	O
are	O
not	O
summed	O
out	O
a	O
few	O
moments	O
thought	O
will	O
show	O
that	O
the	O
required	O
joint	O
distribution	O
can	O
be	O
written	O
in	O
the	O
form	O
pxn	O
xn	O
z	O
n	O
xn	O
thus	O
we	O
can	O
obtain	O
the	O
joint	O
distributions	O
over	O
all	O
of	O
the	O
sets	O
of	O
variables	O
in	O
each	O
of	O
the	O
potentials	O
directly	O
once	O
we	O
have	O
completed	O
the	O
message	B
passing	I
required	O
to	O
obtain	O
the	O
marginals	O
this	O
is	O
a	O
useful	O
result	O
because	O
in	O
practice	O
we	O
may	O
wish	O
to	O
use	O
parametric	O
forms	O
for	O
the	O
clique	B
potentials	O
or	O
equivalently	O
for	O
the	O
conditional	B
distributions	O
if	O
we	O
started	O
from	O
a	O
directed	B
graph	O
in	O
order	O
to	O
learn	O
the	O
parameters	O
of	O
these	O
potentials	O
in	O
situations	O
where	O
not	O
all	O
of	O
the	O
variables	O
are	O
observed	O
we	O
can	O
employ	O
the	O
em	B
algorithm	I
and	O
it	O
turns	O
out	O
that	O
the	O
local	B
joint	O
distributions	O
of	O
the	O
cliques	O
conditioned	O
on	O
any	O
observed	O
data	O
is	O
precisely	O
what	O
is	O
needed	O
in	O
the	O
e	O
step	O
we	O
shall	O
consider	O
some	O
examples	O
of	O
this	O
in	O
detail	O
in	O
chapter	O
trees	O
we	O
have	O
seen	O
that	O
exact	O
inference	B
on	O
a	O
graph	O
comprising	O
a	O
chain	O
of	O
nodes	O
can	O
be	O
performed	O
efficiently	O
in	O
time	O
that	O
is	O
linear	O
in	O
the	O
number	O
of	O
nodes	O
using	O
an	O
algorithm	O
exercise	O
chapter	O
inference	B
in	O
graphical	O
models	O
figure	O
examples	O
treestructured	O
graphs	O
showing	O
an	O
undirected	B
tree	B
a	O
directed	B
tree	B
and	O
a	O
directed	B
polytree	B
of	O
that	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
messages	O
passed	O
along	O
the	O
chain	O
more	O
generally	O
inference	B
can	O
be	O
performed	O
efficiently	O
using	O
local	B
message	B
passing	I
on	O
a	O
broader	O
class	O
of	O
graphs	O
called	O
trees	O
in	O
particular	O
we	O
shall	O
shortly	O
generalize	O
the	O
message	B
passing	I
formalism	O
derived	O
above	O
for	O
chains	O
to	O
give	O
the	O
sum-product	B
algorithm	I
which	O
provides	O
an	O
efficient	O
framework	O
for	O
exact	O
inference	B
in	O
tree-structured	O
graphs	O
in	O
the	O
case	O
of	O
an	O
undirected	B
graph	O
a	O
tree	B
is	O
defined	O
as	O
a	O
graph	O
in	O
which	O
there	O
is	O
one	O
and	O
only	O
one	O
path	O
between	O
any	O
pair	O
of	O
nodes	O
such	O
graphs	O
therefore	O
do	O
not	O
have	O
loops	O
in	O
the	O
case	O
of	O
directed	B
graphs	O
a	O
tree	B
is	O
defined	O
such	O
that	O
there	O
is	O
a	O
single	O
node	B
called	O
the	O
root	O
which	O
has	O
no	O
parents	O
and	O
all	O
other	O
nodes	O
have	O
one	O
parent	O
if	O
we	O
convert	O
a	O
directed	B
tree	B
into	O
an	O
undirected	B
graph	O
we	O
see	O
that	O
the	O
moralization	B
step	O
will	O
not	O
add	O
any	O
links	O
as	O
all	O
nodes	O
have	O
at	O
most	O
one	O
parent	O
and	O
as	O
a	O
consequence	O
the	O
corresponding	O
moralized	O
graph	O
will	O
be	O
an	O
undirected	B
tree	B
examples	O
of	O
undirected	B
and	O
directed	B
trees	O
are	O
shown	O
in	O
figure	O
and	O
note	O
that	O
a	O
distribution	O
represented	O
as	O
a	O
directed	B
tree	B
can	O
easily	O
be	O
converted	O
into	O
one	O
represented	O
by	O
an	O
undirected	B
tree	B
and	O
vice	O
versa	O
if	O
there	O
are	O
nodes	O
in	O
a	O
directed	B
graph	O
that	O
have	O
more	O
than	O
one	O
parent	O
but	O
there	O
is	O
still	O
only	O
one	O
path	O
the	O
direction	O
of	O
the	O
arrows	O
between	O
any	O
two	O
nodes	O
then	O
the	O
graph	O
is	O
a	O
called	O
a	O
polytree	B
as	O
illustrated	O
in	O
figure	O
such	O
a	O
graph	O
will	O
have	O
more	O
than	O
one	O
node	B
with	O
the	O
property	O
of	O
having	O
no	O
parents	O
and	O
furthermore	O
the	O
corresponding	O
moralized	O
undirected	B
graph	O
will	O
have	O
loops	O
factor	O
graphs	O
the	O
sum-product	B
algorithm	I
that	O
we	O
derive	O
in	O
the	O
next	O
section	O
is	O
applicable	O
to	O
undirected	B
and	O
directed	B
trees	O
and	O
to	O
polytrees	O
it	O
can	O
be	O
cast	O
in	O
a	O
particularly	O
simple	O
and	O
general	O
form	O
if	O
we	O
first	O
introduce	O
a	O
new	O
graphical	O
construction	O
called	O
a	O
factor	B
graph	I
kschischnang	O
et	O
al	O
exercise	O
both	O
directed	B
and	O
undirected	B
graphs	O
allow	O
a	O
global	O
function	O
of	O
several	O
variables	O
to	O
be	O
expressed	O
as	O
a	O
product	O
of	O
factors	O
over	O
subsets	O
of	O
those	O
variables	O
factor	O
graphs	O
make	O
this	O
decomposition	O
explicit	O
by	O
introducing	O
additional	O
nodes	O
for	O
the	O
factors	O
themselves	O
in	O
addition	O
to	O
the	O
nodes	O
representing	O
the	O
variables	O
they	O
also	O
allow	O
us	O
to	O
be	O
more	O
explicit	O
about	O
the	O
details	O
of	O
the	O
factorization	B
as	O
we	O
shall	O
see	O
let	O
us	O
write	O
the	O
joint	O
distribution	O
over	O
a	O
set	O
of	O
variables	O
in	O
the	O
form	O
of	O
a	O
product	O
of	O
factors	O
px	O
fsxs	O
where	O
xs	O
denotes	O
a	O
subset	O
of	O
the	O
variables	O
for	O
convenience	O
we	O
shall	O
denote	O
the	O
s	O
graphical	O
models	O
figure	O
example	O
of	O
a	O
factor	B
graph	I
which	O
corresponds	O
to	O
the	O
factorization	B
fa	O
fb	O
fc	O
fd	O
individual	O
variables	O
by	O
xi	O
however	O
as	O
in	O
earlier	O
discussions	O
these	O
can	O
comprise	O
groups	O
of	O
variables	O
as	O
vectors	O
or	O
matrices	O
each	O
factor	O
fs	O
is	O
a	O
function	O
of	O
a	O
corresponding	O
set	O
of	O
variables	O
xs	O
directed	B
graphs	O
whose	O
factorization	B
is	O
defined	O
by	O
represent	O
special	O
cases	O
of	O
in	O
which	O
the	O
factors	O
fsxs	O
are	O
local	B
conditional	B
distributions	O
similarly	O
undirected	B
graphs	O
given	O
by	O
are	O
a	O
special	O
case	O
in	O
which	O
the	O
factors	O
are	O
potential	O
functions	O
over	O
the	O
maximal	O
cliques	O
normalizing	O
coefficient	O
can	O
be	O
viewed	O
as	O
a	O
factor	O
defined	O
over	O
the	O
empty	O
set	O
of	O
variables	O
in	O
a	O
factor	B
graph	I
there	O
is	O
a	O
node	B
as	O
usual	O
by	O
a	O
circle	O
for	O
every	O
variable	O
in	O
the	O
distribution	O
as	O
was	O
the	O
case	O
for	O
directed	B
and	O
undirected	B
graphs	O
there	O
are	O
also	O
additional	O
nodes	O
by	O
small	O
squares	O
for	O
each	O
factor	O
fsxs	O
in	O
the	O
joint	O
distribution	O
finally	O
there	O
are	O
undirected	B
links	O
connecting	O
each	O
factor	O
node	B
to	O
all	O
of	O
the	O
variables	O
nodes	O
on	O
which	O
that	O
factor	O
depends	O
consider	O
for	O
example	O
a	O
distribution	O
that	O
is	O
expressed	O
in	O
terms	O
of	O
the	O
factorization	B
px	O
this	O
can	O
be	O
expressed	O
by	O
the	O
factor	B
graph	I
shown	O
in	O
figure	O
note	O
that	O
there	O
are	O
two	O
factors	O
and	O
that	O
are	O
defined	O
over	O
the	O
same	O
set	O
of	O
variables	O
in	O
an	O
undirected	B
graph	O
the	O
product	O
of	O
two	O
such	O
factors	O
would	O
simply	O
be	O
lumped	O
together	O
into	O
the	O
same	O
clique	B
potential	O
similarly	O
and	O
could	O
be	O
combined	O
into	O
a	O
single	O
potential	O
over	O
and	O
the	O
factor	B
graph	I
however	O
keeps	O
such	O
factors	O
explicit	O
and	O
so	O
is	O
able	O
to	O
convey	O
more	O
detailed	O
information	O
about	O
the	O
underlying	O
factorization	B
f	O
fb	O
fa	O
figure	O
an	O
undirected	B
graph	O
with	O
a	O
single	O
clique	B
potential	O
a	O
factor	B
graph	I
with	O
factor	O
f	O
representing	O
the	O
same	O
distribution	O
as	O
the	O
undirected	B
graph	O
a	O
different	O
factor	B
graph	I
representing	O
the	O
same	O
distribution	O
whose	O
factors	O
satisfy	O
inference	B
in	O
graphical	O
models	O
f	O
fc	O
fa	O
fb	O
figure	O
a	O
directed	B
graph	O
with	O
the	O
factorization	B
a	O
factor	B
graph	I
representing	O
the	O
same	O
distribution	O
as	O
the	O
directed	B
graph	O
whose	O
factor	O
satisfies	O
f	O
a	O
different	O
factor	B
graph	I
representing	O
the	O
same	O
distribution	O
with	O
factors	O
and	O
factor	O
graphs	O
are	O
said	O
to	O
be	O
bipartite	B
because	O
they	O
consist	O
of	O
two	O
distinct	O
kinds	O
of	O
nodes	O
and	O
all	O
links	O
go	O
between	O
nodes	O
of	O
opposite	O
type	O
in	O
general	O
factor	O
graphs	O
can	O
therefore	O
always	O
be	O
drawn	O
as	O
two	O
rows	O
of	O
nodes	O
nodes	O
at	O
the	O
top	O
and	O
factor	O
nodes	O
at	O
the	O
bottom	O
with	O
links	O
between	O
the	O
rows	O
as	O
shown	O
in	O
the	O
example	O
in	O
figure	O
in	O
some	O
situations	O
however	O
other	O
ways	O
of	O
laying	O
out	O
the	O
graph	O
may	O
be	O
more	O
intuitive	O
for	O
example	O
when	O
the	O
factor	B
graph	I
is	O
derived	O
from	O
a	O
directed	B
or	O
undirected	B
graph	O
as	O
we	O
shall	O
see	O
if	O
we	O
are	O
given	O
a	O
distribution	O
that	O
is	O
expressed	O
in	O
terms	O
of	O
an	O
undirected	B
graph	O
then	O
we	O
can	O
readily	O
convert	O
it	O
to	O
a	O
factor	B
graph	I
to	O
do	O
this	O
we	O
create	O
variable	O
nodes	O
corresponding	O
to	O
the	O
nodes	O
in	O
the	O
original	O
undirected	B
graph	O
and	O
then	O
create	O
additional	O
factor	O
nodes	O
corresponding	O
to	O
the	O
maximal	O
cliques	O
xs	O
the	O
factors	O
fsxs	O
are	O
then	O
set	O
equal	O
to	O
the	O
clique	B
potentials	O
note	O
that	O
there	O
may	O
be	O
several	O
different	O
factor	O
graphs	O
that	O
correspond	O
to	O
the	O
same	O
undirected	B
graph	O
these	O
concepts	O
are	O
illustrated	O
in	O
figure	O
similarly	O
to	O
convert	O
a	O
directed	B
graph	O
to	O
a	O
factor	B
graph	I
we	O
simply	O
create	O
variable	O
nodes	O
in	O
the	O
factor	B
graph	I
corresponding	O
to	O
the	O
nodes	O
of	O
the	O
directed	B
graph	O
and	O
then	O
create	O
factor	O
nodes	O
corresponding	O
to	O
the	O
conditional	B
distributions	O
and	O
then	O
finally	O
add	O
the	O
appropriate	O
links	O
again	O
there	O
can	O
be	O
multiple	O
factor	O
graphs	O
all	O
of	O
which	O
correspond	O
to	O
the	O
same	O
directed	B
graph	O
the	O
conversion	O
of	O
a	O
directed	B
graph	O
to	O
a	O
factor	B
graph	I
is	O
illustrated	O
in	O
figure	O
we	O
have	O
already	O
noted	O
the	O
importance	O
of	O
tree-structured	O
graphs	O
for	O
performing	O
efficient	O
inference	B
if	O
we	O
take	O
a	O
directed	B
or	O
undirected	B
tree	B
and	O
convert	O
it	O
into	O
a	O
factor	B
graph	I
then	O
the	O
result	O
will	O
again	O
be	O
a	O
tree	B
other	O
words	O
the	O
factor	B
graph	I
will	O
have	O
no	O
loops	O
and	O
there	O
will	O
be	O
one	O
and	O
only	O
one	O
path	O
connecting	O
any	O
two	O
nodes	O
in	O
the	O
case	O
of	O
a	O
directed	B
polytree	B
conversion	O
to	O
an	O
undirected	B
graph	O
results	O
in	O
loops	O
due	O
to	O
the	O
moralization	B
step	O
whereas	O
conversion	O
to	O
a	O
factor	B
graph	I
again	O
results	O
in	O
a	O
tree	B
as	O
illustrated	O
in	O
figure	O
in	O
fact	O
local	B
cycles	O
in	O
a	O
directed	B
graph	O
due	O
to	O
links	O
connecting	O
parents	O
of	O
a	O
node	B
can	O
be	O
removed	O
on	O
conversion	O
to	O
a	O
factor	B
graph	I
by	O
defining	O
the	O
appropriate	O
factor	O
function	O
as	O
shown	O
in	O
figure	O
we	O
have	O
seen	O
that	O
multiple	O
different	O
factor	O
graphs	O
can	O
represent	O
the	O
same	O
directed	B
or	O
undirected	B
graph	O
this	O
allows	O
factor	O
graphs	O
to	O
be	O
more	O
specific	O
about	O
the	O
graphical	O
models	O
figure	O
a	O
directed	B
polytree	B
the	O
result	O
of	O
converting	O
the	O
polytree	B
into	O
an	O
undirected	B
graph	O
showing	O
the	O
creation	O
of	O
loops	O
the	O
result	O
of	O
converting	O
the	O
polytree	B
into	O
a	O
factor	B
graph	I
which	O
retains	O
the	O
tree	B
structure	O
precise	O
form	O
of	O
the	O
factorization	B
figure	O
shows	O
an	O
example	O
of	O
a	O
fully	B
connected	I
undirected	B
graph	O
along	O
with	O
two	O
different	O
factor	O
graphs	O
in	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
a	O
general	O
form	O
px	O
whereas	O
in	O
it	O
is	O
given	O
by	O
the	O
more	O
specific	O
factorization	B
px	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
factorization	B
in	O
does	O
not	O
correspond	O
to	O
any	O
conditional	B
independence	I
properties	O
the	O
sum-product	B
algorithm	I
we	O
shall	O
now	O
make	O
use	O
of	O
the	O
factor	B
graph	I
framework	O
to	O
derive	O
a	O
powerful	O
class	O
of	O
efficient	O
exact	O
inference	B
algorithms	O
that	O
are	O
applicable	O
to	O
tree-structured	O
graphs	O
here	O
we	O
shall	O
focus	O
on	O
the	O
problem	O
of	O
evaluating	O
local	B
marginals	O
over	O
nodes	O
or	O
subsets	O
of	O
nodes	O
which	O
will	O
lead	O
us	O
to	O
the	O
sum-product	B
algorithm	I
later	O
we	O
shall	O
modify	O
the	O
technique	O
to	O
allow	O
the	O
most	O
probable	O
state	O
to	O
be	O
found	O
giving	O
rise	O
to	O
the	O
max-sum	B
algorithm	I
also	O
we	O
shall	O
suppose	O
that	O
all	O
of	O
the	O
variables	O
in	O
the	O
model	O
are	O
discrete	O
and	O
so	O
marginalization	O
corresponds	O
to	O
performing	O
sums	O
the	O
framework	O
however	O
is	O
equally	O
applicable	O
to	O
linear-gaussian	O
models	O
in	O
which	O
case	O
marginalization	O
involves	O
integration	O
and	O
we	O
shall	O
consider	O
an	O
example	O
of	O
this	O
in	O
detail	O
when	O
we	O
discuss	O
linear	O
dynamical	O
systems	O
section	O
figure	O
a	O
fragment	O
of	O
a	O
directed	B
graph	O
having	O
a	O
local	B
cycle	O
conversion	O
to	O
a	O
fragment	O
of	O
a	O
factor	B
graph	I
having	O
a	O
tree	B
structure	O
in	O
which	O
f	O
fa	O
inference	B
in	O
graphical	O
models	O
fb	O
fc	O
figure	O
a	O
fully	B
connected	I
undirected	B
graph	O
and	O
two	O
factor	O
graphs	O
each	O
of	O
which	O
corresponds	O
to	O
the	O
undirected	B
graph	O
in	O
there	O
is	O
an	O
algorithm	O
for	O
exact	O
inference	B
on	O
directed	B
graphs	O
without	O
loops	O
known	O
as	O
belief	B
propagation	I
lauritzen	O
and	O
spiegelhalter	O
and	O
is	O
equivalent	O
to	O
a	O
special	O
case	O
of	O
the	O
sum-product	B
algorithm	I
here	O
we	O
shall	O
consider	O
only	O
the	O
sum-product	B
algorithm	I
because	O
it	O
is	O
simpler	O
to	O
derive	O
and	O
to	O
apply	O
as	O
well	O
as	O
being	O
more	O
general	O
we	O
shall	O
assume	O
that	O
the	O
original	O
graph	O
is	O
an	O
undirected	B
tree	B
or	O
a	O
directed	B
tree	B
or	O
polytree	B
so	O
that	O
the	O
corresponding	O
factor	B
graph	I
has	O
a	O
tree	B
structure	O
we	O
first	O
convert	O
the	O
original	O
graph	O
into	O
a	O
factor	B
graph	I
so	O
that	O
we	O
can	O
deal	O
with	O
both	O
directed	B
and	O
undirected	B
models	O
using	O
the	O
same	O
framework	O
our	O
goal	O
is	O
to	O
exploit	O
the	O
structure	O
of	O
the	O
graph	O
to	O
achieve	O
two	O
things	O
to	O
obtain	O
an	O
efficient	O
exact	O
inference	B
algorithm	O
for	O
finding	O
marginals	O
in	O
situations	O
where	O
several	O
marginals	O
are	O
required	O
to	O
allow	O
computations	O
to	O
be	O
shared	O
efficiently	O
we	O
begin	O
by	O
considering	O
the	O
problem	O
of	O
finding	O
the	O
marginal	B
px	O
for	O
particular	O
variable	O
node	B
x	O
for	O
the	O
moment	O
we	O
shall	O
suppose	O
that	O
all	O
of	O
the	O
variables	O
are	O
hidden	O
later	O
we	O
shall	O
see	O
how	O
to	O
modify	O
the	O
algorithm	O
to	O
incorporate	O
evidence	O
corresponding	O
to	O
observed	O
variables	O
by	O
definition	O
the	O
marginal	B
is	O
obtained	O
by	O
summing	O
the	O
joint	O
distribution	O
over	O
all	O
variables	O
except	O
x	O
so	O
that	O
px	O
px	O
xx	O
where	O
x	O
x	O
denotes	O
the	O
set	O
of	O
variables	O
in	O
x	O
with	O
variable	O
x	O
omitted	O
the	O
idea	O
is	O
to	O
substitute	O
for	O
px	O
using	O
the	O
factor	B
graph	I
expression	O
and	O
then	O
interchange	O
summations	O
and	O
products	O
in	O
order	O
to	O
obtain	O
an	O
efficient	O
algorithm	O
consider	O
the	O
fragment	O
of	O
graph	O
shown	O
in	O
figure	O
in	O
which	O
we	O
see	O
that	O
the	O
tree	B
structure	O
of	O
the	O
graph	O
allows	O
us	O
to	O
partition	O
the	O
factors	O
in	O
the	O
joint	O
distribution	O
into	O
groups	O
with	O
one	O
group	O
associated	O
with	O
each	O
of	O
the	O
factor	O
nodes	O
that	O
is	O
a	O
neighbour	O
of	O
the	O
variable	O
node	B
x	O
we	O
see	O
that	O
the	O
joint	O
distribution	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
the	O
form	O
px	O
fsx	O
xs	O
s	O
nex	O
nex	O
denotes	O
the	O
set	O
of	O
factor	O
nodes	O
that	O
are	O
neighbours	O
of	O
x	O
and	O
xs	O
denotes	O
the	O
set	O
of	O
all	O
variables	O
in	O
the	O
subtree	O
connected	O
to	O
the	O
variable	O
node	B
x	O
via	O
the	O
factor	O
node	B
graphical	O
models	O
figure	O
a	O
fragment	O
of	O
a	O
factor	B
graph	I
illustrating	O
the	O
evaluation	O
of	O
the	O
marginal	B
px	O
s	O
x	O
x	O
s	O
f	O
fs	O
xx	O
fs	O
x	O
fs	O
and	O
fsx	O
xs	O
represents	O
the	O
product	O
of	O
all	O
the	O
factors	O
in	O
the	O
group	O
associated	O
with	O
factor	O
fs	O
substituting	O
into	O
and	O
interchanging	O
the	O
sums	O
and	O
products	O
we	O
ob	O
tain	O
px	O
s	O
nex	O
s	O
nex	O
fsx	O
xs	O
xs	O
fs	O
xx	O
fs	O
xx	O
fsx	O
xs	O
here	O
we	O
have	O
introduced	O
a	O
set	O
of	O
functions	O
fs	O
xx	O
defined	O
by	O
xs	O
which	O
can	O
be	O
viewed	O
as	O
messages	O
from	O
the	O
factor	O
nodes	O
fs	O
to	O
the	O
variable	O
node	B
x	O
we	O
see	O
that	O
the	O
required	O
marginal	B
px	O
is	O
given	O
by	O
the	O
product	O
of	O
all	O
the	O
incoming	O
messages	O
arriving	O
at	O
node	B
x	O
in	O
order	O
to	O
evaluate	O
these	O
messages	O
we	O
again	O
turn	O
to	O
figure	O
and	O
note	O
that	O
each	O
factor	O
fsx	O
xs	O
is	O
described	O
by	O
a	O
factor	O
and	O
so	O
can	O
itself	O
be	O
factorized	O
in	O
particular	O
we	O
can	O
write	O
fsx	O
xs	O
fsx	O
xm	O
gm	O
xsm	O
where	O
for	O
convenience	O
we	O
have	O
denoted	O
the	O
variables	O
associated	O
with	O
factor	O
fx	O
in	O
addition	O
to	O
x	O
by	O
xm	O
this	O
factorization	B
is	O
illustrated	O
in	O
figure	O
note	O
that	O
the	O
set	O
of	O
variables	O
xm	O
is	O
the	O
set	O
of	O
variables	O
on	O
which	O
the	O
factor	O
fs	O
depends	O
and	O
so	O
it	O
can	O
also	O
be	O
denoted	O
xs	O
using	O
the	O
notation	O
of	O
substituting	O
into	O
we	O
obtain	O
fs	O
xx	O
fsx	O
xm	O
fsx	O
xm	O
gmxm	O
xsm	O
xxm	O
xm	O
fsxm	O
xm	O
xm	O
m	O
nefsx	O
m	O
nefsx	O
inference	B
in	O
graphical	O
models	O
figure	O
illustration	O
of	O
the	O
factorization	B
of	O
the	O
subgraph	O
as	O
sociated	O
with	O
factor	O
node	B
fs	O
xm	O
xm	O
fsxm	O
fs	O
fs	O
xx	O
x	O
xm	O
gmxm	O
xsm	O
where	O
nefs	O
denotes	O
the	O
set	O
of	O
variable	O
nodes	O
that	O
are	O
neighbours	O
of	O
the	O
factor	O
node	B
fs	O
and	O
nefs	O
x	O
denotes	O
the	O
same	O
set	O
but	O
with	O
node	B
x	O
removed	O
here	O
we	O
have	O
defined	O
the	O
following	O
messages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
xm	O
fsxm	O
gmxm	O
xsm	O
xsm	O
we	O
have	O
therefore	O
introduced	O
two	O
distinct	O
kinds	O
of	O
message	O
those	O
that	O
go	O
from	O
factor	O
nodes	O
to	O
variable	O
nodes	O
denoted	O
f	O
xx	O
and	O
those	O
that	O
go	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
denoted	O
x	O
f	O
in	O
each	O
case	O
we	O
see	O
that	O
messages	O
passed	O
along	O
a	O
link	B
are	O
always	O
a	O
function	O
of	O
the	O
variable	O
associated	O
with	O
the	O
variable	O
node	B
that	O
link	B
connects	O
to	O
the	O
result	O
says	O
that	O
to	O
evaluate	O
the	O
message	O
sent	O
by	O
a	O
factor	O
node	B
to	O
a	O
variable	O
node	B
along	O
the	O
link	B
connecting	O
them	O
take	O
the	O
product	O
of	O
the	O
incoming	O
messages	O
along	O
all	O
other	O
links	O
coming	O
into	O
the	O
factor	O
node	B
multiply	O
by	O
the	O
factor	O
associated	O
with	O
that	O
node	B
and	O
then	O
marginalize	O
over	O
all	O
of	O
the	O
variables	O
associated	O
with	O
the	O
incoming	O
messages	O
this	O
is	O
illustrated	O
in	O
figure	O
it	O
is	O
important	O
to	O
note	O
that	O
a	O
factor	O
node	B
can	O
send	O
a	O
message	O
to	O
a	O
variable	O
node	B
once	O
it	O
has	O
received	O
incoming	O
messages	O
from	O
all	O
other	O
neighbouring	O
variable	O
nodes	O
finally	O
we	O
derive	O
an	O
expression	O
for	O
evaluating	O
the	O
messages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
again	O
by	O
making	O
use	O
of	O
the	O
factorization	B
from	O
figure	O
we	O
see	O
that	O
term	O
gmxm	O
xsm	O
associated	O
with	O
node	B
xm	O
is	O
given	O
by	O
a	O
product	O
of	O
terms	O
flxm	O
xml	O
each	O
associated	O
with	O
one	O
of	O
the	O
factor	O
nodes	O
fl	O
that	O
is	O
linked	O
to	O
node	B
xm	O
node	B
fs	O
so	O
that	O
gmxm	O
xsm	O
flxm	O
xml	O
l	O
nexmfs	O
where	O
the	O
product	O
is	O
taken	O
over	O
all	O
neighbours	O
of	O
node	B
xm	O
except	O
for	O
node	B
fs	O
note	O
that	O
each	O
of	O
the	O
factors	O
flxm	O
xml	O
represents	O
a	O
subtree	O
of	O
the	O
original	O
graph	O
of	O
precisely	O
the	O
same	O
kind	O
as	O
introduced	O
in	O
substituting	O
into	O
we	O
graphical	O
models	O
figure	O
illustration	O
of	O
the	O
evaluation	O
of	O
the	O
message	O
sent	O
by	O
a	O
variable	O
node	B
to	O
an	O
adjacent	O
factor	O
node	B
fl	O
fl	O
xm	O
fs	O
then	O
obtain	O
xm	O
fsxm	O
l	O
nexmfs	O
l	O
nexmfs	O
flxm	O
xml	O
flxm	O
xml	O
xml	O
fl	O
xmxm	O
where	O
we	O
have	O
used	O
the	O
definition	O
of	O
the	O
messages	O
passed	O
from	O
factor	O
nodes	O
to	O
variable	O
nodes	O
thus	O
to	O
evaluate	O
the	O
message	O
sent	O
by	O
a	O
variable	O
node	B
to	O
an	O
adjacent	O
factor	O
node	B
along	O
the	O
connecting	O
link	B
we	O
simply	O
take	O
the	O
product	O
of	O
the	O
incoming	O
messages	O
along	O
all	O
of	O
the	O
other	O
links	O
note	O
that	O
any	O
variable	O
node	B
that	O
has	O
only	O
two	O
neighbours	O
performs	O
no	O
computation	O
but	O
simply	O
passes	O
messages	O
through	O
unchanged	O
also	O
we	O
note	O
that	O
a	O
variable	O
node	B
can	O
send	O
a	O
message	O
to	O
a	O
factor	O
node	B
once	O
it	O
has	O
received	O
incoming	O
messages	O
from	O
all	O
other	O
neighbouring	O
factor	O
nodes	O
recall	O
that	O
our	O
goal	O
is	O
to	O
calculate	O
the	O
marginal	B
for	O
variable	O
node	B
x	O
and	O
that	O
this	O
marginal	B
is	O
given	O
by	O
the	O
product	O
of	O
incoming	O
messages	O
along	O
all	O
of	O
the	O
links	O
arriving	O
at	O
that	O
node	B
each	O
of	O
these	O
messages	O
can	O
be	O
computed	O
recursively	O
in	O
terms	O
of	O
other	O
messages	O
in	O
order	O
to	O
start	O
this	O
recursion	O
we	O
can	O
view	O
the	O
node	B
x	O
as	O
the	O
root	O
of	O
the	O
tree	B
and	O
begin	O
at	O
the	O
leaf	O
nodes	O
from	O
the	O
definition	O
we	O
see	O
that	O
if	O
a	O
leaf	O
node	B
is	O
a	O
variable	O
node	B
then	O
the	O
message	O
that	O
it	O
sends	O
along	O
its	O
one	O
and	O
only	O
link	B
is	O
given	O
by	O
as	O
illustrated	O
in	O
figure	O
similarly	O
if	O
the	O
leaf	O
node	B
is	O
a	O
factor	O
node	B
we	O
see	O
from	O
that	O
the	O
message	O
sent	O
should	O
take	O
the	O
form	O
x	O
f	O
f	O
xx	O
fx	O
figure	O
the	O
sum-product	B
algorithm	I
begins	O
with	O
messages	O
sent	O
by	O
the	O
leaf	O
nodes	O
which	O
depend	O
on	O
whether	O
the	O
leaf	O
node	B
is	O
a	O
variable	O
node	B
or	O
a	O
factor	O
node	B
x	O
f	O
f	O
xx	O
fx	O
x	O
f	O
f	O
x	O
inference	B
in	O
graphical	O
models	O
as	O
illustrated	O
in	O
figure	O
at	O
this	O
point	O
it	O
is	O
worth	O
pausing	O
to	O
summarize	O
the	O
particular	O
version	O
of	O
the	O
sumproduct	O
algorithm	O
obtained	O
so	O
far	O
for	O
evaluating	O
the	O
marginal	B
px	O
we	O
start	O
by	O
viewing	O
the	O
variable	O
node	B
x	O
as	O
the	O
root	O
of	O
the	O
factor	B
graph	I
and	O
initiating	O
messages	O
at	O
the	O
leaves	O
of	O
the	O
graph	O
using	O
and	O
the	O
message	B
passing	I
steps	O
and	O
are	O
then	O
applied	O
recursively	O
until	O
messages	O
have	O
been	O
propagated	O
along	O
every	O
link	B
and	O
the	O
root	B
node	B
has	O
received	O
messages	O
from	O
all	O
of	O
its	O
neighbours	O
each	O
node	B
can	O
send	O
a	O
message	O
towards	O
the	O
root	O
once	O
it	O
has	O
received	O
messages	O
from	O
all	O
of	O
its	O
other	O
neighbours	O
once	O
the	O
root	B
node	B
has	O
received	O
messages	O
from	O
all	O
of	O
its	O
neighbours	O
the	O
required	O
marginal	B
can	O
be	O
evaluated	O
using	O
we	O
shall	O
illustrate	O
this	O
process	O
shortly	O
to	O
see	O
that	O
each	O
node	B
will	O
always	O
receive	O
enough	O
messages	O
to	O
be	O
able	O
to	O
send	O
out	O
a	O
message	O
we	O
can	O
use	O
a	O
simple	O
inductive	O
argument	O
as	O
follows	O
clearly	O
for	O
a	O
graph	O
comprising	O
a	O
variable	O
root	B
node	B
connected	O
directly	O
to	O
several	O
factor	O
leaf	O
nodes	O
the	O
algorithm	O
trivially	O
involves	O
sending	O
messages	O
of	O
the	O
form	O
directly	O
from	O
the	O
leaves	O
to	O
the	O
root	O
now	O
imagine	O
building	O
up	O
a	O
general	O
graph	O
by	O
adding	O
nodes	O
one	O
at	O
a	O
time	O
and	O
suppose	O
that	O
for	O
some	O
particular	O
graph	O
we	O
have	O
a	O
valid	O
algorithm	O
when	O
one	O
more	O
or	O
factor	O
node	B
is	O
added	O
it	O
can	O
be	O
connected	O
only	O
by	O
a	O
single	O
link	B
because	O
the	O
overall	O
graph	O
must	O
remain	O
a	O
tree	B
and	O
so	O
the	O
new	O
node	B
will	O
be	O
a	O
leaf	O
node	B
it	O
therefore	O
sends	O
a	O
message	O
to	O
the	O
node	B
to	O
which	O
it	O
is	O
linked	O
which	O
in	O
turn	O
will	O
therefore	O
receive	O
all	O
the	O
messages	O
it	O
requires	O
in	O
order	O
to	O
send	O
its	O
own	O
message	O
towards	O
the	O
root	O
and	O
so	O
again	O
we	O
have	O
a	O
valid	O
algorithm	O
thereby	O
completing	O
the	O
proof	O
now	O
suppose	O
we	O
wish	O
to	O
find	O
the	O
marginals	O
for	O
every	O
variable	O
node	B
in	O
the	O
graph	O
this	O
could	O
be	O
done	O
by	O
simply	O
running	O
the	O
above	O
algorithm	O
afresh	O
for	O
each	O
such	O
node	B
however	O
this	O
would	O
be	O
very	O
wasteful	O
as	O
many	O
of	O
the	O
required	O
computations	O
would	O
be	O
repeated	O
we	O
can	O
obtain	O
a	O
much	O
more	O
efficient	O
procedure	O
by	O
overlaying	O
these	O
multiple	O
message	B
passing	I
algorithms	O
to	O
obtain	O
the	O
general	O
sum-product	B
algorithm	I
as	O
follows	O
arbitrarily	O
pick	O
any	O
or	O
factor	O
node	B
and	O
designate	O
it	O
as	O
the	O
root	O
propagate	O
messages	O
from	O
the	O
leaves	O
to	O
the	O
root	O
as	O
before	O
at	O
this	O
point	O
the	O
root	B
node	B
will	O
have	O
received	O
messages	O
from	O
all	O
of	O
its	O
neighbours	O
it	O
can	O
therefore	O
send	O
out	O
messages	O
to	O
all	O
of	O
its	O
neighbours	O
these	O
in	O
turn	O
will	O
then	O
have	O
received	O
messages	O
from	O
all	O
of	O
their	O
neighbours	O
and	O
so	O
can	O
send	O
out	O
messages	O
along	O
the	O
links	O
going	O
away	O
from	O
the	O
root	O
and	O
so	O
on	O
in	O
this	O
way	O
messages	O
are	O
passed	O
outwards	O
from	O
the	O
root	O
all	O
the	O
way	O
to	O
the	O
leaves	O
by	O
now	O
a	O
message	O
will	O
have	O
passed	O
in	O
both	O
directions	O
across	O
every	O
link	B
in	O
the	O
graph	O
and	O
every	O
node	B
will	O
have	O
received	O
a	O
message	O
from	O
all	O
of	O
its	O
neighbours	O
again	O
a	O
simple	O
inductive	O
argument	O
can	O
be	O
used	O
to	O
verify	O
the	O
validity	O
of	O
this	O
message	B
passing	I
protocol	O
because	O
every	O
variable	O
node	B
will	O
have	O
received	O
messages	O
from	O
all	O
of	O
its	O
neighbours	O
we	O
can	O
readily	O
calculate	O
the	O
marginal	B
distribution	O
for	O
every	O
variable	O
in	O
the	O
graph	O
the	O
number	O
of	O
messages	O
that	O
have	O
to	O
be	O
computed	O
is	O
given	O
by	O
twice	O
the	O
number	O
of	O
links	O
in	O
the	O
graph	O
and	O
so	O
involves	O
only	O
twice	O
the	O
computation	O
involved	O
in	O
finding	O
a	O
single	O
marginal	B
by	O
comparison	O
if	O
we	O
had	O
run	O
the	O
sum-product	B
algorithm	I
separately	O
for	O
each	O
node	B
the	O
amount	O
of	O
computation	O
would	O
grow	O
quadratically	O
with	O
the	O
size	O
of	O
the	O
graph	O
note	O
that	O
this	O
algorithm	O
is	O
in	O
fact	O
independent	B
of	O
which	O
node	B
was	O
designated	O
as	O
the	O
root	O
exercise	O
graphical	O
models	O
figure	O
the	O
sum-product	B
algorithm	I
can	O
be	O
viewed	O
purely	O
in	O
terms	O
of	O
messages	O
sent	O
out	O
by	O
factor	O
nodes	O
to	O
other	O
factor	O
nodes	O
in	O
this	O
example	O
the	O
outgoing	O
message	O
shown	O
by	O
the	O
blue	O
arrow	O
is	O
obtained	O
by	O
taking	O
the	O
product	O
of	O
all	O
the	O
incoming	O
messages	O
shown	O
by	O
green	O
arrows	O
multiplying	O
by	O
the	O
factor	O
fs	O
and	O
marginalizing	O
over	O
the	O
variables	O
and	O
fs	O
exercise	O
and	O
indeed	O
the	O
notion	O
of	O
one	O
node	B
having	O
a	O
special	O
status	O
was	O
introduced	O
only	O
as	O
a	O
convenient	O
way	O
to	O
explain	O
the	O
message	B
passing	I
protocol	O
next	O
suppose	O
we	O
wish	O
to	O
find	O
the	O
marginal	B
distributions	O
pxs	O
associated	O
with	O
the	O
sets	O
of	O
variables	O
belonging	O
to	O
each	O
of	O
the	O
factors	O
by	O
a	O
similar	O
argument	O
to	O
that	O
used	O
above	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
marginal	B
associated	O
with	O
a	O
factor	O
is	O
given	O
by	O
the	O
product	O
of	O
messages	O
arriving	O
at	O
the	O
factor	O
node	B
and	O
the	O
local	B
factor	O
at	O
that	O
node	B
pxs	O
fsxs	O
xi	O
fsxi	O
i	O
nefs	O
in	O
complete	O
analogy	O
with	O
the	O
marginals	O
at	O
the	O
variable	O
nodes	O
if	O
the	O
factors	O
are	O
parameterized	O
functions	O
and	O
we	O
wish	O
to	O
learn	O
the	O
values	O
of	O
the	O
parameters	O
using	O
the	O
em	B
algorithm	I
then	O
these	O
marginals	O
are	O
precisely	O
the	O
quantities	O
we	O
will	O
need	O
to	O
calculate	O
in	O
the	O
e	O
step	O
as	O
we	O
shall	O
see	O
in	O
detail	O
when	O
we	O
discuss	O
the	O
hidden	B
markov	B
model	I
in	O
chapter	O
the	O
message	O
sent	O
by	O
a	O
variable	O
node	B
to	O
a	O
factor	O
node	B
as	O
we	O
have	O
seen	O
is	O
simply	O
the	O
product	O
of	O
the	O
incoming	O
messages	O
on	O
other	O
links	O
we	O
can	O
if	O
we	O
wish	O
view	O
the	O
sum-product	B
algorithm	I
in	O
a	O
slightly	O
different	O
form	O
by	O
eliminating	O
messages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
and	O
simply	O
considering	O
messages	O
that	O
are	O
sent	O
out	O
by	O
factor	O
nodes	O
this	O
is	O
most	O
easily	O
seen	O
by	O
considering	O
the	O
example	O
in	O
figure	O
so	O
far	O
we	O
have	O
rather	O
neglected	O
the	O
issue	O
of	O
normalization	O
if	O
the	O
factor	B
graph	I
was	O
derived	O
from	O
a	O
directed	B
graph	O
then	O
the	O
joint	O
distribution	O
is	O
already	O
correctly	O
normalized	O
and	O
so	O
the	O
marginals	O
obtained	O
by	O
the	O
sum-product	B
algorithm	I
will	O
similarly	O
be	O
normalized	O
correctly	O
however	O
if	O
we	O
started	O
from	O
an	O
undirected	B
graph	O
then	O
in	O
general	O
there	O
will	O
be	O
an	O
unknown	O
normalization	O
coefficient	O
as	O
with	O
the	O
simple	O
chain	O
example	O
of	O
figure	O
this	O
is	O
easily	O
handled	O
by	O
working	O
with	O
an	O
unnormal	O
ized	O
of	O
the	O
joint	O
distribution	O
where	O
px	O
we	O
first	O
run	O
the	O
sum-product	B
algorithm	I
to	O
find	O
the	O
corresponding	O
unnormalized	O
the	O
directly	O
coefficient	O
is	O
then	O
easily	O
obtained	O
by	O
normalizing	O
any	O
one	O
of	O
these	O
marginals	O
and	O
this	O
is	O
computationally	O
efficient	O
because	O
the	O
normalization	O
is	O
done	O
over	O
a	O
single	O
variable	O
rather	O
than	O
over	O
the	O
entire	O
set	O
of	O
variables	O
as	O
would	O
be	O
required	O
to	O
normalize	O
at	O
this	O
point	O
it	O
may	O
be	O
helpful	O
to	O
consider	O
a	O
simple	O
example	O
to	O
illustrate	O
the	O
operation	O
of	O
the	O
sum-product	B
algorithm	I
figure	O
shows	O
a	O
simple	O
factor	O
figure	O
a	O
simple	O
factor	B
graph	I
used	O
to	O
illustrate	O
the	O
sum-product	B
algorithm	I
inference	B
in	O
graphical	O
models	O
fa	O
fb	O
fc	O
graph	O
whose	O
unnormalized	O
joint	O
distribution	O
is	O
given	O
by	O
in	O
order	O
to	O
apply	O
the	O
sum-product	B
algorithm	I
to	O
this	O
graph	O
let	O
us	O
designate	O
node	B
as	O
the	O
root	O
in	O
which	O
case	O
there	O
are	O
two	O
leaf	O
nodes	O
and	O
starting	O
with	O
the	O
leaf	O
nodes	O
we	O
then	O
have	O
the	O
following	O
sequence	O
of	O
six	O
messages	O
the	O
direction	O
of	O
flow	O
of	O
these	O
messages	O
is	O
illustrated	O
in	O
figure	O
once	O
this	O
message	O
propagation	O
is	O
complete	O
we	O
can	O
then	O
propagate	O
messages	O
from	O
the	O
root	B
node	B
out	O
to	O
the	O
leaf	O
nodes	O
and	O
these	O
are	O
given	O
by	O
fa	O
fc	O
fa	O
fc	O
fb	O
fb	O
fb	O
fb	O
fc	O
fa	O
fa	O
fb	O
fc	O
graphical	O
models	O
figure	O
flow	O
of	O
messages	O
for	O
the	O
sum-product	B
algorithm	I
applied	O
to	O
the	O
example	O
graph	O
in	O
figure	O
from	O
the	O
leaf	O
nodes	O
and	O
towards	O
the	O
root	B
node	B
from	O
the	O
root	B
node	B
towards	O
the	O
leaf	O
nodes	O
one	O
message	O
has	O
now	O
passed	O
in	O
each	O
direction	O
across	O
each	O
link	B
and	O
we	O
can	O
now	O
evaluate	O
the	O
marginals	O
as	O
a	O
simple	O
check	O
let	O
us	O
verify	O
that	O
the	O
marginal	B
is	O
given	O
by	O
the	O
correct	O
expression	O
using	O
and	O
substituting	O
for	O
the	O
messages	O
using	O
the	O
above	O
results	O
we	O
have	O
fa	O
fb	O
fc	O
as	O
required	O
so	O
far	O
we	O
have	O
assumed	O
that	O
all	O
of	O
the	O
variables	O
in	O
the	O
graph	O
are	O
hidden	O
in	O
most	O
practical	O
applications	O
a	O
subset	O
of	O
the	O
variables	O
will	O
be	O
observed	O
and	O
we	O
wish	O
to	O
calculate	O
posterior	O
distributions	O
conditioned	O
on	O
these	O
observations	O
observed	O
nodes	O
are	O
easily	O
handled	O
within	O
the	O
sum-product	B
algorithm	I
as	O
follows	O
suppose	O
we	O
partition	O
x	O
into	O
hidden	O
variables	O
h	O
and	O
observed	O
variables	O
v	O
and	O
that	O
the	O
observed	O
value	O
of	O
v	O
is	O
then	O
we	O
simply	O
multiply	O
the	O
joint	O
distribution	O
px	O
by	O
i	O
where	O
if	O
v	O
and	O
otherwise	O
this	O
product	O
corresponds	O
to	O
ph	O
v	O
and	O
hence	O
is	O
an	O
unnormalized	O
version	O
of	O
phv	O
by	O
runphiv	O
up	O
to	O
a	O
normalization	O
coefficient	O
whose	O
value	O
can	O
be	O
found	O
efficiently	O
ning	O
the	O
sum-product	B
algorithm	I
we	O
can	O
efficiently	O
calculate	O
the	O
posterior	O
marginals	O
using	O
a	O
local	B
computation	O
any	O
summations	O
over	O
variables	O
in	O
v	O
then	O
collapse	O
into	O
a	O
single	O
term	O
we	O
have	O
assumed	O
throughout	O
this	O
section	O
that	O
we	O
are	O
dealing	O
with	O
discrete	O
variables	O
however	O
there	O
is	O
nothing	O
specific	O
to	O
discrete	O
variables	O
either	O
in	O
the	O
graphical	O
framework	O
or	O
in	O
the	O
probabilistic	O
construction	O
of	O
the	O
sum-product	B
algorithm	I
for	O
inference	B
in	O
graphical	O
models	O
table	O
example	O
of	O
a	O
joint	O
distribution	O
over	O
two	O
binary	O
variables	O
for	O
which	O
the	O
maximum	O
of	O
the	O
joint	O
distribution	O
occurs	O
for	O
different	O
variable	O
values	O
compared	O
to	O
the	O
maxima	O
of	O
the	O
two	O
marginals	O
y	O
y	O
x	O
x	O
section	O
continuous	O
variables	O
the	O
summations	O
are	O
simply	O
replaced	O
by	O
integrations	O
we	O
shall	O
give	O
an	O
example	O
of	O
the	O
sum-product	B
algorithm	I
applied	O
to	O
a	O
graph	O
of	O
linear-gaussian	O
variables	O
when	O
we	O
consider	O
linear	O
dynamical	O
systems	O
the	O
max-sum	B
algorithm	I
the	O
sum-product	B
algorithm	I
allows	O
us	O
to	O
take	O
a	O
joint	O
distribution	O
px	O
expressed	O
as	O
a	O
factor	B
graph	I
and	O
efficiently	O
find	O
marginals	O
over	O
the	O
component	O
variables	O
two	O
other	O
common	O
tasks	O
are	O
to	O
find	O
a	O
setting	O
of	O
the	O
variables	O
that	O
has	O
the	O
largest	O
probability	B
and	O
to	O
find	O
the	O
value	O
of	O
that	O
probability	B
these	O
can	O
be	O
addressed	O
through	O
a	O
closely	O
related	O
algorithm	O
called	O
max-sum	O
which	O
can	O
be	O
viewed	O
as	O
an	O
application	O
of	O
dynamic	B
programming	I
in	O
the	O
context	O
of	O
graphical	O
models	O
et	O
al	O
a	O
simple	O
approach	O
to	O
finding	O
latent	B
variable	I
values	O
having	O
high	O
probability	B
would	O
be	O
to	O
run	O
the	O
sum-product	B
algorithm	I
to	O
obtain	O
the	O
marginals	O
pxi	O
for	O
every	O
variable	O
and	O
then	O
for	O
each	O
marginal	B
in	O
turn	O
to	O
find	O
the	O
value	O
i	O
that	O
maximizes	O
that	O
marginal	B
however	O
this	O
would	O
give	O
the	O
set	O
of	O
values	O
that	O
are	O
individually	O
the	O
most	O
probable	O
in	O
practice	O
we	O
typically	O
wish	O
to	O
find	O
the	O
set	O
of	O
values	O
that	O
jointly	O
have	O
the	O
largest	O
probability	B
in	O
other	O
words	O
the	O
vector	O
xmax	O
that	O
maximizes	O
the	O
joint	O
distribution	O
so	O
that	O
xmax	O
arg	O
max	O
px	O
for	O
which	O
the	O
corresponding	O
value	O
of	O
the	O
joint	O
probability	B
will	O
be	O
given	O
by	O
pxmax	O
max	O
x	O
px	O
x	O
in	O
general	O
xmax	O
is	O
not	O
the	O
same	O
as	O
the	O
set	O
of	O
i	O
values	O
as	O
we	O
can	O
easily	O
show	O
using	O
a	O
simple	O
example	O
consider	O
the	O
joint	O
distribution	O
px	O
y	O
over	O
two	O
binary	O
variables	O
x	O
y	O
given	O
in	O
table	O
the	O
joint	O
distribution	O
is	O
maximized	O
by	O
setting	O
x	O
and	O
y	O
corresponding	O
the	O
value	O
however	O
the	O
marginal	B
for	O
px	O
obtained	O
by	O
summing	O
over	O
both	O
values	O
of	O
y	O
is	O
given	O
by	O
px	O
and	O
px	O
and	O
similarly	O
the	O
marginal	B
for	O
y	O
is	O
given	O
by	O
py	O
and	O
py	O
and	O
so	O
the	O
marginals	O
are	O
maximized	O
by	O
x	O
and	O
y	O
which	O
corresponds	O
to	O
a	O
value	O
of	O
for	O
the	O
joint	O
distribution	O
in	O
fact	O
it	O
is	O
not	O
difficult	O
to	O
construct	O
examples	O
for	O
which	O
the	O
set	O
of	O
individually	O
most	O
probable	O
values	O
has	O
probability	B
zero	O
under	O
the	O
joint	O
distribution	O
we	O
therefore	O
seek	O
an	O
efficient	O
algorithm	O
for	O
finding	O
the	O
value	O
of	O
x	O
that	O
maximizes	O
the	O
joint	O
distribution	O
px	O
and	O
that	O
will	O
allow	O
us	O
to	O
obtain	O
the	O
value	O
of	O
the	O
joint	O
distribution	O
at	O
its	O
maximum	O
to	O
address	O
the	O
second	O
of	O
these	O
problems	O
we	O
shall	O
simply	O
write	O
out	O
the	O
max	O
operator	O
in	O
terms	O
of	O
its	O
components	O
max	O
x	O
px	O
max	O
max	O
xm	O
px	O
exercise	O
graphical	O
models	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
variables	O
and	O
then	O
substitute	O
for	O
px	O
using	O
its	O
expansion	O
in	O
terms	O
of	O
a	O
product	O
of	O
factors	O
in	O
deriving	O
the	O
sum-product	B
algorithm	I
we	O
made	O
use	O
of	O
the	O
distributive	O
law	O
for	O
multiplication	O
here	O
we	O
make	O
use	O
of	O
the	O
analogous	O
law	O
for	O
the	O
max	O
operator	O
which	O
holds	O
if	O
a	O
will	O
always	O
be	O
the	O
case	O
for	O
the	O
factors	O
in	O
a	O
graphical	B
model	I
this	O
allows	O
us	O
to	O
exchange	O
products	O
with	O
maximizations	O
maxab	O
ac	O
a	O
maxb	O
c	O
consider	O
first	O
the	O
simple	O
example	O
of	O
a	O
chain	O
of	O
nodes	O
described	O
by	O
the	O
evaluation	O
of	O
the	O
probability	B
maximum	O
can	O
be	O
written	O
as	O
max	O
x	O
px	O
max	O
z	O
max	O
xn	O
z	O
max	O
n	O
xn	O
max	O
n	O
xn	O
xn	O
as	O
with	O
the	O
calculation	O
of	O
marginals	O
we	O
see	O
that	O
exchanging	O
the	O
max	O
and	O
product	O
operators	O
results	O
in	O
a	O
much	O
more	O
efficient	O
computation	O
and	O
one	O
that	O
is	O
easily	O
interpreted	O
in	O
terms	O
of	O
messages	O
passed	O
from	O
node	B
xn	O
backwards	O
along	O
the	O
chain	O
to	O
node	B
we	O
can	O
readily	O
generalize	O
this	O
result	O
to	O
arbitrary	O
tree-structured	O
factor	O
graphs	O
by	O
substituting	O
the	O
expression	O
for	O
the	O
factor	B
graph	I
expansion	O
into	O
and	O
again	O
exchanging	O
maximizations	O
with	O
products	O
the	O
structure	O
of	O
this	O
calculation	O
is	O
identical	O
to	O
that	O
of	O
the	O
sum-product	B
algorithm	I
and	O
so	O
we	O
can	O
simply	O
translate	O
those	O
results	O
into	O
the	O
present	O
context	O
in	O
particular	O
suppose	O
that	O
we	O
designate	O
a	O
particular	O
variable	O
node	B
as	O
the	O
root	O
of	O
the	O
graph	O
then	O
we	O
start	O
a	O
set	O
of	O
messages	O
propagating	O
inwards	O
from	O
the	O
leaves	O
of	O
the	O
tree	B
towards	O
the	O
root	O
with	O
each	O
node	B
sending	O
its	O
message	O
towards	O
the	O
root	O
once	O
it	O
has	O
received	O
all	O
incoming	O
messages	O
from	O
its	O
other	O
neighbours	O
the	O
final	O
maximization	O
is	O
performed	O
over	O
the	O
product	O
of	O
all	O
messages	O
arriving	O
at	O
the	O
root	B
node	B
and	O
gives	O
the	O
maximum	O
value	O
for	O
px	O
this	O
could	O
be	O
called	O
the	O
max-product	O
algorithm	O
and	O
is	O
identical	O
to	O
the	O
sum-product	B
algorithm	I
except	O
that	O
summations	O
are	O
replaced	O
by	O
maximizations	O
note	O
that	O
at	O
this	O
stage	O
messages	O
have	O
been	O
sent	O
from	O
leaves	O
to	O
the	O
root	O
but	O
not	O
in	O
the	O
other	O
direction	O
in	O
practice	O
products	O
of	O
many	O
small	O
probabilities	O
can	O
lead	O
to	O
numerical	O
underflow	O
problems	O
and	O
so	O
it	O
is	O
convenient	O
to	O
work	O
with	O
the	O
logarithm	O
of	O
the	O
joint	O
distribution	O
the	O
logarithm	O
is	O
a	O
monotonic	O
function	O
so	O
that	O
if	O
a	O
b	O
then	O
ln	O
a	O
ln	O
b	O
and	O
hence	O
the	O
max	O
operator	O
and	O
the	O
logarithm	O
function	O
can	O
be	O
interchanged	O
so	O
that	O
ln	O
max	O
x	O
px	O
max	O
x	O
ln	O
px	O
the	O
distributive	O
property	O
is	O
preserved	O
because	O
maxa	O
b	O
a	O
c	O
a	O
maxb	O
c	O
thus	O
taking	O
the	O
logarithm	O
simply	O
has	O
the	O
effect	O
of	O
replacing	O
the	O
products	O
in	O
the	O
max-product	O
algorithm	O
with	O
sums	O
and	O
so	O
we	O
obtain	O
the	O
max-sum	B
algorithm	I
from	O
inference	B
in	O
graphical	O
models	O
the	O
results	O
and	O
derived	O
earlier	O
for	O
the	O
sum-product	B
algorithm	I
we	O
can	O
readily	O
write	O
down	O
the	O
max-sum	B
algorithm	I
in	O
terms	O
of	O
message	B
passing	I
simply	O
by	O
replacing	O
sum	O
with	O
max	O
and	O
replacing	O
products	O
with	O
sums	O
of	O
logarithms	O
to	O
give	O
ln	O
fx	O
xm	O
xm	O
f	O
m	O
nefsx	O
x	O
f	O
fl	O
xx	O
f	O
xx	O
max	O
l	O
nexf	O
the	O
initial	O
messages	O
sent	O
by	O
the	O
leaf	O
nodes	O
are	O
obtained	O
by	O
analogy	O
with	O
and	O
and	O
are	O
given	O
by	O
x	O
f	O
f	O
xx	O
ln	O
fx	O
pmax	O
max	O
x	O
fs	O
xx	O
s	O
nex	O
s	O
nex	O
while	O
at	O
the	O
root	B
node	B
the	O
maximum	O
probability	B
can	O
then	O
be	O
computed	O
by	O
analogy	O
with	O
using	O
so	O
far	O
we	O
have	O
seen	O
how	O
to	O
find	O
the	O
maximum	O
of	O
the	O
joint	O
distribution	O
by	O
propagating	O
messages	O
from	O
the	O
leaves	O
to	O
an	O
arbitrarily	O
chosen	O
root	B
node	B
the	O
result	O
will	O
be	O
the	O
same	O
irrespective	O
of	O
which	O
node	B
is	O
chosen	O
as	O
the	O
root	O
now	O
we	O
turn	O
to	O
the	O
second	O
problem	O
of	O
finding	O
the	O
configuration	O
of	O
the	O
variables	O
for	O
which	O
the	O
joint	O
distribution	O
attains	O
this	O
maximum	O
value	O
so	O
far	O
we	O
have	O
sent	O
messages	O
from	O
the	O
leaves	O
to	O
the	O
root	O
the	O
process	O
of	O
evaluating	O
will	O
also	O
give	O
the	O
value	O
xmax	O
for	O
the	O
most	O
probable	O
value	O
of	O
the	O
root	B
node	B
variable	O
defined	O
by	O
xmax	O
arg	O
max	O
x	O
fs	O
xx	O
at	O
this	O
point	O
we	O
might	O
be	O
tempted	O
simply	O
to	O
continue	O
with	O
the	O
message	B
passing	I
algorithm	O
and	O
send	O
messages	O
from	O
the	O
root	O
back	O
out	O
to	O
the	O
leaves	O
using	O
and	O
then	O
apply	O
to	O
all	O
of	O
the	O
remaining	O
variable	O
nodes	O
however	O
because	O
we	O
are	O
now	O
maximizing	O
rather	O
than	O
summing	O
it	O
is	O
possible	O
that	O
there	O
may	O
be	O
multiple	O
configurations	O
of	O
x	O
all	O
of	O
which	O
give	O
rise	O
to	O
the	O
maximum	O
value	O
for	O
px	O
in	O
such	O
cases	O
this	O
strategy	O
can	O
fail	O
because	O
it	O
is	O
possible	O
for	O
the	O
individual	O
variable	O
values	O
obtained	O
by	O
maximizing	O
the	O
product	O
of	O
messages	O
at	O
each	O
node	B
to	O
belong	O
to	O
different	O
maximizing	O
configurations	O
giving	O
an	O
overall	O
configuration	O
that	O
no	O
longer	O
corresponds	O
to	O
a	O
maximum	O
the	O
problem	O
can	O
be	O
resolved	O
by	O
adopting	O
a	O
rather	O
different	O
kind	O
of	O
message	B
passing	I
from	O
the	O
root	B
node	B
to	O
the	O
leaves	O
to	O
see	O
how	O
this	O
works	O
let	O
us	O
return	O
once	O
again	O
to	O
the	O
simple	O
chain	O
example	O
of	O
n	O
variables	O
xn	O
each	O
having	O
k	O
states	O
graphical	O
models	O
figure	O
a	O
lattice	O
or	O
trellis	O
diagram	O
showing	O
explicitly	O
the	O
k	O
possible	O
states	O
per	O
row	O
of	O
the	O
diagram	O
for	O
each	O
of	O
the	O
variables	O
xn	O
in	O
the	O
in	O
this	O
illustration	O
k	O
the	O
archain	O
model	O
row	O
shows	O
the	O
direction	O
of	O
message	B
passing	I
in	O
the	O
max-product	O
algorithm	O
for	O
every	O
state	O
k	O
of	O
each	O
variable	O
xn	O
to	O
column	O
n	O
of	O
the	O
diagram	O
the	O
function	O
defines	O
a	O
unique	O
state	O
at	O
the	O
previous	O
variable	O
indicated	O
by	O
the	O
black	O
lines	O
the	O
two	O
paths	O
through	O
the	O
lattice	O
correspond	O
to	O
configurations	O
that	O
give	O
the	O
global	O
maximum	O
of	O
the	O
joint	O
probability	B
distribution	O
and	O
either	O
of	O
these	O
can	O
be	O
found	O
by	O
tracing	O
back	O
along	O
the	O
black	O
lines	O
in	O
the	O
opposite	O
direction	O
to	O
the	O
arrow	O
k	O
k	O
k	O
n	O
n	O
n	O
n	O
corresponding	O
to	O
the	O
graph	O
shown	O
in	O
figure	O
suppose	O
we	O
take	O
node	B
xn	O
to	O
be	O
the	O
root	B
node	B
then	O
in	O
the	O
first	O
phase	O
we	O
propagate	O
messages	O
from	O
the	O
leaf	O
node	B
to	O
the	O
root	B
node	B
using	O
xn	O
fn	O
xnxn	O
fn	O
xnxn	O
max	O
xn	O
ln	O
fn	O
xn	O
xn	O
f	O
n	O
which	O
follow	O
from	O
applying	O
and	O
to	O
this	O
particular	O
graph	O
the	O
initial	O
message	O
sent	O
from	O
the	O
leaf	O
node	B
is	O
simply	O
the	O
most	O
probable	O
value	O
for	O
xn	O
is	O
then	O
given	O
by	O
xmax	O
n	O
arg	O
max	O
xn	O
fn	O
xn	O
now	O
we	O
need	O
to	O
determine	O
the	O
states	O
of	O
the	O
previous	O
variables	O
that	O
correspond	O
to	O
the	O
same	O
maximizing	O
configuration	O
this	O
can	O
be	O
done	O
by	O
keeping	O
track	O
of	O
which	O
values	O
of	O
the	O
variables	O
gave	O
rise	O
to	O
the	O
maximum	O
state	O
of	O
each	O
variable	O
in	O
other	O
words	O
by	O
storing	O
quantities	O
given	O
by	O
arg	O
max	O
xn	O
ln	O
fn	O
xn	O
xn	O
f	O
n	O
to	O
understand	O
better	O
what	O
is	O
happening	O
it	O
is	O
helpful	O
to	O
represent	O
the	O
chain	O
of	O
variables	O
in	O
terms	O
of	O
a	O
lattice	O
or	O
trellis	O
diagram	O
as	O
shown	O
in	O
figure	O
note	O
that	O
this	O
is	O
not	O
a	O
probabilistic	O
graphical	B
model	I
because	O
the	O
nodes	O
represent	O
individual	O
states	O
of	O
variables	O
while	O
each	O
variable	O
corresponds	O
to	O
a	O
column	O
of	O
such	O
states	O
in	O
the	O
diagram	O
for	O
each	O
state	O
of	O
a	O
given	O
variable	O
there	O
is	O
a	O
unique	O
state	O
of	O
the	O
previous	O
variable	O
that	O
maximizes	O
the	O
probability	B
are	O
broken	O
either	O
systematically	O
or	O
at	O
random	O
corresponding	O
to	O
the	O
function	O
given	O
by	O
and	O
this	O
is	O
indicated	O
inference	B
in	O
graphical	O
models	O
by	O
the	O
lines	O
connecting	O
the	O
nodes	O
once	O
we	O
know	O
the	O
most	O
probable	O
value	O
of	O
the	O
final	O
node	B
xn	O
we	O
can	O
then	O
simply	O
follow	O
the	O
link	B
back	O
to	O
find	O
the	O
most	O
probable	O
state	O
of	O
node	B
xn	O
and	O
so	O
on	O
back	O
to	O
the	O
initial	O
node	B
this	O
corresponds	O
to	O
propagating	O
a	O
message	O
back	O
down	O
the	O
chain	O
using	O
n	O
xmax	O
n	O
and	O
is	O
known	O
as	O
back-tracking	B
note	O
that	O
there	O
could	O
be	O
several	O
values	O
of	O
xn	O
all	O
of	O
which	O
give	O
the	O
maximum	O
value	O
in	O
provided	O
we	O
chose	O
one	O
of	O
these	O
values	O
when	O
we	O
do	O
the	O
back-tracking	B
we	O
are	O
assured	O
of	O
a	O
globally	O
consistent	B
maximizing	O
configuration	O
in	O
figure	O
we	O
have	O
indicated	O
two	O
paths	O
each	O
of	O
which	O
we	O
shall	O
suppose	O
corresponds	O
to	O
a	O
global	O
maximum	O
of	O
the	O
joint	O
probability	B
distribution	O
if	O
k	O
and	O
k	O
each	O
represent	O
possible	O
values	O
of	O
xmax	O
n	O
then	O
starting	O
from	O
either	O
state	O
and	O
tracing	O
back	O
along	O
the	O
black	O
lines	O
which	O
corresponds	O
to	O
iterating	O
we	O
obtain	O
a	O
valid	O
global	O
maximum	O
configuration	O
note	O
that	O
if	O
we	O
had	O
run	O
a	O
forward	O
pass	O
of	O
max-sum	O
message	B
passing	I
followed	O
by	O
a	O
backward	O
pass	O
and	O
then	O
applied	O
at	O
each	O
node	B
separately	O
we	O
could	O
end	O
up	O
selecting	O
some	O
states	O
from	O
one	O
path	O
and	O
some	O
from	O
the	O
other	O
path	O
giving	O
an	O
overall	O
configuration	O
that	O
is	O
not	O
a	O
global	O
maximizer	O
we	O
see	O
that	O
it	O
is	O
necessary	O
instead	O
to	O
keep	O
track	O
of	O
the	O
maximizing	O
states	O
during	O
the	O
forward	O
pass	O
using	O
the	O
functions	O
and	O
then	O
use	O
back-tracking	B
to	O
find	O
a	O
consistent	B
solution	O
the	O
extension	O
to	O
a	O
general	O
tree-structured	O
factor	B
graph	I
should	O
now	O
be	O
clear	O
if	O
a	O
message	O
is	O
sent	O
from	O
a	O
factor	O
node	B
f	O
to	O
a	O
variable	O
node	B
x	O
a	O
maximization	O
is	O
performed	O
over	O
all	O
other	O
variable	O
nodes	O
xm	O
that	O
are	O
neighbours	O
of	O
that	O
factor	O
node	B
using	O
when	O
we	O
perform	O
this	O
maximization	O
we	O
keep	O
a	O
record	O
of	O
which	O
values	O
of	O
the	O
variables	O
xm	O
gave	O
rise	O
to	O
the	O
maximum	O
then	O
in	O
the	O
back-tracking	B
step	O
having	O
found	O
xmax	O
we	O
can	O
then	O
use	O
these	O
stored	O
values	O
to	O
assign	O
consistent	B
maximizing	O
states	O
xmax	O
m	O
the	O
max-sum	B
algorithm	I
with	O
back-tracking	B
gives	O
an	O
exact	O
maximizing	O
configuration	O
for	O
the	O
variables	O
provided	O
the	O
factor	B
graph	I
is	O
a	O
tree	B
an	O
important	O
application	O
of	O
this	O
technique	O
is	O
for	O
finding	O
the	O
most	O
probable	O
sequence	O
of	O
hidden	O
states	O
in	O
a	O
hidden	B
markov	B
model	I
in	O
which	O
case	O
it	O
is	O
known	O
as	O
the	O
viterbi	B
algorithm	I
xmax	O
as	O
with	O
the	O
sum-product	B
algorithm	I
the	O
inclusion	O
of	O
evidence	O
in	O
the	O
form	O
of	O
observed	O
variables	O
is	O
straightforward	O
the	O
observed	O
variables	O
are	O
clamped	O
to	O
their	O
observed	O
values	O
and	O
the	O
maximization	O
is	O
performed	O
over	O
the	O
remaining	O
hidden	O
variables	O
this	O
can	O
be	O
shown	O
formally	O
by	O
including	O
identity	O
functions	O
for	O
the	O
observed	O
variables	O
into	O
the	O
factor	O
functions	O
as	O
we	O
did	O
for	O
the	O
sum-product	B
algorithm	I
it	O
is	O
interesting	O
to	O
compare	O
max-sum	O
with	O
the	O
iterated	B
conditional	B
modes	I
algorithm	O
described	O
on	O
page	O
each	O
step	O
in	O
icm	O
is	O
computationally	O
simpler	O
because	O
the	O
messages	O
that	O
are	O
passed	O
from	O
one	O
node	B
to	O
the	O
next	O
comprise	O
a	O
single	O
value	O
consisting	O
of	O
the	O
new	O
state	O
of	O
the	O
node	B
for	O
which	O
the	O
conditional	B
distribution	O
is	O
maximized	O
the	O
max-sum	B
algorithm	I
is	O
more	O
complex	O
because	O
the	O
messages	O
are	O
functions	O
of	O
node	B
variables	O
x	O
and	O
hence	O
comprise	O
a	O
set	O
of	O
k	O
values	O
for	O
each	O
possible	O
state	O
of	O
x	O
unlike	O
max-sum	O
however	O
icm	O
is	O
not	O
guaranteed	O
to	O
find	O
a	O
global	O
maximum	O
even	O
for	O
tree-structured	O
graphs	O
section	O
graphical	O
models	O
exact	O
inference	B
in	O
general	O
graphs	O
the	O
sum-product	O
and	O
max-sum	O
algorithms	O
provide	O
efficient	O
and	O
exact	O
solutions	O
to	O
inference	B
problems	O
in	O
tree-structured	O
graphs	O
for	O
many	O
practical	O
applications	O
however	O
we	O
have	O
to	O
deal	O
with	O
graphs	O
having	O
loops	O
the	O
message	B
passing	I
framework	O
can	O
be	O
generalized	B
to	O
arbitrary	O
graph	O
topologies	O
giving	O
an	O
exact	O
inference	B
procedure	O
known	O
as	O
the	O
junction	B
tree	B
algorithm	I
and	O
spiegelhalter	O
jordan	O
here	O
we	O
give	O
a	O
brief	O
outline	O
of	O
the	O
key	O
steps	O
involved	O
this	O
is	O
not	O
intended	O
to	O
convey	O
a	O
detailed	O
understanding	O
of	O
the	O
algorithm	O
but	O
rather	O
to	O
give	O
a	O
flavour	O
of	O
the	O
various	O
stages	O
involved	O
if	O
the	O
starting	O
point	O
is	O
a	O
directed	B
graph	O
it	O
is	O
first	O
converted	O
to	O
an	O
undirected	B
graph	O
by	O
moralization	B
whereas	O
if	O
starting	O
from	O
an	O
undirected	B
graph	O
this	O
step	O
is	O
not	O
required	O
next	O
the	O
graph	O
is	O
triangulated	B
which	O
involves	O
finding	O
chord-less	O
cycles	O
containing	O
four	O
or	O
more	O
nodes	O
and	O
adding	O
extra	O
links	O
to	O
eliminate	O
such	O
chord-less	O
cycles	O
for	O
instance	O
in	O
the	O
graph	O
in	O
figure	O
the	O
cycle	O
a	O
c	O
b	O
d	O
a	O
is	O
chord-less	O
a	O
link	B
could	O
be	O
added	O
between	O
a	O
and	O
b	O
or	O
alternatively	O
between	O
c	O
and	O
d	O
note	O
that	O
the	O
joint	O
distribution	O
for	O
the	O
resulting	O
triangulated	B
graph	I
is	O
still	O
defined	O
by	O
a	O
product	O
of	O
the	O
same	O
potential	O
functions	O
but	O
these	O
are	O
now	O
considered	O
to	O
be	O
functions	O
over	O
expanded	O
sets	O
of	O
variables	O
next	O
the	O
triangulated	B
graph	I
is	O
used	O
to	O
construct	O
a	O
new	O
tree-structured	O
undirected	B
graph	O
called	O
a	O
join	B
tree	B
whose	O
nodes	O
correspond	O
to	O
the	O
maximal	O
cliques	O
of	O
the	O
triangulated	B
graph	I
and	O
whose	O
links	O
connect	O
pairs	O
of	O
cliques	O
that	O
have	O
variables	O
in	O
common	O
the	O
selection	O
of	O
which	O
pairs	O
of	O
cliques	O
to	O
connect	O
in	O
this	O
way	O
is	O
important	O
and	O
is	O
done	O
so	O
as	O
to	O
give	O
a	O
maximal	B
spanning	I
tree	B
defined	O
as	O
follows	O
of	O
all	O
possible	O
trees	O
that	O
link	B
up	O
the	O
cliques	O
the	O
one	O
that	O
is	O
chosen	O
is	O
one	O
for	O
which	O
the	O
weight	O
of	O
the	O
tree	B
is	O
largest	O
where	O
the	O
weight	O
for	O
a	O
link	B
is	O
the	O
number	O
of	O
nodes	O
shared	O
by	O
the	O
two	O
cliques	O
it	O
connects	O
and	O
the	O
weight	O
for	O
the	O
tree	B
is	O
the	O
sum	O
of	O
the	O
weights	O
for	O
the	O
links	O
if	O
the	O
tree	B
is	O
condensed	O
so	O
that	O
any	O
clique	B
that	O
is	O
a	O
subset	O
of	O
another	O
clique	B
is	O
absorbed	O
into	O
the	O
larger	O
clique	B
this	O
gives	O
a	O
junction	O
tree	B
as	O
a	O
consequence	O
of	O
the	O
triangulation	O
step	O
the	O
resulting	O
tree	B
satisfies	O
the	O
running	B
intersection	I
property	I
which	O
means	O
that	O
if	O
a	O
variable	O
is	O
contained	O
in	O
two	O
cliques	O
then	O
it	O
must	O
also	O
be	O
contained	O
in	O
every	O
clique	B
on	O
the	O
path	O
that	O
connects	O
them	O
this	O
ensures	O
that	O
inference	B
about	O
variables	O
will	O
be	O
consistent	B
across	O
the	O
graph	O
finally	O
a	O
two-stage	O
message	B
passing	I
algorithm	O
essentially	O
equivalent	O
to	O
the	O
sum-product	B
algorithm	I
can	O
now	O
be	O
applied	O
to	O
this	O
junction	O
tree	B
in	O
order	O
to	O
find	O
marginals	O
and	O
conditionals	O
although	O
the	O
junction	B
tree	B
algorithm	I
sounds	O
complicated	O
at	O
its	O
heart	O
is	O
the	O
simple	O
idea	O
that	O
we	O
have	O
used	O
already	O
of	O
exploiting	O
the	O
factorization	B
properties	O
of	O
the	O
distribution	O
to	O
allow	O
sums	O
and	O
products	O
to	O
be	O
interchanged	O
so	O
that	O
partial	O
summations	O
can	O
be	O
performed	O
thereby	O
avoiding	O
having	O
to	O
work	O
directly	O
with	O
the	O
joint	O
distribution	O
the	O
role	O
of	O
the	O
junction	O
tree	B
is	O
to	O
provide	O
a	O
precise	O
and	O
efficient	O
way	O
to	O
organize	O
these	O
computations	O
it	O
is	O
worth	O
emphasizing	O
that	O
this	O
is	O
achieved	O
using	O
purely	O
graphical	O
operations	O
the	O
junction	O
tree	B
is	O
exact	O
for	O
arbitrary	O
graphs	O
and	O
is	O
efficient	O
in	O
the	O
sense	O
that	O
for	O
a	O
given	O
graph	O
there	O
does	O
not	O
in	O
general	O
exist	O
a	O
computationally	O
cheaper	O
approach	O
unfortunately	O
the	O
algorithm	O
must	O
work	O
with	O
the	O
joint	O
distributions	O
within	O
each	O
node	B
of	O
which	O
corresponds	O
to	O
a	O
clique	B
of	O
the	O
triangulated	B
graph	I
and	O
so	O
the	O
computational	O
cost	O
of	O
the	O
algorithm	O
is	O
determined	O
by	O
the	O
number	O
of	O
variables	O
in	O
the	O
largest	O
inference	B
in	O
graphical	O
models	O
clique	B
and	O
will	O
grow	O
exponentially	O
with	O
this	O
number	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
an	O
important	O
concept	O
is	O
the	O
treewidth	B
of	O
a	O
graph	O
which	O
is	O
defined	O
in	O
terms	O
of	O
the	O
number	O
of	O
variables	O
in	O
the	O
largest	O
clique	B
in	O
fact	O
it	O
is	O
defined	O
to	O
be	O
as	O
one	O
less	O
than	O
the	O
size	O
of	O
the	O
largest	O
clique	B
to	O
ensure	O
that	O
a	O
tree	B
has	O
a	O
treewidth	B
of	O
because	O
there	O
in	O
general	O
there	O
can	O
be	O
multiple	O
different	O
junction	O
trees	O
that	O
can	O
be	O
constructed	O
from	O
a	O
given	O
starting	O
graph	O
the	O
treewidth	B
is	O
defined	O
by	O
the	O
junction	O
tree	B
for	O
which	O
the	O
largest	O
clique	B
has	O
the	O
fewest	O
variables	O
if	O
the	O
treewidth	B
of	O
the	O
original	O
graph	O
is	O
high	O
the	O
junction	B
tree	B
algorithm	I
becomes	O
impractical	O
loopy	B
belief	B
propagation	I
for	O
many	O
problems	O
of	O
practical	O
interest	O
it	O
will	O
not	O
be	O
feasible	O
to	O
use	O
exact	O
inference	B
and	O
so	O
we	O
need	O
to	O
exploit	O
effective	O
approximation	O
methods	O
an	O
important	O
class	O
of	O
such	O
approximations	O
that	O
can	O
broadly	O
be	O
called	O
variational	B
methods	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
chapter	O
complementing	O
these	O
deterministic	O
approaches	O
is	O
a	O
wide	O
range	O
of	O
sampling	B
methods	I
also	O
called	O
monte	O
carlo	O
methods	O
that	O
are	O
based	O
on	O
stochastic	B
numerical	O
sampling	O
from	O
distributions	O
and	O
that	O
will	O
be	O
discussed	O
at	O
length	O
in	O
chapter	O
here	O
we	O
consider	O
one	O
simple	O
approach	O
to	O
approximate	O
inference	B
in	O
graphs	O
with	O
loops	O
which	O
builds	O
directly	O
on	O
the	O
previous	O
discussion	O
of	O
exact	O
inference	B
in	O
trees	O
the	O
idea	O
is	O
simply	O
to	O
apply	O
the	O
sum-product	B
algorithm	I
even	O
though	O
there	O
is	O
no	O
guarantee	O
that	O
it	O
will	O
yield	O
good	O
results	O
this	O
approach	O
is	O
known	O
as	O
loopy	B
belief	B
propagation	I
and	O
mackay	O
and	O
is	O
possible	O
because	O
the	O
message	B
passing	I
rules	O
and	O
for	O
the	O
sum-product	B
algorithm	I
are	O
purely	O
local	B
however	O
because	O
the	O
graph	O
now	O
has	O
cycles	O
information	O
can	O
flow	O
many	O
times	O
around	O
the	O
graph	O
for	O
some	O
models	O
the	O
algorithm	O
will	O
converge	O
whereas	O
for	O
others	O
it	O
will	O
not	O
in	O
order	O
to	O
apply	O
this	O
approach	O
we	O
need	O
to	O
define	O
a	O
message	B
passing	I
schedule	B
let	O
us	O
assume	O
that	O
one	O
message	O
is	O
passed	O
at	O
a	O
time	O
on	O
any	O
given	O
link	B
and	O
in	O
any	O
given	O
direction	O
each	O
message	O
sent	O
from	O
a	O
node	B
replaces	O
any	O
previous	O
message	O
sent	O
in	O
the	O
same	O
direction	O
across	O
the	O
same	O
link	B
and	O
will	O
itself	O
be	O
a	O
function	O
only	O
of	O
the	O
most	O
recent	O
messages	O
received	O
by	O
that	O
node	B
at	O
previous	O
steps	O
of	O
the	O
algorithm	O
we	O
have	O
seen	O
that	O
a	O
message	O
can	O
only	O
be	O
sent	O
across	O
a	O
link	B
from	O
a	O
node	B
when	O
all	O
other	O
messages	O
have	O
been	O
received	O
by	O
that	O
node	B
across	O
its	O
other	O
links	O
because	O
there	O
are	O
loops	O
in	O
the	O
graph	O
this	O
raises	O
the	O
problem	O
of	O
how	O
to	O
initiate	O
the	O
message	B
passing	I
algorithm	O
to	O
resolve	O
this	O
we	O
suppose	O
that	O
an	O
initial	O
message	O
given	O
by	O
the	O
unit	O
function	O
has	O
been	O
passed	O
across	O
every	O
link	B
in	O
each	O
direction	O
every	O
node	B
is	O
then	O
in	O
a	O
position	O
to	O
send	O
a	O
message	O
there	O
are	O
now	O
many	O
possible	O
ways	O
to	O
organize	O
the	O
message	B
passing	I
schedule	B
for	O
example	O
the	O
flooding	O
schedule	B
simultaneously	O
passes	O
a	O
message	O
across	O
every	O
link	B
in	O
both	O
directions	O
at	O
each	O
time	O
step	O
whereas	O
schedules	O
that	O
pass	O
one	O
message	O
at	O
a	O
time	O
are	O
called	O
serial	O
schedules	O
following	O
kschischnang	O
et	O
al	O
we	O
will	O
say	O
that	O
a	O
or	O
factor	O
node	B
a	O
has	O
a	O
message	O
pending	O
on	O
its	O
link	B
to	O
a	O
node	B
b	O
if	O
node	B
a	O
has	O
received	O
any	O
message	O
on	O
any	O
of	O
its	O
other	O
links	O
since	O
the	O
last	O
time	O
it	O
send	O
a	O
message	O
to	O
b	O
thus	O
when	O
a	O
node	B
receives	O
a	O
message	O
on	O
one	O
of	O
its	O
links	O
this	O
creates	O
pending	O
messages	O
on	O
all	O
of	O
its	O
other	O
links	O
only	O
pending	O
messages	O
need	O
to	O
be	O
transmitted	O
because	O
graphical	O
models	O
exercise	O
other	O
messages	O
would	O
simply	O
duplicate	O
the	O
previous	O
message	O
on	O
the	O
same	O
link	B
for	O
graphs	O
that	O
have	O
a	O
tree	B
structure	O
any	O
schedule	B
that	O
sends	O
only	O
pending	O
messages	O
will	O
eventually	O
terminate	O
once	O
a	O
message	O
has	O
passed	O
in	O
each	O
direction	O
across	O
every	O
link	B
at	O
this	O
point	O
there	O
are	O
no	O
pending	O
messages	O
and	O
the	O
product	O
of	O
the	O
received	O
messages	O
at	O
every	O
variable	O
give	O
the	O
exact	O
marginal	B
in	O
graphs	O
having	O
loops	O
however	O
the	O
algorithm	O
may	O
never	O
terminate	O
because	O
there	O
might	O
always	O
be	O
pending	O
messages	O
although	O
in	O
practice	O
it	O
is	O
generally	O
found	O
to	O
converge	O
within	O
a	O
reasonable	O
time	O
for	O
most	O
applications	O
once	O
the	O
algorithm	O
has	O
converged	O
or	O
once	O
it	O
has	O
been	O
stopped	O
if	O
convergence	O
is	O
not	O
observed	O
the	O
local	B
marginals	O
can	O
be	O
computed	O
using	O
the	O
product	O
of	O
the	O
most	O
recently	O
received	O
incoming	O
messages	O
to	O
each	O
variable	O
node	B
or	O
factor	O
node	B
on	O
every	O
link	B
in	O
some	O
applications	O
the	O
loopy	B
belief	B
propagation	I
algorithm	O
can	O
give	O
poor	O
results	O
whereas	O
in	O
other	O
applications	O
it	O
has	O
proven	O
to	O
be	O
very	O
effective	O
in	O
particular	O
state-of-the-art	O
algorithms	O
for	O
decoding	O
certain	O
kinds	O
of	O
error-correcting	O
codes	O
are	O
equivalent	O
to	O
loopy	B
belief	B
propagation	I
berrou	O
et	O
al	O
mceliece	O
et	O
al	O
mackay	O
and	O
neal	O
frey	O
learning	B
the	O
graph	O
structure	O
in	O
our	O
discussion	O
of	O
inference	B
in	O
graphical	O
models	O
we	O
have	O
assumed	O
that	O
the	O
structure	O
of	O
the	O
graph	O
is	O
known	O
and	O
fixed	O
however	O
there	O
is	O
also	O
interest	O
in	O
going	O
beyond	O
the	O
inference	B
problem	O
and	O
learning	B
the	O
graph	O
structure	O
itself	O
from	O
data	O
and	O
koller	O
this	O
requires	O
that	O
we	O
define	O
a	O
space	O
of	O
possible	O
structures	O
as	O
well	O
as	O
a	O
measure	O
that	O
can	O
be	O
used	O
to	O
score	O
each	O
structure	O
from	O
a	O
bayesian	B
viewpoint	O
we	O
would	O
ideally	O
like	O
to	O
compute	O
a	O
posterior	O
distribution	O
over	O
graph	O
structures	O
and	O
to	O
make	O
predictions	O
by	O
averaging	O
with	O
respect	O
to	O
this	O
distribution	O
if	O
we	O
have	O
a	O
prior	B
pm	O
over	O
graphs	O
indexed	O
by	O
m	O
then	O
the	O
posterior	O
distribution	O
is	O
given	O
by	O
pmd	O
pmpdm	O
where	O
d	O
is	O
the	O
observed	O
data	O
set	O
the	O
model	B
evidence	I
pdm	O
then	O
provides	O
the	O
score	O
for	O
each	O
model	O
however	O
evaluation	O
of	O
the	O
evidence	O
involves	O
marginalization	O
over	O
the	O
latent	O
variables	O
and	O
presents	O
a	O
challenging	O
computational	O
problem	O
for	O
many	O
models	O
exploring	O
the	O
space	O
of	O
structures	O
can	O
also	O
be	O
problematic	O
because	O
the	O
number	O
of	O
different	O
graph	O
structures	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
nodes	O
it	O
is	O
often	O
necessary	O
to	O
resort	O
to	O
heuristics	O
to	O
find	O
good	O
candidates	O
exercises	O
www	O
by	O
marginalizing	O
out	O
the	O
variables	O
in	O
order	O
show	O
that	O
the	O
representation	O
for	O
the	O
joint	O
distribution	O
of	O
a	O
directed	B
graph	O
is	O
correctly	O
normalized	O
provided	O
each	O
of	O
the	O
conditional	B
distributions	O
is	O
normalized	O
www	O
show	O
that	O
the	O
property	O
of	O
there	O
being	O
no	O
directed	B
cycles	O
in	O
a	O
directed	B
graph	O
follows	O
from	O
the	O
statement	O
that	O
there	O
exists	O
an	O
ordered	O
numbering	O
of	O
the	O
nodes	O
such	O
that	O
for	O
each	O
node	B
there	O
are	O
no	O
links	O
going	O
to	O
a	O
lower-numbered	O
node	B
table	O
the	O
joint	O
distribution	O
over	O
three	O
binary	O
variables	O
exercises	O
a	O
b	O
c	O
pa	O
b	O
c	O
consider	O
three	O
binary	O
variables	O
a	O
b	O
c	O
having	O
the	O
joint	O
distribution	O
given	O
in	O
table	O
show	O
by	O
direct	O
evaluation	O
that	O
this	O
distribution	O
has	O
the	O
property	O
that	O
a	O
and	O
b	O
are	O
marginally	O
dependent	O
so	O
that	O
pa	O
b	O
papb	O
but	O
that	O
they	O
become	O
independent	B
when	O
conditioned	O
on	O
c	O
so	O
that	O
pa	O
bc	O
pacpbc	O
for	O
both	O
c	O
and	O
c	O
evaluate	O
the	O
distributions	O
pa	O
pbc	O
and	O
pca	O
corresponding	O
to	O
the	O
joint	O
distribution	O
given	O
in	O
table	O
hence	O
show	O
by	O
direct	O
evaluation	O
that	O
pa	O
b	O
c	O
papcapbc	O
draw	O
the	O
corresponding	O
directed	B
graph	O
www	O
draw	O
a	O
directed	B
probabilistic	O
graphical	B
model	I
corresponding	O
to	O
the	O
relevance	B
vector	I
machine	I
described	O
by	O
and	O
for	O
the	O
model	O
shown	O
in	O
figure	O
we	O
have	O
seen	O
that	O
the	O
number	O
of	O
parameters	O
required	O
to	O
specify	O
the	O
conditional	B
distribution	O
xm	O
where	O
xi	O
could	O
be	O
reduced	O
from	O
to	O
m	O
by	O
making	O
use	O
of	O
the	O
logistic	B
sigmoid	I
representation	O
an	O
alternative	O
representation	O
is	O
given	O
by	O
py	O
xm	O
ixi	O
where	O
the	O
parameters	O
i	O
represent	O
the	O
probabilities	O
pxi	O
and	O
is	O
an	O
additional	O
parameters	O
satisfying	O
the	O
conditional	B
distribution	O
is	O
known	O
as	O
the	O
noisy-or	O
show	O
that	O
this	O
can	O
be	O
interpreted	O
as	O
a	O
soft	B
form	O
of	O
the	O
logical	O
or	O
function	O
the	O
function	O
that	O
gives	O
y	O
whenever	O
at	O
least	O
one	O
of	O
the	O
xi	O
discuss	O
the	O
interpretation	O
of	O
using	O
the	O
recursion	O
relations	O
and	O
show	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
joint	O
distribution	O
for	O
the	O
graph	O
shown	O
in	O
figure	O
are	O
given	O
by	O
and	O
respectively	O
www	O
show	O
that	O
a	O
b	O
c	O
d	O
implies	O
a	O
b	O
d	O
www	O
using	O
the	O
d-separation	B
criterion	O
show	O
that	O
the	O
conditional	B
distribution	O
for	O
a	O
node	B
x	O
in	O
a	O
directed	B
graph	O
conditioned	O
on	O
all	O
of	O
the	O
nodes	O
in	O
the	O
markov	B
blanket	I
is	O
independent	B
of	O
the	O
remaining	O
variables	O
in	O
the	O
graph	O
graphical	O
models	O
figure	O
example	O
of	O
a	O
graphical	B
model	I
used	O
to	O
explore	O
the	O
conditional	B
independence	I
properties	O
of	O
the	O
head-to-head	B
path	I
a	O
c	O
b	O
when	O
a	O
descendant	O
of	O
c	O
namely	O
the	O
node	B
d	O
is	O
observed	O
a	O
b	O
c	O
d	O
consider	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
in	O
which	O
none	O
of	O
the	O
variables	O
is	O
observed	O
show	O
that	O
a	O
b	O
suppose	O
we	O
now	O
observe	O
the	O
variable	O
d	O
show	O
that	O
in	O
general	O
a	O
b	O
d	O
consider	O
the	O
example	O
of	O
the	O
car	O
fuel	B
system	I
shown	O
in	O
figure	O
and	O
suppose	O
that	O
instead	O
of	O
observing	O
the	O
state	O
of	O
the	O
fuel	O
gauge	O
g	O
directly	O
the	O
gauge	O
is	O
seen	O
by	O
the	O
driver	O
d	O
who	O
reports	O
to	O
us	O
the	O
reading	O
on	O
the	O
gauge	O
this	O
report	O
is	O
either	O
that	O
the	O
gauge	O
shows	O
full	O
d	O
or	O
that	O
it	O
shows	O
empty	O
d	O
our	O
driver	O
is	O
a	O
bit	O
unreliable	O
as	O
expressed	O
through	O
the	O
following	O
probabilities	O
pd	O
pd	O
suppose	O
that	O
the	O
driver	O
tells	O
us	O
that	O
the	O
fuel	O
gauge	O
shows	O
empty	O
in	O
other	O
words	O
that	O
we	O
observe	O
d	O
evaluate	O
the	O
probability	B
that	O
the	O
tank	O
is	O
empty	O
given	O
only	O
this	O
observation	O
similarly	O
evaluate	O
the	O
corresponding	O
probability	B
given	O
also	O
the	O
observation	O
that	O
the	O
battery	O
is	O
flat	O
and	O
note	O
that	O
this	O
second	O
probability	B
is	O
lower	O
discuss	O
the	O
intuition	O
behind	O
this	O
result	O
and	O
relate	O
the	O
result	O
to	O
figure	O
www	O
show	O
that	O
there	O
are	O
distinct	O
undirected	B
graphs	O
over	O
a	O
set	O
of	O
m	O
distinct	O
random	O
variables	O
draw	O
the	O
possibilities	O
for	O
the	O
case	O
of	O
m	O
consider	O
the	O
use	O
of	O
iterated	B
conditional	B
modes	I
to	O
minimize	O
the	O
energy	B
function	I
given	O
by	O
write	O
down	O
an	O
expression	O
for	O
the	O
difference	O
in	O
the	O
values	O
of	O
the	O
energy	O
associated	O
with	O
the	O
two	O
states	O
of	O
a	O
particular	O
variable	O
xj	O
with	O
all	O
other	O
variables	O
held	O
fixed	O
and	O
show	O
that	O
it	O
depends	O
only	O
on	O
quantities	O
that	O
are	O
local	B
to	O
xj	O
in	O
the	O
graph	O
consider	O
a	O
particular	O
case	O
of	O
the	O
energy	B
function	I
given	O
by	O
in	O
which	O
the	O
coefficients	O
h	O
show	O
that	O
the	O
most	O
probable	O
configuration	O
of	O
the	O
latent	O
variables	O
is	O
given	O
by	O
xi	O
yi	O
for	O
all	O
i	O
www	O
show	O
that	O
the	O
joint	O
distribution	O
pxn	O
xn	O
for	O
two	O
neighbouring	O
nodes	O
in	O
the	O
graph	O
shown	O
in	O
figure	O
is	O
given	O
by	O
an	O
expression	O
of	O
the	O
form	O
exercises	O
consider	O
the	O
inference	B
problem	O
of	O
evaluating	O
pxnxn	O
for	O
the	O
graph	O
shown	O
in	O
figure	O
for	O
all	O
nodes	O
n	O
n	O
show	O
that	O
the	O
message	B
passing	I
algorithm	O
discussed	O
in	O
section	O
can	O
be	O
used	O
to	O
solve	O
this	O
efficiently	O
and	O
discuss	O
which	O
messages	O
are	O
modified	O
and	O
in	O
what	O
way	O
consider	O
a	O
graph	O
of	O
the	O
form	O
shown	O
in	O
figure	O
having	O
n	O
nodes	O
in	O
which	O
nodes	O
and	O
are	O
observed	O
use	O
d-separation	B
to	O
show	O
that	O
show	O
that	O
if	O
the	O
message	B
passing	I
algorithm	O
of	O
section	O
is	O
applied	O
to	O
the	O
evaluation	O
of	O
the	O
result	O
will	O
be	O
independent	B
of	O
the	O
value	O
of	O
www	O
show	O
that	O
a	O
distribution	O
represented	O
by	O
a	O
directed	B
tree	B
can	O
trivially	O
be	O
written	O
as	O
an	O
equivalent	O
distribution	O
over	O
the	O
corresponding	O
undirected	B
tree	B
also	O
show	O
that	O
a	O
distribution	O
expressed	O
as	O
an	O
undirected	B
tree	B
can	O
by	O
suitable	O
normalization	O
of	O
the	O
clique	B
potentials	O
be	O
written	O
as	O
a	O
directed	B
tree	B
calculate	O
the	O
number	O
of	O
distinct	O
directed	B
trees	O
that	O
can	O
be	O
constructed	O
from	O
a	O
given	O
undirected	B
tree	B
apply	O
the	O
sum-product	B
algorithm	I
derived	O
in	O
section	O
to	O
the	O
chain-ofnodes	O
model	O
discussed	O
in	O
section	O
and	O
show	O
that	O
the	O
results	O
and	O
are	O
recovered	O
as	O
a	O
special	O
case	O
www	O
consider	O
the	O
message	B
passing	I
protocol	O
for	O
the	O
sum-product	B
algorithm	I
on	O
a	O
tree-structured	O
factor	B
graph	I
in	O
which	O
messages	O
are	O
first	O
propagated	O
from	O
the	O
leaves	O
to	O
an	O
arbitrarily	O
chosen	O
root	B
node	B
and	O
then	O
from	O
the	O
root	B
node	B
out	O
to	O
the	O
leaves	O
use	O
proof	O
by	O
induction	O
to	O
show	O
that	O
the	O
messages	O
can	O
be	O
passed	O
in	O
such	O
an	O
order	O
that	O
at	O
every	O
step	O
each	O
node	B
that	O
must	O
send	O
a	O
message	O
has	O
received	O
all	O
of	O
the	O
incoming	O
messages	O
necessary	O
to	O
construct	O
its	O
outgoing	O
messages	O
www	O
show	O
that	O
the	O
marginal	B
distributions	O
pxs	O
over	O
the	O
sets	O
of	O
variables	O
xs	O
associated	O
with	O
each	O
of	O
the	O
factors	O
fxxs	O
in	O
a	O
factor	B
graph	I
can	O
be	O
found	O
by	O
first	O
running	O
the	O
sum-product	O
message	B
passing	I
algorithm	O
and	O
then	O
evaluating	O
the	O
required	O
marginals	O
using	O
consider	O
a	O
tree-structured	O
factor	B
graph	I
in	O
which	O
a	O
given	O
subset	O
of	O
the	O
variable	O
nodes	O
form	O
a	O
connected	O
subgraph	O
any	O
variable	O
node	B
of	O
the	O
subset	O
is	O
connected	O
to	O
at	O
least	O
one	O
of	O
the	O
other	O
variable	O
nodes	O
via	O
a	O
single	O
factor	O
node	B
show	O
how	O
the	O
sum-product	B
algorithm	I
can	O
be	O
used	O
to	O
compute	O
the	O
marginal	B
distribution	O
over	O
that	O
subset	O
www	O
in	O
section	O
we	O
showed	O
that	O
the	O
marginal	B
distribution	O
pxi	O
for	O
a	O
variable	O
node	B
xi	O
in	O
a	O
factor	B
graph	I
is	O
given	O
by	O
the	O
product	O
of	O
the	O
messages	O
arriving	O
at	O
this	O
node	B
from	O
neighbouring	O
factor	O
nodes	O
in	O
the	O
form	O
show	O
that	O
the	O
marginal	B
pxi	O
can	O
also	O
be	O
written	O
as	O
the	O
product	O
of	O
the	O
incoming	O
message	O
along	O
any	O
one	O
of	O
the	O
links	O
with	O
the	O
outgoing	O
message	O
along	O
the	O
same	O
link	B
show	O
that	O
the	O
marginal	B
distribution	O
for	O
the	O
variables	O
xs	O
in	O
a	O
factor	O
fsxs	O
in	O
a	O
tree-structured	O
factor	B
graph	I
after	O
running	O
the	O
sum-product	O
message	B
passing	I
algorithm	O
can	O
be	O
written	O
as	O
the	O
product	O
of	O
the	O
message	O
arriving	O
at	O
the	O
factor	O
node	B
along	O
all	O
its	O
links	O
times	O
the	O
local	B
factor	O
fxs	O
in	O
the	O
form	O
graphical	O
models	O
in	O
we	O
verified	O
that	O
the	O
sum-product	B
algorithm	I
run	O
on	O
the	O
graph	O
in	O
figure	O
with	O
node	B
designated	O
as	O
the	O
root	B
node	B
gives	O
the	O
correct	O
marginal	B
for	O
show	O
that	O
the	O
correct	O
marginals	O
are	O
obtained	O
also	O
for	O
and	O
similarly	O
show	O
that	O
the	O
use	O
of	O
the	O
result	O
after	O
running	O
the	O
sum-product	B
algorithm	I
on	O
this	O
graph	O
gives	O
the	O
correct	O
joint	O
distribution	O
for	O
consider	O
a	O
tree-structured	O
factor	B
graph	I
over	O
discrete	O
variables	O
and	O
suppose	O
we	O
wish	O
to	O
evaluate	O
the	O
joint	O
distribution	O
pxa	O
xb	O
associated	O
with	O
two	O
variables	O
xa	O
and	O
xb	O
that	O
do	O
not	O
belong	O
to	O
a	O
common	O
factor	O
define	O
a	O
procedure	O
for	O
using	O
the	O
sumproduct	O
algorithm	O
to	O
evaluate	O
this	O
joint	O
distribution	O
in	O
which	O
one	O
of	O
the	O
variables	O
is	O
successively	O
clamped	O
to	O
each	O
of	O
its	O
allowed	O
values	O
consider	O
two	O
discrete	O
variables	O
x	O
and	O
y	O
each	O
having	O
three	O
possible	O
states	O
for	O
example	O
x	O
y	O
construct	O
a	O
joint	O
distribution	O
px	O
y	O
over	O
these	O
variables	O
having	O
the	O
property	O
that	O
the	O
that	O
maximizes	O
the	O
marginal	B
px	O
along	O
with	O
the	O
that	O
maximizes	O
the	O
marginal	B
py	O
together	O
have	O
probability	B
zero	O
under	O
the	O
joint	O
distribution	O
so	O
that	O
www	O
the	O
concept	O
of	O
a	O
pending	B
message	I
in	O
the	O
sum-product	B
algorithm	I
for	O
a	O
factor	B
graph	I
was	O
defined	O
in	O
section	O
show	O
that	O
if	O
the	O
graph	O
has	O
one	O
or	O
more	O
cycles	O
there	O
will	O
always	O
be	O
at	O
least	O
one	O
pending	B
message	I
irrespective	O
of	O
how	O
long	O
the	O
algorithm	O
runs	O
www	O
show	O
that	O
if	O
the	O
sum-product	B
algorithm	I
is	O
run	O
on	O
a	O
factor	B
graph	I
with	O
a	O
tree	B
structure	O
loops	O
then	O
after	O
a	O
finite	O
number	O
of	O
messages	O
have	O
been	O
sent	O
there	O
will	O
be	O
no	O
pending	O
messages	O
mixture	B
models	O
and	O
em	B
if	O
we	O
define	O
a	O
joint	O
distribution	O
over	O
observed	O
and	O
latent	O
variables	O
the	O
corresponding	O
distribution	O
of	O
the	O
observed	O
variables	O
alone	O
is	O
obtained	O
by	O
marginalization	O
this	O
allows	O
relatively	O
complex	O
marginal	B
distributions	O
over	O
observed	O
variables	O
to	O
be	O
expressed	O
in	O
terms	O
of	O
more	O
tractable	O
joint	O
distributions	O
over	O
the	O
expanded	O
space	O
of	O
observed	O
and	O
latent	O
variables	O
the	O
introduction	O
of	O
latent	O
variables	O
thereby	O
allows	O
complicated	O
distributions	O
to	O
be	O
formed	O
from	O
simpler	O
components	O
in	O
this	O
chapter	O
we	O
shall	O
see	O
that	O
mixture	B
distributions	O
such	O
as	O
the	O
gaussian	B
mixture	B
discussed	O
in	O
section	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
discrete	O
latent	O
variables	O
continuous	O
latent	O
variables	O
will	O
form	O
the	O
subject	O
of	O
chapter	O
as	O
well	O
as	O
providing	O
a	O
framework	O
for	O
building	O
more	O
complex	O
probability	B
distributions	O
mixture	B
models	O
can	O
also	O
be	O
used	O
to	O
cluster	O
data	O
we	O
therefore	O
begin	O
our	O
discussion	O
of	O
mixture	B
distributions	O
by	O
considering	O
the	O
problem	O
of	O
finding	O
clusters	O
in	O
a	O
set	O
of	O
data	O
points	O
which	O
we	O
approach	O
first	O
using	O
a	O
nonprobabilistic	O
technique	O
called	O
the	O
k-means	O
algorithm	O
then	O
we	O
introduce	O
the	O
latent	B
variable	I
section	O
mixture	B
models	O
and	O
em	B
section	O
section	O
section	O
view	O
of	O
mixture	B
distributions	O
in	O
which	O
the	O
discrete	O
latent	O
variables	O
can	O
be	O
interpreted	O
as	O
defining	O
assignments	O
of	O
data	O
points	O
to	O
specific	O
components	O
of	O
the	O
mixture	B
a	O
general	O
technique	O
for	O
finding	O
maximum	B
likelihood	I
estimators	O
in	O
latent	B
variable	I
models	O
is	O
the	O
expectation-maximization	O
algorithm	O
we	O
first	O
of	O
all	O
use	O
the	O
gaussian	B
mixture	B
distribution	I
to	O
motivate	O
the	O
em	B
algorithm	I
in	O
a	O
fairly	O
informal	O
way	O
and	O
then	O
we	O
give	O
a	O
more	O
careful	O
treatment	O
based	O
on	O
the	O
latent	B
variable	I
viewpoint	O
we	O
shall	O
see	O
that	O
the	O
k-means	O
algorithm	O
corresponds	O
to	O
a	O
particular	O
nonprobabilistic	O
limit	O
of	O
em	B
applied	O
to	O
mixtures	O
of	O
gaussians	O
finally	O
we	O
discuss	O
em	B
in	O
some	O
generality	O
gaussian	B
mixture	B
models	O
are	O
widely	O
used	O
in	O
data	O
mining	O
pattern	O
recognition	O
machine	O
learning	B
and	O
statistical	O
analysis	O
in	O
many	O
applications	O
their	O
parameters	O
are	O
determined	O
by	O
maximum	B
likelihood	I
typically	O
using	O
the	O
em	B
algorithm	I
however	O
as	O
we	O
shall	O
see	O
there	O
are	O
some	O
significant	O
limitations	O
to	O
the	O
maximum	B
likelihood	I
approach	O
and	O
in	O
chapter	O
we	O
shall	O
show	O
that	O
an	O
elegant	O
bayesian	B
treatment	O
can	O
be	O
given	O
using	O
the	O
framework	O
of	O
variational	B
inference	B
this	O
requires	O
little	O
additional	O
computation	O
compared	O
with	O
em	B
and	O
it	O
resolves	O
the	O
principal	O
difficulties	O
of	O
maximum	B
likelihood	I
while	O
also	O
allowing	O
the	O
number	O
of	O
components	O
in	O
the	O
mixture	B
to	O
be	O
inferred	O
automatically	O
from	O
the	O
data	O
k-means	O
clustering	B
we	O
begin	O
by	O
considering	O
the	O
problem	O
of	O
identifying	O
groups	O
or	O
clusters	O
of	O
data	O
points	O
in	O
a	O
multidimensional	O
space	O
suppose	O
we	O
have	O
a	O
data	O
set	O
xn	O
consisting	O
of	O
n	O
observations	O
of	O
a	O
random	O
d-dimensional	O
euclidean	O
variable	O
x	O
our	O
goal	O
is	O
to	O
partition	O
the	O
data	O
set	O
into	O
some	O
number	O
k	O
of	O
clusters	O
where	O
we	O
shall	O
suppose	O
for	O
the	O
moment	O
that	O
the	O
value	O
of	O
k	O
is	O
given	O
intuitively	O
we	O
might	O
think	O
of	O
a	O
cluster	O
as	O
comprising	O
a	O
group	O
of	O
data	O
points	O
whose	O
inter-point	O
distances	O
are	O
small	O
compared	O
with	O
the	O
distances	O
to	O
points	O
outside	O
of	O
the	O
cluster	O
we	O
can	O
formalize	O
this	O
notion	O
by	O
first	O
introducing	O
a	O
set	O
of	O
d-dimensional	O
vectors	O
k	O
where	O
k	O
k	O
in	O
which	O
k	O
is	O
a	O
prototype	O
associated	O
with	O
the	O
kth	O
cluster	O
as	O
we	O
shall	O
see	O
shortly	O
we	O
can	O
think	O
of	O
the	O
k	O
as	O
representing	O
the	O
centres	O
of	O
the	O
clusters	O
our	O
goal	O
is	O
then	O
to	O
find	O
an	O
assignment	O
of	O
data	O
points	O
to	O
clusters	O
as	O
well	O
as	O
a	O
set	O
of	O
vectors	O
k	O
such	O
that	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
distances	O
of	O
each	O
data	O
point	O
to	O
its	O
closest	O
vector	O
k	O
is	O
a	O
minimum	O
it	O
is	O
convenient	O
at	O
this	O
point	O
to	O
define	O
some	O
notation	O
to	O
describe	O
the	O
assignment	O
of	O
data	O
points	O
to	O
clusters	O
for	O
each	O
data	O
point	O
xn	O
we	O
introduce	O
a	O
corresponding	O
set	O
of	O
binary	O
indicator	O
variables	O
rnk	O
where	O
k	O
k	O
describing	O
which	O
of	O
the	O
k	O
clusters	O
the	O
data	O
point	O
xn	O
is	O
assigned	O
to	O
so	O
that	O
if	O
data	O
point	O
xn	O
is	O
assigned	O
to	O
cluster	O
k	O
then	O
rnk	O
and	O
rnj	O
for	O
j	O
k	O
this	O
is	O
known	O
as	O
the	O
coding	O
scheme	O
we	O
can	O
then	O
define	O
an	O
objective	O
function	O
sometimes	O
called	O
a	O
distortion	B
measure	I
given	O
by	O
j	O
which	O
represents	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
distances	O
of	O
each	O
data	O
point	O
to	O
its	O
k-means	O
clustering	B
assigned	O
vector	O
k	O
our	O
goal	O
is	O
to	O
find	O
values	O
for	O
the	O
and	O
the	O
k	O
so	O
as	O
to	O
minimize	O
j	O
we	O
can	O
do	O
this	O
through	O
an	O
iterative	O
procedure	O
in	O
which	O
each	O
iteration	O
involves	O
two	O
successive	O
steps	O
corresponding	O
to	O
successive	O
optimizations	O
with	O
respect	O
to	O
the	O
rnk	O
and	O
the	O
k	O
first	O
we	O
choose	O
some	O
initial	O
values	O
for	O
the	O
k	O
then	O
in	O
the	O
first	O
phase	O
we	O
minimize	O
j	O
with	O
respect	O
to	O
the	O
rnk	O
keeping	O
the	O
k	O
fixed	O
in	O
the	O
second	O
phase	O
we	O
minimize	O
j	O
with	O
respect	O
to	O
the	O
k	O
keeping	O
rnk	O
fixed	O
this	O
two-stage	O
optimization	O
is	O
then	O
repeated	O
until	O
convergence	O
we	O
shall	O
see	O
that	O
these	O
two	O
stages	O
of	O
updating	O
rnk	O
and	O
updating	O
k	O
correspond	O
respectively	O
to	O
the	O
e	O
and	O
m	O
steps	O
of	O
the	O
em	B
algorithm	I
and	O
to	O
emphasize	O
this	O
we	O
shall	O
use	O
the	O
terms	O
e	O
step	O
and	O
m	O
step	O
in	O
the	O
context	O
of	O
the	O
k-means	O
algorithm	O
section	O
consider	O
first	O
the	O
determination	O
of	O
the	O
rnk	O
because	O
j	O
in	O
is	O
a	O
linear	O
function	O
of	O
rnk	O
this	O
optimization	O
can	O
be	O
performed	O
easily	O
to	O
give	O
a	O
closed	O
form	O
solution	O
the	O
terms	O
involving	O
different	O
n	O
are	O
independent	B
and	O
so	O
we	O
can	O
optimize	O
for	O
each	O
n	O
separately	O
by	O
choosing	O
rnk	O
to	O
be	O
for	O
whichever	O
value	O
of	O
k	O
gives	O
the	O
minimum	O
value	O
of	O
in	O
other	O
words	O
we	O
simply	O
assign	O
the	O
nth	O
data	O
point	O
to	O
the	O
closest	O
cluster	O
centre	O
more	O
formally	O
this	O
can	O
be	O
expressed	O
as	O
if	O
k	O
arg	O
minj	O
otherwise	O
rnk	O
now	O
consider	O
the	O
optimization	O
of	O
the	O
k	O
with	O
the	O
rnk	O
held	O
fixed	O
the	O
objective	O
function	O
j	O
is	O
a	O
quadratic	O
function	O
of	O
k	O
and	O
it	O
can	O
be	O
minimized	O
by	O
setting	O
its	O
derivative	B
with	O
respect	O
to	O
k	O
to	O
zero	O
giving	O
k	O
rnkxn	O
k	O
n	O
n	O
rnk	O
which	O
we	O
can	O
easily	O
solve	O
for	O
k	O
to	O
give	O
the	O
denominator	O
in	O
this	O
expression	O
is	O
equal	O
to	O
the	O
number	O
of	O
points	O
assigned	O
to	O
cluster	O
k	O
and	O
so	O
this	O
result	O
has	O
a	O
simple	O
interpretation	O
namely	O
set	O
k	O
equal	O
to	O
the	O
mean	B
of	O
all	O
of	O
the	O
data	O
points	O
xn	O
assigned	O
to	O
cluster	O
k	O
for	O
this	O
reason	O
the	O
procedure	O
is	O
known	O
as	O
the	O
k-means	O
algorithm	O
the	O
two	O
phases	O
of	O
re-assigning	O
data	O
points	O
to	O
clusters	O
and	O
re-computing	O
the	O
cluster	O
means	O
are	O
repeated	O
in	O
turn	O
until	O
there	O
is	O
no	O
further	O
change	O
in	O
the	O
assignments	O
until	O
some	O
maximum	O
number	O
of	O
iterations	O
is	O
exceeded	O
because	O
each	O
phase	O
reduces	O
the	O
value	O
of	O
the	O
objective	O
function	O
j	O
convergence	O
of	O
the	O
algorithm	O
is	O
assured	O
however	O
it	O
may	O
converge	O
to	O
a	O
local	B
rather	O
than	O
global	B
minimum	I
of	O
j	O
the	O
convergence	O
properties	O
of	O
the	O
k-means	O
algorithm	O
were	O
studied	O
by	O
macqueen	O
the	O
k-means	O
algorithm	O
is	O
illustrated	O
using	O
the	O
old	B
faithful	I
data	I
set	O
in	O
figure	O
for	O
the	O
purposes	O
of	O
this	O
example	O
we	O
have	O
made	O
a	O
linear	O
re-scaling	O
of	O
the	O
data	O
known	O
as	O
standardizing	B
such	O
that	O
each	O
of	O
the	O
variables	O
has	O
zero	O
mean	B
and	O
unit	O
standard	B
deviation	I
for	O
this	O
example	O
we	O
have	O
chosen	O
k	O
and	O
so	O
in	O
this	O
exercise	O
appendix	O
a	O
mixture	B
models	O
and	O
em	B
figure	O
illustration	O
of	O
the	O
k-means	O
algorithm	O
using	O
the	O
re-scaled	O
old	B
faithful	I
data	I
set	O
green	O
points	O
denote	O
the	O
data	O
set	O
in	O
a	O
two-dimensional	O
euclidean	O
space	O
the	O
initial	O
choices	O
for	O
centres	O
and	O
are	O
shown	O
by	O
the	O
red	O
and	O
blue	O
crosses	O
respectively	O
in	O
the	O
initial	O
e	O
step	O
each	O
data	O
point	O
is	O
assigned	O
either	O
to	O
the	O
red	O
cluster	O
or	O
to	O
the	O
blue	O
cluster	O
according	O
to	O
which	O
cluster	O
centre	O
is	O
nearer	O
this	O
is	O
equivalent	O
to	O
classifying	O
the	O
points	O
according	O
to	O
which	O
side	O
of	O
the	O
perpendicular	O
bisector	O
of	O
the	O
two	O
cluster	O
centres	O
shown	O
by	O
the	O
magenta	O
line	O
they	O
lie	O
on	O
in	O
the	O
subsequent	O
m	O
step	O
each	O
cluster	O
centre	O
is	O
re-computed	O
to	O
be	O
the	O
mean	B
of	O
the	O
points	O
assigned	O
to	O
the	O
corresponding	O
cluster	O
show	O
successive	O
e	O
and	O
m	O
steps	O
through	O
to	O
final	O
convergence	O
of	O
the	O
algorithm	O
k-means	O
clustering	B
figure	O
plot	O
of	O
the	O
cost	B
function	I
j	O
given	O
by	O
after	O
each	O
e	O
step	O
points	O
and	O
m	O
step	O
points	O
of	O
the	O
kmeans	O
algorithm	O
for	O
the	O
example	O
shown	O
in	O
figure	O
the	O
algorithm	O
has	O
converged	O
after	O
the	O
third	O
m	O
step	O
and	O
the	O
final	O
em	B
cycle	O
produces	O
no	O
changes	O
in	O
either	O
the	O
assignments	O
or	O
the	O
prototype	O
vectors	O
j	O
section	O
section	O
exercise	O
case	O
the	O
assignment	O
of	O
each	O
data	O
point	O
to	O
the	O
nearest	O
cluster	O
centre	O
is	O
equivalent	O
to	O
a	O
classification	B
of	O
the	O
data	O
points	O
according	O
to	O
which	O
side	O
they	O
lie	O
of	O
the	O
perpendicular	O
bisector	O
of	O
the	O
two	O
cluster	O
centres	O
a	O
plot	O
of	O
the	O
cost	B
function	I
j	O
given	O
by	O
for	O
the	O
old	O
faithful	O
example	O
is	O
shown	O
in	O
figure	O
note	O
that	O
we	O
have	O
deliberately	O
chosen	O
poor	O
initial	O
values	O
for	O
the	O
cluster	O
centres	O
so	O
that	O
the	O
algorithm	O
takes	O
several	O
steps	O
before	O
convergence	O
in	O
practice	O
a	O
better	O
initialization	O
procedure	O
would	O
be	O
to	O
choose	O
the	O
cluster	O
centres	O
k	O
to	O
be	O
equal	O
to	O
a	O
random	O
subset	O
of	O
k	O
data	O
points	O
it	O
is	O
also	O
worth	O
noting	O
that	O
the	O
k-means	O
algorithm	O
itself	O
is	O
often	O
used	O
to	O
initialize	O
the	O
parameters	O
in	O
a	O
gaussian	B
mixture	B
model	I
before	O
applying	O
the	O
em	B
algorithm	I
a	O
direct	O
implementation	O
of	O
the	O
k-means	O
algorithm	O
as	O
discussed	O
here	O
can	O
be	O
relatively	O
slow	O
because	O
in	O
each	O
e	O
step	O
it	O
is	O
necessary	O
to	O
compute	O
the	O
euclidean	O
distance	O
between	O
every	O
prototype	O
vector	O
and	O
every	O
data	O
point	O
various	O
schemes	O
have	O
been	O
proposed	O
for	O
speeding	O
up	O
the	O
k-means	O
algorithm	O
some	O
of	O
which	O
are	O
based	O
on	O
precomputing	O
a	O
data	O
structure	O
such	O
as	O
a	O
tree	B
such	O
that	O
nearby	O
points	O
are	O
in	O
the	O
same	O
subtree	O
and	O
paliwal	O
moore	O
other	O
approaches	O
make	O
use	O
of	O
the	O
triangle	O
inequality	O
for	O
distances	O
thereby	O
avoiding	O
unnecessary	O
distance	O
calculations	O
elkan	O
so	O
far	O
we	O
have	O
considered	O
a	O
batch	O
version	O
of	O
k-means	O
in	O
which	O
the	O
whole	O
data	O
set	O
is	O
used	O
together	O
to	O
update	O
the	O
prototype	O
vectors	O
we	O
can	O
also	O
derive	O
an	O
on-line	O
stochastic	B
algorithm	O
by	O
applying	O
the	O
robbins-monro	O
procedure	O
to	O
the	O
problem	O
of	O
finding	O
the	O
roots	O
of	O
the	O
regression	B
function	I
given	O
by	O
the	O
derivatives	O
of	O
j	O
in	O
with	O
respect	O
to	O
k	O
this	O
leads	O
to	O
a	O
sequential	O
update	O
in	O
which	O
for	O
each	O
data	O
point	O
xn	O
in	O
turn	O
we	O
update	O
the	O
nearest	O
prototype	O
k	O
using	O
new	O
k	O
old	O
k	O
nxn	O
old	O
k	O
where	O
n	O
is	O
the	O
learning	B
rate	I
parameter	I
which	O
is	O
typically	O
made	O
to	O
decrease	O
monotonically	O
as	O
more	O
data	O
points	O
are	O
considered	O
the	O
k-means	O
algorithm	O
is	O
based	O
on	O
the	O
use	O
of	O
squared	O
euclidean	O
distance	O
as	O
the	O
measure	O
of	O
dissimilarity	O
between	O
a	O
data	O
point	O
and	O
a	O
prototype	O
vector	O
not	O
only	O
does	O
this	O
limit	O
the	O
type	O
of	O
data	O
variables	O
that	O
can	O
be	O
considered	O
would	O
be	O
inappropriate	O
for	O
cases	O
where	O
some	O
or	O
all	O
of	O
the	O
variables	O
represent	O
categorical	O
labels	O
for	O
instance	O
mixture	B
models	O
and	O
em	B
section	O
but	O
it	O
can	O
also	O
make	O
the	O
determination	O
of	O
the	O
cluster	O
means	O
nonrobust	O
to	O
outliers	B
we	O
can	O
generalize	O
the	O
k-means	O
algorithm	O
by	O
introducing	O
a	O
more	O
general	O
dissimilarity	O
measure	O
vx	O
between	O
two	O
vectors	O
x	O
and	O
and	O
then	O
minimizing	O
the	O
following	O
distortion	B
measure	I
rnkvxn	O
k	O
which	O
gives	O
the	O
k-medoids	B
algorithm	I
the	O
e	O
step	O
again	O
involves	O
for	O
given	O
cluster	O
prototypes	O
k	O
assigning	O
each	O
data	O
point	O
to	O
the	O
cluster	O
for	O
which	O
the	O
dissimilarity	O
to	O
the	O
corresponding	O
prototype	O
is	O
smallest	O
the	O
computational	O
cost	O
of	O
this	O
is	O
okn	O
as	O
is	O
the	O
case	O
for	O
the	O
standard	O
k-means	O
algorithm	O
for	O
a	O
general	O
choice	O
of	O
dissimilarity	O
measure	O
the	O
m	O
step	O
is	O
potentially	O
more	O
complex	O
than	O
for	O
k-means	O
and	O
so	O
it	O
is	O
common	O
to	O
restrict	O
each	O
cluster	O
prototype	O
to	O
be	O
equal	O
to	O
one	O
of	O
the	O
data	O
vectors	O
assigned	O
to	O
that	O
cluster	O
as	O
this	O
allows	O
the	O
algorithm	O
to	O
be	O
implemented	O
for	O
any	O
choice	O
of	O
dissimilarity	O
measure	O
v	O
so	O
long	O
as	O
it	O
can	O
be	O
readily	O
evaluated	O
thus	O
the	O
m	O
step	O
involves	O
for	O
each	O
cluster	O
k	O
a	O
discrete	O
search	O
over	O
the	O
nk	O
points	O
assigned	O
to	O
that	O
cluster	O
which	O
requires	O
on	O
k	O
evaluations	O
of	O
v	O
one	O
notable	O
feature	O
of	O
the	O
k-means	O
algorithm	O
is	O
that	O
at	O
each	O
iteration	O
every	O
data	O
point	O
is	O
assigned	O
uniquely	O
to	O
one	O
and	O
only	O
one	O
of	O
the	O
clusters	O
whereas	O
some	O
data	O
points	O
will	O
be	O
much	O
closer	O
to	O
a	O
particular	O
centre	O
k	O
than	O
to	O
any	O
other	O
centre	O
there	O
may	O
be	O
other	O
data	O
points	O
that	O
lie	O
roughly	O
midway	O
between	O
cluster	O
centres	O
in	O
the	O
latter	O
case	O
it	O
is	O
not	O
clear	O
that	O
the	O
hard	O
assignment	O
to	O
the	O
nearest	O
cluster	O
is	O
the	O
most	O
appropriate	O
we	O
shall	O
see	O
in	O
the	O
next	O
section	O
that	O
by	O
adopting	O
a	O
probabilistic	O
approach	O
we	O
obtain	O
soft	B
assignments	O
of	O
data	O
points	O
to	O
clusters	O
in	O
a	O
way	O
that	O
reflects	O
the	O
level	O
of	O
uncertainty	O
over	O
the	O
most	O
appropriate	O
assignment	O
this	O
probabilistic	O
formulation	O
brings	O
with	O
it	O
numerous	O
benefits	O
image	O
segmentation	O
and	O
compression	O
as	O
an	O
illustration	O
of	O
the	O
application	O
of	O
the	O
k-means	O
algorithm	O
we	O
consider	O
the	O
related	O
problems	O
of	O
image	O
segmentation	O
and	O
image	O
compression	O
the	O
goal	O
of	O
segmentation	O
is	O
to	O
partition	O
an	O
image	O
into	O
regions	O
each	O
of	O
which	O
has	O
a	O
reasonably	O
homogeneous	B
visual	O
appearance	O
or	O
which	O
corresponds	O
to	O
objects	O
or	O
parts	O
of	O
objects	O
and	O
ponce	O
each	O
pixel	O
in	O
an	O
image	O
is	O
a	O
point	O
in	O
a	O
space	O
comprising	O
the	O
intensities	O
of	O
the	O
red	O
blue	O
and	O
green	O
channels	O
and	O
our	O
segmentation	O
algorithm	O
simply	O
treats	O
each	O
pixel	O
in	O
the	O
image	O
as	O
a	O
separate	O
data	O
point	O
note	O
that	O
strictly	O
this	O
space	O
is	O
not	O
euclidean	O
because	O
the	O
channel	O
intensities	O
are	O
bounded	O
by	O
the	O
interval	O
nevertheless	O
we	O
can	O
apply	O
the	O
k-means	O
algorithm	O
without	O
difficulty	O
we	O
illustrate	O
the	O
result	O
of	O
running	O
k-means	O
to	O
convergence	O
for	O
any	O
particular	O
value	O
of	O
k	O
by	O
re-drawing	O
the	O
image	O
replacing	O
each	O
pixel	O
vector	O
with	O
the	O
g	O
b	O
intensity	O
triplet	O
given	O
by	O
the	O
centre	O
k	O
to	O
which	O
that	O
pixel	O
has	O
been	O
assigned	O
results	O
for	O
various	O
values	O
of	O
k	O
are	O
shown	O
in	O
figure	O
we	O
see	O
that	O
for	O
a	O
given	O
value	O
of	O
k	O
the	O
algorithm	O
is	O
representing	O
the	O
image	O
using	O
a	O
palette	O
of	O
only	O
k	O
colours	O
it	O
should	O
be	O
emphasized	O
that	O
this	O
use	O
of	O
k-means	O
is	O
not	O
a	O
particularly	O
sophisticated	O
approach	O
to	O
image	O
segmentation	O
not	O
least	O
because	O
it	O
takes	O
no	O
account	O
of	O
the	O
spatial	O
proximity	O
of	O
different	O
pixels	O
the	O
image	O
segmentation	O
problem	O
is	O
in	O
general	O
extremely	O
difficult	O
k	O
k	O
k	O
original	O
image	O
k-means	O
clustering	B
figure	O
two	O
examples	O
of	O
the	O
application	O
of	O
the	O
k-means	B
clustering	B
algorithm	I
to	O
image	O
segmentation	O
showing	O
the	O
initial	O
images	O
together	O
with	O
their	O
k-means	O
segmentations	O
obtained	O
using	O
various	O
values	O
of	O
k	O
this	O
also	O
illustrates	O
of	O
the	O
use	O
of	O
vector	B
quantization	I
for	O
data	B
compression	I
in	O
which	O
smaller	O
values	O
of	O
k	O
give	O
higher	O
compression	O
at	O
the	O
expense	O
of	O
poorer	O
image	O
quality	O
and	O
remains	O
the	O
subject	O
of	O
active	O
research	O
and	O
is	O
introduced	O
here	O
simply	O
to	O
illustrate	O
the	O
behaviour	O
of	O
the	O
k-means	O
algorithm	O
we	O
can	O
also	O
use	O
the	O
result	O
of	O
a	O
clustering	B
algorithm	O
to	O
perform	O
data	B
compression	I
it	O
is	O
important	O
to	O
distinguish	O
between	O
lossless	B
data	B
compression	I
in	O
which	O
the	O
goal	O
is	O
to	O
be	O
able	O
to	O
reconstruct	O
the	O
original	O
data	O
exactly	O
from	O
the	O
compressed	O
representation	O
and	O
lossy	B
data	B
compression	I
in	O
which	O
we	O
accept	O
some	O
errors	O
in	O
the	O
reconstruction	O
in	O
return	O
for	O
higher	O
levels	O
of	O
compression	O
than	O
can	O
be	O
achieved	O
in	O
the	O
lossless	O
case	O
we	O
can	O
apply	O
the	O
k-means	O
algorithm	O
to	O
the	O
problem	O
of	O
lossy	B
data	B
compression	I
as	O
follows	O
for	O
each	O
of	O
the	O
n	O
data	O
points	O
we	O
store	O
only	O
the	O
identity	O
k	O
of	O
the	O
cluster	O
to	O
which	O
it	O
is	O
assigned	O
we	O
also	O
store	O
the	O
values	O
of	O
the	O
k	O
cluster	O
centres	O
k	O
which	O
typically	O
requires	O
significantly	O
less	O
data	O
provided	O
we	O
choose	O
k	O
n	O
each	O
data	O
point	O
is	O
then	O
approximated	O
by	O
its	O
nearest	O
centre	O
k	O
new	O
data	O
points	O
can	O
similarly	O
be	O
compressed	O
by	O
first	O
finding	O
the	O
nearest	O
k	O
and	O
then	O
storing	O
the	O
label	O
k	O
instead	O
of	O
the	O
original	O
data	O
vector	O
this	O
framework	O
is	O
often	O
called	O
vector	B
quantization	I
and	O
the	O
vectors	O
k	O
are	O
called	O
code-book	B
vectors	I
mixture	B
models	O
and	O
em	B
the	O
image	O
segmentation	O
problem	O
discussed	O
above	O
also	O
provides	O
an	O
illustration	O
of	O
the	O
use	O
of	O
clustering	B
for	O
data	B
compression	I
suppose	O
the	O
original	O
image	O
has	O
n	O
pixels	O
comprising	O
g	O
b	O
values	O
each	O
of	O
which	O
is	O
stored	O
with	O
bits	B
of	O
precision	O
then	O
to	O
transmit	O
the	O
whole	O
image	O
directly	O
would	O
cost	O
bits	B
now	O
suppose	O
we	O
first	O
run	O
k-means	O
on	O
the	O
image	O
data	O
and	O
then	O
instead	O
of	O
transmitting	O
the	O
original	O
pixel	O
intensity	O
vectors	O
we	O
transmit	O
the	O
identity	O
of	O
the	O
nearest	O
vector	O
k	O
because	O
there	O
are	O
k	O
such	O
vectors	O
this	O
requires	O
k	O
bits	B
per	O
pixel	O
we	O
must	O
also	O
transmit	O
the	O
k	O
code	O
book	O
vectors	O
k	O
which	O
requires	O
bits	B
and	O
so	O
the	O
total	O
number	O
of	O
bits	B
required	O
to	O
transmit	O
the	O
image	O
is	O
n	O
k	O
up	O
to	O
the	O
nearest	O
integer	O
the	O
original	O
image	O
shown	O
in	O
figure	O
has	O
pixels	O
and	O
so	O
requires	O
bits	B
to	O
transmit	O
directly	O
by	O
comparison	O
the	O
compressed	O
images	O
require	O
bits	B
bits	B
and	O
bits	B
respectively	O
to	O
transmit	O
these	O
represent	O
compression	O
ratios	O
compared	O
to	O
the	O
original	O
image	O
of	O
and	O
respectively	O
we	O
see	O
that	O
there	O
is	O
a	O
trade-off	O
between	O
degree	O
of	O
compression	O
and	O
image	O
quality	O
note	O
that	O
our	O
aim	O
in	O
this	O
example	O
is	O
to	O
illustrate	O
the	O
k-means	O
algorithm	O
if	O
we	O
had	O
been	O
aiming	O
to	O
produce	O
a	O
good	O
image	O
compressor	O
then	O
it	O
would	O
be	O
more	O
fruitful	O
to	O
consider	O
small	O
blocks	O
of	O
adjacent	O
pixels	O
for	O
instance	O
and	O
thereby	O
exploit	O
the	O
correlations	O
that	O
exist	O
in	O
natural	O
images	O
between	O
nearby	O
pixels	O
mixtures	O
of	O
gaussians	O
in	O
section	O
we	O
motivated	O
the	O
gaussian	B
mixture	B
model	I
as	O
a	O
simple	O
linear	O
superposition	O
of	O
gaussian	B
components	O
aimed	O
at	O
providing	O
a	O
richer	O
class	O
of	O
density	B
models	O
than	O
the	O
single	O
gaussian	B
we	O
now	O
turn	O
to	O
a	O
formulation	O
of	O
gaussian	B
mixtures	O
in	O
terms	O
of	O
discrete	O
latent	O
variables	O
this	O
will	O
provide	O
us	O
with	O
a	O
deeper	O
insight	O
into	O
this	O
important	O
distribution	O
and	O
will	O
also	O
serve	O
to	O
motivate	O
the	O
expectation-maximization	O
algorithm	O
recall	O
from	O
that	O
the	O
gaussian	B
mixture	B
distribution	I
can	O
be	O
written	O
as	O
a	O
linear	O
superposition	O
of	O
gaussians	O
in	O
the	O
form	O
px	O
kn	O
k	O
k	O
let	O
us	O
introduce	O
a	O
k-dimensional	O
binary	O
random	O
variable	O
z	O
having	O
a	O
representation	O
in	O
which	O
a	O
particular	O
element	O
zk	O
is	O
equal	O
to	O
and	O
all	O
other	O
elements	O
are	O
equal	O
to	O
the	O
values	O
of	O
zk	O
therefore	O
satisfy	O
zk	O
and	O
k	O
zk	O
and	O
we	O
see	O
that	O
there	O
are	O
k	O
possible	O
states	O
for	O
the	O
vector	O
z	O
according	O
to	O
which	O
element	O
is	O
nonzero	O
we	O
shall	O
define	O
the	O
joint	O
distribution	O
px	O
z	O
in	O
terms	O
of	O
a	O
marginal	B
distribution	O
pz	O
and	O
a	O
conditional	B
distribution	O
pxz	O
corresponding	O
to	O
the	O
graphical	B
model	I
in	O
figure	O
the	O
marginal	B
distribution	O
over	O
z	O
is	O
specified	O
in	O
terms	O
of	O
the	O
mixing	O
coefficients	O
k	O
such	O
that	O
pzk	O
k	O
mixtures	O
of	O
gaussians	O
figure	O
graphical	O
representation	O
of	O
a	O
mixture	B
model	I
in	O
which	O
the	O
joint	O
distribution	O
is	O
expressed	O
in	O
the	O
form	O
px	O
z	O
pzpxz	O
z	O
x	O
where	O
the	O
parameters	O
k	O
must	O
satisfy	O
together	O
with	O
k	O
k	O
in	O
order	O
to	O
be	O
valid	O
probabilities	O
because	O
z	O
uses	O
a	O
representation	O
we	O
can	O
also	O
write	O
this	O
distribution	O
in	O
the	O
form	O
pz	O
zk	O
k	O
similarly	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
a	O
particular	O
value	O
for	O
z	O
is	O
a	O
gaussian	B
pxzk	O
n	O
k	O
k	O
which	O
can	O
also	O
be	O
written	O
in	O
the	O
form	O
pxz	O
n	O
k	O
kzk	O
exercise	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
pzpxz	O
and	O
the	O
marginal	B
distribution	O
of	O
x	O
is	O
then	O
obtained	O
by	O
summing	O
the	O
joint	O
distribution	O
over	O
all	O
possible	O
states	O
of	O
z	O
to	O
give	O
px	O
pzpxz	O
kn	O
k	O
k	O
z	O
where	O
we	O
have	O
made	O
use	O
of	O
and	O
thus	O
the	O
marginal	B
distribution	O
of	O
x	O
is	O
a	O
gaussian	B
mixture	B
of	O
the	O
form	O
if	O
we	O
have	O
several	O
observations	O
xn	O
then	O
because	O
we	O
have	O
represented	O
the	O
marginal	B
distribution	O
in	O
the	O
form	O
px	O
z	O
px	O
z	O
it	O
follows	O
that	O
for	O
every	O
observed	O
data	O
point	O
xn	O
there	O
is	O
a	O
corresponding	O
latent	B
variable	I
zn	O
we	O
have	O
therefore	O
found	O
an	O
equivalent	O
formulation	O
of	O
the	O
gaussian	B
mixture	B
involving	O
an	O
explicit	O
latent	B
variable	I
it	O
might	O
seem	O
that	O
we	O
have	O
not	O
gained	O
much	O
by	O
doing	O
so	O
however	O
we	O
are	O
now	O
able	O
to	O
work	O
with	O
the	O
joint	O
distribution	O
px	O
z	O
mixture	B
models	O
and	O
em	B
instead	O
of	O
the	O
marginal	B
distribution	O
px	O
and	O
this	O
will	O
lead	O
to	O
significant	O
simplifications	O
most	O
notably	O
through	O
the	O
introduction	O
of	O
the	O
expectation-maximization	O
algorithm	O
another	O
quantity	O
that	O
will	O
play	O
an	O
important	O
role	O
is	O
the	O
conditional	B
probability	B
of	O
z	O
given	O
x	O
we	O
shall	O
use	O
to	O
denote	O
pzk	O
whose	O
value	O
can	O
be	O
found	O
using	O
bayes	B
theorem	O
pzk	O
pzk	O
pzj	O
kn	O
k	O
k	O
jn	O
j	O
j	O
section	O
we	O
shall	O
view	O
k	O
as	O
the	O
prior	B
probability	B
of	O
zk	O
and	O
the	O
quantity	O
as	O
the	O
corresponding	O
posterior	B
probability	B
once	O
we	O
have	O
observed	O
x	O
as	O
we	O
shall	O
see	O
later	O
can	O
also	O
be	O
viewed	O
as	O
the	O
responsibility	B
that	O
component	O
k	O
takes	O
for	O
explaining	O
the	O
observation	O
x	O
we	O
can	O
use	O
the	O
technique	O
of	O
ancestral	B
sampling	I
to	O
generate	O
random	O
samples	O
distributed	O
according	O
to	O
the	O
gaussian	B
mixture	B
model	I
to	O
do	O
this	O
we	O
first	O
generate	O
a	O
value	O
for	O
z	O
which	O
we	O
from	O
the	O
marginal	B
distribution	O
pz	O
and	O
then	O
generate	O
a	O
value	O
for	O
x	O
from	O
the	O
conditional	B
distribution	O
techniques	O
for	O
sampling	O
from	O
standard	O
distributions	O
are	O
discussed	O
in	O
chapter	O
we	O
can	O
depict	O
samples	O
from	O
the	O
joint	O
distribution	O
px	O
z	O
by	O
plotting	O
points	O
at	O
the	O
corresponding	O
values	O
of	O
x	O
and	O
then	O
colouring	O
them	O
according	O
to	O
the	O
value	O
of	O
z	O
in	O
other	O
words	O
according	O
to	O
which	O
gaussian	B
component	O
was	O
responsible	O
for	O
generating	O
them	O
as	O
shown	O
in	O
figure	O
similarly	O
samples	O
from	O
the	O
marginal	B
distribution	O
px	O
are	O
obtained	O
by	O
taking	O
the	O
samples	O
from	O
the	O
joint	O
distribution	O
and	O
ignoring	O
the	O
values	O
of	O
z	O
these	O
are	O
illustrated	O
in	O
figure	O
by	O
plotting	O
the	O
x	O
values	O
without	O
any	O
coloured	O
labels	O
we	O
can	O
also	O
use	O
this	O
synthetic	O
data	O
set	O
to	O
illustrate	O
the	O
responsibilities	O
by	O
evaluating	O
for	O
every	O
data	O
point	O
the	O
posterior	B
probability	B
for	O
each	O
component	O
in	O
the	O
mixture	B
distribution	I
from	O
which	O
this	O
data	O
set	O
was	O
generated	O
in	O
particular	O
we	O
can	O
represent	O
the	O
value	O
of	O
the	O
responsibilities	O
associated	O
with	O
data	O
point	O
xn	O
by	O
plotting	O
the	O
corresponding	O
point	O
using	O
proportions	O
of	O
red	O
blue	O
and	O
green	O
ink	O
given	O
by	O
for	O
k	O
respectively	O
as	O
shown	O
in	O
figure	O
so	O
for	O
instance	O
a	O
data	O
point	O
for	O
which	O
will	O
be	O
coloured	O
red	O
whereas	O
one	O
for	O
which	O
will	O
be	O
coloured	O
with	O
equal	O
proportions	O
of	O
blue	O
and	O
green	O
ink	O
and	O
so	O
will	O
appear	O
cyan	O
this	O
should	O
be	O
compared	O
with	O
figure	O
in	O
which	O
the	O
data	O
points	O
were	O
labelled	O
using	O
the	O
true	O
identity	O
of	O
the	O
component	O
from	O
which	O
they	O
were	O
generated	O
maximum	B
likelihood	I
suppose	O
we	O
have	O
a	O
data	O
set	O
of	O
observations	O
xn	O
and	O
we	O
wish	O
to	O
model	O
this	O
data	O
using	O
a	O
mixture	B
of	I
gaussians	I
we	O
can	O
represent	O
this	O
data	O
set	O
as	O
an	O
n	O
d	O
mixtures	O
of	O
gaussians	O
figure	O
example	O
of	O
points	O
drawn	O
from	O
the	O
mixture	B
of	I
gaussians	I
shown	O
in	O
figure	O
samples	O
from	O
the	O
joint	O
distribution	O
pzpxz	O
in	O
which	O
the	O
three	O
states	O
of	O
z	O
corresponding	O
to	O
the	O
three	O
components	O
of	O
the	O
mixture	B
are	O
depicted	O
in	O
red	O
green	O
and	O
blue	O
and	O
the	O
corresponding	O
samples	O
from	O
the	O
marginal	B
distribution	O
px	O
which	O
is	O
obtained	O
by	O
simply	O
ignoring	O
the	O
values	O
of	O
z	O
and	O
just	O
plotting	O
the	O
x	O
values	O
the	O
data	O
set	O
in	O
is	O
said	O
to	O
be	O
complete	O
whereas	O
that	O
in	O
is	O
incomplete	O
the	O
same	O
samples	O
in	O
which	O
the	O
colours	O
represent	O
the	O
value	O
of	O
the	O
responsibilities	O
associated	O
with	O
data	O
point	O
xn	O
obtained	O
by	O
plotting	O
the	O
corresponding	O
point	O
using	O
proportions	O
of	O
red	O
blue	O
and	O
green	O
ink	O
given	O
by	O
for	O
k	O
respectively	O
matrix	O
x	O
in	O
which	O
the	O
nth	O
row	O
is	O
given	O
by	O
xt	O
n	O
similarly	O
the	O
corresponding	O
latent	O
variables	O
will	O
be	O
denoted	O
by	O
an	O
n	O
k	O
matrix	O
z	O
with	O
rows	O
zt	O
n	O
if	O
we	O
assume	O
that	O
the	O
data	O
points	O
are	O
drawn	O
independently	O
from	O
the	O
distribution	O
then	O
we	O
can	O
express	O
the	O
gaussian	B
mixture	B
model	I
for	O
this	O
i	O
i	O
d	O
data	O
set	O
using	O
the	O
graphical	O
representation	O
shown	O
in	O
figure	O
from	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
ln	O
px	O
ln	O
kn	O
k	O
k	O
before	O
discussing	O
how	O
to	O
maximize	O
this	O
function	O
it	O
is	O
worth	O
emphasizing	O
that	O
there	O
is	O
a	O
significant	O
problem	O
associated	O
with	O
the	O
maximum	B
likelihood	I
framework	O
applied	O
to	O
gaussian	B
mixture	B
models	O
due	O
to	O
the	O
presence	O
of	O
singularities	B
for	O
simplicity	O
consider	O
a	O
gaussian	B
mixture	B
whose	O
components	O
have	O
covariance	B
matrices	O
given	O
by	O
k	O
ki	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
although	O
the	O
conclusions	O
will	O
hold	O
for	O
general	O
covariance	B
matrices	O
suppose	O
that	O
one	O
of	O
the	O
components	O
of	O
the	O
mixture	B
model	I
let	O
us	O
say	O
the	O
jth	O
component	O
has	O
its	O
mean	B
j	O
exactly	O
equal	O
to	O
one	O
of	O
the	O
data	O
figure	O
graphical	O
representation	O
of	O
a	O
gaussian	B
mixture	B
model	I
for	O
a	O
set	O
of	O
n	O
i	O
i	O
d	O
data	O
points	O
with	O
corresponding	O
latent	O
points	O
where	O
n	O
n	O
zn	O
xn	O
n	O
mixture	B
models	O
and	O
em	B
figure	O
illustration	O
of	O
how	O
singularities	B
in	O
the	O
likelihood	B
function	I
arise	O
with	O
mixtures	O
of	O
gaussians	O
this	O
should	O
be	O
compared	O
with	O
the	O
case	O
of	O
a	O
single	O
gaussian	B
shown	O
in	O
figure	O
for	O
which	O
no	O
singularities	B
arise	O
px	O
points	O
so	O
that	O
j	O
xn	O
for	O
some	O
value	O
of	O
n	O
this	O
data	O
point	O
will	O
then	O
contribute	O
a	O
term	O
in	O
the	O
likelihood	B
function	I
of	O
the	O
form	O
x	O
j	O
n	O
j	O
i	O
if	O
we	O
consider	O
the	O
limit	O
j	O
then	O
we	O
see	O
that	O
this	O
term	O
goes	O
to	O
infinity	O
and	O
so	O
the	O
log	O
likelihood	B
function	I
will	O
also	O
go	O
to	O
infinity	O
thus	O
the	O
maximization	O
of	O
the	O
log	O
likelihood	B
function	I
is	O
not	O
a	O
well	O
posed	O
problem	O
because	O
such	O
singularities	B
will	O
always	O
be	O
present	O
and	O
will	O
occur	O
whenever	O
one	O
of	O
the	O
gaussian	B
components	O
collapses	O
onto	O
a	O
specific	O
data	O
point	O
recall	O
that	O
this	O
problem	O
did	O
not	O
arise	O
in	O
the	O
case	O
of	O
a	O
single	O
gaussian	B
distribution	O
to	O
understand	O
the	O
difference	O
note	O
that	O
if	O
a	O
single	O
gaussian	B
collapses	O
onto	O
a	O
data	O
point	O
it	O
will	O
contribute	O
multiplicative	O
factors	O
to	O
the	O
likelihood	B
function	I
arising	O
from	O
the	O
other	O
data	O
points	O
and	O
these	O
factors	O
will	O
go	O
to	O
zero	O
exponentially	O
fast	O
giving	O
an	O
overall	O
likelihood	O
that	O
goes	O
to	O
zero	O
rather	O
than	O
infinity	O
however	O
once	O
we	O
have	O
least	O
two	O
components	O
in	O
the	O
mixture	B
one	O
of	O
the	O
components	O
can	O
have	O
a	O
finite	O
variance	B
and	O
therefore	O
assign	O
finite	O
probability	B
to	O
all	O
of	O
the	O
data	O
points	O
while	O
the	O
other	O
component	O
can	O
shrink	O
onto	O
one	O
specific	O
data	O
point	O
and	O
thereby	O
contribute	O
an	O
ever	O
increasing	O
additive	O
value	O
to	O
the	O
log	O
likelihood	O
this	O
is	O
illustrated	O
in	O
figure	O
these	O
singularities	B
provide	O
another	O
example	O
of	O
the	O
severe	O
over-fitting	B
that	O
can	O
occur	O
in	O
a	O
maximum	B
likelihood	I
approach	O
we	O
shall	O
see	O
that	O
this	O
difficulty	O
does	O
not	O
occur	O
if	O
we	O
adopt	O
a	O
bayesian	B
approach	O
for	O
the	O
moment	O
however	O
we	O
simply	O
note	O
that	O
in	O
applying	O
maximum	B
likelihood	I
to	O
gaussian	B
mixture	B
models	O
we	O
must	O
take	O
steps	O
to	O
avoid	O
finding	O
such	O
pathological	O
solutions	O
and	O
instead	O
seek	O
local	B
maxima	O
of	O
the	O
likelihood	B
function	I
that	O
are	O
well	O
behaved	O
we	O
can	O
hope	O
to	O
avoid	O
the	O
singularities	B
by	O
using	O
suitable	O
heuristics	O
for	O
instance	O
by	O
detecting	O
when	O
a	O
gaussian	B
component	O
is	O
collapsing	O
and	O
resetting	O
its	O
mean	B
to	O
a	O
randomly	O
chosen	O
value	O
while	O
also	O
resetting	O
its	O
covariance	B
to	O
some	O
large	O
value	O
and	O
then	O
continuing	O
with	O
the	O
optimization	O
a	O
further	O
issue	O
in	O
finding	O
maximum	B
likelihood	I
solutions	O
arises	O
from	O
the	O
fact	O
that	O
for	O
any	O
given	O
maximum	B
likelihood	I
solution	O
a	O
k-component	O
mixture	B
will	O
have	O
a	O
total	O
of	O
k	O
equivalent	O
solutions	O
corresponding	O
to	O
the	O
k	O
ways	O
of	O
assigning	O
k	O
sets	O
of	O
parameters	O
to	O
k	O
components	O
in	O
other	O
words	O
for	O
any	O
given	O
point	O
in	O
the	O
space	O
of	O
parameter	O
values	O
there	O
will	O
be	O
a	O
further	O
k	O
additional	O
points	O
all	O
of	O
which	O
give	O
rise	O
to	O
exactly	O
the	O
same	O
distribution	O
this	O
problem	O
is	O
known	O
as	O
section	O
mixtures	O
of	O
gaussians	O
identifiability	B
and	O
berger	O
and	O
is	O
an	O
important	O
issue	O
when	O
we	O
wish	O
to	O
interpret	O
the	O
parameter	O
values	O
discovered	O
by	O
a	O
model	O
identifiability	B
will	O
also	O
arise	O
when	O
we	O
discuss	O
models	O
having	O
continuous	O
latent	O
variables	O
in	O
chapter	O
however	O
for	O
the	O
purposes	O
of	O
finding	O
a	O
good	O
density	B
model	O
it	O
is	O
irrelevant	O
because	O
any	O
of	O
the	O
equivalent	O
solutions	O
is	O
as	O
good	O
as	O
any	O
other	O
maximizing	O
the	O
log	O
likelihood	B
function	I
for	O
a	O
gaussian	B
mixture	B
model	I
turns	O
out	O
to	O
be	O
a	O
more	O
complex	O
problem	O
than	O
for	O
the	O
case	O
of	O
a	O
single	O
gaussian	B
the	O
difficulty	O
arises	O
from	O
the	O
presence	O
of	O
the	O
summation	O
over	O
k	O
that	O
appears	O
inside	O
the	O
logarithm	O
in	O
so	O
that	O
the	O
logarithm	O
function	O
no	O
longer	O
acts	O
directly	O
on	O
the	O
gaussian	B
if	O
we	O
set	O
the	O
derivatives	O
of	O
the	O
log	O
likelihood	O
to	O
zero	O
we	O
will	O
no	O
longer	O
obtain	O
a	O
closed	O
form	O
solution	O
as	O
we	O
shall	O
see	O
shortly	O
one	O
approach	O
is	O
to	O
apply	O
gradient-based	O
optimization	O
techniques	O
nocedal	O
and	O
wright	O
bishop	O
and	O
nabney	O
although	O
gradient-based	O
techniques	O
are	O
feasible	O
and	O
indeed	O
will	O
play	O
an	O
important	O
role	O
when	O
we	O
discuss	O
mixture	B
density	B
networks	O
in	O
chapter	O
we	O
now	O
consider	O
an	O
alternative	O
approach	O
known	O
as	O
the	O
em	B
algorithm	I
which	O
has	O
broad	O
applicability	O
and	O
which	O
will	O
lay	O
the	O
foundations	O
for	O
a	O
discussion	O
of	O
variational	B
inference	B
techniques	O
in	O
chapter	O
em	B
for	O
gaussian	B
mixtures	O
an	O
elegant	O
and	O
powerful	O
method	O
for	O
finding	O
maximum	B
likelihood	I
solutions	O
for	O
models	O
with	O
latent	O
variables	O
is	O
called	O
the	O
expectation-maximization	O
algorithm	O
or	O
em	B
algorithm	I
et	O
al	O
mclachlan	O
and	O
krishnan	O
later	O
we	O
shall	O
give	O
a	O
general	O
treatment	O
of	O
em	B
and	O
we	O
shall	O
also	O
show	O
how	O
em	B
can	O
be	O
generalized	B
to	O
obtain	O
the	O
variational	B
inference	B
framework	O
initially	O
we	O
shall	O
motivate	O
the	O
em	B
algorithm	I
by	O
giving	O
a	O
relatively	O
informal	O
treatment	O
in	O
the	O
context	O
of	O
the	O
gaussian	B
mixture	B
model	I
we	O
emphasize	O
however	O
that	O
em	B
has	O
broad	O
applicability	O
and	O
indeed	O
it	O
will	O
be	O
encountered	O
in	O
the	O
context	O
of	O
a	O
variety	O
of	O
different	O
models	O
in	O
this	O
book	O
let	O
us	O
begin	O
by	O
writing	O
down	O
the	O
conditions	O
that	O
must	O
be	O
satisfied	O
at	O
a	O
maximum	O
of	O
the	O
likelihood	B
function	I
setting	O
the	O
derivatives	O
of	O
ln	O
px	O
in	O
with	O
respect	O
to	O
the	O
means	O
k	O
of	O
the	O
gaussian	B
components	O
to	O
zero	O
we	O
obtain	O
kxn	O
k	O
kn	O
k	O
k	O
j	O
jn	O
j	O
j	O
section	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
form	O
for	O
the	O
gaussian	B
distribution	O
note	O
that	O
the	O
posterior	O
probabilities	O
or	O
responsibilities	O
given	O
by	O
appear	O
naturally	O
on	O
the	O
right-hand	O
side	O
multiplying	O
by	O
we	O
assume	O
to	O
be	O
nonsingular	O
and	O
rearranging	O
we	O
obtain	O
k	O
k	O
nk	O
where	O
we	O
have	O
defined	O
nk	O
mixture	B
models	O
and	O
em	B
section	O
appendix	O
e	O
we	O
can	O
interpret	O
nk	O
as	O
the	O
effective	O
number	O
of	O
points	O
assigned	O
to	O
cluster	O
k	O
note	O
carefully	O
the	O
form	O
of	O
this	O
solution	O
we	O
see	O
that	O
the	O
mean	B
k	O
for	O
the	O
kth	O
gaussian	B
component	O
is	O
obtained	O
by	O
taking	O
a	O
weighted	O
mean	B
of	O
all	O
of	O
the	O
points	O
in	O
the	O
data	O
set	O
in	O
which	O
the	O
weighting	O
factor	O
for	O
data	O
point	O
xn	O
is	O
given	O
by	O
the	O
posterior	B
probability	B
that	O
component	O
k	O
was	O
responsible	O
for	O
generating	O
xn	O
if	O
we	O
set	O
the	O
derivative	B
of	O
ln	O
px	O
with	O
respect	O
to	O
k	O
to	O
zero	O
and	O
follow	O
a	O
similar	O
line	O
of	O
reasoning	O
making	O
use	O
of	O
the	O
result	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
covariance	B
matrix	O
of	O
a	O
single	O
gaussian	B
we	O
obtain	O
kxn	O
kt	O
k	O
nk	O
which	O
has	O
the	O
same	O
form	O
as	O
the	O
corresponding	O
result	O
for	O
a	O
single	O
gaussian	B
fitted	O
to	O
the	O
data	O
set	O
but	O
again	O
with	O
each	O
data	O
point	O
weighted	O
by	O
the	O
corresponding	O
posterior	B
probability	B
and	O
with	O
the	O
denominator	O
given	O
by	O
the	O
effective	O
number	O
of	O
points	O
associated	O
with	O
the	O
corresponding	O
component	O
finally	O
we	O
maximize	O
ln	O
px	O
with	O
respect	O
to	O
the	O
mixing	O
coefficients	O
k	O
here	O
we	O
must	O
take	O
account	O
of	O
the	O
constraint	O
which	O
requires	O
the	O
mixing	O
coefficients	O
to	O
sum	O
to	O
one	O
this	O
can	O
be	O
achieved	O
using	O
a	O
lagrange	B
multiplier	I
and	O
maximizing	O
the	O
following	O
quantity	O
ln	O
px	O
k	O
which	O
gives	O
n	O
k	O
k	O
j	O
jn	O
j	O
j	O
where	O
again	O
we	O
see	O
the	O
appearance	O
of	O
the	O
responsibilities	O
if	O
we	O
now	O
multiply	O
both	O
sides	O
by	O
k	O
and	O
sum	O
over	O
k	O
making	O
use	O
of	O
the	O
constraint	O
we	O
find	O
n	O
using	O
this	O
to	O
eliminate	O
and	O
rearranging	O
we	O
obtain	O
k	O
nk	O
n	O
so	O
that	O
the	O
mixing	B
coefficient	I
for	O
the	O
kth	O
component	O
is	O
given	O
by	O
the	O
average	O
responsibility	B
which	O
that	O
component	O
takes	O
for	O
explaining	O
the	O
data	O
points	O
it	O
is	O
worth	O
emphasizing	O
that	O
the	O
results	O
and	O
do	O
not	O
constitute	O
a	O
closed-form	O
solution	O
for	O
the	O
parameters	O
of	O
the	O
mixture	B
model	I
because	O
the	O
responsibilities	O
depend	O
on	O
those	O
parameters	O
in	O
a	O
complex	O
way	O
through	O
however	O
these	O
results	O
do	O
suggest	O
a	O
simple	O
iterative	O
scheme	O
for	O
finding	O
a	O
solution	O
to	O
the	O
maximum	B
likelihood	I
problem	O
which	O
as	O
we	O
shall	O
see	O
turns	O
out	O
to	O
be	O
an	O
instance	O
of	O
the	O
em	B
algorithm	I
for	O
the	O
particular	O
case	O
of	O
the	O
gaussian	B
mixture	B
model	I
we	O
first	O
choose	O
some	O
initial	O
values	O
for	O
the	O
means	O
covariances	O
and	O
mixing	O
coefficients	O
then	O
we	O
alternate	O
between	O
the	O
following	O
two	O
updates	O
that	O
we	O
shall	O
call	O
the	O
e	O
step	O
mixtures	O
of	O
gaussians	O
l	O
l	O
l	O
l	O
figure	O
illustration	O
of	O
the	O
em	B
algorithm	I
using	O
the	O
old	O
faithful	O
set	O
as	O
used	O
for	O
the	O
illustration	O
of	O
the	O
k-means	O
algorithm	O
in	O
figure	O
see	O
the	O
text	O
for	O
details	O
section	O
and	O
the	O
m	O
step	O
for	O
reasons	O
that	O
will	O
become	O
apparent	O
shortly	O
in	O
the	O
expectation	B
step	I
or	O
e	O
step	O
we	O
use	O
the	O
current	O
values	O
for	O
the	O
parameters	O
to	O
evaluate	O
the	O
posterior	O
probabilities	O
or	O
responsibilities	O
given	O
by	O
we	O
then	O
use	O
these	O
probabilities	O
in	O
the	O
maximization	B
step	I
or	O
m	O
step	O
to	O
re-estimate	O
the	O
means	O
covariances	O
and	O
mixing	O
coefficients	O
using	O
the	O
results	O
and	O
note	O
that	O
in	O
so	O
doing	O
we	O
first	O
evaluate	O
the	O
new	O
means	O
using	O
and	O
then	O
use	O
these	O
new	O
values	O
to	O
find	O
the	O
covariances	O
using	O
in	O
keeping	O
with	O
the	O
corresponding	O
result	O
for	O
a	O
single	O
gaussian	B
distribution	O
we	O
shall	O
show	O
that	O
each	O
update	O
to	O
the	O
parameters	O
resulting	O
from	O
an	O
e	O
step	O
followed	O
by	O
an	O
m	O
step	O
is	O
guaranteed	O
to	O
increase	O
the	O
log	O
likelihood	B
function	I
in	O
practice	O
the	O
algorithm	O
is	O
deemed	O
to	O
have	O
converged	O
when	O
the	O
change	O
in	O
the	O
log	O
likelihood	B
function	I
or	O
alternatively	O
in	O
the	O
parameters	O
falls	O
below	O
some	O
threshold	O
we	O
illustrate	O
the	O
em	B
algorithm	I
for	O
a	O
mixture	B
of	O
two	O
gaussians	O
applied	O
to	O
the	O
rescaled	O
old	B
faithful	I
data	I
set	O
in	O
figure	O
here	O
a	O
mixture	B
of	O
two	O
gaussians	O
is	O
used	O
with	O
centres	O
initialized	O
using	O
the	O
same	O
values	O
as	O
for	O
the	O
k-means	O
algorithm	O
in	O
figure	O
and	O
with	O
precision	O
matrices	O
initialized	O
to	O
be	O
proportional	O
to	O
the	O
unit	O
matrix	O
plot	O
shows	O
the	O
data	O
points	O
in	O
green	O
together	O
with	O
the	O
initial	O
configuration	O
of	O
the	O
mixture	B
model	I
in	O
which	O
the	O
one	O
standard-deviation	O
contours	O
for	O
the	O
two	O
mixture	B
models	O
and	O
em	B
gaussian	B
components	O
are	O
shown	O
as	O
blue	O
and	O
red	O
circles	O
plot	O
shows	O
the	O
result	O
of	O
the	O
initial	O
e	O
step	O
in	O
which	O
each	O
data	O
point	O
is	O
depicted	O
using	O
a	O
proportion	O
of	O
blue	O
ink	O
equal	O
to	O
the	O
posterior	B
probability	B
of	O
having	O
been	O
generated	O
from	O
the	O
blue	O
component	O
and	O
a	O
corresponding	O
proportion	O
of	O
red	O
ink	O
given	O
by	O
the	O
posterior	B
probability	B
of	O
having	O
been	O
generated	O
by	O
the	O
red	O
component	O
thus	O
points	O
that	O
have	O
a	O
significant	O
probability	B
for	O
belonging	O
to	O
either	O
cluster	O
appear	O
purple	O
the	O
situation	O
after	O
the	O
first	O
m	O
step	O
is	O
shown	O
in	O
plot	O
in	O
which	O
the	O
mean	B
of	O
the	O
blue	O
gaussian	B
has	O
moved	O
to	O
the	O
mean	B
of	O
the	O
data	O
set	O
weighted	O
by	O
the	O
probabilities	O
of	O
each	O
data	O
point	O
belonging	O
to	O
the	O
blue	O
cluster	O
in	O
other	O
words	O
it	O
has	O
moved	O
to	O
the	O
centre	O
of	O
mass	O
of	O
the	O
blue	O
ink	O
similarly	O
the	O
covariance	B
of	O
the	O
blue	O
gaussian	B
is	O
set	O
equal	O
to	O
the	O
covariance	B
of	O
the	O
blue	O
ink	O
analogous	O
results	O
hold	O
for	O
the	O
red	O
component	O
plots	O
and	O
show	O
the	O
results	O
after	O
and	O
complete	O
cycles	O
of	O
em	B
respectively	O
in	O
plot	O
the	O
algorithm	O
is	O
close	O
to	O
convergence	O
note	O
that	O
the	O
em	B
algorithm	I
takes	O
many	O
more	O
iterations	O
to	O
reach	O
convergence	O
compared	O
with	O
the	O
k-means	O
algorithm	O
and	O
that	O
each	O
cycle	O
requires	O
significantly	O
more	O
computation	O
it	O
is	O
therefore	O
common	O
to	O
run	O
the	O
k-means	O
algorithm	O
in	O
order	O
to	O
find	O
a	O
suitable	O
initialization	O
for	O
a	O
gaussian	B
mixture	B
model	I
that	O
is	O
subsequently	O
adapted	O
using	O
em	B
the	O
covariance	B
matrices	O
can	O
conveniently	O
be	O
initialized	O
to	O
the	O
sample	O
covariances	O
of	O
the	O
clusters	O
found	O
by	O
the	O
k-means	O
algorithm	O
and	O
the	O
mixing	O
coefficients	O
can	O
be	O
set	O
to	O
the	O
fractions	O
of	O
data	O
points	O
assigned	O
to	O
the	O
respective	O
clusters	O
as	O
with	O
gradient-based	O
approaches	O
for	O
maximizing	O
the	O
log	O
likelihood	O
techniques	O
must	O
be	O
employed	O
to	O
avoid	O
singularities	B
of	O
the	O
likelihood	B
function	I
in	O
which	O
a	O
gaussian	B
component	O
collapses	O
onto	O
a	O
particular	O
data	O
point	O
it	O
should	O
be	O
emphasized	O
that	O
there	O
will	O
generally	O
be	O
multiple	O
local	B
maxima	O
of	O
the	O
log	O
likelihood	B
function	I
and	O
that	O
em	B
is	O
not	O
guaranteed	O
to	O
find	O
the	O
largest	O
of	O
these	O
maxima	O
because	O
the	O
em	B
algorithm	I
for	O
gaussian	B
mixtures	O
plays	O
such	O
an	O
important	O
role	O
we	O
summarize	O
it	O
below	O
em	B
for	O
gaussian	B
mixtures	O
given	O
a	O
gaussian	B
mixture	B
model	I
the	O
goal	O
is	O
to	O
maximize	O
the	O
likelihood	B
function	I
with	O
respect	O
to	O
the	O
parameters	O
the	O
means	O
and	O
covariances	O
of	O
the	O
components	O
and	O
the	O
mixing	O
coefficients	O
initialize	O
the	O
means	O
k	O
covariances	O
k	O
and	O
mixing	O
coefficients	O
k	O
and	O
evaluate	O
the	O
initial	O
value	O
of	O
the	O
log	O
likelihood	O
e	O
step	O
evaluate	O
the	O
responsibilities	O
using	O
the	O
current	O
parameter	O
values	O
kn	O
k	O
k	O
jn	O
j	O
j	O
an	O
alternative	O
view	O
of	O
em	B
m	O
step	O
re-estimate	O
the	O
parameters	O
using	O
the	O
current	O
responsibilities	O
new	O
k	O
nk	O
new	O
k	O
new	O
k	O
nk	O
nk	O
n	O
new	O
k	O
new	O
k	O
where	O
evaluate	O
the	O
log	O
likelihood	O
ln	O
px	O
nk	O
ln	O
kn	O
k	O
k	O
and	O
check	O
for	O
convergence	O
of	O
either	O
the	O
parameters	O
or	O
the	O
log	O
likelihood	O
if	O
the	O
convergence	O
criterion	O
is	O
not	O
satisfied	O
return	O
to	O
step	O
an	O
alternative	O
view	O
of	O
em	B
in	O
this	O
section	O
we	O
present	O
a	O
complementary	O
view	O
of	O
the	O
em	B
algorithm	I
that	O
recognizes	O
the	O
key	O
role	O
played	O
by	O
latent	O
variables	O
we	O
discuss	O
this	O
approach	O
first	O
of	O
all	O
in	O
an	O
abstract	O
setting	O
and	O
then	O
for	O
illustration	O
we	O
consider	O
once	O
again	O
the	O
case	O
of	O
gaussian	B
mixtures	O
the	O
goal	O
of	O
the	O
em	B
algorithm	I
is	O
to	O
find	O
maximum	B
likelihood	I
solutions	O
for	O
models	O
having	O
latent	O
variables	O
we	O
denote	O
the	O
set	O
of	O
all	O
observed	O
data	O
by	O
x	O
in	O
which	O
the	O
nth	O
row	O
represents	O
xt	O
n	O
and	O
similarly	O
we	O
denote	O
the	O
set	O
of	O
all	O
latent	O
variables	O
by	O
z	O
with	O
a	O
corresponding	O
row	O
zt	O
n	O
the	O
set	O
of	O
all	O
model	O
parameters	O
is	O
denoted	O
by	O
and	O
so	O
the	O
log	O
likelihood	B
function	I
is	O
given	O
by	O
ln	O
px	O
ln	O
px	O
z	O
z	O
note	O
that	O
our	O
discussion	O
will	O
apply	O
equally	O
well	O
to	O
continuous	O
latent	O
variables	O
simply	O
by	O
replacing	O
the	O
sum	O
over	O
z	O
with	O
an	O
integral	O
a	O
key	O
observation	O
is	O
that	O
the	O
summation	O
over	O
the	O
latent	O
variables	O
appears	O
inside	O
the	O
logarithm	O
even	O
if	O
the	O
joint	O
distribution	O
px	O
z	O
belongs	O
to	O
the	O
exponential	O
mixture	B
models	O
and	O
em	B
family	O
the	O
marginal	B
distribution	O
px	O
typically	O
does	O
not	O
as	O
a	O
result	O
of	O
this	O
summation	O
the	O
presence	O
of	O
the	O
sum	O
prevents	O
the	O
logarithm	O
from	O
acting	O
directly	O
on	O
the	O
joint	O
distribution	O
resulting	O
in	O
complicated	O
expressions	O
for	O
the	O
maximum	B
likelihood	I
solution	O
now	O
suppose	O
that	O
for	O
each	O
observation	O
in	O
x	O
we	O
were	O
told	O
the	O
corresponding	O
value	O
of	O
the	O
latent	B
variable	I
z	O
we	O
shall	O
call	O
z	O
the	O
complete	B
data	I
set	I
and	O
we	O
shall	O
refer	O
to	O
the	O
actual	O
observed	O
data	O
x	O
as	O
incomplete	O
as	O
illustrated	O
in	O
figure	O
the	O
likelihood	B
function	I
for	O
the	O
complete	B
data	I
set	I
simply	O
takes	O
the	O
form	O
ln	O
px	O
z	O
and	O
we	O
shall	O
suppose	O
that	O
maximization	O
of	O
this	O
complete-data	O
log	O
likelihood	B
function	I
is	O
straightforward	O
in	O
practice	O
however	O
we	O
are	O
not	O
given	O
the	O
complete	B
data	I
set	I
z	O
but	O
only	O
the	O
incomplete	O
data	O
x	O
our	O
state	O
of	O
knowledge	O
of	O
the	O
values	O
of	O
the	O
latent	O
variables	O
in	O
z	O
is	O
given	O
only	O
by	O
the	O
posterior	O
distribution	O
pzx	O
because	O
we	O
cannot	O
use	O
the	O
complete-data	O
log	O
likelihood	O
we	O
consider	O
instead	O
its	O
expected	O
value	O
under	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	B
variable	I
which	O
corresponds	O
we	O
shall	O
see	O
to	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
in	O
the	O
subsequent	O
m	O
step	O
we	O
maximize	O
this	O
expectation	B
if	O
the	O
current	O
estimate	O
for	O
the	O
parameters	O
is	O
denoted	O
old	O
then	O
a	O
pair	O
of	O
successive	O
e	O
and	O
m	O
steps	O
gives	O
rise	O
to	O
a	O
revised	O
estimate	O
new	O
the	O
algorithm	O
is	O
initialized	O
by	O
choosing	O
some	O
starting	O
value	O
for	O
the	O
parameters	O
the	O
use	O
of	O
the	O
expectation	B
may	O
seem	O
somewhat	O
arbitrary	O
however	O
we	O
shall	O
see	O
the	O
motivation	O
for	O
this	O
choice	O
when	O
we	O
give	O
a	O
deeper	O
treatment	O
of	O
em	B
in	O
section	O
in	O
the	O
e	O
step	O
we	O
use	O
the	O
current	O
parameter	O
values	O
old	O
to	O
find	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
given	O
by	O
pzx	O
old	O
we	O
then	O
use	O
this	O
posterior	O
distribution	O
to	O
find	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
evaluated	O
for	O
some	O
general	O
parameter	O
value	O
this	O
expectation	B
denoted	O
q	O
old	O
is	O
given	O
by	O
q	O
old	O
pzx	O
old	O
ln	O
px	O
z	O
z	O
in	O
the	O
m	O
step	O
we	O
determine	O
the	O
revised	O
parameter	O
estimate	O
new	O
by	O
maximizing	O
this	O
function	O
new	O
arg	O
max	O
note	O
that	O
in	O
the	O
definition	O
of	O
q	O
old	O
the	O
logarithm	O
acts	O
directly	O
on	O
the	O
joint	O
distribution	O
px	O
z	O
and	O
so	O
the	O
corresponding	O
m-step	O
maximization	O
will	O
by	O
supposition	O
be	O
tractable	O
q	O
old	O
section	O
the	O
general	O
em	B
algorithm	I
is	O
summarized	O
below	O
it	O
has	O
the	O
property	O
as	O
we	O
shall	O
show	O
later	O
that	O
each	O
cycle	O
of	O
em	B
will	O
increase	O
the	O
incomplete-data	O
log	O
likelihood	O
it	O
is	O
already	O
at	O
a	O
local	B
maximum	O
the	O
general	O
em	B
algorithm	I
given	O
a	O
joint	O
distribution	O
px	O
z	O
over	O
observed	O
variables	O
x	O
and	O
latent	O
variables	O
z	O
governed	O
by	O
parameters	O
the	O
goal	O
is	O
to	O
maximize	O
the	O
likelihood	B
function	I
px	O
with	O
respect	O
to	O
choose	O
an	O
initial	O
setting	O
for	O
the	O
parameters	O
old	O
an	O
alternative	O
view	O
of	O
em	B
e	O
step	O
evaluate	O
pzx	O
old	O
m	O
step	O
evaluate	O
new	O
given	O
by	O
q	O
old	O
new	O
arg	O
max	O
where	O
q	O
old	O
pzx	O
old	O
ln	O
px	O
z	O
exercise	O
z	O
check	O
for	O
convergence	O
of	O
either	O
the	O
log	O
likelihood	O
or	O
the	O
parameter	O
values	O
if	O
the	O
convergence	O
criterion	O
is	O
not	O
satisfied	O
then	O
let	O
old	O
new	O
and	O
return	O
to	O
step	O
the	O
em	B
algorithm	I
can	O
also	O
be	O
used	O
to	O
find	O
map	O
posterior	O
solutions	O
for	O
models	O
in	O
which	O
a	O
prior	B
p	O
is	O
defined	O
over	O
the	O
parameters	O
in	O
this	O
case	O
the	O
e	O
step	O
remains	O
the	O
same	O
as	O
in	O
the	O
maximum	B
likelihood	I
case	O
whereas	O
in	O
the	O
m	O
step	O
the	O
quantity	O
to	O
be	O
maximized	O
is	O
given	O
by	O
q	O
old	O
ln	O
p	O
suitable	O
choices	O
for	O
the	O
prior	B
will	O
remove	O
the	O
singularities	B
of	O
the	O
kind	O
illustrated	O
in	O
figure	O
here	O
we	O
have	O
considered	O
the	O
use	O
of	O
the	O
em	B
algorithm	I
to	O
maximize	O
a	O
likelihood	B
function	I
when	O
there	O
are	O
discrete	O
latent	O
variables	O
however	O
it	O
can	O
also	O
be	O
applied	O
when	O
the	O
unobserved	O
variables	O
correspond	O
to	O
missing	O
values	O
in	O
the	O
data	O
set	O
the	O
distribution	O
of	O
the	O
observed	O
values	O
is	O
obtained	O
by	O
taking	O
the	O
joint	O
distribution	O
of	O
all	O
the	O
variables	O
and	O
then	O
marginalizing	O
over	O
the	O
missing	O
ones	O
em	B
can	O
then	O
be	O
used	O
to	O
maximize	O
the	O
corresponding	O
likelihood	B
function	I
we	O
shall	O
show	O
an	O
example	O
of	O
the	O
application	O
of	O
this	O
technique	O
in	O
the	O
context	O
of	O
principal	B
component	I
analysis	I
in	O
figure	O
this	O
will	O
be	O
a	O
valid	O
procedure	O
if	O
the	O
data	O
values	O
are	O
missing	B
at	I
random	I
meaning	O
that	O
the	O
mechanism	O
causing	O
values	O
to	O
be	O
missing	O
does	O
not	O
depend	O
on	O
the	O
unobserved	O
values	O
in	O
many	O
situations	O
this	O
will	O
not	O
be	O
the	O
case	O
for	O
instance	O
if	O
a	O
sensor	O
fails	O
to	O
return	O
a	O
value	O
whenever	O
the	O
quantity	O
it	O
is	O
measuring	O
exceeds	O
some	O
threshold	O
gaussian	B
mixtures	O
revisited	O
we	O
now	O
consider	O
the	O
application	O
of	O
this	O
latent	B
variable	I
view	O
of	O
em	B
to	O
the	O
specific	O
case	O
of	O
a	O
gaussian	B
mixture	B
model	I
recall	O
that	O
our	O
goal	O
is	O
to	O
maximize	O
the	O
log	O
likelihood	B
function	I
which	O
is	O
computed	O
using	O
the	O
observed	O
data	O
set	O
x	O
and	O
we	O
saw	O
that	O
this	O
was	O
more	O
difficult	O
than	O
for	O
the	O
case	O
of	O
a	O
single	O
gaussian	B
distribution	O
due	O
to	O
the	O
presence	O
of	O
the	O
summation	O
over	O
k	O
that	O
occurs	O
inside	O
the	O
logarithm	O
suppose	O
then	O
that	O
in	O
addition	O
to	O
the	O
observed	O
data	O
set	O
x	O
we	O
were	O
also	O
given	O
the	O
values	O
of	O
the	O
corresponding	O
discrete	O
variables	O
z	O
recall	O
that	O
figure	O
shows	O
a	O
complete	B
data	I
set	I
one	O
that	O
includes	O
labels	O
showing	O
which	O
component	O
generated	O
each	O
data	O
point	O
while	O
figure	O
shows	O
the	O
corresponding	O
incomplete	B
data	I
set	I
the	O
graphical	B
model	I
for	O
the	O
complete	O
data	O
is	O
shown	O
in	O
figure	O
mixture	B
models	O
and	O
em	B
figure	O
this	O
shows	O
the	O
same	O
graph	O
as	O
in	O
figure	O
except	O
that	O
we	O
now	O
suppose	O
that	O
the	O
discrete	O
variables	O
zn	O
are	O
observed	O
as	O
well	O
as	O
the	O
data	O
variables	O
xn	O
zn	O
xn	O
n	O
now	O
consider	O
the	O
problem	O
of	O
maximizing	O
the	O
likelihood	O
for	O
the	O
complete	B
data	I
set	I
z	O
from	O
and	O
this	O
likelihood	B
function	I
takes	O
the	O
form	O
px	O
z	O
k	O
n	O
k	O
kznk	O
znk	O
where	O
znk	O
denotes	O
the	O
kth	O
component	O
of	O
zn	O
taking	O
the	O
logarithm	O
we	O
obtain	O
ln	O
px	O
z	O
znk	O
k	O
lnn	O
k	O
k	O
comparison	O
with	O
the	O
log	O
likelihood	B
function	I
for	O
the	O
incomplete	O
data	O
shows	O
that	O
the	O
summation	O
over	O
k	O
and	O
the	O
logarithm	O
have	O
been	O
interchanged	O
the	O
logarithm	O
now	O
acts	O
directly	O
on	O
the	O
gaussian	B
distribution	O
which	O
itself	O
is	O
a	O
member	O
of	O
the	O
exponential	B
family	I
not	O
surprisingly	O
this	O
leads	O
to	O
a	O
much	O
simpler	O
solution	O
to	O
the	O
maximum	B
likelihood	I
problem	O
as	O
we	O
now	O
show	O
consider	O
first	O
the	O
maximization	O
with	O
respect	O
to	O
the	O
means	O
and	O
covariances	O
because	O
zn	O
is	O
a	O
k-dimensional	O
vector	O
with	O
all	O
elements	O
equal	O
to	O
except	O
for	O
a	O
single	O
element	O
having	O
the	O
value	O
the	O
complete-data	O
log	O
likelihood	B
function	I
is	O
simply	O
a	O
sum	O
of	O
k	O
independent	B
contributions	O
one	O
for	O
each	O
mixture	B
component	I
thus	O
the	O
maximization	O
with	O
respect	O
to	O
a	O
mean	B
or	O
a	O
covariance	B
is	O
exactly	O
as	O
for	O
a	O
single	O
gaussian	B
except	O
that	O
it	O
involves	O
only	O
the	O
subset	O
of	O
data	O
points	O
that	O
are	O
assigned	O
to	O
that	O
component	O
for	O
the	O
maximization	O
with	O
respect	O
to	O
the	O
mixing	O
coefficients	O
we	O
note	O
that	O
these	O
are	O
coupled	O
for	O
different	O
values	O
of	O
k	O
by	O
virtue	O
of	O
the	O
summation	O
constraint	O
again	O
this	O
can	O
be	O
enforced	O
using	O
a	O
lagrange	B
multiplier	I
as	O
before	O
and	O
leads	O
to	O
the	O
result	O
k	O
n	O
znk	O
so	O
that	O
the	O
mixing	O
coefficients	O
are	O
equal	O
to	O
the	O
fractions	O
of	O
data	O
points	O
assigned	O
to	O
the	O
corresponding	O
components	O
thus	O
we	O
see	O
that	O
the	O
complete-data	O
log	O
likelihood	B
function	I
can	O
be	O
maximized	O
trivially	O
in	O
closed	O
form	O
in	O
practice	O
however	O
we	O
do	O
not	O
have	O
values	O
for	O
the	O
latent	O
variables	O
so	O
as	O
discussed	O
earlier	O
we	O
consider	O
the	O
expectation	B
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
of	O
the	O
complete-data	O
log	O
likelihood	O
exercise	O
section	O
exercise	O
an	O
alternative	O
view	O
of	O
em	B
using	O
and	O
together	O
with	O
bayes	B
theorem	O
we	O
see	O
that	O
this	O
posterior	O
distribution	O
takes	O
the	O
form	O
pzx	O
kn	O
k	O
kznk	O
and	O
hence	O
factorizes	O
over	O
n	O
so	O
that	O
under	O
the	O
posterior	O
distribution	O
the	O
are	O
independent	B
this	O
is	O
easily	O
verified	O
by	O
inspection	O
of	O
the	O
directed	B
graph	O
in	O
figure	O
and	O
making	O
use	O
of	O
the	O
d-separation	B
criterion	O
the	O
expected	O
value	O
of	O
the	O
indicator	O
variable	O
znk	O
under	O
this	O
posterior	O
distribution	O
is	O
then	O
given	O
by	O
znk	O
kn	O
k	O
kznk	O
jn	O
j	O
j	O
kn	O
k	O
k	O
jn	O
j	O
j	O
eznk	O
znk	O
znj	O
which	O
is	O
just	O
the	O
responsibility	B
of	O
component	O
k	O
for	O
data	O
point	O
xn	O
the	O
expected	O
value	O
of	O
the	O
complete-data	O
log	O
likelihood	B
function	I
is	O
therefore	O
given	O
by	O
ezln	O
px	O
z	O
k	O
lnn	O
k	O
k	O
we	O
can	O
now	O
proceed	O
as	O
follows	O
first	O
we	O
choose	O
some	O
initial	O
values	O
for	O
the	O
parameters	O
old	O
old	O
and	O
old	O
and	O
use	O
these	O
to	O
evaluate	O
the	O
responsibilities	O
e	O
step	O
we	O
then	O
keep	O
the	O
responsibilities	O
fixed	O
and	O
maximize	O
with	O
respect	O
to	O
k	O
k	O
and	O
k	O
m	O
step	O
this	O
leads	O
to	O
closed	O
form	O
solutions	O
for	O
new	O
new	O
and	O
new	O
given	O
by	O
and	O
as	O
before	O
this	O
is	O
precisely	O
the	O
em	B
algorithm	I
for	O
gaussian	B
mixtures	O
as	O
derived	O
earlier	O
we	O
shall	O
gain	O
more	O
insight	O
into	O
the	O
role	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	B
function	I
when	O
we	O
give	O
a	O
proof	O
of	O
convergence	O
of	O
the	O
em	B
algorithm	I
in	O
section	O
relation	O
to	O
k-means	O
comparison	O
of	O
the	O
k-means	O
algorithm	O
with	O
the	O
em	B
algorithm	I
for	O
gaussian	B
mixtures	O
shows	O
that	O
there	O
is	O
a	O
close	O
similarity	O
whereas	O
the	O
k-means	O
algorithm	O
performs	O
a	O
hard	O
assignment	O
of	O
data	O
points	O
to	O
clusters	O
in	O
which	O
each	O
data	O
point	O
is	O
associated	O
uniquely	O
with	O
one	O
cluster	O
the	O
em	B
algorithm	I
makes	O
a	O
soft	B
assignment	O
based	O
on	O
the	O
posterior	O
probabilities	O
in	O
fact	O
we	O
can	O
derive	O
the	O
k-means	O
algorithm	O
as	O
a	O
particular	O
limit	O
of	O
em	B
for	O
gaussian	B
mixtures	O
as	O
follows	O
consider	O
a	O
gaussian	B
mixture	B
model	I
in	O
which	O
the	O
covariance	B
matrices	O
of	O
the	O
mixture	B
components	O
are	O
given	O
by	O
where	O
is	O
a	O
variance	B
parameter	O
that	O
is	O
shared	O
mixture	B
models	O
and	O
em	B
by	O
all	O
of	O
the	O
components	O
and	O
i	O
is	O
the	O
identity	O
matrix	O
so	O
that	O
px	O
k	O
k	O
exp	O
we	O
now	O
consider	O
the	O
em	B
algorithm	I
for	O
a	O
mixture	B
of	O
k	O
gaussians	O
of	O
this	O
form	O
in	O
which	O
we	O
treat	O
as	O
a	O
fixed	O
constant	O
instead	O
of	O
a	O
parameter	O
to	O
be	O
re-estimated	O
from	O
the	O
posterior	O
probabilities	O
or	O
responsibilities	O
for	O
a	O
particular	O
data	O
point	O
xn	O
are	O
given	O
by	O
k	O
exp	O
j	O
j	O
exp	O
if	O
we	O
consider	O
the	O
limit	O
we	O
see	O
that	O
in	O
the	O
denominator	O
the	O
term	O
for	O
which	O
is	O
smallest	O
will	O
go	O
to	O
zero	O
most	O
slowly	O
and	O
hence	O
the	O
responsibilities	O
for	O
the	O
data	O
point	O
xn	O
all	O
go	O
to	O
zero	O
except	O
for	O
term	O
j	O
for	O
which	O
the	O
responsibility	B
will	O
go	O
to	O
unity	O
note	O
that	O
this	O
holds	O
independently	O
of	O
the	O
values	O
of	O
the	O
k	O
so	O
long	O
as	O
none	O
of	O
the	O
k	O
is	O
zero	O
thus	O
in	O
this	O
limit	O
we	O
obtain	O
a	O
hard	O
assignment	O
of	O
data	O
points	O
to	O
clusters	O
just	O
as	O
in	O
the	O
k-means	O
algorithm	O
so	O
that	O
rnk	O
where	O
rnk	O
is	O
defined	O
by	O
each	O
data	O
point	O
is	O
thereby	O
assigned	O
to	O
the	O
cluster	O
having	O
the	O
closest	O
mean	B
the	O
em	B
re-estimation	O
equation	O
for	O
the	O
k	O
given	O
by	O
then	O
reduces	O
to	O
the	O
k-means	O
result	O
note	O
that	O
the	O
re-estimation	O
formula	O
for	O
the	O
mixing	O
coefficients	O
simply	O
re-sets	O
the	O
value	O
of	O
k	O
to	O
be	O
equal	O
to	O
the	O
fraction	O
of	O
data	O
points	O
assigned	O
to	O
cluster	O
k	O
although	O
these	O
parameters	O
no	O
longer	O
play	O
an	O
active	O
role	O
in	O
the	O
algorithm	O
finally	O
in	O
the	O
limit	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
given	O
by	O
exercise	O
becomes	O
ezln	O
px	O
z	O
const	O
thus	O
we	O
see	O
that	O
in	O
this	O
limit	O
maximizing	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
is	O
equivalent	O
to	O
minimizing	O
the	O
distortion	B
measure	I
j	O
for	O
the	O
k-means	O
algorithm	O
given	O
by	O
note	O
that	O
the	O
k-means	O
algorithm	O
does	O
not	O
estimate	O
the	O
covariances	O
of	O
the	O
clusters	O
but	O
only	O
the	O
cluster	O
means	O
a	O
hard-assignment	O
version	O
of	O
the	O
gaussian	B
mixture	B
model	I
with	O
general	O
covariance	B
matrices	O
known	O
as	O
the	O
elliptical	B
k-means	I
algorithm	O
has	O
been	O
considered	O
by	O
sung	O
and	O
poggio	O
mixtures	O
of	O
bernoulli	B
distributions	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
focussed	O
on	O
distributions	O
over	O
continuous	O
variables	O
described	O
by	O
mixtures	O
of	O
gaussians	O
as	O
a	O
further	O
example	O
of	O
mixture	B
modelling	O
and	O
to	O
illustrate	O
the	O
em	B
algorithm	I
in	O
a	O
different	O
context	O
we	O
now	O
discuss	O
mixtures	O
of	O
discrete	O
binary	O
variables	O
described	O
by	O
bernoulli	B
distributions	O
this	O
model	O
is	O
also	O
known	O
as	O
latent	B
class	I
analysis	I
and	O
henry	O
mclachlan	O
and	O
peel	O
as	O
well	O
as	O
being	O
of	O
practical	O
importance	O
in	O
its	O
own	O
right	O
our	O
discussion	O
of	O
bernoulli	B
mixtures	O
will	O
also	O
lay	O
the	O
foundation	O
for	O
a	O
consideration	O
of	O
hidden	O
markov	O
models	O
over	O
discrete	O
variables	O
section	O
an	O
alternative	O
view	O
of	O
em	B
consider	O
a	O
set	O
of	O
d	O
binary	O
variables	O
xi	O
where	O
i	O
d	O
each	O
of	O
which	O
is	O
governed	O
by	O
a	O
bernoulli	B
distribution	I
with	O
parameter	O
i	O
so	O
that	O
px	O
i	O
xi	O
xi	O
where	O
x	O
xdt	O
and	O
dt	O
we	O
see	O
that	O
the	O
individual	O
variables	O
xi	O
are	O
independent	B
given	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
distribution	O
are	O
easily	O
seen	O
to	O
be	O
ex	O
covx	O
diag	O
i	O
now	O
let	O
us	O
consider	O
a	O
finite	O
mixture	B
of	O
these	O
distributions	O
given	O
by	O
px	O
kpx	O
k	O
where	O
k	O
k	O
and	O
xi	O
xi	O
exercise	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
mixture	B
distribution	I
are	O
given	O
by	O
px	O
k	O
k	O
k	O
ex	O
exext	O
covx	O
k	O
k	O
t	O
k	O
k	O
where	O
k	O
diag	O
ki	O
because	O
the	O
covariance	B
matrix	O
covx	O
is	O
no	O
longer	O
diagonal	B
the	O
mixture	B
distribution	I
can	O
capture	O
correlations	O
between	O
the	O
variables	O
unlike	O
a	O
single	O
bernoulli	B
distribution	I
if	O
we	O
are	O
given	O
a	O
data	O
set	O
x	O
xn	O
then	O
the	O
log	O
likelihood	B
function	I
for	O
this	O
model	O
is	O
given	O
by	O
ln	O
px	O
kpxn	O
k	O
ln	O
again	O
we	O
see	O
the	O
appearance	O
of	O
the	O
summation	O
inside	O
the	O
logarithm	O
so	O
that	O
the	O
maximum	B
likelihood	I
solution	O
no	O
longer	O
has	O
closed	O
form	O
we	O
now	O
derive	O
the	O
em	B
algorithm	I
for	O
maximizing	O
the	O
likelihood	B
function	I
for	O
the	O
mixture	B
of	O
bernoulli	B
distributions	O
to	O
do	O
this	O
we	O
first	O
introduce	O
an	O
explicit	O
latent	O
mixture	B
models	O
and	O
em	B
exercise	O
variable	O
z	O
associated	O
with	O
each	O
instance	O
of	O
x	O
as	O
in	O
the	O
case	O
of	O
the	O
gaussian	B
mixture	B
z	O
zkt	O
is	O
a	O
binary	O
k-dimensional	O
variable	O
having	O
a	O
single	O
component	O
equal	O
to	O
with	O
all	O
other	O
components	O
equal	O
to	O
we	O
can	O
then	O
write	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
the	O
latent	B
variable	I
as	O
pxz	O
px	O
kzk	O
zk	O
k	O
while	O
the	O
prior	B
distribution	O
for	O
the	O
latent	O
variables	O
is	O
the	O
same	O
as	O
for	O
the	O
mixture	B
of	I
gaussians	I
model	O
so	O
that	O
pz	O
if	O
we	O
form	O
the	O
product	O
of	O
pxz	O
and	O
pz	O
and	O
then	O
marginalize	O
over	O
z	O
then	O
we	O
recover	O
in	O
order	O
to	O
derive	O
the	O
em	B
algorithm	I
we	O
first	O
write	O
down	O
the	O
complete-data	O
log	O
likelihood	B
function	I
which	O
is	O
given	O
by	O
ln	O
px	O
z	O
znk	O
ln	O
k	O
ln	O
ki	O
xni	O
ki	O
where	O
x	O
and	O
z	O
next	O
we	O
take	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
to	O
give	O
ezln	O
px	O
z	O
ln	O
k	O
ln	O
ki	O
xni	O
ki	O
where	O
eznk	O
is	O
the	O
posterior	B
probability	B
or	O
responsibility	B
of	O
component	O
k	O
given	O
data	O
point	O
xn	O
in	O
the	O
e	O
step	O
these	O
responsibilities	O
are	O
evaluated	O
using	O
bayes	B
theorem	O
which	O
takes	O
the	O
form	O
eznk	O
znk	O
znk	O
kpxn	O
kznk	O
jpxn	O
j	O
kpxn	O
k	O
jpxn	O
j	O
znj	O
an	O
alternative	O
view	O
of	O
em	B
if	O
we	O
consider	O
the	O
sum	O
over	O
n	O
in	O
we	O
see	O
that	O
the	O
responsibilities	O
enter	O
only	O
through	O
two	O
terms	O
which	O
can	O
be	O
written	O
as	O
nk	O
nk	O
xk	O
where	O
nk	O
is	O
the	O
effective	O
number	O
of	O
data	O
points	O
associated	O
with	O
component	O
k	O
in	O
the	O
m	O
step	O
we	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
parameters	O
k	O
and	O
if	O
we	O
set	O
the	O
derivative	B
of	O
with	O
respect	O
to	O
k	O
equal	O
to	O
zero	O
and	O
rearrange	O
the	O
terms	O
we	O
obtain	O
exercise	O
exercise	O
exercise	O
section	O
k	O
xk	O
we	O
see	O
that	O
this	O
sets	O
the	O
mean	B
of	O
component	O
k	O
equal	O
to	O
a	O
weighted	O
mean	B
of	O
the	O
data	O
with	O
weighting	O
coefficients	O
given	O
by	O
the	O
responsibilities	O
that	O
component	O
k	O
takes	O
for	O
data	O
points	O
for	O
the	O
maximization	O
with	O
respect	O
to	O
k	O
we	O
need	O
to	O
introduce	O
a	O
k	O
k	O
following	O
analogous	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
constraint	O
steps	O
to	O
those	O
used	O
for	O
the	O
mixture	B
of	I
gaussians	I
we	O
then	O
obtain	O
k	O
nk	O
n	O
which	O
represents	O
the	O
intuitively	O
reasonable	O
result	O
that	O
the	O
mixing	B
coefficient	I
for	O
component	O
k	O
is	O
given	O
by	O
the	O
effective	O
fraction	O
of	O
points	O
in	O
the	O
data	O
set	O
explained	O
by	O
that	O
component	O
note	O
that	O
in	O
contrast	O
to	O
the	O
mixture	B
of	I
gaussians	I
there	O
are	O
no	O
singularities	B
in	O
which	O
the	O
likelihood	B
function	I
goes	O
to	O
infinity	O
this	O
can	O
be	O
seen	O
by	O
noting	O
that	O
the	O
likelihood	B
function	I
is	O
bounded	O
above	O
because	O
pxn	O
k	O
there	O
exist	O
singularities	B
at	O
which	O
the	O
likelihood	B
function	I
goes	O
to	O
zero	O
but	O
these	O
will	O
not	O
be	O
found	O
by	O
em	B
provided	O
it	O
is	O
not	O
initialized	O
to	O
a	O
pathological	O
starting	O
point	O
because	O
the	O
em	B
algorithm	I
always	O
increases	O
the	O
value	O
of	O
the	O
likelihood	B
function	I
until	O
a	O
local	B
maximum	O
is	O
found	O
we	O
illustrate	O
the	O
bernoulli	B
mixture	B
model	I
in	O
figure	O
by	O
using	O
it	O
to	O
model	O
handwritten	O
digits	O
here	O
the	O
digit	O
images	O
have	O
been	O
turned	O
into	O
binary	O
vectors	O
by	O
setting	O
all	O
elements	O
whose	O
values	O
exceed	O
to	O
and	O
setting	O
the	O
remaining	O
elements	O
to	O
we	O
now	O
fit	O
a	O
data	O
set	O
of	O
n	O
such	O
digits	O
comprising	O
the	O
digits	O
and	O
with	O
a	O
mixture	B
of	O
k	O
bernoulli	B
distributions	O
by	O
running	O
iterations	O
of	O
the	O
em	B
algorithm	I
the	O
mixing	O
coefficients	O
were	O
initialized	O
to	O
k	O
and	O
the	O
parameters	O
kj	O
were	O
set	O
to	O
random	O
values	O
chosen	O
uniformly	O
in	O
j	O
kj	O
the	O
range	O
and	O
then	O
normalized	O
to	O
satisfy	O
the	O
constraint	O
that	O
we	O
see	O
that	O
a	O
mixture	B
of	O
bernoulli	B
distributions	O
is	O
able	O
to	O
find	O
the	O
three	O
clusters	O
in	O
the	O
data	O
set	O
corresponding	O
to	O
the	O
different	O
digits	O
the	O
conjugate	B
prior	B
for	O
the	O
parameters	O
of	O
a	O
bernoulli	B
distribution	I
is	O
given	O
by	O
the	O
beta	B
distribution	I
and	O
we	O
have	O
seen	O
that	O
a	O
beta	O
prior	B
is	O
equivalent	O
to	O
introducing	O
mixture	B
models	O
and	O
em	B
figure	O
illustration	O
of	O
the	O
bernoulli	B
mixture	B
model	I
in	O
which	O
the	O
top	O
row	O
shows	O
examples	O
from	O
the	O
digits	O
data	O
set	O
after	O
converting	O
the	O
pixel	O
values	O
from	O
grey	O
scale	O
to	O
binary	O
using	O
a	O
threshold	O
of	O
on	O
the	O
bottom	O
row	O
the	O
first	O
three	O
images	O
show	O
the	O
parameters	O
ki	O
for	O
each	O
of	O
the	O
three	O
components	O
in	O
the	O
mixture	B
model	I
as	O
a	O
comparison	O
we	O
also	O
fit	O
the	O
same	O
data	O
set	O
using	O
a	O
single	O
multivariate	O
bernoulli	B
distribution	I
again	O
using	O
maximum	B
likelihood	I
this	O
amounts	O
to	O
simply	O
averaging	O
the	O
counts	O
in	O
each	O
pixel	O
and	O
is	O
shown	O
by	O
the	O
right-most	O
image	O
on	O
the	O
bottom	O
row	O
section	O
exercise	O
exercise	O
additional	O
effective	O
observations	O
of	O
x	O
we	O
can	O
similarly	O
introduce	O
priors	O
into	O
the	O
bernoulli	B
mixture	B
model	I
and	O
use	O
em	B
to	O
maximize	O
the	O
posterior	B
probability	B
distributions	O
it	O
is	O
straightforward	O
to	O
extend	O
the	O
analysis	O
of	O
bernoulli	B
mixtures	O
to	O
the	O
case	O
of	O
multinomial	O
binary	O
variables	O
having	O
m	O
states	O
by	O
making	O
use	O
of	O
the	O
discrete	O
distribution	O
again	O
we	O
can	O
introduce	O
dirichlet	B
priors	O
over	O
the	O
model	O
parameters	O
if	O
desired	O
em	B
for	O
bayesian	B
linear	B
regression	B
as	O
a	O
third	O
example	O
of	O
the	O
application	O
of	O
em	B
we	O
return	O
to	O
the	O
evidence	B
approximation	I
for	O
bayesian	B
linear	B
regression	B
in	O
section	O
we	O
obtained	O
the	O
reestimation	O
equations	O
for	O
the	O
hyperparameters	O
and	O
by	O
evaluation	O
of	O
the	O
evidence	O
and	O
then	O
setting	O
the	O
derivatives	O
of	O
the	O
resulting	O
expression	O
to	O
zero	O
we	O
now	O
turn	O
to	O
an	O
alternative	O
approach	O
for	O
finding	O
and	O
based	O
on	O
the	O
em	B
algorithm	I
recall	O
that	O
our	O
goal	O
is	O
to	O
maximize	O
the	O
evidence	B
function	I
pt	O
given	O
by	O
with	O
respect	O
to	O
and	O
because	O
the	O
parameter	O
vector	O
w	O
is	O
marginalized	O
out	O
we	O
can	O
regard	O
it	O
as	O
a	O
latent	B
variable	I
and	O
hence	O
we	O
can	O
optimize	O
this	O
marginal	B
likelihood	B
function	I
using	O
em	B
in	O
the	O
e	O
step	O
we	O
compute	O
the	O
posterior	O
distribution	O
of	O
w	O
given	O
the	O
current	O
setting	O
of	O
the	O
parameters	O
and	O
and	O
then	O
use	O
this	O
to	O
find	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
in	O
the	O
m	O
step	O
we	O
maximize	O
this	O
quantity	O
with	O
respect	O
to	O
and	O
we	O
have	O
already	O
derived	O
the	O
posterior	O
distribution	O
of	O
w	O
because	O
this	O
is	O
given	O
by	O
the	O
complete-data	O
log	O
likelihood	B
function	I
is	O
then	O
given	O
by	O
ln	O
pt	O
w	O
ln	O
ptw	O
ln	O
pw	O
an	O
alternative	O
view	O
of	O
em	B
where	O
the	O
likelihood	O
ptw	O
and	O
the	O
prior	B
pw	O
are	O
given	O
by	O
and	O
respectively	O
and	O
yx	O
w	O
is	O
given	O
by	O
taking	O
the	O
expectation	B
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
w	O
then	O
gives	O
e	O
pt	O
w	O
m	O
n	O
ln	O
wt	O
e	O
wtw	O
ln	O
e	O
exercise	O
setting	O
the	O
derivatives	O
with	O
respect	O
to	O
to	O
zero	O
we	O
obtain	O
the	O
m	O
step	O
re-estimation	O
equation	O
m	O
e	O
m	O
n	O
mn	O
trsn	O
mt	O
exercise	O
an	O
analogous	O
result	O
holds	O
for	O
note	O
that	O
this	O
re-estimation	O
equation	O
takes	O
a	O
slightly	O
different	O
form	O
from	O
the	O
corresponding	O
result	O
derived	O
by	O
direct	O
evaluation	O
of	O
the	O
evidence	B
function	I
however	O
they	O
each	O
involve	O
computation	O
and	O
inversion	O
eigen	O
decomposition	O
of	O
an	O
m	O
m	O
matrix	O
and	O
hence	O
will	O
have	O
comparable	O
computational	O
cost	O
per	O
iteration	O
these	O
two	O
approaches	O
to	O
determining	O
should	O
of	O
course	O
converge	O
to	O
the	O
same	O
result	O
they	O
find	O
the	O
same	O
local	B
maximum	O
of	O
the	O
evidence	B
function	I
this	O
can	O
be	O
verified	O
by	O
first	O
noting	O
that	O
the	O
quantity	O
is	O
defined	O
by	O
m	O
m	O
trsn	O
i	O
at	O
a	O
stationary	B
point	O
of	O
the	O
evidence	B
function	I
the	O
re-estimation	O
equation	O
will	O
be	O
self-consistently	O
satisfied	O
and	O
hence	O
we	O
can	O
substitute	O
for	O
to	O
give	O
n	O
mn	O
m	O
trsn	O
mt	O
and	O
solving	O
for	O
we	O
obtain	O
which	O
is	O
precisely	O
the	O
em	B
re-estimation	O
equation	O
as	O
a	O
final	O
example	O
we	O
consider	O
a	O
closely	O
related	O
model	O
namely	O
the	O
relevance	B
vector	I
machine	I
for	B
regression	B
discussed	O
in	O
section	O
there	O
we	O
used	O
direct	O
maximization	O
of	O
the	O
marginal	B
likelihood	I
to	O
derive	O
re-estimation	O
equations	O
for	O
the	O
hyperparameters	O
and	O
here	O
we	O
consider	O
an	O
alternative	O
approach	O
in	O
which	O
we	O
view	O
the	O
weight	B
vector	I
w	O
as	O
a	O
latent	B
variable	I
and	O
apply	O
the	O
em	B
algorithm	I
the	O
e	O
step	O
involves	O
finding	O
the	O
posterior	O
distribution	O
over	O
the	O
weights	O
and	O
this	O
is	O
given	O
by	O
in	O
the	O
m	O
step	O
we	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
which	O
is	O
defined	O
by	O
ew	O
ptx	O
w	O
exercise	O
where	O
the	O
expectation	B
is	O
taken	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
computed	O
using	O
the	O
old	O
parameter	O
values	O
to	O
compute	O
the	O
new	O
parameter	O
values	O
we	O
maximize	O
with	O
respect	O
to	O
and	O
to	O
give	O
mixture	B
models	O
and	O
em	B
new	O
i	O
new	O
i	O
ii	O
n	O
i	O
i	O
exercise	O
these	O
re-estimation	O
equations	O
are	O
formally	O
equivalent	O
to	O
those	O
obtained	O
by	O
direct	O
maxmization	O
the	O
em	B
algorithm	I
in	O
general	O
the	O
expectation	B
maximization	I
algorithm	O
or	O
em	B
algorithm	I
is	O
a	O
general	O
technique	O
for	O
finding	O
maximum	B
likelihood	I
solutions	O
for	O
probabilistic	O
models	O
having	O
latent	O
variables	O
et	O
al	O
mclachlan	O
and	O
krishnan	O
here	O
we	O
give	O
a	O
very	O
general	O
treatment	O
of	O
the	O
em	B
algorithm	I
and	O
in	O
the	O
process	O
provide	O
a	O
proof	O
that	O
the	O
em	B
algorithm	I
derived	O
heuristically	O
in	O
sections	O
and	O
for	O
gaussian	B
mixtures	O
does	O
indeed	O
maximize	O
the	O
likelihood	B
function	I
and	O
tusnady	O
hathaway	O
neal	O
and	O
hinton	O
our	O
discussion	O
will	O
also	O
form	O
the	O
basis	O
for	O
the	O
derivation	O
of	O
the	O
variational	B
inference	B
framework	O
section	O
consider	O
a	O
probabilistic	O
model	O
in	O
which	O
we	O
collectively	O
denote	O
all	O
of	O
the	O
observed	O
variables	O
by	O
x	O
and	O
all	O
of	O
the	O
hidden	O
variables	O
by	O
z	O
the	O
joint	O
distribution	O
px	O
z	O
is	O
governed	O
by	O
a	O
set	O
of	O
parameters	O
denoted	O
our	O
goal	O
is	O
to	O
maximize	O
the	O
likelihood	B
function	I
that	O
is	O
given	O
by	O
px	O
px	O
z	O
z	O
here	O
we	O
are	O
assuming	O
z	O
is	O
discrete	O
although	O
the	O
discussion	O
is	O
identical	O
if	O
z	O
comprises	O
continuous	O
variables	O
or	O
a	O
combination	O
of	O
discrete	O
and	O
continuous	O
variables	O
with	O
summation	O
replaced	O
by	O
integration	O
as	O
appropriate	O
we	O
shall	O
suppose	O
that	O
direct	O
optimization	O
of	O
px	O
is	O
difficult	O
but	O
that	O
optimization	O
of	O
the	O
complete-data	O
likelihood	B
function	I
px	O
z	O
is	O
significantly	O
easier	O
next	O
we	O
introduce	O
a	O
distribution	O
qz	O
defined	O
over	O
the	O
latent	O
variables	O
and	O
we	O
observe	O
that	O
for	O
any	O
choice	O
of	O
qz	O
the	O
following	O
decomposition	O
holds	O
ln	O
px	O
lq	O
where	O
we	O
have	O
defined	O
lq	O
qz	O
ln	O
qz	O
ln	O
z	O
z	O
px	O
z	O
qz	O
pzx	O
qz	O
note	O
that	O
lq	O
is	O
a	O
functional	B
appendix	O
d	O
for	O
a	O
discussion	O
of	O
functionals	O
of	O
the	O
distribution	O
qz	O
and	O
a	O
function	O
of	O
the	O
parameters	O
it	O
is	O
worth	O
studying	O
the	O
em	B
algorithm	I
in	O
general	O
figure	O
illustration	O
of	O
the	O
decomposition	O
given	O
by	O
which	O
holds	O
for	O
any	O
choice	O
of	O
distribution	O
qz	O
because	O
the	O
kullback-leibler	B
divergence	I
satisfies	O
we	O
see	O
that	O
the	O
quantity	O
lq	O
is	O
a	O
lower	B
bound	I
on	O
the	O
log	O
likelihood	B
function	I
ln	O
px	O
klqp	O
lq	O
ln	O
px	O
exercise	O
section	O
carefully	O
the	O
forms	O
of	O
the	O
expressions	O
and	O
and	O
in	O
particular	O
noting	O
that	O
they	O
differ	O
in	O
sign	O
and	O
also	O
that	O
lq	O
contains	O
the	O
joint	O
distribution	O
of	O
x	O
and	O
z	O
while	O
contains	O
the	O
conditional	B
distribution	O
of	O
z	O
given	O
x	O
to	O
verify	O
the	O
decomposition	O
we	O
first	O
make	O
use	O
of	O
the	O
product	B
rule	I
of	I
probability	B
to	O
give	O
ln	O
px	O
z	O
ln	O
pzx	O
ln	O
px	O
which	O
we	O
then	O
substitute	O
into	O
the	O
expression	O
for	O
lq	O
this	O
gives	O
rise	O
to	O
two	O
terms	O
one	O
of	O
which	O
cancels	O
while	O
the	O
other	O
gives	O
the	O
required	O
log	O
likelihood	O
ln	O
px	O
after	O
noting	O
that	O
qz	O
is	O
a	O
normalized	O
distribution	O
that	O
sums	O
to	O
from	O
we	O
see	O
that	O
is	O
the	O
kullback-leibler	B
divergence	I
between	O
qz	O
and	O
the	O
posterior	O
distribution	O
pzx	O
recall	O
that	O
the	O
kullback-leibler	B
divergence	I
satisfies	O
with	O
equality	O
if	O
and	O
only	O
if	O
qz	O
pzx	O
it	O
therefore	O
follows	O
from	O
that	O
lq	O
ln	O
px	O
in	O
other	O
words	O
that	O
lq	O
is	O
a	O
lower	B
bound	I
on	O
ln	O
px	O
the	O
decomposition	O
is	O
illustrated	O
in	O
figure	O
the	O
em	B
algorithm	I
is	O
a	O
two-stage	O
iterative	O
optimization	O
technique	O
for	O
finding	O
maximum	B
likelihood	I
solutions	O
we	O
can	O
use	O
the	O
decomposition	O
to	O
define	O
the	O
em	B
algorithm	I
and	O
to	O
demonstrate	O
that	O
it	O
does	O
indeed	O
maximize	O
the	O
log	O
likelihood	O
suppose	O
that	O
the	O
current	O
value	O
of	O
the	O
parameter	O
vector	O
is	O
old	O
in	O
the	O
e	O
step	O
the	O
lower	B
bound	I
lq	O
old	O
is	O
maximized	O
with	O
respect	O
to	O
qz	O
while	O
holding	O
old	O
fixed	O
the	O
solution	O
to	O
this	O
maximization	O
problem	O
is	O
easily	O
seen	O
by	O
noting	O
that	O
the	O
value	O
of	O
ln	O
px	O
old	O
does	O
not	O
depend	O
on	O
qz	O
and	O
so	O
the	O
largest	O
value	O
of	O
lq	O
old	O
will	O
occur	O
when	O
the	O
kullback-leibler	B
divergence	I
vanishes	O
in	O
other	O
words	O
when	O
qz	O
is	O
equal	O
to	O
the	O
posterior	O
distribution	O
pzx	O
old	O
in	O
this	O
case	O
the	O
lower	B
bound	I
will	O
equal	O
the	O
log	O
likelihood	O
as	O
illustrated	O
in	O
figure	O
in	O
the	O
subsequent	O
m	O
step	O
the	O
distribution	O
qz	O
is	O
held	O
fixed	O
and	O
the	O
lower	B
bound	I
lq	O
is	O
maximized	O
with	O
respect	O
to	O
to	O
give	O
some	O
new	O
value	O
new	O
this	O
will	O
cause	O
the	O
lower	B
bound	I
l	O
to	O
increase	O
it	O
is	O
already	O
at	O
a	O
maximum	O
which	O
will	O
necessarily	O
cause	O
the	O
corresponding	O
log	O
likelihood	B
function	I
to	O
increase	O
because	O
the	O
distribution	O
q	O
is	O
determined	O
using	O
the	O
old	O
parameter	O
values	O
rather	O
than	O
the	O
new	O
values	O
and	O
is	O
held	O
fixed	O
during	O
the	O
m	O
step	O
it	O
will	O
not	O
equal	O
the	O
new	O
posterior	O
distribution	O
pzx	O
new	O
and	O
hence	O
there	O
will	O
be	O
a	O
nonzero	O
kl	O
divergence	O
the	O
increase	O
in	O
the	O
log	O
likelihood	B
function	I
is	O
therefore	O
greater	O
than	O
the	O
increase	O
in	O
the	O
lower	B
bound	I
as	O
mixture	B
models	O
and	O
em	B
klqp	O
figure	O
illustration	O
of	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
the	O
q	O
distribution	O
is	O
set	O
equal	O
to	O
the	O
posterior	O
distribution	O
for	O
the	O
current	O
parameter	O
values	O
old	O
causing	O
the	O
lower	B
bound	I
to	O
move	O
up	O
to	O
the	O
same	O
value	O
as	O
the	O
log	O
likelihood	B
function	I
with	O
the	O
kl	O
divergence	O
vanishing	O
lq	O
old	O
ln	O
px	O
old	O
shown	O
in	O
figure	O
if	O
we	O
substitute	O
qz	O
pzx	O
old	O
into	O
we	O
see	O
that	O
after	O
the	O
e	O
step	O
the	O
lower	B
bound	I
takes	O
the	O
form	O
lq	O
pzx	O
old	O
ln	O
px	O
z	O
pzx	O
old	O
ln	O
pzx	O
old	O
z	O
q	O
old	O
const	O
z	O
where	O
the	O
constant	O
is	O
simply	O
the	O
negative	O
entropy	B
of	O
the	O
q	O
distribution	O
and	O
is	O
therefore	O
independent	B
of	O
thus	O
in	O
the	O
m	O
step	O
the	O
quantity	O
that	O
is	O
being	O
maximized	O
is	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
as	O
we	O
saw	O
earlier	O
in	O
the	O
case	O
of	O
mixtures	O
of	O
gaussians	O
note	O
that	O
the	O
variable	O
over	O
which	O
we	O
are	O
optimizing	O
appears	O
only	O
inside	O
the	O
logarithm	O
if	O
the	O
joint	O
distribution	O
pz	O
x	O
comprises	O
a	O
member	O
of	O
the	O
exponential	B
family	I
or	O
a	O
product	O
of	O
such	O
members	O
then	O
we	O
see	O
that	O
the	O
logarithm	O
will	O
cancel	O
the	O
exponential	O
and	O
lead	O
to	O
an	O
m	O
step	O
that	O
will	O
be	O
typically	O
much	O
simpler	O
than	O
the	O
maximization	O
of	O
the	O
corresponding	O
incomplete-data	O
log	O
likelihood	B
function	I
px	O
the	O
operation	O
of	O
the	O
em	B
algorithm	I
can	O
also	O
be	O
viewed	O
in	O
the	O
space	O
of	O
parameters	O
as	O
illustrated	O
schematically	O
in	O
figure	O
here	O
the	O
red	O
curve	O
depicts	O
the	O
klqp	O
figure	O
illustration	O
of	O
the	O
m	O
step	O
of	O
the	O
em	B
the	O
distribution	O
qz	O
algorithm	O
is	O
held	O
fixed	O
and	O
the	O
lower	B
bound	I
lq	O
is	O
maximized	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
to	O
give	O
a	O
revised	O
value	O
new	O
because	O
the	O
kl	O
divergence	O
is	O
nonnegative	O
this	O
causes	O
the	O
log	O
likelihood	O
ln	O
px	O
to	O
increase	O
by	O
at	O
least	O
as	O
much	O
as	O
the	O
lower	B
bound	I
does	O
lq	O
new	O
ln	O
px	O
new	O
figure	O
the	O
em	B
algorithm	I
involves	O
alternately	O
computing	O
a	O
lower	B
bound	I
on	O
the	O
log	O
likelihood	O
for	O
the	O
current	O
parameter	O
values	O
and	O
then	O
maximizing	O
this	O
bound	O
to	O
obtain	O
the	O
new	O
parameter	O
values	O
see	O
the	O
text	O
for	O
a	O
full	O
discussion	O
the	O
em	B
algorithm	I
in	O
general	O
ln	O
px	O
l	O
old	O
new	O
exercise	O
complete	O
data	O
log	O
likelihood	B
function	I
whose	O
value	O
we	O
wish	O
to	O
maximize	O
we	O
start	O
with	O
some	O
initial	O
parameter	O
value	O
old	O
and	O
in	O
the	O
first	O
e	O
step	O
we	O
evaluate	O
the	O
posterior	O
distribution	O
over	O
latent	O
variables	O
which	O
gives	O
rise	O
to	O
a	O
lower	B
bound	I
l	O
whose	O
value	O
equals	O
the	O
log	O
likelihood	O
at	O
as	O
shown	O
by	O
the	O
blue	O
curve	O
note	O
that	O
the	O
bound	O
makes	O
a	O
tangential	O
contact	O
with	O
the	O
log	O
likelihood	O
at	O
so	O
that	O
both	O
curves	O
have	O
the	O
same	O
gradient	O
this	O
bound	O
is	O
a	O
convex	B
function	I
having	O
a	O
unique	O
maximum	O
mixture	B
components	O
from	O
the	O
exponential	B
family	I
in	O
the	O
m	O
step	O
the	O
bound	O
is	O
maximized	O
giving	O
the	O
value	O
which	O
gives	O
a	O
larger	O
value	O
of	O
log	O
likelihood	O
than	O
the	O
subsequent	O
e	O
step	O
then	O
constructs	O
a	O
bound	O
that	O
is	O
tangential	O
at	O
as	O
shown	O
by	O
the	O
green	O
curve	O
for	O
the	O
particular	O
case	O
of	O
an	O
independent	B
identically	I
distributed	I
data	O
set	O
x	O
will	O
comprise	O
n	O
data	O
points	O
while	O
z	O
will	O
comprise	O
n	O
corresponding	O
latent	O
variables	O
where	O
n	O
n	O
from	O
the	O
independence	O
assumption	O
we	O
have	O
n	O
pxn	O
zn	O
and	O
by	O
marginalizing	O
over	O
the	O
we	O
have	O
px	O
px	O
z	O
n	O
pxn	O
using	O
the	O
sum	O
and	O
product	O
rules	O
we	O
see	O
that	O
the	O
posterior	B
probability	B
that	O
is	O
evaluated	O
in	O
the	O
e	O
step	O
takes	O
the	O
form	O
pzx	O
px	O
z	O
px	O
z	O
z	O
z	O
pxn	O
zn	O
pxn	O
zn	O
pznxn	O
and	O
so	O
the	O
posterior	O
distribution	O
also	O
factorizes	O
with	O
respect	O
to	O
n	O
in	O
the	O
case	O
of	O
the	O
gaussian	B
mixture	B
model	I
this	O
simply	O
says	O
that	O
the	O
responsibility	B
that	O
each	O
of	O
the	O
mixture	B
components	O
takes	O
for	O
a	O
particular	O
data	O
point	O
xn	O
depends	O
only	O
on	O
the	O
value	O
of	O
xn	O
and	O
on	O
the	O
parameters	O
of	O
the	O
mixture	B
components	O
not	O
on	O
the	O
values	O
of	O
the	O
other	O
data	O
points	O
we	O
have	O
seen	O
that	O
both	O
the	O
e	O
and	O
the	O
m	O
steps	O
of	O
the	O
em	B
algorithm	I
are	O
increasing	O
the	O
value	O
of	O
a	O
well-defined	O
bound	O
on	O
the	O
log	O
likelihood	B
function	I
and	O
that	O
the	O
mixture	B
models	O
and	O
em	B
complete	O
em	B
cycle	O
will	O
change	O
the	O
model	O
parameters	O
in	O
such	O
a	O
way	O
as	O
to	O
cause	O
the	O
log	O
likelihood	O
to	O
increase	O
it	O
is	O
already	O
at	O
a	O
maximum	O
in	O
which	O
case	O
the	O
parameters	O
remain	O
unchanged	O
we	O
can	O
also	O
use	O
the	O
em	B
algorithm	I
to	O
maximize	O
the	O
posterior	O
distribution	O
p	O
for	O
models	O
in	O
which	O
we	O
have	O
introduced	O
a	O
prior	B
p	O
over	O
the	O
parameters	O
to	O
see	O
this	O
we	O
note	O
that	O
as	O
a	O
function	O
of	O
we	O
have	O
p	O
p	O
xpx	O
and	O
so	O
ln	O
p	O
ln	O
p	O
x	O
ln	O
px	O
making	O
use	O
of	O
the	O
decomposition	O
we	O
have	O
ln	O
p	O
lq	O
ln	O
p	O
ln	O
px	O
lq	O
ln	O
p	O
ln	O
px	O
where	O
ln	O
px	O
is	O
a	O
constant	O
we	O
can	O
again	O
optimize	O
the	O
right-hand	O
side	O
alternately	O
with	O
respect	O
to	O
q	O
and	O
the	O
optimization	O
with	O
respect	O
to	O
q	O
gives	O
rise	O
to	O
the	O
same	O
estep	O
equations	O
as	O
for	O
the	O
standard	O
em	B
algorithm	I
because	O
q	O
only	O
appears	O
in	O
lq	O
the	O
m-step	O
equations	O
are	O
modified	O
through	O
the	O
introduction	O
of	O
the	O
prior	B
term	O
ln	O
p	O
which	O
typically	O
requires	O
only	O
a	O
small	O
modification	O
to	O
the	O
standard	O
maximum	B
likelihood	I
m-step	O
equations	O
the	O
em	B
algorithm	I
breaks	O
down	O
the	O
potentially	O
difficult	O
problem	O
of	O
maximizing	O
the	O
likelihood	B
function	I
into	O
two	O
stages	O
the	O
e	O
step	O
and	O
the	O
m	O
step	O
each	O
of	O
which	O
will	O
often	O
prove	O
simpler	O
to	O
implement	O
nevertheless	O
for	O
complex	O
models	O
it	O
may	O
be	O
the	O
case	O
that	O
either	O
the	O
e	O
step	O
or	O
the	O
m	O
step	O
or	O
indeed	O
both	O
remain	O
intractable	O
this	O
leads	O
to	O
two	O
possible	O
extensions	O
of	O
the	O
em	B
algorithm	I
as	O
follows	O
the	O
generalized	B
em	B
or	O
gem	O
algorithm	O
addresses	O
the	O
problem	O
of	O
an	O
intractable	O
m	O
step	O
instead	O
of	O
aiming	O
to	O
maximize	O
lq	O
with	O
respect	O
to	O
it	O
seeks	O
instead	O
to	O
change	O
the	O
parameters	O
in	O
such	O
a	O
way	O
as	O
to	O
increase	O
its	O
value	O
again	O
because	O
lq	O
is	O
a	O
lower	B
bound	I
on	O
the	O
log	O
likelihood	B
function	I
each	O
complete	O
em	B
cycle	O
of	O
the	O
gem	O
algorithm	O
is	O
guaranteed	O
to	O
increase	O
the	O
value	O
of	O
the	O
log	O
likelihood	O
the	O
parameters	O
already	O
correspond	O
to	O
a	O
local	B
maximum	O
one	O
way	O
to	O
exploit	O
the	O
gem	O
approach	O
would	O
be	O
to	O
use	O
one	O
of	O
the	O
nonlinear	O
optimization	O
strategies	O
such	O
as	O
the	O
conjugate	B
gradients	O
algorithm	O
during	O
the	O
m	O
step	O
another	O
form	O
of	O
gem	O
algorithm	O
known	O
as	O
the	O
expectation	B
conditional	B
maximization	I
or	O
ecm	O
algorithm	O
involves	O
making	O
several	O
constrained	O
optimizations	O
within	O
each	O
m	O
step	O
and	O
rubin	O
for	O
instance	O
the	O
parameters	O
might	O
be	O
partitioned	B
into	O
groups	O
and	O
the	O
m	O
step	O
is	O
broken	O
down	O
into	O
multiple	O
steps	O
each	O
of	O
which	O
involves	O
optimizing	O
one	O
of	O
the	O
subset	O
with	O
the	O
remainder	O
held	O
fixed	O
we	O
can	O
similarly	O
generalize	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
by	O
performing	O
a	O
partial	O
rather	O
than	O
complete	O
optimization	O
of	O
lq	O
with	O
respect	O
to	O
qz	O
and	O
hinton	O
as	O
we	O
have	O
seen	O
for	O
any	O
given	O
value	O
of	O
there	O
is	O
a	O
unique	O
maximum	O
of	O
lq	O
with	O
respect	O
to	O
qz	O
that	O
corresponds	O
to	O
the	O
posterior	O
distribution	O
q	O
pzx	O
and	O
that	O
for	O
this	O
choice	O
of	O
qz	O
the	O
bound	O
lq	O
is	O
equal	O
to	O
the	O
log	O
likelihood	B
function	I
ln	O
px	O
it	O
follows	O
that	O
any	O
algorithm	O
that	O
converges	O
to	O
the	O
global	O
maximum	O
of	O
lq	O
will	O
find	O
a	O
value	O
of	O
that	O
is	O
also	O
a	O
global	O
maximum	O
of	O
the	O
log	O
likelihood	O
ln	O
px	O
provided	O
px	O
z	O
is	O
a	O
continuous	O
function	O
of	O
exercises	O
then	O
by	O
continuity	O
any	O
local	B
maximum	O
of	O
lq	O
will	O
also	O
be	O
a	O
local	B
maximum	O
of	O
ln	O
px	O
consider	O
the	O
case	O
of	O
n	O
independent	B
data	O
points	O
xn	O
with	O
corresponding	O
latent	O
variables	O
zn	O
the	O
joint	O
distribution	O
px	O
z	O
factorizes	O
over	O
the	O
data	O
points	O
and	O
this	O
structure	O
can	O
be	O
exploited	O
in	O
an	O
incremental	O
form	O
of	O
em	B
in	O
which	O
at	O
each	O
em	B
cycle	O
only	O
one	O
data	O
point	O
is	O
processed	O
at	O
a	O
time	O
in	O
the	O
e	O
step	O
instead	O
of	O
recomputing	O
the	O
responsibilities	O
for	O
all	O
of	O
the	O
data	O
points	O
we	O
just	O
re-evaluate	O
the	O
responsibilities	O
for	O
one	O
data	O
point	O
it	O
might	O
appear	O
that	O
the	O
subsequent	O
m	O
step	O
would	O
require	O
computation	O
involving	O
the	O
responsibilities	O
for	O
all	O
of	O
the	O
data	O
points	O
however	O
if	O
the	O
mixture	B
components	O
are	O
members	O
of	O
the	O
exponential	B
family	I
then	O
the	O
responsibilities	O
enter	O
only	O
through	O
simple	O
sufficient	B
statistics	I
and	O
these	O
can	O
be	O
updated	O
efficiently	O
consider	O
for	O
instance	O
the	O
case	O
of	O
a	O
gaussian	B
mixture	B
and	O
suppose	O
we	O
perform	O
an	O
update	O
for	O
data	O
point	O
m	O
in	O
which	O
the	O
corresponding	O
old	O
and	O
new	O
values	O
of	O
the	O
responsibilities	O
are	O
denoted	O
oldzmk	O
and	O
newzmk	O
in	O
the	O
m	O
step	O
the	O
required	O
sufficient	B
statistics	I
can	O
be	O
updated	O
incrementally	O
for	O
instance	O
for	O
the	O
means	O
the	O
sufficient	B
statistics	I
are	O
defined	O
by	O
and	O
from	O
which	O
we	O
obtain	O
new	O
k	O
old	O
k	O
newzmk	O
oldzmk	O
n	O
new	O
k	O
xm	O
old	O
k	O
together	O
with	O
n	O
new	O
k	O
n	O
old	O
k	O
newzmk	O
oldzmk	O
the	O
corresponding	O
results	O
for	O
the	O
covariances	O
and	O
the	O
mixing	O
coefficients	O
are	O
analogous	O
thus	O
both	O
the	O
e	O
step	O
and	O
the	O
m	O
step	O
take	O
fixed	O
time	O
that	O
is	O
independent	B
of	O
the	O
total	O
number	O
of	O
data	O
points	O
because	O
the	O
parameters	O
are	O
revised	O
after	O
each	O
data	O
point	O
rather	O
than	O
waiting	O
until	O
after	O
the	O
whole	O
data	O
set	O
is	O
processed	O
this	O
incremental	O
version	O
can	O
converge	O
faster	O
than	O
the	O
batch	O
version	O
each	O
e	O
or	O
m	O
step	O
in	O
this	O
incremental	O
algorithm	O
is	O
increasing	O
the	O
value	O
of	O
lq	O
and	O
as	O
we	O
have	O
shown	O
above	O
if	O
the	O
algorithm	O
converges	O
to	O
a	O
local	B
global	O
maximum	O
of	O
lq	O
this	O
will	O
correspond	O
to	O
a	O
local	B
global	O
maximum	O
of	O
the	O
log	O
likelihood	B
function	I
ln	O
px	O
www	O
consider	O
the	O
k-means	O
algorithm	O
discussed	O
in	O
section	O
show	O
that	O
as	O
a	O
consequence	O
of	O
there	O
being	O
a	O
finite	O
number	O
of	O
possible	O
assignments	O
for	O
the	O
set	O
of	O
discrete	O
indicator	O
variables	O
rnk	O
and	O
that	O
for	O
each	O
such	O
assignment	O
there	O
is	O
a	O
unique	O
optimum	O
for	O
the	O
k	O
the	O
k-means	O
algorithm	O
must	O
converge	O
after	O
a	O
finite	O
number	O
of	O
iterations	O
apply	O
the	O
robbins-monro	O
sequential	B
estimation	I
procedure	O
described	O
in	O
section	O
to	O
the	O
problem	O
of	O
finding	O
the	O
roots	O
of	O
the	O
regression	B
function	I
given	O
by	O
the	O
derivatives	O
of	O
j	O
in	O
with	O
respect	O
to	O
k	O
show	O
that	O
this	O
leads	O
to	O
a	O
stochastic	B
k-means	O
algorithm	O
in	O
which	O
for	O
each	O
data	O
point	O
xn	O
the	O
nearest	O
prototype	O
k	O
is	O
updated	O
using	O
exercise	O
exercises	O
mixture	B
models	O
and	O
em	B
www	O
consider	O
a	O
gaussian	B
mixture	B
model	I
in	O
which	O
the	O
marginal	B
distribution	O
pz	O
for	O
the	O
latent	B
variable	I
is	O
given	O
by	O
and	O
the	O
conditional	B
distribution	O
pxz	O
for	O
the	O
observed	B
variable	I
is	O
given	O
by	O
show	O
that	O
the	O
marginal	B
distribution	O
px	O
obtained	O
by	O
summing	O
pzpxz	O
over	O
all	O
possible	O
values	O
of	O
z	O
is	O
a	O
gaussian	B
mixture	B
of	O
the	O
form	O
suppose	O
we	O
wish	O
to	O
use	O
the	O
em	B
algorithm	I
to	O
maximize	O
the	O
posterior	O
distribution	O
over	O
parameters	O
p	O
for	O
a	O
model	O
containing	O
latent	O
variables	O
where	O
x	O
is	O
the	O
observed	O
data	O
set	O
show	O
that	O
the	O
e	O
step	O
remains	O
the	O
same	O
as	O
in	O
the	O
maximum	B
likelihood	I
case	O
whereas	O
in	O
the	O
m	O
step	O
the	O
quantity	O
to	O
be	O
maximized	O
is	O
given	O
by	O
q	O
old	O
ln	O
p	O
where	O
q	O
old	O
is	O
defined	O
by	O
consider	O
the	O
directed	B
graph	O
for	O
a	O
gaussian	B
mixture	B
model	I
shown	O
in	O
figure	O
by	O
making	O
use	O
of	O
the	O
d-separation	B
criterion	O
discussed	O
in	O
section	O
show	O
that	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
factorizes	O
with	O
respect	O
to	O
the	O
different	O
data	O
points	O
so	O
that	O
pzx	O
pznxn	O
consider	O
a	O
special	O
case	O
of	O
a	O
gaussian	B
mixture	B
model	I
in	O
which	O
the	O
covariance	B
matrices	O
k	O
of	O
the	O
components	O
are	O
all	O
constrained	O
to	O
have	O
a	O
common	O
value	O
derive	O
the	O
em	B
equations	O
for	O
maximizing	O
the	O
likelihood	B
function	I
under	O
such	O
a	O
model	O
www	O
verify	O
that	O
maximization	O
of	O
the	O
complete-data	O
log	O
likelihood	O
for	O
a	O
gaussian	B
mixture	B
model	I
leads	O
to	O
the	O
result	O
that	O
the	O
means	O
and	O
covariances	O
of	O
each	O
component	O
are	O
fitted	O
independently	O
to	O
the	O
corresponding	O
group	O
of	O
data	O
points	O
and	O
the	O
mixing	O
coefficients	O
are	O
given	O
by	O
the	O
fractions	O
of	O
points	O
in	O
each	O
group	O
www	O
show	O
that	O
if	O
we	O
maximize	O
with	O
respect	O
to	O
k	O
while	O
keeping	O
the	O
responsibilities	O
fixed	O
we	O
obtain	O
the	O
closed	O
form	O
solution	O
given	O
by	O
show	O
that	O
if	O
we	O
maximize	O
with	O
respect	O
to	O
k	O
and	O
k	O
while	O
keeping	O
the	O
responsibilities	O
fixed	O
we	O
obtain	O
the	O
closed	O
form	O
solutions	O
given	O
by	O
and	O
consider	O
a	O
density	B
model	O
given	O
by	O
a	O
mixture	B
distribution	I
px	O
kpxk	O
and	O
suppose	O
that	O
we	O
partition	O
the	O
vector	O
x	O
into	O
two	O
parts	O
so	O
that	O
x	O
xb	O
show	O
that	O
the	O
conditional	B
density	B
pxbxa	O
is	O
itself	O
a	O
mixture	B
distribution	I
and	O
find	O
expressions	O
for	O
the	O
mixing	O
coefficients	O
and	O
for	O
the	O
component	O
densities	O
exercises	O
in	O
section	O
we	O
obtained	O
a	O
relationship	O
between	O
k	O
means	O
and	O
em	B
for	O
gaussian	B
mixtures	O
by	O
considering	O
a	O
mixture	B
model	I
in	O
which	O
all	O
components	O
have	O
covariance	B
show	O
that	O
in	O
the	O
limit	O
maximizing	O
the	O
expected	O
completedata	O
log	O
likelihood	O
for	O
this	O
model	O
given	O
by	O
is	O
equivalent	O
to	O
minimizing	O
the	O
distortion	B
measure	I
j	O
for	O
the	O
k-means	O
algorithm	O
given	O
by	O
www	O
consider	O
a	O
mixture	B
distribution	I
of	O
the	O
form	O
px	O
kpxk	O
where	O
the	O
elements	O
of	O
x	O
could	O
be	O
discrete	O
or	O
continuous	O
or	O
a	O
combination	O
of	O
these	O
denote	O
the	O
mean	B
and	O
covariance	B
of	O
pxk	O
by	O
k	O
and	O
k	O
respectively	O
show	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
mixture	B
distribution	I
are	O
given	O
by	O
and	O
using	O
the	O
re-estimation	O
equations	O
for	O
the	O
em	B
algorithm	I
show	O
that	O
a	O
mixture	B
of	O
bernoulli	B
distributions	O
with	O
its	O
parameters	O
set	O
to	O
values	O
corresponding	O
to	O
a	O
maximum	O
of	O
the	O
likelihood	B
function	I
has	O
the	O
property	O
that	O
ex	O
n	O
xn	O
x	O
nents	O
have	O
the	O
same	O
mean	B
k	O
for	O
k	O
k	O
then	O
the	O
em	B
algorithm	I
will	O
hence	O
show	O
that	O
if	O
the	O
parameters	O
of	O
this	O
model	O
are	O
initialized	O
such	O
that	O
all	O
compo	O
converge	O
after	O
one	O
iteration	O
for	O
any	O
choice	O
of	O
the	O
initial	O
mixing	O
coefficients	O
and	O
that	O
this	O
solution	O
has	O
the	O
property	O
k	O
x	O
note	O
that	O
this	O
represents	O
a	O
degenerate	O
case	O
of	O
the	O
mixture	B
model	I
in	O
which	O
all	O
of	O
the	O
components	O
are	O
identical	O
and	O
in	O
practice	O
we	O
try	O
to	O
avoid	O
such	O
solutions	O
by	O
using	O
an	O
appropriate	O
initialization	O
consider	O
the	O
joint	O
distribution	O
of	O
latent	O
and	O
observed	O
variables	O
for	O
the	O
bernoulli	B
distribution	I
obtained	O
by	O
forming	O
the	O
product	O
of	O
pxz	O
given	O
by	O
and	O
pz	O
given	O
by	O
show	O
that	O
if	O
we	O
marginalize	O
this	O
joint	O
distribution	O
with	O
respect	O
to	O
z	O
then	O
we	O
obtain	O
www	O
show	O
that	O
if	O
we	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	B
function	I
for	O
a	O
mixture	B
of	O
bernoulli	B
distributions	O
with	O
respect	O
to	O
k	O
we	O
obtain	O
the	O
m	O
step	O
equation	O
show	O
that	O
if	O
we	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	B
function	I
for	O
a	O
mixture	B
of	O
bernoulli	B
distributions	O
with	O
respect	O
to	O
the	O
mixing	O
coefficients	O
k	O
using	O
a	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
summation	O
constraint	O
we	O
obtain	O
the	O
m	O
step	O
equation	O
www	O
show	O
that	O
as	O
a	O
consequence	O
of	O
the	O
constraint	O
pxn	O
k	O
for	O
the	O
discrete	O
variable	O
xn	O
the	O
incomplete-data	O
log	O
likelihood	B
function	I
for	O
a	O
mixture	B
of	O
bernoulli	B
distributions	O
is	O
bounded	O
above	O
and	O
hence	O
that	O
there	O
are	O
no	O
singularities	B
for	O
which	O
the	O
likelihood	O
goes	O
to	O
infinity	O
mixture	B
models	O
and	O
em	B
consider	O
a	O
bernoulli	B
mixture	B
model	I
as	O
discussed	O
in	O
section	O
together	O
with	O
a	O
prior	B
distribution	O
p	O
kak	O
bk	O
over	O
each	O
of	O
the	O
parameter	O
vectors	O
k	O
given	O
by	O
the	O
beta	B
distribution	I
and	O
a	O
dirichlet	B
prior	B
p	O
given	O
by	O
derive	O
the	O
em	B
algorithm	I
for	O
maximizing	O
the	O
posterior	B
probability	B
p	O
consider	O
a	O
d-dimensional	O
variable	O
x	O
each	O
of	O
whose	O
components	O
i	O
is	O
itself	O
a	O
multinomial	O
variable	O
of	O
degree	O
m	O
so	O
that	O
x	O
is	O
a	O
binary	O
vector	O
with	O
components	O
xij	O
where	O
i	O
d	O
and	O
j	O
m	O
subject	O
to	O
the	O
constraint	O
that	O
j	O
xij	O
for	O
all	O
i	O
suppose	O
that	O
the	O
distribution	O
of	O
these	O
variables	O
is	O
described	O
by	O
a	O
mixture	B
of	O
the	O
discrete	O
multinomial	O
distributions	O
considered	O
in	O
section	O
so	O
that	O
px	O
where	O
kpx	O
k	O
xij	O
kij	O
px	O
k	O
the	O
parameters	O
kij	O
represent	O
the	O
probabilities	O
pxij	O
k	O
and	O
must	O
satisfy	O
kij	O
together	O
with	O
the	O
constraint	O
j	O
kij	O
for	O
all	O
values	O
of	O
k	O
and	O
i	O
given	O
an	O
observed	O
data	O
set	O
where	O
n	O
n	O
derive	O
the	O
e	O
and	O
m	O
step	O
equations	O
of	O
the	O
em	B
algorithm	I
for	O
optimizing	O
the	O
mixing	O
coefficients	O
k	O
and	O
the	O
component	O
parameters	O
kij	O
of	O
this	O
distribution	O
by	O
maximum	B
likelihood	I
www	O
show	O
that	O
maximization	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	B
function	I
for	O
the	O
bayesian	B
linear	B
regression	B
model	O
leads	O
to	O
the	O
m	O
step	O
reestimation	O
result	O
for	O
using	O
the	O
evidence	O
framework	O
of	O
section	O
derive	O
the	O
m-step	O
re-estimation	O
equations	O
for	O
the	O
parameter	O
in	O
the	O
bayesian	B
linear	B
regression	B
model	O
analogous	O
to	O
the	O
result	O
for	O
by	O
maximization	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
defined	O
by	O
derive	O
the	O
m	O
step	O
equations	O
and	O
for	O
re-estimating	O
the	O
hyperparameters	O
of	O
the	O
relevance	B
vector	I
machine	I
for	B
regression	B
www	O
in	O
section	O
we	O
used	O
direct	O
maximization	O
of	O
the	O
marginal	B
likelihood	I
to	O
derive	O
the	O
re-estimation	O
equations	O
and	O
for	O
finding	O
values	O
of	O
the	O
hyperparameters	O
and	O
for	O
the	O
regression	B
rvm	O
similarly	O
in	O
section	O
we	O
used	O
the	O
em	B
algorithm	I
to	O
maximize	O
the	O
same	O
marginal	B
likelihood	I
giving	O
the	O
re-estimation	O
equations	O
and	O
show	O
that	O
these	O
two	O
sets	O
of	O
re-estimation	O
equations	O
are	O
formally	O
equivalent	O
verify	O
the	O
relation	O
in	O
which	O
lq	O
and	O
are	O
defined	O
by	O
and	O
respectively	O
exercises	O
www	O
show	O
that	O
the	O
lower	B
bound	I
lq	O
given	O
by	O
with	O
qz	O
pzx	O
has	O
the	O
same	O
gradient	O
with	O
respect	O
to	O
as	O
the	O
log	O
likelihood	B
function	I
ln	O
px	O
at	O
the	O
point	O
www	O
consider	O
the	O
incremental	O
form	O
of	O
the	O
em	B
algorithm	I
for	O
a	O
mixture	B
of	I
gaussians	I
in	O
which	O
the	O
responsibilities	O
are	O
recomputed	O
only	O
for	O
a	O
specific	O
data	O
point	O
xm	O
starting	O
from	O
the	O
m-step	O
formulae	O
and	O
derive	O
the	O
results	O
and	O
for	O
updating	O
the	O
component	O
means	O
derive	O
m-step	O
formulae	O
for	O
updating	O
the	O
covariance	B
matrices	O
and	O
mixing	O
coefficients	O
in	O
a	O
gaussian	B
mixture	B
model	I
when	O
the	O
responsibilities	O
are	O
updated	O
incrementally	O
analogous	O
to	O
the	O
result	O
for	O
updating	O
the	O
means	O
approximate	O
inference	B
a	O
central	O
task	O
in	O
the	O
application	O
of	O
probabilistic	O
models	O
is	O
the	O
evaluation	O
of	O
the	O
posterior	O
distribution	O
pzx	O
of	O
the	O
latent	O
variables	O
z	O
given	O
the	O
observed	O
data	O
variables	O
x	O
and	O
the	O
evaluation	O
of	O
expectations	O
computed	O
with	O
respect	O
to	O
this	O
distribution	O
the	O
model	O
might	O
also	O
contain	O
some	O
deterministic	O
parameters	O
which	O
we	O
will	O
leave	O
implicit	O
for	O
the	O
moment	O
or	O
it	O
may	O
be	O
a	O
fully	O
bayesian	B
model	O
in	O
which	O
any	O
unknown	O
parameters	O
are	O
given	O
prior	B
distributions	O
and	O
are	O
absorbed	O
into	O
the	O
set	O
of	O
latent	O
variables	O
denoted	O
by	O
the	O
vector	O
z	O
for	O
instance	O
in	O
the	O
em	B
algorithm	I
we	O
need	O
to	O
evaluate	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
for	O
many	O
models	O
of	O
practical	O
interest	O
it	O
will	O
be	O
infeasible	O
to	O
evaluate	O
the	O
posterior	O
distribution	O
or	O
indeed	O
to	O
compute	O
expectations	O
with	O
respect	O
to	O
this	O
distribution	O
this	O
could	O
be	O
because	O
the	O
dimensionality	O
of	O
the	O
latent	O
space	O
is	O
too	O
high	O
to	O
work	O
with	O
directly	O
or	O
because	O
the	O
posterior	O
distribution	O
has	O
a	O
highly	O
complex	O
form	O
for	O
which	O
expectations	O
are	O
not	O
analytically	O
tractable	O
in	O
the	O
case	O
of	O
continuous	O
variables	O
the	O
required	O
integrations	O
may	O
not	O
have	O
closed-form	O
approximate	O
inference	B
analytical	O
solutions	O
while	O
the	O
dimensionality	O
of	O
the	O
space	O
and	O
the	O
complexity	O
of	O
the	O
integrand	O
may	O
prohibit	O
numerical	O
integration	O
for	O
discrete	O
variables	O
the	O
marginalizations	O
involve	O
summing	O
over	O
all	O
possible	O
configurations	O
of	O
the	O
hidden	O
variables	O
and	O
though	O
this	O
is	O
always	O
possible	O
in	O
principle	O
we	O
often	O
find	O
in	O
practice	O
that	O
there	O
may	O
be	O
exponentially	O
many	O
hidden	O
states	O
so	O
that	O
exact	O
calculation	O
is	O
prohibitively	O
expensive	O
in	O
such	O
situations	O
we	O
need	O
to	O
resort	O
to	O
approximation	O
schemes	O
and	O
these	O
fall	O
broadly	O
into	O
two	O
classes	O
according	O
to	O
whether	O
they	O
rely	O
on	O
stochastic	B
or	O
deterministic	O
approximations	O
stochastic	B
techniques	O
such	O
as	O
markov	B
chain	I
monte	I
carlo	I
described	O
in	O
chapter	O
have	O
enabled	O
the	O
widespread	O
use	O
of	O
bayesian	B
methods	O
across	O
many	O
domains	O
they	O
generally	O
have	O
the	O
property	O
that	O
given	O
infinite	O
computational	O
resource	O
they	O
can	O
generate	O
exact	O
results	O
and	O
the	O
approximation	O
arises	O
from	O
the	O
use	O
of	O
a	O
finite	O
amount	O
of	O
processor	O
time	O
in	O
practice	O
sampling	B
methods	I
can	O
be	O
computationally	O
demanding	O
often	O
limiting	O
their	O
use	O
to	O
small-scale	O
problems	O
also	O
it	O
can	O
be	O
difficult	O
to	O
know	O
whether	O
a	O
sampling	O
scheme	O
is	O
generating	O
independent	B
samples	O
from	O
the	O
required	O
distribution	O
in	O
this	O
chapter	O
we	O
introduce	O
a	O
range	O
of	O
deterministic	O
approximation	O
schemes	O
some	O
of	O
which	O
scale	O
well	O
to	O
large	O
applications	O
these	O
are	O
based	O
on	O
analytical	O
approximations	O
to	O
the	O
posterior	O
distribution	O
for	O
example	O
by	O
assuming	O
that	O
it	O
factorizes	O
in	O
a	O
particular	O
way	O
or	O
that	O
it	O
has	O
a	O
specific	O
parametric	O
form	O
such	O
as	O
a	O
gaussian	B
as	O
such	O
they	O
can	O
never	O
generate	O
exact	O
results	O
and	O
so	O
their	O
strengths	O
and	O
weaknesses	O
are	O
complementary	O
to	O
those	O
of	O
sampling	B
methods	I
in	O
section	O
we	O
discussed	O
the	O
laplace	B
approximation	I
which	O
is	O
based	O
on	O
a	O
local	B
gaussian	B
approximation	O
to	O
a	O
mode	O
a	O
maximum	O
of	O
the	O
distribution	O
here	O
we	O
turn	O
to	O
a	O
family	O
of	O
approximation	O
techniques	O
called	O
variational	B
inference	B
or	O
variational	B
bayes	B
which	O
use	O
more	O
global	O
criteria	O
and	O
which	O
have	O
been	O
widely	O
applied	O
we	O
conclude	O
with	O
a	O
brief	O
introduction	O
to	O
an	O
alternative	O
variational	B
framework	O
known	O
as	O
expectation	B
propagation	I
variational	B
inference	B
variational	B
methods	O
have	O
their	O
origins	O
in	O
the	O
century	O
with	O
the	O
work	O
of	O
euler	B
lagrange	B
and	O
others	O
on	O
the	O
calculus	B
of	I
variations	I
standard	O
calculus	O
is	O
concerned	O
with	O
finding	O
derivatives	O
of	O
functions	O
we	O
can	O
think	O
of	O
a	O
function	O
as	O
a	O
mapping	O
that	O
takes	O
the	O
value	O
of	O
a	O
variable	O
as	O
the	O
input	O
and	O
returns	O
the	O
value	O
of	O
the	O
function	O
as	O
the	O
output	O
the	O
derivative	B
of	O
the	O
function	O
then	O
describes	O
how	O
the	O
output	O
value	O
varies	O
as	O
we	O
make	O
infinitesimal	O
changes	O
to	O
the	O
input	O
value	O
similarly	O
we	O
can	O
define	O
a	O
functional	B
as	O
a	O
mapping	O
that	O
takes	O
a	O
function	O
as	O
the	O
input	O
and	O
that	O
returns	O
the	O
value	O
of	O
the	O
functional	B
as	O
the	O
output	O
an	O
example	O
would	O
be	O
the	O
entropy	B
hp	O
which	O
takes	O
a	O
probability	B
distribution	O
px	O
as	O
the	O
input	O
and	O
returns	O
the	O
quantity	O
hp	O
px	O
ln	O
px	O
dx	O
variational	B
inference	B
as	O
the	O
output	O
we	O
can	O
the	O
introduce	O
the	O
concept	O
of	O
a	O
functional	B
derivative	B
which	O
expresses	O
how	O
the	O
value	O
of	O
the	O
functional	B
changes	O
in	O
response	O
to	O
infinitesimal	O
changes	O
to	O
the	O
input	O
function	O
et	O
al	O
the	O
rules	O
for	O
the	O
calculus	B
of	I
variations	I
mirror	O
those	O
of	O
standard	O
calculus	O
and	O
are	O
discussed	O
in	O
appendix	O
d	O
many	O
problems	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
an	O
optimization	O
problem	O
in	O
which	O
the	O
quantity	O
being	O
optimized	O
is	O
a	O
functional	B
the	O
solution	O
is	O
obtained	O
by	O
exploring	O
all	O
possible	O
input	O
functions	O
to	O
find	O
the	O
one	O
that	O
maximizes	O
or	O
minimizes	O
the	O
functional	B
variational	B
methods	O
have	O
broad	O
applicability	O
and	O
include	O
such	O
areas	O
as	O
finite	O
element	O
methods	O
and	O
maximum	O
entropy	B
although	O
there	O
is	O
nothing	O
intrinsically	O
approximate	O
about	O
variational	B
methods	O
they	O
do	O
naturally	O
lend	O
themselves	O
to	O
finding	O
approximate	O
solutions	O
this	O
is	O
done	O
by	O
restricting	O
the	O
range	O
of	O
functions	O
over	O
which	O
the	O
optimization	O
is	O
performed	O
for	O
instance	O
by	O
considering	O
only	O
quadratic	O
functions	O
or	O
by	O
considering	O
functions	O
composed	O
of	O
a	O
linear	O
combination	O
of	O
fixed	O
basis	O
functions	O
in	O
which	O
only	O
the	O
coefficients	O
of	O
the	O
linear	O
combination	O
can	O
vary	O
in	O
the	O
case	O
of	O
applications	O
to	O
probabilistic	O
inference	B
the	O
restriction	O
may	O
for	O
example	O
take	O
the	O
form	O
of	O
factorization	B
assumptions	O
et	O
al	O
jaakkola	O
now	O
let	O
us	O
consider	O
in	O
more	O
detail	O
how	O
the	O
concept	O
of	O
variational	B
optimization	O
can	O
be	O
applied	O
to	O
the	O
inference	B
problem	O
suppose	O
we	O
have	O
a	O
fully	O
bayesian	B
model	O
in	O
which	O
all	O
parameters	O
are	O
given	O
prior	B
distributions	O
the	O
model	O
may	O
also	O
have	O
latent	O
variables	O
as	O
well	O
as	O
parameters	O
and	O
we	O
shall	O
denote	O
the	O
set	O
of	O
all	O
latent	O
variables	O
and	O
parameters	O
by	O
z	O
similarly	O
we	O
denote	O
the	O
set	O
of	O
all	O
observed	O
variables	O
by	O
x	O
for	O
example	O
we	O
might	O
have	O
a	O
set	O
of	O
n	O
independent	B
identically	I
distributed	I
data	O
for	O
which	O
x	O
xn	O
and	O
z	O
zn	O
our	O
probabilistic	O
model	O
specifies	O
the	O
joint	O
distribution	O
px	O
z	O
and	O
our	O
goal	O
is	O
to	O
find	O
an	O
approximation	O
for	O
the	O
posterior	O
distribution	O
pzx	O
as	O
well	O
as	O
for	O
the	O
model	B
evidence	I
px	O
as	O
in	O
our	O
discussion	O
of	O
em	B
we	O
can	O
decompose	O
the	O
log	O
marginal	B
probability	B
using	O
ln	O
px	O
lq	O
where	O
we	O
have	O
defined	O
lq	O
dz	O
dz	O
qz	O
ln	O
px	O
z	O
qz	O
pzx	O
qz	O
qz	O
ln	O
this	O
differs	O
from	O
our	O
discussion	O
of	O
em	B
only	O
in	O
that	O
the	O
parameter	O
vector	O
no	O
longer	O
appears	O
because	O
the	O
parameters	O
are	O
now	O
stochastic	B
variables	O
and	O
are	O
absorbed	O
into	O
z	O
since	O
in	O
this	O
chapter	O
we	O
will	O
mainly	O
be	O
interested	O
in	O
continuous	O
variables	O
we	O
have	O
used	O
integrations	O
rather	O
than	O
summations	O
in	O
formulating	O
this	O
decomposition	O
however	O
the	O
analysis	O
goes	O
through	O
unchanged	O
if	O
some	O
or	O
all	O
of	O
the	O
variables	O
are	O
discrete	O
simply	O
by	O
replacing	O
the	O
integrations	O
with	O
summations	O
as	O
required	O
as	O
before	O
we	O
can	O
maximize	O
the	O
lower	B
bound	I
lq	O
by	O
optimization	O
with	O
respect	O
to	O
the	O
distribution	O
qz	O
which	O
is	O
equivalent	O
to	O
minimizing	O
the	O
kl	O
divergence	O
if	O
we	O
allow	O
any	O
possible	O
choice	O
for	O
qz	O
then	O
the	O
maximum	O
of	O
the	O
lower	B
bound	I
occurs	O
when	O
the	O
kl	O
divergence	O
vanishes	O
which	O
occurs	O
when	O
qz	O
equals	O
the	O
posterior	O
distribution	O
pzx	O
approximate	O
inference	B
figure	O
illustration	O
of	O
the	O
variational	B
approximation	O
for	O
the	O
example	O
considered	O
earlier	O
in	O
figure	O
the	O
left-hand	O
plot	O
shows	O
the	O
original	O
distribution	O
along	O
with	O
the	O
laplace	B
and	O
variational	B
approximations	O
and	O
the	O
right-hand	O
plot	O
shows	O
the	O
negative	O
logarithms	O
of	O
the	O
corresponding	O
curves	O
however	O
we	O
shall	O
suppose	O
the	O
model	O
is	O
such	O
that	O
working	O
with	O
the	O
true	O
posterior	O
distribution	O
is	O
intractable	O
we	O
therefore	O
consider	O
instead	O
a	O
restricted	O
family	O
of	O
distributions	O
qz	O
and	O
then	O
seek	O
the	O
member	O
of	O
this	O
family	O
for	O
which	O
the	O
kl	O
divergence	O
is	O
minimized	O
our	O
goal	O
is	O
to	O
restrict	O
the	O
family	O
sufficiently	O
that	O
they	O
comprise	O
only	O
tractable	O
distributions	O
while	O
at	O
the	O
same	O
time	O
allowing	O
the	O
family	O
to	O
be	O
sufficiently	O
rich	O
and	O
flexible	O
that	O
it	O
can	O
provide	O
a	O
good	O
approximation	O
to	O
the	O
true	O
posterior	O
distribution	O
it	O
is	O
important	O
to	O
emphasize	O
that	O
the	O
restriction	O
is	O
imposed	O
purely	O
to	O
achieve	O
tractability	O
and	O
that	O
subject	O
to	O
this	O
requirement	O
we	O
should	O
use	O
as	O
rich	O
a	O
family	O
of	O
approximating	O
distributions	O
as	O
possible	O
in	O
particular	O
there	O
is	O
no	O
over-fitting	B
associated	O
with	O
highly	O
flexible	O
distributions	O
using	O
more	O
flexible	O
approximations	O
simply	O
allows	O
us	O
to	O
approach	O
the	O
true	O
posterior	O
distribution	O
more	O
closely	O
one	O
way	O
to	O
restrict	O
the	O
family	O
of	O
approximating	O
distributions	O
is	O
to	O
use	O
a	O
parametric	O
distribution	O
qz	O
governed	O
by	O
a	O
set	O
of	O
parameters	O
the	O
lower	B
bound	I
lq	O
then	O
becomes	O
a	O
function	O
of	O
and	O
we	O
can	O
exploit	O
standard	O
nonlinear	O
optimization	O
techniques	O
to	O
determine	O
the	O
optimal	O
values	O
for	O
the	O
parameters	O
an	O
example	O
of	O
this	O
approach	O
in	O
which	O
the	O
variational	B
distribution	O
is	O
a	O
gaussian	B
and	O
we	O
have	O
optimized	O
with	O
respect	O
to	O
its	O
mean	B
and	O
variance	B
is	O
shown	O
in	O
figure	O
factorized	O
distributions	O
here	O
we	O
consider	O
an	O
alternative	O
way	O
in	O
which	O
to	O
restrict	O
the	O
family	O
of	O
distributions	O
qz	O
suppose	O
we	O
partition	O
the	O
elements	O
of	O
z	O
into	O
disjoint	O
groups	O
that	O
we	O
denote	O
by	O
zi	O
where	O
i	O
m	O
we	O
then	O
assume	O
that	O
the	O
q	O
distribution	O
factorizes	O
with	O
respect	O
to	O
these	O
groups	O
so	O
that	O
qz	O
qizi	O
variational	B
inference	B
it	O
should	O
be	O
emphasized	O
that	O
we	O
are	O
making	O
no	O
further	O
assumptions	O
about	O
the	O
distribution	O
in	O
particular	O
we	O
place	O
no	O
restriction	O
on	O
the	O
functional	B
forms	O
of	O
the	O
individual	O
factors	O
qizi	O
this	O
factorized	O
form	O
of	O
variational	B
inference	B
corresponds	O
to	O
an	O
approximation	O
framework	O
developed	O
in	O
physics	O
called	O
mean	B
field	I
theory	B
amongst	O
all	O
distributions	O
qz	O
having	O
the	O
form	O
we	O
now	O
seek	O
that	O
distribution	O
for	O
which	O
the	O
lower	B
bound	I
lq	O
is	O
largest	O
we	O
therefore	O
wish	O
to	O
make	O
a	O
free	O
form	O
optimization	O
of	O
lq	O
with	O
respect	O
to	O
all	O
of	O
the	O
distributions	O
qizi	O
which	O
we	O
do	O
by	O
optimizing	O
with	O
respect	O
to	O
each	O
of	O
the	O
factors	O
in	O
turn	O
to	O
achieve	O
this	O
we	O
first	O
substitute	O
into	O
and	O
then	O
dissect	O
out	O
the	O
dependence	O
on	O
one	O
of	O
the	O
factors	O
qjzj	O
denoting	O
qjzj	O
by	O
simply	O
qj	O
to	O
keep	O
the	O
notation	O
uncluttered	O
we	O
then	O
obtain	O
lq	O
qj	O
zj	O
dzj	O
where	O
we	O
have	O
defined	O
a	O
new	O
zj	O
by	O
the	O
relation	O
zj	O
px	O
z	O
const	O
here	O
the	O
notation	O
denotes	O
an	O
expectation	B
with	O
respect	O
to	O
the	O
q	O
distributions	O
over	O
all	O
variables	O
zi	O
for	O
i	O
j	O
so	O
that	O
ln	O
px	O
z	O
qj	O
ln	O
qj	O
dzj	O
const	O
qj	O
ln	O
qj	O
dzj	O
const	O
ln	O
px	O
z	O
qi	O
dzi	O
dzj	O
ln	O
qi	O
dz	O
i	O
qi	O
i	O
qj	O
px	O
z	O
ln	O
px	O
z	O
qi	O
dzi	O
now	O
suppose	O
we	O
keep	O
the	O
fixed	O
and	O
maximize	O
lq	O
in	O
with	O
respect	O
to	O
all	O
possible	O
forms	O
for	O
the	O
distribution	O
qjzj	O
this	O
is	O
easily	O
done	O
by	O
recognizing	O
that	O
is	O
a	O
negative	O
kullback-leibler	B
divergence	I
between	O
qjzj	O
and	O
zj	O
thus	O
maximizing	O
is	O
equivalent	O
to	O
minimizing	O
the	O
kullback-leibler	O
leonhard	O
euler	B
euler	B
was	O
a	O
swiss	O
mathematician	O
and	O
physicist	O
who	O
worked	O
in	O
st	O
petersburg	O
and	O
berlin	O
and	O
who	O
is	O
widely	O
considered	O
to	O
be	O
one	O
of	O
the	O
greatest	O
mathematicians	O
of	O
all	O
time	O
he	O
is	O
certainly	O
the	O
most	O
prolific	O
and	O
his	O
collected	O
works	O
fill	O
volumes	O
amongst	O
his	O
many	O
contributions	O
he	O
formulated	O
the	O
modern	O
theory	B
of	O
the	O
function	O
he	O
developed	O
with	O
lagrange	B
the	O
calculus	B
of	I
variations	I
and	O
he	O
discovered	O
the	O
formula	O
ei	O
which	O
relates	O
four	O
of	O
the	O
most	O
important	O
numbers	O
in	O
mathematics	O
during	O
the	O
last	O
years	O
of	O
his	O
life	O
he	O
was	O
almost	O
totally	O
blind	O
and	O
yet	O
he	O
produced	O
nearly	O
half	O
of	O
his	O
results	O
during	O
this	O
period	O
approximate	O
inference	B
divergence	O
and	O
the	O
minimum	O
occurs	O
when	O
qjzj	O
zj	O
thus	O
we	O
obtain	O
a	O
general	O
expression	O
for	O
the	O
optimal	O
solution	O
j	O
given	O
by	O
ln	O
j	O
px	O
z	O
const	O
it	O
is	O
worth	O
taking	O
a	O
few	O
moments	O
to	O
study	O
the	O
form	O
of	O
this	O
solution	O
as	O
it	O
provides	O
the	O
basis	O
for	O
applications	O
of	O
variational	B
methods	O
it	O
says	O
that	O
the	O
log	O
of	O
the	O
optimal	O
solution	O
for	O
factor	O
qj	O
is	O
obtained	O
simply	O
by	O
considering	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
over	O
all	O
hidden	O
and	O
visible	O
variables	O
and	O
then	O
taking	O
the	O
expectation	B
with	O
respect	O
to	O
all	O
of	O
the	O
other	O
factors	O
for	O
i	O
j	O
the	O
additive	O
constant	O
in	O
is	O
set	O
by	O
normalizing	O
the	O
distribution	O
thus	O
if	O
we	O
take	O
the	O
exponential	O
of	O
both	O
sides	O
and	O
normalize	O
we	O
have	O
exp	O
px	O
z	O
j	O
j	O
exp	O
px	O
z	O
dzj	O
in	O
practice	O
we	O
shall	O
find	O
it	O
more	O
convenient	O
to	O
work	O
with	O
the	O
form	O
and	O
then	O
reinstate	O
the	O
normalization	O
constant	O
required	O
by	O
inspection	O
this	O
will	O
become	O
clear	O
from	O
subsequent	O
examples	O
the	O
set	O
of	O
equations	O
given	O
by	O
for	O
j	O
m	O
represent	O
a	O
set	O
of	O
consistency	O
conditions	O
for	O
the	O
maximum	O
of	O
the	O
lower	B
bound	I
subject	O
to	O
the	O
factorization	B
constraint	O
however	O
they	O
do	O
not	O
represent	O
an	O
explicit	O
solution	O
because	O
the	O
expression	O
on	O
the	O
right-hand	O
side	O
of	O
for	O
the	O
optimum	O
j	O
depends	O
on	O
expectations	O
computed	O
with	O
respect	O
to	O
the	O
other	O
factors	O
qizi	O
for	O
i	O
j	O
we	O
will	O
therefore	O
seek	O
a	O
consistent	B
solution	O
by	O
first	O
initializing	O
all	O
of	O
the	O
factors	O
qizi	O
appropriately	O
and	O
then	O
cycling	O
through	O
the	O
factors	O
and	O
replacing	O
each	O
in	O
turn	O
with	O
a	O
revised	O
estimate	O
given	O
by	O
the	O
right-hand	O
side	O
of	O
evaluated	O
using	O
the	O
current	O
estimates	O
for	O
all	O
of	O
the	O
other	O
factors	O
convergence	O
is	O
guaranteed	O
because	O
bound	O
is	O
convex	O
with	O
respect	O
to	O
each	O
of	O
the	O
factors	O
qizi	O
and	O
vandenberghe	O
properties	O
of	O
factorized	O
approximations	O
our	O
approach	O
to	O
variational	B
inference	B
is	O
based	O
on	O
a	O
factorized	O
approximation	O
to	O
the	O
true	O
posterior	O
distribution	O
let	O
us	O
consider	O
for	O
a	O
moment	O
the	O
problem	O
of	O
approximating	O
a	O
general	O
distribution	O
by	O
a	O
factorized	B
distribution	I
to	O
begin	O
with	O
we	O
discuss	O
the	O
problem	O
of	O
approximating	O
a	O
gaussian	B
distribution	O
using	O
a	O
factorized	O
gaussian	B
which	O
will	O
provide	O
useful	O
insight	O
into	O
the	O
types	O
of	O
inaccuracy	O
introduced	O
in	O
using	O
factorized	O
approximations	O
consider	O
a	O
gaussian	B
distribution	O
pz	O
n	O
over	O
two	O
correlated	O
variables	O
z	O
in	O
which	O
the	O
mean	B
and	O
precision	O
have	O
elements	O
and	O
due	O
to	O
the	O
symmetry	O
of	O
the	O
precision	B
matrix	I
now	O
suppose	O
we	O
wish	O
to	O
approximate	O
this	O
distribution	O
using	O
a	O
factorized	O
gaussian	B
of	O
the	O
form	O
qz	O
we	O
first	O
apply	O
the	O
general	O
result	O
to	O
find	O
an	O
expression	O
for	O
the	O
variational	B
inference	B
optimal	O
factor	O
in	O
doing	O
so	O
it	O
is	O
useful	O
to	O
note	O
that	O
on	O
the	O
right-hand	O
side	O
we	O
only	O
need	O
to	O
retain	O
those	O
terms	O
that	O
have	O
some	O
functional	B
dependence	O
on	O
because	O
all	O
other	O
terms	O
can	O
be	O
absorbed	O
into	O
the	O
normalization	O
constant	O
thus	O
we	O
have	O
ln	O
pz	O
const	O
const	O
const	O
next	O
we	O
observe	O
that	O
the	O
right-hand	O
side	O
of	O
this	O
expression	O
is	O
a	O
quadratic	O
function	O
of	O
and	O
so	O
we	O
can	O
identify	O
as	O
a	O
gaussian	B
distribution	O
it	O
is	O
worth	O
emphasizing	O
that	O
we	O
did	O
not	O
assume	O
that	O
qzi	O
is	O
gaussian	B
but	O
rather	O
we	O
derived	O
this	O
result	O
by	O
variational	B
optimization	O
of	O
the	O
kl	O
divergence	O
over	O
all	O
possible	O
distributions	O
qzi	O
note	O
also	O
that	O
we	O
do	O
not	O
need	O
to	O
consider	O
the	O
additive	O
constant	O
in	O
explicitly	O
because	O
it	O
represents	O
the	O
normalization	O
constant	O
that	O
can	O
be	O
found	O
at	O
the	O
end	O
by	O
inspection	O
if	O
required	O
using	O
the	O
technique	O
of	O
completing	B
the	I
square	I
we	O
can	O
identify	O
the	O
mean	B
and	O
precision	O
of	O
this	O
gaussian	B
giving	O
section	O
n	O
where	O
is	O
also	O
gaussian	B
and	O
can	O
be	O
written	O
as	O
by	O
symmetry	O
n	O
in	O
which	O
note	O
that	O
these	O
solutions	O
are	O
coupled	O
so	O
that	O
depends	O
on	O
expectations	O
computed	O
with	O
respect	O
to	O
and	O
vice	O
versa	O
in	O
general	O
we	O
address	O
this	O
by	O
treating	O
the	O
variational	B
solutions	O
as	O
re-estimation	O
equations	O
and	O
cycling	O
through	O
the	O
variables	O
in	O
turn	O
updating	O
them	O
until	O
some	O
convergence	O
criterion	O
is	O
satisfied	O
we	O
shall	O
see	O
an	O
example	O
of	O
this	O
shortly	O
here	O
however	O
we	O
note	O
that	O
the	O
problem	O
is	O
sufficiently	O
simple	O
that	O
a	O
closed	O
form	O
solution	O
can	O
be	O
found	O
in	O
particular	O
because	O
and	O
we	O
see	O
that	O
the	O
two	O
equations	O
are	O
satisfied	O
if	O
we	O
take	O
and	O
and	O
it	O
is	O
easily	O
shown	O
that	O
this	O
is	O
the	O
only	O
solution	O
provided	O
the	O
distribution	O
is	O
nonsingular	O
this	O
result	O
is	O
illustrated	O
in	O
figure	O
we	O
see	O
that	O
the	O
mean	B
is	O
correctly	O
captured	O
but	O
that	O
the	O
variance	B
of	O
qz	O
is	O
controlled	O
by	O
the	O
direction	O
of	O
smallest	O
variance	B
of	O
pz	O
and	O
that	O
the	O
variance	B
along	O
the	O
orthogonal	O
direction	O
is	O
significantly	O
under-estimated	O
it	O
is	O
a	O
general	O
result	O
that	O
a	O
factorized	O
variational	B
approximation	O
tends	O
to	O
give	O
approximations	O
to	O
the	O
posterior	O
distribution	O
that	O
are	O
too	O
compact	O
by	O
way	O
of	O
comparison	O
suppose	O
instead	O
that	O
we	O
had	O
been	O
minimizing	O
the	O
reverse	O
kullback-leibler	B
divergence	I
as	O
we	O
shall	O
see	O
this	O
form	O
of	O
kl	O
divergence	O
exercise	O
approximate	O
inference	B
figure	O
comparison	O
of	O
the	O
the	O
two	O
alternative	O
forms	O
for	O
kullback-leibler	B
divergence	I
the	O
green	O
contours	O
corresponding	O
to	O
and	O
standard	O
deviations	O
for	O
a	O
correlated	O
gaussian	B
distribution	O
pz	O
over	O
two	O
variables	O
and	O
and	O
the	O
red	O
contours	O
represent	O
the	O
corresponding	O
levels	O
for	O
an	O
qz	O
approximating	O
over	O
the	O
same	O
variables	O
given	O
by	O
the	O
product	O
of	O
two	O
independent	B
univariate	O
gaussian	B
distributions	O
whose	O
parameters	O
are	O
obtained	O
by	O
minimization	O
of	O
the	O
kullbackleibler	O
divergence	O
and	O
the	O
reverse	O
kullback-leibler	B
divergence	I
distribution	O
is	O
used	O
in	O
an	O
alternative	O
approximate	O
inference	B
framework	O
called	O
expectation	B
propagation	I
we	O
therefore	O
consider	O
the	O
general	O
problem	O
of	O
minimizing	O
when	O
qz	O
is	O
a	O
factorized	O
approximation	O
of	O
the	O
form	O
the	O
kl	O
divergence	O
can	O
then	O
be	O
written	O
in	O
the	O
form	O
pz	O
ln	O
qizi	O
dz	O
const	O
section	O
exercise	O
where	O
the	O
constant	O
term	O
is	O
simply	O
the	O
entropy	B
of	O
pz	O
and	O
so	O
does	O
not	O
depend	O
on	O
qz	O
we	O
can	O
now	O
optimize	O
with	O
respect	O
to	O
each	O
of	O
the	O
factors	O
qjzj	O
which	O
is	O
easily	O
done	O
using	O
a	O
lagrange	B
multiplier	I
to	O
give	O
j	O
pz	O
dzi	O
pzj	O
in	O
this	O
case	O
we	O
find	O
that	O
the	O
optimal	O
solution	O
for	O
qjzj	O
is	O
just	O
given	O
by	O
the	O
corresponding	O
marginal	B
distribution	O
of	O
pz	O
note	O
that	O
this	O
is	O
a	O
closed-form	O
solution	O
and	O
so	O
does	O
not	O
require	O
iteration	O
to	O
apply	O
this	O
result	O
to	O
the	O
illustrative	O
example	O
of	O
a	O
gaussian	B
distribution	O
pz	O
over	O
a	O
vector	O
z	O
we	O
can	O
use	O
which	O
gives	O
the	O
result	O
shown	O
in	O
figure	O
we	O
see	O
that	O
once	O
again	O
the	O
mean	B
of	O
the	O
approximation	O
is	O
correct	O
but	O
that	O
it	O
places	O
significant	O
probability	B
mass	O
in	O
regions	O
of	O
variable	O
space	O
that	O
have	O
very	O
low	O
probability	B
the	O
difference	O
between	O
these	O
two	O
results	O
can	O
be	O
understood	O
by	O
noting	O
that	O
there	O
is	O
a	O
large	O
positive	O
contribution	O
to	O
the	O
kullback-leibler	B
divergence	I
qz	O
ln	O
dz	O
pz	O
qz	O
variational	B
inference	B
figure	O
another	O
comparison	O
of	O
the	O
two	O
alternative	O
forms	O
for	O
the	O
kullback-leibler	B
divergence	I
the	O
blue	O
contours	O
show	O
a	O
bimodal	O
distribution	O
pz	O
given	O
by	O
a	O
mixture	B
of	O
two	O
gaussians	O
and	O
the	O
red	O
contours	O
correspond	O
to	O
the	O
single	O
gaussian	B
distribution	O
qz	O
that	O
best	O
approximates	O
pz	O
in	O
the	O
sense	O
of	O
minimizing	O
the	O
kullbackleibler	O
divergence	O
as	O
in	O
but	O
now	O
the	O
red	O
contours	O
correspond	O
to	O
a	O
gaussian	B
distribution	O
qz	O
found	O
by	O
numerical	O
minimization	O
of	O
the	O
kullback-leibler	B
divergence	I
as	O
in	O
but	O
showing	O
a	O
different	O
local	B
minimum	I
of	O
the	O
kullback-leibler	B
divergence	I
from	O
regions	O
of	O
z	O
space	O
in	O
which	O
pz	O
is	O
near	O
zero	O
unless	O
qz	O
is	O
also	O
close	O
to	O
zero	O
thus	O
minimizing	O
this	O
form	O
of	O
kl	O
divergence	O
leads	O
to	O
distributions	O
qz	O
that	O
avoid	O
regions	O
in	O
which	O
pz	O
is	O
small	O
conversely	O
the	O
kullback-leibler	B
divergence	I
is	O
minimized	O
by	O
distributions	O
qz	O
that	O
are	O
nonzero	O
in	O
regions	O
where	O
pz	O
is	O
nonzero	O
we	O
can	O
gain	O
further	O
insight	O
into	O
the	O
different	O
behaviour	O
of	O
the	O
two	O
kl	O
divergences	O
if	O
we	O
consider	O
approximating	O
a	O
multimodal	O
distribution	O
by	O
a	O
unimodal	O
one	O
as	O
illustrated	O
in	O
figure	O
in	O
practical	O
applications	O
the	O
true	O
posterior	O
distribution	O
will	O
often	O
be	O
multimodal	O
with	O
most	O
of	O
the	O
posterior	O
mass	O
concentrated	O
in	O
some	O
number	O
of	O
relatively	O
small	O
regions	O
of	O
parameter	O
space	O
these	O
multiple	O
modes	O
may	O
arise	O
through	O
nonidentifiability	B
in	O
the	O
latent	O
space	O
or	O
through	O
complex	O
nonlinear	O
dependence	O
on	O
the	O
parameters	O
both	O
types	O
of	O
multimodality	B
were	O
encountered	O
in	O
chapter	O
in	O
the	O
context	O
of	O
gaussian	B
mixtures	O
where	O
they	O
manifested	O
themselves	O
as	O
multiple	O
maxima	O
in	O
the	O
likelihood	B
function	I
and	O
a	O
variational	B
treatment	O
based	O
on	O
the	O
minimization	O
of	O
will	O
tend	O
to	O
find	O
one	O
of	O
these	O
modes	O
by	O
contrast	O
if	O
we	O
were	O
to	O
minimize	O
the	O
resulting	O
approximations	O
would	O
average	O
across	O
all	O
of	O
the	O
modes	O
and	O
in	O
the	O
context	O
of	O
the	O
mixture	B
model	I
would	O
lead	O
to	O
poor	O
predictive	O
distributions	O
the	O
average	O
of	O
two	O
good	O
parameter	O
values	O
is	O
typically	O
itself	O
not	O
a	O
good	O
parameter	O
value	O
it	O
is	O
possible	O
to	O
make	O
use	O
of	O
to	O
define	O
a	O
useful	O
inference	B
procedure	O
but	O
this	O
requires	O
a	O
rather	O
different	O
approach	O
to	O
the	O
one	O
discussed	O
here	O
and	O
will	O
be	O
considered	O
in	O
detail	O
when	O
we	O
discuss	O
expectation	B
propagation	I
the	O
two	O
forms	O
of	O
kullback-leibler	B
divergence	I
are	O
members	O
of	O
the	O
alpha	O
family	O
section	O
approximate	O
inference	B
of	O
divergences	O
and	O
silvey	O
amari	O
minka	O
defined	O
by	O
d	O
dx	O
where	O
is	O
a	O
continuous	O
parameter	O
the	O
kullback-leibler	B
divergence	I
corresponds	O
to	O
the	O
limit	O
whereas	O
corresponds	O
to	O
the	O
limit	O
for	O
all	O
values	O
of	O
we	O
have	O
d	O
with	O
equality	O
if	O
and	O
only	O
if	O
px	O
qx	O
suppose	O
px	O
is	O
a	O
fixed	O
distribution	O
and	O
we	O
minimize	O
d	O
with	O
respect	O
to	O
some	O
set	O
of	O
distributions	O
qx	O
then	O
for	O
the	O
divergence	O
is	O
zero	O
forcing	O
so	O
that	O
any	O
values	O
of	O
x	O
for	O
which	O
px	O
will	O
have	O
qx	O
and	O
typically	O
qx	O
will	O
under-estimate	O
the	O
support	O
of	O
px	O
and	O
will	O
tend	O
to	O
seek	O
the	O
mode	O
with	O
the	O
largest	O
mass	O
conversely	O
for	O
the	O
divergence	O
is	O
zero-avoiding	O
so	O
that	O
values	O
of	O
x	O
for	O
which	O
px	O
will	O
have	O
qx	O
and	O
typically	O
qx	O
will	O
stretch	O
to	O
cover	O
all	O
of	O
px	O
and	O
will	O
over-estimate	O
the	O
support	O
of	O
px	O
when	O
we	O
obtain	O
a	O
symmetric	O
divergence	O
that	O
is	O
linearly	O
related	O
to	O
the	O
hellinger	B
distance	I
given	O
by	O
dx	O
the	O
square	O
root	O
of	O
the	O
hellinger	B
distance	I
is	O
a	O
valid	O
distance	O
metric	O
example	O
the	O
univariate	O
gaussian	B
we	O
now	O
illustrate	O
the	O
factorized	O
variational	B
approximation	O
using	O
a	O
gaussian	B
distribution	O
over	O
a	O
single	O
variable	O
x	O
our	O
goal	O
is	O
to	O
infer	O
the	O
posterior	O
distribution	O
for	O
the	O
mean	B
and	O
precision	O
given	O
a	O
data	O
set	O
d	O
xn	O
of	O
observed	O
values	O
of	O
x	O
which	O
are	O
assumed	O
to	O
be	O
drawn	O
independently	O
from	O
the	O
gaussian	B
the	O
likelihood	B
function	I
is	O
given	O
by	O
pd	O
p	O
exp	O
we	O
now	O
introduce	O
conjugate	B
prior	B
distributions	O
for	O
and	O
given	O
by	O
where	O
gam	O
is	O
the	O
gamma	B
distribution	I
defined	O
by	O
together	O
these	O
distributions	O
constitute	O
a	O
gaussian-gamma	O
conjugate	B
prior	B
distribution	O
p	O
gam	O
for	O
this	O
simple	O
problem	O
the	O
posterior	O
distribution	O
can	O
be	O
found	O
exactly	O
and	O
again	O
takes	O
the	O
form	O
of	O
a	O
gaussian-gamma	B
distribution	I
however	O
for	O
tutorial	O
purposes	O
we	O
will	O
consider	O
a	O
factorized	O
variational	B
approximation	O
to	O
the	O
posterior	O
distribution	O
given	O
by	O
q	O
q	O
exercise	O
section	O
exercise	O
variational	B
inference	B
note	O
that	O
the	O
true	O
posterior	O
distribution	O
does	O
not	O
factorize	O
in	O
this	O
way	O
the	O
optimum	O
factors	O
q	O
and	O
q	O
can	O
be	O
obtained	O
from	O
the	O
general	O
result	O
as	O
follows	O
for	O
q	O
we	O
have	O
ln	O
e	O
pd	O
ln	O
p	O
const	O
completing	B
the	I
square	I
over	O
we	O
see	O
that	O
q	O
is	O
a	O
gaussian	B
e	O
const	O
n	O
n	O
with	O
exercise	O
mean	B
and	O
precision	O
given	O
by	O
n	O
n	O
x	O
n	O
n	O
ne	O
note	O
that	O
for	O
n	O
this	O
gives	O
the	O
maximum	B
likelihood	I
result	O
in	O
which	O
n	O
x	O
and	O
the	O
precision	O
is	O
infinite	O
similarly	O
the	O
optimal	O
solution	O
for	O
the	O
factor	O
q	O
is	O
given	O
by	O
e	O
pd	O
ln	O
p	O
ln	O
p	O
const	O
ln	O
ln	O
n	O
ln	O
e	O
const	O
and	O
hence	O
q	O
is	O
a	O
gamma	B
distribution	I
gam	O
bn	O
with	O
parameters	O
an	O
n	O
e	O
bn	O
exercise	O
section	O
again	O
this	O
exhibits	O
the	O
expected	O
behaviour	O
when	O
n	O
it	O
should	O
be	O
emphasized	O
that	O
we	O
did	O
not	O
assume	O
these	O
specific	O
functional	B
forms	O
for	O
the	O
optimal	O
distributions	O
q	O
and	O
q	O
they	O
arose	O
naturally	O
from	O
the	O
structure	O
of	O
the	O
likelihood	B
function	I
and	O
the	O
corresponding	O
conjugate	B
priors	O
thus	O
we	O
have	O
expressions	O
for	O
the	O
optimal	O
distributions	O
q	O
and	O
q	O
each	O
of	O
which	O
depends	O
on	O
moments	O
evaluated	O
with	O
respect	O
to	O
the	O
other	O
distribution	O
one	O
approach	O
to	O
finding	O
a	O
solution	O
is	O
therefore	O
to	O
make	O
an	O
initial	O
guess	O
for	O
say	O
the	O
moment	O
e	O
and	O
use	O
this	O
to	O
re-compute	O
the	O
distribution	O
q	O
given	O
this	O
revised	O
distribution	O
we	O
can	O
then	O
extract	O
the	O
required	O
moments	O
e	O
and	O
e	O
and	O
use	O
these	O
to	O
recompute	O
the	O
distribution	O
q	O
and	O
so	O
on	O
since	O
the	O
space	O
of	O
hidden	O
variables	O
for	O
this	O
example	O
is	O
only	O
two	O
dimensional	O
we	O
can	O
illustrate	O
the	O
variational	B
approximation	O
to	O
the	O
posterior	O
distribution	O
by	O
plotting	O
contours	O
of	O
both	O
the	O
true	O
posterior	O
and	O
the	O
factorized	O
approximation	O
as	O
illustrated	O
in	O
figure	O
approximate	O
inference	B
figure	O
illustration	O
of	O
variational	B
inference	B
for	O
the	O
mean	B
and	O
precision	O
of	O
a	O
univariate	O
gaussian	B
distribution	O
contours	O
of	O
the	O
true	O
posterior	O
distribution	O
p	O
are	O
shown	O
in	O
green	O
contours	O
of	O
the	O
initial	O
factorized	O
approximation	O
q	O
are	O
shown	O
in	O
blue	O
after	O
re-estimating	O
the	O
factor	O
q	O
after	O
re-estimating	O
the	O
factor	O
q	O
contours	O
of	O
the	O
optimal	O
factorized	O
approximation	O
to	O
which	O
the	O
iterative	O
scheme	O
converges	O
are	O
shown	O
in	O
red	O
appendix	O
b	O
in	O
general	O
we	O
will	O
need	O
to	O
use	O
an	O
iterative	O
approach	O
such	O
as	O
this	O
in	O
order	O
to	O
solve	O
for	O
the	O
optimal	O
factorized	O
posterior	O
distribution	O
for	O
the	O
very	O
simple	O
example	O
we	O
are	O
considering	O
here	O
however	O
we	O
can	O
find	O
an	O
explicit	O
solution	O
by	O
solving	O
the	O
simultaneous	O
equations	O
for	O
the	O
optimal	O
factors	O
q	O
and	O
q	O
before	O
doing	O
this	O
we	O
can	O
simplify	O
these	O
expressions	O
by	O
considering	O
broad	O
noninformative	B
priors	O
in	O
which	O
although	O
these	O
parameter	O
settings	O
correspond	O
to	O
improper	B
priors	O
we	O
see	O
that	O
the	O
posterior	O
distribution	O
is	O
still	O
well	O
defined	O
using	O
the	O
standard	O
result	O
e	O
an	O
for	O
the	O
mean	B
of	O
a	O
gamma	B
distribution	I
together	O
with	O
and	O
we	O
have	O
e	O
e	O
n	O
e	O
then	O
using	O
and	O
we	O
obtain	O
the	O
first	O
and	O
second	B
order	I
moments	O
of	O
variational	B
inference	B
q	O
in	O
the	O
form	O
e	O
x	O
e	O
n	O
e	O
exercise	O
we	O
can	O
now	O
substitute	O
these	O
moments	O
into	O
and	O
then	O
solve	O
for	O
e	O
to	O
give	O
e	O
n	O
n	O
section	O
we	O
recognize	O
the	O
right-hand	O
side	O
as	O
the	O
familiar	O
unbiased	O
estimator	O
for	O
the	O
variance	B
of	O
a	O
univariate	O
gaussian	B
distribution	O
and	O
so	O
we	O
see	O
that	O
the	O
use	O
of	O
a	O
bayesian	B
approach	O
has	O
avoided	O
the	O
bias	B
of	O
the	O
maximum	B
likelihood	I
solution	O
model	B
comparison	I
as	O
well	O
as	O
performing	O
inference	B
over	O
the	O
hidden	O
variables	O
z	O
we	O
may	O
also	O
wish	O
to	O
compare	O
a	O
set	O
of	O
candidate	O
models	O
labelled	O
by	O
the	O
index	O
m	O
and	O
having	O
prior	B
probabilities	O
pm	O
our	O
goal	O
is	O
then	O
to	O
approximate	O
the	O
posterior	O
probabilities	O
pmx	O
where	O
x	O
is	O
the	O
observed	O
data	O
this	O
is	O
a	O
slightly	O
more	O
complex	O
situation	O
than	O
that	O
considered	O
so	O
far	O
because	O
different	O
models	O
may	O
have	O
different	O
structure	O
and	O
indeed	O
different	O
dimensionality	O
for	O
the	O
hidden	O
variables	O
z	O
we	O
cannot	O
therefore	O
simply	O
consider	O
a	O
factorized	O
approximation	O
qzqm	O
but	O
must	O
instead	O
recognize	O
that	O
the	O
posterior	O
over	O
z	O
must	O
be	O
conditioned	O
on	O
m	O
and	O
so	O
we	O
must	O
consider	O
qz	O
m	O
qzmqm	O
we	O
can	O
readily	O
verify	O
the	O
following	O
decomposition	O
based	O
on	O
this	O
variational	B
distribution	O
pz	O
mx	O
qzmqm	O
ln	O
px	O
lm	O
qzmqm	O
ln	O
where	O
the	O
lm	O
is	O
a	O
lower	B
bound	I
on	O
ln	O
px	O
and	O
is	O
given	O
by	O
lm	O
qzmqm	O
ln	O
pz	O
x	O
m	O
qzmqm	O
m	O
z	O
m	O
z	O
here	O
we	O
are	O
assuming	O
discrete	O
z	O
but	O
the	O
same	O
analysis	O
applies	O
to	O
continuous	O
latent	O
variables	O
provided	O
the	O
summations	O
are	O
replaced	O
with	O
integrations	O
we	O
can	O
maximize	O
lm	O
with	O
respect	O
to	O
the	O
distribution	O
qm	O
using	O
a	O
lagrange	B
multiplier	I
with	O
the	O
result	O
qm	O
pm	O
explm	O
however	O
if	O
we	O
maximize	O
lm	O
with	O
respect	O
to	O
the	O
qzm	O
we	O
find	O
that	O
the	O
solutions	O
for	O
different	O
m	O
are	O
coupled	O
as	O
we	O
expect	O
because	O
they	O
are	O
conditioned	O
on	O
m	O
we	O
proceed	O
instead	O
by	O
first	O
optimizing	O
each	O
of	O
the	O
qzm	O
individually	O
by	O
optimization	O
exercise	O
exercise	O
approximate	O
inference	B
of	O
and	O
then	O
subsequently	O
determining	O
the	O
qm	O
using	O
after	O
normalization	O
the	O
resulting	O
values	O
for	O
qm	O
can	O
be	O
used	O
for	O
model	B
selection	I
or	O
model	B
averaging	I
in	O
the	O
usual	O
way	O
illustration	O
variational	B
mixture	B
of	I
gaussians	I
we	O
now	O
return	O
to	O
our	O
discussion	O
of	O
the	O
gaussian	B
mixture	B
model	I
and	O
apply	O
the	O
variational	B
inference	B
machinery	O
developed	O
in	O
the	O
previous	O
section	O
this	O
will	O
provide	O
a	O
good	O
illustration	O
of	O
the	O
application	O
of	O
variational	B
methods	O
and	O
will	O
also	O
demonstrate	O
how	O
a	O
bayesian	B
treatment	O
elegantly	O
resolves	O
many	O
of	O
the	O
difficulties	O
associated	O
with	O
the	O
maximum	B
likelihood	I
approach	O
the	O
reader	O
is	O
encouraged	O
to	O
work	O
through	O
this	O
example	O
in	O
detail	O
as	O
it	O
provides	O
many	O
insights	O
into	O
the	O
practical	O
application	O
of	O
variational	B
methods	O
many	O
bayesian	B
models	O
corresponding	O
to	O
much	O
more	O
sophisticated	O
distributions	O
can	O
be	O
solved	O
by	O
straightforward	O
extensions	O
and	O
generalizations	O
of	O
this	O
analysis	O
our	O
starting	O
point	O
is	O
the	O
likelihood	B
function	I
for	O
the	O
gaussian	B
mixture	B
model	I
illustrated	O
by	O
the	O
graphical	B
model	I
in	O
figure	O
for	O
each	O
observation	O
xn	O
we	O
have	O
a	O
corresponding	O
latent	B
variable	I
zn	O
comprising	O
a	O
binary	O
vector	O
with	O
elements	O
znk	O
for	O
k	O
k	O
as	O
before	O
we	O
denote	O
the	O
observed	O
data	O
set	O
by	O
x	O
xn	O
and	O
similarly	O
we	O
denote	O
the	O
latent	O
variables	O
by	O
z	O
zn	O
from	O
we	O
can	O
write	O
down	O
the	O
conditional	B
distribution	O
of	O
z	O
given	O
the	O
mixing	O
coefficients	O
in	O
the	O
form	O
pz	O
znk	O
k	O
similarly	O
from	O
we	O
can	O
write	O
down	O
the	O
conditional	B
distribution	O
of	O
the	O
observed	O
data	O
vectors	O
given	O
the	O
latent	O
variables	O
and	O
the	O
component	O
parameters	O
pxz	O
xn	O
k	O
k	O
where	O
k	O
and	O
k	O
note	O
that	O
we	O
are	O
working	O
in	O
terms	O
of	O
precision	O
matrices	O
rather	O
than	O
covariance	B
matrices	O
as	O
this	O
somewhat	O
simplifies	O
the	O
mathematics	O
next	O
we	O
introduce	O
priors	O
over	O
the	O
parameters	O
and	O
the	O
analysis	O
is	O
considerably	O
simplified	O
if	O
we	O
use	O
conjugate	B
prior	B
distributions	O
we	O
therefore	O
choose	O
a	O
dirichlet	B
distribution	I
over	O
the	O
mixing	O
coefficients	O
p	O
dir	O
c	O
k	O
where	O
by	O
symmetry	O
we	O
have	O
chosen	O
the	O
same	O
parameter	O
for	O
each	O
of	O
the	O
components	O
and	O
c	O
is	O
the	O
normalization	O
constant	O
for	O
the	O
dirichlet	B
distribution	I
defined	O
section	O
illustration	O
variational	B
mixture	B
of	I
gaussians	I
figure	O
directed	B
acyclic	I
graph	I
representing	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
model	O
in	O
which	O
the	O
box	O
denotes	O
a	O
set	O
of	O
n	O
i	O
i	O
d	O
observations	O
here	O
denotes	O
k	O
and	O
denotes	O
k	O
zn	O
xn	O
n	O
section	O
by	O
as	O
we	O
have	O
seen	O
the	O
parameter	O
can	O
be	O
interpreted	O
as	O
the	O
effective	O
prior	B
number	O
of	O
observations	O
associated	O
with	O
each	O
component	O
of	O
the	O
mixture	B
if	O
the	O
value	O
of	O
is	O
small	O
then	O
the	O
posterior	O
distribution	O
will	O
be	O
influenced	O
primarily	O
by	O
the	O
data	O
rather	O
than	O
by	O
the	O
prior	B
similarly	O
we	O
introduce	O
an	O
independent	B
gaussian-wishart	O
prior	B
governing	O
the	O
mean	B
and	O
precision	O
of	O
each	O
gaussian	B
component	O
given	O
by	O
p	O
p	O
w	O
k	O
section	O
because	O
this	O
represents	O
the	O
conjugate	B
prior	B
distribution	O
when	O
both	O
the	O
mean	B
and	O
precision	O
are	O
unknown	O
typically	O
we	O
would	O
choose	O
by	O
symmetry	O
the	O
resulting	O
model	O
can	O
be	O
represented	O
as	O
a	O
directed	B
graph	O
as	O
shown	O
in	O
figure	O
note	O
that	O
there	O
is	O
a	O
link	B
from	O
to	O
since	O
the	O
variance	B
of	O
the	O
distribution	O
over	O
in	O
is	O
a	O
function	O
of	O
this	O
example	O
provides	O
a	O
nice	O
illustration	O
of	O
the	O
distinction	O
between	O
latent	O
variables	O
and	O
parameters	O
variables	O
such	O
as	O
zn	O
that	O
appear	O
inside	O
the	O
plate	B
are	O
regarded	O
as	O
latent	O
variables	O
because	O
the	O
number	O
of	O
such	O
variables	O
grows	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
by	O
contrast	O
variables	O
such	O
as	O
that	O
are	O
outside	O
the	O
plate	B
are	O
fixed	O
in	O
number	O
independently	O
of	O
the	O
size	O
of	O
the	O
data	O
set	O
and	O
so	O
are	O
regarded	O
as	O
parameters	O
from	O
the	O
perspective	O
of	O
graphical	O
models	O
however	O
there	O
is	O
really	O
no	O
fundamental	O
difference	O
between	O
them	O
variational	B
distribution	O
in	O
order	O
to	O
formulate	O
a	O
variational	B
treatment	O
of	O
this	O
model	O
we	O
next	O
write	O
down	O
the	O
joint	O
distribution	O
of	O
all	O
of	O
the	O
random	O
variables	O
which	O
is	O
given	O
by	O
px	O
z	O
pxz	O
in	O
which	O
the	O
various	O
factors	O
are	O
defined	O
above	O
the	O
reader	O
should	O
take	O
a	O
moment	O
to	O
verify	O
that	O
this	O
decomposition	O
does	O
indeed	O
correspond	O
to	O
the	O
probabilistic	O
graphical	B
model	I
shown	O
in	O
figure	O
note	O
that	O
only	O
the	O
variables	O
x	O
xn	O
are	O
observed	O
approximate	O
inference	B
we	O
now	O
consider	O
a	O
variational	B
distribution	O
which	O
factorizes	O
between	O
the	O
latent	O
variables	O
and	O
the	O
parameters	O
so	O
that	O
qz	O
qzq	O
it	O
is	O
remarkable	O
that	O
this	O
is	O
the	O
only	O
assumption	O
that	O
we	O
need	O
to	O
make	O
in	O
order	O
to	O
obtain	O
a	O
tractable	O
practical	O
solution	O
to	O
our	O
bayesian	B
mixture	B
model	I
in	O
particular	O
the	O
functional	B
form	O
of	O
the	O
factors	O
qz	O
and	O
q	O
will	O
be	O
determined	O
automatically	O
by	O
optimization	O
of	O
the	O
variational	B
distribution	O
note	O
that	O
we	O
are	O
omitting	O
the	O
subscripts	O
on	O
the	O
q	O
distributions	O
much	O
as	O
we	O
do	O
with	O
the	O
p	O
distributions	O
in	O
and	O
are	O
relying	O
on	O
the	O
arguments	O
to	O
distinguish	O
the	O
different	O
distributions	O
the	O
corresponding	O
sequential	O
update	O
equations	O
for	O
these	O
factors	O
can	O
be	O
easily	O
derived	O
by	O
making	O
use	O
of	O
the	O
general	O
result	O
let	O
us	O
consider	O
the	O
derivation	O
of	O
the	O
update	O
equation	O
for	O
the	O
factor	O
qz	O
the	O
log	O
of	O
the	O
optimized	O
factor	O
is	O
given	O
by	O
ln	O
e	O
px	O
z	O
const	O
we	O
now	O
make	O
use	O
of	O
the	O
decomposition	O
note	O
that	O
we	O
are	O
only	O
interested	O
in	O
the	O
functional	B
dependence	O
of	O
the	O
right-hand	O
side	O
on	O
the	O
variable	O
z	O
thus	O
any	O
terms	O
that	O
do	O
not	O
depend	O
on	O
z	O
can	O
be	O
absorbed	O
into	O
the	O
additive	O
normalization	O
constant	O
giving	O
ln	O
e	O
pz	O
e	O
pxz	O
const	O
substituting	O
for	O
the	O
two	O
conditional	B
distributions	O
on	O
the	O
right-hand	O
side	O
and	O
again	O
absorbing	O
any	O
terms	O
that	O
are	O
independent	B
of	O
z	O
into	O
the	O
additive	O
constant	O
we	O
have	O
ln	O
znk	O
ln	O
nk	O
const	O
where	O
we	O
have	O
defined	O
ln	O
nk	O
eln	O
k	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
the	O
data	O
variable	O
x	O
taking	O
the	O
exponential	O
of	O
both	O
sides	O
of	O
we	O
obtain	O
e	O
k	O
k	O
e	O
k	O
d	O
kt	O
kxn	O
k	O
znk	O
nk	O
rznk	O
nk	O
exercise	O
requiring	O
that	O
this	O
distribution	O
be	O
normalized	O
and	O
noting	O
that	O
for	O
each	O
value	O
of	O
n	O
the	O
quantities	O
znk	O
are	O
binary	O
and	O
sum	O
to	O
over	O
all	O
values	O
of	O
k	O
we	O
obtain	O
where	O
illustration	O
variational	B
mixture	B
of	I
gaussians	I
rnk	O
nk	O
nj	O
we	O
see	O
that	O
the	O
optimal	O
solution	O
for	O
the	O
factor	O
qz	O
takes	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
pz	O
note	O
that	O
because	O
nk	O
is	O
given	O
by	O
the	O
exponential	O
of	O
a	O
real	O
quantity	O
the	O
quantities	O
rnk	O
will	O
be	O
nonnegative	O
and	O
will	O
sum	O
to	O
one	O
as	O
required	O
for	O
the	O
discrete	O
distribution	O
we	O
have	O
the	O
standard	O
result	O
eznk	O
rnk	O
from	O
which	O
we	O
see	O
that	O
the	O
quantities	O
rnk	O
are	O
playing	O
the	O
role	O
of	O
responsibilities	O
note	O
that	O
the	O
optimal	O
solution	O
for	O
depends	O
on	O
moments	O
evaluated	O
with	O
respect	O
to	O
the	O
distributions	O
of	O
other	O
variables	O
and	O
so	O
again	O
the	O
variational	B
update	O
equations	O
are	O
coupled	O
and	O
must	O
be	O
solved	O
iteratively	O
at	O
this	O
point	O
we	O
shall	O
find	O
it	O
convenient	O
to	O
define	O
three	O
statistics	O
of	O
the	O
observed	O
data	O
set	O
evaluated	O
with	O
respect	O
to	O
the	O
responsibilities	O
given	O
by	O
nk	O
nk	O
nk	O
xk	O
sk	O
rnkxn	O
rnkxn	O
xkxn	O
xkt	O
rnk	O
note	O
that	O
these	O
are	O
analogous	O
to	O
quantities	O
evaluated	O
in	O
the	O
maximum	B
likelihood	I
em	B
algorithm	I
for	O
the	O
gaussian	B
mixture	B
model	I
now	O
let	O
us	O
consider	O
the	O
factor	O
q	O
in	O
the	O
variational	B
posterior	O
distribu	O
tion	O
again	O
using	O
the	O
general	O
result	O
we	O
have	O
ln	O
ln	O
p	O
ln	O
p	O
k	O
k	O
ez	O
pz	O
eznk	O
xn	O
k	O
k	O
const	O
we	O
observe	O
that	O
the	O
right-hand	O
side	O
of	O
this	O
expression	O
decomposes	O
into	O
a	O
sum	O
of	O
terms	O
involving	O
only	O
together	O
with	O
terms	O
only	O
involving	O
and	O
which	O
implies	O
that	O
the	O
variational	B
posterior	O
q	O
factorizes	O
to	O
give	O
q	O
furthermore	O
the	O
terms	O
involving	O
and	O
themselves	O
comprise	O
a	O
sum	O
over	O
k	O
of	O
terms	O
involving	O
k	O
and	O
k	O
leading	O
to	O
the	O
further	O
factorization	B
q	O
q	O
q	O
k	O
k	O
approximate	O
inference	B
identifying	O
the	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
that	O
depend	O
on	O
we	O
have	O
ln	O
ln	O
k	O
rnk	O
ln	O
k	O
const	O
where	O
we	O
have	O
used	O
taking	O
the	O
exponential	O
of	O
both	O
sides	O
we	O
recognize	O
as	O
a	O
dirichlet	B
distribution	I
dir	O
exercise	O
exercise	O
where	O
has	O
components	O
k	O
given	O
by	O
k	O
nk	O
finally	O
the	O
variational	B
posterior	O
distribution	O
k	O
k	O
does	O
not	O
factorize	O
into	O
the	O
product	O
of	O
the	O
marginals	O
but	O
we	O
can	O
always	O
use	O
the	O
product	B
rule	I
to	O
write	O
it	O
in	O
the	O
form	O
k	O
k	O
k	O
k	O
the	O
two	O
factors	O
can	O
be	O
found	O
by	O
inspecting	O
and	O
reading	O
off	O
those	O
terms	O
that	O
involve	O
k	O
and	O
k	O
the	O
result	O
as	O
expected	O
is	O
a	O
gaussian-wishart	B
distribution	I
and	O
is	O
given	O
by	O
kmk	O
k	O
k	O
k	O
k	O
w	O
kwk	O
k	O
where	O
we	O
have	O
defined	O
k	O
nk	O
mk	O
k	O
nkxk	O
w	O
w	O
k	O
k	O
nk	O
nksk	O
nk	O
these	O
update	O
equations	O
are	O
analogous	O
to	O
the	O
m-step	O
equations	O
of	O
the	O
em	B
algorithm	I
for	O
the	O
maximum	B
likelihood	I
solution	O
of	O
the	O
mixture	B
of	I
gaussians	I
we	O
see	O
that	O
the	O
computations	O
that	O
must	O
be	O
performed	O
in	O
order	O
to	O
update	O
the	O
variational	B
posterior	O
distribution	O
over	O
the	O
model	O
parameters	O
involve	O
evaluation	O
of	O
the	O
same	O
sums	O
over	O
the	O
data	O
set	O
as	O
arose	O
in	O
the	O
maximum	B
likelihood	I
treatment	O
in	O
order	O
to	O
perform	O
this	O
variational	B
m	O
step	O
we	O
need	O
the	O
expectations	O
eznk	O
rnk	O
representing	O
the	O
responsibilities	O
these	O
are	O
obtained	O
by	O
normalizing	O
the	O
nk	O
that	O
are	O
given	O
by	O
we	O
see	O
that	O
this	O
expression	O
involves	O
expectations	O
with	O
respect	O
to	O
the	O
variational	B
distributions	O
of	O
the	O
parameters	O
and	O
these	O
are	O
easily	O
evaluated	O
to	O
give	O
kt	O
kxn	O
k	O
e	O
k	O
k	O
d	O
k	O
e	O
k	O
k	O
e	O
k	O
k	O
k	O
kxn	O
mktwkxn	O
mk	O
k	O
i	O
d	O
ln	O
lnwk	O
appendix	O
b	O
illustration	O
variational	B
mixture	B
of	I
gaussians	I
where	O
we	O
have	O
introduced	O
definitions	O
k	O
k	O
and	O
is	O
the	O
digamma	B
function	I
defined	O
by	O
if	O
we	O
substitute	O
and	O
into	O
and	O
make	O
use	O
of	O
the	O
standard	O
properties	O
of	O
the	O
wishart	O
and	O
dirichlet	B
distributions	O
we	O
obtain	O
the	O
following	O
result	O
for	O
the	O
responsibilities	O
k	O
k	O
the	O
results	O
and	O
follow	O
from	O
rnk	O
k	O
k	O
exp	O
d	O
k	O
k	O
mktwkxn	O
mk	O
notice	O
the	O
similarity	O
to	O
the	O
corresponding	O
result	O
for	O
the	O
responsibilities	O
in	O
maximum	B
likelihood	I
em	B
which	O
from	O
can	O
be	O
written	O
in	O
the	O
form	O
kt	O
kxn	O
k	O
rnk	O
k	O
exp	O
where	O
we	O
have	O
used	O
the	O
precision	O
in	O
place	O
of	O
the	O
covariance	B
to	O
highlight	O
the	O
similarity	O
to	O
thus	O
the	O
optimization	O
of	O
the	O
variational	B
posterior	O
distribution	O
involves	O
cycling	O
between	O
two	O
stages	O
analogous	O
to	O
the	O
e	O
and	O
m	O
steps	O
of	O
the	O
maximum	B
likelihood	I
em	B
algorithm	I
in	O
the	O
variational	B
equivalent	O
of	O
the	O
e	O
step	O
we	O
use	O
the	O
current	O
distributions	O
over	O
the	O
model	O
parameters	O
to	O
evaluate	O
the	O
moments	O
in	O
and	O
and	O
hence	O
evaluate	O
eznk	O
rnk	O
then	O
in	O
the	O
subsequent	O
variational	B
equivalent	O
of	O
the	O
m	O
step	O
we	O
keep	O
these	O
responsibilities	O
fixed	O
and	O
use	O
them	O
to	O
re-compute	O
the	O
variational	B
distribution	O
over	O
the	O
parameters	O
using	O
and	O
in	O
each	O
case	O
we	O
see	O
that	O
the	O
variational	B
posterior	O
distribution	O
has	O
the	O
same	O
functional	B
form	O
as	O
the	O
corresponding	O
factor	O
in	O
the	O
joint	O
distribution	O
this	O
is	O
a	O
general	O
result	O
and	O
is	O
a	O
consequence	O
of	O
the	O
choice	O
of	O
conjugate	B
distributions	O
figure	O
shows	O
the	O
results	O
of	O
applying	O
this	O
approach	O
to	O
the	O
rescaled	O
old	B
faithful	I
data	I
set	O
for	O
a	O
gaussian	B
mixture	B
model	I
having	O
k	O
components	O
we	O
see	O
that	O
after	O
convergence	O
there	O
are	O
only	O
two	O
components	O
for	O
which	O
the	O
expected	O
values	O
of	O
the	O
mixing	O
coefficients	O
are	O
numerically	O
distinguishable	O
from	O
their	O
prior	B
values	O
this	O
effect	O
can	O
be	O
understood	O
qualitatively	O
in	O
terms	O
of	O
the	O
automatic	O
trade-off	O
in	O
a	O
bayesian	B
model	O
between	O
fitting	O
the	O
data	O
and	O
the	O
complexity	O
of	O
the	O
model	O
in	O
which	O
the	O
complexity	O
penalty	O
arises	O
from	O
components	O
whose	O
parameters	O
are	O
pushed	O
away	O
from	O
their	O
prior	B
values	O
components	O
that	O
take	O
essentially	O
no	O
responsibility	B
for	O
explaining	O
the	O
data	O
points	O
have	O
rnk	O
and	O
hence	O
nk	O
from	O
we	O
see	O
that	O
k	O
and	O
from	O
we	O
see	O
that	O
the	O
other	O
parameters	O
revert	O
to	O
their	O
prior	B
values	O
in	O
principle	O
such	O
components	O
are	O
fitted	O
slightly	O
to	O
the	O
data	O
points	O
but	O
for	O
broad	O
priors	O
this	O
effect	O
is	O
too	O
small	O
to	O
be	O
seen	O
numerically	O
for	O
the	O
variational	B
gaussian	B
mixture	B
model	I
the	O
expected	O
values	O
of	O
the	O
mixing	O
coefficients	O
in	O
the	O
posterior	O
distribution	O
are	O
given	O
by	O
section	O
section	O
exercise	O
consider	O
a	O
component	O
for	O
which	O
nk	O
and	O
k	O
if	O
the	O
prior	B
is	O
broad	O
so	O
that	O
then	O
e	O
k	O
and	O
the	O
component	O
plays	O
no	O
role	O
in	O
the	O
model	O
whereas	O
if	O
e	O
k	O
k	O
nk	O
k	O
n	O
approximate	O
inference	B
figure	O
variational	B
bayesian	B
mixture	B
of	O
k	O
gaussians	O
applied	O
to	O
the	O
old	B
faithful	I
data	I
set	O
in	O
which	O
the	O
ellipses	O
denote	O
the	O
one	O
standard-deviation	O
density	B
contours	O
for	O
each	O
of	O
the	O
components	O
and	O
the	O
density	B
of	O
red	O
ink	O
inside	O
each	O
ellipse	O
corresponds	O
to	O
the	O
mean	B
value	O
of	O
the	O
mixing	B
coefficient	I
for	O
each	O
component	O
the	O
number	O
in	O
the	O
top	O
left	O
of	O
each	O
diagram	O
shows	O
the	O
number	O
of	O
iterations	O
of	O
variational	B
inference	B
components	O
whose	O
expected	O
mixing	B
coefficient	I
are	O
numerically	O
indistinguishable	O
from	O
zero	O
are	O
not	O
plotted	O
the	O
prior	B
tightly	O
constrains	O
the	O
mixing	O
coefficients	O
so	O
that	O
then	O
e	O
k	O
in	O
figure	O
the	O
prior	B
over	O
the	O
mixing	O
coefficients	O
is	O
a	O
dirichlet	B
of	O
the	O
form	O
recall	O
from	O
figure	O
that	O
for	O
the	O
prior	B
favours	O
solutions	O
in	O
which	O
some	O
of	O
the	O
mixing	O
coefficients	O
are	O
zero	O
figure	O
was	O
obtained	O
using	O
and	O
resulted	O
in	O
two	O
components	O
having	O
nonzero	O
mixing	O
coefficients	O
if	O
instead	O
we	O
choose	O
we	O
obtain	O
three	O
components	O
with	O
nonzero	O
mixing	O
coefficients	O
and	O
for	O
all	O
six	O
components	O
have	O
nonzero	O
mixing	O
coefficients	O
as	O
we	O
have	O
seen	O
there	O
is	O
a	O
close	O
similarity	O
between	O
the	O
variational	B
solution	O
for	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
and	O
the	O
em	B
algorithm	I
for	O
maximum	B
likelihood	I
in	O
fact	O
if	O
we	O
consider	O
the	O
limit	O
n	O
then	O
the	O
bayesian	B
treatment	O
converges	O
to	O
the	O
maximum	B
likelihood	I
em	B
algorithm	I
for	O
anything	O
other	O
than	O
very	O
small	O
data	O
sets	O
the	O
dominant	O
computational	O
cost	O
of	O
the	O
variational	B
algorithm	O
for	O
gaussian	B
mixtures	O
arises	O
from	O
the	O
evaluation	O
of	O
the	O
responsibilities	O
together	O
with	O
the	O
evaluation	O
and	O
inversion	O
of	O
the	O
weighted	O
data	O
covariance	B
matrices	O
these	O
computations	O
mirror	O
precisely	O
those	O
that	O
arise	O
in	O
the	O
maximum	B
likelihood	I
em	B
algorithm	I
and	O
so	O
there	O
is	O
little	O
computational	O
overhead	O
in	O
using	O
this	O
bayesian	B
approach	O
as	O
compared	O
to	O
the	O
traditional	O
maximum	B
likelihood	I
one	O
there	O
are	O
however	O
some	O
substantial	O
advantages	O
first	O
of	O
all	O
the	O
singularities	B
that	O
arise	O
in	O
maximum	B
likelihood	I
when	O
a	O
gaussian	B
component	O
collapses	O
onto	O
a	O
specific	O
data	O
point	O
are	O
absent	O
in	O
the	O
bayesian	B
treatment	O
illustration	O
variational	B
mixture	B
of	I
gaussians	I
indeed	O
these	O
singularities	B
are	O
removed	O
if	O
we	O
simply	O
introduce	O
a	O
prior	B
and	O
then	O
use	O
a	O
map	O
estimate	O
instead	O
of	O
maximum	B
likelihood	I
furthermore	O
there	O
is	O
no	O
over-fitting	B
if	O
we	O
choose	O
a	O
large	O
number	O
k	O
of	O
components	O
in	O
the	O
mixture	B
as	O
we	O
saw	O
in	O
figure	O
finally	O
the	O
variational	B
treatment	O
opens	O
up	O
the	O
possibility	O
of	O
determining	O
the	O
optimal	O
number	O
of	O
components	O
in	O
the	O
mixture	B
without	O
resorting	O
to	O
techniques	O
such	O
as	O
cross	O
validation	O
section	O
variational	B
lower	B
bound	I
we	O
can	O
also	O
straightforwardly	O
evaluate	O
the	O
lower	B
bound	I
for	O
this	O
model	O
in	O
practice	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
monitor	O
the	O
bound	O
during	O
the	O
re-estimation	O
in	O
order	O
to	O
test	O
for	O
convergence	O
it	O
can	O
also	O
provide	O
a	O
valuable	O
check	O
on	O
both	O
the	O
mathematical	O
expressions	O
for	O
the	O
solutions	O
and	O
their	O
software	O
implementation	O
because	O
at	O
each	O
step	O
of	O
the	O
iterative	O
re-estimation	O
procedure	O
the	O
value	O
of	O
this	O
bound	O
should	O
not	O
decrease	O
we	O
can	O
take	O
this	O
a	O
stage	O
further	O
to	O
provide	O
a	O
deeper	O
test	O
of	O
the	O
correctness	O
of	O
both	O
the	O
mathematical	O
derivation	O
of	O
the	O
update	O
equations	O
and	O
of	O
their	O
software	O
implementation	O
by	O
using	O
finite	O
differences	O
to	O
check	O
that	O
each	O
update	O
does	O
indeed	O
give	O
a	O
maximum	O
of	O
the	O
bound	O
en	O
and	O
bishop	O
for	O
the	O
variational	B
mixture	B
of	I
gaussians	I
the	O
lower	B
bound	I
is	O
given	O
by	O
l	O
px	O
z	O
qz	O
ln	O
qz	O
eln	O
px	O
z	O
eln	O
qz	O
eln	O
pxz	O
eln	O
pz	O
eln	O
p	O
eln	O
p	O
d	O
d	O
d	O
z	O
eln	O
qz	O
eln	O
q	O
eln	O
q	O
where	O
to	O
keep	O
the	O
notation	O
uncluttered	O
we	O
have	O
omitted	O
the	O
superscript	O
on	O
the	O
q	O
distributions	O
along	O
with	O
the	O
subscripts	O
on	O
the	O
expectation	B
operators	O
because	O
each	O
expectation	B
is	O
taken	O
with	O
respect	O
to	O
all	O
of	O
the	O
random	O
variables	O
in	O
its	O
argument	O
the	O
various	O
terms	O
in	O
the	O
bound	O
are	O
easily	O
evaluated	O
to	O
give	O
the	O
following	O
results	O
exercise	O
k	O
d	O
nk	O
kxk	O
mktwkxk	O
mk	O
d	O
k	O
ktrskwk	O
rnk	O
k	O
eln	O
pxz	O
eln	O
pz	O
eln	O
p	O
ln	O
c	O
k	O
approximate	O
inference	B
eln	O
p	O
d	O
ln	O
k	O
d	O
k	O
k	O
ln	O
ktrw	O
wk	O
kmk	O
d	O
k	O
k	O
k	O
ln	O
c	O
k	O
d	O
rnk	O
ln	O
rnk	O
ln	O
k	O
eln	O
qz	O
eln	O
q	O
eln	O
q	O
h	O
k	O
d	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
hq	O
k	O
is	O
the	O
entropy	B
of	O
the	O
wishart	B
distribution	I
given	O
by	O
and	O
the	O
coefficients	O
c	O
and	O
bw	O
are	O
defined	O
by	O
and	O
respectively	O
note	O
that	O
the	O
terms	O
involving	O
expectations	O
of	O
the	O
logs	O
of	O
the	O
q	O
distributions	O
simply	O
represent	O
the	O
negative	O
entropies	O
of	O
those	O
distributions	O
some	O
simplifications	O
and	O
combination	O
of	O
terms	O
can	O
be	O
performed	O
when	O
these	O
expressions	O
are	O
summed	O
to	O
give	O
the	O
lower	B
bound	I
however	O
we	O
have	O
kept	O
the	O
expressions	O
separate	O
for	O
ease	O
of	O
understanding	O
finally	O
it	O
is	O
worth	O
noting	O
that	O
the	O
lower	B
bound	I
provides	O
an	O
alternative	O
approach	O
for	O
deriving	O
the	O
variational	B
re-estimation	O
equations	O
obtained	O
in	O
section	O
to	O
do	O
this	O
we	O
use	O
the	O
fact	O
that	O
since	O
the	O
model	O
has	O
conjugate	B
priors	O
the	O
functional	B
form	O
of	O
the	O
factors	O
in	O
the	O
variational	B
posterior	O
distribution	O
is	O
known	O
namely	O
discrete	O
for	O
z	O
dirichlet	B
for	O
and	O
gaussian-wishart	O
for	O
k	O
k	O
by	O
taking	O
general	O
parametric	O
forms	O
for	O
these	O
distributions	O
we	O
can	O
derive	O
the	O
form	O
of	O
the	O
lower	B
bound	I
as	O
a	O
function	O
of	O
the	O
parameters	O
of	O
the	O
distributions	O
maximizing	O
the	O
bound	O
with	O
respect	O
to	O
these	O
parameters	O
then	O
gives	O
the	O
required	O
re-estimation	O
equations	O
exercise	O
predictive	O
density	B
in	O
applications	O
of	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
model	O
we	O
will	O
often	O
be	O
interested	O
in	O
the	O
predictive	O
density	B
for	O
a	O
new	O
of	O
the	O
observed	B
variable	I
associated	O
with	O
this	O
observation	O
will	O
be	O
a	O
corresponding	O
latent	O
and	O
the	O
pre	O
d	O
d	O
d	O
dictive	O
density	B
is	O
then	O
given	O
by	O
bz	O
illustration	O
variational	B
mixture	B
of	I
gaussians	I
where	O
p	O
is	O
the	O
true	O
posterior	O
distribution	O
of	O
the	O
parameters	O
using	O
and	O
we	O
can	O
first	O
perform	O
the	O
summation	O
to	O
give	O
k	O
p	O
d	O
d	O
d	O
k	O
because	O
the	O
remaining	O
integrations	O
are	O
intractable	O
we	O
approximate	O
the	O
predictive	O
density	B
by	O
replacing	O
the	O
true	O
posterior	O
distribution	O
p	O
with	O
its	O
variational	B
approximation	O
q	O
to	O
give	O
k	O
k	O
q	O
k	O
k	O
d	O
d	O
k	O
d	O
k	O
exercise	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
factorization	B
and	O
in	O
each	O
term	O
we	O
have	O
implicitly	O
integrated	O
out	O
all	O
variables	O
j	O
j	O
for	O
j	O
k	O
the	O
remaining	O
integrations	O
can	O
now	O
be	O
evaluated	O
analytically	O
giving	O
a	O
mixture	B
of	O
student	O
s	O
t-distributions	O
lk	O
k	O
d	O
in	O
which	O
the	O
kth	O
component	O
has	O
mean	B
mk	O
and	O
the	O
precision	O
is	O
given	O
by	O
lk	O
k	O
d	O
k	O
k	O
wk	O
exercise	O
in	O
which	O
k	O
is	O
given	O
by	O
when	O
the	O
size	O
n	O
of	O
the	O
data	O
set	O
is	O
large	O
the	O
predictive	B
distribution	I
reduces	O
to	O
a	O
mixture	B
of	I
gaussians	I
section	O
exercise	O
determining	O
the	O
number	O
of	O
components	O
we	O
have	O
seen	O
that	O
the	O
variational	B
lower	B
bound	I
can	O
be	O
used	O
to	O
determine	O
a	O
posterior	O
distribution	O
over	O
the	O
number	O
k	O
of	O
components	O
in	O
the	O
mixture	B
model	I
there	O
is	O
however	O
one	O
subtlety	O
that	O
needs	O
to	O
be	O
addressed	O
for	O
any	O
given	O
setting	O
of	O
the	O
parameters	O
in	O
a	O
gaussian	B
mixture	B
model	I
for	O
specific	O
degenerate	O
settings	O
there	O
will	O
exist	O
other	O
parameter	O
settings	O
for	O
which	O
the	O
density	B
over	O
the	O
observed	O
variables	O
will	O
be	O
identical	O
these	O
parameter	O
values	O
differ	O
only	O
through	O
a	O
re-labelling	O
of	O
the	O
components	O
for	O
instance	O
consider	O
a	O
mixture	B
of	O
two	O
gaussians	O
and	O
a	O
single	O
observed	B
variable	I
x	O
in	O
which	O
the	O
parameters	O
have	O
the	O
values	O
a	O
b	O
c	O
d	O
e	O
f	O
then	O
the	O
parameter	O
values	O
b	O
a	O
d	O
c	O
f	O
e	O
in	O
which	O
the	O
two	O
components	O
have	O
been	O
exchanged	O
will	O
by	O
symmetry	O
give	O
rise	O
to	O
the	O
same	O
value	O
of	O
px	O
if	O
we	O
have	O
a	O
mixture	B
model	I
comprising	O
k	O
components	O
then	O
each	O
parameter	O
setting	O
will	O
be	O
a	O
member	O
of	O
a	O
family	O
of	O
k	O
equivalent	O
settings	O
in	O
the	O
context	O
of	O
maximum	B
likelihood	I
this	O
redundancy	O
is	O
irrelevant	O
because	O
the	O
parameter	O
optimization	O
algorithm	O
example	O
em	B
will	O
depending	O
on	O
the	O
initialization	O
of	O
the	O
parameters	O
find	O
one	O
specific	O
solution	O
and	O
the	O
other	O
equivalent	O
solutions	O
play	O
no	O
role	O
in	O
a	O
bayesian	B
setting	O
however	O
we	O
marginalize	O
over	O
all	O
possible	O
approximate	O
inference	B
figure	O
plot	O
of	O
the	O
variational	B
lower	B
bound	I
l	O
versus	O
the	O
number	O
k	O
of	O
components	O
in	O
the	O
gaussian	B
mixture	B
model	I
for	O
the	O
old	B
faithful	I
data	I
showing	O
a	O
distinct	O
peak	O
at	O
k	O
components	O
for	O
each	O
value	O
of	O
k	O
the	O
model	O
is	O
trained	O
from	O
different	O
random	O
starts	O
and	O
the	O
results	O
shown	O
as	O
symbols	O
plotted	O
with	O
small	O
random	O
horizontal	O
perturbations	O
so	O
that	O
they	O
can	O
be	O
distinguished	O
note	O
that	O
some	O
solutions	O
find	O
suboptimal	O
local	B
maxima	O
but	O
that	O
this	O
happens	O
infrequently	O
pdk	O
k	O
parameter	O
values	O
we	O
have	O
seen	O
in	O
figure	O
that	O
if	O
the	O
true	O
posterior	O
distribution	O
is	O
multimodal	O
variational	B
inference	B
based	O
on	O
the	O
minimization	O
of	O
will	O
tend	O
to	O
approximate	O
the	O
distribution	O
in	O
the	O
neighbourhood	O
of	O
one	O
of	O
the	O
modes	O
and	O
ignore	O
the	O
others	O
again	O
because	O
equivalent	O
modes	O
have	O
equivalent	O
predictive	O
densities	O
this	O
is	O
of	O
no	O
concern	O
provided	O
we	O
are	O
considering	O
a	O
model	O
having	O
a	O
specific	O
number	O
k	O
of	O
components	O
if	O
however	O
we	O
wish	O
to	O
compare	O
different	O
values	O
of	O
k	O
then	O
we	O
need	O
to	O
take	O
account	O
of	O
this	O
multimodality	B
a	O
simple	O
approximate	O
solution	O
is	O
to	O
add	O
a	O
term	O
ln	O
k	O
onto	O
the	O
lower	B
bound	I
when	O
used	O
for	O
model	B
comparison	I
and	O
averaging	O
figure	O
shows	O
a	O
plot	O
of	O
the	O
lower	B
bound	I
including	O
the	O
multimodality	B
factor	O
versus	O
the	O
number	O
k	O
of	O
components	O
for	O
the	O
old	B
faithful	I
data	I
set	O
it	O
is	O
worth	O
emphasizing	O
once	O
again	O
that	O
maximum	B
likelihood	I
would	O
lead	O
to	O
values	O
of	O
the	O
likelihood	B
function	I
that	O
increase	O
monotonically	O
with	O
k	O
the	O
singular	O
solutions	O
have	O
been	O
avoided	O
and	O
discounting	O
the	O
effects	O
of	O
local	B
maxima	O
and	O
so	O
cannot	O
be	O
used	O
to	O
determine	O
an	O
appropriate	O
model	O
complexity	O
by	O
contrast	O
bayesian	B
inference	B
automatically	O
makes	O
the	O
trade-off	O
between	O
model	O
complexity	O
and	O
fitting	O
the	O
data	O
this	O
approach	O
to	O
the	O
determination	O
of	O
k	O
requires	O
that	O
a	O
range	O
of	O
models	O
having	O
different	O
k	O
values	O
be	O
trained	O
and	O
compared	O
an	O
alternative	O
approach	O
to	O
determining	O
a	O
suitable	O
value	O
for	O
k	O
is	O
to	O
treat	O
the	O
mixing	O
coefficients	O
as	O
parameters	O
and	O
make	O
point	O
estimates	O
of	O
their	O
values	O
by	O
maximizing	O
the	O
lower	B
bound	I
and	O
bishop	O
with	O
respect	O
to	O
instead	O
of	O
maintaining	O
a	O
probability	B
distribution	O
over	O
them	O
as	O
in	O
the	O
fully	O
bayesian	B
approach	O
this	O
leads	O
to	O
the	O
re-estimation	O
equation	O
k	O
n	O
rnk	O
and	O
this	O
maximization	O
is	O
interleaved	O
with	O
the	O
variational	B
updates	O
for	O
the	O
q	O
distribution	O
over	O
the	O
remaining	O
parameters	O
components	O
that	O
provide	O
insufficient	O
contribution	O
exercise	O
section	O
exercise	O
section	O
illustration	O
variational	B
mixture	B
of	I
gaussians	I
to	O
explaining	O
the	O
data	O
will	O
have	O
their	O
mixing	O
coefficients	O
driven	O
to	O
zero	O
during	O
the	O
optimization	O
and	O
so	O
they	O
are	O
effectively	O
removed	O
from	O
the	O
model	O
through	O
automatic	B
relevance	I
determination	I
this	O
allows	O
us	O
to	O
make	O
a	O
single	O
training	B
run	O
in	O
which	O
we	O
start	O
with	O
a	O
relatively	O
large	O
initial	O
value	O
of	O
k	O
and	O
allow	O
surplus	O
components	O
to	O
be	O
pruned	O
out	O
of	O
the	O
model	O
the	O
origins	O
of	O
the	O
sparsity	B
when	O
optimizing	O
with	O
respect	O
to	O
hyperparameters	O
is	O
discussed	O
in	O
detail	O
in	O
the	O
context	O
of	O
the	O
relevance	B
vector	I
machine	I
induced	O
factorizations	O
in	O
deriving	O
these	O
variational	B
update	O
equations	O
for	O
the	O
gaussian	B
mixture	B
model	I
we	O
assumed	O
a	O
particular	O
factorization	B
of	O
the	O
variational	B
posterior	O
distribution	O
given	O
by	O
however	O
the	O
optimal	O
solutions	O
for	O
the	O
various	O
factors	O
exhibit	O
additional	O
factorizations	O
in	O
particular	O
the	O
solution	O
for	O
is	O
given	O
by	O
the	O
product	O
of	O
an	O
independent	B
distribution	O
k	O
k	O
over	O
each	O
of	O
the	O
components	O
k	O
of	O
the	O
mixture	B
whereas	O
the	O
variational	B
posterior	O
distribution	O
over	O
the	O
latent	O
variables	O
given	O
by	O
factorizes	O
into	O
an	O
independent	B
distribution	O
for	O
each	O
observation	O
n	O
that	O
it	O
does	O
not	O
further	O
factorize	O
with	O
respect	O
to	O
k	O
because	O
for	O
each	O
value	O
of	O
n	O
the	O
znk	O
are	O
constrained	O
to	O
sum	O
to	O
one	O
over	O
k	O
these	O
additional	O
factorizations	O
are	O
a	O
consequence	O
of	O
the	O
interaction	O
between	O
the	O
assumed	O
factorization	B
and	O
the	O
conditional	B
independence	I
properties	O
of	O
the	O
true	O
distribution	O
as	O
characterized	O
by	O
the	O
directed	B
graph	O
in	O
figure	O
we	O
shall	O
refer	O
to	O
these	O
additional	O
factorizations	O
as	O
induced	O
factorizations	O
because	O
they	O
arise	O
from	O
an	O
interaction	O
between	O
the	O
factorization	B
assumed	O
in	O
the	O
variational	B
posterior	O
distribution	O
and	O
the	O
conditional	B
independence	I
properties	O
of	O
the	O
true	O
joint	O
distribution	O
in	O
a	O
numerical	O
implementation	O
of	O
the	O
variational	B
approach	O
it	O
is	O
important	O
to	O
take	O
account	O
of	O
such	O
additional	O
factorizations	O
for	O
instance	O
it	O
would	O
be	O
very	O
inefficient	O
to	O
maintain	O
a	O
full	O
precision	B
matrix	I
for	O
the	O
gaussian	B
distribution	O
over	O
a	O
set	O
of	O
variables	O
if	O
the	O
optimal	O
form	O
for	O
that	O
distribution	O
always	O
had	O
a	O
diagonal	B
precision	B
matrix	I
to	O
a	O
factorization	B
with	O
respect	O
to	O
the	O
individual	O
variables	O
described	O
by	O
that	O
gaussian	B
such	O
induced	O
factorizations	O
can	O
easily	O
be	O
detected	O
using	O
a	O
simple	O
graphical	O
test	O
based	O
on	O
d-separation	B
as	O
follows	O
we	O
partition	O
the	O
latent	O
variables	O
into	O
three	O
disjoint	O
groups	O
a	O
b	O
c	O
and	O
then	O
let	O
us	O
suppose	O
that	O
we	O
are	O
assuming	O
a	O
factorization	B
between	O
c	O
and	O
the	O
remaining	O
latent	O
variables	O
so	O
that	O
qa	O
b	O
c	O
qa	O
bqc	O
using	O
the	O
general	O
result	O
together	O
with	O
the	O
product	B
rule	I
for	O
probabilities	O
we	O
see	O
that	O
the	O
optimal	O
solution	O
for	O
qa	O
b	O
is	O
given	O
by	O
ln	O
b	O
ecln	O
px	O
a	O
b	O
c	O
const	O
ecln	O
pa	O
bx	O
c	O
const	O
we	O
now	O
ask	O
whether	O
this	O
resulting	O
solution	O
will	O
factorize	O
between	O
a	O
and	O
b	O
in	O
other	O
words	O
whether	O
b	O
this	O
will	O
happen	O
if	O
and	O
only	O
if	O
ln	O
pa	O
bx	O
c	O
ln	O
pax	O
c	O
ln	O
pbx	O
c	O
that	O
is	O
if	O
the	O
conditional	B
independence	I
relation	O
a	O
b	O
x	O
c	O
approximate	O
inference	B
is	O
satisfied	O
we	O
can	O
test	O
to	O
see	O
if	O
this	O
relation	O
does	O
hold	O
for	O
any	O
choice	O
of	O
a	O
and	O
b	O
by	O
making	O
use	O
of	O
the	O
d-separation	B
criterion	O
to	O
illustrate	O
this	O
consider	O
again	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
represented	O
by	O
the	O
directed	B
graph	O
in	O
figure	O
in	O
which	O
we	O
are	O
assuming	O
a	O
variational	B
factorization	B
given	O
by	O
we	O
can	O
see	O
immediately	O
that	O
the	O
variational	B
posterior	O
distribution	O
over	O
the	O
parameters	O
must	O
factorize	O
between	O
and	O
the	O
remaining	O
parameters	O
and	O
because	O
all	O
paths	O
connecting	O
to	O
either	O
or	O
must	O
pass	O
through	O
one	O
of	O
the	O
nodes	O
zn	O
all	O
of	O
which	O
are	O
in	O
the	O
conditioning	O
set	O
for	O
our	O
conditional	B
independence	I
test	O
and	O
all	O
of	O
which	O
are	O
head-to-tail	O
with	O
respect	O
to	O
such	O
paths	O
variational	B
linear	B
regression	B
exercise	O
as	O
a	O
second	O
illustration	O
of	O
variational	B
inference	B
we	O
return	O
to	O
the	O
bayesian	B
linear	B
regression	B
model	O
of	O
section	O
in	O
the	O
evidence	O
framework	O
we	O
approximated	O
the	O
integration	O
over	O
and	O
by	O
making	O
point	O
estimates	O
obtained	O
by	O
maximizing	O
the	O
log	O
marginal	B
likelihood	I
a	O
fully	O
bayesian	B
approach	O
would	O
integrate	O
over	O
the	O
hyperparameters	O
as	O
well	O
as	O
over	O
the	O
parameters	O
although	O
exact	O
integration	O
is	O
intractable	O
we	O
can	O
use	O
variational	B
methods	O
to	O
find	O
a	O
tractable	O
approximation	O
in	O
order	O
to	O
simplify	O
the	O
discussion	O
we	O
shall	O
suppose	O
that	O
the	O
noise	O
precision	B
parameter	I
is	O
known	O
and	O
is	O
fixed	O
to	O
its	O
true	O
value	O
although	O
the	O
framework	O
is	O
easily	O
extended	B
to	O
include	O
the	O
distribution	O
over	O
for	O
the	O
linear	B
regression	B
model	O
the	O
variational	B
treatment	O
will	O
turn	O
out	O
to	O
be	O
equivalent	O
to	O
the	O
evidence	O
framework	O
nevertheless	O
it	O
provides	O
a	O
good	O
exercise	O
in	O
the	O
use	O
of	O
variational	B
methods	O
and	O
will	O
also	O
lay	O
the	O
foundation	O
for	O
variational	B
treatment	O
of	O
bayesian	B
logistic	B
regression	B
in	O
section	O
recall	O
that	O
the	O
likelihood	B
function	I
for	O
w	O
and	O
the	O
prior	B
over	O
w	O
are	O
given	O
by	O
n	O
n	O
ptw	O
pw	O
n	O
where	O
n	O
we	O
now	O
introduce	O
a	O
prior	B
distribution	O
over	O
from	O
our	O
discussion	O
in	O
section	O
we	O
know	O
that	O
the	O
conjugate	B
prior	B
for	O
the	O
precision	O
of	O
a	O
gaussian	B
is	O
given	O
by	O
a	O
gamma	B
distribution	I
and	O
so	O
we	O
choose	O
where	O
gam	O
is	O
defined	O
by	O
thus	O
the	O
joint	O
distribution	O
of	O
all	O
the	O
variables	O
is	O
given	O
by	O
p	O
gam	O
pt	O
w	O
ptwpw	O
this	O
can	O
be	O
represented	O
as	O
a	O
directed	B
graphical	B
model	I
as	O
shown	O
in	O
figure	O
variational	B
distribution	O
our	O
first	O
goal	O
is	O
to	O
find	O
an	O
approximation	O
to	O
the	O
posterior	O
distribution	O
pw	O
to	O
do	O
this	O
we	O
employ	O
the	O
variational	B
framework	O
of	O
section	O
with	O
a	O
variational	B
variational	B
linear	B
regression	B
figure	O
probabilistic	O
graphical	B
model	I
representing	O
the	O
joint	O
disregression	O
the	O
bayesian	B
linear	O
for	O
tribution	O
model	O
w	O
n	O
tn	O
n	O
posterior	O
distribution	O
given	O
by	O
the	O
factorized	O
expression	O
qw	O
qwq	O
we	O
can	O
find	O
re-estimation	O
equations	O
for	O
the	O
factors	O
in	O
this	O
distribution	O
by	O
making	O
use	O
of	O
the	O
general	O
result	O
recall	O
that	O
for	O
each	O
factor	O
we	O
take	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
over	O
all	O
variables	O
and	O
then	O
average	O
with	O
respect	O
to	O
those	O
variables	O
not	O
in	O
that	O
factor	O
consider	O
first	O
the	O
distribution	O
over	O
keeping	O
only	O
terms	O
that	O
have	O
a	O
functional	B
dependence	O
on	O
we	O
have	O
ln	O
ln	O
p	O
ew	O
pw	O
const	O
ln	O
m	O
ln	O
ewtw	O
const	O
we	O
recognize	O
this	O
as	O
the	O
log	O
of	O
a	O
gamma	B
distribution	I
and	O
so	O
identifying	O
the	O
coefficients	O
of	O
and	O
ln	O
we	O
obtain	O
gam	O
bn	O
where	O
an	O
m	O
ewtw	O
bn	O
similarly	O
we	O
can	O
find	O
the	O
variational	B
re-estimation	O
equation	O
for	O
the	O
posterior	O
distribution	O
over	O
w	O
again	O
using	O
the	O
general	O
result	O
and	O
keeping	O
only	O
those	O
terms	O
that	O
have	O
a	O
functional	B
dependence	O
on	O
w	O
we	O
have	O
ln	O
ln	O
ptw	O
e	O
pw	O
const	O
n	O
e	O
const	O
wt	O
e	O
t	O
w	O
wt	O
tt	O
const	O
because	O
this	O
is	O
a	O
quadratic	O
form	O
the	O
distribution	O
is	O
gaussian	B
and	O
so	O
we	O
can	O
complete	O
the	O
square	O
in	O
the	O
usual	O
way	O
to	O
identify	O
the	O
mean	B
and	O
covariance	B
giving	O
n	O
sn	O
approximate	O
inference	B
where	O
mn	O
sn	O
tt	O
sn	O
e	O
t	O
note	O
the	O
close	O
similarity	O
to	O
the	O
posterior	O
distribution	O
obtained	O
when	O
was	O
treated	O
as	O
a	O
fixed	O
parameter	O
the	O
difference	O
is	O
that	O
here	O
is	O
replaced	O
by	O
its	O
expectation	B
e	O
under	O
the	O
variational	B
distribution	O
indeed	O
we	O
have	O
chosen	O
to	O
use	O
the	O
same	O
notation	O
for	O
the	O
covariance	B
matrix	O
sn	O
in	O
both	O
cases	O
using	O
the	O
standard	O
results	O
and	O
we	O
can	O
obtain	O
the	O
required	O
moments	O
as	O
follows	O
e	O
an	O
ewwt	O
mn	O
mt	O
n	O
sn	O
the	O
evaluation	O
of	O
the	O
variational	B
posterior	O
distribution	O
begins	O
by	O
initializing	O
the	O
parameters	O
of	O
one	O
of	O
the	O
distributions	O
qw	O
or	O
q	O
and	O
then	O
alternately	O
re-estimates	O
these	O
factors	O
in	O
turn	O
until	O
a	O
suitable	O
convergence	O
criterion	O
is	O
satisfied	O
specified	O
in	O
terms	O
of	O
the	O
lower	B
bound	I
to	O
be	O
discussed	O
shortly	O
it	O
is	O
instructive	O
to	O
relate	O
the	O
variational	B
solution	O
to	O
that	O
found	O
using	O
the	O
evidence	O
framework	O
in	O
section	O
to	O
do	O
this	O
consider	O
the	O
case	O
corresponding	O
to	O
the	O
limit	O
of	O
an	O
infinitely	O
broad	O
prior	B
over	O
the	O
mean	B
of	O
the	O
variational	B
posterior	O
distribution	O
q	O
is	O
then	O
given	O
by	O
e	O
an	O
bn	O
m	O
n	O
mn	O
trsn	O
mt	O
comparison	O
with	O
shows	O
that	O
in	O
the	O
case	O
of	O
this	O
particularly	O
simple	O
model	O
the	O
variational	B
approach	O
gives	O
precisely	O
the	O
same	O
expression	O
as	O
that	O
obtained	O
by	O
maximizing	O
the	O
evidence	B
function	I
using	O
em	B
except	O
that	O
the	O
point	O
estimate	O
for	O
is	O
replaced	O
by	O
its	O
expected	O
value	O
because	O
the	O
distribution	O
qw	O
depends	O
on	O
q	O
only	O
through	O
the	O
expectation	B
e	O
we	O
see	O
that	O
the	O
two	O
approaches	O
will	O
give	O
identical	O
results	O
for	O
the	O
case	O
of	O
an	O
infinitely	O
broad	O
prior	B
predictive	B
distribution	I
the	O
predictive	B
distribution	I
over	O
t	O
given	O
a	O
new	O
input	O
x	O
is	O
easily	O
evaluated	O
for	O
this	O
model	O
using	O
the	O
gaussian	B
variational	B
posterior	O
for	O
the	O
parameters	O
ptx	O
t	O
ptx	O
wpwt	O
dw	O
ptx	O
wqw	O
dw	O
n	O
n	O
n	O
sn	O
dw	O
variational	B
linear	B
regression	B
where	O
we	O
have	O
evaluated	O
the	O
integral	O
by	O
making	O
use	O
of	O
the	O
result	O
for	O
the	O
linear-gaussian	B
model	I
here	O
the	O
input-dependent	O
variance	B
is	O
given	O
by	O
note	O
that	O
this	O
takes	O
the	O
same	O
form	O
as	O
the	O
result	O
obtained	O
with	O
fixed	O
except	O
that	O
now	O
the	O
expected	O
value	O
e	O
appears	O
in	O
the	O
definition	O
of	O
sn	O
lower	B
bound	I
another	O
quantity	O
of	O
importance	O
is	O
the	O
lower	B
bound	I
l	O
defined	O
by	O
lq	O
eln	O
pw	O
t	O
eln	O
qw	O
e	O
qww	O
eln	O
q	O
ewln	O
ptw	O
ew	O
pw	O
e	O
p	O
exercise	O
evaluation	O
of	O
the	O
various	O
terms	O
is	O
straightforward	O
making	O
use	O
of	O
results	O
obtained	O
in	O
previous	O
chapters	O
and	O
gives	O
eln	O
ptww	O
n	O
ln	O
tr	O
eln	O
pw	O
m	O
an	O
ttt	O
mt	O
n	O
tt	O
t	O
mt	O
m	O
n	O
sn	O
ln	O
bn	O
eln	O
p	O
ln	O
ln	O
bn	O
mt	O
n	O
mn	O
trsn	O
ln	O
an	O
bn	O
lnsn	O
m	O
eln	O
qww	O
eln	O
q	O
ln	O
ln	O
bn	O
an	O
figure	O
shows	O
a	O
plot	O
of	O
the	O
lower	B
bound	I
lq	O
versus	O
the	O
degree	O
of	O
a	O
polynomial	O
model	O
for	O
a	O
synthetic	O
data	O
set	O
generated	O
from	O
a	O
degree	O
three	O
polynomial	O
here	O
the	O
prior	B
parameters	O
have	O
been	O
set	O
to	O
corresponding	O
to	O
the	O
noninformative	B
prior	B
p	O
which	O
is	O
uniform	O
over	O
ln	O
as	O
discussed	O
in	O
section	O
as	O
we	O
saw	O
in	O
section	O
the	O
quantity	O
l	O
represents	O
lower	B
bound	I
on	O
the	O
log	O
marginal	B
likelihood	I
ptm	O
for	O
the	O
model	O
if	O
we	O
assign	O
equal	O
prior	B
probabilities	O
pm	O
to	O
the	O
different	O
values	O
of	O
m	O
then	O
we	O
can	O
interpret	O
l	O
as	O
an	O
approximation	O
to	O
the	O
posterior	O
model	O
probability	B
pmt	O
thus	O
the	O
variational	B
framework	O
assigns	O
the	O
highest	O
probability	B
to	O
the	O
model	O
with	O
m	O
this	O
should	O
be	O
contrasted	O
with	O
the	O
maximum	B
likelihood	I
result	O
which	O
assigns	O
ever	O
smaller	O
residual	O
error	B
to	O
models	O
of	O
increasing	O
complexity	O
until	O
the	O
residual	O
error	B
is	O
driven	O
to	O
zero	O
causing	O
maximum	B
likelihood	I
to	O
favour	O
severely	O
over-fitted	O
models	O
approximate	O
inference	B
figure	O
plot	O
of	O
the	O
lower	B
bound	I
l	O
versus	O
the	O
order	O
m	O
of	O
the	O
polynomial	O
for	O
a	O
polynomial	O
model	O
in	O
which	O
a	O
set	O
of	O
data	O
points	O
is	O
generated	O
from	O
a	O
polynomial	O
with	O
m	O
sampled	O
over	O
the	O
interval	O
with	O
additive	O
gaussian	B
noise	O
of	O
variance	B
the	O
value	O
of	O
the	O
bound	O
gives	O
the	O
log	O
probability	B
of	O
the	O
model	O
and	O
we	O
see	O
that	O
the	O
value	O
of	O
the	O
bound	O
peaks	O
at	O
m	O
corresponding	O
to	O
the	O
true	O
model	O
from	O
which	O
the	O
data	O
set	O
was	O
generated	O
exponential	B
family	I
distributions	O
in	O
chapter	O
we	O
discussed	O
the	O
important	O
role	O
played	O
by	O
the	O
exponential	B
family	I
of	O
distributions	O
and	O
their	O
conjugate	B
priors	O
for	O
many	O
of	O
the	O
models	O
discussed	O
in	O
this	O
book	O
the	O
complete-data	O
likelihood	O
is	O
drawn	O
from	O
the	O
exponential	B
family	I
however	O
in	O
general	O
this	O
will	O
not	O
be	O
the	O
case	O
for	O
the	O
marginal	B
likelihood	B
function	I
for	O
the	O
observed	O
data	O
for	O
example	O
in	O
a	O
mixture	B
of	I
gaussians	I
the	O
joint	O
distribution	O
of	O
observations	O
xn	O
and	O
corresponding	O
hidden	O
variables	O
zn	O
is	O
a	O
member	O
of	O
the	O
exponential	B
family	I
whereas	O
the	O
marginal	B
distribution	O
of	O
xn	O
is	O
a	O
mixture	B
of	I
gaussians	I
and	O
hence	O
is	O
not	O
up	O
to	O
now	O
we	O
have	O
grouped	O
the	O
variables	O
in	O
the	O
model	O
into	O
observed	O
variables	O
and	O
hidden	O
variables	O
we	O
now	O
make	O
a	O
further	O
distinction	O
between	O
latent	O
variables	O
denoted	O
z	O
and	O
parameters	O
denoted	O
where	O
parameters	O
are	O
intensive	O
in	O
number	O
independent	B
of	O
the	O
size	O
of	O
the	O
data	O
set	O
whereas	O
latent	O
variables	O
are	O
extensive	O
in	O
number	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
for	O
example	O
in	O
a	O
gaussian	B
mixture	B
model	I
the	O
indicator	O
variables	O
zkn	O
specify	O
which	O
component	O
k	O
is	O
responsible	O
for	O
generating	O
data	O
point	O
xn	O
represent	O
the	O
latent	O
variables	O
whereas	O
the	O
means	O
k	O
precisions	O
k	O
and	O
mixing	O
proportions	O
k	O
represent	O
the	O
parameters	O
consider	O
the	O
case	O
of	O
independent	B
identically	I
distributed	I
data	O
we	O
denote	O
the	O
data	O
values	O
by	O
x	O
where	O
n	O
n	O
with	O
corresponding	O
latent	O
variables	O
z	O
now	O
suppose	O
that	O
the	O
joint	O
distribution	O
of	O
observed	O
and	O
latent	O
variables	O
is	O
a	O
member	O
of	O
the	O
exponential	B
family	I
parameterized	O
by	O
natural	B
parameters	I
so	O
that	O
px	O
z	O
hxn	O
zng	O
exp	O
tuxn	O
zn	O
we	O
shall	O
also	O
use	O
a	O
conjugate	B
prior	B
for	O
which	O
can	O
be	O
written	O
as	O
p	O
f	O
exp	O
o	O
t	O
recall	O
that	O
the	O
conjugate	B
prior	B
distribution	O
can	O
be	O
interpreted	O
as	O
a	O
prior	B
number	O
of	O
observations	O
all	O
having	O
the	O
value	O
for	O
the	O
u	O
vector	O
now	O
consider	O
a	O
variational	B
exponential	B
family	I
distributions	O
distribution	O
that	O
factorizes	O
between	O
the	O
latent	O
variables	O
and	O
the	O
parameters	O
so	O
that	O
qz	O
qzq	O
using	O
the	O
general	O
result	O
we	O
can	O
solve	O
for	O
the	O
two	O
factors	O
as	O
follows	O
ln	O
e	O
px	O
z	O
const	O
ln	O
hxn	O
zn	O
e	O
tuxn	O
zn	O
const	O
thus	O
we	O
see	O
that	O
this	O
decomposes	O
into	O
a	O
sum	O
of	O
independent	B
terms	O
one	O
for	O
each	O
value	O
of	O
n	O
and	O
hence	O
the	O
solution	O
for	O
will	O
factorize	O
over	O
n	O
so	O
that	O
n	O
this	O
is	O
an	O
example	O
of	O
an	O
induced	B
factorization	B
taking	O
the	O
exponential	O
section	O
of	O
both	O
sides	O
we	O
have	O
hxn	O
zng	O
exp	O
e	O
tuxn	O
zn	O
where	O
the	O
normalization	O
coefficient	O
has	O
been	O
re-instated	O
by	O
comparison	O
with	O
the	O
standard	O
form	O
for	O
the	O
exponential	B
family	I
similarly	O
for	O
the	O
variational	B
distribution	O
over	O
the	O
parameters	O
we	O
have	O
ln	O
ln	O
p	O
ezln	O
px	O
z	O
const	O
ln	O
g	O
t	O
ln	O
g	O
t	O
eznuxn	O
zn	O
const	O
again	O
taking	O
the	O
exponential	O
of	O
both	O
sides	O
and	O
re-instating	O
the	O
normalization	O
coefficient	O
by	O
inspection	O
we	O
have	O
f	O
n	O
n	O
n	O
exp	O
t	O
n	O
where	O
we	O
have	O
defined	O
n	O
n	O
n	O
eznuxn	O
zn	O
note	O
that	O
the	O
solutions	O
for	O
and	O
are	O
coupled	O
and	O
so	O
we	O
solve	O
them	O
iteratively	O
in	O
a	O
two-stage	O
procedure	O
in	O
the	O
variational	B
e	O
step	O
we	O
evaluate	O
the	O
expected	O
sufficient	B
statistics	I
euxn	O
zn	O
using	O
the	O
current	O
posterior	O
distribution	O
qzn	O
over	O
the	O
latent	O
variables	O
and	O
use	O
this	O
to	O
compute	O
a	O
revised	O
posterior	O
distribution	O
q	O
over	O
the	O
parameters	O
then	O
in	O
the	O
subsequent	O
variational	B
m	O
step	O
we	O
use	O
this	O
revised	O
parameter	O
posterior	O
distribution	O
to	O
find	O
the	O
expected	O
natural	B
parameters	I
e	O
t	O
which	O
gives	O
rise	O
to	O
a	O
revised	O
variational	B
distribution	O
over	O
the	O
latent	O
variables	O
variational	B
message	B
passing	I
we	O
have	O
illustrated	O
the	O
application	O
of	O
variational	B
methods	O
by	O
considering	O
a	O
specific	O
model	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
in	O
some	O
detail	O
this	O
model	O
can	O
be	O
approximate	O
inference	B
described	O
by	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
here	O
we	O
consider	O
more	O
generally	O
the	O
use	O
of	O
variational	B
methods	O
for	O
models	O
described	O
by	O
directed	B
graphs	O
and	O
derive	O
a	O
number	O
of	O
widely	O
applicable	O
results	O
the	O
joint	O
distribution	O
corresponding	O
to	O
a	O
directed	B
graph	O
can	O
be	O
written	O
using	O
the	O
decomposition	O
px	O
pxipai	O
where	O
xi	O
denotes	O
the	O
variables	O
associated	O
with	O
node	B
i	O
and	O
pai	O
denotes	O
the	O
parent	O
set	O
corresponding	O
to	O
node	B
i	O
note	O
that	O
xi	O
may	O
be	O
a	O
latent	B
variable	I
or	O
it	O
may	O
belong	O
to	O
the	O
set	O
of	O
observed	O
variables	O
now	O
consider	O
a	O
variational	B
approximation	O
in	O
which	O
the	O
distribution	O
qx	O
is	O
assumed	O
to	O
factorize	O
with	O
respect	O
to	O
the	O
xi	O
so	O
that	O
qx	O
qixi	O
i	O
i	O
ln	O
pxipai	O
note	O
that	O
for	O
observed	O
nodes	O
there	O
is	O
no	O
factor	O
qxi	O
in	O
the	O
variational	B
distribution	O
we	O
now	O
substitute	O
into	O
our	O
general	O
result	O
to	O
give	O
ln	O
j	O
const	O
i	O
any	O
terms	O
on	O
the	O
right-hand	O
side	O
that	O
do	O
not	O
depend	O
on	O
xj	O
can	O
be	O
absorbed	O
into	O
in	O
fact	O
the	O
only	O
terms	O
that	O
do	O
depend	O
on	O
xj	O
are	O
the	O
conthe	O
additive	O
constant	O
ditional	O
distribution	O
for	O
xj	O
given	O
by	O
pxjpaj	O
together	O
with	O
any	O
other	O
conditional	B
distributions	O
that	O
have	O
xj	O
in	O
the	O
conditioning	O
set	O
by	O
definition	O
these	O
conditional	B
distributions	O
correspond	O
to	O
the	O
children	O
of	O
node	B
j	O
and	O
they	O
therefore	O
also	O
depend	O
on	O
the	O
co-parents	B
of	O
the	O
child	O
nodes	O
i	O
e	O
the	O
other	O
parents	O
of	O
the	O
child	O
nodes	O
besides	O
node	B
xj	O
itself	O
we	O
see	O
that	O
the	O
set	O
of	O
all	O
nodes	O
on	O
which	O
depends	O
corresponds	O
to	O
the	O
markov	B
blanket	I
of	O
node	B
xj	O
as	O
illustrated	O
in	O
figure	O
thus	O
the	O
update	O
of	O
the	O
factors	O
in	O
the	O
variational	B
posterior	O
distribution	O
represents	O
a	O
local	B
calculation	O
on	O
the	O
graph	O
this	O
makes	O
possible	O
the	O
construction	O
of	O
general	O
purpose	O
software	O
for	O
variational	B
inference	B
in	O
which	O
the	O
form	O
of	O
the	O
model	O
does	O
not	O
need	O
to	O
be	O
specified	O
in	O
advance	O
et	O
al	O
if	O
we	O
now	O
specialize	O
to	O
the	O
case	O
of	O
a	O
model	O
in	O
which	O
all	O
of	O
the	O
conditional	B
distributions	O
have	O
a	O
conjugate-exponential	O
structure	O
then	O
the	O
variational	B
update	O
procedure	O
can	O
be	O
cast	O
in	O
terms	O
of	O
a	O
local	B
message	B
passing	I
algorithm	O
and	O
bishop	O
in	O
particular	O
the	O
distribution	O
associated	O
with	O
a	O
particular	O
node	B
can	O
be	O
updated	O
once	O
that	O
node	B
has	O
received	O
messages	O
from	O
all	O
of	O
its	O
parents	O
and	O
all	O
of	O
its	O
children	O
this	O
in	O
turn	O
requires	O
that	O
the	O
children	O
have	O
already	O
received	O
messages	O
from	O
their	O
coparents	O
the	O
evaluation	O
of	O
the	O
lower	B
bound	I
can	O
also	O
be	O
simplified	O
because	O
many	O
of	O
the	O
required	O
quantities	O
are	O
already	O
evaluated	O
as	O
part	O
of	O
the	O
message	B
passing	I
scheme	O
this	O
distributed	O
message	B
passing	I
formulation	O
has	O
good	O
scaling	O
properties	O
and	O
is	O
well	O
suited	O
to	O
large	O
networks	O
local	B
variational	B
methods	O
local	B
variational	B
methods	O
the	O
variational	B
framework	O
discussed	O
in	O
sections	O
and	O
can	O
be	O
considered	O
a	O
global	O
method	O
in	O
the	O
sense	O
that	O
it	O
directly	O
seeks	O
an	O
approximation	O
to	O
the	O
full	O
posterior	O
distribution	O
over	O
all	O
random	O
variables	O
an	O
alternative	O
local	B
approach	O
involves	O
finding	O
bounds	O
on	O
functions	O
over	O
individual	O
variables	O
or	O
groups	O
of	O
variables	O
within	O
a	O
model	O
for	O
instance	O
we	O
might	O
seek	O
a	O
bound	O
on	O
a	O
conditional	B
distribution	O
pyx	O
which	O
is	O
itself	O
just	O
one	O
factor	O
in	O
a	O
much	O
larger	O
probabilistic	O
model	O
specified	O
by	O
a	O
directed	B
graph	O
the	O
purpose	O
of	O
introducing	O
the	O
bound	O
of	O
course	O
is	O
to	O
simplify	O
the	O
resulting	O
distribution	O
this	O
local	B
approximation	O
can	O
be	O
applied	O
to	O
multiple	O
variables	O
in	O
turn	O
until	O
a	O
tractable	O
approximation	O
is	O
obtained	O
and	O
in	O
section	O
we	O
shall	O
give	O
a	O
practical	O
example	O
of	O
this	O
approach	O
in	O
the	O
context	O
of	O
logistic	B
regression	B
here	O
we	O
focus	O
on	O
developing	O
the	O
bounds	O
themselves	O
we	O
have	O
already	O
seen	O
in	O
our	O
discussion	O
of	O
the	O
kullback-leibler	B
divergence	I
that	O
the	O
convexity	O
of	O
the	O
logarithm	O
function	O
played	O
a	O
key	O
role	O
in	O
developing	O
the	O
lower	B
bound	I
in	O
the	O
global	O
variational	B
approach	O
we	O
have	O
defined	O
a	O
convex	B
function	I
as	O
one	O
for	O
which	O
every	O
chord	O
lies	O
above	O
the	O
function	O
convexity	O
also	O
plays	O
a	O
central	O
role	O
in	O
the	O
local	B
variational	B
framework	O
note	O
that	O
our	O
discussion	O
will	O
apply	O
equally	O
to	O
concave	O
functions	O
with	O
min	O
and	O
max	O
interchanged	O
and	O
with	O
lower	O
bounds	O
replaced	O
by	O
upper	O
bounds	O
let	O
us	O
begin	O
by	O
considering	O
a	O
simple	O
example	O
namely	O
the	O
function	O
fx	O
exp	O
x	O
which	O
is	O
a	O
convex	B
function	I
of	O
x	O
and	O
which	O
is	O
shown	O
in	O
the	O
left-hand	O
plot	O
of	O
figure	O
our	O
goal	O
is	O
to	O
approximate	O
fx	O
by	O
a	O
simpler	O
function	O
in	O
particular	O
a	O
linear	O
function	O
of	O
x	O
from	O
figure	O
we	O
see	O
that	O
this	O
linear	O
function	O
will	O
be	O
a	O
lower	B
bound	I
on	O
fx	O
if	O
it	O
corresponds	O
to	O
a	O
tangent	O
we	O
can	O
obtain	O
the	O
tangent	O
line	O
yx	O
at	O
a	O
specific	O
value	O
of	O
x	O
say	O
x	O
by	O
making	O
a	O
first	O
order	O
taylor	O
expansion	O
section	O
so	O
that	O
yx	O
fx	O
with	O
equality	O
when	O
x	O
for	O
our	O
example	O
function	O
fx	O
yx	O
f	O
f	O
figure	O
in	O
the	O
left-hand	O
figure	O
the	O
red	O
curve	O
shows	O
the	O
function	O
exp	O
x	O
and	O
the	O
blue	O
line	O
shows	O
the	O
tangent	O
at	O
x	O
defined	O
by	O
with	O
this	O
line	O
has	O
slope	O
exp	O
note	O
that	O
any	O
other	O
tangent	O
line	O
for	O
example	O
the	O
ones	O
shown	O
in	O
green	O
will	O
have	O
a	O
smaller	O
value	O
of	O
y	O
at	O
x	O
the	O
right-hand	O
figure	O
shows	O
the	O
corresponding	O
plot	O
of	O
the	O
function	O
g	O
where	O
g	O
is	O
given	O
by	O
versus	O
for	O
in	O
which	O
the	O
maximum	O
corresponds	O
to	O
exp	O
x	O
g	O
approximate	O
inference	B
y	O
fx	O
y	O
g	O
fx	O
x	O
x	O
x	O
x	O
g	O
figure	O
in	O
the	O
left-hand	O
plot	O
the	O
red	O
curve	O
shows	O
a	O
convex	B
function	I
f	O
and	O
the	O
blue	O
line	O
represents	O
the	O
linear	O
function	O
x	O
which	O
is	O
a	O
lower	B
bound	I
on	O
f	O
because	O
f	O
x	O
for	O
all	O
x	O
for	O
the	O
given	O
value	O
of	O
slope	O
the	O
contact	O
point	O
of	O
the	O
tangent	O
line	O
having	O
the	O
same	O
slope	O
is	O
found	O
by	O
minimizing	O
with	O
respect	O
to	O
x	O
the	O
discrepancy	O
by	O
the	O
green	O
dashed	O
lines	O
given	O
by	O
f	O
x	O
this	O
defines	O
the	O
dual	O
function	O
g	O
which	O
corresponds	O
to	O
the	O
of	O
the	O
intercept	O
of	O
the	O
tangent	O
line	O
having	O
slope	O
exp	O
x	O
we	O
therefore	O
obtain	O
the	O
tangent	O
line	O
in	O
the	O
form	O
yx	O
exp	O
exp	O
which	O
is	O
a	O
linear	O
function	O
parameterized	O
by	O
for	O
consistency	O
with	O
subsequent	O
discussion	O
let	O
us	O
define	O
exp	O
so	O
that	O
yx	O
x	O
ln	O
different	O
values	O
of	O
correspond	O
to	O
different	O
tangent	O
lines	O
and	O
because	O
all	O
such	O
lines	O
are	O
lower	O
bounds	O
on	O
the	O
function	O
we	O
have	O
fx	O
yx	O
thus	O
we	O
can	O
write	O
the	O
function	O
in	O
the	O
form	O
fx	O
max	O
x	O
ln	O
we	O
have	O
succeeded	O
in	O
approximating	O
the	O
convex	B
function	I
fx	O
by	O
a	O
simpler	O
linear	O
function	O
yx	O
the	O
price	O
we	O
have	O
paid	O
is	O
that	O
we	O
have	O
introduced	O
a	O
variational	B
parameter	O
and	O
to	O
obtain	O
the	O
tightest	O
bound	O
we	O
must	O
optimize	O
with	O
respect	O
to	O
we	O
can	O
formulate	O
this	O
approach	O
more	O
generally	O
using	O
the	O
framework	O
of	O
convex	B
duality	I
jordan	O
et	O
al	O
consider	O
the	O
illustration	O
of	O
a	O
convex	B
function	I
fx	O
shown	O
in	O
the	O
left-hand	O
plot	O
in	O
figure	O
in	O
this	O
example	O
the	O
function	O
x	O
is	O
a	O
lower	B
bound	I
on	O
fx	O
but	O
it	O
is	O
not	O
the	O
best	O
lower	B
bound	I
that	O
can	O
be	O
achieved	O
by	O
a	O
linear	O
function	O
having	O
slope	O
because	O
the	O
tightest	O
bound	O
is	O
given	O
by	O
the	O
tangent	O
line	O
let	O
us	O
write	O
the	O
equation	O
of	O
the	O
tangent	O
line	O
having	O
slope	O
as	O
x	O
g	O
where	O
the	O
intercept	O
g	O
clearly	O
depends	O
on	O
the	O
slope	O
of	O
the	O
tangent	O
to	O
determine	O
the	O
intercept	O
we	O
note	O
that	O
the	O
line	O
must	O
be	O
moved	O
vertically	O
by	O
an	O
amount	O
equal	O
to	O
the	O
smallest	O
vertical	O
distance	O
between	O
the	O
line	O
and	O
the	O
function	O
as	O
shown	O
in	O
figure	O
thus	O
g	O
min	O
x	O
max	O
x	O
x	O
fx	O
x	O
local	B
variational	B
methods	O
now	O
instead	O
of	O
fixing	O
and	O
varying	O
x	O
we	O
can	O
consider	O
a	O
particular	O
x	O
and	O
then	O
adjust	O
until	O
the	O
tangent	O
plane	O
is	O
tangent	O
at	O
that	O
particular	O
x	O
because	O
the	O
y	O
value	O
of	O
the	O
tangent	O
line	O
at	O
a	O
particular	O
x	O
is	O
maximized	O
when	O
that	O
value	O
coincides	O
with	O
its	O
contact	O
point	O
we	O
have	O
x	O
g	O
fx	O
max	O
we	O
see	O
that	O
the	O
functions	O
fx	O
and	O
g	O
play	O
a	O
dual	O
role	O
and	O
are	O
related	O
through	O
and	O
let	O
us	O
apply	O
these	O
duality	O
relations	O
to	O
our	O
simple	O
example	O
fx	O
exp	O
x	O
from	O
we	O
see	O
that	O
the	O
maximizing	O
value	O
of	O
x	O
is	O
given	O
by	O
ln	O
and	O
back-substituting	O
we	O
obtain	O
the	O
conjugate	B
function	O
g	O
in	O
the	O
form	O
g	O
ln	O
as	O
obtained	O
previously	O
the	O
function	O
g	O
is	O
shown	O
for	O
in	O
the	O
right-hand	O
plot	O
in	O
figure	O
as	O
a	O
check	O
we	O
can	O
substitute	O
into	O
which	O
gives	O
the	O
maximizing	O
value	O
of	O
exp	O
x	O
and	O
back-substituting	O
then	O
recovers	O
the	O
original	O
function	O
fx	O
exp	O
x	O
for	O
concave	O
functions	O
we	O
can	O
follow	O
a	O
similar	O
argument	O
to	O
obtain	O
upper	O
bounds	O
in	O
which	O
max	O
is	O
replaced	O
with	O
min	O
so	O
that	O
fx	O
min	O
g	O
min	O
x	O
x	O
g	O
x	O
fx	O
if	O
the	O
function	O
of	O
interest	O
is	O
not	O
convex	O
concave	O
then	O
we	O
cannot	O
directly	O
apply	O
the	O
method	O
above	O
to	O
obtain	O
a	O
bound	O
however	O
we	O
can	O
first	O
seek	O
invertible	O
transformations	O
either	O
of	O
the	O
function	O
or	O
of	O
its	O
argument	O
which	O
change	O
it	O
into	O
a	O
convex	O
form	O
we	O
then	O
calculate	O
the	O
conjugate	B
function	O
and	O
then	O
transform	O
back	O
to	O
the	O
original	O
variables	O
an	O
important	O
example	O
which	O
arises	O
frequently	O
in	O
pattern	O
recognition	O
is	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
e	O
x	O
as	O
it	O
stands	O
this	O
function	O
is	O
neither	O
convex	O
nor	O
concave	O
however	O
if	O
we	O
take	O
the	O
logarithm	O
we	O
obtain	O
a	O
function	O
which	O
is	O
concave	O
as	O
is	O
easily	O
verified	O
by	O
finding	O
the	O
second	O
derivative	B
from	O
the	O
corresponding	O
conjugate	B
function	O
then	O
takes	O
the	O
form	O
g	O
min	O
x	O
x	O
fx	O
ln	O
which	O
we	O
recognize	O
as	O
the	O
binary	B
entropy	B
function	O
for	O
a	O
variable	O
whose	O
probability	B
of	O
having	O
the	O
value	O
is	O
using	O
we	O
then	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
log	O
sigmoid	O
ln	O
x	O
g	O
exercise	O
appendix	O
b	O
approximate	O
inference	B
figure	O
the	O
left-hand	O
plot	O
shows	O
the	O
logistic	B
sigmoid	I
function	O
defined	O
by	O
in	O
red	O
together	O
with	O
two	O
examples	O
of	O
the	O
exponential	O
upper	O
bound	O
shown	O
in	O
blue	O
the	O
right-hand	O
plot	O
shows	O
the	O
logistic	B
sigmoid	I
again	O
in	O
red	O
together	O
with	O
the	O
gaussian	B
lower	B
bound	I
shown	O
in	O
blue	O
here	O
the	O
parameter	O
and	O
the	O
bound	O
is	O
exact	O
at	O
x	O
and	O
x	O
denoted	O
by	O
the	O
dashed	O
green	O
lines	O
and	O
taking	O
the	O
exponential	O
we	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
logistic	B
sigmoid	I
itself	O
of	O
the	O
form	O
exp	O
x	O
g	O
exercise	O
which	O
is	O
plotted	O
for	O
two	O
values	O
of	O
on	O
the	O
left-hand	O
plot	O
in	O
figure	O
we	O
can	O
also	O
obtain	O
a	O
lower	B
bound	I
on	O
the	O
sigmoid	O
having	O
the	O
functional	B
form	O
of	O
a	O
gaussian	B
to	O
do	O
this	O
we	O
follow	O
jaakkola	O
and	O
jordan	O
and	O
make	O
transformations	O
both	O
of	O
the	O
input	O
variable	O
and	O
of	O
the	O
function	O
itself	O
first	O
we	O
take	O
the	O
log	O
of	O
the	O
logistic	O
function	O
and	O
then	O
decompose	O
it	O
so	O
that	O
x	O
ln	O
ln	O
e	O
e	O
we	O
now	O
note	O
that	O
the	O
function	O
fx	O
e	O
is	O
a	O
convex	B
function	I
of	O
the	O
variable	O
as	O
can	O
again	O
be	O
verified	O
by	O
finding	O
the	O
second	O
derivative	B
this	O
leads	O
to	O
a	O
lower	B
bound	I
on	O
fx	O
which	O
is	O
a	O
linear	O
function	O
of	O
whose	O
conjugate	B
function	O
is	O
given	O
by	O
e	O
e	O
g	O
max	O
f	O
the	O
stationarity	O
condition	O
leads	O
to	O
dx	O
d	O
dx	O
x	O
fx	O
tanh	O
if	O
we	O
denote	O
this	O
value	O
of	O
x	O
corresponding	O
to	O
the	O
contact	O
point	O
of	O
the	O
tangent	O
line	O
for	O
this	O
particular	O
value	O
of	O
by	O
then	O
we	O
have	O
tanh	O
local	B
variational	B
methods	O
instead	O
of	O
thinking	O
of	O
as	O
the	O
variational	B
parameter	O
we	O
can	O
let	O
play	O
this	O
role	O
as	O
this	O
leads	O
to	O
simpler	O
expressions	O
for	O
the	O
conjugate	B
function	O
which	O
is	O
then	O
given	O
by	O
g	O
f	O
lne	O
e	O
hence	O
the	O
bound	O
on	O
fx	O
can	O
be	O
written	O
as	O
fx	O
g	O
lne	O
e	O
the	O
bound	O
on	O
the	O
sigmoid	O
then	O
becomes	O
exp	O
section	O
section	O
where	O
is	O
defined	O
by	O
this	O
bound	O
is	O
illustrated	O
in	O
the	O
right-hand	O
plot	O
of	O
figure	O
we	O
see	O
that	O
the	O
bound	O
has	O
the	O
form	O
of	O
the	O
exponential	O
of	O
a	O
quadratic	O
function	O
of	O
x	O
which	O
will	O
prove	O
useful	O
when	O
we	O
seek	O
gaussian	B
representations	O
of	O
posterior	O
distributions	O
defined	O
through	O
logistic	B
sigmoid	I
functions	O
the	O
logistic	B
sigmoid	I
arises	O
frequently	O
in	O
probabilistic	O
models	O
over	O
binary	O
variables	O
because	O
it	O
is	O
the	O
function	O
that	O
transforms	O
a	O
log	B
odds	I
ratio	O
into	O
a	O
posterior	B
probability	B
the	O
corresponding	O
transformation	O
for	O
a	O
multiclass	B
distribution	O
is	O
given	O
by	O
the	O
softmax	B
function	I
unfortunately	O
the	O
lower	B
bound	I
derived	O
here	O
for	O
the	O
logistic	B
sigmoid	I
does	O
not	O
directly	O
extend	O
to	O
the	O
softmax	O
gibbs	B
proposes	O
a	O
method	O
for	O
constructing	O
a	O
gaussian	B
distribution	O
that	O
is	O
conjectured	O
to	O
be	O
a	O
bound	O
no	O
rigorous	O
proof	O
is	O
given	O
which	O
may	O
be	O
used	O
to	O
apply	O
local	B
variational	B
methods	O
to	O
multiclass	B
problems	O
we	O
shall	O
see	O
an	O
example	O
of	O
the	O
use	O
of	O
local	B
variational	B
bounds	O
in	O
sections	O
for	O
the	O
moment	O
however	O
it	O
is	O
instructive	O
to	O
consider	O
in	O
general	O
terms	O
how	O
these	O
bounds	O
can	O
be	O
used	O
suppose	O
we	O
wish	O
to	O
evaluate	O
an	O
integral	O
of	O
the	O
form	O
i	O
da	O
where	O
is	O
the	O
logistic	B
sigmoid	I
and	O
pa	O
is	O
a	O
gaussian	B
probability	B
density	B
such	O
integrals	O
arise	O
in	O
bayesian	B
models	O
when	O
for	O
instance	O
we	O
wish	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
in	O
which	O
case	O
pa	O
represents	O
a	O
posterior	O
parameter	O
distribution	O
because	O
the	O
integral	O
is	O
intractable	O
we	O
employ	O
the	O
variational	B
bound	O
which	O
we	O
write	O
in	O
the	O
form	O
fa	O
where	O
is	O
a	O
variational	B
parameter	O
the	O
integral	O
now	O
becomes	O
the	O
product	O
of	O
two	O
exponential-quadratic	O
functions	O
and	O
so	O
can	O
be	O
integrated	O
analytically	O
to	O
give	O
a	O
bound	O
on	O
i	O
fa	O
da	O
f	O
i	O
we	O
now	O
have	O
the	O
freedom	O
to	O
choose	O
the	O
variational	B
parameter	O
which	O
we	O
do	O
by	O
finding	O
the	O
value	O
that	O
maximizes	O
the	O
function	O
f	O
the	O
resulting	O
value	O
f	O
represents	O
the	O
tightest	O
bound	O
within	O
this	O
family	O
of	O
bounds	O
and	O
can	O
be	O
used	O
as	O
an	O
approximation	O
to	O
i	O
this	O
optimized	O
bound	O
however	O
will	O
in	O
general	O
not	O
be	O
exact	O
approximate	O
inference	B
although	O
the	O
bound	O
fa	O
on	O
the	O
logistic	B
sigmoid	I
can	O
be	O
optimized	O
exactly	O
the	O
required	O
choice	O
for	O
depends	O
on	O
the	O
value	O
of	O
a	O
so	O
that	O
the	O
bound	O
is	O
exact	O
for	O
one	O
value	O
of	O
a	O
only	O
because	O
the	O
quantity	O
f	O
is	O
obtained	O
by	O
integrating	O
over	O
all	O
values	O
of	O
a	O
the	O
value	O
of	O
represents	O
a	O
compromise	O
weighted	O
by	O
the	O
distribution	O
pa	O
variational	B
logistic	B
regression	B
we	O
now	O
illustrate	O
the	O
use	O
of	O
local	B
variational	B
methods	O
by	O
returning	O
to	O
the	O
bayesian	B
logistic	B
regression	B
model	O
studied	O
in	O
section	O
there	O
we	O
focussed	O
on	O
the	O
use	O
of	O
the	O
laplace	B
approximation	I
while	O
here	O
we	O
consider	O
a	O
variational	B
treatment	O
based	O
on	O
the	O
approach	O
of	O
jaakkola	O
and	O
jordan	O
like	O
the	O
laplace	B
method	O
this	O
also	O
leads	O
to	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
however	O
the	O
greater	O
flexibility	O
of	O
the	O
variational	B
approximation	O
leads	O
to	O
improved	O
accuracy	O
compared	O
to	O
the	O
laplace	B
method	O
furthermore	O
the	O
laplace	B
method	O
the	O
variational	B
approach	O
is	O
optimizing	O
a	O
well	O
defined	O
objective	O
function	O
given	O
by	O
a	O
rigourous	O
bound	O
on	O
the	O
model	B
evidence	I
logistic	B
regression	B
has	O
also	O
been	O
treated	O
by	O
dybowski	O
and	O
roberts	O
from	O
a	O
bayesian	B
perspective	O
using	O
monte	B
carlo	I
sampling	I
techniques	O
variational	B
posterior	O
distribution	O
here	O
we	O
shall	O
make	O
use	O
of	O
a	O
variational	B
approximation	O
based	O
on	O
the	O
local	B
bounds	O
introduced	O
in	O
section	O
this	O
allows	O
the	O
likelihood	B
function	I
for	O
logistic	B
regression	B
which	O
is	O
governed	O
by	O
the	O
logistic	B
sigmoid	I
to	O
be	O
approximated	O
by	O
the	O
exponential	O
of	O
a	O
quadratic	O
form	O
it	O
is	O
therefore	O
again	O
convenient	O
to	O
choose	O
a	O
conjugate	B
gaussian	B
prior	B
of	O
the	O
form	O
for	O
the	O
moment	O
we	O
shall	O
treat	O
the	O
hyperparameters	O
and	O
as	O
fixed	O
constants	O
in	O
section	O
we	O
shall	O
demonstrate	O
how	O
the	O
variational	B
formalism	O
can	O
be	O
extended	B
to	O
the	O
case	O
where	O
there	O
are	O
unknown	O
hyperparameters	O
whose	O
values	O
are	O
to	O
be	O
inferred	O
from	O
the	O
data	O
in	O
the	O
variational	B
framework	O
we	O
seek	O
to	O
maximize	O
a	O
lower	B
bound	I
on	O
the	O
marginal	B
likelihood	I
for	O
the	O
bayesian	B
logistic	B
regression	B
model	O
the	O
marginal	B
likelihood	I
takes	O
the	O
form	O
ptnw	O
pw	O
dw	O
we	O
first	O
note	O
that	O
the	O
conditional	B
distribution	O
for	O
t	O
can	O
be	O
written	O
as	O
pt	O
t	O
ptwpw	O
dw	O
ptw	O
t	O
e	O
a	O
e	O
a	O
a	O
e	O
a	O
eat	O
a	O
e	O
eat	O
where	O
a	O
wt	O
in	O
order	O
to	O
obtain	O
a	O
lower	B
bound	I
on	O
pt	O
we	O
make	O
use	O
of	O
the	O
variational	B
lower	B
bound	I
on	O
the	O
logistic	B
sigmoid	I
function	O
given	O
by	O
which	O
variational	B
logistic	B
regression	B
we	O
reproduce	O
here	O
for	O
convenience	O
exp	O
where	O
we	O
can	O
therefore	O
write	O
ptw	O
eat	O
a	O
eat	O
exp	O
note	O
that	O
because	O
this	O
bound	O
is	O
applied	O
to	O
each	O
of	O
the	O
terms	O
in	O
the	O
likelihood	B
function	I
separately	O
there	O
is	O
a	O
variational	B
parameter	O
n	O
corresponding	O
to	O
each	O
training	B
set	I
observation	O
n	O
tn	O
using	O
a	O
wt	O
and	O
multiplying	O
by	O
the	O
prior	B
distribution	O
we	O
obtain	O
the	O
following	O
bound	O
on	O
the	O
joint	O
distribution	O
of	O
t	O
and	O
w	O
pt	O
w	O
ptwpw	O
hw	O
where	O
denotes	O
the	O
set	O
n	O
of	O
variational	B
parameters	O
and	O
nwt	O
n	O
wt	O
ntn	O
n	O
hw	O
n	O
exp	O
evaluation	O
of	O
the	O
exact	O
posterior	O
distribution	O
would	O
require	O
normalization	O
of	O
the	O
lefthand	O
side	O
of	O
this	O
inequality	O
because	O
this	O
is	O
intractable	O
we	O
work	O
instead	O
with	O
the	O
right-hand	O
side	O
note	O
that	O
the	O
function	O
on	O
the	O
right-hand	O
side	O
cannot	O
be	O
interpreted	O
as	O
a	O
probability	B
density	B
because	O
it	O
is	O
not	O
normalized	O
once	O
it	O
is	O
normalized	O
to	O
give	O
a	O
variational	B
posterior	O
distribution	O
qw	O
however	O
it	O
no	O
longer	O
represents	O
a	O
bound	O
because	O
the	O
logarithm	O
function	O
is	O
monotonically	O
increasing	O
the	O
inequality	O
a	O
b	O
implies	O
ln	O
a	O
ln	O
b	O
this	O
gives	O
a	O
lower	B
bound	I
on	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
of	O
t	O
and	O
w	O
of	O
the	O
form	O
lnptwpw	O
ln	O
pw	O
ln	O
n	O
wt	O
ntn	O
n	O
nwt	O
n	O
substituting	O
for	O
the	O
prior	B
pw	O
the	O
right-hand	O
side	O
of	O
this	O
inequality	O
becomes	O
as	O
a	O
function	O
of	O
w	O
wt	O
ntn	O
nwt	O
n	O
t	O
nw	O
const	O
approximate	O
inference	B
where	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
w	O
and	O
so	O
we	O
can	O
obtain	O
the	O
corresponding	O
variational	B
approximation	O
to	O
the	O
posterior	O
distribution	O
by	O
identifying	O
the	O
linear	O
and	O
quadratic	O
terms	O
in	O
w	O
giving	O
a	O
gaussian	B
variational	B
posterior	O
of	O
the	O
form	O
qw	O
n	O
sn	O
mn	O
sn	O
s	O
n	O
s	O
s	O
n	O
n	O
n	O
t	O
n	O
as	O
with	O
the	O
laplace	B
framework	O
we	O
have	O
again	O
obtained	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
however	O
the	O
additional	O
flexibility	O
provided	O
by	O
the	O
variational	B
parameters	O
n	O
leads	O
to	O
improved	O
accuracy	O
in	O
the	O
approximation	O
and	O
jordan	O
here	O
we	O
have	O
considered	O
a	O
batch	O
learning	B
context	O
in	O
which	O
all	O
of	O
the	O
training	B
data	O
is	O
available	O
at	O
once	O
however	O
bayesian	B
methods	O
are	O
intrinsically	O
well	O
suited	O
to	O
sequential	B
learning	B
in	O
which	O
the	O
data	O
points	O
are	O
processed	O
one	O
at	O
a	O
time	O
and	O
then	O
discarded	O
the	O
formulation	O
of	O
this	O
variational	B
approach	O
for	O
the	O
sequential	O
case	O
is	O
straightforward	O
note	O
that	O
the	O
bound	O
given	O
by	O
applies	O
only	O
to	O
the	O
two-class	O
problem	O
and	O
so	O
this	O
approach	O
does	O
not	O
directly	O
generalize	O
to	O
classification	B
problems	O
with	O
k	O
classes	O
an	O
alternative	O
bound	O
for	O
the	O
multiclass	B
case	O
has	O
been	O
explored	O
by	O
gibbs	B
exercise	O
optimizing	O
the	O
variational	B
parameters	O
we	O
now	O
have	O
a	O
normalized	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
which	O
we	O
shall	O
use	O
shortly	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
for	O
new	O
data	O
points	O
first	O
however	O
we	O
need	O
to	O
determine	O
the	O
variational	B
parameters	O
n	O
by	O
maximizing	O
the	O
lower	B
bound	I
on	O
the	O
marginal	B
likelihood	I
to	O
do	O
this	O
we	O
substitute	O
the	O
inequality	O
back	O
into	O
the	O
marginal	B
likeli	O
hood	O
to	O
give	O
ln	O
pt	O
ln	O
ptwpw	O
dw	O
ln	O
hw	O
dw	O
l	O
as	O
with	O
the	O
optimization	O
of	O
the	O
hyperparameter	B
in	O
the	O
linear	B
regression	B
model	O
of	O
section	O
there	O
are	O
two	O
approaches	O
to	O
determining	O
the	O
n	O
in	O
the	O
first	O
approach	O
we	O
recognize	O
that	O
the	O
function	O
l	O
is	O
defined	O
by	O
an	O
integration	O
over	O
w	O
and	O
so	O
we	O
can	O
view	O
w	O
as	O
a	O
latent	B
variable	I
and	O
invoke	O
the	O
em	B
algorithm	I
in	O
the	O
second	O
approach	O
we	O
integrate	O
over	O
w	O
analytically	O
and	O
then	O
perform	O
a	O
direct	O
maximization	O
over	O
let	O
us	O
begin	O
by	O
considering	O
the	O
em	B
approach	O
n	O
which	O
we	O
denote	O
collectively	O
by	O
old	O
the	O
em	B
algorithm	I
starts	O
by	O
choosing	O
some	O
initial	O
values	O
for	O
the	O
parameters	O
in	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
variational	B
logistic	B
regression	B
we	O
then	O
use	O
these	O
parameter	O
values	O
to	O
find	O
the	O
posterior	O
distribution	O
over	O
w	O
which	O
is	O
given	O
by	O
in	O
the	O
m	O
step	O
we	O
then	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
which	O
is	O
given	O
by	O
q	O
old	O
e	O
hw	O
where	O
the	O
expectation	B
is	O
taken	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
qw	O
evaluated	O
using	O
old	O
noting	O
that	O
pw	O
does	O
not	O
depend	O
on	O
and	O
substituting	O
for	O
hw	O
we	O
obtain	O
q	O
old	O
ln	O
n	O
n	O
t	O
n	O
ewwt	O
n	O
n	O
const	O
where	O
const	O
denotes	O
terms	O
that	O
are	O
independent	B
of	O
we	O
now	O
set	O
the	O
derivative	B
with	O
respect	O
to	O
n	O
equal	O
to	O
zero	O
a	O
few	O
lines	O
of	O
algebra	O
making	O
use	O
of	O
the	O
definitions	O
of	O
and	O
then	O
gives	O
n	O
t	O
n	O
ewwt	O
n	O
n	O
is	O
a	O
monotonic	O
function	O
of	O
for	O
and	O
that	O
we	O
can	O
we	O
now	O
note	O
that	O
restrict	O
attention	O
to	O
nonnegative	O
values	O
of	O
without	O
loss	O
of	O
generality	O
due	O
to	O
the	O
and	O
hence	O
we	O
obtain	O
the	O
symmetry	O
of	O
the	O
bound	O
around	O
thus	O
following	O
re-estimation	O
equations	O
new	O
n	O
t	O
n	O
ewwt	O
n	O
t	O
n	O
sn	O
mn	O
mt	O
n	O
n	O
where	O
we	O
have	O
used	O
let	O
us	O
summarize	O
the	O
em	B
algorithm	I
for	O
finding	O
the	O
variational	B
posterior	O
distribution	O
we	O
first	O
initialize	O
the	O
variational	B
parameters	O
old	O
in	O
the	O
e	O
step	O
we	O
evaluate	O
the	O
posterior	O
distribution	O
over	O
w	O
given	O
by	O
in	O
which	O
the	O
mean	B
and	O
covariance	B
are	O
defined	O
by	O
and	O
in	O
the	O
m	O
step	O
we	O
then	O
use	O
this	O
variational	B
posterior	O
to	O
compute	O
a	O
new	O
value	O
for	O
given	O
by	O
the	O
e	O
and	O
m	O
steps	O
are	O
repeated	O
until	O
a	O
suitable	O
convergence	O
criterion	O
is	O
satisfied	O
which	O
in	O
practice	O
typically	O
requires	O
only	O
a	O
few	O
iterations	O
an	O
alternative	O
approach	O
to	O
obtaining	O
re-estimation	O
equations	O
for	O
is	O
to	O
note	O
that	O
in	O
the	O
integral	O
over	O
w	O
in	O
the	O
definition	O
of	O
the	O
lower	B
bound	I
l	O
the	O
integrand	O
has	O
a	O
gaussian-like	O
form	O
and	O
so	O
the	O
integral	O
can	O
be	O
evaluated	O
analytically	O
having	O
evaluated	O
the	O
integral	O
we	O
can	O
then	O
differentiate	O
with	O
respect	O
to	O
n	O
it	O
turns	O
out	O
that	O
this	O
gives	O
rise	O
to	O
exactly	O
the	O
same	O
re-estimation	O
equations	O
as	O
does	O
the	O
em	B
approach	O
given	O
by	O
as	O
we	O
have	O
emphasized	O
already	O
in	O
the	O
application	O
of	O
variational	B
methods	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
evaluate	O
the	O
lower	B
bound	I
l	O
given	O
by	O
the	O
integration	O
over	O
w	O
can	O
be	O
performed	O
analytically	O
by	O
noting	O
that	O
pw	O
is	O
gaussian	B
and	O
hw	O
is	O
the	O
exponential	O
of	O
a	O
quadratic	O
function	O
of	O
w	O
thus	O
by	O
completing	B
the	I
square	I
and	O
making	O
use	O
of	O
the	O
standard	O
result	O
for	O
the	O
normalization	O
coefficient	O
of	O
a	O
gaussian	B
distribution	O
we	O
can	O
obtain	O
a	O
closed	O
form	O
solution	O
which	O
takes	O
the	O
form	O
exercise	O
exercise	O
exercise	O
approximate	O
inference	B
figure	O
illustration	O
of	O
the	O
bayesian	B
approach	O
to	O
logistic	B
regression	B
for	O
a	O
simple	O
linearly	B
separable	I
data	O
set	O
the	O
plot	O
on	O
the	O
left	O
shows	O
the	O
predictive	B
distribution	I
obtained	O
using	O
variational	B
inference	B
we	O
see	O
that	O
the	O
decision	B
boundary	I
lies	O
roughly	O
mid	O
way	O
between	O
the	O
clusters	O
of	O
data	O
points	O
and	O
that	O
the	O
contours	O
of	O
the	O
predictive	B
distribution	I
splay	O
out	O
away	O
from	O
the	O
data	O
reflecting	O
the	O
greater	O
uncertainty	O
in	O
the	O
classification	B
of	O
such	O
regions	O
the	O
plot	O
on	O
the	O
right	O
shows	O
the	O
decision	O
boundaries	O
corresponding	O
to	O
five	O
samples	O
of	O
the	O
parameter	O
vector	O
w	O
drawn	O
from	O
the	O
posterior	O
distribution	O
pwt	O
l	O
ln	O
mt	O
ln	O
n	O
n	O
s	O
mt	O
s	O
n	O
mn	O
n	O
n	O
n	O
this	O
variational	B
framework	O
can	O
also	O
be	O
applied	O
to	O
situations	O
in	O
which	O
the	O
data	O
is	O
arriving	O
sequentially	O
and	O
jordan	O
in	O
this	O
case	O
we	O
maintain	O
a	O
gaussian	B
posterior	O
distribution	O
over	O
w	O
which	O
is	O
initialized	O
using	O
the	O
prior	B
pw	O
as	O
each	O
data	O
point	O
arrives	O
the	O
posterior	O
is	O
updated	O
by	O
making	O
use	O
of	O
the	O
bound	O
and	O
then	O
normalized	O
to	O
give	O
an	O
updated	O
posterior	O
distribution	O
the	O
predictive	B
distribution	I
is	O
obtained	O
by	O
marginalizing	O
over	O
the	O
posterior	O
distribution	O
and	O
takes	O
the	O
same	O
form	O
as	O
for	O
the	O
laplace	B
approximation	I
discussed	O
in	O
section	O
figure	O
shows	O
the	O
variational	B
predictive	O
distributions	O
for	O
a	O
synthetic	O
data	O
set	O
this	O
example	O
provides	O
interesting	O
insights	O
into	O
the	O
concept	O
of	O
large	O
margin	B
which	O
was	O
discussed	O
in	O
section	O
and	O
which	O
has	O
qualitatively	O
similar	O
behaviour	O
to	O
the	O
bayesian	B
solution	O
inference	B
of	O
hyperparameters	O
so	O
far	O
we	O
have	O
treated	O
the	O
hyperparameter	B
in	O
the	O
prior	B
distribution	O
as	O
a	O
known	O
constant	O
we	O
now	O
extend	O
the	O
bayesian	B
logistic	B
regression	B
model	O
to	O
allow	O
the	O
value	O
of	O
this	O
parameter	O
to	O
be	O
inferred	O
from	O
the	O
data	O
set	O
this	O
can	O
be	O
achieved	O
by	O
combining	O
the	O
global	O
and	O
local	B
variational	B
approximations	O
into	O
a	O
single	O
framework	O
so	O
as	O
to	O
maintain	O
a	O
lower	B
bound	I
on	O
the	O
marginal	B
likelihood	I
at	O
each	O
stage	O
such	O
a	O
combined	O
approach	O
was	O
adopted	O
by	O
bishop	O
and	O
svens	O
en	O
in	O
the	O
context	O
of	O
a	O
bayesian	B
treatment	O
of	O
the	O
hierarchical	B
mixture	B
of	I
experts	I
model	O
variational	B
logistic	B
regression	B
specifically	O
we	O
consider	O
once	O
again	O
a	O
simple	O
isotropic	B
gaussian	B
prior	B
distribu	O
tion	O
of	O
the	O
form	O
pw	O
n	O
our	O
analysis	O
is	O
readily	O
extended	B
to	O
more	O
general	O
gaussian	B
priors	O
for	O
instance	O
if	O
we	O
wish	O
to	O
associate	O
a	O
different	O
hyperparameter	B
with	O
different	O
subsets	O
of	O
the	O
parameters	O
wj	O
as	O
usual	O
we	O
consider	O
a	O
conjugate	B
hyperprior	B
over	O
given	O
by	O
a	O
gamma	B
distribution	I
p	O
gam	O
governed	O
by	O
the	O
constants	O
and	O
the	O
marginal	B
likelihood	I
for	O
this	O
model	O
now	O
takes	O
the	O
form	O
pt	O
pw	O
t	O
dw	O
d	O
where	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
pw	O
t	O
ptwpw	O
we	O
are	O
now	O
faced	O
with	O
an	O
analytically	O
intractable	O
integration	O
over	O
w	O
and	O
which	O
we	O
shall	O
tackle	O
by	O
using	O
both	O
the	O
local	B
and	O
global	O
variational	B
approaches	O
in	O
the	O
same	O
model	O
to	O
begin	O
with	O
we	O
introduce	O
a	O
variational	B
distribution	O
qw	O
and	O
then	O
apply	O
the	O
decomposition	O
which	O
in	O
this	O
instance	O
takes	O
the	O
form	O
where	O
the	O
lower	B
bound	I
lq	O
and	O
the	O
kullback-leibler	B
divergence	I
are	O
defined	O
by	O
ln	O
pt	O
lq	O
qw	O
ln	O
pw	O
t	O
qw	O
pw	O
qw	O
dw	O
d	O
qw	O
ln	O
at	O
this	O
point	O
the	O
lower	B
bound	I
lq	O
is	O
still	O
intractable	O
due	O
to	O
the	O
form	O
of	O
the	O
likelihood	O
factor	O
ptw	O
we	O
therefore	O
apply	O
the	O
local	B
variational	B
bound	O
to	O
each	O
of	O
the	O
logistic	B
sigmoid	I
factors	O
as	O
before	O
this	O
allows	O
us	O
to	O
use	O
the	O
inequality	O
and	O
place	O
a	O
lower	B
bound	I
on	O
lq	O
which	O
will	O
therefore	O
also	O
be	O
a	O
lower	B
bound	I
on	O
the	O
log	O
marginal	B
likelihood	I
dw	O
d	O
lq	O
ln	O
pt	O
lq	O
qw	O
ln	O
hw	O
qw	O
dw	O
d	O
next	O
we	O
assume	O
that	O
the	O
variational	B
distribution	O
factorizes	O
between	O
parameters	O
and	O
hyperparameters	O
so	O
that	O
qw	O
qwq	O
approximate	O
inference	B
with	O
this	O
factorization	B
we	O
can	O
appeal	O
to	O
the	O
general	O
result	O
to	O
find	O
expressions	O
for	O
the	O
optimal	O
factors	O
consider	O
first	O
the	O
distribution	O
qw	O
discarding	O
terms	O
that	O
are	O
independent	B
of	O
w	O
we	O
have	O
ln	O
qw	O
e	O
const	O
ln	O
hw	O
e	O
pw	O
const	O
we	O
now	O
substitute	O
for	O
ln	O
hw	O
using	O
and	O
for	O
ln	O
pw	O
using	O
giving	O
ln	O
qw	O
e	O
wtw	O
n	O
nwt	O
n	O
t	O
nw	O
const	O
we	O
see	O
that	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
w	O
and	O
so	O
the	O
solution	O
for	O
qw	O
will	O
be	O
gaussian	B
completing	B
the	I
square	I
in	O
the	O
usual	O
way	O
we	O
obtain	O
where	O
we	O
have	O
defined	O
n	O
n	O
qw	O
n	O
n	O
n	O
n	O
n	O
e	O
n	O
n	O
t	O
n	O
similarly	O
the	O
optimal	O
solution	O
for	O
the	O
factor	O
q	O
is	O
obtained	O
from	O
ln	O
q	O
ew	O
pw	O
ln	O
p	O
const	O
substituting	O
for	O
ln	O
pw	O
using	O
and	O
for	O
ln	O
p	O
using	O
we	O
obtain	O
ln	O
q	O
m	O
ln	O
e	O
wtw	O
ln	O
const	O
we	O
recognize	O
this	O
as	O
the	O
log	O
of	O
a	O
gamma	B
distribution	I
and	O
so	O
we	O
obtain	O
q	O
gam	O
bn	O
where	O
an	O
m	O
ew	O
bn	O
wtw	O
expectation	B
propagation	I
maximizing	O
the	O
lower	O
omitting	O
terms	O
that	O
are	O
independent	B
of	O
and	O
we	O
also	O
need	O
to	O
optimize	O
the	O
variational	B
parameters	O
n	O
and	O
this	O
is	O
also	O
done	O
by	O
integrating	O
over	O
we	O
have	O
qw	O
ln	O
hw	O
dw	O
const	O
note	O
that	O
this	O
has	O
precisely	O
the	O
same	O
form	O
as	O
and	O
so	O
we	O
can	O
again	O
appeal	O
to	O
our	O
earlier	O
result	O
which	O
can	O
be	O
obtained	O
by	O
direct	O
optimization	O
of	O
the	O
marginal	B
likelihood	B
function	I
leading	O
to	O
re-estimation	O
equations	O
of	O
the	O
form	O
new	O
n	O
t	O
n	O
n	O
n	O
t	O
n	O
n	O
appendix	O
b	O
we	O
have	O
obtained	O
re-estimation	O
equations	O
for	O
the	O
three	O
quantities	O
qw	O
q	O
and	O
and	O
so	O
after	O
making	O
suitable	O
initializations	O
we	O
can	O
cycle	O
through	O
these	O
quantities	O
updating	O
each	O
in	O
turn	O
the	O
required	O
moments	O
are	O
given	O
by	O
e	O
e	O
an	O
bn	O
wtw	O
n	O
t	O
n	O
n	O
expectation	B
propagation	I
we	O
conclude	O
this	O
chapter	O
by	O
discussing	O
an	O
alternative	O
form	O
of	O
deterministic	O
approximate	O
inference	B
known	O
as	O
expectation	B
propagation	I
or	O
ep	O
minka	O
as	O
with	O
the	O
variational	B
bayes	B
methods	O
discussed	O
so	O
far	O
this	O
too	O
is	O
based	O
on	O
the	O
minimization	O
of	O
a	O
kullback-leibler	B
divergence	I
but	O
now	O
of	O
the	O
reverse	O
form	O
which	O
gives	O
the	O
approximation	O
rather	O
different	O
properties	O
consider	O
for	O
a	O
moment	O
the	O
problem	O
of	O
minimizing	O
with	O
respect	O
to	O
qz	O
when	O
pz	O
is	O
a	O
fixed	O
distribution	O
and	O
qz	O
is	O
a	O
member	O
of	O
the	O
exponential	B
family	I
and	O
so	O
from	O
can	O
be	O
written	O
in	O
the	O
form	O
qz	O
hzg	O
exp	O
tuz	O
as	O
a	O
function	O
of	O
the	O
kullback-leibler	B
divergence	I
then	O
becomes	O
ln	O
g	O
t	O
epzuz	O
const	O
where	O
the	O
constant	O
terms	O
are	O
independent	B
of	O
the	O
natural	B
parameters	I
we	O
can	O
minimize	O
within	O
this	O
family	O
of	O
distributions	O
by	O
setting	O
the	O
gradient	O
with	O
respect	O
to	O
to	O
zero	O
giving	O
however	O
we	O
have	O
already	O
seen	O
in	O
that	O
the	O
negative	O
gradient	O
of	O
ln	O
g	O
is	O
given	O
by	O
the	O
expectation	B
of	O
uz	O
under	O
the	O
distribution	O
qz	O
equating	O
these	O
two	O
results	O
we	O
obtain	O
ln	O
g	O
epzuz	O
eqzuz	O
epzuz	O
approximate	O
inference	B
we	O
see	O
that	O
the	O
optimum	O
solution	O
simply	O
corresponds	O
to	O
matching	O
the	O
expected	O
sufficient	B
statistics	I
so	O
for	O
instance	O
if	O
qz	O
is	O
a	O
gaussian	B
n	O
then	O
we	O
minimize	O
the	O
kullback-leibler	B
divergence	I
by	O
setting	O
the	O
mean	B
of	O
qz	O
equal	O
to	O
the	O
mean	B
of	O
the	O
distribution	O
pz	O
and	O
the	O
covariance	B
equal	O
to	O
the	O
covariance	B
of	O
pz	O
this	O
is	O
sometimes	O
called	O
moment	B
matching	I
an	O
example	O
of	O
this	O
was	O
seen	O
in	O
figure	O
now	O
let	O
us	O
exploit	O
this	O
result	O
to	O
obtain	O
a	O
practical	O
algorithm	O
for	O
approximate	O
inference	B
for	O
many	O
probabilistic	O
models	O
the	O
joint	O
distribution	O
of	O
data	O
d	O
and	O
hidden	O
variables	O
parameters	O
comprises	O
a	O
product	O
of	O
factors	O
in	O
the	O
form	O
pd	O
fi	O
i	O
this	O
would	O
arise	O
for	O
example	O
in	O
a	O
model	O
for	O
independent	B
identically	I
distributed	I
data	O
in	O
which	O
there	O
is	O
one	O
factor	O
fn	O
pxn	O
for	O
each	O
data	O
point	O
xn	O
along	O
with	O
a	O
factor	O
p	O
corresponding	O
to	O
the	O
prior	B
more	O
generally	O
it	O
would	O
also	O
apply	O
to	O
any	O
model	O
defined	O
by	O
a	O
directed	B
probabilistic	O
graph	O
in	O
which	O
each	O
factor	O
is	O
a	O
conditional	B
distribution	O
corresponding	O
to	O
one	O
of	O
the	O
nodes	O
or	O
an	O
undirected	B
graph	O
in	O
which	O
each	O
factor	O
is	O
a	O
clique	B
potential	O
we	O
are	O
interested	O
in	O
evaluating	O
the	O
posterior	O
distribution	O
p	O
for	O
the	O
purpose	O
of	O
making	O
predictions	O
as	O
well	O
as	O
the	O
model	B
evidence	I
pd	O
for	O
the	O
purpose	O
of	O
model	B
comparison	I
from	O
the	O
posterior	O
is	O
given	O
by	O
p	O
fi	O
i	O
pd	O
i	O
and	O
the	O
model	B
evidence	I
is	O
given	O
by	O
pd	O
fi	O
d	O
here	O
we	O
are	O
considering	O
continuous	O
variables	O
but	O
the	O
following	O
discussion	O
applies	O
equally	O
to	O
discrete	O
variables	O
with	O
integrals	O
replaced	O
by	O
summations	O
we	O
shall	O
suppose	O
that	O
the	O
marginalization	O
over	O
along	O
with	O
the	O
marginalizations	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
required	O
to	O
make	O
predictions	O
are	O
intractable	O
so	O
that	O
some	O
form	O
of	O
approximation	O
is	O
required	O
expectation	B
propagation	I
is	O
based	O
on	O
an	O
approximation	O
to	O
the	O
posterior	O
distribu	O
tion	O
which	O
is	O
also	O
given	O
by	O
a	O
product	O
of	O
factors	O
q	O
in	O
which	O
each	O
in	O
the	O
approximation	O
corresponds	O
to	O
one	O
of	O
the	O
factors	O
obtain	O
a	O
practical	O
algorithm	O
we	O
need	O
to	O
constrain	O
the	O
in	O
some	O
way	O
fi	O
in	O
the	O
true	O
posterior	O
and	O
the	O
factor	O
is	O
the	O
normalizing	O
constant	O
needed	O
to	O
ensure	O
that	O
the	O
left-hand	O
side	O
of	O
integrates	O
to	O
unity	O
in	O
order	O
to	O
i	O
and	O
in	O
particular	O
we	O
shall	O
assume	O
that	O
they	O
come	O
from	O
the	O
exponential	B
family	I
the	O
product	O
of	O
the	O
factors	O
will	O
therefore	O
also	O
be	O
from	O
the	O
exponential	B
family	I
and	O
so	O
can	O
z	O
expectation	B
propagation	I
be	O
described	O
by	O
a	O
finite	O
set	O
of	O
sufficient	B
statistics	I
for	O
example	O
if	O
each	O
of	O
ideally	O
we	O
would	O
like	O
to	O
determine	O
by	O
minimizing	O
the	O
kullback-leibler	O
is	O
a	O
gaussian	B
then	O
the	O
overall	O
approximation	O
q	O
will	O
also	O
be	O
gaussian	B
divergence	O
between	O
the	O
true	O
posterior	O
and	O
the	O
approximation	O
given	O
by	O
i	O
z	O
i	O
kl	O
kl	O
pd	O
fi	O
note	O
that	O
this	O
is	O
the	O
reverse	O
form	O
of	O
kl	O
divergence	O
compared	O
with	O
that	O
used	O
in	O
variational	B
inference	B
in	O
general	O
this	O
minimization	O
will	O
be	O
intractable	O
because	O
the	O
kl	O
divergence	O
involves	O
averaging	O
with	O
respect	O
to	O
the	O
true	O
distribution	O
as	O
a	O
rough	O
approximation	O
we	O
could	O
instead	O
minimize	O
the	O
kl	O
divergences	O
between	O
the	O
corresponding	O
pairs	O
fi	O
of	O
factors	O
this	O
represents	O
a	O
much	O
simpler	O
problem	O
to	O
solve	O
and	O
has	O
the	O
advantage	O
that	O
the	O
algorithm	O
is	O
noniterative	O
however	O
because	O
each	O
factor	O
is	O
individually	O
approximated	O
the	O
product	O
of	O
the	O
factors	O
could	O
well	O
give	O
a	O
poor	O
approximation	O
this	O
is	O
similar	O
in	O
spirit	O
to	O
the	O
update	O
of	O
factors	O
in	O
the	O
variational	B
bayes	B
framework	O
expectation	B
propagation	I
makes	O
a	O
much	O
better	O
approximation	O
by	O
optimizing	O
each	O
factor	O
in	O
turn	O
in	O
the	O
context	O
of	O
all	O
of	O
the	O
remaining	O
factors	O
it	O
starts	O
by	O
initializing	O
the	O
and	O
then	O
cycles	O
through	O
the	O
factors	O
refining	O
them	O
one	O
at	O
a	O
time	O
considered	O
earlier	O
suppose	O
we	O
wish	O
to	O
refine	O
we	O
first	O
remove	O
this	O
conceptually	O
we	O
will	O
now	O
determine	O
a	O
revised	O
form	O
of	O
the	O
by	O
ensuring	O
that	O
the	O
product	O
factor	O
from	O
the	O
product	O
to	O
give	O
qnew	O
fj	O
is	O
as	O
close	O
as	O
possible	O
to	O
in	O
which	O
we	O
keep	O
fixed	O
all	O
of	O
the	O
for	O
i	O
j	O
this	O
ensures	O
that	O
the	O
to	O
the	O
clutter	B
problem	I
to	O
achieve	O
this	O
we	O
first	O
remove	O
the	O
from	O
the	O
approximation	O
is	O
most	O
accurate	O
in	O
the	O
regions	O
of	O
high	O
posterior	B
probability	B
as	O
defined	O
by	O
the	O
remaining	O
factors	O
we	O
shall	O
see	O
an	O
example	O
of	O
this	O
effect	O
when	O
we	O
apply	O
ep	O
current	O
approximation	O
to	O
the	O
posterior	O
by	O
defining	O
the	O
unnormalized	O
distribution	O
from	O
the	O
product	O
of	O
factors	O
i	O
j	O
although	O
note	O
that	O
we	O
could	O
instead	O
find	O
q	O
in	O
practice	O
division	O
is	O
usually	O
easier	O
this	O
is	O
now	O
combined	O
with	O
the	O
factor	O
fj	O
to	O
give	O
a	O
distribution	O
q	O
q	O
fj	O
zj	O
section	O
approximate	O
inference	B
figure	O
illustration	O
of	O
the	O
expectation	B
propagation	I
approximation	O
using	O
a	O
gaussian	B
distribution	O
for	O
the	O
example	O
considered	O
earlier	O
in	O
figures	O
and	O
the	O
left-hand	O
plot	O
shows	O
the	O
original	O
distribution	O
along	O
with	O
the	O
laplace	B
global	O
variational	B
and	O
ep	O
approximations	O
and	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
negative	O
logarithms	O
of	O
the	O
distributions	O
note	O
that	O
the	O
ep	O
distribution	O
is	O
broader	O
than	O
that	O
variational	B
inference	B
as	O
a	O
consequence	O
of	O
the	O
different	O
form	O
of	O
kl	O
divergence	O
where	O
zj	O
is	O
the	O
normalization	O
constant	O
given	O
by	O
we	O
now	O
determine	O
a	O
revised	O
by	O
minimizing	O
the	O
kullback-leibler	O
diver	O
zj	O
fj	O
d	O
gence	O
kl	O
fj	O
zj	O
qnew	O
this	O
is	O
easily	O
solved	O
because	O
the	O
approximating	O
distribution	O
qnew	O
is	O
from	O
the	O
exponential	B
family	I
and	O
so	O
we	O
can	O
appeal	O
to	O
the	O
result	O
which	O
tells	O
us	O
that	O
the	O
parameters	O
of	O
qnew	O
are	O
obtained	O
by	O
matching	O
its	O
expected	O
sufficient	B
statistics	I
to	O
the	O
corresponding	O
moments	O
of	O
we	O
shall	O
assume	O
that	O
this	O
is	O
a	O
tractable	O
operation	O
for	O
example	O
if	O
we	O
choose	O
q	O
to	O
be	O
a	O
gaussian	B
distribution	O
n	O
then	O
and	O
is	O
is	O
set	O
equal	O
to	O
the	O
mean	B
of	O
the	O
distribution	O
fj	O
set	O
to	O
its	O
covariance	B
more	O
generally	O
it	O
is	O
straightforward	O
to	O
obtain	O
the	O
required	O
expectations	O
for	O
any	O
member	O
of	O
the	O
exponential	B
family	I
provided	O
it	O
can	O
be	O
normalized	O
because	O
the	O
expected	O
statistics	O
can	O
be	O
related	O
to	O
the	O
derivatives	O
of	O
the	O
normalization	O
coefficient	O
as	O
given	O
by	O
the	O
ep	O
approximation	O
is	O
illustrated	O
in	O
figure	O
from	O
we	O
see	O
that	O
the	O
revised	O
factor	O
can	O
be	O
found	O
by	O
taking	O
qnew	O
and	O
dividing	O
out	O
the	O
remaining	O
factors	O
so	O
that	O
k	O
qnew	O
qj	O
where	O
we	O
have	O
used	O
the	O
coefficient	O
k	O
is	O
determined	O
by	O
multiplying	O
both	O
expectation	B
propagation	I
sides	O
of	O
by	O
q	O
and	O
integrating	O
to	O
give	O
k	O
d	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
qnew	O
is	O
normalized	O
the	O
value	O
of	O
k	O
can	O
therefore	O
be	O
found	O
by	O
matching	O
zeroth-order	O
moments	O
d	O
d	O
fj	O
combining	O
this	O
with	O
we	O
then	O
see	O
that	O
k	O
zj	O
and	O
so	O
can	O
be	O
found	O
by	O
evaluating	O
the	O
integral	O
in	O
in	O
practice	O
several	O
passes	O
are	O
made	O
through	O
the	O
set	O
of	O
factors	O
revising	O
each	O
factor	O
in	O
turn	O
the	O
posterior	O
distribution	O
p	O
is	O
then	O
approximated	O
using	O
and	O
the	O
model	B
evidence	I
pd	O
can	O
be	O
approximated	O
by	O
using	O
with	O
the	O
factors	O
fi	O
replaced	O
by	O
their	O
expectation	B
propagation	I
we	O
are	O
given	O
a	O
joint	O
distribution	O
over	O
observed	O
data	O
d	O
and	O
stochastic	B
variables	O
in	O
the	O
form	O
of	O
a	O
product	O
of	O
factors	O
pd	O
fi	O
and	O
we	O
wish	O
to	O
approximate	O
the	O
posterior	O
distribution	O
p	O
by	O
a	O
distribution	O
of	O
the	O
form	O
i	O
z	O
q	O
we	O
also	O
wish	O
to	O
approximate	O
the	O
model	B
evidence	I
pd	O
initialize	O
all	O
of	O
the	O
approximating	O
initialize	O
the	O
posterior	O
approximation	O
by	O
setting	O
q	O
i	O
i	O
until	O
convergence	O
choose	O
a	O
to	O
refine	O
from	O
the	O
posterior	O
by	O
division	O
q	O
q	O
approximate	O
inference	B
evaluate	O
the	O
new	O
posterior	O
by	O
setting	O
the	O
sufficient	B
statistics	I
including	O
evaluation	O
of	O
the	O
of	O
qnew	O
equal	O
to	O
those	O
of	O
q	O
normalization	O
constant	O
zj	O
d	O
q	O
evaluate	O
and	O
store	O
the	O
new	O
factor	O
qnew	O
qj	O
evaluate	O
the	O
approximation	O
to	O
the	O
model	B
evidence	I
zj	O
pd	O
d	O
i	O
a	O
special	O
case	O
of	O
ep	O
known	O
as	O
assumed	B
density	B
filtering	I
or	O
moment	B
matching	I
lauritzen	O
boyen	O
and	O
koller	O
opper	O
and	O
winther	O
is	O
obtained	O
by	O
initializing	O
all	O
of	O
the	O
approximating	O
factors	O
except	O
the	O
first	O
to	O
unity	O
and	O
then	O
making	O
one	O
pass	O
through	O
the	O
factors	O
updating	O
each	O
of	O
them	O
once	O
assumed	B
density	B
filtering	I
can	O
be	O
appropriate	O
for	O
on-line	O
learning	B
in	O
which	O
data	O
points	O
are	O
arriving	O
in	O
a	O
sequence	O
and	O
we	O
need	O
to	O
learn	O
from	O
each	O
data	O
point	O
and	O
then	O
discard	O
it	O
before	O
considering	O
the	O
next	O
point	O
however	O
in	O
a	O
batch	O
setting	O
we	O
have	O
the	O
opportunity	O
to	O
re-use	O
the	O
data	O
points	O
many	O
times	O
in	O
order	O
to	O
achieve	O
improved	O
accuracy	O
and	O
it	O
is	O
this	O
idea	O
that	O
is	O
exploited	O
in	O
expectation	B
propagation	I
furthermore	O
if	O
we	O
apply	O
adf	O
to	O
batch	O
data	O
the	O
results	O
will	O
have	O
an	O
undesirable	O
dependence	O
on	O
the	O
order	O
in	O
which	O
the	O
data	O
points	O
are	O
considered	O
which	O
again	O
ep	O
can	O
overcome	O
one	O
disadvantage	O
of	O
expectation	B
propagation	I
is	O
that	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
iterations	O
will	O
converge	O
however	O
for	O
approximations	O
q	O
in	O
the	O
exponential	B
family	I
if	O
the	O
iterations	O
do	O
converge	O
the	O
resulting	O
solution	O
will	O
be	O
a	O
stationary	B
point	O
of	O
a	O
particular	O
energy	B
function	I
although	O
each	O
iteration	O
of	O
ep	O
does	O
not	O
necessarily	O
decrease	O
the	O
value	O
of	O
this	O
energy	B
function	I
this	O
is	O
in	O
contrast	O
to	O
variational	B
bayes	B
which	O
iteratively	O
maximizes	O
a	O
lower	B
bound	I
on	O
the	O
log	O
marginal	B
likelihood	I
in	O
which	O
each	O
iteration	O
is	O
guaranteed	O
not	O
to	O
decrease	O
the	O
bound	O
it	O
is	O
possible	O
to	O
optimize	O
the	O
ep	O
cost	B
function	I
directly	O
in	O
which	O
case	O
it	O
is	O
guaranteed	O
to	O
converge	O
although	O
the	O
resulting	O
algorithms	O
can	O
be	O
slower	O
and	O
more	O
complex	O
to	O
implement	O
another	O
difference	O
between	O
variational	B
bayes	B
and	O
ep	O
arises	O
from	O
the	O
form	O
of	O
kl	O
divergence	O
that	O
is	O
minimized	O
by	O
the	O
two	O
algorithms	O
because	O
the	O
former	O
minimizes	O
whereas	O
the	O
latter	O
minimizes	O
as	O
we	O
saw	O
in	O
figure	O
for	O
distributions	O
p	O
which	O
are	O
multimodal	O
minimizing	O
can	O
lead	O
to	O
poor	O
approximations	O
in	O
particular	O
if	O
ep	O
is	O
applied	O
to	O
mixtures	O
the	O
results	O
are	O
not	O
sensible	O
because	O
the	O
approximation	O
tries	O
to	O
capture	O
all	O
of	O
the	O
modes	O
of	O
the	O
posterior	O
distribution	O
conversely	O
in	O
logistic-type	O
models	O
ep	O
often	O
out-performs	O
both	O
local	B
variational	B
methods	O
and	O
the	O
laplace	B
approximation	I
and	O
rasmussen	O
expectation	B
propagation	I
figure	O
illustration	O
of	O
the	O
clutter	B
problem	I
for	O
a	O
data	O
space	O
dimensionality	O
of	O
d	O
training	B
data	O
points	O
denoted	O
by	O
the	O
crosses	O
are	O
drawn	O
from	O
a	O
mixture	B
of	O
two	O
gaussians	O
with	O
components	O
shown	O
in	O
red	O
and	O
green	O
the	O
goal	O
is	O
to	O
infer	O
the	O
mean	B
of	O
the	O
green	O
gaussian	B
from	O
the	O
observed	O
data	O
x	O
example	O
the	O
clutter	B
problem	I
following	O
minka	O
we	O
illustrate	O
the	O
ep	O
algorithm	O
using	O
a	O
simple	O
example	O
in	O
which	O
the	O
goal	O
is	O
to	O
infer	O
the	O
mean	B
of	O
a	O
multivariate	O
gaussian	B
distribution	O
over	O
a	O
variable	O
x	O
given	O
a	O
set	O
of	O
observations	O
drawn	O
from	O
that	O
distribution	O
to	O
make	O
the	O
problem	O
more	O
interesting	O
the	O
observations	O
are	O
embedded	O
in	O
background	O
clutter	O
which	O
itself	O
is	O
also	O
gaussian	B
distributed	O
as	O
illustrated	O
in	O
figure	O
the	O
distribution	O
of	O
observed	O
values	O
x	O
is	O
therefore	O
a	O
mixture	B
of	I
gaussians	I
which	O
we	O
take	O
to	O
be	O
of	O
the	O
form	O
px	O
wn	O
i	O
wn	O
ai	O
where	O
w	O
is	O
the	O
proportion	O
of	O
background	O
clutter	O
and	O
is	O
assumed	O
to	O
be	O
known	O
the	O
prior	B
over	O
is	O
taken	O
to	O
be	O
gaussian	B
p	O
n	O
bi	O
and	O
minka	O
chooses	O
the	O
parameter	O
values	O
a	O
b	O
and	O
w	O
the	O
joint	O
distribution	O
of	O
n	O
observations	O
d	O
xn	O
and	O
is	O
given	O
by	O
pd	O
p	O
pxn	O
and	O
so	O
the	O
posterior	O
distribution	O
comprises	O
a	O
mixture	B
of	I
gaussians	I
thus	O
the	O
computational	O
cost	O
of	O
solving	O
this	O
problem	O
exactly	O
would	O
grow	O
exponentially	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
and	O
so	O
an	O
exact	O
solution	O
is	O
intractable	O
for	O
moderately	O
large	O
n	O
to	O
apply	O
ep	O
to	O
the	O
clutter	B
problem	I
we	O
first	O
identify	O
the	O
factors	O
p	O
and	O
fn	O
pxn	O
next	O
we	O
select	O
an	O
approximating	O
distribution	O
from	O
the	O
exponential	B
family	I
and	O
for	O
this	O
example	O
it	O
is	O
convenient	O
to	O
choose	O
a	O
spherical	O
gaussian	B
q	O
n	O
vi	O
approximate	O
inference	B
snn	O
vni	O
the	O
factor	O
approximations	O
will	O
therefore	O
take	O
the	O
form	O
of	O
exponential-quadratic	O
functions	O
of	O
the	O
form	O
where	O
n	O
n	O
and	O
we	O
equal	O
to	O
the	O
prior	B
p	O
note	O
that	O
the	O
use	O
of	O
convenient	O
shorthand	O
notation	O
the	O
for	O
n	O
n	O
can	O
n	O
does	O
not	O
imply	O
that	O
the	O
right-hand	O
side	O
is	O
a	O
well-defined	O
gaussian	B
density	B
fact	O
as	O
we	O
shall	O
see	O
the	O
variance	B
parameter	O
vn	O
can	O
be	O
negative	O
but	O
is	O
simply	O
a	O
be	O
initialized	O
to	O
unity	O
corresponding	O
to	O
sn	O
vn	O
and	O
mn	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
and	O
hence	O
of	O
the	O
initial	O
q	O
defined	O
by	O
is	O
therefore	O
equal	O
to	O
the	O
prior	B
we	O
then	O
iteratively	O
refine	O
the	O
factors	O
by	O
taking	O
one	O
factor	O
fn	O
at	O
a	O
time	O
and	O
applying	O
and	O
note	O
that	O
we	O
do	O
not	O
need	O
to	O
revise	O
the	O
term	O
because	O
an	O
ep	O
update	O
will	O
leave	O
this	O
term	O
unchanged	O
here	O
we	O
state	O
the	O
results	O
and	O
leave	O
the	O
reader	O
to	O
fill	O
in	O
the	O
details	O
first	O
we	O
remove	O
the	O
current	O
from	O
q	O
by	O
division	O
using	O
which	O
has	O
mean	B
and	O
inverse	B
variance	B
given	O
by	O
n	O
mn	O
mn	O
m	O
v	O
v	O
v	O
n	O
exercise	O
exercise	O
to	O
give	O
q	O
next	O
we	O
evaluate	O
the	O
normalization	O
constant	O
zn	O
using	O
to	O
give	O
zn	O
wn	O
wn	O
ai	O
exercise	O
similarly	O
we	O
compute	O
the	O
mean	B
and	O
variance	B
of	O
qnew	O
by	O
finding	O
the	O
mean	B
and	O
variance	B
of	O
q	O
to	O
give	O
v	O
m	O
mn	O
n	O
vn	O
n	O
vn	O
v	O
v	O
mn	O
n	O
dvn	O
where	O
the	O
quantity	O
we	O
use	O
to	O
compute	O
the	O
refined	O
whose	O
parameters	O
are	O
given	O
by	O
has	O
a	O
simple	O
interpretation	O
as	O
the	O
probability	B
of	O
the	O
point	O
xn	O
not	O
being	O
clutter	O
then	O
n	O
w	O
zn	O
n	O
ai	O
n	O
v	O
mn	O
mn	O
v	O
sn	O
zn	O
vni	O
mn	O
this	O
refinement	O
process	O
is	O
repeated	O
until	O
a	O
suitable	O
termination	O
criterion	O
is	O
satisfied	O
for	O
instance	O
that	O
the	O
maximum	O
change	O
in	O
parameter	O
values	O
resulting	O
from	O
a	O
complete	O
expectation	B
propagation	I
figure	O
examples	O
of	O
the	O
approximation	O
of	O
specific	O
factors	O
for	O
a	O
one-dimensional	O
version	O
of	O
the	O
clutter	B
problem	I
showing	O
fn	O
in	O
blue	O
efn	O
in	O
red	O
and	O
qn	O
in	O
green	O
notice	O
that	O
the	O
current	O
form	O
for	O
qn	O
controls	O
the	O
range	O
of	O
over	O
which	O
efn	O
will	O
be	O
a	O
good	O
approximation	O
to	O
fn	O
pass	O
through	O
all	O
factors	O
is	O
less	O
than	O
some	O
threshold	O
finally	O
we	O
use	O
to	O
evaluate	O
the	O
approximation	O
to	O
the	O
model	B
evidence	I
given	O
by	O
pd	O
vn	O
mt	O
nmn	O
vn	O
where	O
b	O
v	O
examples	O
factor	O
approximations	O
for	O
the	O
clutter	B
problem	I
with	O
a	O
one-dimensional	O
parameter	O
space	O
are	O
shown	O
in	O
figure	O
note	O
that	O
the	O
factor	O
approximations	O
can	O
have	O
infinite	O
or	O
even	O
negative	O
values	O
for	O
the	O
variance	B
parameter	O
vn	O
this	O
simply	O
corresponds	O
to	O
approximations	O
that	O
curve	O
upwards	O
instead	O
of	O
downwards	O
and	O
are	O
not	O
necessarily	O
problematic	O
provided	O
the	O
overall	O
approximate	O
posterior	O
q	O
has	O
positive	O
variance	B
figure	O
compares	O
the	O
performance	O
of	O
ep	O
with	O
variational	B
bayes	B
field	O
theory	B
and	O
the	O
laplace	B
approximation	I
on	O
the	O
clutter	B
problem	I
expectation	B
propagation	I
on	O
graphs	O
so	O
far	O
in	O
our	O
general	O
discussion	O
of	O
ep	O
we	O
have	O
allowed	O
the	O
factors	O
fi	O
in	O
the	O
distribution	O
p	O
to	O
be	O
functions	O
of	O
all	O
of	O
the	O
components	O
of	O
and	O
similarly	O
for	O
the	O
approximating	O
in	O
the	O
approximating	O
distribution	O
q	O
we	O
now	O
consider	O
situations	O
in	O
which	O
the	O
factors	O
depend	O
only	O
on	O
subsets	O
of	O
the	O
variables	O
such	O
restrictions	O
can	O
be	O
conveniently	O
expressed	O
using	O
the	O
framework	O
of	O
probabilistic	O
graphical	O
models	O
as	O
discussed	O
in	O
chapter	O
here	O
we	O
use	O
a	O
factor	B
graph	I
representation	O
because	O
this	O
encompasses	O
both	O
directed	B
and	O
undirected	B
graphs	O
approximate	O
inference	B
posterior	O
mean	B
r	O
o	O
r	O
r	O
e	O
laplace	B
vb	O
ep	O
flops	O
evidence	O
vb	O
r	O
o	O
r	O
r	O
e	O
laplace	B
flops	O
ep	O
figure	O
comparison	O
of	O
expectation	B
propagation	I
variational	B
inference	B
and	O
the	O
laplace	B
approximation	I
on	O
the	O
clutter	B
problem	I
the	O
left-hand	O
plot	O
shows	O
the	O
error	B
in	O
the	O
predicted	O
posterior	O
mean	B
versus	O
the	O
number	O
of	O
floating	O
point	O
operations	O
and	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
results	O
for	O
the	O
model	B
evidence	I
we	O
shall	O
focus	O
on	O
the	O
case	O
in	O
which	O
the	O
approximating	O
distribution	O
is	O
fully	O
factorized	O
and	O
we	O
shall	O
show	O
that	O
in	O
this	O
case	O
expectation	B
propagation	I
reduces	O
to	O
loopy	B
belief	B
propagation	I
to	O
start	O
with	O
we	O
show	O
this	O
in	O
the	O
context	O
of	O
a	O
simple	O
example	O
and	O
then	O
we	O
shall	O
explore	O
the	O
general	O
case	O
first	O
of	O
all	O
recall	O
from	O
that	O
if	O
we	O
minimize	O
the	O
kullback-leibler	B
divergence	I
with	O
respect	O
to	O
a	O
factorized	B
distribution	I
q	O
then	O
the	O
optimal	O
solution	O
for	O
each	O
factor	O
is	O
simply	O
the	O
corresponding	O
marginal	B
of	O
p	O
section	O
now	O
consider	O
the	O
factor	B
graph	I
shown	O
on	O
the	O
left	O
in	O
figure	O
which	O
was	O
introduced	O
earlier	O
in	O
the	O
context	O
of	O
the	O
sum-product	B
algorithm	I
the	O
joint	O
distribution	O
is	O
given	O
by	O
we	O
seek	O
an	O
approximation	O
qx	O
that	O
has	O
the	O
same	O
factorization	B
so	O
that	O
px	O
qx	O
note	O
that	O
normalization	O
constants	O
have	O
been	O
omitted	O
and	O
these	O
can	O
be	O
re-instated	O
at	O
the	O
end	O
by	O
local	B
normalization	O
as	O
is	O
generally	O
done	O
in	O
belief	B
propagation	I
now	O
suppose	O
we	O
restrict	O
attention	O
to	O
approximations	O
in	O
which	O
the	O
factors	O
themselves	O
factorize	O
with	O
respect	O
to	O
the	O
individual	O
variables	O
so	O
that	O
qx	O
which	O
corresponds	O
to	O
the	O
factor	B
graph	I
shown	O
on	O
the	O
right	O
in	O
figure	O
because	O
the	O
individual	O
factors	O
are	O
factorized	O
the	O
overall	O
distribution	O
qx	O
is	O
itself	O
fully	O
factorized	O
now	O
we	O
apply	O
the	O
ep	O
algorithm	O
using	O
the	O
fully	O
factorized	O
approximation	O
suppose	O
that	O
we	O
have	O
initialized	O
all	O
of	O
the	O
factors	O
and	O
that	O
we	O
choose	O
to	O
refine	O
factor	O
expectation	B
propagation	I
fa	O
fb	O
fc	O
figure	O
on	O
the	O
left	O
is	O
a	O
simple	O
factor	B
graph	I
from	O
figure	O
and	O
reproduced	O
here	O
for	O
convenience	O
on	O
the	O
right	O
is	O
the	O
corresponding	O
factorized	O
approximation	O
we	O
first	O
remove	O
this	O
factor	O
from	O
the	O
approximating	O
distribution	O
to	O
give	O
q	O
and	O
we	O
then	O
multiply	O
this	O
by	O
the	O
exact	O
factor	O
to	O
give	O
the	O
result	O
as	O
noted	O
above	O
is	O
that	O
qnewz	O
comprises	O
the	O
product	O
of	O
factors	O
one	O
for	O
each	O
variable	O
xi	O
in	O
which	O
each	O
factor	O
is	O
given	O
by	O
the	O
corresponding	O
marginal	B
of	O
q	O
we	O
now	O
find	O
qnewx	O
by	O
minimizing	O
the	O
kullback-leibler	B
divergence	I
these	O
four	O
marginals	O
are	O
given	O
by	O
and	O
qnewx	O
is	O
obtained	O
by	O
multiplying	O
these	O
marginals	O
together	O
we	O
see	O
that	O
the	O
only	O
factors	O
in	O
qx	O
that	O
change	O
when	O
we	O
are	O
those	O
that	O
involve	O
the	O
variables	O
in	O
fb	O
namely	O
and	O
to	O
obtain	O
the	O
refined	O
we	O
simply	O
divide	O
qnewx	O
by	O
q	O
which	O
gives	O
approximate	O
inference	B
section	O
fb	O
sent	O
by	O
factor	O
node	B
fb	O
to	O
variable	O
node	B
and	O
is	O
given	O
by	O
simi	O
these	O
are	O
precisely	O
the	O
messages	O
obtained	O
using	O
belief	B
propagation	I
in	O
which	O
messages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
have	O
been	O
folded	O
into	O
the	O
messages	O
from	O
factor	O
nodes	O
to	O
variable	O
nodes	O
in	O
particular	O
corresponds	O
to	O
the	O
message	O
larly	O
if	O
we	O
substitute	O
into	O
we	O
obtain	O
in	O
corresponds	O
to	O
fa	O
corresponds	O
to	O
fc	O
giving	O
the	O
message	O
which	O
corresponds	O
to	O
fb	O
factors	O
at	O
a	O
time	O
for	O
instance	O
if	O
we	O
refine	O
is	O
unchanged	O
by	O
definition	O
while	O
the	O
refined	O
version	O
is	O
again	O
given	O
by	O
if	O
this	O
result	O
differs	O
slightly	O
from	O
standard	O
belief	B
propagation	I
in	O
that	O
messages	O
are	O
passed	O
in	O
both	O
directions	O
at	O
the	O
same	O
time	O
we	O
can	O
easily	O
modify	O
the	O
ep	O
procedure	O
to	O
give	O
the	O
standard	O
form	O
of	O
the	O
sum-product	B
algorithm	I
by	O
updating	O
just	O
one	O
of	O
the	O
we	O
are	O
refining	O
only	O
one	O
term	O
at	O
a	O
time	O
then	O
we	O
can	O
choose	O
the	O
order	O
in	O
which	O
the	O
refinements	O
are	O
done	O
as	O
we	O
wish	O
in	O
particular	O
for	O
a	O
tree-structured	O
graph	O
we	O
can	O
follow	O
a	O
two-pass	O
update	O
scheme	O
corresponding	O
to	O
the	O
standard	O
belief	B
propagation	I
schedule	B
which	O
will	O
result	O
in	O
exact	O
inference	B
of	O
the	O
variable	O
and	O
factor	O
marginals	O
the	O
initialization	O
of	O
the	O
approximation	O
factors	O
in	O
this	O
case	O
is	O
unimportant	O
now	O
let	O
us	O
consider	O
a	O
general	O
factor	B
graph	I
corresponding	O
to	O
the	O
distribution	O
p	O
fi	O
i	O
where	O
i	O
represents	O
the	O
subset	O
of	O
variables	O
associated	O
with	O
factor	O
fi	O
we	O
approximate	O
this	O
using	O
a	O
fully	O
factorized	B
distribution	I
of	O
the	O
form	O
i	O
k	O
q	O
i	O
k	O
where	O
k	O
corresponds	O
to	O
an	O
individual	O
variable	O
node	B
suppose	O
that	O
we	O
wish	O
to	O
refine	O
the	O
particular	O
l	O
keeping	O
all	O
other	O
terms	O
fixed	O
we	O
first	O
remove	O
the	O
term	O
j	O
from	O
q	O
to	O
give	O
and	O
then	O
multiply	O
by	O
the	O
exact	O
factor	O
fj	O
j	O
to	O
determine	O
the	O
refined	O
l	O
k	O
k	O
q	O
we	O
need	O
only	O
consider	O
the	O
functional	B
dependence	O
on	O
l	O
and	O
so	O
we	O
simply	O
find	O
the	O
corresponding	O
marginal	B
of	O
up	O
to	O
a	O
multiplicative	O
constant	O
this	O
involves	O
taking	O
the	O
marginal	B
of	O
fj	O
j	O
multiplied	O
that	O
are	O
functions	O
of	O
any	O
of	O
the	O
variables	O
in	O
j	O
terms	O
that	O
by	O
any	O
terms	O
from	O
q	O
correspond	O
to	O
other	O
factors	O
i	O
for	O
i	O
j	O
will	O
cancel	O
between	O
numerator	O
and	O
q	O
j	O
denominator	O
when	O
we	O
subsequently	O
divide	O
by	O
q	O
l	O
fj	O
j	O
j	O
we	O
therefore	O
obtain	O
k	O
m	O
exercises	O
we	O
recognize	O
this	O
as	O
the	O
sum-product	O
rule	O
in	O
the	O
form	O
in	O
which	O
messages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
have	O
been	O
eliminated	O
as	O
illustrated	O
by	O
the	O
example	O
shown	O
in	O
figure	O
the	O
m	O
corresponds	O
to	O
the	O
message	O
fj	O
m	O
m	O
which	O
factor	O
node	B
j	O
sends	O
to	O
variable	O
node	B
m	O
and	O
the	O
product	O
over	O
k	O
in	O
is	O
over	O
all	O
factors	O
that	O
depend	O
on	O
the	O
variables	O
m	O
that	O
have	O
variables	O
than	O
variable	O
l	O
in	O
common	O
with	O
factor	O
fj	O
j	O
in	O
other	O
words	O
to	O
compute	O
the	O
outgoing	O
message	O
from	O
a	O
factor	O
node	B
we	O
take	O
the	O
product	O
of	O
all	O
the	O
incoming	O
messages	O
from	O
other	O
factor	O
nodes	O
multiply	O
by	O
the	O
local	B
factor	O
and	O
then	O
marginalize	O
thus	O
the	O
sum-product	B
algorithm	I
arises	O
as	O
a	O
special	O
case	O
of	O
expectation	B
propagation	I
if	O
we	O
use	O
an	O
approximating	O
distribution	O
that	O
is	O
fully	O
factorized	O
this	O
suggests	O
that	O
more	O
flexible	O
approximating	O
distributions	O
corresponding	O
to	O
partially	O
disconnected	O
graphs	O
could	O
be	O
used	O
to	O
achieve	O
higher	O
accuracy	O
another	O
generalization	B
is	O
to	O
group	O
factors	O
fi	O
i	O
together	O
into	O
sets	O
and	O
to	O
refine	O
all	O
the	O
factors	O
in	O
a	O
set	O
together	O
at	O
each	O
iteration	O
both	O
of	O
these	O
approaches	O
can	O
lead	O
to	O
improvements	O
in	O
accuracy	O
in	O
general	O
the	O
problem	O
of	O
choosing	O
the	O
best	O
combination	O
of	O
grouping	O
and	O
disconnection	O
is	O
an	O
open	O
research	O
issue	O
we	O
have	O
seen	O
that	O
variational	B
message	B
passing	I
and	O
expectation	B
propagation	I
optimize	O
two	O
different	O
forms	O
of	O
the	O
kullback-leibler	B
divergence	I
minka	O
has	O
shown	O
that	O
a	O
broad	O
range	O
of	O
message	B
passing	I
algorithms	O
can	O
be	O
derived	O
from	O
a	O
common	O
framework	O
involving	O
minimization	O
of	O
members	O
of	O
the	O
alpha	O
family	O
of	O
divergences	O
given	O
by	O
these	O
include	O
variational	B
message	B
passing	I
loopy	B
belief	B
propagation	I
and	O
expectation	B
propagation	I
as	O
well	O
as	O
a	O
range	O
of	O
other	O
algorithms	O
which	O
we	O
do	O
not	O
have	O
space	O
to	O
discuss	O
here	O
such	O
as	O
tree-reweighted	B
message	B
passing	I
et	O
al	O
fractional	B
belief	B
propagation	I
and	O
heskes	O
and	O
power	B
ep	I
exercises	O
www	O
verify	O
that	O
the	O
log	O
marginal	B
distribution	O
of	O
the	O
observed	O
data	O
ln	O
px	O
can	O
be	O
decomposed	O
into	O
two	O
terms	O
in	O
the	O
form	O
where	O
lq	O
is	O
given	O
by	O
and	O
is	O
given	O
by	O
use	O
the	O
properties	O
and	O
to	O
solve	O
the	O
simultaneous	O
equations	O
and	O
and	O
hence	O
show	O
that	O
provided	O
the	O
original	O
distribution	O
pz	O
is	O
nonsingular	O
the	O
unique	O
solution	O
for	O
the	O
means	O
of	O
the	O
factors	O
in	O
the	O
approximation	O
distribution	O
is	O
given	O
by	O
and	O
www	O
consider	O
a	O
factorized	O
variational	B
distribution	O
qz	O
of	O
the	O
form	O
by	O
using	O
the	O
technique	O
of	O
lagrange	B
multipliers	O
verify	O
that	O
minimization	O
of	O
the	O
kullback-leibler	B
divergence	I
with	O
respect	O
to	O
one	O
of	O
the	O
factors	O
qizi	O
keeping	O
all	O
other	O
factors	O
fixed	O
leads	O
to	O
the	O
solution	O
suppose	O
that	O
px	O
is	O
some	O
fixed	O
distribution	O
and	O
that	O
we	O
wish	O
to	O
approximate	O
it	O
using	O
a	O
gaussian	B
distribution	O
qx	O
n	O
by	O
writing	O
down	O
the	O
form	O
of	O
the	O
kl	O
divergence	O
for	O
a	O
gaussian	B
qx	O
and	O
then	O
differentiating	O
show	O
that	O
approximate	O
inference	B
minimization	O
of	O
with	O
respect	O
to	O
and	O
leads	O
to	O
the	O
result	O
that	O
is	O
given	O
by	O
the	O
expectation	B
of	O
x	O
under	O
px	O
and	O
that	O
is	O
given	O
by	O
the	O
covariance	B
www	O
consider	O
a	O
model	O
in	O
which	O
the	O
set	O
of	O
all	O
hidden	O
stochastic	B
variables	O
denoted	O
collectively	O
by	O
z	O
comprises	O
some	O
latent	O
variables	O
z	O
together	O
with	O
some	O
model	O
parameters	O
suppose	O
we	O
use	O
a	O
variational	B
distribution	O
that	O
factorizes	O
between	O
latent	O
variables	O
and	O
parameters	O
so	O
that	O
qz	O
qzzq	O
in	O
which	O
the	O
distribution	O
q	O
is	O
approximated	O
by	O
a	O
point	O
estimate	O
of	O
the	O
form	O
q	O
where	O
is	O
a	O
vector	O
of	O
free	O
parameters	O
show	O
that	O
variational	B
optimization	O
of	O
this	O
factorized	B
distribution	I
is	O
equivalent	O
to	O
an	O
em	B
algorithm	I
in	O
which	O
the	O
e	O
step	O
optimizes	O
qzz	O
and	O
the	O
m	O
step	O
maximizes	O
the	O
expected	O
complete-data	O
log	O
posterior	O
distribution	O
of	O
with	O
respect	O
to	O
the	O
alpha	O
family	O
of	O
divergences	O
is	O
defined	O
by	O
show	O
that	O
the	O
kullbackleibler	O
divergence	O
corresponds	O
to	O
this	O
can	O
be	O
done	O
by	O
writing	O
p	O
exp	O
ln	O
p	O
ln	O
p	O
and	O
then	O
taking	O
similarly	O
show	O
that	O
corresponds	O
to	O
consider	O
the	O
problem	O
of	O
inferring	O
the	O
mean	B
and	O
precision	O
of	O
a	O
univariate	O
gaussian	B
using	O
a	O
factorized	O
variational	B
approximation	O
as	O
considered	O
in	O
section	O
show	O
that	O
the	O
factor	O
q	O
is	O
a	O
gaussian	B
of	O
the	O
form	O
n	O
n	O
n	O
with	O
mean	B
and	O
precision	O
given	O
by	O
and	O
respectively	O
similarly	O
show	O
that	O
the	O
factor	O
q	O
is	O
a	O
gamma	B
distribution	I
of	O
the	O
form	O
gam	O
bn	O
with	O
parameters	O
given	O
by	O
and	O
consider	O
the	O
variational	B
posterior	O
distribution	O
for	O
the	O
precision	O
of	O
a	O
univariate	O
gaussian	B
whose	O
parameters	O
are	O
given	O
by	O
and	O
by	O
using	O
the	O
standard	O
results	O
for	O
the	O
mean	B
and	O
variance	B
of	O
the	O
gamma	B
distribution	I
given	O
by	O
and	O
show	O
that	O
if	O
we	O
let	O
n	O
this	O
variational	B
posterior	O
distribution	O
has	O
a	O
mean	B
given	O
by	O
the	O
inverse	B
of	O
the	O
maximum	B
likelihood	I
estimator	O
for	O
the	O
variance	B
of	O
the	O
data	O
and	O
a	O
variance	B
that	O
goes	O
to	O
zero	O
by	O
making	O
use	O
of	O
the	O
standard	O
result	O
e	O
an	O
for	O
the	O
mean	B
of	O
a	O
gamma	B
distribution	I
together	O
with	O
and	O
derive	O
the	O
result	O
for	O
the	O
reciprocal	O
of	O
the	O
expected	O
precision	O
in	O
the	O
factorized	O
variational	B
treatment	O
of	O
a	O
univariate	O
gaussian	B
www	O
derive	O
the	O
decomposition	O
given	O
by	O
that	O
is	O
used	O
to	O
find	O
approxi	O
mate	O
posterior	O
distributions	O
over	O
models	O
using	O
variational	B
inference	B
www	O
by	O
using	O
a	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
normalization	O
constraint	O
on	O
the	O
distribution	O
qm	O
show	O
that	O
the	O
maximum	O
of	O
the	O
lower	B
bound	I
is	O
given	O
by	O
starting	O
from	O
the	O
joint	O
distribution	O
and	O
applying	O
the	O
general	O
result	O
show	O
that	O
the	O
optimal	O
variational	B
distribution	O
over	O
the	O
latent	O
variables	O
for	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
is	O
given	O
by	O
by	O
verifying	O
the	O
steps	O
given	O
in	O
the	O
text	O
exercises	O
www	O
starting	O
from	O
derive	O
the	O
result	O
for	O
the	O
optimum	O
variational	B
posterior	O
distribution	O
over	O
k	O
and	O
k	O
in	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
and	O
hence	O
verify	O
the	O
expressions	O
for	O
the	O
parameters	O
of	O
this	O
distribution	O
given	O
by	O
using	O
the	O
distribution	O
verify	O
the	O
result	O
using	O
the	O
result	O
show	O
that	O
the	O
expected	O
value	O
of	O
the	O
mixing	O
coefficients	O
in	O
the	O
variational	B
mixture	B
of	I
gaussians	I
is	O
given	O
by	O
www	O
verify	O
the	O
results	O
and	O
for	O
the	O
first	O
two	O
terms	O
in	O
the	O
lower	B
bound	I
for	O
the	O
variational	B
gaussian	B
mixture	B
model	I
given	O
by	O
verify	O
the	O
results	O
for	O
the	O
remaining	O
terms	O
in	O
the	O
lower	B
bound	I
for	O
the	O
variational	B
gaussian	B
mixture	B
model	I
given	O
by	O
in	O
this	O
exercise	O
we	O
shall	O
derive	O
the	O
variational	B
re-estimation	O
equations	O
for	O
the	O
gaussian	B
mixture	B
model	I
by	O
direct	O
differentiation	O
of	O
the	O
lower	B
bound	I
to	O
do	O
this	O
we	O
assume	O
that	O
the	O
variational	B
distribution	O
has	O
the	O
factorization	B
defined	O
by	O
and	O
with	O
factors	O
given	O
by	O
and	O
substitute	O
these	O
into	O
and	O
hence	O
obtain	O
the	O
lower	B
bound	I
as	O
a	O
function	O
of	O
the	O
parameters	O
of	O
the	O
variational	B
distribution	O
then	O
by	O
maximizing	O
the	O
bound	O
with	O
respect	O
to	O
these	O
parameters	O
derive	O
the	O
re-estimation	O
equations	O
for	O
the	O
factors	O
in	O
the	O
variational	B
distribution	O
and	O
show	O
that	O
these	O
are	O
the	O
same	O
as	O
those	O
obtained	O
in	O
section	O
derive	O
the	O
result	O
for	O
the	O
predictive	B
distribution	I
in	O
the	O
variational	B
treat	O
ment	O
of	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
model	O
www	O
this	O
exercise	O
explores	O
the	O
variational	B
bayes	B
solution	O
for	O
the	O
mixture	B
of	I
gaussians	I
model	O
when	O
the	O
size	O
n	O
of	O
the	O
data	O
set	O
is	O
large	O
and	O
shows	O
that	O
it	O
reduces	O
we	O
would	O
expect	O
to	O
the	O
maximum	B
likelihood	I
solution	O
based	O
on	O
em	B
derived	O
in	O
chapter	O
note	O
that	O
results	O
from	O
appendix	O
b	O
may	O
be	O
used	O
to	O
help	O
answer	O
this	O
exercise	O
first	O
show	O
that	O
the	O
posterior	O
distribution	O
k	O
of	O
the	O
precisions	O
becomes	O
sharply	O
peaked	O
around	O
the	O
maximum	B
likelihood	I
solution	O
do	O
the	O
same	O
for	O
the	O
posterior	O
distribution	O
of	O
the	O
means	O
k	O
k	O
next	O
consider	O
the	O
posterior	O
distribution	O
for	O
the	O
mixing	O
coefficients	O
and	O
show	O
that	O
this	O
too	O
becomes	O
sharply	O
peaked	O
around	O
the	O
maximum	B
likelihood	I
solution	O
similarly	O
show	O
that	O
the	O
responsibilities	O
become	O
equal	O
to	O
the	O
corresponding	O
maximum	B
likelihood	I
values	O
for	O
large	O
n	O
by	O
making	O
use	O
of	O
the	O
following	O
asymptotic	O
result	O
for	O
the	O
digamma	B
function	I
for	O
large	O
x	O
ln	O
x	O
o	O
finally	O
by	O
making	O
use	O
of	O
show	O
that	O
for	O
large	O
n	O
the	O
predictive	B
distribution	I
becomes	O
a	O
mixture	B
of	I
gaussians	I
show	O
that	O
the	O
number	O
of	O
equivalent	O
parameter	O
settings	O
due	O
to	O
interchange	O
sym	O
metries	O
in	O
a	O
mixture	B
model	I
with	O
k	O
components	O
is	O
k	O
approximate	O
inference	B
we	O
have	O
seen	O
that	O
each	O
mode	O
of	O
the	O
posterior	O
distribution	O
in	O
a	O
gaussian	B
mixture	B
model	I
is	O
a	O
member	O
of	O
a	O
family	O
of	O
k	O
equivalent	O
modes	O
suppose	O
that	O
the	O
result	O
of	O
running	O
the	O
variational	B
inference	B
algorithm	O
is	O
an	O
approximate	O
posterior	O
distribution	O
q	O
that	O
is	O
localized	O
in	O
the	O
neighbourhood	O
of	O
one	O
of	O
the	O
modes	O
we	O
can	O
then	O
approximate	O
the	O
full	O
posterior	O
distribution	O
as	O
a	O
mixture	B
of	O
k	O
such	O
q	O
distributions	O
once	O
centred	O
on	O
each	O
mode	O
and	O
having	O
equal	O
mixing	O
coefficients	O
show	O
that	O
if	O
we	O
assume	O
negligible	O
overlap	O
between	O
the	O
components	O
of	O
the	O
q	O
mixture	B
the	O
resulting	O
lower	B
bound	I
differs	O
from	O
that	O
for	O
a	O
single	O
component	O
q	O
distribution	O
through	O
the	O
addition	O
of	O
an	O
extra	O
term	O
ln	O
k	O
www	O
consider	O
a	O
variational	B
gaussian	B
mixture	B
model	I
in	O
which	O
there	O
is	O
no	O
prior	B
distribution	O
over	O
mixing	O
coefficients	O
k	O
instead	O
the	O
mixing	O
coefficients	O
are	O
treated	O
as	O
parameters	O
whose	O
values	O
are	O
to	O
be	O
found	O
by	O
maximizing	O
the	O
variational	B
lower	B
bound	I
on	O
the	O
log	O
marginal	B
likelihood	I
show	O
that	O
maximizing	O
this	O
lower	B
bound	I
with	O
respect	O
to	O
the	O
mixing	O
coefficients	O
using	O
a	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
constraint	O
that	O
the	O
mixing	O
coefficients	O
sum	O
to	O
one	O
leads	O
to	O
the	O
re-estimation	O
result	O
note	O
that	O
there	O
is	O
no	O
need	O
to	O
consider	O
all	O
of	O
the	O
terms	O
in	O
the	O
lower	B
bound	I
but	O
only	O
the	O
dependence	O
of	O
the	O
bound	O
on	O
the	O
k	O
www	O
we	O
have	O
seen	O
in	O
section	O
that	O
the	O
singularities	B
arising	O
in	O
the	O
maximum	B
likelihood	I
treatment	O
of	O
gaussian	B
mixture	B
models	O
do	O
not	O
arise	O
in	O
a	O
bayesian	B
treatment	O
discuss	O
whether	O
such	O
singularities	B
would	O
arise	O
if	O
the	O
bayesian	B
model	O
were	O
solved	O
using	O
maximum	B
posterior	I
estimation	O
the	O
variational	B
treatment	O
of	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
discussed	O
in	O
section	O
made	O
use	O
of	O
a	O
factorized	O
approximation	O
to	O
the	O
posterior	O
distribution	O
as	O
we	O
saw	O
in	O
figure	O
the	O
factorized	O
assumption	O
causes	O
the	O
variance	B
of	O
the	O
posterior	O
distribution	O
to	O
be	O
under-estimated	O
for	O
certain	O
directions	O
in	O
parameter	O
space	O
discuss	O
qualitatively	O
the	O
effect	O
this	O
will	O
have	O
on	O
the	O
variational	B
approximation	O
to	O
the	O
model	B
evidence	I
and	O
how	O
this	O
effect	O
will	O
vary	O
with	O
the	O
number	O
of	O
components	O
in	O
the	O
mixture	B
hence	O
explain	O
whether	O
the	O
variational	B
gaussian	B
mixture	B
will	O
tend	O
to	O
under-estimate	O
or	O
over-estimate	O
the	O
optimal	O
number	O
of	O
components	O
extend	O
the	O
variational	B
treatment	O
of	O
bayesian	B
linear	B
regression	B
to	O
include	O
a	O
gamma	O
hyperprior	B
gam	O
over	O
and	O
solve	O
variationally	O
by	O
assuming	O
a	O
factorized	O
variational	B
distribution	O
of	O
the	O
form	O
qwq	O
derive	O
the	O
variational	B
update	O
equations	O
for	O
the	O
three	O
factors	O
in	O
the	O
variational	B
distribution	O
and	O
also	O
obtain	O
an	O
expression	O
for	O
the	O
lower	B
bound	I
and	O
for	O
the	O
predictive	B
distribution	I
by	O
making	O
use	O
of	O
the	O
formulae	O
given	O
in	O
appendix	O
b	O
show	O
that	O
the	O
variational	B
lower	B
bound	I
for	O
the	O
linear	O
basis	B
function	I
regression	B
model	O
defined	O
by	O
can	O
be	O
written	O
in	O
the	O
form	O
with	O
the	O
various	O
terms	O
defined	O
by	O
rewrite	O
the	O
model	O
for	O
the	O
bayesian	B
mixture	B
of	I
gaussians	I
introduced	O
in	O
section	O
as	O
a	O
conjugate	B
model	O
from	O
the	O
exponential	B
family	I
as	O
discussed	O
in	O
section	O
hence	O
use	O
the	O
general	O
results	O
and	O
to	O
derive	O
the	O
specific	O
results	O
and	O
exercises	O
www	O
show	O
that	O
the	O
function	O
fx	O
lnx	O
is	O
concave	O
for	O
x	O
by	O
computing	O
its	O
second	O
derivative	B
determine	O
the	O
form	O
of	O
the	O
dual	O
function	O
g	O
defined	O
by	O
and	O
verify	O
that	O
minimization	O
of	O
x	O
g	O
with	O
respect	O
to	O
according	O
to	O
indeed	O
recovers	O
the	O
function	O
lnx	O
by	O
evaluating	O
the	O
second	O
derivative	B
show	O
that	O
the	O
log	O
logistic	O
function	O
fx	O
e	O
x	O
is	O
concave	O
derive	O
the	O
variational	B
upper	O
bound	O
directly	O
by	O
making	O
a	O
second	B
order	I
taylor	O
expansion	O
of	O
the	O
log	O
logistic	O
function	O
around	O
a	O
point	O
x	O
by	O
finding	O
the	O
second	O
derivative	B
with	O
respect	O
to	O
x	O
show	O
that	O
the	O
function	O
fx	O
e	O
is	O
a	O
concave	B
function	I
of	O
x	O
now	O
consider	O
the	O
second	O
derivatives	O
with	O
respect	O
to	O
the	O
variable	O
and	O
hence	O
show	O
that	O
it	O
is	O
a	O
convex	B
function	I
of	O
plot	O
graphs	O
of	O
fx	O
against	O
x	O
and	O
against	O
derive	O
the	O
lower	B
bound	I
on	O
the	O
logistic	B
sigmoid	I
function	O
directly	O
by	O
making	O
a	O
first	O
order	O
taylor	O
series	O
expansion	O
of	O
the	O
function	O
fx	O
in	O
the	O
variable	O
centred	O
on	O
the	O
value	O
www	O
consider	O
the	O
variational	B
treatment	O
of	O
logistic	B
regression	B
with	O
sequential	B
learning	B
in	O
which	O
data	O
points	O
are	O
arriving	O
one	O
at	O
a	O
time	O
and	O
each	O
must	O
be	O
processed	O
and	O
discarded	O
before	O
the	O
next	O
data	O
point	O
arrives	O
show	O
that	O
a	O
gaussian	B
approximation	O
to	O
the	O
posterior	O
distribution	O
can	O
be	O
maintained	O
through	O
the	O
use	O
of	O
the	O
lower	B
bound	I
in	O
which	O
the	O
distribution	O
is	O
initialized	O
using	O
the	O
prior	B
and	O
as	O
each	O
data	O
point	O
is	O
absorbed	O
its	O
corresponding	O
variational	B
parameter	O
n	O
is	O
optimized	O
by	O
differentiating	O
the	O
quantity	O
q	O
old	O
defined	O
by	O
with	O
respect	O
to	O
the	O
variational	B
parameter	O
n	O
show	O
that	O
the	O
update	O
equation	O
for	O
n	O
for	O
the	O
bayesian	B
logistic	B
regression	B
model	O
is	O
given	O
by	O
in	O
this	O
exercise	O
we	O
derive	O
re-estimation	O
equations	O
for	O
the	O
variational	B
parameters	O
in	O
the	O
bayesian	B
logistic	B
regression	B
model	O
of	O
section	O
by	O
direct	O
maximization	O
of	O
the	O
lower	B
bound	I
given	O
by	O
to	O
do	O
this	O
set	O
the	O
derivative	B
of	O
l	O
with	O
respect	O
to	O
n	O
equal	O
to	O
zero	O
making	O
use	O
of	O
the	O
result	O
for	O
the	O
derivative	B
of	O
the	O
log	O
of	O
a	O
determinant	O
together	O
with	O
the	O
expressions	O
and	O
which	O
define	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
variational	B
posterior	O
distribution	O
qw	O
derive	O
the	O
result	O
for	O
the	O
lower	B
bound	I
l	O
in	O
the	O
variational	B
logistic	B
regression	B
model	O
this	O
is	O
most	O
easily	O
done	O
by	O
substituting	O
the	O
expressions	O
for	O
the	O
gaussian	B
prior	B
qw	O
n	O
together	O
with	O
the	O
lower	B
bound	I
hw	O
on	O
the	O
likelihood	B
function	I
into	O
the	O
integral	O
which	O
defines	O
l	O
next	O
gather	O
together	O
the	O
terms	O
which	O
depend	O
on	O
w	O
in	O
the	O
exponential	O
and	O
complete	O
the	O
square	O
to	O
give	O
a	O
gaussian	B
integral	O
which	O
can	O
then	O
be	O
evaluated	O
by	O
invoking	O
the	O
standard	O
result	O
for	O
the	O
normalization	O
coefficient	O
of	O
a	O
multivariate	O
gaussian	B
finally	O
take	O
the	O
logarithm	O
to	O
obtain	O
consider	O
the	O
adf	O
approximation	O
scheme	O
discussed	O
in	O
section	O
and	O
show	O
that	O
inclusion	O
of	O
the	O
factor	O
fj	O
leads	O
to	O
an	O
update	O
of	O
the	O
model	B
evidence	I
of	O
the	O
form	O
pjd	O
pj	O
approximate	O
inference	B
where	O
zj	O
is	O
the	O
normalization	O
constant	O
defined	O
by	O
by	O
applying	O
this	O
result	O
recursively	O
and	O
initializing	O
with	O
derive	O
the	O
result	O
zj	O
j	O
pd	O
www	O
consider	O
the	O
expectation	B
propagation	I
algorithm	O
from	O
section	O
and	O
suppose	O
that	O
one	O
of	O
the	O
factors	O
in	O
the	O
definition	O
has	O
the	O
same	O
exponential	B
family	I
functional	B
form	O
as	O
the	O
approximating	O
distribution	O
q	O
show	O
that	O
if	O
the	O
is	O
initialized	O
to	O
be	O
then	O
an	O
ep	O
update	O
to	O
leaves	O
unchanged	O
this	O
situation	O
typically	O
arises	O
when	O
one	O
of	O
the	O
factors	O
is	O
the	O
prior	B
p	O
and	O
so	O
we	O
see	O
that	O
the	O
prior	B
factor	O
can	O
be	O
incorporated	O
once	O
exactly	O
and	O
does	O
not	O
need	O
to	O
be	O
refined	O
in	O
this	O
exercise	O
and	O
the	O
next	O
we	O
shall	O
verify	O
the	O
results	O
for	O
the	O
expectation	B
propagation	I
algorithm	O
applied	O
to	O
the	O
clutter	B
problem	I
begin	O
by	O
using	O
the	O
division	O
formula	O
to	O
derive	O
the	O
expressions	O
and	O
by	O
completing	B
the	I
square	I
inside	O
the	O
exponential	O
to	O
identify	O
the	O
mean	B
and	O
variance	B
also	O
show	O
that	O
the	O
normalization	O
constant	O
zn	O
defined	O
by	O
is	O
given	O
for	O
the	O
clutter	B
problem	I
by	O
this	O
can	O
be	O
done	O
by	O
making	O
use	O
of	O
the	O
general	O
result	O
show	O
that	O
the	O
mean	B
and	O
variance	B
of	O
qnew	O
for	O
ep	O
applied	O
to	O
the	O
clutter	B
problem	I
are	O
given	O
by	O
and	O
to	O
do	O
this	O
first	O
prove	O
the	O
following	O
results	O
for	O
the	O
expectations	O
of	O
and	O
t	O
under	O
qnew	O
e	O
mn	O
v	O
mn	O
ln	O
zn	O
e	O
t	O
vn	O
ln	O
zn	O
and	O
then	O
make	O
use	O
of	O
the	O
result	O
for	O
zn	O
next	O
prove	O
the	O
results	O
by	O
using	O
and	O
completing	B
the	I
square	I
in	O
the	O
exponential	O
finally	O
use	O
to	O
derive	O
the	O
result	O
sampling	B
methods	I
for	O
most	O
probabilistic	O
models	O
of	O
practical	O
interest	O
exact	O
inference	B
is	O
intractable	O
and	O
so	O
we	O
have	O
to	O
resort	O
to	O
some	O
form	O
of	O
approximation	O
in	O
chapter	O
we	O
discussed	O
inference	B
algorithms	O
based	O
on	O
deterministic	O
approximations	O
which	O
include	O
methods	O
such	O
as	O
variational	B
bayes	B
and	O
expectation	B
propagation	I
here	O
we	O
consider	O
approximate	O
inference	B
methods	O
based	O
on	O
numerical	O
sampling	O
also	O
known	O
as	O
monte	O
carlo	O
techniques	O
although	O
for	O
some	O
applications	O
the	O
posterior	O
distribution	O
over	O
unobserved	O
variables	O
will	O
be	O
of	O
direct	O
interest	O
in	O
itself	O
for	O
most	O
situations	O
the	O
posterior	O
distribution	O
is	O
required	O
primarily	O
for	O
the	O
purpose	O
of	O
evaluating	O
expectations	O
for	O
example	O
in	O
order	O
to	O
make	O
predictions	O
the	O
fundamental	O
problem	O
that	O
we	O
therefore	O
wish	O
to	O
address	O
in	O
this	O
chapter	O
involves	O
finding	O
the	O
expectation	B
of	O
some	O
function	O
fz	O
with	O
respect	O
to	O
a	O
probability	B
distribution	O
pz	O
here	O
the	O
components	O
of	O
z	O
might	O
comprise	O
discrete	O
or	O
continuous	O
variables	O
or	O
some	O
combination	O
of	O
the	O
two	O
thus	O
in	O
the	O
case	O
of	O
continuous	O
sampling	B
methods	I
figure	O
schematic	O
illustration	O
of	O
a	O
function	O
f	O
whose	O
expectation	B
is	O
to	O
be	O
evaluated	O
with	O
respect	O
to	O
a	O
distribution	O
pz	O
pz	O
fz	O
z	O
variables	O
we	O
wish	O
to	O
evaluate	O
the	O
expectation	B
ef	O
fzpz	O
dz	O
where	O
the	O
integral	O
is	O
replaced	O
by	O
summation	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
this	O
is	O
illustrated	O
schematically	O
for	O
a	O
single	O
continuous	O
variable	O
in	O
figure	O
we	O
shall	O
suppose	O
that	O
such	O
expectations	O
are	O
too	O
complex	O
to	O
be	O
evaluated	O
exactly	O
using	O
analytical	O
techniques	O
the	O
general	O
idea	O
behind	O
sampling	B
methods	I
is	O
to	O
obtain	O
a	O
set	O
of	O
samples	O
zl	O
l	O
l	O
drawn	O
independently	O
from	O
the	O
distribution	O
pz	O
this	O
allows	O
the	O
expectation	B
to	O
be	O
approximated	O
by	O
a	O
finite	O
sum	O
as	O
long	O
as	O
the	O
samples	O
zl	O
are	O
drawn	O
from	O
the	O
distribution	O
pz	O
then	O
ef	O
and	O
so	O
the	O
has	O
the	O
correct	O
mean	B
the	O
variance	B
of	O
the	O
estimator	O
is	O
given	O
fzl	O
l	O
exercise	O
by	O
l	O
e	O
is	O
the	O
variance	B
of	O
the	O
function	O
fz	O
under	O
the	O
distribution	O
pz	O
it	O
is	O
worth	O
emphasizing	O
that	O
the	O
accuracy	O
of	O
the	O
estimator	O
therefore	O
does	O
not	O
depend	O
on	O
the	O
dimensionality	O
of	O
z	O
and	O
that	O
in	O
principle	O
high	O
accuracy	O
may	O
be	O
achievable	O
with	O
a	O
relatively	O
small	O
number	O
of	O
samples	O
zl	O
in	O
practice	O
ten	O
or	O
twenty	O
independent	B
samples	O
may	O
suffice	O
to	O
estimate	O
an	O
expectation	B
to	O
sufficient	O
accuracy	O
the	O
problem	O
however	O
is	O
that	O
the	O
samples	O
might	O
not	O
be	O
independent	B
and	O
so	O
the	O
effective	O
sample	O
size	O
might	O
be	O
much	O
smaller	O
than	O
the	O
apparent	O
sample	O
size	O
also	O
referring	O
back	O
to	O
figure	O
we	O
note	O
that	O
if	O
fz	O
is	O
small	O
in	O
regions	O
where	O
pz	O
is	O
large	O
and	O
vice	O
versa	O
then	O
the	O
expectation	B
may	O
be	O
dominated	O
by	O
regions	O
of	O
small	O
probability	B
implying	O
that	O
relatively	O
large	O
sample	O
sizes	O
will	O
be	O
required	O
to	O
achieve	O
sufficient	O
accuracy	O
for	O
many	O
models	O
the	O
joint	O
distribution	O
pz	O
is	O
conveniently	O
specified	O
in	O
terms	O
of	O
a	O
graphical	B
model	I
in	O
the	O
case	O
of	O
a	O
directed	B
graph	O
with	O
no	O
observed	O
variables	O
it	O
is	O
sampling	B
methods	I
straightforward	O
to	O
sample	O
from	O
the	O
joint	O
distribution	O
that	O
it	O
is	O
possible	O
to	O
sample	O
from	O
the	O
conditional	B
distributions	O
at	O
each	O
node	B
using	O
the	O
following	O
ancestral	B
sampling	I
approach	O
discussed	O
briefly	O
in	O
section	O
the	O
joint	O
distribution	O
is	O
specified	O
by	O
pz	O
pzipai	O
where	O
zi	O
are	O
the	O
set	O
of	O
variables	O
associated	O
with	O
node	B
i	O
and	O
pai	O
denotes	O
the	O
set	O
of	O
variables	O
associated	O
with	O
the	O
parents	O
of	O
node	B
i	O
to	O
obtain	O
a	O
sample	O
from	O
the	O
joint	O
distribution	O
we	O
make	O
one	O
pass	O
through	O
the	O
set	O
of	O
variables	O
in	O
the	O
order	O
zm	O
sampling	O
from	O
the	O
conditional	B
distributions	O
pzipai	O
this	O
is	O
always	O
possible	O
because	O
at	O
each	O
step	O
all	O
of	O
the	O
parent	O
values	O
will	O
have	O
been	O
instantiated	O
after	O
one	O
pass	O
through	O
the	O
graph	O
we	O
will	O
have	O
obtained	O
a	O
sample	O
from	O
the	O
joint	O
distribution	O
now	O
consider	O
the	O
case	O
of	O
a	O
directed	B
graph	O
in	O
which	O
some	O
of	O
the	O
nodes	O
are	O
instantiated	O
with	O
observed	O
values	O
we	O
can	O
in	O
principle	O
extend	O
the	O
above	O
procedure	O
at	O
least	O
in	O
the	O
case	O
of	O
nodes	O
representing	O
discrete	O
variables	O
to	O
give	O
the	O
following	O
logic	B
sampling	I
approach	O
which	O
can	O
be	O
seen	O
as	O
a	O
special	O
case	O
of	O
importance	B
sampling	I
discussed	O
in	O
section	O
at	O
each	O
step	O
when	O
a	O
sampled	O
value	O
is	O
obtained	O
for	O
a	O
variable	O
zi	O
whose	O
value	O
is	O
observed	O
the	O
sampled	O
value	O
is	O
compared	O
to	O
the	O
observed	O
value	O
and	O
if	O
they	O
agree	O
then	O
the	O
sample	O
value	O
is	O
retained	O
and	O
the	O
algorithm	O
proceeds	O
to	O
the	O
next	O
variable	O
in	O
turn	O
however	O
if	O
the	O
sampled	O
value	O
and	O
the	O
observed	O
value	O
disagree	O
then	O
the	O
whole	O
sample	O
so	O
far	O
is	O
discarded	O
and	O
the	O
algorithm	O
starts	O
again	O
with	O
the	O
first	O
node	B
in	O
the	O
graph	O
this	O
algorithm	O
samples	O
correctly	O
from	O
the	O
posterior	O
distribution	O
because	O
it	O
corresponds	O
simply	O
to	O
drawing	O
samples	O
from	O
the	O
joint	O
distribution	O
of	O
hidden	O
variables	O
and	O
data	O
variables	O
and	O
then	O
discarding	O
those	O
samples	O
that	O
disagree	O
with	O
the	O
observed	O
data	O
the	O
slight	O
saving	O
of	O
not	O
continuing	O
with	O
the	O
sampling	O
from	O
the	O
joint	O
distribution	O
as	O
soon	O
as	O
one	O
contradictory	O
value	O
is	O
observed	O
however	O
the	O
overall	O
probability	B
of	O
accepting	O
a	O
sample	O
from	O
the	O
posterior	O
decreases	O
rapidly	O
as	O
the	O
number	O
of	O
observed	O
variables	O
increases	O
and	O
as	O
the	O
number	O
of	O
states	O
that	O
those	O
variables	O
can	O
take	O
increases	O
and	O
so	O
this	O
approach	O
is	O
rarely	O
used	O
in	O
practice	O
in	O
the	O
case	O
of	O
probability	B
distributions	O
defined	O
by	O
an	O
undirected	B
graph	O
there	O
is	O
no	O
one-pass	O
sampling	O
strategy	O
that	O
will	O
sample	O
even	O
from	O
the	O
prior	B
distribution	O
with	O
no	O
observed	O
variables	O
instead	O
computationally	O
more	O
expensive	O
techniques	O
must	O
be	O
employed	O
such	O
as	O
gibbs	B
sampling	I
which	O
is	O
discussed	O
in	O
section	O
as	O
well	O
as	O
sampling	O
from	O
conditional	B
distributions	O
we	O
may	O
also	O
require	O
samples	O
from	O
a	O
marginal	B
distribution	O
if	O
we	O
already	O
have	O
a	O
strategy	O
for	O
sampling	O
from	O
a	O
joint	O
distribution	O
pu	O
v	O
then	O
it	O
is	O
straightforward	O
to	O
obtain	O
samples	O
from	O
the	O
marginal	B
distribution	O
pu	O
simply	O
by	O
ignoring	O
the	O
values	O
for	O
v	O
in	O
each	O
sample	O
there	O
are	O
numerous	O
texts	O
dealing	O
with	O
monte	O
carlo	O
methods	O
those	O
of	O
particular	O
interest	O
from	O
the	O
statistical	O
inference	B
perspective	O
include	O
chen	O
et	O
al	O
gamerman	O
gilks	O
et	O
al	O
liu	O
neal	O
and	O
robert	O
and	O
casella	O
also	O
there	O
are	O
review	O
articles	O
by	O
besag	O
et	O
al	O
brooks	O
diaconis	O
and	O
saloff-coste	O
jerrum	O
and	O
sinclair	O
neal	O
tierney	O
and	O
andrieu	O
et	O
al	O
that	O
provide	O
additional	O
information	O
on	O
sampling	O
sampling	B
methods	I
methods	O
for	O
statistical	O
inference	B
diagnostic	O
tests	O
for	O
convergence	O
of	O
markov	B
chain	I
monte	I
carlo	I
algorithms	O
are	O
summarized	O
in	O
robert	O
and	O
casella	O
and	O
some	O
practical	O
guidance	O
on	O
the	O
use	O
of	O
sampling	B
methods	I
in	O
the	O
context	O
of	O
machine	O
learning	B
is	O
given	O
in	O
bishop	O
and	O
nabney	O
basic	O
sampling	O
algorithms	O
in	O
this	O
section	O
we	O
consider	O
some	O
simple	O
strategies	O
for	O
generating	O
random	O
samples	O
from	O
a	O
given	O
distribution	O
because	O
the	O
samples	O
will	O
be	O
generated	O
by	O
a	O
computer	O
algorithm	O
they	O
will	O
in	O
fact	O
be	O
pseudo-random	B
numbers	I
that	O
is	O
they	O
will	O
be	O
deterministically	O
calculated	O
but	O
must	O
nevertheless	O
pass	O
appropriate	O
tests	O
for	O
randomness	O
generating	O
such	O
numbers	O
raises	O
several	O
subtleties	O
et	O
al	O
that	O
lie	O
outside	O
the	O
scope	O
of	O
this	O
book	O
here	O
we	O
shall	O
assume	O
that	O
an	O
algorithm	O
has	O
been	O
provided	O
that	O
generates	O
pseudo-random	B
numbers	I
distributed	O
uniformly	O
over	O
and	O
indeed	O
most	O
software	O
environments	O
have	O
such	O
a	O
facility	O
built	O
in	O
standard	O
distributions	O
we	O
first	O
consider	O
how	O
to	O
generate	O
random	O
numbers	O
from	O
simple	O
nonuniform	O
distributions	O
assuming	O
that	O
we	O
already	O
have	O
available	O
a	O
source	O
of	O
uniformly	O
distributed	O
random	O
numbers	O
suppose	O
that	O
z	O
is	O
uniformly	O
distributed	O
over	O
the	O
interval	O
and	O
that	O
we	O
transform	O
the	O
values	O
of	O
z	O
using	O
some	O
function	O
f	O
so	O
that	O
y	O
fz	O
the	O
distribution	O
of	O
y	O
will	O
be	O
governed	O
by	O
py	O
pz	O
dy	O
dz	O
exercise	O
z	O
hy	O
where	O
in	O
this	O
case	O
pz	O
our	O
goal	O
is	O
to	O
choose	O
the	O
function	O
fz	O
such	O
that	O
the	O
resulting	O
values	O
of	O
y	O
have	O
some	O
specific	O
desired	O
distribution	O
py	O
integrating	O
we	O
obtain	O
y	O
and	O
so	O
we	O
have	O
to	O
which	O
is	O
the	O
indefinite	O
integral	O
of	O
py	O
thus	O
y	O
h	O
transform	O
the	O
uniformly	O
distributed	O
random	O
numbers	O
using	O
a	O
function	O
which	O
is	O
the	O
inverse	B
of	O
the	O
indefinite	O
integral	O
of	O
the	O
desired	O
distribution	O
this	O
is	O
illustrated	O
in	O
figure	O
consider	O
for	O
example	O
the	O
exponential	B
distribution	I
py	O
exp	O
y	O
where	O
y	O
in	O
this	O
case	O
the	O
lower	O
limit	O
of	O
the	O
integral	O
in	O
is	O
and	O
so	O
hy	O
exp	O
y	O
thus	O
if	O
we	O
transform	O
our	O
uniformly	O
distributed	O
variable	O
z	O
using	O
y	O
z	O
then	O
y	O
will	O
have	O
an	O
exponential	B
distribution	I
basic	O
sampling	O
algorithms	O
figure	O
geometrical	O
interpretation	O
of	O
the	O
transformation	O
method	O
for	O
generating	O
nonuniformly	O
distributed	O
random	O
numbers	O
hy	O
is	O
the	O
indefinite	O
integral	O
of	O
the	O
desired	O
distribution	O
py	O
if	O
a	O
uniformly	O
distributed	O
random	O
variable	O
z	O
is	O
transformed	O
using	O
y	O
h	O
then	O
y	O
will	O
be	O
distributed	O
according	O
to	O
py	O
hy	O
py	O
y	O
another	O
example	O
of	O
a	O
distribution	O
to	O
which	O
the	O
transformation	O
method	O
can	O
be	O
applied	O
is	O
given	O
by	O
the	O
cauchy	B
distribution	I
py	O
exercise	O
in	O
this	O
case	O
the	O
inverse	B
of	O
the	O
indefinite	O
integral	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
tan	O
function	O
the	O
generalization	B
to	O
multiple	O
variables	O
is	O
straightforward	O
and	O
involves	O
the	O
ja	O
cobian	O
of	O
the	O
change	O
of	O
variables	O
so	O
that	O
ym	O
zm	O
zm	O
ym	O
as	O
a	O
final	O
example	O
of	O
the	O
transformation	O
method	O
we	O
consider	O
the	O
box-muller	B
method	I
for	O
generating	O
samples	O
from	O
a	O
gaussian	B
distribution	O
first	O
suppose	O
we	O
generate	O
pairs	O
of	O
uniformly	O
distributed	O
random	O
numbers	O
which	O
we	O
can	O
do	O
by	O
transforming	O
a	O
variable	O
distributed	O
uniformly	O
over	O
using	O
z	O
this	O
leads	O
to	O
a	O
uniform	O
next	O
we	O
discard	O
each	O
pair	O
unless	O
it	O
satisfies	O
distribution	O
of	O
points	O
inside	O
the	O
unit	O
circle	O
with	O
as	O
illustrated	O
in	O
figure	O
then	O
for	O
each	O
pair	O
we	O
evaluate	O
the	O
quantities	O
figure	O
the	O
box-muller	B
method	I
for	O
generating	O
gaussian	B
distributed	O
random	O
numbers	O
starts	O
by	O
generating	O
samples	O
from	O
a	O
uniform	B
distribution	I
inside	O
the	O
unit	O
circle	O
sampling	B
methods	I
ln	O
ln	O
exercise	O
exercise	O
where	O
then	O
the	O
joint	O
distribution	O
of	O
and	O
is	O
given	O
by	O
exp	O
exp	O
and	O
so	O
and	O
are	O
independent	B
and	O
each	O
has	O
a	O
gaussian	B
distribution	O
with	O
zero	O
mean	B
and	O
unit	O
variance	B
if	O
y	O
has	O
a	O
gaussian	B
distribution	O
with	O
zero	O
mean	B
and	O
unit	O
variance	B
then	O
y	O
will	O
have	O
a	O
gaussian	B
distribution	O
with	O
mean	B
and	O
variance	B
to	O
generate	O
vectorvalued	O
variables	O
having	O
a	O
multivariate	O
gaussian	B
distribution	O
with	O
mean	B
and	O
covariance	B
we	O
can	O
make	O
use	O
of	O
the	O
cholesky	B
decomposition	I
which	O
takes	O
the	O
form	O
llt	O
et	O
al	O
then	O
if	O
z	O
is	O
a	O
vector	O
valued	O
random	O
variable	O
whose	O
components	O
are	O
independent	B
and	O
gaussian	B
distributed	O
with	O
zero	O
mean	B
and	O
unit	O
variance	B
then	O
y	O
lz	O
will	O
have	O
mean	B
and	O
covariance	B
obviously	O
the	O
transformation	O
technique	O
depends	O
for	O
its	O
success	O
on	O
the	O
ability	O
to	O
calculate	O
and	O
then	O
invert	O
the	O
indefinite	O
integral	O
of	O
the	O
required	O
distribution	O
such	O
operations	O
will	O
only	O
be	O
feasible	O
for	O
a	O
limited	O
number	O
of	O
simple	O
distributions	O
and	O
so	O
we	O
must	O
turn	O
to	O
alternative	O
approaches	O
in	O
search	O
of	O
a	O
more	O
general	O
strategy	O
here	O
we	O
consider	O
two	O
techniques	O
called	O
rejection	B
sampling	I
and	O
importance	B
sampling	I
although	O
mainly	O
limited	O
to	O
univariate	O
distributions	O
and	O
thus	O
not	O
directly	O
applicable	O
to	O
complex	O
problems	O
in	O
many	O
dimensions	O
they	O
do	O
form	O
important	O
components	O
in	O
more	O
general	O
strategies	O
rejection	B
sampling	I
the	O
rejection	B
sampling	I
framework	O
allows	O
us	O
to	O
sample	O
from	O
relatively	O
complex	O
distributions	O
subject	O
to	O
certain	O
constraints	O
we	O
begin	O
by	O
considering	O
univariate	O
distributions	O
and	O
discuss	O
the	O
extension	O
to	O
multiple	O
dimensions	O
subsequently	O
suppose	O
we	O
wish	O
to	O
sample	O
from	O
a	O
distribution	O
pz	O
that	O
is	O
not	O
one	O
of	O
the	O
simple	O
standard	O
distributions	O
considered	O
so	O
far	O
and	O
that	O
sampling	O
directly	O
from	O
pz	O
is	O
difficult	O
furthermore	O
suppose	O
as	O
is	O
often	O
the	O
case	O
that	O
we	O
are	O
easily	O
able	O
to	O
evaluate	O
pz	O
for	O
any	O
given	O
value	O
of	O
z	O
up	O
to	O
some	O
normalizing	O
constant	O
z	O
so	O
that	O
zp	O
can	O
readily	O
be	O
evaluated	O
but	O
zp	O
is	O
unknown	O
pz	O
in	O
order	O
to	O
apply	O
rejection	B
sampling	I
we	O
need	O
some	O
simpler	O
distribution	O
qz	O
sometimes	O
called	O
a	O
proposal	B
distribution	I
from	O
which	O
we	O
can	O
readily	O
draw	O
samples	O
figure	O
in	O
the	O
rejection	B
sampling	I
method	O
samples	O
are	O
drawn	O
from	O
a	O
simple	O
distribution	O
qz	O
and	O
rejected	O
if	O
they	O
fall	O
in	O
the	O
grey	O
area	O
between	O
the	O
unnormalized	O
distribution	O
epz	O
and	O
the	O
scaled	O
distribution	O
kqz	O
the	O
resulting	O
samples	O
are	O
distributed	O
according	O
to	O
pz	O
which	O
is	O
the	O
normalized	O
version	O
of	O
epz	O
basic	O
sampling	O
algorithms	O
kqz	O
pz	O
exercise	O
z	O
we	O
next	O
introduce	O
a	O
constant	O
k	O
whose	O
value	O
is	O
chosen	O
such	O
that	O
kqz	O
for	O
all	O
values	O
of	O
z	O
the	O
function	O
kqz	O
is	O
called	O
the	O
comparison	O
function	O
and	O
is	O
illustrated	O
for	O
a	O
univariate	O
distribution	O
in	O
figure	O
each	O
step	O
of	O
the	O
rejection	O
sampler	O
involves	O
generating	O
two	O
random	O
numbers	O
first	O
we	O
generate	O
a	O
number	O
from	O
the	O
distribution	O
qz	O
next	O
we	O
generate	O
a	O
number	O
from	O
the	O
uniform	B
distribution	I
over	O
this	O
pair	O
of	O
random	O
numbers	O
has	O
uniform	B
distribution	I
under	O
the	O
curve	O
of	O
the	O
function	O
kqz	O
finally	O
if	O
then	O
the	O
sample	O
is	O
rejected	O
otherwise	O
ure	O
the	O
remaining	O
pairs	O
then	O
have	O
uniform	B
distribution	I
under	O
the	O
curve	O
ples	O
are	O
then	O
accepted	O
with	O
and	O
so	O
the	O
probability	B
that	O
a	O
and	O
hence	O
the	O
corresponding	O
z	O
values	O
are	O
distributed	O
according	O
to	O
pz	O
as	O
desired	O
the	O
original	O
values	O
of	O
z	O
are	O
generated	O
from	O
the	O
distribution	O
qz	O
and	O
these	O
sam	O
is	O
retained	O
thus	O
the	O
pair	O
is	O
rejected	O
if	O
it	O
lies	O
in	O
the	O
grey	O
shaded	O
region	O
in	O
fig	O
paccept	O
sample	O
will	O
be	O
accepted	O
is	O
given	O
by	O
qz	O
dz	O
dz	O
the	O
area	O
under	O
the	O
unnormalized	O
to	O
the	O
area	O
under	O
the	O
curve	O
kqz	O
limitation	O
that	O
kqz	O
must	O
be	O
nowhere	O
less	O
we	O
therefore	O
see	O
that	O
the	O
constant	O
k	O
should	O
be	O
as	O
small	O
as	O
possible	O
subject	O
to	O
the	O
thus	O
the	O
fraction	O
of	O
points	O
that	O
are	O
rejected	O
by	O
this	O
method	O
depends	O
on	O
the	O
ratio	O
of	O
k	O
as	O
an	O
illustration	O
of	O
the	O
use	O
of	O
rejection	B
sampling	I
consider	O
the	O
task	O
of	O
sampling	O
from	O
the	O
gamma	B
distribution	I
gamza	O
b	O
baza	O
exp	O
bz	O
which	O
for	O
a	O
has	O
a	O
bell-shaped	O
form	O
as	O
shown	O
in	O
figure	O
a	O
suitable	O
proposal	B
distribution	I
is	O
therefore	O
the	O
cauchy	O
because	O
this	O
too	O
is	O
bell-shaped	O
and	O
because	O
we	O
can	O
use	O
the	O
transformation	O
method	O
discussed	O
earlier	O
to	O
sample	O
from	O
it	O
we	O
need	O
to	O
generalize	O
the	O
cauchy	O
slightly	O
to	O
ensure	O
that	O
it	O
nowhere	O
has	O
a	O
smaller	O
value	O
than	O
the	O
gamma	B
distribution	I
this	O
can	O
be	O
achieved	O
by	O
transforming	O
a	O
uniform	O
random	O
variable	O
y	O
using	O
z	O
b	O
tan	O
y	O
c	O
which	O
gives	O
random	O
numbers	O
distributed	O
according	O
to	O
exercise	O
sampling	B
methods	I
figure	O
plot	O
showing	O
the	O
gamma	B
distribution	I
given	O
by	O
as	O
the	O
green	O
curve	O
with	O
a	O
scaled	O
cauchy	O
proposal	B
distribution	I
shown	O
by	O
the	O
red	O
curve	O
samples	O
from	O
the	O
gamma	B
distribution	I
can	O
be	O
obtained	O
by	O
sampling	O
from	O
the	O
cauchy	O
and	O
then	O
applying	O
the	O
rejection	B
sampling	I
criterion	O
pz	O
z	O
qz	O
the	O
minimum	O
reject	O
rate	O
is	O
obtained	O
by	O
setting	O
c	O
a	O
and	O
choosing	O
the	O
constant	O
k	O
to	O
be	O
as	O
small	O
as	O
possible	O
while	O
still	O
satisfying	O
the	O
requirement	O
kqz	O
the	O
resulting	O
comparison	O
function	O
is	O
also	O
illustrated	O
in	O
figure	O
k	O
adaptive	B
rejection	B
sampling	I
in	O
many	O
instances	O
where	O
we	O
might	O
wish	O
to	O
apply	O
rejection	B
sampling	I
it	O
proves	O
difficult	O
to	O
determine	O
a	O
suitable	O
analytic	O
form	O
for	O
the	O
envelope	O
distribution	O
qz	O
an	O
alternative	O
approach	O
is	O
to	O
construct	O
the	O
envelope	O
function	O
on	O
the	O
fly	O
based	O
on	O
measured	O
values	O
of	O
the	O
distribution	O
pz	O
and	O
wild	O
construction	O
of	O
an	O
envelope	O
function	O
is	O
particularly	O
straightforward	O
for	O
cases	O
in	O
which	O
pz	O
is	O
log	O
concave	O
in	O
other	O
words	O
when	O
ln	O
pz	O
has	O
derivatives	O
that	O
are	O
nonincreasing	O
functions	O
of	O
z	O
the	O
construction	O
of	O
a	O
suitable	O
envelope	O
function	O
is	O
illustrated	O
graphically	O
in	O
figure	O
the	O
function	O
ln	O
pz	O
and	O
its	O
gradient	O
are	O
evaluated	O
at	O
some	O
initial	O
set	O
of	O
grid	O
points	O
and	O
the	O
intersections	O
of	O
the	O
resulting	O
tangent	O
lines	O
are	O
used	O
to	O
construct	O
the	O
envelope	O
function	O
next	O
a	O
sample	O
value	O
is	O
drawn	O
from	O
the	O
envelope	O
distribution	O
this	O
is	O
straightforward	O
because	O
the	O
log	O
of	O
the	O
envelope	O
distribution	O
is	O
a	O
succession	O
exercise	O
figure	O
in	O
the	O
case	O
of	O
distributions	O
that	O
are	O
log	O
concave	O
an	O
envelope	O
function	O
for	O
use	O
in	O
rejection	B
sampling	I
can	O
be	O
constructed	O
using	O
the	O
tangent	O
lines	O
computed	O
at	O
a	O
set	O
of	O
grid	O
points	O
if	O
a	O
sample	O
point	O
is	O
rejected	O
it	O
is	O
added	O
to	O
the	O
set	O
of	O
grid	O
points	O
and	O
used	O
to	O
refine	O
the	O
envelope	O
distribution	O
ln	O
pz	O
z	O
basic	O
sampling	O
algorithms	O
figure	O
illustrative	O
example	O
of	O
rejection	B
sampling	I
involving	O
sampling	O
from	O
a	O
gaussian	B
distribution	O
pz	O
shown	O
by	O
the	O
green	O
curve	O
by	O
using	O
rejection	B
sampling	I
from	O
a	O
proposal	B
distribution	I
qz	O
that	O
is	O
also	O
gaussian	B
and	O
whose	O
scaled	O
version	O
kqz	O
is	O
shown	O
by	O
the	O
red	O
curve	O
pz	O
z	O
of	O
linear	O
functions	O
and	O
hence	O
the	O
envelope	O
distribution	O
itself	O
comprises	O
a	O
piecewise	O
exponential	B
distribution	I
of	O
the	O
form	O
qz	O
ki	O
i	O
exp	O
iz	O
zi	O
zi	O
z	O
zi	O
once	O
a	O
sample	O
has	O
been	O
drawn	O
the	O
usual	O
rejection	O
criterion	O
can	O
be	O
applied	O
if	O
the	O
sample	O
is	O
accepted	O
then	O
it	O
will	O
be	O
a	O
draw	O
from	O
the	O
desired	O
distribution	O
if	O
however	O
the	O
sample	O
is	O
rejected	O
then	O
it	O
is	O
incorporated	O
into	O
the	O
set	O
of	O
grid	O
points	O
a	O
new	O
tangent	O
line	O
is	O
computed	O
and	O
the	O
envelope	O
function	O
is	O
thereby	O
refined	O
as	O
the	O
number	O
of	O
grid	O
points	O
increases	O
so	O
the	O
envelope	O
function	O
becomes	O
a	O
better	O
approximation	O
of	O
the	O
desired	O
distribution	O
pz	O
and	O
the	O
probability	B
of	O
rejection	O
decreases	O
a	O
variant	O
of	O
the	O
algorithm	O
exists	O
that	O
avoids	O
the	O
evaluation	O
of	O
derivatives	O
the	O
adaptive	B
rejection	B
sampling	I
framework	O
can	O
also	O
be	O
extended	B
to	O
distributions	O
that	O
are	O
not	O
log	O
concave	O
simply	O
by	O
following	O
each	O
rejection	B
sampling	I
step	O
with	O
a	O
metropolis-hastings	O
step	O
be	O
discussed	O
in	O
section	O
giving	O
rise	O
to	O
adaptive	O
rejection	O
metropolis	O
sampling	O
et	O
al	O
clearly	O
for	O
rejection	B
sampling	I
to	O
be	O
of	O
practical	O
value	O
we	O
require	O
that	O
the	O
comparison	O
function	O
be	O
close	O
to	O
the	O
required	O
distribution	O
so	O
that	O
the	O
rate	O
of	O
rejection	O
is	O
kept	O
to	O
a	O
minimum	O
now	O
let	O
us	O
examine	O
what	O
happens	O
when	O
we	O
try	O
to	O
use	O
rejection	B
sampling	I
in	O
spaces	O
of	O
high	O
dimensionality	O
consider	O
for	O
the	O
sake	O
of	O
illustration	O
a	O
somewhat	O
artificial	O
problem	O
in	O
which	O
we	O
wish	O
to	O
sample	O
from	O
a	O
zero-mean	O
multivariate	O
gaussian	B
distribution	O
with	O
covariance	B
pi	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
by	O
rejection	B
sampling	I
from	O
a	O
proposal	B
distribution	I
that	O
is	O
itself	O
a	O
zero-mean	O
gaussian	B
distribution	O
having	O
covariance	B
p	O
in	O
order	O
that	O
there	O
exists	O
a	O
k	O
such	O
that	O
kqz	O
pz	O
in	O
d-dimensions	O
the	O
optimum	O
value	O
of	O
k	O
is	O
given	O
by	O
k	O
q	O
pd	O
as	O
illustrated	O
for	O
d	O
in	O
figure	O
the	O
acceptance	O
rate	O
will	O
be	O
the	O
ratio	O
of	O
volumes	O
under	O
pz	O
and	O
kqz	O
which	O
because	O
both	O
distributions	O
are	O
normalized	O
is	O
just	O
thus	O
the	O
acceptance	O
rate	O
diminishes	O
exponentially	O
with	O
dimensionality	O
even	O
if	O
q	O
exceeds	O
p	O
by	O
just	O
one	O
percent	O
for	O
d	O
the	O
acceptance	O
ratio	O
will	O
be	O
approximately	O
in	O
this	O
illustrative	O
example	O
the	O
comparison	O
function	O
is	O
close	O
to	O
the	O
required	O
distribution	O
for	O
more	O
practical	O
examples	O
where	O
the	O
desired	O
distribution	O
may	O
be	O
multimodal	O
and	O
sharply	O
peaked	O
it	O
will	O
be	O
extremely	O
difficult	O
to	O
find	O
a	O
good	O
proposal	B
distribution	I
and	O
comparison	O
function	O
qi	O
obviously	O
we	O
must	O
have	O
q	O
sampling	B
methods	I
figure	O
importance	B
sampling	I
addresses	O
the	O
problem	O
of	O
evaluating	O
the	O
expectation	B
of	O
a	O
function	O
f	O
with	O
respect	O
to	O
a	O
distribution	O
pz	O
from	O
which	O
it	O
is	O
difficult	O
to	O
draw	O
samples	O
diinstead	O
samples	O
are	O
drawn	O
rectly	O
from	O
a	O
simpler	O
distribution	O
qz	O
and	O
the	O
corresponding	O
terms	O
in	O
the	O
summation	O
are	O
weighted	O
by	O
the	O
ratios	O
pzlqzl	O
pz	O
qz	O
fz	O
z	O
furthermore	O
the	O
exponential	O
decrease	O
of	O
acceptance	O
rate	O
with	O
dimensionality	O
is	O
a	O
generic	O
feature	O
of	O
rejection	B
sampling	I
although	O
rejection	O
can	O
be	O
a	O
useful	O
technique	O
in	O
one	O
or	O
two	O
dimensions	O
it	O
is	O
unsuited	O
to	O
problems	O
of	O
high	O
dimensionality	O
it	O
can	O
however	O
play	O
a	O
role	O
as	O
a	O
subroutine	O
in	O
more	O
sophisticated	O
algorithms	O
for	O
sampling	O
in	O
high	O
dimensional	O
spaces	O
importance	B
sampling	I
one	O
of	O
the	O
principal	O
reasons	O
for	O
wishing	O
to	O
sample	O
from	O
complicated	O
probability	B
distributions	O
is	O
to	O
be	O
able	O
to	O
evaluate	O
expectations	O
of	O
the	O
form	O
the	O
technique	O
of	O
importance	B
sampling	I
provides	O
a	O
framework	O
for	O
approximating	O
expectations	O
directly	O
but	O
does	O
not	O
itself	O
provide	O
a	O
mechanism	O
for	O
drawing	O
samples	O
from	O
distribution	O
pz	O
the	O
finite	O
sum	O
approximation	O
to	O
the	O
expectation	B
given	O
by	O
depends	O
on	O
being	O
able	O
to	O
draw	O
samples	O
from	O
the	O
distribution	O
pz	O
suppose	O
however	O
that	O
it	O
is	O
impractical	O
to	O
sample	O
directly	O
from	O
pz	O
but	O
that	O
we	O
can	O
evaluate	O
pz	O
easily	O
for	O
any	O
given	O
value	O
of	O
z	O
one	O
simplistic	O
strategy	O
for	O
evaluating	O
expectations	O
would	O
be	O
to	O
discretize	O
z-space	O
into	O
a	O
uniform	O
grid	O
and	O
to	O
evaluate	O
the	O
integrand	O
as	O
a	O
sum	O
of	O
the	O
form	O
ef	O
pzlfzl	O
an	O
obvious	O
problem	O
with	O
this	O
approach	O
is	O
that	O
the	O
number	O
of	O
terms	O
in	O
the	O
summation	O
grows	O
exponentially	O
with	O
the	O
dimensionality	O
of	O
z	O
furthermore	O
as	O
we	O
have	O
already	O
noted	O
the	O
kinds	O
of	O
probability	B
distributions	O
of	O
interest	O
will	O
often	O
have	O
much	O
of	O
their	O
mass	O
confined	O
to	O
relatively	O
small	O
regions	O
of	O
z	O
space	O
and	O
so	O
uniform	B
sampling	I
will	O
be	O
very	O
inefficient	O
because	O
in	O
high-dimensional	O
problems	O
only	O
a	O
very	O
small	O
proportion	O
of	O
the	O
samples	O
will	O
make	O
a	O
significant	O
contribution	O
to	O
the	O
sum	O
we	O
would	O
really	O
like	O
to	O
choose	O
the	O
sample	O
points	O
to	O
fall	O
in	O
regions	O
where	O
pz	O
is	O
large	O
or	O
ideally	O
where	O
the	O
product	O
pzfz	O
is	O
large	O
as	O
in	O
the	O
case	O
of	O
rejection	B
sampling	I
importance	B
sampling	I
is	O
based	O
on	O
the	O
use	O
of	O
a	O
proposal	B
distribution	I
qz	O
from	O
which	O
it	O
is	O
easy	O
to	O
draw	O
samples	O
as	O
illustrated	O
in	O
figure	O
we	O
can	O
then	O
express	O
the	O
expectation	B
in	O
the	O
form	O
of	O
a	O
finite	O
sum	O
over	O
basic	O
sampling	O
algorithms	O
samples	O
drawn	O
from	O
qz	O
ef	O
l	O
fzpz	O
dz	O
fz	O
pz	O
qz	O
qz	O
dz	O
pzl	O
qzl	O
fzl	O
the	O
quantities	O
rl	O
pzlqzl	O
are	O
known	O
as	O
importance	B
weights	I
and	O
they	O
correct	O
the	O
bias	B
introduced	O
by	O
sampling	O
from	O
the	O
wrong	O
distribution	O
note	O
that	O
unlike	O
rejection	B
sampling	I
all	O
of	O
the	O
samples	O
generated	O
are	O
retained	O
normalization	O
constant	O
so	O
that	O
pz	O
can	O
be	O
evaluated	O
easily	O
distribution	O
qz	O
which	O
has	O
the	O
same	O
property	O
we	O
then	O
have	O
it	O
will	O
often	O
be	O
the	O
case	O
that	O
the	O
distribution	O
pz	O
can	O
only	O
be	O
evaluated	O
up	O
to	O
a	O
whereas	O
zp	O
is	O
unknown	O
similarly	O
we	O
may	O
wish	O
to	O
use	O
an	O
importance	B
sampling	I
we	O
can	O
use	O
the	O
same	O
sample	O
set	O
to	O
evaluate	O
the	O
ratio	O
l	O
ef	O
fz	O
fzpz	O
dz	O
zq	O
zp	O
zq	O
zp	O
qz	O
dz	O
qz	O
dz	O
dz	O
ef	O
wlfzl	O
zq	O
l	O
zpzq	O
with	O
the	O
result	O
zp	O
zq	O
and	O
hence	O
where	O
we	O
have	O
defined	O
wl	O
as	O
with	O
rejection	B
sampling	I
the	O
success	O
of	O
the	O
importance	B
sampling	I
approach	O
depends	O
crucially	O
on	O
how	O
well	O
the	O
sampling	O
distribution	O
qz	O
matches	O
the	O
desired	O
sampling	B
methods	I
distribution	O
pz	O
if	O
as	O
is	O
often	O
the	O
case	O
pzfz	O
is	O
strongly	O
varying	O
and	O
has	O
a	O
significant	O
proportion	O
of	O
its	O
mass	O
concentrated	O
over	O
relatively	O
small	O
regions	O
of	O
z	O
space	O
then	O
the	O
set	O
of	O
importance	B
weights	I
may	O
be	O
dominated	O
by	O
a	O
few	O
weights	O
having	O
large	O
values	O
with	O
the	O
remaining	O
weights	O
being	O
relatively	O
insignificant	O
thus	O
the	O
effective	O
sample	O
size	O
can	O
be	O
much	O
smaller	O
than	O
the	O
apparent	O
sample	O
size	O
l	O
the	O
problem	O
is	O
even	O
more	O
severe	O
if	O
none	O
of	O
the	O
samples	O
falls	O
in	O
the	O
regions	O
where	O
pzfz	O
is	O
large	O
in	O
that	O
case	O
the	O
apparent	O
variances	O
of	O
rl	O
and	O
rlfzl	O
may	O
be	O
small	O
even	O
though	O
the	O
estimate	O
of	O
the	O
expectation	B
may	O
be	O
severely	O
wrong	O
hence	O
a	O
major	O
drawback	O
of	O
the	O
importance	B
sampling	I
method	O
is	O
the	O
potential	O
to	O
produce	O
results	O
that	O
are	O
arbitrarily	O
in	O
error	B
and	O
with	O
no	O
diagnostic	O
indication	O
this	O
also	O
highlights	O
a	O
key	O
requirement	O
for	O
the	O
sampling	O
distribution	O
qz	O
namely	O
that	O
it	O
should	O
not	O
be	O
small	O
or	O
zero	O
in	O
regions	O
where	O
pz	O
may	O
be	O
significant	O
for	O
distributions	O
defined	O
in	O
terms	O
of	O
a	O
graphical	B
model	I
we	O
can	O
apply	O
the	O
importance	B
sampling	I
technique	O
in	O
various	O
ways	O
for	O
discrete	O
variables	O
a	O
simple	O
approach	O
is	O
called	O
uniform	B
sampling	I
the	O
joint	O
distribution	O
for	O
a	O
directed	B
graph	O
is	O
defined	O
by	O
each	O
sample	O
from	O
the	O
joint	O
distribution	O
is	O
obtained	O
by	O
first	O
setting	O
those	O
variables	O
zi	O
that	O
are	O
in	O
the	O
evidence	O
set	O
equal	O
to	O
their	O
observed	O
values	O
each	O
of	O
the	O
remaining	O
variables	O
is	O
then	O
sampled	O
independently	O
from	O
a	O
uniform	B
distribution	I
over	O
the	O
space	O
of	O
possible	O
instantiations	O
to	O
determine	O
the	O
corresponding	O
weight	O
associ	O
ated	O
with	O
a	O
sample	O
zl	O
we	O
note	O
that	O
the	O
sampling	O
is	O
uniform	O
over	O
the	O
possible	O
choices	O
for	O
z	O
and	O
where	O
x	O
denotes	O
the	O
subset	O
of	O
variables	O
that	O
are	O
observed	O
and	O
the	O
equality	O
follows	O
from	O
the	O
fact	O
that	O
every	O
sample	O
z	O
that	O
is	O
generated	O
is	O
necessarily	O
consistent	B
with	O
the	O
evidence	O
thus	O
the	O
weights	O
rl	O
are	O
simply	O
proportional	O
to	O
pz	O
note	O
that	O
the	O
variables	O
can	O
be	O
sampled	O
in	O
any	O
order	O
this	O
approach	O
can	O
yield	O
poor	O
results	O
if	O
the	O
posterior	O
distribution	O
is	O
far	O
from	O
uniform	O
as	O
is	O
often	O
the	O
case	O
in	O
practice	O
an	O
improvement	O
on	O
this	O
approach	O
is	O
called	O
likelihood	B
weighted	I
sampling	I
and	O
chang	O
shachter	O
and	O
peot	O
and	O
is	O
based	O
on	O
ancestral	B
sampling	I
of	O
the	O
variables	O
for	O
each	O
variable	O
in	O
turn	O
if	O
that	O
variable	O
is	O
in	O
the	O
evidence	O
set	O
then	O
it	O
is	O
just	O
set	O
to	O
its	O
instantiated	O
value	O
if	O
it	O
is	O
not	O
in	O
the	O
evidence	O
set	O
then	O
it	O
is	O
sampled	O
from	O
the	O
conditional	B
distribution	O
pzipai	O
in	O
which	O
the	O
conditioning	O
variables	O
are	O
set	O
to	O
their	O
currently	O
sampled	O
values	O
the	O
weighting	O
associated	O
with	O
the	O
resulting	O
sample	O
z	O
is	O
then	O
given	O
by	O
pzipai	O
pzipai	O
e	O
rz	O
pzipai	O
pzipai	O
zi	O
e	O
zi	O
e	O
this	O
method	O
can	O
be	O
further	O
extended	B
using	O
self-importance	O
sampling	O
and	O
peot	O
in	O
which	O
the	O
importance	B
sampling	I
distribution	O
is	O
continually	O
updated	O
to	O
reflect	O
the	O
current	O
estimated	O
posterior	O
distribution	O
sampling-importance-resampling	B
the	O
rejection	B
sampling	I
method	O
discussed	O
in	O
section	O
depends	O
in	O
part	O
for	O
its	O
success	O
on	O
the	O
determination	O
of	O
a	O
suitable	O
value	O
for	O
the	O
constant	O
k	O
for	O
many	O
pairs	O
of	O
distributions	O
pz	O
and	O
qz	O
it	O
will	O
be	O
impractical	O
to	O
determine	O
a	O
suitable	O
basic	O
sampling	O
algorithms	O
value	O
for	O
k	O
in	O
that	O
any	O
value	O
that	O
is	O
sufficiently	O
large	O
to	O
guarantee	O
a	O
bound	O
on	O
the	O
desired	O
distribution	O
will	O
lead	O
to	O
impractically	O
small	O
acceptance	O
rates	O
as	O
in	O
the	O
case	O
of	O
rejection	B
sampling	I
the	O
sampling-importance-resampling	B
approach	O
also	O
makes	O
use	O
of	O
a	O
sampling	O
distribution	O
qz	O
but	O
avoids	O
having	O
to	O
determine	O
the	O
constant	O
k	O
there	O
are	O
two	O
stages	O
to	O
the	O
scheme	O
in	O
the	O
first	O
stage	O
l	O
samples	O
zl	O
are	O
drawn	O
from	O
qz	O
then	O
in	O
the	O
second	O
stage	O
weights	O
wl	O
are	O
constructed	O
using	O
finally	O
a	O
second	O
set	O
of	O
l	O
samples	O
is	O
drawn	O
from	O
the	O
discrete	O
distribution	O
zl	O
with	O
probabilities	O
given	O
by	O
the	O
weights	O
wl	O
the	O
resulting	O
l	O
samples	O
are	O
only	O
approximately	O
distributed	O
according	O
to	O
pz	O
but	O
the	O
distribution	O
becomes	O
correct	O
in	O
the	O
limit	O
l	O
to	O
see	O
this	O
consider	O
the	O
univariate	O
case	O
and	O
note	O
that	O
the	O
cumulative	O
distribution	O
of	O
the	O
resampled	O
values	O
is	O
given	O
by	O
l	O
izl	O
wl	O
pz	O
a	O
where	O
i	O
is	O
the	O
indicator	O
function	O
equals	O
if	O
its	O
argument	O
is	O
true	O
and	O
otherwise	O
taking	O
the	O
limit	O
l	O
and	O
assuming	O
suitable	O
regularity	O
of	O
the	O
distributions	O
we	O
can	O
replace	O
the	O
sums	O
by	O
integrals	O
weighted	O
according	O
to	O
the	O
original	O
sampling	O
distribution	O
qz	O
iz	O
qz	O
dz	O
qz	O
dz	O
iz	O
dz	O
dz	O
pz	O
a	O
iz	O
apz	O
dz	O
which	O
is	O
the	O
cumulative	B
distribution	I
function	I
of	O
pz	O
again	O
we	O
see	O
that	O
the	O
normalization	O
of	O
pz	O
is	O
not	O
required	O
for	O
a	O
finite	O
value	O
of	O
l	O
and	O
a	O
given	O
initial	O
sample	O
set	O
the	O
resampled	O
values	O
will	O
only	O
approximately	O
be	O
drawn	O
from	O
the	O
desired	O
distribution	O
as	O
with	O
rejection	B
sampling	I
the	O
approximation	O
improves	O
as	O
the	O
sampling	O
distribution	O
qz	O
gets	O
closer	O
to	O
the	O
desired	O
distribution	O
pz	O
when	O
qz	O
pz	O
the	O
initial	O
samples	O
zl	O
have	O
the	O
desired	O
distribution	O
and	O
the	O
weights	O
wn	O
so	O
that	O
the	O
resampled	O
values	O
also	O
have	O
the	O
desired	O
distribution	O
if	O
moments	O
with	O
respect	O
to	O
the	O
distribution	O
pz	O
are	O
required	O
then	O
they	O
can	O
be	O
sampling	B
methods	I
evaluated	O
directly	O
using	O
the	O
original	O
samples	O
together	O
with	O
the	O
weights	O
because	O
efz	O
fzpz	O
dz	O
dz	O
dz	O
wlfzl	O
sampling	O
and	O
the	O
em	B
algorithm	I
in	O
addition	O
to	O
providing	O
a	O
mechanism	O
for	O
direct	O
implementation	O
of	O
the	O
bayesian	B
framework	O
monte	O
carlo	O
methods	O
can	O
also	O
play	O
a	O
role	O
in	O
the	O
frequentist	B
paradigm	O
for	O
example	O
to	O
find	O
maximum	B
likelihood	I
solutions	O
in	O
particular	O
sampling	B
methods	I
can	O
be	O
used	O
to	O
approximate	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
for	O
models	O
in	O
which	O
the	O
e	O
step	O
cannot	O
be	O
performed	O
analytically	O
consider	O
a	O
model	O
with	O
hidden	O
variables	O
z	O
visible	O
variables	O
x	O
and	O
parameters	O
the	O
function	O
that	O
is	O
optimized	O
with	O
respect	O
to	O
in	O
the	O
m	O
step	O
is	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
given	O
by	O
q	O
old	O
pzx	O
old	O
ln	O
pz	O
x	O
dz	O
we	O
can	O
use	O
sampling	B
methods	I
to	O
approximate	O
this	O
integral	O
by	O
a	O
finite	O
sum	O
over	O
samples	O
which	O
are	O
drawn	O
from	O
the	O
current	O
estimate	O
for	O
the	O
posterior	O
distribution	O
pzx	O
old	O
so	O
that	O
q	O
old	O
l	O
ln	O
pzl	O
x	O
the	O
q	O
function	O
is	O
then	O
optimized	O
in	O
the	O
usual	O
way	O
in	O
the	O
m	O
step	O
this	O
procedure	O
is	O
called	O
the	O
monte	B
carlo	I
em	B
algorithm	I
it	O
is	O
straightforward	O
to	O
extend	O
this	O
to	O
the	O
problem	O
of	O
finding	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
over	O
map	O
estimate	O
when	O
a	O
prior	B
distribution	O
p	O
has	O
been	O
defined	O
simply	O
by	O
adding	O
ln	O
p	O
to	O
the	O
function	O
q	O
old	O
before	O
performing	O
the	O
m	O
step	O
a	O
particular	O
instance	O
of	O
the	O
monte	B
carlo	I
em	B
algorithm	I
called	O
stochastic	B
em	B
arises	O
if	O
we	O
consider	O
a	O
finite	O
mixture	B
model	I
and	O
draw	O
just	O
one	O
sample	O
at	O
each	O
e	O
step	O
here	O
the	O
latent	B
variable	I
z	O
characterizes	O
which	O
of	O
the	O
k	O
components	O
of	O
the	O
mixture	B
is	O
responsible	O
for	O
generating	O
each	O
data	O
point	O
in	O
the	O
e	O
step	O
a	O
sample	O
of	O
z	O
is	O
taken	O
from	O
the	O
posterior	O
distribution	O
pzx	O
old	O
where	O
x	O
is	O
the	O
data	O
set	O
this	O
effectively	O
makes	O
a	O
hard	O
assignment	O
of	O
each	O
data	O
point	O
to	O
one	O
of	O
the	O
components	O
in	O
the	O
mixture	B
in	O
the	O
m	O
step	O
this	O
sampled	O
approximation	O
to	O
the	O
posterior	O
distribution	O
is	O
used	O
to	O
update	O
the	O
model	O
parameters	O
in	O
the	O
usual	O
way	O
markov	B
chain	I
monte	I
carlo	I
now	O
suppose	O
we	O
move	O
from	O
a	O
maximum	B
likelihood	I
approach	O
to	O
a	O
full	O
bayesian	B
treatment	O
in	O
which	O
we	O
wish	O
to	O
sample	O
from	O
the	O
posterior	O
distribution	O
over	O
the	O
parameter	O
vector	O
in	O
principle	O
we	O
would	O
like	O
to	O
draw	O
samples	O
from	O
the	O
joint	O
posterior	O
p	O
zx	O
but	O
we	O
shall	O
suppose	O
that	O
this	O
is	O
computationally	O
difficult	O
suppose	O
further	O
that	O
it	O
is	O
relatively	O
straightforward	O
to	O
sample	O
from	O
the	O
complete-data	O
parameter	O
posterior	O
p	O
x	O
this	O
inspires	O
the	O
data	B
augmentation	I
algorithm	O
which	O
alternates	O
between	O
two	O
steps	O
known	O
as	O
the	O
i-step	O
step	O
analogous	O
to	O
an	O
e	O
step	O
and	O
the	O
p-step	O
step	O
analogous	O
to	O
an	O
m	O
step	O
ip	O
algorithm	O
i-step	O
we	O
wish	O
to	O
sample	O
from	O
pzx	O
but	O
we	O
cannot	O
do	O
this	O
directly	O
we	O
therefore	O
note	O
the	O
relation	O
pzx	O
pz	O
xp	O
d	O
and	O
hence	O
for	O
l	O
l	O
we	O
first	O
draw	O
a	O
sample	O
from	O
the	O
current	O
estimate	O
for	O
p	O
and	O
then	O
use	O
this	O
to	O
draw	O
a	O
sample	O
zl	O
from	O
pz	O
x	O
p-step	O
given	O
the	O
relation	O
p	O
p	O
xpzx	O
dz	O
we	O
use	O
the	O
samples	O
obtained	O
from	O
the	O
i-step	O
to	O
compute	O
a	O
revised	O
estimate	O
of	O
the	O
posterior	O
distribution	O
over	O
given	O
by	O
p	O
l	O
p	O
x	O
by	O
assumption	O
it	O
will	O
be	O
feasible	O
to	O
sample	O
from	O
this	O
approximation	O
in	O
the	O
i-step	O
note	O
that	O
we	O
are	O
making	O
a	O
artificial	O
distinction	O
between	O
parameters	O
and	O
hidden	O
variables	O
z	O
from	O
now	O
on	O
we	O
blur	O
this	O
distinction	O
and	O
focus	O
simply	O
on	O
the	O
problem	O
of	O
drawing	O
samples	O
from	O
a	O
given	O
posterior	O
distribution	O
markov	B
chain	I
monte	I
carlo	I
in	O
the	O
previous	O
section	O
we	O
discussed	O
the	O
rejection	B
sampling	I
and	O
importance	B
sampling	I
strategies	O
for	O
evaluating	O
expectations	O
of	O
functions	O
and	O
we	O
saw	O
that	O
they	O
suffer	O
from	O
severe	O
limitations	O
particularly	O
in	O
spaces	O
of	O
high	O
dimensionality	O
we	O
therefore	O
turn	O
in	O
this	O
section	O
to	O
a	O
very	O
general	O
and	O
powerful	O
framework	O
called	O
markov	B
chain	I
monte	I
carlo	I
which	O
allows	O
sampling	O
from	O
a	O
large	O
class	O
of	O
distributions	O
sampling	B
methods	I
and	O
which	O
scales	O
well	O
with	O
the	O
dimensionality	O
of	O
the	O
sample	O
space	O
markov	B
chain	I
monte	I
carlo	I
methods	O
have	O
their	O
origins	O
in	O
physics	O
and	O
ulam	O
and	O
it	O
was	O
only	O
towards	O
the	O
end	O
of	O
the	O
that	O
they	O
started	O
to	O
have	O
a	O
significant	O
impact	O
in	O
the	O
field	O
of	O
statistics	O
as	O
with	O
rejection	O
and	O
importance	B
sampling	I
we	O
again	O
sample	O
from	O
a	O
proposal	B
distribution	I
this	O
time	O
however	O
we	O
maintain	O
a	O
record	O
of	O
the	O
current	O
state	O
z	O
and	O
the	O
proposal	B
distribution	I
qzz	O
depends	O
on	O
this	O
current	O
state	O
and	O
so	O
the	O
sequence	O
of	O
samples	O
forms	O
a	O
markov	B
chain	I
again	O
if	O
we	O
write	O
pz	O
we	O
will	O
assume	O
can	O
readily	O
be	O
evaluated	O
for	O
any	O
given	O
value	O
of	O
z	O
although	O
section	O
the	O
value	O
of	O
zp	O
may	O
be	O
unknown	O
the	O
proposal	B
distribution	I
itself	O
is	O
chosen	O
to	O
be	O
sufficiently	O
simple	O
that	O
it	O
is	O
straightforward	O
to	O
draw	O
samples	O
from	O
it	O
directly	O
at	O
each	O
cycle	O
of	O
the	O
algorithm	O
we	O
generate	O
a	O
candidate	O
sample	O
from	O
the	O
proposal	B
distribution	I
and	O
then	O
accept	O
the	O
sample	O
according	O
to	O
an	O
appropriate	O
criterion	O
in	O
the	O
basic	O
metropolis	B
algorithm	I
et	O
al	O
we	O
assume	O
that	O
the	O
proposal	B
distribution	I
is	O
symmetric	O
that	O
is	O
qzazb	O
qzbza	O
for	O
all	O
values	O
of	O
za	O
and	O
zb	O
the	O
candidate	O
sample	O
is	O
then	O
accepted	O
with	O
probability	B
z	O
min	O
this	O
can	O
be	O
achieved	O
by	O
choosing	O
a	O
random	O
number	O
u	O
with	O
uniform	B
distribution	I
over	O
the	O
unit	O
interval	O
and	O
then	O
accepting	O
the	O
sample	O
if	O
z	O
u	O
note	O
that	O
if	O
the	O
step	O
from	O
z	O
to	O
causes	O
an	O
increase	O
in	O
the	O
value	O
of	O
pz	O
then	O
the	O
candidate	O
point	O
is	O
certain	O
to	O
be	O
kept	O
if	O
the	O
candidate	O
sample	O
is	O
accepted	O
then	O
z	O
otherwise	O
the	O
candidate	O
point	O
is	O
discarded	O
z	O
is	O
set	O
to	O
z	O
and	O
another	O
candidate	O
sample	O
is	O
drawn	O
from	O
the	O
distribution	O
qzz	O
this	O
is	O
in	O
contrast	O
to	O
rejection	B
sampling	I
where	O
rejected	O
samples	O
are	O
simply	O
discarded	O
in	O
the	O
metropolis	B
algorithm	I
when	O
a	O
candidate	O
point	O
is	O
rejected	O
the	O
previous	O
sample	O
is	O
included	O
instead	O
in	O
the	O
final	O
list	O
of	O
samples	O
leading	O
to	O
multiple	O
copies	O
of	O
samples	O
of	O
course	O
in	O
a	O
practical	O
implementation	O
only	O
a	O
single	O
copy	O
of	O
each	O
retained	O
sample	O
would	O
be	O
kept	O
along	O
with	O
an	O
integer	O
weighting	O
factor	O
recording	O
how	O
many	O
times	O
that	O
state	O
appears	O
as	O
we	O
shall	O
see	O
as	O
long	O
as	O
qzazb	O
is	O
positive	O
for	O
any	O
values	O
of	O
za	O
and	O
zb	O
is	O
a	O
sufficient	O
but	O
not	O
necessary	O
condition	O
the	O
distribution	O
of	O
z	O
tends	O
to	O
pz	O
as	O
it	O
should	O
be	O
emphasized	O
however	O
that	O
the	O
sequence	O
is	O
not	O
a	O
set	O
of	O
independent	B
samples	O
from	O
pz	O
because	O
successive	O
samples	O
are	O
highly	O
correlated	O
if	O
we	O
wish	O
to	O
obtain	O
independent	B
samples	O
then	O
we	O
can	O
discard	O
most	O
of	O
the	O
sequence	O
and	O
just	O
retain	O
every	O
m	O
th	O
sample	O
for	O
m	O
sufficiently	O
large	O
the	O
retained	O
samples	O
will	O
for	O
all	O
practical	O
purposes	O
be	O
independent	B
figure	O
shows	O
a	O
simple	O
illustrative	O
example	O
of	O
sampling	O
from	O
a	O
two-dimensional	O
gaussian	B
distribution	O
using	O
the	O
metropolis	B
algorithm	I
in	O
which	O
the	O
proposal	B
distribution	I
is	O
an	O
isotropic	B
gaussian	B
further	O
insight	O
into	O
the	O
nature	O
of	O
markov	B
chain	I
monte	I
carlo	I
algorithms	O
can	O
be	O
gleaned	O
by	O
looking	O
at	O
the	O
properties	O
of	O
a	O
specific	O
example	O
namely	O
a	O
simple	O
random	O
markov	B
chain	I
monte	I
carlo	I
figure	O
a	O
simple	O
illustration	O
using	O
metropolis	B
algorithm	I
to	O
sample	O
from	O
a	O
gaussian	B
distribution	O
whose	O
one	O
standard-deviation	O
contour	O
is	O
shown	O
by	O
the	O
ellipse	O
the	O
proposal	B
distribution	I
is	O
an	O
isotropic	B
gaussian	B
distribution	O
whose	O
standard	B
deviation	I
is	O
steps	O
that	O
are	O
accepted	O
are	O
shown	O
as	O
green	O
lines	O
and	O
rejected	O
steps	O
are	O
shown	O
in	O
red	O
a	O
total	O
of	O
candidate	O
samples	O
are	O
generated	O
of	O
which	O
are	O
rejected	O
exercise	O
walk	O
consider	O
a	O
state	O
space	O
z	O
consisting	O
of	O
the	O
integers	O
with	O
probabilities	O
pz	O
z	O
pz	O
z	O
pz	O
z	O
where	O
z	O
denotes	O
the	O
state	O
at	O
step	O
if	O
the	O
initial	O
state	O
is	O
then	O
by	O
symmetry	O
the	O
expected	O
state	O
at	O
time	O
will	O
also	O
be	O
zero	O
ez	O
and	O
similarly	O
it	O
is	O
easily	O
seen	O
that	O
ez	O
thus	O
after	O
steps	O
the	O
random	O
walk	O
has	O
only	O
travelled	O
a	O
distance	O
that	O
on	O
average	O
is	O
proportional	O
to	O
the	O
square	O
root	O
of	O
this	O
square	O
root	O
dependence	O
is	O
typical	O
of	O
random	O
walk	O
behaviour	O
and	O
shows	O
that	O
random	O
walks	O
are	O
very	O
inefficient	O
in	O
exploring	O
the	O
state	O
space	O
as	O
we	O
shall	O
see	O
a	O
central	O
goal	O
in	O
designing	O
markov	B
chain	I
monte	I
carlo	I
methods	O
is	O
to	O
avoid	O
random	O
walk	O
behaviour	O
markov	O
chains	O
before	O
discussing	O
markov	B
chain	I
monte	I
carlo	I
methods	O
in	O
more	O
detail	O
it	O
is	O
useful	O
to	O
study	O
some	O
general	O
properties	O
of	O
markov	O
chains	O
in	O
more	O
detail	O
in	O
particular	O
we	O
ask	O
under	O
what	O
circumstances	O
will	O
a	O
markov	B
chain	I
converge	O
to	O
the	O
desired	O
distribution	O
a	O
first-order	O
markov	B
chain	I
is	O
defined	O
to	O
be	O
a	O
series	O
of	O
random	O
variables	O
zm	O
such	O
that	O
the	O
following	O
conditional	B
independence	I
property	O
holds	O
for	O
m	O
m	O
zm	O
this	O
of	O
course	O
can	O
be	O
represented	O
as	O
a	O
directed	B
graph	O
in	O
the	O
form	O
of	O
a	O
chain	O
an	O
example	O
of	O
which	O
is	O
shown	O
in	O
figure	O
we	O
can	O
then	O
specify	O
the	O
markov	B
chain	I
by	O
giving	O
the	O
probability	B
distribution	O
for	O
the	O
initial	O
variable	O
together	O
with	O
the	O
sampling	B
methods	I
conditional	B
probabilities	O
for	O
subsequent	O
variables	O
in	O
the	O
form	O
of	O
transition	O
probabilities	O
tmzm	O
a	O
markov	B
chain	I
is	O
called	O
homogeneous	B
if	O
the	O
transition	O
probabilities	O
are	O
the	O
same	O
for	O
all	O
m	O
the	O
marginal	B
probability	B
for	O
a	O
particular	O
variable	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
marginal	B
probability	B
for	O
the	O
previous	O
variable	O
in	O
the	O
chain	O
in	O
the	O
form	O
zm	O
a	O
distribution	O
is	O
said	O
to	O
be	O
invariant	O
or	O
stationary	B
with	O
respect	O
to	O
a	O
markov	B
chain	I
if	O
each	O
step	O
in	O
the	O
chain	O
leaves	O
that	O
distribution	O
invariant	O
thus	O
for	O
a	O
homogeneous	B
markov	B
chain	I
with	O
transition	O
probabilities	O
t	O
z	O
the	O
distribution	O
is	O
invariant	O
if	O
t	O
note	O
that	O
a	O
given	O
markov	B
chain	I
may	O
have	O
more	O
than	O
one	O
invariant	O
distribution	O
for	O
instance	O
if	O
the	O
transition	O
probabilities	O
are	O
given	O
by	O
the	O
identity	O
transformation	O
then	O
any	O
distribution	O
will	O
be	O
invariant	O
a	O
sufficient	O
not	O
necessary	O
condition	O
for	O
ensuring	O
that	O
the	O
required	O
distribution	O
pz	O
is	O
invariant	O
is	O
to	O
choose	O
the	O
transition	O
probabilities	O
to	O
satisfy	O
the	O
property	O
of	O
detailed	O
balance	O
defined	O
by	O
z	O
for	O
the	O
particular	O
distribution	O
it	O
is	O
easily	O
seen	O
that	O
a	O
transition	B
probability	B
that	O
satisfies	O
detailed	O
balance	O
with	O
respect	O
to	O
a	O
particular	O
distribution	O
will	O
leave	O
that	O
distribution	O
invariant	O
because	O
z	O
a	O
markov	B
chain	I
that	O
respects	O
detailed	O
balance	O
is	O
said	O
to	O
be	O
reversible	O
our	O
goal	O
is	O
to	O
use	O
markov	O
chains	O
to	O
sample	O
from	O
a	O
given	O
distribution	O
we	O
can	O
achieve	O
this	O
if	O
we	O
set	O
up	O
a	O
markov	B
chain	I
such	O
that	O
the	O
desired	O
distribution	O
is	O
invariant	O
however	O
we	O
must	O
also	O
require	O
that	O
for	O
m	O
the	O
distribution	O
pzm	O
converges	O
to	O
the	O
required	O
invariant	O
distribution	O
irrespective	O
of	O
the	O
choice	O
of	O
initial	O
distribution	O
this	O
property	O
is	O
called	O
ergodicity	O
and	O
the	O
invariant	O
distribution	O
is	O
then	O
called	O
the	O
equilibrium	O
distribution	O
clearly	O
an	O
ergodic	O
markov	B
chain	I
can	O
have	O
only	O
one	O
equilibrium	O
distribution	O
it	O
can	O
be	O
shown	O
that	O
a	O
homogeneous	B
markov	B
chain	I
will	O
be	O
ergodic	O
subject	O
only	O
to	O
weak	O
restrictions	O
on	O
the	O
invariant	O
distribution	O
and	O
the	O
transition	O
probabilities	O
in	O
practice	O
we	O
often	O
construct	O
the	O
transition	O
probabilities	O
from	O
a	O
set	O
of	O
base	O
transitions	O
bk	O
this	O
can	O
be	O
achieved	O
through	O
a	O
mixture	B
distribution	I
of	O
the	O
form	O
t	O
z	O
z	O
markov	B
chain	I
monte	I
carlo	I
for	O
some	O
set	O
of	O
mixing	O
coefficients	O
k	O
satisfying	O
k	O
and	O
k	O
k	O
alternatively	O
the	O
base	O
transitions	O
may	O
be	O
combined	O
through	O
successive	O
application	O
so	O
that	O
t	O
bk	O
zk	O
z	O
z	O
zn	O
if	O
a	O
distribution	O
is	O
invariant	O
with	O
respect	O
to	O
each	O
of	O
the	O
base	O
transitions	O
then	O
obviously	O
it	O
will	O
also	O
be	O
invariant	O
with	O
respect	O
to	O
either	O
of	O
the	O
t	O
z	O
given	O
by	O
or	O
for	O
the	O
case	O
of	O
the	O
mixture	B
if	O
each	O
of	O
the	O
base	O
transitions	O
satisfies	O
detailed	O
balance	O
then	O
the	O
mixture	B
transition	O
t	O
will	O
also	O
satisfy	O
detailed	O
balance	O
this	O
does	O
not	O
hold	O
for	O
the	O
transition	B
probability	B
constructed	O
using	O
although	O
by	O
symmetrizing	O
the	O
order	O
of	O
application	O
of	O
the	O
base	O
transitions	O
in	O
the	O
form	O
bk	O
bk	O
detailed	O
balance	O
can	O
be	O
restored	O
a	O
common	O
example	O
of	O
the	O
use	O
of	O
composite	O
transition	O
probabilities	O
is	O
where	O
each	O
base	O
transition	O
changes	O
only	O
a	O
subset	O
of	O
the	O
variables	O
the	O
metropolis-hastings	B
algorithm	I
earlier	O
we	O
introduced	O
the	O
basic	O
metropolis	B
algorithm	I
without	O
actually	O
demonstrating	O
that	O
it	O
samples	O
from	O
the	O
required	O
distribution	O
before	O
giving	O
a	O
proof	O
we	O
first	O
discuss	O
a	O
generalization	B
known	O
as	O
the	O
metropolis-hastings	B
algorithm	I
to	O
the	O
case	O
where	O
the	O
proposal	B
distribution	I
is	O
no	O
longer	O
a	O
symmetric	O
function	O
of	O
its	O
arguments	O
in	O
particular	O
at	O
step	O
of	O
the	O
algorithm	O
in	O
which	O
the	O
current	O
state	O
is	O
z	O
we	O
draw	O
a	O
sample	O
from	O
the	O
distribution	O
qkzz	O
and	O
then	O
accept	O
it	O
with	O
probability	B
z	O
where	O
z	O
min	O
here	O
k	O
labels	O
the	O
members	O
of	O
the	O
set	O
of	O
possible	O
transitions	O
being	O
considered	O
again	O
the	O
evaluation	O
of	O
the	O
acceptance	B
criterion	I
does	O
not	O
require	O
knowledge	O
of	O
the	O
normal	O
izing	O
constant	O
zp	O
in	O
the	O
probability	B
distribution	O
pz	O
for	O
a	O
symmetric	O
proposal	B
distribution	I
the	O
metropolis-hastings	O
criterion	O
reduces	O
to	O
the	O
standard	O
metropolis	O
criterion	O
given	O
by	O
we	O
can	O
show	O
that	O
pz	O
is	O
an	O
invariant	O
distribution	O
of	O
the	O
markov	B
chain	I
defined	O
by	O
the	O
metropolis-hastings	B
algorithm	I
by	O
showing	O
that	O
detailed	O
balance	O
defined	O
by	O
is	O
satisfied	O
using	O
we	O
have	O
z	O
min	O
min	O
as	O
required	O
the	O
specific	O
choice	O
of	O
proposal	B
distribution	I
can	O
have	O
a	O
marked	O
effect	O
on	O
the	O
performance	O
of	O
the	O
algorithm	O
for	O
continuous	O
state	O
spaces	O
a	O
common	O
choice	O
is	O
a	O
gaussian	B
centred	O
on	O
the	O
current	O
state	O
leading	O
to	O
an	O
important	O
trade-off	O
in	O
determinif	O
the	O
variance	B
is	O
small	O
then	O
the	O
ing	O
the	O
variance	B
parameter	O
of	O
this	O
distribution	O
sampling	B
methods	I
figure	O
schematic	O
illustration	O
of	O
the	O
use	O
of	O
an	O
isotropic	B
gaussian	B
proposal	B
distribution	I
circle	O
to	O
sample	O
from	O
a	O
correlated	O
multivariate	O
gaussian	B
distribution	O
ellipse	O
having	O
very	O
different	O
standard	O
deviations	O
in	O
different	O
directions	O
using	O
the	O
metropolis-hastings	B
algorithm	I
in	O
order	O
to	O
keep	O
the	O
rejection	O
rate	O
low	O
the	O
scale	O
of	O
the	O
proposal	B
distribution	I
should	O
be	O
on	O
the	O
order	O
of	O
the	O
smallest	O
standard	B
deviation	I
min	O
which	O
leads	O
to	O
random	O
walk	O
behaviour	O
in	O
which	O
the	O
number	O
of	O
steps	O
separating	O
states	O
that	O
are	O
approximately	O
independent	B
is	O
of	O
order	O
max	O
where	O
max	O
is	O
the	O
largest	O
standard	B
deviation	I
min	O
max	O
proportion	O
of	O
accepted	O
transitions	O
will	O
be	O
high	O
but	O
progress	O
through	O
the	O
state	O
space	O
takes	O
the	O
form	O
of	O
a	O
slow	O
random	O
walk	O
leading	O
to	O
long	O
correlation	O
times	O
however	O
if	O
the	O
variance	B
parameter	O
is	O
large	O
then	O
the	O
rejection	O
rate	O
will	O
be	O
high	O
because	O
in	O
the	O
kind	O
of	O
complex	O
problems	O
we	O
are	O
considering	O
many	O
of	O
the	O
proposed	O
steps	O
will	O
be	O
to	O
states	O
for	O
which	O
the	O
probability	B
pz	O
is	O
low	O
consider	O
a	O
multivariate	O
distribution	O
pz	O
having	O
strong	O
correlations	O
between	O
the	O
components	O
of	O
z	O
as	O
illustrated	O
in	O
figure	O
the	O
scale	O
of	O
the	O
proposal	B
distribution	I
should	O
be	O
as	O
large	O
as	O
possible	O
without	O
incurring	O
high	O
rejection	O
rates	O
this	O
suggests	O
that	O
should	O
be	O
of	O
the	O
same	O
order	O
as	O
the	O
smallest	O
length	O
scale	O
min	O
the	O
system	O
then	O
explores	O
the	O
distribution	O
along	O
the	O
more	O
extended	B
direction	O
by	O
means	O
of	O
a	O
random	O
walk	O
and	O
so	O
the	O
number	O
of	O
steps	O
to	O
arrive	O
at	O
a	O
state	O
that	O
is	O
more	O
or	O
less	O
independent	B
of	O
the	O
original	O
state	O
is	O
of	O
order	O
max	O
in	O
fact	O
in	O
two	O
dimensions	O
the	O
increase	O
in	O
rejection	O
rate	O
as	O
increases	O
is	O
offset	O
by	O
the	O
larger	O
steps	O
sizes	O
of	O
those	O
transitions	O
that	O
are	O
accepted	O
and	O
more	O
generally	O
for	O
a	O
multivariate	O
gaussian	B
the	O
number	O
of	O
steps	O
required	O
to	O
obtain	O
independent	B
samples	O
scales	O
like	O
max	O
where	O
is	O
the	O
second-smallest	O
standard	B
deviation	I
these	O
details	O
aside	O
it	O
remains	O
the	O
case	O
that	O
if	O
the	O
length	O
scales	O
over	O
which	O
the	O
distributions	O
vary	O
are	O
very	O
different	O
in	O
different	O
directions	O
then	O
the	O
metropolis	O
hastings	O
algorithm	O
can	O
have	O
very	O
slow	O
convergence	O
gibbs	B
sampling	I
gibbs	B
sampling	I
and	O
geman	O
is	O
a	O
simple	O
and	O
widely	O
applicable	O
markov	B
chain	I
monte	I
carlo	I
algorithm	O
and	O
can	O
be	O
seen	O
as	O
a	O
special	O
case	O
of	O
the	O
metropolishastings	O
algorithm	O
consider	O
the	O
distribution	O
pz	O
zm	O
from	O
which	O
we	O
wish	O
to	O
sample	O
and	O
suppose	O
that	O
we	O
have	O
chosen	O
some	O
initial	O
state	O
for	O
the	O
markov	B
chain	I
each	O
step	O
of	O
the	O
gibbs	B
sampling	I
procedure	O
involves	O
replacing	O
the	O
value	O
of	O
one	O
of	O
the	O
variables	O
by	O
a	O
value	O
drawn	O
from	O
the	O
distribution	O
of	O
that	O
variable	O
conditioned	O
on	O
the	O
values	O
of	O
the	O
remaining	O
variables	O
thus	O
we	O
replace	O
zi	O
by	O
a	O
value	O
drawn	O
from	O
the	O
distribution	O
pzizi	O
where	O
zi	O
denotes	O
the	O
ith	O
component	O
of	O
z	O
and	O
zi	O
denotes	O
zm	O
but	O
with	O
zi	O
omitted	O
this	O
procedure	O
is	O
repeated	O
either	O
by	O
cycling	O
through	O
the	O
variables	O
gibbs	B
sampling	I
in	O
some	O
particular	O
order	O
or	O
by	O
choosing	O
the	O
variable	O
to	O
be	O
updated	O
at	O
each	O
step	O
at	O
random	O
from	O
some	O
distribution	O
and	O
at	O
step	O
of	O
the	O
algorithm	O
we	O
have	O
selected	O
values	O
z	O
replace	O
z	O
bution	O
for	O
example	O
suppose	O
we	O
have	O
a	O
distribution	O
over	O
three	O
variables	O
we	O
first	O
obtained	O
by	O
sampling	O
from	O
the	O
conditional	B
by	O
a	O
new	O
value	O
z	O
z	O
and	O
z	O
z	O
next	O
we	O
replace	O
z	O
distribution	O
by	O
a	O
value	O
z	O
obtained	O
by	O
sampling	O
from	O
the	O
conditional	B
z	O
so	O
that	O
the	O
new	O
value	O
for	O
is	O
used	O
straight	O
away	O
in	O
subsequent	O
sampling	O
steps	O
then	O
we	O
update	O
with	O
a	O
sample	O
z	O
drawn	O
from	O
z	O
and	O
so	O
on	O
cycling	O
through	O
the	O
three	O
variables	O
in	O
turn	O
gibbs	B
sampling	I
initialize	O
i	O
m	O
for	O
t	O
sample	O
z	O
sample	O
z	O
z	O
z	O
z	O
z	O
m	O
m	O
sample	O
z	O
sample	O
z	O
j	O
pzjz	O
z	O
j	O
z	O
z	O
m	O
m	O
pzmz	O
z	O
z	O
m	O
josiah	O
willard	O
gibbs	B
gibbs	B
spent	O
almost	O
his	O
entire	O
life	O
living	O
in	O
a	O
house	O
built	O
by	O
his	O
father	O
in	O
new	O
haven	O
connecticut	O
in	O
gibbs	B
was	O
granted	O
the	O
first	O
phd	O
in	O
engineering	O
in	O
the	O
united	O
states	O
and	O
in	O
he	O
was	O
appointed	O
to	O
the	O
first	O
chair	O
of	O
mathematical	O
physics	O
in	O
the	O
united	O
states	O
at	O
yale	O
a	O
post	O
for	O
which	O
he	O
received	O
no	O
salary	O
because	O
at	O
the	O
time	O
he	O
had	O
no	O
publications	O
he	O
developed	O
the	O
field	O
of	O
vector	O
analysis	O
and	O
made	O
contributions	O
to	O
crystallography	O
and	O
planetary	O
orbits	O
his	O
most	O
famous	O
work	O
entitled	O
ontheequilibriumofheterogeneous	O
substances	O
laid	O
the	O
foundations	O
for	O
the	O
science	O
of	O
physical	O
chemistry	O
sampling	B
methods	I
to	O
show	O
that	O
this	O
procedure	O
samples	O
from	O
the	O
required	O
distribution	O
we	O
first	O
of	O
all	O
note	O
that	O
the	O
distribution	O
pz	O
is	O
an	O
invariant	O
of	O
each	O
of	O
the	O
gibbs	B
sampling	I
steps	O
individually	O
and	O
hence	O
of	O
the	O
whole	O
markov	B
chain	I
this	O
follows	O
from	O
the	O
fact	O
that	O
when	O
we	O
sample	O
from	O
pzizi	O
the	O
marginal	B
distribution	O
pzi	O
is	O
clearly	O
invariant	O
because	O
the	O
value	O
of	O
zi	O
is	O
unchanged	O
also	O
each	O
step	O
by	O
definition	O
samples	O
from	O
the	O
correct	O
conditional	B
distribution	O
pzizi	O
because	O
these	O
conditional	B
and	O
marginal	B
distributions	O
together	O
specify	O
the	O
joint	O
distribution	O
we	O
see	O
that	O
the	O
joint	O
distribution	O
is	O
itself	O
invariant	O
the	O
second	O
requirement	O
to	O
be	O
satisfied	O
in	O
order	O
that	O
the	O
gibbs	B
sampling	I
procedure	O
samples	O
from	O
the	O
correct	O
distribution	O
is	O
that	O
it	O
be	O
ergodic	O
a	O
sufficient	O
condition	O
for	O
ergodicity	O
is	O
that	O
none	O
of	O
the	O
conditional	B
distributions	O
be	O
anywhere	O
zero	O
if	O
this	O
is	O
the	O
case	O
then	O
any	O
point	O
in	O
z	O
space	O
can	O
be	O
reached	O
from	O
any	O
other	O
point	O
in	O
a	O
finite	O
number	O
of	O
steps	O
involving	O
one	O
update	O
of	O
each	O
of	O
the	O
component	O
variables	O
if	O
this	O
requirement	O
is	O
not	O
satisfied	O
so	O
that	O
some	O
of	O
the	O
conditional	B
distributions	O
have	O
zeros	O
then	O
ergodicity	O
if	O
it	O
applies	O
must	O
be	O
proven	O
explicitly	O
the	O
distribution	O
of	O
initial	O
states	O
must	O
also	O
be	O
specified	O
in	O
order	O
to	O
complete	O
the	O
algorithm	O
although	O
samples	O
drawn	O
after	O
many	O
iterations	O
will	O
effectively	O
become	O
independent	B
of	O
this	O
distribution	O
of	O
course	O
successive	O
samples	O
from	O
the	O
markov	B
chain	I
will	O
be	O
highly	O
correlated	O
and	O
so	O
to	O
obtain	O
samples	O
that	O
are	O
nearly	O
independent	B
it	O
will	O
be	O
necessary	O
to	O
subsample	O
the	O
sequence	O
we	O
can	O
obtain	O
the	O
gibbs	B
sampling	I
procedure	O
as	O
a	O
particular	O
instance	O
of	O
the	O
metropolis-hastings	B
algorithm	I
as	O
follows	O
consider	O
a	O
metropolis-hastings	O
sampling	O
step	O
involving	O
the	O
variable	O
zk	O
in	O
which	O
the	O
remaining	O
variables	O
zk	O
remain	O
fixed	O
and	O
for	O
which	O
the	O
transition	B
probability	B
from	O
z	O
to	O
is	O
given	O
by	O
kzk	O
we	O
note	O
that	O
zk	O
because	O
these	O
components	O
are	O
unchanged	O
by	O
the	O
sampling	O
step	O
also	O
pz	O
pzkzkpzk	O
thus	O
the	O
factor	O
that	O
determines	O
the	O
acceptance	O
probability	B
in	O
the	O
metropolis-hastings	O
is	O
given	O
by	O
z	O
kzk	O
where	O
we	O
have	O
used	O
zk	O
thus	O
the	O
metropolis-hastings	O
steps	O
are	O
always	O
accepted	O
as	O
with	O
the	O
metropolis	B
algorithm	I
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
behaviour	O
of	O
gibbs	B
sampling	I
by	O
investigating	O
its	O
application	O
to	O
a	O
gaussian	B
distribution	O
consider	O
a	O
correlated	O
gaussian	B
in	O
two	O
variables	O
as	O
illustrated	O
in	O
figure	O
having	O
conditional	B
distributions	O
of	O
width	O
l	O
and	O
marginal	B
distributions	O
of	O
width	O
l	O
the	O
typical	O
step	O
size	O
is	O
governed	O
by	O
the	O
conditional	B
distributions	O
and	O
will	O
be	O
of	O
order	O
l	O
because	O
the	O
state	O
evolves	O
according	O
to	O
a	O
random	O
walk	O
the	O
number	O
of	O
steps	O
needed	O
to	O
obtain	O
independent	B
samples	O
from	O
the	O
distribution	O
will	O
be	O
of	O
order	O
of	O
course	O
if	O
the	O
gaussian	B
distribution	O
were	O
uncorrelated	O
then	O
the	O
gibbs	B
sampling	I
procedure	O
would	O
be	O
optimally	O
efficient	O
for	O
this	O
simple	O
problem	O
we	O
could	O
rotate	O
the	O
coordinate	O
system	O
in	O
order	O
to	O
decorrelate	O
the	O
variables	O
however	O
in	O
practical	O
applications	O
it	O
will	O
generally	O
be	O
infeasible	O
to	O
find	O
such	O
transformations	O
one	O
approach	O
to	O
reducing	O
random	O
walk	O
behaviour	O
in	O
gibbs	B
sampling	I
is	O
called	O
over-relaxation	B
in	O
its	O
original	O
form	O
this	O
applies	O
to	O
problems	O
for	O
which	O
gibbs	B
sampling	I
l	O
figure	O
illustration	O
of	O
gibbs	B
sampling	I
by	O
alternate	O
updates	O
of	O
two	O
variables	O
whose	O
distribution	O
is	O
a	O
correlated	O
gaussian	B
the	O
step	O
size	O
is	O
governed	O
by	O
the	O
standard	B
deviation	I
of	O
the	O
conditional	B
distribution	O
curve	O
and	O
is	O
ol	O
leading	O
to	O
slow	O
progress	O
in	O
the	O
direction	O
of	O
elongation	O
of	O
the	O
joint	O
distribution	O
ellipse	O
the	O
number	O
of	O
steps	O
needed	O
to	O
obtain	O
an	O
independent	B
sample	O
from	O
the	O
distribution	O
is	O
l	O
the	O
conditional	B
distributions	O
are	O
gaussian	B
which	O
represents	O
a	O
more	O
general	O
class	O
of	O
distributions	O
than	O
the	O
multivariate	O
gaussian	B
because	O
for	O
example	O
the	O
non-gaussian	O
distribution	O
pz	O
y	O
exp	O
has	O
gaussian	B
conditional	B
distributions	O
at	O
each	O
step	O
of	O
the	O
gibbs	B
sampling	I
algorithm	O
the	O
conditional	B
distribution	O
for	O
a	O
particular	O
component	O
zi	O
has	O
some	O
mean	B
i	O
and	O
some	O
variance	B
i	O
in	O
the	O
over-relaxation	B
framework	O
the	O
value	O
of	O
zi	O
is	O
replaced	O
with	O
i	O
i	O
i	O
i	O
z	O
i	O
then	O
so	O
too	O
does	O
z	O
where	O
is	O
a	O
gaussian	B
random	O
variable	O
with	O
zero	O
mean	B
and	O
unit	O
variance	B
and	O
is	O
a	O
parameter	O
such	O
that	O
for	O
the	O
method	O
is	O
equivalent	O
to	O
standard	O
gibbs	B
sampling	I
and	O
for	O
the	O
step	O
is	O
biased	O
to	O
the	O
opposite	O
side	O
of	O
the	O
mean	B
this	O
step	O
leaves	O
the	O
desired	O
distribution	O
invariant	O
because	O
if	O
zi	O
has	O
mean	B
i	O
and	O
variance	B
i	O
the	O
effect	O
of	O
over-relaxation	B
is	O
to	O
encourage	O
directed	B
motion	O
through	O
state	O
space	O
when	O
the	O
variables	O
are	O
highly	O
correlated	O
the	O
framework	O
of	O
ordered	B
over-relaxation	B
generalizes	O
this	O
approach	O
to	O
nongaussian	O
distributions	O
the	O
practical	O
applicability	O
of	O
gibbs	B
sampling	I
depends	O
on	O
the	O
ease	O
with	O
which	O
samples	O
can	O
be	O
drawn	O
from	O
the	O
conditional	B
distributions	O
pzkzk	O
in	O
the	O
case	O
of	O
probability	B
distributions	O
specified	O
using	O
graphical	O
models	O
the	O
conditional	B
distributions	O
for	O
individual	O
nodes	O
depend	O
only	O
on	O
the	O
variables	O
in	O
the	O
corresponding	O
markov	O
blankets	O
as	O
illustrated	O
in	O
figure	O
for	O
directed	B
graphs	O
a	O
wide	O
choice	O
of	O
conditional	B
distributions	O
for	O
the	O
individual	O
nodes	O
conditioned	O
on	O
their	O
parents	O
will	O
lead	O
to	O
conditional	B
distributions	O
for	O
gibbs	B
sampling	I
that	O
are	O
log	O
concave	O
the	O
adaptive	B
rejection	B
sampling	B
methods	I
discussed	O
in	O
section	O
therefore	O
provide	O
a	O
framework	O
for	O
monte	B
carlo	I
sampling	I
from	O
directed	B
graphs	O
with	O
broad	O
applicability	O
if	O
the	O
graph	O
is	O
constructed	O
using	O
distributions	O
from	O
the	O
exponential	B
family	I
and	O
if	O
the	O
parent-child	O
relationships	O
preserve	O
conjugacy	O
then	O
the	O
full	O
conditional	B
distributions	O
arising	O
in	O
gibbs	B
sampling	I
will	O
have	O
the	O
same	O
functional	B
form	O
as	O
the	O
orig	O
sampling	B
methods	I
figure	O
the	O
gibbs	B
sampling	I
method	O
requires	O
samples	O
to	O
be	O
drawn	O
from	O
the	O
conditional	B
distribution	O
of	O
a	O
variable	O
conditioned	O
on	O
the	O
remaining	O
variables	O
for	O
graphical	O
models	O
this	O
conditional	B
distribution	O
is	O
a	O
function	O
only	O
of	O
the	O
states	O
of	O
the	O
nodes	O
in	O
the	O
markov	B
blanket	I
for	O
an	O
undirected	B
graph	O
this	O
comprises	O
the	O
set	O
of	O
neighbours	O
as	O
shown	O
on	O
the	O
left	O
while	O
for	O
a	O
directed	B
graph	O
the	O
markov	B
blanket	I
comprises	O
the	O
parents	O
the	O
children	O
and	O
the	O
co-parents	B
as	O
shown	O
on	O
the	O
right	O
inal	O
conditional	B
distributions	O
on	O
the	O
parents	O
defining	O
each	O
node	B
and	O
so	O
standard	O
sampling	O
techniques	O
can	O
be	O
employed	O
in	O
general	O
the	O
full	O
conditional	B
distributions	O
will	O
be	O
of	O
a	O
complex	O
form	O
that	O
does	O
not	O
permit	O
the	O
use	O
of	O
standard	O
sampling	O
algorithms	O
however	O
if	O
these	O
conditionals	O
are	O
log	O
concave	O
then	O
sampling	O
can	O
be	O
done	O
efficiently	O
using	O
adaptive	B
rejection	B
sampling	I
the	O
corresponding	O
variable	O
is	O
a	O
scalar	O
if	O
at	O
each	O
stage	O
of	O
the	O
gibbs	B
sampling	I
algorithm	O
instead	O
of	O
drawing	O
a	O
sample	O
from	O
the	O
corresponding	O
conditional	B
distribution	O
we	O
make	O
a	O
point	O
estimate	O
of	O
the	O
variable	O
given	O
by	O
the	O
maximum	O
of	O
the	O
conditional	B
distribution	O
then	O
we	O
obtain	O
the	O
iterated	B
conditional	B
modes	I
algorithm	O
discussed	O
in	O
section	O
thus	O
icm	O
can	O
be	O
seen	O
as	O
a	O
greedy	O
approximation	O
to	O
gibbs	B
sampling	I
because	O
the	O
basic	O
gibbs	B
sampling	I
technique	O
considers	O
one	O
variable	O
at	O
a	O
time	O
there	O
are	O
strong	O
dependencies	O
between	O
successive	O
samples	O
at	O
the	O
opposite	O
extreme	O
if	O
we	O
could	O
draw	O
samples	O
directly	O
from	O
the	O
joint	O
distribution	O
operation	O
that	O
we	O
are	O
supposing	O
is	O
intractable	O
then	O
successive	O
samples	O
would	O
be	O
independent	B
we	O
can	O
hope	O
to	O
improve	O
on	O
the	O
simple	O
gibbs	B
sampler	O
by	O
adopting	O
an	O
intermediate	O
strategy	O
in	O
which	O
we	O
sample	O
successively	O
from	O
groups	O
of	O
variables	O
rather	O
than	O
individual	O
variables	O
this	O
is	O
achieved	O
in	O
the	O
blocking	B
gibbs	B
sampling	I
algorithm	O
by	O
choosing	O
blocks	O
of	O
variables	O
not	O
necessarily	O
disjoint	O
and	O
then	O
sampling	O
jointly	O
from	O
the	O
variables	O
in	O
each	O
block	O
in	O
turn	O
conditioned	O
on	O
the	O
remaining	O
variables	O
et	O
al	O
slice	B
sampling	I
we	O
have	O
seen	O
that	O
one	O
of	O
the	O
difficulties	O
with	O
the	O
metropolis	B
algorithm	I
is	O
the	O
sensitivity	O
to	O
step	O
size	O
if	O
this	O
is	O
too	O
small	O
the	O
result	O
is	O
slow	O
decorrelation	O
due	O
to	O
random	O
walk	O
behaviour	O
whereas	O
if	O
it	O
is	O
too	O
large	O
the	O
result	O
is	O
inefficiency	O
due	O
to	O
a	O
high	O
rejection	O
rate	O
the	O
technique	O
of	O
slice	B
sampling	I
provides	O
an	O
adaptive	O
step	O
size	O
that	O
is	O
automatically	O
adjusted	O
to	O
match	O
the	O
characteristics	O
of	O
the	O
distribution	O
again	O
it	O
requires	O
that	O
we	O
are	O
able	O
to	O
evaluate	O
the	O
unnormalized	O
consider	O
first	O
the	O
univariate	O
case	O
slice	B
sampling	I
involves	O
augmenting	O
z	O
with	O
an	O
additional	O
variable	O
u	O
and	O
then	O
drawing	O
samples	O
from	O
the	O
joint	O
u	O
space	O
we	O
shall	O
see	O
another	O
example	O
of	O
this	O
approach	O
when	O
we	O
discuss	O
hybrid	B
monte	I
carlo	I
in	O
section	O
the	O
goal	O
is	O
to	O
sample	O
uniformly	O
from	O
the	O
area	O
under	O
the	O
distribution	O
pz	O
u	O
z	O
slice	B
sampling	I
pz	O
zmin	O
u	O
zmax	O
z	O
z	O
z	O
for	O
a	O
given	O
value	O
z	O
a	O
value	O
of	O
u	O
is	O
chosen	O
uniformly	O
in	O
figure	O
illustration	O
of	O
slice	B
sampling	I
the	O
region	O
u	O
epz	O
which	O
then	O
defines	O
a	O
slice	O
through	O
the	O
distribution	O
shown	O
by	O
the	O
solid	O
horizontal	O
because	O
it	O
is	O
infeasible	O
to	O
sample	O
directly	O
from	O
a	O
slice	O
a	O
new	O
sample	O
of	O
z	O
is	O
drawn	O
from	O
a	O
region	O
lines	O
zmin	O
z	O
zmax	O
which	O
contains	O
the	O
previous	O
value	O
z	O
given	O
by	O
where	O
zp	O
u	O
dz	O
the	O
marginal	B
distribution	O
over	O
z	O
is	O
given	O
by	O
epz	O
u	O
du	O
if	O
u	O
otherwise	O
pz	O
du	O
zp	O
zp	O
values	O
this	O
can	O
be	O
achieved	O
by	O
alternately	O
sampling	O
z	O
and	O
u	O
given	O
the	O
value	O
of	O
z	O
and	O
so	O
we	O
can	O
sample	O
from	O
pz	O
by	O
sampling	O
u	O
and	O
then	O
ignoring	O
the	O
u	O
we	O
and	O
then	O
sample	O
u	O
uniformly	O
in	O
the	O
range	O
u	O
which	O
is	O
distribution	O
defined	O
by	O
u	O
this	O
is	O
illustrated	O
in	O
figure	O
u	O
invariant	O
which	O
can	O
be	O
achieved	O
by	O
ensuring	O
that	O
detailed	O
balance	O
is	O
in	O
practice	O
it	O
can	O
be	O
difficult	O
to	O
sample	O
directly	O
from	O
a	O
slice	O
through	O
the	O
distribution	O
and	O
so	O
instead	O
we	O
define	O
a	O
sampling	O
scheme	O
that	O
leaves	O
the	O
uniform	B
distribution	I
straightforward	O
then	O
we	O
fix	O
u	O
and	O
sample	O
z	O
uniformly	O
from	O
the	O
slice	O
through	O
the	O
satisfied	O
suppose	O
the	O
current	O
value	O
of	O
z	O
is	O
denoted	O
z	O
and	O
that	O
we	O
have	O
obtained	O
a	O
corresponding	O
sample	O
u	O
the	O
next	O
value	O
of	O
z	O
is	O
obtained	O
by	O
considering	O
a	O
region	O
zmin	O
z	O
zmax	O
that	O
contains	O
z	O
it	O
is	O
in	O
the	O
choice	O
of	O
this	O
region	O
that	O
the	O
adaptation	O
to	O
the	O
characteristic	O
length	O
scales	O
of	O
the	O
distribution	O
takes	O
place	O
we	O
want	O
the	O
region	O
to	O
encompass	O
as	O
much	O
of	O
the	O
slice	O
as	O
possible	O
so	O
as	O
to	O
allow	O
large	O
moves	O
in	O
z	O
space	O
while	O
having	O
as	O
little	O
as	O
possible	O
of	O
this	O
region	O
lying	O
outside	O
the	O
slice	O
because	O
this	O
makes	O
the	O
sampling	O
less	O
efficient	O
one	O
approach	O
to	O
the	O
choice	O
of	O
region	O
involves	O
starting	O
with	O
a	O
region	O
containing	O
z	O
having	O
some	O
width	O
w	O
and	O
then	O
testing	O
each	O
of	O
the	O
end	O
points	O
to	O
see	O
if	O
they	O
lie	O
within	O
the	O
slice	O
if	O
either	O
end	O
point	O
does	O
not	O
then	O
the	O
region	O
is	O
extended	B
in	O
that	O
direction	O
by	O
increments	O
of	O
value	O
w	O
until	O
the	O
end	O
point	O
lies	O
outside	O
the	O
region	O
a	O
is	O
then	O
chosen	O
uniformly	O
from	O
this	O
region	O
and	O
if	O
it	O
lies	O
within	O
the	O
candidate	O
value	O
z	O
slice	O
then	O
it	O
forms	O
z	O
if	O
it	O
lies	O
outside	O
the	O
slice	O
then	O
the	O
region	O
is	O
shrunk	O
such	O
forms	O
an	O
end	O
point	O
and	O
such	O
that	O
the	O
region	O
still	O
contains	O
z	O
then	O
another	O
that	O
z	O
sampling	B
methods	I
candidate	O
point	O
is	O
drawn	O
uniformly	O
from	O
this	O
reduced	O
region	O
and	O
so	O
on	O
until	O
a	O
value	O
of	O
z	O
is	O
found	O
that	O
lies	O
within	O
the	O
slice	O
slice	B
sampling	I
can	O
be	O
applied	O
to	O
multivariate	O
distributions	O
by	O
repeatedly	O
sampling	O
each	O
variable	O
in	O
turn	O
in	O
the	O
manner	O
of	O
gibbs	B
sampling	I
this	O
requires	O
that	O
we	O
are	O
able	O
to	O
compute	O
for	O
each	O
component	O
zi	O
a	O
function	O
that	O
is	O
proportional	O
to	O
pzizi	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
as	O
we	O
have	O
already	O
noted	O
one	O
of	O
the	O
major	O
limitations	O
of	O
the	O
metropolis	B
algorithm	I
is	O
that	O
it	O
can	O
exhibit	O
random	O
walk	O
behaviour	O
whereby	O
the	O
distance	O
traversed	O
through	O
the	O
state	O
space	O
grows	O
only	O
as	O
the	O
square	O
root	O
of	O
the	O
number	O
of	O
steps	O
the	O
problem	O
cannot	O
be	O
resolved	O
simply	O
by	O
taking	O
bigger	O
steps	O
as	O
this	O
leads	O
to	O
a	O
high	O
rejection	O
rate	O
in	O
this	O
section	O
we	O
introduce	O
a	O
more	O
sophisticated	O
class	O
of	O
transitions	O
based	O
on	O
an	O
analogy	O
with	O
physical	O
systems	O
and	O
that	O
has	O
the	O
property	O
of	O
being	O
able	O
to	O
make	O
large	O
changes	O
to	O
the	O
system	O
state	O
while	O
keeping	O
the	O
rejection	O
probability	B
small	O
it	O
is	O
applicable	O
to	O
distributions	O
over	O
continuous	O
variables	O
for	O
which	O
we	O
can	O
readily	O
evaluate	O
the	O
gradient	O
of	O
the	O
log	O
probability	B
with	O
respect	O
to	O
the	O
state	O
variables	O
we	O
will	O
discuss	O
the	O
dynamical	O
systems	O
framework	O
in	O
section	O
and	O
then	O
in	O
section	O
we	O
explain	O
how	O
this	O
may	O
be	O
combined	O
with	O
the	O
metropolis	B
algorithm	I
to	O
yield	O
the	O
powerful	O
hybrid	B
monte	I
carlo	I
algorithm	O
a	O
background	O
in	O
physics	O
is	O
not	O
required	O
as	O
this	O
section	O
is	O
self-contained	O
and	O
the	O
key	O
results	O
are	O
all	O
derived	O
from	O
first	O
principles	O
dynamical	O
systems	O
the	O
dynamical	O
approach	O
to	O
stochastic	B
sampling	O
has	O
its	O
origins	O
in	O
algorithms	O
for	O
simulating	O
the	O
behaviour	O
of	O
physical	O
systems	O
evolving	O
under	O
hamiltonian	B
dynamics	I
in	O
a	O
markov	B
chain	I
monte	I
carlo	I
simulation	O
the	O
goal	O
is	O
to	O
sample	O
from	O
a	O
given	O
probability	B
distribution	O
pz	O
the	O
framework	O
of	O
hamiltonian	B
dynamics	I
is	O
exploited	O
by	O
casting	O
the	O
probabilistic	O
simulation	O
in	O
the	O
form	O
of	O
a	O
hamiltonian	O
system	O
in	O
order	O
to	O
remain	O
in	O
keeping	O
with	O
the	O
literature	O
in	O
this	O
area	O
we	O
make	O
use	O
of	O
the	O
relevant	O
dynamical	O
systems	O
terminology	O
where	O
appropriate	O
which	O
will	O
be	O
defined	O
as	O
we	O
go	O
along	O
the	O
dynamics	O
that	O
we	O
consider	O
corresponds	O
to	O
the	O
evolution	O
of	O
the	O
state	O
variable	O
z	O
under	O
continuous	O
time	O
which	O
we	O
denote	O
by	O
classical	B
dynamics	O
is	O
described	O
by	O
newton	O
s	O
second	O
law	O
of	O
motion	O
in	O
which	O
the	O
acceleration	O
of	O
an	O
object	O
is	O
proportional	O
to	O
the	O
applied	O
force	O
corresponding	O
to	O
a	O
second-order	O
differential	B
equation	O
over	O
time	O
we	O
can	O
decompose	O
a	O
second-order	O
equation	O
into	O
two	O
coupled	O
firstorder	O
equations	O
by	O
introducing	O
intermediate	O
momentum	O
variables	O
r	O
corresponding	O
to	O
the	O
rate	O
of	O
change	O
of	O
the	O
state	O
variables	O
z	O
having	O
components	O
ri	O
dzi	O
d	O
where	O
the	O
zi	O
can	O
be	O
regarded	O
as	O
position	O
variables	O
in	O
this	O
dynamics	O
perspective	O
thus	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
for	O
each	O
position	B
variable	I
there	O
is	O
a	O
corresponding	O
momentum	B
variable	I
and	O
the	O
joint	O
space	O
of	O
position	O
and	O
momentum	O
variables	O
is	O
called	O
phase	B
space	I
without	O
loss	O
of	O
generality	O
we	O
can	O
write	O
the	O
probability	B
distribution	O
pz	O
in	O
the	O
form	O
pz	O
exp	O
ez	O
zp	O
where	O
ez	O
is	O
interpreted	O
as	O
the	O
potential	B
energy	I
of	O
the	O
system	O
when	O
in	O
state	O
z	O
the	O
system	O
acceleration	O
is	O
the	O
rate	O
of	O
change	O
of	O
momentum	O
and	O
is	O
given	O
by	O
the	O
applied	O
force	O
which	O
itself	O
is	O
the	O
negative	O
gradient	O
of	O
the	O
potential	B
energy	I
dri	O
d	O
ez	O
zi	O
it	O
is	O
convenient	O
to	O
reformulate	O
this	O
dynamical	B
system	I
using	O
the	O
hamiltonian	O
framework	O
to	O
do	O
this	O
we	O
first	O
define	O
the	O
kinetic	B
energy	I
by	O
kr	O
i	O
i	O
the	O
total	O
energy	O
of	O
the	O
system	O
is	O
then	O
the	O
sum	O
of	O
its	O
potential	O
and	O
kinetic	O
energies	O
hz	O
r	O
ez	O
kr	O
exercise	O
where	O
h	O
is	O
the	O
hamiltonian	B
function	I
using	O
and	O
we	O
can	O
now	O
express	O
the	O
dynamics	O
of	O
the	O
system	O
in	O
terms	O
of	O
the	O
hamiltonian	O
equations	O
given	O
by	O
dzi	O
d	O
dri	O
d	O
h	O
ri	O
h	O
zi	O
william	O
hamilton	B
william	O
rowan	O
hamilton	B
was	O
an	O
irish	O
mathematician	O
and	O
physicist	O
and	O
child	O
prodigy	O
who	O
was	O
appointed	O
professor	O
of	O
astronomy	O
at	O
trinity	O
college	O
dublin	O
in	O
before	O
he	O
had	O
even	O
graduated	O
one	O
of	O
hamilton	B
s	O
most	O
important	O
contributions	O
was	O
a	O
new	O
formulation	O
of	O
dynamics	O
which	O
played	O
a	O
significant	O
role	O
in	O
the	O
later	O
development	O
of	O
quantum	O
mechanics	O
his	O
other	O
great	O
achievement	O
was	O
the	O
development	O
of	O
quaternions	O
which	O
generalize	O
the	O
concept	O
of	O
complex	O
numbers	O
by	O
introducing	O
three	O
distinct	O
square	O
roots	O
of	O
minus	O
one	O
which	O
satisfy	O
ijk	O
it	O
is	O
said	O
that	O
these	O
equations	O
occurred	O
to	O
him	O
while	O
walking	O
along	O
the	O
royal	O
canal	O
in	O
dublin	O
with	O
his	O
wife	O
on	O
october	O
and	O
he	O
promptly	O
carved	O
the	O
equations	O
into	O
the	O
side	O
of	O
broome	O
bridge	O
although	O
there	O
is	O
no	O
longer	O
any	O
evidence	O
of	O
the	O
carving	O
there	O
is	O
now	O
a	O
stone	O
plaque	O
on	O
the	O
bridge	O
commemorating	O
the	O
discovery	O
and	O
displaying	O
the	O
quaternion	O
equations	O
sampling	B
methods	I
during	O
the	O
evolution	O
of	O
this	O
dynamical	B
system	I
the	O
value	O
of	O
the	O
hamiltonian	O
h	O
is	O
constant	O
as	O
is	O
easily	O
seen	O
by	O
differentiation	O
i	O
i	O
dh	O
d	O
h	O
zi	O
dzi	O
d	O
h	O
zi	O
h	O
ri	O
h	O
ri	O
h	O
ri	O
dri	O
d	O
h	O
zi	O
a	O
second	O
important	O
property	O
of	O
hamiltonian	O
dynamical	O
systems	O
known	O
as	O
liouville	O
s	O
theorem	O
is	O
that	O
they	O
preserve	O
volume	O
in	O
phase	B
space	I
in	O
other	O
words	O
if	O
we	O
consider	O
a	O
region	O
within	O
the	O
space	O
of	O
variables	O
r	O
then	O
as	O
this	O
region	O
evolves	O
under	O
the	O
equations	O
of	O
hamiltonian	B
dynamics	I
its	O
shape	O
may	O
change	O
but	O
its	O
volume	O
will	O
not	O
this	O
can	O
be	O
seen	O
by	O
noting	O
that	O
the	O
flow	O
field	O
of	O
change	O
of	O
location	O
in	O
phase	B
space	I
is	O
given	O
by	O
dz	O
d	O
and	O
that	O
the	O
divergence	O
of	O
this	O
field	O
vanishes	O
v	O
dr	O
d	O
i	O
i	O
zi	O
zi	O
dzi	O
d	O
ri	O
dri	O
d	O
h	O
ri	O
ri	O
h	O
zi	O
div	O
v	O
now	O
consider	O
the	O
joint	O
distribution	O
over	O
phase	B
space	I
whose	O
total	O
energy	O
is	O
the	O
hamiltonian	O
i	O
e	O
the	O
distribution	O
given	O
by	O
pz	O
r	O
exp	O
hz	O
r	O
zh	O
using	O
the	O
two	O
results	O
of	O
conservation	O
of	O
volume	O
and	O
conservation	O
of	O
h	O
it	O
follows	O
that	O
the	O
hamiltonian	B
dynamics	I
will	O
leave	O
pz	O
r	O
invariant	O
this	O
can	O
be	O
seen	O
by	O
considering	O
a	O
small	O
region	O
of	O
phase	B
space	I
over	O
which	O
h	O
is	O
approximately	O
constant	O
if	O
we	O
follow	O
the	O
evolution	O
of	O
the	O
hamiltonian	O
equations	O
for	O
a	O
finite	O
time	O
then	O
the	O
volume	O
of	O
this	O
region	O
will	O
remain	O
unchanged	O
as	O
will	O
the	O
value	O
of	O
h	O
in	O
this	O
region	O
and	O
hence	O
the	O
probability	B
density	B
which	O
is	O
a	O
function	O
only	O
of	O
h	O
will	O
also	O
be	O
unchanged	O
although	O
h	O
is	O
invariant	O
the	O
values	O
of	O
z	O
and	O
r	O
will	O
vary	O
and	O
so	O
by	O
integrating	O
the	O
hamiltonian	B
dynamics	I
over	O
a	O
finite	O
time	O
duration	O
it	O
becomes	O
possible	O
to	O
make	O
large	O
changes	O
to	O
z	O
in	O
a	O
systematic	O
way	O
that	O
avoids	O
random	O
walk	O
behaviour	O
evolution	O
under	O
the	O
hamiltonian	B
dynamics	I
will	O
not	O
however	O
sample	O
ergodically	O
from	O
pz	O
r	O
because	O
the	O
value	O
of	O
h	O
is	O
constant	O
in	O
order	O
to	O
arrive	O
at	O
an	O
ergodic	O
sampling	O
scheme	O
we	O
can	O
introduce	O
additional	O
moves	O
in	O
phase	B
space	I
that	O
change	O
the	O
value	O
of	O
h	O
while	O
also	O
leaving	O
the	O
distribution	O
pz	O
r	O
invariant	O
the	O
simplest	O
way	O
to	O
achieve	O
this	O
is	O
to	O
replace	O
the	O
value	O
of	O
r	O
with	O
one	O
drawn	O
from	O
its	O
distribution	O
conditioned	O
on	O
z	O
this	O
can	O
be	O
regarded	O
as	O
a	O
gibbs	B
sampling	I
step	O
and	O
hence	O
from	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
exercise	O
section	O
we	O
see	O
that	O
this	O
also	O
leaves	O
the	O
desired	O
distribution	O
invariant	O
noting	O
that	O
z	O
and	O
r	O
are	O
independent	B
in	O
the	O
distribution	O
pz	O
r	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
prz	O
is	O
a	O
gaussian	B
from	O
which	O
it	O
is	O
straightforward	O
to	O
sample	O
in	O
a	O
practical	O
application	O
of	O
this	O
approach	O
we	O
have	O
to	O
address	O
the	O
problem	O
of	O
performing	O
a	O
numerical	O
integration	O
of	O
the	O
hamiltonian	O
equations	O
this	O
will	O
necessarily	O
introduce	O
numerical	O
errors	O
and	O
so	O
we	O
should	O
devise	O
a	O
scheme	O
that	O
minimizes	O
the	O
impact	O
of	O
such	O
errors	O
in	O
fact	O
it	O
turns	O
out	O
that	O
integration	O
schemes	O
can	O
be	O
devised	O
for	O
which	O
liouville	O
s	O
theorem	O
still	O
holds	O
exactly	O
this	O
property	O
will	O
be	O
important	O
in	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
which	O
is	O
discussed	O
in	O
section	O
one	O
scheme	O
for	O
achieving	O
this	O
is	O
called	O
the	O
leapfrog	B
discretization	I
and	O
involves	O
alternately	O
updat	O
ing	O
discrete-time	O
to	O
the	O
position	O
and	O
momentum	O
variables	O
using	O
e	O
zi	O
e	O
zi	O
we	O
see	O
that	O
this	O
takes	O
the	O
form	O
of	O
a	O
half-step	O
update	O
of	O
the	O
momentum	O
variables	O
with	O
step	O
size	O
followed	O
by	O
a	O
full-step	O
update	O
of	O
the	O
position	O
variables	O
with	O
step	O
size	O
followed	O
by	O
a	O
second	O
half-step	O
update	O
of	O
the	O
momentum	O
variables	O
if	O
several	O
leapfrog	O
steps	O
are	O
applied	O
in	O
succession	O
it	O
can	O
be	O
seen	O
that	O
half-step	O
updates	O
to	O
the	O
momentum	O
variables	O
can	O
be	O
combined	O
into	O
full-step	O
updates	O
with	O
step	O
size	O
the	O
successive	O
updates	O
to	O
position	O
and	O
momentum	O
variables	O
then	O
leapfrog	O
over	O
each	O
other	O
in	O
order	O
to	O
advance	O
the	O
dynamics	O
by	O
a	O
time	O
interval	O
we	O
need	O
to	O
take	O
steps	O
the	O
error	B
involved	O
in	O
the	O
discretized	O
approximation	O
to	O
the	O
continuous	O
time	O
dynamics	O
will	O
go	O
to	O
zero	O
assuming	O
a	O
smooth	O
function	O
ez	O
in	O
the	O
limit	O
however	O
for	O
a	O
nonzero	O
as	O
used	O
in	O
practice	O
some	O
residual	O
error	B
will	O
remain	O
we	O
shall	O
see	O
in	O
section	O
how	O
the	O
effects	O
of	O
such	O
errors	O
can	O
be	O
eliminated	O
in	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
in	O
summary	O
then	O
the	O
hamiltonian	O
dynamical	O
approach	O
involves	O
alternating	O
between	O
a	O
series	O
of	O
leapfrog	O
updates	O
and	O
a	O
resampling	O
of	O
the	O
momentum	O
variables	O
from	O
their	O
marginal	B
distribution	O
note	O
that	O
the	O
hamiltonian	B
dynamics	I
method	O
unlike	O
the	O
basic	O
metropolis	B
algorithm	I
is	O
able	O
to	O
make	O
use	O
of	O
information	O
about	O
the	O
gradient	O
of	O
the	O
log	O
probability	B
distribution	O
as	O
well	O
as	O
about	O
the	O
distribution	O
itself	O
an	O
analogous	O
situation	O
is	O
familiar	O
from	O
the	O
domain	O
of	O
function	O
optimization	O
in	O
most	O
cases	O
where	O
gradient	O
information	O
is	O
available	O
it	O
is	O
highly	O
advantageous	O
to	O
make	O
use	O
of	O
it	O
informally	O
this	O
follows	O
from	O
the	O
fact	O
that	O
in	O
a	O
space	O
of	O
dimension	O
d	O
the	O
additional	O
computational	O
cost	O
of	O
evaluating	O
a	O
gradient	O
compared	O
with	O
evaluating	O
the	O
function	O
itself	O
will	O
typically	O
be	O
a	O
fixed	O
factor	O
independent	B
of	O
d	O
whereas	O
the	O
d-dimensional	O
gradient	O
vector	O
conveys	O
d	O
pieces	O
of	O
information	O
compared	O
with	O
the	O
one	O
piece	O
of	O
information	O
given	O
by	O
the	O
function	O
itself	O
sampling	B
methods	I
hybrid	B
monte	I
carlo	I
as	O
we	O
discussed	O
in	O
the	O
previous	O
section	O
for	O
a	O
nonzero	O
step	O
size	O
the	O
discretization	O
of	O
the	O
leapfrog	O
algorithm	O
will	O
introduce	O
errors	O
into	O
the	O
integration	O
of	O
the	O
hamiltonian	O
dynamical	O
equations	O
hybrid	B
monte	I
carlo	I
et	O
al	O
neal	O
combines	O
hamiltonian	B
dynamics	I
with	O
the	O
metropolis	B
algorithm	I
and	O
thereby	O
removes	O
any	O
bias	B
associated	O
with	O
the	O
discretization	O
specifically	O
the	O
algorithm	O
uses	O
a	O
markov	B
chain	I
consisting	O
of	O
alternate	O
stochastic	B
updates	O
of	O
the	O
momentum	B
variable	I
r	O
and	O
hamiltonian	O
dynamical	O
updates	O
using	O
the	O
leapfrog	O
algorithm	O
after	O
each	O
application	O
of	O
the	O
leapfrog	O
algorithm	O
the	O
resulting	O
candidate	O
state	O
is	O
accepted	O
or	O
rejected	O
according	O
to	O
the	O
metropolis	O
criterion	O
based	O
on	O
the	O
value	O
of	O
the	O
hamiltonian	O
h	O
thus	O
if	O
r	O
is	O
the	O
initial	O
state	O
and	O
is	O
the	O
state	O
after	O
the	O
leapfrog	O
integration	O
then	O
this	O
candidate	O
state	O
is	O
accepted	O
with	O
probability	B
min	O
exphz	O
r	O
if	O
the	O
leapfrog	O
integration	O
were	O
to	O
simulate	O
the	O
hamiltonian	B
dynamics	I
perfectly	O
then	O
every	O
such	O
candidate	O
step	O
would	O
automatically	O
be	O
accepted	O
because	O
the	O
value	O
of	O
h	O
would	O
be	O
unchanged	O
due	O
to	O
numerical	O
errors	O
the	O
value	O
of	O
h	O
may	O
sometimes	O
decrease	O
and	O
we	O
would	O
like	O
the	O
metropolis	O
criterion	O
to	O
remove	O
any	O
bias	B
due	O
to	O
this	O
effect	O
and	O
ensure	O
that	O
the	O
resulting	O
samples	O
are	O
indeed	O
drawn	O
from	O
the	O
required	O
distribution	O
in	O
order	O
for	O
this	O
to	O
be	O
the	O
case	O
we	O
need	O
to	O
ensure	O
that	O
the	O
update	O
equations	O
corresponding	O
to	O
the	O
leapfrog	O
integration	O
satisfy	O
detailed	O
balance	O
this	O
is	O
easily	O
achieved	O
by	O
modifying	O
the	O
leapfrog	O
scheme	O
as	O
follows	O
before	O
the	O
start	O
of	O
each	O
leapfrog	O
integration	O
sequence	O
we	O
choose	O
at	O
random	O
with	O
equal	O
probability	B
whether	O
to	O
integrate	O
forwards	O
in	O
time	O
step	O
size	O
or	O
backwards	O
in	O
time	O
step	O
size	O
we	O
first	O
note	O
that	O
the	O
leapfrog	O
integration	O
scheme	O
and	O
is	O
time-reversible	O
so	O
that	O
integration	O
for	O
l	O
steps	O
using	O
step	O
size	O
will	O
exactly	O
undo	O
the	O
effect	O
of	O
integration	O
for	O
l	O
steps	O
using	O
step	O
size	O
next	O
we	O
show	O
that	O
the	O
leapfrog	O
integration	O
preserves	O
phase-space	O
volume	O
exactly	O
this	O
follows	O
from	O
the	O
fact	O
that	O
each	O
step	O
in	O
the	O
leapfrog	O
scheme	O
updates	O
either	O
a	O
zi	O
variable	O
or	O
an	O
ri	O
variable	O
by	O
an	O
amount	O
that	O
is	O
a	O
function	O
only	O
of	O
the	O
other	O
variable	O
as	O
shown	O
in	O
figure	O
this	O
has	O
the	O
effect	O
of	O
shearing	O
a	O
region	O
of	O
phase	B
space	I
while	O
not	O
altering	O
its	O
volume	O
finally	O
we	O
use	O
these	O
results	O
to	O
show	O
that	O
detailed	O
balance	O
holds	O
consider	O
a	O
small	O
region	O
r	O
of	O
phase	B
space	I
that	O
under	O
a	O
sequence	O
of	O
l	O
leapfrog	O
iterations	O
of	O
step	O
size	O
maps	O
to	O
a	O
region	O
using	O
conservation	O
of	O
volume	O
under	O
the	O
leapfrog	O
iteration	O
we	O
see	O
that	O
if	O
r	O
has	O
volume	O
v	O
then	O
so	O
too	O
will	O
if	O
we	O
choose	O
an	O
initial	O
point	O
from	O
the	O
distribution	O
and	O
then	O
update	O
it	O
using	O
l	O
leapfrog	O
interactions	O
the	O
probability	B
of	O
the	O
transition	O
going	O
from	O
r	O
to	O
is	O
given	O
by	O
zh	O
exp	O
hr	O
v	O
exp	O
hr	O
where	O
the	O
factor	O
of	O
arises	O
from	O
the	O
probability	B
of	O
choosing	O
to	O
integrate	O
with	O
a	O
positive	O
step	O
size	O
rather	O
than	O
a	O
negative	O
one	O
similarly	O
the	O
probability	B
of	O
starting	O
in	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
ri	O
i	O
r	O
zi	O
i	O
z	O
figure	O
each	O
step	O
of	O
the	O
leapfrog	O
algorithm	O
modifies	O
either	O
a	O
position	B
variable	I
zi	O
or	O
a	O
momentum	B
variable	I
ri	O
because	O
the	O
change	O
to	O
one	O
variable	O
is	O
a	O
function	O
only	O
of	O
the	O
other	O
any	O
region	O
in	O
phase	B
space	I
will	O
be	O
sheared	O
without	O
change	O
of	O
volume	O
region	O
and	O
integrating	O
backwards	O
in	O
time	O
to	O
end	O
up	O
in	O
region	O
r	O
is	O
given	O
by	O
zh	O
exp	O
hr	O
exp	O
v	O
exercise	O
it	O
is	O
easily	O
seen	O
that	O
the	O
two	O
probabilities	O
and	O
are	O
equal	O
and	O
hence	O
detailed	O
balance	O
holds	O
note	O
that	O
this	O
proof	O
ignores	O
any	O
overlap	O
between	O
the	O
regions	O
r	O
and	O
but	O
is	O
easily	O
generalized	B
to	O
allow	O
for	O
such	O
overlap	O
it	O
is	O
not	O
difficult	O
to	O
construct	O
examples	O
for	O
which	O
the	O
leapfrog	O
algorithm	O
returns	O
to	O
its	O
starting	O
position	O
after	O
a	O
finite	O
number	O
of	O
iterations	O
in	O
such	O
cases	O
the	O
random	O
replacement	O
of	O
the	O
momentum	O
values	O
before	O
each	O
leapfrog	O
integration	O
will	O
not	O
be	O
sufficient	O
to	O
ensure	O
ergodicity	O
because	O
the	O
position	O
variables	O
will	O
never	O
be	O
updated	O
such	O
phenomena	O
are	O
easily	O
avoided	O
by	O
choosing	O
the	O
magnitude	O
of	O
the	O
step	O
size	O
at	O
random	O
from	O
some	O
small	O
interval	O
before	O
each	O
leapfrog	O
integration	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
behaviour	O
of	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
by	O
considering	O
its	O
application	O
to	O
a	O
multivariate	O
gaussian	B
for	O
convenience	O
consider	O
a	O
gaussian	B
distribution	O
pz	O
with	O
independent	B
components	O
for	O
which	O
the	O
hamiltonian	O
is	O
given	O
by	O
i	O
i	O
hz	O
r	O
i	O
i	O
i	O
our	O
conclusions	O
will	O
be	O
equally	O
valid	O
for	O
a	O
gaussian	B
distribution	O
having	O
correlated	O
components	O
because	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
exhibits	O
rotational	O
isotropy	O
during	O
the	O
leapfrog	O
integration	O
each	O
pair	O
of	O
phase-space	O
variables	O
zi	O
ri	O
evolves	O
independently	O
however	O
the	O
acceptance	O
or	O
rejection	O
of	O
the	O
candidate	O
point	O
is	O
based	O
on	O
the	O
value	O
of	O
h	O
which	O
depends	O
on	O
the	O
values	O
of	O
all	O
of	O
the	O
variables	O
thus	O
a	O
significant	O
integration	O
error	B
in	O
any	O
one	O
of	O
the	O
variables	O
could	O
lead	O
to	O
a	O
high	O
probability	B
of	O
rejection	O
in	O
order	O
that	O
the	O
discrete	O
leapfrog	O
integration	O
be	O
a	O
reasonably	O
sampling	B
methods	I
good	O
approximation	O
to	O
the	O
true	O
continuous-time	O
dynamics	O
it	O
is	O
necessary	O
for	O
the	O
leapfrog	O
integration	O
scale	O
to	O
be	O
smaller	O
than	O
the	O
shortest	O
length-scale	O
over	O
which	O
the	O
potential	O
is	O
varying	O
significantly	O
this	O
is	O
governed	O
by	O
the	O
smallest	O
value	O
of	O
i	O
which	O
we	O
denote	O
by	O
min	O
recall	O
that	O
the	O
goal	O
of	O
the	O
leapfrog	O
integration	O
in	O
hybrid	B
monte	I
carlo	I
is	O
to	O
move	O
a	O
substantial	O
distance	O
through	O
phase	B
space	I
to	O
a	O
new	O
state	O
that	O
is	O
relatively	O
independent	B
of	O
the	O
initial	O
state	O
and	O
still	O
achieve	O
a	O
high	O
probability	B
of	O
acceptance	O
in	O
order	O
to	O
achieve	O
this	O
the	O
leapfrog	O
integration	O
must	O
be	O
continued	O
for	O
a	O
number	O
of	O
iterations	O
of	O
order	O
max	O
min	O
by	O
contrast	O
consider	O
the	O
behaviour	O
of	O
a	O
simple	O
metropolis	B
algorithm	I
with	O
an	O
isotropic	B
gaussian	B
proposal	B
distribution	I
of	O
variance	B
considered	O
earlier	O
in	O
order	O
to	O
avoid	O
high	O
rejection	O
rates	O
the	O
value	O
of	O
s	O
must	O
be	O
of	O
order	O
min	O
the	O
exploration	B
of	O
state	O
space	O
then	O
proceeds	O
by	O
a	O
random	O
walk	O
and	O
takes	O
of	O
order	O
max	O
steps	O
to	O
arrive	O
at	O
a	O
roughly	O
independent	B
state	O
estimating	O
the	O
partition	B
function	I
as	O
we	O
have	O
seen	O
most	O
of	O
the	O
sampling	O
algorithms	O
considered	O
in	O
this	O
chapter	O
require	O
only	O
the	O
functional	B
form	O
of	O
the	O
probability	B
distribution	O
up	O
to	O
a	O
multiplicative	O
constant	O
thus	O
if	O
we	O
write	O
pez	O
exp	O
ez	O
ze	O
then	O
the	O
value	O
of	O
the	O
normalization	O
constant	O
ze	O
also	O
known	O
as	O
the	O
partition	B
function	I
is	O
not	O
needed	O
in	O
order	O
to	O
draw	O
samples	O
from	O
pz	O
however	O
knowledge	O
of	O
the	O
value	O
of	O
ze	O
can	O
be	O
useful	O
for	O
bayesian	B
model	B
comparison	I
since	O
it	O
represents	O
the	O
model	B
evidence	I
the	O
probability	B
of	O
the	O
observed	O
data	O
given	O
the	O
model	O
and	O
so	O
it	O
is	O
of	O
interest	O
to	O
consider	O
how	O
its	O
value	O
might	O
be	O
obtained	O
we	O
assume	O
that	O
direct	O
evaluation	O
by	O
summing	O
or	O
integrating	O
the	O
function	O
exp	O
ez	O
over	O
the	O
state	O
space	O
of	O
z	O
is	O
intractable	O
for	O
model	B
comparison	I
it	O
is	O
actually	O
the	O
ratio	O
of	O
the	O
partition	O
functions	O
for	O
two	O
models	O
that	O
is	O
required	O
multiplication	O
of	O
this	O
ratio	O
by	O
the	O
ratio	O
of	O
prior	B
probabilities	O
gives	O
the	O
ratio	O
of	O
posterior	O
probabilities	O
which	O
can	O
then	O
be	O
used	O
for	O
model	B
selection	I
or	O
model	B
averaging	I
one	O
way	O
to	O
estimate	O
a	O
ratio	O
of	O
partition	O
functions	O
is	O
to	O
use	O
importance	B
sampling	I
ze	O
zg	O
from	O
a	O
distribution	O
with	O
energy	B
function	I
gz	O
z	O
exp	O
ez	O
z	O
exp	O
gz	O
z	O
exp	O
ez	O
gz	O
exp	O
gz	O
z	O
exp	O
gz	O
egzexp	O
e	O
g	O
exp	O
ezl	O
gzl	O
l	O
estimating	O
the	O
partition	B
function	I
where	O
are	O
samples	O
drawn	O
from	O
the	O
distribution	O
defined	O
by	O
pgz	O
if	O
the	O
distribution	O
pg	O
is	O
one	O
for	O
which	O
the	O
partition	B
function	I
can	O
be	O
evaluated	O
analytically	O
for	O
example	O
a	O
gaussian	B
then	O
the	O
absolute	O
value	O
of	O
ze	O
can	O
be	O
obtained	O
this	O
approach	O
will	O
only	O
yield	O
accurate	O
results	O
if	O
the	O
importance	B
sampling	I
distribution	O
pg	O
is	O
closely	O
matched	O
to	O
the	O
distribution	O
pe	O
so	O
that	O
the	O
ratio	O
pepg	O
does	O
not	O
have	O
wide	O
variations	O
in	O
practice	O
suitable	O
analytically	O
specified	O
importance	B
sampling	I
distributions	O
cannot	O
readily	O
be	O
found	O
for	O
the	O
kinds	O
of	O
complex	O
models	O
considered	O
in	O
this	O
book	O
an	O
alternative	O
approach	O
is	O
therefore	O
to	O
use	O
the	O
samples	O
obtained	O
from	O
a	O
markov	B
chain	I
to	O
define	O
the	O
importance-sampling	O
distribution	O
if	O
the	O
transition	B
probability	B
for	O
the	O
markov	B
chain	I
is	O
given	O
by	O
t	O
and	O
the	O
sample	O
set	O
is	O
given	O
by	O
zl	O
then	O
the	O
sampling	O
distribution	O
can	O
be	O
written	O
as	O
exp	O
gz	O
zg	O
which	O
can	O
be	O
used	O
directly	O
in	O
t	O
z	O
methods	O
for	O
estimating	O
the	O
ratio	O
of	O
two	O
partition	O
functions	O
require	O
for	O
their	O
success	O
that	O
the	O
two	O
corresponding	O
distributions	O
be	O
reasonably	O
closely	O
matched	O
this	O
is	O
especially	O
problematic	O
if	O
we	O
wish	O
to	O
find	O
the	O
absolute	O
value	O
of	O
the	O
partition	B
function	I
for	O
a	O
complex	O
distribution	O
because	O
it	O
is	O
only	O
for	O
relatively	O
simple	O
distributions	O
that	O
the	O
partition	B
function	I
can	O
be	O
evaluated	O
directly	O
and	O
so	O
attempting	O
to	O
estimate	O
the	O
ratio	O
of	O
partition	O
functions	O
directly	O
is	O
unlikely	O
to	O
be	O
successful	O
this	O
problem	O
can	O
be	O
tackled	O
using	O
a	O
technique	O
known	O
as	O
chaining	B
barber	O
and	O
bishop	O
which	O
involves	O
introducing	O
a	O
succession	O
of	O
intermediate	O
distributions	O
pm	O
that	O
interpolate	O
between	O
a	O
simple	O
distribution	O
for	O
which	O
we	O
can	O
evaluate	O
the	O
normalization	O
coefficient	O
and	O
the	O
desired	O
complex	O
distribution	O
pm	O
we	O
then	O
have	O
zm	O
zm	O
zm	O
in	O
which	O
the	O
intermediate	O
ratios	O
can	O
be	O
determined	O
using	O
monte	O
carlo	O
methods	O
as	O
discussed	O
above	O
one	O
way	O
to	O
construct	O
such	O
a	O
sequence	O
of	O
intermediate	O
systems	O
is	O
to	O
use	O
an	O
energy	B
function	I
containing	O
a	O
continuous	O
parameter	O
that	O
interpolates	O
between	O
the	O
two	O
distributions	O
e	O
em	B
if	O
the	O
intermediate	O
ratios	O
in	O
are	O
to	O
be	O
found	O
using	O
monte	O
carlo	O
it	O
may	O
be	O
more	O
efficient	O
to	O
use	O
a	O
single	O
markov	B
chain	I
run	O
than	O
to	O
restart	O
the	O
markov	B
chain	I
for	O
each	O
ratio	O
in	O
this	O
case	O
the	O
markov	B
chain	I
is	O
run	O
initially	O
for	O
the	O
system	O
and	O
then	O
after	O
some	O
suitable	O
number	O
of	O
steps	O
moves	O
on	O
to	O
the	O
next	O
distribution	O
in	O
the	O
sequence	O
note	O
however	O
that	O
the	O
system	O
must	O
remain	O
close	O
to	O
the	O
equilibrium	O
distribution	O
at	O
each	O
stage	O
sampling	B
methods	I
exercises	O
www	O
show	O
that	O
the	O
finite	O
sample	O
defined	O
by	O
has	O
mean	B
equal	O
to	O
ef	O
and	O
variance	B
given	O
by	O
suppose	O
that	O
z	O
is	O
a	O
random	O
variable	O
with	O
uniform	B
distribution	I
over	O
and	O
where	O
hy	O
is	O
given	O
by	O
show	O
that	O
y	O
that	O
we	O
transform	O
z	O
using	O
y	O
h	O
has	O
the	O
distribution	O
py	O
given	O
a	O
random	O
variable	O
z	O
that	O
is	O
uniformly	O
distributed	O
over	O
find	O
a	O
trans	O
formation	O
y	O
fz	O
such	O
that	O
y	O
has	O
a	O
cauchy	B
distribution	I
given	O
by	O
suppose	O
that	O
and	O
are	O
uniformly	O
distributed	O
over	O
the	O
unit	O
circle	O
as	O
shown	O
in	O
figure	O
and	O
that	O
we	O
make	O
the	O
change	O
of	O
variables	O
given	O
by	O
and	O
show	O
that	O
will	O
be	O
distributed	O
according	O
to	O
www	O
let	O
z	O
be	O
a	O
d-dimensional	O
random	O
variable	O
having	O
a	O
gaussian	B
distribution	O
with	O
zero	O
mean	B
and	O
unit	O
covariance	B
matrix	O
and	O
suppose	O
that	O
the	O
positive	B
definite	I
symmetric	O
matrix	O
has	O
the	O
cholesky	B
decomposition	I
llt	O
where	O
l	O
is	O
a	O
lowertriangular	O
matrix	O
one	O
with	O
zeros	O
above	O
the	O
leading	O
diagonal	B
show	O
that	O
the	O
variable	O
y	O
lz	O
has	O
a	O
gaussian	B
distribution	O
with	O
mean	B
and	O
covariance	B
this	O
provides	O
a	O
technique	O
for	O
generating	O
samples	O
from	O
a	O
general	O
multivariate	O
gaussian	B
using	O
samples	O
from	O
a	O
univariate	O
gaussian	B
having	O
zero	O
mean	B
and	O
unit	O
variance	B
www	O
in	O
this	O
exercise	O
we	O
show	O
more	O
carefully	O
that	O
rejection	B
sampling	I
does	O
indeed	O
draw	O
samples	O
from	O
the	O
desired	O
distribution	O
pz	O
suppose	O
the	O
proposal	B
distribution	I
is	O
qz	O
and	O
show	O
that	O
the	O
probability	B
of	O
a	O
sample	O
value	O
z	O
being	O
accepted	O
is	O
given	O
is	O
any	O
unnormalized	O
distribution	O
that	O
is	O
proportional	O
to	O
pz	O
and	O
the	O
constant	O
k	O
is	O
set	O
to	O
the	O
smallest	O
value	O
that	O
ensures	O
kqz	O
for	O
all	O
values	O
of	O
z	O
note	O
that	O
the	O
probability	B
of	O
drawing	O
a	O
value	O
z	O
is	O
given	O
by	O
the	O
probability	B
of	O
drawing	O
that	O
value	O
from	O
qz	O
times	O
the	O
probability	B
of	O
accepting	O
that	O
value	O
given	O
that	O
it	O
has	O
been	O
drawn	O
make	O
use	O
of	O
this	O
along	O
with	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
to	O
write	O
down	O
the	O
normalized	O
form	O
for	O
the	O
distribution	O
over	O
z	O
and	O
show	O
that	O
it	O
equals	O
pz	O
suppose	O
that	O
z	O
has	O
a	O
uniform	B
distribution	I
over	O
the	O
interval	O
show	O
that	O
the	O
variable	O
y	O
b	O
tan	O
z	O
c	O
has	O
a	O
cauchy	B
distribution	I
given	O
by	O
determine	O
expressions	O
for	O
the	O
coefficients	O
ki	O
in	O
the	O
envelope	O
distribution	O
for	O
adaptive	B
rejection	B
sampling	I
using	O
the	O
requirements	O
of	O
continuity	O
and	O
normalization	O
by	O
making	O
use	O
of	O
the	O
technique	O
discussed	O
in	O
section	O
for	O
sampling	O
from	O
a	O
single	O
exponential	B
distribution	I
devise	O
an	O
algorithm	O
for	O
sampling	O
from	O
the	O
piecewise	O
exponential	B
distribution	I
defined	O
by	O
show	O
that	O
the	O
simple	O
random	O
walk	O
over	O
the	O
integers	O
defined	O
by	O
and	O
has	O
the	O
property	O
that	O
ez	O
ez	O
and	O
hence	O
by	O
induction	O
that	O
ez	O
figure	O
a	O
probability	B
distribution	O
over	O
two	O
variables	O
and	O
that	O
is	O
uniform	O
over	O
the	O
shaded	O
regions	O
and	O
that	O
is	O
zero	O
everywhere	O
else	O
exercises	O
www	O
show	O
that	O
the	O
gibbs	B
sampling	I
algorithm	O
discussed	O
in	O
section	O
satisfies	O
detailed	O
balance	O
as	O
defined	O
by	O
consider	O
the	O
distribution	O
shown	O
in	O
figure	O
discuss	O
whether	O
the	O
standard	O
gibbs	B
sampling	I
procedure	O
for	O
this	O
distribution	O
is	O
ergodic	O
and	O
therefore	O
whether	O
it	O
would	O
sample	O
correctly	O
from	O
this	O
distribution	O
consider	O
the	O
simple	O
graph	O
shown	O
in	O
figure	O
in	O
which	O
the	O
observed	O
node	B
x	O
is	O
given	O
by	O
a	O
gaussian	B
distribution	O
n	O
with	O
mean	B
and	O
precision	O
suppose	O
that	O
the	O
marginal	B
distributions	O
over	O
the	O
mean	B
and	O
precision	O
are	O
given	O
by	O
n	O
and	O
gam	O
b	O
where	O
gam	O
denotes	O
a	O
gamma	B
distribution	I
write	O
down	O
expressions	O
for	O
the	O
conditional	B
distributions	O
p	O
and	O
p	O
that	O
would	O
be	O
required	O
in	O
order	O
to	O
apply	O
gibbs	B
sampling	I
to	O
the	O
posterior	O
distribution	O
p	O
verify	O
that	O
the	O
over-relaxation	B
update	O
in	O
which	O
zi	O
has	O
mean	B
i	O
and	O
i	O
with	O
variance	B
i	O
and	O
where	O
has	O
zero	O
mean	B
and	O
unit	O
variance	B
gives	O
a	O
value	O
z	O
mean	B
i	O
and	O
variance	B
i	O
www	O
using	O
and	O
show	O
that	O
the	O
hamiltonian	O
equation	O
is	O
equivalent	O
to	O
similarly	O
using	O
show	O
that	O
is	O
equivalent	O
to	O
by	O
making	O
use	O
of	O
and	O
show	O
that	O
the	O
conditional	B
dis	O
tribution	O
prz	O
is	O
a	O
gaussian	B
figure	O
a	O
graph	O
involving	O
an	O
observed	O
gaussian	B
variable	O
x	O
with	O
prior	B
distributions	O
over	O
its	O
mean	B
and	O
precision	O
x	O
sampling	B
methods	I
www	O
verify	O
that	O
the	O
two	O
probabilities	O
and	O
are	O
equal	O
and	O
hence	O
that	O
detailed	O
balance	O
holds	O
for	O
the	O
hybrid	B
monte	I
carlo	I
algorithm	O
appendix	O
a	O
in	O
chapter	O
we	O
discussed	O
probabilistic	O
models	O
having	O
discrete	O
latent	O
variables	O
such	O
as	O
the	O
mixture	B
of	I
gaussians	I
we	O
now	O
explore	O
models	O
in	O
which	O
some	O
or	O
all	O
of	O
the	O
latent	O
variables	O
are	O
continuous	O
an	O
important	O
motivation	O
for	O
such	O
models	O
is	O
that	O
many	O
data	O
sets	O
have	O
the	O
property	O
that	O
the	O
data	O
points	O
all	O
lie	O
close	O
to	O
a	O
manifold	B
of	O
much	O
lower	O
dimensionality	O
than	O
that	O
of	O
the	O
original	O
data	O
space	O
to	O
see	O
why	O
this	O
might	O
arise	O
consider	O
an	O
artificial	O
data	O
set	O
constructed	O
by	O
taking	O
one	O
of	O
the	O
off-line	O
digits	O
represented	O
by	O
a	O
x	O
pixel	O
grey-level	O
image	O
and	O
embedding	O
it	O
in	O
a	O
larger	O
image	O
of	O
size	O
x	O
by	O
padding	O
with	O
pixels	O
having	O
the	O
value	O
zero	O
to	O
white	O
pixels	O
in	O
which	O
the	O
location	O
and	O
orientation	O
of	O
the	O
digit	O
is	O
varied	O
at	O
random	O
as	O
illustrated	O
in	O
figure	O
each	O
of	O
the	O
resulting	O
images	O
is	O
represented	O
by	O
a	O
point	O
in	O
the	O
x	O
ooo-dimensional	O
data	O
space	O
however	O
across	O
a	O
data	O
set	O
of	O
such	O
images	O
there	O
are	O
only	O
three	O
degrees	O
offreedom	O
of	O
variability	O
corresponding	O
to	O
the	O
vertical	O
and	O
horizontal	O
translations	O
and	O
the	O
rotations	O
the	O
data	O
points	O
will	O
therefore	O
live	O
on	O
a	O
subspace	O
of	O
the	O
data	O
space	O
whose	O
intrinsic	B
dimensionality	I
is	O
three	O
note	O
continuous	O
latent	O
variables	O
figure	O
a	O
synthetic	O
data	O
sel	O
obtained	O
by	O
taking	O
one	O
of	O
the	O
off-line	O
digit	O
images	O
and	O
creating	O
ple	O
copies	O
in	O
each	O
of	O
which	O
the	O
digit	O
has	O
undergone	O
a	O
random	O
displacement	O
and	O
rotation	O
within	O
some	O
larger	O
image	O
field	O
the	O
resulting	O
images	O
each	O
have	O
pixels	O
that	O
the	O
manifold	B
will	O
be	O
nonlinear	O
because	O
for	O
instance	O
if	O
we	O
translate	O
the	O
digit	O
past	O
a	O
particular	O
pixel	O
that	O
pixel	O
value	O
will	O
go	O
from	O
zero	O
one	O
and	O
back	O
to	O
zero	O
again	O
which	O
is	O
clearly	O
a	O
nonlinear	O
function	O
of	O
the	O
digit	O
position	O
in	O
this	O
example	O
lranslation	O
and	O
rotation	O
parameters	O
are	O
latent	O
variables	O
because	O
we	O
observe	O
only	O
the	O
image	O
vectors	O
and	O
are	O
not	O
told	O
which	O
values	O
of	O
the	O
translation	O
or	O
rotation	O
variables	O
were	O
used	O
to	O
create	O
them	O
for	O
real	O
digit	O
image	O
data	O
there	O
will	O
be	O
a	O
funher	O
degree	O
of	O
freedom	O
arising	O
from	O
scaling	O
moreover	O
there	O
will	O
be	O
multiple	O
addilional	O
degrees	B
of	I
freedom	I
associaled	O
wilh	O
more	O
complex	O
deformations	O
due	O
to	O
the	O
variability	O
in	O
an	O
individuals	O
wriling	O
well	O
as	O
lhe	O
differences	O
in	O
writing	O
slyles	O
between	O
individuals	O
evenheless	O
the	O
number	O
of	O
such	O
degrees	B
of	I
freedom	I
will	O
be	O
small	O
compared	O
to	O
the	O
dimensionality	O
of	O
ihe	O
data	O
set	O
another	O
example	O
is	O
provided	O
by	O
the	O
oil	O
flow	O
data	O
set	O
in	O
which	O
a	O
given	O
geometrical	O
configuration	O
of	O
the	O
gas	O
woller	O
and	O
oil	O
phases	O
there	O
are	O
only	O
two	O
degrees	B
of	I
freedom	I
of	O
variability	O
corresponding	O
to	O
the	O
fraction	O
of	O
oil	O
in	O
the	O
pipe	O
and	O
the	O
tion	O
of	O
water	O
fraction	O
of	O
gas	O
ihen	O
being	O
determined	O
ahhough	O
the	O
data	O
space	O
comprises	O
measuremenls	O
a	O
data	O
set	O
of	O
points	O
will	O
lie	O
close	O
to	O
a	O
iwo-dimensional	O
manifold	B
embedded	O
within	O
this	O
space	O
in	O
this	O
case	O
the	O
manifold	B
comprises	O
scveral	O
distinct	O
segments	O
corresponding	O
to	O
different	O
flow	O
regimes	O
each	O
such	O
segment	O
being	O
a	O
continuous	O
two-dimensional	O
manifold	B
if	O
our	O
goal	O
is	O
data	B
compression	I
or	O
density	B
modelling	O
then	O
there	O
can	O
be	O
benefits	O
in	O
exploiling	O
this	O
manifold	B
struclure	O
the	O
data	O
points	O
will	O
not	O
be	O
confined	O
precisely	O
to	O
a	O
smooth	O
dimensional	O
manifold	B
and	O
we	O
can	O
interpret	O
the	O
departures	O
of	O
data	O
points	O
from	O
the	O
manifold	B
as	O
noise	O
this	O
leads	O
naturally	O
to	O
a	O
generative	O
view	O
of	O
such	O
models	O
in	O
which	O
we	O
first	O
select	O
a	O
poinl	O
within	O
the	O
manifold	B
according	O
to	O
some	O
latent	B
variable	I
distribution	O
and	O
then	O
generate	O
an	O
observed	O
data	O
point	O
by	O
noise	O
drawn	O
from	O
some	O
conditional	B
distribution	O
of	O
the	O
data	O
varillbles	O
given	O
the	O
latent	O
varillbles	O
in	O
praclice	O
thc	O
simplest	O
continuous	O
latent	B
variable	I
model	O
assumes	O
gaussian	B
distributions	O
for	O
both	O
thc	O
latent	O
and	O
observed	O
variables	O
and	O
makes	O
use	O
of	O
a	O
lineargaussian	O
dependence	O
of	O
the	O
observed	O
variables	O
on	O
ihe	O
slate	O
of	O
the	O
latent	O
variables	O
this	O
leads	O
to	O
a	O
probabilislic	O
fonnulation	O
of	O
the	O
well-known	O
technique	O
of	O
principal	B
component	I
analysis	I
as	O
well	O
as	O
a	O
related	O
model	O
called	O
factor	B
analysis	I
in	O
this	O
chapter	O
w	O
will	O
begin	O
wilh	O
a	O
slandard	O
nonprobabilistic	O
treatment	O
of	O
pea	O
and	O
thcn	O
we	O
show	O
how	O
pea	O
arises	O
naturally	O
as	O
the	O
maximum	B
likelihood	I
solution	O
appendix	O
a	O
section	O
section	O
principal	O
analjsis	O
pifcipal	O
compooont	O
a	O
seeks	O
dimensionality	O
ktwil	O
as	O
plno	O
pal	O
subspace	O
denoted	O
ijy	O
the	O
magenta	O
line	O
such	O
itlet	O
the	O
grthogonet	O
data	O
points	O
doisl	O
onto	O
tpns	O
the	O
varia	O
of	O
projated	O
points	O
dois	O
an	O
pca	O
is	O
based	O
on	O
m	O
mizing	O
the	O
squares	O
of	O
projection	O
errors	O
indcated	O
by	O
the	O
bfi	O
e	O
lines	O
scrio	O
sdi	O
a	O
panlcula	O
fonn	O
of	O
linear-gauian	O
latem	O
model	O
this	O
probabilistic	O
mulation	O
bring	O
many	O
adimlags	O
suh	O
as	O
tll	O
use	O
if	O
em	B
for	O
parameter	O
eslimalion	O
rrinciplej	O
ctensioos	O
oliturc	O
of	O
pea	O
model	O
and	O
basian	O
formulatons	O
that	O
allow	O
tbe	O
number	O
of	O
rrincipal	O
comoncnts	O
to	O
be	O
detennined	O
aulomatically	O
from	O
data	O
finally	O
dislus	O
briefly	O
gencraliation	O
of	O
the	O
latent	O
yariable	O
concept	O
that	O
gl	O
beood	O
tbe	O
linear-gaussian	O
assumption	O
including	O
non	O
gauin	O
i	O
tcnt	O
abies	O
lea	O
to	O
tbe	O
fr	O
me	O
ork	O
of	O
indrlmj	O
m	O
compon	O
nl	O
anal-	O
as	O
a	O
models	O
haing	O
a	O
nonlinear	O
rclationship	O
bet	O
latent	O
and	O
ooseej	O
principal	B
component	I
analysis	I
principal	O
compooem	O
analy	O
or	O
rca	O
s	O
a	O
technique	O
tha	O
is	O
ued	O
for	O
appli	O
cations	O
such	O
as	O
dimensionality	O
lossy	O
data	O
comprcion	O
feature	O
etracti	O
and	O
data	O
vualizatioll	O
its	O
also	O
kno	O
as	O
tile	O
karoan	O
n	O
i	O
tran	O
f	O
lbcrc	O
an	O
t	O
o	O
commonly	O
used	O
definitions	O
of	O
pea	O
that	O
giye	O
rise	O
to	O
the	O
algorithm	O
pea	O
can	O
be	O
defined	O
as	O
the	O
unhoglnal	O
projtttion	O
of	O
the	O
data	O
a	O
lo	O
er	O
dimensionallincar	O
space	O
kno	O
n	O
as	O
the	O
prilcip	O
al	O
p	O
aa	O
soch	O
that	O
the	O
of	O
the	O
projttted	O
data	O
i	O
maimiej	O
equialemlyt	O
can	O
be	O
defined	O
as	O
tbe	O
linear	O
projection	O
that	O
minimi	O
the	O
average	O
projttlion	O
cost	O
defined	O
as	O
t	O
mean	B
squa	O
-ed	O
distance	O
the	O
data	O
and	O
tbeir	O
pojtttioo	O
the	O
ljocs	O
of	O
onhogonal	O
projection	O
i	O
illustraled	O
in	O
figute	O
we	O
conider	O
each	O
of	O
these	O
definitions	O
in	O
tum	O
mllximllm	O
variance	B
lormulation	O
conider	O
a	O
dala	O
set	O
obserlations	O
where	O
s	O
and	O
x	O
i	O
a	O
euclidean	O
variable	O
dimenionality	O
d	O
our	O
goal	O
is	O
to	O
project	O
if	O
data	O
onto	O
a	O
haing	O
dimenionality	O
m	O
d	O
hile	O
the	O
of	O
the	O
projttted	O
data	O
for	O
the	O
we	O
assume	O
that	O
tbe	O
of	O
m	O
is	O
g	O
en	O
latcr	O
in	O
this	O
continuous	O
latent	O
variables	O
chapter	O
we	O
shall	O
consider	O
techniques	O
to	O
determine	O
an	O
appropriate	O
value	O
of	O
iv	O
from	O
the	O
data	O
to	O
begin	O
with	O
consider	O
the	O
projection	O
onto	O
a	O
one-dimensional	O
space	O
we	O
can	O
define	O
the	O
direction	O
of	O
this	O
space	O
using	O
a	O
d-dimensional	O
vector	O
ul	O
which	O
for	O
convenience	O
without	O
loss	O
of	O
generality	O
we	O
shall	O
choose	O
to	O
be	O
a	O
unit	O
vector	O
so	O
that	O
uf	O
ul	O
that	O
we	O
are	O
only	O
interested	O
in	O
the	O
direction	O
defined	O
by	O
ul	O
not	O
in	O
the	O
magnitude	O
of	O
ul	O
itself	O
each	O
data	O
point	O
x	O
n	O
is	O
then	O
projected	O
onto	O
a	O
scalar	O
value	O
uf	O
x	O
n	O
the	O
mean	B
of	O
the	O
projected	O
data	O
is	O
ufx	O
where	O
x	O
is	O
the	O
sample	O
set	O
mean	B
given	O
by	O
and	O
the	O
variance	B
of	O
the	O
projected	O
data	O
is	O
given	O
by	O
where	O
s	O
is	O
the	O
data	O
covariance	B
matrix	O
defined	O
by	O
s	O
xxn	O
xt	O
n	O
nlj	O
nl	O
appendix	O
e	O
we	O
now	O
maximize	O
the	O
projected	O
variance	B
ufsul	O
with	O
respect	O
to	O
ul	O
clearly	O
this	O
has	O
to	O
be	O
a	O
constrained	O
maximization	O
to	O
prevent	O
ilulll	O
the	O
appropriate	O
constraint	O
comes	O
from	O
the	O
normalization	O
condition	O
uf	O
ul	O
to	O
enforce	O
this	O
constraint	O
we	O
introduce	O
a	O
lagrange	B
multiplier	I
that	O
we	O
shall	O
denote	O
by	O
ai	O
and	O
then	O
make	O
an	O
unconstrained	O
maximization	O
of	O
by	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
ul	O
equal	O
to	O
zero	O
we	O
see	O
that	O
this	O
quantity	O
will	O
have	O
a	O
stationary	B
point	O
when	O
which	O
says	O
that	O
ul	O
must	O
be	O
an	O
eigenvector	O
of	O
s	O
if	O
we	O
left-multiply	O
by	O
uf	O
and	O
make	O
use	O
of	O
uf	O
ul	O
we	O
see	O
that	O
the	O
variance	B
is	O
given	O
by	O
and	O
so	O
the	O
variance	B
will	O
be	O
a	O
maximum	O
when	O
we	O
set	O
ul	O
equal	O
to	O
the	O
eigenvector	O
having	O
the	O
largest	O
eigenvalue	O
ai	O
this	O
eigenvector	O
is	O
known	O
as	O
the	O
first	O
principal	O
component	O
we	O
can	O
define	O
additional	O
principal	O
components	O
in	O
an	O
incremental	O
fashion	O
by	O
choosing	O
each	O
new	O
direction	O
to	O
be	O
that	O
which	O
maximizes	O
the	O
projected	O
variance	B
exercise	O
section	O
appendix	O
c	O
principal	B
component	I
analysis	I
amongst	O
all	O
possible	O
directions	O
orthogonal	O
to	O
those	O
already	O
considered	O
if	O
we	O
sider	O
the	O
general	O
case	O
of	O
an	O
m	O
projection	O
space	O
the	O
optimal	O
linear	O
jection	O
for	O
which	O
the	O
variance	B
of	O
the	O
projected	O
data	O
is	O
maximized	O
is	O
now	O
defined	O
by	O
the	O
m	O
eigenvectors	O
u	O
u	O
m	O
of	O
the	O
data	O
covariance	B
matrix	O
s	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
this	O
is	O
easily	O
shown	O
using	O
proof	O
by	O
induction	O
to	O
summarize	O
principal	B
component	I
analysis	I
involves	O
evaluating	O
the	O
mean	B
x	O
and	O
the	O
covariance	B
matrix	O
s	O
of	O
the	O
data	O
set	O
and	O
then	O
finding	O
the	O
m	O
eigenvectors	O
of	O
s	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
algorithms	O
for	O
finding	O
eigenvectors	O
and	O
eigenvalues	O
as	O
well	O
as	O
additional	O
theorems	O
related	O
to	O
eigenvector	O
decomposition	O
can	O
be	O
found	O
in	O
golub	O
and	O
van	O
loan	O
note	O
that	O
the	O
computational	O
cost	O
of	O
computing	O
the	O
full	O
eigenvector	O
decomposition	O
for	O
a	O
matrix	O
of	O
size	O
d	O
x	O
dis	O
if	O
we	O
plan	O
to	O
project	O
our	O
data	O
onto	O
the	O
first	O
m	O
principal	O
components	O
then	O
we	O
only	O
need	O
to	O
find	O
the	O
first	O
m	O
eigenvalues	O
and	O
eigenvectors	O
this	O
can	O
be	O
done	O
with	O
more	O
efficient	O
techniques	O
such	O
as	O
the	O
power	B
method	I
and	O
van	O
loan	O
that	O
scale	O
like	O
omd	O
or	O
alternatively	O
we	O
can	O
make	O
use	O
of	O
the	O
em	B
algorithm	I
minimum-error	O
formulation	O
we	O
now	O
discuss	O
an	O
alternative	O
formulation	O
of	O
pea	O
based	O
on	O
projection	O
error	B
minimization	O
to	O
do	O
this	O
we	O
introduce	O
a	O
complete	O
orthonormal	O
set	O
of	O
d-dimensional	O
basis	O
vectors	O
where	O
i	O
d	O
that	O
satisfy	O
because	O
this	O
basis	O
is	O
complete	O
each	O
data	O
point	O
can	O
be	O
represented	O
exactly	O
by	O
a	O
linear	O
combination	O
of	O
the	O
basis	O
vectors	O
d	O
x	O
n	O
laniui	O
il	O
where	O
the	O
coefficients	O
ani	O
will	O
be	O
different	O
for	O
different	O
data	O
points	O
this	O
simply	O
corresponds	O
to	O
a	O
rotation	O
of	O
the	O
coordinate	O
system	O
to	O
a	O
new	O
system	O
defined	O
by	O
the	O
and	O
the	O
original	O
d	O
components	O
xnd	O
are	O
replaced	O
by	O
an	O
equivalent	O
set	O
taking	O
the	O
inner	O
product	O
with	O
uj	O
and	O
making	O
use	O
of	O
the	O
thonormality	O
property	O
we	O
obtain	O
anj	O
xuj	O
and	O
so	O
without	O
loss	O
of	O
generality	O
we	O
can	O
write	O
d	O
x	O
n	O
l	O
ui	O
il	O
our	O
goal	O
however	O
is	O
to	O
approximate	O
this	O
data	O
point	O
using	O
a	O
representation	O
volving	O
a	O
restricted	O
number	O
m	O
d	O
of	O
variables	O
corresponding	O
to	O
a	O
projection	O
onto	O
a	O
lower-dimensional	O
subspace	O
the	O
m	O
linear	O
subspace	O
can	O
be	O
sented	O
without	O
loss	O
of	O
generality	O
by	O
the	O
first	O
m	O
of	O
the	O
basis	O
vectors	O
and	O
so	O
we	O
approximate	O
each	O
data	O
point	O
x	O
n	O
by	O
m	O
xn	O
l	O
d	O
zniui	O
l	O
biui	O
il	O
iml	O
continuous	O
latent	O
variables	O
where	O
the	O
depend	O
on	O
the	O
particular	O
data	O
point	O
whereas	O
the	O
are	O
constants	O
that	O
are	O
the	O
same	O
for	O
all	O
data	O
points	O
we	O
are	O
free	O
to	O
choose	O
the	O
the	O
and	O
the	O
so	O
as	O
to	O
minimize	O
the	O
distortion	O
introduced	O
by	O
the	O
reduction	O
in	O
ity	O
as	O
our	O
distortion	B
measure	I
we	O
shall	O
use	O
the	O
squared	O
distance	O
between	O
the	O
original	O
data	O
point	O
x	O
n	O
and	O
its	O
approximation	O
xn	O
averaged	O
over	O
the	O
data	O
set	O
so	O
that	O
our	O
goal	O
is	O
to	O
minimize	O
n	O
j	O
l	O
ilxn	O
xn	O
nl	O
consider	O
first	O
of	O
all	O
the	O
minimization	O
with	O
respect	O
to	O
the	O
quantities	O
stituting	O
for	O
xn	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
znj	O
to	O
zero	O
and	O
making	O
use	O
of	O
the	O
orthonormality	O
conditions	O
we	O
obtain	O
where	O
j	O
similarly	O
setting	O
the	O
derivative	B
of	O
j	O
with	O
respect	O
to	O
bi	O
to	O
zero	O
and	O
again	O
making	O
use	O
of	O
the	O
orthonormality	O
relations	O
gives	O
where	O
j	O
m	O
if	O
we	O
substitute	O
for	O
zni	O
and	O
bi	O
and	O
make	O
use	O
of	O
the	O
general	O
expansion	O
we	O
obtain	O
b	O
j	O
x	O
uj	O
x	O
n	O
x	O
n	O
l	O
x	O
n	O
xtud	O
ui	O
d	O
iml	O
from	O
which	O
we	O
see	O
that	O
the	O
displacement	O
vector	O
from	O
x	O
n	O
to	O
xn	O
lies	O
in	O
the	O
space	O
orthogonal	O
to	O
the	O
principal	B
subspace	I
because	O
it	O
is	O
a	O
linear	O
combination	O
of	O
for	O
i	O
m	O
d	O
as	O
illustrated	O
in	O
figure	O
this	O
is	O
to	O
be	O
expected	O
because	O
the	O
projected	O
points	O
xn	O
must	O
lie	O
within	O
the	O
principal	B
subspace	I
but	O
we	O
can	O
move	O
them	O
freely	O
within	O
that	O
subspace	O
and	O
so	O
the	O
minimum	O
error	B
is	O
given	O
by	O
the	O
orthogonal	O
projection	O
we	O
therefore	O
obtain	O
an	O
expression	O
for	O
the	O
distortion	B
measure	I
j	O
as	O
a	O
function	O
purely	O
of	O
the	O
in	O
the	O
form	O
j	O
n	O
l	O
l	O
l	O
u	O
i	O
sui	O
t	O
d	O
x	O
n	O
ui	O
x	O
ui	O
nl	O
iml	O
iml	O
there	O
remains	O
the	O
task	O
of	O
minimizing	O
j	O
with	O
respect	O
to	O
the	O
which	O
must	O
be	O
a	O
constrained	O
minimization	O
otherwise	O
we	O
will	O
obtain	O
the	O
vacuous	O
result	O
ui	O
o	O
the	O
constraints	O
arise	O
from	O
the	O
orthonormality	O
conditions	O
and	O
as	O
we	O
shall	O
see	O
the	O
solution	O
will	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
eigenvector	O
expansion	O
of	O
the	O
covariance	B
matrix	O
before	O
considering	O
a	O
formal	O
solution	O
let	O
us	O
try	O
to	O
obtain	O
some	O
intuition	O
about	O
the	O
result	O
by	O
considering	O
the	O
case	O
of	O
a	O
two-dimensional	O
data	O
space	O
d	O
and	O
a	O
dimensional	O
principal	B
subspace	I
m	O
we	O
have	O
to	O
choose	O
a	O
direction	O
so	O
as	O
to	O
principal	B
component	I
analysis	I
minimize	O
j	O
subject	O
to	O
the	O
normalization	O
constraint	O
ui	O
using	O
a	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
constraint	O
we	O
consider	O
the	O
minimization	O
of	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
to	O
zero	O
we	O
obtain	O
so	O
that	O
is	O
an	O
eigenvector	O
of	O
s	O
with	O
eigenvalue	O
thus	O
any	O
eigenvector	O
will	O
define	O
a	O
tionary	O
point	O
of	O
the	O
distortion	B
measure	I
to	O
find	O
the	O
value	O
of	O
j	O
at	O
the	O
minimum	O
we	O
back-substitute	O
the	O
solution	O
for	O
into	O
the	O
distortion	B
measure	I
to	O
give	O
j	O
we	O
therefore	O
obtain	O
the	O
minimum	O
value	O
of	O
j	O
by	O
choosing	O
to	O
be	O
the	O
eigenvector	O
sponding	O
to	O
the	O
smaller	O
of	O
the	O
two	O
eigenvalues	O
thus	O
we	O
should	O
choose	O
the	O
principal	B
subspace	I
to	O
be	O
aligned	O
with	O
the	O
eigenvector	O
having	O
the	O
larger	O
eigenvalue	O
this	O
result	O
accords	O
with	O
our	O
intuition	O
that	O
in	O
order	O
to	O
minimize	O
the	O
average	O
squared	O
projection	O
distance	O
we	O
should	O
choose	O
the	O
principal	O
component	O
subspace	O
to	O
pass	O
through	O
the	O
mean	B
of	O
the	O
data	O
points	O
and	O
to	O
be	O
aligned	O
with	O
the	O
directions	O
of	O
maximum	O
variance	B
for	O
the	O
case	O
when	O
the	O
eigenvalues	O
are	O
equal	O
any	O
choice	O
of	O
principal	O
direction	O
will	O
give	O
rise	O
to	O
the	O
same	O
value	O
of	O
j	O
the	O
general	O
solution	O
to	O
the	O
minimization	O
of	O
j	O
for	O
arbitrary	O
d	O
and	O
arbitrary	O
m	O
d	O
is	O
obtained	O
by	O
choosing	O
the	O
to	O
be	O
eigenvectors	O
of	O
the	O
covariance	B
matrix	O
given	O
by	O
where	O
i	O
and	O
as	O
usual	O
the	O
eigenvectors	O
are	O
chosen	O
to	O
be	O
mal	O
the	O
corresponding	O
value	O
of	O
the	O
distortion	B
measure	I
is	O
then	O
given	O
by	O
sui	O
aiui	O
d	O
j	O
l	O
ai	O
iml	O
which	O
is	O
simply	O
the	O
sum	O
of	O
the	O
eigenvalues	O
of	O
those	O
eigenvectors	O
that	O
are	O
orthogonal	O
to	O
the	O
principal	B
subspace	I
we	O
therefore	O
obtain	O
the	O
minimum	O
value	O
of	O
j	O
by	O
selecting	O
these	O
eigenvectors	O
to	O
be	O
those	O
having	O
the	O
d	O
m	O
smallest	O
eigenvalues	O
and	O
hence	O
the	O
eigenvectors	O
defining	O
the	O
principal	B
subspace	I
are	O
those	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
although	O
we	O
have	O
considered	O
m	O
d	O
the	O
pca	O
analysis	O
still	O
holds	O
if	O
m	O
d	O
in	O
which	O
case	O
there	O
is	O
no	O
dimensionality	O
reduction	O
but	O
simply	O
a	O
rotation	O
of	O
the	O
coordinate	O
axes	O
to	O
align	O
with	O
principal	O
components	O
finally	O
it	O
is	O
worth	O
noting	O
that	O
there	O
exists	O
a	O
closely	O
related	O
linear	O
dimensionality	O
reduction	O
technique	O
called	O
canonical	B
correlation	I
analysis	I
or	O
cca	O
bach	O
and	O
jordan	O
whereas	O
pca	O
works	O
with	O
a	O
single	O
random	O
variable	O
cca	O
considers	O
two	O
more	O
variables	O
and	O
tries	O
to	O
find	O
a	O
corresponding	O
pair	O
of	O
linear	O
subspaces	O
that	O
have	O
high	O
cross-correlation	O
so	O
that	O
each	O
component	O
within	O
one	O
of	O
the	O
subspaces	O
is	O
correlated	O
with	O
a	O
single	O
component	O
from	O
the	O
other	O
subspace	O
its	O
solution	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
a	O
generalized	B
eigenvector	O
problem	O
applications	O
of	O
pea	O
we	O
can	O
illustrate	O
the	O
use	O
of	O
pca	O
for	O
data	B
compression	I
by	O
considering	O
the	O
line	O
digits	O
data	O
set	O
because	O
each	O
eigenvector	O
of	O
the	O
covariance	B
matrix	O
is	O
a	O
vector	O
exercise	O
appendix	O
a	O
coltinuolis	O
latfit	O
figure	O
the	O
mean	B
x	O
aklog	O
with	O
iit	O
lou	O
pca	O
egerrveclrll	O
ul	O
cligits	O
data	O
set	O
tgetller	O
with	O
correspondi	O
lor	O
the	O
the	O
d-limensional	O
space	O
we	O
can	O
represent	O
tho	O
eigenwctos	O
as	O
imago	O
of	O
tho	O
same	O
silo	O
as	O
data	O
poi	O
first	O
ihe	O
along	O
wich	O
tlo	O
sponding	O
are	O
in	O
figure	O
a	O
plo	O
ofllo	O
complete	O
spectm	O
uf	O
oigo	O
alue	O
sone	O
into	O
decreasing	O
order	O
is	O
shown	O
in	O
figure	O
the	O
ditortion	O
measure	O
j	O
assqciated	O
wilh	O
chooing	O
a	O
particular	O
value	O
of	O
m	O
is	O
gi	O
en	O
by	O
tho	O
sum	O
of	O
the	O
eig	O
nlues	O
from	O
m	O
i	O
up	O
to	O
and	O
is	O
pto	O
ted	O
for	O
different	O
of	O
in	O
figure	O
if	O
and	O
into	O
we	O
can	O
write	O
the	O
ica	O
appro	O
imation	O
to	O
a	O
data	O
x	O
i	O
the	O
fonn	O
lxu	O
i	O
m	O
xlxu-xtuu	O
m	O
to	O
fiiiure	O
piol	O
at	O
ejoinv	O
loo	O
lor	O
the	O
off	O
digits	O
data	O
set	O
sum	O
at	O
the	O
which	O
s	O
um-ol	O
sqes	O
distortlon	O
j	O
i	O
by	O
projectixl	O
the	O
data	O
onto	O
a	O
pincipal	O
componenl	O
slllspaee	O
dimensionalitv	O
m	O
anal	O
fiiiur	O
an	O
irom	O
lie	O
digils	O
data	O
with	O
its	O
pea	O
reonstnxlions	O
oblair	O
by	O
jincipal	O
various	O
val	O
as	O
increason	O
reonstuctiofi	O
more	O
aourate	O
and	O
woukl	O
portee	O
when	O
k	O
d	O
x	O
ai	O
a	O
seeron	O
where	O
we	O
hae	O
made	O
moe	O
of	O
the	O
relation	O
x	O
l	O
u	O
which	O
follow	O
from	O
the	O
completene	O
of	O
the	O
i	O
thi	O
represent	O
a	O
contpreioo	O
the	O
data	O
ilttaue	O
for	O
each	O
data	O
poim	O
we	O
ha	O
repla	O
d	O
the	O
v	O
dimensiooal	O
x	O
wilh	O
an	O
having	O
componem	O
x	O
the	O
of	O
m	O
the	O
greater	O
the	O
degree	O
of	O
comp	O
-eion	O
example	O
of	O
pea	O
of	O
data	O
points	O
for	O
the	O
digits	O
data	O
set	O
are	O
shown	O
in	O
figure	O
anolher	O
application	O
of	O
priocipal	O
compcmenl	O
analyi	O
i	O
to	O
data	O
pre-processing	O
in	O
thi	O
case	O
lhe	O
goal	O
is	O
no	O
dimensionality	O
but	O
rather	O
the	O
tmnformmion	O
of	O
a	O
data	O
sel	O
in	O
ork	O
to	O
standalli	O
e	O
eenain	O
of	O
ils	O
pmpenies	O
this	O
can	O
be	O
inportanl	O
in	O
allowing	O
pallem	O
algorithm	O
be	O
applied	O
successfully	O
the	O
data	O
typically	O
il	O
is	O
done	O
wilen	O
the	O
original	O
are	O
meaured	O
in	O
dif	O
ferent	O
unil	O
or	O
significantly	O
difterent	O
for	O
instance	O
in	O
the	O
old	B
faithful	I
data	I
sel	O
the	O
time	O
betv	O
-een	O
eruption	O
i	O
typicany	O
an	O
order	O
of	O
greater	O
than	O
lhe	O
durali	O
of	O
n	O
erupt	O
when	O
we	O
applied	O
the	O
algorill	O
thi	O
data	O
set	O
first	O
made	O
a	O
separ	O
te	O
linear	O
re-sealing	O
of	O
the	O
individual	O
socb	O
thm	O
each	O
had	O
zero	O
mean	B
and	O
unit	O
llus	O
is	O
known	O
as	O
slllnlardiv	O
the	O
dota	O
and	O
the	O
coanance	O
matrix	O
for	O
lhe	O
dala	O
has	O
components	O
where	O
is	O
the	O
of	O
this	O
i	O
known	O
as	O
the	O
matri	O
of	O
the	O
original	O
dota	O
and	O
ha	O
the	O
propeny	O
thai	O
if	O
to	O
rompooent	O
x	O
and	O
x	O
of	O
the	O
data	O
are	O
correl	O
ted	O
then	O
ai	O
i	O
nd	O
if	O
they	O
a	O
-e	O
uocorrelated	O
then	O
ai	O
o	O
using	O
pea	O
we	O
can	O
make	O
a	O
itofe	O
subst	O
mial	O
nonnalizatoo	O
of	O
the	O
data	O
to	O
gic	O
it	O
zero	O
mean	B
and	O
unit	O
co	O
ariance	O
so	O
that	O
different	O
become	O
latel	O
to	O
do	O
this	O
we	O
first	O
the	O
equation	O
in	O
the	O
form	O
su	O
ul	O
continuous	O
latent	O
variables	O
b	O
o	O
tj	O
cpo	O
o	O
figure	O
illustration	O
of	O
the	O
effects	O
of	O
linear	O
pre-processing	O
applied	O
to	O
the	O
old	B
faithful	I
data	I
set	O
the	O
plot	O
on	O
the	O
left	O
shows	O
the	O
original	O
data	O
the	O
centre	O
plot	O
shows	O
the	O
result	O
of	O
standardizing	B
the	O
individual	O
variables	O
to	O
zero	O
mean	B
and	O
unit	O
variance	B
also	O
shown	O
are	O
the	O
principal	O
axes	O
of	O
this	O
normalized	O
data	O
set	O
plotted	O
over	O
the	O
range	O
the	O
plot	O
on	O
the	O
right	O
shows	O
the	O
result	O
of	O
whitening	B
of	O
the	O
data	O
to	O
give	O
it	O
zero	O
mean	B
and	O
unit	O
covariance	B
where	O
l	O
is	O
a	O
d	O
x	O
d	O
diagonal	B
matrix	O
with	O
elements	O
ai	O
and	O
u	O
is	O
a	O
d	O
x	O
d	O
onal	O
matrix	O
with	O
columns	O
given	O
by	O
ui	O
then	O
we	O
define	O
for	O
each	O
data	O
point	O
x	O
n	O
a	O
transformed	O
value	O
given	O
by	O
where	O
x	O
is	O
the	O
sample	B
mean	B
defined	O
by	O
clearly	O
the	O
set	O
has	O
zero	O
mean	B
and	O
its	O
covariance	B
is	O
given	O
by	O
the	O
identity	O
matrix	O
because	O
n	O
l	O
l	O
xxn	O
l	O
l	O
i	O
nl	O
appendix	O
a	O
appendix	O
a	O
this	O
operation	O
is	O
known	O
as	O
whitening	B
or	O
sphereing	B
the	O
data	O
and	O
is	O
illustrated	O
for	O
the	O
old	B
faithful	I
data	I
set	O
in	O
figure	O
it	O
is	O
interesting	O
to	O
compare	O
pca	O
with	O
the	O
fisher	B
linear	B
discriminant	I
which	O
was	O
discussed	O
in	O
section	O
both	O
methods	O
can	O
be	O
viewed	O
as	O
techniques	O
for	O
linear	O
dimensionality	O
reduction	O
however	O
pca	O
is	O
unsupervised	O
and	O
depends	O
only	O
on	O
the	O
values	O
x	O
n	O
whereas	O
fisher	B
linear	B
discriminant	I
also	O
uses	O
class-label	O
information	O
this	O
difference	O
is	O
highlighted	O
by	O
the	O
example	O
in	O
figure	O
another	O
common	O
application	O
of	O
principal	B
component	I
analysis	I
is	O
to	O
data	O
ization	O
here	O
each	O
data	O
point	O
is	O
projected	O
onto	O
a	O
two-dimensional	O
principal	B
subspace	I
so	O
that	O
a	O
data	O
point	O
x	O
n	O
is	O
plotted	O
at	O
cartesian	O
coordinates	O
given	O
by	O
xj	O
and	O
xj	O
where	O
ul	O
and	O
are	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
largest	O
and	O
second	O
largest	O
eigenvalues	O
an	O
example	O
of	O
such	O
a	O
plot	O
for	O
the	O
oil	O
flow	O
data	O
set	O
is	O
shown	O
in	O
figure	O
iincipal	O
clm	O
anals	O
fig	O
a	O
comparison	O
proipal	O
mnt	O
analysis	O
fishas	O
linaar	O
discriminant	O
ality	O
rduclion	O
here	O
too	O
data	O
in	O
two	O
dimansions	O
belonging	O
to	O
two	O
classes	O
in	O
red	O
and	O
blue	O
is	O
to	O
be	O
pfoicled	O
onto	O
a	O
s	O
ingle	O
di	O
mension	O
pca	O
cxlsas	O
the	O
direc	O
tion	O
maximum	O
variae	O
try	O
tha	O
co	O
leads	O
to	O
strong	O
class	O
overlap	O
whereas	O
fislef	O
iimar	O
discfornillant	O
takes	O
too	O
class	O
labels	O
and	O
leads	O
to	O
a	O
projection	O
onto	O
the	O
gean	O
cum	O
giving	O
much	O
tetler	O
class	O
separation	O
fig	O
visualilatlon	O
oilllow	O
liet	O
obtained	O
try	O
projoecting	O
the	O
onto	O
the	O
lirst	O
two	O
prin	O
cipal	O
the	O
blue	O
and	O
points	O
corre-spond	O
to	O
and	O
flow	O
oonligurations	O
pea	O
for	O
high-dimensional	O
data	O
in	O
some	O
application	O
of	O
plitlcipal	O
component	O
analysis	O
the	O
number	O
of	O
data	O
points	O
is	O
smaller	O
than	O
t	O
c	O
dimensionality	O
of	O
troe	O
data	O
foi	O
example	O
might	O
want	O
to	O
apply	O
pea	O
to	O
a	O
data	O
of	O
a	O
few	O
hundred	O
images	O
each	O
of	O
rorrespooos	O
to	O
a	O
in	O
a	O
of	O
poientially	O
million	O
dimensiolls	O
tn	O
thfle	O
enlour	O
for	O
each	O
of	O
the	O
pi	O
ls	O
in	O
troe	O
image	O
noie	O
that	O
in	O
a	O
d-limenional	O
space	O
a	O
set	O
of	O
jy	O
points	O
n	O
d	O
defines	O
a	O
linear	O
subspae	O
dimensinality	O
is	O
at	O
n	O
and	O
so	O
there	O
is	O
linle	O
point	O
in	O
applying	O
pea	O
for	O
of	O
m	O
thai	O
greater	O
than	O
n	O
indeed	O
if	O
pelf	O
pea	O
we	O
will	O
find	O
that	O
at	O
least	O
d	O
n	O
i	O
of	O
the	O
eigen	O
lues	O
art	O
lero	O
eorrespnnding	O
tq	O
eigenvectors	O
aloog	O
direclioos	O
the	O
data	O
has	O
varianee	O
funhemore	O
typical	O
algolithm	O
for	O
finding	O
the	O
eigeneet	O
of	O
a	O
d	O
x	O
d	O
matrix	O
hae	O
a	O
computatiooal	O
eosl	O
thm	O
scales	O
like	O
o	O
dj	O
aoo	O
so	O
for	O
appliealions	O
such	O
as	O
the	O
image	O
eample	O
a	O
direc	O
application	O
of	O
pea	O
will	O
be	O
computatiooally	O
infe-sibje	O
w	O
can	O
resohe	O
this	O
problem	O
as	O
foil	O
firl	O
let	O
us	O
define	O
x	O
to	O
be	O
the	O
dj	O
i	O
continuous	O
latent	O
variables	O
dimensional	O
centred	O
data	O
matrix	O
whose	O
nth	O
row	O
is	O
given	O
by	O
n	O
xt	O
the	O
ance	O
matrix	O
can	O
then	O
be	O
written	O
as	O
s	O
n-	O
and	O
the	O
corresponding	O
eigenvector	O
equation	O
becomes	O
t	O
xui	O
aiui	O
n	O
now	O
pre-multiply	O
both	O
sides	O
by	O
x	O
to	O
give	O
nxx	O
aixui	O
t	O
if	O
we	O
now	O
define	O
vi	O
xui	O
we	O
obtain	O
t	O
vi	O
aivi	O
n	O
which	O
is	O
an	O
eigenvector	O
equation	O
for	O
the	O
n	O
x	O
n	O
matrix	O
n-	O
we	O
see	O
that	O
this	O
has	O
the	O
same	O
n	O
eigenvalues	O
as	O
the	O
original	O
covariance	B
matrix	O
itself	O
has	O
an	O
additional	O
d	O
n	O
eigenvalues	O
of	O
value	O
zero	O
thus	O
we	O
can	O
solve	O
the	O
eigenvector	O
problem	O
in	O
spaces	O
of	O
lower	O
dimensionality	O
with	O
computational	O
cost	O
instead	O
of	O
od	O
in	O
order	O
to	O
determine	O
the	O
eigenvectors	O
we	O
multiply	O
both	O
sides	O
of	O
by	O
x	O
t	O
to	O
give	O
t	O
nx	O
x	O
vi	O
aix	O
vi	O
t	O
t	O
from	O
which	O
we	O
see	O
that	O
is	O
an	O
eigenvector	O
of	O
s	O
with	O
eigenvalue	O
ai	O
note	O
however	O
that	O
these	O
eigenvectors	O
need	O
not	O
be	O
normalized	O
to	O
determine	O
the	O
ate	O
normalization	O
we	O
re-scale	O
ui	O
ex	O
x	O
tvi	O
by	O
a	O
constant	O
such	O
that	O
ilui	O
ii	O
which	O
assuming	O
vi	O
has	O
been	O
normalized	O
to	O
unit	O
length	O
gives	O
ui	O
x	O
vi	O
t	O
in	O
summary	O
to	O
apply	O
this	O
approach	O
we	O
first	O
evaluate	O
xxt	O
and	O
then	O
find	O
its	O
vectors	O
and	O
eigenvalues	O
and	O
then	O
compute	O
the	O
eigenvectors	O
in	O
the	O
original	O
data	O
space	O
using	O
probabilistic	O
pea	O
the	O
formulation	O
of	O
pca	O
discussed	O
in	O
the	O
previous	O
section	O
was	O
based	O
on	O
a	O
linear	O
projection	O
of	O
the	O
data	O
onto	O
a	O
subspace	O
of	O
lower	O
dimensionality	O
than	O
the	O
original	O
data	O
space	O
we	O
now	O
show	O
that	O
pca	O
can	O
also	O
be	O
expressed	O
as	O
the	O
maximum	B
likelihood	I
solution	O
of	O
a	O
probabilistic	O
latent	B
variable	I
model	O
this	O
reformulation	O
of	O
pca	O
known	O
as	O
probabilistic	O
pea	O
brings	O
several	O
advantages	O
compared	O
with	O
conventional	O
pca	O
probabilistic	B
pca	I
represents	O
a	O
constrained	O
form	O
of	O
the	O
gaussian	B
distribution	O
in	O
which	O
the	O
number	O
of	O
free	O
parameters	O
can	O
be	O
restricted	O
while	O
still	O
allowing	O
the	O
model	O
to	O
capture	O
the	O
dominant	O
correlations	O
in	O
a	O
data	O
set	O
section	O
probabilistic	O
pea	O
we	O
can	O
derive	O
an	O
em	B
algorithm	I
for	O
pca	O
that	O
is	O
computationally	O
efficient	O
in	O
situations	O
where	O
only	O
a	O
few	O
leading	O
eigenvectors	O
are	O
required	O
and	O
that	O
avoids	O
having	O
to	O
evaluate	O
the	O
data	O
covariance	B
matrix	O
as	O
an	O
intermediate	O
step	O
the	O
combination	O
of	O
a	O
probabilistic	O
model	O
and	O
em	B
allows	O
us	O
to	O
deal	O
with	O
ing	O
values	O
in	O
the	O
data	O
set	O
mixtures	O
of	O
probabilistic	B
pca	I
models	O
can	O
be	O
formulated	O
in	O
a	O
principled	O
way	O
and	O
trained	O
using	O
the	O
em	B
algorithm	I
section	O
probabilistic	B
pca	I
forms	O
the	O
basis	O
for	O
a	O
bayesian	B
treatment	O
of	O
pca	O
in	O
which	O
the	O
dimensionality	O
of	O
the	O
principal	B
subspace	I
can	O
be	O
found	O
automatically	O
from	O
the	O
data	O
the	O
existence	O
of	O
a	O
likelihood	B
function	I
allows	O
direct	O
comparison	O
with	O
other	O
probabilistic	O
density	B
models	O
by	O
contrast	O
conventional	O
pca	O
will	O
assign	O
a	O
low	O
reconstruction	O
cost	O
to	O
data	O
points	O
that	O
are	O
close	O
to	O
the	O
principal	B
subspace	I
even	O
if	O
they	O
lie	O
arbitrarily	O
far	O
from	O
the	O
training	B
data	O
probabilistic	B
pca	I
can	O
be	O
used	O
to	O
model	O
class-conditional	O
densities	O
and	O
hence	O
be	O
applied	O
to	O
classification	B
problems	O
the	O
probabilistic	B
pca	I
model	O
can	O
be	O
run	O
generatively	O
to	O
provide	O
samples	O
from	O
the	O
distribution	O
this	O
formulation	O
of	O
pca	O
as	O
a	O
probabilistic	O
model	O
was	O
proposed	O
independently	O
by	O
tipping	O
and	O
bishop	O
and	O
by	O
roweis	O
as	O
we	O
shall	O
see	O
later	O
it	O
is	O
closely	O
related	O
to	O
factor	B
analysis	I
probabilistic	B
pca	I
is	O
a	O
simple	O
example	O
of	O
the	O
linear-gaussian	O
framework	O
in	O
which	O
all	O
of	O
the	O
marginal	B
and	O
conditional	B
distributions	O
are	O
gaussian	B
we	O
can	O
late	O
probabilistic	B
pca	I
by	O
first	O
introducing	O
an	O
explicit	O
latent	B
variable	I
z	O
corresponding	O
to	O
the	O
principal-component	O
subspace	O
next	O
we	O
define	O
a	O
gaussian	B
prior	B
distribution	O
pz	O
over	O
the	O
latent	B
variable	I
together	O
with	O
a	O
gaussian	B
conditional	B
distribution	O
pxl	O
z	O
for	O
the	O
observed	B
variable	I
x	O
conditioned	O
on	O
the	O
value	O
of	O
the	O
latent	B
variable	I
cally	O
the	O
prior	B
distribution	O
over	O
z	O
is	O
given	O
by	O
a	O
zero-mean	O
unit-covariance	O
gaussian	B
pz	O
nzio	O
i	O
similarly	O
the	O
conditional	B
distribution	O
of	O
the	O
observed	B
variable	I
x	O
conditioned	O
on	O
the	O
value	O
of	O
the	O
latent	B
variable	I
z	O
is	O
again	O
gaussian	B
of	O
the	O
form	O
pxlz	O
nxlwz	O
j-l	O
a	O
in	O
which	O
the	O
mean	B
of	O
x	O
is	O
a	O
general	O
linear	O
function	O
of	O
z	O
governed	O
by	O
the	O
d	O
x	O
m	O
matrix	O
wand	O
the	O
d-dimensional	O
vector	O
j-l	O
note	O
that	O
this	O
factorizes	O
with	O
respect	O
to	O
the	O
elements	O
of	O
x	O
in	O
other	O
words	O
this	O
is	O
an	O
example	O
of	O
the	O
naive	B
bayes	B
model	I
as	O
we	O
shall	O
see	O
shortly	O
the	O
columns	O
of	O
w	O
span	O
a	O
linear	O
subspace	O
within	O
the	O
data	O
space	O
that	O
corresponds	O
to	O
the	O
principal	B
subspace	I
the	O
other	O
parameter	O
in	O
this	O
model	O
is	O
the	O
scalar	O
a	O
governing	O
the	O
variance	B
of	O
the	O
conditional	B
distribution	O
note	O
that	O
there	O
is	O
no	O
section	O
section	O
continuous	O
lat	O
nt	O
flgu	O
in	O
oilte	O
iifative	O
viw	O
potabi	O
st	O
pea	O
modeifof	O
two-dimensiooal	O
space	O
and	O
a	O
space	O
an	O
oberved	O
point	O
x	O
is	O
generated	O
by	O
first	O
drawing	O
a	O
value	O
i	O
fof	O
vafiatlle	O
prior	B
distt	O
p	O
and	O
itlen	O
drawing	O
a	O
val	O
fof	O
x	O
lrom	O
an	O
isofopk	O
gaussian	B
distrt	O
by	O
the	O
red	O
ciries	O
having	O
mean	B
wi	O
and	O
the	O
lfer	O
ellips	O
show	O
l	O
le	O
density	B
the	O
pix	O
loss	O
of	O
gerajity	O
in	O
assuming	O
a	O
zero	O
mean	B
unit	O
coariance	O
gauian	O
for	O
the	O
latent	O
distributin	O
iiz	O
because	O
a	O
more	O
gcneral	O
diributin	O
would	O
gie	O
rise	O
to	O
an	O
equivalent	O
probabiliic	O
nodel	O
we	O
can	O
view	O
the	O
probabilistic	O
pea	O
model	O
from	O
a	O
geoeratie	O
in	O
a	O
sampled	O
of	O
the	O
obyed	O
is	O
obiained	O
by	O
first	O
chooing	O
a	O
for	O
the	O
latent	O
aod	O
then	O
the	O
ooej	O
cooditioned	O
on	O
this	O
lao	O
tent	O
specifically	O
the	O
v-dimenional	O
ooed	O
x	O
is	O
defined	O
by	O
a	O
lin	O
ea	O
tranformati	O
of	O
the	O
dimeninal	O
latcnt	O
z	O
plu	O
additi-e	O
gaussian	B
that	O
w	O
ere	O
z	O
is	O
an	O
m-dinsional	O
gaussian	B
lalent	O
variable	O
and	O
is	O
a	O
v	O
dimensinal	O
gau	O
ian-distributed	O
noi	O
witb	O
co-ariance	O
this	O
generative	O
process	O
is	O
illustrated	O
in	O
figure	O
noie	O
that	O
this	O
frame	O
-orl	O
is	O
based	O
on	O
a	O
mapping	O
from	O
latent	O
data	O
space	O
in	O
contrast	O
the	O
nll	O
ica	O
dis	O
cusd	O
alxe	O
mapping	O
from	O
data	O
space	O
to	O
the	O
latent	O
space	O
he	O
oolained	O
using	O
ha	O
ycs	O
lhwnm	O
suf	O
llosc	O
we	O
wish	O
detenine	O
the	O
ofllo	O
parameters	O
i	O
and	O
using	O
maximum	O
likelihuol	O
to	O
write	O
lhe	O
likeliltood	O
function	O
we	O
need	O
an	O
for	O
tlo	O
marginal	B
distributioo	O
p	O
of	O
tlo	O
this	O
is	O
exprt	O
sed	O
fmn	O
the	O
sum	O
aod	O
poduct	O
rules	O
in	O
the	O
form	O
ee-ise	O
llaus	O
this	O
corresponds	O
to	O
a	O
linear	O
gauin	O
thi	O
marginal	B
ditribulion	O
is	O
again	O
gaussian	B
atld	O
is	O
given	O
by	O
nxllfc	O
probabilistic	O
pea	O
where	O
the	O
d	O
x	O
d	O
covariance	B
matrix	O
c	O
is	O
defined	O
by	O
c	O
wwt	O
this	O
result	O
can	O
also	O
be	O
derived	O
more	O
directly	O
by	O
noting	O
that	O
the	O
predictive	B
distribution	I
will	O
be	O
gaussian	B
and	O
then	O
evaluating	O
its	O
mean	B
and	O
covariance	B
using	O
this	O
gives	O
iex	O
covx	O
iewz	O
jl	O
e	O
jl	O
ie	O
ewz	O
et	O
ie	O
ieeet	O
wwt	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
z	O
and	O
e	O
are	O
independent	B
random	O
variables	O
and	O
hence	O
are	O
uncorrelated	O
intuitively	O
we	O
can	O
think	O
of	O
the	O
distribution	O
px	O
as	O
being	O
defined	O
by	O
taking	O
an	O
isotropic	B
gaussian	B
can	O
and	O
moving	O
it	O
across	O
the	O
principal	B
subspace	I
spraying	O
and	O
weighted	O
by	O
the	O
prior	B
distribution	O
gaussian	B
ink	O
with	O
density	B
determined	O
by	O
accumulated	O
ink	O
density	B
gives	O
rise	O
to	O
a	O
shaped	O
distribution	O
ing	O
the	O
marginal	B
density	B
px	O
the	O
predictive	B
distribution	I
px	O
is	O
governed	O
by	O
the	O
parameters	O
jl	O
w	O
and	O
however	O
there	O
is	O
redundancy	O
in	O
this	O
parameterization	O
corresponding	O
to	O
rotations	O
of	O
the	O
latent	O
space	O
coordinates	O
to	O
see	O
this	O
consider	O
a	O
matrix	O
w	O
wr	O
where	O
r	O
is	O
an	O
orthogonal	O
matrix	O
using	O
the	O
orthogonality	O
property	O
rrt	O
i	O
we	O
see	O
that	O
the	O
quantity	O
wwt	O
that	O
appears	O
in	O
the	O
covariance	B
matrix	O
c	O
takes	O
the	O
form	O
and	O
hence	O
is	O
independent	B
of	O
r	O
thus	O
there	O
is	O
a	O
whole	O
family	O
of	O
matrices	O
w	O
all	O
of	O
which	O
give	O
rise	O
to	O
the	O
same	O
predictive	B
distribution	I
this	O
invariance	B
can	O
be	O
understood	O
in	O
terms	O
of	O
rotations	O
within	O
the	O
latent	O
space	O
we	O
shall	O
return	O
to	O
a	O
discussion	O
of	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
this	O
model	O
later	O
when	O
we	O
evaluate	O
the	O
predictive	B
distribution	I
we	O
require	O
c-	O
which	O
involves	O
the	O
inversion	O
of	O
a	O
d	O
x	O
d	O
matrix	O
the	O
computation	O
required	O
to	O
do	O
this	O
can	O
be	O
reduced	O
by	O
making	O
use	O
of	O
the	O
matrix	O
inversion	O
identity	O
to	O
give	O
c-	O
t	O
where	O
the	O
m	O
x	O
m	O
matrix	O
m	O
is	O
defined	O
by	O
m	O
wtw	O
because	O
we	O
invert	O
m	O
rather	O
than	O
inverting	O
c	O
directly	O
the	O
cost	O
of	O
evaluating	O
c-	O
is	O
reduced	O
from	O
to	O
as	O
well	O
as	O
the	O
predictive	B
distribution	I
px	O
we	O
will	O
also	O
require	O
the	O
posterior	O
distributionpzlx	O
which	O
can	O
again	O
be	O
written	O
down	O
directly	O
using	O
the	O
result	O
for	O
linear-gaussian	O
models	O
to	O
give	O
note	O
that	O
the	O
posterior	O
mean	B
depends	O
on	O
x	O
whereas	O
the	O
posterior	O
covariance	B
is	O
dependent	O
of	O
x	O
exercise	O
continuous	O
latent	O
variables	O
figure	O
the	O
probabilistic	O
pea	O
model	O
for	O
a	O
data	O
set	O
of	O
n	O
vations	O
of	O
x	O
can	O
be	O
expressed	O
as	O
a	O
directed	B
graph	O
in	O
which	O
each	O
observation	O
x	O
n	O
is	O
associated	O
with	O
a	O
value	O
zn	O
of	O
the	O
latent	B
variable	I
n	O
maximum	B
likelihood	I
pea	O
we	O
next	O
consider	O
the	O
determination	O
of	O
the	O
model	O
parameters	O
using	O
maximum	B
likelihood	I
given	O
a	O
data	O
set	O
x	O
of	O
observed	O
data	O
points	O
the	O
probabilistic	O
pea	O
model	O
can	O
be	O
expressed	O
as	O
a	O
directed	B
graph	O
as	O
shown	O
in	O
figure	O
the	O
corresponding	O
log	O
likelihood	B
function	I
is	O
given	O
from	O
by	O
inpxijl	O
wo	O
n	O
l	O
ln	O
pxn	O
nl	O
n	O
n	O
nd	O
ln	O
ie	O
l	O
xn	O
jl	O
c-	O
jl	O
t	O
nl	O
setting	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	O
with	O
respect	O
to	O
jl	O
equal	O
to	O
zero	O
gives	O
the	O
expected	O
result	O
jl	O
x	O
where	O
x	O
is	O
the	O
data	O
mean	B
defined	O
by	O
back-substituting	O
we	O
can	O
then	O
write	O
the	O
log	O
likelihood	B
function	I
in	O
the	O
form	O
inpxiw	O
jl	O
in	O
ie	O
tr	O
n	O
where	O
s	O
is	O
the	O
data	O
covariance	B
matrix	O
defined	O
by	O
because	O
the	O
log	O
likelihood	O
is	O
a	O
quadratic	O
function	O
of	O
jl	O
this	O
solution	O
represents	O
the	O
unique	O
maximum	O
as	O
can	O
be	O
confirmed	O
by	O
computing	O
second	O
derivatives	O
maximization	O
with	O
respect	O
to	O
w	O
and	O
is	O
more	O
complex	O
but	O
nonetheless	O
has	O
an	O
exact	O
closed-form	O
solution	O
it	O
was	O
shown	O
by	O
tipping	O
and	O
bishop	O
that	O
all	O
of	O
the	O
stationary	B
points	O
of	O
the	O
log	O
likelihood	B
function	I
can	O
be	O
written	O
as	O
where	O
u	O
m	O
is	O
a	O
d	O
x	O
m	O
matrix	O
whose	O
columns	O
are	O
given	O
by	O
any	O
subset	O
size	O
m	O
of	O
the	O
eigenvectors	O
of	O
the	O
data	O
covariance	B
matrix	O
s	O
the	O
m	O
x	O
m	O
diagonal	B
matrix	O
l	O
m	O
has	O
elements	O
given	O
by	O
the	O
corresponding	O
eigenvalues	O
and	O
r	O
is	O
an	O
arbitrary	O
m	O
x	O
m	O
orthogonal	O
matrix	O
furthermore	O
tipping	O
and	O
bishop	O
showed	O
that	O
the	O
maximum	O
of	O
the	O
lihood	O
function	O
is	O
obtained	O
when	O
the	O
m	O
eigenvectors	O
are	O
chosen	O
to	O
be	O
those	O
whose	O
eigenvalues	O
are	O
the	O
m	O
largest	O
other	O
solutions	O
being	O
saddle	O
points	O
a	O
similar	O
sult	O
was	O
conjectured	O
independently	O
by	O
roweis	O
although	O
no	O
proof	O
was	O
given	O
probabilistic	O
pea	O
again	O
we	O
shall	O
assume	O
that	O
the	O
eigenvectors	O
have	O
been	O
arranged	O
in	O
order	O
of	O
ing	O
values	O
of	O
the	O
corresponding	O
eigenvalues	O
so	O
that	O
the	O
m	O
principal	O
eigenvectors	O
are	O
ul	O
um	O
in	O
this	O
case	O
the	O
columns	O
of	O
w	O
define	O
the	O
principal	B
subspace	I
of	O
dard	O
pca	O
the	O
corresponding	O
maximum	B
likelihood	I
solution	O
for	O
is	O
then	O
given	O
by	O
d-m	O
l	O
ai	O
d	O
iml	O
so	O
that	O
is	O
the	O
average	O
variance	B
associated	O
with	O
the	O
discarded	O
dimensions	O
because	O
r	O
is	O
orthogonal	O
it	O
can	O
be	O
interpreted	O
as	O
a	O
rotation	O
matrix	O
in	O
the	O
m	O
x	O
m	O
latent	O
space	O
if	O
we	O
substitute	O
the	O
solution	O
for	O
w	O
into	O
the	O
expression	O
for	O
c	O
and	O
make	O
use	O
of	O
the	O
orthogonality	O
property	O
rrt	O
i	O
we	O
see	O
that	O
c	O
is	O
independent	B
of	O
r	O
this	O
simply	O
says	O
that	O
the	O
predictive	O
density	B
is	O
unchanged	O
by	O
rotations	O
in	O
the	O
latent	O
space	O
as	O
discussed	O
earlier	O
for	O
the	O
particular	O
case	O
of	O
r	O
i	O
we	O
see	O
that	O
the	O
columns	O
of	O
w	O
are	O
the	O
principal	O
component	O
eigenvectors	O
scaled	O
by	O
the	O
variance	B
parameters	O
ai	O
the	O
interpretation	O
of	O
these	O
scaling	O
factors	O
is	O
clear	O
once	O
we	O
recognize	O
that	O
for	O
a	O
convolution	O
of	O
independent	B
gaussian	B
distributions	O
this	O
case	O
the	O
latent	O
space	O
distribution	O
and	O
the	O
noise	O
model	O
the	O
variances	O
are	O
additive	O
thus	O
the	O
variance	B
ai	O
in	O
the	O
direction	O
of	O
an	O
eigenvector	O
ui	O
is	O
composed	O
of	O
the	O
sum	O
of	O
a	O
contribution	O
ai	O
from	O
the	O
projection	O
of	O
the	O
unit-variance	O
latent	O
space	O
distribution	O
into	O
data	O
space	O
through	O
the	O
corresponding	O
column	O
of	O
w	O
plus	O
an	O
isotropic	B
contribution	O
of	O
variance	B
which	O
is	O
added	O
in	O
all	O
directions	O
by	O
the	O
noise	O
model	O
it	O
is	O
worth	O
taking	O
a	O
moment	O
to	O
study	O
the	O
form	O
of	O
the	O
covariance	B
matrix	O
given	O
by	O
consider	O
the	O
variance	B
of	O
the	O
predictive	B
distribution	I
along	O
some	O
direction	O
specified	O
by	O
the	O
unit	O
vector	O
v	O
where	O
vtv	O
which	O
is	O
given	O
by	O
vtcv	O
first	O
suppose	O
that	O
v	O
is	O
orthogonal	O
to	O
the	O
principal	B
subspace	I
in	O
other	O
words	O
it	O
is	O
given	O
by	O
some	O
linear	O
combination	O
of	O
the	O
discarded	O
eigenvectors	O
then	O
v	O
tv	O
and	O
hence	O
v	O
tcv	O
thus	O
the	O
model	O
predicts	O
a	O
noise	O
variance	B
orthogonal	O
to	O
the	O
principal	B
subspace	I
which	O
from	O
is	O
just	O
the	O
average	O
of	O
the	O
discarded	O
eigenvalues	O
now	O
suppose	O
that	O
v	O
ui	O
where	O
ui	O
is	O
one	O
of	O
the	O
retained	O
eigenvectors	O
defining	O
the	O
cipal	O
subspace	O
then	O
vtcv	O
ai	O
in	O
other	O
words	O
this	O
model	O
correctly	O
captures	O
the	O
variance	B
of	O
the	O
data	O
along	O
the	O
principal	O
axes	O
and	O
approximates	O
the	O
variance	B
in	O
all	O
remaining	O
directions	O
with	O
a	O
single	O
average	O
value	O
one	O
way	O
to	O
construct	O
the	O
maximum	B
likelihood	I
density	B
model	O
would	O
simply	O
be	O
to	O
find	O
the	O
eigenvectors	O
and	O
eigenvalues	O
of	O
the	O
data	O
covariance	B
matrix	O
and	O
then	O
to	O
evaluate	O
wand	O
using	O
the	O
results	O
given	O
above	O
in	O
this	O
case	O
we	O
would	O
choose	O
r	O
i	O
for	O
convenience	O
however	O
if	O
the	O
maximum	B
likelihood	I
solution	O
is	O
found	O
by	O
numerical	O
optimization	O
of	O
the	O
likelihood	B
function	I
for	O
instance	O
using	O
an	O
algorithm	O
such	O
as	O
conjugate	B
gradients	O
nocedal	O
and	O
wright	O
bishop	O
and	O
nabney	O
or	O
through	O
the	O
em	B
algorithm	I
then	O
the	O
resulting	O
value	O
of	O
r	O
is	O
sentially	O
arbitrary	O
this	O
implies	O
that	O
the	O
columns	O
of	O
w	O
need	O
not	O
be	O
orthogonal	O
if	O
an	O
orthogonal	O
basis	O
is	O
required	O
the	O
matrix	O
w	O
can	O
be	O
post-processed	O
appropriately	O
and	O
van	O
loan	O
alternatively	O
the	O
em	B
algorithm	I
can	O
be	O
modified	O
in	O
such	O
a	O
way	O
as	O
to	O
yield	O
orthonormal	O
principal	O
directions	O
sorted	O
in	O
descending	O
order	O
of	O
the	O
corresponding	O
eigenvalues	O
directly	O
and	O
oh	O
section	O
continuous	O
latent	O
variables	O
the	O
rotational	O
invariance	B
in	O
latent	O
space	O
represents	O
a	O
form	O
of	O
statistical	O
tifiability	O
analogous	O
to	O
that	O
encountered	O
for	O
mixture	B
models	O
in	O
the	O
case	O
of	O
discrete	O
latent	O
variables	O
here	O
there	O
is	O
a	O
continuum	O
of	O
parameters	O
all	O
of	O
which	O
lead	O
to	O
the	O
same	O
predictive	O
density	B
in	O
contrast	O
to	O
the	O
discrete	O
nonidentifiability	B
associated	O
with	O
component	O
re-labelling	O
in	O
the	O
mixture	B
setting	O
if	O
we	O
consider	O
the	O
case	O
of	O
m	O
d	O
so	O
that	O
there	O
is	O
no	O
reduction	O
of	O
ality	O
then	O
u	O
m	O
u	O
and	O
l	O
m	O
l	O
making	O
use	O
of	O
the	O
orthogonality	O
properties	O
uut	O
i	O
and	O
rrt	O
i	O
we	O
see	O
that	O
the	O
covariance	B
c	O
of	O
the	O
marginal	B
distribution	O
for	O
x	O
becomes	O
and	O
so	O
we	O
obtain	O
the	O
standard	O
maximum	B
likelihood	I
solution	O
for	O
an	O
unconstrained	O
gaussian	B
distribution	O
in	O
which	O
the	O
covariance	B
matrix	O
is	O
given	O
by	O
the	O
sample	O
ance	O
conventional	O
pca	O
is	O
generally	O
formulated	O
as	O
a	O
projection	O
of	O
points	O
from	O
the	O
dimensional	O
data	O
space	O
onto	O
an	O
m	O
linear	O
subspace	O
probabilistic	B
pca	I
however	O
is	O
most	O
naturally	O
expressed	O
as	O
a	O
mapping	O
from	O
the	O
latent	O
space	O
into	O
the	O
data	O
space	O
via	O
for	O
applications	O
such	O
as	O
visualization	B
and	O
data	B
compression	I
we	O
can	O
reverse	O
this	O
mapping	O
using	O
bayes	B
theorem	O
any	O
point	O
x	O
in	O
data	O
space	O
can	O
then	O
be	O
summarized	O
by	O
its	O
posterior	O
mean	B
and	O
covariance	B
in	O
latent	O
space	O
from	O
the	O
mean	B
is	O
given	O
by	O
where	O
m	O
is	O
given	O
by	O
this	O
projects	O
to	O
a	O
point	O
in	O
data	O
space	O
given	O
by	O
wlezlx	O
j-l	O
section	O
note	O
that	O
this	O
takes	O
the	O
same	O
form	O
as	O
the	O
equations	O
for	O
regularized	O
linear	B
regression	B
and	O
is	O
a	O
consequence	O
of	O
maximizing	O
the	O
likelihood	B
function	I
for	O
a	O
linear	O
gaussian	B
model	O
similarly	O
the	O
posterior	O
covariance	B
is	O
given	O
from	O
by	O
and	O
is	O
independent	B
of	O
x	O
if	O
we	O
take	O
the	O
limit	O
then	O
the	O
posterior	O
mean	B
reduces	O
to	O
exercise	O
exercise	O
section	O
which	O
represents	O
an	O
orthogonal	O
projection	O
of	O
the	O
data	O
point	O
onto	O
the	O
latent	O
space	O
and	O
so	O
we	O
recover	O
the	O
standard	O
pca	O
model	O
the	O
posterior	O
covariance	B
in	O
this	O
limit	O
is	O
the	O
latent	O
projection	O
zero	O
however	O
and	O
the	O
density	B
becomes	O
singular	O
for	O
shifted	O
towards	O
the	O
origin	O
relative	B
to	O
the	O
orthogonal	O
projection	O
finally	O
we	O
note	O
that	O
an	O
important	O
role	O
for	O
the	O
probabilistic	B
pca	I
model	O
is	O
in	O
defining	O
a	O
multivariate	O
gaussian	B
distribution	O
in	O
which	O
the	O
number	O
of	O
degrees	O
of	O
dom	O
in	O
other	O
words	O
the	O
number	O
of	O
independent	B
parameters	O
can	O
be	O
controlled	O
whilst	O
still	O
allowing	O
the	O
model	O
to	O
capture	O
the	O
dominant	O
correlations	O
in	O
the	O
data	O
recall	O
that	O
a	O
general	O
gaussian	B
distribution	O
has	O
dd	O
independent	B
parameters	O
in	O
its	O
covariance	B
matrix	O
another	O
d	O
parameters	O
in	O
its	O
mean	B
thus	O
the	O
number	O
of	O
parameters	O
scales	O
quadratically	O
with	O
d	O
and	O
can	O
become	O
excessive	O
in	O
spaces	O
of	O
high	O
probabilistic	O
pea	O
dimensionality	O
if	O
we	O
restrict	O
the	O
covariance	B
matrix	O
to	O
be	O
diagonal	B
then	O
it	O
has	O
only	O
d	O
independent	B
parameters	O
and	O
so	O
the	O
number	O
of	O
parameters	O
now	O
grows	O
linearly	O
with	O
dimensionality	O
however	O
it	O
now	O
treats	O
the	O
variables	O
as	O
if	O
they	O
were	O
independent	B
and	O
hence	O
can	O
no	O
longer	O
express	O
any	O
correlations	O
between	O
them	O
probabilistic	O
pea	O
vides	O
an	O
elegant	O
compromise	O
in	O
which	O
the	O
m	O
most	O
significant	O
correlations	O
can	O
be	O
captured	O
while	O
still	O
ensuring	O
that	O
the	O
total	O
number	O
of	O
parameters	O
grows	O
only	O
linearly	O
with	O
d	O
we	O
can	O
see	O
this	O
by	O
evaluating	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
ppca	O
model	O
as	O
follows	O
the	O
covariance	B
matrix	O
c	O
depends	O
on	O
the	O
parameters	O
w	O
giving	O
a	O
total	O
parameter	O
count	O
of	O
dm	O
however	O
which	O
has	O
size	O
d	O
x	O
m	O
and	O
a	O
we	O
have	O
seen	O
that	O
there	O
is	O
some	O
redundancy	O
in	O
this	O
parameterization	O
associated	O
with	O
rotations	O
of	O
the	O
coordinate	O
system	O
in	O
the	O
latent	O
space	O
the	O
orthogonal	O
matrix	O
r	O
that	O
expresses	O
these	O
rotations	O
has	O
size	O
m	O
x	O
m	O
in	O
the	O
first	O
column	O
of	O
this	O
matrix	O
there	O
are	O
m	O
independent	B
parameters	O
because	O
the	O
column	O
vector	O
must	O
be	O
normalized	O
to	O
unit	O
length	O
in	O
the	O
second	O
column	O
there	O
are	O
m	O
independent	B
parameters	O
because	O
the	O
column	O
must	O
be	O
normalized	O
and	O
also	O
must	O
be	O
orthogonal	O
to	O
the	O
previous	O
column	O
and	O
so	O
on	O
summing	O
this	O
arithmetic	O
series	O
we	O
see	O
that	O
r	O
has	O
a	O
total	O
of	O
mm	O
independent	B
parameters	O
thus	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
covariance	B
matrix	O
c	O
is	O
given	O
by	O
dm	O
mm	O
exercise	O
section	O
section	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
this	O
model	O
therefore	O
only	O
grows	O
linearly	O
with	O
d	O
for	O
fixed	O
m	O
if	O
we	O
take	O
m	O
d	O
then	O
we	O
recover	O
the	O
standard	O
result	O
for	O
a	O
full	O
covariance	B
gaussian	B
in	O
this	O
case	O
the	O
variance	B
along	O
d	O
linearly	O
dependent	O
directions	O
is	O
controlled	O
by	O
the	O
columns	O
of	O
w	O
and	O
the	O
variance	B
along	O
the	O
remaining	O
direction	O
is	O
given	O
by	O
a	O
if	O
m	O
the	O
model	O
is	O
equivalent	O
to	O
the	O
isotropic	B
covariance	B
case	O
em	B
algorithm	I
for	O
pea	O
as	O
we	O
have	O
seen	O
the	O
probabilistic	B
pca	I
model	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
a	O
marginalization	O
over	O
a	O
continuous	O
latent	O
space	O
z	O
in	O
which	O
for	O
each	O
data	O
point	O
x	O
n	O
there	O
is	O
a	O
corresponding	O
latent	B
variable	I
zn	O
we	O
can	O
therefore	O
make	O
use	O
of	O
the	O
em	B
algorithm	I
to	O
find	O
maximum	B
likelihood	I
estimates	O
of	O
the	O
model	O
parameters	O
this	O
may	O
seem	O
rather	O
pointless	O
because	O
we	O
have	O
already	O
obtained	O
an	O
exact	O
closed-form	O
lution	O
for	O
the	O
maximum	B
likelihood	I
parameter	O
values	O
however	O
in	O
spaces	O
of	O
high	O
dimensionality	O
there	O
may	O
be	O
computational	O
advantages	O
in	O
using	O
an	O
iterative	O
em	B
procedure	O
rather	O
than	O
working	O
directly	O
with	O
the	O
sample	O
covariance	B
matrix	O
this	O
em	B
procedure	O
can	O
also	O
be	O
extended	B
to	O
the	O
factor	B
analysis	I
model	O
for	O
which	O
there	O
is	O
no	O
closed-form	O
solution	O
finally	O
it	O
allows	O
missing	B
data	I
to	O
be	O
handled	O
in	O
a	O
principled	O
way	O
we	O
can	O
derive	O
the	O
em	B
algorithm	I
for	O
probabilistic	B
pca	I
by	O
following	O
the	O
general	O
framework	O
for	O
em	B
thus	O
we	O
write	O
down	O
the	O
complete-data	O
log	O
likelihood	O
and	O
take	O
its	O
expectation	B
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
distribution	O
evaluated	O
using	O
parameter	O
values	O
maximization	O
of	O
this	O
expected	O
data	O
log	O
likelihood	O
then	O
yields	O
the	O
parameter	O
values	O
because	O
the	O
data	O
points	O
continuous	O
latent	O
variables	O
are	O
assumed	O
independent	B
the	O
complete-data	O
log	O
likelihood	B
function	I
takes	O
the	O
form	O
inp	O
zijl	O
w	O
l	O
lnpzn	O
n	O
nl	O
where	O
the	O
nth	O
row	O
of	O
the	O
matrix	O
z	O
is	O
given	O
by	O
zn	O
we	O
already	O
know	O
that	O
the	O
exact	O
maximum	B
likelihood	I
solution	O
for	O
jl	O
is	O
given	O
by	O
the	O
sample	B
mean	B
x	O
defined	O
by	O
and	O
it	O
is	O
convenient	O
to	O
substitute	O
for	O
jl	O
at	O
this	O
stage	O
making	O
use	O
of	O
the	O
expressions	O
and	O
for	O
the	O
latent	O
and	O
conditional	B
distributions	O
respectively	O
and	O
ing	O
the	O
expectation	B
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
over	O
the	O
latent	O
variables	O
we	O
obtain	O
note	O
that	O
this	O
depends	O
on	O
the	O
posterior	O
distribution	O
only	O
through	O
the	O
sufficient	O
tics	O
of	O
the	O
gaussian	B
thus	O
in	O
the	O
e	O
step	O
we	O
use	O
the	O
old	O
parameter	O
values	O
to	O
evaluate	O
x	O
leznleznt	O
which	O
follow	O
directly	O
from	O
the	O
posterior	O
distribution	O
together	O
with	O
the	O
dard	O
result	O
leznz	O
covzn	O
jeznjeznt	O
here	O
m	O
is	O
defined	O
by	O
in	O
the	O
m	O
step	O
we	O
maximize	O
with	O
respect	O
to	O
wand	O
keeping	O
the	O
posterior	O
statistics	O
fixed	O
maximization	O
with	O
respect	O
to	O
is	O
straightforward	O
for	O
the	O
mization	O
with	O
respect	O
to	O
w	O
we	O
make	O
use	O
of	O
and	O
obtain	O
the	O
m-step	O
equations	O
exercise	O
w	O
new	O
nd	O
l	O
new	O
nl	O
x	O
n	O
the	O
em	B
algorithm	I
for	O
probabilistic	B
pca	I
proceeds	O
by	O
initializing	O
the	O
parameters	O
and	O
then	O
alternately	O
computing	O
the	O
sufficient	B
statistics	I
of	O
the	O
latent	O
space	O
posterior	O
distribution	O
using	O
and	O
in	O
the	O
e	O
step	O
and	O
revising	O
the	O
parameter	O
values	O
using	O
and	O
in	O
the	O
m	O
step	O
one	O
of	O
the	O
benefits	O
of	O
the	O
em	B
algorithm	I
for	O
pca	O
is	O
computational	O
efficiency	O
for	O
large-scale	O
applications	O
unlike	O
conventional	O
pca	O
based	O
on	O
an	O
probabilistic	O
pea	O
eigenvector	O
decomposition	O
of	O
the	O
sample	O
covariance	B
matrix	O
the	O
em	B
approach	O
is	O
iterative	O
and	O
so	O
might	O
appear	O
to	O
be	O
less	O
attractive	O
however	O
each	O
cycle	O
of	O
the	O
em	B
algorithm	I
can	O
be	O
computationally	O
much	O
more	O
efficient	O
than	O
conventional	O
pca	O
in	O
spaces	O
of	O
high	O
dimensionality	O
to	O
see	O
this	O
we	O
note	O
that	O
the	O
eigendecomposition	O
of	O
the	O
covariance	B
matrix	O
requires	O
od	O
computation	O
often	O
we	O
are	O
interested	O
only	O
in	O
the	O
first	O
m	O
eigenvectors	O
and	O
their	O
corresponding	O
eigenvalues	O
in	O
which	O
case	O
we	O
can	O
use	O
algorithms	O
that	O
are	O
however	O
the	O
evaluation	O
of	O
the	O
covariance	B
matrix	O
itself	O
takes	O
computations	O
where	O
n	O
is	O
the	O
number	O
of	O
data	O
points	O
algorithms	O
such	O
as	O
the	O
snapshot	O
method	O
which	O
assume	O
that	O
the	O
eigenvectors	O
are	O
linear	O
combinations	O
of	O
the	O
data	O
vectors	O
avoid	O
direct	O
evaluation	O
of	O
the	O
covariance	B
matrix	O
but	O
are	O
and	O
hence	O
unsuited	O
to	O
large	O
data	O
sets	O
the	O
em	B
algorithm	I
described	O
here	O
also	O
does	O
not	O
construct	O
the	O
covariance	B
matrix	O
explicitly	O
instead	O
the	O
most	O
computationally	O
demanding	O
steps	O
are	O
those	O
involving	O
sums	O
over	O
the	O
data	O
set	O
that	O
are	O
d	O
m	O
for	O
large	O
d	O
and	O
m	O
d	O
this	O
can	O
be	O
a	O
significant	O
saving	O
compared	O
to	O
and	O
can	O
offset	O
the	O
iterative	O
nature	O
of	O
the	O
em	B
algorithm	I
note	O
that	O
this	O
em	B
algorithm	I
can	O
be	O
implemented	O
in	O
an	O
on-line	O
form	O
in	O
which	O
each	O
d-dimensional	O
data	O
point	O
is	O
read	O
in	O
and	O
processed	O
and	O
then	O
discarded	O
before	O
the	O
next	O
data	O
point	O
is	O
considered	O
to	O
see	O
this	O
note	O
that	O
the	O
quantities	O
evaluated	O
in	O
the	O
e	O
step	O
m-dimensional	O
vector	O
and	O
an	O
m	O
x	O
m	O
matrix	O
can	O
be	O
computed	O
for	O
each	O
data	O
point	O
separately	O
and	O
in	O
the	O
m	O
step	O
we	O
need	O
to	O
accumulate	O
sums	O
over	O
data	O
points	O
which	O
we	O
can	O
do	O
incrementally	O
this	O
approach	O
can	O
be	O
advantageous	O
if	O
both	O
nand	O
d	O
are	O
large	O
because	O
we	O
now	O
have	O
a	O
fully	O
probabilistic	O
model	O
for	O
pca	O
we	O
can	O
deal	O
with	O
missing	B
data	I
provided	O
that	O
it	O
is	O
missing	B
at	I
random	I
by	O
marginalizing	O
over	O
the	O
tribution	O
of	O
the	O
unobserved	O
variables	O
again	O
these	O
missing	O
values	O
can	O
be	O
treated	O
using	O
the	O
em	B
algorithm	I
we	O
give	O
an	O
example	O
of	O
the	O
use	O
of	O
this	O
approach	O
for	O
data	O
visualization	B
in	O
figure	O
another	O
elegant	O
feature	O
ofthe	O
em	B
approach	O
is	O
that	O
we	O
can	O
take	O
the	O
limit	O
a	O
corresponding	O
to	O
standard	O
pca	O
and	O
still	O
obtain	O
a	O
valid	O
em-like	O
algorithm	O
from	O
we	O
see	O
that	O
the	O
only	O
quantity	O
we	O
need	O
to	O
compute	O
in	O
the	O
estep	O
is	O
jezn	O
furthermore	O
the	O
m	O
step	O
is	O
simplifie	O
because	O
m	O
wtw	O
to	O
emphasize	O
the	O
simplicity	O
of	O
the	O
algorithm	O
let	O
us	O
define	O
x	O
to	O
be	O
a	O
matrix	O
of	O
size	O
n	O
x	O
d	O
whose	O
nth	O
row	O
is	O
given	O
by	O
the	O
vector	O
x	O
n	O
x	O
and	O
similarly	O
define	O
to	O
be	O
a	O
matrix	O
of	O
size	O
d	O
x	O
m	O
whose	O
nth	O
row	O
is	O
given	O
by	O
the	O
vector	O
jezn	O
the	O
estep	O
of	O
the	O
em	B
algorithm	I
for	O
pca	O
then	O
becomes	O
o	O
wold-lwdx	O
and	O
the	O
m	O
step	O
takes	O
the	O
form	O
w	O
new	O
xtotoot-l	O
again	O
these	O
can	O
be	O
implemented	O
in	O
an	O
on-line	O
form	O
these	O
equations	O
have	O
a	O
simple	O
interpretation	O
as	O
follows	O
from	O
our	O
earlier	O
discussion	O
we	O
see	O
that	O
the	O
e	O
step	O
involves	O
an	O
orthogonal	O
projection	O
of	O
the	O
data	O
points	O
onto	O
the	O
current	O
estimate	O
for	O
the	O
principal	B
subspace	I
correspondingly	O
the	O
m	O
step	O
represents	O
a	O
re-estimation	O
of	O
the	O
principal	O
contlnljoljs	O
ihtiit	O
vi	O
riarles	O
fig	O
probabilistic	B
pca	I
visoozsbon	O
a	O
portion	O
data	O
setlo	O
ihe	O
einls	O
the	O
left	O
nd	O
plot	O
oiiows	O
ihe	O
ioleoo	O
mean	B
oilhfi	O
poims	O
on	O
lhe	O
principal	B
subspace	I
the	O
hind	O
plot	O
is	O
obtained	O
by	O
firsl	O
ranlomly	O
omitting	O
variable	O
and	O
lhen	O
usrlg	O
em	B
mndie	O
i	O
mi	O
values	O
note	O
i	O
iai	O
data	O
nos	O
allea	O
one	O
missing	O
mea	O
uement	O
but	O
lhoallhe	O
plot	O
i	O
to	O
lhe	O
ona	O
obtained	O
witl	O
miss	O
vallll	O
ewrrise	O
subspace	O
to	O
minimize	O
squared	O
reoonslructioo	O
error	B
in	O
the	O
projetion	O
are	O
c	O
n	O
we	O
ean	O
ghe	O
a	O
physical	B
analogy	I
for	O
this	O
em	B
algorithm	I
which	O
is	O
easily	O
visualized	O
for	O
d	O
and	O
m	O
cooider	O
a	O
collectioo	O
nf	O
data	O
point	O
twi	O
dimension	O
aod	O
let	O
tile	O
u-dimensiunal	O
principal	B
subspace	I
be	O
represented	O
by	O
a	O
rod	O
now	O
atlach	O
each	O
data	O
point	O
to	O
the	O
nxi	O
via	O
a	O
ooing	O
hooie	O
law	O
energy	O
i	O
square	O
of	O
lile	O
spring	O
length	O
in	O
e	O
we	O
keep	O
the	O
nxi	O
hed	O
and	O
allow	O
the	O
attachment	O
point	O
tn	O
up	O
and	O
nxi	O
a	O
to	O
minimize	O
elly	O
this	O
cau	O
each	O
attachment	O
point	O
position	O
itself	O
at	O
the	O
orthogonal	O
pmjeclion	O
of	O
the	O
csponding	O
data	O
point	O
onto	O
the	O
nxi	O
in	O
the	O
m	O
we	O
keep	O
the	O
attachment	O
poiol	O
filed	O
and	O
then	O
release	O
tile	O
nxi	O
and	O
allow	O
it	O
to	O
me	O
tile	O
minimum	O
energy	O
posilion	O
e	O
and	O
m	O
are	O
then	O
repeated	O
until	O
a	O
cvergence	O
cri	O
eri	O
is	O
a	O
is	O
illuslrated	O
in	O
figure	O
bayesian	B
pea	O
sj	O
far	O
in	O
oilr	O
diioo	O
of	O
pea	O
we	O
have	O
ihal	O
tile	O
for	O
dlnenionalit	O
of	O
tile	O
principal	O
is	O
gien	O
in	O
praclice	O
nlmt	O
cooose	O
a	O
suilable	O
according	O
the	O
application	O
for	O
we	O
geny	O
choose	O
whereas	O
for	O
oiher	O
application	O
the	O
approrrialc	O
choice	O
for	O
ma	O
be	O
less	O
dea	O
one	O
appmaoh	O
i	O
pi	O
the	O
eigenalue	O
for	O
lhe	O
data	O
set	O
analog	O
the	O
example	O
in	O
figure	O
for	O
the	O
off	O
line	O
digits	O
dala	O
sci	O
and	O
look	O
to	O
see	O
if	O
lite	O
eigei	O
nmurally	O
form	O
two	O
groups	O
comprising	O
a	O
set	O
of	O
separated	O
by	O
a	O
gap	O
from	O
a	O
of	O
relativel	O
large	O
indicating	O
a	O
natural	O
cholcc	O
fr	O
ai	O
in	O
practice	O
such	O
a	O
gap	O
i	O
oflen	O
seen	O
o	O
o	O
o	O
o	O
flgu	O
syntelic	O
illustrating	O
too	O
em	B
algorithm	I
pca	O
defined	O
by	O
and	O
a	O
data	O
set	O
x	O
with	O
the	O
data	O
points	O
shown	O
in	O
l	O
tlltm	O
tim	O
pmdpal	O
as	O
scaled	O
by	O
ite	O
squafll	O
the	O
eigejllllluel	O
initial	O
configurat	O
too	O
principalsulslat	O
defined	O
by	O
w	O
shown	O
in	O
md	O
toolhfir	O
with	O
the	O
fkijeclions	O
the	O
points	O
z	O
inlo	O
too	O
space	O
giitoo	O
by	O
zwt	O
shown	O
in	O
cyan	O
alter	O
m	O
step	O
too	O
laten	O
sib	O
l	O
pas	O
been	O
update	O
wiih	O
z	O
rell	O
nxed	O
me	O
tte	O
success	O
e	O
slep	O
ite	O
z	O
havu	O
been	O
ihogoooal	O
rrojecliqn	O
with	O
w	O
hk	O
fixed	O
aft	O
tile	O
seoll	O
m	O
s	O
flp	O
after	O
l	O
ie	O
mcl	O
e	O
stl	O
suion	O
i	O
j	O
beau	O
th	O
pmxlhi	O
lilic	O
pea	O
modd	O
has	O
a	O
well	O
defined	O
likelillood	O
fflction	O
we	O
employ	O
to	O
delermine	O
the	O
of	O
dinsiooa	O
ity	O
by	O
tit	O
larget	O
log	O
likelihood	O
a	O
data	O
set	O
such	O
an	O
opprooch	O
hov	O
can	O
become	O
computationally	O
rolly	O
if	O
we	O
cqnsid	O
probabilistic	O
mixlure	O
of	O
pea	O
modds	O
and	O
bishop	O
in	O
we	O
seek	O
the	O
appropriate	O
dimenionalily	O
for	O
toch	O
componenl	O
in	O
mixm	O
gi-en	O
thai	O
w	O
ha-e	O
a	O
probabilislic	O
formulalion	O
of	O
pea	O
il	O
s	O
ms	O
natural	O
s	O
k	O
u	O
buyeian	O
approach	O
model	O
seleclion	O
to	O
do	O
thi	O
nee	O
marginalize	O
the	O
model	O
paramele	O
und	O
wilh	O
to	O
appropriate	O
prior	B
distribution	O
this	O
can	O
be	O
done	O
by	O
uing	O
a	O
framework	O
to	O
the	O
allulylic	O
lly	O
intractable	O
murginaliuoioo	O
marginal	B
likelihood	I
v	O
lues	O
given	O
by	O
ttle	O
bour	O
d	O
cun	O
lhen	O
be	O
cmpund	O
for	O
a	O
r	O
nge	O
of	O
different	O
ar	O
d	O
itie	O
giving	O
iht	O
largest	O
marginal	B
likelihood	I
we	O
consider	O
simpler	O
approach	O
introducoo	O
by	O
b	O
ased	O
on	O
the	O
rddmu	O
continuous	O
latent	O
variables	O
figure	O
probabilistic	O
graphical	B
model	I
for	O
bayesian	B
pea	O
in	O
which	O
the	O
distribution	O
over	O
the	O
parameter	O
matrix	O
w	O
is	O
governed	O
by	O
a	O
vector	O
a	O
of	O
hyperparameters	O
w	O
n	O
proximation	O
which	O
is	O
appropriate	O
when	O
the	O
number	O
of	O
data	O
points	O
is	O
relatively	O
large	O
and	O
the	O
corresponding	O
posterior	O
distribution	O
is	O
tightly	O
peaked	O
it	O
involves	O
a	O
specific	O
choice	O
of	O
prior	B
over	O
w	O
that	O
allows	O
surplus	O
dimensions	O
in	O
the	O
principal	B
subspace	I
to	O
be	O
pruned	O
out	O
of	O
the	O
model	O
this	O
corresponds	O
to	O
an	O
example	O
of	O
automatic	B
relevance	I
determination	I
or	O
ard	O
discussed	O
in	O
section	O
specifically	O
we	O
define	O
an	O
independent	B
gaussian	B
prior	B
over	O
each	O
column	O
of	O
w	O
which	O
represent	O
the	O
vectors	O
defining	O
the	O
principal	B
subspace	I
each	O
such	O
gaussian	B
has	O
an	O
independent	B
variance	B
governed	O
by	O
a	O
precision	O
hyperparameter	B
oi	O
so	O
that	O
where	O
wi	O
is	O
the	O
i	O
th	O
column	O
of	O
w	O
the	O
resulting	O
model	O
can	O
be	O
represented	O
using	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
the	O
values	O
for	O
oi	O
will	O
be	O
found	O
iteratively	O
by	O
maximizing	O
the	O
hood	O
function	O
in	O
which	O
w	O
has	O
been	O
integrated	O
out	O
as	O
a	O
result	O
of	O
this	O
optimization	O
some	O
of	O
the	O
oi	O
may	O
be	O
driven	O
to	O
infinity	O
with	O
the	O
corresponding	O
parameters	O
tor	O
wi	O
being	O
driven	O
to	O
zero	O
posterior	O
distribution	O
becomes	O
a	O
delta	O
function	O
at	O
the	O
origin	O
giving	O
a	O
sparse	O
solution	O
the	O
effective	O
dimensionality	O
of	O
the	O
principal	B
subspace	I
is	O
then	O
determined	O
by	O
the	O
number	O
of	O
finite	O
oi	O
values	O
and	O
the	O
ing	O
vectors	O
wi	O
can	O
be	O
thought	O
of	O
as	O
for	O
modelling	O
the	O
data	O
distribution	O
in	O
this	O
way	O
the	O
bayesian	B
approach	O
is	O
automatically	O
making	O
the	O
trade-off	O
between	O
improving	O
the	O
fit	O
to	O
the	O
data	O
by	O
using	O
a	O
larger	O
number	O
of	O
vectors	O
wi	O
with	O
their	O
responding	O
eigenvalues	O
ai	O
each	O
tuned	O
to	O
the	O
data	O
and	O
reducing	O
the	O
complexity	O
of	O
the	O
model	O
by	O
suppressing	O
some	O
of	O
the	O
wi	O
vectors	O
the	O
origins	O
of	O
this	O
sparsity	B
were	O
discussed	O
earlier	O
in	O
the	O
context	O
of	O
relevance	B
vector	I
machines	O
the	O
values	O
of	O
oi	O
are	O
re-estimated	O
during	O
training	B
by	O
maximizing	O
the	O
log	O
marginal	B
likelihood	I
given	O
by	O
pxla	O
j-l	O
jpxiw	O
j-l	O
dw	O
where	O
the	O
log	O
ofpxiw	O
j-l	O
is	O
given	O
by	O
note	O
that	O
for	O
simplicity	O
we	O
also	O
treat	O
j-l	O
and	O
as	O
parameters	O
to	O
be	O
estimated	O
rather	O
than	O
defining	O
priors	O
over	O
these	O
parameters	O
section	O
probabilistic	O
pea	O
section	O
section	O
because	O
this	O
integration	O
is	O
intractable	O
we	O
make	O
use	O
of	O
the	O
laplace	B
tion	O
if	O
we	O
assume	O
that	O
the	O
posterior	O
distribution	O
is	O
sharply	O
peaked	O
as	O
will	O
occur	O
for	O
sufficiently	O
large	O
data	O
sets	O
then	O
the	O
re-estimation	O
equations	O
obtained	O
by	O
maximizing	O
the	O
marginal	B
likelihood	I
with	O
respect	O
to	O
ai	O
take	O
the	O
simple	O
form	O
which	O
follows	O
from	O
noting	O
that	O
the	O
dimensionality	O
of	O
wi	O
is	O
d	O
these	O
estimations	O
are	O
interleaved	O
with	O
the	O
em	B
algorithm	I
updates	O
for	O
determining	O
wand	O
a	O
the	O
e-step	O
equations	O
are	O
again	O
given	O
by	O
and	O
similarly	O
the	O
step	O
equation	O
for	O
a	O
is	O
again	O
given	O
by	O
the	O
only	O
change	O
is	O
to	O
the	O
m-step	O
equation	O
for	O
w	O
which	O
is	O
modified	O
to	O
give	O
where	O
a	O
diagai	O
the	O
value	O
of	O
i-	O
is	O
given	O
by	O
the	O
sample	B
mean	B
as	O
before	O
if	O
we	O
choose	O
m	O
d	O
then	O
if	O
all	O
ai	O
values	O
are	O
finite	O
the	O
model	O
represents	O
a	O
full-covariance	O
gaussian	B
while	O
if	O
all	O
the	O
ai	O
go	O
to	O
infinity	O
the	O
model	O
is	O
equivalent	O
to	O
an	O
isotropic	B
gaussian	B
and	O
so	O
the	O
model	O
can	O
encompass	O
all	O
pennissible	O
values	O
for	O
the	O
effective	O
dimensionality	O
of	O
the	O
principal	B
subspace	I
it	O
is	O
also	O
possible	O
to	O
consider	O
smaller	O
values	O
of	O
m	O
which	O
will	O
save	O
on	O
computational	O
cost	O
but	O
which	O
will	O
limit	O
the	O
maximum	O
dimensionality	O
of	O
the	O
subspace	O
a	O
comparison	O
of	O
the	O
results	O
of	O
this	O
algorithm	O
with	O
standard	O
probabilistic	B
pca	I
is	O
shown	O
in	O
figure	O
bayesian	B
pca	O
provides	O
an	O
opportunity	O
to	O
illustrate	O
the	O
gibbs	B
sampling	I
rithm	O
discussed	O
in	O
section	O
figure	O
shows	O
an	O
example	O
of	O
the	O
samples	O
from	O
the	O
hyperparameters	O
in	O
ai	O
for	O
a	O
data	O
set	O
in	O
d	O
dimensions	O
in	O
which	O
the	O
mensionality	O
of	O
the	O
latent	O
space	O
is	O
m	O
but	O
in	O
which	O
the	O
data	O
set	O
is	O
generated	O
from	O
a	O
probabilistic	B
pca	I
model	O
having	O
one	O
direction	O
of	O
high	O
variance	B
with	O
the	O
remaining	O
directions	O
comprising	O
low	O
variance	B
noise	O
this	O
result	O
shows	O
clearly	O
the	O
presence	O
of	O
three	O
distinct	O
modes	O
in	O
the	O
posterior	O
distribution	O
at	O
each	O
step	O
of	O
the	O
iteration	O
one	O
of	O
the	O
hyperparameters	O
has	O
a	O
small	O
value	O
and	O
the	O
remaining	O
two	O
have	O
large	O
values	O
so	O
that	O
two	O
of	O
the	O
three	O
latent	O
variables	O
are	O
suppressed	O
during	O
the	O
course	O
of	O
the	O
gibbs	B
sampling	I
the	O
solution	O
makes	O
sharp	O
transitions	O
between	O
the	O
three	O
modes	O
the	O
model	O
described	O
here	O
involves	O
a	O
prior	B
only	O
over	O
the	O
matrix	O
w	O
a	O
fully	O
bayesian	B
treatment	O
of	O
pca	O
including	O
priors	O
over	O
a	O
and	O
n	O
and	O
solved	O
ing	O
variational	B
methods	O
is	O
described	O
in	O
bishop	O
for	O
a	O
discussion	O
of	O
ous	O
bayesian	B
approaches	O
to	O
detennining	O
the	O
appropriate	O
dimensionality	O
for	O
a	O
pca	O
model	O
see	O
minka	O
factor	B
analysis	I
factor	B
analysis	I
is	O
a	O
linear-gaussian	O
latent	B
variable	I
model	O
that	O
is	O
closely	O
related	O
to	O
probabilistic	B
pca	I
its	O
definition	O
differs	O
from	O
that	O
of	O
probabilistic	B
pca	I
only	O
in	O
that	O
the	O
conditional	B
distribution	O
of	O
the	O
observed	B
variable	I
x	O
given	O
the	O
latent	B
variable	I
z	O
is	O
continuous	O
latent	O
variables	O
figure	O
diagrams	O
of	O
the	O
matrix	O
w	O
in	O
which	O
each	O
element	O
the	O
matrix	O
is	O
depicted	O
as	O
a	O
square	O
lor	O
positive	O
and	O
black	O
lor	O
negative	O
values	O
whose	O
area	O
is	O
proportional	O
to	O
the	O
magnitude	O
of	O
that	O
element	O
the	O
synthetic	O
data	O
sel	O
comprises	O
data	O
points	O
in	O
d	O
dimensions	O
sampled	O
from	O
a	O
gaussian	B
distribution	O
having	O
standard	B
deviation	I
in	O
directions	O
and	O
standard	B
deviation	I
in	O
the	O
remaining	O
directions	O
for	O
a	O
data	O
set	O
in	O
d	O
dimensions	O
having	O
at	O
directions	O
with	O
larger	O
variance	B
than	O
the	O
remaining	O
directions	O
the	O
left-hand	O
plol	O
shows	O
the	O
result	O
irom	O
maximum	B
likelihood	I
probabilistic	B
pca	I
and	O
the	O
left	O
hand	O
plot	O
shows	O
the	O
corresponding	O
resuft	O
from	O
bayesian	B
pea	O
we	O
see	O
how	O
the	O
bayesian	B
model	O
is	O
able	O
to	O
discover	O
the	O
appropriate	O
dimensionality	O
by	O
suppressing	O
the	O
surplus	O
degrees	B
of	I
freedom	I
taken	O
to	O
have	O
a	O
diagonal	B
rather	O
than	O
an	O
isotropic	B
covariance	B
so	O
that	O
pxlz	O
nxlwz	O
where	O
ill	O
is	O
a	O
d	O
x	O
d	O
diagonal	B
matrix	O
note	O
that	O
the	O
factor	B
analysis	I
model	O
in	O
common	O
with	O
probabilistic	B
pca	I
assumes	O
that	O
the	O
observed	O
variables	O
xl	O
are	O
dent	O
given	O
the	O
latent	B
variable	I
z	O
in	O
essence	O
the	O
factor	B
analysis	I
model	O
is	O
explaining	O
the	O
observed	O
covariance	B
structure	O
of	O
the	O
data	O
by	O
representing	O
the	O
independent	B
ance	O
associated	O
with	O
each	O
coordinate	O
in	O
the	O
matrix	O
and	O
capturing	O
the	O
covariance	B
between	O
variables	O
in	O
the	O
matrix	O
w	O
in	O
the	O
factor	B
analysis	I
literature	O
the	O
columns	O
of	O
w	O
which	O
capture	O
the	O
correlations	O
between	O
observed	O
variables	O
are	O
calledfaclor	O
loadings	O
and	O
the	O
diagonal	B
elements	O
of	O
which	O
represent	O
the	O
independent	B
noise	O
variances	O
for	O
each	O
of	O
the	O
variables	O
are	O
called	O
llniqllenesses	O
the	O
origins	O
of	O
factor	B
analysis	I
are	O
as	O
old	O
as	O
those	O
of	O
pca	O
and	O
discussions	O
of	O
factor	B
analysis	I
can	O
be	O
found	O
in	O
the	O
books	O
by	O
everitt	O
bartholomew	O
and	O
basilevsky	O
links	O
between	O
factor	B
analysis	I
and	O
pca	O
were	O
investigated	O
by	O
lilwley	O
and	O
anderson	O
who	O
showed	O
that	O
at	O
stationary	B
points	O
of	O
the	O
likelihood	B
function	I
for	O
a	O
faclor	O
analysis	O
model	O
with	O
the	O
columns	O
of	O
w	O
are	O
scaled	O
eigenvectors	O
of	O
the	O
sample	O
covariance	B
matrix	O
and	O
is	O
the	O
average	O
of	O
the	O
discarded	O
eigenvalues	O
later	O
tipping	O
and	O
bishop	O
showed	O
that	O
the	O
maximum	O
of	O
the	O
log	O
likelihood	B
function	I
occurs	O
when	O
the	O
eigenvectors	O
comprising	O
ware	O
chosen	O
to	O
be	O
the	O
principal	O
eigenvectors	O
making	O
use	O
of	O
we	O
see	O
that	O
the	O
marginal	B
distribution	O
for	O
the	O
observed	O
ica	O
gillbs	O
baylslan	O
pca	O
shming	O
plots	O
oj	O
ino	O
versus	O
number	O
br	O
three	O
showing	O
trtions	O
tbe	O
th	O
moots	O
posterior	O
distribution	O
betw	O
values	O
eurre	O
sna	O
i	O
gi-n	O
by	O
n	O
xlj	O
c	O
whe	O
now	O
cwwti	O
as	O
with	O
probabilistic	O
pc	O
a	O
thi	O
momi	O
is	O
im-ri	O
rrl	O
to	O
in	O
latent	O
histoocally	O
factor	O
anals	O
has	O
been	O
lhe	O
of	O
wroe	O
a	O
tempt	O
h-e	O
bttn	O
to	O
place	O
an	O
intcptlioo	O
on	O
the	O
indvidual	O
faclon	O
coofdinates	O
in	O
z	O
space	O
which	O
pmen	O
problematic	O
due	O
to	O
lr	O
e	O
of	O
factof	O
analysis	O
associmed	O
with	O
mation	O
in	O
this	O
from	O
oor	O
perspeoh-e	O
howe-er	O
we	O
shall	O
factor	B
analysis	I
as	O
a	O
form	O
of	O
lalent	O
densily	O
model	O
in	O
which	O
the	O
form	O
of	O
tlc	O
lalent	O
i	O
of	O
interest	O
but	O
no	O
the	O
particular	O
choicc	O
of	O
coordinates	O
used	O
to	O
descritc	O
il	O
if	O
we	O
wish	O
to	O
remove	O
the	O
degeneracy	O
asociated	O
with	O
latent	O
roiations	O
mut	O
conider	O
non-gaussian	O
latent	O
ditribution	O
giirrg	O
rise	O
independent	B
component	O
models	O
we	O
can	O
detennie	O
the	O
parameters	O
i	O
in	O
the	O
fac	O
of	O
an	O
lyi	O
model	O
by	O
muimum	O
likelihood	O
solution	O
for	O
i	O
i	O
agin	O
given	O
by	O
the	O
how	O
eyc	O
probabilitic	O
lca	O
lllcre	O
i	O
no	O
longer	O
a	O
closed-form	O
maximum	B
likelihood	I
solution	O
for	O
mu	O
ltherdorc	O
be	O
found	O
because	O
faclor	O
anali	O
is	O
a	O
latent	B
variable	I
model	O
thi	O
can	O
be	O
don	O
using	O
an	O
em	B
algorilhm	O
and	O
thayer	O
is	O
to	O
the	O
one	O
used	O
pmllbili	O
tie	O
pea	O
specihcally	O
lhe	O
e-lep	O
eqnjtioo	O
are	O
g-en	O
by	O
ezoj	O
gwt-xn	O
xl	O
ezzj	O
g	O
ezoezt	O
where	O
he	O
defid	O
noie	O
tht	O
thi	O
i	O
e	O
preed	O
in	O
a	O
for	O
thai	O
in-ohes	O
inycrsin	O
of	O
mal	O
rices	O
silo	O
i	O
x	O
rathelhan	O
d	O
x	O
d	O
for	O
tbe	O
d	O
x	O
d	O
diagooal	O
matrix	O
oj	O
in-erse	O
i	O
continuous	O
latent	O
variables	O
exercise	O
to	O
compute	O
in	O
od	O
steps	O
which	O
is	O
convenient	O
because	O
often	O
m	O
d	O
similarly	O
the	O
m-step	O
equations	O
take	O
the	O
form	O
w	O
new	O
diags-w	O
w	O
xl	O
where	O
the	O
operator	O
sets	O
all	O
of	O
the	O
nondiagonal	O
elements	O
of	O
a	O
matrix	O
to	O
zero	O
a	O
bayesian	B
treatment	O
of	O
the	O
factor	B
analysis	I
model	O
can	O
be	O
obtained	O
by	O
a	O
straightforward	O
application	O
of	O
the	O
techniques	O
discussed	O
in	O
this	O
book	O
another	O
difference	O
between	O
probabilistic	B
pca	I
and	O
factor	B
analysis	I
concerns	O
their	O
different	O
behaviour	O
under	O
transformations	O
of	O
the	O
data	O
set	O
for	O
pca	O
and	O
tic	O
pca	O
if	O
we	O
rotate	O
the	O
coordinate	O
system	O
in	O
data	O
space	O
then	O
we	O
obtain	O
exactly	O
the	O
same	O
fit	O
to	O
the	O
data	O
but	O
with	O
the	O
w	O
matrix	O
transformed	O
by	O
the	O
corresponding	O
rotation	O
matrix	O
however	O
for	O
factor	B
analysis	I
the	O
analogous	O
property	O
is	O
that	O
if	O
we	O
make	O
a	O
component-wise	O
re-scaling	O
of	O
the	O
data	O
vectors	O
then	O
this	O
is	O
absorbed	O
into	O
a	O
corresponding	O
re-scaling	O
of	O
the	O
elements	O
of	O
exercise	O
kernel	O
pea	O
in	O
chapter	O
we	O
saw	O
how	O
the	O
technique	O
of	O
kernel	B
substitution	I
allows	O
us	O
to	O
take	O
an	O
algorithm	O
expressed	O
in	O
terms	O
of	O
scalar	O
products	O
of	O
the	O
form	O
x	O
t	O
x	O
and	O
generalize	O
that	O
algorithm	O
by	O
replacing	O
the	O
scalar	O
products	O
with	O
a	O
nonlinear	O
kernel	O
here	O
we	O
apply	O
this	O
technique	O
of	O
kernel	B
substitution	I
to	O
principal	B
component	I
analysis	I
thereby	O
obtaining	O
a	O
nonlinear	O
generalization	B
called	O
kernel	O
pea	O
et	O
al	O
consider	O
a	O
data	O
set	O
of	O
observations	O
where	O
n	O
n	O
in	O
a	O
space	O
of	O
dimensionality	O
d	O
in	O
order	O
to	O
keep	O
the	O
notation	O
uncluttered	O
we	O
shall	O
assume	O
that	O
we	O
have	O
already	O
subtracted	O
the	O
sample	B
mean	B
from	O
each	O
of	O
the	O
vectors	O
x	O
n	O
so	O
that	O
ln	O
x	O
n	O
o	O
the	O
first	O
step	O
is	O
to	O
express	O
conventional	O
pca	O
in	O
such	O
a	O
form	O
that	O
the	O
data	O
vectors	O
n	O
appear	O
only	O
in	O
the	O
form	O
of	O
the	O
scalar	O
products	O
x	O
x	O
m	O
recall	O
that	O
the	O
principal	O
components	O
are	O
defined	O
by	O
the	O
eigenvectors	O
ui	O
of	O
the	O
covariance	B
matrix	O
where	O
i	O
here	O
the	O
d	O
x	O
d	O
sample	O
covariance	B
matrix	O
s	O
is	O
defined	O
by	O
sui	O
aiui	O
and	O
the	O
eigenvectors	O
are	O
normalized	O
such	O
that	O
ut	O
ui	O
now	O
consider	O
a	O
nonlinear	O
transformation	O
into	O
an	O
m	O
feature	B
space	I
so	O
that	O
each	O
data	O
point	O
x	O
n	O
is	O
thereby	O
projected	O
onto	O
a	O
point	O
we	O
can	O
kmci	O
lco	O
sctiematic	O
kernel	O
pea	O
a	O
hi	O
in	O
lhe	O
oflglnal	O
space	O
l	O
plot	O
pfoleled	O
by	O
tranllklfmalion	O
fa	O
tur	O
space	O
plot	O
by	O
ib	O
pca	O
in	O
the	O
we	O
oblaoilha	O
pmeiilai	O
tnt	O
ie	O
in	O
blue	O
by	O
lha	O
plolklio	O
onio	O
lhe	O
iirsl	O
poiridl	O
poofcllu	O
in	O
oillll	O
hole	O
iiuiiin	O
gmefm	O
la	O
nol	O
pox	O
poi	O
v	O
tha	O
gr-	O
in	O
imiun	O
apam	O
indicma	O
iha	O
ptrform	O
pea	O
ill	O
fnlllk	O
lopice	O
liiils	O
model	O
ill	O
onpnll	O
cbuo	O
as	O
in	O
fllift	O
princlpai	O
lei	O
los	O
oulllt	O
illt	O
diu	O
lobo	O
halnro	O
mean	B
fu	O
ji	O
l	O
o	O
we	O
dwl	O
itl	O
pol	O
co-	O
ullcc	O
mmfu	O
l	O
by	O
l	O
c	O
and	O
opanion	O
i	O
lined	O
by	O
cv	O
av	O
m	O
our	O
goal	O
is	O
soh	O
lhis	O
eigenlilue	O
problem	O
wilhoul	O
hainlllo	O
work	O
in	O
f	O
liture	O
from	O
definilion	O
of	O
c	O
lhe	O
equal	O
ions	O
lell	O
u	O
thai	O
y	O
l	O
v	O
a	O
tilt	O
v	O
is	O
lin	O
by	O
ii	O
romblllaon	O
of	O
illt	O
d	O
j	O
jo	O
he	O
iillhc	O
l	O
v	O
continuous	O
latent	O
variables	O
substituting	O
this	O
expansion	O
back	O
into	O
the	O
eigenvector	O
equation	O
we	O
obtain	O
n	O
n	O
l	O
nl	O
n	O
l	O
n	O
aim	O
ai	O
l	O
ml	O
nl	O
ain	O
the	O
key	O
step	O
is	O
now	O
to	O
express	O
this	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
kxn	O
x	O
m	O
which	O
we	O
do	O
by	O
multiplying	O
both	O
sides	O
by	O
to	O
give	O
n	O
m	O
n	O
lkxixn	O
l	O
nl	O
ml	O
aimkxnxm	O
ai	O
lainkxixn	O
n	O
nl	O
this	O
can	O
be	O
written	O
in	O
matrix	O
notation	O
as	O
where	O
ai	O
is	O
an	O
n-dimensional	O
column	O
vector	O
with	O
elements	O
ani	O
for	O
n	O
we	O
can	O
find	O
solutions	O
for	O
ai	O
by	O
solving	O
the	O
following	O
eigenvalue	O
problem	O
exercise	O
in	O
which	O
we	O
have	O
removed	O
a	O
factor	O
of	O
k	O
from	O
both	O
sides	O
of	O
note	O
that	O
the	O
solutions	O
of	O
and	O
differ	O
only	O
by	O
eigenvectors	O
of	O
k	O
having	O
zero	O
eigenvalues	O
that	O
do	O
not	O
affect	O
the	O
principal	O
components	O
projection	O
the	O
normalization	O
condition	O
for	O
the	O
coefficients	O
ai	O
is	O
obtained	O
by	O
requiring	O
that	O
the	O
eigenvectors	O
in	O
feature	B
space	I
be	O
normalized	O
using	O
and	O
we	O
have	O
vvi	O
l	O
l	O
n	O
n	O
nl	O
ml	O
ainaim	O
ak	O
ainaai	O
having	O
solved	O
the	O
eigenvector	O
problem	O
the	O
resulting	O
principal	O
component	O
jections	O
can	O
then	O
also	O
be	O
cast	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
so	O
that	O
using	O
the	O
projection	O
of	O
a	O
point	O
x	O
onto	O
eigenvector	O
i	O
is	O
given	O
by	O
yix	O
l	O
ain	O
l	O
ainkx	O
x	O
n	O
n	O
n	O
nl	O
nl	O
and	O
so	O
again	O
is	O
expressed	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
in	O
the	O
original	O
d-dimensional	O
x	O
space	O
there	O
are	O
d	O
orthogonal	O
eigenvectors	O
and	O
hence	O
we	O
can	O
find	O
at	O
most	O
d	O
linear	O
principal	O
components	O
the	O
dimensionality	O
m	O
of	O
the	O
feature	B
space	I
however	O
can	O
be	O
much	O
larger	O
than	O
d	O
infinite	O
and	O
thus	O
we	O
can	O
find	O
a	O
number	O
of	O
nonlinear	O
principal	O
components	O
that	O
can	O
exceed	O
d	O
note	O
however	O
that	O
the	O
number	O
of	O
nonzero	O
eigenvalues	O
cannot	O
exceed	O
the	O
number	O
n	O
of	O
data	O
points	O
because	O
if	O
m	O
n	O
the	O
covariance	B
matrix	O
in	O
feature	B
space	I
has	O
rank	O
at	O
most	O
equal	O
to	O
n	O
this	O
is	O
reflected	O
in	O
the	O
fact	O
that	O
kernel	B
pca	I
involves	O
the	O
eigenvector	O
expansion	O
of	O
the	O
n	O
x	O
n	O
matrix	O
k	O
kernel	B
pca	I
so	O
far	O
we	O
have	O
assumed	O
that	O
the	O
projected	O
data	O
set	O
given	O
by	O
has	O
zero	O
mean	B
which	O
in	O
general	O
will	O
not	O
be	O
the	O
case	O
we	O
cannot	O
simply	O
compute	O
and	O
then	O
subtract	O
off	O
the	O
mean	B
since	O
we	O
wish	O
to	O
avoid	O
working	O
directly	O
in	O
feature	B
space	I
and	O
so	O
again	O
we	O
formulate	O
the	O
algorithm	O
purely	O
in-	O
erms	O
of	O
the	O
kernel	B
function	I
the	O
projected	O
data	O
points	O
after	O
centralizing	O
denoted	O
are	O
given	O
by	O
and	O
the	O
corresponding	O
elements	O
of	O
the	O
gram	B
matrix	I
are	O
given	O
by	O
k	O
nm	O
n	O
n	O
l	O
zl	O
n	O
n	O
n	O
n	O
l	O
ll	O
zl	O
jl	O
zl	O
n	O
kxn	O
x	O
m	O
n	O
l	O
kxz	O
x	O
m	O
zl	O
n	O
lkxnxz	O
llkxjxl	O
n	O
n	O
jl	O
n	O
zl	O
this	O
can	O
be	O
expressed	O
in	O
matrix	O
notation	O
as	O
exercise	O
where	O
in	O
denotes	O
the	O
n	O
x	O
n	O
matrix	O
in	O
which	O
every	O
element	O
takes	O
the	O
value	O
ln	O
thus	O
we	O
can	O
evaluate	O
k	O
using	O
only	O
the	O
kernel	B
function	I
and	O
then	O
use	O
k	O
to	O
determine	O
the	O
eigenvalues	O
and	O
eigenvectors	O
note	O
that	O
the	O
standard	O
pca	O
algorithm	O
is	O
recovered	O
as	O
a	O
special	O
case	O
if	O
we	O
use	O
a	O
linear	O
kernel	O
kx	O
x	O
xtx	O
figure	O
shows	O
an	O
example	O
of	O
kernel	B
pca	I
applied	O
to	O
a	O
synthetic	O
data	O
set	O
et	O
al	O
here	O
a	O
kernel	O
of	O
the	O
form	O
kx	O
x	O
exp-llx	O
is	O
applied	O
to	O
a	O
synthetic	O
data	O
set	O
the	O
lines	O
correspond	O
to	O
contours	O
along	O
which	O
the	O
projection	O
onto	O
the	O
corresponding	O
principal	O
component	O
defined	O
by	O
n	O
l	O
nl	O
ainkx	O
x	O
n	O
is	O
constant	O
continuous	O
latent	O
valuables	O
figure	O
ellmple	O
kernel	B
pca	I
with	O
a	O
gaussian	B
kernel	I
awiioo	O
a	O
synthetic	O
sat	O
in	O
two	O
showing	O
firsl	O
flight	O
eigenfunclions	O
along	O
wh	O
l	O
eir	O
the	O
oootours	O
am	O
lines	O
along	O
which	O
onlo	O
t	O
coffaspmding	O
principal	O
costam	O
nola	O
haw	O
ihe	O
firsl	O
two	O
th	O
dusters	O
ill	O
spiii	O
oilhe	O
eluste	O
into	O
hamos	O
and	O
t	O
loliowing	O
ihree	O
again	O
spi	O
he	O
duste	O
into	O
halves	O
along	O
directions	O
orthogonal	O
tho	O
premous	O
splils	O
one	O
obvioo	O
dls	O
ajmotaeof	O
iemel	O
is	O
thaf	O
if	O
invohes	O
finding	O
lhe	O
tors	O
of	O
the	O
n	O
x	O
n	O
malri	O
k	O
raw	O
ihan	O
lhe	O
d	O
x	O
d	O
malri	O
s	O
of	O
cor	O
emionallinear	O
and	O
io	O
in	O
for	O
large	O
data	O
approlmation	O
are	O
often	O
usd	O
finally	O
ooie	O
that	O
i	O
linear	O
ica	O
we	O
often	O
retain	O
some	O
redocel	O
num	O
ber	O
l	O
dof	O
eigenvectors	O
and	O
then	O
approlmale	O
data	O
vttlr	O
xn	O
b	O
its	O
projection	O
i	O
lhe	O
l-dimensional	O
principal	B
subspace	I
defined	O
by	O
i-l	O
i	O
kernellca	O
this	O
will	O
in	O
not	O
be	O
floslble	O
to	O
see	O
thl	O
ooie	O
ihat	O
the	O
ping	O
maps	O
the	O
d-dimensional	O
x	O
space	O
it	O
d-dimensioo	O
l	O
manijqiii	O
in	O
lhe	O
m-dimemioo	O
l	O
femure	O
space	O
tlie	O
x	O
i	O
koown	O
a	O
lhe	O
f	O
imagr	O
of	O
lhe	O
cponding	O
poil	O
however	O
fhe	O
of	O
poinl	O
in	O
feature	O
the	O
linear	O
rca	O
in	O
that	O
will	O
typically	O
lie	O
on	O
fhe	O
nonlinear	O
dimensional	O
manifold	B
and	O
will	O
nul	O
ha	O
a	O
cpondlng	O
p	O
lmoein	O
dolo	O
spa	O
c	O
technlque	O
ho	O
-e	O
lherefore	O
bttn	O
proposed	O
for	O
finding	O
approximale	O
pre-image	O
iblr	O
nat	O
nonlinear	O
latent	B
variable	I
models	O
nonlinear	O
latent	B
variable	I
models	O
in	O
this	O
chapter	O
we	O
have	O
focussed	O
on	O
the	O
simplest	O
class	O
of	O
models	O
having	O
continuous	O
latent	O
variables	O
namely	O
those	O
based	O
on	O
linear-gaussian	O
distributions	O
as	O
well	O
as	O
having	O
great	O
practical	O
importance	O
these	O
models	O
are	O
relatively	O
easy	O
to	O
analyse	O
and	O
to	O
fit	O
to	O
data	O
and	O
can	O
also	O
be	O
used	O
as	O
components	O
in	O
more	O
complex	O
models	O
here	O
we	O
consider	O
briefly	O
some	O
generalizations	O
of	O
this	O
framework	O
to	O
models	O
that	O
are	O
either	O
nonlinear	O
or	O
non-gaussian	O
or	O
both	O
in	O
fact	O
the	O
issues	O
of	O
nonlinearity	O
and	O
non-gaussianity	O
are	O
related	O
because	O
a	O
general	O
probability	B
density	B
can	O
be	O
obtained	O
from	O
a	O
simple	O
fixed	O
reference	O
density	B
such	O
as	O
a	O
gaussian	B
by	O
making	O
a	O
nonlinear	O
change	O
of	O
variables	O
this	O
idea	O
forms	O
the	O
basis	O
of	O
several	O
practical	O
latent	B
variable	I
models	O
as	O
we	O
shall	O
see	O
shortly	O
exercise	O
independent	B
component	I
analysis	I
we	O
begin	O
by	O
considering	O
models	O
in	O
which	O
the	O
observed	O
variables	O
are	O
related	O
linearly	O
to	O
the	O
latent	O
variables	O
but	O
for	O
which	O
the	O
latent	O
distribution	O
is	O
non-gaussian	O
an	O
important	O
class	O
of	O
such	O
models	O
known	O
as	O
independent	B
component	I
analysis	I
or	O
lea	O
arises	O
when	O
we	O
consider	O
a	O
distribution	O
over	O
the	O
latent	O
variables	O
that	O
factorizes	O
so	O
that	O
m	O
pz	O
iipzj	O
jl	O
to	O
understand	O
the	O
role	O
of	O
such	O
models	O
consider	O
a	O
situation	O
in	O
which	O
two	O
people	O
are	O
talking	O
at	O
the	O
same	O
time	O
and	O
we	O
record	O
their	O
voices	O
using	O
two	O
microphones	O
if	O
we	O
ignore	O
effects	O
such	O
as	O
time	O
delay	O
and	O
echoes	O
then	O
the	O
signals	O
received	O
by	O
the	O
microphones	O
at	O
any	O
point	O
in	O
time	O
will	O
be	O
given	O
by	O
linear	O
combinations	O
of	O
the	O
amplitudes	O
of	O
the	O
two	O
voices	O
the	O
coefficients	O
of	O
this	O
linear	O
combination	O
will	O
be	O
constant	O
and	O
if	O
we	O
can	O
infer	O
their	O
values	O
from	O
sample	O
data	O
then	O
we	O
can	O
invert	O
the	O
mixing	O
process	O
it	O
is	O
nonsingular	O
and	O
thereby	O
obtain	O
two	O
clean	O
signals	O
each	O
of	O
which	O
contains	O
the	O
voice	O
of	O
just	O
one	O
person	O
this	O
is	O
an	O
example	O
of	O
a	O
problem	O
called	O
blind	B
source	I
separation	I
in	O
which	O
refers	O
to	O
the	O
fact	O
that	O
we	O
are	O
given	O
only	O
the	O
mixed	O
data	O
and	O
neither	O
the	O
original	O
sources	O
nor	O
the	O
mixing	O
coefficients	O
are	O
observed	O
this	O
type	O
of	O
problem	O
is	O
sometimes	O
addressed	O
using	O
the	O
following	O
approach	O
in	O
which	O
we	O
ignore	O
the	O
temporal	O
nature	O
of	O
the	O
signals	O
and	O
treat	O
the	O
successive	O
samples	O
as	O
i	O
i	O
d	O
we	O
consider	O
a	O
generative	B
model	I
in	O
which	O
there	O
are	O
two	O
latent	O
variables	O
corresponding	O
to	O
the	O
unobserved	O
speech	O
signal	O
amplitudes	O
and	O
there	O
are	O
two	O
observed	O
variables	O
given	O
by	O
the	O
signal	O
values	O
at	O
the	O
microphones	O
the	O
latent	O
variables	O
have	O
a	O
joint	O
distribution	O
that	O
factorizes	O
as	O
above	O
and	O
the	O
observed	O
variables	O
are	O
given	O
by	O
a	O
linear	O
combination	O
of	O
the	O
latent	O
variables	O
there	O
is	O
no	O
need	O
to	O
include	O
a	O
noise	O
distribution	O
because	O
the	O
number	O
of	O
latent	O
variables	O
equals	O
the	O
number	O
of	O
served	O
variables	O
and	O
therefore	O
the	O
marginal	B
distribution	O
of	O
the	O
observed	O
variables	O
will	O
not	O
in	O
general	O
be	O
singular	O
so	O
the	O
observed	O
variables	O
are	O
simply	O
deterministic	O
linear	O
combinations	O
of	O
the	O
latent	O
variables	O
given	O
a	O
data	O
set	O
of	O
observations	O
the	O
continuous	O
latent	O
variables	O
likelihood	B
function	I
for	O
this	O
model	O
is	O
a	O
function	O
of	O
the	O
coefficients	O
in	O
the	O
linear	O
bination	O
the	O
log	O
likelihood	O
can	O
be	O
maximized	O
using	O
gradient-based	O
optimization	O
giving	O
rise	O
to	O
a	O
particular	O
version	O
of	O
independent	B
component	I
analysis	I
the	O
success	O
of	O
this	O
approach	O
requires	O
that	O
the	O
latent	O
variables	O
have	O
non-gaussian	O
distributions	O
to	O
see	O
this	O
recall	O
that	O
in	O
probabilistic	B
pca	I
in	O
factor	B
analysis	I
the	O
latent-space	O
distribution	O
is	O
given	O
by	O
a	O
zero-mean	O
isotropic	B
gaussian	B
the	O
model	O
therefore	O
cannot	O
distinguish	O
between	O
two	O
different	O
choices	O
for	O
the	O
latent	O
variables	O
where	O
these	O
differ	O
simply	O
by	O
a	O
rotation	O
in	O
latent	O
space	O
this	O
can	O
be	O
verified	O
directly	O
by	O
noting	O
that	O
the	O
marginal	B
density	B
and	O
hence	O
the	O
likelihood	B
function	I
is	O
unchanged	O
if	O
we	O
make	O
the	O
transformation	O
w	O
wr	O
where	O
r	O
is	O
an	O
orthogonal	O
matrix	O
satisfying	O
rrt	O
i	O
because	O
the	O
matrix	O
c	O
given	O
by	O
is	O
itself	O
invariant	O
extending	O
the	O
model	O
to	O
allow	O
more	O
general	O
gaussian	B
latent	O
distributions	O
does	O
not	O
change	O
this	O
conclusion	O
because	O
as	O
we	O
have	O
seen	O
such	O
a	O
model	O
is	O
equivalent	O
to	O
the	O
zero-mean	O
isotropic	B
gaussian	B
latent	B
variable	I
model	O
another	O
way	O
to	O
see	O
why	O
a	O
gaussian	B
latent	B
variable	I
distribution	O
in	O
a	O
linear	O
model	O
is	O
insufficient	O
to	O
find	O
independent	B
components	O
is	O
to	O
note	O
that	O
the	O
principal	O
nents	O
represent	O
a	O
rotation	O
of	O
the	O
coordinate	O
system	O
in	O
data	O
space	O
such	O
as	O
to	O
ize	O
the	O
covariance	B
matrix	O
so	O
that	O
the	O
data	O
distribution	O
in	O
the	O
new	O
coordinates	O
is	O
then	O
uncorrelated	O
although	O
zero	O
correlation	O
is	O
a	O
necessary	O
condition	O
for	O
independence	O
it	O
is	O
not	O
however	O
sufficient	O
in	O
practice	O
a	O
common	O
choice	O
for	O
the	O
latent-variable	O
distribution	O
is	O
given	O
by	O
exercise	O
pz	O
j	O
which	O
has	O
heavy	O
tails	O
compared	O
to	O
a	O
gaussian	B
reflecting	O
the	O
observation	O
that	O
many	O
real-world	O
distributions	O
also	O
exhibit	O
this	O
property	O
the	O
original	O
ica	O
model	O
and	O
sejnowski	O
was	O
based	O
on	O
the	O
tion	O
of	O
an	O
objective	O
function	O
defined	O
by	O
information	O
maximization	O
one	O
advantage	O
of	O
a	O
probabilistic	O
latent	B
variable	I
formulation	O
is	O
that	O
it	O
helps	O
to	O
motivate	O
and	O
late	O
generalizations	O
of	O
basic	O
ica	O
for	O
instance	O
independent	B
factor	B
analysis	I
considers	O
a	O
model	O
in	O
which	O
the	O
number	O
of	O
latent	O
and	O
observed	O
variables	O
can	O
differ	O
the	O
observed	O
variables	O
are	O
noisy	O
and	O
the	O
individual	O
latent	O
variables	O
have	O
ible	O
distributions	O
modelled	O
by	O
mixtures	O
of	O
gaussians	O
the	O
log	O
likelihood	O
for	O
this	O
model	O
is	O
maximized	O
using	O
em	B
and	O
the	O
reconstruction	O
of	O
the	O
latent	O
variables	O
is	O
proximated	O
using	O
a	O
variational	B
approach	O
many	O
other	O
types	O
of	O
model	O
have	O
been	O
considered	O
and	O
there	O
is	O
now	O
a	O
huge	O
literature	O
on	O
ica	O
and	O
its	O
applications	O
and	O
herault	O
comon	O
et	O
at	O
amari	O
et	O
at	O
pearlmutter	O
and	O
parra	O
hyvarinen	O
and	O
oja	O
hinton	O
et	O
at	O
miskin	O
and	O
mackay	O
hojen-sorensen	O
et	O
at	O
choudrey	O
and	O
roberts	O
chan	O
et	O
at	O
stone	O
autoassociative	O
neural	O
networks	O
in	O
chapter	O
we	O
considered	O
neural	O
networks	O
in	O
the	O
context	O
of	O
supervised	O
ing	O
where	O
the	O
role	O
of	O
the	O
network	O
is	O
to	O
predict	O
the	O
output	O
variables	O
given	O
values	O
nonlinear	O
latent	B
variable	I
models	O
figure	O
an	O
autoassociative	O
multilayer	B
perceptron	B
having	O
two	O
layers	O
of	O
weights	O
such	O
a	O
network	O
is	O
trained	O
to	O
map	O
input	O
vectors	O
onto	O
themselves	O
by	O
tion	O
ot	O
a	O
sum-ot-squares	O
error	B
even	O
with	O
linear	O
units	O
in	O
the	O
hidden	O
layer	O
such	O
a	O
network	O
is	O
equivalent	O
to	O
linear	O
principal	O
component	O
ysis	O
links	O
representing	O
bias	B
parameters	O
have	O
been	O
omitted	O
for	O
clarity	O
inputs	O
outputs	O
for	O
the	O
input	O
variables	O
however	O
neural	O
networks	O
have	O
also	O
been	O
applied	O
to	O
supervised	B
learning	B
where	O
they	O
have	O
been	O
used	O
for	O
dimensionality	O
reduction	O
this	O
is	O
achieved	O
by	O
using	O
a	O
network	O
having	O
the	O
same	O
number	O
of	O
outputs	O
as	O
inputs	O
and	O
optimizing	O
the	O
weights	O
so	O
as	O
to	O
minimize	O
some	O
measure	O
of	O
the	O
reconstruction	O
error	B
between	O
inputs	O
and	O
outputs	O
with	O
respect	O
to	O
a	O
set	O
of	O
training	B
data	O
consider	O
first	O
a	O
multilayer	B
perceptron	B
of	O
the	O
form	O
shown	O
in	O
figure	O
ing	O
d	O
inputs	O
d	O
output	O
units	O
and	O
m	O
hidden	O
units	O
with	O
m	O
d	O
the	O
targets	O
used	O
to	O
train	O
the	O
network	O
are	O
simply	O
the	O
input	O
vectors	O
themselves	O
so	O
that	O
the	O
network	O
is	O
attempting	O
to	O
map	O
each	O
input	O
vector	O
onto	O
itself	O
such	O
a	O
network	O
is	O
said	O
to	O
form	O
an	O
autoassociative	O
mapping	O
since	O
the	O
number	O
of	O
hidden	O
units	O
is	O
smaller	O
than	O
the	O
number	O
of	O
inputs	O
a	O
perfect	O
reconstruction	O
of	O
all	O
input	O
vectors	O
is	O
not	O
in	O
general	O
sible	O
we	O
therefore	O
determine	O
the	O
network	O
parameters	O
w	O
by	O
minimizing	O
an	O
error	B
function	I
which	O
captures	O
the	O
degree	O
of	O
mismatch	O
between	O
the	O
input	O
vectors	O
and	O
their	O
reconstructions	O
in	O
particular	O
we	O
shall	O
choose	O
a	O
sum-of-squares	B
error	B
of	O
the	O
form	O
n	O
ew	O
l	O
ilyxn	O
w	O
xn	O
nl	O
if	O
the	O
hidden	O
units	O
have	O
linear	O
activations	O
functions	O
then	O
it	O
can	O
be	O
shown	O
that	O
the	O
error	B
function	I
has	O
a	O
unique	O
global	B
minimum	I
and	O
that	O
at	O
this	O
minimum	O
the	O
network	O
performs	O
a	O
projection	O
onto	O
the	O
m	O
subspace	O
which	O
is	O
spanned	O
by	O
the	O
first	O
m	O
principal	O
components	O
of	O
the	O
data	O
and	O
kamp	O
baldi	O
and	O
hornik	O
thus	O
the	O
vectors	O
of	O
weights	O
which	O
lead	O
into	O
the	O
hidden	O
units	O
in	O
figure	O
form	O
a	O
basis	O
set	O
which	O
spans	O
the	O
principal	B
subspace	I
note	O
however	O
that	O
these	O
tors	O
need	O
not	O
be	O
orthogonal	O
or	O
normalized	O
this	O
result	O
is	O
unsurprising	O
since	O
both	O
principal	B
component	I
analysis	I
and	O
the	O
neural	B
network	I
are	O
using	O
linear	O
dimensionality	O
reduction	O
and	O
are	O
minimizing	O
the	O
same	O
sum-of-squares	B
error	B
function	I
it	O
might	O
be	O
thought	O
that	O
the	O
limitations	O
of	O
a	O
linear	O
dimensionality	O
reduction	O
could	O
be	O
overcome	O
by	O
using	O
nonlinear	O
activation	O
functions	O
for	O
the	O
hidden	O
units	O
in	O
the	O
network	O
in	O
figure	O
however	O
even	O
with	O
nonlinear	O
hidden	O
units	O
the	O
imum	O
error	B
solution	O
is	O
again	O
given	O
by	O
the	O
projection	O
onto	O
the	O
principal	O
component	O
subspace	O
and	O
kamp	O
there	O
is	O
therefore	O
no	O
advantage	O
in	O
using	O
layer	O
neural	O
networks	O
to	O
perform	O
dimensionality	O
reduction	O
standard	O
techniques	O
for	O
principal	B
component	I
analysis	I
on	O
singular	B
value	I
decomposition	I
are	O
teed	O
to	O
give	O
the	O
correct	O
solution	O
in	O
finite	O
time	O
and	O
they	O
also	O
generate	O
an	O
ordered	O
set	O
of	O
eigenvalues	O
with	O
corresponding	O
orthonormal	O
eigenvectors	O
continuous	O
latent	O
variables	O
figure	O
addition	O
of	O
extra	O
hidden	O
ers	O
of	O
noolinear	O
units	O
gives	O
an	O
auloassocialive	O
network	O
which	O
can	O
perform	O
a	O
noolinear	O
siooality	O
reduction	O
f	O
f	O
inputs	O
x	O
outputs	O
x	O
the	O
situation	O
is	O
different	O
however	O
if	O
additional	O
hidden	O
layers	O
are	O
pcrmillcd	O
in	O
the	O
network	O
consider	O
the	O
four-layer	O
autoassociativc	O
network	O
shown	O
in	O
figure	O
again	O
the	O
output	O
units	O
are	O
linear	O
and	O
the	O
m	O
units	O
in	O
the	O
second	O
hidden	O
layer	O
can	O
also	O
be	O
linear	O
however	O
the	O
first	O
and	O
third	O
hidden	O
layers	O
have	O
sigmoidal	O
nonlinear	O
tion	O
functions	O
the	O
network	O
is	O
again	O
trained	O
by	O
minimization	O
of	O
the	O
error	B
function	I
we	O
can	O
view	O
this	O
network	O
as	O
two	O
successive	O
functional	B
mappings	O
f	O
and	O
f	O
as	O
indicated	O
in	O
figure	O
the	O
first	O
mapping	O
f	O
projects	O
the	O
original	O
dimensional	O
data	O
onto	O
an	O
ai-dimensional	O
subspace	O
s	O
defined	O
by	O
the	O
activations	O
of	O
the	O
units	O
in	O
the	O
second	O
hidden	O
layer	O
because	O
of	O
the	O
presence	O
of	O
the	O
first	O
hidden	O
layer	O
of	O
nonlinear	O
units	O
this	O
mapping	O
is	O
very	O
general	O
and	O
in	O
particular	O
is	O
not	O
restricted	O
to	O
being	O
linear	O
similarly	O
the	O
second	O
half	O
of	O
the	O
network	O
defines	O
an	O
arbitrary	O
functional	B
mapping	O
from	O
the	O
m	O
space	O
back	O
into	O
the	O
original	O
d-dimensional	O
input	O
space	O
this	O
has	O
a	O
simple	O
geometrical	O
interpretation	O
as	O
indicated	O
for	O
the	O
case	O
d	O
and	O
m	O
in	O
figure	O
such	O
a	O
network	O
effectively	O
perfonns	O
a	O
nonlinear	O
principal	B
component	I
analysis	I
x	O
figure	O
geometrical	O
interpretation	O
of	O
the	O
mappings	O
performed	O
by	O
the	O
network	O
in	O
figure	O
g	O
for	O
the	O
case	O
of	O
inputs	O
and	O
ai	O
units	O
in	O
the	O
middle	O
hidden	O
layer	O
the	O
function	O
f	O
maps	O
from	O
an	O
m-dimensional	O
space	O
s	O
into	O
a	O
d-dimensiooal	O
space	O
and	O
therefore	O
defines	O
the	O
way	O
in	O
which	O
the	O
space	O
s	O
is	O
embedded	O
within	O
the	O
original	O
x-space	O
since	O
the	O
mapping	O
f	O
can	O
be	O
rillinear	O
the	O
embedding	O
s	O
can	O
be	O
nonplanar	O
as	O
indicated	O
in	O
the	O
figure	O
the	O
mapping	O
f	O
then	O
defines	O
a	O
projectiorl	O
of	O
points	O
in	O
the	O
original	O
d-dimensional	O
space	O
into	O
the	O
m	O
subspace	O
s	O
nonlinear	O
latent	B
variable	I
models	O
it	O
has	O
the	O
advantage	O
of	O
not	O
being	O
limited	O
to	O
linear	O
transformations	O
although	O
it	O
tains	O
standard	O
principal	B
component	I
analysis	I
as	O
a	O
special	O
case	O
however	O
training	B
the	O
network	O
now	O
involves	O
a	O
nonlinear	O
optimization	O
problem	O
since	O
the	O
error	B
function	I
is	O
no	O
longer	O
a	O
quadratic	O
function	O
of	O
the	O
network	O
parameters	O
ally	O
intensive	O
nonlinear	O
optimization	O
techniques	O
must	O
be	O
used	O
and	O
there	O
is	O
the	O
risk	O
of	O
finding	O
a	O
suboptimal	O
local	B
minimum	I
of	O
the	O
error	B
function	I
also	O
the	O
dimensionality	O
of	O
the	O
subspace	O
must	O
be	O
specified	O
before	O
training	B
the	O
network	O
modelling	O
nonlinear	O
manifolds	O
as	O
we	O
have	O
already	O
noted	O
many	O
natural	O
sources	O
of	O
data	O
correspond	O
to	O
dimensional	O
possibly	O
noisy	O
nonlinear	O
manifolds	O
embedded	O
within	O
the	O
higher	O
mensional	O
observed	O
data	O
space	O
capturing	O
this	O
property	O
explicitly	O
can	O
lead	O
to	O
proved	O
density	B
modelling	O
compared	O
with	O
more	O
general	O
methods	O
here	O
we	O
consider	O
briefly	O
a	O
range	O
of	O
techniques	O
that	O
attempt	O
to	O
do	O
this	O
one	O
way	O
to	O
model	O
the	O
nonlinear	O
structure	O
is	O
through	O
a	O
combination	O
of	O
linear	O
models	O
so	O
that	O
we	O
make	O
a	O
piece-wise	O
linear	O
approximation	O
to	O
the	O
manifold	B
this	O
can	O
be	O
obtained	O
for	O
instance	O
by	O
using	O
a	O
clustering	B
technique	O
such	O
as	O
k	O
based	O
on	O
euclidean	O
distance	O
to	O
partition	O
the	O
data	O
set	O
into	O
local	B
groups	O
with	O
standard	O
pca	O
plied	O
to	O
each	O
group	O
a	O
better	O
approach	O
is	O
to	O
use	O
the	O
reconstruction	O
error	B
for	O
cluster	O
assignment	O
and	O
leen	O
hinton	O
et	O
al	O
as	O
then	O
a	O
common	O
cost	B
function	I
is	O
being	O
optimized	O
in	O
each	O
stage	O
however	O
these	O
approaches	O
still	O
suffer	O
from	O
limitations	O
due	O
to	O
the	O
absence	O
of	O
an	O
overall	O
density	B
model	O
by	O
using	O
abilistic	O
pca	O
it	O
is	O
straightforward	O
to	O
define	O
a	O
fully	O
probabilistic	O
model	O
simply	O
by	O
considering	O
a	O
mixture	B
distribution	I
in	O
which	O
the	O
components	O
are	O
probabilistic	B
pca	I
models	O
and	O
bishop	O
such	O
a	O
model	O
has	O
both	O
discrete	O
latent	O
ables	O
corresponding	O
to	O
the	O
discrete	O
mixture	B
as	O
well	O
as	O
continuous	O
latent	O
variables	O
and	O
the	O
likelihood	B
function	I
can	O
be	O
maximized	O
using	O
the	O
em	B
algorithm	I
a	O
fully	O
bayesian	B
treatment	O
based	O
on	O
variational	B
inference	B
and	O
winn	O
allows	O
the	O
number	O
of	O
components	O
in	O
the	O
mixture	B
as	O
well	O
as	O
the	O
effective	O
dimensionalities	O
of	O
the	O
individual	O
models	O
to	O
be	O
inferred	O
from	O
the	O
data	O
there	O
are	O
many	O
variants	O
of	O
this	O
model	O
in	O
which	O
parameters	O
such	O
as	O
the	O
w	O
matrix	O
or	O
the	O
noise	O
variances	O
are	O
tied	O
across	O
components	O
in	O
the	O
mixture	B
or	O
in	O
which	O
the	O
isotropic	B
noise	O
distributions	O
are	O
replaced	O
by	O
diagonal	B
ones	O
giving	O
rise	O
to	O
a	O
mixture	B
of	O
factor	O
analysers	O
and	O
hinton	O
ghahramani	O
and	O
beal	O
the	O
mixture	B
of	O
probabilistic	B
pca	I
models	O
can	O
also	O
be	O
extended	B
hierarchically	O
to	O
produce	O
an	O
interactive	O
data	O
tion	O
algorithm	O
and	O
tipping	O
an	O
alternative	O
to	O
considering	O
a	O
mixture	B
of	O
linear	O
models	O
is	O
to	O
consider	O
a	O
single	O
nonlinear	O
model	O
recall	O
that	O
conventional	O
pca	O
finds	O
a	O
linear	O
subspace	O
that	O
passes	O
close	O
to	O
the	O
data	O
in	O
a	O
least-squares	O
sense	O
this	O
concept	O
can	O
be	O
extended	B
to	O
dimensional	O
nonlinear	O
surfaces	O
in	O
the	O
form	O
of	O
principal	O
curves	O
and	O
stuetzle	O
we	O
can	O
describe	O
a	O
curve	O
in	O
a	O
d-dimensional	O
data	O
space	O
using	O
a	O
vector-valued	O
function	O
f	O
which	O
is	O
a	O
vector	O
each	O
of	O
whose	O
elements	O
is	O
a	O
function	O
of	O
the	O
scalar	O
there	O
are	O
many	O
possible	O
ways	O
to	O
parameterize	O
the	O
curve	O
of	O
which	O
a	O
natural	O
choice	O
is	O
the	O
arc	B
length	O
along	O
the	O
curve	O
for	O
any	O
given	O
point	O
xin	O
data	O
space	O
we	O
can	O
find	O
the	O
point	O
on	O
the	O
curve	O
that	O
is	O
closest	O
in	O
euclidean	O
distance	O
we	O
denote	O
this	O
point	O
by	O
continuous	O
latent	O
variables	O
gfx	O
because	O
it	O
depends	O
on	O
the	O
particular	O
curve	O
f	O
for	O
a	O
continuous	O
data	O
density	B
px	O
a	O
principal	B
curve	I
is	O
defined	O
as	O
one	O
for	O
which	O
every	O
point	O
on	O
the	O
curve	O
is	O
the	O
mean	B
of	O
all	O
those	O
points	O
in	O
data	O
space	O
that	O
project	O
to	O
it	O
so	O
that	O
je	O
f	O
for	O
a	O
given	O
continuous	O
density	B
there	O
can	O
be	O
many	O
principal	O
curves	O
in	O
practice	O
we	O
are	O
interested	O
in	O
finite	O
data	O
sets	O
and	O
we	O
also	O
wish	O
to	O
restrict	O
attention	O
to	O
smooth	O
curves	O
hastie	O
and	O
stuetzle	O
propose	O
a	O
two-stage	O
iterative	O
procedure	O
for	O
ing	O
such	O
principal	O
curves	O
somewhat	O
reminiscent	O
of	O
the	O
em	B
algorithm	I
for	O
pca	O
the	O
curve	O
is	O
initialized	O
using	O
the	O
first	O
principal	O
component	O
and	O
then	O
the	O
algorithm	O
nates	O
between	O
a	O
data	O
projection	O
step	O
and	O
curve	O
re-estimation	O
step	O
in	O
the	O
projection	O
step	O
each	O
data	O
point	O
is	O
assigned	O
to	O
a	O
value	O
of	O
corresponding	O
to	O
the	O
closest	O
point	O
on	O
the	O
curve	O
then	O
in	O
the	O
re-estimation	O
step	O
each	O
point	O
on	O
the	O
curve	O
is	O
given	O
by	O
a	O
weighted	O
average	O
of	O
those	O
points	O
that	O
project	O
to	O
nearby	O
points	O
on	O
the	O
curve	O
with	O
points	O
closest	O
on	O
the	O
curve	O
given	O
the	O
greatest	O
weight	O
in	O
the	O
case	O
where	O
the	O
subspace	O
is	O
constrained	O
to	O
be	O
linear	O
the	O
procedure	O
converges	O
to	O
the	O
first	O
principal	O
component	O
and	O
is	O
equivalent	O
to	O
the	O
power	B
method	I
for	O
finding	O
the	O
largest	O
eigenvector	O
of	O
the	O
variance	B
matrix	O
principal	O
curves	O
can	O
be	O
generalized	B
to	O
multidimensional	O
manifolds	O
called	O
principal	O
surfaces	O
although	O
these	O
have	O
found	O
limited	O
use	O
due	O
to	O
the	O
difficulty	O
of	O
data	O
smoothing	O
in	O
higher	O
dimensions	O
even	O
for	O
two-dimensional	O
manifolds	O
pca	O
is	O
often	O
used	O
to	O
project	O
a	O
data	O
set	O
onto	O
a	O
lower-dimensional	O
space	O
for	O
ample	O
two	O
dimensional	O
for	O
the	O
purposes	O
of	O
visualization	B
another	O
linear	O
technique	O
with	O
a	O
similar	O
aim	O
is	O
multidimensional	B
scaling	I
or	O
mds	O
and	O
cox	O
it	O
finds	O
a	O
low-dimensional	O
projection	O
of	O
the	O
data	O
such	O
as	O
to	O
preserve	O
as	O
closely	O
as	O
possible	O
the	O
pairwise	O
distances	O
between	O
data	O
points	O
and	O
involves	O
finding	O
the	O
eigenvectors	O
of	O
the	O
distance	O
matrix	O
in	O
the	O
case	O
where	O
the	O
distances	O
are	O
euclidean	O
it	O
gives	O
equivalent	O
results	O
to	O
pca	O
the	O
mds	O
concept	O
can	O
be	O
extended	B
to	O
a	O
wide	O
variety	O
of	O
data	O
types	O
specified	O
in	O
terms	O
of	O
a	O
similarity	O
matrix	O
giving	O
nonmetric	O
mds	O
two	O
other	O
nonprobabilistic	O
methods	O
for	O
dimensionality	O
reduction	O
and	O
data	O
sualization	O
are	O
worthy	O
of	O
mention	O
locally	B
linear	I
embedding	I
or	O
lle	O
and	O
saul	O
first	O
computes	O
the	O
set	O
of	O
coefficients	O
that	O
best	O
reconstructs	O
each	O
data	O
point	O
from	O
its	O
neighbours	O
these	O
coefficients	O
are	O
arranged	O
to	O
be	O
invariant	O
to	O
tions	O
translations	O
and	O
scalings	O
of	O
that	O
data	O
point	O
and	O
its	O
neighbours	O
and	O
hence	O
they	O
characterize	O
the	O
local	B
geometrical	O
properties	O
of	O
the	O
neighbourhood	O
lle	O
then	O
maps	O
the	O
high-dimensional	O
data	O
points	O
down	O
to	O
a	O
lower	O
dimensional	O
space	O
while	O
if	O
the	O
local	B
neighbourhood	O
for	O
a	O
particular	O
ing	O
these	O
neighbourhood	O
coefficients	O
data	O
point	O
can	O
be	O
considered	O
linear	O
then	O
the	O
transformation	O
can	O
be	O
achieved	O
using	O
a	O
combination	O
of	O
translation	O
rotation	O
and	O
scaling	O
such	O
as	O
to	O
preserve	O
the	O
angles	O
formed	O
between	O
the	O
data	O
points	O
and	O
their	O
neighbours	O
because	O
the	O
weights	O
are	O
variant	O
to	O
these	O
transformations	O
we	O
expect	O
the	O
same	O
weight	O
values	O
to	O
reconstruct	O
the	O
data	O
points	O
in	O
the	O
low-dimensional	O
space	O
as	O
in	O
the	O
high-dimensional	O
data	O
space	O
in	O
spite	O
of	O
the	O
nonlinearity	O
the	O
optimization	O
for	O
lle	O
does	O
not	O
exhibit	O
local	B
minima	O
in	O
isometric	O
feature	O
mapping	O
or	O
isomap	B
et	O
ai	O
the	O
goal	O
is	O
to	O
project	O
the	O
data	O
to	O
a	O
lower-dimensional	O
space	O
using	O
mds	O
but	O
where	O
the	O
ilarities	O
are	O
defined	O
in	O
terms	O
of	O
the	O
geodesic	O
distances	O
measured	O
along	O
the	O
mani	O
nonlinear	O
latent	B
variable	I
models	O
fold	O
for	O
instance	O
if	O
two	O
points	O
lie	O
on	O
a	O
circle	O
then	O
the	O
geodesic	O
is	O
the	O
arc-length	O
distance	O
measured	O
around	O
the	O
circumference	O
of	O
the	O
circle	O
not	O
the	O
straight	O
line	O
tance	O
measured	O
along	O
the	O
chord	O
connecting	O
them	O
the	O
algorithm	O
first	O
defines	O
the	O
neighbourhood	O
for	O
each	O
data	O
point	O
either	O
by	O
finding	O
the	O
k	B
nearest	I
neighbours	I
or	O
by	O
finding	O
all	O
points	O
within	O
a	O
sphere	O
of	O
radius	O
e	O
a	O
graph	O
is	O
then	O
constructed	O
by	O
ing	O
all	O
neighbouring	O
points	O
and	O
labelling	O
them	O
with	O
their	O
euclidean	O
distance	O
the	O
geodesic	B
distance	I
between	O
any	O
pair	O
of	O
points	O
is	O
then	O
approximated	O
by	O
the	O
sum	O
of	O
the	O
arc	B
lengths	O
along	O
the	O
shortest	O
path	O
connecting	O
them	O
itself	O
is	O
found	O
using	O
standard	O
algorithms	O
finally	O
metric	O
mds	O
is	O
applied	O
to	O
the	O
geodesic	B
distance	I
matrix	O
to	O
find	O
the	O
low-dimensional	O
projection	O
our	O
focus	O
in	O
this	O
chapter	O
has	O
been	O
on	O
models	O
for	O
which	O
the	O
observed	O
ables	O
are	O
continuous	O
we	O
can	O
also	O
consider	O
models	O
having	O
continuous	O
latent	O
ables	O
together	O
with	O
discrete	O
observed	O
variables	O
giving	O
rise	O
to	O
latent	O
trait	O
models	O
in	O
this	O
case	O
the	O
marginalization	O
over	O
the	O
continuous	O
latent	O
variables	O
even	O
for	O
a	O
linear	O
relationship	O
between	O
latent	O
and	O
observed	O
variables	O
not	O
be	O
performed	O
analytically	O
and	O
so	O
more	O
sophisticated	O
techniques	O
are	O
required	O
tipping	O
uses	O
variational	B
inference	B
in	O
a	O
model	O
with	O
a	O
two-dimensional	O
latent	O
space	O
allowing	O
a	O
binary	O
data	O
set	O
to	O
be	O
visualized	O
analogously	O
to	O
the	O
use	O
of	O
pca	O
to	O
visualize	O
continuous	O
data	O
note	O
that	O
this	O
model	O
is	O
the	O
dual	O
of	O
the	O
bayesian	B
logistic	B
regression	B
problem	O
discussed	O
in	O
section	O
in	O
the	O
case	O
of	O
logistic	B
regression	B
we	O
have	O
n	O
observations	O
of	O
the	O
feature	O
vector	O
which	O
are	O
parameterized	O
by	O
a	O
single	O
parameter	O
vector	O
w	O
whereas	O
in	O
the	O
latent	O
space	O
visualization	B
model	O
there	O
is	O
a	O
single	O
latent	O
space	O
variable	O
x	O
to	O
and	O
n	O
copies	O
of	O
the	O
latent	B
variable	I
w	O
n	O
a	O
generalization	B
of	O
probabilistic	O
latent	B
variable	I
models	O
to	O
general	O
exponential	B
family	I
distributions	O
is	O
described	O
in	O
collins	O
et	O
al	O
we	O
have	O
already	O
noted	O
that	O
an	O
arbitrary	O
distribution	O
can	O
be	O
formed	O
by	O
taking	O
a	O
gaussian	B
random	O
variable	O
and	O
transforming	O
it	O
through	O
a	O
suitable	O
nonlinearity	O
this	O
is	O
exploited	O
in	O
a	O
general	O
latent	B
variable	I
model	O
called	O
a	O
density	B
network	I
mackay	O
and	O
gibbs	B
in	O
which	O
the	O
nonlinear	O
function	O
is	O
governed	O
by	O
a	O
multilayered	O
neural	B
network	I
if	O
the	O
network	O
has	O
enough	O
hidden	O
units	O
it	O
can	O
imate	O
a	O
given	O
nonlinear	O
function	O
to	O
any	O
desired	O
accuracy	O
the	O
downside	O
of	O
having	O
such	O
a	O
flexible	O
model	O
is	O
that	O
the	O
marginalization	O
over	O
the	O
latent	O
variables	O
required	O
in	O
order	O
to	O
obtain	O
the	O
likelihood	B
function	I
is	O
no	O
longer	O
analytically	O
tractable	O
instead	O
the	O
likelihood	O
is	O
approximated	O
using	O
monte	O
carlo	O
techniques	O
by	O
drawing	O
samples	O
from	O
the	O
gaussian	B
prior	B
the	O
marginalization	O
over	O
the	O
latent	O
variables	O
then	O
becomes	O
a	O
simple	O
sum	O
with	O
one	O
term	O
for	O
each	O
sample	O
however	O
because	O
a	O
large	O
number	O
of	O
sample	O
points	O
may	O
be	O
required	O
in	O
order	O
to	O
give	O
an	O
accurate	O
representation	O
of	O
the	O
marginal	B
this	O
procedure	O
can	O
be	O
computationally	O
costly	O
if	O
we	O
consider	O
more	O
restricted	O
forms	O
for	O
the	O
nonlinear	O
function	O
and	O
make	O
an	O
propriate	O
choice	O
of	O
the	O
latent	B
variable	I
distribution	O
then	O
we	O
can	O
construct	O
a	O
latent	O
able	O
model	O
that	O
is	O
both	O
nonlinear	O
and	O
efficient	O
to	O
train	O
the	O
generative	B
topographic	I
mapping	I
or	O
gtm	O
et	O
ai	O
bishop	O
et	O
ai	O
bishop	O
et	O
ai	O
uses	O
a	O
latent	O
distribution	O
that	O
is	O
defined	O
by	O
a	O
finite	O
regular	O
grid	O
of	O
delta	O
functions	O
over	O
the	O
two-dimensional	O
latent	O
space	O
marginalization	O
over	O
the	O
latent	O
space	O
then	O
simply	O
involves	O
summing	O
over	O
the	O
contributions	O
from	O
each	O
of	O
the	O
grid	O
locations	O
chapter	O
chapter	O
jj	O
continuous	O
latent	O
va	O
ot	O
trle	O
oillkyw	O
wllisualiz	O
ed	O
using	O
pea	O
on	O
the	O
left	O
and	O
gtm	O
on	O
itle	O
ngrt	O
fof	O
tile	O
gtm	O
flliu	O
e	O
model	O
each	O
poinils	O
plollfld	O
at	O
tile	O
mean	B
ot	O
its	O
posmk	O
dislribution	O
in	O
sace	O
tile	O
mlhe	O
gtm	O
sepamlion	O
betwoon	O
the	O
groups	O
of	O
data	O
points	O
to	O
be	O
ckl	O
arfy	O
chllf	O
j	O
setioo	O
the	O
noliotar	O
mapping	O
is	O
gien	O
by	O
a	O
linear	B
regression	B
model	O
thai	O
allow	O
for	O
general	O
iiollinearily	O
while	O
being	O
a	O
linear	O
fuoction	O
of	O
tile	O
adapli-e	O
parameler	O
noie	O
thai	O
tilt	O
usual	O
limitation	O
of	O
linear	B
regression	B
models	O
arising	O
from	O
the	O
en	O
of	O
dimeniooalily	O
does	O
arise	O
in	O
the	O
of	O
lhe	O
gti	O
sie	O
the	O
generall	O
ha	O
to	O
diltn	O
sions	O
irrespecti-e	O
of	O
the	O
dimensionality	O
of	O
the	O
data	O
space	O
a	O
coo	O
nce	O
of	O
illese	O
cooices	O
is	O
that	O
the	O
likelihood	O
funclion	O
can	O
be	O
epressed	O
analytically	O
in	O
dosed	O
form	O
and	O
can	O
be	O
optimilc	O
efficiently	O
oing	O
the	O
em	B
algorithm	I
the	O
resolting	O
gtm	O
model	O
his	O
a	O
lwo-dimensional	O
nonlinear	O
manifold	B
tile	O
dala	O
sel	O
and	O
by	O
ealualing	O
the	O
posterior	O
distriljlion	O
latent	O
space	O
for	O
the	O
data	O
poi	O
they	O
can	O
he	O
projecttj	O
back	O
to	O
the	O
lalent	O
for	O
purposes	O
figure	O
sls	O
a	O
comparison	O
of	O
the	O
oil	O
wilh	O
lincar	O
pea	O
and	O
wilh	O
lhe	O
iiolhnear	O
gti	O
tilt	O
gtm	O
can	O
be	O
seen	O
as	O
a	O
probabilistic	O
of	O
an	O
earlier	O
nlodl	O
callm	O
the	O
orgnidng	O
or	O
som	O
kobonen	O
which	O
also	O
represents	O
a	O
iwo-dimensiooal	O
iiollincar	O
manifoid	O
as	O
a	O
regular	O
array	O
of	O
discle	O
points	O
the	O
som	O
i	O
somewhat	O
reminnt	O
of	O
the	O
k	O
trlcan	O
algorithm	O
in	O
that	O
data	O
points	O
are	O
a	O
igr	O
ed	O
to	O
nearby	O
prololjc	O
thai	O
are	O
lhen	O
subscjuenlly	O
updale	O
initially	O
lhe	O
proioijls	O
are	O
distribuled	O
at	O
random	O
and	O
during	O
the	O
training	B
process	O
they	O
organize	O
so	O
as	O
to	O
aplroimalea	O
smoolh	O
manifold	B
unlike	O
k	O
howee	O
the	O
som	O
is	O
tioi	O
optimizing	O
any	O
well	O
ddine	O
cost	B
function	I
al	O
making	O
difficult	O
to	O
s	O
the	O
parameters	O
of	O
the	O
model	O
and	O
assess	O
con-ergence	O
there	O
i	O
also	O
no	O
guarantee	O
that	O
the	O
will	O
take	O
place	O
this	O
is	O
depennl	O
the	O
choice	O
of	O
appropriate	O
paranlttcr	O
f	O
any	O
particular	O
data	O
sel	O
by	O
oofitrast	O
gtm	O
optimize	O
the	O
log	O
likelihood	O
functioo	O
and	O
the	O
resolting	O
model	O
define	O
a	O
probabilily	O
denity	O
in	O
dma	O
in	O
fael	O
il	O
correponds	O
to	O
a	O
conmincd	O
miture	O
of	O
gaussian	B
in	O
which	O
the	O
component	O
a	O
conlnlon	O
nd	O
the	O
mean	B
are	O
contrained	O
to	O
lie	O
on	O
a	O
tw-o-diitlcniooal	O
this	O
proba	O
section	O
exercises	O
appendix	O
e	O
exercises	O
bilistic	O
foundation	O
also	O
makes	O
it	O
very	O
straightforward	O
to	O
define	O
generalizations	O
of	O
gtm	O
et	O
al	O
such	O
as	O
a	O
bayesian	B
treatment	O
dealing	O
with	O
missing	O
values	O
a	O
principled	O
extension	O
to	O
discrete	O
variables	O
the	O
use	O
of	O
gaussian	B
processes	O
to	O
define	O
the	O
manifold	B
or	O
a	O
hierarchical	B
gtm	O
model	O
and	O
nabney	O
because	O
the	O
manifold	B
in	O
gtm	O
is	O
defined	O
as	O
a	O
continuous	O
surface	O
not	O
just	O
at	O
the	O
prototype	O
vectors	O
as	O
in	O
the	O
som	O
it	O
is	O
possible	O
to	O
compute	O
the	O
magnification	O
factors	O
corresponding	O
to	O
the	O
local	B
expansions	O
and	O
compressions	O
of	O
the	O
manifold	B
needed	O
to	O
fit	O
the	O
data	O
set	O
et	O
al	O
as	O
well	O
as	O
the	O
directional	O
curvatures	O
of	O
the	O
manifold	B
et	O
al	O
these	O
can	O
be	O
visualized	O
along	O
with	O
the	O
projected	O
data	O
and	O
provide	O
additional	O
insight	O
into	O
the	O
model	O
lib	O
in	O
this	O
exercise	O
we	O
use	O
proof	O
by	O
induction	O
to	O
show	O
that	O
the	O
linear	O
projection	O
onto	O
an	O
m	O
subspace	O
that	O
maximizes	O
the	O
variance	B
of	O
the	O
jected	O
data	O
is	O
defined	O
by	O
the	O
m	O
eigenvectors	O
of	O
the	O
data	O
covariance	B
matrix	O
s	O
given	O
by	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
in	O
section	O
this	O
result	O
was	O
proven	O
for	O
the	O
case	O
of	O
m	O
now	O
suppose	O
the	O
result	O
holds	O
for	O
some	O
general	O
value	O
of	O
m	O
and	O
show	O
that	O
it	O
consequently	O
holds	O
for	O
dimensionality	O
m	O
to	O
do	O
this	O
first	O
set	O
the	O
derivative	B
of	O
the	O
variance	B
of	O
the	O
projected	O
data	O
with	O
respect	O
to	O
a	O
vector	O
defining	O
the	O
new	O
direction	O
in	O
data	O
space	O
equal	O
to	O
zero	O
this	O
should	O
be	O
done	O
subject	O
to	O
the	O
constraints	O
that	O
um	O
be	O
orthogonal	O
to	O
the	O
existing	O
vectors	O
um	O
and	O
also	O
that	O
it	O
be	O
normalized	O
to	O
unit	O
length	O
use	O
lagrange	B
multipliers	O
to	O
enforce	O
these	O
constraints	O
then	O
make	O
use	O
of	O
the	O
orthonormality	O
properties	O
of	O
the	O
vectors	O
um	O
to	O
show	O
that	O
the	O
new	O
vector	O
is	O
an	O
eigenvector	O
of	O
s	O
finally	O
show	O
that	O
the	O
variance	B
is	O
maximized	O
if	O
the	O
eigenvector	O
is	O
chosen	O
to	O
be	O
the	O
one	O
corresponding	O
to	O
eigenvector	O
where	O
the	O
eigenvalues	O
have	O
been	O
ordered	O
in	O
decreasing	O
value	O
show	O
that	O
the	O
minimum	O
value	O
of	O
the	O
pca	O
distortion	B
measure	I
j	O
given	O
by	O
with	O
respect	O
to	O
the	O
ui	O
subject	O
to	O
the	O
orthonormality	O
constraints	O
is	O
obtained	O
when	O
the	O
ui	O
are	O
eigenvectors	O
of	O
the	O
data	O
covariance	B
matrix	O
s	O
to	O
do	O
this	O
introduce	O
a	O
matrix	O
h	O
of	O
lagrange	B
multipliers	O
one	O
for	O
each	O
constraint	O
so	O
that	O
the	O
modified	O
distortion	B
measure	I
in	O
matrix	O
notation	O
reads	O
tr	O
utsu	O
tr	O
hi	O
utu	O
where	O
uis	O
a	O
mtrix	O
of	O
dimensiod	O
x	O
m	O
whose	O
columns	O
are	O
gi	O
en	O
b	O
ui	O
now	O
minimize	O
j	O
with	O
respect	O
to	O
u	O
and	O
show	O
that	O
the	O
sution	O
satisfies	O
su	O
uh	O
clearly	O
one	O
possible	O
solution	O
is	O
that	O
the	O
columns	O
of	O
u	O
are	O
eigenvectors	O
of	O
s	O
in	O
which	O
case	O
h	O
is	O
a	O
diagonal	B
matrix	O
containing	O
the	O
corresponding	O
eigenvalues	O
to	O
obtain	O
the	O
general	O
solution	O
show	O
that	O
h	O
can	O
be	O
assumed	O
to	O
be	O
a	O
symmetr	O
maix	O
and	O
by	O
using	O
its	O
eigenvect	O
r	O
expansion	O
show	O
that	O
the	O
general	O
solution	O
to	O
su	O
gives	O
the	O
same	O
value	O
for	O
j	O
as	O
the	O
specific	O
solution	O
in	O
which	O
the	O
columns	O
of	O
u	O
are	O
continuous	O
latent	O
variables	O
the	O
eigenvectors	O
of	O
s	O
because	O
these	O
solutions	O
are	O
all	O
equivalent	O
it	O
is	O
convenient	O
to	O
choose	O
the	O
eigenvector	O
solution	O
verify	O
that	O
the	O
eigenvectors	O
defined	O
by	O
are	O
normalized	O
to	O
unit	O
length	O
assuming	O
that	O
the	O
eigenvectors	O
vi	O
have	O
unit	O
length	O
imm	O
suppose	O
we	O
replace	O
the	O
zero-mean	O
unit-covariance	O
latent	O
space	O
bution	O
in	O
the	O
probabilistic	B
pca	I
model	O
by	O
a	O
general	O
gaussian	B
distribution	O
of	O
the	O
formnzlm	O
by	O
redefining	O
the	O
parameters	O
of	O
the	O
model	O
show	O
that	O
this	O
leads	O
to	O
an	O
identical	O
model	O
for	O
the	O
marginal	B
distribution	O
px	O
over	O
the	O
observed	O
variables	O
for	O
any	O
valid	O
choice	O
of	O
m	O
and	O
let	O
x	O
be	O
a	O
d-dimensional	O
random	O
variable	O
having	O
a	O
gaussian	B
distribution	O
given	O
by	O
nxijl	O
and	O
consider	O
the	O
m-dimensional	O
random	O
variable	O
given	O
by	O
y	O
ax	O
b	O
where	O
a	O
is	O
an	O
m	O
x	O
d	O
matrix	O
show	O
that	O
y	O
also	O
has	O
a	O
gaussian	B
distribution	O
and	O
find	O
expressions	O
for	O
its	O
mean	B
and	O
covariance	B
discuss	O
the	O
form	O
of	O
this	O
gaussian	B
distribution	O
for	O
m	O
d	O
for	O
m	O
d	O
and	O
for	O
m	O
d	O
imm	O
draw	O
a	O
directed	B
probabilistic	O
graph	O
for	O
the	O
probabilistic	B
pca	I
model	O
described	O
in	O
section	O
in	O
which	O
the	O
components	O
of	O
the	O
observed	B
variable	I
x	O
are	O
shown	O
explicitly	O
as	O
separate	O
nodes	O
hence	O
verify	O
that	O
the	O
probabilistic	B
pca	I
model	O
has	O
the	O
same	O
independence	O
structure	O
as	O
the	O
naive	B
bayes	B
model	I
discussed	O
in	O
tion	O
by	O
making	O
use	O
of	O
the	O
results	O
and	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
a	O
general	O
distribution	O
derive	O
the	O
result	O
for	O
the	O
marginal	B
distribution	O
px	O
in	O
the	O
probabilistic	B
pca	I
model	O
by	O
making	O
use	O
of	O
the	O
result	O
show	O
that	O
the	O
posterior	O
distribution	O
pzlx	O
for	O
the	O
probabilistic	B
pca	I
model	O
is	O
given	O
by	O
verify	O
that	O
maximizing	O
the	O
log	O
likelihood	O
for	O
the	O
probabilistic	B
pca	I
model	O
with	O
respect	O
to	O
the	O
parameter	O
jl	O
gives	O
the	O
result	O
jlml	O
x	O
where	O
x	O
is	O
the	O
mean	B
of	O
the	O
data	O
vectors	O
by	O
evaluating	O
the	O
second	O
derivatives	O
of	O
the	O
log	O
likelihood	B
function	I
for	O
the	O
probabilistic	B
pca	I
model	O
with	O
respect	O
to	O
the	O
parameter	O
jl	O
show	O
that	O
the	O
stationary	B
point	O
jlml	O
x	O
represents	O
the	O
unique	O
maximum	O
imm	O
show	O
that	O
in	O
the	O
limit	O
the	O
posterior	O
mean	B
for	O
the	O
probabilistic	B
pca	I
model	O
becomes	O
an	O
orthogonal	O
projection	O
onto	O
the	O
principal	B
subspace	I
as	O
in	O
conventional	O
pca	O
for	O
show	O
that	O
the	O
posterior	O
mean	B
in	O
the	O
probabilistic	B
pca	I
model	O
is	O
shifted	O
towards	O
the	O
origin	O
relative	B
to	O
the	O
orthogonal	O
projection	O
show	O
that	O
the	O
optimal	O
reconstruction	O
of	O
a	O
data	O
point	O
under	O
probabilistic	B
pca	I
according	O
to	O
the	O
least	O
squares	O
projection	O
cost	O
of	O
conventional	O
pca	O
is	O
given	O
by	O
exercises	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
covariance	B
matrix	O
for	O
the	O
bilistic	O
pca	O
model	O
with	O
an	O
m	O
latent	O
space	O
and	O
a	O
d-dimensional	O
data	O
space	O
is	O
given	O
by	O
verify	O
that	O
in	O
the	O
case	O
of	O
m	O
d	O
the	O
number	O
of	O
independent	B
parameters	O
is	O
the	O
same	O
as	O
in	O
a	O
general	O
covariance	B
gaussian	B
whereas	O
for	O
m	O
it	O
is	O
the	O
same	O
as	O
for	O
a	O
gaussian	B
with	O
an	O
isotropic	B
covariance	B
iiii	O
i	O
derive	O
the	O
m-step	O
equations	O
and	O
for	O
the	O
probabilistic	B
pca	I
model	O
by	O
maximization	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	B
function	I
given	O
by	O
in	O
figure	O
we	O
showed	O
an	O
application	O
of	O
probabilistic	B
pca	I
to	O
a	O
data	O
set	O
in	O
which	O
some	O
of	O
the	O
data	O
values	O
were	O
missing	B
at	I
random	I
derive	O
the	O
em	B
algorithm	I
for	O
maximizing	O
the	O
likelihood	B
function	I
for	O
the	O
probabilistic	B
pca	I
model	O
in	O
this	O
ation	O
note	O
that	O
the	O
as	O
well	O
as	O
the	O
missing	B
data	I
values	O
that	O
are	O
components	O
of	O
the	O
vectors	O
n	O
are	O
now	O
latent	O
variables	O
show	O
that	O
in	O
the	O
special	O
case	O
in	O
which	O
all	O
of	O
the	O
data	O
values	O
are	O
observed	O
this	O
reduces	O
to	O
the	O
em	B
algorithm	I
for	O
probabilistic	B
pca	I
derived	O
in	O
section	O
iiii	O
i	O
let	O
w	O
be	O
a	O
d	O
x	O
m	O
matrix	O
whose	O
columns	O
define	O
a	O
linear	O
subspace	O
of	O
dimensionality	O
m	O
embedded	O
within	O
a	O
data	O
space	O
of	O
dimensionality	O
d	O
and	O
let	O
be	O
a	O
d-dimensional	O
vector	O
given	O
a	O
data	O
set	O
n	O
where	O
n	O
n	O
we	O
can	O
approximate	O
the	O
data	O
points	O
using	O
a	O
linear	O
mapping	O
from	O
a	O
set	O
of	O
m	O
vectors	O
so	O
that	O
xn	O
is	O
approximated	O
by	O
w	O
zn	O
the	O
associated	O
squares	O
reconstruction	O
cost	O
is	O
given	O
by	O
n	O
j	O
l	O
ilxn	O
wzn	O
nl	O
first	O
show	O
that	O
minimizing	O
j	O
with	O
respect	O
to	O
to	O
an	O
analogous	O
expression	O
with	O
x	O
n	O
and	O
zn	O
replaced	O
by	O
zero-mean	O
variables	O
x	O
n	O
x	O
and	O
zn	O
z	O
respectively	O
where	O
x	O
and	O
z	O
denote	O
sample	O
means	O
then	O
show	O
that	O
minimizing	O
j	O
with	O
respect	O
to	O
zn	O
where	O
w	O
is	O
kept	O
fixed	O
gives	O
rise	O
to	O
the	O
pca	O
estep	O
and	O
that	O
minimizing	O
j	O
with	O
respect	O
to	O
w	O
where	O
is	O
kept	O
fixed	O
gives	O
rise	O
to	O
the	O
pca	O
m	O
step	O
derive	O
an	O
expression	O
for	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
factor	B
analysis	I
model	O
described	O
in	O
section	O
iiii	O
i	O
show	O
that	O
the	O
factor	B
analysis	I
model	O
described	O
in	O
section	O
is	O
invariant	O
under	O
rotations	O
of	O
the	O
latent	O
space	O
coordinates	O
by	O
considering	O
second	O
derivatives	O
show	O
that	O
the	O
only	O
stationary	B
point	O
of	O
the	O
log	O
likelihood	B
function	I
for	O
the	O
factor	B
analysis	I
model	O
discussed	O
in	O
section	O
with	O
respect	O
to	O
the	O
parameter	O
is	O
given	O
by	O
the	O
sample	B
mean	B
defined	O
by	O
furthermore	O
show	O
that	O
this	O
stationary	B
point	O
is	O
a	O
maximum	O
derive	O
the	O
formulae	O
and	O
for	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
for	O
factor	B
analysis	I
note	O
that	O
from	O
the	O
result	O
of	O
exercise	O
the	O
parameter	O
can	O
be	O
replaced	O
by	O
the	O
sample	B
mean	B
x	O
continuous	O
latent	O
variables	O
write	O
down	O
an	O
expression	O
for	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
tion	O
for	O
the	O
factor	B
analysis	I
model	O
and	O
hence	O
derive	O
the	O
corresponding	O
m	O
step	O
tions	O
and	O
iii	O
i	O
draw	O
a	O
directed	B
probabilistic	O
graphical	B
model	I
representing	O
a	O
discrete	O
mixture	B
of	O
probabilistic	B
pca	I
models	O
in	O
which	O
each	O
pca	O
model	O
has	O
its	O
own	O
values	O
of	O
w	O
jl	O
and	O
now	O
draw	O
a	O
modified	O
graph	O
in	O
which	O
these	O
parameter	O
values	O
are	O
shared	O
between	O
the	O
components	O
of	O
the	O
mixture	B
we	O
saw	O
in	O
section	O
that	O
students	O
t-distribution	O
can	O
be	O
viewed	O
as	O
an	O
infinite	O
mixture	B
of	I
gaussians	I
in	O
which	O
we	O
marginalize	O
with	O
respect	O
to	O
a	O
ous	O
latent	B
variable	I
by	O
exploiting	O
this	O
representation	O
formulate	O
an	O
em	B
algorithm	I
for	O
maximizing	O
the	O
log	O
likelihood	B
function	I
for	O
a	O
multivariate	O
students	O
t-distribution	O
given	O
an	O
observed	O
set	O
of	O
data	O
points	O
and	O
derive	O
the	O
forms	O
of	O
the	O
e	O
and	O
m	O
step	O
tions	O
iii	O
i	O
consider	O
a	O
linear-gaussian	O
latent-variable	O
model	O
having	O
a	O
latent	O
space	O
distribution	O
pz	O
nxio	O
i	O
and	O
a	O
conditional	B
distribution	O
for	O
the	O
observed	O
able	O
pxlz	O
nxlwz	O
il	O
where	O
is	O
an	O
arbitrary	O
symmetric	O
definite	O
noise	O
covariance	B
matrix	O
now	O
suppose	O
that	O
we	O
make	O
a	O
nonsingular	O
linear	O
transformation	O
of	O
the	O
data	O
variables	O
x	O
ax	O
where	O
a	O
is	O
a	O
d	O
x	O
d	O
matrix	O
if	O
jlml	O
w	O
ml	O
and	O
represent	O
the	O
maximum	B
likelihood	I
solution	O
corresponding	O
to	O
the	O
original	O
untransformed	O
data	O
show	O
that	O
ajlml	O
awml	O
and	O
a	O
will	O
resent	O
the	O
corresponding	O
maximum	B
likelihood	I
solution	O
for	O
the	O
transformed	O
data	O
set	O
finally	O
show	O
that	O
the	O
form	O
of	O
the	O
model	O
is	O
preserved	O
in	O
two	O
cases	O
a	O
is	O
a	O
diagonal	B
matrix	O
and	O
is	O
a	O
diagonal	B
matrix	O
this	O
corresponds	O
to	O
the	O
case	O
of	O
factor	B
analysis	I
the	O
transformed	O
remains	O
diagonal	B
and	O
hence	O
factor	B
analysis	I
is	O
covariant	O
under	O
component-wise	O
re-scaling	O
of	O
the	O
data	O
variables	O
a	O
is	O
orthogonal	O
and	O
is	O
this	O
corresponds	O
to	O
probabilistic	B
pca	I
portional	O
to	O
the	O
unit	O
matrix	O
so	O
that	O
transformed	O
matrix	O
remains	O
proportional	O
to	O
the	O
unit	O
matrix	O
and	O
hence	O
bilistic	O
pca	O
is	O
covariant	O
under	O
a	O
rotation	O
of	O
the	O
axes	O
of	O
data	O
space	O
as	O
is	O
the	O
case	O
for	O
conventional	O
pca	O
show	O
that	O
any	O
vector	O
ai	O
that	O
satisfies	O
will	O
also	O
satisfy	O
also	O
show	O
that	O
for	O
any	O
solution	O
of	O
having	O
eigenvalue	O
a	O
we	O
can	O
add	O
any	O
multiple	O
of	O
an	O
eigenvector	O
of	O
k	O
having	O
zero	O
eigenvalue	O
and	O
obtain	O
a	O
solution	O
to	O
that	O
also	O
has	O
eigenvalue	O
a	O
finally	O
show	O
that	O
such	O
modifications	O
do	O
not	O
affect	O
the	O
principal-component	O
projection	O
given	O
by	O
show	O
that	O
the	O
conventional	O
linear	O
pca	O
algorithm	O
is	O
recovered	O
as	O
a	O
special	O
case	O
of	O
kernel	B
pca	I
if	O
we	O
choose	O
the	O
linear	O
kernel	B
function	I
given	O
by	O
kx	O
x	O
x	O
t	O
x	O
iii	O
i	O
use	O
the	O
transformation	O
property	O
of	O
a	O
probability	B
density	B
under	O
a	O
change	O
of	O
variable	O
to	O
show	O
that	O
any	O
density	B
py	O
can	O
be	O
obtained	O
from	O
a	O
fixed	O
density	B
qx	O
that	O
is	O
everywhere	O
nonzero	O
by	O
making	O
a	O
nonlinear	O
change	O
of	O
variable	O
y	O
fx	O
in	O
which	O
fx	O
is	O
a	O
monotonic	O
function	O
so	O
that	O
jx	O
write	O
down	O
the	O
differential	B
equation	O
satisfied	O
by	O
f	O
and	O
draw	O
a	O
diagram	O
illustrating	O
the	O
transformation	O
of	O
the	O
density	B
exercises	O
suppose	O
that	O
two	O
variables	O
zl	O
and	O
are	O
independent	B
so	O
thatpzl	O
show	O
that	O
the	O
covariance	B
matrix	O
between	O
these	O
variables	O
is	O
diagonal	B
this	O
shows	O
that	O
independence	O
is	O
a	O
sufficient	O
condition	O
for	O
two	O
variables	O
to	O
be	O
correlated	O
now	O
consider	O
two	O
variables	O
yl	O
and	O
in	O
which	O
yl	O
and	O
yg	O
write	O
down	O
the	O
conditional	B
distribution	O
and	O
observe	O
that	O
this	O
is	O
dependent	O
on	O
yb	O
showing	O
that	O
the	O
two	O
variables	O
are	O
not	O
independent	B
now	O
show	O
that	O
the	O
covariance	B
matrix	O
between	O
these	O
two	O
variables	O
is	O
again	O
diagonal	B
to	O
do	O
this	O
use	O
the	O
relation	O
pyl	O
pyi	O
to	O
show	O
that	O
the	O
off-diagonal	O
terms	O
are	O
zero	O
this	O
counter-example	O
shows	O
that	O
zero	O
correlation	O
is	O
not	O
a	O
sufficient	O
condition	O
for	O
independence	O
sequential	B
data	I
so	O
far	O
in	O
this	O
book	O
we	O
have	O
focussed	O
primarily	O
on	O
sets	O
of	O
data	O
points	O
that	O
were	O
assumed	O
to	O
be	O
independent	B
and	O
identically	O
distributed	O
this	O
assumption	O
allowed	O
us	O
to	O
express	O
the	O
likelihood	B
function	I
as	O
the	O
product	O
over	O
all	O
data	O
points	O
of	O
the	O
probability	B
distribution	O
evaluated	O
at	O
each	O
data	O
point	O
for	O
many	O
applications	O
however	O
the	O
i	O
i	O
d	O
assumption	O
will	O
be	O
a	O
poor	O
one	O
here	O
we	O
consider	O
a	O
particularly	O
important	O
class	O
of	O
such	O
data	O
sets	O
namely	O
those	O
that	O
describe	O
sequential	B
data	I
these	O
often	O
arise	O
through	O
measurement	O
of	O
time	O
series	O
for	O
example	O
the	O
rainfall	O
measurements	O
on	O
successive	O
days	O
at	O
a	O
particular	O
location	O
or	O
the	O
daily	O
values	O
of	O
a	O
currency	O
exchange	O
rate	O
or	O
the	O
acoustic	O
features	O
at	O
successive	O
time	O
frames	O
used	O
for	O
speech	B
recognition	I
an	O
example	O
involving	O
speech	O
data	O
is	O
shown	O
in	O
figure	O
sequential	B
data	I
can	O
also	O
arise	O
in	O
contexts	O
other	O
than	O
time	O
series	O
for	O
example	O
the	O
sequence	O
of	O
nucleotide	O
base	O
pairs	O
along	O
a	O
strand	O
of	O
dna	B
or	O
the	O
sequence	O
of	O
characters	O
in	O
an	O
english	O
sentence	O
for	O
convenience	O
we	O
shall	O
sometimes	O
refer	O
to	O
past	O
and	O
future	O
observations	O
in	O
a	O
sequence	O
however	O
the	O
models	O
explored	O
in	O
this	O
chapter	O
are	O
equally	O
applicable	O
to	O
all	O
sequential	B
data	I
figure	O
example	O
of	O
a	O
spectrogram	B
of	O
the	O
spoken	O
words	O
bayes	B
theorem	O
showing	O
a	O
plot	O
of	O
the	O
intensity	O
of	O
the	O
spectral	O
coefficients	O
versus	O
time	O
index	O
forms	O
of	O
sequential	B
data	I
not	O
just	O
temporal	O
sequences	O
it	O
is	O
useful	O
to	O
distinguish	O
between	O
stationary	B
and	O
nonstationary	O
sequential	O
distributions	O
in	O
the	O
stationary	B
case	O
the	O
data	O
evolves	O
in	O
time	O
but	O
the	O
distribution	O
from	O
which	O
it	O
is	O
generated	O
remains	O
the	O
same	O
for	O
the	O
more	O
complex	O
nonstationary	O
situation	O
the	O
generative	O
distribution	O
itself	O
is	O
evolving	O
with	O
time	O
here	O
we	O
shall	O
focus	O
on	O
the	O
stationary	B
case	O
for	O
many	O
applications	O
such	O
as	O
financial	O
forecasting	O
we	O
wish	O
to	O
be	O
able	O
to	O
predict	O
the	O
next	O
value	O
in	O
a	O
time	O
series	O
given	O
observations	O
of	O
the	O
previous	O
values	O
intuitively	O
we	O
expect	O
that	O
recent	O
observations	O
are	O
likely	O
to	O
be	O
more	O
informative	O
than	O
more	O
historical	O
observations	O
in	O
predicting	O
future	O
values	O
the	O
example	O
in	O
figure	O
shows	O
that	O
successive	O
observations	O
of	O
the	O
speech	O
spectrum	O
are	O
indeed	O
highly	O
correlated	O
furthermore	O
it	O
would	O
be	O
impractical	O
to	O
consider	O
a	O
general	O
dependence	O
of	O
future	O
observations	O
on	O
all	O
previous	O
observations	O
because	O
the	O
complexity	O
of	O
such	O
a	O
model	O
would	O
grow	O
without	O
limit	O
as	O
the	O
number	O
of	O
observations	O
increases	O
this	O
leads	O
us	O
to	O
consider	O
markov	O
models	O
in	O
which	O
we	O
assume	O
that	O
future	O
predictions	O
are	O
inde	O
markov	O
models	O
figure	O
the	O
simplest	O
approach	O
to	O
modelling	O
a	O
sequence	O
of	O
observations	O
is	O
to	O
treat	O
them	O
as	O
independent	B
corresponding	O
to	O
a	O
graph	O
without	O
links	O
pendent	O
of	O
all	O
but	O
the	O
most	O
recent	O
observations	O
although	O
such	O
models	O
are	O
tractable	O
they	O
are	O
also	O
severely	O
limited	O
we	O
can	O
obtain	O
a	O
more	O
general	O
framework	O
while	O
still	O
retaining	O
tractability	O
by	O
the	O
introduction	O
of	O
latent	O
variables	O
leading	O
to	O
state	O
space	O
models	O
as	O
in	O
chapters	O
and	O
we	O
shall	O
see	O
that	O
complex	O
models	O
can	O
thereby	O
be	O
constructed	O
from	O
simpler	O
components	O
particular	O
from	O
distributions	O
belonging	O
to	O
the	O
exponential	B
family	I
and	O
can	O
be	O
readily	O
characterized	O
using	O
the	O
framework	O
of	O
probabilistic	O
graphical	O
models	O
here	O
we	O
focus	O
on	O
the	O
two	O
most	O
important	O
examples	O
of	O
state	O
space	O
models	O
namely	O
the	O
hidden	B
markov	B
model	I
in	O
which	O
the	O
latent	O
variables	O
are	O
discrete	O
and	O
linear	O
dynamical	O
systems	O
in	O
which	O
the	O
latent	O
variables	O
are	O
gaussian	B
both	O
models	O
are	O
described	O
by	O
directed	B
graphs	O
having	O
a	O
tree	B
structure	O
loops	O
for	O
which	O
inference	B
can	O
be	O
performed	O
efficiently	O
using	O
the	O
sum-product	B
algorithm	I
markov	O
models	O
the	O
easiest	O
way	O
to	O
treat	O
sequential	B
data	I
would	O
be	O
simply	O
to	O
ignore	O
the	O
sequential	O
aspects	O
and	O
treat	O
the	O
observations	O
as	O
i	O
i	O
d	O
corresponding	O
to	O
the	O
graph	O
in	O
figure	O
such	O
an	O
approach	O
however	O
would	O
fail	O
to	O
exploit	O
the	O
sequential	O
patterns	O
in	O
the	O
data	O
such	O
as	O
correlations	O
between	O
observations	O
that	O
are	O
close	O
in	O
the	O
sequence	O
suppose	O
for	O
instance	O
that	O
we	O
observe	O
a	O
binary	O
variable	O
denoting	O
whether	O
on	O
a	O
particular	O
day	O
it	O
rained	O
or	O
not	O
given	O
a	O
time	O
series	O
of	O
recent	O
observations	O
of	O
this	O
variable	O
we	O
wish	O
to	O
predict	O
whether	O
it	O
will	O
rain	O
on	O
the	O
next	O
day	O
if	O
we	O
treat	O
the	O
data	O
as	O
i	O
i	O
d	O
then	O
the	O
only	O
information	O
we	O
can	O
glean	O
from	O
the	O
data	O
is	O
the	O
relative	B
frequency	O
of	O
rainy	O
days	O
however	O
we	O
know	O
in	O
practice	O
that	O
the	O
weather	O
often	O
exhibits	O
trends	O
that	O
may	O
last	O
for	O
several	O
days	O
observing	O
whether	O
or	O
not	O
it	O
rains	O
today	O
is	O
therefore	O
of	O
significant	O
help	O
in	O
predicting	O
if	O
it	O
will	O
rain	O
tomorrow	O
to	O
express	O
such	O
effects	O
in	O
a	O
probabilistic	O
model	O
we	O
need	O
to	O
relax	O
the	O
i	O
i	O
d	O
assumption	O
and	O
one	O
of	O
the	O
simplest	O
ways	O
to	O
do	O
this	O
is	O
to	O
consider	O
a	O
markov	B
model	I
first	O
of	O
all	O
we	O
note	O
that	O
without	O
loss	O
of	O
generality	O
we	O
can	O
use	O
the	O
product	B
rule	I
to	O
express	O
the	O
joint	O
distribution	O
for	O
a	O
sequence	O
of	O
observations	O
in	O
the	O
form	O
xn	O
xn	O
if	O
we	O
now	O
assume	O
that	O
each	O
of	O
the	O
conditional	B
distributions	O
on	O
the	O
right-hand	O
side	O
is	O
independent	B
of	O
all	O
previous	O
observations	O
except	O
the	O
most	O
recent	O
we	O
obtain	O
the	O
first-order	O
markov	B
chain	I
which	O
is	O
depicted	O
as	O
a	O
graphical	B
model	I
in	O
figure	O
the	O
sequential	B
data	I
figure	O
a	O
first-order	O
markov	B
chain	I
of	O
observations	O
in	O
which	O
the	O
distribution	O
pxnxn	O
of	O
a	O
particular	O
observation	O
xn	O
is	O
conditioned	O
on	O
the	O
value	O
of	O
the	O
previous	O
observation	O
xn	O
joint	O
distribution	O
for	O
a	O
sequence	O
of	O
n	O
observations	O
under	O
this	O
model	O
is	O
given	O
by	O
xn	O
pxnxn	O
section	O
exercise	O
from	O
the	O
d-separation	B
property	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
for	O
observation	O
xn	O
given	O
all	O
of	O
the	O
observations	O
up	O
to	O
time	O
n	O
is	O
given	O
by	O
xn	O
pxnxn	O
which	O
is	O
easily	O
verified	O
by	O
direct	O
evaluation	O
starting	O
from	O
and	O
using	O
the	O
product	B
rule	I
of	I
probability	B
thus	O
if	O
we	O
use	O
such	O
a	O
model	O
to	O
predict	O
the	O
next	O
observation	O
in	O
a	O
sequence	O
the	O
distribution	O
of	O
predictions	O
will	O
depend	O
only	O
on	O
the	O
value	O
of	O
the	O
immediately	O
preceding	O
observation	O
and	O
will	O
be	O
independent	B
of	O
all	O
earlier	O
observations	O
in	O
most	O
applications	O
of	O
such	O
models	O
the	O
conditional	B
distributions	O
pxnxn	O
that	O
define	O
the	O
model	O
will	O
be	O
constrained	O
to	O
be	O
equal	O
corresponding	O
to	O
the	O
assumption	O
of	O
a	O
stationary	B
time	O
series	O
the	O
model	O
is	O
then	O
known	O
as	O
a	O
homogeneous	B
markov	B
chain	I
for	O
instance	O
if	O
the	O
conditional	B
distributions	O
depend	O
on	O
adjustable	O
parameters	O
values	O
might	O
be	O
inferred	O
from	O
a	O
set	O
of	O
training	B
data	O
then	O
all	O
of	O
the	O
conditional	B
distributions	O
in	O
the	O
chain	O
will	O
share	O
the	O
same	O
values	O
of	O
those	O
parameters	O
although	O
this	O
is	O
more	O
general	O
than	O
the	O
independence	O
model	O
it	O
is	O
still	O
very	O
restrictive	O
for	O
many	O
sequential	O
observations	O
we	O
anticipate	O
that	O
the	O
trends	O
in	O
the	O
data	O
over	O
several	O
successive	O
observations	O
will	O
provide	O
important	O
information	O
in	O
predicting	O
the	O
next	O
value	O
one	O
way	O
to	O
allow	O
earlier	O
observations	O
to	O
have	O
an	O
influence	O
is	O
to	O
move	O
to	O
higher-order	O
markov	O
chains	O
if	O
we	O
allow	O
the	O
predictions	O
to	O
depend	O
also	O
on	O
the	O
previous-but-one	O
value	O
we	O
obtain	O
a	O
second-order	O
markov	B
chain	I
represented	O
by	O
the	O
graph	O
in	O
figure	O
the	O
joint	O
distribution	O
is	O
now	O
given	O
by	O
xn	O
pxnxn	O
xn	O
again	O
using	O
d-separation	B
or	O
by	O
direct	O
evaluation	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
of	O
xn	O
given	O
xn	O
and	O
xn	O
is	O
independent	B
of	O
all	O
observations	O
xn	O
figure	O
a	O
second-order	O
markov	B
chain	I
in	O
which	O
the	O
conditional	B
distribution	O
of	O
a	O
particular	O
observation	O
xn	O
depends	O
on	O
the	O
values	O
of	O
the	O
two	O
previous	O
observations	O
xn	O
and	O
xn	O
figure	O
we	O
can	O
represent	O
sequential	B
data	I
using	O
a	O
markov	B
chain	I
of	O
latent	O
variables	O
with	O
each	O
observation	O
conditioned	O
on	O
the	O
state	O
of	O
the	O
corresponding	O
latent	B
variable	I
this	O
important	O
graphical	O
structure	O
forms	O
the	O
foundation	O
both	O
for	O
the	O
hidden	B
markov	B
model	I
and	O
for	O
linear	O
dynamical	O
systems	O
markov	O
models	O
zn	O
zn	O
xn	O
xn	O
each	O
observation	O
is	O
now	O
influenced	O
by	O
two	O
previous	O
observations	O
we	O
can	O
similarly	O
consider	O
extensions	O
to	O
an	O
m	O
th	O
order	O
markov	B
chain	I
in	O
which	O
the	O
conditional	B
distribution	O
for	O
a	O
particular	O
variable	O
depends	O
on	O
the	O
previous	O
m	O
variables	O
however	O
we	O
have	O
paid	O
a	O
price	O
for	O
this	O
increased	O
flexibility	O
because	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	O
is	O
now	O
much	O
larger	O
suppose	O
the	O
observations	O
are	O
discrete	O
variables	O
having	O
k	O
states	O
then	O
the	O
conditional	B
distribution	O
pxnxn	O
in	O
a	O
first-order	O
markov	B
chain	I
will	O
be	O
specified	O
by	O
a	O
set	O
of	O
k	O
parameters	O
for	O
each	O
of	O
the	O
k	O
states	O
of	O
xn	O
giving	O
a	O
total	O
of	O
kk	O
parameters	O
now	O
suppose	O
we	O
extend	O
the	O
model	O
to	O
an	O
m	O
th	O
order	O
markov	B
chain	I
so	O
that	O
the	O
joint	O
distribution	O
is	O
built	O
up	O
from	O
conditionals	O
pxnxn	O
m	O
xn	O
if	O
the	O
variables	O
are	O
discrete	O
and	O
if	O
the	O
conditional	B
distributions	O
are	O
represented	O
by	O
general	O
conditional	B
probability	B
tables	O
then	O
the	O
number	O
of	O
parameters	O
in	O
such	O
a	O
model	O
will	O
have	O
km	O
parameters	O
because	O
this	O
grows	O
exponentially	O
with	O
m	O
it	O
will	O
often	O
render	O
this	O
approach	O
impractical	O
for	O
larger	O
values	O
of	O
m	O
for	O
continuous	O
variables	O
we	O
can	O
use	O
linear-gaussian	O
conditional	B
distributions	O
in	O
which	O
each	O
node	B
has	O
a	O
gaussian	B
distribution	O
whose	O
mean	B
is	O
a	O
linear	O
function	O
of	O
its	O
parents	O
this	O
is	O
known	O
as	O
an	O
autoregressive	B
or	O
ar	O
model	O
et	O
al	O
thiesson	O
et	O
al	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
a	O
parametric	O
model	O
for	O
pxnxn	O
m	O
xn	O
such	O
as	O
a	O
neural	B
network	I
this	O
technique	O
is	O
sometimes	O
called	O
a	O
tapped	B
delay	I
line	I
because	O
it	O
corresponds	O
to	O
storing	O
the	O
previous	O
m	O
values	O
of	O
the	O
observed	B
variable	I
in	O
order	O
to	O
predict	O
the	O
next	O
value	O
the	O
number	O
of	O
parameters	O
can	O
then	O
be	O
much	O
smaller	O
than	O
in	O
a	O
completely	O
general	O
model	O
example	O
it	O
may	O
grow	O
linearly	O
with	O
m	O
although	O
this	O
is	O
achieved	O
at	O
the	O
expense	O
of	O
a	O
restricted	O
family	O
of	O
conditional	B
distributions	O
suppose	O
we	O
wish	O
to	O
build	O
a	O
model	O
for	O
sequences	O
that	O
is	O
not	O
limited	O
by	O
the	O
markov	O
assumption	O
to	O
any	O
order	O
and	O
yet	O
that	O
can	O
be	O
specified	O
using	O
a	O
limited	O
number	O
of	O
free	O
parameters	O
we	O
can	O
achieve	O
this	O
by	O
introducing	O
additional	O
latent	O
variables	O
to	O
permit	O
a	O
rich	O
class	O
of	O
models	O
to	O
be	O
constructed	O
out	O
of	O
simple	O
components	O
as	O
we	O
did	O
with	O
mixture	B
distributions	O
in	O
chapter	O
and	O
with	O
continuous	O
latent	B
variable	I
models	O
in	O
chapter	O
for	O
each	O
observation	O
xn	O
we	O
introduce	O
a	O
corresponding	O
latent	B
variable	I
zn	O
may	O
be	O
of	O
different	O
type	O
or	O
dimensionality	O
to	O
the	O
observed	B
variable	I
we	O
now	O
assume	O
that	O
it	O
is	O
the	O
latent	O
variables	O
that	O
form	O
a	O
markov	B
chain	I
giving	O
rise	O
to	O
the	O
graphical	O
structure	O
known	O
as	O
a	O
state	B
space	I
model	I
which	O
is	O
shown	O
in	O
figure	O
it	O
satisfies	O
the	O
key	O
conditional	B
independence	I
property	O
that	O
zn	O
and	O
are	O
independent	B
given	O
zn	O
so	O
that	O
zn	O
zn	O
sequential	B
data	I
the	O
joint	O
distribution	O
for	O
this	O
model	O
is	O
given	O
by	O
xn	O
zn	O
pznzn	O
pxnzn	O
using	O
the	O
d-separation	B
criterion	O
we	O
see	O
that	O
there	O
is	O
always	O
a	O
path	O
connecting	O
any	O
two	O
observed	O
variables	O
xn	O
and	O
xm	O
via	O
the	O
latent	O
variables	O
and	O
that	O
this	O
path	O
is	O
never	O
blocked	O
thus	O
the	O
predictive	B
distribution	I
xn	O
for	O
observation	O
given	O
all	O
previous	O
observations	O
does	O
not	O
exhibit	O
any	O
conditional	B
independence	I
properties	O
and	O
so	O
our	O
predictions	O
for	O
depends	O
on	O
all	O
previous	O
observations	O
the	O
observed	O
variables	O
however	O
do	O
not	O
satisfy	O
the	O
markov	O
property	O
at	O
any	O
order	O
we	O
shall	O
discuss	O
how	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
in	O
later	O
sections	O
of	O
this	O
chapter	O
there	O
are	O
two	O
important	O
models	O
for	O
sequential	B
data	I
that	O
are	O
described	O
by	O
this	O
graph	O
if	O
the	O
latent	O
variables	O
are	O
discrete	O
then	O
we	O
obtain	O
the	O
hidden	B
markov	B
model	I
or	O
hmm	O
et	O
al	O
note	O
that	O
the	O
observed	O
variables	O
in	O
an	O
hmm	O
may	O
be	O
discrete	O
or	O
continuous	O
and	O
a	O
variety	O
of	O
different	O
conditional	B
distributions	O
can	O
be	O
used	O
to	O
model	O
them	O
if	O
both	O
the	O
latent	O
and	O
the	O
observed	O
variables	O
are	O
gaussian	B
a	O
linear-gaussian	O
dependence	O
of	O
the	O
conditional	B
distributions	O
on	O
their	O
parents	O
then	O
we	O
obtain	O
the	O
linear	B
dynamical	B
system	I
section	O
section	O
hidden	O
markov	O
models	O
the	O
hidden	B
markov	B
model	I
can	O
be	O
viewed	O
as	O
a	O
specific	O
instance	O
of	O
the	O
state	B
space	I
model	I
of	O
figure	O
in	O
which	O
the	O
latent	O
variables	O
are	O
discrete	O
however	O
if	O
we	O
examine	O
a	O
single	O
time	O
slice	O
of	O
the	O
model	O
we	O
see	O
that	O
it	O
corresponds	O
to	O
a	O
mixture	B
distribution	I
with	O
component	O
densities	O
given	O
by	O
pxz	O
it	O
can	O
therefore	O
also	O
be	O
interpreted	O
as	O
an	O
extension	O
of	O
a	O
mixture	B
model	I
in	O
which	O
the	O
choice	O
of	O
mixture	B
component	I
for	O
each	O
observation	O
is	O
not	O
selected	O
independently	O
but	O
depends	O
on	O
the	O
choice	O
of	O
component	O
for	O
the	O
previous	O
observation	O
the	O
hmm	O
is	O
widely	O
used	O
in	O
speech	B
recognition	I
rabiner	O
and	O
juang	O
natural	B
language	I
modelling	I
and	O
sch	O
utze	O
on-line	O
handwriting	B
recognition	I
et	O
al	O
and	O
for	O
the	O
analysis	O
of	O
biological	O
sequences	O
such	O
as	O
proteins	O
and	O
dna	B
et	O
al	O
durbin	O
et	O
al	O
baldi	O
and	O
brunak	O
as	O
in	O
the	O
case	O
of	O
a	O
standard	O
mixture	B
model	I
the	O
latent	O
variables	O
are	O
the	O
discrete	O
multinomial	O
variables	O
zn	O
describing	O
which	O
component	O
of	O
the	O
mixture	B
is	O
responsible	O
for	O
generating	O
the	O
corresponding	O
observation	O
xn	O
again	O
it	O
is	O
convenient	O
to	O
use	O
a	O
coding	O
scheme	O
as	O
used	O
for	O
mixture	B
models	O
in	O
chapter	O
we	O
now	O
allow	O
the	O
probability	B
distribution	O
of	O
zn	O
to	O
depend	O
on	O
the	O
state	O
of	O
the	O
previous	O
latent	B
variable	I
zn	O
through	O
a	O
conditional	B
distribution	O
pznzn	O
because	O
the	O
latent	O
variables	O
are	O
k-dimensional	O
binary	O
variables	O
this	O
conditional	B
distribution	O
corresponds	O
to	O
a	O
table	O
of	O
numbers	O
that	O
we	O
denote	O
by	O
a	O
the	O
elements	O
of	O
which	O
are	O
known	O
as	O
transition	O
probabilities	O
they	O
are	O
given	O
by	O
ajk	O
pznk	O
and	O
because	O
they	O
are	O
probabilities	O
they	O
satisfy	O
ajk	O
with	O
k	O
ajk	O
so	O
that	O
the	O
matrix	O
a	O
hidden	O
markov	O
models	O
figure	O
transition	O
diagram	O
showing	O
a	O
model	O
whose	O
latent	O
variables	O
have	O
three	O
possible	O
states	O
corresponding	O
to	O
the	O
three	O
boxes	O
the	O
black	O
lines	O
denote	O
the	O
elements	O
of	O
the	O
transition	O
matrix	O
ajk	O
k	O
k	O
k	O
has	O
kk	O
independent	B
parameters	O
we	O
can	O
then	O
write	O
the	O
conditional	B
distribution	O
explicitly	O
in	O
the	O
form	O
pznzn	O
zn	O
znk	O
jk	O
a	O
the	O
initial	O
latent	O
node	B
is	O
special	O
in	O
that	O
it	O
does	O
not	O
have	O
a	O
parent	B
node	B
and	O
so	O
it	O
has	O
a	O
marginal	B
distribution	O
represented	O
by	O
a	O
vector	O
of	O
probabilities	O
with	O
elements	O
k	O
so	O
that	O
k	O
where	O
k	O
k	O
the	O
transition	O
matrix	O
is	O
sometimes	O
illustrated	O
diagrammatically	O
by	O
drawing	O
the	O
states	O
as	O
nodes	O
in	O
a	O
state	O
transition	O
diagram	O
as	O
shown	O
in	O
figure	O
for	O
the	O
case	O
of	O
k	O
note	O
that	O
this	O
does	O
not	O
represent	O
a	O
probabilistic	O
graphical	B
model	I
because	O
the	O
nodes	O
are	O
not	O
separate	O
variables	O
but	O
rather	O
states	O
of	O
a	O
single	O
variable	O
and	O
so	O
we	O
have	O
shown	O
the	O
states	O
as	O
boxes	O
rather	O
than	O
circles	O
section	O
it	O
is	O
sometimes	O
useful	O
to	O
take	O
a	O
state	O
transition	O
diagram	O
of	O
the	O
kind	O
shown	O
in	O
figure	O
and	O
unfold	O
it	O
over	O
time	O
this	O
gives	O
an	O
alternative	O
representation	O
of	O
the	O
transitions	O
between	O
latent	O
states	O
known	O
as	O
a	O
lattice	O
or	O
trellis	O
diagram	O
and	O
which	O
is	O
shown	O
for	O
the	O
case	O
of	O
the	O
hidden	B
markov	B
model	I
in	O
figure	O
the	O
specification	O
of	O
the	O
probabilistic	O
model	O
is	O
completed	O
by	O
defining	O
the	O
conditional	B
distributions	O
of	O
the	O
observed	O
variables	O
pxnzn	O
where	O
is	O
a	O
set	O
of	O
parameters	O
governing	O
the	O
distribution	O
these	O
are	O
known	O
as	O
emission	O
probabilities	O
and	O
might	O
for	O
example	O
be	O
given	O
by	O
gaussians	O
of	O
the	O
form	O
if	O
the	O
elements	O
of	O
x	O
are	O
continuous	O
variables	O
or	O
by	O
conditional	B
probability	B
tables	O
if	O
x	O
is	O
discrete	O
because	O
xn	O
is	O
observed	O
the	O
distribution	O
pxnzn	O
consists	O
for	O
a	O
given	O
value	O
of	O
of	O
a	O
vector	O
of	O
k	O
numbers	O
corresponding	O
to	O
the	O
k	O
possible	O
states	O
of	O
the	O
binary	O
vector	O
zn	O
sequential	B
data	I
figure	O
if	O
we	O
unfold	O
the	O
state	O
transition	O
diagram	O
of	O
figure	O
over	O
time	O
we	O
obtain	O
a	O
lattice	O
or	O
trellis	O
representation	O
of	O
the	O
latent	O
states	O
each	O
column	O
of	O
this	O
diagram	O
corresponds	O
to	O
one	O
of	O
the	O
latent	O
variables	O
zn	O
k	O
k	O
k	O
n	O
n	O
n	O
n	O
we	O
can	O
represent	O
the	O
emission	O
probabilities	O
in	O
the	O
form	O
pxnzn	O
pxn	O
kznk	O
we	O
shall	O
focuss	O
attention	O
on	O
homogeneous	B
models	O
for	O
which	O
all	O
of	O
the	O
conditional	B
distributions	O
governing	O
the	O
latent	O
variables	O
share	O
the	O
same	O
parameters	O
a	O
and	O
similarly	O
all	O
of	O
the	O
emission	O
distributions	O
share	O
the	O
same	O
parameters	O
extension	O
to	O
more	O
general	O
cases	O
is	O
straightforward	O
note	O
that	O
a	O
mixture	B
model	I
for	O
an	O
i	O
i	O
d	O
data	O
set	O
corresponds	O
to	O
the	O
special	O
case	O
in	O
which	O
the	O
parameters	O
ajk	O
are	O
the	O
same	O
for	O
all	O
values	O
of	O
j	O
so	O
that	O
the	O
conditional	B
distribution	O
pznzn	O
is	O
independent	B
of	O
zn	O
this	O
corresponds	O
to	O
deleting	O
the	O
horizontal	O
links	O
in	O
the	O
graphical	B
model	I
shown	O
in	O
figure	O
the	O
joint	O
probability	B
distribution	O
over	O
both	O
latent	O
and	O
observed	O
variables	O
is	O
then	O
given	O
by	O
px	O
z	O
pznzn	O
a	O
pxmzm	O
exercise	O
where	O
x	O
xn	O
z	O
zn	O
and	O
a	O
denotes	O
the	O
set	O
of	O
parameters	O
governing	O
the	O
model	O
most	O
of	O
our	O
discussion	O
of	O
the	O
hidden	B
markov	B
model	I
will	O
be	O
independent	B
of	O
the	O
particular	O
choice	O
of	O
the	O
emission	O
probabilities	O
indeed	O
the	O
model	O
is	O
tractable	O
for	O
a	O
wide	O
range	O
of	O
emission	O
distributions	O
including	O
discrete	O
tables	O
gaussians	O
and	O
mixtures	O
of	O
gaussians	O
it	O
is	O
also	O
possible	O
to	O
exploit	O
discriminative	O
models	O
such	O
as	O
neural	O
networks	O
these	O
can	O
be	O
used	O
to	O
model	O
the	O
emission	O
density	B
pxz	O
directly	O
or	O
to	O
provide	O
a	O
representation	O
for	O
pzx	O
that	O
can	O
be	O
converted	O
into	O
the	O
required	O
emission	O
density	B
pxz	O
using	O
bayes	B
theorem	O
et	O
al	O
we	O
can	O
gain	O
a	O
better	O
understanding	O
of	O
the	O
hidden	B
markov	B
model	I
by	O
considering	O
it	O
from	O
a	O
generative	O
point	O
of	O
view	O
recall	O
that	O
to	O
generate	O
samples	O
from	O
a	O
mixture	B
of	O
k	O
k	O
k	O
hidden	O
markov	O
models	O
figure	O
illustration	O
of	O
sampling	O
from	O
a	O
hidden	B
markov	B
model	I
having	O
a	O
latent	B
variable	I
z	O
and	O
a	O
gaussian	B
emission	O
model	O
pxz	O
where	O
x	O
is	O
contours	O
of	O
constant	O
probability	B
density	B
for	O
the	O
emission	O
distributions	O
corresponding	O
to	O
each	O
of	O
the	O
three	O
states	O
of	O
the	O
latent	B
variable	I
a	O
sample	O
of	O
points	O
drawn	O
from	O
the	O
hidden	B
markov	B
model	I
colour	O
coded	O
according	O
to	O
the	O
component	O
that	O
generated	O
them	O
and	O
with	O
lines	O
connecting	O
the	O
successive	O
observations	O
here	O
the	O
transition	O
matrix	O
was	O
fixed	O
so	O
that	O
in	O
any	O
state	O
there	O
is	O
a	O
probability	B
of	O
making	O
a	O
transition	O
to	O
each	O
of	O
the	O
other	O
states	O
and	O
consequently	O
a	O
probability	B
of	O
remaining	O
in	O
the	O
same	O
state	O
gaussians	O
we	O
first	O
chose	O
one	O
of	O
the	O
components	O
at	O
random	O
with	O
probability	B
given	O
by	O
the	O
mixing	O
coefficients	O
k	O
and	O
then	O
generate	O
a	O
sample	O
vector	O
x	O
from	O
the	O
corresponding	O
gaussian	B
component	O
this	O
process	O
is	O
repeated	O
n	O
times	O
to	O
generate	O
a	O
data	O
set	O
of	O
n	O
independent	B
samples	O
in	O
the	O
case	O
of	O
the	O
hidden	B
markov	B
model	I
this	O
procedure	O
is	O
modified	O
as	O
follows	O
we	O
first	O
choose	O
the	O
initial	O
latent	B
variable	I
with	O
probabilities	O
governed	O
by	O
the	O
parameters	O
k	O
and	O
then	O
sample	O
the	O
corresponding	O
observation	O
now	O
we	O
choose	O
the	O
state	O
of	O
the	O
variable	O
according	O
to	O
the	O
transition	O
probabilities	O
using	O
the	O
already	O
instantiated	O
value	O
of	O
thus	O
suppose	O
that	O
the	O
sample	O
for	O
corresponds	O
to	O
state	O
j	O
then	O
we	O
choose	O
the	O
state	O
k	O
of	O
with	O
probabilities	O
ajk	O
for	O
k	O
k	O
once	O
we	O
know	O
we	O
can	O
draw	O
a	O
sample	O
for	O
and	O
also	O
sample	O
the	O
next	O
latent	B
variable	I
and	O
so	O
on	O
this	O
is	O
an	O
example	O
of	O
ancestral	B
sampling	I
for	O
a	O
directed	B
graphical	B
model	I
if	O
for	O
instance	O
we	O
have	O
a	O
model	O
in	O
which	O
the	O
diagonal	B
transition	O
elements	O
akk	O
are	O
much	O
larger	O
than	O
the	O
off-diagonal	O
elements	O
then	O
a	O
typical	O
data	O
sequence	O
will	O
have	O
long	O
runs	O
of	O
points	O
generated	O
from	O
a	O
single	O
component	O
with	O
infrequent	O
transitions	O
from	O
one	O
component	O
to	O
another	O
the	O
generation	O
of	O
samples	O
from	O
a	O
hidden	B
markov	B
model	I
is	O
illustrated	O
in	O
figure	O
there	O
are	O
many	O
variants	O
of	O
the	O
standard	O
hmm	O
model	O
obtained	O
for	O
instance	O
by	O
imposing	O
constraints	O
on	O
the	O
form	O
of	O
the	O
transition	O
matrix	O
a	O
here	O
we	O
mention	O
one	O
of	O
particular	O
practical	O
importance	O
called	O
the	O
left-to-right	B
hmm	O
which	O
is	O
obtained	O
by	O
setting	O
the	O
elements	O
ajk	O
of	O
a	O
to	O
zero	O
if	O
k	O
j	O
as	O
illustrated	O
in	O
the	O
section	O
sequential	B
data	I
figure	O
example	O
of	O
the	O
state	O
transition	O
diagram	O
for	O
a	O
left-to-right	B
hidden	B
markov	B
model	I
note	O
that	O
once	O
a	O
state	O
has	O
been	O
vacated	O
it	O
cannot	O
later	O
be	O
re-entered	O
k	O
k	O
k	O
state	O
transition	O
diagram	O
for	O
a	O
hmm	O
in	O
figure	O
typically	O
for	O
such	O
models	O
the	O
initial	O
state	O
probabilities	O
for	O
are	O
modified	O
so	O
that	O
and	O
for	O
j	O
in	O
other	O
words	O
every	O
sequence	O
is	O
constrained	O
to	O
start	O
in	O
state	O
j	O
the	O
transition	O
matrix	O
may	O
be	O
further	O
constrained	O
to	O
ensure	O
that	O
large	O
changes	O
in	O
the	O
state	O
index	O
do	O
not	O
occur	O
so	O
that	O
ajk	O
if	O
k	O
j	O
this	O
type	O
of	O
model	O
is	O
illustrated	O
using	O
a	O
lattice	B
diagram	I
in	O
figure	O
many	O
applications	O
of	O
hidden	O
markov	O
models	O
for	O
example	O
speech	B
recognition	I
or	O
on-line	O
character	O
recognition	O
make	O
use	O
of	O
left-to-right	B
architectures	O
as	O
an	O
illustration	O
of	O
the	O
left-to-right	B
hidden	B
markov	B
model	I
we	O
consider	O
an	O
example	O
involving	O
handwritten	O
digits	O
this	O
uses	O
on-line	O
data	O
meaning	O
that	O
each	O
digit	O
is	O
represented	O
by	O
the	O
trajectory	O
of	O
the	O
pen	O
as	O
a	O
function	O
of	O
time	O
in	O
the	O
form	O
of	O
a	O
sequence	O
of	O
pen	O
coordinates	O
in	O
contrast	O
to	O
the	O
off-line	O
digits	O
data	O
discussed	O
in	O
appendix	O
a	O
which	O
comprises	O
static	O
two-dimensional	O
pixellated	O
images	O
of	O
the	O
ink	O
examples	O
of	O
the	O
online	O
digits	O
are	O
shown	O
in	O
figure	O
here	O
we	O
train	O
a	O
hidden	B
markov	B
model	I
on	O
a	O
subset	O
of	O
data	O
comprising	O
examples	O
of	O
the	O
digit	O
there	O
are	O
k	O
states	O
each	O
of	O
which	O
can	O
generate	O
a	O
line	O
segment	O
of	O
fixed	O
length	O
having	O
one	O
of	O
possible	O
angles	O
and	O
so	O
the	O
emission	O
distribution	O
is	O
simply	O
a	O
table	O
of	O
probabilities	O
associated	O
with	O
the	O
allowed	O
angle	O
values	O
for	O
each	O
state	O
index	O
value	O
transition	O
probabilities	O
are	O
all	O
set	O
to	O
zero	O
except	O
for	O
those	O
that	O
keep	O
the	O
state	O
index	O
k	O
the	O
same	O
or	O
that	O
increment	O
it	O
by	O
and	O
the	O
model	O
parameters	O
are	O
optimized	O
using	O
iterations	O
of	O
em	B
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
resulting	O
model	O
by	O
running	O
it	O
generatively	O
as	O
shown	O
in	O
figure	O
figure	O
lattice	B
diagram	I
for	O
a	O
leftto-right	O
hmm	O
in	O
which	O
the	O
state	O
index	O
k	O
is	O
allowed	O
to	O
increase	O
by	O
at	O
most	O
at	O
each	O
transition	O
k	O
k	O
k	O
n	O
n	O
n	O
n	O
hidden	O
markov	O
models	O
figure	O
top	O
row	O
examples	O
of	O
on-line	O
handwritten	O
digits	O
bottom	O
row	O
synthetic	O
digits	O
sampled	O
generatively	O
from	O
a	O
left-to-right	B
hidden	B
markov	B
model	I
that	O
has	O
been	O
trained	O
on	O
a	O
data	O
set	O
of	O
handwritten	O
digits	O
one	O
of	O
the	O
most	O
powerful	O
properties	O
of	O
hidden	O
markov	O
models	O
is	O
their	O
ability	O
to	O
exhibit	O
some	O
degree	O
of	O
invariance	B
to	O
local	B
warping	O
and	O
stretching	O
of	O
the	O
time	O
axis	O
to	O
understand	O
this	O
consider	O
the	O
way	O
in	O
which	O
the	O
digit	O
is	O
written	O
in	O
the	O
on-line	O
handwritten	O
digits	O
example	O
a	O
typical	O
digit	O
comprises	O
two	O
distinct	O
sections	O
joined	O
at	O
a	O
cusp	O
the	O
first	O
part	O
of	O
the	O
digit	O
which	O
starts	O
at	O
the	O
top	O
left	O
has	O
a	O
sweeping	O
arc	B
down	O
to	O
the	O
cusp	O
or	O
loop	O
at	O
the	O
bottom	O
left	O
followed	O
by	O
a	O
second	O
moreor-less	O
straight	O
sweep	O
ending	O
at	O
the	O
bottom	O
right	O
natural	O
variations	O
in	O
writing	O
style	O
will	O
cause	O
the	O
relative	B
sizes	O
of	O
the	O
two	O
sections	O
to	O
vary	O
and	O
hence	O
the	O
location	O
of	O
the	O
cusp	O
or	O
loop	O
within	O
the	O
temporal	O
sequence	O
will	O
vary	O
from	O
a	O
generative	O
perspective	O
such	O
variations	O
can	O
be	O
accommodated	O
by	O
the	O
hidden	B
markov	B
model	I
through	O
changes	O
in	O
the	O
number	O
of	O
transitions	O
to	O
the	O
same	O
state	O
versus	O
the	O
number	O
of	O
transitions	O
to	O
the	O
successive	O
state	O
note	O
however	O
that	O
if	O
a	O
digit	O
is	O
written	O
in	O
the	O
reverse	O
order	O
that	O
is	O
starting	O
at	O
the	O
bottom	O
right	O
and	O
ending	O
at	O
the	O
top	O
left	O
then	O
even	O
though	O
the	O
pen	O
tip	O
coordinates	O
may	O
be	O
identical	O
to	O
an	O
example	O
from	O
the	O
training	B
set	I
the	O
probability	B
of	O
the	O
observations	O
under	O
the	O
model	O
will	O
be	O
extremely	O
small	O
in	O
the	O
speech	B
recognition	I
context	O
warping	O
of	O
the	O
time	O
axis	O
is	O
associated	O
with	O
natural	O
variations	O
in	O
the	O
speed	O
of	O
speech	O
and	O
again	O
the	O
hidden	B
markov	B
model	I
can	O
accommodate	O
such	O
a	O
distortion	O
and	O
not	O
penalize	O
it	O
too	O
heavily	O
maximum	B
likelihood	I
for	O
the	O
hmm	O
if	O
we	O
have	O
observed	O
a	O
data	O
set	O
x	O
xn	O
we	O
can	O
determine	O
the	O
parameters	O
of	O
an	O
hmm	O
using	O
maximum	B
likelihood	I
the	O
likelihood	B
function	I
is	O
obtained	O
from	O
the	O
joint	O
distribution	O
by	O
marginalizing	O
over	O
the	O
latent	O
variables	O
px	O
px	O
z	O
z	O
because	O
the	O
joint	O
distribution	O
px	O
z	O
does	O
not	O
factorize	O
over	O
n	O
contrast	O
to	O
the	O
mixture	B
distribution	I
considered	O
in	O
chapter	O
we	O
cannot	O
simply	O
treat	O
each	O
of	O
the	O
summations	O
over	O
zn	O
independently	O
nor	O
can	O
we	O
perform	O
the	O
summations	O
explicitly	O
because	O
there	O
are	O
n	O
variables	O
to	O
be	O
summed	O
over	O
each	O
of	O
which	O
has	O
k	O
states	O
resulting	O
in	O
a	O
total	O
of	O
k	O
n	O
terms	O
thus	O
the	O
number	O
of	O
terms	O
in	O
the	O
summation	O
grows	O
sequential	B
data	I
exponentially	O
with	O
the	O
length	O
of	O
the	O
chain	O
in	O
fact	O
the	O
summation	O
in	O
corresponds	O
to	O
summing	O
over	O
exponentially	O
many	O
paths	O
through	O
the	O
lattice	B
diagram	I
in	O
figure	O
we	O
have	O
already	O
encountered	O
a	O
similar	O
difficulty	O
when	O
we	O
considered	O
the	O
inference	B
problem	O
for	O
the	O
simple	O
chain	O
of	O
variables	O
in	O
figure	O
there	O
we	O
were	O
able	O
to	O
make	O
use	O
of	O
the	O
conditional	B
independence	I
properties	O
of	O
the	O
graph	O
to	O
re-order	O
the	O
summations	O
in	O
order	O
to	O
obtain	O
an	O
algorithm	O
whose	O
cost	O
scales	O
linearly	O
instead	O
of	O
exponentially	O
with	O
the	O
length	O
of	O
the	O
chain	O
we	O
shall	O
apply	O
a	O
similar	O
technique	O
to	O
the	O
hidden	B
markov	B
model	I
a	O
further	O
difficulty	O
with	O
the	O
expression	O
for	O
the	O
likelihood	B
function	I
is	O
that	O
because	O
it	O
corresponds	O
to	O
a	O
generalization	B
of	O
a	O
mixture	B
distribution	I
it	O
represents	O
a	O
summation	O
over	O
the	O
emission	O
models	O
for	O
different	O
settings	O
of	O
the	O
latent	O
variables	O
direct	O
maximization	O
of	O
the	O
likelihood	B
function	I
will	O
therefore	O
lead	O
to	O
complex	O
expressions	O
with	O
no	O
closed-form	O
solutions	O
as	O
was	O
the	O
case	O
for	O
simple	O
mixture	B
models	O
that	O
a	O
mixture	B
model	I
for	O
i	O
i	O
d	O
data	O
is	O
a	O
special	O
case	O
of	O
the	O
hmm	O
section	O
we	O
therefore	O
turn	O
to	O
the	O
expectation	B
maximization	I
algorithm	O
to	O
find	O
an	O
efficient	O
framework	O
for	O
maximizing	O
the	O
likelihood	B
function	I
in	O
hidden	O
markov	O
models	O
the	O
em	B
algorithm	I
starts	O
with	O
some	O
initial	O
selection	O
for	O
the	O
model	O
parameters	O
which	O
we	O
denote	O
by	O
old	O
in	O
the	O
e	O
step	O
we	O
take	O
these	O
parameter	O
values	O
and	O
find	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
pzx	O
old	O
we	O
then	O
use	O
this	O
posterior	O
distribution	O
to	O
evaluate	O
the	O
expectation	B
of	O
the	O
logarithm	O
of	O
the	O
complete-data	O
likelihood	B
function	I
as	O
a	O
function	O
of	O
the	O
parameters	O
to	O
give	O
the	O
function	O
q	O
old	O
defined	O
by	O
q	O
old	O
pzx	O
old	O
ln	O
px	O
z	O
z	O
at	O
this	O
point	O
it	O
is	O
convenient	O
to	O
introduce	O
some	O
notation	O
we	O
shall	O
use	O
to	O
denote	O
the	O
marginal	B
posterior	O
distribution	O
of	O
a	O
latent	B
variable	I
zn	O
and	O
zn	O
to	O
denote	O
the	O
joint	O
posterior	O
distribution	O
of	O
two	O
successive	O
latent	O
variables	O
so	O
that	O
pznx	O
old	O
zn	O
pzn	O
znx	O
old	O
for	O
each	O
value	O
of	O
n	O
we	O
can	O
store	O
using	O
a	O
set	O
of	O
k	O
nonnegative	O
numbers	O
that	O
sum	O
to	O
unity	O
and	O
similarly	O
we	O
can	O
store	O
zn	O
using	O
a	O
k	O
k	O
matrix	O
of	O
nonnegative	O
numbers	O
that	O
again	O
sum	O
to	O
unity	O
we	O
shall	O
also	O
use	O
to	O
denote	O
the	O
conditional	B
probability	B
of	O
znk	O
with	O
a	O
similar	O
use	O
of	O
notation	O
for	O
znk	O
and	O
for	O
other	O
probabilistic	O
variables	O
introduced	O
later	O
because	O
the	O
expectation	B
of	O
a	O
binary	O
random	O
variable	O
is	O
just	O
the	O
probability	B
that	O
it	O
takes	O
the	O
value	O
we	O
have	O
eznk	O
znk	O
ezn	O
z	O
if	O
we	O
substitute	O
the	O
joint	O
distribution	O
px	O
z	O
given	O
by	O
into	O
z	O
exercise	O
exercise	O
hidden	O
markov	O
models	O
and	O
make	O
use	O
of	O
the	O
definitions	O
of	O
and	O
we	O
obtain	O
q	O
old	O
ln	O
k	O
znk	O
ln	O
ajk	O
ln	O
pxn	O
k	O
the	O
goal	O
of	O
the	O
e	O
step	O
will	O
be	O
to	O
evaluate	O
the	O
quantities	O
and	O
zn	O
efficiently	O
and	O
we	O
shall	O
discuss	O
this	O
in	O
detail	O
shortly	O
in	O
the	O
m	O
step	O
we	O
maximize	O
q	O
old	O
with	O
respect	O
to	O
the	O
parameters	O
a	O
in	O
which	O
we	O
treat	O
and	O
zn	O
as	O
constant	O
maximization	O
with	O
respect	O
to	O
and	O
a	O
is	O
easily	O
achieved	O
using	O
appropriate	O
lagrange	B
multipliers	O
with	O
the	O
results	O
k	O
ajk	O
znk	O
znl	O
the	O
em	B
algorithm	I
must	O
be	O
initialized	O
by	O
choosing	O
starting	O
values	O
for	O
and	O
a	O
which	O
should	O
of	O
course	O
respect	O
the	O
summation	O
constraints	O
associated	O
with	O
their	O
probabilistic	O
interpretation	O
note	O
that	O
any	O
elements	O
of	O
or	O
a	O
that	O
are	O
set	O
to	O
zero	O
initially	O
will	O
remain	O
zero	O
in	O
subsequent	O
em	B
updates	O
a	O
typical	O
initialization	O
procedure	O
would	O
involve	O
selecting	O
random	O
starting	O
values	O
for	O
these	O
parameters	O
subject	O
to	O
the	O
summation	O
and	O
non-negativity	O
constraints	O
note	O
that	O
no	O
particular	O
modification	O
to	O
the	O
em	B
results	O
are	O
required	O
for	O
the	O
case	O
of	O
left-to-right	B
models	O
beyond	O
choosing	O
initial	O
values	O
for	O
the	O
elements	O
ajk	O
in	O
which	O
the	O
appropriate	O
elements	O
are	O
set	O
to	O
zero	O
because	O
these	O
will	O
remain	O
zero	O
throughout	O
to	O
maximize	O
q	O
old	O
with	O
respect	O
to	O
k	O
we	O
notice	O
that	O
only	O
the	O
final	O
term	O
in	O
depends	O
on	O
k	O
and	O
furthermore	O
this	O
term	O
has	O
exactly	O
the	O
same	O
form	O
as	O
the	O
data-dependent	O
term	O
in	O
the	O
corresponding	O
function	O
for	O
a	O
standard	O
mixture	B
distribution	I
for	O
i	O
i	O
d	O
data	O
as	O
can	O
be	O
seen	O
by	O
comparison	O
with	O
for	O
the	O
case	O
of	O
a	O
gaussian	B
mixture	B
here	O
the	O
quantities	O
are	O
playing	O
the	O
role	O
of	O
the	O
responsibilities	O
if	O
the	O
parameters	O
k	O
are	O
independent	B
for	O
the	O
different	O
components	O
then	O
this	O
term	O
decouples	O
into	O
a	O
sum	O
of	O
terms	O
one	O
for	O
each	O
value	O
of	O
k	O
each	O
of	O
which	O
can	O
be	O
maximized	O
independently	O
we	O
are	O
then	O
simply	O
maximizing	O
the	O
weighted	O
log	O
likelihood	B
function	I
for	O
the	O
emission	O
density	B
px	O
k	O
with	O
weights	O
here	O
we	O
shall	O
suppose	O
that	O
this	O
maximization	O
can	O
be	O
done	O
efficiently	O
for	O
instance	O
in	O
the	O
case	O
of	O
sequential	B
data	I
gaussian	B
emission	O
densities	O
we	O
have	O
px	O
k	O
n	O
k	O
k	O
and	O
maximization	O
of	O
the	O
function	O
q	O
old	O
then	O
gives	O
k	O
kxn	O
kt	O
k	O
exercise	O
section	O
for	O
the	O
case	O
of	O
discrete	O
multinomial	O
observed	O
variables	O
the	O
conditional	B
distribution	O
of	O
the	O
observations	O
takes	O
the	O
form	O
pxz	O
xizk	O
ik	O
and	O
the	O
corresponding	O
m-step	O
equations	O
are	O
given	O
by	O
ik	O
an	O
analogous	O
result	O
holds	O
for	O
bernoulli	B
observed	O
variables	O
the	O
em	B
algorithm	I
requires	O
initial	O
values	O
for	O
the	O
parameters	O
of	O
the	O
emission	O
distribution	O
one	O
way	O
to	O
set	O
these	O
is	O
first	O
to	O
treat	O
the	O
data	O
initially	O
as	O
i	O
i	O
d	O
and	O
fit	O
the	O
emission	O
density	B
by	O
maximum	B
likelihood	I
and	O
then	O
use	O
the	O
resulting	O
values	O
to	O
initialize	O
the	O
parameters	O
for	O
em	B
the	O
forward-backward	B
algorithm	I
next	O
we	O
seek	O
an	O
efficient	O
procedure	O
for	O
evaluating	O
the	O
quantities	O
and	O
znk	O
corresponding	O
to	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
the	O
graph	O
for	O
the	O
hidden	B
markov	B
model	I
shown	O
in	O
figure	O
is	O
a	O
tree	B
and	O
so	O
we	O
know	O
that	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
can	O
be	O
obtained	O
efficiently	O
using	O
a	O
twostage	O
message	B
passing	I
algorithm	O
in	O
the	O
particular	O
context	O
of	O
the	O
hidden	B
markov	B
model	I
this	O
is	O
known	O
as	O
the	O
forward-backward	B
algorithm	I
or	O
the	O
baum-welch	B
algorithm	I
there	O
are	O
in	O
fact	O
several	O
variants	O
of	O
the	O
basic	O
algorithm	O
all	O
of	O
which	O
lead	O
to	O
the	O
exact	O
marginals	O
according	O
to	O
the	O
precise	O
form	O
of	O
hidden	O
markov	O
models	O
the	O
messages	O
that	O
are	O
propagated	O
along	O
the	O
chain	O
we	O
shall	O
focus	O
on	O
the	O
most	O
widely	O
used	O
of	O
these	O
known	O
as	O
the	O
alpha-beta	O
algorithm	O
as	O
well	O
as	O
being	O
of	O
great	O
practical	O
importance	O
in	O
its	O
own	O
right	O
the	O
forwardbackward	O
algorithm	O
provides	O
us	O
with	O
a	O
nice	O
illustration	O
of	O
many	O
of	O
the	O
concepts	O
introduced	O
in	O
earlier	O
chapters	O
we	O
shall	O
therefore	O
begin	O
in	O
this	O
section	O
with	O
a	O
conventional	O
derivation	O
of	O
the	O
forward-backward	O
equations	O
making	O
use	O
of	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
and	O
exploiting	O
conditional	B
independence	I
properties	O
which	O
we	O
shall	O
obtain	O
from	O
the	O
corresponding	O
graphical	B
model	I
using	O
d-separation	B
then	O
in	O
section	O
we	O
shall	O
see	O
how	O
the	O
forward-backward	B
algorithm	I
can	O
be	O
obtained	O
very	O
simply	O
as	O
a	O
specific	O
example	O
of	O
the	O
sum-product	B
algorithm	I
introduced	O
in	O
section	O
it	O
is	O
worth	O
emphasizing	O
that	O
evaluation	O
of	O
the	O
posterior	O
distributions	O
of	O
the	O
latent	O
variables	O
is	O
independent	B
of	O
the	O
form	O
of	O
the	O
emission	O
density	B
pxz	O
or	O
indeed	O
of	O
whether	O
the	O
observed	O
variables	O
are	O
continuous	O
or	O
discrete	O
all	O
we	O
require	O
is	O
the	O
values	O
of	O
the	O
quantities	O
pxnzn	O
for	O
each	O
value	O
of	O
zn	O
for	O
every	O
n	O
also	O
in	O
this	O
section	O
and	O
the	O
next	O
we	O
shall	O
omit	O
the	O
explicit	O
dependence	O
on	O
the	O
model	O
parameters	O
old	O
because	O
these	O
fixed	O
throughout	O
we	O
therefore	O
begin	O
by	O
writing	O
down	O
the	O
following	O
conditional	B
independence	I
properties	O
pxzn	O
xnzn	O
pxn	O
zn	O
pxn	O
pzn	O
x	O
pzn	O
xnzn	O
xn	O
zn	O
xn	O
xn	O
zn	O
xn	O
xnzn	O
pxzn	O
zn	O
xn	O
xnzn	O
where	O
x	O
xn	O
these	O
relations	O
are	O
most	O
easily	O
proved	O
using	O
d-separation	B
for	O
instance	O
in	O
the	O
first	O
of	O
these	O
results	O
we	O
note	O
that	O
every	O
path	O
from	O
any	O
one	O
of	O
the	O
nodes	O
xn	O
to	O
the	O
node	B
xn	O
passes	O
through	O
the	O
node	B
zn	O
which	O
is	O
observed	O
because	O
all	O
such	O
paths	O
are	O
head-to-tail	O
it	O
follows	O
that	O
the	O
conditional	B
independence	I
property	O
must	O
hold	O
the	O
reader	O
should	O
take	O
a	O
few	O
moments	O
to	O
verify	O
each	O
of	O
these	O
properties	O
in	O
turn	O
as	O
an	O
exercise	O
in	O
the	O
application	O
of	O
d-separation	B
these	O
relations	O
can	O
also	O
be	O
proved	O
directly	O
though	O
with	O
significantly	O
greater	O
effort	O
from	O
the	O
joint	O
distribution	O
for	O
the	O
hidden	B
markov	B
model	I
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
let	O
us	O
begin	O
by	O
evaluating	O
recall	O
that	O
for	O
a	O
discrete	O
multinomial	O
random	O
variable	O
the	O
expected	O
value	O
of	O
one	O
of	O
its	O
components	O
is	O
just	O
the	O
probability	B
of	O
that	O
component	O
having	O
the	O
value	O
thus	O
we	O
are	O
interested	O
in	O
finding	O
the	O
posterior	O
distribution	O
xn	O
of	O
zn	O
given	O
the	O
observed	O
data	O
set	O
xn	O
this	O
exercise	O
sequential	B
data	I
represents	O
a	O
vector	O
of	O
length	O
k	O
whose	O
entries	O
correspond	O
to	O
the	O
expected	O
values	O
of	O
znk	O
using	O
bayes	B
theorem	O
we	O
have	O
pznx	O
pxznpzn	O
px	O
note	O
that	O
the	O
denominator	O
px	O
is	O
implicitly	O
conditioned	O
on	O
the	O
parameters	O
old	O
of	O
the	O
hmm	O
and	O
hence	O
represents	O
the	O
likelihood	B
function	I
using	O
the	O
conditional	B
independence	I
property	O
together	O
with	O
the	O
product	B
rule	I
of	I
probability	B
we	O
obtain	O
xn	O
xnzn	O
px	O
px	O
where	O
we	O
have	O
defined	O
xn	O
zn	O
xnzn	O
the	O
quantity	O
represents	O
the	O
joint	O
probability	B
of	O
observing	O
all	O
of	O
the	O
given	O
data	O
up	O
to	O
time	O
n	O
and	O
the	O
value	O
of	O
zn	O
whereas	O
represents	O
the	O
conditional	B
probability	B
of	O
all	O
future	O
data	O
from	O
time	O
n	O
up	O
to	O
n	O
given	O
the	O
value	O
of	O
zn	O
again	O
and	O
each	O
represent	O
set	O
of	O
k	O
numbers	O
one	O
for	O
each	O
of	O
the	O
possible	O
settings	O
of	O
the	O
coded	O
binary	O
vector	O
zn	O
we	O
shall	O
use	O
the	O
notation	O
to	O
denote	O
the	O
value	O
of	O
when	O
znk	O
with	O
an	O
analogous	O
interpretation	O
of	O
we	O
now	O
derive	O
recursion	O
relations	O
that	O
allow	O
and	O
to	O
be	O
evaluated	O
efficiently	O
again	O
we	O
shall	O
make	O
use	O
of	O
conditional	B
independence	I
properties	O
in	O
particular	O
and	O
together	O
with	O
the	O
sum	O
and	O
product	O
rules	O
allowing	O
us	O
to	O
express	O
in	O
terms	O
of	O
as	O
follows	O
xn	O
zn	O
xnznpzn	O
xn	O
xn	O
zn	O
pxnzn	O
xn	O
zn	O
zn	O
zn	O
zn	O
zn	O
zn	O
pxnzn	O
pxnzn	O
pxnzn	O
xn	O
znzn	O
xn	O
xn	O
zn	O
making	O
use	O
of	O
the	O
definition	O
for	O
we	O
then	O
obtain	O
pxnzn	O
zn	O
hidden	O
markov	O
models	O
figure	O
illustration	O
of	O
the	O
forward	O
recursion	O
for	O
evaluation	O
of	O
the	O
variables	O
in	O
this	O
fragment	O
of	O
the	O
lattice	O
we	O
see	O
that	O
the	O
quantity	O
is	O
obtained	O
by	O
taking	O
the	O
elements	O
of	O
at	O
step	O
n	O
and	O
summing	O
them	O
up	O
with	O
weights	O
given	O
by	O
corresponding	O
to	O
the	O
values	O
of	O
pznzn	O
and	O
then	O
multiplying	O
by	O
the	O
data	O
contribution	O
k	O
k	O
k	O
n	O
n	O
it	O
is	O
worth	O
taking	O
a	O
moment	O
to	O
study	O
this	O
recursion	O
relation	O
in	O
some	O
detail	O
note	O
that	O
there	O
are	O
k	O
terms	O
in	O
the	O
summation	O
and	O
the	O
right-hand	O
side	O
has	O
to	O
be	O
evaluated	O
for	O
each	O
of	O
the	O
k	O
values	O
of	O
zn	O
so	O
each	O
step	O
of	O
the	O
recursion	O
has	O
computational	O
cost	O
that	O
scaled	O
like	O
ok	O
the	O
forward	O
recursion	O
equation	O
for	O
is	O
illustrated	O
using	O
a	O
lattice	B
diagram	I
in	O
figure	O
in	O
order	O
to	O
start	O
this	O
recursion	O
we	O
need	O
an	O
initial	O
condition	O
that	O
is	O
given	O
by	O
xnzn	O
which	O
tells	O
us	O
that	O
for	O
k	O
k	O
takes	O
the	O
value	O
k	O
starting	O
at	O
the	O
first	O
node	B
of	O
the	O
chain	O
we	O
can	O
then	O
work	O
along	O
the	O
chain	O
and	O
evaluate	O
for	O
every	O
latent	O
node	B
because	O
each	O
step	O
of	O
the	O
recursion	O
involves	O
multiplying	O
by	O
a	O
k	O
k	O
matrix	O
the	O
overall	O
cost	O
of	O
evaluating	O
these	O
quantities	O
for	O
the	O
whole	O
chain	O
is	O
of	O
ok	O
we	O
can	O
similarly	O
find	O
a	O
recursion	O
relation	O
for	O
the	O
quantities	O
by	O
making	O
use	O
of	O
the	O
conditional	B
independence	I
properties	O
and	O
giving	O
xnzn	O
xn	O
sequential	B
data	I
figure	O
illustration	O
of	O
the	O
backward	O
recursion	O
for	O
evaluation	O
of	O
the	O
variables	O
in	O
this	O
fragment	O
of	O
the	O
lattice	O
we	O
see	O
that	O
the	O
quantity	O
is	O
obtained	O
by	O
taking	O
the	O
components	O
of	O
at	O
step	O
n	O
and	O
summing	O
them	O
up	O
with	O
weights	O
given	O
by	O
the	O
products	O
of	O
corresponding	O
to	O
the	O
values	O
of	O
and	O
the	O
corresponding	O
values	O
of	O
the	O
emission	O
density	B
k	O
k	O
k	O
n	O
n	O
making	O
use	O
of	O
the	O
definition	O
for	O
we	O
then	O
obtain	O
note	O
that	O
in	O
this	O
case	O
we	O
have	O
a	O
backward	O
message	B
passing	I
algorithm	O
that	O
evaluates	O
in	O
terms	O
of	O
at	O
each	O
step	O
we	O
absorb	O
the	O
effect	O
of	O
observation	O
through	O
the	O
emission	B
probability	B
multiply	O
by	O
the	O
transition	O
matrix	O
and	O
then	O
marginalize	O
out	O
this	O
is	O
illustrated	O
in	O
figure	O
again	O
we	O
need	O
a	O
starting	O
condition	O
for	O
the	O
recursion	O
namely	O
a	O
value	O
for	O
this	O
can	O
be	O
obtained	O
by	O
setting	O
n	O
n	O
in	O
and	O
replacing	O
with	O
its	O
definition	O
to	O
give	O
pznx	O
px	O
zn	O
px	O
which	O
we	O
see	O
will	O
be	O
correct	O
provided	O
we	O
take	O
for	O
all	O
settings	O
of	O
zn	O
in	O
the	O
m	O
step	O
equations	O
the	O
quantity	O
px	O
will	O
cancel	O
out	O
as	O
can	O
be	O
seen	O
for	O
instance	O
in	O
the	O
m-step	O
equation	O
for	O
k	O
given	O
by	O
which	O
takes	O
the	O
form	O
k	O
however	O
the	O
quantity	O
px	O
represents	O
the	O
likelihood	B
function	I
whose	O
value	O
we	O
typically	O
wish	O
to	O
monitor	O
during	O
the	O
em	B
optimization	O
and	O
so	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
evaluate	O
it	O
if	O
we	O
sum	O
both	O
sides	O
of	O
over	O
zn	O
and	O
use	O
the	O
fact	O
that	O
the	O
left-hand	O
side	O
is	O
a	O
normalized	O
distribution	O
we	O
obtain	O
px	O
zn	O
hidden	O
markov	O
models	O
thus	O
we	O
can	O
evaluate	O
the	O
likelihood	B
function	I
by	O
computing	O
this	O
sum	O
for	O
any	O
convenient	O
choice	O
of	O
n	O
for	O
instance	O
if	O
we	O
only	O
want	O
to	O
evaluate	O
the	O
likelihood	B
function	I
then	O
we	O
can	O
do	O
this	O
by	O
running	O
the	O
recursion	O
from	O
the	O
start	O
to	O
the	O
end	O
of	O
the	O
chain	O
and	O
then	O
use	O
this	O
result	O
for	O
n	O
n	O
making	O
use	O
of	O
the	O
fact	O
that	O
is	O
a	O
vector	O
of	O
in	O
this	O
case	O
no	O
recursion	O
is	O
required	O
and	O
we	O
simply	O
have	O
px	O
zn	O
let	O
us	O
take	O
a	O
moment	O
to	O
interpret	O
this	O
result	O
for	O
px	O
recall	O
that	O
to	O
compute	O
the	O
likelihood	O
we	O
should	O
take	O
the	O
joint	O
distribution	O
px	O
z	O
and	O
sum	O
over	O
all	O
possible	O
values	O
of	O
z	O
each	O
such	O
value	O
represents	O
a	O
particular	O
choice	O
of	O
hidden	O
state	O
for	O
every	O
time	O
step	O
in	O
other	O
words	O
every	O
term	O
in	O
the	O
summation	O
is	O
a	O
path	O
through	O
the	O
lattice	B
diagram	I
and	O
recall	O
that	O
there	O
are	O
exponentially	O
many	O
such	O
paths	O
by	O
expressing	O
the	O
likelihood	B
function	I
in	O
the	O
form	O
we	O
have	O
reduced	O
the	O
computational	O
cost	O
from	O
being	O
exponential	O
in	O
the	O
length	O
of	O
the	O
chain	O
to	O
being	O
linear	O
by	O
swapping	O
the	O
order	O
of	O
the	O
summation	O
and	O
multiplications	O
so	O
that	O
at	O
each	O
time	O
step	O
n	O
we	O
sum	O
the	O
contributions	O
from	O
all	O
paths	O
passing	O
through	O
each	O
of	O
the	O
states	O
znk	O
to	O
give	O
the	O
intermediate	O
quantities	O
next	O
we	O
consider	O
the	O
evaluation	O
of	O
the	O
quantities	O
zn	O
which	O
correspond	O
to	O
the	O
values	O
of	O
the	O
conditional	B
probabilities	O
pzn	O
znx	O
for	O
each	O
of	O
the	O
k	O
k	O
settings	O
for	O
zn	O
using	O
the	O
definition	O
of	O
zn	O
and	O
applying	O
bayes	B
theorem	O
we	O
have	O
zn	O
pzn	O
znx	O
pxzn	O
znpzn	O
zn	O
xn	O
xnznpznzn	O
px	O
px	O
px	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
conditional	B
independence	I
property	O
together	O
with	O
the	O
definitions	O
of	O
and	O
given	O
by	O
and	O
thus	O
we	O
can	O
calculate	O
the	O
zn	O
directly	O
by	O
using	O
the	O
results	O
of	O
the	O
and	O
recursions	O
let	O
us	O
summarize	O
the	O
steps	O
required	O
to	O
train	O
a	O
hidden	B
markov	B
model	I
using	O
the	O
em	B
algorithm	I
we	O
first	O
make	O
an	O
initial	O
selection	O
of	O
the	O
parameters	O
old	O
where	O
a	O
the	O
a	O
and	O
parameters	O
are	O
often	O
initialized	O
either	O
uniformly	O
or	O
randomly	O
from	O
a	O
uniform	B
distribution	I
their	O
non-negativity	O
and	O
summation	O
constraints	O
initialization	O
of	O
the	O
parameters	O
will	O
depend	O
on	O
the	O
form	O
of	O
the	O
distribution	O
for	O
instance	O
in	O
the	O
case	O
of	O
gaussians	O
the	O
parameters	O
k	O
might	O
be	O
initialized	O
by	O
applying	O
the	O
k-means	O
algorithm	O
to	O
the	O
data	O
and	O
k	O
might	O
be	O
initialized	O
to	O
the	O
covariance	B
matrix	O
of	O
the	O
corresponding	O
k	O
means	O
cluster	O
then	O
we	O
run	O
both	O
the	O
forward	O
recursion	O
and	O
the	O
backward	O
recursion	O
and	O
use	O
the	O
results	O
to	O
evaluate	O
and	O
zn	O
at	O
this	O
stage	O
we	O
can	O
also	O
evaluate	O
the	O
likelihood	B
function	I
sequential	B
data	I
this	O
completes	O
the	O
e	O
step	O
and	O
we	O
use	O
the	O
results	O
to	O
find	O
a	O
revised	O
set	O
of	O
parameters	O
new	O
using	O
the	O
m-step	O
equations	O
from	O
section	O
we	O
then	O
continue	O
to	O
alternate	O
between	O
e	O
and	O
m	O
steps	O
until	O
some	O
convergence	O
criterion	O
is	O
satisfied	O
for	O
instance	O
when	O
the	O
change	O
in	O
the	O
likelihood	B
function	I
is	O
below	O
some	O
threshold	O
note	O
that	O
in	O
these	O
recursion	O
relations	O
the	O
observations	O
enter	O
through	O
conditional	B
distributions	O
of	O
the	O
form	O
pxnzn	O
the	O
recursions	O
are	O
therefore	O
independent	B
of	O
the	O
type	O
or	O
dimensionality	O
of	O
the	O
observed	O
variables	O
or	O
the	O
form	O
of	O
this	O
conditional	B
distribution	O
so	O
long	O
as	O
its	O
value	O
can	O
be	O
computed	O
for	O
each	O
of	O
the	O
k	O
possible	O
states	O
of	O
zn	O
since	O
the	O
observed	O
variables	O
are	O
fixed	O
the	O
quantities	O
pxnzn	O
can	O
be	O
pre-computed	O
as	O
functions	O
of	O
zn	O
at	O
the	O
start	O
of	O
the	O
em	B
algorithm	I
and	O
remain	O
fixed	O
throughout	O
exercise	O
we	O
have	O
seen	O
in	O
earlier	O
chapters	O
that	O
the	O
maximum	B
likelihood	I
approach	O
is	O
most	O
effective	O
when	O
the	O
number	O
of	O
data	O
points	O
is	O
large	O
in	O
relation	O
to	O
the	O
number	O
of	O
parameters	O
here	O
we	O
note	O
that	O
a	O
hidden	B
markov	B
model	I
can	O
be	O
trained	O
effectively	O
using	O
maximum	B
likelihood	I
provided	O
the	O
training	B
sequence	O
is	O
sufficiently	O
long	O
alternatively	O
we	O
can	O
make	O
use	O
of	O
multiple	O
shorter	O
sequences	O
which	O
requires	O
a	O
straightforward	O
modification	O
of	O
the	O
hidden	B
markov	B
model	I
em	B
algorithm	I
in	O
the	O
case	O
of	O
left-to-right	B
models	O
this	O
is	O
particularly	O
important	O
because	O
in	O
a	O
given	O
observation	O
sequence	O
a	O
given	O
state	O
transition	O
corresponding	O
to	O
a	O
nondiagonal	O
element	O
of	O
a	O
will	O
seen	O
at	O
most	O
once	O
another	O
quantity	O
of	O
interest	O
is	O
the	O
predictive	B
distribution	I
in	O
which	O
the	O
observed	O
data	O
is	O
x	O
xn	O
and	O
we	O
wish	O
to	O
predict	O
xn	O
which	O
would	O
be	O
important	O
for	O
real-time	O
applications	O
such	O
as	O
financial	O
forecasting	O
again	O
we	O
make	O
use	O
of	O
the	O
sum	O
and	O
product	O
rules	O
together	O
with	O
the	O
conditional	B
independence	I
properties	O
and	O
giving	O
pxn	O
px	O
pxn	O
zn	O
pxn	O
pxn	O
pxn	O
pzn	O
znx	O
pzn	O
zn	O
zn	O
zn	O
pxn	O
pzn	O
pzn	O
x	O
px	O
pxn	O
pzn	O
zn	O
which	O
can	O
be	O
evaluated	O
by	O
first	O
running	O
a	O
forward	O
recursion	O
and	O
then	O
computing	O
the	O
final	O
summations	O
over	O
zn	O
and	O
zn	O
the	O
result	O
of	O
the	O
first	O
summation	O
over	O
zn	O
can	O
be	O
stored	O
and	O
used	O
once	O
the	O
value	O
of	O
xn	O
is	O
observed	O
in	O
order	O
to	O
run	O
the	O
recursion	O
forward	O
to	O
the	O
next	O
step	O
in	O
order	O
to	O
predict	O
the	O
subsequent	O
value	O
xn	O
figure	O
a	O
fragment	O
of	O
the	O
factor	B
graph	I
representation	O
for	O
the	O
hidden	B
markov	B
model	I
hidden	O
markov	O
models	O
zn	O
n	O
zn	O
gn	O
gn	O
xn	O
xn	O
note	O
that	O
in	O
the	O
influence	O
of	O
all	O
data	O
from	O
to	O
xn	O
is	O
summarized	O
in	O
the	O
k	O
values	O
of	O
thus	O
the	O
predictive	B
distribution	I
can	O
be	O
carried	O
forward	O
indefinitely	O
using	O
a	O
fixed	O
amount	O
of	O
storage	O
as	O
may	O
be	O
required	O
for	O
real-time	O
applications	O
here	O
we	O
have	O
discussed	O
the	O
estimation	O
of	O
the	O
parameters	O
of	O
an	O
hmm	O
using	O
maximum	B
likelihood	I
this	O
framework	O
is	O
easily	O
extended	B
to	O
regularized	O
maximum	B
likelihood	I
by	O
introducing	O
priors	O
over	O
the	O
model	O
parameters	O
a	O
and	O
whose	O
values	O
are	O
then	O
estimated	O
by	O
maximizing	O
their	O
posterior	B
probability	B
this	O
can	O
again	O
be	O
done	O
using	O
the	O
em	B
algorithm	I
in	O
which	O
the	O
e	O
step	O
is	O
the	O
same	O
as	O
discussed	O
above	O
and	O
the	O
m	O
step	O
involves	O
adding	O
the	O
log	O
of	O
the	O
prior	B
distribution	O
p	O
to	O
the	O
function	O
q	O
old	O
before	O
maximization	O
and	O
represents	O
a	O
straightforward	O
application	O
of	O
the	O
techniques	O
developed	O
at	O
various	O
points	O
in	O
this	O
book	O
furthermore	O
we	O
can	O
use	O
variational	B
methods	O
to	O
give	O
a	O
fully	O
bayesian	B
treatment	O
of	O
the	O
hmm	O
in	O
which	O
we	O
marginalize	O
over	O
the	O
parameter	O
distributions	O
as	O
with	O
maximum	B
likelihood	I
this	O
leads	O
to	O
a	O
two-pass	O
forward-backward	O
recursion	O
to	O
compute	O
posterior	O
probabilities	O
the	O
sum-product	B
algorithm	I
for	O
the	O
hmm	O
the	O
directed	B
graph	O
that	O
represents	O
the	O
hidden	B
markov	B
model	I
shown	O
in	O
figure	O
is	O
a	O
tree	B
and	O
so	O
we	O
can	O
solve	O
the	O
problem	O
of	O
finding	O
local	B
marginals	O
for	O
the	O
hidden	O
variables	O
using	O
the	O
sum-product	B
algorithm	I
not	O
surprisingly	O
this	O
turns	O
out	O
to	O
be	O
equivalent	O
to	O
the	O
forward-backward	B
algorithm	I
considered	O
in	O
the	O
previous	O
section	O
and	O
so	O
the	O
sum-product	B
algorithm	I
therefore	O
provides	O
us	O
with	O
a	O
simple	O
way	O
to	O
derive	O
the	O
alpha-beta	O
recursion	O
formulae	O
we	O
begin	O
by	O
transforming	O
the	O
directed	B
graph	O
of	O
figure	O
into	O
a	O
factor	B
graph	I
of	O
which	O
a	O
representative	O
fragment	O
is	O
shown	O
in	O
figure	O
this	O
form	O
of	O
the	O
factor	B
graph	I
shows	O
all	O
variables	O
both	O
latent	O
and	O
observed	O
explicitly	O
however	O
for	O
the	O
purpose	O
of	O
solving	O
the	O
inference	B
problem	O
we	O
shall	O
always	O
be	O
conditioning	O
on	O
the	O
variables	O
xn	O
and	O
so	O
we	O
can	O
simplify	O
the	O
factor	B
graph	I
by	O
absorbing	O
the	O
emission	O
probabilities	O
into	O
the	O
transition	B
probability	B
factors	O
this	O
leads	O
to	O
the	O
simplified	O
factor	B
graph	I
representation	O
in	O
figure	O
in	O
which	O
the	O
factors	O
are	O
given	O
by	O
fnzn	O
zn	O
pznzn	O
section	O
section	O
sequential	B
data	I
figure	O
a	O
simplified	O
form	O
of	O
factor	B
graph	I
to	O
describe	O
the	O
hidden	B
markov	B
model	I
h	O
fn	O
zn	O
zn	O
to	O
derive	O
the	O
alpha-beta	O
algorithm	O
we	O
denote	O
the	O
final	O
hidden	B
variable	I
zn	O
as	O
the	O
root	B
node	B
and	O
first	O
pass	O
messages	O
from	O
the	O
leaf	O
node	B
h	O
to	O
the	O
root	O
from	O
the	O
general	O
results	O
and	O
for	O
message	O
propagation	O
we	O
see	O
that	O
the	O
messages	O
which	O
are	O
propagated	O
in	O
the	O
hidden	B
markov	B
model	I
take	O
the	O
form	O
zn	O
fnzn	O
fn	O
zn	O
fn	O
znzn	O
fnzn	O
zn	O
zn	O
fnzn	O
zn	O
these	O
equations	O
represent	O
the	O
propagation	O
of	O
messages	O
forward	O
along	O
the	O
chain	O
and	O
are	O
equivalent	O
to	O
the	O
alpha	O
recursions	O
derived	O
in	O
the	O
previous	O
section	O
as	O
we	O
shall	O
now	O
show	O
note	O
that	O
because	O
the	O
variable	O
nodes	O
zn	O
have	O
only	O
two	O
neighbours	O
they	O
perform	O
no	O
computation	O
sion	O
for	O
the	O
f	O
z	O
messages	O
of	O
the	O
form	O
we	O
can	O
eliminate	O
zn	O
fnzn	O
from	O
using	O
to	O
give	O
a	O
recur	O
fn	O
znzn	O
fnzn	O
zn	O
fn	O
zn	O
if	O
we	O
now	O
recall	O
the	O
definition	O
and	O
if	O
we	O
define	O
zn	O
fn	O
znzn	O
then	O
we	O
obtain	O
the	O
alpha	O
recursion	O
given	O
by	O
we	O
also	O
need	O
to	O
verify	O
that	O
the	O
quantities	O
are	O
themselves	O
equivalent	O
to	O
those	O
defined	O
previously	O
this	O
is	O
easily	O
done	O
by	O
using	O
the	O
initial	O
condition	O
and	O
noting	O
that	O
is	O
given	O
by	O
which	O
is	O
identical	O
to	O
because	O
the	O
initial	O
is	O
the	O
same	O
and	O
because	O
they	O
are	O
iteratively	O
computed	O
using	O
the	O
same	O
equation	O
all	O
subsequent	O
quantities	O
must	O
be	O
the	O
same	O
next	O
we	O
consider	O
the	O
messages	O
that	O
are	O
propagated	O
from	O
the	O
root	B
node	B
back	O
to	O
the	O
leaf	O
node	B
these	O
take	O
the	O
form	O
fnzn	O
where	O
as	O
before	O
we	O
have	O
eliminated	O
the	O
messages	O
of	O
the	O
type	O
z	O
f	O
since	O
the	O
variable	O
nodes	O
perform	O
no	O
computation	O
using	O
the	O
definition	O
to	O
substitute	O
for	O
and	O
defining	O
znzn	O
hidden	O
markov	O
models	O
we	O
obtain	O
the	O
beta	B
recursion	I
given	O
by	O
again	O
we	O
can	O
verify	O
that	O
the	O
beta	O
variables	O
themselves	O
are	O
equivalent	O
by	O
noting	O
that	O
implies	O
that	O
the	O
initial	O
message	O
send	O
by	O
the	O
root	O
variable	O
node	B
is	O
zn	O
fn	O
which	O
is	O
identical	O
to	O
the	O
initialization	O
of	O
given	O
in	O
section	O
the	O
sum-product	B
algorithm	I
also	O
specifies	O
how	O
to	O
evaluate	O
the	O
marginals	O
once	O
all	O
the	O
messages	O
have	O
been	O
evaluated	O
in	O
particular	O
the	O
result	O
shows	O
that	O
the	O
local	B
marginal	B
at	O
the	O
node	B
zn	O
is	O
given	O
by	O
the	O
product	O
of	O
the	O
incoming	O
messages	O
because	O
we	O
have	O
conditioned	O
on	O
the	O
variables	O
x	O
xn	O
we	O
are	O
computing	O
the	O
joint	O
distribution	O
pzn	O
x	O
fn	O
znzn	O
znzn	O
dividing	O
both	O
sides	O
by	O
px	O
we	O
then	O
obtain	O
pzn	O
x	O
px	O
px	O
exercise	O
in	O
agreement	O
with	O
the	O
result	O
can	O
similarly	O
be	O
derived	O
from	O
scaling	O
factors	O
there	O
is	O
an	O
important	O
issue	O
that	O
must	O
be	O
addressed	O
before	O
we	O
can	O
make	O
use	O
of	O
the	O
forward	O
backward	O
algorithm	O
in	O
practice	O
from	O
the	O
recursion	O
relation	O
we	O
note	O
that	O
at	O
each	O
step	O
the	O
new	O
value	O
is	O
obtained	O
from	O
the	O
previous	O
value	O
by	O
multiplying	O
by	O
quantities	O
pznzn	O
and	O
pxnzn	O
because	O
these	O
probabilities	O
are	O
often	O
significantly	O
less	O
than	O
unity	O
as	O
we	O
work	O
our	O
way	O
forward	O
along	O
the	O
chain	O
the	O
values	O
of	O
can	O
go	O
to	O
zero	O
exponentially	O
quickly	O
for	O
moderate	O
lengths	O
of	O
chain	O
or	O
so	O
the	O
calculation	O
of	O
the	O
will	O
soon	O
exceed	O
the	O
dynamic	O
range	O
of	O
the	O
computer	O
even	O
if	O
double	O
precision	O
floating	O
point	O
is	O
used	O
in	O
the	O
case	O
of	O
i	O
i	O
d	O
data	O
we	O
implicitly	O
circumvented	O
this	O
problem	O
with	O
the	O
evaluation	O
of	O
likelihood	O
functions	O
by	O
taking	O
logarithms	O
unfortunately	O
this	O
will	O
not	O
help	O
here	O
because	O
we	O
are	O
forming	O
sums	O
of	O
products	O
of	O
small	O
numbers	O
are	O
in	O
fact	O
implicitly	O
summing	O
over	O
all	O
possible	O
paths	O
through	O
the	O
lattice	B
diagram	I
of	O
figure	O
we	O
therefore	O
work	O
with	O
re-scaled	O
versions	O
of	O
and	O
whose	O
values	O
remain	O
of	O
order	O
unity	O
as	O
we	O
shall	O
see	O
the	O
corresponding	O
scaling	O
factors	O
cancel	O
out	O
when	O
we	O
use	O
these	O
re-scaled	O
quantities	O
in	O
the	O
em	B
algorithm	I
in	O
we	O
defined	O
xn	O
zn	O
representing	O
the	O
joint	O
distribution	O
of	O
all	O
the	O
observations	O
up	O
to	O
xn	O
and	O
the	O
latent	B
variable	I
zn	O
now	O
we	O
define	O
a	O
normalized	O
version	O
of	O
given	O
by	O
xn	O
xn	O
which	O
we	O
expect	O
to	O
be	O
well	O
behaved	O
numerically	O
because	O
it	O
is	O
a	O
probability	B
distribution	O
over	O
k	O
variables	O
for	O
any	O
value	O
of	O
n	O
in	O
order	O
to	O
relate	O
the	O
scaled	O
and	O
original	O
alpha	O
variables	O
we	O
introduce	O
scaling	O
factors	O
defined	O
by	O
conditional	B
distributions	O
over	O
the	O
observed	O
variables	O
cn	O
xn	O
sequential	B
data	I
from	O
the	O
product	B
rule	I
we	O
then	O
have	O
cm	O
xn	O
and	O
so	O
cm	O
xn	O
pxnzn	O
we	O
can	O
then	O
turn	O
the	O
recursion	O
equation	O
for	O
into	O
one	O
given	O
by	O
note	O
that	O
at	O
each	O
stage	O
of	O
the	O
forward	O
message	B
passing	I
phase	O
used	O
to	O
that	O
normalizes	O
the	O
right-hand	O
side	O
of	O
to	O
we	O
can	O
similarly	O
define	O
re-scaled	O
using	O
are	O
simply	O
the	O
ratio	O
of	O
two	O
conditional	B
probabilities	O
which	O
will	O
again	O
remain	O
within	O
machine	O
precision	O
because	O
from	O
the	O
quan	O
we	O
have	O
to	O
evaluate	O
and	O
store	O
cn	O
which	O
is	O
easily	O
done	O
because	O
it	O
is	O
the	O
coefficient	O
cm	O
zn	O
xnzn	O
xn	O
the	O
recursion	O
result	O
for	O
then	O
gives	O
the	O
following	O
recursion	O
for	O
the	O
re-scaled	O
variables	O
in	O
applying	O
this	O
recursion	O
relation	O
we	O
make	O
use	O
of	O
the	O
scaling	O
factors	O
cn	O
that	O
were	O
previously	O
computed	O
in	O
the	O
phase	O
from	O
we	O
see	O
that	O
the	O
likelihood	B
function	I
can	O
be	O
found	O
using	O
px	O
cn	O
exercise	O
similarly	O
using	O
and	O
together	O
with	O
we	O
see	O
that	O
the	O
required	O
marginals	O
are	O
given	O
by	O
zn	O
section	O
hidden	O
markov	O
models	O
finally	O
we	O
note	O
that	O
there	O
is	O
an	O
alternative	O
formulation	O
of	O
the	O
forward-backward	B
algorithm	I
in	O
which	O
the	O
backward	O
pass	O
is	O
defined	O
by	O
a	O
recursion	O
based	O
the	O
quantities	O
instead	O
of	O
this	O
recursion	O
requires	O
that	O
the	O
forward	O
pass	O
be	O
completed	O
first	O
so	O
that	O
all	O
the	O
quantities	O
are	O
available	O
for	O
the	O
backward	O
pass	O
whereas	O
the	O
forward	O
and	O
backward	O
passes	O
of	O
the	O
algorithm	O
can	O
be	O
done	O
independently	O
although	O
these	O
two	O
algorithms	O
have	O
comparable	O
computational	O
cost	O
the	O
version	O
is	O
the	O
most	O
commonly	O
encountered	O
one	O
in	O
the	O
case	O
of	O
hidden	O
markov	O
models	O
whereas	O
for	O
linear	O
dynamical	O
systems	O
a	O
recursion	O
analogous	O
to	O
the	O
form	O
is	O
more	O
usual	O
the	O
viterbi	B
algorithm	I
in	O
many	O
applications	O
of	O
hidden	O
markov	O
models	O
the	O
latent	O
variables	O
have	O
some	O
meaningful	O
interpretation	O
and	O
so	O
it	O
is	O
often	O
of	O
interest	O
to	O
find	O
the	O
most	O
probable	O
sequence	O
of	O
hidden	O
states	O
for	O
a	O
given	O
observation	O
sequence	O
for	O
instance	O
in	O
speech	B
recognition	I
we	O
might	O
wish	O
to	O
find	O
the	O
most	O
probable	O
phoneme	O
sequence	O
for	O
a	O
given	O
series	O
of	O
acoustic	O
observations	O
because	O
the	O
graph	O
for	O
the	O
hidden	B
markov	B
model	I
is	O
a	O
directed	B
tree	B
this	O
problem	O
can	O
be	O
solved	O
exactly	O
using	O
the	O
max-sum	B
algorithm	I
we	O
recall	O
from	O
our	O
discussion	O
in	O
section	O
that	O
the	O
problem	O
of	O
finding	O
the	O
most	O
probable	O
sequence	O
of	O
latent	O
states	O
is	O
not	O
the	O
same	O
as	O
that	O
of	O
finding	O
the	O
set	O
of	O
states	O
that	O
are	O
individually	O
the	O
most	O
probable	O
the	O
latter	O
problem	O
can	O
be	O
solved	O
by	O
first	O
running	O
the	O
forward-backward	B
algorithm	I
to	O
find	O
the	O
latent	B
variable	I
marginals	O
and	O
then	O
maximizing	O
each	O
of	O
these	O
individually	O
et	O
al	O
however	O
the	O
set	O
of	O
such	O
states	O
will	O
not	O
in	O
general	O
correspond	O
to	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
in	O
fact	O
this	O
set	O
of	O
states	O
might	O
even	O
represent	O
a	O
sequence	O
having	O
zero	O
probability	B
if	O
it	O
so	O
happens	O
that	O
two	O
successive	O
states	O
which	O
in	O
isolation	O
are	O
individually	O
the	O
most	O
probable	O
are	O
such	O
that	O
the	O
transition	O
matrix	O
element	O
connecting	O
them	O
is	O
zero	O
in	O
practice	O
we	O
are	O
usually	O
interested	O
in	O
finding	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
and	O
this	O
can	O
be	O
solved	O
efficiently	O
using	O
the	O
max-sum	B
algorithm	I
which	O
in	O
the	O
context	O
of	O
hidden	O
markov	O
models	O
is	O
known	O
as	O
the	O
viterbi	B
algorithm	I
note	O
that	O
the	O
max-sum	B
algorithm	I
works	O
with	O
log	O
probabilities	O
and	O
so	O
there	O
is	O
no	O
need	O
to	O
use	O
re-scaled	O
variables	O
as	O
was	O
done	O
with	O
the	O
forward-backward	B
algorithm	I
figure	O
shows	O
a	O
fragment	O
of	O
the	O
hidden	B
markov	B
model	I
expanded	O
as	O
lattice	B
diagram	I
as	O
we	O
have	O
already	O
noted	O
the	O
number	O
of	O
possible	O
paths	O
through	O
the	O
lattice	O
grows	O
exponentially	O
with	O
the	O
length	O
of	O
the	O
chain	O
the	O
viterbi	B
algorithm	I
searches	O
this	O
space	O
of	O
paths	O
efficiently	O
to	O
find	O
the	O
most	O
probable	O
path	O
with	O
a	O
computational	O
cost	O
that	O
grows	O
only	O
linearly	O
with	O
the	O
length	O
of	O
the	O
chain	O
as	O
with	O
the	O
sum-product	B
algorithm	I
we	O
first	O
represent	O
the	O
hidden	B
markov	B
model	I
as	O
a	O
factor	B
graph	I
as	O
shown	O
in	O
figure	O
again	O
we	O
treat	O
the	O
variable	O
node	B
zn	O
as	O
the	O
root	O
and	O
pass	O
messages	O
to	O
the	O
root	O
starting	O
with	O
the	O
leaf	O
nodes	O
using	O
the	O
results	O
and	O
we	O
see	O
that	O
the	O
messages	O
passed	O
in	O
the	O
max-sum	B
algorithm	I
are	O
given	O
by	O
zn	O
fn	O
znzn	O
max	O
zn	O
ln	O
zn	O
sequential	B
data	I
figure	O
a	O
fragment	O
of	O
the	O
hmm	O
lattice	O
showing	O
two	O
possible	O
paths	O
the	O
viterbi	B
algorithm	I
efficiently	O
determines	O
the	O
most	O
probable	O
path	O
from	O
amongst	O
the	O
exponentially	O
many	O
possibilities	O
for	O
any	O
given	O
path	O
the	O
corresponding	O
probability	B
is	O
given	O
by	O
the	O
product	O
of	O
the	O
elements	O
of	O
the	O
transition	O
matrix	O
ajk	O
corresponding	O
to	O
the	O
probabilities	O
for	O
each	O
segment	O
of	O
the	O
path	O
along	O
with	O
the	O
emission	O
densities	O
pxnk	O
associated	O
with	O
each	O
node	B
on	O
the	O
path	O
k	O
k	O
k	O
n	O
n	O
n	O
n	O
if	O
we	O
eliminate	O
zn	O
between	O
these	O
two	O
equations	O
and	O
make	O
use	O
of	O
we	O
obtain	O
a	O
recursion	O
for	O
the	O
f	O
z	O
messages	O
of	O
the	O
form	O
ln	O
max	O
zn	O
where	O
we	O
have	O
introduced	O
the	O
notation	O
fn	O
znzn	O
from	O
and	O
these	O
messages	O
are	O
initialized	O
using	O
ln	O
ln	O
where	O
we	O
have	O
used	O
note	O
that	O
to	O
keep	O
the	O
notation	O
uncluttered	O
we	O
omit	O
the	O
dependence	O
on	O
the	O
model	O
parameters	O
that	O
are	O
held	O
fixed	O
when	O
finding	O
the	O
most	O
probable	O
sequence	O
exercise	O
the	O
viterbi	B
algorithm	I
can	O
also	O
be	O
derived	O
directly	O
from	O
the	O
definition	O
of	O
the	O
joint	O
distribution	O
by	O
taking	O
the	O
logarithm	O
and	O
then	O
exchanging	O
maximizations	O
and	O
summations	O
it	O
is	O
easily	O
seen	O
that	O
the	O
quantities	O
have	O
the	O
probabilistic	O
interpretation	O
max	O
xn	O
zn	O
once	O
we	O
have	O
completed	O
the	O
final	O
maximization	O
over	O
zn	O
we	O
will	O
obtain	O
the	O
value	O
of	O
the	O
joint	O
distribution	O
px	O
z	O
corresponding	O
to	O
the	O
most	O
probable	O
path	O
we	O
also	O
wish	O
to	O
find	O
the	O
sequence	O
of	O
latent	B
variable	I
values	O
that	O
corresponds	O
to	O
this	O
path	O
to	O
do	O
this	O
we	O
simply	O
make	O
use	O
of	O
the	O
back-tracking	B
procedure	O
discussed	O
in	O
section	O
specifically	O
we	O
note	O
that	O
the	O
maximization	O
over	O
zn	O
must	O
be	O
performed	O
for	O
each	O
of	O
the	O
k	O
possible	O
values	O
of	O
suppose	O
we	O
keep	O
a	O
record	O
of	O
the	O
values	O
of	O
zn	O
that	O
correspond	O
to	O
the	O
maxima	O
for	O
each	O
value	O
of	O
the	O
k	O
values	O
of	O
let	O
us	O
denote	O
this	O
function	O
by	O
where	O
k	O
k	O
once	O
we	O
have	O
passed	O
messages	O
to	O
the	O
end	O
of	O
the	O
chain	O
and	O
found	O
the	O
most	O
probable	O
state	O
of	O
zn	O
we	O
can	O
then	O
use	O
this	O
function	O
to	O
backtrack	O
along	O
the	O
chain	O
by	O
applying	O
it	O
recursively	O
kmax	O
n	O
hidden	O
markov	O
models	O
intuitively	O
we	O
can	O
understand	O
the	O
viterbi	B
algorithm	I
as	O
follows	O
naively	O
we	O
could	O
consider	O
explicitly	O
all	O
of	O
the	O
exponentially	O
many	O
paths	O
through	O
the	O
lattice	O
evaluate	O
the	O
probability	B
for	O
each	O
and	O
then	O
select	O
the	O
path	O
having	O
the	O
highest	O
probability	B
however	O
we	O
notice	O
that	O
we	O
can	O
make	O
a	O
dramatic	O
saving	O
in	O
computational	O
cost	O
as	O
follows	O
suppose	O
that	O
for	O
each	O
path	O
we	O
evaluate	O
its	O
probability	B
by	O
summing	O
up	O
products	O
of	O
transition	O
and	O
emission	O
probabilities	O
as	O
we	O
work	O
our	O
way	O
forward	O
along	O
each	O
path	O
through	O
the	O
lattice	O
consider	O
a	O
particular	O
time	O
step	O
n	O
and	O
a	O
particular	O
state	O
k	O
at	O
that	O
time	O
step	O
there	O
will	O
be	O
many	O
possible	O
paths	O
converging	O
on	O
the	O
corresponding	O
node	B
in	O
the	O
lattice	B
diagram	I
however	O
we	O
need	O
only	O
retain	O
that	O
particular	O
path	O
that	O
so	O
far	O
has	O
the	O
highest	O
probability	B
because	O
there	O
are	O
k	O
states	O
at	O
time	O
step	O
n	O
we	O
need	O
to	O
keep	O
track	O
of	O
k	O
such	O
paths	O
at	O
time	O
step	O
n	O
there	O
will	O
be	O
k	O
possible	O
paths	O
to	O
consider	O
comprising	O
k	O
possible	O
paths	O
leading	O
out	O
of	O
each	O
of	O
the	O
k	O
current	O
states	O
but	O
again	O
we	O
need	O
only	O
retain	O
k	O
of	O
these	O
corresponding	O
to	O
the	O
best	O
path	O
for	O
each	O
state	O
at	O
time	O
when	O
we	O
reach	O
the	O
final	O
time	O
step	O
n	O
we	O
will	O
discover	O
which	O
state	O
corresponds	O
to	O
the	O
overall	O
most	O
probable	O
path	O
because	O
there	O
is	O
a	O
unique	O
path	O
coming	O
into	O
that	O
state	O
we	O
can	O
trace	O
the	O
path	O
back	O
to	O
step	O
n	O
to	O
see	O
what	O
state	O
it	O
occupied	O
at	O
that	O
time	O
and	O
so	O
on	O
back	O
through	O
the	O
lattice	O
to	O
the	O
state	O
n	O
extensions	O
of	O
the	O
hidden	B
markov	B
model	I
the	O
basic	O
hidden	B
markov	B
model	I
along	O
with	O
the	O
standard	O
training	B
algorithm	O
based	O
on	O
maximum	B
likelihood	I
has	O
been	O
extended	B
in	O
numerous	O
ways	O
to	O
meet	O
the	O
requirements	O
of	O
particular	O
applications	O
here	O
we	O
discuss	O
a	O
few	O
of	O
the	O
more	O
important	O
examples	O
we	O
see	O
from	O
the	O
digits	O
example	O
in	O
figure	O
that	O
hidden	O
markov	O
models	O
can	O
be	O
quite	O
poor	O
generative	O
models	O
for	O
the	O
data	O
because	O
many	O
of	O
the	O
synthetic	O
digits	O
look	O
quite	O
unrepresentative	O
of	O
the	O
training	B
data	O
if	O
the	O
goal	O
is	O
sequence	O
classification	B
there	O
can	O
be	O
significant	O
benefit	O
in	O
determining	O
the	O
parameters	O
of	O
hidden	O
markov	O
models	O
using	O
discriminative	O
rather	O
than	O
maximum	B
likelihood	I
techniques	O
suppose	O
we	O
have	O
a	O
training	B
set	I
of	O
r	O
observation	O
sequences	O
xr	O
where	O
r	O
r	O
each	O
of	O
which	O
is	O
labelled	O
according	O
to	O
its	O
class	O
m	O
where	O
m	O
m	O
for	O
each	O
class	O
we	O
have	O
a	O
separate	O
hidden	B
markov	B
model	I
with	O
its	O
own	O
parameters	O
m	O
and	O
we	O
treat	O
the	O
problem	O
of	O
determining	O
the	O
parameter	O
values	O
as	O
a	O
standard	O
classification	B
problem	O
in	O
which	O
we	O
optimize	O
the	O
cross-entropy	O
ln	O
pmrxr	O
using	O
bayes	B
theorem	O
this	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
sequence	O
probabilities	O
associated	O
with	O
the	O
hidden	O
markov	O
models	O
ln	O
pxr	O
rpmr	O
pxr	O
lplr	O
where	O
pm	O
is	O
the	O
prior	B
probability	B
of	O
class	O
m	O
optimization	O
of	O
this	O
cost	B
function	I
is	O
more	O
complex	O
than	O
for	O
maximum	B
likelihood	I
and	O
in	O
particular	O
sequential	B
data	I
figure	O
section	O
of	O
an	O
autoregressive	B
hidden	B
markov	B
model	I
in	O
which	O
the	O
distribution	O
of	O
the	O
observation	O
xn	O
depends	O
on	O
a	O
subset	O
of	O
the	O
previous	O
observations	O
as	O
well	O
as	O
on	O
the	O
hidden	O
state	O
zn	O
in	O
this	O
example	O
the	O
distribution	O
of	O
xn	O
depends	O
on	O
the	O
two	O
previous	O
observations	O
xn	O
and	O
xn	O
zn	O
zn	O
xn	O
xn	O
requires	O
that	O
every	O
training	B
sequence	O
be	O
evaluated	O
under	O
each	O
of	O
the	O
models	O
in	O
order	O
to	O
compute	O
the	O
denominator	O
in	O
hidden	O
markov	O
models	O
coupled	O
with	O
discriminative	O
training	B
methods	O
are	O
widely	O
used	O
in	O
speech	B
recognition	I
a	O
significant	O
weakness	O
of	O
the	O
hidden	B
markov	B
model	I
is	O
the	O
way	O
in	O
which	O
it	O
represents	O
the	O
distribution	O
of	O
times	O
for	O
which	O
the	O
system	O
remains	O
in	O
a	O
given	O
state	O
to	O
see	O
the	O
problem	O
note	O
that	O
the	O
probability	B
that	O
a	O
sequence	O
sampled	O
from	O
a	O
given	O
hidden	B
markov	B
model	I
will	O
spend	O
precisely	O
t	O
steps	O
in	O
state	O
k	O
and	O
then	O
make	O
a	O
transition	O
to	O
a	O
different	O
state	O
is	O
given	O
by	O
pt	O
akk	O
exp	O
t	O
ln	O
akk	O
and	O
so	O
is	O
an	O
exponentially	O
decaying	O
function	O
of	O
t	O
for	O
many	O
applications	O
this	O
will	O
be	O
a	O
very	O
unrealistic	O
model	O
of	O
state	O
duration	O
the	O
problem	O
can	O
be	O
resolved	O
by	O
modelling	O
state	O
duration	O
directly	O
in	O
which	O
the	O
diagonal	B
coefficients	O
akk	O
are	O
all	O
set	O
to	O
zero	O
and	O
each	O
state	O
k	O
is	O
explicitly	O
associated	O
with	O
a	O
probability	B
distribution	O
ptk	O
of	O
possible	O
duration	O
times	O
from	O
a	O
generative	O
point	O
of	O
view	O
when	O
a	O
state	O
k	O
is	O
entered	O
a	O
value	O
t	O
representing	O
the	O
number	O
of	O
time	O
steps	O
that	O
the	O
system	O
will	O
remain	O
in	O
state	O
k	O
is	O
then	O
drawn	O
from	O
ptk	O
the	O
model	O
then	O
emits	O
t	O
values	O
of	O
the	O
observed	B
variable	I
xt	O
which	O
are	O
generally	O
assumed	O
to	O
be	O
independent	B
so	O
that	O
the	O
corresponding	O
pxtk	O
this	O
approach	O
requires	O
some	O
straightforward	O
sion	O
density	B
is	O
simply	O
modifications	O
to	O
the	O
em	B
optimization	O
procedure	O
another	O
limitation	O
of	O
the	O
standard	O
hmm	O
is	O
that	O
it	O
is	O
poor	O
at	O
capturing	O
longrange	O
correlations	O
between	O
the	O
observed	O
variables	O
between	O
variables	O
that	O
are	O
separated	O
by	O
many	O
time	O
steps	O
because	O
these	O
must	O
be	O
mediated	O
via	O
the	O
first-order	O
markov	B
chain	I
of	O
hidden	O
states	O
longer-range	O
effects	O
could	O
in	O
principle	O
be	O
included	O
by	O
adding	O
extra	O
links	O
to	O
the	O
graphical	B
model	I
of	O
figure	O
one	O
way	O
to	O
address	O
this	O
is	O
to	O
generalize	O
the	O
hmm	O
to	O
give	O
the	O
autoregressive	B
hidden	B
markov	B
model	I
et	O
al	O
an	O
example	O
of	O
which	O
is	O
shown	O
in	O
figure	O
for	O
discrete	O
observations	O
this	O
corresponds	O
to	O
expanded	O
tables	O
of	O
conditional	B
probabilities	O
for	O
the	O
emission	O
distributions	O
in	O
the	O
case	O
of	O
a	O
gaussian	B
emission	O
density	B
we	O
can	O
use	O
the	O
lineargaussian	O
framework	O
in	O
which	O
the	O
conditional	B
distribution	O
for	O
xn	O
given	O
the	O
values	O
of	O
the	O
previous	O
observations	O
and	O
the	O
value	O
of	O
zn	O
is	O
a	O
gaussian	B
whose	O
mean	B
is	O
a	O
linear	O
combination	O
of	O
the	O
values	O
of	O
the	O
conditioning	O
variables	O
clearly	O
the	O
number	O
of	O
additional	O
links	O
in	O
the	O
graph	O
must	O
be	O
limited	O
to	O
avoid	O
an	O
excessive	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
example	O
shown	O
in	O
figure	O
each	O
observation	O
depends	O
on	O
figure	O
example	O
of	O
an	O
input-output	B
hidden	B
markov	B
model	I
in	O
this	O
case	O
both	O
the	O
emission	O
probabilities	O
and	O
the	O
transition	O
probabilities	O
depend	O
on	O
the	O
values	O
of	O
a	O
sequence	O
of	O
observations	O
un	O
hidden	O
markov	O
models	O
un	O
un	O
zn	O
zn	O
xn	O
xn	O
the	O
two	O
preceding	O
observed	O
variables	O
as	O
well	O
as	O
on	O
the	O
hidden	O
state	O
although	O
this	O
graph	O
looks	O
messy	O
we	O
can	O
again	O
appeal	O
to	O
d-separation	B
to	O
see	O
that	O
in	O
fact	O
it	O
still	O
has	O
a	O
simple	O
probabilistic	O
structure	O
in	O
particular	O
if	O
we	O
imagine	O
conditioning	O
on	O
zn	O
we	O
see	O
that	O
as	O
with	O
the	O
standard	O
hmm	O
the	O
values	O
of	O
zn	O
and	O
are	O
independent	B
corresponding	O
to	O
the	O
conditional	B
independence	I
property	O
this	O
is	O
easily	O
verified	O
by	O
noting	O
that	O
every	O
path	O
from	O
node	B
zn	O
to	O
node	B
passes	O
through	O
at	O
least	O
one	O
observed	O
node	B
that	O
is	O
head-to-tail	O
with	O
respect	O
to	O
that	O
path	O
as	O
a	O
consequence	O
we	O
can	O
again	O
use	O
a	O
forward-backward	O
recursion	O
in	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
to	O
determine	O
the	O
posterior	O
distributions	O
of	O
the	O
latent	O
variables	O
in	O
a	O
computational	O
time	O
that	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
chain	O
similarly	O
the	O
m	O
step	O
involves	O
only	O
a	O
minor	O
modification	O
of	O
the	O
standard	O
m-step	O
equations	O
in	O
the	O
case	O
of	O
gaussian	B
emission	O
densities	O
this	O
involves	O
estimating	O
the	O
parameters	O
using	O
the	O
standard	O
linear	B
regression	B
equations	O
discussed	O
in	O
chapter	O
we	O
have	O
seen	O
that	O
the	O
autoregressive	B
hmm	O
appears	O
as	O
a	O
natural	O
extension	O
of	O
the	O
standard	O
hmm	O
when	O
viewed	O
as	O
a	O
graphical	B
model	I
in	O
fact	O
the	O
probabilistic	O
graphical	O
modelling	O
viewpoint	O
motivates	O
a	O
plethora	O
of	O
different	O
graphical	O
structures	O
based	O
on	O
the	O
hmm	O
another	O
example	O
is	O
the	O
input-output	B
hidden	B
markov	B
model	I
and	O
frasconi	O
in	O
which	O
we	O
have	O
a	O
sequence	O
of	O
observed	O
variables	O
un	O
in	O
addition	O
to	O
the	O
output	O
variables	O
xn	O
whose	O
values	O
influence	O
either	O
the	O
distribution	O
of	O
latent	O
variables	O
or	O
output	O
variables	O
or	O
both	O
an	O
example	O
is	O
shown	O
in	O
figure	O
this	O
extends	O
the	O
hmm	O
framework	O
to	O
the	O
domain	O
of	O
supervised	B
learning	B
for	O
sequential	B
data	I
it	O
is	O
again	O
easy	O
to	O
show	O
through	O
the	O
use	O
of	O
the	O
d-separation	B
criterion	O
that	O
the	O
markov	O
property	O
for	O
the	O
chain	O
of	O
latent	O
variables	O
still	O
holds	O
to	O
verify	O
this	O
simply	O
note	O
that	O
there	O
is	O
only	O
one	O
path	O
from	O
node	B
zn	O
to	O
node	B
and	O
this	O
is	O
head-to-tail	O
with	O
respect	O
to	O
the	O
observed	O
node	B
zn	O
this	O
conditional	B
independence	I
property	O
again	O
allows	O
the	O
formulation	O
of	O
a	O
computationally	O
efficient	O
learning	B
algorithm	O
in	O
particular	O
we	O
can	O
determine	O
the	O
parameters	O
of	O
the	O
model	O
by	O
maximizing	O
the	O
likelihood	B
function	I
l	O
pxu	O
where	O
u	O
is	O
a	O
matrix	O
whose	O
rows	O
are	O
given	O
by	O
ut	O
n	O
as	O
a	O
consequence	O
of	O
the	O
conditional	B
independence	I
property	O
this	O
likelihood	B
function	I
can	O
be	O
maximized	O
efficiently	O
using	O
an	O
em	B
algorithm	I
in	O
which	O
the	O
e	O
step	O
involves	O
forward	O
and	O
backward	O
recursions	O
another	O
variant	O
of	O
the	O
hmm	O
worthy	O
of	O
mention	O
is	O
the	O
factorial	B
hidden	B
markov	B
model	I
and	O
jordan	O
in	O
which	O
there	O
are	O
multiple	O
independent	B
exercise	O
sequential	B
data	I
figure	O
a	O
factorial	B
hidden	B
markov	B
model	I
comprising	O
two	O
markov	O
chains	O
of	O
latent	O
variables	O
for	O
continuous	O
observed	O
variables	O
x	O
one	O
possible	O
choice	O
of	O
emission	O
model	O
is	O
a	O
linear-gaussian	O
density	B
in	O
which	O
the	O
mean	B
of	O
the	O
gaussian	B
is	O
a	O
linear	O
combination	O
of	O
the	O
states	O
of	O
the	O
corresponding	O
latent	O
variables	O
n	O
n	O
n	O
n	O
xn	O
xn	O
markov	O
chains	O
of	O
latent	O
variables	O
and	O
the	O
distribution	O
of	O
the	O
observed	B
variable	I
at	O
a	O
given	O
time	O
step	O
is	O
conditional	B
on	O
the	O
states	O
of	O
all	O
of	O
the	O
corresponding	O
latent	O
variables	O
at	O
that	O
same	O
time	O
step	O
figure	O
shows	O
the	O
corresponding	O
graphical	B
model	I
the	O
motivation	O
for	O
considering	O
factorial	B
hmm	O
can	O
be	O
seen	O
by	O
noting	O
that	O
in	O
order	O
to	O
represent	O
say	O
bits	B
of	O
information	O
at	O
a	O
given	O
time	O
step	O
a	O
standard	O
hmm	O
would	O
need	O
k	O
latent	O
states	O
whereas	O
a	O
factorial	B
hmm	O
could	O
make	O
use	O
of	O
binary	O
latent	O
chains	O
the	O
primary	O
disadvantage	O
of	O
factorial	B
hmms	O
however	O
lies	O
in	O
the	O
additional	O
complexity	O
of	O
training	B
them	O
the	O
m	O
step	O
for	O
the	O
factorial	B
hmm	O
model	O
is	O
straightforward	O
however	O
observation	O
of	O
the	O
x	O
variables	O
introduces	O
dependencies	O
between	O
the	O
latent	O
chains	O
leading	O
to	O
difficulties	O
with	O
the	O
e	O
step	O
this	O
can	O
be	O
seen	O
by	O
noting	O
that	O
in	O
figure	O
the	O
variables	O
n	O
are	O
connected	O
by	O
a	O
path	O
which	O
is	O
head-to-head	O
at	O
node	B
xn	O
and	O
hence	O
they	O
are	O
not	O
d-separated	O
the	O
exact	O
e	O
step	O
for	O
this	O
model	O
does	O
not	O
correspond	O
to	O
running	O
forward	O
and	O
backward	O
recursions	O
along	O
the	O
m	O
markov	O
chains	O
independently	O
this	O
is	O
confirmed	O
by	O
noting	O
that	O
the	O
key	O
conditional	B
independence	I
property	O
is	O
not	O
satisfied	O
for	O
the	O
individual	O
markov	O
chains	O
in	O
the	O
factorial	B
hmm	O
model	O
as	O
is	O
shown	O
using	O
d-separation	B
in	O
figure	O
now	O
suppose	O
that	O
there	O
are	O
m	O
chains	O
of	O
hidden	O
nodes	O
and	O
for	O
simplicity	O
suppose	O
that	O
all	O
latent	O
variables	O
have	O
the	O
same	O
number	O
k	O
of	O
states	O
then	O
one	O
approach	O
would	O
be	O
to	O
note	O
that	O
there	O
are	O
km	O
combinations	O
of	O
latent	O
variables	O
at	O
a	O
given	O
time	O
step	O
n	O
and	O
figure	O
example	O
of	O
a	O
path	O
highlighted	O
in	O
green	O
which	O
is	O
head-to-head	O
at	O
the	O
observed	O
nodes	O
xn	O
and	O
and	O
head-to-tail	O
at	O
the	O
unobserved	O
nodes	O
n	O
and	O
thus	O
the	O
path	O
is	O
not	O
blocked	O
and	O
so	O
the	O
conditional	B
independence	I
property	O
does	O
not	O
hold	O
for	O
the	O
individual	O
latent	O
chains	O
of	O
the	O
factorial	B
hmm	O
model	O
as	O
a	O
consequence	O
there	O
is	O
no	O
efficient	O
exact	O
e	O
step	O
for	O
this	O
model	O
n	O
n	O
n	O
n	O
n	O
xn	O
xn	O
section	O
linear	O
dynamical	O
systems	O
and	O
so	O
we	O
can	O
transform	O
the	O
model	O
into	O
an	O
equivalent	O
standard	O
hmm	O
having	O
a	O
single	O
chain	O
of	O
latent	O
variables	O
each	O
of	O
which	O
has	O
km	O
latent	O
states	O
we	O
can	O
then	O
run	O
the	O
standard	O
forward-backward	O
recursions	O
in	O
the	O
e	O
step	O
this	O
has	O
computational	O
complexity	O
on	O
k	O
that	O
is	O
exponential	O
in	O
the	O
number	O
m	O
of	O
latent	O
chains	O
and	O
so	O
will	O
be	O
intractable	O
for	O
anything	O
other	O
than	O
small	O
values	O
of	O
m	O
one	O
solution	O
would	O
be	O
to	O
use	O
sampling	B
methods	I
in	O
chapter	O
as	O
an	O
elegant	O
deterministic	O
alternative	O
ghahramani	O
and	O
jordan	O
exploited	O
variational	B
inference	B
techniques	O
to	O
obtain	O
a	O
tractable	O
algorithm	O
for	O
approximate	O
inference	B
this	O
can	O
be	O
done	O
using	O
a	O
simple	O
variational	B
posterior	O
distribution	O
that	O
is	O
fully	O
factorized	O
with	O
respect	O
to	O
the	O
latent	O
variables	O
or	O
alternatively	O
by	O
using	O
a	O
more	O
powerful	O
approach	O
in	O
which	O
the	O
variational	B
distribution	O
is	O
described	O
by	O
independent	B
markov	O
chains	O
corresponding	O
to	O
the	O
chains	O
of	O
latent	O
variables	O
in	O
the	O
original	O
model	O
in	O
the	O
latter	O
case	O
the	O
variational	B
inference	B
algorithms	O
involves	O
running	O
independent	B
forward	O
and	O
backward	O
recursions	O
along	O
each	O
chain	O
which	O
is	O
computationally	O
efficient	O
and	O
yet	O
is	O
also	O
able	O
to	O
capture	O
correlations	O
between	O
variables	O
within	O
the	O
same	O
chain	O
clearly	O
there	O
are	O
many	O
possible	O
probabilistic	O
structures	O
that	O
can	O
be	O
constructed	O
according	O
to	O
the	O
needs	O
of	O
particular	O
applications	O
graphical	O
models	O
provide	O
a	O
general	O
technique	O
for	O
motivating	O
describing	O
and	O
analysing	O
such	O
structures	O
and	O
variational	B
methods	O
provide	O
a	O
powerful	O
framework	O
for	O
performing	O
inference	B
in	O
those	O
models	O
for	O
which	O
exact	O
solution	O
is	O
intractable	O
linear	O
dynamical	O
systems	O
in	O
order	O
to	O
motivate	O
the	O
concept	O
of	O
linear	O
dynamical	O
systems	O
let	O
us	O
consider	O
the	O
following	O
simple	O
problem	O
which	O
often	O
arises	O
in	O
practical	O
settings	O
suppose	O
we	O
wish	O
to	O
measure	O
the	O
value	O
of	O
an	O
unknown	O
quantity	O
z	O
using	O
a	O
noisy	O
sensor	O
that	O
returns	O
a	O
observation	O
x	O
representing	O
the	O
value	O
of	O
z	O
plus	O
zero-mean	O
gaussian	B
noise	O
given	O
a	O
single	O
measurement	O
our	O
best	O
guess	O
for	O
z	O
is	O
to	O
assume	O
that	O
z	O
x	O
however	O
we	O
can	O
improve	O
our	O
estimate	O
for	O
z	O
by	O
taking	O
lots	O
of	O
measurements	O
and	O
averaging	O
them	O
because	O
the	O
random	O
noise	O
terms	O
will	O
tend	O
to	O
cancel	O
each	O
other	O
now	O
let	O
s	O
make	O
the	O
situation	O
more	O
complicated	O
by	O
assuming	O
that	O
we	O
wish	O
to	O
measure	O
a	O
quantity	O
z	O
that	O
is	O
changing	O
over	O
time	O
we	O
can	O
take	O
regular	O
measurements	O
of	O
x	O
so	O
that	O
at	O
some	O
point	O
in	O
time	O
we	O
have	O
obtained	O
xn	O
and	O
we	O
wish	O
to	O
find	O
the	O
corresponding	O
values	O
xn	O
if	O
we	O
simply	O
average	O
the	O
measurements	O
the	O
error	B
due	O
to	O
random	O
noise	O
will	O
be	O
reduced	O
but	O
unfortunately	O
we	O
will	O
just	O
obtain	O
a	O
single	O
averaged	O
estimate	O
in	O
which	O
we	O
have	O
averaged	O
over	O
the	O
changing	O
value	O
of	O
z	O
thereby	O
introducing	O
a	O
new	O
source	O
of	O
error	B
intuitively	O
we	O
could	O
imagine	O
doing	O
a	O
bit	O
better	O
as	O
follows	O
to	O
estimate	O
the	O
value	O
of	O
zn	O
we	O
take	O
only	O
the	O
most	O
recent	O
few	O
measurements	O
say	O
xn	O
l	O
xn	O
and	O
just	O
average	O
these	O
if	O
z	O
is	O
changing	O
slowly	O
and	O
the	O
random	O
noise	O
level	O
in	O
the	O
sensor	O
is	O
high	O
it	O
would	O
make	O
sense	O
to	O
choose	O
a	O
relatively	O
long	O
window	O
of	O
observations	O
to	O
average	O
conversely	O
if	O
the	O
signal	O
is	O
changing	O
quickly	O
and	O
the	O
noise	O
levels	O
are	O
small	O
we	O
might	O
be	O
better	O
just	O
to	O
use	O
xn	O
directly	O
as	O
our	O
estimate	O
of	O
zn	O
perhaps	O
we	O
could	O
do	O
even	O
better	O
if	O
we	O
take	O
a	O
weighted	O
average	O
in	O
which	O
more	O
recent	O
measurements	O
sequential	B
data	I
make	O
a	O
greater	O
contribution	O
than	O
less	O
recent	O
ones	O
although	O
this	O
sort	O
of	O
intuitive	O
argument	O
seems	O
plausible	O
it	O
does	O
not	O
tell	O
us	O
how	O
to	O
form	O
a	O
weighted	O
average	O
and	O
any	O
sort	O
of	O
hand-crafted	O
weighing	O
is	O
hardly	O
likely	O
to	O
be	O
optimal	O
fortunately	O
we	O
can	O
address	O
problems	O
such	O
as	O
this	O
much	O
more	O
systematically	O
by	O
defining	O
a	O
probabilistic	O
model	O
that	O
captures	O
the	O
time	O
evolution	O
and	O
measurement	O
processes	O
and	O
then	O
applying	O
the	O
inference	B
and	O
learning	B
methods	O
developed	O
in	O
earlier	O
chapters	O
here	O
we	O
shall	O
focus	O
on	O
a	O
widely	O
used	O
model	O
known	O
as	O
a	O
linear	B
dynamical	B
system	I
as	O
we	O
have	O
seen	O
the	O
hmm	O
corresponds	O
to	O
the	O
state	B
space	I
model	I
shown	O
in	O
figure	O
in	O
which	O
the	O
latent	O
variables	O
are	O
discrete	O
but	O
with	O
arbitrary	O
emission	B
probability	B
distributions	O
this	O
graph	O
of	O
course	O
describes	O
a	O
much	O
broader	O
class	O
of	O
probability	B
distributions	O
all	O
of	O
which	O
factorize	O
according	O
to	O
we	O
now	O
consider	O
extensions	O
to	O
other	O
distributions	O
for	O
the	O
latent	O
variables	O
in	O
particular	O
we	O
consider	O
continuous	O
latent	O
variables	O
in	O
which	O
the	O
summations	O
of	O
the	O
sum-product	B
algorithm	I
become	O
integrals	O
the	O
general	O
form	O
of	O
the	O
inference	B
algorithms	O
will	O
however	O
be	O
the	O
same	O
as	O
for	O
the	O
hidden	B
markov	B
model	I
it	O
is	O
interesting	O
to	O
note	O
that	O
historically	O
hidden	O
markov	O
models	O
and	O
linear	O
dynamical	O
systems	O
were	O
developed	O
independently	O
once	O
they	O
are	O
both	O
expressed	O
as	O
graphical	O
models	O
however	O
the	O
deep	O
relationship	O
between	O
them	O
immediately	O
becomes	O
apparent	O
one	O
key	O
requirement	O
is	O
that	O
we	O
retain	O
an	O
efficient	O
algorithm	O
for	O
inference	B
which	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
chain	O
this	O
requires	O
that	O
for	O
instance	O
when	O
we	O
take	O
xn	O
and	O
multiply	O
by	O
the	O
transition	B
probability	B
pznzn	O
and	O
the	O
emission	B
probability	B
pxnzn	O
and	O
then	O
marginalize	O
over	O
zn	O
we	O
obtain	O
a	O
distribution	O
over	O
a	O
representing	O
the	O
posterior	B
probability	B
of	O
zn	O
given	O
observations	O
zn	O
that	O
is	O
of	O
the	O
same	O
functional	B
form	O
as	O
that	O
over	O
that	O
is	O
to	O
say	O
the	O
distribution	O
must	O
not	O
become	O
more	O
complex	O
at	O
each	O
stage	O
but	O
must	O
only	O
change	O
in	O
its	O
parameter	O
values	O
not	O
surprisingly	O
the	O
only	O
distributions	O
that	O
have	O
this	O
property	O
of	O
being	O
closed	O
under	O
multiplication	O
are	O
those	O
belonging	O
to	O
the	O
exponential	B
family	I
here	O
we	O
consider	O
the	O
most	O
important	O
example	O
from	O
a	O
practical	O
perspective	O
which	O
is	O
the	O
gaussian	B
in	O
particular	O
we	O
consider	O
a	O
linear-gaussian	O
state	B
space	I
model	I
so	O
that	O
the	O
latent	O
variables	O
as	O
well	O
as	O
the	O
observed	O
variables	O
are	O
multivariate	O
gaussian	B
distributions	O
whose	O
means	O
are	O
linear	O
functions	O
of	O
the	O
states	O
of	O
their	O
parents	O
in	O
the	O
graph	O
we	O
have	O
seen	O
that	O
a	O
directed	B
graph	O
of	O
linear-gaussian	O
units	O
is	O
equivalent	O
to	O
a	O
joint	O
gaussian	B
distribution	O
over	O
all	O
of	O
the	O
variables	O
furthermore	O
marginals	O
such	O
are	O
also	O
gaussian	B
so	O
that	O
the	O
functional	B
form	O
of	O
the	O
meseach	O
of	O
which	O
has	O
a	O
mean	B
that	O
is	O
linear	O
in	O
zn	O
then	O
even	O
is	O
gaussian	B
the	O
will	O
be	O
a	O
mixture	B
of	O
k	O
gaussians	O
will	O
be	O
a	O
mixture	B
of	O
k	O
sages	O
is	O
preserved	O
and	O
we	O
will	O
obtain	O
an	O
efficient	O
inference	B
algorithm	O
by	O
contrast	O
suppose	O
that	O
the	O
emission	O
densities	O
pxnzn	O
comprise	O
a	O
mixture	B
of	O
k	O
gaussians	O
gaussians	O
and	O
so	O
on	O
and	O
exact	O
inference	B
will	O
not	O
be	O
of	O
practical	O
value	O
we	O
have	O
seen	O
that	O
the	O
hidden	B
markov	B
model	I
can	O
be	O
viewed	O
as	O
an	O
extension	O
of	O
the	O
mixture	B
models	O
of	O
chapter	O
to	O
allow	O
for	O
sequential	O
correlations	O
in	O
the	O
data	O
in	O
a	O
similar	O
way	O
we	O
can	O
view	O
the	O
linear	B
dynamical	B
system	I
as	O
a	O
generalization	B
of	O
the	O
continuous	O
latent	B
variable	I
models	O
of	O
chapter	O
such	O
as	O
probabilistic	B
pca	I
and	O
factor	B
analysis	I
each	O
pair	O
of	O
nodes	O
xn	O
represents	O
a	O
linear-gaussian	O
latent	B
variable	I
linear	O
dynamical	O
systems	O
model	O
for	O
that	O
particular	O
observation	O
however	O
the	O
latent	O
variables	O
are	O
no	O
longer	O
treated	O
as	O
independent	B
but	O
now	O
form	O
a	O
markov	B
chain	I
because	O
the	O
model	O
is	O
represented	O
by	O
a	O
tree-structured	O
directed	B
graph	O
inference	B
problems	O
can	O
be	O
solved	O
efficiently	O
using	O
the	O
sum-product	B
algorithm	I
the	O
forward	O
recursions	O
analogous	O
to	O
the	O
messages	O
of	O
the	O
hidden	B
markov	B
model	I
are	O
known	O
as	O
the	O
kalman	B
filter	I
equations	O
zarchan	O
and	O
musoff	O
and	O
the	O
backward	O
recursions	O
analogous	O
to	O
the	O
messages	O
are	O
known	O
as	O
the	O
kalman	B
smoother	I
equations	O
or	O
the	O
rauch-tung-striebel	B
equations	I
et	O
al	O
the	O
kalman	B
filter	I
is	O
widely	O
used	O
in	O
many	O
real-time	O
tracking	O
applications	O
because	O
the	O
linear	B
dynamical	B
system	I
is	O
a	O
linear-gaussian	B
model	I
the	O
joint	O
distribution	O
over	O
all	O
variables	O
as	O
well	O
as	O
all	O
marginals	O
and	O
conditionals	O
will	O
be	O
gaussian	B
it	O
follows	O
that	O
the	O
sequence	O
of	O
individually	O
most	O
probable	O
latent	B
variable	I
values	O
is	O
the	O
same	O
as	O
the	O
most	O
probable	O
latent	O
sequence	O
there	O
is	O
thus	O
no	O
need	O
to	O
consider	O
the	O
analogue	O
of	O
the	O
viterbi	B
algorithm	I
for	O
the	O
linear	B
dynamical	B
system	I
because	O
the	O
model	O
has	O
linear-gaussian	O
conditional	B
distributions	O
we	O
can	O
write	O
the	O
transition	O
and	O
emission	O
distributions	O
in	O
the	O
general	O
form	O
pznzn	O
n	O
pxnzn	O
n	O
the	O
initial	O
latent	B
variable	I
also	O
has	O
a	O
gaussian	B
distribution	O
which	O
we	O
write	O
as	O
n	O
exercise	O
exercise	O
note	O
that	O
in	O
order	O
to	O
simplify	O
the	O
notation	O
we	O
have	O
omitted	O
additive	O
constant	O
terms	O
from	O
the	O
means	O
of	O
the	O
gaussians	O
in	O
fact	O
it	O
is	O
straightforward	O
to	O
include	O
them	O
if	O
desired	O
traditionally	O
these	O
distributions	O
are	O
more	O
commonly	O
expressed	O
in	O
an	O
equivalent	O
form	O
in	O
terms	O
of	O
noisy	O
linear	O
equations	O
given	O
by	O
zn	O
azn	O
wn	O
xn	O
czn	O
vn	O
u	O
where	O
the	O
noise	O
terms	O
have	O
the	O
distributions	O
w	O
n	O
v	O
n	O
u	O
n	O
the	O
parameters	O
of	O
the	O
model	O
denoted	O
by	O
c	O
can	O
be	O
determined	O
using	O
maximum	B
likelihood	I
through	O
the	O
em	B
algorithm	I
in	O
the	O
e	O
step	O
we	O
need	O
to	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
local	B
posterior	O
marginals	O
for	O
the	O
latent	O
variables	O
which	O
can	O
be	O
solved	O
efficiently	O
using	O
the	O
sum-product	B
algorithm	I
as	O
we	O
discuss	O
in	O
the	O
next	O
section	O
sequential	B
data	I
inference	B
in	O
lds	O
we	O
now	O
turn	O
to	O
the	O
problem	O
of	O
finding	O
the	O
marginal	B
distributions	O
for	O
the	O
latent	O
variables	O
conditional	B
on	O
the	O
observation	O
sequence	O
for	O
given	O
parameter	O
settings	O
we	O
also	O
wish	O
to	O
make	O
predictions	O
of	O
the	O
next	O
latent	O
state	O
zn	O
and	O
of	O
the	O
next	O
observation	O
xn	O
conditioned	O
on	O
the	O
observed	O
data	O
xn	O
for	O
use	O
in	O
real-time	O
applications	O
these	O
inference	B
problems	O
can	O
be	O
solved	O
efficiently	O
using	O
the	O
sum-product	B
algorithm	I
which	O
in	O
the	O
context	O
of	O
the	O
linear	B
dynamical	B
system	I
gives	O
rise	O
to	O
the	O
kalman	B
filter	I
and	O
kalman	B
smoother	I
equations	O
it	O
is	O
worth	O
emphasizing	O
that	O
because	O
the	O
linear	B
dynamical	B
system	I
is	O
a	O
lineargaussian	O
model	O
the	O
joint	O
distribution	O
over	O
all	O
latent	O
and	O
observed	O
variables	O
is	O
simply	O
a	O
gaussian	B
and	O
so	O
in	O
principle	O
we	O
could	O
solve	O
inference	B
problems	O
by	O
using	O
the	O
standard	O
results	O
derived	O
in	O
previous	O
chapters	O
for	O
the	O
marginals	O
and	O
conditionals	O
of	O
a	O
multivariate	O
gaussian	B
the	O
role	O
of	O
the	O
sum-product	B
algorithm	I
is	O
to	O
provide	O
a	O
more	O
efficient	O
way	O
to	O
perform	O
such	O
computations	O
linear	O
dynamical	O
systems	O
have	O
the	O
identical	O
factorization	B
given	O
by	O
to	O
hidden	O
markov	O
models	O
and	O
are	O
again	O
described	O
by	O
the	O
factor	O
graphs	O
in	O
figures	O
and	O
inference	B
algorithms	O
therefore	O
take	O
precisely	O
the	O
same	O
form	O
except	O
that	O
summations	O
over	O
latent	O
variables	O
are	O
replaced	O
by	O
integrations	O
we	O
begin	O
by	O
considering	O
the	O
forward	O
equations	O
in	O
which	O
we	O
treat	O
zn	O
as	O
the	O
root	B
node	B
and	O
propagate	O
messages	O
from	O
the	O
leaf	O
node	B
to	O
the	O
root	O
from	O
the	O
initial	O
message	O
will	O
be	O
gaussian	B
and	O
because	O
each	O
of	O
the	O
factors	O
is	O
gaussian	B
all	O
subsequent	O
messages	O
will	O
also	O
be	O
gaussian	B
by	O
convention	O
we	O
shall	O
propagate	O
messages	O
that	O
are	O
normalized	O
marginal	B
distributions	O
corresponding	O
to	O
xn	O
which	O
we	O
denote	O
by	O
this	O
is	O
precisely	O
analogous	O
to	O
the	O
propagation	O
of	O
scaled	O
given	O
by	O
n	O
n	O
vn	O
in	O
the	O
discrete	O
case	O
of	O
the	O
hidden	B
markov	B
model	I
and	O
so	O
the	O
recursion	O
equation	O
now	O
takes	O
the	O
form	O
pxnzn	O
dzn	O
substituting	O
for	O
the	O
conditionals	O
pznzn	O
and	O
pxnzn	O
using	O
and	O
respectively	O
and	O
making	O
use	O
of	O
we	O
see	O
that	O
becomes	O
cnn	O
n	O
vn	O
n	O
n	O
n	O
vn	O
dzn	O
here	O
we	O
are	O
supposing	O
that	O
n	O
and	O
vn	O
are	O
known	O
and	O
by	O
evaluating	O
the	O
integral	O
in	O
we	O
wish	O
to	O
determine	O
values	O
for	O
n	O
and	O
vn	O
the	O
integral	O
is	O
easily	O
evaluated	O
by	O
making	O
use	O
of	O
the	O
result	O
from	O
which	O
it	O
follows	O
that	O
n	O
n	O
vn	O
dzn	O
n	O
n	O
pn	O
linear	O
dynamical	O
systems	O
where	O
we	O
have	O
defined	O
pn	O
avn	O
we	O
can	O
now	O
combine	O
this	O
result	O
with	O
the	O
first	O
factor	O
on	O
the	O
right-hand	O
side	O
of	O
by	O
making	O
use	O
of	O
and	O
to	O
give	O
n	O
a	O
n	O
knxn	O
ca	O
n	O
vn	O
kncpn	O
cn	O
n	O
n	O
cpn	O
here	O
we	O
have	O
made	O
use	O
of	O
the	O
matrix	O
inverse	B
identities	O
and	O
and	O
also	O
defined	O
the	O
kalman	B
gain	I
matrix	I
kn	O
pn	O
cpn	O
thus	O
given	O
the	O
values	O
of	O
n	O
and	O
vn	O
together	O
with	O
the	O
new	O
observation	O
xn	O
we	O
can	O
evaluate	O
the	O
gaussian	B
marginal	B
for	O
zn	O
having	O
mean	B
n	O
and	O
covariance	B
vn	O
as	O
well	O
as	O
the	O
normalization	O
coefficient	O
cn	O
the	O
initial	O
conditions	O
for	O
these	O
recursion	O
equations	O
are	O
obtained	O
from	O
because	O
is	O
given	O
by	O
and	O
is	O
given	O
by	O
we	O
can	O
again	O
make	O
use	O
of	O
to	O
calculate	O
and	O
to	O
calculate	O
and	O
giving	O
c	O
n	O
where	O
similarly	O
the	O
likelihood	B
function	I
for	O
the	O
linear	B
dynamical	B
system	I
is	O
given	O
by	O
in	O
which	O
the	O
factors	O
cn	O
are	O
found	O
using	O
the	O
kalman	O
filtering	O
equations	O
we	O
can	O
interpret	O
the	O
steps	O
involved	O
in	O
going	O
from	O
the	O
posterior	O
marginal	B
over	O
zn	O
to	O
the	O
posterior	O
marginal	B
over	O
zn	O
as	O
follows	O
in	O
we	O
can	O
view	O
the	O
quantity	O
a	O
n	O
as	O
the	O
prediction	O
of	O
the	O
mean	B
over	O
zn	O
obtained	O
by	O
simply	O
taking	O
the	O
mean	B
over	O
zn	O
and	O
projecting	O
it	O
forward	O
one	O
step	O
using	O
the	O
transition	B
probability	B
matrix	O
a	O
this	O
predicted	O
mean	B
would	O
give	O
a	O
predicted	O
observation	O
for	O
xn	O
given	O
by	O
cazn	O
obtained	O
by	O
applying	O
the	O
emission	B
probability	B
matrix	O
c	O
to	O
the	O
predicted	O
hidden	O
state	O
mean	B
we	O
can	O
view	O
the	O
update	O
equation	O
for	O
the	O
mean	B
of	O
the	O
hidden	B
variable	I
distribution	O
as	O
taking	O
the	O
predicted	O
mean	B
a	O
n	O
and	O
then	O
adding	O
a	O
correction	O
that	O
is	O
proportional	O
to	O
the	O
error	B
xn	O
cazn	O
between	O
the	O
predicted	O
observation	O
and	O
the	O
actual	O
observation	O
the	O
coefficient	O
of	O
this	O
correction	O
is	O
given	O
by	O
the	O
kalman	B
gain	I
matrix	I
thus	O
we	O
can	O
view	O
the	O
kalman	B
filter	I
as	O
a	O
process	O
of	O
making	O
successive	O
predictions	O
and	O
then	O
correcting	O
these	O
predictions	O
in	O
the	O
light	O
of	O
the	O
new	O
observations	O
this	O
is	O
illustrated	O
graphically	O
in	O
figure	O
sequential	B
data	I
zn	O
zn	O
zn	O
figure	O
the	O
linear	B
dynamical	B
system	I
can	O
be	O
viewed	O
as	O
a	O
sequence	O
of	O
steps	O
in	O
which	O
increasing	O
uncertainty	O
in	O
the	O
state	O
variable	O
due	O
to	O
diffusion	O
is	O
compensated	O
by	O
the	O
arrival	O
of	O
new	O
data	O
in	O
the	O
left-hand	O
plot	O
the	O
blue	O
curve	O
shows	O
the	O
distribution	O
pzn	O
xn	O
which	O
incorporates	O
all	O
the	O
data	O
up	O
to	O
step	O
n	O
the	O
diffusion	O
arising	O
from	O
the	O
nonzero	O
variance	B
of	O
the	O
transition	B
probability	B
pznzn	O
gives	O
the	O
distribution	O
xn	O
shown	O
in	O
red	O
in	O
the	O
centre	O
plot	O
note	O
that	O
this	O
is	O
broader	O
and	O
shifted	O
relative	B
to	O
the	O
blue	O
curve	O
is	O
shown	O
dashed	O
in	O
the	O
centre	O
plot	O
for	O
comparison	O
the	O
next	O
data	O
observation	O
xn	O
contributes	O
through	O
the	O
emission	O
density	B
pxnzn	O
which	O
is	O
shown	O
as	O
a	O
function	O
of	O
zn	O
in	O
green	O
on	O
the	O
right-hand	O
plot	O
note	O
that	O
this	O
is	O
not	O
a	O
density	B
with	O
respect	O
to	O
zn	O
and	O
so	O
is	O
not	O
normalized	O
to	O
one	O
inclusion	O
of	O
this	O
new	O
data	O
point	O
leads	O
to	O
a	O
revised	O
distribution	O
xn	O
for	O
the	O
state	O
density	B
shown	O
in	O
blue	O
we	O
see	O
that	O
observation	O
of	O
the	O
data	O
has	O
shifted	O
and	O
narrowed	O
the	O
distribution	O
compared	O
to	O
xn	O
is	O
shown	O
in	O
dashed	O
in	O
the	O
right-hand	O
plot	O
for	O
comparison	O
exercise	O
exercise	O
if	O
we	O
consider	O
a	O
situation	O
in	O
which	O
the	O
measurement	O
noise	O
is	O
small	O
compared	O
to	O
the	O
rate	O
at	O
which	O
the	O
latent	B
variable	I
is	O
evolving	O
then	O
we	O
find	O
that	O
the	O
posterior	O
distribution	O
for	O
zn	O
depends	O
only	O
on	O
the	O
current	O
measurement	O
xn	O
in	O
accordance	O
with	O
the	O
intuition	O
from	O
our	O
simple	O
example	O
at	O
the	O
start	O
of	O
the	O
section	O
similarly	O
if	O
the	O
latent	B
variable	I
is	O
evolving	O
slowly	O
relative	B
to	O
the	O
observation	O
noise	O
level	O
we	O
find	O
that	O
the	O
posterior	O
mean	B
for	O
zn	O
is	O
obtained	O
by	O
averaging	O
all	O
of	O
the	O
measurements	O
obtained	O
up	O
to	O
that	O
time	O
one	O
of	O
the	O
most	O
important	O
applications	O
of	O
the	O
kalman	B
filter	I
is	O
to	O
tracking	O
and	O
this	O
is	O
illustrated	O
using	O
a	O
simple	O
example	O
of	O
an	O
object	O
moving	O
in	O
two	O
dimensions	O
in	O
figure	O
so	O
far	O
we	O
have	O
solved	O
the	O
inference	B
problem	O
of	O
finding	O
the	O
posterior	O
marginal	B
for	O
a	O
node	B
zn	O
given	O
observations	O
from	O
up	O
to	O
xn	O
next	O
we	O
turn	O
to	O
the	O
problem	O
of	O
finding	O
the	O
marginal	B
for	O
a	O
node	B
zn	O
given	O
all	O
observations	O
to	O
xn	O
for	O
temporal	O
data	O
this	O
corresponds	O
to	O
the	O
inclusion	O
of	O
future	O
as	O
well	O
as	O
past	O
observations	O
although	O
this	O
cannot	O
be	O
used	O
for	O
real-time	O
prediction	O
it	O
plays	O
a	O
key	O
role	O
in	O
learning	B
the	O
parameters	O
of	O
the	O
model	O
by	O
analogy	O
with	O
the	O
hidden	B
markov	B
model	I
this	O
problem	O
can	O
be	O
solved	O
by	O
propagating	O
messages	O
from	O
node	B
xn	O
back	O
to	O
node	B
and	O
combining	O
this	O
information	O
with	O
that	O
obtained	O
during	O
the	O
forward	O
message	B
passing	I
stage	O
used	O
to	O
compute	O
of	O
rather	O
than	O
in	O
terms	O
because	O
must	O
also	O
be	O
in	O
the	O
lds	O
literature	O
it	O
is	O
usual	O
to	O
formulate	O
this	O
backward	O
recursion	O
in	O
terms	O
gaussian	B
we	O
write	O
it	O
in	O
the	O
form	O
n	O
to	O
derive	O
the	O
required	O
recursion	O
we	O
start	O
from	O
the	O
backward	O
recursion	O
for	O
linear	O
dynamical	O
systems	O
figure	O
an	O
illustration	O
of	O
a	O
linear	B
dynamical	B
system	I
being	O
used	O
to	O
track	O
a	O
moving	O
object	O
the	O
blue	O
points	O
indicate	O
the	O
true	O
positions	O
of	O
the	O
object	O
in	O
a	O
two-dimensional	O
space	O
at	O
successive	O
time	O
steps	O
the	O
green	O
points	O
denote	O
noisy	O
measurements	O
of	O
the	O
positions	O
and	O
the	O
red	O
crosses	O
indicate	O
the	O
means	O
of	O
the	O
inferred	O
posterior	O
distributions	O
of	O
the	O
positions	O
obtained	O
by	O
running	O
the	O
kalman	O
filtering	O
equations	O
the	O
covarithe	O
inferred	O
positions	O
ances	O
of	O
are	O
indicated	O
by	O
the	O
red	O
ellipses	O
which	O
correspond	O
to	O
contours	O
having	O
one	O
standard	B
deviation	I
which	O
for	O
continuous	O
latent	O
variables	O
can	O
be	O
written	O
in	O
the	O
form	O
we	O
now	O
multiply	O
both	O
sides	O
of	O
and	O
substitute	O
for	O
n	O
n	O
jn	O
vn	O
jn	O
and	O
using	O
and	O
then	O
we	O
make	O
use	O
of	O
and	O
together	O
with	O
and	O
after	O
some	O
manipulation	O
we	O
obtain	O
a	O
n	O
pn	O
jt	O
n	O
where	O
we	O
have	O
defined	O
exercise	O
exercise	O
and	O
we	O
have	O
made	O
use	O
of	O
avn	O
pnjt	O
n	O
note	O
that	O
these	O
recursions	O
require	O
that	O
the	O
forward	O
pass	O
be	O
completed	O
first	O
so	O
that	O
the	O
quantities	O
n	O
and	O
vn	O
will	O
be	O
available	O
for	O
the	O
backward	O
pass	O
jn	O
vnat	O
zn	O
can	O
be	O
obtained	O
from	O
in	O
the	O
form	O
for	O
the	O
em	B
algorithm	I
we	O
also	O
require	O
the	O
pairwise	O
posterior	O
marginals	O
which	O
n	O
n	O
vn	O
substituting	O
using	O
and	O
rearranging	O
we	O
see	O
that	O
zn	O
is	O
a	O
gaussian	B
with	O
mean	B
given	O
with	O
components	O
and	O
and	O
a	O
covariance	B
between	O
zn	O
and	O
zn	O
given	O
by	O
covzn	O
zn	O
jn	O
sequential	B
data	I
learning	B
in	O
lds	O
so	O
far	O
we	O
have	O
considered	O
the	O
inference	B
problem	O
for	O
linear	O
dynamical	O
systems	O
assuming	O
that	O
the	O
model	O
parameters	O
c	O
are	O
known	O
next	O
we	O
consider	O
the	O
determination	O
of	O
these	O
parameters	O
using	O
maximum	B
likelihood	I
and	O
hinton	O
because	O
the	O
model	O
has	O
latent	O
variables	O
this	O
can	O
be	O
addressed	O
using	O
the	O
em	B
algorithm	I
which	O
was	O
discussed	O
in	O
general	O
terms	O
in	O
chapter	O
we	O
can	O
derive	O
the	O
em	B
algorithm	I
for	O
the	O
linear	B
dynamical	B
system	I
as	O
follows	O
let	O
us	O
denote	O
the	O
estimated	O
parameter	O
values	O
at	O
some	O
particular	O
cycle	O
of	O
the	O
algorithm	O
by	O
old	O
for	O
these	O
parameter	O
values	O
we	O
can	O
run	O
the	O
inference	B
algorithm	O
to	O
determine	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
pzx	O
old	O
or	O
more	O
precisely	O
those	O
in	O
particular	O
we	O
shall	O
local	B
posterior	O
marginals	O
that	O
are	O
required	O
in	O
the	O
m	O
step	O
require	O
the	O
following	O
expectations	O
e	O
e	O
n	O
t	O
t	O
jn	O
n	O
znzt	O
n	O
znzt	O
n	O
e	O
n	O
where	O
we	O
have	O
used	O
now	O
we	O
consider	O
the	O
complete-data	O
log	O
likelihood	B
function	I
which	O
is	O
obtained	O
by	O
taking	O
the	O
logarithm	O
of	O
and	O
is	O
therefore	O
given	O
by	O
ln	O
px	O
z	O
ln	O
ln	O
pznzn	O
a	O
ln	O
pxnzn	O
c	O
in	O
which	O
we	O
have	O
made	O
the	O
dependence	O
on	O
the	O
parameters	O
explicit	O
we	O
now	O
take	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
pzx	O
old	O
which	O
defines	O
the	O
function	O
q	O
old	O
ez	O
old	O
px	O
z	O
in	O
the	O
m	O
step	O
this	O
function	O
is	O
maximized	O
with	O
respect	O
to	O
the	O
components	O
of	O
consider	O
first	O
the	O
parameters	O
and	O
if	O
we	O
substitute	O
for	O
in	O
using	O
and	O
then	O
take	O
the	O
expectation	B
with	O
respect	O
to	O
z	O
we	O
obtain	O
const	O
q	O
old	O
ez	O
old	O
where	O
all	O
terms	O
not	O
dependent	O
on	O
or	O
have	O
been	O
absorbed	O
into	O
the	O
additive	O
constant	O
maximization	O
with	O
respect	O
to	O
and	O
is	O
easily	O
performed	O
by	O
making	O
use	O
of	O
the	O
maximum	B
likelihood	I
solution	O
for	O
a	O
gaussian	B
distribution	O
discussed	O
in	O
section	O
giving	O
exercise	O
linear	O
dynamical	O
systems	O
similarly	O
to	O
optimize	O
a	O
and	O
we	O
substitute	O
for	O
pznzn	O
a	O
in	O
new	O
vnew	O
using	O
giving	O
q	O
old	O
n	O
ez	O
old	O
ln	O
azn	O
azn	O
const	O
in	O
which	O
the	O
constant	O
comprises	O
terms	O
that	O
are	O
independent	B
of	O
a	O
and	O
maximizing	O
with	O
respect	O
to	O
these	O
parameters	O
then	O
gives	O
anew	O
e	O
znzt	O
n	O
zn	O
n	O
new	O
n	O
znzt	O
n	O
e	O
e	O
anew	O
e	O
znzt	O
n	O
anew	O
anew	O
e	O
zn	O
n	O
e	O
zn	O
n	O
note	O
that	O
anew	O
must	O
be	O
evaluated	O
first	O
and	O
the	O
result	O
can	O
then	O
be	O
used	O
to	O
determine	O
new	O
pxnzn	O
c	O
in	O
using	O
giving	O
finally	O
in	O
order	O
to	O
determine	O
the	O
new	O
values	O
of	O
c	O
and	O
we	O
substitute	O
for	O
q	O
old	O
n	O
maximizing	O
with	O
respect	O
to	O
c	O
and	O
then	O
gives	O
cznt	O
czn	O
const	O
ln	O
ez	O
old	O
n	O
xne	O
cnew	O
xne	O
zt	O
n	O
new	O
n	O
cnew	O
xnxt	O
e	O
xt	O
n	O
e	O
znzt	O
n	O
zt	O
n	O
cnew	O
cnew	O
e	O
znzt	O
n	O
cnew	O
exercise	O
exercise	O
sequential	B
data	I
we	O
have	O
approached	O
parameter	O
learning	B
in	O
the	O
linear	B
dynamical	B
system	I
using	O
maximum	B
likelihood	I
inclusion	O
of	O
priors	O
to	O
give	O
a	O
map	O
estimate	O
is	O
straightforward	O
and	O
a	O
fully	O
bayesian	B
treatment	O
can	O
be	O
found	O
by	O
applying	O
the	O
analytical	O
approximation	O
techniques	O
discussed	O
in	O
chapter	O
though	O
a	O
detailed	O
treatment	O
is	O
precluded	O
here	O
due	O
to	O
lack	O
of	O
space	O
extensions	O
of	O
lds	O
as	O
with	O
the	O
hidden	B
markov	B
model	I
there	O
is	O
considerable	O
interest	O
in	O
extending	O
the	O
basic	O
linear	B
dynamical	B
system	I
in	O
order	O
to	O
increase	O
its	O
capabilities	O
although	O
the	O
assumption	O
of	O
a	O
linear-gaussian	B
model	I
leads	O
to	O
efficient	O
algorithms	O
for	O
inference	B
and	O
learning	B
it	O
also	O
implies	O
that	O
the	O
marginal	B
distribution	O
of	O
the	O
observed	O
variables	O
is	O
simply	O
a	O
gaussian	B
which	O
represents	O
a	O
significant	O
limitation	O
one	O
simple	O
extension	O
of	O
the	O
linear	B
dynamical	B
system	I
is	O
to	O
use	O
a	O
gaussian	B
mixture	B
as	O
the	O
initial	O
distribution	O
for	O
if	O
this	O
mixture	B
has	O
k	O
components	O
then	O
the	O
forward	O
recursion	O
equations	O
will	O
lead	O
to	O
a	O
mixture	B
of	O
k	O
gaussians	O
over	O
each	O
hidden	B
variable	I
zn	O
and	O
so	O
the	O
model	O
is	O
again	O
tractable	O
for	O
many	O
applications	O
the	O
gaussian	B
emission	O
density	B
is	O
a	O
poor	O
approximation	O
if	O
instead	O
we	O
try	O
to	O
use	O
a	O
mixture	B
of	O
k	O
gaussians	O
as	O
the	O
emission	O
density	B
then	O
the	O
will	O
also	O
be	O
a	O
mixture	B
of	O
k	O
gaussians	O
however	O
from	O
the	O
will	O
comprise	O
a	O
mixture	B
of	O
k	O
gaussians	O
and	O
so	O
on	O
being	O
given	O
by	O
a	O
mixture	B
of	O
k	O
n	O
gaussians	O
thus	O
the	O
number	O
of	O
components	O
grows	O
exponentially	O
with	O
the	O
length	O
of	O
the	O
chain	O
and	O
so	O
this	O
model	O
is	O
impractical	O
chapter	O
more	O
generally	O
introducing	O
transition	O
or	O
emission	O
models	O
that	O
depart	O
from	O
the	O
linear-gaussian	O
other	O
exponential	B
family	I
model	O
leads	O
to	O
an	O
intractable	O
inference	B
problem	O
we	O
can	O
make	O
deterministic	O
approximations	O
such	O
as	O
assumed	B
density	B
filtering	I
or	O
expectation	B
propagation	I
or	O
we	O
can	O
make	O
use	O
of	O
sampling	B
methods	I
as	O
discussed	O
in	O
section	O
one	O
widely	O
used	O
approach	O
is	O
to	O
make	O
a	O
gaussian	B
approximation	O
by	O
linearizing	O
around	O
the	O
mean	B
of	O
the	O
predicted	O
distribution	O
which	O
gives	O
rise	O
to	O
the	O
extended	B
kalman	B
filter	I
and	O
musoff	O
as	O
with	O
hidden	O
markov	O
models	O
we	O
can	O
develop	O
interesting	O
extensions	O
of	O
the	O
basic	O
linear	B
dynamical	B
system	I
by	O
expanding	O
its	O
graphical	O
representation	O
for	O
example	O
the	O
switching	B
state	B
space	I
model	I
and	O
hinton	O
can	O
be	O
viewed	O
as	O
a	O
combination	O
of	O
the	O
hidden	B
markov	B
model	I
with	O
a	O
set	O
of	O
linear	O
dynamical	O
systems	O
the	O
model	O
has	O
multiple	O
markov	O
chains	O
of	O
continuous	O
linear-gaussian	O
latent	O
variables	O
each	O
of	O
which	O
is	O
analogous	O
to	O
the	O
latent	O
chain	O
of	O
the	O
linear	B
dynamical	B
system	I
discussed	O
earlier	O
together	O
with	O
a	O
markov	B
chain	I
of	O
discrete	O
variables	O
of	O
the	O
form	O
used	O
in	O
a	O
hidden	B
markov	B
model	I
the	O
output	O
at	O
each	O
time	O
step	O
is	O
determined	O
by	O
stochastically	O
choosing	O
one	O
of	O
the	O
continuous	O
latent	O
chains	O
using	O
the	O
state	O
of	O
the	O
discrete	O
latent	B
variable	I
as	O
a	O
switch	O
and	O
then	O
emitting	O
an	O
observation	O
from	O
the	O
corresponding	O
conditional	B
output	O
distribution	O
exact	O
inference	B
in	O
this	O
model	O
is	O
intractable	O
but	O
variational	B
methods	O
lead	O
to	O
an	O
efficient	O
inference	B
scheme	O
involving	O
forward-backward	O
recursions	O
along	O
each	O
of	O
the	O
continuous	O
and	O
discrete	O
markov	O
chains	O
independently	O
note	O
that	O
if	O
we	O
consider	O
multiple	O
chains	O
of	O
discrete	O
latent	O
variables	O
and	O
use	O
one	O
as	O
the	O
switch	O
to	O
select	O
from	O
the	O
remainder	O
we	O
obtain	O
an	O
analogous	O
model	O
having	O
only	O
discrete	O
latent	O
variables	O
known	O
as	O
the	O
switching	B
hidden	B
markov	B
model	I
efzn	O
fznpznxn	O
dzn	O
fznpznxn	O
xn	O
dzn	O
fznpxnznpznxn	O
dzn	O
pxnznpznxn	O
dzn	O
linear	O
dynamical	O
systems	O
chapter	O
particle	O
filters	O
for	O
dynamical	O
systems	O
which	O
do	O
not	O
have	O
a	O
linear-gaussian	O
for	O
example	O
if	O
they	O
use	O
a	O
non-gaussian	O
emission	O
density	B
we	O
can	O
turn	O
to	O
sampling	B
methods	I
in	O
order	O
to	O
find	O
a	O
tractable	O
inference	B
algorithm	O
in	O
particular	O
we	O
can	O
apply	O
the	O
samplingimportance-resampling	O
formalism	O
of	O
section	O
to	O
obtain	O
a	O
sequential	O
monte	O
carlo	O
algorithm	O
known	O
as	O
the	O
particle	B
filter	I
consider	O
the	O
class	O
of	O
distributions	O
represented	O
by	O
the	O
graphical	B
model	I
in	O
figure	O
and	O
suppose	O
we	O
are	O
given	O
the	O
observed	O
values	O
xn	O
xn	O
and	O
we	O
wish	O
to	O
draw	O
l	O
samples	O
from	O
the	O
posterior	O
distribution	O
pznxn	O
using	O
bayes	B
theorem	O
we	O
have	O
wl	O
n	O
fzl	O
n	O
where	O
n	O
is	O
a	O
set	O
of	O
samples	O
drawn	O
from	O
pznxn	O
and	O
we	O
have	O
made	O
use	O
of	O
the	O
conditional	B
independence	I
property	O
pxnzn	O
xn	O
pxnzn	O
which	O
follows	O
from	O
the	O
graph	O
in	O
figure	O
the	O
sampling	O
weights	O
n	O
are	O
defined	O
by	O
pxnzl	O
n	O
pxnzm	O
n	O
wl	O
n	O
where	O
the	O
same	O
samples	O
are	O
used	O
in	O
the	O
numerator	O
as	O
in	O
the	O
denominator	O
thus	O
the	O
posterior	O
distribution	O
pznxn	O
is	O
represented	O
by	O
the	O
set	O
of	O
samples	O
n	O
together	O
with	O
the	O
corresponding	O
weights	O
n	O
and	O
n	O
note	O
that	O
these	O
weights	O
satisfy	O
w	O
n	O
because	O
we	O
wish	O
to	O
find	O
a	O
sequential	O
sampling	O
scheme	O
we	O
shall	O
suppose	O
that	O
a	O
set	O
of	O
samples	O
and	O
weights	O
have	O
been	O
obtained	O
at	O
time	O
step	O
n	O
and	O
that	O
we	O
have	O
subsequently	O
observed	O
the	O
value	O
of	O
and	O
we	O
wish	O
to	O
find	O
the	O
weights	O
and	O
samples	O
at	O
time	O
step	O
n	O
we	O
first	O
sample	O
from	O
the	O
distribution	O
this	O
is	O
l	O
w	O
sequential	B
data	I
straightforward	O
since	O
again	O
using	O
bayes	B
theorem	O
xnpznxn	O
dzn	O
dzn	O
xn	O
dzn	O
dzn	O
pxnznpznxn	O
dzn	O
n	O
wl	O
n	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
conditional	B
independence	I
properties	O
l	O
xn	O
pxnzn	O
xn	O
pxnzn	O
which	O
follow	O
from	O
the	O
application	O
of	O
the	O
d-separation	B
criterion	O
to	O
the	O
graph	O
in	O
figure	O
the	O
distribution	O
given	O
by	O
is	O
a	O
mixture	B
distribution	I
and	O
samples	O
can	O
be	O
drawn	O
by	O
choosing	O
a	O
component	O
l	O
with	O
probability	B
given	O
by	O
the	O
mixing	O
coefficients	O
wl	O
and	O
then	O
drawing	O
a	O
sample	O
from	O
the	O
corresponding	O
component	O
in	O
summary	O
we	O
can	O
view	O
each	O
step	O
of	O
the	O
particle	B
filter	I
algorithm	O
as	O
comprising	O
two	O
stages	O
at	O
time	O
step	O
n	O
we	O
have	O
a	O
sample	O
representation	O
of	O
the	O
posterior	O
distribution	O
pznxn	O
expressed	O
as	O
samples	O
n	O
this	O
can	O
be	O
viewed	O
as	O
a	O
mixture	B
representation	O
of	O
the	O
form	O
to	O
obtain	O
the	O
corresponding	O
representation	O
for	O
the	O
next	O
time	O
step	O
we	O
first	O
draw	O
l	O
samples	O
from	O
the	O
mixture	B
distribution	I
and	O
then	O
for	O
each	O
sample	O
we	O
use	O
the	O
new	O
this	O
is	O
vation	O
to	O
evaluate	O
the	O
corresponding	O
weights	O
w	O
illustrated	O
for	O
the	O
case	O
of	O
a	O
single	O
variable	O
z	O
in	O
figure	O
n	O
with	O
corresponding	O
weights	O
the	O
particle	O
filtering	O
or	O
sequential	O
monte	O
carlo	O
approach	O
has	O
appeared	O
in	O
the	O
literature	O
under	O
various	O
names	O
including	O
the	O
bootstrap	B
filter	I
et	O
al	O
survival	B
of	I
the	I
fittest	I
et	O
al	O
and	O
the	O
condensation	B
algorithm	I
and	O
blake	O
exercises	O
www	O
use	O
the	O
technique	O
of	O
d-separation	B
discussed	O
in	O
section	O
to	O
verify	O
that	O
the	O
markov	B
model	I
shown	O
in	O
figure	O
having	O
n	O
nodes	O
in	O
total	O
satisfies	O
the	O
conditional	B
independence	I
properties	O
for	O
n	O
n	O
similarly	O
show	O
that	O
a	O
model	O
described	O
by	O
the	O
graph	O
in	O
figure	O
in	O
which	O
there	O
are	O
n	O
nodes	O
in	O
total	O
pznxn	O
exercises	O
z	O
figure	O
schematic	O
illustration	O
of	O
the	O
operation	O
of	O
the	O
particle	B
filter	I
for	O
a	O
one-dimensional	O
latent	O
space	O
at	O
time	O
step	O
n	O
the	O
posterior	O
pznxn	O
is	O
represented	O
as	O
a	O
mixture	B
distribution	I
shown	O
schematically	O
as	O
circles	O
whose	O
sizes	O
are	O
proportional	O
to	O
the	O
weights	O
wl	O
n	O
a	O
set	O
of	O
l	O
samples	O
is	O
then	O
drawn	O
from	O
this	O
distribution	O
and	O
the	O
new	O
weights	O
wl	O
evaluated	O
using	O
satisfies	O
the	O
conditional	B
independence	I
properties	O
xn	O
pxnxn	O
xn	O
for	O
n	O
n	O
consider	O
the	O
joint	O
probability	B
distribution	O
corresponding	O
to	O
the	O
directed	B
graph	O
of	O
figure	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
verify	O
that	O
this	O
joint	O
distribution	O
satisfies	O
the	O
conditional	B
independence	I
property	O
for	O
n	O
n	O
similarly	O
show	O
that	O
the	O
second-order	O
markov	B
model	I
described	O
by	O
the	O
joint	O
distribution	O
satisfies	O
the	O
conditional	B
independence	I
property	O
xn	O
pxnxn	O
xn	O
for	O
n	O
n	O
by	O
using	O
d-separation	B
show	O
that	O
the	O
distribution	O
xn	O
of	O
the	O
observed	O
data	O
for	O
the	O
state	B
space	I
model	I
represented	O
by	O
the	O
directed	B
graph	O
in	O
figure	O
does	O
not	O
satisfy	O
any	O
conditional	B
independence	I
properties	O
and	O
hence	O
does	O
not	O
exhibit	O
the	O
markov	O
property	O
at	O
any	O
finite	O
order	O
www	O
consider	O
a	O
hidden	B
markov	B
model	I
in	O
which	O
the	O
emission	O
densities	O
are	O
represented	O
by	O
a	O
parametric	O
model	O
pxz	O
w	O
such	O
as	O
a	O
linear	B
regression	B
model	O
or	O
a	O
neural	B
network	I
in	O
which	O
w	O
is	O
a	O
vector	O
of	O
adaptive	O
parameters	O
describe	O
how	O
the	O
parameters	O
w	O
can	O
be	O
learned	O
from	O
data	O
using	O
maximum	B
likelihood	I
sequential	B
data	I
verify	O
the	O
m-step	O
equations	O
and	O
for	O
the	O
initial	O
state	O
probabilities	O
and	O
transition	B
probability	B
parameters	O
of	O
the	O
hidden	B
markov	B
model	I
by	O
maximization	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	B
function	I
using	O
appropriate	O
lagrange	B
multipliers	O
to	O
enforce	O
the	O
summation	O
constraints	O
on	O
the	O
components	O
of	O
and	O
a	O
show	O
that	O
if	O
any	O
elements	O
of	O
the	O
parameters	O
or	O
a	O
for	O
a	O
hidden	B
markov	B
model	I
are	O
initially	O
set	O
to	O
zero	O
then	O
those	O
elements	O
will	O
remain	O
zero	O
in	O
all	O
subsequent	O
updates	O
of	O
the	O
em	B
algorithm	I
consider	O
a	O
hidden	B
markov	B
model	I
with	O
gaussian	B
emission	O
densities	O
show	O
that	O
maximization	O
of	O
the	O
function	O
q	O
old	O
with	O
respect	O
to	O
the	O
mean	B
and	O
covariance	B
parameters	O
of	O
the	O
gaussians	O
gives	O
rise	O
to	O
the	O
m-step	O
equations	O
and	O
www	O
for	O
a	O
hidden	B
markov	B
model	I
having	O
discrete	O
observations	O
governed	O
by	O
a	O
multinomial	B
distribution	I
show	O
that	O
the	O
conditional	B
distribution	O
of	O
the	O
observations	O
given	O
the	O
hidden	O
variables	O
is	O
given	O
by	O
and	O
the	O
corresponding	O
m	O
step	O
equations	O
are	O
given	O
by	O
write	O
down	O
the	O
analogous	O
equations	O
for	O
the	O
conditional	B
distribution	O
and	O
the	O
m	O
step	O
equations	O
for	O
the	O
case	O
of	O
a	O
hidden	O
markov	O
with	O
multiple	O
binary	O
output	O
variables	O
each	O
of	O
which	O
is	O
governed	O
by	O
a	O
bernoulli	B
conditional	B
distribution	O
hint	O
refer	O
to	O
sections	O
and	O
for	O
a	O
discussion	O
of	O
the	O
corresponding	O
maximum	B
likelihood	I
solutions	O
for	O
i	O
i	O
d	O
data	O
if	O
required	O
www	O
use	O
the	O
d-separation	B
criterion	O
to	O
verify	O
that	O
the	O
conditional	B
independence	I
properties	O
are	O
satisfied	O
by	O
the	O
joint	O
distribution	O
for	O
the	O
hidden	B
markov	B
model	I
defined	O
by	O
by	O
applying	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
verify	O
that	O
the	O
conditional	B
independence	I
properties	O
are	O
satisfied	O
by	O
the	O
joint	O
distribution	O
for	O
the	O
hidden	B
markov	B
model	I
defined	O
by	O
starting	O
from	O
the	O
expression	O
for	O
the	O
marginal	B
distribution	O
over	O
the	O
variables	O
of	O
a	O
factor	O
in	O
a	O
factor	B
graph	I
together	O
with	O
the	O
results	O
for	O
the	O
messages	O
in	O
the	O
sum-product	B
algorithm	I
obtained	O
in	O
section	O
derive	O
the	O
result	O
for	O
the	O
joint	O
posterior	O
distribution	O
over	O
two	O
successive	O
latent	O
variables	O
in	O
a	O
hidden	B
markov	B
model	I
suppose	O
we	O
wish	O
to	O
train	O
a	O
hidden	B
markov	B
model	I
by	O
maximum	B
likelihood	I
using	O
data	O
that	O
comprises	O
r	O
independent	B
sequences	O
of	O
observations	O
which	O
we	O
denote	O
by	O
xr	O
where	O
r	O
r	O
show	O
that	O
in	O
the	O
e	O
step	O
of	O
the	O
em	B
algorithm	I
we	O
simply	O
evaluate	O
posterior	O
probabilities	O
for	O
the	O
latent	O
variables	O
by	O
running	O
the	O
and	O
recursions	O
independently	O
for	O
each	O
of	O
the	O
sequences	O
also	O
show	O
that	O
in	O
the	O
m	O
step	O
the	O
initial	O
probability	B
and	O
transition	B
probability	B
parameters	O
are	O
re-estimated	O
using	O
modified	O
forms	O
of	O
and	O
given	O
by	O
k	O
ajk	O
n	O
z	O
nk	O
n	O
z	O
nl	O
exercises	O
where	O
for	O
notational	O
convenience	O
we	O
have	O
assumed	O
that	O
the	O
sequences	O
are	O
of	O
the	O
same	O
length	O
generalization	B
to	O
sequences	O
of	O
different	O
lengths	O
is	O
straightforward	O
similarly	O
show	O
that	O
the	O
m-step	O
equation	O
for	O
re-estimation	O
of	O
the	O
means	O
of	O
gaussian	B
emission	O
models	O
is	O
given	O
by	O
k	O
nk	O
n	O
nk	O
note	O
that	O
the	O
m-step	O
equations	O
for	O
other	O
emission	O
model	O
parameters	O
and	O
distributions	O
take	O
an	O
analogous	O
form	O
www	O
use	O
the	O
definition	O
of	O
the	O
messages	O
passed	O
from	O
a	O
factor	O
node	B
to	O
a	O
variable	O
node	B
in	O
a	O
factor	B
graph	I
together	O
with	O
the	O
expression	O
for	O
the	O
joint	O
distribution	O
in	O
a	O
hidden	B
markov	B
model	I
to	O
show	O
that	O
the	O
definition	O
of	O
the	O
alpha	O
message	O
is	O
the	O
same	O
as	O
the	O
definition	O
use	O
the	O
definition	O
of	O
the	O
messages	O
passed	O
from	O
a	O
factor	O
node	B
to	O
a	O
variable	O
node	B
in	O
a	O
factor	B
graph	I
together	O
with	O
the	O
expression	O
for	O
the	O
joint	O
distribution	O
in	O
a	O
hidden	B
markov	B
model	I
to	O
show	O
that	O
the	O
definition	O
of	O
the	O
beta	O
message	O
is	O
the	O
same	O
as	O
the	O
definition	O
use	O
the	O
expressions	O
and	O
for	O
the	O
marginals	O
in	O
a	O
hidden	B
markov	B
model	I
to	O
derive	O
the	O
corresponding	O
results	O
and	O
expressed	O
in	O
terms	O
of	O
re-scaled	O
variables	O
in	O
this	O
exercise	O
we	O
derive	O
the	O
forward	O
message	B
passing	I
equation	O
for	O
the	O
viterbi	B
algorithm	I
directly	O
from	O
the	O
expression	O
for	O
the	O
joint	O
distribution	O
this	O
involves	O
maximizing	O
over	O
all	O
of	O
the	O
hidden	O
variables	O
zn	O
by	O
taking	O
the	O
logarithm	O
and	O
then	O
exchanging	O
maximizations	O
and	O
summations	O
derive	O
the	O
recursion	O
sequential	B
data	I
where	O
the	O
quantities	O
are	O
defined	O
by	O
show	O
that	O
the	O
initial	O
condition	O
for	O
this	O
recursion	O
is	O
given	O
by	O
www	O
show	O
that	O
the	O
directed	B
graph	O
for	O
the	O
input-output	B
hidden	B
markov	B
model	I
given	O
in	O
figure	O
can	O
be	O
expressed	O
as	O
a	O
tree-structured	O
factor	B
graph	I
of	O
the	O
form	O
shown	O
in	O
figure	O
and	O
write	O
down	O
expressions	O
for	O
the	O
initial	O
factor	O
and	O
for	O
the	O
general	O
factor	O
fnzn	O
zn	O
where	O
n	O
n	O
using	O
the	O
result	O
of	O
exercise	O
derive	O
the	O
recursion	O
equations	O
including	O
the	O
initial	O
conditions	O
for	O
the	O
forward-backward	B
algorithm	I
for	O
the	O
input-output	B
hidden	B
markov	B
model	I
shown	O
in	O
figure	O
www	O
the	O
kalman	B
filter	I
and	O
smoother	O
equations	O
allow	O
the	O
posterior	O
distributions	O
over	O
individual	O
latent	O
variables	O
conditioned	O
on	O
all	O
of	O
the	O
observed	O
variables	O
to	O
be	O
found	O
efficiently	O
for	O
linear	O
dynamical	O
systems	O
show	O
that	O
the	O
sequence	O
of	O
latent	B
variable	I
values	O
obtained	O
by	O
maximizing	O
each	O
of	O
these	O
posterior	O
distributions	O
individually	O
is	O
the	O
same	O
as	O
the	O
most	O
probable	O
sequence	O
of	O
latent	O
values	O
to	O
do	O
this	O
simply	O
note	O
that	O
the	O
joint	O
distribution	O
of	O
all	O
latent	O
and	O
observed	O
variables	O
in	O
a	O
linear	B
dynamical	B
system	I
is	O
gaussian	B
and	O
hence	O
all	O
conditionals	O
and	O
marginals	O
will	O
also	O
be	O
gaussian	B
and	O
then	O
make	O
use	O
of	O
the	O
result	O
www	O
use	O
the	O
result	O
to	O
prove	O
use	O
the	O
results	O
and	O
together	O
with	O
the	O
matrix	O
identities	O
and	O
to	O
derive	O
the	O
results	O
and	O
where	O
the	O
kalman	B
gain	I
matrix	I
kn	O
is	O
defined	O
by	O
www	O
using	O
together	O
with	O
the	O
definitions	O
and	O
and	O
the	O
result	O
derive	O
using	O
together	O
with	O
the	O
definitions	O
and	O
and	O
the	O
result	O
derive	O
and	O
www	O
consider	O
a	O
generalization	B
of	O
and	O
in	O
which	O
we	O
include	O
constant	O
terms	O
a	O
and	O
c	O
in	O
the	O
gaussian	B
means	O
so	O
that	O
pznzn	O
n	O
a	O
pxnzn	O
n	O
c	O
show	O
that	O
this	O
extension	O
can	O
be	O
re-case	O
in	O
the	O
framework	O
discussed	O
in	O
this	O
chapter	O
by	O
defining	O
a	O
state	O
vector	O
z	O
with	O
an	O
additional	O
component	O
fixed	O
at	O
unity	O
and	O
then	O
augmenting	O
the	O
matrices	O
a	O
and	O
c	O
using	O
extra	O
columns	O
corresponding	O
to	O
the	O
parameters	O
a	O
and	O
c	O
in	O
this	O
exercise	O
we	O
show	O
that	O
when	O
the	O
kalman	B
filter	I
equations	O
are	O
applied	O
to	O
independent	B
observations	O
they	O
reduce	O
to	O
the	O
results	O
given	O
in	O
section	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
a	O
single	O
gaussian	B
distribution	O
consider	O
the	O
problem	O
of	O
finding	O
the	O
mean	B
of	O
a	O
single	O
gaussian	B
random	O
variable	O
x	O
in	O
which	O
we	O
are	O
given	O
a	O
set	O
of	O
independent	B
observations	O
xn	O
to	O
model	O
this	O
we	O
can	O
use	O
exercises	O
a	O
linear	B
dynamical	B
system	I
governed	O
by	O
and	O
with	O
latent	O
variables	O
zn	O
in	O
which	O
c	O
becomes	O
the	O
identity	O
matrix	O
and	O
where	O
the	O
transition	B
probability	B
a	O
because	O
the	O
observations	O
are	O
independent	B
let	O
the	O
parameters	O
and	O
of	O
the	O
initial	O
state	O
be	O
denoted	O
by	O
and	O
respectively	O
and	O
suppose	O
that	O
becomes	O
write	O
down	O
the	O
corresponding	O
kalman	B
filter	I
equations	O
starting	O
from	O
the	O
general	O
results	O
and	O
together	O
with	O
and	O
show	O
that	O
these	O
are	O
equivalent	O
to	O
the	O
results	O
and	O
obtained	O
directly	O
by	O
considering	O
independent	B
data	O
consider	O
a	O
special	O
case	O
of	O
the	O
linear	B
dynamical	B
system	I
of	O
section	O
that	O
is	O
equivalent	O
to	O
probabilistic	B
pca	I
so	O
that	O
the	O
transition	O
matrix	O
a	O
the	O
covariance	B
i	O
and	O
the	O
noise	O
covariance	B
by	O
making	O
use	O
of	O
the	O
matrix	O
inversion	O
identity	O
show	O
that	O
if	O
the	O
emission	O
density	B
matrix	O
c	O
is	O
denoted	O
w	O
then	O
the	O
posterior	O
distribution	O
over	O
the	O
hidden	O
states	O
defined	O
by	O
and	O
reduces	O
to	O
the	O
result	O
for	O
probabilistic	B
pca	I
www	O
consider	O
a	O
linear	B
dynamical	B
system	I
of	O
the	O
form	O
discussed	O
in	O
section	O
in	O
which	O
the	O
amplitude	O
of	O
the	O
observation	O
noise	O
goes	O
to	O
zero	O
so	O
that	O
show	O
that	O
the	O
posterior	O
distribution	O
for	O
zn	O
has	O
mean	B
xn	O
and	O
zero	O
variance	B
this	O
accords	O
with	O
our	O
intuition	O
that	O
if	O
there	O
is	O
no	O
noise	O
we	O
should	O
just	O
use	O
the	O
current	O
observation	O
xn	O
to	O
estimate	O
the	O
state	O
variable	O
zn	O
and	O
ignore	O
all	O
previous	O
observations	O
consider	O
a	O
special	O
case	O
of	O
the	O
linear	B
dynamical	B
system	I
of	O
section	O
in	O
which	O
the	O
state	O
variable	O
zn	O
is	O
constrained	O
to	O
be	O
equal	O
to	O
the	O
previous	O
state	O
variable	O
which	O
corresponds	O
to	O
a	O
i	O
and	O
for	O
simplicity	O
assume	O
also	O
that	O
so	O
that	O
the	O
initial	O
conditions	O
for	O
z	O
are	O
unimportant	O
and	O
the	O
predictions	O
are	O
determined	O
purely	O
by	O
the	O
data	O
use	O
proof	O
by	O
induction	O
to	O
show	O
that	O
the	O
posterior	O
mean	B
for	O
state	O
zn	O
is	O
determined	O
by	O
the	O
average	O
of	O
xn	O
this	O
corresponds	O
to	O
the	O
intuitive	O
result	O
that	O
if	O
the	O
state	O
variable	O
is	O
constant	O
our	O
best	O
estimate	O
is	O
obtained	O
by	O
averaging	O
the	O
observations	O
starting	O
from	O
the	O
backwards	O
recursion	O
equation	O
derive	O
the	O
rts	O
smoothing	O
equations	O
and	O
for	O
the	O
gaussian	B
linear	B
dynamical	B
system	I
starting	O
from	O
the	O
result	O
for	O
the	O
pairwise	O
posterior	O
marginal	B
in	O
a	O
state	B
space	I
model	I
derive	O
the	O
specific	O
form	O
for	O
the	O
case	O
of	O
the	O
gaussian	B
linear	B
dynamical	B
system	I
starting	O
from	O
the	O
result	O
and	O
by	O
substituting	O
using	O
verify	O
the	O
result	O
for	O
the	O
covariance	B
between	O
zn	O
and	O
zn	O
www	O
verify	O
the	O
results	O
and	O
for	O
the	O
m-step	O
equations	O
for	O
and	O
in	O
the	O
linear	B
dynamical	B
system	I
verify	O
the	O
results	O
and	O
for	O
the	O
m-step	O
equations	O
for	O
a	O
and	O
in	O
the	O
linear	B
dynamical	B
system	I
sequential	B
data	I
verify	O
the	O
results	O
and	O
for	O
the	O
m-step	O
equations	O
for	O
c	O
and	O
in	O
the	O
linear	B
dynamical	B
system	I
combining	B
models	I
in	O
earlier	O
chapters	O
we	O
have	O
explored	O
a	O
range	O
of	O
different	O
models	O
for	O
solving	O
classification	B
and	O
regression	B
problems	O
it	O
is	O
often	O
found	O
that	O
improved	O
performance	O
can	O
be	O
obtained	O
by	O
combining	O
multiple	O
models	O
together	O
in	O
some	O
way	O
instead	O
of	O
just	O
using	O
a	O
single	O
model	O
in	O
isolation	O
for	O
instance	O
we	O
might	O
train	O
l	O
different	O
models	O
and	O
then	O
make	O
predictions	O
using	O
the	O
average	O
of	O
the	O
predictions	O
made	O
by	O
each	O
model	O
such	O
combinations	O
of	O
models	O
are	O
sometimes	O
called	O
committees	O
in	O
section	O
we	O
discuss	O
ways	O
to	O
apply	O
the	O
committee	B
concept	O
in	O
practice	O
and	O
we	O
also	O
give	O
some	O
insight	O
into	O
why	O
it	O
can	O
sometimes	O
be	O
an	O
effective	O
procedure	O
one	O
important	O
variant	O
of	O
the	O
committee	B
method	O
known	O
as	O
boosting	B
involves	O
training	B
multiple	O
models	O
in	O
sequence	O
in	O
which	O
the	O
error	B
function	I
used	O
to	O
train	O
a	O
particular	O
model	O
depends	O
on	O
the	O
performance	O
of	O
the	O
previous	O
models	O
this	O
can	O
produce	O
substantial	O
improvements	O
in	O
performance	O
compared	O
to	O
the	O
use	O
of	O
a	O
single	O
model	O
and	O
is	O
discussed	O
in	O
section	O
instead	O
of	O
averaging	O
the	O
predictions	O
of	O
a	O
set	O
of	O
models	O
an	O
alternative	O
form	O
of	O
combining	B
models	I
model	O
combination	O
is	O
to	O
select	O
one	O
of	O
the	O
models	O
to	O
make	O
the	O
prediction	O
in	O
which	O
the	O
choice	O
of	O
model	O
is	O
a	O
function	O
of	O
the	O
input	O
variables	O
thus	O
different	O
models	O
become	O
responsible	O
for	O
making	O
predictions	O
in	O
different	O
regions	O
of	O
input	O
space	O
one	O
widely	O
used	O
framework	O
of	O
this	O
kind	O
is	O
known	O
as	O
a	O
decision	B
tree	B
in	O
which	O
the	O
selection	O
process	O
can	O
be	O
described	O
as	O
a	O
sequence	O
of	O
binary	O
selections	O
corresponding	O
to	O
the	O
traversal	O
of	O
a	O
tree	B
structure	O
and	O
is	O
discussed	O
in	O
section	O
in	O
this	O
case	O
the	O
individual	O
models	O
are	O
generally	O
chosen	O
to	O
be	O
very	O
simple	O
and	O
the	O
overall	O
flexibility	O
of	O
the	O
model	O
arises	O
from	O
the	O
input-dependent	O
selection	O
process	O
decision	O
trees	O
can	O
be	O
applied	O
to	O
both	O
classification	B
and	O
regression	B
problems	O
one	O
limitation	O
of	O
decision	O
trees	O
is	O
that	O
the	O
division	O
of	O
input	O
space	O
is	O
based	O
on	O
hard	O
splits	O
in	O
which	O
only	O
one	O
model	O
is	O
responsible	O
for	O
making	O
predictions	O
for	O
any	O
given	O
value	O
of	O
the	O
input	O
variables	O
the	O
decision	O
process	O
can	O
be	O
softened	O
by	O
moving	O
to	O
a	O
probabilistic	O
framework	O
for	O
combining	B
models	I
as	O
discussed	O
in	O
section	O
for	O
example	O
if	O
we	O
have	O
a	O
set	O
of	O
k	O
models	O
for	O
a	O
conditional	B
distribution	O
ptx	O
k	O
where	O
x	O
is	O
the	O
input	O
variable	O
t	O
is	O
the	O
target	O
variable	O
and	O
k	O
k	O
indexes	O
the	O
model	O
then	O
we	O
can	O
form	O
a	O
probabilistic	O
mixture	B
of	O
the	O
form	O
ptx	O
kxptx	O
k	O
in	O
which	O
kx	O
pkx	O
represent	O
the	O
input-dependent	O
mixing	O
coefficients	O
such	O
models	O
can	O
be	O
viewed	O
as	O
mixture	B
distributions	O
in	O
which	O
the	O
component	O
densities	O
as	O
well	O
as	O
the	O
mixing	O
coefficients	O
are	O
conditioned	O
on	O
the	O
input	O
variables	O
and	O
are	O
known	O
as	O
mixtures	O
of	O
experts	O
they	O
are	O
closely	O
related	O
to	O
the	O
mixture	B
density	B
network	I
model	O
discussed	O
in	O
section	O
bayesian	B
model	B
averaging	I
it	O
is	O
important	O
to	O
distinguish	O
between	O
model	O
combination	O
methods	O
and	O
bayesian	B
model	B
averaging	I
as	O
the	O
two	O
are	O
often	O
confused	O
to	O
understand	O
the	O
difference	O
consider	O
the	O
example	O
of	O
density	B
estimation	I
using	O
a	O
mixture	B
of	I
gaussians	I
in	O
which	O
several	O
gaussian	B
components	O
are	O
combined	O
probabilistically	O
the	O
model	O
contains	O
a	O
binary	O
latent	B
variable	I
z	O
that	O
indicates	O
which	O
component	O
of	O
the	O
mixture	B
is	O
responsible	O
for	O
generating	O
the	O
corresponding	O
data	O
point	O
thus	O
the	O
model	O
is	O
specified	O
in	O
terms	O
of	O
a	O
joint	O
distribution	O
and	O
the	O
corresponding	O
density	B
over	O
the	O
observed	B
variable	I
x	O
is	O
obtained	O
by	O
marginalizing	O
over	O
the	O
latent	B
variable	I
px	O
z	O
px	O
px	O
z	O
z	O
section	O
committees	O
in	O
the	O
case	O
of	O
our	O
gaussian	B
mixture	B
example	O
this	O
leads	O
to	O
a	O
distribution	O
of	O
the	O
form	O
px	O
kn	O
k	O
k	O
with	O
the	O
usual	O
interpretation	O
of	O
the	O
symbols	O
this	O
is	O
an	O
example	O
of	O
model	O
combi	O
nation	O
for	O
independent	B
identically	I
distributed	I
data	O
we	O
can	O
use	O
to	O
write	O
the	O
marginal	B
probability	B
of	O
a	O
data	O
set	O
x	O
xn	O
in	O
the	O
form	O
px	O
pxn	O
pxn	O
zn	O
zn	O
thus	O
we	O
see	O
that	O
each	O
observed	O
data	O
point	O
xn	O
has	O
a	O
corresponding	O
latent	B
variable	I
zn	O
now	O
suppose	O
we	O
have	O
several	O
different	O
models	O
indexed	O
by	O
h	O
h	O
with	O
prior	B
probabilities	O
ph	O
for	O
instance	O
one	O
model	O
might	O
be	O
a	O
mixture	B
of	I
gaussians	I
and	O
another	O
model	O
might	O
be	O
a	O
mixture	B
of	O
cauchy	O
distributions	O
the	O
marginal	B
distribution	O
over	O
the	O
data	O
set	O
is	O
given	O
by	O
px	O
pxhph	O
this	O
is	O
an	O
example	O
of	O
bayesian	B
model	B
averaging	I
the	O
interpretation	O
of	O
this	O
summation	O
over	O
h	O
is	O
that	O
just	O
one	O
model	O
is	O
responsible	O
for	O
generating	O
the	O
whole	O
data	O
set	O
and	O
the	O
probability	B
distribution	O
over	O
h	O
simply	O
reflects	O
our	O
uncertainty	O
as	O
to	O
which	O
model	O
that	O
is	O
as	O
the	O
size	O
of	O
the	O
data	O
set	O
increases	O
this	O
uncertainty	O
reduces	O
and	O
the	O
posterior	O
probabilities	O
phx	O
become	O
increasingly	O
focussed	O
on	O
just	O
one	O
of	O
the	O
models	O
this	O
highlights	O
the	O
key	O
difference	O
between	O
bayesian	B
model	B
averaging	I
and	O
model	O
combination	O
because	O
in	O
bayesian	B
model	B
averaging	I
the	O
whole	O
data	O
set	O
is	O
generated	O
by	O
a	O
single	O
model	O
by	O
contrast	O
when	O
we	O
combine	O
multiple	O
models	O
as	O
in	O
we	O
see	O
that	O
different	O
data	O
points	O
within	O
the	O
data	O
set	O
can	O
potentially	O
be	O
generated	O
from	O
different	O
values	O
of	O
the	O
latent	B
variable	I
z	O
and	O
hence	O
by	O
different	O
components	O
although	O
we	O
have	O
considered	O
the	O
marginal	B
probability	B
px	O
the	O
same	O
considerations	O
apply	O
for	O
the	O
predictive	O
density	B
pxx	O
or	O
for	O
conditional	B
distributions	O
such	O
as	O
ptx	O
x	O
t	O
exercise	O
committees	O
section	O
the	O
simplest	O
way	O
to	O
construct	O
a	O
committee	B
is	O
to	O
average	O
the	O
predictions	O
of	O
a	O
set	O
of	O
individual	O
models	O
such	O
a	O
procedure	O
can	O
be	O
motivated	O
from	O
a	O
frequentist	B
perspective	O
by	O
considering	O
the	O
trade-off	O
between	O
bias	B
and	O
variance	B
which	O
decomposes	O
the	O
error	B
due	O
to	O
a	O
model	O
into	O
the	O
bias	B
component	O
that	O
arises	O
from	O
differences	O
between	O
the	O
model	O
and	O
the	O
true	O
function	O
to	O
be	O
predicted	O
and	O
the	O
variance	B
component	O
that	O
represents	O
the	O
sensitivity	O
of	O
the	O
model	O
to	O
the	O
individual	O
data	O
points	O
recall	O
from	O
figure	O
combining	B
models	I
that	O
when	O
we	O
trained	O
multiple	O
polynomials	O
using	O
the	O
sinusoidal	B
data	I
and	O
then	O
averaged	O
the	O
resulting	O
functions	O
the	O
contribution	O
arising	O
from	O
the	O
variance	B
term	O
tended	O
to	O
cancel	O
leading	O
to	O
improved	O
predictions	O
when	O
we	O
averaged	O
a	O
set	O
of	O
low-bias	O
models	O
to	O
higher	O
order	O
polynomials	O
we	O
obtained	O
accurate	O
predictions	O
for	O
the	O
underlying	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
were	O
generated	O
in	O
practice	O
of	O
course	O
we	O
have	O
only	O
a	O
single	O
data	O
set	O
and	O
so	O
we	O
have	O
to	O
find	O
a	O
way	O
to	O
introduce	O
variability	O
between	O
the	O
different	O
models	O
within	O
the	O
committee	B
one	O
approach	O
is	O
to	O
use	O
bootstrap	B
data	O
sets	O
discussed	O
in	O
section	O
consider	O
a	O
regression	B
problem	O
in	O
which	O
we	O
are	O
trying	O
to	O
predict	O
the	O
value	O
of	O
a	O
single	O
continuous	O
variable	O
and	O
suppose	O
we	O
generate	O
m	O
bootstrap	B
data	O
sets	O
and	O
then	O
use	O
each	O
to	O
train	O
a	O
separate	O
copy	O
ymx	O
of	O
a	O
predictive	O
model	O
where	O
m	O
m	O
the	O
committee	B
prediction	O
is	O
given	O
by	O
ymx	O
ycomx	O
m	O
this	O
procedure	O
is	O
known	O
as	O
bootstrap	B
aggregation	O
or	O
bagging	B
suppose	O
the	O
true	O
regression	B
function	I
that	O
we	O
are	O
trying	O
to	O
predict	O
is	O
given	O
by	O
hx	O
so	O
that	O
the	O
output	O
of	O
each	O
of	O
the	O
models	O
can	O
be	O
written	O
as	O
the	O
true	O
value	O
plus	O
an	O
error	B
in	O
the	O
form	O
ymx	O
hx	O
the	O
average	O
sum-of-squares	B
error	B
then	O
takes	O
the	O
form	O
ex	O
where	O
ex	O
denotes	O
a	O
frequentist	B
expectation	B
with	O
respect	O
to	O
the	O
distribution	O
of	O
the	O
input	O
vector	O
x	O
the	O
average	O
error	B
made	O
by	O
the	O
models	O
acting	O
individually	O
is	O
therefore	O
ex	O
eav	O
ex	O
m	O
m	O
m	O
ecom	O
ex	O
ex	O
ymx	O
hx	O
similarly	O
the	O
expected	O
error	B
from	O
the	O
committee	B
is	O
given	O
by	O
if	O
we	O
assume	O
that	O
the	O
errors	O
have	O
zero	O
mean	B
and	O
are	O
uncorrelated	O
so	O
that	O
ex	O
ex	O
m	O
l	O
exercise	O
then	O
we	O
obtain	O
boosting	B
ecom	O
m	O
eav	O
this	O
apparently	O
dramatic	O
result	O
suggests	O
that	O
the	O
average	O
error	B
of	O
a	O
model	O
can	O
be	O
reduced	O
by	O
a	O
factor	O
of	O
m	O
simply	O
by	O
averaging	O
m	O
versions	O
of	O
the	O
model	O
unfortunately	O
it	O
depends	O
on	O
the	O
key	O
assumption	O
that	O
the	O
errors	O
due	O
to	O
the	O
individual	O
models	O
are	O
uncorrelated	O
in	O
practice	O
the	O
errors	O
are	O
typically	O
highly	O
correlated	O
and	O
the	O
reduction	O
in	O
overall	O
error	B
is	O
generally	O
small	O
it	O
can	O
however	O
be	O
shown	O
that	O
the	O
expected	O
committee	B
error	B
will	O
not	O
exceed	O
the	O
expected	O
error	B
of	O
the	O
constituent	O
models	O
so	O
that	O
ecom	O
eav	O
in	O
order	O
to	O
achieve	O
more	O
significant	O
improvements	O
we	O
turn	O
to	O
a	O
more	O
sophisticated	O
technique	O
for	O
building	O
committees	O
known	O
as	O
boosting	B
exercise	O
boosting	B
boosting	B
is	O
a	O
powerful	O
technique	O
for	O
combining	O
multiple	O
base	O
classifiers	O
to	O
produce	O
a	O
form	O
of	O
committee	B
whose	O
performance	O
can	O
be	O
significantly	O
better	O
than	O
that	O
of	O
any	O
of	O
the	O
base	O
classifiers	O
here	O
we	O
describe	O
the	O
most	O
widely	O
used	O
form	O
of	O
boosting	B
algorithm	O
called	O
adaboost	B
short	O
for	O
adaptive	O
boosting	B
developed	O
by	O
freund	O
and	O
schapire	O
boosting	B
can	O
give	O
good	O
results	O
even	O
if	O
the	O
base	O
classifiers	O
have	O
a	O
performance	O
that	O
is	O
only	O
slightly	O
better	O
than	O
random	O
and	O
hence	O
sometimes	O
the	O
base	O
classifiers	O
are	O
known	O
as	O
weak	O
learners	O
originally	O
designed	O
for	O
solving	O
classification	B
problems	O
boosting	B
can	O
also	O
be	O
extended	B
to	O
regression	B
the	O
principal	O
difference	O
between	O
boosting	B
and	O
the	O
committee	B
methods	O
such	O
as	O
bagging	B
discussed	O
above	O
is	O
that	O
the	O
base	O
classifiers	O
are	O
trained	O
in	O
sequence	O
and	O
each	O
base	O
classifier	O
is	O
trained	O
using	O
a	O
weighted	O
form	O
of	O
the	O
data	O
set	O
in	O
which	O
the	O
weighting	O
coefficient	O
associated	O
with	O
each	O
data	O
point	O
depends	O
on	O
the	O
performance	O
of	O
the	O
previous	O
classifiers	O
in	O
particular	O
points	O
that	O
are	O
misclassified	O
by	O
one	O
of	O
the	O
base	O
classifiers	O
are	O
given	O
greater	O
weight	O
when	O
used	O
to	O
train	O
the	O
next	O
classifier	O
in	O
the	O
sequence	O
once	O
all	O
the	O
classifiers	O
have	O
been	O
trained	O
their	O
predictions	O
are	O
then	O
combined	O
through	O
a	O
weighted	O
majority	O
voting	O
scheme	O
as	O
illustrated	O
schematically	O
in	O
figure	O
consider	O
a	O
two-class	O
classification	B
problem	O
in	O
which	O
the	O
training	B
data	O
comprises	O
input	O
vectors	O
xn	O
along	O
with	O
corresponding	O
binary	O
target	O
variables	O
tn	O
where	O
tn	O
each	O
data	O
point	O
is	O
given	O
an	O
associated	O
weighting	O
parameter	O
wn	O
which	O
is	O
initially	O
set	O
for	O
all	O
data	O
points	O
we	O
shall	O
suppose	O
that	O
we	O
have	O
a	O
procedure	O
available	O
for	O
training	B
a	O
base	O
classifier	O
using	O
weighted	O
data	O
to	O
give	O
a	O
function	O
yx	O
at	O
each	O
stage	O
of	O
the	O
algorithm	O
adaboost	B
trains	O
a	O
new	O
classifier	O
using	O
a	O
data	O
set	O
in	O
which	O
the	O
weighting	O
coefficients	O
are	O
adjusted	O
according	O
to	O
the	O
performance	O
of	O
the	O
previously	O
trained	O
classifier	O
so	O
as	O
to	O
give	O
greater	O
weight	O
to	O
the	O
misclassified	O
data	O
points	O
finally	O
when	O
the	O
desired	O
number	O
of	O
base	O
classifiers	O
have	O
been	O
trained	O
they	O
are	O
combined	O
to	O
form	O
a	O
committee	B
using	O
coefficients	O
that	O
give	O
different	O
weight	O
to	O
different	O
base	O
classifiers	O
the	O
precise	O
form	O
of	O
the	O
adaboost	B
algorithm	O
is	O
given	O
below	O
combining	B
models	I
figure	O
schematic	O
illustration	O
of	O
the	O
boosting	B
framework	O
each	O
base	O
classifier	O
ymx	O
is	O
trained	O
on	O
a	O
weighted	O
form	O
of	O
the	O
training	B
set	I
arrows	O
in	O
which	O
the	O
weights	O
wm	O
depend	O
on	O
the	O
performance	O
of	O
the	O
previous	O
base	O
classifier	O
ym	O
arrows	O
once	O
all	O
base	O
classifiers	O
have	O
been	O
trained	O
they	O
are	O
combined	O
to	O
give	O
the	O
final	O
classifier	O
ym	O
arrows	O
n	O
n	O
n	O
n	O
ym	O
ym	O
sign	O
mymx	O
m	O
adaboost	B
initialize	O
the	O
data	O
weighting	O
coefficients	O
by	O
setting	O
w	O
n	O
for	O
n	O
n	O
for	O
m	O
m	O
fit	O
a	O
classifier	O
ymx	O
to	O
the	O
training	B
data	O
by	O
minimizing	O
the	O
weighted	O
error	B
function	I
jm	O
n	O
iymxn	O
tn	O
wm	O
where	O
iymxn	O
tn	O
is	O
the	O
indicator	O
function	O
and	O
equals	O
when	O
ymxn	O
tn	O
and	O
otherwise	O
evaluate	O
the	O
quantities	O
n	O
iymxn	O
tn	O
wm	O
wm	O
n	O
and	O
then	O
use	O
these	O
to	O
evaluate	O
m	O
ln	O
update	O
the	O
data	O
weighting	O
coefficients	O
n	O
wm	O
n	O
exp	O
miymxn	O
tn	O
make	O
predictions	O
using	O
the	O
final	O
model	O
which	O
is	O
given	O
by	O
boosting	B
ym	O
sign	O
mymx	O
we	O
see	O
that	O
the	O
first	O
base	O
classifier	O
is	O
trained	O
using	O
weighting	O
n	O
that	O
are	O
all	O
equal	O
which	O
therefore	O
corresponds	O
to	O
the	O
usual	O
procedure	O
cients	O
w	O
for	O
training	B
a	O
single	O
classifier	O
from	O
we	O
see	O
that	O
in	O
subsequent	O
iterations	O
the	O
weighting	O
coefficients	O
w	O
are	O
increased	O
for	O
data	O
points	O
that	O
are	O
misclassified	O
n	O
and	O
decreased	O
for	O
data	O
points	O
that	O
are	O
correctly	O
classified	O
successive	O
classifiers	O
are	O
therefore	O
forced	O
to	O
place	O
greater	O
emphasis	O
on	O
points	O
that	O
have	O
been	O
misclassified	O
by	O
previous	O
classifiers	O
and	O
data	O
points	O
that	O
continue	O
to	O
be	O
misclassified	O
by	O
successive	O
classifiers	O
receive	O
ever	O
greater	O
weight	O
the	O
quantities	O
represent	O
weighted	O
measures	O
of	O
the	O
error	B
rates	O
of	O
each	O
of	O
the	O
base	O
classifiers	O
on	O
the	O
data	O
set	O
we	O
therefore	O
see	O
that	O
the	O
weighting	O
coefficients	O
m	O
defined	O
by	O
give	O
greater	O
weight	O
to	O
the	O
more	O
accurate	O
classifiers	O
when	O
computing	O
the	O
overall	O
output	O
given	O
by	O
the	O
adaboost	B
algorithm	O
is	O
illustrated	O
in	O
figure	O
using	O
a	O
subset	O
of	O
data	O
points	O
taken	O
from	O
the	O
toy	O
classification	B
data	O
set	O
shown	O
in	O
figure	O
here	O
each	O
base	O
learners	O
consists	O
of	O
a	O
threshold	O
on	O
one	O
of	O
the	O
input	O
variables	O
this	O
simple	O
classifier	O
corresponds	O
to	O
a	O
form	O
of	O
decision	B
tree	B
known	O
as	O
a	O
decision	O
stumps	O
i	O
e	O
a	O
decision	B
tree	B
with	O
a	O
single	O
node	B
thus	O
each	O
base	O
learner	O
classifies	O
an	O
input	O
according	O
to	O
whether	O
one	O
of	O
the	O
input	O
features	O
exceeds	O
some	O
threshold	O
and	O
therefore	O
simply	O
partitions	O
the	O
space	O
into	O
two	O
regions	O
separated	O
by	O
a	O
linear	O
decision	O
surface	O
that	O
is	O
parallel	O
to	O
one	O
of	O
the	O
axes	O
section	O
minimizing	O
exponential	O
error	B
boosting	B
was	O
originally	O
motivated	O
using	O
statistical	O
learning	B
theory	B
leading	O
to	O
upper	O
bounds	O
on	O
the	O
generalization	B
error	B
however	O
these	O
bounds	O
turn	O
out	O
to	O
be	O
too	O
loose	O
to	O
have	O
practical	O
value	O
and	O
the	O
actual	O
performance	O
of	O
boosting	B
is	O
much	O
better	O
than	O
the	O
bounds	O
alone	O
would	O
suggest	O
friedman	O
et	O
al	O
gave	O
a	O
different	O
and	O
very	O
simple	O
interpretation	O
of	O
boosting	B
in	O
terms	O
of	O
the	O
sequential	O
minimization	O
of	O
an	O
exponential	O
error	B
function	I
consider	O
the	O
exponential	O
error	B
function	I
defined	O
by	O
exp	O
tnfmxn	O
e	O
where	O
fmx	O
is	O
a	O
classifier	O
defined	O
in	O
terms	O
of	O
a	O
linear	O
combination	O
of	O
base	O
classifiers	O
ylx	O
of	O
the	O
form	O
fmx	O
and	O
tn	O
are	O
the	O
training	B
set	I
target	O
values	O
our	O
goal	O
is	O
to	O
minimize	O
e	O
with	O
respect	O
to	O
both	O
the	O
weighting	O
coefficients	O
l	O
and	O
the	O
parameters	O
of	O
the	O
base	O
classifiers	O
ylx	O
lylx	O
combining	B
models	I
m	O
m	O
m	O
m	O
m	O
m	O
figure	O
illustration	O
of	O
boosting	B
in	O
which	O
the	O
base	O
learners	O
consist	O
of	O
simple	O
thresholds	O
applied	O
to	O
one	O
or	O
other	O
of	O
the	O
axes	O
each	O
figure	O
shows	O
the	O
number	O
m	O
of	O
base	O
learners	O
trained	O
so	O
far	O
along	O
with	O
the	O
decision	B
boundary	I
of	O
the	O
most	O
recent	O
base	O
learner	O
black	O
line	O
and	O
the	O
combined	O
decision	B
boundary	I
of	O
the	O
ensemble	O
green	O
line	O
each	O
data	O
point	O
is	O
depicted	O
by	O
a	O
circle	O
whose	O
radius	O
indicates	O
the	O
weight	O
assigned	O
to	O
that	O
data	O
point	O
when	O
training	B
the	O
most	O
recently	O
added	O
base	O
learner	O
thus	O
for	O
instance	O
we	O
see	O
that	O
points	O
that	O
are	O
misclassified	O
by	O
the	O
m	O
base	O
learner	O
are	O
given	O
greater	O
weight	O
when	O
training	B
the	O
m	O
base	O
learner	O
instead	O
of	O
doing	O
a	O
global	O
error	B
function	I
minimization	O
however	O
we	O
shall	O
suppose	O
that	O
the	O
base	O
classifiers	O
ym	O
are	O
fixed	O
as	O
are	O
their	O
coefficients	O
m	O
and	O
so	O
we	O
are	O
minimizing	O
only	O
with	O
respect	O
to	O
m	O
and	O
ymx	O
separating	O
off	O
the	O
contribution	O
from	O
base	O
classifier	O
ymx	O
we	O
can	O
then	O
write	O
the	O
error	B
function	I
in	O
the	O
form	O
e	O
exp	O
tnfm	O
tn	O
mymxn	O
wm	O
n	O
exp	O
tn	O
mymxn	O
n	O
exp	O
tnfm	O
can	O
be	O
viewed	O
as	O
constants	O
where	O
the	O
coefficients	O
w	O
if	O
we	O
denote	O
by	O
tm	O
the	O
set	O
of	O
because	O
we	O
are	O
optimizing	O
only	O
m	O
and	O
ymx	O
data	O
points	O
that	O
are	O
correctly	O
classified	O
by	O
ymx	O
and	O
if	O
we	O
denote	O
the	O
remaining	O
misclassified	O
points	O
by	O
mm	O
then	O
we	O
can	O
in	O
turn	O
rewrite	O
the	O
error	B
function	I
in	O
the	O
boosting	B
n	O
mm	O
wm	O
n	O
form	O
e	O
e	O
wm	O
n	O
e	O
n	O
tm	O
e	O
n	O
iymxn	O
tn	O
e	O
wm	O
wm	O
n	O
exercise	O
exercise	O
when	O
we	O
minimize	O
this	O
with	O
respect	O
to	O
ymx	O
we	O
see	O
that	O
the	O
second	O
term	O
is	O
constant	O
and	O
so	O
this	O
is	O
equivalent	O
to	O
minimizing	O
because	O
the	O
overall	O
multiplicative	O
factor	O
in	O
front	O
of	O
the	O
summation	O
does	O
not	O
affect	O
the	O
location	O
of	O
the	O
minimum	O
similarly	O
minimizing	O
with	O
respect	O
to	O
m	O
we	O
obtain	O
in	O
which	O
is	O
defined	O
by	O
from	O
we	O
see	O
that	O
having	O
found	O
m	O
and	O
ymx	O
the	O
weights	O
on	O
the	O
data	O
tn	O
mymxn	O
points	O
are	O
updated	O
using	O
n	O
wm	O
n	O
exp	O
making	O
use	O
of	O
the	O
fact	O
that	O
tnymxn	O
tn	O
we	O
see	O
that	O
the	O
weights	O
w	O
n	O
are	O
updated	O
at	O
the	O
next	O
iteration	O
using	O
exp	O
exp	O
miymxn	O
tn	O
wm	O
n	O
n	O
because	O
the	O
term	O
exp	O
is	O
independent	B
of	O
n	O
we	O
see	O
that	O
it	O
weights	O
all	O
data	O
points	O
by	O
the	O
same	O
factor	O
and	O
so	O
can	O
be	O
discarded	O
thus	O
we	O
obtain	O
finally	O
once	O
all	O
the	O
base	O
classifiers	O
are	O
trained	O
new	O
data	O
points	O
are	O
classified	O
by	O
evaluating	O
the	O
sign	O
of	O
the	O
combined	O
function	O
defined	O
according	O
to	O
because	O
the	O
factor	O
of	O
does	O
not	O
affect	O
the	O
sign	O
it	O
can	O
be	O
omitted	O
giving	O
error	B
functions	O
for	O
boosting	B
the	O
exponential	O
error	B
function	I
that	O
is	O
minimized	O
by	O
the	O
adaboost	B
algorithm	O
differs	O
from	O
those	O
considered	O
in	O
previous	O
chapters	O
to	O
gain	O
some	O
insight	O
into	O
the	O
nature	O
of	O
the	O
exponential	O
error	B
function	I
we	O
first	O
consider	O
the	O
expected	O
error	B
given	O
by	O
ext	O
tyx	O
exp	O
tyxptxpx	O
dx	O
t	O
if	O
we	O
perform	O
a	O
variational	B
minimization	O
with	O
respect	O
to	O
all	O
possible	O
functions	O
yx	O
we	O
obtain	O
yx	O
ln	O
pt	O
pt	O
combining	B
models	I
figure	O
plot	O
of	O
the	O
exponential	O
and	O
rescaled	O
cross-entropy	O
error	B
functions	O
along	O
with	O
the	O
hinge	O
error	B
used	O
in	O
support	B
vector	I
machines	O
and	O
the	O
misclassification	O
for	O
large	O
error	B
note	O
that	O
negative	O
values	O
of	O
z	O
tyx	O
the	O
cross-entropy	O
gives	O
a	O
linearly	O
increasing	O
penalty	O
whereas	O
the	O
exponential	O
loss	O
gives	O
an	O
exponentially	O
increasing	O
penalty	O
ez	O
z	O
which	O
is	O
half	O
the	O
log-odds	O
thus	O
the	O
adaboost	B
algorithm	O
is	O
seeking	O
the	O
best	O
approximation	O
to	O
the	O
log	B
odds	I
ratio	O
within	O
the	O
space	O
of	O
functions	O
represented	O
by	O
the	O
linear	O
combination	O
of	O
base	O
classifiers	O
subject	O
to	O
the	O
constrained	O
minimization	O
resulting	O
from	O
the	O
sequential	O
optimization	O
strategy	O
this	O
result	O
motivates	O
the	O
use	O
of	O
the	O
sign	O
function	O
in	O
to	O
arrive	O
at	O
the	O
final	O
classification	B
decision	O
we	O
have	O
already	O
seen	O
that	O
the	O
minimizer	O
yx	O
of	O
the	O
cross-entropy	O
error	B
for	O
two-class	O
classification	B
is	O
given	O
by	O
the	O
posterior	O
class	O
probability	B
in	O
the	O
case	O
of	O
a	O
target	O
variable	O
t	O
we	O
have	O
seen	O
that	O
the	O
error	B
function	I
is	O
given	O
by	O
exp	O
yt	O
this	O
is	O
compared	O
with	O
the	O
exponential	O
error	B
function	I
in	O
figure	O
where	O
we	O
have	O
divided	O
the	O
cross-entropy	O
error	B
by	O
a	O
constant	O
factor	O
so	O
that	O
it	O
passes	O
through	O
the	O
point	O
for	O
ease	O
of	O
comparison	O
we	O
see	O
that	O
both	O
can	O
be	O
seen	O
as	O
continuous	O
approximations	O
to	O
the	O
ideal	O
misclassification	O
error	B
function	I
an	O
advantage	O
of	O
the	O
exponential	O
error	B
is	O
that	O
its	O
sequential	O
minimization	O
leads	O
to	O
the	O
simple	O
adaboost	B
scheme	O
one	O
drawback	O
however	O
is	O
that	O
it	O
penalizes	O
large	O
negative	O
values	O
of	O
tyx	O
much	O
more	O
strongly	O
than	O
cross-entropy	O
in	O
particular	O
we	O
see	O
that	O
for	O
large	O
negative	O
values	O
of	O
ty	O
the	O
cross-entropy	O
grows	O
linearly	O
with	O
whereas	O
the	O
exponential	O
error	B
function	I
grows	O
exponentially	O
with	O
thus	O
the	O
exponential	O
error	B
function	I
will	O
be	O
much	O
less	O
robust	O
to	O
outliers	B
or	O
misclassified	O
data	O
points	O
another	O
important	O
difference	O
between	O
cross-entropy	O
and	O
the	O
exponential	O
error	B
function	I
is	O
that	O
the	O
latter	O
cannot	O
be	O
interpreted	O
as	O
the	O
log	O
likelihood	B
function	I
of	O
any	O
well-defined	O
probabilistic	O
model	O
furthermore	O
the	O
exponential	O
error	B
does	O
not	O
generalize	O
to	O
classification	B
problems	O
having	O
k	O
classes	O
again	O
in	O
contrast	O
to	O
the	O
cross-entropy	O
for	O
a	O
probabilistic	O
model	O
which	O
is	O
easily	O
generalized	B
to	O
give	O
the	O
interpretation	O
of	O
boosting	B
as	O
the	O
sequential	O
optimization	O
of	O
an	O
additive	O
model	O
under	O
an	O
exponential	O
error	B
et	O
al	O
opens	O
the	O
door	O
to	O
a	O
wide	O
range	O
of	O
boosting-like	O
algorithms	O
including	O
multiclass	B
extensions	O
by	O
altering	O
the	O
choice	O
of	O
error	B
function	I
it	O
also	O
motivates	O
the	O
extension	O
to	O
regression	B
problems	O
if	O
we	O
consider	O
a	O
sum-of-squares	B
error	B
function	I
for	B
regression	B
then	O
sequential	O
minimization	O
of	O
an	O
additive	O
model	O
of	O
the	O
form	O
simply	O
involves	O
fitting	O
each	O
new	O
base	O
classifier	O
to	O
the	O
residual	O
errors	O
tn	O
fm	O
from	O
the	O
previous	O
model	O
as	O
we	O
have	O
noted	O
however	O
the	O
sum-of-squares	B
error	B
is	O
not	O
robust	O
to	O
outliers	B
and	O
this	O
section	O
exercise	O
section	O
exercise	O
figure	O
comparison	O
of	O
the	O
squared	O
error	B
with	O
the	O
absolute	O
error	B
showing	O
how	O
the	O
latter	O
places	O
much	O
less	O
emphasis	O
on	O
large	O
errors	O
and	O
hence	O
is	O
more	O
robust	O
to	O
outliers	B
and	O
mislabelled	O
data	O
points	O
tree-based	O
models	O
ez	O
z	O
can	O
be	O
addressed	O
by	O
basing	O
the	O
boosting	B
algorithm	O
on	O
the	O
absolute	O
deviation	O
t	O
instead	O
these	O
two	O
error	B
functions	O
are	O
compared	O
in	O
figure	O
tree-based	O
models	O
there	O
are	O
various	O
simple	O
but	O
widely	O
used	O
models	O
that	O
work	O
by	O
partitioning	O
the	O
input	O
space	O
into	O
cuboid	O
regions	O
whose	O
edges	O
are	O
aligned	O
with	O
the	O
axes	O
and	O
then	O
assigning	O
a	O
simple	O
model	O
example	O
a	O
constant	O
to	O
each	O
region	O
they	O
can	O
be	O
viewed	O
as	O
a	O
model	O
combination	O
method	O
in	O
which	O
only	O
one	O
model	O
is	O
responsible	O
for	O
making	O
predictions	O
at	O
any	O
given	O
point	O
in	O
input	O
space	O
the	O
process	O
of	O
selecting	O
a	O
specific	O
model	O
given	O
a	O
new	O
input	O
x	O
can	O
be	O
described	O
by	O
a	O
sequential	O
decision	O
making	O
process	O
corresponding	O
to	O
the	O
traversal	O
of	O
a	O
binary	O
tree	B
that	O
splits	O
into	O
two	O
branches	O
at	O
each	O
node	B
here	O
we	O
focus	O
on	O
a	O
particular	O
tree-based	O
framework	O
called	O
classification	B
and	I
regression	B
trees	I
or	O
cart	O
et	O
al	O
although	O
there	O
are	O
many	O
other	O
variants	O
going	O
by	O
such	O
names	O
as	O
and	O
quinlan	O
figure	O
shows	O
an	O
illustration	O
of	O
a	O
recursive	O
binary	O
partitioning	O
of	O
the	O
input	O
space	O
along	O
with	O
the	O
corresponding	O
tree	B
structure	O
in	O
this	O
example	O
the	O
first	O
step	O
figure	O
illustration	O
of	O
a	O
two-dimensional	O
input	O
space	O
that	O
has	O
been	O
partitioned	B
into	O
five	O
regions	O
using	O
axis-aligned	O
boundaries	O
b	O
a	O
e	O
c	O
d	O
combining	B
models	I
figure	O
binary	O
tree	B
corresponding	O
to	O
the	O
parinput	O
space	O
shown	O
in	O
fig	O
titioning	O
of	O
ure	O
a	O
b	O
c	O
d	O
e	O
divides	O
the	O
whole	O
of	O
the	O
input	O
space	O
into	O
two	O
regions	O
according	O
to	O
whether	O
or	O
where	O
is	O
a	O
parameter	O
of	O
the	O
model	O
this	O
creates	O
two	O
subregions	O
each	O
of	O
which	O
can	O
then	O
be	O
subdivided	O
independently	O
for	O
instance	O
the	O
region	O
is	O
further	O
subdivided	O
according	O
to	O
whether	O
or	O
giving	O
rise	O
to	O
the	O
regions	O
denoted	O
a	O
and	O
b	O
the	O
recursive	O
subdivision	O
can	O
be	O
described	O
by	O
the	O
traversal	O
of	O
the	O
binary	O
tree	B
shown	O
in	O
figure	O
for	O
any	O
new	O
input	O
x	O
we	O
determine	O
which	O
region	O
it	O
falls	O
into	O
by	O
starting	O
at	O
the	O
top	O
of	O
the	O
tree	B
at	O
the	O
root	B
node	B
and	O
following	O
a	O
path	O
down	O
to	O
a	O
specific	O
leaf	O
node	B
according	O
to	O
the	O
decision	O
criteria	O
at	O
each	O
node	B
note	O
that	O
such	O
decision	O
trees	O
are	O
not	O
probabilistic	O
graphical	O
models	O
within	O
each	O
region	O
there	O
is	O
a	O
separate	O
model	O
to	O
predict	O
the	O
target	O
variable	O
for	O
instance	O
in	O
regression	B
we	O
might	O
simply	O
predict	O
a	O
constant	O
over	O
each	O
region	O
or	O
in	O
classification	B
we	O
might	O
assign	O
each	O
region	O
to	O
a	O
specific	O
class	O
a	O
key	O
property	O
of	O
treebased	O
models	O
which	O
makes	O
them	O
popular	O
in	O
fields	O
such	O
as	O
medical	O
diagnosis	O
for	O
example	O
is	O
that	O
they	O
are	O
readily	O
interpretable	O
by	O
humans	O
because	O
they	O
correspond	O
to	O
a	O
sequence	O
of	O
binary	O
decisions	O
applied	O
to	O
the	O
individual	O
input	O
variables	O
for	O
instance	O
to	O
predict	O
a	O
patient	O
s	O
disease	O
we	O
might	O
first	O
ask	O
is	O
their	O
temperature	O
greater	O
than	O
some	O
threshold	O
if	O
the	O
answer	O
is	O
yes	O
then	O
we	O
might	O
next	O
ask	O
is	O
their	O
blood	O
pressure	O
less	O
than	O
some	O
threshold	O
each	O
leaf	O
of	O
the	O
tree	B
is	O
then	O
associated	O
with	O
a	O
specific	O
diagnosis	O
in	O
order	O
to	O
learn	O
such	O
a	O
model	O
from	O
a	O
training	B
set	I
we	O
have	O
to	O
determine	O
the	O
structure	O
of	O
the	O
tree	B
including	O
which	O
input	O
variable	O
is	O
chosen	O
at	O
each	O
node	B
to	O
form	O
the	O
split	O
criterion	O
as	O
well	O
as	O
the	O
value	O
of	O
the	O
threshold	B
parameter	I
i	O
for	O
the	O
split	O
we	O
also	O
have	O
to	O
determine	O
the	O
values	O
of	O
the	O
predictive	O
variable	O
within	O
each	O
region	O
consider	O
first	O
a	O
regression	B
problem	O
in	O
which	O
the	O
goal	O
is	O
to	O
predict	O
a	O
single	O
target	O
variable	O
t	O
from	O
a	O
d-dimensional	O
vector	O
x	O
xdt	O
of	O
input	O
variables	O
the	O
training	B
data	O
consists	O
of	O
input	O
vectors	O
xn	O
along	O
with	O
the	O
corresponding	O
continuous	O
labels	O
tn	O
if	O
the	O
partitioning	O
of	O
the	O
input	O
space	O
is	O
given	O
and	O
we	O
minimize	O
the	O
sum-of-squares	B
error	B
function	I
then	O
the	O
optimal	O
value	O
of	O
the	O
predictive	O
variable	O
within	O
any	O
given	O
region	O
is	O
just	O
given	O
by	O
the	O
average	O
of	O
the	O
values	O
of	O
tn	O
for	O
those	O
data	O
points	O
that	O
fall	O
in	O
that	O
region	O
now	O
consider	O
how	O
to	O
determine	O
the	O
structure	O
of	O
the	O
decision	B
tree	B
even	O
for	O
a	O
fixed	O
number	O
of	O
nodes	O
in	O
the	O
tree	B
the	O
problem	O
of	O
determining	O
the	O
optimal	O
structure	O
choice	O
of	O
input	O
variable	O
for	O
each	O
split	O
as	O
well	O
as	O
the	O
corresponding	O
thresh	O
exercise	O
tree-based	O
models	O
olds	O
to	O
minimize	O
the	O
sum-of-squares	B
error	B
is	O
usually	O
computationally	O
infeasible	O
due	O
to	O
the	O
combinatorially	O
large	O
number	O
of	O
possible	O
solutions	O
instead	O
a	O
greedy	O
optimization	O
is	O
generally	O
done	O
by	O
starting	O
with	O
a	O
single	O
root	B
node	B
corresponding	O
to	O
the	O
whole	O
input	O
space	O
and	O
then	O
growing	O
the	O
tree	B
by	O
adding	O
nodes	O
one	O
at	O
a	O
time	O
at	O
each	O
step	O
there	O
will	O
be	O
some	O
number	O
of	O
candidate	O
regions	O
in	O
input	O
space	O
that	O
can	O
be	O
split	O
corresponding	O
to	O
the	O
addition	O
of	O
a	O
pair	O
of	O
leaf	O
nodes	O
to	O
the	O
existing	O
tree	B
for	O
each	O
of	O
these	O
there	O
is	O
a	O
choice	O
of	O
which	O
of	O
the	O
d	O
input	O
variables	O
to	O
split	O
as	O
well	O
as	O
the	O
value	O
of	O
the	O
threshold	O
the	O
joint	O
optimization	O
of	O
the	O
choice	O
of	O
region	O
to	O
split	O
and	O
the	O
choice	O
of	O
input	O
variable	O
and	O
threshold	O
can	O
be	O
done	O
efficiently	O
by	O
exhaustive	O
search	O
noting	O
that	O
for	O
a	O
given	O
choice	O
of	O
split	O
variable	O
and	O
threshold	O
the	O
optimal	O
choice	O
of	O
predictive	O
variable	O
is	O
given	O
by	O
the	O
local	B
average	O
of	O
the	O
data	O
as	O
noted	O
earlier	O
this	O
is	O
repeated	O
for	O
all	O
possible	O
choices	O
of	O
variable	O
to	O
be	O
split	O
and	O
the	O
one	O
that	O
gives	O
the	O
smallest	O
residual	O
sum-of-squares	B
error	B
is	O
retained	O
given	O
a	O
greedy	O
strategy	O
for	O
growing	O
the	O
tree	B
there	O
remains	O
the	O
issue	O
of	O
when	O
to	O
stop	O
adding	O
nodes	O
a	O
simple	O
approach	O
would	O
be	O
to	O
stop	O
when	O
the	O
reduction	O
in	O
residual	O
error	B
falls	O
below	O
some	O
threshold	O
however	O
it	O
is	O
found	O
empirically	O
that	O
often	O
none	O
of	O
the	O
available	O
splits	O
produces	O
a	O
significant	O
reduction	O
in	O
error	B
and	O
yet	O
after	O
several	O
more	O
splits	O
a	O
substantial	O
error	B
reduction	O
is	O
found	O
for	O
this	O
reason	O
it	O
is	O
common	O
practice	O
to	O
grow	O
a	O
large	O
tree	B
using	O
a	O
stopping	O
criterion	O
based	O
on	O
the	O
number	O
of	O
data	O
points	O
associated	O
with	O
the	O
leaf	O
nodes	O
and	O
then	O
prune	O
back	O
the	O
resulting	O
tree	B
the	O
pruning	O
is	O
based	O
on	O
a	O
criterion	O
that	O
balances	O
residual	O
error	B
against	O
a	O
measure	O
of	O
model	O
complexity	O
if	O
we	O
denote	O
the	O
starting	O
tree	B
for	O
pruning	O
by	O
then	O
we	O
define	O
t	O
to	O
be	O
a	O
subtree	O
of	O
if	O
it	O
can	O
be	O
obtained	O
by	O
pruning	O
nodes	O
from	O
other	O
words	O
by	O
collapsing	O
internal	O
nodes	O
by	O
combining	O
the	O
corresponding	O
regions	O
suppose	O
the	O
leaf	O
nodes	O
are	O
indexed	O
by	O
with	O
leaf	O
node	B
representing	O
a	O
region	O
r	O
of	O
input	O
space	O
having	O
n	O
data	O
points	O
and	O
denoting	O
the	O
total	O
number	O
of	O
leaf	O
nodes	O
the	O
optimal	O
prediction	O
for	O
region	O
r	O
is	O
then	O
given	O
by	O
y	O
tn	O
and	O
the	O
corresponding	O
contribution	O
to	O
the	O
residual	O
sum-of-squares	O
is	O
then	O
q	O
y	O
xn	O
r	O
n	O
xn	O
r	O
the	O
pruning	O
criterion	O
is	O
then	O
given	O
by	O
ct	O
q	O
the	O
regularization	B
parameter	O
determines	O
the	O
trade-off	O
between	O
the	O
overall	O
residual	O
sum-of-squares	B
error	B
and	O
the	O
complexity	O
of	O
the	O
model	O
as	O
measured	O
by	O
the	O
number	O
of	O
leaf	O
nodes	O
and	O
its	O
value	O
is	O
chosen	O
by	O
cross-validation	B
for	O
classification	B
problems	O
the	O
process	O
of	O
growing	O
and	O
pruning	O
the	O
tree	B
is	O
similar	O
except	O
that	O
the	O
sum-of-squares	B
error	B
is	O
replaced	O
by	O
a	O
more	O
appropriate	O
measure	O
combining	B
models	I
of	O
performance	O
if	O
we	O
define	O
p	O
k	O
to	O
be	O
the	O
proportion	O
of	O
data	O
points	O
in	O
region	O
r	O
assigned	O
to	O
class	O
k	O
where	O
k	O
k	O
then	O
two	O
commonly	O
used	O
choices	O
are	O
the	O
cross-entropy	O
q	O
p	O
k	O
ln	O
p	O
k	O
and	O
the	O
gini	B
index	I
q	O
p	O
k	O
p	O
k	O
exercise	O
these	O
both	O
vanish	O
for	O
p	O
k	O
and	O
p	O
k	O
and	O
have	O
a	O
maximum	O
at	O
p	O
k	O
they	O
encourage	O
the	O
formation	O
of	O
regions	O
in	O
which	O
a	O
high	O
proportion	O
of	O
the	O
data	O
points	O
are	O
assigned	O
to	O
one	O
class	O
the	O
cross	O
entropy	B
and	O
the	O
gini	B
index	I
are	O
better	O
measures	O
than	O
the	O
misclassification	O
rate	O
for	O
growing	O
the	O
tree	B
because	O
they	O
are	O
more	O
sensitive	O
to	O
the	O
node	B
probabilities	O
also	O
unlike	O
misclassification	O
rate	O
they	O
are	O
differentiable	O
and	O
hence	O
better	O
suited	O
to	O
gradient	O
based	O
optimization	O
methods	O
for	O
subsequent	O
pruning	O
of	O
the	O
tree	B
the	O
misclassification	O
rate	O
is	O
generally	O
used	O
the	O
human	O
interpretability	O
of	O
a	O
tree	B
model	O
such	O
as	O
cart	O
is	O
often	O
seen	O
as	O
its	O
major	O
strength	O
however	O
in	O
practice	O
it	O
is	O
found	O
that	O
the	O
particular	O
tree	B
structure	O
that	O
is	O
learned	O
is	O
very	O
sensitive	O
to	O
the	O
details	O
of	O
the	O
data	O
set	O
so	O
that	O
a	O
small	O
change	O
to	O
the	O
training	B
data	O
can	O
result	O
in	O
a	O
very	O
different	O
set	O
of	O
splits	O
et	O
al	O
there	O
are	O
other	O
problems	O
with	O
tree-based	O
methods	O
of	O
the	O
kind	O
considered	O
in	O
this	O
section	O
one	O
is	O
that	O
the	O
splits	O
are	O
aligned	O
with	O
the	O
axes	O
of	O
the	O
feature	B
space	I
which	O
may	O
be	O
very	O
suboptimal	O
for	O
instance	O
to	O
separate	O
two	O
classes	O
whose	O
optimal	O
decision	B
boundary	I
runs	O
at	O
degrees	O
to	O
the	O
axes	O
would	O
need	O
a	O
large	O
number	O
of	O
axis-parallel	O
splits	O
of	O
the	O
input	O
space	O
as	O
compared	O
to	O
a	O
single	O
non-axis-aligned	O
split	O
furthermore	O
the	O
splits	O
in	O
a	O
decision	B
tree	B
are	O
hard	O
so	O
that	O
each	O
region	O
of	O
input	O
space	O
is	O
associated	O
with	O
one	O
and	O
only	O
one	O
leaf	O
node	B
model	O
the	O
last	O
issue	O
is	O
particularly	O
problematic	O
in	O
regression	B
where	O
we	O
are	O
typically	O
aiming	O
to	O
model	O
smooth	O
functions	O
and	O
yet	O
the	O
tree	B
model	O
produces	O
piecewise-constant	O
predictions	O
with	O
discontinuities	O
at	O
the	O
split	O
boundaries	O
conditional	B
mixture	B
models	O
we	O
have	O
seen	O
that	O
standard	O
decision	O
trees	O
are	O
restricted	O
by	O
hard	O
axis-aligned	O
splits	O
of	O
the	O
input	O
space	O
these	O
constraints	O
can	O
be	O
relaxed	O
at	O
the	O
expense	O
of	O
interpretability	O
by	O
allowing	O
soft	B
probabilistic	O
splits	O
that	O
can	O
be	O
functions	O
of	O
all	O
of	O
the	O
input	O
variables	O
not	O
just	O
one	O
of	O
them	O
at	O
a	O
time	O
if	O
we	O
also	O
give	O
the	O
leaf	O
models	O
a	O
probabilistic	O
interpretation	O
we	O
arrive	O
at	O
a	O
fully	O
probabilistic	O
tree-based	O
model	O
called	O
the	O
hierarchical	B
mixture	B
of	I
experts	I
which	O
we	O
consider	O
in	O
section	O
an	O
alternative	O
way	O
to	O
motivate	O
the	O
hierarchical	B
mixture	B
of	I
experts	I
model	O
is	O
to	O
start	O
with	O
a	O
standard	O
probabilistic	O
mixtures	O
of	O
unconditional	O
density	B
models	O
such	O
as	O
gaussians	O
and	O
replace	O
the	O
component	O
densities	O
with	O
conditional	B
distributions	O
here	O
we	O
consider	O
mixtures	O
of	O
linear	B
regression	B
models	O
and	O
mixtures	O
of	O
chapter	O
conditional	B
mixture	B
models	O
logistic	B
regression	B
models	O
in	O
the	O
simplest	O
case	O
the	O
mixing	O
coefficients	O
are	O
independent	B
of	O
the	O
input	O
variables	O
if	O
we	O
make	O
a	O
further	O
generalization	B
to	O
allow	O
the	O
mixing	O
coefficients	O
also	O
to	O
depend	O
on	O
the	O
inputs	O
then	O
we	O
obtain	O
a	O
mixture	B
of	I
experts	I
model	O
finally	O
if	O
we	O
allow	O
each	O
component	O
in	O
the	O
mixture	B
model	I
to	O
be	O
itself	O
a	O
mixture	B
of	I
experts	I
model	O
then	O
we	O
obtain	O
a	O
hierarchical	B
mixture	B
of	I
experts	I
mixtures	O
of	O
linear	B
regression	B
models	O
one	O
of	O
the	O
many	O
advantages	O
of	O
giving	O
a	O
probabilistic	O
interpretation	O
to	O
the	O
linear	B
regression	B
model	O
is	O
that	O
it	O
can	O
then	O
be	O
used	O
as	O
a	O
component	O
in	O
more	O
complex	O
probabilistic	O
models	O
this	O
can	O
be	O
done	O
for	O
instance	O
by	O
viewing	O
the	O
conditional	B
distribution	O
representing	O
the	O
linear	B
regression	B
model	O
as	O
a	O
node	B
in	O
a	O
directed	B
probabilistic	O
graph	O
here	O
we	O
consider	O
a	O
simple	O
example	O
corresponding	O
to	O
a	O
mixture	B
of	O
linear	B
regression	B
models	O
which	O
represents	O
a	O
straightforward	O
extension	O
of	O
the	O
gaussian	B
mixture	B
model	I
discussed	O
in	O
section	O
to	O
the	O
case	O
of	O
conditional	B
gaussian	B
distributions	O
we	O
therefore	O
consider	O
k	O
linear	B
regression	B
models	O
each	O
governed	O
by	O
its	O
own	O
weight	B
parameter	I
wk	O
in	O
many	O
applications	O
it	O
will	O
be	O
appropriate	O
to	O
use	O
a	O
common	O
noise	O
variance	B
governed	O
by	O
a	O
precision	B
parameter	I
for	O
all	O
k	O
components	O
and	O
this	O
is	O
the	O
case	O
we	O
consider	O
here	O
we	O
will	O
once	O
again	O
restrict	O
attention	O
to	O
a	O
single	O
target	O
variable	O
t	O
though	O
the	O
extension	O
to	O
multiple	O
outputs	O
is	O
straightforward	O
if	O
we	O
denote	O
the	O
mixing	O
coefficients	O
by	O
k	O
then	O
the	O
mixture	B
distribution	I
can	O
be	O
written	O
pt	O
kn	O
k	O
where	O
denotes	O
the	O
set	O
of	O
all	O
adaptive	O
parameters	O
in	O
the	O
model	O
namely	O
w	O
k	O
and	O
the	O
log	O
likelihood	B
function	I
for	O
this	O
model	O
given	O
a	O
data	O
set	O
of	O
observations	O
n	O
tn	O
then	O
takes	O
the	O
form	O
ln	O
pt	O
ln	O
kn	O
k	O
n	O
where	O
t	O
tnt	O
denotes	O
the	O
vector	O
of	O
target	O
variables	O
in	O
order	O
to	O
maximize	O
this	O
likelihood	B
function	I
we	O
can	O
once	O
again	O
appeal	O
to	O
the	O
em	B
algorithm	I
which	O
will	O
turn	O
out	O
to	O
be	O
a	O
simple	O
extension	O
of	O
the	O
em	B
algorithm	I
for	O
unconditional	O
gaussian	B
mixtures	O
of	O
section	O
we	O
can	O
therefore	O
build	O
on	O
our	O
experience	O
with	O
the	O
unconditional	O
mixture	B
and	O
introduce	O
a	O
set	O
z	O
of	O
binary	O
latent	O
variables	O
where	O
znk	O
in	O
which	O
for	O
each	O
data	O
point	O
n	O
all	O
of	O
the	O
elements	O
k	O
k	O
are	O
zero	O
except	O
for	O
a	O
single	O
value	O
of	O
indicating	O
which	O
component	O
of	O
the	O
mixture	B
was	O
responsible	O
for	O
generating	O
that	O
data	O
point	O
the	O
joint	O
distribution	O
over	O
latent	O
and	O
observed	O
variables	O
can	O
be	O
represented	O
by	O
the	O
graphical	B
model	I
shown	O
in	O
figure	O
the	O
complete-data	O
log	O
likelihood	B
function	I
then	O
takes	O
the	O
form	O
ln	O
pt	O
z	O
znk	O
ln	O
kn	O
k	O
n	O
exercise	O
exercise	O
combining	B
models	I
figure	O
probabilistic	O
directed	B
graph	O
representing	O
a	O
mixture	B
of	O
linear	B
regression	B
models	O
defined	O
by	O
w	O
zn	O
n	O
tn	O
n	O
the	O
em	B
algorithm	I
begins	O
by	O
first	O
choosing	O
an	O
initial	O
value	O
old	O
for	O
the	O
model	O
parameters	O
in	O
the	O
e	O
step	O
these	O
parameter	O
values	O
are	O
then	O
used	O
to	O
evaluate	O
the	O
posterior	O
probabilities	O
or	O
responsibilities	O
of	O
each	O
component	O
k	O
for	O
every	O
data	O
point	O
n	O
given	O
by	O
nk	O
eznk	O
pk	O
n	O
old	O
kn	O
j	O
jn	O
k	O
n	O
j	O
n	O
exercise	O
the	O
responsibilities	O
are	O
then	O
used	O
to	O
determine	O
the	O
expectation	B
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
pzt	O
old	O
of	O
the	O
complete-data	O
log	O
likelihood	O
which	O
takes	O
the	O
form	O
q	O
old	O
ez	O
pt	O
z	O
in	O
the	O
m	O
step	O
we	O
maximize	O
the	O
function	O
q	O
old	O
with	O
respect	O
to	O
keeping	O
the	O
nk	O
fixed	O
for	O
the	O
optimization	O
with	O
respect	O
to	O
the	O
mixing	O
coefficients	O
k	O
we	O
need	O
k	O
k	O
which	O
can	O
be	O
done	O
with	O
the	O
aid	O
of	O
a	O
to	O
take	O
account	O
of	O
the	O
constraint	O
lagrange	B
multiplier	I
leading	O
to	O
an	O
m-step	O
re-estimation	O
equation	O
for	O
k	O
in	O
the	O
form	O
ln	O
k	O
lnn	O
k	O
n	O
nk	O
k	O
n	O
nk	O
note	O
that	O
this	O
has	O
exactly	O
the	O
same	O
form	O
as	O
the	O
corresponding	O
result	O
for	O
a	O
simple	O
mixture	B
of	O
unconditional	O
gaussians	O
given	O
by	O
next	O
consider	O
the	O
maximization	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
wk	O
of	O
the	O
kth	O
linear	B
regression	B
model	O
substituting	O
for	O
the	O
gaussian	B
distribution	O
we	O
see	O
that	O
the	O
function	O
q	O
old	O
as	O
a	O
function	O
of	O
the	O
parameter	O
vector	O
wk	O
takes	O
the	O
form	O
nk	O
q	O
old	O
tn	O
wt	O
k	O
n	O
const	O
where	O
the	O
constant	O
term	O
includes	O
the	O
contributions	O
from	O
other	O
weight	O
vectors	O
wj	O
for	O
j	O
k	O
note	O
that	O
the	O
quantity	O
we	O
are	O
maximizing	O
is	O
similar	O
to	O
the	O
of	O
the	O
standard	O
sum-of-squares	B
error	B
for	O
a	O
single	O
linear	B
regression	B
model	O
but	O
with	O
the	O
inclusion	O
of	O
the	O
responsibilities	O
nk	O
this	O
represents	O
a	O
weighted	B
least	I
squares	I
conditional	B
mixture	B
models	O
problem	O
in	O
which	O
the	O
term	O
corresponding	O
to	O
the	O
nth	O
data	O
point	O
carries	O
a	O
weighting	O
coefficient	O
given	O
by	O
nk	O
which	O
could	O
be	O
interpreted	O
as	O
an	O
effective	O
precision	O
for	O
each	O
data	O
point	O
we	O
see	O
that	O
each	O
component	O
linear	B
regression	B
model	O
in	O
the	O
mixture	B
governed	O
by	O
its	O
own	O
parameter	O
vector	O
wk	O
is	O
fitted	O
separately	O
to	O
the	O
whole	O
data	O
set	O
in	O
the	O
m	O
step	O
but	O
with	O
each	O
data	O
point	O
n	O
weighted	O
by	O
the	O
responsibility	B
nk	O
that	O
model	O
k	O
takes	O
for	O
that	O
data	O
point	O
setting	O
the	O
derivative	B
of	O
with	O
respect	O
to	O
wk	O
equal	O
to	O
zero	O
gives	O
nk	O
tn	O
wt	O
k	O
n	O
n	O
which	O
we	O
can	O
write	O
in	O
matrix	O
notation	O
as	O
where	O
rk	O
diag	O
nk	O
is	O
a	O
diagonal	B
matrix	O
of	O
size	O
n	O
n	O
solving	O
for	O
wk	O
we	O
obtain	O
trkt	O
wk	O
this	O
represents	O
a	O
set	O
of	O
modified	O
normal	B
equations	I
corresponding	O
to	O
the	O
weighted	B
least	I
squares	I
problem	O
of	O
the	O
same	O
form	O
as	O
found	O
in	O
the	O
context	O
of	O
logistic	B
regression	B
note	O
that	O
after	O
each	O
e	O
step	O
the	O
matrix	O
rk	O
will	O
change	O
and	O
so	O
we	O
will	O
have	O
to	O
solve	O
the	O
normal	B
equations	I
afresh	O
in	O
the	O
subsequent	O
m	O
step	O
finally	O
we	O
maximize	O
q	O
old	O
with	O
respect	O
to	O
keeping	O
only	O
terms	O
that	O
trkt	O
wk	O
trk	O
depend	O
on	O
the	O
function	O
q	O
old	O
can	O
be	O
written	O
q	O
old	O
ln	O
tn	O
wt	O
k	O
n	O
nk	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
equal	O
to	O
zero	O
and	O
rearranging	O
we	O
obtain	O
the	O
m-step	O
equation	O
for	O
in	O
the	O
form	O
n	O
nk	O
tn	O
wt	O
k	O
n	O
in	O
figure	O
we	O
illustrate	O
this	O
em	B
algorithm	I
using	O
the	O
simple	O
example	O
of	O
fitting	O
a	O
mixture	B
of	O
two	O
straight	O
lines	O
to	O
a	O
data	O
set	O
having	O
one	O
input	O
variable	O
x	O
and	O
one	O
target	O
variable	O
t	O
the	O
predictive	O
density	B
is	O
plotted	O
in	O
figure	O
using	O
the	O
converged	O
parameter	O
values	O
obtained	O
from	O
the	O
em	B
algorithm	I
corresponding	O
to	O
the	O
right-hand	O
plot	O
in	O
figure	O
also	O
shown	O
in	O
this	O
figure	O
is	O
the	O
result	O
of	O
fitting	O
a	O
single	O
linear	B
regression	B
model	O
which	O
gives	O
a	O
unimodal	O
predictive	O
density	B
we	O
see	O
that	O
the	O
mixture	B
model	I
gives	O
a	O
much	O
better	O
representation	O
of	O
the	O
data	O
distribution	O
and	O
this	O
is	O
reflected	O
in	O
the	O
higher	O
likelihood	O
value	O
however	O
the	O
mixture	B
model	I
also	O
assigns	O
significant	O
probability	B
mass	O
to	O
regions	O
where	O
there	O
is	O
no	O
data	O
because	O
its	O
predictive	B
distribution	I
is	O
bimodal	O
for	O
all	O
values	O
of	O
x	O
this	O
problem	O
can	O
be	O
resolved	O
by	O
extending	O
the	O
model	O
to	O
allow	O
the	O
mixture	B
coefficients	O
themselves	O
to	O
be	O
functions	O
of	O
x	O
leading	O
to	O
models	O
such	O
as	O
the	O
mixture	B
density	B
networks	O
discussed	O
in	O
section	O
and	O
hierarchical	B
mixture	B
of	I
experts	I
discussed	O
in	O
section	O
combining	B
models	I
figure	O
example	O
of	O
a	O
synthetic	O
data	O
set	O
shown	O
by	O
the	O
green	O
points	O
having	O
one	O
input	O
variable	O
x	O
and	O
one	O
target	O
variable	O
t	O
together	O
with	O
a	O
mixture	B
of	O
two	O
linear	B
regression	B
models	O
whose	O
mean	B
functions	O
yx	O
wk	O
where	O
k	O
are	O
shown	O
by	O
the	O
blue	O
and	O
red	O
lines	O
the	O
upper	O
three	O
plots	O
show	O
the	O
initial	O
configuration	O
the	O
result	O
of	O
running	O
iterations	O
of	O
em	B
and	O
the	O
result	O
after	O
iterations	O
of	O
em	B
here	O
was	O
initialized	O
to	O
the	O
reciprocal	O
of	O
the	O
true	O
variance	B
of	O
the	O
set	O
of	O
target	O
values	O
the	O
lower	O
three	O
plots	O
show	O
the	O
corresponding	O
responsibilities	O
plotted	O
as	O
a	O
vertical	O
line	O
for	O
each	O
data	O
point	O
in	O
which	O
the	O
length	O
of	O
the	O
blue	O
segment	O
gives	O
the	O
posterior	B
probability	B
of	O
the	O
blue	O
line	O
for	O
that	O
data	O
point	O
similarly	O
for	O
the	O
red	O
segment	O
mixtures	O
of	O
logistic	O
models	O
because	O
the	O
logistic	B
regression	B
model	O
defines	O
a	O
conditional	B
distribution	O
for	O
the	O
target	O
variable	O
given	O
the	O
input	O
vector	O
it	O
is	O
straightforward	O
to	O
use	O
it	O
as	O
the	O
component	O
distribution	O
in	O
a	O
mixture	B
model	I
thereby	O
giving	O
rise	O
to	O
a	O
richer	O
family	O
of	O
conditional	B
distributions	O
compared	O
to	O
a	O
single	O
logistic	B
regression	B
model	O
this	O
example	O
involves	O
a	O
straightforward	O
combination	O
of	O
ideas	O
encountered	O
in	O
earlier	O
sections	O
of	O
the	O
book	O
and	O
will	O
help	O
consolidate	O
these	O
for	O
the	O
reader	O
the	O
conditional	B
distribution	O
of	O
the	O
target	O
variable	O
for	O
a	O
probabilistic	O
mixture	B
of	O
k	O
logistic	B
regression	B
models	O
is	O
given	O
by	O
pt	O
kyt	O
k	O
t	O
where	O
is	O
the	O
feature	O
vector	O
yk	O
denotes	O
the	O
adjustable	O
parameters	O
namely	O
k	O
and	O
is	O
the	O
output	O
of	O
component	O
k	O
and	O
now	O
suppose	O
we	O
are	O
given	O
a	O
data	O
set	O
n	O
tn	O
the	O
corresponding	O
likelihood	O
k	O
wt	O
conditional	B
mixture	B
models	O
figure	O
the	O
left	O
plot	O
shows	O
the	O
predictive	O
conditional	B
density	B
corresponding	O
to	O
the	O
converged	O
solution	O
in	O
figure	O
this	O
gives	O
a	O
log	O
likelihood	O
value	O
of	O
a	O
vertical	O
slice	O
through	O
one	O
of	O
these	O
plots	O
at	O
a	O
particular	O
value	O
of	O
x	O
represents	O
the	O
corresponding	O
conditional	B
distribution	O
ptx	O
which	O
we	O
see	O
is	O
bimodal	O
the	O
plot	O
on	O
the	O
right	O
shows	O
the	O
predictive	O
density	B
for	O
a	O
single	O
linear	B
regression	B
model	O
fitted	O
to	O
the	O
same	O
data	O
set	O
using	O
maximum	B
likelihood	I
this	O
model	O
has	O
a	O
smaller	O
log	O
likelihood	O
of	O
function	O
is	O
then	O
given	O
by	O
pt	O
kytn	O
nk	O
tn	O
where	O
ynk	O
k	O
n	O
and	O
t	O
tnt	O
we	O
can	O
maximize	O
this	O
likelihood	B
function	I
iteratively	O
by	O
making	O
use	O
of	O
the	O
em	B
algorithm	I
this	O
involves	O
introducing	O
latent	O
variables	O
znk	O
that	O
correspond	O
to	O
a	O
coded	O
binary	O
indicator	O
variable	O
for	O
each	O
data	O
point	O
n	O
the	O
complete-data	O
likelihood	B
function	I
is	O
then	O
given	O
by	O
pt	O
z	O
kytn	O
nk	O
tn	O
where	O
z	O
is	O
the	O
matrix	O
of	O
latent	O
variables	O
with	O
elements	O
znk	O
we	O
initialize	O
the	O
em	B
algorithm	I
by	O
choosing	O
an	O
initial	O
value	O
old	O
for	O
the	O
model	O
parameters	O
in	O
the	O
e	O
step	O
we	O
then	O
use	O
these	O
parameter	O
values	O
to	O
evaluate	O
the	O
posterior	O
probabilities	O
of	O
the	O
components	O
k	O
for	O
each	O
data	O
point	O
n	O
which	O
are	O
given	O
by	O
nk	O
eznk	O
pk	O
n	O
old	O
kytn	O
j	O
jytn	O
nk	O
tn	O
nj	O
tn	O
these	O
responsibilities	O
are	O
then	O
used	O
to	O
find	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
as	O
a	O
function	O
of	O
given	O
by	O
q	O
old	O
ez	O
pt	O
z	O
nk	O
k	O
tn	O
ln	O
ynk	O
tn	O
ln	O
ynk	O
combining	B
models	I
n	O
the	O
m	O
step	O
involves	O
maximization	O
of	O
this	O
function	O
with	O
respect	O
to	O
keeping	O
old	O
and	O
hence	O
nk	O
fixed	O
maximization	O
with	O
respect	O
to	O
k	O
can	O
be	O
done	O
in	O
the	O
usual	O
way	O
k	O
k	O
giving	O
with	O
a	O
lagrange	B
multiplier	I
to	O
enforce	O
the	O
summation	O
constraint	O
the	O
familiar	O
result	O
k	O
nk	O
to	O
determine	O
the	O
we	O
note	O
that	O
the	O
q	O
old	O
function	O
comprises	O
a	O
sum	O
over	O
terms	O
indexed	O
by	O
k	O
each	O
of	O
which	O
depends	O
only	O
on	O
one	O
of	O
the	O
vectors	O
wk	O
so	O
that	O
the	O
different	O
vectors	O
are	O
decoupled	O
in	O
the	O
m	O
step	O
of	O
the	O
em	B
algorithm	I
in	O
other	O
words	O
the	O
different	O
components	O
interact	O
only	O
via	O
the	O
responsibilities	O
which	O
are	O
fixed	O
during	O
the	O
m	O
step	O
note	O
that	O
the	O
m	O
step	O
does	O
not	O
have	O
a	O
closed-form	O
solution	O
and	O
must	O
be	O
solved	O
iteratively	O
using	O
for	O
instance	O
the	O
iterative	B
reweighted	I
least	I
squares	I
algorithm	O
the	O
gradient	O
and	O
the	O
hessian	O
for	O
the	O
vector	O
wk	O
are	O
given	O
by	O
kq	O
nktn	O
ynk	O
n	O
hk	O
k	O
kq	O
ynk	O
n	O
t	O
n	O
section	O
where	O
k	O
denotes	O
the	O
gradient	O
with	O
respect	O
to	O
wk	O
for	O
fixed	O
nk	O
these	O
are	O
independent	B
of	O
for	O
j	O
k	O
and	O
so	O
we	O
can	O
solve	O
for	O
each	O
wk	O
separately	O
using	O
the	O
irls	O
algorithm	O
thus	O
the	O
m-step	O
equations	O
for	O
component	O
k	O
correspond	O
simply	O
to	O
fitting	O
a	O
single	O
logistic	B
regression	B
model	O
to	O
a	O
weighted	O
data	O
set	O
in	O
which	O
data	O
point	O
n	O
carries	O
a	O
weight	O
nk	O
figure	O
shows	O
an	O
example	O
of	O
the	O
mixture	B
of	O
logistic	B
regression	B
models	O
applied	O
to	O
a	O
simple	O
classification	B
problem	O
the	O
extension	O
of	O
this	O
model	O
to	O
a	O
mixture	B
of	O
softmax	O
models	O
for	O
more	O
than	O
two	O
classes	O
is	O
straightforward	O
section	O
exercise	O
mixtures	O
of	O
experts	O
in	O
section	O
we	O
considered	O
a	O
mixture	B
of	O
linear	B
regression	B
models	O
and	O
in	O
section	O
we	O
discussed	O
the	O
analogous	O
mixture	B
of	O
linear	O
classifiers	O
although	O
these	O
simple	O
mixtures	O
extend	O
the	O
flexibility	O
of	O
linear	O
models	O
to	O
include	O
more	O
complex	O
multimodal	O
predictive	O
distributions	O
they	O
are	O
still	O
very	O
limited	O
we	O
can	O
further	O
increase	O
the	O
capability	O
of	O
such	O
models	O
by	O
allowing	O
the	O
mixing	O
coefficients	O
themselves	O
to	O
be	O
functions	O
of	O
the	O
input	O
variable	O
so	O
that	O
ptx	O
kxpktx	O
this	O
is	O
known	O
as	O
a	O
mixture	B
of	I
experts	I
model	O
et	O
al	O
in	O
which	O
the	O
mixing	O
coefficients	O
kx	O
are	O
known	O
as	O
gating	O
functions	O
and	O
the	O
individual	O
component	O
densities	O
pktx	O
are	O
called	O
experts	O
the	O
notion	O
behind	O
the	O
terminology	O
is	O
that	O
different	O
components	O
can	O
model	O
the	O
distribution	O
in	O
different	O
regions	O
of	O
input	O
space	O
conditional	B
mixture	B
models	O
figure	O
illustration	O
of	O
a	O
mixture	B
of	O
logistic	B
regression	B
models	O
the	O
left	O
plot	O
shows	O
data	O
points	O
drawn	O
from	O
two	O
classes	O
denoted	O
red	O
and	O
blue	O
in	O
which	O
the	O
background	O
colour	O
varies	O
from	O
pure	O
red	O
to	O
pure	O
blue	O
denotes	O
the	O
true	O
probability	B
of	O
the	O
class	O
label	O
the	O
centre	O
plot	O
shows	O
the	O
result	O
of	O
fitting	O
a	O
single	O
logistic	B
regression	B
model	O
using	O
maximum	B
likelihood	I
in	O
which	O
the	O
background	O
colour	O
denotes	O
the	O
corresponding	O
probability	B
of	O
the	O
class	O
label	O
because	O
the	O
colour	O
is	O
a	O
near-uniform	O
purple	O
we	O
see	O
that	O
the	O
model	O
assigns	O
a	O
probability	B
of	O
around	O
to	O
each	O
of	O
the	O
classes	O
over	O
most	O
of	O
input	O
space	O
the	O
right	O
plot	O
shows	O
the	O
result	O
of	O
fitting	O
a	O
mixture	B
of	O
two	O
logistic	B
regression	B
models	O
which	O
now	O
gives	O
much	O
higher	O
probability	B
to	O
the	O
correct	O
labels	O
for	O
many	O
of	O
the	O
points	O
in	O
the	O
blue	O
class	O
are	O
experts	O
at	O
making	O
predictions	O
in	O
their	O
own	O
regions	O
and	O
the	O
gating	O
functions	O
determine	O
which	O
components	O
are	O
dominant	O
in	O
which	O
region	O
the	O
gating	O
functions	O
kx	O
must	O
satisfy	O
the	O
usual	O
constraints	O
for	O
mixing	O
coefficients	O
namely	O
kx	O
and	O
k	O
kx	O
they	O
can	O
therefore	O
be	O
represented	O
for	O
example	O
by	O
linear	O
softmax	O
models	O
of	O
the	O
form	O
and	O
if	O
the	O
experts	O
are	O
also	O
linear	O
or	O
classification	B
models	O
then	O
the	O
whole	O
model	O
can	O
be	O
fitted	O
efficiently	O
using	O
the	O
em	B
algorithm	I
with	O
iterative	B
reweighted	I
least	I
squares	I
being	O
employed	O
in	O
the	O
m	O
step	O
and	O
jacobs	O
such	O
a	O
model	O
still	O
has	O
significant	O
limitations	O
due	O
to	O
the	O
use	O
of	O
linear	O
models	O
for	O
the	O
gating	O
and	O
expert	O
functions	O
a	O
much	O
more	O
flexible	O
model	O
is	O
obtained	O
by	O
using	O
a	O
multilevel	O
gating	B
function	I
to	O
give	O
the	O
hierarchical	B
mixture	B
of	I
experts	I
or	O
hme	O
model	O
and	O
jacobs	O
to	O
understand	O
the	O
structure	O
of	O
this	O
model	O
imagine	O
a	O
mixture	B
distribution	I
in	O
which	O
each	O
component	O
in	O
the	O
mixture	B
is	O
itself	O
a	O
mixture	B
distribution	I
for	O
simple	O
unconditional	O
mixtures	O
this	O
hierarchical	B
mixture	B
is	O
trivially	O
equivalent	O
to	O
a	O
single	O
flat	O
mixture	B
distribution	I
however	O
when	O
the	O
mixing	O
coefficients	O
are	O
input	O
dependent	O
this	O
hierarchical	B
model	O
becomes	O
nontrivial	O
the	O
hme	O
model	O
can	O
also	O
be	O
viewed	O
as	O
a	O
probabilistic	O
version	O
of	O
decision	O
trees	O
discussed	O
in	O
section	O
and	O
can	O
again	O
be	O
trained	O
efficiently	O
by	O
maximum	B
likelihood	I
using	O
an	O
em	B
algorithm	I
with	O
irls	O
in	O
the	O
m	O
step	O
a	O
bayesian	B
treatment	O
of	O
the	O
hme	O
has	O
been	O
given	O
by	O
bishop	O
and	O
svens	O
en	O
based	O
on	O
variational	B
inference	B
we	O
shall	O
not	O
discuss	O
the	O
hme	O
in	O
detail	O
here	O
however	O
it	O
is	O
worth	O
pointing	O
out	O
the	O
close	O
connection	O
with	O
the	O
mixture	B
density	B
network	I
discussed	O
in	O
section	O
the	O
principal	O
advantage	O
of	O
the	O
mixtures	O
of	O
experts	O
model	O
is	O
that	O
it	O
can	O
be	O
optimized	O
by	O
em	B
in	O
which	O
the	O
m	O
step	O
for	O
each	O
mixture	B
component	I
and	O
gating	O
model	O
involves	O
a	O
convex	O
optimization	O
the	O
overall	O
optimization	O
is	O
nonconvex	O
by	O
contrast	O
the	O
advantage	O
of	O
the	O
mixture	B
density	B
network	I
approach	O
is	O
that	O
the	O
component	O
exercise	O
section	O
combining	B
models	I
exercises	O
densities	O
and	O
the	O
mixing	O
coefficients	O
share	O
the	O
hidden	O
units	O
of	O
the	O
neural	B
network	I
furthermore	O
in	O
the	O
mixture	B
density	B
network	I
the	O
splits	O
of	O
the	O
input	O
space	O
are	O
further	O
relaxed	O
compared	O
to	O
the	O
hierarchical	B
mixture	B
of	I
experts	I
in	O
that	O
they	O
are	O
not	O
only	O
soft	B
and	O
not	O
constrained	O
to	O
be	O
axis	O
aligned	O
but	O
they	O
can	O
also	O
be	O
nonlinear	O
www	O
consider	O
a	O
set	O
models	O
of	O
the	O
form	O
ptx	O
zh	O
h	O
h	O
in	O
which	O
x	O
is	O
the	O
input	O
vector	O
t	O
is	O
the	O
target	B
vector	I
h	O
indexes	O
the	O
different	O
models	O
zh	O
is	O
a	O
latent	B
variable	I
for	O
model	O
h	O
and	O
h	O
is	O
the	O
set	O
of	O
parameters	O
for	O
model	O
h	O
suppose	O
the	O
models	O
have	O
prior	B
probabilities	O
ph	O
and	O
that	O
we	O
are	O
given	O
a	O
training	B
set	I
x	O
xn	O
and	O
t	O
tn	O
write	O
down	O
the	O
formulae	O
needed	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
ptx	O
x	O
t	O
in	O
which	O
the	O
latent	O
variables	O
and	O
the	O
model	O
index	O
are	O
marginalized	O
out	O
use	O
these	O
formulae	O
to	O
highlight	O
the	O
difference	O
between	O
bayesian	B
averaging	O
of	O
different	O
models	O
and	O
the	O
use	O
of	O
latent	O
variables	O
within	O
a	O
single	O
model	O
the	O
expected	O
sum-of-squares	B
error	B
eav	O
for	O
a	O
simple	O
committee	B
model	O
can	O
be	O
defined	O
by	O
and	O
the	O
expected	O
error	B
of	O
the	O
committee	B
itself	O
is	O
given	O
by	O
assuming	O
that	O
the	O
individual	O
errors	O
satisfy	O
and	O
derive	O
the	O
result	O
www	O
by	O
making	O
use	O
of	O
jensen	O
s	O
inequality	O
for	O
the	O
special	O
case	O
of	O
the	O
convex	B
function	I
fx	O
show	O
that	O
the	O
average	O
expected	O
sum-of-squares	B
error	B
eav	O
of	O
the	O
members	O
of	O
a	O
simple	O
committee	B
model	O
given	O
by	O
and	O
the	O
expected	O
error	B
ecom	O
of	O
the	O
committee	B
itself	O
given	O
by	O
satisfy	O
ecom	O
eav	O
by	O
making	O
use	O
of	O
jensen	O
s	O
in	O
equality	O
show	O
that	O
the	O
result	O
derived	O
in	O
the	O
previous	O
exercise	O
hods	O
for	O
any	O
error	B
function	I
ey	O
not	O
just	O
sum-ofsquares	O
provided	O
it	O
is	O
a	O
convex	B
function	I
of	O
y	O
www	O
consider	O
a	O
committee	B
in	O
which	O
we	O
allow	O
unequal	O
weighting	O
of	O
the	O
constituent	O
models	O
so	O
that	O
ycomx	O
mymx	O
in	O
order	O
to	O
ensure	O
that	O
the	O
predictions	O
ycomx	O
remain	O
within	O
sensible	O
limits	O
suppose	O
that	O
we	O
require	O
that	O
they	O
be	O
bounded	O
at	O
each	O
value	O
of	O
x	O
by	O
the	O
minimum	O
and	O
maximum	O
values	O
given	O
by	O
any	O
of	O
the	O
members	O
of	O
the	O
committee	B
so	O
that	O
yminx	O
ycomx	O
ymaxx	O
show	O
that	O
a	O
necessary	O
and	O
sufficient	O
condition	O
for	O
this	O
constraint	O
is	O
that	O
the	O
coefficients	O
m	O
satisfy	O
m	O
m	O
exercises	O
www	O
by	O
differentiating	O
the	O
error	B
function	I
with	O
respect	O
to	O
m	O
show	O
that	O
the	O
parameters	O
m	O
in	O
the	O
adaboost	B
algorithm	O
are	O
updated	O
using	O
in	O
which	O
is	O
defined	O
by	O
by	O
making	O
a	O
variational	B
minimization	O
of	O
the	O
expected	O
exponential	O
error	B
function	I
given	O
by	O
with	O
respect	O
to	O
all	O
possible	O
functions	O
yx	O
show	O
that	O
the	O
minimizing	O
function	O
is	O
given	O
by	O
show	O
that	O
the	O
exponential	O
error	B
function	I
which	O
is	O
minimized	O
by	O
the	O
adaboost	B
algorithm	O
does	O
not	O
correspond	O
to	O
the	O
log	O
likelihood	O
of	O
any	O
well-behaved	O
probabilistic	O
model	O
this	O
can	O
be	O
done	O
by	O
showing	O
that	O
the	O
corresponding	O
conditional	B
distribution	O
ptx	O
cannot	O
be	O
correctly	O
normalized	O
www	O
show	O
that	O
the	O
sequential	O
minimization	O
of	O
the	O
sum-of-squares	B
error	B
function	I
for	O
an	O
additive	O
model	O
of	O
the	O
form	O
in	O
the	O
style	O
of	O
boosting	B
simply	O
involves	O
fitting	O
each	O
new	O
base	O
classifier	O
to	O
the	O
residual	O
errors	O
tn	O
fm	O
from	O
the	O
previous	O
model	O
verify	O
that	O
if	O
we	O
minimize	O
the	O
sum-of-squares	B
error	B
between	O
a	O
set	O
of	O
training	B
values	O
and	O
a	O
single	O
predictive	O
value	O
t	O
then	O
the	O
optimal	O
solution	O
for	O
t	O
is	O
given	O
by	O
the	O
mean	B
of	O
the	O
consider	O
a	O
data	O
set	O
comprising	O
data	O
points	O
from	O
class	O
and	O
data	O
points	O
from	O
class	O
suppose	O
that	O
a	O
tree	B
model	O
a	O
splits	O
these	O
into	O
at	O
the	O
first	O
leaf	O
node	B
and	O
at	O
the	O
second	O
leaf	O
node	B
where	O
m	O
denotes	O
that	O
n	O
points	O
are	O
assigned	O
to	O
and	O
m	O
points	O
are	O
assigned	O
to	O
similarly	O
suppose	O
that	O
a	O
second	O
tree	B
model	O
b	O
splits	O
them	O
into	O
and	O
evaluate	O
the	O
misclassification	O
rates	O
for	O
the	O
two	O
trees	O
and	O
hence	O
show	O
that	O
they	O
are	O
equal	O
similarly	O
evaluate	O
the	O
cross-entropy	O
and	O
gini	B
index	I
for	O
the	O
two	O
trees	O
and	O
show	O
that	O
they	O
are	O
both	O
lower	O
for	O
tree	B
b	O
than	O
for	O
tree	B
a	O
extend	O
the	O
results	O
of	O
section	O
for	O
a	O
mixture	B
of	O
linear	B
regression	B
models	O
to	O
the	O
case	O
of	O
multiple	O
target	O
values	O
described	O
by	O
a	O
vector	O
t	O
to	O
do	O
this	O
make	O
use	O
of	O
the	O
results	O
of	O
section	O
www	O
verify	O
that	O
the	O
complete-data	O
log	O
likelihood	B
function	I
for	O
the	O
mixture	B
of	O
linear	B
regression	B
models	O
is	O
given	O
by	O
use	O
the	O
technique	O
of	O
lagrange	B
multipliers	O
e	O
to	O
show	O
that	O
the	O
m-step	O
re-estimation	O
equation	O
for	O
the	O
mixing	O
coefficients	O
in	O
the	O
mixture	B
of	O
linear	B
regression	B
models	O
trained	O
by	O
maximum	B
likelihood	I
em	B
is	O
given	O
by	O
www	O
we	O
have	O
already	O
noted	O
that	O
if	O
we	O
use	O
a	O
squared	O
loss	B
function	I
in	O
a	O
regression	B
problem	O
the	O
corresponding	O
optimal	O
prediction	O
of	O
the	O
target	O
variable	O
for	O
a	O
new	O
input	O
vector	O
is	O
given	O
by	O
the	O
conditional	B
mean	B
of	O
the	O
predictive	B
distribution	I
show	O
that	O
the	O
conditional	B
mean	B
for	O
the	O
mixture	B
of	O
linear	B
regression	B
models	O
discussed	O
in	O
section	O
is	O
given	O
by	O
a	O
linear	O
combination	O
of	O
the	O
means	O
of	O
each	O
component	O
distribution	O
note	O
that	O
if	O
the	O
conditional	B
distribution	O
of	O
the	O
target	O
data	O
is	O
multimodal	O
the	O
conditional	B
mean	B
can	O
give	O
poor	O
predictions	O
combining	B
models	I
extend	O
the	O
logistic	B
regression	B
mixture	B
model	I
of	O
section	O
to	O
a	O
mixture	B
of	O
softmax	O
classifiers	O
representing	O
c	O
classes	O
write	O
down	O
the	O
em	B
algorithm	I
for	O
determining	O
the	O
parameters	O
of	O
this	O
model	O
through	O
maximum	B
likelihood	I
www	O
consider	O
a	O
mixture	B
model	I
for	O
a	O
conditional	B
distribution	O
ptx	O
of	O
the	O
form	O
ptx	O
k	O
ktx	O
in	O
which	O
each	O
mixture	B
component	I
ktx	O
is	O
itself	O
a	O
mixture	B
model	I
show	O
that	O
this	O
two-level	O
hierarchical	B
mixture	B
is	O
equivalent	O
to	O
a	O
conventional	O
single-level	O
mixture	B
model	I
now	O
suppose	O
that	O
the	O
mixing	O
coefficients	O
in	O
both	O
levels	O
of	O
such	O
a	O
hierarchical	B
model	O
are	O
arbitrary	O
functions	O
of	O
x	O
again	O
show	O
that	O
this	O
hierarchical	B
model	O
is	O
again	O
equivalent	O
to	O
a	O
single-level	O
model	O
with	O
x-dependent	O
mixing	O
coefficients	O
finally	O
consider	O
the	O
case	O
in	O
which	O
the	O
mixing	O
coefficients	O
at	O
both	O
levels	O
of	O
the	O
hierarchical	B
mixture	B
are	O
constrained	O
to	O
be	O
linear	O
classification	B
or	O
softmax	O
models	O
show	O
that	O
the	O
hierarchical	B
mixture	B
cannot	O
in	O
general	O
be	O
represented	O
by	O
a	O
single-level	O
mixture	B
having	O
linear	O
classification	B
models	O
for	O
the	O
mixing	O
coefficients	O
hint	O
to	O
do	O
this	O
it	O
is	O
sufficient	O
to	O
construct	O
a	O
single	O
counter-example	O
so	O
consider	O
a	O
mixture	B
of	O
two	O
components	O
in	O
which	O
one	O
of	O
those	O
components	O
is	O
itself	O
a	O
mixture	B
of	O
two	O
components	O
with	O
mixing	O
coefficients	O
given	O
by	O
linear-logistic	O
models	O
show	O
that	O
this	O
cannot	O
be	O
represented	O
by	O
a	O
single-level	O
mixture	B
of	O
components	O
having	O
mixing	O
coefficients	O
determined	O
by	O
a	O
linear-softmax	O
model	O
appendix	O
a	O
data	O
sets	O
in	O
this	O
appendix	O
we	O
give	O
a	O
brief	O
introduction	O
to	O
the	O
data	O
sets	O
used	O
to	O
illustrate	O
some	O
of	O
the	O
algorithms	O
described	O
in	O
this	O
book	O
detailed	O
information	O
on	O
file	O
formats	O
for	O
these	O
data	O
sets	O
as	O
well	O
as	O
the	O
data	O
files	O
themselves	O
can	O
be	O
obtained	O
from	O
the	O
book	O
web	O
site	O
httpresearch	O
microsoft	O
com	O
cmbishopprml	O
handwritten	O
digits	O
the	O
digits	O
data	O
used	O
in	O
this	O
book	O
is	O
taken	O
from	O
the	O
mnist	B
data	I
set	O
et	O
al	O
which	O
itself	O
was	O
constructed	O
by	O
modifying	O
a	O
subset	O
of	O
the	O
much	O
larger	O
data	O
set	O
produced	O
by	O
nist	O
national	O
institute	O
of	O
standards	O
and	O
technology	O
it	O
comprises	O
a	O
training	B
set	I
of	O
examples	O
and	O
a	O
test	B
set	I
of	O
examples	O
some	O
of	O
the	O
data	O
was	O
collected	O
from	O
census	O
bureau	O
employees	O
and	O
the	O
rest	O
was	O
collected	O
from	O
high-school	O
children	O
and	O
care	O
was	O
taken	O
to	O
ensure	O
that	O
the	O
test	O
examples	O
were	O
written	O
by	O
different	O
individuals	O
to	O
the	O
training	B
examples	O
the	O
original	O
nist	O
data	O
had	O
binary	O
or	O
white	O
pixels	O
to	O
create	O
mnist	O
these	O
images	O
were	O
size	O
normalized	O
to	O
fit	O
in	O
a	O
pixel	O
box	O
while	O
preserving	O
their	O
aspect	O
ratio	O
as	O
a	O
consequence	O
of	O
the	O
anti-aliasing	O
used	O
to	O
change	O
the	O
resolution	O
of	O
the	O
images	O
the	O
resulting	O
mnist	O
digits	O
are	O
grey	O
scale	O
these	O
images	O
were	O
then	O
centred	O
in	O
a	O
box	O
examples	O
of	O
the	O
mnist	O
digits	O
are	O
shown	O
in	O
figure	O
error	B
rates	O
for	O
classifying	O
the	O
digits	O
range	O
from	O
for	O
a	O
simple	O
linear	O
classifier	O
through	O
for	O
a	O
carefully	O
designed	O
support	B
vector	I
machine	I
to	O
for	O
a	O
convolutional	B
neural	B
network	I
et	O
al	O
a	O
data	O
sets	O
figure	O
one	O
hundred	O
examples	O
of	O
the	O
mnist	O
digits	O
chosen	O
at	O
random	O
from	O
the	O
training	B
set	I
oil	O
flow	O
this	O
is	O
a	O
synthetic	O
data	O
set	O
that	O
arose	O
out	O
of	O
a	O
project	O
aimed	O
at	O
measuring	O
noninvasively	O
the	O
proportions	O
of	O
oil	O
water	O
and	O
gas	O
in	O
north	O
sea	O
oil	O
transfer	O
pipelines	O
and	O
james	O
it	O
is	O
based	O
on	O
the	O
principle	O
of	O
dual-energy	B
gamma	B
densitometry	I
the	O
ideas	O
is	O
that	O
if	O
a	O
narrow	O
beam	O
of	O
gamma	O
rays	O
is	O
passed	O
through	O
the	O
pipe	O
the	O
attenuation	O
in	O
the	O
intensity	O
of	O
the	O
beam	O
provides	O
information	O
about	O
the	O
density	B
of	O
material	O
along	O
its	O
path	O
thus	O
for	O
instance	O
the	O
beam	O
will	O
be	O
attenuated	O
more	O
strongly	O
by	O
oil	O
than	O
by	O
gas	O
a	O
single	O
attenuation	O
measurement	O
alone	O
is	O
not	O
sufficient	O
because	O
there	O
are	O
two	O
degrees	B
of	I
freedom	I
corresponding	O
to	O
the	O
fraction	O
of	O
oil	O
and	O
the	O
fraction	O
of	O
water	O
fraction	O
of	O
gas	O
is	O
redundant	O
because	O
the	O
three	O
fractions	O
must	O
add	O
to	O
one	O
to	O
address	O
this	O
two	O
gamma	O
beams	O
of	O
different	O
energies	O
other	O
words	O
different	O
frequencies	O
or	O
wavelengths	O
are	O
passed	O
through	O
the	O
pipe	O
along	O
the	O
same	O
path	O
and	O
the	O
attenuation	O
of	O
each	O
is	O
measured	O
because	O
the	O
absorbtion	O
properties	O
of	O
different	O
materials	O
vary	O
differently	O
as	O
a	O
function	O
of	O
energy	O
measurement	O
of	O
the	O
attenuations	O
at	O
the	O
two	O
energies	O
provides	O
two	O
independent	B
pieces	O
of	O
information	O
given	O
the	O
known	O
absorbtion	O
properties	O
of	O
oil	O
water	O
and	O
gas	O
at	O
the	O
two	O
energies	O
it	O
is	O
then	O
a	O
simple	O
matter	O
to	O
calculate	O
the	O
average	O
fractions	O
of	O
oil	O
and	O
water	O
hence	O
of	O
gas	O
measured	O
along	O
the	O
path	O
of	O
the	O
gamma	O
beams	O
there	O
is	O
a	O
further	O
complication	O
however	O
associated	O
with	O
the	O
motion	O
of	O
the	O
materials	O
along	O
the	O
pipe	O
if	O
the	O
flow	O
velocity	O
is	O
small	O
then	O
the	O
oil	O
floats	O
on	O
top	O
of	O
the	O
water	O
with	O
the	O
gas	O
sitting	O
above	O
the	O
oil	O
this	O
is	O
known	O
as	O
a	O
laminar	O
or	O
stratified	O
figure	O
the	O
three	O
geometrical	O
configurations	O
of	O
the	O
oil	O
water	O
and	O
gas	O
phases	O
used	O
to	O
generate	O
the	O
oilflow	O
data	O
set	O
for	O
each	O
configuration	O
the	O
proportions	O
of	O
the	O
three	O
phases	O
can	O
vary	O
a	O
data	O
sets	O
stratified	O
annular	O
oil	O
water	O
gas	O
mix	O
homogeneous	B
flow	O
configuration	O
and	O
is	O
illustrated	O
in	O
figure	O
as	O
the	O
flow	O
velocity	O
is	O
increased	O
more	O
complex	O
geometrical	O
configurations	O
of	O
the	O
oil	O
water	O
and	O
gas	O
can	O
arise	O
for	O
the	O
purposes	O
of	O
this	O
data	O
set	O
two	O
specific	O
idealizations	O
are	O
considered	O
in	O
the	O
annular	O
configuration	O
the	O
oil	O
water	O
and	O
gas	O
form	O
concentric	O
cylinders	O
with	O
the	O
water	O
around	O
the	O
outside	O
and	O
the	O
gas	O
in	O
the	O
centre	O
whereas	O
in	O
the	O
homogeneous	B
configuration	O
the	O
oil	O
water	O
and	O
gas	O
are	O
assumed	O
to	O
be	O
intimately	O
mixed	O
as	O
might	O
occur	O
at	O
high	O
flow	O
velocities	O
under	O
turbulent	O
conditions	O
these	O
configurations	O
are	O
also	O
illustrated	O
in	O
figure	O
we	O
have	O
seen	O
that	O
a	O
single	O
dual-energy	O
beam	O
gives	O
the	O
oil	O
and	O
water	O
fractions	O
measured	O
along	O
the	O
path	O
length	O
whereas	O
we	O
are	O
interested	O
in	O
the	O
volume	O
fractions	O
of	O
oil	O
and	O
water	O
this	O
can	O
be	O
addressed	O
by	O
using	O
multiple	O
dual-energy	O
gamma	O
densitometers	O
whose	O
beams	O
pass	O
through	O
different	O
regions	O
of	O
the	O
pipe	O
for	O
this	O
particular	O
data	O
set	O
there	O
are	O
six	O
such	O
beams	O
and	O
their	O
spatial	O
arrangement	O
is	O
shown	O
in	O
figure	O
a	O
single	O
observation	O
is	O
therefore	O
represented	O
by	O
a	O
vector	O
comprising	O
the	O
fractions	O
of	O
oil	O
and	O
water	O
measured	O
along	O
the	O
paths	O
of	O
each	O
of	O
the	O
beams	O
we	O
are	O
however	O
interested	O
in	O
obtaining	O
the	O
overall	O
volume	O
fractions	O
of	O
the	O
three	O
phases	O
in	O
the	O
pipe	O
this	O
is	O
much	O
like	O
the	O
classical	B
problem	O
of	O
tomographic	O
reconstruction	O
used	O
in	O
medical	O
imaging	O
for	O
example	O
in	O
which	O
a	O
two-dimensional	O
dis	O
figure	O
cross	O
section	O
of	O
the	O
pipe	O
showing	O
the	O
arrangement	O
of	O
the	O
six	O
beam	O
lines	O
each	O
of	O
which	O
comprises	O
a	O
single	O
dualenergy	O
gamma	O
densitometer	O
note	O
that	O
the	O
vertical	O
beams	O
are	O
asymmetrically	O
arranged	O
relative	B
to	O
the	O
central	O
axis	O
by	O
the	O
dotted	O
line	O
a	O
data	O
sets	O
tribution	O
is	O
to	O
be	O
reconstructed	O
from	O
an	O
number	O
of	O
one-dimensional	O
averages	O
here	O
there	O
are	O
far	O
fewer	O
line	O
measurements	O
than	O
in	O
a	O
typical	O
tomography	B
application	O
on	O
the	O
other	O
hand	O
the	O
range	O
of	O
geometrical	O
configurations	O
is	O
much	O
more	O
limited	O
and	O
so	O
the	O
configuration	O
as	O
well	O
as	O
the	O
phase	O
fractions	O
can	O
be	O
predicted	O
with	O
reasonable	O
accuracy	O
from	O
the	O
densitometer	O
data	O
for	O
safety	O
reasons	O
the	O
intensity	O
of	O
the	O
gamma	O
beams	O
is	O
kept	O
relatively	O
weak	O
and	O
so	O
to	O
obtain	O
an	O
accurate	O
measurement	O
of	O
the	O
attenuation	O
the	O
measured	O
beam	O
intensity	O
is	O
integrated	O
over	O
a	O
specific	O
time	O
interval	O
for	O
a	O
finite	O
integration	O
time	O
there	O
are	O
random	O
fluctuations	O
in	O
the	O
measured	O
intensity	O
due	O
to	O
the	O
fact	O
that	O
the	O
gamma	O
beams	O
comprise	O
discrete	O
packets	O
of	O
energy	O
called	O
photons	O
in	O
practice	O
the	O
integration	O
time	O
is	O
chosen	O
as	O
a	O
compromise	O
between	O
reducing	O
the	O
noise	O
level	O
requires	O
a	O
long	O
integration	O
time	O
and	O
detecting	O
temporal	O
variations	O
in	O
the	O
flow	O
requires	O
a	O
short	O
integration	O
time	O
the	O
oil	O
flow	O
data	O
set	O
is	O
generated	O
using	O
realistic	O
known	O
values	O
for	O
the	O
absorption	O
properties	O
of	O
oil	O
water	O
and	O
gas	O
at	O
the	O
two	O
gamma	O
energies	O
used	O
and	O
with	O
a	O
specific	O
choice	O
of	O
integration	O
time	O
seconds	O
chosen	O
as	O
characteristic	O
of	O
a	O
typical	O
practical	O
setup	O
each	O
point	O
in	O
the	O
data	O
set	O
is	O
generated	O
independently	O
using	O
the	O
following	O
steps	O
choose	O
one	O
of	O
the	O
three	O
phase	O
configurations	O
at	O
random	O
with	O
equal	O
probability	B
choose	O
three	O
random	O
numbers	O
and	O
from	O
the	O
uniform	B
distribution	I
over	O
and	O
define	O
foil	O
fwater	O
this	O
treats	O
the	O
three	O
phases	O
on	O
an	O
equal	O
footing	O
and	O
ensures	O
that	O
the	O
volume	O
fractions	O
add	O
to	O
one	O
for	O
each	O
of	O
the	O
six	O
beam	O
lines	O
calculate	O
the	O
effective	O
path	O
lengths	O
through	O
oil	O
and	O
water	O
for	O
the	O
given	O
phase	O
configuration	O
perturb	O
the	O
path	O
lengths	O
using	O
the	O
poisson	O
distribution	O
based	O
on	O
the	O
known	O
beam	O
intensities	O
and	O
integration	O
time	O
to	O
allow	O
for	O
the	O
effect	O
of	O
photon	O
statistics	O
each	O
point	O
in	O
the	O
data	O
set	O
comprises	O
the	O
path	O
length	O
measurements	O
together	O
with	O
the	O
fractions	O
of	O
oil	O
and	O
water	O
and	O
a	O
binary	O
label	O
describing	O
the	O
phase	O
configuration	O
the	O
data	O
set	O
is	O
divided	O
into	O
training	B
validation	O
and	O
test	O
sets	O
each	O
of	O
which	O
comprises	O
independent	B
data	O
points	O
details	O
of	O
the	O
data	O
format	O
are	O
available	O
from	O
the	O
book	O
web	O
site	O
in	O
bishop	O
and	O
james	O
statistical	O
machine	O
learning	B
techniques	O
were	O
used	O
to	O
predict	O
the	O
volume	O
fractions	O
and	O
also	O
the	O
geometrical	O
configuration	O
of	O
the	O
phases	O
shown	O
in	O
figure	O
from	O
the	O
vector	O
of	O
measurements	O
the	O
observation	O
vectors	O
can	O
also	O
be	O
used	O
to	O
test	O
data	O
visualization	B
algorithms	O
this	O
data	O
set	O
has	O
a	O
rich	O
and	O
interesting	O
structure	O
as	O
follows	O
for	O
any	O
given	O
configuration	O
there	O
are	O
two	O
degrees	B
of	I
freedom	I
corresponding	O
to	O
the	O
fractions	O
of	O
a	O
data	O
sets	O
oil	O
and	O
water	O
and	O
so	O
for	O
infinite	O
integration	O
time	O
the	O
data	O
will	O
locally	O
live	O
on	O
a	O
twodimensional	O
manifold	B
for	O
a	O
finite	O
integration	O
time	O
the	O
individual	O
data	O
points	O
will	O
be	O
perturbed	O
away	O
from	O
the	O
manifold	B
by	O
the	O
photon	B
noise	I
in	O
the	O
homogeneous	B
phase	O
configuration	O
the	O
path	O
lengths	O
in	O
oil	O
and	O
water	O
are	O
linearly	O
related	O
to	O
the	O
fractions	O
of	O
oil	O
and	O
water	O
and	O
so	O
the	O
data	O
points	O
lie	O
close	O
to	O
a	O
linear	O
manifold	B
for	O
the	O
annular	O
configuration	O
the	O
relationship	O
between	O
phase	O
fraction	O
and	O
path	O
length	O
is	O
nonlinear	O
and	O
so	O
the	O
manifold	B
will	O
be	O
nonlinear	O
in	O
the	O
case	O
of	O
the	O
laminar	O
configuration	O
the	O
situation	O
is	O
even	O
more	O
complex	O
because	O
small	O
variations	O
in	O
the	O
phase	O
fractions	O
can	O
cause	O
one	O
of	O
the	O
horizontal	O
phase	O
boundaries	O
to	O
move	O
across	O
one	O
of	O
the	O
horizontal	O
beam	O
lines	O
leading	O
to	O
a	O
discontinuous	O
jump	O
in	O
the	O
observation	O
space	O
in	O
this	O
way	O
the	O
two-dimensional	O
nonlinear	O
manifold	B
for	O
the	O
laminar	O
configuration	O
is	O
broken	O
into	O
six	O
distinct	O
segments	O
note	O
also	O
that	O
some	O
of	O
the	O
manifolds	O
for	O
different	O
phase	O
configurations	O
meet	O
at	O
specific	O
points	O
for	O
example	O
if	O
the	O
pipe	O
is	O
filled	O
entirely	O
with	O
oil	O
it	O
corresponds	O
to	O
specific	O
instances	O
of	O
the	O
laminar	O
annular	O
and	O
homogeneous	B
configurations	O
old	O
faithful	O
old	O
faithful	O
shown	O
in	O
figure	O
is	O
a	O
hydrothermal	O
geyser	O
in	O
yellowstone	B
national	I
park	I
in	O
the	O
state	O
of	O
wyoming	O
u	O
s	O
a	O
and	O
is	O
a	O
popular	O
tourist	O
attraction	O
its	O
name	O
stems	O
from	O
the	O
supposed	O
regularity	O
of	O
its	O
eruptions	O
the	O
data	O
set	O
comprises	O
observations	O
each	O
of	O
which	O
represents	O
a	O
single	O
eruption	O
and	O
contains	O
two	O
variables	O
corresponding	O
to	O
the	O
duration	O
in	O
minutes	O
of	O
the	O
eruption	O
and	O
the	O
time	O
until	O
the	O
next	O
eruption	O
also	O
in	O
minutes	O
figure	O
shows	O
a	O
plot	O
of	O
the	O
time	O
to	O
the	O
next	O
eruption	O
versus	O
the	O
duration	O
of	O
the	O
eruptions	O
it	O
can	O
be	O
seen	O
that	O
the	O
time	O
to	O
the	O
next	O
eruption	O
varies	O
considerably	O
although	O
knowledge	O
of	O
the	O
duration	O
of	O
the	O
current	O
eruption	O
allows	O
it	O
to	O
be	O
predicted	O
more	O
accurately	O
note	O
that	O
there	O
exist	O
several	O
other	O
data	O
sets	O
relating	O
to	O
the	O
eruptions	O
of	O
old	O
faithful	O
figure	O
the	O
old	O
faithful	O
geyser	O
national	O
t	O
gourley	O
in	O
park	O
www	O
brucegourley	O
com	O
yellowstone	O
a	O
data	O
sets	O
figure	O
plot	O
of	O
the	O
time	O
to	O
the	O
next	O
eruption	O
in	O
minutes	O
axis	O
versus	O
the	O
duration	O
of	O
the	O
eruption	O
in	O
minutes	O
axis	O
for	O
the	O
old	B
faithful	I
data	I
set	O
synthetic	O
data	O
throughout	O
the	O
book	O
we	O
use	O
two	O
simple	O
synthetic	B
data	I
sets	I
to	O
illustrate	O
many	O
of	O
the	O
algorithms	O
the	O
first	O
of	O
these	O
is	O
a	O
regression	B
problem	O
based	O
on	O
the	O
sinusoidal	O
function	O
shown	O
in	O
figure	O
the	O
input	O
values	O
are	O
generated	O
uniformly	O
in	O
range	O
and	O
the	O
corresponding	O
target	O
values	O
are	O
obtained	O
by	O
first	O
computing	O
the	O
corresponding	O
values	O
of	O
the	O
function	O
x	O
and	O
then	O
adding	O
random	O
noise	O
with	O
a	O
gaussian	B
distribution	O
having	O
standard	B
deviation	I
various	O
forms	O
of	O
this	O
data	O
set	O
having	O
different	O
numbers	O
of	O
data	O
points	O
are	O
used	O
in	O
the	O
book	O
the	O
second	O
data	O
set	O
is	O
a	O
classification	B
problem	O
having	O
two	O
classes	O
with	O
equal	O
prior	B
probabilities	O
and	O
is	O
shown	O
in	O
figure	O
the	O
blue	O
class	O
is	O
generated	O
from	O
a	O
single	O
gaussian	B
while	O
the	O
red	O
class	O
comes	O
from	O
a	O
mixture	B
of	O
two	O
gaussians	O
because	O
we	O
know	O
the	O
class	O
priors	O
and	O
the	O
class-conditional	O
densities	O
it	O
is	O
straightforward	O
to	O
evaluate	O
and	O
plot	O
the	O
true	O
posterior	O
probabilities	O
as	O
well	O
as	O
the	O
minimum	O
misclassification-rate	O
decision	B
boundary	I
as	O
shown	O
in	O
figure	O
a	O
data	O
sets	O
t	O
t	O
x	O
x	O
figure	O
the	O
left-hand	O
plot	O
shows	O
the	O
synthetic	O
regression	B
data	O
set	O
along	O
with	O
the	O
underlying	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
points	O
were	O
generated	O
the	O
right-hand	O
plot	O
shows	O
the	O
true	O
conditional	B
distribution	O
ptx	O
from	O
which	O
the	O
labels	O
are	O
generated	O
in	O
which	O
the	O
green	O
curve	O
denotes	O
the	O
mean	B
and	O
the	O
shaded	O
region	O
spans	O
one	O
standard	B
deviation	I
on	O
each	O
side	O
of	O
the	O
mean	B
figure	O
the	O
left	O
plot	O
shows	O
the	O
synthetic	O
classification	B
data	O
set	O
with	O
data	O
from	O
the	O
two	O
classes	O
shown	O
in	O
red	O
and	O
blue	O
on	O
the	O
right	O
is	O
a	O
plot	O
of	O
the	O
true	O
posterior	O
probabilities	O
shown	O
on	O
a	O
colour	O
scale	O
going	O
from	O
pure	O
red	O
denoting	O
probability	B
of	O
the	O
red	O
class	O
is	O
to	O
pure	O
blue	O
denoting	O
probability	B
of	O
the	O
red	O
class	O
is	O
because	O
these	O
probabilities	O
are	O
known	O
the	O
optimal	O
decision	B
boundary	I
for	O
minimizing	O
the	O
misclassification	O
rate	O
corresponds	O
to	O
the	O
contour	O
along	O
which	O
the	O
posterior	O
probabilities	O
for	O
each	O
class	O
equal	O
can	O
be	O
evaluated	O
and	O
is	O
shown	O
by	O
the	O
green	O
curve	O
this	O
decision	B
boundary	I
is	O
also	O
plotted	O
on	O
the	O
left-hand	O
figure	O
appendix	O
b	O
probability	B
distributions	O
in	O
this	O
appendix	O
we	O
summarize	O
the	O
main	O
properties	O
of	O
some	O
of	O
the	O
most	O
widely	O
used	O
probability	B
distributions	O
and	O
for	O
each	O
distribution	O
we	O
list	O
some	O
key	O
statistics	O
such	O
as	O
the	O
expectation	B
ex	O
the	O
variance	B
covariance	B
the	O
mode	O
and	O
the	O
entropy	B
hx	O
all	O
of	O
these	O
distributions	O
are	O
members	O
of	O
the	O
exponential	B
family	I
and	O
are	O
widely	O
used	O
as	O
building	O
blocks	O
for	O
more	O
sophisticated	O
probabilistic	O
models	O
bernoulli	B
this	O
is	O
the	O
distribution	O
for	O
a	O
single	O
binary	O
variable	O
x	O
representing	O
for	O
example	O
the	O
result	O
of	O
flipping	O
a	O
coin	O
it	O
is	O
governed	O
by	O
a	O
single	O
continuous	O
parameter	O
that	O
represents	O
the	O
probability	B
of	O
x	O
bernx	O
x	O
ex	O
varx	O
hx	O
ln	O
modex	O
if	O
otherwise	O
the	O
bernoulli	B
is	O
a	O
special	O
case	O
of	O
the	O
binomial	B
distribution	I
for	O
the	O
case	O
of	O
a	O
single	O
observation	O
its	O
conjugate	B
prior	B
for	O
is	O
the	O
beta	B
distribution	I
b	O
probability	B
distributions	O
beta	O
this	O
is	O
a	O
distribution	O
over	O
a	O
continuous	O
variable	O
which	O
is	O
often	O
used	O
to	O
represent	O
the	O
probability	B
for	O
some	O
binary	O
event	O
it	O
is	O
governed	O
by	O
two	O
parameters	O
a	O
and	O
b	O
that	O
are	O
constrained	O
by	O
a	O
and	O
b	O
to	O
ensure	O
that	O
the	O
distribution	O
can	O
be	O
normalized	O
beta	O
b	O
b	O
a	O
e	O
var	O
mode	O
a	O
a	O
b	O
ab	O
b	O
a	O
a	O
b	O
the	O
beta	O
is	O
the	O
conjugate	B
prior	B
for	O
the	O
bernoulli	B
distribution	I
for	O
which	O
a	O
and	O
b	O
can	O
be	O
interpreted	O
as	O
the	O
effective	O
prior	B
number	O
of	O
observations	O
of	O
x	O
and	O
x	O
respectively	O
its	O
density	B
is	O
finite	O
if	O
a	O
and	O
b	O
otherwise	O
there	O
is	O
a	O
singularity	O
at	O
andor	O
for	O
a	O
b	O
it	O
reduces	O
to	O
a	O
uniform	B
distribution	I
the	O
beta	B
distribution	I
is	O
a	O
special	O
case	O
of	O
the	O
k-state	O
dirichlet	B
distribution	I
for	O
k	O
binomial	O
the	O
binomial	B
distribution	I
gives	O
the	O
probability	B
of	O
observing	O
m	O
occurrences	O
of	O
x	O
in	O
a	O
set	O
of	O
n	O
samples	O
from	O
a	O
bernoulli	B
distribution	I
where	O
the	O
probability	B
of	O
observing	O
x	O
is	O
m	O
binmn	O
n	O
m	O
em	B
n	O
varm	O
n	O
modem	O
where	O
denotes	O
the	O
largest	O
integer	O
that	O
is	O
less	O
than	O
or	O
equal	O
to	O
and	O
the	O
quantity	O
n	O
m	O
n	O
m	O
n	O
m	O
denotes	O
the	O
number	O
of	O
ways	O
of	O
choosing	O
m	O
objects	O
out	O
of	O
a	O
total	O
of	O
n	O
identical	O
objects	O
here	O
m	O
pronounced	O
factorial	B
m	O
denotes	O
the	O
product	O
m	O
the	O
particular	O
case	O
of	O
the	O
binomial	B
distribution	I
for	O
n	O
is	O
known	O
as	O
the	O
bernoulli	B
distribution	I
and	O
for	O
large	O
n	O
the	O
binomial	B
distribution	I
is	O
approximately	O
gaussian	B
the	O
conjugate	B
prior	B
for	O
is	O
the	O
beta	B
distribution	I
b	O
probability	B
distributions	O
dirichlet	B
the	O
dirichlet	B
is	O
a	O
multivariate	O
distribution	O
over	O
k	O
random	O
variables	O
k	O
where	O
k	O
k	O
subject	O
to	O
the	O
constraints	O
k	O
k	O
denoting	O
kt	O
and	O
kt	O
we	O
have	O
dir	O
c	O
k	O
k	O
e	O
k	O
var	O
k	O
k	O
k	O
eln	O
k	O
k	O
h	O
k	O
cov	O
j	O
k	O
j	O
k	O
mode	O
k	O
k	O
k	O
ln	O
c	O
c	O
k	O
k	O
d	O
da	O
ln	O
where	O
and	O
here	O
is	O
known	O
as	O
the	O
digamma	B
function	I
and	O
stegun	O
the	O
parameters	O
k	O
are	O
subject	O
to	O
the	O
constraint	O
k	O
in	O
order	O
to	O
ensure	O
that	O
the	O
distribution	O
can	O
be	O
normalized	O
the	O
dirichlet	B
forms	O
the	O
conjugate	B
prior	B
for	O
the	O
multinomial	B
distribution	I
and	O
represents	O
a	O
generalization	B
of	O
the	O
beta	B
distribution	I
in	O
this	O
case	O
the	O
parameters	O
k	O
can	O
be	O
interpreted	O
as	O
effective	O
numbers	O
of	O
observations	O
of	O
the	O
corresponding	O
values	O
of	O
the	O
k-dimensional	O
binary	O
observation	O
vector	O
x	O
as	O
with	O
the	O
beta	B
distribution	I
the	O
dirichlet	B
has	O
finite	O
density	B
everywhere	O
provided	O
k	O
for	O
all	O
k	O
b	O
probability	B
distributions	O
gamma	O
the	O
gamma	O
is	O
a	O
probability	B
distribution	O
over	O
a	O
positive	O
random	O
variable	O
governed	O
by	O
parameters	O
a	O
and	O
b	O
that	O
are	O
subject	O
to	O
the	O
constraints	O
a	O
and	O
b	O
to	O
ensure	O
that	O
the	O
distribution	O
can	O
be	O
normalized	O
gam	O
b	O
ba	O
a	O
b	O
e	O
a	O
b	O
var	O
a	O
mode	O
a	O
eln	O
ln	O
b	O
h	O
ln	O
ln	O
b	O
a	O
for	O
b	O
where	O
is	O
the	O
digamma	B
function	I
defined	O
by	O
the	O
gamma	B
distribution	I
is	O
the	O
conjugate	B
prior	B
for	O
the	O
precision	O
variance	B
of	O
a	O
univariate	O
gaussian	B
for	O
a	O
the	O
density	B
is	O
everywhere	O
finite	O
and	O
the	O
special	O
case	O
of	O
a	O
is	O
known	O
as	O
the	O
exponential	B
distribution	I
gaussian	B
the	O
gaussian	B
is	O
the	O
most	O
widely	O
used	O
distribution	O
for	O
continuous	O
variables	O
it	O
is	O
also	O
known	O
as	O
the	O
normal	O
distribution	O
in	O
the	O
case	O
of	O
a	O
single	O
variable	O
x	O
it	O
is	O
governed	O
by	O
two	O
parameters	O
the	O
mean	B
and	O
the	O
variance	B
n	O
exp	O
ex	O
varx	O
modex	O
hx	O
ln	O
the	O
inverse	B
of	O
the	O
variance	B
is	O
called	O
the	O
precision	O
and	O
the	O
square	O
root	O
of	O
the	O
variance	B
is	O
called	O
the	O
standard	B
deviation	I
the	O
conjugate	B
prior	B
for	O
is	O
the	O
gaussian	B
and	O
the	O
conjugate	B
prior	B
for	O
is	O
the	O
gamma	B
distribution	I
if	O
both	O
and	O
are	O
unknown	O
their	O
joint	O
conjugate	B
prior	B
is	O
the	O
gaussian-gamma	B
distribution	I
for	O
a	O
d-dimensional	O
vector	O
x	O
the	O
gaussian	B
is	O
governed	O
by	O
a	O
d-dimensional	O
mean	B
vector	O
and	O
a	O
d	O
d	O
covariance	B
matrix	O
that	O
must	O
be	O
symmetric	O
and	O
b	O
probability	B
distributions	O
positive-definite	O
n	O
exp	O
ex	O
covx	O
modex	O
hx	O
ln	O
d	O
is	O
the	O
precision	B
matrix	I
which	O
is	O
also	O
the	O
inverse	B
of	O
the	O
covariance	B
matrix	O
symmetric	O
and	O
positive	B
definite	I
averages	O
of	O
random	O
variables	O
tend	O
to	O
a	O
gaussian	B
by	O
the	O
central	B
limit	I
theorem	I
and	O
the	O
sum	O
of	O
two	O
gaussian	B
variables	O
is	O
again	O
gaussian	B
the	O
gaussian	B
is	O
the	O
distribution	O
that	O
maximizes	O
the	O
entropy	B
for	O
a	O
given	O
variance	B
covariance	B
any	O
linear	O
transformation	O
of	O
a	O
gaussian	B
random	O
variable	O
is	O
again	O
gaussian	B
the	O
marginal	B
distribution	O
of	O
a	O
multivariate	O
gaussian	B
with	O
respect	O
to	O
a	O
subset	O
of	O
the	O
variables	O
is	O
itself	O
gaussian	B
and	O
similarly	O
the	O
conditional	B
distribution	O
is	O
also	O
gaussian	B
the	O
conjugate	B
prior	B
for	O
is	O
the	O
gaussian	B
the	O
conjugate	B
prior	B
for	O
is	O
the	O
wishart	O
and	O
the	O
conjugate	B
prior	B
for	O
is	O
the	O
gaussian-wishart	O
if	O
we	O
have	O
a	O
marginal	B
gaussian	B
distribution	O
for	O
x	O
and	O
a	O
conditional	B
gaussian	B
distribution	O
for	O
y	O
given	O
x	O
in	O
the	O
form	O
px	O
n	O
pyx	O
n	O
b	O
l	O
then	O
the	O
marginal	B
distribution	O
of	O
y	O
and	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
y	O
are	O
given	O
by	O
py	O
n	O
b	O
l	O
a	O
pxy	O
n	O
b	O
where	O
if	O
we	O
have	O
a	O
joint	O
gaussian	B
distribution	O
n	O
with	O
and	O
we	O
atla	O
define	O
the	O
following	O
partitions	O
x	O
xa	O
xb	O
aa	O
ab	O
ba	O
bb	O
a	O
b	O
aa	O
ab	O
ba	O
bb	O
then	O
the	O
conditional	B
distribution	O
pxaxb	O
is	O
given	O
by	O
aa	O
pxaxb	O
n	O
ab	O
ab	O
a	O
aa	O
abxb	O
b	O
b	O
probability	B
distributions	O
and	O
the	O
marginal	B
distribution	O
pxa	O
is	O
given	O
by	O
pxa	O
n	O
a	O
aa	O
gaussian-gamma	O
this	O
is	O
the	O
conjugate	B
prior	B
distribution	O
for	O
a	O
univariate	O
gaussian	B
n	O
in	O
which	O
the	O
mean	B
and	O
the	O
precision	O
are	O
both	O
unknown	O
and	O
is	O
also	O
called	O
the	O
normal-gamma	B
distribution	I
it	O
comprises	O
the	O
product	O
of	O
a	O
gaussian	B
distribution	O
for	O
whose	O
precision	O
is	O
proportional	O
to	O
and	O
a	O
gamma	B
distribution	I
over	O
p	O
a	O
b	O
o	O
gam	O
b	O
gaussian-wishart	O
this	O
is	O
the	O
conjugate	B
prior	B
distribution	O
for	O
a	O
multivariate	O
gaussian	B
n	O
in	O
which	O
both	O
the	O
mean	B
and	O
the	O
precision	O
are	O
unknown	O
and	O
is	O
also	O
called	O
the	O
normal-wishart	B
distribution	I
it	O
comprises	O
the	O
product	O
of	O
a	O
gaussian	B
distribution	O
for	O
whose	O
precision	O
is	O
proportional	O
to	O
and	O
a	O
wishart	B
distribution	I
over	O
p	O
w	O
w	O
for	O
the	O
particular	O
case	O
of	O
a	O
scalar	O
x	O
this	O
is	O
equivalent	O
to	O
the	O
gaussian-gamma	B
distribution	I
multinomial	O
if	O
we	O
generalize	O
the	O
bernoulli	B
distribution	I
to	O
an	O
k-dimensional	O
binary	O
variable	O
x	O
with	O
components	O
xk	O
such	O
that	O
k	O
xk	O
then	O
we	O
obtain	O
the	O
following	O
discrete	O
distribution	O
px	O
xk	O
k	O
exk	O
k	O
varxk	O
k	O
covxjxk	O
ijk	O
k	O
hx	O
k	O
ln	O
k	O
b	O
probability	B
distributions	O
where	O
ijk	O
is	O
the	O
j	O
k	O
element	O
of	O
the	O
identity	O
matrix	O
because	O
pxk	O
k	O
the	O
parameters	O
must	O
satisfy	O
k	O
and	O
k	O
k	O
the	O
multinomial	B
distribution	I
is	O
a	O
multivariate	O
generalization	B
of	O
the	O
binomial	O
and	O
gives	O
the	O
distribution	O
over	O
counts	O
mk	O
for	O
a	O
k-state	O
discrete	O
variable	O
to	O
be	O
in	O
state	O
k	O
given	O
a	O
total	O
number	O
of	O
observations	O
n	O
mk	O
n	O
n	O
mm	O
emk	O
n	O
k	O
varmk	O
n	O
k	O
covmjmk	O
n	O
j	O
k	O
where	O
kt	O
and	O
the	O
quantity	O
n	O
mk	O
n	O
mk	O
mk	O
k	O
gives	O
the	O
number	O
of	O
ways	O
of	O
taking	O
n	O
identical	O
objects	O
and	O
assigning	O
mk	O
of	O
them	O
to	O
bin	O
k	O
for	O
k	O
k	O
the	O
value	O
of	O
k	O
gives	O
the	O
probability	B
of	O
the	O
random	O
variable	O
taking	O
state	O
k	O
and	O
so	O
these	O
parameters	O
are	O
subject	O
to	O
the	O
constraints	O
k	O
k	O
k	O
the	O
conjugate	B
prior	B
distribution	O
for	O
the	O
parameters	O
k	O
is	O
the	O
and	O
dirichlet	B
normal	O
the	O
normal	O
distribution	O
is	O
simply	O
another	O
name	O
for	O
the	O
gaussian	B
in	O
this	O
book	O
we	O
use	O
the	O
term	O
gaussian	B
throughout	O
although	O
we	O
retain	O
the	O
conventional	O
use	O
of	O
the	O
symbol	O
n	O
to	O
denote	O
this	O
distribution	O
for	O
consistency	O
we	O
shall	O
refer	O
to	O
the	O
normalgamma	O
distribution	O
as	O
the	O
gaussian-gamma	B
distribution	I
and	O
similarly	O
the	O
normalwishart	O
is	O
called	O
the	O
gaussian-wishart	O
student	O
s	O
t	O
this	O
distribution	O
was	O
published	O
by	O
william	O
gosset	O
in	O
but	O
his	O
employer	O
guiness	O
breweries	O
required	O
him	O
to	O
publish	O
under	O
a	O
pseudonym	O
so	O
he	O
chose	O
student	O
in	O
the	O
univariate	O
form	O
student	O
s	O
t-distribution	O
is	O
obtained	O
by	O
placing	O
a	O
conjugate	B
gamma	O
prior	B
over	O
the	O
precision	O
of	O
a	O
univariate	O
gaussian	B
distribution	O
and	O
then	O
integrating	O
out	O
the	O
precision	O
variable	O
it	O
can	O
therefore	O
be	O
viewed	O
as	O
an	O
infinite	O
mixture	B
b	O
probability	B
distributions	O
of	O
gaussians	O
having	O
the	O
same	O
mean	B
but	O
different	O
variances	O
stx	O
ex	O
modex	O
varx	O
for	O
for	O
here	O
is	O
called	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
of	O
the	O
distribution	O
the	O
particular	O
case	O
of	O
is	O
called	O
the	O
cauchy	B
distribution	I
for	O
a	O
d-dimensional	O
variable	O
x	O
student	O
s	O
t-distribution	O
corresponds	O
to	O
marginalizing	O
the	O
precision	B
matrix	I
of	O
a	O
multivariate	O
gaussian	B
with	O
respect	O
to	O
a	O
conjugate	B
wishart	O
prior	B
and	O
takes	O
the	O
form	O
stx	O
ex	O
for	O
covx	O
modex	O
for	O
where	O
is	O
the	O
squared	O
mahalanobis	B
distance	I
defined	O
by	O
in	O
the	O
limit	O
the	O
t-distribution	O
reduces	O
to	O
a	O
gaussian	B
with	O
mean	B
and	O
precision	O
student	O
s	O
t-distribution	O
provides	O
a	O
generalization	B
of	O
the	O
gaussian	B
whose	O
maximum	B
likelihood	I
parameter	O
values	O
are	O
robust	O
to	O
outliers	B
uniform	O
this	O
is	O
a	O
simple	O
distribution	O
for	O
a	O
continuous	O
variable	O
x	O
defined	O
over	O
a	O
finite	O
interval	O
x	O
b	O
where	O
b	O
a	O
uxa	O
b	O
ex	O
b	O
a	O
a	O
varx	O
hx	O
lnb	O
a	O
if	O
x	O
has	O
distribution	O
then	O
a	O
ax	O
will	O
have	O
distribution	O
uxa	O
b	O
b	O
probability	B
distributions	O
von	O
mises	O
the	O
von	B
mises	I
distribution	I
also	O
known	O
as	O
the	O
circular	O
normal	O
or	O
the	O
circular	O
gaussian	B
is	O
a	O
univariate	O
gaussian-like	O
periodic	O
distribution	O
for	O
a	O
variable	O
p	O
m	O
expm	O
cos	O
where	O
is	O
the	O
zeroth-order	O
bessel	O
function	O
of	O
the	O
first	O
kind	O
the	O
distribution	O
has	O
period	O
so	O
that	O
p	O
p	O
for	O
all	O
care	O
must	O
be	O
taken	O
in	O
interpreting	O
this	O
distribution	O
because	O
simple	O
expectations	O
will	O
be	O
dependent	O
on	O
the	O
choice	O
of	O
origin	O
for	O
the	O
variable	O
the	O
parameter	O
is	O
analogous	O
to	O
the	O
mean	B
of	O
a	O
univariate	O
gaussian	B
and	O
the	O
parameter	O
m	O
known	O
as	O
the	O
concentration	B
parameter	I
is	O
analogous	O
to	O
the	O
precision	O
variance	B
for	O
large	O
m	O
the	O
von	B
mises	I
distribution	I
is	O
approximately	O
a	O
gaussian	B
centred	O
on	O
wishart	O
the	O
wishart	B
distribution	I
is	O
the	O
conjugate	B
prior	B
for	O
the	O
precision	B
matrix	I
of	O
a	O
multivariate	O
gaussian	B
w	O
bw	O
d	O
exp	O
where	O
bw	O
dd	O
trw	O
i	O
e	O
w	O
e	O
i	O
d	O
ln	O
lnw	O
h	O
ln	O
bw	O
d	O
where	O
w	O
is	O
a	O
d	O
d	O
symmetric	O
positive	B
definite	I
matrix	I
and	O
is	O
the	O
digamma	B
function	I
defined	O
by	O
the	O
parameter	O
is	O
called	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
of	O
the	O
distribution	O
and	O
is	O
restricted	O
to	O
d	O
to	O
ensure	O
that	O
the	O
gamma	B
function	I
in	O
the	O
normalization	O
factor	O
is	O
well-defined	O
in	O
one	O
dimension	O
the	O
wishart	O
reduces	O
to	O
the	O
gamma	B
distribution	I
gam	O
b	O
given	O
by	O
with	O
parameters	O
a	O
and	O
b	O
e	O
d	O
appendix	O
c	O
properties	O
of	O
matrices	O
in	O
this	O
appendix	O
we	O
gather	O
together	O
some	O
useful	O
properties	O
and	O
identities	O
involving	O
matrices	O
and	O
determinants	O
this	O
is	O
not	O
intended	O
to	O
be	O
an	O
introductory	O
tutorial	O
and	O
it	O
is	O
assumed	O
that	O
the	O
reader	O
is	O
already	O
familiar	O
with	O
basic	O
linear	O
algebra	O
for	O
some	O
results	O
we	O
indicate	O
how	O
to	O
prove	O
them	O
whereas	O
in	O
more	O
complex	O
cases	O
we	O
leave	O
the	O
interested	O
reader	O
to	O
refer	O
to	O
standard	O
textbooks	O
on	O
the	O
subject	O
in	O
all	O
cases	O
we	O
assume	O
that	O
inverses	O
exist	O
and	O
that	O
matrix	O
dimensions	O
are	O
such	O
that	O
the	O
formulae	O
are	O
correctly	O
defined	O
a	O
comprehensive	O
discussion	O
of	O
linear	O
algebra	O
can	O
be	O
found	O
in	O
golub	O
and	O
van	O
loan	O
and	O
an	O
extensive	O
collection	O
of	O
matrix	O
properties	O
is	O
given	O
by	O
l	O
utkepohl	O
matrix	O
derivatives	O
are	O
discussed	O
in	O
magnus	O
and	O
neudecker	O
basic	O
matrix	O
identities	O
a	O
matrix	O
a	O
has	O
elements	O
aij	O
where	O
i	O
indexes	O
the	O
rows	O
and	O
j	O
indexes	O
the	O
columns	O
we	O
use	O
in	O
to	O
denote	O
the	O
n	O
n	O
identity	O
matrix	O
called	O
the	O
unit	O
matrix	O
and	O
where	O
there	O
is	O
no	O
ambiguity	O
over	O
dimensionality	O
we	O
simply	O
use	O
i	O
the	O
transpose	O
matrix	O
at	O
has	O
elements	O
aji	O
from	O
the	O
definition	O
of	O
transpose	O
we	O
have	O
which	O
can	O
be	O
verified	O
by	O
writing	O
out	O
the	O
indices	O
the	O
inverse	B
of	O
a	O
denoted	O
a	O
satisfies	O
btat	O
aa	O
a	O
i	O
because	O
abb	O
i	O
we	O
have	O
also	O
we	O
have	O
b	O
at	O
a	O
c	O
properties	O
of	O
matrices	O
which	O
is	O
easily	O
proven	O
by	O
taking	O
the	O
transpose	O
of	O
and	O
applying	O
a	O
useful	O
identity	O
involving	O
matrix	O
inverses	O
is	O
the	O
following	O
btr	O
pbtbpbt	O
r	O
which	O
is	O
easily	O
verified	O
by	O
right	O
multiplying	O
both	O
sides	O
by	O
r	O
suppose	O
that	O
p	O
has	O
dimensionality	O
n	O
n	O
while	O
r	O
has	O
dimensionality	O
m	O
m	O
so	O
that	O
b	O
is	O
m	O
n	O
then	O
if	O
m	O
n	O
it	O
will	O
be	O
much	O
cheaper	O
to	O
evaluate	O
the	O
right-hand	O
side	O
of	O
than	O
the	O
left-hand	O
side	O
a	O
special	O
case	O
that	O
sometimes	O
arises	O
is	O
ab	O
ai	O
ba	O
another	O
useful	O
identity	O
involving	O
inverses	O
is	O
the	O
following	O
bd	O
a	O
a	O
ca	O
which	O
is	O
known	O
as	O
the	O
woodbury	B
identity	I
and	O
which	O
can	O
be	O
verified	O
by	O
multiplying	O
both	O
sides	O
by	O
bd	O
this	O
is	O
useful	O
for	O
instance	O
when	O
a	O
is	O
large	O
and	O
diagonal	B
and	O
hence	O
easy	O
to	O
invert	O
while	O
b	O
has	O
many	O
rows	O
but	O
few	O
columns	O
conversely	O
for	O
c	O
so	O
that	O
the	O
right-hand	O
side	O
is	O
much	O
cheaper	O
to	O
evaluate	O
than	O
the	O
left-hand	O
side	O
a	O
set	O
of	O
vectors	O
an	O
is	O
said	O
to	O
be	O
linearly	O
independent	B
if	O
the	O
relation	O
n	O
nan	O
holds	O
only	O
if	O
all	O
n	O
this	O
implies	O
that	O
none	O
of	O
the	O
vectors	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
combination	O
of	O
the	O
remainder	O
the	O
rank	O
of	O
a	O
matrix	O
is	O
the	O
maximum	O
number	O
of	O
linearly	O
independent	B
rows	O
equivalently	O
the	O
maximum	O
number	O
of	O
linearly	O
independent	B
columns	O
traces	O
and	O
determinants	O
trace	O
and	O
determinant	O
apply	O
to	O
square	O
matrices	O
the	O
trace	O
tra	O
of	O
a	O
matrix	O
a	O
is	O
defined	O
as	O
the	O
sum	O
of	O
the	O
elements	O
on	O
the	O
leading	O
diagonal	B
by	O
writing	O
out	O
the	O
indices	O
we	O
see	O
that	O
trab	O
trba	O
by	O
applying	O
this	O
formula	O
multiple	O
times	O
to	O
the	O
product	O
of	O
three	O
matrices	O
we	O
see	O
that	O
trabc	O
trcab	O
trbca	O
which	O
is	O
known	O
as	O
the	O
cyclic	O
property	O
of	O
the	O
trace	O
operator	O
and	O
which	O
clearly	O
extends	O
to	O
the	O
product	O
of	O
any	O
number	O
of	O
matrices	O
the	O
determinant	O
of	O
an	O
n	O
n	O
matrix	O
a	O
is	O
defined	O
by	O
an	O
in	O
in	O
which	O
the	O
sum	O
is	O
taken	O
over	O
all	O
products	O
consisting	O
of	O
precisely	O
one	O
element	O
from	O
each	O
row	O
and	O
one	O
element	O
from	O
each	O
column	O
with	O
a	O
coefficient	O
or	O
according	O
c	O
properties	O
of	O
matrices	O
to	O
whether	O
the	O
permutation	O
in	O
is	O
even	O
or	O
odd	O
respectively	O
note	O
that	O
thus	O
for	O
a	O
matrix	O
the	O
determinant	O
takes	O
the	O
form	O
the	O
determinant	O
of	O
a	O
product	O
of	O
two	O
matrices	O
is	O
given	O
by	O
as	O
can	O
be	O
shown	O
from	O
also	O
the	O
determinant	O
of	O
an	O
inverse	B
matrix	O
is	O
given	O
by	O
atb	O
abt	O
atb	O
abt	O
a	O
useful	O
special	O
case	O
is	O
where	O
a	O
and	O
b	O
are	O
n-dimensional	O
column	O
vectors	O
matrix	O
derivatives	O
which	O
can	O
be	O
shown	O
by	O
taking	O
the	O
determinant	O
of	O
and	O
applying	O
if	O
a	O
and	O
b	O
are	O
matrices	O
of	O
size	O
n	O
m	O
then	O
sometimes	O
we	O
need	O
to	O
consider	O
derivatives	O
of	O
vectors	O
and	O
matrices	O
with	O
respect	O
to	O
scalars	O
the	O
derivative	B
of	O
a	O
vector	O
a	O
with	O
respect	O
to	O
a	O
scalar	O
x	O
is	O
itself	O
a	O
vector	O
whose	O
components	O
are	O
given	O
by	O
a	O
x	O
ai	O
x	O
with	O
an	O
analogous	O
definition	O
for	O
the	O
derivative	B
of	O
a	O
matrix	O
derivatives	O
with	O
respect	O
to	O
vectors	O
and	O
matrices	O
can	O
also	O
be	O
defined	O
for	O
instance	O
i	O
i	O
x	O
a	O
x	O
ai	O
and	O
similarly	O
the	O
following	O
is	O
easily	O
proven	O
by	O
writing	O
out	O
the	O
components	O
a	O
b	O
ai	O
bj	O
ij	O
x	O
xta	O
x	O
atx	O
a	O
c	O
properties	O
of	O
matrices	O
similarly	O
x	O
a	O
x	O
b	O
a	O
b	O
x	O
the	O
derivative	B
of	O
the	O
inverse	B
of	O
a	O
matrix	O
can	O
be	O
expressed	O
as	O
a	O
x	O
a	O
a	O
x	O
a	O
as	O
can	O
be	O
shown	O
by	O
differentiating	O
the	O
equation	O
a	O
i	O
using	O
and	O
then	O
right	O
multiplying	O
by	O
a	O
also	O
lna	O
tr	O
x	O
a	O
a	O
x	O
which	O
we	O
shall	O
prove	O
later	O
if	O
we	O
choose	O
x	O
to	O
be	O
one	O
of	O
the	O
elements	O
of	O
a	O
we	O
have	O
aij	O
tr	O
bji	O
as	O
can	O
be	O
seen	O
by	O
writing	O
out	O
the	O
matrices	O
using	O
index	O
notation	O
we	O
can	O
write	O
this	O
result	O
more	O
compactly	O
in	O
the	O
form	O
with	O
this	O
notation	O
we	O
have	O
the	O
following	O
properties	O
tr	O
bt	O
a	O
a	O
tr	O
atb	O
b	O
a	O
tra	O
i	O
trabat	O
ab	O
bt	O
a	O
a	O
which	O
can	O
again	O
be	O
proven	O
by	O
writing	O
out	O
the	O
matrix	O
indices	O
we	O
also	O
have	O
lna	O
a	O
which	O
follows	O
from	O
and	O
eigenvector	O
equation	O
for	O
a	O
square	O
matrix	O
a	O
of	O
size	O
m	O
m	O
the	O
eigenvector	O
equation	O
is	O
defined	O
by	O
aui	O
iui	O
c	O
properties	O
of	O
matrices	O
for	O
i	O
m	O
where	O
ui	O
is	O
an	O
eigenvector	O
and	O
i	O
is	O
the	O
corresponding	O
eigenvalue	O
this	O
can	O
be	O
viewed	O
as	O
a	O
set	O
of	O
m	O
simultaneous	O
homogeneous	B
linear	O
equations	O
and	O
the	O
condition	O
for	O
a	O
solution	O
is	O
that	O
ii	O
which	O
is	O
known	O
as	O
the	O
characteristic	O
equation	O
because	O
this	O
is	O
a	O
polynomial	O
of	O
order	O
m	O
in	O
i	O
it	O
must	O
have	O
m	O
solutions	O
these	O
need	O
not	O
all	O
be	O
distinct	O
the	O
rank	O
of	O
a	O
is	O
equal	O
to	O
the	O
number	O
of	O
nonzero	O
eigenvalues	O
of	O
particular	O
interest	O
are	O
symmetric	O
matrices	O
which	O
arise	O
as	O
covariance	B
matrices	O
kernel	O
matrices	O
and	O
hessians	O
symmetric	O
matrices	O
have	O
the	O
property	O
that	O
aij	O
aji	O
or	O
equivalently	O
at	O
a	O
the	O
inverse	B
of	O
a	O
symmetric	O
matrix	O
is	O
also	O
symmetric	O
as	O
can	O
be	O
seen	O
by	O
taking	O
the	O
transpose	O
of	O
a	O
i	O
and	O
using	O
aa	O
i	O
together	O
with	O
the	O
symmetry	O
of	O
i	O
in	O
general	O
the	O
eigenvalues	O
of	O
a	O
matrix	O
are	O
complex	O
numbers	O
but	O
for	O
symmetric	O
matrices	O
the	O
eigenvalues	O
i	O
are	O
real	O
this	O
can	O
be	O
seen	O
by	O
first	O
left	O
multiplying	O
by	O
i	O
where	O
denotes	O
the	O
complex	O
conjugate	B
to	O
give	O
i	O
aui	O
i	O
i	O
ui	O
next	O
we	O
take	O
the	O
complex	O
conjugate	B
of	O
and	O
left	O
multiply	O
by	O
ut	O
i	O
to	O
give	O
i	O
ut	O
i	O
i	O
ut	O
i	O
i	O
where	O
we	O
have	O
used	O
a	O
because	O
we	O
consider	O
only	O
real	O
matrices	O
a	O
taking	O
the	O
transpose	O
of	O
the	O
second	O
of	O
these	O
equations	O
and	O
using	O
at	O
a	O
we	O
see	O
that	O
the	O
left-hand	O
sides	O
of	O
the	O
two	O
equations	O
are	O
equal	O
and	O
hence	O
that	O
i	O
i	O
and	O
so	O
i	O
must	O
be	O
real	O
the	O
eigenvectors	O
ui	O
of	O
a	O
real	O
symmetric	O
matrix	O
can	O
be	O
chosen	O
to	O
be	O
orthonormal	O
orthogonal	O
and	O
of	O
unit	O
length	O
so	O
that	O
ut	O
i	O
uj	O
iij	O
where	O
iij	O
are	O
the	O
elements	O
of	O
the	O
identity	O
matrix	O
i	O
to	O
show	O
this	O
we	O
first	O
left	O
multiply	O
by	O
ut	O
j	O
to	O
give	O
and	O
hence	O
by	O
exchange	O
of	O
indices	O
we	O
have	O
ut	O
j	O
aui	O
iut	O
j	O
ui	O
ut	O
i	O
auj	O
jut	O
i	O
uj	O
we	O
now	O
take	O
the	O
transpose	O
of	O
the	O
second	O
equation	O
and	O
make	O
use	O
of	O
the	O
symmetry	O
property	O
at	O
a	O
and	O
then	O
subtract	O
the	O
two	O
equations	O
to	O
give	O
i	O
j	O
ut	O
hence	O
for	O
i	O
j	O
we	O
have	O
ut	O
i	O
uj	O
and	O
hence	O
ui	O
and	O
uj	O
are	O
orthogonal	O
if	O
the	O
two	O
eigenvalues	O
are	O
equal	O
then	O
any	O
linear	O
combination	O
ui	O
uj	O
is	O
also	O
an	O
eigenvector	O
with	O
the	O
same	O
eigenvalue	O
so	O
we	O
can	O
select	O
one	O
linear	O
combination	O
arbitrarily	O
i	O
uj	O
c	O
properties	O
of	O
matrices	O
and	O
then	O
choose	O
the	O
second	O
to	O
be	O
orthogonal	O
to	O
the	O
first	O
can	O
be	O
shown	O
that	O
the	O
degenerate	O
eigenvectors	O
are	O
never	O
linearly	O
dependent	O
hence	O
the	O
eigenvectors	O
can	O
be	O
chosen	O
to	O
be	O
orthogonal	O
and	O
by	O
normalizing	O
can	O
be	O
set	O
to	O
unit	O
length	O
because	O
there	O
are	O
m	O
eigenvalues	O
the	O
corresponding	O
m	O
orthogonal	O
eigenvectors	O
form	O
a	O
complete	O
set	O
and	O
so	O
any	O
m-dimensional	O
vector	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
combination	O
of	O
the	O
eigenvectors	O
we	O
can	O
take	O
the	O
eigenvectors	O
ui	O
to	O
be	O
the	O
columns	O
of	O
an	O
m	O
m	O
matrix	O
u	O
which	O
from	O
orthonormality	O
satisfies	O
utu	O
i	O
such	O
a	O
matrix	O
is	O
said	O
to	O
be	O
orthogonal	O
interestingly	O
the	O
rows	O
of	O
this	O
matrix	O
are	O
also	O
orthogonal	O
so	O
that	O
uut	O
i	O
to	O
show	O
this	O
note	O
that	O
implies	O
utuu	O
u	O
ut	O
and	O
so	O
uu	O
uut	O
i	O
using	O
it	O
also	O
follows	O
that	O
the	O
eigenvector	O
equation	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
u	O
in	O
the	O
form	O
where	O
is	O
an	O
m	O
m	O
diagonal	B
matrix	O
whose	O
diagonal	B
elements	O
are	O
given	O
by	O
the	O
eigenvalues	O
i	O
au	O
u	O
if	O
we	O
consider	O
a	O
column	O
vector	O
x	O
that	O
is	O
transformed	O
by	O
an	O
orthogonal	O
matrix	O
u	O
to	O
give	O
a	O
new	O
vector	O
then	O
the	O
length	O
of	O
the	O
vector	O
is	O
preserved	O
because	O
and	O
similarly	O
the	O
angle	O
between	O
any	O
two	O
such	O
vectors	O
is	O
preserved	O
because	O
ux	O
xtutux	O
xtx	O
xtutuy	O
xty	O
thus	O
multiplication	O
by	O
u	O
can	O
be	O
interpreted	O
as	O
a	O
rigid	O
rotation	O
of	O
the	O
coordinate	O
system	O
from	O
it	O
follows	O
that	O
utau	O
and	O
because	O
is	O
a	O
diagonal	B
matrix	O
we	O
say	O
that	O
the	O
matrix	O
a	O
is	O
diagonalized	O
by	O
the	O
matrix	O
u	O
if	O
we	O
left	O
multiply	O
by	O
u	O
and	O
right	O
multiply	O
by	O
ut	O
we	O
obtain	O
taking	O
the	O
inverse	B
of	O
this	O
equation	O
and	O
using	O
together	O
with	O
u	O
ut	O
we	O
have	O
a	O
u	O
ut	O
a	O
u	O
a	O
a	O
iuiut	O
i	O
i	O
uiut	O
i	O
i	O
c	O
properties	O
of	O
matrices	O
these	O
last	O
two	O
equations	O
can	O
also	O
be	O
written	O
in	O
the	O
form	O
if	O
we	O
take	O
the	O
determinant	O
of	O
and	O
use	O
we	O
obtain	O
similarly	O
taking	O
the	O
trace	O
of	O
and	O
using	O
the	O
cyclic	O
property	O
of	O
the	O
trace	O
operator	O
together	O
with	O
utu	O
i	O
we	O
have	O
tra	O
i	O
we	O
leave	O
it	O
as	O
an	O
exercise	O
for	O
the	O
reader	O
to	O
verify	O
by	O
making	O
use	O
of	O
the	O
results	O
and	O
a	O
matrix	O
a	O
is	O
said	O
to	O
be	O
positive	B
definite	I
denoted	O
by	O
a	O
if	O
wtaw	O
for	O
all	O
values	O
of	O
the	O
vector	O
w	O
equivalently	O
a	O
positive	B
definite	I
matrix	I
has	O
i	O
for	O
all	O
of	O
its	O
eigenvalues	O
can	O
be	O
seen	O
by	O
setting	O
w	O
to	O
each	O
of	O
the	O
eigenvectors	O
in	O
turn	O
and	O
by	O
noting	O
that	O
an	O
arbitrary	O
vector	O
can	O
be	O
expanded	O
as	O
a	O
linear	O
combination	O
of	O
the	O
eigenvectors	O
note	O
that	O
positive	B
definite	I
is	O
not	O
the	O
same	O
as	O
all	O
the	O
elements	O
being	O
positive	O
for	O
example	O
the	O
matrix	O
has	O
eigenvalues	O
and	O
a	O
matrix	O
is	O
said	O
to	O
be	O
positive	O
semidefinite	O
if	O
wtaw	O
holds	O
for	O
all	O
values	O
of	O
w	O
which	O
is	O
denoted	O
a	O
and	O
is	O
equivalent	O
to	O
i	O
appendix	O
d	O
calculus	B
of	I
variations	I
we	O
can	O
think	O
of	O
a	O
function	O
yx	O
as	O
being	O
an	O
operator	O
that	O
for	O
any	O
input	O
value	O
x	O
returns	O
an	O
output	O
value	O
y	O
in	O
the	O
same	O
way	O
we	O
can	O
define	O
a	O
functional	B
f	O
to	O
be	O
an	O
operator	O
that	O
takes	O
a	O
function	O
yx	O
and	O
returns	O
an	O
output	O
value	O
f	O
an	O
example	O
of	O
a	O
functional	B
is	O
the	O
length	O
of	O
a	O
curve	O
drawn	O
in	O
a	O
two-dimensional	O
plane	O
in	O
which	O
the	O
path	O
of	O
the	O
curve	O
is	O
defined	O
in	O
terms	O
of	O
a	O
function	O
in	O
the	O
context	O
of	O
machine	O
learning	B
a	O
widely	O
used	O
functional	B
is	O
the	O
entropy	B
hx	O
for	O
a	O
continuous	O
variable	O
x	O
because	O
for	O
any	O
choice	O
of	O
probability	B
density	B
function	O
px	O
it	O
returns	O
a	O
scalar	O
value	O
representing	O
the	O
entropy	B
of	O
x	O
under	O
that	O
density	B
thus	O
the	O
entropy	B
of	O
px	O
could	O
equally	O
well	O
have	O
been	O
written	O
as	O
hp	O
a	O
common	O
problem	O
in	O
conventional	O
calculus	O
is	O
to	O
find	O
a	O
value	O
of	O
x	O
that	O
maximizes	O
minimizes	O
a	O
function	O
yx	O
similarly	O
in	O
the	O
calculus	B
of	I
variations	I
we	O
seek	O
a	O
function	O
yx	O
that	O
maximizes	O
minimizes	O
a	O
functional	B
f	O
that	O
is	O
of	O
all	O
possible	O
functions	O
yx	O
we	O
wish	O
to	O
find	O
the	O
particular	O
function	O
for	O
which	O
the	O
functional	B
f	O
is	O
a	O
maximum	O
minimum	O
the	O
calculus	B
of	I
variations	I
can	O
be	O
used	O
for	O
instance	O
to	O
show	O
that	O
the	O
shortest	O
path	O
between	O
two	O
points	O
is	O
a	O
straight	O
line	O
or	O
that	O
the	O
maximum	O
entropy	B
distribution	O
is	O
a	O
gaussian	B
if	O
we	O
weren	O
t	O
familiar	O
with	O
the	O
rules	O
of	O
ordinary	O
calculus	O
we	O
could	O
evaluate	O
a	O
conventional	O
derivative	B
dy	O
dx	O
by	O
making	O
a	O
small	O
change	O
to	O
the	O
variable	O
x	O
and	O
then	O
expanding	O
in	O
powers	O
of	O
so	O
that	O
yx	O
yx	O
and	O
finally	O
taking	O
the	O
limit	O
similarly	O
for	O
a	O
function	O
of	O
several	O
variables	O
xd	O
the	O
corresponding	O
partial	O
derivatives	O
are	O
defined	O
by	O
dy	O
dx	O
xd	O
xd	O
y	O
xi	O
the	O
analogous	O
definition	O
of	O
a	O
functional	B
derivative	B
arises	O
when	O
we	O
consider	O
how	O
much	O
a	O
functional	B
f	O
changes	O
when	O
we	O
make	O
a	O
small	O
change	O
to	O
the	O
function	O
d	O
calculus	B
of	I
variations	I
figure	O
a	O
functional	B
derivative	B
can	O
be	O
defined	O
by	O
considering	O
how	O
the	O
value	O
of	O
a	O
functional	B
f	O
changes	O
when	O
the	O
function	O
yx	O
is	O
changed	O
to	O
yx	O
where	O
is	O
an	O
arbitrary	O
function	O
of	O
x	O
yx	O
yx	O
x	O
yx	O
where	O
is	O
an	O
arbitrary	O
function	O
of	O
x	O
as	O
illustrated	O
in	O
figure	O
we	O
denote	O
the	O
functional	B
derivative	B
of	O
ef	O
with	O
respect	O
to	O
fx	O
by	O
f	O
fx	O
and	O
define	O
it	O
by	O
the	O
following	O
relation	O
f	O
f	O
f	O
yx	O
dx	O
this	O
can	O
be	O
seen	O
as	O
a	O
natural	O
extension	O
of	O
in	O
which	O
f	O
now	O
depends	O
on	O
a	O
continuous	O
set	O
of	O
variables	O
namely	O
the	O
values	O
of	O
y	O
at	O
all	O
points	O
x	O
requiring	O
that	O
the	O
functional	B
be	O
stationary	B
with	O
respect	O
to	O
small	O
variations	O
in	O
the	O
function	O
yx	O
gives	O
e	O
yx	O
dx	O
because	O
this	O
must	O
hold	O
for	O
an	O
arbitrary	O
choice	O
of	O
it	O
follows	O
that	O
the	O
functional	B
derivative	B
must	O
vanish	O
to	O
see	O
this	O
imagine	O
choosing	O
a	O
perturbation	O
that	O
is	O
zero	O
everywhere	O
except	O
in	O
the	O
neighbourhood	O
of	O
a	O
in	O
which	O
case	O
the	O
functional	B
derivative	B
must	O
be	O
zero	O
at	O
x	O
however	O
because	O
this	O
must	O
be	O
true	O
for	O
every	O
choice	O
the	O
functional	B
derivative	B
must	O
vanish	O
for	O
all	O
values	O
of	O
x	O
consider	O
a	O
functional	B
that	O
is	O
defined	O
by	O
an	O
integral	O
over	O
a	O
function	O
gy	O
y	O
that	O
depends	O
on	O
both	O
yx	O
and	O
its	O
derivative	B
y	O
dence	O
on	O
x	O
f	O
g	O
y	O
x	O
as	O
well	O
as	O
having	O
a	O
direct	O
x	O
dx	O
where	O
the	O
value	O
of	O
yx	O
is	O
assumed	O
to	O
be	O
fixed	O
at	O
the	O
boundary	O
of	O
the	O
region	O
of	O
integration	O
might	O
be	O
at	O
infinity	O
if	O
we	O
now	O
consider	O
variations	O
in	O
the	O
function	O
yx	O
we	O
obtain	O
f	O
f	O
g	O
y	O
g	O
dx	O
we	O
now	O
have	O
to	O
cast	O
this	O
in	O
the	O
form	O
to	O
do	O
so	O
we	O
integrate	O
the	O
second	O
term	O
by	O
parts	O
and	O
make	O
use	O
of	O
the	O
fact	O
that	O
must	O
vanish	O
at	O
the	O
boundary	O
of	O
the	O
integral	O
yx	O
is	O
fixed	O
at	O
the	O
boundary	O
this	O
gives	O
d	O
dx	O
f	O
f	O
dx	O
g	O
g	O
y	O
d	O
calculus	B
of	I
variations	I
from	O
which	O
we	O
can	O
read	O
off	O
the	O
functional	B
derivative	B
by	O
comparison	O
with	O
requiring	O
that	O
the	O
functional	B
derivative	B
vanishes	O
then	O
gives	O
g	O
y	O
d	O
dx	O
g	O
which	O
are	O
known	O
as	O
the	O
euler-lagrange	B
equations	I
for	O
example	O
if	O
g	O
then	O
the	O
euler-lagrange	B
equations	I
take	O
the	O
form	O
yx	O
this	O
second	B
order	I
differential	B
equation	O
can	O
be	O
solved	O
for	O
yx	O
by	O
making	O
use	O
of	O
the	O
boundary	O
conditions	O
on	O
yx	O
often	O
we	O
consider	O
functionals	O
defined	O
by	O
integrals	O
whose	O
integrands	O
take	O
the	O
form	O
gy	O
x	O
and	O
that	O
do	O
not	O
depend	O
on	O
the	O
derivatives	O
of	O
yx	O
in	O
this	O
case	O
stationarity	O
simply	O
requires	O
that	O
g	O
yx	O
for	O
all	O
values	O
of	O
x	O
if	O
we	O
are	O
optimizing	O
a	O
functional	B
with	O
respect	O
to	O
a	O
probability	B
distribution	O
then	O
we	O
need	O
to	O
maintain	O
the	O
normalization	O
constraint	O
on	O
the	O
probabilities	O
this	O
is	O
often	O
most	O
conveniently	O
done	O
using	O
a	O
lagrange	B
multiplier	I
which	O
then	O
allows	O
an	O
unconstrained	O
optimization	O
to	O
be	O
performed	O
the	O
extension	O
of	O
the	O
above	O
results	O
to	O
a	O
multidimensional	O
variable	O
x	O
is	O
straightforward	O
for	O
a	O
more	O
comprehensive	O
discussion	O
of	O
the	O
calculus	B
of	I
variations	I
see	O
sagan	O
appendix	O
e	O
appendix	O
e	O
lagrange	B
multipliers	O
lagrange	B
multipliers	O
also	O
sometimes	O
called	O
undetermined	O
multipliers	O
are	O
used	O
to	O
find	O
the	O
stationary	B
points	O
of	O
a	O
function	O
of	O
several	O
variables	O
subject	O
to	O
one	O
or	O
more	O
constraints	O
consider	O
the	O
problem	O
of	O
finding	O
the	O
maximum	O
of	O
a	O
function	O
subject	O
to	O
a	O
constraint	O
relating	O
and	O
which	O
we	O
write	O
in	O
the	O
form	O
one	O
approach	O
would	O
be	O
to	O
solve	O
the	O
constraint	O
equation	O
and	O
thus	O
express	O
as	O
a	O
function	O
of	O
in	O
the	O
form	O
this	O
can	O
then	O
be	O
substituted	O
into	O
to	O
give	O
a	O
function	O
of	O
alone	O
of	O
the	O
form	O
the	O
maximum	O
with	O
respect	O
to	O
could	O
then	O
be	O
found	O
by	O
differentiation	O
in	O
the	O
usual	O
way	O
to	O
give	O
the	O
stationary	B
value	O
with	O
the	O
corresponding	O
value	O
of	O
given	O
by	O
one	O
problem	O
with	O
this	O
approach	O
is	O
that	O
it	O
may	O
be	O
difficult	O
to	O
find	O
an	O
analytic	O
solution	O
of	O
the	O
constraint	O
equation	O
that	O
allows	O
to	O
be	O
expressed	O
as	O
an	O
explicit	O
function	O
of	O
also	O
this	O
approach	O
treats	O
and	O
differently	O
and	O
so	O
spoils	O
the	O
natural	O
symmetry	O
between	O
these	O
variables	O
a	O
more	O
elegant	O
and	O
often	O
simpler	O
approach	O
is	O
based	O
on	O
the	O
introduction	O
of	O
a	O
parameter	O
called	O
a	O
lagrange	B
multiplier	I
we	O
shall	O
motivate	O
this	O
technique	O
from	O
a	O
geometrical	O
perspective	O
consider	O
a	O
d-dimensional	O
variable	O
x	O
with	O
components	O
xd	O
the	O
constraint	O
equation	O
gx	O
then	O
represents	O
a	O
surface	O
in	O
x-space	O
as	O
indicated	O
in	O
figure	O
we	O
first	O
note	O
that	O
at	O
any	O
point	O
on	O
the	O
constraint	O
surface	O
the	O
gradient	O
gx	O
of	O
the	O
constraint	O
function	O
will	O
be	O
orthogonal	O
to	O
the	O
surface	O
to	O
see	O
this	O
consider	O
a	O
point	O
x	O
that	O
lies	O
on	O
the	O
constraint	O
surface	O
and	O
consider	O
a	O
nearby	O
point	O
x	O
that	O
also	O
lies	O
on	O
the	O
surface	O
if	O
we	O
make	O
a	O
taylor	O
expansion	O
around	O
x	O
we	O
have	O
gx	O
gx	O
gx	O
because	O
both	O
x	O
and	O
x	O
lie	O
on	O
the	O
constraint	O
surface	O
we	O
have	O
gx	O
gx	O
and	O
hence	O
gx	O
in	O
the	O
limit	O
we	O
have	O
gx	O
and	O
because	O
is	O
e	O
lagrange	B
multipliers	O
figure	O
a	O
geometrical	O
picture	O
of	O
the	O
technique	O
of	O
lagrange	B
multipliers	O
in	O
which	O
we	O
seek	O
to	O
maximize	O
a	O
function	O
f	O
subject	O
to	O
the	O
constraint	O
gx	O
if	O
x	O
is	O
d	O
dimensional	O
the	O
constraint	O
gx	O
corresponds	O
to	O
a	O
subspace	O
of	O
dimensionality	O
d	O
indicated	O
by	O
the	O
red	O
curve	O
the	O
problem	O
can	O
be	O
solved	O
by	O
optimizing	O
the	O
lagrangian	B
function	O
lx	O
f	O
gx	O
fx	O
xa	O
gx	O
gx	O
then	O
parallel	O
to	O
the	O
constraint	O
surface	O
gx	O
we	O
see	O
that	O
the	O
vector	O
g	O
is	O
normal	O
to	O
the	O
surface	O
next	O
we	O
seek	O
a	O
point	O
on	O
the	O
constraint	O
surface	O
such	O
that	O
fx	O
is	O
maximized	O
such	O
a	O
point	O
must	O
have	O
the	O
property	O
that	O
the	O
vector	O
fx	O
is	O
also	O
orthogonal	O
to	O
the	O
constraint	O
surface	O
as	O
illustrated	O
in	O
figure	O
because	O
otherwise	O
we	O
could	O
increase	O
the	O
value	O
of	O
fx	O
by	O
moving	O
a	O
short	O
distance	O
along	O
the	O
constraint	O
surface	O
thus	O
f	O
and	O
g	O
are	O
parallel	O
anti-parallel	O
vectors	O
and	O
so	O
there	O
must	O
exist	O
a	O
parameter	O
such	O
that	O
where	O
is	O
known	O
as	O
a	O
lagrange	B
multiplier	I
note	O
that	O
can	O
have	O
either	O
sign	O
at	O
this	O
point	O
it	O
is	O
convenient	O
to	O
introduce	O
the	O
lagrangian	B
function	O
defined	O
by	O
f	O
g	O
lx	O
fx	O
gx	O
the	O
constrained	O
stationarity	O
condition	O
is	O
obtained	O
by	O
setting	O
xl	O
furthermore	O
the	O
condition	O
l	O
leads	O
to	O
the	O
constraint	O
equation	O
gx	O
thus	O
to	O
find	O
the	O
maximum	O
of	O
a	O
function	O
fx	O
subject	O
to	O
the	O
constraint	O
gx	O
we	O
define	O
the	O
lagrangian	B
function	O
given	O
by	O
and	O
we	O
then	O
find	O
the	O
stationary	B
point	O
of	O
lx	O
with	O
respect	O
to	O
both	O
x	O
and	O
for	O
a	O
d-dimensional	O
vector	O
x	O
this	O
gives	O
d	O
equations	O
that	O
determine	O
both	O
the	O
stationary	B
point	O
and	O
the	O
value	O
of	O
if	O
we	O
are	O
only	O
interested	O
in	O
then	O
we	O
can	O
eliminate	O
from	O
the	O
stationarity	O
equations	O
without	O
needing	O
to	O
find	O
its	O
value	O
the	O
term	O
undetermined	O
multiplier	O
as	O
a	O
simple	O
example	O
suppose	O
we	O
wish	O
to	O
find	O
the	O
stationary	B
point	O
of	O
the	O
function	O
subject	O
to	O
the	O
constraint	O
as	O
illustrated	O
in	O
figure	O
the	O
corresponding	O
lagrangian	B
function	O
is	O
given	O
by	O
the	O
conditions	O
for	O
this	O
lagrangian	B
to	O
be	O
stationary	B
with	O
respect	O
to	O
and	O
give	O
the	O
following	O
coupled	O
equations	O
lx	O
e	O
lagrange	B
multipliers	O
figure	O
a	O
simple	O
example	O
of	O
the	O
use	O
of	O
lagrange	B
multipliers	O
in	O
which	O
the	O
aim	O
is	O
to	O
maximize	O
f	O
subject	O
to	O
the	O
constraint	O
where	O
the	O
circles	O
show	O
contours	O
of	O
the	O
function	O
f	O
and	O
the	O
diagonal	B
line	O
shows	O
the	O
constraint	O
surface	O
solution	O
of	O
these	O
equations	O
then	O
gives	O
the	O
stationary	B
point	O
as	O
the	O
corresponding	O
value	O
for	O
the	O
lagrange	B
multiplier	I
is	O
and	O
so	O
far	O
we	O
have	O
considered	O
the	O
problem	O
of	O
maximizing	O
a	O
function	O
subject	O
to	O
an	O
equality	B
constraint	I
of	O
the	O
form	O
gx	O
we	O
now	O
consider	O
the	O
problem	O
of	O
maximizing	O
fx	O
subject	O
to	O
an	O
inequality	B
constraint	I
of	O
the	O
form	O
gx	O
as	O
illustrated	O
in	O
figure	O
there	O
are	O
now	O
two	O
kinds	O
of	O
solution	O
possible	O
according	O
to	O
whether	O
the	O
constrained	O
stationary	B
point	O
lies	O
in	O
the	O
region	O
where	O
gx	O
in	O
which	O
case	O
the	O
constraint	O
is	O
inactive	O
or	O
whether	O
it	O
lies	O
on	O
the	O
boundary	O
gx	O
in	O
which	O
case	O
the	O
constraint	O
is	O
said	O
to	O
be	O
active	O
in	O
the	O
former	O
case	O
the	O
function	O
gx	O
plays	O
no	O
role	O
and	O
so	O
the	O
stationary	B
condition	O
is	O
simply	O
fx	O
this	O
again	O
corresponds	O
to	O
a	O
stationary	B
point	O
of	O
the	O
lagrange	B
function	O
but	O
this	O
time	O
with	O
the	O
latter	O
case	O
where	O
the	O
solution	O
lies	O
on	O
the	O
boundary	O
is	O
analogous	O
to	O
the	O
equality	B
constraint	I
discussed	O
previously	O
and	O
corresponds	O
to	O
a	O
stationary	B
point	O
of	O
the	O
lagrange	B
function	O
with	O
now	O
however	O
the	O
sign	O
of	O
the	O
lagrange	B
multiplier	I
is	O
crucial	O
because	O
the	O
function	O
fx	O
will	O
only	O
be	O
at	O
a	O
maximum	O
if	O
its	O
gradient	O
is	O
oriented	O
away	O
from	O
the	O
region	O
gx	O
as	O
illustrated	O
in	O
figure	O
we	O
therefore	O
have	O
fx	O
gx	O
for	O
some	O
value	O
of	O
for	O
either	O
of	O
these	O
two	O
cases	O
the	O
product	O
gx	O
thus	O
the	O
solution	O
to	O
the	O
figure	O
illustration	O
of	O
f	O
subject	O
gx	O
the	O
problem	O
of	O
maximizing	O
to	O
the	O
inequality	B
constraint	I
fx	O
xa	O
gx	O
xb	O
gx	O
gx	O
e	O
lagrange	B
multipliers	O
problem	O
of	O
maximizing	O
fx	O
subject	O
to	O
gx	O
is	O
obtained	O
by	O
optimizing	O
the	O
lagrange	B
function	O
with	O
respect	O
to	O
x	O
and	O
subject	O
to	O
the	O
conditions	O
gx	O
gx	O
these	O
are	O
known	O
as	O
the	O
karush-kuhn-tucker	B
conditions	I
kuhn	O
and	O
tucker	O
note	O
that	O
if	O
we	O
wish	O
to	O
minimize	O
than	O
maximize	O
the	O
function	O
fx	O
subject	O
to	O
an	O
inequality	B
constraint	I
gx	O
then	O
we	O
minimize	O
the	O
lagrangian	B
function	O
lx	O
fx	O
gx	O
with	O
respect	O
to	O
x	O
again	O
subject	O
to	O
finally	O
it	O
is	O
straightforward	O
to	O
extend	O
the	O
technique	O
of	O
lagrange	B
multipliers	O
to	O
the	O
case	O
of	O
multiple	O
equality	O
and	O
inequality	O
constraints	O
suppose	O
we	O
wish	O
to	O
maximize	O
fx	O
subject	O
to	O
gjx	O
for	O
j	O
j	O
and	O
hkx	O
for	O
k	O
k	O
we	O
then	O
introduce	O
lagrange	B
multipliers	O
j	O
and	O
k	O
and	O
then	O
optimize	O
the	O
lagrangian	B
function	O
given	O
by	O
lx	O
j	O
k	O
fx	O
jgjx	O
khkx	O
appendix	O
d	O
subject	O
to	O
k	O
and	O
khkx	O
for	O
k	O
k	O
extensions	O
to	O
constrained	O
functional	B
derivatives	O
are	O
similarly	O
straightforward	O
for	O
a	O
more	O
detailed	O
discussion	O
of	O
the	O
technique	O
of	O
lagrange	B
multipliers	O
see	O
nocedal	O
and	O
wright	O
references	O
references	O
abramowitz	O
m	O
and	O
i	O
a	O
stegun	O
handbook	O
of	O
mathematical	O
functions	O
dover	O
adler	O
s	O
l	O
over-relaxation	B
method	O
for	O
the	O
monte	O
carlo	O
evaluation	O
of	O
the	O
partition	B
function	I
for	O
multiquadratic	O
actions	O
physical	O
review	O
d	O
ahn	O
j	O
h	O
and	O
j	O
h	O
oh	O
a	O
constrained	O
em	B
algorithm	I
for	O
principal	B
component	I
analysis	I
neural	O
computation	O
aizerman	O
m	O
a	O
e	O
m	O
braverman	O
and	O
l	O
i	O
rozonoer	O
the	O
probability	B
problem	O
of	O
pattern	O
recognition	O
learning	B
and	O
the	O
method	O
of	O
potential	O
functions	O
automation	O
and	O
remote	O
control	O
akaike	O
h	O
a	O
new	O
look	O
at	O
statistical	O
model	O
identification	O
ieee	O
transactions	O
on	O
automatic	O
control	O
ali	O
s	O
m	O
and	O
s	O
d	O
silvey	O
a	O
general	O
class	O
of	O
coefficients	O
of	O
divergence	O
of	O
one	O
distribution	O
from	O
another	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
allwein	O
e	O
l	O
r	O
e	O
schapire	O
and	O
y	O
singer	O
reducing	O
multiclass	B
to	O
binary	O
a	O
unifying	O
approach	O
for	O
margin	B
classifiers	O
journal	O
of	O
machine	O
learning	B
research	O
amari	O
s	O
differential-geometrical	O
methods	O
in	O
statistics	O
springer	O
amari	O
s	O
a	O
cichocki	O
and	O
h	O
h	O
yang	O
a	O
new	O
learning	B
algorithm	O
for	O
blind	O
signal	O
separation	O
in	O
d	O
s	O
touretzky	O
m	O
c	O
mozer	O
and	O
m	O
e	O
hasselmo	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
amari	O
s	O
i	O
natural	O
gradient	O
works	O
ciently	O
in	O
learning	B
neural	O
computation	O
anderson	O
j	O
a	O
and	O
e	O
rosenfeld	O
neurocomputing	O
foundations	O
of	O
research	O
mit	O
press	O
anderson	O
t	O
w	O
asymptotic	O
theory	B
for	O
principal	B
component	I
analysis	I
annals	O
of	O
mathematical	O
statistics	O
andrieu	O
c	O
n	O
de	O
freitas	O
a	O
doucet	O
and	O
m	O
i	O
jordan	O
an	O
introduction	O
to	O
mcmc	O
for	O
machine	O
learning	B
machine	O
learning	B
anthony	O
m	O
and	O
n	O
biggs	O
an	O
introduction	O
to	O
computational	B
learning	B
theory	B
cambridge	O
university	O
press	O
attias	O
h	O
independent	B
factor	B
analysis	I
neu	O
ral	O
computation	O
attias	O
h	O
inferring	O
parameters	O
and	O
structure	O
of	O
latent	B
variable	I
models	O
by	O
variational	B
bayes	B
in	O
k	O
b	O
laskey	O
and	O
h	O
prade	O
references	O
uncertainty	O
in	O
artificial	O
intelligence	O
proceedings	O
of	O
the	O
fifth	O
conference	O
pp	O
morgan	O
kaufmann	O
bach	O
f	O
r	O
and	O
m	O
i	O
jordan	O
kernel	O
independent	B
component	I
analysis	I
journal	O
of	O
machine	O
learning	B
research	O
bakir	O
g	O
h	O
j	O
weston	O
and	O
b	O
sch	O
olkopf	O
learning	B
to	O
find	O
pre-images	O
in	O
s	O
thrun	O
l	O
k	O
saul	O
and	O
b	O
sch	O
olkopf	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
baldi	O
p	O
and	O
s	O
brunak	O
bioinformatics	O
the	O
machine	O
learning	B
approach	O
ed	O
mit	O
press	O
baldi	O
p	O
and	O
k	O
hornik	O
neural	O
networks	O
and	O
principal	B
component	I
analysis	I
learning	B
from	O
examples	O
without	O
local	B
minima	O
neural	O
networks	O
barber	O
d	O
and	O
c	O
m	O
bishop	O
bayesian	B
model	B
comparison	I
by	O
monte	O
carlo	O
chaining	B
in	O
m	O
mozer	O
m	O
jordan	O
and	O
t	O
petsche	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
barber	O
d	O
and	O
c	O
m	O
bishop	O
ensemble	O
learning	B
for	O
multi-layer	O
networks	O
in	O
m	O
i	O
jordan	O
k	O
j	O
kearns	O
and	O
s	O
a	O
solla	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
barber	O
d	O
and	O
c	O
m	O
bishop	O
ensemble	O
learning	B
in	O
bayesian	B
neural	O
networks	O
in	O
c	O
m	O
bishop	O
generalization	B
in	O
neural	O
networks	O
and	O
machine	O
learning	B
pp	O
springer	O
bartholomew	O
d	O
j	O
latent	B
variable	I
models	O
and	O
factor	B
analysis	I
charles	O
griffin	O
basilevsky	O
a	O
statistical	O
factor	B
analysis	I
and	O
related	O
methods	O
theory	B
and	O
applications	O
wiley	O
baum	O
l	O
e	O
an	O
inequality	O
and	O
associated	O
maximization	O
technique	O
in	O
statistical	O
estimation	O
of	O
probabilistic	O
functions	O
of	O
markov	O
processes	O
inequalities	O
becker	O
s	O
and	O
y	O
le	O
cun	O
improving	O
the	O
convergence	O
of	O
back-propagation	O
learning	B
with	O
second	B
order	I
methods	O
in	O
d	O
touretzky	O
g	O
e	O
hinton	O
and	O
t	O
j	O
sejnowski	O
proceedings	O
of	O
the	O
connectionist	O
models	O
summer	O
school	O
pp	O
morgan	O
kaufmann	O
bell	O
a	O
j	O
and	O
t	O
j	O
sejnowski	O
an	O
information	O
maximization	O
approach	O
to	O
blind	O
separation	O
and	O
blind	O
deconvolution	O
neural	O
computation	O
bellman	O
r	O
adaptive	O
control	O
processes	O
a	O
guided	O
tour	O
princeton	O
university	O
press	O
bengio	O
y	O
and	O
p	O
frasconi	O
an	O
input	O
output	O
hmm	O
architecture	O
in	O
g	O
tesauro	O
d	O
s	O
touretzky	O
and	O
t	O
k	O
leen	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
bennett	O
k	O
p	O
robust	O
linear	O
programming	O
discrimination	O
of	O
two	O
linearly	B
separable	I
sets	O
optimization	O
methods	O
and	O
software	O
berger	O
j	O
o	O
statistical	O
decision	B
theory	B
and	O
bayesian	B
analysis	I
ed	O
springer	O
bernardo	O
j	O
m	O
and	O
a	O
f	O
m	O
smith	O
bayesian	B
theory	B
wiley	O
berrou	O
c	O
a	O
glavieux	O
and	O
p	O
thitimajshima	O
near	O
shannon	B
limit	O
error-correcting	O
coding	O
and	O
decoding	O
turbo-codes	O
in	O
proceedings	O
icc	O
pp	O
besag	O
j	O
on	O
spatio-temporal	O
models	O
and	O
markov	O
fields	O
in	O
transactions	O
of	O
the	O
prague	O
conference	O
on	O
information	B
theory	B
statistical	O
decision	O
functions	O
and	O
random	O
processes	O
pp	O
academia	O
bather	O
j	O
decision	B
theory	B
an	O
introduction	O
to	O
dynamic	B
programming	I
and	O
sequential	O
decisions	O
wiley	O
besag	O
j	O
on	O
the	O
statistical	O
analysis	O
of	O
dirty	O
pictures	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
baudat	O
g	O
and	O
f	O
anouar	O
generalized	B
discriminant	O
analysis	O
using	O
a	O
kernel	O
approach	O
neural	O
computation	O
besag	O
j	O
p	O
j	O
green	O
d	O
hidgon	O
and	O
k	O
megersen	O
bayesian	B
computation	O
and	O
stochastic	B
systems	O
statistical	O
science	O
references	O
bishop	O
c	O
m	O
a	O
fast	O
procedure	O
for	O
retraining	O
the	O
multilayer	B
perceptron	B
international	O
journal	O
of	O
neural	O
systems	O
bishop	O
c	O
m	O
exact	O
calculation	O
of	O
the	O
hessian	B
matrix	I
for	O
the	O
multilayer	B
perceptron	B
neural	O
computation	O
bishop	O
c	O
m	O
curvature-driven	O
smoothing	O
a	O
learning	B
algorithm	O
for	O
feedforward	O
networks	O
ieee	O
transactions	O
on	O
neural	O
networks	O
bishop	O
c	O
m	O
novelty	B
detection	I
and	O
neural	B
network	I
validation	O
iee	O
proceedings	O
vision	O
image	O
and	O
signal	O
processing	O
special	O
issue	O
on	O
applications	O
of	O
neural	O
networks	O
bishop	O
c	O
m	O
neural	O
networks	O
for	O
pattern	O
recognition	O
oxford	O
university	O
press	O
bishop	O
c	O
m	O
training	B
with	O
noise	O
is	O
equivalent	O
to	O
tikhonov	B
regularization	B
neural	O
computation	O
bishop	O
c	O
m	O
bayesian	B
pca	O
in	O
m	O
s	O
kearns	O
s	O
a	O
solla	O
and	O
d	O
a	O
cohn	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
variational	B
principal	O
bishop	O
c	O
m	O
components	O
in	O
proceedings	O
ninth	O
international	O
conference	O
on	O
artificial	O
neural	O
networks	O
icann	O
volume	O
pp	O
iee	O
bishop	O
c	O
m	O
and	O
g	O
d	O
james	O
analysis	O
of	O
multiphase	O
flows	O
using	O
dual-energy	B
gamma	B
densitometry	I
and	O
neural	O
networks	O
nuclear	O
instruments	O
and	O
methods	O
in	O
physics	O
research	O
bishop	O
c	O
m	O
and	O
i	O
t	O
nabney	O
modelling	O
conditional	B
probability	B
distributions	O
for	O
periodic	O
variables	O
neural	O
computation	O
bishop	O
c	O
m	O
and	O
i	O
t	O
nabney	O
pattern	O
recognition	O
and	O
machine	O
learning	B
a	O
matlab	O
companion	O
springer	O
in	O
preparation	O
bishop	O
c	O
m	O
d	O
spiegelhalter	O
and	O
j	O
winn	O
vibes	O
a	O
variational	B
inference	B
engine	O
for	O
bayesian	B
networks	O
in	O
s	O
becker	O
s	O
thrun	O
and	O
k	O
obermeyer	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
bishop	O
c	O
m	O
and	O
m	O
svens	O
en	O
bayesian	B
hierarchical	B
mixtures	O
of	O
experts	O
in	O
u	O
kjaerulff	O
and	O
c	O
meek	O
proceedings	O
nineteenth	O
conference	O
on	O
uncertainty	O
in	O
artificial	O
intelligence	O
pp	O
morgan	O
kaufmann	O
bishop	O
c	O
m	O
m	O
svens	O
en	O
and	O
g	O
e	O
hinton	O
distinguishing	O
text	O
from	O
graphics	O
in	O
online	O
handwritten	O
ink	O
in	O
f	O
kimura	O
and	O
h	O
fujisawa	O
proceedings	O
ninth	O
international	O
workshop	O
on	O
frontiers	O
in	O
handwriting	B
recognition	I
tokyo	O
japan	O
pp	O
bishop	O
c	O
m	O
m	O
svens	O
en	O
and	O
c	O
k	O
i	O
williams	O
em	B
optimization	O
of	O
latent	B
variable	I
density	B
models	O
in	O
d	O
s	O
touretzky	O
m	O
c	O
mozer	O
and	O
m	O
e	O
hasselmo	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
bishop	O
c	O
m	O
m	O
svens	O
en	O
and	O
c	O
k	O
i	O
williams	O
gtm	O
a	O
principled	O
alternative	O
to	O
the	O
self-organizing	B
map	I
in	O
m	O
c	O
mozer	O
m	O
i	O
jordan	O
and	O
t	O
petche	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
bishop	O
c	O
m	O
m	O
svens	O
en	O
and	O
c	O
k	O
i	O
williams	O
magnification	O
factors	O
for	O
the	O
gtm	O
algorithm	O
in	O
proceedings	O
iee	O
fifth	O
international	O
conference	O
on	O
artificial	O
neural	O
networks	O
cambridge	O
u	O
k	O
pp	O
institute	O
of	O
electrical	O
engineers	O
bishop	O
c	O
m	O
m	O
svens	O
en	O
and	O
c	O
k	O
i	O
williams	O
developments	O
of	O
the	O
generative	B
topographic	I
mapping	I
neurocomputing	O
bishop	O
c	O
m	O
m	O
svens	O
en	O
and	O
c	O
k	O
i	O
williams	O
gtm	O
the	O
generative	B
topographic	I
mapping	I
neural	O
computation	O
bishop	O
c	O
m	O
and	O
m	O
e	O
tipping	O
a	O
hierarchical	B
latent	B
variable	I
model	O
for	O
data	O
visualization	B
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
references	O
bishop	O
c	O
m	O
and	O
j	O
winn	O
non-linear	O
bayesian	B
image	O
modelling	O
in	O
proceedings	O
sixth	O
european	O
conference	O
on	O
computer	O
vision	O
dublin	O
volume	O
pp	O
springer	O
blei	O
d	O
m	O
m	O
i	O
jordan	O
and	O
a	O
y	O
ng	O
hierarchical	B
bayesian	B
models	O
for	O
applications	O
in	O
information	O
retrieval	O
in	O
j	O
m	O
b	O
et	O
al	O
bayesian	B
statistics	O
pp	O
oxford	O
university	O
press	O
block	O
h	O
d	O
the	O
perceptron	B
a	O
model	O
for	O
brain	O
functioning	O
reviews	O
of	O
modern	O
physics	O
reprinted	O
in	O
anderson	O
and	O
rosenfeld	O
blum	O
j	O
a	O
multidimensional	O
stochastic	B
approximation	O
methods	O
annals	O
of	O
mathematical	O
statistics	O
bodlaender	O
h	O
a	O
tourist	O
guide	O
through	O
treewidth	B
acta	O
cybernetica	O
boser	O
b	O
e	O
i	O
m	O
guyon	O
and	O
v	O
n	O
vapnik	O
a	O
training	B
algorithm	O
for	O
optimal	O
margin	B
classifiers	O
in	O
d	O
haussler	O
proceedings	O
fifth	O
annual	O
workshop	O
on	O
computational	B
learning	B
theory	B
pp	O
acm	O
bourlard	O
h	O
and	O
y	O
kamp	O
auto-association	O
by	O
multilayer	O
perceptrons	O
and	O
singular	B
value	I
decomposition	I
biological	O
cybernetics	O
box	O
g	O
e	O
p	O
g	O
m	O
jenkins	O
and	O
g	O
c	O
reinsel	O
time	O
series	O
analysis	O
prentice	O
hall	O
box	O
g	O
e	O
p	O
and	O
g	O
c	O
tao	O
bayesian	B
infer	O
ence	O
in	O
statistical	O
analysis	O
wiley	O
boyd	O
s	O
and	O
l	O
vandenberghe	O
convex	O
opti	O
mization	O
cambridge	O
university	O
press	O
boyen	O
x	O
and	O
d	O
koller	O
tractable	O
inference	B
for	O
complex	O
stochastic	B
processes	O
in	O
g	O
f	O
cooper	O
and	O
s	O
moral	O
proceedings	O
annual	O
conference	O
on	O
uncertainty	O
in	O
artificial	O
intelligence	O
pp	O
morgan	O
kaufmann	O
breiman	O
l	O
bagging	B
predictors	O
machine	O
learning	B
breiman	O
l	O
j	O
h	O
friedman	O
r	O
a	O
olshen	O
and	O
p	O
j	O
stone	O
classification	B
and	I
regression	B
trees	I
wadsworth	O
brooks	O
s	O
p	O
markov	B
chain	I
monte	I
carlo	I
method	O
and	O
its	O
application	O
the	O
statistician	O
broomhead	O
d	O
s	O
and	O
d	O
lowe	O
multivariable	O
functional	B
interpolation	O
and	O
adaptive	O
networks	O
complex	O
systems	O
buntine	O
w	O
and	O
a	O
weigend	O
bayesian	B
back	O
propagation	O
complex	O
systems	O
buntine	O
w	O
l	O
and	O
a	O
s	O
weigend	O
computing	O
second	O
derivatives	O
in	O
feed-forward	O
networks	O
a	O
review	O
ieee	O
transactions	O
on	O
neural	O
networks	O
burges	O
c	O
j	O
c	O
a	O
tutorial	O
on	O
support	B
vector	I
machines	O
for	O
pattern	O
recognition	O
knowledge	O
discovery	O
and	O
data	O
mining	O
cardoso	O
j	O
-f	O
blind	O
signal	O
separation	O
statistical	O
principles	O
proceedings	O
of	O
the	O
ieee	O
casella	O
g	O
and	O
r	O
l	O
berger	O
statistical	O
in	O
ference	O
ed	O
duxbury	O
castillo	O
e	O
j	O
m	O
guti	O
errez	O
and	O
a	O
s	O
hadi	O
expert	O
systems	O
and	O
probabilistic	O
network	O
models	O
springer	O
chan	O
k	O
t	O
lee	O
and	O
t	O
j	O
sejnowski	O
variational	B
bayesian	B
learning	B
of	O
ica	O
with	O
missing	B
data	I
neural	O
computation	O
chen	O
a	O
m	O
h	O
lu	O
and	O
r	O
hecht-nielsen	O
on	O
the	O
geometry	O
of	O
feedforward	O
neural	B
network	I
error	B
surfaces	O
neural	O
computation	O
chen	O
m	O
h	O
q	O
m	O
shao	O
and	O
j	O
g	O
ibrahim	O
monte	O
carlo	O
methods	O
for	O
bayesian	B
computation	O
springer	O
boykov	O
y	O
o	O
veksler	O
and	O
r	O
zabih	O
fast	O
approximate	O
energy	O
minimization	O
via	O
graph	O
cuts	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
chen	O
s	O
c	O
f	O
n	O
cowan	O
and	O
p	O
m	O
grant	O
orthogonal	B
least	I
squares	I
learning	B
algorithm	O
for	O
radial	B
basis	B
function	I
networks	O
ieee	O
transactions	O
on	O
neural	O
networks	O
choudrey	O
r	O
a	O
and	O
s	O
j	O
roberts	O
variational	B
mixture	B
of	O
bayesian	B
independent	B
component	O
analyzers	O
neural	O
computation	O
clifford	O
p	O
markov	O
random	O
fields	O
in	O
statistics	O
in	O
g	O
r	O
grimmett	O
and	O
d	O
j	O
a	O
welsh	O
disorder	O
in	O
physical	O
systems	O
a	O
volume	O
in	O
honour	O
of	O
john	O
m	O
hammersley	O
pp	O
oxford	O
university	O
press	O
collins	O
m	O
s	O
dasgupta	O
and	O
r	O
e	O
schapire	O
a	O
generalization	B
of	O
principal	B
component	I
analysis	I
to	O
the	O
exponential	B
family	I
in	O
t	O
g	O
dietterich	O
s	O
becker	O
and	O
z	O
ghahramani	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
comon	O
p	O
c	O
jutten	O
and	O
j	O
herault	O
blind	B
source	I
separation	I
problems	O
statement	O
signal	O
processing	O
corduneanu	O
a	O
and	O
c	O
m	O
bishop	O
variational	B
bayesian	B
model	B
selection	I
for	O
mixture	B
distributions	O
in	O
t	O
richardson	O
and	O
t	O
jaakkola	O
proceedings	O
eighth	O
international	O
conference	O
on	O
artificial	O
intelligence	O
and	O
statistics	O
pp	O
morgan	O
kaufmann	O
cormen	O
t	O
h	O
c	O
e	O
leiserson	O
r	O
l	O
rivest	O
and	O
c	O
stein	O
introduction	O
to	O
algorithms	O
ed	O
mit	O
press	O
cortes	O
c	O
and	O
v	O
n	O
vapnik	O
support	B
vector	I
networks	O
machine	O
learning	B
cotter	O
n	O
e	O
the	O
stone-weierstrass	O
theorem	O
and	O
its	O
application	O
to	O
neural	O
networks	O
ieee	O
transactions	O
on	O
neural	O
networks	O
cover	O
t	O
and	O
p	O
hart	O
nearest	O
neighbor	O
pattern	O
classification	B
ieee	O
transactions	O
on	O
information	B
theory	B
cover	O
t	O
m	O
and	O
j	O
a	O
thomas	O
elements	O
of	O
information	B
theory	B
wiley	O
cowell	O
r	O
g	O
a	O
p	O
dawid	O
s	O
l	O
lauritzen	O
and	O
d	O
j	O
spiegelhalter	O
probabilistic	O
networks	O
and	O
expert	O
systems	O
springer	O
cox	O
r	O
t	O
probability	B
frequency	O
and	O
reasonable	O
expectation	B
american	O
journal	O
of	O
physics	O
references	O
cox	O
t	O
f	O
and	O
m	O
a	O
a	O
cox	O
multidimensional	B
scaling	I
ed	O
chapman	O
and	O
hall	O
cressie	O
n	O
statistics	O
for	O
spatial	O
data	O
wiley	O
cristianini	O
n	O
and	O
j	O
shawe-taylor	O
support	B
vector	I
machines	O
and	O
other	O
kernel-based	O
learning	B
methods	O
cambridge	O
university	O
press	O
csat	O
o	O
l	O
and	O
m	O
opper	O
sparse	O
on-line	O
gaussian	B
processes	O
neural	O
computation	O
csiszar	O
i	O
and	O
g	O
tusnady	O
information	B
geometry	I
and	O
alternating	O
minimization	O
procedures	O
statistics	O
and	O
decisions	O
cybenko	O
g	O
approximation	O
by	O
superpositions	O
of	O
a	O
sigmoidal	O
function	O
mathematics	O
of	O
control	O
signals	O
and	O
systems	O
dawid	O
a	O
p	O
conditional	B
independence	I
in	O
statistical	O
theory	B
discussion	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
dawid	O
a	O
p	O
conditional	B
independence	I
for	O
statistical	O
operations	O
annals	O
of	O
statistics	O
definetti	O
b	O
theory	B
of	O
probability	B
wiley	O
and	O
sons	O
dempster	O
a	O
p	O
n	O
m	O
laird	O
and	O
d	O
b	O
rubin	O
maximum	B
likelihood	I
from	O
incomplete	O
data	O
via	O
the	O
em	B
algorithm	I
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
denison	O
d	O
g	O
t	O
c	O
c	O
holmes	O
b	O
k	O
mallick	O
and	O
a	O
f	O
m	O
smith	O
bayesian	B
methods	O
for	O
nonlinear	O
classification	B
and	O
regression	B
wiley	O
diaconis	O
p	O
and	O
l	O
saloff-coste	O
what	O
do	O
we	O
know	O
about	O
the	O
metropolis	B
algorithm	I
journal	O
of	O
computer	O
and	O
system	O
sciences	O
dietterich	O
t	O
g	O
and	O
g	O
bakiri	O
solving	O
multiclass	B
learning	B
problems	O
via	O
error-correcting	B
output	I
codes	I
journal	O
of	O
artificial	O
intelligence	O
research	O
duane	O
s	O
a	O
d	O
kennedy	O
b	O
j	O
pendleton	O
and	O
d	O
roweth	O
hybrid	B
monte	I
carlo	I
physics	O
letters	O
b	O
duda	O
r	O
o	O
and	O
p	O
e	O
hart	O
pattern	O
classifi	O
cation	O
and	O
scene	O
analysis	O
wiley	O
references	O
duda	O
r	O
o	O
p	O
e	O
hart	O
and	O
d	O
g	O
stork	O
pat	O
fletcher	O
r	O
practical	O
methods	O
of	O
optimiza	O
tern	O
classification	B
ed	O
wiley	O
tion	O
ed	O
wiley	O
durbin	O
r	O
s	O
eddy	O
a	O
krogh	O
and	O
g	O
mitchison	O
biological	B
sequence	I
analysis	O
cambridge	O
university	O
press	O
dybowski	O
r	O
and	O
s	O
roberts	O
an	O
anthology	O
of	O
probabilistic	O
models	O
for	O
medical	O
informatics	O
in	O
d	O
husmeier	O
r	O
dybowski	O
and	O
s	O
roberts	O
probabilistic	O
modeling	O
in	O
bioinformatics	O
and	O
medical	O
informatics	O
pp	O
springer	O
efron	O
b	O
bootstrap	B
methods	O
another	O
look	O
at	O
the	O
jackknife	O
annals	O
of	O
statistics	O
elkan	O
c	O
using	O
the	O
triangle	O
inequality	O
to	O
accelerate	O
k-means	O
in	O
proceedings	O
of	O
the	O
twelfth	O
international	O
conference	O
on	O
machine	O
learning	B
pp	O
aaai	O
elliott	O
r	O
j	O
l	O
aggoun	O
and	O
j	O
b	O
moore	O
hidden	O
markov	O
models	O
estimation	O
and	O
control	O
springer	O
ephraim	O
y	O
d	O
malah	O
and	O
b	O
h	O
juang	O
on	O
the	O
application	O
of	O
hidden	O
markov	O
models	O
for	O
enhancing	O
noisy	O
speech	O
ieee	O
transactions	O
on	O
acoustics	O
speech	O
and	O
signal	O
processing	O
erwin	O
e	O
k	O
obermayer	O
and	O
k	O
schulten	O
self-organizing	O
maps	O
ordering	O
convergence	O
properties	O
and	O
energy	O
functions	O
biological	O
cybernetics	O
everitt	O
b	O
s	O
an	O
introduction	O
to	O
latent	O
vari	O
able	O
models	O
chapman	O
and	O
hall	O
faul	O
a	O
c	O
and	O
m	O
e	O
tipping	O
analysis	O
of	O
sparse	O
bayesian	B
learning	B
in	O
t	O
g	O
dietterich	O
s	O
becker	O
and	O
z	O
ghahramani	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
feller	O
w	O
an	O
introduction	O
to	O
probability	B
theory	B
and	O
its	O
applications	O
ed	O
volume	O
wiley	O
feynman	O
r	O
p	O
r	O
b	O
leighton	O
and	O
m	O
sands	O
the	O
feynman	O
lectures	O
of	O
physics	O
volume	O
two	O
addison-wesley	O
chapter	O
forsyth	O
d	O
a	O
and	O
j	O
ponce	O
computer	O
vi	O
sion	O
a	O
modern	O
approach	O
prentice	O
hall	O
freund	O
y	O
and	O
r	O
e	O
schapire	O
experiments	O
with	O
a	O
new	O
boosting	B
algorithm	O
in	O
l	O
saitta	O
thirteenth	O
international	O
conference	O
on	O
machine	O
learning	B
pp	O
morgan	O
kaufmann	O
frey	O
b	O
j	O
graphical	O
models	O
for	O
machine	O
learning	B
and	O
digital	O
communication	O
mit	O
press	O
frey	O
b	O
j	O
and	O
d	O
j	O
c	O
mackay	O
a	O
revolution	O
belief	B
propagation	I
in	O
graphs	O
with	O
cycles	O
in	O
m	O
i	O
jordan	O
m	O
j	O
kearns	O
and	O
s	O
a	O
solla	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
mit	O
press	O
friedman	O
j	O
h	O
greedy	O
function	O
approximation	O
a	O
gradient	O
boosting	B
machine	O
annals	O
of	O
statistics	O
friedman	O
j	O
h	O
t	O
hastie	O
and	O
r	O
tibshirani	O
additive	O
logistic	B
regression	B
a	O
statistical	O
view	O
of	O
boosting	B
annals	O
of	O
statistics	O
friedman	O
n	O
and	O
d	O
koller	O
being	O
bayesian	B
about	O
network	O
structure	O
a	O
bayesian	B
approach	O
to	O
structure	O
discovery	O
in	O
bayesian	B
networks	O
machine	O
learning	B
frydenberg	O
m	O
the	O
chain	B
graph	I
markov	O
property	O
scandinavian	O
journal	O
of	O
statistics	O
fukunaga	O
k	O
introduction	O
to	O
statistical	O
pattern	O
recognition	O
ed	O
academic	O
press	O
funahashi	O
k	O
on	O
the	O
approximate	O
realization	O
of	O
continuous	O
mappings	O
by	O
neural	O
networks	O
neural	O
networks	O
fung	O
r	O
and	O
k	O
c	O
chang	O
weighting	O
and	O
integrating	O
evidence	O
for	O
stochastic	B
simulation	O
in	O
bayesian	B
networks	O
in	O
p	O
p	O
bonissone	O
m	O
henrion	O
l	O
n	O
kanal	O
and	O
j	O
f	O
lemmer	O
uncertainty	O
in	O
artificial	O
intelligence	O
volume	O
pp	O
elsevier	O
gallager	O
r	O
g	O
low-density	O
parity-check	O
codes	O
mit	O
press	O
gamerman	O
d	O
markov	B
chain	I
monte	I
carlo	I
stochastic	B
simulation	O
for	O
bayesian	B
inference	B
chapman	O
and	O
hall	O
gelman	O
a	O
j	O
b	O
carlin	O
h	O
s	O
stern	O
and	O
d	O
b	O
rubin	O
bayesian	B
data	O
analysis	O
ed	O
chapman	O
and	O
hall	O
geman	O
s	O
and	O
d	O
geman	O
stochastic	B
relaxation	O
gibbs	B
distributions	O
and	O
the	O
bayesian	B
restoration	O
of	O
images	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
ghahramani	O
z	O
and	O
m	O
j	O
beal	O
variational	B
inference	B
for	O
bayesian	B
mixtures	O
of	O
factor	O
analyzers	O
in	O
s	O
a	O
solla	O
t	O
k	O
leen	O
and	O
k	O
r	O
m	O
uller	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
ghahramani	O
z	O
and	O
g	O
e	O
hinton	O
the	O
em	B
algorithm	I
for	O
mixtures	O
of	O
factor	O
analyzers	O
technical	O
report	O
university	O
of	O
toronto	O
ghahramani	O
z	O
and	O
g	O
e	O
hinton	O
parameter	O
estimation	O
for	O
linear	O
dynamical	O
systems	O
technical	O
report	O
university	O
of	O
toronto	O
ghahramani	O
z	O
and	O
g	O
e	O
hinton	O
variational	B
learning	B
for	O
switching	B
state-space	O
models	O
neural	O
computation	O
ghahramani	O
z	O
and	O
m	O
i	O
jordan	O
supervised	B
learning	B
from	O
incomplete	O
data	O
via	O
an	O
em	B
appproach	O
in	O
j	O
d	O
cowan	O
g	O
t	O
tesauro	O
and	O
j	O
alspector	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
morgan	O
kaufmann	O
ghahramani	O
z	O
and	O
m	O
i	O
jordan	O
factorial	B
hidden	O
markov	O
models	O
machine	O
learning	B
gibbs	B
m	O
n	O
bayesian	B
gaussian	B
processes	O
for	B
regression	B
and	O
classification	B
phd	O
thesis	O
university	O
of	O
cambridge	O
gibbs	B
m	O
n	O
and	O
d	O
j	O
c	O
mackay	O
variational	B
gaussian	B
process	I
classifiers	O
ieee	O
transactions	O
on	O
neural	O
networks	O
references	O
gilks	O
w	O
r	O
derivative-free	O
adaptive	B
rejection	B
sampling	I
for	O
gibbs	B
in	O
j	O
bernardo	O
j	O
berger	O
a	O
p	O
dawid	O
and	O
a	O
f	O
m	O
smith	O
bayesian	B
statistics	O
volume	O
oxford	O
university	O
press	O
sampling	O
gilks	O
w	O
r	O
n	O
g	O
best	O
and	O
k	O
k	O
c	O
tan	O
adaptive	O
rejection	O
metropolis	O
sampling	O
applied	O
statistics	O
gilks	O
w	O
r	O
s	O
richardson	O
and	O
d	O
j	O
spiegelhalter	O
markov	B
chain	I
monte	I
carlo	I
in	O
practice	O
chapman	O
and	O
hall	O
gilks	O
w	O
r	O
and	O
p	O
wild	O
adaptive	B
rejection	B
sampling	I
for	O
gibbs	B
sampling	I
applied	O
statistics	O
gill	O
p	O
e	O
w	O
murray	O
and	O
m	O
h	O
wright	O
practical	O
optimization	O
academic	O
press	O
goldberg	O
p	O
w	O
c	O
k	O
i	O
williams	O
and	O
c	O
m	O
bishop	O
regression	B
with	O
input-dependent	O
noise	O
a	O
gaussian	B
process	I
treatment	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
golub	O
g	O
h	O
and	O
c	O
f	O
van	O
loan	O
matrix	O
computations	O
ed	O
john	O
hopkins	O
university	O
press	O
good	O
i	O
probability	B
and	O
the	O
weighing	O
of	O
ev	O
idence	O
hafners	O
gordon	O
n	O
j	O
d	O
j	O
salmond	O
and	O
a	O
f	O
m	O
smith	O
approach	O
to	O
nonlinearnoniee	O
novel	O
gaussian	B
bayesian	B
proceedings-f	O
estimation	O
state	O
graepel	O
t	O
solving	O
noisy	O
linear	O
operator	O
equations	O
by	O
gaussian	B
processes	O
application	O
to	O
ordinary	O
and	O
partial	O
differential	B
equations	O
in	O
proceedings	O
of	O
the	O
twentieth	O
international	O
conference	O
on	O
machine	O
learning	B
pp	O
greig	O
d	O
b	O
porteous	O
and	O
a	O
seheult	O
exact	O
maximum	O
a-posteriori	O
estimation	O
for	O
binary	O
images	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
gull	O
s	O
f	O
developments	O
in	O
maximum	O
entropy	B
data	O
analysis	O
in	O
j	O
skilling	O
maximum	O
entropy	B
and	O
bayesian	B
methods	O
pp	O
kluwer	O
references	O
hassibi	O
b	O
and	O
d	O
g	O
stork	O
second	B
order	I
derivatives	O
for	O
network	O
pruning	O
optimal	O
brain	O
surgeon	O
in	O
s	O
j	O
hanson	O
j	O
d	O
cowan	O
and	O
c	O
l	O
giles	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
morgan	O
kaufmann	O
hastie	O
t	O
and	O
w	O
stuetzle	O
principal	O
curves	O
the	O
american	O
statistical	O
associa	O
journal	O
of	O
tion	O
hastie	O
t	O
r	O
tibshirani	O
and	O
j	O
friedman	O
the	O
elements	O
of	O
statistical	O
learning	B
springer	O
hastings	O
w	O
k	O
monte	B
carlo	I
sampling	B
methods	I
using	O
markov	O
chains	O
and	O
their	O
applications	O
biometrika	O
hathaway	O
r	O
j	O
another	O
interpretation	O
of	O
the	O
em	B
algorithm	I
for	O
mixture	B
distributions	O
statistics	O
and	O
probability	B
letters	O
haussler	O
d	O
convolution	O
kernels	O
on	O
discrete	O
structures	O
technical	O
report	O
university	O
of	O
california	O
santa	O
cruz	O
computer	O
science	O
department	O
henrion	O
m	O
propagation	O
of	O
uncertainty	O
by	O
logic	B
sampling	I
in	O
bayes	B
networks	O
in	O
j	O
f	O
lemmer	O
and	O
l	O
n	O
kanal	O
uncertainty	O
in	O
artificial	O
intelligence	O
volume	O
pp	O
north	O
holland	O
herbrich	O
r	O
learning	B
kernel	O
classifiers	O
mit	O
press	O
hertz	O
j	O
a	O
krogh	O
and	O
r	O
g	O
palmer	O
introduction	O
to	O
the	O
theory	B
of	O
neural	O
computation	O
addison	O
wesley	O
hinton	O
g	O
e	O
p	O
dayan	O
and	O
m	O
revow	O
modelling	O
the	O
manifolds	O
of	O
images	O
of	O
handwritten	O
digits	O
ieee	O
transactions	O
on	O
neural	O
networks	O
hinton	O
g	O
e	O
and	O
d	O
van	O
camp	O
keeping	O
neural	O
networks	O
simple	O
by	O
minimizing	O
the	O
description	O
length	O
of	O
the	O
weights	O
in	O
proceedings	O
of	O
the	O
sixth	O
annual	O
conference	O
on	O
computational	B
learning	B
theory	B
pp	O
acm	O
hinton	O
g	O
e	O
m	O
welling	O
y	O
w	O
teh	O
and	O
s	O
osindero	O
a	O
new	O
view	O
of	O
ica	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
independent	B
component	I
analysis	I
and	O
blind	O
signal	O
separation	O
volume	O
hodgson	O
m	O
e	O
reducing	O
computational	O
requirements	O
of	O
the	O
minimum-distance	O
classifier	O
remote	O
sensing	O
of	O
environments	O
hoerl	O
a	O
e	O
and	O
r	O
kennard	O
ridge	B
regression	B
biased	O
estimation	O
for	O
nonorthogonal	O
problems	O
technometrics	O
hofmann	O
t	O
learning	B
the	O
similarity	O
of	O
documents	O
an	O
information-geometric	O
approach	O
to	O
document	B
retrieval	I
and	O
classification	B
in	O
s	O
a	O
solla	O
t	O
k	O
leen	O
and	O
k	O
r	O
m	O
uller	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
hojen-sorensen	O
p	O
a	O
o	O
winther	O
and	O
l	O
k	O
hansen	O
mean	B
field	O
approaches	O
to	O
independent	B
component	I
analysis	I
neural	O
computation	O
hornik	O
k	O
approximation	O
capabilities	O
of	O
multilayer	O
feedforward	O
networks	O
neural	O
networks	O
hornik	O
k	O
m	O
stinchcombe	O
and	O
h	O
white	O
multilayer	O
feedforward	O
networks	O
are	O
universal	O
approximators	O
neural	O
networks	O
hotelling	O
h	O
analysis	O
of	O
a	O
complex	O
of	O
statistical	O
variables	O
into	O
principal	O
components	O
journal	O
of	O
educational	O
psychology	O
hotelling	O
h	O
relations	O
between	O
two	O
sets	O
of	O
variables	O
biometrika	O
hyv	O
arinen	O
a	O
and	O
e	O
oja	O
a	O
fast	O
fixed-point	O
algorithm	O
for	O
independent	B
component	I
analysis	I
neural	O
computation	O
isard	O
m	O
and	O
a	O
blake	O
condensation	O
conditional	B
density	B
propagation	O
for	O
visual	O
tracking	O
international	O
journal	O
of	O
computer	O
vision	O
ito	O
y	O
representation	O
of	O
functions	O
by	O
superpositions	O
of	O
a	O
step	O
or	O
sigmoid	O
function	O
and	O
their	O
applications	O
to	O
neural	B
network	I
theory	B
neural	O
networks	O
references	O
jaakkola	O
t	O
and	O
m	O
i	O
jordan	O
bayesian	B
parameter	O
estimation	O
via	O
variational	B
methods	O
statistics	O
and	O
computing	O
jaakkola	O
t	O
s	O
tutorial	O
on	O
variational	B
approximation	O
methods	O
in	O
m	O
opper	O
and	O
d	O
saad	O
advances	O
in	O
mean	B
field	O
methods	O
pp	O
mit	O
press	O
jaakkola	O
t	O
s	O
and	O
d	O
haussler	O
exploiting	O
generative	O
models	O
in	O
discriminative	O
classifiers	O
in	O
m	O
s	O
kearns	O
s	O
a	O
solla	O
and	O
d	O
a	O
cohn	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
mit	O
press	O
jacobs	O
r	O
a	O
m	O
i	O
jordan	O
s	O
j	O
nowlan	O
and	O
g	O
e	O
hinton	O
adaptive	O
mixtures	O
of	O
local	B
experts	O
neural	O
computation	O
jaynes	O
e	O
t	O
probability	B
theory	B
the	O
logic	O
of	O
science	O
cambridge	O
university	O
press	O
jebara	O
t	O
machine	O
learning	B
discrimina	O
tive	O
and	O
generative	O
kluwer	O
jeffries	O
h	O
an	O
invariant	O
form	O
for	O
the	O
prior	B
probability	B
in	O
estimation	O
problems	O
pro	O
roy	O
soc	O
aa	O
jelinek	O
f	O
statistical	O
methods	O
for	O
speech	B
recognition	I
mit	O
press	O
jensen	O
c	O
a	O
kong	O
and	O
u	O
kjaerulff	O
blocking	B
gibbs	B
sampling	I
in	O
very	O
large	O
probabilistic	O
expert	O
systems	O
international	O
journal	O
of	O
human	O
computer	O
studies	O
special	O
issue	O
on	O
real-world	O
applications	O
of	O
uncertain	O
reasoning	O
jordan	O
m	O
i	O
an	O
introduction	O
to	O
probabilis	O
tic	O
graphical	O
models	O
in	O
preparation	O
jordan	O
m	O
i	O
z	O
ghahramani	O
t	O
s	O
jaakkola	O
and	O
l	O
k	O
saul	O
an	O
introduction	O
to	O
variational	B
methods	O
for	O
graphical	O
models	O
in	O
m	O
i	O
jordan	O
learning	B
in	O
graphical	O
models	O
pp	O
mit	O
press	O
jordan	O
m	O
i	O
and	O
r	O
a	O
jacobs	O
hierarchical	B
mixtures	O
of	O
experts	O
and	O
the	O
em	B
algorithm	I
neural	O
computation	O
jutten	O
c	O
and	O
j	O
herault	O
blind	O
separation	O
of	O
sources	O
an	O
adaptive	O
algorithm	O
based	O
on	O
neuromimetic	O
architecture	O
signal	O
processing	O
kalman	O
r	O
e	O
a	O
new	O
approach	O
to	O
linear	O
filtering	O
and	O
prediction	O
problems	O
transactions	O
of	O
the	O
american	O
society	O
for	O
mechanical	O
engineering	O
series	O
d	O
journal	O
of	O
basic	O
engineering	O
kambhatla	O
n	O
and	O
t	O
k	O
leen	O
dimension	O
reduction	O
by	O
local	B
principal	B
component	I
analysis	I
neural	O
computation	O
kanazawa	O
k	O
d	O
koller	O
and	O
s	O
russel	O
stochastic	B
simulation	O
algorithms	O
for	O
dynamic	O
probabilistic	O
networks	O
in	O
uncertainty	O
in	O
artificial	O
intelligence	O
volume	O
morgan	O
kaufmann	O
kapadia	O
s	O
discriminative	O
training	B
of	O
hidden	O
markov	O
models	O
phd	O
thesis	O
university	O
of	O
cambridge	O
u	O
k	O
kapur	O
j	O
maximum	O
entropy	B
methods	O
in	O
sci	O
jensen	O
f	O
v	O
an	O
introduction	O
to	O
bayesian	B
ence	O
and	O
engineering	O
wiley	O
networks	O
ucl	O
press	O
jerrum	O
m	O
and	O
a	O
sinclair	O
the	O
markov	B
chain	I
monte	I
carlo	I
method	O
an	O
approach	O
to	O
approximate	O
counting	O
and	O
integration	O
in	O
d	O
s	O
hochbaum	O
approximation	O
algorithms	O
for	O
np-hard	O
problems	O
pws	O
publishing	O
jolliffe	O
i	O
t	O
principal	B
component	I
analysis	I
ed	O
springer	O
jordan	O
m	O
i	O
learning	B
in	O
graphical	O
models	O
mit	O
press	O
karush	O
w	O
minima	O
of	O
functions	O
of	O
several	O
variables	O
with	O
inequalities	O
as	O
side	O
constraints	O
master	O
s	O
thesis	O
department	O
of	O
mathematics	O
university	O
of	O
chicago	O
kass	O
r	O
e	O
and	O
a	O
e	O
raftery	O
bayes	B
factors	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
kearns	O
m	O
j	O
and	O
u	O
v	O
vazirani	O
an	O
introduction	O
to	O
computational	B
learning	B
theory	B
mit	O
press	O
references	O
kindermann	O
r	O
and	O
j	O
l	O
snell	O
markov	O
random	O
fields	O
and	O
their	O
applications	O
american	O
mathematical	O
society	O
kittler	O
j	O
and	O
j	O
f	O
oglein	O
contextual	O
classification	B
of	O
multispectral	O
pixel	O
data	O
image	O
and	O
vision	O
computing	O
kohonen	O
t	O
self-organized	O
formation	O
of	O
topologically	O
correct	O
feature	O
maps	O
biological	O
cybernetics	O
kohonen	O
t	O
self-organizing	O
maps	O
springer	O
kolmogorov	O
v	O
and	O
r	O
zabih	O
what	O
energy	O
functions	O
can	O
be	O
minimized	O
via	O
graph	O
cuts	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
kreinovich	O
v	O
y	O
arbitrary	O
nonlinearity	O
is	O
sufficient	O
to	O
represent	O
all	O
functions	O
by	O
neural	O
networks	O
a	O
theorem	O
neural	O
networks	O
krogh	O
a	O
m	O
brown	O
i	O
s	O
mian	O
k	O
sj	O
olander	O
and	O
d	O
haussler	O
hidden	O
markov	O
models	O
in	O
computational	O
biology	O
applications	O
to	O
protein	O
modelling	O
journal	O
of	O
molecular	O
biology	O
kschischnang	O
f	O
r	O
b	O
j	O
frey	O
and	O
h	O
a	O
loeliger	O
factor	O
graphs	O
and	O
the	O
sum-product	B
algorithm	I
ieee	O
transactions	O
on	O
information	B
theory	B
kuhn	O
h	O
w	O
and	O
a	O
w	O
tucker	O
nonlinear	O
programming	O
in	O
proceedings	O
of	O
the	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	O
and	O
probabilities	O
pp	O
university	O
of	O
california	O
press	O
kullback	O
s	O
and	O
r	O
a	O
leibler	O
on	O
information	O
and	O
sufficiency	O
annals	O
of	O
mathematical	O
statistics	O
k	O
urkov	O
a	O
v	O
and	O
p	O
c	O
kainen	O
functionally	O
equivalent	O
feed-forward	O
neural	O
networks	O
neural	O
computation	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
number	O
mit	O
press	O
in	O
press	O
lasserre	O
j	O
c	O
m	O
bishop	O
and	O
t	O
minka	O
principled	O
hybrids	O
of	O
generative	O
and	O
discriminative	O
models	O
in	O
proceedings	O
ieee	O
conference	O
on	O
computer	O
vision	O
and	O
pattern	O
recognition	O
new	O
york	O
lauritzen	O
s	O
and	O
n	O
wermuth	O
graphical	O
models	O
for	O
association	O
between	O
variables	O
some	O
of	O
which	O
are	O
qualitative	O
some	O
quantitative	O
annals	O
of	O
statistics	O
lauritzen	O
s	O
l	O
propagation	O
of	O
probabilities	O
means	O
and	O
variances	O
in	O
mixed	O
graphical	O
association	O
models	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
lauritzen	O
s	O
l	O
graphical	O
models	O
oxford	O
university	O
press	O
lauritzen	O
s	O
l	O
and	O
d	O
j	O
spiegelhalter	O
local	B
computations	O
with	O
probabailities	O
on	O
graphical	O
structures	O
and	O
their	O
application	O
to	O
expert	O
systems	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
lawley	O
d	O
n	O
a	O
modified	O
method	O
of	O
estimation	O
in	O
factor	B
analysis	I
and	O
some	O
large	O
sample	O
results	O
in	O
uppsala	O
symposium	O
on	O
psychological	O
factor	B
analysis	I
number	O
in	O
nordisk	O
psykologi	O
monograph	O
series	O
pp	O
uppsala	O
almqvist	O
and	O
wiksell	O
lawrence	O
n	O
d	O
a	O
i	O
t	O
rowstron	O
c	O
m	O
bishop	O
and	O
m	O
j	O
taylor	O
optimising	O
synchronisation	O
times	O
for	O
mobile	O
devices	O
in	O
t	O
g	O
dietterich	O
s	O
becker	O
and	O
z	O
ghahramani	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
lazarsfeld	O
p	O
f	O
and	O
n	O
w	O
henry	O
latent	O
structure	O
analysis	O
houghton	O
mifflin	O
le	O
cun	O
y	O
b	O
boser	O
j	O
s	O
denker	O
d	O
henderson	O
r	O
e	O
howard	O
w	O
hubbard	O
and	O
l	O
d	O
jackel	O
backpropagation	B
applied	O
to	O
handwritten	O
zip	O
code	O
recognition	O
neural	O
computation	O
kuss	O
m	O
and	O
c	O
rasmussen	O
assessing	O
approximations	O
for	O
gaussian	B
process	I
classification	B
le	O
cun	O
y	O
j	O
s	O
denker	O
and	O
s	O
a	O
solla	O
optimal	O
brain	O
damage	O
in	O
d	O
s	O
touretzky	O
references	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
morgan	O
kaufmann	O
lecun	O
y	O
l	O
bottou	O
y	O
bengio	O
and	O
p	O
haffner	O
gradient-based	O
learning	B
applied	O
to	O
document	O
recognition	O
proceedings	O
of	O
the	O
ieee	O
lee	O
y	O
y	O
lin	O
and	O
g	O
wahba	O
multicategory	O
support	B
vector	I
machines	O
technical	O
report	O
department	O
of	O
statistics	O
university	O
of	O
madison	O
wisconsin	O
leen	O
t	O
k	O
from	O
data	O
distributions	O
to	O
regularization	B
in	O
invariant	O
learning	B
neural	O
computation	O
lindley	O
d	O
v	O
scoring	O
rules	O
and	O
the	O
inevitability	O
of	O
probability	B
international	O
statistical	O
review	O
liu	O
j	O
s	O
monte	O
carlo	O
strategies	O
in	O
scientific	O
computing	O
springer	O
lloyd	O
s	O
p	O
least	O
squares	O
quantization	O
in	O
pcm	O
ieee	O
transactions	O
on	O
information	B
theory	B
l	O
utkepohl	O
h	O
handbook	O
of	O
matrices	O
wiley	O
mackay	O
d	O
j	O
c	O
bayesian	B
interpolation	O
neural	O
computation	O
mackay	O
d	O
j	O
c	O
the	O
evidence	O
framework	O
applied	O
to	O
classification	B
networks	O
neural	O
computation	O
mackay	O
d	O
j	O
c	O
a	O
practical	O
bayesian	B
framework	O
for	O
back-propagation	O
networks	O
neural	O
computation	O
mackay	O
d	O
j	O
c	O
bayesian	B
methods	O
for	O
backprop	O
networks	O
in	O
e	O
domany	O
j	O
l	O
van	O
hemmen	O
and	O
k	O
schulten	O
models	O
of	O
neural	O
networks	O
iii	O
chapter	O
pp	O
springer	O
mackay	O
d	O
j	O
c	O
bayesian	B
neural	O
networks	O
and	O
density	B
networks	O
nuclear	O
instruments	O
and	O
methods	O
in	O
physics	O
research	O
a	O
mackay	O
d	O
j	O
c	O
ensemble	O
learning	B
for	O
hidden	O
markov	O
models	O
unpublished	O
manuscript	O
department	O
of	O
physics	O
university	O
of	O
cambridge	O
mackay	O
d	O
j	O
c	O
introduction	O
to	O
gaussian	B
processes	O
in	O
c	O
m	O
bishop	O
neural	O
networks	O
and	O
machine	O
learning	B
pp	O
springer	O
mackay	O
d	O
j	O
c	O
comparison	O
of	O
approximate	O
methods	O
for	O
handling	O
hyperparameters	O
neural	O
computation	O
mackay	O
d	O
j	O
c	O
information	B
theory	B
inference	B
and	O
learning	B
algorithms	O
cambridge	O
university	O
press	O
mackay	O
d	O
j	O
c	O
and	O
m	O
n	O
gibbs	B
density	B
networks	O
in	O
j	O
w	O
kay	O
and	O
d	O
m	O
titterington	O
statistics	O
and	O
neural	O
networks	O
advances	O
at	O
the	O
interface	O
chapter	O
pp	O
oxford	O
university	O
press	O
mackay	O
d	O
j	O
c	O
and	O
r	O
m	O
neal	O
good	O
errorcorrecting	O
codes	O
based	O
on	O
very	O
sparse	O
matrices	O
ieee	O
transactions	O
on	O
information	B
theory	B
macqueen	O
j	O
some	O
methods	O
for	O
classification	B
and	O
analysis	O
of	O
multivariate	O
observations	O
in	O
l	O
m	O
lecam	O
and	O
j	O
neyman	O
proceedings	O
of	O
the	O
fifth	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	O
and	O
probability	B
volume	O
i	O
pp	O
university	O
of	O
california	O
press	O
magnus	O
j	O
r	O
and	O
h	O
neudecker	O
matrix	O
differential	B
calculus	O
with	O
applications	O
in	O
statistics	O
and	O
econometrics	O
wiley	O
mallat	O
s	O
a	O
wavelet	O
tour	O
of	O
signal	O
process	O
ing	O
ed	O
academic	O
press	O
manning	O
c	O
d	O
and	O
h	O
sch	O
utze	O
foundations	O
of	O
statistical	O
natural	O
language	O
processing	O
mit	O
press	O
mardia	O
k	O
v	O
and	O
p	O
e	O
jupp	O
directional	O
statistics	O
wiley	O
maybeck	O
p	O
s	O
stochastic	B
models	O
estima	O
tion	O
and	O
control	O
academic	O
press	O
mcallester	O
d	O
a	O
pac-bayesian	O
stochastic	B
model	B
selection	I
machine	O
learning	B
references	O
mccullagh	O
p	O
and	O
j	O
a	O
nelder	O
generalized	B
linear	O
models	O
ed	O
chapman	O
and	O
hall	O
mcculloch	O
w	O
s	O
and	O
w	O
pitts	O
a	O
logical	O
calculus	O
of	O
the	O
ideas	O
immanent	O
in	O
nervous	O
activity	O
bulletin	O
of	O
mathematical	O
biophysics	O
reprinted	O
in	O
anderson	O
and	O
rosenfeld	O
mceliece	O
r	O
j	O
d	O
j	O
c	O
mackay	O
and	O
j	O
f	O
cheng	O
turbo	O
decoding	O
as	O
an	O
instance	O
of	O
pearl	O
s	O
belief	O
ppropagation	O
algorithm	O
ieee	O
journal	O
on	O
selected	O
areas	O
in	O
communications	O
mclachlan	O
g	O
j	O
and	O
k	O
e	O
basford	O
mixture	B
models	O
inference	B
and	O
applications	O
to	O
clustering	B
marcel	O
dekker	O
mclachlan	O
g	O
j	O
and	O
t	O
krishnan	O
the	O
em	B
algorithm	I
and	O
its	O
extensions	O
wiley	O
mclachlan	O
g	O
j	O
and	O
d	O
peel	O
finite	O
mixture	B
models	O
wiley	O
meng	O
x	O
l	O
and	O
d	O
b	O
rubin	O
maximum	B
likelihood	I
estimation	O
via	O
the	O
ecm	O
algorithm	O
a	O
general	O
framework	O
biometrika	O
metropolis	O
n	O
a	O
w	O
rosenbluth	O
m	O
n	O
rosenbluth	O
a	O
h	O
teller	O
and	O
e	O
teller	O
equation	O
of	O
state	O
calculations	O
by	O
fast	O
computing	O
machines	O
journal	O
of	O
chemical	O
physics	O
metropolis	O
n	O
and	O
s	O
ulam	O
the	O
monte	O
carlo	O
method	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
mika	O
s	O
g	O
r	O
atsch	O
j	O
weston	O
and	O
b	O
sch	O
olkopf	O
fisher	B
discriminant	O
analysis	O
with	O
kernels	O
in	O
y	O
h	O
hu	O
j	O
larsen	O
e	O
wilson	O
and	O
s	O
douglas	O
neural	O
networks	O
for	O
signal	O
processing	O
ix	O
pp	O
ieee	O
minka	O
t	O
expectation	B
propagation	I
for	O
approximate	O
bayesian	B
inference	B
in	O
j	O
breese	O
and	O
d	O
koller	O
proceedings	O
of	O
the	O
seventeenth	O
conference	O
on	O
uncertainty	O
in	O
artificial	O
intelligence	O
pp	O
morgan	O
kaufmann	O
minka	O
t	O
a	O
family	O
of	O
approximate	O
algorithms	O
for	O
bayesian	B
inference	B
ph	O
d	O
thesis	O
mit	O
minka	O
t	O
power	B
ep	I
technical	O
report	O
microsoft	O
research	O
cambridge	O
minka	O
t	O
divergence	O
measures	O
and	O
message	B
passing	I
technical	O
report	O
microsoft	O
research	O
cambridge	O
minka	O
t	O
p	O
automatic	O
choice	O
of	O
dimensionality	O
for	O
pca	O
in	O
t	O
k	O
leen	O
t	O
g	O
dietterich	O
and	O
v	O
tresp	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
minsky	O
m	O
l	O
and	O
s	O
a	O
papert	O
perceptrons	O
mit	O
press	O
expanded	O
edition	O
miskin	O
j	O
w	O
and	O
d	O
j	O
c	O
mackay	O
ensemble	O
learning	B
for	O
blind	B
source	I
separation	I
in	O
s	O
j	O
roberts	O
and	O
r	O
m	O
everson	O
independent	B
component	I
analysis	I
principles	O
and	O
practice	O
cambridge	O
university	O
press	O
m	O
ller	O
m	O
efficient	O
training	B
of	O
feedforward	O
neural	O
networks	O
ph	O
d	O
thesis	O
aarhus	O
university	O
denmark	O
moody	O
j	O
and	O
c	O
j	O
darken	O
fast	O
learning	B
in	O
networks	O
of	O
locally-tuned	O
processing	O
units	O
neural	O
computation	O
moore	O
a	O
w	O
the	O
anchors	O
hierarch	O
using	O
the	O
triangle	O
inequality	O
to	O
survive	O
high	O
dimensional	O
data	O
in	O
proceedings	O
of	O
the	O
twelfth	O
conference	O
on	O
uncertainty	O
in	O
artificial	O
intelligence	O
pp	O
m	O
uller	O
k	O
r	O
s	O
mika	O
g	O
r	O
atsch	O
k	O
tsuda	O
and	O
b	O
sch	O
olkopf	O
an	O
introduction	O
to	O
kernelbased	O
learning	B
algorithms	O
ieee	O
transactions	O
on	O
neural	O
networks	O
m	O
uller	O
p	O
and	O
f	O
a	O
quintana	O
nonparametric	O
bayesian	B
data	O
analysis	O
statistical	O
science	O
nabney	O
i	O
t	O
netlab	O
algorithms	O
for	O
pattern	O
recognition	O
springer	O
nadaraya	O
e	O
a	O
on	O
estimating	O
regression	B
theory	B
of	O
probability	B
and	O
its	O
applications	O
references	O
nag	O
r	O
k	O
wong	O
and	O
f	O
fallside	O
script	O
recognition	O
using	O
hidden	O
markov	O
models	O
in	O
pp	O
ieee	O
neal	O
r	O
m	O
probabilistic	O
inference	B
using	O
markov	B
chain	I
monte	I
carlo	I
methods	O
technical	O
report	O
department	O
of	O
computer	O
science	O
university	O
of	O
toronto	O
canada	O
neal	O
r	O
m	O
bayesian	B
learning	B
for	O
neural	O
networks	O
springer	O
lecture	O
notes	O
in	O
statistics	O
neal	O
r	O
m	O
monte	O
carlo	O
implementation	O
of	O
gaussian	B
process	I
models	O
for	O
bayesian	B
regression	B
and	O
classification	B
technical	O
report	O
department	O
of	O
computer	O
statistics	O
university	O
of	O
toronto	O
neal	O
r	O
m	O
suppressing	O
random	O
walks	O
in	O
markov	B
chain	I
monte	I
carlo	I
using	O
ordered	O
overrelaxation	O
in	O
m	O
i	O
jordan	O
learning	B
in	O
graphical	O
models	O
pp	O
mit	O
press	O
neal	O
r	O
m	O
markov	B
chain	I
sampling	O
for	O
dirichlet	B
process	O
mixture	B
models	O
journal	O
of	O
computational	O
and	O
graphical	O
statistics	O
neal	O
r	O
m	O
slice	B
sampling	I
annals	O
of	O
statis	O
tics	O
neal	O
r	O
m	O
and	O
g	O
e	O
hinton	O
a	O
new	O
view	O
of	O
the	O
em	B
algorithm	I
that	O
justifies	O
incremental	O
and	O
other	O
variants	O
in	O
m	O
i	O
jordan	O
learning	B
in	O
graphical	O
models	O
pp	O
mit	O
press	O
nelder	O
j	O
a	O
and	O
r	O
w	O
m	O
wedderburn	O
generalized	B
linear	O
models	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
a	O
nilsson	O
n	O
j	O
learning	B
machines	O
mcgrawhill	O
reprinted	O
as	O
the	O
mathematical	O
foundations	O
of	O
learning	B
machines	O
morgan	O
kaufmann	O
nocedal	O
j	O
and	O
s	O
j	O
wright	O
numerical	O
op	O
timization	O
springer	O
nowlan	O
s	O
j	O
and	O
g	O
e	O
hinton	O
simplifying	O
neural	O
networks	O
by	O
soft	B
weight	B
sharing	I
neural	O
computation	O
ogden	O
r	O
t	O
essential	O
wavelets	B
for	O
statistical	O
applications	O
and	O
data	O
analysis	O
birkh	O
auser	O
opper	O
m	O
and	O
o	O
winther	O
a	O
bayesian	B
approach	O
to	O
on-line	O
learning	B
in	O
d	O
saad	O
online	O
learning	B
in	O
neural	O
networks	O
pp	O
cambridge	O
university	O
press	O
opper	O
m	O
and	O
o	O
winther	O
gaussian	B
processes	O
and	O
svm	O
mean	B
field	I
theory	B
and	O
leave-one-out	B
in	O
a	O
j	O
smola	O
p	O
l	O
bartlett	O
b	O
sch	O
olkopf	O
and	O
d	O
shuurmans	O
advances	O
in	O
large	O
margin	B
classifiers	O
pp	O
mit	O
press	O
opper	O
m	O
and	O
o	O
winther	O
gaussian	B
processes	O
for	O
classification	B
neural	O
computation	O
osuna	O
e	O
r	O
freund	O
and	O
f	O
girosi	O
support	B
vector	I
machines	O
training	B
and	O
applications	O
a	O
i	O
memo	O
mit	O
papoulis	O
a	O
probability	B
random	O
variables	O
and	O
stochastic	B
processes	O
ed	O
mcgrawhill	O
parisi	O
g	O
statistical	O
field	O
theory	B
addison	O
wesley	O
pearl	O
j	O
probabilistic	O
reasoning	O
in	O
intelli	O
gent	O
systems	O
morgan	O
kaufmann	O
pearlmutter	O
b	O
a	O
fast	O
exact	O
multiplication	O
by	O
the	O
hessian	O
neural	O
computation	O
pearlmutter	O
b	O
a	O
and	O
l	O
c	O
parra	O
maximum	B
likelihood	I
source	O
separation	O
a	O
context-sensitive	O
generalization	B
of	O
ica	O
in	O
m	O
c	O
mozer	O
m	O
i	O
jordan	O
and	O
t	O
petsche	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
pearson	O
k	O
on	O
lines	O
and	O
planes	O
of	O
closest	O
fit	O
to	O
systems	O
of	O
points	O
in	O
space	O
the	O
london	O
edinburgh	O
and	O
dublin	O
philosophical	O
magazine	O
and	O
journal	O
of	O
science	O
sixth	O
series	O
platt	O
j	O
c	O
fast	O
training	B
of	O
support	B
vector	I
machines	O
using	O
sequential	B
minimal	I
optimization	I
in	O
b	O
sch	O
olkopf	O
c	O
j	O
c	O
burges	O
and	O
a	O
j	O
smola	O
advances	O
in	O
kernel	O
methods	O
support	B
vector	I
learning	B
pp	O
mit	O
press	O
references	O
platt	O
j	O
c	O
probabilities	O
for	O
sv	O
machines	O
in	O
a	O
j	O
smola	O
p	O
l	O
bartlett	O
b	O
sch	O
olkopf	O
and	O
d	O
shuurmans	O
advances	O
in	O
large	O
margin	B
classifiers	O
pp	O
mit	O
press	O
platt	O
j	O
c	O
n	O
cristianini	O
and	O
j	O
shawe-taylor	O
large	O
margin	B
dags	O
for	O
multiclass	B
classification	B
in	O
s	O
a	O
solla	O
t	O
k	O
leen	O
and	O
k	O
r	O
m	O
uller	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
poggio	O
t	O
and	O
f	O
girosi	O
networks	O
for	O
approximation	O
and	O
learning	B
proceedings	O
of	O
the	O
ieee	O
powell	O
m	O
j	O
d	O
radial	O
basis	O
functions	O
for	O
multivariable	O
interpolation	O
a	O
review	O
in	O
j	O
c	O
mason	O
and	O
m	O
g	O
cox	O
algorithms	O
for	O
approximation	O
pp	O
oxford	O
university	O
press	O
press	O
w	O
h	O
s	O
a	O
teukolsky	O
w	O
t	O
vetterling	O
and	O
b	O
p	O
flannery	O
numerical	O
recipes	O
in	O
c	O
the	O
art	O
of	O
scientific	O
computing	O
ed	O
cambridge	O
university	O
press	O
qazaz	O
c	O
s	O
c	O
k	O
i	O
williams	O
and	O
c	O
m	O
bishop	O
an	O
upper	O
bound	O
on	O
the	O
bayesian	B
error	B
bars	O
for	O
generalized	B
linear	B
regression	B
in	O
s	O
w	O
ellacott	O
j	O
c	O
mason	O
and	O
i	O
j	O
anderson	O
mathematics	O
of	O
neural	O
networks	O
models	O
algorithms	O
and	O
applications	O
pp	O
kluwer	O
quinlan	O
j	O
r	O
induction	O
of	O
decision	O
trees	O
machine	O
learning	B
quinlan	O
j	O
r	O
programs	O
for	O
machine	O
learning	B
morgan	O
kaufmann	O
rabiner	O
l	O
and	O
b	O
h	O
juang	O
fundamentals	O
of	O
speech	B
recognition	I
prentice	O
hall	O
rabiner	O
l	O
r	O
a	O
tutorial	O
on	O
hidden	O
markov	O
models	O
and	O
selected	O
applications	O
in	O
speech	O
the	O
ieee	O
recognition	O
proceedings	O
of	O
ramasubramanian	O
v	O
and	O
k	O
k	O
paliwal	O
a	O
generalized	B
optimization	O
of	O
the	O
k-d	O
tree	B
for	O
fast	O
nearest-neighbour	O
search	O
in	O
proceedings	O
fourth	O
ieee	O
region	O
international	O
conference	O
pp	O
ramsey	O
f	O
truth	O
and	O
probability	B
in	O
r	O
braithwaite	O
the	O
foundations	O
of	O
mathematics	O
and	O
other	O
logical	O
essays	O
humanities	O
press	O
rao	O
c	O
r	O
and	O
s	O
k	O
mitra	O
generalized	B
in	O
verse	O
of	O
matrices	O
and	O
its	O
applications	O
wiley	O
rasmussen	O
c	O
e	O
evaluation	O
of	O
gaussian	B
processes	O
and	O
other	O
methods	O
for	O
non-linear	O
regression	B
ph	O
d	O
thesis	O
university	O
of	O
toronto	O
rasmussen	O
c	O
e	O
and	O
j	O
qui	O
nonero-candela	O
healing	O
the	O
relevance	B
vector	I
machine	I
by	O
augmentation	O
in	O
l	O
d	O
raedt	O
and	O
s	O
wrobel	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	B
pp	O
rasmussen	O
c	O
e	O
and	O
c	O
k	O
i	O
williams	O
gaussian	B
processes	O
for	O
machine	O
learning	B
mit	O
press	O
rauch	O
h	O
e	O
f	O
tung	O
and	O
c	O
t	O
striebel	O
maximum	B
likelihood	I
estimates	O
of	O
linear	O
dynamical	O
systems	O
aiaa	O
journal	O
ricotti	O
l	O
p	O
s	O
ragazzini	O
and	O
g	O
martinelli	O
learning	B
of	O
word	O
stress	O
in	O
a	O
sub-optimal	O
second	B
order	I
backpropagation	B
neural	B
network	I
in	O
proceedings	O
of	O
the	O
ieee	O
international	O
conference	O
on	O
neural	O
networks	O
volume	O
pp	O
ieee	O
ripley	O
b	O
d	O
pattern	O
recognition	O
and	O
neu	O
ral	O
networks	O
cambridge	O
university	O
press	O
robbins	O
h	O
and	O
s	O
monro	O
a	O
stochastic	B
approximation	O
method	O
annals	O
of	O
mathematical	O
statistics	O
robert	O
c	O
p	O
and	O
g	O
casella	O
monte	O
carlo	O
statistical	O
methods	O
springer	O
rockafellar	O
r	O
convex	O
analysis	O
princeton	O
university	O
press	O
rosenblatt	B
f	O
principles	O
of	O
neurodynamics	O
perceptrons	O
and	O
the	O
theory	B
of	O
brain	O
mechanisms	O
spartan	O
roth	O
v	O
and	O
v	O
steinhage	O
nonlinear	O
discriminant	O
analysis	O
using	O
kernel	O
functions	O
in	O
s	O
a	O
references	O
solla	O
t	O
k	O
leen	O
and	O
k	O
r	O
m	O
uller	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
mit	O
press	O
roweis	O
s	O
em	B
algorithms	O
for	O
pca	O
and	O
spca	O
in	O
m	O
i	O
jordan	O
m	O
j	O
kearns	O
and	O
s	O
a	O
solla	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
roweis	O
s	O
and	O
z	O
ghahramani	O
a	O
unifying	O
review	O
of	O
linear	O
gaussian	B
models	O
neural	O
computation	O
roweis	O
s	O
and	O
l	O
saul	O
december	O
nonlinear	O
dimensionality	O
reduction	O
by	O
locally	B
linear	I
embedding	I
science	O
rubin	O
d	O
b	O
iteratively	O
reweighted	O
least	O
squares	O
in	O
encyclopedia	O
of	O
statistical	O
sciences	O
volume	O
pp	O
wiley	O
rubin	O
d	O
b	O
and	O
d	O
t	O
thayer	O
em	B
algorithms	O
for	O
ml	O
factor	B
analysis	I
psychometrika	O
rumelhart	O
d	O
e	O
g	O
e	O
hinton	O
and	O
r	O
j	O
williams	O
learning	B
internal	O
representations	O
by	O
error	B
propagation	O
in	O
d	O
e	O
rumelhart	O
j	O
l	O
mcclelland	O
and	O
the	O
pdp	O
research	O
group	O
parallel	O
distributed	O
processing	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
volume	O
foundations	O
pp	O
mit	O
press	O
reprinted	O
in	O
anderson	O
and	O
rosenfeld	O
rumelhart	O
d	O
e	O
j	O
l	O
mcclelland	O
and	O
the	O
pdp	O
research	O
group	O
parallel	O
distributed	O
processing	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
volume	O
foundations	O
mit	O
press	O
sagan	O
h	O
introduction	O
to	O
the	O
calculus	B
of	I
variations	I
dover	O
savage	O
l	O
j	O
the	O
subjective	O
basis	O
of	O
statistical	O
practice	O
technical	O
report	O
department	O
of	O
statistics	O
university	O
of	O
michigan	O
ann	O
arbor	O
sch	O
olkopf	O
b	O
j	O
platt	O
j	O
shawe-taylor	O
a	O
smola	O
and	O
r	O
c	O
williamson	O
estimating	O
the	O
support	O
of	O
a	O
high-dimensional	O
distribution	O
neural	O
computation	O
sch	O
olkopf	O
b	O
a	O
smola	O
and	O
k	O
-r	O
m	O
uller	O
nonlinear	O
component	O
analysis	O
as	O
a	O
kernel	O
eigenvalue	O
problem	O
neural	O
computation	O
sch	O
olkopf	O
b	O
a	O
smola	O
r	O
c	O
williamson	O
and	O
p	O
l	O
bartlett	O
new	O
support	B
vector	I
algorithms	O
neural	O
computation	O
sch	O
olkopf	O
b	O
and	O
a	O
j	O
smola	O
learning	B
with	O
kernels	O
mit	O
press	O
schwarz	O
g	O
estimating	O
the	O
dimension	O
of	O
a	O
model	O
annals	O
of	O
statistics	O
schwarz	O
h	O
r	O
finite	O
element	O
methods	O
aca	O
demic	O
press	O
seeger	O
m	O
bayesian	B
gaussian	B
process	I
models	O
pac-bayesian	O
generalization	B
error	B
bounds	O
and	O
sparse	O
approximations	O
ph	O
d	O
thesis	O
university	O
of	O
edinburg	O
seeger	O
m	O
c	O
k	O
i	O
williams	O
and	O
n	O
lawrence	O
fast	O
forward	O
selection	O
to	O
speed	O
up	O
sparse	O
gaussian	B
processes	O
in	O
c	O
m	O
bishop	O
and	O
b	O
frey	O
proceedings	O
ninth	O
international	O
workshop	O
on	O
artificial	O
intelligence	O
and	O
statistics	O
key	O
west	O
florida	O
shachter	O
r	O
d	O
and	O
m	O
peot	O
simulation	O
approaches	O
to	O
general	O
probabilistic	O
inference	B
on	O
belief	O
networks	O
in	O
p	O
p	O
bonissone	O
m	O
henrion	O
l	O
n	O
kanal	O
and	O
j	O
f	O
lemmer	O
uncertainty	O
in	O
artificial	O
intelligence	O
volume	O
elsevier	O
shannon	B
c	O
e	O
a	O
mathematical	O
theory	B
of	O
communication	O
the	O
bell	O
system	O
technical	O
journal	O
and	O
shawe-taylor	O
j	O
and	O
n	O
cristianini	O
kernel	O
methods	O
for	O
pattern	O
analysis	O
cambridge	O
university	O
press	O
sietsma	O
j	O
and	O
r	O
j	O
f	O
dow	O
creating	O
artificial	O
neural	O
networks	O
that	O
generalize	O
neural	O
networks	O
simard	O
p	O
y	O
le	O
cun	O
and	O
j	O
denker	O
efficient	O
pattern	O
recognition	O
using	O
a	O
new	O
transformation	O
distance	O
in	O
s	O
j	O
hanson	O
j	O
d	O
cowan	O
and	O
references	O
c	O
l	O
giles	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
morgan	O
kaufmann	O
simard	O
p	O
b	O
victorri	O
y	O
le	O
cun	O
and	O
j	O
denker	O
tangent	O
prop	O
a	O
formalism	O
for	O
specifying	O
selected	O
invariances	O
in	O
an	O
adaptive	O
network	O
in	O
j	O
e	O
moody	O
s	O
j	O
hanson	O
and	O
r	O
p	O
lippmann	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
morgan	O
kaufmann	O
simard	O
p	O
y	O
d	O
steinkraus	O
and	O
j	O
platt	O
best	O
practice	O
for	O
convolutional	B
neural	O
networks	O
applied	O
to	O
visual	O
document	O
analysis	O
in	O
proceedings	O
international	O
conference	O
on	O
document	O
analysis	O
and	O
recognition	O
pp	O
ieee	O
computer	O
society	O
sirovich	O
l	O
turbulence	O
and	O
the	O
dynamics	O
of	O
coherent	O
structures	O
quarterly	O
applied	O
mathematics	O
smola	O
a	O
j	O
and	O
p	O
bartlett	O
sparse	O
greedy	O
gaussian	B
process	I
regression	B
in	O
t	O
k	O
leen	O
t	O
g	O
dietterich	O
and	O
v	O
tresp	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
spiegelhalter	O
d	O
and	O
s	O
lauritzen	O
sequential	O
updating	O
of	O
conditional	B
probabilities	O
on	O
directed	B
graphical	O
structures	O
networks	O
stinchecombe	O
m	O
and	O
h	O
white	O
universal	O
approximation	O
using	O
feed-forward	O
networks	O
with	O
non-sigmoid	O
hidden	O
layer	O
activation	O
functions	O
in	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
volume	O
pp	O
ieee	O
stone	O
j	O
v	O
independent	B
component	O
analy	O
sis	O
a	O
tutorial	O
introduction	O
mit	O
press	O
sung	O
k	O
k	O
and	O
t	O
poggio	O
example-based	O
learning	B
for	O
view-based	O
human	O
face	B
detection	I
a	O
i	O
memo	O
mit	O
sutton	O
r	O
s	O
and	O
a	O
g	O
barto	O
reinforcement	B
learning	B
an	O
introduction	O
mit	O
press	O
tarassenko	O
l	O
novelty	B
detection	I
for	O
the	O
identification	O
of	O
masses	O
in	O
mamograms	O
in	O
proceedings	O
fourth	O
iee	O
international	O
conference	O
on	O
artificial	O
neural	O
networks	O
volume	O
pp	O
iee	O
tax	O
d	O
and	O
r	O
duin	O
data	O
domain	O
description	O
by	O
support	O
vectors	O
in	O
m	O
verleysen	O
proceedings	O
european	O
symposium	O
on	O
artificial	O
neural	O
networks	O
esann	O
pp	O
d	O
facto	O
press	O
teh	O
y	O
w	O
m	O
i	O
jordan	O
m	O
j	O
beal	O
and	O
d	O
m	O
blei	O
hierarchical	B
dirichlet	B
processes	O
journal	O
of	O
the	O
americal	O
statistical	O
association	O
to	O
appear	O
tenenbaum	O
j	O
b	O
v	O
de	O
silva	O
and	O
j	O
c	O
langford	O
december	O
a	O
global	O
framework	O
for	O
nonlinear	O
dimensionality	O
reduction	O
science	O
tesauro	O
g	O
td-gammon	O
a	O
self-teaching	O
backgammon	B
program	O
achieves	O
master-level	O
play	O
neural	O
computation	O
thiesson	O
b	O
d	O
m	O
chickering	O
d	O
heckerman	O
and	O
c	O
meek	O
arma	O
time-series	O
modelling	O
with	O
graphical	O
models	O
in	O
m	O
chickering	O
and	O
j	O
halpern	O
proceedings	O
of	O
the	O
twentieth	O
conference	O
on	O
uncertainty	O
in	O
artificial	O
intelligence	O
banff	O
canada	O
pp	O
auai	O
press	O
tibshirani	O
r	O
regression	B
shrinkage	B
and	O
selection	O
via	O
the	O
lasso	B
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
tierney	O
l	O
markov	O
chains	O
for	O
exploring	O
posterior	O
distributions	O
annals	O
of	O
statistics	O
tikhonov	B
a	O
n	O
and	O
v	O
y	O
arsenin	O
solutions	O
of	O
ill-posed	O
problems	O
v	O
h	O
winston	O
tino	O
p	O
and	O
i	O
t	O
nabney	O
hierarchical	B
gtm	O
constructing	O
localized	O
non-linear	O
projection	O
manifolds	O
in	O
a	O
principled	O
way	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
svens	O
en	O
m	O
and	O
c	O
m	O
bishop	O
robust	O
bayesian	B
mixture	B
modelling	O
neurocomputing	O
tino	O
p	O
i	O
t	O
nabney	O
and	O
y	O
sun	O
using	O
directional	O
curvatures	O
to	O
visualize	O
folding	O
patterns	O
of	O
the	O
gtm	O
projection	O
manifolds	O
in	O
references	O
g	O
dorffner	O
h	O
bischof	O
and	O
k	O
hornik	O
artificial	O
neural	O
networks	O
icann	O
pp	O
springer	O
vapnik	O
v	O
n	O
estimation	O
of	O
dependences	O
based	O
on	O
empirical	O
data	O
springer	O
vapnik	O
v	O
n	O
the	O
nature	O
of	O
statistical	O
learn	O
tipping	O
m	O
e	O
probabilistic	O
visualisation	O
of	O
high-dimensional	O
binary	O
data	O
in	O
m	O
s	O
kearns	O
s	O
a	O
solla	O
and	O
d	O
a	O
cohn	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
tipping	O
m	O
e	O
sparse	O
bayesian	B
learning	B
and	O
the	O
relevance	B
vector	I
machine	I
journal	O
of	O
machine	O
learning	B
research	O
tipping	O
m	O
e	O
and	O
c	O
m	O
bishop	O
probabilistic	O
principal	B
component	I
analysis	I
technical	O
report	O
neural	O
computing	O
research	O
group	O
aston	O
university	O
tipping	O
m	O
e	O
and	O
c	O
m	O
bishop	O
mixtures	O
of	O
probabilistic	O
principal	O
component	O
analyzers	O
neural	O
computation	O
tipping	O
m	O
e	O
and	O
c	O
m	O
bishop	O
probabilistic	O
principal	B
component	I
analysis	I
journal	O
of	O
the	O
royal	O
statistical	O
society	O
series	O
b	O
tipping	O
m	O
e	O
and	O
a	O
faul	O
fast	O
marginal	B
likelihood	I
maximization	O
for	O
sparse	O
bayesian	B
models	O
in	O
c	O
m	O
bishop	O
and	O
b	O
frey	O
proceedings	O
ninth	O
international	O
workshop	O
on	O
artificial	O
intelligence	O
and	O
statistics	O
key	O
west	O
florida	O
tong	O
s	O
and	O
d	O
koller	O
restricted	O
bayes	B
optimal	O
classifiers	O
in	O
proceedings	O
national	O
conference	O
on	O
artificial	O
intelligence	O
pp	O
aaai	O
tresp	O
v	O
scaling	O
kernel-based	O
systems	O
to	O
large	O
data	O
sets	O
data	O
mining	O
and	O
knowledge	O
discovery	O
uhlenbeck	O
g	O
e	O
and	O
l	O
s	O
ornstein	O
on	O
the	O
theory	B
of	O
brownian	O
motion	O
phys	O
rev	O
ing	B
theory	B
springer	O
vapnik	O
v	O
n	O
statistical	O
learning	B
theory	B
wi	O
ley	O
veropoulos	O
k	O
c	O
campbell	O
and	O
n	O
cristianini	O
controlling	O
the	O
sensitivity	O
of	O
support	B
vector	I
machines	O
in	O
proceedings	O
of	O
the	O
international	O
joint	O
conference	O
on	O
artificial	O
intelligence	O
workshop	O
pp	O
vidakovic	O
b	O
statistical	O
modelling	O
by	O
wavelets	B
wiley	O
viola	O
p	O
and	O
m	O
jones	O
robust	O
real-time	O
face	B
detection	I
international	O
journal	O
of	O
computer	O
vision	O
viterbi	O
a	O
j	O
error	B
bounds	O
for	O
convolutional	B
codes	O
and	O
an	O
asymptotically	O
optimum	O
decoding	O
algorithm	O
ieee	O
transactions	O
on	O
information	B
theory	B
viterbi	O
a	O
j	O
and	O
j	O
k	O
omura	O
principles	O
of	O
digital	O
communication	O
and	O
coding	O
mcgrawhill	O
wahba	O
g	O
a	O
comparison	O
of	O
gcv	O
and	O
gml	O
for	O
choosing	O
the	O
smoothing	B
parameter	I
in	O
the	O
generalized	B
spline	O
smoothing	O
problem	O
numerical	O
mathematics	O
wainwright	O
m	O
j	O
t	O
s	O
jaakkola	O
and	O
a	O
s	O
willsky	O
a	O
new	O
class	O
of	O
upper	O
bounds	O
on	O
the	O
log	O
partition	B
function	I
ieee	O
transactions	O
on	O
information	B
theory	B
walker	O
a	O
m	O
on	O
the	O
asymptotic	O
behaviour	O
of	O
posterior	O
distributions	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
walker	O
s	O
g	O
p	O
damien	O
p	O
w	O
laud	O
and	O
a	O
f	O
m	O
smith	O
bayesian	B
nonparametric	O
inference	B
for	O
random	O
distributions	O
and	O
related	O
functions	O
discussion	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
valiant	O
l	O
g	O
a	O
theory	B
of	O
the	O
learnable	O
communications	O
of	O
the	O
association	O
for	O
computing	O
machinery	O
watson	O
g	O
s	O
smooth	O
regression	B
analysis	O
sankhy	O
a	O
the	O
indian	O
journal	O
of	O
statistics	O
series	O
a	O
references	O
webb	O
a	O
r	O
functional	B
approximation	O
by	O
feed-forward	O
networks	O
a	O
least-squares	O
approach	O
to	O
generalisation	O
ieee	O
transactions	O
on	O
neural	O
networks	O
williams	O
o	O
a	O
blake	O
and	O
r	O
cipolla	O
sparse	O
bayesian	B
learning	B
for	O
efficient	O
visual	O
tracking	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
williams	O
p	O
m	O
using	O
neural	O
networks	O
to	O
model	O
conditional	B
multivariate	O
densities	O
neural	O
computation	O
winn	O
j	O
and	O
c	O
m	O
bishop	O
variational	B
message	B
passing	I
journal	O
of	O
machine	O
learning	B
research	O
zarchan	O
p	O
and	O
h	O
musoff	O
fundamentals	O
of	O
kalman	O
filtering	O
a	O
practical	O
approach	O
ed	O
aiaa	O
weisstein	O
e	O
w	O
crc	O
concise	O
encyclopedia	O
of	O
mathematics	O
chapman	O
and	O
hall	O
and	O
crc	O
weston	O
j	O
and	O
c	O
watkins	O
multi-class	O
support	B
vector	I
machines	O
in	O
m	O
verlysen	O
proceedings	O
esann	O
brussels	O
d-facto	O
publications	O
whittaker	O
j	O
graphical	O
models	O
in	O
applied	O
multivariate	O
statistics	O
wiley	O
widrow	O
b	O
and	O
m	O
e	O
hoff	O
adaptive	O
switching	B
circuits	O
in	O
ire	O
wescon	O
convention	O
record	O
volume	O
pp	O
reprinted	O
in	O
anderson	O
and	O
rosenfeld	O
widrow	O
b	O
and	O
m	O
a	O
lehr	O
years	O
of	O
adaptive	O
neural	O
networks	O
perceptron	B
madeline	O
and	O
backpropagation	B
proceedings	O
of	O
the	O
ieee	O
wiegerinck	O
w	O
and	O
t	O
heskes	O
fractional	B
belief	B
propagation	I
in	O
s	O
becker	O
s	O
thrun	O
and	O
k	O
obermayer	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
williams	O
c	O
k	O
i	O
computation	O
with	O
infinite	O
neural	O
networks	O
neural	O
computation	O
williams	O
c	O
k	O
i	O
prediction	O
with	O
gaussian	B
processes	O
from	O
linear	B
regression	B
to	O
linear	O
prediction	O
and	O
beyond	O
in	O
m	O
i	O
jordan	O
learning	B
in	O
graphical	O
models	O
pp	O
mit	O
press	O
williams	O
c	O
k	O
i	O
and	O
d	O
barber	O
bayesian	B
classification	B
with	O
gaussian	B
processes	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
williams	O
c	O
k	O
i	O
and	O
m	O
seeger	O
using	O
the	O
nystrom	O
method	O
to	O
speed	O
up	O
kernel	O
machines	O
in	O
t	O
k	O
leen	O
t	O
g	O
dietterich	O
and	O
v	O
tresp	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pp	O
mit	O
press	O
index	O
index	O
page	O
numbers	O
in	O
bold	O
indicate	O
the	O
primary	O
source	O
of	O
information	O
for	O
the	O
corresponding	O
topic	O
coding	O
scheme	O
acceptance	B
criterion	I
activation	B
function	I
active	B
constraint	I
adaboost	B
adaline	B
adaptive	B
rejection	B
sampling	I
adf	O
see	O
assumed	B
density	B
filtering	I
aic	O
see	O
akaike	B
information	B
criterion	I
akaike	B
information	B
criterion	I
family	O
of	O
divergences	O
recursion	O
ancestral	B
sampling	I
annular	O
flow	O
ar	O
model	O
see	O
autoregressive	B
model	I
arc	B
ard	O
see	O
automatic	B
relevance	I
determination	I
arma	O
see	O
autoregressive	B
moving	I
average	I
assumed	B
density	B
filtering	I
autoassociative	B
networks	I
automatic	B
relevance	I
determination	I
autoregressive	B
hidden	B
markov	B
model	I
autoregressive	B
model	I
autoregressive	B
moving	I
average	I
back-tracking	B
backgammon	B
backpropagation	B
bagging	B
basis	B
function	I
batch	B
training	B
baum-welch	B
algorithm	I
bayes	B
theorem	O
bayes	B
thomas	O
bayesian	B
analysis	I
vii	O
hierarchical	B
model	B
averaging	I
bayesian	B
information	B
criterion	I
bayesian	B
model	B
comparison	I
bayesian	B
network	I
bayesian	B
probability	B
belief	B
propagation	I
bernoulli	B
distribution	I
mixture	B
model	I
bernoulli	B
jacob	O
beta	B
distribution	I
beta	B
recursion	I
between-class	B
covariance	B
bias	B
bias	B
parameter	I
bias-variance	B
trade-off	I
bic	O
see	O
bayesian	B
information	B
criterion	I
binary	B
entropy	B
binomial	B
distribution	I
index	O
biological	B
sequence	I
bipartite	B
graph	I
bits	B
blind	B
source	I
separation	I
blocked	B
path	I
boltzmann	B
distribution	I
boltzmann	B
ludwig	O
eduard	O
boolean	B
logic	I
boosting	B
bootstrap	B
bootstrap	B
filter	I
box	B
constraints	I
box-muller	B
method	I
calculus	B
of	I
variations	I
canonical	B
correlation	I
analysis	I
canonical	B
link	B
function	I
cart	O
see	O
classification	B
and	I
regression	B
trees	I
cauchy	B
distribution	I
causality	B
cca	O
see	O
canonical	B
correlation	I
analysis	I
central	B
differences	I
central	B
limit	I
theorem	I
chain	B
graph	I
chaining	B
chapman-kolmogorov	B
equations	I
child	B
node	B
cholesky	B
decomposition	I
chunking	B
circular	O
normal	O
see	O
von	B
mises	I
distribution	I
classical	B
probability	B
classification	B
classification	B
and	I
regression	B
trees	I
clique	B
clustering	B
clutter	B
problem	I
co-parents	B
code-book	B
vectors	I
combining	B
models	I
committee	B
complete	B
data	I
set	I
completing	B
the	I
square	I
computational	B
learning	B
theory	B
concave	B
function	I
concentration	B
parameter	I
condensation	B
algorithm	I
conditional	B
entropy	B
conditional	B
expectation	B
conditional	B
independence	I
conditional	B
mixture	B
model	I
see	O
mixture	B
model	I
conditional	B
probability	B
conjugate	B
prior	B
convex	B
duality	I
convex	B
function	I
convolutional	B
neural	B
network	I
correlation	B
matrix	I
cost	B
function	I
covariance	B
between-class	B
within-class	B
covariance	B
matrix	O
diagonal	B
isotropic	B
partitioned	B
positive	B
definite	I
cox	O
s	O
axioms	O
credit	B
assignment	I
cross-entropy	B
error	B
function	I
cross-validation	B
cumulative	B
distribution	I
function	I
curse	B
of	I
dimensionality	I
curve	B
fitting	I
d	O
map	O
see	O
dependency	B
map	I
d-separation	B
dag	O
see	O
directed	B
acyclic	I
graph	I
dagsvm	B
data	B
augmentation	I
data	B
compression	I
decision	B
boundary	I
decision	B
region	I
decision	O
surface	O
see	O
decision	B
boundary	I
decision	B
theory	B
decision	B
tree	B
decomposition	B
methods	I
degrees	B
of	I
freedom	I
degrees-of-freedom	B
parameter	I
density	B
estimation	I
index	O
density	B
network	I
dependency	B
map	I
descendant	B
node	B
design	B
matrix	I
differential	B
entropy	B
digamma	B
function	I
directed	B
acyclic	I
graph	I
directed	B
cycle	I
directed	B
factorization	B
dirichlet	B
distribution	I
dirichlet	B
lejeune	O
discriminant	B
function	I
discriminative	B
model	I
distortion	B
measure	I
distributive	B
law	I
of	I
multiplication	I
dna	B
document	B
retrieval	I
dual	B
representation	I
dual-energy	B
gamma	B
densitometry	I
dynamic	B
programming	I
dynamical	B
system	I
e	O
step	O
see	O
expectation	B
step	I
early	B
stopping	I
ecm	O
see	O
expectation	B
conditional	B
maximization	I
edge	B
effective	B
number	I
of	I
observations	I
effective	B
number	I
of	I
parameters	I
elliptical	B
k-means	I
em	B
see	O
expectation	B
maximization	I
emission	B
probability	B
empirical	O
bayes	B
see	O
evidence	B
approximation	I
energy	B
function	I
entropy	B
conditional	B
differential	B
relative	B
ep	O
see	O
expectation	B
propagation	I
error	B
function	I
equality	B
constraint	I
equivalent	B
kernel	I
erf	B
function	I
error	B
backpropagation	B
see	O
backpropagation	B
error	B
function	I
error-correcting	B
output	I
codes	I
euler	B
leonhard	O
euler-lagrange	B
equations	I
evidence	B
approximation	I
evidence	B
function	I
expectation	B
expectation	B
conditional	B
maximization	I
expectation	B
maximization	I
gaussian	B
mixture	B
generalized	B
sampling	B
methods	I
expectation	B
propagation	I
expectation	B
step	I
explaining	B
away	I
exploitation	B
exploration	B
exponential	B
distribution	I
exponential	B
family	I
extensive	B
variables	I
face	B
detection	I
face	B
tracking	I
factor	B
analysis	I
mixture	B
model	I
factor	B
graph	I
factor	B
loading	I
factorial	B
hidden	B
markov	B
model	I
factorized	B
distribution	I
feature	B
extraction	I
feature	B
map	I
feature	B
space	I
fisher	B
information	I
matrix	I
fisher	B
kernel	I
fisher	B
s	O
linear	B
discriminant	I
flooding	O
schedule	B
forward	B
kinematics	I
forward	B
problem	I
forward	B
propagation	I
forward-backward	B
algorithm	I
fractional	B
belief	B
propagation	I
frequentist	B
probability	B
fuel	B
system	I
function	B
interpolation	I
functional	B
derivative	B
index	O
gamma	B
densitometry	I
gamma	B
distribution	I
gamma	B
function	I
gating	B
function	I
gauss	B
carl	O
friedrich	O
gaussian	B
conditional	B
marginal	B
maximum	B
likelihood	I
mixture	B
sequential	B
estimation	I
sufficient	B
statistics	I
wrapped	B
gaussian	B
kernel	I
gaussian	B
process	I
gaussian	B
random	I
field	I
gaussian-gamma	B
distribution	I
gaussian-wishart	B
distribution	I
gem	O
see	O
expectation	B
maximization	I
generalized	B
generalization	B
generalized	B
linear	I
model	I
generalized	B
maximum	B
likelihood	I
see	O
evidence	O
ap	O
proximation	O
generative	B
model	I
generative	B
topographic	I
mapping	I
directional	B
curvature	I
magnification	B
factor	I
geodesic	B
distance	I
gibbs	B
sampling	I
blocking	B
gibbs	B
josiah	O
willard	O
gini	B
index	I
global	B
minimum	I
gradient	B
descent	I
gram	B
matrix	I
graph-cut	B
algorithm	I
graphical	B
model	I
bipartite	B
directed	B
factorization	B
fully	B
connected	I
inference	B
tree	B
treewidth	B
triangulated	B
undirected	B
green	O
s	O
function	O
gtm	O
see	O
generative	B
topographic	I
mapping	I
hamilton	B
william	O
rowan	O
hamiltonian	B
dynamics	I
hamiltonian	B
function	I
hammersley-clifford	B
theorem	I
handwriting	B
recognition	I
handwritten	B
digit	I
head-to-head	B
path	I
head-to-tail	B
path	I
heaviside	B
step	I
function	I
hellinger	B
distance	I
hessian	B
matrix	I
diagonal	B
approximation	I
exact	B
evaluation	I
fast	B
multiplication	I
finite	O
differences	O
inverse	B
outer	B
product	I
approximation	I
heteroscedastic	B
hidden	B
markov	B
model	I
autoregressive	B
factorial	B
forward-backward	B
algorithm	I
input-output	B
left-to-right	B
maximum	B
likelihood	I
scaling	B
factor	I
sum-product	B
algorithm	I
switching	B
variational	B
inference	B
hidden	B
unit	I
hidden	B
variable	I
hierarchical	B
bayesian	B
model	I
hierarchical	B
mixture	B
of	I
experts	I
hinge	B
error	B
function	I
hinton	B
diagram	I
histogram	B
density	B
estimation	I
hme	O
see	O
hierarchical	B
mixture	B
of	I
experts	I
hold-out	B
set	I
homogeneous	B
flow	O
homogeneous	B
kernel	I
homogeneous	B
markov	B
chain	I
hooke	O
s	O
law	O
hybrid	B
monte	I
carlo	I
hyperparameter	B
hyperprior	B
i	O
map	O
see	O
independence	B
map	I
i	O
i	O
d	O
see	O
independent	B
identically	I
distributed	I
ica	O
see	O
independent	B
component	I
analysis	I
icm	O
see	O
iterated	B
conditional	B
modes	I
identifiability	B
image	B
de-noising	I
importance	B
sampling	I
importance	B
weights	I
improper	B
prior	B
imputation	B
step	I
imputation-posterior	B
algorithm	I
inactive	B
constraint	I
incomplete	B
data	I
set	I
independence	B
map	I
independent	B
component	I
analysis	I
independent	B
factor	B
analysis	I
independent	B
identically	I
distributed	I
independent	B
variables	I
independent	B
identically	I
distributed	I
induced	B
factorization	B
inequality	B
constraint	I
inference	B
information	B
criterion	I
information	B
geometry	I
information	B
theory	B
input-output	B
hidden	B
markov	B
model	I
intensive	B
variables	I
intrinsic	B
dimensionality	I
invariance	B
inverse	B
gamma	B
distribution	I
inverse	B
kinematics	I
inverse	B
problem	I
inverse	B
wishart	B
distribution	I
ip	O
algorithm	O
see	O
imputation-posterior	B
algorithm	I
irls	O
see	O
iterative	B
reweighted	I
least	I
squares	I
ising	B
model	I
isomap	B
isometric	B
feature	B
map	I
iterated	B
conditional	B
modes	I
index	O
iterative	B
reweighted	I
least	I
squares	I
jacobian	B
matrix	I
jensen	O
s	O
inequality	O
join	B
tree	B
junction	B
tree	B
algorithm	I
k	B
nearest	I
neighbours	I
k-means	B
clustering	B
algorithm	I
k-medoids	B
algorithm	I
kalman	B
filter	I
extended	B
kalman	B
gain	I
matrix	I
kalman	B
smoother	I
karhunen-loeve	O
transform	O
karush-kuhn-tucker	B
conditions	I
kernel	B
density	B
estimator	I
kernel	B
function	I
fisher	B
gaussian	B
homogeneous	B
nonvectorial	B
inputs	I
stationary	B
kernel	B
pca	I
kernel	B
regression	B
kernel	B
substitution	I
kernel	B
trick	I
kinetic	B
energy	I
kkt	O
see	O
karush-kuhn-tucker	B
conditions	I
kl	O
divergence	O
see	O
kullback-leibler	B
divergence	I
kriging	O
see	O
gaussian	B
process	I
kullback-leibler	B
divergence	I
lagrange	B
multiplier	I
lagrange	B
joseph-louis	O
lagrangian	B
laminar	O
flow	O
laplace	B
approximation	I
laplace	B
pierre-simon	O
large	O
margin	B
see	O
margin	B
lasso	B
latent	B
class	I
analysis	I
latent	B
trait	I
model	I
latent	B
variable	I
index	O
lattice	B
diagram	I
lds	O
see	O
linear	B
dynamical	B
system	I
leapfrog	B
discretization	I
learning	B
learning	B
rate	I
parameter	I
least-mean-squares	B
algorithm	I
leave-one-out	B
likelihood	B
function	I
likelihood	B
weighted	I
sampling	I
linear	B
discriminant	I
fisher	B
linear	B
dynamical	B
system	I
inference	B
linear	B
independence	I
linear	B
regression	B
em	B
mixture	B
model	I
variational	B
linear	B
smoother	I
linear-gaussian	B
model	I
linearly	B
separable	I
link	B
link	B
function	I
liouville	O
s	O
theorem	O
lle	O
see	O
locally	B
linear	I
embedding	I
lms	O
algorithm	O
see	O
least-mean-squares	B
algorithm	I
local	B
minimum	I
local	B
receptive	I
field	I
locally	B
linear	I
embedding	I
location	B
parameter	I
log	B
odds	I
logic	B
sampling	I
logistic	B
regression	B
bayesian	B
mixture	B
model	I
multiclass	B
logistic	B
sigmoid	I
logit	B
function	I
loopy	B
belief	B
propagation	I
loss	B
function	I
loss	B
matrix	I
lossless	B
data	B
compression	I
lossy	B
data	B
compression	I
lower	B
bound	I
m	O
step	O
see	O
maximization	B
step	I
machine	O
learning	B
vii	O
macrostate	B
mahalanobis	B
distance	I
manifold	B
map	O
see	O
maximum	B
posterior	I
margin	B
error	B
soft	B
marginal	B
likelihood	I
marginal	B
probability	B
markov	B
blanket	I
markov	O
boundary	O
see	O
markov	B
blanket	I
markov	B
chain	I
first	O
order	O
homogeneous	B
second	B
order	I
markov	B
chain	I
monte	I
carlo	I
markov	B
model	I
homogeneous	B
markov	O
network	O
see	O
markov	B
random	I
field	I
markov	B
random	I
field	I
max-sum	B
algorithm	I
maximal	B
clique	B
maximal	B
spanning	I
tree	B
maximization	B
step	I
maximum	B
likelihood	I
gaussian	B
mixture	B
singularities	B
type	O
see	O
evidence	B
approximation	I
maximum	O
margin	B
see	O
margin	B
maximum	B
posterior	I
mcmc	O
see	O
markov	B
chain	I
monte	I
carlo	I
mdn	O
see	O
mixture	B
density	B
network	I
mds	O
see	O
multidimensional	B
scaling	I
mean	B
mean	B
field	I
theory	B
mean	B
value	I
theorem	I
measure	B
theory	B
memory-based	B
methods	I
message	B
passing	I
pending	B
message	I
schedule	B
variational	B
metropolis	B
algorithm	I
metropolis-hastings	B
algorithm	I
index	O
microstate	B
minimum	B
risk	I
minkowski	B
loss	I
missing	B
at	I
random	I
missing	B
data	I
mixing	B
coefficient	I
mixture	B
component	I
mixture	B
density	B
network	I
mixture	B
distribution	I
see	O
mixture	B
model	I
mixture	B
model	I
conditional	B
linear	B
regression	B
logistic	B
regression	B
symmetries	B
mixture	B
of	I
experts	I
mixture	B
of	I
gaussians	I
mlp	O
see	O
multilayer	B
perceptron	B
mnist	B
data	I
model	B
comparison	I
model	B
evidence	I
model	B
selection	I
moment	B
matching	I
momentum	B
variable	I
monte	B
carlo	I
em	B
algorithm	I
monte	B
carlo	I
sampling	I
moore-penrose	O
pseudo-inverse	B
see	O
pseudo-inverse	B
moralization	B
mrf	O
see	O
markov	B
random	I
field	I
multidimensional	B
scaling	I
multilayer	B
perceptron	B
multimodality	B
multinomial	B
distribution	I
multiplicity	B
mutual	B
information	I
nadaraya-watson	O
see	O
kernel	B
regression	B
naive	B
bayes	B
model	I
nats	B
natural	B
language	I
modelling	I
natural	B
parameters	I
nearest-neighbour	B
methods	I
neural	B
network	I
convolutional	B
regularization	B
relation	B
to	I
gaussian	B
process	I
newton-raphson	B
node	B
noiseless	B
coding	I
theorem	I
nonidentifiability	B
noninformative	B
prior	B
nonparametric	B
methods	I
normal	O
distribution	O
see	O
gaussian	B
normal	B
equations	I
normal-gamma	B
distribution	I
normal-wishart	B
distribution	I
normalized	O
exponential	O
see	O
softmax	B
function	I
novelty	B
detection	I
object	B
recognition	I
observed	B
variable	I
occam	B
factor	I
oil	O
flow	O
data	O
old	B
faithful	I
data	I
on-line	O
learning	B
see	O
sequential	B
learning	B
one-versus-one	B
classifier	I
one-versus-the-rest	B
classifier	I
ordered	B
over-relaxation	B
ornstein-uhlenbeck	B
process	I
orthogonal	B
least	I
squares	I
outlier	B
outliers	B
over-fitting	B
over-relaxation	B
pac	O
learning	B
see	O
probably	B
approximately	I
correct	I
pac-bayesian	B
framework	I
parameter	B
shrinkage	B
parent	B
node	B
particle	B
filter	I
partition	B
function	I
parzen	O
estimator	O
see	O
kernel	B
density	B
estimator	I
parzen	B
window	I
pattern	O
recognition	O
vii	O
pca	O
see	O
principal	B
component	I
analysis	I
pending	B
message	I
perceptron	B
convergence	B
theorem	I
hardware	B
perceptron	B
criterion	I
perfect	B
map	I
index	O
periodic	B
variable	I
phase	B
space	I
photon	B
noise	I
plate	B
polynomial	B
curve	B
fitting	I
polytree	B
position	B
variable	I
positive	B
definite	I
covariance	B
positive	B
definite	I
matrix	I
positive	B
semidefinite	I
covariance	B
positive	B
semidefinite	I
matrix	I
posterior	B
probability	B
posterior	B
step	I
potential	B
energy	I
potential	B
function	I
power	B
ep	I
power	B
method	I
precision	B
matrix	I
precision	B
parameter	I
predictive	B
distribution	I
preprocessing	B
principal	B
component	I
analysis	I
bayesian	B
em	B
algorithm	I
gibbs	B
sampling	I
mixture	B
distribution	I
physical	B
analogy	I
principal	B
curve	I
principal	B
subspace	I
principal	B
surface	I
prior	B
conjugate	B
consistent	B
improper	B
noninformative	B
probabilistic	O
graphical	B
model	I
see	O
graphical	B
model	I
probabilistic	B
pca	I
probability	B
bayesian	B
classical	B
density	B
frequentist	B
mass	B
function	I
prior	B
product	B
rule	I
sum	B
rule	I
theory	B
probably	B
approximately	I
correct	I
probit	B
function	I
probit	B
regression	B
product	B
rule	I
of	I
probability	B
proposal	B
distribution	I
protected	B
conjugate	B
gradients	I
protein	B
sequence	I
pseudo-inverse	B
pseudo-random	B
numbers	I
quadratic	B
discriminant	I
quality	B
parameter	I
radial	B
basis	B
function	I
rauch-tung-striebel	B
equations	I
regression	B
regression	B
function	I
regularization	B
tikhonov	B
regularized	B
least	I
squares	I
reinforcement	B
learning	B
reject	B
option	I
rejection	B
sampling	I
relative	B
entropy	B
relevance	B
vector	I
relevance	B
vector	I
machine	I
responsibility	B
ridge	B
regression	B
rms	O
error	B
see	O
root-mean-square	B
error	B
robbins-monro	B
algorithm	I
robot	B
arm	I
robustness	B
root	B
node	B
root-mean-square	B
error	B
rosenblatt	B
frank	O
rotation	B
invariance	B
rts	O
equations	O
see	O
rauch-tung-striebel	B
equations	I
running	B
intersection	I
property	I
rvm	O
see	O
relevance	B
vector	I
machine	I
sample	B
mean	B
sample	B
variance	B
sampling-importance-resampling	B
scale	B
invariance	B
scale	B
parameter	I
scaling	B
factor	I
schwarz	O
criterion	O
see	O
bayesian	B
information	O
crite	O
rion	O
self-organizing	B
map	I
sequential	B
data	I
sequential	B
estimation	I
sequential	B
gradient	B
descent	I
sequential	B
learning	B
sequential	B
minimal	I
optimization	I
serial	B
message	B
passing	I
schedule	B
shannon	B
claude	O
shared	B
parameters	I
shrinkage	B
shur	B
complement	I
sigmoid	O
see	O
logistic	B
sigmoid	I
simplex	B
single-class	B
support	B
vector	I
machine	I
singular	B
value	I
decomposition	I
sinusoidal	B
data	I
sir	O
see	O
sampling-importance-resampling	B
skip-layer	B
connection	I
slack	B
variable	I
slice	B
sampling	I
smo	O
see	O
sequential	B
minimal	I
optimization	I
smoother	B
matrix	I
smoothing	B
parameter	I
soft	B
margin	B
soft	B
weight	B
sharing	I
softmax	B
function	I
som	O
see	O
self-organizing	B
map	I
sparsity	B
sparsity	B
parameter	I
spectrogram	B
speech	B
recognition	I
sphereing	B
spline	B
functions	I
standard	B
deviation	I
standardizing	B
state	B
space	I
model	I
switching	B
stationary	B
kernel	I
statistical	O
bias	B
see	O
bias	B
statistical	O
independence	O
see	O
independent	B
variables	I
index	O
statistical	O
learning	B
theory	B
see	O
computational	O
learn	O
ing	B
theory	B
steepest	B
descent	I
stirling	O
s	O
approximation	O
stochastic	B
stochastic	B
em	B
stochastic	B
gradient	B
descent	I
stochastic	B
process	I
stratified	O
flow	O
student	O
s	O
t-distribution	O
subsampling	B
sufficient	B
statistics	I
sum	B
rule	I
of	I
probability	B
sum-of-squares	B
error	B
sum-product	B
algorithm	I
for	B
hidden	B
markov	B
model	I
supervised	B
learning	B
support	B
vector	I
support	B
vector	I
machine	I
for	B
regression	B
multiclass	B
survival	B
of	I
the	I
fittest	I
svd	O
see	O
singular	B
value	I
decomposition	I
svm	O
see	O
support	B
vector	I
machine	I
switching	B
hidden	B
markov	B
model	I
switching	B
state	B
space	I
model	I
synthetic	B
data	I
sets	I
tail-to-tail	B
path	I
tangent	B
distance	I
tangent	B
propagation	I
tapped	B
delay	I
line	I
target	B
vector	I
test	B
set	I
threshold	B
parameter	I
tied	B
parameters	I
tikhonov	B
regularization	B
time	B
warping	I
tomography	B
training	B
training	B
set	I
transition	B
probability	B
translation	B
invariance	B
tree-reweighted	B
message	B
passing	I
treewidth	B
index	O
trellis	O
diagram	O
see	O
lattice	B
diagram	I
triangulated	B
graph	I
type	O
maximum	B
likelihood	I
see	O
evidence	O
approxi	O
mation	O
undetermined	O
multiplier	O
see	O
lagrange	B
multiplier	I
undirected	B
graph	O
see	O
markov	B
random	I
field	I
uniform	B
distribution	I
uniform	B
sampling	I
uniquenesses	B
unobserved	O
variable	O
see	O
latent	B
variable	I
unsupervised	B
learning	B
utility	B
function	I
validation	B
set	I
vapnik-chervonenkis	B
dimension	I
variance	B
variational	B
inference	B
for	B
gaussian	B
mixture	B
for	B
hidden	B
markov	B
model	I
local	B
vc	O
dimension	O
see	O
vapnik-chervonenkis	O
dimen	O
sion	O
vector	B
quantization	I
vertex	O
see	O
node	B
visualization	B
viterbi	B
algorithm	I
von	B
mises	I
distribution	I
wavelets	B
weak	B
learner	I
weight	B
decay	I
weight	B
parameter	I
weight	B
sharing	I
soft	B
weight	B
vector	I
weight-space	B
symmetry	I
weighted	B
least	I
squares	I
well-determined	B
parameters	I
whitening	B
wishart	B
distribution	I
within-class	B
covariance	B
woodbury	B
identity	I
wrapped	B
distribution	I
yellowstone	B
national	I
park	I
