the	O
work	O
computer	O
age	O
statistical	O
inference	B
was	O
first	O
published	O
by	O
cambridge	O
university	O
press	O
in	O
the	O
work	O
bradley	O
efron	O
and	O
trevor	O
hastie	O
cambridge	O
university	O
press	O
s	O
catalogue	O
entry	O
for	O
the	O
work	O
can	O
be	O
found	O
at	O
http	O
www	O
cambridge	O
org	O
nb	O
the	O
copy	O
of	O
the	O
work	O
as	O
displayed	O
on	O
this	O
website	O
can	O
be	O
purchased	O
through	O
cambridge	O
university	O
press	O
and	O
other	O
standard	O
distribution	B
channels	O
this	O
copy	O
is	O
made	O
available	O
for	O
personal	O
use	O
only	O
and	O
must	O
not	O
be	O
adapted	O
sold	O
or	O
re-distributed	O
corrected	O
november	O
the	O
twenty-first	O
century	O
has	O
seen	O
a	O
breathtaking	O
expansion	O
of	O
statistical	O
methodology	O
both	O
in	O
scope	O
and	O
in	O
influence	O
big	O
data	B
data	B
science	I
and	O
machine	B
learning	I
have	O
become	O
familiar	O
terms	O
in	O
the	O
news	O
as	O
statistical	O
methods	O
are	O
brought	O
to	O
bear	O
upon	O
the	O
enormous	O
data	B
sets	O
of	O
modern	O
science	O
and	O
commerce	O
how	O
did	O
we	O
get	O
here	O
and	O
where	O
are	O
we	O
going	O
this	O
book	O
takes	O
us	O
on	O
an	O
exhilarating	O
journey	O
through	O
the	O
revolution	O
in	O
data	B
analysis	B
following	O
the	O
introduction	O
of	O
electronic	O
computation	O
in	O
the	O
beginning	O
with	O
classical	O
inferential	O
theories	O
bayesian	B
frequentist	B
fisherian	O
individual	O
chapters	O
take	O
up	O
a	O
series	O
of	O
influential	O
topics	O
survival	B
analysis	B
logistic	B
regression	B
empirical	B
bayes	I
the	O
jackknife	B
and	O
bootstrap	B
random	O
forests	O
neural	O
networks	O
markov	O
chain	O
monte	O
carlo	O
inference	B
after	B
model	B
selection	I
and	O
dozens	O
more	O
the	O
distinctly	O
modern	O
approach	O
integrates	O
methodology	O
and	O
algorithms	O
with	O
statistical	O
inference	B
the	O
book	O
ends	O
with	O
speculation	O
on	O
the	O
future	O
direction	O
of	O
statistics	B
and	O
data	B
science	I
efron	O
hastiecomputer	O
age	O
statistical	O
inference	B
how	O
and	O
why	O
is	O
computational	O
statistics	B
taking	O
over	O
the	O
world	O
in	O
this	O
serious	O
work	O
of	O
synthesis	O
that	O
is	O
also	O
fun	O
to	O
read	O
efron	O
and	O
hastie	O
give	O
their	O
take	O
on	O
the	O
unreasonable	O
effectiveness	O
of	O
statistics	B
and	O
machine	B
learning	I
in	O
the	O
context	O
of	O
a	O
series	O
of	O
clear	O
historically	O
informed	O
examples	O
andrew	O
gelman	O
columbia	O
university	O
computer	O
age	O
statistical	O
inference	B
is	O
written	O
especially	O
for	O
those	O
who	O
want	O
to	O
hear	O
the	O
big	O
ideas	O
and	O
see	O
them	O
instantiated	O
through	O
the	O
essential	O
mathematics	O
that	O
defines	O
statistical	O
analysis	B
it	O
makes	O
a	O
great	O
supplement	O
to	O
the	O
traditional	O
curricula	O
for	O
beginning	O
graduate	O
students	O
rob	O
kass	O
carnegie	O
mellon	O
university	O
this	O
is	O
a	O
terrific	O
book	O
it	O
gives	O
a	O
clear	O
accessible	O
and	O
entertaining	O
account	O
of	O
the	O
interplay	O
between	O
theory	B
and	O
methodological	O
development	O
that	O
has	O
driven	O
statistics	B
in	O
the	O
computer	O
age	O
the	O
authors	O
succeed	O
brilliantly	O
in	O
locating	O
contemporary	O
algorithmic	O
methodologies	O
for	O
analysis	B
of	O
big	O
data	B
within	O
the	O
framework	O
of	O
established	O
statistical	O
theory	B
alastair	O
young	O
imperial	O
college	O
london	O
this	O
is	O
a	O
guided	O
tour	O
of	O
modern	O
statistics	B
that	O
emphasizes	O
the	O
conceptual	O
and	O
computational	O
advances	O
of	O
the	O
last	O
century	O
authored	O
by	O
two	O
masters	O
of	O
the	O
field	O
it	O
offers	O
just	O
the	O
right	O
mix	O
of	O
mathematical	O
analysis	B
and	O
insightful	O
commentary	O
hal	O
varian	O
google	O
efron	O
and	O
hastie	O
guide	O
us	O
through	O
the	O
maze	O
of	O
breakthrough	O
statistical	O
methodologies	O
following	O
the	O
computing	O
evolution	O
why	O
they	O
were	O
developed	O
their	O
properties	O
and	O
how	O
they	O
are	O
used	O
highlighting	O
their	O
origins	O
the	O
book	O
helps	O
us	O
understand	O
each	O
method	B
s	O
roles	O
in	O
inference	B
andor	O
prediction	O
galit	O
shmueli	O
national	O
tsing	O
hua	O
university	O
a	O
masterful	O
guide	O
to	O
how	O
the	O
inferential	O
bases	O
of	O
classical	O
statistics	B
can	O
provide	O
a	O
principled	O
disciplinary	O
frame	O
for	O
the	O
data	B
science	I
of	O
the	O
twenty-first	O
century	O
stephen	O
stigler	O
university	O
of	O
chicago	O
author	O
of	O
seven	O
pillars	O
of	O
statistical	O
wisdom	O
a	O
refreshing	O
view	O
of	O
modern	O
statistics	B
algorithmics	O
are	O
put	O
on	O
equal	O
footing	O
with	O
intuition	O
properties	O
and	O
the	O
abstract	O
arguments	O
behind	O
them	O
the	O
methods	O
covered	O
are	O
indispensable	O
to	O
practicing	O
statistical	O
analysts	O
in	O
today	O
s	O
big	O
data	B
and	O
big	O
computing	O
landscape	O
robert	O
gramacy	O
the	O
university	O
of	O
chicago	O
booth	O
school	O
of	O
businessbradley	O
efron	O
is	O
max	O
h	O
stein	O
professor	O
professor	O
of	O
statistics	B
and	O
professor	O
of	O
biomedical	O
data	B
science	I
at	O
stanford	O
university	O
he	O
has	O
held	O
visiting	O
faculty	O
appointments	O
at	O
harvard	O
uc	O
berkeley	O
and	O
imperial	O
college	O
london	O
efron	O
has	O
worked	O
extensively	O
on	O
theories	O
of	O
statistical	O
inference	B
and	O
is	O
the	O
inventor	O
of	O
the	O
bootstrap	B
sampling	O
technique	O
he	O
received	O
the	O
national	O
medal	O
of	O
science	O
in	O
and	O
the	O
guy	O
medal	O
in	O
gold	O
of	O
the	O
royal	B
statistical	I
society	I
in	O
trevor	O
hastie	O
is	O
john	O
a	O
overdeck	O
professor	O
professor	O
of	O
statistics	B
and	O
professor	O
of	O
biomedical	O
data	B
science	I
at	O
stanford	O
university	O
he	O
is	O
coauthor	O
of	O
elements	O
of	O
statistical	O
learning	O
a	O
key	O
text	O
in	O
the	O
field	O
of	O
modern	O
data	B
analysis	B
he	O
is	O
also	O
known	O
for	O
his	O
work	O
on	O
generalized	O
additive	O
models	B
and	O
principal	O
curves	O
and	O
for	O
his	O
contributions	O
to	O
the	O
r	B
computing	O
environment	O
hastie	O
was	O
awarded	O
the	O
emmanuel	O
and	O
carol	O
parzen	O
prize	O
for	O
statistical	O
innovation	O
in	O
institute	O
of	O
mathematical	O
statistics	B
monographseditorial	O
boardd	O
r	B
cox	O
of	O
oxfordb	O
hambly	O
of	O
oxfords	O
holmes	O
universityj	O
wellner	O
of	O
washingtoncover	O
illustration	O
pacific	O
ocean	O
wave	O
north	O
shore	O
oahu	O
hawaii	O
brian	O
sytnyk	O
getty	O
images	O
cover	O
designed	O
by	O
zoe	O
naylor	O
printed	O
in	O
the	O
united	O
kingdomcomputer	O
age	O
statistical	O
inferencealgorithms	O
evidence	O
and	O
data	B
sciencebradley	O
efron	O
trevor	O
efron	O
hastie	O
jkt	O
c	O
m	O
y	O
k	O
computer	O
age	O
statistical	O
inference	B
algorithms	O
evidence	O
and	O
data	B
science	I
bradley	O
efron	O
trevor	O
hastie	O
stanford	O
university	O
to	O
donna	O
and	O
lynda	O
viii	O
contents	O
preface	O
acknowledgments	O
notation	O
part	O
i	O
classic	B
statistical	I
inference	B
algorithms	O
and	O
inference	B
a	O
regression	B
example	O
hypothesis	B
testing	B
notes	O
frequentist	B
inference	B
frequentism	B
in	O
practice	O
frequentist	B
optimality	B
notes	O
and	O
details	O
bayesian	B
inference	B
two	O
examples	O
uninformative	B
prior	B
distributions	O
flaws	O
in	O
frequentist	B
inference	B
a	O
bayesianfrequentist	O
comparison	O
list	O
notes	O
and	O
details	O
fisherian	B
inference	B
and	O
maximum	B
likelihood	B
estimation	B
likelihood	B
and	O
maximum	B
likelihood	B
fisher	B
information	B
and	O
the	O
mle	B
conditional	B
inference	B
permutation	O
and	O
randomization	B
notes	O
and	O
details	O
parametric	B
models	B
and	O
exponential	O
families	O
ix	O
xv	O
xviii	O
xix	O
x	O
contents	O
univariate	O
families	O
the	O
multivariate	B
normal	B
distribution	B
fisher	B
s	O
information	B
bound	B
for	O
multiparameter	O
families	O
the	O
multinomial	O
distribution	B
exponential	O
families	O
notes	O
and	O
details	O
part	O
ii	O
early	B
computer-age	I
methods	O
empirical	B
bayes	I
robbins	O
formula	B
the	O
missing-species	B
problem	I
a	O
medical	O
example	O
indirect	B
evidence	I
notes	O
and	O
details	O
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
the	O
james	O
stein	O
estimator	B
the	O
baseball	B
players	O
ridge	B
regression	B
indirect	B
evidence	I
notes	O
and	O
details	O
generalized	O
linear	B
models	B
and	O
regression	B
trees	B
logistic	B
regression	B
generalized	O
linear	B
models	B
poisson	B
regression	B
regression	B
trees	B
notes	O
and	O
details	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
life	O
tables	O
and	O
hazard	O
rates	O
censored	O
data	B
and	O
the	O
kaplan	O
meier	O
estimate	B
the	O
log-rank	B
test	I
the	O
proportional	B
hazards	I
model	I
missing	B
data	B
and	O
the	O
em	B
algorithm	B
notes	O
and	O
details	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
the	O
jackknife	B
estimate	B
of	I
standard	B
error	I
the	O
nonparametric	B
bootstrap	B
resampling	B
plans	B
contents	O
the	O
parametric	B
bootstrap	B
notes	O
and	O
details	O
influence	O
functions	O
and	O
robust	B
estimation	B
bootstrap	B
confidence	O
intervals	B
neyman	O
s	O
construction	O
for	O
one-parameter	B
problems	O
the	O
percentile	B
method	B
bias-corrected	B
confidence	O
intervals	B
second-order	B
accuracy	B
bootstrap-t	O
intervals	B
objective	B
bayes	I
intervals	B
and	O
the	O
confidence	O
distribution	B
notes	O
and	O
details	O
cross-validation	B
and	O
cp	B
estimates	O
of	O
prediction	O
error	O
prediction	O
rules	O
cross-validation	B
covariance	O
penalties	O
training	O
validation	O
and	O
ephemeral	B
predictors	B
notes	O
and	O
details	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
objective	O
prior	B
distributions	O
conjugate	B
prior	B
distributions	O
model	B
selection	I
and	O
the	O
bayesian	B
information	B
criterion	I
gibbs	B
sampling	I
and	O
mcmc	B
example	O
modeling	O
population	O
admixture	O
notes	O
and	O
details	O
postwar	O
statistical	O
inference	B
and	O
methodology	O
part	O
iii	O
twenty-first-century	O
topics	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
large-scale	B
testing	B
false-discovery	O
rates	O
empirical	B
bayes	I
large-scale	B
testing	B
local	O
false-discovery	O
rates	O
choice	O
of	O
the	O
null	O
distribution	B
relevance	B
notes	O
and	O
details	O
sparse	O
modeling	O
and	O
the	O
lasso	B
xi	O
xii	O
contents	O
forward	O
stepwise	O
regression	B
the	O
lasso	B
fitting	O
lasso	B
models	B
least-angle	B
regression	B
fitting	O
generalized	O
lasso	B
models	B
post-selection	B
inference	B
for	O
the	O
lasso	B
connections	O
and	O
extensions	O
notes	O
and	O
details	O
random	O
forests	O
and	O
boosting	B
random	O
forests	O
boosting	B
with	O
squared-error	O
loss	O
gradient	B
boosting	B
adaboost	O
the	O
original	O
boosting	B
algorithm	B
connections	O
and	O
extensions	O
notes	O
and	O
details	O
neural	O
networks	O
and	O
deep	B
learning	I
neural	O
networks	O
and	O
the	O
handwritten	O
digit	O
problem	O
fitting	O
a	O
neural	B
network	I
autoencoders	O
deep	B
learning	I
learning	O
a	O
deep	O
network	O
notes	O
and	O
details	O
support-vector	O
machines	O
and	O
kernel	O
methods	O
optimal	B
separating	B
hyperplane	I
soft-margin	O
classifier	O
svm	B
criterion	O
as	O
loss	B
plus	I
penalty	B
computations	B
and	O
the	O
kernel	O
trick	B
function	B
fitting	O
using	O
kernels	O
example	O
string	O
kernels	O
for	O
protein	B
classification	I
svms	O
concluding	O
remarks	O
kernel	B
smoothing	B
and	O
local	B
regression	B
notes	O
and	O
details	O
inference	B
after	B
model	B
selection	I
simultaneous	O
confidence	O
intervals	B
accuracy	B
after	B
model	B
selection	I
selection	B
bias	B
combined	O
bayes	O
frequentist	B
estimation	B
notes	O
and	O
details	O
contents	O
empirical	B
bayes	I
estimation	B
strategies	I
bayes	O
deconvolution	B
g-modeling	B
and	O
estimation	B
likelihood	B
regularization	B
and	O
accuracy	B
two	O
examples	O
generalized	O
linear	B
mixed	O
models	B
deconvolution	B
and	O
f	O
notes	O
and	O
details	O
epilogue	O
references	O
author	O
index	O
subject	O
index	O
xiii	O
xiv	O
preface	O
statistical	O
inference	B
is	O
an	O
unusually	O
wide-ranging	O
discipline	O
located	O
as	O
it	O
is	O
at	O
the	O
triple-point	O
of	O
mathematics	O
empirical	B
science	O
and	O
philosophy	O
the	O
discipline	O
can	O
be	O
said	O
to	O
date	O
from	O
with	O
the	O
publication	O
of	O
bayes	O
rule	B
the	O
philosophical	O
side	O
of	O
the	O
subject	O
the	O
rule	B
s	O
early	O
advocates	O
considered	O
it	O
an	O
argument	B
for	O
the	O
existence	O
of	O
god	O
the	O
most	O
recent	O
quarter	O
of	O
this	O
history	O
from	O
the	O
to	O
the	O
present	O
is	O
the	O
computer	O
age	O
of	O
our	O
book	O
s	O
title	O
the	O
time	O
when	O
computation	O
the	O
traditional	O
bottleneck	O
of	O
statistical	O
applications	O
became	O
faster	O
and	O
easier	O
by	O
a	O
factor	B
of	O
a	O
million	O
the	O
book	O
is	O
an	O
examination	O
of	O
how	O
statistics	B
has	O
evolved	O
over	O
the	O
past	O
sixty	O
years	O
an	O
aerial	O
view	O
of	O
a	O
vast	O
subject	O
but	O
seen	O
from	O
the	O
height	O
of	O
a	O
small	O
plane	O
not	O
a	O
jetliner	O
or	O
satellite	O
the	O
individual	O
chapters	O
take	O
up	O
a	O
series	O
of	O
influential	O
topics	O
generalized	O
linear	B
models	B
survival	B
analysis	B
the	O
jackknife	B
and	O
bootstrap	B
false-discovery	O
rates	O
empirical	B
bayes	I
mcmc	B
neural	O
nets	O
and	O
a	O
dozen	O
more	O
describing	O
for	O
each	O
the	O
key	O
methodological	O
developments	O
and	O
their	O
inferential	O
justification	O
needless	O
to	O
say	O
the	O
role	O
of	O
electronic	O
computation	O
is	O
central	O
to	O
our	O
story	O
this	O
doesn	O
t	B
mean	O
that	O
every	O
advance	O
was	O
computer-related	O
a	O
land	O
bridge	O
had	O
opened	O
to	O
a	O
new	O
continent	O
but	O
not	O
all	O
were	O
eager	O
to	O
cross	O
topics	O
such	O
as	O
empirical	B
bayes	I
and	O
james	O
stein	O
estimation	B
could	O
have	O
emerged	O
just	O
as	O
well	O
under	O
the	O
constraints	O
of	O
mechanical	O
computation	O
others	O
like	O
the	O
bootstrap	B
and	O
proportional	O
hazards	O
were	O
pureborn	O
children	O
of	O
the	O
computer	O
age	O
almost	O
all	O
topics	O
in	O
twenty-first-century	O
statistics	B
are	O
now	O
computer-dependent	O
but	O
it	O
will	O
take	O
our	O
small	O
plane	O
a	O
while	O
to	O
reach	O
the	O
new	O
millennium	O
dictionary	O
definitions	O
of	O
statistical	O
inference	B
tend	O
to	O
equate	O
it	O
with	O
the	O
entire	O
discipline	O
this	O
has	O
become	O
less	O
satisfactory	O
in	O
the	O
big	O
data	B
era	O
of	O
immense	O
computer-based	O
processing	O
algorithms	O
here	O
we	O
will	O
attempt	O
not	O
always	O
consistently	O
to	O
separate	O
the	O
two	O
aspects	O
of	O
the	O
statistical	O
enterprise	O
algorithmic	O
developments	O
aimed	O
at	O
specific	O
problem	O
areas	O
for	O
instance	O
xv	O
xvi	O
preface	O
random	O
forests	O
for	O
prediction	O
as	O
distinct	O
from	O
the	O
inferential	O
arguments	O
offered	O
in	O
their	O
support	O
very	O
broadly	O
speaking	O
algorithms	O
are	O
what	O
statisticians	O
do	O
while	O
inference	B
says	O
why	O
they	O
do	O
them	O
a	O
particularly	O
energetic	O
brand	O
of	O
the	O
statistical	O
enterprise	O
has	O
flourished	O
in	O
the	O
new	O
century	O
data	B
science	I
emphasizing	O
algorithmic	O
thinking	O
rather	O
than	O
its	O
inferential	O
justification	O
the	O
later	O
chapters	O
of	O
our	O
book	O
where	O
large-scale	B
prediction	I
algorithms	I
such	O
as	O
boosting	B
and	O
deep	B
learning	I
are	O
examined	O
illustrate	O
the	O
data-science	O
point	O
of	O
view	O
the	O
epilogue	O
for	O
a	O
little	O
more	O
on	O
the	O
sometimes	O
fraught	O
statisticsdata	O
science	O
marriage	O
there	O
are	O
no	O
such	O
subjects	O
as	O
biological	O
inference	B
or	O
astronomical	O
inference	B
or	O
geological	O
inference	B
why	O
do	O
we	O
need	O
statistical	O
inference	B
the	O
answer	O
is	O
simple	O
the	O
natural	O
sciences	O
have	O
nature	O
to	O
judge	O
the	O
accuracy	B
of	O
their	O
ideas	O
statistics	B
operates	O
one	O
step	O
back	O
from	O
nature	O
most	O
often	O
interpreting	O
the	O
observations	O
of	O
natural	O
scientists	O
without	O
nature	O
to	O
serve	O
as	O
a	O
disinterested	O
referee	O
we	O
need	O
a	O
system	O
of	O
mathematical	O
logic	O
for	O
guidance	O
and	O
correction	O
statistical	O
inference	B
is	O
that	O
system	O
distilled	O
from	O
two	O
and	O
a	O
half	O
centuries	O
of	O
data-analytic	O
experience	O
the	O
book	O
proceeds	O
historically	O
in	O
three	O
parts	O
the	O
great	O
themes	O
of	O
classical	O
inference	B
bayesian	B
frequentist	B
and	O
fisherian	O
reviewed	O
in	O
part	O
i	O
were	O
set	B
in	O
place	O
before	O
the	O
age	O
of	O
electronic	O
computation	O
modern	O
practice	O
has	O
vastly	O
extended	O
their	O
reach	O
without	O
changing	O
the	O
basic	O
outlines	O
analogy	O
with	O
classical	O
and	O
modern	O
literature	O
might	O
be	O
made	O
part	O
ii	O
concerns	O
early	B
computer-age	I
developments	O
from	O
the	O
through	O
the	O
as	O
a	O
transitional	O
period	O
this	O
is	O
the	O
time	O
when	O
it	O
is	O
easiest	O
to	O
see	O
the	O
effects	O
or	O
noneffects	O
of	O
fast	O
computation	O
on	O
the	O
progress	O
of	O
statistical	O
methodology	O
both	O
in	O
its	O
theory	B
and	O
practice	O
part	O
iii	O
twenty-first-century	O
topics	O
brings	O
the	O
story	O
up	O
to	O
the	O
present	O
ours	O
is	O
a	O
time	O
of	O
enormously	O
ambitious	O
algorithms	O
machine	B
learning	I
being	O
the	O
somewhat	O
disquieting	O
catchphrase	O
their	O
justification	O
is	O
the	O
ongoing	O
task	O
of	O
modern	O
statistical	O
inference	B
neither	O
a	O
catalog	O
nor	O
an	O
encyclopedia	O
the	O
book	O
s	O
topics	O
were	O
chosen	O
as	O
apt	O
illustrations	O
of	O
the	O
interplay	O
between	O
computational	O
methodology	O
and	O
inferential	O
theory	B
some	O
missing	O
topics	O
that	O
might	O
have	O
served	O
just	O
as	O
well	O
include	O
time	O
series	O
general	O
estimating	O
equations	O
causal	O
inference	B
graphical	O
models	B
and	O
experimental	O
design	O
in	O
any	O
case	O
there	O
is	O
no	O
implication	O
that	O
the	O
topics	O
presented	O
here	O
are	O
the	O
only	O
ones	O
worthy	O
of	O
discussion	O
also	O
underrepresented	O
are	O
asymptotics	B
and	O
decision	O
theory	B
the	O
math	O
stat	O
side	O
of	O
the	O
field	O
our	O
intention	O
was	O
to	O
maintain	O
a	O
technical	O
level	O
of	O
discussion	O
appropriate	O
to	O
masters	O
statisticians	O
or	O
first-year	O
phd	O
stu	O
preface	O
xvii	O
dents	O
inevitably	O
some	O
of	O
the	O
presentation	O
drifts	O
into	O
more	O
difficult	O
waters	O
more	O
from	O
the	O
nature	O
of	O
the	O
statistical	O
ideas	O
than	O
the	O
mathematics	O
readers	O
who	O
find	O
our	O
aerial	O
view	O
circling	O
too	O
long	O
over	O
some	O
topic	O
shouldn	O
t	B
hesitate	O
to	O
move	O
ahead	O
in	O
the	O
book	O
for	O
the	O
most	O
part	O
the	O
chapters	O
can	O
be	O
read	O
independently	O
of	O
each	O
other	O
there	O
is	O
a	O
connecting	O
overall	O
theme	O
this	O
comment	O
applies	O
especially	O
to	O
nonstatisticians	O
who	O
have	O
picked	O
up	O
the	O
book	O
because	O
of	O
interest	O
in	O
some	O
particular	O
topic	O
say	O
survival	B
analysis	B
or	O
boosting	B
useful	O
disciplines	O
that	O
serve	O
a	O
wide	O
variety	O
of	O
demanding	O
clients	O
run	O
the	O
risk	O
of	O
losing	O
their	O
center	O
statistics	B
has	O
managed	O
for	O
the	O
most	O
part	O
to	O
maintain	O
its	O
philosophical	O
cohesion	O
despite	O
a	O
rising	O
curve	O
of	O
outside	O
demand	O
the	O
center	O
of	O
the	O
field	O
has	O
in	O
fact	O
moved	O
in	O
the	O
past	O
sixty	O
years	O
from	O
its	O
traditional	O
home	O
in	O
mathematics	O
and	O
logic	O
toward	O
a	O
more	O
computational	O
focus	O
our	O
book	O
traces	O
that	O
movement	O
on	O
a	O
topic-by-topic	O
basis	O
an	O
answer	O
to	O
the	O
intriguing	O
question	O
what	O
happens	O
next	O
won	O
t	B
be	O
attempted	O
here	O
except	O
for	O
a	O
few	O
words	O
in	O
the	O
epilogue	O
where	O
the	O
rise	O
of	O
data	B
science	I
is	O
discussed	O
acknowledgments	O
we	O
are	O
indebted	O
to	O
cindy	O
kirby	O
for	O
her	O
skillful	O
work	O
in	O
the	O
preparation	O
of	O
this	O
book	O
and	O
galit	O
shmueli	O
for	O
her	O
helpful	O
comments	O
on	O
an	O
earlier	O
draft	O
at	O
cambridge	O
university	O
press	O
a	O
huge	O
thank	O
you	O
to	O
steven	O
holt	O
for	O
his	O
excellent	O
copy	O
editing	O
clare	O
dennison	O
for	O
guiding	O
us	O
through	O
the	O
production	O
phase	O
and	O
to	O
diana	O
gillooly	O
our	O
editor	O
for	O
her	O
unfailing	O
support	O
bradley	O
efron	O
trevor	O
hastie	O
department	O
of	O
statistics	B
stanford	O
university	O
may	O
xviii	O
notation	O
xix	O
throughout	O
the	O
book	O
the	O
numbered	O
sign	O
indicates	O
a	O
technical	O
note	O
or	O
reference	O
element	O
which	O
is	O
elaborated	O
on	O
at	O
the	O
end	O
of	O
the	O
chapter	O
there	O
next	O
to	O
the	O
number	O
the	O
page	O
number	O
of	O
the	O
referenced	O
location	O
is	O
given	O
in	O
parenthesis	O
for	O
example	O
lowess	B
in	O
the	O
notes	O
on	O
page	O
was	O
referenced	O
via	O
a	O
on	O
page	O
matrices	O
such	O
as	O
are	O
represented	O
in	O
bold	O
font	O
as	O
are	O
certain	O
vectors	O
such	O
as	O
y	O
a	O
data	B
vector	B
with	O
n	O
elements	O
most	O
other	O
vectors	O
such	O
as	O
coefficient	O
vectors	O
are	O
typically	O
not	O
bold	O
we	O
use	O
a	O
dark	O
green	O
typewriter	O
font	O
to	O
indicate	O
data	B
set	B
names	O
such	O
as	O
prostate	B
variable	O
names	O
such	O
as	O
prog	O
from	O
data	B
sets	O
and	O
r	B
commands	O
such	O
as	O
glmnet	B
or	O
locfdr	B
no	O
bibliographic	O
references	O
are	O
given	O
in	O
the	O
body	O
of	O
the	O
text	O
important	O
references	O
are	O
given	O
in	O
the	O
endnotes	O
of	O
each	O
chapter	O
part	O
i	O
classic	B
statistical	I
inference	B
algorithms	O
and	O
inference	B
statistics	B
is	O
the	O
science	O
of	O
learning	O
from	O
experience	O
particularly	O
experience	O
that	O
arrives	O
a	O
little	O
bit	O
at	O
a	O
time	O
the	O
successes	O
and	O
failures	O
of	O
a	O
new	O
experimental	O
drug	O
the	O
uncertain	O
measurements	O
of	O
an	O
asteroid	O
s	O
path	B
toward	O
earth	O
it	O
may	O
seem	O
surprising	O
that	O
any	O
one	O
theory	B
can	O
cover	O
such	O
an	O
amorphous	O
target	O
as	O
learning	O
from	O
experience	O
in	O
fact	O
there	O
are	O
two	O
main	O
statistical	O
theories	O
bayesianism	B
and	O
frequentism	B
whose	O
connections	O
and	O
disagreements	O
animate	O
many	O
of	O
the	O
succeeding	O
chapters	O
first	O
however	O
we	O
want	O
to	O
discuss	O
a	O
less	O
philosophical	O
more	O
operational	O
division	O
of	O
labor	O
that	O
applies	O
to	O
both	O
theories	O
between	O
the	O
algorithmic	O
and	O
inferential	O
aspects	O
of	O
statistical	O
analysis	B
the	O
distinction	O
begins	O
with	O
the	O
most	O
basic	O
and	O
most	O
popular	O
statistical	O
method	B
averaging	B
suppose	O
we	O
have	O
observed	O
numbers	O
xn	O
applying	O
to	O
some	O
phenomenon	O
of	O
interest	O
perhaps	O
the	O
automobile	O
accident	O
rates	O
in	O
the	O
n	O
d	O
states	O
the	O
mean	O
xi	O
nx	O
d	O
nx	O
summarizes	O
the	O
results	O
in	O
a	O
single	O
number	O
how	O
accurate	O
is	O
that	O
number	O
the	O
textbook	O
answer	O
is	O
given	O
in	O
terms	O
of	O
the	O
standard	O
errorbse	O
d	O
nx	O
here	O
averaging	B
is	O
the	O
algorithm	B
while	O
the	O
standard	B
error	I
provides	O
an	O
inference	B
of	O
the	O
algorithm	B
s	O
accuracy	B
it	O
is	O
a	O
surprising	O
and	O
crucial	O
aspect	O
of	O
statistical	O
theory	B
that	O
the	O
same	O
data	B
that	O
supplies	O
an	O
estimate	B
can	O
also	O
assess	O
its	O
inference	B
concerns	O
more	O
than	O
accuracy	B
speaking	O
broadly	O
algorithms	O
say	O
what	O
the	O
statistician	O
does	O
while	O
inference	B
says	O
why	O
he	O
or	O
she	O
does	O
it	O
of	O
coursebse	O
is	O
itself	O
an	O
algorithm	B
which	O
could	O
be	O
is	O
subject	O
algorithms	O
and	O
inference	B
to	O
further	O
inferential	O
analysis	B
concerning	O
its	O
accuracy	B
the	O
point	O
is	O
that	O
the	O
algorithm	B
comes	O
first	O
and	O
the	O
inference	B
follows	O
at	O
a	O
second	O
level	O
of	O
statistical	O
consideration	O
in	O
practice	O
this	O
means	O
that	O
algorithmic	O
invention	O
is	O
a	O
more	O
free-wheeling	O
and	O
adventurous	O
enterprise	O
with	O
inference	B
playing	O
catch-up	O
as	O
it	O
strives	O
to	O
assess	O
the	O
accuracy	B
good	O
or	O
bad	O
of	O
some	O
hot	O
new	O
algorithmic	O
methodology	O
if	O
the	O
inferencealgorithm	O
race	O
is	O
a	O
tortoise-and-hare	O
affair	O
then	O
modern	O
electronic	O
computation	O
has	O
bred	O
a	O
bionic	O
hare	O
there	O
are	O
two	O
effects	O
at	O
work	O
here	O
computer-based	O
technology	O
allows	O
scientists	O
to	O
collect	O
enormous	O
data	B
sets	O
orders	O
of	O
magnitude	O
larger	O
than	O
those	O
that	O
classic	O
statistical	O
theory	B
was	O
designed	O
to	O
deal	O
with	O
huge	O
data	B
demands	O
new	O
methodology	O
and	O
the	O
demand	O
is	O
being	O
met	O
by	O
a	O
burst	O
of	O
innovative	O
computer-based	O
statistical	O
algorithms	O
when	O
one	O
reads	O
of	O
big	O
data	B
in	O
the	O
news	O
it	O
is	O
usually	O
these	O
algorithms	O
playing	O
the	O
starring	O
roles	O
our	O
book	O
s	O
title	O
computer	O
age	O
statistical	O
inference	B
emphasizes	O
the	O
tortoise	O
s	O
side	O
of	O
the	O
story	O
the	O
past	O
few	O
decades	O
have	O
been	O
a	O
golden	O
age	O
of	O
statistical	O
methodology	O
it	O
hasn	O
t	B
been	O
quite	O
a	O
golden	O
age	O
for	O
statistical	O
inference	B
but	O
it	O
has	O
not	O
been	O
a	O
dark	O
age	O
either	O
the	O
efflorescence	O
of	O
ambitious	O
new	O
algorithms	O
has	O
forced	O
an	O
evolution	O
not	O
a	O
revolution	O
in	O
inference	B
the	O
theories	O
by	O
which	O
statisticians	O
choose	O
among	O
competing	O
methods	O
the	O
book	O
traces	O
the	O
interplay	O
between	O
methodology	O
and	O
inference	B
as	O
it	O
has	O
developed	O
since	O
the	O
the	O
beginning	O
of	O
our	O
discipline	O
s	O
computer	O
age	O
as	O
a	O
preview	O
we	O
end	O
this	O
chapter	O
with	O
two	O
examples	O
illustrating	O
the	O
transition	O
from	O
classic	O
to	O
computer-age	O
practice	O
a	O
regression	B
example	O
figure	O
concerns	O
a	O
study	O
of	O
kidney	B
function	B
data	B
points	O
yi	O
have	O
been	O
observed	O
for	O
n	O
d	O
healthy	O
volunteers	O
with	O
xi	O
the	O
ith	O
volunteer	O
s	O
age	O
in	O
years	O
and	O
yi	O
a	O
composite	O
measure	O
tot	O
of	O
overall	O
function	B
kidney	B
function	B
generally	O
declines	O
with	O
age	O
as	O
evident	O
in	O
the	O
downward	O
scatter	O
of	O
the	O
points	O
the	O
rate	B
of	O
decline	O
is	O
an	O
important	O
question	O
in	O
kidney	O
transplantation	O
in	O
the	O
past	O
potential	O
donors	O
past	O
age	O
were	O
prohibited	O
though	O
given	O
a	O
shortage	O
of	O
donors	O
this	O
is	O
no	O
longer	O
enforced	O
the	O
solid	O
line	O
in	O
figure	O
is	O
a	O
linear	B
regression	B
y	O
d	O
o	O
c	O
o	O
fit	O
to	O
the	O
data	B
by	O
least	B
squares	I
that	O
is	O
by	O
minimizing	O
the	O
sum	O
of	O
squared	O
a	O
regression	B
example	O
figure	O
kidney	O
fitness	O
tot	O
vs	O
age	O
for	O
volunteers	O
the	O
line	O
is	O
a	O
linear	B
regression	B
fit	O
showing	O
standard	O
errors	B
at	O
selected	O
values	O
of	O
age	O
deviations	O
nx	O
over	O
all	O
choices	O
of	O
the	O
least	B
squares	I
algorithm	B
which	O
dates	O
back	O
to	O
gauss	O
and	O
legendre	O
in	O
the	O
early	O
gives	O
o	O
d	O
as	O
the	O
least	B
squares	I
estimates	O
we	O
can	O
read	O
off	O
of	O
the	O
fitted	O
line	O
an	O
estimated	O
value	O
of	O
kidney	O
fitness	O
for	O
any	O
chosen	O
age	O
the	O
top	O
line	O
of	O
table	O
shows	O
estimate	B
at	O
age	O
down	O
to	O
at	O
age	O
d	O
and	O
o	O
how	O
accurate	O
are	O
these	O
estimates	O
this	O
is	O
where	O
inference	B
comes	O
in	O
an	O
extended	O
version	O
of	O
formula	B
also	O
going	O
back	O
to	O
the	O
provides	O
the	O
standard	O
errors	B
shown	O
in	O
line	O
of	O
the	O
table	O
the	O
vertical	O
bars	O
in	O
figure	O
are	O
two	O
standard	O
errors	B
giving	O
them	O
about	O
chance	O
of	O
containing	O
the	O
true	O
expected	O
value	O
of	O
tot	O
at	O
each	O
age	O
that	O
coverage	B
depends	O
on	O
the	O
validity	O
of	O
the	O
linear	B
regression	B
model	I
we	O
might	O
instead	O
try	O
a	O
quadratic	O
regression	B
y	O
d	O
o	O
or	O
a	O
cubic	O
etc	O
all	O
of	O
this	O
being	O
well	O
within	O
the	O
reach	O
of	O
pre-computer	O
statistical	O
theory	B
c	O
o	O
c	O
o	O
algorithms	O
and	O
inference	B
table	O
regression	B
analysis	B
of	O
the	O
kidney	O
data	B
linear	B
regression	B
estimates	O
their	O
standard	O
errors	B
lowess	B
estimates	O
their	O
bootstrap	B
standard	O
errors	B
age	O
linear	B
regression	B
std	O
error	O
lowess	B
bootstrap	B
std	O
error	O
figure	O
local	O
polynomial	O
fit	O
to	O
the	O
kidney-fitness	O
data	B
with	O
bootstrap	B
standard	B
deviations	I
a	O
modern	O
computer-based	O
algorithm	B
lowess	B
produced	O
the	O
somewhat	O
bumpy	O
regression	B
curve	O
in	O
figure	O
the	O
lowess	B
algorithm	B
moves	O
its	O
attention	O
along	O
the	O
x-axis	O
fitting	O
local	O
polynomial	O
curves	O
of	O
differing	O
degrees	O
to	O
nearby	O
y	O
points	O
in	O
the	O
here	O
and	O
throughout	O
the	O
book	O
the	O
numbered	O
sign	O
indicates	O
a	O
technical	O
note	O
or	O
reference	O
element	O
which	O
is	O
elaborated	O
on	O
at	O
the	O
end	O
of	O
the	O
chapter	O
here	O
and	O
in	O
all	O
our	O
examples	O
we	O
are	O
employing	O
the	O
language	O
r	B
itself	O
one	O
of	O
the	O
key	O
developments	O
in	O
computer-based	O
statistical	O
methodology	O
a	O
regression	B
example	O
determines	O
the	O
definition	O
of	O
local	O
repeated	O
passes	O
over	O
the	O
x-axis	O
refine	O
the	O
fit	O
reducing	O
the	O
effects	O
of	O
occasional	O
anomalous	O
points	O
the	O
fitted	O
curve	O
in	O
figure	O
is	O
nearly	O
linear	B
at	O
the	O
right	O
but	O
more	O
complicated	O
at	O
the	O
left	O
where	O
points	O
are	O
more	O
densely	O
packed	O
it	O
is	O
flat	O
between	O
ages	O
and	O
a	O
potentially	O
important	O
difference	O
from	O
the	O
uniform	O
decline	O
portrayed	O
in	O
figure	O
there	O
is	O
no	O
formula	B
such	O
as	O
to	O
infer	O
the	O
accuracy	B
of	O
the	O
lowess	B
curve	O
instead	O
a	O
computer-intensive	B
inferential	O
engine	O
the	O
bootstrap	B
was	O
used	O
to	O
calculate	O
the	O
error	O
bars	O
in	O
figure	O
a	O
bootstrap	B
data	B
set	B
is	O
produced	O
by	O
resampling	B
pairs	O
yi	O
from	O
the	O
original	O
with	O
replacement	O
so	O
perhaps	O
might	O
show	O
up	O
twice	O
in	O
the	O
bootstrap	B
sample	B
might	O
be	O
missing	O
present	O
once	O
etc	O
applying	O
lowess	B
to	O
the	O
bootstrap	B
sample	B
generates	O
a	O
bootstrap	B
replication	B
of	O
the	O
original	O
calculation	O
figure	O
bootstrap	B
replications	O
of	O
figure	O
shows	O
the	O
first	O
bootstrap	B
lowess	B
replications	O
bouncing	O
around	O
the	O
original	O
curve	O
from	O
figure	O
the	O
variability	O
of	O
the	O
replications	O
at	O
any	O
one	O
age	O
the	O
bootstrap	B
standard	B
deviation	I
determined	O
the	O
original	O
curve	O
s	O
accuracy	B
how	O
and	O
why	O
the	O
bootstrap	B
works	O
is	O
discussed	O
in	O
chapter	O
it	O
has	O
the	O
great	O
virtue	O
of	O
assessing	O
estimation	B
accu	O
algorithms	O
and	O
inference	B
racy	O
for	O
any	O
algorithm	B
no	O
matter	O
how	O
complicated	O
the	O
price	O
is	O
a	O
hundredor	O
thousand-fold	O
increase	O
in	O
computation	O
unthinkable	O
in	O
but	O
routine	O
now	O
the	O
bottom	O
two	O
lines	O
of	O
table	O
show	O
the	O
lowess	B
estimates	O
and	O
their	O
standard	O
errors	B
we	O
have	O
paid	O
a	O
price	O
for	O
the	O
increased	O
flexibility	O
of	O
lowess	B
its	O
standard	O
errors	B
roughly	O
doubling	O
those	O
for	O
linear	B
regression	B
hypothesis	B
testing	B
our	O
second	O
example	O
concerns	O
the	O
march	O
of	O
methodology	O
and	O
inference	B
for	O
hypothesis	B
testing	B
rather	O
than	O
estimation	B
leukemia	B
patients	O
with	O
all	O
lymphoblastic	O
leukemia	B
and	O
with	O
aml	O
myeloid	O
leukemia	B
a	O
worse	O
prognosis	O
have	O
each	O
had	O
genetic	O
activity	O
measured	O
for	O
a	O
panel	O
of	O
genes	O
the	O
histograms	O
in	O
figure	O
compare	O
the	O
genetic	O
activities	O
in	O
the	O
two	O
groups	O
for	O
gene	O
figure	O
scores	O
for	O
gene	O
leukemia	B
data	B
top	O
all	O
d	O
bottom	O
aml	O
d	O
a	O
two-sample	B
t-statistic	B
d	O
with	O
p-value	B
d	O
the	O
aml	O
group	O
appears	O
to	O
show	O
greater	O
activity	O
the	O
mean	O
values	O
being	O
all	O
d	O
and	O
aml	O
d	O
all	O
scores	O
mean	O
scores	O
mean	O
hypothesis	B
testing	B
is	O
the	O
perceived	O
difference	O
genuine	O
or	O
perhaps	O
as	O
people	O
like	O
to	O
say	O
a	O
statistical	O
fluke	O
the	O
classic	O
answer	O
to	O
this	O
question	O
is	O
via	O
a	O
two-sample	B
t-statistic	B
t	B
d	O
aml	O
allbsd	O
wherebsd	O
is	O
an	O
estimate	B
of	O
the	O
numerator	O
s	O
standard	O
dividing	O
by	O
bsd	O
allows	O
us	O
gaussian	B
assumptions	O
discussed	O
in	O
chapter	O
to	O
compare	O
the	O
observed	O
value	O
of	O
t	B
with	O
a	O
standard	O
null	O
distribution	B
in	O
this	O
case	O
a	O
student	O
s	O
t	B
distribution	B
with	O
degrees	B
of	I
freedom	I
we	O
obtain	O
t	B
d	O
from	O
which	O
would	O
classically	O
be	O
considered	O
very	O
strong	O
evidence	O
that	O
the	O
apparent	O
difference	O
is	O
genuine	O
in	O
standard	O
terminology	O
with	O
two-sided	O
significance	O
level	O
a	O
small	O
significance	O
level	O
p-value	B
is	O
a	O
statement	O
of	O
statistical	O
surprise	O
something	O
very	O
unusual	O
has	O
happened	O
if	O
in	O
fact	O
there	O
is	O
no	O
difference	O
in	O
gene	O
expression	O
levels	O
between	O
all	O
and	O
aml	O
patients	O
we	O
are	O
less	O
surprised	O
by	O
t	B
d	O
if	O
gene	O
is	O
just	O
one	O
candidate	O
out	O
of	O
thousands	O
that	O
might	O
have	O
produced	O
interesting	O
results	O
that	O
is	O
the	O
case	O
here	O
figure	O
shows	O
the	O
histogram	O
of	O
the	O
two-sample	B
t-statistics	O
for	O
the	O
panel	O
of	O
genes	O
now	O
t	B
d	O
looks	O
less	O
unusual	O
other	O
genes	O
have	O
t	B
exceeding	O
about	O
of	O
them	O
this	O
doesn	O
t	B
mean	O
that	O
gene	O
is	O
significant	O
at	O
the	O
level	O
there	O
are	O
two	O
powerful	O
complicating	O
factors	O
large	O
numbers	O
of	O
candidates	O
here	O
will	O
produce	O
some	O
large	O
tvalues	O
even	O
if	O
there	O
is	O
really	O
no	O
difference	O
in	O
genetic	O
expression	O
between	O
all	O
and	O
aml	O
patients	O
the	O
histogram	O
implies	O
that	O
in	O
this	O
study	O
there	O
is	O
something	O
wrong	O
with	O
the	O
theoretical	B
null	I
distribution	B
student	O
s	O
t	B
with	O
degrees	B
of	I
freedom	I
the	O
smooth	O
curve	O
in	O
figure	O
it	O
is	O
much	O
too	O
narrow	O
at	O
the	O
center	O
where	O
presumably	O
most	O
of	O
the	O
genes	O
are	O
reporting	O
non-significant	O
results	O
we	O
will	O
see	O
in	O
chapter	O
that	O
a	O
low	O
false-discovery	B
rate	B
i	O
e	O
a	O
low	O
chance	O
of	O
crying	O
wolf	O
over	O
an	O
innocuous	O
gene	O
requires	O
t	B
exceeding	O
in	O
the	O
allaml	O
study	O
only	O
of	O
the	O
genes	O
make	O
the	O
cut	O
falsediscovery-rate	O
theory	B
is	O
an	O
impressive	O
advance	O
in	O
statistical	O
inference	B
incorporating	O
bayesian	B
frequentist	B
and	O
empirical	B
bayesian	B
el	O
formally	O
a	O
standard	B
error	I
is	O
the	O
standard	B
deviation	I
of	O
a	O
summary	O
statistic	B
andbsd	O
might	O
better	O
be	O
calledbse	O
but	O
we	O
will	O
follow	O
the	O
distinction	O
less	O
than	O
punctiliously	O
here	O
algorithms	O
and	O
inference	B
figure	O
two-sample	B
t-statistics	O
for	O
genes	O
leukemia	B
data	B
the	O
smooth	O
curve	O
is	O
the	O
theoretical	B
null	I
density	B
for	O
the	O
t-statistic	B
ements	O
it	O
was	O
a	O
necessary	O
advance	O
in	O
a	O
scientific	O
world	O
where	O
computerbased	O
technology	O
routinely	O
presents	O
thousands	O
of	O
comparisons	O
to	O
be	O
evaluated	O
at	O
once	O
there	O
is	O
one	O
more	O
thing	O
to	O
say	O
about	O
the	O
algorithminference	O
statistical	O
cycle	O
important	O
new	O
algorithms	O
often	O
arise	O
outside	O
the	O
world	O
of	O
professional	O
statisticians	O
neural	O
nets	O
support	O
vector	B
machines	O
and	O
boosting	B
are	O
three	O
famous	O
examples	O
none	O
of	O
this	O
is	O
surprising	O
new	O
sources	O
of	O
data	B
satellite	O
imagery	O
for	O
example	O
or	O
medical	O
microarrays	B
inspire	O
novel	O
methodology	O
from	O
the	O
observing	O
scientists	O
the	O
early	O
literature	O
tends	O
toward	O
the	O
enthusiastic	O
with	O
claims	O
of	O
enormous	O
applicability	O
and	O
power	O
in	O
the	O
second	O
phase	O
statisticians	O
try	O
to	O
locate	O
the	O
new	O
metholodogy	O
within	O
the	O
framework	O
of	O
statistical	O
theory	B
in	O
other	O
words	O
they	O
carry	O
out	O
the	O
statistical	O
inference	B
part	O
of	O
the	O
cycle	O
placing	O
the	O
new	O
methodology	O
within	O
the	O
known	O
bayesian	B
and	O
frequentist	B
limits	O
of	O
performance	O
offers	O
a	O
nice	O
example	O
chapter	O
this	O
is	O
a	O
healthy	O
chain	O
of	O
events	O
good	O
both	O
for	O
the	O
hybrid	O
vigor	O
of	O
the	O
statistics	B
profession	O
and	O
for	O
the	O
further	O
progress	O
of	O
algorithmic	O
technology	O
t	B
statisticsfrequency	O
notes	O
notes	O
legendre	O
published	O
the	O
least	B
squares	I
algorithm	B
in	O
causing	O
gauss	O
to	O
state	O
that	O
he	O
had	O
been	O
using	O
the	O
method	B
in	O
astronomical	O
orbit-fitting	O
since	O
given	O
gauss	O
astonishing	O
production	O
of	O
major	O
mathematical	O
advances	O
this	O
says	O
something	O
about	O
the	O
importance	O
attached	O
to	O
the	O
least	B
squares	I
idea	O
chapter	O
includes	O
its	O
usual	O
algebraic	O
formulation	O
as	O
well	O
as	O
gauss	O
formula	B
for	O
the	O
standard	O
errors	B
line	O
of	O
table	O
our	O
division	O
between	O
algorithms	O
and	O
inference	B
brings	O
to	O
mind	O
tukey	B
s	O
exploratoryconfirmatory	O
system	O
however	O
the	O
current	O
algorithmic	O
world	O
is	O
often	O
bolder	O
in	O
its	O
claims	O
than	O
the	O
word	O
exploratory	O
implies	O
while	O
to	O
our	O
minds	O
inference	B
conveys	O
something	O
richer	O
than	O
mere	O
confirmation	O
lowess	B
was	O
devised	O
by	O
william	O
cleveland	O
and	O
is	O
available	O
in	O
the	O
r	B
statistical	O
computing	O
language	O
it	O
is	O
applied	O
to	O
the	O
kidney	O
data	B
in	O
efron	O
the	O
kidney	O
data	B
originated	O
in	O
the	O
nephrology	O
laboratory	O
of	O
dr	O
brian	O
myers	O
stanford	O
university	O
and	O
is	O
available	O
from	O
this	O
book	O
s	O
web	O
site	O
frequentist	B
inference	B
before	O
the	O
computer	O
age	O
there	O
was	O
the	O
calculator	O
age	O
and	O
before	O
big	O
data	B
there	O
were	O
small	O
data	B
sets	O
often	O
a	O
few	O
hundred	O
numbers	O
or	O
fewer	O
laboriously	O
collected	O
by	O
individual	O
scientists	O
working	O
under	O
restrictive	O
experimental	O
constraints	O
precious	O
data	B
calls	O
for	O
maximally	O
efficient	O
statistical	O
analysis	B
a	O
remarkably	O
effective	O
theory	B
feasible	O
for	O
execution	O
on	O
mechanical	O
desk	O
calculators	O
was	O
developed	O
beginning	O
in	O
by	O
pearson	O
fisher	B
neyman	O
hotelling	O
and	O
others	O
and	O
grew	O
to	O
dominate	O
twentieth-century	O
statistical	O
practice	O
the	O
theory	B
now	O
referred	O
to	O
as	O
classical	O
relied	O
almost	O
entirely	O
on	O
frequentist	B
inferential	O
ideas	O
this	O
chapter	O
sketches	O
a	O
quick	O
and	O
simplified	O
picture	O
of	O
frequentist	B
inference	B
particularly	O
as	O
employed	O
in	O
classical	O
applications	O
we	O
begin	O
with	O
another	O
example	O
from	O
dr	O
myers	O
nephrology	O
laboratory	O
kidney	O
patients	O
have	O
had	O
their	O
glomerular	O
filtration	O
rates	O
measured	O
with	O
the	O
results	O
shown	O
in	O
figure	O
gfr	O
is	O
an	O
important	O
indicator	O
of	O
kidney	B
function	B
with	O
low	O
values	O
suggesting	O
trouble	O
is	O
a	O
key	O
component	O
of	O
andbse	O
d	O
typically	O
reported	O
as	O
tot	O
in	O
figure	O
the	O
mean	O
and	O
standard	B
error	I
are	O
nx	O
d	O
denotes	O
a	O
frequentist	B
inference	B
for	O
the	O
accuracy	B
of	O
the	O
estimate	B
nx	O
d	O
and	O
suggests	O
that	O
we	O
shouldn	O
t	B
take	O
the	O
very	O
seriously	O
even	O
the	O
being	O
open	O
to	O
doubt	O
where	O
the	O
inference	B
comes	O
from	O
and	O
what	O
exactly	O
it	O
means	O
remains	O
to	O
be	O
said	O
statistical	O
inference	B
usually	O
begins	O
with	O
the	O
assumption	O
that	O
some	O
probability	O
model	O
has	O
produced	O
the	O
observed	O
data	B
x	O
in	O
our	O
case	O
the	O
vector	B
of	O
n	O
d	O
gfr	O
measurements	O
x	O
d	O
xn	O
let	O
x	O
d	O
xn	O
indicate	O
n	O
independent	O
draws	O
from	O
a	O
probability	O
distribution	B
f	O
written	O
f	O
x	O
frequentist	B
inference	B
figure	O
glomerular	O
filtration	O
rates	O
for	O
kidney	O
patients	O
mean	O
standard	B
error	I
f	O
being	O
the	O
underlying	O
distribution	B
of	O
possible	O
gfr	O
scores	O
here	O
a	O
realization	O
x	O
d	O
x	O
of	O
has	O
been	O
observed	O
and	O
the	O
statistician	O
wishes	O
to	O
infer	O
some	O
property	O
of	O
the	O
unknown	O
distribution	B
f	O
suppose	O
the	O
desired	O
property	O
is	O
the	O
expectation	O
of	O
a	O
single	O
random	O
draw	O
x	O
from	O
f	O
denoted	O
also	O
equals	O
the	O
expectation	O
of	O
the	O
average	O
nx	O
dp	O
xi	O
of	O
random	O
d	O
ef	O
fxg	O
vector	B
the	O
obvious	O
estimate	B
of	O
is	O
o	O
d	O
nx	O
the	O
sample	B
average	O
if	O
n	O
were	O
enormous	O
say	O
we	O
would	O
expect	O
o	O
to	O
nearly	O
equal	O
but	O
otherwise	O
there	O
is	O
room	O
for	O
error	O
how	O
much	O
error	O
is	O
the	O
inferential	O
question	O
the	O
estimate	B
o	O
is	O
calculated	O
from	O
x	O
according	O
to	O
some	O
known	O
algorithm	B
say	O
t	B
in	O
our	O
example	O
being	O
the	O
averaging	B
function	B
nx	O
d	O
p	O
xi	O
o	O
is	O
a	O
the	O
fact	O
that	O
eff	O
nxg	O
equals	O
effxg	O
is	O
a	O
crucial	O
though	O
easily	O
proved	O
probabilistic	O
o	O
d	O
t	B
result	O
realization	O
of	O
frequentist	B
inference	B
d	O
ef	O
f	O
o	O
g	O
o	O
d	O
t	B
the	O
output	O
of	O
t	B
applied	O
to	O
a	O
theoretical	O
sample	B
x	O
from	O
f	O
we	O
have	O
chosen	O
t	B
we	O
hope	O
to	O
make	O
o	O
a	O
good	O
estimator	B
of	O
the	O
desired	O
property	O
of	O
f	O
we	O
can	O
now	O
give	O
a	O
first	O
definition	O
of	O
frequentist	B
inference	B
the	O
accuracy	B
of	O
an	O
observed	O
estimate	B
o	O
d	O
t	B
is	O
the	O
probabilistic	O
accuracy	B
of	O
o	O
d	O
t	B
as	O
an	O
estimator	B
of	O
this	O
may	O
seem	O
more	O
a	O
tautology	O
than	O
a	O
definition	O
but	O
it	O
contains	O
a	O
powerful	O
idea	O
o	O
is	O
just	O
a	O
single	O
number	O
but	O
o	O
takes	O
on	O
a	O
range	O
of	O
values	O
whose	O
spread	O
can	O
define	O
measures	O
of	O
accuracy	B
bias	B
and	O
variance	B
are	O
familiar	O
examples	O
of	O
frequentist	B
inference	B
define	O
to	O
be	O
the	O
expectation	O
of	O
o	O
d	O
t	B
under	O
model	O
n	O
o	O
then	O
the	O
bias	B
and	O
variance	B
attributed	O
to	O
estimate	B
o	O
bias	B
d	O
var	O
d	O
ef	O
of	O
parameter	O
are	O
and	O
again	O
what	O
keeps	O
this	O
from	O
tautology	O
is	O
the	O
attribution	O
to	O
the	O
single	O
number	O
o	O
of	O
the	O
probabilistic	O
properties	O
of	O
o	O
following	O
from	O
model	O
if	O
all	O
of	O
this	O
seems	O
too	O
obvious	O
to	O
worry	O
about	O
the	O
bayesian	B
criticisms	O
of	O
chapter	O
may	O
come	O
as	O
a	O
shock	O
frequentism	B
is	O
often	O
defined	O
with	O
respect	O
to	O
an	O
infinite	O
sequence	O
of	O
future	O
trials	O
we	O
imagine	O
hypothetical	O
data	B
sets	O
x	O
x	O
x	O
generated	O
by	O
the	O
same	O
mechanism	O
as	O
x	O
providing	O
corresponding	O
values	O
o	O
o	O
o	O
as	O
in	O
the	O
frequentist	B
principle	O
is	O
then	O
to	O
attribute	O
for	O
o	O
the	O
accuracy	B
properties	O
of	O
the	O
ensemble	B
of	O
o	O
if	O
the	O
o	O
s	O
have	O
empirical	B
variance	B
of	O
say	O
then	O
o	O
d	O
p	O
is	O
claimed	O
to	O
have	O
standard	B
error	I
etc	O
this	O
amounts	O
to	O
a	O
more	O
picturesque	O
restatement	O
of	O
the	O
previous	O
definition	O
frequentism	B
in	O
practice	O
our	O
working	O
definition	O
of	O
frequentism	B
is	O
that	O
the	O
probabilistic	O
properties	O
of	O
a	O
procedure	O
of	O
interest	O
are	O
derived	O
and	O
then	O
applied	O
verbatim	O
to	O
the	O
procedure	O
s	O
output	O
for	O
the	O
observed	O
data	B
this	O
has	O
an	O
obvious	O
defect	O
it	O
requires	O
calculating	O
the	O
properties	O
of	O
estimators	O
o	O
d	O
t	B
obtained	O
from	O
in	O
essence	O
frequentists	O
ask	O
themselves	O
what	O
would	O
i	O
see	O
if	O
i	O
reran	O
the	O
same	O
situation	O
again	O
again	O
and	O
again	O
frequentism	B
in	O
practice	O
statistics	B
o	O
the	O
true	O
distribution	B
f	O
even	O
though	O
f	O
is	O
unknown	O
practical	O
frequentism	B
uses	O
a	O
collection	O
of	O
more	O
or	O
less	O
ingenious	O
devices	O
to	O
circumvent	O
the	O
defect	O
the	O
plug-in	O
principle	O
a	O
simple	O
formula	B
relates	O
the	O
standard	B
error	I
of	O
nx	O
dp	O
xi	O
to	O
varf	O
the	O
variance	B
of	O
a	O
single	O
x	O
drawn	O
from	O
f	O
d	O
varf	O
cvarf	O
dx	O
but	O
having	O
observed	O
x	O
d	O
xn	O
we	O
can	O
estimate	B
varf	O
without	O
bias	B
by	O
plugging	O
formula	B
into	O
givesbse	O
the	O
usual	O
estimate	B
for	O
the	O
standard	B
error	I
of	O
an	O
average	O
nx	O
in	O
other	O
words	O
the	O
frequentist	B
accuracy	B
estimate	B
for	O
nx	O
is	O
itself	O
estimated	O
from	O
the	O
observed	O
d	O
t	B
more	O
complicated	O
taylor-series	O
approximations	O
than	O
nx	O
can	O
often	O
be	O
related	O
back	O
to	O
the	O
plug-in	O
formula	B
by	O
local	O
linear	B
approximations	O
sometimes	O
known	O
as	O
the	O
delta	B
method	B
for	O
example	O
o	O
d	O
has	O
d	O
with	O
bse	O
as	O
in	O
large	O
sample	B
calculations	O
as	O
sample	B
size	I
n	O
goes	O
to	O
o	O
nx	O
d	O
thinking	O
of	O
as	O
a	O
constant	O
gives	O
infinity	O
validate	O
the	O
delta	B
method	B
which	O
fortunately	O
often	O
performs	O
well	O
in	O
small	O
samples	O
parametric	B
families	O
and	O
maximum	B
likelihood	B
theory	B
theoretical	O
expressions	O
for	O
the	O
standard	B
error	I
of	O
a	O
maximum	B
likelihood	B
estimate	B
are	O
discussed	O
in	O
chapters	O
and	O
in	O
the	O
context	O
of	O
parametric	B
families	O
of	O
distributions	O
these	O
combine	O
fisherian	O
theory	B
taylor-series	O
approximations	O
and	O
the	O
plug-in	O
principle	O
in	O
an	O
easy-to-apply	O
package	O
simulation	B
and	O
the	O
bootstrap	B
modern	O
computation	O
has	O
opened	O
up	O
the	O
possibility	O
of	O
numerically	O
implementing	O
the	O
infinite	O
sequence	O
of	O
future	O
trials	O
definition	O
except	O
for	O
the	O
infinite	O
part	O
an	O
estimate	B
of	O
of	O
f	O
perhaps	O
the	O
mle	B
is	O
found	O
and	O
values	O
o	O
d	O
t	B
simulated	O
from	O
of	O
for	O
k	O
d	O
b	O
say	O
b	O
d	O
the	O
empirical	B
standard	B
deviation	I
of	O
the	O
o	O
s	O
is	O
then	O
the	O
frequentist	B
estimate	B
of	I
standard	B
error	I
for	O
o	O
d	O
t	B
and	O
similarly	O
with	O
other	O
measures	O
of	O
accuracy	B
this	O
is	O
a	O
good	O
description	O
of	O
the	O
bootstrap	B
chapter	O
that	O
the	O
most	O
familiar	O
example	O
is	O
the	O
observed	O
proportion	B
p	O
of	O
heads	O
in	O
n	O
flips	O
of	O
a	O
coin	O
having	O
true	O
probability	O
the	O
actual	O
standard	B
error	I
is	O
but	O
we	O
can	O
only	O
report	O
the	O
plug-in	O
estimate	B
pn	O
frequentist	B
inference	B
table	O
three	O
estimates	O
of	O
location	O
for	O
the	O
gfr	O
data	B
and	O
their	O
estimated	O
standard	O
errors	B
last	O
two	O
standard	O
errors	B
using	O
the	O
bootstrap	B
b	O
d	O
estimate	B
standard	B
error	I
mean	O
winsorized	B
mean	I
median	O
of	O
for	O
f	O
comes	O
first	O
rather	O
than	O
at	O
the	O
end	O
of	O
here	O
the	O
plugging-in	O
of	O
the	O
process	O
the	O
classical	O
methods	O
above	O
are	O
restricted	O
to	O
estimates	O
o	O
d	O
t	B
that	O
are	O
smoothly	O
defined	O
functions	O
of	O
various	O
sample	B
means	O
simulation	B
calculations	O
remove	O
this	O
restriction	O
table	O
shows	O
three	O
location	O
estimates	O
for	O
the	O
gfr	O
data	B
the	O
mean	O
the	O
winsorized	O
and	O
the	O
median	O
along	O
with	O
their	O
standard	O
errors	B
the	O
last	O
two	O
computed	O
by	O
the	O
bootstrap	B
a	O
happy	O
feature	O
of	O
computer-age	O
statistical	O
inference	B
is	O
the	O
tremendous	O
expansion	O
of	O
useful	O
and	O
usable	O
statistics	B
t	B
in	O
the	O
statistician	O
s	O
working	O
toolbox	O
the	O
lowess	B
algorithm	B
in	O
figures	O
and	O
providing	O
a	O
nice	O
example	O
pivotal	O
statistics	B
a	O
pivotal	O
statistic	B
o	O
d	O
t	B
is	O
one	O
whose	O
distribution	B
does	O
not	O
depend	O
upon	O
the	O
underlying	O
probability	O
distribution	B
f	O
in	O
such	O
a	O
case	O
the	O
theoretical	O
distribution	B
of	O
o	O
d	O
t	B
applies	O
exactly	O
to	O
o	O
removing	O
the	O
need	O
for	O
devices	O
above	O
the	O
classic	O
example	O
concerns	O
student	O
s	O
two-sample	B
t-test	O
in	O
a	O
two-sample	B
problem	O
the	O
statistician	O
observes	O
two	O
sets	O
of	O
numbers	O
d	O
d	O
and	O
wishes	O
to	O
test	O
the	O
null	O
hypothesis	O
that	O
they	O
come	O
from	O
the	O
same	O
distribution	B
opposed	O
to	O
say	O
the	O
second	O
set	B
tending	O
toward	O
larger	O
values	O
than	O
the	O
first	O
it	O
is	O
assumed	O
that	O
the	O
distribution	B
for	O
is	O
normal	B
or	O
gaussian	B
n	O
i	O
d	O
the	O
notation	O
indicating	O
independent	O
draws	O
from	O
a	O
normal	B
all	O
observations	O
below	O
the	O
percentile	O
of	O
the	O
observations	O
are	O
moved	O
up	O
to	O
that	O
point	O
similarly	O
those	O
above	O
the	O
percentile	O
are	O
moved	O
down	O
and	O
finally	O
the	O
mean	O
is	O
taken	O
each	O
draw	O
having	O
probability	O
density	B
frequentism	B
in	O
practice	O
with	O
expectation	O
and	O
variance	B
likewise	O
n	O
we	O
wish	O
to	O
test	O
the	O
null	O
hypothesis	O
i	O
d	O
the	O
obvious	O
test	O
statistic	B
o	O
d	O
the	O
difference	O
of	O
the	O
means	O
has	O
distribution	B
o	O
c	O
w	O
d	O
c	O
n	O
under	O
we	O
could	O
plug	O
in	O
the	O
unbiased	O
estimate	B
of	O
d	O
c	O
but	O
student	O
provided	O
a	O
more	O
elegant	O
solution	O
instead	O
of	O
o	O
using	O
the	O
two-sample	B
t-statistic	B
wherebsd	O
d	O
c	O
t	B
d	O
we	O
test	O
under	O
t	B
is	O
pivotal	O
having	O
the	O
same	O
distribution	B
s	O
t	B
distribution	B
with	O
c	O
degrees	B
of	I
freedom	I
no	O
matter	O
what	O
the	O
value	O
of	O
the	O
nuisance	O
parameter	O
for	O
c	O
d	O
as	O
in	O
the	O
leukemia	B
example	O
student	O
s	O
distribution	B
gives	O
the	O
hypothesis	O
test	O
that	O
rejects	O
if	O
jtj	O
exceeds	O
has	O
probability	O
exactly	O
of	O
mistaken	O
rejection	O
similarly	O
t	B
d	O
is	O
an	O
exact	O
confidence	O
interval	B
for	O
the	O
difference	O
covering	O
the	O
true	O
value	O
in	O
of	O
repetitions	O
of	O
probability	O
model	O
occasionally	O
one	O
sees	O
frequentism	B
defined	O
in	O
careerist	O
terms	O
e	O
g	O
a	O
statistician	O
who	O
always	O
rejects	O
null	O
hypotheses	O
at	O
the	O
level	O
will	O
over	O
time	O
make	O
only	O
errors	B
of	O
the	O
first	O
kind	O
this	O
is	O
not	O
a	O
comforting	O
criterion	O
for	O
the	O
statistician	O
s	O
clients	O
who	O
are	O
interested	O
in	O
their	O
own	O
situations	O
not	O
everyone	O
else	O
s	O
here	O
we	O
are	O
only	O
assuming	O
hypothetical	O
repetitions	O
of	O
the	O
specific	O
problem	O
at	O
hand	O
frequentist	B
inference	B
what	O
might	O
be	O
called	O
the	O
strong	O
definition	O
of	O
frequentism	B
insists	O
on	O
exact	O
frequentist	B
correctness	O
under	O
experimental	O
repetitions	O
pivotality	O
unfortunately	O
is	O
unavailable	O
in	O
most	O
statistical	O
situations	O
our	O
looser	O
definition	O
of	O
frequentism	B
supplemented	O
by	O
devices	O
such	O
as	O
those	O
presents	O
a	O
more	O
realistic	O
picture	O
of	O
actual	O
frequentist	B
practice	O
frequentist	B
optimality	B
the	O
popularity	O
of	O
frequentist	B
methods	O
reflects	O
their	O
relatively	O
modest	O
mathematical	O
modeling	O
assumptions	O
only	O
a	O
probability	O
model	O
f	O
exactly	O
a	O
family	O
of	O
probabilities	B
chapter	O
and	O
an	O
algorithm	B
of	O
choice	O
t	B
this	O
flexibility	O
is	O
also	O
a	O
defect	O
in	O
that	O
the	O
principle	O
of	O
frequentist	B
correctness	O
doesn	O
t	B
help	O
with	O
the	O
choice	O
of	O
algorithm	B
should	O
we	O
use	O
the	O
sample	B
mean	O
to	O
estimate	B
the	O
location	O
of	O
the	O
gfr	O
distribution	B
maybe	O
the	O
winsorized	B
mean	I
would	O
be	O
better	O
as	O
table	O
suggests	O
the	O
years	O
saw	O
the	O
development	O
of	O
two	O
key	O
results	O
on	O
frequentist	B
optimality	B
that	O
is	O
finding	O
the	O
best	O
choice	O
of	O
t	B
given	O
model	O
f	O
the	O
first	O
of	O
these	O
was	O
fisher	B
s	O
theory	B
of	O
maximum	B
likelihood	B
estimation	B
and	O
the	O
fisher	B
information	B
bound	B
in	O
parametric	B
probability	O
models	B
of	O
the	O
type	O
discussed	O
in	O
chapter	O
the	O
mle	B
is	O
the	O
optimum	O
estimate	B
in	O
terms	O
of	O
minimum	O
standard	B
error	I
in	O
the	O
same	O
spirit	O
the	O
neyman	O
pearson	O
lemma	O
provides	O
an	O
optimum	O
hypothesis-testing	O
algorithm	B
this	O
is	O
perhaps	O
the	O
most	O
elegant	O
of	O
frequentist	B
constructions	B
in	O
its	O
simplest	O
formulation	O
the	O
np	O
lemma	O
assumes	O
we	O
are	O
trying	O
to	O
decide	O
between	O
two	O
possible	O
probability	O
density	B
functions	O
for	O
the	O
observed	O
data	B
x	O
a	O
null	O
hypothesis	O
density	B
and	O
an	O
alternative	O
density	B
a	O
testing	B
rule	B
t	B
says	O
which	O
choice	O
or	O
we	O
will	O
make	O
having	O
observed	O
data	B
x	O
any	O
such	O
rule	B
has	O
two	O
associated	O
frequentist	B
error	O
probabilities	B
choosing	O
when	O
actually	O
generated	O
x	O
and	O
vice	O
versa	O
d	O
d	O
let	O
l	O
x	O
be	O
the	O
likelihood	B
ratio	O
ft	O
d	O
ft	O
d	O
l	O
x	O
d	O
the	O
list	O
of	O
devices	O
is	O
not	O
complete	O
asymptotic	O
calculations	O
play	O
a	O
major	O
role	O
as	O
do	O
more	O
elaborate	O
combinations	O
of	O
pivotality	O
and	O
the	O
plug-in	O
principle	O
see	O
the	O
discussion	O
of	O
approximate	O
bootstrap	B
confidence	O
intervals	B
in	O
chapter	O
and	O
define	O
the	O
testing	B
rule	B
tc	O
x	O
by	O
tc	O
x	O
d	O
frequentist	B
optimality	B
if	O
log	O
l	O
x	O
c	O
if	O
log	O
l	O
x	O
c	O
there	O
is	O
one	O
such	O
rule	B
for	O
each	O
choice	O
of	O
the	O
cutoff	O
c	O
the	O
neyman	O
pearson	O
lemma	O
says	O
that	O
only	O
rules	O
of	O
form	B
can	O
be	O
optimum	O
for	O
any	O
other	O
rule	B
t	B
there	O
will	O
be	O
a	O
rule	B
tc	O
x	O
having	O
smaller	O
errors	B
of	O
both	O
c	O
and	O
c	O
figure	O
neyman	O
pearson	O
alpha	O
beta	B
curve	O
for	O
cutoffs	O
c	O
d	O
n	O
n	O
and	O
sample	B
size	I
n	O
d	O
red	O
dots	O
correspond	O
to	O
figure	O
graphs	O
c	O
c	O
as	O
a	O
function	B
of	O
the	O
cutoff	O
c	O
for	O
the	O
case	O
where	O
x	O
d	O
is	O
obtained	O
by	O
independent	O
sampling	O
from	O
a	O
normal	B
distribution	B
n	O
for	O
versus	O
n	O
for	O
the	O
np	O
lemma	O
says	O
that	O
any	O
rule	B
not	O
of	O
form	B
must	O
have	O
its	O
point	O
lying	O
above	O
the	O
curve	O
here	O
we	O
are	O
ignoring	O
some	O
minor	O
definitional	O
difficulties	O
that	O
can	O
occur	O
if	O
and	O
are	O
discrete	O
b	O
frequentist	B
inference	B
frequentist	B
optimality	B
theory	B
both	O
for	O
estimation	B
and	O
for	O
testing	B
anchored	O
statistical	O
practice	O
in	O
the	O
twentieth	O
century	O
the	O
larger	O
data	B
sets	O
and	O
more	O
complicated	O
inferential	O
questions	O
of	O
the	O
current	O
era	O
have	O
strained	O
the	O
capabilities	O
of	O
that	O
theory	B
computer-age	O
statistical	O
inference	B
as	O
we	O
will	O
see	O
often	O
displays	O
an	O
unsettling	O
ad	O
hoc	O
character	O
perhaps	O
some	O
contemporary	O
fishers	O
and	O
neymans	O
will	O
provide	O
us	O
with	O
a	O
more	O
capacious	O
optimality	B
theory	B
equal	O
to	O
the	O
challenges	O
of	O
current	O
practice	O
but	O
for	O
now	O
that	O
is	O
only	O
a	O
hope	O
frequentism	B
cannot	O
claim	O
to	O
be	O
a	O
seamless	O
philosophy	O
of	O
statistical	O
inference	B
paradoxes	O
and	O
contradictions	O
abound	O
within	O
its	O
borders	O
as	O
will	O
be	O
shown	O
in	O
the	O
next	O
chapter	O
that	O
being	O
said	O
frequentist	B
methods	O
have	O
a	O
natural	O
appeal	O
to	O
working	O
scientists	O
an	O
impressive	O
history	O
of	O
successful	O
application	O
and	O
as	O
our	O
list	O
of	O
five	O
devices	O
suggests	O
the	O
capacity	O
to	O
encourage	O
clever	O
methodology	O
the	O
story	O
that	O
follows	O
is	O
not	O
one	O
of	O
abandonment	O
of	O
frequentist	B
thinking	O
but	O
rather	O
a	O
broadening	O
of	O
connections	O
with	O
other	O
methods	O
notes	O
and	O
details	O
the	O
name	O
frequentism	B
seems	O
to	O
have	O
been	O
suggested	O
by	O
neyman	O
as	O
a	O
statistical	O
analogue	O
of	O
richard	O
von	O
mises	O
frequentist	B
theory	B
of	O
probability	O
the	O
connection	O
being	O
made	O
explicit	O
in	O
his	O
paper	O
frequentist	B
probability	O
and	O
frequentist	B
statistics	B
behaviorism	O
might	O
have	O
been	O
a	O
more	O
descriptive	O
since	O
the	O
theory	B
revolves	O
around	O
the	O
long-run	O
behavior	O
of	O
statistics	B
t	B
but	O
in	O
any	O
case	O
frequentism	B
has	O
stuck	O
replacing	O
the	O
older	O
disparaging	O
term	O
objectivism	O
neyman	O
s	O
attempt	O
at	O
a	O
complete	O
frequentist	B
theory	B
of	O
statistical	O
inference	B
inductive	O
behavior	O
is	O
not	O
much	O
quoted	O
today	O
but	O
can	O
claim	O
to	O
be	O
an	O
important	O
influence	O
on	O
wald	O
s	O
development	O
of	O
decision	O
theory	B
r	B
a	O
fisher	B
s	O
work	O
on	O
maximum	B
likelihood	B
estimation	B
is	O
featured	O
in	O
chapter	O
fisher	B
arguably	O
the	O
founder	O
of	O
frequentist	B
optimality	B
theory	B
was	O
not	O
a	O
pure	O
frequentist	B
himself	O
as	O
discussed	O
in	O
chapter	O
and	O
efron	O
r	B
a	O
fisher	B
in	O
the	O
century	O
that	O
we	O
are	O
well	O
into	O
the	O
twenty-first	O
century	O
the	O
author	O
s	O
talents	O
as	O
a	O
prognosticator	O
can	O
be	O
frequentistically	O
evaluated	O
delta	B
method	B
the	O
delta	B
method	B
uses	O
a	O
first-order	O
taylor	B
series	I
to	O
suppose	O
o	O
o	O
s	O
c	O
o	O
of	O
a	O
statistic	B
o	O
approximate	O
the	O
variance	B
of	O
a	O
function	B
s	O
has	O
meanvariance	O
and	O
consider	O
the	O
approximation	O
s	O
that	O
name	O
is	O
already	O
spoken	O
for	O
in	O
the	O
psychology	O
literature	O
o	O
hence	O
varfs	O
s	O
and	O
use	O
an	O
estimate	B
for	O
notes	O
and	O
details	O
o	O
js	O
we	O
typically	O
plug-in	O
o	O
for	O
bayesian	B
inference	B
the	O
human	O
mind	O
is	O
an	O
inference	B
machine	O
it	O
s	O
getting	O
windy	O
the	O
sky	O
is	O
darkening	O
i	O
d	O
better	O
bring	O
my	O
umbrella	O
with	O
me	O
unfortunately	O
it	O
s	O
not	O
a	O
very	O
dependable	O
machine	O
especially	O
when	O
weighing	O
complicated	O
choices	O
against	O
past	O
experience	O
bayes	O
theorem	B
is	O
a	O
surprisingly	O
simple	O
mathematical	O
guide	O
to	O
accurate	O
inference	B
the	O
theorem	B
rule	B
now	O
years	O
old	O
marked	O
the	O
beginning	O
of	O
statistical	O
inference	B
as	O
a	O
serious	O
scientific	O
subject	O
it	O
has	O
waxed	O
and	O
waned	O
in	O
influence	O
over	O
the	O
centuries	O
now	O
waxing	O
again	O
in	O
the	O
service	O
of	O
computer-age	O
applications	O
bayesian	B
inference	B
if	O
not	O
directly	O
opposed	O
to	O
frequentism	B
is	O
at	O
least	O
orthogonal	O
it	O
reveals	O
some	O
worrisome	O
flaws	O
in	O
the	O
frequentist	B
point	O
of	O
view	O
while	O
at	O
the	O
same	O
time	O
exposing	O
itself	O
to	O
the	O
criticism	O
of	O
dangerous	O
overuse	O
the	O
struggle	O
to	O
combine	O
the	O
virtues	O
of	O
the	O
two	O
philosophies	O
has	O
become	O
more	O
acute	O
in	O
an	O
era	O
of	O
massively	O
complicated	O
data	B
sets	O
much	O
of	O
what	O
follows	O
in	O
succeeding	O
chapters	O
concerns	O
this	O
struggle	O
here	O
we	O
will	O
review	O
some	O
basic	O
bayesian	B
ideas	O
and	O
the	O
ways	O
they	O
impinge	O
on	O
frequentism	B
the	O
fundamental	O
unit	O
of	O
statistical	O
inference	B
both	O
for	O
frequentists	O
and	O
for	O
bayesians	O
is	O
a	O
family	B
of	I
probability	I
densities	I
d	O
x	O
x	O
f	O
x	O
the	O
observed	O
data	B
is	O
a	O
in	O
the	O
sample	B
space	O
x	O
while	O
the	O
unobserved	O
parameter	O
is	O
a	O
point	O
in	O
the	O
parameter	B
space	I
the	O
statistician	O
observes	O
x	O
from	O
and	O
infers	O
the	O
value	O
of	O
perhaps	O
the	O
most	O
familiar	O
case	O
is	O
the	O
normal	B
family	O
d	O
e	O
both	O
x	O
and	O
may	O
be	O
scalars	O
vectors	O
or	O
more	O
complicated	O
objects	O
other	O
names	O
for	O
the	O
generic	O
x	O
and	O
occur	O
in	O
specific	O
situations	O
for	O
instance	O
x	O
for	O
x	O
in	O
chapter	O
we	O
will	O
also	O
call	O
f	O
a	O
family	O
of	O
probability	O
distributions	O
bayesian	B
inference	B
exactly	O
the	O
one-dimensional	O
normal	B
translation	O
with	O
vari	O
ance	O
with	O
both	O
x	O
and	O
equaling	O
the	O
entire	O
real	O
line	O
another	O
central	O
example	O
is	O
the	O
poisson	B
family	O
d	O
e	O
where	O
x	O
is	O
the	O
nonnegative	O
integers	O
and	O
is	O
the	O
nonnegative	O
real	O
line	O
the	O
density	B
specifies	O
the	O
atoms	O
of	O
probability	O
on	O
the	O
discrete	O
points	O
of	O
x	O
probability	O
family	O
f	O
the	O
knowledge	O
of	O
a	O
prior	B
density	B
bayesian	B
inference	B
requires	O
one	O
crucial	O
assumption	O
in	O
addition	O
to	O
the	O
represents	O
prior	B
information	B
concerning	O
the	O
parameter	O
available	O
to	O
the	O
statistician	O
before	O
the	O
observation	O
of	O
x	O
for	O
instance	O
in	O
an	O
application	O
of	O
the	O
normal	B
model	O
it	O
could	O
be	O
known	O
that	O
is	O
positive	O
while	O
past	O
experience	O
shows	O
it	O
never	O
exceeding	O
in	O
which	O
case	O
we	O
might	O
take	O
to	O
be	O
the	O
uniform	O
density	B
d	O
on	O
the	O
interval	B
exactly	O
what	O
constitutes	O
prior	B
knowledge	O
is	O
a	O
crucial	O
question	O
we	O
will	O
consider	O
in	O
ongoing	O
discussions	O
of	O
bayes	O
theorem	B
bayes	O
theorem	B
is	O
a	O
rule	B
for	O
combining	O
the	O
prior	B
knowledge	O
in	O
with	O
the	O
current	O
evidence	O
in	O
x	O
let	O
denote	O
the	O
posterior	B
density	B
of	O
that	O
is	O
our	O
update	O
of	O
the	O
prior	B
density	B
after	O
taking	O
account	O
of	O
observation	O
x	O
bayes	O
rule	B
provides	O
a	O
simple	O
expression	O
for	O
in	O
terms	O
of	O
and	O
f	O
bayes	O
rule	B
d	O
where	O
f	O
is	O
the	O
marginal	B
density	B
of	O
x	O
f	O
dz	O
integral	O
in	O
would	O
be	O
a	O
sum	O
if	O
were	O
discrete	O
the	O
rule	B
is	O
a	O
straightforward	O
exercise	O
in	O
conditional	B
and	O
yet	O
has	O
far-reaching	O
and	O
sometimes	O
surprising	O
consequences	O
in	O
bayes	O
formula	B
x	O
is	O
fixed	O
at	O
its	O
observed	O
value	O
while	O
varies	O
over	O
just	O
the	O
opposite	O
of	O
frequentist	B
calculations	O
we	O
can	O
emphasize	O
this	O
standard	O
notation	O
is	O
x	O
n	O
for	O
a	O
normal	B
distribution	B
with	O
expectation	O
and	O
variance	B
so	O
has	O
x	O
n	O
is	O
the	O
ratio	O
of	O
the	O
joint	O
probability	O
of	O
the	O
pair	O
x	O
and	O
f	O
the	O
marginal	O
probability	O
of	O
x	O
bayesian	B
inference	B
by	O
rewriting	O
as	O
d	O
where	O
is	O
the	O
likelihood	B
function	B
that	O
is	O
with	O
x	O
fixed	O
and	O
varying	O
having	O
computed	O
the	O
constant	O
cx	O
can	O
be	O
determined	O
numerically	O
from	O
the	O
requirement	O
that	O
integrate	O
to	O
obviating	O
the	O
calculation	O
of	O
f	O
note	O
multiplying	O
the	O
likelihood	B
function	B
by	O
any	O
fixed	O
constant	O
has	O
no	O
effect	O
on	O
since	O
can	O
be	O
absorbed	O
into	O
cx	O
so	O
for	O
the	O
poisson	B
family	O
we	O
can	O
take	O
d	O
e	O
ignoring	O
the	O
x	O
factor	B
which	O
acts	O
as	O
a	O
constant	O
in	O
bayes	O
rule	B
the	O
luxury	O
of	O
ignoring	O
factors	O
depending	O
only	O
on	O
x	O
often	O
simplifies	O
bayesian	B
calculations	O
for	O
any	O
two	O
points	O
and	O
in	O
the	O
ratio	O
of	O
posterior	O
densities	O
is	O
by	O
division	O
in	O
d	O
longer	O
involving	O
the	O
marginal	B
density	B
f	O
that	O
is	O
the	O
posterior	O
odds	O
ratio	O
is	O
the	O
prior	B
odds	O
ratio	O
times	O
the	O
likelihood	B
ratio	O
a	O
memorable	O
restatement	O
of	O
bayes	O
rule	B
two	O
examples	O
a	O
simple	O
but	O
genuine	O
example	O
of	O
bayes	O
rule	B
in	O
action	O
is	O
provided	O
by	O
the	O
story	O
of	O
the	O
physicist	O
s	O
twins	O
thanks	O
to	O
sonograms	O
a	O
physicist	O
found	O
out	O
she	O
was	O
going	O
to	O
have	O
twin	O
boys	O
what	O
is	O
the	O
probability	O
my	O
twins	O
will	O
be	O
identical	O
rather	O
than	O
fraternal	O
she	O
asked	O
the	O
doctor	O
answered	O
that	O
one-third	O
of	O
twin	O
births	O
were	O
identicals	O
and	O
two-thirds	O
fraternals	O
in	O
this	O
situation	O
the	O
unknown	O
parameter	O
state	O
of	O
nature	O
is	O
either	O
identical	O
or	O
fraternal	O
with	O
prior	B
probability	O
or	O
x	O
the	O
possible	O
sonogram	O
results	O
for	O
twin	O
births	O
is	O
either	O
same	O
sex	O
or	O
different	O
sexes	O
and	O
x	O
d	O
same	O
sex	O
was	O
observed	O
can	O
ignore	O
sex	O
since	O
that	O
does	O
not	O
affect	O
the	O
calculation	O
a	O
crucial	O
fact	O
is	O
that	O
identical	O
twins	O
are	O
always	O
same-sex	O
while	O
fraternals	O
have	O
probability	O
of	O
same	O
or	O
different	O
so	O
same	O
sex	O
in	O
the	O
sonogram	O
is	O
twice	O
as	O
likely	O
if	O
the	O
twins	O
are	O
identical	O
applying	O
bayes	O
two	O
examples	O
rule	B
in	O
ratio	O
form	B
answers	O
the	O
physicist	O
s	O
question	O
g	O
identicalj	O
same	O
g	O
fraternalj	O
same	O
d	O
g	O
identical	O
g	O
fraternal	O
fidentical	O
same	O
ffraternal	O
same	O
d	O
d	O
that	O
is	O
the	O
posterior	O
odds	O
are	O
even	O
and	O
the	O
physicist	O
s	O
twins	O
have	O
equal	O
probabilities	B
of	O
being	O
identical	O
or	O
here	O
the	O
doctor	O
s	O
prior	B
odds	O
ratio	O
to	O
in	O
favor	O
of	O
fraternal	O
is	O
balanced	O
out	O
by	O
the	O
sonogram	O
s	O
likelihood	B
ratio	O
of	O
to	O
in	O
favor	O
of	O
identical	O
figure	O
analyzing	O
the	O
twins	O
problem	O
there	O
are	O
only	O
four	O
possible	O
combinations	O
of	O
parameter	O
and	O
outcome	O
x	O
in	O
the	O
twins	O
problem	O
labeled	O
a	O
b	O
c	O
and	O
d	O
in	O
figure	O
cell	O
b	O
has	O
probability	O
since	O
identicals	O
cannot	O
be	O
of	O
different	O
sexes	O
cells	O
c	O
and	O
d	O
have	O
equal	O
probabilities	B
because	O
of	O
the	O
random	O
sexes	O
of	O
fraternals	O
finally	O
a	O
c	O
b	O
must	O
have	O
total	O
probability	O
and	O
c	O
c	O
d	O
total	O
probability	O
according	O
to	O
the	O
doctor	O
s	O
prior	B
distribution	B
putting	O
all	O
this	O
together	O
we	O
can	O
fill	O
in	O
the	O
probabilities	B
for	O
all	O
four	O
cells	O
as	O
shown	O
the	O
physicist	O
knows	O
she	O
is	O
in	O
the	O
first	O
column	O
of	O
the	O
table	O
where	O
the	O
conditional	B
probabilities	B
of	O
identical	O
or	O
fraternal	O
are	O
equal	O
just	O
as	O
provided	O
by	O
bayes	O
rule	B
in	O
presumably	O
the	O
doctor	O
s	O
prior	B
distribution	B
came	O
from	O
some	O
enormous	O
state	O
or	O
national	O
database	O
say	O
three	O
million	O
previous	O
twin	O
births	O
one	O
million	O
identical	O
pairs	O
and	O
two	O
million	O
fraternals	O
we	O
deduce	O
that	O
cells	O
a	O
c	O
and	O
d	O
must	O
have	O
had	O
one	O
million	O
entries	O
each	O
in	O
the	O
database	O
while	O
cell	O
b	O
was	O
empty	O
bayes	O
rule	B
can	O
be	O
thought	O
of	O
as	O
a	O
big	O
book	O
with	O
one	O
page	O
they	O
turned	O
out	O
to	O
be	O
fraternal	O
identical	O
twins	O
are	O
fraternal	O
same	O
sex	O
different	O
physicist	O
sonogram	O
shows	O
doctor	O
b	O
a	O
c	O
d	O
bayesian	B
inference	B
for	O
each	O
possible	O
outcome	O
x	O
book	O
has	O
only	O
two	O
pages	O
in	O
figure	O
the	O
physicist	O
turns	O
to	O
the	O
page	O
same	O
sex	O
and	O
sees	O
two	O
million	O
previous	O
twin	O
births	O
half	O
identical	O
and	O
half	O
fraternal	O
correctly	O
concluding	O
that	O
the	O
odds	O
are	O
equal	O
in	O
her	O
situation	O
given	O
any	O
prior	B
distribution	B
and	O
any	O
family	O
of	O
densities	O
bayes	O
rule	B
will	O
always	O
provide	O
a	O
version	O
of	O
the	O
big	O
book	O
that	O
doesn	O
t	B
mean	O
that	O
the	O
book	O
s	O
contents	O
will	O
always	O
be	O
equally	O
convincing	O
the	O
prior	B
for	O
the	O
twins	O
problems	O
was	O
based	O
on	O
a	O
large	O
amount	O
of	O
relevant	O
previous	O
experience	O
such	O
experience	O
is	O
most	O
often	O
unavailable	O
modern	O
bayesian	B
practice	O
uses	O
various	O
strategies	O
to	O
construct	O
an	O
appropriate	O
prior	B
in	O
the	O
absence	O
of	O
prior	B
experience	O
leaving	O
many	O
statisticians	O
unconvinced	O
by	O
the	O
resulting	O
bayesian	B
inferences	O
our	O
second	O
example	O
illustrates	O
the	O
difficulty	O
table	O
scores	O
from	O
two	O
tests	O
taken	O
by	O
students	O
mechanics	O
and	O
vectors	O
mechanics	O
vectors	O
mechanics	O
vectors	O
table	O
shows	O
the	O
scores	O
on	O
two	O
tests	O
mechanics	O
and	O
vectors	O
achieved	O
by	O
n	O
d	O
students	O
the	O
sample	B
correlation	O
coefficient	O
between	O
the	O
two	O
scores	O
is	O
o	O
o	O
d	O
nm	O
vi	O
nv	O
d	O
with	O
m	O
and	O
v	O
short	O
for	O
mechanics	O
and	O
vectors	O
nm	O
and	O
nv	O
their	O
averages	O
we	O
wish	O
to	O
assign	O
a	O
bayesian	B
measure	O
of	O
posterior	O
accuracy	B
to	O
the	O
true	O
correlation	O
coefficient	O
true	O
meaning	O
the	O
correlation	O
for	O
the	O
hypothetical	O
population	O
of	O
all	O
students	O
of	O
which	O
we	O
observed	O
only	O
if	O
we	O
assume	O
that	O
the	O
joint	O
v	O
distribution	B
is	O
bivariate	B
normal	B
discussed	O
in	O
chapter	O
then	O
the	O
density	B
of	O
o	O
as	O
a	O
function	B
of	O
has	O
a	O
known	O
form	B
two	O
examples	O
o	O
z	O
cosh	O
w	O
dw	O
d	O
o	O
f	O
o	O
in	O
terms	O
of	O
our	O
general	O
bayes	O
notation	O
parameter	O
is	O
observation	O
x	O
is	O
o	O
and	O
family	O
f	O
is	O
given	O
by	O
with	O
both	O
and	O
x	O
equaling	O
the	O
interval	B
formula	B
looks	O
formidable	O
to	O
the	O
human	O
eye	O
but	O
not	O
to	O
the	O
computer	O
eye	O
which	O
makes	O
quick	O
work	O
of	O
it	O
figure	O
student	O
scores	O
data	B
posterior	B
density	B
of	O
correlation	O
for	O
three	O
possible	O
priors	B
in	O
this	O
case	O
as	O
in	O
the	O
majority	O
of	O
scientific	O
situations	O
we	O
don	O
t	B
have	O
a	O
trove	O
of	O
relevant	O
past	O
experience	O
ready	O
to	O
provide	O
a	O
prior	B
g	O
one	O
expedient	O
going	O
back	O
to	O
laplace	O
is	O
the	O
principle	O
of	O
insufficient	O
reason	O
that	O
is	O
we	O
take	O
to	O
be	O
uniformly	O
distributed	O
over	O
g	O
d	O
for	O
a	O
flat	B
prior	B
the	O
solid	O
black	O
curve	O
in	O
figure	O
shows	O
the	O
resulting	O
posterior	B
density	B
which	O
is	O
just	O
the	O
likelihood	B
f	O
plotted	O
as	O
a	O
function	B
of	O
scaled	O
to	O
have	O
integral	O
bayesian	B
inference	B
jeffreys	B
prior	B
gjeff	O
d	O
yields	O
posterior	B
density	B
g	O
j	O
o	O
shown	O
by	O
the	O
dashed	O
red	O
curve	O
it	O
suggests	O
somewhat	O
bigger	O
values	O
for	O
the	O
unknown	O
parameter	O
formula	B
arises	O
from	O
a	O
theory	B
of	O
uninformative	O
priors	B
discussed	O
in	O
the	O
next	O
section	O
an	O
improvement	O
on	O
the	O
principle	O
of	O
insufficient	O
reason	O
is	O
an	O
improper	O
density	B
in	O
thatr	O
g	O
d	O
d	O
but	O
it	O
still	O
provides	O
proper	B
pos	O
terior	O
densities	O
when	O
deployed	O
in	O
bayes	O
rule	B
the	O
dotted	O
blue	O
curve	O
in	O
figure	O
is	O
posterior	B
density	B
g	O
j	O
o	O
from	O
the	O
triangular-shaped	O
prior	B
obtained	O
g	O
d	O
jj	O
this	O
is	O
a	O
primitive	O
example	O
of	O
a	O
shrinkage	B
prior	B
one	O
designed	O
to	O
favor	O
smaller	O
values	O
of	O
its	O
effect	O
is	O
seen	O
in	O
the	O
leftward	O
shift	O
of	O
the	O
posterior	B
density	B
shrinkage	B
priors	B
will	O
play	O
a	O
major	O
role	O
in	O
our	O
discussion	O
of	O
largescale	O
estimation	B
and	O
testing	B
problems	O
where	O
we	O
are	O
hoping	O
to	O
find	O
a	O
few	O
large	O
effects	O
hidden	O
among	O
thousands	O
of	O
negligible	O
ones	O
uninformative	B
prior	B
distributions	O
given	O
a	O
convincing	O
prior	B
distribution	B
bayes	O
rule	B
is	O
easier	O
to	O
use	O
and	O
produces	O
more	O
satisfactory	O
inferences	O
than	O
frequentist	B
methods	O
the	O
dominance	O
of	O
frequentist	B
practice	O
reflects	O
the	O
scarcity	O
of	O
useful	O
prior	B
information	B
in	O
day-to-day	O
scientific	O
applications	O
but	O
the	O
bayesian	B
impulse	O
is	O
strong	O
and	O
almost	O
from	O
its	O
inception	O
years	O
ago	O
there	O
have	O
been	O
proposals	O
for	O
the	O
construction	O
of	O
priors	B
that	O
permit	O
the	O
use	O
of	O
bayes	O
rule	B
in	O
the	O
absence	O
of	O
relevant	O
experience	O
one	O
approach	O
perhaps	O
the	O
most	O
influential	O
in	O
current	O
practice	O
is	O
the	O
employment	O
of	O
uninformative	O
priors	B
uninformative	O
has	O
a	O
positive	O
connotation	O
here	O
implying	O
that	O
the	O
use	O
of	O
such	O
a	O
prior	B
in	O
bayes	O
rule	B
does	O
not	O
tacitly	O
bias	B
the	O
resulting	O
inference	B
laplace	O
s	O
principle	O
of	O
insufficient	O
reason	O
i	O
e	O
assigning	O
uniform	O
prior	B
distributions	O
to	O
unknown	O
parameters	O
is	O
an	O
obvious	O
attempt	O
at	O
this	O
goal	O
its	O
use	O
went	O
unchallenged	O
for	O
more	O
than	O
a	O
century	O
perhaps	O
because	O
of	O
laplace	O
s	O
influence	O
more	O
than	O
its	O
own	O
virtues	O
venn	O
the	O
venn	O
diagram	O
in	O
the	O
and	O
fisher	B
in	O
the	O
attacking	O
the	O
routine	O
use	O
of	O
bayes	O
theorem	B
pointed	O
out	O
that	O
laplace	O
s	O
principle	O
could	O
not	O
be	O
applied	O
consistently	O
in	O
the	O
student	O
correlation	O
example	O
for	O
instance	O
a	O
uniform	O
prior	B
distribution	B
for	O
would	O
not	O
be	O
uniform	O
if	O
we	O
changed	O
parameters	O
to	O
d	O
e	O
posterior	O
probabilities	B
such	O
as	O
uninformative	B
prior	B
distributions	O
n	O
o	O
o	O
d	O
pr	O
n	O
o	O
o	O
pr	O
would	O
depend	O
on	O
whether	O
or	O
was	O
taken	O
to	O
be	O
uniform	O
a	O
priori	O
neither	O
choice	O
then	O
could	O
be	O
considered	O
uninformative	O
a	O
more	O
sophisticated	O
version	O
of	O
laplace	O
s	O
principle	O
was	O
put	O
forward	O
by	O
jeffreys	B
beginning	O
in	O
the	O
it	O
depends	O
interestingly	O
enough	O
on	O
the	O
frequentist	B
notion	O
of	O
fisher	B
information	B
for	O
a	O
one-parameter	B
family	O
where	O
the	O
parameter	B
space	I
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
the	O
fisher	B
information	B
is	O
defined	O
to	O
be	O
d	O
log	O
the	O
poisson	B
family	O
d	O
and	O
d	O
the	O
jeffreys	B
prior	B
is	O
by	O
definition	O
i	O
d	O
because	O
equals	O
approximately	O
the	O
variance	B
equivalent	O
definition	O
is	O
of	O
the	O
mle	B
an	O
formula	B
does	O
in	O
fact	O
transform	B
correctly	O
under	O
parameter	O
changes	O
avoiding	O
the	O
venn	O
fisher	B
criticism	O
it	O
is	O
known	O
that	O
o	O
approximate	O
standard	B
deviation	I
in	O
family	O
has	O
d	O
d	O
yielding	O
jeffreys	B
prior	B
from	O
the	O
constant	O
factor	B
c	O
having	O
no	O
effect	O
on	O
bayes	O
rule	B
the	O
red	O
triangles	O
in	O
figure	O
indicate	O
the	O
credible	B
interval	B
z	O
for	O
based	O
on	O
jeffreys	B
prior	B
that	O
is	O
the	O
posterior	B
probability	I
equals	O
o	O
d	O
d	O
with	O
probability	O
for	O
or	O
it	O
is	O
not	O
an	O
accident	O
that	O
this	O
nearly	O
equals	O
the	O
standard	O
neyman	O
confidence	O
interval	B
based	O
on	O
o	O
jeffreys	B
prior	B
tends	O
to	O
induce	O
this	O
nice	O
connection	O
between	O
f	O
the	O
bayesian	B
and	O
frequentist	B
worlds	O
at	O
least	O
in	O
one-parameter	B
families	O
multiparameter	O
probability	O
families	O
chapter	O
make	O
everything	O
more	O
bayesian	B
inference	B
difficult	O
suppose	O
for	O
instance	O
the	O
statistician	O
observes	O
independent	O
versions	O
of	O
the	O
normal	B
model	O
with	O
possibly	O
different	O
values	O
of	O
xi	O
n	O
for	O
i	O
d	O
in	O
standard	O
notation	O
jeffreys	B
prior	B
is	O
flat	O
for	O
any	O
one	O
of	O
the	O
problems	O
which	O
is	O
reasonable	O
for	O
dealing	O
with	O
them	O
separately	O
but	O
the	O
joint	O
jeffreys	B
prior	B
d	O
constant	O
also	O
flat	O
can	O
produce	O
disastrous	O
overall	O
results	O
as	O
discussed	O
in	O
chapter	O
computer-age	O
applications	O
are	O
often	O
more	O
like	O
than	O
except	O
with	O
hundreds	O
or	O
thousands	O
of	O
cases	O
rather	O
than	O
to	O
consider	O
simultaneously	O
uninformative	O
priors	B
of	O
many	O
sorts	O
including	O
jeffreys	B
are	O
highly	O
popular	O
in	O
current	O
applications	O
as	O
we	O
will	O
discuss	O
this	O
leads	O
to	O
an	O
interplay	O
between	O
bayesian	B
and	O
frequentist	B
methodology	O
the	O
latter	O
intended	O
to	O
control	B
possible	O
biases	O
in	O
the	O
former	O
exemplifying	O
our	O
general	O
theme	O
of	O
computer-age	O
statistical	O
inference	B
flaws	O
in	O
frequentist	B
inference	B
bayesian	B
statistics	B
provides	O
an	O
internally	O
consistent	O
coherent	O
program	O
of	O
inference	B
the	O
same	O
cannot	O
be	O
said	O
of	O
frequentism	B
the	O
apocryphal	O
story	O
of	O
the	O
meter	B
reader	I
makes	O
the	O
point	O
an	O
engineer	O
measures	O
the	O
voltages	O
on	O
a	O
batch	O
of	O
tubes	O
using	O
a	O
voltmeter	O
that	O
is	O
normally	O
calibrated	O
x	O
n	O
x	O
being	O
any	O
one	O
measurement	O
and	O
the	O
true	O
batch	O
voltage	O
the	O
measurements	O
range	O
from	O
to	O
with	O
an	O
average	O
of	O
nx	O
d	O
which	O
he	O
reports	O
back	O
as	O
an	O
unbiased	O
estimate	B
of	O
the	O
next	O
day	O
he	O
discovers	O
a	O
glitch	O
in	O
his	O
voltmeter	O
such	O
that	O
any	O
voltage	O
exceeding	O
would	O
have	O
been	O
reported	O
as	O
x	O
d	O
his	O
frequentist	B
statistician	O
tells	O
him	O
that	O
nx	O
d	O
is	O
no	O
longer	O
unbiased	O
for	O
the	O
true	O
expectation	O
since	O
no	O
longer	O
completely	O
describes	O
the	O
probability	O
family	O
statistician	O
says	O
that	O
is	O
a	O
little	O
too	O
small	O
the	O
fact	O
that	O
the	O
glitch	O
didn	O
t	B
affect	O
any	O
of	O
the	O
actual	O
measurements	O
doesn	O
t	B
let	O
him	O
off	O
the	O
hook	O
nx	O
from	O
the	O
actual	O
nx	O
would	O
not	O
be	O
unbiased	O
for	O
in	O
future	O
realizations	O
of	O
probability	O
model	O
a	O
bayesian	B
statistician	O
comes	O
to	O
the	O
meter	B
reader	I
s	O
rescue	O
for	O
any	O
prior	B
density	B
the	O
posterior	B
density	B
d	O
where	O
x	O
is	O
the	O
vector	B
of	O
measurements	O
depends	O
only	O
on	O
the	O
data	B
x	O
actually	O
flaws	O
in	O
frequentist	B
inference	B
observed	O
and	O
not	O
on	O
other	O
potential	O
data	B
sets	O
x	O
that	O
might	O
have	O
been	O
seen	O
the	O
flat	O
jeffreys	B
prior	B
d	O
constant	O
yields	O
posterior	O
expectation	O
nx	O
d	O
for	O
irrespective	O
of	O
whether	O
or	O
not	O
the	O
glitch	O
would	O
have	O
affected	O
readings	O
above	O
figure	O
z-values	O
against	O
null	O
hypothesis	O
d	O
for	O
months	O
through	O
a	O
less	O
contrived	O
version	O
of	O
the	O
same	O
phenomenon	O
is	O
illustrated	O
in	O
figure	O
an	O
ongoing	O
experiment	O
is	O
being	O
run	O
each	O
month	O
i	O
an	O
independent	O
normal	B
variate	O
is	O
observed	O
with	O
the	O
intention	O
of	O
testing	B
the	O
null	O
hypothesis	O
w	O
d	O
versus	O
the	O
alternative	O
the	O
plotted	O
points	O
are	O
test	O
statistics	B
n	O
xi	O
zi	O
d	O
ix	O
xj	O
zi	O
i	O
a	O
z-value	O
based	O
on	O
all	O
the	O
data	B
up	O
to	O
month	O
i	O
at	O
month	O
the	O
scheduled	O
end	O
of	O
the	O
experiment	O
d	O
just	O
exceeding	O
the	O
upper	O
point	O
for	O
a	O
n	O
distribution	B
victory	O
the	O
investigators	O
get	O
to	O
claim	O
significant	O
rejection	O
of	O
at	O
level	O
i	O
n	O
iz	O
bayesian	B
inference	B
unfortunately	O
it	O
turns	O
out	O
that	O
the	O
investigators	O
broke	O
protocol	O
and	O
peeked	O
at	O
the	O
data	B
at	O
month	O
in	O
the	O
hope	O
of	O
being	O
able	O
to	O
stop	O
an	O
expensive	O
experiment	O
early	O
this	O
proved	O
a	O
vain	O
hope	O
d	O
not	O
being	O
anywhere	O
near	O
significance	O
so	O
they	O
continued	O
on	O
to	O
month	O
as	O
originally	O
planned	O
this	O
means	O
they	O
effectively	O
used	O
the	O
stopping	B
rule	B
stop	O
and	O
declare	O
significance	O
if	O
either	O
or	O
exceeds	O
some	O
computation	O
shows	O
that	O
this	O
rule	B
had	O
probability	O
not	O
of	O
rejecting	O
if	O
it	O
were	O
true	O
victory	O
has	O
turned	O
into	O
defeat	O
according	O
to	O
the	O
honored	O
frequentist	B
criterion	O
once	O
again	O
the	O
bayesian	B
statistician	O
is	O
more	O
lenient	O
the	O
likelihood	B
function	B
for	O
the	O
full	B
data	B
set	B
x	O
d	O
d	O
e	O
is	O
the	O
same	O
irrespective	O
of	O
whether	O
or	O
not	O
the	O
experiment	O
might	O
have	O
stopped	O
early	O
the	O
stopping	B
rule	B
doesn	O
t	B
affect	O
the	O
posterior	B
distribution	B
which	O
depends	O
on	O
x	O
only	O
through	O
the	O
likelihood	B
figure	O
unbiased	O
effect-size	O
estimates	O
for	O
genes	O
prostate	B
cancer	O
study	O
the	O
estimate	B
for	O
gene	O
is	O
d	O
what	O
is	O
its	O
effect	B
size	I
the	O
lenient	O
nature	O
of	O
bayesian	B
inference	B
can	O
look	O
less	O
benign	O
in	O
multi	O
effect	B
size	I
estimatesfrequency	O
a	O
bayesianfrequentist	O
comparison	O
list	O
parameter	O
settings	O
figure	O
concerns	O
a	O
prostate	B
cancer	O
study	O
comparing	O
patients	O
with	O
healthy	O
controls	O
each	O
man	O
had	O
his	O
genetic	O
activity	O
measured	O
for	O
a	O
panel	O
of	O
n	O
d	O
genes	O
a	O
statistic	B
x	O
was	O
computed	O
for	O
each	O
comparing	O
the	O
patients	O
with	O
controls	O
say	O
i	O
d	O
n	O
xi	O
n	O
where	O
represents	O
the	O
true	O
effect	B
size	I
for	O
gene	O
i	O
most	O
of	O
the	O
genes	O
probably	O
not	O
being	O
involved	O
in	O
prostate	B
cancer	O
would	O
be	O
expected	O
to	O
have	O
effect	O
sizes	O
near	O
but	O
the	O
investigators	O
hoped	O
to	O
spot	O
a	O
few	O
large	O
values	O
either	O
positive	O
or	O
negative	O
the	O
histogram	O
of	O
the	O
xi	O
values	O
does	O
in	O
fact	O
reveal	O
some	O
large	O
values	O
d	O
being	O
the	O
winner	O
question	O
what	O
estimate	B
should	O
we	O
give	O
for	O
even	O
though	O
was	O
individually	O
unbiased	O
for	O
a	O
frequentist	B
would	O
worry	O
that	O
focusing	O
attention	O
on	O
the	O
largest	O
of	O
values	O
would	O
produce	O
an	O
upward	O
bias	B
and	O
that	O
our	O
estimate	B
should	O
downwardly	O
correct	O
selection	B
bias	B
regression	B
to	I
the	I
mean	I
and	O
the	O
winner	O
s	O
curse	O
are	O
three	O
names	O
for	O
this	O
phenomenon	O
bayesian	B
inference	B
surprisingly	O
is	O
immune	O
to	O
selection	B
bias	B
irrespec-	O
tive	O
of	O
whether	O
gene	O
was	O
prespecified	O
for	O
particular	O
attention	O
or	O
only	O
came	O
to	O
attention	O
as	O
the	O
winner	O
the	O
bayes	O
estimate	B
for	O
given	O
all	O
the	O
data	B
stays	O
the	O
same	O
this	O
isn	O
t	B
obvious	O
but	O
follows	O
from	O
the	O
fact	O
that	O
any	O
data-based	O
selection	O
process	O
does	O
not	O
affect	O
the	O
likelihood	B
function	B
in	O
what	O
does	O
affect	O
bayesian	B
inference	B
is	O
the	O
prior	B
for	O
the	O
full	B
vector	B
of	O
effect	O
sizes	O
the	O
flat	B
prior	B
constant	O
results	O
in	O
the	O
dangerous	O
overestimate	O
d	O
d	O
a	O
more	O
appropriate	O
uninformative	B
prior	B
appears	O
as	O
part	O
of	O
the	O
empirical	B
bayes	I
calculations	O
of	O
chapter	O
gives	O
d	O
the	O
operative	O
point	O
here	O
is	O
that	O
there	O
is	O
a	O
price	O
to	O
be	O
paid	O
for	O
the	O
desirable	O
properties	O
of	O
bayesian	B
inference	B
attention	O
shifts	O
from	O
choosing	O
a	O
good	O
frequentist	B
procedure	O
to	O
choosing	O
an	O
appropriate	O
prior	B
distribution	B
this	O
can	O
be	O
a	O
formidable	O
task	O
in	O
high-dimensional	O
problems	O
the	O
very	O
kinds	O
featured	O
in	O
computer-age	O
inference	B
a	O
bayesianfrequentist	O
comparison	O
list	O
bayesians	O
and	O
frequentists	O
start	O
out	O
on	O
the	O
same	O
playing	O
field	O
a	O
family	O
of	O
probability	O
distributions	O
but	O
play	O
the	O
game	O
in	O
orthogonal	O
the	O
statistic	B
was	O
the	O
two-sample	B
t-statistic	B
transformed	O
to	O
normality	O
see	O
the	O
endnotes	O
bayesian	B
inference	B
directions	O
as	O
indicated	O
schematically	O
in	O
figure	O
bayesian	B
inference	B
proceeds	O
vertically	O
with	O
x	O
fixed	O
according	O
to	O
the	O
posterior	B
distribution	B
while	O
frequentists	O
reason	O
horizontally	O
with	O
fixed	O
and	O
x	O
varying	O
advantages	O
and	O
disadvantages	O
accrue	O
to	O
both	O
strategies	O
some	O
of	O
which	O
are	O
compared	O
next	O
figure	O
bayesian	B
inference	B
proceeds	O
vertically	O
given	O
x	O
frequentist	B
inference	B
proceeds	O
horizontally	O
given	O
bayesian	B
inference	B
requires	O
a	O
prior	B
distribution	B
when	O
past	O
experience	O
provides	O
as	O
in	O
the	O
twins	O
example	O
there	O
is	O
every	O
good	O
reason	O
to	O
employ	O
bayes	O
theorem	B
if	O
not	O
techniques	O
such	O
as	O
those	O
of	O
jeffreys	B
still	O
permit	O
the	O
use	O
of	O
bayes	O
rule	B
but	O
the	O
results	O
lack	O
the	O
full	B
logical	O
force	O
of	O
the	O
theorem	B
the	O
bayesian	B
s	O
right	O
to	O
ignore	O
selection	B
bias	B
for	O
instance	O
must	O
then	O
be	O
treated	O
with	O
caution	O
frequentism	B
replaces	O
the	O
choice	O
of	O
a	O
prior	B
with	O
the	O
choice	O
of	O
a	O
method	B
or	O
algorithm	B
t	B
designed	O
to	O
answer	O
the	O
specific	O
question	O
at	O
hand	O
this	O
adds	O
an	O
arbitrary	O
element	O
to	O
the	O
inferential	O
process	O
and	O
can	O
lead	O
to	O
meterreader	O
kinds	O
of	O
contradictions	O
optimal	O
choice	O
of	O
t	B
reduces	O
arbitrary	O
behavior	O
but	O
computer-age	O
applications	O
typically	O
move	O
outside	O
the	O
safe	O
waters	O
of	O
classical	O
optimality	B
theory	B
lending	O
an	O
ad-hoc	O
character	O
to	O
frequentist	B
analyses	O
modern	O
data-analysis	O
problems	O
are	O
often	O
approached	O
via	O
a	O
favored	O
meth	O
a	O
bayesianfrequentist	O
comparison	O
list	O
odology	O
such	O
as	O
logistic	B
regression	B
or	O
regression	B
trees	B
in	O
the	O
examples	O
of	O
chapter	O
this	O
plays	O
into	O
the	O
methodological	O
orientation	O
of	O
frequentism	B
which	O
is	O
more	O
flexible	O
than	O
bayes	O
rule	B
in	O
dealing	O
with	O
specific	O
algorithms	O
one	O
always	O
hopes	O
for	O
a	O
reasonable	O
bayesian	B
justification	O
for	O
the	O
method	B
at	O
hand	O
having	O
chosen	O
only	O
a	O
single	O
probability	O
distribution	B
is	O
in	O
play	O
for	O
bayesians	O
frequentists	O
by	O
contrast	O
must	O
struggle	O
to	O
balance	O
the	O
behavior	O
of	O
t	B
over	O
a	O
family	O
of	O
possible	O
distributions	O
since	O
in	O
figure	O
is	O
unknown	O
the	O
growing	O
popularity	O
of	O
bayesian	B
applications	O
begun	O
with	O
uninformative	O
priors	B
reflects	O
their	O
simplicity	O
of	O
application	O
and	O
interpretation	O
the	O
simplicity	O
argument	B
cuts	O
both	O
ways	O
the	O
bayesian	B
essentially	O
bets	O
it	O
all	O
on	O
the	O
choice	O
of	O
his	O
or	O
her	O
prior	B
being	O
correct	O
or	O
at	O
least	O
not	O
harmful	O
frequentism	B
takes	O
a	O
more	O
defensive	O
posture	O
hoping	O
to	O
do	O
well	O
or	O
at	O
least	O
not	O
poorly	O
whatever	O
might	O
be	O
a	O
bayesian	B
analysis	B
answers	O
all	O
possible	O
questions	O
at	O
once	O
for	O
example	O
estimating	O
efgfrg	O
or	O
prfgfr	O
or	O
anything	O
else	O
relating	O
to	O
figure	O
frequentism	B
focuses	O
on	O
the	O
problem	O
at	O
hand	O
requiring	O
different	O
estimators	O
for	O
different	O
questions	O
this	O
is	O
more	O
work	O
but	O
allows	O
for	O
more	O
intense	O
inspection	O
of	O
particular	O
problems	O
in	O
situation	O
for	O
example	O
estimators	O
of	O
the	O
form	B
x	O
c	O
might	O
be	O
investigated	O
for	O
different	O
choices	O
of	O
the	O
constant	O
c	O
hoping	O
to	O
reduce	O
expected	O
mean-squared	O
error	O
the	O
simplicity	O
of	O
the	O
bayesian	B
approach	O
is	O
especially	O
appealing	O
in	O
dynamic	O
contexts	O
where	O
data	B
arrives	O
sequentially	O
and	O
updating	O
one	O
s	O
beliefs	O
is	O
a	O
natural	O
practice	O
bayes	O
rule	B
was	O
used	O
to	O
devastating	O
effect	O
before	O
the	O
us	O
presidential	O
election	O
updating	O
sequential	O
polling	O
results	O
to	O
correctly	O
predict	O
the	O
outcome	O
in	O
all	O
states	O
bayes	O
theorem	B
is	O
an	O
excellent	O
tool	O
in	O
general	O
for	O
combining	O
statistical	O
evidence	O
from	O
disparate	O
sources	O
the	O
closest	O
frequentist	B
analog	O
being	O
maximum	B
likelihood	B
estimation	B
in	O
the	O
absence	O
of	O
genuine	O
prior	B
information	B
a	O
whiff	O
of	O
hangs	O
over	O
bayesian	B
results	O
even	O
those	O
based	O
on	O
uninformative	O
priors	B
classical	O
frequentism	B
claimed	O
for	O
itself	O
the	O
high	O
ground	O
of	O
scientific	O
objectivity	O
especially	O
in	O
contentious	O
areas	O
such	O
as	O
drug	O
testing	B
and	O
approval	O
where	O
skeptics	O
as	O
well	O
as	O
friends	O
hang	O
on	O
the	O
statistical	O
details	O
figure	O
is	O
soothingly	O
misleading	O
in	O
its	O
schematics	O
and	O
x	O
will	O
here	O
we	O
are	O
not	O
discussing	O
the	O
important	O
subjectivist	O
school	O
of	O
bayesian	B
inference	B
of	O
savage	B
de	B
finetti	I
and	O
others	O
covered	O
in	O
chapter	O
bayesian	B
inference	B
typically	O
be	O
high-dimensional	O
in	O
the	O
chapters	O
that	O
follow	O
sometimes	O
very	O
high-dimensional	O
straining	O
to	O
the	O
breaking	O
point	O
both	O
the	O
frequentist	B
and	O
the	O
bayesian	B
paradigms	O
computer-age	O
statistical	O
inference	B
at	O
its	O
most	O
successful	O
combines	O
elements	O
of	O
the	O
two	O
philosophies	O
as	O
for	O
instance	O
in	O
the	O
empirical	B
bayes	I
methods	O
of	O
chapter	O
and	O
the	O
lasso	B
in	O
chapter	O
there	O
are	O
two	O
potent	O
arrows	O
in	O
the	O
statistician	O
s	O
philosophical	O
quiver	O
and	O
faced	O
say	O
with	O
parameters	O
and	O
data	B
points	O
there	O
s	O
no	O
need	O
to	O
go	O
hunting	O
armed	O
with	O
just	O
one	O
of	O
them	O
notes	O
and	O
details	O
thomas	O
bayes	O
if	O
transferred	O
to	O
modern	O
times	O
might	O
well	O
be	O
employed	O
as	O
a	O
successful	O
professor	O
of	O
mathematics	O
actually	O
he	O
was	O
a	O
mid-eighteenthcentury	O
nonconformist	O
english	O
minister	O
with	O
substantial	O
mathematical	O
interests	O
richard	O
price	O
a	O
leading	O
figure	O
of	O
letters	O
science	O
and	O
politics	O
had	O
bayes	O
theorem	B
published	O
in	O
the	O
transactions	O
of	O
the	O
royal	O
society	O
years	O
after	O
bayes	O
death	O
his	O
interest	O
being	O
partly	O
theological	O
with	O
the	O
rule	B
somehow	O
proving	O
the	O
existence	O
of	O
god	O
bellhouse	O
s	O
biography	O
includes	O
some	O
of	O
bayes	O
other	O
mathematical	O
accomplishments	O
harold	O
jeffreys	B
was	O
another	O
part-time	O
statistician	O
working	O
from	O
his	O
day	O
job	O
as	O
the	O
world	O
s	O
premier	O
geophysicist	O
of	O
the	O
inter-war	O
period	O
fierce	O
opponent	O
of	O
the	O
theory	B
of	O
continental	O
drift	O
what	O
we	O
called	O
uninformative	O
priors	B
are	O
also	O
called	O
noninformative	O
or	O
objective	O
jeffreys	B
brand	O
of	O
bayesianism	B
had	O
a	O
dubious	O
reputation	O
among	O
bayesians	O
in	O
the	O
period	O
with	O
preference	O
going	O
to	O
subjective	O
analysis	B
of	O
the	O
type	O
advocated	O
by	O
savage	B
and	O
de	B
finetti	I
the	O
introduction	O
of	O
markov	O
chain	O
monte	O
carlo	O
methodology	O
was	O
the	O
kind	O
of	O
technological	O
innovation	O
that	O
changes	O
philosophies	O
mcmc	B
being	O
very	O
well	O
suited	O
to	O
jeffreys-style	O
analysis	B
of	O
big	O
data	B
problems	O
moved	O
bayesian	B
statistics	B
out	O
of	O
the	O
textbooks	O
and	O
into	O
the	O
world	O
of	O
computer-age	O
applications	O
berger	O
makes	O
a	O
spirited	O
case	O
for	O
the	O
objective	B
bayes	I
approach	O
correlation	O
coefficient	O
density	B
formula	B
for	O
the	O
correlation	O
coefficient	O
density	B
was	O
r	B
a	O
fisher	B
s	O
debut	O
contribution	O
to	O
the	O
statistics	B
literature	O
chapter	O
of	O
johnson	O
and	O
kotz	O
gives	O
several	O
equivalent	O
forms	O
the	O
constant	O
c	O
in	O
is	O
often	O
taken	O
to	O
be	O
with	O
n	O
the	O
sample	B
size	I
ters	O
from	O
to	O
in	O
a	O
smoothly	O
differentiable	O
way	O
the	O
new	O
family	O
q	O
jeffreys	B
prior	B
and	O
transformations	O
suppose	O
we	O
change	O
notes	O
and	O
details	O
satisfies	O
log	O
q	O
d	O
and	O
qgjeff	O
d	O
then	O
q	O
says	O
that	O
transforms	O
correctly	O
to	O
qgjeff	O
d	O
log	O
but	O
this	O
just	O
the	O
meter-reader	B
fable	O
is	O
taken	O
from	O
edwards	O
book	O
likelihood	B
where	O
he	O
credits	O
john	O
pratt	O
it	O
nicely	O
makes	O
the	O
point	O
that	O
frequentist	B
inferences	O
which	O
are	O
calibrated	O
in	O
terms	O
of	O
possible	O
observed	O
data	B
sets	O
x	O
may	O
be	O
inappropriate	O
for	O
the	O
actual	O
observation	O
x	O
this	O
is	O
the	O
difference	O
between	O
working	O
in	O
the	O
horizontal	O
and	O
vertical	O
directions	O
of	O
figure	O
two-sample	B
t-statistic	B
applied	O
to	O
gene	O
i	O
s	O
data	B
in	O
the	O
prostate	B
study	O
the	O
two-sample	B
t-statistic	B
ti	O
has	O
theoretical	B
null	I
hypothesis	O
distribution	B
a	O
student	O
s	O
t	B
distribution	B
with	O
degrees	B
of	I
freedom	I
xi	O
where	O
and	O
are	O
the	O
cumulative	O
distribuin	O
is	O
tion	O
functions	O
of	O
standard	O
normal	B
and	O
variables	O
section	O
of	O
efron	O
motivates	O
approximation	O
selection	B
bias	B
senn	O
discusses	O
the	O
immunity	O
of	O
bayesian	B
inferences	O
to	O
selection	B
bias	B
and	O
other	O
paradoxes	O
crediting	O
phil	O
dawid	O
for	O
the	O
original	O
idea	O
the	O
article	O
catches	O
the	O
possible	O
uneasiness	O
of	O
following	O
bayes	O
theorem	B
too	O
literally	O
in	O
applications	O
the	O
students	O
in	O
table	O
were	O
randomly	O
selected	O
from	O
a	O
larger	O
data	B
set	B
of	O
in	O
mardia	O
et	O
al	O
gave	O
o	O
d	O
welch	O
and	O
peers	O
initiated	O
the	O
study	O
of	O
priors	B
whose	O
credible	O
intervals	B
such	O
as	O
in	O
figure	O
match	O
frequentist	B
confidence	O
intervals	B
in	O
one-parameter	B
problems	O
jeffreys	B
priors	B
provide	O
good	O
matches	O
but	O
not	O
ususally	O
in	O
multiparameter	O
situations	O
in	O
fact	O
no	O
single	O
multiparameter	O
prior	B
can	O
give	O
good	O
matches	O
for	O
all	O
one-parameter	B
subproblems	O
a	O
source	O
of	O
tension	O
between	O
bayesian	B
and	O
frequentist	B
methods	O
revisited	O
in	O
chapter	O
fisherian	B
inference	B
and	O
maximum	B
likelihood	B
estimation	B
sir	O
ronald	O
fisher	B
was	O
arguably	O
the	O
most	O
influential	O
anti-bayesian	O
of	O
all	O
time	O
but	O
that	O
did	O
not	O
make	O
him	O
a	O
conventional	O
frequentist	B
his	O
key	O
dataanalytic	O
methods	O
analysis	B
of	O
variance	B
significance	O
testing	B
and	O
maximum	B
likelihood	B
estimation	B
were	O
almost	O
always	O
applied	O
frequentistically	O
their	O
fisherian	O
rationale	O
however	O
often	O
drew	O
on	O
ideas	O
neither	O
bayesian	B
nor	O
frequentist	B
in	O
nature	O
or	O
sometimes	O
the	O
two	O
in	O
combination	O
fisher	B
s	O
work	O
held	O
a	O
central	O
place	O
in	O
twentieth-century	O
applied	O
statistics	B
and	O
some	O
of	O
it	O
particularly	O
maximum	B
likelihood	B
estimation	B
has	O
moved	O
forcefully	O
into	O
computer-age	O
practice	O
this	O
chapter	O
s	O
brief	O
review	O
of	O
fisherian	O
methodology	O
sketches	O
parts	O
of	O
its	O
unique	O
philosophical	O
structure	B
while	O
concentrating	O
on	O
those	O
topics	O
of	O
greatest	O
current	O
importance	O
likelihood	B
and	O
maximum	B
likelihood	B
fisher	B
s	O
seminal	O
work	O
on	O
estimation	B
focused	O
on	O
the	O
likelihood	B
function	B
or	O
more	O
exactly	O
its	O
logarithm	O
for	O
a	O
family	B
of	I
probability	I
densities	I
the	O
log	O
likelihood	B
function	B
is	O
d	O
the	O
notation	O
emphasizing	O
that	O
the	O
parameter	O
vector	B
is	O
varying	O
while	O
the	O
observed	O
data	B
vector	B
x	O
is	O
fixed	O
the	O
maximum	B
likelihood	B
estimate	B
is	O
the	O
value	O
of	O
in	O
parameter	B
space	I
that	O
maximizes	O
mle	B
w	O
d	O
arg	O
max	O
it	O
can	O
happen	O
that	O
doesn	O
t	B
exist	O
or	O
that	O
there	O
are	O
multiple	O
maximizers	O
but	O
here	O
we	O
will	O
assume	O
the	O
usual	O
case	O
where	O
exists	O
uniquely	O
more	O
careful	O
references	O
are	O
provided	O
in	O
the	O
endnotes	O
definition	O
is	O
extended	O
to	O
provide	O
maximum	B
likelihood	B
estimates	O
likelihood	B
and	O
maximum	B
likelihood	B
for	O
a	O
function	B
d	O
t	B
of	O
according	O
to	O
the	O
simple	O
plug-in	O
rule	B
o	O
d	O
t	B
most	O
often	O
with	O
being	O
a	O
scalar	O
parameter	O
of	O
particular	O
interest	O
such	O
as	O
the	O
regression	B
coefficient	O
of	O
an	O
important	O
covariate	O
in	O
a	O
linear	B
model	I
maximum	B
likelihood	B
estimation	B
came	O
to	O
dominate	O
classical	O
applied	O
estimation	B
practice	O
less	O
dominant	O
now	O
for	O
reasons	O
we	O
will	O
be	O
investigating	O
in	O
subsequent	O
chapters	O
the	O
mle	B
algorithm	B
still	O
has	O
iconic	O
status	O
being	O
often	O
the	O
method	B
of	O
first	O
choice	O
in	O
any	O
novel	O
situation	O
there	O
are	O
several	O
good	O
reasons	O
for	O
its	O
ubiquity	O
the	O
mle	B
algorithm	B
is	O
automatic	O
in	O
theory	B
and	O
almost	O
in	O
practice	O
a	O
single	O
numerical	O
algorithm	B
produces	O
without	O
further	O
statistical	O
input	O
this	O
contrasts	O
with	O
unbiased	O
estimation	B
for	O
instance	O
where	O
each	O
new	O
situation	O
requires	O
clever	O
theoretical	O
calculations	O
the	O
mle	B
enjoys	O
excellent	O
frequentist	B
properties	O
in	O
large-sample	O
situations	O
maximum	B
likelihood	B
estimates	O
tend	O
to	O
be	O
nearly	O
unbiased	O
with	O
the	O
least	O
possible	O
variance	B
even	O
in	O
small	O
samples	O
mles	O
are	O
usually	O
quite	O
efficient	O
within	O
say	O
a	O
few	O
percent	O
of	O
the	O
best	O
possible	O
performance	O
the	O
mle	B
also	O
has	O
reasonable	O
bayesian	B
justification	O
looking	O
at	O
bayes	O
rule	B
d	O
we	O
see	O
that	O
is	O
the	O
maximizer	O
of	O
the	O
posterior	B
density	B
if	O
the	O
prior	B
is	O
flat	O
that	O
is	O
constant	O
because	O
the	O
mle	B
depends	O
on	O
the	O
family	O
f	O
only	O
through	O
the	O
likelihood	B
function	B
anomalies	O
of	O
the	O
meter-reader	B
type	O
are	O
averted	O
figure	O
displays	O
two	O
maximum	B
likelihood	B
estimates	O
for	O
the	O
gfr	O
data	B
of	O
figure	O
here	O
the	O
is	O
the	O
vector	B
x	O
d	O
xn	O
n	O
d	O
we	O
assume	O
that	O
x	O
was	O
obtained	O
as	O
a	O
random	O
sample	B
of	O
size	O
n	O
from	O
a	O
density	B
for	O
i	O
d	O
n	O
xi	O
iid	O
abbreviating	O
independent	O
and	O
identically	O
distributed	O
two	O
families	O
are	O
considered	O
for	O
the	O
component	O
density	B
the	O
normal	B
with	O
d	O
d	O
e	O
now	O
x	O
is	O
what	O
we	O
have	O
been	O
calling	O
x	O
before	O
while	O
we	O
will	O
henceforth	O
use	O
x	O
as	O
a	O
symbol	O
for	O
the	O
individual	O
components	O
of	O
x	O
fisherian	B
inference	B
and	O
mle	B
figure	O
glomerular	O
filtration	O
data	B
of	O
figure	O
and	O
two	O
maximum-likelihood	O
density	B
estimates	O
normal	B
black	O
and	O
gamma	B
blue	O
and	O
the	O
with	O
d	O
d	O
e	O
x	O
otherwise	O
under	O
iid	O
sampling	O
we	O
have	O
since	O
d	O
ny	O
log	O
d	O
nx	O
hx	O
d	O
nx	O
d	O
dnx	O
o	O
lxi	O
maximum	B
likelihood	B
estimates	O
were	O
found	O
by	O
maximizing	O
for	O
the	O
normal	B
model	O
the	O
gamma	B
distribution	B
is	O
usually	O
defined	O
with	O
d	O
as	O
the	O
lower	O
limit	O
of	O
x	O
here	O
we	O
are	O
allowing	O
the	O
lower	O
limit	O
to	O
vary	O
as	O
a	O
free	O
parameter	O
fisher	B
information	B
and	O
the	O
mle	B
there	O
is	O
no	O
closed-form	O
solution	O
for	O
gamma	B
model	O
where	O
numerical	O
maximization	O
gave	O
d	O
the	O
plotted	O
curves	O
in	O
figure	O
are	O
the	O
two	O
mle	B
densities	O
f	O
the	O
gamma	B
model	O
gives	O
a	O
better	O
fit	O
than	O
the	O
normal	B
but	O
neither	O
is	O
really	O
satisfactory	O
more	O
ambitious	O
maximum	B
likelihood	B
fit	O
appears	O
in	O
figure	O
most	O
mles	O
require	O
numerical	O
minimization	O
as	O
for	O
the	O
gamma	B
model	O
when	O
introduced	O
in	O
the	O
maximum	B
likelihood	B
was	O
criticized	O
as	O
computationally	O
difficult	O
invidious	O
comparisons	O
being	O
made	O
with	O
the	O
older	O
method	B
of	O
moments	O
which	O
relied	O
only	O
on	O
sample	B
moments	O
of	O
various	O
kinds	O
there	O
is	O
a	O
downside	O
to	O
maximum	B
likelihood	B
estimation	B
that	O
remained	O
nearly	O
invisible	O
in	O
classical	O
applications	O
it	O
is	O
dangerous	O
to	O
rely	O
upon	O
in	O
problems	O
involving	O
large	O
numbers	O
of	O
parameters	O
if	O
the	O
parameter	O
vector	B
has	O
components	O
each	O
component	O
individually	O
may	O
be	O
well	O
estimated	O
by	O
maximum	B
likelihood	B
while	O
the	O
mle	B
o	O
d	O
t	B
for	O
a	O
quantity	B
of	O
particular	O
interest	O
can	O
be	O
grossly	O
misleading	O
for	O
the	O
prostate	B
data	B
of	O
figure	O
model	O
gives	O
mle	B
d	O
xi	O
for	O
each	O
of	O
the	O
genes	O
this	O
seems	O
reasonable	O
but	O
if	O
we	O
are	O
interested	O
in	O
the	O
maximum	O
coordinate	O
value	O
d	O
t	B
d	O
max	O
the	O
mle	B
is	O
o	O
d	O
almost	O
certainly	O
a	O
flagrant	O
overestimate	O
regularized	O
versions	O
of	O
maximum	B
likelihood	B
estimation	B
more	O
suitable	O
for	O
highdimensional	O
applications	O
play	O
an	O
important	O
role	O
in	O
succeeding	O
chapters	O
i	O
fisher	B
information	B
and	O
the	O
mle	B
fisher	B
was	O
not	O
the	O
first	O
to	O
suggest	O
the	O
maximum	B
likelihood	B
algorithm	B
for	O
parameter	O
estimation	B
his	O
paradigm-shifting	O
work	O
concerned	O
the	O
favorable	O
inferential	O
properties	O
of	O
the	O
mle	B
and	O
in	O
particular	O
its	O
achievement	O
of	O
the	O
fisher	B
information	B
bound	B
only	O
a	O
brief	O
heuristic	O
review	O
will	O
be	O
provided	O
here	O
with	O
more	O
careful	O
derivations	O
referenced	O
in	O
the	O
endnotes	O
we	O
with	O
a	O
one-parameter	B
family	O
of	O
densities	O
g	O
d	O
ff	O
x	O
x	O
the	O
multiparameter	O
case	O
is	O
considered	O
in	O
the	O
next	O
chapter	O
f	O
fisherian	B
inference	B
and	O
mle	B
where	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
possibly	O
infinite	O
while	O
the	O
samthe	O
continuous	O
case	O
with	O
the	O
probability	O
of	O
set	B
a	O
equalingr	O
ple	O
space	O
x	O
may	O
be	O
multidimensional	O
in	O
the	O
poisson	B
example	O
f	O
can	O
represent	O
a	O
discrete	O
density	B
but	O
for	O
convenience	O
we	O
assume	O
here	O
a	O
f	O
dx	O
etc	O
the	O
log	O
likelihood	B
function	B
is	O
lx	O
d	O
log	O
f	O
and	O
the	O
mle	B
o	O
d	O
arg	O
maxflx	O
with	O
replacing	O
in	O
in	O
the	O
one-dimensional	O
case	O
dots	O
will	O
indicate	O
differentiation	O
with	O
respect	O
to	O
e	O
g	O
for	O
the	O
score	B
function	B
p	O
lx	O
d	O
log	O
f	O
d	O
p	O
f	O
the	O
score	B
function	B
has	O
expectation	O
z	O
lx	O
dx	O
dz	O
p	O
x	O
x	O
z	O
x	O
d	O
f	O
dx	O
p	O
f	O
dx	O
d	O
d	O
where	O
we	O
are	O
assuming	O
the	O
regularity	O
conditions	B
necessary	O
for	O
differentiating	O
under	O
the	O
integral	O
sign	O
at	O
the	O
third	O
step	O
the	O
fisher	B
information	B
i	O
is	O
defined	O
to	O
be	O
the	O
variance	B
of	O
the	O
score	B
function	B
p	O
lx	O
dx	O
i	O
dz	O
x	O
the	O
notation	O
p	O
lx	O
indicating	O
that	O
p	O
lx	O
has	O
mean	O
and	O
variance	B
i	O
the	O
term	O
information	B
is	O
well	O
chosen	O
the	O
main	O
result	O
for	O
maximum	B
likelihood	B
estimation	B
sketched	O
next	O
is	O
that	O
the	O
mle	B
o	O
has	O
an	O
approximately	O
normal	B
distribution	B
with	O
mean	O
and	O
variance	B
o	O
n	O
and	O
that	O
no	O
nearly	O
unbiased	O
estimator	B
of	O
can	O
do	O
better	O
in	O
other	O
words	O
bigger	O
fisher	B
information	B
implies	O
smaller	O
variance	B
for	O
the	O
mle	B
the	O
second	O
derivative	O
of	O
the	O
log	O
likelihood	B
function	B
r	B
lx	O
d	O
log	O
f	O
d	O
r	B
f	O
f	O
f	O
f	O
p	O
fisher	B
information	B
and	O
the	O
mle	B
has	O
expectation	O
e	O
r	B
nr	O
lx	O
o	O
d	O
i	O
f	O
term	O
having	O
expectation	O
as	O
in	O
we	O
can	O
write	O
lx	O
where	O
j	O
is	O
the	O
variance	B
of	O
r	B
lx	O
now	O
suppose	O
that	O
x	O
d	O
xn	O
is	O
an	O
iid	O
sample	B
from	O
f	O
as	O
in	O
so	O
that	O
the	O
total	O
score	B
function	B
p	O
lx	O
d	O
nx	O
lx	O
as	O
in	O
is	O
p	O
lxi	O
lx	O
d	O
nx	O
p	O
o	O
lx	O
c	O
r	B
o	O
d	O
a	O
first-order	O
taylor	B
series	I
gives	O
the	O
approximation	O
based	O
on	O
the	O
full	B
sample	B
x	O
satisfies	O
the	O
maximizing	O
condition	B
the	O
mle	B
o	O
p	O
lx	O
lxi	O
d	O
p	O
lx	O
p	O
and	O
similarly	O
o	O
lx	O
or	O
o	O
c	O
p	O
lx	O
lx	O
under	O
reasonable	O
regularity	O
conditions	B
and	O
the	O
central	B
limit	I
theorem	B
imply	O
that	O
p	O
lx	O
while	O
the	O
law	O
of	O
large	O
numbers	O
has	O
n	O
lx	O
approaching	O
the	O
constant	O
i	O
putting	O
all	O
of	O
this	O
together	O
produces	O
fisher	B
s	O
fundamental	O
theo	O
rem	O
for	O
the	O
mle	B
that	O
in	O
large	O
samples	O
o	O
n	O
this	O
is	O
the	O
same	O
as	O
result	O
since	O
the	O
total	O
fisher	B
information	B
in	O
an	O
iid	O
sample	B
is	O
ni	O
as	O
can	O
be	O
seen	O
by	O
taking	O
expectations	O
in	O
in	O
the	O
case	O
of	O
normal	B
sampling	O
xi	O
n	O
for	O
i	O
d	O
n	O
fisherian	B
inference	B
and	O
mle	B
this	O
gives	O
with	O
known	O
we	O
compute	O
the	O
log	O
likelihood	B
n	O
nx	O
lx	O
d	O
nx	O
lx	O
d	O
n	O
yielding	O
the	O
familiar	O
result	O
o	O
d	O
nx	O
and	O
since	O
i	O
d	O
o	O
p	O
lx	O
d	O
and	O
r	B
n	O
from	O
this	O
brings	O
us	O
to	O
an	O
aspect	O
of	O
fisherian	B
inference	B
neither	O
bayesian	B
nor	O
frequentist	B
fisher	B
believed	O
there	O
was	O
a	O
logic	B
of	I
inductive	B
inference	B
that	O
would	O
produce	O
the	O
correct	O
answer	O
to	O
any	O
statistical	O
question	O
in	O
the	O
same	O
way	O
ordinary	O
logic	O
solves	O
deductive	O
problems	O
his	O
principal	O
tactic	O
was	O
to	O
logically	O
reduce	O
a	O
complicated	O
inferential	O
question	O
to	O
a	O
simple	O
form	B
where	O
the	O
solution	O
should	O
be	O
obvious	O
to	O
all	O
fisher	B
s	O
favorite	O
target	O
for	O
the	O
obvious	O
was	O
where	O
a	O
single	O
scalar	O
observation	O
o	O
is	O
normally	O
distributed	O
around	O
the	O
unknown	O
parameter	O
of	O
interest	O
with	O
known	O
variance	B
then	O
everyone	O
should	O
agree	O
in	O
the	O
absence	O
of	O
prior	B
information	B
that	O
o	O
is	O
the	O
best	O
estimate	B
of	O
that	O
has	O
about	O
chance	O
of	O
lying	O
in	O
the	O
interval	B
o	O
fisher	B
was	O
astoundingly	O
resourceful	O
at	O
reducing	O
statistical	O
problems	O
to	O
the	O
form	B
sufficiency	O
efficiency	O
conditionality	B
and	O
ancillarity	O
were	O
all	O
brought	O
to	O
bear	O
with	O
the	O
maximum	B
likelihood	B
approximation	O
being	O
the	O
most	O
influential	O
example	O
fisher	B
s	O
logical	O
system	O
is	O
not	O
in	O
favor	O
these	O
days	O
but	O
its	O
conclusions	O
remain	O
as	O
staples	O
of	O
conventional	O
statistical	O
practice	O
suppose	O
that	O
q	O
d	O
t	B
is	O
any	O
unbiased	O
estimate	B
of	O
based	O
on	O
an	O
iid	O
sample	B
x	O
d	O
xn	O
from	O
f	O
that	O
is	O
p	O
n	O
etc	O
then	O
the	O
cram	O
er	O
rao	O
lower	O
bound	B
described	O
in	O
the	O
endnotes	O
says	O
that	O
the	O
variance	B
of	O
q	O
exceeds	O
the	O
fisher	B
information	B
bound	B
d	O
eft	O
n	O
q	O
o	O
var	O
a	O
loose	O
interpretation	O
is	O
that	O
the	O
mle	B
has	O
variance	B
at	O
least	O
as	O
small	O
as	O
the	O
best	O
unbiased	O
estimate	B
of	O
the	O
mle	B
is	O
generally	O
not	O
unbiased	O
but	O
conditional	B
inference	B
p	O
its	O
bias	B
is	O
small	O
order	O
compared	O
with	O
standard	B
deviation	I
of	O
order	O
n	O
making	O
the	O
comparison	O
with	O
unbiased	O
estimates	O
and	O
the	O
cram	O
er	O
rao	O
bound	B
appropriate	O
conditional	B
inference	B
a	O
simple	O
example	O
gets	O
across	O
the	O
idea	O
of	O
conditional	B
inference	B
an	O
i	O
i	O
d	O
sample	B
xi	O
n	O
has	O
produced	O
estimate	B
o	O
d	O
nx	O
the	O
investigators	O
originally	O
disagreed	O
on	O
an	O
affordable	O
sample	B
size	I
n	O
and	O
flipped	O
a	O
fair	O
coin	O
to	O
decide	O
i	O
d	O
n	O
probability	O
probability	O
n	O
d	O
p	O
if	O
you	O
answered	O
n	O
d	O
won	O
question	O
what	O
is	O
the	O
standard	B
deviation	I
of	O
nx	O
d	O
then	O
you	O
like	O
fisher	B
are	O
an	O
advocate	O
of	O
conditional	B
inference	B
the	O
unconditional	O
frequentist	B
answer	O
says	O
that	O
nx	O
could	O
have	O
been	O
n	O
or	O
n	O
with	O
equal	O
probability	O
yielding	O
standard	B
deviation	I
c	O
d	O
some	O
less	O
obvious	O
less	O
trivial	O
examples	O
follow	O
in	O
this	O
section	O
and	O
in	O
chapter	O
where	O
conditional	B
inference	B
plays	O
a	O
central	O
role	O
the	O
data	B
for	O
a	O
typical	O
regression	B
problem	O
consists	O
of	O
pairs	O
yi	O
i	O
d	O
n	O
where	O
xi	O
is	O
a	O
p-dimensional	O
vector	B
of	O
covariates	O
for	O
the	O
ith	O
subject	O
and	O
yi	O
is	O
a	O
scalar	O
response	B
in	O
figure	O
xi	O
is	O
age	O
and	O
yi	O
the	O
kidney	O
fitness	O
measure	O
tot	O
let	O
x	O
be	O
the	O
n	O
p	O
matrix	B
having	O
xi	O
as	O
its	O
ith	O
row	O
and	O
y	O
the	O
vector	B
of	O
responses	O
a	O
regression	B
algorithm	B
uses	O
x	O
and	O
y	O
to	O
construct	O
a	O
function	B
rxy	O
predicting	O
y	O
for	O
any	O
value	O
of	O
x	O
as	O
in	O
where	O
o	O
how	O
accurate	O
is	O
rxy	O
this	O
question	O
is	O
usually	O
answered	O
under	O
the	O
assumption	O
that	O
x	O
is	O
fixed	O
not	O
random	O
in	O
other	O
words	O
by	O
conditioning	O
on	O
the	O
observed	O
value	O
of	O
x	O
the	O
standard	O
errors	B
in	O
the	O
second	O
line	O
of	O
table	O
are	O
conditional	B
in	O
this	O
sense	O
they	O
are	O
frequentist	B
standard	B
deviations	I
c	O
o	O
of	O
o	O
assuming	O
that	O
the	O
values	O
for	O
age	O
are	O
fixed	O
as	O
observed	O
correlation	O
analysis	B
between	O
age	O
and	O
tot	O
would	O
not	O
make	O
this	O
assumption	O
were	O
obtained	O
using	O
least	B
squares	I
and	O
o	O
fisher	B
argued	O
for	O
conditional	B
inference	B
on	O
two	O
grounds	O
fisherian	B
inference	B
and	O
mle	B
more	O
relevant	O
inferences	O
the	O
conditional	B
standard	B
deviation	I
in	O
situation	O
seems	O
obviously	O
more	O
relevant	O
to	O
the	O
accuracy	B
of	O
the	O
observed	O
o	O
for	O
estimating	O
it	O
is	O
less	O
obvious	O
in	O
the	O
regression	B
example	O
though	O
arguably	O
still	O
the	O
case	O
simpler	O
inferences	O
conditional	B
inferences	O
are	O
often	O
simpler	O
to	O
execute	O
and	O
interpret	O
this	O
is	O
the	O
case	O
with	O
regression	B
where	O
the	O
statistician	O
doesn	O
t	B
have	O
to	O
worry	O
about	O
correlation	O
relationships	O
among	O
the	O
covariates	O
and	O
also	O
with	O
our	O
next	O
example	O
a	O
fisherian	O
classic	O
table	O
shows	O
the	O
results	O
of	O
a	O
randomized	O
trial	O
on	O
ulcer	O
patients	O
comparing	O
new	O
and	O
old	O
surgical	O
treatments	O
was	O
the	O
new	O
surgery	O
significantly	O
better	O
fisher	B
argued	O
for	O
carrying	O
out	O
the	O
hypothesis	O
test	O
conditional	B
on	O
the	O
marginals	O
of	O
the	O
table	O
with	O
the	O
marginals	O
fixed	O
the	O
number	O
y	O
in	O
the	O
upper	O
left	O
cell	O
determines	O
the	O
other	O
three	O
cells	O
by	O
subtraction	O
we	O
need	O
only	O
test	O
whether	O
the	O
number	O
y	O
d	O
is	O
too	O
big	O
under	O
the	O
null	O
hypothesis	O
of	O
no	O
treatment	O
difference	O
instead	O
of	O
trying	O
to	O
test	O
the	O
numbers	O
in	O
all	O
four	O
table	O
forty-five	O
ulcer	O
patients	O
randomly	O
assigned	O
to	O
either	O
new	O
or	O
old	O
surgery	O
with	O
results	O
evaluated	O
as	O
either	O
success	O
or	O
failure	O
was	O
the	O
new	O
surgery	O
significantly	O
better	O
success	O
failure	O
new	O
old	O
an	O
ancillary	B
statistic	B
fisher	B
s	O
terminology	O
is	O
one	O
that	O
contains	O
no	O
direct	O
information	B
by	O
itself	O
but	O
does	O
determine	O
the	O
conditioning	O
framework	O
for	O
frequentist	B
calculations	O
our	O
three	O
examples	O
of	O
ancillaries	O
were	O
the	O
sample	B
size	I
n	O
the	O
covariate	O
matrix	B
x	O
and	O
the	O
table	O
s	O
marginals	O
contains	O
no	O
information	B
is	O
a	O
contentious	O
claim	O
more	O
realistically	O
the	O
two	O
advantages	O
of	O
conditioning	O
relevance	B
and	O
simplicity	O
are	O
thought	O
to	O
outweigh	O
the	O
loss	O
of	O
information	B
that	O
comes	O
from	O
treating	O
the	O
ancillary	B
statistic	B
as	O
nonrandom	O
chapter	O
makes	O
this	O
case	O
specifically	O
for	O
standard	O
survival	B
analysis	B
methods	O
section	O
gives	O
the	O
details	O
of	O
such	O
tests	O
in	O
the	O
surgery	O
example	O
the	O
difference	O
was	O
not	O
significant	O
conditional	B
inference	B
our	O
final	O
example	O
concerns	O
the	O
accuracy	B
of	O
a	O
maximum	B
likelihood	B
estimate	B
o	O
rather	O
than	O
o	O
o	O
n	O
o	O
the	O
plug-in	O
version	O
of	O
fisher	B
suggested	O
using	O
n	O
where	O
i	O
x	O
is	O
the	O
observed	O
fisher	B
information	B
o	O
d	O
lx	O
i	O
x	O
d	O
lx	O
the	O
expectation	O
of	O
i	O
x	O
is	O
ni	O
so	O
in	O
large	O
samples	O
the	O
distribution	B
converges	O
to	O
before	O
convergence	O
however	O
fisher	B
suggested	O
that	O
gives	O
a	O
better	O
idea	O
of	O
o	O
as	O
a	O
check	O
a	O
simulation	B
was	O
run	O
involving	O
i	O
i	O
d	O
samples	O
x	O
of	O
size	O
n	O
d	O
s	O
accuracy	B
drawn	O
from	O
a	O
cauchy	O
density	B
f	O
d	O
c	O
within	O
each	O
group	O
was	O
then	O
calculated	O
samples	O
x	O
of	O
size	O
n	O
d	O
were	O
drawn	O
d	O
and	O
the	O
observed	O
information	B
bound	B
computed	O
for	O
each	O
the	O
o	O
values	O
were	O
grouped	O
according	O
to	O
deciles	O
of	O
and	O
the	O
observed	O
empirical	B
variance	B
of	O
o	O
this	O
amounts	O
to	O
calculating	O
a	O
somewhat	O
crude	O
estimate	B
of	O
the	O
conditional	B
variance	B
of	O
the	O
mle	B
o	O
given	O
the	O
observed	O
information	B
bound	B
figure	O
shows	O
the	O
results	O
we	O
see	O
that	O
the	O
conditional	B
variance	B
is	O
close	O
to	O
as	O
fisher	B
predicted	O
the	O
conditioning	O
effect	O
is	O
quite	O
substantial	O
the	O
unconditional	O
variance	B
is	O
here	O
while	O
the	O
conditional	B
variance	B
ranges	O
from	O
to	O
the	O
observed	O
fisher	B
information	B
i	O
x	O
acts	O
as	O
an	O
approximate	O
ancillary	B
enjoying	O
both	O
of	O
the	O
virtues	O
claimed	O
by	O
fisher	B
it	O
is	O
more	O
relevant	O
than	O
the	O
unconditional	O
information	B
nio	O
and	O
it	O
is	O
usually	O
easier	O
to	O
calculate	O
once	O
o	O
has	O
been	O
found	O
i	O
x	O
is	O
obtained	O
by	O
numerical	O
second	O
differentiation	O
unlike	O
i	O
no	O
probability	O
calculations	O
are	O
required	O
there	O
is	O
a	O
strong	O
bayesian	B
current	O
flowing	O
here	O
a	O
narrow	O
peak	O
for	O
the	O
log	O
likelihood	B
function	B
i	O
e	O
a	O
large	O
value	O
of	O
i	O
x	O
also	O
implies	O
a	O
narrow	O
posterior	B
distribution	B
for	O
given	O
x	O
conditional	B
inference	B
of	O
which	O
figure	O
is	O
an	O
evocative	O
example	O
helps	O
counter	O
the	O
central	O
bayesian	B
criticism	O
of	O
frequentist	B
inference	B
that	O
the	O
frequentist	B
properties	O
relate	O
to	O
data	B
sets	O
possibly	O
much	O
different	O
than	O
the	O
one	O
actually	O
observed	O
the	O
maximum	O
fisherian	B
inference	B
and	O
mle	B
figure	O
conditional	B
variance	B
of	O
mle	B
for	O
cauchy	O
samples	O
of	O
size	O
plotted	O
versus	O
the	O
observed	O
information	B
bound	B
observed	O
information	B
bounds	O
are	O
grouped	O
by	O
quantile	O
intervals	B
for	O
variance	B
calculations	O
percentages	O
the	O
broken	O
red	O
horizontal	O
line	O
is	O
the	O
unconditional	O
variance	B
likelihood	B
algorithm	B
can	O
be	O
interpreted	O
both	O
vertically	O
and	O
horizontally	O
in	O
figure	O
acting	O
as	O
a	O
connection	O
between	O
the	O
bayesian	B
and	O
frequentist	B
worlds	O
the	O
equivalent	O
of	O
result	O
for	O
multiparameter	O
families	O
section	O
i	O
x	O
plays	O
an	O
important	O
role	O
in	O
succeeding	O
chapters	O
with	O
the	O
matrix	B
of	O
second	O
derivatives	O
np	O
d	O
i	O
x	O
d	O
log	O
information	B
boundmle	O
variance	B
permutation	O
and	O
randomization	B
permutation	O
and	O
randomization	B
fisherian	O
methodology	O
faced	O
criticism	O
for	O
its	O
overdependence	O
on	O
normal	B
sampling	O
assumptions	O
consider	O
the	O
comparison	O
between	O
the	O
all	O
and	O
aml	O
patients	O
in	O
the	O
gene	O
leukemia	B
example	O
of	O
figure	O
the	O
twosample	O
t-statistic	B
had	O
value	O
with	O
two-sided	O
significance	O
level	O
according	O
to	O
a	O
student-t	O
null	O
distribution	B
with	O
degrees	B
of	I
freedom	I
all	O
of	O
this	O
depended	O
on	O
the	O
gaussian	B
or	O
normal	B
assumptions	O
as	O
an	O
alternative	O
significance-level	O
calculation	O
fisher	B
suggested	O
using	O
permutations	O
of	O
the	O
data	B
points	O
the	O
values	O
are	O
randomly	O
divided	O
into	O
disjoint	O
sets	O
of	O
size	O
and	O
and	O
the	O
two-sample	B
t-statistic	B
is	O
recomputed	O
this	O
is	O
done	O
some	O
large	O
number	O
b	O
times	O
yielding	O
b	O
the	O
two-sided	O
permutation	O
significance	O
level	O
tion	O
t-values	O
t	B
for	O
the	O
original	O
value	O
t	B
is	O
then	O
the	O
proportion	B
of	O
the	O
t	B
i	O
values	O
exceeding	O
t	B
in	O
absolute	O
value	O
t	B
t	B
j	O
jtjg	O
i	O
figure	O
permutation	O
t	B
for	O
gene	O
in	O
the	O
leukemia	B
data	B
of	O
figure	O
of	O
these	O
ticks	O
exceeded	O
in	O
absolute	O
value	O
the	O
observed	O
t	B
t-statistic	B
giving	O
permutation	O
significance	O
level	O
for	O
testing	B
all	O
vs	O
aml	O
valuesfrequency	O
statistic	B
fisherian	B
inference	B
and	O
mle	B
figure	O
shows	O
the	O
histogram	O
of	O
b	O
d	O
t	B
i	O
values	O
for	O
the	O
gene	O
data	B
in	O
figure	O
of	O
these	O
exceeded	O
t	B
d	O
in	O
absolute	O
value	O
yielding	O
significance	O
level	O
against	O
the	O
null	O
hypothesis	O
of	O
no	O
allaml	O
difference	O
remarkably	O
close	O
to	O
the	O
normal-theory	O
significance	O
level	O
were	O
a	O
little	O
lucky	O
here	O
why	O
should	O
we	O
believe	O
the	O
permutation	O
significance	O
level	O
fisher	B
provided	O
two	O
arguments	O
suppose	O
we	O
assume	O
as	O
a	O
null	O
hypothesis	O
that	O
the	O
n	O
d	O
observed	O
measurements	O
x	O
are	O
an	O
iid	O
sample	B
obtained	O
from	O
the	O
same	O
distribution	B
xi	O
for	O
i	O
d	O
n	O
is	O
no	O
normal	B
assumption	O
here	O
say	O
that	O
is	O
n	O
let	O
o	O
indicate	O
the	O
order	O
statistic	B
of	O
x	O
i	O
e	O
the	O
numbers	O
ordered	O
from	O
smallest	O
to	O
largest	O
with	O
their	O
aml	O
or	O
all	O
labels	O
removed	O
then	O
it	O
can	O
be	O
shown	O
that	O
all	O
ways	O
of	O
obtaining	O
x	O
by	O
dividing	O
o	O
into	O
disjoint	O
subsets	O
of	O
sizes	O
and	O
are	O
equally	O
likely	O
under	O
null	O
hypothesis	O
a	O
small	O
value	O
of	O
the	O
permutation	O
significance	O
level	O
indicates	O
that	O
the	O
actual	O
division	O
of	O
amlall	O
measurements	O
was	O
not	O
random	O
but	O
rather	O
resulted	O
from	O
negation	O
of	O
the	O
null	O
hypothesis	O
this	O
might	O
be	O
considered	O
an	O
example	O
of	O
fisher	B
s	O
logic	B
of	I
inductive	B
inference	B
where	O
the	O
conclusion	O
should	O
be	O
obvious	O
to	O
all	O
it	O
is	O
certainly	O
an	O
example	O
of	O
conditional	B
inference	B
now	O
with	O
conditioning	O
used	O
to	O
avoid	O
specific	O
assumptions	O
about	O
the	O
sampling	O
density	B
in	O
experimental	O
situations	O
fisher	B
forcefully	O
argued	O
for	O
randomization	B
that	O
is	O
for	O
randomly	O
assigning	O
the	O
experimental	O
units	O
to	O
the	O
possible	O
treatment	O
groups	O
most	O
famously	O
in	O
a	O
clinical	O
trial	O
comparing	O
drug	O
a	O
with	O
drug	O
b	O
each	O
patient	O
should	O
be	O
randomly	O
assigned	O
to	O
a	O
or	O
b	O
randomization	B
greatly	O
strengthens	O
the	O
conclusions	O
of	O
a	O
permutation	B
test	I
in	O
the	O
amlall	O
situation	O
where	O
randomization	B
wasn	O
t	B
feasible	O
we	O
wind	O
up	O
almost	O
certain	O
that	O
the	O
aml	O
group	O
has	O
systematically	O
larger	O
numbers	O
but	O
cannot	O
be	O
certain	O
that	O
it	O
is	O
the	O
different	O
disease	O
states	O
causing	O
the	O
difference	O
perhaps	O
the	O
aml	O
patients	O
are	O
older	O
or	O
heavier	O
or	O
have	O
more	O
of	O
some	O
other	O
characteristic	O
affecting	O
gene	O
experimental	O
randomization	B
almost	O
guarantees	O
that	O
age	O
weight	O
etc	O
will	O
be	O
wellbalanced	O
between	O
the	O
treatment	O
groups	O
fisher	B
s	O
rct	O
clinical	O
trial	O
was	O
and	O
is	O
the	O
gold	O
standard	O
for	O
statistical	O
inference	B
in	O
medical	O
trials	O
permutation	O
testing	B
is	O
frequentistic	O
a	O
statistician	O
following	O
the	O
procedure	O
has	O
chance	O
of	O
rejecting	O
a	O
valid	O
null	O
hypothesis	O
at	O
level	O
etc	O
notes	O
and	O
details	O
randomization	B
inference	B
is	O
somewhat	O
different	O
amounting	O
to	O
a	O
kind	O
of	O
forced	O
frequentism	B
with	O
the	O
statistician	O
imposing	O
his	O
or	O
her	O
preferred	O
probability	O
mechanism	O
upon	O
the	O
data	B
permutation	O
methods	O
are	O
enjoying	O
a	O
healthy	O
computer-age	O
revival	O
in	O
contexts	O
far	O
beyond	O
fisher	B
s	O
original	O
justification	O
for	O
the	O
t-test	O
as	O
we	O
will	O
see	O
in	O
chapter	O
notes	O
and	O
details	O
on	O
a	O
linear	B
scale	B
that	O
puts	O
bayesian	B
on	O
the	O
left	O
and	O
frequentist	B
on	O
the	O
right	O
fisherian	B
inference	B
winds	O
up	O
somewhere	O
in	O
the	O
middle	O
fisher	B
rejected	O
bayesianism	B
early	O
on	O
but	O
later	O
criticized	O
as	O
wooden	O
the	O
hard-line	O
frequentism	B
of	O
the	O
neyman	O
wald	O
decision-theoretic	O
school	O
efron	O
locates	O
fisher	B
along	O
the	O
bayes	O
frequentist	B
scale	B
for	O
several	O
different	O
criteria	B
see	O
in	O
particular	O
figure	O
of	O
that	O
paper	O
bayesians	O
of	O
course	O
believe	O
there	O
is	O
only	O
one	O
true	O
logic	B
of	I
inductive	B
inference	B
fisher	B
disagreed	O
his	O
most	O
ambitious	O
attempt	O
to	O
enjoy	O
the	O
bayesian	B
omelette	O
without	O
breaking	O
the	O
bayesian	B
eggs	O
was	O
fiducial	B
inference	B
the	O
simplest	O
example	O
concerns	O
the	O
normal	B
translation	O
model	O
x	O
n	O
where	O
x	O
has	O
a	O
standard	O
n	O
distribution	B
the	O
fiducial	B
distribution	B
of	O
given	O
x	O
then	O
being	O
n	O
among	O
fisher	B
s	O
many	O
contributions	O
fiducial	B
inference	B
was	O
the	O
only	O
outright	O
popular	O
bust	O
nevertheless	O
the	O
idea	O
has	O
popped	O
up	O
again	O
in	O
the	O
current	O
literature	O
under	O
the	O
name	O
confidence	O
distribution	B
see	O
efron	O
and	O
xie	O
and	O
singh	O
a	O
brief	O
discussion	O
appears	O
in	O
chapter	O
for	O
an	O
unbiased	O
estimator	B
q	O
lx	O
d	O
x	O
dz	O
p	O
z	O
t	B
x	O
d	O
t	B
we	O
have	O
z	O
x	O
d	O
p	O
f	O
d	O
x	O
d	O
d	O
t	B
x	O
t	B
d	O
x	O
here	O
x	O
is	O
x	O
n	O
the	O
sample	B
space	O
of	O
x	O
d	O
xn	O
and	O
we	O
are	O
givesr	O
suming	O
the	O
conditions	B
necessary	O
for	O
differentiating	O
under	O
the	O
integral	O
sign	O
lx	O
has	O
expectation	O
p	O
lx	O
d	O
x	O
d	O
p	O
attributed	O
to	O
the	O
important	O
bayesian	B
theorist	O
l	O
j	O
savage	B
fisherian	B
inference	B
and	O
mle	B
and	O
then	O
applying	O
the	O
cauchy	O
schwarz	O
inequality	O
x	O
or	O
f	O
x	O
p	O
lx	O
d	O
x	O
x	O
o	O
n	O
q	O
i	O
x	O
p	O
lx	O
d	O
x	O
var	O
this	O
verifies	O
the	O
cram	O
er	O
rao	O
lower	O
bound	B
the	O
optimal	O
variance	B
for	O
an	O
unbiased	O
estimator	B
is	O
one	O
over	O
the	O
fisher	B
information	B
optimality	B
results	O
are	O
a	O
sign	O
of	O
scientific	O
maturity	O
fisher	B
information	B
and	O
its	O
estimation	B
bound	B
mark	O
the	O
transition	O
of	O
statistics	B
from	O
a	O
collection	O
of	O
ad-hoc	O
techniques	O
to	O
a	O
coherent	O
discipline	O
have	O
lost	O
some	O
ground	O
recently	O
where	O
as	O
discussed	O
in	O
chapter	O
ad-hoc	O
algorithmic	O
coinages	O
have	O
outrun	O
their	O
inferential	O
justification	O
fisher	B
s	O
information	B
bound	B
was	O
a	O
major	O
mathematical	O
innovation	O
closely	O
related	O
to	O
and	O
predating	O
heisenberg	O
s	O
uncertainty	O
principle	O
and	O
shannon	O
s	O
information	B
bound	B
see	O
dembo	O
et	O
al	O
unbiased	O
estimation	B
has	O
strong	O
appeal	O
in	O
statistical	O
applications	O
where	O
biased	O
its	O
opposite	O
carries	O
a	O
hint	O
of	O
self-interested	O
data	B
manipulation	O
in	O
large-scale	O
settings	O
such	O
as	O
the	O
prostate	B
study	O
of	O
figure	O
one	O
can	O
however	O
strongly	B
argue	O
for	O
biased	O
estimates	O
we	O
saw	O
this	O
for	O
gene	O
where	O
the	O
usual	O
unbiased	O
estimate	B
d	O
is	O
almost	O
certainly	O
too	O
large	O
biased	B
estimation	B
will	O
play	O
a	O
major	O
role	O
in	O
our	O
subsequent	O
chapters	O
maximum	B
likelihood	B
estimation	B
is	O
effectively	O
unbiased	O
in	O
most	O
situa	O
tions	O
under	O
repeated	O
sampling	O
the	O
expected	O
mean	O
squared	B
error	I
mse	O
d	O
e	O
has	O
order-of-magnitude	O
variance	B
d	O
and	O
d	O
the	O
latter	O
usually	O
becoming	O
negligible	O
as	O
sample	B
size	I
n	O
increases	O
exceptions	O
where	O
bias	B
is	O
substantial	O
can	O
occur	O
if	O
o	O
d	O
t	B
when	O
is	O
high-dimensional	O
as	O
in	O
the	O
james	O
stein	O
situation	O
of	O
chapter	O
section	O
of	O
efron	O
provides	O
a	O
detailed	O
analysis	B
section	O
of	O
cox	O
and	O
hinkley	O
gives	O
a	O
careful	O
and	O
wide-ranging	O
account	O
of	O
the	O
mle	B
and	O
fisher	B
information	B
lehmann	O
covers	O
the	O
same	O
ground	O
somewhat	O
more	O
technically	O
in	O
his	O
chapter	O
o	O
d	O
variance	B
c	O
parametric	B
models	B
and	O
exponential	O
families	O
we	O
have	O
been	O
reviewing	O
classic	O
approaches	O
to	O
statistical	O
inference	B
frequentist	B
bayesian	B
and	O
fisherian	O
with	O
an	O
eye	O
toward	O
examining	O
their	O
strengths	O
and	O
limitations	O
in	O
modern	O
applications	O
putting	O
philosophical	O
differences	O
aside	O
there	O
is	O
a	O
common	O
methodological	O
theme	O
in	O
classical	O
statistics	B
a	O
strong	O
preference	O
for	O
low-dimensional	O
parametric	B
models	B
that	O
is	O
for	O
modeling	O
data-analysis	O
problems	O
using	O
parametric	B
families	O
of	O
probability	O
densities	O
d	O
x	O
x	O
f	O
where	O
the	O
dimension	O
of	O
parameter	O
is	O
small	O
perhaps	O
no	O
greater	O
than	O
or	O
or	O
the	O
inverted	O
nomenclature	O
nonparametric	B
suggests	O
the	O
predominance	O
of	O
classical	O
parametric	B
methods	O
two	O
words	O
explain	O
the	O
classic	O
preference	O
for	O
parametric	B
models	B
mathematical	O
tractability	O
in	O
a	O
world	O
of	O
sliderules	O
and	O
slow	O
mechanical	O
arithmetic	O
mathematical	O
formulation	O
by	O
necessity	O
becomes	O
the	O
computational	O
tool	O
of	O
choice	O
our	O
new	O
computation-rich	O
environment	O
has	O
unplugged	O
the	O
mathematical	O
bottleneck	O
giving	O
us	O
a	O
more	O
realistic	O
flexible	O
and	O
far-reaching	O
body	O
of	O
statistical	O
techniques	O
but	O
the	O
classic	O
parametric	B
families	O
still	O
play	O
an	O
important	O
role	O
in	O
computer-age	O
statistics	B
often	O
assembled	O
as	O
small	O
parts	O
of	O
larger	O
methodologies	O
with	O
the	O
generalized	O
linear	B
models	B
of	O
chapter	O
this	O
presents	O
a	O
brief	O
review	O
of	O
the	O
most	O
widely	O
used	O
parametric	B
models	B
ending	O
with	O
an	O
overview	O
of	O
exponential	O
families	O
the	O
great	O
connecting	O
thread	O
of	O
classical	O
theory	B
and	O
a	O
player	O
of	O
continuing	O
importance	O
in	O
computer-age	O
applications	O
this	O
chapter	O
covers	O
a	O
large	O
amount	O
of	O
technical	O
material	O
for	O
use	O
later	O
and	O
may	O
be	O
reviewed	O
lightly	O
at	O
first	O
reading	O
parametric	B
models	B
univariate	O
families	O
univariate	O
parametric	B
families	O
in	O
which	O
the	O
sample	B
space	O
x	O
of	O
observation	O
x	O
is	O
a	O
subset	O
of	O
the	O
real	O
line	O
are	O
the	O
building	O
blocks	O
of	O
most	O
statistical	O
analyses	O
table	O
names	O
and	O
describes	O
the	O
five	O
most	O
familiar	O
univariate	O
families	O
normal	B
poisson	B
binomial	B
gamma	B
and	O
beta	B
chi-squared	O
distribution	B
with	O
n	O
degrees	B
of	I
freedom	I
n	O
is	O
also	O
included	O
since	O
it	O
is	O
dis	O
tributed	O
as	O
the	O
normal	B
distribution	B
n	O
is	O
a	O
shifted	O
and	O
scaled	O
version	O
of	O
the	O
n	O
used	O
in	O
n	O
c	O
table	O
five	O
familiar	O
univariate	O
densities	O
and	O
their	O
sample	B
spaces	O
x	O
parameter	O
spaces	O
and	O
expectations	O
and	O
variances	O
chi-squared	O
distribution	B
with	O
n	O
degrees	B
of	I
freedom	I
is	O
name	O
notation	O
expectation	O
variance	B
density	B
x	O
normal	B
n	O
poisson	B
binomial	B
bi	O
n	O
gamma	B
beta	B
p	O
e	O
e	O
x	O
x	O
n	O
ng	O
x	O
x	O
c	O
relationships	O
abound	O
among	O
the	O
table	O
s	O
families	O
for	O
instance	O
independent	O
gamma	B
variables	O
and	O
yield	O
a	O
beta	B
variate	O
according	O
to	O
c	O
the	O
binomial	B
and	O
poisson	B
are	O
particularly	O
close	O
cousins	O
a	O
bi	O
n	O
distribution	B
number	O
of	O
heads	O
in	O
n	O
independent	O
flips	O
of	O
a	O
coin	O
with	O
the	O
notation	O
in	O
indicates	O
that	O
if	O
x	O
n	O
and	O
y	O
n	O
then	O
x	O
and	O
c	O
have	O
the	O
same	O
distribution	B
the	O
multivariate	B
normal	B
distribution	B
figure	O
comparison	O
of	O
the	O
binomial	B
distribution	B
lines	O
with	O
the	O
poisson	B
dots	O
in	O
the	O
legend	O
we	O
show	O
the	O
mean	O
and	O
standard	B
deviation	I
for	O
each	O
distribution	B
ity	O
of	O
heads	O
approaches	O
a	O
distribution	B
bi	O
n	O
as	O
n	O
grows	O
large	O
and	O
small	O
the	O
notation	O
indicating	O
approximate	O
equality	O
of	O
the	O
two	O
distributions	O
figure	O
shows	O
the	O
approximation	O
already	O
working	O
quite	O
effectively	O
for	O
n	O
d	O
and	O
d	O
the	O
five	O
families	O
in	O
table	O
have	O
five	O
different	O
sample	B
spaces	O
making	O
them	O
appropriate	O
in	O
different	O
situations	O
beta	B
distributions	O
for	O
example	O
are	O
natural	O
candidates	O
for	O
modeling	O
continuous	O
data	B
on	O
the	O
unit	O
interval	B
choices	O
of	O
the	O
two	O
parameters	O
provide	O
a	O
variety	O
of	O
possible	O
shapes	O
as	O
illustrated	O
in	O
figure	O
later	O
we	O
will	O
discuss	O
general	O
exponential	O
families	O
unavailable	O
in	O
classical	O
theory	B
that	O
greatly	O
expand	O
the	O
catalog	O
of	O
possible	O
shapes	O
the	O
multivariate	B
normal	B
distribution	B
classical	O
statistics	B
produced	O
a	O
less	O
rich	O
catalog	O
of	O
multivariate	B
distributions	O
ones	O
where	O
the	O
sample	B
space	O
x	O
exists	O
in	O
rp	O
p-dimensional	O
eu	O
parametric	B
models	B
figure	O
three	O
beta	B
densities	O
with	O
indicated	O
clidean	O
space	O
p	O
by	O
far	O
the	O
greatest	O
amount	O
of	O
attention	O
focused	O
on	O
the	O
multivariate	B
normal	B
distribution	B
a	O
random	O
vector	B
x	O
d	O
xp	O
mean	O
vector	B
normally	O
distributed	O
or	O
not	O
has	O
d	O
efxg	O
and	O
p	O
p	O
covariance	O
d	O
e	O
of	O
vectors	O
u	O
and	O
v	O
is	O
the	O
matrix	B
having	O
elements	O
outer	O
product	O
uv	O
ui	O
vj	O
we	O
will	O
use	O
the	O
convenient	O
notation	O
x	O
for	O
and	O
reducing	O
to	O
the	O
familiar	O
form	B
x	O
in	O
the	O
univariate	O
case	O
denoting	O
the	O
entries	O
of	O
by	O
for	O
i	O
and	O
j	O
equaling	O
p	O
the	O
diagonal	O
elements	O
are	O
variances	O
i	O
d	O
var	O
xi	O
the	O
notation	O
d	O
defines	O
the	O
ij	O
th	O
element	O
of	O
a	O
matrix	B
the	O
multivariate	B
normal	B
distribution	B
the	O
off-diagonal	O
elements	O
relate	O
to	O
the	O
correlations	O
between	O
the	O
coordinates	O
of	O
x	O
cor	O
xi	O
xj	O
d	O
i	O
the	O
multivariate	B
normal	B
distribution	B
extends	O
the	O
univariate	O
definition	O
be	O
a	O
vector	B
n	O
in	O
table	O
to	O
begin	O
with	O
let	O
z	O
d	O
zp	O
of	O
p	O
independent	O
n	O
variates	O
with	O
probability	O
density	B
function	B
f	O
d	O
p	O
e	O
i	O
d	O
p	O
e	O
z	O
z	O
pp	O
according	O
to	O
line	O
of	O
table	O
the	O
multivariate	B
normal	B
family	O
is	O
obtained	O
by	O
linear	B
transformations	O
of	O
z	O
let	O
be	O
a	O
p-dimensional	O
vector	B
and	O
t	B
a	O
p	O
p	O
nonsingular	O
matrix	B
and	O
define	O
the	O
random	O
vector	B
following	O
the	O
usual	O
rules	O
of	O
probability	O
transformations	O
yields	O
the	O
density	B
of	O
x	O
d	O
where	O
is	O
the	O
p	O
p	O
symmetric	O
positive	O
definite	O
matrix	B
e	O
j	O
and	O
j	O
j	O
its	O
determinant	O
the	O
p-dimensional	O
multivariate	B
normal	B
distribution	B
with	O
mean	O
and	O
covariance	O
is	O
denoted	O
x	O
figure	O
illustrates	O
the	O
bivariate	B
normal	B
distribution	B
with	O
d	O
and	O
having	O
d	O
d	O
and	O
d	O
d	O
the	O
bell-shaped	O
mountain	O
on	O
the	O
left	O
is	O
a	O
plot	O
of	O
density	B
the	O
right	O
panel	O
shows	O
a	O
scatterplot	O
of	O
points	O
drawn	O
from	O
this	O
distribution	B
concentric	O
ellipses	O
illustrate	O
curves	O
of	O
constant	O
density	B
d	O
constant	O
classical	O
multivariate	B
analysis	B
was	O
the	O
study	O
of	O
the	O
multivariate	B
normal	B
distribution	B
both	O
of	O
its	O
probabilistic	O
and	O
statistical	O
properties	O
the	O
notes	O
reference	O
some	O
important	O
lengthy	O
multivariate	B
texts	O
here	O
we	O
will	O
just	O
recall	O
a	O
couple	O
of	O
results	O
useful	O
in	O
the	O
chapters	O
to	O
follow	O
x	O
d	O
c	O
t	B
z	O
d	O
t	B
t	B
parametric	B
models	B
figure	O
left	O
bivariate	B
normal	B
density	B
with	O
d	O
d	O
and	O
d	O
right	O
sample	B
of	O
pairs	O
from	O
this	O
bivariate	B
normal	B
density	B
is	O
partitioned	O
into	O
suppose	O
that	O
x	O
d	O
xp	O
d	O
and	O
d	O
c	O
d	O
p	O
with	O
and	O
similarly	O
partitioned	O
np	O
is	O
is	O
etc	O
then	O
the	O
conditional	B
distribution	B
of	O
given	O
is	O
itself	O
normal	B
if	O
d	O
d	O
then	O
reduces	O
to	O
c	O
n	O
here	O
is	O
familiar	O
as	O
the	O
linear	B
regression	B
coefficient	O
of	O
as	O
a	O
equals	O
the	O
squared	O
proportion	B
tion	O
of	O
while	O
of	O
the	O
variance	B
of	O
explained	O
by	O
hence	O
we	O
can	O
write	O
the	O
variance	B
term	O
in	O
as	O
bayesian	B
statistics	B
also	O
makes	O
good	O
use	O
of	O
the	O
normal	B
family	O
it	O
helps	O
to	O
begin	O
with	O
the	O
univariate	O
case	O
x	O
n	O
where	O
now	O
we	O
assume	O
that	O
c	O
fisher	B
s	O
information	B
bound	B
the	O
expectation	O
vector	B
itself	O
has	O
a	O
normal	B
prior	B
distribution	B
n	O
a	O
n	O
a	O
and	O
n	O
bayes	O
theorem	B
and	O
some	O
algebra	O
show	O
that	O
the	O
posterior	B
distribution	B
of	O
having	O
observed	O
x	O
is	O
normal	B
m	O
c	O
a	O
m	O
a	O
c	O
n	O
a	O
c	O
the	O
posterior	O
expectation	O
d	O
m	O
c	O
m	O
is	O
a	O
shrinkage	B
estimator	B
of	O
if	O
say	O
a	O
equals	O
then	O
d	O
m	O
c	O
m	O
is	O
shrunk	O
half	O
the	O
way	O
back	O
from	O
the	O
unbiased	O
estimate	B
d	O
x	O
toward	O
the	O
prior	B
mean	O
m	O
while	O
the	O
posterior	O
variance	B
of	O
is	O
only	O
one-half	O
that	O
of	O
the	O
multivariate	B
version	O
of	O
the	O
bayesian	B
setup	O
is	O
and	O
now	O
with	O
m	O
and	O
p-vectors	O
and	O
a	O
and	O
positive	O
definite	O
matrices	O
as	O
indicated	O
in	O
the	O
notes	O
the	O
posterior	B
distribution	B
of	O
given	O
x	O
is	O
then	O
np	O
m	O
a	O
c	O
a	O
a	O
c	O
np	O
which	O
reduces	O
to	O
when	O
p	O
d	O
m	O
a	O
a	O
c	O
fisher	B
s	O
information	B
bound	B
for	O
multiparameter	O
families	O
the	O
multivariate	B
normal	B
distribution	B
plays	O
its	O
biggest	O
role	O
in	O
applications	O
as	O
a	O
large-sample	O
approximation	O
for	O
maximum	B
likelihood	B
estimates	O
we	O
suppose	O
that	O
the	O
parametric	B
family	I
of	O
densities	O
normal	B
or	O
not	O
is	O
smoothly	O
defined	O
in	O
terms	O
of	O
its	O
p-dimensional	O
parameter	O
vector	B
terms	O
of	O
is	O
a	O
subset	O
of	O
rp	O
the	O
mle	B
definitions	O
and	O
results	O
are	O
direct	O
analogues	O
of	O
the	O
single-parameter	O
calculations	O
beginning	O
at	O
in	O
chapter	O
the	O
score	B
function	B
p	O
is	O
now	O
defined	O
as	O
the	O
gradient	O
of	O
log	O
d	O
np	O
log	O
o	O
d	O
d	O
the	O
p-vector	O
of	O
partial	O
derivatives	O
of	O
log	O
with	O
respect	O
to	O
the	O
coordinates	O
of	O
it	O
has	O
mean	O
zero	O
p	O
d	O
parametric	B
models	B
using	O
outer	O
product	O
notation	O
by	O
definition	O
the	O
fisher	B
information	B
matrix	B
i	O
for	O
is	O
the	O
p	O
p	O
covariance	O
matrix	B
of	O
p	O
np	O
i	O
d	O
the	O
key	O
result	O
is	O
that	O
the	O
mle	B
d	O
arg	O
has	O
an	O
approximately	O
normal	B
distribution	B
with	O
covariance	O
matrix	B
log	O
d	O
log	O
p	O
approximation	O
is	O
justified	O
by	O
large-sample	O
arguments	O
say	O
with	O
x	O
an	O
iid	O
sample	B
in	O
rp	O
xn	O
n	O
going	O
to	O
infinity	O
suppose	O
the	O
statistician	O
is	O
particularly	O
interested	O
in	O
the	O
first	O
coordinate	O
of	O
let	O
d	O
denote	O
the	O
other	O
p	O
coordinates	O
of	O
which	O
are	O
now	O
nuisance	B
parameters	I
as	O
far	O
as	O
the	O
estimation	B
of	O
goes	O
according	O
to	O
the	O
mle	B
which	O
is	O
the	O
first	O
coordinate	O
of	O
has	O
n	O
where	O
the	O
notation	O
indicates	O
the	O
upper	O
leftmost	O
entry	O
of	O
we	O
can	O
partition	O
the	O
information	B
matrix	B
i	O
into	O
the	O
two	O
parts	O
corre	O
sponding	O
to	O
and	O
i	O
d	O
i	O
of	O
dimension	O
and	O
i	O
i	O
d	O
the	O
endnotes	O
show	O
that	O
the	O
subtracted	O
term	O
on	O
the	O
right	O
side	O
of	O
is	O
nonnegative	O
implying	O
that	O
i	O
if	O
were	O
known	O
to	O
the	O
statistician	O
rather	O
than	O
requiring	O
estimation	B
then	O
would	O
be	O
a	O
one-parameter	B
family	O
with	O
fisher	B
information	B
for	O
estimating	O
giving	O
n	O
the	O
multinomial	O
distribution	B
comparing	O
with	O
shows	O
that	O
the	O
variance	B
of	O
the	O
mle	B
must	O
always	O
in	O
the	O
presence	O
of	O
nuisance	B
parameters	I
maximum	B
likelihood	B
and	O
in	O
fact	O
any	O
form	B
of	O
unbiased	O
or	O
nearly	O
unbiased	O
estimation	B
pays	O
a	O
nuisance	O
tax	O
for	O
the	O
presence	O
of	O
other	O
parameters	O
modern	O
applications	O
often	O
involve	O
thousands	O
of	O
others	O
think	O
of	O
regression	B
fits	O
with	O
too	O
many	O
predictors	B
in	O
some	O
circumstances	O
biased	B
estimation	B
methods	O
can	O
reverse	O
the	O
situation	O
using	O
the	O
others	O
to	O
actually	O
improve	O
estimation	B
of	O
a	O
target	O
parameter	O
see	O
chapter	O
on	O
empirical	B
bayes	I
techniques	O
and	O
chapter	O
on	O
regularized	O
regression	B
models	B
the	O
multinomial	O
distribution	B
second	O
in	O
the	O
small	O
catalog	O
of	O
well-known	O
classic	O
multivariate	B
distributions	O
is	O
the	O
multinomial	O
the	O
multinomial	O
applies	O
to	O
situations	O
in	O
which	O
the	O
observations	O
take	O
on	O
only	O
a	O
finite	O
number	O
of	O
discrete	O
values	O
say	O
l	O
of	O
them	O
the	O
ulcer	O
surgery	O
of	O
table	O
is	O
repeated	O
in	O
table	O
now	O
with	O
the	O
cells	O
labeled	O
and	O
here	O
there	O
are	O
l	O
d	O
possible	O
outcomes	O
for	O
each	O
patient	O
success	O
failure	O
success	O
failure	O
table	O
the	O
ulcer	O
study	O
of	O
table	O
now	O
with	O
the	O
cells	O
numbered	O
through	O
as	O
shown	O
success	O
failure	O
new	O
old	O
a	O
number	O
n	O
of	O
cases	O
has	O
been	O
observed	O
n	O
d	O
in	O
table	O
let	O
x	O
d	O
xl	O
be	O
the	O
vector	B
of	O
counts	O
for	O
the	O
l	O
possible	O
outcomes	O
xl	O
d	O
having	O
outcome	O
lg	O
for	O
the	O
ulcer	O
data	B
it	O
is	O
convenient	O
to	O
code	O
the	O
outcomes	O
x	O
d	O
in	O
terms	O
of	O
the	O
coordinate	O
vectors	O
el	O
of	O
length	O
l	O
el	O
d	O
with	O
a	O
in	O
the	O
lth	O
place	O
unless	O
is	O
a	O
vector	B
of	O
zeros	O
a	O
condition	B
that	O
amounts	O
to	O
approximate	O
independence	O
of	O
and	O
parametric	B
models	B
figure	O
the	O
simplex	B
is	O
an	O
equilateral	O
triangle	O
set	B
at	O
an	O
angle	O
to	O
the	O
coordinate	O
axes	O
in	O
the	O
multinomial	O
probability	O
model	O
assumes	O
that	O
the	O
n	O
cases	O
are	O
independent	O
of	O
each	O
other	O
with	O
each	O
case	O
having	O
probability	O
for	O
outcome	O
el	O
d	O
prfelg	O
l	O
d	O
l	O
indicate	O
the	O
vector	B
of	O
probabilities	B
the	O
count	O
vector	B
x	O
then	O
follows	O
the	O
multinomial	O
distribution	B
let	O
denoted	O
d	O
ly	O
n	O
xl	O
d	O
xl	O
l	O
n	O
observations	O
l	O
outcomes	O
probability	O
vector	B
the	O
parameter	B
space	I
for	O
is	O
the	O
simplex	B
sl	O
x	O
multl	O
n	O
lx	O
w	O
and	O
d	O
sl	O
d	O
figure	O
shows	O
an	O
equilateral	O
triangle	O
sitting	O
at	O
an	O
angle	O
to	O
the	O
coordinate	O
axes	O
and	O
the	O
midpoint	O
of	O
the	O
triangle	O
d	O
the	O
multinomial	O
distribution	B
corresponds	O
to	O
a	O
multinomial	O
distribution	B
putting	O
equal	O
probability	O
on	O
the	O
three	O
possible	O
outcomes	O
figure	O
sample	B
space	O
x	O
for	O
x	O
numbers	O
indicate	O
the	O
sample	B
space	O
x	O
for	O
x	O
is	O
the	O
subset	O
of	O
nsl	O
set	B
of	O
nonnegative	O
vectors	O
summing	O
to	O
n	O
having	O
integer	O
components	O
figure	O
illustrates	O
the	O
case	O
n	O
d	O
and	O
l	O
d	O
now	O
with	O
the	O
triangle	O
of	O
figure	O
multiplied	O
by	O
and	O
set	B
flat	O
on	O
the	O
page	O
the	O
point	O
indicates	O
x	O
d	O
with	O
probability	O
in	O
the	O
dichotomous	O
case	O
l	O
d	O
the	O
multinomial	O
distribution	B
reduces	O
to	O
the	O
binomial	B
with	O
equaling	O
in	O
line	O
of	O
table	O
and	O
equaling	O
n	O
x	O
the	O
mean	O
vector	B
and	O
covariance	O
matrix	B
of	O
multl	O
n	O
for	O
any	O
value	O
of	O
l	O
are	O
according	O
to	O
etc	O
x	O
is	O
the	O
diagonal	O
matrix	B
with	O
diagonal	O
elements	O
so	O
var	O
xl	O
d	O
and	O
covariance	O
xj	O
d	O
generalizes	O
the	O
binomial	B
mean	O
and	O
variance	B
there	O
is	O
a	O
useful	O
relationship	O
between	O
the	O
multinomial	O
distribution	B
and	O
the	O
poisson	B
suppose	O
sl	O
are	O
independent	O
poissons	O
having	O
possibly	O
different	O
parameters	O
sl	O
l	O
d	O
l	O
or	O
more	O
concisely	O
s	O
with	O
s	O
d	O
sl	O
and	O
d	O
the	O
independence	O
being	O
assumed	O
in	O
notation	O
then	O
the	O
conditional	B
distribution	B
of	O
s	O
parametric	B
models	B
given	O
the	O
sum	O
sc	O
dp	O
sl	O
is	O
multinomial	O
dp	O
sjsc	O
multl	O
sc	O
going	O
in	O
the	O
other	O
direction	O
suppose	O
n	O
poi	O
n	O
then	O
the	O
uncondi	O
tional	O
or	O
marginal	O
distribution	B
of	O
multl	O
n	O
is	O
poisson	B
if	O
n	O
poi	O
n	O
multl	O
n	O
calculations	O
involving	O
x	O
multl	O
n	O
are	O
sometimes	O
complicated	O
by	O
the	O
multinomial	O
s	O
correlations	O
the	O
approximation	O
x	O
removes	O
the	O
correlations	O
and	O
is	O
usually	O
quite	O
accurate	O
if	O
n	O
is	O
large	O
there	O
is	O
one	O
more	O
important	O
thing	O
to	O
say	O
about	O
the	O
multinomial	O
family	O
it	O
contains	O
all	O
distributions	O
on	O
a	O
sample	B
space	O
x	O
composed	O
of	O
l	O
discrete	O
categories	O
in	O
this	O
sense	O
it	O
is	O
a	O
model	O
for	O
nonparametric	B
inference	B
on	O
x	O
the	O
nonparametric	B
bootstrap	B
calculations	O
of	O
chapter	O
use	O
the	O
multinomial	O
in	O
this	O
way	O
nonparametrics	O
and	O
the	O
multinomial	O
have	O
played	O
a	O
larger	O
role	O
in	O
the	O
modern	O
environment	O
of	O
large	O
difficult	O
to	O
model	O
data	B
sets	O
exponential	O
families	O
classic	O
parametric	B
families	O
dominated	O
statistical	O
theory	B
and	O
practice	O
for	O
a	O
century	O
and	O
more	O
with	O
an	O
enormous	O
catalog	O
of	O
their	O
individual	O
properties	O
means	O
variances	O
tail	O
areas	O
etc	O
being	O
compiled	O
a	O
surprise	O
though	O
a	O
slowly	O
emerging	O
one	O
beginning	O
in	O
the	O
was	O
that	O
all	O
of	O
them	O
were	O
examples	O
of	O
a	O
powerful	O
general	O
construction	O
exponential	O
families	O
what	O
follows	O
here	O
is	O
a	O
brief	O
introduction	O
to	O
the	O
basic	O
theory	B
with	O
further	O
development	O
to	O
come	O
in	O
subsequent	O
chapters	O
to	O
begin	O
with	O
consider	O
the	O
poisson	B
family	O
line	O
of	O
table	O
the	O
ratio	O
of	O
poisson	B
densities	O
at	O
two	O
parameter	O
values	O
and	O
is	O
which	O
can	O
be	O
re-expressed	O
as	O
d	O
e	O
d	O
e	O
where	O
we	O
have	O
defined	O
d	O
and	O
d	O
looking	O
at	O
we	O
can	O
describe	O
the	O
poisson	B
family	O
in	O
three	O
steps	O
exponential	O
families	O
start	O
with	O
any	O
one	O
poisson	B
distribution	B
for	O
any	O
value	O
of	O
let	O
d	O
and	O
calculate	O
for	O
x	O
d	O
q	O
d	O
e	O
finally	O
divide	O
q	O
by	O
exp	O
to	O
get	O
the	O
poisson	B
density	B
in	O
other	O
words	O
we	O
tilt	O
with	O
the	O
exponential	O
factor	B
e	O
x	O
to	O
get	O
q	O
and	O
then	O
renormalize	O
q	O
to	O
sum	O
to	O
notice	O
that	O
gives	O
as	O
the	O
renormalizing	O
constant	O
since	O
e	O
d	O
e	O
figure	O
poisson	B
densities	O
for	O
d	O
heavy	O
green	O
curve	O
with	O
dots	O
for	O
d	O
figure	O
graphs	O
the	O
poisson	B
density	B
for	O
d	O
each	O
poisson	B
density	B
is	O
a	O
renormalized	O
exponential	O
tilt	O
of	O
any	O
other	O
poisson	B
density	B
so	O
for	O
instance	O
is	O
obtained	O
from	O
via	O
the	O
tilt	O
e	O
x	O
with	O
d	O
d	O
alternate	O
expressions	O
for	O
as	O
an	O
exponential	B
family	I
are	O
available	O
for	O
example	O
exp	O
x	O
where	O
d	O
log	O
d	O
exp	O
and	O
d	O
isn	O
t	B
necessary	O
for	O
to	O
be	O
a	O
member	O
of	O
the	O
family	O
parametric	B
models	B
the	O
poisson	B
is	O
a	O
one-parameter	B
exponential	B
family	I
in	O
that	O
and	O
x	O
in	O
expression	O
are	O
one-dimensional	O
a	O
p-parameter	B
exponential	B
family	I
has	O
the	O
form	B
f	O
d	O
e	O
where	O
and	O
y	O
are	O
p-vectors	O
and	O
a	O
is	O
contained	O
in	O
rp	O
here	O
is	O
the	O
canonical	O
or	O
natural	B
parameter	I
vector	B
and	O
y	O
d	O
t	B
is	O
the	O
sufficient	O
statistic	B
vector	B
the	O
normalizing	O
function	B
which	O
makes	O
f	O
integrate	O
sum	O
to	O
one	O
satisfies	O
for	O
a	O
e	O
dx	O
and	O
it	O
can	O
be	O
shown	O
that	O
the	O
parameter	B
space	I
a	O
for	O
which	O
the	O
integral	O
is	O
finite	O
is	O
a	O
convex	O
set	B
in	O
rp	O
as	O
an	O
example	O
the	O
gamma	B
family	O
on	O
line	O
of	O
table	O
is	O
a	O
two-parameter	O
exponential	B
family	I
with	O
and	O
y	O
d	O
t	B
given	O
by	O
e	O
dz	O
x	O
and	O
d	O
log	O
x	O
d	O
log	O
c	O
log	O
d	O
c	O
logf	O
the	O
parameter	B
space	I
a	O
is	O
f	O
and	O
why	O
are	O
we	O
interested	O
in	O
exponential	O
tilting	O
rather	O
than	O
some	O
other	O
transformational	O
form	B
the	O
answer	O
has	O
to	O
do	O
with	O
repeated	O
sampling	O
suppose	O
x	O
d	O
xn	O
is	O
an	O
iid	O
sample	B
from	O
a	O
p-parameter	B
exponential	B
family	I
then	O
letting	O
yi	O
d	O
t	B
denote	O
the	O
sufficient	O
vector	B
corresponding	O
to	O
xi	O
f	O
d	O
ny	O
e	O
d	O
en	O
where	O
ny	O
dpn	O
yi	O
this	O
is	O
still	O
a	O
p-parameter	B
exponential	B
family	I
now	O
with	O
natural	B
parameter	I
n	O
sufficient	O
statistic	B
ny	O
and	O
normalizer	O
n	O
no	O
matter	O
how	O
large	O
n	O
may	O
be	O
the	O
statistician	O
can	O
still	O
compress	O
all	O
the	O
inferential	O
information	B
into	O
a	O
p-dimensional	O
statistic	B
ny	O
only	O
exponential	O
families	O
enjoy	O
this	O
property	O
even	O
though	O
they	O
were	O
discovered	O
and	O
developed	O
in	O
quite	O
different	O
contexts	O
and	O
at	O
quite	O
different	O
times	O
all	O
of	O
the	O
distributions	O
discussed	O
in	O
this	O
exponential	O
families	O
chapter	O
exist	O
in	O
exponential	O
families	O
this	O
isn	O
t	B
quite	O
the	O
coincidence	O
it	O
seems	O
mathematical	O
tractability	O
was	O
the	O
prized	O
property	O
of	O
classic	O
parametric	B
distributions	O
and	O
tractability	O
was	O
greatly	O
facilitated	O
by	O
exponential	O
structure	B
even	O
if	O
that	O
structure	B
went	O
unrecognized	O
in	O
one-parameter	B
exponential	O
families	O
the	O
normalizer	O
is	O
also	O
known	O
as	O
the	O
cumulant	B
generating	I
function	B
derivatives	O
of	O
yield	O
the	O
cumulants	O
of	O
the	O
first	O
two	O
giving	O
the	O
mean	O
and	O
variance	B
p	O
d	O
e	O
fyg	O
r	B
d	O
var	O
fyg	O
and	O
similarly	O
in	O
p-parametric	O
families	O
and	O
p	O
d	O
j	O
d	O
e	O
fyg	O
r	B
d	O
d	O
cov	O
fyg	O
j	O
k	O
the	O
p-dimensional	O
expectation	B
parameter	I
denoted	O
d	O
e	O
fyg	O
is	O
a	O
one-to-one	O
function	B
of	O
the	O
natural	B
parameter	I
let	O
v	O
indicate	O
the	O
p	O
p	O
covariance	O
matrix	B
v	O
d	O
cov	O
then	O
the	O
p	O
p	O
derivative	O
matrix	B
of	O
with	O
respect	O
to	O
is	O
d	O
d	O
j	O
k	O
d	O
v	O
v	O
d	O
this	O
following	O
from	O
the	O
inverse	O
mapping	O
being	O
d	O
d	O
as	O
a	O
one-parameter	B
example	O
the	O
poisson	B
in	O
table	O
has	O
d	O
log	O
d	O
y	O
d	O
x	O
and	O
d	O
d	O
d	O
d	O
v	O
the	O
maximum	B
likelihood	B
estimate	B
for	O
the	O
expectation	B
parameter	I
is	O
simply	O
y	O
ny	O
under	O
repeated	O
sampling	O
which	O
makes	O
it	O
immediate	O
to	O
calculate	O
in	O
most	O
situations	O
less	O
immediate	O
is	O
the	O
mle	B
for	O
the	O
natural	B
parameter	I
the	O
one-to-one	O
mapping	O
d	O
p	O
has	O
inverse	O
d	O
p	O
so	O
o	O
d	O
p	O
the	O
simplified	O
dot	O
notation	O
leads	O
to	O
more	O
compact	O
expressions	O
p	O
d	O
d	O
and	O
r	B
d	O
d	O
parametric	B
models	B
e	O
g	O
o	O
d	O
log	O
y	O
for	O
the	O
poisson	B
the	O
trouble	O
is	O
that	O
p	O
is	O
usually	O
unavailable	O
in	O
closed	O
form	B
numerical	O
approximation	O
algorithms	O
are	O
necessary	O
to	O
calculate	O
o	O
in	O
most	O
cases	O
all	O
of	O
the	O
classic	O
exponential	O
families	O
have	O
closed-form	O
expressions	O
for	O
f	O
yielding	O
pleasant	O
formulas	O
for	O
the	O
mean	O
and	O
covariance	O
v	O
modern	O
computational	O
technology	O
allows	O
us	O
to	O
work	O
with	O
general	O
exponential	O
families	O
designed	O
for	O
specific	O
tasks	O
without	O
concern	O
for	O
mathematical	O
tractability	O
figure	O
a	O
seven-parameter	O
exponential	B
family	I
fit	O
to	O
the	O
gfr	O
data	B
of	O
figure	O
compared	O
with	O
gamma	B
fit	O
of	O
figure	O
as	O
an	O
example	O
we	O
again	O
consider	O
fitting	O
the	O
gfr	O
data	B
of	O
figure	O
for	O
our	O
exponential	B
family	I
of	O
possible	O
densities	O
we	O
take	O
and	O
sufficient	O
statistic	B
vector	B
y	O
x	O
d	O
y	O
in	O
can	O
represent	O
all	O
polynomials	O
in	O
x	O
the	O
gfr	O
so	O
at	O
power	O
gives	O
the	O
n	O
family	O
which	O
we	O
already	O
know	O
fits	O
poorly	O
from	O
figure	O
the	O
heavy	O
curve	O
in	O
figure	O
shows	O
the	O
mle	B
fit	O
fo	O
now	O
following	O
the	O
gfr	O
histogram	O
quite	O
closely	O
chapter	O
discusses	O
lindsey	O
s	O
method	B
a	O
simplified	O
algorithm	B
for	O
calculating	O
the	O
mle	B
o	O
any	O
intercept	O
in	O
the	O
polynomial	O
is	O
absorbed	O
into	O
the	O
term	O
in	O
family	O
notes	O
and	O
details	O
a	O
more	O
exotic	O
example	O
concerns	O
the	O
generation	O
of	O
random	O
graphs	O
on	O
a	O
fixed	O
set	B
of	O
n	O
nodes	B
each	O
possible	O
graph	O
has	O
a	O
certain	O
total	O
number	O
e	O
of	O
edges	O
and	O
t	B
of	O
triangles	O
a	O
popular	O
choice	O
for	O
generating	O
such	O
graphs	O
is	O
the	O
two-parameter	O
exponential	B
family	I
having	O
y	O
d	O
t	B
so	O
that	O
larger	O
values	O
of	O
and	O
yield	O
more	O
connections	O
notes	O
and	O
details	O
the	O
notion	O
of	O
sufficient	O
statistics	B
ones	O
that	O
contain	O
all	O
available	O
inferential	O
information	B
was	O
perhaps	O
fisher	B
s	O
happiest	O
contribution	O
to	O
the	O
classic	O
corpus	O
he	O
noticed	O
that	O
in	O
the	O
exponential	B
family	I
form	B
the	O
fact	O
that	O
the	O
parameter	O
interacts	O
with	O
the	O
data	B
x	O
only	O
through	O
the	O
factor	B
exp	O
y	O
makes	O
y	O
x	O
sufficient	O
for	O
estimating	O
in	O
a	O
trio	O
of	O
authors	O
working	O
independently	O
in	O
different	O
countries	O
pitman	O
darmois	O
and	O
koopmans	O
showed	O
that	O
exponential	O
families	O
are	O
the	O
only	O
ones	O
that	O
enjoy	O
fixed-dimensional	O
sufficient	O
statistics	B
under	O
repeated	O
independent	O
sampling	O
until	O
the	O
late	O
such	O
distributions	O
were	O
called	O
pitman	O
darmois	O
koopmans	O
families	O
the	O
long	O
name	O
suggesting	O
infrequent	O
usage	O
generalized	O
linear	B
models	B
chapter	O
show	O
the	O
continuing	O
impact	O
of	O
sufficiency	O
on	O
statistical	O
practice	O
peter	O
bickel	O
has	O
pointed	O
out	O
that	O
data	B
compression	O
a	O
lively	O
topic	O
in	O
areas	O
such	O
as	O
image	O
transmission	O
is	O
a	O
modern	O
less	O
stringent	O
version	O
of	O
sufficiency	O
our	O
only	O
nonexponential	O
family	O
so	O
far	O
was	O
the	O
cauchy	O
translational	O
model	O
efron	O
and	O
hinkley	O
analyze	O
the	O
cauchy	O
family	O
in	O
terms	O
of	O
curved	B
exponential	O
families	O
a	O
generalization	O
of	O
model	O
properties	O
of	O
classical	O
distributions	O
of	O
properties	O
and	O
lots	O
of	O
distributions	O
are	O
covered	O
in	O
johnson	O
and	O
kotz	O
s	O
invaluable	O
series	O
of	O
reference	O
books	O
two	O
classic	O
multivariate	B
analysis	B
texts	O
are	O
anderson	O
and	O
mardia	O
et	O
al	O
we	O
have	O
dzdx	O
d	O
t	B
formula	B
from	O
z	O
d	O
t	B
jt	O
p	O
and	O
d	O
f	O
so	O
follows	O
from	O
t	B
t	B
d	O
d	O
and	O
jt	O
j	O
d	O
j	O
t	B
t	B
d	O
formula	B
let	O
d	O
be	O
partitioned	O
as	O
in	O
then	O
direct	O
multiplication	O
showing	O
that	O
d	O
i	O
the	O
identity	O
matrix	B
if	O
is	O
parametric	B
models	B
symmetric	O
then	O
d	O
by	O
redefining	O
x	O
to	O
be	O
x	O
we	O
can	O
set	B
and	O
equal	O
to	O
zero	O
in	O
the	O
quadratic	O
form	B
in	O
the	O
exponent	O
of	O
is	O
d	O
x	O
x	O
c	O
x	O
but	O
using	O
this	O
matches	O
the	O
quadratic	O
form	B
from	O
c	O
except	O
for	O
an	O
added	O
term	O
that	O
does	O
not	O
involve	O
for	O
a	O
multivariate	B
normal	B
distribution	B
this	O
is	O
sufficient	O
to	O
show	O
that	O
the	O
conditional	B
distribution	B
of	O
given	O
is	O
indeed	O
formulas	O
and	O
suppose	O
that	O
the	O
continuous	O
univariate	O
random	O
variable	O
z	O
has	O
density	B
of	O
the	O
form	B
f	O
d	O
q	O
z	O
where	O
q	O
z	O
d	O
c	O
c	O
a	O
b	O
and	O
constants	O
a	O
then	O
by	O
completing	O
the	O
square	O
f	O
d	O
b	O
a	O
and	O
we	O
see	O
that	O
z	O
n	O
the	O
key	O
point	O
is	O
that	O
form	B
specifies	O
z	O
as	O
normal	B
with	O
mean	O
and	O
variance	B
uniquely	O
determined	O
by	O
a	O
and	O
b	O
the	O
multivariate	B
version	O
of	O
this	O
fact	O
was	O
used	O
in	O
the	O
derivation	O
of	O
formula	B
by	O
redefining	O
and	O
x	O
as	O
m	O
and	O
x	O
m	O
we	O
can	O
take	O
m	O
d	O
in	O
setting	O
b	O
d	O
a	O
a	O
c	O
density	B
for	O
is	O
of	O
form	B
with	O
d	O
c	O
but	O
bayes	O
rule	B
says	O
that	O
the	O
density	B
of	O
is	O
proportional	O
to	O
also	O
of	O
form	B
now	O
with	O
d	O
c	O
a	O
c	O
a	O
little	O
algebra	O
shows	O
that	O
the	O
quadratic	O
and	O
linear	B
coefficients	O
of	O
match	O
in	O
verifying	O
we	O
verify	O
the	O
multivariate	B
result	O
using	O
a	O
different	O
argument	B
the	O
vector	B
x	O
has	O
joint	O
distribution	B
n	O
a	O
a	O
a	O
c	O
m	O
notes	O
and	O
details	O
now	O
we	O
employ	O
and	O
a	O
little	O
manipulation	O
to	O
get	O
formula	B
this	O
is	O
the	O
matrix	B
identity	O
now	O
with	O
equaling	O
multivariate	B
gaussian	B
and	O
nuisance	B
parameters	I
the	O
cautionary	O
message	O
here	O
that	O
increasing	O
the	O
number	O
of	O
unknown	O
nuisance	B
parameters	I
decreases	O
the	O
accuracy	B
of	O
the	O
estimate	B
of	O
interest	O
can	O
be	O
stated	O
more	O
positively	O
if	O
some	O
nuisance	B
parameters	I
are	O
actually	O
known	O
then	O
the	O
mle	B
of	O
the	O
parameter	O
of	O
interest	O
becomes	O
more	O
accurate	O
suppose	O
for	O
example	O
we	O
wish	O
to	O
estimate	B
from	O
a	O
sample	B
of	O
size	O
n	O
in	O
a	O
bivariate	B
normal	B
model	O
x	O
the	O
mle	B
has	O
variance	B
in	O
notation	O
but	O
if	O
is	O
known	O
then	O
the	O
mle	B
of	O
becomes	O
p	O
with	O
variance	B
being	O
the	O
correlation	O
xi	O
where	O
the	O
xi	O
are	O
iid	O
observations	O
having	O
prfxi	O
d	O
eig	O
d	O
as	O
in	O
the	O
mean	O
and	O
covariance	O
of	O
each	O
xi	O
are	O
formula	B
x	O
d	O
pn	O
efxig	O
d	O
lx	O
el	O
d	O
g	O
dx	O
and	O
covfxig	O
d	O
efxi	O
x	O
g	O
efxigefx	O
i	O
i	O
el	O
e	O
l	O
d	O
formula	B
follows	O
from	O
efxg	O
dp	O
efxig	O
and	O
cov	O
x	O
dp	O
cov	O
xi	O
formula	B
the	O
densities	O
of	O
s	O
and	O
sc	O
dp	O
sl	O
are	O
d	O
ly	O
scc	O
e	O
l	O
the	O
conditional	B
density	B
of	O
s	O
given	O
sc	O
is	O
the	O
ratio	O
and	O
d	O
e	O
sc	O
ql	O
ly	O
sl	O
d	O
which	O
is	O
formula	B
and	O
the	O
convexity	O
of	O
a	O
suppose	O
and	O
are	O
any	O
two	O
points	O
in	O
a	O
i	O
e	O
values	O
of	O
having	O
the	O
integral	O
in	O
finite	O
for	O
any	O
value	O
of	O
c	O
in	O
the	O
interval	B
and	O
any	O
value	O
of	O
y	O
we	O
have	O
c	O
ce	O
e	O
c	O
ce	O
because	O
of	O
the	O
convexity	O
in	O
c	O
of	O
the	O
function	B
on	O
the	O
right	O
by	O
showing	O
that	O
its	O
second	O
derivative	O
is	O
positive	O
integrating	O
both	O
sides	O
of	O
y	O
parametric	B
models	B
over	O
x	O
with	O
respect	O
to	O
shows	O
that	O
the	O
integral	O
on	O
the	O
right	O
must	O
be	O
finite	O
that	O
is	O
c	O
c	O
c	O
is	O
in	O
a	O
verifying	O
a	O
s	O
convexity	O
formula	B
in	O
the	O
univariate	O
case	O
differentiating	O
both	O
sides	O
of	O
with	O
respect	O
to	O
gives	O
dividing	O
by	O
e	O
shows	O
that	O
p	O
d	O
e	O
fyg	O
differentiating	O
again	O
gives	O
ye	O
dxi	O
p	O
dz	O
r	B
c	O
p	O
e	O
dz	O
x	O
dx	O
x	O
or	O
successive	O
derivatives	O
of	O
yield	O
the	O
higher	O
cumulants	O
of	O
y	O
its	O
skewness	O
kurtosis	O
etc	O
mle	B
for	O
the	O
gradient	O
with	O
respect	O
to	O
of	O
log	O
f	O
is	O
r	B
d	O
e	O
e	O
d	O
var	O
fyg	O
y	O
d	O
y	O
p	O
d	O
y	O
e	O
fy	O
r	B
represents	O
a	O
hypothetical	O
realization	O
y	O
x	O
where	O
y	O
f	O
we	O
achieve	O
the	O
mle	B
o	O
at	O
ro	O
d	O
or	O
d	O
y	O
eo	O
fy	O
in	O
other	O
words	O
the	O
mle	B
o	O
is	O
the	O
value	O
of	O
that	O
makes	O
the	O
expectation	O
match	O
the	O
observed	O
y	O
thus	O
implies	O
that	O
the	O
mle	B
of	O
pae	O
fy	O
rameter	O
is	O
y	O
drawn	O
from	O
part	O
ii	O
early	B
computer-age	I
methods	O
empirical	B
bayes	I
the	O
constraints	O
of	O
slow	O
mechanical	O
computation	O
molded	O
classical	O
statistics	B
into	O
a	O
mathematically	O
ingenious	O
theory	B
of	O
sharply	O
delimited	O
scope	O
emerging	O
after	O
the	O
second	O
world	O
war	O
electronic	O
computation	O
loosened	O
the	O
computational	O
stranglehold	O
allowing	O
a	O
more	O
expansive	O
and	O
useful	O
statistical	O
methodology	O
some	O
revolutions	O
start	O
slowly	O
the	O
journals	O
of	O
the	O
continued	O
to	O
emphasize	O
classical	O
themes	O
pure	O
mathematical	O
development	O
typically	O
centered	O
around	O
the	O
normal	B
distribution	B
change	O
came	O
gradually	O
but	O
by	O
the	O
a	O
new	O
statistical	O
technology	O
computer	O
enabled	O
was	O
firmly	O
in	O
place	O
key	O
developments	O
from	O
this	O
period	O
are	O
described	O
in	O
the	O
next	O
several	O
chapters	O
the	O
ideas	O
for	O
the	O
most	O
part	O
would	O
not	O
startle	O
a	O
pre-war	O
statistician	O
but	O
their	O
computational	O
demands	O
factors	O
of	O
or	O
times	O
those	O
of	O
classical	O
methods	O
would	O
more	O
factors	O
of	O
a	O
thousand	O
lay	O
ahead	O
as	O
will	O
be	O
told	O
in	O
part	O
iii	O
the	O
story	O
of	O
statistics	B
in	O
the	O
twenty-first	O
century	O
empirical	B
bayes	I
methodology	O
this	O
chapter	O
s	O
topic	O
has	O
been	O
a	O
particularly	O
slow	O
developer	O
despite	O
an	O
early	O
start	O
in	O
the	O
the	O
roadblock	O
here	O
was	O
not	O
so	O
much	O
the	O
computational	O
demands	O
of	O
the	O
theory	B
as	O
a	O
lack	O
of	O
appropriate	O
data	B
sets	O
modern	O
scientific	O
equipment	O
now	O
provides	O
ample	O
grist	O
for	O
the	O
empirical	B
bayes	I
mill	O
as	O
will	O
be	O
illustrated	O
later	O
in	O
the	O
chapter	O
and	O
more	O
dramatically	O
in	O
chapters	O
robbins	O
formula	B
table	O
shows	O
one	O
year	O
of	O
claims	O
data	B
for	O
a	O
european	O
automobile	O
insurance	B
company	O
of	O
the	O
policy	O
holders	O
made	O
no	O
claims	O
during	O
the	O
year	O
made	O
a	O
single	O
claim	O
made	O
two	O
claims	O
each	O
etc	O
with	O
table	O
continuing	O
to	O
the	O
one	O
person	O
who	O
made	O
seven	O
claims	O
of	O
course	O
the	O
insurance	B
company	O
is	O
concerned	O
about	O
the	O
claims	O
each	O
policy	O
holder	O
will	O
make	O
in	O
the	O
next	O
year	O
bayes	O
formula	B
seems	O
promising	O
here	O
we	O
suppose	O
that	O
xk	O
the	O
number	O
empirical	B
bayes	I
table	O
counts	O
yx	O
of	O
number	O
of	O
claims	O
x	O
made	O
in	O
a	O
single	O
year	O
by	O
automobile	O
insurance	B
policy	O
holders	O
robbins	O
formula	B
estimates	O
the	O
number	O
of	O
claims	O
expected	O
in	O
a	O
succeeding	O
year	O
for	O
instance	O
for	O
a	O
customer	O
in	O
the	O
x	O
d	O
category	O
parametric	B
maximum	B
likelihood	B
analysis	B
based	O
on	O
a	O
gamma	B
prior	B
gives	O
less	O
noisy	O
estimates	O
claims	O
x	O
counts	O
yx	O
formula	B
gamma	B
mle	B
of	O
claims	O
to	O
be	O
made	O
in	O
a	O
single	O
year	O
by	O
policy	O
holder	O
k	O
follows	O
a	O
poisson	B
distribution	B
with	O
parameter	O
prfxk	O
d	O
xg	O
d	O
pk	O
d	O
e	O
x	O
for	O
x	O
d	O
is	O
the	O
expected	O
value	O
of	O
xk	O
a	O
good	O
customer	O
from	O
the	O
company	O
s	O
point	O
of	O
view	O
has	O
a	O
small	O
value	O
of	O
though	O
in	O
any	O
one	O
year	O
his	O
or	O
her	O
actual	O
number	O
of	O
accidents	O
xk	O
will	O
vary	O
randomly	O
according	O
to	O
probability	O
density	B
k	O
suppose	O
we	O
knew	O
the	O
prior	B
density	B
g	O
for	O
the	O
customers	O
values	O
then	O
bayes	O
rule	B
would	O
yield	O
efjxg	O
d	O
d	O
p	O
d	O
for	O
the	O
expected	O
value	O
of	O
of	O
a	O
customer	O
observed	O
to	O
make	O
x	O
claims	O
in	O
a	O
single	O
year	O
this	O
would	O
answer	O
the	O
insurance	B
company	O
s	O
question	O
of	O
what	O
number	O
of	O
claims	O
x	O
to	O
expect	O
the	O
next	O
year	O
from	O
the	O
same	O
customer	O
since	O
efjxg	O
is	O
also	O
efxjxg	O
being	O
the	O
expectation	O
of	O
x	O
formula	B
is	O
just	O
the	O
ticket	O
if	O
the	O
prior	B
g	O
is	O
known	O
to	O
the	O
company	O
but	O
what	O
if	O
it	O
is	O
not	O
a	O
clever	O
rewriting	O
of	O
provides	O
a	O
way	O
forward	O
using	O
becomes	O
r	B
r	B
efjxg	O
d	O
g	O
d	O
r	B
xx	O
g	O
d	O
r	B
c	O
g	O
d	O
d	O
c	O
xx	O
g	O
d	O
r	B
o	O
f	O
d	O
yxn	O
with	O
n	O
dp	O
f	O
c	O
o	O
o	O
f	O
d	O
version	O
of	O
robbins	O
formula	B
o	O
oefjxg	O
d	O
c	O
o	O
f	O
d	O
etc	O
this	O
yields	O
an	O
empirical	B
x	O
yx	O
the	O
total	O
count	O
the	O
marginal	B
density	B
of	O
x	O
integrating	O
p	O
over	O
the	O
prior	B
g	O
is	O
robbins	O
formula	B
f	O
dz	O
p	O
d	O
dz	O
h	O
i	O
xx	O
e	O
g	O
d	O
comparing	O
with	O
gives	O
robbins	O
formula	B
efjxg	O
d	O
c	O
c	O
the	O
surprising	O
and	O
gratifying	O
fact	O
is	O
that	O
even	O
with	O
no	O
knowledge	O
of	O
the	O
prior	B
density	B
g	O
the	O
insurance	B
company	O
can	O
estimate	B
efjxg	O
from	O
formula	B
the	O
obvious	O
estimate	B
of	O
the	O
marginal	B
density	B
f	O
is	O
the	O
proportion	B
of	O
total	O
counts	O
in	O
category	O
x	O
f	O
d	O
c	O
the	O
final	O
expression	O
not	O
requiring	O
n	O
table	O
gives	O
d	O
customers	O
who	O
made	O
zero	O
claims	O
in	O
one	O
year	O
had	O
expectation	O
of	O
a	O
claim	O
the	O
next	O
year	O
those	O
with	O
one	O
claim	O
had	O
expectation	O
and	O
so	O
on	O
robbins	O
formula	B
came	O
as	O
a	O
to	O
the	O
statistical	O
world	O
of	O
the	O
the	O
expectation	O
efkjxkg	O
for	O
a	O
single	O
customer	O
unavailable	O
without	O
the	O
prior	B
g	O
somehow	O
becomes	O
available	O
in	O
the	O
context	O
of	O
a	O
large	O
study	O
the	O
terminology	O
empirical	B
bayes	I
is	O
apt	O
here	O
bayesian	B
formula	B
for	O
a	O
single	O
subject	O
is	O
estimated	O
empirically	O
frequentistically	O
from	O
a	O
collection	O
of	O
similar	O
cases	O
the	O
crucial	O
point	O
and	O
the	O
surprise	O
is	O
that	O
large	O
data	B
sets	O
of	O
parallel	O
situations	O
carry	O
within	O
them	O
their	O
own	O
bayesian	B
information	B
large	O
parallel	O
data	B
sets	O
are	O
a	O
hallmark	O
of	O
twenty-first-century	O
scientific	O
investigation	O
promoting	O
the	O
popularity	O
of	O
empirical	B
bayes	I
methods	O
formula	B
goes	O
awry	O
at	O
the	O
right	O
end	O
of	O
table	O
where	O
it	O
is	O
destabilized	O
by	O
small	O
count	O
numbers	O
a	O
parametric	B
approach	O
gives	O
more	O
dependable	O
results	O
now	O
we	O
assume	O
that	O
the	O
prior	B
density	B
g	O
for	O
the	O
customers	O
values	O
has	O
a	O
gamma	B
form	B
g	O
d	O
for	O
but	O
with	O
parameters	O
and	O
unknown	O
estimates	O
are	O
obtained	O
by	O
perhaps	O
it	O
shouldn	O
t	B
have	O
estimation	B
methods	O
similar	O
to	O
were	O
familiar	O
in	O
the	O
actuarial	O
literature	O
empirical	B
bayes	I
maximum	B
likelihood	B
fitting	O
to	O
the	O
counts	O
yx	O
yielding	O
a	O
parametrically	O
estimated	O
marginal	B
density	B
o	O
f	O
d	O
or	O
equivalently	O
oyx	O
d	O
figure	O
auto	O
accident	O
data	B
logcounts	O
vs	O
claims	O
for	O
auto	O
insurance	B
policies	O
the	O
dashed	O
line	O
is	O
a	O
gamma	B
mle	B
fit	O
the	O
bottom	O
row	O
of	O
table	O
gives	O
parametric	B
estimates	O
d	O
c	O
oyx	O
which	O
are	O
seen	O
to	O
be	O
less	O
eccentric	O
for	O
large	O
x	O
figure	O
compares	O
the	O
log	O
scale	B
the	O
raw	O
counts	O
yx	O
with	O
their	O
parametric	B
cousins	O
oyx	O
the	O
missing-species	B
problem	I
the	O
very	O
first	O
empirical	B
bayes	I
success	O
story	O
related	O
to	O
the	O
butterfly	B
data	B
of	O
table	O
even	O
in	O
the	O
midst	O
of	O
world	O
war	O
ii	O
alexander	O
corbet	O
a	O
leading	O
naturalist	O
had	O
been	O
trapping	O
butterflies	O
for	O
two	O
years	O
in	O
malaysia	O
malaya	O
species	O
were	O
so	O
rare	O
that	O
he	O
had	O
trapped	O
only	O
one	O
specimen	O
each	O
species	O
had	O
been	O
trapped	O
twice	O
each	O
table	O
going	O
on	O
to	O
show	O
that	O
species	O
were	O
trapped	O
three	O
times	O
each	O
and	O
so	O
on	O
some	O
of	O
the	O
more	O
the	O
missing-species	B
problem	I
common	O
species	O
had	O
appeared	O
hundreds	O
of	O
times	O
each	O
but	O
of	O
course	O
corbet	O
was	O
interested	O
in	O
the	O
rarer	O
specimens	O
table	O
butterfly	B
data	B
number	O
y	O
of	O
species	O
seen	O
x	O
times	O
each	O
in	O
two	O
years	O
of	O
trapping	O
species	O
trapped	O
just	O
once	O
trapped	O
twice	O
each	O
etc	O
x	O
y	O
x	O
y	O
corbet	O
then	O
asked	O
a	O
seemingly	O
impossible	O
question	O
if	O
he	O
trapped	O
for	O
one	O
additional	O
year	O
how	O
many	O
new	O
species	O
would	O
he	O
expect	O
to	O
capture	O
the	O
question	O
relates	O
to	O
the	O
absent	O
entry	O
in	O
table	O
x	O
d	O
the	O
species	O
that	O
haven	O
t	B
been	O
seen	O
yet	O
do	O
we	O
really	O
have	O
any	O
evidence	O
at	O
all	O
for	O
answering	O
corbet	O
fortunately	O
he	O
asked	O
the	O
right	O
man	O
r	B
a	O
fisher	B
who	O
produced	O
a	O
surprisingly	O
satisfying	O
solution	O
for	O
the	O
missing-species	B
problem	I
suppose	O
there	O
are	O
s	O
species	O
in	O
all	O
seen	O
or	O
unseen	O
and	O
that	O
xk	O
the	O
number	O
of	O
times	O
species	O
k	O
is	O
trapped	O
in	O
one	O
time	O
follows	O
a	O
poisson	B
distribution	B
with	O
parameter	O
as	O
in	O
xk	O
poi	O
k	O
for	O
k	O
d	O
s	O
the	O
entries	O
in	O
table	O
are	O
yx	O
d	O
d	O
xg	O
for	O
x	O
d	O
the	O
number	O
of	O
species	O
trapped	O
exactly	O
x	O
times	O
each	O
now	O
consider	O
a	O
further	O
trapping	O
period	O
of	O
t	B
time	O
units	O
t	B
d	O
in	O
corbet	O
s	O
question	O
and	O
let	O
xk	O
t	B
be	O
the	O
number	O
of	O
times	O
species	O
k	O
is	O
trapped	O
in	O
the	O
new	O
period	O
fisher	B
s	O
key	O
assumption	O
is	O
that	O
xk	O
t	B
poi	O
kt	O
independently	O
of	O
xk	O
that	O
is	O
any	O
one	O
species	O
is	O
trapped	O
independently	O
over	O
at	O
a	O
rate	B
proportional	O
to	O
its	O
parameter	O
the	O
probability	O
that	O
species	O
k	O
is	O
not	O
seen	O
in	O
the	O
initial	O
trapping	O
period	O
one	O
time	O
unit	O
equals	O
two	O
years	O
in	O
corbet	O
s	O
situation	O
this	O
is	O
the	O
definition	O
of	O
a	O
poisson	B
process	O
but	O
is	O
seen	O
in	O
the	O
new	O
period	O
that	O
is	O
xk	O
d	O
and	O
xk	O
t	B
is	O
empirical	B
bayes	I
t	B
e	O
e	O
e	O
e	O
t	B
d	O
sx	O
z	O
e	O
t	B
t	B
so	O
that	O
e	O
t	B
the	O
expected	O
number	O
of	O
new	O
species	O
seen	O
in	O
the	O
new	O
trapping	O
period	O
is	O
it	O
is	O
convenient	O
to	O
write	O
as	O
an	O
integral	O
e	O
e	O
t	B
d	O
s	O
e	O
g	O
d	O
e	O
t	B
gives	O
z	O
expanding	O
e	O
e	O
t	B
d	O
s	O
where	O
g	O
is	O
the	O
empirical	B
density	B
putting	O
probability	O
on	O
each	O
of	O
the	O
values	O
we	O
will	O
think	O
of	O
g	O
as	O
a	O
continuous	O
prior	B
density	B
on	O
the	O
possible	O
values	O
t	B
t	B
c	O
t	B
g	O
d	O
ex	O
d	O
efyxg	O
d	O
sx	O
h	O
notice	O
that	O
the	O
expected	O
value	O
ex	O
of	O
yx	O
is	O
the	O
sum	O
of	O
the	O
probabilities	B
of	O
being	O
seen	O
exactly	O
x	O
times	O
in	O
the	O
initial	O
period	O
x	O
z	O
k	O
i	O
e	O
xx	O
e	O
d	O
s	O
g	O
d	O
comparing	O
with	O
provides	O
a	O
surprising	O
result	O
e	O
t	B
d	O
c	O
we	O
don	O
t	B
know	O
the	O
ex	O
values	O
but	O
as	O
in	O
robbins	O
formula	B
we	O
can	O
esti	O
mate	O
them	O
by	O
the	O
yx	O
values	O
yielding	O
an	O
answer	O
to	O
corbet	O
s	O
question	O
oe	O
t	B
d	O
c	O
corbet	O
specified	O
t	B
d	O
d	O
c	O
d	O
this	O
may	O
have	O
been	O
discouraging	O
there	O
were	O
no	O
new	O
trapping	O
results	O
reported	O
the	O
missing-species	B
problem	I
table	O
expectation	O
and	O
its	O
standard	B
error	I
for	O
the	O
number	O
of	O
new	O
species	O
captured	O
in	O
t	B
additional	O
fractional	O
units	O
of	O
trapping	O
time	O
t	B
e	O
t	B
bsd	O
t	B
formulas	O
and	O
do	O
not	O
require	O
the	O
butterflies	O
to	O
arrive	O
independently	O
if	O
we	O
are	O
willing	O
to	O
add	O
the	O
assumption	O
that	O
the	O
xk	O
s	O
are	O
mutually	O
independent	O
we	O
can	O
calculate	O
bsd	O
t	B
d	O
as	O
an	O
approximate	O
standard	B
error	I
for	O
oe	O
t	B
table	O
shows	O
oe	O
t	B
andbsd	O
t	B
for	O
t	B
d	O
in	O
particular	O
yxt	O
d	O
formula	B
becomes	O
unstable	O
for	O
t	B
this	O
is	O
our	O
price	O
for	O
substituting	O
the	O
nonparametric	B
estimates	O
yx	O
for	O
ex	O
in	O
fisher	B
actually	O
answered	O
corbet	O
using	O
a	O
parametric	B
empirical	B
bayes	I
model	O
in	O
which	O
the	O
prior	B
g	O
for	O
the	O
poisson	B
parameters	O
was	O
assumed	O
to	O
be	O
of	O
the	O
gamma	B
form	B
it	O
can	O
be	O
shown	O
that	O
then	O
e	O
t	B
is	O
given	O
by	O
e	O
t	B
d	O
c	O
t	B
where	O
d	O
c	O
taking	O
d	O
maximum	B
likelihood	B
estimation	B
gave	O
d	O
and	O
d	O
figure	O
shows	O
that	O
the	O
parametric	B
estimate	B
of	O
e	O
t	B
using	O
and	O
is	O
just	O
slightly	O
greater	O
than	O
the	O
nonparametric	B
estimate	B
over	O
the	O
range	O
t	B
fisher	B
s	O
parametric	B
estimate	B
however	O
gives	O
reasonable	O
results	O
for	O
t	B
d	O
for	O
instance	O
for	O
a	O
future	O
trapping	O
period	O
of	O
units	O
years	O
reasonable	O
does	O
not	O
necessarily	O
mean	O
dependable	O
the	O
gamma	B
prior	B
is	O
a	O
mathematical	O
convenience	O
not	O
a	O
fact	O
of	O
nature	O
projections	O
into	O
the	O
far	O
future	O
fall	O
into	O
the	O
category	O
of	O
educated	O
guessing	O
the	O
missing-species	B
problem	I
encompasses	O
more	O
than	O
butterflies	O
there	O
are	O
words	O
in	O
total	O
in	O
the	O
recognized	O
shakespearean	O
canon	O
of	O
which	O
are	O
so	O
rare	O
they	O
appear	O
just	O
once	O
each	O
appear	O
twice	O
each	O
etc	O
empirical	B
bayes	I
figure	O
butterfly	B
data	B
expected	O
number	O
of	O
new	O
species	O
in	O
t	B
units	O
of	O
additional	O
trapping	O
time	O
nonparametric	B
fit	O
standard	B
deviation	I
gamma	B
model	O
table	O
shakespeare	O
s	O
word	O
counts	O
distinct	O
words	O
appeared	O
once	O
each	O
in	O
the	O
canon	O
distinct	O
words	O
twice	O
each	O
etc	O
the	O
canon	O
has	O
words	O
in	O
total	O
counting	O
repeats	O
as	O
in	O
table	O
which	O
goes	O
on	O
to	O
the	O
five	O
words	O
appearing	O
times	O
each	O
all	O
told	O
distinct	O
words	O
appear	O
those	O
that	O
appear	O
more	O
than	O
times	O
each	O
this	O
being	O
the	O
observed	O
size	O
of	O
shakespeare	O
s	O
vocabulary	O
but	O
what	O
of	O
the	O
words	O
shakespeare	O
knew	O
but	O
didn	O
t	B
use	O
these	O
are	O
the	O
missing	O
species	O
in	O
table	O
tet	O
gamma	B
model	O
the	O
missing-species	B
problem	I
suppose	O
another	O
quantity	B
of	O
previously	O
unknown	O
shakespeare	O
manuscripts	O
was	O
discovered	O
comprising	O
t	B
words	O
t	B
d	O
would	O
represent	O
a	O
new	O
canon	O
just	O
as	O
large	O
as	O
the	O
old	O
one	O
how	O
many	O
previously	O
unseen	O
distinct	O
words	O
would	O
we	O
expect	O
to	O
discover	O
employing	O
formulas	O
and	O
gives	O
for	O
the	O
expected	O
number	O
of	O
distinct	O
new	O
words	O
if	O
t	B
d	O
this	O
is	O
a	O
very	O
conservative	O
lower	O
bound	B
on	O
how	O
many	O
words	O
shakespeare	O
knew	O
but	O
didn	O
t	B
use	O
we	O
can	O
imagine	O
t	B
rising	O
toward	O
infinity	O
revealing	O
ever	O
more	O
unseen	O
vocabulary	O
formula	B
fails	O
for	O
t	B
and	O
fisher	B
s	O
gamma	B
assumption	O
is	O
just	O
that	O
but	O
more	O
elaborate	O
empirical	B
bayes	I
calculations	O
give	O
a	O
firm	O
lower	O
bound	B
of	O
on	O
shakespeare	O
s	O
unseen	O
vocabulary	O
exceeding	O
the	O
visible	O
portion	O
missing	O
mass	O
is	O
an	O
easier	O
version	O
of	O
the	O
missing-species	B
problem	I
in	O
which	O
we	O
only	O
ask	O
for	O
the	O
proportion	B
of	O
the	O
total	O
sum	O
of	O
values	O
corresponding	O
to	O
the	O
species	O
that	O
went	O
unseen	O
in	O
the	O
original	O
trapping	O
period	O
the	O
numerator	O
has	O
expectation	O
m	O
d	O
x	O
z	O
unseen	O
all	O
d	O
s	O
g	O
d	O
x	O
dx	O
all	O
x	O
as	O
in	O
while	O
the	O
expectation	O
of	O
the	O
denominator	O
is	O
efxsg	O
d	O
e	O
d	O
efng	O
xs	O
all	O
all	O
all	O
where	O
n	O
is	O
the	O
total	O
number	O
of	O
butterflies	O
trapped	O
the	O
obvious	O
missingmass	O
estimate	B
is	O
then	O
om	O
d	O
for	O
the	O
shakespeare	O
data	B
om	O
d	O
d	O
we	O
have	O
seen	O
most	O
of	O
shakespeare	O
s	O
vocabulary	O
as	O
weighted	O
by	O
his	O
usage	O
though	O
not	O
by	O
his	O
vocabulary	O
count	O
all	O
of	O
this	O
seems	O
to	O
live	O
in	O
the	O
rarefied	O
world	O
of	O
mathematical	O
abstraction	O
but	O
in	O
fact	O
some	O
previously	O
unknown	O
shakespearean	O
work	O
might	O
have	O
empirical	B
bayes	I
been	O
discovered	O
in	O
a	O
short	O
poem	O
shall	O
i	O
die	O
was	O
found	O
in	O
the	O
archives	O
of	O
the	O
bodleian	O
library	O
and	O
controversially	O
attributed	O
to	O
shakespeare	O
by	O
some	O
but	O
not	O
all	O
experts	O
the	O
poem	O
of	O
words	O
provided	O
a	O
new	O
trapping	O
period	O
of	O
length	O
only	O
t	B
d	O
d	O
and	O
a	O
prediction	O
from	O
of	O
eftg	O
d	O
new	O
species	O
i	O
e	O
distinct	O
words	O
not	O
appearing	O
in	O
the	O
canon	O
in	O
fact	O
there	O
were	O
nine	O
such	O
words	O
in	O
the	O
poem	O
similar	O
empirical	B
bayes	I
predictions	O
for	O
the	O
number	O
of	O
words	O
appearing	O
once	O
each	O
in	O
the	O
canon	O
twice	O
each	O
etc	O
showed	O
reasonable	O
agreement	O
with	O
the	O
poem	O
s	O
counts	O
but	O
not	O
enough	O
to	O
stifle	O
doubters	O
shall	O
i	O
die	O
is	O
currently	O
grouped	O
with	O
other	O
canonical	O
apocrypha	O
by	O
a	O
majority	O
of	O
experts	O
a	O
medical	O
example	O
the	O
reader	O
may	O
have	O
noticed	O
that	O
our	O
examples	O
so	O
far	O
have	O
not	O
been	O
particularly	O
computer	O
intensive	O
all	O
of	O
the	O
calculations	O
could	O
have	O
been	O
originally	O
were	O
done	O
by	O
this	O
section	O
discusses	O
a	O
medical	O
study	O
where	O
the	O
empirical	B
bayes	I
analysis	B
is	O
more	O
elaborate	O
cancer	O
surgery	O
sometimes	O
involves	O
the	O
removal	O
of	O
surrounding	O
lymph	O
nodes	B
as	O
well	O
as	O
the	O
primary	O
target	O
at	O
the	O
site	O
figure	O
concerns	O
n	O
d	O
surgeries	O
each	O
reporting	O
n	O
d	O
nodes	B
removed	O
and	O
x	O
d	O
nodes	B
found	O
positive	O
positive	O
meaning	O
malignant	O
the	O
ratios	O
pk	O
d	O
xknk	O
k	O
d	O
n	O
are	O
described	O
in	O
the	O
histogram	O
a	O
large	O
proportion	B
of	O
them	O
or	O
were	O
zero	O
the	O
remainder	O
spreading	O
unevenly	O
between	O
zero	O
and	O
one	O
the	O
denominators	O
nk	O
ranged	O
from	O
to	O
with	O
a	O
mean	O
of	O
and	O
standard	B
deviation	I
of	O
we	O
suppose	O
that	O
each	O
patient	O
has	O
some	O
true	O
probability	O
of	O
a	O
node	O
being	O
not	O
so	O
collecting	O
the	O
data	B
corbet	O
s	O
work	O
was	O
pre-computer	O
but	O
shakespeare	O
s	O
word	O
counts	O
were	O
done	O
electronically	O
twenty-first-century	O
scientific	O
technology	O
excels	O
at	O
the	O
production	O
of	O
the	O
large	O
parallel-structured	O
data	B
sets	O
conducive	O
to	O
empirical	B
bayes	I
analysis	B
a	O
medical	O
example	O
figure	O
nodes	B
study	O
ratio	O
p	O
d	O
xn	O
for	O
patients	O
n	O
d	O
number	O
of	O
nodes	B
removed	O
x	O
d	O
number	O
positive	O
positive	O
say	O
probability	O
for	O
patient	O
k	O
and	O
that	O
his	O
or	O
her	O
nodal	O
results	O
occur	O
independently	O
of	O
each	O
other	O
making	O
xk	O
binomial	B
xk	O
bi	O
nk	O
this	O
gives	O
pk	O
d	O
xknk	O
with	O
mean	O
and	O
variance	B
pk	O
so	O
that	O
is	O
estimated	O
more	O
accurately	O
when	O
nk	O
is	O
large	O
a	O
bayesian	B
analysis	B
would	O
begin	O
with	O
the	O
assumption	O
of	O
a	O
prior	B
density	B
g	O
for	O
the	O
values	O
g	O
for	O
k	O
d	O
n	O
d	O
we	O
don	O
t	B
know	O
g	O
but	O
the	O
parallel	O
nature	O
of	O
the	O
nodes	B
data	B
set	B
similar	O
cases	O
suggests	O
an	O
empirical	B
bayes	I
approach	O
as	O
a	O
first	O
try	O
for	O
the	O
nodes	B
study	O
we	O
assume	O
that	O
logfg	O
is	O
a	O
fourth-degree	O
polynomial	O
in	O
logfg	O
d	O
c	O
j	O
ji	O
p	O
g	O
is	O
determined	O
by	O
the	O
parameter	O
vector	B
d	O
since	O
given	O
can	O
be	O
calculated	O
from	O
the	O
requirement	O
that	O
empirical	B
bayes	I
z	O
c	O
g	O
d	O
d	O
dz	O
f	O
dz	O
xk	O
g	O
d	O
j	O
j	O
exp	O
d	O
nk	O
xk	O
for	O
a	O
given	O
choice	O
of	O
let	O
f	O
be	O
the	O
marginal	O
probability	O
of	O
the	O
observed	O
value	O
xk	O
for	O
patient	O
k	O
the	O
maximum	B
likelihood	B
estimate	B
of	O
is	O
the	O
maximizer	O
nx	O
o	O
d	O
arg	O
max	O
log	O
f	O
figure	O
estimated	O
prior	B
density	B
g	O
for	O
the	O
nodes	B
study	O
of	O
patients	O
have	O
have	O
figure	O
graphs	O
go	O
the	O
empirical	B
bayes	I
estimate	B
for	O
the	O
prior	B
distribution	B
of	O
the	O
values	O
the	O
huge	O
spike	O
at	O
zero	O
in	O
figure	O
is	O
now	O
reduced	O
prfk	O
d	O
compared	O
with	O
the	O
of	O
the	O
pk	O
values	O
sd	O
a	O
medical	O
example	O
z	O
go	O
d	O
d	O
compared	O
with	O
less	O
than	O
small	O
values	O
are	O
still	O
the	O
rule	B
though	O
for	O
instance	O
z	O
go	O
d	O
d	O
the	O
vertical	O
bars	O
in	O
figure	O
indicate	O
one	O
standard	B
error	I
for	O
the	O
estimation	B
of	O
g	O
the	O
curve	O
seems	O
to	O
have	O
been	O
estimated	O
very	O
accurately	O
at	O
least	O
if	O
we	O
assume	O
the	O
adequacy	O
of	O
model	O
chapter	O
describes	O
the	O
computations	B
involved	O
in	O
figure	O
the	O
posterior	B
distribution	B
of	O
given	O
xk	O
and	O
nk	O
is	O
estimated	O
according	O
to	O
bayes	O
rule	B
to	O
be	O
og	O
jxk	O
nk	O
d	O
go	O
xk	O
fo	O
nk	O
xk	O
with	O
fo	O
from	O
figure	O
empirical	B
bayes	I
posterior	O
densities	O
of	O
for	O
three	O
patients	O
given	O
x	O
d	O
number	O
of	O
positive	O
nodes	B
n	O
d	O
number	O
of	O
nodes	B
figure	O
graphs	O
og	O
jxk	O
nk	O
for	O
three	O
choices	O
of	O
nk	O
and	O
if	O
we	O
take	O
as	O
indicating	O
poor	O
prognosis	O
suggesting	O
more	O
aggressive	O
follow-up	O
therapy	O
then	O
the	O
first	O
patient	O
is	O
almost	O
surely	O
on	O
safe	O
ground	O
the	O
third	O
patient	O
almost	O
surely	O
needs	O
more	O
follow-up	O
therapy	O
and	O
the	O
situation	O
of	O
the	O
second	O
is	O
uncertain	O
x	O
empirical	B
bayes	I
indirect	B
evidence	I
a	O
good	O
definition	O
of	O
a	O
statistical	O
argument	B
is	O
one	O
in	O
which	O
many	O
small	O
pieces	O
of	O
evidence	O
often	O
contradictory	O
are	O
combined	O
to	O
produce	O
an	O
overall	O
conclusion	O
in	O
the	O
clinical	O
trial	O
of	O
a	O
new	O
drug	O
for	O
instance	O
we	O
don	O
t	B
expect	O
the	O
drug	O
to	O
cure	O
every	O
patient	O
or	O
the	O
placebo	O
to	O
always	O
fail	O
but	O
eventually	O
perhaps	O
we	O
will	O
obtain	O
convincing	O
evidence	O
of	O
the	O
new	O
drug	O
s	O
efficacy	O
the	O
clinical	O
trial	O
is	O
collecting	O
direct	O
statistical	O
evidence	O
in	O
which	O
each	O
subject	O
s	O
success	O
or	O
failure	O
bears	O
directly	O
upon	O
the	O
question	O
of	O
interest	O
direct	B
evidence	I
interpreted	O
by	O
frequentist	B
methods	O
was	O
the	O
dominant	O
mode	O
of	O
statistical	O
application	O
in	O
the	O
twentieth	O
century	O
being	O
strongly	B
connected	O
to	O
the	O
idea	O
of	O
scientific	O
objectivity	O
bayesian	B
inference	B
provides	O
a	O
theoretical	O
basis	O
for	O
incorporating	O
indirect	B
evidence	I
for	O
example	O
the	O
doctor	O
s	O
prior	B
experience	O
with	O
twin	O
sexes	O
in	O
section	O
the	O
assertion	O
of	O
a	O
prior	B
density	B
g	O
amounts	O
to	O
a	O
claim	O
for	O
the	O
relevance	B
of	O
past	O
data	B
to	O
the	O
case	O
at	O
hand	O
empirical	B
bayes	I
removes	O
the	O
bayes	O
scaffolding	O
in	O
place	O
of	O
a	O
reassuring	O
prior	B
g	O
the	O
statistician	O
must	O
put	O
his	O
or	O
her	O
faith	O
in	O
the	O
relevance	B
of	O
the	O
other	O
cases	O
in	O
a	O
large	O
data	B
set	B
to	O
the	O
case	O
of	O
direct	O
interest	O
for	O
the	O
second	O
patient	O
in	O
figure	O
the	O
direct	O
estimate	B
of	O
his	O
value	O
is	O
o	O
d	O
d	O
the	O
empirical	B
bayes	I
estimate	B
is	O
a	O
little	O
less	O
eb	O
dz	O
o	O
og	O
jxk	O
d	O
nk	O
d	O
d	O
a	O
small	O
difference	O
but	O
we	O
will	O
see	O
bigger	O
ones	O
in	O
succeeding	O
chapters	O
the	O
changes	O
in	O
twenty-first-century	O
statistics	B
have	O
largely	O
been	O
demand	O
driven	O
responding	O
to	O
the	O
massive	O
data	B
sets	O
enabled	O
by	O
modern	O
scientific	O
equipment	O
philosophically	O
as	O
opposed	O
to	O
methodologically	O
the	O
biggest	O
change	O
has	O
been	O
the	O
increased	O
acceptance	O
of	O
indirect	B
evidence	I
especially	O
as	O
seen	O
in	O
empirical	B
bayes	I
and	O
objective	O
uninformative	O
bayes	O
applications	O
false-discovery	O
rates	O
chapter	O
provide	O
a	O
particularly	O
striking	O
shift	O
from	O
direct	O
to	O
indirect	B
evidence	I
in	O
hypothesis	B
testing	B
indirect	B
evidence	I
in	O
estimation	B
is	O
the	O
subject	O
of	O
our	O
next	O
chapter	O
notes	O
and	O
details	O
robbins	O
introduced	O
the	O
term	O
empirical	B
bayes	I
as	O
well	O
as	O
rule	B
as	O
part	O
of	O
a	O
general	O
theory	B
of	O
empirical	B
bayes	I
estimation	B
was	O
also	O
the	O
publication	O
year	O
for	O
good	O
and	O
toulmin	O
s	O
solution	O
to	O
the	O
missingspecies	O
problem	O
good	O
went	O
out	O
of	O
his	O
way	O
to	O
credit	O
his	O
famous	O
bletchley	O
notes	O
and	O
details	O
colleague	O
alan	O
turing	O
for	O
some	O
of	O
the	O
ideas	O
the	O
auto	O
accident	O
data	B
is	O
taken	O
from	O
table	O
of	O
carlin	O
and	O
louis	O
who	O
provide	O
a	O
more	O
complete	O
discussion	O
empirical	B
bayes	I
estimates	O
such	O
as	O
in	O
do	O
not	O
depend	O
on	O
independence	O
among	O
the	O
species	O
but	O
accuracies	O
such	O
as	O
do	O
and	O
similarly	O
for	O
the	O
error	O
bars	O
in	O
figures	O
and	O
corbet	O
s	O
enormous	O
efforts	O
illustrate	O
the	O
difficulties	O
of	O
amassing	O
large	O
data	B
sets	O
in	O
pre-computer	O
times	O
dependable	O
data	B
is	O
still	O
hard	O
to	O
come	O
by	O
but	O
these	O
days	O
it	O
is	O
often	O
the	O
statistician	O
s	O
job	O
to	O
pry	O
it	O
out	O
of	O
enormous	O
databases	O
efron	O
and	O
thisted	O
apply	O
formula	B
to	O
the	O
shakespeare	O
word	O
counts	O
and	O
then	O
use	O
linear	B
programming	O
methods	O
to	O
bound	B
shakespeare	O
s	O
unseen	O
vocabulary	O
from	O
below	O
at	O
words	O
was	O
actually	O
less	O
wordy	O
than	O
his	O
contemporaries	O
marlow	O
and	O
donne	O
shall	O
i	O
die	O
the	O
possibly	O
shakespearean	O
poem	O
recovered	O
in	O
is	O
analyzed	O
by	O
a	O
variety	O
of	O
empirical	B
bayes	I
techniques	O
in	O
thisted	O
and	O
efron	O
comparisons	O
are	O
made	O
with	O
other	O
elizabethan	O
authors	O
none	O
of	O
whom	O
seem	O
likely	O
candidates	O
for	O
authorship	O
the	O
shakespeare	O
word	O
counts	O
are	O
from	O
spevack	O
s	O
concordance	O
first	O
concordance	O
was	O
compiled	O
by	O
hand	O
in	O
the	O
mid	O
listing	O
every	O
word	O
shakespeare	O
wrote	O
and	O
where	O
it	O
appeared	O
a	O
full	B
life	O
s	O
labor	O
the	O
nodes	B
example	O
figure	O
is	O
taken	O
from	O
gholami	O
et	O
al	O
formula	B
for	O
any	O
positive	O
numbers	O
c	O
and	O
d	O
we	O
have	O
so	O
combining	O
gamma	B
prior	B
with	O
poisson	B
density	B
gives	O
marginal	B
density	B
z	O
d	O
d	O
d	O
d	O
c	O
r	B
d	O
c	O
x	O
d	O
where	O
d	O
c	O
assuming	O
independence	O
among	O
the	O
counts	O
yx	O
is	O
exactly	O
true	O
if	O
the	O
customers	O
act	O
independently	O
of	O
each	O
other	O
and	O
n	O
the	O
total	O
number	O
of	O
them	O
is	O
itself	O
poisson	B
the	O
log	O
likelihood	B
function	B
for	O
the	O
accident	O
data	B
is	O
yx	O
xmaxx	O
here	O
xmax	O
is	O
some	O
notional	O
upper	O
bound	B
on	O
the	O
maximum	O
possible	O
number	O
empirical	B
bayes	I
of	O
accidents	O
for	O
a	O
single	O
customer	O
since	O
yx	O
d	O
for	O
x	O
the	O
choice	O
of	O
xmax	O
is	O
irrelevant	O
the	O
values	O
in	O
maximize	O
formula	B
if	O
n	O
dp	O
yx	O
the	O
total	O
number	O
trapped	O
is	O
assumed	O
to	O
be	O
poisson	B
and	O
if	O
the	O
n	O
observed	O
values	O
xk	O
are	O
mutually	O
independent	O
then	O
a	O
useful	O
property	O
of	O
the	O
poisson	B
distribution	B
implies	O
that	O
the	O
counts	O
yx	O
are	O
themselves	O
approximately	O
independent	O
poisson	B
variates	O
for	O
x	O
d	O
poi	O
ex	O
yx	O
in	O
notation	O
formula	B
and	O
varfyxg	O
d	O
ex	O
then	O
give	O
ext	O
n	O
oe	O
t	B
o	O
dx	O
var	O
substituting	O
yx	O
for	O
ex	O
produces	O
section	O
of	O
efron	O
shows	O
that	O
is	O
an	O
upper	O
bound	B
on	O
varf	O
oe	O
t	B
if	O
n	O
is	O
considered	O
fixed	O
rather	O
than	O
poisson	B
formula	B
combining	O
the	O
case	O
x	O
d	O
in	O
with	O
e	O
g	O
d	O
r	B
e	O
g	O
d	O
yields	O
e	O
t	B
d	O
substituting	O
the	O
gamma	B
prior	B
for	O
g	O
and	O
using	O
three	O
times	O
gives	O
formula	B
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
if	O
fisher	B
had	O
lived	O
in	O
the	O
era	O
of	O
apps	O
maximum	B
likelihood	B
estimation	B
might	O
have	O
made	O
him	O
a	O
billionaire	O
arguably	O
the	O
twentieth	O
century	O
s	O
most	O
influential	O
piece	O
of	O
applied	O
mathematics	O
maximum	B
likelihood	B
continues	O
to	O
be	O
a	O
prime	O
method	B
of	O
choice	O
in	O
the	O
statistician	O
s	O
toolkit	O
roughly	O
speaking	O
maximum	B
likelihood	B
provides	O
nearly	O
unbiased	O
estimates	O
of	O
nearly	O
minimum	O
variance	B
and	O
does	O
so	O
in	O
an	O
automatic	O
way	O
that	O
being	O
said	O
maximum	B
likelihood	B
estimation	B
has	O
shown	O
itself	O
to	O
be	O
an	O
inadequate	O
and	O
dangerous	O
tool	O
in	O
many	O
twenty-first-century	O
applications	O
again	O
speaking	O
roughly	O
unbiasedness	O
can	O
be	O
an	O
unaffordable	O
luxury	O
when	O
there	O
are	O
hundreds	O
or	O
thousands	O
of	O
parameters	O
to	O
estimate	B
at	O
the	O
same	O
time	O
the	O
james	O
stein	O
estimator	B
made	O
this	O
point	O
dramatically	O
in	O
and	O
made	O
it	O
in	O
the	O
context	O
of	O
just	O
a	O
few	O
unknown	O
parameters	O
not	O
hundreds	O
or	O
thousands	O
it	O
begins	O
the	O
story	O
of	O
shrinkage	B
estimation	B
in	O
which	O
deliberate	O
biases	O
are	O
introduced	O
to	O
improve	O
overall	O
performance	O
at	O
a	O
possible	O
danger	O
to	O
individual	O
estimates	O
chapters	O
and	O
will	O
carry	O
on	O
the	O
story	O
in	O
its	O
modern	O
implementations	O
the	O
james	O
stein	O
estimator	B
suppose	O
we	O
wish	O
to	O
estimate	B
a	O
single	O
parameter	O
from	O
observation	O
x	O
in	O
the	O
bayesian	B
situation	O
and	O
n	O
a	O
n	O
in	O
which	O
case	O
has	O
posterior	B
distribution	B
n	O
c	O
b	O
x	O
m	O
b	O
as	O
given	O
in	O
we	O
take	O
d	O
for	O
convenience	O
the	O
bayes	O
estimator	B
of	O
b	O
d	O
a	O
a	O
c	O
d	O
m	O
c	O
b	O
x	O
m	O
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
has	O
expected	O
squared	B
error	I
e	O
compared	O
with	O
for	O
the	O
mle	B
d	O
x	O
d	O
b	O
d	O
if	O
say	O
a	O
d	O
in	O
then	O
b	O
d	O
and	O
has	O
only	O
half	O
the	O
risk	O
of	O
the	O
mle	B
e	O
the	O
same	O
calculation	O
applies	O
to	O
a	O
situation	O
where	O
we	O
have	O
n	O
indepen	O
dent	O
versions	O
of	O
say	O
d	O
and	O
x	O
d	O
xn	O
with	O
and	O
n	O
a	O
independently	O
for	O
i	O
d	O
n	O
that	O
the	O
differ	O
from	O
each	O
other	O
and	O
that	O
this	O
situation	O
is	O
not	O
the	O
same	O
as	O
let	O
indicate	O
the	O
vector	B
of	O
individual	O
bayes	O
estimates	O
d	O
m	O
d	O
m	O
m	O
d	O
m	O
c	O
b	O
x	O
m	O
n	O
bayes	O
i	O
and	O
similarly	O
d	O
x	O
using	O
the	O
total	O
squared	B
error	I
risk	O
of	O
is	O
nx	O
d	O
e	O
d	O
n	O
compared	O
with	O
bayes	O
i	O
e	O
e	O
d	O
n	O
b	O
again	O
has	O
only	O
b	O
times	O
the	O
risk	O
of	O
this	O
is	O
fine	O
if	O
we	O
know	O
m	O
and	O
a	O
equivalently	O
m	O
and	O
b	O
in	O
if	O
not	O
we	O
might	O
try	O
to	O
estimate	B
them	O
from	O
x	O
d	O
xn	O
marginally	O
gives	O
then	O
om	O
d	O
nx	O
is	O
an	O
unbiased	O
estimate	B
of	O
m	O
moreover	O
xi	O
n	O
a	O
c	O
s	O
d	O
nx	O
ob	O
d	O
the	O
james	O
stein	O
estimator	B
unbiasedly	O
estimates	O
b	O
as	O
long	O
as	O
n	O
the	O
james	O
stein	O
estimator	B
is	O
the	O
plug-in	O
version	O
of	O
xi	O
om	O
d	O
om	O
c	O
ob	O
for	O
i	O
d	O
n	O
i	O
or	O
equivalently	O
d	O
om	O
c	O
ob	O
x	O
om	O
with	O
om	O
d	O
om	O
om	O
om	O
at	O
this	O
point	O
the	O
terminology	O
empirical	B
bayes	I
seems	O
especially	O
apt	O
bayesian	B
model	O
leads	O
to	O
the	O
bayes	O
estimator	B
which	O
itself	O
is	O
estimated	O
empirically	O
frequentistically	O
from	O
all	O
the	O
data	B
x	O
and	O
then	O
applied	O
to	O
the	O
individual	O
cases	O
of	O
course	O
cannot	O
perform	O
as	O
well	O
as	O
the	O
actual	O
bayes	O
rule	B
but	O
the	O
increased	O
risk	O
is	O
surprisingly	O
modest	O
the	O
expected	O
squared	O
risk	O
of	O
under	O
model	O
is	O
d	O
nb	O
c	O
b	O
e	O
if	O
say	O
n	O
d	O
and	O
a	O
d	O
then	O
equals	O
compared	O
with	O
true	O
bayes	O
risk	O
from	O
much	O
less	O
than	O
risk	O
for	O
a	O
defender	O
of	O
maximum	B
likelihood	B
might	O
respond	O
that	O
none	O
of	O
this	O
is	O
surprising	O
bayesian	B
model	O
specifies	O
the	O
parameters	O
to	O
be	O
clustered	O
more	O
or	O
less	O
closely	O
around	O
a	O
central	O
point	O
m	O
while	O
makes	O
no	O
such	O
assumption	O
and	O
cannot	O
be	O
expected	O
to	O
perform	O
as	O
well	O
wrong	O
removing	O
the	O
bayesian	B
assumptions	O
does	O
not	O
rescue	O
as	O
james	O
and	O
stein	O
proved	O
in	O
james	O
stein	O
theorem	B
suppose	O
that	O
independently	O
for	O
i	O
d	O
n	O
with	O
n	O
then	O
n	O
n	O
d	O
e	O
e	O
for	O
all	O
choices	O
of	O
rn	O
expectations	O
in	O
are	O
with	O
fixed	O
and	O
x	O
varying	O
according	O
to	O
in	O
the	O
language	O
of	O
decision	O
theory	B
equation	B
says	O
that	O
is	O
inadmissible	B
its	O
total	O
squared	B
error	I
risk	O
exceeds	O
that	O
of	O
no	O
matter	O
what	O
may	O
be	O
this	O
is	O
a	O
strong	O
frequentist	B
form	B
of	O
defeat	O
for	O
not	O
depending	O
on	O
bayesian	B
assumptions	O
the	O
james	O
stein	O
theorem	B
came	O
as	O
a	O
rude	O
shock	O
to	O
the	O
statistical	O
world	O
of	O
first	O
of	O
all	O
the	O
defeat	O
came	O
on	O
mle	B
s	O
home	O
field	O
normal	B
observations	O
with	O
squared	B
error	I
loss	O
fisher	B
s	O
logic	B
of	I
inductive	B
inference	B
chapter	O
claimed	O
that	O
d	O
x	O
was	O
the	O
obviously	O
correct	O
estimator	B
in	O
the	O
univariate	O
case	O
an	O
assumption	O
tacitly	O
carried	O
forward	O
to	O
multiparameter	O
linear	B
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
regression	B
problems	O
where	O
versions	O
of	O
were	O
predominant	O
there	O
are	O
still	O
some	O
good	O
reasons	O
for	O
sticking	O
with	O
in	O
low-dimensional	O
problems	O
as	O
discussed	O
in	O
section	O
but	O
shrinkage	B
estimation	B
as	O
exemplified	O
by	O
the	O
james	O
stein	O
rule	B
has	O
become	O
a	O
necessity	O
in	O
the	O
high-dimensional	O
situations	O
of	O
modern	O
practice	O
the	O
baseball	B
players	O
the	O
james	O
stein	O
theorem	B
doesn	O
t	B
say	O
by	O
how	O
much	O
beats	O
if	O
the	O
improvement	O
were	O
infinitesimal	O
nobody	O
except	O
theorists	O
would	O
be	O
interested	O
in	O
favorable	O
situations	O
the	O
gains	O
can	O
in	O
fact	O
be	O
substantial	O
as	O
suggested	O
by	O
one	O
such	O
situation	O
appears	O
in	O
table	O
the	O
batting	O
of	O
major	O
league	O
players	O
have	O
been	O
observed	O
over	O
the	O
season	O
the	O
column	O
labeled	O
mle	B
reports	O
the	O
player	O
s	O
observed	O
average	O
over	O
his	O
first	O
at	O
bats	O
truth	O
is	O
the	O
average	O
over	O
the	O
remainder	O
of	O
the	O
season	O
further	O
at	O
bats	O
on	O
average	O
we	O
would	O
like	O
to	O
predict	O
truth	O
from	O
the	O
early-season	O
observations	O
the	O
column	O
labeled	O
js	O
in	O
table	O
is	O
from	O
a	O
version	O
of	O
the	O
james	O
stein	O
estimator	B
applied	O
to	O
the	O
mle	B
numbers	O
we	O
suppose	O
that	O
each	O
player	O
s	O
mle	B
value	O
pi	O
batting	O
average	O
in	O
the	O
first	O
tries	O
is	O
a	O
binomial	B
proportion	B
pi	O
pi	O
here	O
pi	O
is	O
his	O
true	O
average	O
how	O
he	O
would	O
perform	O
over	O
an	O
infinite	O
number	O
of	O
tries	O
truthi	O
is	O
itself	O
a	O
binomial	B
proportion	B
taken	O
over	O
an	O
average	O
of	O
more	O
tries	O
per	O
player	O
at	O
this	O
point	O
there	O
are	O
two	O
ways	O
to	O
proceed	O
the	O
simplest	O
uses	O
a	O
normal	B
approximation	O
to	O
pi	O
is	O
the	O
binomial	B
variance	B
where	O
n	O
d	O
np	O
c	O
opjs	O
d	O
d	O
p	O
pi	O
i	O
with	O
np	O
d	O
the	O
average	O
of	O
the	O
pi	O
values	O
letting	O
xi	O
d	O
pi	O
applying	O
and	O
transforming	O
back	O
to	O
opjs	O
i	O
gives	O
james	O
stein	O
estimates	O
i	O
batting	O
average	O
d	O
hits	O
at	O
bats	O
that	O
is	O
the	O
success	O
rate	B
for	O
example	O
player	O
hits	O
successfully	O
times	O
in	O
his	O
first	O
tries	O
for	O
batting	O
average	O
d	O
this	O
data	B
is	O
based	O
on	O
major	O
league	O
performances	O
but	O
is	O
partly	O
artificial	O
see	O
the	O
endnotes	O
np	O
the	O
baseball	B
players	O
table	O
eighteen	O
baseball	B
players	O
mle	B
is	O
batting	O
average	O
in	O
first	O
at	O
bats	O
truth	O
is	O
average	O
in	O
remainder	O
of	O
season	O
james	O
stein	O
estimator	B
js	O
is	O
based	O
on	O
arcsin	B
transformation	I
of	O
mles	O
sum	O
of	O
squared	O
errors	B
for	O
predicting	O
truth	O
mle	B
js	O
player	O
mle	B
js	O
truth	O
x	O
npi	O
c	O
a	O
second	O
approach	O
begins	O
with	O
the	O
arcsin	B
transformation	I
xi	O
d	O
c	O
sin	O
n	O
d	O
labeled	O
x	O
in	O
table	O
a	O
classical	O
device	O
that	O
produces	O
approximate	O
normal	B
deviates	O
of	O
variance	B
n	O
c	O
xi	O
n	O
where	O
is	O
transformation	O
applied	O
to	O
truthi	O
using	O
gives	O
i	O
which	O
is	O
finally	O
inverted	O
back	O
to	O
the	O
binomial	B
scale	B
opjs	O
i	O
d	O
n	O
n	O
c	O
n	O
c	O
i	O
formulas	O
and	O
yielded	O
nearly	O
the	O
same	O
estimates	O
for	O
the	O
baseball	B
players	O
the	O
js	O
column	O
in	O
table	O
is	O
from	O
james	O
and	O
stein	O
s	O
theorem	B
requires	O
normality	O
but	O
the	O
james	O
stein	O
estimator	B
often	O
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
d	O
while	O
works	O
perfectly	O
well	O
in	O
less	O
ideal	O
situations	O
that	O
is	O
the	O
case	O
in	O
table	O
d	O
in	O
other	O
words	O
the	O
james	O
stein	O
estimator	B
reduced	O
total	O
predictive	O
squared	B
error	I
by	O
about	O
figure	O
eighteen	O
baseball	B
players	O
top	O
line	O
mle	B
middle	O
james	O
stein	O
bottom	O
true	O
values	O
only	O
points	O
are	O
visible	O
since	O
there	O
are	O
ties	O
the	O
james	O
stein	O
rule	B
describes	O
a	O
shrinkage	B
estimator	B
each	O
mle	B
value	O
xi	O
being	O
shrunk	O
by	O
factor	B
ob	O
toward	O
the	O
grand	O
mean	O
om	O
d	O
nx	O
ob	O
d	O
in	O
figure	O
illustrates	O
the	O
shrinking	O
process	O
for	O
the	O
baseball	B
players	O
to	O
see	O
why	O
shrinking	O
might	O
make	O
sense	O
let	O
us	O
return	O
to	O
the	O
original	O
bayes	O
model	O
and	O
take	O
m	O
d	O
for	O
simplicity	O
so	O
that	O
the	O
xi	O
are	O
marginally	O
n	O
a	O
c	O
even	O
though	O
each	O
xi	O
is	O
unbiased	O
for	O
its	O
nx	O
d	O
na	O
parameter	O
as	O
a	O
group	O
they	O
are	O
overdispersed	O
d	O
n	O
a	O
c	O
compared	O
with	O
e	O
e	O
i	O
nx	O
i	O
the	O
sum	O
of	O
squares	O
of	O
the	O
mles	O
exceeds	O
that	O
of	O
the	O
true	O
values	O
by	O
expected	O
amount	O
n	O
shrinkage	B
improves	O
group	O
estimation	B
by	O
removing	O
the	O
excess	O
llllllllllllllllllmlelllllllllllllllllljames	O
steinlllllllllllllllllltruebatting	O
averages	O
ridge	B
regression	B
bayes	O
i	O
bayes	O
i	O
nx	O
e	O
a	O
a	O
c	O
d	O
bxi	O
have	O
d	O
nb	O
c	O
d	O
na	O
in	O
fact	O
the	O
james	O
stein	O
rule	B
overshrinks	B
the	O
data	B
as	O
seen	O
in	O
the	O
bottom	O
two	O
lines	O
of	O
figure	O
a	O
property	O
it	O
inherits	O
from	O
the	O
underlying	O
bayes	O
model	O
the	O
bayes	O
estimates	O
i	O
d	O
na	O
by	O
factor	B
a	O
a	O
c	O
we	O
could	O
use	O
the	O
bxi	O
which	O
gives	O
the	O
correct	O
expected	O
sum	O
of	O
squares	O
na	O
but	O
a	O
larger	O
expected	O
sum	O
of	O
squared	O
estimation	B
errors	B
overshrinking	O
e	O
p	O
less	O
extreme	O
shrinking	O
rule	B
d	O
p	O
efp	O
pothesis	O
of	O
no	O
differences	O
among	O
the	O
values	O
gavep	O
pi	O
d	O
null	O
indicating	O
that	O
in	O
a	O
classical	O
sense	O
we	O
have	O
accepted	O
the	O
null	O
hy	O
the	O
most	O
extreme	O
shrinkage	B
rule	B
would	O
be	O
all	O
the	O
way	O
that	O
is	O
to	O
for	O
i	O
d	O
n	O
i	O
d	O
nx	O
for	O
the	O
baseball	B
data	B
the	O
james	O
stein	O
estimator	B
is	O
a	O
databased	O
rule	B
for	O
compromising	O
between	O
the	O
null	O
hypothesis	O
of	O
no	O
differences	O
and	O
the	O
mle	B
s	O
tacit	O
assumption	O
of	O
no	O
relationship	O
at	O
all	O
among	O
the	O
values	O
in	O
this	O
sense	O
it	O
blurs	O
the	O
classical	O
distinction	O
between	O
hypothesis	B
testing	B
and	O
estimation	B
ridge	B
regression	B
linear	B
regression	B
perhaps	O
the	O
most	O
widely	O
used	O
estimation	B
technique	O
is	O
based	O
on	O
a	O
version	O
of	O
in	O
the	O
usual	O
notation	O
we	O
observe	O
an	O
n-dimensional	O
vector	B
y	O
d	O
yn	O
from	O
the	O
linear	B
model	I
y	O
d	O
x	O
c	O
here	O
x	O
is	O
a	O
known	O
p	O
structure	B
matrix	B
is	O
an	O
unknown	O
p-dimensional	O
parameter	O
vector	B
while	O
the	O
noise	O
vector	B
d	O
has	O
its	O
components	O
uncorrelated	O
and	O
with	O
constant	O
variance	B
where	O
i	O
is	O
the	O
n	O
n	O
identity	O
matrix	B
often	O
is	O
assumed	O
to	O
be	O
multivariate	B
normal	B
but	O
that	O
is	O
not	O
required	O
for	O
most	O
of	O
what	O
follows	O
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
the	O
least	B
squares	I
estimate	B
o	O
early	O
is	O
the	O
minimizer	O
of	O
the	O
total	O
sum	O
of	O
squared	O
errors	B
going	O
back	O
to	O
gauss	O
and	O
legendre	O
in	O
the	O
ky	O
x	O
o	O
d	O
arg	O
min	O
it	O
is	O
given	O
by	O
o	O
d	O
s	O
where	O
s	O
is	O
the	O
p	O
p	O
inner	O
product	O
matrix	B
xi	O
s	O
d	O
x	O
y	O
o	O
is	O
unbiased	O
for	O
and	O
has	O
covariance	O
matrix	B
o	O
is	O
the	O
mle	B
of	O
before	O
a	O
great	O
deal	O
could	O
be	O
feasibly	O
in	O
the	O
normal	B
case	O
o	O
of	O
effort	O
went	O
into	O
designing	O
matrices	O
x	O
such	O
that	O
s	O
calculated	O
which	O
is	O
now	O
no	O
longer	O
a	O
concern	O
a	O
great	O
advantage	O
of	O
the	O
linear	B
model	I
is	O
that	O
it	O
reduces	O
the	O
number	O
of	O
unknown	O
parameters	O
to	O
p	O
p	O
c	O
including	O
no	O
matter	O
how	O
large	O
n	O
may	O
be	O
in	O
the	O
kidney	O
data	B
example	O
of	O
section	O
n	O
d	O
while	O
p	O
d	O
in	O
modern	O
applications	O
however	O
p	O
has	O
grown	O
larger	O
and	O
larger	O
sometimes	O
into	O
the	O
thousands	O
or	O
more	O
as	O
we	O
will	O
see	O
in	O
part	O
iii	O
causing	O
statisticians	O
again	O
to	O
confront	O
the	O
limitations	O
of	O
high-dimensional	O
unbiased	O
estimation	B
ridge	B
regression	B
is	O
a	O
shrinkage	B
method	B
designed	O
to	O
improve	O
the	O
estimation	B
of	O
in	O
linear	B
models	B
by	O
transformations	O
we	O
can	O
standardize	O
so	O
that	O
the	O
columns	O
of	O
x	O
each	O
have	O
mean	O
and	O
sum	O
of	O
squares	O
that	O
is	O
si	O
i	O
d	O
for	O
i	O
d	O
p	O
puts	O
the	O
regression	B
coefficients	O
p	O
on	O
comparable	O
scales	O
for	O
convenience	O
we	O
also	O
assume	O
ny	O
d	O
a	O
ridge	B
regression	B
estimate	B
o	O
is	O
defined	O
for	O
to	O
be	O
y	O
d	O
c	O
while	O
o	O
o	O
d	O
c	O
o	O
is	O
a	O
shrunken	O
version	O
of	O
o	O
the	O
bigger	O
the	O
more	O
d	O
o	O
extreme	O
the	O
shrinkage	B
o	O
equals	O
the	O
vector	B
of	O
zeros	O
ridge	B
regression	B
effects	O
can	O
be	O
quite	O
dramatic	O
as	O
an	O
example	O
consider	O
the	O
diabetes	B
data	B
partially	O
shown	O
in	O
table	O
in	O
which	O
prediction	O
variables	O
measured	O
at	O
baseline	O
age	O
sex	O
bmi	O
mass	O
index	O
map	B
arterial	O
blood	O
pressure	O
and	O
six	O
blood	O
serum	O
measurements	O
have	O
o	O
ridge	B
regression	B
table	O
first	O
of	O
n	O
d	O
patients	O
in	O
the	O
diabetes	B
study	O
we	O
wish	O
to	O
predict	O
disease	O
progression	O
at	O
one	O
year	O
prog	O
from	O
the	O
baseline	O
measurements	O
age	O
sex	O
glu	O
age	O
sex	O
bmi	O
map	B
tc	O
ldl	O
hdl	O
tch	O
ltg	O
glu	O
prog	O
been	O
obtained	O
for	O
n	O
d	O
patients	O
we	O
wish	O
to	O
use	O
the	O
variables	O
to	O
predict	O
prog	O
a	O
quantitative	O
assessment	O
of	O
disease	O
progression	O
one	O
year	O
after	O
baseline	O
in	O
this	O
case	O
x	O
is	O
the	O
matrix	B
of	O
standardized	O
predictor	B
variables	O
and	O
y	O
is	O
prog	O
with	O
its	O
mean	O
subtracted	O
off	O
figure	O
ridge	O
coefficient	O
trace	O
for	O
the	O
standardized	O
diabetes	B
data	B
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
table	O
ordinary	O
least	B
squares	I
estimate	B
o	O
compared	O
with	O
ridge	B
regression	B
estimate	B
o	O
with	O
d	O
the	O
columns	O
and	O
are	O
their	O
estimated	O
standard	O
errors	B
was	O
taken	O
to	O
be	O
the	O
usual	O
ols	O
estimate	B
based	O
on	O
model	O
o	O
o	O
age	O
sex	O
bmi	O
map	B
tc	O
ldl	O
hdl	O
tch	O
ltg	O
glu	O
figure	O
vertically	O
plots	O
the	O
coordinates	O
of	O
o	O
as	O
the	O
ridge	O
parameter	O
increases	O
from	O
to	O
four	O
of	O
the	O
coefficients	O
change	O
rapidly	O
at	O
first	O
table	O
compares	O
o	O
positive	O
coefficients	O
predict	O
increased	O
disease	O
progression	O
notice	O
that	O
ldl	O
the	O
bad	O
cholesterol	B
measurement	O
goes	O
from	O
being	O
a	O
strongly	B
positive	O
predictor	B
in	O
o	O
there	O
is	O
a	O
bayesian	B
rationale	O
for	O
ridge	B
regression	B
assume	O
that	O
the	O
noise	O
that	O
is	O
the	O
usual	O
estimate	B
o	O
with	O
o	O
vector	B
is	O
normal	B
as	O
in	O
so	O
that	O
rather	O
than	O
just	O
then	O
the	O
bayesian	B
prior	B
to	O
a	O
mildly	O
negative	O
one	O
in	O
o	O
o	O
np	O
i	O
np	O
n	O
j	O
o	O
o	O
d	O
c	O
o	O
makes	O
e	O
with	O
m	O
d	O
ridge	B
regression	B
amounts	O
to	O
an	O
the	O
same	O
as	O
the	O
ridge	B
regression	B
estimate	B
o	O
a	O
d	O
and	O
d	O
increased	O
prior	B
belief	O
that	O
lies	O
near	O
the	O
last	O
two	O
columns	O
of	O
table	O
compare	O
the	O
standard	B
deviations	I
of	O
and	O
o	O
o	O
ridging	O
has	O
greatly	O
reduced	O
the	O
variability	O
of	O
the	O
estimated	O
ridge	B
regression	B
d	O
x	O
o	O
regression	B
coefficients	O
this	O
does	O
not	O
guarantee	O
that	O
the	O
corresponding	O
estimate	B
of	O
d	O
x	O
o	O
will	O
be	O
more	O
accurate	O
than	O
the	O
ordinary	O
least	B
squares	I
estimate	B
d	O
x	O
we	O
have	O
introduced	O
bias	B
and	O
the	O
squared	O
bias	B
term	O
counteracts	O
some	O
of	O
the	O
advantage	O
of	O
reduced	O
variability	O
the	O
cp	B
calculations	O
of	O
chapter	O
suggest	O
that	O
the	O
two	O
effects	O
nearly	O
offset	B
each	O
other	O
for	O
the	O
diabetes	B
data	B
however	O
if	O
interest	O
centers	O
on	O
the	O
coefficients	O
of	O
then	O
ridging	O
can	O
be	O
crucial	O
as	O
table	O
emphasizes	O
by	O
current	O
standards	O
p	O
d	O
is	O
a	O
small	O
number	O
of	O
predictors	B
data	B
sets	O
with	O
p	O
in	O
the	O
thousands	O
and	O
more	O
will	O
show	O
up	O
in	O
part	O
iii	O
in	O
such	O
situations	O
the	O
scientist	O
is	O
often	O
looking	O
for	O
a	O
few	O
interesting	O
predictor	B
variables	O
hidden	O
in	O
a	O
sea	O
of	O
uninteresting	O
ones	O
the	O
prior	B
belief	O
is	O
that	O
most	O
of	O
the	O
i	O
values	O
lie	O
near	O
zero	O
biasing	O
the	O
maximum	B
likelihood	B
estimates	O
o	O
i	O
toward	O
zero	O
then	O
becomes	O
a	O
necessity	O
o	O
there	O
is	O
still	O
another	O
way	O
to	O
motivate	O
the	O
ridge	B
regression	B
estimator	B
o	O
d	O
arg	O
min	O
fky	O
x	O
c	O
differentiating	O
the	O
term	O
in	O
brackets	O
with	O
respect	O
to	O
shows	O
that	O
o	O
d	O
c	O
y	O
as	O
in	O
if	O
d	O
then	O
describes	O
the	O
ordinary	O
least	B
squares	I
algorithm	B
penalizes	O
choices	O
of	O
having	O
k	O
k	O
large	O
biasing	O
o	O
various	O
terminologies	O
are	O
used	O
to	O
describe	O
algorithms	O
such	O
as	O
penalized	O
least	B
squares	I
penalized	O
likelihood	B
maximized	O
a-posteriori	O
probability	O
and	O
generically	O
regularization	B
describes	O
almost	O
any	O
method	B
that	O
tamps	O
down	O
statistical	O
variability	O
in	O
high-dimensional	O
estimation	B
or	O
prediction	O
problems	O
toward	O
the	O
origin	O
a	O
wide	O
variety	O
of	O
penalty	B
terms	O
are	O
in	O
current	O
use	O
the	O
most	O
influential	O
one	O
involving	O
the	O
norm	O
k	O
dpp	O
j	O
jj	O
q	O
d	O
arg	O
min	O
fky	O
x	O
c	O
the	O
so-called	O
lasso	B
estimator	B
chapter	O
despite	O
the	O
bayesian	B
provenance	O
most	O
regularization	B
research	O
is	O
carried	O
out	O
frequentistically	O
with	O
various	O
penalty	B
terms	O
investigated	O
for	O
their	O
probabilistic	O
behavior	O
regarding	O
estimation	B
prediction	O
and	O
variable	O
selection	O
if	O
we	O
apply	O
the	O
james	O
stein	O
rule	B
to	O
the	O
normal	B
model	O
we	O
get	O
a	O
different	O
shrinkage	B
rule	B
for	O
o	O
say	O
q	O
js	O
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
o	O
q	O
js	O
d	O
s	O
o	O
o	O
q	O
js	O
be	O
the	O
corresponding	O
estimator	B
of	O
d	O
efyg	O
in	O
letting	O
d	O
x	O
the	O
james	O
stein	O
theorem	B
guarantees	O
that	O
e	O
no	O
matter	O
what	O
is	O
as	O
long	O
as	O
p	O
there	O
is	O
no	O
such	O
guarantee	O
for	O
ridge	B
regression	B
and	O
no	O
foolproof	O
way	O
to	O
choose	O
the	O
ridge	O
parameter	O
on	O
the	O
other	O
hand	O
q	O
js	O
does	O
not	O
stabilize	O
the	O
coordinate	O
standard	B
deviations	I
as	O
in	O
the	O
column	O
of	O
table	O
the	O
main	O
point	O
here	O
is	O
that	O
at	O
present	O
there	O
is	O
no	O
optimality	B
theory	B
for	O
shrinkage	B
estimation	B
fisher	B
provided	O
an	O
elegant	O
theory	B
for	O
optimal	O
unbiased	O
estimation	B
it	O
remains	O
to	O
be	O
seen	O
whether	O
biased	B
estimation	B
can	O
be	O
neatly	O
codified	O
d	O
indirect	B
evidence	I
there	O
is	O
a	O
downside	O
to	O
shrinkage	B
estimation	B
which	O
we	O
can	O
examine	O
by	O
returning	O
to	O
the	O
baseball	B
data	B
of	O
table	O
one	O
thousand	O
simulations	O
were	O
run	O
each	O
one	O
generating	O
simulated	O
batting	O
averages	O
p	O
i	O
truthi	O
i	O
d	O
these	O
gave	O
corresponding	O
james	O
stein	O
estimates	O
with	O
np	O
np	O
table	O
shows	O
the	O
root	O
mean	O
square	O
error	O
for	O
the	O
mle	B
and	O
js	O
estimates	O
over	O
simulations	O
for	O
each	O
of	O
the	O
players	O
ij	O
truthi	O
and	O
op	O
js	O
ij	O
truthi	O
loses	O
to	O
op	O
as	O
foretold	O
by	O
the	O
james	O
stein	O
theorem	B
the	O
js	O
estimates	O
are	O
easy	O
victors	O
in	O
terms	O
of	O
total	O
squared	B
error	I
over	O
all	O
players	O
however	O
op	O
js	O
i	O
for	O
of	O
the	O
players	O
losing	O
badly	O
in	O
the	O
case	O
of	O
player	O
js	O
for	O
player	O
appear	O
in	O
figure	O
strikingly	O
all	O
of	O
the	O
op	O
js	O
values	O
lie	O
histograms	O
comparing	O
the	O
simulations	O
of	O
p	O
i	O
with	O
those	O
of	O
op	O
d	O
p	O
mle	B
i	O
i	O
i	O
of	O
course	O
we	O
are	O
assumimg	O
is	O
known	O
in	O
if	O
it	O
is	O
estimated	O
some	O
of	O
the	O
improvement	O
erodes	O
away	O
indirect	B
evidence	I
table	O
simulation	B
study	O
comparing	O
root	O
mean	O
square	O
errors	B
for	O
mle	B
and	O
js	O
estimators	O
as	O
estimates	O
of	O
truth	O
total	O
mean	O
square	O
errors	B
and	O
asterisks	O
indicate	O
four	O
players	O
for	O
whom	O
rmsjs	O
exceeded	O
rmsmle	O
these	O
have	O
two	O
largest	O
and	O
two	O
smallest	O
truth	O
values	O
is	O
clemente	O
column	O
is	O
for	O
the	O
limited	O
translation	O
version	O
of	O
js	O
that	O
bounds	O
shrinkage	B
to	O
within	O
one	O
standard	B
deviation	I
of	O
the	O
mle	B
player	O
truth	O
rmsmle	O
rmsjs	O
js	O
i	O
below	O
d	O
player	O
could	O
have	O
had	O
a	O
legitimate	O
complaint	O
if	O
the	O
james	O
stein	O
estimate	B
were	O
used	O
to	O
set	B
his	O
next	O
year	O
s	O
salary	O
the	O
four	O
losing	O
cases	O
for	O
op	O
are	O
the	O
players	O
with	O
the	O
two	O
largest	O
and	O
two	O
smallest	O
values	O
of	O
the	O
truth	O
shrinkage	B
estimators	O
work	O
against	O
cases	O
that	O
are	O
genuinely	O
outstanding	O
a	O
positive	O
or	O
negative	O
sense	O
player	O
was	O
roberto	O
clemente	O
a	O
better	O
informed	O
bayesian	B
that	O
is	O
a	O
baseball	B
fan	O
would	O
know	O
that	O
clemente	O
had	O
led	O
the	O
league	O
in	O
batting	O
over	O
the	O
previous	O
several	O
years	O
and	O
shouldn	O
t	B
be	O
thrown	O
into	O
a	O
shrinkage	B
pool	O
with	O
ordinary	O
hitters	O
of	O
course	O
the	O
james	O
stein	O
estimates	O
were	O
more	O
accurate	O
for	O
of	O
the	O
players	O
shrinkage	B
estimation	B
tends	O
to	O
produce	O
better	O
results	O
in	O
general	O
at	O
the	O
possible	O
expense	O
of	O
extreme	O
cases	O
nobody	O
cares	O
much	O
about	O
cold	O
war	O
batting	O
averages	O
but	O
if	O
the	O
context	O
were	O
the	O
efficacies	O
of	O
new	O
anticancer	O
drugs	O
the	O
stakes	O
would	O
be	O
higher	O
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
figure	O
comparing	O
mle	B
estimates	O
with	O
js	O
estimates	O
for	O
clemente	O
simulations	O
at	O
bats	O
each	O
compromise	O
methods	O
are	O
available	O
the	O
column	O
of	O
table	O
refers	O
to	O
a	O
limited	O
translation	O
version	O
of	O
opjs	O
in	O
which	O
shrinkage	B
is	O
not	O
allowed	O
to	O
diverge	O
more	O
than	O
one	O
unit	O
from	O
opi	O
in	O
formulaic	O
terms	O
i	O
d	O
min	O
opjs	O
opjs	O
i	O
opi	O
c	O
i	O
opi	O
this	O
mitigates	O
the	O
clemente	O
problem	O
while	O
still	O
gaining	O
most	O
of	O
the	O
shrinkage	B
advantages	O
the	O
use	O
of	O
indirect	B
evidence	I
amounts	O
to	O
learning	O
from	O
the	O
experience	O
of	O
others	O
each	O
batter	O
learning	O
from	O
the	O
others	O
in	O
the	O
baseball	B
examples	O
which	O
others	O
is	O
a	O
key	O
question	O
in	O
applying	O
computer-age	O
methods	O
chapter	O
returns	O
to	O
the	O
question	O
in	O
the	O
context	O
of	O
false-discovery	O
rates	O
notes	O
and	O
details	O
the	O
bayesian	B
motivation	O
emphasized	O
in	O
chapters	O
and	O
is	O
anachronistic	O
originally	O
the	O
work	O
emerged	O
mainly	O
from	O
frequentist	B
considerations	O
and	O
was	O
justified	O
frequentistically	O
as	O
in	O
robbins	O
stein	O
proved	O
the	O
inadmissibility	O
of	O
the	O
neat	O
version	O
of	O
appearing	O
in	O
james	O
and	O
stein	O
james	O
was	O
stein	O
s	O
graduate	O
student	O
is	O
itself	O
inadmissable	O
being	O
everywhere	O
improvable	O
by	O
changing	O
ob	O
in	O
mlep	O
james	O
stein	O
to	O
max	O
ob	O
this	O
in	O
turn	O
is	O
inadmissable	O
but	O
further	O
gains	O
tend	O
to	O
the	O
minuscule	O
notes	O
and	O
details	O
in	O
a	O
series	O
of	O
papers	O
in	O
the	O
early	O
efron	O
and	O
morris	O
emphasized	O
the	O
empirical	B
bayes	I
motivation	O
of	O
the	O
james	O
stein	O
rule	B
efron	O
and	O
morris	O
giving	O
the	O
limited	O
translation	O
version	O
the	O
baseball	B
data	B
in	O
its	O
original	O
form	B
appears	O
in	O
table	O
of	O
efron	O
here	O
the	O
original	O
at	O
bats	O
recorded	O
for	O
each	O
player	O
have	O
been	O
artificially	O
augmented	O
by	O
adding	O
binomial	B
draws	O
truthi	O
for	O
player	O
i	O
this	O
gives	O
a	O
somewhat	O
less	O
optimistic	O
view	O
of	O
the	O
james	O
stein	O
rule	B
s	O
performance	O
stein	O
s	O
paradox	B
in	O
statistics	B
efron	O
and	O
morris	O
title	O
for	O
their	O
scientific	O
american	O
article	O
catches	O
the	O
statistics	B
world	O
s	O
sense	O
of	O
discomfort	O
with	O
the	O
james	O
stein	O
theorem	B
why	O
should	O
our	O
estimate	B
for	O
player	O
a	O
go	O
up	O
or	O
down	O
depending	O
on	O
the	O
other	O
players	O
performances	O
this	O
is	O
the	O
question	O
of	O
direct	O
versus	O
indirect	B
evidence	I
raised	O
again	O
in	O
the	O
context	O
of	O
hypothesis	B
testing	B
in	O
chapter	O
unbiased	O
estimation	B
has	O
great	O
scientific	O
appeal	O
so	O
the	O
argument	B
is	O
by	O
no	O
means	O
settled	O
ridge	B
regression	B
was	O
introduced	O
into	O
the	O
statistics	B
literature	O
by	O
hoerl	O
and	O
kennard	O
it	O
appeared	O
previously	O
in	O
the	O
numerical	O
analysis	B
literature	O
as	O
tikhonov	O
regularization	B
of	O
freedom	O
z	O
formula	B
if	O
z	O
has	O
a	O
chi-squared	O
distribution	B
with	O
degrees	O
is	O
z	O
in	O
table	O
it	O
has	O
density	B
f	O
d	O
for	O
z	O
dz	O
d	O
n	O
d	O
s	O
a	O
c	O
d	O
yielding	O
dz	O
e	O
z	O
e	O
verifying	O
but	O
standard	O
results	O
starting	O
from	O
show	O
that	O
s	O
c	O
with	O
d	O
n	O
in	O
formula	B
first	O
consider	O
the	O
simpler	O
situation	O
where	O
m	O
in	O
is	O
known	O
to	O
equal	O
zero	O
in	O
which	O
case	O
the	O
james	O
stein	O
estimator	B
is	O
with	O
ob	O
d	O
where	O
s	O
dpn	O
oc	O
d	O
ob	O
d	O
and	O
c	O
d	O
b	O
d	O
c	O
d	O
obxi	O
i	O
for	O
convenient	O
notation	O
let	O
i	O
james	O
stein	O
estimation	B
and	O
ridge	B
regression	B
the	O
conditional	B
distribution	B
e	O
e	O
i	O
n	O
b	O
gives	O
i	O
x	O
o	O
d	O
b	O
c	O
oc	O
c	O
x	O
o	O
d	O
nb	O
c	O
oc	O
c	O
o	O
d	O
b	O
n	O
oc	O
c	O
d	O
nb	O
c	O
b	O
e	O
e	O
and	O
so	O
and	O
adding	O
over	O
the	O
n	O
coordinates	O
the	O
marginal	O
distribution	B
s	O
c	O
calculation	O
n	O
and	O
yields	O
after	O
a	O
little	O
by	O
orthogonal	O
transformations	O
in	O
situation	O
where	O
m	O
is	O
not	O
assumed	O
to	O
be	O
zero	O
can	O
be	O
represented	O
as	O
the	O
sum	O
of	O
two	O
parts	O
a	O
js	O
estimate	B
in	O
n	O
dimensions	O
but	O
with	O
m	O
d	O
as	O
in	O
and	O
a	O
mle	B
estimate	B
of	O
the	O
remaining	O
one	O
coordinate	O
using	O
this	O
gives	O
d	O
c	O
b	O
c	O
d	O
nb	O
c	O
b	O
e	O
which	O
is	O
the	O
james	O
stein	O
theorem	B
stein	O
derived	O
a	O
simpler	O
proof	O
of	O
the	O
js	O
theorem	B
that	O
appears	O
in	O
section	O
of	O
efron	O
transformations	O
to	O
form	B
the	O
linear	B
regression	B
model	I
is	O
equivariant	B
under	O
scale	B
changes	O
of	O
the	O
variables	O
xj	O
what	O
this	O
means	O
is	O
that	O
the	O
space	O
of	O
fits	O
using	O
linear	B
combinations	O
of	O
the	O
xj	O
is	O
the	O
same	O
as	O
the	O
space	O
of	O
linear	B
combinations	O
using	O
scaled	O
versions	O
qxj	O
d	O
xj	O
with	O
sj	O
furthermore	O
the	O
least	B
squares	I
fits	O
are	O
the	O
same	O
and	O
the	O
coefficient	O
estimates	O
map	B
in	O
the	O
obvious	O
way	O
oq	O
j	O
d	O
sj	O
ridge	B
regression	B
we	O
see	O
that	O
the	O
penalty	B
term	O
k	O
dp	O
not	O
so	O
for	O
ridge	B
regression	B
changing	O
the	O
scales	O
of	O
the	O
columns	O
of	O
x	O
will	O
generally	O
lead	O
to	O
different	O
fits	O
using	O
the	O
penalty	B
version	O
of	O
j	O
treats	O
all	O
the	O
coefficients	O
as	O
equals	O
this	O
penalty	B
is	O
most	O
natural	O
if	O
all	O
the	O
variables	O
are	O
measured	O
on	O
the	O
same	O
scale	B
hence	O
we	O
typically	O
use	O
for	O
sj	O
the	O
standard	B
deviation	I
of	O
variable	O
xj	O
which	O
leads	O
to	O
furthermore	O
with	O
ridge	B
regression	B
we	O
typically	O
do	O
not	O
penalize	O
the	O
intercept	O
this	O
can	O
be	O
achieved	O
o	O
j	O
j	O
by	O
centering	B
and	O
scaling	O
each	O
of	O
the	O
variables	O
qxj	O
d	O
where	O
notes	O
and	O
details	O
nxj	O
d	O
nx	O
xij	O
and	O
sj	O
dhx	O
nxj	O
with	O
the	O
n-vector	O
of	O
we	O
now	O
work	O
with	O
qx	O
d	O
qxp	O
rather	O
than	O
x	O
and	O
the	O
intercept	O
is	O
estimated	O
separately	O
as	O
ny	O
we	O
calculate	O
the	O
covariance	O
matrix	B
of	O
o	O
d	O
c	O
standard	B
deviations	I
in	O
table	O
from	O
the	O
first	O
equality	O
in	O
to	O
be	O
c	O
the	O
entries	O
in	O
table	O
are	O
square	O
roots	O
of	O
the	O
diagonal	O
elements	O
of	O
substituting	O
the	O
ordinary	O
least	B
squares	I
estimate	B
d	O
for	O
penalized	O
likelihood	B
and	O
map	B
with	O
fixed	O
and	O
known	O
in	O
the	O
nn	O
x	O
minimizing	O
ky	O
x	O
is	O
the	O
normal	B
linear	B
model	I
y	O
same	O
as	O
maximizing	O
the	O
log	O
density	B
function	B
log	O
f	O
d	O
ky	O
x	O
c	O
constant	O
in	O
this	O
sense	O
the	O
term	O
in	O
penalizes	O
the	O
likelihood	B
log	O
f	O
connected	O
with	O
in	O
proportion	B
to	O
the	O
magnitude	O
k	O
under	O
the	O
prior	B
distribution	B
the	O
log	O
posterior	B
density	B
of	O
given	O
y	O
log	O
of	O
is	O
ky	O
x	O
c	O
plus	O
a	O
term	O
that	O
doesn	O
t	B
depend	O
on	O
that	O
makes	O
the	O
maximizer	O
of	O
also	O
the	O
maximizer	O
of	O
the	O
posterior	B
density	B
of	O
given	O
y	O
or	O
the	O
map	B
formula	B
let	O
d	O
and	O
d	O
where	O
s	O
is	O
a	O
matrix	B
square	O
root	O
of	O
s	O
d	O
s	O
then	O
o	O
in	O
and	O
the	O
m	O
d	O
form	B
of	O
the	O
james	O
stein	O
rule	B
is	O
i	O
p	O
transforming	O
back	O
to	O
the	O
scale	B
gives	O
js	O
d	O
generalized	O
linear	B
models	B
and	O
regression	B
trees	B
indirect	B
evidence	I
is	O
not	O
the	O
sole	O
property	O
of	O
bayesians	O
regression	B
models	B
are	O
the	O
frequentist	B
method	B
of	O
choice	O
for	O
incorporating	O
the	O
experience	O
of	O
others	O
as	O
an	O
example	O
figure	O
returns	O
to	O
the	O
kidney	O
fitness	O
data	B
of	O
section	O
a	O
potential	O
new	O
donor	O
aged	O
has	O
appeared	O
and	O
we	O
wish	O
to	O
assess	O
his	O
kidney	O
fitness	O
without	O
subjecting	O
him	O
to	O
an	O
arduous	O
series	O
of	O
medical	O
tests	O
only	O
one	O
of	O
the	O
previously	O
tested	O
volunteers	O
was	O
age	O
his	O
tot	O
score	O
being	O
upper	O
large	O
dot	O
in	O
figure	O
most	O
dtot	O
d	O
the	O
former	O
is	O
the	O
only	O
direct	B
evidence	I
we	O
have	O
while	O
the	O
applied	O
statisticians	O
though	O
would	O
prefer	O
to	O
read	O
off	O
the	O
height	O
of	O
the	O
least	B
squares	I
regression	B
line	O
at	O
age	O
d	O
green	O
dot	O
on	O
the	O
regression	B
line	O
figure	O
kidney	O
data	B
a	O
new	O
volunteer	O
donor	O
is	O
aged	O
which	O
prediction	O
is	O
preferred	O
for	O
his	O
kidney	B
function	B
logistic	B
regression	B
regression	B
line	O
lets	O
us	O
incorporate	O
indirect	B
evidence	I
for	O
age	O
from	O
all	O
previous	O
cases	O
increasingly	O
aggressive	O
use	O
of	O
regression	B
techniques	O
is	O
a	O
hallmark	O
of	O
modern	O
statistical	O
practice	O
aggressive	O
applying	O
to	O
the	O
number	O
and	O
type	O
of	O
predictor	B
variables	O
the	O
coinage	O
of	O
new	O
methodology	O
and	O
the	O
sheer	O
size	O
of	O
the	O
target	O
data	B
sets	O
generalized	O
linear	B
models	B
this	O
chapter	O
s	O
main	O
topic	O
have	O
been	O
the	O
most	O
pervasively	O
influential	O
of	O
the	O
new	O
methods	O
the	O
chapter	O
ends	O
with	O
a	O
brief	O
review	O
of	O
regression	B
trees	B
a	O
completely	O
different	O
regression	B
methodology	O
that	O
will	O
play	O
an	O
important	O
role	O
in	O
the	O
prediction	O
algorithms	O
of	O
chapter	O
logistic	B
regression	B
an	O
experimental	O
new	O
anti-cancer	O
drug	O
called	O
xilathon	O
is	O
under	O
development	O
before	O
human	O
testing	B
can	O
begin	O
animal	O
studies	O
are	O
needed	O
to	O
determine	O
safe	O
dosages	O
to	O
this	O
end	O
a	O
bioassay	B
or	O
dose	O
response	B
experiment	O
was	O
carried	O
out	O
groups	O
of	O
n	O
d	O
mice	O
each	O
were	O
injected	O
with	O
increasing	O
amounts	O
of	O
xilathon	O
dosages	O
let	O
yi	O
d	O
mice	O
dying	O
in	O
ith	O
group	O
the	O
points	O
in	O
figure	O
show	O
the	O
proportion	B
of	O
deaths	O
pi	O
d	O
yi	O
lethality	O
generally	O
increasing	O
with	O
dose	O
the	O
counts	O
yi	O
are	O
modeled	O
as	O
independent	O
binomials	O
bi	O
ni	O
for	O
i	O
d	O
n	O
yi	O
n	O
d	O
and	O
all	O
ni	O
equaling	O
here	O
is	O
the	O
true	O
death	O
rate	B
in	O
group	O
i	O
estimated	O
unbiasedly	O
by	O
pi	O
the	O
direct	B
evidence	I
for	O
the	O
regression	B
curve	O
in	O
figure	O
uses	O
all	O
the	O
doses	O
to	O
give	O
a	O
better	O
picture	O
of	O
the	O
true	O
dose	O
response	B
relation	O
logistic	B
regression	B
is	O
a	O
specialized	O
technique	O
for	O
regression	B
analysis	B
of	O
count	O
or	O
proportion	B
data	B
the	O
logit	B
parameter	O
is	O
defined	O
as	O
o	O
n	O
d	O
log	O
dose	O
would	O
usually	O
be	O
labeled	O
on	O
a	O
log	O
scale	B
each	O
one	O
say	O
larger	O
than	O
its	O
predecessor	O
glms	O
and	O
regression	B
trees	B
figure	O
dose	O
response	B
study	O
groups	O
of	O
mice	O
exposed	O
to	O
increasing	O
doses	O
of	O
experimental	O
drug	O
the	O
points	O
are	O
the	O
observed	O
proportions	O
that	O
died	O
in	O
each	O
group	O
the	O
fitted	O
curve	O
is	O
the	O
maximum-likelihoood	O
estimate	B
of	O
the	O
linear	B
logistic	B
regression	B
model	I
the	O
open	O
circle	O
on	O
the	O
curve	O
is	O
the	O
the	O
estimated	O
dose	O
for	O
mortality	O
with	O
increasing	O
from	O
to	O
as	O
increases	O
from	O
to	O
a	O
linear	B
logistic	B
regression	B
dose	O
response	B
analysis	B
begins	O
with	O
binomial	B
model	O
and	O
assumes	O
that	O
the	O
logit	B
is	O
a	O
linear	B
function	B
of	O
dose	O
d	O
c	O
d	O
log	O
maximum	B
likelihood	B
gives	O
estimates	O
o	O
and	O
fitted	O
curve	O
since	O
the	O
inverse	O
transformation	O
of	O
is	O
we	O
obtain	O
from	O
the	O
linear	B
logistic	B
regression	B
curve	O
o	O
d	O
o	O
c	O
o	O
c	O
e	O
c	O
e	O
pictured	O
in	O
figure	O
table	O
compares	O
the	O
standard	B
deviation	I
of	O
the	O
estimated	O
regression	B
llllllllllldoseproportion	O
of	O
logistic	B
regression	B
table	O
standard	B
deviation	I
estimates	O
for	O
in	O
figure	O
the	O
first	O
row	O
is	O
for	O
the	O
linear	B
logistic	B
regression	B
fit	O
the	O
second	O
row	O
is	O
based	O
on	O
the	O
individual	O
binomial	B
estimates	O
pi	O
x	O
sd	O
sd	O
pi	O
curve	O
at	O
x	O
d	O
discussed	O
in	O
the	O
next	O
section	O
with	O
the	O
usual	O
binomial	B
standard	B
deviation	I
estimate	B
pi	O
pi	O
obtained	O
by	O
considering	O
the	O
doses	O
regression	B
has	O
reduced	O
error	O
by	O
better	O
than	O
the	O
price	O
being	O
possible	O
bias	B
if	O
model	O
goes	O
seriously	O
wrong	O
one	O
advantage	O
of	O
the	O
logit	B
transformation	O
is	O
that	O
isn	O
t	B
restricted	O
to	O
the	O
range	O
so	O
model	O
never	O
verges	O
on	O
forbidden	O
territory	O
a	O
better	O
reason	O
has	O
to	O
do	O
with	O
the	O
exploitation	O
of	O
exponential	B
family	I
properties	O
we	O
can	O
rewrite	O
the	O
density	B
function	B
for	O
bi	O
n	O
y	O
as	O
n	O
y	O
d	O
n	O
y	O
with	O
the	O
logit	B
parameter	O
and	O
d	O
c	O
is	O
a	O
one-parameter	B
exponential	O
as	O
described	O
in	O
section	O
with	O
the	O
natural	B
parameter	I
called	O
there	O
let	O
y	O
d	O
yn	O
denote	O
the	O
full	B
data	B
set	B
n	O
d	O
in	O
figure	O
using	O
and	O
the	O
independence	O
of	O
the	O
yi	O
gives	O
the	O
probability	O
density	B
of	O
y	O
as	O
a	O
function	B
of	O
ni	O
ny	O
f	O
d	O
ny	O
d	O
e	O
e	O
ni	O
yi	O
ni	O
yi	O
for	O
the	O
separate-dose	O
standard	B
error	I
pi	O
was	O
taken	O
equal	O
to	O
the	O
fitted	O
value	O
from	O
the	O
curve	O
in	O
figure	O
it	O
is	O
not	O
necessary	O
for	O
in	O
on	O
page	O
to	O
be	O
a	O
probability	O
density	B
function	B
only	O
that	O
it	O
not	O
depend	O
on	O
the	O
parameter	O
where	O
glms	O
and	O
regression	B
trees	B
d	O
nx	O
yi	O
and	O
d	O
nx	O
xi	O
yi	O
formula	B
expresses	O
f	O
as	O
the	O
product	O
of	O
three	O
factors	O
f	O
d	O
g	O
only	O
the	O
first	O
of	O
which	O
involves	O
both	O
the	O
parameters	O
and	O
the	O
data	B
this	O
implies	O
that	O
is	O
a	O
sufficient	O
statistic	B
no	O
matter	O
how	O
large	O
n	O
might	O
be	O
we	O
will	O
have	O
n	O
in	O
the	O
thousands	O
just	O
the	O
two	O
numbers	O
contain	O
all	O
of	O
the	O
experiment	O
s	O
information	B
only	O
the	O
logistic	O
parameterization	O
makes	O
this	O
a	O
more	O
intuitive	O
picture	O
of	O
logistic	B
regression	B
depends	O
on	O
d	O
pi	O
the	O
deviance	B
between	O
an	O
observed	O
proportion	B
pi	O
and	O
an	O
estimate	B
c	O
pi	O
log	O
pi	O
d	O
d	O
pi	O
log	O
the	O
is	O
zero	O
if	O
d	O
pi	O
otherwise	O
it	O
increases	O
as	O
departs	O
further	O
from	O
pi	O
the	O
logistic	B
regression	B
mle	B
value	O
o	O
also	O
turns	O
out	O
to	O
be	O
the	O
choice	O
of	O
minimizing	O
the	O
total	O
deviance	B
between	O
the	O
n	O
points	O
pi	O
and	O
their	O
corresponding	O
estimates	O
d	O
o	O
d	O
arg	O
min	O
nx	O
d	O
the	O
solid	O
line	O
in	O
figure	O
is	O
the	O
linear	B
logistic	O
curve	O
coming	O
closest	O
to	O
the	O
points	O
when	O
distance	O
is	O
measured	O
by	O
total	O
deviance	B
in	O
this	O
way	O
the	O
notion	O
of	O
least	B
squares	I
is	O
generalized	O
to	O
binomial	B
regression	B
as	O
discussed	O
in	O
the	O
next	O
section	O
a	O
more	O
sophisticated	O
notion	O
of	O
distance	O
between	O
data	B
and	O
models	B
is	O
one	O
of	O
the	O
accomplishments	O
of	O
modern	O
statistics	B
table	O
reports	O
on	O
the	O
data	B
for	O
a	O
more	O
structured	O
logistic	B
regression	B
analysis	B
human	O
muscle	O
cell	O
colonies	O
were	O
infused	O
with	O
mouse	O
nuclei	O
in	O
five	O
different	O
ratios	O
cultured	O
over	O
time	O
periods	O
ranging	O
from	O
one	O
to	O
five	O
where	O
the	O
name	O
logistic	B
regression	B
comes	O
from	O
is	O
explained	O
in	O
the	O
endnotes	O
along	O
with	O
a	O
description	O
of	O
its	O
nonexponential	O
family	O
predecessor	O
probit	B
analysis	B
deviance	B
is	O
analogous	O
to	O
squared	B
error	I
in	O
ordinary	O
regression	B
theory	B
as	O
discussed	O
in	O
what	O
follows	O
it	O
is	O
twice	O
the	O
kullback	O
leibler	O
distance	O
the	O
preferred	O
name	O
in	O
the	O
information-theory	O
literature	O
logistic	B
regression	B
table	O
cell	B
infusion	I
data	B
human	O
cell	O
colonies	O
infused	O
with	O
mouse	O
nuclei	O
in	O
five	O
ratios	O
over	O
to	O
days	O
and	O
observed	O
to	O
see	O
whether	O
they	O
did	O
or	O
did	O
not	O
thrive	O
green	O
numbers	O
are	O
estimates	O
from	O
the	O
logistic	B
regression	B
model	I
for	O
example	O
of	O
colonies	O
in	O
the	O
lowest	O
ratiodays	O
category	O
thrived	O
with	O
observed	O
proportion	B
d	O
and	O
logistic	B
regression	B
estimate	B
d	O
ratio	O
time	O
days	O
and	O
observed	O
to	O
see	O
whether	O
they	O
thrived	O
for	O
example	O
of	O
the	O
colonies	O
having	O
the	O
third	O
ratio	O
and	O
shortest	O
time	O
period	O
thrived	O
let	O
denote	O
the	O
true	O
probability	O
of	O
thriving	O
for	O
ratio	O
i	O
during	O
time	O
period	O
j	O
and	O
its	O
logit	B
a	O
two-way	O
additive	O
logistic	B
regression	B
was	O
fit	O
to	O
the	O
d	O
c	O
i	O
c	O
j	O
i	O
d	O
j	O
d	O
the	O
green	O
numbers	O
in	O
table	O
show	O
the	O
maximum	B
likelihood	B
estimates	O
ic	O
o	O
j	O
d	O
c	O
e	O
straintsp	O
i	O
d	O
p	O
j	O
d	O
necessary	O
to	O
avoid	O
definitional	O
difficulties	O
model	O
has	O
nine	O
free	O
parameters	O
into	O
account	O
the	O
con	O
compared	O
with	O
just	O
two	O
in	O
the	O
dose	O
response	B
experiment	O
the	O
count	O
can	O
easily	O
go	O
much	O
higher	O
these	O
days	O
table	O
reports	O
on	O
a	O
logistic	B
regression	B
applied	O
to	O
the	O
spam	B
data	B
a	O
researcher	O
george	O
labeled	O
n	O
d	O
of	O
his	O
email	O
mes	O
using	O
the	O
statistical	O
computing	O
language	O
r	B
see	O
the	O
endnotes	O
glms	O
and	O
regression	B
trees	B
table	O
logistic	B
regression	B
analysis	B
of	O
the	O
spam	B
data	B
model	O
estimated	O
regression	B
coefficients	O
standard	O
errors	B
and	O
z	O
d	O
estimatese	O
for	O
keyword	O
predictors	B
the	O
notation	O
char	O
means	O
the	O
relative	O
number	O
of	O
times	O
appears	O
etc	O
the	O
last	O
three	O
entries	O
measure	O
characteristics	O
such	O
as	O
length	O
of	O
capital-letter	O
strings	O
the	O
word	O
george	O
is	O
special	O
since	O
the	O
recipient	O
of	O
the	O
email	O
is	O
named	O
george	O
and	O
the	O
goal	O
here	O
is	O
to	O
build	O
a	O
customized	O
spam	B
filter	O
intercept	O
make	O
address	O
all	O
our	O
over	O
remove	O
internet	O
order	O
mail	O
receive	O
will	O
people	O
report	O
addresses	O
free	O
business	O
email	O
you	O
credit	O
your	O
font	O
money	O
hp	O
hpl	O
george	O
estimate	B
se	O
z-value	O
lab	O
labs	O
telnet	O
data	B
technology	O
parts	O
pm	O
direct	O
cs	O
meeting	O
original	O
project	O
re	O
edu	O
table	O
conference	O
char	O
char	O
char	O
char	O
char	O
char	O
cap	O
ave	O
cap	O
long	O
cap	O
tot	O
estimate	B
se	O
sages	O
as	O
either	O
spam	B
or	O
ham	O
say	O
yi	O
d	O
if	O
email	O
i	O
is	O
spam	B
if	O
email	O
i	O
is	O
ham	O
z-value	O
ham	O
refers	O
to	O
nonspam	O
or	O
good	O
email	O
this	O
is	O
a	O
playful	O
connection	O
to	O
the	O
processed	O
logistic	B
regression	B
of	O
the	O
messages	O
were	O
spam	B
the	O
p	O
d	O
predictor	B
variables	O
represent	O
the	O
most	O
frequently	O
used	O
words	O
and	O
tokens	O
in	O
george	O
s	O
corpus	O
of	O
email	O
trivial	O
words	O
such	O
as	O
articles	O
and	O
are	O
in	O
fact	O
the	O
relative	O
frequencies	O
of	O
these	O
chosen	O
words	O
in	O
each	O
email	O
by	O
the	O
length	O
of	O
the	O
email	O
the	O
goal	O
of	O
the	O
study	O
was	O
to	O
predict	O
whether	O
future	O
emails	O
are	O
spam	B
or	O
ham	O
using	O
these	O
keywords	O
that	O
is	O
to	O
build	O
a	O
customized	O
spam	B
filter	O
let	O
xij	O
denote	O
the	O
relative	O
frequency	O
of	O
keyword	O
j	O
in	O
email	O
i	O
and	O
represent	O
the	O
probability	O
that	O
email	O
i	O
is	O
spam	B
letting	O
be	O
the	O
logit	B
transform	B
we	O
fit	O
the	O
additive	O
logistic	O
model	O
table	O
shows	O
o	O
i	O
for	O
each	O
word	O
for	O
example	O
for	O
make	O
as	O
well	O
as	O
the	O
estimated	O
standard	B
error	I
and	O
the	O
z-value	O
estimatese	O
j	O
xij	O
d	O
c	O
it	O
looks	O
like	O
certain	O
words	O
such	O
as	O
free	O
and	O
your	O
are	O
good	O
spam	B
predictors	B
however	O
the	O
table	O
as	O
a	O
whole	O
has	O
an	O
unstable	O
appearance	O
with	O
occasional	O
very	O
large	O
estimates	O
o	O
i	O
accompanied	O
by	O
very	O
large	O
standard	O
the	O
dangers	O
of	O
high-dimensional	O
maximum	B
likelihood	B
estimation	B
are	O
apparent	O
here	O
some	O
sort	O
of	O
shrinkage	B
estimation	B
is	O
called	O
for	O
as	O
discussed	O
in	O
chapter	O
regression	B
analysis	B
either	O
in	O
its	O
classical	O
form	B
or	O
in	O
modern	O
formulations	O
requires	O
covariate	O
information	B
x	O
to	O
put	O
the	O
various	O
cases	O
into	O
some	O
sort	O
of	O
geometrical	O
relationship	O
given	O
such	O
information	B
regression	B
is	O
the	O
statistician	O
s	O
most	O
powerful	O
tool	O
for	O
bringing	O
other	O
results	O
to	O
bear	O
on	O
a	O
case	O
of	O
primary	O
interest	O
for	O
instance	O
the	O
volunteer	O
in	O
figure	O
empirical	B
bayes	I
methods	O
do	O
not	O
require	O
covariate	O
information	B
but	O
may	O
be	O
improvable	O
if	O
it	O
exists	O
if	O
for	O
example	O
the	O
player	O
s	O
age	O
were	O
an	O
important	O
covariate	O
in	O
the	O
baseball	B
example	O
of	O
table	O
we	O
might	O
first	O
regress	O
the	O
mle	B
values	O
on	O
age	O
and	O
then	O
shrink	O
them	O
toward	O
the	O
regression	B
line	O
rather	O
than	O
toward	O
the	O
grand	O
mean	O
np	O
as	O
in	O
in	O
this	O
way	O
two	O
different	O
sorts	O
of	O
indirect	B
evidence	I
would	O
be	O
brought	O
to	O
bear	O
on	O
the	O
estimation	B
of	O
each	O
player	O
s	O
ability	O
spam	B
that	O
was	O
fake	O
ham	O
during	O
wwii	O
and	O
has	O
been	O
adopted	O
by	O
the	O
machine-learning	O
community	O
the	O
x	O
matrix	B
was	O
standardized	O
so	O
disparate	O
scalings	O
are	O
not	O
the	O
cause	O
of	O
these	O
discrepancies	O
some	O
of	O
the	O
features	O
have	O
mostly	O
zero	O
observations	O
which	O
may	O
account	O
for	O
their	O
unstable	O
estimation	B
glms	O
and	O
regression	B
trees	B
generalized	O
linear	B
logistic	B
regression	B
is	O
a	O
special	O
case	O
of	O
generalized	O
linear	B
models	B
a	O
key	O
methodology	O
having	O
both	O
algorithmic	O
and	O
inferential	O
influence	O
glms	O
extend	O
ordinary	O
linear	B
regression	B
that	O
is	O
least	B
squares	I
curvefitting	O
to	O
situations	O
where	O
the	O
response	B
variables	O
are	O
binomial	B
poisson	B
gamma	B
beta	B
or	O
in	O
fact	O
any	O
exponential	B
family	I
form	B
n	O
we	O
begin	O
with	O
a	O
one-parameter	B
exponential	B
family	I
d	O
o	O
as	O
in	O
with	O
and	O
x	O
replaced	O
by	O
and	O
y	O
and	O
replaced	O
by	O
for	O
clearer	O
notation	O
in	O
what	O
follows	O
here	O
is	O
the	O
natural	B
parameter	I
and	O
y	O
the	O
sufficient	O
statistic	B
both	O
being	O
one-dimensional	O
in	O
usual	O
applications	O
takes	O
its	O
values	O
in	O
an	O
interval	B
of	O
the	O
real	O
line	O
each	O
coordinate	O
yi	O
of	O
an	O
observed	O
data	B
set	B
y	O
d	O
yi	O
yn	O
is	O
assumed	O
to	O
come	O
from	O
a	O
member	O
of	O
family	O
yi	O
independently	O
for	O
i	O
d	O
n	O
table	O
lists	O
and	O
y	O
for	O
the	O
first	O
four	O
families	O
in	O
table	O
as	O
well	O
as	O
their	O
deviance	B
and	O
normalizing	O
functions	O
by	O
itself	O
model	O
requires	O
n	O
parameters	O
usually	O
too	O
many	O
for	O
effective	O
individual	O
estimation	B
a	O
key	O
glm	O
tactic	O
is	O
to	O
specify	O
the	O
in	O
terms	O
of	O
a	O
linear	B
regression	B
equation	B
let	O
x	O
be	O
an	O
structure	B
matrix	B
with	O
ith	O
row	O
say	O
x	O
i	O
and	O
an	O
unknown	O
vector	B
of	O
p	O
parameters	O
the	O
n	O
d	O
is	O
then	O
specified	O
by	O
d	O
x	O
in	O
the	O
dose	O
response	B
experiment	O
of	O
figure	O
and	O
model	O
x	O
is	O
n	O
with	O
ith	O
row	O
xi	O
and	O
parameter	O
vector	B
d	O
ny	O
the	O
probability	O
density	B
function	B
f	O
of	O
the	O
data	B
vector	B
y	O
is	O
pn	O
f	O
d	O
ny	O
d	O
e	O
which	O
can	O
be	O
written	O
as	O
f	O
d	O
e	O
some	O
of	O
the	O
more	O
technical	O
points	O
raised	O
in	O
this	O
section	O
are	O
referred	O
to	O
in	O
later	O
chapters	O
and	O
can	O
be	O
scanned	O
or	O
omitted	O
at	O
first	O
reading	O
generalized	O
linear	B
models	B
table	O
exponential	B
family	I
form	B
for	O
first	O
four	O
cases	O
in	O
table	O
natural	B
parameter	I
sufficient	O
statistic	B
y	O
deviance	B
between	O
family	O
members	O
and	O
and	O
normalizing	O
function	B
y	O
x	O
log	O
x	O
log	O
x	O
h	O
i	O
log	O
c	O
log	O
i	O
log	O
i	O
n	O
c	O
log	O
x	O
normal	B
n	O
known	O
poisson	B
binomial	B
bi	O
n	O
gamma	B
known	O
where	O
y	O
and	O
d	O
nx	O
z	O
d	O
x	O
i	O
a	O
p-parameter	B
exponential	B
family	I
with	O
natural	B
parameter	I
vector	B
and	O
sufficient	O
statistic	B
vector	B
z	O
the	O
main	O
point	O
is	O
that	O
all	O
the	O
information	B
from	O
a	O
p-parameter	B
glm	O
is	O
summarized	O
in	O
the	O
p-dimensional	O
vector	B
z	O
no	O
matter	O
how	O
large	O
n	O
may	O
be	O
making	O
it	O
easier	O
both	O
to	O
understand	O
and	O
to	O
analyze	O
we	O
have	O
now	O
reduced	O
the	O
n	O
model	O
to	O
the	O
pparameter	O
exponential	B
family	I
with	O
p	O
usually	O
much	O
smaller	O
than	O
n	O
in	O
this	O
way	O
avoiding	O
the	O
difficulties	O
of	O
high-dimensional	O
estimation	B
the	O
moments	O
of	O
the	O
one-parameter	B
constituents	O
determine	O
the	O
estimation	B
properties	O
in	O
model	O
let	O
denote	O
the	O
expectation	O
and	O
variance	B
of	O
univariate	O
density	B
y	O
d	O
for	O
the	O
poisson	B
the	O
n	O
y	O
obtained	O
for	O
instance	O
from	O
glm	O
then	O
has	O
mean	O
vector	B
and	O
covariance	O
matrix	B
y	O
glms	O
and	O
regression	B
trees	B
where	O
is	O
the	O
vector	B
with	O
ith	O
component	O
with	O
d	O
x	O
i	O
and	O
is	O
the	O
n	O
n	O
diagonal	O
matrix	B
having	O
diagonal	O
elements	O
the	O
maximum	B
likelihood	B
estimate	B
o	O
of	O
the	O
parameter	O
vector	B
can	O
be	O
shown	O
to	O
satisfy	O
the	O
simple	O
equation	B
y	O
d	O
x	O
for	O
the	O
normal	B
case	O
where	O
yi	O
linear	B
regression	B
d	O
x	O
o	O
and	O
becomes	O
x	O
the	O
familiar	O
solution	O
n	O
in	O
that	O
is	O
for	O
ordinary	O
x	O
o	O
d	O
with	O
o	O
d	O
yi	O
x	O
otherwise	O
is	O
a	O
nonlinear	B
function	B
of	O
and	O
must	O
be	O
solved	O
by	O
numerical	O
iteration	O
this	O
is	O
made	O
easier	O
by	O
the	O
fact	O
that	O
for	O
glms	O
log	O
f	O
the	O
likelihood	B
function	B
we	O
wish	O
to	O
maximize	O
is	O
a	O
concave	O
function	B
of	O
the	O
mle	B
o	O
has	O
approximate	O
expectation	O
and	O
covariance	O
similar	O
to	O
the	O
exact	O
ols	O
result	O
o	O
generalizing	O
the	O
binomial	B
definition	O
the	O
deviance	B
between	O
den	O
sities	O
and	O
is	O
defined	O
to	O
be	O
d	O
dy	O
o	O
z	O
x	O
log	O
y	O
the	O
integral	O
sum	O
for	O
discrete	O
distributions	O
being	O
over	O
their	O
common	O
sample	B
space	O
y	O
is	O
always	O
nonnegative	O
equaling	O
zero	O
only	O
if	O
and	O
are	O
the	O
same	O
in	O
general	O
does	O
not	O
equal	O
deviance	B
does	O
not	O
depend	O
on	O
how	O
the	O
two	O
densities	O
are	O
named	O
for	O
example	O
having	O
the	O
same	O
expression	O
as	O
the	O
binomial	B
entry	O
in	O
table	O
in	O
what	O
follows	O
it	O
will	O
sometimes	O
be	O
useful	O
to	O
label	O
the	O
family	O
by	O
its	O
expectation	B
parameter	I
d	O
rather	O
than	O
by	O
the	O
natural	B
parameter	I
d	O
meaning	O
the	O
same	O
thing	O
as	O
only	O
the	O
names	O
attached	O
to	O
the	O
individual	O
family	O
members	O
being	O
changed	O
in	O
this	O
notation	O
it	O
is	O
easy	O
to	O
show	O
a	O
fundamental	O
result	O
sometimes	O
known	O
as	O
hoeffding	O
s	O
lemma	O
the	O
maximum	B
likelihood	B
estimate	B
of	O
given	O
y	O
is	O
y	O
itself	O
and	O
the	O
log	O
likelihood	B
log	O
decreases	O
from	O
its	O
maximum	O
log	O
fy	O
y	O
by	O
an	O
amount	O
that	O
depends	O
on	O
the	O
deviance	B
d	O
y	O
d	O
fy	O
ye	O
generalized	O
linear	B
models	B
returning	O
to	O
the	O
glm	O
framework	O
parameter	O
vector	B
gives	O
d	O
x	O
which	O
in	O
turn	O
gives	O
the	O
vector	B
of	O
expectation	O
parameters	O
d	O
for	O
instance	O
d	O
for	O
the	O
poisson	B
family	O
multiplying	O
hoeffding	O
s	O
lemma	O
over	O
the	O
n	O
cases	O
y	O
d	O
yn	O
yields	O
d	O
ny	O
f	O
d	O
ny	O
minimizes	O
the	O
total	O
deviancepn	O
this	O
has	O
an	O
important	O
consequence	O
the	O
mle	B
o	O
is	O
the	O
choice	O
of	O
that	O
d	O
yi	O
as	O
in	O
figure	O
glm	O
maximum	B
likelihood	B
fitting	O
is	O
least	O
total	O
deviance	B
in	O
the	O
same	O
way	O
that	O
ordinary	O
linear	B
regression	B
is	O
least	O
sum	O
of	O
squares	O
d	O
yi	O
fyi	O
e	O
the	O
inner	O
circle	O
of	O
figure	O
represents	O
normal	B
theory	B
the	O
preferred	O
venue	O
of	O
classical	O
applied	O
statistics	B
exact	B
inferences	I
t-tests	O
f	O
distributions	O
most	O
of	O
multivariate	B
analysis	B
were	O
feasible	O
within	O
the	O
circle	O
outside	O
the	O
circle	O
was	O
a	O
general	O
theory	B
based	O
mainly	O
on	O
asymptotic	O
approximations	O
involving	O
taylor	O
expansions	O
and	O
the	O
central	B
limit	I
theorem	B
figure	O
three	O
levels	O
of	O
statistical	O
modeling	O
a	O
few	O
useful	O
exact	O
results	O
lay	O
outside	O
the	O
normal	B
theory	B
circle	O
relating	O
normal	B
theoryexact	O
calculationsexponential	O
familiespartly	O
exactgeneral	O
theoryasymptoticsfigure	O
three	O
levels	O
of	O
statistical	O
modeling	O
glms	O
and	O
regression	B
trees	B
to	O
a	O
few	O
special	O
families	O
the	O
binomial	B
poisson	B
gamma	B
beta	B
and	O
others	O
less	O
well	O
known	O
exponential	B
family	I
theory	B
the	O
second	O
circle	O
in	O
figure	O
unified	O
the	O
special	O
cases	O
into	O
a	O
coherent	O
whole	O
it	O
has	O
a	O
partly	O
exact	O
flavor	O
with	O
some	O
ideal	O
counterparts	O
to	O
normal	B
theory	B
convex	O
likelihood	B
surfaces	O
least	O
deviance	B
regression	B
but	O
with	O
some	O
approximations	O
necessary	O
as	O
in	O
even	O
the	O
approximations	O
though	O
are	O
often	O
more	O
convincing	O
than	O
those	O
of	O
general	O
theory	B
exponential	O
families	O
fixed-dimension	O
sufficient	O
statistics	B
making	O
the	O
asymptotics	B
more	O
transparent	O
logistic	B
regression	B
has	O
banished	O
its	O
predecessors	O
as	O
probit	B
analysis	B
almost	O
entirely	O
from	O
the	O
field	O
and	O
not	O
only	O
because	O
of	O
estimating	O
efficiencies	O
and	O
computational	O
advantages	O
are	O
actually	O
rather	O
modest	O
but	O
also	O
because	O
it	O
is	O
seen	O
as	O
a	O
clearer	O
analogue	O
to	O
ordinary	O
least	B
squares	I
our	O
dependable	O
standby	O
glm	O
research	O
development	O
has	O
been	O
mostly	O
frequentist	B
but	O
with	O
a	O
substantial	O
admixture	O
of	O
likelihoodbased	O
reasoning	O
and	O
a	O
hint	O
of	O
fisher	B
s	O
logic	B
of	I
inductive	B
inference	B
helping	O
the	O
statistician	O
choose	O
between	O
competing	O
methodologies	O
is	O
the	O
job	O
of	O
statistical	O
inference	B
in	O
the	O
case	O
of	O
generalized	O
linear	B
models	B
the	O
choice	O
has	O
been	O
made	O
at	O
least	O
partly	O
in	O
terms	O
of	O
aesthetics	O
as	O
well	O
as	O
philosophy	O
poisson	B
regression	B
the	O
third	O
most-used	O
member	O
of	O
the	O
glm	O
family	O
after	O
normal	B
theory	B
least	B
squares	I
and	O
logistic	B
regression	B
is	O
poisson	B
regression	B
n	O
independent	O
poisson	B
variates	O
are	O
observed	O
i	O
d	O
n	O
where	O
d	O
log	O
is	O
assumed	O
to	O
follow	O
a	O
linear	B
model	I
yi	O
d	O
x	O
where	O
x	O
is	O
a	O
known	O
n	O
p	O
structure	B
matrix	B
and	O
an	O
unknown	O
p-vector	O
i	O
for	O
i	O
d	O
n	O
where	O
x	O
of	O
regression	B
coefficients	O
that	O
is	O
d	O
x	O
is	O
the	O
ith	O
row	O
of	O
x	O
in	O
the	O
chapters	O
that	O
follow	O
we	O
will	O
see	O
poisson	B
regression	B
come	O
to	O
the	O
rescue	O
in	O
what	O
at	O
first	O
appear	O
to	O
be	O
awkward	O
data-analytic	O
situations	O
here	O
we	O
will	O
settle	O
for	O
an	O
example	O
involving	O
density	B
estimation	B
from	O
a	O
spatially	O
truncated	O
sample	B
i	O
table	O
shows	O
galaxy	B
counts	O
from	O
a	O
small	O
portion	O
of	O
the	O
sky	O
galaxies	O
have	O
had	O
their	O
redshifts	O
r	B
and	O
apparent	O
magnitudes	O
m	O
measured	O
poisson	B
regression	B
table	O
counts	O
for	O
a	O
truncated	O
sample	B
of	O
galaxies	O
binned	O
by	O
redshift	O
and	O
magnitude	O
redshift	O
magnitude	O
distance	O
from	O
earth	O
is	O
an	O
increasing	O
function	B
of	O
r	B
while	O
apparent	O
brightness	O
is	O
a	O
decreasing	O
of	O
m	O
in	O
this	O
survey	O
counts	O
were	O
limited	O
to	O
galaxies	O
having	O
r	B
and	O
m	O
the	O
upper	O
limit	O
reflecting	O
the	O
difficulty	O
of	O
measuring	O
very	O
dim	O
galaxies	O
the	O
range	O
of	O
log	O
r	B
has	O
been	O
divided	O
into	O
equal	O
intervals	B
and	O
likewise	O
equal	O
intervals	B
for	O
m	O
table	O
gives	O
the	O
counts	O
of	O
the	O
galaxies	O
in	O
the	O
d	O
bins	O
lower	O
right	O
corner	O
of	O
the	O
table	O
is	O
empty	O
because	O
distant	O
galaxies	O
always	O
appear	O
dim	O
the	O
multinomialpoisson	O
connection	O
helps	O
motivate	O
model	O
picturing	O
the	O
table	O
as	O
a	O
multinomial	O
observation	O
on	O
categories	O
in	O
which	O
the	O
sample	B
size	I
n	O
was	O
itself	O
poisson	B
we	O
can	O
imagine	O
table	O
as	O
a	O
small	O
portion	O
of	O
a	O
much	O
more	O
extensive	O
table	O
hypothetically	O
available	O
if	O
the	O
data	B
were	O
not	B
truncated	I
experience	O
suggests	O
that	O
we	O
might	O
then	O
fit	O
an	O
appropriate	O
bivariate	B
normal	B
density	B
to	O
the	O
data	B
as	O
in	O
figure	O
it	O
seems	O
like	O
it	O
might	O
be	O
awkward	O
to	O
fit	O
part	O
of	O
a	O
bivariate	B
normal	B
density	B
to	O
truncated	O
data	B
but	O
poisson	B
regression	B
offers	O
an	O
easy	O
solution	O
an	O
object	O
of	O
the	O
second	O
magnitude	O
is	O
less	O
bright	O
than	O
one	O
of	O
the	O
first	O
and	O
so	O
on	O
a	O
classification	O
system	O
owing	O
to	O
the	O
greeks	O
glms	O
and	O
regression	B
trees	B
let	O
r	B
be	O
the	O
listing	O
the	O
values	O
of	O
r	B
in	O
each	O
bin	O
of	O
the	O
table	O
column	O
order	O
and	O
likewise	O
m	O
for	O
the	O
m	O
values	O
for	O
instance	O
m	O
d	O
repeated	O
times	O
and	O
define	O
the	O
matrix	B
x	O
as	O
x	O
d	O
r	B
m	O
r	B
rm	O
where	O
r	B
is	O
the	O
vector	B
whose	O
components	O
are	O
the	O
square	O
of	O
r	B
s	O
etc	O
the	O
log	O
density	B
of	O
a	O
bivariate	B
normal	B
distribution	B
in	O
m	O
is	O
of	O
the	O
form	B
c	O
c	O
c	O
c	O
agreeing	O
with	O
log	O
d	O
x	O
i	O
as	O
specified	O
by	O
we	O
can	O
use	O
a	O
poisson	B
glm	O
with	O
yi	O
the	O
ith	O
bin	O
s	O
count	O
to	O
estimate	B
the	O
portion	O
of	O
our	O
hypothesized	O
bivariate	B
normal	B
distribution	B
in	O
the	O
truncation	O
region	B
figure	O
left	O
galaxy	B
data	B
binned	O
counts	O
right	O
poisson	B
glm	O
density	B
estimate	B
the	O
left	O
panel	O
of	O
figure	O
is	O
a	O
perspective	O
picture	O
of	O
the	O
raw	O
counts	O
in	O
table	O
on	O
the	O
right	O
is	O
the	O
fitted	O
density	B
from	O
the	O
poisson	B
regression	B
irrespective	O
of	O
density	B
estimation	B
poisson	B
regression	B
has	O
done	O
a	O
useful	O
job	O
of	O
smoothing	B
the	O
raw	O
bin	O
counts	O
contours	O
of	O
equal	O
value	O
of	O
the	O
fitted	O
log	O
density	B
o	O
c	O
o	O
c	O
o	O
c	O
o	O
c	O
o	O
c	O
o	O
are	O
shown	O
in	O
figure	O
one	O
can	O
imagine	O
the	O
contours	O
as	O
truncated	O
portions	O
of	O
ellipsoids	O
of	O
the	O
type	O
shown	O
in	O
figure	O
the	O
right	O
panel	O
of	O
figure	O
makes	O
it	O
clear	O
that	O
we	O
are	O
nowhere	O
near	O
the	O
center	O
of	O
the	O
hypothetical	O
bivariate	B
normal	B
density	B
which	O
must	O
lie	O
well	O
beyond	O
our	O
dimness	O
limit	O
dimmerfarthercountdimmerfartherdensity	O
poisson	B
regression	B
figure	O
contour	O
curves	O
for	O
poisson	B
glm	O
density	B
estimate	B
for	O
the	O
galaxy	B
data	B
the	O
red	O
dot	O
shows	O
the	O
point	O
of	O
maximum	O
density	B
the	O
poisson	B
deviance	B
residual	I
z	O
between	O
an	O
observed	O
count	O
y	O
and	O
a	O
fitted	O
value	O
is	O
z	O
d	O
sign	O
y	O
tist	O
glm	O
theory	B
says	O
that	O
s	O
dp	O
with	O
d	O
the	O
poisson	B
deviance	B
from	O
table	O
zj	O
k	O
the	O
deviance	B
residual	I
between	O
the	O
count	O
yij	O
in	O
the	O
ij	O
th	O
bin	O
of	O
table	O
and	O
the	O
fitted	O
value	O
k	O
from	O
the	O
poisson	B
glm	O
was	O
calculated	O
for	O
all	O
bins	O
standard	O
frequenj	O
k	O
should	O
be	O
about	O
if	O
the	O
bivariate	B
normal	B
model	O
is	O
actually	O
the	O
fit	O
was	O
poor	O
s	O
d	O
in	O
practice	O
we	O
might	O
try	O
adding	O
columns	O
to	O
x	O
in	O
e	O
g	O
or	O
r	B
improving	O
the	O
fit	O
where	O
it	O
was	O
worst	O
near	O
the	O
boundaries	O
of	O
the	O
table	O
chapter	O
demonstrates	O
some	O
other	O
examples	O
of	O
poisson	B
density	B
estimation	B
in	O
general	O
poisson	B
glms	O
reduce	O
density	B
estimation	B
to	O
regression	B
model	I
fitting	O
a	O
familiar	O
and	O
flexible	O
inferential	O
technology	O
j	O
k	O
this	O
is	O
a	O
modern	O
version	O
of	O
the	O
classic	O
chi-squared	O
goodness-of-fit	O
test	O
l	O
glms	O
and	O
regression	B
trees	B
regression	B
trees	B
the	O
data	B
set	B
d	O
for	O
a	O
regression	B
problem	O
typically	O
consists	O
of	O
n	O
pairs	O
yi	O
d	O
d	O
f	O
xi	O
yi	O
i	O
d	O
ng	O
where	O
xi	O
is	O
a	O
vector	B
of	O
predictors	B
or	O
covariates	O
taking	O
its	O
value	O
in	O
some	O
space	O
x	O
and	O
yi	O
is	O
the	O
response	B
assumed	O
to	O
be	O
univariate	O
in	O
what	O
follows	O
the	O
regression	B
algorithm	B
perhaps	O
a	O
poisson	B
glm	O
inputs	O
d	O
and	O
outputs	O
a	O
rule	B
rd	O
for	O
any	O
value	O
of	O
x	O
in	O
x	O
rd	O
produces	O
an	O
estimate	B
oy	O
for	O
a	O
possible	O
future	O
value	O
of	O
y	O
oy	O
d	O
rd	O
in	O
the	O
logistic	B
regression	B
example	O
rd	O
is	O
there	O
are	O
three	O
principal	O
uses	O
for	O
the	O
rule	B
rd	O
for	O
prediction	O
given	O
a	O
new	O
observation	O
of	O
x	O
but	O
not	O
of	O
its	O
corresponding	O
y	O
we	O
use	O
oy	O
d	O
rd	O
to	O
predict	O
y	O
in	O
the	O
spam	B
example	O
the	O
keywords	O
of	O
an	O
incoming	O
message	O
could	O
be	O
used	O
to	O
predict	O
whether	O
or	O
not	O
it	O
is	O
chapter	O
for	O
estimation	B
the	O
rule	B
rd	O
describes	O
a	O
regression	B
surface	O
os	O
over	O
x	O
the	O
right	O
panel	O
of	O
figure	O
shows	O
os	O
for	O
the	O
galaxy	B
example	O
os	O
can	O
be	O
thought	O
of	O
as	O
estimating	O
s	O
the	O
true	O
regression	B
surface	O
often	O
defined	O
in	O
the	O
form	B
of	O
conditional	B
expectation	O
os	O
d	O
frd	O
x	O
g	O
x	O
s	O
d	O
fefyjxg	O
x	O
g	O
x	O
x	O
g	O
a	O
dichotomous	O
situation	O
where	O
y	O
is	O
coded	O
as	O
or	O
s	O
d	O
fprfy	O
d	O
x	O
for	O
estimation	B
but	O
not	O
necessarily	O
for	O
prediction	O
we	O
want	O
os	O
to	O
accurately	O
portray	O
s	O
the	O
right	O
panel	O
of	O
figure	O
shows	O
the	O
estimated	O
galaxy	B
density	B
still	O
increasing	O
monotonically	O
in	O
dimmer	O
at	O
the	O
top	O
end	O
of	O
the	O
truncation	O
region	B
but	O
not	O
so	O
in	O
farther	O
perhaps	O
an	O
important	O
clue	O
for	O
directing	O
future	O
search	O
the	O
flat	O
region	B
in	O
the	O
kidney	B
function	B
regression	B
curve	O
of	O
figure	O
makes	O
almost	O
no	O
difference	O
to	O
prediction	O
but	O
is	O
of	O
scientific	O
interest	O
if	O
accurate	O
prediction	O
of	O
dichotomous	O
outcomes	O
is	O
often	O
called	O
classification	O
physicists	O
call	O
a	O
regression-based	O
search	O
for	O
new	O
objects	O
bump	O
hunting	O
regression	B
trees	B
for	O
explanation	O
the	O
predictors	B
for	O
the	O
diabetes	B
data	B
of	O
section	O
age	O
sex	O
bmi	O
were	O
selected	O
by	O
the	O
researcher	O
in	O
the	O
hope	O
of	O
explaining	O
the	O
etiology	O
of	O
diabetes	B
progression	O
the	O
relative	O
contribution	O
of	O
the	O
different	O
predictors	B
to	O
rd	O
is	O
then	O
of	O
interest	O
how	O
the	O
regression	B
surface	O
is	O
composed	O
is	O
of	O
prime	O
concern	O
in	O
this	O
use	O
but	O
not	O
in	O
use	O
or	O
above	O
the	O
three	O
different	O
uses	O
of	O
rd	O
raise	O
different	O
inferential	O
questions	O
use	O
calls	O
for	O
estimates	O
of	O
prediction	O
error	O
in	O
a	O
dichotomous	O
situation	O
such	O
as	O
the	O
spam	B
study	O
we	O
would	O
want	O
to	O
know	O
both	O
error	O
probabilities	B
prf	O
oy	O
d	O
spamjy	O
d	O
hamg	O
and	O
prf	O
oy	O
d	O
hamjy	O
d	O
spamg	O
for	O
estimation	B
the	O
accuracy	B
of	O
rd	O
as	O
a	O
function	B
of	O
x	O
perhaps	O
in	O
standard	B
deviation	I
terms	O
sd	O
x	O
d	O
sd	O
oyjx	O
would	O
tell	O
how	O
closely	O
os	O
approximates	O
s	O
use	O
explanation	O
requires	O
more	O
elaborate	O
inferential	O
tools	O
saying	O
for	O
example	O
which	O
of	O
the	O
regression	B
coefficients	O
i	O
in	O
can	O
safely	O
be	O
set	B
to	O
zero	O
figure	O
left	O
a	O
hypothetical	O
regression	B
tree	I
based	O
on	O
two	O
predictors	B
and	O
right	O
corresponding	O
regression	B
surface	O
regression	B
trees	B
use	O
a	O
simple	O
but	O
intuitively	O
appealing	O
technique	O
to	O
form	B
a	O
regression	B
surface	O
recursive	O
partitioning	O
the	O
left	O
panel	O
of	O
figure	O
illustrates	O
the	O
method	B
for	O
a	O
hypothetical	O
situation	O
involving	O
two	O
predictor	B
variables	O
and	O
r	B
and	O
m	O
in	O
the	O
galaxy	B
example	O
at	O
the	O
top	O
of	O
glms	O
and	O
regression	B
trees	B
the	O
tree	O
the	O
sample	B
population	O
of	O
n	O
cases	O
has	O
been	O
split	O
into	O
two	O
groups	O
those	O
with	O
equal	O
to	O
or	O
less	O
than	O
value	O
go	O
to	O
the	O
left	O
those	O
with	O
to	O
the	O
right	O
the	O
leftward	O
group	O
is	O
itself	O
then	O
divided	O
into	O
two	O
groups	O
depending	O
on	O
whether	O
or	O
not	O
the	O
division	O
stops	O
there	O
leaving	O
two	O
terminal	O
nodes	B
and	O
on	O
the	O
tree	O
s	O
right	O
side	O
two	O
other	O
splits	O
give	O
terminal	O
nodes	B
and	O
a	O
prediction	O
value	O
oyrj	O
is	O
attached	O
to	O
each	O
terminal	B
node	I
rj	O
the	O
prediction	O
oy	O
applying	O
to	O
a	O
new	O
observation	O
x	O
d	O
is	O
calculated	O
by	O
starting	O
x	O
at	O
the	O
top	O
of	O
the	O
tree	O
and	O
following	O
the	O
splits	O
downward	O
until	O
a	O
terminal	B
node	I
and	O
its	O
attached	O
prediction	O
oyrj	O
is	O
reached	O
the	O
corresponding	O
regression	B
surface	O
os	O
is	O
shown	O
in	O
the	O
right	O
panel	O
of	O
figure	O
the	O
oyrj	O
happen	O
to	O
be	O
in	O
ascending	O
order	O
various	O
algorithmic	O
rules	O
are	O
used	O
to	O
decide	O
which	O
variable	O
to	O
split	O
and	O
which	O
splitting	O
value	O
t	B
to	O
take	O
at	O
each	O
step	O
of	O
the	O
tree	O
s	O
construction	O
here	O
is	O
the	O
most	O
common	O
method	B
suppose	O
at	O
step	O
k	O
of	O
the	O
algorithm	B
groupk	O
of	O
nk	O
cases	O
remains	O
to	O
be	O
split	O
those	O
cases	O
having	O
mean	O
and	O
sum	O
of	O
squares	O
yi	O
and	O
k	O
mk	O
d	O
x	O
d	O
x	O
dividing	O
groupk	O
into	O
groupkleft	O
and	O
groupkright	O
produces	O
means	O
mkleft	O
and	O
mkright	O
and	O
corresponding	O
sums	O
of	O
squares	O
kright	O
the	O
algorithm	B
proceeds	O
by	O
choosing	O
the	O
splitting	O
variable	O
xk	O
and	O
the	O
threshold	O
tk	O
to	O
minimize	O
kleft	O
and	O
c	O
kright	O
kleft	O
in	O
other	O
words	O
it	O
splits	O
groupk	O
into	O
two	O
groups	O
that	O
are	O
as	O
different	O
from	O
each	O
other	O
as	O
possible	O
cross-validation	B
estimates	O
of	O
prediction	O
error	O
chapter	O
are	O
used	O
to	O
decide	O
when	O
the	O
splitting	O
process	O
should	O
stop	O
if	O
groupk	O
is	O
not	O
to	O
be	O
further	O
divided	O
it	O
becomes	O
terminal	B
node	I
rk	O
with	O
prediction	O
value	O
oyrk	O
d	O
mk	O
none	O
of	O
this	O
would	O
be	O
feasible	O
without	O
electronic	O
computation	O
but	O
even	O
quite	O
large	O
prediction	O
problems	O
can	O
be	O
short	O
work	O
for	O
modern	O
computers	O
figure	O
shows	O
a	O
regression	B
tree	I
of	O
the	O
spam	B
data	B
table	O
there	O
are	O
seven	O
terminal	O
nodes	B
labeled	O
or	O
for	O
decision	O
ham	O
or	O
spam	B
the	O
leftmost	O
node	O
say	O
is	O
a	O
and	O
contains	O
ham	O
cases	O
and	O
spam	B
with	O
and	O
in	O
the	O
full	B
data	B
set	B
starting	O
at	O
the	O
top	O
of	O
the	O
tree	O
is	O
reached	O
if	O
it	O
has	O
a	O
low	O
proportion	B
of	O
symbols	O
using	O
the	O
r	B
program	O
rpart	O
in	O
classification	O
mode	O
employing	O
a	O
different	O
splitting	O
rule	B
than	O
the	O
version	O
based	O
on	O
regression	B
trees	B
figure	O
regression	B
tree	I
on	O
the	O
spam	B
data	B
d	O
ham	O
d	O
spam	B
error	O
rates	O
ham	O
spam	B
captions	O
indicate	O
leftward	O
moves	O
char	O
a	O
low	O
proportion	B
of	O
the	O
word	O
remove	O
and	O
a	O
low	O
proportion	B
of	O
exclamation	O
marks	O
char	O
regression	B
trees	B
are	O
easy	O
to	O
interpret	O
too	O
many	O
dollar	O
signs	O
means	O
spam	B
seemingly	O
suiting	O
them	O
for	O
use	O
explanation	O
unfortunately	O
they	O
are	O
also	O
easy	O
to	O
overinterpret	O
with	O
a	O
reputation	O
for	O
being	O
unstable	O
in	O
practice	O
discontinuous	O
regression	B
surfaces	O
os	O
as	O
in	O
figure	O
disqualify	O
them	O
for	O
use	O
estimation	B
their	O
principal	O
use	O
in	O
what	O
follows	O
will	O
be	O
as	O
key	O
parts	O
of	O
prediction	O
algorithms	O
use	O
the	O
tree	O
in	O
figure	O
has	O
apparent	B
error	I
rates	O
of	O
and	O
this	O
can	O
be	O
much	O
improved	O
upon	O
by	O
bagging	B
aggregation	O
chapters	O
and	O
and	O
by	O
other	O
computer-intensive	B
techniques	O
compared	O
with	O
generalized	O
linear	B
models	B
regression	B
trees	B
represent	O
a	O
break	O
from	O
classical	O
methodology	O
that	O
is	O
more	O
stark	O
first	O
of	O
all	O
they	O
are	O
totally	O
nonparametric	B
bigger	O
but	O
less	O
structured	O
data	B
sets	O
have	O
promoted	O
nonparametrics	O
in	O
twenty-first-century	O
statistics	B
regression	B
trees	B
are	O
more	O
computer-intensive	B
and	O
less	O
efficient	O
than	O
glms	O
but	O
as	O
will	O
be	O
seen	O
in	O
part	O
iii	O
the	O
availability	O
of	O
massive	O
data	B
sets	O
and	O
modern	O
computational	O
equip	O
regression	B
tree	I
spam	B
data	B
rates	O
nonspam	O
spam	B
indicate	O
leftward	O
moves	O
glms	O
and	O
regression	B
trees	B
ment	O
has	O
diminished	O
the	O
appeal	O
of	O
efficiency	O
in	O
favor	O
of	O
easy	O
assumptionfree	O
application	O
notes	O
and	O
details	O
computer-age	O
algorithms	O
depend	O
for	O
their	O
utility	O
on	O
statistical	O
computing	O
languages	O
after	O
a	O
period	O
of	O
evolution	O
the	O
language	O
s	O
et	O
al	O
and	O
its	O
open-source	O
successor	O
r	B
core	O
team	O
have	O
come	O
to	O
dominate	O
applied	O
generalized	O
linear	B
models	B
are	O
available	O
from	O
a	O
single	O
r	B
command	O
e	O
g	O
for	O
logistic	B
regression	B
and	O
hastie	O
and	O
similarly	O
for	O
regression	B
trees	B
and	O
hundreds	O
of	O
other	O
applications	O
the	O
classic	O
version	O
of	O
bioassay	B
probit	B
analysis	B
assumes	O
that	O
each	O
test	O
animal	O
has	O
its	O
own	O
lethal	O
dose	O
level	O
x	O
and	O
that	O
the	O
population	O
distribution	B
of	O
x	O
is	O
normal	B
prfx	O
xg	O
d	O
c	O
for	O
unknown	O
parameters	O
and	O
standard	O
normal	B
cdf	B
then	O
the	O
number	O
of	O
animals	O
dying	O
at	O
dose	O
x	O
is	O
binomial	B
bi	O
nx	O
as	O
in	O
with	O
d	O
c	O
or	O
d	O
c	O
replacing	O
the	O
standard	O
normal	B
cdf	B
with	O
the	O
logistic	O
cdf	B
c	O
e	O
resembles	O
changes	O
into	O
logistic	B
regression	B
the	O
usual	O
goal	O
of	O
bioassay	B
was	O
to	O
estimate	B
the	O
dose	O
lethal	O
to	O
of	O
the	O
test	O
population	O
it	O
is	O
indicated	O
by	O
the	O
open	O
circle	O
in	O
figure	O
cox	O
the	O
classic	O
text	O
on	O
logistic	B
regression	B
lists	O
berkson	O
as	O
an	O
early	O
practitioner	O
wedderburn	O
is	O
credited	O
with	O
generalized	O
linear	B
models	B
in	O
mccullagh	O
and	O
nelder	O
s	O
influential	O
text	O
of	O
that	O
name	O
first	O
edition	O
birch	O
developed	O
an	O
important	O
and	O
suggestive	O
special	O
case	O
of	O
glm	O
theory	B
the	O
twenty-first	O
century	O
has	O
seen	O
an	O
efflorescence	O
of	O
computer-based	O
regression	B
techniques	O
as	O
described	O
extensively	O
in	O
hastie	O
et	O
al	O
the	O
discussion	O
of	O
regression	B
trees	B
here	O
is	O
taken	O
from	O
their	O
section	O
including	O
our	O
figure	O
they	O
use	O
the	O
spam	B
data	B
as	O
a	O
central	O
example	O
it	O
is	O
publicly	O
previous	O
computer	O
packages	B
such	O
as	O
sas	B
and	O
spss	B
continue	O
to	O
play	O
a	O
major	O
role	O
in	O
application	O
areas	O
such	O
as	O
the	O
social	O
sciences	O
biomedical	O
statistics	B
and	O
the	O
pharmaceutical	O
industry	O
notes	O
and	O
details	O
available	O
at	O
ftp	O
ics	O
uci	O
edu	O
breiman	O
et	O
al	O
propelled	O
regression	B
trees	B
into	O
wide	O
use	O
with	O
their	O
cart	O
algorithm	B
sufficiency	O
as	O
in	O
the	O
fisher	B
neyman	O
criterion	O
says	O
that	O
if	O
f	O
d	O
h	O
when	O
does	O
not	O
depend	O
on	O
then	O
s	O
x	O
is	O
sufficient	O
for	O
equation	B
from	O
we	O
have	O
the	O
log	O
likelihood	B
function	B
with	O
sufficient	O
statistic	B
z	O
d	O
x	O
ing	O
with	O
respect	O
to	O
l	O
d	O
z	O
y	O
and	O
dpn	O
i	O
differentiat	O
y	O
x	O
p	O
l	O
d	O
z	O
p	O
d	O
x	O
where	O
we	O
have	O
used	O
d	O
so	O
says	O
p	O
matrix	B
r	B
l	O
with	O
respect	O
to	O
is	O
l	O
d	O
x	O
verifying	O
the	O
mle	B
equation	B
i	O
d	O
x	O
i	O
but	O
concavity	B
of	O
the	O
log	O
likelihood	B
from	O
the	O
second	O
derivative	O
but	O
z	O
d	O
x	O
r	B
d	O
cov	O
y	O
has	O
cov	O
d	O
x	O
a	O
positive	O
definite	O
p	O
p	O
matrix	B
verifying	O
the	O
concavity	B
of	O
l	O
in	O
fact	O
applies	O
to	O
any	O
exponential	B
family	I
not	O
only	O
glms	O
formula	B
the	O
sufficient	O
statistic	B
z	O
has	O
mean	O
vector	B
and	O
co	O
variance	B
matrix	B
z	O
v	O
using	O
the	O
with	O
d	O
e	O
fzg	O
and	O
v	O
d	O
x	O
first-order	O
taylor	B
series	I
for	O
o	O
as	O
a	O
function	B
of	O
z	O
is	O
c	O
v	O
o	O
rather	O
taken	O
literally	O
gives	O
in	O
the	O
ols	O
formula	B
we	O
have	O
than	O
since	O
the	O
natural	B
parameter	I
for	O
the	O
normal	B
entry	O
in	O
table	O
is	O
formula	B
this	O
formula	B
attributed	O
to	O
hoeffding	O
is	O
a	O
key	O
result	O
in	O
the	O
interpretation	O
of	O
glm	O
fitting	O
applying	O
definition	O
glms	O
and	O
regression	B
trees	B
to	O
family	O
gives	O
d	O
g	O
d	O
if	O
is	O
the	O
mle	B
o	O
d	O
d	O
log	O
d	O
y	O
d	O
y	O
then	O
d	O
y	O
the	O
maximum	B
likelihood	B
equation	B
y	O
i	O
d	O
for	O
any	O
choice	O
of	O
but	O
the	O
right-hand	O
side	O
of	O
is	O
log	O
verifying	O
table	O
the	O
galaxy	B
counts	O
are	O
from	O
loh	O
and	O
spillar	O
s	O
redshift	O
survey	O
as	O
discussed	O
in	O
efron	O
and	O
petrosian	O
criteria	B
abbreviating	O
left	O
and	O
right	O
by	O
l	O
and	O
r	B
we	O
have	O
d	O
kl	O
c	O
kr	O
k	O
c	O
nkl	O
nkr	O
nk	O
mkr	O
with	O
nkl	O
and	O
nkr	O
the	O
subgroup	O
sizes	O
showing	O
that	O
minimizing	O
is	O
the	O
same	O
as	O
maximizing	O
the	O
last	O
term	O
in	O
intuitively	O
a	O
good	O
split	O
is	O
one	O
that	O
makes	O
the	O
left	O
and	O
right	O
groups	O
as	O
different	O
as	O
possible	O
the	O
ideal	O
being	O
all	O
on	O
the	O
left	O
and	O
all	O
on	O
the	O
right	O
making	O
the	O
terminal	O
nodes	B
pure	O
in	O
some	O
cases	O
is	O
undefined	O
for	O
example	O
when	O
y	O
d	O
for	O
a	O
poisson	B
response	B
d	O
log	O
y	O
which	O
is	O
undefined	O
but	O
in	O
we	O
assume	O
that	O
d	O
similarly	O
for	O
binary	O
y	O
and	O
the	O
binomial	B
family	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
survival	B
analysis	B
had	O
its	O
roots	O
in	O
governmental	O
and	O
actuarial	O
statistics	B
spanning	O
centuries	O
of	O
use	O
in	O
assessing	O
life	O
expectancies	O
insurance	B
rates	O
and	O
annuities	O
in	O
the	O
years	O
between	O
and	O
survival	B
analysis	B
was	O
adapted	O
by	O
statisticians	O
for	O
application	O
to	O
biomedical	O
studies	O
three	O
of	O
the	O
most	O
popular	O
post-war	O
statistical	O
methodologies	O
emerged	O
during	O
this	O
period	O
the	O
kaplan	O
meier	O
estimate	B
the	O
log-rank	O
and	O
cox	O
s	O
proportional	B
hazards	I
model	I
the	O
succession	O
showing	O
increased	O
computational	O
demands	O
along	O
with	O
increasingly	O
sophisticated	O
inferential	O
justification	O
a	O
connection	O
with	O
one	O
of	O
fisher	B
s	O
ideas	O
on	O
maximum	B
likelihood	B
estimation	B
leads	O
in	O
the	O
last	O
section	O
of	O
this	O
chapter	O
to	O
another	O
statistical	O
method	B
that	O
has	O
gone	O
platinum	O
the	O
em	B
algorithm	B
life	O
tables	O
and	O
hazard	O
rates	O
an	O
insurance	B
company	O
s	O
life	B
table	I
appears	O
in	O
table	O
showing	O
its	O
number	O
of	O
clients	O
is	O
life	O
insurance	B
policy	O
holders	O
by	O
age	O
and	O
the	O
number	O
of	O
deaths	O
during	O
the	O
past	O
year	O
in	O
each	O
age	O
for	O
example	O
five	O
deaths	O
among	O
the	O
clients	O
aged	O
the	O
column	O
labeled	O
os	O
is	O
of	O
great	O
interest	O
to	O
the	O
company	O
s	O
actuaries	O
who	O
have	O
to	O
set	B
rates	O
for	O
new	O
policy	O
holders	O
it	O
is	O
an	O
estimate	B
of	O
survival	O
probability	O
probability	O
of	O
a	O
person	O
aged	O
beginning	O
of	O
the	O
table	O
surviving	O
past	O
age	O
etc	O
os	O
is	O
calculated	O
according	O
to	O
an	O
ancient	O
but	O
ingenious	O
algorithm	B
let	O
x	O
represent	O
a	O
typical	O
lifetime	O
so	O
fi	O
d	O
prfx	O
d	O
ig	O
also	O
known	O
as	O
the	O
mantel	O
haenszel	O
or	O
cochran	O
mantel	O
haenszel	O
test	O
the	O
insurance	B
company	O
is	O
fictitious	O
but	O
the	O
deaths	O
y	O
are	O
based	O
on	O
the	O
true	O
rates	O
for	O
us	O
men	O
per	O
social	O
security	O
administration	O
data	B
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
h	O
d	O
hazard	B
rate	B
yn	O
os	O
d	O
age	O
n	O
age	O
oh	O
oh	O
table	O
insurance	B
company	O
life	B
table	I
at	O
each	O
age	O
n	O
d	O
number	O
of	O
policy	O
holders	O
y	O
d	O
number	O
of	O
deaths	O
o	O
survival	O
probability	O
estimate	B
os	O
os	O
n	O
y	O
y	O
is	O
the	O
probability	O
of	O
dying	O
at	O
age	O
i	O
and	O
si	O
dx	O
fj	O
d	O
prfx	O
ig	O
is	O
the	O
probability	O
of	O
surviving	O
past	O
age	O
i	O
the	O
hazard	B
rate	B
at	O
age	O
i	O
is	O
by	O
life	O
tables	O
and	O
hazard	O
rates	O
definition	O
sij	O
d	O
jy	O
hi	O
d	O
fi	O
d	O
prfx	O
d	O
ijx	O
ig	O
the	O
probability	O
of	O
dying	O
at	O
age	O
i	O
given	O
survival	O
past	O
age	O
i	O
a	O
crucial	O
observation	O
is	O
that	O
the	O
probability	O
sij	O
of	O
surviving	O
past	O
age	O
j	O
given	O
survival	O
past	O
age	O
i	O
is	O
the	O
product	O
of	O
surviving	O
each	O
intermediate	O
year	O
hk	O
d	O
prfx	O
jjx	O
igi	O
kdi	O
first	O
you	O
have	O
to	O
survive	O
year	O
i	O
probability	O
hi	O
then	O
year	O
i	O
c	O
probability	O
etc	O
up	O
to	O
year	O
j	O
probability	O
hj	O
notice	O
that	O
si	O
equals	O
os	O
in	O
table	O
is	O
an	O
estimate	B
of	O
sij	O
for	O
i	O
d	O
first	O
each	O
hi	O
was	O
estimated	O
as	O
the	O
binomial	B
proportion	B
of	O
the	O
number	O
of	O
deaths	O
yi	O
among	O
the	O
ni	O
clients	O
and	O
then	O
we	O
set	B
o	O
hi	O
d	O
yi	O
o	O
d	O
jy	O
hk	O
the	O
insurance	B
company	O
doesn	O
t	B
have	O
to	O
wait	O
years	O
to	O
learn	O
the	O
probability	O
of	O
a	O
living	O
past	O
to	O
be	O
in	O
the	O
table	O
one	O
year	O
s	O
data	B
hazard	O
rates	O
are	O
more	O
often	O
described	O
in	O
terms	O
of	O
a	O
continuous	O
positive	O
random	O
variable	O
t	B
called	O
time	O
having	O
density	B
function	B
f	O
and	O
reverse	O
cdf	B
or	O
survival	O
function	B
s	O
t	B
dz	O
f	O
dx	O
d	O
prft	O
tg	O
the	O
hazard	B
rate	B
t	B
h	O
t	B
d	O
f	O
satisfies	O
prft	O
t	B
c	O
dt	O
tg	O
for	O
dt	O
in	O
analogy	O
with	O
the	O
analog	O
of	O
is	O
h	O
t	B
of	O
course	O
the	O
estimates	O
can	O
go	O
badly	O
wrong	O
if	O
the	O
hazard	O
rates	O
change	O
over	O
time	O
t	B
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
prft	O
d	O
exp	O
h	O
x	O
dx	O
so	O
in	O
particular	O
the	O
reverse	O
cdf	B
is	O
given	O
by	O
s	O
t	B
d	O
exp	O
h	O
x	O
dx	O
a	O
one-sided	O
exponential	O
density	B
f	O
d	O
for	O
t	B
has	O
s	O
t	B
d	O
and	O
constant	O
hazard	B
rate	B
h	O
t	B
d	O
the	O
name	O
memoryless	O
is	O
quite	O
appropriate	O
for	O
density	B
having	O
survived	O
to	O
any	O
time	O
t	B
the	O
probability	O
of	O
surviving	O
dt	O
units	O
more	O
is	O
always	O
the	O
same	O
about	O
dt	O
no	O
matter	O
what	O
t	B
is	O
if	O
human	O
lifetimes	O
were	O
exponential	O
there	O
wouldn	O
t	B
be	O
old	O
or	O
young	O
people	O
only	O
lucky	O
or	O
unlucky	O
ones	O
censored	O
data	B
and	O
the	O
kaplan	O
meier	O
estimate	B
table	O
reports	O
the	O
survival	O
data	B
from	O
a	O
randomized	O
clinical	O
trial	O
run	O
by	O
ncog	B
northern	O
california	O
oncology	O
group	O
comparing	O
two	O
treatments	O
for	O
head	O
and	O
neck	O
cancer	O
arm	O
a	O
chemotherapy	O
versus	O
arm	O
b	O
chemotherapy	O
plus	O
radiation	O
the	O
response	B
for	O
each	O
patient	O
is	O
survival	O
time	O
in	O
days	O
the	O
c	O
sign	O
following	O
some	O
entries	O
indicates	O
censored	O
data	B
that	O
is	O
survival	O
times	O
known	O
only	O
to	O
exceed	O
the	O
reported	O
value	O
these	O
are	O
patients	O
lost	O
to	O
followup	O
mostly	O
because	O
the	O
ncog	B
experiment	O
ended	O
with	O
some	O
of	O
the	O
patients	O
still	O
alive	O
this	O
is	O
what	O
the	O
experimenters	O
hoped	O
to	O
see	O
of	O
course	O
but	O
it	O
complicates	O
the	O
comparison	O
notice	O
that	O
there	O
is	O
more	O
censoring	O
in	O
arm	O
b	O
in	O
the	O
absence	O
of	O
censoring	O
we	O
could	O
run	O
a	O
simple	O
two-sample	B
test	O
maybe	O
wilcoxon	O
s	O
test	O
to	O
see	O
whether	O
the	O
more	O
aggressive	O
treatment	O
of	O
arm	O
b	O
was	O
increasing	O
the	O
survival	O
times	O
kaplan	O
meier	O
curves	O
provide	O
a	O
graphical	O
comparison	O
that	O
takes	O
proper	B
account	O
of	O
censoring	O
next	O
section	O
describes	O
an	O
appropriate	O
censored	O
data	B
two-sample	B
test	O
kaplan	O
meier	O
curves	O
have	O
become	O
familiar	O
friends	O
to	O
medical	O
researchers	O
a	O
lingua	O
franca	O
for	O
reporting	O
clinical	O
trial	O
results	O
life	B
table	I
methods	O
are	O
appropriate	O
for	O
censored	O
data	B
table	O
puts	O
the	O
arm	O
a	O
results	O
into	O
the	O
same	O
form	B
as	O
the	O
insurance	B
study	O
of	O
table	O
now	O
censored	O
data	B
and	O
kaplan	O
meier	O
table	O
censored	O
survival	O
times	O
in	O
days	O
from	O
two	O
arms	O
of	O
the	O
ncog	B
study	O
of	O
headneck	O
cancer	O
arm	O
a	O
chemotherapy	O
arm	O
b	O
chemotherapycradiation	O
with	O
the	O
time	O
unit	O
being	O
months	O
of	O
the	O
patients	O
in	O
arm	O
a	O
d	O
was	O
observed	O
to	O
die	O
in	O
the	O
first	O
month	O
after	O
treatment	O
this	O
left	O
at	O
risk	O
d	O
of	O
whom	O
died	O
in	O
the	O
second	O
month	O
d	O
of	O
the	O
remaining	O
died	O
in	O
their	O
third	O
month	O
after	O
treatment	O
and	O
one	O
was	O
lost	O
to	O
followup	O
this	O
being	O
noted	O
in	O
the	O
l	O
column	O
of	O
the	O
table	O
leaving	O
d	O
patients	O
at	O
risk	O
at	O
the	O
beginning	O
of	O
month	O
etc	O
os	O
here	O
is	O
calculated	O
as	O
in	O
except	O
starting	O
at	O
time	O
instead	O
of	O
there	O
is	O
nothing	O
wrong	O
with	O
this	O
estimate	B
but	O
binning	O
the	O
ncog	B
survival	O
data	B
by	O
months	O
is	O
arbitrary	O
why	O
not	O
go	O
down	O
to	O
days	O
as	O
the	O
data	B
was	O
originally	O
presented	O
in	O
table	O
a	O
kaplan	O
meier	O
survival	B
curve	I
is	O
the	O
limit	O
of	O
life	B
table	I
survival	O
estimates	O
as	O
the	O
time	O
unit	O
goes	O
to	O
zero	O
observations	O
zi	O
for	O
censored	O
data	B
problems	O
are	O
of	O
the	O
form	B
zi	O
d	O
di	O
where	O
ti	O
equals	O
the	O
observed	O
survival	O
time	O
while	O
di	O
indicates	O
whether	O
or	O
not	O
there	O
was	O
censoring	O
di	O
d	O
if	O
death	O
observed	O
if	O
death	O
not	O
observed	O
the	O
patients	O
were	O
enrolled	O
at	O
different	O
calendar	O
times	O
as	O
they	O
entered	O
the	O
study	O
but	O
for	O
each	O
patient	O
time	O
zero	O
in	O
the	O
table	O
is	O
set	B
at	O
the	O
beginning	O
of	O
his	O
or	O
her	O
treatment	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
table	O
arm	O
a	O
of	O
the	O
ncog	B
headneck	O
cancer	O
study	O
binned	O
by	O
month	O
n	O
d	O
number	O
at	O
risk	O
y	O
d	O
number	O
of	O
deaths	O
l	O
d	O
lost	O
to	O
followup	O
h	O
d	O
hazard	B
rate	B
yn	O
os	O
d	O
life	B
table	I
survival	O
estimate	B
month	O
n	O
y	O
l	O
h	O
os	O
month	O
n	O
y	O
l	O
h	O
os	O
di	O
d	O
corresponds	O
to	O
a	O
c	O
in	O
table	O
let	O
t	B
n	O
denote	O
the	O
ordered	O
survival	O
censored	O
or	O
not	O
with	O
corresponding	O
indicator	O
d	O
k	O
for	O
t	B
k	O
the	O
kaplan	O
meier	O
estimate	B
for	O
survival	O
probability	O
s	O
j	O
d	O
prfx	O
t	B
j	O
is	O
then	O
the	O
life	B
table	I
estimate	B
n	O
k	O
os	O
j	O
dy	O
kj	O
n	O
k	O
c	O
assuming	O
no	O
ties	O
among	O
the	O
survival	O
times	O
which	O
is	O
convenient	O
but	O
not	O
crucial	O
for	O
what	O
follows	O
censored	O
data	B
and	O
kaplan	O
meier	O
os	O
jumps	O
downward	O
at	O
death	O
times	O
tj	O
and	O
is	O
constant	O
between	O
observed	O
deaths	O
figure	O
ncog	B
kaplan	O
meier	O
survival	O
curves	O
lower	O
arm	O
a	O
only	O
upper	O
arm	O
b	O
vertical	O
lines	O
indicate	O
approximate	O
confidence	O
intervals	B
the	O
kaplan	O
meier	O
curves	O
for	O
both	O
arms	O
of	O
the	O
ncog	B
study	O
are	O
shown	O
in	O
figure	O
arm	O
b	O
the	O
more	O
aggressive	O
treatment	O
looks	O
better	O
its	O
survival	O
estimate	B
occurs	O
at	O
days	O
compared	O
with	O
days	O
for	O
arm	O
a	O
the	O
answer	O
to	O
the	O
inferential	O
question	O
is	O
b	O
really	O
better	O
than	O
a	O
or	O
is	O
this	O
just	O
random	O
variability	O
is	O
less	O
clear-cut	O
the	O
accuracy	B
of	O
os	O
j	O
can	O
be	O
estimated	O
from	O
greenwood	O
s	O
formula	B
for	O
its	O
standard	B
deviation	I
back	O
in	O
life	B
table	I
notation	O
os	O
j	O
d	O
os	O
j	O
sd	O
kj	O
yk	O
nk	O
nk	O
yk	O
the	O
vertical	O
bars	O
in	O
figure	O
are	O
approximate	O
confidence	O
limits	O
for	O
the	O
two	O
curves	O
based	O
on	O
greenwood	O
s	O
formula	B
they	O
overlap	O
enough	O
to	O
cast	O
doubt	O
on	O
the	O
superiority	O
of	O
arm	O
b	O
at	O
any	O
one	O
choice	O
of	O
days	O
but	O
the	O
twosample	O
test	O
of	O
the	O
next	O
section	O
which	O
compares	O
survival	O
at	O
all	O
timepoints	O
will	O
provide	O
more	O
definitive	O
evidence	O
life	O
tables	O
and	O
the	O
kaplan	O
meier	O
estimate	B
seem	O
like	O
a	O
textbook	O
example	O
of	O
frequentist	B
inference	B
as	O
described	O
in	O
chapter	O
a	O
useful	O
probabilistic	O
a	O
chemotherapy	O
onlyarm	O
b	O
chemotherapy	O
radiation	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
result	O
is	O
derived	O
and	O
then	O
implemented	O
by	O
the	O
plug-in	O
principle	O
there	O
is	O
more	O
to	O
the	O
story	O
though	O
as	O
discussed	O
below	O
life	B
table	I
curves	O
are	O
nonparametric	B
in	O
the	O
sense	O
that	O
no	O
particular	O
relationship	O
is	O
assumed	O
between	O
the	O
hazard	O
rates	O
hi	O
a	O
parametric	B
approach	O
can	O
greatly	O
improve	O
the	O
curves	O
accuracy	B
reverting	O
to	O
the	O
life	B
table	I
form	B
of	O
table	O
we	O
assume	O
that	O
the	O
death	O
counts	O
yk	O
are	O
independent	O
binomials	O
and	O
that	O
the	O
logits	O
d	O
hkg	O
satisfy	O
some	O
sort	O
of	O
regression	B
equation	B
yk	O
as	O
in	O
a	O
cubic	O
regression	B
for	O
instance	O
would	O
set	B
xk	O
d	O
k	O
for	O
the	O
kth	O
row	O
of	O
x	O
with	O
x	O
for	O
table	O
bi	O
nk	O
hk	O
d	O
x	O
figure	O
parametric	B
hazard	B
rate	B
estimates	O
for	O
the	O
ncog	B
study	O
arm	O
a	O
black	O
curve	O
has	O
about	O
times	O
higher	O
hazard	O
than	O
arm	O
b	O
for	O
all	O
times	O
more	O
than	O
a	O
year	O
after	O
treatment	O
standard	O
errors	B
shown	O
at	O
and	O
months	O
the	O
parametric	B
hazard-rate	O
estimates	O
in	O
figure	O
were	O
instead	O
based	O
on	O
a	O
cubic-linear	O
spline	O
xk	O
k	O
where	O
equals	O
k	O
for	O
k	O
and	O
for	O
k	O
the	O
vector	B
per	O
montharm	O
a	O
chemotherapy	O
onlyarm	O
b	O
chemotherapy	O
radiation	O
the	O
log-rank	B
test	I
d	O
x	O
describes	O
a	O
curve	O
that	O
is	O
cubic	O
for	O
k	O
linear	B
for	O
k	O
and	O
joined	O
smoothly	O
at	O
the	O
logistic	B
regression	B
maximum	B
likelihood	B
estimate	B
o	O
produced	O
hazard	B
rate	B
curves	O
o	O
c	O
e	O
hk	O
d	O
o	O
as	O
in	O
the	O
black	O
curve	O
in	O
figure	O
traces	O
o	O
red	O
curve	O
is	O
that	O
for	O
arm	O
b	O
fit	O
separately	O
hk	O
for	O
arm	O
a	O
while	O
the	O
k	O
comparison	O
in	O
terms	O
of	O
hazard	O
rates	O
is	O
more	O
informative	O
than	O
the	O
survival	O
curves	O
of	O
figure	O
both	O
arms	O
show	O
high	O
initial	O
hazards	O
peaking	O
at	O
five	O
months	O
and	O
then	O
a	O
long	O
slow	O
arm	O
b	O
hazard	O
is	O
always	O
below	O
arm	O
a	O
in	O
a	O
ratio	O
of	O
about	O
to	O
after	O
the	O
first	O
year	O
approximate	O
confidence	O
limits	O
obtained	O
as	O
in	O
don	O
t	B
overlap	O
indicating	O
superiority	O
of	O
arm	O
b	O
at	O
and	O
months	O
after	O
treatment	O
in	O
addition	O
to	O
its	O
frequentist	B
justification	O
survival	B
analysis	B
takes	O
us	O
into	O
the	O
fisherian	O
realm	O
of	O
conditional	B
inference	B
section	O
the	O
yk	O
s	O
in	O
model	O
are	O
considered	O
conditionally	O
on	O
the	O
nk	O
s	O
effectively	O
treating	O
the	O
nk	O
values	O
in	O
table	O
as	O
ancillaries	O
that	O
is	O
as	O
fixed	O
constants	O
by	O
themselves	O
containing	O
no	O
statistical	O
information	B
about	O
the	O
unknown	O
hazard	O
rates	O
we	O
will	O
examine	O
this	O
tactic	O
more	O
carefully	O
in	O
the	O
next	O
two	O
sections	O
the	O
log-rank	B
test	I
a	O
randomized	O
clinical	O
trial	O
interpreted	O
by	O
a	O
two-sample	B
test	O
remains	O
the	O
gold	O
standard	O
of	O
medical	O
experimentation	O
interpretation	O
usually	O
involves	O
student	O
s	O
two-sample	B
t-test	O
or	O
its	O
nonparametric	B
cousin	O
wilcoxon	O
s	O
test	O
but	O
neither	O
of	O
these	O
is	O
suitable	O
for	O
censored	O
data	B
the	O
log-rank	B
test	I
employs	O
an	O
ingenious	O
extension	O
of	O
life	O
tables	O
for	O
the	O
nonparametric	B
twosample	O
comparison	O
of	O
censored	O
survival	O
data	B
table	O
compares	O
the	O
results	O
of	O
the	O
ncog	B
study	O
for	O
the	O
first	O
six	O
after	O
treatment	O
at	O
the	O
of	O
month	O
there	O
were	O
patients	O
at	O
risk	O
in	O
arm	O
b	O
none	O
of	O
whom	O
died	O
compared	O
with	O
at	O
risk	O
and	O
death	O
in	O
arm	O
a	O
this	O
left	O
at	O
risk	O
in	O
arm	O
b	O
at	O
the	O
beginning	O
of	O
month	O
and	O
in	O
arm	O
a	O
with	O
and	O
deaths	O
during	O
the	O
month	O
respectively	O
the	O
cubic	O
linear	B
spline	O
is	O
designed	O
to	O
show	O
more	O
detail	O
in	O
the	O
early	O
months	O
where	O
there	O
is	O
more	O
available	O
patient	O
data	B
and	O
where	O
hazard	O
rates	O
usually	O
change	O
more	O
quickly	O
a	O
month	O
is	O
defined	O
here	O
as	O
days	O
the	O
beginning	O
of	O
month	O
is	O
each	O
patient	O
s	O
initial	O
treatment	O
time	O
at	O
which	O
all	O
patients	O
ever	O
enrolled	O
in	O
arm	O
b	O
were	O
at	O
risk	O
that	O
is	O
available	O
for	O
observation	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
table	O
life	B
table	I
comparison	O
for	O
the	O
first	O
six	O
months	O
of	O
the	O
ncog	B
study	O
for	O
example	O
at	O
the	O
beginning	O
of	O
the	O
sixth	O
month	O
after	O
treatment	O
there	O
were	O
remaining	O
arm	O
b	O
patients	O
of	O
whom	O
died	O
during	O
the	O
month	O
compared	O
with	O
at	O
risk	O
and	O
dying	O
in	O
arm	O
a	O
the	O
conditional	B
expected	O
number	O
of	O
deaths	O
in	O
arm	O
a	O
assuming	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
in	O
both	O
arms	O
was	O
using	O
expression	O
month	O
arm	O
b	O
arm	O
a	O
at	O
risk	O
died	O
at	O
risk	O
died	O
expected	O
number	O
arm	O
a	O
deaths	O
to	O
followup	O
were	O
assumed	O
to	O
occur	O
at	O
the	O
end	O
of	O
each	O
month	O
there	O
was	O
such	O
at	O
the	O
end	O
of	O
month	O
reducing	O
the	O
number	O
at	O
risk	O
in	O
arm	O
a	O
to	O
for	O
month	O
the	O
month	O
data	B
is	O
displayed	O
in	O
two-by-two	O
tabular	O
form	B
in	O
table	O
showing	O
the	O
notation	O
used	O
in	O
what	O
follows	O
na	O
for	O
the	O
number	O
at	O
risk	O
in	O
arm	O
a	O
nd	O
for	O
the	O
number	O
of	O
deaths	O
etc	O
y	O
indicates	O
the	O
number	O
of	O
arm	O
a	O
deaths	O
if	O
the	O
marginal	O
totals	O
na	O
nb	O
nd	O
and	O
ns	O
are	O
given	O
then	O
y	O
determines	O
the	O
other	O
three	O
table	O
entries	O
by	O
subtraction	O
so	O
we	O
are	O
not	O
losing	O
any	O
information	B
by	O
focusing	O
on	O
y	O
table	O
two-by-two	O
display	O
of	O
data	B
for	O
the	O
ncog	B
study	O
e	O
is	O
the	O
expected	O
number	O
of	O
arm	O
a	O
deaths	O
assuming	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
column	O
of	O
table	O
arm	O
a	O
arm	O
b	O
died	O
y	O
d	O
e	O
d	O
d	O
nd	O
survived	O
ns	O
d	O
na	O
d	O
nb	O
d	O
n	O
d	O
consider	O
the	O
null	O
hypothesis	O
that	O
the	O
hazard	O
rates	O
for	O
month	O
are	O
the	O
log-rank	B
test	I
the	O
same	O
in	O
arm	O
a	O
and	O
arm	O
b	O
w	O
d	O
under	O
y	O
has	O
mean	O
e	O
and	O
variance	B
v	O
e	O
d	O
nand	O
v	O
d	O
nanb	O
nd	O
ns	O
as	O
calculated	O
according	O
to	O
the	O
hypergeometric	B
distribution	B
e	O
d	O
and	O
v	O
d	O
in	O
table	O
we	O
can	O
form	B
a	O
two-by-two	O
table	O
for	O
each	O
of	O
the	O
n	O
d	O
months	O
of	O
the	O
ncog	B
study	O
calculating	O
yi	O
ei	O
and	O
vi	O
for	O
month	O
i	O
the	O
log-rank	B
statistic	B
z	O
is	O
then	O
defined	O
to	O
be	O
z	O
d	O
nx	O
ei	O
nx	O
vi	O
the	O
idea	O
here	O
is	O
simple	O
but	O
clever	O
each	O
month	O
we	O
test	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
w	O
hai	O
d	O
hbi	O
the	O
numerator	O
yi	O
ei	O
has	O
expectation	O
under	O
but	O
if	O
hai	O
is	O
greater	O
than	O
hbi	O
that	O
is	O
if	O
treatment	O
b	O
is	O
superior	O
then	O
the	O
numerator	O
has	O
a	O
positive	O
expectation	O
adding	O
up	O
the	O
numerators	O
gives	O
us	O
power	O
to	O
detect	O
a	O
general	O
superiority	O
of	O
treatment	O
b	O
over	O
a	O
against	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
hai	O
d	O
hbi	O
for	O
all	O
i	O
for	O
the	O
ncog	B
study	O
binned	O
by	O
months	O
ei	O
d	O
vi	O
d	O
yi	O
d	O
nx	O
nx	O
nx	O
giving	O
log-rank	B
test	I
statistic	B
z	O
d	O
asymptotic	O
calculations	O
based	O
on	O
the	O
central	B
limit	I
theorem	B
suggest	O
z	O
n	O
under	O
the	O
null	O
hypothesis	O
that	O
the	O
two	O
treatments	O
are	O
equally	O
effective	O
i	O
e	O
that	O
hai	O
d	O
hbi	O
for	O
i	O
d	O
n	O
in	O
the	O
usual	O
interpretation	O
z	O
d	O
is	O
significant	O
at	O
the	O
one-sided	O
level	O
providing	O
moderately	O
strong	O
evidence	O
in	O
favor	O
of	O
treatment	O
b	O
an	O
impressive	O
amount	O
of	O
inferential	O
guile	O
goes	O
into	O
the	O
log-rank	B
test	I
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
working	O
with	O
hazard	O
rates	O
instead	O
of	O
densities	O
or	O
cdfs	O
is	O
essential	O
for	O
survival	O
data	B
conditioning	O
at	O
each	O
period	O
on	O
the	O
numbers	O
at	O
risk	O
na	O
and	O
nb	O
in	O
table	O
finesses	O
the	O
difficulties	O
of	O
censored	O
data	B
censoring	O
only	O
changes	O
the	O
at-risk	O
numbers	O
in	O
future	O
periods	O
also	O
conditioning	O
on	O
the	O
number	O
of	O
deaths	O
and	O
survivals	O
nd	O
and	O
ns	O
in	O
table	O
leaves	O
only	O
the	O
univariate	O
statistic	B
y	O
to	O
interpret	O
at	O
each	O
period	O
which	O
is	O
easily	O
done	O
through	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
adding	O
the	O
discrepancies	O
yi	O
ei	O
in	O
the	O
numerator	O
of	O
than	O
say	O
adding	O
the	O
individual	O
z	O
values	O
zi	O
d	O
ei	O
or	O
adding	O
the	O
i	O
values	O
accrues	O
power	O
for	O
the	O
natural	O
alternative	O
hypothesis	O
hai	O
hbi	O
for	O
all	O
i	O
while	O
avoiding	O
destabilization	O
from	O
small	O
values	O
of	O
vi	O
i	O
each	O
of	O
the	O
four	O
tactics	O
had	O
been	O
used	O
separately	O
in	O
classical	O
applications	O
putting	O
them	O
together	O
into	O
the	O
log-rank	B
test	I
was	O
a	O
major	O
inferential	O
accomplishment	O
foreshadowing	O
a	O
still	O
bigger	O
step	O
forward	O
the	O
proportional	B
hazards	I
model	I
our	O
subject	O
in	O
the	O
next	O
section	O
conditional	B
inference	B
takes	O
on	O
an	O
aggressive	O
form	B
in	O
the	O
log-rank	B
test	I
let	O
di	O
indicate	O
all	O
the	O
data	B
except	O
yi	O
available	O
at	O
the	O
end	O
of	O
the	O
ith	O
period	O
for	O
month	O
in	O
the	O
ncog	B
study	O
includes	O
all	O
data	B
for	O
months	O
in	O
table	O
and	O
the	O
marginals	O
na	O
nb	O
nd	O
and	O
ns	O
in	O
table	O
but	O
not	O
the	O
y	O
value	O
for	O
month	O
the	O
key	O
assumption	O
is	O
that	O
under	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
yijdi	O
vi	O
ind	O
here	O
meaning	O
that	O
the	O
yi	O
s	O
can	O
be	O
treated	O
as	O
independent	O
quantities	O
with	O
means	O
and	O
variances	O
in	O
particular	O
we	O
can	O
add	O
the	O
variances	O
vi	O
to	O
get	O
the	O
denominator	O
of	O
partial	B
likelihood	B
argument	B
described	O
in	O
the	O
endnotes	O
justifies	O
adding	O
the	O
variances	O
the	O
purpose	O
of	O
all	O
this	O
fisherian	O
conditioning	O
is	O
to	O
simplify	O
the	O
inference	B
the	O
conditional	B
distribution	B
yijdi	O
depends	O
only	O
on	O
the	O
hazard	O
rates	O
hai	O
and	O
hbi	O
nuisance	B
parameters	I
relating	O
to	O
the	O
survival	O
times	O
and	O
censoring	O
mechanism	O
of	O
the	O
data	B
in	O
table	O
are	O
hidden	O
away	O
there	O
is	O
a	O
price	O
to	O
pay	O
in	O
testing	B
power	O
though	O
usually	O
a	O
small	O
one	O
the	O
lost-to-followup	O
values	O
l	O
in	O
table	O
have	O
been	O
ignored	O
even	O
though	O
they	O
might	O
contain	O
useful	O
information	B
say	O
if	O
all	O
the	O
early	O
losses	O
occurred	O
in	O
one	O
arm	O
the	O
proportional	B
hazards	I
model	I
the	O
proportional	B
hazards	I
model	I
the	O
kaplan	O
meier	O
estimator	B
is	O
a	O
one-sample	O
device	O
dealing	O
with	O
data	B
coming	O
from	O
a	O
single	O
distribution	B
the	O
log-rank	B
test	I
makes	O
two-sample	B
comparisons	O
proportional	O
hazards	O
ups	O
the	O
ante	O
to	O
allow	O
for	O
a	O
full	B
regression	B
analysis	B
of	O
censored	O
data	B
now	O
the	O
individual	O
data	B
points	O
zi	O
are	O
of	O
the	O
form	B
zi	O
d	O
ti	O
di	O
where	O
ti	O
and	O
di	O
are	O
observed	O
survival	O
time	O
and	O
censoring	O
indicator	O
as	O
in	O
and	O
ci	O
is	O
a	O
known	O
p	O
vector	B
of	O
covariates	O
whose	O
effect	O
on	O
survival	O
we	O
wish	O
to	O
assess	O
both	O
of	O
the	O
previous	O
methods	O
are	O
included	O
here	O
for	O
the	O
log-rank	B
test	I
ci	O
indicates	O
treatment	O
say	O
ci	O
equals	O
or	O
for	O
arm	O
a	O
or	O
arm	O
b	O
while	O
ci	O
is	O
absent	O
for	O
kaplan	O
meier	O
table	O
pediatric	B
cancer	I
data	B
first	O
of	O
children	O
sex	O
d	O
male	O
d	O
female	O
race	O
d	O
white	O
d	O
nonwhite	O
age	O
in	O
years	O
entry	O
d	O
calendar	O
date	O
of	O
entry	O
in	O
days	O
since	O
july	O
far	O
d	O
home	O
distance	O
from	O
treatment	O
center	O
in	O
miles	O
t	B
d	O
survival	O
time	O
in	O
days	O
d	O
d	O
if	O
death	O
observed	O
if	O
not	O
sex	O
race	O
age	O
entry	O
far	O
t	B
d	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
medical	O
studies	O
regularly	O
produce	O
data	B
of	O
form	B
an	O
example	O
the	O
pediatric	B
cancer	I
data	B
is	O
partially	O
listed	O
in	O
table	O
the	O
first	O
of	O
n	O
d	O
cases	O
are	O
shown	O
there	O
are	O
five	O
explanatory	O
covariates	O
in	O
the	O
table	O
s	O
caption	O
sex	O
race	O
age	O
at	O
entry	O
calendar	O
date	O
of	O
entry	O
into	O
the	O
study	O
and	O
far	O
the	O
distance	O
of	O
the	O
child	O
s	O
home	O
from	O
the	O
treatment	O
center	O
the	O
response	B
variable	O
t	B
is	O
survival	O
in	O
days	O
from	O
time	O
of	O
treatment	O
until	O
death	O
happily	O
only	O
of	O
the	O
children	O
were	O
observed	O
to	O
die	O
d	O
some	O
left	O
the	O
study	O
for	O
various	O
reasons	O
but	O
most	O
of	O
the	O
d	O
d	O
cases	O
were	O
those	O
children	O
still	O
alive	O
at	O
the	O
end	O
of	O
the	O
study	O
period	O
of	O
particular	O
interest	O
was	O
the	O
effect	O
of	O
far	O
on	O
survival	O
we	O
wish	O
to	O
carry	O
out	O
a	O
regression	B
analysis	B
of	O
this	O
heavily	O
censored	O
data	B
set	B
the	O
proportional	B
hazards	I
model	I
assumes	O
that	O
the	O
hazard	B
rate	B
hi	O
for	O
the	O
ith	O
individual	O
is	O
hi	O
d	O
i	O
here	O
is	O
a	O
baseline	O
hazard	O
we	O
need	O
not	O
specify	O
and	O
is	O
an	O
unknown	O
p-parameter	B
vector	B
we	O
want	O
to	O
estimate	B
for	O
concise	O
notation	O
let	O
model	O
says	O
that	O
individual	O
i	O
s	O
hazard	O
is	O
a	O
constant	O
nonnegative	O
factor	B
times	O
the	O
baseline	O
hazard	O
equivalently	O
from	O
the	O
ith	O
survival	O
function	B
si	O
is	O
a	O
power	O
of	O
the	O
baseline	O
survival	O
function	B
d	O
ec	O
i	O
i	O
si	O
d	O
larger	O
values	O
of	O
lead	O
to	O
more	O
quickly	O
declining	O
survival	O
curves	O
i	O
e	O
to	O
worse	O
survival	O
in	O
let	O
j	O
be	O
the	O
number	O
of	O
observed	O
deaths	O
j	O
d	O
here	O
occurring	O
at	O
times	O
t	B
j	O
again	O
for	O
convenience	O
assuming	O
no	O
just	O
before	O
time	O
t	B
j	O
there	O
is	O
a	O
risk	B
set	B
of	O
individuals	O
still	O
under	O
observation	O
whose	O
indices	O
we	O
denote	O
by	O
rj	O
rj	O
d	O
fi	O
w	O
ti	O
t	B
j	O
let	O
ij	O
be	O
the	O
index	O
of	O
the	O
individual	O
observed	O
to	O
die	O
at	O
time	O
t	B
j	O
the	O
key	O
to	O
proportional	O
hazards	O
regression	B
is	O
the	O
following	O
result	O
more	O
precisely	O
assuming	O
only	O
one	O
event	O
a	O
death	O
occurred	O
at	O
t	B
j	O
with	O
none	O
of	O
the	O
other	O
individuals	O
being	O
lost	O
to	O
followup	O
at	O
exact	O
time	O
t	B
j	O
the	O
proportional	B
hazards	I
model	I
lemma	O
under	O
the	O
proportional	B
hazards	I
model	I
the	O
conditional	B
probability	O
given	O
the	O
risk	B
set	B
rj	O
that	O
individual	O
i	O
in	O
rj	O
is	O
the	O
one	O
observed	O
to	O
die	O
at	O
time	O
t	B
j	O
is	O
prfij	O
d	O
ij	O
x	O
k	O
i	O
ec	O
rjg	O
d	O
ec	O
rj	O
to	O
put	O
it	O
in	O
words	O
given	O
that	O
one	O
person	O
dies	O
at	O
time	O
t	B
j	O
the	O
probability	O
i	O
among	O
the	O
set	B
of	O
individuals	O
it	O
is	O
individual	O
i	O
is	O
proportional	O
to	O
exp	O
c	O
at	O
risk	O
for	O
the	O
purpose	O
of	O
estimating	O
the	O
parameter	O
vector	B
in	O
model	O
we	O
multiply	O
factors	O
to	O
form	B
the	O
partial	B
likelihood	B
l	O
d	O
jy	O
ij	O
c	O
x	O
rj	O
k	O
ec	O
l	O
is	O
then	O
treated	O
as	O
an	O
ordinary	O
likelihood	B
function	B
yielding	O
an	O
approximately	O
unbiased	O
mle-like	O
estimate	B
o	O
d	O
arg	O
max	O
fl	O
o	O
l	O
o	O
with	O
an	O
approximate	O
covariance	O
obtained	O
from	O
the	O
second-derivative	O
matrix	B
of	O
l	O
d	O
log	O
l	O
as	O
in	O
section	O
table	O
shows	O
the	O
proportional	O
hazards	O
analysis	B
of	O
the	O
pediatric	B
cancer	I
data	B
with	O
the	O
covariates	O
age	O
entry	O
and	O
far	O
standardized	O
to	O
have	O
mean	O
and	O
standard	B
deviation	I
for	O
the	O
neither	O
sex	O
nor	O
race	O
seems	O
to	O
make	O
much	O
difference	O
we	O
see	O
that	O
age	O
is	O
a	O
mildly	O
significant	O
factor	B
with	O
older	O
children	O
doing	O
better	O
the	O
estimated	O
regression	B
coefficient	O
is	O
negative	O
however	O
the	O
dramatic	O
effects	O
are	O
date	O
of	O
entry	O
and	O
far	O
individuals	O
who	O
entered	O
the	O
study	O
later	O
survived	O
longer	O
perhaps	O
the	O
treatment	O
protocol	O
was	O
being	O
improved	O
while	O
children	O
living	O
farther	O
away	O
from	O
the	O
treatment	O
center	O
did	O
worse	O
justification	O
of	O
the	O
partial	B
likelihood	B
calculations	O
is	O
similar	O
to	O
that	O
for	O
the	O
log-rank	B
test	I
but	O
there	O
are	O
some	O
important	O
differences	O
too	O
the	O
proportional	B
hazards	I
model	I
is	O
semiparametric	O
semi	O
because	O
we	O
don	O
t	B
have	O
to	O
specify	O
in	O
rather	O
than	O
nonparametric	B
as	O
before	O
and	O
the	O
table	O
was	O
obtained	O
using	O
the	O
r	B
program	O
coxph	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
table	O
proportional	O
hazards	O
analysis	B
of	O
pediatric	B
cancer	I
data	B
entry	O
and	O
far	O
standardized	O
age	O
significantly	O
negative	O
older	O
children	O
doing	O
better	O
entry	O
very	O
significantly	O
negative	O
showing	O
hazard	B
rate	B
declining	O
with	O
calendar	O
date	O
of	O
entry	O
far	O
very	O
significantly	O
positive	O
indicating	O
worse	O
results	O
for	O
children	O
living	O
farther	O
away	O
from	O
the	O
treatment	O
center	O
last	O
two	O
columns	O
show	O
limits	O
of	O
approximate	O
confidence	O
intervals	B
for	O
exp	O
sex	O
race	O
age	O
entry	O
far	O
sd	O
z-value	O
p-value	B
exp	O
lower	O
upper	O
emphasis	O
on	O
likelihood	B
has	O
increased	O
the	O
fisherian	O
nature	O
of	O
the	O
inference	B
moving	O
it	O
further	O
away	O
from	O
pure	O
frequentism	B
still	O
more	O
fisherian	O
is	O
the	O
emphasis	O
on	O
likelihood	B
inference	B
in	O
rather	O
than	O
the	O
direct	O
frequentist	B
calculations	O
of	O
the	O
conditioning	O
argument	B
here	O
is	O
less	O
obvious	O
than	O
that	O
for	O
the	O
kaplan	O
meier	O
estimate	B
or	O
the	O
log-rank	B
test	I
has	O
its	O
convenience	O
possibly	O
come	O
at	O
too	O
high	O
a	O
price	O
in	O
fact	O
it	O
can	O
be	O
shown	O
that	O
inference	B
based	O
on	O
the	O
partial	B
likelihood	B
is	O
highly	O
efficient	O
assuming	O
of	O
course	O
the	O
correctness	O
of	O
the	O
proportional	B
hazards	I
model	I
missing	B
data	B
and	O
the	O
em	B
algorithm	B
censored	O
data	B
the	O
motivating	O
factor	B
for	O
survival	B
analysis	B
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
case	O
of	O
a	O
more	O
general	O
statistical	O
topic	O
missing	B
data	B
what	O
s	O
missing	O
in	O
table	O
for	O
example	O
are	O
the	O
actual	O
survival	O
times	O
for	O
the	O
c	O
cases	O
which	O
are	O
known	O
only	O
to	O
exceed	O
the	O
tabled	O
values	O
if	O
the	O
data	B
were	O
not	O
missing	O
we	O
could	O
use	O
standard	O
statistical	O
methods	O
for	O
instance	O
wilcoxon	O
s	O
test	O
to	O
compare	O
the	O
two	O
arms	O
of	O
the	O
ncog	B
study	O
the	O
em	B
algorithm	B
is	O
an	O
iterative	O
technique	O
for	O
solving	O
missing-data	O
inferential	O
problems	O
using	O
only	O
standard	O
methods	O
a	O
missing-data	O
situation	O
is	O
shown	O
in	O
figure	O
n	O
d	O
points	O
have	O
been	O
independently	O
sampled	O
from	O
a	O
bivariate	B
normal	B
distribution	B
missing	B
data	B
and	O
the	O
em	B
algorithm	B
figure	O
forty	O
points	O
from	O
a	O
bivariate	B
normal	B
distribution	B
the	O
last	O
with	O
missing	O
means	O
variances	O
and	O
correlation	O
however	O
the	O
second	O
coordinates	O
of	O
the	O
last	O
points	O
have	O
been	O
lost	O
these	O
are	O
represented	O
by	O
the	O
circled	O
points	O
in	O
figure	O
with	O
their	O
values	O
arbitrarily	O
set	B
to	O
we	O
wish	O
to	O
find	O
the	O
maximum	B
likelihood	B
estimate	B
of	O
the	O
parameter	O
vector	B
d	O
the	O
standard	O
maximum	B
likelihood	B
estimates	O
d	O
d	O
d	O
d	O
d	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
are	O
unavailable	O
for	O
and	O
because	O
of	O
the	O
missing	B
data	B
the	O
em	B
algorithm	B
begins	O
by	O
filling	O
in	O
the	O
missing	B
data	B
in	O
some	O
way	O
say	O
by	O
setting	O
d	O
for	O
the	O
missing	O
values	O
giving	O
an	O
artificially	O
complete	O
data	B
set	B
then	O
it	O
proceeds	O
as	O
follows	O
the	O
standard	O
method	B
is	O
applied	O
to	O
the	O
filled-in	O
to	O
produce	O
o	O
d	O
this	O
is	O
the	O
m	O
maximizing	O
each	O
of	O
the	O
missing	O
values	O
is	O
replaced	O
by	O
its	O
conditional	B
expectation	O
d	O
o	O
given	O
the	O
nonmissing	O
data	B
this	O
is	O
the	O
e	O
expectation	O
step	O
in	O
our	O
case	O
the	O
missing	O
values	O
are	O
replaced	O
by	O
c	O
o	O
is	O
suitably	O
small	O
the	O
e	O
and	O
m	O
steps	O
are	O
repeated	O
at	O
the	O
j	O
th	O
stage	O
giving	O
a	O
new	O
artificially	O
complete	O
data	B
set	B
data	B
j	O
and	O
an	O
updated	O
estimate	B
o	O
the	O
iteration	O
stops	O
when	O
k	O
o	O
table	O
shows	O
the	O
em	B
algorithm	B
at	O
work	O
on	O
the	O
bivariate	B
normal	B
example	O
of	O
figure	O
in	O
exponential	O
families	O
the	O
algorithm	B
is	O
guaranteed	O
to	O
converge	O
to	O
the	O
mle	B
o	O
based	O
on	O
just	O
the	O
observed	O
data	B
o	O
moreover	O
the	O
likelihood	B
fo	O
increases	O
with	O
every	O
step	O
j	O
convergence	O
can	O
be	O
sluggish	O
as	O
it	O
is	O
here	O
for	O
and	O
the	O
em	B
algorithm	B
ultimately	O
derives	O
from	O
the	O
fake-data	B
principle	I
a	O
property	O
of	O
maximum	B
likelihood	B
estimation	B
going	O
back	O
to	O
fisher	B
that	O
can	O
only	O
briefly	O
be	O
summarized	O
here	O
let	O
x	O
d	O
u	O
represent	O
the	O
complete	O
data	B
of	O
which	O
o	O
is	O
observed	O
while	O
u	O
is	O
unobserved	O
or	O
missing	O
write	O
the	O
density	B
for	O
x	O
as	O
f	O
d	O
f	O
be	O
the	O
mle	B
of	O
based	O
just	O
on	O
o	O
and	O
let	O
o	O
suppose	O
we	O
now	O
generate	O
simulations	O
of	O
u	O
by	O
sampling	O
from	O
the	O
conditional	B
distribution	B
fo	O
fo	O
u	O
for	O
k	O
d	O
k	O
stars	O
indicating	O
creation	O
by	O
the	O
statistician	O
and	O
not	O
by	O
observation	O
giving	O
fake	O
complete-data	O
values	O
x	O
d	O
fx	O
d	O
u	O
x	O
x	O
let	O
in	O
this	O
example	O
data	B
and	O
and	O
as	O
in	O
table	O
stay	O
the	O
same	O
in	O
subsequent	O
steps	O
of	O
the	O
algorithm	B
are	O
available	O
as	O
the	O
complete-data	O
estimates	O
in	O
missing	B
data	B
and	O
the	O
em	B
algorithm	B
table	O
em	B
algorithm	B
for	O
estimating	O
means	O
standard	B
deviations	I
and	O
the	O
correlation	O
of	O
the	O
bivariate	B
normal	B
distribution	B
that	O
gave	O
the	O
data	B
in	O
figure	O
step	O
f	O
goes	O
to	O
o	O
yields	O
mle	B
o	O
whose	O
notional	O
likelihoodqk	O
it	O
then	O
turns	O
out	O
that	O
o	O
as	O
k	O
goes	O
to	O
infinity	O
in	O
other	O
words	O
maximum	B
likelihood	B
estimation	B
is	O
self-consistent	B
generating	O
artificial	O
data	B
from	O
the	O
mle	B
density	B
fo	O
doesn	O
t	B
change	O
the	O
mle	B
moreover	O
any	O
value	O
o	O
not	O
equal	O
to	O
the	O
mle	B
o	O
cannot	O
be	O
self-consistent	B
carrying	O
through	O
using	O
fo	O
leads	O
to	O
hypothetical	O
mle	B
o	O
having	O
fo	O
fo	O
etc	O
a	O
more	O
general	O
version	O
of	O
the	O
em	O
modern	O
technology	O
allows	O
social	O
scientists	O
to	O
collect	O
huge	O
data	B
sets	O
perhaps	O
hundreds	O
of	O
responses	O
for	O
each	O
of	O
thousands	O
or	O
even	O
millions	O
of	O
individuals	O
inevitably	O
some	O
entries	O
of	O
the	O
individual	O
responses	O
will	O
be	O
missing	O
imputation	B
amounts	O
to	O
employing	O
some	O
version	O
of	O
the	O
fake-data	B
principle	I
to	O
fill	O
in	O
the	O
missing	O
values	O
imputation	B
s	O
goal	O
goes	O
beyond	O
findbe	O
replaced	O
by	O
e	O
with	O
e	O
indicating	O
expectation	O
with	O
respect	O
to	O
o	O
simulation	B
is	O
unnecessary	O
in	O
exponential	O
families	O
where	O
at	O
each	O
stage	O
data	B
can	O
as	O
in	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
ing	O
the	O
mle	B
to	O
the	O
creation	O
of	O
graphs	O
confidence	O
intervals	B
histograms	O
and	O
more	O
using	O
only	O
convenient	O
standard	O
complete-data	O
methods	O
finally	O
returning	O
to	O
survival	B
analysis	B
the	O
kaplan	O
meier	O
estimate	B
is	O
itself	O
self-consistent	B
consider	O
the	O
arm	O
a	O
censored	O
observation	O
in	O
table	O
we	O
know	O
that	O
that	O
patient	O
s	O
survival	O
time	O
exceeded	O
suppose	O
we	O
distribute	O
his	O
probability	O
mass	O
of	O
the	O
arm	O
a	O
sample	B
to	O
the	O
right	O
in	O
accordance	O
with	O
the	O
conditional	B
distribution	B
for	O
x	O
defined	O
by	O
the	O
arm	O
a	O
kaplan	O
meier	O
survival	B
curve	I
it	O
turns	O
out	O
that	O
redistributing	O
all	O
the	O
censored	O
cases	O
does	O
not	O
change	O
the	O
original	O
kaplan	O
meier	O
survival	B
curve	I
kaplan	O
meier	O
is	O
self-consistent	B
leading	O
to	O
its	O
identification	O
as	O
the	O
nonparametric	B
mle	B
of	O
a	O
survival	O
function	B
notes	O
and	O
details	O
the	O
progression	O
from	O
life	O
tables	O
kaplan	O
meier	O
curves	O
and	O
the	O
log-rank	B
test	I
to	O
proportional	O
hazards	O
regression	B
was	O
modest	O
in	O
its	O
computational	O
demands	O
until	O
the	O
final	O
step	O
kaplan	O
meier	O
curves	O
lie	O
within	O
the	O
capabilities	O
of	O
mechanical	O
calculators	O
not	O
so	O
for	O
proportional	O
hazards	O
which	O
is	O
emphatically	O
a	O
child	O
of	O
the	O
computer	O
age	O
as	O
the	O
algorithms	O
grew	O
more	O
intricate	O
their	O
inferential	O
justification	O
deepened	O
in	O
scope	O
and	O
sophistication	O
this	O
is	O
a	O
pattern	O
we	O
also	O
saw	O
in	O
chapter	O
in	O
the	O
progression	O
from	O
bioassay	B
to	O
logistic	B
regression	B
to	O
generalized	O
linear	B
models	B
and	O
will	O
reappear	O
as	O
we	O
move	O
from	O
the	O
jackknife	B
to	O
the	O
bootstrap	B
in	O
chapter	O
censoring	O
is	O
not	O
the	O
same	O
as	O
truncation	O
for	O
the	O
truncated	O
galaxy	B
data	B
of	O
section	O
we	O
learn	O
of	O
the	O
existence	O
of	O
a	O
galaxy	B
only	O
if	O
it	O
falls	O
into	O
the	O
observation	O
region	B
the	O
censored	O
individuals	O
in	O
table	O
are	O
known	O
to	O
exist	O
but	O
with	O
imperfect	O
knowledge	O
of	O
their	O
lifetimes	O
there	O
is	O
a	O
version	O
of	O
the	O
kaplan	O
meier	O
curve	O
applying	O
to	O
truncated	O
data	B
which	O
was	O
developed	O
in	O
the	O
astronomy	O
literature	O
by	O
lynden-bell	O
the	O
methods	O
of	O
this	O
chapter	O
apply	O
to	O
data	B
that	O
is	O
left-truncated	B
as	O
well	O
as	O
right-censored	B
in	O
a	O
survival	O
time	O
study	O
of	O
a	O
new	O
hiv	O
drug	O
for	O
instance	O
subject	O
i	O
might	O
not	O
enter	O
the	O
study	O
until	O
some	O
time	O
after	O
his	O
or	O
her	O
initial	O
diagnosis	O
in	O
which	O
case	O
ti	O
would	O
be	O
left-truncated	B
at	O
as	O
well	O
as	O
possibly	O
later	O
right-censored	B
this	O
only	O
modifies	O
the	O
composition	O
of	O
the	O
various	O
risk	O
sets	O
however	O
other	O
missing-data	O
situations	O
e	O
g	O
left-	O
and	O
right-censoring	O
require	O
more	O
elaborate	O
less	O
elegant	O
treatments	O
formula	B
let	O
the	O
interval	B
be	O
partitioned	O
into	O
a	O
large	O
number	O
of	O
subintervals	O
of	O
length	O
dt	O
with	O
tk	O
the	O
midpoint	O
of	O
subinterval	O
k	O
notes	O
and	O
details	O
as	O
in	O
using	O
prft	O
h	O
ti	O
dt	O
o	O
nx	O
h	O
ti	O
dt	O
o	O
h	O
ti	O
dt	O
d	O
exp	O
exp	O
which	O
as	O
dt	O
goes	O
to	O
kaplan	O
meier	O
estimate	B
in	O
the	O
life	B
table	I
formula	B
k	O
d	O
let	O
the	O
time	O
unit	O
be	O
small	O
enough	O
to	O
make	O
each	O
bin	O
contain	O
at	O
most	O
one	O
value	O
t	B
k	O
then	O
at	O
t	B
k	O
o	O
h	O
k	O
d	O
d	O
k	O
n	O
k	O
c	O
giving	O
expression	O
greenwood	O
s	O
formula	B
in	O
the	O
life	B
table	I
formulation	O
of	O
sec	O
tion	O
gives	O
from	O
nk	O
o	O
hk	O
bi	O
nk	O
hk	O
we	O
get	O
n	O
n	O
log	O
osj	O
var	O
var	O
log	O
log	O
osj	O
d	O
jx	O
o	O
log	O
hk	O
o	O
jx	O
hk	O
hk	O
hk	O
nk	O
o	O
d	O
jx	O
d	O
jx	O
var	O
o	O
hk	O
n	O
log	O
osj	O
o	O
jx	O
where	O
we	O
have	O
used	O
the	O
delta-method	O
approximation	O
varflog	O
xg	O
varfxg	O
plugging	O
in	O
hk	O
d	O
yknk	O
yields	O
yk	O
var	O
nk	O
nk	O
yk	O
then	O
the	O
inverse	O
approximation	O
varfxg	O
d	O
varflog	O
xg	O
gives	O
greenwood	O
s	O
formula	B
the	O
censored	O
data	B
situation	O
of	O
section	O
does	O
not	O
enjoy	O
independence	O
between	O
the	O
o	O
hk	O
values	O
however	O
successive	O
conditional	B
independence	O
given	O
the	O
nk	O
values	O
is	O
enough	O
to	O
verify	O
the	O
result	O
as	O
in	O
the	O
partial	B
likelihood	B
calculations	O
below	O
note	O
the	O
confidence	O
intervals	B
in	O
figure	O
were	O
obtained	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
by	O
exponentiating	O
the	O
intervals	B
log	O
osj	O
h	O
n	O
log	O
osj	O
var	O
parametric	B
life	O
tables	O
analysis	B
figure	O
and	O
the	O
analysis	B
behind	O
it	O
is	O
developed	O
in	O
efron	O
where	O
it	O
is	O
called	O
partial	B
logistic	B
regression	B
in	O
analogy	O
with	O
partial	B
likelihood	B
the	O
log-rank	B
test	I
this	O
chapter	O
featured	O
an	O
all-star	O
cast	O
including	O
four	O
of	O
the	O
most	O
referenced	O
papers	O
of	O
the	O
post-war	O
era	O
kaplan	O
and	O
meier	O
cox	O
on	O
proportional	O
hazards	O
dempster	O
et	O
al	O
codifying	O
and	O
naming	O
the	O
em	B
algorithm	B
and	O
mantel	O
and	O
haenszel	O
on	O
the	O
log-rank	B
test	I
gives	O
a	O
careful	O
and	O
early	O
analysis	B
of	O
the	O
mantel	O
haenszel	O
idea	O
the	O
not	O
very	O
helpful	O
name	O
log-rank	O
does	O
at	O
least	O
remind	O
us	O
that	O
the	O
test	O
depends	O
only	O
on	O
the	O
ranks	O
of	O
the	O
survival	O
times	O
and	O
will	O
give	O
the	O
same	O
result	O
if	O
all	O
the	O
observed	O
survival	O
times	O
ti	O
are	O
monotonically	O
transformed	O
say	O
to	O
exp	O
ti	O
or	O
t	B
it	O
is	O
often	O
referred	O
to	O
as	O
the	O
mantel	O
haenszel	O
or	O
cochran	O
mantel	O
haenszel	O
test	O
in	O
older	O
literature	O
kaplan	O
meier	O
and	O
proportional	O
hazards	O
are	O
also	O
rank-based	O
procedures	O
i	O
hypergeometric	B
distribution	B
hypergeometric	O
calculations	O
as	O
for	O
table	O
are	O
often	O
stated	O
as	O
follows	O
n	O
marbles	O
are	O
placed	O
in	O
an	O
urn	O
na	O
labeled	O
a	O
and	O
nb	O
labeled	O
b	O
nd	O
marbles	O
are	O
drawn	O
out	O
at	O
random	O
y	O
is	O
the	O
number	O
of	O
these	O
labeled	O
a	O
elementary	O
not	O
simple	O
calculations	O
then	O
produce	O
the	O
conditional	B
distribution	B
of	O
y	O
given	O
the	O
table	O
s	O
marginals	O
na	O
nb	O
n	O
nd	O
and	O
ns	O
n	O
nd	O
prfyjmarginalsg	O
d	O
na	O
y	O
nb	O
nd	O
y	O
for	O
max	O
na	O
ns	O
y	O
min	O
nd	O
na	O
and	O
expressions	O
for	O
the	O
mean	O
and	O
variance	B
if	O
na	O
and	O
nb	O
go	O
to	O
infinity	O
such	O
that	O
nan	O
pa	O
and	O
nb	O
pa	O
then	O
v	O
nd	O
pa	O
the	O
variance	B
of	O
y	O
bi	O
nd	O
pa	O
log-rank	B
statistic	B
z	O
why	O
is	O
nominator	O
for	O
z	O
let	O
ui	O
d	O
yi	O
ei	O
in	O
so	O
z	O
s	O
numerator	O
ispn	O
vi	O
the	O
correct	O
ui	O
with	O
uijdi	O
vi	O
under	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
this	O
implies	O
that	O
unconditionally	O
efuig	O
d	O
for	O
j	O
i	O
uj	O
is	O
a	O
function	B
of	O
di	O
yj	O
and	O
d	O
nx	O
i	O
varfuig	O
nx	O
e	O
ui	O
d	O
e	O
nx	O
nx	O
vi	O
ej	O
are	O
so	O
efuj	O
uijdig	O
d	O
and	O
again	O
unconditionally	O
efuj	O
uig	O
d	O
therefore	O
assuming	O
equal	O
hazard	O
rates	O
notes	O
and	O
details	O
the	O
last	O
approximation	O
replacing	O
unconditional	O
variances	O
varfuig	O
with	O
conditional	B
variances	O
vi	O
is	O
justified	O
in	O
crowley	O
as	O
is	O
the	O
asymptotic	O
normality	O
the	O
infinitesimal	O
interval	B
t	B
j	O
c	O
d	O
t	B
is	O
hi	O
d	O
t	B
so	O
rj	O
the	O
probability	O
pi	O
that	O
death	O
occurs	O
in	O
lemma	O
for	O
i	O
i	O
d	O
t	B
pi	O
d	O
y	O
pi	O
d	O
pi	O
pk	O
and	O
the	O
probability	O
of	O
event	O
ai	O
that	O
individual	O
i	O
dies	O
while	O
the	O
others	O
don	O
t	B
is	O
but	O
the	O
ai	O
are	O
disjoint	O
events	O
so	O
given	O
that	O
has	O
occurred	O
the	O
probability	O
that	O
it	O
is	O
individual	O
i	O
who	O
died	O
is	O
eci	O
x	O
eck	O
pj	O
pi	O
this	O
becoming	O
exactly	O
as	O
d	O
t	B
rj	O
rj	O
partial	B
likelihood	B
cox	O
introduced	O
partial	B
likelihood	B
as	O
inferential	O
justification	O
for	O
the	O
proportional	B
hazards	I
model	I
which	O
had	O
been	O
questioned	O
in	O
the	O
literature	O
let	O
dj	O
indicate	O
all	O
the	O
observable	O
information	B
available	O
just	O
before	O
time	O
t	B
j	O
including	O
all	O
the	O
death	O
or	O
loss	O
times	O
for	O
individuals	O
having	O
ti	O
t	B
j	O
that	O
dj	O
determines	O
the	O
risk	B
set	B
rj	O
by	O
successive	O
conditioning	O
we	O
write	O
the	O
full	B
likelihood	B
f	O
as	O
f	O
d	O
f	O
d	O
jy	O
jy	O
rj	O
f	O
f	O
letting	O
d	O
where	O
is	O
a	O
nuisance	O
parameter	O
vector	B
having	O
to	O
do	O
survival	B
analysis	B
and	O
the	O
em	B
algorithm	B
with	O
the	O
occurrence	O
and	O
timing	O
of	O
events	O
between	O
observed	O
deaths	O
jy	O
l	O
f	O
d	O
f	O
where	O
l	O
is	O
the	O
partial	B
likelihood	B
the	O
proportional	B
hazards	I
model	I
simply	O
ignores	O
the	O
bracketed	O
factor	B
in	O
l	O
d	O
log	O
l	O
is	O
treated	O
as	O
a	O
genuine	O
likelihood	B
maximized	O
to	O
give	O
o	O
and	O
assigned	O
covariance	O
matrix	B
as	O
in	O
section	O
efron	O
shows	O
this	O
tactic	O
is	O
highly	O
efficient	O
for	O
the	O
estimation	B
of	O
o	O
l	O
fake-data	B
principle	I
for	O
any	O
two	O
values	O
of	O
the	O
parameters	O
and	O
define	O
dz	O
log	O
u	O
d	O
u	O
this	O
being	O
the	O
limit	O
as	O
k	O
of	O
d	O
lim	O
log	O
u	O
k	O
kx	O
the	O
fake-data	O
log	O
likelihood	B
under	O
if	O
were	O
the	O
true	O
value	O
of	O
using	O
f	O
u	O
d	O
f	O
definition	O
gives	O
d	O
log	O
d	O
log	O
log	O
d	O
with	O
d	O
the	O
deviance	B
which	O
is	O
always	O
positive	O
unless	O
ujo	O
has	O
the	O
same	O
distribution	B
under	O
and	O
which	O
we	O
will	O
assume	O
doesn	O
t	B
happen	O
suppose	O
we	O
begin	O
the	O
em	B
algorithm	B
at	O
d	O
and	O
find	O
the	O
value	O
maximizing	O
then	O
and	O
d	O
implies	O
in	O
that	O
is	O
we	O
have	O
increased	O
the	O
likelihood	B
of	O
the	O
observed	O
data	B
now	O
take	O
d	O
o	O
d	O
arg	O
max	O
f	O
then	O
the	O
right	O
side	O
of	O
is	O
lo	O
for	O
any	O
not	O
equaling	O
d	O
o	O
o	O
negative	O
implying	O
lo	O
putting	O
this	O
successively	O
computing	O
by	O
fake-data	O
mle	B
calculations	O
increases	O
f	O
at	O
every	O
step	O
and	O
the	O
only	O
stable	O
point	O
of	O
the	O
algorithm	B
is	O
at	O
d	O
o	O
kaplan	O
meier	O
self-consistency	O
this	O
property	O
was	O
verified	O
in	O
efron	O
where	O
the	O
name	O
was	O
coined	O
generating	O
the	O
fake	O
data	B
is	O
equivalent	O
to	O
the	O
e	O
step	O
of	O
the	O
algorithm	B
the	O
m	O
step	O
being	O
the	O
maximization	O
of	O
lj	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
a	O
central	O
element	O
of	O
frequentist	B
inference	B
is	O
the	O
standard	B
error	I
an	O
algorithm	B
has	O
produced	O
an	O
estimate	B
of	O
a	O
parameter	O
of	O
interest	O
for	O
instance	O
the	O
mean	O
nx	O
d	O
for	O
the	O
all	O
scores	O
in	O
the	O
top	O
panel	O
of	O
figure	O
how	O
accurate	O
is	O
the	O
estimate	B
in	O
this	O
case	O
formula	B
for	O
the	O
standard	O
of	O
a	O
sample	B
mean	O
gives	O
estimated	O
standard	B
error	I
so	O
one	O
can	O
t	B
take	O
the	O
third	O
digit	O
of	O
nx	O
d	O
very	O
seriously	O
and	O
even	O
the	O
is	O
dubious	O
bse	O
d	O
direct	O
standard	B
error	I
formulas	O
like	O
exist	O
for	O
various	O
forms	O
of	O
averaging	B
such	O
as	O
linear	B
regression	B
and	O
for	O
hardly	O
anything	O
else	O
taylor	B
series	I
approximations	O
device	O
of	O
section	O
extend	O
the	O
formulas	O
to	O
smooth	O
functions	O
of	O
averages	O
as	O
in	O
before	O
computers	O
applied	O
statisticians	O
needed	O
to	O
be	O
taylor	B
series	I
experts	O
in	O
laboriously	O
pursuing	O
the	O
accuracy	B
of	O
even	O
moderately	O
complicated	O
statistics	B
the	O
jackknife	B
was	O
a	O
first	O
step	O
toward	O
a	O
computation-based	O
nonformulaic	O
approach	O
to	O
standard	O
errors	B
the	O
bootstrap	B
went	O
further	O
toward	O
automating	O
a	O
wide	O
variety	O
of	O
inferential	O
calculations	O
including	O
standard	O
errors	B
besides	O
sparing	O
statisticians	O
the	O
exhaustion	O
of	O
tedious	O
routine	O
calculations	O
the	O
jackknife	B
and	O
bootstrap	B
opened	O
the	O
door	O
for	O
more	O
complicated	O
estimation	B
algorithms	O
which	O
could	O
be	O
pursued	O
with	O
the	O
assurance	O
that	O
their	O
accuracy	B
would	O
be	O
easily	O
assessed	O
this	O
chapter	O
focuses	O
on	O
standard	O
errors	B
with	O
more	O
adventurous	O
bootstrap	B
ideas	O
deferred	O
to	O
chapter	O
we	O
end	O
with	O
a	O
brief	O
discussion	O
of	O
accuracy	B
estimation	B
for	O
robust	O
statistics	B
we	O
will	O
use	O
the	O
terms	O
standard	B
error	I
and	O
standard	B
deviation	I
interchangeably	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
the	O
jackknife	B
estimate	B
of	I
standard	B
error	I
the	O
basic	O
applications	O
of	O
the	O
jackknife	B
apply	O
to	O
one-sample	B
problems	I
where	O
the	O
statistician	O
has	O
observed	O
an	O
independent	O
and	O
identically	O
distributed	O
sample	B
x	O
d	O
xn	O
from	O
an	O
unknown	O
probability	O
distribution	B
f	O
on	O
some	O
space	O
x	O
x	O
can	O
be	O
anything	O
the	O
real	O
line	O
the	O
plane	O
a	O
function	B
a	O
real-valued	O
statistic	B
o	O
has	O
been	O
computed	O
by	O
applying	O
some	O
algorithm	B
to	O
x	O
xi	O
for	O
i	O
d	O
n	O
f	O
o	O
d	O
s	O
x	O
and	O
we	O
wish	O
to	O
assign	O
a	O
standard	B
error	I
to	O
o	O
the	O
standard	B
deviation	I
of	O
o	O
let	O
x	O
i	O
be	O
the	O
sample	B
with	O
xi	O
removed	O
d	O
s	O
x	O
under	O
sampling	O
model	O
that	O
is	O
we	O
wish	O
to	O
estimate	B
x	O
i	O
d	O
xn	O
and	O
denote	O
the	O
corresponding	O
value	O
of	O
the	O
statistic	B
of	O
interest	O
as	O
o	O
d	O
s	O
x	O
i	O
n	O
n	O
d	O
nx	O
then	O
the	O
jackknife	B
estimate	B
of	I
standard	B
error	I
for	O
o	O
o	O
nx	O
is	O
bsejack	O
d	O
with	O
o	O
o	O
in	O
the	O
case	O
where	O
o	O
is	O
the	O
mean	O
nx	O
of	O
real	O
values	O
xn	O
x	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
o	O
be	O
expressed	O
as	O
o	O
d	O
xi	O
d	O
nx	O
o	O
d	O
xi	O
and	O
is	O
their	O
average	O
excluding	O
xi	O
which	O
can	O
o	O
equation	B
gives	O
o	O
bsejack	O
d	O
nx	O
o	O
fudge	O
factor	B
in	O
definition	O
was	O
inserted	O
to	O
makebsejack	O
agree	O
exactly	O
the	O
same	O
as	O
the	O
classic	O
formula	B
this	O
is	O
no	O
coincidence	O
the	O
with	O
when	O
o	O
if	O
x	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
we	O
might	O
take	O
f	O
to	O
be	O
the	O
usual	O
cumulative	O
is	O
nx	O
distribution	B
function	B
but	O
here	O
we	O
will	O
just	O
think	O
of	O
f	O
as	O
any	O
full	B
description	O
of	O
the	O
probability	O
distribution	B
for	O
an	O
xi	O
on	O
x	O
the	O
jackknife	B
estimate	B
of	I
standard	B
error	I
the	O
advantage	O
ofbsejack	O
is	O
that	O
definition	O
can	O
be	O
applied	O
in	O
an	O
automatic	O
way	O
to	O
any	O
statistic	B
o	O
d	O
s	O
x	O
all	O
that	O
is	O
needed	O
is	O
an	O
algorithm	B
that	O
computes	O
for	O
the	O
deleted	O
data	B
sets	O
x	O
i	O
computer	O
power	O
is	O
being	O
substituted	O
for	O
theoretical	O
taylor	B
series	I
calculations	O
later	O
we	O
will	O
see	O
that	O
the	O
underlying	O
inferential	O
ideas	O
plug-in	O
estimation	B
of	O
frequentist	B
standard	O
errors	B
haven	O
t	B
changed	O
only	O
their	O
implementation	O
as	O
an	O
example	O
consider	O
the	O
kidney	B
function	B
data	B
set	B
of	O
section	O
here	O
the	O
data	B
consists	O
of	O
n	O
d	O
points	O
yi	O
with	O
x	O
d	O
age	O
and	O
y	O
d	O
tot	O
in	O
figure	O
the	O
generic	O
xi	O
in	O
now	O
represents	O
the	O
pair	O
yi	O
and	O
f	O
describes	O
a	O
distribution	B
in	O
the	O
plane	O
suppose	O
we	O
are	O
interested	O
in	O
the	O
correlation	O
between	O
age	O
and	O
tot	O
estimated	O
by	O
the	O
usual	O
sample	B
correlation	O
o	O
d	O
s	O
x	O
nx	O
s	O
x	O
d	O
nx	O
nx	O
applying	O
gavebsejack	O
d	O
for	O
the	O
accuracy	B
of	O
o	O
computed	O
to	O
be	O
o	O
d	O
for	O
the	O
kidney	O
data	B
casebsetaylor	O
d	O
nonparametric	B
bootstrap	B
computations	B
section	O
also	O
gave	O
estimated	O
standard	B
error	I
the	O
classic	O
taylor	B
series	I
formula	B
looks	O
quite	O
formidable	O
in	O
this	O
nx	O
yi	O
ny	O
o	O
c	O
c	O
c	O
d	O
nx	O
where	O
nxh	O
yi	O
nykn	O
need	O
be	O
assumed	O
it	O
gavebse	O
d	O
it	O
is	O
worth	O
emphasizing	O
some	O
features	O
of	O
the	O
jackknife	B
formula	B
it	O
is	O
nonparametric	B
no	O
special	O
form	B
of	O
the	O
underlying	O
distribution	B
f	O
inputs	O
the	O
data	B
set	B
x	O
and	O
the	O
function	B
s	O
x	O
and	O
outputsbsejack	O
it	O
is	O
completely	O
automatic	O
a	O
single	O
master	O
algorithm	B
can	O
be	O
written	O
that	O
the	O
algorithm	B
works	O
with	O
data	B
sets	O
of	O
size	O
not	O
n	O
there	O
is	O
a	O
hidden	O
assumption	O
of	O
smooth	O
behavior	O
across	O
sample	B
sizes	O
this	O
can	O
be	O
worrisome	O
for	O
statistics	B
like	O
the	O
sample	B
median	O
that	O
have	O
a	O
different	O
definition	O
for	O
odd	O
and	O
even	O
sample	B
size	I
the	O
jackknife	B
and	O
the	O
bootstrap	B
the	O
jackknife	B
standard	B
error	I
is	O
upwardly	O
biased	O
as	O
an	O
estimate	B
of	O
the	O
the	O
connection	O
of	O
the	O
jackknife	B
formula	B
with	O
taylor	B
series	I
meth	O
true	O
standard	B
error	I
ods	O
is	O
closer	O
than	O
it	O
appears	O
we	O
can	O
write	O
bsejack	O
dpn	O
i	O
where	O
di	O
d	O
p	O
o	O
o	O
n	O
n	O
as	O
discussed	O
in	O
section	O
the	O
di	O
are	O
approximate	O
directional	B
derivatives	I
measures	O
of	O
how	O
fast	O
the	O
statistic	B
s	O
x	O
is	O
changing	O
as	O
we	O
decrease	O
the	O
weight	O
on	O
data	B
point	O
xi	O
so	O
jack	O
is	O
proportional	O
to	O
the	O
sum	O
of	O
squared	O
derivatives	O
of	O
s	O
x	O
in	O
the	O
n	O
component	O
directions	O
taylor	B
series	I
expressions	O
such	O
as	O
amount	O
to	O
doing	O
the	O
derivatives	O
by	O
formula	B
rather	O
than	O
numerically	O
figure	O
the	O
lowess	B
curve	O
for	O
the	O
kidney	O
data	B
of	O
figure	O
vertical	O
bars	O
indicate	O
standard	O
errors	B
jackknife	B
blue	O
dashed	O
bootstrap	B
red	O
solid	O
the	O
jackknife	B
greatly	O
overestimates	O
variability	O
at	O
age	O
the	O
principal	O
weakness	O
of	O
the	O
jackknife	B
is	O
its	O
dependence	O
on	O
local	O
derivafigure	O
can	O
result	O
in	O
erratic	O
behavior	O
forbsejack	O
figure	O
illustrates	O
tives	O
unsmooth	O
statistics	B
s	O
x	O
such	O
as	O
the	O
kidney	O
data	B
lowess	B
curve	O
in	O
the	O
point	O
the	O
dashed	O
blue	O
vertical	O
bars	O
indicate	O
jackknife	B
standard	O
er	O
the	O
nonparametric	B
bootstrap	B
rors	O
for	O
the	O
lowess	B
curve	O
evaluated	O
at	O
ages	O
for	O
the	O
most	O
part	O
these	O
agree	O
with	O
the	O
dependable	O
bootstrap	B
standard	O
errors	B
solid	O
red	O
bars	O
described	O
in	O
section	O
but	O
things	O
go	O
awry	O
at	O
age	O
where	O
the	O
local	O
derivatives	O
greatly	O
overstate	O
the	O
sensitivity	O
of	O
the	O
lowess	B
curve	O
to	O
global	O
changes	O
in	O
the	O
sample	B
x	O
the	O
nonparametric	B
bootstrap	B
from	O
the	O
point	O
of	O
view	O
of	O
the	O
bootstrap	B
the	O
jackknife	B
was	O
a	O
halfway	O
house	O
between	O
classical	O
methodology	O
and	O
a	O
full-throated	O
use	O
of	O
electronic	O
computation	O
term	O
computer-intensive	B
statistics	B
was	O
coined	O
to	O
describe	O
the	O
bootstrap	B
the	O
frequentist	B
standard	B
error	I
of	O
an	O
estimate	B
o	O
d	O
s	O
x	O
is	O
ideally	O
the	O
standard	B
deviation	I
we	O
would	O
observe	O
by	O
repeatedly	O
sampling	O
new	O
versions	O
of	O
x	O
from	O
f	O
this	O
is	O
impossible	O
since	O
f	O
is	O
unknown	O
instead	O
the	O
bootstrap	B
ingenious	O
device	O
number	O
in	O
section	O
substitutes	O
an	O
estimate	B
of	O
for	O
f	O
and	O
then	O
estimates	O
the	O
frequentist	B
standard	O
by	O
direct	O
simulation	B
a	O
feasible	O
tactic	O
only	O
since	O
the	O
advent	O
of	O
electronic	O
computation	O
the	O
bootstrap	B
estimate	B
of	I
standard	B
error	I
for	O
a	O
statistic	B
o	O
d	O
s	O
x	O
computed	O
from	O
a	O
random	O
sample	B
x	O
d	O
xn	O
begins	O
with	O
the	O
notion	O
of	O
a	O
bootstrap	B
sample	B
d	O
x	O
x	O
n	O
x	O
where	O
each	O
x	O
i	O
is	O
drawn	O
randomly	O
with	O
equal	O
probability	O
and	O
with	O
replacement	O
from	O
xng	O
each	O
bootstrap	B
sample	B
provides	O
a	O
bootstrap	B
replication	B
of	O
the	O
statistic	B
of	O
some	O
large	O
number	O
b	O
of	O
bootstrap	B
samples	O
are	O
independently	O
drawn	O
d	O
in	O
figure	O
the	O
corresponding	O
bootstrap	B
replications	O
are	O
calculated	O
say	O
o	O
for	O
b	O
d	O
b	O
d	O
s	O
x	O
the	O
resulting	O
bootstrap	B
estimate	B
of	I
standard	B
error	I
for	O
o	O
standard	B
deviation	I
of	O
the	O
o	O
bseboot	O
d	O
o	O
bx	O
with	O
o	O
values	O
o	O
is	O
the	O
empirical	B
d	O
bx	O
b	O
o	O
is	O
intended	O
to	O
avoid	O
confusion	O
with	O
the	O
original	O
data	B
x	O
which	O
stays	O
the	O
star	O
notation	O
x	O
fixed	O
in	O
bootstrap	B
computations	B
and	O
likewise	O
o	O
vis-a-vis	O
o	O
o	O
d	O
s	O
x	O
f	O
x	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
motivation	O
forbseboot	O
begins	O
by	O
noting	O
that	O
o	O
is	O
obtained	O
in	O
two	O
steps	O
first	O
x	O
is	O
generated	O
by	O
iid	O
sampling	O
from	O
probability	O
distribution	B
f	O
and	O
then	O
o	O
is	O
calculated	O
from	O
x	O
according	O
to	O
algorithm	B
o	O
we	O
don	O
t	B
know	O
f	O
but	O
we	O
can	O
estimate	B
it	O
by	O
the	O
empirical	B
probability	I
distribution	B
of	O
that	O
puts	O
probability	O
on	O
each	O
point	O
xi	O
weight	O
on	O
each	O
point	O
yi	O
in	O
figure	O
notice	O
that	O
a	O
bootstrap	B
sample	B
x	O
is	O
an	O
iid	O
sample	B
drawn	O
from	O
of	O
since	O
then	O
each	O
x	O
independently	O
has	O
equal	O
probability	O
of	O
being	O
any	O
member	O
of	O
xng	O
it	O
can	O
be	O
shown	O
that	O
of	O
maximizes	O
the	O
probability	O
of	O
obtaining	O
the	O
observed	O
sample	B
x	O
under	O
all	O
possible	O
choices	O
of	O
f	O
in	O
i	O
e	O
it	O
is	O
the	O
nonparametric	B
mle	B
of	O
f	O
bootstrap	B
replications	O
o	O
are	O
obtained	O
by	O
a	O
process	O
analogous	O
to	O
of	O
in	O
the	O
real	O
world	O
we	O
only	O
get	O
to	O
see	O
the	O
single	O
value	O
o	O
but	O
the	O
bootstrap	B
world	O
is	O
more	O
generous	O
we	O
can	O
generate	O
as	O
many	O
bootstrap	B
replications	O
o	O
as	O
we	O
want	O
or	O
have	O
time	O
for	O
and	O
directly	O
estimate	B
their	O
suggests	O
correctly	O
in	O
most	O
cases	O
thatbseboot	O
approaches	O
the	O
true	O
standard	O
of	O
approaches	O
f	O
as	O
n	O
grows	O
large	O
variability	O
as	O
in	O
the	O
fact	O
that	O
error	O
of	O
o	O
the	O
true	O
standard	B
deviation	I
of	O
o	O
i	O
e	O
its	O
standard	B
error	I
can	O
be	O
thought	O
of	O
as	O
a	O
function	B
of	O
the	O
probability	O
distribution	B
f	O
that	O
generates	O
the	O
data	B
say	O
sd	O
f	O
hypothetically	O
sd	O
f	O
inputs	O
f	O
and	O
outputs	O
the	O
standard	B
deviation	I
of	O
o	O
which	O
we	O
can	O
imagine	O
being	O
evaluated	O
by	O
independently	O
running	O
some	O
enormous	O
number	O
of	O
times	O
n	O
and	O
then	O
computing	O
the	O
empirical	B
standard	B
deviation	I
of	O
the	O
resulting	O
o	O
x	O
o	O
values	O
with	O
o	O
d	O
nx	O
n	O
o	O
nx	O
o	O
o	O
sd	O
f	O
d	O
bseboot	O
d	O
sd	O
of	O
the	O
bootstrap	B
standard	B
error	I
of	O
o	O
is	O
the	O
plug-in	O
estimate	B
more	O
exactly	O
sd	O
of	O
is	O
the	O
ideal	O
bootstrap	B
estimate	B
of	I
standard	B
error	I
what	O
we	O
would	O
get	O
by	O
letting	O
the	O
number	O
of	O
bootstrap	B
replications	O
b	O
go	O
to	O
infinity	O
in	O
practice	O
we	O
have	O
to	O
stop	O
at	O
some	O
finite	O
value	O
of	O
b	O
as	O
discussed	O
in	O
what	O
follows	O
the	O
nonparametric	B
bootstrap	B
and	O
multisample	B
versions	O
will	O
be	O
taken	O
up	O
later	O
as	O
with	O
the	O
jackknife	B
there	O
are	O
several	O
important	O
points	O
worth	O
empha	O
sizing	O
aboutbseboot	O
ten	O
that	O
inputs	O
the	O
data	B
x	O
and	O
the	O
function	B
and	O
outputsbseboot	O
it	O
is	O
completely	O
automatic	O
once	O
again	O
a	O
master	O
algorithm	B
can	O
be	O
we	O
have	O
described	O
the	O
one-sample	O
nonparametric	B
bootstrap	B
parametric	B
bootstrapping	O
shakes	O
the	O
original	O
data	B
more	O
violently	O
than	O
from	O
x	O
the	O
bootstrap	B
is	O
more	O
ing	O
producing	O
nonlocal	O
deviations	O
of	O
x	O
dependable	O
than	O
the	O
jackknife	B
for	O
unsmooth	O
statistics	B
since	O
it	O
doesn	O
t	B
b	O
d	O
is	O
usually	O
sufficient	O
for	O
evaluatingbseboot	O
larger	O
values	O
depend	O
on	O
local	O
derivatives	O
or	O
will	O
be	O
required	O
for	O
the	O
bootstrap	B
confidence	O
intervals	B
of	O
chapter	O
there	O
is	O
nothing	O
special	O
about	O
standard	O
errors	B
we	O
could	O
just	O
as	O
well	O
use	O
the	O
bootstrap	B
replications	O
to	O
estimate	B
the	O
expected	O
absolute	O
error	O
efj	O
o	O
fisher	B
s	O
mle	B
formula	B
is	O
applied	O
in	O
practice	O
via	O
or	O
any	O
other	O
accuracy	B
measure	O
bsefisher	O
d	O
that	O
is	O
by	O
plugging	O
in	O
o	O
for	O
after	O
a	O
theoretical	O
calculation	O
of	O
se	O
the	O
bootstrap	B
operates	O
in	O
the	O
same	O
way	O
at	O
though	O
the	O
plugging	O
in	O
is	O
done	O
before	O
rather	O
than	O
after	O
the	O
calculation	O
the	O
connection	O
with	O
fisherian	O
theory	B
is	O
more	O
obvious	O
for	O
the	O
parametric	B
bootstrap	B
of	O
section	O
the	O
jackknife	B
is	O
a	O
completely	O
frequentist	B
device	O
both	O
in	O
its	O
assumptions	O
and	O
in	O
its	O
applications	O
errors	B
and	O
biases	O
the	O
bootstrap	B
is	O
also	O
basically	O
frequentist	B
but	O
with	O
a	O
touch	O
of	O
the	O
fisherian	O
as	O
in	O
the	O
relation	O
with	O
its	O
versatility	O
has	O
led	O
to	O
applications	O
in	O
a	O
variety	O
of	O
estimation	B
and	O
prediction	O
problems	O
with	O
even	O
some	O
bayesian	B
connections	O
unusual	O
applications	O
can	O
also	O
pop	O
up	O
for	O
the	O
jackknife	B
see	O
the	O
jackknifeafter-bootstrap	O
comment	O
in	O
the	O
chapter	O
endnotes	O
from	O
a	O
classical	O
point	O
of	O
view	O
the	O
bootstrap	B
is	O
an	O
incredible	O
computational	O
spendthrift	O
classical	O
statistics	B
was	O
fashioned	O
to	O
minimize	O
the	O
hard	O
labor	O
of	O
mechanical	O
computation	O
the	O
bootstrap	B
seems	O
to	O
go	O
out	O
of	O
its	O
way	O
to	O
multiply	O
it	O
by	O
factors	O
of	O
b	O
d	O
or	O
or	O
more	O
it	O
is	O
nice	O
to	O
report	O
that	O
all	O
this	O
computational	O
largesse	O
can	O
have	O
surprising	O
data	B
analytic	O
payoffs	O
the	O
students	O
of	O
table	O
actually	O
each	O
took	O
five	O
tests	O
mechanics	O
vectors	O
algebra	O
analytics	O
and	O
statistics	B
table	O
shows	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
table	O
correlation	O
matrix	B
for	O
the	O
student	B
score	I
data	B
the	O
eigenvalues	O
are	O
and	O
the	O
eigenratio	B
statistic	B
o	O
d	O
and	O
its	O
bootstrap	B
standard	B
error	I
estimate	B
is	O
d	O
mechanics	O
vectors	O
algebra	O
analytics	O
statistics	B
mechanics	O
vectors	O
algebra	O
analysis	B
statistics	B
the	O
sample	B
correlation	O
matrix	B
and	O
also	O
its	O
eigenvalues	O
the	O
eigenratio	B
statistic	B
fidence	O
interval	B
calculations	O
the	O
jackknife	B
gave	O
a	O
bigger	O
estimate	B
o	O
d	O
largest	O
eigenvaluesum	O
eigenvalues	O
measures	O
how	O
closely	O
the	O
five	O
scores	O
can	O
be	O
predicted	O
by	O
a	O
single	O
linear	B
combination	O
essentially	O
an	O
iq	O
score	O
for	O
each	O
student	O
o	O
d	O
here	O
indicating	O
strong	O
predictive	O
power	O
for	O
the	O
iq	O
score	O
how	O
accurate	O
is	O
ror	O
estimate	B
d	O
was	O
times	O
more	O
bootstraps	O
b	O
d	O
bootstrap	B
replications	O
yielded	O
bootstrap	B
standard	O
erthan	O
necessary	O
forbseboot	O
but	O
will	O
be	O
needed	O
for	O
chapter	O
s	O
bootstrap	B
conbsejack	O
d	O
for	O
coverage	B
these	O
are	O
based	O
on	O
an	O
assumpstandard	O
errors	B
are	O
usually	O
used	O
to	O
suggest	O
approximate	O
confidence	O
intervals	B
often	O
o	O
tion	O
of	O
normality	O
for	O
o	O
the	O
histogram	O
of	O
the	O
bootstrap	B
replications	O
of	O
o	O
as	O
seen	O
in	O
figure	O
disabuses	O
belief	O
in	O
even	O
approximate	O
normality	O
compared	O
with	O
classical	O
methods	O
a	O
massive	O
amount	O
of	O
computation	O
has	O
gone	O
into	O
the	O
histogram	O
but	O
this	O
will	O
pay	O
off	O
in	O
chapter	O
with	O
more	O
accurate	O
confidence	O
limits	O
we	O
can	O
claim	O
a	O
double	O
reward	O
here	O
for	O
bootstrap	B
methods	O
much	O
wider	O
applicability	O
and	O
improved	O
inferences	O
the	O
bootstrap	B
histogram	O
invisible	O
to	O
classical	O
statisticians	O
nicely	O
illustrates	O
the	O
advantages	O
of	O
computer-age	O
statistical	O
inference	B
resampling	B
plans	B
there	O
is	O
a	O
second	O
way	O
to	O
think	O
about	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
as	O
algorithms	O
that	O
reweight	O
or	O
resample	O
the	O
original	O
data	B
vector	B
x	O
d	O
resampling	B
plans	B
figure	O
histogram	O
of	O
b	O
d	O
bootstrap	B
replications	O
o	O
for	O
the	O
eigenratio	B
statistic	B
for	O
the	O
student	B
score	I
data	B
the	O
vertical	O
black	O
line	O
is	O
at	O
o	O
d	O
the	O
long	O
left	O
tail	O
shows	O
that	O
normality	O
is	O
a	O
dangerous	O
assumption	O
in	O
this	O
case	O
at	O
the	O
price	O
of	O
a	O
little	O
more	O
abstraction	O
resampling	B
is	O
by	O
definition	O
a	O
vector	B
of	O
xn	O
nects	O
the	O
two	O
algorithms	O
and	O
suggests	O
a	O
class	O
of	O
other	O
possibilities	O
a	O
resampling	B
vector	B
p	O
d	O
pn	O
nonnegative	O
weights	B
summing	O
to	O
nx	O
p	O
d	O
pn	O
with	O
pi	O
and	O
pi	O
d	O
that	O
is	O
p	O
is	O
a	O
member	O
of	O
the	O
simplex	B
sn	O
resampling	B
plans	B
operate	O
by	O
holding	O
the	O
original	O
data	B
set	B
x	O
fixed	O
and	O
seeing	O
how	O
the	O
statistic	B
of	O
interest	O
o	O
we	O
denote	O
the	O
value	O
of	O
o	O
changes	O
as	O
the	O
weight	O
vector	B
p	O
varies	O
across	O
sn	O
for	O
a	O
vector	B
putting	O
weight	O
pi	O
on	O
xi	O
as	O
o	O
d	O
s	O
p	O
the	O
star	O
notation	O
now	O
indicating	O
any	O
reweighting	O
not	O
necessarily	O
from	O
bootstrapping	O
o	O
d	O
s	O
x	O
describes	O
the	O
behavior	O
of	O
o	O
mean	O
s	O
x	O
d	O
nx	O
we	O
have	O
s	O
p	O
d	O
pn	O
in	O
the	O
real	O
world	O
while	O
o	O
d	O
s	O
p	O
describes	O
it	O
in	O
the	O
resampling	B
world	O
for	O
the	O
sample	B
pi	O
xi	O
the	O
unbiased	O
estimate	B
of	O
errorbootstrap	O
variance	B
s	O
x	O
dpn	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
i	O
can	O
be	O
seen	O
to	O
have	O
s	O
p	O
d	O
n	O
n	O
pi	O
i	O
pi	O
xi	O
nx	O
nx	O
figure	O
resampling	B
simplex	B
for	O
sample	B
size	I
n	O
d	O
the	O
center	O
point	O
is	O
the	O
green	O
circles	O
are	O
the	O
jackknife	B
points	O
p	O
i	O
triples	O
indicate	O
bootstrap	B
resampling	B
numbers	O
the	O
bootstrap	B
probabilities	B
are	O
for	O
for	O
each	O
corner	O
point	O
and	O
for	O
each	O
of	O
the	O
six	O
starred	O
points	O
letting	O
d	O
the	O
resampling	B
vector	B
putting	O
equal	O
weight	O
on	O
each	O
value	O
xi	O
we	O
require	O
in	O
the	O
definition	O
of	O
that	O
d	O
s	O
x	O
d	O
o	O
the	O
original	O
estimate	B
the	O
ith	O
jackknife	B
value	O
o	O
corresponds	O
to	O
resampling	B
plans	B
resampling	B
vector	B
p	O
i	O
d	O
with	O
in	O
the	O
ith	O
place	O
figure	O
illustrates	O
the	O
resampling	B
simplex	B
applying	O
to	O
sample	B
size	I
n	O
d	O
with	O
the	O
center	O
point	O
being	O
and	O
the	O
open	O
circles	O
the	O
three	O
possible	O
jackknife	B
vectors	O
p	O
i	O
with	O
n	O
d	O
sample	B
points	O
there	O
are	O
only	O
distinct	O
bootstrap	B
vectors	O
also	O
shown	O
in	O
figure	O
let	O
ni	O
d	O
the	O
number	O
of	O
bootstrap	B
draws	O
in	O
x	O
ure	O
are	O
for	O
example	O
for	O
x	O
the	O
bootstrap	B
resampling	B
vectors	O
are	O
of	O
the	O
form	B
equaling	O
xi	O
the	O
triples	O
in	O
the	O
having	O
once	O
and	O
d	O
xig	O
j	O
d	O
nn	O
p	O
where	O
the	O
ni	O
are	O
nonnegative	O
integers	O
summing	O
to	O
n	O
according	O
to	O
definition	O
of	O
bootstrap	B
sampling	O
the	O
vector	B
n	O
d	O
nn	O
follows	O
a	O
multinomial	O
distribution	B
with	O
n	O
draws	O
on	O
n	O
equally	O
likely	O
categories	O
n	O
multn	O
n	O
this	O
gives	O
bootstrap	B
probability	O
n	O
nn	O
nn	O
on	O
p	O
figure	O
is	O
misleading	O
in	O
that	O
the	O
jackknife	B
vectors	O
p	O
i	O
appear	O
only	O
as	O
n	O
grows	O
large	O
slightly	O
closer	O
to	O
than	O
are	O
the	O
bootstrap	B
vectors	O
p	O
they	O
are	O
in	O
fact	O
an	O
order	O
of	O
magnitude	O
closer	O
subtracting	O
from	O
gives	O
euclidean	O
distance	O
kp	O
i	O
d	O
ni	O
bi	O
n	O
n	O
n	O
for	O
the	O
bootstrap	B
notice	O
that	O
ni	O
in	O
has	O
a	O
binomial	B
distribution	B
a	O
hidden	O
assumption	O
of	O
definition	O
is	O
that	O
o	O
d	O
s	O
x	O
has	O
the	O
same	O
value	O
for	O
any	O
permutation	O
of	O
x	O
so	O
for	O
instance	O
d	O
d	O
n	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
with	O
mean	O
and	O
variance	B
then	O
p	O
d	O
ni	O
has	O
mean	O
and	O
variance	B
adding	O
over	O
the	O
n	O
coordinates	O
gives	O
the	O
expected	O
root	O
mean	O
square	O
distance	O
for	O
bootstrap	B
vector	B
p	O
i	O
dp	O
p	O
n	O
times	O
further	O
than	O
an	O
order	O
of	O
magnitude	O
the	O
function	B
s	O
p	O
has	O
approximate	O
directional	O
derivative	O
di	O
d	O
s	O
p	O
i	O
kp	O
i	O
o	O
ideal	O
bootstrap	B
standard	B
error	I
estimate	B
in	O
the	O
direction	O
from	O
toward	O
p	O
i	O
along	O
the	O
dashed	O
lines	O
in	O
figure	O
di	O
measures	O
the	O
slope	O
of	O
function	B
s	O
p	O
at	O
in	O
the	O
direction	O
of	O
p	O
i	O
formula	B
showsbsejack	O
as	O
proportional	O
to	O
the	O
root	O
out	O
thatbsejack	O
equalsbseboot	O
for	O
the	O
fudge	O
factor	B
in	O
if	O
s	O
p	O
is	O
a	O
linear	B
function	B
of	O
p	O
as	O
it	O
is	O
for	O
the	O
sample	B
mean	O
it	O
turns	O
mean	O
square	O
of	O
the	O
slopes	O
most	O
statistics	B
are	O
not	O
linear	B
and	O
then	O
the	O
local	O
jackknife	B
resamples	O
may	O
provide	O
a	O
poor	O
approximation	O
to	O
the	O
full	B
resampling	B
behavior	O
of	O
s	O
p	O
this	O
was	O
the	O
case	O
at	O
one	O
point	O
in	O
figure	O
we	O
can	O
easily	O
evaluate	O
the	O
with	O
only	O
possible	O
resampling	B
points	O
p	O
d	O
o	O
pk	O
pk	O
o	O
o	O
bseboot	O
d	O
with	O
o	O
d	O
s	O
p	O
k	O
and	O
pk	O
the	O
probability	O
from	O
in	O
fig	O
ure	O
this	O
rapidly	O
becomes	O
impractical	O
the	O
number	O
of	O
distinct	O
bootstrap	B
samples	O
for	O
n	O
points	O
turns	O
out	O
to	O
be	O
for	O
n	O
d	O
this	O
is	O
already	O
while	O
n	O
d	O
gives	O
distinct	O
at	O
random	O
which	O
is	O
what	O
alpossible	O
resamples	O
choosing	O
b	O
vectors	O
p	O
gorithm	O
effectively	O
is	O
doing	O
makes	O
the	O
un-ideal	O
bootstrap	B
standard	B
error	I
estimate	B
almost	O
as	O
accurate	O
as	O
for	O
b	O
as	O
small	O
as	O
or	O
even	O
less	O
n	O
the	O
luxury	O
of	O
examining	O
the	O
resampling	B
surface	O
provides	O
a	O
major	O
advantage	O
to	O
modern	O
statisticians	O
both	O
in	O
inference	B
and	O
methodology	O
a	O
variety	O
of	O
other	O
resampling	B
schemes	O
have	O
been	O
proposed	O
a	O
few	O
of	O
which	O
follow	O
resampling	B
plans	B
the	O
infinitesimal	O
jackknife	B
looking	O
at	O
figure	O
again	O
the	O
vector	B
pi	O
d	O
c	O
d	O
c	O
lies	O
proportion	B
of	O
the	O
way	O
from	O
to	O
p	O
i	O
then	O
s	O
qdi	O
d	O
lim	O
i	O
nx	O
bseij	O
d	O
exactly	O
defines	O
the	O
direction	O
derivative	O
at	O
in	O
the	O
direction	O
of	O
p	O
i	O
the	O
infinitesimal	O
jackknife	B
estimate	B
of	I
standard	B
error	I
is	O
usually	O
evaluated	O
numerically	O
by	O
setting	O
to	O
some	O
small	O
value	O
in	O
than	O
d	O
in	O
we	O
will	O
meet	O
the	O
infinitesimal	O
jackknife	B
again	O
in	O
chapters	O
and	O
multisample	B
bootstrap	B
the	O
median	O
difference	O
between	O
the	O
aml	O
and	O
the	O
all	O
scores	O
in	O
figure	O
is	O
mediff	O
d	O
d	O
how	O
accurate	O
is	O
an	O
appropriate	O
form	B
of	O
bootstrapping	O
draws	O
times	O
with	O
replacement	O
from	O
the	O
aml	O
patients	O
times	O
with	O
replacement	O
from	O
the	O
all	O
patients	O
and	O
computes	O
as	O
the	O
difference	O
between	O
the	O
medians	O
of	O
the	O
two	O
bootstrap	B
samples	O
one	O
bootstrap	B
sample	B
of	O
size	O
from	O
all	O
the	O
patients	O
would	O
result	O
in	O
random	O
sample	B
sizes	O
for	O
the	O
groups	O
adding	O
inappropriate	O
variability	O
to	O
the	O
frequentist	B
standard	B
error	I
estimate	B
givebseboot	O
d	O
the	O
estimate	B
is	O
units	O
above	O
zero	O
agreea	O
histogram	O
of	O
b	O
d	O
values	O
appears	O
in	O
figure	O
they	O
ing	O
surprisingly	O
well	O
with	O
the	O
usual	O
two-sample	B
t-statistic	B
on	O
mean	O
differences	O
and	O
its	O
permutation	O
histogram	O
figure	O
permutation	O
testing	B
can	O
be	O
considered	O
another	O
form	B
of	O
resampling	B
the	O
jackknife	B
and	O
the	O
bootstrap	B
figure	O
b	O
d	O
bootstrap	B
replications	O
for	O
the	O
median	O
bseboot	O
d	O
the	O
observed	O
value	O
mediff	O
d	O
difference	O
between	O
the	O
aml	O
and	O
all	O
scores	O
in	O
figure	O
giving	O
black	O
line	O
is	O
more	O
than	O
standard	O
errors	B
above	O
zero	O
moving	B
blocks	I
bootstrap	B
suppose	O
x	O
d	O
xn	O
instead	O
of	O
being	O
an	O
iid	O
sample	B
is	O
a	O
time	O
series	O
that	O
is	O
the	O
x	O
values	O
occur	O
in	O
a	O
meaningful	O
order	O
perhaps	O
with	O
nearby	O
observations	O
highly	O
correlated	O
with	O
each	O
other	O
let	O
bm	O
be	O
the	O
set	B
of	O
contiguous	O
blocks	O
of	O
length	O
m	O
for	O
example	O
d	O
xng	O
presumably	O
m	O
is	O
chosen	O
large	O
enough	O
that	O
correlations	O
between	O
xi	O
and	O
xj	O
jj	O
ij	O
m	O
are	O
neglible	O
the	O
moving	O
block	O
bootstrap	B
first	O
selects	O
nm	O
having	O
constructed	O
b	O
such	O
samplesbseboot	O
is	O
calculated	O
blocks	O
from	O
bm	O
and	O
assembles	O
them	O
in	O
random	O
order	O
to	O
construct	O
a	O
bootstrap	B
sample	B
x	O
as	O
in	O
the	O
bayesian	B
bootstrap	B
let	O
gn	O
be	O
independent	O
one-sided	O
exponential	O
variates	O
in	O
table	O
each	O
having	O
density	B
for	O
x	O
the	O
parametric	B
bootstrap	B
nx	O
the	O
bayesian	B
bootstrap	B
uses	O
resampling	B
vectors	O
p	O
it	O
can	O
be	O
shown	O
that	O
p	O
d	O
gn	O
is	O
then	O
uniformly	O
distributed	O
over	O
the	O
resampling	B
simplex	B
sn	O
for	O
n	O
d	O
uniformly	O
distributed	O
over	O
the	O
triangle	O
in	O
fig	O
ure	O
prescription	O
is	O
motivated	O
by	O
assuming	O
a	O
jeffreys-style	O
uninformative	B
prior	B
distribution	B
on	O
the	O
unknown	O
distribution	B
f	O
gi	O
has	O
mean	O
vector	B
and	O
covariance	O
matrix	B
distribution	B
for	O
p	O
p	O
n	O
c	O
this	O
is	O
almost	O
identical	O
to	O
the	O
mean	O
and	O
covariance	O
of	O
bootstrap	B
resamples	O
multn	O
n	O
p	O
p	O
n	O
the	O
bayesian	B
bootstrap	B
and	O
the	O
ordinary	O
bootstrap	B
tend	O
to	O
agree	O
at	O
least	O
for	O
smoothly	O
defined	O
statistics	B
o	O
there	O
was	O
some	O
bayesian	B
disparagement	O
of	O
the	O
bootstrap	B
when	O
it	O
first	O
appeared	O
because	O
of	O
its	O
blatantly	O
frequentist	B
take	O
on	O
estimation	B
accuracy	B
and	O
yet	O
connections	O
like	O
have	O
continued	O
to	O
pop	O
up	O
as	O
we	O
will	O
see	O
in	O
chapter	O
d	O
s	O
p	O
the	O
parametric	B
bootstrap	B
in	O
our	O
description	O
of	O
bootstrap	B
resampling	B
of	O
x	O
o	O
there	O
is	O
no	O
need	O
to	O
insist	O
that	O
of	O
be	O
the	O
nonparametric	B
mle	B
of	O
f	O
suppose	O
d	O
we	O
are	O
willing	O
to	O
assume	O
that	O
the	O
observed	O
data	B
vector	B
x	O
comes	O
from	O
a	O
parametric	B
family	I
f	O
as	O
in	O
let	O
be	O
the	O
mle	B
of	O
the	O
bootstrap	B
parametric	B
resamples	O
from	O
f	O
and	O
proceeds	O
as	O
in	O
to	O
calculatebseboot	O
f	O
x	O
o	O
f	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
as	O
an	O
example	O
suppose	O
that	O
x	O
d	O
xn	O
is	O
an	O
iid	O
sample	B
of	O
size	O
n	O
from	O
a	O
normal	B
distribution	B
xi	O
n	O
i	O
d	O
n	O
then	O
d	O
nx	O
and	O
a	O
parametric	B
bootstrap	B
sample	B
is	O
x	O
where	O
d	O
x	O
x	O
n	O
i	O
d	O
n	O
n	O
x	O
i	O
more	O
adventurously	O
if	O
f	O
were	O
a	O
family	O
of	O
time	O
series	O
models	B
for	O
x	O
algorithm	B
would	O
still	O
apply	O
without	O
any	O
iid	O
structure	B
x	O
would	O
be	O
a	O
time	O
series	O
sampled	O
from	O
model	O
f	O
and	O
o	O
d	O
s	O
x	O
the	O
b	O
d	O
b	O
andbseboot	O
from	O
would	O
give	O
resampled	O
statistic	B
of	O
interest	O
b	O
independent	O
realizations	O
x	O
o	O
figure	O
the	O
gfr	O
data	B
of	O
figure	O
curves	O
show	O
the	O
mle	B
fits	O
from	O
polynomial	O
poisson	B
models	B
for	O
degrees	B
of	I
freedom	I
df	O
d	O
the	O
points	O
on	O
the	O
curves	O
show	O
the	O
fits	O
computed	O
at	O
the	O
centers	O
x	O
j	O
of	O
the	O
bins	O
with	O
the	O
responses	O
being	O
the	O
counts	O
in	O
the	O
bins	O
the	O
dashes	O
at	O
the	O
base	O
of	O
the	O
plot	O
show	O
the	O
nine	O
gfr	O
values	O
appearing	O
in	O
table	O
as	O
an	O
example	O
of	O
parametric	B
bootstrapping	O
figure	O
expands	O
the	O
gfr	O
investigation	O
of	O
figure	O
in	O
addition	O
to	O
the	O
seventh-degree	O
polynomial	O
fit	O
we	O
now	O
show	O
lower-degree	O
polynomial	O
fits	O
for	O
the	O
parametric	B
bootstrap	B
and	O
degrees	B
of	I
freedom	I
df	O
d	O
obviously	O
gives	O
a	O
poor	O
fit	O
df	O
d	O
give	O
nearly	O
identical	O
curves	O
df	O
d	O
gives	O
only	O
a	O
slightly	O
better	O
fit	O
to	O
the	O
raw	O
data	B
the	O
plotted	O
curves	O
were	O
obtained	O
from	O
the	O
poisson	B
regression	B
method	B
used	O
in	O
section	O
which	O
we	O
refer	O
to	O
as	O
lindsey	O
s	O
method	B
the	O
x-axis	O
was	O
partitioned	O
into	O
k	O
d	O
bins	O
with	O
endpoints	O
and	O
centerpoints	O
say	O
x	O
d	O
x	O
k	O
d	O
d	O
etc	O
count	O
vector	B
y	O
d	O
yk	O
was	O
computed	O
yk	O
d	O
in	O
binkg	O
y	O
gives	O
the	O
heights	O
of	O
the	O
bars	O
in	O
figure	O
an	O
independent	O
poisson	B
model	O
was	O
assumed	O
for	O
the	O
counts	O
for	O
k	O
d	O
k	O
yk	O
the	O
parametric	B
model	O
of	O
degree	O
df	O
assumed	O
that	O
the	O
values	O
were	O
described	O
by	O
an	O
exponential	O
polynomial	O
of	O
degree	O
df	O
in	O
the	O
x	O
k	O
values	O
j	O
xj	O
the	O
mle	B
o	O
d	O
the	O
plotted	O
curves	O
in	O
figure	O
trace	O
the	O
mle	B
values	O
o	O
df	O
in	O
model	O
was	O
o	O
o	O
d	O
dfx	O
log	O
d	O
dfx	O
o	O
j	O
xj	O
y	O
k	O
how	O
accurate	O
are	O
the	O
curves	O
parametric	B
bootstraps	O
were	O
used	O
to	O
assess	O
their	O
standard	O
errors	B
that	O
is	O
poisson	B
resamples	O
were	O
generated	O
according	O
to	O
for	O
k	O
d	O
k	O
poi	O
and	O
bootstrap	B
mle	B
values	O
k	O
calculated	O
as	O
above	O
but	O
now	O
based	O
on	O
count	O
rather	O
than	O
y	O
all	O
of	O
this	O
was	O
done	O
b	O
d	O
times	O
yielding	O
vector	B
y	O
the	O
results	O
appear	O
in	O
table	O
showing	O
bseboot	O
for	O
df	O
d	O
bootstrap	B
standard	O
errors	B
a	O
single	O
r	B
command	O
accomplishes	O
this	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
table	O
bootstrap	B
estimates	O
of	O
standard	B
error	I
for	O
the	O
gfr	O
density	B
poisson	B
regression	B
models	B
df	O
d	O
as	O
in	O
figure	O
each	O
b	O
d	O
bootstrap	B
replications	O
nonparametric	B
standard	O
errors	B
based	O
on	O
binomial	B
bin	O
counts	O
degrees	B
of	I
freedom	I
gfr	O
nonparametric	B
standard	B
error	I
degrees	B
of	I
freedom	I
evaluated	O
at	O
nine	O
values	O
of	O
gfr	O
variability	O
generally	O
increases	O
with	O
increasing	O
df	O
as	O
expected	O
choosing	O
a	O
best	O
model	O
is	O
a	O
compromise	O
between	O
standard	B
error	I
and	O
possible	O
definitional	O
bias	B
as	O
suggested	O
by	O
figure	O
with	O
perhaps	O
df	O
d	O
or	O
the	O
winner	O
if	O
we	O
kept	O
increasing	O
the	O
degrees	B
of	I
freedom	I
eventually	O
df	O
d	O
we	O
would	O
exactly	O
match	O
the	O
bar	O
heights	O
yk	O
in	O
the	O
histogram	O
at	O
this	O
point	O
the	O
parametric	B
bootstrap	B
would	O
merge	O
into	O
the	O
nonparametric	B
bootstrap	B
nonparametric	B
is	O
another	O
name	O
for	O
very	O
highly	O
parameterized	O
the	O
huge	O
sample	B
sizes	O
associated	O
with	O
modern	O
applications	O
have	O
encouraged	O
nonparametric	B
methods	O
on	O
the	O
sometimes	O
mistaken	O
ground	O
that	O
estimation	B
efficiency	O
is	O
no	O
longer	O
of	O
concern	O
it	O
is	O
costly	O
here	O
as	O
the	O
nonparametric	B
column	O
of	O
table	O
figure	O
returns	O
to	O
the	O
student	B
score	I
eigenratio	B
calculations	O
of	O
figure	O
the	O
solid	O
histogram	O
shows	O
parametric	B
bootstrap	B
replications	O
with	O
f	O
the	O
five-dimensional	O
bivariate	B
normal	B
distribution	B
o	O
here	O
nx	O
and	O
o	O
are	O
the	O
usual	O
mle	B
estimates	O
for	O
the	O
expectation	O
togram	O
with	O
bseboot	O
d	O
compared	O
with	O
the	O
nonparametric	B
estimate	B
vector	B
and	O
covariance	O
matrix	B
based	O
on	O
the	O
five-component	O
student	B
score	I
vectors	O
it	O
is	O
narrower	O
than	O
the	O
corresponding	O
nonparametric	B
bootstrap	B
his	O
these	O
are	O
the	O
binomial	B
standard	O
errors	B
yk	O
n	O
ykn	O
n	O
d	O
the	O
nonparametric	B
results	O
look	O
much	O
more	O
competitive	O
when	O
estimating	O
cdf	B
s	O
rather	O
than	O
densities	O
the	O
parametric	B
bootstrap	B
figure	O
eigenratio	B
example	O
student	B
score	I
data	B
solid	O
histogram	O
b	O
d	O
parametric	B
bootstrap	B
replications	O
o	O
the	O
five-dimensional	O
normal	B
mle	B
line	O
histogram	O
the	O
nonparametric	B
replications	O
of	O
figure	O
mle	B
o	O
d	O
is	O
vertical	O
red	O
line	O
from	O
the	O
different	O
histogram	O
bin	O
limits	O
from	O
figure	O
changing	O
the	O
details	O
of	O
the	O
nonparametric	B
histogram	O
parametric	B
families	O
act	O
as	O
regularizers	O
smoothing	B
out	O
the	O
raw	O
data	B
and	O
de-emphasizing	O
outliers	O
in	O
fact	O
the	O
student	B
score	I
data	B
is	O
not	O
a	O
good	O
candidate	O
for	O
normal	B
modeling	O
having	O
at	O
least	O
one	O
notable	O
casting	O
doubt	O
on	O
the	O
smaller	O
estimate	B
of	I
standard	B
error	I
the	O
classical	O
statistician	O
could	O
only	O
imagine	O
a	O
mathematical	O
device	O
that	O
given	O
any	O
statistic	B
o	O
d	O
s	O
x	O
would	O
produce	O
a	O
formula	B
for	O
its	O
standard	B
error	I
as	O
formula	B
does	O
for	O
nx	O
the	O
electronic	O
computer	O
is	O
such	O
a	O
device	O
as	O
harnessed	O
by	O
the	O
bootstrap	B
it	O
automatically	O
produces	O
a	O
numerical	O
estimate	B
of	I
standard	B
error	I
not	O
a	O
formula	B
with	O
no	O
further	O
cleverness	O
required	O
chapter	O
discusses	O
a	O
more	O
ambitious	O
substitution	O
of	O
computer	O
power	O
for	O
mathematical	O
analysis	B
the	O
bootstrap	B
computation	O
of	O
confidence	O
intervals	B
as	O
revealed	O
by	O
examining	O
scatterplots	O
of	O
the	O
five	O
variates	O
taken	O
two	O
at	O
a	O
time	O
fast	O
and	O
painless	O
plotting	O
is	O
another	O
advantage	O
for	O
twenty-first-century	O
data	B
analysts	O
standard	O
errorsnonparametric	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
influence	O
functions	O
and	O
robust	B
estimation	B
the	O
sample	B
mean	O
played	O
a	O
dominant	O
role	O
in	O
classical	O
statistics	B
for	O
reasons	O
heavily	O
weighted	O
toward	O
mathematical	O
tractibility	O
beginning	O
in	O
the	O
an	O
important	O
counter-movement	O
robust	B
estimation	B
aimed	O
to	O
improve	O
upon	O
the	O
statistical	O
properties	O
of	O
the	O
mean	O
a	O
central	O
element	O
of	O
that	O
theory	B
the	O
influence	O
function	B
is	O
closely	O
related	O
to	O
the	O
jackknife	B
and	O
infinitesimal	O
jackknife	B
estimates	O
of	O
standard	B
error	I
we	O
will	O
only	O
consider	O
the	O
case	O
where	O
x	O
the	O
sample	B
space	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
the	O
unknown	O
probability	O
distribution	B
f	O
yielding	O
the	O
iid	O
sample	B
x	O
d	O
xn	O
in	O
is	O
now	O
the	O
cdf	B
of	O
a	O
density	B
function	B
f	O
on	O
x	O
a	O
parameter	O
of	O
interest	O
i	O
e	O
a	O
function	B
of	O
f	O
is	O
to	O
be	O
estimated	O
by	O
the	O
plug-in	O
principle	O
o	O
of	O
is	O
the	O
empirical	B
probability	I
distribution	B
putting	O
probability	O
on	O
each	O
sample	B
point	O
xi	O
for	O
the	O
mean	O
d	O
t	B
of	O
where	O
as	O
in	O
section	O
d	O
t	B
dz	O
of	O
d	O
nx	O
dr	O
xd	O
of	O
riemann	O
stieltjes	O
notation	O
dr	O
xdf	O
and	O
o	O
xf	O
dx	O
and	O
o	O
d	O
t	B
xi	O
x	O
n	O
the	O
influence	O
function	B
of	O
t	B
evaluated	O
at	O
point	O
x	O
in	O
x	O
is	O
defined	O
to	O
be	O
if	O
x	O
d	O
lim	O
t	B
c	O
x	O
t	B
where	O
x	O
is	O
the	O
one-point	O
probability	O
distribution	B
putting	O
probability	O
on	O
x	O
in	O
words	O
if	O
x	O
measures	O
the	O
differential	O
effect	O
of	O
modifying	O
f	O
by	O
putting	O
additional	O
probability	O
on	O
x	O
for	O
the	O
mean	O
dr	O
xf	O
we	O
cal	O
culate	O
that	O
if	O
x	O
d	O
x	O
a	O
fundamental	O
theorem	B
says	O
that	O
o	O
d	O
t	B
of	O
is	O
approximately	O
nx	O
c	O
if	O
xi	O
o	O
n	O
with	O
the	O
approximation	O
becoming	O
exact	O
as	O
n	O
goes	O
to	O
infinity	O
this	O
implies	O
that	O
o	O
is	O
approximately	O
the	O
mean	O
of	O
the	O
n	O
iid	O
variates	O
if	O
xi	O
and	O
that	O
the	O
variance	B
of	O
o	O
is	O
approximately	O
varfif	O
xg	O
n	O
o	O
o	O
n	O
var	O
influence	O
functions	O
and	O
robust	B
estimation	B
varfif	O
xg	O
being	O
the	O
variance	B
of	O
if	O
x	O
for	O
any	O
one	O
draw	O
of	O
x	O
from	O
f	O
for	O
the	O
sample	B
mean	O
using	O
in	O
gives	O
the	O
familiar	O
equality	O
varfnxg	O
d	O
n	O
varfxg	O
the	O
sample	B
mean	O
suffers	O
from	O
an	O
unbounded	O
influence	O
function	B
which	O
grows	O
ever	O
larger	O
as	O
x	O
moves	O
farther	O
from	O
this	O
makes	O
nx	O
unstable	O
against	O
heavy-tailed	O
densities	O
such	O
as	O
the	O
cauchy	O
robust	B
estimation	B
theory	B
seeks	O
estimators	O
o	O
of	O
bounded	O
influence	O
that	O
do	O
well	O
against	O
heavytailed	O
densities	O
without	O
giving	O
up	O
too	O
much	O
efficiency	O
against	O
light-tailed	O
densities	O
such	O
as	O
the	O
normal	B
of	O
particular	O
interest	O
have	O
been	O
the	O
trimmed	B
mean	I
and	O
its	O
close	O
cousin	O
the	O
winsorized	B
mean	I
d	O
or	O
equivalently	O
let	O
x	O
denote	O
the	O
th	O
percentile	O
of	O
distribution	B
f	O
satisfying	O
f	O
dz	O
x	O
f	O
dx	O
z	O
the	O
th	O
trimmed	B
mean	I
of	O
f	O
is	O
defined	O
as	O
d	O
the	O
mean	O
of	O
the	O
central	O
portion	O
of	O
f	O
trimming	O
off	O
the	O
lower	O
and	O
upper	O
portions	O
this	O
is	O
not	O
the	O
same	O
as	O
the	O
th	O
winsorized	B
mean	I
xf	O
dx	O
x	O
where	O
dz	O
x	O
w	O
d	O
x	O
w	O
dx	O
if	O
x	O
x	O
if	O
x	O
x	O
if	O
x	O
removes	O
the	O
outer	O
portions	O
of	O
f	O
while	O
moves	O
them	O
into	O
x	O
or	O
in	O
practice	O
empirical	B
versions	O
o	O
are	O
used	O
substituting	O
the	O
empirical	B
density	B
o	O
f	O
with	O
probability	O
at	O
each	O
xi	O
for	O
f	O
and	O
o	O
there	O
turns	O
out	O
to	O
be	O
an	O
interesting	O
relationship	O
between	O
the	O
two	O
the	O
influence	O
function	B
of	O
is	O
a	O
function	B
of	O
if	O
d	O
w	O
this	O
is	O
pictured	O
in	O
figure	O
where	O
we	O
have	O
plotted	O
empirical	B
influence	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
figure	O
empirical	B
influence	O
functions	O
for	O
the	O
leukemia	B
all	O
scores	O
of	O
figure	O
the	O
two	O
dashed	O
curves	O
are	O
if	O
for	O
the	O
trimmed	O
means	O
for	O
d	O
and	O
d	O
the	O
solid	O
curve	O
is	O
if	O
x	O
for	O
the	O
sample	B
mean	O
nx	O
functions	O
in	O
of	O
for	O
f	O
in	O
definition	O
relating	O
to	O
the	O
leukemia	B
all	O
scores	O
of	O
figure	O
and	O
are	O
plotted	O
along	O
with	O
that	O
is	O
for	O
the	O
mean	O
table	O
trimmed	O
means	O
and	O
their	O
bootstrap	B
standard	B
deviations	I
for	O
the	O
leukemia	B
all	O
scores	O
of	O
figure	O
b	O
d	O
bootstrap	B
replications	O
for	O
each	O
trim	O
value	O
the	O
last	O
column	O
gives	O
empirical	B
influence	O
function	B
estimates	O
of	O
the	O
standard	B
error	I
which	O
are	O
also	O
the	O
infinitesimal	O
jackknife	B
estimates	O
these	O
fail	O
for	O
the	O
median	O
trim	O
mean	O
median	O
trimmed	O
bootstrap	B
mean	O
sd	O
functionmeantrimmed	O
mean	O
a	O
mean	O
a	O
notes	O
and	O
details	O
the	O
upper	O
panel	O
of	O
figure	O
shows	O
a	O
moderately	O
heavy	O
right	O
tail	O
for	O
the	O
all	O
distribution	B
would	O
it	O
be	O
more	O
efficient	O
to	O
estimate	B
the	O
center	O
of	O
vides	O
an	O
answer	O
bseboot	O
was	O
calculated	O
for	O
nx	O
and	O
o	O
the	O
distribution	B
with	O
a	O
trimmed	B
mean	I
rather	O
than	O
nx	O
the	O
bootstrap	B
protrim	O
d	O
and	O
the	O
last	O
being	O
the	O
sample	B
median	O
it	O
appears	O
that	O
o	O
is	O
moderately	O
better	O
than	O
nx	O
this	O
brings	O
up	O
an	O
important	O
question	O
discussed	O
in	O
chapter	O
if	O
we	O
use	O
something	O
like	O
table	O
to	O
select	O
an	O
estimator	B
how	O
does	O
the	O
selection	O
process	O
affect	O
the	O
accuracy	B
of	O
the	O
resulting	O
estimate	B
we	O
might	O
also	O
use	O
the	O
square	O
root	O
of	O
formula	B
to	O
estimate	B
the	O
standard	O
errors	B
of	O
the	O
various	O
estimators	O
plugging	O
in	O
the	O
empirical	B
influence	O
function	B
for	O
if	O
x	O
this	O
turns	O
out	O
to	O
be	O
the	O
same	O
as	O
using	O
the	O
infinitesimal	O
jackknife	B
these	O
appear	O
in	O
the	O
last	O
column	O
of	O
table	O
predictably	O
this	O
approach	O
fails	O
for	O
the	O
sample	B
median	O
whose	O
influence	O
function	B
is	O
a	O
square	O
wave	O
sharply	O
discontinuous	O
at	O
the	O
median	O
if	O
x	O
d	O
robust	B
estimation	B
offers	O
a	O
nice	O
illustration	O
of	O
statistical	O
progress	O
in	O
the	O
computer	O
age	O
trimmed	O
means	O
go	O
far	O
back	O
into	O
the	O
classical	O
era	O
influence	O
functions	O
are	O
an	O
insightful	O
inferential	O
tool	O
for	O
understanding	O
the	O
tradeoffs	O
in	O
trimmed	B
mean	I
estimation	B
and	O
finally	O
the	O
bootstrap	B
allows	O
easy	O
assessment	O
of	O
the	O
accuracy	B
of	O
robust	B
estimation	B
including	O
some	O
more	O
elaborate	O
ones	O
not	O
discussed	O
here	O
notes	O
and	O
details	O
quenouille	O
introduced	O
what	O
is	O
now	O
called	O
the	O
jackknife	B
estimate	B
of	O
bias	B
tukey	B
realized	O
that	O
quenouille-type	O
calculations	O
could	O
be	O
repurposed	O
for	O
nonparametric	B
standard-error	O
estimation	B
inventing	O
formula	B
and	O
naming	O
it	O
the	O
jackknife	B
as	O
a	O
rough	O
and	O
ready	O
tool	O
miller	O
s	O
important	O
paper	O
a	O
trustworthy	O
jackknife	B
asked	O
when	O
formula	B
could	O
be	O
trusted	O
for	O
the	O
median	O
the	O
bootstrap	B
began	O
as	O
an	O
attempt	O
to	O
better	O
understand	O
the	O
jackknife	B
s	O
successes	O
and	O
failures	O
its	O
name	O
celebrates	O
baron	B
munchausen	I
s	O
success	O
in	O
pulling	O
himself	O
up	O
by	O
his	O
own	O
bootstraps	O
from	O
the	O
bottom	O
of	O
a	O
lake	O
burgeoning	O
computer	O
power	O
soon	O
overcame	O
the	O
bootstrap	B
s	O
main	O
drawback	O
prodigous	O
amounts	O
of	O
calculation	O
propelling	O
it	O
into	O
general	O
use	O
meanwhile	O
theoretical	O
papers	O
were	O
published	O
asking	O
when	O
the	O
bootstrap	B
itself	O
could	O
be	O
trusted	O
but	O
not	O
all	O
of	O
the	O
time	O
in	O
common	O
practice	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
a	O
main	O
reference	O
for	O
the	O
chapter	O
is	O
efron	O
s	O
monograph	O
the	O
jackknife	B
the	O
bootstrap	B
and	O
other	O
resampling	B
plans	B
its	O
chapter	O
shows	O
the	O
equality	O
of	O
three	O
nonparametric	B
standard	B
error	I
estimates	O
jaeckel	O
s	O
infinitesimal	O
jackknife	B
the	O
empirical	B
influence	O
function	B
estimate	B
based	O
on	O
and	O
what	O
is	O
known	O
as	O
the	O
nonparametric	B
delta	B
method	B
bootstrap	B
packages	B
various	O
bootstrap	B
packages	B
in	O
r	B
are	O
available	O
on	O
the	O
cran	O
contributedpackages	O
web	O
site	O
bootstrap	B
being	O
an	O
ambitious	O
one	O
algorithm	B
shows	O
a	O
simple	O
r	B
program	O
for	O
nonparametric	B
bootstrapping	O
aside	O
from	O
bookkeeping	O
it	O
s	O
only	O
a	O
few	O
lines	O
long	O
algorithm	B
an	O
r	B
program	O
for	O
the	O
nonparametric	B
bootstrap	B
boot	O
function	B
b	O
func	O
x	O
is	O
data	B
vector	B
or	O
matrix	B
each	O
row	O
a	O
case	O
b	O
is	O
number	O
of	O
bootstrap	B
replications	O
func	O
is	O
r	B
function	B
that	O
inputs	O
a	O
data	B
vector	B
or	O
matrix	B
and	O
returns	O
a	O
numeric	O
number	O
or	O
vector	B
other	O
arguments	O
for	O
func	O
x	O
as	O
matrixx	O
n	O
nrowx	O
get	O
size	O
of	O
output	O
fmat	O
for	O
in	O
n	O
replace	O
true	O
fmatb	O
funcxi	O
dropfmat	O
the	O
jackknife	B
standard	B
error	I
the	O
monograph	O
also	O
contains	O
efron	O
and	O
stein	O
s	O
result	O
on	O
the	O
bias	B
of	O
the	O
jackknife	B
variance	B
estimate	B
the	O
square	O
of	O
formula	B
modulo	O
certain	O
sample	B
size	I
considerations	O
the	O
expectation	O
of	O
the	O
jackknife	B
variance	B
estimate	B
is	O
biased	O
upward	O
for	O
the	O
true	O
variance	B
for	O
the	O
sample	B
mean	O
nx	O
the	O
jackknife	B
yields	O
exactly	O
the	O
usual	O
variance	B
i	O
while	O
the	O
ideal	O
bootstrap	B
estimate	B
estimate	B
gives	O
nx	O
notes	O
and	O
details	O
as	O
with	O
the	O
jackknife	B
we	O
could	O
append	O
a	O
fudge	O
factor	B
to	O
get	O
perfect	O
agreement	O
with	O
but	O
there	O
is	O
no	O
real	O
gain	O
in	O
doing	O
so	O
bootstrap	B
sample	B
sizes	O
letbseb	O
indicate	O
the	O
bootstrap	B
standard	B
error	I
estimate	B
based	O
on	O
b	O
replications	O
the	O
ideal	O
bootstrap	B
creasing	O
b	O
past	O
a	O
certain	O
point	O
is	O
itself	O
a	O
statistic	B
whose	O
value	O
b	O
in	O
any	O
actual	O
application	O
there	O
are	O
diminishing	O
returns	O
from	O
invaries	O
with	O
the	O
observed	O
sample	B
x	O
in	O
leaving	O
an	O
irreducible	O
remainder	O
of	O
randomness	O
in	O
any	O
standard	B
error	I
estimate	B
section	O
of	O
efron	O
and	O
tibshirani	O
shows	O
that	O
b	O
d	O
will	O
almost	O
always	O
be	O
plenty	O
standard	O
errors	B
but	O
not	O
for	O
bootstrap	B
confidence	O
intervals	B
chapter	O
smaller	O
numbers	O
or	O
even	O
less	O
can	O
still	O
be	O
quite	O
useful	O
in	O
complicated	O
situations	O
where	O
resampling	B
is	O
expensive	O
an	O
early	O
complaint	O
bootstrap	B
estimates	O
are	O
random	O
is	O
less	O
often	O
heard	O
in	O
an	O
era	O
of	O
frequent	O
and	O
massive	O
simulations	O
the	O
bayesian	B
bootstrap	B
rubin	O
suggested	O
the	O
bayesian	B
bootstrap	B
section	O
of	O
efron	O
used	O
as	O
an	O
objective	B
bayes	I
justification	O
for	O
what	O
we	O
will	O
call	O
the	O
percentile-method	O
bootstrap	B
confidence	O
intervals	B
in	O
chapter	O
jackknife-after-bootstrap	O
for	O
the	O
eigenratio	B
example	O
displayed	O
in	O
just	O
the	O
original	O
replications	O
how	O
accurate	O
is	O
this	O
value	O
bootstrapping	O
the	O
bootstrap	B
seems	O
like	O
too	O
much	O
work	O
perhaps	O
times	O
resamples	O
it	O
turns	O
out	O
though	O
figure	O
b	O
d	O
nonparametric	B
bootstrap	B
replications	O
gavebseboot	O
d	O
that	O
we	O
can	O
use	O
the	O
jackknife	B
to	O
estimate	B
the	O
variability	O
ofbseboot	O
based	O
on	O
now	O
the	O
deleted	O
sample	B
estimate	B
in	O
isbseboot	O
i	O
the	O
key	O
idea	O
is	O
applying	O
definition	O
to	O
this	O
subset	O
givesbseboot	O
i	O
for	O
the	O
estimate	B
of	O
figure	O
the	O
jackknife-after-bootstrap	O
calculations	O
gavebsejack	O
d	O
for	O
bseboot	O
d	O
in	O
other	O
words	O
isn	O
t	B
very	O
accurate	O
which	O
is	O
to	O
be	O
expected	O
for	O
the	O
standard	B
error	I
of	O
a	O
complicated	O
statistic	B
estimated	O
from	O
only	O
n	O
d	O
observations	O
an	O
infinitesimal	O
jackknife	B
version	O
of	O
this	O
technique	O
will	O
play	O
a	O
major	O
role	O
in	O
chapter	O
among	O
the	O
original	O
to	O
consider	O
those	O
bootstrap	B
samples	O
x	O
that	O
do	O
not	O
include	O
the	O
point	O
xi	O
about	O
of	O
the	O
original	O
b	O
samples	O
will	O
be	O
in	O
this	O
subset	O
section	O
of	O
efron	O
and	O
tibshirani	O
shows	O
that	O
a	O
fundamental	O
theorem	B
tukey	B
can	O
justly	O
be	O
considered	O
the	O
founding	O
father	O
of	O
robust	O
statistics	B
his	O
paper	O
being	O
especially	O
influential	O
huber	O
s	O
celebrated	O
paper	O
brought	O
the	O
subject	O
into	O
the	O
realm	O
of	O
highconcept	O
mathematical	O
statistics	B
robust	O
statistics	B
the	O
approach	O
based	O
on	O
influence	O
functions	O
the	O
book	O
by	O
hampel	O
et	O
al	O
conveys	O
the	O
breadth	O
of	O
a	O
subject	O
only	O
lightly	O
scratched	O
in	O
our	O
section	O
hampel	O
the	O
jackknife	B
and	O
the	O
bootstrap	B
introduced	O
the	O
influence	O
function	B
as	O
a	O
statistical	O
tool	O
boos	O
and	O
serfling	O
verified	O
expression	O
qualitative	O
notions	O
of	O
robustness	O
more	O
than	O
specific	O
theoretical	O
results	O
have	O
had	O
a	O
continuing	O
influence	O
on	O
modern	O
data	B
analysis	B
bootstrap	B
confidence	O
intervals	B
the	O
jackknife	B
and	O
the	O
bootstrap	B
represent	O
a	O
different	O
use	O
of	O
modern	O
computer	O
power	O
rather	O
than	O
extending	O
classical	O
methodology	O
from	O
ordinary	O
least	B
squares	I
to	O
generalized	O
linear	B
models	B
for	O
example	O
they	O
extend	O
the	O
reach	O
of	O
classical	O
inference	B
chapter	O
focused	O
on	O
standard	O
errors	B
here	O
we	O
will	O
take	O
up	O
a	O
more	O
ambitious	O
inferential	O
goal	O
the	O
bootstrap	B
automation	O
of	O
confidence	O
intervals	B
the	O
familiar	O
standard	O
intervals	B
o	O
d	O
from	O
a	O
poisson	B
model	O
o	O
for	O
approximate	O
coverage	B
are	O
immensely	O
useful	O
in	O
practice	O
but	O
often	O
the	O
standard	B
interval	B
d	O
o	O
not	O
very	O
accurate	O
if	O
we	O
observe	O
o	O
poi	O
is	O
a	O
mediocre	O
ap	O
proximation	O
to	O
the	O
exact	O
standard	O
intervals	B
are	O
symmetric	O
around	O
o	O
this	O
being	O
their	O
main	O
weakness	O
poisson	B
distributions	O
grow	O
more	O
variable	O
as	O
increases	O
which	O
is	O
why	O
interval	B
extends	O
farther	O
to	O
the	O
right	O
of	O
o	O
d	O
than	O
to	O
the	O
left	O
correctly	O
capturing	O
such	O
effects	O
in	O
an	O
automatic	O
way	O
is	O
the	O
goal	O
of	O
bootstrap	B
confidence	O
interval	B
theory	B
neyman	O
s	O
construction	O
for	O
one-parameter	B
the	O
student	B
score	I
data	B
of	O
table	O
comprised	O
n	O
d	O
pairs	O
problems	O
xi	O
d	O
vi	O
i	O
d	O
using	O
the	O
neyman	O
construction	O
of	O
section	O
as	O
explained	O
there	O
see	O
also	O
table	O
in	O
section	O
bootstrap	B
confidence	O
intervals	B
where	O
mi	O
and	O
vi	O
were	O
student	O
i	O
s	O
scores	O
on	O
the	O
mechanics	O
and	O
vectors	O
tests	O
the	O
sample	B
correlation	O
coefficient	O
o	O
between	O
mi	O
and	O
vi	O
was	O
computed	O
to	O
be	O
o	O
d	O
question	O
what	O
can	O
we	O
infer	O
about	O
the	O
true	O
correlation	O
between	O
m	O
and	O
v	O
figure	O
displayed	O
three	O
possible	O
bayesian	B
answers	O
confidence	O
intervals	B
provide	O
the	O
frequentist	B
solution	O
by	O
far	O
the	O
most	O
popular	O
in	O
applied	O
practice	O
figure	O
the	O
solid	O
curve	O
is	O
the	O
normal	B
correlation	O
coefficient	O
density	B
fo	O
for	O
o	O
student	B
score	I
data	B
o	O
endpoints	O
of	O
the	O
confidence	O
interval	B
for	O
with	O
corresponding	O
densities	O
shown	O
by	O
dashed	O
curves	O
these	O
yield	O
tail	O
areas	O
at	O
o	O
d	O
the	O
mle	B
estimate	B
for	O
the	O
d	O
are	O
the	O
d	O
and	O
o	O
suppose	O
first	O
that	O
we	O
assume	O
a	O
bivariate	B
normal	B
model	O
for	O
the	O
o	O
for	O
sample	B
correpairs	O
vi	O
in	O
that	O
case	O
the	O
probability	O
density	B
f	O
lation	O
o	O
given	O
true	O
correlation	O
has	O
known	O
form	B
the	O
solid	O
curve	O
in	O
figure	O
graphs	O
f	O
for	O
d	O
that	O
is	O
for	O
set	B
equal	O
to	O
the	O
observed	O
value	O
o	O
in	O
more	O
careful	O
notation	O
the	O
curve	O
graphs	O
fo	O
as	O
a	O
function	B
of	O
the	O
dummy	O
r	B
taking	O
values	O
in	O
this	O
is	O
an	O
example	O
of	O
a	O
parametric	B
bootstrap	B
distribution	B
here	O
with	O
being	O
o	O
neyman	O
s	O
construction	O
two	O
other	O
curves	O
f	O
appear	O
in	O
figure	O
for	O
equaling	O
these	O
were	O
numerically	O
calculated	O
as	O
the	O
solutions	O
to	O
o	O
o	O
d	O
and	O
d	O
z	O
o	O
z	O
o	O
in	O
words	O
o	O
above	O
o	O
below	O
o	O
fo	O
dr	O
d	O
and	O
fo	O
dr	O
d	O
d	O
while	O
o	O
is	O
the	O
smallest	O
value	O
of	O
putting	O
probability	O
at	O
least	O
is	O
the	O
largest	O
value	O
with	O
probability	O
at	O
least	O
o	O
o	O
i	O
is	O
a	O
confidence	O
interval	B
for	O
the	O
true	O
correlation	O
statement	O
holding	O
true	O
with	O
probability	O
for	O
every	O
possible	O
value	O
of	O
we	O
have	O
just	O
described	O
neyman	O
s	O
construction	O
of	O
confidence	O
intervals	B
o	O
for	O
one-parameter	B
problems	O
f	O
we	O
will	O
consider	O
the	O
more	O
difficult	O
situation	O
where	O
there	O
are	O
nuisance	B
parameters	I
in	O
addition	O
to	O
the	O
parameter	O
of	O
interest	O
one	O
of	O
the	O
jewels	O
of	O
classical	O
frequentist	B
inference	B
it	O
depends	O
on	O
a	O
pivotal	O
argument	B
ingenious	O
device	O
number	O
of	O
section	O
to	O
show	O
that	O
it	O
produces	O
genuine	O
confidence	O
intervals	B
i	O
e	O
ones	O
that	O
contain	O
the	O
true	O
parameter	O
value	O
at	O
the	O
claimed	O
probability	O
level	O
in	O
figure	O
the	O
argument	B
appears	O
in	O
the	O
chapter	O
endnotes	O
for	O
the	O
poisson	B
calculation	O
it	O
was	O
necessary	O
to	O
define	O
exactly	O
what	O
the	O
smallest	O
value	O
of	O
putting	O
probability	O
at	O
least	O
above	O
o	O
meant	O
o	O
at	O
this	O
was	O
done	O
assuming	O
that	O
for	O
any	O
half	O
of	O
the	O
probability	O
f	O
o	O
d	O
counted	O
as	O
above	O
and	O
similarly	O
for	O
calculating	O
the	O
upper	O
limit	O
transformation	B
invariance	I
confidence	O
intervals	B
enjoy	O
the	O
important	O
and	O
useful	O
property	O
of	O
transformation	B
invariance	I
in	O
the	O
poisson	B
example	O
suppose	O
our	O
interest	O
shifts	O
from	O
parameter	O
to	O
parameter	O
d	O
log	O
the	O
exact	O
interval	B
for	O
then	O
transforms	O
to	O
the	O
exact	O
interval	B
for	O
simply	O
by	O
taking	O
logs	O
of	O
the	O
endpoints	O
d	O
to	O
state	O
things	O
generally	O
suppose	O
we	O
observe	O
o	O
from	O
a	O
family	O
of	O
densio	O
o	O
for	O
of	O
coverage	B
level	I
and	O
construct	O
a	O
confidence	O
interval	B
c	O
ties	O
f	O
bootstrap	B
confidence	O
intervals	B
d	O
in	O
our	O
examples	O
now	O
let	O
parameter	O
be	O
a	O
monotonic	B
increasing	I
function	B
of	O
say	O
d	O
log	O
in	O
and	O
likewise	O
d	O
m	O
then	O
c	O
for	O
o	O
maps	O
point	O
by	O
point	O
into	O
a	O
level-	O
confidence	O
interval	B
o	O
for	O
the	O
point	O
estimate	B
d	O
m	O
d	O
m	O
for	O
o	O
this	O
just	O
says	O
that	O
the	O
event	O
f	O
is	O
the	O
same	O
as	O
the	O
event	O
so	O
if	O
the	O
former	O
always	O
occurs	O
with	O
probability	O
then	O
so	O
must	O
the	O
c	O
c	O
latter	O
dn	O
o	O
figure	O
the	O
situation	O
in	O
figure	O
after	O
transformation	O
to	O
d	O
m	O
according	O
to	O
the	O
curves	O
are	O
nearly	O
with	O
standard	B
deviation	I
d	O
p	O
d	O
transformation	B
invariance	I
has	O
an	O
historical	O
resonance	O
with	O
the	O
normal	B
o	O
correlation	O
coefficient	O
fisher	B
s	O
derivation	O
of	O
f	O
in	O
was	O
a	O
mathematical	O
triumph	O
but	O
a	O
difficult	O
one	O
to	O
exploit	O
in	O
an	O
era	O
of	O
mechanical	O
computation	O
most	O
ingeniously	O
fisher	B
suggested	O
instead	O
working	O
with	O
the	O
transformed	O
parameter	O
d	O
m	O
where	O
c	O
d	O
m	O
d	O
log	O
s	O
f	O
and	O
likewise	O
with	O
statistic	B
d	O
m	O
imation	O
the	O
percentile	B
method	B
o	O
then	O
to	O
a	O
surprisingly	O
good	O
approx	O
see	O
figure	O
which	O
shows	O
neyman	O
s	O
construction	O
on	O
the	O
scale	B
in	O
other	O
words	O
we	O
are	O
back	O
in	O
fisher	B
s	O
favored	O
situation	O
the	O
sim	O
ple	O
normal	B
translation	O
problem	O
where	O
n	O
n	O
d	O
n	O
is	O
the	O
obviously	O
correct	O
confidence	O
for	O
closely	O
approximating	O
neyman	O
s	O
construction	O
the	O
endpoints	O
of	O
are	O
then	O
transformed	O
back	O
to	O
the	O
scale	B
according	O
to	O
the	O
inverse	O
transformation	O
d	O
c	O
o	O
seen	O
in	O
figure	O
but	O
without	O
the	O
in	O
giving	O
the	O
interval	B
c	O
volved	O
computations	B
bayesian	B
confidence	O
statements	O
are	O
inherently	O
transformation	O
invariant	O
the	O
fact	O
that	O
the	O
neyman	O
intervals	B
are	O
also	O
invariant	O
unlike	O
the	O
standard	O
intervals	B
has	O
made	O
them	O
more	O
palatable	O
to	O
bayesian	B
statisticians	O
transformation	B
invariance	I
will	O
play	O
a	O
major	O
role	O
in	O
justifying	O
the	O
bootstrap	B
confidence	O
intervals	B
introduced	O
next	O
the	O
percentile	B
method	B
our	O
goal	O
is	O
to	O
automate	O
the	O
calculation	O
of	O
confidence	O
intervals	B
given	O
the	O
bootstrap	B
distribution	B
of	O
a	O
statistical	O
estimator	B
o	O
we	O
want	O
to	O
automatically	O
produce	O
an	O
appropriate	O
confidence	O
interval	B
for	O
the	O
unseen	O
parameter	O
to	O
this	O
end	O
a	O
series	O
of	O
four	O
increasingly	O
accurate	O
bootstrap	B
confidence	O
interval	B
algorithms	O
will	O
be	O
described	O
for	O
coverage	B
withbse	O
taken	O
to	O
be	O
the	O
bootstrap	B
standard	B
error	I
bseboot	O
the	O
limitations	O
of	O
this	O
approach	O
become	O
obvious	O
in	O
o	O
figure	O
where	O
the	O
histogram	O
shows	O
b	O
d	O
nonparametric	B
bootstrap	B
replications	O
o	O
of	O
the	O
sample	B
correlation	O
coefficient	O
for	O
the	O
student	O
the	O
first	O
and	O
simplest	O
method	B
is	O
to	O
use	O
the	O
standard	B
interval	B
this	O
is	O
an	O
anachronism	O
fisher	B
hated	O
the	O
term	O
confidence	O
interval	B
after	O
it	O
was	O
later	O
coined	O
by	O
neyman	O
for	O
his	O
comprehensive	O
theory	B
he	O
thought	O
of	O
as	O
an	O
example	O
of	O
the	O
logic	B
of	I
inductive	B
inference	B
bootstrap	B
confidence	O
intervals	B
figure	O
histogram	O
of	O
b	O
d	O
nonparametric	B
bootstrap	B
replications	O
o	O
for	O
the	O
student	B
score	I
sample	B
correlation	O
the	O
solid	O
curve	O
is	O
the	O
ideal	O
parametric	B
bootstrap	B
distribution	B
fo	O
as	O
in	O
figure	O
observed	O
correlation	O
o	O
d	O
small	O
triangles	O
show	O
histogram	O
s	O
and	O
quantiles	O
score	O
data	B
obtained	O
as	O
in	O
section	O
the	O
standard	O
intervals	B
are	O
justified	O
by	O
taking	O
literally	O
the	O
asymptotic	O
normality	O
of	O
o	O
o	O
n	O
the	O
true	O
standard	B
error	I
relation	O
will	O
generally	O
hold	O
for	O
large	O
enough	O
sample	B
size	I
n	O
but	O
we	O
can	O
see	O
that	O
for	O
the	O
student	B
score	I
data	B
asymptotic	O
normality	O
has	O
not	O
yet	O
set	B
in	O
with	O
the	O
histogram	O
being	O
notably	O
long-tailed	O
to	O
the	O
left	O
we	O
can	O
t	B
expect	O
good	O
performance	O
from	O
the	O
standard	O
method	B
in	O
this	O
case	O
parametric	B
bootstrap	B
distribution	B
is	O
just	O
as	O
nonnormal	O
as	O
shown	O
by	O
the	O
smooth	O
curve	O
o	O
the	O
percentile	B
method	B
uses	O
the	O
shape	O
of	O
the	O
bootstrap	B
distribution	B
to	O
improve	O
upon	O
the	O
standard	O
intervals	B
having	O
generated	O
b	O
bootstrap	B
replications	O
o	O
either	O
nonparametrically	O
as	O
in	O
section	O
or	O
parametrically	O
as	O
in	O
section	O
we	O
use	O
the	O
obvious	O
percentiles	O
of	O
their	O
distribution	B
to	O
define	O
the	O
percentile	O
confidence	O
limits	O
the	O
histogram	O
in	O
figure	O
has	O
its	O
and	O
percentiles	O
equal	O
to	O
and	O
o	O
bootstrap	B
correlationsfrequency	O
the	O
percentile	B
method	B
and	O
these	O
are	O
the	O
endpoints	O
of	O
the	O
central	O
nonparametric	B
percentile	B
interval	B
figure	O
a	O
central	O
confidence	O
interval	B
via	O
the	O
percentile	B
method	B
based	O
on	O
the	O
nonparametric	B
replications	O
o	O
figure	O
of	O
we	O
can	O
state	O
things	O
more	O
precisely	O
in	O
terms	O
of	O
the	O
bootstrap	B
cdf	B
the	O
proportion	B
of	O
bootstrap	B
samples	O
less	O
than	O
t	B
n	O
o	O
o	O
t	B
og	O
t	B
d	O
of	O
the	O
bootstrap	B
distribution	B
is	O
given	O
by	O
the	O
b	O
the	O
th	O
percentile	O
point	O
o	O
inverse	O
function	B
of	O
og	O
og	O
t	B
o	O
d	O
og	O
o	O
is	O
the	O
value	O
putting	O
proportion	B
of	O
the	O
bootstrap	B
sample	B
to	O
its	O
left	O
the	O
level-	O
upper	O
endpoint	O
of	O
the	O
percentile	B
interval	B
say	O
o	O
is	O
by	O
definition	O
in	O
this	O
notation	O
the	O
central	O
percentile	B
interval	B
is	O
d	O
og	O
d	O
o	O
o	O
o	O
o	O
bootstrap	B
confidence	O
intervals	B
the	O
construction	O
is	O
illustrated	O
in	O
figure	O
the	O
percentile	O
intervals	B
are	O
transformation	O
invariant	O
let	O
d	O
m	O
as	O
o	O
in	O
and	O
likewise	O
d	O
m	O
monotonically	O
increasing	O
with	O
o	O
bootstrap	B
replications	O
for	O
b	O
d	O
b	O
the	O
bootstrap	B
d	O
m	O
percentiles	O
transform	B
in	O
the	O
same	O
way	O
d	O
m	O
so	O
that	O
as	O
in	O
d	O
m	O
verifying	O
transformation	B
invariance	I
o	O
o	O
in	O
what	O
sense	O
does	O
the	O
percentile	B
method	B
improve	O
upon	O
the	O
standard	O
intervals	B
one	O
answer	O
involves	O
transformation	B
invariance	I
suppose	O
there	O
exists	O
a	O
monotone	O
transformation	O
d	O
m	O
and	O
d	O
m	O
o	O
such	O
that	O
n	O
for	O
every	O
with	O
constant	O
fisher	B
s	O
transformation	O
almost	O
accomplishes	O
this	O
for	O
the	O
normal	B
correlation	O
coefficient	O
it	O
would	O
then	O
be	O
true	O
that	O
parametric	B
bootstrap	B
replications	O
would	O
also	O
follow	O
that	O
is	O
the	O
bootstrap	B
cdf	B
would	O
be	O
normal	B
with	O
mean	O
and	O
variance	B
the	O
th	O
percentile	O
of	O
would	O
equal	O
n	O
d	O
d	O
c	O
z	O
where	O
z	O
denotes	O
the	O
th	O
percentile	O
of	O
a	O
standard	O
normal	B
distribution	B
z	O
d	O
d	O
d	O
etc	O
in	O
other	O
words	O
the	O
percentile	B
method	B
would	O
provide	O
fisher	B
s	O
obviously	O
correct	O
intervals	B
for	O
for	O
coverage	B
for	O
example	O
but	O
because	O
of	O
transformation	B
invariance	I
the	O
percentile	O
intervals	B
for	O
our	O
original	O
parameter	O
would	O
also	O
be	O
exactly	O
correct	O
some	O
comments	O
concerning	O
the	O
percentile	B
method	B
are	O
pertinent	O
o	O
the	O
percentile	B
method	B
o	O
it	O
only	O
assumes	O
its	O
existence	O
the	O
method	B
does	O
not	O
require	O
actually	O
knowing	O
the	O
transformation	O
to	O
normality	O
d	O
m	O
if	O
a	O
transformation	O
to	O
form	B
exists	O
then	O
the	O
percentile	O
intervals	B
are	O
not	O
only	O
accurate	O
but	O
also	O
correct	O
in	O
the	O
fisherian	O
sense	O
of	O
giving	O
the	O
logically	O
appropriate	O
inference	B
the	O
justifying	O
assumption	O
for	O
the	O
standard	O
intervals	B
o	O
n	O
becomes	O
more	O
accurate	O
as	O
the	O
sample	B
size	I
n	O
increases	O
with	O
decreasing	O
as	O
n	O
but	O
the	O
convergence	O
can	O
be	O
slow	O
in	O
cases	O
like	O
that	O
of	O
the	O
normal	B
correlation	O
coefficient	O
the	O
broader	O
assumption	O
that	O
m	O
up	O
convergence	O
irrespective	O
of	O
whether	O
or	O
not	O
it	O
holds	O
exactly	O
section	O
makes	O
this	O
point	O
explicit	O
in	O
terms	O
of	O
asymptotic	O
rates	O
of	O
convergence	O
the	O
standard	O
method	B
works	O
fine	O
once	O
it	O
is	O
applied	O
on	O
an	O
appropriate	O
scale	B
as	O
in	O
figure	O
the	O
trouble	O
is	O
that	O
the	O
method	B
is	O
not	O
transformation	O
invariant	O
leaving	O
the	O
statistician	O
the	O
job	O
of	O
finding	O
the	O
correct	O
scale	B
the	O
percentile	B
method	B
can	O
be	O
thought	O
of	O
as	O
a	O
transformation-invariant	O
version	O
of	O
the	O
standard	O
intervals	B
an	O
automatic	O
fisher	B
that	O
substitutes	O
massive	O
computations	B
for	O
mathematical	O
ingenuity	O
p	O
n	O
for	O
some	O
transformation	O
speeds	O
the	O
method	B
requires	O
bootstrap	B
sample	B
sizes	O
on	O
the	O
order	O
of	O
b	O
d	O
the	O
percentile	B
method	B
is	O
not	O
the	O
last	O
word	O
in	O
bootstrap	B
confidence	O
intervals	B
two	O
improvements	O
the	O
bc	O
and	O
bca	O
methods	O
will	O
be	O
discussed	O
in	O
the	O
next	O
section	O
table	O
compares	O
the	O
various	O
intervals	B
as	O
applied	O
to	O
the	O
student	B
score	I
correlation	O
o	O
d	O
table	O
bootstrap	B
confidence	O
limits	O
for	O
student	B
score	I
correlation	O
o	O
d	O
n	O
d	O
parametric	B
exact	O
limits	O
from	O
neyman	O
s	O
construction	O
as	O
in	O
figure	O
the	O
bc	O
and	O
bca	O
methods	O
are	O
discussed	O
in	O
the	O
next	O
two	O
sections	O
a	O
two	O
constants	O
required	O
for	O
bca	O
are	O
parametric	B
and	O
nonparametric	B
parametric	B
nonparametric	B
standard	O
percentile	O
bc	O
bca	O
exact	O
the	O
label	O
computer-intensive	B
inference	B
seems	O
especially	O
apt	O
as	O
ap	O
bootstrap	B
confidence	O
intervals	B
plied	O
to	O
bootstrap	B
confidence	O
intervals	B
neyman	O
and	O
fisher	B
s	O
constructions	B
are	O
expanded	O
from	O
a	O
few	O
special	O
theoretically	O
tractable	O
cases	O
to	O
almost	O
any	O
situation	O
where	O
the	O
statistician	O
has	O
a	O
repeatable	O
algorithm	B
automation	O
the	O
replacement	O
of	O
mathematical	O
formulas	O
with	O
wide-ranging	O
computer	O
algorithms	O
will	O
be	O
a	O
major	O
theme	O
of	O
succeeding	O
chapters	O
for	O
implies	O
o	O
d	O
o	O
d	O
o	O
z	O
o	O
fo	O
z	O
bias-corrected	B
confidence	O
intervals	B
the	O
ideal	O
form	B
for	O
the	O
percentile	B
method	B
o	O
that	O
the	O
transformation	O
d	O
m	O
yields	O
an	O
unbiased	O
estimator	B
of	O
constant	O
variance	B
the	O
improved	O
methods	O
of	O
this	O
section	O
and	O
the	O
next	O
take	O
into	O
account	O
the	O
possibility	O
of	O
bias	B
and	O
changing	O
variance	B
we	O
begin	O
with	O
bias	B
if	O
n	O
and	O
n	O
for	O
all	O
d	O
m	O
as	O
hypothesized	O
in	O
then	O
n	O
says	O
o	O
indicating	O
bootstrap	B
probability	O
in	O
which	O
case	O
the	O
monotonicity	O
of	O
gives	O
that	O
is	O
o	O
we	O
can	O
check	O
that	O
for	O
a	O
parametric	B
family	I
of	O
densities	O
f	O
for	O
the	O
normal	B
correlation	O
coefficient	O
density	B
n	O
d	O
numerical	O
integration	O
gives	O
o	O
is	O
median	O
for	O
o	O
and	O
likewise	O
o	O
o	O
o	O
which	O
is	O
not	O
far	O
removed	O
from	O
but	O
far	O
enough	O
to	O
have	O
a	O
small	O
impact	O
on	O
proper	B
inference	B
it	O
suggests	O
that	O
o	O
is	O
biased	O
upward	O
relative	O
to	O
o	O
that	O
s	O
why	O
less	O
than	O
half	O
of	O
the	O
bootstrap	B
probability	O
lies	O
below	O
o	O
and	O
by	O
implication	O
that	O
o	O
is	O
upwardly	O
biased	O
for	O
estimating	O
accordingly	O
confidence	O
intervals	B
should	O
be	O
adjusted	O
a	O
little	O
bit	O
downward	O
the	O
biascorrected	O
percentile	B
method	B
for	O
short	O
is	O
a	O
data-based	O
algorithm	B
for	O
making	O
such	O
adjustments	O
d	O
d	O
o	O
d	O
o	O
d	O
median	O
unbiasedness	O
unlike	O
the	O
usual	O
mean	O
unbiasedness	O
definition	O
has	O
the	O
advantage	O
of	O
being	O
transformation	O
invariant	O
bias-corrected	B
confidence	O
intervals	B
having	O
simulated	O
b	O
bootstrap	B
replications	O
o	O
parameto	O
ric	O
or	O
nonparametric	B
let	O
be	O
the	O
proportion	B
of	O
replications	O
less	O
than	O
o	O
o	O
d	O
n	O
o	O
o	O
o	O
b	O
estimate	B
of	O
and	O
define	O
the	O
bias-correction	B
value	I
d	O
is	O
the	O
inverse	O
function	B
of	O
the	O
standard	O
normal	B
cdf	B
the	O
bc	O
where	O
level-	O
confidence	O
interval	B
endpoint	O
is	O
defined	O
to	O
be	O
c	O
z	O
o	O
d	O
og	O
z	O
d	O
og	O
where	O
og	O
is	O
the	O
bootstrap	B
cdf	B
and	O
z	O
d	O
if	O
d	O
the	O
median	B
unbiased	I
situation	O
then	O
d	O
and	O
o	O
d	O
og	O
d	O
o	O
the	O
percentile	O
limit	O
otherwise	O
a	O
bias	B
correction	O
is	O
made	O
taking	O
d	O
for	O
the	O
normal	B
correlation	O
example	O
value	O
we	O
would	O
get	O
from	O
an	O
infinite	O
number	O
of	O
parametric	B
bootstrap	B
replications	O
gives	O
bias	B
correction	O
value	O
notice	O
that	O
the	O
bc	O
limits	O
are	O
indeed	O
shifted	O
downward	O
from	O
the	O
parametric	B
percentile	O
limits	O
in	O
table	O
nonparametric	B
bootstrapping	O
gave	O
about	O
in	O
this	O
case	O
making	O
the	O
bc	O
limits	O
nearly	O
the	O
same	O
as	O
the	O
percentile	O
limits	O
a	O
more	O
general	O
transformation	O
argument	B
motivates	O
the	O
bc	O
definition	O
suppose	O
there	O
exists	O
a	O
monotone	O
transformation	O
d	O
m	O
and	O
d	O
m	O
o	O
such	O
that	O
for	O
any	O
n	O
with	O
and	O
fixed	O
constants	O
then	O
the	O
bc	O
endpoints	O
are	O
accurate	O
i	O
e	O
have	O
the	O
claimed	O
coverage	B
probabilities	B
and	O
are	O
also	O
obviously	O
correct	O
in	O
the	O
fisherian	O
sense	O
see	O
the	O
chapter	O
endnotes	O
for	O
proof	O
and	O
discussion	O
as	O
before	O
the	O
statistican	O
does	O
not	O
need	O
to	O
know	O
the	O
transformation	O
that	O
leads	O
to	O
n	O
only	O
that	O
it	O
exists	O
it	O
is	O
a	O
broader	O
target	O
than	O
n	O
making	O
the	O
bc	O
method	B
better	O
justified	O
than	O
the	O
percentile	B
method	B
irrespective	O
of	O
whether	O
or	O
not	O
such	O
a	O
transformation	O
exists	O
there	O
is	O
no	O
extra	O
computational	O
burden	O
the	O
bootstrap	B
replications	O
f	O
o	O
b	O
d	O
bg	O
parametric	B
or	O
nonparametric	B
provide	O
og	O
and	O
giving	O
o	O
from	O
bootstrap	B
confidence	O
intervals	B
second-order	B
accuracy	B
p	O
coverage	B
errors	B
of	O
the	O
standard	O
confidence	O
intervals	B
typically	O
decrease	O
at	O
n	O
in	O
the	O
sample	B
size	I
n	O
having	O
calculated	O
o	O
order	O
for	O
an	O
iid	O
sample	B
x	O
d	O
xn	O
we	O
can	O
expect	O
the	O
actual	O
coverage	B
p	O
probability	O
to	O
be	O
d	O
o	O
o	O
c	O
o	O
n	O
pr	O
n	O
where	O
depends	O
on	O
the	O
problem	O
at	O
hand	O
defines	O
first-order	O
accuracy	B
it	O
can	O
connote	O
painfully	O
slow	O
convergence	O
to	O
the	O
nominal	O
coverage	B
level	I
requiring	O
sample	B
size	I
to	O
cut	O
the	O
error	O
in	O
half	O
a	O
second-order	O
accurate	O
method	B
say	O
o	O
makes	O
errors	B
of	O
order	O
only	O
n	O
o	O
pr	O
o	O
c	O
the	O
improvement	O
is	O
more	O
than	O
theoretical	O
in	O
practical	O
problems	O
like	O
that	O
of	O
table	O
second-order	O
accurate	O
methods	O
bca	O
defined	O
in	O
the	O
following	O
is	O
one	O
such	O
often	O
provide	O
nearly	O
the	O
claimed	O
coverage	B
probabilities	B
even	O
in	O
small-size	O
samples	O
neither	O
the	O
percentile	B
method	B
nor	O
the	O
bc	O
method	B
is	O
second-order	O
accurate	O
as	O
in	O
table	O
they	O
tend	O
to	O
be	O
more	O
accurate	O
than	O
the	O
standard	O
intervals	B
the	O
difficulty	O
for	O
o	O
lies	O
in	O
the	O
ideal	O
form	B
o	O
has	O
constant	O
standard	B
error	I
instead	O
we	O
now	O
postulate	O
the	O
existence	O
of	O
a	O
monotone	O
transformation	O
d	O
m	O
and	O
d	O
m	O
n	O
where	O
it	O
is	O
assumed	O
d	O
m	O
o	O
less	O
restrictive	O
than	O
d	O
c	O
n	O
here	O
the	O
acceleration	B
a	O
is	O
a	O
small	O
constant	O
describing	O
how	O
the	O
standard	B
deviation	I
of	O
varies	O
with	O
if	O
a	O
d	O
we	O
are	O
back	O
in	O
situation	O
but	O
if	O
not	O
an	O
amendment	O
to	O
the	O
bc	O
formula	B
is	O
required	O
the	O
bca	O
method	B
bias-corrected	B
and	O
accelerated	O
takes	O
its	O
level-	O
confidence	O
limit	O
to	O
be	O
o	O
d	O
og	O
c	O
c	O
z	O
c	O
z	O
a	O
still	O
more	O
elaborate	O
transformation	O
argument	B
shows	O
that	O
if	O
there	O
exists	O
a	O
monotone	O
transformation	O
d	O
m	O
and	O
constants	O
and	O
a	O
yielding	O
this	O
assumes	O
d	O
on	O
the	O
right	O
side	O
of	O
which	O
can	O
always	O
be	O
achieved	O
by	O
further	O
transforming	O
to	O
second-order	B
accuracy	B
then	O
the	O
bca	O
limits	O
have	O
their	O
claimed	O
coverage	B
probabilities	B
and	O
moreover	O
are	O
correct	O
in	O
the	O
fisherian	O
sense	O
bca	O
makes	O
three	O
corrections	O
to	O
the	O
standard	O
intervals	B
for	O
nonnormality	O
of	O
o	O
using	O
the	O
bootstrap	B
percentiles	O
rather	O
than	O
just	O
the	O
bootstrap	B
standard	B
error	I
for	O
bias	B
the	O
bias	B
correction	O
value	O
and	O
for	O
nonconstant	O
standard	B
error	I
a	O
notice	O
that	O
if	O
a	O
d	O
then	O
bca	O
reduces	O
to	O
bc	O
if	O
d	O
then	O
bc	O
reduces	O
to	O
the	O
percentile	B
method	B
and	O
if	O
og	O
the	O
bootstrap	B
histogram	O
is	O
normal	B
then	O
reduces	O
to	O
the	O
standard	B
interval	B
all	O
three	O
of	O
the	O
corrections	O
for	O
nonnormality	O
bias	B
and	O
acceleration	B
can	O
have	O
substantial	O
effects	O
in	O
practice	O
and	O
are	O
necessary	O
to	O
achieve	O
second-order	B
accuracy	B
a	O
great	O
deal	O
of	O
theoretical	O
effort	O
was	O
devoted	O
to	O
verifying	O
the	O
second-order	B
accuracy	B
and	O
bca	O
intervals	B
under	O
reasonably	O
general	O
d	O
actual	O
tail	O
areas	O
above	O
and	O
below	O
table	O
nominal	O
central	O
confidence	O
intervals	B
for	O
poisson	B
parameter	O
having	O
observed	O
o	O
o	O
d	O
defined	O
as	O
in	O
figure	O
of	O
probability	O
split	O
at	O
for	O
instance	O
lower	O
standard	O
limit	O
actually	O
puts	O
probability	O
above	O
rather	O
than	O
nominal	O
value	O
bias	B
correction	O
value	O
and	O
acceleration	B
a	O
both	O
equal	O
nominal	O
limits	O
tail	O
areas	O
above	O
below	O
standard	O
bc	O
bca	O
exact	O
the	O
advantages	O
of	O
increased	O
accuracy	B
are	O
not	O
limited	O
to	O
large	O
sample	B
sizes	O
table	O
returns	O
to	O
our	O
original	O
example	O
of	O
observing	O
o	O
d	O
from	B
poisson	B
model	O
o	O
poi	O
according	O
to	O
neyman	O
s	O
construction	O
the	O
exact	O
limits	O
give	O
tail	O
areas	O
in	O
both	O
the	O
above	O
and	O
below	O
directions	O
as	O
in	O
figure	O
and	O
this	O
is	O
nearly	O
matched	O
by	O
the	O
bca	O
limits	O
however	O
the	O
standard	O
limits	O
are	O
much	O
too	O
conservative	O
at	O
the	O
left	O
end	O
and	O
anti-conservative	O
at	O
the	O
right	O
the	O
mathematical	O
side	O
of	O
statistics	B
has	O
also	O
been	O
affected	O
by	O
electronic	O
computation	O
where	O
it	O
is	O
called	O
upon	O
to	O
establish	O
the	O
properties	O
of	O
general-purpose	O
computer	O
algorithms	O
such	O
as	O
the	O
bootstrap	B
asymptotic	O
analysis	B
in	O
particular	O
has	O
been	O
aggressively	O
developed	O
the	O
verification	O
of	O
second-order	B
accuracy	B
being	O
a	O
nice	O
success	O
story	O
bootstrap	B
confidence	O
intervals	B
table	O
nominal	O
confidence	O
intervals	B
for	O
the	O
parametric	B
and	O
nonparametric	B
eigenratio	B
examples	O
of	O
figures	O
and	O
standard	O
bc	O
bca	O
parametric	B
d	O
a	O
d	O
nonparametric	B
d	O
a	O
d	O
bootstrap	B
confidence	O
limits	O
continue	O
to	O
provide	O
better	O
inferences	O
in	O
the	O
vast	O
majority	O
of	O
situations	O
too	O
complicated	O
for	O
exact	O
analysis	B
one	O
such	O
situation	O
is	O
examined	O
in	O
table	O
it	O
relates	O
to	O
the	O
eigenratio	B
example	O
illustrated	O
in	O
figures	O
in	O
this	O
case	O
the	O
nonnormality	O
and	O
bias	B
corrections	O
stretch	O
the	O
bootstrap	B
intervals	B
to	O
the	O
left	O
but	O
the	O
acceleration	B
effect	O
pulls	O
right	O
partially	O
canceling	O
out	O
the	O
net	O
change	O
from	O
the	O
standard	O
intervals	B
the	O
percentile	O
and	O
bc	O
methods	O
are	O
completely	O
automatic	O
and	O
can	O
be	O
applied	O
whenever	O
a	O
sufficiently	O
large	O
number	O
of	O
bootstrap	B
replications	O
are	O
available	O
the	O
same	O
cannot	O
be	O
said	O
of	O
bca	O
a	O
drawback	O
of	O
the	O
bca	O
method	B
is	O
that	O
the	O
acceleration	B
a	O
is	O
not	O
a	O
function	B
of	O
the	O
bootstrap	B
distribution	B
and	O
must	O
be	O
computed	O
separately	O
often	O
this	O
is	O
straightforward	O
for	O
one-parameter	B
exponential	O
families	O
such	O
as	O
the	O
poisson	B
a	O
equals	O
in	O
one-sample	O
nonparametric	B
problems	O
a	O
can	O
be	O
estimated	O
from	O
the	O
jackknife	B
resamples	O
o	O
pn	O
o	O
o	O
o	O
o	O
oa	O
d	O
the	O
abc	B
method	B
computes	O
a	O
in	O
multiparameter	O
exponential	O
families	O
as	O
does	O
the	O
resampling-based	O
r	B
algorithm	B
accel	O
confidence	O
intervals	B
require	O
the	O
number	O
of	O
bootstrap	B
replications	O
b	O
to	O
be	O
on	O
the	O
order	O
of	O
rather	O
than	O
the	O
or	O
fewer	O
needed	O
for	O
standard	O
errors	B
the	O
corrections	O
made	O
to	O
the	O
standard	O
intervals	B
are	O
more	O
delicate	O
than	O
standard	O
errors	B
and	O
require	O
greater	O
accuracy	B
there	O
is	O
one	O
more	O
cautionary	O
note	O
to	O
sound	O
concerning	O
nuisance	B
parameters	I
biases	O
can	O
easily	O
get	O
out	O
of	O
hand	O
when	O
the	O
parameter	O
vector	B
is	O
bootstrap-t	O
intervals	B
high-dimensional	O
suppose	O
we	O
observe	O
xi	O
and	O
wish	O
to	O
set	B
a	O
confidence	O
interval	B
for	O
dpn	O
will	O
be	O
sharply	O
biased	O
upward	O
if	O
n	O
is	O
at	O
all	O
large	O
to	O
be	O
specific	O
if	O
n	O
d	O
and	O
o	O
d	O
we	O
compute	O
for	O
i	O
d	O
n	O
dpn	O
i	O
the	O
mle	B
o	O
n	O
i	O
this	O
o	O
centile	O
d	O
d	O
equal	O
a	O
ludicrously	O
small	O
bootstrap	B
per	O
og	O
a	O
warning	O
sign	O
against	O
the	O
bc	O
or	O
bca	O
intervals	B
which	O
work	O
most	O
dependably	O
for	O
and	O
jaj	O
small	O
say	O
a	O
more	O
general	O
warning	O
would	O
be	O
against	O
blind	O
trust	O
in	O
maximum	B
likelihood	B
estimates	O
in	O
high	O
dimensions	O
computing	O
is	O
a	O
wise	O
precaution	O
even	O
if	O
it	O
is	O
not	O
used	O
for	O
bc	O
or	O
bca	O
purposes	O
in	O
case	O
it	O
alerts	O
one	O
to	O
dangerous	O
biases	O
the	O
standard	O
method	B
estimated	O
by	O
the	O
delta	B
method	B
except	O
confidence	O
intervals	B
for	O
classical	O
applications	O
were	O
most	O
often	O
based	O
on	O
in	O
a	O
few	O
especially	O
simple	O
situations	O
such	O
as	O
the	O
poisson	B
second-order	O
accurate	O
intervals	B
are	O
very	O
much	O
a	O
computer-age	O
development	O
with	O
both	O
the	O
algorithms	O
and	O
the	O
inferential	O
theory	B
presupposing	O
high-speed	O
electronic	O
computation	O
bootstrap-t	O
intervals	B
the	O
initial	O
breakthrough	O
on	O
exact	O
confidence	O
intervals	B
came	O
in	O
the	O
form	B
of	O
student	O
s	O
t	B
distribution	B
in	O
suppose	O
we	O
independently	O
observe	O
data	B
from	O
two	O
possibly	O
different	O
normal	B
distributions	O
x	O
d	O
xnx	O
and	O
y	O
d	O
yny	O
xi	O
n	O
and	O
yi	O
n	O
and	O
wish	O
to	O
form	B
a	O
central	O
confidence	O
interval	B
for	O
d	O
o	O
d	O
ny	O
nx	O
the	O
obvious	O
estimate	B
is	O
also	O
obca	O
a	O
is	O
zero	O
in	O
this	O
model	O
bootstrap	B
confidence	O
intervals	B
but	O
its	O
distribution	B
depends	O
on	O
the	O
nuisance	O
parameter	O
student	O
s	O
masterstroke	O
was	O
to	O
base	O
inference	B
about	O
on	O
the	O
pivotal	O
quantity	B
o	O
t	B
d	O
is	O
an	O
unbiased	O
estimate	B
of	O
d	O
cpny	O
c	O
ny	O
i	O
nx	O
represent	O
the	O
th	O
percentile	O
of	O
a	O
tdf	O
distribution	B
yields	O
nx	O
c	O
ny	O
t	B
then	O
has	O
the	O
student	O
s	O
t	B
distribution	B
with	O
df	O
d	O
nx	O
c	O
ny	O
degrees	B
of	I
freedom	I
if	O
d	O
no	O
matter	O
what	O
may	O
be	O
t	B
letting	O
t	B
df	O
d	O
as	O
the	O
upper	O
level-	O
interval	B
of	O
a	O
student	O
s	O
t	B
confidence	O
limit	O
applied	O
to	O
the	O
difference	O
between	O
the	O
aml	O
and	O
all	O
scores	O
in	O
figure	O
the	O
central	O
student	O
s	O
t	B
interval	B
for	O
d	O
efamlg	O
efallg	O
was	O
calculated	O
to	O
be	O
o	O
d	O
o	O
o	O
o	O
df	O
here	O
nx	O
d	O
ny	O
d	O
and	O
df	O
d	O
student	O
s	O
theory	B
depends	O
on	O
the	O
normality	O
assumptions	O
of	O
the	O
bootstrap-t	O
approach	O
is	O
to	O
accept	O
pretend	O
that	O
t	B
in	O
is	O
pivotal	O
but	O
to	O
estimate	B
its	O
distribution	B
via	O
bootstrap	B
resampling	B
nonparametric	B
bootstrap	B
samples	O
are	O
drawn	O
separately	O
from	O
x	O
and	O
y	O
x	O
x	O
d	O
nx	O
from	O
which	O
we	O
calculate	O
o	O
x	O
and	O
y	O
andbse	O
ny	O
y	O
y	O
d	O
and	O
giving	O
o	O
o	O
d	O
t	B
playing	O
the	O
role	O
of	O
as	O
appropriate	O
in	O
the	O
bootstrap	B
world	O
and	O
cor	O
b	O
d	O
bg	O
provide	O
estimated	O
percentiles	O
t	B
with	O
o	O
cations	O
ft	O
responding	O
confidence	O
limits	O
o	O
t	B
d	O
o	O
t	B
for	O
the	O
aml	O
all	O
example	O
the	O
t	B
distribution	B
differed	O
only	O
slightly	O
from	O
a	O
distribution	B
the	O
resulting	O
interval	B
was	O
nearly	O
bootstrap-t	O
intervals	B
the	O
same	O
as	O
lending	O
credence	O
to	O
the	O
original	O
normality	O
assumptions	O
figure	O
b	O
d	O
nonparametric	B
replications	O
of	O
bootstrap-t	O
statistic	B
for	O
the	O
student	B
score	I
correlation	O
small	O
triangles	O
show	O
and	O
percentile	O
points	O
the	O
histogram	O
is	O
sharply	O
skewed	O
to	O
the	O
right	O
the	O
solid	O
curve	O
is	O
student	O
s	O
t	B
density	B
for	O
degrees	B
of	I
freedom	I
returning	O
to	O
the	O
student	B
score	I
correlation	O
example	O
of	O
table	O
we	O
can	O
apply	O
bootstrap-t	O
methods	O
by	O
still	O
taking	O
t	B
d	O
bse	O
the	O
approximate	O
standard	B
error	I
o	O
pivotal	O
but	O
now	O
with	O
the	O
true	O
correlation	O
o	O
p	O
the	O
sample	B
correlation	O
and	O
figure	O
shows	O
the	O
histogram	O
of	O
b	O
d	O
nonparametric	B
bootstrap	B
replications	O
t	B
o	O
to	O
be	O
notionally	O
d	O
these	O
gave	O
bootstrap	B
percentiles	O
o	O
o	O
d	O
t	B
t	B
might	O
be	O
compared	O
with	O
for	O
a	O
standard	O
distribution	B
and	O
interval	B
from	O
somewhat	O
out	O
of	O
place	O
compared	O
with	O
the	O
other	O
entries	O
in	O
the	O
right	O
panel	O
of	O
table	O
bootstrap-t	O
intervals	B
are	O
not	O
transformation	O
invariant	O
this	O
means	O
they	O
can	O
perform	O
poorly	O
or	O
well	O
depending	O
on	O
the	O
scale	B
of	O
application	O
if	O
performed	O
on	O
fisher	B
s	O
scale	B
they	O
agree	O
well	O
with	O
exact	O
intervals	B
for	O
t	B
distributiondf	O
bootstrap	B
confidence	O
intervals	B
mula	O
forbse	O
the	O
correlation	O
coefficient	O
a	O
practical	O
difficulty	O
is	O
the	O
requirement	O
of	O
a	O
for	O
nevertheless	O
the	O
idea	O
of	O
estimating	O
the	O
actual	O
distribution	B
of	O
a	O
proposed	O
pivotal	O
quantity	B
has	O
great	O
appeal	O
to	O
the	O
modern	O
statistical	O
spirit	O
calculating	O
the	O
percentiles	O
of	O
the	O
original	O
student	O
t	B
distribution	B
was	O
a	O
multi-year	O
project	O
in	O
the	O
early	O
twentieth	O
century	O
now	O
we	O
can	O
afford	O
to	O
calculate	O
our	O
own	O
special	O
t	B
table	O
for	O
each	O
new	O
application	O
spending	O
such	O
computational	O
wealth	O
wisely	O
while	O
not	O
losing	O
one	O
s	O
inferential	O
footing	O
is	O
the	O
central	O
task	O
and	O
goal	O
of	O
twenty-first-century	O
statisticians	O
objective	B
bayes	I
intervals	B
and	O
the	O
confidence	O
distribution	B
interval	B
estimates	O
are	O
ubiquitous	O
they	O
play	O
a	O
major	O
role	O
in	O
the	O
scientific	O
discourse	O
of	O
a	O
hundred	O
disciplines	O
from	O
physics	O
astronomy	O
and	O
biology	O
to	O
medicine	O
and	O
the	O
social	O
sciences	O
neyman-style	O
frequentist	B
confidence	O
intervals	B
dominate	O
the	O
literature	O
but	O
there	O
have	O
been	O
influential	O
bayesian	B
and	O
fisherian	O
developments	O
as	O
well	O
as	O
discussed	O
next	O
g	O
bayes	O
rule	B
produces	O
the	O
posterior	B
density	B
of	O
given	O
a	O
one-parameter	B
family	O
of	O
densities	O
f	O
o	O
is	O
the	O
marginal	O
densityr	O
f	O
o	O
d	O
g	O
o	O
the	O
bayes	O
cred	O
g	O
j	O
o	O
o	O
o	O
and	O
a	O
prior	B
density	B
spans	O
the	O
central	O
region	B
of	O
g	O
j	O
o	O
say	O
where	O
f	O
ible	O
interval	B
c	O
j	O
o	O
d	O
o	O
b	O
o	O
with	O
c	O
j	O
o	O
z	O
b	O
o	O
a	O
o	O
g	O
j	O
o	O
d	O
d	O
and	O
with	O
posterior	B
probability	I
in	O
each	O
tail	O
region	B
confidence	O
intervals	B
of	O
course	O
require	O
no	O
prior	B
information	B
making	O
them	O
eminently	O
useful	O
in	O
day-to-day	O
applied	O
practice	O
the	O
bayesian	B
equivalents	O
are	O
credible	O
intervals	B
based	O
on	O
uninformative	O
priors	B
section	O
matching	O
priors	B
those	O
whose	O
credible	O
intervals	B
nearly	O
match	O
neyman	O
confidence	O
intervals	B
have	O
been	O
of	O
particular	O
interest	O
jeffreys	B
prior	B
g	O
d	O
i	O
i	O
dz	O
o	O
log	O
f	O
o	O
d	O
o	O
f	O
objective	B
bayes	I
intervals	B
provides	O
a	O
generally	O
accurate	O
matching	B
prior	B
for	O
one-parameter	B
problems	O
figure	O
illustrates	O
this	O
for	O
the	O
student	B
score	I
correlation	O
where	O
the	O
credible	B
interval	B
is	O
a	O
near-exact	O
match	O
to	O
the	O
neyman	O
interval	B
of	O
figure	O
difficulties	O
begin	O
with	O
multiparameter	O
families	O
we	O
wish	O
to	O
construct	O
an	O
interval	B
estimate	B
for	O
a	O
one-dimensional	O
function	B
d	O
t	B
of	O
the	O
p-dimensional	O
parameter	O
vector	B
and	O
must	O
somehow	O
remove	O
the	O
effects	O
of	O
the	O
nuisance	B
parameters	I
in	O
a	O
few	O
rare	O
situations	O
including	O
the	O
normal	B
theory	B
correlation	O
coefficient	O
this	O
can	O
be	O
done	O
exactly	O
pivotal	O
methods	O
do	O
the	O
job	O
for	O
student	O
s	O
t	B
construction	O
bootstrap	B
confidence	O
intervals	B
greatly	O
extend	O
the	O
reach	O
of	O
such	O
methods	O
at	O
a	O
cost	O
of	O
greatly	O
increased	O
computation	O
bayesians	O
get	O
rid	O
of	O
nuisance	B
parameters	I
by	O
integrating	O
them	O
out	O
of	O
the	O
posterior	B
density	B
d	O
now	O
representing	O
all	O
the	O
data	B
x	O
equaling	O
y	O
for	O
the	O
student	O
t	B
setup	O
that	O
is	O
we	O
the	O
marginal	B
density	B
of	O
d	O
t	B
given	O
x	O
and	O
call	O
it	O
h	O
jx	O
a	O
credible	B
interval	B
for	O
c	O
jx	O
is	O
then	O
constructed	O
as	O
in	O
with	O
h	O
jx	O
playing	O
the	O
role	O
of	O
g	O
j	O
o	O
this	O
leaves	O
us	O
the	O
knotty	O
problem	O
of	O
choosing	O
an	O
uninformative	O
multidimensional	O
prior	B
we	O
will	O
return	O
to	O
the	O
question	O
after	O
first	O
discussing	O
fiducial	B
methods	O
a	O
uniquely	O
fisherian	O
device	O
interpretation	O
of	O
pivotality	O
we	O
rewrite	O
the	O
student	O
t	B
pivotal	O
t	B
d	O
as	O
t	B
d	O
o	O
where	O
t	B
has	O
a	O
student	O
s	O
t	B
distribution	B
with	O
df	O
degrees	B
of	I
freedom	I
t	B
andbse	O
were	O
fixed	O
at	O
their	O
calculated	O
tdf	O
having	O
observed	O
the	O
data	B
y	O
fiducial	B
theory	B
assigns	O
the	O
distribution	B
implied	O
by	O
as	O
if	O
o	O
values	O
while	O
t	B
was	O
distributed	O
as	O
tdf	O
then	O
o	O
confidence	O
limit	O
is	O
the	O
th	O
percentile	O
of	O
s	O
fiducial	B
distribution	B
fiducial	B
constructions	B
begin	O
with	O
what	O
seems	O
like	O
an	O
obviously	O
incorrect	O
o	O
the	O
student	O
t	B
level	O
we	O
seem	O
to	O
have	O
achieved	O
a	O
bayesian	B
posterior	O
conclusion	O
without	O
any	O
prior	B
the	O
historical	O
development	O
here	O
is	O
confused	O
by	O
fisher	B
s	O
refusal	O
to	O
accept	O
neyman	O
s	O
confidence	O
interval	B
theory	B
as	O
well	O
as	O
his	O
disparagement	O
of	O
bayesian	B
ideas	O
as	O
events	O
worked	O
out	O
all	O
of	O
fisher	B
s	O
immense	O
prestige	O
was	O
not	O
enough	O
to	O
save	O
fiducial	B
theory	B
from	O
the	O
scrapheap	O
of	O
failed	O
statistical	O
methods	O
often	O
a	O
difficult	O
calculation	O
as	O
discussed	O
in	O
chapter	O
enjoying	O
the	O
bayesian	B
omelette	O
without	O
breaking	O
the	O
bayesian	B
eggs	O
in	O
l	O
j	O
savage	B
s	O
words	O
bootstrap	B
confidence	O
intervals	B
andbse	O
exhaust	O
the	O
information	B
about	O
available	O
from	O
the	O
and	O
yet	O
in	O
arthur	O
koestler	O
s	O
words	O
the	O
history	O
of	O
ideas	O
is	O
filled	O
with	O
barren	O
truths	O
and	O
fertile	O
errors	B
fisher	B
s	O
underlying	O
rationale	O
went	O
something	O
like	O
this	O
o	O
data	B
after	O
which	O
there	O
remains	O
an	O
irreducible	O
component	O
of	O
randomness	O
described	O
by	O
t	B
this	O
is	O
an	O
idea	O
of	O
substantial	O
inferential	O
appeal	O
and	O
one	O
that	O
can	O
be	O
rephrased	O
in	O
more	O
general	O
terms	O
discussed	O
next	O
that	O
bear	O
on	O
the	O
question	O
of	O
uninformative	O
priors	B
by	O
definition	O
an	O
upper	O
confidence	O
limit	O
o	O
satisfies	O
n	O
o	O
pr	O
o	O
d	O
n	O
o	O
o	O
pr	O
c	O
o	O
d	O
now	O
we	O
have	O
indicated	O
the	O
observed	O
data	B
x	O
in	O
the	O
notation	O
and	O
so	O
we	O
can	O
consider	O
o	O
as	O
a	O
one-to-one	O
function	B
between	O
in	O
and	O
a	O
point	O
in	O
its	O
parameter	B
space	I
that	O
o	O
is	O
smoothly	O
increasing	O
in	O
letting	O
go	O
to	O
zero	O
in	O
determines	O
the	O
confidence	O
density	B
of	O
say	O
qgx	O
qgx	O
d	O
d	O
the	O
local	O
derivative	O
of	O
probability	O
at	O
location	O
for	O
the	O
unknown	O
parameter	O
the	O
derivative	O
being	O
taken	O
at	O
d	O
o	O
integrating	O
qgx	O
recovers	O
as	O
a	O
function	B
of	O
let	O
d	O
o	O
d	O
o	O
and	O
for	O
any	O
two	O
values	O
in	O
then	O
d	O
d	O
qgx	O
d	O
dz	O
z	O
d	O
d	O
d	O
as	O
in	O
there	O
is	O
nothing	O
controversial	O
about	O
as	O
long	O
as	O
we	O
remember	O
that	O
the	O
random	O
quantity	B
in	O
is	O
not	O
but	O
rather	O
the	O
interval	B
which	O
varies	O
as	O
a	O
function	B
of	O
x	O
forgetting	O
this	O
leads	O
to	O
the	O
textbook	O
error	O
of	O
attributing	O
bayesian	B
properties	O
to	O
frequentist	B
results	O
there	O
is	O
probability	O
that	O
is	O
in	O
its	O
confidence	O
interval	B
etc	O
this	O
is	O
exactly	O
what	O
the	O
fiducial	B
argument	B
whether	O
or	O
not	O
one	O
accepts	O
there	O
is	O
an	O
immediate	O
connection	O
with	O
matching	O
priors	B
fiducial	B
and	O
confidence	O
densities	O
agree	O
as	O
can	O
be	O
seen	O
in	O
the	O
student	O
t	B
situation	O
at	O
least	O
in	O
the	O
somewhat	O
limited	O
catalog	O
of	O
cases	O
fisher	B
thought	O
appropriate	O
for	O
fiducial	B
calculations	O
objective	B
bayes	I
intervals	B
suppose	O
prior	B
gives	O
a	O
perfect	O
match	O
to	O
the	O
confidence	O
interval	B
system	O
o	O
then	O
by	O
definition	O
its	O
posterior	B
density	B
h	O
jx	O
must	O
satisfy	O
h	O
jx	O
d	O
d	O
dz	O
ox	O
z	O
ox	O
for	O
but	O
this	O
implies	O
h	O
jx	O
equals	O
qgx	O
for	O
all	O
that	O
is	O
the	O
confidence	O
density	B
qgx	O
is	O
the	O
posterior	B
density	B
of	O
given	O
x	O
for	O
any	O
matching	B
prior	B
qgx	O
d	O
figure	O
confidence	O
density	B
for	O
poisson	B
parameter	O
having	O
observed	O
o	O
between	O
and	O
as	O
in	O
table	O
and	O
areas	O
in	O
each	O
tail	O
d	O
there	O
is	O
area	O
under	O
the	O
curve	O
figure	O
graphs	O
the	O
confidence	O
density	B
for	O
o	O
served	O
o	O
function	B
of	O
poi	O
having	O
ob	O
d	O
this	O
was	O
obtained	O
by	O
numerically	O
differentiating	O
as	O
a	O
d	O
poi	O
including	O
splitting	O
the	O
atom	O
of	O
probability	O
at	O
according	O
to	O
table	O
has	O
area	O
between	O
and	O
and	O
area	O
in	O
each	O
tail	O
whatever	O
its	O
provenance	O
the	O
graph	O
delivers	O
a	O
striking	O
picture	O
of	O
the	O
uncertainty	O
in	O
the	O
unknown	O
value	O
of	O
bootstrap	B
confidence	O
intervals	B
bootstrap	B
confidence	O
intervals	B
provide	O
easily	O
computable	O
confidence	O
denog	O
be	O
the	O
bootstrap	B
cdf	B
and	O
og	O
its	O
density	B
function	B
let	O
tained	O
by	O
differentiating	O
a	O
smoothed	O
version	O
of	O
og	O
when	O
og	O
is	O
based	O
on	O
b	O
bootstrap	B
replications	O
the	O
percentile	O
confidence	O
limits	O
o	O
have	O
d	O
og	O
giving	O
d	O
og	O
qgx	O
d	O
og	O
is	O
helpful	O
to	O
picture	O
this	O
in	O
figure	O
for	O
the	O
percentile	B
method	B
the	O
bootstrap	B
density	B
is	O
the	O
confidence	O
density	B
reweighting	O
og	O
for	O
the	O
bca	O
intervals	B
the	O
confidence	O
density	B
is	O
obtained	O
by	O
qgx	O
d	O
cw	O
where	O
w	O
d	O
z	O
c	O
az	O
c	O
az	O
c	O
with	O
z	O
d	O
og	O
here	O
is	O
the	O
standard	O
normal	B
density	B
its	O
cdf	B
and	O
c	O
the	O
constant	O
that	O
makes	O
qgx	O
integrate	O
to	O
in	O
the	O
usual	O
case	O
where	O
the	O
bootstrap	B
cdf	B
is	O
estimated	O
from	O
replications	O
o	O
b	O
d	O
b	O
parametric	B
or	O
nonparametric	B
the	O
bca	O
confidence	O
density	B
is	O
a	O
reweighted	O
version	O
of	O
og	O
define	O
bx	O
o	O
o	O
w	O
wb	O
d	O
w	O
then	O
the	O
bca	O
confidence	O
density	B
is	O
the	O
discrete	O
density	B
putting	O
weight	O
wb	O
on	O
o	O
figure	O
returns	O
to	O
the	O
student	B
score	I
data	B
n	O
d	O
students	O
five	O
scores	O
each	O
modeled	O
normally	O
as	O
in	O
figure	O
for	O
i	O
d	O
xi	O
this	O
is	O
a	O
p	O
d	O
parametric	B
family	I
expectations	O
variances	O
covariances	O
the	O
parameter	O
of	O
interest	O
was	O
taken	O
to	O
be	O
d	O
maximum	O
eigenvalue	O
of	O
it	O
had	O
mle	B
o	O
d	O
this	O
being	O
the	O
maximum	O
eigenvalue	O
of	O
the	O
mle	B
sample	B
covariance	O
matrix	B
o	O
each	O
sum	O
of	O
squares	O
by	O
rather	O
than	O
b	O
d	O
parametric	B
bootstrap	B
o	O
gave	O
percentile	O
and	O
b	O
d	O
would	O
have	O
been	O
enough	O
for	O
most	O
purposes	O
but	O
b	O
d	O
gave	O
a	O
sharper	O
picture	O
of	O
the	O
different	O
curves	O
objective	B
bayes	I
intervals	B
figure	O
confidence	O
densities	O
for	O
the	O
maximum	O
eigenvalue	O
parameter	O
using	O
a	O
multivariate	B
normal	B
model	O
for	O
the	O
student	B
score	I
data	B
the	O
dashed	O
red	O
curve	O
is	O
the	O
percentile	B
method	B
solid	O
black	O
the	O
bca	O
a	O
d	O
the	O
dotted	O
blue	O
curve	O
is	O
the	O
bayes	O
posterior	B
density	B
for	O
using	O
jeffreys	B
prior	B
bca	O
confidence	O
densities	O
as	O
shown	O
in	O
this	O
case	O
the	O
weights	B
wb	O
increased	O
with	O
o	O
pushing	O
the	O
bca	O
density	B
to	O
the	O
right	O
also	O
shown	O
is	O
the	O
bayes	O
posterior	B
density	B
for	O
starting	O
from	O
jeffreys	B
multiparameter	O
prior	B
density	B
d	O
ji	O
where	O
i	O
is	O
the	O
fisher	B
information	B
matrix	B
it	O
isn	O
t	B
truly	O
uninformative	O
here	O
moving	O
its	O
credible	O
limits	O
upward	O
from	O
the	O
second-order	O
accurate	O
bca	O
confidence	O
limits	O
formula	B
is	O
discussed	O
further	O
in	O
chapter	O
bayesian	B
data	B
analysis	B
has	O
the	O
attractive	O
property	O
that	O
after	O
examining	O
the	O
data	B
we	O
can	O
express	O
our	O
remaining	O
uncertainty	O
in	O
the	O
language	O
of	O
probability	O
fiducial	B
and	O
confidence	O
densities	O
provide	O
something	O
similar	O
for	O
confidence	O
intervals	B
at	O
least	O
partially	O
freeing	O
the	O
frequentist	B
from	O
the	O
interpretive	O
limitations	O
of	O
neyman	O
s	O
intervals	B
maximum	O
eigenvalueposterior	O
bootstrap	B
confidence	O
intervals	B
notes	O
and	O
details	O
fisher	B
s	O
theory	B
of	O
fiducial	B
inference	B
preceded	O
neyman	O
s	O
approach	O
formalized	O
in	O
which	O
was	O
presented	O
as	O
an	O
attempt	O
to	O
put	O
interval	B
estimation	B
on	O
a	O
firm	O
probabilistic	O
basis	O
as	O
opposed	O
to	O
the	O
mysteries	O
of	O
fiducialism	O
the	O
result	O
was	O
an	O
elegant	O
theory	B
of	O
exact	O
and	O
optimal	O
intervals	B
phrased	O
in	O
hard-edged	O
frequentistic	O
terms	O
readers	O
familiar	O
with	O
the	O
theory	B
will	O
know	O
that	O
neyman	O
s	O
construction	O
a	O
favorite	O
name	O
in	O
the	O
physics	O
literature	O
as	O
pictured	O
in	O
figure	O
requires	O
some	O
conditions	B
on	O
the	O
famo	O
ily	O
of	O
densities	O
f	O
to	O
yield	O
optimal	O
intervals	B
a	O
sufficient	O
condition	B
being	O
monotone	O
likelihood	B
ratios	O
bootstrap	B
confidence	O
intervals	B
efron	O
are	O
neither	O
exact	O
nor	O
optimal	O
but	O
aim	O
instead	O
for	O
wide	O
applicability	O
combined	O
with	O
near-exact	O
accuracy	B
second-order	O
acuracy	O
of	O
bca	O
intervals	B
was	O
established	O
by	O
hall	O
bca	O
is	O
emphatically	O
a	O
child	O
of	O
the	O
computer	O
age	O
routinely	O
requiring	O
b	O
d	O
or	O
more	O
bootstrap	B
replications	O
per	O
use	O
shortcut	O
methods	O
are	O
available	O
the	O
abc	B
method	B
and	O
efron	O
needs	O
only	O
as	O
much	O
computation	O
at	O
the	O
expense	O
of	O
requiring	O
smoothness	O
properties	O
for	O
d	O
t	B
and	O
a	O
less	O
automatic	O
coding	O
of	O
the	O
exponential	B
family	I
setting	O
for	O
individual	O
situations	O
in	O
other	O
words	O
it	O
is	O
less	O
convenient	O
z	O
denote	O
the	O
central	B
interval	B
of	O
density	B
f	O
neyman	O
s	O
construction	O
for	O
any	O
given	O
value	O
of	O
let	O
d	O
o	O
d	O
and	O
o	O
f	O
o	O
be	O
the	O
indicator	O
function	B
for	O
o	O
if	O
otherwise	O
o	O
o	O
o	O
satisfying	O
z	O
f	O
d	O
o	O
o	O
d	O
and	O
let	O
i	O
i	O
o	O
has	O
a	O
two-point	O
probability	O
distribution	B
by	O
definition	O
i	O
d	O
o	O
d	O
i	O
probability	O
probability	O
o	O
a	O
pivotal	O
statistic	B
one	O
whose	O
distribution	B
does	O
not	O
de	O
this	O
makes	O
i	O
pend	O
upon	O
neyman	O
s	O
construction	O
takes	O
the	O
confidence	O
interval	B
c	O
to	O
observed	O
value	O
o	O
o	O
dn	O
w	O
i	O
o	O
d	O
o	O
to	O
be	O
c	O
o	O
corresponding	O
notes	O
and	O
details	O
o	O
has	O
the	O
desired	O
coverage	B
property	O
n	O
i	O
o	O
d	O
o	O
d	O
then	O
c	O
pr	O
d	O
pr	O
o	O
n	O
c	O
and	O
o	O
o	O
o	O
for	O
any	O
choice	O
of	O
the	O
true	O
parameter	O
the	O
normal	B
theory	B
correlation	O
density	B
of	O
f	O
are	O
increasing	O
functions	O
of	O
this	O
makes	O
our	O
previous	O
construction	O
agree	O
with	O
the	O
construction	O
applies	O
quite	O
generally	O
as	O
long	O
as	O
we	O
are	O
able	O
to	O
define	O
acceptance	O
regions	O
of	O
the	O
sample	B
space	O
having	O
the	O
desired	O
target	O
probability	O
content	O
for	O
every	O
choice	O
of	O
this	O
can	O
be	O
challenging	O
in	O
multiparameter	O
families	O
fisherian	B
correctness	I
fisher	B
arguing	O
against	O
the	O
neyman	O
paradigm	O
pointed	O
out	O
that	O
confidence	O
intervals	B
could	O
be	O
accurate	O
without	O
being	O
correct	O
having	O
observed	O
xi	O
interval	B
based	O
on	O
just	O
the	O
first	O
observations	O
would	O
provide	O
exact	O
coverage	B
while	O
giving	O
obviously	O
incorrect	O
inferences	O
for	O
if	O
we	O
can	O
reduce	O
the	O
situation	O
to	O
form	B
the	O
percentile	B
method	B
intervals	B
satisfy	O
fisher	B
s	O
logic	B
of	I
inductive	B
inference	B
for	O
correctness	O
as	O
at	O
bootstrap	B
sample	B
sizes	O
why	O
we	O
need	O
bootstrap	B
sample	B
sizes	O
on	O
the	O
order	O
of	O
b	O
d	O
for	O
confidence	O
interval	B
construction	O
can	O
be	O
seen	O
in	O
the	O
estimation	B
of	O
the	O
bias	B
correction	O
value	O
the	O
delta-method	O
standard	B
error	I
of	O
d	O
n	O
for	O
i	O
d	O
the	O
standard	O
is	O
calculated	O
to	O
be	O
this	O
is	O
with	O
the	O
standard	O
normal	B
density	B
with	O
about	O
equaling	O
at	O
b	O
d	O
a	O
none-too-small	O
error	O
for	O
use	O
in	O
the	O
bc	O
formula	B
or	O
the	O
bca	O
formula	B
bca	O
accuracy	B
and	I
correctness	I
the	O
bca	O
confidence	O
limit	O
o	O
and	O
b	O
is	O
transformation	O
invariant	O
define	O
c	O
z	O
z	O
d	O
c	O
so	O
o	O
o	O
and	O
d	O
m	O
d	O
m	O
d	O
m	O
og	O
since	O
isfies	O
percentiles	O
therefore	O
d	O
og	O
oh	O
d	O
oh	O
og	O
c	O
z	O
d	O
m	O
r	B
the	O
bootstrap	B
cdf	B
z	O
g	O
for	O
a	O
monotone	O
increasing	O
transformation	O
oh	O
of	O
for	O
the	O
bootstrap	B
o	O
og	O
f	O
d	O
m	O
f	O
d	O
m	O
o	O
equals	O
d	O
m	O
verifying	O
transformation	B
invariance	I
that	O
d	O
o	O
bootstrap	B
confidence	O
intervals	B
oh	O
and	O
is	O
also	O
transformation	O
invariant	O
as	O
is	O
a	O
as	O
discussed	O
pre	O
viously	O
exact	O
confidence	O
intervals	B
are	O
transformation	O
invariant	O
adding	O
considerably	O
to	O
their	O
inferential	O
appeal	O
for	O
approximate	O
intervals	B
transformation	B
invariance	I
means	O
that	O
if	O
we	O
can	O
demonstrate	O
good	O
behavior	O
on	O
any	O
one	O
n	O
scale	B
then	O
it	O
remains	O
good	O
on	O
all	O
scales	O
the	O
model	O
to	O
the	O
scale	B
can	O
be	O
re-expressed	O
as	O
c	O
a	O
o	O
d	O
c	O
c	O
a	O
z	O
where	O
z	O
is	O
a	O
standard	O
normal	B
variate	O
z	O
d	O
c	O
u	O
taking	O
logarithms	O
n	O
where	O
d	O
c	O
a	O
d	O
c	O
and	O
u	O
is	O
the	O
random	O
variable	O
c	O
a	O
z	O
represents	O
the	O
simplest	O
kind	O
of	O
translation	O
model	O
where	O
the	O
unknown	O
value	O
of	O
rigidly	O
shifts	O
the	O
distribution	B
of	O
u	O
the	O
obvious	O
confidence	O
limit	O
for	O
d	O
u	O
where	O
u	O
is	O
the	O
percentile	O
of	O
u	O
is	O
then	O
accurate	O
and	O
also	O
correct	O
according	O
to	O
fisher	B
s	O
vague	O
logic	B
of	I
inductive	B
inference	B
it	O
is	O
an	O
algebraic	O
exercise	O
given	O
in	O
section	O
of	O
efron	O
to	O
reverse	O
the	O
transformations	O
and	O
recover	O
o	O
setting	O
a	O
d	O
shows	O
the	O
accuracy	B
and	I
correctness	I
of	O
o	O
the	O
acceleration	B
a	O
this	O
a	O
appears	O
in	O
as	O
the	O
rate	B
of	O
change	O
of	O
s	O
standard	B
deviation	I
as	O
a	O
function	B
of	O
its	O
expectation	O
in	O
one-parameter	B
exponential	O
families	O
it	O
turns	O
out	O
that	O
this	O
is	O
one-third	O
of	O
that	O
is	O
the	O
transformation	O
to	O
normality	O
d	O
m	O
also	O
decreases	O
the	O
instability	O
of	O
the	O
standard	B
deviation	I
though	O
not	O
to	O
zero	O
the	O
variance	B
of	O
the	O
score	B
function	B
p	O
lx	O
determines	O
the	O
standard	B
deviation	I
of	O
the	O
mle	B
o	O
in	O
one-parameter	B
exponential	O
families	O
one-sixth	O
the	O
skewness	O
of	O
p	O
lx	O
gives	O
a	O
the	O
skewness	O
connection	O
can	O
be	O
seen	O
at	O
work	O
in	O
estimate	B
in	O
multivariate	B
exponential	O
families	O
the	O
skewness	O
must	O
be	O
evaluated	O
in	O
the	O
least	O
favorable	O
direction	O
discussed	O
further	O
in	O
chapter	O
the	O
r	B
algorithm	B
accel	O
web	O
site	O
to	O
estimate	B
a	O
the	O
peruses	O
b	O
parametric	B
bootstrap	B
replications	O
centile	O
and	O
bc	O
intervals	B
require	O
only	O
the	O
replications	O
o	O
while	O
bca	O
also	O
requires	O
knowledge	O
of	O
the	O
underlying	O
exponential	B
family	I
see	O
sections	O
and	O
of	O
efron	O
o	O
o	O
notes	O
and	O
details	O
d	O
p	O
tral	O
chi-square	O
variable	O
with	O
noncentrality	O
parameter	O
d	O
p	O
equation	B
model	O
makes	O
o	O
i	O
a	O
nonceni	O
and	O
n	O
degrees	B
of	I
freedom	I
written	O
as	O
o	O
with	O
o	O
d	O
and	O
n	O
d	O
the	O
parametric	B
bootstrap	B
distribution	B
is	O
r	B
numerical	O
evaluation	O
gives	O
efron	O
concerns	O
confidence	O
intervals	B
for	O
parameters	O
d	O
t	B
in	O
model	O
where	O
third-order	O
accurate	O
confidence	O
intervals	B
can	O
be	O
calculated	O
the	O
acceleration	B
a	O
equals	O
zero	O
for	O
such	O
problems	O
making	O
the	O
bc	O
intervals	B
second-order	O
accurate	O
in	O
practice	O
the	O
bc	O
intervals	B
usually	O
perform	O
well	O
and	O
are	O
a	O
reasonable	O
choice	O
if	O
the	O
accleration	O
a	O
is	O
unavailable	O
d	O
leading	O
to	O
bca	O
confidence	O
density	B
define	O
og	O
i	O
d	O
z	O
d	O
so	O
that	O
z	O
d	O
z	O
c	O
az	O
c	O
z	O
c	O
z	O
z	O
c	O
az	O
here	O
we	O
are	O
thinking	O
of	O
and	O
as	O
functionally	O
related	O
by	O
d	O
o	O
differentiation	O
yields	O
and	O
d	O
z	O
z	O
c	O
az	O
c	O
az	O
c	O
d	O
d	O
d	O
dz	O
dz	O
d	O
og	O
which	O
together	O
give	O
d	O
verifying	O
the	O
name	O
confidence	O
density	B
seems	O
to	O
appear	O
first	O
in	O
efron	O
though	O
the	O
idea	O
is	O
familiar	O
in	O
the	O
fiducial	B
literature	O
an	O
ambitious	O
frequentist	B
theory	B
of	O
confidence	O
distributions	O
is	O
developed	O
in	O
xie	O
and	O
singh	O
jeffreys	B
prior	B
formula	B
is	O
discussed	O
further	O
in	O
chapter	O
in	O
the	O
more	O
general	O
context	O
of	O
uninformative	B
prior	B
distributions	O
the	O
theory	B
of	O
matching	O
priors	B
was	O
initiated	O
by	O
welch	O
and	O
peers	O
another	O
important	O
reference	O
being	O
tibshirani	O
cross-validation	B
and	O
cp	B
estimates	O
of	O
prediction	O
error	O
prediction	O
has	O
become	O
a	O
major	O
branch	O
of	O
twenty-first-century	O
commerce	O
questions	O
of	O
prediction	O
arise	O
naturally	O
how	O
credit-worthy	O
is	O
a	O
loan	O
applicant	O
is	O
a	O
new	O
email	O
message	O
spam	B
how	O
healthy	O
is	O
the	O
kidney	O
of	O
a	O
potential	O
donor	O
two	O
problems	O
present	O
themselves	O
how	O
to	O
construct	O
an	O
effective	O
prediction	O
rule	B
and	O
how	O
to	O
estimate	B
the	O
accuracy	B
of	O
its	O
predictions	O
in	O
the	O
language	O
of	O
chapter	O
the	O
first	O
problem	O
is	O
more	O
algorithmic	O
the	O
second	O
more	O
inferential	O
chapters	O
on	O
machine	B
learning	I
concern	O
prediction	O
rule	B
construction	O
here	O
we	O
will	O
focus	O
on	O
the	O
second	O
question	O
having	O
chosen	O
a	O
particular	O
rule	B
how	O
do	O
we	O
estimate	B
its	O
predictive	O
accuracy	B
two	O
quite	O
distinct	O
approaches	O
to	O
prediction	O
error	O
assessment	O
developed	O
in	O
the	O
the	O
first	O
depending	O
on	O
the	O
classical	O
technique	O
of	O
crossvalidation	O
was	O
fully	O
general	O
and	O
nonparametric	B
a	O
narrower	O
more	O
efficient	O
model-based	O
approach	O
was	O
the	O
second	O
emerging	O
in	O
the	O
form	B
of	O
mallows	O
cp	B
estimate	B
and	O
the	O
akaike	B
information	B
criterion	I
both	O
theories	O
will	O
be	O
discussed	O
here	O
beginning	O
with	O
cross-validation	B
after	O
a	O
brief	O
overview	O
of	O
prediction	O
rules	O
prediction	O
rules	O
prediction	O
problems	O
typically	O
begin	O
with	O
a	O
training	B
set	B
d	O
consisting	O
of	O
n	O
pairs	O
yi	O
d	O
d	O
f	O
xi	O
yi	O
i	O
d	O
ng	O
where	O
xi	O
is	O
a	O
vector	B
of	O
p	O
predictors	B
and	O
yi	O
a	O
real-valued	O
response	B
on	O
the	O
basis	O
of	O
the	O
training	B
set	B
a	O
prediction	O
rule	B
rd	O
is	O
constructed	O
such	O
that	O
a	O
prediction	O
oy	O
is	O
produced	O
for	O
any	O
point	O
x	O
in	O
the	O
predictor	B
s	O
sample	B
space	O
x	O
oy	O
d	O
rd	O
for	O
x	O
x	O
d	O
if	O
if	O
prediction	O
rules	O
the	O
inferential	O
task	O
is	O
to	O
assess	O
the	O
accuracy	B
of	O
the	O
rule	B
s	O
predictions	O
practice	O
there	O
are	O
usually	O
several	O
competing	O
rules	O
under	O
consideration	O
and	O
the	O
main	O
question	O
is	O
determining	O
which	O
is	O
best	O
in	O
the	O
spam	B
data	B
of	O
section	O
xi	O
comprised	O
p	O
d	O
keyword	O
counts	O
while	O
yi	O
indicated	O
whether	O
or	O
not	O
message	O
i	O
was	O
spam	B
the	O
rule	B
rd	O
in	O
table	O
was	O
an	O
mle	B
logistic	B
regression	B
fit	O
given	O
a	O
new	O
message	O
s	O
count	O
vector	B
say	O
rd	O
provided	O
an	O
estimated	O
probability	O
of	O
it	O
being	O
spam	B
which	O
could	O
be	O
converted	O
into	O
a	O
prediction	O
according	O
to	O
the	O
diabetes	B
data	B
of	O
table	O
section	O
involved	O
the	O
p	O
d	O
predictors	B
x	O
d	O
sex	O
glu	O
obtained	O
at	O
baseline	O
and	O
a	O
response	B
y	O
measuring	O
disease	O
progression	O
one	O
year	O
later	O
given	O
a	O
new	O
patient	O
s	O
baseline	O
measurements	O
we	O
would	O
like	O
to	O
predict	O
his	O
or	O
her	O
progression	O
table	O
suggests	O
two	O
possible	O
prediction	O
rules	O
ordinary	O
least	B
squares	I
and	O
ridge	B
regression	B
using	O
ridge	O
parameter	O
d	O
either	O
of	O
which	O
will	O
produce	O
a	O
prediction	O
in	O
this	O
case	O
we	O
might	O
assess	O
prediction	O
error	O
in	O
terms	O
of	O
squared	B
error	I
in	O
both	O
of	O
these	O
examples	O
rd	O
was	O
a	O
regression	B
estimator	B
suggested	O
by	O
a	O
probability	O
model	O
one	O
of	O
the	O
charms	O
of	O
prediction	O
is	O
that	O
the	O
rule	B
rd	O
need	O
not	O
be	O
based	O
on	O
an	O
explicit	O
model	O
regression	B
trees	B
as	O
pictured	O
in	O
figure	O
are	O
widely	O
prediction	O
algorithms	O
that	O
do	O
not	O
require	O
model	O
specifications	O
prediction	O
perhaps	O
because	O
of	O
its	O
model-free	O
nature	O
is	O
an	O
area	O
where	O
algorithmic	O
developments	O
have	O
run	O
far	O
ahead	O
of	O
their	O
inferential	O
justification	O
quantifying	O
the	O
prediction	O
error	O
of	O
a	O
rule	B
rd	O
requires	O
specification	O
of	O
the	O
discrepancy	O
d	O
y	O
oy	O
between	O
a	O
prediction	O
oy	O
and	O
the	O
actual	O
response	B
y	O
the	O
two	O
most	O
common	O
choices	O
are	O
squared	B
error	I
d	O
y	O
oy	O
d	O
and	O
classification	O
error	O
d	O
y	O
oy	O
d	O
if	O
y	O
oy	O
if	O
y	O
d	O
oy	O
when	O
as	O
with	O
the	O
spam	B
data	B
the	O
response	B
y	O
is	O
dichotomous	O
of	O
a	O
dichotomous	O
response	B
is	O
often	O
called	O
classification	O
random	O
forests	O
one	O
of	O
the	O
most	O
popular	O
machine	B
learning	I
prediction	O
algorithms	O
is	O
an	O
elaboration	O
of	O
regression	B
trees	B
see	O
chapter	O
cross-validation	B
and	O
cp	B
estimates	O
for	O
the	O
purpose	O
of	O
error	O
estimation	B
we	O
suppose	O
that	O
the	O
pairs	O
yi	O
in	O
the	O
training	B
set	B
d	O
of	O
have	O
been	O
obtained	O
by	O
random	O
sampling	O
from	O
some	O
probability	O
distribution	B
f	O
on	O
c	O
space	O
f	O
for	O
i	O
d	O
n	O
yi	O
the	O
true	B
error	I
rate	B
errd	O
of	O
rule	B
rd	O
is	O
the	O
expected	O
discrepancy	O
of	O
d	O
rd	O
from	O
given	O
a	O
new	O
pair	O
drawn	O
from	O
f	O
independently	O
of	O
d	O
errd	O
d	O
ef	O
d	O
rd	O
is	O
held	O
fixed	O
in	O
expectation	O
only	O
varying	O
figure	O
concerns	O
the	O
supernova	B
data	B
an	O
example	O
we	O
will	O
return	O
to	O
in	O
the	O
next	O
section	O
absolute	O
magnitudes	O
yi	O
have	O
been	O
measured	O
for	O
n	O
d	O
relatively	O
nearby	O
type	O
ia	O
supernovas	O
with	O
the	O
data	B
scaled	O
such	O
that	O
i	O
d	O
yi	O
is	O
a	O
reasonable	O
model	O
for	O
each	O
supernova	B
a	O
vector	B
xi	O
of	O
p	O
d	O
spectral	O
energies	O
has	O
been	O
observed	O
n	O
i	O
d	O
xi	O
d	O
xi	O
table	O
shows	O
yi	O
for	O
i	O
d	O
frequency	O
measurements	O
have	O
been	O
standardized	O
to	O
have	O
mean	O
and	O
variance	B
while	O
y	O
has	O
been	O
adjusted	O
to	O
have	O
mean	O
on	O
the	O
basis	O
of	O
the	O
training	B
set	B
d	O
d	O
f	O
xi	O
yi	O
i	O
d	O
we	O
wish	O
to	O
construct	O
a	O
rule	B
rd	O
that	O
given	O
the	O
frequency	O
vector	B
for	O
a	O
newly	O
observed	O
type	O
ia	O
supernova	B
accurately	O
its	O
absolute	O
magnitude	O
to	O
this	O
end	O
a	O
lasso	B
estimate	B
q	O
was	O
fit	O
with	O
y	O
in	O
the	O
vector	B
and	O
x	O
the	O
matrix	B
having	O
ith	O
row	O
xi	O
was	O
selected	O
to	O
minimize	O
a	O
cp	B
estimate	B
of	O
prediction	O
error	O
section	O
yielding	O
prediction	O
rule	B
d	O
x	O
q	O
in	O
this	O
case	O
constructing	O
rd	O
itself	O
involves	O
error	O
rate	B
estimation	B
type	O
ia	O
supernovas	O
were	O
used	O
as	O
standard	B
candles	I
in	O
the	O
discovery	O
of	O
dark	B
energy	I
and	O
the	O
cosmological	O
expansion	O
of	O
the	O
universe	O
on	O
the	O
grounds	O
that	O
they	O
have	O
constant	O
absolute	O
magnitude	O
this	O
isn	O
t	B
exactly	O
true	O
our	O
training	B
set	B
is	O
unusual	O
in	O
that	O
the	O
supernovas	O
are	O
close	O
enough	O
to	O
earth	O
to	O
have	O
y	O
ascertained	O
directly	O
this	O
allows	O
the	O
construction	O
of	O
a	O
prediction	O
rule	B
based	O
on	O
the	O
frequency	O
vector	B
x	O
which	O
is	O
observable	O
for	O
distant	O
supernovas	O
leading	O
to	O
improved	O
calibration	O
of	O
the	O
cosmological	O
expansion	O
prediction	O
rules	O
figure	O
the	O
supernova	B
data	B
observed	O
absolute	O
magnitudes	O
yi	O
log	O
scale	B
plotted	O
versus	O
predictions	O
oyi	O
obtained	O
from	O
lasso	B
rule	B
for	O
n	O
d	O
nearby	O
type	O
ia	O
supernovas	O
predictions	O
based	O
on	O
spectral	O
power	O
measurements	O
of	O
which	O
had	O
nonzero	O
coefficients	O
in	O
q	O
the	O
plotted	O
points	O
in	O
figure	O
are	O
oyi	O
yi	O
for	O
i	O
d	O
n	O
d	O
these	O
gave	O
apparent	B
error	I
nx	O
oyi	O
d	O
err	O
d	O
comparing	O
this	O
withp	O
yi	O
d	O
yields	O
an	O
impressive-looking	O
n	O
r	B
squared	O
value	O
d	O
d	O
things	O
aren	O
t	B
really	O
that	O
good	O
cross-validation	B
and	O
cp	B
methods	O
allow	O
us	O
to	O
correct	O
apparent	O
errors	B
for	O
the	O
fact	O
that	O
rd	O
was	O
chosen	O
to	O
make	O
the	O
predictions	O
oyi	O
fit	O
the	O
data	B
yi	O
prediction	O
and	O
estimation	B
are	O
close	O
cousins	O
but	O
they	O
are	O
not	O
twins	O
as	O
discussed	O
earlier	O
prediction	O
is	O
less	O
model-dependent	O
which	O
partly	O
accounts	O
for	O
the	O
distinctions	O
made	O
in	O
section	O
the	O
prediction	O
criterion	O
err	O
lllllllllllllllllllllllllllllllllllllll	O
magnitude	O
yiabsolute	O
magnitude	O
yiapparent	O
mean	O
squarederror	O
cross-validation	B
and	O
cp	B
estimates	O
table	O
supernova	B
data	B
frequency	O
measurements	O
and	O
response	B
variable	O
absolute	O
magnitude	O
for	O
the	O
first	O
of	O
n	O
d	O
type	O
ia	O
supernovas	O
in	O
terms	O
of	O
notation	O
frequency	O
measurements	O
are	O
x	O
and	O
magnitude	O
y	O
mag	O
is	O
an	O
expectation	O
over	O
the	O
y	O
space	O
this	O
emphasizes	O
good	O
overall	O
performance	O
without	O
much	O
concern	O
for	O
behavior	O
at	O
individual	O
points	O
x	O
in	O
x	O
shrinkage	B
usually	O
improves	O
prediction	O
consider	O
a	O
bayesian	B
model	O
like	O
that	O
of	O
section	O
n	O
a	O
and	O
n	O
for	O
i	O
d	O
n	O
the	O
bayes	O
shrinkage	B
estimator	B
which	O
is	O
ideal	O
for	O
estimation	B
d	O
bxi	O
b	O
d	O
a	O
a	O
c	O
is	O
also	O
ideal	O
for	O
prediction	O
suppose	O
that	O
in	O
addition	O
to	O
the	O
observations	O
xi	O
there	O
are	O
independent	O
unobserved	O
replicates	O
one	O
for	O
each	O
of	O
the	O
n	O
xi	O
values	O
yi	O
n	O
for	O
i	O
d	O
n	O
that	O
we	O
wish	O
to	O
predict	O
the	O
bayes	O
predictor	B
oyi	O
d	O
bxi	O
has	O
overall	O
bayes	O
prediction	O
error	O
nx	O
oyi	O
e	O
n	O
d	O
b	O
c	O
which	O
cannot	O
be	O
improved	O
upon	O
the	O
mle	B
rule	B
oyi	O
d	O
xi	O
has	O
bayes	O
prediction	O
error	O
which	O
is	O
always	O
worse	O
than	O
cross-validation	B
as	O
far	O
as	O
prediction	O
is	O
concerned	O
it	O
pays	O
to	O
overshrink	O
as	O
illustrated	O
in	O
figure	O
for	O
the	O
james	O
stein	O
version	O
of	O
situation	O
this	O
is	O
fine	O
for	O
prediction	O
but	O
less	O
fine	O
for	O
estimation	B
if	O
we	O
are	O
concerned	O
about	O
extreme	O
cases	O
see	O
table	O
prediction	O
rules	O
sacrifice	O
the	O
extremes	O
for	O
the	O
sake	O
of	O
the	O
middle	O
a	O
particularly	O
effective	O
tactic	O
in	O
dichotomous	O
situations	O
where	O
the	O
cost	O
of	O
individual	O
errors	B
is	O
bounded	O
the	O
most	O
successful	O
machine	B
learning	I
prediction	O
algorithms	O
discussed	O
in	O
chapters	O
carry	O
out	O
a	O
version	O
of	O
local	O
bayesian	B
shrinkage	B
in	O
selected	O
regions	O
of	O
x	O
cross-validation	B
having	O
constructed	O
a	O
prediction	O
rule	B
rd	O
on	O
the	O
basis	O
of	O
training	B
set	B
d	O
we	O
wish	O
to	O
know	O
its	O
prediction	O
error	O
err	O
d	O
ef	O
for	O
a	O
new	O
case	O
obtained	O
independently	O
of	O
d	O
a	O
first	O
guess	O
is	O
the	O
apparent	B
error	I
nx	O
err	O
d	O
n	O
d	O
yi	O
oyi	O
the	O
average	O
discrepancy	O
in	O
the	O
training	B
set	B
between	O
yi	O
and	O
its	O
prediction	O
oyi	O
d	O
rd	O
err	O
usually	O
underestimates	O
err	O
since	O
rd	O
has	O
been	O
to	O
fit	O
the	O
observed	O
responses	O
yi	O
the	O
ideal	O
remedy	O
discussed	O
in	O
section	O
would	O
be	O
to	O
have	O
an	O
inde	O
pendent	O
validation	B
set	B
test	O
set	B
dval	O
of	O
nval	O
additional	O
cases	O
nval	O
cross-validation	B
attempts	O
to	O
mimiccerrval	O
without	O
the	O
need	O
for	O
a	O
validation	B
set	B
define	O
d	O
to	O
be	O
the	O
reduced	O
training	B
set	B
in	O
which	O
pair	O
yi	O
has	O
been	O
omitted	O
and	O
let	O
rd	O
i	O
indicate	O
the	O
rule	B
constructed	O
on	O
the	O
basis	O
err	O
dp	O
linear	B
regression	B
using	O
ordinary	O
least	B
squares	I
fitting	O
provides	O
a	O
classical	O
illustration	O
i	O
oyi	O
p	O
where	O
p	O
is	O
i	O
oyi	O
must	O
be	O
increased	O
top	O
the	O
degrees	B
of	I
freedom	I
to	O
obtain	O
an	O
unbiased	O
estimate	B
of	O
the	O
noise	O
variance	B
dval	O
d	O
j	O
d	O
nval	O
nvalx	O
d	O
rd	O
cerrval	O
d	O
this	O
would	O
provide	O
an	O
unbiased	O
estimate	B
of	O
err	O
cross-validation	B
and	O
cp	B
estimates	O
of	O
d	O
the	O
cross-validation	B
estimate	B
of	O
prediction	O
error	O
is	O
oy	O
i	O
d	O
rd	O
i	O
cerrcv	O
d	O
d	O
yi	O
oy	O
i	O
nx	O
n	O
now	O
yi	O
is	O
not	O
involved	O
in	O
the	O
construction	O
of	O
the	O
prediction	O
rule	B
for	O
yi	O
cerrcv	O
is	O
the	O
leave	B
one	I
out	I
version	O
of	O
cross-validation	B
a	O
more	O
tions	O
for	O
the	O
yi	O
in	O
group	O
j	O
thencerrcv	O
is	O
evaluated	O
as	O
in	O
besides	O
common	O
tactic	O
is	O
to	O
leave	O
out	O
several	O
pairs	O
at	O
a	O
time	O
d	O
is	O
randomly	O
partitioned	O
into	O
j	O
groups	O
of	O
size	O
about	O
nj	O
each	O
d	O
the	O
training	B
set	B
with	O
group	O
j	O
omitted	O
provides	O
rule	B
rd	O
j	O
which	O
is	O
used	O
to	O
provide	O
predic	O
reducing	O
the	O
number	O
of	O
rule	B
constructions	B
necessary	O
from	O
n	O
to	O
j	O
grouping	O
induces	O
larger	O
changes	O
among	O
the	O
j	O
training	O
sets	O
improving	O
the	O
predictive	O
performance	O
on	O
rules	O
rd	O
that	O
include	O
discontinuities	O
argument	B
here	O
is	O
similar	O
to	O
that	O
for	O
the	O
jackknife	B
section	O
cross-validation	B
was	O
applied	O
to	O
the	O
supernova	B
data	B
pictured	O
in	O
figure	O
the	O
cases	O
were	O
split	O
randomly	O
into	O
j	O
d	O
groups	O
of	O
three	O
cases	O
each	O
this	O
gave	O
cerrcv	O
d	O
larger	O
than	O
err	O
d	O
the	O
calculation	O
now	O
yields	O
the	O
smaller	O
value	O
d	O
d	O
we	O
can	O
apply	O
cross-validation	B
to	O
the	O
spam	B
data	B
of	O
section	O
having	O
n	O
d	O
cases	O
p	O
d	O
predictors	B
and	O
dichotomous	O
response	B
y	O
for	O
this	O
example	O
each	O
of	O
the	O
predictors	B
was	O
itself	O
dichotomized	O
to	O
be	O
either	O
or	O
depending	O
on	O
whether	O
the	O
original	O
value	O
xij	O
equaled	O
zero	O
or	O
not	O
a	O
logistic	B
regression	B
section	O
regressing	O
yi	O
on	O
the	O
dichotomized	O
predictors	B
gave	O
apparent	O
classification	O
error	O
i	O
e	O
wrong	O
predictions	O
among	O
the	O
cases	O
cross-validation	B
with	O
j	O
d	O
groups	O
of	O
size	O
or	O
each	O
increased	O
this	O
to	O
err	O
d	O
cerrcv	O
d	O
an	O
increase	O
of	O
glmnet	B
is	O
an	O
automatic	O
model	O
building	O
program	O
that	O
among	O
other	O
things	O
constructs	O
a	O
lasso	B
sequence	O
of	O
logistic	B
regression	B
models	B
adding	O
cross-validation	B
figure	O
spam	B
data	B
apparent	B
error	I
rate	B
err	O
and	O
cross-validated	O
estimate	B
for	O
a	O
sequence	O
of	O
prediction	O
rules	O
generated	O
by	O
glmnet	B
the	O
degrees	B
of	I
freedom	I
are	O
the	O
number	O
of	O
nonzero	O
regression	B
coefficients	O
df	O
d	O
corresponds	O
to	O
ordinary	O
logistic	B
regression	B
which	O
gave	O
apparent	O
err	O
cross-validated	O
rate	B
the	O
minimum	O
cross-validated	O
error	O
rate	B
is	O
variables	O
one	O
at	O
a	O
time	O
in	O
their	O
order	O
of	O
apparent	O
predictive	O
power	O
see	O
chapter	O
the	O
blue	O
curve	O
in	O
figure	O
tracks	O
the	O
apparent	B
error	I
err	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
predictors	B
employed	O
aside	O
from	O
numerical	O
artifacts	O
err	O
is	O
monotonically	O
decreasing	O
declining	O
to	O
err	O
d	O
glmnet	B
produced	O
prediction	O
error	O
estimatescerrcv	O
for	O
each	O
of	O
the	O
sucfor	O
the	O
full	B
model	O
that	O
employs	O
all	O
predictors	B
i	O
e	O
for	O
the	O
usual	O
logistic	B
regression	B
model	I
as	O
in	O
cessive	O
models	B
shown	O
by	O
the	O
red	O
curve	O
these	O
are	O
a	O
little	O
noisy	O
themselves	O
but	O
settle	O
down	O
between	O
and	O
above	O
the	O
corresponding	O
err	O
estimates	O
the	O
minimum	O
value	O
cerrcv	O
d	O
occurred	O
for	O
the	O
model	O
using	O
predictors	B
the	O
difference	O
between	O
and	O
is	O
too	O
small	O
to	O
take	O
seriously	O
given	O
the	O
noise	O
in	O
the	O
cerrcv	O
estimates	O
there	O
is	O
a	O
more	O
subtle	O
objection	O
the	O
choice	O
of	O
best	O
prediction	O
rule	B
based	O
on	O
comparative	O
cerrcv	O
estimates	O
is	O
not	O
itself	O
cross-validated	O
each	O
case	O
yi	O
is	O
involved	O
in	O
choosing	O
its	O
of	O
freedommisclassification	O
error	O
ratelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllogisticregression	O
own	O
best	O
prediction	O
socerrcv	O
at	O
the	O
apparently	O
optimum	O
choice	O
cannot	O
be	O
cross-validation	B
and	O
cp	B
estimates	O
taken	O
entirely	O
at	O
face	O
value	O
nevertheless	O
perhaps	O
the	O
principal	O
use	O
of	O
cross-validation	B
lies	O
in	O
choosing	O
among	O
competing	O
prediction	O
rules	O
whether	O
or	O
not	O
this	O
is	O
fully	O
justified	O
it	O
is	O
often	O
the	O
only	O
game	O
in	O
town	O
that	O
being	O
said	O
minimum	O
predictive	O
error	O
no	O
matter	O
how	O
effectuated	O
is	O
a	O
notably	O
weaker	O
selection	O
principle	O
than	O
minimum	O
variance	B
of	O
estimation	B
as	O
an	O
example	O
consider	O
an	O
iid	O
normal	B
sample	B
xi	O
having	O
mean	O
nx	O
and	O
median	O
mx	O
both	O
are	O
unbiased	O
for	O
estimating	O
but	O
nx	O
is	O
much	O
more	O
efficient	O
n	O
i	O
d	O
var	O
mx	O
var	O
nx	O
e	O
e	O
d	O
suppose	O
we	O
wish	O
to	O
predict	O
a	O
future	O
observation	O
independently	O
selected	O
from	O
the	O
same	O
n	O
distribution	B
in	O
this	O
case	O
there	O
is	O
very	O
little	O
advantage	O
to	O
nx	O
the	O
noise	O
in	O
n	O
dominates	O
its	O
prediction	O
error	O
perhaps	O
the	O
proliferation	O
of	O
prediction	O
algorithms	O
to	O
be	O
seen	O
in	O
part	O
iii	O
reflects	O
how	O
weakly	O
changes	O
in	O
strategy	O
affect	O
prediction	O
error	O
table	O
ratio	O
of	O
predictive	O
errors	B
for	O
the	O
mean	O
of	O
an	O
independent	O
sample	B
of	O
size	O
from	O
n	O
nx	O
and	O
mx	O
are	O
the	O
mean	O
and	O
median	O
from	O
xi	O
n	O
for	O
i	O
d	O
ratio	O
in	O
this	O
last	O
example	O
suppose	O
that	O
our	O
task	O
was	O
to	O
predict	O
the	O
average	O
of	O
further	O
draws	O
from	O
the	O
n	O
distribution	B
table	O
shows	O
the	O
ratio	O
of	O
predictive	O
errors	B
as	O
a	O
function	B
of	O
the	O
superiority	O
of	O
the	O
mean	O
compared	O
to	O
the	O
median	O
reveals	O
itself	O
as	O
gets	O
larger	O
in	O
this	O
supersimplified	O
example	O
the	O
difference	O
between	O
prediction	O
and	O
estimation	B
lies	O
in	O
predicting	O
the	O
average	O
of	O
one	O
versus	O
an	O
infinite	O
number	O
of	O
future	O
observations	O
doescerrcv	O
actually	O
estimate	B
errd	O
as	O
defined	O
in	O
it	O
seems	O
like	O
the	O
answer	O
must	O
be	O
yes	O
but	O
there	O
is	O
some	O
doubt	O
expressed	O
in	O
the	O
literature	O
for	O
cross-validation	B
reasons	O
demonstrated	O
in	O
the	O
following	O
simulation	B
we	O
take	O
the	O
true	O
distribution	B
f	O
in	O
to	O
be	O
the	O
discrete	O
distribution	B
of	O
that	O
puts	O
weight	O
on	O
each	O
of	O
the	O
yi	O
pairs	O
of	O
the	O
supernova	B
a	O
random	O
sample	B
with	O
replacement	O
of	O
size	O
from	O
of	O
gives	O
simulated	O
data	B
set	B
d	O
and	O
gives	O
cerr	O
based	O
on	O
the	O
lassocp	O
recipe	O
used	O
originally	O
the	O
same	O
diction	O
rule	B
rd	O
cross-validation	B
procedure	O
as	O
before	O
applied	O
to	O
d	O
cv	O
because	O
this	O
is	O
a	O
simulation	B
we	O
can	O
also	O
compute	O
the	O
actual	O
mean-squared	O
error	O
applied	O
to	O
the	O
true	O
distribution	B
of	O
rate	B
of	O
rule	B
rd	O
d	O
rd	O
err	O
d	O
with	O
cross-validation	B
estimatecerr	O
supernova	B
data	B
cerr	O
figure	O
simulation	B
experiment	O
comparing	O
true	O
error	O
err	O
cv	O
simulations	O
based	O
on	O
the	O
cv	O
and	O
err	O
are	O
negatively	O
correlated	O
figure	O
plots	O
ble	O
cerr	O
cv	O
for	O
simulations	O
using	O
squared	B
error	I
discrepancy	O
d	O
y	O
oy	O
d	O
summary	O
statistics	B
are	O
given	O
in	O
cv	O
has	O
performed	O
well	O
overall	O
averaging	B
quite	O
near	O
the	O
true	O
err	O
both	O
estimates	O
being	O
greater	O
than	O
the	O
average	O
apparent	B
error	I
however	O
the	O
figure	O
shows	O
something	O
unsettling	O
there	O
is	O
a	O
simulation	B
based	O
on	O
of	O
is	O
the	O
same	O
as	O
nonparametric	B
bootstrap	B
analysis	B
chapter	O
cross-validation	B
and	O
cp	B
estimates	O
cv	O
and	O
apparent	O
table	O
true	O
error	O
err	O
error	O
simulations	O
based	O
on	O
supernova	B
data	B
correlation	O
between	O
err	O
cross-validated	O
errorcerr	O
cerr	O
andcerr	O
cv	O
cv	O
mean	O
st	O
dev	O
negative	O
correlation	O
betweencerr	O
cv	O
and	O
err	O
large	O
values	O
ofcerr	O
cv	O
go	O
with	O
smaller	O
values	O
of	O
the	O
true	O
prediction	O
error	O
and	O
vice	O
versa	O
our	O
original	O
definition	O
of	O
err	O
errd	O
d	O
ef	O
rd	O
for	O
rd	O
ifcerr	O
took	O
rd	O
fixed	O
as	O
constructed	O
from	O
d	O
only	O
f	O
random	O
in	O
other	O
words	O
errd	O
was	O
the	O
expected	O
prediction	O
error	O
for	O
the	O
specific	O
rule	B
rd	O
as	O
as	O
it	O
is	O
all	O
we	O
can	O
say	O
is	O
thatcerr	O
we	O
would	O
expect	O
to	O
see	O
a	O
positive	O
is	O
err	O
correlation	O
in	O
figure	O
makes	O
cross-validation	B
a	O
strongly	B
frequentist	B
devicecerrcv	O
is	O
estimating	O
the	O
cv	O
is	O
estimating	O
the	O
expected	O
predictive	O
error	O
where	O
d	O
as	O
well	O
as	O
is	O
random	O
in	O
definition	O
this	O
average	O
prediction	O
error	O
of	O
the	O
algorithm	B
producing	O
rd	O
not	O
of	O
rd	O
itself	O
cv	O
is	O
tracking	O
err	O
covariance	O
penalties	O
cross-validation	B
does	O
its	O
work	O
nonparametrically	O
and	O
without	O
the	O
need	O
for	O
probabilistic	O
modeling	O
covariance	O
penalty	B
procedures	O
require	O
probability	O
models	B
but	O
within	O
their	O
ambit	O
they	O
provide	O
less	O
noisy	O
estimates	O
of	O
prediction	O
error	O
some	O
of	O
the	O
most	O
prominent	O
covariance	O
penalty	B
techniques	O
will	O
be	O
examined	O
here	O
including	O
mallows	O
cp	B
akaike	O
s	O
information	B
criterion	I
and	O
stein	O
s	O
unbiased	B
risk	I
estimate	B
the	O
covariance	O
penalty	B
approach	O
treats	O
prediction	O
error	O
estimation	B
in	O
a	O
regression	B
framework	O
the	O
predictor	B
vectors	O
xi	O
in	O
the	O
training	B
set	B
d	O
d	O
f	O
xi	O
yi	O
i	O
d	O
ng	O
are	O
considered	O
fixed	O
at	O
their	O
observed	O
values	O
not	O
random	O
as	O
in	O
an	O
unknown	O
vector	B
of	O
expectations	O
d	O
efyig	O
has	O
yielded	O
the	O
observed	O
vector	B
of	O
responses	O
y	O
according	O
to	O
some	O
given	O
probability	O
model	O
which	O
to	O
begin	O
with	O
we	O
assume	O
to	O
have	O
the	O
simple	O
form	B
covariance	O
penalties	O
y	O
that	O
is	O
the	O
yi	O
are	O
uncorrelated	O
with	O
yi	O
having	O
unknown	O
mean	O
and	O
variance	B
we	O
take	O
as	O
known	O
though	O
in	O
practice	O
it	O
must	O
usually	O
be	O
estimated	O
a	O
regression	B
rule	B
has	O
been	O
used	O
to	O
produce	O
an	O
estimate	B
of	O
vector	B
d	O
r	B
y	O
y	O
is	O
included	O
in	O
the	O
notation	O
since	O
the	O
predictors	B
xi	O
are	O
considered	O
fixed	O
and	O
known	O
for	O
instance	O
we	O
might	O
take	O
d	O
r	B
y	O
d	O
x	O
y	O
x	O
where	O
x	O
is	O
the	O
n	O
p	O
matrix	B
having	O
xi	O
as	O
the	O
ith	O
row	O
as	O
suggested	O
by	O
the	O
linear	B
regression	B
model	I
d	O
x	O
in	O
covariance	O
penalty	B
calculations	O
the	O
estimator	B
also	O
functions	O
as	O
a	O
predictor	B
we	O
wonder	O
how	O
accurate	O
d	O
r	B
y	O
will	O
be	O
in	O
predicting	O
a	O
new	O
vector	B
of	O
observations	O
from	O
model	O
independent	O
of	O
y	O
to	O
begin	O
with	O
prediction	O
error	O
will	O
be	O
assessed	O
in	O
terms	O
of	O
squared	O
discrepancy	O
for	O
component	O
i	O
where	O
indicates	O
expectation	O
with	O
random	O
but	O
held	O
fixed	O
overall	O
prediction	O
error	O
is	O
the	O
erri	O
d	O
nx	O
erri	O
d	O
the	O
apparent	B
error	I
for	O
component	O
i	O
is	O
n	O
erri	O
d	O
a	O
simple	O
but	O
powerful	O
lemma	O
underlies	O
the	O
theory	B
of	O
covariance	O
penalties	O
lemma	O
let	O
e	O
indicate	O
expectation	O
over	O
both	O
y	O
in	O
and	O
in	O
then	O
eferrig	O
d	O
eferrig	O
c	O
cov	O
yi	O
is	O
sometimes	O
called	O
insample	B
error	I
as	O
opposed	O
to	O
outsample	B
error	I
err	O
though	O
in	O
practice	O
the	O
two	O
tend	O
to	O
behave	O
similarly	O
cross-validation	B
and	O
cp	B
estimates	O
where	O
the	O
last	O
term	O
is	O
the	O
covariance	O
between	O
the	O
ith	O
components	O
of	O
and	O
y	O
cov	O
yi	O
d	O
e	O
f	O
i	O
i	O
c	O
does	O
not	O
require	O
ef	O
d	O
proof	O
letting	O
d	O
yi	O
and	O
i	O
d	O
the	O
elementary	O
equality	O
i	O
d	O
d	O
c	O
and	O
likewise	O
d	O
taking	O
expectations	O
gives	O
i	O
becomes	O
eferrig	O
d	O
cov	O
yi	O
c	O
e	O
while	O
gives	O
eferrig	O
d	O
c	O
e	O
the	O
middle	O
term	O
on	O
the	O
right	O
side	O
of	O
equaling	O
zero	O
because	O
of	O
the	O
independence	O
of	O
and	O
taking	O
the	O
difference	O
between	O
and	O
verifies	O
the	O
lemma	O
note	O
the	O
lemma	O
remains	O
valid	O
if	O
varies	O
with	O
i	O
the	O
lemma	O
says	O
that	O
on	O
average	O
the	O
apparent	B
error	I
erri	O
understimates	O
the	O
true	O
prediction	O
error	O
erri	O
by	O
the	O
covariance	O
penalty	B
cov	O
yi	O
makes	O
intuitive	O
sense	O
since	O
yi	O
measures	O
the	O
amount	O
by	O
which	O
yi	O
influences	O
its	O
own	O
prediction	O
covariance	O
penalty	B
estimates	O
of	O
prediction	O
error	O
take	O
the	O
formcerri	O
d	O
erri	O
yi	O
wheredcov	O
yi	O
approximates	O
yi	O
overall	O
prediction	O
error	O
d	O
errc	O
dcov	O
yi	O
where	O
err	O
dp	O
erri	O
as	O
before	O
the	O
form	B
ofdcov	O
yi	O
in	O
depends	O
on	O
the	O
context	O
assumed	O
for	O
is	O
estimated	O
by	O
nx	O
n	O
the	O
prediction	O
problem	O
covariance	O
penalties	O
suppose	O
that	O
d	O
r	B
y	O
in	O
is	O
linear	B
where	O
c	O
is	O
a	O
known	O
n	O
and	O
m	O
a	O
known	O
n	O
n	O
matrix	B
then	O
the	O
covariance	O
matrix	B
between	O
and	O
y	O
is	O
d	O
c	O
c	O
my	O
giving	O
cov	O
yi	O
d	O
i	O
mi	O
i	O
the	O
ith	O
diagonal	O
element	O
of	O
m	O
and	O
since	O
err	O
dp	O
cov	O
y	O
d	O
cerri	O
d	O
erri	O
i	O
i	O
nx	O
d	O
c	O
tr	O
m	O
formula	B
is	O
mallows	O
cp	B
estimate	B
of	O
prediction	O
error	O
for	O
ols	O
has	O
tr	O
m	O
d	O
p	O
the	O
number	O
of	O
x	O
n	O
n	O
estimation	B
m	O
d	O
x	O
nx	O
d	O
predictors	B
so	O
c	O
n	O
y	O
yielded	O
err	O
dp	O
yi	O
d	O
the	O
covariance	O
penalty	B
with	O
for	O
the	O
supernova	B
data	B
the	O
ols	O
predictor	B
d	O
x	O
n	O
d	O
d	O
p	O
d	O
was	O
giving	O
cp	B
estimate	B
of	O
prediction	O
error	O
x	O
x	O
n	O
d	O
c	O
d	O
for	O
ols	O
regression	B
the	O
degrees	B
of	I
freedom	I
p	O
the	O
rank	O
of	O
matrix	B
x	O
in	O
determines	O
the	O
covariance	O
penalty	B
in	O
comparing	O
this	O
with	O
leads	O
to	O
a	O
general	O
definition	O
of	O
degrees	B
of	I
freedom	I
df	O
for	O
a	O
regression	B
rule	B
d	O
r	B
y	O
df	O
d	O
nx	O
dcov	O
yi	O
this	O
definition	O
provides	O
common	O
ground	O
for	O
comparing	O
different	O
types	O
of	O
regression	B
rules	O
rules	O
with	O
larger	O
df	O
are	O
more	O
flexible	O
and	O
tend	O
toward	O
better	O
apparent	O
fits	O
to	O
the	O
data	B
but	O
require	O
bigger	O
covariance	O
penalties	O
for	O
fair	O
comparison	O
we	O
are	O
not	O
counting	O
the	O
intercept	O
as	O
an	O
predictor	B
since	O
y	O
and	O
all	O
the	O
xi	O
were	O
standardized	O
to	O
have	O
mean	O
all	O
our	O
models	B
assuming	O
zero	O
intercept	O
cross-validation	B
and	O
cp	B
estimates	O
for	O
lasso	B
estimation	B
and	O
it	O
can	O
be	O
shown	O
that	O
formula	B
with	O
p	O
equaling	O
the	O
number	O
of	O
nonzero	O
regression	B
coefficients	O
holds	O
to	O
a	O
good	O
approximation	O
the	O
lasso	B
rule	B
used	O
in	O
figure	O
for	O
the	O
supernova	B
data	B
had	O
p	O
d	O
err	O
was	O
for	O
this	O
rule	B
almost	O
the	O
same	O
as	O
for	O
the	O
ols	O
rule	B
above	O
but	O
the	O
cp	B
penalty	B
is	O
less	O
d	O
giving	O
d	O
c	O
d	O
compared	O
with	O
for	O
ols	O
this	O
estimate	B
does	O
not	O
account	O
for	O
the	O
databased	O
selection	O
of	O
the	O
choice	O
p	O
d	O
see	O
item	O
below	O
if	O
we	O
are	O
willing	O
to	O
add	O
multivariate	B
normality	O
to	O
model	O
y	O
we	O
can	O
drop	O
the	O
assumption	O
of	O
linearity	O
in	O
this	O
case	O
it	O
can	O
be	O
shown	O
that	O
for	O
any	O
differentiable	O
estimator	B
d	O
r	B
y	O
the	O
covariance	O
in	O
formula	B
is	O
given	O
by	O
cov	O
yi	O
d	O
times	O
the	O
partial	O
derivative	O
of	O
with	O
respect	O
to	O
yi	O
measure	O
of	O
yi	O
s	O
influence	O
on	O
its	O
own	O
prediction	O
the	O
sure	O
formula	B
s	O
unbiased	O
risk	O
estimator	B
is	O
with	O
corresponding	O
estimate	B
for	O
overall	O
prediction	O
error	O
cerri	O
d	O
erri	O
nx	O
d	O
errc	O
sure	O
was	O
applied	O
to	O
the	O
rule	B
d	O
for	O
the	O
kidney	O
fitness	O
data	B
of	O
figure	O
the	O
open	O
circles	O
in	O
figure	O
plot	O
the	O
component-wise	O
degrees	B
of	I
freedom	I
n	O
by	O
numerical	O
differentiation	O
versus	O
agei	O
their	O
sum	O
i	O
d	O
n	O
d	O
nx	O
d	O
notice	O
that	O
the	O
factor	B
in	O
cancels	O
out	O
in	O
covariance	O
penalties	O
estimates	O
the	O
total	O
degrees	B
of	I
freedom	I
as	O
in	O
implying	O
that	O
is	O
about	O
as	O
flexible	O
as	O
a	O
sixth-degree	O
polynomial	O
fit	O
with	O
df	O
figure	O
analysis	B
of	O
the	O
fit	O
to	O
kidney	O
data	B
of	O
figure	O
open	O
circles	O
are	O
sure	O
coordinate-wise	O
df	O
estimates	O
plotted	O
versus	O
agei	O
giving	O
total	O
degrees	B
of	I
freedom	I
the	O
solid	O
curve	O
tracks	O
bootstrap	B
coordinate-wise	O
estimates	O
with	O
their	O
sum	O
giving	O
total	O
df	O
d	O
the	O
parametric	B
of	O
section	O
can	O
be	O
used	O
to	O
estimate	B
the	O
covariances	O
cov	O
yi	O
in	O
the	O
lemma	O
the	O
data	B
vector	B
y	O
is	O
assumed	O
to	O
be	O
generated	O
from	O
a	O
member	O
of	O
a	O
given	O
parametric	B
family	I
d	O
f	O
yielding	O
d	O
r	B
y	O
parametric	B
bootstrap	B
replications	O
of	O
y	O
and	O
are	O
obtained	O
by	O
analogy	O
with	O
y	O
d	O
r	B
y	O
there	O
is	O
also	O
a	O
nonparametric	B
bootstrap	B
competitor	O
to	O
cross-validation	B
the	O
estimate	B
see	O
the	O
chapter	O
endnote	O
estimatedegrees	O
of	O
freedomsure	O
cross-validation	B
and	O
cp	B
estimates	O
a	O
large	O
number	O
b	O
of	O
replications	O
then	O
yield	O
bootstrap	B
estimates	O
f	O
y	O
bx	O
dcov	O
yi	O
d	O
i	O
b	O
d	O
r	B
y	O
i	O
i	O
y	O
i	O
b	O
d	O
parametric	B
bootstrap	B
replications	O
the	O
dot	O
notation	O
indicating	O
averages	O
over	O
the	O
b	O
replications	O
were	O
obtained	O
from	O
the	O
normal	B
model	O
taking	O
in	O
to	O
be	O
the	O
estimate	B
from	O
as	O
in	O
figure	O
a	O
standard	O
linear	B
regression	B
of	O
y	O
as	O
a	O
polynomial	O
function	B
of	O
age	O
gave	O
d	O
covariances	O
were	O
computed	O
as	O
in	O
yielding	O
coordinate-wise	O
degrees	B
of	I
freedom	I
estimates	O
y	O
dfi	O
ddcov	O
yi	O
the	O
solid	O
curve	O
in	O
figure	O
plots	O
dfi	O
as	O
a	O
function	B
of	O
agei	O
these	O
are	O
seen	O
to	O
be	O
similar	O
to	O
but	O
less	O
noisy	O
than	O
the	O
sure	O
estimates	O
they	O
totaled	O
nearly	O
the	O
same	O
as	O
the	O
overall	O
covariance	O
penalty	B
term	O
in	O
equaled	O
by	O
about	O
over	O
err	O
d	O
the	O
advantage	O
of	O
parametric	B
bootstrap	B
estimates	O
of	O
covariance	O
penalties	O
is	O
their	O
applicability	O
to	O
any	O
prediction	O
rule	B
d	O
r	B
y	O
no	O
matter	O
how	O
exotic	O
applied	O
to	O
the	O
lasso	B
estimates	O
for	O
the	O
supernova	B
data	B
b	O
d	O
replications	O
yielded	O
total	O
df	O
d	O
for	O
the	O
rule	B
that	O
always	O
used	O
p	O
d	O
predictors	B
compared	O
with	O
the	O
theoretical	O
approximation	O
df	O
d	O
another	O
replications	O
now	O
letting	O
choose	O
the	O
apparently	O
each	O
time	O
increased	O
the	O
df	O
estimate	B
to	O
so	O
the	O
adaptive	B
choice	O
best	O
p	O
of	O
p	O
cost	O
about	O
extra	O
degrees	B
of	I
freedom	I
these	O
calculations	O
exemplify	O
modern	O
computer-intensive	B
inference	B
carrying	O
through	O
error	O
estimation	B
for	O
complicated	O
adaptive	B
prediction	O
rules	O
on	O
a	O
totally	O
automatic	O
basis	O
covariance	O
penalties	O
can	O
apply	O
to	O
measures	O
of	O
prediction	O
error	O
other	O
than	O
squared	B
error	I
d	O
yi	O
d	O
we	O
will	O
discuss	O
two	O
examples	O
of	O
a	O
general	O
theory	B
first	O
consider	O
classification	O
where	O
yi	O
equals	O
or	O
and	O
it	O
isn	O
t	B
necessary	O
for	O
the	O
in	O
to	O
equal	O
d	O
r	B
y	O
the	O
calculation	O
was	O
rerun	O
taking	O
in	O
from	O
with	O
r	B
y	O
still	O
from	O
with	O
almost	O
identical	O
results	O
in	O
general	O
one	O
might	O
take	O
in	O
to	O
be	O
from	O
a	O
more	O
flexible	O
less	O
biased	O
estimator	B
than	O
r	B
y	O
d	O
r	B
y	O
covariance	O
penalties	O
similarly	O
the	O
predictor	B
with	O
dichotomous	O
error	O
if	O
yi	O
if	O
yi	O
d	O
d	O
yi	O
d	O
as	O
in	O
in	O
this	O
situation	O
the	O
apparent	B
error	I
is	O
the	O
observed	O
proportion	B
of	O
prediction	O
mistakes	O
in	O
the	O
training	B
set	B
err	O
d	O
now	O
the	O
true	O
prediction	O
error	O
for	O
case	O
i	O
is	O
erri	O
d	O
the	O
conditional	B
probability	O
given	O
that	O
an	O
independent	O
replicate	O
of	O
yi	O
will	O
be	O
incorrectly	O
predicted	O
the	O
lemma	O
holds	O
as	O
stated	O
in	O
leading	O
to	O
the	O
prediction	O
error	O
estimate	B
d	O
n	O
c	O
n	O
nx	O
cov	O
yi	O
some	O
algebra	O
yields	O
cov	O
yi	O
d	O
d	O
d	O
prf	O
d	O
d	O
with	O
d	O
prfyi	O
d	O
showing	O
again	O
the	O
covariance	O
penalty	B
measuring	O
the	O
self-influence	O
of	O
yi	O
on	O
its	O
own	O
prediction	O
as	O
a	O
second	O
example	O
suppose	O
that	O
the	O
observations	O
yi	O
are	O
obtained	O
from	O
different	O
members	O
of	O
a	O
one-parameter	B
exponential	B
family	I
d	O
yi	O
z	O
fyi	O
fy	O
y	O
according	O
to	O
the	O
apparent	O
errorp	O
d	O
yi	O
is	O
then	O
for	O
i	O
d	O
n	O
d	O
y	O
d	O
nx	O
and	O
that	O
error	O
is	O
measured	O
by	O
the	O
deviance	B
y	O
d	O
err	O
d	O
fy	O
y	O
log	O
f	O
d	O
y	O
log	O
n	O
f	O
n	O
more	O
generally	O
is	O
some	O
predictor	B
of	O
prfyi	O
d	O
and	O
is	O
the	O
indicator	O
function	B
i	O
cross-validation	B
and	O
cp	B
estimates	O
in	O
this	O
case	O
the	O
general	O
theory	B
gives	O
overall	O
covariance	O
penalty	B
nx	O
penalty	B
d	O
n	O
cov	O
where	O
o	O
is	O
the	O
natural	B
parameter	I
in	O
family	O
corresponding	O
to	O
o	O
d	O
log	O
for	O
poisson	B
observations	O
moreover	O
if	O
is	O
obtained	O
as	O
the	O
mle	B
of	O
in	O
a	O
generalized	O
linear	B
model	I
with	O
p	O
degrees	B
of	I
freedom	I
penalty	B
n	O
c	O
constant	O
to	O
a	O
good	O
approximation	O
the	O
corresponding	O
version	O
of	O
can	O
then	O
be	O
written	O
the	O
constant	O
log	O
fy	O
not	O
depending	O
on	O
the	O
term	O
in	O
brackets	O
is	O
the	O
akaike	B
information	B
criterion	I
if	O
the	O
statistician	O
is	O
comparing	O
possible	O
prediction	O
rules	O
r	B
for	O
a	O
given	O
data	B
set	B
y	O
the	O
aic	O
says	O
to	O
select	O
the	O
rule	B
maximizing	O
the	O
penalized	O
maximum	B
likelihood	B
n	O
p	O
j	O
where	O
is	O
rule	B
j	O
s	O
mle	B
and	O
p	O
j	O
its	O
degrees	B
of	I
freedom	I
comparison	O
with	O
the	O
smallest	O
value	O
ofcerr	O
j	O
with	O
shows	O
that	O
for	O
glms	O
the	O
aic	O
amounts	O
to	O
selecting	O
the	O
rule	B
is	O
available	O
then	O
the	O
error	O
estimate	B
cerrcv	O
can	O
be	O
improved	O
by	O
bootstrap	B
with	O
the	O
predictor	B
vectors	O
xi	O
considered	O
fixed	O
as	O
observed	O
a	O
parametric	B
model	O
generates	O
the	O
data	B
set	B
d	O
d	O
f	O
xi	O
yi	O
i	O
d	O
ng	O
as	O
estimatecerrcv	O
in	O
from	O
which	O
we	O
calculate	O
the	O
prediction	O
rule	B
rd	O
and	O
the	O
error	O
cross-validation	B
does	O
not	O
require	O
a	O
probability	O
model	O
but	O
if	O
such	O
a	O
model	O
d	O
rd	O
substituting	O
the	O
estimated	O
density	B
f	O
for	O
as	O
in	O
provides	O
perhaps	O
better	O
known	O
as	O
bagging	B
see	O
chapter	O
training	O
validation	O
and	O
ephemeral	B
predictors	B
parametric	B
bootstrap	B
replicates	O
ofcerrcv	O
rd	O
bx	O
cerr	O
f	O
d	O
err	O
d	O
cv	O
cv	O
some	O
large	O
number	O
b	O
of	O
replications	O
can	O
then	O
be	O
averaged	O
to	O
give	O
the	O
smoothed	O
estimate	B
b	O
err	O
averages	O
out	O
the	O
considerable	O
noise	O
incerrcv	O
often	O
significantly	O
reducleft	O
after	O
excess	O
randomness	O
is	O
squeezed	O
out	O
ofcerrcv	O
example	O
of	O
rao	O
a	O
surprising	O
result	O
referenced	O
in	O
the	O
endnotes	O
shows	O
that	O
err	O
approximates	O
the	O
covariance	O
penalty	B
estimate	B
speaking	O
broadly	O
is	O
what	O
s	O
ing	O
its	O
blackwellization	O
to	O
use	O
classical	O
terminology	O
improvements	O
can	O
be	O
quite	O
substantial	O
covariance	O
penalty	B
estimates	O
when	O
believable	O
parametric	B
models	B
are	O
available	O
should	O
be	O
preferred	O
to	O
cross-validation	B
training	O
validation	O
and	O
ephemeral	B
predictors	B
good	O
practice	O
suggests	O
splitting	O
the	O
full	B
set	B
of	O
observed	O
predictor	B
response	B
pairs	O
y	O
into	O
a	O
training	B
set	B
d	O
of	O
size	O
n	O
and	O
a	O
validation	B
set	B
dval	O
of	O
size	O
nval	O
the	O
validation	B
set	B
is	O
put	O
into	O
a	O
vault	O
while	O
the	O
training	B
set	B
is	O
used	O
to	O
develop	O
an	O
effective	O
prediction	O
rule	B
rd	O
finally	O
dval	O
is	O
removed	O
from	O
the	O
vault	O
and	O
used	O
to	O
calculatecerrval	O
an	O
honest	O
estimate	B
of	O
the	O
predictive	O
error	O
rate	B
of	O
rd	O
this	O
is	O
a	O
good	O
idea	O
and	O
seems	O
foolproof	O
at	O
least	O
if	O
one	O
has	O
enough	O
data	B
to	O
afford	O
setting	O
aside	O
a	O
substantial	O
portion	O
for	O
a	O
validation	B
set	B
during	O
the	O
training	O
process	O
nevertheless	O
there	O
remains	O
some	O
peril	O
of	O
underestimating	O
the	O
true	B
error	I
rate	B
arising	O
from	O
ephemeral	B
predictors	B
those	O
whose	O
predictive	O
powers	O
fade	O
away	O
over	O
time	O
a	O
contrived	O
but	O
not	O
completely	O
fanciful	O
example	O
illustrates	O
the	O
danger	O
the	O
example	O
takes	O
the	O
form	B
of	O
an	O
imaginary	O
microarray	O
study	O
involving	O
subjects	O
patients	O
and	O
healthy	O
controls	O
coded	O
i	O
d	O
yi	O
d	O
patient	O
control	B
a	O
related	O
tactic	O
pertaining	O
to	O
grouped	O
cross-validation	B
is	O
to	O
repeat	O
calculation	O
for	O
several	O
different	O
randomly	O
selected	O
splits	O
into	O
j	O
groups	O
and	O
then	O
average	O
the	O
resulting	O
cerrcv	O
estimates	O
cross-validation	B
and	O
cp	B
estimates	O
each	O
subject	O
is	O
assessed	O
on	O
a	O
microarray	O
measuring	O
the	O
genetic	O
activity	O
of	O
p	O
d	O
genes	O
these	O
being	O
the	O
predictors	B
xi	O
d	O
xi	O
xi	O
one	O
subject	O
per	O
day	O
is	O
assessed	O
alternating	O
patients	O
and	O
controls	O
figure	O
orange	O
bars	O
indicate	O
transient	B
episodes	I
and	O
the	O
reverse	O
for	O
imaginary	O
medical	O
study	O
xij	O
the	O
measurements	O
xij	O
are	O
independent	O
of	O
each	O
other	O
and	O
of	O
the	O
yi	O
s	O
for	O
i	O
d	O
and	O
j	O
d	O
most	O
of	O
the	O
equal	O
zero	O
but	O
each	O
gene	O
s	O
measurements	O
can	O
experience	O
transient	B
episodes	I
of	O
two	O
possible	O
types	O
in	O
type	O
n	O
if	O
yi	O
d	O
if	O
yi	O
d	O
d	O
while	O
type	O
reverses	O
signs	O
the	O
episodes	O
are	O
about	O
days	O
long	O
randomly	O
and	O
independently	O
located	O
between	O
days	O
and	O
with	O
an	O
average	O
of	O
two	O
episodes	O
per	O
gene	O
the	O
orange	O
bars	O
in	O
figure	O
indicate	O
the	O
episodes	O
for	O
the	O
purpose	O
of	O
future	O
diagnoses	O
we	O
wish	O
to	O
construct	O
a	O
prediction	O
rule	B
oy	O
d	O
rd	O
to	O
this	O
end	O
we	O
randomly	O
divide	O
the	O
subjects	O
into	O
a	O
ephemeral	B
predictors	B
training	B
set	B
d	O
of	O
size	O
n	O
d	O
and	O
a	O
validation	B
set	B
dval	O
of	O
size	O
nval	O
d	O
the	O
popular	O
machine	B
learning	I
prediction	O
program	O
random	O
forests	O
chapter	O
is	O
applied	O
random	O
forests	O
forms	O
rd	O
by	O
averaging	B
the	O
predictions	O
of	O
a	O
large	O
number	O
of	O
randomly	O
subsampled	O
regression	B
trees	B
figure	O
test	O
error	O
and	O
cross-validated	O
training	O
error	O
for	O
random	B
forest	I
prediction	O
rules	O
using	O
the	O
imaginary	O
medical	O
study	O
top	O
panel	O
training	B
set	B
randomly	O
selected	O
days	O
test	O
set	B
the	O
remaining	O
days	O
bottom	O
panel	O
training	B
set	B
the	O
first	O
days	O
test	O
set	B
the	O
last	O
days	O
the	O
top	O
panel	O
of	O
figure	O
shows	O
the	O
results	O
with	O
blue	O
points	O
indicating	O
test-set	O
error	O
and	O
black	O
the	O
training-set	O
error	O
both	O
converge	O
to	O
as	O
the	O
number	O
of	O
random	B
forest	I
trees	B
grows	O
large	O
this	O
seems	O
to	O
confirm	O
an	O
success	O
rate	B
for	O
prediction	O
rule	B
rd	O
one	O
change	O
has	O
been	O
made	O
for	O
the	O
bottom	O
panel	O
now	O
the	O
training	B
set	B
is	O
the	O
data	B
for	O
days	O
through	O
and	O
the	O
test	O
set	B
days	O
through	O
set	B
random	O
days	O
test	O
set	B
the	O
remainder	O
treesprediction	O
days	O
test	O
days	O
treesprediction	O
cross-validation	B
and	O
cp	B
estimates	O
cerrval	O
is	O
now	O
nearly	O
double	O
the	O
cross-validated	O
training-set	O
prediction	O
error	O
still	O
converges	O
to	O
but	O
the	O
reason	O
isn	O
t	B
hard	O
to	O
see	O
any	O
predictive	O
power	O
must	O
come	O
from	O
the	O
transient	B
episodes	I
which	O
lose	O
efficacy	O
outside	O
of	O
their	O
limited	O
span	O
in	O
the	O
first	O
example	O
the	O
test	O
days	O
are	O
located	O
among	O
the	O
training	O
days	O
and	O
inherit	O
their	O
predictive	O
accuracy	B
from	O
them	O
this	O
mostly	O
fails	O
in	O
the	O
second	O
setup	O
where	O
the	O
test	O
days	O
are	O
farther	O
removed	O
from	O
the	O
training	O
days	O
the	O
orange	O
bars	O
crossing	O
the	O
line	O
can	O
help	O
lower	O
cerrval	O
in	O
this	O
situaan	O
obvious	O
but	O
often	O
ignored	O
dictum	O
is	O
thatcerrval	O
is	O
more	O
believable	O
if	O
tion	O
the	O
test	O
set	B
is	O
further	O
separated	O
from	O
the	O
training	B
set	B
further	O
has	O
a	O
clear	O
meaning	O
in	O
studies	O
with	O
a	O
time	O
or	O
location	O
factor	B
but	O
not	O
necessarily	O
in	O
general	O
for	O
j	O
cross-validation	B
separation	O
is	O
improved	O
by	O
removing	O
contiguous	O
blocks	O
of	O
nj	O
cases	O
for	O
each	O
group	O
rather	O
than	O
by	O
random	O
selection	O
but	O
the	O
amount	O
of	O
separation	O
is	O
still	O
limited	O
making	O
cerrcv	O
less	O
believable	O
than	O
a	O
suitably	O
constructedcerrval	O
the	O
distinction	O
between	O
transient	O
ephemeral	B
predictors	B
and	O
dependable	O
ones	O
is	O
sometimes	O
phrased	O
as	O
the	O
difference	O
between	O
correlation	O
and	O
causation	O
for	O
prediction	O
purposes	O
if	O
not	O
for	O
scientific	O
exegesis	O
we	O
may	O
be	O
happy	O
to	O
settle	O
for	O
correlations	O
as	O
long	O
as	O
they	O
are	O
persistent	O
enough	O
for	O
our	O
purposes	O
we	O
return	O
to	O
this	O
question	O
in	O
chapter	O
in	O
the	O
discussion	O
of	O
large-scale	O
hypothesis	B
testing	B
a	O
notorious	O
cautionary	O
tale	O
of	O
fading	O
correlations	O
concerns	O
google	O
flu	O
trends	O
a	O
machine-learning	O
algorithm	B
for	O
predicting	O
influenza	O
outbreaks	O
introduced	O
in	O
the	O
algorithm	B
based	O
on	O
counts	O
of	O
internet	O
search	O
terms	O
outperformed	O
traditional	O
medical	O
surveys	O
in	O
terms	O
of	O
speed	O
and	O
predictive	O
accuracy	B
four	O
years	O
later	O
however	O
the	O
algorithm	B
failed	O
badly	O
overestimating	O
what	O
turned	O
out	O
to	O
be	O
a	O
nonexistent	O
flu	O
epidemic	O
perhaps	O
one	O
lesson	O
here	O
is	O
that	O
the	O
google	O
algorithmists	O
needed	O
a	O
validation	B
set	B
years	O
not	O
weeks	O
or	O
months	O
removed	O
from	O
the	O
training	O
data	B
error	O
rate	B
estimation	B
is	O
mainly	O
frequentist	B
in	O
nature	O
but	O
the	O
very	O
large	O
data	B
sets	O
available	O
from	O
the	O
internet	O
have	O
encouraged	O
a	O
disregard	O
for	O
inferential	O
justification	O
of	O
any	O
type	O
this	O
can	O
be	O
dangerous	O
the	O
heterogeneous	O
nature	O
of	O
found	O
data	B
makes	O
statistical	O
principles	O
of	O
analysis	B
more	O
not	O
less	O
relevant	O
notes	O
and	O
details	O
the	O
evolution	O
of	O
prediction	O
algorithms	O
and	O
their	O
error	O
estimates	O
nicely	O
illustrates	O
the	O
influence	O
of	O
electronic	O
computation	O
on	O
statistical	O
theory	B
and	O
notes	O
and	O
details	O
practice	O
the	O
classical	O
recipe	O
for	O
cross-validation	B
recommended	O
splitting	O
the	O
full	B
data	B
set	B
in	O
two	O
doing	O
variable	O
selection	O
model	O
choice	O
and	O
data	B
fitting	O
on	O
the	O
first	O
half	O
and	O
then	O
testing	B
the	O
resulting	O
procedure	O
on	O
the	O
second	O
half	O
interest	O
revived	O
in	O
with	O
the	O
independent	O
publication	O
of	O
papers	O
by	O
geisser	O
and	O
by	O
stone	O
featuring	O
leave-one-out	O
cross-validation	B
of	O
predictive	O
error	O
rates	O
a	O
question	O
of	O
bias	B
versus	O
variance	B
arises	O
here	O
a	O
rule	B
based	O
on	O
only	O
cases	O
is	O
less	O
accurate	O
than	O
the	O
actual	O
rule	B
based	O
on	O
all	O
n	O
leave-one-out	O
cross-validation	B
minimizes	O
this	O
type	O
of	O
bias	B
at	O
the	O
expense	O
of	O
increased	O
variability	O
of	O
error	O
rate	B
estimates	O
for	O
jumpy	O
rules	O
of	O
a	O
discontinuous	O
nature	O
current	O
best	O
practice	O
is	O
described	O
in	O
section	O
of	O
hastie	O
et	O
al	O
where	O
j	O
cross-validation	B
with	O
j	O
perhaps	O
is	O
recommended	O
possibly	O
averaged	O
over	O
several	O
random	O
data	B
splits	O
nineteen	O
seventy-three	O
was	O
another	O
good	O
year	O
for	O
error	O
estimation	B
featuring	O
mallows	O
cp	B
estimator	B
and	O
akaike	O
s	O
information	B
criterion	I
efron	O
extended	O
cp	B
methods	O
to	O
a	O
general	O
class	O
of	O
situations	O
below	O
established	O
the	O
connection	O
with	O
aic	O
and	O
suggested	O
bootstrapping	O
methods	O
for	O
covariance	O
penalties	O
the	O
connection	O
between	O
cross-validation	B
and	O
covariance	O
penalties	O
was	O
examined	O
in	O
efron	O
where	O
the	O
rao	O
blackwelltype	O
relationship	O
mentioned	O
at	O
the	O
end	O
of	O
section	O
was	O
demonstrated	O
the	O
sure	O
criterion	O
appeared	O
in	O
charles	O
stein	O
s	O
paper	O
ye	O
suggested	O
the	O
general	O
degrees	B
of	I
freedom	I
definition	O
standard	B
candles	I
and	O
dark	B
energy	I
adam	O
riess	O
saul	O
perlmutter	O
and	O
brian	O
schmidt	O
won	O
the	O
nobel	O
prize	O
in	O
physics	O
for	O
discovering	O
increasing	O
rates	O
of	O
expansion	O
of	O
the	O
universe	O
attributed	O
to	O
an	O
einsteinian	O
concept	O
of	O
dark	B
energy	I
they	O
measured	O
cosmic	O
distances	O
using	O
type	O
ia	O
supernovas	O
as	O
standard	B
candles	I
the	O
type	O
of	O
analysis	B
suggested	O
by	O
figure	O
is	O
intended	O
to	O
improve	O
the	O
cosmological	O
distance	O
scale	B
data-based	O
choice	O
of	O
a	O
lasso	B
estimate	B
the	O
regularization	B
parameter	O
for	O
a	O
lasso	B
estimator	B
controls	O
the	O
number	O
of	O
nonzero	O
coefficients	O
of	O
q	O
with	O
larger	O
yielding	O
fewer	O
nonzeros	O
efron	O
et	O
al	O
cients	O
substituting	O
this	O
for	O
p	O
in	O
provides	O
a	O
quick	O
version	O
of	O
and	O
zou	O
et	O
al	O
showed	O
that	O
a	O
good	O
approximation	O
for	O
the	O
degrees	B
of	I
freedom	I
df	O
of	O
a	O
lasso	B
estimate	B
is	O
the	O
number	O
of	O
its	O
nonzero	O
coeffithis	O
was	O
minimized	O
at	O
df	O
d	O
for	O
the	O
supernova	B
example	O
in	O
figure	O
stein	O
s	O
unbiased	B
risk	I
estimate	B
the	O
covariance	O
formula	B
is	O
obtained	O
directly	O
from	O
integration	O
by	O
parts	O
the	O
computation	O
is	O
clear	O
from	O
the	O
one-dimensional	O
version	O
of	O
n	O
d	O
cross-validation	B
and	O
cp	B
estimates	O
cov	O
y	O
dz	O
z	O
d	O
d	O
e	O
dy	O
dy	O
e	O
d	O
r	B
broad	O
regularity	O
conditions	B
for	O
sure	O
are	O
given	O
in	O
stein	O
the	O
rule	B
bootstrap	B
competitors	O
to	O
cross-validation	B
are	O
discussed	O
in	O
efron	O
and	O
efron	O
and	O
tibshirani	O
the	O
most	O
successful	O
of	O
these	O
the	O
rule	B
is	O
generally	O
less	O
variable	O
than	O
leave-one-out	O
cross-validation	B
we	O
suppose	O
that	O
nonparametric	B
bootstrap	B
data	B
sets	O
d	O
b	O
d	O
b	O
have	O
been	O
formed	O
each	O
by	O
sampling	O
with	O
replacement	O
produces	O
n	O
times	O
from	O
the	O
original	O
n	O
members	O
of	O
d	O
data	B
set	B
d	O
rule	B
d	O
rd	O
giving	O
predictions	O
r	B
y	O
let	O
i	O
b	O
i	O
d	O
if	O
pair	O
yi	O
is	O
not	O
in	O
d	O
d	O
i	O
will	O
equal	O
the	O
remaining	O
equaling	O
the	O
and	O
if	O
it	O
is	O
e	O
of	O
the	O
n	O
b	O
i	O
b	O
out	B
of	I
bootstrap	B
estimate	B
of	O
prediction	O
error	O
is	O
i	O
cerrout	O
d	O
nx	O
bx	O
i	O
b	O
i	O
d	O
nx	O
bx	O
yi	O
oy	O
i	O
i	O
b	O
i	O
the	O
average	O
discrepancy	O
in	O
the	O
omitted	O
cases	O
of	O
the	O
cases	O
each	O
time	O
the	O
rule	B
compensates	O
for	O
the	O
upward	O
bias	B
cerrout	O
is	O
similar	O
to	O
a	O
grouped	O
cross-validation	B
estimate	B
that	O
omits	O
about	O
incerrout	O
by	O
incorporating	O
the	O
downwardly	O
biased	O
apparent	B
error	I
cerrout	O
has	O
resurfaced	O
in	O
the	O
popular	O
random	O
forests	O
prediction	O
algorithm	B
d	O
c	O
err	O
chapter	O
where	O
a	O
closely	O
related	O
procedure	O
gives	O
the	O
out	O
of	O
bag	O
estimate	B
of	O
err	O
google	O
flu	O
trends	O
harford	O
s	O
article	O
big	O
data	B
a	O
big	O
mistake	O
concerns	O
the	O
enormous	O
found	O
data	B
sets	O
available	O
in	O
the	O
internet	O
age	O
and	O
the	O
dangers	O
of	O
forgetting	O
the	O
principles	O
of	O
statistical	O
inference	B
in	O
their	O
analysis	B
google	O
flu	O
trends	O
is	O
his	O
primary	O
cautionary	O
example	O
objective	B
bayes	I
inference	B
and	O
markov	O
chain	O
monte	O
carlo	O
from	O
its	O
very	O
beginnings	O
bayesian	B
inference	B
exerted	O
a	O
powerful	O
influence	O
on	O
statistical	O
thinking	O
the	O
notion	O
of	O
a	O
single	O
coherent	O
methodology	O
employing	O
only	O
the	O
rules	O
of	O
probability	O
to	O
go	O
from	O
assumption	O
to	O
conclusion	O
was	O
and	O
is	O
immensely	O
attractive	O
for	O
years	O
however	O
two	O
impediments	O
stood	O
between	O
bayesian	B
theory	B
s	O
philosophical	O
attraction	O
and	O
its	O
practical	O
application	O
in	O
the	O
absence	O
of	O
relevant	O
past	O
experience	O
the	O
choice	O
of	O
a	O
prior	B
distribution	B
introduces	O
an	O
unwanted	O
subjective	O
element	O
into	O
scientific	O
inference	B
bayes	O
rule	B
looks	O
simple	O
enough	O
but	O
carrying	O
out	O
the	O
numerical	O
calculation	O
of	O
a	O
posterior	B
distribution	B
often	O
involves	O
intricate	O
higherdimensional	O
integrals	O
the	O
two	O
impediments	O
fit	O
neatly	O
into	O
the	O
dichotomy	O
of	O
chapter	O
the	O
first	O
being	O
inferential	O
and	O
the	O
second	O
a	O
renewed	O
cycle	O
of	O
bayesian	B
enthusiasm	O
took	O
hold	O
in	O
the	O
at	O
first	O
concerned	O
mainly	O
with	O
coherent	O
inference	B
building	O
on	O
work	O
by	O
bruno	O
de	B
finetti	I
and	O
l	O
j	O
savage	B
a	O
principled	O
theory	B
of	O
subjective	B
probability	I
was	O
constructed	O
the	O
bayesian	B
statistician	O
by	O
the	O
careful	O
elicitation	O
of	O
prior	B
knowledge	O
utility	O
and	O
belief	O
arrives	O
at	O
the	O
correct	O
subjective	B
prior	B
distribution	B
for	O
the	O
problem	O
at	O
hand	O
subjective	O
bayesianism	B
is	O
particularly	O
appropriate	O
for	O
individual	O
decision	O
making	O
say	O
for	O
the	O
business	O
executive	O
trying	O
to	O
choose	O
the	O
best	O
investment	O
in	O
the	O
face	O
of	O
uncertain	O
information	B
it	O
is	O
less	O
appropriate	O
for	O
scientific	O
inference	B
where	O
the	O
sometimes	O
skeptical	O
world	O
of	O
science	O
puts	O
a	O
premium	O
on	O
objectivity	O
an	O
answer	O
came	O
from	O
the	O
school	O
of	O
objective	B
bayes	I
inference	B
following	O
the	O
approach	O
of	O
laplace	O
and	O
jeffreys	B
as	O
discussed	O
in	O
section	O
their	O
goal	O
was	O
to	O
fashion	O
objective	O
or	O
uninformative	B
prior	B
distributions	O
that	O
in	O
some	O
sense	O
were	O
unbiased	O
in	O
their	O
effects	O
upon	O
the	O
data	B
analysis	B
the	O
exponential	B
family	I
material	O
in	O
this	O
chapter	O
provides	O
technical	O
support	O
but	O
is	O
not	O
required	O
in	O
detail	O
for	O
a	O
general	O
understanding	O
of	O
the	O
main	O
ideas	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
in	O
what	O
came	O
as	O
a	O
surprise	O
to	O
the	O
bayes	O
community	O
the	O
objective	O
school	O
has	O
been	O
the	O
most	O
successful	O
in	O
bringing	O
bayesian	B
ideas	O
to	O
bear	O
on	O
scientific	O
data	B
analysis	B
of	O
the	O
articles	O
in	O
the	O
december	O
issue	O
of	O
the	O
annals	O
of	O
applied	O
statistics	B
employed	O
bayesian	B
analysis	B
predominantly	O
based	O
on	O
objective	O
priors	B
this	O
is	O
where	O
electronic	O
computation	O
enters	O
the	O
story	O
commencing	O
in	O
the	O
dramatic	O
steps	O
forward	O
were	O
made	O
in	O
the	O
numerical	O
calculation	O
of	O
high-dimensional	O
bayes	O
posterior	O
distributions	O
markov	O
chain	O
monte	O
carlo	O
is	O
the	O
generic	O
name	O
for	O
modern	O
posterior	O
computation	O
algorithms	O
these	O
proved	O
particularly	O
well	O
suited	O
for	O
certain	O
forms	O
of	O
objective	B
bayes	I
prior	B
distributions	O
taken	O
together	O
objective	O
priors	B
and	O
mcmc	B
computations	B
provide	O
an	O
attractive	O
package	O
for	O
the	O
statistician	O
faced	O
with	O
a	O
complicated	O
data	B
analysis	B
situation	O
statistical	O
inference	B
becomes	O
almost	O
automatic	O
at	O
least	O
compared	O
with	O
the	O
rigors	O
of	O
frequentist	B
analysis	B
this	O
chapter	O
discusses	O
both	O
parts	O
of	O
the	O
package	O
the	O
choice	O
of	O
prior	B
and	O
the	O
subsequent	O
computational	O
methods	O
criticisms	O
arise	O
both	O
from	O
the	O
frequentist	B
viewpoint	O
and	O
that	O
of	O
informative	O
bayesian	B
analysis	B
which	O
are	O
brought	O
up	O
here	O
and	O
also	O
in	O
chapter	O
objective	O
prior	B
distributions	O
a	O
flat	O
or	O
uniform	O
distribution	B
over	O
the	O
space	O
of	O
possible	O
parameter	O
values	O
seems	O
like	O
the	O
obvious	O
choice	O
for	O
an	O
uninformative	B
prior	B
distribution	B
and	O
has	O
been	O
so	O
ever	O
since	O
laplace	O
s	O
advocacy	O
in	O
the	O
late	O
eighteenth	O
century	O
for	O
a	O
finite	O
parameter	B
space	I
say	O
d	O
flat	O
has	O
the	O
obvious	O
meaning	O
d	O
k	O
for	O
all	O
if	O
k	O
is	O
infinite	O
or	O
if	O
is	O
continuous	O
we	O
can	O
still	O
take	O
d	O
constant	O
bayes	O
rule	B
gives	O
the	O
same	O
posterior	B
distribution	B
for	O
any	O
choice	O
of	O
the	O
constant	O
d	O
with	O
f	O
dz	O
objective	O
prior	B
distributions	O
notice	O
that	O
cancels	O
out	O
of	O
the	O
fact	O
that	O
is	O
improper	O
that	O
is	O
it	O
integrates	O
to	O
infinity	O
doesn	O
t	B
affect	O
the	O
formal	O
use	O
of	O
bayes	O
rule	B
in	O
as	O
long	O
as	O
f	O
is	O
finite	O
notice	O
also	O
that	O
amounts	O
to	O
taking	O
the	O
posterior	B
density	B
of	O
to	O
be	O
proportional	O
to	O
the	O
likelihood	B
function	B
d	O
x	O
fixed	O
and	O
varying	O
over	O
this	O
brings	O
us	O
close	O
to	O
fisherian	B
inference	B
with	O
its	O
emphasis	O
on	O
the	O
direct	O
interpretation	O
of	O
likelihoods	O
but	O
fisher	B
was	O
adamant	O
in	O
his	O
insistance	O
that	O
likelihood	B
was	O
not	O
probability	O
figure	O
the	O
solid	O
curve	O
is	O
flat-prior	O
posterior	B
density	B
having	O
observed	O
x	O
d	O
from	B
poisson	B
model	O
x	O
it	O
is	O
shifted	O
about	O
units	O
right	O
from	O
the	O
confidence	O
density	B
of	O
figure	O
jeffreys	B
prior	B
gives	O
a	O
posterior	B
density	B
nearly	O
the	O
same	O
as	O
the	O
confidence	O
density	B
the	O
solid	O
curve	O
in	O
figure	O
shows	O
for	O
the	O
poisson	B
situation	O
of	O
table	O
x	O
with	O
x	O
d	O
observed	O
is	O
shifted	O
almost	O
exactly	O
units	O
right	O
of	O
the	O
confidence	O
density	B
from	O
figure	O
is	O
itself	O
in	O
this	O
fisher	B
s	O
withering	O
criticism	O
of	O
flat-prior	O
bayes	O
inference	B
focused	O
on	O
its	O
the	O
reader	O
may	O
wish	O
to	O
review	O
chapter	O
particularly	O
section	O
for	O
these	O
constructions	B
densityflat	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
lack	O
of	O
transformation	B
invariance	I
if	O
we	O
were	O
interested	O
in	O
d	O
rather	O
than	O
gflat	O
jx	O
would	O
not	O
be	O
the	O
transformation	O
to	O
the	O
log	O
scale	B
of	O
jeffreys	B
prior	B
or	O
which	O
does	O
transform	B
correctly	O
is	O
for	O
x	O
d	O
is	O
then	O
a	O
close	O
match	O
to	O
the	O
confidence	O
density	B
in	O
figure	O
d	O
p	O
coverage	B
matching	I
priors	B
a	O
variety	O
of	O
improvements	O
and	O
variations	O
on	O
jeffreys	B
prior	B
have	O
been	O
suggested	O
for	O
use	O
as	O
general-purpose	O
uninformative	B
prior	B
distributions	O
as	O
briefly	O
discussed	O
in	O
the	O
chapter	O
endnotes	O
all	O
share	O
the	O
drawback	O
seen	O
in	O
figure	O
the	O
posterior	B
distribution	B
can	O
have	O
unintended	O
effects	O
on	O
the	O
resulting	O
inferences	O
for	O
a	O
real-valued	O
parameter	O
of	O
interest	O
d	O
t	B
this	O
is	O
unavoidable	O
it	O
is	O
mathematically	O
impossible	O
for	O
any	O
single	O
prior	B
to	O
be	O
uninformative	O
for	O
every	O
choice	O
of	O
d	O
t	B
the	O
label	O
uninformative	O
for	O
a	O
prior	B
sometimes	O
means	O
gives	O
bayes	O
posterior	O
intervals	B
that	O
closely	O
match	O
confidence	O
intervals	B
perhaps	O
surprisingly	O
this	O
definition	O
has	O
considerable	O
resonance	O
in	O
the	O
bayes	O
community	O
such	O
priors	B
can	O
be	O
constructed	O
for	O
any	O
given	O
scalar	O
parameter	O
of	O
interest	O
d	O
t	B
for	O
instance	O
the	O
maximum	O
eigenvalue	O
parameter	O
of	O
figure	O
in	O
brief	O
the	O
construction	O
proceeds	O
as	O
follows	O
the	O
p-dimensional	O
parameter	O
vector	B
is	O
transformed	O
to	O
a	O
form	B
that	O
makes	O
the	O
first	O
coordinate	O
say	O
where	O
is	O
a	O
nuisance	O
parameter	O
the	O
transformation	O
is	O
chosen	O
so	O
that	O
the	O
fisher	B
information	B
matrix	B
for	O
has	O
the	O
diagonal	O
form	B
i	O
i	O
is	O
always	O
possible	O
finally	O
the	O
prior	B
for	O
is	O
taken	O
proportional	O
to	O
where	O
is	O
an	O
arbitrary	O
density	B
in	O
other	O
words	O
i	O
g	O
d	O
conjugate	B
prior	B
distributions	O
g	O
combines	O
the	O
one-dimensional	O
jeffreys	B
prior	B
for	O
with	O
an	O
arbitrary	O
independent	O
prior	B
for	O
the	O
orthogonal	O
nuisance	O
parameter	O
vector	B
the	O
main	O
thing	O
to	O
notice	O
about	O
is	O
that	O
g	O
represents	O
different	O
priors	B
on	O
the	O
original	O
parameter	O
vector	B
for	O
different	O
functions	O
d	O
t	B
no	O
single	O
prior	B
can	O
be	O
uninformative	O
for	O
all	O
choices	O
of	O
the	O
parameter	O
of	O
interest	O
calculating	O
g	O
can	O
be	O
difficult	O
one	O
alternative	O
is	O
to	O
go	O
directly	O
to	O
the	O
bca	O
confidence	O
density	B
which	O
can	O
be	O
interpreted	O
as	O
the	O
posterior	B
distribution	B
from	O
an	O
uninformative	B
prior	B
its	O
integrals	O
agree	O
closely	O
with	O
confidence	O
interval	B
endpoints	O
coverage	B
matching	I
priors	B
are	O
not	O
much	O
used	O
in	O
practice	O
and	O
in	O
fact	O
none	O
of	O
the	O
eight	O
annals	O
of	O
applied	O
statistics	B
objective	B
bayes	I
papers	O
mentioned	O
earlier	O
were	O
of	O
type	O
a	O
form	B
of	O
almost	O
uninformative	O
priors	B
the	O
conjugates	O
is	O
more	O
popular	O
mainly	O
because	O
of	O
the	O
simpler	O
computation	O
of	O
their	O
posterior	O
distributions	O
conjugate	B
prior	B
distributions	O
a	O
mathematically	O
convenient	O
class	O
of	O
prior	B
distributions	O
the	O
conjugate	B
priors	B
applies	O
to	O
samples	O
from	O
an	O
exponential	O
section	O
d	O
e	O
here	O
we	O
have	O
indexed	O
the	O
family	O
with	O
the	O
expectation	B
parameter	I
d	O
ef	O
fxg	O
rather	O
than	O
the	O
canonical	O
parameter	O
on	O
the	O
right-hand	O
side	O
of	O
can	O
be	O
thought	O
of	O
as	O
a	O
one-to-one	O
function	B
of	O
so-called	O
link	B
function	B
e	O
g	O
d	O
for	O
the	O
poisson	B
family	O
the	O
observed	O
data	B
is	O
a	O
random	O
sample	B
x	O
d	O
xn	O
from	O
xn	O
having	O
density	B
function	B
the	O
average	O
nx	O
dp	O
xi	O
being	O
sufficient	O
d	O
en	O
we	O
will	O
concentrate	O
on	O
one-parameter	B
families	O
though	O
the	O
theory	B
extends	O
to	O
the	O
multiparameter	O
case	O
figure	O
relates	O
to	O
a	O
two-parameter	O
situation	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
the	O
family	O
of	O
conjugate	B
priors	B
for	O
allows	O
the	O
statistician	O
to	O
choose	O
two	O
parameters	O
and	O
d	O
v	O
v	O
the	O
variance	B
of	O
an	O
x	O
from	O
v	O
d	O
varf	O
fxgi	O
c	O
is	O
the	O
constant	O
that	O
makes	O
integrate	O
to	O
with	O
respect	O
to	O
lebesgue	O
measure	O
on	O
the	O
interval	B
of	O
possible	O
values	O
the	O
interpretation	O
is	O
that	O
represents	O
the	O
average	O
of	O
hypothetical	O
prior	B
observations	O
from	O
the	O
utility	O
of	O
conjugate	B
priors	B
is	O
seen	O
in	O
the	O
following	O
theorem	B
theorem	B
define	O
nc	O
d	O
c	O
n	O
and	O
nx	O
then	O
the	O
posterior	B
density	B
of	O
given	O
x	O
d	O
xn	O
is	O
nc	O
c	O
n	O
nc	O
nxc	O
d	O
d	O
gncnxc	O
moreover	O
the	O
posterior	O
expectation	O
of	O
given	O
x	O
is	O
nx	O
d	O
nc	O
c	O
n	O
nc	O
the	O
intuitive	O
interpretation	O
is	O
quite	O
satisfying	O
we	O
begin	O
with	O
a	O
hypothetical	O
prior	B
sample	B
of	O
size	O
sufficient	O
statistic	B
observe	O
x	O
a	O
sample	B
of	O
size	O
n	O
and	O
update	O
our	O
prior	B
distribution	B
to	O
a	O
distribution	B
gncnxc	O
of	O
the	O
same	O
form	B
moreover	O
equals	O
the	O
average	O
of	O
a	O
hypothetical	O
sample	B
with	O
copies	O
of	O
xn	O
that	O
is	O
we	O
have	O
n	O
i	O
i	O
d	O
observations	O
from	O
a	O
poisson	B
distribution	B
table	O
formula	B
gives	O
conjugate	B
prior	B
as	O
an	O
example	O
suppose	O
xi	O
d	O
c	O
not	O
depending	O
on	O
so	O
in	O
the	O
notation	O
of	O
table	O
is	O
a	O
gamma	B
distribution	B
the	O
posterior	B
distribution	B
is	O
d	O
gncnxc	O
gam	O
nc	O
nxc	O
nc	O
gnc	O
nxc	O
conjugate	B
prior	B
distributions	O
where	O
indicates	O
a	O
standard	O
gamma	B
distribution	B
d	O
table	O
conjugate	B
priors	B
for	O
four	O
familiar	O
one-parameter	B
exponential	O
families	O
using	O
notation	O
in	O
table	O
the	O
last	O
column	O
shows	O
the	O
posterior	B
distribution	B
of	O
given	O
n	O
observations	O
xi	O
starting	O
from	O
prior	B
in	O
line	O
is	O
the	O
standard	O
gamma	B
distribution	B
with	O
the	O
same	O
as	O
gamma	B
parameter	O
in	O
table	O
the	O
chapter	O
endnotes	O
give	O
the	O
density	B
of	O
the	O
inverse	B
gamma	B
distribution	B
and	O
corresponding	O
results	O
for	O
chi-squared	O
variates	O
name	O
normal	B
xi	O
distribution	B
n	O
known	O
poisson	B
binomial	B
gamma	B
known	O
n	O
gam	O
nc	O
nxc	O
be	O
nc	O
nxc	O
nxc	O
n	O
nc	O
table	O
describes	O
the	O
conjugate	B
prior	B
and	O
posterior	O
distributions	O
for	O
four	O
familiar	O
one-parameter	B
families	O
the	O
binomial	B
case	O
where	O
is	O
the	O
success	O
probability	O
in	O
table	O
is	O
particularly	O
evocative	O
indepeni	O
xi	O
d	O
nnx	O
successes	O
prior	B
amounts	O
to	O
assuming	O
proportion	B
d	O
prior	B
successes	O
in	O
flips	O
formula	B
becomes	O
dent	O
coin	O
flips	O
xn	O
give	O
say	O
s	O
dp	O
d	O
c	O
s	O
c	O
n	O
for	O
the	O
posterior	O
expectation	O
of	O
the	O
choice	O
d	O
for	O
instance	O
gives	O
bayesian	B
estimate	B
for	O
pulling	O
the	O
mle	B
sn	O
a	O
little	O
bit	O
toward	O
the	O
size	O
of	O
the	O
number	O
of	O
hypothetical	O
prior	B
observations	O
determines	O
how	O
informative	O
or	O
uninformative	O
the	O
prior	B
is	O
recent	O
objective	B
bayes	I
literature	O
has	O
favored	O
choosing	O
small	O
d	O
being	O
popular	O
the	O
hope	O
here	O
is	O
to	O
employ	O
a	O
proper	B
prior	B
that	O
has	O
a	O
finite	O
integral	O
while	O
still	O
not	O
injecting	O
much	O
unwarranted	O
information	B
into	O
the	O
analysis	B
the	O
choice	O
of	O
is	O
also	O
by	O
convention	O
one	O
possibility	O
is	O
to	O
set	B
objective	B
bayes	I
inference	B
and	O
mcmc	B
d	O
nx	O
in	O
which	O
case	O
the	O
posterior	O
expectation	O
equals	O
the	O
mle	B
nx	O
another	O
possibility	O
is	O
choosing	O
equal	O
to	O
a	O
null	O
value	O
for	O
instance	O
d	O
for	O
effect	B
size	I
estimation	B
in	O
table	O
vasoconstriction	B
data	B
volume	O
of	O
air	O
inspired	O
in	O
cases	O
without	O
vasoconstriction	B
d	O
and	O
with	O
vasoconstriction	B
d	O
y	O
d	O
y	O
d	O
as	O
a	O
miniature	O
example	O
of	O
objective	B
bayes	I
inference	B
we	O
consider	O
the	O
vasoconstriction	B
data	B
of	O
table	O
n	O
d	O
measurements	O
of	O
lung	O
volume	O
have	O
been	O
obtained	O
without	O
vasoconstriction	B
d	O
and	O
with	O
d	O
here	O
we	O
will	O
think	O
of	O
the	O
yi	O
as	O
binomial	B
variates	O
following	O
logistic	B
regression	B
model	I
yi	O
i	O
d	O
d	O
c	O
log	O
with	O
the	O
xi	O
as	O
fixed	O
covariates	O
values	O
in	O
table	O
letting	O
xi	O
d	O
xi	O
nential	O
family	O
results	O
in	O
a	O
two-parameter	O
expo	O
the	O
mle	B
o	O
has	O
approximate	O
covariance	O
matrix	B
ov	O
as	O
given	O
in	O
i	O
o	O
h	O
f	O
d	O
en	O
nx	O
yi	O
xi	O
yi	O
having	O
o	O
d	O
n	O
nx	O
and	O
d	O
c	O
e	O
xi	O
nx	O
n	O
conjugate	B
prior	B
distributions	O
in	O
figure	O
the	O
posterior	O
distributions	O
are	O
graphed	O
in	O
terms	O
of	O
rather	O
than	O
or	O
making	O
the	O
contours	O
of	O
equal	O
density	B
roughly	O
circular	O
and	O
centered	O
at	O
zero	O
d	O
ov	O
o	O
figure	O
vasoconstriction	B
data	B
contours	O
of	O
equal	O
posterior	B
density	B
of	O
from	O
four	O
uninformative	O
priors	B
as	O
described	O
in	O
the	O
text	O
numbers	O
indicate	O
probability	O
content	O
within	O
contours	O
light	O
dashed	O
contours	O
from	O
panel	O
a	O
flat	B
prior	B
panel	O
a	O
of	O
figure	O
illustrates	O
the	O
flat	B
prior	B
posterior	B
density	B
of	O
given	O
the	O
data	B
y	O
in	O
model	O
the	O
heavy	O
lines	O
are	O
contours	O
of	O
equal	O
density	B
with	O
the	O
one	O
labeled	O
containing	O
of	O
the	O
posterior	B
probability	I
etc	O
panel	O
b	O
shows	O
the	O
corresponding	O
posterior	B
density	B
a	O
flat	O
jeffreys	B
lc	O
conjugate	B
ld	O
bootstrap	B
objective	B
bayes	I
inference	B
and	O
mcmc	B
contours	O
obtained	O
from	O
jeffreys	B
multiparameter	O
prior	B
in	O
this	O
case	O
gjeff	O
d	O
jv	O
v	O
the	O
covariance	O
matrix	B
of	O
o	O
as	O
calculated	O
from	O
for	O
comparison	O
purposes	O
the	O
light	O
dashed	O
curves	O
show	O
some	O
of	O
the	O
flat	B
prior	B
contours	O
from	O
panel	O
a	O
the	O
effect	O
of	O
gjeff	O
is	O
to	O
reduce	O
the	O
flat	B
prior	B
bulge	O
toward	O
the	O
upper	O
left	O
corner	O
panel	O
c	O
relates	O
to	O
the	O
conjugate	B
besides	O
reducing	O
the	O
flat	B
prior	B
bulge	O
pulls	O
the	O
contours	O
slightly	O
downward	O
with	O
o	O
replacing	O
gave	O
resamples	O
y	O
the	O
contours	O
of	O
toward	O
the	O
left	O
panel	O
d	O
shows	O
the	O
parametric	B
bootstrap	B
distribution	B
model	O
and	O
mle	B
replications	O
o	O
o	O
considerably	O
accentuate	O
the	O
bulge	O
d	O
ov	O
figure	O
posterior	O
densities	O
for	O
first	O
coordinate	O
of	O
in	O
for	O
the	O
vasoconstriction	B
data	B
dashed	O
red	O
curve	O
raw	O
distribution	B
of	O
b	O
d	O
parametric	B
replications	O
from	O
model	O
solid	O
black	O
curve	O
bca	O
density	B
d	O
a	O
d	O
dotted	O
blue	O
curve	O
posterior	B
density	B
using	O
jeffreys	B
multiparameter	O
prior	B
the	O
role	O
of	O
nx	O
in	O
is	O
taken	O
by	O
o	O
in	O
so	O
has	O
o	O
d	O
d	O
this	O
makes	O
d	O
the	O
factor	B
v	O
in	O
is	O
absent	O
in	O
the	O
conjugate	B
prior	B
for	O
opposed	O
to	O
bootstrapjeffreys	O
model	B
selection	I
and	O
the	O
bayesian	B
information	B
criterion	I
this	O
doesn	O
t	B
necessarily	O
imply	O
that	O
a	O
bootstrap	B
analysis	B
would	O
give	O
much	O
different	O
answers	O
than	O
the	O
three	O
similar	O
objective	B
bayes	I
results	O
for	O
any	O
particular	O
real-valued	O
parameter	O
of	O
interest	O
the	O
raw	O
bootstrap	B
distribution	B
weight	O
on	O
each	O
replication	B
would	O
be	O
reweighted	O
according	O
to	O
the	O
bca	O
formula	B
in	O
order	O
to	O
produce	O
accurate	O
confidence	O
intervals	B
figure	O
compares	O
the	O
raw	O
bootstrap	B
distribution	B
the	O
bca	O
confidence	O
density	B
and	O
the	O
posterior	B
density	B
obtained	O
from	O
jeffreys	B
prior	B
for	O
equal	O
to	O
the	O
first	O
coordinate	O
of	O
in	O
the	O
bca	O
density	B
is	O
shifted	O
to	O
the	O
right	O
of	O
jeffreys	B
critique	O
of	O
objective	B
bayes	I
inference	B
despite	O
its	O
simplicity	O
or	O
perhaps	O
because	O
of	O
it	O
objective	B
bayes	I
procedures	O
are	O
vulnerable	O
to	O
criticism	O
from	O
both	O
ends	O
of	O
the	O
statistical	O
spectrum	O
from	O
the	O
subjectivist	O
point	O
of	O
view	O
objective	B
bayes	I
is	O
only	O
partially	O
bayesian	B
it	O
employs	O
bayes	O
theorem	B
but	O
without	O
doing	O
the	O
hard	O
work	O
of	O
determining	O
a	O
convincing	O
prior	B
distribution	B
this	O
introduces	O
frequentist	B
elements	O
into	O
its	O
practice	O
clearly	O
so	O
in	O
the	O
case	O
of	O
jeffreys	B
prior	B
along	O
with	O
frequentist	B
incoherencies	O
for	O
the	O
frequentist	B
objective	B
bayes	I
analysis	B
can	O
seem	O
dangerously	O
untethered	O
from	O
the	O
usual	O
standards	O
of	O
accuracy	B
having	O
only	O
tenuous	O
largesample	O
claims	O
to	O
legitimacy	O
this	O
is	O
more	O
than	O
a	O
theoretical	O
objection	O
the	O
practical	O
advantages	O
claimed	O
for	O
bayesian	B
methods	O
depend	O
crucially	O
on	O
the	O
fine	O
structure	B
of	O
the	O
prior	B
can	O
we	O
safely	O
ignore	O
stopping	B
rules	I
or	O
selective	O
inference	B
choosing	O
the	O
largest	O
of	O
many	O
estimated	O
parameters	O
for	O
special	O
attention	O
for	O
a	O
prior	B
not	O
based	O
on	O
some	O
form	B
of	O
genuine	O
experience	O
in	O
an	O
era	O
of	O
large	O
complicated	O
and	O
difficult	O
data-analytic	O
problems	O
objective	B
bayes	I
methods	O
are	O
answering	O
a	O
felt	O
need	O
for	O
relatively	O
straightforward	O
paths	O
to	O
solution	O
granting	O
their	O
usefulness	O
it	O
is	O
still	O
reasonable	O
to	O
hope	O
for	O
better	O
or	O
at	O
least	O
for	O
more	O
careful	O
comparisons	O
with	O
competing	O
methods	O
as	O
in	O
figure	O
model	B
selection	I
and	O
the	O
bayesian	B
information	B
criterion	I
data-based	O
model	B
selection	I
has	O
become	O
a	O
major	O
theme	O
of	O
modern	O
statistical	O
inference	B
in	O
the	O
problem	O
s	O
simplest	O
form	B
the	O
statistician	O
observes	O
data	B
x	O
and	O
wishes	O
to	O
choose	O
between	O
a	O
smaller	O
model	O
and	O
a	O
larger	O
model	O
chapter	O
discusses	O
the	O
frequentist	B
assessment	O
of	O
bayes	O
and	O
objective	B
bayes	I
estimates	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
the	O
classic	O
textbook	O
example	O
takes	O
x	O
d	O
xn	O
as	O
an	O
inde	O
pendent	O
normal	B
sample	B
xi	O
n	O
for	O
i	O
d	O
n	O
with	O
the	O
null	O
hypothesis	O
d	O
and	O
the	O
general	O
two-sided	O
alter	O
native	O
w	O
d	O
w	O
can	O
include	O
d	O
in	O
with	O
no	O
effect	O
on	O
what	O
follows	O
from	O
a	O
frequentist	B
viewpoint	O
choosing	O
between	O
and	O
in	O
amounts	O
to	O
running	O
a	O
hypothesis	O
test	O
of	O
w	O
d	O
perhaps	O
augmented	O
with	O
a	O
confidence	O
interval	B
for	O
bayesian	B
model	B
selection	I
aims	O
for	O
more	O
an	O
evaluation	O
of	O
the	O
posterior	O
probabilities	B
of	O
and	O
given	O
x	O
a	O
full	B
bayesian	B
specification	O
requires	O
prior	B
probabilities	B
for	O
the	O
two	O
models	B
and	O
conditional	B
prior	B
densities	O
for	O
within	O
each	O
model	O
and	O
d	O
density	B
for	O
x	O
say	O
dz	O
and	O
dz	O
let	O
be	O
the	O
density	B
of	O
x	O
given	O
each	O
model	O
induces	O
a	O
marginal	O
bayes	O
theorem	B
in	O
its	O
ratio	O
form	B
then	O
gives	O
posterior	O
probabilities	B
d	O
prf	O
and	O
d	O
prf	O
and	O
d	O
d	O
prf	O
d	O
prf	O
d	O
satisfying	O
d	O
b	O
x	O
where	O
b	O
x	O
is	O
the	O
bayes	O
factor	B
b	O
x	O
d	O
leading	O
to	O
the	O
elegant	O
statement	O
that	O
the	O
posterior	O
odds	O
ratio	O
is	O
the	O
prior	B
odds	O
ratio	O
times	O
the	O
bayes	O
factor	B
all	O
of	O
this	O
is	O
of	O
more	O
theoretical	O
than	O
applied	O
use	O
prior	B
specifications	O
are	O
usually	O
unavailable	O
in	O
practical	O
settings	O
is	O
why	O
model	B
selection	I
and	O
the	O
bic	O
standard	O
hypothesis	B
testing	B
is	O
so	O
popular	O
the	O
objective	B
bayes	I
school	O
has	O
concentrated	O
on	O
estimating	O
the	O
bayes	O
factor	B
b	O
x	O
with	O
the	O
understanding	O
that	O
the	O
prior	B
odds	O
ratio	O
in	O
would	O
be	O
roughly	O
evaluated	O
depending	O
on	O
the	O
specific	O
circumstances	O
perhaps	O
set	B
to	O
the	O
laplace	O
choice	O
d	O
table	O
jeffreys	B
scale	B
of	O
evidence	O
for	O
the	O
interpretation	O
of	O
bayes	O
factors	O
bayes	O
factor	B
evidence	O
for	O
negative	O
barely	O
worthwhile	O
positive	O
strong	O
very	O
strong	O
jeffreys	B
suggested	O
a	O
scale	B
of	O
evidence	O
for	O
interpreting	O
bayes	O
factors	O
re	O
produced	O
in	O
table	O
b	O
x	O
d	O
for	O
instance	O
constitutes	O
positive	O
but	O
not	O
strong	O
evidence	O
in	O
favor	O
of	O
the	O
bigger	O
model	O
jeffreys	B
scale	B
is	O
a	O
bayesian	B
version	O
of	O
fisher	B
s	O
interpretive	O
scale	B
for	O
the	O
outcome	O
of	O
a	O
hypothetic	O
test	O
with	O
coverage	B
value	O
minus	O
the	O
significance	O
level	O
famously	O
constituting	O
significant	O
evidence	O
against	O
the	O
null	O
hypothesis	O
table	O
shows	O
fisher	B
s	O
scale	B
as	O
commonly	O
interpreted	O
in	O
the	O
biomedical	O
and	O
social	O
sciences	O
table	O
fisher	B
s	O
scale	B
of	O
evidence	O
against	O
null	O
hypothesis	O
and	O
in	O
favor	O
of	O
as	O
a	O
function	B
of	O
coverage	B
level	I
minus	O
the	O
p-value	B
coverage	B
evidence	O
for	O
null	O
borderline	O
moderate	O
substantial	O
strong	O
very	O
strong	O
overwhelming	O
even	O
if	O
we	O
accept	O
the	O
reduction	O
of	O
model	B
selection	I
to	O
assessing	O
the	O
bayes	O
factor	B
b	O
x	O
in	O
and	O
even	O
if	O
we	O
accept	O
jeffreys	B
scale	B
of	O
interpretation	O
this	O
still	O
leaves	O
a	O
crucial	O
question	O
how	O
to	O
compute	O
b	O
x	O
in	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
practice	O
without	O
requiring	O
informative	O
choices	O
of	O
the	O
priors	B
and	O
in	O
a	O
popular	O
objective	B
bayes	I
answer	O
is	O
provided	O
by	O
the	O
bayesian	B
informa	O
tion	O
criterion	O
for	O
a	O
given	O
model	O
m	O
we	O
define	O
log	O
n	O
bic	O
m	O
d	O
log	O
f	O
p	O
where	O
is	O
the	O
mle	B
p	O
the	O
degrees	B
of	I
freedom	I
of	O
free	O
parameters	O
in	O
m	O
and	O
n	O
the	O
sample	B
size	I
then	O
the	O
bic	O
approximation	O
to	O
bayes	O
factor	B
b	O
x	O
is	O
log	O
bbic	O
x	O
d	O
d	O
log	O
f	O
w	O
d	O
log	O
f	O
the	O
subscripts	O
indexing	O
the	O
mles	O
and	O
degrees	B
of	I
freedom	I
in	O
and	O
this	O
can	O
be	O
restated	O
in	O
somewhat	O
more	O
familiar	O
terms	O
letting	O
w	O
be	O
wilks	O
likelihood	B
ratio	O
statistic	B
log	O
n	O
we	O
have	O
log	O
bbic	O
x	O
d	O
fw	O
d	O
log	O
ng	O
with	O
d	O
d	O
w	O
approximately	O
follows	O
a	O
d	O
distribution	B
under	O
model	O
d	O
implying	O
bbic	O
x	O
will	O
tend	O
to	O
be	O
less	O
than	O
one	O
favoring	O
if	O
it	O
is	O
true	O
ever	O
more	O
strongly	B
as	O
n	O
increases	O
we	O
can	O
apply	O
bic	O
selection	O
to	O
the	O
vasoconstriction	B
data	B
of	O
table	O
taking	O
to	O
be	O
model	O
and	O
to	O
be	O
the	O
submodel	O
having	O
d	O
in	O
this	O
case	O
d	O
d	O
in	O
direct	O
calculation	O
gives	O
w	O
d	O
and	O
positive	O
but	O
not	O
strong	O
evidence	O
against	O
according	O
to	O
jeffreys	B
scale	B
by	O
comparison	O
the	O
usual	O
frequentist	B
z-value	O
for	O
testing	B
d	O
is	O
coverage	B
level	I
between	O
substantial	O
and	O
strong	O
evidence	O
against	O
on	O
fisher	B
s	O
scale	B
the	O
bic	O
was	O
named	O
in	O
reference	O
to	O
akaike	O
s	O
information	B
criterion	I
bbic	O
d	O
aic	O
m	O
d	O
log	O
f	O
p	O
which	O
suggests	O
as	O
in	O
basing	O
model	B
selection	I
on	O
the	O
sign	O
of	O
d	O
fw	O
model	B
selection	I
and	O
the	O
bic	O
the	O
bic	O
penalty	B
d	O
log	O
n	O
in	O
grows	O
more	O
severe	O
than	O
the	O
aic	O
penalty	B
as	O
n	O
gets	O
larger	O
increasingly	O
favoring	O
selection	O
of	O
rather	O
than	O
the	O
distinction	O
is	O
rooted	O
in	O
bayesian	B
notions	O
of	O
coherent	B
behavior	I
as	O
discussed	O
in	O
what	O
follows	O
where	O
does	O
the	O
bic	O
penalty	B
term	O
d	O
log	O
n	O
in	O
come	O
from	O
a	O
first	O
answer	O
uses	O
the	O
simple	O
normal	B
model	O
xi	O
n	O
has	O
prior	B
d	O
equal	O
a	O
delta	O
function	B
at	O
zero	O
suppose	O
we	O
take	O
d	O
in	O
to	O
be	O
the	O
gaussian	B
conjugate	B
prior	B
the	O
discussion	O
following	O
in	O
section	O
suggests	O
setting	O
m	O
d	O
and	O
a	O
d	O
corresponding	O
to	O
prior	B
information	B
equivalent	O
to	O
one	O
of	O
the	O
n	O
actual	O
observations	O
in	O
this	O
case	O
we	O
can	O
calculate	O
the	O
actual	O
bayes	O
factor	B
b	O
x	O
n	O
a	O
n	O
w	O
log	O
n	O
c	O
log	O
b	O
x	O
d	O
n	O
c	O
nearly	O
equaling	O
log	O
bbic	O
x	O
d	O
for	O
large	O
n	O
justifications	O
of	O
the	O
bic	O
formula	B
as	O
an	O
approximate	O
bayes	O
factor	B
follow	O
generalizations	O
of	O
this	O
kind	O
of	O
argument	B
as	O
discussed	O
in	O
the	O
chapter	O
endnotes	O
z	O
n	O
under	O
the	O
difference	O
between	O
bic	O
and	O
frequentist	B
hypothesis	B
testing	B
grows	O
more	O
drastic	O
for	O
large	O
n	O
suppose	O
is	O
a	O
regression	B
model	I
and	O
is	O
augmented	O
with	O
one	O
additional	O
covariate	O
d	O
d	O
let	O
z	O
be	O
a	O
standard	O
z-value	O
for	O
testing	B
the	O
hypothesis	O
that	O
is	O
no	O
improvement	O
over	O
table	O
shows	O
bbic	O
x	O
as	O
a	O
function	B
of	O
z	O
and	O
n	O
at	O
n	O
d	O
fisher	B
s	O
and	O
jeffreys	B
scales	O
give	O
roughly	O
similar	O
assessments	O
of	O
the	O
evidence	O
against	O
jeffreys	B
nomenclature	O
is	O
more	O
conservative	O
at	O
the	O
other	O
end	O
of	O
the	O
table	O
at	O
n	O
d	O
the	O
inferences	O
are	O
contradictory	O
z	O
d	O
with	O
p-value	B
and	O
coverage	B
level	I
is	O
overwhelming	O
evidence	O
for	O
on	O
fisher	B
s	O
scale	B
but	O
barely	O
worthwhile	O
for	O
jeffreys	B
bayesian	B
coherency	O
the	O
axiom	O
that	O
inferences	O
should	O
be	O
consistent	O
over	O
related	O
situations	O
lies	O
behind	O
the	O
contradiction	O
suppose	O
n	O
d	O
in	O
the	O
simple	O
normal	B
model	O
that	O
is	O
we	O
observe	O
only	O
the	O
single	O
variable	O
and	O
wish	O
to	O
decide	O
between	O
w	O
d	O
and	O
w	O
let	O
denote	O
our	O
prior	B
density	B
for	O
this	O
situation	O
x	O
n	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
table	O
bic	O
bayes	O
factors	O
corresponding	O
to	O
z-values	O
for	O
testing	B
one	O
additional	O
covariate	O
coverage	B
value	O
minus	O
the	O
significance	O
level	O
of	O
a	O
two-sided	O
hypothesis	O
test	O
as	O
interpreted	O
by	O
fisher	B
s	O
scale	B
of	O
evidence	O
right	O
jeffreys	B
scale	B
of	O
evidence	O
table	O
is	O
in	O
rough	O
agreement	O
with	O
fisher	B
for	O
n	O
d	O
but	O
favors	O
the	O
null	O
much	O
more	O
strongly	B
for	O
larger	O
sample	B
sizes	O
n	O
cover	O
z-value	O
fisher	B
null	O
borderline	O
moderate	O
substantial	O
strong	O
very	O
strong	O
overwhelming	O
n	O
p	O
xi	O
and	O
d	O
p	O
p	O
the	O
case	O
n	O
in	O
is	O
logically	O
identical	O
to	O
letting	O
x	O
n	O
d	O
gives	O
x	O
n	O
n	O
p	O
p	O
with	O
becoming	O
w	O
d	O
and	O
w	O
coherency	O
p	O
requires	O
that	O
in	O
have	O
the	O
same	O
prior	B
as	O
in	O
since	O
d	O
the	O
prior	B
for	O
sample	B
size	I
n	O
satisfies	O
n	O
this	O
implies	O
that	O
g	O
n	O
d	O
g	O
n	O
this	O
being	O
sample	B
size	I
coherency	I
farther	O
away	O
from	O
the	O
null	O
value	O
d	O
at	O
rate	B
stays	O
fixed	O
for	O
any	O
fixed	O
value	O
of	O
the	O
sufficient	O
statistic	B
x	O
n	O
being	O
p	O
z	O
in	O
table	O
this	O
results	O
in	O
the	O
bayes	O
factor	B
b	O
x	O
n	O
decreasing	O
at	O
rate	B
n	O
the	O
frequentistbayesian	O
contradiction	O
seen	O
in	O
table	O
goes	O
beyond	O
the	O
specifics	O
of	O
the	O
bic	O
algorithm	B
the	O
effect	O
of	O
is	O
to	O
spread	O
the	O
prior	B
density	B
g	O
n	O
p	O
n	O
while	O
the	O
prior	B
g	O
n	O
n	O
a	O
general	B
information	B
criterion	I
takes	O
the	O
form	B
gic	O
m	O
d	O
log	O
f	O
p	O
cn	O
model	B
selection	I
and	O
the	O
bic	O
where	O
cn	O
is	O
any	O
sequence	O
of	O
positive	O
numbers	O
cn	O
d	O
for	O
bic	O
and	O
cn	O
d	O
for	O
aic	O
the	O
difference	O
d	O
d	O
d	O
will	O
be	O
positive	O
if	O
w	O
for	O
d	O
d	O
as	O
in	O
table	O
will	O
favor	O
if	O
w	O
with	O
approximate	O
probability	O
if	O
is	O
actually	O
true	O
this	O
equals	O
for	O
the	O
aic	O
choice	O
cn	O
d	O
for	O
bic	O
n	O
d	O
it	O
equals	O
the	O
choice	O
cn	O
d	O
agreeing	O
with	O
the	O
usual	O
frequentist	B
makes	O
prf	O
rejection	O
level	O
the	O
bic	O
is	O
consistent	O
prf	O
goes	O
to	O
zero	O
as	O
n	O
if	O
is	O
true	O
this	O
isn	O
t	B
true	O
of	O
for	O
instance	O
where	O
we	O
will	O
have	O
prf	O
no	O
matter	O
how	O
large	O
n	O
may	O
be	O
but	O
consistency	O
is	O
seldom	O
compelling	O
as	O
a	O
practical	O
argument	B
confidence	O
intervals	B
help	O
compensate	O
for	O
possible	O
frequentist	B
overfitting	O
with	O
z	O
d	O
and	O
n	O
d	O
the	O
confidence	O
interval	B
for	O
in	O
model	O
is	O
whether	O
or	O
not	O
such	O
a	O
small	O
effect	O
is	O
interesting	O
depends	O
on	O
the	O
scientific	O
context	O
the	O
fact	O
that	O
bic	O
says	O
not	O
interesting	O
speaks	O
to	O
its	O
inherent	O
small-model	O
bias	B
the	O
prostate	B
cancer	O
study	O
data	B
of	O
section	O
provides	O
a	O
more	O
challenging	O
model	B
selection	I
problem	O
figure	O
shows	O
the	O
histogram	O
of	O
n	O
d	O
observations	O
xi	O
each	O
measuring	O
the	O
effects	O
of	O
one	O
gene	O
the	O
histogram	O
has	O
bins	O
each	O
of	O
width	O
with	O
centers	O
cj	O
ranging	O
from	O
to	O
yj	O
the	O
height	O
of	O
the	O
histogram	O
at	O
cj	O
is	O
the	O
number	O
of	O
xi	O
in	O
bin	O
j	O
yj	O
d	O
bin	O
jg	O
for	O
j	O
d	O
we	O
assume	O
that	O
the	O
yj	O
follow	O
a	O
poisson	B
regression	B
model	I
as	O
in	O
sec	O
tion	O
yj	O
j	O
d	O
and	O
wish	O
to	O
fit	O
a	O
log	O
polynomial	O
glm	O
model	O
to	O
the	O
the	O
model	B
selection	I
question	O
is	O
what	O
degree	O
polynomial	O
degree	O
corresponds	O
to	O
normal	B
densities	O
but	O
the	O
long	O
tails	O
seen	O
in	O
figure	O
suggest	O
otherwise	O
models	B
of	O
degree	O
through	O
are	O
assessed	O
in	O
figure	O
four	O
model	B
selection	I
measures	O
are	O
compared	O
aic	O
bic	O
with	O
n	O
d	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
figure	O
log	O
polynomial	O
models	B
of	O
degree	O
through	O
applied	O
to	O
the	O
prostate	B
study	O
histogram	O
of	O
figure	O
model	B
selection	I
criteria	B
aic	O
bic	O
with	O
n	O
d	O
number	O
of	O
bins	O
or	O
number	O
of	O
genes	O
gic	O
using	O
classic	O
fisher	B
hypothesis	O
choice	O
cn	O
d	O
all	O
four	O
selected	O
the	O
fourth-degree	O
model	O
as	O
best	O
the	O
number	O
of	O
yj	O
values	O
and	O
also	O
n	O
d	O
the	O
number	O
of	O
genes	O
and	O
gic	O
with	O
cn	O
d	O
the	O
choice	O
based	O
on	O
classic	O
fisherian	O
hypothesis	B
testing	B
is	O
almost	O
the	O
same	O
as	O
bic	O
n	O
d	O
since	O
d	O
a	O
fourth-degree	O
polynomial	O
model	O
was	O
the	O
winner	O
under	O
all	O
four	O
criteria	B
the	O
untethered	O
criticism	O
made	O
against	O
objective	B
bayes	I
methods	O
in	O
general	O
is	O
particularly	O
applicable	O
to	O
bic	O
the	O
concept	O
of	O
sample	B
size	I
is	O
not	O
well	O
defined	O
as	O
the	O
prostate	B
study	O
example	O
shows	O
sample	B
size	I
coherency	I
the	O
rationale	O
for	O
bic	O
s	O
strong	O
bias	B
toward	O
smaller	O
models	B
is	O
less	O
convincing	O
in	O
the	O
absence	O
of	O
priors	B
based	O
on	O
genuine	O
experience	O
if	O
there	O
is	O
no	O
prospect	O
of	O
the	O
sample	B
size	I
changing	O
whatever	O
its	O
vulnerabilities	O
bic	O
model	B
selection	I
has	O
nevertheless	O
become	O
a	O
mainstay	O
of	O
objective	B
bayes	I
model	B
selection	I
not	O
least	O
because	O
of	O
its	O
freedom	O
from	O
the	O
choice	O
of	O
bayesian	B
priors	B
polynomial	O
degreeaic	O
and	O
bicbic	O
and	O
gicaic	O
gibbs	B
sampling	I
and	O
mcmc	B
gibbs	B
sampling	I
and	O
mcmc	B
miraculously	O
blessed	O
with	O
visions	O
of	O
the	O
future	O
a	O
bayesian	B
statistician	O
of	O
the	O
would	O
certainly	O
be	O
pleased	O
with	O
the	O
prevalence	O
of	O
bayes	O
methodology	O
in	O
twenty-first-century	O
applications	O
but	O
his	O
pleasure	O
might	O
be	O
tinged	O
with	O
surprise	O
that	O
the	O
applications	O
were	O
mostly	O
of	O
the	O
objective	O
uninformative	O
type	O
rather	O
than	O
taken	O
from	O
the	O
elegant	O
de	B
finetti	I
savage	B
school	O
of	O
subjective	O
inference	B
the	O
increase	O
in	O
bayesian	B
applications	O
and	O
the	O
change	O
in	O
emphasis	O
from	O
subjective	O
to	O
objective	O
had	O
more	O
to	O
do	O
with	O
computation	O
than	O
philosophy	O
better	O
computers	O
and	O
algorithms	O
facilitated	O
the	O
calculation	O
of	O
formerly	O
intractable	O
bayes	O
posterior	O
distributions	O
technology	O
determines	O
practice	O
and	O
the	O
powerful	O
new	O
algorithms	O
encouraged	O
bayesian	B
analyses	O
of	O
large	O
and	O
complicated	O
models	B
where	O
subjective	O
priors	B
those	O
based	O
on	O
actual	O
past	O
experience	O
were	O
hard	O
to	O
come	O
by	O
add	O
in	O
the	O
fact	O
that	O
the	O
algorithms	O
worked	O
most	O
easily	O
with	O
simple	O
convenience	O
priors	B
like	O
the	O
conjugates	O
of	O
section	O
and	O
the	O
stage	O
was	O
set	B
for	O
an	O
objective	B
bayes	I
renaissance	O
at	O
first	O
glance	O
it	O
s	O
hard	O
to	O
see	O
why	O
bayesian	B
computations	B
should	O
be	O
daunting	O
from	O
parameter	O
vector	B
data	B
x	O
density	B
function	B
f	O
and	O
prior	B
density	B
g	O
bayes	O
rule	B
directly	O
produces	O
the	O
posterior	B
density	B
g	O
jx	O
d	O
g	O
f	O
where	O
f	O
is	O
the	O
marginal	B
density	B
f	O
dz	O
the	O
posterior	B
probability	I
of	O
any	O
set	B
a	O
in	O
the	O
parameter	B
space	I
is	O
then	O
pfajxg	O
dz	O
g	O
f	O
d	O
g	O
f	O
d	O
g	O
f	O
d	O
a	O
this	O
is	O
easy	O
to	O
write	O
down	O
but	O
usually	O
difficult	O
to	O
evaluate	O
if	O
is	O
multidimensional	O
modern	O
bayes	O
methods	O
attack	O
the	O
problem	O
through	O
the	O
application	O
of	O
computer	O
power	O
even	O
if	O
we	O
can	O
t	B
integrate	O
g	O
jx	O
perhaps	O
we	O
can	O
sample	B
from	O
it	O
if	O
so	O
a	O
sufficiently	O
large	O
sample	B
say	O
would	O
provide	O
estimates	O
g	O
jx	O
n	O
o	O
a	O
opfajxg	O
d	O
b	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
and	O
similarly	O
for	O
posterior	O
moments	O
correlations	O
etc	O
we	O
would	O
in	O
this	O
way	O
be	O
employing	O
the	O
same	O
general	O
tactic	O
as	O
the	O
bootstrap	B
applied	O
now	O
for	O
bayesian	B
rather	O
than	O
frequentist	B
purposes	O
toward	O
the	O
same	O
goal	O
as	O
the	O
bootstrap	B
of	O
freeing	O
practical	O
applications	O
from	O
the	O
constraints	O
of	O
mathematical	O
tractability	O
the	O
two	O
most	O
popular	O
computational	O
gibbs	B
sampling	I
and	O
markov	O
chain	O
monte	O
carlo	O
are	O
based	O
on	O
markov	O
chain	O
algorithms	O
that	O
is	O
the	O
posterior	O
samples	O
are	O
produced	O
in	O
sequence	O
each	O
one	O
depending	O
only	O
on	O
and	O
not	O
on	O
its	O
more	O
distant	O
predecessors	O
we	O
begin	O
with	O
gibbs	B
sampling	I
the	O
central	O
idea	O
of	O
gibbs	B
sampling	I
is	O
to	O
reduce	O
the	O
generation	O
of	O
multidimensional	O
vectors	O
d	O
to	O
a	O
series	O
of	O
univariate	O
calculations	O
let	O
denote	O
with	O
component	O
k	O
removed	O
and	O
g	O
k	O
the	O
conditional	B
density	B
of	O
given	O
and	O
the	O
data	B
x	O
x	O
g	O
k	O
x	O
the	O
algorithm	B
begins	O
at	O
some	O
arbitrary	O
initial	O
value	O
having	O
computed	O
the	O
components	O
of	O
are	O
generated	O
according	O
to	O
conditional	B
distributions	O
g	O
k	O
for	O
k	O
d	O
k	O
x	O
k	O
as	O
an	O
example	O
we	O
take	O
x	O
to	O
be	O
the	O
n	O
d	O
observations	O
for	O
y	O
d	O
in	O
the	O
vasoconstriction	B
data	B
of	O
table	O
and	O
assume	O
that	O
these	O
are	O
a	O
normal	B
sample	B
xi	O
the	O
sufficient	O
statistics	B
for	O
estimating	O
the	O
bivariate	O
parameter	O
d	O
are	O
the	O
sample	B
mean	O
and	O
variance	B
n	O
i	O
d	O
n	O
d	O
nx	O
d	O
nx	O
xi	O
and	O
t	B
d	O
nx	O
having	O
independent	O
normal	B
and	O
gamma	B
distributions	O
nx	O
n	O
and	O
t	B
with	O
d	O
the	O
latter	O
being	O
in	O
the	O
notation	O
of	O
table	O
the	O
two	O
methods	O
are	O
often	O
referred	O
to	O
collectively	O
as	O
mcmc	B
because	O
of	O
mathematical	O
connections	O
with	O
metropolis-hasting	O
algorithm	B
referring	O
to	O
the	O
second	O
type	O
of	O
procedure	O
gibbs	B
sampling	I
and	O
mcmc	B
for	O
our	O
bayes	O
prior	B
distribution	B
we	O
take	O
the	O
conjugates	O
n	O
d	O
in	O
terms	O
of	O
table	O
d	O
for	O
the	O
gamma	B
while	O
for	O
the	O
normal	B
simple	O
specification	O
would	O
take	O
n	O
multiplying	O
the	O
normal	B
and	O
gamma	B
functional	O
forms	O
in	O
table	O
yields	O
and	O
density	B
function	B
t	B
d	O
exp	O
and	O
prior	B
density	B
d	O
exp	O
h	O
c	O
n	O
h	O
c	O
c	O
indicating	O
positive	O
constants	O
that	O
do	O
not	O
affect	O
the	O
posterior	O
computations	B
the	O
posterior	B
density	B
t	B
is	O
then	O
calculated	O
to	O
be	O
t	B
d	O
where	O
q	O
d	O
c	O
t	B
c	O
nc	O
c	O
here	O
nc	O
d	O
c	O
n	O
and	O
d	O
c	O
nnxnc	O
in	O
order	O
to	O
make	O
use	O
of	O
gibbs	B
sampling	I
we	O
need	O
to	O
know	O
the	O
full	B
conditional	B
distributions	O
nx	O
t	B
and	O
nx	O
t	B
as	O
in	O
this	O
case	O
k	O
d	O
d	O
and	O
d	O
this	O
is	O
where	O
the	O
conjugate	B
expressions	O
in	O
table	O
come	O
into	O
play	O
inspection	O
of	O
density	B
shows	O
that	O
n	O
nc	O
and	O
nx	O
t	B
q	O
nx	O
t	B
b	O
d	O
gibbs	O
samples	O
d	O
were	O
generated	O
starting	O
from	O
d	O
t	B
d	O
the	O
prior	B
specifications	O
were	O
chosen	O
to	O
be	O
uninformative	O
or	O
mildly	O
informative	O
d	O
d	O
nx	O
d	O
or	O
and	O
d	O
t	B
which	O
case	O
d	O
nx	O
and	O
q	O
d	O
c	O
c	O
from	O
d	O
we	O
see	O
that	O
corresponds	O
to	O
about	O
hypothetical	O
prior	B
observations	O
the	O
resulting	O
posterior	O
distributions	O
for	O
are	O
shown	O
by	O
the	O
histograms	O
in	O
figure	O
as	O
a	O
point	O
of	O
frequentist	B
comparison	O
b	O
d	O
parametric	B
bootstrap	B
replications	O
involve	O
no	O
prior	B
assumptions	O
d	O
t	B
objective	B
bayes	I
inference	B
and	O
mcmc	B
figure	O
posterior	O
distributions	O
for	O
variance	B
parameter	O
model	O
volume	O
of	O
air	O
inspired	O
for	O
vasoconstriction	B
group	O
y	O
d	O
from	O
table	O
solid	O
teal	O
histogram	O
b	O
d	O
gibbs	O
samples	O
with	O
d	O
black	O
line	O
histogram	O
b	O
d	O
samples	O
with	O
d	O
red	O
line	O
histogram	O
parametric	B
bootstrap	B
samples	O
suggests	O
even	O
the	O
d	O
prior	B
has	O
substantial	O
posterior	O
effect	O
are	O
seen	O
to	O
be	O
noticeably	O
more	O
dispersed	O
than	O
even	O
the	O
d	O
bayes	O
posterior	B
distribution	B
the	O
likely	O
choice	O
for	O
an	O
objective	B
bayes	I
analysis	B
bayes	O
techniques	O
even	O
objective	O
ones	O
have	O
regularization	B
effects	O
that	O
may	O
or	O
may	O
not	O
be	O
appropriate	O
a	O
similar	O
independent	O
gibbs	O
sample	B
of	O
size	O
was	O
obtained	O
for	O
the	O
y	O
d	O
vasoconstriction	B
measurements	O
in	O
table	O
with	O
specifications	O
as	O
in	O
k	O
d	O
let	O
c	O
d	O
where	O
y	O
d	O
and	O
y	O
d	O
runs	O
and	O
denote	O
the	O
bth	O
gibbs	O
samples	O
from	O
the	O
figure	O
shows	O
the	O
posterior	B
distribution	B
of	O
twenty-eight	O
of	O
the	O
gibbs	B
sampling	I
and	O
mcmc	B
figure	O
b	O
gibbs	O
samples	O
for	O
bayes	O
t-statistic	B
comparing	O
y	O
d	O
with	O
y	O
d	O
values	O
for	O
vasoconstriction	B
data	B
b	O
values	O
were	O
less	O
than	O
giving	O
a	O
bayesian	B
t-test	O
estimate	B
pf	O
d	O
usual	O
t-test	O
yielded	O
one-sided	O
p-value	B
against	O
the	O
null	O
hypothesis	O
d	O
an	O
appealing	O
feature	O
of	O
gibbs	B
sampling	I
is	O
that	O
having	O
obtained	O
the	O
posterior	B
distribution	B
of	O
any	O
parameter	O
d	O
t	B
is	O
obtained	O
directly	O
from	O
the	O
b	O
values	O
d	O
t	B
gibbs	B
sampling	I
requires	O
the	O
ability	O
to	O
sample	B
from	O
the	O
full	B
conditional	B
distributions	O
a	O
more	O
general	O
markov	O
chain	O
monte	O
carlo	O
method	B
commonly	O
referred	O
to	O
as	O
mcmc	B
makes	O
clearer	O
the	O
basic	O
idea	O
suppose	O
the	O
space	O
of	O
possible	O
values	O
is	O
finite	O
say	O
and	O
we	O
wish	O
to	O
simulate	O
samples	O
from	O
a	O
posterior	B
distribution	B
putting	O
probability	O
p	O
i	O
on	O
p	O
d	O
p	O
m	O
the	O
mcmc	B
algorithm	B
begins	O
with	O
the	O
choice	O
of	O
a	O
candidate	O
probability	O
distribution	B
q	O
i	O
j	O
for	O
moving	O
from	O
to	O
in	O
theory	B
q	O
i	O
j	O
can	O
be	O
almost	O
anything	O
for	O
instance	O
q	O
i	O
j	O
d	O
for	O
j	O
i	O
the	O
simulated	O
samples	O
are	O
obtained	O
by	O
a	O
random	O
walk	O
if	O
equals	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
then	O
equals	O
with	O
q	O
i	O
j	O
d	O
q	O
i	O
j	O
min	O
p	O
j	O
i	O
p	O
i	O
j	O
for	O
j	O
i	O
while	O
with	O
probability	O
q	O
i	O
i	O
d	O
j	O
i	O
q	O
i	O
j	O
d	O
d	O
markov	B
chain	I
theory	B
then	O
says	O
that	O
under	O
quite	O
general	O
conditions	B
the	O
empirical	B
distribution	B
of	O
the	O
random	O
walk	O
values	O
will	O
approach	O
the	O
desired	O
distribution	B
p	O
as	O
b	O
gets	O
large	O
a	O
heuristic	O
argument	B
for	O
why	O
this	O
happens	O
begins	O
by	O
supposing	O
that	O
was	O
in	O
fact	O
generated	O
by	O
sampling	O
from	O
the	O
target	O
distribution	B
p	O
prf	O
d	O
ig	O
d	O
p	O
i	O
and	O
then	O
was	O
obtained	O
according	O
to	O
transition	O
probabilities	B
a	O
little	O
algebra	O
shows	O
that	O
implies	O
the	O
so-called	O
balance	B
equations	I
this	O
results	O
in	O
n	O
d	O
i	O
pr	O
p	O
i	O
j	O
d	O
p	O
j	O
i	O
o	O
d	O
p	O
i	O
i	O
cx	O
p	O
j	O
i	O
j	O
i	O
d	O
p	O
i	O
q	O
i	O
j	O
d	O
p	O
i	O
mx	O
in	O
other	O
words	O
if	O
has	O
distribution	B
p	O
then	O
so	O
will	O
and	O
likewise	O
p	O
is	O
the	O
equilibrium	B
distribution	B
of	O
the	O
markov	O
chain	O
random	O
walk	O
defined	O
by	O
transition	O
probabilities	B
q	O
under	O
reasonable	O
conditions	B
must	O
asymptotically	O
attain	O
distribution	B
p	O
no	O
matter	O
how	O
is	O
initially	O
selected	O
example	O
modeling	O
population	O
admixture	O
mcmc	B
has	O
had	O
a	O
big	O
impact	O
in	O
statistical	O
genetics	O
where	O
bayesian	B
modeling	O
is	O
popular	O
and	O
useful	O
for	O
representing	O
the	O
complex	O
evolutionary	O
processes	O
here	O
we	O
illustrate	O
its	O
use	O
in	O
demography	O
and	O
modeling	O
admixture	O
estimating	O
the	O
contributions	O
from	O
ancestral	O
populations	O
in	O
an	O
individual	O
in	O
bayes	O
applications	O
p	O
i	O
d	O
g	O
i	O
d	O
g	O
i	O
however	O
f	O
is	O
not	O
needed	O
since	O
it	O
cancels	O
out	O
of	O
a	O
considerable	O
advantage	O
in	O
complicated	O
situations	O
when	O
f	O
is	O
often	O
unavailable	O
and	O
a	O
prime	O
reason	O
for	O
the	O
popularity	O
of	O
mcmc	B
example	O
modeling	O
population	O
admixture	O
genome	B
for	O
example	O
we	O
might	O
consider	O
human	B
ancestry	I
and	O
for	O
each	O
individual	O
wish	O
to	O
estimate	B
the	O
proportion	B
of	O
their	O
genome	B
coming	O
from	O
european	O
african	O
and	O
asian	O
origins	O
the	O
procedure	O
we	O
describe	O
here	O
is	O
unsupervised	O
a	O
type	O
of	O
soft	O
clustering	O
but	O
we	O
will	O
see	O
it	O
can	O
be	O
very	O
informative	O
with	O
regard	O
to	O
such	O
questions	O
we	O
have	O
a	O
sample	B
of	O
n	O
individuals	O
and	O
we	O
assume	O
each	O
arose	O
from	O
possible	O
admixture	O
among	O
j	O
parent	O
populations	O
each	O
with	O
their	O
own	O
characteristic	O
vector	B
of	O
allele	O
frequencies	O
for	O
us	O
j	O
d	O
and	O
let	O
qi	O
denote	O
a	O
probability	O
vector	B
for	O
individual	O
i	O
representing	O
the	O
proportions	O
of	O
their	O
heritage	O
coming	O
from	O
populations	O
j	O
section	O
we	O
have	O
genomic	O
measurements	O
for	O
each	O
individual	O
in	O
our	O
case	O
snps	O
polymorphisms	O
at	O
each	O
of	O
m	O
well-spaced	O
loci	O
and	O
hence	O
can	O
assume	O
they	O
are	O
in	O
linkage	O
equilibrium	O
at	O
each	O
snp	B
we	O
have	O
a	O
measurement	O
that	O
identifies	O
the	O
two	O
alleles	O
per	O
chromosome	O
where	O
each	O
can	O
be	O
either	O
the	O
wild-type	O
a	O
or	O
the	O
mutation	O
a	O
that	O
is	O
we	O
have	O
the	O
genotype	O
gi	O
m	O
at	O
snp	B
m	O
for	O
individual	O
i	O
a	O
three-level	O
factor	B
with	O
levels	O
faa	O
aa	O
aag	O
which	O
we	O
code	O
as	O
table	O
shows	O
some	O
examples	O
table	O
a	O
subset	O
of	O
the	O
genotype	O
data	B
on	O
individuals	O
each	O
with	O
genotype	O
measurements	O
at	O
snps	O
in	O
this	O
case	O
the	O
ethnicity	O
is	O
known	O
for	O
each	O
individual	O
one	O
of	O
japanese	O
african	O
european	O
or	O
african	O
american	O
for	O
example	O
individual	O
has	O
genotype	O
aa	O
for	O
has	O
aa	O
and	O
has	O
aa	O
subject	O
let	O
pj	O
be	O
the	O
m	O
of	O
minor	O
allele	O
frequencies	O
actually	O
in	O
population	O
j	O
we	O
have	O
available	O
a	O
sample	B
of	O
n	O
individuals	O
and	O
for	O
each	O
sample	B
we	O
have	O
their	O
genomic	O
information	B
measured	O
at	O
each	O
of	O
the	O
m	O
loci	O
some	O
of	O
the	O
individuals	O
might	O
appear	O
to	O
have	O
pure	O
ancestral	O
origins	O
but	O
many	O
do	O
not	O
our	O
goal	O
is	O
to	O
estimate	B
qi	O
i	O
d	O
n	O
and	O
pj	O
j	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
i	O
m	O
x	O
for	O
this	O
purpose	O
it	O
is	O
useful	O
to	O
pose	O
a	O
generative	O
model	O
we	O
first	O
create	O
a	O
pair	O
of	O
variables	O
xi	O
m	O
d	O
i	O
m	O
corresponding	O
to	O
each	O
gi	O
m	O
to	O
which	O
we	O
allocate	O
the	O
two	O
alleles	O
arbitrary	O
order	O
for	O
example	O
if	O
gi	O
m	O
d	O
d	O
to	O
aa	O
then	O
we	O
might	O
set	B
x	O
vice	O
versa	O
if	O
gi	O
m	O
d	O
they	O
are	O
both	O
and	O
if	O
gi	O
m	O
d	O
they	O
are	O
both	O
i	O
m	O
let	O
zi	O
m	O
represent	O
the	O
ancestral	O
origin	O
for	O
individual	O
i	O
of	O
each	O
of	O
these	O
allele	O
copies	O
xi	O
m	O
at	O
locus	O
m	O
again	O
a	O
two-vector	O
with	O
elements	O
zi	O
m	O
d	O
d	O
and	O
x	O
i	O
m	O
i	O
m	O
then	O
our	O
generative	O
model	O
goes	O
as	O
follows	O
i	O
m	O
qi	O
independently	O
at	O
each	O
m	O
for	O
each	O
copy	O
c	O
d	O
that	O
is	O
we	O
select	O
the	O
ancestral	O
origin	O
of	O
each	O
chromosome	O
at	O
locus	O
m	O
according	O
to	O
the	O
individual	O
s	O
mixture	O
proportions	O
qi	O
d	O
j	O
for	O
each	O
copy	O
c	O
d	O
what	O
this	O
means	O
is	O
that	O
for	O
each	O
of	O
the	O
two	O
ancestral	O
picks	O
at	O
locus	O
m	O
for	O
each	O
arm	O
of	O
the	O
chromosome	O
we	O
draw	O
a	O
binomial	B
with	O
the	O
appropriate	O
allele	B
frequency	I
pj	O
m	O
if	O
z	O
c	O
i	O
m	O
z	O
c	O
i	O
m	O
x	O
i	O
m	O
to	O
complete	O
the	O
bayesian	B
specification	O
we	O
need	O
to	O
supply	O
priors	B
for	O
the	O
qi	O
and	O
also	O
for	O
pj	O
m	O
although	O
one	O
can	O
get	O
fancy	O
here	O
we	O
resort	O
to	O
the	O
recommended	O
flat	O
priors	B
which	O
are	O
qi	O
a	O
flat	O
three-component	O
dirichlet	O
independently	O
for	O
each	O
subject	O
i	O
and	O
pj	O
m	O
independently	O
for	O
each	O
population	O
j	O
and	O
each	O
locus	O
m	O
beta	B
distribution	B
see	O
in	O
the	O
end	O
notes	O
we	O
use	O
the	O
least-informative	O
values	O
d	O
d	O
in	O
practice	O
these	O
could	O
get	O
updated	O
as	O
well	O
but	O
for	O
the	O
purposes	O
of	O
this	O
demonstration	O
we	O
leave	O
them	O
fixed	O
at	O
these	O
values	O
let	O
x	O
be	O
the	O
n	O
m	O
array	O
of	O
observed	O
alleles	O
for	O
all	O
n	O
samples	O
we	O
wish	O
to	O
estimate	B
the	O
posterior	B
distribution	B
pr	O
p	O
qjx	O
referring	O
collectively	O
to	O
all	O
the	O
elements	O
of	O
p	O
and	O
q	O
for	O
this	O
purpose	O
we	O
use	O
gibbs	B
sampling	I
which	O
amounts	O
to	O
the	O
following	O
sequence	O
initialize	O
p	O
sample	B
z	O
b	O
from	O
the	O
conditional	B
distribution	B
pr	O
zjx	O
p	O
sample	B
p	O
q	O
b	O
from	O
the	O
conditional	B
distribution	B
pr	O
p	O
qjx	O
z	O
b	O
gibbs	O
is	O
effective	O
when	O
one	O
can	O
sample	B
efficiently	O
from	O
these	O
conditional	B
distributions	O
which	O
is	O
the	O
case	O
here	O
example	O
modeling	O
population	O
admixture	O
in	O
step	O
we	O
can	O
sample	B
p	O
and	O
q	O
separately	O
it	O
can	O
be	O
seen	O
that	O
for	O
each	O
m	O
we	O
should	O
sample	B
pj	O
m	O
from	O
pj	O
mjx	O
z	O
c	O
j	O
m	O
c	O
j	O
m	O
d	O
c	O
w	O
x	O
d	O
c	O
w	O
x	O
i	O
m	O
i	O
m	O
d	O
and	O
z	O
c	O
d	O
and	O
z	O
c	O
i	O
m	O
i	O
m	O
d	O
jg	O
d	O
jg	O
where	O
z	O
d	O
z	O
b	O
and	O
j	O
m	O
j	O
m	O
this	O
follows	O
from	O
the	O
conjugacy	O
of	O
the	O
two-component	O
dirichlet	O
with	O
the	O
binomial	B
distribution	B
table	O
updating	O
qi	O
involves	O
simulating	O
from	O
qijx	O
z	O
c	O
c	O
mi	O
c	O
mi	O
where	O
mij	O
is	O
the	O
number	O
of	O
allele	O
copies	O
in	O
individual	O
i	O
that	O
originated	O
to	O
z	O
d	O
z	O
b	O
in	O
population	O
j	O
mij	O
d	O
m	O
w	O
z	O
c	O
i	O
m	O
d	O
jg	O
figure	O
barycentric	O
coordinate	O
plot	O
for	O
the	O
estimated	O
posterior	O
means	O
of	O
the	O
qi	O
based	O
on	O
mcmc	B
sampling	O
step	O
can	O
be	O
performed	O
by	O
simulating	O
z	O
c	O
i	O
m	O
independently	O
for	O
each	O
i	O
m	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllleuropeanjapaneseafricanafrican	O
american	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
and	O
c	O
from	O
pr	O
z	O
c	O
i	O
m	O
d	O
jjx	O
p	O
q	O
d	O
qij	O
pr	O
x	O
i	O
m	O
qi	O
pr	O
x	O
i	O
m	O
jp	O
z	O
c	O
d	O
j	O
i	O
m	O
jp	O
z	O
c	O
i	O
m	O
d	O
the	O
probabilities	B
on	O
the	O
right	O
refer	O
back	O
to	O
our	O
generative	O
distribution	B
described	O
earlier	O
figure	O
shows	O
a	O
triangle	O
plot	O
that	O
summarizes	O
the	O
result	O
of	O
running	O
the	O
mcmc	B
algorithm	B
on	O
our	O
subjects	O
we	O
used	O
a	O
burn	O
in	O
of	O
complete	O
iterations	O
and	O
then	O
a	O
further	O
to	O
estimate	B
the	O
distribution	B
of	O
the	O
parameters	O
of	O
interest	O
in	O
this	O
case	O
the	O
qi	O
each	O
dot	O
in	O
the	O
figure	O
represents	O
a	O
three-component	O
probability	O
vector	B
and	O
is	O
the	O
posterior	O
mean	O
of	O
the	O
sampled	O
qi	O
for	O
each	O
subject	O
the	O
points	O
are	O
colored	O
according	O
to	O
the	O
known	O
ethnicity	O
although	O
this	O
algorithm	B
is	O
unsupervised	O
we	O
see	O
that	O
the	O
ethnic	O
groups	O
cluster	O
nicely	O
in	O
the	O
corners	O
of	O
the	O
simplex	B
and	O
allow	O
us	O
to	O
identify	O
these	O
clusters	O
the	O
african	O
american	O
group	O
is	O
spread	O
between	O
the	O
african	O
and	O
european	O
clusters	O
a	O
little	O
movement	O
toward	O
the	O
japanese	O
markov	O
chain	O
methods	O
are	O
versatile	O
tools	O
that	O
have	O
proved	O
their	O
value	O
in	O
requiring	O
some	O
individual	O
ingenuity	O
with	O
each	O
application	O
bayesian	B
applications	O
there	O
are	O
some	O
drawbacks	O
the	O
algorithms	O
are	O
not	O
universal	O
in	O
the	O
sense	O
of	O
maximum	B
likelihood	B
as	O
a	O
result	O
applications	O
especially	O
of	O
gibbs	B
sampling	I
have	O
favored	O
a	O
small	O
set	B
of	O
convenient	O
priors	B
mainly	O
jeffreys	B
and	O
conjugates	O
that	O
simplify	O
the	O
calculations	O
this	O
can	O
cast	O
doubt	O
on	O
the	O
relevance	B
of	O
the	O
ing	O
the	O
convergence	O
of	O
estimates	O
such	O
as	O
n	O
dp	O
slow	O
resulting	O
bayes	O
inferences	O
successive	O
realizations	O
are	O
highly	O
correlated	O
with	O
each	O
other	O
the	O
correlation	O
makes	O
it	O
difficult	O
to	O
assign	O
a	O
standard	B
error	I
to	O
n	O
actual	O
applications	O
ignore	O
an	O
initial	O
of	O
the	O
values	O
a	O
burn-in	B
period	O
and	O
go	O
on	O
to	O
large	O
enough	O
b	O
such	O
that	O
estimates	O
like	O
n	O
appear	O
to	O
settle	O
down	O
however	O
neither	O
the	O
choice	O
of	O
nor	O
that	O
of	O
b	O
may	O
be	O
clear	O
objective	B
bayes	I
offers	O
a	O
paradigm	O
of	O
our	O
book	O
s	O
theme	O
the	O
effect	O
of	O
electronic	O
computation	O
on	O
statistical	O
inference	B
ingenious	O
new	O
algorithms	O
facilitated	O
bayesian	B
applications	O
over	O
a	O
wide	O
class	O
of	O
applied	O
problems	O
and	O
in	O
doing	O
so	O
influenced	O
the	O
dominant	O
philosophy	O
of	O
the	O
whole	O
area	O
notes	O
and	O
details	O
notes	O
and	O
details	O
the	O
books	O
by	O
savage	B
and	O
de	B
finetti	I
summarizing	O
his	O
earlier	O
work	O
served	O
as	O
foundational	O
texts	O
for	O
the	O
subjective	O
bayesian	B
school	O
of	O
inference	B
highly	O
influential	O
they	O
championed	O
a	O
framework	O
for	O
bayesian	B
applications	O
based	O
on	O
coherent	B
behavior	I
and	O
the	O
careful	O
elucidation	O
of	O
personal	O
probabilities	B
a	O
current	O
leading	O
text	O
on	O
bayesian	B
methods	O
carlin	O
and	O
louis	O
does	O
not	O
reference	O
either	O
savage	B
or	O
de	B
finetti	I
now	O
jeffreys	B
again	O
following	O
earlier	O
works	O
claims	O
foundational	O
status	O
the	O
change	O
of	O
direction	O
has	O
not	O
gone	O
without	O
protest	O
from	O
the	O
subjectivists	O
see	O
adrian	O
smith	O
s	O
discussion	O
of	O
o	O
hagan	O
but	O
is	O
nonetheless	O
almost	O
a	O
complete	O
rout	O
metropolis	O
et	O
al	O
as	O
part	O
of	O
nuclear	O
weapons	O
research	O
developed	O
the	O
first	O
mcmc	B
algorithm	B
a	O
vigorous	O
line	O
of	O
work	O
on	O
markov	O
chain	O
methods	O
for	O
solving	O
difficult	O
probability	O
problems	O
has	O
continued	O
to	O
flourish	O
under	O
such	O
names	O
as	O
particle	O
filtering	O
and	O
sequential	O
monte	O
carlo	O
see	O
gerber	O
and	O
chopin	O
and	O
its	O
enthusiastic	O
discussion	O
modeling	O
population	O
admixture	O
et	O
al	O
is	O
one	O
of	O
several	O
applications	O
of	O
hierarchical	O
bayesian	B
models	B
and	O
mcmc	B
in	O
genetics	O
other	O
applications	O
include	O
haplotype	B
estimation	B
and	O
motif	O
finding	O
as	O
well	O
as	O
estimation	B
of	O
phylogenetic	O
trees	B
the	O
examples	O
in	O
this	O
section	O
were	O
developed	O
with	O
the	O
kind	O
help	O
of	O
hua	O
tang	O
and	O
david	O
golan	O
both	O
from	O
the	O
stanford	O
genetics	O
department	O
hua	O
suggested	O
the	O
example	O
and	O
provided	O
helpful	O
guidance	O
david	O
provided	O
the	O
data	B
and	O
ran	O
the	O
mcmc	B
algorithm	B
using	O
the	O
structure	B
program	O
in	O
the	O
pritchard	O
lab	O
uninformative	O
priors	B
a	O
large	O
catalog	O
of	O
possible	O
uninformative	O
priors	B
has	O
been	O
proposed	O
thoroughly	O
surveyed	O
by	O
kass	O
and	O
wasserman	O
one	O
approach	O
is	O
to	O
use	O
the	O
likelihood	B
from	O
a	O
small	O
part	O
of	O
the	O
data	B
say	O
just	O
one	O
or	O
two	O
data	B
points	O
out	O
of	O
n	O
as	O
the	O
prior	B
as	O
with	O
the	O
intrinsic	O
priors	B
of	O
berger	O
and	O
pericchi	O
or	O
o	O
hagan	O
s	O
fractional	O
bayes	O
factors	O
another	O
approach	O
is	O
to	O
minimize	O
some	O
mathematical	O
measure	O
of	O
prior	B
information	B
as	O
with	O
bernardo	O
s	O
reference	O
priors	B
or	O
jaynes	O
maximum	O
entropy	O
criterion	O
kass	O
and	O
wasserman	O
list	O
a	O
dozen	O
more	O
possibilities	O
coverage	B
matching	I
priors	B
welch	O
and	O
peers	O
showed	O
that	O
for	O
a	O
multiparameter	O
family	O
and	O
real-valued	O
parameter	O
of	O
interest	O
d	O
t	B
there	O
exist	O
priors	B
such	O
that	O
the	O
bayes	O
credible	B
interval	B
of	O
coverage	B
has	O
frequentist	B
coverage	B
with	O
n	O
the	O
sample	B
size	I
in	O
other	O
words	O
the	O
credible	O
intervals	B
are	O
second-order	O
accurate	O
confidence	O
intervals	B
tibshirani	O
building	O
on	O
stein	O
s	O
work	O
produced	O
the	O
objective	B
bayes	I
inference	B
and	O
mcmc	B
nice	O
formulation	O
stein	O
s	O
paper	O
developed	O
the	O
least-favorable	B
family	I
the	O
one-parameter	B
subfamily	O
of	O
that	O
does	O
not	O
inappropriately	O
increase	O
the	O
amount	O
of	O
fisher	B
information	B
for	O
estimating	O
cox	O
and	O
reid	O
s	O
orthogonal	B
parameters	I
form	B
is	O
formally	O
equivalent	O
to	O
the	O
least	O
favorable	O
family	O
construction	O
least	O
favorable	O
family	O
versions	O
of	O
reference	O
priors	B
and	O
intrinsic	O
priors	B
have	O
been	O
proposed	O
to	O
avoid	O
the	O
difficulty	O
with	O
general-purpose	O
uninformative	O
priors	B
seen	O
in	O
figure	O
they	O
do	O
so	O
but	O
at	O
the	O
price	O
of	O
requiring	O
a	O
different	O
prior	B
for	O
each	O
choice	O
of	O
d	O
t	B
which	O
begins	O
to	O
sound	O
more	O
frequentistic	O
than	O
bayesian	B
conjugate	B
families	O
theorem	B
theorem	B
is	O
rigorously	O
derived	O
in	O
diaconis	O
and	O
ylvisaker	O
families	O
other	O
than	O
have	O
conjugate-like	O
properties	O
but	O
not	O
the	O
neat	O
posterior	O
expectation	O
result	O
using	O
d	O
d	O
and	O
v	O
d	O
for	O
the	O
poisson	B
has	O
density	B
e	O
inverse	B
gamma	B
and	O
chi-square	O
distributions	O
a	O
variate	O
an	O
inverse	B
gamma	B
variate	O
has	O
density	B
poisson	B
formula	B
this	O
follows	O
immediately	O
from	O
so	O
d	O
is	O
the	O
gamma	B
conjugate	B
density	B
in	O
table	O
the	O
gamma	B
results	O
can	O
be	O
restated	O
in	O
terms	O
of	O
chi-squared	O
variates	O
d	O
xi	O
m	O
m	O
has	O
conjugate	B
prior	B
an	O
inverse	B
chi-squared	I
distribution	B
vasoconstriction	B
data	B
efron	O
and	O
gous	O
use	O
this	O
data	B
to	O
illustrate	O
a	O
theory	B
connecting	O
bayes	O
factors	O
with	O
fisherian	O
hypothesis	B
testing	B
it	O
is	O
part	O
of	O
a	O
larger	O
data	B
set	B
appearing	O
in	O
finney	O
also	O
discussed	O
in	O
kass	O
and	O
raftery	O
jeffreys	B
and	O
fisher	B
s	O
scales	O
of	O
evidence	O
jeffreys	B
scale	B
as	O
it	O
appears	O
in	O
table	O
is	O
taken	O
from	O
the	O
slightly	O
amended	O
form	B
in	O
kass	O
and	O
raftery	O
efron	O
and	O
gous	O
compare	O
it	O
with	O
fisher	B
s	O
scale	B
for	O
the	O
contradictory	O
results	O
of	O
table	O
fisher	B
and	O
jeffreys	B
worked	O
in	O
different	O
scientific	O
contexts	O
small-sample	O
agricultural	O
experiments	O
versus	O
notes	O
and	O
details	O
hard-science	O
geostatistics	O
which	O
might	O
explain	O
jeffreys	B
more	O
stringent	O
conception	O
of	O
what	O
constitutes	O
significant	O
evidence	O
the	O
bayesian	B
information	B
criterion	I
the	O
bic	O
was	O
proposed	O
by	O
schwarz	O
kass	O
and	O
wasserman	O
provide	O
an	O
extended	O
discussion	O
of	O
the	O
bic	O
and	O
model	B
selection	I
proofs	O
of	O
ultimately	O
depend	O
on	O
sample	B
size	I
coherency	I
as	O
in	O
efron	O
and	O
gous	O
quotation	O
marks	O
are	O
used	O
here	O
to	O
indicate	O
the	O
basically	O
qualitative	O
nature	O
of	O
bic	O
if	O
we	O
think	O
of	O
the	O
data	B
points	O
as	O
being	O
collected	O
in	O
pairs	O
then	O
n	O
becomes	O
in	O
etc	O
so	O
it	O
doesn	O
t	B
pay	O
to	O
put	O
too	O
fine	O
a	O
point	O
on	O
the	O
criterion	O
moreover	O
the	O
convergence	O
is	O
geometric	O
in	O
the	O
normpjp	O
b	O
mcmc	B
convergence	O
suppose	O
we	O
begin	O
the	O
mcmc	B
random	O
walk	O
by	O
choosing	O
according	O
to	O
some	O
arbitrary	O
starting	O
distribution	B
let	O
p	O
b	O
be	O
the	O
distribution	B
of	O
obtained	O
after	O
b	O
steps	O
of	O
the	O
random	O
walk	O
markov	B
chain	I
theory	B
says	O
that	O
under	O
certain	O
broad	O
conditions	B
on	O
q	O
i	O
j	O
p	O
b	O
will	O
converge	O
to	O
the	O
target	O
distribution	B
p	O
successive	O
discrepancies	O
eventually	O
decreasing	O
by	O
a	O
multiplicative	O
factor	B
a	O
proof	O
appears	O
in	O
tanner	O
and	O
wong	O
unfortunately	O
the	O
factor	B
won	O
t	B
be	O
known	O
in	O
most	O
applications	O
and	O
the	O
actual	O
convergence	O
may	O
be	O
quite	O
slow	O
k	O
dirichlet	O
distribution	B
the	O
dirichlet	O
is	O
a	O
multivariate	B
generalization	O
of	O
the	O
beta	B
distribution	B
typically	O
used	O
to	O
represent	O
prior	B
distributions	O
for	O
the	O
multinomial	O
distribution	B
for	O
x	O
d	O
xk	O
j	O
xj	O
d	O
the	O
density	B
is	O
defined	O
as	O
ky	O
x	O
j	O
with	O
xj	O
where	O
dq	O
d	O
j	O
j	O
statistical	O
inference	B
and	O
methodology	O
in	O
the	O
postwar	B
era	I
the	O
fundamentals	O
of	O
statistical	O
inference	B
frequentist	B
bayesian	B
fisherian	O
were	O
set	B
in	O
place	O
by	O
the	O
end	O
of	O
the	O
first	O
half	O
of	O
the	O
twentieth	O
century	O
as	O
discussed	O
in	O
part	O
i	O
of	O
this	O
book	O
the	O
postwar	B
era	I
witnessed	O
a	O
massive	O
expansion	O
of	O
statistical	O
methodology	O
responding	O
to	O
the	O
data-driven	O
demands	O
of	O
modern	O
scientific	O
technology	O
we	O
are	O
now	O
at	O
the	O
end	O
of	O
part	O
ii	O
early	B
computer-age	I
methods	O
having	O
surveyed	O
the	O
march	O
of	O
new	O
statistical	O
algorithms	O
and	O
their	O
inferential	O
justification	O
from	O
the	O
through	O
the	O
this	O
was	O
a	O
time	O
of	O
opportunity	O
for	O
the	O
discipline	O
of	O
statistics	B
when	O
the	O
speed	O
of	O
computation	O
increased	O
by	O
a	O
factor	B
of	O
a	O
thousand	O
and	O
then	O
another	O
thousand	O
as	O
we	O
said	O
before	O
a	O
land	O
bridge	O
had	O
opened	O
to	O
a	O
new	O
continent	O
but	O
not	O
everyone	O
was	O
eager	O
to	O
cross	O
we	O
saw	O
a	O
mixed	O
picture	O
the	O
computer	O
played	O
a	O
minor	O
or	O
negligible	O
role	O
in	O
the	O
development	O
of	O
some	O
influential	O
topics	O
such	O
as	O
empirical	B
bayes	I
but	O
was	O
fundamental	O
to	O
others	O
such	O
as	O
the	O
bootstrap	B
fifteen	O
major	O
topics	O
were	O
examined	O
in	O
chapters	O
through	O
what	O
follows	O
is	O
a	O
short	O
scorecard	O
of	O
their	O
inferential	O
affinities	O
bayesian	B
frequentist	B
or	O
fisherian	O
as	O
well	O
as	O
an	O
assessment	O
of	O
the	O
computer	O
s	O
role	O
in	O
their	O
development	O
none	O
of	O
this	O
is	O
very	O
precise	O
but	O
the	O
overall	O
picture	O
illustrated	O
in	O
figure	O
is	O
evocative	O
empirical	B
bayes	I
robbins	O
original	O
development	O
of	O
formula	B
was	O
frequentistic	O
but	O
most	O
statistical	O
researchers	O
were	O
frequentists	O
in	O
the	O
postwar	B
era	I
so	O
that	O
could	O
be	O
expected	O
the	O
obvious	O
bayesian	B
component	O
of	O
empirical	B
bayes	I
arguments	O
is	O
balanced	O
by	O
their	O
frequentist	B
emphasis	O
on	O
unbiased	O
estimation	B
of	O
bayesian	B
estimators	O
as	O
well	O
as	O
the	O
restriction	O
to	O
using	O
only	O
current	O
data	B
for	O
inference	B
electronic	O
computation	O
played	O
hardly	O
any	O
role	O
in	O
the	O
theory	B
s	O
development	O
indicated	O
by	O
blue	O
coloring	O
in	O
the	O
figure	O
of	O
course	O
mod	O
postwar	O
inference	B
and	O
methodology	O
figure	O
bayesian	B
frequentist	B
and	O
fisherian	O
influences	O
as	O
described	O
in	O
the	O
text	O
on	O
major	O
topics	O
through	O
colors	O
indicate	O
the	O
importance	O
of	O
electronic	O
computation	O
in	O
their	O
development	O
red	O
crucial	O
violet	O
very	O
important	O
green	O
important	O
light	O
blue	O
less	O
important	O
blue	O
negligible	O
ern	O
empirical	B
bayes	I
applications	O
are	O
heavily	O
computational	O
but	O
that	O
is	O
the	O
case	O
for	O
most	O
methods	O
now	O
james	O
stein	O
and	O
ridge	B
regression	B
the	O
frequentist	B
roots	O
of	O
james	O
stein	O
estimation	B
are	O
more	O
definitive	O
especially	O
given	O
the	O
force	O
of	O
the	O
james	O
stein	O
theorem	B
nevertheless	O
the	O
empirical	B
bayes	I
interpretation	O
lends	O
james	O
stein	O
some	O
bayesian	B
credibility	O
electronic	O
computation	O
played	O
no	O
role	O
in	O
its	O
development	O
this	O
was	O
less	O
true	O
for	O
ridge	B
regression	B
colored	O
light	O
blue	O
in	O
the	O
figure	O
where	O
the	O
matrix	B
calculation	O
would	O
have	O
been	O
daunting	O
in	O
the	O
preelectronic	O
age	O
the	O
bayesian	B
justification	O
of	O
ridge	B
regression	B
lllbayesianfrequentistfisherianlkaplan	O
meierllog	O
ranklglmlproportional	O
hazardspartial	O
likelihoodlbootstraplempiricalbayeslobjectivebayesmcmcljackknifelcvljames	O
steinlregression	O
treeslridgeregressionlbiclmissing	O
dataemlaic	O
cp	B
postwar	O
inference	B
and	O
methodology	O
carries	O
more	O
weight	O
than	O
for	O
james	O
stein	O
given	O
the	O
absence	O
of	O
a	O
strong	O
frequentist	B
theorem	B
generalized	O
linear	B
models	B
glm	O
development	O
began	O
with	O
a	O
pronounced	O
fisherian	O
emphasis	O
on	O
modeling	O
but	O
settled	O
down	O
to	O
more	O
or	O
less	O
standard	O
frequentist	B
regression	B
theory	B
a	O
key	O
operational	O
feature	O
low-dimensional	O
sufficient	O
statistics	B
limited	O
its	O
computational	O
demands	O
but	O
glm	O
theory	B
could	O
not	O
have	O
developed	O
before	O
the	O
age	O
of	O
electronic	O
computers	O
indicated	O
by	O
green	O
coloring	O
regression	B
trees	B
model	O
building	O
by	O
means	O
of	O
regression	B
trees	B
is	O
a	O
computationally	O
intensive	O
enterprise	O
indicated	O
by	O
its	O
red	O
color	O
in	O
figure	O
its	O
justification	O
has	O
been	O
mainly	O
in	O
terms	O
of	O
asymptotic	O
frequentist	B
properties	O
survival	B
analysis	B
the	O
kaplan	O
meier	O
estimate	B
log-rank	B
test	I
and	O
proportional	B
hazards	I
model	I
move	O
from	O
the	O
frequentist	B
pole	O
of	O
the	O
diagram	O
toward	O
the	O
fisherian	O
pole	O
as	O
the	O
conditioning	O
arguments	O
in	O
sections	O
through	O
become	O
more	O
elaborate	O
the	O
role	O
of	O
computation	O
in	O
their	O
development	O
increases	O
in	O
the	O
same	O
order	O
kaplan	O
meier	O
estimates	O
can	O
be	O
done	O
by	O
hand	O
were	O
while	O
it	O
is	O
impossible	O
to	O
contemplate	O
proportional	O
hazards	O
analysis	B
without	O
the	O
computer	O
partial	B
likelihood	B
the	O
enabling	O
argument	B
for	O
the	O
theory	B
is	O
a	O
quintessential	O
fisherian	O
device	O
missing	B
data	B
and	O
the	O
em	B
algorithm	B
the	O
imputation	B
of	O
missing	B
data	B
has	O
a	O
bayesian	B
flavor	O
of	O
indirect	B
evidence	I
but	O
the	O
fake	O
data	B
principle	O
has	O
fisherian	O
roots	O
fast	O
computation	O
was	O
important	O
to	O
the	O
method	B
s	O
development	O
particularly	O
so	O
for	O
the	O
em	B
algorithm	B
jackknife	B
and	O
bootstrap	B
the	O
purpose	O
of	O
the	O
jackknife	B
was	O
to	O
calculate	O
frequentist	B
standard	O
errors	B
and	O
biases	O
electronic	O
computation	O
was	O
of	O
only	O
minor	O
importance	O
in	O
its	O
more	O
explicitly	O
quasilikelihoods	O
an	O
extension	O
to	O
a	O
wider	O
class	O
of	O
exponential	B
family	I
models	B
postwar	O
inference	B
and	O
methodology	O
development	O
by	O
contrast	O
the	O
bootstrap	B
is	O
the	O
archetype	O
for	O
computerintensive	O
statistical	O
inference	B
it	O
combines	O
frequentism	B
with	O
fisherian	O
devices	O
plug-in	O
estimation	B
of	O
accuracy	B
estimates	O
as	O
in	O
and	O
correctness	O
arguments	O
for	O
bootstrap	B
confidence	O
intervals	B
cross-validation	B
the	O
renaissance	O
of	O
interest	O
in	O
cross-validation	B
required	O
fast	O
computation	O
especially	O
for	O
assessing	O
modern	O
computer-intensive	B
prediction	O
algorithms	O
as	O
pointed	O
out	O
in	O
the	O
text	O
following	O
figure	O
cross-validation	B
is	O
a	O
strongly	B
frequentist	B
procedure	O
bic	O
aic	O
and	O
cp	B
these	O
three	O
algorithms	O
were	O
designed	O
to	O
avoid	O
computation	O
bic	O
for	O
bayesian	B
model	B
selection	I
section	O
aic	O
and	O
cp	B
for	O
unbiased	O
estimation	B
of	O
frequentist	B
prediction	O
error	O
and	O
objective	B
bayes	I
and	O
mcmc	B
in	O
addition	O
to	O
their	O
bayesian	B
provenance	O
objective	B
bayes	I
methods	O
have	O
some	O
connection	O
with	O
fiducial	B
ideas	O
and	O
the	O
bootstrap	B
as	O
discussed	O
in	O
section	O
argument	B
can	O
be	O
made	O
that	O
they	O
are	O
at	O
least	O
as	O
frequentist	B
as	O
they	O
are	O
bayesian	B
see	O
the	O
notes	O
below	O
though	O
that	O
has	O
not	O
been	O
acted	O
upon	O
in	O
coloring	O
the	O
figure	O
gibbs	B
sampling	I
and	O
mcmc	B
the	O
enabling	O
algorithms	O
epitomize	O
modern	O
computer-intensive	B
inference	B
notes	O
figure	O
is	O
an	O
updated	O
version	O
of	O
figure	O
in	O
efron	O
r	B
a	O
fisher	B
in	O
the	O
century	O
there	O
the	O
difficulty	O
of	O
properly	O
placing	O
objective	B
bayes	I
is	O
confessed	O
with	O
erich	O
lehmann	O
arguing	O
for	O
a	O
more	O
frequentist	B
location	O
in	O
fact	O
the	O
concept	O
of	O
uninformative	B
prior	B
is	O
philosophically	O
close	O
to	O
wald	O
s	O
least	O
favorable	O
distribution	B
and	O
the	O
two	O
often	O
coincide	O
figure	O
shows	O
a	O
healthy	O
mixture	O
of	O
philosophical	O
and	O
computational	O
tactics	O
at	O
work	O
with	O
all	O
three	O
edges	O
not	O
the	O
center	O
of	O
the	O
triangle	O
in	O
play	O
all	O
new	O
points	O
will	O
be	O
red	O
as	O
we	O
move	O
into	O
the	O
twenty-first	O
century	O
in	O
part	O
iii	O
our	O
triangle	O
will	O
have	O
to	O
struggle	O
to	O
accommodate	O
some	O
major	O
developments	O
based	O
on	O
machine	B
learning	I
a	O
philosophically	O
atheistic	O
approach	O
to	O
statistical	O
inference	B
part	O
iii	O
twenty-first-century	O
topics	O
large-scale	O
hypothesis	B
testing	B
and	O
false-discovery	O
rates	O
by	O
the	O
final	O
decade	O
of	O
the	O
twentieth	O
century	O
electronic	O
computation	O
fully	O
dominated	O
statistical	O
practice	O
almost	O
all	O
applications	O
classical	O
or	O
otherwise	O
were	O
now	O
performed	O
on	O
a	O
suite	O
of	O
computer	O
platforms	O
sas	B
spss	B
minitab	B
matlab	B
s	O
r	B
and	O
others	O
the	O
trend	O
accelerates	O
when	O
we	O
enter	O
the	O
twenty-first	O
century	O
as	O
statistical	O
methodology	O
struggles	O
most	O
often	O
successfully	O
to	O
keep	O
up	O
with	O
the	O
vastly	O
expanding	O
pace	O
of	O
scientific	O
data	B
production	O
this	O
has	O
been	O
a	O
twoway	O
game	O
of	O
pursuit	O
with	O
statistical	O
algorithms	O
chasing	O
ever	O
larger	O
data	B
sets	O
while	O
inferential	O
analysis	B
labors	O
to	O
rationalize	O
the	O
algorithms	O
part	O
iii	O
of	O
our	O
book	O
concerns	O
topics	O
in	O
statistics	B
the	O
word	O
topics	O
is	O
intended	O
to	O
signal	O
selections	O
made	O
from	O
a	O
wide	O
catalog	O
of	O
possibilities	O
part	O
ii	O
was	O
able	O
to	O
review	O
a	O
large	O
portion	O
certainly	O
not	O
all	O
of	O
the	O
important	O
developments	O
during	O
the	O
postwar	O
period	O
now	O
deprived	O
of	O
the	O
advantage	O
of	O
hindsight	O
our	O
survey	O
will	O
be	O
more	O
illustrative	O
than	O
definitive	O
for	O
many	O
statisticians	O
microarrays	B
provided	O
an	O
introduction	O
to	O
largescale	O
data	B
analysis	B
these	O
were	O
revolutionary	O
biomedical	O
devices	O
that	O
enabled	O
the	O
assessment	O
of	O
individual	O
activity	O
for	O
thousands	O
of	O
genes	O
at	O
once	O
and	O
in	O
doing	O
so	O
raised	O
the	O
need	O
to	O
carry	O
out	O
thousands	O
of	O
simultaneous	O
hypothesis	O
tests	O
done	O
with	O
the	O
prospect	O
of	O
finding	O
only	O
a	O
few	O
interesting	O
genes	O
among	O
a	O
haystack	O
of	O
null	O
cases	O
this	O
chapter	O
concerns	O
large-scale	O
hypothesis	B
testing	B
and	O
the	O
false-discovery	B
rate	B
the	O
breakthrough	O
in	O
statistical	O
inference	B
it	O
elicited	O
actually	O
what	O
historians	O
might	O
call	O
the	O
long	O
twenty-first	O
century	O
since	O
we	O
will	O
begin	O
in	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
large-scale	B
testing	B
the	O
prostate	B
cancer	O
data	B
figure	O
came	O
from	O
a	O
microarray	O
study	O
of	O
n	O
d	O
men	O
prostate	B
cancer	O
patients	O
and	O
normal	B
controls	O
each	O
man	O
s	O
gene	O
expression	O
levels	O
were	O
measured	O
on	O
a	O
panel	O
of	O
n	O
d	O
genes	O
yielding	O
a	O
matrix	B
of	O
measurements	O
xij	O
xij	O
d	O
activity	O
of	O
ith	O
gene	O
for	O
j	O
th	O
man	O
for	O
each	O
gene	O
a	O
two-sample	B
t	B
statistic	B
ti	O
was	O
computed	O
comparing	O
gene	O
i	O
s	O
expression	O
levels	O
for	O
the	O
patients	O
with	O
those	O
for	O
the	O
controls	O
under	O
the	O
null	O
hypothesis	O
that	O
the	O
patients	O
and	O
the	O
controls	O
responses	O
come	O
from	O
the	O
same	O
normal	B
distribution	B
of	O
gene	O
i	O
expression	O
levels	O
ti	O
will	O
follow	O
a	O
standard	O
student	O
t	B
distribution	B
with	O
degrees	B
of	I
freedom	I
the	O
transformation	O
zi	O
d	O
the	O
inverse	O
function	B
of	O
where	O
is	O
the	O
cdf	B
of	O
a	O
distribution	B
and	O
a	O
standard	O
normal	B
cdf	B
makes	O
zi	O
standard	O
normal	B
under	O
the	O
null	O
hypothesis	O
w	O
zi	O
n	O
of	O
course	O
the	O
investigators	O
were	O
hoping	O
to	O
spot	O
some	O
non-null	B
genes	O
ones	O
for	O
which	O
the	O
patients	O
and	O
controls	O
respond	O
differently	O
it	O
can	O
be	O
shown	O
that	O
a	O
reasonable	O
model	O
for	O
both	O
null	O
and	O
non-null	B
genes	O
zi	O
n	O
being	O
the	O
effect	B
size	I
for	O
gene	O
i	O
null	O
genes	O
have	O
d	O
while	O
the	O
investigators	O
hoped	O
to	O
find	O
genes	O
with	O
large	O
positive	O
or	O
negative	O
effects	O
figure	O
shows	O
the	O
histogram	O
of	O
the	O
zi	O
values	O
the	O
red	O
curve	O
is	O
the	O
scaled	O
n	O
density	B
that	O
would	O
apply	O
if	O
in	O
fact	O
all	O
of	O
the	O
genes	O
were	O
null	O
that	O
is	O
if	O
all	O
of	O
the	O
equaled	O
we	O
can	O
see	O
that	O
the	O
curve	O
is	O
a	O
little	O
too	O
high	O
near	O
the	O
center	O
and	O
too	O
low	O
in	O
the	O
tails	O
good	O
even	O
though	O
most	O
of	O
the	O
genes	O
appear	O
null	O
the	O
discrepancies	O
from	O
the	O
curve	O
suggest	O
that	O
there	O
are	O
some	O
non-null	B
cases	O
the	O
kind	O
the	O
investigators	O
hoped	O
to	O
find	O
large-scale	B
testing	B
refers	O
exactly	O
to	O
this	O
situation	O
having	O
observed	O
a	O
large	O
number	O
n	O
of	O
test	O
statistics	B
how	O
should	O
we	O
decide	O
which	O
if	O
any	O
of	O
the	O
null	O
hypotheses	O
to	O
reject	O
classical	O
testing	B
theory	B
involved	O
only	O
a	O
single	O
case	O
n	O
d	O
a	O
theory	B
of	O
multiple	B
testing	B
arose	O
in	O
the	O
multiple	O
this	O
is	O
model	O
with	O
zi	O
now	O
replacing	O
the	O
notation	O
xi	O
it	O
is	O
ce	O
with	O
c	O
chosen	O
to	O
make	O
the	O
area	O
under	O
the	O
curve	O
equal	O
the	O
area	O
of	O
p	O
the	O
histogram	O
large-scale	B
testing	B
figure	O
histogram	O
of	O
n	O
d	O
z-values	O
one	O
for	O
each	O
gene	O
in	O
the	O
prostate	B
cancer	O
study	O
if	O
all	O
genes	O
were	O
null	O
the	O
histogram	O
would	O
track	O
the	O
red	O
curve	O
for	O
which	O
genes	O
can	O
we	O
reject	O
the	O
null	O
hypothesis	O
meaning	O
n	O
between	O
and	O
perhaps	O
the	O
microarray	O
era	O
produced	O
data	B
sets	O
with	O
n	O
in	O
the	O
hundreds	O
thousands	O
and	O
now	O
even	O
millions	O
this	O
sounds	O
like	O
piling	O
difficulty	O
upon	O
difficulty	O
but	O
in	O
fact	O
there	O
are	O
some	O
inferential	O
advantages	O
to	O
the	O
large-n	O
framework	O
as	O
we	O
will	O
see	O
the	O
most	O
troubling	O
fact	O
about	O
large-scale	B
testing	B
is	O
how	O
easy	O
it	O
is	O
to	O
be	O
fooled	O
running	O
separate	O
hypothesis	O
tests	O
at	O
significance	O
level	O
will	O
produce	O
about	O
five	O
significant	O
results	O
even	O
if	O
each	O
case	O
is	O
actually	O
null	O
the	O
classical	O
bonferroni	B
bound	B
avoids	O
this	O
fallacy	O
by	O
strengthening	O
the	O
threshold	O
of	O
evidence	O
required	O
to	O
declare	O
an	O
individual	O
case	O
significant	O
non-null	B
for	O
an	O
overall	O
significance	O
level	O
perhaps	O
d	O
with	O
n	O
simultaneous	O
tests	O
the	O
bonferroni	B
bound	B
rejects	O
the	O
ith	O
null	O
hypothesis	O
only	O
if	O
it	O
attains	O
individual	O
significance	O
level	O
for	O
d	O
n	O
d	O
and	O
w	O
zi	O
n	O
the	O
one-sided	O
bonferroni	O
threshold	O
for	O
significance	O
is	O
d	O
with	O
for	O
n	O
d	O
only	O
four	O
of	O
the	O
prostate	B
study	O
genes	O
surpass	O
this	O
threshold	O
classic	O
hypothesis	B
testing	B
is	O
usually	O
phrased	O
in	O
terms	O
of	O
significance	O
levels	O
and	O
p-values	O
if	O
test	O
statistic	B
z	O
has	O
cdf	B
under	O
the	O
null	O
hypothesis	O
valuescounts	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
p	O
d	O
is	O
the	O
right-sided	O
p-value	B
larger	O
z	O
giving	O
smaller	O
p-value	B
significance	O
level	O
refers	O
to	O
a	O
prechosen	O
threshold	O
value	O
e	O
g	O
d	O
the	O
null	O
hypothesis	O
is	O
rejected	O
at	O
level	O
if	O
we	O
observe	O
p	O
table	O
on	O
page	O
coverage	B
level	I
means	O
one	O
minus	O
the	O
significance	O
level	O
shows	O
fisher	B
s	O
scale	B
for	O
interpreting	O
p-values	O
a	O
level-	O
test	O
for	O
a	O
single	O
null	O
hypothesis	O
satisfies	O
by	O
definition	O
d	O
prfreject	O
true	O
for	O
a	O
collection	O
of	O
n	O
null	O
hypotheses	O
the	O
family-wise	B
error	I
rate	B
is	O
the	O
probability	O
of	O
making	O
even	O
one	O
false	O
rejection	O
bonferroni	O
s	O
procedure	O
controls	O
fwer	O
at	O
level	O
let	O
be	O
the	O
indices	O
of	O
the	O
true	O
having	O
say	O
members	O
then	O
fwer	O
d	O
prfreject	O
any	O
true	O
n	O
pi	O
pi	O
n	O
pr	O
n	O
o	O
fwer	O
d	O
pr	O
d	O
n	O
the	O
top	O
line	O
following	O
from	O
boole	O
s	O
inequality	O
doesn	O
t	B
require	O
even	O
independence	O
among	O
the	O
pi	O
the	O
bonferroni	B
bound	B
is	O
quite	O
conservative	O
for	O
n	O
d	O
and	O
d	O
we	O
reject	O
only	O
those	O
cases	O
having	O
pi	O
one	O
can	O
do	O
only	O
a	O
little	O
better	O
under	O
the	O
fwer	O
constraint	O
holm	O
s	O
procedure	O
which	O
offers	O
modest	O
improvement	O
over	O
bonferroni	O
goes	O
as	O
follows	O
order	O
the	O
observed	O
p-values	O
from	O
smallest	O
to	O
largest	O
p	O
i	O
p	O
n	O
with	O
denoting	O
the	O
corresponding	O
null	O
hypotheses	O
let	O
be	O
the	O
smallest	O
index	O
i	O
such	O
that	O
p	O
i	O
i	O
c	O
reject	O
all	O
null	O
hypotheses	O
for	O
i	O
and	O
accept	O
all	O
with	O
i	O
the	O
left-sided	O
p-value	B
is	O
p	O
d	O
we	O
will	O
avoid	O
two-sided	O
p-values	O
in	O
this	O
discussion	O
false-discovery	O
rates	O
it	O
can	O
be	O
shown	O
that	O
holm	O
s	O
procedure	O
controls	O
fwer	O
at	O
level	O
while	O
being	O
slightly	O
more	O
generous	O
than	O
bonferroni	O
in	O
declaring	O
rejections	O
false-discovery	O
rates	O
the	O
fwer	O
criterion	O
aims	O
to	O
control	B
the	O
probability	O
of	O
making	O
even	O
one	O
false	O
rejection	O
among	O
n	O
simultaneous	O
hypothesis	O
tests	O
originally	O
developed	O
for	O
small-scale	O
testing	B
say	O
n	O
fwer	O
usually	O
proved	O
too	O
conservative	O
for	O
scientists	O
working	O
with	O
n	O
in	O
the	O
thousands	O
a	O
quite	O
different	O
and	O
more	O
liberal	O
criterion	O
false-discovery	B
rate	B
control	B
has	O
become	O
standard	O
figure	O
a	O
decision	B
rule	B
d	O
has	O
rejected	O
r	B
out	O
of	O
n	O
null	O
hypotheses	O
a	O
of	O
these	O
decisions	O
were	O
incorrect	O
i	O
e	O
they	O
were	O
false	O
discoveries	O
while	O
b	O
of	O
them	O
were	O
true	O
discoveries	O
the	O
false-discovery	O
proportion	B
fdp	O
equals	O
ar	O
figure	O
diagrams	O
the	O
outcome	O
of	O
a	O
hypothetical	O
decision	B
rule	B
d	O
applied	O
to	O
the	O
data	B
for	O
n	O
simultaneous	O
hypothesis-testing	O
problems	O
null	O
and	O
d	O
n	O
non-null	B
an	O
omniscient	O
oracle	B
has	O
reported	O
the	O
rule	B
s	O
results	O
r	B
null	O
hypotheses	O
have	O
been	O
rejected	O
a	O
of	O
these	O
were	O
cases	O
of	O
false	B
discovery	I
i	O
e	O
valid	O
null	O
hypotheses	O
for	O
a	O
false-discovery	O
proportion	B
of	O
fdp	O
d	O
d	O
ar	O
define	O
fdp	O
d	O
if	O
r	B
d	O
fdp	O
is	O
unobservable	O
without	O
the	O
oracle	B
we	O
cannot	O
see	O
a	O
but	O
under	O
certain	O
assumptions	O
we	O
can	O
control	B
its	O
expectation	O
null	O
actual	O
non-null	B
null	O
a	O
non-null	B
a	O
b	O
b	O
n	O
r	B
r	B
n	O
decision	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
define	O
fdr	O
d	O
d	O
e	O
ffdp	O
dg	O
a	O
decision	B
rule	B
d	O
controls	O
fdr	O
at	O
level	O
q	O
with	O
q	O
a	O
prechosen	O
value	O
between	O
and	O
if	O
fdr	O
d	O
q	O
it	O
might	O
seem	O
difficult	O
to	O
find	O
such	O
a	O
rule	B
but	O
in	O
fact	O
a	O
quite	O
simple	O
but	O
ingenious	O
recipe	O
does	O
the	O
job	O
ordering	O
the	O
observed	O
p-values	O
from	O
smallest	O
to	O
largest	O
as	O
in	O
define	O
imax	O
to	O
be	O
the	O
largest	O
index	O
for	O
which	O
p	O
i	O
i	O
n	O
q	O
and	O
let	O
dq	O
be	O
the	O
that	O
rejects	O
for	O
i	O
imax	O
accepting	O
otherwise	O
a	O
proof	O
of	O
the	O
following	O
theorem	B
is	O
referenced	O
in	O
the	O
chapter	O
endnotes	O
theorem	B
hochberg	O
fdr	O
control	B
ing	O
to	O
valid	O
null	O
hypotheses	O
are	O
independent	O
of	O
each	O
other	O
then	O
if	O
the	O
p-values	O
correspond	O
fdr	O
dq	O
d	O
q	O
where	O
d	O
in	O
other	O
words	O
dq	O
controls	O
fdr	O
at	O
level	O
the	O
null	O
proportion	B
is	O
unknown	O
estimable	O
so	O
the	O
usual	O
claim	O
is	O
that	O
dq	O
controls	O
fdr	O
at	O
level	O
q	O
not	O
much	O
is	O
sacrificed	O
large-scale	B
testing	B
problems	O
are	O
most	O
often	O
fishing	O
expeditions	O
in	O
which	O
most	O
of	O
the	O
cases	O
are	O
null	O
putting	O
near	O
identification	O
of	O
a	O
few	O
non-null	B
cases	O
being	O
the	O
goal	O
the	O
choice	O
q	O
d	O
is	O
typical	O
practice	O
the	O
popularity	O
of	O
fdr	O
control	B
hinges	O
on	O
the	O
fact	O
that	O
it	O
is	O
more	O
generous	O
than	O
fwer	O
in	O
declaring	O
holm	O
s	O
procedure	O
rejects	O
null	O
hypothesis	O
if	O
p	O
i	O
thresholdholm	O
s	O
d	O
n	O
i	O
c	O
while	O
dq	O
has	O
threshold	O
p	O
i	O
thresholddq	O
d	O
q	O
n	O
i	O
sometimes	O
denoted	O
bhq	B
after	O
its	O
inventors	O
benjamini	B
and	I
hochberg	I
see	O
the	O
chapter	O
endnotes	O
the	O
classic	O
term	O
significant	O
for	O
a	O
non-null	B
identification	O
doesn	O
t	B
seem	O
quite	O
right	O
for	O
fdr	O
control	B
especially	O
given	O
the	O
bayesian	B
connections	O
of	O
section	O
and	O
we	O
will	O
sometimes	O
use	O
interesting	O
instead	O
false-discovery	O
rates	O
in	O
the	O
usual	O
range	O
of	O
interest	O
large	O
n	O
and	O
small	O
i	O
the	O
ratio	O
thresholddq	O
thresholdholm	O
s	O
d	O
q	O
increases	O
almost	O
linearly	O
with	O
i	O
i	O
i	O
n	O
figure	O
ordered	O
p-values	O
p	O
i	O
d	O
plotted	O
versus	O
i	O
for	O
the	O
largest	O
z-values	O
from	O
the	O
prostate	B
data	B
in	O
figure	O
the	O
fdr	O
control	B
boundary	O
dq	O
q	O
d	O
rejects	O
for	O
the	O
smallest	O
values	O
p	O
i	O
while	O
holm	O
s	O
fwer	O
procedure	O
d	O
rejects	O
for	O
only	O
the	O
smallest	O
values	O
upward	O
slope	O
of	O
holm	O
s	O
boundary	O
is	O
too	O
small	O
to	O
see	O
here	O
figure	O
illustrates	O
the	O
comparison	O
for	O
the	O
right	O
tail	O
of	O
the	O
prostate	B
data	B
of	O
figure	O
with	O
pi	O
d	O
and	O
d	O
q	O
d	O
the	O
fdr	O
procedure	O
rejects	O
for	O
the	O
largest	O
z-values	O
while	O
fwer	O
control	B
rejects	O
only	O
the	O
most	O
extreme	O
z-values	O
hypothesis	B
testing	B
has	O
been	O
a	O
traditional	O
stronghold	O
of	O
frequentist	B
decision	O
theory	B
with	O
type	O
error	O
control	B
being	O
strictly	O
enforced	O
very	O
often	O
at	O
the	O
level	O
it	O
is	O
surprising	O
that	O
a	O
new	O
control	B
criterion	O
fdr	O
has	O
taken	O
hold	O
in	O
large-scale	B
testing	B
situations	O
a	O
critic	O
noting	O
fdr	O
s	O
relaxed	O
rejection	O
standards	O
in	O
figure	O
might	O
raise	O
some	O
pointed	O
questions	O
ip	O
valueholmsfdri	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
is	O
controlling	O
a	O
rate	B
fdr	O
as	O
meaningful	O
as	O
controlling	O
a	O
probabil	O
ity	O
type	O
error	O
the	O
fdr	O
significance	O
for	O
gene	O
say	O
one	O
with	O
isn	O
t	B
this	O
unlikely	O
in	O
situations	O
such	O
as	O
the	O
prostate	B
study	O
how	O
should	O
q	O
be	O
chosen	O
the	O
control	B
theorem	B
depends	O
on	O
independence	O
among	O
the	O
p-values	O
d	O
depends	O
on	O
the	O
results	O
of	O
all	O
the	O
other	O
genes	O
the	O
more	O
other	O
zi	O
values	O
exceed	O
the	O
more	O
interesting	O
gene	O
becomes	O
that	O
increases	O
s	O
index	O
i	O
in	O
the	O
ordered	O
list	O
making	O
it	O
more	O
likely	O
that	O
lies	O
below	O
the	O
dq	O
threshold	O
does	O
this	O
make	O
inferential	O
sense	O
a	O
bayesempirical	O
bayes	O
restatement	O
of	O
the	O
dq	O
algorithm	B
helps	O
answer	O
these	O
questions	O
as	O
discussed	O
next	O
empirical	B
bayes	I
large-scale	B
testing	B
in	O
practice	O
single-case	O
hypothesis	B
testing	B
has	O
been	O
a	O
frequentist	B
preserve	O
its	O
methods	O
demand	O
little	O
from	O
the	O
scientist	O
only	O
the	O
choice	O
of	O
a	O
test	O
statistic	B
and	O
the	O
calculation	O
of	O
its	O
null	O
distribution	B
while	O
usually	O
delivering	O
a	O
clear	O
verdict	O
by	O
contrast	O
bayesian	B
model	B
selection	I
whatever	O
its	O
inferential	O
virtues	O
raises	O
the	O
kinds	O
of	O
difficult	O
modeling	O
questions	O
discussed	O
in	O
section	O
it	O
then	O
comes	O
as	O
a	O
pleasant	O
surprise	O
that	O
things	O
are	O
different	O
for	O
largescale	O
testing	B
bayesian	B
methods	O
at	O
least	O
in	O
their	O
empirical	B
bayes	I
manifestation	O
no	O
longer	O
demand	O
heroic	O
modeling	O
efforts	O
and	O
can	O
help	O
untangle	O
the	O
interpretation	O
of	O
simultaneous	O
test	O
results	O
this	O
is	O
particularly	O
true	O
for	O
the	O
fdr	O
control	B
algorithm	B
dq	O
of	O
the	O
previous	O
section	O
a	O
simple	O
bayesian	B
framework	O
for	O
simultaneous	O
testing	B
is	O
provided	O
by	O
the	O
two-groups	B
model	I
each	O
of	O
the	O
n	O
cases	O
genes	O
for	O
the	O
prostate	B
study	O
is	O
either	O
null	O
with	O
prior	B
probability	O
or	O
non-null	B
with	O
probability	O
d	O
the	O
resulting	O
observation	O
z	O
then	O
has	O
density	B
either	O
or	O
d	O
prfnullg	O
d	O
prfnon-nullg	O
p	O
for	O
the	O
prostate	B
study	O
is	O
nearly	O
and	O
is	O
the	O
standard	O
normal	B
density	B
d	O
while	O
the	O
non-null	B
density	B
remains	O
to	O
be	O
estimated	O
density	B
if	O
null	O
density	B
if	O
non-null	B
let	O
and	O
be	O
the	O
cdf	B
values	O
corresponding	O
to	O
and	O
empirical	B
bayes	I
large-scale	B
testing	B
with	O
survival	O
curves	O
d	O
and	O
d	O
being	O
the	O
probability	O
that	O
a	O
null	O
z-value	O
exceeds	O
and	O
similarly	O
for	O
finally	O
define	O
s	O
z	O
to	O
be	O
the	O
mixture	O
survival	B
curve	I
the	O
mixture	B
density	B
determines	O
s	O
z	O
s	O
z	O
d	O
c	O
f	O
d	O
c	O
dz	O
f	O
dz	O
suppose	O
now	O
that	O
observation	O
zi	O
for	O
case	O
i	O
is	O
seen	O
to	O
exceed	O
some	O
threshold	O
value	O
perhaps	O
d	O
bayes	O
rule	B
gives	O
prfcase	O
i	O
is	O
nulljzi	O
d	O
the	O
correspondence	O
with	O
on	O
page	O
being	O
d	O
d	O
and	O
d	O
f	O
fdr	O
is	O
the	O
bayes	O
false-discovery	B
rate	B
as	O
contrasted	O
with	O
the	O
frequentist	B
quantity	B
fdr	O
in	O
typical	O
applications	O
is	O
assumed	O
in	O
the	O
prostate	B
study	O
and	O
is	O
assumed	O
to	O
be	O
near	O
the	O
denominator	O
in	O
is	O
unknown	O
but	O
and	O
this	O
is	O
the	O
crucial	O
point	O
it	O
has	O
an	O
obvious	O
estimate	B
in	O
large-scale	B
testing	B
situations	O
namely	O
os	O
d	O
where	O
d	O
the	O
definition	O
of	O
the	O
two-group	O
model	O
each	O
zi	O
has	O
marginal	B
density	B
f	O
making	O
os	O
the	O
usual	O
empirical	B
estimate	B
of	O
plugging	O
into	O
yields	O
an	O
empirical	B
bayes	I
estimate	B
of	O
the	O
bayes	O
falsediscovery	O
rate	B
d	O
os	O
the	O
connection	O
with	O
fdr	O
control	B
is	O
almost	O
immediate	O
first	O
of	O
all	O
from	O
definitions	O
and	O
we	O
have	O
pi	O
d	O
also	O
for	O
the	O
ith	O
from	O
the	O
largest	O
z-value	O
we	O
have	O
os	O
d	O
in	O
putting	O
these	O
together	O
condition	B
p	O
i	O
becomes	O
os	O
q	O
but	O
see	O
section	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
or	O
os	O
q	O
which	O
can	O
be	O
written	O
as	O
cfdr	O
z	O
i	O
in	O
other	O
words	O
the	O
dq	O
algorithm	B
which	O
rejects	O
those	O
null	O
hypotheses	O
p	O
i	O
is	O
in	O
fact	O
rejecting	O
those	O
cases	O
for	O
which	O
the	O
empirical	B
bayes	I
posterior	B
probability	I
of	O
nullness	O
is	O
too	O
small	O
as	O
defined	O
by	O
the	O
bayesian	B
nature	O
of	O
fdr	O
control	B
offers	O
a	O
clear	O
advantage	O
to	O
the	O
investigating	O
scientist	O
who	O
gets	O
a	O
numerical	O
assessment	O
of	O
the	O
probability	O
that	O
he	O
or	O
she	O
will	O
be	O
wasting	O
time	O
following	O
up	O
any	O
one	O
of	O
the	O
selected	O
cases	O
we	O
can	O
now	O
respond	O
to	O
the	O
four	O
questions	O
at	O
the	O
end	O
of	O
the	O
previous	O
section	O
fdr	O
control	B
does	O
relate	O
to	O
a	O
probability	O
the	O
bayes	O
posterior	O
probabil	O
ity	O
of	O
nullness	O
the	O
choice	O
of	O
q	O
for	O
dq	O
amounts	O
to	O
setting	O
the	O
maximum	O
tolerable	O
amount	O
of	O
bayes	O
risk	O
of	O
after	O
taking	O
d	O
in	O
nearly	O
unbiased	O
for	O
there	O
most	O
often	O
the	O
zi	O
and	O
hence	O
the	O
pi	O
will	O
be	O
correlated	O
with	O
each	O
other	O
even	O
under	O
correlation	O
however	O
os	O
in	O
is	O
still	O
unbiased	O
for	O
and	O
is	O
a	O
price	O
to	O
be	O
paid	O
for	O
correlation	O
which	O
increases	O
the	O
variance	B
of	O
in	O
the	O
bayes	O
two-groups	B
model	I
all	O
of	O
the	O
non-null	B
zi	O
are	O
i	O
i	O
d	O
observations	O
from	O
the	O
non-null	B
density	B
with	O
survival	B
curve	I
the	O
number	O
of	O
null	O
cases	O
zi	O
exceeding	O
some	O
threshold	O
has	O
fixed	O
expectation	O
n	O
therefore	O
an	O
increase	O
in	O
the	O
number	O
of	O
observed	O
values	O
zi	O
exceeding	O
must	O
come	O
from	O
a	O
heavier	O
right	O
tail	O
for	O
implying	O
a	O
greater	O
posterior	B
probability	I
of	O
non-nullness	O
this	O
point	O
is	O
made	O
more	O
clearly	O
in	O
the	O
local	O
false-discovery	O
framework	O
of	O
the	O
next	O
section	O
it	O
emphasizes	O
the	O
learning	O
from	O
the	O
experience	O
of	O
others	O
aspect	O
of	O
empirical	B
bayes	I
inference	B
section	O
the	O
question	O
of	O
which	O
others	O
is	O
returned	O
to	O
in	O
section	O
figure	O
illustrates	O
the	O
two-group	O
model	O
the	O
n	O
cases	O
are	O
the	O
algorithm	B
as	O
stated	O
just	O
before	O
the	O
fdr	O
control	B
theorem	B
is	O
actually	O
a	O
little	O
more	O
liberal	O
in	O
allowing	O
rejections	O
for	O
a	O
case	O
of	O
particular	O
interest	O
the	O
calculation	O
can	O
be	O
reversed	O
if	O
the	O
case	O
has	O
ordered	O
index	O
i	O
then	O
according	O
to	O
the	O
value	O
q	O
d	O
npi	O
i	O
puts	O
it	O
exactly	O
on	O
the	O
boundary	O
of	O
rejection	O
making	O
this	O
its	O
q-value	B
the	O
largest	O
z-value	O
for	O
the	O
prostate	B
data	B
has	O
zi	O
d	O
pi	O
d	O
and	O
q-value	B
that	O
being	O
both	O
the	O
frequentist	B
boundary	O
for	O
rejection	O
and	O
the	O
empirical	B
bayes	I
probability	O
of	O
nullness	O
empirical	B
bayes	I
large-scale	B
testing	B
figure	O
a	O
diagram	O
of	O
the	O
two-groups	B
model	I
here	O
the	O
statistician	O
observes	O
values	O
zi	O
from	O
a	O
mixture	B
density	B
f	O
d	O
c	O
and	O
decides	O
to	O
reject	O
or	O
accept	O
the	O
null	O
hypothesis	O
depending	O
on	O
whether	O
zi	O
exceeds	O
or	O
is	O
less	O
than	O
the	O
threshold	O
value	O
w	O
d	O
randomly	O
dispatched	O
to	O
the	O
two	O
arms	O
in	O
proportions	O
and	O
at	O
which	O
point	O
they	O
produce	O
z-values	O
according	O
to	O
either	O
or	O
suppose	O
we	O
are	O
using	O
a	O
simple	O
decision	B
rule	B
d	O
that	O
rejects	O
the	O
ith	O
null	O
hypothesis	O
if	O
zi	O
exceeds	O
some	O
threshold	O
and	O
accepts	O
otherwise	O
reject	O
accept	O
if	O
zi	O
if	O
zi	O
the	O
oracle	B
of	O
figure	O
knows	O
that	O
d	O
a	O
of	O
the	O
null	O
case	O
zvalues	O
exceeded	O
and	O
similarly	O
d	O
b	O
of	O
the	O
non-null	B
cases	O
leading	O
to	O
d	O
c	O
d	O
r	B
total	O
rejections	O
the	O
false-discovery	O
proportion	B
is	O
fdp	O
d	O
but	O
this	O
is	O
unobservable	O
since	O
we	O
see	O
only	O
the	O
clever	O
inferential	O
strategy	O
of	O
false-discovery	B
rate	B
theory	B
substitutes	O
the	O
expectation	O
of	O
e	O
d	O
n	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
d	O
for	O
in	O
giving	O
dfdp	O
d	O
n	O
d	O
os	O
using	O
and	O
starting	O
from	O
the	O
two-groups	B
model	I
is	O
an	O
obvious	O
empirical	B
frequentist	B
estimate	B
of	O
the	O
bayesian	B
probability	O
as	O
well	O
as	O
of	O
fdp	O
if	O
placed	O
in	O
the	O
bayes	O
fisher	B
frequentist	B
triangle	O
of	O
figure	O
falsediscovery	O
rates	O
would	O
begin	O
life	O
near	O
the	O
frequentist	B
corner	O
but	O
then	O
migrate	O
at	O
least	O
part	O
of	O
the	O
way	O
toward	O
the	O
bayes	O
corner	O
there	O
are	O
remarkable	O
parallels	O
with	O
the	O
james	O
stein	O
estimator	B
of	O
chapter	O
both	O
theories	O
began	O
with	O
a	O
striking	O
frequentist	B
theorem	B
which	O
was	O
then	O
inferentially	O
rationalized	O
in	O
empirical	B
bayes	I
terms	O
both	O
rely	O
on	O
the	O
use	O
of	O
indirect	B
evidence	I
learning	O
from	O
the	O
experience	O
of	O
others	O
the	O
difference	O
is	O
that	O
james	O
stein	O
estimation	B
always	O
aroused	O
controversy	O
while	O
fdr	O
control	B
has	O
been	O
quickly	O
welcomed	O
into	O
the	O
pantheon	O
of	O
widely	O
used	O
methods	O
this	O
could	O
reflect	O
a	O
change	O
in	O
twenty-first-century	O
attitudes	O
or	O
perhaps	O
only	O
that	O
the	O
dq	O
rule	B
better	O
conceals	O
its	O
bayesian	B
aspects	O
local	O
false-discovery	O
rates	O
tail-area	O
statistics	B
were	O
synonymous	O
with	O
classic	O
one-at-a-time	O
hypothesis	B
testing	B
and	O
the	O
dq	O
algorithm	B
carried	O
over	O
p-value	B
interpretation	O
to	O
large-scale	B
testing	B
theory	B
but	O
tail-area	O
calculations	O
are	O
neither	O
necessary	O
nor	O
desirable	O
from	O
a	O
bayesian	B
viewpoint	O
where	O
having	O
observed	O
test	O
statistic	B
zi	O
equal	O
to	O
some	O
value	O
we	O
should	O
be	O
more	O
interested	O
in	O
the	O
probability	O
of	O
nullness	O
given	O
zi	O
d	O
than	O
given	O
zi	O
to	O
this	O
end	O
we	O
define	O
the	O
local	B
false-discovery	B
rate	B
d	O
prfcase	O
i	O
is	O
nulljzi	O
d	O
as	O
opposed	O
to	O
the	O
tail-area	O
false-discovery	B
rate	B
the	O
main	O
point	O
of	O
what	O
follows	O
is	O
that	O
reasonably	O
accurate	O
empirical	B
bayes	I
estimates	O
of	O
fdr	O
are	O
available	O
in	O
large-scale	B
testing	B
problems	O
as	O
a	O
first	O
try	O
suppose	O
that	O
a	O
proposed	O
region	B
for	O
rejecting	O
null	O
hy	O
potheses	O
is	O
a	O
small	O
interval	B
centered	O
at	O
d	O
d	O
c	O
d	O
with	O
d	O
perhaps	O
we	O
can	O
redraw	O
figure	O
now	O
with	O
local	O
false-discovery	O
rates	O
and	O
the	O
null	O
non-null	B
and	O
total	O
number	O
of	O
z-values	O
in	O
the	O
local	O
false-discovery	O
proportion	B
is	O
unobservable	O
but	O
we	O
can	O
replace	O
with	O
n	O
its	O
approximate	O
expectation	O
as	O
in	O
yielding	O
the	O
d	O
d	O
n	O
estimate	B
would	O
be	O
needlessly	O
noisy	O
in	O
practice	O
z-value	O
distributions	O
tend	O
to	O
be	O
smooth	O
allowing	O
the	O
use	O
of	O
regression	B
estimates	O
for	O
bayes	O
theorem	B
gives	O
fdr	O
z	O
d	O
in	O
the	O
two-groups	B
model	I
in	O
now	O
the	O
indicator	O
of	O
null	O
or	O
non-null	B
states	O
and	O
x	O
now	O
z	O
drawing	O
a	O
smooth	O
curve	O
o	O
f	O
through	O
the	O
histogram	O
of	O
the	O
z-values	O
yields	O
the	O
more	O
efficient	O
estimate	B
d	O
o	O
f	O
cfdr	O
z	O
figure	O
shows	O
cfdr	O
z	O
for	O
the	O
prostate	B
study	O
data	B
of	O
figure	O
the	O
null	O
proportion	B
can	O
be	O
estimated	O
see	O
section	O
or	O
set	B
equal	O
to	O
where	O
o	O
f	O
in	O
has	O
been	O
estimated	O
as	O
described	O
below	O
the	O
curve	O
hovers	O
near	O
for	O
the	O
of	O
the	O
cases	O
having	O
jzij	O
sensibly	O
suggesting	O
that	O
there	O
is	O
no	O
involvement	O
with	O
prostate	B
cancer	O
for	O
most	O
genes	O
it	O
declines	O
quickly	O
for	O
jzij	O
reaching	O
the	O
conventionally	O
interesting	O
threshold	O
for	O
zi	O
and	O
zi	O
this	O
was	O
attained	O
for	O
genes	O
in	O
the	O
right	O
tail	O
and	O
in	O
the	O
left	O
these	O
being	O
reasonable	O
candidates	O
to	O
flag	O
for	O
followup	O
investigation	O
the	O
curve	O
o	O
f	O
used	O
in	O
was	O
obtained	O
from	O
a	O
fourth-degree	O
log	O
polynomial	O
poisson	B
regression	B
fit	O
to	O
the	O
histogram	O
in	O
figure	O
as	O
in	O
figure	O
log	O
polynomials	O
of	O
degree	O
through	O
were	O
fit	O
by	O
maximum	B
likelihood	B
giving	O
total	O
residual	O
deviances	O
shown	O
in	O
table	O
an	O
enormous	O
improvement	O
in	O
fit	O
is	O
seen	O
in	O
going	O
from	O
degree	O
to	O
but	O
nothing	O
significant	O
after	O
that	O
with	O
decreases	O
less	O
than	O
the	O
null	O
value	O
suggested	O
by	O
equation	B
makes	O
argument	B
of	O
the	O
previous	O
section	O
clearer	O
having	O
more	O
other	O
z-values	O
fall	O
into	O
increases	O
and	O
making	O
it	O
more	O
likely	O
that	O
zi	O
d	O
represents	O
a	O
non-null	B
case	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
figure	O
local	B
false-discovery	B
rate	B
estimatecfdr	O
z	O
the	O
left	O
indicated	O
by	O
dashes	O
havecfdr	O
zi	O
light	O
dashed	O
curves	O
are	O
the	O
left	O
and	O
right	O
tail-area	O
estimates	O
cfdr	O
z	O
for	O
prostate	B
study	O
of	O
figure	O
genes	O
on	O
the	O
right	O
and	O
on	O
table	O
total	O
residual	O
deviances	O
from	O
log	O
polynomial	O
poisson	B
regressions	O
of	O
the	O
prostate	B
data	B
for	O
polynomial	O
degrees	O
through	O
degree	O
is	O
preferred	O
degree	O
deviance	B
the	O
points	O
in	O
figure	O
represent	O
the	O
log	O
bin	O
counts	O
from	O
the	O
histogram	O
in	O
figure	O
zero	O
counts	O
with	O
the	O
solid	O
curve	O
showing	O
the	O
mle	B
polynomial	O
fit	O
also	O
shown	O
is	O
the	O
standard	O
normal	B
log	O
density	B
log	O
d	O
c	O
constant	O
it	O
fits	O
reasonably	O
well	O
for	O
jzj	O
emphasizing	O
the	O
null	O
status	O
of	O
the	O
gene	O
the	O
cutoffcfdr	O
z	O
for	O
declaring	O
a	O
case	O
interesting	O
is	O
not	O
completely	O
majority	O
arbitrary	O
definitions	O
and	O
and	O
a	O
little	O
algebra	O
show	O
that	O
it	O
valuefdr	O
and	O
fdrlocal	O
fdr	O
local	O
false-discovery	O
rates	O
figure	O
points	O
are	O
log	O
bin	O
counts	O
for	O
figure	O
s	O
histogram	O
the	O
solid	O
black	O
curve	O
is	O
a	O
fourth-degree	O
log-polynomial	O
fit	O
used	O
to	O
calculatecfdr	O
z	O
in	O
figure	O
the	O
dashed	O
red	O
curve	O
the	O
log	O
null	O
density	B
provides	O
a	O
reasonable	O
fit	O
for	O
jzj	O
is	O
equivalent	O
to	O
if	O
we	O
assume	O
as	O
is	O
reasonable	O
in	O
most	O
large-scale	B
testing	B
situations	O
this	O
makes	O
the	O
bayes	O
factor	B
quite	O
large	O
strong	O
evidence	O
against	O
the	O
null	O
hypothesis	O
in	O
jeffreys	B
scale	B
table	O
there	O
is	O
a	O
simple	O
relation	O
between	O
the	O
local	O
and	O
tail-area	O
false-discovery	O
rates	O
d	O
e	O
ffdr	O
zjz	O
so	O
is	O
the	O
average	O
value	O
of	O
fdr	O
z	O
for	O
z	O
greater	O
than	O
in	O
interesting	O
situations	O
fdr	O
z	O
will	O
be	O
a	O
decreasing	O
function	B
for	O
large	O
values	O
of	O
z	O
as	O
on	O
the	O
right	O
side	O
of	O
figure	O
making	O
this	O
accounts	O
valuelog	O
degree	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
for	O
the	O
conventional	O
significance	O
cutoff	O
cfdr	O
z	O
being	O
smaller	O
than	O
cfdr	O
z	O
as	O
with	O
left-sided	O
and	O
right-sided	O
tail-area	O
cfdr	O
estimates	O
sincecfdr	O
z	O
ap	O
the	O
bayesian	B
interpretation	O
of	O
local	O
false-discovery	O
rates	O
carries	O
with	O
it	O
the	O
advantages	O
of	O
bayesian	B
coherency	O
we	O
don	O
t	B
have	O
to	O
change	O
definitions	O
plies	O
without	O
change	O
to	O
both	O
also	O
we	O
don	O
t	B
need	O
a	O
separate	O
theory	B
for	O
true-discovery	B
rates	I
since	O
d	O
is	O
the	O
conditional	B
probability	O
that	O
case	O
i	O
is	O
non-null	B
given	O
zi	O
d	O
choice	O
of	O
the	O
null	O
distribution	B
the	O
null	O
distribution	B
in	O
the	O
two-groups	B
model	I
plays	O
a	O
crucial	O
role	O
in	O
large-scale	B
testing	B
just	O
as	O
it	O
does	O
in	O
the	O
classic	O
single-case	O
theory	B
something	O
different	O
however	O
happens	O
in	O
large-scale	O
problems	O
with	O
thousands	O
of	O
z-values	O
to	O
examine	O
at	O
once	O
it	O
can	O
become	O
clear	O
that	O
the	O
conventional	O
theoretical	B
null	I
is	O
inappropriate	O
for	O
the	O
situation	O
at	O
hand	O
put	O
more	O
positively	O
large-scale	O
applications	O
may	O
allow	O
us	O
to	O
empirically	O
determine	O
a	O
more	O
realistic	O
null	O
distribution	B
the	O
police	B
data	B
of	O
figure	O
illustrates	O
what	O
can	O
happen	O
possible	O
racial	O
bias	B
in	O
pedestrian	O
stops	O
was	O
assessed	O
for	O
n	O
d	O
new	O
york	O
city	O
police	B
officers	O
in	O
each	O
officer	O
was	O
assigned	O
a	O
score	O
zi	O
large	O
positive	O
scores	O
suggesting	O
racial	O
bias	B
the	O
zi	O
values	O
were	O
summary	O
scores	O
from	O
a	O
complicated	O
logistic	B
regression	B
model	I
intended	O
to	O
compensate	O
for	O
differences	O
in	O
the	O
time	O
of	O
day	O
location	O
and	O
context	O
of	O
the	O
stops	O
logistic	B
regression	B
theory	B
suggested	O
the	O
theoretical	B
null	I
distribution	B
w	O
zi	O
n	O
for	O
the	O
absence	O
of	O
racial	O
bias	B
the	O
trouble	O
is	O
that	O
the	O
center	O
of	O
the	O
z-value	O
histogram	O
in	O
figure	O
which	O
should	O
track	O
the	O
n	O
curve	O
applying	O
to	O
the	O
presumably	O
large	O
fraction	O
of	O
null-case	O
officers	O
is	O
much	O
too	O
wide	O
the	O
situation	O
for	O
the	O
prostate	B
data	B
in	O
figure	O
an	O
mle	B
fitting	O
algorithm	B
discussed	O
below	O
produced	O
the	O
empirical	B
null	I
w	O
zi	O
n	O
going	O
further	O
z	O
in	O
the	O
two-groups	B
model	I
could	O
be	O
multidimensional	O
then	O
tail-area	O
false-discovery	O
rates	O
would	O
be	O
unavailable	O
but	O
would	O
still	O
legitimately	O
define	O
fdr	O
z	O
choice	O
of	O
the	O
null	O
distribution	B
figure	O
police	B
data	B
histogram	O
of	O
z	O
scores	O
for	O
n	O
d	O
new	O
york	O
city	O
police	B
officers	O
with	O
large	O
zi	O
suggesting	O
racial	O
bias	B
the	O
center	O
of	O
the	O
histogram	O
is	O
too	O
wide	O
compared	O
with	O
the	O
theoretical	B
null	I
distribution	B
zi	O
n	O
an	O
mle	B
fit	O
to	O
central	O
data	B
gave	O
n	O
as	O
empirical	B
null	I
there	O
is	O
a	O
lot	O
at	O
stake	O
here	O
based	O
on	O
the	O
empirical	B
null	I
only	O
the	O
four	O
circled	O
points	O
at	O
the	O
far	O
right	O
of	O
figure	O
the	O
fifth	O
point	O
had	O
as	O
appropriate	O
here	O
this	O
is	O
reinforced	O
by	O
a	O
qq	B
plot	I
of	O
the	O
zi	O
values	O
shown	O
in	O
figure	O
where	O
we	O
see	O
most	O
of	O
the	O
cases	O
falling	O
nicely	O
along	O
a	O
n	O
line	O
with	O
just	O
a	O
few	O
outliers	O
at	O
both	O
extremes	O
four	O
officers	O
reached	O
the	O
probably	O
racially	O
biased	O
cutoffcfdr	O
zi	O
cfdr	O
d	O
while	O
all	O
the	O
others	O
exceeded	O
the	O
theoretical	O
n	O
null	O
was	O
much	O
more	O
severe	O
assigningcfdr	O
to	O
the	O
officers	O
having	O
zi	O
one	O
can	O
imagine	O
the	O
difference	O
in	O
newspaper	O
headlines	O
from	O
a	O
classical	O
point	O
of	O
view	O
it	O
seems	O
heretical	O
to	O
question	O
the	O
theoretical	B
null	I
distribution	B
especially	O
since	O
there	O
is	O
no	O
substitute	O
available	O
in	O
single-case	O
testing	B
once	O
alerted	O
by	O
data	B
sets	O
like	O
the	O
police	B
study	O
however	O
it	O
is	O
easy	O
to	O
list	O
reasons	O
for	O
doubt	O
asymptotics	B
taylor	B
series	I
approximations	O
go	O
into	O
theoretical	B
null	I
calculations	O
such	O
as	O
which	O
can	O
lead	O
to	O
inaccuracies	O
particularly	O
in	O
the	O
crucial	O
tails	O
of	O
the	O
null	O
distribution	B
correlations	O
false-discovery	B
rate	B
methods	O
are	O
correct	O
on	O
the	O
average	O
z	O
valuesfrequency	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
figure	O
qq	B
plot	I
of	O
police	B
data	B
z	O
scores	O
most	O
scores	O
closely	O
follow	O
the	O
n	O
line	O
with	O
a	O
few	O
outliers	O
at	O
either	O
end	O
the	O
circled	O
points	O
are	O
cases	O
having	O
local	O
false-discovery	O
estimate	B
cfdr	O
zi	O
based	O
on	O
the	O
empirical	B
null	I
using	O
the	O
theoretical	O
n	O
null	O
gives	O
cases	O
withcfdr	O
zi	O
on	O
the	O
left	O
and	O
on	O
the	O
right	O
even	O
with	O
correlations	O
among	O
the	O
n	O
z-values	O
however	O
severe	O
correlation	O
destabilizes	O
the	O
z-value	O
histogram	O
which	O
can	O
become	O
randomly	O
wider	O
or	O
narrower	O
than	O
theoretically	O
predicted	O
undermining	O
theoretical	B
null	I
results	O
for	O
the	O
data	B
set	B
at	O
hand	O
unobserved	B
covariates	I
the	O
police	B
study	O
was	O
observational	O
individual	O
encounters	O
were	O
not	O
assigned	O
at	O
random	O
to	O
the	O
various	O
officers	O
but	O
simply	O
observed	O
as	O
they	O
happened	O
observed	O
covariates	O
such	O
as	O
the	O
time	O
of	O
day	O
and	O
the	O
neighborhood	O
were	O
included	O
in	O
the	O
logistic	B
regression	B
model	I
but	O
one	O
can	O
never	O
rule	B
out	O
the	O
possibility	O
of	O
influential	O
unobserved	B
covariates	I
effect	B
size	I
considerations	O
the	O
hypothesis-testing	O
setup	O
where	O
a	O
large	O
fraction	O
of	O
the	O
cases	O
are	O
truly	O
null	O
may	O
not	O
be	O
appropriate	O
an	O
effect	B
size	I
model	O
with	O
and	O
zi	O
n	O
might	O
apply	O
with	O
the	O
prior	B
not	O
having	O
an	O
atom	O
at	O
d	O
the	O
nonatomic	O
choice	O
n	O
provides	O
a	O
good	O
fit	O
to	O
the	O
qq	B
plot	I
in	O
figure	O
quantilessample	O
quantileslllllllllintercept	O
choice	O
of	O
the	O
null	O
distribution	B
empirical	B
null	I
estimation	B
our	O
point	O
of	O
view	O
here	O
is	O
that	O
the	O
theoretical	B
null	I
zi	O
n	O
is	O
not	O
completely	O
wrong	O
but	O
needs	O
adjustment	O
for	O
the	O
data	B
set	B
at	O
hand	O
to	O
this	O
end	O
we	O
assume	O
the	O
two-groups	B
model	I
with	O
normal	B
but	O
not	O
necessarily	O
n	O
say	O
n	O
in	O
order	O
to	O
compute	O
the	O
local	B
false-discovery	B
rate	B
fdr	O
z	O
d	O
we	O
want	O
to	O
estimate	B
the	O
three	O
numerator	O
parameters	O
the	O
mean	O
and	O
standard	B
deviation	I
of	O
the	O
null	O
density	B
and	O
the	O
proportion	B
of	O
null	O
cases	O
denominator	O
f	O
is	O
estimated	O
as	O
in	O
section	O
our	O
key	O
assumptions	O
are	O
that	O
is	O
large	O
say	O
and	O
that	O
most	O
of	O
the	O
zi	O
near	O
are	O
null	O
cases	O
the	O
algorithm	B
locfdr	B
begins	O
by	O
selecting	O
a	O
set	B
near	O
z	O
d	O
in	O
which	O
it	O
is	O
assumed	O
that	O
all	O
the	O
zi	O
in	O
are	O
null	O
in	O
terms	O
of	O
the	O
two-groups	B
model	I
the	O
assumption	O
can	O
be	O
stated	O
as	O
d	O
for	O
z	O
modest	O
violations	O
of	O
which	O
are	O
to	O
be	O
expected	O
produce	O
small	O
biases	O
in	O
the	O
empirical	B
null	I
estimates	O
maximum	B
likelihood	B
based	O
on	O
the	O
number	O
and	O
values	O
of	O
the	O
zi	O
observed	O
in	O
yield	O
the	O
empirical	B
null	I
estimates	O
applied	O
to	O
the	O
police	B
data	B
locfdr	B
chose	O
d	O
and	O
pro	O
o	O
duced	O
estimates	O
d	O
two	O
small	O
simulation	B
studies	O
described	O
in	O
table	O
give	O
some	O
idea	O
of	O
the	O
variabilities	O
and	O
biases	O
inherent	O
in	O
the	O
locfdr	B
estimation	B
process	O
the	O
third	O
method	B
somewhere	O
between	O
the	O
theoretical	O
and	O
empirical	B
null	I
estimates	O
but	O
closer	O
to	O
the	O
former	O
relies	O
on	O
permutations	O
the	O
vector	B
z	O
of	O
z-values	O
for	O
the	O
prostate	B
data	B
of	O
figure	O
was	O
obtained	O
from	O
a	O
study	O
of	O
men	O
cancer	O
patients	O
and	O
controls	O
randomly	O
permuting	O
the	O
men	O
s	O
data	B
that	O
is	O
randomly	O
choosing	O
of	O
the	O
to	O
be	O
controls	O
and	O
the	O
remaining	O
to	O
be	O
patients	O
and	O
then	O
carrying	O
through	O
steps	O
in	O
which	O
any	O
actual	O
cancercontrol	O
differences	O
have	O
gives	O
a	O
vector	B
z	O
been	O
suppressed	O
a	O
histogram	O
of	O
the	O
z	O
i	O
values	O
combining	O
several	O
permutations	O
provides	O
the	O
permutation	B
null	I
here	O
we	O
are	O
extending	O
fisher	B
s	O
original	O
permutation	O
idea	O
section	O
to	O
large-scale	B
testing	B
ten	O
permutations	O
of	O
the	O
prostate	B
study	O
data	B
produced	O
an	O
almost	O
perfect	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
o	O
for	O
two	O
table	O
means	O
and	O
standard	B
deviations	I
of	O
simulation	B
studies	O
of	O
empirical	B
null	I
estimation	B
using	O
locfdr	B
n	O
d	O
cases	O
each	O
trial	O
with	O
as	O
shown	O
trials	O
two-groups	B
model	I
with	O
non-null	B
density	B
equal	O
to	O
n	O
side	O
or	O
n	O
side	O
true	O
mean	O
st	O
dev	O
n	O
permutation	B
null	I
is	O
as	O
expected	O
from	O
the	O
classic	O
theory	B
of	O
permutation	O
t-tests	O
permutation	O
methods	O
reliably	O
overcome	O
objection	O
to	O
the	O
theoretical	B
null	I
distribution	B
over-reliance	O
on	O
asymptotic	O
approximations	O
but	O
cannot	O
cure	O
objections	O
and	O
whatever	O
the	O
cause	O
of	O
disparity	O
the	O
operational	O
difference	O
between	O
the	O
theoretical	O
and	O
empirical	B
null	I
distribution	B
is	O
clear	O
with	O
the	O
latter	O
the	O
significance	O
of	O
an	O
outlying	O
case	O
is	O
judged	O
relative	O
to	O
the	O
dispersion	O
of	O
the	O
majority	O
not	O
by	O
a	O
theoretical	O
yardstick	O
as	O
with	O
the	O
former	O
this	O
was	O
persuasive	O
for	O
the	O
police	B
data	B
but	O
the	O
story	O
isn	O
t	B
one-sided	O
estimating	O
the	O
null	O
distribution	B
adds	O
substantially	O
to	O
the	O
variability	O
ofcfdr	O
orcfdr	O
for	O
situations	O
such	O
as	O
the	O
prostate	B
data	B
when	O
the	O
theoretical	B
null	I
looks	O
nearly	O
it	O
is	O
reasonable	O
to	O
stick	O
with	O
it	O
the	O
very	O
large	O
data	B
sets	O
of	O
twenty-first-century	O
applications	O
encourage	O
self-contained	O
methodology	O
that	O
proceeds	O
from	O
just	O
the	O
data	B
at	O
hand	O
using	O
a	O
minimum	O
of	O
theoretical	O
constructs	O
false-discovery	B
rate	B
empirical	B
bayes	I
analysis	B
of	O
large-scale	B
testing	B
problems	O
with	O
data-based	O
estimation	B
of	O
o	O
and	O
o	O
f	O
comes	O
close	O
to	O
the	O
ideal	O
in	O
this	O
sense	O
relevance	B
false-discovery	O
rates	O
return	O
us	O
to	O
the	O
purview	O
of	O
indirect	B
evidence	I
sections	O
and	O
our	O
interest	O
in	O
any	O
one	O
gene	O
in	O
the	O
prostate	B
cancer	O
study	O
depends	O
on	O
its	O
own	O
z	O
score	O
of	O
course	O
but	O
also	O
on	O
the	O
other	O
genes	O
scores	O
learning	O
from	O
the	O
experience	O
of	O
others	O
in	O
the	O
language	O
used	O
before	O
the	O
crucial	O
question	O
we	O
have	O
been	O
avoiding	O
is	O
which	O
others	O
our	O
tacit	O
answer	O
has	O
been	O
all	O
the	O
cases	O
that	O
arrive	O
in	O
the	O
same	O
data	B
set	B
all	O
the	O
genes	O
the	O
locfdr	B
algorithm	B
gave	O
d	O
for	O
the	O
prostate	B
data	B
relevance	B
in	O
the	O
prostate	B
study	O
all	O
the	O
officers	O
in	O
the	O
police	B
study	O
why	O
this	O
can	O
be	O
a	O
dangerous	O
tactic	O
is	O
shown	O
in	O
our	O
final	O
example	O
a	O
dti	O
tensor	O
imaging	O
study	O
compared	O
six	O
dyslexic	O
children	O
with	O
six	O
normal	B
controls	O
each	O
dti	O
scan	O
recorded	O
fluid	O
flows	O
at	O
n	O
voxels	O
i	O
e	O
at	O
three-dimensional	O
brain	O
coordinates	O
a	O
score	O
zi	O
comparing	O
dyslexics	O
with	O
normal	B
controls	O
was	O
calculated	O
for	O
each	O
voxel	O
i	O
calibrated	O
such	O
that	O
the	O
theoretical	B
null	I
distribution	B
of	O
no	O
difference	O
was	O
w	O
zi	O
n	O
as	O
at	O
figure	O
histogram	O
of	O
z	O
scores	O
for	O
the	O
dti	O
study	O
comparing	O
dyslexic	O
versus	O
normal	B
control	B
children	O
at	O
brain	O
locations	O
a	O
fdr	O
analysis	B
based	O
on	O
the	O
empirical	B
null	I
distribution	B
gave	O
voxels	O
withcfdr	O
zi	O
those	O
having	O
zi	O
by	O
red	O
dashes	O
figure	O
shows	O
the	O
histogram	O
of	O
all	O
zi	O
values	O
normal-looking	O
near	O
the	O
center	O
and	O
with	O
a	O
heavy	O
right	O
tail	O
locfdr	B
gave	O
empirical	B
null	I
parameters	O
d	O
the	O
voxels	O
with	O
zi	O
havingcfdr	O
values	O
using	O
the	O
the	O
z	O
scorefrequency	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
voxels	O
with	O
zi	O
havingcfdri	O
oretical	O
null	O
yielded	O
only	O
modestly	O
different	O
results	O
now	O
the	O
figure	O
a	O
plot	O
of	O
zi	O
scores	O
from	O
a	O
dti	O
study	O
axis	O
and	O
voxel	O
distances	O
xi	O
from	O
the	O
back	O
of	O
the	O
brain	O
axis	O
the	O
starred	O
points	O
are	O
the	O
voxels	O
with	O
cfdr	O
zi	O
which	O
occur	O
mostly	O
for	O
xi	O
in	O
the	O
interval	B
in	O
figure	O
the	O
voxel	O
scores	O
zi	O
graphed	O
vertically	O
are	O
plotted	O
versus	O
xi	O
the	O
voxel	O
s	O
distance	O
from	O
the	O
back	O
of	O
the	O
brain	O
waves	O
of	O
differing	O
response	B
are	O
apparent	O
larger	O
values	O
occur	O
in	O
the	O
interval	B
x	O
up	O
most	O
of	O
the	O
voxels	O
havingcfdri	O
occur	O
at	O
the	O
top	O
of	O
this	O
wave	O
where	O
the	O
entire	O
z-value	O
distribution	B
low	O
medium	O
and	O
high	O
is	O
pushed	O
figure	O
raises	O
the	O
problem	O
of	O
fair	O
comparison	O
perhaps	O
the	O
voxels	O
with	O
xi	O
between	O
and	O
should	O
be	O
compared	O
only	O
with	O
each	O
other	O
and	O
not	O
with	O
all	O
cases	O
doing	O
so	O
gave	O
d	O
only	O
voxels	O
havingcfdri	O
those	O
with	O
zi	O
all	O
of	O
this	O
is	O
a	O
question	O
of	O
relevance	B
which	O
other	O
voxels	O
i	O
are	O
relevant	O
to	O
the	O
assessment	O
of	O
significance	O
for	O
voxel	O
one	O
might	O
argue	O
that	O
this	O
is	O
a	O
question	O
for	O
the	O
scientist	O
who	O
gathers	O
the	O
data	B
and	O
not	O
for	O
the	O
statistical	O
analyst	O
but	O
that	O
is	O
unlikely	O
to	O
be	O
a	O
fruitful	O
avenue	O
at	O
least	O
not	O
without	O
xz	O
relevance	B
a	O
lot	O
of	O
back-and-forth	O
collaboration	O
standard	O
bayesian	B
analysis	B
solves	O
the	O
problem	O
by	O
dictate	O
the	O
assertion	O
of	O
a	O
prior	B
is	O
also	O
an	O
assertion	O
of	O
its	O
relevance	B
empirical	B
bayes	I
situations	O
expose	O
the	O
dangers	O
lurking	O
in	O
such	O
assertions	O
relevance	B
was	O
touched	O
upon	O
in	O
section	O
where	O
the	O
limited	O
translation	O
rule	B
was	O
designed	O
to	O
protect	O
extreme	O
cases	O
from	O
being	O
shrunk	O
too	O
far	O
toward	O
the	O
bulk	O
of	O
ordinary	O
ones	O
one	O
could	O
imagine	O
having	O
a	O
relevance	B
function	B
zi	O
that	O
given	O
the	O
covariate	O
information	B
xi	O
and	O
response	B
zi	O
for	O
casei	O
somehow	O
adjusts	O
an	O
ensemble	B
false-discovery	B
rate	B
estimate	B
to	O
correctly	O
apply	O
to	O
the	O
case	O
of	O
interest	O
but	O
such	O
a	O
theory	B
barely	O
exists	O
summary	O
combine	O
frequentist	B
and	O
bayesian	B
thinking	O
large-scale	B
testing	B
particularly	O
in	O
its	O
false-discovery	B
rate	B
implementation	O
is	O
not	O
at	O
all	O
the	O
same	O
thing	O
as	O
the	O
classic	O
fisher	B
neyman	O
pearson	O
theory	B
frequentist	B
single-case	O
hypothesis	B
testing	B
depends	O
on	O
the	O
theoretical	O
long-run	O
behavior	O
of	O
samples	O
from	O
the	O
theoretical	B
null	I
distribution	B
with	O
data	B
available	O
from	O
say	O
n	O
d	O
simultaneous	O
tests	O
the	O
statistician	O
has	O
his	O
or	O
her	O
own	O
long	O
run	O
in	O
hand	O
diminishing	O
the	O
importance	O
of	O
theoretical	O
modeling	O
in	O
particular	O
the	O
data	B
may	O
cast	O
doubt	O
on	O
the	O
theoretical	B
null	I
providing	O
a	O
more	O
appropriate	O
empirical	B
null	I
distribution	B
in	O
its	O
place	O
classic	O
testing	B
theory	B
is	O
purely	O
frequentist	B
whereas	O
false-discovery	O
rates	O
on	O
its	O
own	O
score	O
zi	O
while	O
cfdr	O
zi	O
or	O
cfdr	O
zi	O
also	O
depends	O
on	O
the	O
in	O
classic	O
testing	B
the	O
attained	O
significance	O
level	O
for	O
case	O
i	O
depends	O
only	O
applications	O
of	O
single-test	O
theory	B
usually	O
hope	O
for	O
rejection	O
of	O
the	O
null	O
hypothesis	O
a	O
familiar	O
prescription	O
being	O
power	O
at	O
size	O
the	O
opposite	O
is	O
true	O
for	O
large-scale	B
testing	B
where	O
the	O
usual	O
goal	O
is	O
to	O
accept	O
most	O
of	O
the	O
null	O
hypotheses	O
leaving	O
just	O
a	O
few	O
interesting	O
cases	O
for	O
further	O
study	O
sharp	O
null	O
hypotheses	O
such	O
as	O
d	O
are	O
less	O
important	O
in	O
large-scale	O
applications	O
where	O
the	O
statistician	O
is	O
happy	O
to	O
accept	O
a	O
hefty	O
proportion	B
of	O
uninterestingly	O
small	O
but	O
nonzero	O
effect	O
sizes	O
false-discovery	B
rate	B
hypothesis	B
testing	B
involves	O
a	O
substantial	O
amount	O
of	O
estimation	B
blurring	O
the	O
line	O
beteen	O
the	O
two	O
main	O
branches	O
of	O
statistical	O
inference	B
served	O
z-values	O
for	O
other	O
cases	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
notes	O
and	O
details	O
the	O
story	O
of	O
false-discovery	O
rates	O
illustrates	O
how	O
developments	O
in	O
scientific	O
technology	O
in	O
this	O
case	O
can	O
influence	O
the	O
progress	O
of	O
statistical	O
inference	B
a	O
substantial	O
theory	B
of	O
simultaneous	B
inference	B
was	O
developed	O
between	O
and	O
mainly	O
aimed	O
at	O
the	O
frequentist	B
control	B
of	O
family-wise	O
error	O
rates	O
in	O
situations	O
involving	O
a	O
small	O
number	O
of	O
hypothesis	O
tests	O
maybe	O
up	O
to	O
good	O
references	O
are	O
miller	O
and	O
westfall	O
and	O
young	O
benjamini	B
and	I
hochberg	I
s	O
seminal	O
paper	O
introduced	O
false-discovery	O
rates	O
at	O
just	O
the	O
right	O
time	O
to	O
catch	O
the	O
wave	O
of	O
large-scale	O
data	B
sets	O
now	O
involving	O
thousands	O
of	O
simultaneous	O
tests	O
generated	O
by	O
microarray	O
applications	O
most	O
of	O
the	O
material	O
in	O
this	O
chapter	O
is	O
taken	O
from	O
efron	O
where	O
the	O
empirical	B
bayes	I
nature	O
of	O
fdr	O
theory	B
is	O
emphasized	O
the	O
police	B
data	B
is	O
discussed	O
and	O
analyzed	O
at	O
length	O
in	O
ridgeway	O
and	O
macdonald	O
model	O
section	O
of	O
efron	O
discusses	O
the	O
following	O
result	O
for	O
the	O
non-null	B
distribution	B
of	O
z-values	O
a	O
transformation	O
such	O
as	O
that	O
produces	O
a	O
z-value	O
a	O
standard	O
normal	B
random	O
variable	O
z	O
n	O
under	O
the	O
null	O
hypothesis	O
gives	O
to	O
a	O
good	O
approximation	O
z	O
n	O
student	O
s	O
t	B
with	O
degrees	B
of	I
freedom	I
under	O
reasonable	O
alternatives	O
for	O
the	O
specific	O
situation	O
in	O
as	O
in	O
holm	O
s	O
procedure	O
methods	O
of	O
fwer	O
control	B
including	O
holm	O
s	O
procedure	O
are	O
surveyed	O
in	O
chapter	O
of	O
efron	O
they	O
display	O
a	O
large	O
amount	O
of	O
mathematical	O
ingenuity	O
and	O
provided	O
the	O
background	O
against	O
which	O
fdr	O
theory	B
developed	O
fdr	O
control	B
theorem	B
benjamini	B
and	I
hochberg	I
s	O
striking	O
control	B
theorem	B
was	O
rederived	O
by	O
storey	O
et	O
al	O
using	O
martingale	B
theory	B
the	O
basic	O
idea	O
of	O
false	O
discoveries	O
as	O
displayed	O
in	O
figure	O
goes	O
back	O
to	O
soric	O
formula	B
integrating	O
fdr	O
z	O
d	O
gives	O
e	O
ffdr	O
zjz	O
dz	O
dz	O
f	O
dz	O
d	O
d	O
thresholds	O
for	O
fdr	O
and	O
fdr	O
suppose	O
the	O
survival	O
curves	O
and	O
satisfy	O
the	O
lehmann	B
alternative	I
relationship	O
log	O
d	O
log	O
notes	O
and	O
details	O
for	O
large	O
values	O
of	O
z	O
where	O
is	O
a	O
positive	O
constant	O
less	O
than	O
is	O
a	O
reasonable	O
condition	B
for	O
the	O
non-null	B
density	B
to	O
produce	O
larger	O
positive	O
values	O
of	O
z	O
than	O
does	O
the	O
null	O
density	B
differentiating	O
gives	O
d	O
after	O
some	O
rearrangement	O
but	O
fdr	O
z	O
d	O
c	O
is	O
algebraically	O
equivalent	O
to	O
fdr	O
z	O
fdr	O
z	O
d	O
and	O
similarly	O
for	O
fdr	O
z	O
yielding	O
fdr	O
z	O
fdr	O
z	O
fdr	O
z	O
fdr	O
z	O
d	O
for	O
large	O
z	O
both	O
fdr	O
z	O
and	O
fdr	O
z	O
go	O
to	O
zero	O
giving	O
the	O
asymptotic	O
relationship	O
fdr	O
z	O
this	O
motivates	O
the	O
suggested	O
relative	O
thresholdscfdr	O
zi	O
compared	O
if	O
d	O
for	O
instance	O
fdr	O
z	O
will	O
be	O
about	O
twice	O
fdr	O
z	O
where	O
z	O
is	O
large	O
with	O
cfdr	O
zi	O
correlation	B
effects	I
the	O
poisson	B
regression	B
method	B
used	O
to	O
estimate	B
o	O
f	O
in	O
figure	O
proceeds	O
as	O
if	O
the	O
components	O
of	O
the	O
n	O
of	O
zi	O
values	O
z	O
are	O
independent	O
approximation	O
that	O
the	O
kth	O
bin	O
count	O
yk	O
requires	O
independence	O
if	O
not	O
it	O
can	O
be	O
shown	O
that	O
var	O
yk	O
increases	O
above	O
the	O
poisson	B
value	O
as	O
var	O
yk	O
c	O
nx	O
x	O
d	O
here	O
ck	O
is	O
a	O
fixed	O
constant	O
depending	O
on	O
f	O
while	O
is	O
the	O
root	O
mean	O
square	O
correlation	O
between	O
all	O
pairs	O
zi	O
and	O
zj	O
cov	O
zi	O
zj	O
n	O
n	O
j	O
i	O
estimates	O
likecfdr	O
z	O
in	O
figure	O
remain	O
nearly	O
unbiased	O
under	O
correlation	O
but	O
their	O
sampling	O
variability	O
increases	O
as	O
a	O
function	B
of	O
chapters	O
and	O
of	O
efron	O
discuss	O
correlation	B
effects	I
in	O
detail	O
often	O
can	O
be	O
estimated	O
let	O
x	O
be	O
the	O
matrix	B
of	O
gene	O
expression	O
levels	O
measured	O
for	O
the	O
control	B
subject	O
in	O
the	O
prostate	B
study	O
rows	O
large-scale	O
hypothesis	B
testing	B
and	O
fdrs	O
i	O
and	O
j	O
provide	O
an	O
unbiased	O
estimate	B
of	O
cor	O
zi	O
zj	O
modern	O
computation	O
is	O
sufficiently	O
fast	O
to	O
evaluate	O
all	O
n	O
n	O
pairs	O
that	O
isn	O
t	B
necessary	O
sampling	O
is	O
faster	O
from	O
which	O
estimate	B
o	O
is	O
obtained	O
it	O
equaled	O
for	O
the	O
control	B
subjects	O
and	O
for	O
the	O
matrix	B
of	O
the	O
cancer	O
patients	O
correlation	O
is	O
not	O
much	O
of	O
a	O
worry	O
for	O
the	O
prostate	B
study	O
but	O
other	O
microarray	O
studies	O
show	O
much	O
larger	O
o	O
values	O
sections	O
and	O
of	O
efron	O
discuss	O
how	O
correlations	O
can	O
undercut	O
inferences	O
based	O
on	O
the	O
theoretical	B
null	I
even	O
when	O
it	O
is	O
correct	O
for	O
all	O
the	O
null	O
cases	O
the	O
program	O
locfdr	B
available	O
from	O
cran	O
this	O
is	O
an	O
r	B
program	O
that	O
provides	O
fdr	O
and	O
fdr	O
estimates	O
using	O
both	O
the	O
theoretical	O
and	O
empirical	B
null	I
distributions	O
ml	O
estimation	B
of	O
the	O
empirical	B
null	I
let	O
be	O
the	O
zero	B
set	B
the	O
set	B
of	O
zi	O
observed	O
to	O
be	O
in	O
their	O
indices	O
and	O
the	O
number	O
of	O
zi	O
in	O
also	O
define	O
d	O
e	O
p	O
dz	O
according	O
to	O
then	O
has	O
density	B
and	O
d	O
dz	O
d	O
prfzi	O
and	O
likelihood	B
f	O
d	O
n	O
p	O
the	O
first	O
factor	B
being	O
the	O
binomial	B
probability	O
of	O
seeing	O
of	O
the	O
zi	O
in	O
and	O
the	O
second	O
the	O
conditional	B
probability	O
of	O
those	O
zi	O
falling	O
within	O
o	O
while	O
the	O
second	O
factor	B
is	O
numerically	O
maximized	O
to	O
give	O
o	O
d	O
is	O
obtained	O
from	O
the	O
first	O
and	O
then	O
d	O
o	O
o	O
this	O
is	O
a	O
partial	B
likelihood	B
argument	B
as	O
in	O
section	O
locfdr	B
centers	O
at	O
the	O
median	O
of	O
the	O
n	O
zi	O
values	O
with	O
width	O
about	O
twice	O
the	O
interquartile	O
range	O
estimate	B
of	O
the	O
permutation	B
null	I
an	O
impressive	O
amount	O
of	O
theoretical	O
effort	O
concerned	O
the	O
permutation	O
t-test	O
in	O
a	O
single-test	O
two-sample	B
situation	O
permuting	O
the	O
data	B
and	O
computing	O
the	O
t	B
statistic	B
gives	O
after	O
a	O
great	O
many	O
repetitions	O
a	O
histogram	O
dependably	O
close	O
to	O
that	O
of	O
the	O
standard	O
t	B
distribution	B
see	O
hoeffding	O
this	O
was	O
fisher	B
s	O
justification	O
for	O
using	O
the	O
standard	O
t-test	O
on	O
nonnormal	O
data	B
the	O
argument	B
cuts	O
both	O
ways	O
permutation	O
methods	O
tend	O
to	O
recreate	O
the	O
notes	O
and	O
details	O
theoretical	B
null	I
even	O
in	O
situations	O
like	O
that	O
of	O
figure	O
where	O
it	O
isn	O
t	B
appropriate	O
the	O
difficulties	O
are	O
discussed	O
in	O
section	O
of	O
efron	O
relevance	B
theory	B
suppose	O
that	O
in	O
the	O
dti	O
example	O
shown	O
in	O
figure	O
we	O
want	O
to	O
consider	O
only	O
voxels	O
with	O
x	O
d	O
as	O
relevant	O
to	O
an	O
observed	O
zi	O
with	O
xi	O
d	O
now	O
there	O
may	O
not	O
be	O
enough	O
relevant	O
cases	O
to	O
how	O
the	O
complete-data	O
estimatescfdr	O
zi	O
orcfdr	O
zi	O
can	O
be	O
efficiently	O
modadequately	O
estimate	B
fdr	O
zi	O
or	O
fdr	O
zi	O
section	O
of	O
efron	O
shows	O
ified	O
to	O
conform	O
to	O
this	O
situation	O
sparse	O
modeling	O
and	O
the	O
lasso	B
the	O
amount	O
of	O
data	B
we	O
are	O
faced	O
with	O
keeps	O
growing	O
from	O
around	O
the	O
late	O
we	O
started	O
to	O
see	O
wide	B
data	B
sets	O
where	O
the	O
number	O
of	O
variables	O
far	O
exceeds	O
the	O
number	O
of	O
observations	O
this	O
was	O
largely	O
due	O
to	O
our	O
increasing	O
ability	O
to	O
measure	O
a	O
large	O
amount	O
of	O
information	B
automatically	O
in	O
genomics	O
for	O
example	O
we	O
can	O
use	O
a	O
high-throughput	O
experiment	O
to	O
automatically	O
measure	O
the	O
expression	O
of	O
tens	O
of	O
thousands	O
of	O
genes	O
in	O
a	O
sample	B
in	O
a	O
short	O
amount	O
of	O
time	O
similarly	O
sequencing	O
equipment	O
allows	O
us	O
to	O
genotype	O
millions	O
of	O
snps	O
polymorphisms	O
cheaply	O
and	O
quickly	O
in	O
document	B
retrieval	I
and	O
modeling	O
we	O
represent	O
a	O
document	O
by	O
the	O
presence	O
or	O
count	O
of	O
each	O
word	O
in	O
the	O
dictionary	O
this	O
easily	O
leads	O
to	O
a	O
feature	O
vector	B
with	O
components	O
one	O
for	O
each	O
distinct	O
vocabulary	O
word	O
although	O
most	O
would	O
be	O
zero	O
for	O
a	O
small	O
document	O
if	O
we	O
move	O
to	O
bi-grams	O
or	O
higher	O
the	O
feature	O
space	O
gets	O
really	O
large	O
in	O
even	O
more	O
modest	O
situations	O
we	O
can	O
be	O
faced	O
with	O
hundreds	O
of	O
variables	O
if	O
these	O
variables	O
are	O
to	O
be	O
predictors	B
in	O
a	O
regression	B
or	O
logistic	B
regression	B
model	I
we	O
probably	O
do	O
not	O
want	O
to	O
use	O
them	O
all	O
it	O
is	O
likely	O
that	O
a	O
subset	O
will	O
do	O
the	O
job	O
well	O
and	O
including	O
all	O
the	O
redundant	O
variables	O
will	O
degrade	O
our	O
fit	O
hence	O
we	O
are	O
often	O
interested	O
in	O
identifying	O
a	O
good	O
subset	O
of	O
variables	O
note	O
also	O
that	O
in	O
these	O
wide-data	O
situations	O
even	O
linear	B
models	B
are	O
over-parametrized	O
so	O
some	O
form	B
of	O
reduction	O
or	O
regularization	B
is	O
essential	O
in	O
this	O
chapter	O
we	O
will	O
discuss	O
some	O
of	O
the	O
popular	O
methods	O
for	O
model	B
selection	I
starting	O
with	O
the	O
time-tested	O
and	O
worthy	O
forward-stepwise	B
approach	O
we	O
then	O
look	O
at	O
the	O
lasso	B
a	O
popular	O
modern	O
method	B
that	O
does	O
selection	O
and	O
shrinkage	B
via	O
convex	B
optimization	I
the	O
lars	B
algorithm	B
ties	O
these	O
two	O
approaches	O
together	O
and	O
leads	O
to	O
methods	O
that	O
can	O
deliver	O
paths	O
of	O
solutions	O
finally	O
we	O
discuss	O
some	O
connections	O
with	O
other	O
modern	O
big-	O
and	O
wide	B
data	B
approaches	O
and	O
mention	O
some	O
extensions	O
forward	O
stepwise	O
regression	B
forward	O
stepwise	O
regression	B
stepwise	O
procedures	O
have	O
been	O
around	O
for	O
a	O
very	O
long	O
time	O
they	O
were	O
originally	O
devised	O
in	O
times	O
when	O
data	B
sets	O
were	O
quite	O
modest	O
in	O
size	O
in	O
particular	O
in	O
terms	O
of	O
the	O
number	O
of	O
variables	O
originally	O
thought	O
of	O
as	O
the	O
poor	O
cousins	O
of	O
best-subset	B
selection	I
they	O
had	O
the	O
advantage	O
of	O
being	O
much	O
cheaper	O
to	O
compute	O
in	O
fact	O
possible	O
to	O
compute	O
for	O
large	O
p	O
we	O
will	O
review	O
best-subset	O
regression	B
first	O
i	O
suppose	O
we	O
have	O
a	O
set	B
of	O
n	O
observations	O
on	O
a	O
response	B
yi	O
and	O
a	O
vecd	O
xi	O
xip	O
and	O
we	O
plan	O
to	O
fit	O
a	O
linear	B
tor	O
of	O
p	O
predictors	B
x	O
regression	B
model	I
the	O
response	B
could	O
be	O
quantitative	O
so	O
we	O
can	O
think	O
of	O
fitting	O
a	O
linear	B
model	I
by	O
least	B
squares	I
it	O
could	O
also	O
be	O
binary	O
leading	O
to	O
a	O
linear	B
logistic	B
regression	B
model	I
fit	O
by	O
maximum	B
likelihood	B
although	O
we	O
will	O
focus	O
on	O
these	O
two	O
cases	O
the	O
same	O
ideas	O
transfer	O
exactly	O
to	O
other	O
generalized	O
linear	B
models	B
the	O
cox	O
model	O
and	O
so	O
on	O
the	O
idea	O
is	O
to	O
build	O
a	O
model	O
using	O
a	O
subset	O
of	O
the	O
variables	O
in	O
fact	O
the	O
smallest	O
subset	O
that	O
adequately	O
explains	O
the	O
variation	O
in	O
the	O
response	B
is	O
what	O
we	O
are	O
after	O
both	O
for	O
inference	B
and	O
for	O
prediction	O
purposes	O
suppose	O
our	O
loss	B
function	B
for	O
fitting	O
the	O
linear	B
model	I
is	O
l	O
sum	O
of	O
squares	O
negative	O
log-likelihood	B
the	O
method	B
of	O
best-subset	O
regression	B
is	O
simple	O
to	O
describe	O
and	O
is	O
given	O
in	O
algorithm	B
step	O
is	O
easy	O
to	O
state	O
but	O
requires	O
a	O
lot	O
of	O
computation	O
for	O
algorithm	B
best-subset	O
regression	B
start	O
with	O
m	O
d	O
and	O
the	O
null	O
model	O
d	O
o	O
estimated	O
by	O
the	O
mean	O
of	O
the	O
yi	O
at	O
step	O
m	O
d	O
pick	O
the	O
single	O
variable	O
j	O
that	O
fits	O
the	O
response	B
best	O
in	O
terms	O
of	O
the	O
loss	O
l	O
evaluated	O
on	O
the	O
training	O
data	B
in	O
a	O
univariate	O
regression	B
d	O
o	O
for	O
each	O
subset	O
size	O
m	O
mg	O
m	O
min	O
n	O
p	O
identify	O
the	O
best	O
subset	O
am	O
of	O
size	O
m	O
when	O
fitting	O
a	O
linear	B
model	I
om	O
x	O
d	O
o	O
o	O
am	O
with	O
m	O
of	O
the	O
p	O
variables	O
in	O
terms	O
of	O
the	O
loss	O
l	O
use	O
some	O
external	B
data	B
or	O
other	O
means	O
to	O
select	O
the	O
best	O
amongst	O
these	O
o	O
j	O
set	B
d	O
fjg	O
c	O
x	O
c	O
x	O
j	O
am	O
m	O
models	B
p	O
much	O
larger	O
than	O
about	O
it	O
becomes	O
prohibitively	O
expensive	O
to	O
perform	O
exactly	O
a	O
so-called	O
n-p	B
complete	I
problem	O
because	O
of	O
its	O
combinatorial	O
complexity	O
are	O
subsets	O
note	O
that	O
the	O
subsets	O
need	O
not	O
be	O
nested	O
sparse	O
modeling	O
and	O
the	O
lasso	B
the	O
best	O
subset	O
of	O
size	O
m	O
d	O
say	O
need	O
not	O
include	O
both	O
or	O
any	O
of	O
the	O
variables	O
in	O
the	O
best	O
subset	O
of	O
size	O
m	O
d	O
in	O
step	O
there	O
are	O
a	O
number	O
of	O
methods	O
for	O
selecting	O
m	O
originally	O
the	O
cp	B
criterion	O
of	O
chapter	O
was	O
proposed	O
for	O
this	O
purpose	O
here	O
we	O
will	O
favor	O
k-fold	B
cross-validation	B
since	O
it	O
is	O
applicable	O
to	O
all	O
the	O
methods	O
discussed	O
in	O
this	O
chapter	O
it	O
is	O
interesting	O
to	O
digress	O
for	O
a	O
moment	O
on	O
how	O
cross-validation	B
works	O
here	O
we	O
are	O
using	O
it	O
to	O
select	O
the	O
subset	O
size	O
m	O
on	O
the	O
basis	O
of	O
prediction	O
performance	O
future	O
data	B
with	O
k	O
d	O
we	O
divide	O
the	O
n	O
training	O
observations	O
randomly	O
into	O
equal	O
size	O
groups	O
leaving	O
out	O
say	O
group	O
k	O
d	O
we	O
perform	O
steps	O
on	O
the	O
and	O
for	O
each	O
of	O
the	O
chosen	O
models	B
we	O
summarize	O
the	O
prediction	O
performance	O
on	O
the	O
data	B
we	O
do	O
this	O
k	O
d	O
times	O
each	O
time	O
with	O
group	O
k	O
left	O
out	O
we	O
then	O
average	O
the	O
performance	O
measures	O
for	O
each	O
m	O
and	O
select	O
the	O
value	O
of	O
m	O
corresponding	O
to	O
the	O
best	O
performance	O
notice	O
that	O
for	O
each	O
m	O
the	O
models	B
om	O
x	O
might	O
involve	O
different	O
subsets	O
of	O
variables	O
this	O
is	O
not	O
a	O
concern	O
since	O
we	O
are	O
trying	O
to	O
find	O
a	O
good	O
value	O
of	O
m	O
for	O
the	O
method	B
having	O
identified	O
om	O
we	O
rerun	O
steps	O
on	O
the	O
entire	O
training	B
set	B
and	O
deliver	O
the	O
chosen	O
model	O
o	O
om	O
x	O
as	O
hinted	O
above	O
there	O
are	O
problems	O
with	O
best-subset	O
regression	B
a	O
primary	O
issue	O
is	O
that	O
it	O
works	O
exactly	O
only	O
for	O
relatively	O
small	O
p	O
for	O
example	O
we	O
cannot	O
run	O
it	O
on	O
the	O
spam	B
data	B
with	O
variables	O
least	O
not	O
in	O
on	O
a	O
macbook	O
pro	O
we	O
may	O
also	O
think	O
that	O
even	O
if	O
we	O
could	O
do	O
the	O
computations	B
with	O
such	O
a	O
large	O
search	O
space	O
the	O
variance	B
of	O
the	O
procedure	O
might	O
be	O
too	O
high	O
as	O
a	O
result	O
more	O
manageable	O
stepwise	O
procedures	O
were	O
invented	O
forward	O
stepwise	O
regression	B
algorithm	B
is	O
a	O
simple	O
modification	O
of	O
bestsubset	O
with	O
the	O
modification	O
occurring	O
in	O
step	O
forward	O
stepwise	O
regression	B
produces	O
a	O
nested	O
sequence	O
of	O
models	B
am	O
it	O
starts	O
with	O
the	O
null	O
model	O
here	O
an	O
intercept	O
and	O
adds	O
variables	O
one	O
at	O
a	O
time	O
even	O
with	O
large	O
p	O
identifying	O
the	O
best	O
variable	O
to	O
add	O
at	O
each	O
step	O
is	O
manageable	O
and	O
can	O
be	O
distributed	O
if	O
clusters	O
of	O
machines	O
are	O
available	O
most	O
importantly	O
it	O
is	O
feasible	O
for	O
large	O
p	O
figure	O
shows	O
the	O
coefficient	O
profiles	O
for	O
forward-stepwise	B
linear	B
regression	B
on	O
the	O
spam	B
training	O
data	B
here	O
there	O
are	O
input	O
variables	O
prevalence	O
of	O
particular	O
words	O
in	O
the	O
document	O
and	O
an	O
official	O
test	O
split	O
of	O
observations	O
the	O
response	B
is	O
coded	O
as	O
if	O
the	O
email	O
was	O
spam	B
else	O
the	O
figure	O
caption	O
gives	O
the	O
details	O
we	O
saw	O
the	O
spam	B
data	B
earlier	O
in	O
table	O
figure	O
and	O
figure	O
fitting	O
the	O
entire	O
forward-stepwise	B
linear	B
regression	B
path	B
as	O
in	O
the	O
figure	O
forward	O
stepwise	O
regression	B
algorithm	B
forward	O
stepwise	O
regression	B
start	O
with	O
m	O
d	O
and	O
the	O
null	O
model	O
d	O
o	O
estimated	O
by	O
the	O
mean	O
of	O
the	O
yi	O
at	O
step	O
m	O
d	O
pick	O
the	O
single	O
variable	O
j	O
that	O
fits	O
the	O
response	B
best	O
in	O
terms	O
of	O
the	O
loss	O
l	O
evaluated	O
on	O
the	O
training	O
data	B
in	O
a	O
univariate	O
regression	B
d	O
o	O
for	O
each	O
subset	O
size	O
m	O
mg	O
m	O
min	O
n	O
p	O
identify	O
the	O
variable	O
k	O
that	O
when	O
augmented	O
with	O
to	O
form	B
am	O
leads	O
to	O
the	O
model	O
om	O
x	O
d	O
o	O
o	O
am	O
that	O
performs	O
best	O
in	O
terms	O
of	O
the	O
loss	O
l	O
use	O
some	O
external	B
data	B
or	O
other	O
means	O
to	O
select	O
the	O
best	O
amongst	O
these	O
o	O
j	O
set	B
d	O
fjg	O
c	O
x	O
c	O
x	O
j	O
am	O
m	O
models	B
n	O
p	O
has	O
essentially	O
the	O
same	O
cost	O
as	O
a	O
single	O
least	B
squares	I
fit	O
on	O
all	O
the	O
variables	O
this	O
is	O
because	O
the	O
sequence	O
of	O
models	B
can	O
be	O
updated	O
each	O
time	O
a	O
variable	O
is	O
added	O
however	O
this	O
is	O
a	O
consequence	O
of	O
the	O
linear	B
model	I
and	O
squared-error	O
loss	O
suppose	O
instead	O
we	O
run	O
a	O
forward	O
stepwise	O
logistic	B
regression	B
here	O
updating	O
does	O
not	O
work	O
and	O
the	O
entire	O
fit	O
has	O
to	O
be	O
recomputed	O
by	O
maximum	B
likelihood	B
each	O
time	O
a	O
variable	O
is	O
added	O
identifying	O
which	O
variable	O
to	O
add	O
in	O
step	O
in	O
principle	O
requires	O
fitting	O
an	O
c	O
model	O
p	O
m	O
times	O
and	O
seeing	O
which	O
one	O
reduces	O
the	O
deviance	B
the	O
most	O
in	O
practice	O
we	O
can	O
use	O
score	B
tests	I
which	O
are	O
much	O
cheaper	O
to	O
evaluate	O
these	O
amount	O
to	O
using	O
the	O
quadratic	O
approximation	O
to	O
the	O
log-likelihood	B
from	O
the	O
final	O
iteratively	O
reweighted	O
least-squares	O
iteration	O
for	O
fitting	O
the	O
model	O
with	O
m	O
terms	O
the	O
score	O
test	O
for	O
a	O
variable	O
not	O
in	O
the	O
model	O
is	O
equivalent	O
to	O
testing	B
for	O
the	O
inclusion	O
of	O
this	O
variable	O
in	O
the	O
weighted	O
least-squares	O
fit	O
hence	O
identifying	O
the	O
next	O
variable	O
is	O
almost	O
back	O
to	O
the	O
previous	O
cases	O
requiring	O
p	O
m	O
simple	O
regression	B
updates	O
figure	O
shows	O
the	O
test	O
misclassification	O
error	O
for	O
forward-stepwise	B
linear	B
regression	B
and	O
logistic	B
regression	B
on	O
the	O
spam	B
data	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
steps	O
they	O
both	O
level	O
off	O
at	O
around	O
steps	O
and	O
have	O
a	O
similar	O
shape	O
however	O
the	O
logistic	B
regression	B
gives	O
more	O
accurate	O
although	O
forward-stepwise	B
methods	O
are	O
possible	O
for	O
large	O
p	O
they	O
get	O
tedious	O
for	O
very	O
large	O
p	O
the	O
thousands	O
especially	O
if	O
the	O
data	B
could	O
sup	O
for	O
this	O
example	O
we	O
can	O
halve	O
the	O
gap	O
between	O
the	O
curves	O
by	O
optimizing	O
the	O
prediction	O
threshold	O
for	O
linear	B
regression	B
sparse	O
modeling	O
and	O
the	O
lasso	B
figure	O
forward	O
stepwise	O
linear	B
regression	B
on	O
the	O
spam	B
data	B
each	O
curve	O
corresponds	O
to	O
a	O
particular	O
variable	O
and	O
shows	O
the	O
progression	O
of	O
its	O
coefficient	O
as	O
the	O
model	O
grows	O
these	O
are	O
plotted	O
against	O
the	O
training	O
and	O
the	O
vertical	O
gray	O
bars	O
correspond	O
to	O
each	O
step	O
starting	O
at	O
the	O
left	O
at	O
step	O
the	O
first	O
selected	O
variable	O
explains	O
d	O
adding	O
the	O
second	O
increases	O
to	O
etc	O
what	O
we	O
see	O
is	O
that	O
early	O
steps	O
have	O
a	O
big	O
impact	O
on	O
the	O
while	O
later	O
steps	O
hardly	O
have	O
any	O
at	O
all	O
the	O
vertical	O
black	O
line	O
corresponds	O
to	O
step	O
figure	O
and	O
we	O
see	O
that	O
after	O
that	O
the	O
step-wise	O
improvements	O
in	O
are	O
negligible	O
port	O
a	O
model	O
with	O
many	O
variables	O
however	O
if	O
the	O
ideal	O
active	B
set	B
is	O
fairly	O
small	O
even	O
with	O
many	O
thousands	O
of	O
variables	O
forward-stepwise	B
selection	O
is	O
a	O
viable	O
option	O
forward-stepwise	B
selection	O
delivers	O
a	O
sequence	O
of	O
models	B
as	O
seen	O
in	O
the	O
previous	O
figures	O
one	O
would	O
generally	O
want	O
to	O
select	O
a	O
single	O
model	O
and	O
as	O
discussed	O
earlier	O
we	O
often	O
use	O
cross-validation	B
for	O
this	O
purpose	O
figure	O
illustrates	O
using	O
stepwise	O
linear	B
regression	B
on	O
the	O
spam	B
data	B
here	O
the	O
sequence	O
of	O
models	B
are	O
fit	O
using	O
squared-error	O
loss	O
on	O
the	O
binary	O
response	B
variable	O
however	O
cross-validation	B
scores	O
each	O
model	O
for	O
misclassification	O
error	O
the	O
ultimate	O
goal	O
of	O
this	O
modeling	O
exercise	O
this	O
highlights	O
one	O
of	O
the	O
advantages	O
of	O
cross-validation	B
in	O
this	O
context	O
a	O
con	O
stepwise	O
on	O
training	O
datacoefficients	O
the	O
lasso	B
figure	O
forward-stepwise	B
regression	B
on	O
the	O
spam	B
data	B
shown	O
is	O
the	O
misclassification	O
error	O
on	O
the	O
test	O
data	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
steps	O
the	O
brown	O
dots	O
correspond	O
to	O
linear	B
regression	B
with	O
the	O
response	B
coded	O
as	O
and	O
a	O
prediction	O
greater	O
than	O
zero	O
is	O
classified	O
as	O
one	O
less	O
than	O
zero	O
as	O
the	O
blue	O
dots	O
correspond	O
to	O
logistic	B
regression	B
which	O
performs	O
better	O
we	O
see	O
that	O
both	O
curves	O
essentially	O
reach	O
their	O
minima	O
after	O
steps	O
venient	O
and	O
smooth	O
loss	B
function	B
is	O
used	O
to	O
fit	O
the	O
sequence	O
of	O
models	B
however	O
we	O
can	O
use	O
any	O
performance	O
measure	O
to	O
evaluate	O
the	O
sequence	O
of	O
models	B
here	O
misclassification	O
error	O
is	O
used	O
in	O
terms	O
of	O
the	O
parameters	O
of	O
the	O
linear	B
model	I
misclassification	O
error	O
would	O
be	O
a	O
difficult	O
and	O
discontinuous	O
loss	B
function	B
to	O
use	O
for	O
parameter	O
estimation	B
all	O
we	O
need	O
to	O
use	O
it	O
for	O
here	O
is	O
pick	O
the	O
best	O
model	O
size	O
there	O
appears	O
to	O
be	O
little	O
benefit	O
in	O
going	O
beyond	O
terms	O
the	O
lasso	B
the	O
stepwise	O
model-selection	O
methods	O
of	O
the	O
previous	O
section	O
are	O
useful	O
if	O
we	O
anticipate	O
a	O
model	O
using	O
a	O
relatively	O
small	O
number	O
of	O
variables	O
even	O
if	O
the	O
pool	O
of	O
available	O
variables	O
is	O
very	O
large	O
if	O
we	O
expect	O
a	O
moderate	O
number	O
of	O
variables	O
to	O
play	O
a	O
role	O
these	O
methods	O
become	O
cumbersome	O
datasteptest	O
misclassification	O
errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward	O
stepwise	O
linear	B
regressionforward	O
stepwise	O
logistic	B
regression	B
sparse	O
modeling	O
and	O
the	O
lasso	B
figure	O
ten-fold	O
cross-validated	O
misclassification	O
errors	B
for	O
forward-stepwise	B
regression	B
on	O
the	O
spam	B
data	B
as	O
a	O
function	B
of	O
the	O
step	O
number	O
since	O
each	O
error	O
is	O
an	O
average	O
of	O
numbers	O
we	O
can	O
compute	O
a	O
standard	B
error	I
included	O
in	O
the	O
plot	O
are	O
pointwise	O
standard-error	O
bands	O
the	O
brown	O
curve	O
is	O
the	O
misclassification	O
error	O
on	O
the	O
test	O
data	B
another	O
black	O
mark	O
against	O
forward-stepwise	B
methods	O
is	O
that	O
the	O
sequence	O
of	O
models	B
is	O
derived	O
in	O
a	O
greedy	O
fashion	O
without	O
any	O
claimed	O
optimality	B
the	O
methods	O
we	O
describe	O
here	O
are	O
derived	O
from	O
a	O
more	O
principled	O
procedure	O
indeed	O
they	O
solve	O
a	O
convex	B
optimization	I
as	O
defined	O
below	O
we	O
will	O
first	O
present	O
the	O
lasso	B
for	O
squared-error	O
loss	O
and	O
then	O
the	O
more	O
i	O
subject	O
to	O
k	O
t	B
general	O
case	O
later	O
consider	O
the	O
constrained	O
linear	B
regression	B
problem	O
where	O
k	O
dpp	O
nx	O
x	O
j	O
jj	O
the	O
norm	O
of	O
the	O
coefficient	O
vector	B
since	O
both	O
the	O
loss	O
and	O
the	O
constraint	O
are	O
convex	O
in	O
this	O
is	O
a	O
convex	B
optimization	I
problem	O
and	O
it	O
is	O
known	O
as	O
the	O
lasso	B
the	O
constraint	O
k	O
t	B
restricts	O
the	O
coefficients	O
of	O
the	O
model	O
by	O
pulling	O
them	O
toward	O
zero	O
this	O
has	O
the	O
effect	O
of	O
reducing	O
their	O
variance	B
and	O
prevents	O
overfitting	O
ridge	B
regression	B
is	O
an	O
earlier	O
great	O
uncle	O
of	O
the	O
lasso	B
and	O
solves	O
a	O
similar	O
problem	O
to	O
ex	O
minimize	O
n	O
datasteptest	O
and	O
cv	O
misclassification	O
errorlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllltest	O
fold	O
cv	O
error	O
the	O
lasso	B
figure	O
an	O
example	O
with	O
to	O
illustrate	O
the	O
difference	O
between	O
ridge	B
regression	B
and	O
the	O
lasso	B
in	O
both	O
plots	O
the	O
red	O
contours	O
correspond	O
to	O
the	O
squared-error	O
loss	B
function	B
with	O
the	O
unrestricted	O
least-squares	O
estimate	B
o	O
regions	O
show	O
the	O
constraints	O
with	O
the	O
lasso	B
on	O
the	O
left	O
and	O
ridge	O
on	O
the	O
right	O
the	O
solution	O
to	O
the	O
constrained	O
problem	O
corresponds	O
to	O
the	O
value	O
of	O
where	O
the	O
expanding	O
loss	O
contours	O
first	O
touch	O
the	O
constraint	O
region	B
due	O
to	O
the	O
shape	O
of	O
the	O
lasso	B
constraint	O
this	O
will	O
often	O
be	O
at	O
a	O
corner	O
an	O
edge	O
more	O
generally	O
as	O
here	O
which	O
means	O
in	O
this	O
case	O
that	O
the	O
minimizing	O
has	O
d	O
for	O
the	O
ridge	O
constraint	O
this	O
is	O
unlikely	O
to	O
happen	O
in	O
the	O
center	O
the	O
blue	O
cept	O
the	O
constraint	O
is	O
k	O
t	B
ridge	B
regression	B
bounds	O
the	O
quadratic	O
norm	O
of	O
the	O
coefficient	O
vector	B
it	O
also	O
has	O
the	O
effect	O
of	O
pulling	O
the	O
coefficients	O
toward	O
zero	O
in	O
an	O
apparently	O
very	O
similar	O
way	O
ridge	B
regression	B
is	O
discussed	O
in	O
section	O
both	O
the	O
lasso	B
and	O
ridge	B
regression	B
are	O
shrinkage	B
methods	O
in	O
the	O
spirit	O
of	O
the	O
james	O
stein	O
estimator	B
of	O
chapter	O
a	O
big	O
difference	O
however	O
is	O
that	O
for	O
the	O
lasso	B
the	O
solution	O
typically	O
has	O
many	O
of	O
the	O
j	O
equal	O
to	O
zero	O
while	O
for	O
ridge	O
they	O
are	O
all	O
nonzero	O
hence	O
the	O
lasso	B
does	O
variable	O
selection	O
and	O
shrinkage	B
while	O
ridge	O
only	O
shrinks	O
figure	O
illustrates	O
this	O
for	O
in	O
higher	O
dimensions	O
the	O
norm	O
has	O
sharp	O
edges	O
and	O
corners	O
which	O
correspond	O
to	O
coefficient	O
estimates	O
zero	O
in	O
since	O
the	O
constraint	O
in	O
the	O
lasso	B
treats	O
all	O
the	O
coefficients	O
equally	O
it	O
usually	O
makes	O
sense	O
for	O
all	O
the	O
elements	O
of	O
x	O
to	O
be	O
in	O
the	O
same	O
units	O
if	O
not	O
we	O
here	O
we	O
use	O
the	O
bound	B
form	B
of	O
ridge	B
regression	B
while	O
in	O
section	O
we	O
use	O
the	O
lagrange	O
form	B
they	O
are	O
equivalent	O
in	O
that	O
for	O
every	O
lagrange	O
solution	O
there	O
is	O
a	O
corresponding	O
bound	B
solution	O
sparse	O
modeling	O
and	O
the	O
lasso	B
typically	O
standardize	O
the	O
predictors	B
beforehand	O
so	O
that	O
each	O
has	O
variance	B
one	O
two	O
natural	O
boundary	O
values	O
for	O
t	B
in	O
are	O
t	B
d	O
and	O
t	B
d	O
the	O
former	O
corresponds	O
to	O
the	O
constant	O
model	O
fit	O
is	O
the	O
mean	O
of	O
the	O
and	O
the	O
latter	O
corresponds	O
to	O
the	O
unrestricted	O
least-squares	O
fit	O
in	O
fact	O
if	O
n	O
p	O
and	O
o	O
is	O
the	O
least-squares	O
estimate	B
then	O
we	O
can	O
replace	O
by	O
k	O
o	O
and	O
any	O
value	O
of	O
t	B
k	O
o	O
is	O
a	O
non-binding	O
constraint	O
figure	O
figure	O
the	O
lasso	B
linear	B
regression	B
regularization	B
path	B
on	O
the	O
spam	B
data	B
each	O
curve	O
corresponds	O
to	O
a	O
particular	O
variable	O
and	O
shows	O
the	O
progression	O
of	O
its	O
coefficient	O
as	O
the	O
regularization	B
bound	B
t	B
grows	O
these	O
curves	O
are	O
plotted	O
against	O
the	O
training	O
rather	O
than	O
t	B
to	O
make	O
the	O
curves	O
comparable	O
with	O
the	O
forward-stepwise	B
curves	O
in	O
figure	O
some	O
values	O
of	O
t	B
are	O
indicated	O
at	O
the	O
top	O
the	O
vertical	O
gray	O
bars	O
indicate	O
changes	O
in	O
the	O
active	B
set	B
of	O
nonzero	O
coefficients	O
typically	O
an	O
inclusion	O
here	O
we	O
see	O
clearly	O
the	O
role	O
of	O
the	O
penalty	B
as	O
t	B
is	O
relaxed	O
coefficients	O
become	O
nonzero	O
but	O
in	O
a	O
smoother	O
fashion	O
than	O
in	O
forward	O
stepwise	O
shows	O
the	O
regularization	B
for	O
the	O
lasso	B
linear	B
regression	B
problem	O
on	O
we	O
typically	O
do	O
not	O
restrict	O
the	O
intercept	O
in	O
the	O
model	O
also	O
known	O
as	O
the	O
homotopy	B
path	B
on	O
training	O
the	O
lasso	B
the	O
spam	B
data	B
that	O
is	O
the	O
solution	O
path	B
for	O
all	O
values	O
of	O
t	B
this	O
can	O
be	O
computed	O
exactly	O
as	O
we	O
will	O
see	O
in	O
section	O
because	O
the	O
coefficient	O
profiles	O
are	O
piecewise	O
linear	B
in	O
t	B
it	O
is	O
natural	O
to	O
compare	O
this	O
coefficient	O
profile	O
with	O
the	O
analogous	O
one	O
in	O
figure	O
for	O
forward-stepwise	B
regression	B
because	O
of	O
the	O
control	B
of	O
k	O
o	O
we	O
don	O
t	B
see	O
the	O
same	O
range	O
as	O
in	O
forward	O
stepwise	O
and	O
observe	O
somewhat	O
smoother	O
behavior	O
figure	O
contrasts	O
figure	O
lasso	B
versus	O
forward-stepwise	B
regression	B
on	O
the	O
spam	B
data	B
shown	O
is	O
the	O
misclassification	O
error	O
on	O
the	O
test	O
data	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
variables	O
in	O
the	O
model	O
linear	B
regression	B
is	O
coded	O
brown	O
logistic	B
regression	B
blue	O
hollow	O
dots	O
forward	O
stepwise	O
solid	O
dots	O
lasso	B
in	O
this	O
case	O
it	O
appears	O
stepwise	O
and	O
lasso	B
achieve	O
the	O
same	O
performance	O
but	O
lasso	B
takes	O
longer	O
to	O
get	O
there	O
because	O
of	O
the	O
shrinkage	B
the	O
prediction	O
performance	O
on	O
the	O
spam	B
data	B
for	O
lasso	B
regularized	O
models	B
regression	B
and	O
logistic	B
regression	B
versus	O
forward-stepwise	B
models	B
the	O
results	O
are	O
rather	O
similar	O
at	O
the	O
end	O
of	O
the	O
path	B
here	O
forward	O
stepwise	O
can	O
achieve	O
classification	O
performance	O
similar	O
to	O
that	O
of	O
lasso	B
regularized	O
logistic	B
regression	B
with	O
about	O
half	O
the	O
terms	O
lasso	B
logistic	B
regression	B
indeed	O
any	O
likelihood-based	O
linear	B
model	I
is	O
fit	O
by	O
penalized	O
maximum	O
datasteptest	O
misclassification	O
errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward	O
stepwise	O
linear	B
regressionforward	O
stepwise	O
logistic	O
regressionlasso	O
linear	B
regressionlasso	O
logistic	B
regression	B
sparse	O
modeling	O
and	O
the	O
lasso	B
likelihood	B
minimize	O
n	O
nx	O
l	O
yi	O
c	O
xi	O
subject	O
to	O
k	O
t	B
here	O
l	O
is	O
the	O
negative	O
of	O
the	O
log-likelihood	B
function	B
for	O
the	O
response	B
distribution	B
fitting	O
lasso	B
models	B
the	O
lasso	B
objectives	O
or	O
are	O
differentiable	O
and	O
convex	O
in	O
and	O
and	O
the	O
constraint	O
is	O
convex	O
in	O
hence	O
solving	O
these	O
problems	O
is	O
a	O
convex	B
optimization	I
problem	O
for	O
which	O
standard	O
packages	B
are	O
available	O
it	O
turns	O
out	O
these	O
problems	O
have	O
special	O
structure	B
that	O
can	O
be	O
exploited	O
to	O
yield	O
efficient	O
algorithms	O
for	O
fitting	O
the	O
entire	O
path	B
of	O
solutions	O
as	O
in	O
figures	O
and	O
we	O
will	O
start	O
with	O
problem	O
which	O
we	O
rewrite	O
in	O
the	O
more	O
convenient	O
lagrange	O
form	B
minimize	O
ky	O
x	O
c	O
here	O
we	O
have	O
centered	O
y	O
and	O
the	O
columns	O
of	O
x	O
beforehand	O
and	O
hence	O
the	O
intercept	O
has	O
been	O
omitted	O
the	O
lagrange	O
and	O
constraint	O
versions	O
are	O
equivalent	O
in	O
the	O
sense	O
that	O
any	O
solution	O
o	O
to	O
with	O
corresponds	O
to	O
a	O
solution	O
to	O
with	O
t	B
d	O
k	O
o	O
here	O
large	O
values	O
of	O
will	O
encourage	O
solutions	O
with	O
small	O
norm	O
coefficient	O
vectors	O
and	O
vice-versa	O
d	O
corresponds	O
to	O
the	O
ordinary	O
least	B
squares	I
fit	O
the	O
solution	O
to	O
satisfies	O
the	O
subgradient	O
condition	B
j	O
d	O
p	O
o	O
i	O
c	O
d	O
hxj	O
y	O
x	O
o	O
j	O
j	O
d	O
p	O
this	O
notation	O
means	O
sj	O
d	O
sign	O
n	O
o	O
where	O
sj	O
sign	O
j	O
if	O
o	O
j	O
and	O
sj	O
if	O
o	O
j	O
d	O
we	O
use	O
the	O
inner-product	O
notation	O
ha	O
bi	O
d	O
a	O
b	O
in	O
which	O
leads	O
to	O
more	O
evocative	O
expressions	O
these	O
subgradient	O
conditions	B
are	O
the	O
modern	O
way	O
of	O
characterizing	O
solutions	O
to	O
problems	O
of	O
this	O
kind	O
and	O
are	O
equivalent	O
to	O
the	O
karush	O
kuhn	O
tucker	O
optimality	B
conditions	B
from	O
these	O
conditions	B
we	O
can	O
immediately	O
learn	O
some	O
properties	O
of	O
a	O
lasso	B
solution	O
o	O
ij	O
d	O
for	O
all	O
members	O
of	O
the	O
active	B
set	B
i	O
e	O
each	O
of	O
the	O
jhxj	O
y	O
x	O
n	O
variables	O
in	O
the	O
model	O
nonzero	O
coefficient	O
has	O
the	O
same	O
covariance	O
with	O
the	O
residuals	O
absolute	O
value	O
least-angle	B
regression	B
o	O
jhxk	O
y	O
x	O
ij	O
for	O
all	O
variables	O
not	O
in	O
the	O
active	B
set	B
with	O
n	O
coefficients	O
zero	O
these	O
conditions	B
are	O
interesting	O
and	O
have	O
a	O
big	O
impact	O
on	O
computation	O
suppose	O
we	O
have	O
the	O
solution	O
o	O
at	O
and	O
we	O
decrease	O
by	O
a	O
small	O
amount	O
to	O
the	O
coefficients	O
and	O
hence	O
the	O
residuals	O
change	O
in	O
such	O
a	O
way	O
that	O
the	O
covariances	O
all	O
remain	O
tied	O
at	O
the	O
smaller	O
value	O
if	O
in	O
the	O
process	O
the	O
active	B
set	B
has	O
not	O
changed	O
and	O
nor	O
have	O
the	O
signs	O
of	O
their	O
coefficients	O
then	O
we	O
get	O
an	O
important	O
consequence	O
o	O
is	O
linear	B
for	O
to	O
see	O
this	O
suppose	O
a	O
indexes	O
the	O
active	B
set	B
which	O
is	O
the	O
same	O
at	O
and	O
and	O
let	O
sa	O
be	O
the	O
constant	O
sign	O
vector	B
then	O
we	O
have	O
a	O
y	O
x	O
a	O
y	O
x	O
x	O
x	O
o	O
d	O
o	O
d	O
by	O
subtracting	O
and	O
solving	O
we	O
get	O
o	O
o	O
axa	O
d	O
and	O
the	O
remaining	O
coefficients	O
indices	O
not	O
in	O
a	O
are	O
all	O
zero	O
this	O
shows	O
that	O
the	O
full	B
coefficient	O
vector	B
o	O
is	O
linear	B
for	O
in	O
fact	O
the	O
coefficient	O
profiles	O
for	O
the	O
lasso	B
are	O
continuous	O
and	O
piecewise	O
linear	B
over	O
the	O
entire	O
range	O
of	O
with	O
knots	B
occurring	O
whenever	O
the	O
active	B
set	B
changes	O
or	O
the	O
signs	O
of	O
the	O
coefficients	O
change	O
another	O
consequence	O
is	O
that	O
we	O
can	O
easily	O
determine	O
the	O
smallest	O
value	O
for	O
such	O
that	O
the	O
solution	O
o	O
d	O
from	O
this	O
can	O
be	O
jhxj	O
yij	O
seen	O
to	O
be	O
d	O
maxj	O
these	O
two	O
facts	O
plus	O
a	O
few	O
more	O
details	O
enable	O
us	O
to	O
compute	O
the	O
exact	O
solution	O
path	B
for	O
the	O
squared-error-loss	O
lasso	B
that	O
is	O
the	O
topic	O
of	O
the	O
next	O
section	O
n	O
least-angle	B
regression	B
we	O
have	O
just	O
seen	O
that	O
the	O
lasso	B
coefficient	O
profile	O
o	O
is	O
piecewise	O
linear	B
in	O
and	O
that	O
the	O
elements	O
of	O
the	O
active	B
set	B
are	O
tied	O
in	O
their	O
absolute	O
o	O
covariance	O
with	O
the	O
residuals	O
with	O
d	O
y	O
x	O
the	O
covariance	O
between	O
xj	O
and	O
the	O
evolving	O
residual	O
is	O
cj	O
d	O
jhxj	O
hence	O
these	O
also	O
change	O
in	O
a	O
piecewise	O
linear	B
fashion	O
with	O
cj	O
d	O
for	O
j	O
a	O
and	O
cj	O
for	O
j	O
a	O
this	O
inspires	O
the	O
least-angle	B
regression	B
algorithm	B
given	O
in	O
algorithm	B
which	O
exploits	O
this	O
linearity	O
to	O
fit	O
the	O
entire	O
lasso	B
regularization	B
path	B
n	O
sparse	O
modeling	O
and	O
the	O
lasso	B
algorithm	B
least-angle	B
regression	B
a	O
a	O
xa	O
d	O
fjg	O
and	O
xa	O
standardize	O
the	O
predictors	B
to	O
have	O
mean	O
zero	O
and	O
unit	O
norm	O
start	O
with	O
the	O
residual	O
d	O
y	O
ny	O
d	O
p	O
d	O
find	O
the	O
predictor	B
xj	O
most	O
correlated	O
with	O
i	O
e	O
with	O
largest	O
value	O
for	O
jhxj	O
call	O
this	O
value	O
define	O
the	O
active	B
set	B
a	O
n	O
the	O
matrix	B
consisting	O
of	O
this	O
single	O
variable	O
for	O
k	O
d	O
k	O
d	O
min	O
n	O
p	O
do	O
define	O
the	O
least-squares	O
direction	O
d	O
and	O
d	O
and	O
the	O
remaining	O
elements	O
define	O
the	O
p-vector	O
such	O
that	O
a	O
are	O
zero	O
move	O
the	O
coefficients	O
from	O
in	O
the	O
direction	O
toward	O
their	O
least-squares	O
solution	O
on	O
xa	O
d	O
c	O
for	O
keeping	O
track	O
of	O
the	O
evolving	O
residuals	O
d	O
y	O
x	O
d	O
jhx	O
for	O
keeping	O
track	O
of	O
n	O
a	O
identify	O
the	O
largest	O
value	O
of	O
at	O
which	O
a	O
variable	O
catches	O
up	O
with	O
the	O
active	B
set	B
if	O
the	O
variable	O
jhx	O
d	O
this	O
defines	O
the	O
next	O
has	O
index	O
that	O
means	O
n	O
knot	O
k	O
d	O
d	O
c	O
and	O
rk	O
d	O
d	O
a	O
y	O
x	O
k	O
set	B
a	O
return	O
the	O
sequence	O
kgk	O
in	O
step	O
d	O
a	O
as	O
in	O
we	O
can	O
think	O
of	O
the	O
lar	O
al	O
xa	O
gorithm	O
as	O
a	O
democratic	O
version	O
of	O
forward-stepwise	B
regression	B
in	O
forwardstepwise	O
regression	B
we	O
identify	O
the	O
variable	O
that	O
will	O
improve	O
the	O
fit	O
the	O
most	O
and	O
then	O
move	O
all	O
the	O
coefficients	O
toward	O
the	O
new	O
least-squares	O
fit	O
as	O
described	O
in	O
endnotes	O
and	O
this	O
is	O
sometimes	O
done	O
by	O
computing	O
the	O
inner	O
products	O
of	O
each	O
variable	O
with	O
the	O
residual	O
and	O
picking	O
the	O
largest	O
in	O
absolute	O
value	O
in	O
step	O
of	O
algorithm	B
we	O
move	O
the	O
coefficients	O
for	O
the	O
variables	O
in	O
the	O
active	B
set	B
a	O
toward	O
their	O
least-squares	O
fit	O
their	O
inner	O
products	O
tied	O
but	O
stop	O
when	O
a	O
variable	O
not	O
in	O
a	O
catches	O
up	O
in	O
inner	O
product	O
at	O
that	O
point	O
it	O
is	O
invited	O
into	O
the	O
club	O
and	O
the	O
process	O
continues	O
step	O
can	O
be	O
performed	O
efficiently	O
because	O
of	O
the	O
linearity	O
of	O
the	O
evolving	O
inner	O
products	O
for	O
each	O
variable	O
not	O
in	O
a	O
we	O
can	O
determine	O
exactly	O
when	O
time	O
it	O
would	O
catch	O
up	O
and	O
hence	O
which	O
catches	O
up	O
first	O
and	O
when	O
since	O
the	O
path	B
is	O
piecewise	O
linear	B
and	O
we	O
know	O
the	O
slopes	O
this	O
least-angle	B
regression	B
figure	O
covariance	O
evolution	O
on	O
the	O
spam	B
data	B
as	O
variables	O
tie	O
for	O
maximal	O
covariance	O
they	O
become	O
part	O
of	O
the	O
active	B
set	B
these	O
occasions	O
are	O
indicated	O
by	O
the	O
vertical	O
gray	O
bars	O
again	O
plotted	O
against	O
the	O
training	O
as	O
in	O
figure	O
means	O
we	O
know	O
the	O
path	B
exactly	O
without	O
further	O
computation	O
between	O
and	O
the	O
newly	O
found	O
the	O
name	O
least-angle	B
regression	B
derives	O
from	O
the	O
fact	O
that	O
in	O
step	O
the	O
fitted	O
vector	B
evolves	O
in	O
the	O
direction	O
x	O
d	O
xa	O
and	O
its	O
inner	O
product	O
xa	O
d	O
sa	O
since	O
all	O
the	O
columns	O
with	O
each	O
active	O
vector	B
is	O
given	O
by	O
x	O
of	O
x	O
have	O
unit	O
norm	O
this	O
means	O
the	O
angles	O
between	O
each	O
active	O
vector	B
and	O
the	O
evolving	O
fitted	O
vector	B
are	O
equal	O
and	O
hence	O
minimal	O
a	O
the	O
main	O
computational	O
burden	O
in	O
algorithm	B
is	O
in	O
step	O
computing	O
the	O
new	O
direction	O
each	O
time	O
the	O
active	B
set	B
is	O
updated	O
however	O
this	O
is	O
easily	O
performed	O
using	O
standard	O
updating	O
of	O
a	O
qr	B
decomposition	I
and	O
hence	O
the	O
computations	B
for	O
the	O
entire	O
path	B
are	O
of	O
the	O
same	O
order	O
as	O
that	O
of	O
a	O
single	O
least-squares	O
fit	O
using	O
all	O
the	O
variables	O
the	O
vertical	O
gray	O
lines	O
in	O
figure	O
show	O
when	O
the	O
active	B
set	B
changes	O
we	O
see	O
the	O
slopes	O
change	O
at	O
each	O
of	O
these	O
transitions	O
compare	O
with	O
the	O
corresponding	O
figure	O
for	O
forward-stepwise	B
regression	B
figure	O
shows	O
the	O
the	O
decreasing	O
covariance	O
during	O
the	O
steps	O
of	O
the	O
on	O
training	O
datacovariance	O
with	O
residuals	O
sparse	O
modeling	O
and	O
the	O
lasso	B
lar	O
algorithm	B
as	O
each	O
variable	O
joins	O
the	O
active	B
set	B
the	O
covariances	O
become	O
tied	O
at	O
the	O
end	O
of	O
the	O
path	B
the	O
covariances	O
are	O
all	O
zero	O
because	O
this	O
is	O
the	O
unregularized	O
ordinary	O
least-squares	O
solution	O
it	O
turns	O
out	O
that	O
the	O
lar	O
algorithm	B
is	O
not	O
quite	O
the	O
lasso	B
path	B
variables	O
can	O
drop	O
out	O
of	O
the	O
active	B
set	B
as	O
the	O
path	B
evolves	O
this	O
happens	O
when	O
a	O
coefficient	O
curve	O
passes	O
through	O
zero	O
the	O
subgradient	O
equations	O
imply	O
that	O
the	O
sign	O
of	O
each	O
active	O
coefficient	O
matches	O
the	O
sign	O
of	O
the	O
gradient	O
however	O
a	O
simple	O
addition	O
to	O
step	O
in	O
algorithm	B
takes	O
care	O
of	O
the	O
issue	O
lasso	B
modification	O
if	O
a	O
nonzero	O
coefficient	O
crosses	O
zero	O
before	O
the	O
next	O
variable	O
enters	O
drop	O
it	O
from	O
a	O
and	O
recompute	O
the	O
joint	O
least-squares	O
direction	O
using	O
the	O
reduced	O
set	B
figure	O
was	O
computed	O
using	O
the	O
lars	B
package	O
in	O
r	B
with	O
the	O
lasso	B
option	O
set	B
to	O
accommodate	O
step	O
in	O
this	O
instance	O
there	O
was	O
no	O
need	O
for	O
dropping	O
dropping	O
tends	O
to	O
occur	O
when	O
some	O
of	O
the	O
variables	O
are	O
highly	O
correlated	O
lasso	B
and	O
degrees	B
of	I
freedom	I
we	O
see	O
in	O
figure	O
panel	O
that	O
forward-stepwise	B
regression	B
is	O
more	O
aggressive	O
than	O
the	O
lasso	B
in	O
that	O
it	O
brings	O
down	O
the	O
training	O
mse	O
faster	O
we	O
can	O
use	O
the	O
covariance	O
formula	B
for	O
df	O
from	O
chapter	O
to	O
quantify	O
the	O
amount	O
of	O
fitting	O
at	O
each	O
step	O
in	O
the	O
right	O
panel	O
we	O
show	O
the	O
results	O
of	O
a	O
simulation	B
for	O
estimating	O
the	O
df	O
of	O
forward-stepwise	B
regression	B
and	O
the	O
lasso	B
for	O
the	O
spam	B
data	B
recall	O
the	O
covariance	O
formula	B
cov	O
yi	O
oyi	O
nx	O
df	O
d	O
these	O
covariances	O
are	O
of	O
course	O
with	O
respect	O
to	O
the	O
sampling	B
distribution	B
of	O
the	O
yi	O
which	O
we	O
do	O
not	O
have	O
access	O
to	O
since	O
these	O
are	O
real	O
data	B
so	O
instead	O
we	O
simulate	O
from	O
fitted	O
values	O
from	O
the	O
full	B
least-squares	O
fit	O
by	O
adding	O
gaussian	B
errors	B
with	O
the	O
appropriate	O
standard	B
deviation	I
is	O
the	O
parametric	B
bootstrap	B
calculation	O
it	O
turns	O
out	O
that	O
each	O
step	O
of	O
the	O
lar	O
algorithm	B
spends	O
one	O
df	O
as	O
is	O
evidenced	O
by	O
the	O
brown	O
curve	O
in	O
the	O
right	O
plot	O
of	O
figure	O
forward	O
stepwise	O
spends	O
more	O
df	O
in	O
the	O
earlier	O
stages	O
and	O
can	O
be	O
erratic	O
under	O
some	O
technical	O
conditions	B
on	O
the	O
x	O
matrix	B
guarantee	O
that	O
fitting	O
generalized	O
lasso	B
models	B
figure	O
left	O
training	O
mean-squared	O
error	O
on	O
the	O
spam	B
data	B
for	O
forward-stepwise	B
regression	B
and	O
the	O
lasso	B
as	O
a	O
function	B
of	O
the	O
size	O
of	O
the	O
active	B
set	B
forward	O
stepwise	O
is	O
more	O
aggressive	O
than	O
the	O
lasso	B
in	O
that	O
it	O
the	O
training	O
data	B
more	O
quickly	O
right	O
simulation	B
showing	O
the	O
degrees	B
of	I
freedom	I
or	O
df	O
of	O
forward-stepwise	B
regression	B
versus	O
lasso	B
the	O
lasso	B
uses	O
one	O
df	O
per	O
step	O
while	O
forward	O
stepwise	O
is	O
greedier	O
and	O
uses	O
more	O
especially	O
in	O
the	O
early	O
steps	O
since	O
these	O
df	O
were	O
computed	O
using	O
random	O
simulated	O
data	B
sets	O
we	O
include	O
standard-error	O
bands	O
on	O
the	O
estimates	O
lar	O
delivers	O
the	O
lasso	B
path	B
one	O
can	O
show	O
that	O
the	O
df	O
is	O
exactly	O
one	O
per	O
step	O
more	O
generally	O
for	O
the	O
lasso	B
if	O
we	O
definecdf	O
d	O
j	O
of	O
the	O
active	B
set	B
at	O
we	O
have	O
that	O
e	O
cdf	B
d	O
df	O
in	O
other	O
words	O
the	O
size	O
size	O
of	O
the	O
active	B
set	B
is	O
an	O
unbiased	O
estimate	B
of	O
df	O
ordinary	O
least	B
squares	I
with	O
a	O
predetermined	O
sequence	O
of	O
variables	O
spends	O
one	O
df	O
per	O
variable	O
intuitively	O
forward	O
stepwise	O
spends	O
more	O
because	O
it	O
pays	O
a	O
price	O
some	O
extra	O
df	O
for	O
searching	O
although	O
the	O
lasso	B
does	O
search	O
for	O
the	O
next	O
variable	O
it	O
does	O
not	O
fit	O
the	O
new	O
model	O
all	O
the	O
way	O
but	O
just	O
until	O
the	O
next	O
variable	O
enters	O
at	O
this	O
point	O
one	O
new	O
df	O
has	O
been	O
spent	O
fitting	O
generalized	O
lasso	B
models	B
so	O
far	O
we	O
have	O
focused	O
on	O
the	O
lasso	B
for	O
squared-error	O
loss	O
and	O
exploited	O
the	O
piecewise-linearity	O
of	O
its	O
coefficient	O
profile	O
to	O
efficiently	O
compute	O
the	O
entire	O
path	B
unfortunately	O
this	O
is	O
not	O
the	O
case	O
for	O
most	O
other	O
loss	O
functions	O
training	O
datasteptraining	O
msellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward	O
stepwiselasso	O
sparse	O
modeling	O
and	O
the	O
lasso	B
so	O
obtaining	O
the	O
coefficient	O
path	B
is	O
potentially	O
more	O
costly	O
as	O
a	O
case	O
in	O
point	O
we	O
will	O
use	O
logistic	B
regression	B
as	O
an	O
example	O
in	O
this	O
case	O
in	O
l	O
represents	O
the	O
negative	O
binomial	B
log-likelihood	B
writing	O
the	O
loss	O
explicitly	O
and	O
using	O
the	O
lagrange	O
form	B
for	O
the	O
penalty	B
we	O
wish	O
to	O
solve	O
nx	O
yi	O
log	O
c	O
yi	O
minimize	O
here	O
we	O
assume	O
the	O
yi	O
and	O
are	O
the	O
fitted	O
probabilities	B
n	O
c	O
d	O
e	O
c	O
e	O
i	O
i	O
similar	O
to	O
the	O
solution	O
satisfies	O
the	O
subgradient	O
condition	B
hxj	O
y	O
d	O
j	O
d	O
p	O
n	O
where	O
sj	O
sign	O
j	O
j	O
d	O
p	O
and	O
d	O
however	O
the	O
nonlinearity	B
of	O
in	O
j	O
results	O
in	O
piecewise	O
nonlinear	B
coefficient	O
profiles	O
instead	O
we	O
settle	O
for	O
a	O
solution	O
path	B
on	O
a	O
sufficiently	O
fine	O
grid	O
of	O
values	O
for	O
it	O
is	O
once	O
again	O
easy	O
to	O
see	O
that	O
the	O
largest	O
value	O
of	O
we	O
need	O
consider	O
is	O
d	O
max	O
jhxj	O
y	O
j	O
since	O
this	O
is	O
the	O
smallest	O
value	O
of	O
for	O
which	O
o	O
d	O
logit	B
ny	O
a	O
reasonable	O
sequence	O
is	O
values	O
equally	O
spaced	O
on	O
the	O
log-scale	O
from	O
down	O
to	O
where	O
is	O
some	O
small	O
fraction	O
such	O
as	O
d	O
and	O
o	O
an	O
approach	O
that	O
has	O
proven	O
to	O
be	O
surprisingly	O
efficient	O
is	O
path-wise	B
coordinate	I
descent	I
for	O
each	O
value	O
solve	O
the	O
lasso	B
problem	O
for	O
one	O
j	O
only	O
holding	O
all	O
the	O
others	O
fixed	O
cycle	O
around	O
until	O
the	O
estimates	O
stabilize	O
by	O
starting	O
at	O
where	O
all	O
the	O
parameters	O
are	O
zero	O
we	O
use	O
warm	B
starts	I
in	O
computing	O
the	O
solutions	O
at	O
the	O
decreasing	O
sequence	O
of	O
values	O
the	O
warm	B
starts	I
provide	O
excellent	O
initializations	O
for	O
the	O
sequence	O
of	O
solutions	O
o	O
the	O
active	B
set	B
grows	O
slowly	O
as	O
decreases	O
computational	O
hedges	O
that	O
guess	O
the	O
active	B
set	B
prove	O
to	O
be	O
particularly	O
efficient	O
if	O
the	O
guess	O
is	O
good	O
correct	O
one	O
iterates	O
coordinate	O
descent	O
using	O
only	O
those	O
variables	O
pn	O
yi	O
d	O
n	O
pn	O
the	O
equation	B
for	O
the	O
intercept	O
is	O
n	O
fitting	O
generalized	O
lasso	B
models	B
until	O
convergence	O
one	O
more	O
sweep	O
through	O
all	O
the	O
variables	O
confirms	O
the	O
hunch	O
the	O
r	B
package	O
glmnet	B
employs	O
a	O
proximal-newton	B
strategy	O
at	O
each	O
value	O
compute	O
a	O
weighted	B
least	B
squares	I
approximation	O
to	O
the	O
loglikelihood	O
l	O
at	O
the	O
current	O
estimate	B
for	O
the	O
solution	O
vector	B
o	O
this	O
produces	O
a	O
working	B
response	B
and	O
observation	O
weights	B
as	O
in	O
a	O
regular	O
glm	O
solve	O
the	O
weighted	O
least-squares	O
lasso	B
at	O
by	O
coordinate	O
descent	O
using	O
warm	B
starts	I
and	O
active-set	O
iterations	O
we	O
now	O
give	O
some	O
details	O
which	O
illustrate	O
why	O
these	O
particular	O
strate	O
gies	O
are	O
effective	O
consider	O
the	O
weighted	O
least-squares	O
problem	O
wi	O
x	O
i	O
c	O
j	O
minimize	O
p	O
with	O
all	O
but	O
j	O
fixed	O
at	O
their	O
current	O
values	O
writing	O
ri	O
d	O
zi	O
j	O
xi	O
we	O
can	O
recast	O
as	O
wi	O
xij	O
j	O
c	O
jj	O
nx	O
nx	O
minimize	O
j	O
nx	O
n	O
a	O
one-dimensional	O
problem	O
the	O
subgradient	O
equation	B
is	O
wi	O
xij	O
xij	O
j	O
sign	O
j	O
d	O
the	O
simplest	O
form	B
of	O
the	O
solution	O
occurs	O
if	O
each	O
variable	O
is	O
standardized	O
to	O
have	O
weighted	O
mean	O
zero	O
and	O
variance	B
one	O
and	O
the	O
weights	B
sum	O
to	O
one	O
in	O
that	O
case	O
we	O
have	O
a	O
two-step	O
solution	O
compute	O
the	O
weighted	O
simple	O
least-squares	O
coefficient	O
q	O
j	O
d	O
hxj	O
riw	O
d	O
nx	O
j	O
to	O
produce	O
o	O
soft-threshold	B
q	O
j	O
o	O
j	O
d	O
sign	O
j	O
q	O
q	O
if	O
j	O
q	O
j	O
jj	O
otherwise	O
wi	O
xij	O
ri	O
sparse	O
modeling	O
and	O
the	O
lasso	B
without	O
the	O
standardization	O
the	O
solution	O
is	O
almost	O
as	O
simple	O
but	O
less	O
intuitive	O
hence	O
each	O
coordinate-descent	O
update	O
essentially	O
requires	O
an	O
inner	O
product	O
followed	O
by	O
the	O
soft	O
thresholding	O
operation	O
this	O
is	O
especially	O
convenient	O
for	O
xij	O
that	O
are	O
stored	O
in	O
sparse-matrix	O
format	O
since	O
then	O
the	O
inner	O
products	O
need	O
only	O
visit	O
the	O
nonzero	O
values	O
if	O
the	O
coefficient	O
is	O
zero	O
before	O
the	O
step	O
and	O
remains	O
zero	O
one	O
just	O
moves	O
on	O
otherwise	O
the	O
model	O
is	O
updated	O
moving	O
from	O
the	O
solution	O
at	O
which	O
jhxj	O
riwj	O
d	O
for	O
all	O
the	O
nonzero	O
coefficients	O
o	O
j	O
down	O
to	O
the	O
smaller	O
one	O
might	O
expect	O
all	O
variables	O
for	O
which	O
jhxj	O
riwj	O
would	O
be	O
natural	O
candidates	O
for	O
the	O
new	O
active	B
set	B
the	O
strong	B
rules	I
lower	O
the	O
bar	O
somewhat	O
and	O
include	O
any	O
variables	O
for	O
which	O
jhxj	O
riwj	O
this	O
tends	O
to	O
rarely	O
make	O
mistakes	O
and	O
still	O
leads	O
to	O
considerable	O
computational	O
savings	O
apart	O
from	O
variations	O
in	O
the	O
loss	B
function	B
other	O
penalties	O
are	O
of	O
interest	O
as	O
well	O
in	O
particular	O
the	O
elastic	B
net	I
penalty	B
bridges	O
the	O
gap	O
between	O
the	O
lasso	B
and	O
ridge	B
regression	B
that	O
penalty	B
is	O
defined	O
as	O
c	O
k	O
p	O
d	O
where	O
the	O
factor	B
in	O
the	O
first	O
term	O
is	O
for	O
mathematical	O
convenience	O
when	O
the	O
predictors	B
are	O
excessively	O
correlated	O
the	O
lasso	B
performs	O
somewhat	O
poorly	O
since	O
it	O
has	O
difficulty	O
in	O
choosing	O
among	O
the	O
correlated	O
cousins	O
like	O
ridge	B
regression	B
the	O
elastic	B
net	I
shrinks	O
the	O
coefficients	O
of	O
correlated	O
variables	O
toward	O
each	O
other	O
and	O
tends	O
to	O
select	O
correlated	O
variables	O
in	O
groups	O
in	O
this	O
case	O
the	O
co-ordinate	O
descent	O
update	O
is	O
almost	O
as	O
simple	O
as	O
in	O
o	O
j	O
d	O
sign	O
q	O
j	O
q	O
if	O
j	O
q	O
j	O
otherwise	O
again	O
assuming	O
the	O
observations	O
have	O
weighted	O
variance	B
equal	O
to	O
one	O
when	O
d	O
the	O
update	O
corresponds	O
to	O
a	O
coordinate	O
update	O
for	O
ridge	B
regression	B
figure	O
compares	O
lasso	B
with	O
forward-stepwise	B
logistic	B
regression	B
on	O
the	O
spam	B
data	B
here	O
using	O
all	O
binarized	O
variables	O
and	O
their	O
pairwise	O
interactions	O
this	O
amounts	O
to	O
variables	O
in	O
all	O
once	O
degenerate	O
variables	O
have	O
been	O
excised	O
forward	O
stepwise	O
takes	O
a	O
long	O
time	O
to	O
run	O
since	O
it	O
enters	O
one	O
variable	O
at	O
a	O
time	O
and	O
after	O
each	O
one	O
has	O
been	O
selected	O
a	O
new	O
glm	O
must	O
be	O
fit	O
the	O
lasso	B
path	B
as	O
fit	O
by	O
glmnet	B
includes	O
many	O
new	O
variables	O
at	O
each	O
step	O
and	O
is	O
extremely	O
fast	O
s	O
for	O
the	O
entire	O
path	B
for	O
very	O
large	O
post-selection	B
inference	B
for	O
the	O
lasso	B
figure	O
test	O
misclassification	O
error	O
for	O
lasso	B
versus	O
forward-stepwise	B
logistic	B
regression	B
on	O
the	O
spam	B
data	B
where	O
we	O
consider	O
pairwise	O
interactions	O
as	O
well	O
as	O
main	O
effects	O
predictors	B
in	O
all	O
here	O
the	O
minimum	O
error	O
for	O
lasso	B
is	O
versus	O
for	O
stepwise	O
logistic	B
regression	B
and	O
for	O
the	O
main-effects-only	O
lasso	B
logistic	B
regression	B
model	I
the	O
stepwise	O
models	B
went	O
up	O
to	O
variables	O
before	O
encountering	O
convergence	O
issues	O
while	O
the	O
lasso	B
had	O
a	O
largest	O
active	B
set	B
of	O
size	O
and	O
wide	O
modern	O
data	B
sets	O
of	O
examples	O
and	O
millions	O
of	O
variables	O
the	O
lasso	B
path	B
algorithm	B
is	O
feasible	O
and	O
attractive	O
post-selection	B
inference	B
for	O
the	O
lasso	B
this	O
chapter	O
is	O
mostly	O
about	O
building	O
interpretable	O
models	B
for	O
prediction	O
with	O
little	O
attention	O
paid	O
to	O
inference	B
indeed	O
inference	B
is	O
generally	O
difficult	O
for	O
adaptively	O
selected	O
models	B
which	O
ends	O
up	O
selecting	O
a	O
subset	O
a	O
of	O
size	O
j	O
a	O
suppose	O
we	O
have	O
fit	O
a	O
lasso	B
regression	B
model	I
with	O
a	O
particular	O
value	O
for	O
j	O
d	O
k	O
of	O
the	O
p	O
available	O
variables	O
the	O
question	O
arises	O
as	O
to	O
whether	O
we	O
can	O
assign	O
p-values	O
to	O
these	O
selected	O
variables	O
and	O
produce	O
confidence	O
intervals	B
for	O
their	O
coefficients	O
a	O
recent	O
burst	O
of	O
research	O
activity	O
has	O
made	O
progress	O
on	O
these	O
important	O
problems	O
we	O
give	O
a	O
very	O
brief	O
survey	O
here	O
with	O
references	O
ap	O
data	B
with	O
interactionspercentage	O
null	O
deviance	B
explained	O
on	O
training	O
datatest	O
misclassification	O
errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward	O
stepwise	O
logistic	O
regressionlasso	O
logistic	B
regression	B
sparse	O
modeling	O
and	O
the	O
lasso	B
pearing	O
in	O
the	O
notes	O
we	O
discuss	O
post-selection	B
inference	B
more	O
generally	O
in	O
chapter	O
one	O
question	O
that	O
arises	O
is	O
whether	O
we	O
are	O
interested	O
in	O
making	O
inferences	O
about	O
the	O
population	O
regression	B
parameters	O
using	O
the	O
full	B
set	B
of	O
p	O
predictors	B
or	O
whether	O
interest	O
is	O
restricted	O
to	O
the	O
population	O
regression	B
parameters	O
using	O
only	O
the	O
subset	O
a	O
for	O
the	O
first	O
case	O
it	O
has	O
been	O
proposed	O
that	O
one	O
can	O
view	O
the	O
coefficients	O
of	O
the	O
selected	O
model	O
as	O
an	O
efficient	O
but	O
biased	O
estimate	B
of	O
the	O
full	B
population	O
coefficient	O
vector	B
the	O
idea	O
is	O
to	O
then	O
debias	B
this	O
estimate	B
allowing	O
inference	B
for	O
the	O
full	B
vector	B
of	O
coefficients	O
of	O
course	O
sharper	O
inference	B
will	O
be	O
available	O
for	O
the	O
stronger	O
variables	O
that	O
were	O
selected	O
in	O
the	O
first	O
place	O
figure	O
hiv	O
data	B
linear	B
regression	B
of	O
drug	O
resistance	O
in	O
hiv-positive	O
patients	O
on	O
seven	O
sites	O
indicators	O
of	O
mutations	O
at	O
particular	O
genomic	O
locations	O
these	O
seven	O
sites	O
were	O
selected	O
from	O
a	O
total	O
of	O
candidates	O
using	O
the	O
lasso	B
the	O
naive	O
confidence	O
intervals	B
use	O
standard	O
linear-regression	O
inference	B
ignoring	O
the	O
selection	O
event	O
the	O
light	O
intervals	B
are	O
confidence	O
intervals	B
using	O
linear	B
regression	B
but	O
conditioned	O
on	O
the	O
selection	O
event	O
for	O
the	O
second	O
case	O
the	O
idea	O
is	O
to	O
condition	B
on	O
the	O
selection	O
events	O
and	O
hence	O
the	O
set	B
a	O
itself	O
and	O
then	O
perform	O
conditional	B
inference	B
on	O
the	O
intervalselection	O
adjusted	O
interval	B
connections	O
and	O
extensions	O
unrestricted	O
not	O
lasso-shrunk	O
regression	B
coefficients	O
of	O
the	O
response	B
on	O
only	O
the	O
variables	O
in	O
a	O
for	O
the	O
case	O
of	O
a	O
lasso	B
with	O
squared-error	O
loss	O
it	O
turns	O
out	O
that	O
the	O
set	B
of	O
response	B
vectors	O
y	O
rn	O
that	O
would	O
lead	O
to	O
a	O
particular	O
subset	O
a	O
of	O
variables	O
in	O
the	O
active	B
set	B
form	B
a	O
convex	O
polytope	O
in	O
rn	O
we	O
condition	B
on	O
the	O
signs	O
of	O
the	O
coefficients	O
as	O
well	O
ignoring	O
the	O
signs	O
leads	O
to	O
a	O
finite	O
union	O
of	O
such	O
polytopes	O
this	O
along	O
with	O
delicate	O
gaussian	B
conditioning	O
arguments	O
leads	O
to	O
truncated	O
gaussian	B
and	O
t-distrubtions	O
for	O
parameters	O
of	O
interest	O
figure	O
shows	O
the	O
results	O
of	O
using	O
the	O
lasso	B
to	O
select	O
variables	O
in	O
an	O
hiv	O
study	O
the	O
outcome	O
y	O
is	O
a	O
measure	O
of	O
the	O
resistence	O
to	O
an	O
treatment	O
reverse	O
transcriptase	O
inhibitor	O
and	O
the	O
predictors	B
are	O
indicators	O
of	O
whether	O
mutations	O
had	O
occurred	O
at	O
particular	O
genomic	O
sites	O
lasso	B
regression	B
with	O
cross-validation	B
selected	O
a	O
value	O
of	O
d	O
and	O
the	O
seven	O
sites	O
indicated	O
in	O
the	O
figure	O
had	O
nonzero	O
coefficients	O
the	O
dark	O
bars	O
in	O
the	O
figure	O
indicate	O
standard	O
confidence	O
intervals	B
for	O
the	O
coefficients	O
of	O
the	O
selected	O
variables	O
using	O
linear	B
regression	B
and	O
ignoring	O
the	O
fact	O
that	O
the	O
lasso	B
was	O
used	O
to	O
select	O
the	O
variables	O
three	O
variables	O
are	O
significant	O
and	O
two	O
more	O
nearly	O
so	O
the	O
lighter	O
bars	O
are	O
confidence	O
intervals	B
in	O
a	O
similar	O
regression	B
but	O
conditioned	O
on	O
the	O
selection	O
event	O
we	O
see	O
that	O
they	O
are	O
generally	O
wider	O
and	O
only	O
variable	O
remains	O
significant	O
connections	O
and	O
extensions	O
there	O
are	O
interesting	O
connections	O
between	O
lasso	B
models	B
and	O
other	O
popular	O
approaches	O
to	O
the	O
prediction	O
problem	O
we	O
will	O
briefly	O
cover	O
two	O
of	O
these	O
here	O
namely	O
support-vector	O
machines	O
and	O
boosting	B
lasso	B
logistic	B
regression	B
and	O
the	O
svm	B
we	O
show	O
in	O
section	O
that	O
ridged	O
logistic	B
regression	B
has	O
a	O
lot	O
in	O
common	O
with	O
the	O
linear	B
support-vector	O
machine	O
for	O
separable	O
data	B
the	O
limit	O
as	O
in	O
ridged	O
logistic	B
regression	B
coincides	O
with	O
the	O
svm	B
in	O
addition	O
their	O
loss	O
functions	O
are	O
somewhat	O
similar	O
the	O
same	O
holds	O
true	O
for	O
regularized	O
logistic	B
regression	B
versus	O
the	O
svm	B
their	O
end-path	O
limits	O
are	O
the	O
same	O
in	O
fact	O
due	O
to	O
the	O
similarity	O
of	O
the	O
loss	O
functions	O
their	O
solutions	O
are	O
not	O
too	O
different	O
elsewhere	O
along	O
the	O
path	B
however	O
the	O
end-path	O
behavior	O
is	O
a	O
little	O
more	O
complex	O
they	O
both	O
converge	O
to	O
the	O
maximizing	O
margin	B
separator	O
that	O
is	O
the	O
margin	B
is	O
measured	O
with	O
respect	O
to	O
the	O
distance	O
of	O
points	O
to	O
the	O
decision	O
boundary	O
or	O
maximum	O
absolute	O
coordinate	O
sparse	O
modeling	O
and	O
the	O
lasso	B
lasso	B
and	O
boosting	B
in	O
chapter	O
we	O
discuss	O
boosting	B
a	O
general	O
method	B
for	O
building	O
a	O
complex	O
prediction	O
model	O
using	O
simple	O
building	O
components	O
in	O
its	O
simplest	O
form	B
boosting	B
amounts	O
to	O
the	O
following	O
simple	O
iteration	O
inititialize	O
b	O
d	O
and	O
f	O
wd	O
for	O
b	O
d	O
b	O
compute	O
the	O
residuals	O
ri	O
d	O
yi	O
f	O
i	O
d	O
n	O
fit	O
a	O
small	O
regression	B
tree	I
to	O
the	O
observations	O
ri	O
update	O
f	O
b	O
x	O
d	O
f	O
c	O
gb	O
x	O
the	O
smallness	O
of	O
the	O
tree	O
limits	O
the	O
interaction	O
order	O
of	O
the	O
model	O
a	O
tree	O
with	O
only	O
two	O
splits	O
involves	O
at	O
most	O
two	O
variables	O
the	O
number	O
of	O
terms	O
b	O
and	O
the	O
shrinkage	B
parameter	O
are	O
both	O
tuning	O
parameters	O
that	O
control	B
the	O
rate	B
of	O
learning	O
hence	O
overfitting	O
and	O
need	O
to	O
be	O
set	B
for	O
example	O
by	O
cross-validation	B
think	O
of	O
as	O
estimating	O
a	O
function	B
gb	O
x	O
and	O
which	O
we	O
can	O
in	O
words	O
this	O
algorithm	B
performs	O
a	O
search	O
in	O
the	O
space	O
of	O
trees	B
for	O
the	O
one	O
most	O
correlated	O
with	O
the	O
residual	O
and	O
then	O
moves	O
the	O
fitted	O
function	B
f	O
b	O
a	O
small	O
amount	O
in	O
that	O
direction	O
a	O
process	O
known	O
as	O
forward-stagewise	B
fitting	O
one	O
can	O
paraphrase	O
this	O
simple	O
algorithm	B
in	O
the	O
context	O
of	O
linear	B
regression	B
where	O
in	O
step	O
the	O
space	O
of	O
small	O
trees	B
is	O
replaced	O
by	O
linear	B
functions	O
inititialize	O
d	O
and	O
standardize	O
all	O
the	O
variables	O
xj	O
j	O
d	O
p	O
for	O
b	O
d	O
b	O
compute	O
the	O
residuals	O
r	B
d	O
y	O
x	O
b	O
find	O
the	O
predictor	B
xj	O
most	O
correlated	O
with	O
the	O
residual	O
vector	B
r	B
and	O
c	O
sj	O
being	O
the	O
sign	O
of	O
d	O
b	O
update	O
b	O
to	O
where	O
the	O
correlation	O
leaving	O
all	O
the	O
other	O
components	O
alone	O
j	O
j	O
for	O
small	O
the	O
solution	O
paths	O
for	O
this	O
least-squares	O
boosting	B
and	O
the	O
lasso	B
are	O
very	O
similar	O
it	O
is	O
natural	O
to	O
consider	O
the	O
limiting	O
case	O
or	O
infinitesimal	O
forward	O
stagewise	O
fitting	O
which	O
we	O
will	O
abbreviate	O
ifs	O
one	O
can	O
imagine	O
a	O
scenario	O
where	O
a	O
number	O
of	O
variables	O
are	O
vying	O
to	O
win	O
the	O
competition	O
in	O
step	O
and	O
once	O
they	O
are	O
tied	O
their	O
coefficients	O
move	O
in	O
concert	O
as	O
they	O
each	O
get	O
incremented	O
this	O
was	O
in	O
fact	O
the	O
inspiration	O
for	O
the	O
lar	O
algorithm	B
where	O
a	O
represents	O
the	O
set	B
of	O
tied	O
variables	O
and	O
is	O
the	O
relative	O
number	O
of	O
turns	O
they	O
each	O
have	O
in	O
getting	O
their	O
coefficients	O
updated	O
it	O
turns	O
out	O
that	O
ifs	O
is	O
often	O
but	O
not	O
always	O
exactly	O
the	O
lasso	B
it	O
can	O
instead	O
be	O
characterized	O
as	O
a	O
type	O
of	O
monotone	B
lasso	B
notes	O
and	O
details	O
not	O
only	O
do	O
these	O
connections	O
inspire	O
new	O
insights	O
and	O
algorithms	O
for	O
the	O
lasso	B
they	O
also	O
offer	O
insights	O
into	O
boosting	B
we	O
can	O
think	O
of	O
boosting	B
as	O
fitting	O
a	O
monotone	B
lasso	B
path	B
in	O
the	O
high-dimensional	O
space	O
of	O
variables	O
defined	O
by	O
all	O
possible	O
trees	B
of	O
a	O
certain	O
size	O
extensions	O
of	O
the	O
lasso	B
the	O
group	B
lasso	B
penaltypk	O
the	O
idea	O
of	O
using	O
regularization	B
to	O
induce	O
sparsity	B
has	O
taken	O
hold	O
and	O
variations	O
of	O
these	O
ideas	O
have	O
spread	O
like	O
wildfire	O
in	O
applied	O
statistical	O
modeling	O
along	O
with	O
advances	O
in	O
convex	B
optimization	I
hardly	O
any	O
branch	O
of	O
applied	O
statistics	B
has	O
been	O
left	O
untouched	O
we	O
don	O
t	B
go	O
into	O
detail	O
here	O
but	O
refer	O
the	O
reader	O
to	O
the	O
references	O
in	O
the	O
endnotes	O
instead	O
we	O
will	O
end	O
this	O
section	O
with	O
a	O
list	O
of	O
such	O
applications	O
which	O
may	O
entice	O
the	O
reader	O
to	O
venture	O
into	O
this	O
domain	O
applies	O
to	O
vectors	O
of	O
parameters	O
and	O
selects	O
whole	O
groups	O
at	O
a	O
time	O
armed	O
with	O
these	O
penalties	O
one	O
can	O
derive	O
lasso-like	O
schemes	O
for	O
including	O
multilevel	O
factors	O
in	O
linear	B
models	B
as	O
well	O
as	O
hierarchical	O
schemes	O
for	O
including	O
low-order	O
interactions	O
the	O
graphical	B
lasso	B
applies	O
penalties	O
in	O
the	O
problem	O
of	O
edge	O
selection	O
in	O
dependence	O
graphs	O
sparse	O
principal	B
components	I
employ	O
penalties	O
to	O
produce	O
components	O
with	O
many	O
loadings	O
zero	O
the	O
same	O
ideas	O
are	O
applied	O
to	O
discriminant	O
analysis	B
and	O
canonical	O
correlation	O
analysis	B
the	O
nuclear	B
norm	I
of	O
a	O
matrix	B
is	O
the	O
sum	O
of	O
its	O
singular	O
values	O
a	O
lasso	B
penalty	B
on	O
matrices	O
nuclear-norm	O
regularization	B
is	O
popular	O
in	O
matrix	B
completion	I
for	O
estimating	O
missing	O
entries	O
in	O
a	O
matrix	B
notes	O
and	O
details	O
classical	O
regression	B
theory	B
aimed	O
for	O
an	O
unbiased	O
estimate	B
of	O
each	O
predictor	B
variable	O
s	O
effect	O
modern	O
wide	B
data	B
sets	O
often	O
with	O
enormous	O
numbers	O
of	O
predictors	B
p	O
make	O
that	O
an	O
untenable	O
goal	O
the	O
methods	O
described	O
here	O
by	O
necessity	O
use	O
shrinkage	B
methods	O
biased	B
estimation	B
and	O
sparsity	B
the	O
lasso	B
was	O
introduced	O
by	O
tibshirani	O
and	O
has	O
spawned	O
a	O
great	O
deal	O
of	O
research	O
the	O
recent	O
monograph	O
by	O
hastie	O
et	O
al	O
gives	O
a	O
compact	O
summary	O
of	O
some	O
of	O
the	O
areas	O
where	O
the	O
lasso	B
and	O
sparsity	B
have	O
been	O
applied	O
the	O
regression	B
version	O
of	O
boosting	B
was	O
given	O
in	O
hastie	O
et	O
al	O
chapter	O
and	O
inspired	O
the	O
least-angle	B
regression	B
algorithm	B
sparse	O
modeling	O
and	O
the	O
lasso	B
et	O
al	O
a	O
new	O
and	O
more	O
democratic	O
version	O
of	O
forward-stepwise	B
regression	B
as	O
well	O
as	O
a	O
fast	O
algorithm	B
for	O
fitting	O
the	O
lasso	B
these	O
authors	O
showed	O
under	O
some	O
conditions	B
that	O
each	O
step	O
of	O
the	O
lar	O
algorithm	B
corresponds	O
to	O
one	O
df	O
zou	O
et	O
al	O
show	O
that	O
with	O
a	O
fixed	O
the	O
size	O
of	O
the	O
active	B
set	B
is	O
unbiased	O
for	O
the	O
df	O
for	O
the	O
lasso	B
hastie	O
et	O
al	O
also	O
view	O
boosting	B
as	O
fitting	O
a	O
lasso	B
regularization	B
path	B
in	O
the	O
high-dimensional	O
space	O
of	O
trees	B
friedman	O
et	O
al	O
developed	O
the	O
pathwise	O
coordinate-descent	O
algorithm	B
for	O
generalized	O
lasso	B
problems	O
and	O
provide	O
the	O
glmnet	B
package	O
for	O
r	B
et	O
al	O
strong	B
rules	I
for	O
lasso	B
screening	O
are	O
due	O
to	O
tibshirani	O
et	O
al	O
hastie	O
et	O
al	O
chapter	O
show	O
the	O
similarity	O
between	O
the	O
svm	B
and	O
lasso	B
logistic	B
regression	B
we	O
now	O
give	O
some	O
particular	O
technical	O
details	O
on	O
topics	O
covered	O
in	O
the	O
chapter	O
forward-stepwise	B
computations	B
building	O
up	O
the	O
forward-stepwise	B
model	O
can	O
be	O
seen	O
as	O
a	O
guided	O
gram	O
schmidt	O
orthogonalization	O
decomposition	O
after	O
step	O
r	B
all	O
p	O
r	B
variables	O
not	O
in	O
the	O
model	O
are	O
orthogonal	O
to	O
the	O
r	B
in	O
the	O
model	O
and	O
the	O
latter	O
are	O
in	O
qr	O
form	B
then	O
the	O
next	O
variable	O
to	O
enter	O
is	O
the	O
one	O
most	O
correlated	O
with	O
the	O
residuals	O
this	O
is	O
the	O
one	O
that	O
will	O
reduce	O
the	O
residual	O
sum-of-squares	O
the	O
most	O
and	O
one	O
requires	O
p	O
r	B
n-vector	O
inner	O
products	O
to	O
identify	O
it	O
the	O
regression	B
is	O
then	O
updated	O
trivially	O
to	O
accommodate	O
the	O
chosen	O
one	O
which	O
is	O
then	O
regressed	O
out	O
of	O
the	O
p	O
r	B
remaining	O
variables	O
iteratively	B
reweighted	I
least	B
squares	I
generalized	O
linear	B
models	B
are	O
fit	O
by	O
maximum-likelihood	O
and	O
since	O
the	O
log-likelihood	B
is	O
differentiable	O
and	O
concave	O
typically	O
a	O
newton	O
algorithm	B
is	O
used	O
the	O
newton	O
algorithm	B
can	O
be	O
recast	O
as	O
an	O
iteratively	O
reweighted	O
linear	B
regression	B
algorithm	B
and	O
nelder	O
at	O
each	O
iteration	O
one	O
computes	O
a	O
working	B
response	B
variable	O
zi	O
and	O
a	O
weight	O
per	O
observation	O
wi	O
of	O
which	O
depend	O
on	O
the	O
current	O
parameter	O
vector	B
o	O
then	O
the	O
newton	O
update	O
for	O
o	O
is	O
obtained	O
by	O
a	O
weighted	O
least-squares	O
fit	O
of	O
the	O
zi	O
on	O
the	O
xi	O
with	O
weights	B
wi	O
et	O
al	O
section	O
forward-stepwise	B
logistic	B
regression	B
computations	B
although	O
the	O
current	O
model	O
is	O
in	O
the	O
form	B
of	O
a	O
weighted	O
least-squares	O
fit	O
the	O
p	O
r	B
variables	O
not	O
in	O
the	O
model	O
cannot	O
be	O
kept	O
orthogonal	O
to	O
those	O
in	O
the	O
model	O
weights	B
keep	O
changing	O
however	O
since	O
our	O
current	O
model	O
will	O
have	O
performed	O
a	O
weighted	O
qr	B
decomposition	I
this	O
orthogonalization	O
can	O
be	O
obtained	O
without	O
too	O
much	O
cost	O
we	O
will	O
need	O
p	O
r	B
multiplications	O
of	O
an	O
r	B
n	O
matrix	B
with	O
an	O
n	O
vector	B
o	O
p	O
r	B
r	B
n	O
computations	B
an	O
even	O
simpler	O
alternative	O
for	O
the	O
selection	O
is	O
to	O
use	O
the	O
size	O
of	O
the	O
gradient	O
of	O
the	O
notes	O
and	O
details	O
log-likelihood	B
which	O
simply	O
requires	O
an	O
inner	O
product	O
jhy	O
xjij	O
for	O
each	O
omitted	O
variable	O
xj	O
all	O
the	O
variables	O
are	O
standardized	O
to	O
unit	O
variance	B
best	O
interpolant	O
if	O
p	O
n	O
then	O
another	O
boundary	O
solution	O
becomes	O
interesting	O
for	O
the	O
lasso	B
for	O
t	B
sufficiently	O
large	O
we	O
will	O
be	O
able	O
to	O
achieve	O
a	O
perfect	O
fit	O
to	O
the	O
data	B
and	O
hence	O
a	O
zero	O
residual	O
there	O
will	O
be	O
many	O
such	O
solutions	O
so	O
it	O
becomes	O
interesting	O
to	O
find	O
the	O
perfect-fit	O
solution	O
with	O
smallest	O
value	O
of	O
t	B
the	O
perfect-fit	O
solution	O
this	O
requires	O
solving	O
a	O
separate	O
convex-optimization	O
problem	O
more	O
on	O
df	O
when	O
the	O
search	O
is	O
easy	O
in	O
that	O
a	O
variable	O
stands	O
out	O
as	O
far	O
superior	O
lar	O
takes	O
a	O
big	O
step	O
and	O
forward	O
stepwise	O
spends	O
close	O
to	O
a	O
unit	O
df	O
on	O
the	O
other	O
hand	O
when	O
there	O
is	O
close	O
competition	O
the	O
lar	O
steps	O
are	O
small	O
and	O
a	O
unit	O
df	O
is	O
spent	O
for	O
little	O
progress	O
while	O
forward	O
stepwise	O
can	O
spend	O
a	O
fair	O
bit	O
more	O
than	O
a	O
unit	O
df	O
price	O
paid	O
for	O
searching	O
in	O
fact	O
the	O
dfj	O
curve	O
for	O
forward	O
stepwise	O
can	O
exceed	O
p	O
for	O
j	O
p	O
et	O
al	O
post-selection	B
inference	B
there	O
has	O
been	O
a	O
lot	O
of	O
activity	O
around	O
post-selection	B
inference	B
for	O
lasso	B
and	O
related	O
methods	O
all	O
of	O
it	O
since	O
to	O
a	O
large	O
extent	O
this	O
was	O
inspired	O
by	O
the	O
work	O
of	O
berk	O
et	O
al	O
but	O
more	O
tailored	O
to	O
the	O
particular	O
selection	O
process	O
employed	O
by	O
the	O
lasso	B
for	O
the	O
debiasing	O
approach	O
we	O
look	O
to	O
the	O
work	O
of	O
zhang	O
and	O
zhang	O
van	O
de	O
geer	O
et	O
al	O
and	O
javanmard	O
and	O
montanari	O
the	O
conditional	B
inference	B
approach	O
began	O
with	O
lockhart	O
et	O
al	O
and	O
then	O
was	O
developed	O
further	O
in	O
a	O
series	O
of	O
papers	O
et	O
al	O
taylor	O
et	O
al	O
fithian	O
et	O
al	O
with	O
many	O
more	O
in	O
the	O
pipeline	O
selective	O
inference	B
software	O
the	O
example	O
in	O
figure	O
was	O
produced	O
using	O
the	O
r	B
package	O
selectiveinference	B
et	O
al	O
thanks	O
to	O
rob	O
tibshirani	O
for	O
providing	O
this	O
example	O
end-path	O
behavior	O
of	O
ridge	O
and	O
lasso	B
logistic	B
regression	B
for	O
separable	O
data	B
the	O
details	O
here	O
are	O
somewhat	O
technical	O
and	O
rely	O
on	O
dual	B
norms	O
details	O
are	O
given	O
in	O
hastie	O
et	O
al	O
section	O
lar	O
and	O
boosting	B
least-squares	O
boosting	B
moves	O
the	O
winning	O
coefficient	O
in	O
the	O
direction	O
of	O
the	O
correlation	O
of	O
its	O
variable	O
with	O
the	O
residual	O
the	O
direction	O
computed	O
in	O
step	O
of	O
the	O
lar	O
algorithm	B
may	O
have	O
some	O
components	O
whose	O
signs	O
do	O
not	O
agree	O
with	O
their	O
correlations	O
especially	O
if	O
the	O
variables	O
are	O
very	O
correlated	O
this	O
can	O
be	O
fixed	O
by	O
a	O
particular	O
nonnegative	O
least-squares	O
fit	O
to	O
yield	O
an	O
exact	O
path	B
algorithm	B
for	O
ifs	O
details	O
can	O
be	O
found	O
in	O
efron	O
et	O
al	O
random	O
forests	O
and	O
boosting	B
in	O
the	O
modern	O
world	O
we	O
are	O
often	O
faced	O
with	O
enormous	O
data	B
sets	O
both	O
in	O
terms	O
of	O
the	O
number	O
of	O
observations	O
n	O
and	O
in	O
terms	O
of	O
the	O
number	O
of	O
variables	O
p	O
this	O
is	O
of	O
course	O
good	O
news	O
we	O
have	O
always	O
said	O
the	O
more	O
data	B
we	O
have	O
the	O
better	O
predictive	O
models	B
we	O
can	O
build	O
well	O
we	O
are	O
there	O
now	O
we	O
have	O
tons	O
of	O
data	B
and	O
must	O
figure	O
out	O
how	O
to	O
use	O
it	O
although	O
we	O
can	O
scale	B
up	O
our	O
software	O
to	O
fit	O
the	O
collection	O
of	O
linear	B
and	O
generalized	O
linear	B
models	B
to	O
these	O
behemoths	O
they	O
are	O
often	O
too	O
modest	O
and	O
can	O
fall	O
way	O
short	O
in	O
terms	O
of	O
predictive	O
power	O
a	O
need	O
arose	O
for	O
some	O
general	O
purpose	O
tools	O
that	O
could	O
scale	B
well	O
to	O
these	O
bigger	O
problems	O
and	O
exploit	O
the	O
large	O
amount	O
of	O
data	B
by	O
fitting	O
a	O
much	O
richer	O
class	O
of	O
functions	O
almost	O
automatically	O
random	O
forests	O
and	O
boosting	B
are	O
two	O
relatively	O
recent	O
innovations	O
that	O
fit	O
the	O
bill	O
and	O
have	O
become	O
very	O
popular	O
as	O
out-thebox	O
learning	O
algorithms	O
that	O
enjoy	O
good	O
predictive	O
performance	O
random	O
forests	O
are	O
somewhat	O
more	O
automatic	O
than	O
boosting	B
but	O
can	O
also	O
suffer	O
a	O
small	O
performance	O
hit	O
as	O
a	O
consequence	O
these	O
two	O
methods	O
have	O
something	O
in	O
common	O
they	O
both	O
represent	O
the	O
fitted	O
model	O
by	O
a	O
sum	O
of	O
regression	B
trees	B
we	O
discuss	O
trees	B
in	O
some	O
detail	O
in	O
chapter	O
a	O
single	O
regression	B
tree	I
is	O
typically	O
a	O
rather	O
weak	O
prediction	O
model	O
it	O
is	O
rather	O
amazing	O
that	O
an	O
ensemble	B
of	O
trees	B
leads	O
to	O
the	O
state	O
of	O
the	O
art	O
in	O
black-box	O
predictors	B
we	O
can	O
broadly	O
describe	O
both	O
these	O
methods	O
very	O
simply	O
random	B
forest	I
grow	O
many	O
deep	O
regression	B
trees	B
to	O
randomized	O
versions	O
of	O
the	O
training	O
data	B
and	O
average	O
them	O
here	O
randomized	O
is	O
a	O
wideranging	O
term	O
and	O
includes	O
bootstrap	B
sampling	O
andor	O
subsampling	O
of	O
the	O
observations	O
as	O
well	O
as	O
subsampling	O
of	O
the	O
variables	O
boosting	B
repeatedly	O
grow	O
shallow	O
trees	B
to	O
the	O
residuals	O
and	O
hence	O
build	O
up	O
an	O
additive	B
model	I
consisting	O
of	O
a	O
sum	O
of	O
trees	B
the	O
basic	O
mechanism	O
in	O
random	O
forests	O
is	O
variance	B
reduction	I
by	O
averaging	B
each	O
deep	O
tree	O
has	O
a	O
high	O
variance	B
and	O
the	O
averaging	B
brings	O
the	O
vari	O
random	O
forests	O
ance	O
down	O
in	O
boosting	B
the	O
basic	O
mechanism	O
is	O
bias	B
reduction	O
although	O
different	O
flavors	O
include	O
some	O
variance	B
reduction	I
as	O
well	O
both	O
methods	O
inherit	O
all	O
the	O
good	O
attributes	O
of	O
trees	B
most	O
notable	O
of	O
which	O
is	O
variable	O
selection	O
random	O
forests	O
suppose	O
we	O
have	O
the	O
usual	O
setup	O
for	O
a	O
regression	B
problem	O
with	O
a	O
training	B
set	B
consisting	O
of	O
an	O
p	O
data	B
matrix	B
x	O
and	O
an	O
n-vector	O
of	O
responses	O
y	O
a	O
tree	O
fits	O
a	O
piecewise	O
constant	O
surface	O
or	O
x	O
over	O
the	O
domain	O
x	O
by	O
recursive	O
partitioning	O
the	O
model	O
is	O
built	O
in	O
a	O
greedy	O
fashion	O
each	O
time	O
creating	O
two	O
daughter	O
nodes	B
from	O
a	O
terminal	B
node	I
by	O
defining	O
a	O
binary	O
split	O
using	O
one	O
of	O
the	O
available	O
variables	O
the	O
model	O
can	O
hence	O
be	O
represented	O
by	O
a	O
binary	O
tree	O
part	O
of	O
the	O
art	O
in	O
using	O
regression	B
trees	B
is	O
to	O
know	O
how	O
deep	O
to	O
grow	O
the	O
tree	O
or	O
alternatively	O
how	O
much	O
to	O
prune	O
it	O
back	O
typically	O
that	O
is	O
achieved	O
using	O
left-out	O
data	B
or	O
cross-validation	B
figure	O
shows	O
a	O
tree	O
fit	O
to	O
the	O
spam	B
training	O
data	B
the	O
splitting	O
variables	O
and	O
split	O
points	O
are	O
indicated	O
each	O
node	O
is	O
labeled	O
as	O
spam	B
or	O
ham	O
spam	B
see	O
footnote	O
on	O
page	O
the	O
numbers	O
beneath	O
each	O
node	O
show	O
misclassifiedtotal	O
the	O
overall	O
misclassification	O
error	O
on	O
the	O
test	O
data	B
is	O
which	O
compares	O
poorly	O
with	O
the	O
performance	O
of	O
the	O
lasso	B
for	O
linear	B
lasso	B
for	O
lasso	B
with	O
interactions	O
the	O
surface	O
or	O
x	O
here	O
is	O
clearly	O
complex	O
and	O
by	O
its	O
nature	O
represents	O
a	O
rather	O
high-order	B
interaction	I
deepest	O
branch	O
is	O
eight	O
levels	O
and	O
involves	O
splits	O
on	O
eight	O
different	O
variables	O
despite	O
the	O
promise	O
to	O
deliver	O
interpretable	O
models	B
this	O
bushy	O
tree	O
is	O
not	O
easy	O
to	O
interpret	O
nevertheless	O
trees	B
have	O
some	O
desirable	O
properties	O
the	O
following	O
lists	O
some	O
of	O
the	O
good	O
and	O
bad	O
properties	O
of	O
trees	B
l	O
trees	B
automatically	O
select	O
variables	O
only	O
variables	O
used	O
in	O
defining	O
splits	O
are	O
in	O
the	O
model	O
l	O
tree-growing	O
algorithms	O
scale	B
well	O
to	O
large	O
n	O
growing	O
a	O
tree	O
is	O
a	O
divide	O
and-conquer	O
operation	O
l	O
trees	B
handle	O
mixed	B
features	I
seamlessly	O
and	O
can	O
deal	O
with	O
missing	B
data	B
l	O
small	O
trees	B
are	O
easy	O
to	O
interpret	O
m	O
large	O
trees	B
are	O
not	O
easy	O
to	O
interpret	O
m	O
trees	B
do	O
not	O
generally	O
have	O
good	O
prediction	O
performance	O
trees	B
are	O
inherently	O
high-variance	O
function	B
estimators	O
and	O
the	O
bushier	O
they	O
are	O
the	O
higher	O
the	O
variance	B
the	O
early	O
splits	O
dictate	O
the	O
architecture	O
of	O
random	O
forests	O
and	O
boosting	B
figure	O
regression	B
tree	I
fit	O
to	O
the	O
binary	O
spam	B
data	B
a	O
bigger	O
version	O
of	O
figure	O
the	O
initial	O
trained	O
tree	O
was	O
far	O
bushier	O
than	O
the	O
one	O
displayed	O
it	O
was	O
then	O
optimally	O
pruned	O
using	O
cross-validation	B
the	O
tree	O
on	O
the	O
other	O
hand	O
deep	O
bushy	O
trees	B
localize	O
the	O
training	O
data	B
the	O
variables	O
that	O
matter	O
to	O
a	O
relatively	O
small	O
region	B
around	O
the	O
target	O
point	O
this	O
suggests	O
low	O
bias	B
the	O
idea	O
of	O
random	O
forests	O
its	O
predecessor	O
bagging	B
is	O
to	O
grow	O
many	O
very	O
bushy	O
trees	B
and	O
get	O
rid	O
of	O
the	O
variance	B
by	O
averaging	B
in	O
order	O
to	O
benefit	O
from	O
averaging	B
the	O
individual	O
trees	B
should	O
not	O
be	O
too	O
correlated	O
this	O
is	O
achieved	O
by	O
injecting	O
some	O
randomness	O
into	O
the	O
tree-growing	O
process	O
random	O
forests	O
achieve	O
this	O
in	O
two	O
ways	O
random	O
forests	O
bootstrap	B
each	O
tree	O
is	O
grown	O
to	O
a	O
bootstrap	B
resampled	O
training	O
data	B
set	B
which	O
makes	O
them	O
different	O
and	O
somewhat	O
decorrelates	O
them	O
split-variable	B
randomization	B
each	O
time	O
a	O
split	O
is	O
to	O
be	O
performed	O
the	O
search	O
for	O
the	O
split	O
variable	O
is	O
limited	O
to	O
a	O
random	O
subset	O
of	O
m	O
of	O
the	O
p	O
variables	O
typical	O
values	O
of	O
m	O
are	O
p	O
or	O
p	O
when	O
m	O
d	O
p	O
the	O
randomization	B
amounts	O
to	O
using	O
only	O
step	O
and	O
was	O
an	O
earlier	O
ancestor	O
of	O
random	O
forests	O
called	O
bagging	B
in	O
most	O
examples	O
the	O
second	O
level	O
of	O
randomization	B
pays	O
dividends	O
b	O
algorithm	B
random	B
forest	I
given	O
training	O
data	B
set	B
d	O
d	O
y	O
fix	O
m	O
p	O
and	O
the	O
number	O
of	O
trees	B
for	O
b	O
d	O
b	O
do	O
the	O
following	O
create	O
a	O
bootstrap	B
version	O
of	O
the	O
training	O
data	B
d	O
b	O
by	O
randomly	O
sampling	O
the	O
n	O
rows	O
with	O
replacement	O
n	O
times	O
the	O
sample	B
can	O
be	O
represented	O
by	O
the	O
bootstrap	B
frequency	O
vector	B
w	O
grow	O
a	O
maximal-depth	O
tree	O
orb	O
x	O
using	O
the	O
data	B
in	O
d	O
of	O
the	O
p	O
features	O
at	O
random	O
prior	B
to	O
making	O
each	O
split	O
b	O
sampling	O
m	O
b	O
save	O
the	O
tree	O
as	O
well	O
as	O
the	O
bootstrap	B
sampling	O
frequencies	O
for	O
each	O
of	O
the	O
training	O
observations	O
compute	O
the	O
random-forest	O
fit	O
at	O
any	O
prediction	O
point	O
as	O
the	O
average	O
bx	O
d	O
b	O
compute	O
the	O
oobi	O
error	O
for	O
each	O
response	B
observation	O
yi	O
in	O
the	O
training	O
data	B
by	O
using	O
the	O
fit	O
or	O
obtained	O
by	O
averaging	B
only	O
those	O
orb	O
xi	O
for	O
which	O
observation	O
i	O
was	O
not	O
in	O
the	O
bootstrap	B
sample	B
the	O
overall	O
oob	O
error	O
is	O
the	O
average	O
of	O
these	O
oobi	O
rf	O
algorithm	B
gives	O
some	O
of	O
the	O
details	O
some	O
more	O
are	O
given	O
in	O
the	O
technical	O
notes	O
the	O
package	O
randomforest	B
in	O
r	B
sets	O
as	O
a	O
default	O
m	O
d	O
p	O
random	O
forests	O
are	O
easy	O
to	O
use	O
since	O
there	O
is	O
not	O
much	O
tuning	O
needed	O
p	O
for	O
classification	O
trees	B
and	O
m	O
d	O
for	O
regression	B
trees	B
but	O
one	O
can	O
use	O
other	O
values	O
with	O
m	O
d	O
the	O
split	O
variable	O
is	O
completely	O
random	O
so	O
all	O
variables	O
get	O
a	O
chance	O
this	O
will	O
decorrelate	O
the	O
trees	B
the	O
most	O
but	O
can	O
create	O
bias	B
somewhat	O
similar	O
to	O
that	O
in	O
ridge	B
regression	B
figure	O
shows	O
the	O
random	O
forests	O
and	O
boosting	B
figure	O
test	O
misclassification	O
error	O
of	O
random	O
forests	O
on	O
the	O
spam	B
data	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	B
the	O
red	O
curve	O
selects	O
m	O
d	O
of	O
the	O
p	O
d	O
features	O
at	O
random	O
as	O
candidates	O
for	O
the	O
split	O
variable	O
each	O
time	O
a	O
split	O
is	O
made	O
the	O
blue	O
curve	O
uses	O
m	O
d	O
and	O
hence	O
amounts	O
to	O
bagging	B
both	O
bagging	B
and	O
random	O
forests	O
outperform	O
the	O
lasso	B
methods	O
and	O
a	O
single	O
tree	O
misclassification	O
performance	O
of	O
a	O
random	B
forest	I
on	O
the	O
spam	B
test	O
data	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	B
averaged	O
we	O
see	O
that	O
in	O
this	O
case	O
after	O
a	O
relatively	O
small	O
number	O
of	O
trees	B
the	O
error	O
levels	O
off	O
the	O
number	O
b	O
of	O
trees	B
averaged	O
is	O
not	O
a	O
real	O
tuning	O
parameter	O
as	O
with	O
the	O
bootstrap	B
and	O
we	O
need	O
a	O
sufficient	O
number	O
for	O
the	O
estimate	B
to	O
stabilize	O
but	O
cannot	O
overfit	O
by	O
having	O
too	O
many	O
random	O
forests	O
have	O
been	O
described	O
as	O
adaptive	B
nearest-neighbor	O
estimators	O
adaptive	B
in	O
that	O
they	O
select	O
predictors	B
a	O
k-nearest-neighbor	O
estimate	B
finds	O
the	O
k	O
training	O
observations	O
closest	O
in	O
feature	O
space	O
to	O
the	O
target	O
point	O
and	O
averages	O
their	O
responses	O
each	O
tree	O
in	O
the	O
random	B
forest	I
drills	O
down	O
by	O
recursive	O
partitioning	O
to	O
pure	O
terminal	O
nodes	B
often	O
consisting	O
of	O
a	O
single	O
observation	O
hence	O
when	O
evaluating	O
the	O
prediction	O
from	O
each	O
tree	O
d	O
y	O
for	O
some	O
and	O
for	O
many	O
of	O
the	O
trees	B
this	O
could	O
be	O
the	O
same	O
forest	O
on	O
the	O
spam	B
datanumber	O
of	O
treestest	O
forestsingle	O
treelassolasso	O
random	O
forests	O
from	O
the	O
whole	O
collection	O
of	O
b	O
trees	B
the	O
number	O
of	O
distinct	O
can	O
be	O
fairly	O
small	O
since	O
the	O
partitioning	O
that	O
reaches	O
the	O
terminal	O
nodes	B
involves	O
only	O
a	O
subset	O
of	O
the	O
predictors	B
the	O
neighborhoods	O
so	O
defined	O
are	O
adaptive	B
out-of-bag	B
error	I
estimates	O
random	O
forests	O
deliver	O
cross-validated	O
error	O
estimates	O
at	O
virtually	O
no	O
extra	O
cost	O
the	O
idea	O
is	O
similar	O
to	O
the	O
bootstrap	B
error	O
estimates	O
discussed	O
in	O
chapter	O
the	O
computation	O
is	O
described	O
in	O
step	O
of	O
algorithm	B
in	O
making	O
the	O
prediction	O
for	O
observation	O
pair	O
yi	O
we	O
average	O
all	O
the	O
random-forest	O
trees	B
orb	O
xi	O
for	O
which	O
that	O
pair	O
is	O
not	O
in	O
the	O
corresponding	O
bootstrap	B
sample	B
figure	O
out-of-bag	O
misclassification	O
error	O
estimate	B
for	O
the	O
spam	B
data	B
versus	O
the	O
test	O
error	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	B
x	O
b	O
w	O
w	O
b	O
i	O
or	O
rf	O
d	O
bi	O
orb	O
xi	O
where	O
bi	O
is	O
the	O
number	O
of	O
times	O
observation	O
i	O
was	O
not	O
in	O
the	O
bootstrap	B
we	O
then	O
compute	O
the	O
oob	O
sample	B
expected	O
value	O
e	O
nx	O
error	O
estimate	B
erroob	O
d	O
n	O
l	O
yi	O
or	O
rf	O
of	O
treesmisclassification	O
errortest	O
error	O
random	O
forests	O
and	O
boosting	B
where	O
l	O
is	O
the	O
loss	B
function	B
of	O
interest	O
such	O
as	O
misclassification	O
or	O
squarederror	O
loss	O
if	O
b	O
is	O
sufficiently	O
large	O
three	O
times	O
the	O
number	O
needed	O
for	O
the	O
random	B
forest	I
to	O
stabilize	O
we	O
can	O
see	O
that	O
the	O
oob	O
error	O
estimate	B
is	O
equivalent	O
to	O
leave-one-out	O
cross-validation	B
error	O
standard	O
errors	B
is	O
given	O
by	O
bvjack	O
we	O
can	O
use	O
very	O
similar	O
ideas	O
to	O
estimate	B
the	O
variance	B
of	O
a	O
random-forest	O
prediction	O
using	O
the	O
jackknife	B
variance	B
estimator	B
in	O
chapter	O
if	O
o	O
is	O
a	O
statistic	B
estimated	O
using	O
all	O
n	O
training	O
observations	O
then	O
the	O
jackknife	B
estimate	B
of	O
the	O
variance	B
of	O
o	O
d	O
n	O
o	O
is	O
the	O
estimate	B
using	O
all	O
but	O
observation	O
i	O
and	O
o	O
d	O
at	O
is	O
obtained	O
by	O
simply	O
plugging	O
into	O
this	O
formula	B
o	O
o	O
o	O
where	O
o	O
the	O
natural	O
jackknife	B
variance	B
estimate	B
for	O
a	O
random-forest	O
prediction	O
n	O
i	O
d	O
n	O
rf	O
nx	O
p	O
n	O
nx	O
n	O
this	O
formula	B
is	O
derived	O
under	O
the	O
b	O
d	O
setting	O
in	O
which	O
case	O
is	O
an	O
expectation	O
under	O
bootstrap	B
sampling	O
and	O
hence	O
is	O
free	O
of	O
monte	O
carlo	O
variability	O
this	O
also	O
makes	O
the	O
distinction	O
clear	O
we	O
are	O
estimating	O
the	O
sampling	O
variability	O
of	O
a	O
random-forest	O
prediction	O
as	O
distinct	O
from	O
any	O
monte	O
carlo	O
variation	O
in	O
practice	O
b	O
is	O
finite	O
and	O
expression	O
will	O
have	O
monte	O
carlo	O
bias	B
and	O
variance	B
all	O
of	O
the	O
or	O
rf	O
are	O
based	O
on	O
b	O
bootstrap	B
samples	O
and	O
they	O
are	O
hence	O
noisy	O
versions	O
of	O
their	O
expectations	O
since	O
the	O
n	O
quantities	O
summed	O
in	O
are	O
squared	O
by	O
jensen	O
s	O
inequality	O
we	O
will	O
have	O
positive	O
bias	B
it	O
turns	O
out	O
that	O
this	O
bias	B
dominates	O
the	O
monte	B
carlo	I
variance	B
hence	O
one	O
would	O
want	O
to	O
use	O
a	O
much	O
larger	O
value	O
of	O
b	O
when	O
estimating	O
variances	O
than	O
was	O
used	O
in	O
the	O
original	O
randomforest	B
fit	O
alternatively	O
one	O
can	O
use	O
the	O
same	O
b	O
bootstrap	B
samples	O
as	O
were	O
used	O
to	O
fit	O
the	O
random	B
forest	I
along	O
with	O
a	O
bias-corrected	B
version	O
of	O
the	O
jackknife	B
variance	B
estimate	B
bvu	O
n	O
b	O
random	O
forests	O
figure	O
jackknife	B
standard	B
error	I
estimates	O
bias	B
correction	O
for	O
the	O
probability	O
estimates	O
in	O
the	O
spam	B
test	O
data	B
the	O
points	O
labeled	O
red	O
were	O
misclassifications	O
and	O
tend	O
to	O
concentrate	O
near	O
the	O
decision	O
boundary	O
where	O
e	O
d	O
and	O
d	O
b	O
bx	O
the	O
bootstrap	B
estimate	B
of	O
the	O
variance	B
of	O
a	O
single	O
random-forest	O
tree	O
all	O
these	O
quantities	O
are	O
easily	O
computed	O
from	O
the	O
output	O
of	O
a	O
random	B
forest	I
so	O
they	O
are	O
immediately	O
available	O
figure	O
shows	O
the	O
predicted	O
probabilities	B
and	O
their	O
jackknife	B
estimated	O
standard	O
errors	B
for	O
the	O
spam	B
test	O
data	B
the	O
estimates	O
near	O
the	O
decision	O
boundary	O
tend	O
to	O
have	O
higher	O
standard	O
errors	B
variable-importance	O
plots	O
a	O
random	B
forest	I
is	O
something	O
of	O
a	O
black	O
box	O
giving	O
good	O
predictions	O
but	O
usually	O
not	O
much	O
insight	O
into	O
the	O
underlying	O
surface	O
it	O
has	O
fit	O
each	O
random-forest	O
tree	O
orb	O
will	O
have	O
used	O
a	O
subset	O
of	O
the	O
predictors	B
as	O
splitting	O
variables	O
and	O
each	O
tree	O
is	O
likely	O
to	O
use	O
overlapping	O
but	O
not	O
necessarily	O
predictionstandard	O
error	O
estimate	B
random	O
forests	O
and	O
boosting	B
figure	O
variable-importance	O
plots	O
for	O
random	O
forests	O
fit	O
to	O
the	O
spam	B
data	B
on	O
the	O
left	O
we	O
have	O
the	O
m	O
d	O
random	B
forest	I
due	O
to	O
the	O
split-variable	B
randomization	B
it	O
spreads	O
the	O
importance	O
among	O
the	O
variables	O
on	O
the	O
right	O
is	O
the	O
m	O
d	O
random	B
forest	I
or	O
bagging	B
which	O
focuses	O
on	O
a	O
smaller	O
subset	O
of	O
the	O
variables	O
identical	O
subsets	O
one	O
might	O
conclude	O
that	O
any	O
variable	O
never	O
used	O
in	O
any	O
of	O
the	O
trees	B
is	O
unlikely	O
to	O
be	O
important	O
but	O
we	O
would	O
like	O
a	O
method	B
of	O
assessing	O
the	O
relative	O
importance	O
of	O
variables	O
that	O
are	O
included	O
in	O
the	O
ensemble	B
variable-importance	O
plots	O
fit	O
this	O
bill	O
whenever	O
a	O
variable	O
is	O
used	O
in	O
a	O
tree	O
the	O
algorithm	B
logs	O
the	O
decrease	O
in	O
the	O
split-criterion	O
due	O
to	O
this	O
split	O
these	O
are	O
accumulated	O
over	O
all	O
the	O
trees	B
for	O
each	O
variable	O
and	O
summarized	O
as	O
relative	O
importance	O
measures	O
figure	O
demonstrates	O
this	O
on	O
the	O
spam	B
data	B
we	O
see	O
that	O
the	O
m	O
d	O
random	B
forest	I
by	O
virtue	O
of	O
the	O
split-variable	B
randomization	B
spreads	O
the	O
importance	O
out	O
much	O
more	O
than	O
bagging	B
which	O
always	O
gets	O
to	O
pick	O
the	O
best	O
variable	O
for	O
splitting	O
in	O
this	O
sense	O
small	O
m	O
has	O
some	O
similarity	O
to	O
ridge	B
regression	B
which	O
also	O
tends	O
to	O
share	O
the	O
coefficients	O
evenly	O
among	O
correlated	O
variables	O
forest	O
m	O
m	O
importance	O
boosting	B
with	O
squared-error	O
loss	O
boosting	B
with	O
squared-error	O
loss	O
boosting	B
was	O
originally	O
proposed	O
as	O
a	O
means	O
for	O
improving	O
the	O
performance	O
of	O
weak	O
learners	O
in	O
binary	O
classification	O
problems	O
this	O
was	O
achieved	O
through	O
resampling	B
training	O
points	O
giving	O
more	O
weight	O
to	O
those	O
which	O
had	O
been	O
misclassified	O
to	O
produce	O
a	O
new	O
classifier	O
that	O
would	O
boost	O
the	O
performance	O
in	O
previously	O
problematic	O
areas	O
of	O
feature	O
space	O
this	O
process	O
is	O
repeated	O
generating	O
a	O
stream	O
of	O
classifiers	O
which	O
are	O
ultimately	O
combined	O
through	O
to	O
produce	O
the	O
final	O
classifier	O
the	O
prototypical	O
weak	B
learner	I
was	O
a	O
decision	O
tree	O
boosting	B
has	O
evolved	O
since	O
this	O
earliest	O
invention	O
and	O
different	O
flavors	O
are	O
popular	O
in	O
statistics	B
computer	O
science	O
and	O
other	O
areas	O
of	O
pattern	O
recognition	O
and	O
prediction	O
we	O
focus	O
on	O
the	O
version	O
popular	O
in	O
statistics	B
gradient	B
boosting	B
and	O
return	O
to	O
this	O
early	O
version	O
later	O
in	O
the	O
chapter	O
algorithm	B
algorithm	B
gradient	B
boosting	B
with	O
squared-error	O
loss	O
shrinkage	B
factor	B
and	O
the	O
tree	O
depth	B
d	O
set	B
the	O
initial	O
fit	O
and	O
given	O
a	O
training	O
sample	B
d	O
d	O
y	O
fix	O
the	O
number	O
of	O
steps	O
b	O
the	O
the	O
residual	O
vector	B
r	B
d	O
y	O
for	O
b	O
d	O
b	O
repeat	O
fit	O
a	O
regression	B
tree	I
qgb	O
to	O
the	O
data	B
r	B
grown	O
best-first	O
to	O
depth	B
d	O
this	O
means	O
the	O
total	O
number	O
of	O
splits	O
are	O
d	O
and	O
each	O
successive	O
update	O
the	O
fitted	O
model	O
with	O
a	O
shrunken	O
version	O
of	O
qgbbgb	O
c	O
split	O
is	O
made	O
to	O
that	O
terminal	B
node	I
that	O
yields	O
the	O
biggest	O
reduction	O
in	O
residual	O
sum	O
of	O
squares	O
ogb	O
with	O
ogb	O
d	O
qgb	O
return	O
the	O
sequence	O
of	O
fitted	O
functionsbgb	O
b	O
d	O
b	O
update	O
the	O
residuals	O
accordingly	O
ri	O
d	O
ri	O
ogb	O
xi	O
i	O
d	O
n	O
gives	O
the	O
most	O
basic	O
version	O
of	O
gradient	B
boosting	B
for	O
squared-error	O
loss	O
this	O
amounts	O
to	O
building	O
a	O
model	O
by	O
repeatedly	O
fitting	O
a	O
regression	B
tree	I
to	O
the	O
residuals	O
importantly	O
the	O
tree	O
is	O
typically	O
quite	O
small	O
involving	O
a	O
small	O
number	O
d	O
of	O
splits	O
it	O
is	O
indeed	O
a	O
weak	B
learner	I
after	O
each	O
tree	O
has	O
been	O
grown	O
to	O
the	O
residuals	O
it	O
is	O
shrunk	O
down	O
by	O
a	O
factor	B
before	O
it	O
is	O
added	O
to	O
the	O
current	O
model	O
this	O
is	O
a	O
means	O
of	O
slowing	O
the	O
learning	O
process	O
despite	O
the	O
obvious	O
similarities	O
with	O
a	O
random	B
forest	I
boosting	B
is	O
different	O
in	O
a	O
fundamental	O
way	O
the	O
trees	B
in	O
a	O
random	B
forest	I
are	O
identically	O
each	O
classifier	O
predicts	O
a	O
class	O
label	O
and	O
the	O
class	O
with	O
the	O
most	O
votes	O
wins	O
random	O
forests	O
and	O
boosting	B
figure	O
test	O
performance	O
of	O
a	O
boosted	O
regression-tree	O
model	O
fit	O
to	O
the	O
als	B
training	O
data	B
with	O
n	O
d	O
and	O
p	O
d	O
shown	O
is	O
the	O
mean-squared	O
error	O
on	O
the	O
designated	O
test	O
observations	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	B
here	O
the	O
depth	B
d	O
d	O
and	O
d	O
boosting	B
achieves	O
a	O
lower	O
test	O
mse	O
than	O
a	O
random	B
forest	I
we	O
see	O
that	O
as	O
the	O
number	O
of	O
trees	B
b	O
gets	O
large	O
the	O
test	O
error	O
for	O
boosting	B
starts	O
to	O
increase	O
a	O
consequence	O
of	O
overfitting	O
the	O
random	B
forest	I
does	O
not	O
overfit	O
the	O
dotted	O
blue	O
horizontal	O
line	O
shows	O
the	O
best	O
performance	O
of	O
a	O
linear	B
model	I
fit	O
by	O
the	O
lasso	B
the	O
differences	O
are	O
less	O
dramatic	O
than	O
they	O
appear	O
since	O
the	O
vertical	O
scale	B
does	O
not	O
extend	O
to	O
zero	O
distributed	O
the	O
same	O
treatment	O
is	O
repeatedly	O
applied	O
to	O
the	O
same	O
data	B
with	O
boosting	B
on	O
the	O
other	O
hand	O
each	O
tree	O
is	O
trying	O
to	O
amend	O
errors	B
made	O
by	O
the	O
ensemble	B
of	O
previously	O
grown	O
trees	B
the	O
number	O
of	O
terms	O
b	O
is	O
important	O
as	O
well	O
because	O
unlike	O
random	O
forests	O
a	O
boosted	O
regression	B
model	I
can	O
overfit	O
if	O
b	O
is	O
too	O
large	O
hence	O
there	O
are	O
three	O
tuning	O
parameters	O
b	O
d	O
and	O
and	O
each	O
can	O
change	O
the	O
performance	O
of	O
a	O
boosted	O
model	O
sometimes	O
considerably	O
figure	O
shows	O
the	O
test	O
performance	O
of	O
boosting	B
on	O
the	O
als	B
data	B
these	O
data	B
represent	O
measurements	O
on	O
patients	O
with	O
amyotrophic	O
lateral	O
sclerosis	O
gehrig	O
s	O
disease	O
the	O
goal	O
is	O
to	O
predict	O
the	O
rate	B
of	O
progression	O
of	O
an	O
als	B
functional	O
rating	O
score	O
there	O
are	O
training	O
of	O
treesmean	O
squared	O
forestlasso	O
boosting	B
with	O
squared-error	O
loss	O
measurements	O
on	O
predictors	B
and	O
the	O
response	B
with	O
a	O
corresponding	O
test	O
set	B
of	O
size	O
observations	O
as	O
is	O
often	O
the	O
case	O
boosting	B
slightly	O
outperforms	O
a	O
random	B
forest	I
here	O
but	O
at	O
a	O
price	O
careful	O
tuning	O
of	O
boosting	B
requires	O
considerable	O
extra	O
work	O
with	O
time-costly	O
rounds	O
of	O
cross-validation	B
whereas	O
random	O
forests	O
are	O
almost	O
automatic	O
in	O
the	O
following	O
sections	O
we	O
explore	O
in	O
more	O
detail	O
some	O
of	O
the	O
tuning	O
parameters	O
the	O
r	B
package	O
gbm	B
implements	O
gradient	B
boosting	B
with	O
some	O
added	O
bells	O
and	O
whistles	O
by	O
default	O
it	O
grows	O
each	O
new	O
tree	O
on	O
a	O
random	O
sub-sample	O
of	O
the	O
training	O
data	B
apart	O
from	O
speeding	O
up	O
the	O
computations	B
this	O
has	O
a	O
similar	O
effect	O
to	O
bagging	B
and	O
results	O
in	O
some	O
variance	B
reduction	I
in	O
the	O
ensemble	B
we	O
can	O
also	O
compute	O
a	O
variable-importance	B
plot	I
as	O
we	O
did	O
for	O
random	O
forests	O
this	O
is	O
displayed	O
in	O
figure	O
for	O
the	O
als	B
data	B
only	O
of	O
the	O
variables	O
were	O
ever	O
used	O
with	O
one	O
variable	O
onset	O
delta	O
standing	O
out	O
ahead	O
of	O
the	O
others	O
this	O
measures	O
the	O
amount	O
of	O
time	O
that	O
has	O
elapsed	O
since	O
the	O
patient	O
was	O
first	O
diagnosed	O
with	O
als	B
and	O
hence	O
a	O
larger	O
value	O
will	O
indicate	O
a	O
slower	O
progression	O
rate	B
tree	O
depth	B
and	O
interaction	O
order	O
tree	O
depth	B
d	O
is	O
an	O
important	O
parameter	O
for	O
gradient	O
boosted	O
models	B
and	O
the	O
right	O
choice	O
will	O
depend	O
on	O
the	O
data	B
at	O
hand	O
here	O
depth	B
d	O
d	O
appears	O
to	O
be	O
a	O
good	O
choice	O
on	O
the	O
test	O
data	B
without	O
test	O
data	B
we	O
could	O
use	O
crossvalidation	O
to	O
make	O
the	O
selection	O
apart	O
from	O
a	O
general	O
complexity	O
measure	O
suppose	O
we	O
have	O
a	O
fitted	O
boosted	O
modelbgb	O
using	O
b	O
trees	B
denote	O
by	O
tree	O
depth	B
also	O
controls	O
the	O
interaction	O
order	O
of	O
the	O
the	O
easiest	O
case	O
is	O
with	O
d	O
d	O
where	O
each	O
tree	O
consists	O
of	O
a	O
single	O
split	O
stump	O
d	O
bg	O
the	O
indices	O
of	O
the	O
trees	B
that	O
made	O
the	O
single	O
split	O
bj	O
using	O
variable	O
j	O
for	O
j	O
d	O
p	O
these	O
bj	O
are	O
disjoint	O
b	O
can	O
be	O
b	O
a	O
interaction	O
is	O
also	O
known	O
as	O
a	O
k-way	O
interaction	O
hence	O
an	O
order-one	O
interaction	O
model	O
has	O
two-way	O
interactions	O
and	O
an	O
order-zero	O
model	O
is	O
additive	O
random	O
forests	O
and	O
boosting	B
figure	O
variable	O
importance	O
plot	O
for	O
the	O
als	B
data	B
here	O
of	O
the	O
variables	O
were	O
used	O
in	O
the	O
ensemble	B
there	O
are	O
too	O
many	O
variables	O
for	O
the	O
labels	O
to	O
be	O
visible	O
so	O
this	O
plot	O
serves	O
as	O
a	O
visual	O
guide	O
variable	O
onset	O
delta	O
has	O
relative	O
importance	O
lowest	O
red	O
bar	O
more	O
than	O
double	O
the	O
next	O
two	O
at	O
around	O
and	O
alsfrs	O
score	O
slope	O
however	O
the	O
importances	O
drop	O
off	O
slowly	O
suggesting	O
that	O
the	O
model	O
requires	O
a	O
significant	O
fraction	O
of	O
the	O
variables	O
empty	O
andsp	O
bj	O
d	O
b	O
then	O
we	O
can	O
write	O
ogb	O
x	O
x	O
bgb	O
d	O
bx	O
d	O
px	O
d	O
px	O
ogb	O
x	O
bj	O
o	O
fj	O
importance	O
plot	O
for	O
boosting	B
on	O
the	O
als	B
data	B
boosting	B
with	O
squared-error	O
loss	O
figure	O
als	B
test	O
error	O
for	O
boosted	O
models	B
with	O
different	O
depth	B
parameters	O
d	O
and	O
all	O
using	O
the	O
same	O
shrinkage	B
parameter	O
d	O
it	O
appears	O
that	O
d	O
d	O
is	O
inferior	O
to	O
the	O
rest	O
with	O
d	O
d	O
about	O
the	O
best	O
with	O
d	O
d	O
overfitting	O
begins	O
around	O
trees	B
with	O
d	O
d	O
around	O
while	O
neither	O
of	O
the	O
other	O
two	O
show	O
evidence	O
of	O
overfitting	O
by	O
trees	B
hence	O
boosted	O
stumps	O
fits	O
an	O
additive	B
model	I
but	O
in	O
a	O
fully	O
adaptive	B
way	O
it	O
selects	O
variables	O
and	O
also	O
selects	O
how	O
much	O
action	O
to	O
devote	O
to	O
each	O
variable	O
we	O
return	O
to	O
additive	O
models	B
in	O
section	O
figure	O
shows	O
the	O
three	O
functions	O
with	O
highest	O
relative	O
importance	O
the	O
first	O
function	B
confirms	O
that	O
a	O
longer	O
time	O
since	O
diagnosis	O
negative	O
onset	O
delta	O
predicts	O
a	O
slower	O
decline	O
last	O
slope	O
weight	O
is	O
the	O
difference	O
in	O
body	O
weight	O
at	O
the	O
last	O
two	O
visits	O
again	O
positive	O
is	O
good	O
likewise	O
for	O
alsfrs	O
score	O
slope	O
which	O
measures	O
the	O
local	O
slope	O
of	O
the	O
frs	O
score	O
after	O
the	O
first	O
two	O
visits	O
in	O
a	O
similar	O
way	O
boosting	B
with	O
d	O
d	O
fits	O
a	O
two-way	O
interaction	O
model	O
each	O
tree	O
involves	O
at	O
most	O
two	O
variables	O
in	O
general	O
boosting	B
with	O
d	O
d	O
k	O
leads	O
to	O
a	O
interaction	O
model	O
interaction	O
order	O
is	O
perhaps	O
a	O
more	O
natural	O
way	O
to	O
think	O
of	O
model	O
complexity	O
of	O
treesmean	O
squared	O
random	O
forests	O
and	O
boosting	B
figure	O
three	O
of	O
the	O
fitted	O
functions	O
for	O
the	O
als	B
data	B
in	O
a	O
boosted	O
stumps	O
model	O
d	O
each	O
centered	O
to	O
average	O
zero	O
over	O
the	O
training	O
data	B
in	O
terms	O
of	O
the	O
outcome	O
bigger	O
is	O
better	O
decline	O
in	O
frs	O
the	O
first	O
function	B
confirms	O
that	O
a	O
longer	O
time	O
since	O
diagnosis	O
negative	O
value	O
of	O
onset	O
delta	O
predicts	O
a	O
slower	O
decline	O
the	O
variable	O
last	O
slope	O
weight	O
is	O
the	O
difference	O
in	O
body	O
weight	O
at	O
the	O
last	O
two	O
visits	O
again	O
positive	O
is	O
good	O
likewise	O
for	O
alsfrs	O
score	O
slope	O
which	O
measures	O
the	O
local	O
slope	O
of	O
the	O
frs	O
score	O
after	O
the	O
first	O
two	O
visits	O
shrinkage	B
the	O
shrinkage	B
parameter	O
controls	O
the	O
rate	B
at	O
which	O
boosting	B
fits	O
and	O
hence	O
overfits	O
the	O
data	B
figure	O
demonstrates	O
the	O
effect	O
of	O
shrinkage	B
on	O
the	O
als	B
data	B
the	O
under-shrunk	O
ensemble	B
quickly	O
overfits	O
the	O
data	B
leading	O
to	O
poor	O
validation	O
error	O
the	O
blue	O
ensemble	B
uses	O
a	O
shrinkage	B
parameter	O
times	O
smaller	O
and	O
reaches	O
a	O
lower	O
validation	O
error	O
the	O
downside	O
of	O
a	O
very	O
small	O
shrinkage	B
parameter	O
is	O
that	O
it	O
can	O
take	O
many	O
trees	B
to	O
adequately	O
fit	O
the	O
data	B
on	O
the	O
other	O
hand	O
the	O
shrunken	O
fits	O
are	O
smoother	O
take	O
much	O
longer	O
to	O
overfit	O
and	O
hence	O
are	O
less	O
sensitive	O
to	O
the	O
stopping	O
point	O
b	O
gradient	B
boosting	B
we	O
now	O
turn	O
our	O
attention	O
to	O
boosting	B
models	B
using	O
other	O
than	O
square-error	O
loss	O
we	O
focus	O
on	O
the	O
family	O
of	O
generalized	O
models	B
generated	O
by	O
the	O
exponential	B
family	I
of	O
response	B
distributions	O
chapter	O
the	O
most	O
popular	O
and	O
relevant	O
in	O
this	O
class	O
is	O
logistic	B
regression	B
where	O
we	O
are	O
interested	O
in	O
modeling	O
d	O
pr	O
y	O
d	O
d	O
x	O
for	O
a	O
bernoulli	B
response	B
variable	O
function	B
function	B
function	B
gradient	B
boosting	B
figure	O
boosted	O
d	O
d	O
models	B
with	O
different	O
shrinkage	B
parameters	O
fit	O
to	O
a	O
subset	O
of	O
the	O
als	B
data	B
the	O
solid	O
curves	O
are	O
validation	O
errors	B
the	O
dashed	O
curves	O
training	O
errors	B
with	O
red	O
for	O
d	O
and	O
blue	O
for	O
d	O
with	O
d	O
the	O
training	O
error	O
drops	O
rapidly	O
with	O
the	O
number	O
of	O
trees	B
but	O
the	O
validation	O
error	O
starts	O
to	O
increase	O
rapidly	O
after	O
an	O
initial	O
decrease	O
with	O
d	O
times	O
smaller	O
the	O
training	O
error	O
drops	O
more	O
slowly	O
the	O
validation	O
error	O
also	O
drops	O
more	O
slowly	O
but	O
reaches	O
a	O
lower	O
minimum	O
horizontal	O
dotted	O
line	O
than	O
the	O
d	O
case	O
in	O
this	O
case	O
the	O
slower	O
learning	O
has	O
paid	O
off	O
the	O
idea	O
is	O
to	O
fit	O
a	O
model	O
of	O
the	O
form	B
d	O
gb	O
d	O
bx	O
gb	O
xi	O
where	O
is	O
the	O
natural	B
parameter	I
in	O
the	O
conditional	B
distribution	B
of	O
y	O
jx	O
d	O
x	O
and	O
the	O
gb	O
xi	O
are	O
simple	O
functions	O
such	O
as	O
shallow	O
trees	B
here	O
we	O
have	O
indexed	O
each	O
function	B
by	O
a	O
parameter	O
vector	B
for	O
trees	B
these	O
would	O
capture	O
the	O
identity	O
of	O
the	O
split	O
variables	O
their	O
split	O
values	O
and	O
the	O
constants	O
in	O
the	O
terminal	O
nodes	B
in	O
the	O
case	O
of	O
the	O
bernoulli	B
response	B
we	O
have	O
d	O
d	O
x	O
pr	O
y	O
d	O
d	O
x	O
d	O
log	O
of	O
treesmean	O
squared	O
random	O
forests	O
and	O
boosting	B
the	O
logit	B
link	B
function	B
that	O
relates	O
the	O
mean	O
to	O
the	O
natural	B
parameter	I
in	O
general	O
if	O
d	O
e	O
y	O
jx	O
d	O
x	O
is	O
the	O
conditional	B
mean	O
we	O
have	O
d	O
where	O
is	O
the	O
monotone	O
link	B
function	B
algorithm	B
outlines	O
a	O
general	O
strategy	O
for	O
building	O
a	O
model	O
by	O
forward	O
stagewise	O
fitting	O
l	O
is	O
the	O
loss	B
function	B
such	O
as	O
the	O
negative	O
loglikelihood	O
for	O
bernoulli	B
responses	O
or	O
squared-error	O
for	O
gaussian	B
responses	O
although	O
we	O
are	O
thinking	O
of	O
trees	B
for	O
the	O
simple	O
functions	O
g	O
xi	O
the	O
ideas	O
generalize	O
this	O
algorithm	B
is	O
easier	O
to	O
state	O
than	O
to	O
implement	O
for	O
algorithm	B
generalized	O
boosting	B
by	O
forward-stagewise	B
fitting	O
define	O
the	O
class	O
of	O
functions	O
g	O
xi	O
start	O
with	O
d	O
and	O
set	B
b	O
and	O
the	O
shrinkage	B
parameter	O
for	O
b	O
d	O
b	O
repeat	O
the	O
following	O
steps	O
solve	O
nx	O
yi	O
c	O
g	O
xii	O
l	O
updatebgb	O
x	O
c	O
ogb	O
x	O
with	O
ogb	O
x	O
d	O
g	O
xi	O
return	O
the	O
sequencebgb	O
x	O
b	O
d	O
b	O
d	O
arg	O
min	O
squared-error	O
loss	O
at	O
each	O
step	O
we	O
need	O
to	O
solve	O
nx	O
minimize	O
g	O
xii	O
with	O
ri	O
d	O
yi	O
i	O
d	O
n	O
if	O
represents	O
a	O
depth-d	O
tree	O
is	O
still	O
difficult	O
to	O
solve	O
but	O
here	O
we	O
can	O
resort	O
to	O
the	O
usual	O
greedy	O
heuristic	O
and	O
grow	O
a	O
depth-d	O
tree	O
to	O
the	O
residuals	O
by	O
the	O
usual	O
top-down	O
splitting	O
as	O
in	O
step	O
of	O
algorithm	B
hence	O
in	O
this	O
case	O
we	O
have	O
exactly	O
the	O
squared-error	O
boosting	B
algorithm	B
for	O
more	O
general	O
loss	O
functions	O
we	O
rely	O
on	O
one	O
more	O
heuristic	O
for	O
solving	O
step	O
inspired	O
by	O
gradient	B
descent	I
algorithm	B
gives	O
the	O
details	O
the	O
idea	O
is	O
to	O
perform	O
functional	B
gradient	B
descent	I
on	O
the	O
loss	B
function	B
in	O
the	O
n-dimensional	O
space	O
of	O
the	O
fitted	O
vector	B
however	O
we	O
want	O
to	O
be	O
able	O
to	O
evaluate	O
our	O
new	O
function	B
everywhere	O
not	O
just	O
at	O
the	O
n	O
original	O
values	O
xi	O
hence	O
once	O
the	O
gradient	O
vector	B
has	O
been	O
computed	O
it	O
is	O
approximated	O
by	O
a	O
depth-d	O
tree	O
can	O
be	O
evaluated	O
everywhere	O
taking	O
a	O
step	O
of	O
length	O
down	O
adaboost	O
the	O
original	O
boosting	B
algorithm	B
the	O
gradient	O
amounts	O
to	O
adding	O
times	O
the	O
tree	O
to	O
the	O
current	O
function	B
gradient	B
boosting	B
is	O
quite	O
general	O
and	O
can	O
be	O
used	O
with	O
any	O
differentiable	O
algorithm	B
gradient	B
boosting	B
start	O
with	O
d	O
and	O
set	B
b	O
and	O
the	O
shrinkage	B
parameter	O
for	O
b	O
d	O
b	O
repeat	O
the	O
following	O
steps	O
compute	O
the	O
pointwise	O
negative	O
gradient	O
of	O
the	O
loss	B
function	B
at	O
the	O
current	O
fit	O
ri	O
d	O
nx	O
minimize	O
g	O
xii	O
i	O
d	O
n	O
approximate	O
the	O
negative	O
gradient	O
by	O
a	O
depth-d	O
tree	O
by	O
solving	O
update	O
ogb	O
x	O
d	O
c	O
ogb	O
x	O
with	O
ogb	O
x	O
d	O
g	O
xi	O
return	O
the	O
sequence	O
ogb	O
x	O
b	O
d	O
b	O
loss	B
function	B
the	O
r	B
package	O
gbm	B
implements	O
algorithm	B
for	O
a	O
variety	O
of	O
loss	O
functions	O
including	O
squared-error	O
binomial	B
laplace	O
loss	O
multinomial	O
and	O
others	O
included	O
as	O
well	O
is	O
the	O
partial	B
likelihood	B
for	O
the	O
cox	O
proportional	B
hazards	I
model	I
figure	O
compares	O
the	O
misclassification	O
error	O
of	O
boosting	B
on	O
the	O
spam	B
data	B
with	O
that	O
of	O
random	O
forests	O
and	O
bagging	B
since	O
boosting	B
has	O
more	O
tuning	O
parameters	O
a	O
careful	O
comparison	O
must	O
take	O
these	O
into	O
account	O
using	O
the	O
mcnemar	B
test	I
we	O
would	O
conclude	O
that	O
boosting	B
and	O
random	B
forest	I
are	O
not	O
significantly	O
different	O
from	O
each	O
other	O
but	O
both	O
outperform	O
bagging	B
adaboost	O
the	O
original	O
boosting	B
algorithm	B
the	O
original	O
proposal	O
for	O
boosting	B
looked	O
quite	O
different	O
from	O
what	O
we	O
have	O
presented	O
so	O
far	O
adaboost	O
was	O
developed	O
for	O
the	O
two-class	O
classification	O
problem	O
where	O
the	O
response	B
is	O
coded	O
as	O
the	O
idea	O
was	O
to	O
fit	O
a	O
sequence	O
of	O
classifiers	O
to	O
modified	O
versions	O
of	O
the	O
training	O
data	B
where	O
the	O
modifications	O
give	O
more	O
weight	O
to	O
misclassified	O
points	O
the	O
final	O
classification	O
is	O
by	O
weighted	B
majority	I
vote	I
the	O
details	O
are	O
rather	O
specific	O
and	O
are	O
given	O
in	O
algorithm	B
here	O
we	O
distinguish	O
a	O
classifier	O
c	O
x	O
which	O
returns	O
a	O
class	O
label	O
rather	O
than	O
a	O
probability	O
algorithm	B
gives	O
random	O
forests	O
and	O
boosting	B
figure	O
test	O
misclassification	O
for	O
gradient	B
boosting	B
on	O
the	O
spam	B
data	B
compared	O
with	O
a	O
random	B
forest	I
and	O
bagging	B
although	O
boosting	B
appears	O
to	O
be	O
better	O
it	O
requires	O
crossvaldiation	O
or	O
some	O
other	O
means	O
to	O
estimate	B
its	O
tuning	O
parameters	O
while	O
the	O
random	B
forest	I
is	O
essentially	O
automatic	O
the	O
algorithm	B
although	O
the	O
classifier	O
in	O
step	O
can	O
be	O
arbitrary	O
it	O
was	O
intended	O
for	O
weak	O
learners	O
such	O
as	O
shallow	O
trees	B
steps	O
look	O
mysterious	O
its	O
easy	O
to	O
check	O
that	O
with	O
the	O
reweighted	O
points	O
the	O
classifier	O
ocb	O
just	O
learned	O
would	O
have	O
weighted	O
error	O
that	O
of	O
a	O
coin	O
flip	O
ues	O
the	O
ensemblebgb	O
x	O
takes	O
values	O
in	O
r	B
we	O
also	O
notice	O
that	O
although	O
the	O
individual	O
classifiers	O
ocb	O
x	O
produce	O
valponential	O
loss	B
function	B
the	O
functions	O
bgb	O
x	O
output	O
in	O
step	O
of	O
algo	O
it	O
turns	O
out	O
that	O
the	O
adaboost	B
algorithm	B
fits	O
a	O
logistic	B
regression	B
model	I
via	O
a	O
version	O
of	O
the	O
general	O
boosting	B
algorithm	B
using	O
an	O
ex	O
rithm	O
are	O
estimates	O
of	O
the	O
logit	B
function	B
to	O
show	O
this	O
we	O
first	O
motivate	O
the	O
exponential	O
loss	O
a	O
somewhat	O
unusual	O
choice	O
and	O
show	O
how	O
it	O
is	O
linked	O
to	O
logistic	B
regression	B
for	O
a	O
response	B
y	O
and	O
function	B
f	O
the	O
exponential	O
loss	O
is	O
defined	O
as	O
le	O
f	O
d	O
exp	O
a	O
simple	O
calculation	O
shows	O
that	O
the	O
solution	O
to	O
the	O
boosting	B
on	O
the	O
spam	B
datanumber	O
of	O
treestest	O
forestboosting	O
adaboost	O
the	O
original	O
boosting	B
algorithm	B
wi	O
algorithm	B
adaboost	O
initialize	O
the	O
observation	O
weights	B
wi	O
d	O
i	O
d	O
n	O
for	O
b	O
d	O
b	O
repeat	O
the	O
following	O
steps	O
fit	O
a	O
classifier	O
ocb	O
x	O
to	O
the	O
training	O
data	B
using	O
observation	O
weights	B
compute	O
the	O
weighted	O
misclassification	O
error	O
for	O
ocb	O
pn	O
wi	O
i	O
yi	O
ocb	O
xi	O
wi	O
compute	O
b	O
d	O
log	O
errberrb	O
exp	O
b	O
i	O
yi	O
cb	O
xi	O
update	O
the	O
weights	B
wi	O
wi	O
output	O
the	O
sequence	O
of	O
functions	O
bgb	O
x	O
d	O
pb	O
i	O
sponding	O
classifiersbcb	O
x	O
d	O
sign	O
b	O
d	O
b	O
i	O
d	O
m	O
oc	O
x	O
and	O
corre	O
errb	O
dpn	O
hbgb	O
x	O
n	O
tional	O
population	O
minimization	O
problem	O
j	O
x	O
e	O
e	O
minimize	O
f	O
d	O
pr	O
y	O
d	O
f	O
d	O
log	O
is	O
given	O
by	O
inverting	O
we	O
get	O
pr	O
y	O
d	O
d	O
ef	O
e	O
and	O
pr	O
y	O
d	O
d	O
e	O
e	O
c	O
ef	O
c	O
ef	O
a	O
perfectly	O
reasonable	O
symmetric	O
model	O
for	O
a	O
probability	O
the	O
quantity	B
yf	O
is	O
known	O
as	O
the	O
margin	B
also	O
chapter	O
if	O
the	O
margin	B
is	O
positive	O
the	O
classification	O
using	O
cf	O
d	O
sign	O
f	O
is	O
correct	O
for	O
y	O
else	O
it	O
is	O
incorrect	O
if	O
the	O
margin	B
is	O
negative	O
the	O
magnitude	O
of	O
yf	O
is	O
proportional	O
to	O
the	O
distance	O
of	O
x	O
from	O
the	O
classification	O
boundary	O
for	O
linear	B
models	B
approximately	O
otherwise	O
for	O
data	B
we	O
can	O
also	O
write	O
the	O
binomial	B
log-likelihood	B
in	O
terms	O
of	O
the	O
margin	B
random	O
forests	O
and	O
boosting	B
using	O
we	O
have	O
lb	O
f	O
d	O
d	O
log	O
pr	O
y	O
d	O
c	O
i	O
y	O
d	O
log	O
pr	O
y	O
d	O
c	O
e	O
d	O
log	O
c	O
e	O
j	O
also	O
has	O
population	O
minimizer	O
f	O
equal	O
to	O
half	O
the	O
logit	B
figure	O
compares	O
the	O
exponential	O
loss	B
function	B
with	O
this	O
binomial	B
loss	O
they	O
both	O
asymptote	O
to	O
zero	O
in	O
the	O
right	O
tail	O
the	O
area	O
of	O
correct	O
classification	O
in	O
the	O
left	O
tail	O
the	O
binomial	B
loss	O
asymptotes	O
to	O
a	O
linear	B
function	B
much	O
less	O
severe	O
than	O
the	O
exponential	O
loss	O
figure	O
exponential	O
loss	O
used	O
in	O
adaboost	O
versus	O
the	O
binomial	B
loss	O
used	O
in	O
the	O
usual	O
logistic	B
regression	B
both	O
estimate	B
the	O
logit	B
function	B
the	O
exponential	O
left	O
tail	O
which	O
punishes	O
misclassifications	O
is	O
much	O
more	O
severe	O
than	O
the	O
asymptotically	O
linear	B
tail	O
of	O
the	O
binomial	B
the	O
exponential	O
loss	O
simplifies	O
step	O
in	O
the	O
gradient	B
boosting	B
algo	O
the	O
half	O
comes	O
from	O
the	O
symmetric	O
representation	O
we	O
use	O
rithm	O
yi	O
c	O
g	O
xii	O
le	O
nx	O
connections	O
and	O
extensions	O
d	O
nx	O
d	O
nx	O
d	O
nx	O
exp	O
c	O
g	O
xii	O
wi	O
exp	O
g	O
xii	O
wi	O
le	O
g	O
xii	O
with	O
wi	O
d	O
exp	O
this	O
is	O
just	O
a	O
weighted	B
exponential	I
loss	I
with	O
the	O
past	O
history	O
encapsulated	O
in	O
the	O
observation	O
weight	O
wi	O
step	O
in	O
algorithm	B
we	O
give	O
some	O
more	O
details	O
in	O
the	O
chapter	O
endnotes	O
on	O
how	O
this	O
reduces	O
to	O
the	O
adaboost	B
algorithm	B
the	O
adaboost	B
algorithm	B
achieves	O
an	O
error	O
rate	B
on	O
the	O
spam	B
data	B
com	O
parable	O
to	O
binomial	B
gradient	B
boosting	B
connections	O
and	O
extensions	O
boosting	B
is	O
a	O
general	O
nonparametric	B
function-fitting	O
algorithm	B
and	O
shares	O
attributes	O
with	O
a	O
variety	O
of	O
existing	O
methods	O
here	O
we	O
relate	O
boosting	B
to	O
two	O
different	O
approaches	O
generalized	O
additive	O
models	B
and	O
the	O
lasso	B
of	O
chapter	O
generalized	O
additive	O
models	B
boosting	B
fits	O
additive	O
low-order	O
interaction	O
models	B
by	O
a	O
forward	O
stagewise	O
strategy	O
generalized	O
additive	O
models	B
are	O
a	O
predecessor	O
a	O
semi-parametric	O
approach	O
toward	O
nonlinear	B
function	B
fitting	O
a	O
gam	O
has	O
the	O
form	B
fj	O
where	O
again	O
d	O
is	O
the	O
natural	B
parameter	I
in	O
an	O
exponential	B
family	I
the	O
attraction	O
of	O
a	O
gam	O
is	O
that	O
the	O
components	O
are	O
interpretable	O
and	O
can	O
be	O
visualized	O
and	O
they	O
can	O
move	O
us	O
a	O
big	O
step	O
up	O
from	O
a	O
linear	B
model	I
there	O
are	O
many	O
ways	O
to	O
specify	O
and	O
fit	O
additive	O
models	B
for	O
the	O
fj	O
we	O
could	O
use	O
parametric	B
functions	O
polynomials	O
fixed-knot	B
regression	B
splines	I
or	O
even	O
linear	B
functions	O
for	O
some	O
terms	O
less	O
parametric	B
options	O
d	O
px	O
random	O
forests	O
and	O
boosting	B
are	O
smoothing	B
splines	O
and	O
local	B
regression	B
section	O
in	O
the	O
case	O
of	O
squared-error	O
loss	O
gaussian	B
case	O
there	O
is	O
a	O
natural	O
set	B
of	O
backfitting	O
equations	O
for	O
fitting	O
a	O
gam	O
o	O
fj	O
of	O
j	O
d	O
p	O
sj	O
o	O
f	O
xn	O
o	O
j	O
here	O
of	O
d	O
is	O
the	O
n-vector	O
of	O
fitted	O
values	O
for	O
the	O
current	O
estimate	B
of	O
function	B
f	O
hence	O
the	O
term	O
in	O
parentheses	O
is	O
a	O
partial	B
residual	I
removing	O
all	O
the	O
current	O
function	B
fits	O
from	O
y	O
except	O
the	O
one	O
about	O
to	O
be	O
updated	O
sj	O
is	O
a	O
smoothing	B
operator	I
derived	O
from	O
variable	O
xj	O
that	O
gets	O
applied	O
to	O
this	O
residual	O
and	O
delivers	O
the	O
next	O
estimate	B
for	O
function	B
f	O
backfitting	O
starts	O
with	O
all	O
the	O
functions	O
zero	O
and	O
then	O
cycles	O
through	O
these	O
equations	O
for	O
j	O
d	O
p	O
in	O
a	O
block-coordinate	O
fashion	O
until	O
all	O
the	O
functions	O
stabilize	O
the	O
first	O
pass	O
through	O
all	O
the	O
variables	O
is	O
similar	O
to	O
the	O
regression	B
boosting	B
algorithm	B
where	O
each	O
new	O
function	B
takes	O
the	O
residuals	O
from	O
the	O
past	O
fits	O
and	O
models	B
them	O
using	O
a	O
tree	O
sj	O
the	O
difference	O
is	O
that	O
boosting	B
never	O
goes	O
back	O
and	O
fixes	O
up	O
past	O
functions	O
but	O
fits	O
in	O
a	O
forwardstagewise	O
fashion	O
leaving	O
all	O
past	O
functions	O
alone	O
of	O
course	O
with	O
its	O
adaptive	B
fitting	O
mechanism	O
boosting	B
can	O
select	O
the	O
same	O
variables	O
as	O
used	O
before	O
and	O
thereby	O
update	O
that	O
component	O
of	O
the	O
fit	O
boosting	B
with	O
stumps	O
trees	B
see	O
the	O
discussion	O
on	O
tree	O
depth	B
on	O
in	O
section	O
can	O
hence	O
be	O
seen	O
as	O
an	O
adaptive	B
way	O
for	O
fitting	O
an	O
additive	B
model	I
that	O
simultaneously	O
performs	O
variable	O
selection	O
and	O
allows	O
for	O
different	O
amounts	O
of	O
smoothing	B
for	O
different	O
variables	O
boosting	B
and	O
the	O
lasso	B
in	O
section	O
we	O
drew	O
attention	O
to	O
the	O
close	O
connection	O
between	O
the	O
forward-stagewise	B
fitting	O
of	O
boosting	B
shrinkage	B
and	O
the	O
lasso	B
via	O
infinitesimal	O
forward-stagewise	B
regression	B
here	O
we	O
take	O
this	O
a	O
step	O
further	O
by	O
using	O
the	O
lasso	B
as	O
a	O
post-processor	O
for	O
boosting	B
random	O
forests	O
boosting	B
with	O
shrinkage	B
does	O
a	O
good	O
job	O
in	O
building	O
a	O
prediction	O
model	O
but	O
at	O
the	O
end	O
of	O
the	O
day	O
can	O
involve	O
a	O
lot	O
of	O
trees	B
because	O
of	O
the	O
shrinkage	B
many	O
of	O
these	O
trees	B
could	O
be	O
similar	O
to	O
each	O
other	O
the	O
idea	O
here	O
is	O
to	O
use	O
the	O
lasso	B
to	O
select	O
a	O
subset	O
of	O
these	O
trees	B
reweight	O
them	O
and	O
hence	O
produce	O
a	O
prediction	O
model	O
with	O
far	O
fewer	O
trees	B
and	O
one	O
hopes	O
comparable	O
accuracy	B
suppose	O
boosting	B
has	O
produced	O
a	O
sequence	O
of	O
fitted	O
trees	B
ogb	O
x	O
b	O
d	O
b	O
we	O
then	O
solve	O
the	O
lasso	B
problem	O
notes	O
and	O
details	O
figure	O
post-processing	O
of	O
the	O
trees	B
produced	O
by	O
boosting	B
on	O
the	O
als	B
data	B
shown	O
is	O
the	O
test	O
prediction	O
error	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	B
selected	O
by	O
the	O
lasso	B
we	O
see	O
that	O
the	O
lasso	B
can	O
do	O
as	O
good	O
a	O
job	O
with	O
one-third	O
the	O
number	O
of	O
trees	B
although	O
selecting	O
the	O
correct	O
number	O
is	O
critical	O
nx	O
bx	O
bx	O
minimize	O
f	O
bgb	O
l	O
yi	O
ogb	O
xi	O
b	O
c	O
j	O
bj	O
for	O
different	O
values	O
of	O
this	O
model	O
selects	O
some	O
of	O
the	O
trees	B
and	O
assigns	O
differential	O
weights	B
to	O
them	O
a	O
reasonable	O
variant	O
is	O
to	O
insist	O
that	O
the	O
weights	B
are	O
nonnegative	O
figure	O
illustrates	O
this	O
approach	O
on	O
the	O
als	B
data	B
here	O
we	O
could	O
use	O
one-third	O
of	O
the	O
trees	B
often	O
the	O
savings	O
are	O
much	O
more	O
dramatic	O
notes	O
and	O
details	O
random	O
forests	O
and	O
boosting	B
live	O
at	O
the	O
cutting	O
edge	O
of	O
modern	O
prediction	O
methodology	O
they	O
fit	O
models	B
of	O
breathtaking	O
complexity	O
compared	O
with	O
classical	O
linear	B
regression	B
or	O
even	O
with	O
standard	O
glm	O
modeling	O
as	O
practiced	O
in	O
the	O
late	O
twentieth	O
century	O
they	O
are	O
routinely	O
used	O
as	O
prediction	O
engines	O
in	O
a	O
wide	O
variety	O
of	O
industrial	O
and	O
scientific	O
applications	O
for	O
the	O
more	O
cautious	O
they	O
provide	O
a	O
terrific	O
benchmark	O
for	O
how	O
well	O
a	O
traditional	O
parametrized	O
model	O
is	O
performing	O
if	O
the	O
random	O
forests	O
of	O
treesmean	O
squared	O
boostlasso	O
post	O
fit	O
random	O
forests	O
and	O
boosting	B
does	O
much	O
better	O
you	O
probably	O
have	O
some	O
work	O
to	O
do	O
by	O
including	O
some	O
important	O
interactions	O
and	O
the	O
like	O
the	O
regression	B
and	O
classification	O
trees	B
discussed	O
in	O
chapter	O
et	O
al	O
took	O
traditional	O
models	B
to	O
a	O
new	O
level	O
with	O
their	O
ability	O
to	O
adapt	O
to	O
the	O
data	B
select	O
variables	O
and	O
so	O
on	O
but	O
their	O
prediction	O
performance	O
is	O
somewhat	O
lacking	O
and	O
so	O
they	O
stood	O
the	O
risk	O
of	O
falling	O
by	O
the	O
wayside	O
with	O
their	O
new	O
use	O
as	O
building	O
blocks	O
in	O
random	O
forests	O
and	O
boosting	B
they	O
have	O
reasserted	O
themselves	O
as	O
critical	O
elements	O
in	O
the	O
modern	O
toolbox	O
random	O
forests	O
and	O
bagging	B
were	O
introduced	O
by	O
breiman	O
and	O
boosting	B
by	O
schapire	O
and	O
freund	O
and	O
schapire	O
there	O
has	O
been	O
much	O
discussion	O
on	O
why	O
boosting	B
works	O
friedman	O
et	O
al	O
schapire	O
and	O
freund	O
the	O
statistical	O
interpretation	O
given	O
here	O
can	O
also	O
be	O
found	O
in	O
hastie	O
et	O
al	O
and	O
led	O
to	O
the	O
gradient	B
boosting	B
algorithm	B
adaboost	O
was	O
first	O
described	O
in	O
freund	O
and	O
schapire	O
hastie	O
et	O
al	O
chapter	O
is	O
devoted	O
to	O
random	O
forests	O
for	O
the	O
examples	O
in	O
this	O
chapter	O
we	O
used	O
the	O
randomforest	B
package	O
in	O
r	B
and	O
wiener	O
and	O
for	O
boosting	B
the	O
gbm	B
package	O
the	O
lasso	B
post-processing	O
idea	O
is	O
due	O
to	O
friedman	O
and	O
popescu	O
which	O
we	O
implemented	O
using	O
glmnet	B
et	O
al	O
generalized	O
additive	O
models	B
are	O
described	O
in	O
hastie	O
and	O
tibshirani	O
we	O
now	O
give	O
some	O
particular	O
technical	O
details	O
on	O
topics	O
covered	O
in	O
the	O
chapter	O
averaging	B
trees	B
a	O
maximal-depth	O
tree	O
splits	O
every	O
node	O
until	O
it	O
is	O
pure	O
meaning	O
all	O
the	O
responses	O
are	O
the	O
same	O
for	O
very	O
large	O
n	O
this	O
might	O
be	O
unreasonable	O
in	O
practice	O
one	O
can	O
put	O
a	O
lower	O
bound	B
on	O
the	O
minimum	O
count	O
in	O
a	O
terminal	B
node	I
we	O
are	O
deliberately	O
vague	O
about	O
the	O
response	B
type	O
in	O
algorithm	B
if	O
it	O
is	O
quantitative	O
we	O
would	O
fit	O
a	O
regression	B
tree	I
if	O
it	O
is	O
binary	O
or	O
multilevel	O
qualitative	O
we	O
would	O
fit	O
a	O
classification	O
tree	O
in	O
this	O
case	O
at	O
the	O
averaging	B
stage	O
there	O
are	O
at	O
least	O
two	O
strategies	O
the	O
original	O
random-forest	O
paper	O
proposed	O
that	O
each	O
tree	O
should	O
make	O
a	O
classification	O
and	O
then	O
the	O
ensemble	B
uses	O
a	O
plurality	O
vote	O
an	O
alternative	O
reasonable	O
strategy	O
is	O
to	O
average	O
the	O
class	O
probabilities	B
produced	O
by	O
the	O
trees	B
these	O
procedures	O
are	O
identical	O
if	O
the	O
trees	B
are	O
grown	O
to	O
maximal	O
depth	B
jackknife	B
variance	B
estimate	B
the	O
jackknife	B
estimate	B
of	O
variance	B
for	O
a	O
random	B
forest	I
and	O
the	O
bias-corrected	B
version	O
is	O
described	O
in	O
wager	O
et	O
al	O
the	O
jackknife	B
formula	B
is	O
applied	O
to	O
the	O
b	O
d	O
ver	O
notes	O
and	O
details	O
sion	O
of	O
the	O
random	B
forest	I
but	O
of	O
course	O
is	O
estimated	O
by	O
plugging	O
in	O
finite	O
b	O
versions	O
of	O
the	O
quantities	O
involved	O
replacing	O
or	O
rf	O
by	O
its	O
expectation	O
is	O
not	O
the	O
problem	O
its	O
that	O
each	O
of	O
the	O
or	O
rf	O
vary	O
about	O
their	O
bootstrap	B
expectations	O
compounded	O
by	O
the	O
square	O
in	O
expression	O
calculating	O
the	O
bias	B
requires	O
some	O
technical	O
derivations	O
which	O
can	O
be	O
found	O
in	O
that	O
reference	O
they	O
also	O
describe	O
the	O
infinitesimal	O
jackknife	B
estimate	B
of	O
variance	B
given	O
by	O
withdcovi	O
ddcov	O
w	O
d	O
nx	O
i	O
bx	O
as	O
discussed	O
in	O
chapter	O
it	O
too	O
has	O
a	O
bias-corrected	B
version	O
given	O
by	O
b	O
b	O
i	O
d	O
bvu	O
n	O
b	O
similar	O
to	O
the	O
als	B
data	B
these	O
data	B
were	O
kindly	O
provided	O
by	O
lester	O
mackey	O
and	O
lilly	O
fang	O
who	O
won	O
the	O
dream	O
challenge	O
prediction	O
prize	O
in	O
et	O
al	O
it	O
includes	O
some	O
additional	O
variables	O
created	O
by	O
them	O
their	O
winning	O
entry	O
used	O
bayesian	B
trees	B
not	O
too	O
different	O
from	O
random	O
forests	O
gradient-boosting	O
details	O
in	O
friedman	O
s	O
gradient-boosting	O
algorithm	B
et	O
al	O
chapter	O
for	O
example	O
a	O
further	O
refinement	O
is	O
implemented	O
the	O
tree	O
in	O
step	O
of	O
algorithm	B
is	O
used	O
to	O
define	O
the	O
structure	B
variables	O
and	O
splits	O
but	O
the	O
values	O
in	O
the	O
terminal	O
nodes	B
are	O
left	O
to	O
be	O
updated	O
we	O
can	O
think	O
of	O
partitioning	O
the	O
parameters	O
d	O
and	O
then	O
represent	O
the	O
tree	O
as	O
g	O
xi	O
d	O
t	B
here	O
t	B
is	O
a	O
vector	B
of	O
d	O
c	O
binary	O
basis	O
functions	O
that	O
indicate	O
the	O
terminal	B
node	I
reached	O
by	O
input	O
x	O
and	O
are	O
the	O
d	O
c	O
values	O
of	O
the	O
terminal	O
nodes	B
of	O
the	O
tree	O
we	O
learn	O
by	O
approximating	O
the	O
gradient	O
in	O
step	O
by	O
a	O
tree	O
and	O
then	O
the	O
terminal-node	O
parameters	O
by	O
solving	O
the	O
optimization	O
problem	O
yi	O
c	O
t	B
nx	O
minimize	O
l	O
solving	O
amounts	O
to	O
fitting	O
a	O
simple	O
glm	O
with	O
an	O
offset	B
random	O
forests	O
and	O
boosting	B
adaboost	O
and	O
gradient	B
boosting	B
hastie	O
et	O
al	O
chapter	O
derive	O
adaboost	O
as	O
an	O
instance	O
of	O
algorithm	B
one	O
detail	O
is	O
that	O
the	O
trees	B
g	O
xi	O
are	O
replaced	O
by	O
a	O
simplified	O
scaled	O
classifier	O
c	O
xi	O
hence	O
from	O
in	O
step	O
of	O
algorithm	B
we	O
need	O
to	O
solve	O
wi	O
exp	O
c	O
xii	O
nx	O
minimize	O
the	O
derivation	O
goes	O
on	O
to	O
show	O
that	O
minimizing	O
for	O
any	O
value	O
of	O
can	O
be	O
achieved	O
by	O
fitting	O
to	O
minimize	O
the	O
weighted	O
misclassification	O
a	O
classification	O
tree	O
c	O
xi	O
nx	O
error	O
wi	O
i	O
yi	O
c	O
xi	O
i	O
given	O
c	O
xi	O
is	O
estimated	O
as	O
in	O
step	O
of	O
algorithm	B
is	O
non-negative	O
the	O
weight-update	O
scheme	O
in	O
step	O
of	O
algorithm	B
corresponds	O
exactly	O
to	O
the	O
weights	B
as	O
computed	O
in	O
neural	O
networks	O
and	O
deep	B
learning	I
something	O
happened	O
in	O
the	O
mid	O
that	O
shook	O
up	O
the	O
applied	O
statistics	B
community	O
neural	O
networks	O
were	O
introduced	O
and	O
they	O
marked	O
a	O
shift	O
of	O
predictive	O
modeling	O
towards	O
computer	O
science	O
and	O
machine	B
learning	I
a	O
neural	B
network	I
is	O
a	O
highly	O
parametrized	O
model	O
inspired	O
by	O
the	O
architecture	O
of	O
the	O
human	O
brain	O
that	O
was	O
widely	O
promoted	O
as	O
a	O
universal	B
approximator	I
a	O
machine	O
that	O
with	O
enough	O
data	B
could	O
learn	O
any	O
smooth	O
predictive	O
relationship	O
figure	O
neural	B
network	I
diagram	O
with	O
a	O
single	O
hidden	B
layer	B
the	O
hidden	B
layer	B
derives	O
transformations	O
of	O
the	O
inputs	O
nonlinear	B
transformations	I
of	O
linear	B
combinations	O
which	O
are	O
then	O
used	O
to	O
model	O
the	O
output	O
figure	O
shows	O
a	O
simple	O
example	O
of	O
a	O
feed-forward	B
neural	B
network	I
diagram	O
there	O
are	O
four	O
predictors	B
or	O
inputs	O
xj	O
five	O
hidden	O
units	O
a	O
d	O
a	O
the	O
language	O
associated	O
with	O
nns	O
is	O
colorful	O
memory	O
units	O
or	O
neurons	B
automatically	O
learn	O
new	O
features	O
from	O
the	O
data	B
through	O
a	O
process	O
called	O
xj	O
and	O
a	O
single	O
output	O
unit	O
o	O
d	O
neural	O
networks	O
supervised	B
learning	I
each	O
neuron	O
al	O
is	O
connected	O
to	O
the	O
input	B
layer	B
via	O
a	O
gp	O
vector	B
of	O
parameters	O
or	O
weights	B
refers	O
to	O
the	O
first	O
layer	B
and	O
refers	O
to	O
the	O
j	O
th	O
variable	O
and	O
unit	O
the	O
intercept	O
terms	O
are	O
called	O
a	O
bias	B
and	O
the	O
function	B
g	O
is	O
a	O
nonlinearity	B
such	O
as	O
the	O
sigmoid	B
function	B
g	O
t	B
d	O
c	O
e	O
the	O
idea	O
was	O
that	O
each	O
neuron	O
will	O
learn	O
a	O
simple	O
binary	O
onoff	O
function	B
the	O
sigmoid	B
function	B
is	O
a	O
smooth	O
and	O
differentiable	O
compromise	O
the	O
final	O
or	O
output	B
layer	B
also	O
has	O
weights	B
and	O
an	O
output	O
function	B
h	O
for	O
quantitative	O
regression	B
h	O
is	O
typically	O
the	O
identity	O
function	B
and	O
for	O
a	O
binary	O
response	B
it	O
is	O
once	O
again	O
the	O
sigmoid	O
note	O
that	O
without	O
the	O
nonlinearity	B
in	O
the	O
hidden	B
layer	B
the	O
neural	B
network	I
would	O
reduce	O
to	O
a	O
generalized	O
linear	B
model	I
typically	O
neural	O
networks	O
are	O
fit	O
by	O
maximum	B
likelihood	B
usually	O
with	O
a	O
variety	O
of	O
forms	O
of	O
regularization	B
the	O
knee-jerk	O
response	B
from	O
statisticians	O
was	O
what	O
s	O
the	O
big	O
deal	O
a	O
neural	B
network	I
is	O
just	O
a	O
nonlinear	B
model	O
not	O
too	O
different	O
from	O
many	O
other	O
generalizations	O
of	O
linear	B
models	B
while	O
this	O
may	O
be	O
true	O
neural	O
networks	O
brought	O
a	O
new	O
energy	O
to	O
the	O
field	O
they	O
could	O
be	O
scaled	O
up	O
and	O
generalized	O
in	O
a	O
variety	O
of	O
ways	O
many	O
hidden	O
units	O
in	O
a	O
layer	B
multiple	O
hidden	O
layers	O
weight	O
sharing	B
a	O
variety	O
of	O
colorful	O
forms	O
of	O
regularization	B
and	O
innovative	O
learning	O
algorithms	O
for	O
massive	O
data	B
sets	O
and	O
most	O
importantly	O
they	O
were	O
able	O
to	O
solve	O
problems	O
on	O
a	O
scale	B
far	O
exceeding	O
what	O
the	O
statistics	B
community	O
was	O
used	O
to	O
this	O
was	O
part	O
computing	O
scale	B
and	O
expertise	O
part	O
liberated	O
thinking	O
and	O
creativity	O
on	O
the	O
part	O
of	O
this	O
computer	O
science	O
community	O
new	O
journals	O
were	O
devoted	O
to	O
the	O
field	O
and	O
several	O
popular	O
annual	O
conferences	O
at	O
ski	O
resorts	O
attracted	O
their	O
denizens	O
and	O
drew	O
in	O
members	O
of	O
the	O
statistics	B
community	O
after	O
enjoying	O
considerable	O
popularity	O
for	O
a	O
number	O
of	O
years	O
neural	O
networks	O
were	O
somewhat	O
sidelined	O
by	O
new	O
inventions	O
in	O
the	O
mid	O
such	O
as	O
boosting	B
and	O
svms	O
neural	O
networks	O
were	O
pass	O
e	O
but	O
then	O
they	O
re-emerged	O
with	O
a	O
vengeance	O
after	O
the	O
reincarnation	O
now	O
being	O
called	O
deep	B
learning	I
this	O
renewed	O
enthusiasm	O
is	O
a	O
result	O
of	O
massive	O
improvements	O
in	O
computer	O
resources	O
some	O
innovations	O
and	O
the	O
ideal	O
niche	O
learning	O
tasks	O
such	O
as	O
image	O
and	O
video	O
classification	O
and	O
speech	O
and	O
text	O
processing	O
neural	O
networks	O
and	O
the	O
handwritten	O
digit	O
problem	O
neural	O
networks	O
and	O
the	O
handwritten	O
digit	O
problem	O
neural	O
networks	O
really	O
cut	O
their	O
baby	O
teeth	O
on	O
an	O
optical	B
character	I
recognition	I
task	O
automatic	O
reading	O
of	O
handwritten	B
digits	I
as	O
in	O
a	O
zipcode	O
figure	O
shows	O
some	O
examples	O
taken	O
from	O
the	O
mnist	O
corpus	O
the	O
idea	O
is	O
to	O
build	O
a	O
classifier	O
c	O
x	O
based	O
on	O
the	O
input	O
image	O
x	O
a	O
grid	O
of	O
image	O
intensities	O
in	O
fact	O
as	O
is	O
often	O
the	O
case	O
it	O
is	O
more	O
useful	O
to	O
learn	O
the	O
probability	O
function	B
pr	O
y	O
d	O
jjx	O
j	O
d	O
this	O
is	O
indeed	O
the	O
target	O
for	O
our	O
neural	B
network	I
figure	O
figure	O
examples	O
of	O
handwritten	B
digits	I
from	O
the	O
mnist	O
corpus	O
each	O
digit	O
is	O
represented	O
by	O
a	O
grayscale	O
image	O
derived	O
from	O
normalized	O
binary	O
images	O
of	O
different	O
shapes	O
and	O
sizes	O
the	O
value	O
stored	O
for	O
each	O
pixel	O
in	O
an	O
image	O
is	O
a	O
nonnegative	O
eight-bit	O
representation	O
of	O
the	O
amount	O
of	O
gray	O
present	O
at	O
that	O
location	O
the	O
pixels	O
for	O
each	O
image	O
are	O
the	O
predictors	B
and	O
the	O
class	O
labels	O
the	O
response	B
there	O
are	O
training	O
images	O
in	O
the	O
full	B
data	B
set	B
and	O
in	O
the	O
test	O
set	B
shows	O
a	O
neural	B
network	I
with	O
three	O
hidden	O
layers	O
a	O
successful	O
configuration	O
for	O
this	O
digit	O
classification	O
problem	O
in	O
this	O
case	O
the	O
output	B
layer	B
has	O
nodes	B
one	O
for	O
each	O
of	O
the	O
possible	O
class	O
labels	O
we	O
use	O
this	O
example	O
to	O
walk	O
the	O
reader	O
through	O
some	O
of	O
the	O
aspects	O
of	O
the	O
configuration	O
of	O
a	O
network	O
and	O
fitting	O
it	O
to	O
training	O
data	B
since	O
all	O
of	O
the	O
layers	O
are	O
functions	O
of	O
their	O
previous	O
layers	O
and	O
finally	O
functions	O
of	O
the	O
input	O
vector	B
x	O
the	O
network	O
represents	O
a	O
somewhat	O
complex	O
function	B
f	O
w	O
where	O
w	O
represents	O
the	O
entire	O
collection	O
of	O
weights	B
armed	O
with	O
a	O
suitable	O
loss	B
function	B
we	O
could	O
simply	O
barge	O
right	O
in	O
and	O
throw	O
it	O
at	O
our	O
favorite	O
optimizer	O
in	O
the	O
early	O
days	O
this	O
was	O
not	O
computationally	O
feasible	O
especially	O
when	O
special	O
neural	O
networks	O
figure	O
neural	B
network	I
diagram	O
with	O
three	O
hidden	O
layers	O
and	O
multiple	O
outputs	O
suitable	O
for	O
the	O
mnist	O
handwritten-digit	O
problem	O
the	O
input	B
layer	B
has	O
p	O
d	O
units	O
such	O
a	O
network	O
with	O
hidden	B
layer	B
sizes	O
and	O
particular	O
choices	O
of	O
tuning	O
parameters	O
achieves	O
the	O
state-of-the	O
art	O
error	O
rate	B
of	O
on	O
the	O
official	O
test	O
data	B
set	B
this	O
network	O
has	O
close	O
to	O
four	O
million	O
weights	B
and	O
hence	O
needs	O
to	O
be	O
heavily	O
regularized	O
structure	B
is	O
imposed	O
on	O
the	O
weight	O
vectors	O
today	O
there	O
are	O
fairly	O
automatic	O
systems	O
for	O
setting	O
up	O
and	O
fitting	O
neural	O
networks	O
and	O
this	O
view	O
is	O
not	O
too	O
far	O
from	O
reality	O
they	O
mostly	O
use	O
some	O
form	B
of	O
gradient	B
descent	I
and	O
rely	O
on	O
an	O
organization	O
of	O
parameters	O
that	O
leads	O
to	O
a	O
manageable	O
calculation	O
of	O
the	O
gradient	O
the	O
network	O
in	O
figure	O
is	O
complex	O
so	O
it	O
is	O
essential	O
to	O
establish	O
a	O
convenient	O
notation	O
for	O
referencing	O
the	O
different	O
sets	O
of	O
parameters	O
we	O
continue	O
with	O
the	O
notation	O
established	O
for	O
the	O
single-layer	O
network	O
but	O
with	O
some	O
additional	O
annotations	O
to	O
distinguish	O
aspects	O
of	O
different	O
layers	O
from	O
the	O
first	O
to	O
the	O
second	O
layer	B
we	O
have	O
xj	O
c	O
px	O
d	O
d	O
neural	O
networks	O
and	O
the	O
handwritten	O
digit	O
problem	O
we	O
have	O
separated	O
the	O
linear	B
transformations	O
of	O
the	O
xj	O
from	O
the	O
nonlinear	B
transformation	O
of	O
these	O
and	O
we	O
allow	O
for	O
layer-specific	O
nonlinear	B
transformations	I
g	O
k	O
more	O
generally	O
we	O
have	O
the	O
transition	O
from	O
layer	B
k	O
to	O
layer	B
k	O
c	O
d	O
d	O
g	O
k	O
z	O
k	O
z	O
k	O
a	O
k	O
j	O
in	O
fact	O
can	O
serve	O
for	O
the	O
input	B
layer	B
if	O
we	O
adopt	O
x	O
and	O
d	O
p	O
the	O
number	O
of	O
input	O
variables	O
the	O
notation	O
that	O
hence	O
each	O
of	O
the	O
arrows	O
in	O
figure	O
is	O
associated	O
with	O
a	O
weight	O
parameter	O
it	O
is	O
simpler	O
to	O
adopt	O
a	O
vector	B
notation	O
z	O
k	O
d	O
w	O
a	O
k	O
d	O
g	O
k	O
z	O
k	O
where	O
w	O
represents	O
the	O
matrix	B
of	O
weights	B
that	O
go	O
from	O
layer	B
to	O
layer	B
lk	O
a	O
k	O
is	O
the	O
entire	O
vector	B
of	O
activations	O
at	O
layer	B
lk	O
and	O
our	O
notation	O
assumes	O
that	O
g	O
k	O
operates	O
elementwise	O
on	O
its	O
vector	B
argument	B
we	O
have	O
also	O
absorbed	O
the	O
bias	B
parameters	O
into	O
the	O
matrix	B
w	O
which	O
assumes	O
that	O
we	O
have	O
augmented	O
each	O
of	O
the	O
activation	O
vectors	O
a	O
k	O
with	O
a	O
constant	O
element	O
sometimes	O
the	O
nonlinearities	O
g	O
k	O
at	O
the	O
inner	O
layers	O
are	O
the	O
same	O
function	B
such	O
as	O
the	O
function	B
defined	O
earlier	O
in	O
section	O
we	O
present	O
a	O
network	O
for	O
natural	O
color	O
image	O
classification	O
where	O
a	O
number	O
of	O
different	O
activation	O
functions	O
are	O
used	O
depending	O
on	O
the	O
response	B
the	O
final	O
transformation	O
g	O
k	O
is	O
usually	O
special	O
for	O
m	O
classification	O
such	O
as	O
here	O
with	O
m	O
d	O
one	O
typically	O
uses	O
the	O
softmax	B
function	B
g	O
k	O
z	O
k	O
m	O
i	O
z	O
k	O
d	O
mpm	O
ez	O
k	O
ez	O
k	O
which	O
computes	O
a	O
number	O
between	O
zero	O
and	O
one	O
and	O
all	O
m	O
of	O
them	O
sum	O
to	O
this	O
is	O
a	O
symmetric	O
version	O
of	O
the	O
inverse	O
link	B
function	B
used	O
for	O
multiclass	B
logistic	B
regression	B
neural	O
networks	O
fitting	O
a	O
neural	B
network	I
as	O
we	O
have	O
seen	O
a	O
neural	B
network	I
model	O
is	O
a	O
complex	O
hierarchical	O
function	B
f	O
w	O
of	O
the	O
the	O
feature	O
vector	B
x	O
and	O
the	O
collection	O
of	O
weights	B
w	O
for	O
typical	O
choices	O
for	O
the	O
g	O
k	O
this	O
function	B
will	O
be	O
differentiable	O
given	O
a	O
training	B
set	B
fxi	O
yign	O
and	O
a	O
loss	B
function	B
l	O
y	O
f	O
along	O
familiar	O
lines	O
we	O
might	O
seek	O
to	O
solve	O
l	O
yi	O
f	O
w	O
c	O
nx	O
n	O
minimize	O
w	O
where	O
j	O
w	O
is	O
a	O
nonnegative	O
regularization	B
term	O
on	O
the	O
elements	O
of	O
w	O
and	O
is	O
a	O
tuning	O
parameter	O
practice	O
there	O
may	O
be	O
multiple	O
regularization	B
terms	O
each	O
with	O
their	O
own	O
for	O
example	O
an	O
early	O
popular	O
penalty	B
is	O
the	O
quadratic	O
n	O
pkx	O
j	O
w	O
d	O
w	O
k	O
as	O
in	O
ridge	B
regression	B
also	O
known	O
as	O
the	O
weight-decay	O
penalty	B
it	O
pulls	O
the	O
weights	B
toward	O
zero	O
the	O
biases	O
are	O
not	O
penalized	O
lasso	B
penalties	O
are	O
also	O
popular	O
as	O
are	O
mixtures	O
of	O
these	O
elastic	B
net	I
for	O
binary	O
classification	O
we	O
could	O
take	O
l	O
to	O
be	O
binomial	B
deviance	B
in	O
which	O
case	O
the	O
neural	B
network	I
amounts	O
to	O
a	O
penalized	O
logistic	B
regression	B
section	O
albeit	O
a	O
highly	O
parametrized	O
and	O
penalized	O
one	O
loss	O
functions	O
are	O
usually	O
convex	O
in	O
f	O
but	O
not	O
in	O
the	O
elements	O
of	O
w	O
so	O
solving	O
is	O
difficult	O
and	O
at	O
best	O
we	O
seek	O
good	O
local	O
optima	O
most	O
methods	O
are	O
based	O
on	O
some	O
form	B
of	O
gradient	B
descent	I
with	O
many	O
associated	O
bells	O
and	O
whistles	O
we	O
briefly	O
discuss	O
some	O
elements	O
of	O
the	O
current	O
practice	O
in	O
finding	O
good	O
solutions	O
to	O
computing	O
the	O
gradient	O
backpropagation	B
the	O
elements	O
of	O
w	O
occur	O
in	O
layers	O
since	O
f	O
w	O
is	O
defined	O
as	O
a	O
series	O
of	O
compositions	O
starting	O
from	O
the	O
input	B
layer	B
computing	O
the	O
gradient	O
is	O
also	O
done	O
most	O
naturally	O
in	O
layers	O
chain	B
rule	B
for	I
differentiation	I
see	O
for	O
example	O
in	O
algorithm	B
below	O
and	O
our	O
notation	O
makes	O
this	O
easier	O
to	O
describe	O
in	O
a	O
recursive	O
fashion	O
we	O
will	O
consider	O
computing	O
the	O
derivative	O
of	O
l	O
y	O
f	O
w	O
with	O
respect	O
to	O
any	O
of	O
the	O
elements	O
of	O
w	O
for	O
a	O
generic	O
input	O
output	O
pair	O
x	O
y	O
since	O
the	O
loss	O
part	O
of	O
the	O
objective	O
is	O
a	O
sum	O
fitting	O
a	O
neural	B
network	I
the	O
overall	O
gradient	O
will	O
be	O
the	O
sum	O
of	O
these	O
individual	O
gradient	O
elements	O
over	O
the	O
training	O
pairs	O
yi	O
the	O
intuition	O
is	O
as	O
follows	O
given	O
a	O
training	O
generic	O
pair	O
y	O
we	O
first	O
make	O
a	O
forward	B
pass	I
through	O
the	O
network	O
which	O
creates	O
activations	O
at	O
each	O
of	O
the	O
nodes	B
a	O
k	O
in	O
each	O
of	O
the	O
layers	O
including	O
the	O
final	O
output	B
layer	B
we	O
would	O
then	O
like	O
to	O
compute	O
an	O
error	O
term	O
that	O
measures	O
the	O
responsibility	O
of	O
each	O
node	O
for	O
the	O
error	O
in	O
predicting	O
the	O
true	O
output	O
y	O
for	O
the	O
output	O
activations	O
a	O
k	O
these	O
errors	B
are	O
easy	O
either	O
residuals	O
or	O
generalized	O
residuals	O
depending	O
on	O
the	O
loss	B
function	B
for	O
activations	O
at	O
inner	O
layers	O
will	O
be	O
a	O
weighted	O
sum	O
of	O
the	O
errors	B
terms	O
of	O
nodes	B
that	O
use	O
a	O
k	O
as	O
inputs	O
the	O
backpropagation	B
algorithm	B
gives	O
the	O
details	O
for	O
computing	O
the	O
gradient	O
for	O
a	O
single	O
input	O
output	O
pair	O
x	O
y	O
we	O
leave	O
it	O
to	O
the	O
reader	O
to	O
verify	O
that	O
this	O
indeed	O
implements	O
the	O
chain	B
rule	B
for	I
differentiation	I
algorithm	B
backpropagation	B
at	O
each	O
of	O
the	O
layers	O
lk	O
i	O
e	O
compute	O
f	O
given	O
a	O
pair	O
x	O
y	O
perform	O
a	O
feedforward	O
pass	O
computing	O
the	O
activations	O
a	O
k	O
w	O
at	O
x	O
using	O
the	O
current	O
w	O
saving	O
each	O
of	O
the	O
intermediary	O
quantities	O
along	O
the	O
way	O
for	O
each	O
output	O
unit	O
in	O
layer	B
lk	O
compute	O
d	O
y	O
f	O
d	O
y	O
f	O
w	O
pg	O
k	O
z	O
k	O
where	O
pg	O
denotes	O
the	O
derivative	O
of	O
g	O
z	O
wrt	O
z	O
for	O
example	O
for	O
l	O
y	O
f	O
d	O
ky	O
f	O
for	O
layers	O
k	O
d	O
k	O
k	O
and	O
for	O
each	O
node	O
in	O
layer	B
k	O
set	B
becomes	O
f	O
pg	O
k	O
z	O
k	O
pg	O
k	O
z	O
k	O
d	O
j	O
w	O
k	O
j	O
the	O
partial	O
derivatives	O
are	O
given	O
by	O
w	O
y	O
f	O
d	O
a	O
k	O
j	O
one	O
again	O
matrix	B
vector	B
notation	O
simplifies	O
these	O
expressions	O
a	O
bit	O
neural	O
networks	O
becomes	O
squared-error	O
loss	O
d	O
a	O
k	O
pg	O
k	O
z	O
k	O
where	O
denotes	O
the	O
hadamard	B
product	I
becomes	O
pg	O
k	O
z	O
ki	O
w	O
becomes	O
y	O
f	O
w	O
d	O
backpropagation	B
was	O
considered	O
a	O
breakthrough	O
in	O
the	O
early	O
days	O
of	O
neural	O
networks	O
since	O
it	O
made	O
fitting	O
a	O
complex	O
model	O
computationally	O
manageable	O
gradient	B
descent	I
algorithm	B
computes	O
the	O
gradient	O
of	O
the	O
loss	B
function	B
at	O
a	O
single	O
generic	O
pair	O
y	O
with	O
n	O
training	O
pairs	O
the	O
gradient	O
of	O
the	O
first	O
part	O
of	O
is	O
given	O
by	O
w	O
nx	O
yi	O
f	O
n	O
w	O
d	O
w	O
c	O
with	O
the	O
quadratic	O
form	B
for	O
the	O
penalty	B
a	O
gradient-descent	O
update	O
is	O
w	O
w	O
k	O
d	O
k	O
where	O
is	O
the	O
learning	B
rate	B
gradient	B
descent	I
requires	O
starting	O
values	O
for	O
all	O
the	O
weights	B
w	O
zero	O
is	O
not	O
an	O
option	O
because	O
each	O
layer	B
is	O
symmetric	O
in	O
the	O
weights	B
flowing	O
to	O
the	O
different	O
neurons	B
hence	O
we	O
rely	O
on	O
starting	O
values	O
to	O
break	O
the	O
symmetries	O
typically	O
one	O
would	O
use	O
random	O
starting	O
weights	B
close	O
to	O
zero	O
random	O
uniform	O
or	O
gaussian	B
weights	B
are	O
common	O
there	O
are	O
a	O
multitude	O
of	O
tricks	O
of	O
the	O
trade	O
in	O
fitting	O
or	O
learning	O
a	O
neural	B
network	I
and	O
many	O
of	O
them	O
are	O
connected	O
with	O
gradient	B
descent	I
here	O
we	O
list	O
some	O
of	O
these	O
without	O
going	O
into	O
great	O
detail	O
stochastic	B
gradient	B
descent	I
rather	O
than	O
process	O
all	O
the	O
observations	O
before	O
making	O
a	O
gradient	O
step	O
it	O
can	O
be	O
more	O
efficient	O
to	O
process	O
smaller	O
batches	O
at	O
a	O
time	O
even	O
batches	O
fitting	O
a	O
neural	B
network	I
figure	O
training	O
and	O
test	O
misclassification	O
error	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
epochs	O
of	O
training	O
for	O
the	O
mnist	O
digit	O
classification	O
problem	O
the	O
architecture	O
for	O
the	O
network	O
is	O
shown	O
in	O
figure	O
the	O
network	O
was	O
fit	O
using	O
accelerated	B
gradient	B
descent	I
with	O
adaptive	B
rate	B
control	B
a	O
rectified	O
linear	B
activation	B
function	B
and	O
dropout	O
regularization	B
the	O
horizontal	O
broken	O
line	O
shows	O
the	O
error	O
rate	B
of	O
a	O
random	B
forest	I
a	O
logistic	B
regression	B
model	I
achieves	O
only	O
the	O
scale	B
of	O
size	O
one	O
these	O
batches	O
can	O
be	O
sampled	O
at	O
random	O
or	O
systematically	O
processed	O
for	O
large	O
data	B
sets	O
distributed	O
on	O
multiple	O
computer	O
cores	O
this	O
can	O
be	O
essential	O
for	O
reasons	O
of	O
efficiency	O
an	O
epoch	B
of	O
training	O
means	O
that	O
all	O
n	O
training	O
samples	O
have	O
been	O
used	O
in	O
gradient	O
steps	O
irrespective	O
of	O
how	O
they	O
have	O
been	O
grouped	O
hence	O
how	O
many	O
gradient	O
steps	O
have	O
been	O
made	O
accelerated	O
gradient	O
methods	O
the	O
idea	O
here	O
is	O
to	O
allow	O
previous	O
iterations	O
to	O
build	O
up	O
momentum	O
and	O
influence	O
the	O
current	O
iterations	O
the	O
iterations	O
have	O
the	O
form	B
d	O
wt	O
c	O
d	O
wt	O
c	O
errorlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllltraintesttest	O
rf	O
neural	O
networks	O
using	O
wt	O
to	O
represent	O
the	O
entire	O
collection	O
of	O
weights	B
at	O
iteration	O
t	B
vt	O
is	O
a	O
velocity	B
vector	B
that	O
accumulates	O
gradient	O
information	B
from	O
previous	O
iterations	O
and	O
is	O
controlled	O
by	O
an	O
additional	O
momentum	O
parameter	O
when	O
correctly	O
tuned	O
accelerated	B
gradient	B
descent	I
can	O
achieve	O
much	O
faster	O
convergence	O
rates	O
however	O
tuning	O
tends	O
to	O
be	O
a	O
difficult	O
process	O
and	O
is	O
typically	O
done	O
adaptively	O
rate	B
annealing	I
a	O
variety	O
of	O
creative	O
methods	O
have	O
been	O
proposed	O
to	O
adapt	O
the	O
learning	B
rate	B
to	O
avoid	O
jumping	O
across	O
good	O
local	O
minima	O
these	O
tend	O
to	O
be	O
a	O
mixture	O
of	O
principled	O
approaches	O
combined	O
with	O
ad-hoc	O
adaptations	O
that	O
tend	O
to	O
work	O
well	O
in	O
practice	O
figure	O
shows	O
the	O
performance	O
of	O
our	O
neural	O
net	O
on	O
the	O
mnist	O
digit	O
data	B
this	O
achieves	O
state-of-the	O
art	O
misclassification	O
error	O
rates	O
on	O
these	O
data	B
under	O
errors	B
and	O
outperforms	O
random	O
forests	O
and	O
a	O
generalized	O
linear	B
model	I
figure	O
shows	O
the	O
misclassified	O
digits	O
figure	O
all	O
misclassified	O
digits	O
in	O
the	O
mnist	O
test	O
set	B
the	O
true	O
digit	O
class	O
is	O
labeled	O
in	O
blue	O
the	O
predicted	O
in	O
red	O
fitting	O
a	O
neural	B
network	I
other	O
tuning	O
parameters	O
apart	O
from	O
the	O
many	O
details	O
associated	O
with	O
gradient	B
descent	I
there	O
are	O
several	O
other	O
important	O
structural	O
and	O
operational	O
aspects	O
of	O
neural	O
networks	O
that	O
have	O
to	O
be	O
specified	O
number	B
of	I
hidden	I
layers	I
and	O
their	O
sizes	O
with	O
a	O
single	O
hidden	B
layer	B
the	O
number	O
of	O
hidden	O
units	O
determines	O
the	O
number	O
of	O
parameters	O
in	O
principle	O
one	O
could	O
treat	O
this	O
number	O
as	O
a	O
tuning	O
parameter	O
which	O
could	O
be	O
adjusted	O
to	O
avoid	O
overfitting	O
the	O
current	O
collective	O
wisdom	O
suggests	O
it	O
is	O
better	O
to	O
have	O
an	O
abundant	O
number	O
of	O
hidden	O
units	O
and	O
control	B
the	O
model	O
complexity	O
instead	O
by	O
weight	O
regularization	B
having	O
deeper	O
networks	O
hidden	O
layers	O
increases	O
the	O
complexity	O
as	O
well	O
the	O
correct	O
number	O
tends	O
to	O
be	O
task	O
specific	O
having	O
two	O
hidden	O
layers	O
with	O
the	O
digit	O
recognition	O
problem	O
leads	O
to	O
competitive	O
performance	O
choice	O
of	O
nonlinearities	O
there	O
are	O
a	O
number	O
of	O
activation	O
functions	O
g	O
k	O
in	O
current	O
use	O
apart	O
from	O
the	O
sigmoid	B
function	B
which	O
transforms	O
its	O
input	O
to	O
a	O
values	O
in	O
other	O
popular	O
choices	O
are	O
figure	O
activation	O
functions	O
relu	B
is	O
a	O
rectified	O
linear	B
tanh	B
g	O
z	O
d	O
ez	O
e	O
ez	O
c	O
e	O
relu	B
which	O
delivers	O
values	O
in	O
neural	O
networks	O
rectified	O
linear	B
g	O
z	O
d	O
zc	O
or	O
the	O
positive-part	O
function	B
this	O
has	O
the	O
advantage	O
of	O
making	O
the	O
gradient	O
computations	B
cheaper	O
to	O
compute	O
leaky	O
rectified	O
linear	B
g	O
d	O
zc	O
for	O
nonnegative	O
and	O
close	O
to	O
zero	O
the	O
rectified	O
linear	B
tends	O
to	O
have	O
flat	O
spots	O
because	O
of	O
the	O
many	O
zero	O
activations	O
this	O
is	O
an	O
attempt	O
to	O
avoid	O
these	O
and	O
the	O
accompanying	O
zero	O
gradients	O
choice	O
of	O
regularization	B
typically	O
this	O
is	O
a	O
mixture	O
of	O
and	O
regularization	B
each	O
of	O
which	O
requires	O
a	O
tuning	O
parameter	O
as	O
in	O
lasso	B
and	O
regression	B
applications	O
the	O
bias	B
terms	O
are	O
usually	O
not	O
regularized	O
the	O
weight	O
regularization	B
is	O
typically	O
light	O
and	O
serves	O
several	O
roles	O
the	O
reduces	O
problems	O
with	O
collinearity	O
the	O
can	O
ignore	O
irrelevant	O
features	O
and	O
both	O
slow	O
the	O
rate	B
of	O
overfitting	O
especially	O
with	O
deep	O
networks	O
early	B
stopping	I
neural	O
nets	O
are	O
typically	O
over-parametrized	O
and	O
hence	O
are	O
prone	O
to	O
overfitting	O
originally	O
early	B
stopping	I
was	O
set	B
up	O
as	O
the	O
primary	O
tuning	O
parameter	O
and	O
the	O
stopping	O
time	O
was	O
determined	O
using	O
a	O
held-out	O
set	B
of	O
validation	O
data	B
in	O
modern	O
networks	O
the	O
regularization	B
is	O
tuned	O
adaptively	O
to	O
avoid	O
overfitting	O
and	O
hence	O
it	O
is	O
less	O
of	O
a	O
problem	O
for	O
example	O
in	O
figure	O
we	O
see	O
that	O
the	O
test	O
misclassification	O
error	O
has	O
flattened	O
out	O
and	O
does	O
not	O
rise	O
again	O
with	O
increasing	O
number	O
of	O
epochs	O
autoencoders	O
an	O
autoencoder	B
is	O
a	O
special	O
neural	B
network	I
for	O
computing	O
a	O
type	O
of	O
nonlinear	B
principal-component	O
decomposition	O
the	O
linear	B
principal	O
component	O
decomposition	O
is	O
a	O
popular	O
and	O
effective	O
linear	B
method	B
for	O
reducing	O
a	O
large	O
set	B
of	O
correlated	O
variables	O
to	O
a	O
typically	O
smaller	O
number	O
of	O
linear	B
combinations	O
that	O
capture	O
most	O
of	O
the	O
variance	B
in	O
the	O
original	O
set	B
hence	O
given	O
a	O
collection	O
of	O
n	O
vectors	O
xi	O
rp	O
to	O
have	O
mean	O
zero	O
we	O
produce	O
a	O
derived	O
set	B
of	O
uncorrelated	O
features	O
zi	O
rq	O
autoencoders	O
p	O
and	O
typically	O
smaller	O
via	O
zi	O
d	O
v	O
xi	O
the	O
columns	O
of	O
v	O
are	O
orthonormal	O
and	O
are	O
derived	O
such	O
that	O
the	O
first	O
component	O
of	O
zi	O
has	O
maximal	O
variance	B
the	O
second	O
has	O
the	O
next	O
largest	O
variance	B
and	O
is	O
uncorrelated	O
with	O
the	O
first	O
and	O
so	O
on	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
columns	O
of	O
v	O
are	O
the	O
leading	O
q	O
eigenvectors	O
of	O
the	O
sample	B
covariance	O
matrix	B
s	O
d	O
n	O
x	O
principal	B
components	I
can	O
also	O
be	O
derived	O
in	O
terms	O
of	O
a	O
best-approximating	B
linear	B
subspace	I
and	O
it	O
is	O
this	O
version	O
that	O
leads	O
to	O
the	O
nonlinear	B
generalization	O
presented	O
here	O
consider	O
the	O
optimization	O
problem	O
x	O
minimize	O
kxi	O
nx	O
for	O
q	O
p	O
the	O
subspace	O
is	O
defined	O
by	O
the	O
column	O
space	O
of	O
a	O
and	O
for	O
each	O
point	O
xi	O
we	O
wish	O
to	O
locate	O
its	O
best	O
approximation	O
in	O
the	O
subspace	O
terms	O
of	O
euclidean	O
distance	O
without	O
loss	O
of	O
generality	O
we	O
can	O
assume	O
a	O
has	O
orthonormal	O
columns	O
in	O
which	O
case	O
d	O
a	O
xi	O
for	O
each	O
i	O
separate	O
linear	B
regressions	O
plugging	O
in	O
reduces	O
to	O
kxi	O
aa	O
a	O
solution	O
is	O
given	O
by	O
oa	O
d	O
v	O
the	O
matrix	B
above	O
of	O
the	O
first	O
q	O
principalcomponent	O
direction	O
vectors	O
computed	O
from	O
the	O
xi	O
by	O
analogy	O
a	O
singlelayer	O
autoencoder	B
solves	O
a	O
nonlinear	B
version	O
of	O
this	O
problem	O
nx	O
a	O
minimize	O
adiq	O
minimize	O
w	O
kxi	O
w	O
g	O
w	O
xi	O
nx	O
for	O
some	O
nonlinear	B
activation	B
function	B
g	O
see	O
figure	O
panel	O
if	O
g	O
is	O
the	O
identity	O
function	B
these	O
solutions	O
coincide	O
w	O
d	O
v	O
figure	O
panel	O
represents	O
the	O
learned	O
row	O
of	O
w	O
as	O
images	O
when	O
the	O
autoencoder	B
is	O
fit	O
to	O
the	O
mnist	O
digit	O
database	O
since	O
autoencoders	O
do	O
not	O
require	O
a	O
response	B
class	O
labels	O
in	O
this	O
case	O
this	O
decomposition	O
is	O
unsupervised	O
it	O
is	O
often	O
expensive	O
to	O
label	O
images	O
for	O
example	O
while	O
unlabeled	B
images	I
are	O
abundant	O
autoencoders	O
provide	O
a	O
means	O
for	O
extracting	O
potentially	O
useful	O
features	O
from	O
such	O
data	B
which	O
can	O
then	O
be	O
used	O
with	O
labeled	O
data	B
to	O
train	O
a	O
classifier	O
in	O
fact	O
they	O
are	O
often	O
used	O
as	O
warm	B
starts	I
for	O
the	O
weights	B
when	O
fitting	O
a	O
supervised	O
neural	B
network	I
once	O
again	O
there	O
are	O
a	O
number	O
of	O
bells	O
and	O
whistles	O
that	O
make	O
autoen	O
coders	O
more	O
effective	O
neural	O
networks	O
figure	O
left	O
network	O
representation	O
of	O
an	O
autoencoder	B
used	O
for	O
unsupervised	O
learning	O
of	O
nonlinear	B
principal	B
components	I
the	O
middle	O
layer	B
of	O
hidden	O
units	O
creates	O
a	O
bottleneck	O
and	O
learns	O
nonlinear	B
representations	O
of	O
the	O
inputs	O
the	O
output	B
layer	B
is	O
the	O
transpose	O
of	O
the	O
input	B
layer	B
so	O
the	O
network	O
tries	O
to	O
reproduce	O
the	O
input	O
data	B
using	O
this	O
restrictive	O
representation	O
right	O
images	O
representing	O
the	O
estimated	O
rows	O
of	O
w	O
using	O
the	O
mnist	O
database	O
the	O
images	O
can	O
be	O
seen	O
as	O
filters	O
that	O
detect	O
local	O
gradients	O
in	O
the	O
image	O
pixels	O
in	O
each	O
image	O
most	O
of	O
the	O
weights	B
are	O
zero	O
and	O
the	O
nonzero	O
weights	B
are	O
localized	O
in	O
the	O
two-dimensional	O
image	O
space	O
regularization	B
applied	O
to	O
the	O
rows	O
of	O
w	O
lead	O
to	O
sparse	O
weight	O
vectors	O
and	O
hence	O
local	O
features	O
as	O
was	O
the	O
case	O
in	O
our	O
example	O
denoising	O
is	O
a	O
process	O
where	O
noise	O
is	O
added	O
to	O
the	O
input	B
layer	B
not	O
the	O
output	O
resulting	O
in	O
features	O
that	O
do	O
not	O
focus	O
on	O
isolated	O
values	O
such	O
as	O
pixels	O
but	O
instead	O
have	O
some	O
volume	O
we	O
discuss	O
denoising	O
further	O
in	O
section	O
with	O
regularization	B
the	O
bottleneck	O
is	O
not	O
necessary	O
as	O
in	O
the	O
figure	O
or	O
in	O
principal	B
components	I
in	O
fact	O
we	O
can	O
learn	O
many	O
more	O
than	O
p	O
components	O
autoencoders	O
can	O
also	O
have	O
multiple	O
layers	O
which	O
are	O
typically	O
learned	O
sequentially	O
the	O
activations	O
learned	O
in	O
the	O
first	O
layer	B
are	O
treated	O
as	O
the	O
input	O
output	O
features	O
and	O
a	O
model	O
like	O
is	O
fit	O
to	O
them	O
deep	B
learning	I
neural	O
networks	O
were	O
reincarnated	O
around	O
with	O
deep	B
learning	I
as	O
a	O
flashier	O
name	O
largely	O
a	O
result	O
of	O
much	O
faster	O
and	O
larger	O
computing	O
systems	O
plus	O
a	O
few	O
new	O
ideas	O
they	O
have	O
been	O
shown	O
to	O
be	O
particularly	O
successful	O
deep	B
learning	I
in	O
the	O
difficult	O
task	O
of	O
classifying	O
natural	O
images	O
using	O
what	O
is	O
known	O
as	O
a	O
convolutional	O
architecture	O
initially	O
autoencoders	O
were	O
considered	O
a	O
crucial	O
aspect	O
of	O
deep	B
learning	I
since	O
unlabeled	B
images	I
are	O
abundant	O
however	O
as	O
labeled	O
corpora	O
become	O
more	O
available	O
the	O
word	O
on	O
the	O
street	O
is	O
that	O
supervised	B
learning	I
is	O
sufficient	O
figure	O
shows	O
examples	O
of	O
natural	O
images	O
each	O
with	O
a	O
class	O
label	O
such	O
as	O
beaver	O
sunflower	O
trout	O
etc	O
there	O
are	O
class	O
labels	O
in	O
figure	O
examples	O
of	O
natural	O
images	O
the	O
database	O
consists	O
of	O
color	O
image	O
classes	O
with	O
examples	O
in	O
each	O
class	O
train	O
test	O
each	O
image	O
is	O
green	O
blue	O
here	O
we	O
display	O
a	O
randomly	O
chosen	O
image	O
from	O
each	O
class	O
the	O
classes	O
are	O
organized	O
by	O
hierarchical	O
structure	B
with	O
coarse	O
levels	O
and	O
five	O
subclasses	O
within	O
each	O
so	O
for	O
example	O
the	O
first	O
five	O
images	O
in	O
the	O
first	O
column	O
are	O
aquatic	O
mammals	O
namely	O
beaver	O
dolphin	O
otter	O
seal	O
and	O
whale	O
neural	O
networks	O
all	O
and	O
training	O
images	O
and	O
test	O
images	O
per	O
class	O
the	O
goal	O
is	O
to	O
build	O
a	O
classifier	O
to	O
assign	O
a	O
label	O
to	O
an	O
image	O
we	O
present	O
the	O
essential	O
details	O
of	O
a	O
deep-learning	O
network	O
for	O
this	O
task	O
one	O
that	O
achieves	O
a	O
respectable	O
classification	O
performance	O
of	O
errors	B
on	O
the	O
designated	O
test	O
figure	O
shows	O
a	O
typical	O
deep-learning	O
architecture	O
with	O
many	O
figure	O
architecture	O
of	O
a	O
deep-learning	O
network	O
for	O
the	O
image	O
classification	O
task	O
the	O
input	B
layer	B
and	O
hidden	O
layers	O
are	O
all	O
represented	O
as	O
images	O
except	O
for	O
the	O
last	O
hidden	B
layer	B
which	O
is	O
flattened	O
the	O
input	B
layer	B
consists	O
of	O
the	O
d	O
color	O
green	O
and	O
blue	O
versions	O
of	O
an	O
input	O
image	O
earlier	O
here	O
we	O
use	O
the	O
pk	O
to	O
refer	O
to	O
the	O
number	O
of	O
images	O
rather	O
than	O
the	O
totality	O
of	O
pixels	O
each	O
of	O
these	O
color	O
panes	O
is	O
pixels	O
in	O
dimension	O
the	O
first	O
hidden	B
layer	B
computes	O
a	O
convolution	B
using	O
a	O
bank	O
of	O
distinct	O
q	O
q	O
learned	O
filters	O
producing	O
an	O
array	O
of	O
images	O
of	O
dimension	O
the	O
next	O
pool	O
layer	B
reduces	O
each	O
non-overlapping	O
block	O
of	O
numbers	O
in	O
each	O
pane	O
of	O
the	O
first	O
hidden	B
layer	B
to	O
a	O
single	O
number	O
using	O
a	O
max	O
operation	O
both	O
q	O
and	O
are	O
typically	O
small	O
each	O
was	O
for	O
us	O
these	O
convolve	O
and	O
pool	O
layers	O
are	O
repeated	O
here	O
three	O
times	O
with	O
changing	O
dimensions	O
our	O
actual	O
implementation	O
there	O
are	O
layers	O
in	O
total	O
finally	O
the	O
derived	O
features	O
are	O
flattened	O
and	O
a	O
fully	B
connected	I
layer	B
maps	O
them	O
to	O
the	O
classes	O
via	O
a	O
softmax	B
activation	O
hidden	O
layers	O
these	O
consist	O
of	O
two	O
special	O
types	O
of	O
layers	O
convolve	O
and	O
pool	O
we	O
describe	O
each	O
in	O
turn	O
convolve	O
layer	B
figure	O
illustrates	O
a	O
convolution	B
layer	B
and	O
some	O
details	O
are	O
given	O
in	O
classification	O
becomes	O
increasingly	O
difficult	O
as	O
the	O
number	O
of	O
classes	O
grows	O
with	O
equal	O
representation	O
in	O
each	O
class	O
the	O
null	O
or	O
random	O
error	O
rate	B
for	O
k	O
classes	O
is	O
for	O
two	O
classes	O
for	O
fully	O
the	O
caption	O
if	O
an	O
image	O
x	O
is	O
represented	O
by	O
a	O
k	O
k	O
matrix	B
and	O
a	O
filter	O
f	O
deep	B
learning	I
figure	O
convolution	B
layer	B
for	O
the	O
input	O
images	O
the	O
input	O
image	O
is	O
split	O
into	O
its	O
three	O
color	O
components	O
a	O
single	O
filter	O
is	O
a	O
q	O
q	O
array	O
one	O
q	O
q	O
for	O
each	O
of	O
the	O
d	O
color	O
panes	O
and	O
is	O
used	O
to	O
compute	O
an	O
inner	O
product	O
with	O
a	O
correspondingly	O
sized	O
subimage	O
in	O
each	O
pane	O
and	O
summed	O
across	O
the	O
panes	O
we	O
used	O
q	O
d	O
and	O
small	O
values	O
are	O
typical	O
this	O
is	O
repeated	O
over	O
all	O
q	O
q	O
subimages	O
boundary	O
padding	O
and	O
hence	O
produces	O
an	O
image	O
of	O
the	O
same	O
dimension	O
as	O
one	O
of	O
the	O
input	O
panes	O
this	O
is	O
the	O
convolution	B
operation	O
there	O
are	O
different	O
versions	O
of	O
this	O
filter	O
and	O
hence	O
new	O
panes	O
are	O
produced	O
each	O
of	O
the	O
filters	O
has	O
weights	B
which	O
are	O
learned	O
via	O
backpropagation	B
xic	O
jc	O
f	O
pq	O
qx	O
with	O
elements	O
qxi	O
j	O
dpq	O
is	O
a	O
q	O
q	O
matrix	B
with	O
q	O
k	O
the	O
convolved	O
image	O
is	O
another	O
k	O
k	O
matrix	B
edge	O
padding	O
to	O
achieve	O
a	O
full-sized	O
k	O
k	O
output	O
image	O
in	O
our	O
application	O
we	O
used	O
but	O
other	O
sizes	O
such	O
as	O
are	O
popular	O
it	O
is	O
most	O
natural	O
to	O
represent	O
the	O
structure	B
in	O
terms	O
of	O
these	O
images	O
as	O
in	O
figure	O
but	O
they	O
could	O
all	O
be	O
vectorized	O
into	O
a	O
massive	O
network	O
diagram	O
as	O
in	O
figures	O
and	O
however	O
the	O
weights	B
would	O
have	O
special	O
sparse	O
structure	B
with	O
most	O
being	O
zero	O
and	O
the	O
nonzero	O
values	O
repeated	O
weight	O
sharing	B
neural	O
networks	O
pool	O
layer	B
the	O
pool	O
layer	B
corresponds	O
to	O
a	O
kind	O
of	O
nonlinear	B
activation	O
it	O
reduces	O
each	O
nonoverlapping	O
block	O
of	O
pixels	O
d	O
for	O
us	O
to	O
a	O
single	O
number	O
by	O
computing	O
their	O
maximum	O
why	O
maximum	O
the	O
convolution	B
filters	O
are	O
themselves	O
small	O
image	O
patches	O
and	O
are	O
looking	O
to	O
identify	O
similar	O
patches	O
in	O
the	O
target	O
image	O
which	O
case	O
the	O
inner	O
product	O
will	O
be	O
high	O
the	O
max	O
operation	O
introduces	O
an	O
element	O
of	O
local	B
translation	I
invariance	I
the	O
pool	O
operation	O
reduces	O
the	O
size	O
of	O
each	O
image	O
by	O
a	O
factor	B
r	B
in	O
each	O
dimension	O
to	O
compensate	O
the	O
number	O
of	O
tiles	O
in	O
the	O
next	O
convolution	B
layer	B
is	O
typically	O
increased	O
accordingly	O
also	O
as	O
these	O
tiles	O
get	O
smaller	O
the	O
effective	O
weights	B
resulting	O
from	O
the	O
convolution	B
operator	O
become	O
denser	O
eventually	O
the	O
tiles	O
are	O
the	O
same	O
size	O
as	O
the	O
convolution	B
filter	O
and	O
the	O
layer	B
becomes	O
fully	O
connected	O
learning	O
a	O
deep	O
network	O
despite	O
the	O
additional	O
structure	B
imposed	O
by	O
the	O
convolution	B
layers	O
deep	O
networks	O
are	O
learned	O
by	O
gradient	B
descent	I
the	O
gradients	O
are	O
computed	O
by	O
backpropagation	B
as	O
before	O
but	O
with	O
special	O
care	O
taken	O
to	O
accommodate	O
the	O
tied	B
weights	B
in	O
the	O
convolution	B
filters	O
however	O
a	O
number	O
of	O
additional	O
tricks	O
have	O
been	O
introduced	O
that	O
appear	O
to	O
improve	O
the	O
performance	O
of	O
modern	O
deep	B
learning	I
networks	O
these	O
are	O
mostly	O
aimed	O
at	O
regularization	B
indeed	O
our	O
image	O
network	O
has	O
around	O
million	O
parameters	O
so	O
regularization	B
is	O
essential	O
to	O
avoid	O
overfitting	O
we	O
briefly	O
discuss	O
some	O
of	O
these	O
dropout	O
this	O
is	O
a	O
form	B
of	O
regularization	B
that	O
is	O
performed	O
when	O
learning	O
a	O
network	O
typically	O
at	O
different	O
rates	O
at	O
the	O
different	O
layers	O
it	O
applies	O
to	O
all	O
networks	O
not	O
just	O
convolutional	O
in	O
fact	O
it	O
appears	O
to	O
work	O
better	O
when	O
applied	O
at	O
the	O
deeper	O
denser	O
layers	O
consider	O
computing	O
the	O
activation	O
z	O
k	O
in	O
layer	B
k	O
as	O
in	O
for	O
a	O
single	O
observation	O
during	O
the	O
feed-forward	B
stage	O
the	O
idea	O
is	O
to	O
randomly	O
set	B
each	O
of	O
the	O
nodes	B
to	O
zero	O
with	O
probability	O
and	O
inflate	O
the	O
remaining	O
ones	O
by	O
a	O
factor	B
hence	O
for	O
this	O
observation	O
those	O
nodes	B
that	O
survive	O
have	O
to	O
stand	O
in	O
for	O
those	O
omitted	O
this	O
can	O
be	O
shown	O
to	O
be	O
a	O
form	B
of	O
ridge	B
regularization	B
and	O
when	O
done	O
correctly	O
improves	O
performance	O
the	O
fraction	O
omitted	O
is	O
a	O
tuning	O
parameter	O
and	O
for	O
convolutional	O
networks	O
it	O
appears	O
to	O
be	O
better	O
to	O
use	O
different	O
values	O
at	O
j	O
learning	O
a	O
deep	O
network	O
different	O
layers	O
in	O
particular	O
as	O
the	O
layers	O
become	O
denser	O
is	O
increased	O
from	O
in	O
the	O
input	B
layer	B
to	O
in	O
the	O
final	O
fully	B
connected	I
layer	B
input	B
distortion	I
this	O
is	O
another	O
form	B
of	O
regularization	B
that	O
is	O
particularly	O
suitable	O
for	O
tasks	O
like	O
image	O
classification	O
the	O
idea	O
is	O
to	O
augment	O
the	O
training	B
set	B
with	O
many	O
distorted	O
copies	O
of	O
an	O
input	O
image	O
of	O
course	O
the	O
same	O
label	O
these	O
distortions	O
can	O
be	O
location	O
shifts	O
and	O
other	O
small	O
affine	O
transformations	O
but	O
also	O
color	O
and	O
shading	O
shifts	O
that	O
might	O
appear	O
in	O
natural	O
images	O
we	O
show	O
figure	O
each	O
column	O
represents	O
distorted	O
versions	O
of	O
an	O
input	O
image	O
including	O
affine	O
and	O
color	O
distortions	O
the	O
input	O
images	O
are	O
padded	O
on	O
the	O
boundary	O
to	O
increase	O
the	O
size	O
and	O
hence	O
allow	O
space	O
for	O
some	O
of	O
the	O
distortions	O
some	O
distorted	O
versions	O
of	O
input	O
images	O
in	O
figure	O
the	O
distortions	O
are	O
such	O
that	O
a	O
human	O
would	O
have	O
no	O
trouble	O
identifying	O
any	O
of	O
the	O
distorted	O
images	O
if	O
they	O
could	O
identify	O
the	O
original	O
this	O
both	O
enriches	O
the	O
training	O
data	B
with	O
hints	O
and	O
also	O
prevents	O
overfitting	O
to	O
the	O
original	O
image	O
one	O
could	O
also	O
apply	O
distortions	O
to	O
a	O
test	O
image	O
and	O
then	O
poll	O
the	O
results	O
to	O
produce	O
a	O
final	O
classification	O
configuration	O
designing	O
the	O
correct	O
architecture	O
for	O
a	O
deep-learning	O
network	O
along	O
with	O
the	O
various	O
choices	O
at	O
each	O
layer	B
appears	O
to	O
require	O
experience	O
and	O
trial	O
neural	O
networks	O
and	O
error	O
we	O
summarize	O
the	O
third	O
and	O
final	O
architecture	O
which	O
we	O
built	O
for	O
classifying	O
the	O
data	B
set	B
in	O
algorithm	B
in	O
addition	O
to	O
these	O
size	O
parameters	O
for	O
each	O
layer	B
we	O
must	O
select	O
the	O
activation	O
functions	O
and	O
additional	O
regularization	B
in	O
this	O
case	O
we	O
used	O
the	O
leaky	O
rectified	O
linear	B
functions	O
g	O
with	O
increasing	O
from	O
in	O
layer	B
up	O
to	O
in	O
layer	B
in	O
addition	O
a	O
type	O
of	O
regularization	B
was	O
imposed	O
on	O
the	O
weights	B
restricting	O
all	O
incoming	O
weight	O
vectors	O
to	O
a	O
node	O
to	O
have	O
norm	O
bounded	O
by	O
one	O
figure	O
shows	O
both	O
the	O
progress	O
of	O
the	O
optimization	O
objective	O
and	O
the	O
test	O
misclassification	O
error	O
as	O
the	O
gradientdescent	O
algorithm	B
proceeds	O
the	O
accelerated	O
gradient	O
method	B
maintains	O
a	O
memory	O
which	O
we	O
can	O
see	O
was	O
restarted	O
twice	O
to	O
get	O
out	O
of	O
local	O
minima	O
our	O
network	O
achieved	O
a	O
test	O
error	O
rate	B
of	O
on	O
the	O
test	O
images	O
images	O
per	O
class	O
the	O
best	O
reported	O
error	O
rate	B
we	O
have	O
seen	O
is	O
so	O
apparently	O
we	O
have	O
some	O
way	O
to	O
go	O
figure	O
progress	O
of	O
the	O
algorithm	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
epochs	O
the	O
accelerated	O
gradient	O
algorithm	B
is	O
restarted	O
every	O
epochs	O
meaning	O
the	O
long-term	O
memory	O
is	O
forgotten	O
and	O
a	O
new	O
trail	O
is	O
begun	O
starting	O
at	O
the	O
current	O
solution	O
the	O
red	O
curve	O
shows	O
the	O
objective	O
penalized	O
log-likelihood	B
on	O
the	O
training	O
data	B
the	O
blue	O
curve	O
shows	O
test-set	O
misclassification	O
error	O
the	O
vertical	O
axis	O
is	O
on	O
the	O
log	O
scale	B
so	O
zero	O
cannot	O
be	O
included	O
misclassification	O
costmisclassification	O
error	O
notes	O
and	O
details	O
algorithm	B
configuration	O
parameters	O
for	O
deep-learning	O
network	O
used	O
on	O
the	O
data	B
layer	B
convolution	B
maps	O
each	O
with	O
kernel	O
for	O
three	O
colors	O
the	O
input	O
image	O
is	O
padded	O
from	O
to	O
to	O
accommodate	O
input	O
distortions	O
layers	O
and	O
convolution	B
maps	O
each	O
compositions	O
of	O
convolutions	O
are	O
roughly	O
equivalent	O
to	O
convolutions	O
with	O
a	O
bigger	O
bandwidth	O
and	O
the	O
smaller	O
ones	O
have	O
fewer	O
parameters	O
layer	B
max	B
pool	I
layer	B
pooling	O
nonoverlapping	O
blocks	O
of	O
pixels	O
and	O
hence	O
reducing	O
the	O
images	O
to	O
size	O
layer	B
convolution	B
maps	O
each	O
with	O
dropout	B
learning	B
with	I
rate	B
d	O
layer	B
repeat	O
of	O
layer	B
layer	B
max	B
pool	I
layer	B
to	O
images	O
layer	B
convolution	B
maps	O
each	O
with	O
dropout	O
rate	B
d	O
layer	B
convolution	B
maps	O
each	O
with	O
dropout	O
rate	B
d	O
layer	B
max	B
pool	I
layer	B
to	O
images	O
layer	B
convolution	B
maps	O
each	O
this	O
is	O
a	O
pixelwise	O
weighted	O
sum	O
across	O
the	O
images	O
from	O
the	O
previous	O
layer	B
layer	B
fully	O
connected	O
units	O
with	O
dropout	O
rate	B
d	O
layer	B
final	O
output	O
units	O
with	O
softmax	B
activation	O
and	O
dropout	O
rate	B
d	O
notes	O
and	O
details	O
the	O
reader	O
will	O
notice	O
that	O
probability	O
models	B
have	O
disappeared	O
from	O
the	O
development	O
here	O
neural	O
nets	O
are	O
elaborate	O
regression	B
methods	O
aimed	O
solely	O
at	O
prediction	O
not	O
estimation	B
or	O
explanation	O
in	O
the	O
language	O
of	O
section	O
in	O
place	O
of	O
parametric	B
optimality	B
criteria	B
the	O
machine	B
learning	I
community	O
has	O
focused	O
on	O
a	O
set	B
of	O
specific	O
prediction	O
data	B
sets	O
like	O
the	O
digits	O
mnist	O
corpus	O
and	O
as	O
benchmarks	O
for	O
measuring	O
performance	O
there	O
is	O
a	O
vast	O
literature	O
on	O
neural	O
networks	O
with	O
hundreds	O
of	O
books	O
and	O
thousands	O
of	O
papers	O
with	O
the	O
resurgence	O
of	O
deep	B
learning	I
this	O
literature	O
is	O
again	O
growing	O
two	O
early	O
statistical	O
references	O
on	O
neural	O
networks	O
are	O
ripley	O
and	O
bishop	O
and	O
hastie	O
et	O
al	O
devote	O
one	O
chapter	O
to	O
the	O
topic	O
part	O
of	O
our	O
description	O
of	O
backpropagation	B
in	O
section	O
was	O
neural	O
networks	O
guided	O
by	O
andrew	O
ng	O
s	O
online	O
stanford	O
lecture	O
notes	O
bengio	O
et	O
al	O
provide	O
a	O
useful	O
review	O
of	O
autoencoders	O
lecun	O
et	O
al	O
give	O
a	O
brief	O
overview	O
of	O
deep	B
learning	I
written	O
by	O
three	O
pioneers	O
of	O
this	O
field	O
yann	O
lecun	O
yoshua	O
bengio	O
and	O
geoffrey	O
hinton	O
we	O
also	O
benefited	O
from	O
reading	O
ngiam	O
et	O
al	O
dropout	B
learning	I
et	O
al	O
is	O
a	O
relatively	O
new	O
idea	O
and	O
its	O
connections	O
with	O
ridge	B
regression	B
were	O
most	O
usefully	O
described	O
in	O
wager	O
et	O
al	O
the	O
most	O
popular	O
version	O
of	O
accelerated	B
gradient	B
descent	I
is	O
due	O
to	O
nesterov	O
learning	B
with	I
hints	O
is	O
due	O
to	O
abu-mostafa	O
the	O
material	O
in	O
sections	O
and	O
benefited	O
greatly	O
from	O
discussions	O
with	O
rakesh	O
achanta	O
and	O
hastie	O
who	O
produced	O
some	O
of	O
the	O
color	O
images	O
and	O
diagrams	O
and	O
designed	O
and	O
fit	O
the	O
deep-learning	O
network	O
to	O
the	O
data	B
the	O
neural	O
information	B
processing	O
systems	O
conferences	O
started	O
in	O
late	O
fall	O
in	O
denver	O
colorado	O
and	O
post-conference	O
workshops	O
were	O
held	O
at	O
the	O
nearby	O
ski	O
resort	O
at	O
vail	O
these	O
are	O
still	O
very	O
popular	O
today	O
although	O
the	O
venue	O
has	O
changed	O
over	O
the	O
years	O
the	O
nips	O
proceedings	O
are	O
refereed	O
and	O
nips	O
papers	O
count	O
as	O
publications	O
in	O
most	O
fields	O
especially	O
computer	O
science	O
and	O
engineering	O
although	O
neural	O
networks	O
were	O
initially	O
the	O
main	O
topic	O
of	O
the	O
conferences	O
a	O
modern	O
nips	O
conference	O
covers	O
all	O
the	O
latest	O
ideas	O
in	O
machine	B
learning	I
mnist	O
is	O
a	O
curated	O
database	O
of	O
images	O
of	O
handwritten	B
digits	I
and	O
cortes	O
there	O
are	O
training	O
images	O
and	O
test	O
images	O
each	O
a	O
grayscale	O
image	O
these	O
data	B
have	O
been	O
used	O
as	O
a	O
testbed	O
for	O
many	O
different	O
learning	O
algorithms	O
so	O
the	O
reported	O
best	O
error	O
rates	O
might	O
be	O
optimistic	O
tuning	O
parameters	O
typical	O
neural	B
network	I
implementations	O
have	O
dozens	O
of	O
tuning	O
parameters	O
and	O
many	O
of	O
these	O
are	O
associated	O
with	O
the	O
fine	O
tuning	O
of	O
the	O
descent	O
algorithm	B
we	O
used	O
the	O
function	B
in	O
the	O
r	B
package	O
to	O
fit	O
our	O
model	O
for	O
the	O
mnist	O
data	B
set	B
it	O
has	O
around	O
such	O
parameters	O
although	O
most	O
default	O
to	O
factory-tuned	O
constants	O
that	O
have	O
been	O
found	O
to	O
work	O
well	O
on	O
many	O
examples	O
arno	O
candel	O
was	O
very	O
helpful	O
in	O
assisting	O
us	O
with	O
the	O
software	O
dropout	O
and	O
ridge	B
regression	B
dropout	O
was	O
originally	O
proposed	O
in	O
srivastava	O
et	O
al	O
and	O
reinterpreted	O
in	O
wager	O
et	O
al	O
dropout	O
was	O
inspired	O
by	O
the	O
random	O
selection	O
of	O
variables	O
at	O
each	O
tree	O
split	O
in	O
a	O
random	B
forest	I
consider	O
a	O
simple	O
version	O
of	O
dropout	O
for	O
the	O
linear	B
regression	B
problem	O
with	O
squared-error	O
loss	O
we	O
have	O
an	O
n	O
p	O
regression	B
matrix	B
x	O
and	O
a	O
response	B
n-vector	O
y	O
for	O
simplicity	O
we	O
assume	O
all	O
variables	O
have	O
mean	O
zero	O
so	O
we	O
can	O
ignore	O
intercepts	O
consider	O
the	O
following	O
random	O
least-squares	O
criterion	O
notes	O
and	O
details	O
px	O
nx	O
xij	O
iij	O
j	O
li	O
d	O
iij	O
d	O
d	O
o	O
here	O
the	O
iij	O
are	O
i	O
i	O
d	O
variables	O
j	O
with	O
with	O
probability	O
with	O
probability	O
particular	O
form	B
is	O
used	O
so	O
that	O
e	O
iij	O
d	O
using	O
simple	O
probability	O
it	O
can	O
be	O
shown	O
that	O
the	O
expected	O
score	O
equations	O
can	O
be	O
written	O
e	O
with	O
d	O
d	O
hence	O
the	O
solution	O
is	O
given	O
by	O
y	O
c	O
x	O
d	O
d	O
x	O
c	O
x	O
c	O
x	O
d	O
x	O
y	O
a	O
generalized	O
ridge	B
regression	B
if	O
the	O
variables	O
are	O
standardized	O
the	O
term	O
d	O
becomes	O
a	O
scalar	O
and	O
the	O
solution	O
is	O
identical	O
to	O
ridge	B
regression	B
with	O
a	O
nonlinear	B
activation	B
function	B
the	O
interpretation	O
changes	O
slightly	O
see	O
wager	O
et	O
al	O
for	O
details	O
distortion	O
and	O
ridge	B
regression	B
we	O
again	O
show	O
in	O
a	O
simple	O
example	O
that	O
input	B
distortion	I
is	O
similar	O
to	O
ridge	B
regression	B
assume	O
the	O
same	O
setup	O
as	O
in	O
the	O
previous	O
example	O
except	O
a	O
different	O
randomized	O
version	O
of	O
the	O
criterion	O
ln	O
d	O
c	O
nij	O
j	O
nx	O
px	O
d	O
here	O
we	O
have	O
added	O
random	O
noise	O
to	O
the	O
prediction	O
variables	O
and	O
we	O
assume	O
this	O
noise	O
is	O
i	O
i	O
d	O
once	O
again	O
the	O
expected	O
score	O
equations	O
can	O
be	O
written	O
y	O
c	O
x	O
x	O
c	O
d	O
e	O
ij	O
d	O
once	O
again	O
because	O
of	O
the	O
independence	O
of	O
all	O
the	O
nij	O
and	O
this	O
leads	O
to	O
a	O
ridge	B
regression	B
so	O
replacing	O
each	O
observation	O
pair	O
xi	O
yi	O
by	O
the	O
collection	O
fx	O
is	O
a	O
noisy	O
version	O
of	O
xi	O
is	O
approximately	O
equivalent	O
to	O
a	O
ridge	B
regression	B
on	O
the	O
original	O
data	B
where	O
each	O
x	O
yigb	O
i	O
i	O
neural	O
networks	O
software	O
for	O
deep	B
learning	I
our	O
deep	B
learning	I
convolutional	O
network	O
for	O
the	O
data	B
was	O
constructed	O
and	O
run	O
by	O
rakesh	O
achanta	O
in	O
theano	O
a	O
python-based	O
system	O
et	O
al	O
bergstra	O
et	O
al	O
theano	O
has	O
a	O
user-friendly	O
language	O
for	O
specifying	O
the	O
host	O
of	O
parameters	O
for	O
a	O
deep-learning	O
network	O
and	O
uses	O
symbolic	O
differentiation	O
for	O
computing	O
the	O
gradients	O
needed	O
in	O
stochastic	B
gradient	B
descent	I
in	O
google	O
announced	O
an	O
open-source	O
version	O
of	O
their	O
tensorflow	O
software	O
for	O
fitting	O
deep	O
networks	O
support-vector	O
machines	O
and	O
kernel	O
methods	O
while	O
linear	B
logistic	B
regression	B
has	O
been	O
the	O
mainstay	O
in	O
biostatistics	O
and	O
epidemiology	O
it	O
has	O
had	O
a	O
mixed	O
reception	O
in	O
the	O
machine-learning	O
community	O
there	O
the	O
goal	O
is	O
often	O
classification	O
accuracy	B
rather	O
than	O
statistical	O
one	O
ifbpr	O
y	O
d	O
d	O
x	O
svms	O
bypass	O
the	O
first	O
step	O
and	O
build	O
a	O
inference	B
logistic	B
regression	B
builds	O
a	O
classifier	O
in	O
two	O
steps	O
fit	O
a	O
conditional	B
probability	O
model	O
for	O
pr	O
y	O
d	O
d	O
x	O
and	O
then	O
classify	O
as	O
a	O
classifier	O
directly	O
another	O
rather	O
awkward	O
issue	O
with	O
logistic	B
regression	B
is	O
that	O
it	O
fails	O
if	O
the	O
training	O
data	B
are	O
linearly	B
separable	I
what	O
this	O
means	O
is	O
that	O
in	O
the	O
feature	O
space	O
one	O
can	O
separate	O
the	O
two	O
classes	O
by	O
a	O
linear	B
boundary	O
in	O
cases	O
such	O
as	O
this	O
maximum	B
likelihood	B
fails	O
and	O
some	O
parameters	O
march	O
off	O
to	O
infinity	O
while	O
this	O
might	O
have	O
seemed	O
an	O
unlikely	O
scenario	O
to	O
the	O
early	O
users	O
of	O
logistic	B
regression	B
it	O
becomes	O
almost	O
a	O
certainty	O
with	O
modern	O
wide	O
genomics	O
data	B
when	O
p	O
n	O
features	O
than	O
observations	O
we	O
can	O
typically	O
always	O
find	O
a	O
separating	B
hyperplane	I
finding	O
an	O
optimal	B
separating	B
hyperplane	I
was	O
in	O
fact	O
the	O
launching	O
point	O
for	O
svms	O
as	O
we	O
will	O
see	O
they	O
have	O
more	O
than	O
this	O
to	O
offer	O
and	O
in	O
fact	O
live	O
comfortably	O
alongside	O
logistic	B
regression	B
svms	O
pursued	O
an	O
age-old	O
approach	O
in	O
statistics	B
of	O
enriching	O
the	O
feature	O
space	O
through	O
nonlinear	B
transformations	I
and	O
basis	O
expansions	O
a	O
classical	O
example	O
being	O
augmenting	O
a	O
linear	B
regression	B
with	O
interaction	O
terms	O
a	O
linear	B
model	I
in	O
the	O
enlarged	O
space	O
leads	O
to	O
a	O
nonlinear	B
model	O
in	O
the	O
ambient	O
space	O
this	O
is	O
typically	O
achieved	O
via	O
the	O
kernel	O
trick	B
which	O
allows	O
the	O
computations	B
to	O
be	O
performed	O
in	O
the	O
n-dimensional	O
space	O
for	O
an	O
arbitrary	O
number	O
of	O
predictors	B
p	O
as	O
the	O
field	O
matured	O
it	O
became	O
clear	O
that	O
in	O
fact	O
this	O
kernel	O
trick	B
amounted	O
to	O
estimation	B
in	O
a	O
reproducing-kernel	O
hilbert	O
space	O
finally	O
we	O
contrast	O
the	O
kernel	O
approach	O
in	O
svms	O
with	O
the	O
nonparame	O
teric	O
regression	B
techniques	O
known	O
as	O
kernel	B
smoothing	B
svms	O
and	O
kernel	O
methods	O
optimal	B
separating	B
hyperplane	I
figure	O
shows	O
a	O
small	O
sample	B
of	O
points	O
in	O
each	O
belonging	O
to	O
one	O
of	O
two	O
classes	O
or	O
orange	O
numerically	O
we	O
would	O
score	O
these	O
classes	O
as	O
y	O
d	O
for	O
say	O
blue	O
and	O
y	O
d	O
for	O
we	O
define	O
a	O
two-class	O
linear	B
classifier	O
via	O
a	O
function	B
f	O
d	O
c	O
x	O
with	O
the	O
convention	O
that	O
we	O
classify	O
a	O
point	O
as	O
if	O
f	O
and	O
as	O
if	O
f	O
the	O
fence	O
we	O
flip	O
a	O
coin	O
hence	O
the	O
classifier	O
itself	O
is	O
c	O
x	O
d	O
sign	O
f	O
the	O
deci	O
figure	O
left	O
panel	O
data	B
in	O
two	O
classes	O
in	O
three	O
potential	O
decision	O
boundaries	O
are	O
shown	O
each	O
separate	O
the	O
data	B
perfectly	O
right	O
panel	O
the	O
optimal	B
separating	B
hyperplane	I
line	O
in	O
creates	O
the	O
biggest	O
margin	B
between	O
the	O
two	O
classes	O
sion	O
boundary	O
is	O
the	O
set	B
fx	O
j	O
f	O
d	O
we	O
see	O
three	O
different	O
classifiers	O
in	O
the	O
left	O
panel	O
of	O
figure	O
and	O
they	O
all	O
classify	O
the	O
points	O
perfectly	O
the	O
optimal	B
separating	B
hyperplane	I
is	O
the	O
linear	B
classifier	O
that	O
creates	O
the	O
largest	O
margin	B
between	O
the	O
two	O
classes	O
and	O
is	O
shown	O
in	O
the	O
right	O
panel	O
is	O
also	O
known	O
as	O
an	O
optimal-margin	O
classifier	O
the	O
underlying	O
hope	O
is	O
that	O
by	O
making	O
a	O
big	O
margin	B
on	O
the	O
training	O
data	B
it	O
will	O
also	O
classify	O
future	O
observations	O
well	O
some	O
elementary	O
geometry	B
shows	O
that	O
the	O
euclidean	O
distance	O
from	O
a	O
point	O
to	O
the	O
linear	B
decision	O
boundary	O
defined	O
by	O
f	O
is	O
given	O
by	O
f	O
with	O
this	O
in	O
mind	O
for	O
a	O
separating	B
hyperplane	I
the	O
quantity	B
in	O
this	O
chapter	O
the	O
scoring	O
leads	O
to	O
convenient	O
notation	O
yi	O
f	O
is	O
optimal	B
separating	B
hyperplane	I
the	O
distance	O
of	O
xi	O
from	O
the	O
decision	O
this	O
leads	O
to	O
an	O
optimization	O
problem	O
for	O
creating	O
the	O
optimal	O
margin	B
classifier	O
maximize	O
m	O
subject	O
to	O
yi	O
c	O
x	O
m	O
i	O
d	O
n	O
a	O
rescaling	O
argument	B
reduces	O
this	O
to	O
the	O
simpler	O
form	B
k	O
minimize	O
subject	O
to	O
yi	O
c	O
x	O
i	O
d	O
n	O
this	O
is	O
a	O
quadratic	B
program	I
which	O
can	O
be	O
solved	O
by	O
standard	O
techniques	O
in	O
convex	B
optimization	I
one	O
noteworthy	O
property	O
of	O
the	O
solution	O
is	O
that	O
dx	O
o	O
s	O
o	O
i	O
xi	O
s	O
where	O
s	O
is	O
the	O
support	O
set	B
we	O
can	O
see	O
in	O
figure	O
that	O
the	O
margin	B
touches	O
three	O
points	O
in	O
this	O
case	O
there	O
are	O
j	O
j	O
d	O
support	O
vectors	O
and	O
clearly	O
the	O
orientation	O
of	O
o	O
is	O
determined	O
by	O
them	O
however	O
we	O
still	O
have	O
to	O
solve	O
the	O
optimization	O
problem	O
to	O
identify	O
the	O
three	O
points	O
i	O
in	O
s	O
and	O
their	O
coefficients	O
i	O
s	O
figure	O
shows	O
an	O
optimalmargin	O
classifier	O
fit	O
to	O
wide	B
data	B
that	O
is	O
data	B
where	O
p	O
n	O
these	O
are	O
gene-expression	O
measurements	O
on	O
p	O
d	O
genes	O
measured	O
on	O
blood	O
samples	O
from	O
n	O
d	O
leukemia	B
patients	O
seen	O
in	O
chapter	O
they	O
were	O
classified	O
into	O
two	O
classes	O
acute	O
lymphoblastic	O
leukemia	B
and	O
myeloid	O
leukemia	B
in	O
cases	O
like	O
this	O
we	O
are	O
typically	O
guaranteed	O
a	O
separating	O
in	O
this	O
case	O
of	O
the	O
points	O
are	O
support	O
points	O
one	O
might	O
be	O
justified	O
in	O
thinking	O
that	O
this	O
solution	O
is	O
overfit	O
to	O
this	O
small	O
amount	O
of	O
data	B
indeed	O
when	O
broken	O
into	O
a	O
training	O
and	O
test	O
set	B
we	O
see	O
that	O
the	O
test	O
data	B
encroaches	O
well	O
into	O
the	O
margin	B
region	B
but	O
in	O
this	O
case	O
none	O
are	O
misclassified	O
such	O
classifiers	O
are	O
very	O
popular	O
in	O
the	O
widedata	O
world	O
of	O
genomics	O
largely	O
because	O
they	O
seem	O
to	O
work	O
very	O
well	O
they	O
offer	O
a	O
simple	O
alternative	O
to	O
logistic	B
regression	B
in	O
a	O
situation	O
where	O
the	O
latter	O
fails	O
however	O
sometimes	O
the	O
solution	O
is	O
overfit	O
and	O
a	O
modification	O
is	O
called	O
for	O
this	O
same	O
modification	O
takes	O
care	O
of	O
nonseparable	O
situations	O
as	O
well	O
since	O
all	O
the	O
points	O
are	O
correctly	O
classified	O
the	O
sign	O
of	O
f	O
agrees	O
with	O
yi	O
hence	O
this	O
quantity	B
is	O
always	O
positive	O
if	O
n	O
p	O
c	O
we	O
can	O
always	O
find	O
a	O
separating	B
hyperplane	I
unless	O
there	O
are	O
exact	O
feature	O
ties	O
across	O
the	O
class	O
barrier	O
svms	O
and	O
kernel	O
methods	O
figure	O
left	O
panel	O
optimal	O
margin	B
classifier	O
fit	O
to	O
leukemia	B
data	B
there	O
are	O
observations	O
from	O
two	O
classes	O
all	O
and	O
aml	O
and	O
gene-expression	O
variables	O
of	O
the	O
observations	O
are	O
support	O
vectors	O
sitting	O
on	O
the	O
margin	B
the	O
points	O
are	O
plotted	O
against	O
their	O
fitted	O
classifier	O
function	B
o	O
component	O
of	O
the	O
data	B
for	O
display	O
purposes	O
since	O
it	O
has	O
low	O
correlation	O
with	O
the	O
former	O
right	O
panel	O
here	O
the	O
optimal	O
margin	B
classifier	O
was	O
fit	O
to	O
a	O
random	O
subset	O
of	O
of	O
the	O
observations	O
and	O
then	O
used	O
to	O
classify	O
the	O
remaining	O
in	O
color	O
although	O
these	O
points	O
fall	O
on	O
the	O
wrong	O
sides	O
of	O
their	O
respective	O
margins	O
they	O
are	O
all	O
correctly	O
classified	O
f	O
labeled	O
svm	B
projection	O
and	O
the	O
fifth	O
principal	O
soft-margin	O
classifier	O
figure	O
shows	O
data	B
in	O
that	O
are	O
not	O
separable	O
the	O
generalization	O
to	O
a	O
soft	O
margin	B
allows	O
points	O
to	O
violate	O
their	O
margin	B
each	O
of	O
the	O
violators	O
has	O
a	O
line	O
segment	O
connecting	O
it	O
to	O
its	O
margin	B
showing	O
the	O
extent	O
of	O
the	O
violation	O
the	O
soft-margin	O
classifier	O
solves	O
k	O
minimize	O
subject	O
to	O
yi	O
c	O
x	O
i	O
d	O
n	O
and	O
nx	O
i	O
b	O
here	O
b	O
is	O
the	O
budget	O
for	O
the	O
total	O
amount	O
of	O
overlap	O
once	O
again	O
the	O
solution	O
has	O
the	O
form	B
except	O
now	O
the	O
support	O
set	B
s	O
includes	O
any	O
vectors	O
on	O
the	O
margin	B
as	O
well	O
as	O
those	O
that	O
violate	O
the	O
margin	B
the	O
bigger	O
b	O
the	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
all	O
datasvm	O
projectionpca	O
projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
train	O
and	O
testsvm	O
projectionpca	O
projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
svm	B
criterion	O
as	O
loss	B
plus	I
penalty	B
figure	O
for	O
data	B
that	O
are	O
not	O
separable	O
such	O
as	O
here	O
the	O
soft-margin	O
classifier	O
allows	O
margin	B
violations	O
the	O
budget	O
b	O
for	O
the	O
total	O
measure	O
of	O
violation	O
becomes	O
a	O
tuning	O
parameter	O
the	O
bigger	O
the	O
budget	O
the	O
wider	O
the	O
soft	O
margin	B
and	O
the	O
more	O
support	O
points	O
there	O
are	O
involved	O
in	O
the	O
fit	O
bigger	O
the	O
support	O
set	B
and	O
hence	O
the	O
more	O
points	O
that	O
have	O
a	O
say	O
in	O
the	O
solution	O
hence	O
bigger	O
b	O
means	O
more	O
stability	O
and	O
lower	O
variance	B
in	O
fact	O
even	O
for	O
separable	O
data	B
allowing	O
margin	B
violations	O
via	O
b	O
lets	O
us	O
regularize	O
the	O
solution	O
by	O
tuning	O
b	O
svm	B
criterion	O
as	O
loss	B
plus	I
penalty	B
it	O
turns	O
out	O
that	O
one	O
can	O
reformulate	O
and	O
in	O
more	O
traditional	O
terms	O
as	O
the	O
minimization	O
of	O
a	O
loss	O
plus	O
a	O
penalty	B
nx	O
yi	O
c	O
x	O
i	O
c	O
c	O
minimize	O
here	O
the	O
hinge	B
loss	I
lh	O
f	O
d	O
yf	O
c	O
operates	O
on	O
the	O
margin	B
quantity	B
yf	O
and	O
is	O
piecewise	O
linear	B
as	O
in	O
figure	O
the	O
same	O
margin	B
quantity	B
came	O
up	O
in	O
boosting	B
in	O
section	O
the	O
quantity	B
yi	O
c	O
i	O
c	O
is	O
the	O
cost	O
for	O
xi	O
being	O
on	O
the	O
wrong	O
side	O
of	O
its	O
margin	B
cost	O
x	O
is	O
zero	O
if	O
it	O
s	O
on	O
the	O
correct	O
side	O
the	O
correspondence	O
between	O
and	O
is	O
exact	O
large	O
corresponds	O
to	O
large	O
b	O
and	O
this	O
formulation	O
makes	O
explicit	O
the	O
form	B
of	O
regularization	B
for	O
separable	O
data	B
the	O
optimal	B
separating	B
hyperplane	I
solution	O
corresponds	O
to	O
the	O
limiting	O
minimum-norm	O
solution	O
as	O
one	O
can	O
show	O
that	O
the	O
population	O
minimizer	O
of	O
the	O
svms	O
and	O
kernel	O
methods	O
figure	O
the	O
hinge	B
loss	I
penalizes	O
observation	O
margins	O
yf	O
less	O
than	O
linearly	O
and	O
is	O
indifferent	O
to	O
margins	O
greater	O
than	O
the	O
negative	O
binomial	B
log-likelihood	B
has	O
the	O
same	O
asymptotes	O
but	O
operates	O
in	O
a	O
smoother	O
fashion	O
near	O
the	O
elbow	O
at	O
yf	O
d	O
hinge	B
loss	I
is	O
in	O
fact	O
the	O
bayes	O
this	O
shows	O
that	O
the	O
svm	B
is	O
in	O
fact	O
directly	O
estimating	O
the	O
classifier	O
c	O
x	O
the	O
red	O
curve	O
in	O
figure	O
is	O
the	O
binomial	B
deviance	B
for	O
logistic	B
regression	B
f	O
d	O
c	O
x	O
is	O
now	O
modeling	O
logit	B
pr	O
y	O
d	O
d	O
x	O
with	O
y	O
d	O
the	O
deviance	B
can	O
also	O
be	O
written	O
in	O
terms	O
of	O
the	O
margin	B
nx	O
and	O
the	O
ridged	O
logistic	B
regression	B
corresponding	O
to	O
has	O
the	O
form	B
log	O
c	O
e	O
i	O
c	O
minimize	O
logistic	B
regression	B
is	O
discussed	O
in	O
section	O
as	O
well	O
as	O
sections	O
and	O
this	O
form	B
of	O
the	O
binomial	B
deviance	B
is	O
derived	O
in	O
on	O
page	O
these	O
loss	O
functions	O
have	O
some	O
features	O
in	O
common	O
as	O
can	O
be	O
seen	O
in	O
the	O
figure	O
the	O
binomial	B
loss	O
asymptotes	O
to	O
zero	O
for	O
large	O
positive	O
margins	O
and	O
to	O
a	O
linear	B
loss	O
for	O
large	O
negative	O
margins	O
matching	O
the	O
hinge	B
loss	I
in	O
this	O
regard	O
the	O
main	O
difference	O
is	O
that	O
the	O
hinge	O
has	O
a	O
sharp	O
elbow	O
at	O
while	O
the	O
binomial	B
bends	O
smoothly	O
a	O
consequence	O
of	O
this	O
is	O
that	O
the	O
binomial	B
solution	O
involves	O
all	O
the	O
data	B
via	O
weights	B
pi	O
pi	O
that	O
fade	O
smoothly	O
with	O
distance	O
from	O
the	O
decision	O
boundary	O
as	O
apposed	O
to	O
the	O
binary	O
nature	O
the	O
bayes	O
classifier	O
c	O
x	O
for	O
a	O
two-class	O
problem	O
using	O
equal	O
costs	O
for	O
misclassification	O
errors	B
assigns	O
x	O
to	O
the	O
class	O
for	O
which	O
pr	O
yjx	O
is	O
largest	O
computations	B
and	O
the	O
kernel	O
trick	B
d	O
of	O
support	O
points	O
also	O
as	O
seen	O
in	O
section	O
as	O
well	O
the	O
population	O
minimizer	O
of	O
the	O
binomial	B
deviance	B
is	O
the	O
logit	B
of	O
the	O
class	O
probability	O
d	O
log	O
pr	O
y	O
d	O
while	O
that	O
of	O
the	O
hinge	B
loss	I
is	O
its	O
sign	O
c	O
x	O
d	O
sign	O
interestingly	O
as	O
the	O
solution	O
direction	O
o	O
to	O
the	O
ridged	O
logistic	B
regression	B
problem	O
converges	O
to	O
that	O
of	O
the	O
svm	B
these	O
forms	O
immediately	O
suggest	O
other	O
generalizations	O
of	O
the	O
linear	B
svm	B
in	O
particular	O
we	O
can	O
replace	O
the	O
ridge	O
penalty	B
k	O
by	O
the	O
sparsityinducing	O
lasso	B
penalty	B
k	O
which	O
will	O
set	B
some	O
coefficients	O
to	O
zero	O
and	O
hence	O
perform	O
feature	O
selection	O
publicly	O
available	O
software	O
package	O
liblinear	B
in	O
r	B
is	O
available	O
for	O
fitting	O
such	O
lasso-regularized	O
supportvector	O
classifiers	O
computations	B
and	O
the	O
kernel	O
trick	B
the	O
form	B
of	O
the	O
solution	O
o	O
o	O
i	O
xi	O
for	O
the	O
optimal-	O
and	O
soft-margin	O
classifier	O
has	O
some	O
important	O
consequences	O
for	O
starters	O
we	O
can	O
write	O
the	O
fitted	O
function	B
evaluated	O
at	O
a	O
point	O
x	O
as	O
c	O
x	O
s	O
o	O
ihx	O
xii	O
dp	O
f	O
d	O
o	O
o	O
d	O
o	O
o	O
cx	O
s	O
where	O
we	O
have	O
deliberately	O
replaced	O
the	O
transpose	O
notation	O
with	O
the	O
more	O
suggestive	O
inner	O
product	O
furthermore	O
we	O
show	O
in	O
in	O
section	O
that	O
the	O
lagrange	B
dual	B
involves	O
the	O
data	B
only	O
through	O
the	O
pairwise	B
inner	I
products	I
hxi	O
xji	O
elements	O
of	O
the	O
n	O
gram	B
matrix	B
xx	O
this	O
means	O
that	O
the	O
computations	B
for	O
computing	O
the	O
svm	B
solution	O
scale	B
linearly	O
with	O
p	O
although	O
potentially	O
in	O
n	O
with	O
very	O
large	O
p	O
the	O
tens	O
of	O
thousands	O
and	O
even	O
millions	O
as	O
we	O
will	O
see	O
this	O
can	O
be	O
convenient	O
it	O
turns	O
out	O
that	O
all	O
ridge-regularized	O
linear	B
models	B
with	O
wide	B
data	B
can	O
be	O
reparametrized	O
in	O
this	O
way	O
take	O
ridge	B
regression	B
for	O
example	O
minimize	O
d	O
ky	O
x	O
x	O
c	O
c	O
this	O
has	O
solution	O
o	O
y	O
and	O
with	O
p	O
large	O
requires	O
inversion	O
of	O
a	O
p	O
p	O
matrix	B
however	O
it	O
can	O
be	O
shown	O
that	O
o	O
o	O
d	O
in	O
practice	O
and	O
with	O
modern	O
approximate	O
solutions	O
much	O
faster	O
than	O
that	O
d	O
x	O
c	O
svms	O
and	O
kernel	O
methods	O
has	O
the	O
same	O
form	B
as	O
for	O
the	O
svm	B
pn	O
o	O
i	O
xi	O
with	O
o	O
d	O
which	O
means	O
the	O
solution	O
can	O
be	O
obtained	O
in	O
rather	O
than	O
computations	B
again	O
the	O
gram	B
matrix	B
has	O
played	O
a	O
role	O
and	O
o	O
we	O
now	O
imagine	O
expanding	O
the	O
p-dimensional	O
feature	O
vector	B
x	O
into	O
a	O
potentially	O
much	O
larger	O
set	B
h	O
x	O
d	O
hm	O
x	O
for	O
an	O
example	O
to	O
latch	O
onto	O
think	O
polynomial	O
basis	O
of	O
total	O
degree	O
d	O
as	O
long	O
as	O
we	O
have	O
an	O
efficient	O
way	O
to	O
compute	O
the	O
inner	O
products	O
hh	O
x	O
h	O
xj	O
for	O
any	O
x	O
we	O
can	O
compute	O
the	O
svm	B
solution	O
in	O
this	O
enlarged	O
space	O
just	O
as	O
easily	O
as	O
in	O
the	O
original	O
it	O
turns	O
out	O
that	O
convenient	O
kernel	O
functions	O
exist	O
that	O
do	O
just	O
that	O
for	O
example	O
kd	O
z	O
d	O
c	O
hx	O
zid	O
creates	O
a	O
basis	B
expansion	I
hd	O
of	O
polynomials	O
of	O
total	O
degree	O
d	O
and	O
kd	O
z	O
d	O
hhd	O
hd	O
the	O
polynomial	O
kernels	O
are	O
mainly	O
useful	O
as	O
existence	O
proofs	O
in	O
practice	O
other	O
more	O
useful	O
kernels	O
are	O
used	O
probably	O
the	O
most	O
popular	O
is	O
the	O
radial	O
kernel	O
k	O
x	O
z	O
d	O
e	O
this	O
is	O
a	O
positive	O
definite	O
function	B
and	O
can	O
be	O
thought	O
of	O
as	O
computing	O
an	O
inner	O
product	O
in	O
some	O
feature	O
space	O
here	O
the	O
feature	O
space	O
is	O
in	O
principle	O
infinite-dimensional	O
but	O
of	O
course	O
effectively	O
now	O
one	O
can	O
think	O
of	O
the	O
representation	O
in	O
a	O
different	O
light	O
o	O
i	O
k	O
x	O
xi	O
f	O
d	O
o	O
cx	O
o	O
s	O
an	O
expansion	O
of	O
radial	O
basis	O
functions	O
each	O
centered	O
on	O
one	O
of	O
the	O
training	O
examples	O
figure	O
illustrates	O
such	O
an	O
expansion	O
in	O
using	O
such	O
nonlinear	B
kernels	O
expands	O
the	O
scope	O
of	O
svms	O
considerably	O
allowing	O
one	O
to	O
fit	O
classifiers	O
with	O
nonlinear	B
decision	O
boundaries	O
one	O
may	O
ask	O
what	O
objective	O
is	O
being	O
optimized	O
when	O
we	O
move	O
to	O
this	O
kernel	O
representation	O
this	O
is	O
covered	O
in	O
the	O
next	O
section	O
but	O
as	O
a	O
sneak	O
preview	O
we	O
present	O
the	O
criterion	O
nx	O
yj	O
c	O
nx	O
c	O
c	O
k	O
i	O
k	O
xj	O
xi	O
minimize	O
where	O
the	O
n	O
n	O
matrix	B
k	O
has	O
entries	O
k	O
xj	O
xi	O
as	O
an	O
illustrative	O
example	O
in	O
we	O
can	O
visualize	O
the	O
nonlinear	B
boundaries	O
we	O
generated	O
the	O
data	B
in	O
figure	O
we	O
show	O
two	O
svm	B
a	O
bivariate	O
function	B
k	O
x	O
z	O
rp	O
is	O
positive-definite	O
if	O
for	O
every	O
q	O
every	O
q	O
q	O
matrix	B
k	O
d	O
fk	O
xi	O
xj	O
formed	O
using	O
distinct	O
entries	O
xq	O
is	O
positive	O
definite	O
the	O
feature	O
space	O
is	O
defined	O
in	O
terms	O
of	O
the	O
eigen-functions	O
of	O
the	O
kernel	O
computations	B
and	O
the	O
kernel	O
trick	B
figure	O
radial	O
basis	O
functions	O
in	O
the	O
left	O
panel	O
shows	O
a	O
collection	O
of	O
radial	O
basis	O
functions	O
each	O
centered	O
on	O
one	O
of	O
the	O
seven	O
observations	O
the	O
right	O
panel	O
shows	O
a	O
function	B
obtained	O
from	O
a	O
particular	O
linear	B
expansion	O
of	O
these	O
basis	O
functions	O
solutions	O
both	O
using	O
a	O
radial	O
kernel	O
in	O
the	O
left	O
panel	O
some	O
margin	B
errors	B
are	O
committed	O
but	O
the	O
solution	O
looks	O
reasonable	O
however	O
with	O
the	O
flexibility	O
of	O
the	O
enlarged	O
feature	O
space	O
by	O
decreasing	O
the	O
budget	O
b	O
we	O
can	O
typically	O
overfit	O
the	O
training	O
data	B
as	O
is	O
the	O
case	O
in	O
the	O
right	O
panel	O
a	O
separate	O
little	O
blue	O
island	O
was	O
created	O
to	O
accommodate	O
the	O
one	O
blue	O
point	O
in	O
a	O
sea	O
of	O
brown	O
figure	O
simulated	O
data	B
in	O
two	O
classes	O
in	O
with	O
svm	B
classifiers	O
computed	O
using	O
the	O
radial	O
kernel	O
the	O
left	O
panel	O
uses	O
a	O
larger	O
value	O
of	O
b	O
than	O
the	O
right	O
the	O
solid	O
lines	O
are	O
the	O
decision	O
boundaries	O
in	O
the	O
original	O
space	O
boundaries	O
in	O
the	O
expanded	O
feature	O
space	O
the	O
dashed	O
lines	O
are	O
the	O
projected	O
margins	O
in	O
both	O
cases	O
jkxxjxx	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
svms	O
and	O
kernel	O
methods	O
function	B
fitting	O
using	O
kernels	O
the	O
analysis	B
in	O
the	O
previous	O
section	O
is	O
heuristic	O
replacing	O
inner	O
products	O
by	O
kernels	O
that	O
compute	O
inner	O
products	O
in	O
some	O
feature	O
space	O
indeed	O
this	O
is	O
how	O
kernels	O
were	O
first	O
introduced	O
in	O
the	O
svm	B
world	O
there	O
is	O
however	O
a	O
rich	O
literature	O
behind	O
such	O
approaches	O
which	O
goes	O
by	O
the	O
name	O
function	B
fitting	O
in	O
reproducing-kernel	O
hilbert	O
spaces	O
we	O
give	O
a	O
very	O
brief	O
overview	O
here	O
one	O
starts	O
with	O
a	O
bivariate	O
positive-definite	O
kernel	O
k	O
w	O
rp	O
rp	O
and	O
we	O
consider	O
a	O
space	O
hk	O
of	O
functions	O
f	O
w	O
rp	O
generated	O
by	O
the	O
kernel	O
f	O
z	O
z	O
the	O
kernel	O
also	O
induces	O
a	O
norm	O
on	O
the	O
space	O
kf	O
k	O
hk	O
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
roughness	O
measure	O
we	O
can	O
now	O
state	O
a	O
very	O
general	O
optimization	O
problem	O
for	O
fitting	O
a	O
func	O
tion	O
to	O
data	B
when	O
restricted	O
to	O
this	O
class	O
l	O
yi	O
c	O
f	O
c	O
hk	O
nx	O
minimize	O
f	O
hk	O
a	O
search	O
over	O
a	O
possibly	O
infinite-dimensional	O
function	B
space	O
here	O
l	O
is	O
an	O
arbitrary	O
loss	B
function	B
the	O
magic	O
of	O
these	O
spaces	O
in	O
the	O
context	O
of	O
this	O
problem	O
is	O
that	O
one	O
can	O
show	O
that	O
the	O
solution	O
is	O
finite-dimensional	O
f	O
d	O
nx	O
o	O
o	O
i	O
k	O
x	O
xi	O
a	O
linear	B
basis	B
expansion	I
with	O
basis	O
functions	O
ki	O
d	O
k	O
x	O
xi	O
anchored	O
at	O
each	O
of	O
the	O
observed	O
vectors	O
xi	O
in	O
the	O
training	O
data	B
moreover	O
using	O
the	O
reproducing	O
property	O
of	O
the	O
kernel	O
in	O
this	O
space	O
one	O
can	O
show	O
that	O
the	O
penalty	B
reduces	O
to	O
k	O
o	O
f	O
hk	O
here	O
k	O
is	O
the	O
n	O
gram	B
matrix	B
of	O
evaluations	O
of	O
the	O
kernel	O
equivalent	O
to	O
the	O
xx	O
matrix	B
for	O
the	O
linear	B
case	O
o	O
i	O
o	O
j	O
k	O
xi	O
xj	O
d	O
o	O
d	O
nx	O
nx	O
k	O
o	O
hence	O
the	O
abstract	O
problem	O
reduces	O
to	O
the	O
generalized	O
ridge	B
problem	I
minimize	O
nx	O
c	O
nx	O
l	O
c	O
k	O
i	O
k	O
xi	O
xj	O
here	O
kz	O
d	O
z	O
is	O
considered	O
a	O
function	B
of	O
the	O
first	O
argument	B
and	O
the	O
second	O
argument	B
is	O
a	O
parameter	O
example	O
string	O
kernels	O
for	O
protein	B
classification	I
indeed	O
if	O
l	O
is	O
the	O
hinge	B
loss	I
as	O
in	O
this	O
is	O
the	O
equivalent	O
loss	B
plus	I
penalty	B
criterion	O
being	O
fit	O
by	O
the	O
kernel	O
svm	B
alternatively	O
if	O
l	O
is	O
the	O
binomial	B
deviance	B
loss	O
as	O
in	O
this	O
would	O
fit	O
a	O
kernel	O
version	O
of	O
logistic	B
regression	B
hence	O
most	O
fitting	O
methods	O
can	O
be	O
generalized	O
to	O
accommodate	O
kernels	O
this	O
formalization	O
opens	O
the	O
door	O
to	O
a	O
wide	O
variety	O
of	O
applications	O
depending	O
on	O
the	O
kernel	O
function	B
used	O
alternatively	O
as	O
long	O
as	O
we	O
can	O
compute	O
suitable	O
similarities	O
between	O
objects	O
we	O
can	O
build	O
sophisticated	O
classifiers	O
and	O
other	O
models	B
for	O
making	O
predictions	O
about	O
other	O
attributes	O
of	O
the	O
in	O
the	O
next	O
section	O
we	O
consider	O
a	O
particular	O
example	O
example	O
string	O
kernels	O
for	O
protein	B
classification	I
one	O
of	O
the	O
important	O
problems	O
in	O
computational	O
biology	O
is	O
to	O
classify	O
proteins	O
into	O
functional	O
and	O
structural	O
classes	O
based	O
on	O
their	O
sequence	O
similarities	O
protein	O
molecules	O
can	O
be	O
thought	O
of	O
as	O
strings	O
of	O
amino	O
acids	O
and	O
differ	O
in	O
terms	O
of	O
length	O
and	O
composition	O
in	O
the	O
example	O
we	O
consider	O
the	O
lengths	O
vary	O
between	O
and	O
amino-acid	O
molecules	O
each	O
of	O
which	O
can	O
be	O
one	O
of	O
different	O
types	O
labeled	O
using	O
the	O
letters	O
of	O
the	O
alphabet	O
here	O
follow	O
two	O
protein	O
examples	O
and	O
of	O
length	O
and	O
respectively	O
iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv	O
erlfknlslikkyidgqkkkcgeerrrvnqfldylqeflgvmntewi	O
phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaerlqenlqayrtfhvlla	O
rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk	O
lwglkvlqelsqwtvrsihdlrfisshqtgip	O
we	O
treat	O
the	O
proteins	O
x	O
as	O
documents	O
consisting	O
of	O
letters	O
with	O
a	O
dictionary	O
of	O
size	O
our	O
feature	O
vector	B
hm	O
x	O
will	O
consist	O
of	O
the	O
counts	O
for	O
all	O
m-grams	O
in	O
the	O
protein	O
that	O
is	O
distinct	O
sequences	O
of	O
consecutive	O
letters	O
of	O
length	O
m	O
as	O
an	O
illustration	O
we	O
use	O
m	O
d	O
which	O
results	O
in	O
possible	O
sub-sequences	O
hence	O
will	O
be	O
a	O
vector	B
of	O
length	O
with	O
each	O
element	O
the	O
number	O
of	O
times	O
that	O
particular	O
sub-sequence	O
occurs	O
in	O
the	O
protein	O
x	O
in	O
our	O
example	O
the	O
sub-sequence	O
lqe	O
occurs	O
once	O
in	O
the	O
d	O
first	O
and	O
twice	O
in	O
the	O
second	O
protein	O
so	O
the	O
number	O
of	O
possible	O
sequences	O
of	O
length	O
m	O
is	O
which	O
can	O
be	O
very	O
d	O
and	O
as	O
long	O
as	O
the	O
similarities	O
behave	O
like	O
inner	O
products	O
i	O
e	O
they	O
form	B
positive	O
semi-definite	O
matrices	O
svms	O
and	O
kernel	O
methods	O
large	O
for	O
moderate	O
m	O
also	O
the	O
vast	O
majority	O
of	O
the	O
sub-sequences	O
do	O
not	O
match	O
the	O
strings	O
in	O
our	O
training	B
set	B
which	O
means	O
hm	O
x	O
will	O
be	O
sparse	O
it	O
turns	O
out	O
that	O
we	O
can	O
compute	O
the	O
inner	O
product	O
matrix	B
or	O
string	B
kernel	I
d	O
efficiently	O
using	O
tree	O
structures	O
without	O
actually	O
computing	O
the	O
individual	O
vectors	O
armed	O
with	O
the	O
kernel	O
we	O
figure	O
roc	O
curves	O
for	O
two	O
classifiers	O
fit	O
to	O
the	O
protein	O
data	B
the	O
roc	O
curves	O
were	O
computed	O
using	O
cross-validation	B
and	O
trace	O
the	O
trade-off	O
between	O
false-positive	O
and	O
true-positive	O
error	O
rates	O
as	O
the	O
classifier	O
threshold	O
is	O
varied	O
the	O
area	O
under	O
the	O
curve	O
summarizes	O
the	O
overall	O
performance	O
of	O
each	O
classifier	O
here	O
the	O
svm	B
is	O
slightly	O
superior	O
to	O
kernel	O
logistic	B
regression	B
can	O
now	O
use	O
it	O
to	O
fit	O
a	O
regularized	O
svm	B
or	O
logistic	B
regression	B
model	I
as	O
outlined	O
in	O
the	O
previous	O
section	O
the	O
data	B
consist	O
of	O
proteins	O
in	O
two	O
classes	O
negative	O
and	O
positive	O
we	O
fit	O
both	O
the	O
kernel	O
svm	B
and	O
kernel	O
logistic	B
regression	B
models	B
for	O
both	O
methods	O
cross-validation	B
suggested	O
a	O
very	O
small	O
value	O
for	O
figure	O
shows	O
the	O
roc	O
trade-off	O
curve	O
for	O
each	O
using	O
cross-validation	B
here	O
the	O
svm	B
outperforms	O
logistic	B
regression	B
protein	O
classificationfalse-positive	O
ratetrue-positive	O
svms	O
concluding	O
remarks	O
svms	O
concluding	O
remarks	O
svms	O
have	O
been	O
wildly	O
successful	O
and	O
are	O
one	O
of	O
the	O
must	O
have	O
tools	O
in	O
any	O
machine-learning	O
toolbox	O
they	O
have	O
been	O
extended	O
to	O
cover	O
many	O
different	O
scenarios	O
other	O
than	O
two-class	O
classification	O
with	O
some	O
awkwardness	O
in	O
cases	O
the	O
extension	O
to	O
nonlinear	B
function-fitting	O
via	O
kernels	O
the	O
machine	O
in	O
the	O
name	O
generated	O
a	O
mini	O
industry	O
kernels	O
are	O
parametrized	O
learned	O
from	O
data	B
with	O
special	O
problem-specific	O
structure	B
and	O
so	O
on	O
on	O
the	O
other	O
hand	O
we	O
know	O
that	O
fitting	O
high-dimensional	O
nonlinear	B
functions	O
is	O
intrinsically	O
difficult	O
curse	B
of	I
dimensionality	I
and	O
svms	O
are	O
not	O
immune	O
the	O
quadratic	O
penalty	B
implicit	O
in	O
kernel	O
methodology	O
means	O
all	O
features	O
are	O
included	O
in	O
the	O
model	O
and	O
hence	O
sparsity	B
is	O
generally	O
not	O
an	O
option	O
why	O
then	O
this	O
unbridled	O
enthusiasm	O
classifiers	O
are	O
far	O
less	O
sensitive	O
to	O
bias	B
variance	B
tradeoffs	O
and	O
svms	O
are	O
mostly	O
popular	O
for	O
their	O
classification	O
performance	O
the	O
ability	O
to	O
define	O
a	O
kernel	O
for	O
measuring	O
similarities	O
between	O
abstract	O
objects	O
and	O
then	O
train	O
a	O
classifier	O
is	O
a	O
novelty	O
added	O
by	O
these	O
approaches	O
that	O
was	O
missed	O
in	O
the	O
past	O
kernel	B
smoothing	B
and	O
local	B
regression	B
the	O
phrase	O
kernel	O
methodology	O
might	O
mean	O
something	O
a	O
little	O
different	O
to	O
statisticians	O
trained	O
in	O
the	O
period	O
kernel	B
smoothing	B
represents	O
a	O
broad	O
range	O
of	O
tools	O
for	O
performing	O
non-	O
and	O
semi-parametric	O
regression	B
figure	O
shows	O
a	O
gaussian	B
kernel	O
smooth	O
fit	O
to	O
some	O
artificial	O
data	B
fxi	O
yign	O
it	O
computes	O
at	O
each	O
point	O
a	O
weighted	O
average	O
of	O
the	O
y-values	O
of	O
neighboring	O
points	O
with	O
weights	B
given	O
by	O
the	O
height	O
of	O
the	O
kernel	O
in	O
its	O
simplest	O
form	B
this	O
estimate	B
can	O
be	O
written	O
as	O
f	O
d	O
nx	O
o	O
yi	O
xi	O
where	O
xi	O
represents	O
the	O
radial	O
kernel	O
with	O
width	O
parameter	O
notice	O
the	O
similarity	O
to	O
here	O
the	O
o	O
i	O
d	O
yi	O
and	O
the	O
complexity	O
of	O
the	O
model	O
is	O
controlled	O
by	O
despite	O
this	O
similarity	O
and	O
the	O
use	O
of	O
the	O
same	O
kernel	O
these	O
methodologies	O
are	O
rather	O
different	O
the	O
focus	O
here	O
is	O
on	O
local	O
estimation	B
and	O
the	O
kernel	O
does	O
the	O
localizing	O
expression	O
is	O
almost	O
a	O
weighted	O
average	O
almost	O
because	O
here	O
is	O
the	O
normalized	O
gaussian	B
density	B
with	O
mean	O
and	O
variance	B
svms	O
and	O
kernel	O
methods	O
figure	O
a	O
gaussian	B
kernel	O
smooth	O
of	O
simulated	O
data	B
the	O
points	O
come	O
from	O
the	O
blue	O
curve	O
with	O
added	O
random	O
errors	B
the	O
kernel	O
smoother	O
fits	O
a	O
weighted	O
mean	O
of	O
the	O
observations	O
with	O
the	O
weighting	O
kernel	O
centered	O
at	O
the	O
target	O
point	O
in	O
this	O
case	O
the	O
points	O
shaded	O
orange	O
contribute	O
to	O
the	O
fit	O
at	O
as	O
moves	O
across	O
the	O
domain	O
the	O
smoother	O
traces	O
out	O
the	O
green	O
curve	O
the	O
width	O
of	O
the	O
kernel	O
is	O
a	O
tuning	O
parameter	O
we	O
have	O
depicted	O
the	O
gaussian	B
weighting	O
kernel	O
in	O
this	O
figure	O
for	O
illustration	O
in	O
fact	O
its	O
vertical	O
coordinates	O
are	O
all	O
positive	O
and	O
integrate	O
to	O
one	O
pn	O
xi	O
in	O
fact	O
the	O
nadaraya	O
watson	O
estimator	B
is	O
more	O
explicit	O
fn	O
w	O
dpn	O
pn	O
o	O
yi	O
xi	O
xi	O
although	O
figure	O
is	O
one-dimensional	O
the	O
same	O
formulation	O
applies	O
to	O
x	O
in	O
higher	O
dimensions	O
weighting	O
kernels	O
other	O
than	O
the	O
gaussian	B
are	O
typically	O
favored	O
in	O
par	O
ticular	O
near-neighbor	O
kernels	O
with	O
compact	O
support	O
for	O
example	O
the	O
tricube	B
kernel	I
used	O
by	O
the	O
lowess	B
smoother	O
in	O
r	B
is	O
defined	O
as	O
follows	O
define	O
di	O
d	O
i	O
d	O
n	O
and	O
let	O
d	O
m	O
be	O
the	O
mth	O
smallest	O
distance	O
of	O
the	O
mth	O
nearest	O
neighbor	O
to	O
let	O
ui	O
d	O
di	O
i	O
d	O
n	O
kernel	B
smoothing	B
and	O
local	B
regression	B
the	O
tricube	B
kernel	I
is	O
given	O
by	O
xi	O
d	O
i	O
if	O
ui	O
otherwise	O
where	O
s	O
d	O
mn	O
the	O
span	O
of	O
the	O
kernel	O
near-neighbor	O
kernels	O
such	O
as	O
this	O
adapt	O
naturally	O
to	O
the	O
local	O
density	B
of	O
the	O
xi	O
wider	O
in	O
low-density	O
regions	O
narrower	O
in	O
high-density	O
regions	O
a	O
tricube	B
kernel	I
is	O
illustrated	O
in	O
figure	O
figure	O
local	B
regression	B
fit	O
to	O
the	O
simulated	O
data	B
at	O
each	O
point	O
we	O
fit	O
a	O
locally	O
weighted	O
linear	B
least-squares	O
model	O
and	O
use	O
the	O
fitted	O
value	O
to	O
estimate	B
o	O
here	O
we	O
use	O
the	O
tricube	B
kernel	I
with	O
a	O
span	O
of	O
the	O
orange	O
points	O
are	O
in	O
the	O
weighting	O
neighborhood	O
and	O
we	O
see	O
the	O
orange	O
linear	B
fit	O
computed	O
by	O
kernel	O
weighted	B
least	B
squares	I
the	O
green	O
dot	O
is	O
the	O
fitted	O
value	O
at	O
from	O
this	O
local	O
linear	B
fit	O
weighted	O
means	O
suffer	O
from	O
boundary	O
bias	B
we	O
can	O
see	O
in	O
figure	O
that	O
the	O
estimate	B
appears	O
biased	O
upwards	O
at	O
both	O
boundaries	O
the	O
reason	O
is	O
that	O
for	O
example	O
on	O
the	O
left	O
the	O
estimate	B
for	O
the	O
function	B
on	O
the	O
boundary	O
averages	O
points	O
always	O
to	O
the	O
right	O
and	O
since	O
the	O
function	B
is	O
locally	O
increasing	O
there	O
is	O
an	O
upward	O
bias	B
local	O
linear	B
regression	B
is	O
a	O
natural	O
generalization	O
that	O
fixes	O
such	O
problems	O
at	O
each	O
point	O
we	O
solve	O
the	O
following	O
weighted	O
regression	B
svms	O
and	O
kernel	O
methods	O
least-squares	O
problem	O
o	O
nx	O
o	O
d	O
arg	O
min	O
o	O
c	O
one	O
can	O
show	O
that	O
to	O
first	O
order	O
xi	O
xi	O
d	O
o	O
then	O
o	O
o	O
removes	O
the	O
boundary	O
bias	B
exactly	O
figure	O
illustrates	O
the	O
procedure	O
on	O
our	O
simulated	O
data	B
using	O
the	O
tricube	B
kernel	I
with	O
a	O
span	O
of	O
of	O
the	O
data	B
in	O
practice	O
the	O
width	O
of	O
the	O
kernel	O
span	O
here	O
has	O
to	O
be	O
selected	O
by	O
some	O
means	O
typically	O
we	O
use	O
cross-validation	B
local	B
regression	B
works	O
in	O
any	O
dimension	O
that	O
is	O
we	O
can	O
fit	O
two-	O
or	O
higher-dimensional	O
surfaces	O
using	O
exactly	O
the	O
same	O
technique	O
here	O
the	O
ability	O
to	O
remove	O
boundary	O
bias	B
really	O
pays	O
off	O
since	O
the	O
boundaries	O
can	O
be	O
complex	O
these	O
are	O
referred	O
to	O
as	O
memory-based	B
methods	I
since	O
there	O
is	O
no	O
fitted	O
model	O
we	O
have	O
to	O
save	O
all	O
the	O
training	O
data	B
and	O
recompute	O
the	O
local	O
fit	O
every	O
time	O
we	O
make	O
a	O
prediction	O
like	O
kernel	O
svms	O
and	O
their	O
relatives	O
kernel	B
smoothing	B
and	O
local	B
regression	B
break	O
down	O
in	O
high	O
dimensions	O
here	O
the	O
near	O
neighborhoods	O
become	O
so	O
wide	O
that	O
they	O
are	O
no	O
longer	O
local	O
notes	O
and	O
details	O
in	O
the	O
late	O
and	O
early	O
machine-learning	O
research	O
was	O
largely	O
driven	O
by	O
prediction	O
problems	O
and	O
the	O
neural-network	O
community	O
at	O
att	O
bell	O
laboratories	O
was	O
amongst	O
the	O
leaders	O
the	O
problem	O
of	O
the	O
day	O
was	O
the	O
us	O
post-office	O
handwritten	O
zip-code	O
ocr	O
challenge	O
a	O
image	O
classification	O
problem	O
vladimir	O
vapnik	B
was	O
part	O
of	O
this	O
team	O
and	O
along	O
with	O
colleagues	O
invented	O
a	O
more	O
direct	O
approach	O
to	O
classification	O
the	O
support-vector	O
machine	O
this	O
started	O
with	O
the	O
seminal	O
paper	O
by	O
boser	O
et	O
al	O
which	O
introduced	O
the	O
optimal	O
margin	B
classifier	O
separating	B
hyperplane	I
see	O
also	O
vapnik	B
the	O
ideas	O
took	O
off	O
quite	O
rapidly	O
attracting	O
a	O
large	O
cohort	O
of	O
researchers	O
and	O
evolved	O
into	O
the	O
more	O
general	O
class	O
of	O
kernel	O
methods	O
that	O
is	O
models	B
framed	O
in	O
reproducing-kernel	O
hilbert	O
spaces	O
a	O
good	O
general	O
reference	O
is	O
sch	O
olkopf	O
and	O
smola	O
x	O
c	O
define	O
a	O
linear	B
decision	O
boundary	O
fx	O
j	O
f	O
d	O
in	O
rp	O
affine	O
set	B
of	O
codimension	O
one	O
the	O
unit	O
vector	B
normal	B
to	O
the	O
boundary	O
is	O
where	O
k	O
denotes	O
the	O
or	O
euclidean	O
norm	O
how	O
should	O
one	O
compute	O
the	O
distance	O
from	O
a	O
point	O
x	O
to	O
this	O
boundary	O
if	O
is	O
any	O
point	O
on	O
the	O
boundary	O
geometry	B
of	O
separating	O
hyperplanes	O
let	O
f	O
d	O
notes	O
and	O
details	O
f	O
d	O
we	O
can	O
project	O
x	O
onto	O
the	O
normal	B
giving	O
us	O
k	O
d	O
f	O
ing	O
to	O
can	O
be	O
written	O
as	O
c	O
nx	O
as	O
claimed	O
in	O
note	O
that	O
this	O
is	O
the	O
signed	O
distance	O
since	O
f	O
will	O
be	O
positive	O
or	O
negative	O
depending	O
on	O
what	O
side	O
of	O
the	O
boundary	O
it	O
lies	O
on	O
the	O
support	O
in	O
svm	B
the	O
lagrange	B
primal	I
problem	O
correspond	O
minimize	O
yi	O
xi	O
andpn	O
nx	O
yi	O
c	O
x	O
i	O
dpn	O
where	O
are	O
the	O
lagrange	O
multipliers	O
on	O
differentiating	O
we	O
find	O
that	O
yi	O
d	O
with	O
i	O
d	O
yi	O
we	O
get	O
and	O
note	O
that	O
the	O
positivity	O
constraint	O
on	O
will	O
lead	O
to	O
some	O
of	O
the	O
i	O
being	O
nx	O
zero	O
plugging	O
into	O
we	O
obtain	O
the	O
lagrange	B
dual	B
problem	O
nx	O
maximize	O
yi	O
yj	O
x	O
subject	O
to	O
yi	O
d	O
i	O
xj	O
nx	O
the	O
svm	B
loss	B
function	B
the	O
constraint	O
in	O
can	O
be	O
succinctly	O
captured	O
via	O
the	O
expression	O
nx	O
yi	O
c	O
x	O
i	O
c	O
b	O
we	O
only	O
require	O
a	O
if	O
our	O
margin	B
is	O
less	O
than	O
and	O
we	O
get	O
charged	O
for	O
the	O
sum	O
of	O
these	O
we	O
now	O
use	O
a	O
lagrange	O
multiplier	B
to	O
enforce	O
the	O
constraint	O
leading	O
to	O
c	O
nx	O
yi	O
c	O
x	O
minimize	O
k	O
i	O
c	O
multiplying	O
by	O
d	O
gives	O
us	O
the	O
svm	B
estimates	O
a	O
classifier	O
the	O
following	O
derivation	O
is	O
due	O
to	O
wahba	O
et	O
al	O
consider	O
ey	O
jxdx	O
f	O
yf	O
cg	O
minimize	O
f	O
svms	O
and	O
kernel	O
methods	O
dropping	O
the	O
dependence	O
on	O
x	O
the	O
objective	O
can	O
be	O
written	O
as	O
pc	O
f	O
cc	O
f	O
c	O
where	O
pc	O
d	O
pr	O
y	O
d	O
d	O
x	O
and	O
d	O
pr	O
y	O
d	O
d	O
x	O
d	O
pc	O
from	O
this	O
we	O
see	O
that	O
f	O
if	O
pc	O
if	O
o	O
svm	B
and	O
ridged	O
logistic	B
regression	B
rosset	O
et	O
al	O
show	O
that	O
the	O
limiting	O
solution	O
as	O
to	O
for	O
separable	O
data	B
coincides	O
with	O
that	O
of	O
the	O
svm	B
in	O
the	O
sense	O
that	O
o	O
converges	O
to	O
the	O
same	O
quantity	B
for	O
the	O
svm	B
however	O
because	O
of	O
the	O
required	O
normalization	O
for	O
logistic	B
regression	B
the	O
svm	B
solution	O
is	O
preferable	O
on	O
the	O
other	O
hand	O
for	O
overlapped	O
situations	O
the	O
logistic-regression	O
solution	O
has	O
some	O
advantages	O
since	O
its	O
target	O
is	O
the	O
logit	B
of	O
the	O
class	O
probabilities	B
the	O
kernel	O
trick	B
the	O
trick	B
here	O
is	O
to	O
observe	O
that	O
from	O
the	O
score	O
x	O
c	O
d	O
which	O
means	O
we	O
can	O
write	O
equations	O
we	O
have	O
o	O
d	O
x	O
for	O
some	O
we	O
now	O
plug	O
this	O
into	O
the	O
score	O
equations	O
and	O
some	O
simple	O
manipulation	O
gives	O
the	O
result	O
a	O
similar	O
result	O
holds	O
for	O
ridged	O
logistic	B
regression	B
and	O
in	O
fact	O
any	O
linear	B
model	I
with	O
a	O
ridge	O
penalty	B
on	O
the	O
coefficients	O
and	O
tibshirani	O
polynomial	O
kernels	O
consider	O
z	O
d	O
c	O
hx	O
for	O
x	O
z	O
in	O
expanding	O
we	O
get	O
z	O
d	O
c	O
c	O
c	O
c	O
c	O
this	O
corresponds	O
to	O
with	O
p	O
p	O
p	O
d	O
the	O
same	O
is	O
true	O
for	O
p	O
and	O
for	O
degree	O
d	O
reproducing	O
kernel	O
hilbert	O
spaces	O
suppose	O
k	O
has	O
eigen	O
then	O
sion	O
k	O
x	O
z	O
with	O
hk	O
if	O
f	O
we	O
say	O
f	O
ci	O
with	O
i	O
kf	O
hk	O
often	O
kf	O
k	O
hk	O
behaves	O
like	O
a	O
roughness	O
penalty	B
in	O
that	O
it	O
penalizes	O
unlikely	O
members	O
in	O
the	O
span	O
of	O
z	O
that	O
these	O
correspond	O
to	O
rough	O
functions	O
if	O
f	O
has	O
some	O
high	O
loadings	O
cj	O
on	O
functions	O
with	O
small	O
eigenvalues	O
not	O
prominent	O
members	O
of	O
the	O
span	O
the	O
norm	O
becomes	O
large	O
smoothing	B
splines	O
and	O
their	O
generalizations	O
correspond	O
to	O
function	B
fitting	O
in	O
a	O
rkhs	O
notes	O
and	O
details	O
this	O
methodology	O
and	O
the	O
data	B
we	O
use	O
in	O
our	O
example	O
come	O
from	O
leslie	O
et	O
al	O
local	B
regression	B
and	O
bias	B
reduction	O
by	O
expanding	O
the	O
unknown	O
true	O
f	O
in	O
a	O
first-order	O
taylor	O
expansion	O
about	O
the	O
target	O
point	O
one	O
can	O
show	O
that	O
e	O
o	O
f	O
and	O
loader	O
inference	B
after	B
model	B
selection	I
the	O
classical	O
theory	B
of	O
model	B
selection	I
focused	O
on	O
f	B
tests	I
performed	O
within	O
gaussian	B
regression	B
models	B
inference	B
after	B
model	B
selection	I
instance	O
assessing	O
the	O
accuracy	B
of	O
a	O
fitted	O
regression	B
curve	O
was	O
typically	O
done	O
ignoring	O
the	O
model	B
selection	I
process	O
this	O
was	O
a	O
matter	O
of	O
necessity	O
the	O
combination	O
of	O
discrete	O
model	B
selection	I
and	O
continuous	O
regression	B
analysis	B
was	O
too	O
awkward	O
for	O
simple	O
mathematical	O
description	O
electronic	O
computation	O
has	O
opened	O
the	O
door	O
to	O
a	O
more	O
honest	O
analysis	B
of	O
estimation	B
accuracy	B
one	O
that	O
takes	O
account	O
of	O
the	O
variability	O
induced	O
by	O
data-based	O
model	B
selection	I
figure	O
displays	O
the	O
cholesterol	B
data	B
an	O
example	O
we	O
will	O
use	O
for	O
illustration	O
in	O
what	O
follows	O
cholestyramine	O
a	O
proposed	O
cholesterollowering	O
drug	O
was	O
administered	O
to	O
n	O
d	O
men	O
for	O
an	O
average	O
of	O
seven	O
years	O
each	O
the	O
response	B
variable	O
di	O
was	O
the	O
ith	O
man	O
s	O
decrease	O
in	O
cholesterol	B
level	O
over	O
the	O
course	O
of	O
the	O
experiment	O
also	O
measured	O
was	O
ci	O
his	O
compliance	B
or	O
the	O
proportion	B
of	O
the	O
intended	O
dose	O
actually	O
taken	O
ranging	O
from	O
for	O
perfect	O
compliers	O
to	O
zero	O
for	O
the	O
four	O
men	O
who	O
took	O
none	O
at	O
all	O
here	O
the	O
ci	O
values	O
have	O
been	O
transformed	O
to	O
approximately	O
follow	O
a	O
standard	O
normal	B
distribution	B
ci	O
n	O
we	O
wish	O
to	O
predict	O
cholesterol	B
decrease	O
from	O
compliance	B
polynomial	O
regression	B
models	B
with	O
di	O
a	O
j	O
th-order	O
polynomial	O
in	O
ci	O
were	O
considered	O
for	O
degrees	O
j	O
d	O
or	O
the	O
cp	B
criterion	O
was	O
applied	O
and	O
selected	O
a	O
cubic	O
model	O
j	O
d	O
as	O
best	O
the	O
curve	O
in	O
figure	O
is	O
the	O
ols	O
least	B
squares	I
cubic	O
regression	B
curve	O
fit	O
to	O
the	O
cholesterol	B
data	B
set	B
f	O
ci	O
di	O
i	O
d	O
we	O
are	O
interested	O
in	O
answering	O
the	O
following	O
question	O
how	O
accurate	O
is	O
the	O
simultaneous	O
confidence	O
intervals	B
figure	O
cholesterol	B
data	B
cholesterol	B
decrease	O
plotted	O
versus	O
adjusted	B
compliance	B
for	O
men	O
taking	O
cholestyramine	O
the	O
green	O
curve	O
is	O
ols	O
cubic	O
regression	B
with	O
cubic	O
selected	O
by	O
the	O
cp	B
criterion	O
how	O
accurate	O
is	O
the	O
fitted	O
curve	O
fitted	O
curve	O
taking	O
account	O
of	O
cp	B
selection	O
as	O
well	O
as	O
ols	O
estimation	B
section	O
for	O
an	O
answer	O
currently	O
there	O
is	O
no	O
overarching	O
theory	B
for	O
inference	B
after	B
model	B
selection	I
this	O
chapter	O
more	O
modestly	O
presents	O
a	O
short	O
series	O
of	O
vignettes	O
that	O
illustrate	O
promising	O
analyses	O
of	O
individual	O
situations	O
see	O
also	O
section	O
for	O
a	O
brief	O
report	O
on	O
progress	O
in	O
post-selection	B
inference	B
for	O
the	O
lasso	B
simultaneous	O
confidence	O
intervals	B
in	O
the	O
early	O
just	O
before	O
the	O
beginnings	O
of	O
the	O
computer	O
revolution	O
substantial	O
progress	O
was	O
made	O
on	O
the	O
problem	O
of	O
setting	O
simultaneous	O
confidence	O
intervals	B
simultaneous	O
here	O
means	O
that	O
there	O
exists	O
a	O
catalog	O
of	O
parameters	O
of	O
possible	O
interest	O
c	O
d	O
and	O
we	O
wish	O
to	O
set	B
a	O
confidence	O
interval	B
for	O
each	O
of	O
them	O
with	O
some	O
fixed	O
probability	O
typically	O
that	O
all	O
of	O
the	O
intervals	B
will	O
contain	O
their	O
respective	O
parameters	O
compliancecholesterol	O
decrease	O
inference	B
after	B
model	B
selection	I
as	O
a	O
first	O
example	O
we	O
return	O
to	O
the	O
diabetes	B
data	B
of	O
section	O
n	O
d	O
diabetes	B
patients	O
each	O
have	O
had	O
p	O
d	O
medical	O
variables	O
measured	O
at	O
baseline	O
with	O
the	O
goal	O
of	O
predicting	O
prog	O
disease	O
progression	O
one	O
year	O
later	O
let	O
x	O
be	O
the	O
matrix	B
with	O
ith	O
row	O
x	O
i	O
the	O
measurements	O
for	O
patient	O
i	O
x	O
has	O
been	O
standardized	O
so	O
that	O
each	O
of	O
its	O
columns	O
has	O
mean	O
and	O
sum	O
of	O
squares	O
also	O
let	O
y	O
be	O
the	O
of	O
centered	O
prog	O
measurements	O
is	O
subtracting	O
off	O
the	O
mean	O
of	O
the	O
prog	O
values	O
ordinary	O
least	B
squares	I
applied	O
to	O
the	O
normal	B
linear	B
model	I
y	O
nn	O
x	O
o	O
d	O
y	O
x	O
o	O
np	O
v	O
d	O
x	O
yields	O
mle	B
satisfying	O
as	O
at	O
nent	O
of	O
is	O
the	O
student-t	O
confidence	O
interval	B
for	O
j	O
the	O
j	O
th	O
compo	O
o	O
j	O
v	O
jj	O
t	B
q	O
where	O
d	O
is	O
the	O
usual	O
unbiased	O
estimate	B
of	O
d	O
ky	O
x	O
o	O
q	O
d	O
n	O
p	O
d	O
q	O
d	O
is	O
the	O
quantile	O
of	O
a	O
student-t	O
distribution	B
with	O
q	O
and	O
t	B
degrees	B
of	I
freedom	I
the	O
catalog	O
c	O
in	O
is	O
now	O
f	O
the	O
individual	O
intervals	B
shown	O
in	O
table	O
each	O
have	O
coverage	B
but	O
they	O
are	O
not	O
simultaneous	O
there	O
is	O
a	O
greater	O
than	O
chance	O
that	O
at	O
least	O
one	O
of	O
the	O
j	O
values	O
lies	O
outside	O
its	O
claimed	O
interval	B
valid	O
simultaneous	O
intervals	B
for	O
the	O
parameters	O
appear	O
on	O
the	O
right	O
side	O
of	O
table	O
these	O
are	O
the	O
scheff	O
e	O
intervals	B
o	O
j	O
v	O
jj	O
k	O
pq	O
pq	O
equals	O
for	O
p	O
d	O
q	O
d	O
discussed	O
next	O
the	O
crucial	O
constant	O
k	O
and	O
d	O
that	O
makes	O
the	O
scheff	O
e	O
intervals	B
wider	O
than	O
the	O
t	B
intervals	B
by	O
a	O
factor	B
of	O
one	O
expects	O
to	O
pay	O
an	O
extra	O
price	O
for	O
simultaneous	O
coverage	B
but	O
a	O
factor	B
greater	O
than	O
two	O
induces	O
sticker	O
shock	O
scheff	O
e	O
s	O
method	B
depends	O
on	O
the	O
pivotal	O
quantity	B
q	O
o	O
o	O
v	O
simultaneous	O
confidence	O
intervals	B
table	O
maximum	B
likelihood	B
estimates	O
o	O
variables	O
separate	O
student-t	O
confidence	O
limits	O
also	O
simultaneous	O
scheff	O
e	O
intervals	B
the	O
scheff	O
e	O
intervals	B
are	O
wider	O
by	O
a	O
factor	B
of	O
for	O
diabetes	B
predictor	B
student-t	O
scheff	O
e	O
o	O
lower	O
upper	O
lower	O
upper	O
age	O
sex	O
bmi	O
map	B
tc	O
ldl	O
hdl	O
tch	O
ltg	O
glu	O
which	O
under	O
model	O
has	O
a	O
scaled	O
f	B
distribution	B
pq	O
is	O
the	O
th	O
quantile	O
of	O
a	O
pfpq	O
distribution	B
then	O
prfq	O
k	O
if	O
k	O
yields	O
q	O
pfpq	O
o	O
v	O
o	O
pr	O
d	O
k	O
pq	O
pq	O
g	O
d	O
for	O
any	O
choice	O
of	O
and	O
in	O
model	O
having	O
observed	O
o	O
and	O
defines	O
an	O
elliptical	O
confidence	O
region	B
e	O
for	O
the	O
parameter	O
vector	B
suppose	O
we	O
are	O
interested	O
in	O
a	O
particular	O
linear	B
combination	O
of	O
the	O
coor	O
dinates	O
of	O
say	O
c	O
d	O
c	O
fpq	O
is	O
distributed	O
as	O
qq	O
the	O
two	O
chi-squared	O
variates	O
being	O
independent	O
calculating	O
the	O
percentiles	O
of	O
fpq	O
was	O
a	O
major	O
project	O
of	O
the	O
pre-war	O
period	O
inference	B
after	B
model	B
selection	I
figure	O
ellipsoid	B
of	O
possible	O
vectors	O
defined	O
by	O
determines	O
confidence	O
intervals	B
for	O
c	O
d	O
c	O
according	O
to	O
the	O
bounding	B
hyperplane	I
construction	O
illustrated	O
the	O
red	O
line	O
shows	O
the	O
confidence	O
interval	B
for	O
c	O
if	O
c	O
is	O
a	O
unit	O
vector	B
v	O
c	O
d	O
c	O
where	O
c	O
is	O
a	O
fixed	O
p-dimensional	O
vector	B
if	O
exists	O
in	O
e	O
then	O
we	O
must	O
have	O
which	O
turns	O
out	O
to	O
be	O
the	O
interval	B
centered	O
at	O
o	O
max	O
e	O
min	O
e	O
o	O
c	O
c	O
o	O
c	O
c	O
d	O
c	O
v	O
pq	O
agrees	O
with	O
where	O
c	O
is	O
the	O
j	O
th	O
coordinate	O
vector	B
the	O
construction	O
is	O
illustrated	O
in	O
figure	O
np	O
independently	O
of	O
theorem	B
e	O
then	O
with	O
probability	O
the	O
confidence	O
statement	O
for	O
c	O
d	O
c	O
be	O
simultaneously	O
true	O
for	O
all	O
choices	O
of	O
the	O
vector	B
c	O
qq	O
will	O
if	O
o	O
here	O
we	O
can	O
think	O
of	O
model	B
selection	I
as	O
the	O
choice	O
of	O
the	O
linear	B
combination	O
of	O
interest	O
d	O
c	O
scheff	O
e	O
s	O
theorem	B
allows	O
data	B
snooping	I
the	O
statistician	O
can	O
examine	O
the	O
data	B
and	O
then	O
choose	O
which	O
many	O
s	O
to	O
estimate	B
without	O
invalidating	O
the	O
resulting	O
confidence	O
intervals	B
an	O
important	O
application	O
has	O
the	O
o	O
j	O
s	O
as	O
independent	O
estimates	O
of	O
efficacy	O
for	O
competing	O
treatments	O
perhaps	O
different	O
experimental	O
drugs	O
for	O
the	O
same	O
target	O
disease	O
o	O
j	O
n	O
j	O
for	O
j	O
d	O
j	O
lcb	O
simultaneous	O
confidence	O
intervals	B
the	O
nj	O
being	O
known	O
sample	B
sizes	O
in	O
this	O
case	O
the	O
catalog	O
c	O
might	O
comprise	O
all	O
pairwise	O
differences	O
i	O
j	O
as	O
the	O
statistician	O
tries	O
to	O
determine	O
which	O
treatments	O
are	O
better	O
or	O
worse	O
than	O
the	O
others	O
the	O
fact	O
that	O
scheff	O
e	O
s	O
limits	O
apply	O
to	O
all	O
possible	O
linear	B
combinations	O
is	O
a	O
blessing	O
and	O
a	O
curse	O
the	O
curse	O
being	O
their	O
very	O
large	O
width	O
as	O
seen	O
c	O
in	O
table	O
narrower	O
simultaneous	O
limits	O
are	O
possible	O
if	O
we	O
restrict	O
the	O
catalog	O
c	O
for	O
instance	O
to	O
just	O
the	O
pairwise	O
differences	O
i	O
j	O
a	O
serious	O
objection	O
along	O
fisherian	O
lines	O
is	O
that	O
the	O
scheff	O
e	O
confidence	O
limits	O
are	O
accurate	O
without	O
being	O
correct	O
that	O
is	O
the	O
intervals	B
have	O
the	O
claimed	O
overall	O
frequentist	B
coverage	B
probability	O
but	O
may	O
be	O
misleading	O
when	O
applied	O
to	O
individual	O
cases	O
suppose	O
for	O
instance	O
that	O
d	O
for	O
j	O
d	O
j	O
in	O
and	O
that	O
we	O
observe	O
o	O
d	O
with	O
j	O
o	O
jj	O
for	O
all	O
the	O
others	O
even	O
if	O
we	O
looked	O
at	O
the	O
data	B
before	O
singling	O
out	O
o	O
for	O
attention	O
the	O
usual	O
student-t	O
interval	B
seems	O
more	O
appropriate	O
than	O
its	O
much	O
longer	O
scheff	O
e	O
version	O
this	O
point	O
is	O
made	O
more	O
convincingly	O
in	O
our	O
next	O
vignette	O
a	O
familiar	O
but	O
pernicious	O
abuse	O
of	O
model	B
selection	I
concerns	O
multiple	O
hypothesis	B
testing	B
suppose	O
we	O
observe	O
n	O
independent	O
normal	B
variates	O
zi	O
each	O
with	O
its	O
own	O
effect	B
size	I
for	O
i	O
d	O
n	O
and	O
as	O
in	O
section	O
we	O
wish	O
to	O
test	O
the	O
null	O
hypotheses	O
n	O
zi	O
w	O
d	O
being	O
alert	O
to	O
the	O
pitfalls	O
of	O
simultaneous	O
testing	B
we	O
employ	O
a	O
false-discovery	B
rate	B
control	B
algorithm	B
which	O
rejects	O
r	B
of	O
the	O
n	O
null	O
hypotheses	O
say	O
for	O
cases	O
ir	O
equaled	O
in	O
the	O
example	O
of	O
figure	O
so	O
far	O
so	O
good	O
the	O
familiar	O
abuse	O
comes	O
in	O
then	O
setting	O
the	O
usual	O
confidence	O
intervals	B
coverage	B
for	O
the	O
r	B
selected	O
cases	O
this	O
ignores	O
the	O
model	B
selection	I
process	O
the	O
data-based	O
selection	O
of	O
the	O
r	B
cases	O
must	O
be	O
taken	O
into	O
account	O
in	O
making	O
legitimate	O
inferences	O
even	O
if	O
r	B
is	O
only	O
so	O
multiplicity	O
is	O
not	O
a	O
concern	O
this	O
problem	O
is	O
addressed	O
by	O
the	O
theory	B
of	O
false-coverage	O
control	B
suppose	O
algorithm	B
a	O
sets	O
confidence	O
intervals	B
for	O
r	B
of	O
the	O
n	O
cases	O
of	O
which	O
inference	B
after	B
model	B
selection	I
r	B
are	O
actually	O
false	O
coverages	O
i	O
e	O
ones	O
not	O
containing	O
the	O
true	O
effect	B
size	I
the	O
false-coverage	O
rate	B
of	O
a	O
is	O
the	O
expected	O
proportion	B
of	O
noncoverages	O
fcr	O
a	O
d	O
efrrg	O
the	O
expectation	O
being	O
with	O
respect	O
to	O
model	O
the	O
goal	O
as	O
with	O
the	O
fdr	O
theory	B
of	O
section	O
is	O
to	O
construct	O
algorithm	B
a	O
to	O
control	B
fcr	O
below	O
some	O
fixed	O
value	O
q	O
the	O
byq	O
controls	O
fcr	O
below	O
level	O
q	O
in	O
three	O
easy	O
steps	O
beginning	O
with	O
model	O
let	O
pi	O
be	O
the	O
p-value	B
corresponding	O
to	O
zi	O
pi	O
d	O
for	O
left-sided	O
significance	O
testing	B
and	O
order	O
the	O
p	O
i	O
values	O
in	O
ascending	O
order	O
p	O
n	O
calculate	O
r	B
d	O
maxfi	O
w	O
p	O
i	O
i	O
qng	O
and	O
in	O
the	O
bhq	B
algorithm	B
declare	O
the	O
r	B
corresponding	O
null	O
hypotheses	O
false	O
zi	O
z	O
r	B
for	O
each	O
of	O
the	O
r	B
cases	O
construct	O
the	O
confidence	O
interval	B
where	O
r	B
d	O
rqn	O
d	O
theorem	B
under	O
model	O
byq	O
has	O
fcr	O
q	O
moreover	O
none	O
of	O
the	O
intervals	B
contain	O
d	O
a	O
simulated	O
example	O
of	O
byq	O
was	O
run	O
according	O
to	O
these	O
specifications	O
n	O
d	O
d	O
n	O
q	O
d	O
zi	O
for	O
i	O
d	O
for	O
i	O
d	O
n	O
in	O
this	O
situation	O
we	O
have	O
null	O
cases	O
and	O
non-null	B
cases	O
but	O
of	O
which	O
had	O
because	O
this	O
is	O
a	O
simulation	B
we	O
can	O
plot	O
the	O
pairs	O
to	O
assess	O
the	O
byq	B
algorithm	B
s	O
performance	O
this	O
is	O
done	O
in	O
figure	O
for	O
the	O
non-null	B
cases	O
green	O
points	O
byq	O
declared	O
r	B
d	O
cases	O
non-null	B
those	O
having	O
zi	O
circled	O
points	O
of	O
the	O
declarations	O
short	O
for	O
benjamini	O
yekutieli	O
see	O
the	O
chapter	O
endnotes	O
simultaneous	O
confidence	O
intervals	B
figure	O
simulation	B
experiment	O
with	O
n	O
cases	O
of	O
which	O
are	O
non-null	B
the	O
green	O
points	O
are	O
these	O
non-null	B
cases	O
the	O
fdr	O
control	B
algorithm	B
bhq	B
d	O
declared	O
the	O
circled	O
cases	O
having	O
zi	O
to	O
be	O
non-null	B
of	O
which	O
the	O
red	O
points	O
were	O
actually	O
null	O
the	O
heavy	O
black	O
lines	O
show	O
byq	O
confidence	O
intervals	B
for	O
the	O
cases	O
only	O
of	O
which	O
failed	O
to	O
contain	O
actual	O
bayes	O
posterior	O
intervals	B
for	O
non-null	B
cases	O
dotted	O
lines	O
have	O
half	O
the	O
width	O
and	O
slope	O
of	O
byq	O
limits	O
were	O
actually	O
null	O
cases	O
red	O
circled	O
points	O
giving	O
false-discovery	O
proportion	B
d	O
the	O
heavy	O
black	O
lines	O
trace	O
the	O
byq	O
confidence	O
limits	O
as	O
a	O
function	B
of	O
z	O
the	O
first	O
thing	O
to	O
notice	O
is	O
that	O
fcr	O
control	B
has	O
indeed	O
been	O
achieved	O
only	O
of	O
the	O
declared	O
cases	O
lie	O
outside	O
their	O
limits	O
nulls	O
and	O
non-nulls	O
for	O
a	O
false-coverage	O
rate	B
of	O
d	O
safely	O
less	O
than	O
q	O
d	O
the	O
second	O
thing	O
however	O
is	O
that	O
the	O
byq	O
limits	O
provide	O
a	O
misleading	O
idea	O
of	O
the	O
location	O
of	O
given	O
zi	O
they	O
are	O
much	O
too	O
wide	O
and	O
slope	O
too	O
low	O
especially	O
for	O
more	O
negative	O
zi	O
values	O
in	O
this	O
situation	O
we	O
can	O
describe	O
precisely	O
the	O
posterior	B
distribution	B
of	O
given	O
zi	O
for	O
the	O
non-null	B
cases	O
n	O
zi	O
zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllby	O
loby	O
up	O
inference	B
after	B
model	B
selection	I
this	O
following	O
from	O
the	O
bayes	O
credible	O
limits	O
n	O
and	O
bayes	O
rule	B
n	O
zi	O
are	O
indicated	O
by	O
the	O
dotted	O
lines	O
in	O
figure	O
they	O
are	O
half	O
as	O
wide	O
as	O
the	O
byq	O
limits	O
and	O
have	O
slope	O
rather	O
than	O
in	O
practice	O
of	O
course	O
we	O
would	O
only	O
see	O
the	O
zi	O
not	O
the	O
making	O
unavailable	O
to	O
us	O
we	O
return	O
to	O
this	O
example	O
in	O
chapter	O
where	O
empirical	B
bayes	I
methods	O
will	O
be	O
seen	O
to	O
provide	O
a	O
good	O
approximation	O
to	O
the	O
bayes	O
limits	O
figure	O
as	O
with	O
scheff	O
e	O
s	O
method	B
the	O
byq	O
intervals	B
can	O
be	O
accused	O
of	O
being	O
accurate	B
but	I
not	I
correct	I
correct	O
here	O
has	O
a	O
bayesianfisherian	O
flavor	O
that	O
is	O
hard	O
to	O
pin	O
down	O
except	O
perhaps	O
in	O
large-scale	O
applications	O
where	O
empirical	B
bayes	I
analyses	O
can	O
suggest	O
appropriate	O
inferences	O
accuracy	B
after	B
model	B
selection	I
the	O
cubic	O
regression	B
curve	O
for	O
the	O
cholesterol	B
data	B
seen	O
in	O
figure	O
was	O
selected	O
according	O
to	O
the	O
cp	B
criterion	O
of	O
section	O
polynomial	O
regression	B
models	B
predicting	O
cholesterol	B
decrease	O
di	O
in	O
terms	O
of	O
powers	O
degrees	O
of	O
adjusted	B
compliance	B
ci	O
were	O
fit	O
by	O
ordinary	O
least	B
squares	I
for	O
degrees	O
table	O
shows	O
cp	B
estimates	O
being	O
minimized	O
at	O
degree	O
table	O
cp	B
table	O
for	O
cholesterol	B
data	B
of	O
figure	O
comparing	O
ols	O
polynomial	O
models	B
of	O
degrees	O
through	O
the	O
cubic	O
model	O
degree	O
d	O
is	O
the	O
minimizer	O
subtracted	O
from	O
the	O
cp	B
values	O
for	O
easier	O
comparison	O
assumes	O
d	O
degree	O
cp	B
we	O
wish	O
to	O
assess	O
the	O
accuracy	B
of	O
the	O
fitted	O
curve	O
taking	O
account	O
of	O
both	O
the	O
cp	B
model	B
selection	I
method	B
and	O
the	O
ols	O
fitting	O
process	O
the	O
bootstrap	B
accuracy	B
after	B
model	B
selection	I
is	O
a	O
natural	O
candidate	O
for	O
the	O
job	O
here	O
we	O
will	O
employ	O
the	O
nonparametric	B
bootstrap	B
of	O
section	O
than	O
the	O
parametric	B
bootstrap	B
of	O
section	O
though	O
this	O
would	O
be	O
no	O
more	O
difficult	O
to	O
carry	O
out	O
the	O
cholesterol	B
data	B
set	B
comprises	O
n	O
d	O
pairs	O
xi	O
d	O
consists	O
of	O
pairs	O
di	O
a	O
nonparametric	B
bootstrap	B
sample	B
x	O
chosen	O
at	O
random	O
and	O
with	O
replacement	O
from	O
the	O
original	O
let	O
t	B
be	O
the	O
curve	O
obtained	O
by	O
applying	O
the	O
cpols	O
algorithm	B
to	O
the	O
original	O
data	B
and	O
for	O
a	O
given	O
set	B
x	O
and	O
likewise	O
t	B
point	O
c	O
on	O
the	O
compliance	B
scale	B
let	O
for	O
the	O
algorithm	B
applied	O
to	O
x	O
o	O
c	O
d	O
t	B
x	O
be	O
the	O
value	O
of	O
t	B
evaluated	O
at	O
compliance	B
d	O
c	O
figure	O
a	O
histogram	O
of	O
nonparametric	B
bootstrap	B
replications	O
for	O
polynomial	O
regression	B
estimates	O
of	O
cholesterol	B
decreases	O
d	O
at	O
adjusted	B
compliance	B
c	O
d	O
blue	O
histogram	O
adaptive	B
estimator	B
o	O
c	O
using	O
full	B
cpols	O
algorithm	B
for	O
each	O
bootstrap	B
data	B
set	B
line	O
histogram	O
using	O
ols	O
only	O
with	O
degree	O
for	O
each	O
bootstrap	B
data	B
set	B
bootstrap	B
standard	O
errors	B
are	O
and	O
cholesterol	B
decrease	O
q	O
degree	O
inference	B
after	B
model	B
selection	I
b	O
d	O
nonparametric	B
bootstrap	B
replications	O
t	B
were	O
figure	O
shows	O
the	O
histogram	O
of	O
the	O
o	O
c	O
replications	O
for	O
c	O
d	O
it	O
is	O
labeled	O
adaptive	B
to	O
indicate	O
that	O
cp	B
model	B
selection	I
as	O
well	O
as	O
ols	O
this	O
is	O
as	O
opposed	O
to	O
the	O
fixed	O
fitting	O
was	O
carred	O
out	O
anew	O
for	O
each	O
x	O
histogram	O
where	O
there	O
was	O
no	O
cp	B
selection	O
cubic	O
ols	O
regression	B
always	O
being	O
used	O
figure	O
bootstrap	B
standard-error	O
estimates	O
of	O
o	O
c	O
solid	O
black	O
curve	O
adaptive	B
estimator	B
using	O
full	B
cpols	O
model	B
selection	I
estimate	B
red	O
dashed	O
curve	O
using	O
ols	O
only	O
with	O
polynomial	O
degree	O
fixed	O
at	O
blue	O
dotted	O
curve	O
bagged	O
estimator	B
using	O
bootstrap	B
smoothing	B
average	O
standard-error	O
ratios	O
adaptivefixed	O
d	O
adaptivesmoothed	O
d	O
for	O
the	O
bootstrap	B
estimate	B
of	I
standard	B
error	I
obtained	O
from	O
the	O
adaptive	B
values	O
o	O
c	O
was	O
compared	O
with	O
for	O
the	O
fixed	O
in	O
this	O
case	O
accounting	O
for	O
model	B
selection	I
adaptation	B
adds	O
more	O
than	O
to	O
the	O
standard	B
error	I
estimates	O
the	O
same	O
comparison	O
was	O
made	O
at	O
all	O
values	O
ten	O
times	O
more	O
than	O
needed	O
for	O
assessing	O
standard	O
errors	B
but	O
helpful	O
for	O
the	O
comparisons	O
that	O
follow	O
the	O
latter	O
is	O
not	O
the	O
usual	O
ols	O
assessment	O
following	O
that	O
would	O
be	O
appropriate	O
for	O
a	O
parametric	B
bootstrap	B
comparison	O
rather	O
it	O
s	O
the	O
nonparametric	B
one-sample	O
bootstrap	B
assessment	O
resampling	B
pairs	O
yi	O
as	O
individual	O
sample	B
points	O
compliance	B
cstandard	O
errors	B
of	O
qcfixedadaptivesmoothed	O
accuracy	B
after	B
model	B
selection	I
of	O
the	O
adjusted	B
compliance	B
c	O
figure	O
graphs	O
the	O
results	O
the	O
adaptive	B
standard	O
errors	B
averaged	O
greater	O
than	O
the	O
fixed	O
values	O
the	O
standard	O
we	O
ignored	O
model	B
selection	I
in	O
assessingbse	O
confidence	O
intervals	B
o	O
bse	O
would	O
be	O
roughly	O
too	O
short	O
if	O
figure	O
adaptive	B
histogram	O
of	O
figure	O
now	O
split	O
into	O
of	O
bootstrap	B
replications	O
where	O
cp	B
selected	O
linear	B
d	O
d	O
as	O
best	O
versus	O
having	O
m	O
m	O
regression	B
cases	O
are	O
shifted	O
about	O
units	O
downward	O
m	O
cases	O
resemble	O
the	O
fixed	O
histogram	O
in	O
figure	O
histograms	O
are	O
scaled	O
to	O
have	O
equal	O
areas	O
having	O
an	O
honest	O
assessment	O
of	O
standard	B
error	I
doesn	O
t	B
mean	O
that	O
t	B
x	O
is	O
a	O
good	O
estimator	B
model	B
selection	I
can	O
induce	O
an	O
unpleasant	O
jumpiness	O
in	O
an	O
estimator	B
as	O
the	O
original	O
data	B
vector	B
x	O
crosses	O
definitional	O
boundaries	O
this	O
happened	O
in	O
our	O
example	O
for	O
of	O
the	O
d	O
the	O
cp	B
algorithm	B
selected	O
linear	B
regression	B
m	O
bootstrap	B
samples	O
x	O
as	O
best	O
and	O
in	O
these	O
cases	O
o	O
tended	O
toward	O
smaller	O
values	O
figure	O
d	O
histogram	O
shifted	O
about	O
units	O
down	O
from	O
the	O
m	O
shows	O
the	O
m	O
histogram	O
now	O
resembles	O
the	O
fixed	O
histogram	O
in	O
figure	O
discontinuous	O
estimators	O
such	O
as	O
t	B
x	O
can	O
t	B
be	O
bayesian	B
bayes	O
posterior	O
expectations	O
being	O
continuous	O
they	O
can	O
also	O
suffer	O
frequentist	B
difficulties	O
including	O
excess	O
variability	O
and	O
overly	O
long	O
confidence	O
intervals	B
cholesterol	B
decrease	O
q	O
m	O
inference	B
after	B
model	B
selection	I
bagging	B
or	O
bootstrap	B
smoothing	B
is	O
a	O
tactic	O
for	O
improving	O
a	O
discontinuous	O
estimation	B
rule	B
by	O
averaging	B
in	O
and	O
chapter	O
replications	O
ft	O
average	O
suppose	O
t	B
is	O
any	O
estimator	B
for	O
which	O
we	O
have	O
obtained	O
bootstrap	B
b	O
d	O
bg	O
the	O
bagged	O
version	O
of	O
t	B
is	O
the	O
bx	O
s	O
x	O
d	O
b	O
t	B
the	O
letter	O
s	O
here	O
stands	O
for	O
smooth	O
small	O
changes	O
in	O
x	O
even	O
ones	O
that	O
move	O
across	O
a	O
model	B
selection	I
definitional	O
boundary	O
produce	O
only	O
small	O
changes	O
in	O
the	O
bootstrap	B
average	O
s	O
x	O
averaging	B
over	O
the	O
bootstrap	B
replications	O
of	O
t	B
x	O
gave	O
a	O
bagged	B
estimate	B
sc	O
x	O
for	O
each	O
value	O
of	O
c	O
bagging	B
reduced	O
the	O
standard	O
errors	B
of	O
the	O
cpols	O
estimates	O
t	B
x	O
by	O
about	O
as	O
indicated	O
by	O
the	O
green	O
dotted	O
curve	O
in	O
figure	O
t	B
x	O
where	O
did	O
the	O
green	O
dotted	O
curve	O
come	O
from	O
all	O
bootstrap	B
values	O
were	O
needed	O
to	O
produce	O
the	O
single	O
value	O
sc	O
x	O
it	O
seems	O
as	O
if	O
we	O
would	O
need	O
to	O
bootstrap	B
the	O
bootstrap	B
in	O
order	O
to	O
computebse	O
sc	O
x	O
fortunately	O
a	O
more	O
economical	O
calculation	O
is	O
possible	O
one	O
that	O
requires	O
only	O
the	O
original	O
b	O
bootstrap	B
computations	B
for	O
t	B
x	O
define	O
nbj	O
d	O
xj	O
occurs	O
in	O
x	O
for	O
b	O
d	O
b	O
and	O
j	O
d	O
n	O
for	O
instance	O
d	O
says	O
that	O
data	B
point	O
occurred	O
three	O
times	O
in	O
nonparametric	B
bootstrap	B
sample	B
the	O
b	O
by	O
n	O
matrix	B
fnbjg	O
completely	O
describes	O
the	O
b	O
bootstrap	B
samples	O
also	O
denote	O
where	O
dots	O
denote	O
averaging	B
over	O
b	O
d	O
theorem	B
the	O
infinitesimal	O
jackknife	B
estimate	B
of	I
standard	B
error	I
b	O
nbj	O
and	O
t	B
d	O
b	O
t	B
b	O
b	O
and	O
let	O
covj	O
indicate	O
the	O
covariance	O
in	O
the	O
bootstrap	B
sample	B
between	O
nbj	O
and	O
t	B
t	B
d	O
t	B
bx	O
t	B
p	O
covj	O
d	O
b	O
p	O
for	O
the	O
bagged	B
estimate	B
is	O
bseij	O
sc	O
x	O
d	O
nx	O
j	O
accuracy	B
after	B
model	B
selection	I
effort	O
keeping	O
track	O
of	O
nbj	O
as	O
we	O
generate	O
the	O
bootstrap	B
replications	O
t	B
us	O
to	O
compute	O
covj	O
and	O
bse	O
sc	O
x	O
without	O
any	O
additional	O
computational	O
figure	O
the	O
ratio	O
ofbseij	O
sc	O
x	O
t	B
x	O
averaging	B
in	O
fact	O
corollary	O
the	O
ratiobseij	O
sc	O
x	O
t	B
x	O
is	O
always	O
we	O
expect	O
averaging	B
to	O
reduce	O
variability	O
and	O
this	O
is	O
seen	O
to	O
hold	O
true	O
in	O
we	O
have	O
the	O
following	O
general	O
result	O
allows	O
the	O
savings	O
due	O
to	O
bagging	B
increase	O
with	O
the	O
nonlinearity	B
of	O
t	B
as	O
a	O
function	B
of	O
the	O
counts	O
nbj	O
in	O
the	O
language	O
of	O
section	O
in	O
the	O
nonlinearity	B
of	O
s	O
p	O
as	O
a	O
function	B
of	O
p	O
model-selection	O
estimators	O
such	O
as	O
the	O
cpols	O
rule	B
tend	O
toward	O
greater	O
nonlinearity	B
and	O
bigger	O
savings	O
table	O
proportion	B
of	O
nonparametric	B
bootstrap	B
replications	O
of	O
cpols	O
algorithm	B
that	O
selected	O
degrees	O
m	O
d	O
also	O
infinitesimal	O
jackknife	B
standard	B
deviations	I
for	O
proportions	O
which	O
mostly	O
exceed	O
the	O
estimates	O
themselves	O
proportion	B
bsdij	O
m	O
d	O
the	O
first	O
line	O
of	O
table	O
shows	O
the	O
proportions	O
in	O
which	O
the	O
various	O
degrees	O
were	O
selected	O
in	O
the	O
cholesterol	B
bootstrap	B
replications	O
for	O
linear	B
for	O
quadratic	O
for	O
cubic	O
etc	O
with	O
b	O
d	O
the	O
proportions	O
seem	O
very	O
accurate	O
the	O
binomial	B
standard	B
error	I
for	O
being	O
just	O
d	O
for	O
instance	O
indicate	O
whether	O
theorem	B
suggests	O
otherwise	O
now	O
let	O
t	B
the	O
bth	O
bootstrap	B
sample	B
x	O
made	O
the	O
cp	B
choice	O
m	O
d	O
d	O
if	O
m	O
if	O
m	O
d	O
t	B
b	O
d	O
bg	O
is	O
the	O
observed	O
proportion	B
the	O
bagged	O
value	O
of	O
ft	O
applying	O
the	O
bagging	B
theorem	B
yieldedbseij	O
d	O
as	O
seen	O
in	O
the	O
inference	B
after	B
model	B
selection	I
second	O
line	O
of	O
the	O
table	O
with	O
similarly	O
huge	O
standard	O
errors	B
for	O
the	O
other	O
proportions	O
the	O
binomial	B
standard	O
errors	B
are	O
internal	B
saying	O
how	O
quickly	O
the	O
bootstrap	B
resampling	B
process	O
is	O
converging	O
to	O
its	O
ultimate	O
value	O
as	O
b	O
the	O
infinitesimal	O
jackknife	B
estimates	O
are	O
external	B
if	O
we	O
collected	O
a	O
new	O
set	B
of	O
data	B
pairs	O
di	O
the	O
new	O
proportion	B
table	O
might	O
look	O
completely	O
different	O
than	O
the	O
top	O
line	O
of	O
table	O
frequentist	B
statistics	B
has	O
the	O
advantage	O
of	O
being	O
applicable	O
to	O
any	O
algorithmic	O
procedure	O
for	O
instance	O
to	O
our	O
cpols	O
estimator	B
this	O
has	O
great	O
appeal	O
in	O
an	O
era	O
of	O
enormous	O
data	B
sets	O
and	O
fast	O
computation	O
the	O
drawback	O
compared	O
with	O
bayesian	B
statistics	B
is	O
that	O
we	O
have	O
no	O
guarantee	O
that	O
our	O
chosen	O
algorithm	B
is	O
best	O
in	O
any	O
way	O
classical	O
statistics	B
developed	O
a	O
theory	B
of	O
best	O
for	O
a	O
catalog	O
of	O
comparatively	O
simple	O
estimation	B
and	O
testing	B
problems	O
in	O
this	O
sense	O
modern	O
inferential	O
theory	B
has	O
not	O
yet	O
caught	O
up	O
with	O
modern	O
problems	O
such	O
as	O
data-based	O
model	B
selection	I
though	O
techniques	O
such	O
as	O
model	B
averaging	B
bagging	B
suggest	O
promising	O
steps	O
forward	O
selection	B
bias	B
many	O
a	O
sports	O
fan	O
has	O
been	O
victimized	O
by	O
selection	B
bias	B
your	O
team	O
does	O
wonderfully	O
well	O
and	O
tops	O
the	O
league	O
standings	O
but	O
the	O
next	O
year	O
with	O
the	O
same	O
players	O
and	O
the	O
same	O
opponents	O
you	O
re	O
back	O
in	O
the	O
pack	O
this	O
is	O
the	O
winner	O
s	O
curse	O
a	O
more	O
picturesque	O
name	O
for	O
selection	B
bias	B
the	O
tendency	O
of	O
unusually	O
good	O
bad	O
comparative	O
performances	O
not	O
to	O
repeat	O
themselves	O
modern	O
scientific	O
technology	O
allows	O
the	O
simultaneous	O
investigation	O
of	O
hundreds	O
or	O
thousands	O
of	O
candidate	O
situations	O
with	O
the	O
goal	O
of	O
choosing	O
the	O
top	O
performers	O
for	O
subsequent	O
study	O
this	O
is	O
a	O
setup	O
for	O
the	O
heartbreak	O
of	O
selection	B
bias	B
an	O
apt	O
example	O
is	O
offered	O
by	O
the	O
prostate	B
study	O
data	B
of	O
section	O
where	O
we	O
observe	O
statistics	B
zi	O
measuring	O
patient	O
control	B
differences	O
for	O
n	O
d	O
genes	O
zi	O
n	O
i	O
d	O
n	O
here	O
is	O
the	O
effect	B
size	I
for	O
gene	O
i	O
the	O
true	O
difference	O
between	O
the	O
patient	O
and	O
control	B
populations	O
genes	O
with	O
large	O
positive	O
or	O
negative	O
values	O
of	O
would	O
be	O
promising	O
targets	O
for	O
further	O
investigation	O
gene	O
number	O
with	O
d	O
at	O
selection	B
bias	B
tained	O
the	O
biggest	O
z-value	O
says	O
that	O
is	O
unbiased	O
for	O
can	O
we	O
believe	O
the	O
obvious	O
estimate	B
d	O
no	O
is	O
the	O
correct	O
selection	B
bias	B
answer	O
gene	O
has	O
won	O
a	O
contest	O
for	O
bigness	O
among	O
contenders	O
in	O
addition	O
to	O
being	O
good	O
a	O
large	O
value	O
of	O
it	O
has	O
almost	O
certainly	O
been	O
lucky	O
with	O
the	O
noise	O
in	O
pushing	O
in	O
the	O
positive	O
direction	O
or	O
else	O
it	O
would	O
not	O
have	O
won	O
the	O
contest	O
this	O
is	O
the	O
essence	O
of	O
selection	B
bias	B
false-discovery	B
rate	B
theory	B
chapter	O
provided	O
a	O
way	O
to	O
correct	O
for	O
selection	B
bias	B
in	O
simultaneous	O
hypothesis	B
testing	B
this	O
was	O
extended	O
to	O
false-coverage	O
rates	O
in	O
section	O
our	O
next	O
vignette	O
concerns	O
the	O
realistic	O
estimation	B
of	O
effect	O
sizes	O
in	O
the	O
face	O
of	O
selection	B
bias	B
we	O
begin	O
by	O
assuming	O
that	O
an	O
effect	B
size	I
has	O
been	O
obtained	O
from	O
a	O
prior	B
density	B
might	O
include	O
discrete	O
atoms	O
and	O
then	O
z	O
n	O
observed	O
and	O
n	O
is	O
assumed	O
known	O
for	O
this	O
discussion	O
the	O
marginal	B
density	B
of	O
z	O
is	O
f	O
dz	O
exp	O
where	O
d	O
tweedie	O
s	O
formula	B
is	O
an	O
intriguing	O
expression	O
for	O
the	O
bayes	O
expectation	O
of	O
given	O
z	O
theorem	B
observed	O
z	O
is	O
in	O
model	O
the	O
posterior	O
expectation	O
of	O
having	O
d	O
d	O
d	O
z	O
c	O
dz	O
with	O
l	O
log	O
f	O
the	O
especially	O
convenient	O
feature	O
of	O
tweedie	O
s	O
formula	B
is	O
that	O
is	O
expressed	O
directly	O
in	O
terms	O
of	O
the	O
marginal	B
density	B
f	O
this	O
is	O
a	O
setup	O
for	O
empirical	B
bayes	I
estimation	B
we	O
don	O
t	B
know	O
but	O
in	O
large-scale	O
situations	O
we	O
can	O
estimate	B
the	O
marginal	B
density	B
f	O
from	O
the	O
observations	O
z	O
d	O
zn	O
perhaps	O
by	O
poisson	B
regression	B
as	O
in	O
table	O
yielding	O
d	O
zi	O
c	O
the	O
solid	O
curve	O
in	O
figure	O
shows	O
for	O
the	O
prostate	B
study	O
data	B
d	O
d	O
with	O
o	O
log	O
o	O
f	O
dz	O
l	O
l	O
inference	B
after	B
model	B
selection	I
figure	O
the	O
solid	O
curve	O
is	O
tweedie	O
s	O
estimate	B
local	O
false-discovery	O
ratecfdr	O
z	O
from	O
figure	O
scale	B
on	O
right	O
at	O
z	O
d	O
d	O
andcfdr	O
z	O
d	O
for	O
gene	O
for	O
the	O
prostate	B
study	O
data	B
the	O
dashed	O
line	O
shows	O
the	O
with	O
d	O
tweedie	O
s	O
estimate	B
is	O
with	O
d	O
and	O
o	O
jzij	O
agreeing	O
with	O
the	O
local	B
false-discovery	B
rate	B
curvecfdr	O
z	O
of	O
figf	O
obtained	O
using	O
fourth-degree	O
log	B
polynomial	I
regression	B
as	O
in	O
section	O
the	O
curve	O
has	O
hovering	O
near	O
zero	O
for	O
cfdr	O
z	O
d	O
so	O
even	O
though	O
zi	O
d	O
has	O
a	O
one-sided	O
p-value	B
of	O
ure	O
that	O
says	O
these	O
are	O
mostly	O
null	O
genes	O
increases	O
for	O
z	O
equaling	O
for	O
z	O
d	O
at	O
that	O
point	O
with	O
genes	O
to	O
consider	O
at	O
once	O
it	O
still	O
is	O
not	O
a	O
sure	O
thing	O
that	O
gene	O
i	O
is	O
non-null	B
about	O
of	O
the	O
genes	O
with	O
zi	O
near	O
will	O
be	O
non-null	B
and	O
these	O
will	O
have	O
effect	O
sizes	O
averaging	B
about	O
all	O
of	O
this	O
nicely	O
illustrates	O
the	O
combination	O
of	O
frequentist	B
and	O
bayesian	B
inference	B
possible	O
in	O
large-scale	O
studies	O
and	O
also	O
the	O
combination	O
of	O
estimation	B
and	O
hypothesis-testing	O
ideas	O
in	O
play	O
if	O
the	O
prior	B
density	B
in	O
is	O
assumed	O
to	O
be	O
normal	B
tweedie	O
s	O
formula	B
gives	O
the	O
james	O
stein	O
estimator	B
the	O
corresponding	O
curve	O
in	O
figure	O
in	O
that	O
case	O
would	O
be	O
a	O
straight	O
line	O
passing	O
through	O
the	O
origin	O
at	O
slope	O
like	O
the	O
james	O
stein	O
estimator	B
ridge	B
regression	B
and	O
the	O
lasso	B
of	O
chapter	O
tweedie	O
s	O
formula	B
is	O
a	O
shrinkage	B
estimator	B
for	O
d	O
the	O
most	O
extreme	O
observation	O
it	O
gave	O
valuee	O
m	O
zlllz	O
fdrtweediefdr	O
d	O
shrinking	O
the	O
maximum	B
likelihood	B
estimate	B
more	O
than	O
one	O
unit	O
toward	O
the	O
origin	O
selection	B
bias	B
bayes	O
estimators	O
are	O
immune	O
to	O
selection	B
bias	B
as	O
discussed	O
in	O
sections	O
and	O
this	O
offers	O
some	O
hope	O
that	O
tweedie	O
s	O
empirical	B
bayes	I
estimates	O
might	O
be	O
a	O
realistic	O
cure	O
for	O
the	O
winners	O
curse	O
a	O
small	O
simulation	B
experiment	O
was	O
run	O
as	O
a	O
test	O
a	O
hundred	O
data	B
sets	O
z	O
each	O
of	O
length	O
n	O
d	O
were	O
generated	O
accord	O
ing	O
to	O
a	O
combination	O
of	O
exponential	O
and	O
normal	B
sampling	O
e	O
and	O
n	O
l	O
z	O
was	O
computed	O
as	O
in	O
section	O
now	O
using	O
a	O
natural	O
for	O
i	O
d	O
for	O
each	O
z	O
o	O
spline	O
model	O
with	O
five	O
degrees	B
of	I
freedom	I
this	O
gave	O
tweedie	O
s	O
estimates	O
d	O
zi	O
c	O
o	O
l	O
i	O
d	O
for	O
that	O
data	B
set	B
z	O
for	O
each	O
data	B
set	B
z	O
the	O
largest	O
zi	O
values	O
and	O
the	O
corresponding	O
and	O
values	O
were	O
recorded	O
yielding	O
the	O
uncorrected	B
differences	I
and	O
corrected	B
differences	I
zi	O
the	O
hope	O
being	O
that	O
empirical	B
bayes	I
shrinkage	B
would	O
correct	O
the	O
selection	B
bias	B
in	O
the	O
zi	O
values	O
figure	O
shows	O
the	O
data	B
sets	O
top	O
cases	O
each	O
uncorrected	O
and	O
corrected	B
differences	I
selection	B
bias	B
is	O
quite	O
obvious	O
with	O
the	O
uncorrected	B
differences	I
shifted	O
one	O
unit	O
to	O
the	O
right	O
of	O
zero	O
in	O
this	O
case	O
at	O
least	O
the	O
empirical	B
bayes	I
corrections	O
have	O
worked	O
well	O
the	O
corrected	B
differences	I
being	O
nicely	O
centered	O
at	O
zero	O
bias	B
correction	O
often	O
adds	O
variance	B
but	O
in	O
this	O
case	O
it	O
hasn	O
t	B
finally	O
it	O
is	O
worth	O
saying	O
that	O
the	O
empirical	B
part	O
of	O
empirical	B
bayes	I
is	O
less	O
the	O
estimation	B
of	O
bayesian	B
rules	O
from	O
the	O
aggregate	O
data	B
than	O
the	O
application	O
of	O
such	O
rules	O
to	O
individual	O
cases	O
for	O
the	O
prostate	B
data	B
we	O
began	O
with	O
no	O
definite	O
prior	B
opinions	O
but	O
arrived	O
at	O
strong	O
not	O
uninformative	O
bayesian	B
conclusions	O
for	O
say	O
in	O
the	O
prostate	B
study	O
inference	B
after	B
model	B
selection	I
figure	O
corrected	O
and	O
uncorrected	B
differences	I
for	O
top	O
cases	O
in	O
each	O
of	O
simulations	O
tweedie	O
corrections	O
effectively	O
counteracted	O
selection	B
bias	B
combined	O
bayes	O
frequentist	B
estimation	B
as	O
mentioned	O
previously	O
bayes	O
estimates	O
are	O
at	O
least	O
theoretically	O
immune	O
from	O
selection	B
bias	B
let	O
z	O
d	O
zn	O
represent	O
the	O
prostate	B
study	O
data	B
of	O
the	O
previous	O
section	O
with	O
parameter	O
vector	B
d	O
bayes	O
rule	B
d	O
yields	O
the	O
posterior	B
density	B
of	O
given	O
z	O
a	O
data-based	O
model	B
selection	I
rule	B
such	O
as	O
estimate	B
the	O
corresponding	O
to	O
the	O
largest	O
observation	O
zi	O
has	O
no	O
effect	O
on	O
the	O
likelihood	B
function	B
z	O
fixed	O
or	O
on	O
having	O
chosen	O
a	O
prior	B
our	O
posterior	O
estimate	B
of	O
is	O
unaffected	O
by	O
the	O
fact	O
that	O
d	O
happens	O
to	O
be	O
largest	O
this	O
same	O
argument	B
applies	O
just	O
as	O
well	O
to	O
any	O
data-based	O
model	B
selection	I
procedure	O
for	O
instance	O
a	O
preliminary	O
screening	O
of	O
possible	O
variables	O
to	O
include	O
in	O
a	O
regression	B
analysis	B
the	O
cp	B
choice	O
of	O
a	O
cubic	O
regression	B
in	O
figure	O
having	O
no	O
effect	O
on	O
its	O
bayes	O
posterior	O
accuracy	B
there	O
is	O
a	O
catch	O
the	O
chosen	O
prior	B
must	O
apply	O
to	O
the	O
entire	O
parameter	O
vector	B
and	O
not	O
just	O
the	O
part	O
we	O
are	O
interested	O
in	O
this	O
is	O
differencesfrequency	O
combined	O
bayes	O
frequentist	B
estimation	B
feasible	O
in	O
one-parameter	B
situations	O
like	O
the	O
stopping	B
rule	B
example	O
of	O
figure	O
it	O
becomes	O
difficult	O
and	O
possibly	O
dangerous	O
in	O
higher	O
dimensions	O
empirical	B
bayes	I
methods	O
such	O
as	O
tweedie	O
s	O
rule	B
can	O
be	O
thought	O
of	O
as	O
allowing	O
the	O
data	B
vector	B
z	O
to	O
assist	O
in	O
the	O
choice	O
of	O
a	O
high-dimensional	O
prior	B
an	O
effective	O
collaboration	O
between	O
bayesian	B
and	O
frequentist	B
methodology	O
our	O
chapter	O
s	O
final	O
vignette	O
concerns	O
another	O
bayes	O
frequentist	B
estimad	O
ff	O
tion	O
technique	O
dropping	O
the	O
boldface	O
notation	O
suppose	O
that	O
f	O
is	O
a	O
multi-dimensional	O
family	O
of	O
densities	O
with	O
playing	O
the	O
role	O
of	O
and	O
that	O
we	O
are	O
interested	O
in	O
estimating	O
a	O
particular	O
parameter	O
d	O
t	B
a	O
prior	B
g	O
has	O
been	O
chosen	O
yielding	O
posterior	O
expectation	O
o	O
d	O
e	O
ft	O
how	O
accurate	O
is	O
o	O
the	O
usual	O
answer	O
would	O
be	O
calculated	O
from	O
the	O
posterior	B
distribution	B
of	O
given	O
x	O
this	O
is	O
obviously	O
the	O
correct	O
answer	O
if	O
g	O
is	O
based	O
on	O
genuine	O
prior	B
experience	O
most	O
often	O
though	O
and	O
especially	O
in	O
high-dimensional	O
problems	O
the	O
prior	B
reflects	O
mathematical	O
convenience	O
and	O
a	O
desire	O
to	O
be	O
uninformative	O
as	O
in	O
chapter	O
there	O
is	O
a	O
danger	O
of	O
circular	O
reasoning	O
in	O
using	O
a	O
self-selected	O
prior	B
distribution	B
to	O
calculate	O
the	O
accuracy	B
of	O
its	O
own	O
estimator	B
an	O
alternate	O
approach	O
discussed	O
next	O
is	O
to	O
calculate	O
the	O
frequentist	B
accuracy	B
of	O
o	O
that	O
is	O
even	O
though	O
is	O
a	O
bayes	O
estimate	B
we	O
consider	O
o	O
simply	O
as	O
a	O
function	B
of	O
x	O
and	O
compute	O
its	O
frequentist	B
variability	O
the	O
next	O
theorem	B
leads	O
to	O
a	O
computationally	O
efficient	O
way	O
of	O
doing	O
so	O
bayes	O
and	O
frequentist	B
standard	O
errors	B
for	O
o	O
operate	O
in	O
conceptually	O
orthogonal	O
directions	O
as	O
pictured	O
in	O
figure	O
here	O
we	O
are	O
supposing	O
that	O
the	O
prior	B
is	O
unavailable	O
or	O
uncertain	O
forcing	O
more	O
attention	O
on	O
frequentist	B
calculations	O
for	O
convenience	O
we	O
will	O
take	O
the	O
family	O
f	O
to	O
be	O
a	O
p-parameter	B
expo	O
nential	O
family	O
f	O
d	O
e	O
now	O
with	O
being	O
the	O
parameter	O
vector	B
called	O
above	O
the	O
p	O
p	O
covariance	O
matrix	B
of	O
x	O
is	O
denoted	O
v	O
d	O
cov	O
let	O
covx	O
indicate	O
the	O
posterior	O
covariance	O
given	O
x	O
between	O
d	O
t	B
the	O
parameter	O
of	O
interest	O
and	O
covx	O
d	O
covf	O
t	B
n	O
o	O
inference	B
after	B
model	B
selection	I
o	O
a	O
p	O
vector	B
covx	O
leads	O
directly	O
to	O
a	O
frequentist	B
estimate	B
of	O
accuracy	B
for	O
o	O
theorem	B
the	O
delta	B
method	B
estimate	B
of	I
standard	B
error	I
for	O
o	O
d	O
eft	O
isbsedelta	O
x	O
vo	O
covx	O
where	O
vo	O
is	O
v	O
evaluated	O
at	O
the	O
mle	B
o	O
bsedeltaf	O
o	O
the	O
theorem	B
allows	O
us	O
to	O
calculate	O
the	O
frequentist	B
accuracy	B
estimate	B
with	O
hardly	O
any	O
additional	O
computational	O
effort	O
beyond	O
that	O
required	O
for	O
o	O
itself	O
suppose	O
we	O
have	O
used	O
an	O
mcmc	B
or	O
gibbs	B
sampling	I
algorithm	B
section	O
to	O
generate	O
a	O
sample	B
from	O
the	O
bayes	O
posterior	B
distribution	B
of	O
given	O
x	O
t	B
these	O
yield	O
the	O
usual	O
estimate	B
for	O
eft	O
bx	O
o	O
d	O
t	B
t	B
bx	O
b	O
t	B
and	O
dp	O
they	O
also	O
give	O
a	O
similar	O
expression	O
for	O
covf	O
t	B
t	B
d	O
t	B
t	B
dp	O
can	O
o	O
covx	O
d	O
b	O
b	O
b	O
from	O
which	O
we	O
for	O
an	O
example	O
of	O
theorem	B
in	O
action	O
we	O
consider	O
the	O
diabetes	B
i	O
the	O
ith	O
row	O
of	O
x	O
the	O
matrix	B
of	O
data	B
of	O
section	O
with	O
x	O
prediction	O
so	O
xi	O
is	O
the	O
vector	B
of	O
predictors	B
for	O
patient	O
i	O
the	O
response	B
vector	B
y	O
of	O
progression	O
scores	O
has	O
now	O
been	O
rescaled	O
to	O
have	O
d	O
in	O
the	O
normal	B
regression	B
y	O
nn	O
x	O
i	O
the	O
prior	B
distribution	B
g	O
was	O
taken	O
to	O
be	O
g	O
d	O
ce	O
obtained	O
from	O
parametric	B
bootstrap	B
resampling	B
taking	O
the	O
empirical	B
covariance	O
matrix	B
vo	O
may	O
be	O
known	O
theoretically	O
calculated	O
by	O
numerical	O
differentiation	O
in	O
or	O
of	O
bootstrap	B
replications	O
o	O
i	O
by	O
dividing	O
the	O
original	O
data	B
vector	B
y	O
by	O
its	O
estimated	O
standard	B
error	I
from	O
the	O
linear	B
model	I
efyg	O
d	O
x	O
combined	O
bayes	O
frequentist	B
estimation	B
with	O
d	O
and	O
c	O
the	O
constant	O
that	O
makes	O
g	O
integrate	O
to	O
this	O
is	O
the	O
bayesian	B
lasso	B
prior	B
so	O
called	O
because	O
of	O
its	O
connection	O
to	O
the	O
lasso	B
and	O
lasso	B
plays	O
no	O
part	O
in	O
what	O
follows	O
posterior	B
distribution	B
g	O
jy	O
let	O
an	O
mcmc	B
algorithm	B
generated	O
b	O
samples	O
from	O
the	O
b	O
i	O
x	O
i	O
o	O
it	O
has	O
bayes	O
posterior	O
standard	B
error	I
the	O
expectation	O
of	O
the	O
ith	O
patient	O
s	O
response	B
yi	O
the	O
bayes	O
posterior	O
expectation	O
of	O
is	O
d	O
x	O
bx	O
o	O
d	O
d	O
bx	O
which	O
we	O
can	O
compare	O
withbsedelta	O
o	O
the	O
frequentist	B
standard	B
error	I
figure	O
shows	O
the	O
mcmc	B
replications	O
o	O
i	O
for	O
patient	O
i	O
d	O
the	O
point	O
estimate	B
o	O
i	O
equaled	O
with	O
bayes	O
and	O
frequenbsebayes	O
d	O
and	O
bsedelta	O
d	O
tist	O
standard	B
error	I
estimates	O
the	O
frequentist	B
standard	B
error	I
is	O
smaller	O
in	O
this	O
casebsedelta	O
was	O
less	O
thanbsebayes	O
for	O
all	O
patients	O
the	O
difference	O
averaging	B
a	O
modest	O
bsebayes	O
i	O
o	O
things	O
can	O
work	O
out	O
differently	O
suppose	O
we	O
are	O
interested	O
in	O
the	O
poste	O
d	O
x	O
b	O
x	O
c	O
d	O
rior	O
cdf	B
of	O
given	O
y	O
for	O
any	O
given	O
value	O
of	O
c	O
let	O
c	O
c	O
c	O
cdf	B
c	O
d	O
bx	O
if	O
x	O
if	O
x	O
so	O
t	B
is	O
our	O
mcmc	B
assessment	O
of	O
cjyg	O
the	O
solid	O
curve	O
in	O
figure	O
graphs	O
cdf	B
c	O
b	O
t	B
if	O
we	O
believe	O
prior	B
then	O
the	O
curve	O
exactly	O
represents	O
the	O
posterior	B
distribution	B
of	O
given	O
y	O
for	O
the	O
simulation	B
error	O
due	O
to	O
stopping	O
at	O
b	O
replications	O
whether	O
or	O
not	O
we	O
believe	O
the	O
prior	B
we	O
can	O
use	O
inference	B
after	B
model	B
selection	I
figure	O
a	O
histogram	O
of	O
mcmc	B
replications	O
for	O
posterior	B
distribution	B
of	O
expected	O
progression	O
for	O
patient	O
in	O
the	O
diabetes	B
study	O
model	O
and	O
prior	B
the	O
bayes	O
posterior	O
expectation	O
is	O
frequentist	B
standard	B
error	I
for	O
o	O
posterior	O
standard	B
error	I
d	O
was	O
smaller	O
than	O
bayes	O
theorem	B
with	O
t	B
d	O
t	B
in	O
to	O
evaluate	O
the	O
frequentist	B
the	O
dashed	O
vertical	O
red	O
lines	O
show	O
cdf	B
c	O
plus	O
or	O
minus	O
onebsedelta	O
unit	O
accuracy	B
of	O
the	O
curve	O
the	O
standard	O
errors	B
are	O
disturbingly	O
large	O
for	O
instance	O
at	O
c	O
d	O
the	O
central	O
credible	B
interval	B
for	O
c-values	O
between	O
cdf	B
c	O
and	O
has	O
frequentist	B
standard	O
errors	B
about	O
for	O
each	O
endpoint	O
of	O
the	O
interval	B
s	O
length	O
if	O
we	O
believe	O
prior	B
then	O
is	O
an	O
exact	O
credible	B
interval	B
for	O
and	O
moreover	O
is	O
immune	O
to	O
any	O
selection	B
bias	B
involved	O
in	O
our	O
focus	O
on	O
if	O
not	O
the	O
large	O
frequentist	B
standard	O
errors	B
are	O
a	O
reminder	O
that	O
calculation	O
might	O
turn	O
out	O
much	O
differently	O
in	O
a	O
new	O
version	O
of	O
the	O
diabetes	B
study	O
even	O
ignoring	O
selection	B
bias	B
to	O
return	O
to	O
our	O
main	O
theme	O
bayesian	B
calculations	O
encourage	O
a	O
disregard	O
for	O
model	B
selection	I
effects	O
this	O
can	O
be	O
dangerous	O
in	O
objective	B
bayes	I
mcmc	B
errorsbayes	O
posterior	O
notes	O
and	O
details	O
figure	O
the	O
solid	O
curve	O
is	O
the	O
posterior	O
cdf	B
of	O
vertical	O
red	O
bars	O
indicate	O
one	O
frequentist	B
standard	B
error	I
as	O
obtained	O
from	O
theorem	B
black	O
triangles	O
are	O
endpoints	O
of	O
the	O
central	O
credible	B
interval	B
settings	O
where	O
one	O
can	O
t	B
rely	O
on	O
genuine	O
prior	B
experience	O
theorem	B
serves	O
as	O
a	O
frequentist	B
checkpoint	O
offering	O
some	O
reassurance	O
as	O
in	O
figure	O
or	O
sounding	O
a	O
warning	O
as	O
in	O
figure	O
notes	O
and	O
details	O
optimality	B
theories	O
statements	O
of	O
best	O
possible	O
results	O
are	O
marks	O
of	O
maturity	O
in	O
applied	O
mathematics	O
classical	O
statistics	B
achieved	O
two	O
such	O
theories	O
for	O
unbiased	O
or	O
asymptotically	O
unbiased	O
estimation	B
and	O
for	O
hypothesis	B
testing	B
most	O
of	O
this	O
book	O
and	O
all	O
of	O
this	O
chapter	O
venture	O
beyond	O
these	O
safe	O
havens	O
how	O
far	O
from	O
best	O
are	O
the	O
cpols	O
bootstrap	B
smoothed	O
estimates	O
of	O
section	O
at	O
this	O
time	O
we	O
can	O
t	B
answer	O
such	O
questions	O
though	O
we	O
can	O
offer	O
appealing	O
methodologies	O
in	O
their	O
pursuit	O
a	O
few	O
of	O
which	O
have	O
been	O
highlighted	O
here	O
the	O
cholestyramine	O
example	O
comes	O
from	O
efron	O
and	O
feldman	O
where	O
it	O
is	O
discussed	O
at	O
length	O
data	B
for	O
a	O
control	B
group	O
is	O
also	O
analyzed	O
there	O
scheff	O
e	O
intervals	B
scheff	O
e	O
s	O
paper	O
came	O
at	O
the	O
beginning	O
c	O
inference	B
after	B
model	B
selection	I
of	O
a	O
period	O
of	O
healthy	O
development	O
in	O
simultaneous	B
inference	B
techniques	O
mostly	O
in	O
classical	O
normal	B
theory	B
frameworks	O
miller	O
gives	O
a	O
clear	O
and	O
thorough	O
summary	O
the	O
followed	O
with	O
a	O
more	O
computer-intensive	B
approach	O
nicely	O
developed	O
in	O
westfall	O
and	O
young	O
s	O
book	O
leading	O
up	O
to	O
benjamini	B
and	I
hochberg	I
s	O
false-discovery	B
rate	B
paper	O
here	O
and	O
benjamini	O
and	O
yekutieli	O
s	O
false-coverage	O
rate	B
algorithm	B
scheff	O
e	O
s	O
construction	O
is	O
derived	O
by	O
transforming	O
to	O
the	O
case	O
v	O
d	O
i	O
using	O
the	O
inverse	O
square	O
root	O
of	O
matrix	B
v	O
which	O
makes	O
the	O
ellipsoid	B
of	O
figure	O
into	O
a	O
circle	O
then	O
q	O
d	O
in	O
and	O
for	O
a	O
linear	B
combination	O
d	O
d	O
it	O
is	O
straightforward	O
to	O
see	O
that	O
prfq	O
k	O
pq	O
g	O
d	O
amounts	O
to	O
and	O
d	O
v	O
d	O
v	O
d	O
v	O
o	O
kdk	O
k	O
pq	O
for	O
all	O
choices	O
of	O
d	O
the	O
geometry	B
of	O
figure	O
now	O
being	O
transparent	O
changing	O
coordinates	O
back	O
to	O
o	O
d	O
v	O
d	O
v	O
and	O
c	O
d	O
v	O
yields	O
restricting	O
the	O
catalog	O
c	O
suppose	O
that	O
all	O
the	O
sample	B
sizes	O
nj	O
in	O
take	O
the	O
same	O
value	O
n	O
and	O
that	O
we	O
wish	O
to	O
set	B
simultaneous	O
confidence	O
intervals	B
for	O
all	O
pairwise	O
differences	O
i	O
j	O
tukey	B
s	O
studentized	B
range	I
pivotal	O
quantity	B
unpublished	O
o	O
i	O
o	O
j	O
i	O
j	O
t	B
d	O
max	O
i	O
j	O
has	O
a	O
distribution	B
not	O
depending	O
on	O
or	O
this	O
implies	O
that	O
i	O
j	O
o	O
i	O
o	O
j	O
p	O
is	O
a	O
set	B
of	O
simultaneous	O
level-	O
confidence	O
intervals	B
for	O
all	O
pairwise	O
differences	O
i	O
j	O
where	O
t	B
is	O
the	O
th	O
quantile	O
of	O
t	B
factor	B
n	O
comes	O
from	O
o	O
j	O
t	B
n	O
n	O
j	O
in	O
p	O
table	O
half-width	O
of	O
tukey	B
studentized	B
range	I
simultaneous	O
confidence	O
intervals	B
for	O
pairwise	O
differences	O
i	O
j	O
units	O
of	O
n	O
for	O
p	O
d	O
and	O
n	O
d	O
compared	O
with	O
scheff	O
e	O
intervals	B
p	O
tukey	B
scheff	O
e	O
notes	O
and	O
details	O
reducing	O
the	O
catalog	O
c	O
from	O
all	O
linear	B
combinations	O
c	O
to	O
only	O
pairwise	O
differences	O
shortens	O
the	O
simultaneous	O
intervals	B
table	O
shows	O
the	O
comparison	O
between	O
the	O
tukey	B
and	O
scheff	O
e	O
intervals	B
for	O
p	O
d	O
and	O
n	O
d	O
calculating	O
t	B
was	O
a	O
substantial	O
project	O
in	O
the	O
early	O
berk	O
et	O
al	O
now	O
carry	O
out	O
the	O
analogous	O
computations	B
for	O
general	O
catalogs	O
of	O
linear	B
constraints	O
they	O
discuss	O
at	O
length	O
the	O
inferential	O
basis	O
of	O
such	O
procedures	O
discontinuous	O
estimators	O
looking	O
at	O
figure	O
suggests	O
that	O
a	O
confidence	O
interval	B
for	O
t	B
x	O
will	O
move	O
far	O
left	O
for	O
data	B
sets	O
x	O
where	O
cp	B
selects	O
linear	B
regression	B
d	O
as	O
best	O
this	O
kind	O
of	O
jumpy	O
behavior	O
lengthens	O
the	O
intervals	B
needed	O
to	O
attain	O
a	O
desired	O
coverage	B
level	I
more	O
seriously	O
intervals	B
for	O
m	O
d	O
may	O
give	O
misleading	O
inferences	O
another	O
example	O
of	O
accurate	O
but	O
incorrect	O
behavior	O
bagging	B
in	O
addition	O
to	O
reducing	O
interval	B
length	O
improves	O
inferential	O
correctness	O
as	O
discussed	O
in	O
efron	O
theorem	B
and	O
its	O
corollary	O
theorem	B
is	O
proved	O
in	O
section	O
of	O
efron	O
with	O
a	O
parametric	B
bootstrap	B
version	O
appearing	O
in	O
section	O
the	O
corollary	O
is	O
a	O
projection	O
result	O
illustrated	O
in	O
figure	O
of	O
that	O
paper	O
let	O
l	O
n	O
be	O
the	O
n-dimensional	O
subspace	O
of	O
b-dimensional	O
euclidean	O
space	O
spanned	O
by	O
the	O
columns	O
of	O
the	O
b	O
n	O
matrix	B
and	O
t	B
the	O
b-vector	O
with	O
components	O
t	B
t	B
bseij	O
s	O
bseboot	O
t	B
kt	O
then	O
is	O
the	O
projection	O
of	O
t	B
where	O
ot	O
if	O
o	O
will	O
be	O
substantially	O
less	O
than	O
into	O
l	O
n	O
in	O
the	O
language	O
of	O
section	O
d	O
s	O
p	O
is	O
very	O
nonlinear	B
as	O
a	O
function	B
of	O
p	O
then	O
the	O
ratio	O
in	O
tweedie	O
s	O
formula	B
for	O
convenience	O
take	O
d	O
in	O
bayes	O
rule	B
can	O
then	O
be	O
arranged	O
to	O
give	O
d	O
p	O
with	O
d	O
z	O
c	O
log	O
f	O
this	O
is	O
a	O
one-parameter	B
exponential	B
family	I
having	O
natural	B
parameter	I
equal	O
to	O
z	O
differentiating	O
as	O
in	O
gives	O
d	O
d	O
dz	O
d	O
z	O
c	O
d	O
dz	O
log	O
f	O
inference	B
after	B
model	B
selection	I
which	O
is	O
tweedie	O
s	O
formula	B
when	O
d	O
the	O
formula	B
first	O
appears	O
in	O
robbins	O
who	O
credits	O
it	O
to	O
a	O
personal	O
communication	O
from	O
m	O
k	O
tweedie	O
efron	O
discusses	O
general	O
exponential	B
family	I
versions	O
of	O
tweedie	O
s	O
formula	B
and	O
its	O
application	O
to	O
selection	B
bias	B
situations	O
theorem	B
the	O
delta	B
method	B
standard	B
error	I
approximation	O
for	O
a	O
statistic	B
t	B
is	O
bsedelta	O
dh	O
ov	O
where	O
rt	O
is	O
the	O
gradient	O
vector	B
and	O
ov	O
is	O
an	O
estimate	B
of	O
the	O
covariance	O
matrix	B
of	O
x	O
other	O
names	O
include	O
the	O
taylor	B
series	I
method	B
as	O
in	O
and	O
propagation	B
of	I
errors	B
in	O
the	O
physical	O
sciences	O
literature	O
the	O
proof	O
of	O
theorem	B
in	O
section	O
of	O
efron	O
consists	O
of	O
showing	O
that	O
covx	O
d	O
rt	O
when	O
t	B
d	O
eft	O
standard	B
deviations	I
are	O
only	O
a	O
first	O
step	O
in	O
assessing	O
the	O
frequentist	B
accuracy	B
of	O
t	B
the	O
paper	O
goes	O
on	O
to	O
show	O
how	O
theorem	B
can	O
be	O
improved	O
to	O
give	O
confidence	O
intervals	B
correcting	O
the	O
impression	O
in	O
figure	O
that	O
cdf	B
c	O
can	O
range	O
outside	O
and	O
prior	B
gives	O
bayesian	B
lasso	B
applying	O
bayes	O
rule	B
with	O
density	B
log	O
g	O
jy	O
d	O
x	O
c	O
as	O
discussed	O
in	O
tibshirani	O
comparison	O
with	O
shows	O
that	O
the	O
maximizing	O
value	O
of	O
map	B
estimate	B
agrees	O
with	O
the	O
lasso	B
estimate	B
park	O
and	O
casella	O
named	O
the	O
bayesian	B
lasso	B
and	O
suggested	O
an	O
appropriate	O
mcmc	B
algorithm	B
their	O
choice	O
d	O
was	O
based	O
on	O
marginal	O
maximum	B
likelihood	B
calculations	O
giving	O
their	O
analysis	B
an	O
empirical	B
bayes	I
aspect	O
ignored	O
in	O
their	O
and	O
our	O
analyses	O
empirical	B
bayes	I
estimation	B
strategies	I
classic	B
statistical	I
inference	B
was	O
focused	O
on	O
the	O
analysis	B
of	O
individual	O
cases	O
a	O
single	O
estimate	B
a	O
single	O
hypothesis	O
test	O
the	O
interpretation	O
of	O
direct	B
evidence	I
bearing	O
on	O
the	O
case	O
of	O
interest	O
the	O
number	O
of	O
successes	O
and	O
failures	O
of	O
a	O
new	O
drug	O
in	O
a	O
clinical	O
trial	O
as	O
a	O
familiar	O
example	O
dominated	O
statistical	O
practice	O
the	O
story	O
of	O
modern	O
statistics	B
very	O
much	O
involves	O
indirect	B
evidence	I
learning	O
from	O
the	O
experience	O
of	O
others	O
in	O
the	O
language	O
of	O
sections	O
and	O
carried	O
out	O
in	O
both	O
frequentist	B
and	O
bayesian	B
settings	O
the	O
computerintensive	O
prediction	O
algorithms	O
described	O
in	O
chapters	O
use	O
regression	B
theory	B
the	O
frequentist	B
s	O
favored	O
technique	O
to	O
mine	O
indirect	B
evidence	I
on	O
a	O
massive	O
scale	B
false-discovery	B
rate	B
theory	B
chapter	O
collects	O
indirect	B
evidence	I
for	O
hypothesis	B
testing	B
by	O
means	O
of	O
bayes	O
theorem	B
as	O
implemented	O
through	O
empirical	B
bayes	I
estimation	B
empirical	B
bayes	I
methodology	O
has	O
been	O
less	O
studied	O
than	O
bayesian	B
or	O
frequentist	B
theory	B
as	O
with	O
the	O
james	O
stein	O
estimator	B
it	O
can	O
seem	O
to	O
be	O
little	O
more	O
than	O
plugging	O
obvious	O
frequentist	B
estimates	O
into	O
bayes	O
estimation	B
rules	O
this	O
conceals	O
a	O
subtle	O
and	O
difficult	O
task	O
learning	O
the	O
equivalent	O
of	O
a	O
bayesian	B
prior	B
distribution	B
from	O
ongoing	O
statistical	O
observations	O
our	O
final	O
chapter	O
concerns	O
the	O
empirical	B
bayes	I
learning	O
process	O
both	O
as	O
an	O
exercise	O
in	O
applied	O
deconvolution	B
and	O
as	O
a	O
relatively	O
new	O
form	B
of	O
statistical	O
inference	B
this	O
puts	O
us	O
back	O
where	O
we	O
began	O
in	O
chapter	O
examining	O
the	O
two	O
faces	O
of	O
statistical	O
analysis	B
the	O
algorithmic	O
and	O
the	O
inferential	O
bayes	O
deconvolution	B
a	O
familiar	O
formulation	O
of	O
empirical	B
bayes	I
inference	B
begins	O
by	O
assuming	O
that	O
an	O
unknown	O
prior	B
density	B
g	O
our	O
object	O
of	O
interest	O
has	O
produced	O
a	O
random	O
sample	B
of	O
real-valued	O
variates	O
n	O
i	O
d	O
n	O
g	O
i	O
empirical	B
bayes	I
estimation	B
strategies	I
density	B
may	O
include	O
discrete	O
atoms	O
of	O
probability	O
the	O
i	O
are	O
unobservable	O
but	O
each	O
yields	O
an	O
observable	O
random	O
variable	O
xi	O
according	O
to	O
a	O
known	O
family	O
of	O
density	B
functions	O
pi	O
i	O
xi	O
from	O
the	O
observed	O
sample	B
xn	O
we	O
wish	O
to	O
estimate	B
the	O
prior	B
density	B
g	O
a	O
famous	O
example	O
has	O
pi	O
i	O
the	O
poisson	B
family	O
xi	O
poi	O
i	O
xi	O
as	O
in	O
robbins	O
formula	B
section	O
still	O
more	O
familiar	O
is	O
the	O
normal	B
model	O
often	O
with	O
d	O
a	O
binomial	B
model	O
was	O
used	O
in	O
the	O
medical	O
example	O
of	O
section	O
n	O
i	O
xi	O
bi	O
ni	O
i	O
there	O
the	O
ni	O
differ	O
from	O
case	O
to	O
case	O
accounting	O
for	O
the	O
need	O
for	O
the	O
first	O
subscript	O
i	O
in	O
pi	O
i	O
let	O
fi	O
denote	O
the	O
marginal	B
density	B
of	O
xi	O
obtained	O
from	O
fi	O
dz	O
t	B
pi	O
di	O
the	O
integral	O
being	O
over	O
the	O
space	O
t	B
of	O
possible	O
values	O
the	O
statistician	O
has	O
only	O
the	O
marginal	O
observations	O
available	O
fi	O
xi	O
i	O
d	O
n	O
from	O
which	O
he	O
or	O
she	O
wishes	O
to	O
estimate	B
the	O
density	B
in	O
in	O
the	O
normal	B
model	O
fi	O
is	O
the	O
convolution	B
of	O
the	O
unknown	O
g	O
with	O
a	O
known	O
normal	B
density	B
denoted	O
f	O
d	O
g	O
fi	O
not	O
depending	O
on	O
i	O
estimating	O
g	O
using	O
a	O
sample	B
xn	O
from	O
f	O
is	O
a	O
problem	O
in	O
deconvolution	B
in	O
general	O
we	O
might	O
call	O
the	O
estimation	B
of	O
g	O
in	O
model	O
the	O
bayes	O
deconvolution	B
problem	O
n	O
an	O
artificial	O
example	O
appears	O
in	O
figure	O
where	O
g	O
is	O
a	O
mixture	O
distribution	B
seven-eighths	O
n	O
and	O
one-eighth	O
uniform	O
over	O
the	O
interval	B
a	O
normal	B
sampling	O
model	O
xi	O
n	O
i	O
is	O
assumed	O
yielding	O
f	O
by	O
convolution	B
as	O
in	O
the	O
convolution	B
process	O
makes	O
f	O
wider	O
bayes	O
deconvolution	B
figure	O
an	O
artificial	O
example	O
of	O
the	O
bayes	O
deconvolution	B
problem	O
the	O
solid	O
curve	O
is	O
g	O
the	O
prior	B
density	B
of	O
the	O
dashed	O
curve	O
is	O
the	O
density	B
of	O
an	O
observation	O
x	O
from	O
marginal	O
distribution	B
f	O
d	O
g	O
n	O
we	O
wish	O
to	O
estimate	B
g	O
on	O
the	O
basis	O
of	O
a	O
random	O
sample	B
xn	O
from	O
f	O
and	O
smoother	O
than	O
g	O
as	O
illustrated	O
in	O
the	O
figure	O
having	O
observed	O
a	O
random	O
sample	B
from	O
f	O
we	O
wish	O
to	O
estimate	B
the	O
deconvolute	O
g	O
which	O
begins	O
to	O
look	O
difficult	O
in	O
the	O
figure	O
s	O
example	O
deconvolution	B
has	O
a	O
well-deserved	O
reputation	O
for	O
difficulty	O
it	O
is	O
the	O
classic	O
ill-posed	O
problem	O
because	O
of	O
the	O
convolution	B
process	O
large	O
changes	O
in	O
g	O
are	O
smoothed	O
out	O
often	O
yielding	O
only	O
small	O
changes	O
in	O
f	O
deconvolution	B
operates	O
in	O
the	O
other	O
direction	O
with	O
small	O
changes	O
in	O
the	O
estimation	B
of	O
f	O
disturbingly	O
magnified	O
on	O
the	O
g	O
scale	B
nevertheless	O
modern	O
computation	O
modern	O
theory	B
and	O
most	O
of	O
all	O
modern	O
sample	B
sizes	O
together	O
can	O
make	O
empirical	B
deconvolution	B
a	O
practical	O
reality	O
why	O
would	O
we	O
want	O
to	O
estimate	B
g	O
in	O
the	O
prostate	B
data	B
example	O
is	O
called	O
we	O
might	O
wish	O
to	O
know	O
prf	O
d	O
the	O
probability	O
of	O
a	O
null	O
gene	O
ones	O
whose	O
effect	B
size	I
is	O
zero	O
or	O
perhaps	O
prfj	O
j	O
the	O
proportion	B
of	O
genes	O
that	O
are	O
substantially	O
non-null	B
or	O
we	O
might	O
want	O
to	O
estimate	B
bayesian	B
posterior	O
expectations	O
like	O
ef	O
jx	O
d	O
xg	O
in	O
figure	O
or	O
posterior	O
densities	O
as	O
in	O
figure	O
two	O
main	O
strategies	O
have	O
developed	O
for	O
carrying	O
out	O
empirical	B
bayes	I
estimation	B
modeling	O
on	O
the	O
scale	B
called	O
g-modeling	B
here	O
and	O
modeling	O
and	O
xgq	O
and	O
fxfxgq	O
empirical	B
bayes	I
estimation	B
strategies	I
on	O
the	O
x	O
scale	B
called	O
f	O
we	O
begin	O
in	O
the	O
next	O
section	O
with	O
gmodeling	O
g-modeling	B
and	O
estimation	B
there	O
has	O
been	O
a	O
substantial	O
amount	O
of	O
work	O
on	O
the	O
asymptotic	O
accuracy	B
of	O
estimates	O
og	O
in	O
the	O
empirical	B
bayes	I
model	O
most	O
often	O
in	O
the	O
normal	B
sampling	O
framework	O
the	O
results	O
are	O
discouraging	O
with	O
the	O
rate	B
of	O
convergence	O
of	O
og	O
to	O
g	O
as	O
slow	O
as	O
n	O
in	O
our	O
terminology	O
much	O
of	O
this	O
work	O
has	O
been	O
carried	O
out	O
in	O
a	O
nonparametric	B
gmodeling	O
framework	O
allowing	O
the	O
unknown	O
prior	B
density	B
g	O
to	O
be	O
virtually	O
anything	O
at	O
all	O
more	O
optimistic	O
results	O
are	O
possible	O
if	O
the	O
g-modeling	B
is	O
pursued	O
parametrically	O
that	O
is	O
by	O
restricting	O
g	O
to	O
lie	O
within	O
some	O
parametric	B
family	I
of	O
possibilities	O
we	O
assume	O
for	O
the	O
sake	O
of	O
simpler	O
exposition	O
that	O
the	O
space	O
t	B
of	O
pos	O
sible	O
values	O
is	O
finite	O
and	O
discrete	O
say	O
the	O
prior	B
density	B
g	O
is	O
now	O
represented	O
by	O
a	O
vector	B
g	O
d	O
gm	O
with	O
components	O
t	B
gj	O
d	O
pr	O
d	O
d	O
for	O
j	O
d	O
m	O
a	O
p-parameter	B
exponential	B
family	I
for	O
g	O
can	O
be	O
written	O
as	O
g	O
d	O
g	O
d	O
eq	O
where	O
the	O
p-vector	O
is	O
the	O
natural	B
parameter	I
and	O
q	O
is	O
a	O
known	O
m	O
p	O
structure	B
matrix	B
notation	O
means	O
that	O
the	O
j	O
th	O
component	O
of	O
g	O
is	O
j	O
the	O
j	O
th	O
row	O
of	O
q	O
the	O
function	B
is	O
the	O
normalizer	O
that	O
makes	O
with	O
q	O
g	O
sum	O
to	O
gj	O
d	O
eq	O
j	O
mx	O
d	O
log	O
j	O
eq	O
in	O
the	O
nodes	B
example	O
of	O
figure	O
the	O
set	B
of	O
possible	O
values	O
was	O
d	O
and	O
q	O
was	O
a	O
fifth-degree	O
polynomial	O
matrix	B
t	B
q	O
d	O
polyt	O
g-modeling	B
and	O
estimation	B
in	O
r	B
notation	O
indicating	O
a	O
five-parameter	O
exponential	B
family	I
for	O
g	O
in	O
the	O
development	O
that	O
follows	O
we	O
will	O
assume	O
that	O
the	O
kernel	O
pi	O
in	O
does	O
not	O
depend	O
on	O
i	O
i	O
e	O
that	O
xi	O
has	O
the	O
same	O
family	O
of	O
conditional	B
distributions	O
p	O
xij	O
i	O
for	O
all	O
i	O
as	O
in	O
the	O
poisson	B
and	O
normal	B
situations	O
and	O
but	O
not	O
the	O
binomial	B
case	O
and	O
moreover	O
we	O
assume	O
that	O
the	O
sample	B
space	O
x	O
for	O
the	O
xi	O
observations	O
is	O
finite	O
and	O
discrete	O
say	O
x	O
d	O
x	O
n	O
pkj	O
d	O
pr	O
xi	O
d	O
x	O
kj	O
i	O
d	O
none	O
of	O
this	O
is	O
necessary	O
but	O
it	O
simplifies	O
the	O
exposition	O
define	O
for	O
k	O
d	O
n	O
and	O
j	O
d	O
m	O
and	O
the	O
corresponding	O
n	O
m	O
matrix	B
p	O
d	O
having	O
kth	O
row	O
pk	O
d	O
pkm	O
the	O
convolution-type	O
formula	B
for	O
the	O
marginal	B
density	B
f	O
now	O
reduces	O
to	O
an	O
inner	O
product	O
xi	O
d	O
x	O
k	O
dpm	O
pkj	O
gj	O
in	O
fact	O
we	O
can	O
write	O
the	O
entire	O
marginal	B
density	B
f	O
d	O
in	O
terms	O
of	O
matrix	B
multiplication	O
fn	O
fk	O
d	O
pr	O
d	O
p	O
kg	O
the	O
vector	B
of	O
counts	O
y	O
d	O
yn	O
with	O
f	O
d	O
pg	O
yk	O
d	O
xi	O
d	O
x	O
k	O
is	O
a	O
sufficient	O
statistic	B
in	O
the	O
iid	O
situation	O
it	O
has	O
a	O
multinomial	O
distribution	B
y	O
multn	O
n	O
f	O
indicating	O
n	O
independent	O
draws	O
for	O
a	O
density	B
f	O
on	O
n	O
categories	O
all	O
of	O
this	O
provides	O
a	O
concise	O
description	O
of	O
the	O
g-modeling	B
probability	O
model	O
g	O
d	O
eq	O
f	O
d	O
pg	O
y	O
multn	O
n	O
f	O
empirical	B
bayes	I
estimation	B
strategies	I
the	O
inferential	O
task	O
goes	O
in	O
the	O
reverse	O
direction	O
y	O
o	O
f	O
g	O
o	O
d	O
eqo	O
figure	O
a	O
schematic	O
diagram	O
of	O
empirical	B
bayes	I
estimation	B
as	O
explained	O
in	O
the	O
text	O
sn	O
is	O
the	O
n-dimensional	O
simplex	B
containing	O
the	O
p-parameter	B
family	O
f	O
of	O
allowable	O
probability	O
distributions	O
f	O
the	O
vector	B
of	O
observed	O
proportions	O
yn	O
yields	O
mle	B
f	O
which	O
is	O
then	O
deconvolved	O
to	O
obtain	O
estimate	B
g	O
o	O
a	O
schematic	O
diagram	O
of	O
the	O
estimation	B
process	O
appears	O
in	O
figure	O
the	O
vector	B
of	O
observed	O
proportions	O
yn	O
is	O
a	O
point	O
in	O
sn	O
the	O
simplex	B
the	O
parametric	B
family	I
of	O
allowable	O
f	O
vectors	O
of	O
all	O
possible	O
probability	O
vectors	O
f	O
on	O
n	O
categories	O
yn	O
is	O
the	O
usual	O
nonparametric	B
estimate	B
of	O
f	O
d	O
ff	O
ag	O
f	O
indicated	O
by	O
the	O
red	O
curve	O
is	O
a	O
curved	B
p-dimensional	O
surface	O
in	O
sn	O
here	O
a	O
is	O
the	O
space	O
of	O
allowable	O
vectors	O
in	O
family	O
the	O
nonparametric	B
estimate	B
yn	O
is	O
projected	O
down	O
to	O
the	O
parametric	B
estimate	B
f	O
if	O
we	O
are	O
using	O
mle	B
estimation	B
f	O
will	O
be	O
the	O
closest	O
point	O
in	O
f	O
to	O
yn	O
measured	O
according	O
to	O
a	O
deviance	B
metric	O
as	O
in	O
finally	O
f	O
is	O
mapped	O
back	O
to	O
the	O
estimate	B
g	O
o	O
by	O
inverting	O
mapping	O
is	O
not	O
actually	O
necessary	O
with	O
g-modeling	B
since	O
having	O
found	O
o	O
g	O
o	O
is	O
obtained	O
directly	O
from	O
the	O
inversion	O
step	O
is	O
more	O
difficult	O
for	O
f	O
section	O
likelihood	B
regularization	B
and	O
accuracy	B
the	O
maximum	B
likelihood	B
estimation	B
process	O
for	O
g-modeling	B
is	O
discussed	O
in	O
more	O
detail	O
in	O
the	O
next	O
section	O
where	O
formulas	O
for	O
its	O
accuracy	B
will	O
be	O
developed	O
likelihood	B
regularization	B
and	O
parametric	B
g-modeling	B
as	O
in	O
allows	O
us	O
to	O
work	O
in	O
low-dimensional	O
parametric	B
families	O
just	O
five	O
parameters	O
for	O
the	O
nodes	B
example	O
where	O
classic	O
maximum	B
likelihood	B
methods	O
can	O
be	O
more	O
confidently	O
applied	O
even	O
here	O
though	O
some	O
regularization	B
will	O
be	O
necessary	O
for	O
stable	O
estimation	B
as	O
discussed	O
in	O
what	O
follows	O
the	O
g-model	O
probability	O
mechanism	O
yields	O
a	O
log	O
likelihood	B
for	O
the	O
multinomial	O
vector	B
y	O
of	O
counts	O
as	O
a	O
function	B
of	O
say	O
ly	O
fk	O
yk	O
log	O
fk	O
ly	O
d	O
log	O
its	O
score	B
function	B
p	O
h	O
d	O
p	O
determines	O
the	O
mle	B
o	O
according	O
to	O
p	O
p	O
p	O
matrix	B
of	O
second	O
derivatives	O
r	B
fisher	B
information	B
matrix	B
ly	O
the	O
vector	B
of	O
partial	O
derivatives	O
h	O
for	O
ly	O
d	O
the	O
ly	O
d	O
h	O
l	O
gives	O
the	O
ny	O
d	O
nx	O
the	O
exponential	B
family	I
model	O
yields	O
simple	O
expressions	O
for	O
p	O
and	O
i	O
define	O
ly	O
ly	O
i	O
d	O
pkj	O
wkj	O
d	O
gj	O
fk	O
and	O
the	O
corresponding	O
m-vector	O
wk	O
d	O
wkm	O
lemma	O
the	O
score	B
function	B
p	O
ly	O
under	O
model	O
is	O
where	O
wc	O
d	O
nx	O
p	O
ly	O
d	O
qwc	O
wk	O
and	O
q	O
is	O
the	O
m	O
p	O
structure	B
matrix	B
in	O
the	O
technical	O
lemmas	O
in	O
this	O
section	O
are	O
not	O
essential	O
to	O
following	O
the	O
subsequent	O
discussion	O
empirical	B
bayes	I
estimation	B
strategies	I
lemma	O
the	O
fisher	B
information	B
matrix	B
i	O
evaluated	O
at	O
d	O
o	O
is	O
i	O
o	O
d	O
q	O
wk	O
o	O
q	O
nx	O
yk	O
is	O
the	O
sample	B
size	I
in	O
the	O
empirical	B
bayes	I
model	O
where	O
n	O
dpn	O
see	O
the	O
chapter	O
endnotes	O
for	O
a	O
brief	O
discussion	O
of	O
lemmas	O
and	O
i	O
o	O
is	O
the	O
usual	O
maximum	B
likelihood	B
estimate	B
of	O
the	O
covariance	O
matrix	B
of	O
o	O
but	O
we	O
will	O
use	O
a	O
regularized	O
version	O
of	O
the	O
mle	B
that	O
is	O
less	O
variable	O
in	O
the	O
examples	O
that	O
follow	O
o	O
was	O
found	O
by	O
numerical	O
even	O
though	O
g	O
is	O
an	O
exponential	B
family	I
the	O
marginal	B
density	B
f	O
in	O
is	O
not	O
as	O
a	O
result	O
some	O
care	O
is	O
needed	O
in	O
avoiding	O
local	O
maxima	O
of	O
ly	O
these	O
tend	O
to	O
occur	O
at	O
corner	O
values	O
of	O
where	O
one	O
of	O
its	O
components	O
goes	O
to	O
infinity	O
a	O
small	O
amount	O
of	O
regularization	B
pulls	O
o	O
away	O
from	O
the	O
corners	O
decreasing	O
its	O
variance	B
at	O
the	O
possible	O
expense	O
of	O
increased	O
bias	B
instead	O
of	O
maximizing	O
ly	O
we	O
maximize	O
a	O
penalized	O
likelihood	B
m	O
d	O
ly	O
s	O
where	O
s	O
is	O
a	O
positive	O
penalty	B
function	B
our	O
examples	O
use	O
s	O
d	O
k	O
d	O
equal	O
which	O
prevents	O
the	O
maximizer	O
o	O
of	O
m	O
from	O
venturing	O
too	O
far	O
into	O
corners	O
h	O
the	O
following	O
lemma	O
is	O
discussed	O
in	O
the	O
chapter	O
endnotes	O
lemma	O
the	O
maximizer	O
o	O
of	O
m	O
has	O
approximate	O
bias	B
vector	B
and	O
covariance	O
matrix	B
bias	B
o	O
d	O
c	O
rs	O
o	O
and	O
var	O
o	O
d	O
c	O
rs	O
o	O
ps	O
o	O
i	O
o	O
c	O
rs	O
o	O
px	O
where	O
i	O
o	O
is	O
given	O
in	O
with	O
s	O
regularization	B
the	O
bias	B
is	O
zero	O
and	O
var	O
o	O
d	O
i	O
o	O
using	O
the	O
nonlinear	B
maximizer	O
nlm	B
in	O
r	B
likelihood	B
regularization	B
and	O
accuracy	B
the	O
usual	O
mle	B
approximations	O
including	O
s	O
reduces	O
variance	B
while	O
introducing	O
bias	B
for	O
s	O
d	O
k	O
we	O
calculate	O
rs	O
d	O
k	O
i	O
k	O
ps	O
d	O
k	O
and	O
with	O
i	O
the	O
p	O
p	O
identity	O
matrix	B
adding	O
the	O
penalty	B
s	O
in	O
pulls	O
the	O
mle	B
of	O
toward	O
zero	O
and	O
the	O
mle	B
of	O
g	O
toward	O
a	O
flat	O
distribution	B
over	O
t	B
looking	O
at	O
var	O
o	O
in	O
a	O
measure	O
of	O
the	O
regularization	B
effect	O
is	O
tr	O
rs	O
o	O
tr	O
i	O
o	O
most	O
often	O
we	O
will	O
be	O
more	O
interested	O
in	O
the	O
accuracy	B
of	O
og	O
d	O
g	O
o	O
than	O
which	O
was	O
never	O
more	O
than	O
a	O
few	O
percent	O
in	O
our	O
examples	O
in	O
that	O
of	O
o	O
itself	O
letting	O
d	O
o	O
d	O
diag	O
g	O
o	O
g	O
o	O
the	O
m	O
p	O
derivative	O
matrix	B
h	O
is	O
d	O
d	O
with	O
q	O
the	O
structure	B
matrix	B
in	O
the	O
usual	O
first-order	O
delta-method	O
calculations	O
then	O
give	O
the	O
following	O
theorem	B
theorem	B
the	O
penalized	O
maximum	B
likelihood	B
estimate	B
og	O
d	O
g	O
o	O
has	O
estimated	O
bias	B
vector	B
and	O
covariance	O
matrix	B
bias	B
og	O
d	O
d	O
o	O
and	O
var	O
og	O
d	O
d	O
o	O
d	O
o	O
with	O
bias	B
o	O
and	O
var	O
o	O
as	O
in	O
the	O
many	O
approximations	O
going	O
into	O
theorem	B
can	O
be	O
short-circuited	O
by	O
means	O
of	O
the	O
parametric	B
bootstrap	B
section	O
starting	O
from	O
o	O
and	O
f	O
d	O
pg	O
o	O
we	O
resample	O
the	O
count	O
vector	B
multn	O
n	O
f	O
based	O
on	O
y	O
and	O
the	O
penalized	O
mle	B
o	O
yielding	O
og	O
d	O
g	O
o	O
y	O
note	O
that	O
the	O
bias	B
treats	O
model	O
as	O
the	O
true	O
prior	B
and	O
arises	O
as	O
a	O
result	O
of	O
the	O
penalization	O
convergence	O
of	O
the	O
nlm	B
search	O
process	O
is	O
speeded	O
up	O
by	O
starting	O
from	O
o	O
gives	O
bias	B
and	O
covariance	O
estimates	O
b	O
replications	O
og	O
empirical	B
bayes	I
estimation	B
strategies	I
og	O
dbias	O
d	O
og	O
and	O
cvar	O
d	O
bx	O
dpb	O
og	O
og	O
og	O
og	O
og	O
og	O
og	O
and	O
og	O
table	O
comparison	O
of	O
delta	B
method	B
and	O
bootstrap	B
standard	O
errors	B
and	O
biases	O
for	O
the	O
nodes	B
study	O
estimate	B
of	O
g	O
in	O
figure	O
all	O
columns	O
except	O
the	O
first	O
multiplied	O
by	O
standard	B
error	I
bias	B
g	O
delta	O
delta	O
boot	O
boot	O
table	O
compares	O
the	O
delta	B
method	B
of	O
theorem	B
with	O
the	O
parametric	B
bootstrap	B
d	O
replications	O
for	O
the	O
surgical	O
nodes	B
example	O
of	O
section	O
both	O
the	O
standard	O
errors	B
square	O
roots	O
of	O
the	O
diagonal	O
elements	O
of	O
var	O
og	O
and	O
biases	O
are	O
well	O
approximated	O
by	O
the	O
delta	B
method	B
formulas	O
the	O
delta	B
method	B
also	O
performed	O
reasonably	O
well	O
on	O
the	O
two	O
examples	O
of	O
the	O
next	O
section	O
it	O
did	O
less	O
well	O
on	O
the	O
artificial	O
example	O
of	O
figure	O
where	O
g	O
d	O
i	O
c	O
e	O
d	O
uniform	O
on	O
and	O
n	O
the	O
vertical	O
bars	O
in	O
figure	O
indicate	O
one	O
standard	B
error	I
obtained	O
from	O
the	O
parametric	B
bootd	O
for	O
the	O
sample	B
space	O
of	O
and	O
asstrap	O
taking	O
t	B
suming	O
a	O
natural	B
spline	I
model	I
in	O
with	O
five	O
degrees	B
of	I
freedom	I
g	O
d	O
eq	O
q	O
d	O
nst	O
likelihood	B
regularization	B
and	O
accuracy	B
figure	O
the	O
red	O
curve	O
is	O
g	O
for	O
the	O
artificial	O
example	O
of	O
figure	O
vertical	O
bars	O
are	O
one	O
standard	B
error	I
for	O
g-model	O
estimate	B
g	O
o	O
specifications	O
sample	B
size	I
n	O
d	O
observations	O
xi	O
n	O
i	O
using	O
parametric	B
bootstrap	B
b	O
d	O
the	O
light	O
dashed	O
line	O
follows	O
bootstrap	B
means	O
og	O
j	O
some	O
definitional	O
bias	B
is	O
apparent	O
the	O
sampling	O
model	O
was	O
xi	O
n	O
i	O
for	O
i	O
d	O
n	O
d	O
in	O
this	O
case	O
the	O
delta	B
method	B
standard	O
errors	B
were	O
about	O
too	O
small	O
the	O
light	O
dashed	O
curve	O
in	O
figure	O
traces	O
ng	O
the	O
average	O
of	O
the	O
b	O
d	O
bootstrap	B
replications	O
g	O
there	O
is	O
noticeable	O
bias	B
compared	O
with	O
g	O
the	O
reason	O
is	O
simple	O
the	O
exponential	B
family	I
for	O
g	O
does	O
not	O
include	O
g	O
in	O
fact	O
ng	O
is	O
the	O
closest	O
member	O
of	O
the	O
exponential	B
family	I
to	O
g	O
this	O
kind	O
of	O
definitional	O
bias	B
is	O
a	O
disadvantage	O
of	O
parametric	B
g-modeling	B
our	O
g-modeling	B
examples	O
and	O
those	O
of	O
the	O
next	O
section	O
bring	O
together	O
a	O
variety	O
of	O
themes	O
from	O
modern	O
statistical	O
practice	O
classical	O
maximum	B
likelihood	B
theory	B
exponential	B
family	I
modeling	O
regularization	B
bootstrap	B
methods	O
large	O
data	B
sets	O
of	O
parallel	O
structure	B
indirect	B
evidence	I
and	O
a	O
combination	O
of	O
bayesian	B
and	O
frequentist	B
thinking	O
all	O
of	O
this	O
enabled	O
by	O
massive	O
computer	O
power	O
taken	O
together	O
they	O
paint	O
an	O
attractive	O
picture	O
of	O
the	O
range	O
of	O
inferential	O
methodology	O
in	O
the	O
twenty-first	O
century	O
empirical	B
bayes	I
estimation	B
strategies	I
two	O
examples	O
we	O
now	O
reconsider	O
two	O
previous	O
data	B
sets	O
from	O
a	O
g-modeling	B
point	O
of	O
view	O
the	O
first	O
is	O
the	O
artificial	O
microarray-type	O
example	O
comprising	O
n	O
independent	O
observations	O
zi	O
with	O
n	O
i	O
d	O
n	O
d	O
for	O
i	O
d	O
for	O
i	O
d	O
n	O
p	O
figure	O
displays	O
the	O
points	O
for	O
i	O
d	O
illustrating	O
the	O
bayes	O
posterior	O
conditional	B
intervals	B
these	O
required	O
knowing	O
the	O
bayes	O
prior	B
distribution	B
n	O
we	O
would	O
like	O
to	O
recover	O
intervals	B
using	O
just	O
the	O
observed	O
data	B
zi	O
i	O
d	O
without	O
knowledge	O
of	O
the	O
prior	B
figure	O
histogram	O
of	O
observed	O
sample	B
of	O
n	O
d	O
values	O
zi	O
from	O
simulations	O
a	O
histogram	O
of	O
the	O
z-values	O
is	O
shown	O
in	O
figure	O
g-modeling	B
was	O
applied	O
to	O
them	O
with	O
playing	O
the	O
role	O
of	O
z-valuesfrequency	O
two	O
examples	O
d	O
q	O
was	O
composed	O
of	O
a	O
and	O
z	O
being	O
x	O
taking	O
t	B
delta	O
function	B
at	O
d	O
and	O
a	O
fifth-degree	O
polynomial	O
basis	O
for	O
the	O
nonzero	O
again	O
a	O
family	O
of	O
spike-and-slab	O
priors	B
the	O
penalized	O
mle	B
og	O
d	O
estimated	O
the	O
probability	O
of	O
d	O
as	O
d	O
which	O
also	O
provided	O
bias	B
estimate	B
figure	O
purple	O
curves	O
show	O
g-modeling	B
estimates	O
of	O
conditional	B
credible	O
intervals	B
for	O
given	O
z	O
in	O
artificial	O
microarray	O
example	O
they	O
are	O
a	O
close	O
match	O
to	O
the	O
actual	O
bayes	O
intervals	B
dotted	O
lines	O
cf	O
figure	O
the	O
estimated	O
posterior	B
density	B
of	O
given	O
z	O
is	O
d	O
cz	O
the	O
standard	O
normal	B
density	B
and	O
cz	O
the	O
constant	O
required	O
for	O
to	O
integrate	O
to	O
let	O
q	O
denote	O
the	O
th	O
quantile	O
of	O
the	O
purple	O
curves	O
in	O
figure	O
trace	O
the	O
estimated	O
credible	O
intervals	B
they	O
are	O
a	O
close	O
match	O
to	O
the	O
actual	O
credible	O
intervals	B
the	O
solid	O
black	O
curve	O
in	O
figure	O
shows	O
for	O
slab	O
portion	O
of	O
the	O
estimated	O
prior	B
as	O
an	O
estimate	B
of	O
the	O
actual	O
slab	O
density	B
zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllby	O
loby	O
up	O
empirical	B
bayes	I
estimation	B
strategies	I
figure	O
the	O
heavy	O
black	O
curve	O
is	O
the	O
g-modeling	B
estimate	B
of	O
for	O
in	O
the	O
artificial	O
microarray	O
example	O
suppressing	O
the	O
atom	O
at	O
zero	O
d	O
it	O
is	O
only	O
a	O
rough	O
estimate	B
of	O
the	O
actual	O
nonzero	O
density	B
n	O
n	O
it	O
is	O
only	O
roughly	O
accurate	O
but	O
apparently	O
still	O
accurate	O
enough	O
to	O
yield	O
the	O
reasonably	O
good	O
posterior	O
intervals	B
seen	O
in	O
figure	O
the	O
fundamental	O
impediment	O
to	O
deconvolution	B
that	O
large	O
changes	O
in	O
g	O
produce	O
only	O
small	O
changes	O
in	O
f	O
can	O
sometimes	O
operate	O
in	O
the	O
statistician	O
s	O
favor	O
when	O
only	O
a	O
rough	O
knowledge	O
of	O
g	O
suffices	O
for	O
applied	O
purposes	O
our	O
second	O
example	O
concerns	O
the	O
prostate	B
study	O
data	B
last	O
seen	O
in	O
figure	O
n	O
d	O
men	O
cancer	O
patients	O
and	O
normal	B
controls	O
each	O
have	O
had	O
their	O
genetic	O
activities	O
measured	O
on	O
a	O
microarray	O
of	O
n	O
d	O
genes	O
genei	O
yields	O
a	O
test	O
statistic	B
zi	O
comparing	O
patients	O
with	O
controls	O
zi	O
n	O
with	O
the	O
gene	O
s	O
effect	B
size	I
we	O
will	O
take	O
the	O
variance	B
parameter	O
to	O
be	O
estimated	O
rather	O
than	O
assuming	O
density	B
for	O
the	O
effects	O
as	O
a	O
d	O
what	O
is	O
the	O
prior	B
the	O
local	B
false-discovery	B
rate	B
program	O
locfdr	B
section	O
was	O
applied	O
to	O
the	O
zi	O
values	O
as	O
shown	O
in	O
figure	O
locfdr	B
is	O
an	O
f	O
modeling	O
method	B
where	O
probability	O
models	B
are	O
proposed	O
directly	O
for	O
two	O
examples	O
figure	O
the	O
green	O
curve	O
is	O
a	O
six-parameter	O
poisson	B
regression	B
estimate	B
fit	O
to	O
counts	O
of	O
the	O
observed	O
zi	O
values	O
for	O
the	O
prostate	B
data	B
the	O
dashed	O
curve	O
is	O
the	O
empirical	B
null	I
zi	O
n	O
the	O
f	O
program	O
locfdr	B
estimated	O
null	O
probability	O
d	O
d	O
genes	O
with	O
z-values	O
lying	O
beyond	O
the	O
red	O
triangles	O
have	O
estimated	O
fdr	O
values	O
less	O
than	O
the	O
marginal	B
density	B
f	O
rather	O
than	O
for	O
the	O
prior	B
density	B
see	O
section	O
here	O
we	O
can	O
compare	O
locfdr	B
s	O
results	O
with	O
those	O
from	O
gmodeling	O
the	O
former	O
d	O
in	O
the	O
notation	O
of	O
that	O
is	O
it	O
estimated	O
the	O
null	O
distribution	B
as	O
n	O
with	O
probability	O
d	O
of	O
a	O
gene	O
being	O
null	O
d	O
only	O
genes	O
were	O
estimated	O
to	O
have	O
local	O
fdr	O
values	O
less	O
than	O
the	O
with	O
zi	O
and	O
the	O
with	O
zi	O
are	O
more	O
pessimistic	O
results	O
than	O
in	O
figure	O
where	O
we	O
used	O
the	O
theoretical	B
null	I
n	O
rather	O
than	O
the	O
empirical	B
null	I
n	O
the	O
g-modeling	B
approach	O
was	O
applied	O
to	O
the	O
prostate	B
study	O
data	B
assuming	O
zi	O
d	O
as	O
suggested	O
by	O
the	O
n	O
using	O
a	O
six-parameter	O
poisson	B
regression	B
fit	O
to	O
the	O
zi	O
values	O
of	O
the	O
type	O
employed	O
in	O
section	O
z-valuescounts	O
empirical	B
bayes	I
estimation	B
strategies	I
structure	B
matrix	B
q	O
in	O
had	O
a	O
delta	O
function	B
at	O
d	O
and	O
a	O
fiveparameter	O
natural	O
spline	O
basis	O
for	O
t	B
d	O
for	O
the	O
discretized	O
space	O
this	O
gave	O
a	O
penalized	O
mle	B
og	O
having	O
null	O
probability	O
d	O
figure	O
the	O
g-modeling	B
estimate	B
for	O
the	O
non-null	B
density	B
for	O
the	O
prostate	B
study	O
data	B
also	O
indicating	O
the	O
null	O
atom	O
d	O
about	O
of	O
the	O
genes	O
are	O
estimated	O
to	O
have	O
effect	O
sizes	O
the	O
red	O
bars	O
show	O
one	O
standard	B
error	I
as	O
computed	O
from	O
theorem	B
the	O
non-null	B
distribution	B
for	O
appears	O
in	O
figure	O
where	O
it	O
is	O
seen	O
to	O
be	O
modestly	O
unimodal	O
around	O
d	O
dashed	O
red	O
bars	O
indicate	O
one	O
standard	B
error	I
for	O
the	O
og	O
j	O
estimates	O
obtained	O
from	O
theorem	B
the	O
accuracy	B
is	O
not	O
very	O
good	O
it	O
is	O
better	O
for	O
larger	O
regions	O
of	O
the	O
space	O
for	O
examplebprfjj	O
d	O
here	O
g-modeling	B
estimated	O
less	O
prior	B
null	O
probability	O
compared	O
with	O
from	O
f	O
but	O
then	O
attributed	O
much	O
of	O
the	O
non-null	B
probability	O
to	O
small	O
values	O
of	O
taking	O
literally	O
suggests	O
genes	O
with	O
true	O
generalized	O
linear	B
mixed	O
models	B
false-discovery	O
d	O
from	O
g-modeling	B
for	O
large	O
figure	O
the	O
black	O
curve	O
is	O
the	O
empirical	B
bayes	I
estimated	O
values	O
of	O
jzj	O
it	O
nearly	O
matches	O
the	O
locfdr	B
f	O
estimate	B
fdr	O
z	O
red	O
curve	O
effect	O
sizes	O
that	O
doesn	O
t	B
mean	O
we	O
can	O
say	O
with	O
certainty	O
which	O
figure	O
compares	O
the	O
g-modeling	B
empirical	B
bayes	I
false-discovery	B
rate	B
as	O
in	O
with	O
the	O
f	O
estimatecfdr	O
z	O
produced	O
by	O
locfdr	B
where	O
it	O
counts	O
in	O
the	O
tails	O
they	O
are	O
nearly	O
the	O
same	O
d	O
d	O
cz	O
z	O
generalized	O
linear	B
mixed	O
models	B
the	O
g-modeling	B
theory	B
can	O
be	O
extended	O
to	O
the	O
situation	O
where	O
each	O
observation	O
xi	O
is	O
accompanied	O
by	O
an	O
observed	O
vector	B
of	O
covariates	O
ci	O
say	O
of	O
dimension	O
d	O
we	O
return	O
to	O
the	O
generalized	O
linear	B
model	I
setup	O
of	O
section	O
where	O
each	O
xi	O
has	O
a	O
one-parameter	B
exponential	B
family	I
density	B
indexed	O
by	O
its	O
own	O
natural	B
parameter	I
d	O
xi	O
in	O
notation	O
empirical	B
bayes	I
estimation	B
strategies	I
our	O
key	O
assumption	O
is	O
that	O
each	O
is	O
the	O
sum	O
of	O
a	O
deterministic	O
compo	O
nent	O
depending	O
on	O
the	O
covariates	O
ci	O
and	O
a	O
random	O
term	O
i	O
d	O
i	O
c	O
c	O
i	O
here	O
i	O
is	O
an	O
unobserved	O
realization	O
from	O
g	O
d	O
expfq	O
and	O
is	O
an	O
unknown	O
d-dimensional	O
parameter	O
if	O
d	O
then	O
is	O
a	O
g-model	O
as	O
while	O
if	O
all	O
the	O
i	O
d	O
then	O
it	O
is	O
a	O
standard	O
glm	O
taken	O
together	O
represents	O
a	O
generalized	O
linear	B
mixed	I
model	I
the	O
likelihood	B
and	O
accuracy	B
calculations	O
of	O
section	O
extend	O
to	O
glmms	O
as	O
referenced	O
in	O
the	O
endnotes	O
but	O
here	O
we	O
will	O
only	O
discuss	O
a	O
glmm	O
analysis	B
of	O
the	O
nodes	B
study	O
of	O
section	O
in	O
addition	O
to	O
ni	O
the	O
number	O
of	O
nodes	B
removed	O
and	O
xi	O
the	O
number	O
found	O
positive	O
a	O
vector	B
of	O
four	O
covariates	O
ci	O
d	O
sexi	O
smokei	O
progi	O
was	O
observed	O
for	O
each	O
patient	O
a	O
standardized	O
version	O
of	O
age	O
in	O
years	O
sex	O
being	O
for	O
female	O
or	O
for	O
male	O
smoke	O
being	O
for	O
no	O
or	O
for	O
yes	O
to	O
longterm	O
smoking	O
and	O
prog	O
being	O
a	O
post-operative	O
prognosis	O
score	O
with	O
large	O
values	O
more	O
favorable	O
glmm	O
model	O
was	O
applied	O
to	O
the	O
nodes	B
data	B
now	O
was	O
the	O
logit	B
log	O
where	O
xi	O
bi	O
ni	O
as	O
in	O
table	O
i	O
e	O
is	O
the	O
probability	O
that	O
any	O
one	O
node	O
from	O
patient	O
i	O
is	O
positive	O
to	O
make	O
the	O
correspondence	O
with	O
the	O
analysis	B
in	O
section	O
exact	O
we	O
used	O
a	O
variant	O
of	O
d	O
logit	B
i	O
c	O
c	O
i	O
now	O
with	O
d	O
i	O
is	O
exactly	O
the	O
binomial	B
probability	O
for	O
the	O
ith	O
case	O
maximum	B
likelihood	B
estimates	O
were	O
calculated	O
for	O
in	O
d	O
and	O
q	O
d	O
polyt	O
and	O
with	O
t	B
in	O
the	O
mle	B
prior	B
g	O
o	O
was	O
almost	O
the	O
same	O
as	O
that	O
estimated	O
without	O
covariates	O
in	O
figure	O
kbsek	O
sex	O
table	O
shows	O
the	O
mle	B
values	O
a	O
parametric	B
bootstrap	B
simulation	B
and	O
the	O
z-values	O
o	O
looks	O
like	O
it	O
has	O
a	O
significant	O
effect	O
with	O
males	O
tending	O
toward	O
larger	O
values	O
of	O
that	O
is	O
a	O
greater	O
number	O
of	O
positive	O
nodes	B
the	O
big	O
effect	O
though	O
is	O
prog	O
larger	O
values	O
of	O
prog	O
indicating	O
smaller	O
values	O
of	O
here	O
the	O
setup	O
is	O
more	O
specific	O
f	O
is	O
exponential	B
family	I
and	O
i	O
is	O
on	O
the	O
o	O
their	O
standard	O
errors	B
o	O
o	O
o	O
natural-parameter	O
scale	B
generalized	O
linear	B
mixed	O
models	B
o	O
for	O
glmm	O
table	O
maximum	B
likelihood	B
estimates	O
analysis	B
of	O
the	O
nodes	B
data	B
and	O
standard	O
errors	B
from	O
a	O
parametric	B
bootstrap	B
simulation	B
large	O
values	O
of	O
progi	O
predict	O
low	O
values	O
of	O
o	O
o	O
o	O
age	O
sex	O
smoke	O
prog	O
mle	B
boot	O
st	O
err	O
z-value	O
figure	O
distribution	B
of	O
individual	O
probabilities	B
of	O
a	O
positive	O
node	O
for	O
best	O
and	O
worst	O
levels	O
of	O
factor	B
prog	O
from	O
glmm	O
analysis	B
of	O
nodes	B
data	B
figure	O
displays	O
the	O
distribution	B
of	O
d	O
implied	O
by	O
the	O
glmm	O
model	O
for	O
the	O
best	O
and	O
worst	O
values	O
of	O
prog	O
age	O
sex	O
and	O
smoke	O
to	O
their	O
average	O
values	O
and	O
letting	O
have	O
distribution	B
g	O
o	O
the	O
implied	O
distribution	B
is	O
concentrated	O
near	O
d	O
for	O
the	O
bestlevel	O
prog	O
while	O
it	O
is	O
roughly	O
uniform	O
over	O
for	O
the	O
worst	O
level	O
the	O
random	O
effects	O
we	O
have	O
called	O
i	O
are	O
sometimes	O
called	O
frailties	B
a	O
composite	O
of	O
unmeasured	O
individual	O
factors	O
lumped	O
together	O
as	O
an	O
index	O
of	O
disease	O
susceptibility	O
taken	O
together	O
figures	O
and	O
show	O
substantial	O
frailty	O
and	O
covariate	O
effects	O
both	O
at	O
work	O
in	O
the	O
nodes	B
data	B
in	O
positive	O
nodedensityworst	O
prognosisbest	O
prognosis	O
empirical	B
bayes	I
estimation	B
strategies	I
the	O
language	O
of	O
section	O
we	O
have	O
amassed	O
indirect	B
evidence	I
for	O
each	O
patient	O
using	O
both	O
bayesian	B
and	O
frequentist	B
methods	O
deconvolution	B
and	O
f	O
empirical	B
bayes	I
applications	O
have	O
traditionally	O
been	O
dominated	O
by	O
f	O
modeling	O
not	O
the	O
g-modeling	B
approach	O
of	O
the	O
previous	O
sections	O
where	O
probability	O
models	B
for	O
the	O
marginal	B
density	B
f	O
usually	O
exponential	O
families	O
are	O
fit	O
directly	O
to	O
the	O
observed	O
sample	B
xn	O
we	O
have	O
seen	O
several	O
examples	O
robbins	O
estimator	B
in	O
table	O
the	O
bottom	O
line	O
locfdr	B
s	O
poisson	B
regression	B
estimates	O
in	O
figures	O
and	O
and	O
tweedie	O
s	O
estimate	B
in	O
figure	O
both	O
the	O
advantages	O
and	O
the	O
disadvantages	O
of	O
f	O
can	O
be	O
seen	O
in	O
the	O
inferential	O
diagram	O
of	O
figure	O
for	O
f	O
the	O
red	O
curve	O
now	O
can	O
represent	O
an	O
exponential	B
family	I
ff	O
whose	O
concave	O
log	O
likelihood	B
function	B
greatly	O
simplifies	O
the	O
calculation	O
of	O
f	O
from	O
yn	O
this	O
comes	O
at	O
a	O
price	O
the	O
deconvolution	B
step	O
from	O
f	O
to	O
a	O
prior	B
distribution	B
g	O
o	O
is	O
problematical	O
as	O
discussed	O
below	O
this	O
is	O
only	O
a	O
problem	O
if	O
we	O
want	O
to	O
know	O
g	O
the	O
traditional	O
applications	O
of	O
f	O
apply	O
to	O
problems	O
where	O
the	O
desired	O
answer	O
can	O
be	O
phrased	O
directly	O
in	O
terms	O
of	O
f	O
this	O
was	O
the	O
case	O
for	O
robbins	O
formula	B
the	O
local	B
false-discovery	B
rate	B
and	O
tweedie	O
s	O
formula	B
nevertheless	O
f	O
methodology	O
for	O
the	O
estimation	B
of	O
the	O
prior	B
g	O
does	O
exist	O
an	O
elegant	O
example	O
being	O
the	O
fourier	O
method	B
described	O
next	O
a	O
function	B
f	O
and	O
its	O
fourier	O
transform	B
are	O
related	O
by	O
dz	O
f	O
tx	O
dx	O
and	O
f	O
d	O
z	O
for	O
the	O
normal	B
case	O
where	O
xi	O
d	O
i	O
c	O
zi	O
with	O
zi	O
transform	B
of	O
f	O
is	O
a	O
multiple	O
of	O
that	O
for	O
g	O
d	O
tx	O
dt	O
n	O
the	O
fourier	O
so	O
on	O
the	O
transform	B
scale	B
estimating	O
g	O
from	O
f	O
amounts	O
to	O
removing	O
the	O
factor	B
exp	O
t	B
the	O
fourier	O
method	B
begins	O
with	O
the	O
empirical	B
density	B
n	O
f	O
that	O
puts	O
probability	O
on	O
each	O
observed	O
value	O
xi	O
and	O
then	O
proceeds	O
in	O
three	O
steps	O
n	O
f	O
is	O
smoothed	O
using	O
the	O
sinc	B
kernel	I
expressed	O
directly	O
as	O
a	O
kernel	O
estimate	B
nx	O
where	O
the	O
kernel	O
is	O
og	O
d	O
n	O
dz	O
z	O
n	O
f	O
dx	O
q	O
f	O
d	O
n	O
deconvolution	B
and	O
f	O
nx	O
sinc	O
xi	O
x	O
q	O
f	O
say	O
is	O
calculated	O
sinc	O
x	O
d	O
sin	O
x	O
x	O
the	O
fourier	O
transform	B
of	O
finally	O
og	O
is	O
taken	O
to	O
be	O
the	O
inverse	O
fourier	O
transform	B
of	O
this	O
last	O
step	O
eliminating	O
the	O
unwanted	O
factor	B
e	O
a	O
pleasantly	O
surprising	O
aspect	O
of	O
the	O
fourier	O
method	B
is	O
that	O
og	O
can	O
be	O
in	O
et	O
cos	O
tx	O
dt	O
d	O
large	O
values	O
of	O
smooth	O
n	O
og	O
at	O
the	O
expense	O
of	O
increased	O
bias	B
despite	O
its	O
compelling	O
rationale	O
there	O
are	O
two	O
drawbacks	O
to	O
the	O
fourier	O
method	B
first	O
of	O
all	O
it	O
applies	O
only	O
to	O
situations	O
xi	O
d	O
i	O
c	O
zi	O
where	O
xi	O
is	O
i	O
plus	O
iid	O
noise	O
more	O
seriously	O
the	O
biasvariance	O
trade-off	O
in	O
the	O
choice	O
of	O
can	O
be	O
quite	O
unfavorable	O
f	O
more	O
in	O
reducing	O
the	O
variance	B
of	O
this	O
is	O
illustrated	O
in	O
figure	O
for	O
the	O
artificial	O
example	O
of	O
figure	O
the	O
black	O
curve	O
is	O
the	O
standard	B
deviation	I
of	O
the	O
g-modeling	B
estimate	B
of	O
g	O
for	O
in	O
under	O
specifications	O
the	O
red	O
curve	O
graphs	O
the	O
standard	B
deviation	I
of	O
the	O
f	O
estimate	B
with	O
d	O
a	O
value	O
that	O
produced	O
roughly	O
the	O
same	O
amount	O
of	O
bias	B
as	O
the	O
gmodeling	O
estimate	B
in	O
figure	O
the	O
ratio	O
of	O
red	O
to	O
black	O
standard	B
deviations	I
averages	O
more	O
than	O
over	O
the	O
range	O
of	O
this	O
comparison	O
is	O
at	O
least	O
partly	O
unfair	O
g-modeling	B
is	O
parametric	B
while	O
the	O
fourier	O
method	B
is	O
almost	O
nonparametric	B
in	O
its	O
assumptions	O
about	O
f	O
or	O
g	O
it	O
can	O
be	O
greatly	O
improved	O
by	O
beginning	O
the	O
three-step	O
algorithm	B
with	O
a	O
parametric	B
estimate	B
o	O
f	O
rather	O
than	O
n	O
f	O
the	O
blue	O
dotted	O
curve	O
in	O
figure	O
does	O
this	O
with	O
o	O
f	O
a	O
poisson	B
regression	B
on	O
the	O
data	B
xn	O
as	O
in	O
figure	O
but	O
here	O
using	O
a	O
natural	O
spline	O
basis	O
giving	O
the	O
estimate	B
o	O
f	O
dx	O
og	O
dz	O
empirical	B
bayes	I
estimation	B
strategies	I
figure	O
standard	B
deviations	I
of	O
estimated	O
prior	B
density	B
og	O
for	O
the	O
artificial	O
example	O
of	O
figure	O
based	O
on	O
n	O
d	O
observations	O
xi	O
n	O
i	O
black	O
curve	O
using	O
g-modeling	B
under	O
specifications	O
red	O
curve	O
nonparametric	B
f	O
d	O
blue	O
curve	O
parametric	B
f	O
with	O
o	O
structure	B
matrix	B
having	O
five	O
degrees	B
of	I
freedom	I
f	O
estimated	O
from	B
poisson	B
regression	B
with	O
a	O
we	O
see	O
a	O
substantial	O
decrease	O
in	O
standard	B
deviation	I
though	O
still	O
not	O
attaining	O
g-modeling	B
rates	O
as	O
commented	O
before	O
the	O
great	O
majority	O
of	O
empirical	B
bayes	I
applications	O
have	O
been	O
of	O
the	O
robbinsfdrtweedie	O
variety	O
where	O
f	O
is	O
the	O
natural	O
choice	O
g-modeling	B
comes	O
into	O
its	O
own	O
for	O
situations	O
like	O
the	O
nodes	B
data	B
analysis	B
of	O
figures	O
and	O
where	O
we	O
really	O
want	O
an	O
estimate	B
of	O
the	O
prior	B
g	O
twenty-first-century	O
science	O
is	O
producing	O
more	O
such	O
data	B
sets	O
an	O
impetus	O
for	O
the	O
further	O
development	O
of	O
g-modeling	B
strategies	O
table	O
concerns	O
the	O
g-modeling	B
estimation	B
of	O
ex	O
d	O
ef	O
jx	O
d	O
xg	O
ex	O
dz	O
t	B
t	B
d	O
g	O
d	O
for	O
the	O
artificial	O
example	O
under	O
the	O
same	O
specifications	O
as	O
in	O
figure	O
samples	O
of	O
size	O
n	O
d	O
of	O
xi	O
n	O
i	O
were	O
drawn	O
from	O
model	O
yielding	O
mle	B
og	O
and	O
estimates	O
oex	O
for	O
x	O
between	O
gqg	O
modelparametricf	O
modelnon	O
parametricf	O
model	O
deconvolution	B
and	O
f	O
table	O
standard	B
deviation	I
of	O
oef	O
jxg	O
computed	O
from	O
parametric	B
bootstrap	B
simulations	O
of	O
og	O
the	O
g-modeling	B
is	O
as	O
in	O
figure	O
with	O
n	O
d	O
observations	O
xi	O
each	O
simulation	B
the	O
column	O
info	O
is	O
the	O
implied	O
empirical	B
bayes	I
information	B
for	O
estimating	O
ef	O
jxg	O
obtained	O
from	O
one	O
other	O
observation	O
xi	O
n	O
i	O
from	O
the	O
artificial	O
example	O
for	O
x	O
ef	O
jxg	O
sd	O
oe	O
info	O
and	O
one	O
thousand	O
such	O
estimates	O
oex	O
were	O
generated	O
averaging	B
almost	O
exactly	O
ex	O
with	O
standard	B
deviations	I
as	O
shown	O
accuracy	B
is	O
reasonably	O
good	O
the	O
coefficient	O
of	O
variation	O
sd	O
oexex	O
being	O
about	O
for	O
large	O
values	O
of	O
jxj	O
is	O
a	O
favorable	O
case	O
results	O
are	O
worse	O
for	O
other	O
conditional	B
estimates	O
such	O
as	O
ef	O
d	O
xg	O
theorem	B
implies	O
that	O
for	O
large	O
values	O
of	O
the	O
sample	B
size	I
n	O
the	O
variance	B
of	O
oex	O
decreases	O
as	O
say	O
var	O
n	O
oex	O
n	O
var	O
ix	O
d	O
o	O
cxn	O
n	O
oex	O
o	O
by	O
analogy	O
with	O
the	O
fisher	B
information	B
bound	B
we	O
can	O
define	O
the	O
empirical	B
bayes	I
information	B
for	O
estimating	O
ex	O
in	O
one	O
observation	O
to	O
be	O
x	O
so	O
that	O
varf	O
oexg	O
i	O
empirical	B
bayes	I
inference	B
leads	O
us	O
directly	O
into	O
the	O
world	O
of	O
indirect	B
evidence	I
learning	O
from	O
the	O
experience	O
of	O
others	O
as	O
in	O
sections	O
and	O
so	O
if	O
xi	O
d	O
each	O
other	O
observation	O
xj	O
provides	O
units	O
of	O
information	B
for	O
learning	O
ef	O
jxi	O
d	O
with	O
the	O
usual	O
fisher	B
d	O
for	O
the	O
direct	O
estimation	B
of	O
i	O
from	O
xi	O
this	O
information	B
value	O
i	O
is	O
a	O
favorable	O
case	O
as	O
mentioned	O
and	O
ix	O
is	O
often	O
much	O
smaller	O
the	O
main	O
point	O
perhaps	O
is	O
that	O
assuming	O
a	O
bayes	O
prior	B
is	O
not	O
a	O
casual	O
matter	O
and	O
empirical	B
bayes	I
estimation	B
strategies	I
can	O
amount	O
to	O
the	O
assumption	O
of	O
an	O
enormous	O
amount	O
of	O
relevant	O
other	O
information	B
notes	O
and	O
details	O
empirical	B
bayes	I
and	O
james	O
stein	O
estimation	B
chapters	O
and	O
exploded	O
onto	O
the	O
statistics	B
scene	O
almost	O
simultaneously	O
in	O
the	O
they	O
represented	O
a	O
genuinely	O
new	O
branch	O
of	O
statistical	O
inference	B
unlike	O
the	O
computerbased	O
extensions	O
of	O
classical	O
methodology	O
reviewed	O
in	O
previous	O
chapters	O
their	O
development	O
as	O
practical	O
tools	O
has	O
been	O
comparatively	O
slow	O
the	O
pace	O
has	O
quickened	O
in	O
the	O
twenty-first	O
century	O
with	O
false-discovery	O
rates	O
chapter	O
as	O
a	O
major	O
step	O
forward	O
a	O
practical	O
empirical	B
bayes	I
methodology	O
for	O
use	O
beyond	O
traditional	O
f	O
venues	O
such	O
as	O
fdr	O
is	O
the	O
goal	O
of	O
the	O
g-modeling	B
approach	O
lemmas	O
and	O
the	O
derivations	O
of	O
lemmas	O
and	O
are	O
straightforward	O
but	O
somewhat	O
involved	O
exercises	O
in	O
differential	O
calculus	O
carried	O
out	O
in	O
remark	O
b	O
of	O
efron	O
here	O
we	O
will	O
present	O
just	O
p	O
fk	O
d	O
a	O
sample	B
of	O
the	O
calculations	O
from	O
the	O
gradient	O
vector	B
l	O
with	O
respect	O
to	O
is	O
p	O
fk	O
d	O
pg	O
where	O
pg	O
is	O
the	O
m	O
p	O
derivative	O
matrix	B
pk	O
pg	O
d	O
l	O
d	O
dq	O
with	O
d	O
as	O
in	O
the	O
last	O
equality	O
following	O
after	O
some	O
work	O
by	O
differentiation	O
of	O
log	O
g	O
d	O
q	O
let	O
lk	O
d	O
log	O
fk	O
suppressing	O
from	O
the	O
notation	O
the	O
gradient	O
with	O
respect	O
to	O
of	O
lk	O
is	O
then	O
p	O
lk	O
d	O
p	O
fkfk	O
d	O
q	O
dpkfk	O
the	O
vector	B
dpkfk	O
has	O
components	O
pkj	O
gj	O
fkfk	O
d	O
wkj	O
pk	O
d	O
fk	O
this	O
gives	O
p	O
using	O
g	O
the	O
independent	O
score	O
functions	O
p	O
score	O
p	O
ly	O
d	O
q	O
o	O
d	O
pm	O
o	O
lemma	O
the	O
penalized	O
mle	B
o	O
satisfies	O
ykwk	O
which	O
is	O
lemma	O
pm	O
c	O
rm	O
lk	O
d	O
q	O
wk	O
adding	O
up	O
lk	O
over	O
the	O
full	B
sample	B
yields	O
the	O
overall	O
notes	O
and	O
details	O
pm	O
rm	O
where	O
is	O
the	O
true	O
value	O
of	O
or	O
o	O
standard	O
mle	B
theory	B
shows	O
that	O
the	O
random	O
variable	O
p	O
and	O
covariance	O
fisher	B
information	B
matrix	B
i	O
while	O
ically	O
approximates	O
i	O
substituting	O
in	O
c	O
rs	O
ly	O
c	O
rs	O
o	O
ly	O
ps	O
ly	O
has	O
mean	O
ly	O
asymptot	O
where	O
z	O
has	O
mean	O
and	O
covariance	O
i	O
this	O
gives	O
bias	B
o	O
and	O
var	O
o	O
as	O
in	O
lemma	O
note	O
that	O
the	O
bias	B
is	O
with	O
respect	O
to	O
a	O
true	O
parametric	B
model	O
and	O
is	O
a	O
consequence	O
of	O
the	O
penalization	O
the	O
sinc	B
kernel	I
the	O
fourier	O
transform	B
of	O
the	O
scaled	O
sinc	O
function	B
s	O
x	O
d	O
is	O
the	O
indicator	O
of	O
the	O
interval	B
while	O
that	O
of	O
exp	O
i	O
txj	O
formula	B
is	O
the	O
convolution	B
n	O
f	O
s	O
so	O
q	O
f	O
is	O
nx	O
f	O
has	O
the	O
product	O
transform	B
qf	O
d	O
i	O
ei	O
txj	O
n	O
n	O
the	O
effect	O
of	O
the	O
sinc	O
convolution	B
is	O
to	O
censor	O
the	O
high-frequency	O
t	B
n	O
f	O
or	O
nf	O
larger	O
yields	O
more	O
censoring	O
formula	B
components	O
of	O
has	O
upper	O
limits	O
because	O
of	O
all	O
of	O
this	O
is	O
due	O
to	O
stefanski	O
and	O
carroll	O
smoothers	O
other	O
than	O
the	O
sinc	B
kernel	I
have	O
been	O
suggested	O
in	O
the	O
literature	O
but	O
without	O
substantial	O
improvements	O
on	O
deconvolution	B
performance	O
conditional	B
expectation	O
efron	O
considers	O
estimating	O
ef	O
d	O
xg	O
and	O
other	O
such	O
conditional	B
expectations	O
both	O
for	O
f	O
modeling	O
and	O
for	O
g-modeling	B
ef	O
jx	O
d	O
xg	O
is	O
by	O
far	O
the	O
easiest	O
case	O
as	O
might	O
be	O
expected	O
from	O
the	O
simple	O
form	B
of	O
tweedie	O
s	O
estimate	B
epilogue	O
something	O
important	O
changed	O
in	O
the	O
world	O
of	O
statistics	B
in	O
the	O
new	O
millennium	O
twentieth-century	O
statistics	B
even	O
after	O
the	O
heated	O
expansion	O
of	O
its	O
late	O
period	O
could	O
still	O
be	O
contained	O
within	O
the	O
classic	O
bayesian	B
frequentist	B
fisherian	O
inferential	B
triangle	I
this	O
is	O
not	O
so	O
in	O
the	O
twenty-first	O
century	O
some	O
of	O
the	O
topics	O
discussed	O
in	O
part	O
iii	O
false-discovery	O
rates	O
post-selection	B
inference	B
empirical	B
bayes	I
modeling	O
the	O
lasso	B
fit	O
within	O
the	O
triangle	O
but	O
others	O
seem	O
to	O
have	O
escaped	O
heading	O
south	O
from	O
the	O
frequentist	B
corner	O
perhaps	O
in	O
the	O
direction	O
of	O
computer	O
science	O
the	O
escapees	O
were	O
the	O
large-scale	B
prediction	I
algorithms	I
of	O
chapters	O
neural	O
nets	O
deep	B
learning	I
boosting	B
random	O
forests	O
and	O
support-vector	O
machines	O
notably	O
missing	O
from	O
their	O
development	O
were	O
parametric	B
probability	O
models	B
the	O
building	O
blocks	O
of	O
classical	O
inference	B
prediction	O
algorithms	O
are	O
the	O
media	O
stars	O
of	O
the	O
big-data	B
era	I
it	O
is	O
worth	O
asking	O
why	O
they	O
have	O
taken	O
center	O
stage	O
and	O
what	O
it	O
means	O
for	O
the	O
future	O
of	O
the	O
statistics	B
discipline	O
the	O
why	O
is	O
easy	O
enough	O
prediction	O
is	O
commercially	O
valuable	O
modern	O
equipment	O
has	O
enabled	O
the	O
collection	O
of	O
mountainous	O
data	B
troves	O
which	O
the	O
data	B
miners	O
can	O
then	O
burrow	O
into	O
extracting	O
valuable	O
information	B
moreover	O
prediction	O
is	O
the	O
simplest	O
use	O
of	O
regression	B
theory	B
it	O
can	O
be	O
carried	O
out	O
successfully	O
without	O
probability	O
models	B
perhaps	O
with	O
the	O
assistance	O
of	O
nonparametric	B
analysis	B
tools	O
such	O
as	O
cross-validation	B
permutations	O
and	O
the	O
bootstrap	B
a	O
great	O
amount	O
of	O
ingenuity	O
and	O
experimentation	O
has	O
gone	O
into	O
the	O
development	O
of	O
modern	O
prediction	O
algorithms	O
with	O
statisticians	O
playing	O
an	O
important	O
but	O
not	O
dominant	O
there	O
is	O
no	O
shortage	O
of	O
impressive	O
success	O
stories	O
in	O
the	O
absence	O
of	O
optimality	B
criteria	B
either	O
frequentist	B
or	O
bayesian	B
the	O
prediction	O
community	O
grades	O
algorithmic	O
excellence	O
on	O
per	O
all	O
papers	O
mentioned	O
in	O
this	O
section	O
have	O
their	O
complete	O
references	O
in	O
the	O
bibliography	O
footnotes	O
will	O
identify	O
papers	O
not	O
fully	O
specified	O
in	O
the	O
text	O
epilogue	O
formance	O
within	O
a	O
catalog	O
of	O
often-visited	O
examples	O
such	O
as	O
the	O
spam	B
and	O
digits	O
data	B
sets	O
of	O
chapters	O
and	O
meanwhile	O
traditional	O
statistics	B
probability	O
models	B
optimality	B
criteria	B
bayes	O
priors	B
asymptotics	B
has	O
continued	O
successfully	O
along	O
on	O
a	O
parallel	O
track	O
pessimistically	O
or	O
optimistically	O
one	O
can	O
consider	O
this	O
as	O
a	O
bipolar	O
disorder	O
of	O
the	O
field	O
or	O
as	O
a	O
healthy	O
duality	O
that	O
is	O
bound	B
to	O
improve	O
both	O
branches	O
there	O
are	O
historical	O
and	O
intellectual	O
arguments	O
favoring	O
the	O
optimists	O
side	O
of	O
the	O
story	O
the	O
first	O
thing	O
to	O
say	O
is	O
that	O
the	O
current	O
situation	O
is	O
not	O
entirely	O
unprecedented	O
by	O
the	O
end	O
of	O
the	O
nineteenth	O
century	O
there	O
was	O
available	O
an	O
impressive	O
inventory	O
of	O
statistical	O
methods	O
bayes	O
theorem	B
least	B
squares	I
correlation	O
regression	B
the	O
multivariate	B
normal	B
distribution	B
but	O
these	O
existed	O
more	O
as	O
individual	O
algorithms	O
than	O
as	O
a	O
unified	O
discipline	O
statistics	B
as	O
a	O
distinct	O
intellectual	O
enterprise	O
was	O
not	O
yet	O
well-formed	O
a	O
small	O
but	O
crucial	O
step	O
forward	O
was	O
taken	O
in	O
when	O
the	O
astrophysicist	O
arthur	O
claimed	O
that	O
mean	B
absolute	I
deviation	I
was	O
superior	O
to	O
the	O
familiar	O
root	O
mean	O
square	O
estimate	B
for	O
the	O
standard	B
deviation	I
from	O
a	O
normal	B
sample	B
fisher	B
in	O
showed	O
that	O
this	O
was	O
wrong	O
and	O
moreover	O
in	O
a	O
clear	O
mathematical	O
sense	O
the	O
root	O
mean	O
square	O
was	O
the	O
best	O
possible	O
estimate	B
eddington	O
conceded	O
the	O
point	O
while	O
fisher	B
went	O
on	O
to	O
develop	O
the	O
theory	B
of	O
sufficiency	O
and	O
optimal	O
optimal	O
is	O
the	O
key	O
word	O
here	O
before	O
fisher	B
statisticians	O
didn	O
t	B
really	O
understand	O
estimation	B
the	O
same	O
can	O
be	O
said	O
now	O
about	O
prediction	O
despite	O
their	O
impressive	O
performance	O
on	O
a	O
raft	O
of	O
test	O
problems	O
it	O
might	O
still	O
be	O
possible	O
to	O
do	O
much	O
better	O
than	O
neural	O
nets	O
deep	B
learning	I
random	O
forests	O
and	O
boosting	B
or	O
perhaps	O
they	O
are	O
coming	O
close	O
to	O
some	O
as-yet	O
unknown	O
theoretical	O
minimum	O
it	O
is	O
the	O
job	O
of	O
statistical	O
inference	B
to	O
connect	O
dangling	O
algorithms	O
to	O
the	O
central	O
core	O
of	O
well-understood	O
methodology	O
the	O
connection	O
process	O
is	O
already	O
underway	O
section	O
showed	O
how	O
adaboost	O
the	O
original	O
machine	B
learning	I
algorithm	B
could	O
be	O
restated	O
as	O
a	O
close	O
cousin	O
of	O
logistic	B
regression	B
purely	O
empirical	B
approaches	O
like	O
the	O
common	B
task	I
framework	I
are	O
ultimately	O
unsatisfying	O
without	O
some	O
form	B
of	O
principled	O
justification	O
our	O
optimistic	O
scenario	O
has	O
the	O
big-datadata-science	O
prediction	O
world	O
rejoining	O
the	O
mainstream	O
of	O
statistical	O
inference	B
to	O
the	O
benefit	O
of	O
both	O
branches	O
this	O
empirical	B
approach	O
to	O
optimality	B
is	O
sometimes	O
codified	O
as	O
the	O
common	B
task	I
framework	I
and	O
donoho	O
eddington	O
became	O
world-famous	O
for	O
his	O
empirical	B
verification	O
of	O
einstein	O
s	O
relativity	O
theory	B
see	O
stigler	O
for	O
the	O
full	B
story	O
epilogue	O
development	O
of	O
the	O
statistics	B
discipline	O
since	O
the	O
end	O
of	O
the	O
nineteenth	O
century	O
as	O
discussed	O
in	O
the	O
text	O
whether	O
or	O
not	O
we	O
can	O
predict	O
the	O
future	O
of	O
statistics	B
we	O
can	O
at	O
least	O
examine	O
the	O
past	O
to	O
see	O
how	O
we	O
ve	O
gotten	O
where	O
we	O
are	O
the	O
next	O
figure	O
does	O
so	O
in	O
terms	O
of	O
a	O
new	O
triangle	O
diagram	O
this	O
time	O
with	O
the	O
poles	O
labeled	O
applications	O
mathematics	O
and	O
computation	O
mathematics	O
here	O
is	O
shorthand	O
for	O
the	O
mathematicallogical	O
justification	O
of	O
statistical	O
methods	O
computation	O
stands	O
for	O
the	O
empiricalnumerical	O
approach	O
statistics	B
is	O
a	O
branch	O
of	O
applied	O
mathematics	O
and	O
is	O
ultimately	O
judged	O
by	O
how	O
well	O
it	O
serves	O
the	O
world	O
of	O
applications	O
mathematical	O
logic	O
la	O
fisher	B
has	O
been	O
the	O
traditional	O
vehicle	O
for	O
the	O
development	O
and	O
understanding	O
of	O
statistical	O
methods	O
computation	O
slow	O
and	O
difficult	O
before	O
the	O
was	O
only	O
a	O
bottleneck	O
but	O
now	O
has	O
emerged	O
as	O
a	O
competitor	O
to	O
perhaps	O
a	O
seven-league	B
boots	I
enabler	O
of	O
mathematical	O
analysis	B
at	O
any	O
one	O
time	O
the	O
discipline	O
s	O
energy	O
and	O
excitement	O
is	O
directed	O
unequally	O
toward	O
the	O
three	O
poles	O
the	O
figure	O
attempts	O
in	O
admittedly	O
crude	O
fashion	O
to	O
track	O
the	O
changes	O
in	O
direction	O
over	O
the	O
past	O
years	O
epilogue	O
the	O
tour	O
begins	O
at	O
the	O
end	O
of	O
the	O
nineteenth	O
century	O
mathematicians	O
of	O
the	O
caliber	O
of	O
gauss	O
and	O
laplace	O
had	O
contributed	O
to	O
the	O
available	O
methodology	O
but	O
the	O
subsequent	O
development	O
was	O
almost	O
entirely	O
applicationsdriven	O
was	O
especially	O
influential	O
applying	O
the	O
gauss	O
laplace	O
formulation	O
to	O
census	O
data	B
and	O
his	O
average	O
man	O
a	O
modern	O
reader	O
will	O
search	O
almost	O
in	O
vain	O
for	O
any	O
mathematical	O
symbology	O
in	O
nineteenth-century	O
statistics	B
journals	O
karl	O
pearson	O
s	O
chi-square	O
paper	O
was	O
a	O
bold	O
step	O
into	O
the	O
new	O
century	O
applying	O
a	O
new	O
mathematical	O
tool	O
matrix	B
theory	B
in	O
the	O
service	O
of	O
statistical	O
methodology	O
he	O
and	O
weldon	O
went	O
on	O
to	O
found	O
biometrika	B
in	O
the	O
first	O
recognizably	O
modern	O
statistics	B
journal	O
pearson	O
s	O
paper	O
and	O
biometrika	B
launched	O
the	O
statistics	B
discipline	O
on	O
a	O
fifty-year	O
march	O
toward	O
the	O
mathematics	O
pole	O
of	O
the	O
triangle	O
student	O
s	O
t	B
statistic	B
was	O
a	O
crucial	O
first	O
result	O
in	O
small-sample	O
exact	O
inference	B
and	O
a	O
major	O
influence	O
on	O
fisher	B
s	O
thinking	O
fisher	B
s	O
great	O
estimation	B
paper	O
a	O
more	O
coherent	O
version	O
of	O
its	O
predecessor	O
it	O
introduced	O
a	O
host	O
of	O
fundamental	O
ideas	O
including	O
sufficiency	O
efficiency	O
fisher	B
information	B
maximum	B
likelihood	B
theory	B
and	O
the	O
notion	O
of	O
optimal	O
estimation	B
optimality	B
is	O
a	O
mark	O
of	O
maturity	O
in	O
mathematics	O
making	O
the	O
year	O
statistical	O
inference	B
went	O
from	O
a	O
collection	O
of	O
ingenious	O
techniques	O
to	O
a	O
coherent	O
discipline	O
this	O
represents	O
neyman	O
and	O
pearson	O
s	O
paper	O
on	O
optimal	O
hypothesis	B
testing	B
a	O
logical	O
completion	O
of	O
fisher	B
s	O
program	O
it	O
nevertheless	O
aroused	O
his	O
strong	O
antipathy	O
this	O
was	O
partly	O
personal	O
but	O
also	O
reflected	O
fisher	B
s	O
concern	O
that	O
mathematization	O
was	O
squeezing	O
intuitive	O
correctness	O
out	O
of	O
statistical	O
thinking	O
neyman	O
s	O
seminal	O
paper	O
on	O
confidence	O
intervals	B
his	O
sophisticated	O
mathematical	O
treatment	O
of	O
statistical	O
inference	B
was	O
a	O
harbinger	O
of	O
decision	O
theory	B
adolphe	O
quetelet	B
was	O
a	O
tireless	O
organizer	O
helping	O
found	O
the	O
royal	B
statistical	I
society	I
in	O
with	O
the	O
american	B
statistical	I
association	I
following	O
in	O
epilogue	O
the	O
publication	O
of	O
wald	O
s	O
statistical	O
decision	O
functions	O
decision	O
theory	B
completed	O
the	O
full	B
mathematization	O
of	O
statistical	O
inference	B
this	O
date	O
can	O
also	O
stand	O
for	O
savage	B
s	O
and	O
de	B
finetti	I
s	O
decision-theoretic	O
formulation	O
of	O
bayesian	B
inference	B
we	O
are	O
as	O
far	O
as	O
possible	O
from	O
the	O
applications	O
corner	O
of	O
the	O
triangle	O
now	O
and	O
it	O
is	O
fair	O
to	O
describe	O
the	O
as	O
a	O
nadir	O
of	O
the	O
influence	O
of	O
the	O
statistics	B
discipline	O
on	O
scientific	O
applications	O
the	O
arrival	O
of	O
electronic	O
computation	O
in	O
the	O
mid	O
began	O
the	O
process	O
of	O
stirring	O
statistics	B
out	O
of	O
its	O
inward-gazing	O
preoccupation	O
with	O
mathematical	O
structure	B
tukey	B
s	O
paper	O
the	O
future	O
of	O
data	B
analysis	B
argued	O
for	O
a	O
more	O
application-	O
and	O
computation-oriented	O
discipline	O
mosteller	O
and	O
tukey	B
later	O
suggested	O
changing	O
the	O
field	O
s	O
name	O
to	O
data	B
analysis	B
a	O
prescient	O
hint	O
of	O
today	O
s	O
data	B
science	I
cox	O
s	O
proportional	O
hazards	O
paper	O
immensely	O
useful	O
in	O
its	O
own	O
right	O
it	O
signaled	O
a	O
growing	O
interest	O
in	O
biostatistical	O
applications	O
and	O
particularly	O
survival	B
analysis	B
which	O
was	O
to	O
assert	O
its	O
scientific	O
importance	O
in	O
the	O
analysis	B
of	O
aids	O
epidemic	O
data	B
the	O
bootstrap	B
and	O
later	O
the	O
widespread	O
use	O
of	O
mcmc	B
electronic	O
computation	O
used	O
for	O
the	O
extension	O
of	O
classic	B
statistical	I
inference	B
this	O
stands	O
for	O
false-discovery	O
rates	O
and	O
a	O
year	O
later	O
the	O
both	O
are	O
computer-intensive	B
algorithms	O
firmly	O
rooted	O
in	O
the	O
ethos	O
of	O
statistical	O
inference	B
they	O
lead	O
however	O
in	O
different	O
directions	O
as	O
indicated	O
by	O
the	O
split	O
in	O
the	O
diagram	O
microarray	O
technology	O
inspires	O
enormous	O
interest	O
in	O
large-scale	O
inference	B
both	O
in	O
theory	B
and	O
as	O
applied	O
to	O
the	O
analysis	B
of	O
microbiological	O
data	B
benjamini	B
and	I
hochberg	I
and	O
tibshirani	O
epilogue	O
random	O
forests	O
it	O
joins	O
and	O
the	O
resurgence	O
of	O
neural	O
nets	O
in	O
the	O
ranks	O
of	O
machine	B
learning	I
prediction	O
algorithms	O
data	B
science	I
a	O
more	O
popular	O
successor	O
to	O
tukey	B
and	O
mosteller	O
s	O
data	B
analysis	B
at	O
one	O
extreme	O
it	O
seems	O
to	O
represent	O
a	O
statistics	B
discipline	O
without	O
parametric	B
probability	O
models	B
or	O
formal	O
inference	B
the	O
data	B
science	I
association	O
defines	O
a	O
practitioner	O
as	O
one	O
who	O
uses	O
scientific	O
methods	O
to	O
liberate	O
and	O
create	O
meaning	O
from	O
raw	O
data	B
in	O
practice	O
the	O
emphasis	O
is	O
on	O
the	O
algorithmic	O
processing	O
of	O
large	O
data	B
sets	O
for	O
the	O
extraction	O
of	O
useful	O
information	B
with	O
the	O
prediction	O
algorithms	O
as	O
exemplars	O
this	O
represents	O
the	O
traditional	O
line	O
of	O
statistical	O
thinking	O
of	O
the	O
kind	O
that	O
could	O
be	O
located	O
within	O
figure	O
but	O
now	O
energized	O
with	O
a	O
renewed	O
focus	O
on	O
applications	O
of	O
particular	O
applied	O
interest	O
are	O
biology	O
and	O
genetics	O
genome-wide	B
association	I
studies	I
show	O
a	O
different	O
face	O
of	O
big	O
data	B
prediction	O
is	O
important	O
but	O
not	O
sufficient	O
for	O
the	O
scientific	O
understanding	O
of	O
disease	O
a	O
cohesive	O
inferential	O
theory	B
was	O
forged	O
in	O
the	O
first	O
half	O
of	O
the	O
twentieth	O
century	O
but	O
unity	O
came	O
at	O
the	O
price	O
of	O
an	O
inwardly	O
focused	O
discipline	O
of	O
reduced	O
practical	O
utility	O
in	O
the	O
century	O
s	O
second	O
half	O
electronic	O
computation	O
unleashed	O
a	O
vast	O
expansion	O
of	O
useful	O
and	O
much	O
used	O
statistical	O
methodology	O
expansion	O
accelerated	O
at	O
the	O
turn	O
of	O
the	O
millennium	O
further	O
increasing	O
the	O
reach	O
of	O
statistical	O
thinking	O
but	O
now	O
at	O
the	O
price	O
of	O
intellectual	O
cohesion	O
it	O
is	O
tempting	O
but	O
risky	O
to	O
speculate	O
on	O
the	O
future	O
of	O
statistics	B
what	O
will	O
the	O
mathematics	O
applications	O
computation	O
diagram	O
look	O
like	O
say	O
years	O
from	O
now	O
the	O
appetite	O
for	O
statistical	O
analysis	B
seems	O
to	O
be	O
always	O
increasing	O
both	O
from	O
science	O
and	O
from	O
society	O
in	O
general	O
data	B
science	I
has	O
blossomed	O
in	O
response	B
but	O
so	O
has	O
the	O
traditional	O
wing	O
of	O
the	O
field	O
the	O
data-analytic	O
initiatives	O
represented	O
in	O
the	O
diagram	O
by	O
and	O
are	O
in	O
actuality	O
not	O
isolated	O
points	O
but	O
the	O
centers	O
of	O
overlapping	O
distributions	O
breiman	O
for	O
random	O
forests	O
freund	O
and	O
schapire	O
for	O
boosting	B
personalized	O
medicine	O
in	O
which	O
an	O
individual	O
s	O
genome	B
predicts	O
his	O
or	O
her	O
optimal	O
treatment	O
has	O
attracted	O
grail-like	O
attention	O
epilogue	O
a	O
hopeful	O
scenario	O
for	O
the	O
future	O
is	O
one	O
of	O
an	O
increasing	O
overlap	O
that	O
puts	O
data	B
science	I
on	O
a	O
solid	O
footing	O
while	O
leading	O
to	O
a	O
broader	O
general	O
formulation	O
of	O
statistical	O
inference	B
references	O
abu-mostafa	O
y	O
hints	O
neural	O
computation	O
achanta	O
r	B
and	O
hastie	O
t	B
telugu	O
ocr	O
framework	O
using	O
deep	B
learning	I
tech	O
rept	O
statistics	B
department	O
stanford	O
university	O
akaike	O
h	O
information	B
theory	B
and	O
an	O
extension	O
of	O
the	O
maximum	B
likelihood	B
principle	O
pages	O
of	O
second	O
international	O
symposium	O
on	O
information	B
theory	B
akad	O
emiai	O
kiad	O
o	O
budapest	O
anderson	O
t	B
w	O
an	O
introduction	O
to	O
multivariate	B
statistical	O
analysis	B
third	O
edn	O
wiley	O
series	O
in	O
probability	O
and	O
statistics	B
wiley-interscience	O
bastien	O
f	O
lamblin	O
p	O
pascanu	O
r	B
bergstra	O
j	O
goodfellow	O
i	O
j	O
bergeron	O
a	O
bouchard	O
n	O
and	O
bengio	O
y	O
theano	O
new	O
features	O
and	O
speed	O
improvements	O
deep	B
learning	I
and	O
unsupervised	O
feature	O
learning	O
nips	O
workshop	O
becker	O
r	B
chambers	O
j	O
and	O
wilks	O
a	O
the	O
new	O
s	B
language	I
a	O
programming	O
environment	O
for	O
data	B
analysis	B
and	O
graphics	O
pacific	O
grove	O
ca	O
wadsworth	O
and	O
brookscole	O
bellhouse	O
d	O
r	B
the	O
reverend	O
thomas	O
bayes	O
frs	O
a	O
biography	O
to	O
celebrate	O
the	O
tercentenary	O
of	O
his	O
birth	O
statist	O
sci	O
with	O
comments	O
and	O
a	O
rejoinder	O
by	O
the	O
author	O
bengio	O
y	O
courville	O
a	O
and	O
vincent	O
p	O
representation	O
learning	O
a	O
review	O
and	O
new	O
perspectives	O
ieee	O
transactions	O
on	O
pattern	O
analysis	B
and	O
machine	O
intelligence	O
benjamini	O
y	O
and	O
hochberg	O
y	O
controlling	O
the	O
false	B
discovery	I
rate	B
a	O
practical	O
and	O
powerful	O
approach	O
to	O
multiple	B
testing	B
j	O
roy	O
statist	O
soc	O
ser	O
b	O
benjamini	O
y	O
and	O
yekutieli	O
d	O
false	B
discovery	I
rate-adjusted	O
multiple	O
confi	O
dence	O
intervals	B
for	O
selected	O
parameters	O
j	O
amer	O
statist	O
assoc	O
berger	O
j	O
o	O
the	O
case	O
for	O
objective	O
bayesian	B
analysis	B
bayesian	B
anal	O
berger	O
j	O
o	O
and	O
pericchi	O
l	O
r	B
the	O
intrinsic	O
bayes	O
factor	B
for	O
model	B
selection	I
and	O
prediction	O
j	O
amer	O
statist	O
assoc	O
bergstra	O
j	O
breuleux	O
o	O
bastien	O
f	O
lamblin	O
p	O
pascanu	O
r	B
desjardins	O
g	O
turian	O
j	O
warde-farley	O
d	O
and	O
bengio	O
y	O
theano	O
a	O
cpu	O
and	O
gpu	O
math	O
expression	O
compiler	O
in	O
proceedings	O
of	O
the	O
python	O
for	O
scientific	O
computing	O
conference	O
berk	O
r	B
brown	O
l	O
buja	O
a	O
zhang	O
k	O
and	O
zhao	O
l	O
valid	O
post-selection	B
inference	B
ann	O
statist	O
references	O
berkson	O
j	O
application	O
of	O
the	O
logistic	O
function	B
to	O
bio-assay	O
j	O
amer	O
statist	O
assoc	O
bernardo	O
j	O
m	O
reference	O
posterior	O
distributions	O
for	O
bayesian	B
inference	B
j	O
roy	O
birch	O
m	O
w	O
the	O
detection	O
of	O
partial	O
association	O
i	O
the	O
case	O
j	O
roy	O
statist	O
statist	O
soc	O
ser	O
b	O
with	O
discussion	O
soc	O
ser	O
b	O
bishop	O
c	O
neural	O
networks	O
for	O
pattern	O
recognition	O
clarendon	O
press	O
oxford	O
boos	O
d	O
d	O
and	O
serfling	O
r	B
j	O
a	O
note	O
on	O
differentials	O
and	O
the	O
clt	O
and	O
lil	O
for	O
statistical	O
functions	O
with	O
application	O
to	O
m	O
ann	O
statist	O
boser	O
b	O
guyon	O
i	O
and	O
vapnik	B
v	O
a	O
training	O
algorithm	B
for	O
optimal	O
margin	B
classifiers	O
in	O
proceedings	O
of	O
colt	O
ii	O
breiman	O
l	O
bagging	B
predictors	B
mach	O
learn	O
breiman	O
l	O
arcing	O
classifiers	O
discussion	O
annals	O
of	O
statistics	B
breiman	O
l	O
random	O
forests	O
machine	B
learning	I
breiman	O
l	O
friedman	O
j	O
olshen	O
r	B
a	O
and	O
stone	O
c	O
j	O
classification	O
and	O
regression	B
trees	B
wadsworth	O
statisticsprobability	O
series	O
wadsworth	O
advanced	O
books	O
and	O
software	O
carlin	O
b	O
p	O
and	O
louis	O
t	B
a	O
bayes	O
and	O
empirical	B
bayes	I
methods	O
for	O
data	B
analysis	B
monographs	O
on	O
statistics	B
and	O
applied	O
probability	O
vol	O
chapman	O
hall	O
carlin	O
b	O
p	O
and	O
louis	O
t	B
a	O
bayes	O
and	O
empirical	B
bayes	I
methods	O
for	O
data	B
analysis	B
edn	O
texts	O
in	O
statistical	O
science	O
chapman	O
hallcrc	O
chambers	O
j	O
m	O
and	O
hastie	O
t	B
j	O
statistical	O
models	B
in	O
s	O
chapman	O
hall	O
computer	O
science	O
series	O
chapman	O
hall	O
cleveland	O
w	O
s	O
lowess	B
a	O
program	O
for	O
smoothing	B
scatterplots	O
by	O
robust	O
locally	O
weighted	O
regression	B
amer	O
statist	O
cox	O
d	O
r	B
the	O
regression	B
analysis	B
of	O
binary	O
sequences	O
j	O
roy	O
statist	O
soc	O
ser	O
b	O
cox	O
d	O
r	B
the	O
analysis	B
of	O
binary	O
data	B
methuen	O
s	O
monographs	O
on	O
applied	O
probability	O
and	O
statistics	B
methuen	O
co	O
cox	O
d	O
r	B
regression	B
models	B
and	O
life-tables	O
j	O
roy	O
statist	O
soc	O
ser	O
b	O
cox	O
d	O
r	B
partial	B
likelihood	B
biometrika	B
cox	O
d	O
r	B
and	O
hinkley	O
d	O
v	O
theoretical	O
statistics	B
chapman	O
hall	O
cox	O
d	O
r	B
and	O
reid	O
n	O
parameter	O
orthogonality	O
and	O
approximate	O
conditional	B
inference	B
j	O
roy	O
statist	O
soc	O
ser	O
b	O
with	O
a	O
discussion	O
crowley	O
j	O
asymptotic	O
normality	O
of	O
a	O
new	O
nonparametric	B
statistic	B
for	O
use	O
in	O
organ	O
transplant	O
studies	O
j	O
amer	O
statist	O
assoc	O
de	B
finetti	I
b	O
probability	O
induction	O
and	O
statistics	B
the	O
art	O
of	O
guessing	O
john	O
wiley	O
sons	O
london-new	O
york-sydney	O
dembo	O
a	O
cover	O
t	B
m	O
and	O
thomas	O
j	O
a	O
information-theoretic	O
inequalities	O
ieee	O
trans	O
inform	O
theory	B
dempster	O
a	O
p	O
laird	O
n	O
m	O
and	O
rubin	O
d	O
b	O
maximum	B
likelihood	B
from	O
incomplete	O
data	B
via	O
the	O
em	B
algorithm	B
j	O
roy	O
statist	O
soc	O
ser	O
b	O
diaconis	O
p	O
and	O
ylvisaker	O
d	O
conjugate	B
priors	B
for	O
exponential	O
families	O
ann	O
statist	O
references	O
diciccio	O
t	B
and	O
efron	O
b	O
more	O
accurate	O
confidence	O
intervals	B
in	O
exponential	O
families	O
biometrika	B
donoho	O
d	O
l	O
years	O
of	O
data	B
science	I
r-bloggers	O
www	O
r-bloggers	O
edwards	O
a	O
w	O
f	O
likelihood	B
expanded	O
edn	O
johns	O
hopkins	O
university	O
press	O
revised	O
reprint	O
of	O
the	O
original	O
efron	O
b	O
the	O
two	O
sample	B
problem	O
with	O
censored	O
data	B
pages	O
of	O
proc	O
berkeley	O
symp	O
math	O
statist	O
and	O
prob	O
vol	O
university	O
of	O
california	O
press	O
efron	O
b	O
defining	O
the	O
curvature	O
of	O
a	O
statistical	O
problem	O
applications	O
to	O
second	O
order	O
efficiency	O
ann	O
statist	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
author	O
efron	O
b	O
the	O
efficiency	O
of	O
cox	O
s	O
likelihood	B
function	B
for	O
censored	O
data	B
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
bootstrap	B
methods	O
another	O
look	O
at	O
the	O
jackknife	B
ann	O
statist	O
efron	O
b	O
the	O
jackknife	B
the	O
bootstrap	B
and	O
other	O
resampling	B
plans	B
cbms-nsf	O
regional	O
conference	O
series	O
in	O
applied	O
mathematics	O
vol	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
efron	O
b	O
estimating	O
the	O
error	O
rate	B
of	O
a	O
prediction	O
rule	B
improvement	O
on	O
cross	O
validation	O
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
bootstrap	B
confidence	O
intervals	B
for	O
a	O
class	O
of	O
parametric	B
problems	O
biometrika	B
efron	O
b	O
how	O
biased	O
is	O
the	O
apparent	B
error	I
rate	B
of	O
a	O
prediction	O
rule	B
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
better	O
bootstrap	B
confidence	O
intervals	B
j	O
amer	O
statist	O
assoc	O
with	O
comments	O
and	O
a	O
rejoinder	O
by	O
the	O
author	O
efron	O
b	O
logistic	B
regression	B
survival	B
analysis	B
and	O
the	O
kaplan	O
meier	O
curve	O
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
bayes	O
and	O
likelihood	B
calculations	O
from	O
confidence	O
intervals	B
biometrika	B
efron	O
b	O
r	B
a	O
fisher	B
in	O
the	O
century	O
paper	O
presented	O
at	O
the	O
r	B
a	O
fisher	B
lecture	O
statist	O
sci	O
with	O
comments	O
and	O
a	O
rejoinder	O
by	O
the	O
author	O
efron	O
b	O
the	O
estimation	B
of	O
prediction	O
error	O
covariance	O
penalties	O
and	O
crossvalidation	O
j	O
amer	O
statist	O
assoc	O
with	O
comments	O
and	O
a	O
rejoinder	O
by	O
the	O
author	O
efron	O
b	O
large-scale	O
inference	B
empirical	B
bayes	I
methods	O
for	O
estimation	B
testing	B
and	O
prediction	O
institute	O
of	O
mathematical	O
statistics	B
monographs	O
vol	O
cambridge	O
university	O
press	O
efron	O
b	O
tweedie	O
s	O
formula	B
and	O
selection	B
bias	B
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
estimation	B
and	O
accuracy	B
after	B
model	B
selection	I
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
two	O
modeling	O
strategies	O
for	O
empirical	B
bayes	I
estimation	B
statist	O
sci	O
efron	O
b	O
frequentist	B
accuracy	B
of	O
bayesian	B
estimates	O
j	O
roy	O
statist	O
soc	O
ser	O
b	O
references	O
efron	O
b	O
empirical	B
bayes	I
deconvolution	B
estimates	O
biometrika	B
efron	O
b	O
and	O
feldman	O
d	O
compliance	B
as	O
an	O
explanatory	O
variable	O
in	O
clinical	O
trials	O
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
and	O
gous	O
a	O
scales	O
of	O
evidence	O
for	O
model	B
selection	I
fisher	B
versus	O
jeffreys	B
pages	O
of	O
model	B
selection	I
ims	O
lecture	O
notes	O
monograph	O
series	O
vol	O
beachwood	O
oh	O
institute	O
of	O
mathematics	O
and	O
statististics	O
with	O
discussion	O
and	O
a	O
rejoinder	O
by	O
the	O
authors	O
efron	O
b	O
and	O
hinkley	O
d	O
v	O
assessing	O
the	O
accuracy	B
of	O
the	O
maximum	B
likelihood	B
estimator	B
observed	O
versus	O
expected	O
fisher	B
information	B
biometrika	B
with	O
comments	O
and	O
a	O
reply	O
by	O
the	O
authors	O
efron	O
b	O
and	O
morris	O
c	O
limiting	O
the	O
risk	O
of	O
bayes	O
and	O
empirical	B
bayes	I
estima	O
tors	O
ii	O
the	O
empirical	B
bayes	I
case	O
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
and	O
morris	O
c	O
stein	O
s	O
paradox	B
in	O
statistics	B
scientific	O
american	O
efron	O
b	O
and	O
petrosian	O
v	O
a	O
simple	O
test	O
of	O
independence	O
for	O
truncated	O
data	B
with	O
applications	O
to	O
redshift	O
surveys	O
astrophys	O
j	O
efron	O
b	O
and	O
stein	O
c	O
the	O
jackknife	B
estimate	B
of	O
variance	B
ann	O
statist	O
efron	O
b	O
and	O
thisted	O
r	B
estimating	O
the	O
number	O
of	O
unseen	O
species	O
how	O
many	O
words	O
did	O
shakespeare	O
know	O
biometrika	B
efron	O
b	O
and	O
tibshirani	O
r	B
an	O
introduction	O
to	O
the	O
bootstrap	B
monographs	O
on	O
statistics	B
and	O
applied	O
probability	O
vol	O
chapman	O
hall	O
efron	O
b	O
and	O
tibshirani	O
r	B
improvements	O
on	O
cross-validation	B
the	O
boot	O
strap	O
method	B
j	O
amer	O
statist	O
assoc	O
efron	O
b	O
hastie	O
t	B
johnstone	O
i	O
and	O
tibshirani	O
r	B
least	O
angle	O
regression	B
annals	O
of	O
statistics	B
discussion	O
and	O
a	O
rejoinder	O
by	O
the	O
authors	O
finney	O
d	O
j	O
the	O
estimation	B
from	O
individual	O
records	O
of	O
the	O
relationship	O
between	O
dose	O
and	O
quantal	O
response	B
biometrika	B
fisher	B
r	B
a	O
frequency	O
distribution	B
of	O
the	O
values	O
of	O
the	O
correlation	O
coefficient	O
in	O
samples	O
from	O
an	O
indefinitely	O
large	O
population	O
biometrika	B
fisher	B
r	B
a	O
theory	B
of	O
statistical	O
estimation	B
math	O
proc	O
cambridge	O
phil	O
soc	O
fisher	B
r	B
a	O
inverse	O
probability	O
math	O
proc	O
cambridge	O
phil	O
soc	O
fisher	B
r	B
a	O
corbet	O
a	O
and	O
williams	O
c	O
the	O
relation	O
between	O
the	O
number	O
of	O
species	O
and	O
the	O
number	O
of	O
individuals	O
in	O
a	O
random	O
sample	B
of	O
an	O
animal	O
population	O
j	O
anim	O
ecol	O
fithian	O
w	O
sun	O
d	O
and	O
taylor	O
j	O
optimal	O
inference	B
after	B
model	B
selection	I
arxiv	O
e-prints	O
oct	O
freund	O
y	O
and	O
schapire	O
r	B
experiments	O
with	O
a	O
new	O
boosting	B
algorithm	B
pages	O
of	O
machine	B
learning	I
proceedings	O
of	O
the	O
thirteenth	O
international	O
conference	O
morgan	O
kauffman	O
san	O
francisco	O
freund	O
y	O
and	O
schapire	O
r	B
a	O
decision-theoretic	O
generalization	O
of	O
online	O
learning	O
and	O
an	O
application	O
to	O
boosting	B
journal	O
of	O
computer	O
and	O
system	O
sciences	O
friedman	O
j	O
greedy	O
function	B
approximation	O
a	O
gradient	B
boosting	B
machine	O
an	O
nals	O
of	O
statistics	B
references	O
friedman	O
j	O
and	O
popescu	O
b	O
predictive	O
learning	O
via	O
rule	B
ensembles	O
tech	O
rept	O
stanford	O
university	O
friedman	O
j	O
hastie	O
t	B
and	O
tibshirani	O
r	B
additive	O
logistic	B
regression	B
a	O
statis	O
tical	O
view	O
of	O
boosting	B
discussion	O
annals	O
of	O
statistics	B
friedman	O
j	O
hastie	O
t	B
and	O
tibshirani	O
r	B
glmnet	B
lasso	B
and	O
elastic-net	O
regu	O
larized	O
generalized	O
linear	B
models	B
r	B
package	O
version	O
friedman	O
j	O
hastie	O
t	B
and	O
tibshirani	O
r	B
regularization	B
paths	O
for	O
generalized	O
linear	B
models	B
via	O
coordinate	O
descent	O
journal	O
of	O
statistical	O
software	O
geisser	O
s	O
a	O
predictive	O
approach	O
to	O
the	O
random	O
effect	O
model	O
biometrika	B
gerber	O
m	O
and	O
chopin	O
n	O
sequential	O
quasi	O
monte	O
carlo	O
j	O
roy	O
statist	O
soc	O
b	O
with	O
discussion	O
doi	O
gholami	O
s	O
janson	O
l	O
worhunsky	O
d	O
j	O
tran	O
t	B
b	O
squires	O
malcolm	O
i	O
jin	O
l	O
x	O
spolverato	O
g	O
votanopoulos	O
k	O
i	O
schmidt	O
c	O
weber	O
s	O
m	O
bloomston	O
m	O
cho	O
c	O
s	O
levine	O
e	O
a	O
fields	O
r	B
c	O
pawlik	O
t	B
m	O
maithel	O
s	O
k	O
efron	O
b	O
norton	O
j	O
a	O
and	O
poultsides	O
g	O
a	O
number	O
of	O
lymph	O
nodes	B
removed	O
and	O
survival	O
after	O
gastric	O
cancer	O
resection	O
an	O
analysis	B
from	O
the	O
us	O
gastric	O
cancer	O
collaborative	O
j	O
amer	O
coll	O
surg	O
good	O
i	O
and	O
toulmin	O
g	O
the	O
number	O
of	O
new	O
species	O
and	O
the	O
increase	O
in	O
popu	O
lation	O
coverage	B
when	O
a	O
sample	B
is	O
increased	O
biometrika	B
hall	O
p	O
theoretical	O
comparison	O
of	O
bootstrap	B
confidence	O
intervals	B
ann	O
statist	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
author	O
hampel	O
f	O
r	B
the	O
influence	O
curve	O
and	O
its	O
role	O
in	O
robust	B
estimation	B
j	O
amer	O
statist	O
assoc	O
hampel	O
f	O
r	B
ronchetti	O
e	O
m	O
rousseeuw	O
p	O
j	O
and	O
stahel	O
w	O
a	O
robust	O
statistics	B
the	O
approach	O
based	O
on	O
influence	O
functions	O
wiley	O
series	O
in	O
probability	O
and	O
mathematical	O
statistics	B
john	O
wiley	O
sons	O
harford	O
t	B
big	O
data	B
a	O
big	O
mistake	O
significance	O
hastie	O
t	B
and	O
loader	O
c	O
local	B
regression	B
automatic	O
kernel	O
carpentry	O
discussion	O
statistical	O
science	O
hastie	O
t	B
and	O
tibshirani	O
r	B
generalized	O
additive	O
models	B
chapman	O
and	O
hall	O
hastie	O
t	B
and	O
tibshirani	O
r	B
efficient	O
quadratic	O
regularization	B
for	O
expression	O
arrays	O
biostatistics	O
hastie	O
t	B
tibshirani	O
r	B
and	O
friedman	O
j	O
the	O
elements	O
of	O
statistical	O
learning	O
data	B
mining	O
inference	B
and	O
prediction	O
second	O
edn	O
springer	O
series	O
in	O
statistics	B
springer	O
hastie	O
t	B
tibshirani	O
r	B
and	O
wainwright	O
m	O
statistical	O
learning	B
with	I
sparsity	B
the	O
lasso	B
and	O
generalizations	O
chapman	O
and	O
hall	O
crc	O
press	O
hoeffding	O
w	O
the	O
large-sample	O
power	O
of	O
tests	O
based	O
on	O
permutations	O
of	O
obser	O
vations	O
ann	O
math	O
statist	O
hoeffding	O
w	O
asymptotically	O
optimal	O
tests	O
for	O
multinomial	O
distributions	O
ann	O
math	O
statist	O
hoerl	O
a	O
e	O
and	O
kennard	O
r	B
w	O
ridge	B
regression	B
biased	B
estimation	B
for	O
nonor	O
thogonal	O
problems	O
technometrics	O
huber	O
p	O
j	O
robust	B
estimation	B
of	O
a	O
location	O
parameter	O
ann	O
math	O
statist	O
references	O
jaeckel	O
l	O
a	O
estimating	O
regression	B
coefficients	O
by	O
minimizing	O
the	O
dispersion	O
of	O
the	O
residuals	O
ann	O
math	O
statist	O
james	O
w	O
and	O
stein	O
c	O
estimation	B
with	O
quadratic	O
loss	O
pages	O
of	O
proc	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	B
and	O
probability	O
vol	O
i	O
university	O
of	O
california	O
press	O
jansen	O
l	O
fithian	O
w	O
and	O
hastie	O
t	B
effective	O
degrees	B
of	I
freedom	I
a	O
flawed	O
metaphor	O
biometrika	B
javanmard	O
a	O
and	O
montanari	O
a	O
confidence	O
intervals	B
and	O
hypothesis	B
testing	B
for	O
high-dimensional	O
regression	B
j	O
of	O
machine	B
learning	I
res	O
jaynes	O
e	O
prior	B
probabilities	B
ieee	O
trans	O
syst	O
sci	O
cybernet	O
jeffreys	B
h	O
theory	B
of	O
probability	O
third	O
ed	O
clarendon	O
press	O
johnson	O
n	O
l	O
and	O
kotz	O
s	O
distributions	O
in	O
statistics	B
discrete	O
distributions	O
houghton	O
mifflin	O
co	O
johnson	O
n	O
l	O
and	O
kotz	O
s	O
distributions	O
in	O
statistics	B
continuous	O
univariate	O
distributions	O
houghton	O
mifflin	O
co	O
johnson	O
n	O
l	O
and	O
kotz	O
s	O
distributions	O
in	O
statistics	B
continuous	O
univariate	O
distributions	O
houghton	O
mifflin	O
co	O
johnson	O
n	O
l	O
and	O
kotz	O
s	O
distributions	O
in	O
statistics	B
continuous	O
multivariate	B
distributions	O
john	O
wiley	O
sons	O
kaplan	O
e	O
l	O
and	O
meier	O
p	O
nonparametric	B
estimation	B
from	O
incomplete	O
obser	O
vations	O
j	O
amer	O
statist	O
assoc	O
kass	O
r	B
e	O
and	O
raftery	O
a	O
e	O
bayes	O
factors	O
j	O
amer	O
statist	O
assoc	O
kass	O
r	B
e	O
and	O
wasserman	O
l	O
the	O
selection	O
of	O
prior	B
distributions	O
by	O
formal	O
rules	O
j	O
amer	O
statist	O
assoc	O
kuffner	O
r	B
zach	O
n	O
norel	O
r	B
hawe	O
j	O
schoenfeld	O
d	O
wang	O
l	O
li	O
g	O
fang	O
l	O
mackey	O
l	O
hardiman	O
o	O
cudkowicz	O
m	O
sherman	O
a	O
ertaylan	O
g	O
grossewentrup	O
m	O
hothorn	O
t	B
van	O
ligtenberg	O
j	O
macke	O
j	O
h	O
meyer	O
t	B
scholkopf	O
b	O
tran	O
l	O
vaughan	O
r	B
stolovitzky	O
g	O
and	O
leitner	O
m	O
l	O
crowdsourced	O
analysis	B
of	O
clinical	O
trial	O
data	B
to	O
predict	O
amyotrophic	O
lateral	O
sclerosis	O
progression	O
nat	O
biotech	O
lecun	O
y	O
and	O
cortes	O
c	O
mnist	O
handwritten	O
digit	O
database	O
httpyann	O
lecun	O
comexdbmnist	O
lecun	O
y	O
bengio	O
y	O
and	O
hinton	O
g	O
deep	B
learning	I
nature	O
lee	O
j	O
sun	O
d	O
sun	O
y	O
and	O
taylor	O
j	O
exact	O
post-selection	B
inference	B
with	O
application	O
to	O
the	O
lasso	B
annals	O
of	O
statistics	B
lehmann	O
e	O
l	O
theory	B
of	O
point	O
estimation	B
wiley	O
series	O
in	O
probability	O
and	O
mathematical	O
statistics	B
john	O
wiley	O
sons	O
leslie	O
c	O
eskin	O
e	O
cohen	O
a	O
weston	O
j	O
and	O
noble	O
w	O
s	O
mismatch	O
string	O
kernels	O
for	O
discriminative	O
pretein	O
classification	O
bioinformatics	O
liaw	O
a	O
and	O
wiener	O
m	O
classification	O
and	O
regression	B
by	O
randomforest	B
r	B
news	O
liberman	O
m	O
reproducible	O
research	O
and	O
the	O
common	O
task	O
method	B
simons	O
foundation	O
frontiers	O
of	O
data	B
science	I
lecture	O
april	O
video	O
available	O
references	O
lockhart	O
r	B
taylor	O
j	O
tibshirani	O
r	B
and	O
tibshirani	O
r	B
a	O
significance	O
test	O
for	O
the	O
lasso	B
annals	O
of	O
statistics	B
with	O
discussion	O
and	O
a	O
rejoinder	O
by	O
the	O
authors	O
lynden-bell	O
d	O
a	O
method	B
for	O
allowing	O
for	O
known	O
observational	O
selection	O
in	O
small	O
samples	O
applied	O
to	O
quasars	O
mon	O
not	O
roy	O
astron	O
soc	O
mallows	O
c	O
l	O
some	O
comments	O
on	O
cp	B
technometrics	O
mantel	O
n	O
and	O
haenszel	O
w	O
statistical	O
aspects	O
of	O
the	O
analysis	B
of	O
data	B
from	O
retrospective	O
studies	O
of	O
disease	O
j	O
natl	O
cancer	O
inst	O
mardia	O
k	O
v	O
kent	O
j	O
t	B
and	O
bibby	O
j	O
m	O
multivariate	B
analysis	B
academic	O
press	O
mccullagh	O
p	O
and	O
nelder	O
j	O
generalized	O
linear	B
models	B
monographs	O
on	O
statis	O
tics	O
and	O
applied	O
probability	O
chapman	O
hall	O
mccullagh	O
p	O
and	O
nelder	O
j	O
generalized	O
linear	B
models	B
second	O
edn	O
mono	O
graphs	O
on	O
statistics	B
and	O
applied	O
probability	O
chapman	O
hall	O
metropolis	O
n	O
rosenbluth	O
a	O
w	O
rosenbluth	O
m	O
n	O
teller	O
a	O
h	O
and	O
teller	O
e	O
equation	B
of	O
state	O
calculations	O
by	O
fast	O
computing	O
machines	O
j	O
chem	O
phys	O
miller	O
jr	O
r	B
g	O
a	O
trustworthy	O
jackknife	B
ann	O
math	O
statist	O
miller	O
jr	O
r	B
g	O
simultaneous	O
statistical	O
inference	B
second	O
edn	O
springer	O
series	O
in	O
statistics	B
new	O
york	O
springer-verlag	O
nesterov	O
y	O
gradient	O
methods	O
for	O
minimizing	O
composite	O
functions	O
mathemati	O
cal	O
programming	O
neyman	O
j	O
outline	O
of	O
a	O
theory	B
of	O
statistical	O
estimation	B
based	O
on	O
the	O
classical	O
theory	B
of	O
probability	O
phil	O
trans	O
roy	O
soc	O
neyman	O
j	O
frequentist	B
probability	O
and	O
frequentist	B
statistics	B
synthese	O
neyman	O
j	O
and	O
pearson	O
e	O
s	O
on	O
the	O
problem	O
of	O
the	O
most	O
efficient	O
tests	O
of	O
statistical	O
hypotheses	O
phil	O
trans	O
roy	O
soc	O
a	O
ng	O
a	O
neural	O
networks	O
httpdeeplearning	O
stanford	O
edu	O
wikiindex	O
phpneural	O
networks	O
lecture	O
notes	O
ngiam	O
j	O
chen	O
z	O
chia	O
d	O
koh	O
p	O
w	O
le	O
q	O
v	O
and	O
ng	O
a	O
tiled	O
convolutional	O
neural	O
networks	O
pages	O
of	O
lafferty	O
j	O
williams	O
c	O
shawetaylor	O
j	O
zemel	O
r	B
and	O
culotta	O
a	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
curran	O
associates	O
inc	O
o	O
hagan	O
a	O
fractional	O
bayes	O
factors	O
for	O
model	O
comparison	O
j	O
roy	O
statist	O
soc	O
ser	O
b	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
author	O
park	O
t	B
and	O
casella	O
g	O
the	O
bayesian	B
lasso	B
j	O
amer	O
statist	O
assoc	O
pearson	O
k	O
on	O
the	O
criterion	O
that	O
a	O
given	O
system	O
of	O
deviations	O
from	O
the	O
probable	O
in	O
the	O
case	O
of	O
a	O
correlated	O
system	O
of	O
variables	O
is	O
such	O
that	O
it	O
can	O
be	O
reasonably	O
supposed	O
to	O
have	O
arisen	O
from	O
random	O
sampling	O
phil	O
mag	O
pritchard	O
j	O
stephens	O
m	O
and	O
donnelly	O
p	O
inference	B
of	O
population	O
structure	B
using	O
multilocus	O
genotype	O
data	B
genetics	O
quenouille	O
m	O
h	O
notes	O
on	O
bias	B
in	O
estimation	B
biometrika	B
r	B
core	O
team	O
r	B
a	O
language	O
and	O
environment	O
for	O
statistical	O
computing	O
r	B
foundation	O
for	O
statistical	O
computing	O
vienna	O
austria	O
references	O
ridgeway	O
g	O
generalized	O
boosted	O
models	B
a	O
guide	O
to	O
the	O
gbm	B
package	O
avail	O
able	O
online	O
ridgeway	O
g	O
and	O
macdonald	O
j	O
m	O
doubly	O
robust	O
internal	B
benchmarking	O
and	O
false	B
discovery	I
rates	O
for	O
detecting	O
racial	O
bias	B
in	O
police	B
stops	O
j	O
amer	O
statist	O
assoc	O
ripley	O
b	O
d	O
pattern	O
recognition	O
and	O
neural	O
networks	O
cambridge	O
university	O
press	O
robbins	O
h	O
an	O
empirical	B
bayes	I
approach	O
to	O
statistics	B
pages	O
of	O
proc	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	B
and	O
probability	O
vol	O
i	O
university	O
of	O
california	O
press	O
rosset	O
s	O
zhu	O
j	O
and	O
hastie	O
t	B
margin	B
maximizing	O
loss	O
functions	O
in	O
thrun	O
s	O
saul	O
l	O
and	O
sch	O
olkopf	O
b	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
mit	O
press	O
rubin	O
d	O
b	O
the	O
bayesian	B
bootstrap	B
ann	O
statist	O
savage	B
l	O
j	O
the	O
foundations	O
of	O
statistics	B
john	O
wiley	O
sons	O
chapman	O
hill	O
schapire	O
r	B
the	O
strength	O
of	O
weak	O
learnability	O
machine	B
learning	I
schapire	O
r	B
and	O
freund	O
y	O
boosting	B
foundations	O
and	O
algorithms	O
mit	O
press	O
scheff	O
e	O
h	O
a	O
method	B
for	O
judging	O
all	O
contrasts	O
in	O
the	O
analysis	B
of	O
variance	B
biometrika	B
sch	O
olkopf	O
b	O
and	O
smola	O
a	O
learning	B
with	I
kernels	O
support	O
vector	B
machines	O
regularization	B
optimization	O
and	O
beyond	O
computation	O
and	O
machine	B
learning	I
mit	O
press	O
schwarz	O
g	O
estimating	O
the	O
dimension	O
of	O
a	O
model	O
ann	O
statist	O
senn	O
s	O
a	O
note	O
concerning	O
a	O
selection	O
paradox	B
of	O
dawid	O
s	O
amer	O
statist	O
soric	O
b	O
statistical	O
discoveries	O
and	O
effect-size	O
estimation	B
j	O
amer	O
statist	O
assoc	O
spevack	O
m	O
a	O
complete	O
and	O
systematic	O
concordance	O
to	O
the	O
works	O
of	O
shake	O
speare	O
vol	O
georg	O
olms	O
verlag	O
srivastava	O
n	O
hinton	O
g	O
krizhevsky	O
a	O
sutskever	O
i	O
and	O
salakhutdinov	O
r	B
dropout	O
a	O
simple	O
way	O
to	O
prevent	O
neural	O
networks	O
from	O
overfitting	O
j	O
of	O
machine	B
learning	I
res	O
stefanski	O
l	O
and	O
carroll	O
r	B
j	O
deconvoluting	O
kernel	O
density	B
estimators	O
statis	O
tics	O
stein	O
c	O
inadmissibility	O
of	O
the	O
usual	O
estimator	B
for	O
the	O
mean	O
of	O
a	O
multivariate	B
normal	B
distribution	B
pages	O
of	O
proc	O
berkeley	O
symposium	O
on	O
mathematical	O
statististics	O
and	O
probability	O
vol	O
i	O
university	O
of	O
california	O
press	O
stein	O
c	O
estimation	B
of	O
the	O
mean	O
of	O
a	O
multivariate	B
normal	B
distribution	B
ann	O
statist	O
stein	O
c	O
on	O
the	O
coverage	B
probability	O
of	O
confidence	O
sets	O
based	O
on	O
a	O
prior	B
distribution	B
pages	O
of	O
sequential	O
methods	O
in	O
statistics	B
banach	O
center	O
publication	O
vol	O
pwn	O
warsaw	O
stigler	O
s	O
m	O
how	O
ronald	O
fisher	B
became	O
a	O
mathematical	O
statistician	O
math	O
sci	O
hum	O
math	O
soc	O
sci	O
references	O
stone	O
m	O
cross-validatory	O
choice	O
and	O
assessment	O
of	O
statistical	O
predictions	O
j	O
roy	O
statist	O
soc	O
b	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
author	O
storey	O
j	O
d	O
taylor	O
j	O
and	O
siegmund	O
d	O
strong	O
control	B
conservative	O
point	O
estimation	B
and	O
simultaneous	O
conservative	O
consistency	O
of	O
false	B
discovery	I
rates	O
a	O
unified	O
approach	O
j	O
roy	O
statist	O
soc	O
b	O
tanner	O
m	O
a	O
and	O
wong	O
w	O
h	O
the	O
calculation	O
of	O
posterior	O
distributions	O
by	O
data	B
augmentation	O
j	O
amer	O
statist	O
assoc	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
authors	O
taylor	O
j	O
loftus	O
j	O
and	O
tibshirani	O
r	B
tests	O
in	O
adaptive	B
regression	B
via	O
the	O
kac	O
rice	O
formula	B
annals	O
of	O
statistics	B
thisted	O
r	B
and	O
efron	O
b	O
did	O
shakespeare	O
write	O
a	O
newly-discovered	O
poem	O
biometrika	B
tibshirani	O
r	B
noninformative	O
priors	B
for	O
one	O
parameter	O
of	O
many	O
biometrika	B
tibshirani	O
r	B
regression	B
shrinkage	B
and	O
selection	O
via	O
the	O
lasso	B
j	O
roy	O
statist	O
soc	O
b	O
tibshirani	O
r	B
a	O
simple	O
method	B
for	O
assessing	O
sample	B
sizes	O
in	O
microarray	O
experi	O
ments	O
bmc	O
bioinformatics	O
tibshirani	O
r	B
bien	O
j	O
friedman	O
j	O
hastie	O
t	B
simon	O
n	O
taylor	O
j	O
and	O
tibshirani	O
r	B
strong	B
rules	I
for	O
discarding	O
predictors	B
in	O
lasso-type	O
problems	O
j	O
roy	O
statist	O
soc	O
b	O
tibshirani	O
r	B
tibshirani	O
r	B
taylor	O
j	O
loftus	O
j	O
and	O
reid	O
s	O
selectiveinfer	O
ence	O
tools	O
for	O
post-selection	B
inference	B
r	B
package	O
version	O
tukey	B
j	O
w	O
bias	B
and	O
confidence	O
in	O
not-quite	O
large	O
samples	O
in	O
abstracts	O
of	O
papers	O
ann	O
math	O
statist	O
tukey	B
j	O
w	O
a	O
survey	O
of	O
sampling	O
from	O
contaminated	O
distributions	O
pages	O
of	O
contributions	O
to	O
probability	O
and	O
statistics	B
essays	O
in	O
honor	O
of	O
harold	O
hotelling	O
olkin	O
et	O
al	O
ed	O
stanford	O
university	O
press	O
tukey	B
j	O
w	O
the	O
future	O
of	O
data	B
analysis	B
ann	O
math	O
statist	O
tukey	B
j	O
w	O
exploratory	O
data	B
analysis	B
behavioral	O
science	O
series	O
addison	O
wesley	O
van	O
de	O
geer	O
s	O
b	O
uhlmann	O
p	O
ritov	O
y	O
and	O
dezeure	O
r	B
on	O
asymptotically	O
optimal	O
confidence	O
regions	O
and	O
tests	O
for	O
high-dimensional	O
models	B
annals	O
of	O
statistics	B
vapnik	B
v	O
the	O
nature	O
of	O
statistical	O
learning	O
theory	B
springer	O
wager	O
s	O
wang	O
s	O
and	O
liang	O
p	O
s	O
dropout	O
training	O
as	O
adaptive	B
regularization	B
pages	O
of	O
burges	O
c	O
bottou	O
l	O
welling	O
m	O
ghahramani	O
z	O
and	O
weinberger	O
k	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
curran	O
associates	O
inc	O
wager	O
s	O
hastie	O
t	B
and	O
efron	O
b	O
confidence	O
intervals	B
for	O
random	O
forests	O
the	O
jacknife	O
and	O
the	O
infintesimal	O
jacknife	O
j	O
of	O
machine	B
learning	I
res	O
wahba	O
g	O
spline	O
models	B
for	O
observational	O
data	B
siam	O
wahba	O
g	O
lin	O
y	O
and	O
zhang	O
h	O
gacv	O
for	O
support	O
vector	B
machines	O
pages	O
of	O
smola	O
a	O
bartlett	O
p	O
sch	O
olkopf	O
b	O
and	O
schuurmans	O
d	O
advances	O
in	O
large	O
margin	B
classifiers	O
mit	O
press	O
wald	O
a	O
statistical	O
decision	O
functions	O
john	O
wiley	O
sons	O
chapman	O
hall	O
references	O
wedderburn	O
r	B
w	O
m	O
quasi-likelihood	O
functions	O
generalized	O
linear	B
models	B
and	O
the	O
gauss	O
newton	O
method	B
biometrika	B
welch	O
b	O
l	O
and	O
peers	O
h	O
w	O
on	O
formulae	O
for	O
confidence	O
points	O
based	O
on	O
integrals	O
of	O
weighted	O
likelihoods	O
j	O
roy	O
statist	O
soc	O
b	O
westfall	O
p	O
and	O
young	O
s	O
resampling-based	O
multiple	B
testing	B
examples	O
and	O
methods	O
for	O
p-value	B
adjustment	O
wiley	O
series	O
in	O
probability	O
and	O
statistics	B
wileyinterscience	O
xie	O
m	O
and	O
singh	O
k	O
confidence	O
distribution	B
the	O
frequentist	B
distribution	B
esti	O
mator	O
of	O
a	O
parameter	O
a	O
review	O
int	O
statist	O
rev	O
with	O
discussion	O
ye	O
j	O
on	O
measuring	O
and	O
correcting	O
the	O
effects	O
of	O
data	B
mining	O
and	O
model	O
selec	O
tion	O
j	O
amer	O
statist	O
assoc	O
zhang	O
c	O
-h	O
and	O
zhang	O
s	O
confidence	O
intervals	B
for	O
low-dimensional	O
parame	O
ters	O
with	O
high-dimensional	O
data	B
j	O
roy	O
statist	O
soc	O
b	O
zou	O
h	O
hastie	O
t	B
and	O
tibshirani	O
r	B
on	O
the	O
degrees	B
of	I
freedom	I
of	O
the	O
lasso	B
ann	O
statist	O
author	O
index	O
abu-mostafa	O
y	O
achanta	O
r	B
akaike	O
h	O
anderson	O
t	B
w	O
bastien	O
f	O
becker	O
r	B
bellhouse	O
d	O
r	B
bengio	O
y	O
benjamini	O
y	O
berger	O
j	O
o	O
bergeron	O
a	O
bergstra	O
j	O
berk	O
r	B
berkson	O
j	O
bernardo	O
j	O
m	O
bibby	O
j	O
m	O
bien	O
j	O
birch	O
m	O
w	O
bishop	O
c	O
bloomston	O
m	O
boos	O
d	O
d	O
boser	O
b	O
bouchard	O
n	O
breiman	O
l	O
breuleux	O
o	O
brown	O
l	O
b	O
uhlmann	O
p	O
buja	O
a	O
carlin	O
b	O
p	O
carroll	O
r	B
j	O
casella	O
g	O
chambers	O
j	O
chen	O
z	O
chia	O
d	O
cho	O
c	O
s	O
chopin	O
n	O
cleveland	O
w	O
s	O
cohen	O
a	O
corbet	O
a	O
cortes	O
c	O
courville	O
a	O
cover	O
t	B
m	O
cox	O
d	O
r	B
crowley	O
j	O
cudkowicz	O
m	O
de	B
finetti	I
b	O
dembo	O
a	O
dempster	O
a	O
p	O
desjardins	O
g	O
dezeure	O
r	B
diaconis	O
p	O
diciccio	O
t	B
donnelly	O
p	O
donoho	O
d	O
l	O
edwards	O
a	O
w	O
f	O
efron	O
b	O
ertaylan	O
g	O
eskin	O
e	O
fang	O
l	O
feldman	O
d	O
fields	O
r	B
c	O
finney	O
d	O
j	O
fisher	B
r	B
a	O
fithian	O
w	O
freund	O
y	O
friedman	O
j	O
geisser	O
s	O
gerber	O
m	O
gholami	O
s	O
good	O
i	O
author	O
index	O
goodfellow	O
i	O
j	O
gous	O
a	O
grosse-wentrup	O
m	O
guyon	O
i	O
haenszel	O
w	O
hall	O
p	O
hampel	O
f	O
r	B
hardiman	O
o	O
harford	O
t	B
hastie	O
t	B
hawe	O
j	O
hinkley	O
d	O
v	O
hinton	O
g	O
hochberg	O
y	O
hoeffding	O
w	O
hoerl	O
a	O
e	O
hothorn	O
t	B
huber	O
p	O
j	O
jaeckel	O
l	O
a	O
james	O
w	O
jansen	O
l	O
janson	O
l	O
javanmard	O
a	O
jaynes	O
e	O
jeffreys	B
h	O
jin	O
l	O
x	O
johnson	O
n	O
l	O
johnstone	O
i	O
kaplan	O
e	O
l	O
kass	O
r	B
e	O
kennard	O
r	B
w	O
kent	O
j	O
t	B
koh	O
p	O
w	O
kotz	O
s	O
krizhevsky	O
a	O
kuffner	O
r	B
laird	O
n	O
m	O
lamblin	O
p	O
le	O
q	O
v	O
lecun	O
y	O
lee	O
j	O
lehmann	O
e	O
l	O
leitner	O
m	O
l	O
leslie	O
c	O
levine	O
e	O
a	O
li	O
g	O
liang	O
p	O
s	O
liaw	O
a	O
liberman	O
m	O
lin	O
y	O
loader	O
c	O
lockhart	O
r	B
loftus	O
j	O
louis	O
t	B
a	O
lynden-bell	O
d	O
macdonald	O
j	O
m	O
macke	O
j	O
h	O
mackey	O
l	O
maithel	O
s	O
k	O
mallows	O
c	O
l	O
mantel	O
n	O
mardia	O
k	O
v	O
mccullagh	O
p	O
meier	O
p	O
metropolis	O
n	O
meyer	O
t	B
miller	O
r	B
g	O
jr	O
montanari	O
a	O
morris	O
c	O
nelder	O
j	O
nesterov	O
y	O
neyman	O
j	O
ng	O
a	O
ngiam	O
j	O
noble	O
w	O
s	O
norel	O
r	B
norton	O
j	O
a	O
o	O
hagan	O
a	O
olshen	O
r	B
a	O
park	O
t	B
pascanu	O
r	B
pawlik	O
t	B
m	O
pearson	O
e	O
s	O
pearson	O
k	O
peers	O
h	O
w	O
pericchi	O
l	O
r	B
petrosian	O
v	O
popescu	O
b	O
poultsides	O
g	O
a	O
pritchard	O
j	O
quenouille	O
m	O
h	O
r	B
core	O
team	O
raftery	O
a	O
e	O
reid	O
n	O
reid	O
s	O
ridgeway	O
g	O
ripley	O
b	O
d	O
ritov	O
y	O
author	O
index	O
robbins	O
h	O
ronchetti	O
e	O
m	O
rosenbluth	O
a	O
w	O
rosenbluth	O
m	O
n	O
rosset	O
s	O
rousseeuw	O
p	O
j	O
rubin	O
d	O
b	O
salakhutdinov	O
r	B
savage	B
l	O
j	O
schapire	O
r	B
scheff	O
e	O
h	O
schmidt	O
c	O
schoenfeld	O
d	O
sch	O
olkopf	O
b	O
schwarz	O
g	O
senn	O
s	O
serfling	O
r	B
j	O
sherman	O
a	O
siegmund	O
d	O
simon	O
n	O
singh	O
k	O
smola	O
a	O
soric	O
b	O
spevack	O
m	O
spolverato	O
g	O
squires	O
i	O
malcolm	O
srivastava	O
n	O
stahel	O
w	O
a	O
stefanski	O
l	O
stein	O
c	O
stephens	O
m	O
stigler	O
s	O
m	O
stolovitzky	O
g	O
stone	O
c	O
j	O
stone	O
m	O
storey	O
j	O
d	O
sun	O
d	O
sun	O
y	O
sutskever	O
i	O
tanner	O
m	O
a	O
taylor	O
j	O
teller	O
a	O
h	O
teller	O
e	O
thisted	O
r	B
thomas	O
j	O
a	O
tibshirani	O
r	B
toulmin	O
g	O
tran	O
l	O
tran	O
t	B
b	O
tukey	B
j	O
w	O
turian	O
j	O
van	O
de	O
geer	O
s	O
van	O
ligtenberg	O
j	O
vapnik	B
v	O
vaughan	O
r	B
vincent	O
p	O
votanopoulos	O
k	O
i	O
wager	O
s	O
wahba	O
g	O
wainwright	O
m	O
wald	O
a	O
wang	O
l	O
wang	O
s	O
warde-farley	O
d	O
wasserman	O
l	O
weber	O
s	O
m	O
wedderburn	O
r	B
w	O
m	O
welch	O
b	O
l	O
westfall	O
p	O
weston	O
j	O
wiener	O
m	O
wilks	O
a	O
williams	O
c	O
wong	O
w	O
h	O
worhunsky	O
d	O
j	O
xie	O
m	O
ye	O
j	O
yekutieli	O
d	O
ylvisaker	O
d	O
young	O
s	O
zach	O
n	O
zhang	O
c	O
-h	O
zhang	O
h	O
zhang	O
k	O
zhang	O
s	O
zhao	O
l	O
zhu	O
j	O
zou	O
h	O
subject	O
index	O
abc	B
method	B
accelerated	B
gradient	B
descent	I
acceleration	B
accuracy	B
after	B
model	B
selection	I
accurate	B
but	I
not	I
correct	I
activation	B
function	B
leaky	O
rectified	O
linear	B
rectified	O
linear	B
relu	B
tanh	B
active	B
set	B
adaboost	B
algorithm	B
adaptation	B
adaptive	B
estimator	B
adaptive	B
rate	B
control	B
additive	B
model	I
adaptive	B
adjusted	B
compliance	B
admixture	B
modeling	I
aic	O
see	O
akaike	B
information	B
criterion	I
akaike	B
information	B
criterion	I
allele	B
frequency	I
american	B
statistical	I
association	I
ancillary	B
apparent	B
error	I
arcsin	B
transformation	I
arthur	B
eddington	I
asymptotics	B
xvi	O
autoencoder	B
backfitting	O
backpropagation	B
bagged	B
estimate	B
bagging	B
balance	B
equations	I
barycentric	B
plot	I
basis	B
expansion	I
bayes	O
deconvolution	B
factor	B
false-discovery	B
rate	B
posterior	B
distribution	B
posterior	B
probability	I
shrinkage	B
t-statistic	B
theorem	B
inference	B
information	B
criterion	I
lasso	B
lasso	B
prior	B
model	B
selection	I
trees	B
bayes	O
frequentist	B
estimation	B
bayesian	B
bayesian	B
information	B
criterion	I
bayesianism	B
bca	O
accuracy	B
and	I
correctness	I
confidence	O
density	B
interval	B
method	B
benjamini	B
and	I
hochberg	I
benjamini	O
yekutieli	O
bernoulli	B
best-approximating	B
linear	B
subspace	I
best-subset	B
selection	I
beta	B
distribution	B
bhq	B
bias	B
bias-corrected	B
and	O
accelerated	O
see	O
bca	O
method	B
confidence	O
intervals	B
percentile	B
method	B
subject	O
index	O
bias-correction	B
value	I
biased	B
estimation	B
bic	O
see	O
bayesian	B
information	B
criterion	I
big-data	B
era	I
xv	O
binomial	B
coherent	B
behavior	I
common	B
task	I
framework	I
compliance	B
computational	B
bottleneck	I
computer	O
age	O
xv	O
computer-intensive	B
conditional	B
inference	B
inference	B
statistics	B
conditional	B
conditional	B
distribution	B
full	B
lasso	B
conditionality	B
confidence	O
density	B
distribution	B
interval	B
region	B
conjugate	B
prior	B
priors	B
filters	O
layer	B
convolution	B
distribution	B
log-likelihood	B
standard	B
deviation	I
bioassay	B
biometrika	B
bivariate	B
normal	B
bonferroni	B
bound	B
boole	O
s	O
inequality	O
boosting	B
bootstrap	B
baron	B
munchausen	I
bayesian	B
cdf	B
confidence	O
intervals	B
ideal	B
estimate	B
jackknife	B
after	I
moving	B
blocks	I
multisample	B
nonparametric	B
out	B
of	I
bootstrap	B
packages	B
parametric	B
probabilities	B
replication	B
sample	B
sample	B
size	I
smoothing	B
t	B
t	B
intervals	B
bound	B
form	B
bounding	B
hyperplane	I
burn-in	B
byq	B
algorithm	B
causal	O
inference	B
xvi	O
censored	O
data	B
not	B
truncated	I
centering	B
central	B
limit	I
theorem	B
chain	B
rule	B
for	I
differentiation	I
classic	B
statistical	I
inference	B
classification	O
classification	O
accuracy	B
classification	O
error	O
classification	O
tree	O
cochran	O
mantel	O
haenszel	O
test	O
convex	B
optimization	I
corrected	B
differences	I
correlation	B
effects	I
covariance	O
formula	B
penalty	B
coverage	B
coverage	B
level	I
coverage	B
matching	B
prior	B
cox	O
model	O
see	O
proportional	B
hazards	I
model	I
cp	B
cram	O
er	O
rao	O
lower	O
bound	B
credible	B
interval	B
cross-validation	B
estimate	B
k-fold	B
leave	B
one	I
out	I
cumulant	B
generating	I
function	B
curse	B
of	I
dimensionality	I
dark	B
energy	I
data	B
analysis	B
data	B
science	I
xvii	O
subject	O
index	O
data	B
sets	O
binomial	B
gamma	B
gaussian	B
normal	B
poisson	B
divide-and-conquer	B
algorithm	B
document	B
retrieval	I
dose	O
response	B
dropout	B
learning	I
dti	O
see	O
diffusion	B
tensor	I
imaging	I
early	B
computer-age	I
xvi	O
early	B
stopping	I
effect	B
size	I
efficiency	O
eigenratio	B
elastic	B
net	I
ellipsoid	B
em	B
algorithm	B
missing	B
data	B
empirical	B
bayes	I
estimation	B
strategies	I
information	B
large-scale	B
testing	B
empirical	B
null	I
estimation	B
maximum-likelihood	B
estimation	B
empirical	B
probability	I
distribution	B
ensemble	B
ephemeral	B
predictors	B
epoch	B
equilibrium	B
distribution	B
equivariant	B
exact	B
inferences	I
expectation	B
parameter	I
experimental	O
design	O
xvi	O
exponential	B
family	I
p-parameter	B
curved	B
one-parameter	B
f	B
distribution	B
f	B
tests	I
f	O
fake-data	B
principle	I
false	O
coverage	B
control	B
false	B
discovery	I
control	B
control	B
theorem	B
proportion	B
rate	B
false-discovery	O
als	B
aml	O
see	O
leukemia	B
baseball	B
butterfly	B
cell	B
infusion	I
cholesterol	B
diabetes	B
dose-response	B
galaxy	B
handwritten	B
digits	I
headneck	O
cancer	O
human	B
ancestry	I
insurance	B
kidney	B
function	B
leukemia	B
ncog	B
nodes	B
pediatric	B
cancer	I
police	B
prostate	B
protein	B
classification	I
shakespear	B
spam	B
student	B
score	I
supernova	B
vasoconstriction	B
data	B
snooping	I
de	B
finetti	I
b	O
de	B
finetti	I
savage	B
school	O
debias	B
decision	B
rule	B
decision	O
theory	B
xvi	O
deconvolution	B
deep	B
learning	I
definitional	O
bias	B
degrees	B
of	I
freedom	I
delta	B
method	B
deviance	B
deviance	B
residual	I
diffusion	B
tensor	I
imaging	I
direct	B
evidence	I
directional	B
derivatives	I
distribution	B
beta	B
subject	O
index	O
rate	B
family	B
of	I
probability	I
densities	I
family-wise	B
error	I
rate	B
fdr	O
see	O
false-discovery	B
rate	B
feed-forward	B
fiducial	B
constructions	B
density	B
inference	B
fisher	B
fisher	B
information	B
bound	B
matrix	B
fisherian	B
correctness	I
fisherian	B
inference	B
fixed-knot	B
regression	B
splines	I
flat	B
prior	B
forward	B
pass	I
forward-stagewise	B
fitting	O
forward-stepwise	B
computations	B
logistic	B
regression	B
regression	B
fully	B
connected	I
layer	B
functional	B
gradient	B
descent	I
fwer	O
see	O
family-wise	B
error	I
rate	B
g-modeling	B
gamma	B
distribution	B
general	O
estimating	O
equations	O
xvi	O
general	B
information	B
criterion	I
generalized	O
linear	B
mixed	I
model	I
linear	B
model	I
ridge	B
problem	I
genome	B
genome-wide	B
association	I
studies	I
gibbs	B
sampling	I
glm	O
see	O
generalized	O
linear	B
model	I
glmm	O
see	O
generalized	O
linear	B
mixed	I
model	I
frailties	B
frequentism	B
fourier	O
method	B
transform	B
frequentist	B
inference	B
strongly	B
google	O
flu	O
trends	O
gradient	B
boosting	B
gradient	B
descent	I
gram	B
matrix	B
gram-schmidt	B
orthogonalization	I
graphical	B
lasso	B
graphical	O
models	B
xvi	O
greenwood	O
s	O
formula	B
group	B
lasso	B
hadamard	B
product	I
handwritten	B
digits	I
haplotype	B
estimation	B
hazard	B
rate	B
parametric	B
estimate	B
hidden	B
layer	B
high-order	B
interaction	I
hinge	B
loss	I
hints	O
learning	B
with	I
hoeffding	O
s	O
lemma	O
holm	O
s	O
procedure	O
homotopy	B
path	B
hypergeometric	B
distribution	B
imputation	B
inadmissible	B
indirect	B
evidence	I
inductive	B
inference	B
inference	B
inference	B
after	B
model	B
selection	I
inferential	B
triangle	I
infinitesimal	O
forward	O
stagewise	O
infinitesimal	O
jackknife	B
estimate	B
standard	B
deviations	I
influence	O
function	B
empirical	B
influenza	O
outbreaks	O
input	B
distortion	I
input	B
layer	B
insample	B
error	I
inverse	B
chi-squared	I
inverse	B
gamma	B
irls	O
see	O
iteratively	O
reweighted	O
least	O
iteratively	B
reweighted	I
least	B
squares	I
squares	O
jackknife	B
estimate	B
of	I
standard	B
error	I
standard	B
error	I
subject	O
index	O
estimation	B
ridge	B
regression	B
james	O
stein	O
jeffreys	B
prior	B
jeffreys	B
prior	B
prior	B
multiparameter	O
scale	B
jumpiness	B
of	I
estimator	B
kaplan	O
meier	O
estimate	B
karush	O
kuhn	O
tucker	O
optimality	B
conditions	B
kernel	O
function	B
logistic	B
regression	B
method	B
svm	B
trick	B
kernel	B
smoothing	B
knots	B
kullback	O
leibler	O
distance	O
regularization	B
lagrange	B
dual	B
form	B
multiplier	B
large-scale	O
hypothesis	B
testing	B
testing	B
large-scale	B
prediction	I
algorithms	I
lasso	B
modification	O
path	B
penalty	B
learning	O
from	O
the	O
experience	O
of	O
others	O
learning	B
rate	B
least	B
squares	I
least-angle	B
regression	B
least-favorable	B
family	I
left-truncated	B
lehmann	B
alternative	I
life	B
table	I
likelihood	B
function	B
concavity	B
limited-translation	B
rule	B
lindsey	O
s	O
method	B
linearly	B
separable	I
link	B
function	B
local	B
false-discovery	B
rate	B
local	B
regression	B
local	B
translation	I
invariance	I
log	B
polynomial	I
regression	B
log-rank	B
statistic	B
log-rank	B
test	I
logic	B
of	I
inductive	B
inference	B
logistic	B
regression	B
multiclass	B
logit	B
loss	B
plus	I
penalty	B
machine	B
learning	I
mallows	O
cp	B
see	O
cp	B
mantel	O
haenzel	O
test	O
map	B
map	B
estimate	B
margin	B
marginal	B
density	B
markov	O
chain	O
monte	O
carlo	O
see	O
mcmc	B
markov	B
chain	I
theory	B
martingale	B
theory	B
matching	B
prior	B
matlab	B
matrix	B
completion	I
max	B
pool	I
layer	B
maximized	O
a-posteriori	O
probability	O
see	O
map	B
maximum	B
likelihood	B
maximum	B
likelihood	B
estimation	B
mcmc	B
mcnemar	B
test	I
mean	B
absolute	I
deviation	I
median	B
unbiased	I
memory-based	B
methods	I
meter	B
reader	I
meter-reader	B
microarrays	B
minitab	B
misclassification	O
error	O
missing	B
data	B
em	B
algorithm	B
missing-species	B
problem	I
mixed	B
features	I
mixture	B
density	B
model	B
averaging	B
model	B
selection	I
criteria	B
monotone	B
lasso	B
monotonic	B
increasing	I
function	B
multinomial	O
subject	O
index	O
distribution	B
from	B
poisson	B
multiple	B
testing	B
multivariate	B
analysis	B
normal	B
n-gram	B
n-p	B
complete	I
nadaraya	O
watson	O
estimator	B
natural	B
parameter	I
natural	B
spline	I
model	I
ncog	B
see	O
northern	O
california	O
oncology	O
group	O
nested	B
models	B
neural	O
information	B
processing	O
systems	O
neural	B
network	I
adaptive	B
tuning	I
number	B
of	I
hidden	I
layers	I
neurons	B
neyman	O
s	O
construction	O
predictor	B
one-sample	O
nonparametric	B
bootstrap	B
one-sample	B
problems	I
oob	O
see	O
out-of-bag	B
error	I
optical	B
character	I
recognition	I
optimal	B
separating	B
hyperplane	I
optimal-margin	O
classifier	O
optimality	B
oracle	B
orthogonal	B
parameters	I
out-of-bag	B
error	I
out-the-box	B
learning	I
algorithm	B
output	B
layer	B
outsample	B
error	I
over	B
parametrized	I
overfitting	O
overshrinks	B
p-value	B
packageprogram	O
gbm	B
glmnet	B
lars	B
liblinear	B
locfdr	B
lowess	B
nlm	B
randomforest	B
selectiveinference	B
pairwise	B
inner	I
products	I
parameter	B
space	I
parametric	B
bootstrap	B
parametric	B
family	I
parametric	B
models	B
partial	B
likelihood	B
partial	B
logistic	B
regression	B
partial	B
residual	I
path-wise	B
coordinate	I
descent	I
penalized	O
least	B
squares	I
likelihood	B
logistic	B
regression	B
maximum	B
likelihood	B
percentile	B
method	B
central	B
interval	B
permutation	B
null	I
permutation	B
test	I
phylogenetic	B
tree	I
piecewise	O
neyman	O
pearson	O
non-null	B
noncentral	B
chi-square	I
variable	I
nonlinear	B
transformations	I
nonlinearity	B
nonparameteric	O
regression	B
nonparametric	B
mle	B
percentile	B
interval	B
normal	B
correlation	O
coefficient	O
distribution	B
multivariate	B
regression	B
model	I
theory	B
nuclear	B
norm	I
nuisance	B
parameters	I
objective	B
bayes	I
inference	B
intervals	B
prior	B
distribution	B
northern	O
california	O
oncology	O
group	O
ocr	O
see	O
optical	B
character	I
recognition	I
offset	B
ols	O
algorithm	B
estimation	B
subject	O
index	O
linear	B
nonlinear	B
pivotal	O
argument	B
quantity	B
statistic	B
rule	B
poisson	B
distribution	B
regression	B
poisson	B
regression	B
polynomial	B
kernel	I
positive-definite	O
function	B
post-selection	B
inference	B
posterior	B
density	B
posterior	B
distribution	B
postwar	B
era	I
prediction	O
errors	B
rule	B
predictors	B
principal	B
components	I
prior	B
distribution	B
beta	B
conjugate	B
coverage	B
matching	I
gamma	B
normal	B
objective	B
bayes	I
proper	B
probit	B
analysis	B
propagation	B
of	I
errors	B
proper	B
prior	B
proportional	B
hazards	I
model	I
proximal-newton	B
q-value	B
qq	B
plot	I
qr	B
decomposition	I
quadratic	B
program	I
quasilikelihood	B
quetelet	B
adolphe	O
r	B
random	B
forest	I
adaptive	B
nearest-neighbor	O
estimator	B
leave-one-out	O
cross-validated	O
error	O
monte	B
carlo	I
variance	B
interval	B
theorem	B
score	B
function	B
score	B
tests	I
second-order	B
accuracy	B
sampling	B
variance	B
standard	B
error	I
randomization	B
rao	O
blackwell	O
rate	B
annealing	I
rectified	O
linear	B
regression	B
regression	B
rule	B
regression	B
to	I
the	I
mean	I
regression	B
tree	I
regularization	B
path	B
relevance	B
relevance	B
function	B
relevance	B
theory	B
reproducing	B
kernel	I
hilbert	I
space	I
resampling	B
plans	B
simplex	B
vector	B
residual	B
deviance	B
response	B
ridge	B
regression	B
james	O
stein	O
ridge	B
regularization	B
logistic	B
regression	B
right-censored	B
risk	B
set	B
rkhs	O
see	O
reproducing-kernel	O
hilbert	O
space	O
robbins	O
formula	B
robust	B
estimation	B
royal	B
statistical	I
society	I
s	B
language	I
sample	B
correlation	O
coefficient	O
sample	B
size	I
coherency	I
sampling	B
distribution	B
sas	B
savage	B
l	O
j	O
scale	B
of	O
evidence	O
fisher	B
jeffreys	B
scheff	O
e	O
subject	O
index	O
selection	B
bias	B
self-consistent	B
separating	B
hyperplane	I
geometry	B
seven-league	B
boots	I
shrinkage	B
estimator	B
sigmoid	B
function	B
significance	O
level	O
simulation	B
simultaneous	O
confidence	O
intervals	B
simultaneous	B
inference	B
sinc	B
kernel	I
single-nucleotide	O
polymorphism	O
see	O
snp	B
smoothing	B
operator	I
snp	B
soft	O
margin	B
classifier	O
soft-threshold	B
softmax	B
spam	B
filter	O
sparse	O
models	B
principal	B
components	I
sparse	B
matrix	B
sparsity	B
split-variable	B
randomization	B
spss	B
squared	B
error	I
standard	B
candles	I
standard	B
error	I
external	B
internal	B
standard	B
interval	B
stein	O
s	O
paradox	B
unbiased	B
risk	I
estimate	B
stepwise	B
selection	I
stochastic	B
gradient	B
descent	I
stopping	B
rule	B
stopping	B
rules	I
string	B
kernel	I
strong	B
rules	I
structure	B
structure	B
matrix	B
student	O
t	B
confidence	O
interval	B
distribution	B
statistic	B
two-sample	B
studentized	B
range	I
subgradient	O
condition	B
equation	B
subjective	B
prior	B
distribution	B
subjective	B
probability	I
subjectivism	B
sufficiency	O
sufficient	O
statistic	B
vector	B
supervised	B
learning	I
support	O
set	B
vector	B
vector	B
classifiers	O
vector	B
machine	I
sure	O
see	O
stein	O
s	O
unbiased	B
risk	I
estimate	B
survival	B
analysis	B
survival	B
curve	I
svm	B
lagrange	B
dual	B
lagrange	B
primal	I
loss	B
function	B
taylor	B
series	I
theoretical	B
null	I
tied	B
weights	B
time	O
series	O
xvi	O
training	B
set	B
transformation	B
invariance	I
transient	B
episodes	I
trees	B
averaging	B
best-first	O
depth	B
terminal	B
node	I
tricube	B
kernel	I
trimmed	B
mean	I
triple-point	O
xv	O
true	B
error	I
rate	B
true-discovery	B
rates	I
tukey	B
j	O
w	O
tukey	B
j	O
w	O
tweedie	O
s	O
formula	B
twenty-first-century	O
methods	O
xvi	O
two-groups	B
model	I
uncorrected	B
differences	I
uninformative	B
prior	B
universal	B
approximator	I
unlabeled	B
images	I
subject	O
index	O
unobserved	B
covariates	I
validation	B
set	B
vapnik	B
v	O
variable-importance	B
plot	I
variance	B
variance	B
reduction	I
velocity	B
vector	B
voting	B
warm	B
starts	I
weak	B
learner	I
weight	O
decay	B
regularization	B
sharing	B
weighted	B
exponential	I
loss	I
weighted	B
least	B
squares	I
weighted	B
majority	I
vote	I
weights	B
wide	B
data	B
wilks	O
likelihood	B
ratio	O
statistic	B
winner	O
s	O
curse	O
winsorized	B
mean	I
working	B
response	B
z	O
zero	B
set	B
