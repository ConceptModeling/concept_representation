machine	O
learning	O
neural	O
and	O
statistical	B
classification	B
editors	O
d	O
michie	O
d	O
j	O
spiegelhalter	O
c	O
c	O
taylor	O
february	O
contents	O
introduction	O
classification	B
issues	O
conclusions	O
statistical	B
approaches	O
machine	O
learning	O
caution	O
in	O
the	O
interpretations	O
of	O
comparisons	O
introduction	O
classification	B
perspectives	O
on	O
classification	B
neural	B
networks	I
the	O
statlog	B
project	O
quality	O
control	O
the	O
structure	O
of	O
this	O
volume	O
definition	O
of	O
classification	B
rationale	O
class	B
definitions	I
accuracy	B
examples	B
of	I
classifiers	I
fisher	O
s	O
linear	O
discriminants	O
decision	O
tree	O
and	O
rule-based	B
methods	I
k-nearest-neighbour	O
choice	B
of	I
variables	I
classification	B
of	O
classification	B
procedures	O
extensions	B
to	I
linear	B
discrimination	B
transformations	O
and	O
combinations	B
of	I
variables	I
decision	B
trees	I
and	O
rule-based	B
methods	I
ii	O
classical	O
statistical	B
methods	O
modern	O
statistical	B
techniques	O
density	B
estimates	I
separating	O
classes	B
linear	O
discriminants	O
by	O
least	O
squares	O
regularisation	B
and	O
smoothed	O
estimates	O
choice	O
of	O
regularisation	B
parameters	O
a	O
general	O
structure	O
for	O
classification	B
problems	O
prior	B
probabilities	I
and	O
the	O
default	B
rule	I
misclassification	B
costs	B
bayes	B
rule	I
given	O
data	O
bayes	B
rule	I
in	O
statistics	O
reference	O
texts	O
introduction	O
linear	O
discriminants	O
special	O
case	O
of	O
two	O
classes	B
linear	O
discriminants	O
by	O
maximum	B
likelihood	I
more	O
than	O
two	O
classes	B
quadratic	B
discriminant	I
quadratic	B
discriminant	I
programming	O
details	O
logistic	B
discriminant	I
logistic	B
discriminant	I
programming	O
details	O
bayes	O
rules	O
example	B
logistic	B
discriminant	I
quadratic	B
discriminant	I
introduction	O
density	B
estimation	I
example	B
neighbour	O
example	B
projection	B
pursuit	I
classification	B
example	B
naive	B
bayes	I
causal	B
networks	I
example	B
other	O
recent	O
approaches	O
ace	B
mars	B
linear	B
discriminant	I
sec	O
iii	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
decision	B
trees	I
stopping	O
rules	O
and	O
class	B
probability	I
trees	I
manufacturing	O
new	O
attributes	B
rule-learning	B
algorithms	O
itrule	B
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
data	O
fit	O
and	O
mental	B
fit	I
of	O
classifiers	O
specific-to-general	B
a	O
paradigm	O
for	O
rule-learning	B
general-to-specific	B
top-down	O
induction	O
of	O
trees	O
splitting	B
criteria	I
getting	O
a	O
right-sized	O
tree	O
statlog	B
s	O
ml	O
algorithms	O
tree-learning	B
further	O
features	B
of	O
newid	B
further	O
features	B
of	O
cart	B
bayes	B
tree	I
beyond	O
the	O
complexity	O
barrier	O
trees	O
into	O
rules	O
inherent	O
limits	O
of	O
propositional-level	O
learning	O
a	O
human-machine	O
compromise	O
structured	B
induction	I
introduction	O
supervised	B
networks	I
for	O
classification	B
multi	B
layer	I
perceptron	B
structure	O
and	O
functionality	B
improving	O
the	O
generalisation	O
of	O
feed-forward	B
networks	I
unsupervised	B
learning	I
the	O
k-means	B
clustering	B
algorithm	O
introduction	O
pairwise	O
linear	B
regression	I
clustering	B
of	O
classes	B
description	O
of	O
the	O
classification	B
procedure	O
kohonen	B
networks	I
and	O
learning	B
vector	B
quantizers	I
ramnets	B
learning	O
procedure	O
radial	B
basis	I
function	I
networks	O
perceptrons	O
and	O
multi	O
layer	O
perceptrons	O
neural	B
networks	I
iv	O
methods	O
for	O
comparison	O
train-and-test	B
dataset	O
descriptions	O
and	O
results	O
review	O
of	O
previous	O
empirical	O
comparisons	O
bootstrap	B
evaluation	B
assistant	B
collection	O
of	O
datasets	O
statistical	B
measures	B
information	O
theoretic	O
measures	B
bootstrap	B
optimisation	B
of	O
parameters	O
large	O
number	O
of	O
categories	O
estimation	O
of	O
error	O
rates	O
in	O
classification	B
rules	O
cross-validation	O
organisation	O
of	O
comparative	B
trials	I
cross-validation	O
characterisation	O
of	O
datasets	O
simple	O
measures	B
pre-processing	O
missing	B
values	I
feature	O
selection	O
and	O
extraction	O
bias	B
in	O
class	B
proportions	O
hierarchical	O
attributes	B
preprocessing	B
strategy	O
in	O
statlog	B
introduction	O
basic	O
toolbox	O
of	O
algorithms	O
difficulties	O
in	O
previous	O
studies	O
previous	O
empirical	O
comparisons	O
individual	O
results	O
machine	O
learning	O
vs	O
neural	O
network	O
studies	O
involving	O
ml	O
k-nn	B
and	O
statistics	O
some	O
empirical	O
studies	O
relating	O
to	O
credit	O
risk	O
machine	O
learning	O
and	O
neural	B
networks	I
introduction	O
credit	B
datasets	I
australian	B
credit	I
image	B
datasets	I
handwritten	O
digits	O
karhunen-loeve	B
digits	I
vehicle	B
silhouettes	O
traditional	O
and	O
statistical	B
approaches	O
credit	B
management	I
letter	B
recognition	I
sec	O
v	O
image	B
segmentation	I
german	B
credit	I
analysis	B
of	I
results	I
head	B
injury	I
satellite	B
image	I
dataset	I
shuttle	B
control	I
shuttle	B
control	I
diabetes	B
belgian	O
power	O
belgian	B
power	I
ii	I
chromosomes	B
landsat	O
satellite	B
image	I
cut	B
datasets	O
with	O
costs	B
head	B
injury	I
heart	B
disease	I
other	O
datasets	O
dna	B
technical	B
machine	B
faults	I
tsetse	O
fly	O
distribution	O
statistical	B
and	O
information	B
measures	B
kl-digits	O
dataset	O
vehicle	B
silhouettes	O
heart	B
disease	I
technical	B
belgian	B
power	I
ii	I
introduction	O
results	O
by	O
subject	O
areas	O
credit	B
datasets	I
image	B
datasets	I
datasets	O
with	O
costs	B
top	O
five	O
algorithms	O
multidimensional	B
scaling	I
hierarchical	B
clustering	B
of	O
algorithms	O
scaling	O
of	O
datasets	O
performance	O
related	O
to	O
measures	B
theoretical	O
normal	O
distributions	O
best	O
algorithms	O
for	O
datasets	O
clustering	B
of	O
datasets	O
other	O
datasets	O
dominators	B
absolute	O
performance	O
quadratic	B
discriminants	I
scaling	O
of	O
algorithms	O
vi	O
conclusions	O
discriminants	O
characterizing	O
predictive	O
power	O
application	O
assistant	B
criticism	O
of	O
metalevel	B
learning	I
approach	O
criticism	O
of	O
measures	B
relative	O
performance	O
logdisc	B
vs	O
pruning	B
of	O
decision	B
trees	I
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
objectives	B
using	O
test	O
results	O
in	O
metalevel	B
learning	I
rules	O
generated	O
in	O
metalevel	B
learning	I
prediction	O
of	O
performance	O
ml	B
on	I
ml	I
vs	O
regression	O
introduction	O
user	O
s	O
guide	O
to	O
programs	O
statistical	B
algorithms	O
nearest	B
neighbour	I
smart	B
naive	B
bayes	I
castle	B
decision	B
trees	I
and	O
newid	B
cart	B
and	O
indcart	B
bayes	B
tree	I
rule-based	B
methods	I
neural	B
networks	I
backprop	B
kohonen	B
and	O
lvq	B
radial	B
basis	I
function	I
neural	O
network	O
memory	B
and	O
time	B
memory	B
time	B
general	O
issues	O
interpretation	O
of	O
error	O
rates	O
cost	B
matrices	I
structuring	O
the	O
results	O
removal	O
of	O
irrelevant	B
attributes	B
itrule	B
sec	O
vii	O
knowledge	O
representation	O
special	O
features	B
diagnostics	O
and	O
plotting	O
exploratory	O
data	O
from	O
classification	B
to	O
knowledge	O
organisation	O
and	O
synthesis	O
introduction	O
learning	O
measurement	O
and	O
representation	O
prototypes	B
experiment	O
experiment	O
experiment	O
discussion	O
function	B
approximation	I
discussion	O
genetic	B
algorithms	I
propositional	B
learning	I
systems	I
discussion	O
relations	O
and	O
background	B
knowledge	I
discussion	O
conclusions	O
introduction	O
experimental	O
domain	O
learning	O
to	O
control	O
from	O
scratch	O
boxes	B
boxes	B
refinements	O
of	O
boxes	B
learning	O
to	O
control	O
from	O
scratch	O
genetic	B
learning	O
robustness	O
and	O
adaptation	O
exploiting	O
partial	O
explicit	O
knowledge	O
boxes	B
with	O
partial	O
knowledge	O
exploiting	O
domain	B
knowledge	I
in	O
genetic	B
learning	O
of	O
control	O
exploiting	O
operator	O
s	O
skill	O
learning	O
to	O
pilot	O
a	O
plane	O
learning	O
to	O
control	O
container	B
cranes	I
conclusions	O
dataset	O
availability	O
software	O
sources	O
and	O
details	O
contributors	O
a	O
b	O
c	O
learning	O
to	O
control	O
dynamic	O
systems	O
introduction	O
of	O
leeds	O
d	O
michie	O
d	O
j	O
spiegelhalter	O
and	O
c	O
c	O
taylor	O
university	O
of	O
strathclyde	O
mrc	O
biostatistics	O
unit	O
cambridge	O
and	O
university	O
introduction	O
the	O
aim	O
of	O
this	O
book	O
is	O
to	O
provide	O
an	O
up-to-date	O
review	O
of	O
different	O
approaches	O
to	O
classification	B
compare	O
their	O
performance	O
on	O
a	O
wide	O
range	O
of	O
challenging	O
data-sets	O
and	O
draw	O
conclusions	O
on	O
their	O
applicability	O
to	O
realistic	O
industrial	O
problems	O
before	O
describing	O
the	O
contents	O
we	O
first	O
need	O
to	O
define	O
what	O
we	O
mean	O
by	O
classification	B
give	O
some	O
background	O
to	O
the	O
different	O
perspectives	O
on	O
the	O
task	O
and	O
introduce	O
the	O
european	O
community	O
statlog	B
project	O
whose	O
results	O
form	O
the	O
basis	O
for	O
this	O
book	O
classification	B
the	O
task	O
of	O
classification	B
occurs	O
in	O
a	O
wide	O
range	O
of	O
human	O
activity	O
at	O
its	O
broadest	O
the	O
term	O
could	O
cover	B
any	O
context	O
in	O
which	O
some	O
decision	O
or	O
forecast	O
is	O
made	O
on	O
the	O
basis	O
of	O
currently	O
available	O
information	O
and	O
a	O
classification	B
procedure	O
is	O
then	O
some	O
formal	O
method	O
for	O
repeatedly	O
making	O
such	O
judgments	O
in	O
new	O
situations	O
in	O
this	O
book	O
we	O
shall	O
consider	O
a	O
more	O
restricted	O
interpretation	O
we	O
shall	O
assume	O
that	O
the	O
problem	O
concerns	O
the	O
construction	O
of	O
a	O
procedure	O
that	O
will	O
be	O
applied	O
to	O
a	O
continuing	O
sequence	O
of	O
cases	O
in	O
which	O
each	O
new	O
case	O
must	O
be	O
assigned	O
to	O
one	O
of	O
a	O
set	O
of	O
pre-defined	O
classes	B
on	O
the	O
basis	O
of	O
observed	O
attributes	B
or	O
features	B
the	O
construction	O
of	O
a	O
classification	B
procedure	O
from	O
a	O
set	O
of	O
data	O
for	O
which	O
the	O
true	O
classes	B
are	O
known	O
has	O
also	O
been	O
variously	O
termed	O
pattern	B
recognition	I
discrimination	B
or	O
supervised	B
learning	I
order	O
to	O
distinguish	O
it	O
from	O
unsupervised	B
learning	I
or	O
clustering	B
in	O
which	O
the	O
classes	B
are	O
inferred	O
from	O
the	O
data	O
contexts	O
in	O
which	O
a	O
classification	B
task	O
is	O
fundamental	O
include	O
for	O
example	B
mechanical	O
procedures	O
for	O
sorting	O
letters	O
on	O
the	O
basis	O
of	O
machine-read	O
postcodes	O
assigning	O
individuals	O
to	O
credit	O
status	O
on	O
the	O
basis	O
of	O
financial	O
and	O
other	O
personal	O
information	O
and	O
the	O
preliminary	O
diagnosis	O
of	O
a	O
patient	O
s	O
disease	O
in	O
order	O
to	O
select	O
immediate	O
treatment	O
while	O
awaiting	O
definitive	O
test	O
results	O
in	O
fact	O
some	O
of	O
the	O
most	O
urgent	O
problems	O
arising	O
in	O
science	O
industry	O
address	O
for	O
correspondence	O
mrc	O
biostatistics	O
unit	O
institute	O
of	O
public	O
health	O
university	O
forvie	O
site	O
robinson	O
way	O
cambridge	O
u	O
k	O
introduction	O
and	O
commerce	O
can	O
be	O
regarded	O
as	O
classification	B
or	O
decision	B
problems	I
using	O
complex	B
and	O
often	O
very	O
extensive	O
data	O
we	O
note	O
that	O
many	O
other	O
topics	O
come	O
under	O
the	O
broad	O
heading	O
of	O
classification	B
these	O
include	O
problems	O
of	O
control	O
which	O
is	O
briefly	O
covered	O
in	O
chapter	O
perspectives	O
on	O
classification	B
as	O
the	O
book	O
s	O
title	O
suggests	O
a	O
wide	O
variety	O
of	O
approaches	O
has	O
been	O
taken	O
towards	O
this	O
task	O
three	O
main	O
historical	O
strands	O
of	O
research	O
can	O
be	O
identified	O
statistical	B
machine	O
learning	O
and	O
neural	O
network	O
these	O
have	O
largely	O
involved	O
different	O
professional	O
and	O
academic	O
groups	O
and	O
emphasised	O
different	O
issues	O
all	O
groups	O
have	O
however	O
had	O
some	O
objectives	B
in	O
common	O
they	O
have	O
all	O
attempted	O
to	O
derive	O
procedures	O
that	O
would	O
be	O
able	O
to	O
equal	O
if	O
not	O
exceed	O
a	O
human	O
decision-maker	O
s	O
behaviour	O
but	O
have	O
the	O
advantage	O
of	O
consistency	O
and	O
to	O
a	O
variable	O
extent	O
explicitness	O
to	O
handle	O
a	O
wide	O
variety	O
of	O
problems	O
and	O
given	O
enough	O
data	O
to	O
be	O
extremely	O
general	O
to	O
be	O
used	O
in	O
practical	O
settings	O
with	O
proven	O
success	O
statistical	B
approaches	O
two	O
main	O
phases	O
of	O
work	O
on	O
classification	B
can	O
be	O
identified	O
within	O
the	O
statistical	B
community	O
the	O
first	O
classical	O
phase	O
concentrated	O
on	O
derivatives	O
of	O
fisher	O
s	O
early	O
work	O
on	O
linear	B
discrimination	B
the	O
second	O
modern	O
phase	O
exploits	O
more	O
flexible	O
classes	B
of	O
models	O
many	O
of	O
which	O
attempt	O
to	O
provide	O
an	O
estimate	O
of	O
the	O
joint	O
distribution	O
of	O
the	O
features	B
within	O
each	O
class	B
which	O
can	O
in	O
turn	O
provide	O
a	O
classification	B
rule	I
statistical	B
approaches	O
are	O
generally	O
characterised	O
by	O
having	O
an	O
explicit	O
underlying	O
probability	O
model	O
which	O
provides	O
a	O
probability	O
of	O
being	O
in	O
each	O
class	B
rather	O
than	O
simply	O
a	O
classification	B
in	O
addition	O
it	O
is	O
usually	O
assumed	O
that	O
the	O
techniques	O
will	O
be	O
used	O
by	O
statisticians	O
and	O
hence	O
some	O
human	O
intervention	O
is	O
assumed	O
with	O
regard	O
to	O
variable	O
selection	O
and	O
transformation	B
and	O
overall	O
structuring	O
of	O
the	O
problem	O
machine	O
learning	O
machine	O
learning	O
is	O
generally	O
taken	O
to	O
encompass	O
automatic	O
computing	O
procedures	O
based	O
on	O
logical	O
or	O
binary	O
operations	O
that	O
learn	O
a	O
task	O
from	O
a	O
series	O
of	O
examples	O
here	O
we	O
are	O
just	O
concerned	O
with	O
classification	B
and	O
it	O
is	O
arguable	O
what	O
should	O
come	O
under	O
the	O
machine	O
learning	O
umbrella	O
attention	O
has	O
focussed	O
on	O
decision-tree	O
approaches	O
in	O
which	O
classification	B
results	O
from	O
a	O
sequence	O
of	O
logical	O
steps	O
these	O
are	O
capable	O
of	O
representing	O
the	O
most	O
complex	B
problem	O
given	O
sufficient	O
data	O
this	O
may	O
mean	O
an	O
enormous	O
amount	O
other	O
techniques	O
such	O
as	O
genetic	B
algorithms	I
and	O
inductive	O
logic	O
procedures	O
are	O
currently	O
under	O
active	O
development	O
and	O
in	O
principle	O
would	O
allow	O
us	O
to	O
deal	O
with	O
more	O
general	O
types	O
of	O
data	O
including	O
cases	O
where	O
the	O
number	O
and	O
type	O
of	O
attributes	B
may	O
vary	O
and	O
where	O
additional	O
layers	O
of	O
learning	O
are	O
superimposed	O
with	O
hierarchical	B
structure	I
of	O
attributes	B
and	O
classes	B
and	O
so	O
on	O
machine	O
learning	O
aims	O
to	O
generate	O
classifying	O
expressions	O
simple	O
enough	O
to	O
be	O
understood	O
easily	O
by	O
the	O
human	O
they	O
must	O
mimic	O
human	O
reasoning	O
sufficiently	O
to	O
provide	O
insight	O
into	O
the	O
decision	O
process	O
like	O
statistical	B
approaches	O
background	B
knowledge	I
may	O
be	O
exploited	O
in	O
development	O
but	O
operation	O
is	O
assumed	O
without	O
human	O
intervention	O
sec	O
neural	B
networks	I
perspectives	O
on	O
classification	B
the	O
field	O
of	O
neural	B
networks	I
has	O
arisen	O
from	O
diverse	O
sources	O
ranging	O
from	O
the	O
fascination	O
of	O
mankind	O
with	O
understanding	O
and	O
emulating	O
the	O
human	B
brain	I
to	O
broader	O
issues	O
of	O
copying	O
human	O
abilities	O
such	O
as	O
speech	O
and	O
the	O
use	O
of	O
language	O
to	O
the	O
practical	O
commercial	O
scientific	O
and	O
engineering	O
disciplines	O
of	O
pattern	B
recognition	I
modelling	O
and	O
prediction	O
the	O
pursuit	O
of	O
technology	O
is	O
a	O
strong	O
driving	O
force	O
for	O
researchers	O
both	O
in	O
academia	O
and	O
industry	O
in	O
many	O
fields	O
of	O
science	O
and	O
engineering	O
in	O
neural	B
networks	I
as	O
in	O
machine	O
learning	O
the	O
excitement	O
of	O
technological	O
progress	O
is	O
supplemented	O
by	O
the	O
challenge	O
of	O
reproducing	O
intelligence	O
itself	O
a	O
broad	O
class	B
of	O
techniques	O
can	O
come	O
under	O
this	O
heading	O
but	O
generally	O
neural	B
networks	I
consist	O
of	O
layers	O
of	O
interconnected	O
nodes	O
each	O
node	O
producing	O
a	O
non-linear	O
function	O
of	O
its	O
input	B
the	O
input	B
to	O
a	O
node	O
may	O
come	O
from	O
other	O
nodes	O
or	O
directly	O
from	O
the	O
input	B
data	O
also	O
some	O
nodes	O
are	O
identified	O
with	O
the	O
output	B
of	O
the	O
network	O
the	O
complete	O
network	O
therefore	O
represents	O
a	O
very	O
complex	B
set	O
of	O
interdependencies	O
which	O
may	O
incorporate	O
any	O
degree	O
of	O
nonlinearity	O
allowing	O
very	O
general	O
functions	O
to	O
be	O
modelled	O
in	O
the	O
simplest	O
networks	O
the	O
output	B
from	O
one	O
node	O
is	O
fed	O
into	O
another	O
node	O
in	O
such	O
a	O
way	O
as	O
to	O
propagate	O
messages	O
through	O
layers	O
of	O
interconnecting	O
nodes	O
more	O
complex	B
behaviour	O
may	O
be	O
modelled	O
by	O
networks	O
in	O
which	O
the	O
final	O
output	B
nodes	O
are	O
connected	O
with	O
earlier	O
nodes	O
and	O
then	O
the	O
system	O
has	O
the	O
characteristics	O
of	O
a	O
highly	O
nonlinear	O
system	O
with	O
feedback	O
it	O
has	O
been	O
argued	O
that	O
neural	B
networks	I
mirror	O
to	O
a	O
certain	O
extent	O
the	O
behaviour	O
of	O
networks	O
of	O
neurons	B
in	O
the	O
brain	O
neural	B
network	I
approaches	I
combine	O
the	O
complexity	O
of	O
some	O
of	O
the	O
statistical	B
techniques	O
with	O
the	O
machine	O
learning	O
objective	O
of	O
imitating	O
human	O
intelligence	O
however	O
this	O
is	O
done	O
at	O
a	O
more	O
unconscious	O
level	O
and	O
hence	O
there	O
is	O
no	O
accompanying	O
ability	O
to	O
make	O
learned	O
concepts	O
transparent	O
to	O
the	O
user	O
conclusions	O
the	O
three	O
broad	O
approachesoutlined	O
above	O
form	O
the	O
basis	O
of	O
the	O
grouping	O
of	O
procedures	O
used	O
in	O
this	O
book	O
the	O
correspondence	O
between	O
type	O
of	O
technique	O
and	O
professional	O
background	O
is	O
inexact	O
for	O
example	B
techniques	O
that	O
use	O
decision	B
trees	I
have	O
been	O
developed	O
in	O
parallel	O
both	O
within	O
the	O
machine	O
learning	O
community	O
motivated	O
by	O
psychological	O
research	O
or	O
knowledge	O
acquisition	O
for	O
expert	B
systems	I
and	O
within	O
the	O
statistical	B
profession	O
as	O
a	O
response	O
to	O
the	O
perceived	O
limitations	O
of	O
classical	B
discrimination	B
techniques	I
based	O
on	O
linear	O
functions	O
similarly	O
strong	O
parallels	O
may	O
be	O
drawn	O
between	O
advanced	O
regression	O
techniques	O
developed	O
in	O
statistics	O
and	O
neural	O
network	O
models	O
with	O
a	O
background	O
in	O
psychology	O
computer	O
science	O
and	O
artificial	O
intelligence	O
it	O
is	O
the	O
aim	O
of	O
this	O
book	O
to	O
put	O
all	O
methods	O
to	O
the	O
test	O
of	O
experiment	O
and	O
to	O
give	O
an	O
objective	O
assessment	O
of	O
their	O
strengths	O
and	O
weaknesses	O
techniques	O
have	O
been	O
grouped	O
according	O
to	O
the	O
above	O
categories	O
it	O
is	O
not	O
always	O
straightforward	O
to	O
select	O
a	O
group	O
for	O
example	B
some	O
procedures	O
can	O
be	O
considered	O
as	O
a	O
development	O
from	O
linear	B
regression	I
but	O
have	O
strong	O
affinity	O
to	O
neural	B
networks	I
when	O
deciding	O
on	O
a	O
group	O
for	O
a	O
specific	O
technique	O
we	O
have	O
attempted	O
to	O
ignore	O
its	O
professional	O
pedigree	O
and	O
classify	O
according	O
to	O
its	O
essential	O
nature	O
introduction	O
the	O
statlog	B
project	O
the	O
fragmentation	O
amongst	O
different	O
disciplines	O
has	O
almost	O
certainly	O
hindered	O
communi	O
cation	O
and	O
progress	O
the	O
statlog	B
project	O
was	O
designed	O
to	O
break	O
down	O
these	O
divisions	O
by	O
selecting	O
classification	B
procedures	O
regardless	O
of	O
historical	O
pedigree	O
testing	O
them	O
on	O
large-scale	O
and	O
commercially	O
important	O
problems	O
and	O
hence	O
to	O
determine	O
to	O
what	O
extent	O
the	O
various	O
techniques	O
met	O
the	O
needs	O
of	O
industry	O
this	O
depends	O
critically	O
on	O
a	O
clear	O
understanding	O
of	O
measures	B
of	O
performance	O
or	O
benchmarks	O
to	O
monitor	O
the	O
success	O
of	O
the	O
method	O
in	O
a	O
the	O
aims	O
of	O
each	O
classificationdecision	O
procedure	O
the	O
class	B
of	O
problems	O
for	O
which	O
it	O
is	O
most	O
suited	O
particular	O
application	O
about	O
procedures	O
were	O
considered	O
for	O
about	O
datasetsso	O
that	O
results	O
were	O
obtained	O
from	O
around	O
large	O
scale	O
experiments	O
the	O
set	O
of	O
methods	O
to	O
be	O
considered	O
was	O
pruned	O
after	O
early	O
experiments	O
using	O
criteria	O
developed	O
for	O
multi-input	O
many	O
treatments	O
and	O
multiple	O
criteria	O
experiments	O
a	O
management	O
hierarchy	B
led	O
by	O
daimler-benz	O
controlled	O
the	O
full	O
project	O
the	O
objectives	B
of	O
the	O
project	O
were	O
threefold	O
to	O
provide	O
critical	O
performance	O
measurements	O
on	O
available	O
classification	B
procedures	O
to	O
indicate	O
the	O
nature	O
and	O
scope	O
of	O
further	O
development	O
which	O
particular	O
methods	O
require	O
to	O
meet	O
the	O
expectations	O
of	O
industrial	O
users	O
to	O
indicate	O
the	O
most	O
promising	O
avenues	O
of	O
development	O
for	O
the	O
commercially	O
immature	O
approaches	O
quality	O
control	O
the	O
project	O
laid	O
down	O
strict	O
guidelines	O
for	O
the	O
testing	O
procedure	O
first	O
an	O
agreed	O
data	O
format	O
was	O
established	O
algorithms	O
were	O
deposited	O
at	O
one	O
site	O
with	O
appropriate	O
instructions	O
this	O
version	O
would	O
be	O
used	O
in	O
the	O
case	O
of	O
any	O
future	O
dispute	O
each	O
dataset	O
was	O
then	O
divided	O
into	O
a	O
training	B
set	I
and	O
a	O
testing	O
set	O
and	O
any	O
parameters	O
in	O
an	O
algorithm	O
could	O
be	O
tuned	O
or	O
estimated	O
only	O
by	O
reference	O
to	O
the	O
training	B
set	I
once	O
a	O
rule	O
had	O
been	O
determined	O
it	O
was	O
then	O
applied	O
to	O
the	O
test	O
data	O
this	O
procedure	O
was	O
validated	O
at	O
another	O
site	O
by	O
another	O
na	O
ve	O
user	O
for	O
each	O
dataset	O
in	O
the	O
first	O
phase	O
of	O
the	O
project	O
this	O
ensured	O
that	O
the	O
guidelines	O
for	O
parameter	O
selection	O
were	O
not	O
violated	O
and	O
also	O
gave	O
some	O
information	O
on	O
the	O
ease-of-use	O
for	O
a	O
non-expert	O
in	O
the	O
domain	O
unfortunately	O
these	O
guidelines	O
were	O
not	O
followed	O
for	O
the	O
radial	B
basis	I
function	I
algorithm	O
which	O
for	O
some	O
datasets	O
determined	O
the	O
number	O
of	O
centres	O
and	O
locations	O
with	O
reference	O
to	O
the	O
test	B
set	I
so	O
these	O
results	O
should	O
be	O
viewed	O
with	O
some	O
caution	O
however	O
it	O
is	O
thought	O
that	O
the	O
conclusions	O
will	O
be	O
unaffected	O
caution	O
in	O
the	O
interpretations	O
of	O
comparisons	O
there	O
are	O
some	O
strong	O
caveats	O
that	O
must	O
be	O
made	O
concerning	O
comparisons	O
between	O
techniques	O
in	O
a	O
project	O
such	O
as	O
this	O
first	O
the	O
exercise	O
is	O
necessarily	O
somewhat	O
contrived	O
in	O
any	O
real	O
application	O
there	O
should	O
be	O
an	O
iterative	O
process	O
in	O
which	O
the	O
constructor	O
of	O
the	O
classifier	B
interacts	O
with	O
the	O
esprit	O
project	O
comparative	O
testing	O
and	O
evaluation	O
of	O
statistical	B
and	O
logical	O
learning	O
algorithms	O
on	O
large-scale	O
applications	O
to	O
classification	B
prediction	O
and	O
control	O
sec	O
the	O
structure	O
of	O
this	O
volume	O
expert	O
in	O
the	O
domain	O
gaining	O
understanding	O
of	O
the	O
problem	O
and	O
any	O
limitations	O
in	O
the	O
data	O
and	O
receiving	O
feedback	O
as	O
to	O
the	O
quality	O
of	O
preliminary	O
investigations	O
in	O
contrast	O
statlog	B
datasets	O
were	O
simply	O
distributed	O
and	O
used	O
as	O
test	O
cases	O
for	O
a	O
wide	O
variety	O
of	O
techniques	O
each	O
applied	O
in	O
a	O
somewhat	O
automatic	O
fashion	O
second	O
the	O
results	O
obtained	O
by	O
applying	O
a	O
technique	O
to	O
a	O
test	O
problem	O
depend	O
on	O
three	O
factors	O
the	O
essential	O
quality	O
and	O
appropriateness	O
of	O
the	O
technique	O
the	O
actual	O
implementation	O
of	O
the	O
technique	O
as	O
a	O
computer	O
program	O
the	O
skill	O
of	O
the	O
user	O
in	O
coaxing	O
the	O
best	O
out	O
of	O
the	O
technique	O
in	O
appendix	O
b	O
we	O
have	O
described	O
the	O
implementations	O
used	O
for	O
each	O
technique	O
and	O
the	O
availability	O
of	O
more	O
advanced	O
versions	O
if	O
appropriate	O
however	O
it	O
is	O
extremely	O
difficult	O
to	O
control	O
adequately	O
the	O
variations	O
in	O
the	O
background	O
and	O
ability	O
of	O
all	O
the	O
experimenters	O
in	O
statlog	B
particularly	O
with	O
regard	O
to	O
data	O
analysis	O
and	O
facility	O
in	O
tuning	O
procedures	O
to	O
give	O
their	O
best	O
individual	O
techniques	O
may	O
therefore	O
have	O
suffered	O
from	O
poor	O
implementation	O
and	O
use	O
but	O
we	O
hope	O
that	O
there	O
is	O
no	O
overall	O
bias	B
against	O
whole	O
classes	B
of	O
procedure	O
the	O
structure	O
of	O
this	O
volume	O
the	O
present	O
text	O
has	O
been	O
produced	O
by	O
a	O
variety	O
of	O
authors	O
from	O
widely	O
differing	O
backgrounds	O
but	O
with	O
the	O
common	O
aim	O
of	O
making	O
the	O
results	O
of	O
the	O
statlog	B
project	O
accessible	O
to	O
a	O
wide	O
range	O
of	O
workers	O
in	O
the	O
fields	O
of	O
machine	O
learning	O
statistics	O
and	O
neural	B
networks	I
and	O
to	O
help	O
the	O
cross-fertilisation	O
of	O
ideas	O
between	O
these	O
groups	O
after	O
discussing	O
the	O
general	O
classification	B
problem	O
in	O
chapter	O
the	O
next	O
chapters	O
detail	O
the	O
methods	O
that	O
have	O
been	O
investigated	O
divided	O
up	O
according	O
to	O
broad	O
headings	O
of	O
classical	O
statistics	O
modern	O
statistical	B
techniques	O
decision	B
trees	I
and	O
rules	O
and	O
neural	B
networks	I
the	O
next	O
part	O
of	O
the	O
book	O
concerns	O
the	O
evaluation	O
experiments	O
and	O
includes	O
chapters	O
on	O
evaluation	O
criteria	O
a	O
survey	O
of	O
previous	O
comparative	O
studies	O
a	O
description	O
of	O
the	O
data-sets	O
and	O
the	O
results	O
for	O
the	O
different	O
methods	O
and	O
an	O
analysis	O
of	O
the	O
results	O
which	O
explores	O
the	O
characteristics	O
of	O
data-sets	O
that	O
make	O
them	O
suitable	O
for	O
particular	O
approaches	O
we	O
might	O
call	O
this	O
machine	O
learning	O
on	O
machine	O
learning	O
the	O
conclusions	O
concerning	O
the	O
experiments	O
are	O
summarised	O
in	O
chapter	O
the	O
final	O
chapters	O
of	O
the	O
book	O
broaden	O
the	O
interpretation	O
of	O
the	O
basic	O
classification	B
problem	O
the	O
fundamental	O
theme	O
of	O
representing	O
knowledge	O
using	O
different	O
formalisms	O
is	O
discussed	O
with	O
relation	O
to	O
constructing	O
classification	B
techniques	O
followed	O
by	O
a	O
summary	O
of	O
current	O
approaches	O
to	O
dynamic	O
control	O
now	O
arising	O
from	O
a	O
rephrasing	O
of	O
the	O
problem	O
in	O
terms	O
of	O
classification	B
and	O
learning	O
classification	B
r	O
j	O
henery	O
university	O
of	O
strathclyde	O
definition	O
of	O
classification	B
classification	B
has	O
two	O
distinct	O
meanings	O
we	O
may	O
be	O
given	O
a	O
set	O
of	O
observations	O
with	O
the	O
aim	O
of	O
establishing	O
the	O
existence	O
of	O
classes	B
or	O
clusters	O
in	O
the	O
data	O
or	O
we	O
may	O
know	O
for	O
certain	O
that	O
there	O
are	O
so	O
many	O
classes	B
and	O
the	O
aim	O
is	O
to	O
establish	O
a	O
rule	O
whereby	O
we	O
can	O
classify	O
a	O
new	O
observation	O
into	O
one	O
of	O
the	O
existing	O
classes	B
the	O
former	O
type	O
is	O
known	O
as	O
unsupervised	B
learning	I
clustering	B
the	O
latter	O
as	O
supervised	B
learning	I
in	O
this	O
book	O
when	O
we	O
use	O
the	O
term	O
classification	B
we	O
are	O
talking	O
of	O
supervised	B
learning	I
in	O
the	O
statistical	B
literature	O
supervised	B
learning	I
is	O
usually	O
but	O
not	O
always	O
referred	O
to	O
as	O
discrimination	B
by	O
which	O
is	O
meant	O
the	O
establishing	O
of	O
the	O
classification	B
rule	I
from	O
given	O
correctly	O
classified	O
data	O
the	O
existence	O
of	O
correctly	O
classified	O
data	O
presupposes	O
that	O
someone	O
supervisor	B
is	O
able	O
to	O
classify	O
without	O
error	O
so	O
the	O
question	O
naturally	O
arises	O
why	O
is	O
it	O
necessary	O
to	O
replace	O
this	O
exact	O
classification	B
by	O
some	O
approximation	O
rationale	O
there	O
are	O
many	O
reasons	O
why	O
we	O
may	O
wish	O
to	O
set	O
up	O
a	O
classification	B
procedure	O
and	O
some	O
of	O
these	O
are	O
discussed	O
later	O
in	O
relation	O
to	O
the	O
actual	O
datasets	O
used	O
in	O
this	O
book	O
here	O
we	O
outline	O
possible	O
reasons	O
for	O
the	O
examples	O
in	O
section	O
mechanical	O
classification	B
procedures	O
may	O
be	O
much	O
faster	O
for	O
example	B
postal	O
code	O
reading	O
machines	O
may	O
be	O
able	O
to	O
sort	O
the	O
majority	O
of	O
letters	O
leaving	O
the	O
difficult	O
cases	O
to	O
human	O
readers	O
a	O
mail	O
order	O
firm	O
must	O
take	O
a	O
decision	O
on	O
the	O
granting	O
of	O
credit	O
purely	O
on	O
the	O
basis	O
of	O
information	O
supplied	O
in	O
the	O
application	O
form	O
human	O
operators	O
may	O
well	O
have	O
biases	O
i	O
e	O
may	O
make	O
decisions	O
on	O
irrelevant	O
information	O
and	O
may	O
turn	O
away	O
good	O
customers	O
address	O
for	O
correspondence	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
university	O
of	O
strathclyde	O
glasgow	O
u	O
k	O
sec	O
definition	O
in	O
the	O
medical	O
field	O
we	O
may	O
wish	O
to	O
avoid	O
the	O
surgery	O
that	O
would	O
be	O
the	O
only	O
sure	O
way	O
of	O
making	O
an	O
exact	O
diagnosis	O
so	O
we	O
ask	O
if	O
a	O
reliable	O
diagnosis	O
can	O
be	O
made	O
on	O
purely	O
external	O
symptoms	O
the	O
supervisor	B
to	O
above	O
may	O
be	O
the	O
verdict	O
of	O
history	O
as	O
in	O
meteorology	O
or	O
stock-exchange	O
transaction	O
or	O
investment	O
and	O
loan	O
decisions	O
in	O
this	O
case	O
the	O
issue	O
is	O
one	O
of	O
forecasting	O
issues	O
there	O
are	O
also	O
many	O
issues	O
of	O
concern	O
to	O
the	O
would-be	O
classifier	B
we	O
list	O
below	O
a	O
few	O
of	O
these	O
accuracy	B
there	O
is	O
the	O
reliability	O
of	O
the	O
rule	O
usually	O
represented	O
by	O
the	O
proportion	O
of	O
correct	O
classifications	O
although	O
it	O
may	O
be	O
that	O
some	O
errors	O
are	O
more	O
serious	O
than	O
others	O
and	O
it	O
may	O
be	O
important	O
to	O
control	O
the	O
error	B
rate	I
for	O
some	O
key	O
class	B
speed	B
in	O
some	O
circumstances	O
the	O
speed	B
of	O
the	O
classifier	B
is	O
a	O
major	O
issue	O
a	O
classifier	B
that	O
is	O
accurate	O
may	O
be	O
preferred	O
over	O
one	O
that	O
is	O
accurate	O
if	O
it	O
is	O
times	O
faster	O
in	O
testing	O
such	O
differences	O
in	O
time-scales	O
are	O
not	O
uncommon	O
in	O
neural	B
networks	I
for	O
example	B
such	O
considerations	O
would	O
be	O
important	O
for	O
the	O
automatic	O
reading	O
of	O
postal	O
codes	O
or	O
automatic	O
fault	O
detection	O
of	O
items	O
on	O
a	O
production	O
line	O
for	O
example	B
comprehensibility	B
if	O
it	O
is	O
a	O
human	O
operator	O
that	O
must	O
apply	O
the	O
classification	B
procedure	O
the	O
procedure	O
must	O
be	O
easily	O
understood	O
else	O
mistakes	O
will	O
be	O
made	O
in	O
applying	O
the	O
rule	O
it	O
is	O
important	O
also	O
that	O
human	O
operators	O
believe	O
the	O
system	O
an	O
oft-quoted	O
example	B
is	O
the	O
three-mile	B
island	I
case	O
where	O
the	O
automatic	O
devices	O
correctly	O
recommended	O
a	O
shutdown	O
but	O
this	O
recommendation	O
was	O
not	O
acted	O
upon	O
by	O
the	O
human	O
operators	O
who	O
did	O
not	O
believe	O
that	O
the	O
recommendation	O
was	O
well	O
founded	O
a	O
similar	O
story	O
applies	O
to	O
the	O
chernobyl	B
disaster	O
time	B
to	I
learn	I
especially	O
in	O
a	O
rapidly	O
changing	O
environment	O
it	O
may	O
be	O
necessary	O
to	O
learn	O
a	O
classification	B
rule	I
quickly	O
or	O
make	O
adjustments	O
to	O
an	O
existing	O
rule	O
in	O
real	O
time	B
quickly	O
might	O
imply	O
also	O
that	O
we	O
need	O
only	O
a	O
small	O
number	O
of	O
observations	O
to	O
establish	O
our	O
rule	O
at	O
one	O
extreme	O
consider	O
the	O
na	O
ve	O
neighbour	O
rule	O
in	O
which	O
the	O
training	B
set	I
is	O
searched	O
for	O
the	O
nearest	O
a	O
defined	O
sense	O
previous	O
example	B
whose	O
class	B
is	O
then	O
assumed	O
for	O
the	O
new	O
case	O
this	O
is	O
very	O
fast	O
to	O
learn	O
time	B
at	O
all	O
but	O
is	O
very	O
slow	O
in	O
practice	O
if	O
all	O
the	O
data	O
are	O
used	O
if	O
you	O
have	O
a	O
massively	O
parallel	O
computer	O
you	O
might	O
speed	B
up	O
the	O
method	O
considerably	O
at	O
the	O
other	O
extreme	O
there	O
are	O
cases	O
where	O
it	O
is	O
very	O
useful	O
to	O
have	O
a	O
quick-and-dirty	O
method	O
possibly	O
for	O
eyeball	O
checking	O
of	O
data	O
or	O
for	O
providing	O
a	O
quick	O
cross-checking	O
on	O
the	O
results	O
of	O
another	O
procedure	O
for	O
example	B
a	O
bank	O
manager	O
might	O
know	O
that	O
the	O
simple	O
rule-of-thumb	O
only	O
give	O
credit	O
to	O
applicants	O
who	O
already	O
have	O
a	O
bank	O
account	O
is	O
a	O
fairly	O
reliable	O
rule	O
if	O
she	O
notices	O
that	O
the	O
new	O
assistant	B
the	O
new	O
automated	O
procedure	O
is	O
mostly	O
giving	O
credit	O
to	O
customers	O
who	O
do	O
not	O
have	O
a	O
bank	O
account	O
she	O
would	O
probably	O
wish	O
to	O
check	O
that	O
the	O
new	O
assistant	B
new	O
procedure	O
was	O
operating	O
correctly	O
classification	B
class	B
definitions	I
an	O
important	O
question	O
that	O
is	O
improperly	O
understood	O
in	O
many	O
studies	O
of	O
classification	B
is	O
the	O
nature	O
of	O
the	O
classes	B
and	O
the	O
way	O
that	O
they	O
are	O
defined	O
we	O
can	O
distinguish	O
three	O
common	O
cases	O
only	O
the	O
first	O
leading	O
to	O
what	O
statisticians	O
would	O
term	O
classification	B
classes	B
correspond	O
to	O
labels	O
for	O
different	O
populations	O
membership	O
of	O
the	O
various	O
populations	O
is	O
not	O
in	O
question	O
for	O
example	B
dogs	O
and	O
cats	O
form	O
quite	O
separate	O
classes	B
or	O
populations	O
and	O
it	O
is	O
known	O
with	O
certainty	O
whether	O
an	O
animal	O
is	O
a	O
dog	O
or	O
a	O
cat	O
neither	O
membership	O
of	O
a	O
class	B
or	O
population	O
is	O
determined	O
by	O
an	O
independent	O
authority	O
supervisor	B
the	O
allocation	O
to	O
a	O
class	B
being	O
determined	O
independently	O
of	O
any	O
particular	O
attributes	B
or	O
variables	O
classes	B
result	O
from	O
a	O
prediction	O
problem	O
here	O
class	B
is	O
essentially	O
an	O
outcome	O
that	O
must	O
be	O
predicted	O
from	O
a	O
knowledge	O
of	O
the	O
attributes	B
in	O
statistical	B
terms	O
the	O
class	B
is	O
a	O
random	O
variable	O
a	O
typical	O
example	B
is	O
in	O
the	O
prediction	O
of	O
interest	O
rates	O
frequently	O
the	O
question	O
is	O
put	O
will	O
interest	O
rates	O
rise	O
or	O
not	O
classes	B
are	O
pre-defined	O
by	O
a	O
partition	O
of	O
the	O
sample	O
space	O
i	O
e	O
of	O
the	O
attributes	B
themselves	O
we	O
may	O
say	O
that	O
class	B
is	O
a	O
function	O
of	O
the	O
attributes	B
thus	O
a	O
manufactured	O
item	O
may	O
be	O
classed	O
as	O
faulty	O
if	O
some	O
attributes	B
are	O
outside	O
predetermined	O
limits	O
and	O
not	O
faulty	O
otherwise	O
there	O
is	O
a	O
rule	O
that	O
has	O
already	O
classified	O
the	O
data	O
from	O
the	O
attributes	B
the	O
problem	O
is	O
to	O
create	O
a	O
rule	O
that	O
mimics	O
the	O
actual	O
rule	O
as	O
closely	O
as	O
possible	O
many	O
credit	B
datasets	I
are	O
of	O
this	O
type	O
in	O
practice	O
datasets	O
may	O
be	O
mixtures	O
of	O
these	O
types	O
or	O
may	O
be	O
somewhere	O
in	O
between	O
accuracy	B
on	O
the	O
question	O
of	O
accuracy	B
we	O
should	O
always	O
bear	O
in	O
mind	O
that	O
accuracy	B
as	O
measured	O
on	O
the	O
training	B
set	I
and	O
accuracy	B
as	O
measured	O
on	O
unseen	O
data	O
test	B
set	I
are	O
often	O
very	O
different	O
indeed	O
it	O
is	O
not	O
uncommon	O
especially	O
in	O
machine	O
learning	O
applications	O
for	O
the	O
training	B
set	I
to	O
be	O
perfectly	O
fitted	O
but	O
performance	O
on	O
the	O
test	B
set	I
to	O
be	O
very	O
disappointing	O
usually	O
it	O
is	O
the	O
accuracy	B
on	O
the	O
unseen	O
data	O
when	O
the	O
true	O
classification	B
is	O
unknown	O
that	O
is	O
of	O
practical	O
importance	O
the	O
generally	O
accepted	O
method	O
for	O
estimating	O
this	O
is	O
to	O
use	O
the	O
given	O
data	O
in	O
which	O
we	O
assume	O
that	O
all	O
class	B
memberships	O
are	O
known	O
as	O
follows	O
firstly	O
we	O
use	O
a	O
substantial	O
proportion	O
training	B
set	I
of	O
the	O
given	O
data	O
to	O
train	O
the	O
procedure	O
this	O
rule	O
is	O
then	O
tested	O
on	O
the	O
remaining	O
data	O
test	B
set	I
and	O
the	O
results	O
compared	O
with	O
the	O
known	O
classifications	O
the	O
proportion	O
correct	O
in	O
the	O
test	B
set	I
is	O
an	O
unbiased	O
estimate	O
of	O
the	O
accuracy	B
of	O
the	O
rule	O
provided	O
that	O
the	O
training	B
set	I
is	O
randomly	O
sampled	O
from	O
the	O
given	O
data	O
examples	B
of	I
classifiers	I
to	O
illustrate	O
the	O
basic	O
types	O
of	O
classifiers	O
we	O
will	O
use	O
the	O
well-known	O
iris	O
dataset	O
which	O
is	O
given	O
in	O
full	O
in	O
kendall	O
stuart	O
there	O
are	O
three	O
varieties	O
of	O
iris	O
setosa	O
versicolor	O
and	O
virginica	O
the	O
length	O
and	O
breadth	O
of	O
both	O
petal	O
and	O
sepal	O
were	O
measured	O
on	O
flowers	O
of	O
each	O
variety	O
the	O
original	O
problem	O
is	O
to	O
classify	O
a	O
new	O
iris	O
flower	O
into	O
one	O
of	O
these	O
three	O
types	O
on	O
the	O
basis	O
of	O
the	O
four	O
attributes	B
and	O
sepal	O
length	O
and	O
width	O
to	O
keep	O
this	O
example	B
simple	O
however	O
we	O
will	O
look	O
for	O
a	O
classification	B
rule	I
by	O
which	O
the	O
varieties	O
can	O
be	O
distinguished	O
purely	O
on	O
the	O
basis	O
of	O
the	O
two	O
measurements	O
on	O
petal	O
length	O
sec	O
examples	B
of	I
classifiers	I
and	O
width	O
we	O
have	O
available	O
fifty	O
pairs	O
of	O
measurements	O
of	O
each	O
variety	O
from	O
which	O
to	O
learn	O
the	O
classification	B
rule	I
fisher	O
s	O
linear	O
discriminants	O
this	O
is	O
one	O
of	O
the	O
oldest	O
classification	B
procedures	O
and	O
is	O
the	O
most	O
commonly	O
implemented	O
in	O
computer	O
packages	O
the	O
idea	O
is	O
to	O
divide	O
sample	O
space	O
by	O
a	O
series	O
of	O
lines	O
in	O
two	O
dimensions	O
planes	O
in	O
and	O
generally	O
hyperplanes	O
in	O
many	O
dimensions	O
the	O
line	O
dividing	O
two	O
classes	B
is	O
drawn	O
to	O
bisect	O
the	O
line	O
joining	O
the	O
centres	O
of	O
those	O
classes	B
the	O
direction	O
of	O
the	O
line	O
is	O
determined	O
by	O
the	O
shape	O
of	O
the	O
clusters	O
of	O
points	O
for	O
example	B
to	O
differentiate	O
between	O
versicolor	O
and	O
virginica	O
the	O
following	O
rule	O
is	O
applied	O
fisher	O
s	O
linear	O
discriminants	O
applied	O
to	O
the	O
iris	B
data	I
are	O
shown	O
in	O
figure	O
six	O
of	O
the	O
observations	O
would	O
be	O
misclassified	O
if	O
petal	O
width	O
if	O
petal	O
width	O
petal	O
length	O
then	O
versicolor	O
petal	O
length	O
then	O
virginica	O
virginica	O
t	O
h	O
d	O
w	O
i	O
l	O
t	O
a	O
e	O
p	O
setosa	O
s	O
s	O
s	O
s	O
s	O
ss	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ssss	O
ss	O
ss	O
s	O
s	O
s	O
ss	O
ss	O
s	O
s	O
s	O
ss	O
ss	O
sss	O
ss	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
a	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
e	O
a	O
a	O
a	O
a	O
a	O
e	O
aa	O
a	O
e	O
e	O
e	O
a	O
e	O
versicolor	O
petal	O
length	O
fig	O
classification	B
by	O
linear	O
discriminants	O
iris	B
data	I
decision	O
tree	O
and	O
rule-based	B
methods	I
one	O
class	B
of	O
classification	B
procedures	O
is	O
based	O
on	O
recursive	B
partitioning	I
of	O
the	O
sample	O
space	O
space	O
is	O
divided	O
into	O
boxes	B
and	O
at	O
each	O
stage	O
in	O
the	O
procedure	O
each	O
box	O
is	O
examined	O
to	O
see	O
if	O
it	O
may	O
be	O
split	O
into	O
two	O
boxes	B
the	O
split	O
usually	O
being	O
parallel	O
to	O
the	O
coordinate	O
axes	O
an	O
example	B
for	O
the	O
iris	B
data	I
follows	O
if	O
petal	O
length	O
then	O
setosa	O
if	O
petal	O
length	O
then	O
virginica	O
classification	B
if	O
petal	O
length	O
then	O
if	O
petal	O
width	O
then	O
versicolor	O
if	O
petal	O
width	O
then	O
virginica	O
the	O
resulting	O
partition	O
is	O
shown	O
in	O
figure	O
note	O
that	O
this	O
classification	B
rule	I
has	O
three	O
mis-classifications	O
h	O
t	O
i	O
d	O
w	O
l	O
a	O
t	O
e	O
p	O
setosa	O
s	O
s	O
s	O
s	O
s	O
ss	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ssss	O
ss	O
ss	O
s	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ss	O
s	O
ss	O
sss	O
ss	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
e	O
a	O
a	O
a	O
a	O
a	O
a	O
aa	O
e	O
e	O
e	O
e	O
a	O
e	O
virginica	O
virginica	O
a	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
versicolor	O
petal	O
length	O
fig	O
classification	B
by	O
decision	O
tree	O
iris	B
data	I
otherwise	O
versicolor	O
weiss	O
kapouleas	O
give	O
an	O
alternative	O
classification	B
rule	I
for	O
the	O
iris	B
data	I
that	O
is	O
very	O
directly	O
related	O
to	O
figure	O
their	O
rule	O
can	O
be	O
obtained	O
from	O
figure	O
by	O
continuing	O
the	O
dotted	O
line	O
to	O
the	O
left	O
and	O
can	O
be	O
stated	O
thus	O
notice	O
that	O
this	O
rule	O
while	O
equivalent	O
to	O
the	O
rule	O
illustrated	O
in	O
figure	O
is	O
stated	O
more	O
concisely	O
and	O
this	O
formulation	O
may	O
be	O
preferred	O
for	O
this	O
reason	O
notice	O
also	O
that	O
the	O
rule	O
is	O
if	O
petal	O
length	O
then	O
setosa	O
if	O
petal	O
length	O
or	O
petal	O
width	O
then	O
virginica	O
ambiguous	O
if	O
petal	O
length	O
and	O
petal	O
width	O
the	O
quoted	O
rules	O
may	O
be	O
made	O
unambiguous	O
by	O
applying	O
them	O
in	O
the	O
given	O
order	O
and	O
they	O
are	O
then	O
just	O
a	O
re-statement	O
of	O
the	O
previous	O
decision	O
tree	O
the	O
rule	O
discussed	O
here	O
is	O
an	O
instance	O
of	O
a	O
rule-based	O
method	O
such	O
methods	O
have	O
very	O
close	O
links	O
with	O
decision	B
trees	I
k-nearest-neighbour	O
we	O
illustrate	O
this	O
technique	O
on	O
the	O
iris	B
data	I
suppose	O
a	O
new	O
iris	O
is	O
to	O
be	O
classified	O
the	O
idea	O
is	O
that	O
it	O
is	O
most	O
likely	O
to	O
be	O
near	O
to	O
observations	O
from	O
its	O
own	O
proper	O
population	O
so	O
we	O
look	O
at	O
the	O
five	O
nearest	O
observations	O
from	O
all	O
previously	O
recorded	O
irises	O
and	O
classify	O
sec	O
variable	O
selection	O
the	O
observation	O
according	O
to	O
the	O
most	O
frequent	O
class	B
among	O
its	O
neighbours	O
in	O
figure	O
the	O
apparent	O
elliptical	O
shape	O
is	O
due	O
to	O
the	O
differing	O
horizontal	O
and	O
vertical	O
scales	O
but	O
the	O
proper	O
scaling	O
of	O
the	O
observations	O
is	O
a	O
major	O
difficulty	O
of	O
this	O
method	O
and	O
the	O
nearest	O
observations	O
lie	O
within	O
the	O
circle	O
the	O
new	O
observation	O
is	O
marked	O
by	O
a	O
centred	O
on	O
the	O
this	O
is	O
illustrated	O
in	O
figure	O
where	O
an	O
observation	O
centred	O
at	O
would	O
be	O
classified	O
as	O
virginica	O
since	O
it	O
has	O
virginica	O
among	O
its	O
nearest	O
neighbours	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
e	O
a	O
a	O
a	O
a	O
a	O
a	O
aa	O
e	O
e	O
e	O
e	O
a	O
e	O
virginica	O
a	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
h	O
t	O
i	O
d	O
w	O
l	O
a	O
t	O
e	O
p	O
s	O
s	O
s	O
s	O
s	O
ss	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ssss	O
ss	O
ss	O
s	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ss	O
s	O
ss	O
sss	O
ss	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
petal	O
length	O
fig	O
classification	B
by	O
iris	B
data	I
choice	B
of	I
variables	I
as	O
we	O
have	O
just	O
pointed	O
out	O
in	O
relation	O
to	O
k-nearest	B
neighbour	I
it	O
may	O
be	O
necessary	O
to	O
reduce	O
the	O
weight	O
attached	O
to	O
some	O
variables	O
by	O
suitable	O
scaling	O
at	O
one	O
extreme	O
we	O
might	O
remove	O
some	O
variables	O
altogether	O
if	O
they	O
do	O
not	O
contribute	O
usefully	O
to	O
the	O
discrimination	B
although	O
this	O
is	O
not	O
always	O
easy	O
to	O
decide	O
there	O
are	O
established	O
procedures	O
example	B
forward	B
stepwise	B
selection	I
for	O
removing	O
unnecessary	O
variables	O
in	O
linear	O
discriminants	O
but	O
for	O
large	O
datasets	O
the	O
performance	O
of	O
linear	O
discriminants	O
is	O
not	O
seriously	O
affected	O
by	O
including	O
such	O
unnecessary	O
variables	O
in	O
contrast	O
the	O
presence	O
of	O
irrelevant	O
variables	O
is	O
always	O
a	O
problem	O
with	O
k-nearest	B
neighbour	I
regardless	O
of	O
dataset	O
size	O
transformations	O
and	O
combinations	B
of	I
variables	I
often	O
problems	O
can	O
be	O
simplified	O
by	O
a	O
judicious	O
transformation	B
of	O
variables	O
with	O
statistical	B
procedures	O
the	O
aim	O
is	O
usually	O
to	O
transform	O
the	O
attributes	B
so	O
that	O
their	O
marginal	O
density	O
is	O
approximately	O
normal	O
usually	O
by	O
applying	O
a	O
monotonic	O
transformation	B
of	O
the	O
power	O
law	O
type	O
monotonic	O
transformations	O
do	O
not	O
affect	O
the	O
machine	O
learning	O
methods	O
but	O
they	O
can	O
benefit	O
by	O
combining	O
variables	O
for	O
example	B
by	O
taking	O
ratios	O
or	O
differences	O
of	O
key	O
variables	O
background	B
knowledge	I
of	O
the	O
problem	O
is	O
of	O
help	O
in	O
determining	O
what	O
transformation	B
or	O
classification	B
combination	O
to	O
use	O
for	O
example	B
in	O
the	O
iris	B
data	I
the	O
product	O
of	O
the	O
variables	O
petal	O
length	O
and	O
petal	O
width	O
gives	O
a	O
single	O
attribute	O
which	O
has	O
the	O
dimensions	O
of	O
area	O
and	O
might	O
be	O
labelled	O
as	O
petal	O
area	O
it	O
so	O
happens	O
that	O
a	O
decision	O
rule	O
based	O
on	O
the	O
single	O
variable	O
petal	O
area	O
is	O
a	O
good	O
classifier	B
with	O
only	O
four	O
errors	O
if	O
petal	O
area	O
then	O
setosa	O
if	O
petal	O
area	O
then	O
virginica	O
if	O
petal	O
area	O
then	O
virginica	O
this	O
tree	O
while	O
it	O
has	O
one	O
more	O
error	O
than	O
the	O
decision	O
tree	O
quoted	O
earlier	O
might	O
be	O
preferred	O
on	O
the	O
grounds	O
of	O
conceptual	O
simplicity	O
as	O
it	O
involves	O
only	O
one	O
concept	B
namely	O
petal	O
area	O
also	O
one	O
less	O
arbitrary	O
constant	O
need	O
be	O
remembered	O
there	O
is	O
one	O
less	O
node	O
or	O
cut-point	O
in	O
the	O
decision	B
trees	I
classification	B
of	O
classification	B
procedures	O
the	O
above	O
three	O
procedures	O
discrimination	B
decision-tree	O
and	O
rule-based	O
k-nearest	B
neighbour	I
are	O
prototypes	B
for	O
three	O
types	O
of	O
classification	B
procedure	O
not	O
surprisingly	O
they	O
have	O
been	O
refined	O
and	O
extended	O
but	O
they	O
still	O
represent	O
the	O
major	O
strands	O
in	O
current	O
classification	B
practice	O
and	O
research	O
the	O
procedures	O
investigated	O
in	O
this	O
book	O
can	O
be	O
directly	O
linked	O
to	O
one	O
or	O
other	O
of	O
the	O
above	O
however	O
within	O
this	O
book	O
the	O
methods	O
have	O
been	O
grouped	O
around	O
the	O
more	O
traditional	O
headings	O
of	O
classical	O
statistics	O
modern	O
statistical	B
techniques	O
machine	O
learning	O
and	O
neural	B
networks	I
chapters	O
respectively	O
are	O
devoted	O
to	O
each	O
of	O
these	O
for	O
some	O
methods	O
the	O
classification	B
is	O
rather	O
abitrary	O
extensions	B
to	I
linear	B
discrimination	B
we	O
can	O
include	O
in	O
this	O
group	O
those	O
procedures	O
that	O
start	O
from	O
linear	O
combinations	O
of	O
the	O
measurements	O
even	O
if	O
these	O
combinations	O
are	O
subsequently	O
subjected	O
to	O
some	O
nonlinear	O
transformation	B
there	O
are	O
procedures	O
of	O
this	O
type	O
linear	O
discriminants	O
logistic	O
discriminants	O
quadratic	B
discriminants	I
multi-layer	O
perceptron	B
and	O
cascade	B
and	O
projection	B
pursuit	I
note	O
that	O
this	O
group	O
consists	O
of	O
statistical	B
and	O
neural	O
network	O
multilayer	O
perceptron	B
methods	O
only	O
decision	B
trees	I
and	O
rule-based	B
methods	I
cart	B
indcart	B
bayes	B
tree	I
and	O
itrule	B
chapter	O
this	O
is	O
the	O
most	O
numerous	O
group	O
in	O
the	O
book	O
with	O
procedures	O
newid	B
density	B
estimates	I
this	O
group	O
is	O
a	O
little	O
less	O
homogeneous	O
but	O
the	O
members	O
have	O
this	O
in	O
common	O
the	O
procedure	O
is	O
intimately	O
linked	O
with	O
the	O
estimation	O
of	O
the	O
local	O
probability	O
density	O
at	O
each	O
point	O
in	O
sample	O
space	O
the	O
density	O
estimate	O
group	O
contains	O
k-nearest	B
neighbour	I
radial	O
basis	O
functions	O
naive	B
bayes	I
polytrees	B
kohonen	B
self-organising	I
net	I
lvq	B
and	O
the	O
kernel	O
density	O
method	O
this	O
group	O
also	O
contains	O
only	O
statistical	B
and	O
neural	O
net	O
methods	O
a	O
general	O
structure	O
for	O
classification	B
problems	O
there	O
are	O
three	O
essential	O
components	O
to	O
a	O
classification	B
problem	O
the	O
relative	O
frequency	O
with	O
which	O
the	O
classes	B
occur	O
in	O
the	O
population	O
of	O
interest	O
expressed	O
formally	O
as	O
the	O
prior	O
probability	O
distribution	O
sec	O
costs	B
of	O
misclassification	O
an	O
implicit	O
or	O
explicit	O
criterion	O
for	O
separating	O
the	O
classes	B
we	O
may	O
think	O
of	O
an	O
underlying	O
inputoutput	O
relation	O
that	O
uses	O
observed	O
attributes	B
to	O
distinguish	O
a	O
random	O
individual	O
from	O
each	O
class	B
the	O
cost	O
associated	O
with	O
making	O
a	O
wrong	O
classification	B
most	O
techniques	O
implicitly	O
confound	O
components	O
and	O
for	O
example	B
produce	O
a	O
classification	B
rule	I
that	O
is	O
derived	O
conditional	O
on	O
a	O
particular	O
prior	O
distribution	O
and	O
cannot	O
easily	O
be	O
adapted	O
to	O
a	O
change	O
in	O
class	B
frequency	O
however	O
in	O
theory	O
each	O
of	O
these	O
components	O
may	O
be	O
individually	O
studied	O
and	O
then	O
the	O
results	O
formally	O
combined	O
into	O
a	O
classification	B
rule	I
we	O
shall	O
describe	O
this	O
development	O
below	O
prior	B
probabilities	I
and	O
the	O
default	B
rule	I
is	O
that	O
with	O
the	O
least	O
expected	O
cost	O
below	O
for	O
the	O
class	B
be	O
irrespective	O
of	O
the	O
attributes	B
of	O
the	O
example	B
this	O
no-data	O
or	O
default	B
rule	I
may	O
even	O
be	O
adopted	O
in	O
practice	O
if	O
the	O
cost	O
of	O
gathering	O
the	O
data	O
is	O
too	O
high	O
thus	O
banks	O
may	O
give	O
credit	O
to	O
all	O
their	O
established	O
customers	O
for	O
the	O
sake	O
of	O
good	O
customer	O
relations	O
here	O
the	O
cost	O
of	O
gathering	O
the	O
data	O
is	O
the	O
risk	O
of	O
losing	O
customers	O
the	O
default	B
rule	I
relies	O
only	O
on	O
knowledge	O
of	O
the	O
prior	B
probabilities	I
and	O
clearly	O
the	O
decision	O
rule	O
that	O
has	O
the	O
greatest	O
chance	O
of	O
success	O
is	O
to	O
allocate	O
every	O
new	O
observation	O
to	O
the	O
most	O
frequent	O
class	B
however	O
if	O
some	O
classification	B
errors	O
are	O
more	O
serious	O
than	O
others	O
we	O
adopt	O
the	O
minimum	O
risk	O
we	O
need	O
to	O
introduce	O
some	O
notation	O
let	O
the	O
classes	B
be	O
denoted	O
and	O
let	O
the	O
prior	O
probability	O
it	O
is	O
always	O
possible	O
to	O
use	O
the	O
no-data	O
rule	O
classify	O
any	O
new	O
observation	O
as	O
expected	O
cost	O
rule	O
and	O
the	O
suppose	O
we	O
are	O
able	O
to	O
observe	O
data	O
on	O
an	O
individual	O
and	O
that	O
we	O
know	O
the	O
probability	O
distribution	O
of	O
within	O
each	O
then	O
for	O
any	O
two	O
to	O
likelihood	O
provides	O
the	O
theoretical	O
optimal	O
form	O
for	O
discriminating	O
the	O
classes	B
on	O
the	O
basis	O
of	O
data	O
object	O
as	O
class	B
suppose	O
the	O
cost	O
of	O
misclassifying	O
a	O
class	B
decisions	O
should	O
all	O
new	O
observations	O
to	O
the	O
classi	O
using	O
suffixj	O
as	O
label	O
for	O
the	O
decision	B
class	B
when	O
examples	O
decision	O
is	O
made	O
for	O
all	O
new	O
examples	O
a	O
cost	O
is	O
incurred	O
for	O
class	B
i	O
of	O
making	O
decision	O
so	O
the	O
expected	O
cost	O
and	O
these	O
occur	O
with	O
probability	O
the	O
bayes	B
minimum	B
cost	I
rule	I
chooses	O
that	O
class	B
that	O
has	O
the	O
lowest	O
expected	O
cost	O
to	O
see	O
the	O
relation	O
between	O
the	O
minimum	O
error	O
and	O
minimum	O
cost	O
rules	O
suppose	O
the	O
cost	O
of	O
the	O
majority	O
of	O
techniques	O
featured	O
in	O
this	O
book	O
can	O
be	O
thought	O
of	O
as	O
implicitly	O
or	O
explicitly	O
deriving	O
an	O
approximate	O
form	O
for	O
this	O
likelihood	B
ratio	I
be	O
based	O
on	O
the	O
principle	O
that	O
the	O
total	O
cost	O
of	O
misclassifications	O
should	O
be	O
minimised	O
for	O
a	O
new	O
observation	O
this	O
means	O
minimising	O
the	O
expected	O
cost	O
of	O
misclassification	O
let	O
us	O
first	O
consider	O
the	O
expected	O
cost	O
of	O
applying	O
the	O
default	B
decision	O
rule	O
allocate	O
the	O
is	O
separating	O
classes	B
misclassification	B
costs	B
a	O
i	O
i	O
i	O
classification	B
misclassifications	O
to	O
be	O
the	O
same	O
for	O
all	O
errors	O
and	O
zero	O
when	O
a	O
class	B
is	O
correctly	O
identified	O
foryzg	O
then	O
the	O
expected	O
cost	O
is	O
i	O
e	O
suppose	O
forut	O
fsf	O
and	O
the	O
minimum	B
cost	I
rule	I
is	O
to	O
allocate	O
to	O
the	O
class	B
with	O
the	O
greatest	O
prior	O
probability	O
misclassification	B
costs	B
are	O
very	O
difficult	O
to	O
obtain	O
in	O
practice	O
even	O
in	O
situations	O
where	O
it	O
is	O
very	O
clear	O
that	O
there	O
are	O
very	O
great	O
inequalities	O
in	O
the	O
sizes	O
of	O
the	O
possible	O
penalties	O
or	O
rewards	O
for	O
making	O
the	O
wrong	O
or	O
right	O
decision	O
it	O
is	O
often	O
very	O
difficult	O
to	O
quantify	O
them	O
typically	O
they	O
may	O
vary	O
from	O
individual	O
to	O
individual	O
as	O
in	O
the	O
case	O
of	O
applications	O
for	O
credit	O
of	O
varying	O
amounts	O
in	O
widely	O
differing	O
circumstances	O
in	O
one	O
dataset	O
we	O
have	O
assumed	O
the	O
misclassification	B
costs	B
to	O
be	O
the	O
same	O
for	O
all	O
individuals	O
practice	O
creditgranting	O
companies	O
must	O
assess	O
the	O
potential	O
costs	B
for	O
each	O
applicant	O
and	O
in	O
this	O
case	O
the	O
classification	B
algorithm	O
usually	O
delivers	O
an	O
assessment	O
of	O
probabilities	O
and	O
the	O
decision	O
is	O
left	O
to	O
the	O
human	O
operator	O
if	O
we	O
wish	O
to	O
use	O
a	O
minimum	B
cost	I
rule	I
we	O
must	O
first	O
calculate	O
the	O
expected	O
costs	B
of	O
the	O
we	O
can	O
now	O
see	O
how	O
the	O
three	O
components	O
introduced	O
above	O
may	O
be	O
combined	O
into	O
a	O
classification	B
procedure	O
about	O
an	O
individual	O
the	O
situation	O
is	O
in	O
principle	O
unchanged	O
from	O
the	O
no-data	O
situation	O
the	O
difference	O
is	O
that	O
all	O
probabilities	O
must	O
now	O
again	O
the	O
decision	O
rule	O
with	O
least	O
probability	O
of	O
error	O
is	O
to	O
allocate	O
to	O
the	O
class	B
with	O
the	O
highest	O
probability	O
of	O
occurrence	O
but	O
now	O
the	O
bayes	B
rule	I
given	O
data	O
when	O
we	O
are	O
given	O
information	O
be	O
interpreted	O
as	O
conditional	O
on	O
the	O
data	O
given	O
the	O
data	O
of	O
class	B
relevant	O
probability	O
is	O
the	O
conditional	O
given	O
various	O
decisions	O
conditional	O
on	O
the	O
given	O
information	O
is	O
made	O
for	O
examples	O
with	O
attributes	B
now	O
when	O
decision	O
a	O
cost	O
examples	O
and	O
these	O
occur	O
with	O
as	O
the	O
is	O
incurred	O
for	O
class	B
depend	O
on	O
i	O
cost	O
of	O
making	O
decision	O
when	O
bayes	B
theorem	I
is	O
used	O
to	O
calculate	O
the	O
conditional	O
for	O
the	O
are	O
calculated	O
from	O
a	O
knowledge	O
of	O
the	O
prior	B
probabilities	I
and	O
the	O
of	O
the	O
data	O
for	O
each	O
class	B
thus	O
for	O
suppose	O
conditional	O
that	O
the	O
probability	O
of	O
observing	O
data	O
bayes	B
theorem	I
gives	O
the	O
posterior	O
as	O
for	O
class	B
in	O
the	O
special	O
case	O
of	O
equal	O
misclassification	B
costs	B
the	O
minimum	B
cost	I
rule	I
is	O
to	O
allocate	O
to	O
the	O
class	B
with	O
the	O
greatest	O
posterior	O
probability	O
classes	B
we	O
refer	O
to	O
them	O
as	O
the	O
posterior	O
probabilities	O
of	O
the	O
classes	B
then	O
the	O
posterior	O
so	O
too	O
will	O
the	O
decision	O
rule	O
so	O
too	O
will	O
the	O
expected	O
i	O
n	O
n	O
i	O
n	O
i	O
i	O
i	O
i	O
i	O
n	O
a	O
a	O
a	O
for	O
which	O
is	O
a	O
minimum	O
assuming	O
now	O
that	O
the	O
attributes	B
have	O
continuous	O
distributions	O
the	O
probabilities	O
above	O
if	O
sec	O
bayes	B
rule	I
is	O
proportional	O
the	O
divisor	O
is	O
common	O
to	O
all	O
classes	B
so	O
we	O
may	O
use	O
the	O
fact	O
i	O
with	O
minimum	O
expected	O
cost	O
risk	O
is	O
therefore	O
that	O
the	O
class	B
to	O
have	O
become	O
probability	O
densities	O
suppose	O
that	O
observations	O
drawn	O
from	O
population	O
probability	O
density	O
functionb	O
and	O
that	O
the	O
prior	O
probability	O
that	O
an	O
observation	O
belongs	O
to	O
is	O
d	O
then	O
bayes	B
theorem	I
computes	O
the	O
probability	O
that	O
an	O
observation	O
belongs	O
to	O
class	B
as	O
a	O
classification	B
rule	I
then	O
assigns	O
to	O
the	O
classi	O
with	O
maximal	O
a	O
posteriori	O
probability	O
given	O
max	O
as	O
before	O
the	O
classi	O
with	O
minimum	O
expected	O
cost	O
risk	O
is	O
that	O
for	O
which	O
a	O
then	O
and	O
consider	O
the	O
problem	O
of	O
discriminating	O
between	O
just	O
two	O
classes	B
we	O
should	O
allocate	O
to	O
class	B
assuming	O
as	O
before	O
rather	O
than	O
via	O
bayes	B
theorem	I
we	O
could	O
also	O
use	O
the	O
empirical	O
frequency	O
i	O
with	O
among	O
these	O
examples	O
the	O
minimum	O
error	O
rule	O
is	O
to	O
allocate	O
to	O
the	O
class	B
which	O
shows	O
the	O
pivotal	O
role	O
of	O
the	O
likelihood	B
ratio	I
which	O
must	O
be	O
greater	O
than	O
the	O
ratio	O
of	O
prior	B
probabilities	I
times	O
the	O
relative	O
costs	B
of	O
the	O
errors	O
we	O
note	O
the	O
symmetry	O
in	O
the	O
above	O
expression	O
changes	O
in	O
costs	B
can	O
be	O
compensated	O
in	O
changes	O
in	O
prior	O
to	O
keep	O
constant	O
the	O
threshold	O
that	O
defines	O
the	O
classification	B
rule	I
this	O
facility	O
is	O
exploited	O
in	O
some	O
techniques	O
although	O
for	O
more	O
than	O
two	O
groups	O
this	O
property	O
only	O
exists	O
under	O
restrictive	O
assumptions	O
breiman	O
et	O
al	O
page	O
version	O
of	O
bayes	B
rule	I
which	O
in	O
practice	O
would	O
require	O
prohibitively	O
large	O
amounts	O
of	O
data	O
however	O
in	O
principle	O
the	O
procedure	O
is	O
to	O
gather	O
together	O
all	O
examples	O
in	O
the	O
training	B
set	I
that	O
have	O
the	O
same	O
attributes	B
as	O
the	O
given	O
example	B
and	O
to	O
find	O
class	B
proportions	O
unless	O
the	O
number	O
of	O
attributes	B
is	O
very	O
small	O
and	O
the	O
training	O
dataset	O
very	O
large	O
it	O
will	O
be	O
necessary	O
to	O
use	O
approximations	O
to	O
estimate	O
the	O
posterior	O
class	B
probabilities	O
for	O
example	B
is	O
a	O
minimum	O
or	O
equivalently	O
bayes	B
rule	I
in	O
statistics	O
highest	O
posterior	O
probability	O
n	O
b	O
n	O
a	O
a	O
b	O
a	O
i	O
n	O
a	O
a	O
b	O
b	O
a	O
a	O
classification	B
one	O
way	O
of	O
finding	O
an	O
approximate	O
bayes	B
rule	I
would	O
be	O
to	O
use	O
not	O
just	O
examples	O
with	O
attributes	B
matching	O
exactly	O
those	O
of	O
the	O
given	O
example	B
but	O
to	O
use	O
examples	O
that	O
were	O
near	O
the	O
given	O
example	B
in	O
some	O
sense	O
the	O
minimum	O
error	O
decision	O
rule	O
would	O
be	O
to	O
allocate	O
to	O
the	O
most	O
frequent	O
class	B
among	O
these	O
matching	O
examples	O
partitioning	O
algorithms	O
and	O
decision	B
trees	I
in	O
particular	O
divide	O
up	O
attribute	O
space	O
into	O
regions	O
of	O
self-similarity	O
all	O
data	O
within	O
a	O
given	O
box	O
are	O
treated	O
as	O
similar	O
and	O
posterior	O
class	B
probabilities	O
are	O
constant	O
within	O
the	O
box	O
decision	O
rules	O
based	O
on	O
bayes	O
rules	O
are	O
optimal	O
no	O
other	O
rule	O
has	O
lower	O
expected	O
error	B
rate	I
or	O
lower	O
expected	O
misclassification	B
costs	B
although	O
unattainable	O
in	O
practice	O
they	O
provide	O
the	O
logical	O
basis	O
for	O
all	O
statistical	B
algorithms	O
they	O
are	O
unattainable	O
because	O
they	O
assume	O
complete	O
information	O
is	O
known	O
about	O
the	O
statistical	B
distributions	O
in	O
each	O
class	B
statistical	B
procedures	O
try	O
to	O
supply	O
the	O
missing	O
distributional	O
information	O
in	O
a	O
variety	O
of	O
ways	O
but	O
there	O
are	O
two	O
main	O
lines	O
parametric	O
and	O
non-parametric	O
parametric	B
methods	I
make	O
assumptions	O
about	O
the	O
nature	O
of	O
the	O
distributions	O
it	O
is	O
assumed	O
that	O
the	O
distributions	O
are	O
gaussian	O
and	O
the	O
problem	O
is	O
reduced	O
to	O
estimating	O
the	O
parameters	O
of	O
the	O
distributions	O
and	O
variances	O
in	O
the	O
case	O
of	O
gaussians	O
non-parametric	O
methods	O
make	O
no	O
assumptions	O
about	O
the	O
specific	O
distributions	O
involved	O
and	O
are	O
therefore	O
described	O
perhaps	O
more	O
accurately	O
as	O
distribution-free	O
reference	O
texts	O
there	O
are	O
several	O
good	O
textbooks	O
that	O
we	O
can	O
recommend	O
weiss	O
kulikowski	O
give	O
an	O
overall	O
view	O
of	O
classification	B
methods	O
in	O
a	O
text	O
that	O
is	O
probably	O
the	O
most	O
accessible	O
to	O
the	O
machine	O
learning	O
community	O
hand	O
lachenbruch	O
mickey	O
and	O
kendall	O
et	O
al	O
give	O
the	O
statistical	B
approach	O
breiman	O
et	O
al	O
describe	O
cart	B
which	O
is	O
a	O
partitioning	O
algorithm	O
developed	O
by	O
statisticians	O
and	O
silverman	O
discusses	O
density	B
estimation	I
methods	O
for	O
neural	O
net	O
approaches	O
the	O
book	O
by	O
hertz	O
et	O
al	O
is	O
probably	O
the	O
most	O
comprehensive	O
and	O
reliable	O
two	O
excellent	O
texts	O
on	O
pattern	B
recognition	I
are	O
those	O
of	O
fukunaga	O
who	O
gives	O
a	O
thorough	O
treatment	O
of	O
classification	B
problems	O
and	O
devijver	O
kittler	O
who	O
concentrate	O
on	O
the	O
k-nearest	B
neighbour	I
approach	O
a	O
thorough	O
treatment	O
of	O
statistical	B
procedures	O
is	O
given	O
in	O
mclachlan	O
who	O
also	O
mentions	O
the	O
more	O
important	O
alternative	O
approaches	O
a	O
recent	O
text	O
dealing	O
with	O
pattern	B
recognition	I
from	O
a	O
variety	O
of	O
perspectives	O
is	O
schalkoff	O
classical	O
statistical	B
methods	O
j	O
m	O
o	O
mitchell	O
university	O
of	O
strathclyde	O
introduction	O
this	O
chapter	O
provides	O
an	O
introduction	O
to	O
the	O
classical	O
statistical	B
discrimination	B
techniques	O
and	O
is	O
intended	O
for	O
the	O
non-statistical	O
reader	O
it	O
begins	O
with	O
fisher	O
s	O
linear	B
discriminant	I
which	O
requires	O
no	O
probability	O
assumptionsand	O
then	O
introduces	O
methods	O
based	O
on	O
maximum	B
likelihood	I
these	O
are	O
linear	B
discriminant	I
quadratic	B
discriminant	I
and	O
logistic	B
discriminant	I
next	O
there	O
is	O
a	O
brief	O
section	O
on	O
bayes	O
rules	O
which	O
indicates	O
how	O
each	O
of	O
the	O
methods	O
can	O
be	O
adapted	O
to	O
deal	O
with	O
unequal	O
prior	B
probabilities	I
and	O
unequal	O
misclassification	B
costs	B
finally	O
there	O
is	O
an	O
illustrative	O
example	B
showing	O
the	O
result	O
of	O
applying	O
all	O
three	O
methods	O
to	O
a	O
two	O
class	B
and	O
two	O
attribute	O
problem	O
for	O
full	O
details	O
of	O
the	O
statistical	B
theory	O
involved	O
the	O
reader	O
should	O
consult	O
a	O
statistical	B
text	O
book	O
for	O
example	B
n	O
methods	O
require	O
numerical	O
attribute	O
vectors	O
and	O
also	O
require	O
that	O
none	O
of	O
the	O
values	O
is	O
missing	O
where	O
an	O
attribute	O
is	O
categorical	O
with	O
two	O
values	O
an	O
indicator	O
is	O
used	O
i	O
e	O
an	O
attribute	O
which	O
takes	O
the	O
value	O
for	O
one	O
category	O
and	O
for	O
the	O
other	O
where	O
there	O
are	O
more	O
than	O
two	O
categorical	O
values	O
indicators	O
are	O
normally	O
set	O
up	O
for	O
each	O
of	O
the	O
values	O
however	O
there	O
is	O
then	O
redundancy	O
among	O
these	O
new	O
attributes	B
and	O
the	O
usual	O
procedure	O
is	O
the	O
training	B
set	I
will	O
consist	O
of	O
examples	O
drawn	O
from	O
known	O
classes	B
will	O
be	O
the	O
values	O
numerically-valued	O
attributes	B
will	O
be	O
known	O
for	O
each	O
ofj	O
examples	O
and	O
these	O
form	O
the	O
attribute	O
it	O
should	O
be	O
noted	O
that	O
these	O
to	O
drop	O
one	O
of	O
them	O
in	O
this	O
way	O
a	O
single	O
categorical	O
attribute	O
withg	O
values	O
is	O
replaced	O
by	O
attributes	B
whose	O
values	O
are	O
or	O
where	O
the	O
attribute	O
values	O
are	O
ordered	O
it	O
may	O
be	O
address	O
for	O
correspondence	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
university	O
of	O
strathclyde	O
linear	O
discriminants	O
there	O
are	O
two	O
quite	O
different	O
justifications	O
for	O
using	O
fisher	O
s	O
linear	B
discriminant	I
rule	O
the	O
first	O
as	O
given	O
by	O
fisher	O
is	O
that	O
it	O
maximises	O
the	O
separation	O
between	O
the	O
classes	B
in	O
acceptable	O
to	O
use	O
a	O
single	O
numerical-valued	O
attribute	O
care	O
has	O
to	O
be	O
taken	O
that	O
the	O
numbers	O
used	O
reflect	O
the	O
spacing	O
of	O
the	O
categories	O
in	O
an	O
appropriate	O
fashion	O
glasgow	O
u	O
k	O
g	O
classical	O
statistical	B
methods	O
a	O
least-squares	O
sense	O
the	O
second	O
is	O
by	O
maximum	B
likelihood	I
section	O
we	O
will	O
give	O
a	O
brief	O
outline	O
of	O
these	O
approaches	O
for	O
a	O
proof	O
that	O
they	O
arrive	O
at	O
the	O
same	O
solution	O
we	O
refer	O
the	O
reader	O
to	O
mclachlan	O
line	O
i	O
e	O
the	O
wrong	O
side	O
of	O
and	O
this	O
is	O
easily	O
seen	O
to	O
be	O
linear	O
discriminants	O
by	O
least	O
squares	O
fisher	O
s	O
linear	B
discriminant	I
is	O
an	O
empirical	O
method	O
for	O
classification	B
based	O
purely	O
on	O
attribute	O
vectors	O
a	O
hyperplane	O
in	O
two	O
dimensions	O
plane	O
in	O
three	O
dimensions	O
the	O
discriminant	O
between	O
the	O
classes	B
we	O
wish	O
the	O
discriminants	O
for	O
the	O
two	O
classes	B
to	O
the	O
attribute	O
vectors	O
overall	O
and	O
for	O
the	O
two	O
classes	B
suppose	O
that	O
we	O
are	O
given	O
a	O
set	O
of	O
between	O
the	O
mean	O
discriminants	O
for	O
the	O
two	O
classes	B
divided	O
by	O
the	O
standard	O
deviation	O
of	O
as	O
possible	O
points	O
are	O
classified	O
according	O
to	O
the	O
side	O
of	O
the	O
hyperplane	O
that	O
they	O
fall	O
on	O
for	O
example	B
see	O
figure	O
which	O
illustrates	O
discrimination	B
between	O
two	O
digits	O
with	O
the	O
continuous	O
line	O
as	O
the	O
discriminating	O
hyperplane	O
between	O
the	O
two	O
populations	O
this	O
procedure	O
is	O
also	O
equivalent	O
to	O
a	O
t-test	O
or	O
f-test	O
for	O
a	O
significant	O
difference	O
between	O
the	O
mean	O
discriminants	O
for	O
the	O
two	O
samples	O
the	O
t-statistic	O
or	O
f-statistic	O
being	O
constructed	O
to	O
have	O
the	O
largest	O
possible	O
value	O
etc	O
in	O
attribute	O
space	O
is	O
chosen	O
to	O
separate	O
the	O
known	O
classes	B
as	O
well	O
more	O
precisely	O
in	O
the	O
case	O
of	O
two	O
classes	B
leto	O
be	O
respectively	O
the	O
means	O
of	O
n	O
and	O
let	O
us	O
call	O
the	O
particular	O
linear	O
combination	B
of	I
attributes	B
coefficientsp	O
differ	O
as	O
much	O
as	O
possible	O
and	O
one	O
measure	B
for	O
this	O
is	O
the	O
differencer	O
the	O
discriminantsu	O
v	O
say	O
giving	O
the	O
following	O
measure	B
of	O
discrimination	B
the	O
assumption	O
of	O
a	O
multivariate	O
normal	B
distribution	I
forr	O
that	O
the	O
normal	O
random	O
variabler	O
where	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
thatr	O
are	O
not	O
of	O
equal	O
sizes	O
or	O
if	O
as	O
is	O
very	O
frequently	O
the	O
case	O
the	O
variance	O
ofr	O
of	O
variance	O
the	O
sum	B
of	I
squares	I
ofr	O
within	O
class	B
this	O
measure	B
of	O
discrimination	B
is	O
related	O
to	O
an	O
estimate	O
of	O
misclassification	O
error	O
based	O
on	O
that	O
this	O
is	O
a	O
weaker	O
assumption	O
than	O
saying	O
that	O
x	O
has	O
a	O
normal	B
distribution	I
for	O
the	O
sake	O
of	O
argument	O
we	O
set	O
the	O
dividing	O
line	O
between	O
the	O
two	O
classes	B
at	O
the	O
midpoint	O
between	O
the	O
two	O
class	B
means	O
then	O
we	O
may	O
estimate	O
the	O
probability	O
of	O
misclassification	O
for	O
one	O
class	B
as	O
the	O
probability	O
for	O
that	O
class	B
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
dividing	O
is	O
negative	O
if	O
the	O
classes	B
is	O
not	O
the	O
same	O
for	O
the	O
two	O
classes	B
the	O
dividing	O
line	O
is	O
best	O
drawn	O
at	O
some	O
point	O
other	O
than	O
the	O
midpoint	O
rather	O
than	O
use	O
the	O
simple	O
measure	B
quoted	O
above	O
it	O
is	O
more	O
convenient	O
algebraically	O
to	O
use	O
an	O
equivalent	O
measure	B
defined	O
in	O
terms	O
of	O
sums	O
of	O
squared	O
deviations	O
as	O
in	O
analysis	O
is	O
k	O
k	O
k	O
r	O
p	O
a	O
a	O
k	O
r	O
k	O
r	O
o	O
k	O
r	O
o	O
k	O
u	O
v	O
r	O
k	O
r	O
k	O
x	O
r	O
o	O
k	O
r	O
o	O
k	O
u	O
v	O
k	O
r	O
k	O
sec	O
linear	B
discrimination	B
say	O
is	O
the	O
sum	O
of	O
these	O
quantities	O
for	O
the	O
two	O
classes	B
is	O
the	O
quantity	O
that	O
would	O
give	O
where	O
this	O
last	O
sum	O
is	O
now	O
over	O
both	O
classes	B
by	O
subtraction	O
the	O
pooled	O
sum	B
of	I
squares	I
calculate	O
the	O
f-statistic	O
the	O
pooled	O
sum	B
of	I
squares	I
within	O
classesz	O
the	O
sum	O
being	O
over	O
the	O
examples	O
in	O
class	B
v	O
the	O
total	O
sum	B
of	I
squares	I
ofr	O
us	O
a	O
standard	O
deviationu	O
say	O
z	O
and	O
this	O
last	O
quantity	O
is	O
proportional	O
between	O
classes	B
is	O
in	O
terms	O
of	O
the	O
f-test	O
for	O
the	O
significance	O
of	O
the	O
differencer	O
we	O
would	O
clearly	O
maximising	O
the	O
f-ratio	O
statistic	O
is	O
equivalent	O
to	O
maximising	O
the	O
ratioldhz	O
so	O
the	O
coefficientspa	O
may	O
be	O
chosen	O
to	O
maximise	O
the	O
ratiold	O
z	O
this	O
maximisation	O
problem	O
may	O
be	O
solved	O
analytically	O
giving	O
an	O
explicit	O
solution	O
for	O
the	O
coefficientsp	O
is	O
to	O
normalise	O
thep	O
arbitrary	O
multiplicative	O
constant	O
so	O
that	O
the	O
separationr	O
between	O
the	O
class	B
ratio	O
is	O
now	O
equivalent	O
to	O
minimising	O
the	O
total	O
sum	O
of	O
squaresz	O
put	O
this	O
way	O
the	O
problem	O
there	O
is	O
however	O
an	O
arbitrary	O
multiplicative	O
constant	O
in	O
the	O
solution	O
and	O
the	O
usual	O
practice	O
in	O
some	O
way	O
so	O
that	O
the	O
solution	O
is	O
uniquely	O
determined	O
often	O
one	O
coefficient	O
is	O
taken	O
to	O
be	O
unity	O
avoiding	O
a	O
multiplication	O
however	O
the	O
detail	O
of	O
this	O
need	O
not	O
concern	O
us	O
here	O
the	O
main	O
point	O
about	O
this	O
method	O
is	O
that	O
it	O
is	O
a	O
linear	O
function	O
of	O
the	O
attributes	B
that	O
is	O
used	O
to	O
carry	O
out	O
the	O
classification	B
this	O
often	O
works	O
well	O
but	O
it	O
is	O
easy	O
to	O
see	O
that	O
it	O
may	O
work	O
badly	O
if	O
a	O
linear	O
separator	O
is	O
not	O
appropriate	O
this	O
could	O
happen	O
for	O
example	B
if	O
the	O
data	O
for	O
one	O
class	B
formed	O
a	O
tight	O
cluster	O
and	O
the	O
the	O
values	O
for	O
the	O
other	O
class	B
were	O
widely	O
spread	O
around	O
it	O
however	O
the	O
coordinate	O
system	O
used	O
is	O
of	O
no	O
importance	O
equivalent	O
results	O
will	O
be	O
obtained	O
after	O
any	O
linear	B
transformation	B
of	O
the	O
coordinates	O
is	O
identical	O
to	O
a	O
regression	O
of	O
class	B
numerically	O
on	O
the	O
attributes	B
the	O
dependent	O
variable	O
class	B
being	O
zero	O
for	O
one	O
class	B
and	O
unity	O
for	O
the	O
other	O
mean	O
discriminants	O
is	O
equal	O
to	O
some	O
predetermined	O
value	O
unity	O
maximising	O
the	O
f	O
to	O
justify	O
the	O
least	O
squares	O
of	O
the	O
title	O
for	O
this	O
section	O
note	O
that	O
we	O
may	O
choose	O
the	O
a	O
practical	O
complication	O
is	O
that	O
for	O
the	O
algorithm	O
to	O
work	O
the	O
pooled	O
sample	O
covariance	B
examples	O
from	O
matrix	O
must	O
be	O
invertible	O
the	O
covariance	B
matrix	I
for	O
a	O
dataset	O
withj	O
is	O
class	B
jy	O
is	O
row-vector	O
is	O
thejy	O
matrix	O
of	O
attribute	O
values	O
ando	O
of	O
attribute	O
means	O
the	O
pooled	B
covariance	B
matrix	I
where	O
where	O
the	O
is	O
summation	O
is	O
over	O
all	O
the	O
classes	B
and	O
the	O
divisorj	O
is	O
chosen	O
to	O
make	O
the	O
pooled	B
covariance	B
matrix	I
unbiased	O
for	O
invertibility	O
the	O
attributes	B
must	O
be	O
linearly	O
independent	O
which	O
means	O
that	O
no	O
attribute	O
may	O
be	O
an	O
exact	O
linear	O
combination	O
of	O
other	O
attributes	B
in	O
order	O
to	O
achieve	O
this	O
some	O
attributes	B
may	O
have	O
to	O
be	O
dropped	O
moreover	O
no	O
attribute	O
can	O
be	O
constant	O
within	O
each	O
class	B
of	O
course	O
an	O
attribute	O
which	O
is	O
constant	O
within	O
each	O
class	B
but	O
not	O
overall	O
may	O
be	O
an	O
excellent	O
discriminator	O
and	O
is	O
likely	O
to	O
be	O
utilised	O
in	O
decision	O
tree	O
algorithms	O
however	O
it	O
will	O
cause	O
the	O
linear	B
discriminant	I
algorithm	O
to	O
fail	O
this	O
situation	O
can	O
be	O
treated	O
by	O
adding	O
a	O
small	O
positive	O
constant	O
to	O
the	O
corresponding	O
diagonal	O
element	O
of	O
n	O
r	O
r	O
r	O
r	O
o	O
r	O
o	O
k	O
r	O
o	O
k	O
k	O
r	O
k	O
a	O
o	O
k	O
r	O
o	O
k	O
o	O
k	O
o	O
k	O
classical	O
statistical	B
methods	O
the	O
pooled	B
covariance	B
matrix	I
or	O
by	O
adding	O
random	O
noise	B
to	O
the	O
attribute	O
before	O
applying	O
the	O
algorithm	O
in	O
order	O
to	O
deal	O
with	O
the	O
case	O
of	O
more	O
than	O
two	O
classes	B
fisher	O
suggested	O
the	O
use	O
of	O
canonical	B
variates	I
first	O
a	O
linear	O
combination	O
of	O
the	O
attributes	B
is	O
chosen	O
to	O
minimise	O
the	O
ratio	O
of	O
the	O
pooled	O
within	O
class	B
sum	B
of	I
squares	I
to	O
the	O
total	O
sum	B
of	I
squares	I
then	O
further	O
linear	O
functions	O
are	O
found	O
to	O
improve	O
the	O
discrimination	B
coefficients	O
in	O
these	O
functions	O
are	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
non-zero	O
eigenvalues	O
of	O
a	O
certain	O
matrix	O
in	O
general	O
there	O
will	O
be	O
canonical	B
variates	I
it	O
may	O
turn	O
out	O
that	O
only	O
a	O
few	O
of	O
the	O
canonical	B
variates	I
are	O
important	O
then	O
an	O
observation	O
can	O
be	O
assigned	O
to	O
the	O
class	B
whose	O
centroid	O
is	O
closest	O
in	O
the	O
subspace	O
defined	O
by	O
these	O
variates	O
it	O
is	O
especially	O
useful	O
when	O
the	O
class	B
means	O
are	O
ordered	O
or	O
lie	O
along	O
a	O
simple	O
curve	O
in	O
attribute-space	O
in	O
the	O
simplest	O
case	O
the	O
class	B
means	O
lie	O
along	O
a	O
straight	O
line	O
this	O
is	O
the	O
case	O
for	O
the	O
head	B
injury	I
data	O
section	O
for	O
example	B
and	O
in	O
general	O
arises	O
when	O
the	O
classes	B
are	O
ordered	O
in	O
some	O
sense	O
in	O
this	O
book	O
this	O
procedure	O
was	O
not	O
used	O
as	O
a	O
classifier	B
but	O
rather	O
in	O
a	O
qualitative	O
sense	O
to	O
give	O
some	O
measure	B
of	O
reduced	O
dimensionality	O
in	O
attribute	O
space	O
since	O
this	O
technique	O
can	O
also	O
be	O
used	O
as	O
a	O
basis	O
for	O
explaining	O
differences	O
in	O
mean	O
vectors	O
as	O
in	O
analysis	O
of	O
variance	O
the	O
procedure	O
may	O
be	O
called	O
manova	B
standing	O
for	O
multivariate	O
analysis	O
of	O
variance	O
special	O
case	O
of	O
two	O
classes	B
the	O
linear	B
discriminant	I
procedure	O
is	O
particularly	O
easy	O
to	O
program	O
when	O
there	O
are	O
just	O
two	O
classes	B
for	O
then	O
the	O
fisher	O
discriminant	O
problem	O
is	O
equivalent	O
to	O
a	O
multiple	O
regression	O
problem	O
with	O
the	O
attributes	B
being	O
used	O
to	O
predict	O
the	O
class	B
value	O
which	O
is	O
treated	O
as	O
for	O
a	O
numerical-valued	O
variable	O
the	O
class	B
values	O
are	O
converted	O
to	O
numerical	O
values	O
is	O
given	O
the	O
value	O
a	O
standard	O
multiple	O
regression	O
package	O
is	O
then	O
used	O
to	O
predict	O
the	O
class	B
value	O
if	O
the	O
two	O
classes	B
are	O
equiprobable	O
the	O
discriminating	O
hyperplane	O
bisects	O
the	O
line	O
joining	O
the	O
class	B
centroids	O
otherwise	O
the	O
discriminating	O
hyperplane	O
is	O
closer	O
to	O
the	O
less	O
frequent	O
class	B
the	O
formulae	O
are	O
most	O
easily	O
derived	O
by	O
considering	O
the	O
multiple	O
regression	O
predictor	O
as	O
a	O
single	O
attribute	O
that	O
is	O
to	O
be	O
used	O
as	O
a	O
one-dimensional	O
discriminant	O
and	O
then	O
applying	O
the	O
formulae	O
of	O
the	O
following	O
section	O
the	O
procedure	O
is	O
simple	O
but	O
the	O
details	O
cannot	O
be	O
expressed	O
simply	O
see	O
ripley	O
for	O
the	O
explicit	O
connection	O
between	O
discrimination	B
and	O
regression	O
is	O
given	O
the	O
value	O
and	O
class	B
example	B
class	B
linear	O
discriminants	O
by	O
maximum	B
likelihood	I
the	O
justification	O
of	O
the	O
other	O
statistical	B
algorithms	O
depends	O
on	O
the	O
consideration	O
of	O
probability	O
distributions	O
and	O
the	O
linear	B
discriminant	I
procedure	O
itself	O
has	O
a	O
justification	O
of	O
this	O
kind	O
new	O
point	O
with	O
attribute	B
vector	I
x	O
is	O
then	O
assigned	O
to	O
that	O
class	B
for	O
which	O
the	O
probability	O
is	O
greatest	O
this	O
is	O
a	O
maximum	B
likelihood	I
method	O
a	O
frequently	O
made	O
assumption	O
is	O
that	O
the	O
distributions	O
are	O
normal	O
gaussian	O
with	O
different	O
means	O
but	O
the	O
same	O
covariance	B
matrix	I
the	O
probability	O
density	O
function	O
of	O
the	O
normal	B
distribution	I
is	O
are	O
independent	O
it	O
is	O
assumed	O
that	O
the	O
attribute	O
vectors	O
for	O
examples	O
of	O
class	B
a	O
and	O
follow	O
a	O
certain	O
probability	O
distribution	O
with	O
probability	O
density	O
function	O
density	O
functionb	O
sec	O
linear	B
discrimination	B
is	O
vector	O
denoting	O
the	O
mean	O
for	O
a	O
class	B
and	O
positive	O
definite	O
matrix	O
the	O
covariance	B
matrix	I
that	O
we	O
saw	O
earlier	O
is	O
the	O
sample	O
analogue	O
of	O
this	O
covariance	B
matrix	I
which	O
is	O
best	O
thought	O
of	O
as	O
a	O
set	O
of	O
coefficients	O
in	O
the	O
pdf	O
or	O
a	O
set	O
of	O
parameters	O
for	O
the	O
distribution	O
this	O
means	O
that	O
the	O
points	O
for	O
the	O
class	B
are	O
distributed	O
in	O
a	O
cluster	O
centered	O
each	O
cluster	O
has	O
the	O
same	O
orientation	O
and	O
spread	O
though	O
their	O
means	O
will	O
of	O
course	O
be	O
different	O
should	O
be	O
noted	O
that	O
there	O
is	O
in	O
theory	O
no	O
absolute	O
boundary	O
for	O
the	O
clusters	O
but	O
the	O
contours	O
for	O
the	O
probability	O
density	O
function	O
in	O
practice	O
occurrences	O
of	O
examples	O
outside	O
a	O
certain	O
ellipsoid	O
have	O
ellipsoidal	O
shape	O
will	O
be	O
extremely	O
rare	O
in	O
this	O
case	O
it	O
can	O
be	O
shown	O
that	O
the	O
boundary	O
separating	O
two	O
classes	B
defined	O
by	O
equality	O
of	O
the	O
two	O
pdfs	O
is	O
indeed	O
a	O
hyperplane	O
and	O
it	O
passes	O
through	O
the	O
mid-point	O
of	O
the	O
two	O
centres	O
its	O
equation	O
is	O
where	O
the	O
covariance	B
matrix	I
is	O
at	O
of	O
ellipsoidal	O
shape	O
described	O
by	O
where	O
denotes	O
the	O
population	O
mean	O
for	O
however	O
in	O
classification	B
the	O
exact	O
the	O
distributions	O
with	O
two	O
classes	B
if	O
the	O
sample	O
means	O
are	O
substituted	O
for	O
and	O
the	O
pooled	O
sample	O
covariance	B
matrix	I
for	O
then	O
fisher	O
s	O
linear	B
discriminant	I
is	O
obtained	O
with	O
more	O
than	O
two	O
classes	B
this	O
method	O
does	O
not	O
in	O
general	O
give	O
the	O
same	O
results	O
as	O
fisher	O
s	O
discriminant	O
distribution	O
is	O
usually	O
not	O
known	O
and	O
it	O
becomes	O
necessary	O
to	O
estimate	O
the	O
parameters	O
for	O
is	O
the	O
probability	O
density	O
more	O
than	O
two	O
classes	B
when	O
there	O
are	O
more	O
than	O
two	O
classes	B
it	O
is	O
no	O
longer	O
possible	O
to	O
use	O
a	O
single	O
linear	B
discriminant	I
score	O
to	O
separate	O
the	O
classes	B
the	O
simplest	O
procedure	O
is	O
to	O
calculate	O
a	O
linear	B
discriminant	I
for	O
each	O
class	B
this	O
discriminant	O
being	O
just	O
the	O
logarithm	O
of	O
the	O
estimated	O
probability	O
density	O
function	O
for	O
the	O
appropriate	O
class	B
with	O
constant	O
terms	O
dropped	O
sample	O
values	O
are	O
substituted	O
for	O
population	O
values	O
where	O
these	O
are	O
unknown	O
gives	O
the	O
plugin	O
estimates	O
where	O
the	O
prior	O
class	B
proportions	O
are	O
unknown	O
they	O
would	O
be	O
estimated	O
by	O
the	O
relative	O
frequencies	O
in	O
the	O
training	B
set	I
similarly	O
the	O
sample	O
means	O
and	O
pooled	B
covariance	B
matrix	I
are	O
substituted	O
for	O
the	O
population	O
means	O
and	O
covariance	B
matrix	I
suppose	O
the	O
prior	O
probability	O
of	O
is	O
and	O
of	O
in	O
and	O
is	O
the	O
normal	O
density	O
given	O
in	O
equation	O
the	O
joint	O
probability	O
of	O
observing	O
class	B
and	O
attribute	O
and	O
the	O
logarithm	O
of	O
the	O
probability	O
of	O
observing	O
class	B
and	O
attributek	O
log	O
are	O
given	O
by	O
the	O
coefficients	O
of	O
x	O
to	O
within	O
an	O
additive	O
constant	O
so	O
the	O
coefficients	O
by	O
and	O
the	O
additive	O
constant	O
log	O
to	O
obtain	O
the	O
corresponding	O
plug-in	O
formulae	O
substitute	O
the	O
rameters	O
i	O
and	O
corresponding	O
sample	O
estimators	O
for	O
for	O
i	O
for	O
examples	O
proportion	O
of	O
class	B
the	O
above	O
formulae	O
are	O
stated	O
in	O
terms	O
of	O
the	O
unknown	O
population	O
pa	O
though	O
these	O
can	O
be	O
simplified	O
by	O
subtracting	O
the	O
coefficients	O
for	O
the	O
last	O
class	B
is	O
the	O
sample	O
is	O
k	O
k	O
classical	O
statistical	B
methods	O
quadratic	B
discriminant	I
quadratic	O
discrimination	B
is	O
similar	O
to	O
linear	B
discrimination	B
but	O
the	O
boundary	O
between	O
two	O
discrimination	B
regions	O
is	O
now	O
allowed	O
to	O
be	O
a	O
quadratic	O
surface	O
when	O
the	O
assumption	O
of	O
equal	O
covariance	B
matrices	O
is	O
dropped	O
then	O
in	O
the	O
maximum	B
likelihood	I
argument	O
with	O
normal	O
distributions	O
a	O
quadratic	O
surface	O
example	B
ellipsoid	O
hyperboloid	O
etc	O
is	O
obtained	O
this	O
type	O
of	O
discrimination	B
can	O
deal	O
with	O
classifications	O
where	O
the	O
set	O
of	O
attribute	O
values	O
for	O
one	O
class	B
to	O
some	O
extent	O
surrounds	O
that	O
for	O
another	O
clarke	O
et	O
al	O
find	O
that	O
the	O
quadratic	B
discriminant	I
procedure	O
is	O
robust	O
to	O
small	O
departures	O
from	O
normality	O
and	O
that	O
heavy	O
kurtosis	B
tailed	O
distributions	O
than	O
gaussian	O
does	O
not	O
substantially	O
reduce	O
accuracy	B
however	O
the	O
number	O
of	O
parameters	O
to	O
be	O
estimated	O
and	O
the	O
difference	O
between	O
the	O
variances	O
would	O
need	O
to	O
be	O
considerable	O
to	O
justify	O
the	O
use	O
of	O
this	O
method	O
especially	O
for	O
small	O
or	O
moderate	O
sized	O
datasets	O
dunn	O
occasionally	O
differences	O
in	O
the	O
covariances	O
are	O
of	O
scale	O
only	O
and	O
some	O
simplification	O
may	O
occur	O
et	O
al	O
linear	B
discriminant	I
is	O
thought	O
to	O
be	O
still	O
effective	O
if	O
the	O
departure	O
from	O
equality	O
of	O
covariances	O
is	O
small	O
some	O
aspects	O
of	O
quadratic	O
dependence	O
may	O
be	O
included	O
in	O
the	O
linear	O
or	O
logistic	O
form	O
below	O
by	O
adjoining	O
new	O
attributes	B
that	O
are	O
quadratic	O
functions	O
of	O
the	O
given	O
attributes	B
quadratic	B
discriminant	I
programming	O
details	O
the	O
quadratic	B
discriminant	I
function	O
is	O
most	O
simply	O
defined	O
as	O
the	O
logarithm	O
of	O
the	O
appropriate	O
probability	O
density	O
function	O
so	O
that	O
one	O
quadratic	B
discriminant	I
is	O
calculated	O
for	O
each	O
class	B
the	O
procedure	O
used	O
is	O
to	O
take	O
the	O
logarithm	O
of	O
the	O
probability	O
density	O
function	O
and	O
to	O
substitute	O
the	O
sample	O
means	O
and	O
covariance	B
matrices	O
in	O
place	O
of	O
the	O
population	O
values	O
giving	O
the	O
so-called	O
plug-in	B
estimates	I
taking	O
the	O
logarithm	O
of	O
equation	O
in	O
classification	B
the	O
quadratic	B
discriminant	I
is	O
calculated	O
for	O
each	O
class	B
and	O
the	O
class	B
with	O
the	O
largest	O
discriminant	O
is	O
chosen	O
to	O
find	O
the	O
a	O
posteriori	O
class	B
probabilities	O
explicitly	O
the	O
exponential	O
is	O
taken	O
of	O
the	O
discriminant	O
and	O
the	O
resulting	O
quantities	O
normalised	O
to	O
sum	O
we	O
obtain	O
and	O
allowing	O
for	O
differing	O
prior	O
class	B
probabilities	O
log	O
here	O
it	O
is	O
understood	O
that	O
the	O
suffix	O
refers	O
to	O
as	O
the	O
quadratic	B
discriminant	I
for	O
class	B
the	O
sample	O
of	O
values	O
from	O
class	B
are	O
given	O
by	O
to	O
unity	O
section	O
thus	O
the	O
posterior	O
class	B
exp	O
and	O
associated	O
expected	O
costs	B
explicitly	O
using	O
to	O
calculate	O
the	O
class	B
the	O
formulae	O
of	O
section	O
the	O
most	O
frequent	O
problem	O
with	O
quadratic	B
discriminants	I
is	O
caused	O
when	O
some	O
attribute	O
has	O
zero	B
variance	I
in	O
one	O
class	B
for	O
then	O
the	O
covariance	B
matrix	I
cannot	O
be	O
inverted	O
one	O
way	O
of	O
avoiding	O
this	O
problem	O
is	O
to	O
add	O
a	O
small	O
positive	O
constant	O
term	O
to	O
the	O
diagonal	O
terms	O
in	O
the	O
covariance	B
matrix	I
corresponds	O
to	O
adding	O
random	O
noise	B
to	O
the	O
attributes	B
another	O
way	O
adopted	O
in	O
our	O
own	O
implementation	O
is	O
to	O
use	O
some	O
combination	O
of	O
the	O
class	B
covariance	B
and	O
the	O
pooled	O
covariance	B
if	O
there	O
is	O
a	O
cost	B
matrix	I
then	O
no	O
matter	O
the	O
number	O
of	O
classes	B
the	O
simplest	O
procedure	O
is	O
apart	O
from	O
a	O
normalising	O
factor	O
b	O
sec	O
quadratic	O
discrimination	B
is	O
the	O
sample	O
for	O
for	O
i	O
once	O
again	O
the	O
above	O
formulae	O
are	O
stated	O
in	O
terms	O
of	O
the	O
unknown	O
population	O
pa	O
has	O
an	O
option	O
for	O
quadratic	O
discrimination	B
sas	O
also	O
does	O
quadratic	O
discrimination	B
many	O
statistical	B
packages	O
allow	O
for	O
quadratic	O
discrimination	B
example	B
minitab	O
to	O
obtain	O
the	O
corresponding	O
plug-in	O
formulae	O
substitute	O
the	O
rameters	O
i	O
and	O
corresponding	O
sample	O
estimators	O
examples	O
proportion	O
of	O
class	B
regularisation	B
and	O
smoothed	O
estimates	O
the	O
main	O
problem	O
with	O
quadratic	B
discriminants	I
is	O
the	O
large	O
number	O
of	O
parameters	O
that	O
need	O
to	O
be	O
estimated	O
and	O
the	O
resulting	O
large	O
variance	O
of	O
the	O
estimated	O
discriminants	O
a	O
related	O
problem	O
is	O
the	O
presence	O
of	O
zero	O
or	O
near	O
zero	O
eigenvalues	O
of	O
the	O
sample	O
covariance	B
matrices	O
attempts	O
to	O
alleviate	O
this	O
problem	O
are	O
known	O
as	O
regularisation	B
methods	O
and	O
the	O
most	O
practically	O
useful	O
of	O
these	O
was	O
put	O
forward	B
by	O
friedman	O
who	O
proposed	O
a	O
compromise	O
between	O
linear	O
and	O
quadratic	B
discriminants	I
via	O
a	O
two-parameter	O
family	O
of	O
estimates	O
one	O
parameter	O
controls	O
the	O
smoothing	O
of	O
the	O
class	B
covariance	B
matrix	I
estimates	O
for	O
friedman	O
makes	O
the	O
is	O
the	O
pooled	B
covariance	B
matrix	I
is	O
a	O
constant	O
term	O
that	O
is	O
added	O
to	O
the	O
diagonals	O
of	O
the	O
covariance	B
matrices	O
this	O
is	O
done	O
to	O
make	O
the	O
covariance	B
matrix	I
non-singular	O
and	O
also	O
has	O
the	O
effect	O
of	O
smoothing	O
out	O
the	O
covariance	B
matrices	O
as	O
we	O
have	O
already	O
mentioned	O
in	O
connection	O
with	O
linear	O
discriminants	O
any	O
singularity	O
of	O
the	O
covariance	B
matrix	I
will	O
cause	O
problems	O
and	O
as	O
there	O
is	O
now	O
one	O
covariance	B
matrix	I
for	O
each	O
class	B
the	O
likelihood	O
of	O
such	O
a	O
problem	O
is	O
much	O
greater	O
especially	O
for	O
the	O
classes	B
with	O
small	O
sample	O
sizes	O
the	O
smoothed	O
estimate	O
of	O
the	O
class	B
covariance	B
matrix	I
is	O
where	O
is	O
the	O
class	B
sample	O
covariance	B
matrix	I
and	O
when	O
is	O
zero	O
there	O
is	O
no	O
smoothing	O
and	O
the	O
estimated	O
class	B
covariance	B
matrix	I
is	O
just	O
the	O
i	O
th	O
sample	O
covariance	B
matrix	I
when	O
the	O
are	O
unity	O
all	O
classes	B
have	O
the	O
same	O
covariance	B
matrix	I
namely	O
the	O
pooled	B
covariance	B
matrix	I
value	O
of	O
smaller	O
for	O
classes	B
with	O
larger	O
numbers	O
for	O
the	O
i	O
th	O
sample	O
withj	O
observations	O
where	O
j	O
the	O
other	O
parameter	O
ordinary	O
linear	O
discriminants	O
quadratic	B
discriminants	I
and	O
the	O
values	O
w	O
correspond	O
to	O
a	O
minimum	O
euclidean	O
distance	B
rule	O
error	O
to	O
choose	O
the	O
values	O
of	O
and	O
and	O
the	O
default	B
values	O
of	O
were	O
adopted	O
for	O
the	O
majority	O
of	O
statlog	B
datasets	O
default	B
values	O
were	O
used	O
for	O
the	O
head	B
injury	I
dataset	I
and	O
the	O
dna	B
dataset	I
this	O
type	O
of	O
regularisation	B
has	O
been	O
incorporated	O
in	O
the	O
strathclyde	O
version	O
of	O
quadisc	B
very	O
little	O
extra	O
programming	O
effort	O
is	O
required	O
however	O
it	O
is	O
up	O
to	O
the	O
user	O
by	O
trial	O
and	O
friedman	O
gives	O
various	O
shortcut	O
methods	O
for	O
this	O
two-parameter	O
family	O
of	O
procedures	O
is	O
described	O
by	O
friedman	O
as	O
regularised	O
discriminant	O
analysis	O
various	O
simple	O
procedures	O
are	O
included	O
as	O
special	O
cases	O
the	O
exceptions	O
were	O
those	O
cases	O
where	O
a	O
covariance	B
matrix	I
was	O
not	O
invertible	O
non	O
the	O
philosophy	O
being	O
to	O
keep	O
the	O
procedure	O
pure	O
quadratic	O
choice	O
of	O
regularisation	B
parameters	O
reducing	O
the	O
amount	O
of	O
computation	O
k	O
classical	O
statistical	B
methods	O
approx	O
in	O
practice	O
great	O
improvements	O
in	O
the	O
performance	O
of	O
quadratic	B
discriminants	I
may	O
result	O
from	O
the	O
use	O
of	O
regularisation	B
especially	O
in	O
the	O
smaller	O
datasets	O
logistic	B
discriminant	I
exactly	O
as	O
in	O
section	O
logistic	O
regression	O
operates	O
by	O
choosing	O
a	O
hyperplane	O
to	O
separate	O
the	O
classes	B
as	O
well	O
as	O
possible	O
but	O
the	O
criterion	O
for	O
a	O
good	O
separation	O
is	O
changed	O
fisher	O
s	O
linear	O
discriminants	O
optimises	O
a	O
quadratic	O
cost	O
function	O
whereas	O
in	O
logistic	B
discrimination	B
it	O
is	O
a	O
conditional	O
likelihood	O
that	O
is	O
maximised	O
however	O
in	O
practice	O
there	O
is	O
often	O
very	O
little	O
difference	O
between	O
the	O
two	O
and	O
the	O
linear	O
discriminants	O
provide	O
good	O
starting	O
values	O
for	O
the	O
logistic	O
logistic	B
discrimination	B
is	O
identical	O
in	O
theory	O
to	O
linear	B
discrimination	B
for	O
normal	O
distributions	O
with	O
equal	O
covariances	O
and	O
also	O
for	O
independent	O
binary	B
attributes	B
so	O
the	O
greatest	O
differences	O
between	O
the	O
two	O
are	O
to	O
be	O
expected	O
when	O
we	O
are	O
far	O
from	O
these	O
two	O
cases	O
for	O
example	B
when	O
the	O
attributes	B
have	O
very	O
non-normal	O
distributions	O
with	O
very	O
dissimilar	O
covariances	O
the	O
method	O
is	O
only	O
partially	O
parametric	O
as	O
the	O
actual	O
pdfs	O
for	O
the	O
classes	B
are	O
not	O
modelled	O
but	O
rather	O
the	O
ratios	O
between	O
them	O
likelihood	O
the	O
model	O
implies	O
that	O
given	O
attribute	O
values	O
x	O
the	O
conditional	O
class	B
probabilities	O
for	O
classes	B
times	O
the	O
ratios	O
of	O
the	O
probability	O
density	O
functions	O
for	O
the	O
classes	B
are	O
modelled	O
as	O
linear	O
functions	O
of	O
the	O
attributes	B
thus	O
for	O
two	O
classes	B
are	O
the	O
parameters	O
of	O
the	O
model	O
that	O
are	O
to	O
be	O
estimated	O
the	O
case	O
of	O
normal	O
distributions	O
with	O
equal	O
covariance	B
is	O
a	O
special	O
case	O
of	O
this	O
for	O
which	O
the	O
parameters	O
are	O
functions	O
of	O
the	O
prior	B
probabilities	I
the	O
class	B
means	O
and	O
the	O
common	O
covariance	B
matrix	I
however	O
the	O
model	O
covers	O
other	O
cases	O
too	O
such	O
as	O
that	O
where	O
the	O
attributes	B
are	O
independent	O
with	O
values	O
or	O
one	O
of	O
the	O
attractions	O
is	O
that	O
the	O
is	O
specifically	O
the	O
logarithms	O
of	O
the	O
prior	O
odds	B
log	O
c	O
k	O
and	O
vector	O
where	O
discriminant	O
scale	O
covers	O
all	O
real	O
numbers	O
a	O
large	O
positive	O
value	O
indicates	O
that	O
class	B
likely	O
while	O
a	O
large	O
negative	O
value	O
indicates	O
that	O
class	B
in	O
practice	O
the	O
parameters	O
are	O
estimated	O
by	O
maximumf	O
and	O
parameters	O
and	O
bq	O
c	O
sample	O
is	O
defined	O
to	O
be	O
and	O
the	O
parameter	O
estimates	O
are	O
the	O
values	O
that	O
maximise	O
this	O
likelihood	O
they	O
are	O
found	O
by	O
iterative	O
methods	O
as	O
proposed	O
by	O
cox	O
and	O
day	O
kerridge	O
logistic	O
models	O
s	O
sample	O
respectively	O
given	O
independent	O
samples	O
from	O
the	O
two	O
classes	B
the	O
conditional	O
likelihood	O
for	O
the	O
d	O
is	O
likely	O
take	O
the	O
forms	O
b	O
b	O
sec	O
logistic	B
discrimination	B
take	O
the	O
forms	O
is	O
defined	O
to	O
be	O
belong	O
to	O
the	O
class	B
of	O
generalised	O
linear	O
models	O
which	O
generalise	O
the	O
use	O
of	O
linear	B
regression	I
models	O
to	O
deal	O
with	O
non-normal	O
random	O
variables	O
and	O
in	O
particular	O
to	O
deal	O
with	O
binomial	B
variables	O
in	O
this	O
context	O
the	O
binomial	B
variable	O
is	O
an	O
indicator	O
variable	O
that	O
counts	O
again	O
the	O
parameters	O
are	O
estimated	O
by	O
maximum	B
conditional	I
likelihood	I
given	O
at	O
whether	O
an	O
example	B
is	O
class	B
or	O
not	O
when	O
there	O
are	O
more	O
than	O
two	O
classes	B
one	O
class	B
is	O
taken	O
as	O
a	O
reference	B
class	B
and	O
there	O
are	O
sets	O
of	O
parameters	O
for	O
the	O
odds	B
of	O
each	O
class	B
relative	O
to	O
the	O
reference	B
class	B
to	O
discuss	O
this	O
case	O
we	O
abbreviate	O
the	O
notation	O
for	O
w	O
for	O
the	O
remainder	O
of	O
this	O
section	O
therefore	O
x	O
is	O
to	O
the	O
simpler	O
corresponds	O
to	O
the	O
constant	O
vector	O
with	O
leading	O
term	O
unity	O
and	O
the	O
leading	O
term	O
in	O
wheret	O
and	O
the	O
tribute	O
values	O
x	O
the	O
conditional	O
class	B
probability	O
for	O
class	B
conditional	O
class	B
probability	O
for	O
c	O
c	O
respectively	O
given	O
independent	O
samples	O
from	O
the	O
classes	B
the	O
conditional	O
likelihood	O
for	O
the	O
parameters	O
y	O
sample	O
sample	O
values	O
occur	O
it	O
will	O
generally	O
be	O
necessary	O
when	O
categorical	O
attributes	B
with	O
binary	B
attributes	B
before	O
using	O
the	O
algorithm	O
especially	O
if	O
the	O
to	O
convert	O
them	O
into	O
categories	O
are	O
not	O
ordered	O
anderson	O
points	O
out	O
that	O
it	O
may	O
be	O
appropriate	O
to	O
include	O
transformations	O
or	O
products	O
of	O
the	O
attributes	B
in	O
the	O
linear	O
function	O
but	O
for	O
large	O
datasets	O
this	O
may	O
involve	O
much	O
computation	O
see	O
mclachlan	O
for	O
useful	O
hints	O
one	O
way	O
to	O
increase	O
complexity	O
of	O
model	O
without	O
sacrificing	O
intelligibility	O
is	O
to	O
add	O
parameters	O
in	O
a	O
hierarchical	O
fashion	O
and	O
there	O
are	O
then	O
links	O
with	O
graphical	O
models	O
and	O
polytrees	B
in	O
the	O
basic	O
form	O
of	O
the	O
algorithm	O
an	O
example	B
is	O
assigned	O
to	O
the	O
class	B
for	O
which	O
the	O
posterior	O
is	O
greatest	O
if	O
that	O
is	O
greater	O
than	O
or	O
to	O
the	O
reference	B
class	B
if	O
all	O
posteriors	O
are	O
negative	O
more	O
complicated	O
models	O
can	O
be	O
accommodated	O
by	O
adding	O
transformations	O
of	O
the	O
given	O
attributes	B
for	O
example	B
products	O
of	O
pairs	O
of	O
attributes	B
as	O
mentioned	O
in	O
section	O
once	O
again	O
the	O
parameter	O
estimates	O
are	O
the	O
values	O
that	O
maximise	O
this	O
likelihood	O
s	O
sample	O
logistic	B
discriminant	I
programming	O
details	O
most	O
statistics	O
packages	O
can	O
deal	O
with	O
linear	B
discriminant	I
analysis	O
for	O
two	O
classes	B
systat	O
has	O
in	O
addition	O
a	O
version	O
of	O
logistic	O
regression	O
capable	O
of	O
handling	O
problems	O
with	O
more	O
than	O
two	O
classes	B
if	O
a	O
package	O
has	O
only	O
binary	O
logistic	O
regression	O
can	O
only	O
deal	O
with	O
two	O
classes	B
begg	O
gray	O
suggest	O
an	O
approximate	O
procedure	O
whereby	O
classes	B
are	O
all	O
compared	O
to	O
a	O
reference	B
class	B
by	O
means	O
of	O
logistic	O
regressions	O
and	O
the	O
results	O
then	O
combined	O
the	O
approximation	O
is	O
fairly	O
good	O
in	O
practice	O
according	O
to	O
begg	O
gray	O
k	O
k	O
n	O
a	O
a	O
n	O
a	O
a	O
classical	O
statistical	B
methods	O
many	O
statistical	B
packages	O
splus	B
genstat	O
now	O
include	O
a	O
generalised	O
linear	O
model	O
function	O
enabling	O
logistic	O
regression	O
to	O
be	O
programmed	O
easily	O
in	O
two	O
or	O
three	O
lines	O
of	O
code	O
occurrences	O
the	O
indicator	O
variable	O
is	O
then	O
declared	O
to	O
be	O
a	O
binomial	B
variable	O
with	O
the	O
logit	O
link	B
function	I
and	O
generalised	O
regression	O
performed	O
on	O
the	O
attributes	B
we	O
used	O
the	O
package	O
splus	B
for	O
this	O
purpose	O
this	O
is	O
fine	O
for	O
two	O
classes	B
and	O
has	O
the	O
merit	O
of	O
requiring	O
little	O
extra	O
programming	O
effort	O
for	O
more	O
than	O
two	O
classes	B
the	O
complexity	O
of	O
the	O
problem	O
increases	O
substantially	O
and	O
although	O
it	O
is	O
technically	O
still	O
possible	O
to	O
use	O
glm	O
procedures	O
the	O
programming	O
effort	O
is	O
substantially	O
greater	O
and	O
much	O
less	O
efficient	O
the	O
procedure	O
is	O
to	O
define	O
an	O
indicator	O
variable	O
for	O
class	B
numbers	O
in	O
the	O
various	O
classes	B
i	O
e	O
the	O
maximum	B
likelihood	I
solution	O
can	O
be	O
found	O
via	O
a	O
newton-raphson	O
iterative	O
procedure	O
as	O
it	O
is	O
quite	O
easy	O
to	O
write	O
down	O
the	O
necessary	O
derivatives	O
of	O
the	O
likelihood	O
first	O
iteration	O
of	O
course	O
an	O
alternative	O
would	O
be	O
to	O
use	O
the	O
linear	B
discriminant	I
parameters	O
as	O
starting	O
values	O
in	O
subsequent	O
iterations	O
the	O
step	O
size	O
may	O
occasionally	O
have	O
to	O
be	O
reduced	O
but	O
usually	O
the	O
procedure	O
converges	O
in	O
about	O
iterations	O
this	O
is	O
the	O
procedure	O
we	O
adopted	O
where	O
possible	O
coeffiequivalently	O
the	O
log-likelihood	O
the	O
simplest	O
starting	O
procedure	O
is	O
to	O
set	O
the	O
which	O
are	O
set	O
to	O
the	O
logarithms	O
of	O
the	O
cients	O
to	O
zero	O
except	O
for	O
the	O
leading	O
coefficients	O
wherej	O
is	O
the	O
number	O
of	O
class	B
logj	O
are	O
those	O
of	O
the	O
linear	B
discriminant	I
after	O
the	O
examples	O
this	O
ensures	O
that	O
the	O
values	O
of	O
rows	O
and	O
each	O
term	O
requires	O
a	O
summation	O
over	O
all	O
the	O
observations	O
in	O
the	O
thus	O
there	O
are	O
of	O
order	O
m	O
in	O
the	O
kl	B
digits	B
dataset	I
section	O
for	O
example	B
so	O
the	O
number	O
of	O
operations	O
is	O
of	O
order	O
and	O
om	O
in	O
each	O
iteration	O
in	O
such	O
cases	O
it	O
is	O
preferable	O
to	O
use	O
a	O
purely	O
numerical	O
search	O
procedure	O
or	O
as	O
we	O
did	O
when	O
the	O
newton-raphson	O
procedure	O
was	O
too	O
time-consuming	O
to	O
use	O
a	O
method	O
based	O
on	O
an	O
approximate	O
hessian	O
the	O
approximation	O
uses	O
the	O
fact	O
that	O
the	O
hessian	O
for	O
the	O
zero	O
th	O
order	O
iteration	O
is	O
simply	O
a	O
replicate	O
of	O
the	O
design	O
matrix	O
covariance	B
matrix	I
used	O
by	O
the	O
linear	B
discriminant	I
rule	O
this	O
zero-order	O
hessian	O
is	O
used	O
for	O
all	O
iterations	O
in	O
situations	O
where	O
there	O
is	O
little	O
difference	O
between	O
the	O
linear	O
and	O
logistic	O
parameters	O
the	O
approximation	O
is	O
very	O
good	O
and	O
convergence	O
is	O
fairly	O
fast	O
a	O
few	O
more	O
iterations	O
are	O
generally	O
required	O
however	O
in	O
the	O
more	O
interesting	O
case	O
that	O
the	O
linear	O
and	O
logistic	O
parameters	O
are	O
very	O
different	O
convergence	O
using	O
this	O
procedure	O
is	O
very	O
slow	O
and	O
it	O
may	O
still	O
be	O
quite	O
far	O
from	O
convergence	O
after	O
say	O
iterations	O
we	O
generally	O
stopped	O
after	O
iterations	O
although	O
the	O
parameter	O
values	O
were	O
generally	O
not	O
stable	O
the	O
predicted	O
classes	B
for	O
the	O
data	O
were	O
reasonably	O
stable	O
so	O
the	O
predictive	O
power	O
of	O
the	O
resulting	O
rule	O
may	O
not	O
be	O
seriously	O
affected	O
this	O
aspect	O
of	O
logistic	O
regression	O
has	O
not	O
been	O
explored	O
however	O
each	O
iteration	O
requires	O
a	O
separate	O
calculation	O
of	O
the	O
hessian	O
and	O
it	O
is	O
here	O
that	O
the	O
bulk	O
of	O
the	O
computational	O
work	O
is	O
required	O
the	O
hessian	O
is	O
a	O
square	O
matrix	O
with	O
whole	O
dataset	O
some	O
saving	O
can	O
by	O
achieved	O
using	O
the	O
symmetries	O
of	O
the	O
hessian	O
computations	O
required	O
to	O
find	O
the	O
hessian	O
matrix	O
at	O
each	O
iteration	O
the	O
final	O
program	O
used	O
for	O
the	O
trials	O
reported	O
in	O
this	O
book	O
was	O
coded	O
in	O
fortran	O
since	O
the	O
splus	B
procedure	O
had	O
prohibitive	O
memory	B
requirements	O
availablility	O
of	O
the	O
fortran	O
code	O
can	O
be	O
found	O
in	O
appendix	O
b	O
linear	B
discrimination	B
has	O
the	O
equation	O
sec	O
bayes	O
rules	O
bayes	O
rules	O
methods	O
based	O
on	O
likelihood	O
ratios	O
can	O
be	O
adapted	O
to	O
cover	B
the	O
case	O
of	O
unequal	O
misclassification	B
costs	B
andor	O
unequal	O
prior	B
probabilities	I
let	O
the	O
prior	B
probabilities	I
be	O
when	O
there	O
are	O
more	O
than	O
two	O
classes	B
the	O
simplest	O
procedure	O
is	O
to	O
calculate	O
the	O
as	O
in	O
section	O
the	O
minimum	O
expected	O
cost	O
solution	O
is	O
to	O
assign	O
the	O
data	O
x	O
to	O
class	B
and	O
denote	O
the	O
cost	O
incurred	O
by	O
classifying	O
an	O
example	B
h	O
a	O
into	O
class	B
of	O
class	B
i	O
chosen	O
to	O
minimise	O
in	O
the	O
case	O
of	O
two	O
classes	B
the	O
hyperplane	O
in	O
k	O
f	O
and	O
associated	O
expected	O
costs	B
explicitly	O
using	O
the	O
formulae	O
of	O
class	B
example	B
as	O
illustration	O
of	O
the	O
differences	O
between	O
the	O
linear	O
quadratic	O
and	O
logistic	O
discriminants	O
we	O
consider	O
a	O
subset	O
of	O
the	O
karhunen-loeve	O
version	O
of	O
the	O
digits	O
data	O
later	O
studied	O
in	O
this	O
book	O
for	O
simplicity	O
we	O
consider	O
only	O
the	O
digits	O
and	O
and	O
to	O
differentiate	O
between	O
them	O
we	O
use	O
only	O
the	O
first	O
two	O
attributes	B
are	O
available	O
so	O
this	O
is	O
a	O
substantial	O
reduction	O
in	O
potential	O
information	O
the	O
full	O
sample	O
of	O
points	O
for	O
each	O
digit	O
was	O
used	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
discriminants	O
although	O
only	O
a	O
subset	O
of	O
points	O
for	O
each	O
digit	O
is	O
plotted	O
in	O
figure	O
as	O
much	O
of	O
the	O
detail	O
is	O
obscured	O
when	O
the	O
full	O
set	O
is	O
plotted	O
log	O
the	O
right	O
hand	O
side	O
replacing	O
that	O
we	O
had	O
in	O
equation	O
section	O
linear	B
discriminant	I
also	O
shown	O
in	O
figure	O
are	O
the	O
sample	O
centres	O
of	O
gravity	O
by	O
a	O
cross	O
because	O
there	O
are	O
equal	O
numbers	O
in	O
the	O
samples	O
the	O
linear	B
discriminant	I
boundary	O
on	O
the	O
diagram	O
by	O
a	O
full	O
line	O
intersects	O
the	O
line	O
joining	O
the	O
centres	O
of	O
gravity	O
at	O
its	O
mid-point	O
any	O
new	O
point	O
is	O
classified	O
as	O
a	O
if	O
it	O
lies	O
below	O
the	O
line	O
i	O
e	O
is	O
on	O
the	O
same	O
side	O
as	O
the	O
centre	O
of	O
the	O
s	O
in	O
the	O
diagram	O
there	O
are	O
s	O
below	O
the	O
line	O
so	O
they	O
would	O
be	O
misclassified	O
logistic	B
discriminant	I
the	O
logistic	B
discriminant	I
procedure	O
usually	O
starts	O
with	O
the	O
linear	B
discriminant	I
line	O
and	O
then	O
adjusts	O
the	O
slope	O
and	O
intersect	O
to	O
maximise	O
the	O
conditional	O
likelihood	O
arriving	O
at	O
the	O
dashed	O
line	O
of	O
the	O
diagram	O
essentially	O
the	O
line	O
is	O
shifted	O
towards	O
the	O
centre	O
of	O
the	O
s	O
so	O
as	O
to	O
reduce	O
the	O
number	O
of	O
misclassified	O
s	O
this	O
gives	O
fewer	O
misclassified	O
s	O
more	O
misclassified	O
s	O
in	O
the	O
diagram	O
quadratic	B
discriminant	I
the	O
quadratic	B
discriminant	I
starts	O
by	O
constructing	O
for	O
each	O
sample	O
an	O
ellipse	O
centred	O
on	O
the	O
centre	O
of	O
gravity	O
of	O
the	O
points	O
in	O
figure	O
it	O
is	O
clear	O
that	O
the	O
distributions	O
are	O
of	O
different	O
shape	O
and	O
spread	O
with	O
the	O
distribution	O
of	O
s	O
being	O
roughly	O
circular	O
in	O
shape	O
and	O
the	O
s	O
being	O
more	O
elliptical	O
the	O
line	O
of	O
equal	O
likelihood	O
is	O
now	O
itself	O
an	O
ellipse	O
general	O
a	O
conic	O
section	O
as	O
shown	O
in	O
the	O
figure	O
all	O
points	O
within	O
the	O
ellipse	O
are	O
classified	O
classical	O
statistical	B
methods	O
as	O
s	O
relative	O
to	O
the	O
logistic	O
boundary	O
i	O
e	O
in	O
the	O
region	O
between	O
the	O
dashed	O
line	O
and	O
the	O
ellipse	O
the	O
quadratic	O
rule	O
misclassifies	O
an	O
extra	O
s	O
the	O
upper	O
half	O
of	O
the	O
diagram	O
but	O
correctly	O
classifies	O
an	O
extra	O
s	O
the	O
lower	O
half	O
of	O
the	O
diagram	O
so	O
the	O
performance	O
of	O
the	O
quadratic	O
classifier	B
is	O
about	O
the	O
same	O
as	O
the	O
logistic	B
discriminant	I
in	O
this	O
case	O
probably	O
due	O
to	O
the	O
skewness	B
of	O
the	O
distribution	O
linear	O
logistic	O
and	O
quadratic	B
discriminants	I
linear	O
logistic	O
e	O
t	O
a	O
i	O
r	O
a	O
v	O
l	O
k	O
d	O
n	O
quadratic	O
kl-variate	O
fig	O
decision	O
boundaries	O
for	O
the	O
three	O
discriminants	O
quadratic	O
linear	O
line	O
and	O
logistic	O
line	O
the	O
data	O
are	O
the	O
first	O
two	O
karhunen-loeve	O
components	O
for	O
the	O
digits	O
and	O
modern	O
statistical	B
techniques	O
r	O
molina	O
n	O
p	O
erez	O
de	O
la	O
blanca	O
and	O
c	O
c	O
taylor	O
university	O
of	O
granada	O
and	O
university	O
of	O
leeds	O
introduction	O
in	O
the	O
previous	O
chapter	O
we	O
studied	O
the	O
classification	B
problem	O
from	O
the	O
statistical	B
point	O
of	O
view	O
assuming	O
that	O
the	O
form	O
of	O
the	O
underlying	O
density	O
functions	O
their	O
ratio	O
was	O
known	O
however	O
in	O
most	O
real	O
problems	O
this	O
assumption	O
does	O
not	O
necessarily	O
hold	O
in	O
this	O
chapter	O
we	O
examine	O
distribution-free	O
called	O
nonparametric	O
classification	B
procedures	O
that	O
can	O
be	O
used	O
without	O
assuming	O
that	O
the	O
form	O
of	O
the	O
underlying	O
densities	O
are	O
known	O
the	O
bayesian	O
approach	O
for	O
allocating	O
observations	O
to	O
classes	B
has	O
already	O
been	O
outlined	O
in	O
section	O
it	O
is	O
clear	O
that	O
to	O
apply	O
the	O
bayesian	O
approach	O
to	O
classification	B
we	O
have	O
recall	O
denote	O
the	O
number	O
of	O
classes	B
of	O
examples	O
and	O
attributes	B
respec	O
tively	O
classes	B
will	O
be	O
denoted	O
by	O
and	O
attribute	O
values	O
for	O
example	B
n	O
n	O
elements	O
in	O
will	O
be	O
a	O
nonparametric	B
methods	I
to	O
do	O
this	O
job	O
will	O
be	O
to	O
will	O
be	O
denoted	O
by	O
vectorky	O
and	O
discussed	O
in	O
this	O
chapter	O
we	O
begin	O
in	O
section	O
with	O
kernel	B
density	B
estimation	I
a	O
close	O
relative	O
to	O
this	O
approach	O
is	O
the	O
k-nearest	B
neighbour	I
which	O
is	O
outlined	O
in	O
section	O
bayesian	B
methods	I
which	O
either	O
allow	O
for	O
or	O
prohibit	O
dependence	O
between	O
the	O
variables	O
are	O
discussed	O
in	O
sections	O
and	O
a	O
final	O
section	O
deals	O
with	O
promising	O
methods	O
which	O
have	O
been	O
developed	O
recently	O
but	O
for	O
various	O
reasons	O
must	O
be	O
regarded	O
as	O
methods	O
for	O
the	O
future	O
to	O
a	O
greater	O
or	O
lesser	O
extent	O
these	O
methods	O
have	O
been	O
tried	O
out	O
in	O
the	O
project	O
but	O
the	O
results	O
were	O
disappointing	O
in	O
some	O
cases	O
this	O
is	O
due	O
to	O
limitations	O
of	O
size	O
and	O
memory	B
as	O
implemented	O
in	O
splus	B
the	O
pruned	O
implementation	O
of	O
mars	B
in	O
splus	B
also	O
suffered	O
in	O
a	O
similar	O
way	O
but	O
a	O
standalone	O
version	O
which	O
also	O
does	O
classification	B
is	O
expected	O
shortly	O
we	O
believe	O
that	O
these	O
methods	O
will	O
have	O
a	O
place	O
in	O
classification	B
practice	O
once	O
some	O
relatively	O
minor	O
technical	B
problems	O
have	O
been	O
resolved	O
as	O
yet	O
however	O
we	O
cannot	O
recommend	O
them	O
on	O
the	O
basis	O
of	O
our	O
empirical	O
trials	O
address	O
for	O
correspondence	O
department	O
of	O
computer	O
science	O
and	O
ai	O
facultad	O
de	O
ciencas	O
university	O
of	O
granada	O
granada	O
spain	O
a	O
a	O
modern	O
statistical	B
techniques	O
is	O
given	O
by	O
furthermore	O
then	O
is	O
the	O
length	O
of	O
the	O
edge	O
of	O
the	O
hypercube	O
we	O
have	O
this	O
leads	O
to	O
the	O
following	O
procedure	O
to	O
estimate	O
density	B
estimation	I
a	O
nonparametric	O
approach	O
proposed	O
in	O
fix	O
hodges	O
is	O
to	O
estimate	O
the	O
densities	O
by	O
nonparametric	O
density	B
estimation	I
then	O
once	O
we	O
have	O
estimated	O
and	O
the	O
prior	B
probabilities	I
pa	O
we	O
can	O
use	O
the	O
formulae	O
of	O
section	O
and	O
the	O
costs	B
to	O
classifyk	O
by	O
minimum	O
risk	O
or	O
minimum	O
error	O
dimensional	O
density	O
to	O
introduce	O
the	O
method	O
we	O
assume	O
that	O
we	O
have	O
to	O
estimate	O
of	O
an	O
unknown	O
distribution	O
note	O
that	O
we	O
will	O
have	O
to	O
perform	O
this	O
process	O
for	O
each	O
of	O
the	O
densitiesb	O
that	O
a	O
vectork	O
then	O
the	O
probability	O
will	O
fall	O
in	O
a	O
region	O
m	O
k	O
suppose	O
thatj	O
observations	O
are	O
drawn	O
independently	O
according	O
then	O
we	O
can	O
is	O
the	O
number	O
of	O
thesej	O
observations	O
falling	O
in	O
approach	O
does	O
not	O
vary	O
appreciably	O
within	O
we	O
can	O
write	O
where	O
is	O
the	O
volume	O
enclosed	O
by	O
be	O
the	O
number	O
of	O
samples	O
falling	O
in	O
w	O
the	O
density	O
atk	O
let	O
be	O
the	O
volume	O
of	O
w	O
the	O
estimate	O
based	O
on	O
a	O
sample	O
of	O
sizej	O
and	O
b	O
d	O
j	O
b	O
m	O
dimensional	O
is	O
equation	O
can	O
be	O
written	O
in	O
a	O
much	O
more	O
suggestive	O
way	O
if	O
w	O
hypercube	O
and	O
if	O
m	O
ky	O
b	O
m	O
m	O
then	O
expresses	O
our	O
estimate	O
as	O
an	O
average	O
function	O
ofk	O
and	O
the	O
samplesky	O
b	O
where	O
are	O
kernel	O
functions	O
for	O
instance	O
we	O
could	O
use	O
instead	O
of	O
the	O
parzen	O
n	O
exp	O
is	O
clear	O
for	O
if	O
the	O
role	O
played	O
by	O
m	O
m	O
changes	O
very	O
slowly	O
withk	O
resulting	O
in	O
a	O
very	O
smooth	O
estimate	O
on	O
the	O
other	O
hand	O
if	O
b	O
is	O
the	O
superposition	O
ofj	O
small	O
then	O
centered	O
at	O
the	O
samples	O
producing	O
a	O
very	O
erratic	O
estimate	O
the	O
analysis	O
for	O
the	O
a	O
is	O
very	O
large	O
is	O
sharp	O
normal	O
distributions	O
with	O
small	O
variances	O
g	O
where	O
otherwise	O
in	O
general	O
we	O
could	O
use	O
window	O
defined	O
above	O
parzen	B
window	I
is	O
similar	O
a	O
j	O
n	O
k	O
a	O
j	O
n	O
n	O
a	O
a	O
sec	O
density	B
estimation	I
further	O
details	O
independent	O
coordinates	O
i	O
e	O
is	O
an	O
averaged	O
value	O
of	O
the	O
unknown	O
density	O
for	O
the	O
mean	O
and	O
variance	O
of	O
the	O
estimator	O
these	O
can	O
be	O
used	O
to	O
derive	O
plug-in	B
estimates	I
before	O
going	O
into	O
details	O
about	O
the	O
kernel	O
functions	O
we	O
use	O
in	O
the	O
classification	B
problem	O
we	O
briefly	O
comment	O
on	O
the	O
mean	O
we	O
now	O
consider	O
our	O
classification	B
problem	O
two	O
choices	O
have	O
to	O
be	O
made	O
in	O
order	O
to	O
estimate	O
the	O
density	O
the	O
specification	O
of	O
the	O
kernel	O
and	O
the	O
value	O
of	O
the	O
smoothing	B
parameter	I
it	O
is	O
fairly	O
widely	O
recognised	O
that	O
the	O
choice	O
of	O
the	O
smoothing	B
parameter	I
is	O
much	O
more	O
important	O
with	O
regard	O
to	O
the	O
kernel	B
function	I
we	O
will	O
restrict	O
our	O
attention	O
to	O
and	O
about	O
the	O
estimation	O
of	O
the	O
smoothing	B
parameter	I
we	O
have	O
behaviour	O
of	O
b	O
m	O
b	O
and	O
so	O
the	O
expected	O
value	O
of	O
the	O
estimate	O
in	O
a	O
taylor	O
series	O
m	O
about	O
x	O
one	O
can	O
derive	O
asymptotic	O
formulae	O
b	O
by	O
expanding	O
for	O
which	O
are	O
well-suited	O
to	O
the	O
goal	O
of	O
density	B
estimation	I
see	O
silverman	O
for	O
kernels	B
withw	O
indicating	O
the	O
kernel	B
function	I
component	O
of	O
theg	O
th	O
attribute	O
and	O
being	O
not	O
a	O
dependent	O
ong	O
it	O
is	O
very	O
important	O
to	O
note	O
that	O
as	O
stressed	O
by	O
aitchison	O
aitken	O
it	O
is	O
clear	O
that	O
kernels	B
could	O
have	O
a	O
more	O
complex	B
form	O
and	O
that	O
the	O
smoothing	B
parameter	I
could	O
be	O
coordinate	O
dependent	O
we	O
will	O
not	O
discuss	O
in	O
detail	O
that	O
possibility	O
here	O
mclachlan	O
for	O
details	O
some	O
comments	O
will	O
be	O
made	O
at	O
the	O
end	O
of	O
this	O
section	O
this	O
factorisation	O
does	O
not	O
imply	O
the	O
independence	O
of	O
the	O
attributes	B
for	O
the	O
density	O
we	O
are	O
estimating	O
the	O
kernels	B
we	O
use	O
depend	O
on	O
the	O
type	O
of	O
variable	O
for	O
continuous	O
variables	O
a-	O
a	O
ad	O
for	O
binary	O
variables	O
a-	O
-	O
a-	O
a-	O
log	O
exp	O
a	O
log	O
a	O
g	O
q	O
for	O
nominal	O
variables	O
with	O
ma	O
nominal	O
values	O
a	O
h	O
g	O
a-	O
log	O
g	O
q	O
n	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
modern	O
statistical	B
techniques	O
continuous	O
binary	O
nominal	O
ordinal	O
a	O
a-	O
a	O
defined	O
depending	O
on	O
the	O
type	O
of	O
variable	O
by	O
for	O
the	O
above	O
expressions	O
we	O
can	O
see	O
that	O
in	O
all	O
cases	O
we	O
can	O
write	O
for	O
all	O
the	O
is	O
whereu	O
if	O
where	O
for	O
ordinal	O
variables	O
with	O
ad	O
a-	O
-	O
a	O
a-	O
a	O
the	O
problem	O
is	O
that	O
since	O
we	O
want	O
to	O
use	O
the	O
same	O
smoothing	B
parameter	I
variables	O
we	O
have	O
to	O
normalise	O
them	O
to	O
do	O
so	O
we	O
substitute	O
by	O
otherwise	O
a	O
nominal	O
values	O
k	O
q	O
a	O
where	O
denotes	O
the	O
number	O
of	O
examples	O
for	O
which	O
attributeg	O
has	O
the	O
ando	O
is	O
the	O
sample	O
mean	O
of	O
theg	O
th	O
attribute	O
with	O
this	O
selection	O
ofu	O
we	O
have	O
a-	O
u	O
for	O
discrete	O
variables	O
the	O
range	O
of	O
the	O
smoothness	O
parameter	O
is	O
the	O
one	O
a	O
pa	O
a-	O
a-	O
a	O
have	O
to	O
be	O
for	O
continuous	O
variables	O
the	O
range	O
is	O
and	O
and	O
regarded	O
as	O
limiting	O
cases	O
as	O
we	O
get	O
the	O
uniform	B
distribution	I
over	O
the	O
real	O
line	O
we	O
get	O
the	O
dirac	O
spike	O
function	O
situated	O
at	O
the	O
a	O
and	O
as	O
as	O
having	O
defined	O
the	O
kernels	B
we	O
will	O
use	O
we	O
need	O
to	O
choose	O
density	O
approaches	O
zero	O
at	O
allk	O
except	O
at	O
the	O
samples	O
where	O
it	O
isd	O
j	O
function	O
this	O
precludes	O
choosing	O
by	O
maximizing	O
the	O
log	B
likelihood	I
with	O
respect	O
to	O
where	O
to	O
maximise	O
and	O
takes	O
the	O
estimated	O
times	O
the	O
dirac	O
delta	O
to	O
estimate	O
a	O
good	O
choice	O
of	O
smoothing	B
parameter	I
a	O
jackknife	B
modification	O
of	O
the	O
maximum	B
likelihood	I
method	O
can	O
be	O
used	O
this	O
was	O
proposed	O
by	O
habbema	O
et	O
al	O
and	O
duin	O
so	O
we	O
can	O
understand	O
the	O
above	O
process	O
as	O
rescaling	O
all	O
the	O
variables	O
to	O
the	O
same	O
scale	O
extreme	O
leads	O
to	O
the	O
uniform	B
distribution	I
and	O
the	O
other	O
to	O
a	O
one-point	O
distribution	O
if	O
if	O
a	O
i	O
o	O
a	O
o	O
a	O
j	O
a	O
j	O
n	O
o	O
a	O
j	O
a	O
g	O
a	O
a	O
a	O
t	O
b	O
j	O
n	O
n	O
sec	O
density	B
estimation	I
this	O
criterion	O
makes	O
the	O
smoothness	O
data	O
dependent	O
leads	O
to	O
an	O
algorithm	O
for	O
an	O
arbitrary	O
dimensionality	O
of	O
the	O
data	O
and	O
possesses	O
consistency	O
requirements	O
as	O
discussed	O
by	O
aitchison	O
aitken	O
tends	O
to	O
be	O
constant	O
over	O
large	O
regions	O
roughly	O
approximating	O
the	O
fixed	O
kernel	O
model	O
the	O
variation	O
in	O
smoothness	O
of	O
the	O
estimated	O
density	O
over	O
the	O
different	O
regions	O
if	O
for	O
the	O
euclidean	O
distance	B
measured	O
after	O
standardisation	O
of	O
all	O
variables	O
the	O
proportionality	O
the	O
smoothing	O
value	O
is	O
now	O
determined	O
by	O
two	O
the	O
so-called	O
variable	O
kernel	O
model	O
an	O
extensive	O
description	O
of	O
this	O
model	O
was	O
first	O
given	O
by	O
breiman	O
et	O
al	O
this	O
method	O
has	O
promising	O
results	O
especially	O
when	O
lognormal	O
is	O
thus	O
proportional	O
to	O
the	O
dependent	O
on	O
th	O
nearest	O
an	O
extension	O
of	O
the	O
above	O
model	O
for	O
is	O
to	O
make	O
so	O
that	O
we	O
have	O
a	O
for	O
each	O
sample	O
point	O
this	O
gives	O
rise	O
to	O
neighbour	O
distance	B
tok	O
or	O
skewed	O
distributions	O
are	O
estimated	O
the	O
kernel	O
width	O
denoted	O
byj	O
cj	O
i	O
e	O
th	O
nearest	B
neighbour	I
distance	B
ink	O
we	O
take	O
forj	O
is	O
dependent	O
factor	O
parameters	O
can	O
be	O
though	O
of	O
as	O
an	O
overall	O
smoothing	B
parameter	I
defines	O
the	O
smoothness	O
will	O
vary	O
locally	O
while	O
for	O
values	O
the	O
smoothness	O
m	O
exp	O
a	O
cj	O
cj	O
to	O
optimise	O
for	O
dimensional	O
optimisation	B
problem	O
of	O
the	O
likelihood	O
function	O
with	O
one	O
continuous	O
parameter	O
and	O
one	O
discrete	O
parameter	O
silverman	O
sections	O
and	O
studies	O
the	O
advantages	O
and	O
disadvantages	O
of	O
this	O
approach	O
he	O
also	O
proposes	O
another	O
method	O
to	O
estimate	O
the	O
smoothing	B
parameters	I
in	O
a	O
variable	O
kernel	O
model	O
silverman	O
and	O
mclachlan	O
for	O
details	O
the	O
jackknife	B
modification	O
of	O
the	O
maximum	B
likelihood	I
method	O
can	O
again	O
be	O
applied	O
however	O
for	O
the	O
variable	O
kernel	O
this	O
leads	O
to	O
a	O
more	O
difficult	O
two	O
the	O
algorithm	O
we	O
mainly	O
used	O
in	O
our	O
trials	O
to	O
classify	O
by	O
density	B
estimation	I
is	O
by	O
hermans	O
at	O
al	O
appendix	O
b	O
for	O
source	O
we	O
use	O
a	O
normal	B
distribution	I
for	O
the	O
component	O
a-	O
a-	O
example	B
we	O
illustrate	O
the	O
kernel	O
classifier	B
with	O
some	O
simulated	O
data	O
which	O
comprise	O
observations	O
from	O
a	O
standard	O
normal	B
distribution	I
say	O
and	O
total	O
values	O
from	O
an	O
equal	O
mixture	O
of	O
the	O
resulting	O
estimates	O
can	O
then	O
be	O
used	O
as	O
a	O
basis	O
for	O
classifying	O
future	O
observations	O
to	O
one	O
or	O
other	O
class	B
various	O
scenarios	O
are	O
given	O
in	O
figure	O
where	O
a	O
black	O
segment	O
indicates	O
that	O
observations	O
will	O
be	O
allocated	O
to	O
class	B
and	O
otherwise	O
to	O
class	B
in	O
this	O
example	B
we	O
have	O
used	O
equal	O
priors	O
for	O
the	O
classes	B
they	O
are	O
not	O
equally	O
represented	O
and	O
hence	O
allocations	O
are	O
based	O
on	O
maximum	O
estimated	O
likelihood	O
it	O
is	O
clear	O
that	O
the	O
rule	O
will	O
depend	O
on	O
the	O
smoothing	B
parameters	I
and	O
can	O
result	O
in	O
very	O
disconnected	O
sets	O
in	O
higher	O
dimensions	O
these	O
segments	O
will	O
become	O
regions	O
with	O
potentially	O
very	O
nonlinear	O
boundaries	O
and	O
possibly	O
disconnected	O
depending	O
on	O
the	O
smoothing	B
parameters	I
used	O
for	O
comparison	O
we	O
also	O
draw	O
the	O
population	O
probability	O
densities	O
and	O
the	O
true	O
decision	O
regions	O
in	O
figure	O
which	O
are	O
still	O
disconnected	O
but	O
very	O
much	O
smoother	O
than	O
some	O
of	O
those	O
constructed	O
from	O
the	O
kernels	B
a	O
a	O
a	O
a	O
modern	O
statistical	B
techniques	O
true	O
probability	O
densities	O
with	O
decision	O
regions	O
f	O
f	O
f	O
x	O
x	O
kernel	O
estimates	O
with	O
decision	O
regions	O
smoothing	O
values	O
smoothing	O
values	O
smoothing	O
values	O
smoothing	O
values	O
x	O
f	O
f	O
x	O
x	O
fig	O
classification	B
regions	O
for	O
kernel	O
classifier	B
with	O
true	O
probability	O
densities	O
the	O
smoothing	B
parameters	I
quoted	O
in	O
are	O
the	O
values	O
of	O
used	O
in	O
equation	O
for	O
class	B
and	O
class	B
respectively	O
sec	O
k	B
nearest	B
neighbour	I
neighbour	O
this	O
leads	O
immediately	O
there	O
is	O
a	O
problem	O
that	O
is	O
important	O
to	O
mention	O
in	O
the	O
above	O
analysis	O
it	O
is	O
assumed	O
that	O
however	O
it	O
could	O
be	O
the	O
case	O
that	O
our	O
sample	O
did	O
not	O
estimate	O
by	O
a	O
nearest	O
neigh	O
suppose	O
we	O
consider	O
estimating	O
the	O
bour	O
method	O
if	O
we	O
have	O
training	O
data	O
in	O
which	O
there	O
arejs	O
observations	O
from	O
class	B
with	O
and	O
the	O
hypersphere	O
aroundk	O
containing	O
nearest	O
observations	O
has	O
volume	O
jc	O
j	O
respectively	O
then	O
observations	O
of	O
classes	B
and	O
is	O
estimated	O
which	O
then	O
gives	O
dhj	O
is	O
estimated	O
byj	O
by	O
substitution	O
as	O
an	O
estimate	O
max	O
to	O
the	O
classification	B
rule	I
classifyk	O
as	O
belonging	O
to	O
class	B
this	O
is	O
known	O
as	O
neighbour	O
classification	B
rule	I
for	O
the	O
special	O
case	O
when	O
it	O
is	O
simply	O
termed	O
the	O
nearest-neighbour	O
classification	B
rule	I
d	O
j	O
is	O
estimated	O
byj	O
the	O
nearest-neighbour	O
rule	O
should	O
work	O
to	O
begin	O
with	O
note	O
that	O
the	O
class	B
e	O
with	O
the	O
nearest	B
neighbour	I
is	O
a	O
random	O
variable	O
and	O
the	O
probability	O
that	O
e	O
k	O
e	O
wherek	O
b	O
is	O
is	O
the	O
sample	O
nearest	O
tok	O
samples	O
is	O
very	O
large	O
it	O
is	O
reasonable	O
to	O
assume	O
thatk	O
b	O
is	O
sufficiently	O
close	O
tok	O
k	O
b	O
in	O
this	O
case	O
we	O
can	O
view	O
the	O
nearest-neighbour	O
rule	O
as	O
a	O
with	O
probability	O
randomised	O
decision	O
rule	O
that	O
classifiesk	O
by	O
selecting	O
the	O
category	O
as	O
a	O
nonparametric	B
density	I
estimator	I
the	O
nearest	B
neighbour	I
approach	O
yields	O
a	O
non-smooth	O
curve	O
which	O
does	O
not	O
integrate	O
to	O
unity	O
and	O
as	O
a	O
method	O
of	O
density	B
estimation	I
it	O
is	O
unlikely	O
to	O
be	O
appropriate	O
however	O
these	O
poor	O
qualities	O
need	O
not	O
extend	O
to	O
the	O
domain	O
of	O
classification	B
note	O
also	O
that	O
the	O
nearest	B
neighbour	I
method	O
is	O
equivalent	O
to	O
the	O
kernel	O
density	O
estimate	O
as	O
the	O
smoothing	B
parameter	I
tends	O
to	O
zero	O
when	O
the	O
normal	O
kernel	B
function	I
is	O
used	O
see	O
scott	O
for	O
details	O
when	O
the	O
number	O
of	O
so	O
properly	O
the	O
group-prior	O
probabilities	O
this	O
issue	O
is	O
studied	O
in	O
davies	O
we	O
study	O
in	O
some	O
depth	O
the	O
nn	O
rule	O
we	O
first	O
try	O
to	O
get	O
a	O
heuristic	O
understanding	O
of	O
why	O
associated	O
it	O
is	O
obvious	O
that	O
the	O
use	O
of	O
this	O
rule	O
involves	O
choice	O
of	O
a	O
suitable	O
metric	O
i	O
e	O
how	O
is	O
the	O
distance	B
to	O
the	O
nearest	O
points	O
to	O
be	O
measured	O
in	O
some	O
datasets	O
there	O
is	O
no	O
problem	O
but	O
for	O
multivariate	O
data	O
where	O
the	O
measurements	O
are	O
measured	O
on	O
different	O
scales	O
some	O
standardisation	O
is	O
usually	O
required	O
this	O
is	O
usually	O
taken	O
to	O
be	O
either	O
the	O
standard	O
deviation	O
or	O
the	O
range	O
of	O
the	O
variable	O
if	O
there	O
are	O
indicator	B
variables	I
will	O
occur	O
for	O
nominal	O
data	O
then	O
the	O
data	O
is	O
usually	O
transformed	O
so	O
that	O
all	O
observations	O
lie	O
in	O
the	O
unit	O
hypercube	O
note	O
that	O
the	O
metric	O
can	O
also	O
be	O
class	B
dependent	O
so	O
that	O
one	O
obtains	O
a	O
distance	B
conditional	O
on	O
the	O
class	B
this	O
will	O
increase	O
the	O
processing	O
and	O
classification	B
time	B
but	O
may	O
lead	O
to	O
a	O
considerable	O
increase	O
in	O
performance	O
for	O
classes	B
with	O
few	O
samples	O
a	O
compromise	O
is	O
to	O
use	O
a	O
regularised	O
value	O
in	O
which	O
there	O
is	O
some	O
trade-off	O
between	O
the	O
within	O
class	B
value	O
and	O
the	O
global	O
value	O
of	O
the	O
rescaling	O
parameters	O
a	O
study	O
on	O
the	O
influence	O
of	O
data	O
transformation	B
and	O
metrics	O
on	O
the	O
k-nn	B
rule	O
can	O
be	O
found	O
in	O
todeschini	O
to	O
speed	B
up	O
the	O
process	O
of	O
finding	O
the	O
nearest	O
neighbours	O
several	O
approaches	O
have	O
been	O
proposed	O
fukunaka	O
narendra	O
used	O
a	O
branch	O
and	O
bound	O
algorithm	O
to	O
increase	O
the	O
speed	B
to	O
compute	O
the	O
nearest	B
neighbour	I
the	O
idea	O
is	O
to	O
divide	O
the	O
attribute	O
space	O
in	O
regions	O
and	O
explore	O
a	O
region	O
only	O
when	O
there	O
are	O
possibilities	O
of	O
finding	O
there	O
a	O
nearest	B
neighbour	I
the	O
regions	O
are	O
hierarchically	O
decomposed	O
to	O
subsets	O
sub-subsets	O
and	O
so	O
on	O
other	O
ways	O
to	O
speed	B
up	O
the	O
process	O
are	O
to	O
use	O
a	O
condensed-nearest-neighbour	O
rule	O
modern	O
statistical	B
techniques	O
a	O
reduced-nearest-neighbour-rule	O
or	O
the	O
edited-nearest-neighbour-rule	O
batchelor	O
these	O
methods	O
all	O
reduce	O
the	O
training	B
set	I
by	O
retaining	O
those	O
observations	O
which	O
are	O
used	O
to	O
correctly	O
classify	O
the	O
discarded	O
points	O
thus	O
speeding	O
up	O
the	O
classification	B
process	O
however	O
they	O
have	O
not	O
been	O
implemented	O
in	O
the	O
k-nn	B
programs	O
used	O
in	O
this	O
book	O
is	O
split	O
and	O
the	O
second	O
part	O
classified	O
using	O
a	O
k-nn	B
rule	O
however	O
in	O
large	O
datasets	O
this	O
method	O
can	O
be	O
prohibitive	O
in	O
cpu	O
time	B
indeed	O
for	O
large	O
datasets	O
the	O
method	O
is	O
very	O
time	B
classification	B
enas	O
choi	O
have	O
looked	O
at	O
this	O
problem	O
in	O
a	O
simulation	O
study	O
and	O
for	O
the	O
two	O
classes	B
problem	O
see	O
mclachlan	O
for	O
the	O
choice	O
can	O
be	O
made	O
by	O
cross-validation	O
methods	O
whereby	O
the	O
training	O
data	O
consuming	O
since	O
all	O
the	O
training	O
data	O
must	O
be	O
stored	O
and	O
examined	O
for	O
each	O
proposed	O
rules	O
for	O
in	O
the	O
trials	O
reported	O
in	O
this	O
book	O
we	O
used	O
the	O
nearest	B
neighbour	I
classifier	B
with	O
was	O
chosen	O
by	O
cross-validation	O
distances	O
were	O
scaled	O
using	O
the	O
standard	O
deviation	O
for	O
each	O
attribute	O
with	O
the	O
calculation	O
conditional	O
on	O
the	O
class	B
ties	O
were	O
broken	O
by	O
a	O
majority	O
vote	O
or	O
as	O
a	O
last	O
resort	O
the	O
default	B
rule	I
no	O
condensing	O
exception	O
to	O
this	O
was	O
the	O
satellite	O
dataset	O
see	O
section	O
in	O
which	O
details	O
example	B
nearest	B
neighbour	I
classifier	B
a	O
e	O
r	O
a	O
e	O
s	O
o	O
c	O
u	O
g	O
l	O
relative	O
weight	O
fig	O
nearest	B
neighbour	I
classifier	B
for	O
one	O
test	O
example	B
the	O
following	O
example	B
shows	O
how	O
the	O
nearest	B
neighbour	I
classifier	B
works	O
the	O
data	O
are	O
a	O
random	O
subset	O
of	O
dataset	O
in	O
andrews	O
herzberg	O
which	O
examines	O
the	O
relationship	O
between	O
chemical	O
subclinical	O
and	O
overt	O
nonketotic	O
diabetes	B
in	O
patients	O
above	O
for	O
more	O
details	O
for	O
ease	O
of	O
presentation	O
we	O
have	O
used	O
only	O
patients	O
and	O
two	O
of	O
the	O
six	O
variables	O
relative	O
weight	O
and	O
glucose	O
area	O
and	O
the	O
data	O
are	O
shown	O
in	O
figure	O
the	O
classifications	O
of	O
patients	O
are	O
one	O
of	O
overt	O
diabetic	O
chemical	O
diabetic	O
and	O
are	O
labeled	O
on	O
the	O
graph	O
in	O
this	O
example	B
it	O
can	O
be	O
seen	O
that	O
glucose	O
area	O
sec	O
projection	B
pursuit	I
classification	B
is	O
more	O
useful	O
in	O
separating	O
the	O
three	O
classes	B
and	O
that	O
class	B
is	O
easier	O
to	O
distinguish	O
than	O
classes	B
and	O
a	O
new	O
patient	O
whose	O
condition	O
is	O
supposed	O
unknown	O
is	O
assigned	O
the	O
same	O
classification	B
as	O
his	O
nearest	B
neighbour	I
on	O
the	O
graph	O
the	O
distance	B
as	O
measured	O
to	O
each	O
point	O
needs	O
to	O
be	O
scaled	O
in	O
some	O
way	O
to	O
take	O
account	O
for	O
different	O
variability	O
in	O
the	O
different	O
directions	O
in	O
this	O
case	O
the	O
patient	O
is	O
classified	O
as	O
being	O
in	O
class	B
and	O
is	O
classified	O
correctly	O
the	O
decision	O
regions	O
for	O
the	O
nearest	B
neighbour	I
are	O
composed	O
of	O
piecewise	O
linear	O
boundaries	O
which	O
may	O
be	O
disconnected	O
regions	O
these	O
regions	O
are	O
the	O
union	O
of	O
dirichlet	O
cells	O
each	O
cell	O
consists	O
of	O
points	O
which	O
are	O
nearer	O
an	O
appropriate	O
metric	O
to	O
a	O
given	O
observation	O
than	O
to	O
any	O
other	O
for	O
this	O
data	O
we	O
have	O
shaded	O
each	O
cell	O
according	O
to	O
the	O
class	B
of	O
its	O
centre	O
and	O
the	O
resulting	O
decision	O
regions	O
are	O
shown	O
in	O
figure	O
nearest	B
neighbour	I
decision	O
regions	O
a	O
e	O
r	O
a	O
e	O
s	O
o	O
c	O
u	O
g	O
l	O
relative	O
weight	O
fig	O
decision	O
regions	O
for	O
nearest	B
neighbour	I
classifier	B
projection	B
pursuit	I
classification	B
as	O
we	O
have	O
seen	O
in	O
the	O
previous	O
sections	O
our	O
goal	O
has	O
been	O
to	O
estimate	O
minimum	O
risk	O
decision	O
problem	O
into	O
a	O
minimum	O
error	O
decision	O
problem	O
to	O
do	O
so	O
we	O
when	O
to	O
class	B
in	O
order	O
to	O
assignk	O
and	O
to	O
simplify	O
problems	O
transform	O
our	O
we	O
assume	O
that	O
we	O
know	O
such	O
that	O
and	O
simply	O
alter	O
h	O
to	O
h	O
and	O
f-	O
constraining	O
ifgyt	O
then	O
an	O
approximation	O
to	O
to	O
be	O
of	O
the	O
form	O
constant	O
otherwise	O
is	O
a	O
a	O
n	O
a	O
n	O
a	O
a	O
a	O
a	O
a	O
a	O
f	O
a	O
modern	O
statistical	B
techniques	O
or	O
breiman	O
et	O
al	O
for	O
details	O
that	O
we	O
are	O
trying	O
to	O
estimate	O
the	O
problem	O
can	O
be	O
put	O
into	O
a	O
different	O
setting	O
that	O
resolves	O
the	O
difficulty	O
let	O
a	O
with	O
these	O
new	O
prior	O
and	O
costsk	O
is	O
assigned	O
to	O
classea	O
when	O
b	O
b	O
so	O
our	O
final	O
goal	O
is	O
to	O
build	O
a	O
good	O
estimator	O
we	O
could	O
use	O
to	O
define	O
the	O
quality	O
of	O
an	O
however	O
is	O
obviously	O
the	O
best	O
estimator	O
useless	O
since	O
it	O
contains	O
the	O
unknown	O
quantities	O
a	O
random	O
vector	O
on	O
h	O
with	O
and	O
define	O
new	O
by	O
variables	O
if	O
then	O
we	O
then	O
define	O
the	O
mean	O
square	O
error	O
by	O
ky	O
estimatorj	O
we	O
have	O
and	O
so	O
to	O
compare	O
two	O
estimatorsj	O
andj	O
we	O
can	O
compare	O
the	O
values	O
of	O
and	O
when	O
projection	B
pursuit	I
techniques	O
are	O
used	O
in	O
classification	B
problems	O
ky	O
the	O
coefficients	O
and	O
with	O
and	O
the	O
functions	O
ya	O
if	O
in	O
observation	O
the	O
very	O
interesting	O
point	O
is	O
that	O
it	O
can	O
be	O
easily	O
shown	O
that	O
for	O
any	O
class	B
probability	O
are	O
parameters	O
of	O
the	O
model	O
and	O
are	O
estimated	O
by	O
least	O
squares	O
equation	O
is	O
approximated	O
by	O
is	O
ky	O
a	O
with	O
otherwise	O
otherwise	O
modelled	O
as	O
n	O
a	O
g	O
a	O
g	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
n	O
a	O
a	O
a	O
a	O
n	O
n	O
n	O
a	O
a	O
a	O
n	O
a	O
a	O
n	O
j	O
n	O
n	O
n	O
n	O
a	O
sec	O
projection	B
pursuit	I
classification	B
the	O
projection	O
part	O
of	O
the	O
term	O
projection	B
pursuit	I
indicates	O
that	O
the	O
vector	O
x	O
is	O
of	O
the	O
projections	O
and	O
the	O
pursuit	O
part	O
indicates	O
that	O
the	O
optimization	O
technique	O
is	O
used	O
functions	O
are	O
in	O
order	O
they	O
are	O
special	O
scatterplot	B
smoother	I
designed	O
to	O
have	O
the	O
following	O
features	B
they	O
are	O
very	O
fast	O
to	O
compute	O
and	O
have	O
a	O
variable	O
span	O
aee	O
statsci	O
for	O
details	O
it	O
is	O
the	O
purpose	O
of	O
the	O
projection	B
pursuit	I
algorithm	O
to	O
minimise	O
with	O
respect	O
to	O
given	O
the	O
number	O
of	O
predictive	O
terms	O
comprising	O
the	O
model	O
increasing	O
the	O
number	O
of	O
terms	O
decreases	O
the	O
bias	B
specification	O
error	O
at	O
the	O
expense	O
of	O
increasing	O
the	O
variance	O
of	O
the	O
and	O
parameter	O
estimates	O
then	O
the	O
above	O
expression	O
is	O
minimised	O
with	O
respect	O
to	O
the	O
parameters	O
and	O
the	O
functions	O
projected	O
onto	O
the	O
direction	O
vectors	O
to	O
get	O
the	O
lengths	O
to	O
find	O
good	O
direction	O
vectors	O
a	O
few	O
words	O
on	O
the	O
the	O
parameters	O
and	O
functions	O
the	O
training	O
data	O
the	O
principal	O
task	O
of	O
the	O
user	O
is	O
to	O
choose	O
and	O
find	O
and	O
less	O
that	O
is	O
solutions	O
that	O
minimise	O
the	O
strategy	O
is	O
to	O
start	O
with	O
a	O
relatively	O
large	O
value	O
of	O
are	O
found	O
for	O
all	O
models	O
of	O
size	O
in	O
order	O
of	O
decreasing	O
for	O
the	O
numerical	O
search	O
in	O
each	O
model	O
are	O
the	O
solution	O
values	O
for	O
the	O
most	O
important	O
ofs	O
e	O
terms	O
of	O
the	O
previous	O
model	O
the	O
importance	O
is	O
measured	O
as	O
all	O
the	O
is	O
one	O
the	O
starting	O
point	O
for	O
the	O
minimisation	O
of	O
the	O
largest	O
model	O
is	O
given	O
by	O
an	O
normalised	O
so	O
that	O
the	O
most	O
important	O
term	O
has	O
unit	O
importance	O
that	O
the	O
variance	O
of	O
term	O
stagewise	O
model	O
stuetzle	O
and	O
statsci	O
for	O
the	O
sequence	O
of	O
solutions	O
generated	O
in	O
this	O
manner	O
is	O
then	O
examined	O
by	O
the	O
user	O
and	O
a	O
final	O
model	O
is	O
chosen	O
according	O
to	O
the	O
guidelines	O
above	O
a	O
very	O
precise	O
description	O
of	O
the	O
process	O
the	O
algorithm	O
we	O
used	O
in	O
the	O
trials	O
to	O
classify	O
by	O
projection	B
pursuit	I
is	O
smart	B
the	O
starting	O
parameter	O
values	O
friedman	O
for	O
details	O
and	O
appendix	O
b	O
for	O
availability	O
example	B
this	O
method	O
is	O
illustrated	O
using	O
a	O
dataset	O
with	O
three	O
classes	B
relating	O
to	O
chemical	O
and	O
overt	O
diabetes	B
the	O
data	O
can	O
be	O
found	O
in	O
dataset	O
of	O
andrews	O
herzberg	O
and	O
were	O
first	O
published	O
in	O
reaven	O
miller	O
the	O
smart	B
model	O
can	O
be	O
examined	O
by	O
plotting	O
the	O
smooth	O
functions	O
in	O
the	O
two	O
projected	O
data	O
co-ordinates	O
these	O
are	O
given	O
in	O
figure	O
which	O
also	O
shows	O
the	O
class	B
values	O
given	O
by	O
the	O
projected	O
points	O
of	O
the	O
selected	O
training	O
data	O
of	O
the	O
patients	O
the	O
remainder	O
of	O
the	O
model	O
to	O
obtain	O
a	O
linear	O
combination	O
of	O
the	O
functions	O
which	O
can	O
then	O
be	O
used	O
to	O
model	O
the	O
conditional	O
probabilities	O
in	O
this	O
example	B
we	O
get	O
chooses	O
the	O
values	O
of	O
y	O
t	O
n	O
a	O
g	O
n	O
modern	O
statistical	B
techniques	O
smooth	O
functions	O
with	O
training	O
data	O
projections	O
projected	O
point	O
f	O
f	O
projected	O
point	O
fig	O
projected	O
training	O
data	O
with	O
smooth	O
functions	O
the	O
remaining	O
patients	O
were	O
used	O
as	O
a	O
test	O
data	O
set	O
and	O
for	O
each	O
class	B
the	O
unscaled	O
conditional	O
probability	O
can	O
be	O
obtained	O
using	O
the	O
relevant	O
coefficients	O
for	O
that	O
class	B
these	O
are	O
shown	O
in	O
figure	O
where	O
we	O
have	O
plotted	O
the	O
predicted	O
value	O
against	O
only	O
one	O
of	O
the	O
projected	O
co-ordinate	O
axes	O
it	O
is	O
clear	O
that	O
if	O
we	O
choose	O
the	O
model	O
hence	O
the	O
class	B
to	O
maximise	O
this	O
value	O
then	O
we	O
will	O
choose	O
the	O
correct	O
class	B
each	O
time	B
naive	B
bayes	I
all	O
the	O
nonparametric	B
methods	I
described	O
so	O
far	O
in	O
this	O
chapter	O
suffer	O
from	O
the	O
requirements	O
that	O
all	O
of	O
the	O
sample	O
must	O
be	O
stored	O
since	O
a	O
large	O
number	O
of	O
observations	O
is	O
needed	O
to	O
obtain	O
good	O
estimates	O
the	O
memory	B
requirements	O
can	O
be	O
severe	O
in	O
this	O
section	O
we	O
will	O
make	O
independence	O
assumptions	O
to	O
be	O
described	O
later	O
among	O
the	O
variables	O
involved	O
in	O
the	O
classification	B
problem	O
in	O
the	O
next	O
section	O
we	O
will	O
address	O
the	O
problem	O
of	O
estimating	O
the	O
relations	O
between	O
the	O
variables	O
involved	O
in	O
a	O
problem	O
and	O
display	O
such	O
relations	O
by	O
mean	O
of	O
a	O
directed	O
acyclic	O
graph	O
the	O
na	O
ve	O
bayes	O
classifier	B
is	O
obtained	O
as	O
follows	O
we	O
assume	O
that	O
the	O
joint	O
distribution	O
of	O
classes	B
and	O
attributes	B
can	O
be	O
written	O
as	O
the	O
problem	O
is	O
then	O
to	O
obtain	O
the	O
probabilities	O
of	O
independence	O
makes	O
it	O
much	O
easier	O
to	O
estimate	O
these	O
probabilities	O
since	O
each	O
attribute	O
can	O
be	O
treated	O
separately	O
if	O
an	O
attribute	O
takes	O
a	O
continuous	O
value	O
the	O
usual	O
procedure	O
is	O
to	O
discretise	O
the	O
interval	O
and	O
to	O
use	O
the	O
appropriate	O
frequency	O
of	O
the	O
interval	O
although	O
there	O
is	O
an	O
option	O
to	O
use	O
the	O
normal	B
distribution	I
to	O
calculate	O
probabilities	O
the	O
assumption	O
the	O
implementation	O
used	O
in	O
our	O
trials	O
to	O
obtain	O
a	O
na	O
ve	O
bayes	O
classifier	B
comes	O
from	O
the	O
ind	B
package	I
of	O
machine	O
learning	O
algorithms	O
ind	O
by	O
wray	O
buntine	O
appendix	O
b	O
for	O
availability	O
n	O
a	O
a	O
a	O
sec	O
causal	B
networks	I
estimated	O
conditional	O
probabilities	O
projected	O
point	O
a	O
finite	O
state	O
where	O
we	O
use	O
the	O
short	O
notation	O
is	O
associated	O
the	O
total	O
set	O
of	O
configuration	O
is	O
the	O
set	O
fig	O
projected	O
test	O
data	O
with	O
conditional	O
probablities	O
for	O
three	O
classes	B
class	B
class	B
class	B
causal	B
networks	I
we	O
start	O
this	O
section	O
by	O
introducing	O
the	O
concept	B
of	O
causal	B
network	I
be	O
a	O
directed	O
acyclic	O
graph	O
with	O
each	O
nodez	O
lete	O
space	O
are	O
denoted	O
we	O
assume	O
that	O
typical	O
elements	O
of	O
and	O
elements	O
of	O
we	O
have	O
a	O
probability	O
over	O
be	O
a	O
directed	O
acyclic	O
graph	O
for	O
eachzu	O
definition	O
be	O
the	O
set	O
of	O
all	O
parents	O
ofz	O
be	O
the	O
set	O
of	O
all	O
descendent	O
ofz	O
furthermore	O
forz	O
be	O
the	O
set	O
of	O
variables	O
in	O
excludingz	O
andz	O
s	O
descendent	O
andz	O
are	O
conditionally	O
independent	O
the	O
then	O
if	O
for	O
every	O
subset	O
there	O
are	O
two	O
key	O
results	O
establishing	O
the	O
relations	O
between	O
a	O
causal	O
the	O
proofs	O
can	O
be	O
found	O
in	O
neapolitan	O
the	O
first	O
theorem	O
establishes	O
that	O
g	O
is	O
a	O
causal	B
network	I
can	O
thus	O
in	O
a	O
causal	B
network	I
if	O
one	O
knows	O
the	O
conditional	O
probability	O
distribution	O
of	O
each	O
variable	O
given	O
its	O
parents	O
one	O
can	O
compute	O
the	O
joint	O
probability	O
distribution	O
of	O
all	O
the	O
variables	O
in	O
the	O
network	O
this	O
obviously	O
can	O
reduce	O
the	O
complexity	O
of	O
determining	O
the	O
is	O
called	O
a	O
causal	O
or	O
bayesian	O
network	O
be	O
written	O
as	O
let	O
modern	O
statistical	B
techniques	O
distribution	O
enormously	O
the	O
theorem	O
just	O
established	O
shows	O
that	O
if	O
we	O
know	O
that	O
a	O
dag	O
and	O
a	O
probability	O
distribution	O
constitute	O
a	O
causal	B
network	I
then	O
the	O
joint	O
distribution	O
can	O
be	O
retrieved	O
from	O
the	O
conditional	O
distribution	O
of	O
every	O
variable	O
given	O
its	O
parents	O
this	O
does	O
not	O
imply	O
however	O
that	O
if	O
we	O
arbitrarily	O
specify	O
a	O
dag	O
and	O
conditional	O
probability	O
distributions	O
of	O
every	O
variables	O
given	O
its	O
parents	O
we	O
will	O
necessary	O
have	O
a	O
causal	B
network	I
this	O
inverse	O
result	O
can	O
be	O
stated	O
as	O
follows	O
is	O
uniquely	O
determined	O
by	O
we	O
illustrate	O
the	O
notion	O
of	O
network	O
with	O
a	O
simple	O
example	B
taken	O
from	O
cooper	O
suppose	O
that	O
metastatic	O
cancer	O
is	O
a	O
cause	O
of	O
brain	O
tumour	O
and	O
can	O
also	O
cause	O
an	O
increase	O
in	O
total	O
serum	O
calcium	O
suppose	O
further	O
that	O
either	O
a	O
brain	O
tumor	O
or	O
an	O
increase	O
in	O
total	O
serum	O
calcium	O
could	O
cause	O
a	O
patient	O
to	O
fall	O
into	O
a	O
coma	O
and	O
that	O
a	O
brain	O
tumor	O
could	O
cause	O
papilledema	O
let	O
be	O
a	O
set	O
of	O
finite	O
sets	O
of	O
alternatives	O
are	O
not	O
yet	O
calling	O
the	O
members	O
of	O
let	O
variables	O
since	O
we	O
do	O
not	O
yet	O
have	O
a	O
probability	O
distribution	O
and	O
let	O
be	O
a	O
dag	O
in	O
addition	O
forz	O
be	O
the	O
set	O
of	O
all	O
parents	O
ofz	O
and	O
let	O
a	O
conditional	O
probability	O
distribution	O
ofz	O
be	O
specified	O
for	O
every	O
event	O
that	O
is	O
we	O
have	O
then	O
a	O
joint	O
probability	O
distribution	O
of	O
the	O
vertices	O
a	O
probability	O
distribution	O
in	O
constitutes	O
a	O
causal	B
network	I
and	O
metastatic	O
cancer	O
present	O
serum	O
calcium	O
increased	O
brain	O
tumor	O
present	O
coma	O
present	O
papilledema	O
present	O
cancer	O
not	O
present	O
serum	O
calcium	O
not	O
increased	O
brain	O
tumor	O
not	O
present	O
coma	O
not	O
present	O
papilledema	O
not	O
present	O
fig	O
dag	O
for	O
the	O
cancer	O
problem	O
then	O
the	O
structure	O
of	O
our	O
knowledge-base	O
is	O
represented	O
by	O
the	O
dag	O
in	O
figure	O
this	O
structure	O
together	O
with	O
quantitative	O
knowledge	O
of	O
the	O
conditional	O
probability	O
of	O
every	O
variable	O
given	O
all	O
possible	O
parent	O
states	O
define	O
a	O
causal	B
network	I
that	O
can	O
be	O
used	O
as	O
device	O
to	O
perform	O
efficient	O
inference	O
knowledge	O
about	O
variables	O
as	O
it	O
arrives	O
be	O
able	O
to	O
see	O
the	O
effect	O
on	O
the	O
other	O
variables	O
of	O
one	O
variable	O
taking	O
a	O
particular	O
value	O
and	O
so	O
on	O
see	O
pearl	O
and	O
lauritzen	O
spiegelhalter	O
p	O
p	O
f	O
f	O
j	O
j	O
sec	O
causal	B
networks	I
so	O
once	O
a	O
causal	B
network	I
has	O
been	O
built	O
it	O
constitutes	O
an	O
efficient	O
device	O
to	O
perform	O
probabilistic	B
inference	I
however	O
there	O
remains	O
the	O
previous	O
problem	O
of	O
building	O
such	O
a	O
network	O
that	O
is	O
to	O
provide	O
the	O
structure	O
and	O
conditional	O
probabilities	O
necessary	O
for	O
characterizing	O
the	O
network	O
a	O
very	O
interesting	O
task	O
is	O
then	O
to	O
develop	O
methods	O
able	O
to	O
learn	O
the	O
net	O
directly	O
from	O
raw	O
data	O
as	O
an	O
alternative	O
to	O
the	O
method	O
of	O
eliciting	O
opinions	O
from	O
the	O
experts	O
in	O
the	O
problem	O
of	O
learning	B
graphical	I
representations	I
it	O
could	O
be	O
said	O
that	O
the	O
statistical	B
community	O
has	O
mainly	O
worked	O
in	O
the	O
direction	O
of	O
building	O
undirected	O
representations	O
chapter	O
of	O
whittaker	O
provides	O
a	O
good	O
survey	O
on	O
selection	O
of	O
undirected	O
graphical	O
representations	O
up	O
to	O
from	O
the	O
statistical	B
point	O
of	O
view	O
the	O
program	O
bifrost	B
jsgaard	O
et	O
al	O
has	O
been	O
developed	O
very	O
recently	O
to	O
obtain	O
causal	O
models	O
a	O
second	O
literature	O
on	O
model	O
selection	O
devoted	O
to	O
the	O
construction	O
of	O
directed	O
graphs	O
can	O
be	O
found	O
in	O
the	O
social	O
sciences	O
et	O
al	O
spirtes	O
et	O
al	O
and	O
the	O
artificial	O
intelligence	O
community	O
herkovsits	O
cooper	O
cooper	O
herkovsits	O
and	O
fung	O
crawford	O
in	O
this	O
section	O
we	O
will	O
concentrate	O
on	O
methods	O
to	O
build	O
a	O
simplified	O
kind	O
of	O
causal	O
structure	O
polytrees	B
connected	O
networks	O
networks	O
where	O
no	O
more	O
than	O
one	O
path	O
exists	O
between	O
any	O
two	O
nodes	O
polytrees	B
are	O
directed	O
graphs	O
which	O
do	O
not	O
contain	O
loops	O
in	O
the	O
skeleton	O
network	O
without	O
the	O
arrows	O
that	O
allow	O
an	O
extremely	O
efficient	O
local	O
propagation	O
procedure	O
before	O
describing	O
how	O
to	O
build	O
polytrees	B
from	O
data	O
we	O
comment	O
on	O
how	O
to	O
use	O
a	O
polytree	O
in	O
a	O
classification	B
problem	O
in	O
any	O
classification	B
problem	O
we	O
have	O
a	O
set	O
of	O
variables	O
that	O
have	O
influence	O
on	O
a	O
distinguished	O
classification	B
the	O
problem	O
is	O
given	O
a	O
particular	O
instantiation	O
of	O
these	O
variables	O
to	O
predict	O
for	O
this	O
task	O
we	O
need	O
a	O
set	O
of	O
examples	O
and	O
their	O
correct	O
classification	B
acting	O
as	O
a	O
training	O
sample	O
in	O
this	O
context	O
we	O
first	O
estimate	O
from	O
this	O
training	O
sample	O
a	O
network	O
next	O
in	O
propagation	O
mode	O
given	O
a	O
new	O
case	O
with	O
unknown	O
classification	B
we	O
will	O
instantiate	O
and	O
propagate	O
the	O
available	O
information	O
showing	O
the	O
more	O
likely	O
value	O
of	O
the	O
classification	B
variable	O
that	O
is	O
to	O
classify	O
this	O
particular	O
case	O
in	O
one	O
of	O
the	O
possible	O
categories	O
of	O
the	O
value	O
of	O
structure	O
displaying	O
the	O
causal	O
relationships	O
among	O
the	O
variables	O
variable	O
moreover	O
the	O
network	O
shows	O
the	O
variables	O
in	O
value	O
of	O
all	O
the	O
variables	O
in	O
directly	O
have	O
influence	O
on	O
the	O
children	O
of	O
in	O
fact	O
the	O
parents	O
of	O
knowledge	O
of	O
these	O
variables	O
makes	O
parents	O
of	O
the	O
children	O
of	O
the	O
rest	O
of	O
variables	O
in	O
theory	O
to	O
build	O
polytree-based	O
representations	O
for	O
a	O
general	O
set	O
of	O
variables	O
assume	O
that	O
the	O
of	O
to	O
estimate	O
can	O
be	O
represented	O
by	O
some	O
unknown	O
polytree	O
that	O
has	O
the	O
form	O
it	O
is	O
important	O
to	O
note	O
that	O
this	O
classifier	B
can	O
be	O
used	O
even	O
when	O
we	O
do	O
not	O
know	O
the	O
that	O
and	O
the	O
other	O
independent	O
of	O
so	O
the	O
rest	O
of	O
the	O
network	O
could	O
be	O
pruned	O
thus	O
reducing	O
the	O
complexity	O
and	O
increasing	O
the	O
efficiency	O
of	O
the	O
classifier	B
however	O
since	O
the	O
process	O
of	O
building	O
the	O
network	O
does	O
not	O
take	O
into	O
account	O
the	O
fact	O
that	O
we	O
are	O
only	O
interested	O
in	O
classifying	O
we	O
should	O
expect	O
as	O
a	O
classifier	B
a	O
poorer	O
performance	O
than	O
other	O
classification	B
oriented	O
methods	O
however	O
the	O
built	O
networks	O
are	O
able	O
to	O
display	O
insights	O
into	O
the	O
classification	B
problem	O
that	O
other	O
methods	O
lack	O
we	O
now	O
proceed	O
to	O
describe	O
the	O
discrete-value	O
variables	O
we	O
are	O
trying	O
modern	O
statistical	B
techniques	O
skeleton	O
we	O
have	O
the	O
following	O
theorem	O
as	O
a	O
parent	O
is	O
defined	O
by	O
is	O
the	O
empty	O
set	O
of	O
direct	O
parents	O
of	O
the	O
variable	O
the	O
first	O
step	O
in	O
the	O
process	O
of	O
building	O
a	O
polytree	O
is	O
to	O
learn	O
the	O
skeleton	O
to	O
build	O
the	O
it	O
is	O
important	O
to	O
keep	O
in	O
mind	O
that	O
a	O
na	O
ve	O
bayes	O
classifier	B
can	O
be	O
represented	O
by	O
a	O
polytree	O
more	O
precisely	O
a	O
tree	O
in	O
which	O
each	O
attribute	O
node	O
has	O
the	O
class	B
at	O
simpler	O
representations	O
than	O
the	O
one	O
displayed	O
in	O
figure	O
the	O
skeleton	O
of	O
the	O
graph	O
involved	O
in	O
that	O
example	B
is	O
not	O
a	O
tree	O
theorem	O
any	O
maximum	O
weight	O
spanning	O
tree	O
where	O
the	O
weight	O
of	O
the	O
branch	O
connecting	O
then	O
according	O
to	O
key	O
results	O
seen	O
at	O
the	O
beginning	O
of	O
this	O
section	O
we	O
have	O
a	O
causal	O
is	O
nondegenerate	O
meaning	O
that	O
there	O
exists	O
a	O
connected	O
dag	O
that	O
displays	O
all	O
the	O
dependencies	O
and	O
is	O
a	O
polytree	O
we	O
will	O
assume	O
is	O
representable	O
by	O
a	O
polytree	O
where	O
in	O
and	O
the	O
parents	O
of	O
each	O
variable	O
are	O
mutually	O
independent	O
so	O
we	O
are	O
aiming	O
network	O
independencies	O
embedded	O
in	O
variable	O
if	O
a	O
nondegenerate	O
and	O
log	O
a	O
will	O
unambiguously	O
recover	O
the	O
skeleton	O
of	O
eracy	O
implies	O
that	O
for	O
any	O
pairs	O
of	O
log	O
p	O
jsj	O
having	O
found	O
the	O
skeleton	O
of	O
the	O
polytree	O
we	O
move	O
on	O
to	O
find	O
the	O
directionality	O
of	O
the	O
branches	O
to	O
recover	O
the	O
directions	O
of	O
the	O
branches	O
we	O
use	O
the	O
following	O
facts	O
nondegenthat	O
do	O
not	O
have	O
a	O
common	O
descendent	O
p	O
jsjh	O
p	O
jsjh	O
we	O
have	O
we	O
have	O
where	O
we	O
have	O
then	O
and	O
for	O
any	O
of	O
the	O
patterns	O
furthermore	O
for	O
the	O
pattern	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
n	O
f	O
f	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
n	O
f	O
f	O
f	O
a	O
a	O
a	O
g	O
a	O
g	O
a	O
a	O
a	O
a	O
sec	O
causal	B
networks	I
taking	O
all	O
these	O
facts	O
into	O
account	O
we	O
can	O
recover	O
the	O
head	O
to	O
head	O
patterns	O
which	O
are	O
the	O
really	O
important	O
ones	O
the	O
rest	O
of	O
the	O
branches	O
can	O
be	O
assigned	O
any	O
direction	O
as	O
long	O
as	O
we	O
do	O
not	O
produce	O
more	O
head	O
to	O
head	O
patterns	O
the	O
algorithm	O
to	O
direct	O
the	O
skeleton	O
can	O
be	O
found	O
in	O
pearl	O
p	O
usal	O
the	O
program	O
to	O
estimate	O
causal	O
polytrees	B
used	O
in	O
our	O
trials	O
is	O
castle	B
ructures	O
from	O
inductive	O
arning	O
it	O
has	O
been	O
developed	O
at	O
the	O
university	O
of	O
granada	O
for	O
the	O
esprit	O
project	O
statlog	B
et	O
al	O
acid	O
et	O
al	O
see	O
appendix	O
b	O
for	O
availability	O
example	B
we	O
now	O
illustrate	O
the	O
use	O
of	O
the	O
bayesian	O
learning	O
methodology	O
in	O
a	O
simple	O
model	O
the	O
digit	O
recognition	O
in	O
a	O
calculator	O
digits	O
are	O
ordinarily	O
displayed	O
on	O
electronic	O
watches	O
and	O
calculators	O
using	O
seven	O
horizontal	O
and	O
vertical	O
lights	O
in	O
on	O
off	O
configurations	O
figure	O
we	O
number	O
the	O
lights	O
to	O
be	O
an	O
eight	O
dimensional	O
as	O
shown	O
in	O
figure	O
we	O
take	O
fig	O
digits	O
the	O
we	O
generate	O
examples	O
from	O
a	O
faulty	O
calculator	O
the	O
data	O
consist	O
of	O
outcomes	O
from	O
vector	O
where	O
denotes	O
theil	O
and	O
when	O
fixing	O
to	O
is	O
a	O
seven	O
dimensional	O
vector	O
of	O
zeros	O
and	O
ones	O
with	O
otherwise	O
position	O
is	O
on	O
for	O
theil	O
digit	O
and	O
if	O
the	O
light	O
in	O
the	O
the	O
random	O
vector	O
w	O
i	O
where	O
is	O
the	O
class	B
label	O
the	O
digit	O
and	O
assumes	O
the	O
values	O
in	O
with	O
equal	O
probability	O
and	O
the	O
i	O
are	O
zero-one	O
variables	O
given	O
the	O
value	O
of	O
the	O
i	O
are	O
each	O
independently	O
equal	O
to	O
the	O
and	O
are	O
in	O
error	O
with	O
probability	O
with	O
probabilityd	O
value	O
corresponding	O
to	O
the	O
our	O
aim	O
is	O
to	O
build	O
up	O
the	O
polytree	O
displaying	O
the	O
in	O
j	O
jy	O
jsj	O
ft	O
i	O
q	O
p	O
p	O
jsjk	O
we	O
generate	O
four	O
hundred	O
samples	O
of	O
this	O
distributionand	O
use	O
them	O
as	O
a	O
learning	O
sample	O
after	O
reading	O
in	O
the	O
sample	O
estimating	O
the	O
skeleton	O
and	O
directing	O
the	O
skeleton	O
the	O
polytree	O
estimated	O
by	O
castle	B
is	O
the	O
one	O
shown	O
in	O
figure	O
castle	B
then	O
tells	O
us	O
what	O
we	O
had	O
expected	O
finally	O
we	O
examine	O
the	O
predictive	O
power	O
of	O
this	O
polytree	O
the	O
posterior	O
probabilities	O
of	O
each	O
digit	O
given	O
some	O
observed	O
patterns	O
are	O
shown	O
in	O
figure	O
a	O
r	O
modern	O
statistical	B
techniques	O
fig	O
obtained	O
polytree	O
digit	O
other	O
recent	O
approaches	O
the	O
methods	O
discussed	O
in	O
this	O
section	O
are	O
available	O
via	O
anonymous	O
ftp	O
from	O
statlib	O
internet	O
address	O
a	O
version	O
of	O
ace	B
for	O
nonlinear	O
discriminant	O
analysis	O
is	O
available	O
fig	O
probabilitiesl	O
for	O
some	O
digits	O
jwuf	O
mars	B
is	O
available	O
in	O
a	O
fortran	O
version	O
since	O
these	O
algorithms	O
were	O
not	O
formally	O
included	O
in	O
the	O
statlog	B
trials	O
various	O
reasons	O
we	O
give	O
only	O
a	O
brief	O
introduction	O
as	O
the	O
s	O
coded	O
functionr	O
ace	B
nonlinear	O
transformation	B
of	O
variables	O
is	O
a	O
commonly	O
used	O
practice	O
in	O
regression	O
problems	O
the	O
alternating	B
conditional	I
expectation	I
algorithm	O
friedman	O
is	O
a	O
simple	O
iterative	O
scheme	O
using	O
only	O
bivariate	O
conditional	O
expectations	O
which	O
finds	O
those	O
transformations	O
that	O
produce	O
the	O
best	O
fitting	O
additive	O
model	O
approaches	O
this	O
problem	O
by	O
minimising	O
the	O
squared-error	O
objective	O
suppose	O
we	O
have	O
two	O
random	O
variables	O
the	O
response	O
so	O
that	O
and	O
the	O
predictor	O
w	O
the	O
ace	B
algorithm	O
seek	O
transformationsm	O
for	O
fixedm	O
the	O
minimisingb	O
conversely	O
for	O
fixedb	O
ism	O
the	O
key	O
idea	O
in	O
the	O
ace	B
algorithm	O
is	O
to	O
begin	O
with	O
minimisingm	O
the	O
and	O
we	O
using	O
an	O
automatic	O
smoothing	O
procedure	O
this	O
constitutes	O
one	O
iteration	O
of	O
the	O
algorithm	O
which	O
terminates	O
when	O
an	O
iteration	O
fails	O
to	O
ace	B
places	O
no	O
restriction	O
on	O
the	O
type	O
of	O
each	O
variable	O
the	O
transformation	B
functions	O
representing	O
the	O
class	B
labels	O
sec	O
other	O
recent	O
approaches	O
some	O
starting	O
functions	O
and	O
alternate	O
these	O
two	O
steps	O
until	O
convergence	O
with	O
multiple	O
n	O
ace	B
seeks	O
to	O
minimise	O
in	O
practice	O
given	O
a	O
dataset	O
estimates	O
of	O
the	O
conditional	O
expectations	O
are	O
constructed	O
in	O
order	O
to	O
stop	O
the	O
iterates	O
from	O
shrinking	O
is	O
scaled	O
to	O
have	O
unit	O
variance	O
in	O
each	O
iteration	O
also	O
without	O
loss	O
of	O
generality	O
the	O
condition	O
is	O
imposed	O
the	O
algorithm	O
minimises	O
equation	O
through	O
a	O
series	O
of	O
single-function	O
minimisations	O
involving	O
smoothed	O
estimates	O
of	O
bivariate	O
predictors	O
m	O
to	O
zero	O
functions	O
which	O
trivially	O
minimise	O
the	O
squared	O
error	O
criterion	O
m	O
mu	O
n	O
minimising	O
with	O
conditional	O
expectations	O
for	O
a	O
given	O
set	O
of	O
functionsb	O
yields	O
a	O
newm	O
respect	O
tom	O
rq	O
m	O
in	O
turn	O
with	O
givenm	O
is	O
minimised	O
for	O
eachbt	O
and	O
yx	O
uwv	O
withu	O
yielding	O
the	O
solution	O
b-a	O
b	O
m	O
a	O
assume	O
values	O
on	O
the	O
real	O
line	O
but	O
their	O
arguments	O
may	O
assume	O
m	O
ace	B
then	O
finds	O
the	O
transformations	O
that	O
make	O
the	O
relationship	O
ofm	O
to	O
theb	O
as	O
linear	O
as	O
possible	O
order	O
regression	O
spline	O
function	O
one	O
predictor	O
variable	O
b	O
an	O
approximating	O
obtained	O
by	O
dividing	O
the	O
range	O
of	O
values	O
into	O
disjoint	O
regions	O
separated	O
by	O
degree	O
polynomial	O
in	O
called	O
knots	O
the	O
approximation	O
takes	O
the	O
form	O
of	O
a	O
separate	O
each	O
region	O
constrained	O
so	O
that	O
the	O
function	O
and	O
its	O
derivatives	O
are	O
continuous	O
each	O
degree	O
polynomial	O
is	O
defined	O
by	O
parameters	O
so	O
there	O
are	O
a	O
total	O
be	O
continuity	O
requirements	O
place	O
constraints	O
at	O
each	O
knot	O
location	O
making	O
a	O
total	O
of	O
constraints	O
mars	B
the	O
mars	B
adaptive	O
regression	O
spline	O
procedure	O
is	O
based	O
on	O
a	O
generalisation	O
of	O
spline	O
methods	O
for	O
function	O
fitting	O
consider	O
the	O
case	O
of	O
only	O
is	O
points	O
values	O
on	O
any	O
set	O
so	O
ordered	O
real	O
ordered	O
and	O
unordered	O
categorical	O
and	O
binary	O
variables	O
can	O
all	O
be	O
incorporated	O
in	O
the	O
same	O
regression	O
equation	O
for	O
categorical	B
variables	I
the	O
procedure	O
can	O
be	O
regarded	O
as	O
estimating	O
optimal	O
scores	O
for	O
each	O
of	O
their	O
values	O
for	O
use	O
in	O
classification	B
problems	O
the	O
response	O
is	O
replaced	O
by	O
a	O
categorical	O
variable	O
parameters	O
to	O
be	O
adjusted	O
to	O
best	O
fit	O
the	O
data	O
generally	O
the	O
order	O
of	O
the	O
spline	O
is	O
taken	O
to	O
n	O
n	O
a	O
b	O
a	O
a	O
b	O
b	O
n	O
n	O
a	O
t	O
t	O
t	O
q	O
n	O
a	O
b	O
a	O
a	O
s	O
t	O
t	O
t	O
b	O
n	O
b	O
a	O
a	O
n	O
n	O
modern	O
statistical	B
techniques	O
while	O
regression	O
spline	O
fitting	O
can	O
be	O
implemented	O
by	O
directly	O
solving	O
this	O
constrained	O
minimisation	O
problem	O
it	O
is	O
more	O
usual	O
to	O
convert	O
the	O
problem	O
to	O
an	O
unconstrained	O
optimi	O
the	O
chosen	O
knot	O
locations	O
and	O
performing	O
a	O
linear	O
least	O
squares	O
fit	O
of	O
the	O
response	O
on	O
this	O
basis	O
function	O
set	O
in	O
this	O
case	O
the	O
approximation	O
takes	O
the	O
form	O
order	O
spline	O
functions	O
are	O
unconstrained	O
and	O
the	O
continu	O
one	O
such	O
regions	O
and	O
the	O
truncated	O
power	O
functions	O
are	O
defined	O
basis	O
the	O
truncated	O
power	O
basis	O
is	O
comprised	O
of	O
the	O
functions	O
sation	O
by	O
chosing	O
a	O
set	O
of	O
basis	O
functions	O
that	O
span	O
the	O
space	O
of	O
all	O
where	O
the	O
values	O
of	O
the	O
expansion	O
coefficients	O
ity	O
constraints	O
are	O
intrinsically	O
embodied	O
in	O
the	O
basis	O
functions	O
are	O
the	O
knot	O
locations	O
defining	O
the	O
where	O
the	O
flexibility	O
of	O
the	O
regression	O
spline	O
approach	O
can	O
be	O
enhanced	O
by	O
incorporating	O
an	O
automatic	O
knot	O
selection	O
strategy	O
as	O
part	O
of	O
the	O
data	O
fitting	O
process	O
a	O
simple	O
and	O
effective	O
strategy	O
for	O
automatically	O
selecting	O
both	O
the	O
number	O
and	O
locations	O
for	O
the	O
knots	O
was	O
described	O
by	O
who	O
suggested	O
using	O
the	O
truncated	O
power	O
basis	O
in	O
a	O
numerical	O
minimisation	O
of	O
the	O
least	O
squares	O
criterion	O
max	O
can	O
be	O
regarded	O
as	O
the	O
parameters	O
associated	O
with	O
the	O
multivariate	O
adaptive	O
regression	O
spline	O
method	O
can	O
be	O
viewed	O
as	O
thereby	O
estimating	O
the	O
global	O
amount	O
of	O
smoothing	O
to	O
be	O
applied	O
as	O
well	O
as	O
estimating	O
the	O
separate	O
relative	O
amount	O
of	O
smoothing	O
to	O
be	O
applied	O
locally	O
at	O
different	O
locations	O
adding	O
or	O
deleting	O
a	O
knot	O
is	O
viewed	O
as	O
adding	O
or	O
deleting	O
the	O
corresponding	O
the	O
strategy	O
involves	O
starting	O
with	O
a	O
very	O
large	O
number	O
of	O
eligible	O
knot	O
we	O
may	O
choose	O
one	O
at	O
every	O
interior	O
data	O
point	O
and	O
considering	O
as	O
candidates	O
to	O
be	O
selected	O
through	O
a	O
statistical	B
variable	O
subset	B
selection	I
procedure	O
this	O
approach	O
to	O
knot	O
selection	O
is	O
both	O
elegant	O
and	O
here	O
the	O
coefficients	O
ak	O
a	O
multiple	O
linear	O
least	O
squares	O
regression	O
of	O
the	O
response	O
on	O
the	O
variables	O
and	O
locations	O
k	O
max	O
corresponding	O
variables	O
powerful	O
it	O
automatically	O
selects	O
the	O
number	O
of	O
knots	O
and	O
their	O
locations	O
b	O
ofj	O
a	O
multivariate	O
generalisation	O
of	O
this	O
strategy	O
an	O
approximating	O
spline	O
function	O
variables	O
is	O
defined	O
analogously	O
to	O
that	O
for	O
one	O
variable	O
thej	O
space	O
b	O
divided	O
into	O
a	O
set	O
of	O
disjoint	O
regions	O
and	O
within	O
each	O
one	O
inj	O
variables	O
with	O
the	O
maximum	O
degree	O
of	O
any	O
single	O
variable	O
being	O
the	O
approximation	O
region	O
the	O
approximating	O
polynomials	O
in	O
seperate	O
regions	O
along	O
orderj	O
spline	O
functions	O
function	O
set	O
that	O
spans	O
the	O
space	O
of	O
all	O
and	O
its	O
derivatives	O
are	O
constrained	O
to	O
be	O
everywhere	O
continuous	O
this	O
places	O
constraints	O
on	O
is	O
is	O
taken	O
to	O
be	O
a	O
polynomial	O
is	O
most	O
easily	O
constructed	O
using	O
a	O
basis	O
boundaries	O
as	O
in	O
the	O
univariate	O
case	O
b	O
n	O
a	O
a	O
n	O
z	O
n	O
a	O
a	O
a	O
n	O
a	O
b	O
sec	O
other	O
recent	O
approaches	O
mars	B
implements	O
a	O
forwardbackward	O
stepwise	B
selection	I
strategy	O
the	O
forward	B
sein	O
each	O
in	O
the	O
model	O
iteration	O
we	O
consider	O
adding	O
two	O
terms	O
to	O
the	O
model	O
lection	O
begins	O
with	O
only	O
the	O
constant	O
basis	O
functiona	O
is	O
one	O
of	O
the	O
basis	O
functions	O
already	O
chosen	O
wherea	O
not	O
represented	O
ina	O
is	O
one	O
of	O
the	O
predictor	O
variables	O
is	O
a	O
knot	O
location	O
on	O
that	O
variable	O
the	O
two	O
terms	O
of	O
this	O
form	O
which	O
cause	O
the	O
greatest	O
decrease	O
in	O
the	O
residual	O
sum	B
of	I
squares	I
are	O
added	O
to	O
the	O
model	O
the	O
forward	B
selection	O
process	O
continues	O
until	O
a	O
relatively	O
large	O
number	O
of	O
basis	O
functions	O
is	O
included	O
in	O
a	O
deliberate	O
attempt	O
to	O
overfit	O
the	O
data	O
the	O
backward	B
pruning	B
procedure	O
standard	O
stepwise	O
linear	B
regression	I
is	O
then	O
applied	O
with	O
the	O
basis	O
functions	O
representing	O
the	O
stock	O
of	O
variables	O
the	O
best	O
fitting	O
model	O
is	O
chosen	O
with	O
the	O
fit	O
measured	O
by	O
a	O
cross-validation	O
criterion	O
a	O
and	O
mars	B
is	O
able	O
to	O
incorporate	O
variables	O
of	O
different	O
type	O
continuous	O
discrete	O
and	O
categorical	O
a	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
c	O
feng	O
and	O
d	O
michie	O
the	O
turing	O
institute	O
and	O
university	O
of	O
strathclyde	O
this	O
chapter	O
is	O
arranged	O
in	O
three	O
sections	O
section	O
introduces	O
the	O
broad	O
ideas	O
underlying	O
the	O
main	O
rule-learning	B
and	O
tree-learning	B
methods	O
section	O
summarises	O
the	O
specific	O
characteristics	O
of	O
algorithms	O
used	O
for	O
comparative	B
trials	I
in	O
the	O
statlog	B
project	O
section	O
looks	O
beyond	O
the	O
limitations	O
of	O
these	O
particular	O
trials	O
to	O
new	O
approaches	O
and	O
emerging	O
principles	O
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
data	O
fit	O
and	O
mental	B
fit	I
of	O
classifiers	O
in	O
a	O
lecture	O
text	O
see	O
carpenter	O
doran	O
a	O
m	O
turing	O
identified	O
machine	O
learning	O
as	O
a	O
precondition	O
for	O
intelligent	O
systems	O
a	O
more	O
specific	O
engineering	O
expression	O
of	O
the	O
same	O
idea	O
was	O
given	O
by	O
claude	O
shannon	O
in	O
and	O
that	O
year	O
also	O
saw	O
the	O
first	O
computational	O
learning	O
experiments	O
by	O
christopher	O
strachey	O
muggleton	O
after	O
steady	O
growth	O
ml	O
has	O
reached	O
practical	O
maturity	O
under	O
two	O
distinct	O
headings	O
as	O
a	O
means	O
of	O
engineering	O
rule-based	O
software	O
example	B
in	O
expert	B
systems	I
from	O
sample	O
cases	O
volunteered	O
interactively	O
and	O
as	O
a	O
method	O
of	O
data	O
analysis	O
whereby	O
rulestructured	O
classifiers	O
for	O
predicting	O
the	O
classes	B
of	O
newly	O
sampled	O
cases	O
are	O
obtained	O
from	O
a	O
training	B
set	I
of	O
pre-classified	O
cases	O
we	O
are	O
here	O
concerned	O
with	O
heading	O
exemplified	O
by	O
michalski	O
and	O
chilausky	O
s	O
landmark	O
use	O
of	O
the	O
algorithm	O
larson	O
to	O
generate	O
automatically	O
a	O
rule-based	O
classifier	B
for	O
crop	O
farmers	O
rules	O
for	O
classifying	O
soybean	B
diseases	O
were	O
inductively	O
derived	O
from	O
a	O
training	B
set	I
of	O
records	O
each	O
comprised	O
a	O
description	O
in	O
the	O
form	O
of	O
attribute-values	O
together	O
with	O
a	O
confirmed	O
allocation	O
to	O
one	O
or	O
another	O
of	O
main	O
soybean	B
diseases	O
when	O
used	O
to	O
canada	O
donald	O
michie	O
academic	O
research	O
associates	O
inveralmond	O
grove	O
edinburgh	O
addresses	O
for	O
correspondence	O
cao	O
feng	O
department	O
of	O
computer	O
science	O
university	O
of	O
ottowa	O
ottowa	O
u	O
k	O
this	O
chapter	O
confines	O
itself	O
to	O
a	O
subset	O
of	O
machine	O
learning	O
algorithms	O
i	O
e	O
those	O
that	O
output	B
propositional	O
classifiers	O
inductive	B
logic	I
programming	I
uses	O
the	O
symbol	O
system	O
of	O
predicate	O
opposed	O
to	O
propositional	O
logic	O
and	O
is	O
described	O
in	O
chapter	O
sec	O
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
classify	O
or	O
so	O
new	O
cases	O
machine-learned	O
rules	O
proved	O
to	O
be	O
markedly	O
more	O
accurate	O
than	O
the	O
best	O
existing	O
rules	O
used	O
by	O
soybean	B
experts	O
as	O
important	O
as	O
a	O
good	O
fit	O
to	O
the	O
data	O
is	O
a	O
property	O
that	O
can	O
be	O
termed	O
mental	B
fit	I
as	O
statisticians	O
breiman	O
and	O
colleagues	O
see	O
data-derived	O
classifications	O
as	O
serving	O
two	O
purposes	O
to	O
predict	O
the	O
response	O
variable	O
corresponding	O
to	O
future	O
measurement	O
vectors	O
as	O
accurately	O
as	O
possible	O
to	O
understand	O
the	O
structural	O
relationships	O
between	O
the	O
response	O
and	O
the	O
measured	O
variables	O
ml	O
takes	O
purpose	O
one	O
step	O
further	O
the	O
soybean	B
rules	O
were	O
sufficiently	O
meaningful	O
to	O
the	O
plant	O
pathologist	O
associated	O
with	O
the	O
project	O
that	O
he	O
eventually	O
adopted	O
them	O
in	O
place	O
of	O
his	O
own	O
previous	O
reference	O
set	O
ml	O
requires	O
that	O
classifiers	O
should	O
not	O
only	O
classify	O
but	O
should	O
also	O
constitute	O
explicit	O
concepts	O
that	O
is	O
expressions	O
in	O
symbolic	O
form	O
meaningful	O
to	O
humans	O
and	O
evaluable	O
in	O
the	O
head	O
we	O
need	O
to	O
dispose	O
of	O
confusion	O
between	O
the	O
kinds	O
of	O
computer-aided	O
descriptions	O
which	O
form	O
the	O
ml	O
practitioner	O
s	O
goal	O
and	O
those	O
in	O
view	O
by	O
statisticians	O
knowledgecompilations	O
meaningful	O
to	O
humans	O
and	O
evaluable	O
in	O
the	O
head	O
are	O
available	O
in	O
michalski	O
chilausky	O
s	O
paper	O
appendix	O
and	O
in	O
shapiro	O
michie	O
their	O
appendix	O
b	O
in	O
shapiro	O
his	O
appendix	O
a	O
and	O
in	O
bratko	O
mozetic	O
lavrac	O
their	O
appendix	O
a	O
among	O
other	O
sources	O
a	O
glance	O
at	O
any	O
of	O
these	O
computer-authored	O
constructions	O
will	O
suffice	O
to	O
show	O
their	O
remoteness	O
from	O
the	O
main-stream	O
of	O
statistics	O
and	O
its	O
goals	O
yet	O
ml	O
practitioners	O
increasingly	O
need	O
to	O
assimilate	O
and	O
use	O
statistical	B
techniques	O
once	O
they	O
are	O
ready	O
to	O
go	O
it	O
alone	O
machine	O
learned	O
bodies	O
of	O
knowledge	O
typically	O
need	O
little	O
further	O
human	O
intervention	O
but	O
a	O
substantial	O
synthesis	O
may	O
require	O
months	O
or	O
years	O
of	O
prior	O
interactive	O
work	O
first	O
to	O
shape	O
and	O
test	O
the	O
overall	O
logic	O
then	O
to	O
develop	O
suitable	O
sets	O
of	O
attributes	B
and	O
definitions	O
and	O
finally	O
to	O
select	O
or	O
synthesize	O
voluminous	O
data	O
files	O
as	O
training	O
material	O
this	O
contrast	O
has	O
engendered	O
confusion	O
as	O
to	O
the	O
role	O
of	O
human	O
interaction	O
like	O
music	O
teachers	O
ml	O
engineers	O
abstain	O
from	O
interaction	O
only	O
when	O
their	O
pupil	O
reaches	O
the	O
concert	O
hall	O
thereafter	O
abstention	O
is	O
total	O
clearing	O
the	O
way	O
for	O
new	O
forms	O
of	O
interaction	O
intrinsic	O
to	O
the	O
pupil	O
s	O
delivery	O
of	O
what	O
has	O
been	O
acquired	O
but	O
during	O
the	O
process	O
of	O
extracting	O
descriptions	O
from	O
data	O
the	O
working	O
method	O
of	O
ml	O
engineers	O
resemble	O
that	O
of	O
any	O
other	O
data	O
analyst	O
being	O
essentially	O
iterative	O
and	O
interactive	O
in	O
ml	O
the	O
knowledge	O
orientation	O
is	O
so	O
important	O
that	O
data-derived	O
classifiers	O
however	O
accurate	O
are	O
not	O
ordinarily	O
acceptable	O
in	O
the	O
absence	O
of	O
mental	B
fit	I
the	O
reader	O
should	O
bear	O
this	O
point	O
in	O
mind	O
when	O
evaluating	O
empirical	O
studies	O
reported	O
elsewhere	O
in	O
this	O
book	O
statlog	B
s	O
use	O
of	O
ml	O
algorithms	O
has	O
not	O
always	O
conformed	O
to	O
purpose	O
above	O
hence	O
the	O
reader	O
is	O
warned	O
that	O
the	O
book	O
s	O
use	O
of	O
the	O
phrase	O
machine	O
learning	O
in	O
such	O
contexts	O
is	O
by	O
courtesy	O
and	O
convenience	O
only	O
the	O
michalski-chilausky	O
soybean	B
experiment	O
exemplifies	O
supervised	B
learning	I
given	O
a	O
sample	O
of	O
input-output	O
pairs	O
of	O
an	O
unknown	O
class-membership	O
function	O
required	O
a	O
conjectured	O
reconstruction	O
of	O
the	O
function	O
in	O
the	O
form	O
of	O
a	O
rule-based	O
expression	O
human-evaluable	O
over	O
the	O
domain	O
note	O
that	O
the	O
function	O
s	O
output-set	O
is	O
unordered	O
consisting	O
of	O
categoric	O
rather	O
than	O
numerical	O
values	O
and	O
its	O
outputs	O
are	O
taken	O
to	O
be	O
names	O
of	O
classes	B
the	O
derived	O
functionexpression	O
is	O
then	O
a	O
classifier	B
in	O
contrast	O
to	O
the	O
prediction	O
of	O
numerical	O
quantities	O
this	O
book	O
confines	O
itself	O
to	O
the	O
classification	B
problem	O
and	O
follows	O
a	O
scheme	O
depicted	O
in	O
figure	O
constructing	O
ml-type	O
expressions	O
from	O
sample	O
data	O
is	O
known	O
as	O
concept	B
learning	I
machine	O
learning	O
of	O
rules	O
and	O
trees	O
t	O
r	O
a	O
i	O
n	O
i	O
n	O
g	O
d	O
a	O
t	O
a	O
l	O
e	O
a	O
r	O
n	O
i	O
n	O
g	O
a	O
l	O
g	O
o	O
r	O
i	O
t	O
h	O
m	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
r	O
u	O
l	O
e	O
s	O
t	O
e	O
s	O
t	O
i	O
n	O
g	O
d	O
a	O
t	O
a	O
fig	O
classification	B
process	O
from	O
training	O
to	O
testing	O
the	O
first	O
such	O
learner	O
was	O
described	O
by	O
earl	O
hunt	O
this	O
was	O
followed	O
by	O
hunt	O
marin	O
stone	O
s	O
cls	B
the	O
acronym	O
stands	O
for	O
concept	B
learning	I
system	I
in	O
ml	O
the	O
requirement	O
for	O
user-transparency	O
imparts	O
a	O
bias	B
towards	O
logical	O
in	O
preference	O
to	O
arithmetical	O
combinations	O
of	O
attributes	B
connectives	O
such	O
as	O
and	O
or	O
and	O
if-then	O
supply	O
the	O
glue	O
for	O
building	O
rule-structured	O
classifiers	O
as	O
in	O
the	O
following	O
englished	O
form	O
of	O
a	O
rule	O
from	O
michalski	O
and	O
chilausky	O
s	O
soybean	B
study	O
if	O
then	O
leaf	O
malformation	O
is	O
absent	O
and	O
stem	O
is	O
abnormal	O
and	O
internal	O
discoloration	O
is	O
black	O
diagnosis	O
is	O
charcoal	O
rot	O
example	B
cases	O
training	B
set	I
or	O
learning	O
sample	O
are	O
represented	O
as	O
vectors	O
of	O
attribute-values	O
paired	O
with	O
class	B
names	O
the	O
generic	O
problem	O
is	O
to	O
find	O
an	O
expression	O
that	O
predicts	O
the	O
classes	B
of	O
new	O
cases	O
test	B
set	I
taken	O
at	O
random	O
from	O
the	O
same	O
population	O
goodness	O
of	O
agreement	O
between	O
the	O
true	O
classes	B
and	O
the	O
classes	B
picked	O
by	O
the	O
classifier	B
is	O
then	O
used	O
to	O
measure	B
accuracy	B
an	O
underlying	O
assumption	O
is	O
that	O
either	O
training	O
and	O
test	O
sets	O
are	O
randomly	O
sampled	O
from	O
the	O
same	O
data	O
source	O
or	O
full	O
statistical	B
allowance	O
can	O
be	O
made	O
for	O
departures	O
from	O
such	O
a	O
regime	O
symbolic	B
learning	I
is	O
used	O
for	O
the	O
computer-based	O
construction	O
of	O
bodies	O
of	O
articulate	O
expertise	O
in	O
domains	O
which	O
lie	O
partly	O
at	O
least	O
beyond	O
the	O
introspective	O
reach	O
of	O
domain	O
experts	O
thus	O
the	O
above	O
rule	O
was	O
not	O
of	O
human	O
expert	O
authorship	O
although	O
an	O
expert	O
can	O
assimilate	O
it	O
and	O
pass	O
it	O
on	O
to	O
ascend	O
an	O
order	O
of	O
magnitude	O
in	O
scale	O
kardio	B
s	O
comprehensive	O
treatise	O
on	O
ecg	B
interpretation	O
et	O
al	O
does	O
not	O
contain	O
a	O
single	O
rule	O
of	O
human	O
authorship	O
above	O
the	O
level	O
of	O
primitive	O
descriptors	O
every	O
formulation	O
was	O
data-derived	O
and	O
every	O
data	O
item	O
was	O
generated	O
from	O
a	O
computable	O
logic	O
of	O
heartelectrocardiograph	O
interaction	O
independently	O
constructed	O
statistical	B
diagnosis	O
systems	O
are	O
commercially	O
available	O
in	O
computer-driven	O
ecg	B
kits	O
and	O
exhibit	O
accuracies	O
in	O
the	O
range	O
here	O
the	O
ml	O
product	O
scores	O
higher	O
being	O
subject	O
to	O
error	O
only	O
if	O
the	O
initial	O
logical	O
model	O
contained	O
flaws	O
none	O
have	O
yet	O
come	O
to	O
light	O
but	O
the	O
difference	O
that	O
illuminates	O
the	O
distinctive	O
nature	O
of	O
symbolic	B
ml	I
concerns	O
mental	B
fit	I
because	O
of	O
its	O
mode	O
of	O
construction	O
kardio	B
is	O
able	O
to	O
support	O
its	O
decisions	O
with	O
insight	O
into	O
causes	O
statistically	O
derived	O
systems	O
do	O
not	O
however	O
developments	O
of	O
bayesian	O
treatments	O
ini	O
sec	O
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
tiated	O
by	O
ml-leaning	O
statisticians	O
spiegelhalter	O
and	O
statistically	O
inclined	O
ml	O
theorists	O
pearl	O
may	O
change	O
this	O
although	O
marching	O
to	O
a	O
different	O
drum	O
ml	O
people	O
have	O
for	O
some	O
time	B
been	O
seen	O
as	O
a	O
possibly	O
useful	O
source	O
of	O
algorithms	O
for	O
certain	O
data-analyses	O
required	O
in	O
industry	O
there	O
are	O
two	O
broad	O
circumstances	O
that	O
might	O
favour	O
applicability	O
categorical	O
rather	O
than	O
numerical	O
attributes	B
strong	O
and	O
pervasive	O
conditional	O
dependencies	O
among	O
attributes	B
as	O
an	O
example	B
of	O
what	O
is	O
meant	O
by	O
a	O
conditional	B
dependency	I
let	O
us	O
take	O
the	O
classification	B
of	O
vertebrates	O
and	O
consider	O
two	O
variables	O
namely	O
breeding-ground	O
sea	O
freshwater	O
land	O
and	O
skin-covering	O
scales	O
feathers	O
hair	O
none	O
as	O
a	O
value	O
for	O
the	O
first	O
sea	O
votes	O
overwhelmingly	O
for	O
fish	O
if	O
the	O
second	O
attribute	O
has	O
the	O
value	O
none	O
then	O
on	O
its	O
own	O
this	O
would	O
virtually	O
clinch	O
the	O
case	O
for	O
amphibian	O
but	O
in	O
combination	O
with	O
breeding-ground	O
sea	O
it	O
switches	O
identification	O
decisively	O
to	O
mammal	O
whales	O
and	O
some	O
other	O
sea	O
mammals	O
now	O
remain	O
the	O
only	O
possibility	O
breeding-ground	O
and	O
skin-covering	O
are	O
said	O
to	O
exhibit	O
strong	O
conditional	B
dependency	I
problems	O
characterised	O
by	O
violent	O
attribute-interactions	O
of	O
this	O
kind	O
can	O
sometimes	O
be	O
important	O
in	O
industry	O
in	O
predicting	O
automobile	O
accident	O
risks	O
for	O
example	B
information	O
that	O
a	O
driver	O
is	O
in	O
the	O
agegroup	O
acquires	O
great	O
significance	O
if	O
and	O
only	O
if	O
sex	O
male	O
to	O
examine	O
the	O
horses	O
for	O
courses	O
aspect	O
of	O
comparisons	O
between	O
ml	O
neural-net	O
and	O
statistical	B
algorithms	O
a	O
reasonable	O
principle	O
might	O
be	O
to	O
select	O
datasets	O
approximately	O
evenly	O
among	O
four	O
main	O
categories	O
as	O
shown	O
in	O
figure	O
conditional	O
dependencies	O
strong	O
and	O
pervasive	O
weak	O
or	O
absent	O
attributes	B
all	O
or	O
mainly	O
categorical	O
all	O
or	O
mainly	O
numerical	O
key	O
ml	O
expected	O
to	O
do	O
well	O
ml	O
expected	O
to	O
do	O
well	O
marginally	O
ml	O
expected	O
to	O
do	O
poorly	O
marginally	O
fig	O
relative	O
performance	O
of	O
ml	O
algorithms	O
in	O
statlog	B
collection	O
of	O
datasets	O
necessarily	O
followed	O
opportunity	O
rather	O
than	O
design	O
so	O
that	O
for	O
light	O
upon	O
these	O
particular	O
contrasts	O
the	O
reader	O
will	O
find	O
much	O
that	O
is	O
suggestive	O
but	O
less	O
that	O
is	O
clear-cut	O
attention	O
is	O
however	O
called	O
to	O
the	O
appendices	O
which	O
contain	O
additional	O
information	O
for	O
readers	O
interested	O
in	O
following	O
up	O
particular	O
algorithms	O
and	O
datasets	O
for	O
themselves	O
classification	B
learning	O
is	O
characterised	O
by	O
the	O
data-description	O
language	O
the	O
language	O
for	O
expressing	O
the	O
classifier	B
i	O
e	O
as	O
formulae	O
rules	O
etc	O
and	O
the	O
learning	O
algorithm	O
itself	O
of	O
these	O
and	O
correspond	O
to	O
the	O
observation	B
language	I
and	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
hypothesis	B
language	I
respectively	O
of	O
section	O
under	O
we	O
consider	O
in	O
the	O
present	O
chapter	O
the	O
machine	O
learning	O
of	O
if-then	O
rule-sets	O
and	O
of	O
decision	B
trees	I
the	O
two	O
kinds	O
of	O
language	O
are	O
interconvertible	O
and	O
group	O
themselves	O
around	O
two	O
broad	O
inductive	O
inference	O
strategies	O
namely	O
specific-to-general	B
and	O
general-to-specific	B
specific-to-general	B
a	O
paradigm	O
for	O
rule-learning	B
michalski	O
s	O
and	O
related	O
algorithms	O
were	O
inspired	O
by	O
methods	O
used	O
by	O
electrical	O
engineers	O
for	O
simplifying	O
boolean	O
circuits	O
for	O
example	B
higonnet	O
grea	O
they	O
exemplify	O
the	O
specific-to-general	B
and	O
typically	O
start	O
with	O
a	O
maximally	O
specific	O
rule	O
for	O
assigning	O
cases	O
to	O
a	O
given	O
class	B
for	O
example	B
to	O
the	O
class	B
mammal	O
in	O
a	O
taxonomy	B
of	O
vertebrates	O
such	O
a	O
seed	O
as	O
the	O
starting	O
rule	O
is	O
called	O
specifies	O
a	O
value	O
for	O
every	O
member	O
of	O
the	O
set	O
of	O
attributes	B
characterizing	O
the	O
problem	O
for	O
example	B
rule	O
if	O
skin-covering	O
hair	O
breathing	O
lungs	O
tail	O
none	O
can-fly	O
y	O
reproduction	O
viviparous	O
legs	O
y	O
warm-blooded	O
y	O
diet	O
carnivorous	O
activity	O
nocturnal	O
then	O
mammal	O
we	O
now	O
take	O
the	O
reader	O
through	O
the	O
basics	O
of	O
specific-to-general	B
rule	O
learning	O
as	O
a	O
minimalist	O
tutorial	O
exercise	O
we	O
shall	O
build	O
a	O
mammal-recogniser	O
the	O
initial	O
rule	O
numbered	O
in	O
the	O
above	O
is	O
so	O
specific	O
as	O
probably	O
to	O
be	O
capable	O
only	O
of	O
recognising	O
bats	O
specificity	O
is	O
relaxed	O
by	O
dropping	O
attributes	B
one	O
at	O
a	O
time	B
thus	O
rule	O
rule	O
rule	O
rule	O
rule	O
if	O
breathing	O
lungs	O
tail	O
none	O
can-fly	O
y	O
reproduction	O
viviparous	O
legs	O
y	O
warm-blooded	O
y	O
diet	O
carnivorous	O
activity	O
nocturnal	O
then	O
mammal	O
if	O
skin-covering	O
hair	O
tail	O
none	O
can-fly	O
y	O
reproduction	O
viviparous	O
legs	O
y	O
warm-blooded	O
y	O
diet	O
carnivorous	O
activity	O
nocturnal	O
then	O
mammal	O
if	O
skin-covering	O
hair	O
breathing	O
lungs	O
can-fly	O
y	O
reproduction	O
viviparous	O
legs	O
y	O
warm-blooded	O
y	O
diet	O
carnivorous	O
activity	O
nocturnal	O
then	O
mammal	O
if	O
skin-covering	O
hair	O
breathing	O
lungs	O
tail	O
none	O
reproduction	O
viviparous	O
legs	O
y	O
warm-blooded	O
y	O
diet	O
carnivorous	O
activity	O
nocturnal	O
thenmammal	O
if	O
skin-covering	O
hair	O
breathing	O
lungs	O
tail	O
none	O
can-fly	O
y	O
legs	O
y	O
warm-blooded	O
y	O
diet	O
carnivorous	O
activity	O
nocturnal	O
bf	O
then	O
mammal	O
and	O
so	O
on	O
for	O
all	O
the	O
ways	O
of	O
dropping	O
a	O
single	O
attribute	O
followed	O
by	O
all	O
the	O
ways	O
of	O
dropping	O
two	O
attributes	B
three	O
attributes	B
etc	O
any	O
rule	O
which	O
includes	O
in	O
its	O
cover	B
a	O
negative	O
example	B
i	O
e	O
a	O
non-mammal	O
is	O
incorrect	O
and	O
is	O
discarded	O
during	O
the	O
process	O
the	O
cycle	O
terminates	O
by	O
saving	O
a	O
set	O
of	O
shortest	O
rules	O
covering	O
only	O
mammals	O
as	O
a	O
classifier	B
such	O
a	O
set	O
is	O
guaranteed	O
correct	O
but	O
cannot	O
be	O
guaranteed	O
complete	O
as	O
we	O
shall	O
see	O
later	O
sec	O
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
in	O
the	O
present	O
case	O
the	O
terminating	O
set	O
has	O
the	O
single-attribute	O
description	O
rule	O
if	O
skin-covering	O
hair	O
then	O
mammal	O
the	O
process	O
now	O
iterates	O
using	O
a	O
new	O
seed	O
for	O
each	O
iteration	O
for	O
example	B
rule	O
if	O
skin-covering	O
none	O
breathing	O
lungs	O
tail	O
none	O
can-fly	O
n	O
reproduction	O
viviparous	O
legs	O
n	O
warm-blooded	O
y	O
diet	O
mixed	O
activity	O
diurnal	O
then	O
mammal	O
leading	O
to	O
the	O
following	O
set	O
of	O
shortest	O
rules	O
rule	O
rule	O
rule	O
rule	O
if	O
skin-covering	O
none	O
reproduction	O
viviparous	O
then	O
mammal	O
if	O
skin-covering	O
none	O
warm-blooded	O
y	O
then	O
mammal	O
if	O
legs	O
n	O
warm-blooded	O
y	O
then	O
mammal	O
if	O
reproduction	O
viviparous	O
warm-blooded	O
y	O
then	O
mammal	O
of	O
these	O
the	O
first	O
covers	O
naked	O
mammals	O
amphibians	O
although	O
uniformly	O
naked	O
are	O
oviparous	O
the	O
second	O
has	O
the	O
same	O
cover	B
since	O
amphibians	O
are	O
not	O
warm-blooded	O
and	O
birds	O
although	O
warm-blooded	O
are	O
not	O
naked	O
assume	O
that	O
classification	B
is	O
done	O
on	O
adult	O
forms	O
the	O
third	O
covers	O
various	O
naked	O
marine	O
mammals	O
so	O
far	O
these	O
rules	O
collectively	O
contribute	O
little	O
information	O
merely	O
covering	O
a	O
few	O
overlapping	O
pieces	O
of	O
a	O
large	O
patchwork	O
but	O
the	O
last	O
rule	O
at	O
a	O
stroke	O
covers	O
almost	O
the	O
whole	O
class	B
of	O
mammals	O
every	O
attempt	O
at	O
further	O
generalisation	O
now	O
encounters	O
negative	O
examples	O
dropping	O
warm-blooded	O
causes	O
the	O
rule	O
to	O
cover	B
viviparous	O
groups	O
of	O
fish	O
and	O
of	O
reptiles	O
dropping	O
viviparous	O
causes	O
the	O
rule	O
to	O
cover	B
birds	O
unacceptable	O
in	O
a	O
mammal-recogniser	O
but	O
it	O
also	O
has	O
the	O
effect	O
of	O
including	O
the	O
egg-laying	O
mammals	O
monotremes	O
consisting	O
of	O
the	O
duck-billed	O
platypus	O
and	O
two	O
species	O
of	O
spiny	O
ant-eaters	O
rule	O
fails	O
to	O
cover	B
these	O
and	O
is	O
thus	O
an	O
instance	O
of	O
the	O
earlier-mentioned	O
kind	O
of	O
classifier	B
that	O
can	O
be	O
guaranteed	O
correct	O
but	O
cannot	O
be	O
guaranteed	O
complete	O
conversion	O
into	O
a	O
complete	O
and	O
correct	O
classifier	B
is	O
not	O
an	O
option	O
for	O
this	O
purely	O
specific-to-general	B
process	O
since	O
we	O
have	O
run	O
out	O
of	O
permissible	O
generalisations	O
the	O
construction	O
of	O
rule	O
has	O
thus	O
stalled	O
in	O
sight	O
of	O
the	O
finishing	O
line	O
but	O
linking	O
two	O
or	O
more	O
rules	O
together	O
each	O
correct	O
but	O
not	O
complete	O
can	O
effect	O
the	O
desired	O
result	O
below	O
we	O
combine	O
the	O
rule	O
yielded	O
by	O
the	O
first	O
iteration	O
with	O
in	O
turn	O
the	O
first	O
and	O
the	O
second	O
rule	O
obtained	O
from	O
the	O
second	O
iteration	O
rule	O
rule	O
rule	O
rule	O
if	O
skin-covering	O
hair	O
then	O
mammal	O
if	O
skin-covering	O
none	O
reproduction	O
viviparous	O
then	O
mammal	O
if	O
skin-covering	O
hair	O
then	O
mammal	O
if	O
skin-covering	O
none	O
warm-blooded	O
y	O
then	O
mammal	O
these	O
can	O
equivalently	O
be	O
written	O
as	O
disjunctive	O
rules	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
if	O
or	O
then	O
and	O
if	O
or	O
then	O
skin-covering	O
hair	O
skin-covering	O
none	O
reproduction	O
viviparous	O
mammal	O
skin-covering	O
hair	O
skin-covering	O
none	O
warm-blooded	O
y	O
mammal	O
in	O
rule	O
induction	O
following	O
michalskian	O
attribute-test	O
is	O
called	O
a	O
selector	B
a	O
conjunction	O
of	O
selectors	O
is	O
a	O
complex	B
and	O
a	O
disjunction	O
of	O
complexes	O
is	O
called	O
a	O
cover	B
if	O
a	O
rule	O
is	O
true	O
of	O
an	O
example	B
we	O
say	O
that	O
it	O
covers	O
the	O
example	B
rule	O
learning	O
systems	O
in	O
practical	O
use	O
qualify	O
and	O
elaborate	O
the	O
above	O
simple	O
scheme	O
including	O
by	O
assigning	O
a	O
prominent	O
role	O
to	O
general-to-specific	B
processes	O
in	O
the	O
statlog	B
experiment	O
such	O
algorithms	O
are	O
exemplified	O
by	O
niblett	O
and	O
itrule	B
both	O
generate	O
decision	O
rules	O
for	O
each	O
class	B
in	O
turn	O
for	O
each	O
class	B
starting	O
with	O
a	O
universal	O
rule	O
which	O
assigns	O
all	O
examples	O
to	O
the	O
current	O
class	B
this	O
rule	O
ought	O
to	O
cover	B
at	O
least	O
one	O
of	O
the	O
examples	O
belonging	O
to	O
that	O
class	B
specialisations	O
are	O
then	O
repeatedly	O
generated	O
and	O
explored	O
until	O
all	O
rules	O
consistent	O
with	O
the	O
data	O
are	O
found	O
each	O
rule	O
must	O
correctly	O
classify	O
at	O
least	O
a	O
prespecified	O
percentage	O
of	O
the	O
examples	O
belonging	O
to	O
the	O
current	O
class	B
as	O
few	O
as	O
possible	O
negative	O
examples	O
i	O
e	O
examples	O
in	O
other	O
classes	B
should	O
be	O
covered	O
specialisations	O
are	O
obtained	O
by	O
adding	O
a	O
condition	O
to	O
the	O
left-hand	O
side	O
of	O
the	O
rule	O
is	O
an	O
extension	O
of	O
michalski	O
s	O
algorithm	O
aq	B
with	O
several	O
techniques	O
to	O
is	O
the	O
number	O
classified	O
incorrectly	O
and	O
c	O
is	O
the	O
total	O
number	O
of	O
classes	B
process	O
noise	B
in	O
the	O
data	O
the	O
main	O
technique	O
for	O
reducing	O
error	O
is	O
to	O
minimise	O
cde	O
fdgih	O
g	O
function	O
where	O
k	O
is	O
the	O
number	O
of	O
examples	O
classified	O
correctly	O
cdjelkenm	O
by	O
a	O
rulek	O
itrule	B
produces	O
rules	O
of	O
the	O
form	O
if	O
then	O
with	O
probability	O
this	O
algorithm	O
contains	O
probabilistic	B
inference	I
through	O
the	O
j-measure	B
which	O
evaluates	O
its	O
candidate	O
rules	O
j-measure	B
is	O
a	O
product	O
of	O
prior	B
probabilities	I
for	O
each	O
class	B
and	O
the	O
cross-entropy	B
of	O
class	B
values	O
conditional	O
on	O
the	O
attribute	O
values	O
itrule	B
cannot	O
deal	O
with	O
continuous	O
numeric	O
values	O
it	O
needs	O
accurate	O
evaluation	O
of	O
prior	O
and	O
posterior	O
probabilities	O
so	O
when	O
such	O
information	O
is	O
not	O
present	O
it	O
is	O
prone	O
to	O
misuse	O
detailed	O
accounts	O
of	O
these	O
and	O
other	O
algorithms	O
are	O
given	O
in	O
section	O
decision	B
trees	I
reformulation	O
of	O
the	O
mammal-recogniser	O
as	O
a	O
completed	O
decision	O
tree	O
would	O
require	O
the	O
implicit	O
else	O
not-mammal	O
to	O
be	O
made	O
explicit	O
as	O
in	O
figure	O
construction	O
of	O
the	O
complete	O
outline	O
taxonomy	B
as	O
a	O
set	O
of	O
descriptive	O
concepts	O
whether	O
in	O
rule-structured	O
or	O
tree-structured	O
form	O
would	O
entail	O
repetition	O
of	O
the	O
induction	O
process	O
for	O
bird	O
reptile	O
amphibian	O
and	O
fish	O
in	O
order	O
to	O
be	O
meaningful	O
to	O
the	O
user	O
to	O
satisfy	O
the	O
mental	B
fit	I
criterion	O
it	O
has	O
been	O
found	O
empirically	O
that	O
trees	O
should	O
be	O
as	O
small	O
and	O
as	O
linear	O
as	O
possible	O
in	O
fully	O
linear	B
trees	I
such	O
as	O
that	O
of	O
figure	O
an	O
internal	O
node	O
attribute	O
test	O
can	O
be	O
the	O
parent	O
of	O
at	O
most	O
one	O
internal	O
node	O
all	O
its	O
other	O
children	O
must	O
be	O
end-node	O
or	O
leaves	O
quantitative	O
measures	B
of	O
linearity	O
are	O
discussed	O
by	O
arbab	O
michie	O
who	O
present	O
an	O
algorithm	O
rg	B
for	O
building	O
trees	O
biased	O
towards	O
linearity	O
they	O
also	O
compare	O
rg	B
with	O
bratko	O
s	O
aocdl	B
directed	O
towards	O
the	O
same	O
end	O
we	O
now	O
consider	O
the	O
general	O
sec	O
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
skin-covering	O
none	O
hair	O
scales	O
feathers	O
mammal	O
not-mammal	O
not-mammal	O
viviparous	O
no	O
yes	O
not-mammal	O
mammal	O
qu	O
fig	O
translation	O
of	O
a	O
mammal-recognising	O
rule	O
see	O
text	O
into	O
tree	O
form	O
the	O
attribute-values	O
that	O
figured	O
in	O
the	O
rule-sets	O
built	O
earlier	O
are	O
here	O
set	O
larger	O
in	O
bold	O
type	O
the	O
rest	O
are	O
tagged	O
with	O
not-mammal	O
labels	O
properties	O
of	O
algorithms	O
that	O
grow	O
trees	O
from	O
data	O
general-to-specific	B
top-down	O
induction	O
of	O
trees	O
in	O
common	O
with	O
and	O
itrule	B
but	O
in	O
contrast	O
to	O
the	O
specific-to-general	B
earlier	O
style	O
of	O
michalski	O
s	O
aq	B
family	O
of	O
rule	O
learning	O
decision-tree	O
learning	O
is	O
general-to-specific	B
in	O
illustrating	O
with	O
the	O
vertebrate	B
taxonomy	B
example	B
we	O
will	O
assume	O
that	O
the	O
set	O
of	O
nine	O
attributes	B
are	O
sufficient	O
to	O
classify	O
without	O
error	O
all	O
vertebrate	B
species	I
into	O
one	O
of	O
mammal	O
bird	O
amphibian	O
reptile	O
fish	O
later	O
we	O
will	O
consider	O
elaborations	O
necessary	O
in	O
underspecified	O
or	O
in	O
inherently	O
noisy	B
domains	O
where	O
methods	O
from	O
statistical	B
data	O
analysis	O
enter	O
the	O
picture	O
as	O
shown	O
in	O
figure	O
the	O
starting	O
point	O
is	O
a	O
tree	O
of	O
only	O
one	O
node	O
that	O
allocates	O
all	O
cases	O
in	O
the	O
training	B
set	I
to	O
a	O
single	O
class	B
in	O
the	O
case	O
that	O
a	O
mammal-recogniser	O
is	O
required	O
this	O
default	B
class	B
could	O
be	O
not-mammal	O
the	O
presumption	O
here	O
is	O
that	O
in	O
the	O
population	O
there	O
are	O
more	O
of	O
these	O
than	O
there	O
are	O
mammals	O
unless	O
all	O
vertebrates	O
in	O
the	O
training	B
set	I
are	O
non-mammals	O
some	O
of	O
the	O
training	B
set	I
of	O
cases	O
associated	O
with	O
this	O
single	O
node	O
will	O
be	O
correctly	O
classified	O
and	O
others	O
incorrectly	O
in	O
the	O
terminology	O
of	O
breiman	O
and	O
colleagues	O
such	O
a	O
node	O
is	O
impure	B
each	O
available	O
attribute	O
is	O
now	O
used	O
on	O
a	O
trial	O
basis	O
to	O
split	O
the	O
set	O
into	O
subsets	O
whichever	O
split	O
minimises	O
the	O
estimated	O
impurity	B
of	O
the	O
subsets	O
which	O
it	O
generates	O
is	O
retained	O
and	O
the	O
cycle	O
is	O
repeated	O
on	O
each	O
of	O
the	O
augmented	O
tree	O
s	O
end-nodes	O
numerical	O
measures	B
of	O
impurity	B
are	O
many	O
and	O
various	O
they	O
all	O
aim	O
to	O
capture	O
the	O
degree	O
to	O
which	O
expected	O
frequencies	O
of	O
belonging	O
to	O
given	O
classes	B
estimated	O
for	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
p	O
p	O
p	O
p	O
p	O
p	O
q	O
q	O
q	O
r	O
r	O
r	O
r	O
r	O
r	O
s	O
s	O
s	O
s	O
t	O
t	O
t	O
q	O
q	O
q	O
s	O
q	O
q	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
example	B
in	O
the	O
two-class	O
mammalnot-mammal	O
problem	O
of	O
figure	O
asv	O
are	O
affected	O
by	O
knowledge	O
of	O
attribute	O
values	O
in	O
general	O
the	O
goodness	O
of	O
a	O
split	O
into	O
subsets	O
example	B
by	O
skin-covering	O
by	O
breathing	O
organs	O
by	O
tail-type	O
etc	O
is	O
the	O
weighted	O
mean	O
decrease	O
in	O
impurity	B
weights	O
being	O
proportional	O
to	O
the	O
subset	O
sizes	O
let	O
us	O
see	O
how	O
these	O
ideas	O
work	O
out	O
in	O
a	O
specimen	O
development	O
of	O
a	O
mammal-recognising	O
tree	O
to	O
facilitate	O
comparison	O
with	O
the	O
specific-to-general	B
induction	O
shown	O
earlier	O
the	O
tree	O
is	O
represented	O
in	O
figure	O
as	O
an	O
if-then-else	O
expression	O
we	O
underline	O
class	B
names	O
that	O
label	O
temporary	O
leaves	O
these	O
are	O
nodes	O
that	O
need	O
further	O
splitting	O
to	O
remove	O
or	O
diminish	O
impurity	B
cvwexvy	O
this	O
simple	O
taxonomic	B
example	B
lacks	O
many	O
of	O
the	O
complicating	O
factors	O
encountered	O
in	O
classification	B
generally	O
and	O
lends	O
itself	O
to	O
this	O
simplest	O
form	O
of	O
decision	O
tree	O
learning	O
complications	O
arise	O
from	O
the	O
use	O
of	O
numerical	O
attributes	B
in	O
addition	O
to	O
categorical	O
from	O
the	O
occurrence	O
of	O
error	O
and	O
from	O
the	O
occurrence	O
of	O
unequal	O
misclassification	B
costs	B
error	O
can	O
inhere	O
in	O
the	O
values	O
of	O
attributes	B
or	O
classes	B
noise	B
or	O
the	O
domain	O
may	O
be	O
deterministic	O
yet	O
the	O
supplied	O
set	O
of	O
attributes	B
may	O
not	O
support	O
error-free	O
classification	B
but	O
to	O
round	O
off	O
the	O
taxonomy	B
example	B
the	O
following	O
from	O
quinlan	O
gives	O
the	O
simple	O
essence	O
of	O
tree	O
learning	O
contains	O
no	O
cases	O
contains	O
cases	O
that	O
belong	O
to	O
a	O
mixture	O
of	O
classes	B
the	O
decision	O
tree	O
is	O
again	O
a	O
leaf	O
but	O
the	O
class	B
to	O
be	O
associated	O
with	O
the	O
leaf	O
for	O
example	B
the	O
leaf	O
might	O
be	O
chosen	O
in	O
accordance	O
with	O
some	O
background	B
knowledge	I
of	O
the	O
domain	O
such	O
as	O
the	O
overall	O
majority	O
class	B
to	O
construct	O
a	O
decision	O
tree	O
from	O
a	O
setz	O
of	O
training	O
cases	O
let	O
the	O
classes	B
be	O
denoted	O
d	O
e	O
there	O
are	O
three	O
possibilities	O
contains	O
one	O
or	O
more	O
cases	O
all	O
belonging	O
to	O
a	O
single	O
classb	O
is	O
a	O
leaf	O
identifying	O
class	B
the	O
decision	O
tree	O
forz	O
must	O
be	O
determined	O
from	O
information	O
other	O
thanz	O
in	O
this	O
situation	O
the	O
idea	O
is	O
to	O
refinez	O
d	O
d	O
i	O
outcomes	O
wherez	O
contains	O
all	O
the	O
cases	O
inz	O
the	O
decision	O
tree	O
forz	O
leads	O
to	O
the	O
decision	O
tree	O
constructed	O
from	O
the	O
subsetz	O
into	O
subsets	O
of	O
cases	O
that	O
are	O
or	O
seem	O
to	O
be	O
heading	O
towards	O
single-class	O
collections	O
of	O
cases	O
a	O
test	O
is	O
chosen	O
based	O
on	O
a	O
single	O
attribute	O
that	O
has	O
two	O
or	O
more	O
mutually	O
exclusive	O
that	O
have	O
outcome	O
oi	O
of	O
the	O
chosen	O
test	O
consists	O
of	O
a	O
decision	O
node	O
identifying	O
the	O
test	O
and	O
one	O
branch	O
for	O
each	O
possible	O
outcome	O
the	O
same	O
tree-building	O
machinery	O
is	O
applied	O
recursively	O
to	O
each	O
subset	O
of	O
training	O
cases	O
so	O
that	O
the	O
ith	O
branch	O
d	O
d	O
z	O
of	O
training	O
cases	O
is	O
partitioned	O
into	O
subsetsz	O
note	O
that	O
this	O
schema	O
is	O
general	O
enough	O
to	O
include	O
multi-class	B
trees	I
raising	O
a	O
tactical	O
problem	O
in	O
approaching	O
the	O
taxonomic	B
material	O
should	O
we	O
build	O
in	O
turn	O
a	O
set	O
of	O
yesno	O
recognizers	O
one	O
for	O
mammals	O
one	O
for	O
birds	O
one	O
for	O
reptiles	O
etc	O
and	O
then	O
daisy-chain	O
them	O
into	O
a	O
tree	O
or	O
should	O
we	O
apply	O
the	O
full	O
multi-class	O
procedure	O
to	O
the	O
data	O
wholesale	O
risking	O
a	O
disorderly	O
scattering	O
of	O
different	O
class	B
labels	O
along	O
the	O
resulting	O
tree	O
s	O
perimeter	O
if	O
the	O
entire	O
tree-building	O
process	O
is	O
automated	O
as	O
for	O
the	O
later	O
standardised	O
comparisons	O
the	O
second	O
regime	O
is	O
mandatory	O
but	O
in	O
interactive	O
decision-tree	O
building	O
there	O
is	O
no	O
generally	O
correct	O
answer	O
the	O
analyst	O
must	O
be	O
guided	O
by	O
context	O
by	O
user-requirements	O
and	O
by	O
intermediate	O
results	O
h	O
z	O
z	O
z	O
z	O
z	O
sec	O
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
empty	O
attribute-test	O
not-mammal	O
if	O
no	O
misclassifications	O
confirm	O
leaf	O
lines	O
if	O
misclassifications	O
occur	O
choose	O
an	O
attribute	O
for	O
splitting	O
the	O
set	O
for	O
each	O
calculate	O
a	O
purity	B
measure	B
from	O
the	O
tabulations	O
below	O
empty	O
attribute-test	O
not-mammal	O
and	O
exit	O
skin-covering	O
feathers	O
none	O
hair	O
scales	O
total	O
number	O
of	O
mammals	O
in	O
set	O
number	O
of	O
not-mammals	O
fe	O
yfe	O
sc	O
no	O
ha	O
yno	O
yha	O
ysc	O
breathing	O
number	O
of	O
mammals	O
in	O
subset	O
number	O
of	O
not-mammals	O
lungs	O
lu	O
ylu	O
tail	O
gills	O
gi	O
ygi	O
long	O
short	O
none	O
number	O
of	O
mammals	O
in	O
set	O
number	O
of	O
not-mammals	O
lo	O
ylo	O
no	O
yno	O
and	O
so	O
on	O
sh	O
ysh	O
vy	O
vy	O
vy	O
fig	O
first	O
stage	O
in	O
growing	O
a	O
decision	O
tree	O
from	O
a	O
training	B
set	I
the	O
single	O
end-node	O
is	O
a	O
candidate	O
to	O
be	O
a	O
leaf	O
and	O
is	O
here	O
drawn	O
with	O
broken	O
lines	O
it	O
classifies	O
all	O
cases	O
to	O
not-mammal	O
if	O
correctly	O
the	O
candidate	O
is	O
confirmed	O
as	O
a	O
leaf	O
otherwise	O
available	O
attribute-applications	O
are	O
tried	O
for	O
their	O
abilities	O
to	O
split	O
the	O
set	O
saving	O
for	O
incorporation	O
into	O
the	O
tree	O
whichever	O
maximises	O
some	O
chosen	O
purity	B
measure	B
each	O
saved	O
subset	O
now	O
serves	O
as	O
a	O
candidate	O
for	O
recursive	O
application	O
of	O
the	O
same	O
split-and-test	O
cycle	O
s	O
s	O
s	O
v	O
v	O
v	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
step	O
construct	O
a	O
single-leaf	O
tree	O
rooted	O
in	O
the	O
empty	O
attribute	O
test	O
if	O
then	O
not-mammal	O
if	O
no	O
impure	B
nodes	O
then	O
exit	O
step	O
construct	O
from	O
the	O
training	B
set	I
all	O
single-attribute	O
trees	O
and	O
for	O
each	O
calculate	O
the	O
weighted	O
mean	O
impurity	B
over	O
its	O
leaves	O
step	O
retain	O
the	O
attribute	O
giving	O
least	O
impurity	B
assume	O
this	O
to	O
be	O
skin-covering	O
if	O
hair	O
then	O
mammal	O
if	O
feathers	O
then	O
not-mammal	O
if	O
scales	O
then	O
not-mammal	O
if	O
none	O
then	O
not-mammal	O
step	O
if	O
no	O
impure	B
nodes	O
then	O
exit	O
otherwise	O
apply	O
steps	O
and	O
and	O
recursively	O
to	O
each	O
impure	B
node	I
thus	O
step	O
construct	O
from	O
the	O
not-mammal	O
subset	O
of	O
step	O
all	O
single-attribute	O
trees	O
and	O
for	O
each	O
calculate	O
the	O
weighted	O
mean	O
impurity	B
over	O
its	O
leaves	O
step	O
retain	O
the	O
attribute	O
giving	O
least	O
impurity	B
perfect	O
scores	O
are	O
achieved	O
by	O
viviparous	O
and	O
by	O
warm-blooded	O
giving	O
and	O
if	O
hair	O
then	O
mammal	O
if	O
feathers	O
then	O
not-mammal	O
if	O
scales	O
then	O
not-mammal	O
if	O
none	O
then	O
if	O
viviparous	O
then	O
mammal	O
else	O
not-mammal	O
if	O
hair	O
then	O
mammal	O
if	O
feathers	O
then	O
not-mammal	O
if	O
scales	O
then	O
not-mammal	O
if	O
none	O
then	O
if	O
y	O
then	O
mammal	O
else	O
not-mammal	O
step	O
exit	O
fig	O
illustration	O
using	O
the	O
mammal	O
problem	O
of	O
the	O
basic	O
idea	O
of	O
decision-tree	O
induction	O
sec	O
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
either	O
way	O
the	O
crux	O
is	O
the	O
idea	O
of	O
refining	O
t	O
into	O
subsets	O
of	O
cases	O
that	O
are	O
or	O
seem	O
to	O
be	O
heading	O
towards	O
single-class	O
collections	O
of	O
cases	O
this	O
is	O
the	O
same	O
as	O
the	O
earlier	O
described	O
search	O
for	O
purity	B
departure	O
from	O
purity	B
is	O
used	O
as	O
the	O
splitting	B
criterion	I
i	O
e	O
as	O
the	O
basis	O
on	O
which	O
to	O
select	O
an	O
attribute	O
to	O
apply	O
to	O
the	O
members	O
of	O
a	O
less	O
pure	O
node	O
for	O
partitioning	O
it	O
into	O
purer	O
sub-nodes	O
but	O
how	O
to	O
measure	B
departure	O
from	O
purity	B
in	O
practice	O
as	O
noted	O
by	O
breiman	O
et	O
al	O
overall	O
misclassification	O
rate	O
is	O
not	O
sensitive	O
to	O
the	O
choice	O
of	O
a	O
splitting	O
rule	O
as	O
long	O
as	O
it	O
is	O
within	O
a	O
reasonable	O
class	B
of	O
rules	O
for	O
a	O
more	O
general	O
consideration	O
of	O
splitting	B
criteria	I
we	O
first	O
introduce	O
the	O
case	O
where	O
total	O
purity	B
of	O
nodes	O
is	O
not	O
attainable	O
i	O
e	O
some	O
or	O
all	O
of	O
the	O
leaves	O
necessarily	O
end	O
up	O
mixed	O
with	O
respect	O
to	O
class	B
membership	O
in	O
these	O
circumstances	O
the	O
term	O
noisy	B
data	I
is	O
often	O
applied	O
but	O
we	O
must	O
remember	O
that	O
noise	B
irreducible	O
measurement	O
error	O
merely	O
characterises	O
one	O
particular	O
form	O
of	O
inadequate	O
information	O
imagine	O
the	O
multi-class	O
taxonomy	B
problem	O
under	O
the	O
condition	O
that	O
skin-covering	O
tail	O
and	O
viviparous	O
are	O
omitted	O
from	O
the	O
attribute	O
set	O
owls	O
and	O
bats	O
for	O
example	B
cannot	O
now	O
be	O
discriminated	O
stopping	O
rules	O
based	O
on	O
complete	O
purity	B
have	O
then	O
to	O
be	O
replaced	O
by	O
something	O
less	O
stringent	O
stopping	O
rules	O
and	O
class	B
probability	I
trees	I
one	O
method	O
not	O
necessarily	O
recommended	O
is	O
to	O
stop	O
when	O
the	O
purity	B
measure	B
exceeds	O
some	O
threshold	O
the	O
trees	O
that	O
result	O
are	O
no	O
longer	O
strictly	O
decision	B
trees	I
for	O
brevity	O
we	O
continue	O
to	O
use	O
this	O
generic	O
term	O
since	O
a	O
leaf	O
is	O
no	O
longer	O
guaranteed	O
to	O
contain	O
a	O
single-class	O
collection	O
but	O
instead	O
a	O
frequency	O
distribution	O
over	O
classes	B
such	O
trees	O
are	O
known	O
as	O
class	B
probability	I
trees	I
conversion	O
into	O
classifiers	O
requires	O
a	O
separate	O
mapping	O
from	O
distributions	O
to	O
class	B
labels	O
one	O
popular	O
but	O
simplistic	O
procedure	O
says	O
pick	O
the	O
candidate	O
with	O
the	O
most	O
votes	O
whether	O
or	O
not	O
such	O
a	O
plurality	O
rule	O
makes	O
sense	O
depends	O
in	O
each	O
case	O
on	O
the	O
distribution	O
over	O
the	O
classes	B
in	O
the	O
population	O
from	O
which	O
the	O
training	B
set	I
was	O
drawn	O
i	O
e	O
on	O
the	O
priors	O
and	O
differential	O
misclassification	B
costs	B
consider	O
two	O
errors	O
classifying	O
the	O
shuttle	B
main	O
engine	O
as	O
ok	O
to	O
fly	O
when	O
it	O
is	O
not	O
and	O
classifying	O
it	O
as	O
not	O
ok	O
when	O
it	O
is	O
obviously	O
the	O
two	O
costs	B
are	O
unequal	O
use	O
of	O
purity	B
measures	B
for	O
stopping	O
sometimes	O
called	O
forward	B
pruning	B
has	O
had	O
mixed	O
results	O
the	O
authors	O
of	O
two	O
of	O
the	O
leading	O
decision	O
tree	O
algorithms	O
cart	B
et	O
al	O
and	O
independently	O
arrived	O
at	O
the	O
opposite	O
philosophy	O
summarised	O
by	O
breiman	O
and	O
colleagues	O
as	O
prune	O
instead	O
of	O
stopping	O
grow	O
a	O
tree	O
that	O
is	O
much	O
too	O
large	O
and	O
prune	O
it	O
upward	O
this	O
is	O
sometimes	O
called	O
backward	B
pruning	B
these	O
authors	O
definition	O
of	O
much	O
too	O
large	O
requires	O
that	O
we	O
continue	O
splitting	O
until	O
each	O
terminal	O
node	O
either	O
or	O
or	O
is	O
pure	O
contains	O
only	O
identical	O
attribute-vectorsin	O
which	O
case	O
splitting	O
is	O
impossible	O
has	O
fewer	O
than	O
a	O
pre-specified	O
number	O
of	O
distinct	O
attribute-vectors	O
approaches	O
to	O
the	O
backward	B
pruning	B
of	O
these	O
much	O
too	O
large	O
trees	O
form	O
the	O
topic	O
of	O
a	O
later	O
section	O
we	O
first	O
return	O
to	O
the	O
concept	B
of	O
a	O
node	O
s	O
purity	B
in	O
the	O
context	O
of	O
selecting	O
one	O
attribute	O
in	O
preference	O
to	O
another	O
for	O
splitting	O
a	O
given	O
node	O
splitting	B
criteria	I
readers	O
accustomed	O
to	O
working	O
with	O
categorical	O
data	O
will	O
recognise	O
in	O
figure	O
crosstabulations	O
reminiscent	O
of	O
the	O
contingency	O
tables	O
of	O
statistics	O
for	O
example	B
it	O
only	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
requires	O
completion	O
of	O
the	O
column	O
totals	O
of	O
the	O
second	O
tabulation	O
to	O
create	O
the	O
standard	O
input	B
to	O
a	O
two-by-two	O
by	O
applying	O
a	O
the	O
hypothesis	O
under	O
test	O
is	O
that	O
the	O
distribution	O
of	O
cases	O
between	O
mammals	O
and	O
not-mammals	O
is	O
independent	O
of	O
the	O
distribution	O
between	O
the	O
two	O
breathing	O
modes	O
a	O
possible	O
rule	O
says	O
that	O
the	O
smaller	O
the	O
probability	O
obtained	O
test	O
to	O
this	O
hypothesis	O
then	O
the	O
stronger	O
the	O
splitting	O
credentials	O
of	O
the	O
attribute	O
breathing	O
turning	O
to	O
the	O
construction	O
of	O
multi-class	B
trees	I
rather	O
than	O
yesno	O
concept-recognisers	B
an	O
adequate	O
number	O
of	O
fishes	O
in	O
the	O
training	O
sample	O
would	O
under	O
almost	O
any	O
purity	B
criterion	O
ensure	O
early	O
selection	O
of	O
breathing	O
similarly	O
given	O
adequate	O
representation	O
of	O
reptiles	O
taillong	O
would	O
score	O
highly	O
since	O
lizards	O
and	O
snakes	O
account	O
for	O
of	O
living	O
reptiles	O
the	O
corresponding	O
x	O
contingency	O
table	O
would	O
have	O
the	O
form	O
feg	O
long	O
tail	O
short	O
none	O
totals	O
table	O
cross-tabulation	O
of	O
classes	B
and	O
tail	O
attribute-values	O
given	O
in	O
table	O
on	O
the	O
hypothesis	O
of	O
no	O
association	O
the	O
expected	O
numbers	O
in	O
the	O
j	O
where	O
longh	O
cells	O
can	O
be	O
got	O
from	O
the	O
marginal	O
totals	O
thus	O
expected	O
expectedg	O
h	O
expected	O
is	O
distributed	O
as	O
is	O
the	O
total	O
in	O
the	O
training	B
set	I
then	O
c	O
observed	O
with	O
degrees	O
of	O
freedom	O
equal	O
tocb	O
fdg	O
i	O
e	O
in	O
this	O
case	O
c	O
number	O
in	O
mammal	O
number	O
in	O
bird	O
number	O
in	O
reptile	O
number	O
in	O
amphibian	O
number	O
in	O
fish	O
total	O
suppose	O
however	O
that	O
the	O
tail	O
variable	O
were	O
not	O
presented	O
in	O
the	O
form	O
of	O
a	O
categorical	O
attribute	O
with	O
three	O
unordered	O
values	O
but	O
rather	O
as	O
a	O
number	O
as	O
the	O
ratio	O
for	O
example	B
of	O
the	O
length	O
of	O
the	O
tail	O
to	O
that	O
of	O
the	O
combined	O
body	O
and	O
head	O
sometimes	O
the	O
first	O
step	O
is	O
to	O
apply	O
some	O
form	O
of	O
clustering	B
method	O
or	O
other	O
approximation	O
but	O
virtually	O
every	O
algorithm	O
then	O
selects	O
from	O
all	O
the	O
dichotomous	O
segmentations	O
of	O
the	O
numerical	O
scale	O
meaningful	O
for	O
a	O
given	O
node	O
that	O
segmentation	O
that	O
maximises	O
the	O
chosen	O
purity	B
measure	B
over	O
classes	B
k	O
long	O
short	O
none	O
with	O
suitable	O
refinements	O
the	O
chaid	B
decision-tree	O
algorithm	O
automatic	O
interaction	O
detection	O
uses	O
a	O
splitting	B
criterion	I
such	O
as	O
that	O
illustrated	O
with	O
the	O
foregoing	O
contingency	O
table	O
although	O
not	O
included	O
in	O
the	O
present	O
trials	O
chaid	B
enjoys	O
widespread	O
commercial	O
availability	O
through	O
its	O
inclusion	O
as	O
an	O
optional	O
module	O
in	O
the	O
spss	O
statistical	B
analysis	O
package	O
other	O
approaches	O
to	O
such	O
tabulations	O
as	O
the	O
above	O
use	O
information	B
theory	I
we	O
then	O
enquire	O
what	O
is	O
the	O
expected	O
gain	O
in	O
information	O
about	O
a	O
case	O
s	O
row-membership	O
from	O
knowledge	O
of	O
its	O
column-membership	O
methods	O
and	O
difficulties	O
are	O
discussed	O
by	O
quinlan	O
the	O
reader	O
is	O
also	O
referred	O
to	O
the	O
discussion	O
in	O
section	O
with	O
particular	O
reference	O
to	O
mutual	B
information	I
a	O
related	O
but	O
more	O
direct	O
criterion	O
applies	O
bayesian	O
probability	O
theory	O
to	O
the	O
weighing	O
of	O
evidence	O
good	O
for	O
the	O
classical	O
treatment	O
in	O
a	O
sequential	O
testing	O
framework	O
logarithmic	O
measure	B
is	O
again	O
used	O
namely	O
log-odds	O
or	O
plausibilities	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
k	O
sec	O
rules	O
and	O
trees	O
from	O
data	O
first	O
principles	O
of	O
hypotheses	O
concerning	O
class-membership	O
the	O
plausibility-shift	O
occasioned	O
by	O
each	O
observation	O
is	O
interpreted	O
as	O
the	O
weight	O
of	O
the	O
evidence	O
contributed	O
by	O
that	O
observation	O
class-membership	O
we	O
ask	O
what	O
expected	O
total	O
weight	O
of	O
evidence	O
bearing	O
on	O
the	O
hypotheses	O
is	O
obtainable	O
from	O
knowledge	O
of	O
an	O
attribute	O
s	O
values	O
over	O
the	O
cells	O
preference	O
goes	O
to	O
that	O
attribute	O
contributing	O
the	O
greatest	O
expected	O
total	O
michie	O
al	O
attar	O
the	O
sequential	O
bayes	O
criterion	O
has	O
the	O
merit	O
once	O
the	O
tree	O
is	O
grown	O
of	O
facilitating	O
the	O
recalculation	O
of	O
probability	O
estimates	O
at	O
the	O
leaves	O
in	O
the	O
light	O
of	O
revised	O
knowledge	O
of	O
the	O
priors	O
in	O
their	O
cart	B
work	O
breiman	O
and	O
colleagues	O
initially	O
used	O
an	O
information-theoretic	O
criterion	O
but	O
subsequently	O
adopted	O
their	O
gini	B
index	I
for	O
a	O
given	O
node	O
and	O
classes	B
with	O
g	O
estimated	O
probabilities	O
c	O
authors	O
note	O
a	O
number	O
of	O
interesting	O
interpretations	O
of	O
this	O
expression	O
but	O
they	O
also	O
remark	O
that	O
within	O
a	O
wide	O
range	O
of	O
splitting	B
criteria	I
the	O
properties	O
of	O
the	O
final	O
tree	O
selected	O
are	O
surprisingly	O
insensitive	O
to	O
the	O
choice	O
of	O
splitting	O
rule	O
the	O
criterion	O
used	O
to	O
prune	O
or	O
recombine	O
upward	O
is	O
much	O
more	O
important	O
d	O
e	O
the	O
index	O
can	O
be	O
writtenf	O
g	O
the	O
c	O
getting	O
a	O
right-sized	O
tree	O
cart	B
s	O
and	O
s	O
pruning	B
starts	O
with	O
growing	O
a	O
tree	O
that	O
is	O
much	O
too	O
large	O
how	O
large	O
is	O
too	O
large	O
as	O
tree-growth	O
continues	O
and	O
end-nodes	O
multiply	O
the	O
sizes	O
of	O
their	O
associated	O
samples	O
shrink	O
probability	O
estimates	O
formed	O
from	O
the	O
empirical	O
class-frequencies	O
at	O
the	O
leaves	O
accordingly	O
suffer	O
escalating	O
estimation	O
errors	O
yet	O
this	O
only	O
says	O
that	O
overgrown	O
trees	O
make	O
unreliable	O
probability	O
estimators	O
given	O
an	O
unbiased	O
mapping	O
from	O
probability	O
estimates	O
to	O
decisions	O
why	O
should	O
their	O
performance	O
as	B
classifiers	I
suffer	O
performance	O
is	O
indeed	O
impaired	O
by	O
overfitting	B
typically	O
more	O
severely	O
in	O
tree-learning	B
than	O
in	O
some	O
other	O
multi-variate	O
methods	O
figure	O
typifies	O
a	O
universally	O
observed	O
axis	O
breiman	O
et	O
al	O
from	O
whose	O
book	O
the	O
figure	O
has	O
been	O
taken	O
describe	O
this	O
relationship	O
as	O
a	O
fairly	O
rapid	O
initial	O
decrease	O
followed	O
by	O
a	O
long	O
flat	O
valley	O
and	O
then	O
a	O
gradual	O
increase	O
in	O
this	O
long	O
flat	O
valley	O
the	O
minimum	O
is	O
almost	O
constant	O
except	O
for	O
up-down	O
changes	O
relationship	O
between	O
the	O
number	O
of	O
terminal	O
nodes	O
and	O
misclassification	O
rates	O
f	O
se	O
range	O
meanwhile	O
the	O
performance	O
of	O
the	O
tree	O
on	O
the	O
training	O
sample	O
well	O
within	O
the	O
shown	O
in	O
the	O
figure	O
continues	O
to	O
improve	O
with	O
an	O
increasingly	O
over-optimistic	O
error	B
rate	I
usually	O
referred	O
to	O
as	O
the	O
resubstitution	O
error	O
an	O
important	O
lesson	O
that	O
can	O
be	O
drawn	O
from	O
inspection	O
of	O
the	O
diagram	O
is	O
that	O
large	O
simplifications	O
of	O
the	O
tree	O
can	O
be	O
purchased	O
at	O
the	O
expense	O
of	O
rather	O
small	O
reductions	O
of	O
estimated	O
accuracy	B
overfitting	B
is	O
the	O
process	O
of	O
inferring	O
more	O
structure	O
from	O
the	O
training	O
sample	O
than	O
is	O
justified	O
by	O
the	O
population	O
from	O
which	O
it	O
was	O
drawn	O
quinlan	O
illustrates	O
the	O
seeming	O
paradox	O
that	O
an	O
overfitted	O
tree	O
can	O
be	O
a	O
worse	O
classifier	B
than	O
one	O
that	O
has	O
no	O
information	O
at	O
all	O
beyond	O
the	O
name	O
of	O
the	O
dataset	O
s	O
most	O
numerous	O
class	B
this	O
effect	O
is	O
readily	O
seen	O
in	O
the	O
extreme	O
example	B
of	O
random	O
data	O
in	O
which	O
the	O
class	B
of	O
each	O
case	O
is	O
quite	O
unrelated	O
to	O
its	O
attribute	O
values	O
i	O
constructed	O
an	O
artificial	O
dataset	O
of	O
this	O
kind	O
with	O
ten	O
attributes	B
each	O
of	O
which	O
took	O
the	O
value	O
or	O
with	O
equal	O
probability	O
the	O
class	B
was	O
also	O
binary	O
yes	O
with	O
probability	O
and	O
no	O
with	O
probability	O
one	O
thousand	O
randomly	O
generated	O
cases	O
were	O
split	O
intp	O
a	O
training	B
set	I
of	O
and	O
a	O
test	B
set	I
of	O
from	O
this	O
data	O
s	O
initial	O
tree-building	O
routine	O
f	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
fig	O
a	O
typical	O
plot	O
of	O
misclassification	O
rate	O
against	O
different	O
levels	O
of	O
growth	O
of	O
a	O
fitted	O
tree	O
horizontal	O
axis	O
no	O
of	O
terminal	O
nodes	O
vertical	O
axis	O
misclassification	O
rate	O
measured	O
on	O
test	O
data	O
produces	O
a	O
nonsensical	O
tree	O
of	O
nodes	O
that	O
has	O
an	O
error	B
rate	I
of	O
more	O
than	O
on	O
the	O
test	O
cases	O
the	O
random	O
data	O
above	O
a	O
tree	O
consisting	O
of	O
just	O
the	O
leaf	O
no	O
would	O
have	O
an	O
expected	O
error	B
rate	I
of	O
on	O
unseen	O
cases	O
yet	O
the	O
elaborate	O
tree	O
is	O
noticeably	O
less	O
accurate	O
while	O
the	O
complexity	O
comes	O
as	O
no	O
surprise	O
the	O
increased	O
error	O
attributable	O
to	O
overfitting	B
is	O
not	O
intuitively	O
obvious	O
to	O
explain	O
this	O
suppose	O
we	O
have	O
a	O
two-class	O
task	O
in	O
which	O
a	O
case	O
s	O
class	B
is	O
inherently	O
indeterminate	O
with	O
classifier	B
assigns	O
all	O
such	O
cases	O
to	O
this	O
majority	O
class	B
its	O
expected	O
error	B
rate	I
is	O
if	O
on	O
the	O
other	O
hand	O
the	O
classifier	B
assigns	O
a	O
case	O
to	O
the	O
majority	O
its	O
expected	O
of	O
the	O
cases	O
belonging	O
to	O
the	O
majority	O
class	B
no	O
if	O
a	O
proportion	O
r	O
clearlyf	O
class	B
with	O
probability	O
and	O
to	O
the	O
other	O
class	B
with	O
probabilityf	O
other	O
class	B
c	O
majority	O
classc	O
this	O
is	O
generally	O
greater	O
thanf	O
g	O
and	O
k	O
which	O
comes	O
to	O
k	O
the	O
probability	O
that	O
a	O
case	O
belonging	O
to	O
the	O
other	O
class	B
is	O
assigned	O
to	O
the	O
is	O
at	O
least	O
so	O
the	O
second	O
classifier	B
will	O
have	O
a	O
higher	O
error	B
rate	I
now	O
the	O
complex	B
decision	O
tree	O
bears	O
a	O
close	O
resemblance	O
to	O
this	O
second	O
type	O
of	O
classifier	B
the	O
tests	O
are	O
unrelated	O
to	O
class	B
so	O
like	O
a	O
symbolic	O
pachinko	O
machine	O
the	O
tree	O
sends	O
each	O
case	O
randomly	O
to	O
one	O
of	O
the	O
leaves	O
n	O
g	O
since	O
the	O
probability	O
that	O
a	O
case	O
belonging	O
to	O
the	O
majority	O
class	B
is	O
assigned	O
to	O
the	O
error	B
rate	I
is	O
the	O
sum	O
of	O
l	O
quinlan	O
points	O
out	O
that	O
the	O
probability	O
of	O
reaching	O
a	O
leaf	O
labelled	O
with	O
class	B
c	O
is	O
the	O
same	O
as	O
the	O
relative	O
frequency	O
of	O
c	O
in	O
the	O
training	O
data	O
and	O
concludes	O
that	O
the	O
tree	O
s	O
expected	O
value	O
error	B
rate	I
for	O
the	O
random	O
data	O
above	O
is	O
h	O
given	O
the	O
acknowledged	O
perils	O
of	O
overfitting	B
how	O
should	O
backward	B
pruning	B
be	O
applied	O
to	O
a	O
too-large	O
tree	O
the	O
methods	O
adopted	O
for	O
cart	B
and	O
follow	O
different	O
philosophies	O
and	O
other	O
decision-tree	O
algorithms	O
have	O
adopted	O
their	O
own	O
variants	O
we	O
have	O
now	O
reached	O
the	O
level	O
of	O
detail	O
appropriate	O
to	O
section	O
in	O
which	O
specific	O
features	B
of	O
the	O
various	O
tree	O
and	O
rule	O
learning	O
algorithms	O
including	O
their	O
methods	O
of	O
pruning	B
are	O
examined	O
before	O
proceeding	O
to	O
these	O
candidates	O
for	O
trial	O
it	O
should	O
be	O
emphasized	O
that	O
their	O
selection	O
was	O
or	O
quite	O
close	O
to	O
the	O
observed	O
h	O
f	O
f	O
g	O
f	O
sec	O
statlog	B
s	O
ml	O
algorithms	O
necessarily	O
to	O
a	O
large	O
extent	O
arbitrary	O
having	O
more	O
to	O
do	O
with	O
the	O
practical	O
logic	O
of	O
coordinating	O
a	O
complex	B
and	O
geographically	O
distributed	O
project	O
than	O
with	O
judgements	O
of	O
merit	O
or	O
importance	O
apart	O
from	O
the	O
omission	O
of	O
entire	O
categories	O
of	O
ml	O
with	O
the	O
genetic	B
and	O
ilp	B
algorithms	O
discussed	O
in	O
chapter	O
particular	O
contributions	O
to	O
decision-tree	O
learning	O
should	O
be	O
acknowledged	O
that	O
would	O
otherwise	O
lack	O
mention	O
first	O
a	O
major	O
historical	O
role	O
which	O
continues	O
today	O
belongs	O
to	O
the	O
assistant	B
algorithm	O
developed	O
by	O
ivan	O
bratko	O
s	O
group	O
in	O
slovenia	O
kononenko	O
and	O
bratko	O
assistant	B
introduced	O
many	O
improvements	O
for	O
dealing	O
with	O
missing	B
values	I
attribute	O
splitting	O
and	O
pruning	B
and	O
has	O
also	O
recently	O
incorporated	O
the	O
m-estimate	O
method	O
and	O
bratko	O
see	O
also	O
dzeroski	O
cesnik	O
and	O
petrovski	O
of	O
handling	O
prior	O
probability	O
assumptions	O
second	O
an	O
important	O
niche	O
is	O
occupied	O
in	O
the	O
commercialsector	O
of	O
ml	O
by	O
the	O
xpertrule	B
family	O
of	O
packages	O
developed	O
by	O
attar	O
software	O
ltd	O
facilities	O
for	O
large-scale	O
data	O
analysis	O
are	O
integrated	O
with	O
sophisticated	O
support	O
for	O
structured	B
induction	I
for	O
example	B
attar	O
these	O
and	O
other	O
features	B
make	O
this	O
suite	O
currently	O
the	O
most	O
powerful	O
and	O
versatile	O
facility	O
available	O
for	O
industrial	O
ml	O
statlog	B
s	O
ml	O
algorithms	O
tree-learning	B
further	O
features	B
of	O
the	O
reader	O
should	O
be	O
aware	O
that	O
the	O
two	O
versions	O
of	O
used	O
in	O
the	O
statlog	B
trials	O
differ	O
in	O
certain	O
respects	O
from	O
the	O
present	O
version	O
which	O
was	O
recently	O
presented	O
in	O
quinlan	O
the	O
version	O
on	O
which	O
accounts	O
in	O
section	O
are	O
based	O
is	O
that	O
of	O
the	O
radical	O
upgrade	O
described	O
in	O
quinlan	O
newid	B
newid	B
is	O
a	O
similar	O
decision	O
tree	O
algorithm	O
to	O
similar	O
to	O
newid	B
inputs	O
a	O
set	O
of	O
satisfies	O
the	O
termination	O
condition	O
then	O
output	B
the	O
current	O
tree	O
and	O
halt	O
classification	B
unlike	O
newid	B
does	O
not	O
perform	O
windowing	O
thus	O
its	O
core	O
procedure	O
is	O
simpler	O
and	O
a	O
classm	O
its	O
output	B
is	O
a	O
decision	O
tree	O
which	O
performs	O
a	O
set	O
of	O
attributes	B
examples	O
set	O
the	O
current	O
examples	O
to	O
if	O
determine	O
the	O
value	O
of	O
the	O
evaluation	O
function	O
with	O
the	O
attribute	O
for	O
each	O
attribute	O
that	O
has	O
the	O
largest	O
value	O
of	O
this	O
function	O
divide	O
the	O
set	O
values	O
for	O
each	O
such	O
subset	O
of	O
examples	O
recursively	O
re-enter	O
at	O
step	O
with	O
set	O
to	O
set	O
the	O
subtrees	O
of	O
the	O
current	O
node	O
to	O
be	O
the	O
subtrees	O
thus	O
produced	O
the	O
termination	O
condition	O
is	O
simpler	O
than	O
i	O
e	O
it	O
terminates	O
when	O
the	O
node	O
contains	O
all	O
examples	O
in	O
the	O
same	O
class	B
this	O
simple-minded	O
strategy	O
tries	O
to	O
overfit	O
the	O
training	O
data	O
and	O
will	O
produce	O
a	O
complete	O
tree	O
from	O
the	O
training	O
data	O
newid	B
deals	O
with	O
empty	O
leaf	O
nodes	O
as	O
does	O
but	O
it	O
also	O
considers	O
the	O
possibility	O
of	O
clashing	O
examples	O
if	O
the	O
set	O
of	O
attributes	B
is	O
empty	O
it	O
labels	O
the	O
leaf	O
node	O
as	O
clash	O
meaning	O
that	O
it	O
is	O
impossible	O
to	O
distinguish	O
between	O
the	O
examples	O
in	O
most	O
situations	O
the	O
attribute	O
set	O
will	O
not	O
be	O
empty	O
so	O
newid	B
discards	O
attributes	B
that	O
have	O
been	O
used	O
as	O
they	O
can	O
contribute	O
no	O
more	O
information	O
to	O
the	O
tree	O
into	O
subsets	O
by	O
attribute	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
tion	B
of	O
newid	B
is	O
the	O
information	O
gain	O
function	O
bkbcm	O
for	O
classification	B
problems	O
where	O
the	O
class	B
values	O
are	O
categorical	O
the	O
evaluation	O
funcit	O
does	O
a	O
similar	O
lookahead	O
to	O
determine	O
the	O
best	O
attribute	O
to	O
split	O
on	O
using	O
a	O
greedy	O
search	O
it	O
also	O
handles	O
numeric	O
attributes	B
in	O
the	O
same	O
way	O
as	O
does	O
using	O
the	O
attribute	O
subsetting	O
method	O
numeric	O
class	B
values	O
newid	B
allows	O
numeric	O
class	B
values	O
and	O
can	O
produce	O
a	O
regression	B
tree	I
for	O
each	O
split	O
it	O
aims	O
to	O
reduce	O
the	O
spread	O
of	O
class	B
values	O
in	O
the	O
subsets	O
introduced	O
by	O
the	O
split	O
instead	O
of	O
trying	O
to	O
gain	O
the	O
most	O
information	O
formally	O
for	O
each	O
ordered	O
categorical	O
attribute	O
with	O
g	O
values	O
in	O
the	O
set	O
c	O
class	B
of	O
it	O
chooses	O
the	O
one	O
that	O
minimises	O
the	O
value	O
of	O
attribute	O
value	O
of	O
for	O
numeric	O
attributes	B
the	O
attribute	O
subsetting	O
method	O
is	O
used	O
instead	O
e	O
is	O
a	O
c	O
c	O
when	O
the	O
class	B
value	O
is	O
numeric	O
the	O
termination	O
function	O
of	O
the	O
algorithm	O
will	O
also	O
be	O
different	O
the	O
criterion	O
that	O
all	O
examples	O
share	O
the	O
same	O
class	B
value	O
is	O
no	O
longer	O
appropriate	O
and	O
the	O
following	O
criterion	O
is	O
used	O
instead	O
the	O
algorithm	O
terminates	O
at	O
a	O
node	O
g	O
fdh	O
is	O
the	O
standard	O
deviation	O
split	O
into	O
fractional	O
examples	O
for	O
each	O
possible	O
value	O
of	O
that	O
attribute	O
the	O
fractions	O
of	O
the	O
different	O
values	O
sum	O
to	O
they	O
are	O
estimated	O
from	O
the	O
numbers	O
of	O
examples	O
of	O
the	O
same	O
class	B
with	O
a	O
known	O
value	O
of	O
that	O
attribute	O
user-tunable	O
parameter	O
missing	B
values	I
there	O
are	O
two	O
types	O
of	O
missing	B
values	I
in	O
newid	B
unknown	O
values	O
and	O
don	O
t-care	O
values	O
with	O
examples	O
when	O
c	O
where	O
is	O
the	O
original	O
example	B
set	O
and	O
the	O
constantd	O
during	O
the	O
training	O
phase	O
if	O
an	O
example	B
of	O
classm	O
has	O
an	O
unknown	O
attribute	O
value	O
it	O
is	O
consider	O
attribute	O
with	O
values	O
d	O
andk	O
there	O
are	O
examples	O
at	O
the	O
current	O
node	O
in	O
classm	O
with	O
values	O
for	O
e	O
and	O
missing	O
naively	O
we	O
would	O
split	O
the	O
in	O
the	O
ratio	O
to	O
d	O
and	O
however	O
the	O
laplace	O
criterion	O
gives	O
a	O
better	O
estimate	O
of	O
the	O
expected	O
ratio	O
of	O
e	O
tok	O
using	O
the	O
formula	O
feg	O
h	O
m	O
i	O
dkbc	O
e	O
i	O
ck	O
k	O
feg	O
h	O
c	O
e	O
is	O
the	O
no	O
examples	O
in	O
classm	O
with	O
attribute	O
i	O
is	O
the	O
total	O
no	O
examples	O
in	O
classm	O
is	O
the	O
total	O
no	O
examples	O
in	O
with	O
and	O
similarly	O
for	O
m	O
i	O
dkbcbk	O
don	O
t-care	O
s	O
are	O
intended	O
as	O
a	O
short-hand	O
to	O
cover	B
all	O
the	O
possible	O
values	O
of	O
the	O
don	O
t-care	O
attribute	O
they	O
are	O
handled	O
in	O
a	O
similar	O
way	O
to	O
unknowns	O
except	O
the	O
example	B
is	O
simply	O
duplicated	O
not	O
fractionalised	O
for	O
each	O
value	O
of	O
the	O
attribute	O
when	O
being	O
inspected	O
g	O
this	O
latter	O
laplace	O
estimate	O
is	O
used	O
in	O
newid	B
ck	O
c	O
e	O
e	O
k	O
where	O
f	O
g	O
g	O
d	O
g	O
g	O
g	O
g	O
g	O
k	O
k	O
sec	O
statlog	B
s	O
ml	O
algorithms	O
thus	O
in	O
a	O
similar	O
case	O
with	O
d	O
s	O
s	O
and	O
the	O
example	B
would	O
be	O
considered	O
as	O
examples	O
one	O
with	O
value	O
d	O
and	O
one	O
with	O
valuek	O
this	O
duplication	O
only	O
occurs	O
when	O
inspecting	O
the	O
split	O
caused	O
by	O
attribute	O
if	O
a	O
different	O
attribute	O
the	O
example	B
with	O
is	O
being	O
considered	O
is	O
only	O
considered	O
as	O
example	B
note	O
this	O
is	O
an	O
ad	O
hoc	O
method	O
because	O
the	O
duplication	O
of	O
examples	O
may	O
cause	O
the	O
total	O
number	O
of	O
examples	O
at	O
the	O
leaves	O
to	O
add	O
up	O
to	O
more	O
than	O
the	O
total	O
number	O
of	O
examples	O
originally	O
in	O
the	O
training	B
set	I
and	O
a	O
known	O
value	O
for	O
when	O
a	O
tree	O
is	O
executed	O
and	O
the	O
testing	O
example	B
has	O
an	O
unknown	O
value	O
for	O
the	O
attribute	O
being	O
tested	O
on	O
the	O
example	B
is	O
again	O
split	O
fractionally	O
using	O
the	O
laplace	O
estimate	O
for	O
the	O
ratio	O
but	O
as	O
the	O
testing	O
example	B
s	O
class	B
value	O
is	O
unknown	O
all	O
the	O
training	O
examples	O
at	O
class	B
ratios	O
are	O
the	O
fractional	O
examples	O
arrive	O
rather	O
than	O
predicting	O
the	O
majority	O
class	B
a	O
probabilistic	O
classification	B
is	O
made	O
for	O
to	O
split	O
the	O
testing	O
example	B
into	O
the	O
numbers	O
of	O
training	O
examples	O
at	O
the	O
node	O
are	O
found	O
by	O
back-propagating	O
the	O
example	B
counts	O
recorded	O
at	O
the	O
leaves	O
of	O
the	O
subtree	O
beneath	O
the	O
node	O
back	O
to	O
that	O
node	O
the	O
class	B
predicted	O
at	O
a	O
node	O
is	O
the	O
majority	O
class	B
there	O
a	O
tie	O
with	O
more	O
than	O
one	O
majority	O
class	B
select	O
the	O
first	O
the	O
example	B
may	O
thus	O
be	O
classified	O
the	O
node	O
than	O
just	O
those	O
of	O
classm	O
are	O
used	O
to	O
estimate	O
the	O
appropriate	O
fractions	O
andm	O
wherem	O
and	O
asm	O
say	O
asm	O
are	O
the	O
majority	O
classes	B
at	O
the	O
two	O
leaves	O
where	O
and	O
as	O
andm	O
classifies	O
an	O
example	B
asm	O
example	B
a	O
leaf	O
with	O
for	O
classesm	O
than	O
simply	O
asm	O
for	O
fractional	O
examples	O
the	O
distributions	O
would	O
be	O
weighted	O
and	O
summed	O
for	O
example	B
arrives	O
at	O
leaf	O
at	O
leaf	O
and	O
thus	O
the	O
example	B
is	O
the	O
pruning	B
algorithm	O
works	O
as	O
follows	O
given	O
a	O
treez	O
examples	O
a	O
further	O
pruning	B
set	O
of	O
examples	O
and	O
a	O
threshold	O
value	O
lying	O
below	O
internal	O
node	O
if	O
the	O
subtree	O
ofz	O
of	O
thez	O
for	O
the	O
pruning	B
examples	O
than	O
node	O
sub-tree	O
and	O
make	O
node	O
is	O
set	O
tof	O
a	O
leaf-node	O
by	O
default	B
a	O
testing	O
example	B
tested	O
on	O
an	O
attribute	O
with	O
a	O
don	O
t-care	O
value	O
is	O
simply	O
duplicated	O
for	O
each	O
outgoing	O
branch	O
i	O
e	O
a	O
whole	O
example	B
is	O
sent	O
down	O
every	O
outgoing	O
branch	O
thus	O
counting	O
it	O
as	O
several	O
examples	O
tree	O
pruning	B
induced	O
from	O
a	O
set	O
of	O
learning	O
then	O
for	O
each	O
provides	O
better	O
accuracy	B
but	O
one	O
can	O
modify	O
it	O
does	O
labelled	O
by	O
the	O
majority	O
class	B
for	O
the	O
learning	O
examples	O
at	O
that	O
node	O
then	O
leave	O
the	O
subtree	O
unpruned	O
otherwise	O
prune	O
it	O
delete	O
the	O
to	O
suit	O
different	O
tasks	O
apart	O
from	O
the	O
features	B
described	O
above	O
are	O
more	O
relevant	O
to	O
the	O
version	O
of	O
newid	B
used	O
for	O
statlog	B
newid	B
has	O
a	O
number	O
of	O
other	O
features	B
newid	B
can	O
have	O
binary	O
splits	O
for	O
each	O
attribute	O
at	O
a	O
node	O
of	O
a	O
tree	O
using	O
the	O
subsetting	O
principle	O
it	O
can	O
deal	O
with	O
ordered	O
sequential	O
attributes	B
attributes	B
whose	O
values	O
are	O
ordered	O
newid	B
can	O
also	O
accept	O
a	O
pre-specified	O
ordering	O
of	O
attributes	B
so	O
the	O
more	O
important	O
ones	O
will	O
be	O
considered	O
first	O
and	O
the	O
user	O
can	O
force	O
newid	B
to	O
choose	O
a	O
particular	O
attribute	O
for	O
splitting	O
at	O
a	O
node	O
it	O
can	O
also	O
deal	O
with	O
structured	O
attributes	B
is	O
not	O
a	O
single	O
algorithm	O
it	O
is	O
a	O
knowledge	O
acquisition	O
environment	O
for	O
expert	B
systems	I
which	O
enables	O
its	O
user	O
to	O
build	O
a	O
knowledge	O
base	O
or	O
an	O
expert	O
system	O
from	O
the	O
analysis	O
of	O
examples	O
provided	O
by	O
the	O
human	O
expert	O
thus	O
it	O
placed	O
considerable	O
emphasis	O
on	O
the	O
m	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
graphical	O
interface	O
this	O
interface	O
is	O
consisting	O
of	O
graphical	O
editors	O
which	O
enable	O
the	O
user	O
to	O
define	O
the	O
domain	O
to	O
interactively	O
build	O
the	O
data	O
base	O
and	O
to	O
go	O
through	O
the	O
hierarchy	B
of	O
classes	B
and	O
the	O
decision	O
tree	O
dialog	O
and	O
interaction	O
of	O
the	O
system	O
with	O
the	O
user	O
the	O
user	O
interacts	O
with	O
via	O
a	O
can	O
be	O
viewed	O
as	O
an	O
extension	O
of	O
a	O
tree	O
induction	O
algorithm	O
that	O
is	O
essentially	O
the	O
same	O
as	O
newid	B
because	O
of	O
its	O
user	O
interface	O
it	O
allows	O
a	O
more	O
natural	O
manner	O
of	O
interaction	O
with	O
a	O
domain	O
expert	O
the	O
validation	O
of	O
the	O
trees	O
produced	O
and	O
the	O
test	O
of	O
its	O
accuracy	B
and	O
reliability	O
it	O
also	O
provides	O
a	O
simple	O
fast	O
and	O
cheap	O
method	O
to	O
update	O
the	O
rule	O
and	O
data	O
bases	O
it	O
produces	O
from	O
data	O
and	O
known	O
rules	O
of	O
the	O
domain	O
either	O
a	O
decision	O
tree	O
or	O
a	O
set	O
of	O
rules	O
designed	O
to	O
be	O
used	O
by	O
expert	O
system	O
further	O
features	B
of	O
cart	B
cart	B
classification	B
and	O
regression	B
tree	I
is	O
a	O
binary	O
decision	O
tree	O
algorithm	O
et	O
al	O
which	O
has	O
exactly	O
two	O
branches	O
at	O
each	O
internal	O
node	O
we	O
have	O
used	O
two	O
different	O
implementations	O
of	O
cart	B
the	O
commercial	O
version	O
of	O
cart	B
and	O
indcart	B
which	O
is	O
part	O
of	O
the	O
ind	B
package	I
see	O
naive	B
bayes	I
section	O
indcart	B
differs	O
from	O
cart	B
as	O
described	O
in	O
breiman	O
et	O
al	O
in	O
using	O
a	O
different	O
better	O
way	O
of	O
handling	O
missing	B
values	I
in	O
not	O
implementing	O
the	O
regression	O
part	O
of	O
cart	B
and	O
in	O
the	O
different	O
pruning	B
settings	O
evaluation	O
function	O
for	O
splitting	O
the	O
evaluation	O
function	O
used	O
by	O
cart	B
is	O
different	O
from	O
that	O
in	O
the	O
family	O
of	O
algorithms	O
consider	O
the	O
case	O
of	O
a	O
problem	O
with	O
two	O
classes	B
and	O
a	O
node	O
has	O
examples	O
from	O
each	O
class	B
the	O
node	O
has	O
maximum	O
impurity	B
if	O
a	O
split	O
could	O
be	O
found	O
that	O
split	O
the	O
data	O
into	O
one	O
subgroup	O
of	O
and	O
another	O
of	O
then	O
intuitively	O
the	O
impurity	B
has	O
been	O
reduced	O
the	O
impurity	B
would	O
be	O
completely	O
removed	O
if	O
a	O
split	O
could	O
be	O
found	O
that	O
produced	O
sub-groups	O
and	O
in	O
cart	B
this	O
intuitive	O
idea	O
of	O
impurity	B
is	O
formalised	O
in	O
the	O
gini	B
index	I
for	O
subgroups	O
is	O
summed	O
and	O
the	O
split	O
with	O
the	O
maximum	O
reduction	O
in	O
impurity	B
chosen	O
for	O
ordered	O
and	O
numeric	O
attributes	B
cart	B
considers	O
all	O
possible	O
splits	O
in	O
the	O
sequence	O
the	O
current	O
nodem	O
bk	O
icm	O
where	O
is	O
the	O
probability	O
of	O
class	B
inm	O
for	O
each	O
possible	O
split	O
the	O
impurity	B
of	O
the	O
f	O
splits	O
for	O
categorical	O
attributes	B
cart	B
examines	O
fork	O
values	O
of	O
the	O
attribute	O
there	O
arek	O
all	O
possible	O
binary	O
splits	O
which	O
is	O
the	O
same	O
as	O
attribute	O
subsetting	O
used	O
for	O
fork	O
f	O
splits	O
at	O
each	O
node	O
cart	B
searches	O
through	O
the	O
values	O
of	O
the	O
attribute	O
there	O
are	O
attributes	B
one	O
by	O
one	O
for	O
each	O
attribute	O
it	O
finds	O
the	O
best	O
split	O
then	O
it	O
compares	O
the	O
best	O
single	O
splits	O
and	O
selects	O
the	O
best	O
attribute	O
of	O
the	O
best	O
splits	O
minimal	O
cost	B
complexity	I
tree	O
pruning	B
apart	O
from	O
the	O
evaluation	O
function	O
cart	B
s	O
most	O
crucial	O
difference	O
from	O
the	O
other	O
machine	O
learning	O
algorithms	O
is	O
its	O
sophisticated	O
pruning	B
mechanism	O
cart	B
treats	O
pruning	B
as	O
a	O
tradeoff	O
between	O
two	O
issues	O
getting	O
the	O
right	O
size	O
of	O
a	O
tree	O
and	O
getting	O
accurate	O
estimates	O
of	O
the	O
true	O
probabilities	O
of	O
misclassification	O
this	O
process	O
is	O
known	O
as	O
minimal	O
costcomplexity	O
pruning	B
g	O
f	O
sec	O
statlog	B
s	O
ml	O
algorithms	O
is	O
if	O
be	O
a	O
decision	O
tree	O
used	O
to	O
as	O
the	O
cost	O
for	O
each	O
leaf	O
is	O
such	O
that	O
all	O
other	O
subtrees	O
have	O
higher	O
cost	O
complexities	O
or	O
have	O
the	O
same	O
cost	B
complexity	I
it	O
is	O
a	O
two	O
stage	O
method	O
considering	O
the	O
first	O
stage	O
letz	O
classifyk	O
examples	O
in	O
the	O
training	B
set	I
be	O
the	O
misclassified	O
set	O
of	O
size	O
if	O
bcz	O
let	O
is	O
the	O
number	O
of	O
leaves	O
inz	O
for	O
some	O
parameter	O
the	O
cost	B
complexity	I
ofz	O
e	O
e	O
bcbz	O
if	O
we	O
regard	O
where	O
is	O
the	O
error	O
estimate	O
ofz	O
is	O
a	O
linear	O
combination	O
of	O
its	O
error	O
estimate	O
and	O
a	O
penalty	O
for	O
its	O
complexity	O
if	O
small	O
the	O
penalty	O
for	O
having	O
a	O
large	O
number	O
of	O
leaves	O
is	O
small	O
andz	O
will	O
be	O
large	O
as	O
increases	O
the	O
minimising	O
subtree	O
will	O
decrease	O
in	O
size	O
now	O
if	O
we	O
convert	O
some	O
subtree	O
to	O
a	O
leaf	O
the	O
new	O
treez	O
would	O
misclassifyd	O
more	O
examples	O
but	O
would	O
contain	O
bc	O
is	O
the	O
same	O
as	O
that	O
ofz	O
fewer	O
leaves	O
the	O
cost	B
complexity	I
ofz	O
feg	O
kn	O
dc	O
bc	O
g	O
for	O
any	O
value	O
of	O
which	O
minimises	O
it	O
can	O
be	O
shown	O
that	O
there	O
is	O
a	O
unique	O
subtreez	O
cz	O
as	O
a	O
pruned	O
subtree	O
and	O
havez	O
there	O
is	O
as	O
above	O
let	O
this	O
tree	O
bez	O
we	O
can	O
find	O
the	O
subtree	O
such	O
that	O
forz	O
where	O
each	O
subtree	O
is	O
produced	O
by	O
is	O
then	O
a	O
minimising	O
sequence	O
of	O
treesz	O
fromz	O
we	O
examine	O
each	O
pruning	B
upward	O
from	O
the	O
previous	O
subtree	O
to	O
producez	O
non-leaf	O
subtree	O
ofz	O
and	O
find	O
the	O
minimum	O
value	O
of	O
that	O
value	O
of	O
will	O
be	O
replaced	O
by	O
leaves	O
the	O
best	O
tree	O
is	O
selected	O
from	O
this	O
series	O
of	O
trees	O
would	O
be	O
this	O
latter	O
stage	O
selects	O
a	O
single	O
tree	O
based	O
on	O
its	O
reliability	O
i	O
e	O
classification	B
error	O
the	O
problem	O
of	O
pruning	B
is	O
now	O
reduced	O
to	O
finding	O
which	O
tree	O
in	O
the	O
sequence	O
is	O
the	O
optimally	O
sized	O
one	O
chosen	O
however	O
this	O
is	O
not	O
the	O
case	O
and	O
it	O
tends	O
to	O
underestimate	O
the	O
number	O
of	O
errors	O
a	O
more	O
honest	O
estimate	O
is	O
therefore	O
needed	O
in	O
cart	B
this	O
is	O
produced	O
by	O
using	O
crossvalidation	O
the	O
idea	O
is	O
that	O
instead	O
of	O
using	O
one	O
sample	O
data	O
to	O
build	O
a	O
tree	O
and	O
another	O
sample	O
data	O
to	O
test	O
the	O
tree	O
you	O
can	O
form	O
several	O
pseudo-independent	O
samples	O
from	O
the	O
original	O
sample	O
and	O
use	O
these	O
to	O
form	O
a	O
more	O
accurate	O
estimate	O
of	O
the	O
error	O
the	O
general	O
method	O
is	O
with	O
the	O
classification	B
error	O
not	O
exceeding	O
an	O
expected	O
error	B
rate	I
on	O
some	O
test	B
set	I
which	O
is	O
done	O
at	O
the	O
second	O
stage	O
g	O
was	O
unbiased	O
then	O
the	O
largest	O
treez	O
if	O
the	O
error	O
estimate	O
the	O
one	O
or	O
more	O
subtrees	O
with	O
tok	O
form	O
the	O
cross-validation	O
error	O
estimate	O
as	O
randomly	O
split	O
the	O
original	O
sample	O
intok	O
equal	O
subsamples	O
for	O
a	O
build	O
a	O
tree	O
on	O
the	O
training	B
set	I
n	O
x	O
and	O
b	O
determine	O
the	O
error	O
estimate	O
using	O
the	O
pruning	B
set	O
cross-validation	O
and	O
cost	B
complexity	I
pruning	B
is	O
combined	O
to	O
select	O
the	O
value	O
of	O
the	O
method	O
is	O
to	O
estimate	O
the	O
expected	O
error	O
rates	O
of	O
estimates	O
obtained	O
withz	O
values	O
of	O
using	O
cross-validation	O
from	O
these	O
estimates	O
it	O
is	O
then	O
possible	O
to	O
estimate	O
an	O
optimal	O
value	O
b	O
e	O
of	O
for	O
which	O
the	O
estimated	O
true	O
error	B
rate	I
ofz	O
for	O
all	O
the	O
data	O
is	O
the	O
for	O
all	O
g	O
g	O
g	O
g	O
h	O
k	O
g	O
f	O
d	O
g	O
z	O
z	O
f	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
is	O
that	O
value	O
of	O
a	O
which	O
minimises	O
the	O
mean	O
the	O
cart	B
methodology	O
therefore	O
involves	O
two	O
quite	O
separate	O
calculations	O
first	O
the	O
is	O
determined	O
using	O
cross-validation	O
ten	O
fold	O
cross-validation	O
is	O
recom	O
the	O
value	O
d	O
minimum	O
for	O
all	O
values	O
of	O
cross-validation	O
error	O
estimate	O
oncez	O
has	O
been	O
determined	O
the	O
tree	O
that	O
is	O
finally	O
suggested	O
for	O
use	O
is	O
that	O
which	O
minimises	O
the	O
cost-complexity	O
using	O
d	O
and	O
all	O
the	O
data	O
value	O
of	O
d	O
mended	O
the	O
second	O
step	O
is	O
using	O
this	O
value	O
of	O
and	O
y	O
of	O
a	O
node	O
if	O
the	O
best	O
split	O
of	O
yy	O
on	O
the	O
attributes	B
other	O
than	O
e	O
is	O
the	O
split	O
on	O
the	O
attribute	O
find	O
the	O
split	O
that	O
is	O
most	O
similar	O
to	O
if	O
an	O
example	B
has	O
the	O
value	O
of	O
missing	B
values	I
missing	O
attribute	O
values	O
in	O
the	O
training	O
and	O
test	O
data	O
are	O
dealt	O
with	O
in	O
cart	B
by	O
using	O
surrogate	O
splits	O
the	O
idea	O
is	O
this	O
define	O
a	O
measure	B
of	O
similarity	O
between	O
any	O
two	O
splits	O
missing	O
decide	O
whether	O
it	O
goes	O
to	O
the	O
left	O
or	O
right	O
sub-tree	O
by	O
using	O
the	O
best	O
surrogate	O
split	O
if	O
it	O
is	O
missing	O
the	O
variable	O
containing	O
the	O
best	O
surrogate	O
split	O
then	O
the	O
second	O
best	O
is	O
used	O
and	O
so	O
on	O
to	O
grow	O
the	O
final	O
tree	O
is	O
especially	O
designed	O
for	O
continuous	O
and	O
ordered	O
discrete	O
valued	O
attributes	B
though	O
an	O
added	O
sub-algorithm	O
is	O
able	O
to	O
handle	O
unordered	O
discrete	O
valued	O
attributes	B
as	O
well	O
is	O
a	O
decision	O
threshold	O
similar	O
to	O
other	O
decision	O
tree	O
methods	O
only	O
class	B
areas	O
bounded	O
by	O
hyperplanes	O
parallel	O
to	O
the	O
axes	O
of	O
the	O
feature	O
space	O
are	O
possible	O
evaluation	O
function	O
for	O
splitting	O
the	O
tree	O
will	O
be	O
constructed	O
sequentially	O
starting	O
with	O
one	O
attribute	O
and	O
branching	O
with	O
other	O
attributes	B
recursively	O
if	O
no	O
sufficient	O
discrimination	B
of	O
classes	B
can	O
be	O
achieved	O
that	O
let	O
the	O
examples	O
be	O
sampled	O
from	O
the	O
examples	O
expressed	O
withk	O
attributes	B
separates	O
the	O
examples	O
from	O
thek	O
dimensions	O
into	O
areas	O
represented	O
by	O
subsets	O
g	O
of	O
samples	O
where	O
the	O
classm	O
jc	O
g	O
exists	O
with	O
a	O
probability	O
g	O
cm	O
where	O
f	O
is	O
if	O
at	O
a	O
node	O
no	O
decision	O
for	O
a	O
classm	O
according	O
to	O
the	O
above	O
formula	O
can	O
be	O
made	O
a	O
let	O
be	O
a	O
certain	O
non-leaf	O
node	O
in	O
the	O
tree	O
construction	O
process	O
at	O
first	O
the	O
attribute	O
with	O
the	O
best	O
local	O
discrimination	B
measure	B
at	O
this	O
node	O
has	O
to	O
be	O
determined	O
for	O
that	O
two	O
different	O
methods	O
can	O
be	O
used	O
by	O
an	O
option	O
a	O
statistical	B
and	O
an	O
entropy	B
measure	B
respectively	O
the	O
statistical	B
approach	O
is	O
working	O
without	O
any	O
knowledge	O
about	O
the	O
result	O
of	O
the	O
desired	O
discretisation	O
for	O
continuous	O
attributes	B
the	O
quotient	O
meyer-br	O
otz	O
sch	O
urmann	O
branch	O
formed	O
with	O
a	O
new	O
attribute	O
is	O
appended	O
to	O
the	O
tree	O
if	O
this	O
attribute	O
is	O
continuous	O
a	O
discretisation	O
i	O
e	O
intervals	O
corresponding	O
to	O
qualitative	O
values	O
has	O
to	O
be	O
used	O
e	O
is	O
the	O
standard	O
deviation	O
of	O
is	O
the	O
mean	O
value	O
of	O
the	O
square	O
of	O
distances	O
between	O
the	O
classes	B
this	O
measure	B
has	O
to	O
be	O
computed	O
for	O
each	O
is	O
chosen	O
as	O
the	O
best	O
one	O
for	O
splitting	O
at	O
this	O
node	O
the	O
entropy	B
measure	B
provided	O
as	O
an	O
evaluation	O
function	O
requires	O
an	O
d	O
y	O
i	O
yk	O
c	O
is	O
a	O
discrimination	B
measure	B
for	O
a	O
single	O
attribute	O
where	O
examples	O
in	O
from	O
the	O
centroid	O
of	O
the	O
attribute	O
value	O
and	O
attribute	O
the	O
attribute	O
with	O
the	O
least	O
value	O
of	O
d	O
y	O
i	O
yk	O
c	O
intermediate	O
discretisation	O
at	O
n	O
for	O
each	O
attribute	O
using	O
the	O
splitting	O
procedure	O
described	O
f	O
k	O
f	O
g	O
g	O
sec	O
statlog	B
s	O
ml	O
algorithms	O
are	O
ordered	O
along	O
the	O
axis	O
of	O
the	O
selected	O
the	O
formula	O
for	O
computing	O
this	O
confidence	O
interval	O
can	O
be	O
used	O
to	O
obtain	O
an	O
estimate	O
of	O
the	O
the	O
hypothesis	O
of	O
an	O
already	O
existing	O
interval	O
discretisation	O
well	O
known	O
entropy	B
measure	B
the	O
attribute	O
with	O
the	O
largest	O
value	O
of	O
all	O
is	O
selected	O
and	O
occurs	O
than	O
the	O
discretisation	O
procedure	O
below	O
leads	O
to	O
a	O
refinement	O
g	O
of	O
information	O
will	O
be	O
computed	O
for	O
k	O
by	O
the	O
below	O
then	O
the	O
gain	O
the	O
gain	O
is	O
chosen	O
as	O
the	O
best	O
one	O
for	O
splitting	O
at	O
that	O
node	O
note	O
that	O
at	O
each	O
node	O
d	O
d	O
will	O
be	O
considered	O
again	O
if	O
available	O
attributes	B
already	O
in	O
the	O
path	O
to	O
reaching	O
the	O
current	O
node	O
all	O
examples	O
new	O
attribute	O
according	O
to	O
increasing	O
values	O
intervals	O
which	O
contain	O
an	O
ordered	O
set	O
of	O
values	O
of	O
the	O
attribute	O
are	O
formed	O
recursively	O
on	O
the	O
collecting	O
examples	O
from	O
left	O
to	O
right	O
until	O
a	O
class	B
decision	O
can	O
be	O
made	O
on	O
a	O
given	O
level	O
of	O
confidence	O
the	O
number	O
let	O
be	O
a	O
current	O
interval	O
containingk	O
examples	O
of	O
different	O
classes	B
andk	O
thenk	O
of	O
examples	O
belonging	O
to	O
classm	O
g	O
on	O
the	O
current	O
node	O
probability	O
cm	O
g	O
there	O
exists	O
a	O
classm	O
occurring	O
in	O
with	O
cm	O
occurring	O
in	O
holds	O
on	O
a	O
certain	O
level	O
g	O
the	O
inequality	O
cm	O
for	O
all	O
classesm	O
n	O
yields	O
a	O
confidence	O
interval	O
of	O
confidencef	O
a	O
given	O
x	O
long	O
sequence	O
of	O
examples	O
the	O
true	O
value	O
of	O
probability	O
lies	O
within	O
g	O
for	O
cm	O
g	O
and	O
in	O
a	O
an	O
estimation	O
on	O
the	O
levelf	O
cm	O
g	O
with	O
probability	O
cm	O
x	O
bm	O
m	O
labels	O
for	O
each	O
classm	O
see	O
unger	O
wysotski	O
i	O
e	O
this	O
hypothesis	O
is	O
true	O
if	O
for	O
each	O
classm	O
now	O
the	O
following	O
meta-decision	O
on	O
the	O
dominance	O
of	O
a	O
class	B
in	O
can	O
be	O
defined	O
as	O
if	O
there	O
exists	O
a	O
classm	O
where	O
is	O
true	O
thenm	O
dominates	O
in	O
the	O
interval	O
if	O
for	O
all	O
classes	B
appearing	O
in	O
in	O
this	O
case	O
the	O
interval	O
will	O
be	O
closed	O
too	O
a	O
new	O
test	O
with	O
another	O
attribute	O
is	O
if	O
neither	O
nor	O
occurs	O
the	O
interval	O
has	O
to	O
be	O
extended	O
by	O
the	O
next	O
example	B
of	O
the	O
a	O
majority	O
decision	O
will	O
be	O
made	O
is	O
derived	O
from	O
the	O
tchebyschev	O
inequality	O
by	O
supposing	O
a	O
bernoulli	O
distribution	O
of	O
class	B
i	O
e	O
is	O
true	O
if	O
the	O
complete	O
confidence	O
interval	O
lies	O
above	O
the	O
predefined	O
threshold	O
and	O
order	O
of	O
the	O
current	O
attribute	O
if	O
there	O
are	O
no	O
more	O
examples	O
for	O
a	O
further	O
extension	O
of	O
taking	O
into	O
account	O
this	O
confidence	O
interval	O
the	O
hypotheses	O
and	O
are	O
tested	O
by	O
cm	O
cm	O
cm	O
g	O
g	O
c	O
closed	O
the	O
corresponding	O
path	O
of	O
the	O
tree	O
is	O
terminated	O
the	O
threshold	O
necessary	O
will	O
be	O
tested	O
against	O
m	O
d	O
e	O
the	O
complete	O
confidence	O
interval	O
is	O
less	O
than	O
the	O
hypothesis	O
is	O
true	O
then	O
no	O
class	B
dominates	O
in	O
is	O
f	O
h	O
k	O
f	O
g	O
f	O
f	O
k	O
g	O
e	O
f	O
f	O
g	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
merging	O
is	O
removed	O
if	O
the	O
inequality	O
yield	O
the	O
leaf	O
nodes	O
of	O
the	O
decision	O
tree	O
the	O
same	O
rule	O
is	O
applied	O
for	O
adjacent	O
intervals	O
where	O
no	O
class	B
dominates	O
and	O
which	O
contain	O
identical	O
remaining	O
classes	B
due	O
to	O
the	O
following	O
with	O
the	O
same	O
class	B
label	O
can	O
be	O
merged	O
the	O
resultant	O
intervals	O
is	O
the	O
total	O
number	O
of	O
different	O
class	B
labels	O
occurring	O
in	O
adjacent	O
intervals	O
k	O
elimination	O
procedure	O
a	O
class	B
within	O
an	O
interval	O
g	O
is	O
satisfied	O
wherek	O
cm	O
class	B
will	O
be	O
omitted	O
if	O
its	O
probability	O
in	O
distribution	O
of	O
all	O
classes	B
occurring	O
in	O
these	O
resultant	O
intervals	O
yield	O
the	O
intermediate	O
all	O
terminated	O
note	O
that	O
a	O
majority	O
decision	O
is	O
made	O
at	O
a	O
node	O
if	O
because	O
of	O
a	O
too	O
small	O
nodes	O
in	O
the	O
construction	O
of	O
the	O
decision	O
treefor	O
which	O
further	O
branching	O
will	O
be	O
performed	O
every	O
intermediate	O
node	O
becomes	O
the	O
start	O
node	O
for	O
a	O
further	O
iteration	O
step	O
repeating	O
the	O
steps	O
from	O
sections	O
to	O
the	O
algorithm	O
stops	O
when	O
all	O
intermediate	O
nodes	O
are	O
a	O
is	O
less	O
than	O
the	O
value	O
of	O
an	O
assumed	O
constant	O
no	O
estimation	O
of	O
probability	O
can	O
be	O
done	O
discrete	O
unordered	O
attributes	B
to	O
distinguish	O
between	O
the	O
different	O
types	O
of	O
attributes	B
the	O
program	O
needs	O
a	O
special	O
input	B
vector	O
the	O
algorithm	O
for	O
handling	O
unordered	O
discrete	O
valued	O
attributes	B
is	O
similar	O
to	O
that	O
described	O
in	O
sections	O
to	O
apart	O
from	O
interval	O
construction	O
instead	O
of	O
intervals	O
discrete	O
points	O
on	O
the	O
axis	O
of	O
the	O
current	O
attribute	O
have	O
to	O
be	O
considered	O
all	O
examples	O
with	O
the	O
same	O
value	O
of	O
the	O
current	O
discrete	O
attribute	O
are	O
related	O
to	O
one	O
point	O
on	O
the	O
axis	O
for	O
each	O
point	O
the	O
hypotheses	O
and	O
will	O
be	O
tested	O
and	O
the	O
corresponding	O
actions	O
and	O
performed	O
respectively	O
if	O
neither	O
nor	O
is	O
true	O
a	O
majority	O
decision	O
will	O
be	O
made	O
this	O
approach	O
also	O
allows	O
handling	O
mixed	O
and	O
continuous	O
valued	O
attributes	B
probability	O
threshold	O
and	O
confidence	O
as	O
can	O
be	O
seen	O
from	O
the	O
above	O
two	O
parameters	O
affect	O
the	O
tree	O
construction	O
process	O
the	O
first	O
for	O
accept	O
a	O
node	O
and	O
the	O
second	O
is	O
a	O
predefined	O
confidence	O
level	O
the	O
tree	O
is	O
pre-pruned	O
at	O
should	O
depend	O
on	O
the	O
training	O
pruning	B
set	O
and	O
determines	O
the	O
accuracy	B
of	O
the	O
approximation	O
of	O
the	O
class	B
hyperplane	O
i	O
e	O
the	O
admissible	O
error	B
rate	I
the	O
higher	O
the	O
degree	O
of	O
overlapping	O
of	O
class	B
regions	O
in	O
the	O
feature	O
space	O
the	O
less	O
the	O
threshold	O
has	O
to	O
be	O
for	O
getting	O
a	O
reasonable	O
classification	B
result	O
the	O
accuracy	B
of	O
the	O
approximation	O
and	O
simultaneously	O
the	O
complexity	O
of	O
the	O
resulting	O
tree	O
can	O
be	O
controlled	O
by	O
the	O
user	O
in	O
addition	O
to	O
in	O
a	O
class	B
dependent	O
manner	O
taking	O
into	O
account	O
different	O
costs	B
for	O
misclassification	O
of	O
different	O
classes	B
with	O
other	O
words	O
the	O
influence	O
of	O
a	O
given	O
cost	B
matrix	I
can	O
be	O
taken	O
into	O
account	O
during	O
training	O
if	O
the	O
different	O
costs	B
for	O
misclassification	O
can	O
be	O
reflected	O
by	O
a	O
class	B
dependent	O
threshold	O
vector	O
one	O
approach	O
has	O
been	O
adopted	O
by	O
is	O
a	O
predefined	O
threshold	O
if	O
the	O
conditional	O
probability	O
of	O
a	O
class	B
exceeds	O
the	O
threshold	O
that	O
node	O
the	O
choice	O
of	O
therefore	O
by	O
selecting	O
the	O
value	O
of	O
a	O
constant	O
g	O
of	O
the	O
cost	B
matrix	I
will	O
be	O
summed	O
up	O
every	O
column	O
bc	O
the	O
threshold	O
of	O
that	O
class	B
relating	O
to	O
the	O
column	O
for	O
which	O
has	O
to	O
be	O
chosen	O
by	O
the	O
user	O
like	O
in	O
the	O
case	O
of	O
a	O
constant	O
threshold	O
the	O
other	O
thresholds	O
c	O
will	O
be	O
computed	O
by	O
the	O
formula	O
the	O
algorithm	O
allows	O
to	O
choose	O
the	O
threshold	O
is	O
a	O
maximum	O
f	O
h	O
g	O
f	O
g	O
sec	O
statlog	B
s	O
ml	O
algorithms	O
are	O
proportional	O
to	O
their	O
corresponding	O
column	O
sums	O
of	O
the	O
cost	B
matrix	I
which	O
can	O
be	O
interpreted	O
as	O
a	O
penalty	O
measure	B
for	O
misclassification	O
into	O
those	O
classes	B
for	O
estimating	O
the	O
appropriate	O
class	B
the	O
better	O
the	O
demanded	O
quality	O
of	O
estimation	O
and	O
the	O
worse	O
the	O
ability	O
to	O
separate	O
intervals	O
since	O
the	O
algorithm	O
is	O
enforced	O
to	O
construct	O
large	O
intervals	O
in	O
order	O
to	O
get	O
sufficient	O
statistics	O
from	O
experience	O
should	O
be	O
set	O
to	O
one	O
thus	O
all	O
values	O
of	O
the	O
class	B
dependent	O
thresholds	O
compared	O
with	O
the	O
threshold	O
the	O
confidence	O
level	O
probability	O
has	O
an	O
inversely	O
proportional	O
effect	O
the	O
less	O
the	O
value	O
of	O
and	O
a	O
suitable	O
approach	O
for	O
the	O
automatically	O
choosing	O
the	O
parameters	O
available	O
therefore	O
a	O
program	O
for	O
varying	O
the	O
parameter	O
between	O
by	O
default	B
and	O
between	O
by	O
default	B
is	O
used	O
to	O
predefine	O
the	O
best	O
parameter	O
combination	O
i	O
e	O
that	O
which	O
gives	O
the	O
minimum	O
cost	O
error	B
rate	I
respectively	O
on	O
a	O
test	B
set	I
however	O
this	O
procedure	O
may	O
be	O
computationally	O
expensive	O
in	O
relation	O
to	O
the	O
number	O
of	O
attributes	B
and	O
the	O
size	O
of	O
data	O
set	O
in	O
steps	O
of	O
and	O
is	O
not	O
and	O
bayes	B
tree	I
this	O
is	O
a	O
bayesian	O
approach	O
to	O
decision	B
trees	I
that	O
is	O
described	O
by	O
buntine	O
and	O
is	O
available	O
in	O
the	O
ind	B
package	I
it	O
is	O
based	O
on	O
a	O
full	O
bayesian	O
approach	O
as	O
such	O
it	O
requires	O
the	O
specification	O
of	O
prior	O
class	B
probabilities	O
based	O
on	O
empirical	O
class	B
proportions	O
and	O
a	O
probability	O
model	O
for	O
the	O
decision	O
tree	O
a	O
multiplicative	O
probability	O
model	O
for	O
the	O
probability	O
of	O
a	O
tree	O
is	O
adopted	O
using	O
this	O
form	O
simplifies	O
the	O
problem	O
of	O
computing	O
tree	O
probabilities	O
and	O
the	O
decision	O
to	O
grow	O
a	O
tree	O
from	O
a	O
particular	O
node	O
may	O
then	O
be	O
based	O
on	O
the	O
increase	O
in	O
probability	O
of	O
the	O
resulting	O
tree	O
thus	O
using	O
only	O
information	O
local	O
to	O
that	O
node	O
of	O
all	O
potential	O
splits	O
at	O
that	O
node	O
that	O
split	O
is	O
chosen	O
which	O
increases	O
the	O
posterior	O
probability	O
of	O
the	O
tree	O
by	O
the	O
greatest	O
amount	O
post-pruning	O
is	O
done	O
by	O
using	O
the	O
same	O
principle	O
i	O
e	O
choosing	O
the	O
cut	B
that	O
maximises	O
the	O
posterior	O
probability	O
of	O
the	O
resulting	O
tree	O
of	O
all	O
those	O
tree	O
structures	O
resulting	O
from	O
pruning	B
a	O
node	O
from	O
the	O
given	O
tree	O
choose	O
that	O
which	O
has	O
maximum	O
posterior	O
probability	O
an	O
alternative	O
to	O
post-pruning	O
is	O
to	O
smooth	O
class	B
probabilities	O
as	O
an	O
example	B
is	O
dropped	O
down	O
the	O
tree	O
it	O
goes	O
through	O
various	O
nodes	O
the	O
class	B
probabilities	O
of	O
each	O
node	O
visited	O
contribute	O
to	O
the	O
final	O
class	B
probabilities	O
a	O
weighted	O
sum	O
so	O
that	O
the	O
final	O
class	B
probabilities	O
inherit	O
probabilities	O
evaluated	O
higher	O
up	O
the	O
tree	O
this	O
stabilises	O
the	O
class	B
probability	O
estimates	O
reduces	O
their	O
variance	O
at	O
the	O
expense	O
of	O
introducing	O
bias	B
costs	B
may	O
be	O
included	O
in	O
learning	O
and	O
testing	O
via	O
a	O
utility	O
function	O
for	O
each	O
class	B
utility	O
is	O
the	O
negative	O
of	O
the	O
cost	O
for	O
the	O
two-class	O
case	O
rule-learning	B
algorithms	O
this	O
algorithm	O
of	O
clark	O
and	O
niblett	O
s	O
was	O
sketched	O
earlier	O
it	O
aims	O
to	O
modify	O
the	O
basic	O
aq	B
algorithm	O
of	O
michalski	O
in	O
such	O
a	O
way	O
as	O
to	O
equip	O
it	O
to	O
cope	O
with	O
noise	B
and	O
other	O
complications	O
in	O
the	O
data	O
in	O
particular	O
during	O
its	O
search	O
for	O
good	O
complexes	O
does	O
not	O
automatically	O
remove	O
from	O
consideration	O
a	O
candidate	O
that	O
is	O
found	O
to	O
include	O
one	O
or	O
more	O
negative	O
example	B
rather	O
it	O
retains	O
a	O
set	O
of	O
complexes	O
in	O
its	O
search	O
that	O
is	O
evaluated	O
statistically	O
as	O
covering	O
a	O
large	O
number	O
of	O
examples	O
of	O
a	O
given	O
class	B
and	O
few	O
of	O
other	O
classes	B
moreover	O
the	O
manner	O
in	O
which	O
the	O
search	O
is	O
conducted	O
is	O
general-to-specific	B
each	O
trial	O
specialisation	O
step	O
takes	O
the	O
form	O
of	O
either	O
adding	O
a	O
new	O
conjunctive	O
term	O
or	O
removing	O
a	O
disjunctive	O
one	O
having	O
found	O
a	O
good	O
complex	B
the	O
algorithm	O
removes	O
those	O
examples	O
it	O
f	O
is	O
empty	O
then	O
stop	O
and	O
return	O
rule	O
list	O
and	O
add	O
the	O
rule	O
if	O
best	O
cpx	O
then	O
is	O
the	O
most	O
common	O
class	B
of	O
examples	O
covered	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
covers	O
from	O
the	O
training	B
set	I
and	O
adds	O
the	O
rule	O
if	O
then	O
predict	O
to	O
the	O
end	O
of	O
the	O
rule	O
list	O
the	O
process	O
terminates	O
for	O
each	O
given	O
class	B
when	O
no	O
more	O
acceptable	O
complexes	O
can	O
be	O
found	O
clark	O
niblett	O
s	O
algorithm	O
has	O
the	O
following	O
main	O
features	B
the	O
dependence	O
on	O
specific	O
training	O
examples	O
during	O
search	O
feature	O
of	O
the	O
aq	B
algorithm	O
is	O
removed	O
it	O
combines	O
the	O
efficiency	O
and	O
ability	O
to	O
cope	O
with	O
noisy	B
data	I
of	O
decisiontree	O
learning	O
with	O
the	O
if-then	O
rule	O
form	O
and	O
flexible	O
search	O
strategy	O
of	O
the	O
aq	B
family	O
it	O
contrasts	O
with	O
other	O
approaches	O
to	O
modify	O
aq	B
to	O
handle	O
noise	B
in	O
that	O
the	O
basic	O
aq	B
algorithm	O
itself	O
is	O
generalised	O
rather	O
than	O
patched	O
with	O
additional	O
pre-	O
and	O
post-processing	O
techniques	O
and	O
it	O
produces	O
both	O
ordered	O
and	O
unordered	O
rules	O
and	O
output	B
a	O
set	O
of	O
rules	O
called	O
rule	O
list	O
the	O
core	O
of	O
is	O
the	O
procedure	O
as	O
follows	O
but	O
it	O
needs	O
to	O
use	O
a	O
sub-procedure	O
to	O
return	O
the	O
value	O
of	O
best	O
cpx	O
let	O
rule	O
list	O
be	O
the	O
empty	O
list	O
inputs	O
a	O
set	O
of	O
training	O
examples	O
let	O
best	O
cpx	O
be	O
the	O
best	O
complex	B
found	O
from	O
if	O
best	O
cpx	O
or	O
remove	O
the	O
examples	O
covered	O
by	O
best	O
cpx	O
from	O
classm	O
to	O
the	O
end	O
of	O
rule	O
list	O
wherem	O
by	O
best	O
cpx	O
re-enter	O
at	O
step	O
this	O
subprocedure	O
is	O
used	O
for	O
producing	O
ordered	O
rules	O
also	O
produces	O
a	O
set	O
of	O
unordered	O
rules	O
which	O
uses	O
a	O
slightly	O
different	O
procedure	O
to	O
produce	O
unordered	O
rules	O
the	O
above	O
procedure	O
is	O
repeated	O
for	O
each	O
class	B
in	O
turn	O
in	O
addition	O
in	O
step	O
only	O
the	O
positive	O
examples	O
should	O
be	O
removed	O
the	O
procedure	O
for	O
finding	O
the	O
best	O
complex	B
is	O
as	O
follows	O
let	O
the	O
set	O
star	O
contain	O
only	O
the	O
empty	O
complex	B
and	O
best	O
cpx	O
be	O
nil	O
let	O
selectors	O
be	O
the	O
set	O
of	O
all	O
possible	O
selectors	O
if	O
star	O
is	O
empty	O
then	O
return	O
the	O
current	O
best	O
cpx	O
specialise	O
all	O
complexes	O
in	O
star	O
as	O
newstar	O
which	O
is	O
the	O
set	O
star	O
y	O
d	O
b	O
selectors	O
and	O
remove	O
all	O
complexes	O
in	O
newstar	O
that	O
are	O
either	O
in	O
star	O
the	O
unspecialised	O
ones	O
or	O
are	O
null	O
for	O
every	O
complex	B
in	O
newstar	O
if	O
when	O
tested	O
on	O
then	O
replace	O
the	O
current	O
value	O
of	O
best	O
cpx	O
by	O
remove	O
criteria	O
when	O
tested	O
on	O
all	O
worst	O
complexes	O
from	O
newstar	O
until	O
the	O
size	O
of	O
newstar	O
is	O
below	O
the	O
user-defined	O
maximum	O
set	O
star	O
to	O
newstar	O
and	O
re-enter	O
at	O
step	O
is	O
statistically	O
significant	O
significance	O
and	O
better	O
than	O
goodness	O
best	O
cpx	O
according	O
to	O
user-defined	O
as	O
can	O
be	O
seen	O
from	O
the	O
algorithm	O
the	O
basic	O
operation	O
of	O
is	O
that	O
of	O
generating	O
a	O
complex	B
a	O
conjunct	O
of	O
attribute	O
tests	O
which	O
covers	O
is	O
satisfied	O
by	O
a	O
subset	O
of	O
the	O
training	O
examples	O
this	O
complex	B
forms	O
the	O
condition	O
part	O
of	O
a	O
production	O
rule	O
if	O
condition	O
k	O
then	O
class	B
where	O
class	B
is	O
the	O
most	O
common	O
class	B
in	O
the	O
examples	O
which	O
satisfy	O
the	O
condition	O
the	O
condition	O
is	O
a	O
conjunction	O
of	O
selectors	O
each	O
of	O
which	O
represents	O
a	O
test	O
on	O
the	O
values	O
of	O
an	O
attribute	O
such	O
as	O
weatherwet	O
the	O
search	O
proceeds	O
in	O
both	O
aq	B
and	O
by	O
repeatedly	O
specialising	O
candidate	O
complexes	O
until	O
one	O
which	O
covers	O
a	O
large	O
number	O
of	O
examples	O
of	O
a	O
single	O
class	B
and	O
few	O
of	O
other	O
classes	B
is	O
located	O
details	O
of	O
each	O
search	O
are	O
outlined	O
below	O
sec	O
statlog	B
s	O
ml	O
algorithms	O
the	O
search	O
for	O
specialisations	O
the	O
algorithm	O
works	O
in	O
an	O
iterative	O
fashion	O
each	O
iteration	O
searching	O
for	O
a	O
complex	B
amples	O
of	O
the	O
current	O
class	B
are	O
called	O
positive	O
examples	O
and	O
the	O
other	O
examples	O
are	O
called	O
negative	O
examples	O
the	O
complex	B
must	O
be	O
both	O
predictive	O
and	O
reliable	O
as	O
determined	O
by	O
s	O
evaluation	O
functions	O
having	O
found	O
a	O
good	O
complex	B
those	O
examples	O
it	O
covers	O
are	O
covering	O
a	O
large	O
number	O
of	O
examples	O
of	O
a	O
single	O
classm	O
and	O
few	O
of	O
other	O
classes	B
exremoved	O
from	O
the	O
training	B
set	I
and	O
the	O
rule	O
if	O
complex	B
then	O
classm	O
is	O
added	O
to	O
the	O
end	O
of	O
the	O
rule	O
list	O
this	O
greedy	O
process	O
iterates	O
until	O
no	O
more	O
satisfactory	O
complexes	O
can	O
be	O
found	O
to	O
generate	O
a	O
single	O
rule	O
first	O
starts	O
with	O
the	O
most	O
general	O
rule	O
if	O
true	O
then	O
classc	O
all	O
examples	O
are	O
class	B
c	O
where	O
c	O
is	O
the	O
current	O
class	B
then	O
searches	O
for	O
complexes	O
by	O
carrying	O
out	O
a	O
general-to-specific	B
beam	O
search	O
the	O
extent	O
of	O
the	O
beam	O
search	O
for	O
a	O
complex	B
can	O
be	O
regulated	O
by	O
controlling	O
the	O
width	O
number	O
of	O
complexes	O
explored	O
in	O
parallel	O
of	O
the	O
beam	O
at	O
each	O
stage	O
in	O
the	O
search	O
retains	O
a	O
size-limited	O
the	O
set	O
of	O
all	O
possible	O
selectors	O
with	O
the	O
current	O
star	O
eliminating	O
all	O
the	O
null	O
and	O
unchanged	O
elements	O
in	O
the	O
resulting	O
set	O
of	O
complexes	O
null	O
complex	B
is	O
one	O
that	O
contains	O
a	O
pair	O
of	O
this	O
set	O
carrying	O
out	O
a	O
beam	O
search	O
of	O
the	O
space	O
of	O
complexes	O
a	O
complex	B
is	O
specialised	O
by	O
adding	O
a	O
new	O
conjunctive	O
term	O
in	O
one	O
of	O
its	O
selector	B
each	O
complex	B
can	O
be	O
specialised	O
in	O
several	O
ways	O
and	O
generates	O
and	O
evaluates	O
all	O
such	O
specialisations	O
the	O
star	O
is	O
trimmed	O
after	O
completion	O
of	O
this	O
step	O
by	O
removing	O
its	O
lowest	O
ranking	O
elements	O
as	O
measured	O
by	O
an	O
evaluation	O
function	O
that	O
we	O
will	O
describe	O
shortly	O
set	O
or	O
star	O
of	O
complexes	O
explored	O
so	O
far	O
the	O
system	O
examines	O
only	O
specialisations	O
of	O
the	O
implementation	O
of	O
the	O
specialisation	O
step	O
in	O
is	O
to	O
repeatedly	O
intersect	O
incompatible	O
selectors	O
for	O
example	B
used	O
for	O
further	O
specialisation	O
in	O
the	O
goodness	O
is	O
a	O
measure	B
of	O
the	O
quality	O
of	O
the	O
search	O
heuristics	O
there	O
are	O
two	O
heuristics	O
used	O
in	O
the	O
search	O
for	O
the	O
best	O
complexes	O
and	O
both	O
can	O
be	O
tuned	O
by	O
the	O
user	O
depending	O
on	O
the	O
specific	O
domain	O
the	O
significance	O
level	O
and	O
the	O
goodness	O
measure	B
significance	O
is	O
an	O
absolute	O
threshold	O
such	O
that	O
any	O
complexes	O
below	O
the	O
threshold	O
will	O
not	O
be	O
considered	O
for	O
selecting	O
the	O
best	O
complex	B
they	O
are	O
still	O
complexes	O
so	O
it	O
is	O
used	O
to	O
order	O
the	O
complexes	O
that	O
are	O
above	O
the	O
significance	O
threshold	O
to	O
select	O
the	O
best	O
complex	B
several	O
difference	O
functions	O
can	O
be	O
chosen	O
to	O
guide	O
the	O
search	O
for	O
a	O
good	O
rule	O
in	O
the	O
system	O
for	O
example	B
number	O
of	O
correctly	O
classified	O
examples	O
divided	O
by	O
total	O
number	O
covered	O
this	O
is	O
the	O
traditional	O
aq	B
evaluation	O
function	O
entropy	B
similar	O
to	O
the	O
information	O
gain	O
measure	B
used	O
by	O
and	O
other	O
decision	O
tree	O
algorithms	O
is	O
the	O
number	O
of	O
classes	B
in	O
the	O
problem	O
g	O
where	O
fdgih	O
the	O
laplacian	O
error	O
estimate	O
cbkk	O
m	O
ck	O
e	O
d	O
is	O
the	O
total	O
number	O
of	O
examples	O
covered	O
by	O
the	O
rulek	O
the	O
intersection	O
of	O
set	O
with	O
set	O
examples	O
covered	O
by	O
the	O
rule	O
andd	O
for	O
example	B
using	O
to	O
abbreviate	O
is	O
the	O
set	O
if	O
we	O
now	O
remove	O
intersected	O
with	O
is	O
unchanged	O
elements	O
in	O
this	O
set	O
we	O
obtain	O
e	O
d	O
is	O
the	O
number	O
of	O
positive	O
k	O
d	O
g	O
k	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
uses	O
one	O
of	O
these	O
criteria	O
according	O
to	O
the	O
user	O
s	O
choice	O
to	O
order	O
the	O
goodness	O
of	O
rules	O
to	O
test	O
significance	O
uses	O
the	O
entropy	B
statistic	O
this	O
is	O
given	O
by	O
e	O
d	O
i	O
is	O
the	O
observed	O
frequency	O
distribution	O
of	O
examples	O
among	O
is	O
the	O
expected	O
frequency	O
distribution	O
of	O
the	O
same	O
number	O
of	O
examples	O
under	O
the	O
assumption	O
that	O
the	O
complex	B
selects	O
examples	O
logc	O
where	O
the	O
distribution	O
classes	B
satisfying	O
a	O
given	O
complex	B
and	O
randomly	O
this	O
is	O
taken	O
as	O
the	O
w	O
covered	O
examples	O
distributed	O
among	O
classes	B
r	O
with	O
the	O
same	O
probability	O
as	O
that	O
of	O
examples	O
in	O
the	O
entire	O
training	B
set	I
this	O
statistic	O
provides	O
an	O
information-theoretic	O
measure	B
of	O
the	O
distance	B
between	O
the	O
two	O
distributions	O
the	O
user	O
provides	O
a	O
threshold	O
of	O
significance	O
below	O
which	O
rules	O
are	O
rejected	O
missing	B
values	I
similar	O
to	O
newid	B
can	O
deal	O
with	O
unknown	O
or	O
don	O
t-care	O
values	O
during	O
rule	O
generation	O
a	O
similar	O
policy	O
of	O
handling	O
unknowns	O
and	O
don	O
t-cares	O
is	O
followed	O
unknowns	O
are	O
split	O
into	O
fractional	O
examples	O
and	O
don	O
t-cares	O
are	O
duplicated	O
each	O
rule	O
produced	O
by	O
is	O
associated	O
with	O
a	O
set	O
of	O
counts	O
which	O
corresponds	O
to	O
the	O
number	O
of	O
examples	O
covered	O
by	O
the	O
rule	O
belonging	O
to	O
each	O
class	B
strictly	O
speaking	O
for	O
the	O
ordered	O
rules	O
the	O
counts	O
attached	O
to	O
rules	O
when	O
writing	O
the	O
rule	O
set	O
should	O
be	O
those	O
encountered	O
during	O
rule	O
generation	O
however	O
for	O
unordered	O
rules	O
the	O
counts	O
to	O
attach	O
are	O
generated	O
after	O
rule	O
generation	O
in	O
a	O
second	O
pass	O
following	O
the	O
execution	O
policy	O
of	O
splitting	O
an	O
example	B
with	O
unknown	O
attribute	O
value	O
into	O
equal	O
fractions	O
for	O
each	O
value	O
rather	O
than	O
the	O
laplace-estimated	O
fractions	O
used	O
during	O
rule	O
generation	O
when	O
normally	O
executing	O
unordered	O
rules	O
without	O
unknowns	O
for	O
each	O
rule	O
which	O
fires	O
the	O
class	B
distribution	O
distribution	O
of	O
training	O
examples	O
among	O
classes	B
attached	O
to	O
the	O
rule	O
is	O
collected	O
these	O
are	O
then	O
summed	O
thus	O
a	O
training	O
example	B
satisfying	O
two	O
rules	O
with	O
attached	O
class	B
distributions	O
and	O
has	O
an	O
expected	O
distribution	O
which	O
desired	O
the	O
built-in	O
rule	O
executer	O
follows	O
the	O
first	O
strategy	O
example	B
is	O
classed	O
simply	O
hff	O
if	O
probabilistic	O
classification	B
is	O
being	O
predicted	O
or	O
m	O
results	O
inm	O
with	O
unordered	O
rules	O
an	O
attribute	O
test	O
whose	O
value	O
is	O
unknown	O
in	O
the	O
training	O
example	B
causes	O
the	O
example	B
to	O
be	O
examined	O
if	O
the	O
attribute	O
has	O
three	O
values	O
of	O
the	O
example	B
is	O
deemed	O
to	O
have	O
passed	O
the	O
test	O
and	O
thus	O
the	O
final	O
class	B
distribution	O
is	O
weighted	O
by	O
when	O
collected	O
a	O
similar	O
rule	O
later	O
will	O
again	O
cause	O
of	O
the	O
example	B
to	O
pass	O
the	O
test	O
a	O
don	O
t-care	O
value	O
is	O
always	O
deemed	O
to	O
have	O
passed	O
the	O
attribute	O
test	O
in	O
full	O
weight	O
the	O
normalisation	O
of	O
the	O
class	B
counts	O
means	O
that	O
an	O
example	B
with	O
a	O
don	O
t-care	O
can	O
only	O
count	O
as	O
a	O
single	O
example	B
during	O
testing	O
unlike	O
newid	B
where	O
it	O
may	O
count	O
as	O
representing	O
several	O
examples	O
with	O
ordered	O
rules	O
a	O
similar	O
policy	O
is	O
followed	O
except	O
after	O
a	O
rule	O
has	O
fired	O
absorbing	O
say	O
of	O
the	O
testing	O
example	B
only	O
the	O
remaining	O
are	O
sent	O
down	O
the	O
remainder	O
of	O
class	B
frequency	O
to	O
be	O
collected	O
but	O
a	O
second	O
the	O
rule	O
list	O
the	O
first	O
rule	O
will	O
cause	O
similar	O
rule	O
will	O
cause	O
class	B
frequency	O
to	O
be	O
collected	O
thus	O
the	O
fraction	O
of	O
the	O
example	B
gets	O
less	O
and	O
less	O
as	O
it	O
progresses	O
down	O
the	O
rule	O
list	O
a	O
don	O
t-care	O
value	O
always	O
h	O
g	O
m	O
sec	O
statlog	B
s	O
ml	O
algorithms	O
f	O
evaluations	O
wherek	O
requires	O
optimal	O
split	O
withk	O
passes	O
the	O
attribute	O
test	O
in	O
full	O
and	O
thus	O
no	O
fractional	O
example	B
remains	O
to	O
propagate	O
further	O
down	O
the	O
rule	O
list	O
numeric	O
attributes	B
and	O
rules	O
for	O
numeric	O
attributes	B
will	O
partition	O
the	O
values	O
into	O
two	O
subsets	O
and	O
test	O
which	O
subset	O
each	O
example	B
belongs	O
to	O
the	O
drawback	O
with	O
a	O
na	O
ve	O
implementation	O
of	O
this	O
is	O
that	O
it	O
is	O
the	O
number	O
of	O
attribute	O
values	O
breiman	O
et	O
al	O
proved	O
that	O
in	O
the	O
special	O
case	O
where	O
there	O
are	O
two	O
class	B
values	O
it	O
is	O
possible	O
to	O
find	O
an	O
f	O
comparisons	O
in	O
the	O
general	O
case	O
heuristic	O
methods	O
must	O
be	O
used	O
the	O
aq	B
algorithm	O
produces	O
an	O
unordered	O
set	O
of	O
rules	O
whereas	O
the	O
version	O
of	O
the	O
algorithm	O
used	O
in	O
statlog	B
produces	O
an	O
ordered	O
list	O
of	O
rules	O
unordered	O
rules	O
are	O
on	O
the	O
whole	O
more	O
comprehensible	O
but	O
require	O
also	O
that	O
they	O
are	O
qualified	O
with	O
some	O
numeric	O
confidence	O
measure	B
to	O
handle	O
any	O
clashes	O
which	O
may	O
occur	O
with	O
an	O
ordered	O
list	O
of	O
rules	O
clashes	O
cannot	O
occur	O
as	O
each	O
rule	O
in	O
the	O
list	O
is	O
considered	O
to	O
have	O
precedence	O
over	O
all	O
subsequent	O
rules	O
relation	O
between	O
and	O
aq	B
there	O
are	O
several	O
differences	O
between	O
these	O
two	O
algorithms	O
however	O
it	O
is	O
possible	O
to	O
show	O
that	O
strong	O
relationships	O
exist	O
between	O
the	O
two	O
so	O
much	O
so	O
that	O
simple	O
modifications	O
of	O
the	O
system	O
can	O
be	O
introduced	O
to	O
enable	O
it	O
to	O
emulate	O
the	O
behaviour	O
of	O
the	O
aq	B
algorithm	O
see	O
michalski	O
larson	O
aq	B
searches	O
for	O
rules	O
which	O
are	O
completely	O
consistent	O
with	O
the	O
training	O
data	O
whereas	O
may	O
prematurely	O
halt	O
specialisation	O
of	O
a	O
rule	O
when	O
no	O
further	O
rules	O
above	O
a	O
certain	O
threshold	O
of	O
statistical	B
significance	O
can	O
be	O
generated	O
via	O
specialisation	O
thus	O
the	O
behaviour	O
of	O
aq	B
in	O
this	O
respect	O
is	O
equivalent	O
to	O
setting	O
the	O
threshold	O
to	O
zero	O
when	O
generating	O
specialisations	O
of	O
a	O
rule	O
aq	B
considers	O
only	O
specialisations	O
which	O
exclude	O
a	O
specific	O
negative	O
example	B
from	O
the	O
coverage	O
of	O
a	O
rule	O
whereas	O
considers	O
all	O
specialisations	O
however	O
specialisations	O
generated	O
by	O
which	O
don	O
t	O
exclude	O
any	O
negative	O
examples	O
will	O
be	O
rejected	O
as	O
they	O
do	O
not	O
contribute	O
anything	O
to	O
the	O
predictive	O
accuracy	B
of	O
the	O
rule	O
thus	O
the	O
two	O
algorithms	O
search	O
the	O
same	O
space	O
in	O
different	O
ways	O
whereas	O
published	O
descriptions	O
of	O
aq	B
leave	O
open	O
the	O
choice	O
of	O
evaluation	O
function	O
to	O
use	O
during	O
search	O
the	O
published	O
norm	O
is	O
that	O
of	O
number	O
of	O
correctly	O
classified	O
examples	O
divided	O
by	O
total	O
examples	O
covered	O
the	O
original	O
algorithm	O
uses	O
entropy	B
as	O
its	O
evaluation	O
function	O
to	O
obtain	O
a	O
synthesis	O
of	O
the	O
two	O
systems	O
the	O
choice	O
of	O
evaluation	O
function	O
can	O
be	O
user-selected	O
during	O
start	O
of	O
the	O
system	O
aq	B
generates	O
order-independent	O
rules	O
whereas	O
generates	O
an	O
ordered	O
list	O
of	O
rules	O
to	O
modify	O
to	O
produce	O
order-independent	O
rules	O
requires	O
a	O
change	O
to	O
the	O
evaluation	O
function	O
and	O
a	O
change	O
to	O
the	O
way	O
examples	O
are	O
removed	O
from	O
the	O
training	B
set	I
between	O
iterations	O
of	O
the	O
complex-finding	O
algorithm	O
the	O
basic	O
searchalgorithm	O
remains	O
unchanged	O
itrule	B
the	O
hypotheses	O
during	O
decision	O
rule	O
construction	O
its	O
output	B
is	O
a	O
set	O
of	O
probability	O
rules	O
which	O
are	O
the	O
most	O
informative	O
selected	O
from	O
the	O
possible	O
rules	O
depending	O
on	O
the	O
training	O
data	O
goodman	O
smyth	O
s	O
itrule	B
algorithm	O
uses	O
a	O
function	O
called	O
the	O
to	O
rank	O
to	O
build	O
rules	O
it	O
keeps	O
a	O
ranked	O
list	O
of	O
the	O
algorithm	O
iterates	O
through	O
each	O
attribute	O
the	O
class	B
attribute	O
value	O
in	O
turn	O
best	O
rules	O
determined	O
to	O
that	O
point	O
of	O
the	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
th	O
rule	O
is	O
used	O
as	O
the	O
running	O
minimum	O
to	O
determine	O
whether	O
a	O
new	O
rule	O
should	O
be	O
inserted	O
into	O
the	O
rule	O
list	O
for	O
each	O
attribute	O
value	O
the	O
algorithm	O
must	O
find	O
all	O
possible	O
conditions	O
to	O
add	O
to	O
the	O
left	O
hand	O
side	O
of	O
an	O
over-general	O
rule	O
to	O
specialise	O
it	O
or	O
it	O
may	O
decide	O
to	O
drop	O
a	O
condition	O
to	O
generalise	O
an	O
over-specific	O
rule	O
the	O
rules	O
considered	O
are	O
those	O
limited	O
by	O
the	O
algorithm	O
execution	O
is	O
the	O
size	O
of	O
the	O
beam	O
search	O
the	O
of	O
minimum	O
running	O
value	O
which	O
prevents	O
the	O
algorithm	O
from	O
searching	O
a	O
large	O
three	O
points	O
should	O
be	O
noted	O
first	O
itrule	B
produces	O
rules	O
for	O
each	O
attribute	O
value	O
so	O
it	O
can	O
also	O
capture	O
the	O
dependency	O
relationships	O
between	O
attributes	B
between	O
attributes	B
and	O
classes	B
and	O
between	O
class	B
values	O
secondly	O
itrule	B
not	O
only	O
specialises	O
existing	O
rules	O
but	O
also	O
generalises	O
them	O
if	O
the	O
need	O
arises	O
specialisation	O
is	O
done	O
through	O
adding	O
conditions	O
to	O
the	O
left	O
hand	O
side	O
of	O
the	O
rule	O
and	O
generalisation	O
is	O
done	O
through	O
dropping	O
conditions	O
finally	O
itrule	B
only	O
deals	O
with	O
categorical	O
examples	O
so	O
it	O
generally	O
needs	O
to	O
convert	O
numeric	O
attributes	B
and	O
discrete	O
values	O
rule	O
space	O
such	O
estimates	O
can	O
be	O
reasonable	O
accurate	O
the	O
itrule	B
algorithms	O
uses	O
a	O
maximum	O
entropy	B
estimator	O
evaluation	O
function	O
the	O
and	O
be	O
an	O
attribute	O
with	O
values	O
let	O
be	O
an	O
attribute	O
with	O
values	O
in	O
the	O
set	O
the	O
is	O
a	O
method	O
for	O
calculating	O
the	O
information	O
content	O
in	O
it	O
is	O
g	O
of	O
attribute	O
given	O
the	O
value	O
of	O
attribute	O
oc	O
c	O
c	O
kbc	O
c	O
c	O
g	O
is	O
the	O
a	O
priori	O
g	O
is	O
the	O
conditional	O
probability	O
of	O
given	O
where	O
c	O
these	O
can	O
normally	O
be	O
estimated	O
from	O
the	O
relative	O
probability	O
of	O
frequency	O
of	O
the	O
value	O
of	O
when	O
the	O
distribution	O
is	O
uniform	B
and	O
the	O
data	O
set	O
is	O
sufficient	O
exke	O
e	O
ex	O
and	O
where	O
are	O
parameters	O
of	O
an	O
initial	O
density	O
estimate	O
k	O
in	O
the	O
data	O
and	O
event	O
c	O
c	O
c	O
d	O
g	O
can	O
be	O
interpreted	O
as	O
a	O
measure	B
of	O
the	O
simplicity	O
of	O
the	O
of	O
are	O
zero	O
the	O
first	O
term	O
c	O
hypothesis	O
that	O
cross-entropy	B
is	O
is	O
dependent	O
on	O
the	O
event	O
of	O
the	O
variable	O
with	O
the	O
condition	O
known	O
to	O
measure	B
the	O
goodness	O
of	O
fit	O
between	O
two	O
distributions	O
see	O
goodman	O
smyth	O
rule	O
searching	O
strategy	O
itrule	B
performs	O
both	O
generalisation	O
and	O
specialisation	O
it	O
starts	O
with	O
a	O
model	O
driven	O
strategy	O
much	O
like	O
but	O
its	O
rules	O
all	O
have	O
probability	O
attached	O
from	O
the	O
beginning	O
so	O
a	O
universal	O
rule	O
will	O
be	O
is	O
the	O
total	O
number	O
of	O
is	O
related	O
to	O
the	O
second	O
term	O
the	O
above	O
is	O
true	O
because	O
it	O
takes	O
into	O
account	O
the	O
fact	O
that	O
the	O
probabilities	O
of	O
other	O
values	O
the	O
average	O
information	O
content	O
is	O
therefore	O
defined	O
as	O
c	O
c	O
and	O
c	O
is	O
the	O
number	O
of	O
the	O
is	O
equal	O
to	O
the	O
cross-entropy	B
f	O
f	O
g	O
g	O
g	O
g	O
f	O
e	O
g	O
g	O
g	O
g	O
g	O
g	O
sec	O
beyond	O
the	O
complexity	O
barrier	O
into	O
the	O
rule	O
list	O
this	O
process	O
continues	O
until	O
no	O
rule	O
can	O
be	O
produced	O
to	O
cover	B
remaining	O
examples	O
to	O
it	O
calculates	O
the	O
g	O
m	O
it	O
requires	O
namely	O
the	O
increase	O
in	O
simplicity	O
is	O
sufficiently	O
compensated	O
for	O
i	O
dk	O
then	O
d	O
with	O
probabilityf	O
if	O
dk	O
to	O
specialise	O
a	O
rule	O
such	O
as	O
one	O
with	O
current	O
c	O
all	O
possible	O
values	O
of	O
g	O
for	O
attributem	O
if	O
then	O
it	O
insert	O
the	O
new	O
rule	O
with	O
specialised	O
conditionm	O
generalise	O
a	O
rule	O
with	O
the	O
current	O
value	O
gj	O
c	O
where	O
yk	O
my	O
by	O
the	O
decrease	O
in	O
cross-entropy	B
c	O
m	O
m	O
beyond	O
the	O
complexity	O
barrier	O
all	O
ml	O
designers	O
whether	O
rule-oriented	O
or	O
tree-oriented	O
agree	O
that	O
to	O
the	O
extent	O
that	O
the	O
data	O
permits	O
mental	B
fit	I
is	O
an	O
indispensible	O
hall-mark	O
thus	O
discussing	O
requirements	O
of	O
rule	O
learning	O
systems	O
clark	O
niblett	O
state	O
that	O
for	O
the	O
sake	O
of	O
comprehensibility	B
the	O
induced	O
rules	O
should	O
be	O
as	O
short	O
as	O
possible	O
however	O
when	O
noise	B
is	O
present	O
overfitting	B
can	O
lead	O
to	O
long	O
rules	O
thus	O
to	O
induce	O
short	O
rules	O
one	O
must	O
usually	O
relax	O
the	O
requirement	O
that	O
the	O
induced	O
rules	O
be	O
consistent	O
with	O
all	O
the	O
training	O
data	O
such	O
measures	B
constitute	O
the	O
analogue	O
of	O
pruning	B
of	O
trees	O
but	O
tree	O
pruning	B
and	O
rule-set	O
simplification	O
measures	B
may	O
encounter	O
complexity	O
barriers	O
that	O
limit	O
how	O
much	O
can	O
be	O
done	O
in	O
the	O
direction	O
of	O
mental	B
fit	I
while	O
retaining	O
acceptable	O
accuracy	B
when	O
this	O
occurs	O
are	O
there	O
other	O
directions	O
in	O
which	O
descriptive	O
adequacy	O
can	O
still	O
be	O
sought	O
trees	O
into	O
rules	O
a	O
tree	O
that	O
after	O
pruning	B
still	O
remains	O
too	O
big	O
to	O
be	O
comprehensible	O
is	O
a	O
sign	O
that	O
a	O
more	O
powerful	O
description	O
language	O
is	O
required	O
a	O
modest	O
but	O
often	O
effective	O
step	O
starts	O
by	O
recognising	O
that	O
there	O
is	O
no	O
intrinsic	O
difference	O
in	O
expressive	O
power	O
between	O
rulesets	O
and	O
trees	O
yet	O
rule	O
languages	O
seem	O
to	O
lend	O
themselves	O
more	O
to	O
user-friendliness	O
a	O
successful	O
exploitation	O
of	O
this	O
idea	O
takes	O
the	O
form	O
of	O
a	O
compressive	O
re-organisation	O
of	O
induced	O
trees	O
into	O
rule-sets	O
quinlan	O
s	O
trees-into-rules	B
algorithm	O
his	O
book	O
for	O
the	O
most	O
recent	O
version	O
starts	O
with	O
the	O
set	O
formed	O
from	O
a	O
decision	O
tree	O
by	O
identifying	O
each	O
root-to-leaf	O
path	O
with	O
a	O
rule	O
each	O
rule	O
is	O
simplified	O
by	O
successively	O
dropping	O
conditions	O
in	O
the	O
specific-to-general	B
style	O
illustrated	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
the	O
difference	O
lies	O
in	O
the	O
sophistication	O
of	O
criteria	O
used	O
for	O
retracting	O
a	O
trial	O
generalisation	O
when	O
it	O
is	O
found	O
to	O
result	O
in	O
inclusion	O
of	O
cases	O
not	O
belonging	O
to	O
the	O
rule	O
s	O
decision	B
class	B
in	O
the	O
noise-free	O
taxonomy	B
problem	O
of	O
the	O
earlier	O
tutorial	O
example	B
a	O
single	O
false	O
positive	O
was	O
taken	O
to	O
bar	O
dropping	O
the	O
given	O
condition	O
as	O
with	O
and	O
some	O
other	O
rule-learners	O
a	O
statistical	B
criterion	O
is	O
substituted	O
quinlan	O
s	O
is	O
based	O
on	O
forming	O
from	O
the	O
training	B
set	I
pessimistic	O
estimates	O
of	O
the	O
accuracy	B
that	O
a	O
candidate	O
rule	O
would	O
show	O
on	O
a	O
test	B
set	I
when	O
specific-to-general	B
simplification	O
has	O
run	O
its	O
course	O
for	O
each	O
class	B
in	O
turn	O
a	O
final	O
scan	O
is	O
made	O
over	O
each	O
ruleset	O
for	O
any	O
that	O
in	O
the	O
context	O
of	O
the	O
other	O
rules	O
are	O
not	O
contributing	O
to	O
the	O
ruleset	O
s	O
accuracy	B
any	O
such	O
passengers	O
are	O
dropped	O
the	O
end	O
of	O
this	O
stage	O
leaves	O
as	O
many	O
subsets	O
of	O
if-then	O
rules	O
covers	O
in	O
the	O
earlier	O
terminology	O
as	O
there	O
are	O
classes	B
i	O
e	O
one	O
subset	O
for	O
each	O
class	B
these	O
subsets	O
are	O
then	O
ordered	O
prior	O
to	O
use	O
for	O
classifying	O
new	O
cases	O
the	O
ordering	O
principle	O
first	O
applies	O
the	O
subset	O
which	O
on	O
the	O
g	O
m	O
m	O
g	O
m	O
g	O
m	O
m	O
g	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
training	B
set	I
gives	O
fewest	O
false	O
positives	O
the	O
one	O
with	O
most	O
false	O
positives	O
is	O
the	O
last	O
to	O
be	O
applied	O
by	O
that	O
time	B
some	O
of	O
the	O
false-positive	O
errors	O
that	O
it	O
could	O
have	O
made	O
have	O
been	O
pre-empted	O
by	O
other	O
rule-sets	O
finally	O
a	O
default	B
class	B
is	O
chosen	O
to	O
which	O
all	O
cases	O
which	O
do	O
not	O
match	O
any	O
rule	O
are	O
to	O
be	O
assigned	O
this	O
is	O
calculated	O
from	O
the	O
frequency	O
statistics	O
of	O
such	O
left-overs	O
in	O
the	O
training	B
set	I
whichever	O
class	B
appears	O
most	O
frequently	O
among	O
these	O
left-overs	O
is	O
selected	O
as	O
the	O
default	B
classification	B
rule-structured	O
classifiers	O
generated	O
in	O
this	O
way	O
turn	O
out	O
to	O
be	O
smaller	O
and	O
better	O
in	O
mental	B
fit	I
than	O
the	O
trees	O
from	O
which	O
the	O
process	O
starts	O
yet	O
accuracy	B
is	O
found	O
to	O
be	O
fully	O
preserved	O
when	O
assayed	O
against	O
test	O
data	O
a	O
particularly	O
interesting	O
feature	O
of	O
quinlan	O
s	O
account	O
for	O
which	O
space	O
allows	O
no	O
discussion	O
here	O
is	O
his	O
detailed	O
illustration	O
of	O
the	O
minimum	O
description	O
length	O
principle	O
according	O
to	O
which	O
the	O
storage	B
costs	B
of	O
rulesets	O
and	O
of	O
their	O
exceptions	O
are	O
expressed	O
in	O
a	O
common	O
information-theoretic	O
coinage	O
this	O
is	O
used	O
to	O
address	O
a	O
simplification	O
problem	O
in	O
building	O
rule-sets	O
that	O
is	O
essentially	O
similar	O
to	O
the	O
regulation	O
of	O
the	O
pruning	B
process	O
in	O
decision	B
trees	I
the	O
trade-off	O
in	O
each	O
case	O
is	O
between	O
complexity	O
and	O
predictive	O
accuracy	B
manufacturing	O
new	O
attributes	B
if	O
a	O
user-friendly	O
description	O
still	O
cannot	O
be	O
extracted	O
more	O
radical	O
treatment	O
may	O
be	O
required	O
the	O
data-description	O
language	O
s	O
vocabulary	O
may	O
need	O
extending	O
with	O
new	O
combinations	O
formed	O
from	O
the	O
original	O
primitive	O
attributes	B
the	O
effects	O
can	O
be	O
striking	O
consider	O
the	O
problem	O
of	O
classifying	O
as	O
illegal	O
or	O
legal	O
the	O
chessboard	O
positions	O
formed	O
by	O
randomly	O
placing	O
the	O
three	O
pieces	O
white	O
king	O
white	O
rook	O
and	O
black	O
king	O
combinatorially	O
there	O
are	O
move	O
approximately	O
two	O
thirds	O
of	O
the	O
positions	O
are	O
then	O
illegal	O
two	O
or	O
more	O
pieces	O
may	O
have	O
been	O
placed	O
on	O
the	O
same	O
square	O
or	O
the	O
two	O
kings	O
may	O
be	O
diagonally	O
or	O
directly	O
adjacent	O
additionally	O
positions	O
in	O
which	O
the	O
black	O
king	O
is	O
in	O
check	O
from	O
the	O
white	O
rook	O
are	O
also	O
illegal	O
that	O
it	O
is	O
white	O
to	O
move	O
positions	O
or	O
assume	O
that	O
it	O
is	O
white	O
s	O
turn	O
to	O
a	O
problem	O
is	O
presented	O
for	O
inductive	O
analysis	O
as	O
a	O
training	B
set	I
of	O
n	O
cases	O
sampled	O
randomly	O
from	O
the	O
total	O
space	O
of	O
possible	O
placements	O
as	O
shown	O
in	O
table	O
given	O
sufficiently	O
large	O
n	O
table	O
constitutes	O
what	O
mccarthy	O
and	O
hayes	O
termed	O
an	O
epistemologically	B
adequate	I
representation	O
it	O
supplies	O
whatever	O
facts	O
are	O
in	O
principle	O
needed	O
to	O
obtain	O
solutions	O
but	O
for	O
decision-tree	O
learning	O
the	O
representation	O
is	O
not	O
heuristically	B
adequate	I
michie	O
bain	O
applied	O
a	O
state-of-the-art	O
decision-tree	O
learner	O
of	O
roughly	O
similar	O
power	O
to	O
to	O
training	O
sets	O
of	O
examples	O
the	O
resulting	O
tree	O
performed	O
on	O
test	O
data	O
with	O
only	O
accuracy	B
not	O
differing	O
significantly	O
from	O
that	O
achievable	O
by	O
making	O
the	O
default	B
conjecture	O
illegal	O
for	O
every	O
case	O
the	O
next	O
step	O
was	O
to	O
augment	O
the	O
six	O
attributes	B
with	O
fifteen	O
new	O
ones	O
manufactured	O
by	O
forming	O
all	O
possible	O
pairwise	O
differences	O
among	O
the	O
original	O
six	O
with	O
the	O
augmented	O
attribute	O
set	O
two	O
random	O
partitions	O
of	O
a	O
file	O
of	O
cases	O
were	O
made	O
into	O
a	O
training	B
set	I
of	O
and	O
a	O
test	B
set	I
of	O
trees	O
of	O
and	O
accuracy	B
now	O
resulted	O
with	O
nodes	O
and	O
nodes	O
respectively	O
for	O
making	O
these	O
very	O
successful	O
constructions	O
the	O
algorithm	O
seized	O
on	O
just	O
six	O
attributes	B
all	O
newly	O
manufactured	O
namely	O
the	O
three	O
pairwise	O
differences	O
among	O
attributes	B
and	O
and	O
the	O
three	O
among	O
attributes	B
and	O
in	O
this	O
way	O
even	O
though	O
in	O
a	O
verbose	O
and	O
contorted	O
style	O
it	O
was	O
able	O
to	O
express	O
in	O
decision-tree	O
language	O
certain	O
key	O
sec	O
beyond	O
the	O
complexity	O
barrier	O
table	O
the	O
six	O
attributes	B
encode	O
a	O
position	O
according	O
to	O
the	O
scheme	O
filebk	O
rank	O
filewr	O
rankwr	O
filebk	O
rankbk	O
id	O
no	O
n	O
class	B
yes	O
no	O
no	O
yes	O
yes	O
no	O
sub-descriptions	O
such	O
as	O
the	O
crucial	O
same-file	O
and	O
same-rank	O
relation	O
between	O
white	O
rook	O
and	O
black	O
king	O
whenever	O
one	O
of	O
these	O
relations	O
holds	O
it	O
is	O
a	O
good	O
bet	O
that	O
the	O
position	O
is	O
illegal	O
the	O
gain	O
in	O
classification	B
accuracy	B
is	O
impressive	O
yet	O
no	O
amount	O
of	O
added	O
training	O
data	O
could	O
inductively	O
refine	O
the	O
above	O
excellent	O
bet	O
into	O
a	O
certainty	O
the	O
reason	O
again	O
lies	O
with	O
persisting	O
limitations	O
of	O
the	O
description	O
language	O
to	O
define	O
the	O
cases	O
where	O
the	O
classifier	B
s	O
use	O
of	O
samefilewr	O
bk	O
and	O
samerankwr	O
bk	O
lets	O
it	O
down	O
one	O
needs	O
to	O
say	O
that	O
this	O
happens	O
if	O
and	O
only	O
if	O
the	O
wk	O
is	O
between	O
the	O
wr	O
and	O
bk	O
decision-tree	O
learning	O
with	O
attribute-set	O
augmented	O
as	O
described	O
can	O
patch	O
together	O
subtrees	O
to	O
do	O
duty	O
for	O
samefile	O
and	O
samerank	O
but	O
an	O
equivalent	O
feat	O
for	O
a	O
sophisticated	O
three-place	O
relation	O
such	O
as	O
between	O
is	O
beyond	O
the	O
expressive	O
powers	O
of	O
an	O
attribute-value	O
propositionallevel	O
language	O
moreover	O
the	O
decision-tree	O
learner	O
s	O
constructions	O
were	O
described	O
above	O
as	O
very	O
successful	O
on	O
purely	O
operational	O
grounds	O
of	O
accuracy	B
relative	O
to	O
the	O
restricted	O
amount	O
of	O
training	O
material	O
i	O
e	O
successful	O
in	O
predictivity	O
in	O
terms	O
of	O
descriptivity	O
the	O
trees	O
while	O
not	O
as	O
opaque	O
as	O
those	O
obtained	O
with	O
primitive	O
attributes	B
only	O
were	O
still	O
far	O
from	O
constituting	O
intelligible	O
theories	O
inherent	O
limits	O
of	O
propositional-level	O
learning	O
construction	O
of	O
theories	O
of	O
high	O
descriptivity	O
is	O
the	O
shared	O
goal	O
of	O
human	O
analysts	O
and	O
of	O
ml	O
yet	O
the	O
propositional	O
level	O
of	O
ml	O
is	O
too	O
weak	O
to	O
fully	O
solve	O
even	O
the	O
problem	O
here	O
illustrated	O
the	O
same	O
task	O
however	O
was	O
proved	O
to	O
be	O
well	O
within	O
the	O
powers	O
of	O
dr	O
jane	O
mitchella	O
gifted	O
and	O
experienced	O
human	O
data	O
analyst	O
on	O
the	O
academic	O
staff	O
of	O
strathclyde	O
university	O
and	O
of	O
a	O
predicate-logic	O
ml	O
system	O
belonging	O
to	O
the	O
inductive	B
logic	I
programming	I
family	O
described	O
in	O
chapter	O
the	O
two	O
independently	O
obtained	O
theories	O
were	O
complete	O
and	O
correct	O
one	O
theory-discovery	O
agent	O
was	O
human	O
namely	O
a	O
member	O
of	O
the	O
academic	O
staff	O
of	O
a	O
university	O
statistics	O
department	O
the	O
other	O
was	O
an	O
ilp	B
learner	O
based	O
on	O
muggleton	O
feng	O
s	O
golem	B
with	O
closed	O
world	O
specialization	O
enhancements	O
private	O
communication	O
in	O
essentials	O
the	O
two	O
theories	O
closely	O
approximated	O
to	O
the	O
one	O
shown	O
below	O
in	O
the	O
form	O
of	O
four	O
if-then	O
rules	O
these	O
are	O
here	O
given	O
in	O
english	O
after	O
back-interpretation	O
into	O
chess	O
terms	O
neither	O
of	O
the	O
learning	O
agents	O
had	O
any	O
knowledge	O
of	O
the	O
meaning	O
of	O
the	O
task	O
which	O
was	O
simply	O
presented	O
as	O
in	O
table	O
they	O
did	O
not	O
know	O
that	O
it	O
had	O
anything	O
to	O
do	O
with	O
chess	O
nor	O
even	O
with	O
objects	O
placed	O
on	O
plane	O
surfaces	O
the	O
background	B
knowledge	I
given	O
to	O
the	O
ilp	B
learner	O
was	O
similar	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
in	O
amount	O
to	O
that	O
earlier	O
given	O
to	O
xpertrule	B
in	O
the	O
form	O
of	O
manufactured	O
attributes	B
if	O
wr	O
and	O
bk	O
either	O
occupy	O
same	O
file	O
and	O
wk	O
is	O
not	O
directly	O
between	O
or	O
if	O
they	O
occupy	O
the	O
same	O
rank	O
and	O
wk	O
is	O
not	O
directly	O
between	O
then	O
the	O
position	O
is	O
illegal	O
if	O
wk	O
and	O
bk	O
either	O
are	O
vertically	O
adjacent	O
or	O
are	O
horizontally	O
adjacent	O
or	O
are	O
diagonally	O
adjacent	O
then	O
the	O
position	O
is	O
illegal	O
if	O
any	O
two	O
pieces	O
are	O
on	O
the	O
same	O
square	O
then	O
the	O
position	O
is	O
illegal	O
otherwise	O
the	O
position	O
is	O
legal	O
construction	O
of	O
this	O
theory	O
requires	O
certain	O
key	O
sub-concepts	O
notably	O
of	O
directly	O
between	O
definitions	O
were	O
invented	O
by	O
the	O
machine	O
learner	O
using	O
lower-level	O
concepts	O
such	O
as	O
lessthan	O
as	O
background	B
knowledge	I
directly	O
between	O
holds	O
among	O
the	O
three	O
co-ordinate	O
pairs	O
if	O
either	O
the	O
first	O
co-ordinates	O
are	O
all	O
equal	O
and	O
the	O
second	O
co-ordinates	O
are	O
in	O
ascending	O
or	O
descending	O
progression	O
or	O
the	O
second	O
co-ordinates	O
are	O
all	O
equal	O
and	O
the	O
first	O
co-ordinates	O
show	O
the	O
progression	O
bain	O
s	O
ilp	B
package	O
approached	O
the	O
relation	O
piece-wise	O
via	O
invention	O
of	O
between-file	O
and	O
between-rank	O
the	O
human	O
learner	O
doubtless	O
came	O
ready-equipped	O
with	O
some	O
at	O
least	O
of	O
the	O
concepts	O
that	O
the	O
ml	O
system	O
had	O
to	O
invent	O
none	O
the	O
less	O
with	O
unlimited	O
access	O
to	O
training	O
data	O
and	O
the	O
use	O
of	O
standard	O
statistical	B
analysis	O
and	O
tabulation	O
software	O
the	O
task	O
of	O
theory	O
building	O
still	O
cost	O
two	O
days	O
of	O
systematic	O
work	O
human	O
learners	O
given	O
hours	O
rather	O
than	O
days	O
constructed	O
only	O
partial	O
theories	O
falling	O
far	O
short	O
even	O
of	O
operational	O
adequacy	O
also	O
muggleton	O
s	O
h	O
bain	O
m	O
hayes-michie	O
j	O
e	O
and	O
michie	O
d	O
bain	O
s	O
new	O
work	O
has	O
the	O
further	O
interest	O
that	O
learning	O
takes	O
place	O
incrementally	O
by	O
successive	O
refinement	O
a	O
style	O
sometimes	O
referred	O
to	O
as	O
non-monotonic	O
generalisations	O
made	O
in	O
the	O
first	O
pass	O
through	O
training	O
data	O
yield	O
exceptions	O
when	O
challenged	O
with	O
new	O
data	O
as	O
exceptions	O
accumulate	O
they	O
are	O
themselves	O
generalised	O
over	O
to	O
yield	O
sub-theories	O
which	O
qualify	O
the	O
main	O
theory	O
these	O
refinements	O
are	O
in	O
turn	O
challenged	O
and	O
so	O
forth	O
to	O
any	O
desired	O
level	O
the	O
krk	O
illegality	O
problem	O
was	O
originally	O
included	O
in	O
statlog	B
s	O
datasets	O
in	O
the	O
interests	O
of	O
industrial	O
relevance	O
artificial	O
problems	O
were	O
not	O
retained	O
except	O
for	O
expository	O
purposes	O
no	O
connection	O
however	O
exists	O
between	O
a	O
data-set	O
s	O
industrial	O
importance	O
and	O
its	O
intrinsic	O
difficulty	O
all	O
of	O
the	O
ml	O
algorithms	O
tested	O
by	O
statlog	B
were	O
of	O
propositional	O
type	O
if	O
descriptive	O
adequacy	O
is	O
a	O
desideratum	O
none	O
can	O
begin	O
to	O
solve	O
the	O
krk-illegal	O
problem	O
it	O
would	O
be	O
a	O
mistake	O
however	O
to	O
assume	O
that	O
problems	O
of	O
complex	B
logical	O
structure	O
do	O
not	O
occur	O
in	O
industry	O
they	O
can	O
be	O
found	O
for	O
example	B
in	O
trouble-shooting	O
complex	B
circuitry	O
in	O
inferring	O
biological	O
activity	O
from	O
specifications	O
of	O
macromolecular	O
structure	O
in	O
the	O
pharmaceutical	O
industry	O
last	O
section	O
of	O
chapter	O
and	O
in	O
many	O
other	O
large	O
combinatorial	O
domains	O
as	O
inductive	B
logic	I
programming	I
matures	O
and	O
assimilates	O
techniques	O
from	O
probability	O
and	O
statistics	O
industrial	O
need	O
seems	O
set	O
to	O
explore	O
these	O
more	O
powerful	O
ml	O
description	O
languages	O
sec	O
beyond	O
the	O
complexity	O
barrier	O
a	O
human-machine	O
compromise	O
structured	B
induction	I
in	O
industrial	O
practice	O
more	O
mileage	O
can	O
be	O
got	O
from	O
decision-tree	O
and	O
rule	O
learning	O
than	O
the	O
foregoing	O
account	O
might	O
lead	O
one	O
to	O
expect	O
comparative	B
trials	I
like	O
statlog	B
s	O
having	O
a	O
scientific	O
end	O
in	O
view	O
necessarily	O
exclude	O
approaches	O
in	O
which	O
the	O
algorithm	O
s	O
user	O
intervenes	O
interactively	O
to	O
help	O
it	O
the	O
inability	O
of	O
propositional	O
learning	O
to	O
invent	O
new	O
attributes	B
can	O
be	O
by-passed	O
in	O
practical	O
contexts	O
where	O
human-computer	O
interaction	O
can	O
plug	O
the	O
gap	O
from	O
this	O
an	O
approach	O
known	O
as	O
structured	B
induction	I
has	O
come	O
to	O
dominate	O
commercial	O
ml	O
the	O
method	O
originated	O
by	O
shapiro	O
niblett	O
also	O
shapiro	O
assigns	O
the	O
task	O
of	O
attribute-invention	O
to	O
the	O
user	O
in	O
a	O
manner	O
that	O
partitions	O
the	O
problem	O
into	O
a	O
hierarchy	B
of	O
smaller	O
problems	O
for	O
each	O
smaller	O
problem	O
a	O
solution	O
tree	O
is	O
separately	O
induced	O
structured	B
induction	I
is	O
closely	O
related	O
to	O
the	O
software	O
discipline	O
of	O
structured	O
programming	O
for	O
large	O
problems	O
the	O
industrial	O
stream	O
of	O
ml	O
work	O
will	O
continue	O
to	O
flow	O
along	O
this	O
human-computer	O
channel	O
it	O
may	O
for	O
some	O
time	B
remain	O
exceptional	O
for	O
problem	O
complexity	O
to	O
force	O
users	O
to	O
look	O
beyond	O
rule-based	O
ml	O
and	O
multivariate	O
statistics	O
because	O
the	O
statlog	B
ground-rules	O
of	O
comparative	B
trials	I
necessarily	O
barred	O
the	O
user	O
from	O
substantive	O
importation	O
of	O
domain-specific	O
knowledge	O
structured	B
induction	I
does	O
not	O
figure	O
in	O
this	O
book	O
but	O
industrially	O
oriented	O
readers	O
may	O
find	O
advantage	O
in	O
studying	O
cases	O
of	O
the	O
method	O
s	O
successful	O
field	O
use	O
one	O
such	O
account	O
by	O
leech	O
concerned	O
process	O
and	O
quality	O
control	O
in	O
uranium	O
refining	O
a	O
well-conceived	O
application	O
of	O
structured	O
decisiontree	O
induction	O
transformed	O
the	O
plant	O
from	O
unsatisfactory	O
to	O
highly	O
satisfactory	O
operation	O
and	O
is	O
described	O
in	O
sufficient	O
detail	O
to	O
be	O
used	O
as	O
a	O
working	O
paradigm	O
similar	O
experience	O
has	O
been	O
reported	O
from	O
other	O
industries	O
michie	O
for	O
review	O
neural	B
networks	I
r	O
rohwer	O
m	O
wynne-jones	O
and	O
f	O
wysotzki	O
aston	O
university	O
and	O
fraunhofer-institute	O
introduction	O
the	O
field	O
of	O
neural	B
networks	I
has	O
arisen	O
from	O
diverse	O
sources	O
ranging	O
from	O
the	O
fascination	O
of	O
mankind	O
with	O
understanding	O
and	O
emulating	O
the	O
human	B
brain	I
to	O
broader	O
issues	O
of	O
copying	O
human	O
abilities	O
such	O
as	O
speech	O
and	O
the	O
use	O
of	O
language	O
to	O
the	O
practical	O
commercial	O
scientific	O
and	O
engineering	O
disciplines	O
of	O
pattern	B
recognition	I
modelling	O
and	O
prediction	O
for	O
a	O
good	O
introductory	O
text	O
see	O
hertz	O
et	O
al	O
or	O
wasserman	O
linear	O
discriminants	O
were	O
introduced	O
by	O
fisher	O
as	O
a	O
statistical	B
procedure	O
for	O
classification	B
here	O
the	O
space	O
of	O
attributes	B
can	O
be	O
partitioned	O
by	O
a	O
set	O
of	O
hyperplanes	O
each	O
defined	O
by	O
a	O
linear	O
combination	O
of	O
the	O
attribute	O
variables	O
a	O
similar	O
model	O
for	O
logical	O
processing	O
was	O
suggested	O
by	O
mcculloch	O
pitts	O
as	O
a	O
possible	O
structure	O
bearing	O
similarities	O
to	O
neurons	B
in	O
the	O
human	B
brain	I
and	O
they	O
demonstrated	O
that	O
the	O
model	O
could	O
be	O
used	O
to	O
build	O
any	O
finite	O
logical	O
expression	O
the	O
mcculloch-pitts	B
neuron	I
figure	O
consists	O
of	O
a	O
weighted	O
sum	O
of	O
its	O
inputs	O
followed	O
by	O
a	O
non-linear	O
function	O
called	O
the	O
em	O
activation	O
function	O
originally	O
a	O
threshold	O
function	O
formally	O
fe	O
ji	O
if	O
otherwise	O
lk	O
other	O
neuron	O
models	O
are	O
quite	O
widely	O
used	O
for	O
example	B
in	O
radial	B
basis	I
function	I
networks	O
which	O
are	O
discussed	O
in	O
detail	O
in	O
section	O
networks	O
of	O
mcculloch-pitts	O
neurons	B
for	O
arbitrary	O
logical	O
expressions	O
were	O
handcrafted	O
until	O
the	O
ability	O
to	O
learn	O
by	O
reinforcement	O
of	O
behaviour	O
was	O
developed	O
in	O
hebb	O
s	O
book	O
the	O
organisation	O
of	O
behaviour	O
it	O
was	O
established	O
that	O
the	O
functionality	B
of	O
neural	B
networks	I
was	O
determined	O
by	O
the	O
strengths	O
of	O
the	O
connections	O
between	O
neurons	B
hebb	O
s	O
learning	O
rule	O
prescribes	O
that	O
if	O
the	O
network	O
responds	O
in	O
a	O
desirable	O
way	O
to	O
a	O
given	O
input	B
then	O
the	O
weights	O
should	O
be	O
adjusted	O
to	O
increase	O
the	O
probability	O
of	O
a	O
similar	O
m	O
address	O
for	O
correspondence	O
dept	O
of	O
computer	O
science	O
and	O
applied	O
mathematics	O
aston	O
university	O
birmingham	O
u	O
k	O
g	O
h	O
f	O
sec	O
introduction	O
output	B
npo	O
input	B
vectorrweight	O
vectors	O
weighted	O
sum	O
fig	O
mcculloch	O
and	O
pitts	O
neuron	O
response	O
to	O
similar	O
inputs	O
in	O
the	O
future	O
conversely	O
if	O
the	O
network	O
responds	O
undesirably	O
to	O
an	O
input	B
the	O
weights	O
should	O
be	O
adjusted	O
to	O
decrease	O
the	O
probability	O
of	O
a	O
similar	O
response	O
a	O
distinction	O
is	O
often	O
made	O
in	O
pattern	B
recognition	I
between	O
supervised	O
and	O
unsupervised	B
learning	I
the	O
former	O
describes	O
the	O
case	O
where	O
the	O
the	O
training	O
data	O
measurements	O
on	O
the	O
surroundings	O
are	O
accompanied	O
by	O
labels	O
indicating	O
the	O
class	B
of	O
event	O
that	O
the	O
measurements	O
represent	O
or	O
more	O
generally	O
a	O
desired	O
response	O
to	O
the	O
measurements	O
this	O
is	O
the	O
more	O
usual	O
case	O
in	O
classification	B
tasks	O
such	O
as	O
those	O
forming	O
the	O
empirical	O
basis	O
of	O
this	O
book	O
the	O
supervised	B
learning	I
networks	O
described	O
later	O
in	O
this	O
chapter	O
are	O
the	O
perceptron	B
and	O
multi	B
layer	I
perceptron	B
the	O
cascade	B
correlation	B
learning	O
architecture	O
and	O
radial	B
basis	I
function	I
networks	O
unsupervised	B
learning	I
refers	O
to	O
the	O
case	O
where	O
measurements	O
are	O
not	O
accompanied	O
by	O
class	B
labels	O
networks	O
exist	O
which	O
can	O
model	O
the	O
structure	O
of	O
samples	O
in	O
the	O
measurement	O
or	O
attribute	O
space	O
usually	O
in	O
terms	O
of	O
a	O
probability	O
density	O
function	O
or	O
by	O
representing	O
the	O
data	O
in	O
terms	O
of	O
cluster	O
centres	O
and	O
widths	O
such	O
models	O
include	O
gaussian	O
mixture	O
models	O
and	O
kohonen	B
networks	I
once	O
a	O
model	O
has	O
been	O
made	O
it	O
can	O
be	O
used	O
as	O
a	O
classifier	B
in	O
one	O
of	O
two	O
ways	O
the	O
first	O
is	O
to	O
determine	O
which	O
class	B
of	O
pattern	O
in	O
the	O
training	O
data	O
each	O
node	O
or	O
neuron	O
in	O
the	O
model	O
responds	O
most	O
strongly	O
to	O
most	O
frequently	O
unseen	O
data	O
can	O
then	O
be	O
classified	O
according	O
to	O
the	O
class	B
label	O
of	O
the	O
neuron	O
with	O
the	O
strongest	O
activation	O
for	O
each	O
pattern	O
alternatively	O
the	O
kohonen	B
network	O
or	O
mixture	O
model	O
can	O
be	O
used	O
as	O
the	O
first	O
layer	O
of	O
a	O
radial	B
basis	I
function	I
network	I
with	O
a	O
subsequent	O
layer	O
of	O
weights	O
used	O
to	O
calculate	O
a	O
set	O
of	O
class	B
probabilities	O
the	O
weights	O
in	O
this	O
layer	O
are	O
calculated	O
by	O
a	O
linear	O
one-shot	O
learning	O
algorithm	O
section	O
giving	O
radial	O
basis	O
functions	O
a	O
speed	B
advantage	O
over	O
non-linear	O
training	O
algorithms	O
such	O
as	O
most	O
of	O
the	O
supervised	B
learning	I
methods	O
the	O
first	O
layer	O
of	O
a	O
radial	B
basis	I
function	I
network	I
can	O
alternatively	O
be	O
initialised	O
by	O
choosing	O
a	O
subset	O
of	O
the	O
training	O
data	O
points	O
to	O
use	O
as	O
centres	O
s	O
n	O
q	O
k	O
neural	B
networks	I
and	O
targets	O
typically	O
by	O
minimising	O
the	O
total	O
squared	O
error	O
supervised	B
networks	I
for	O
classification	B
in	O
supervised	B
learning	I
we	O
have	O
an	O
instance	O
of	O
data	O
comprising	O
an	O
attribute	O
vectort	O
we	O
processt	O
with	O
a	O
network	O
to	O
produce	O
an	O
outputv	O
which	O
has	O
and	O
a	O
target	O
vectoru	O
the	O
same	O
form	O
as	O
the	O
target	O
vectoru	O
the	O
parameters	O
of	O
the	O
networkw	O
cbv	O
lu	O
it	O
might	O
seem	O
more	O
natural	O
to	O
use	O
a	O
percentage	O
misclassification	O
error	O
measure	B
in	O
classification	B
problems	O
but	O
the	O
total	O
squared	O
error	O
has	O
helpful	O
smoothness	O
and	O
differentiability	O
properties	O
although	O
the	O
total	O
squared	O
error	O
was	O
used	O
for	O
training	O
in	O
the	O
statlog	B
trials	O
percentage	O
misclassification	O
in	O
the	O
trained	O
networks	O
was	O
used	O
for	O
evaluation	O
are	O
modified	O
to	O
optimise	O
the	O
match	O
between	O
outputs	O
perceptrons	O
and	O
multi	O
layer	O
perceptrons	O
the	O
activation	O
of	O
the	O
mcculloch-pitts	B
neuron	I
has	O
been	O
generalised	O
to	O
the	O
form	O
h	O
byh	O
the	O
threshold	O
level	O
or	O
bias	B
of	O
equation	O
has	O
been	O
included	O
in	O
the	O
sum	O
with	O
the	O
assumption	O
of	O
an	O
extra	O
component	O
in	O
the	O
vector	O
where	O
the	O
activation	O
function	O
can	O
be	O
any	O
non-linear	O
function	O
the	O
nodes	O
have	O
been	O
divided	O
into	O
an	O
input	B
layer	O
and	O
an	O
output	B
layer	O
t	O
whose	O
value	O
is	O
fixed	O
at	O
rosenblatt	O
studied	O
the	O
capabilities	O
of	O
groups	O
of	O
neurons	B
in	O
a	O
learning	O
suitable	O
weights	O
for	O
classification	B
problems	O
when	O
single	O
layer	O
and	O
hence	O
all	O
acting	O
on	O
the	O
same	O
input	B
vectors	O
this	O
structure	O
was	O
termed	O
the	O
perceptron	B
and	O
rosenblatt	O
proposed	O
the	O
perceptron	B
learning	O
rule	O
for	O
is	O
a	O
hard	O
threshold	O
function	O
discontinuously	O
jumps	O
from	O
a	O
lower	O
to	O
an	O
upper	O
limiting	O
value	O
equation	O
defines	O
a	O
non-linear	O
function	O
across	O
a	O
hyperplane	O
in	O
the	O
attribute	O
space	O
with	O
a	O
threshold	O
activation	O
function	O
the	O
neuron	O
output	B
is	O
simply	O
on	O
one	O
side	O
of	O
the	O
hyperplane	O
and	O
on	O
the	O
other	O
when	O
combined	O
in	O
a	O
perceptron	B
structure	O
neurons	B
can	O
segment	O
the	O
attribute	O
space	O
into	O
regions	O
and	O
this	O
forms	O
the	O
basis	O
of	O
the	O
capability	O
of	O
perceptron	B
networks	O
to	O
perform	O
classification	B
minsky	O
and	O
papert	O
pointed	O
out	O
however	O
that	O
many	O
real	O
world	O
problems	O
do	O
not	O
fall	O
into	O
this	O
simple	O
framework	O
citing	O
the	O
exclusive-or	O
problem	O
as	O
the	O
simplest	O
example	B
here	O
it	O
is	O
necessary	O
to	O
isolate	O
two	O
convex	O
regions	O
joining	O
them	O
together	O
in	O
a	O
single	O
class	B
they	O
showed	O
that	O
while	O
this	O
was	O
not	O
possible	O
with	O
a	O
perceptron	B
network	O
it	O
can	O
be	O
done	O
with	O
a	O
two	O
layer	O
perceptron	B
structure	O
papert	O
this	O
formed	O
the	O
multi	B
layer	I
perceptron	B
which	O
is	O
widely	O
in	O
use	O
today	O
although	O
the	O
perceptron	B
learning	O
rule	O
called	O
the	O
delta	O
rule	O
could	O
not	O
be	O
generalised	O
to	O
find	O
weights	O
for	O
this	O
structure	O
a	O
learning	O
rule	O
was	O
proposed	O
in	O
which	O
allows	O
the	O
multi	B
layer	I
perceptron	B
to	O
learn	O
this	O
generalised	B
delta	I
rule	I
defines	O
a	O
notion	O
of	O
back-propagation	O
of	O
error	O
derivatives	O
through	O
the	O
network	O
hinton	O
et	O
al	O
and	O
and	O
enables	O
a	O
large	O
class	B
of	O
models	O
with	O
different	O
connection	O
structures	O
or	O
architectures	B
to	O
be	O
trained	O
these	O
publications	O
initiated	O
the	O
recent	O
academic	O
interest	O
in	O
neural	B
networks	I
and	O
the	O
field	O
subsequently	O
came	O
to	O
the	O
attention	O
of	O
industrial	O
users	O
f	O
g	O
i	O
sec	O
supervised	B
networks	I
for	O
classification	B
output	B
nodes	O
hidden	B
nodes	I
output	B
vector	O
or	O
non-linear	O
weightss	O
weightss	O
non-linear	O
d	O
input	B
nodes	O
input	B
vector	O
fig	O
mlp	B
structure	O
multi	B
layer	I
perceptron	B
structure	O
and	O
functionality	B
figure	O
shows	O
the	O
structure	O
of	O
a	O
standard	O
two-layer	O
perceptron	B
the	O
inputs	O
form	O
the	O
input	B
nodes	O
of	O
the	O
network	O
the	O
outputs	O
are	O
taken	O
from	O
the	O
output	B
nodes	O
the	O
middle	O
layer	O
of	O
nodes	O
visible	O
to	O
neither	O
the	O
inputs	O
nor	O
the	O
outputs	O
is	O
termed	O
the	O
hidden	B
layer	O
and	O
unlike	O
the	O
input	B
and	O
output	B
layers	O
its	O
size	O
is	O
not	O
fixed	O
the	O
hidden	B
layer	O
is	O
generally	O
used	O
to	O
make	O
a	O
bottleneck	O
forcing	O
the	O
network	O
to	O
make	O
a	O
simple	O
model	O
of	O
the	O
system	O
generating	O
the	O
data	O
with	O
the	O
ability	O
to	O
generalise	O
to	O
previously	O
unseen	O
patterns	O
the	O
operation	O
of	O
this	O
network	O
is	O
specified	O
by	O
d	O
via	O
the	O
in	O
a	O
manner	O
parameterised	O
by	O
the	O
two	O
layers	O
of	O
weightsw	O
this	O
specifies	O
how	O
input	B
pattern	O
vector	O
is	O
mapped	O
into	O
output	B
pattern	O
vector	O
d	O
are	O
typically	O
each	O
set	O
to	O
the	O
univariate	O
functions	O
andw	O
hidden	B
pattern	O
vector	O
k	O
tof	O
atl	O
which	O
varies	O
smoothly	O
from	O
at	O
c	O
ex	O
as	O
a	O
threshold	O
function	O
would	O
do	O
abruptly	O
if	O
the	O
number	O
of	O
hidden	B
layer	O
nodes	O
is	O
less	O
than	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
inherent	O
in	O
the	O
training	O
data	O
the	O
activations	O
of	O
the	O
hidden	B
nodes	I
tend	O
to	O
form	O
an	O
orthogonal	O
set	O
of	O
variables	O
either	O
linear	O
or	O
non-linear	O
combinations	O
of	O
the	O
attribute	O
variables	O
which	O
span	O
as	O
large	O
a	O
subspace	O
of	O
the	O
problem	O
as	O
possible	O
with	O
a	O
little	O
extra	O
constraint	O
on	O
r	O
e	O
i	O
g	O
d	O
e	O
i	O
f	O
g	O
g	O
f	O
f	O
neural	B
networks	I
the	O
network	O
these	O
internal	O
variables	O
form	O
a	O
linear	O
or	O
non-linear	O
principal	O
component	O
representation	O
of	O
the	O
attribute	O
space	O
if	O
the	O
data	O
has	O
noise	B
added	O
that	O
is	O
not	O
an	O
inherent	O
part	O
of	O
the	O
generating	O
system	O
then	O
the	O
principal	O
component	O
network	O
acts	O
as	O
a	O
filter	O
of	O
the	O
lower-variance	O
noise	B
signal	O
provided	O
the	O
signal	O
to	O
noise	B
ratio	O
of	O
the	O
data	O
is	O
sufficiently	O
high	O
this	O
property	O
gives	O
mlps	O
the	O
ability	O
to	O
generalise	O
to	O
previously	O
unseen	O
patterns	O
by	O
modelling	O
only	O
the	O
important	O
underlying	O
structure	O
of	O
the	O
generating	O
system	O
the	O
hidden	B
nodes	I
can	O
be	O
regarded	O
as	O
detectors	O
of	O
abstract	O
features	B
of	O
the	O
attribute	O
space	O
universal	B
approximators	I
and	O
universal	B
computers	I
in	O
the	O
multilayer	O
perceptron	B
such	O
as	O
the	O
two-layer	O
version	O
in	O
equation	O
the	O
the	O
weights	O
it	O
can	O
be	O
shown	O
that	O
the	O
two-layer	O
mlp	B
can	O
approximate	O
an	O
arbitrary	O
continuous	O
mapping	O
arbitrarily	O
closely	O
if	O
there	O
is	O
no	O
limit	O
to	O
the	O
number	O
of	O
hidden	B
nodes	I
in	O
this	O
sense	O
the	O
mlp	B
is	O
a	O
universal	O
function	O
approximator	O
this	O
theorem	O
does	O
not	O
imply	O
that	O
more	O
complex	B
mlp	B
architectures	B
are	O
pointless	O
it	O
can	O
be	O
more	O
efficient	O
terms	O
of	O
the	O
number	O
of	O
nodes	O
and	O
weights	O
required	O
to	O
use	O
different	O
numbers	O
of	O
layers	O
for	O
different	O
problems	O
unfortunately	O
there	O
is	O
a	O
shortage	O
of	O
rigorous	O
principles	O
on	O
which	O
to	O
base	O
a	O
choice	O
of	O
architecture	O
but	O
many	O
heuristic	O
principles	O
have	O
been	O
invented	O
and	O
explored	O
prominent	O
among	O
these	O
are	O
symmetry	O
principles	O
et	O
al	O
le	O
cun	O
et	O
al	O
and	O
constructive	B
algorithms	I
output-layer	O
node	O
valuesv	O
are	O
functions	O
of	O
the	O
input-layer	O
node	O
valuest	O
define	O
a	O
recurrent	O
network	O
by	O
feeding	O
the	O
outputs	O
back	O
to	O
the	O
inputs	O
the	O
general	O
form	O
of	O
a	O
recurrent	O
perceptron	B
is	O
the	O
mlp	B
is	O
a	O
feedforward	B
network	I
meaning	O
that	O
the	O
output	B
vectorv	O
the	O
input	B
vectort	O
nm	O
for	O
some	O
vector	O
functionm	O
given	O
in	O
detail	O
by	O
in	O
the	O
case	O
it	O
is	O
also	O
possible	O
to	O
feg	O
d	O
c	O
vjc	O
e	O
feg	O
and	O
some	O
parametersw	O
c	O
pm	O
this	O
is	O
a	O
discrete-time	O
model	O
continuous-time	O
models	O
governed	O
by	O
a	O
differential	O
equation	O
of	O
similar	O
structure	O
are	O
also	O
studied	O
recurrent	B
networks	I
are	O
universal	B
computers	I
in	O
the	O
sense	O
that	O
given	O
an	O
infinite	O
number	O
of	O
nodes	O
they	O
can	O
emulate	O
any	O
calculation	O
which	O
can	O
be	O
done	O
on	O
a	O
universal	O
turing	O
machine	O
infinite	O
number	O
of	O
nodes	O
is	O
needed	O
to	O
simulate	O
the	O
infinite	O
turing	O
tape	O
this	O
result	O
is	O
easily	O
proved	O
for	O
hard-threshold	O
recurrent	O
perceptrons	O
by	O
sketching	O
a	O
network	O
which	O
performs	O
not-and	O
and	O
another	O
which	O
functions	O
as	O
a	O
flip-flop	O
these	O
elements	O
are	O
all	O
that	O
are	O
required	O
to	O
build	O
a	O
computer	O
this	O
chapter	O
focuses	O
on	O
feedforward	O
neural	O
network	O
models	O
because	O
they	O
are	O
simpler	O
to	O
use	O
better	O
understood	O
and	O
closely	O
connected	O
with	O
statistical	B
classification	B
methods	O
however	O
recurrent	B
networks	I
attract	O
a	O
great	O
deal	O
of	O
research	O
interest	O
because	O
of	O
their	O
potential	O
to	O
serve	O
as	O
a	O
vehicle	B
for	O
bringing	O
statistical	B
methods	O
to	O
bear	O
on	O
algorithm	O
design	O
rohwer	O
et	O
al	O
shastri	O
ajjanagadde	O
we	O
can	O
say	O
is	O
a	O
function	O
of	O
which	O
could	O
be	O
written	O
g	O
w	O
v	O
g	O
d	O
e	O
i	O
g	O
f	O
g	O
g	O
sec	O
supervised	B
networks	I
for	O
classification	B
the	O
parameters	O
which	O
minimise	O
the	O
error	O
measure	B
in	O
if	O
to	O
the	O
average	O
target	O
usually	O
classification	B
problems	O
are	O
represented	O
using	O
one-out-of-n	O
output	B
coding	O
one	O
rq	O
cb	O
otherwise	O
training	O
mlps	O
by	O
nonlinear	B
regression	I
in	O
neural	O
network	O
parlancetraining	O
is	O
the	O
process	O
of	O
fitting	O
network	O
parameters	O
weights	O
to	O
given	O
data	O
the	O
training	O
data	O
consists	O
of	O
a	O
set	O
of	O
examples	O
of	O
corresponding	O
inputs	O
and	O
probabilistic	O
interpretation	O
of	O
mlp	B
outputs	O
if	O
there	O
is	O
a	O
one-to-many	O
relationship	O
between	O
the	O
inputs	O
and	O
targets	O
in	O
the	O
training	O
data	O
then	O
it	O
is	O
not	O
possible	O
for	O
any	O
mapping	O
of	O
the	O
form	O
to	O
perform	O
perfectly	O
it	O
is	O
straightforward	O
any	O
given	O
network	O
might	O
or	O
not	O
be	O
able	O
to	O
approximate	O
this	O
mapping	O
well	O
but	O
when	O
trained	O
as	O
well	O
as	O
possible	O
it	O
will	O
form	O
its	O
best	O
possible	O
approximation	O
to	O
this	O
mean	O
many	O
commonly-used	O
error	O
measures	B
in	O
addition	O
to	O
share	O
this	O
property	O
pearlmuter	O
desired	O
outputs	O
or	O
targets	O
let	O
the	O
th	O
example	B
be	O
given	O
by	O
inputy	O
and	O
targetq	O
for	O
input	B
dimension	O
for	O
target	O
dimension	O
usually	O
a	O
least-squares	O
fit	O
is	O
obtained	O
by	O
finding	O
are	O
the	O
output	B
values	O
obtained	O
by	O
substituting	O
the	O
inputsy	O
where	O
for	O
the	O
fit	O
is	O
perfect	O
to	O
show	O
wellekens	O
that	O
if	O
a	O
probability	O
densitys	O
data	O
then	O
the	O
minimum	O
of	O
is	O
attained	O
by	O
the	O
map	O
takingt	O
g	O
describes	O
the	O
output	B
node	O
is	O
allocated	O
for	O
each	O
class	B
and	O
the	O
target	O
vectoru	O
for	O
example	B
is	O
all	O
s	O
except	O
for	O
af	O
on	O
the	O
node	O
indicating	O
the	O
correct	O
class	B
in	O
this	O
case	O
the	O
value	O
computed	O
belongs	O
to	O
class	B
collectively	O
the	O
outputs	O
by	O
the	O
g	O
this	O
not	O
only	O
provides	O
th	O
training	O
inputt	O
for	O
classification	B
problems	O
given	O
that	O
the	O
value	O
output	B
by	O
the	O
th	O
target	O
node	O
given	O
the	O
collection	O
of	O
training	O
outputsu	O
g	O
the	O
probability	O
of	O
the	O
entire	O
feg	O
sof	O
nv	O
bw	O
rq	O
g	O
logc	O
log	O
helpful	O
insight	O
but	O
also	O
provides	O
a	O
principle	O
with	O
which	O
neural	O
network	O
models	O
can	O
be	O
combined	O
with	O
other	O
probabilistic	O
models	O
wellekens	O
therefore	O
the	O
cross-entropy	B
can	O
be	O
used	O
as	O
an	O
error	O
measure	B
instead	O
of	O
a	O
sum	B
of	I
squares	I
it	O
happens	O
that	O
its	O
minimum	O
also	O
lies	O
at	O
the	O
average	O
target	O
so	O
the	O
network	O
outputs	O
can	O
still	O
be	O
interpreted	O
probabilistically	O
and	O
furthermore	O
the	O
minimisation	O
of	O
crossentropy	O
is	O
equivalent	O
to	O
maximisation	O
of	O
the	O
likelihood	O
of	O
the	O
training	O
data	O
in	O
classification	B
problems	O
the	O
cross-entropy	B
has	O
this	O
interpretation	O
when	O
an	O
input	B
can	O
simultaneously	O
be	O
a	O
member	O
of	O
any	O
number	O
of	O
classes	B
and	O
membership	O
of	O
one	O
class	B
provides	O
no	O
information	O
about	O
membership	O
of	O
another	O
if	O
an	O
input	B
can	O
th	O
target	O
node	O
can	O
be	O
directly	O
interpreted	O
as	O
the	O
probability	O
that	O
the	O
input	B
pattern	O
the	O
probabilistic	O
interpretation	O
of	O
the	O
the	O
output	B
nodes	O
leads	O
to	O
a	O
natural	O
error	O
measure	B
this	O
is	O
the	O
exponential	O
of	O
the	O
cross-entropy	B
is	O
e	O
g	O
t	O
t	O
t	O
g	O
u	O
t	O
g	O
f	O
w	O
y	O
q	O
c	O
f	O
f	O
g	O
z	O
neural	B
networks	I
g	O
first	O
order	O
gradient	O
based	O
methods	O
the	O
probabilistic	O
interpretation	O
of	O
mlp	B
outputs	O
in	O
classification	B
problems	O
must	O
be	O
made	O
with	O
some	O
caution	O
it	O
only	O
applies	O
if	O
the	O
network	O
is	O
trained	O
to	O
its	O
minimum	O
error	O
and	O
then	O
belongs	O
to	O
a	O
continuous	O
space	O
or	O
a	O
large	O
discrete	O
set	O
because	O
technically	O
a	O
large	O
or	O
infinite	O
amount	O
of	O
data	O
is	O
required	O
this	O
problem	O
is	O
intimately	O
related	O
to	O
the	O
overtraining	O
and	O
generalisation	O
issues	O
discussed	O
below	O
for	O
the	O
theoretical	O
reasons	O
given	O
here	O
the	O
cross-entropy	B
is	O
the	O
most	O
appropriate	O
error	O
measure	B
for	O
use	O
in	O
classification	B
problems	O
although	O
practical	O
experience	O
suggests	O
it	O
makes	O
little	O
difference	O
the	O
sum	B
of	I
squares	I
was	O
used	O
in	O
the	O
statlog	B
neural	O
network	O
trials	O
minimisation	B
methods	I
so	O
as	O
to	O
minimise	O
an	O
error	O
measure	B
such	O
as	O
in	O
the	O
simplest	O
cases	O
the	O
network	O
outputs	O
are	O
linear	O
in	O
the	O
weights	O
making	O
quadratic	O
then	O
the	O
minimal	O
error	O
can	O
be	O
found	O
by	O
solving	O
a	O
linear	O
system	O
of	O
equations	O
this	O
special	O
case	O
is	O
discussed	O
in	O
section	O
in	O
the	O
context	O
of	O
radial	B
basis	I
function	I
networks	O
which	O
have	O
this	O
property	O
in	O
the	O
generic	O
nonlinear	O
case	O
the	O
minimisation	O
is	O
accomplished	O
using	O
a	O
variant	O
of	O
gradient	B
descent	I
this	O
but	O
not	O
only	O
if	O
the	O
training	O
data	O
accurately	O
represents	O
the	O
underlying	O
probability	O
the	O
latter	O
condition	O
is	O
problematic	O
ift	O
neural	O
network	O
models	O
are	O
trained	O
by	O
adjusting	O
their	O
weight	O
matrix	O
parametersw	O
produces	O
a	O
local	O
minimum	O
aw	O
necessarily	O
the	O
global	O
minimum	O
of	O
from	O
which	O
any	O
infinitesimal	O
change	O
increases	O
g	O
the	O
gradient	O
g	O
of	O
is	O
the	O
vector	O
field	O
of	O
derivatives	O
of	O
d	O
d	O
a	O
linear	O
approximation	O
to	O
field	O
because	O
the	O
vector	O
depends	O
onw	O
imal	O
vicinity	O
of	O
an	O
arbitrary	O
weight	O
matrixw	O
rw	O
e	O
clearly	O
then	O
at	O
any	O
pointw	O
vector	O
changes	O
a	O
given	O
magnitude	O
which	O
one	O
could	O
make	O
tow	O
points	O
in	O
the	O
direction	O
of	O
fastest	O
increase	O
of	O
in	O
the	O
direction	O
of	O
the	O
most	O
consequently	O
an	O
adjustment	O
ofw	O
of	O
increases	O
provides	O
the	O
maximum	O
possible	O
decrease	O
in	O
direction	O
for	O
descent	O
changes	O
whenw	O
chosen	O
small	O
enough	O
for	O
descent	O
algorithm	O
requires	O
a	O
step	O
size	O
parameter	O
w	O
w	O
in	O
practice	O
trial	O
and	O
error	O
is	O
used	O
to	O
look	O
for	O
the	O
largest	O
step	O
size	O
which	O
will	O
work	O
until	O
the	O
error	O
belong	O
to	O
one	O
and	O
only	O
one	O
class	B
then	O
the	O
simple	O
entropy	B
obtained	O
by	O
dropping	O
the	O
terms	O
involving	O
the	O
problem	O
with	O
this	O
method	O
is	O
that	O
the	O
theorem	O
on	O
maximal	O
descent	O
only	O
applies	O
to	O
infinitesimal	O
adjustments	O
the	O
gradient	O
changes	O
as	O
well	O
as	O
the	O
error	O
so	O
the	O
optimal	O
is	O
adjusted	O
the	O
pure	O
gradient	O
to	O
be	O
effectively	O
infinitesimal	O
so	O
far	O
as	O
obtaining	O
descent	O
is	O
concerned	O
but	O
otherwise	O
as	O
large	O
as	O
possible	O
in	O
the	O
interests	O
of	O
speed	B
the	O
weights	O
are	O
repeatedly	O
adjusted	O
by	O
is	O
given	O
by	O
in	O
the	O
infinites	O
of	O
the	O
parameter	O
space	O
space	O
of	O
the	O
network	O
the	O
i	O
e	O
of	O
all	O
the	O
infinitesimal	O
a	O
change	O
in	O
the	O
direction	O
compute	O
the	O
gradient	O
and	O
adjust	O
the	O
weights	O
in	O
the	O
opposite	O
direction	O
the	O
basic	O
strategy	O
in	O
gradient	B
descent	I
is	O
to	O
should	O
be	O
used	O
fails	O
to	O
descend	O
with	O
large	O
step	O
sizes	O
the	O
gradient	O
will	O
tend	O
to	O
change	O
dramatically	O
with	O
each	O
step	O
a	O
t	O
g	O
g	O
c	O
g	O
i	O
g	O
i	O
g	O
g	O
g	O
g	O
g	O
g	O
w	O
g	O
sec	O
supervised	B
networks	I
for	O
classification	B
w	O
old	O
e	O
the	O
quadratic	O
approximation	O
is	O
the	O
matrix	O
with	O
components	O
popular	O
heuristic	O
is	O
to	O
use	O
a	O
moving	O
average	O
of	O
the	O
gradient	O
vector	O
in	O
order	O
find	O
a	O
systematic	O
tendency	O
this	O
is	O
accomplished	O
by	O
adding	O
a	O
momentum	O
term	O
to	O
a	O
parameter	O
these	O
methods	O
offer	O
the	O
benefit	O
of	O
simplicity	O
but	O
their	O
performance	O
depends	O
sensitively	O
different	O
values	O
seem	O
to	O
be	O
appropriate	O
for	O
different	O
problems	O
and	O
for	O
different	O
stages	O
of	O
training	O
in	O
one	O
problem	O
this	O
circumstance	O
has	O
given	O
rise	O
to	O
a	O
plethora	O
of	O
heuristics	O
for	O
adaptive	O
variable	O
step	O
size	O
algorithms	O
silva	O
almeida	O
jacobs	O
second-order	B
methods	O
the	O
underlying	O
difficulty	O
in	O
first	O
order	O
gradient	O
based	O
methods	O
is	O
that	O
the	O
linear	O
approxi	O
to	O
a	O
stationary	O
point	O
of	O
this	O
quadratic	O
form	O
this	O
may	O
be	O
a	O
minimum	O
maximum	O
or	O
saddle	O
point	O
if	O
it	O
is	O
a	O
minimum	O
then	O
a	O
step	O
in	O
that	O
direction	O
seems	O
a	O
good	O
idea	O
if	O
not	O
then	O
a	O
positive	O
or	O
negative	O
step	O
has	O
a	O
negative	O
projection	O
is	O
at	O
least	O
not	O
unreasonable	O
therefore	O
a	O
large	O
class	B
of	O
algorithms	O
has	O
been	O
developed	O
involving	O
the	O
conjugate	B
gradient	I
is	O
roughly	O
half	O
the	O
square	O
of	O
the	O
number	O
of	O
components	O
so	O
for	O
large	O
networks	O
involving	O
many	O
weights	O
such	O
algorithms	O
lead	O
to	O
impractical	O
computer	O
memory	B
requirements	O
but	O
one	O
algorithm	O
generally	O
called	O
the	O
conjugate	B
gradient	I
algorithm	O
or	O
the	O
memoryless	O
conjugate	B
gradient	I
algorithm	O
does	O
not	O
this	O
algorithm	O
f	O
w	O
w	O
here	O
w	O
old	O
refers	O
to	O
the	O
most	O
recent	O
weight	O
change	O
on	O
the	O
parameters	O
and	O
mation	O
ignores	O
the	O
curvature	O
of	O
g	O
this	O
can	O
be	O
redressed	O
by	O
extending	O
to	O
w	O
w	O
e	O
e	O
where	O
a	O
called	O
the	O
inverse	O
hessian	O
the	O
hessian	O
depending	O
on	O
conventions	O
and	O
the	O
change	O
w	O
w	O
bringsw	O
where	O
on	O
the	O
gradient	O
in	O
the	O
conjugate	B
gradient	I
direction	O
u	O
most	O
of	O
these	O
algorithms	O
require	O
explicit	O
computation	O
or	O
estimation	O
of	O
the	O
hessian	O
the	O
number	O
of	O
components	O
of	O
ofw	O
maintains	O
an	O
estimate	O
of	O
the	O
conjugate	O
direction	O
without	O
directly	O
representing	O
searches	O
for	O
the	O
minimum	O
of	O
g	O
starting	O
from	O
the	O
most	O
recent	O
estimate	O
of	O
the	O
minimum	O
orthogonal	O
to	O
the	O
gradient	O
making	O
the	O
variation	O
of	O
the	O
update	O
rule	O
for	O
the	O
conjugate	B
gradient	I
direction	O
and	O
searching	O
for	O
the	O
minimum	O
in	O
the	O
direction	O
of	O
the	O
current	O
estimate	O
of	O
the	O
conjugate	B
gradient	I
linesearch	O
algorithms	O
are	O
comparatively	O
easy	O
because	O
the	O
issue	O
of	O
direction	O
choice	O
reduces	O
to	O
a	O
binary	O
choice	O
but	O
because	O
the	O
linesearch	O
appears	O
in	O
the	O
inner	O
loop	O
of	O
the	O
conjugate	B
gradient	I
algorithm	O
efficiency	O
is	O
important	O
considerable	O
effort	O
therefore	O
goes	O
into	O
it	O
to	O
the	O
extent	O
that	O
the	O
linesearch	O
is	O
typically	O
the	O
most	O
complicated	O
module	O
of	O
a	O
conjugate	B
gradient	I
implementation	O
numerical	O
round-off	O
problems	O
are	O
another	O
design	O
consideration	O
in	O
linesearch	O
implementations	O
because	O
the	O
conjugate	B
gradient	I
is	O
often	O
nearly	O
g	O
along	O
the	O
conjugate	B
gradient	I
is	O
the	O
conjugate	B
gradient	I
algorithm	O
uses	O
a	O
sequence	O
of	O
linesearches	B
one-dimensional	O
especially	O
small	O
where	O
old	O
g	O
g	O
g	O
g	O
g	O
w	O
x	O
w	O
w	O
neural	B
networks	I
to	O
continue	O
this	O
network	O
architecture	O
was	O
used	O
in	O
the	O
work	O
reported	O
in	O
this	O
book	O
an	O
implementation	O
of	O
the	O
conjugate	B
gradient	I
algorithm	O
will	O
have	O
several	O
parameters	O
is	O
the	O
polak-ribiere	B
variant	O
there	O
are	O
others	O
somewhat	O
intricate	O
proofs	O
exist	O
which	O
in	O
practice	O
good	O
performance	O
is	O
often	O
obtained	O
on	O
much	O
more	O
general	O
functions	O
using	O
very	O
imprecise	O
linesearches	B
it	O
is	O
necessary	O
to	O
augment	O
with	O
a	O
rule	O
but	O
unlike	O
the	O
step	O
size	O
and	O
momentum	O
parameters	O
of	O
the	O
simpler	O
methods	O
the	O
performance	O
of	O
the	O
conjugate	B
gradient	I
method	O
is	O
relatively	O
insensitive	O
to	O
its	O
parameters	O
if	O
they	O
are	O
set	O
within	O
reasonable	O
ranges	O
all	O
algorithms	O
are	O
sensitive	O
to	O
process	O
for	O
selecting	O
initial	O
weights	O
and	O
many	O
other	O
factors	O
which	O
remain	O
to	O
be	O
carefully	O
isolated	O
gradient	O
calculations	O
in	O
mlps	O
in	O
the	O
case	O
of	O
an	O
mlp	B
neural	O
network	O
model	O
with	O
an	O
error	O
measure	B
such	O
as	O
the	O
calculation	O
is	O
conveniently	O
organised	O
as	O
a	O
back	O
propagation	O
of	O
error	O
et	O
al	O
rohwer	O
renals	O
for	O
a	O
network	O
with	O
a	O
single	O
layer	O
of	O
hidden	B
nodes	I
this	O
calculation	O
proceeds	O
by	O
propagating	O
forward	B
from	O
the	O
input	B
to	O
output	B
layers	O
for	O
each	O
training	O
example	B
and	O
related	O
to	O
the	O
output	B
errors	O
backwards	O
through	O
a	O
linearised	O
oldz	O
old	O
old	O
show	O
that	O
if	O
were	O
purely	O
quadratic	O
inw	O
were	O
initialised	O
to	O
the	O
gradient	O
and	O
the	O
linesearches	B
were	O
performed	O
exactly	O
then	O
would	O
converge	O
on	O
the	O
conjugate	B
gradient	I
components	O
ofw	O
and	O
would	O
converge	O
on	O
its	O
minimum	O
after	O
as	O
many	O
iterations	O
of	O
as	O
there	O
are	O
to	O
whenever	O
becomes	O
too	O
nearly	O
orthogonal	O
to	O
the	O
gradient	O
for	O
progress	O
to	O
reset	O
controlling	O
the	O
details	O
of	O
the	O
linesearch	O
and	O
others	O
which	O
define	O
exactly	O
when	O
to	O
reset	O
to	O
it	O
remains	O
to	O
discuss	O
the	O
computation	O
of	O
the	O
gradient	O
then	O
propagating	O
quantities	O
version	O
of	O
the	O
network	O
products	O
of	O
s	O
and	O
s	O
then	O
give	O
the	O
gradient	O
in	O
the	O
case	O
of	O
a	O
node	O
output	B
values	O
g	O
a	O
single	O
hidden	B
g	O
and	O
an	O
output	B
or	O
target	O
layer	O
network	O
with	O
an	O
input	B
layerc	O
g	O
the	O
calculation	O
is	O
cbz	O
d	O
rq	O
d	O
d	O
y	O
is	O
summed	O
over	O
training	O
examples	O
while	O
the	O
s	O
andd	O
s	O
refer	O
to	O
nodes	O
and	O
b	O
the	O
index	O
c	O
g	O
c	O
y	O
g	O
d	O
e	O
i	O
f	O
g	O
d	O
e	O
i	O
f	O
g	O
g	O
y	O
i	O
h	O
i	O
y	O
h	O
i	O
y	O
y	O
d	O
y	O
z	O
sec	O
supervised	B
networks	I
for	O
classification	B
linear	O
output	B
weights	O
non-linear	O
receptive	O
fields	O
in	O
attribute	O
space	O
fig	O
a	O
radial	B
basis	I
function	I
network	I
online	O
vs	O
batch	O
and	O
the	O
gradient	O
are	O
a	O
sum	O
over	O
examples	O
these	O
could	O
be	O
estimated	O
by	O
randomly	O
selecting	O
a	O
subset	O
of	O
examples	O
for	O
inclusion	O
in	O
the	O
sum	O
in	O
the	O
extreme	O
a	O
single	O
example	B
might	O
be	O
used	O
for	O
each	O
gradient	O
estimate	O
this	O
is	O
a	O
stochastic	B
gradient	I
method	O
if	O
a	O
similar	O
strategy	O
is	O
used	O
without	O
random	O
selection	O
but	O
with	O
the	O
data	O
taken	O
in	O
the	O
order	O
it	O
comes	O
the	O
method	O
is	O
an	O
online	O
one	O
if	O
a	O
sum	O
over	O
all	O
note	O
that	O
both	O
the	O
error	O
training	O
data	O
is	O
performed	O
for	O
each	O
gradient	O
calculation	O
then	O
the	O
method	O
is	O
a	O
h	O
variety	O
given	O
function	O
ofw	O
which	O
can	O
be	O
evaluated	O
precisely	O
so	O
that	O
meaningful	O
comparisons	O
can	O
gradient	O
method	O
because	O
it	O
is	O
built	O
on	O
procedures	O
and	O
theorems	O
which	O
assume	O
that	O
online	O
and	O
stochastic	B
gradient	B
methods	I
offer	O
a	O
considerable	O
speed	B
advantage	O
if	O
the	O
approximation	O
is	O
serviceable	O
for	O
problems	O
with	O
large	O
amounts	O
of	O
training	O
data	O
they	O
are	O
highly	O
favoured	O
however	O
these	O
approximations	O
cannot	O
be	O
used	O
directly	O
in	O
the	O
conjugate	O
is	O
a	O
be	O
made	O
at	O
nearby	O
arguments	O
therefore	O
the	O
stochastic	B
gradient	I
and	O
online	O
methods	O
tend	O
to	O
be	O
used	O
with	O
simple	O
step-size	O
and	O
momentum	O
methods	O
there	O
is	O
some	O
work	O
on	O
finding	O
a	O
compromise	O
method	O
ller	O
radial	B
basis	I
function	I
networks	O
the	O
radial	B
basis	I
function	I
network	I
consists	O
of	O
a	O
layer	O
of	O
units	O
performing	O
linear	O
or	O
non-linear	O
functions	O
of	O
the	O
attributes	B
followed	O
by	O
a	O
layer	O
of	O
weighted	O
connections	O
to	O
nodes	O
whose	O
outputs	O
have	O
the	O
same	O
form	O
as	O
the	O
target	O
vectors	O
it	O
has	O
a	O
structure	O
like	O
an	O
mlp	B
with	O
one	O
hidden	B
layer	O
except	O
that	O
each	O
node	O
of	O
the	O
the	O
hidden	B
layer	O
computes	O
an	O
arbitrary	O
function	O
of	O
the	O
inputs	O
gaussians	O
being	O
the	O
most	O
popular	O
and	O
the	O
transfer	O
function	O
of	O
each	O
output	B
node	O
is	O
the	O
trivial	O
identity	O
function	O
instead	O
of	O
synaptic	O
strengths	O
the	O
hidden	B
layer	O
has	O
parameters	O
appropriate	O
for	O
whatever	O
functions	O
are	O
being	O
used	O
for	O
example	B
gaussian	O
widths	O
and	O
positions	O
this	O
network	O
offers	O
a	O
number	O
of	O
advantages	O
over	O
the	O
multi	B
layer	I
perceptron	B
under	O
certain	O
conditions	O
although	O
the	O
two	O
models	O
are	O
computationally	O
equivalent	O
these	O
advantages	O
include	O
a	O
linear	O
training	O
rule	O
once	O
the	O
locations	O
in	O
attribute	O
space	O
of	O
the	O
non-linear	O
functions	O
have	O
been	O
determined	O
and	O
an	O
underlying	O
model	O
involving	O
localised	O
functions	O
in	O
the	O
attribute	O
space	O
rather	O
than	O
the	O
long-range	O
functions	O
occurring	O
in	O
perceptron-based	O
models	O
the	O
linear	O
learning	O
rule	O
avoids	O
problems	O
associated	O
with	O
local	O
minima	O
in	O
particular	O
it	O
provides	O
enhanced	O
ability	O
to	O
make	O
statments	O
about	O
the	O
accuracy	B
of	O
neural	B
networks	I
the	O
probabilistic	O
interpretation	O
of	O
the	O
outputs	O
in	O
section	O
figure	O
shows	O
the	O
structure	O
of	O
a	O
radial	B
basis	I
function	I
the	O
non-linearities	O
comprise	O
a	O
position	O
in	O
attribute	O
space	O
at	O
which	O
the	O
function	O
is	O
located	O
referred	O
to	O
as	O
the	O
function	O
s	O
centre	O
and	O
a	O
non-linear	O
function	O
of	O
the	O
distance	B
of	O
an	O
input	B
point	O
from	O
that	O
centre	O
which	O
and	O
produce	O
an	O
interpolating	O
function	O
using	O
non-localised	O
functions	O
they	O
are	O
often	O
found	O
to	O
have	O
better	O
interpolating	O
properties	O
in	O
the	O
region	O
populated	O
by	O
the	O
training	O
data	O
can	O
be	O
any	O
function	O
at	O
all	O
common	O
choices	O
include	O
a	O
gaussian	O
response	O
functionexpci	O
k	O
as	O
well	O
as	O
non-local	O
functions	O
such	O
as	O
thin	O
plate	O
log	O
and	O
multiquadrics	O
splines	O
and	O
inverse	O
multiquadrics	O
although	O
it	O
seems	O
counter-intuitive	O
to	O
try	O
e	O
m	O
exm	O
the	O
radial	B
basis	I
function	I
network	I
approach	O
involves	O
the	O
expansion	O
or	O
pre-processing	O
of	O
input	B
vectors	O
into	O
a	O
high-dimensional	O
space	O
this	O
attempts	O
to	O
exploit	O
a	O
theorem	O
of	O
cover	B
which	O
implies	O
that	O
a	O
classification	B
problem	O
cast	O
in	O
a	O
high-dimensional	O
space	O
is	O
more	O
likely	O
to	O
be	O
linearly	O
separable	O
than	O
would	O
be	O
the	O
case	O
in	O
a	O
low-dimensional	O
space	O
training	O
choosing	O
the	O
centres	O
and	O
non-linearities	O
a	O
number	O
of	O
methods	O
can	O
be	O
used	O
for	O
choosing	O
the	O
centres	O
for	O
a	O
radial	B
basis	I
function	I
network	I
it	O
is	O
important	O
that	O
the	O
distribution	O
of	O
centres	O
in	O
the	O
attribute	O
space	O
should	O
be	O
similar	O
to	O
or	O
at	O
least	O
cover	B
the	O
same	O
region	O
as	O
the	O
training	O
data	O
it	O
is	O
assumed	O
that	O
the	O
training	O
data	O
is	O
representative	O
of	O
the	O
problem	O
otherwise	O
good	O
performance	O
cannot	O
be	O
expected	O
on	O
future	O
unseen	O
patterns	O
a	O
first	O
order	O
technique	O
for	O
choosing	O
centres	O
is	O
to	O
take	O
points	O
on	O
a	O
square	O
grid	O
covering	O
the	O
region	O
of	O
attribute	O
space	O
covered	O
by	O
the	O
training	O
data	O
alternatively	O
better	O
performance	O
might	O
be	O
expected	O
if	O
the	O
centres	O
were	O
sampled	O
at	O
random	O
from	O
the	O
training	O
data	O
itself	O
using	O
some	O
or	O
all	O
samples	O
since	O
the	O
more	O
densely	O
populated	O
regions	O
of	O
the	O
attribute	O
space	O
would	O
have	O
a	O
higher	O
resolution	O
model	O
than	O
sparser	O
regions	O
in	O
this	O
case	O
it	O
is	O
important	O
to	O
ensure	O
that	O
at	O
least	O
one	O
sample	O
from	O
each	O
class	B
is	O
used	O
as	O
a	O
prototype	O
centre	O
in	O
the	O
experiments	O
in	O
this	O
book	O
the	O
number	O
of	O
samples	O
required	O
from	O
each	O
class	B
was	O
calculated	O
before	O
sampling	O
thereby	O
ensuring	O
this	O
condition	O
was	O
met	O
when	O
centre	O
positions	O
are	O
chosen	O
for	O
radial	B
basis	I
function	I
networks	O
with	O
localised	O
non-linear	O
functions	O
such	O
as	O
gaussian	O
receptive	O
fields	O
it	O
is	O
important	O
to	O
calculate	O
suitable	O
variances	O
or	O
spreads	O
for	O
the	O
functions	O
this	O
ensures	O
that	O
large	O
regions	O
of	O
space	O
do	O
not	O
occur	O
between	O
centres	O
where	O
no	O
centres	O
respond	O
to	O
patterns	O
and	O
conversely	O
that	O
no	O
pair	O
of	O
centres	O
respond	O
nearly	O
identically	O
to	O
all	O
patterns	O
this	O
problem	O
is	O
particularly	O
prevalent	O
in	O
high	O
dimensional	O
attribute	O
spaces	O
because	O
volume	O
depends	O
sensitively	O
on	O
radius	O
for	O
a	O
quantitative	O
discussion	O
of	O
this	O
point	O
see	O
prager	O
fallside	O
in	O
the	O
experiments	O
reported	O
in	O
this	O
book	O
the	O
standard	O
deviations	O
of	O
the	O
gaussian	O
functions	O
were	O
set	O
separately	O
for	O
each	O
coordinate	O
direction	O
to	O
the	O
distance	B
to	O
the	O
nearest	O
centre	O
in	O
that	O
direction	O
multiplied	O
by	O
an	O
arbitrary	O
scaling	B
parameter	I
to	O
other	O
methods	O
include	O
using	O
a	O
principled	O
clustering	B
technique	O
to	O
position	O
the	O
centres	O
such	O
as	O
a	O
gaussian	O
mixture	O
model	O
or	O
a	O
kohonen	B
network	O
these	O
models	O
are	O
discussed	O
in	O
section	O
training	O
optimising	O
the	O
weights	O
as	O
mentioned	O
in	O
section	O
radial	B
basis	I
function	I
networks	O
are	O
trained	O
simply	O
by	O
solving	O
a	O
linear	O
system	O
the	O
same	O
problem	O
arises	O
in	O
ordinary	O
linear	B
regression	I
the	O
only	O
difference	O
being	O
that	O
the	O
input	B
to	O
the	O
linear	O
system	O
is	O
the	O
output	B
of	O
the	O
hidden	B
layer	O
of	O
the	O
network	O
not	O
g	O
sec	O
supervised	B
networks	I
for	O
classification	B
written	O
out	O
in	O
full	O
is	O
then	O
is	O
the	O
lies	O
where	O
the	O
gradient	O
vanishes	O
number	O
of	O
radial	O
basis	O
functions	O
which	O
has	O
its	O
minimum	O
where	O
the	O
derivative	O
be	O
the	O
correlation	B
matrix	O
of	O
the	O
radial	B
basis	I
function	I
outputs	O
the	O
attribute	O
variables	O
themselves	O
there	O
are	O
a	O
few	O
subtleties	O
however	O
which	O
are	O
discussed	O
is	O
computed	O
using	O
the	O
weightsi	O
th	O
radial	B
basis	I
function	I
on	O
the	O
th	O
example	B
the	O
output	B
be	O
the	O
output	B
of	O
thed	O
here	O
let	O
of	O
each	O
target	O
node	O
as	O
d	O
let	O
the	O
desired	O
output	B
for	O
example	B
on	O
target	O
node	O
beq	O
the	O
error	O
measure	B
rq	O
d	O
ic	O
d	O
b	O
vanishes	O
let	O
the	O
weight	O
matrixw	O
which	O
minimises	O
thus	O
the	O
problem	O
is	O
solved	O
by	O
inverting	O
the	O
square	O
sition	O
rohwer	O
and	O
et	O
al	O
if	O
if	O
the	O
number	O
of	O
training	O
samples	O
is	O
not	O
at	O
least	O
as	O
great	O
as	O
k	O
be	O
the	O
number	O
of	O
training	O
examples	O
instead	O
of	O
solving	O
the	O
by	O
the	O
derivatives	O
of	O
d	O
unlessk	O
the	O
matrix	O
with	O
elements	O
ofv	O
u	O
v	O
the	O
matrix	O
inversion	O
can	O
be	O
accomplished	O
by	O
standard	O
methods	O
such	O
as	O
lu	O
decompois	O
neither	O
singular	O
nor	O
nearly	O
so	O
this	O
is	O
typically	O
the	O
case	O
but	O
things	O
can	O
go	O
wrong	O
if	O
two	O
radial	B
basis	I
function	I
centres	O
are	O
very	O
close	O
together	O
a	O
singular	O
matrix	O
will	O
result	O
and	O
a	O
singular	O
matrix	O
is	O
guaranteed	O
there	O
is	O
no	O
practical	O
way	O
to	O
ensure	O
a	O
non-singular	O
correlation	B
matrix	O
consequently	O
the	O
safest	O
course	O
of	O
action	O
is	O
to	O
use	O
a	O
slightly	O
more	O
computationally	O
expensive	O
singular	O
value	O
decomposition	O
method	O
such	O
methods	O
provide	O
an	O
approximate	O
inverse	O
by	O
diagonalising	O
the	O
matrix	O
inverting	O
only	O
the	O
eigenvalues	O
which	O
exceed	O
zero	O
by	O
a	O
parameter-specified	O
margin	O
and	O
transforming	O
back	O
to	O
the	O
original	O
coordinates	O
this	O
provides	O
an	O
optimal	O
minimum-norm	O
approximation	O
to	O
the	O
inverse	O
in	O
the	O
least-mean-squares	O
sense	O
another	O
approach	O
to	O
the	O
entire	O
problem	O
is	O
possible	O
lowe	O
let	O
linear	O
system	O
given	O
this	O
method	O
focuses	O
on	O
the	O
linear	O
system	O
embedded	O
in	O
the	O
this	O
is	O
a	O
rectangular	O
system	O
in	O
general	O
an	O
exact	O
solution	O
does	O
not	O
exist	O
but	O
the	O
optimal	O
solution	O
in	O
the	O
least-squares	O
sense	O
is	O
given	O
by	O
the	O
pseudo-inverse	O
r	O
matrix	O
where	O
error	O
formula	O
itself	O
u	O
i	O
g	O
x	O
i	O
z	O
i	O
q	O
w	O
q	O
y	O
z	O
i	O
q	O
w	O
neural	B
networks	I
u	O
transpose	O
can	O
be	O
applied	O
to	O
to	O
show	O
that	O
the	O
pseudo-inverse	O
method	O
gives	O
the	O
same	O
result	O
as	O
this	O
formula	O
is	O
applied	O
directly	O
the	O
identityu	O
that	O
an	O
exact	O
expression	O
exists	O
for	O
updating	O
the	O
inverse	O
correlation	B
improving	O
the	O
generalisation	O
of	O
feed-forward	B
networks	I
the	O
requirement	O
to	O
invert	O
or	O
pseudo-invert	O
a	O
matrix	O
dependent	O
on	O
the	O
entire	O
dataset	O
makes	O
this	O
a	O
batch	O
method	O
however	O
an	O
online	O
variant	O
is	O
possible	O
known	O
as	O
kalman	O
filtering	O
tepedelenlioglu	O
it	O
is	O
based	O
on	O
the	O
somewhat	O
remarkable	O
fact	O
if	O
another	O
example	B
where	O
denotes	O
the	O
matrix	O
is	O
added	O
to	O
the	O
sum	O
which	O
does	O
not	O
require	O
recomputation	O
of	O
the	O
inverse	O
constructive	B
algorithms	I
and	O
pruning	B
a	O
number	O
of	O
techniques	O
have	O
emerged	O
recently	O
which	O
attempt	O
to	O
improve	O
on	O
the	O
perceptron	B
and	O
multilayer	O
perceptron	B
training	O
algorithms	O
by	O
changing	O
the	O
architecture	O
of	O
the	O
networks	O
as	O
training	O
proceeds	O
these	O
techniques	O
include	O
pruning	B
useless	O
nodes	O
or	O
weights	O
and	O
constructive	B
algorithms	I
where	O
extra	O
nodes	O
are	O
added	O
as	O
required	O
the	O
advantages	O
include	O
smaller	O
networks	O
faster	O
training	O
times	O
on	O
serial	O
computers	O
and	O
increased	O
generalisation	O
ability	O
with	O
a	O
consequent	O
immunity	O
to	O
noise	B
in	O
addition	O
it	O
is	O
frequently	O
much	O
easier	O
to	O
interpret	O
what	O
the	O
trained	O
network	O
is	O
doing	O
as	O
was	O
noted	O
earlier	O
a	O
minimalist	O
network	O
uses	O
its	O
hidden	B
layer	O
to	O
model	O
as	O
much	O
of	O
the	O
problem	O
as	O
possible	O
in	O
the	O
limited	O
number	O
of	O
degrees	O
of	O
freedom	O
available	O
in	O
its	O
hidden	B
layer	O
with	O
such	O
a	O
network	O
one	O
can	O
then	O
begin	O
to	O
draw	O
analogies	O
with	O
other	O
pattern	O
classifying	O
techniques	O
such	O
as	O
decision	B
trees	I
and	O
expert	B
systems	I
to	O
make	O
a	O
network	O
with	O
good	O
generalisation	O
ability	O
we	O
must	O
determine	O
a	O
suitable	O
number	O
of	O
hidden	B
nodes	I
if	O
there	O
are	O
too	O
few	O
the	O
network	O
may	O
not	O
learn	O
at	O
all	O
while	O
too	O
many	O
hidden	B
nodes	I
lead	O
to	O
over-learning	O
of	O
individual	O
samples	O
at	O
the	O
expense	O
of	O
forming	O
a	O
near	O
optimal	O
model	O
of	O
the	O
data	O
distributions	O
underlying	O
the	O
training	O
data	O
in	O
this	O
case	O
previously	O
unseen	O
patterns	O
are	O
labeled	O
according	O
to	O
the	O
nearest	B
neighbour	I
rather	O
than	O
in	O
accordance	O
with	O
a	O
good	O
model	O
of	O
the	O
problem	O
an	O
easy	O
to	O
read	O
introduction	O
to	O
the	O
issues	O
invloved	O
in	O
over-training	O
a	O
network	O
can	O
be	O
found	O
in	O
geman	O
early	O
constructive	B
algorithms	I
such	O
as	O
upstart	B
and	O
the	O
tiling	B
algorithm	I
ezard	O
nadal	O
built	O
multi-layer	O
feed-forward	B
networks	I
of	O
perceptron	B
units	O
which	O
could	O
be	O
applied	O
to	O
problems	O
involving	O
binary	O
input	B
patterns	O
convergence	O
of	O
such	O
algorithms	O
is	O
guaranteed	O
if	O
the	O
data	O
is	O
linearly	O
separable	O
and	O
use	O
of	O
the	O
pocket	O
algorithm	O
for	O
training	O
allows	O
an	O
approximate	O
solution	O
to	O
be	O
found	O
for	O
non	O
linearly-separable	O
datasets	O
these	O
networks	O
do	O
not	O
usually	O
include	O
a	O
stopping	O
criterion	O
to	O
halt	O
the	O
creation	O
of	O
new	O
layers	O
or	O
nodes	O
so	O
every	O
sample	O
in	O
the	O
training	O
data	O
is	O
learned	O
this	O
has	O
strong	O
repercussions	O
if	O
the	O
training	B
set	I
is	O
incomplete	O
has	O
noise	B
or	O
is	O
derived	O
from	O
a	O
classification	B
problem	O
where	O
the	O
distributions	O
overlap	O
later	O
methods	O
apply	O
to	O
more	O
general	O
problems	O
and	O
are	O
suitable	O
for	O
statistical	B
classification	B
problems	O
fahlman	O
lebiere	O
hanson	O
refenes	O
vithlani	O
and	O
wynne-jones	O
they	O
often	O
build	O
a	O
single	O
hidden	B
layer	O
and	O
incorporate	O
stopping	O
criteria	O
which	O
allow	O
them	O
to	O
converge	O
to	O
solutions	O
with	O
good	O
generalisation	O
u	O
g	O
w	O
u	O
v	O
v	O
v	O
sec	O
supervised	B
networks	I
for	O
classification	B
ability	O
for	O
statistical	B
problems	O
cascade	B
correlation	B
lebiere	O
is	O
an	O
example	B
of	O
such	O
a	O
network	O
algorithm	O
and	O
is	O
described	O
below	O
pruning	B
has	O
been	O
carried	O
out	O
on	O
networks	O
in	O
three	O
ways	O
the	O
first	O
is	O
a	O
heuristic	O
approach	O
based	O
on	O
identifying	O
which	O
nodes	O
or	O
weights	O
contribute	O
little	O
to	O
the	O
mapping	O
after	O
these	O
have	O
been	O
removed	O
additional	O
training	O
leads	O
to	O
a	O
better	O
network	O
than	O
the	O
original	O
an	O
alternative	O
technique	O
is	O
to	O
include	O
terms	O
in	O
the	O
error	O
function	O
so	O
that	O
weights	O
tend	O
to	O
zero	O
under	O
certain	O
circumstances	O
zero	O
weights	O
can	O
then	O
be	O
removed	O
without	O
degrading	O
the	O
network	O
performance	O
this	O
approach	O
is	O
the	O
basis	O
of	O
regularisation	B
discussed	O
in	O
more	O
detail	O
below	O
finally	O
if	O
we	O
define	O
the	O
sensitivity	O
of	O
the	O
global	O
network	O
error	O
to	O
the	O
removal	O
of	O
a	O
weight	O
or	O
node	O
we	O
can	O
remove	O
the	O
weights	O
or	O
nodes	O
to	O
which	O
the	O
global	O
error	O
is	O
least	O
sensitive	O
the	O
sensitivity	O
measure	B
does	O
not	O
interfere	O
with	O
training	O
and	O
involves	O
only	O
a	O
small	O
amount	O
of	O
extra	O
computational	O
effort	O
a	O
full	O
review	O
of	O
these	O
techniques	O
can	O
be	O
found	O
in	O
wynne-jones	O
cascade	B
correlation	B
a	O
constructive	O
feed-forward	O
network	O
cascade	B
correlation	B
is	O
a	O
paradigm	O
for	O
building	O
a	O
feed-forward	O
network	O
as	O
training	O
proceeds	O
in	O
a	O
supervised	O
mode	O
lebiere	O
instead	O
of	O
adjusting	O
the	O
weights	O
in	O
a	O
fixed	O
architecture	O
it	O
begins	O
with	O
a	O
small	O
network	O
and	O
adds	O
new	O
hidden	B
nodes	I
one	O
by	O
one	O
creating	O
a	O
multi-layer	O
structure	O
once	O
a	O
hidden	B
node	O
has	O
been	O
added	O
to	O
a	O
network	O
its	O
input-side	O
weights	O
are	O
frozen	O
and	O
it	O
becomes	O
a	O
permanent	O
feature-detector	O
in	O
the	O
network	O
available	O
for	O
output	B
or	O
for	O
creating	O
other	O
more	O
complex	B
feature	O
detectors	O
in	O
later	O
layers	O
cascade	B
correlation	B
can	O
offer	O
reduced	O
training	O
time	B
and	O
it	O
determines	O
the	O
size	O
and	O
topology	O
of	O
networks	O
automatically	O
cascade	B
correlation	B
combines	O
two	O
ideas	O
first	O
the	O
cascade	B
architecture	O
in	O
which	O
hidden	B
nodes	I
are	O
added	O
one	O
at	O
a	O
time	B
each	O
using	O
the	O
outputs	O
of	O
all	O
others	O
in	O
addition	O
to	O
the	O
input	B
nodes	O
and	O
second	O
the	O
maximisation	O
of	O
the	O
correlation	B
between	O
a	O
new	O
unit	O
s	O
output	B
and	O
the	O
residual	O
classification	B
error	O
of	O
the	O
parent	O
network	O
each	O
node	O
added	O
to	O
the	O
network	O
may	O
be	O
of	O
any	O
kind	O
examples	O
include	O
linear	O
nodes	O
which	O
can	O
be	O
trained	O
using	O
linear	O
algorithms	O
threshold	O
nodes	O
such	O
as	O
single	O
perceptrons	O
where	O
simple	O
learning	O
rules	O
such	O
as	O
the	O
delta	O
rule	O
or	O
the	O
pocket	O
algorithm	O
can	O
be	O
used	O
or	O
non-linear	O
nodes	O
such	O
as	O
sigmoids	O
or	O
gaussian	O
functions	O
requiring	O
delta	O
rules	O
or	O
more	O
advanced	O
algorithms	O
such	O
as	O
fahlman	O
s	O
quickprop	O
standard	O
mlp	B
sigmoids	O
were	O
used	O
in	O
the	O
statlog	B
trials	O
at	O
each	O
stage	O
in	O
training	O
each	O
node	O
in	O
a	O
pool	O
of	O
candidate	O
nodes	O
is	O
trained	O
on	O
the	O
residual	O
error	O
of	O
the	O
parent	O
network	O
of	O
these	O
nodes	O
the	O
one	O
whose	O
output	B
has	O
the	O
greatest	O
correlation	B
with	O
the	O
error	O
of	O
the	O
parent	O
is	O
added	O
permanently	O
to	O
the	O
network	O
the	O
error	O
the	O
sum	O
over	O
all	O
output	B
units	O
of	O
the	O
magnitude	O
of	O
the	O
candidate	O
unit	O
s	O
value	O
the	O
correlation	B
more	O
precisely	O
the	O
covariance	B
between	O
function	O
minimised	O
in	O
this	O
scheme	O
is	O
for	O
example	B
the	O
residual	O
error	O
observed	O
at	O
output	B
unit	O
and	O
c	O
the	O
quantities	O
are	O
the	O
values	O
of	O
and	O
averaged	O
over	O
all	O
patterns	O
and	O
each	O
of	O
the	O
weights	O
coming	O
into	O
the	O
nodei	O
in	O
order	O
to	O
maximise	O
thus	O
is	O
defined	O
by	O
the	O
partial	O
derivative	O
of	O
the	O
error	O
is	O
calculated	O
with	O
respect	O
to	O
g	O
g	O
neural	B
networks	I
dc	O
is	O
the	O
sign	O
of	O
the	O
correlation	B
between	O
the	O
candidate	O
s	O
value	O
and	O
the	O
output	B
where	O
is	O
the	O
derivative	O
for	O
pattern	O
of	O
the	O
candidate	O
unit	O
s	O
activation	O
function	O
withe	O
respect	O
to	O
the	O
is	O
the	O
input	B
the	O
candidate	O
unit	O
receives	O
for	O
pattern	O
sum	O
of	O
its	O
inputs	O
and	O
the	O
partial	O
derivatives	O
are	O
used	O
to	O
perform	O
gradient	O
ascent	O
to	O
maximise	O
when	O
no	O
longer	O
improves	O
in	O
training	O
for	O
any	O
of	O
the	O
candidate	O
nodes	O
the	O
best	O
candidate	O
is	O
added	O
to	O
the	O
network	O
and	O
the	O
others	O
are	O
scrapped	O
in	O
benchmarks	O
on	O
a	O
toy	O
problem	O
involving	O
classification	B
of	O
data	O
points	O
forming	O
two	O
interlocked	O
spirals	O
cascade	B
correlation	B
is	O
reported	O
to	O
be	O
ten	O
to	O
one	O
hundred	O
times	O
faster	O
than	O
conventional	O
back-propagation	O
of	O
error	O
derivatives	O
in	O
a	O
fixed	O
architecture	O
network	O
empirical	O
tests	O
on	O
a	O
range	O
of	O
real	O
problems	O
honavar	O
indicate	O
a	O
speedup	O
of	O
one	O
to	O
two	O
orders	O
of	O
magnitude	O
with	O
minimal	O
degradation	O
of	O
classification	B
accuracy	B
these	O
results	O
were	O
only	O
obtained	O
after	O
many	O
experiments	O
to	O
determine	O
suitable	O
values	O
for	O
the	O
many	O
parameters	O
which	O
need	O
to	O
be	O
set	O
in	O
the	O
cascade	B
correlation	B
implementation	O
cascade	B
correlation	B
can	O
also	O
be	O
implemented	O
in	O
computers	O
with	O
limited	O
precision	O
and	O
in	O
recurrent	B
networks	I
fahlman	O
bayesian	O
regularisation	B
in	O
recent	O
years	O
the	O
formalism	O
of	O
bayesian	O
probability	O
theory	O
has	O
been	O
applied	O
to	O
the	O
treatment	O
of	O
feedforward	O
neural	O
network	O
models	O
as	O
nonlinear	B
regression	I
problems	O
this	O
has	O
brought	O
about	O
a	O
greatly	O
improved	O
understanding	O
of	O
the	O
generalisation	O
problem	O
and	O
some	O
new	O
techniques	O
to	O
improve	O
generalisation	O
none	O
of	O
these	O
techniques	O
were	O
used	O
in	O
the	O
numerical	O
experiments	O
described	O
in	O
this	O
book	O
but	O
a	O
short	O
introduction	O
to	O
this	O
subject	O
is	O
provided	O
here	O
latter	O
technique	O
is	O
marginalisation	B
a	O
reasonable	O
scenario	O
for	O
a	O
bayesian	O
treatment	O
of	O
feedforward	O
neural	B
networks	I
is	O
to	O
through	O
some	O
network	O
and	O
corrupting	O
the	O
output	B
with	O
noise	B
from	O
a	O
stationary	O
source	O
the	O
network	O
involved	O
is	O
assumed	O
to	O
have	O
been	O
drawn	O
from	O
a	O
probability	O
in	O
this	O
distribution	O
can	O
presume	O
that	O
each	O
target	O
training	O
data	O
vectoru	O
was	O
produced	O
by	O
running	O
the	O
corresponding	O
input	B
training	O
vectort	O
g	O
which	O
is	O
to	O
be	O
estimated	O
the	O
most	O
probablew	O
be	O
used	O
as	O
the	O
optimal	O
classifier	B
or	O
a	O
more	O
sophisticated	O
average	O
g	O
can	O
be	O
used	O
at	O
a	O
particular	O
point	O
for	O
examples	O
g	O
would	O
designate	O
this	O
density	O
at	O
the	O
particular	O
pointw	O
ands	O
unsignificantly	O
has	O
the	O
same	O
name	O
as	O
the	O
label	O
index	O
ofs	O
g	O
whens	O
furthermore	O
adopt	O
the	O
common	O
practice	O
of	O
the	O
notation	O
used	O
here	O
for	O
probability	O
densities	O
is	O
somewhat	O
cavalier	O
in	O
discussions	O
involving	O
several	O
probability	O
density	O
functions	O
the	O
notation	O
should	O
distinguish	O
one	O
density	O
function	O
from	O
another	O
and	O
further	O
notation	O
should	O
be	O
used	O
when	O
such	O
a	O
density	O
is	O
indicated	O
can	O
designate	O
the	O
density	O
function	O
over	O
weights	O
which	O
confusingly	O
and	O
however	O
a	O
tempting	O
opportunity	O
to	O
choose	O
names	O
which	O
introduce	O
this	O
confusion	O
will	O
arise	O
in	O
almost	O
every	O
instance	O
that	O
a	O
density	O
function	O
is	O
mentioned	O
so	O
we	O
shall	O
not	O
only	O
succumb	O
to	O
the	O
temptation	O
but	O
is	O
meant	O
in	O
order	O
to	O
be	O
concise	O
technically	O
this	O
is	O
an	O
appalling	O
case	O
of	O
using	O
a	O
function	O
argument	O
name	O
is	O
ordinarily	O
arbitrary	O
to	O
designate	O
the	O
function	O
the	O
bayesian	O
analysis	O
is	O
built	O
on	O
a	O
probabilistic	O
interpretation	O
of	O
the	O
error	O
measure	B
used	O
in	O
training	O
typically	O
as	O
in	O
equations	O
or	O
it	O
is	O
additive	O
over	O
input-output	O
pairs	O
i	O
g	O
y	O
y	O
w	O
w	O
w	O
g	O
sec	O
supervised	B
networks	I
for	O
classification	B
i	O
e	O
it	O
can	O
be	O
expressed	O
as	O
an	O
integral	O
over	O
all	O
possible	O
target	O
training	O
data	O
sets	O
of	O
the	O
size	O
under	O
consideration	O
is	O
all	O
the	O
training	O
data	O
the	O
set	O
of	O
input-output	O
pairs	O
in	O
the	O
drawn	O
from	O
a	O
distribution	O
with	O
correspond	O
to	O
different	O
probabilistic	O
interpretations	O
given	O
this	O
assumption	O
and	O
the	O
assumption	O
that	O
training	O
data	O
samples	O
are	O
produced	O
independently	O
of	O
each	O
other	O
for	O
some	O
function	O
whereu	O
is	O
composed	O
of	O
all	O
the	O
input	B
datat	O
sum	O
u	O
regarded	O
as	O
fixed	O
and	O
all	O
the	O
target	O
datau	O
function	O
oft	O
regarded	O
as	O
a	O
noise-corruptedw	O
g	O
density	O
assumption	O
g	O
the	O
bayesian	O
argument	O
requires	O
the	O
alone	O
thus	O
different	O
choices	O
of	O
is	O
a	O
function	O
of	O
the	O
relationship	O
between	O
g	O
ands	O
g	O
can	O
only	O
have	O
the	O
form	O
for	O
some	O
parameter	O
un	O
if	O
in	O
is	O
a	O
function	O
only	O
ofv	O
as	O
is	O
then	O
ofw	O
a	O
result	O
which	O
is	O
useful	O
later	O
the	O
only	O
common	O
form	O
of	O
which	O
does	O
not	O
have	O
which	O
case	O
and	O
together	O
justify	O
the	O
assumption	O
thats	O
f	O
so	O
f	O
and	O
is	O
still	O
independent	O
ofw	O
g	O
depends	O
only	O
on	O
g	O
and	O
imply	O
for	O
that	O
the	O
probability	O
of	O
the	O
weights	O
given	O
the	O
probability	O
of	O
the	O
data	O
given	O
the	O
g	O
likelihood	O
but	O
unfortunately	O
the	O
cbw	O
can	O
be	O
used	O
to	O
converts	O
g	O
from	O
equation	O
and	O
a	O
prior	O
over	O
the	O
weightss	O
into	O
the	O
desired	O
distribution	O
the	O
probability	O
of	O
the	O
g	O
g	O
is	O
given	O
by	O
the	O
normalisation	O
there	O
is	O
a	O
further	O
technicality	O
the	O
integral	O
over	O
target	O
data	O
must	O
be	O
with	O
respect	O
to	O
uniform	B
measure	B
density	O
can	O
also	O
be	O
derived	O
from	O
somewhat	O
different	O
assumptions	O
using	O
a	O
maximum-entropy	O
argument	O
van	O
den	O
bout	O
it	O
plays	O
a	O
prominent	O
role	O
in	O
thermodynamics	O
and	O
thermodynamics	O
jargon	O
has	O
drifted	O
into	O
the	O
neural	B
networks	I
literature	O
partly	O
in	O
consequence	O
of	O
the	O
analogies	O
it	O
underlies	O
w	O
this	O
form	O
is	O
the	O
cross-entropy	B
but	O
this	O
is	O
normally	O
used	O
in	O
classification	B
problems	O
in	O
additivity	O
argument	O
does	O
not	O
go	O
through	O
for	O
this	O
instead	O
bayes	B
rule	I
condition	O
as	O
which	O
may	O
not	O
always	O
be	O
reasonable	O
is	O
the	O
normalisation	O
term	O
turns	O
out	O
to	O
be	O
independent	O
is	O
of	O
greater	O
interest	O
than	O
the	O
u	O
g	O
g	O
w	O
w	O
t	O
w	O
g	O
u	O
w	O
g	O
g	O
g	O
w	O
w	O
g	O
f	O
w	O
y	O
u	O
z	O
w	O
w	O
t	O
y	O
u	O
z	O
w	O
w	O
w	O
w	O
u	O
g	O
w	O
u	O
g	O
s	O
w	O
g	O
s	O
g	O
g	O
w	O
g	O
t	O
w	O
g	O
s	O
g	O
neural	B
networks	I
w	O
this	O
ensures	O
that	O
the	O
denominator	O
of	O
g	O
must	O
express	O
the	O
g	O
is	O
given	O
by	O
normalisation	O
assembling	O
all	O
the	O
pieces	O
the	O
posterior	O
probability	O
of	O
the	O
weights	O
given	O
the	O
data	O
is	O
in	O
this	O
case	O
s	O
g	O
the	O
bayesian	O
method	O
helps	O
with	O
one	O
of	O
the	O
most	O
troublesome	O
steps	O
in	O
the	O
regularisation	B
approach	O
to	O
obtaining	O
good	O
generalisation	O
deciding	O
the	O
values	O
of	O
the	O
regularisation	B
is	O
additive	O
over	O
the	O
weights	O
and	O
an	O
independence	O
assumption	O
like	O
is	O
reasonable	O
so	O
given	O
that	O
the	O
prior	O
depends	O
only	O
on	O
the	O
regularisation	B
term	O
then	O
it	O
has	O
the	O
form	O
notion	O
that	O
some	O
weight	O
matrices	O
are	O
more	O
reasonable	O
a	O
priori	O
than	O
others	O
as	O
discussed	O
above	O
this	O
is	O
normally	O
expressed	O
through	O
regularisation	B
terms	O
added	O
to	O
the	O
error	O
measure	B
for	O
example	B
the	O
view	O
that	O
large	O
weights	O
are	O
unreasonable	O
might	O
be	O
expressed	O
by	O
adding	O
a	O
bayesian	B
methods	I
inevitably	O
require	O
a	O
priors	O
to	O
weight	O
decay	O
term	O
of	O
the	O
form	O
w	O
typically	O
the	O
regularisation	B
error	O
b	O
where	O
provided	O
that	O
does	O
not	O
depend	O
onw	O
does	O
not	O
depend	O
onw	O
finds	O
the	O
maximum	O
so	O
the	O
usual	O
training	O
process	O
of	O
minimising	O
hy	O
expresses	O
the	O
relative	O
importance	O
of	O
smoothing	O
and	O
data-fitting	O
parameters	O
the	O
ratio	O
jg	O
jg	O
c	O
if	O
a	O
uniform	B
priors	O
are	O
those	O
which	O
maximise	O
the	O
jg	O
c	O
opposes	O
the	O
goal	O
of	O
weightsw	O
g	O
the	O
regularisation	B
parameters	O
and	O
imising	O
g	O
one	O
attempts	O
to	O
find	O
a	O
priors	O
w	O
fit	O
the	O
datau	O
well	O
this	O
objective	O
is	O
not	O
diametrically	O
opposed	O
to	O
the	O
later	O
objective	O
of	O
g	O
under	O
which	O
usually	O
networks	O
selecting	O
the	O
best-fittingw	O
indeed	O
the	O
distributions	O
is	O
one	O
which	O
is	O
concentrated	O
on	O
a	O
single	O
overfitw	O
g	O
which	O
maximises	O
the	O
evidence	O
jg	O
which	O
is	O
given	O
by	O
the	O
denomcbw	O
this	O
is	O
prevented	O
only	O
if	O
the	O
the	O
distribution	O
of	O
weight	O
matrices	O
parameterised	O
by	O
the	O
regularisation	B
parameters	O
does	O
not	O
include	O
such	O
highly	O
concentrated	O
distributions	O
therefore	O
it	O
remains	O
an	O
art	O
to	O
select	O
reasonable	O
functional	O
forms	O
for	O
the	O
regularisers	O
but	O
once	O
selected	O
the	O
determination	O
of	O
the	O
parameters	O
which	O
deserves	O
to	O
be	O
decided	O
in	O
a	O
principled	O
manner	O
the	O
bayesian	B
evidence	I
formalism	O
provides	O
a	O
principle	O
and	O
an	O
implementation	O
it	O
can	O
be	O
computationally	O
demanding	O
if	O
used	O
precisely	O
but	O
there	O
are	O
practicable	O
approximations	O
the	O
evidence	O
formalism	O
simply	O
assumes	O
a	O
prior	O
distribution	O
over	O
the	O
regularisation	B
inator	O
of	O
note	O
with	O
reference	O
to	O
that	O
the	O
goal	O
of	O
maximising	O
the	O
evidence	O
and	O
the	O
are	O
optimised	O
for	O
opposing	O
purposes	O
this	O
expresses	O
the	O
bayesian	O
quantification	O
this	O
method	O
of	O
setting	O
regularisation	B
parameters	O
does	O
not	O
provide	O
a	O
guarantee	O
against	O
overfitting	B
but	O
it	O
helps	O
in	O
setting	O
the	O
regularisation	B
parameters	O
by	O
max	O
parameters	O
and	O
sharpens	O
it	O
using	O
bayes	B
rule	I
is	O
assumed	O
then	O
the	O
most	O
likely	O
regularisation	B
parameters	O
of	O
the	O
compromise	O
between	O
data	O
fitting	O
and	O
smoothing	O
g	O
g	O
s	O
g	O
f	O
w	O
d	O
u	O
g	O
y	O
u	O
z	O
w	O
d	O
w	O
y	O
y	O
u	O
z	O
d	O
g	O
u	O
u	O
g	O
s	O
g	O
u	O
sec	O
unsupervised	B
learning	I
themselves	O
is	O
a	O
matter	O
of	O
calculation	O
the	O
art	O
of	O
selecting	O
regularisation	B
functions	O
has	O
become	O
an	O
interesting	O
research	O
area	O
hinton	O
the	O
calculation	O
of	O
involves	O
an	O
integration	O
which	O
is	O
generally	O
non-trivial	O
but	O
which	O
can	O
be	O
done	O
easily	O
in	O
a	O
gaussian	O
approximation	O
typically	O
this	O
is	O
good	O
enough	O
this	O
requires	O
computation	O
of	O
the	O
second	O
derivatives	O
of	O
the	O
error	O
measure	B
which	O
is	O
prohibitive	O
for	O
large	O
problems	O
but	O
in	O
this	O
case	O
a	O
further	O
approximation	O
is	O
possible	O
and	O
often	O
adequate	O
unsupervised	B
learning	I
interest	O
in	O
unsupervised	B
learning	I
has	O
increased	O
greatly	O
in	O
recent	O
years	O
it	O
offers	O
the	O
possibility	O
of	O
exploring	O
the	O
structure	O
of	O
data	O
without	O
guidance	O
in	O
the	O
form	O
of	O
class	B
information	O
and	O
can	O
often	O
reveal	O
features	B
not	O
previously	O
expected	O
or	O
known	O
about	O
these	O
might	O
include	O
the	O
division	O
of	O
data	O
that	O
was	O
previously	O
thought	O
to	O
be	O
a	O
single	O
uniform	B
cluster	O
into	O
a	O
number	O
of	O
smaller	O
groups	O
each	O
with	O
separate	O
identifiable	O
properties	O
the	O
clusters	O
found	O
offer	O
a	O
model	O
of	O
the	O
data	O
in	O
terms	O
of	O
cluster	O
centres	O
sizes	O
and	O
shapes	O
which	O
can	O
often	O
be	O
described	O
using	O
less	O
information	O
and	O
in	O
fewer	O
parameters	O
than	O
were	O
required	O
to	O
store	O
the	O
entire	O
training	O
data	O
set	O
this	O
has	O
obvious	O
advantages	O
for	O
storing	O
coding	O
and	O
transmitting	O
stochastically	O
generated	O
data	O
if	O
its	O
distribution	O
in	O
the	O
attribute	O
space	O
is	O
known	O
equivalent	O
data	O
can	O
be	O
generated	O
from	O
the	O
model	O
when	O
required	O
while	O
general	O
unsupervised	B
learning	I
methods	O
such	O
as	O
boltzmann	O
machines	O
are	O
computationally	O
expensive	O
iterative	O
clustering	B
algorithms	O
such	O
as	O
kohonen	B
networks	I
k-means	B
clustering	B
and	O
gaussian	O
mixture	O
models	O
offer	O
the	O
same	O
modelling	O
power	O
with	O
greatly	O
reduced	O
training	O
time	B
indeed	O
while	O
class	B
labels	O
are	O
not	O
used	O
to	O
constrain	O
the	O
structure	O
learned	O
by	O
the	O
models	O
freedom	O
from	O
this	O
constraint	O
coupled	O
with	O
careful	O
initialisation	O
of	O
the	O
models	O
using	O
any	O
prior	O
information	O
available	O
about	O
the	O
data	O
can	O
yield	O
very	O
quick	O
and	O
effective	O
models	O
these	O
models	O
known	O
collectively	O
as	O
vector	B
quantizers	I
can	O
be	O
used	O
as	O
the	O
non-linear	O
part	O
of	O
supervised	B
learning	I
models	O
in	O
this	O
case	O
a	O
linear	O
part	O
is	O
added	O
and	O
trained	O
later	O
to	O
implement	O
the	O
mapping	O
from	O
activation	O
in	O
different	O
parts	O
of	O
the	O
model	O
to	O
probable	O
classes	B
of	O
event	O
generating	O
the	O
data	O
the	O
k-means	B
clustering	B
algorithm	O
the	O
principle	O
of	O
clustering	B
requires	O
a	O
representation	O
of	O
a	O
set	O
of	O
data	O
to	O
be	O
found	O
which	O
offers	O
a	O
model	O
of	O
the	O
distribution	O
of	O
samples	O
in	O
the	O
attribute	O
space	O
the	O
k-means	O
algorithm	O
example	B
krishnaiah	O
kanal	O
achieves	O
this	O
quickly	O
and	O
efficiently	O
as	O
a	O
model	O
with	O
a	O
fixed	O
number	O
of	O
cluster	O
centres	O
determined	O
by	O
the	O
user	O
in	O
advance	O
the	O
cluster	O
centres	O
are	O
initially	O
chosen	O
from	O
the	O
data	O
and	O
each	O
centre	O
forms	O
the	O
code	B
vector	I
for	O
the	O
patch	O
of	O
the	O
input	B
space	O
in	O
which	O
all	O
points	O
are	O
closer	O
to	O
that	O
centre	O
than	O
to	O
any	O
other	O
this	O
division	O
of	O
the	O
space	O
into	O
patches	O
is	O
known	O
as	O
a	O
voronoi	B
tessellation	I
since	O
the	O
initial	O
allocation	O
of	O
centres	O
may	O
not	O
form	O
a	O
good	O
model	O
of	O
the	O
probability	O
distribution	O
function	O
of	O
the	O
input	B
space	O
there	O
follows	O
a	O
series	O
of	O
iterations	O
where	O
each	O
cluster	O
centre	O
is	O
moved	O
to	O
the	O
mean	O
position	O
of	O
all	O
the	O
training	O
patterns	O
in	O
its	O
tessellation	O
region	O
a	O
generalised	O
variant	O
of	O
the	O
k-means	O
algorithm	O
is	O
the	O
gaussian	O
mixture	O
model	O
or	O
adaptive	O
k-means	O
in	O
this	O
scheme	O
voronoi	O
tessellations	O
are	O
replaced	O
with	O
soft	O
transitions	O
from	O
one	O
centre	O
s	O
receptive	O
field	O
to	O
another	O
s	O
this	O
is	O
achieved	O
by	O
assigning	O
a	O
variance	O
to	O
each	O
centre	O
thereby	O
defining	O
a	O
gaussian	O
kernel	O
at	O
each	O
centre	O
these	O
kernels	B
are	O
mixed	O
neural	B
networks	I
fig	O
k-means	B
clustering	B
within	O
each	O
patch	O
the	O
centre	O
is	O
moved	O
to	O
the	O
mean	O
position	O
of	O
the	O
patterns	O
together	O
by	O
a	O
set	O
of	O
mixing	O
weights	O
to	O
approximate	O
the	O
pdf	O
of	O
the	O
input	B
data	O
and	O
an	O
efficient	O
algorithm	O
exists	O
to	O
calculate	O
iteratively	O
a	O
set	O
of	O
mixing	O
weights	O
centres	O
and	O
variances	O
for	O
the	O
centres	O
jain	O
and	O
wu	O
chan	O
while	O
the	O
number	O
of	O
centres	O
for	O
these	O
algorithms	O
is	O
fixed	O
in	O
advance	O
in	O
more	O
popular	O
implementations	O
some	O
techniques	O
are	O
appearing	O
which	O
allow	O
new	O
centres	O
to	O
be	O
added	O
as	O
training	O
proceeds	O
and	O
kohonen	B
networks	I
and	O
learning	B
vector	B
quantizers	I
kohonen	B
s	O
network	O
algorithm	O
also	O
provides	O
a	O
voronoi	B
tessellation	I
of	O
the	O
input	B
space	O
into	O
patches	O
with	O
corresponding	O
code	O
vectors	O
it	O
has	O
the	O
additional	O
feature	O
that	O
the	O
centres	O
are	O
arranged	O
in	O
a	O
low	O
dimensional	O
structure	O
a	O
string	O
or	O
a	O
square	O
grid	O
such	O
that	O
nearby	O
points	O
in	O
the	O
topological	O
structure	O
string	O
or	O
grid	O
map	O
to	O
nearby	O
points	O
in	O
the	O
attribute	O
space	O
structures	O
of	O
this	O
kind	O
are	O
thought	O
to	O
occur	O
in	O
nature	O
for	O
example	B
in	O
the	O
mapping	O
from	O
the	O
ear	O
to	O
the	O
auditory	O
cortex	O
and	O
the	O
retinotopic	O
map	O
from	O
the	O
retina	O
to	O
the	O
visual	O
cortex	O
or	O
optic	O
tectum	O
in	O
training	O
the	O
winning	B
node	O
of	O
the	O
network	O
which	O
is	O
the	O
nearest	O
node	O
in	O
the	O
input	B
space	O
to	O
a	O
given	O
training	O
pattern	O
moves	O
towards	O
that	O
training	O
pattern	O
while	O
dragging	O
with	O
its	O
neighbouring	O
nodes	O
in	O
the	O
network	O
topology	O
this	O
leads	O
to	O
a	O
smooth	O
distribution	O
of	O
the	O
network	O
topology	O
in	O
a	O
non-linear	O
subspace	O
of	O
the	O
training	O
data	O
vector	B
quantizers	I
that	O
conserve	O
topographic	O
relations	O
between	O
centres	O
are	O
also	O
particularly	O
useful	O
in	O
communications	O
where	O
noise	B
added	O
to	O
the	O
coded	O
vectors	O
may	O
corrupt	O
the	O
representation	O
a	O
little	O
the	O
topographic	O
mapping	O
ensures	O
that	O
a	O
small	O
change	O
in	O
code	B
vector	I
is	O
decoded	O
as	O
a	O
small	O
change	O
in	O
attribute	O
space	O
and	O
hence	O
a	O
small	O
change	O
at	O
the	O
output	B
these	O
models	O
have	O
been	O
studied	O
extensively	O
and	O
recently	O
unified	O
under	O
the	O
framework	O
of	O
bayes	O
theory	O
although	O
it	O
is	O
fundamentally	O
an	O
unsupervised	B
learning	I
algorithm	O
the	O
learning	B
vector	I
quantizer	I
can	O
be	O
used	O
as	O
a	O
supervised	B
vector	I
quantizer	O
where	O
network	O
nodes	O
have	O
class	B
labels	O
associated	O
with	O
them	O
the	O
kohonen	B
learning	O
rule	O
is	O
used	O
when	O
the	O
winning	B
node	O
represents	O
the	O
same	O
class	B
as	O
a	O
new	O
training	O
pattern	O
while	O
a	O
difference	O
in	O
class	B
between	O
sec	O
the	O
winning	B
node	O
and	O
a	O
training	O
pattern	O
causes	O
the	O
node	O
to	O
move	O
away	O
from	O
the	O
training	O
pattern	O
by	O
the	O
same	O
distance	B
learning	B
vector	B
quantizers	I
are	O
reported	O
to	O
give	O
excellent	O
performance	O
in	O
studies	O
on	O
statistical	B
and	O
speech	O
data	O
et	O
al	O
argmax	O
ramnets	B
one	O
of	O
the	O
oldest	O
practical	O
neurally-inspired	O
classification	B
algorithms	O
is	O
still	O
one	O
of	O
the	O
best	O
it	O
is	O
the	O
n-tuple	O
recognition	O
method	O
introduced	O
by	O
bledsoe	O
browning	O
and	O
bledsoe	O
which	O
later	O
formed	O
the	O
basis	O
of	O
a	O
commercial	O
product	O
known	O
as	O
wisard	O
et	O
al	O
the	O
algorithm	O
is	O
simple	O
the	O
patterns	O
to	O
be	O
classified	O
are	O
bit	O
these	O
are	O
the	O
n-tuples	O
the	O
restriction	O
of	O
a	O
pattern	O
to	O
an	O
n-tuple	O
can	O
be	O
regarded	O
as	O
an	O
n-bit	O
number	O
which	O
constitutes	O
a	O
feature	O
of	O
the	O
pattern	O
a	O
pattern	O
is	O
classified	O
as	O
belonging	O
to	O
the	O
class	B
for	O
which	O
it	O
has	O
the	O
most	O
features	B
in	O
common	O
with	O
at	O
least	O
pattern	O
in	O
the	O
training	O
data	O
strings	O
of	O
a	O
given	O
length	O
several	O
us	O
say	O
sets	O
ofk	O
bit	O
locations	O
are	O
selected	O
randomly	O
to	O
be	O
precise	O
the	O
class	B
assigned	O
to	O
unclassified	O
pattern	O
xl	O
zz	O
xba	O
db	O
xba	O
d	O
where	O
for	O
is	O
the	O
set	O
of	O
training	O
patterns	O
in	O
classm	O
is	O
the	O
kronecker	O
delta	O
c	O
and	O
otherwise	O
and	O
if	O
feature	O
of	O
pattern	O
x	O
e	O
th	O
bit	O
of	O
and	O
ic	O
th	O
bit	O
of	O
the	O
here	O
is	O
the	O
is	O
the	O
classes	B
to	O
distinguish	O
the	O
system	O
can	O
be	O
implemented	O
as	O
a	O
set	O
of	O
with	O
th	O
ram	O
allocated	O
to	O
classm	O
at	O
address	O
of	O
the	O
in	O
which	O
the	O
memory	B
content	O
xa	O
d	O
is	O
set	O
if	O
any	O
pattern	O
of	O
has	O
feature	O
thus	O
and	O
unset	O
otherwise	O
recognition	O
is	O
accomplished	O
by	O
tallying	O
the	O
set	O
bits	O
in	O
the	O
rams	O
of	O
each	O
class	B
at	O
the	O
addresses	O
given	O
by	O
the	O
features	B
of	O
the	O
unclassified	O
pattern	O
ramnets	B
are	O
impressive	O
in	O
that	O
they	O
can	O
be	O
trained	O
faster	O
than	O
mlps	O
or	O
radial	B
basis	I
function	I
networks	O
by	O
orders	O
of	O
magnitude	O
and	O
often	O
provide	O
comparable	O
results	O
experimental	O
comparisons	O
between	O
ramnets	B
and	O
other	O
methods	O
can	O
be	O
found	O
in	O
rohwer	O
cressy	O
rams	O
is	O
th	O
n-tuple	O
is	O
c	O
for	O
th	O
is	O
the	O
this	O
is	O
something	O
of	O
a	O
hybrid	O
algorithm	O
which	O
has	O
much	O
in	O
common	O
with	O
both	O
logistic	B
discrimination	B
and	O
some	O
of	O
the	O
nonparametric	O
statistical	B
methods	O
however	O
for	O
historical	O
reasons	O
it	O
is	O
included	O
here	O
m	O
x	O
g	O
g	O
f	O
f	O
c	O
g	O
g	O
d	O
g	O
x	O
z	O
neural	B
networks	I
introduction	O
is	O
a	O
learning	O
algorithm	O
which	O
constructs	O
an	O
optimised	O
piecewise	O
linear	O
classifier	B
by	O
a	O
two	O
step	O
procedure	O
in	O
the	O
first	O
step	O
the	O
initial	O
positions	O
of	O
the	O
discriminating	O
hyperplanes	O
are	O
determined	O
by	O
pairwise	O
linear	B
regression	I
to	O
optimise	O
these	O
positions	O
in	O
relation	O
to	O
the	O
misclassified	O
patterns	O
an	O
error	O
criterion	O
function	O
is	O
defined	O
this	O
function	O
is	O
then	O
minimised	O
by	O
a	O
gradient	B
descent	I
procedure	O
for	O
each	O
hyperplane	O
separately	O
as	O
an	O
option	O
in	O
the	O
case	O
of	O
non	O
convex	O
classes	B
if	O
a	O
class	B
has	O
a	O
multimodal	O
probability	O
distribution	O
a	O
clustering	B
procedure	O
decomposing	O
the	O
classes	B
into	O
appropriate	O
subclasses	O
can	O
be	O
applied	O
this	O
case	O
is	O
really	O
a	O
three	O
step	O
procedure	O
seen	O
from	O
a	O
more	O
general	O
point	O
of	O
view	O
is	O
a	O
combination	O
of	O
a	O
statistical	B
part	O
with	O
a	O
learning	O
procedure	O
typical	O
for	O
artificial	O
neural	O
nets	O
compared	O
with	O
most	O
neural	O
net	O
algorithms	O
an	O
advantage	O
of	O
is	O
the	O
possibility	O
to	O
determine	O
the	O
number	O
and	O
initial	O
positions	O
of	O
the	O
discriminating	O
hyperplanes	O
to	O
neurons	B
a	O
priori	O
i	O
e	O
before	O
learning	O
starts	O
using	O
the	O
clustering	B
procedure	O
this	O
is	O
true	O
even	O
in	O
the	O
case	O
that	O
a	O
class	B
has	O
several	O
distinct	O
subclasses	O
there	O
are	O
many	O
relations	O
and	O
similarities	O
between	O
statistical	B
and	O
neural	O
net	O
algorithms	O
but	O
a	O
systematic	O
study	O
of	O
these	O
relations	O
is	O
still	O
lacking	O
another	O
distinguishing	O
feature	O
of	O
is	O
the	O
introduction	O
of	O
boolean	O
variables	O
of	O
the	O
normals	O
of	O
the	O
discriminating	O
hyperplanes	O
for	O
the	O
description	O
of	O
class	B
regions	O
on	O
a	O
symbolic	O
level	O
and	O
using	O
them	O
in	O
the	O
decision	O
procedure	O
this	O
way	O
additional	O
layers	O
of	O
hidden	B
units	O
can	O
be	O
avoided	O
has	O
some	O
similarity	O
with	O
the	O
madaline-system	O
which	O
is	O
also	O
a	O
piecewise	O
linear	O
classification	B
procedure	O
but	O
instead	O
of	O
applying	O
a	O
majority	O
function	O
for	O
class	B
decision	O
on	O
the	O
symbolic	O
level	O
in	O
the	O
case	O
of	O
madaline	B
uses	O
more	O
general	O
boolean	O
descriptions	O
of	O
class	B
and	O
subclass	O
segments	O
respectively	O
this	O
extends	O
the	O
variety	O
of	O
classification	B
problems	O
which	O
can	O
be	O
handled	O
considerably	O
pairwise	O
linear	B
regression	I
then	O
then	O
for	O
for	O
is	O
correctly	O
classified	O
if	O
follows	O
if	O
if	O
suppose	O
thaty	O
d	O
e	O
g	O
then	O
linear	B
regression	I
is	O
used	O
is	O
the	O
set	O
of	O
data	O
c	O
and	O
by	O
defining	O
the	O
dependent	O
variable	O
b	O
as	O
to	O
discriminate	O
between	O
two	O
classes	B
with	O
c	O
let	O
be	O
the	O
linear	B
regression	I
function	O
c	O
then	O
a	O
pattern	O
g	O
c	O
g	O
misclassifications	O
are	O
summed	O
up	O
suppose	O
that	O
learning	O
procedure	O
the	O
following	O
criterion	O
function	O
is	O
defined	O
for	O
all	O
misclassified	O
patterns	O
the	O
squared	O
distances	O
from	O
the	O
corresponding	O
decision	O
hyperplane	O
multiplied	O
by	O
the	O
costs	B
for	O
these	O
defines	O
the	O
decision	O
hyperplane	O
for	O
each	O
pair	O
of	O
classes	B
a	O
discriminating	O
regression	O
function	O
can	O
be	O
calculated	O
d	O
e	O
e	O
f	O
f	O
g	O
i	O
i	O
e	O
e	O
i	O
sec	O
algorithm	O
for	O
each	O
decision	O
surface	O
successively	O
this	O
means	O
that	O
costs	B
are	O
included	O
explicitly	O
in	O
the	O
learning	O
procedure	O
which	O
consists	O
of	O
by	O
a	O
gradient	B
descent	I
i	O
e	O
and	O
c	O
and	O
respectively	O
then	O
let	O
be	O
the	O
set	O
of	O
all	O
misclassified	O
between	O
the	O
classes	B
i	O
e	O
k	O
and	O
c	O
be	O
the	O
set	O
of	O
all	O
misclassified	O
patterns	O
g	O
patterns	O
of	O
class	B
let	O
g	O
g	O
be	O
the	O
costs	B
of	O
the	O
misclassification	O
and	O
letmdc	O
of	O
class	B
we	O
then	O
minimise	O
c	O
c	O
into	O
the	O
class	B
of	O
the	O
class	B
feg	O
exmdc	O
mdc	O
minimizing	O
the	O
criterion	O
function	O
with	O
respect	O
toi	O
e	O
d	O
has	O
been	O
partitioned	O
into	O
cb	O
vectors	O
clustering	B
of	O
classes	B
to	O
handle	O
also	O
problems	O
with	O
non	O
convex	O
non	O
simply	O
connected	O
class	B
regions	O
one	O
can	O
apply	O
a	O
clustering	B
procedure	O
before	O
the	O
linear	B
regression	I
is	O
carried	O
out	O
for	O
solving	O
the	O
clustering	B
problem	O
a	O
minimum	O
squared	O
error	O
algorithm	O
is	O
used	O
suppose	O
that	O
a	O
class	B
g	O
withd	O
elements	O
and	O
mean	O
clusters	O
d	O
d	O
then	O
the	O
criterion	O
function	O
given	O
by	O
the	O
criterion	O
function	O
is	O
calculated	O
patterns	O
are	O
moved	O
from	O
one	O
cluster	O
to	O
another	O
if	O
such	O
a	O
move	O
will	O
improve	O
the	O
mean	O
vectors	O
and	O
the	O
criterion	O
function	O
are	O
updated	O
after	O
each	O
pattern	O
move	O
like	O
hill	O
climbing	O
algorithms	O
in	O
general	O
these	O
approaches	O
guarantee	O
local	O
but	O
not	O
global	O
optimisation	B
different	O
initial	O
partitions	O
and	O
sequences	O
of	O
the	O
training	O
patterns	O
can	O
lead	O
to	O
different	O
solutions	O
in	O
the	O
case	O
of	O
clustering	B
the	O
number	O
of	O
two	O
class	B
problems	O
increases	O
correspondingly	O
we	O
note	O
that	O
by	O
the	O
combination	O
of	O
the	O
clustering	B
algorithm	O
with	O
the	O
regression	O
technique	O
the	O
number	O
and	O
initial	O
positions	O
of	O
discriminating	O
hyperplanes	O
are	O
fixed	O
a	O
priori	O
before	O
learning	O
in	O
a	O
reasonable	O
manner	O
even	O
in	O
the	O
case	O
that	O
some	O
classes	B
have	O
multimodal	O
distributions	O
consist	O
of	O
several	O
subclasses	O
thus	O
a	O
well	O
known	O
bottleneck	O
of	O
artificial	O
neural	O
nets	O
can	O
at	O
least	O
be	O
partly	O
avoided	O
description	O
of	O
the	O
classification	B
procedure	O
tained	O
in	O
the	O
training	B
set	I
or	O
not	O
can	O
be	O
classified	O
i	O
e	O
the	O
class	B
predicted	O
for	O
the	O
pairwise	O
e	O
d	O
g	O
the	O
discriminating	O
hyperplanes	O
were	O
calculated	O
then	O
any	O
pattern	O
c	O
hyperplanes	O
the	O
following	O
dimensional	O
fdgih	O
are	O
calculated	O
the	O
discrimination	B
of	O
the	O
classes	B
vector	O
if	O
the	O
function	O
case	O
of	O
clustering	B
the	O
number	O
is	O
changed	O
into	O
e	O
k	O
then	O
the	O
i-th	O
component	O
and	O
discriminates	O
the	O
classes	B
is	O
formed	O
for	O
each	O
class	B
is	O
equal	O
to	O
if	O
and	O
is	O
is	O
equal	O
to	O
if	O
is	O
defined	O
for	O
each	O
pattern	O
equal	O
to	O
in	O
all	O
other	O
cases	O
on	O
the	O
basis	O
of	O
the	O
discriminant	O
functions	O
a	O
vector	O
function	O
m	O
g	O
f	O
g	O
g	O
g	O
i	O
i	O
f	O
f	O
d	O
c	O
x	O
c	O
c	O
x	O
c	O
i	O
with	O
the	O
function	O
neural	B
networks	I
gig	O
c	O
is	O
the	O
set	O
of	O
integers	O
is	O
defined	O
by	O
c	O
for	O
each	O
class	B
c	O
c	O
is	O
uniquely	O
classified	O
by	O
the	O
discriminating	O
hyperplanes	O
e	O
d	O
a	O
pattern	O
cb	O
into	O
the	O
class	B
c	O
f	O
hyperplanes	O
which	O
discriminate	O
the	O
class	B
i	O
e	O
with	O
respect	O
to	O
the	O
g	O
have	O
the	O
same	O
sign	O
for	O
all	O
for	O
all	O
other	O
classes	B
r	O
f	O
classes	B
the	O
pattern	O
is	O
placed	O
in	O
the	O
halfspace	O
belonging	O
to	O
class	B
and	O
c	O
and	O
is	O
valid	O
because	O
at	O
least	O
with	O
respect	O
to	O
the	O
hyperplane	O
which	O
discriminates	O
class	B
g	O
have	O
not	O
the	O
class	B
the	O
pattern	O
is	O
placed	O
in	O
the	O
halfspace	O
of	O
class	B
c	O
a	O
pattern	O
gj	O
max	O
c	O
max	O
g	O
if	O
there	O
is	O
only	O
one	O
in	O
this	O
case	O
all	O
classes	B
were	O
determined	O
with	O
c	O
c	O
such	O
class	B
then	O
will	O
be	O
assigned	O
to	O
this	O
class	B
if	O
there	O
are	O
several	O
classes	B
letv	O
e	O
d	O
i	O
e	O
for	O
each	O
class	B
all	O
hyperplanes	O
set	O
of	O
the	O
classes	B
with	O
this	O
propertyv	O
x	O
for	O
each	O
class	B
are	O
selected	O
for	O
which	O
discriminating	O
the	O
class	B
against	O
all	O
other	O
classes	B
are	O
found	O
those	O
of	O
the	O
hyperplanes	O
of	O
hyperplanes	O
is	O
misclassified	O
i	O
e	O
for	O
each	O
class	B
a	O
set	O
to	O
all	O
these	O
hyperplanes	O
x	O
are	O
calculated	O
d	O
e	O
is	O
determined	O
for	O
which	O
of	O
class	B
the	O
euclidian	O
distance	B
of	O
is	O
not	O
in	O
the	O
halfspace	O
is	O
assigned	O
to	O
that	O
class	B
for	O
which	O
the	O
minimum	O
is	O
not	O
uniquely	O
classified	O
if	O
min	O
min	O
is	O
reached	O
from	O
the	O
other	O
from	O
c	O
same	O
sign	O
if	O
be	O
the	O
i	O
f	O
f	O
i	O
g	O
g	O
i	O
g	O
f	O
g	O
g	O
f	O
d	O
f	O
f	O
g	O
x	O
x	O
x	O
x	O
x	O
g	O
methods	O
for	O
comparison	O
r	O
j	O
henery	O
university	O
of	O
strathclyde	O
estimation	O
of	O
error	O
rates	O
in	O
classification	B
rules	O
in	O
testing	O
the	O
accuracy	B
of	O
a	O
classification	B
rule	I
it	O
is	O
widely	O
known	O
that	O
error	O
rates	O
tend	O
to	O
be	O
biased	O
if	O
they	O
are	O
estimated	O
from	O
the	O
same	O
set	O
of	O
data	O
as	O
that	O
used	O
to	O
construct	O
the	O
rules	O
at	O
one	O
extreme	O
if	O
a	O
decision	O
tree	O
for	O
example	B
is	O
allowed	O
to	O
grow	O
without	O
limit	O
to	O
the	O
number	O
of	O
leaves	O
in	O
the	O
tree	O
it	O
is	O
possible	O
to	O
classify	O
the	O
given	O
data	O
with	O
accuracy	B
in	O
general	O
at	O
the	O
expense	O
of	O
creating	O
a	O
very	O
complex	B
tree-structure	O
in	O
practice	O
complex	B
structures	O
do	O
not	O
always	O
perform	O
well	O
when	O
tested	O
on	O
unseen	O
data	O
and	O
this	O
is	O
one	O
case	O
of	O
the	O
general	O
phenomenon	O
of	O
over-fitting	B
data	O
of	O
course	O
overfitting	B
is	O
of	O
most	O
concern	O
with	O
noisy	B
data	I
i	O
e	O
data	O
in	O
which	O
correct	O
classification	B
is	O
impossible	O
in	O
principle	O
as	O
there	O
are	O
conflicting	O
examples	O
however	O
the	O
problem	O
also	O
arises	O
with	O
noise-free	O
datasets	O
where	O
in	O
principle	O
correct	O
classification	B
is	O
possible	O
among	O
the	O
statlog	B
datasets	O
for	O
example	B
there	O
is	O
one	O
dataset	O
that	O
is	O
probably	O
noise	B
free	O
and	O
it	O
is	O
possible	O
to	O
classify	O
the	O
given	O
data	O
correctly	O
however	O
certain	O
classes	B
are	O
represented	O
so	O
infrequently	O
that	O
we	O
cannot	O
be	O
sure	O
what	O
the	O
true	O
classification	B
procedure	O
should	O
be	O
as	O
a	O
general	O
rule	O
we	O
expect	O
that	O
very	O
simple	O
structures	O
should	O
be	O
used	O
for	O
noisy	B
data	I
and	O
very	O
complex	B
structures	O
only	O
for	O
data	O
that	O
are	O
noise-free	O
what	O
is	O
clear	O
is	O
that	O
we	O
should	O
adjust	O
the	O
complexity	O
to	O
suit	O
the	O
problem	O
at	O
hand	O
otherwise	O
the	O
procedure	O
will	O
be	O
biased	O
for	O
example	B
most	O
decision	O
tree	O
procedures	O
as	O
cart	B
by	O
breiman	O
et	O
al	O
restrict	O
the	O
size	O
of	O
the	O
decision	O
tree	O
by	O
pruning	B
i	O
e	O
by	O
cutting	O
out	O
some	O
branches	O
if	O
they	O
do	O
not	O
lead	O
to	O
useful	O
dichotomies	O
of	O
the	O
data	O
even	O
if	O
some	O
measure	B
of	O
pruning	B
is	O
added	O
to	O
avoid	O
over-fitting	B
the	O
data	O
the	O
apparent	O
error-rate	O
estimated	O
by	O
applying	O
the	O
induced	O
rule	O
on	O
the	O
original	O
data	O
is	O
usually	O
over-optimistic	O
one	O
way	O
of	O
correcting	O
for	O
this	O
bias	B
is	O
to	O
use	O
two	O
independent	O
samples	O
of	O
data	O
one	O
to	O
learn	O
the	O
rule	O
and	O
another	O
to	O
test	O
it	O
a	O
method	O
that	O
is	O
more	O
suitable	O
for	O
intermediate	O
sample	O
sizes	O
order	O
is	O
cross-validation	O
which	O
first	O
came	O
to	O
prominence	O
when	O
lachenbruch	O
mickey	O
suggested	O
the	O
leave-one-out	B
procedure	O
a	O
closely	O
related	O
method	O
which	O
is	O
used	O
for	O
small	O
sample	O
sizes	O
is	O
the	O
bootstrap	B
m	O
address	O
for	O
correspondence	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
university	O
of	O
strathclyde	O
glasgow	O
u	O
k	O
methods	O
for	O
comparison	O
procedure	O
of	O
efron	O
these	O
three	O
methods	O
of	O
estimating	O
error	O
rates	O
are	O
now	O
described	O
briefly	O
train-and-test	B
the	O
essential	O
idea	O
is	O
this	O
a	O
sample	O
of	O
data	O
training	O
data	O
is	O
given	O
to	O
enable	O
a	O
classification	B
rule	I
to	O
be	O
set	O
up	O
what	O
we	O
would	O
like	O
to	O
know	O
is	O
the	O
proportion	O
of	O
errors	O
made	O
by	O
this	O
rule	O
when	O
it	O
is	O
up-and-running	O
and	O
classifying	O
new	O
observations	O
without	O
the	O
benefit	O
of	O
knowing	O
the	O
true	O
classifications	O
to	O
do	O
this	O
we	O
test	O
the	O
rule	O
on	O
a	O
second	O
independent	O
sample	O
of	O
new	O
observations	O
test	O
data	O
whose	O
true	O
classifications	O
are	O
known	O
but	O
are	O
not	O
told	O
to	O
the	O
classifier	B
the	O
predicted	O
and	O
true	O
classifications	O
on	O
the	O
test	O
data	O
give	O
an	O
unbiased	O
estimate	O
of	O
the	O
error	B
rate	I
of	O
the	O
classifier	B
to	O
enable	O
this	O
procedure	O
to	O
be	O
carried	O
out	O
from	O
a	O
given	O
set	O
of	O
data	O
a	O
proportion	O
of	O
the	O
data	O
is	O
selected	O
at	O
random	O
about	O
and	O
used	O
as	O
the	O
test	O
data	O
the	O
classifier	B
is	O
trained	O
on	O
the	O
remaining	O
data	O
and	O
then	O
tested	O
on	O
the	O
test	O
data	O
there	O
is	O
a	O
slight	O
loss	O
of	O
efficiency	O
here	O
as	O
we	O
do	O
not	O
use	O
the	O
full	O
sample	O
to	O
train	O
the	O
decision	O
rule	O
but	O
with	O
very	O
large	O
datasets	O
this	O
is	O
not	O
a	O
major	O
problem	O
we	O
adopted	O
this	O
procedure	O
when	O
the	O
number	O
of	O
examples	O
was	O
much	O
larger	O
than	O
allowed	O
the	O
use	O
of	O
a	O
test	O
sample	O
of	O
size	O
or	O
so	O
we	O
often	O
refer	O
to	O
this	O
method	O
as	O
one-shot	O
train-and-test	B
cross-validation	O
for	O
moderate-sized	O
samples	O
the	O
procedure	O
we	O
adopted	O
was	O
cross-validation	O
in	O
its	O
most	O
subsamples	O
each	O
fdg	O
subsamples	O
in	O
this	O
way	O
the	O
error	B
rate	I
is	O
estimated	O
efficiently	O
and	O
in	O
an	O
unbiased	O
way	O
the	O
rule	O
finally	O
used	O
is	O
calculated	O
from	O
all	O
the	O
data	O
the	O
leave-one-out	B
method	O
of	O
lachenbruch	O
mickey	O
equal	O
to	O
the	O
number	O
of	O
examples	O
stone	O
describes	O
cross-validation	O
methods	O
for	O
giving	O
unbiased	O
estimates	O
of	O
the	O
error	B
rate	I
a	O
practical	O
difficulty	O
with	O
the	O
use	O
of	O
cross-validation	O
in	O
computer-intensive	O
methods	O
repetition	O
of	O
the	O
learning	O
cycle	O
which	O
may	O
require	O
elementary	O
form	O
cross-validation	O
consists	O
of	O
dividing	O
the	O
data	O
into	O
sub-sample	O
is	O
predicted	O
via	O
the	O
classification	B
rule	I
constructed	O
from	O
the	O
remainingc	O
subsamples	O
and	O
the	O
estimated	O
error	B
rate	I
is	O
the	O
average	O
error	B
rate	I
from	O
these	O
is	O
of	O
course	O
such	O
as	O
neural	B
networks	I
is	O
the	O
cross-validation	O
with	O
much	O
computational	O
effort	O
bootstrap	B
the	O
more	O
serious	O
objection	O
to	O
cross-validation	O
is	O
that	O
the	O
error	O
estimates	O
it	O
produces	O
are	O
too	O
scattered	O
so	O
that	O
the	O
confidence	O
intervals	O
for	O
the	O
true	O
error-rate	O
are	O
too	O
wide	O
the	O
bootstrap	B
procedure	O
gives	O
much	O
narrower	O
confidence	O
limits	O
but	O
the	O
penalty	O
paid	O
is	O
that	O
the	O
estimated	O
error-rates	O
are	O
optimistic	O
are	O
biased	O
downwards	O
the	O
trade-off	O
between	O
bias	B
and	O
random	O
error	O
means	O
that	O
as	O
a	O
general	O
rule	O
the	O
bootstrap	B
method	O
is	O
preferred	O
when	O
the	O
sample	O
size	O
is	O
small	O
and	O
cross-validation	O
when	O
the	O
sample	O
size	O
is	O
large	O
in	O
conducting	O
a	O
comparative	O
trial	O
between	O
methods	O
on	O
the	O
same	O
dataset	O
the	O
amount	O
of	O
bias	B
is	O
not	O
so	O
important	O
so	O
long	O
as	O
the	O
bias	B
is	O
the	O
same	O
for	O
all	O
methods	O
since	O
the	O
bootstrap	B
represents	O
the	O
best	O
way	O
to	O
reduce	O
variability	O
the	O
most	O
effective	O
way	O
to	O
conduct	O
comparisons	O
in	O
small	O
datasets	O
is	O
to	O
use	O
the	O
bootstrap	B
since	O
it	O
is	O
not	O
so	O
widely	O
used	O
in	O
classification	B
trials	O
as	O
perhaps	O
it	O
should	O
be	O
we	O
give	O
an	O
extended	O
description	O
here	O
although	O
it	O
must	O
be	O
admitted	O
sec	O
estimation	O
of	O
error	O
rates	O
in	O
classification	B
rules	O
that	O
we	O
did	O
not	O
use	O
the	O
bootstrap	B
in	O
any	O
of	O
our	O
trials	O
as	O
we	O
judged	O
that	O
our	O
samples	O
were	O
large	O
enough	O
to	O
use	O
either	O
cross-validation	O
or	O
train-and-test	B
in	O
statistical	B
terms	O
the	O
bootstrap	B
is	O
a	O
non-parametric	O
procedure	O
for	O
estimating	O
parameters	O
generally	O
and	O
error-rates	O
in	O
particular	O
the	O
basic	O
idea	O
is	O
to	O
re-use	O
the	O
original	O
dataset	O
sizek	O
to	O
obtain	O
new	O
datasets	O
also	O
of	O
sizek	O
by	O
re-sampling	O
with	O
replacement	O
see	O
efron	O
for	O
the	O
definitive	O
introduction	O
to	O
the	O
subject	O
and	O
crawford	O
for	O
an	O
application	O
to	O
cart	B
breiman	O
et	O
al	O
note	O
that	O
there	O
are	O
practical	O
difficulties	O
in	O
applying	O
the	O
bootstrap	B
to	O
decision	B
trees	I
in	O
the	O
context	O
of	O
classification	B
the	O
bootstrap	B
idea	O
is	O
to	O
replicate	O
the	O
whole	O
classification	B
experiment	O
a	O
large	O
number	O
of	O
times	O
and	O
to	O
estimate	O
quantities	O
like	O
bias	B
from	O
these	O
replicate	O
of	O
bootstrap	B
replicate	O
samples	O
are	O
created	O
each	O
sample	O
being	O
a	O
replicate	O
is	O
taken	O
from	O
the	O
original	O
sample	O
by	O
sampling	O
with	O
replacement	O
sampling	O
with	O
replacement	O
means	O
for	O
of	O
data	O
will	O
not	O
appear	O
in	O
the	O
bootstrap	B
sample	O
also	O
some	O
data	O
points	O
will	O
appear	O
more	O
than	O
once	O
in	O
the	O
bootstrap	B
sample	O
each	O
bootstrap	B
sample	O
is	O
used	O
to	O
construct	O
a	O
classification	B
rule	I
which	O
is	O
then	O
used	O
to	O
predict	O
the	O
classes	B
of	O
those	O
original	O
data	O
that	O
were	O
unused	O
in	O
the	O
of	O
the	O
original	O
data	O
will	O
be	O
used	O
as	O
test	B
set	I
this	O
gives	O
one	O
estimate	O
of	O
the	O
error	B
rate	I
for	O
each	O
bootstrap	B
sample	O
the	O
average	O
error	O
rates	O
over	O
all	O
bootstrap	B
samples	O
are	O
then	O
combined	O
to	O
give	O
an	O
estimated	O
error	B
rate	I
for	O
the	O
original	O
rule	O
see	O
efron	O
and	O
crawford	O
for	O
details	O
the	O
main	O
properties	O
of	O
the	O
bootstrap	B
have	O
been	O
summarised	O
by	O
as	O
follows	O
properties	O
of	O
cross-validation	O
and	O
bootstrap	B
efron	O
gives	O
the	O
following	O
properties	O
of	O
the	O
bootstrap	B
as	O
an	O
estimator	O
of	O
error-rate	O
by	O
experiments	O
thus	O
to	O
estimate	O
the	O
error	B
rate	I
in	O
small	O
samples	O
sizek	O
say	O
a	O
large	O
number	O
chosen	O
of	O
the	O
original	O
sample	O
that	O
is	O
a	O
random	O
sample	O
of	O
sizek	O
example	B
that	O
some	O
data	O
points	O
will	O
be	O
omitted	O
average	O
aboutfdh	O
training	B
set	I
aboutfdh	O
taking	O
very	O
large	O
recommends	O
approximately	O
the	O
statistical	B
variability	O
in	O
the	O
average	O
error	B
rate	I
efron	O
is	O
small	O
and	O
for	O
small	O
sample	O
sizek	O
will	O
have	O
very	O
much	O
smaller	O
statistical	B
variability	O
than	O
the	O
cross-validation	O
estimate	O
this	O
means	O
that	O
the	O
bootstrap	B
the	O
bootstrap	B
and	O
cross-validation	O
estimates	O
are	O
generally	O
close	O
for	O
large	O
sample	O
sizes	O
and	O
the	O
ratio	O
between	O
the	O
two	O
estimates	O
approaches	O
unity	O
as	O
the	O
sample	O
size	O
tends	O
to	O
infinity	O
the	O
bootstrap	B
and	O
cross-validation	O
methods	O
tend	O
to	O
be	O
closer	O
for	O
smoother	O
costfunctions	O
than	O
the	O
loss-function	O
implicit	O
in	O
the	O
error	O
rates	O
discussed	O
above	O
however	O
the	O
bootstrap	B
may	O
be	O
biased	O
even	O
for	O
large	O
samples	O
the	O
effective	O
sample	O
size	O
is	O
determined	O
by	O
the	O
number	O
in	O
the	O
smallest	O
classification	B
group	O
efron	O
quotes	O
a	O
medical	O
example	B
with	O
n	O
cases	O
but	O
primary	O
interest	O
centres	O
on	O
the	O
patients	O
that	O
died	O
the	O
effective	O
sample	O
size	O
here	O
is	O
for	O
large	O
samples	O
group-wise	O
cross-validation	O
may	O
give	O
better	O
results	O
than	O
the	O
leave	O
one-out	O
method	O
although	O
this	O
conclusion	O
seems	O
doubtful	O
optimisation	B
of	O
parameters	O
frequently	O
it	O
is	O
desirable	O
to	O
tune	O
some	O
parameter	O
to	O
get	O
the	O
best	O
performance	O
from	O
an	O
algorithm	O
examples	O
might	O
be	O
the	O
amount	O
of	O
pruning	B
in	O
a	O
decision	O
tree	O
or	O
the	O
number	O
of	O
hidden	B
nodes	I
in	O
the	O
multilayer	O
perceptron	B
when	O
the	O
objective	O
is	O
to	O
minimise	O
the	O
error-rate	O
of	O
the	O
tree	O
or	O
perceptron	B
the	O
training	O
data	O
might	O
be	O
divided	O
into	O
two	O
parts	O
one	O
to	O
build	O
the	O
tree	O
or	O
perceptron	B
and	O
the	O
other	O
to	O
measure	B
the	O
error	B
rate	I
a	O
plot	O
of	O
error-rate	O
against	O
the	O
methods	O
for	O
comparison	O
parameter	O
will	O
indicate	O
what	O
the	O
best	O
choice	O
of	O
parameter	O
should	O
be	O
however	O
the	O
error	B
rate	I
corresponding	O
to	O
this	O
choice	O
of	O
parameter	O
is	O
a	O
biased	O
estimate	O
of	O
the	O
error	B
rate	I
of	O
the	O
classification	B
rule	I
when	O
tested	O
on	O
unseen	O
data	O
when	O
it	O
is	O
necessary	O
to	O
optimise	O
a	O
parameter	O
in	O
this	O
way	O
we	O
recommend	O
a	O
three-stage	O
process	O
for	O
very	O
large	O
datasets	O
hold	O
back	O
as	O
a	O
test	O
sample	O
of	O
the	O
remainder	O
divide	O
into	O
two	O
with	O
one	O
set	O
used	O
for	O
building	O
the	O
rule	O
and	O
the	O
other	O
for	O
choosing	O
the	O
parameter	O
use	O
the	O
chosen	O
parameter	O
to	O
build	O
a	O
rule	O
for	O
the	O
complete	O
training	O
sample	O
of	O
the	O
original	O
data	O
and	O
test	O
this	O
rule	O
on	O
the	O
test	O
sample	O
thus	O
for	O
example	B
watkins	O
gives	O
a	O
description	O
of	O
cross-validation	O
in	O
the	O
context	O
of	O
testing	O
decision-tree	O
classification	B
algorithms	O
and	O
uses	O
cross-validation	O
as	O
a	O
means	O
of	O
selecting	O
better	O
decision	B
trees	I
similarly	O
in	O
this	O
book	O
cross-validation	O
was	O
used	O
by	O
backprop	B
in	O
finding	O
the	O
optimal	O
number	O
of	O
nodes	O
in	O
the	O
hidden	B
layer	O
following	O
the	O
procedure	O
outlined	O
above	O
this	O
was	O
done	O
also	O
for	O
the	O
trials	O
involving	O
cascade	B
however	O
cross-validation	O
runs	O
involve	O
a	O
greatly	O
increased	O
amount	O
of	O
computational	O
labour	O
increasing	O
the	O
learning	O
time	B
fold	O
and	O
this	O
problem	O
is	O
particularly	O
serious	O
for	O
neural	B
networks	I
in	O
statlog	B
most	O
procedures	O
had	O
a	O
tuning	O
parameter	O
that	O
can	O
be	O
set	O
to	O
a	O
default	B
value	O
and	O
where	O
this	O
was	O
possible	O
the	O
default	B
parameters	O
were	O
used	O
this	O
was	O
the	O
case	O
for	O
example	B
with	O
the	O
decision	B
trees	I
generally	O
no	O
attempt	O
was	O
made	O
to	O
find	O
the	O
optimal	O
amount	O
of	O
pruning	B
and	O
accuracy	B
and	O
mental	B
fit	I
chapter	O
is	O
thereby	O
sacrificed	O
for	O
the	O
sake	O
of	O
speed	B
in	O
the	O
learning	O
process	O
organisation	O
of	O
comparative	B
trials	I
we	O
describe	O
in	O
this	O
section	O
what	O
we	O
consider	O
to	O
be	O
the	O
ideal	O
setup	O
for	O
comparing	O
classification	B
procedures	O
it	O
not	O
easy	O
to	O
compare	O
very	O
different	O
algorithms	O
on	O
a	O
large	O
number	O
of	O
datasets	O
and	O
in	O
practice	O
some	O
compromises	O
have	O
to	O
be	O
made	O
we	O
will	O
not	O
detail	O
the	O
compromises	O
that	O
we	O
made	O
in	O
our	O
own	O
trials	O
but	O
attempt	O
to	O
set	O
out	O
the	O
ideals	O
that	O
we	O
tried	O
to	O
follow	O
and	O
give	O
a	O
brief	O
description	O
of	O
the	O
unix-based	O
procedures	O
that	O
we	O
adopted	O
if	O
a	O
potential	O
trialist	O
wishes	O
to	O
perform	O
another	O
set	O
of	O
trials	O
is	O
able	O
to	O
cast	O
the	O
relevant	O
algorithms	O
into	O
the	O
form	O
that	O
we	O
detail	O
here	O
and	O
moreover	O
is	O
able	O
to	O
work	O
within	O
a	O
unix	O
environment	O
then	O
we	O
can	O
recommend	O
that	O
he	O
uses	O
our	O
test	O
procedures	O
this	O
will	O
guarantee	O
comparability	O
with	O
the	O
majority	O
of	O
our	O
own	O
results	O
in	O
the	O
following	O
list	O
of	O
desiderata	O
we	O
use	O
the	O
notation	O
to	O
denote	O
arbitrary	O
files	O
that	O
either	O
provide	O
data	O
or	O
receive	O
output	B
from	O
the	O
system	O
throughout	O
we	O
assume	O
that	O
files	O
used	O
for	O
trainingtesting	O
are	O
representative	O
of	O
the	O
population	O
and	O
are	O
statistically	O
similar	O
to	O
each	O
other	O
training	O
phase	O
the	O
most	O
elementary	O
functionality	B
required	O
of	O
any	O
learning	O
algorithm	O
is	O
to	O
be	O
able	O
to	O
take	O
data	O
from	O
one	O
file	O
assumption	O
contains	O
known	O
classes	B
and	O
create	O
the	O
rules	O
the	O
resulting	O
rules	O
parameters	O
defining	O
the	O
rule	O
may	O
be	O
saved	O
to	O
another	O
file	O
a	O
cost	B
matrix	I
say	O
can	O
be	O
read	O
in	O
and	O
used	O
in	O
building	O
the	O
rules	O
testing	O
phase	O
the	O
algorithm	O
can	O
read	O
in	O
the	O
rules	O
and	O
classify	O
unseen	O
data	O
in	O
the	O
following	O
sequence	O
sec	O
organisation	O
of	O
comparative	B
trials	I
read	O
in	O
the	O
rules	O
or	O
parameters	O
from	O
the	O
training	O
phase	O
passed	O
on	O
directly	O
from	O
the	O
training	O
phase	O
if	O
that	O
immediately	O
precedes	O
the	O
testing	O
phase	O
or	O
read	O
from	O
the	O
file	O
read	O
in	O
a	O
set	O
of	O
unseen	O
data	O
from	O
a	O
file	O
with	O
true	O
classifications	O
that	O
are	O
hidden	B
from	O
the	O
classifier	B
read	O
in	O
a	O
cost	B
matrix	I
from	O
a	O
file	O
and	O
use	O
this	O
cost	B
matrix	I
in	O
the	O
classification	B
procedure	O
output	B
the	O
classifications	O
to	O
a	O
file	O
if	O
true	O
classifications	O
were	O
provided	O
in	O
the	O
test	O
file	O
output	B
to	O
file	O
a	O
confusion	O
matrix	O
whose	O
rows	O
represent	O
the	O
true	O
classifications	O
and	O
whose	O
columns	O
represent	O
the	O
classifications	O
made	O
by	O
the	O
algorithm	O
the	O
two	O
steps	O
above	O
constitute	O
the	O
most	O
basic	O
element	O
of	O
a	O
comparative	O
trial	O
and	O
we	O
describe	O
this	O
basic	O
element	O
as	O
a	O
simple	O
train-and-test	B
procedure	O
all	O
algorithms	O
used	O
in	O
our	O
trials	O
were	O
able	O
to	O
perform	O
the	O
train-and-test	B
procedure	O
cross-validation	O
to	O
follow	O
the	O
cross-validation	O
procedure	O
it	O
is	O
necessary	O
to	O
build	O
an	O
outer	O
loop	O
of	O
control	O
procedures	O
that	O
divide	O
up	O
the	O
original	O
file	O
into	O
its	O
component	O
parts	O
and	O
successively	O
use	O
each	O
part	O
as	O
test	O
file	O
and	O
the	O
remaining	O
part	O
as	O
training	O
file	O
of	O
course	O
the	O
cross-validation	O
procedure	O
results	O
in	O
a	O
succession	O
of	O
mini-confusion	O
matrices	O
and	O
these	O
must	O
be	O
combined	O
to	O
give	O
the	O
overall	O
confusion	O
matrix	O
all	O
this	O
can	O
be	O
done	O
within	O
the	O
evaluation	B
assistant	B
shell	O
provided	O
the	O
classification	B
procedure	O
is	O
capable	O
of	O
the	O
simple	O
train-and-test	B
steps	O
above	O
some	O
more	O
sophisticated	O
algorithms	O
may	O
have	O
a	O
cross-validation	O
procedure	O
built	O
in	O
of	O
course	O
and	O
if	O
so	O
this	O
is	O
a	O
distinct	O
advantage	O
bootstrap	B
the	O
use	O
of	O
the	O
bootstrap	B
procedure	O
makes	O
it	O
imperative	O
that	O
combining	O
of	O
results	O
files	O
etc	O
is	O
done	O
automatically	O
once	O
again	O
if	O
an	O
algorithm	O
is	O
capable	O
of	O
simple	O
train-and-test	B
it	O
can	O
be	O
embedded	O
in	O
a	O
bootstrap	B
loop	O
using	O
evaluation	B
assistant	B
perhaps	O
we	O
should	O
admit	O
that	O
we	O
never	O
used	O
the	O
bootstrap	B
in	O
any	O
of	O
the	O
datasets	O
reported	O
in	O
this	O
book	O
evaluation	B
assistant	B
evaluation	B
assistant	B
is	O
a	O
tool	O
that	O
facilitates	O
the	O
testing	O
of	O
learning	O
algorithms	O
on	O
given	O
datasets	O
and	O
provides	O
standardised	O
performance	B
measures	B
in	O
particular	O
it	O
standardises	O
timings	O
of	O
the	O
various	O
phases	O
such	O
as	O
training	O
and	O
testing	O
it	O
also	O
provides	O
statistics	O
describing	O
the	O
trial	O
error	O
rates	O
total	O
confusion	O
matrices	O
etc	O
etc	O
it	O
can	O
be	O
obtained	O
from	O
j	O
gama	O
of	O
the	O
university	O
of	O
porto	O
for	O
details	O
of	O
this	O
and	O
other	O
publicly	O
available	O
software	O
and	O
datasets	O
see	O
appendices	O
a	O
and	O
b	O
two	O
versions	O
of	O
evaluation	B
assistant	B
exist	O
command	O
version	O
interactive	O
version	O
the	O
command	O
version	O
of	O
evaluation	B
assistant	B
consists	O
of	O
a	O
set	O
of	O
basic	O
commands	O
that	O
enable	O
the	O
user	O
to	O
test	O
learning	O
algorithms	O
this	O
version	O
is	O
implemented	O
as	O
a	O
set	O
of	O
c-shell	O
scripts	O
and	O
c	O
programs	O
the	O
interactive	O
version	O
of	O
evaluation	B
assistant	B
provides	O
an	O
interactive	O
interface	O
that	O
enables	O
the	O
user	O
to	O
set	O
up	O
the	O
basic	O
parameters	O
for	O
testing	O
it	O
is	O
implemented	O
in	O
c	O
and	O
methods	O
for	O
comparison	O
the	O
interactive	O
interface	O
exploits	O
x	O
windows	O
this	O
version	O
generates	O
a	O
customised	O
version	O
of	O
some	O
eac	O
scripts	O
which	O
can	O
be	O
examined	O
and	O
modified	O
before	O
execution	O
both	O
versions	O
run	O
on	O
a	O
sun	O
sparcstation	O
and	O
other	O
compatible	O
workstations	O
characterisation	O
of	O
datasets	O
an	O
important	O
objective	O
is	O
to	O
investigate	O
why	O
certain	O
algorithms	O
do	O
well	O
on	O
some	O
datasets	O
and	O
not	O
so	O
well	O
on	O
others	O
this	O
section	O
describes	O
measures	B
of	O
datasets	O
which	O
may	O
help	O
to	O
explain	O
our	O
findings	O
these	O
measures	B
are	O
of	O
three	O
types	O
very	O
simple	O
measures	B
such	O
as	O
the	O
number	O
of	O
examples	O
statistically	O
based	O
such	O
as	O
the	O
skewness	B
of	O
the	O
attributes	B
and	O
information	O
theoretic	O
such	O
as	O
the	O
information	O
gain	O
of	O
attributes	B
we	O
discuss	O
information	O
theoretic	O
measures	B
in	O
section	O
there	O
is	O
a	O
need	O
for	O
a	O
measure	B
which	O
indicates	O
when	O
decision	B
trees	I
will	O
do	O
well	O
bearing	O
in	O
mind	O
the	O
success	O
of	O
decision	B
trees	I
in	O
image	B
segmentation	I
problems	O
it	O
seems	O
that	O
some	O
measure	B
of	O
multimodality	B
might	O
be	O
useful	O
in	O
this	O
connection	O
some	O
algorithms	O
have	O
built	O
in	O
measures	B
which	O
are	O
given	O
as	O
part	O
of	O
the	O
output	B
for	O
example	B
castle	B
measures	B
the	O
kullback-leibler	B
information	I
in	O
a	O
dataset	O
such	O
measures	B
are	O
useful	O
in	O
establishing	O
the	O
validity	O
of	O
specific	O
assumptions	O
underlying	O
the	O
algorithm	O
and	O
although	O
they	O
do	O
not	O
always	O
suggest	O
what	O
to	O
do	O
if	O
the	O
assumptions	O
do	O
not	O
hold	O
at	O
least	O
they	O
give	O
an	O
indication	O
of	O
internal	O
consistency	O
the	O
measures	B
should	O
continue	O
to	O
be	O
elaborated	O
and	O
refined	O
in	O
the	O
light	O
of	O
experience	O
simple	O
measures	B
the	O
following	O
descriptors	O
of	O
the	O
datasets	O
give	O
very	O
simple	O
measures	B
of	O
the	O
complexity	O
or	O
size	O
of	O
the	O
problem	O
of	O
course	O
these	O
measures	B
might	O
advantageously	O
be	O
combined	O
to	O
give	O
other	O
measures	B
more	O
appropriate	O
for	O
specific	O
tasks	O
for	O
example	B
by	O
taking	O
products	O
ratios	O
or	O
logarithms	O
numerical	O
continuous	O
or	O
ordered	O
attributes	B
statistical	B
measures	B
the	O
following	O
measures	B
are	O
designed	O
principally	O
to	O
explain	O
the	O
performance	O
of	O
statistical	B
algorithms	O
but	O
are	O
likely	O
to	O
be	O
more	O
generally	O
applicable	O
often	O
they	O
are	O
much	O
influenced	O
by	O
the	O
simple	O
measures	B
above	O
for	O
example	B
the	O
skewness	B
measure	B
often	O
reflects	O
the	O
the	O
total	O
number	O
of	O
attributes	B
in	O
the	O
data	O
as	O
used	O
in	O
the	O
trials	O
where	O
categorical	O
attributes	B
were	O
originally	O
present	O
these	O
were	O
converted	O
to	O
binary	O
indicator	B
variables	I
this	O
is	O
the	O
total	O
number	O
of	O
observations	O
in	O
the	O
whole	O
dataset	O
in	O
some	O
respects	O
it	O
might	O
seem	O
more	O
sensible	O
to	O
count	O
only	O
the	O
observations	O
in	O
the	O
training	O
data	O
but	O
this	O
is	O
generally	O
a	O
large	O
fraction	O
of	O
the	O
total	O
number	O
in	O
any	O
case	O
number	O
of	O
observations	O
number	O
of	O
attributes	B
number	O
of	O
classes	B
coded	O
as	O
indicator	B
variables	I
by	O
definition	O
the	O
remaining	O
n	O
bin	O
att	O
attributes	B
are	O
the	O
total	O
number	O
of	O
classes	B
represented	O
in	O
the	O
entire	O
dataset	O
number	O
of	O
binary	B
attributes	B
bin	O
att	O
the	O
total	O
number	O
of	O
number	O
of	O
attributes	B
that	O
are	O
binary	O
categorical	O
attributes	B
sec	O
characterisation	O
of	O
datasets	O
number	O
of	O
binary	B
attributes	B
and	O
if	O
this	O
is	O
so	O
the	O
skewness	B
and	O
kurtosis	B
are	O
directly	O
related	O
to	O
each	O
other	O
however	O
the	O
statistical	B
measures	B
in	O
this	O
section	O
are	O
generally	O
defined	O
only	O
for	O
continuous	O
attributes	B
although	O
it	O
is	O
possible	O
to	O
extend	O
their	O
definitions	O
to	O
include	O
discrete	O
and	O
even	O
categorical	O
attributes	B
the	O
most	O
natural	O
measures	B
for	O
such	O
data	O
are	O
the	O
information	O
theoretic	O
measures	B
discussed	O
in	O
section	O
test	O
statistic	O
for	O
homogeneity	O
of	O
covariances	O
the	O
covariance	B
matrices	O
are	O
fundamental	O
in	O
the	O
theory	O
of	O
linear	O
and	O
quadratic	O
discrimination	B
detailed	O
in	O
sections	O
and	O
and	O
the	O
key	O
in	O
understanding	O
when	O
to	O
apply	O
one	O
and	O
not	O
the	O
other	O
lies	O
in	O
the	O
homogeneity	O
or	O
otherwise	O
of	O
the	O
covariances	O
one	O
measure	B
of	O
the	O
lack	O
of	O
homogeneity	O
of	O
covariances	O
is	O
the	O
geometric	O
mean	O
ratio	O
of	O
standard	O
deviations	O
of	O
the	O
populations	O
of	O
individual	O
classes	B
to	O
the	O
standard	O
deviations	O
of	O
the	O
sample	O
and	O
is	O
given	O
by	O
th	O
sample	O
covariance	B
matrix	I
and	O
the	O
k	O
d	O
e	O
where	O
are	O
the	O
unbiased	O
estimators	O
of	O
the	O
i	O
expressed	O
as	O
the	O
geometric	O
mean	O
ratio	O
of	O
standard	O
deviations	O
of	O
the	O
individual	O
populations	O
to	O
the	O
pooled	O
standard	O
deviations	O
via	O
the	O
expression	O
have	O
a	O
common	O
covariance	B
structure	O
i	O
e	O
to	O
the	O
hypothesis	O
i	O
below	O
this	O
quantity	O
is	O
related	O
to	O
a	O
test	O
of	O
the	O
hypothesis	O
that	O
all	O
populations	O
b	O
which	O
can	O
be	O
tested	O
via	O
box	O
sv	O
feg	O
log	O
cbk	O
h	O
fdg	O
c	O
he	O
d	O
d	O
and	O
and	O
pooled	B
covariance	B
matrix	I
respectively	O
this	O
statistic	O
has	O
an	O
asymptotic	O
distribution	O
and	O
the	O
approximation	O
is	O
good	O
if	O
eachk	O
exceeds	O
and	O
if	O
and	O
are	O
both	O
much	O
smaller	O
than	O
everyk	O
in	O
datasets	O
reported	O
in	O
this	O
volume	O
these	O
criteria	O
are	O
not	O
always	O
met	O
but	O
thev	O
statistic	O
can	O
still	O
be	O
computed	O
and	O
used	O
as	O
a	O
characteristic	O
of	O
the	O
data	O
thev	O
statistic	O
can	O
be	O
reexp	O
b	O
i	O
the	O
b	O
i	O
in	O
every	O
dataset	O
that	O
we	O
looked	O
at	O
thev	O
statistic	O
is	O
significantly	O
different	O
from	O
zero	O
in	O
which	O
case	O
the	O
b	O
i	O
the	O
set	O
of	O
correlations	O
between	O
all	O
pairs	O
of	O
attributes	B
give	O
some	O
indication	O
of	O
the	O
as	O
follows	O
the	O
correlations	O
between	O
all	O
pairs	O
of	O
attributes	B
are	O
calculated	O
for	O
each	O
class	B
which	O
is	O
a	O
measure	B
of	O
interdependence	O
and	O
over	O
all	O
classes	B
giving	O
the	O
measure	B
corr	B
abs	I
is	O
strictly	O
greater	O
than	O
unity	O
if	O
the	O
covariances	O
differ	O
and	O
is	O
equal	O
to	O
unity	O
if	O
and	O
only	O
if	O
the	O
m-statistic	O
is	O
zero	O
i	O
e	O
all	O
individual	O
covariance	B
matrices	O
are	O
equal	O
to	O
the	O
pooled	B
covariance	B
matrix	I
interdependence	O
of	O
the	O
attributes	B
and	O
a	O
measure	B
of	O
that	O
interdependence	O
may	O
be	O
calculated	O
mean	O
absolute	O
correlation	B
coefficient	O
corr	B
abs	I
is	O
significantly	O
greater	O
than	O
unity	O
separately	O
the	O
absolute	O
values	O
of	O
these	O
correlations	O
are	O
averaged	O
over	O
all	O
pairs	O
of	O
attributes	B
between	O
attributes	B
test	O
statistic	O
fdg	O
ck	O
feg	O
v	O
f	O
f	O
c	O
f	O
k	O
f	O
f	O
v	O
methods	O
for	O
comparison	O
if	O
corr	B
abs	I
is	O
near	O
unity	O
there	O
is	O
much	O
redundant	O
information	O
in	O
the	O
attributes	B
and	O
some	O
procedures	O
such	O
as	O
logistic	O
discriminants	O
may	O
have	O
technical	B
problems	O
associated	O
with	O
this	O
also	O
castle	B
for	O
example	B
may	O
be	O
misled	O
substantially	O
by	O
fitting	O
relationships	O
to	O
the	O
attributes	B
instead	O
of	O
concentrating	O
on	O
getting	O
right	O
the	O
relationship	O
between	O
the	O
classes	B
and	O
the	O
attributes	B
canonical	O
discriminant	O
correlations	O
classes	B
form	O
some	O
kind	O
of	O
sequence	O
so	O
that	O
the	O
population	O
means	O
are	O
strung	O
out	O
along	O
some	O
assume	O
that	O
in	O
dimensional	O
space	O
the	O
sample	O
points	O
from	O
one	O
class	B
form	O
clusters	O
of	O
roughly	O
elliptical	O
shape	O
around	O
its	O
population	O
mean	O
in	O
general	O
if	O
there	O
are	O
classes	B
the	O
f	O
dimensional	O
subspace	O
on	O
the	O
other	O
hand	O
it	O
happens	O
frequently	O
that	O
the	O
means	O
lie	O
in	O
a	O
f	O
the	O
simplest	O
case	O
of	O
all	O
occurs	O
curve	O
that	O
lies	O
ind	O
dimensional	O
space	O
whered	O
f	O
and	O
the	O
population	O
means	O
lie	O
along	O
a	O
straight	O
line	O
canonical	B
discriminants	I
whend	O
are	O
a	O
way	O
of	O
systematically	O
projecting	O
the	O
mean	O
vectors	O
in	O
an	O
optimal	O
way	O
to	O
maximise	O
the	O
ratio	O
of	O
between-mean	O
distances	O
to	O
within-cluster	O
distances	O
successive	O
discriminants	O
being	O
orthogonal	O
to	O
earlier	O
discriminants	O
thus	O
the	O
first	O
canonical	O
discriminant	O
gives	O
the	O
best	O
single	O
linear	O
combination	B
of	I
attributes	B
that	O
discriminates	O
between	O
the	O
populations	O
the	O
second	O
canonical	O
discriminant	O
is	O
the	O
best	O
single	O
linear	O
combination	O
orthogonal	O
to	O
the	O
first	O
and	O
so	O
on	O
the	O
success	O
of	O
these	O
discriminants	O
is	O
measured	O
by	O
the	O
canonical	O
correlations	O
if	O
the	O
canonical	O
discriminant	O
matrix	O
divided	O
by	O
the	O
sum	O
of	O
all	O
the	O
eigenvalues	O
represents	O
the	O
this	O
gives	O
a	O
measure	B
of	I
collinearity	I
of	O
the	O
class	B
means	O
when	O
the	O
classes	B
form	O
an	O
ordered	O
sequence	O
for	O
example	B
soil	O
types	O
might	O
be	O
ordered	O
by	O
wetness	O
the	O
class	B
means	O
typically	O
proportion	O
of	O
total	O
variation	O
explained	O
by	O
first	O
k	O
canonical	B
discriminants	I
this	O
is	O
based	O
on	O
the	O
idea	O
of	O
describing	O
how	O
the	O
means	O
for	O
the	O
various	O
populations	O
differ	O
in	O
attribute	O
space	O
each	O
class	B
mean	O
defines	O
a	O
point	O
in	O
attribute	O
space	O
and	O
at	O
its	O
simplest	O
we	O
wish	O
to	O
know	O
if	O
there	O
is	O
some	O
simple	O
relationship	O
between	O
these	O
class	B
the	O
first	O
canonical	B
correlation	B
is	O
close	O
to	O
unity	O
the	O
means	O
lie	O
along	O
a	O
straight	O
line	O
nearly	O
f	O
th	O
canonical	B
correlation	B
is	O
near	O
zero	O
the	O
means	O
lie	O
ind	O
dimensional	O
space	O
if	O
thede	O
means	O
for	O
example	B
if	O
they	O
lie	O
along	O
a	O
straight	O
line	O
the	O
sum	O
of	O
the	O
firstd	O
eigenvalues	O
of	O
variation	O
here	O
is	O
proportion	O
of	O
total	O
variation	O
explained	O
by	O
the	O
firstd	O
canonical	B
discriminants	I
the	O
total	O
g	O
we	O
calculate	O
as	O
fractk	B
the	O
values	O
of	O
e	O
e	O
d	O
g	O
h	O
lie	O
along	O
a	O
curve	O
in	O
low	O
dimensional	O
space	O
the	O
s	O
are	O
the	O
squares	O
of	O
the	O
canonical	O
correlations	O
the	O
significance	O
of	O
the	O
s	O
can	O
be	O
judged	O
from	O
the	O
statistics	O
produced	O
by	O
manova	B
this	O
representation	O
of	O
linear	B
discrimination	B
which	O
is	O
due	O
to	O
fisher	O
is	O
discussed	O
also	O
in	O
section	O
departure	O
from	O
normality	O
the	O
assumption	O
of	O
multivariate	B
normality	I
underlies	O
much	O
of	O
classical	O
discrimination	B
procedures	O
but	O
the	O
effects	O
of	O
departures	O
from	O
normality	O
on	O
the	O
methods	O
are	O
not	O
easily	O
or	O
clearly	O
understood	O
moreover	O
in	O
analysing	O
multiresponse	O
data	O
it	O
is	O
not	O
known	O
how	O
robust	O
classical	O
procedures	O
are	O
to	O
departures	O
from	O
multivariate	B
normality	I
most	O
studies	O
on	O
robustness	O
depend	O
on	O
simulation	O
studies	O
thus	O
it	O
is	O
useful	O
to	O
have	O
measures	B
for	O
verifying	O
the	O
reasonableness	O
of	O
assuming	O
normality	O
for	O
a	O
given	O
dataset	O
if	O
available	O
such	O
a	O
measure	B
would	O
be	O
helpful	O
in	O
guiding	O
the	O
subsequent	O
analysis	O
of	O
the	O
data	O
to	O
make	O
it	O
more	O
normally	O
distributed	O
or	O
suggesting	O
the	O
most	O
appropriate	O
discrimination	B
method	O
andrews	O
et	O
al	O
e	O
for	O
d	O
d	O
e	O
e	O
e	O
g	O
d	O
f	O
sec	O
characterisation	O
of	O
datasets	O
whose	O
excellent	O
presentation	O
we	O
follow	O
in	O
this	O
section	O
discuss	O
a	O
variety	O
of	O
methods	O
for	O
assessing	O
normality	O
with	O
multiresponse	O
data	O
the	O
possibilities	O
for	O
departure	O
from	O
joint	O
normality	O
are	O
many	O
and	O
varied	O
one	O
implication	O
of	O
this	O
is	O
the	O
need	O
for	O
a	O
variety	O
of	O
techniques	O
with	O
differing	O
sensitivities	O
to	O
the	O
different	O
types	O
of	O
departure	O
and	O
to	O
the	O
effects	O
that	O
such	O
departures	O
have	O
on	O
the	O
subsequent	O
analysis	O
of	O
great	O
importance	O
here	O
is	O
the	O
degree	O
of	O
commitment	O
one	O
wishes	O
to	O
make	O
to	O
the	O
coordinate	O
system	O
for	O
the	O
multiresponse	O
observations	O
at	O
one	O
extreme	O
is	O
the	O
situation	O
where	O
the	O
interest	O
is	O
completely	O
confined	O
to	O
the	O
observed	O
coordinates	O
in	O
this	O
case	O
the	O
marginal	O
distributions	O
of	O
each	O
of	O
the	O
observed	O
variables	O
and	O
conditional	O
distributions	O
of	O
certain	O
of	O
these	O
given	O
certain	O
others	O
would	O
be	O
the	O
objects	O
of	O
interest	O
at	O
the	O
other	O
extreme	O
the	O
class	B
of	O
all	O
nonsingular	O
linear	O
transformations	O
of	O
the	O
variables	O
would	O
be	O
of	O
interest	O
one	O
possibility	O
is	O
to	O
look	O
at	O
all	O
possible	O
linear	O
combinations	O
of	O
the	O
variables	O
and	O
find	O
the	O
maximum	O
departure	O
from	O
univariate	O
normality	O
in	O
these	O
combinations	O
mardia	O
et	O
al	O
give	O
multivariate	O
measures	B
of	O
skewness	B
and	O
kurtosis	B
that	O
are	O
invariant	O
to	O
affine	O
transformations	O
of	O
the	O
data	O
critical	O
values	O
of	O
these	O
statistics	O
for	O
small	O
samples	O
are	O
given	O
in	O
mardia	O
these	O
measures	B
are	O
difficult	O
to	O
compare	O
across	O
datasets	O
with	O
differing	O
dimensionality	O
they	O
also	O
have	O
the	O
disadvantage	O
that	O
they	O
do	O
not	O
reduce	O
to	O
the	O
usual	O
univariate	O
statistics	O
when	O
the	O
attributes	B
are	O
independent	O
our	O
approach	O
is	O
to	O
concentrate	O
on	O
the	O
original	O
coordinates	O
by	O
looking	O
at	O
their	O
marginal	O
distributions	O
moreover	O
the	O
emphasis	O
here	O
is	O
on	O
a	O
measure	B
of	O
non-normality	O
rather	O
than	O
on	O
a	O
test	O
that	O
tells	O
us	O
how	O
statistically	O
significant	O
is	O
the	O
departure	O
from	O
normality	O
see	O
ozturk	O
romeu	O
for	O
a	O
review	O
of	O
methods	O
for	O
testing	O
multivariate	B
normality	I
univariate	B
skewness	B
and	O
kurtosis	B
measure	B
is	O
defined	O
via	O
the	O
ratio	O
of	O
the	O
fourth	O
moment	O
about	O
the	O
mean	O
to	O
the	O
fourth	O
power	O
of	O
the	O
standard	O
deviation	O
is	O
generally	O
known	O
as	O
the	O
kurtosis	B
of	O
the	O
distribution	O
however	O
we	O
will	O
itself	O
as	O
the	O
measure	B
of	O
kurtosis	B
since	O
we	O
only	O
use	O
this	O
measure	B
relative	O
to	O
other	O
measurements	O
of	O
the	O
same	O
quantity	O
within	O
this	O
book	O
this	O
slight	O
abuse	O
of	O
the	O
term	O
kurtosis	B
the	O
usual	O
measure	B
of	O
univariate	B
skewness	B
et	O
al	O
is	O
which	O
is	O
the	O
ratio	O
of	O
another	O
although	O
for	O
test	O
purposes	O
it	O
is	O
usual	O
to	O
quote	O
the	O
square	O
of	O
this	O
quantity	O
the	O
quantity	O
refer	O
to	O
and	O
and	O
may	O
be	O
tolerated	O
for	O
the	O
normal	B
distribution	I
the	O
measures	B
are	O
by	O
g	O
as	O
a	O
single	O
in	O
population	O
denote	O
the	O
skewness	B
statistic	O
for	O
attribute	O
c	O
g	O
averaged	O
over	O
all	O
attributes	B
and	O
over	O
all	O
populations	O
this	O
gives	O
the	O
measure	B
cb	O
for	O
a	O
normal	O
population	O
ed	O
dd	O
y	O
are	O
zero	O
and	O
respectively	O
similarly	O
we	O
find	O
variables	O
the	O
theoretical	O
values	O
of	O
dd	O
we	O
will	O
say	O
that	O
the	O
skewness	B
is	O
zero	O
and	O
the	O
kurtosis	B
is	O
although	O
the	O
usual	O
definition	O
of	O
kurtosis	B
gives	O
a	O
value	O
of	O
zero	O
for	O
a	O
normal	B
distribution	I
mean	O
skewness	B
and	O
kurtosis	B
k	O
lk	O
the	O
mean	O
cubed	O
deviation	O
from	O
the	O
mean	O
to	O
the	O
cube	O
of	O
the	O
standard	O
deviation	O
measure	B
of	O
skewness	B
for	O
the	O
whole	O
dataset	O
we	O
quote	O
the	O
mean	O
of	O
the	O
absolute	O
value	O
of	O
is	O
zero	O
for	O
uniform	B
and	O
exponential	O
g	O
h	O
g	O
h	O
i	O
i	O
i	O
methods	O
for	O
comparison	O
cb	O
the	O
mean	O
of	O
the	O
univariate	O
standardised	O
fourth	O
moment	O
and	O
populations	O
this	O
gives	O
the	O
measure	B
for	O
a	O
normal	O
population	O
the	O
corresponding	O
figures	O
for	O
uniform	B
and	O
exponential	O
variables	O
are	O
and	O
respectively	O
univariate	B
skewness	B
and	O
kurtosis	B
of	O
correlated	O
attributes	B
the	O
univariate	O
measures	B
above	O
have	O
very	O
large	O
variances	O
if	O
the	O
attributes	B
are	O
highly	O
correlated	O
it	O
may	O
therefore	O
be	O
desirable	O
to	O
transform	O
to	O
uncorrelated	O
variables	O
before	O
finding	O
the	O
univariate	B
skewness	B
and	O
kurtosis	B
measures	B
this	O
may	O
be	O
achieved	O
via	O
the	O
symmetric	O
inverse	O
square-root	O
of	O
the	O
covariance	B
matrix	I
the	O
corresponding	O
kurtosis	B
and	O
skewness	B
exactly	O
and	O
g	O
averaged	O
over	O
all	O
attributes	B
bk	O
say	O
may	O
be	O
more	O
reliable	O
for	O
correlated	O
attributes	B
by	O
construction	O
these	O
measures	B
reduce	O
to	O
the	O
univariate	O
values	O
if	O
the	O
attributes	B
are	O
uncorrelated	O
although	O
they	O
were	O
calculated	O
for	O
all	O
the	O
datasets	O
these	O
particular	O
measures	B
are	O
not	O
quoted	O
in	O
the	O
tables	O
as	O
they	O
are	O
usually	O
similar	O
to	O
the	O
univariate	O
statistics	O
bk	O
and	O
dd	O
measure	B
d	O
information	O
theoretic	O
measures	B
for	O
the	O
most	O
part	O
the	O
statistical	B
measures	B
above	O
were	O
based	O
on	O
the	O
assumption	O
of	O
continuous	O
attributes	B
the	O
measures	B
we	O
discuss	O
now	O
are	O
motivated	O
by	O
information	B
theory	I
and	O
are	O
most	O
appropriate	O
for	O
discrete	O
indeed	O
categorical	O
attributes	B
although	O
they	O
are	O
able	O
to	O
deal	O
with	O
continuous	O
attributes	B
also	O
for	O
this	O
reason	O
these	O
measures	B
are	O
very	O
much	O
used	O
by	O
the	O
machine	O
learning	O
community	O
and	O
are	O
often	O
used	O
as	O
a	O
basis	O
for	O
splitting	B
criteria	I
when	O
building	O
decision	B
trees	I
they	O
correspond	O
to	O
the	O
deviance	O
statistics	O
that	O
arise	O
in	O
the	O
analysis	O
of	O
contingency	O
tables	O
nelder	O
for	O
a	O
basic	O
introduction	O
to	O
the	O
subject	O
of	O
information	B
theory	I
see	O
for	O
example	B
jones	O
entropy	B
of	I
attributes	B
entropy	B
is	O
a	O
measure	B
of	O
randomness	O
in	O
a	O
random	O
variable	O
in	O
general	O
terms	O
the	O
entropy	B
continuous	O
variable	O
with	O
given	O
variance	O
maximal	O
entropy	B
is	O
attained	O
for	O
normal	O
takes	O
on	O
the	O
i	O
th	O
value	O
conventionally	O
logarithms	O
are	O
to	O
base	O
and	O
entropy	B
is	O
then	O
said	O
to	O
be	O
measured	O
in	O
units	O
called	O
information	O
units	O
in	O
what	O
follows	O
all	O
logarithms	O
are	O
to	O
base	O
the	O
special	O
cases	O
to	O
remember	O
are	O
equal	O
probabilities	O
distribution	O
the	O
entropy	B
of	O
a	O
discrete	O
random	O
variable	O
the	O
maximal	O
cby	O
g	O
of	O
a	O
discrete	O
random	O
variable	O
x	O
is	O
defined	O
as	O
the	O
sum	O
log	O
is	O
the	O
probability	O
thaty	O
where	O
are	O
equal	O
if	O
there	O
ared	O
possible	O
values	O
fory	O
is	O
maximal	O
when	O
all	O
entropy	B
is	O
logd	O
variables	O
and	O
this	O
maximal	O
entropy	B
is	O
the	O
attributes	B
we	O
take	O
the	O
logc	O
g	O
averaged	O
over	O
all	O
attributesy	O
in	O
the	O
context	O
of	O
classification	B
schemes	O
the	O
point	O
to	O
note	O
is	O
that	O
an	O
attribute	O
that	O
does	O
not	O
vary	O
at	O
all	O
and	O
therefore	O
has	O
zero	O
entropy	B
contains	O
no	O
information	O
for	O
discriminating	O
between	O
classes	B
the	O
entropy	B
of	O
a	O
collection	O
of	O
attributes	B
is	O
not	O
simply	O
related	O
to	O
the	O
individual	O
entropies	O
but	O
as	O
a	O
basic	O
measure	B
we	O
can	O
average	O
the	O
entropy	B
over	O
all	O
the	O
attributes	B
and	O
take	O
this	O
as	O
a	O
global	O
measure	B
of	O
entropy	B
of	O
the	O
attributes	B
collectively	O
thus	O
as	O
a	O
measure	B
of	O
entropy	B
of	O
i	O
g	O
g	O
g	O
y	O
g	O
g	O
examples	O
this	O
means	O
choosing	O
sec	O
characterisation	O
of	O
datasets	O
this	O
measure	B
is	O
strictly	O
appropriate	O
only	O
for	O
independent	O
attributes	B
the	O
definition	O
of	O
entropy	B
for	O
continuous	O
distributions	O
is	O
analogous	O
to	O
the	O
discrete	O
case	O
with	O
an	O
integral	O
replacing	O
the	O
summation	O
term	O
this	O
definition	O
is	O
no	O
use	O
for	O
empirical	O
data	O
however	O
unless	O
some	O
very	O
drastic	O
assumptions	O
are	O
made	O
example	B
assuming	O
that	O
the	O
data	O
have	O
a	O
normal	B
distribution	I
and	O
we	O
are	O
forced	O
to	O
apply	O
the	O
discrete	O
definition	O
to	O
all	O
empirical	O
data	O
for	O
the	O
measures	B
defined	O
below	O
we	O
discretised	O
all	O
numerical	O
data	O
into	O
equal-length	O
intervals	O
the	O
number	O
of	O
intervals	O
was	O
chosen	O
so	O
that	O
there	O
was	O
a	O
fair	O
expectation	O
that	O
there	O
would	O
be	O
about	O
ten	O
observations	O
per	O
cell	O
in	O
the	O
two-way	O
table	O
of	O
in	O
many	O
of	O
our	O
datasets	O
some	O
classes	B
have	O
very	O
low	O
probabilities	O
of	O
occurrence	O
and	O
for	O
practical	O
purposes	O
the	O
very	O
infrequent	O
classes	B
play	O
little	O
part	O
in	O
the	O
assessment	O
of	O
classification	B
schemes	O
it	O
is	O
therefore	O
inappropriate	O
merely	O
to	O
count	O
the	O
number	O
of	O
classes	B
intervals	O
a	O
more	O
refined	O
procedure	O
would	O
have	O
the	O
number	O
and	O
width	O
of	O
intervals	O
varying	O
from	O
attribute	O
to	O
attribute	O
and	O
from	O
dataset	O
to	O
dataset	O
unless	O
the	O
data	O
are	O
very	O
extensive	O
the	O
estimated	O
entropies	O
even	O
for	O
discrete	O
variables	O
are	O
likely	O
to	O
be	O
severely	O
biased	O
blyth	O
discusses	O
methods	O
of	O
reducing	O
the	O
bias	B
cells	O
in	O
a	O
two-way	O
table	O
of	O
attribute	O
discrete	O
attribute	O
by	O
class	B
as	O
there	O
are	O
the	O
simplest	O
but	O
not	O
the	O
best	O
procedure	O
is	O
to	O
divide	O
the	O
range	O
of	O
the	O
attribute	O
into	O
equal	O
levels	O
by	O
class	B
classes	B
and	O
there	O
are	O
entropy	B
of	I
classes	B
and	O
use	O
this	O
as	O
a	O
measure	B
of	O
complexity	O
an	O
alternative	O
is	O
to	O
use	O
the	O
entropy	B
c	O
g	O
of	O
the	O
c	O
log	O
where	O
is	O
the	O
prior	O
probability	O
for	O
class	B
entropy	B
is	O
related	O
to	O
the	O
average	O
length	O
of	O
a	O
for	O
example	B
since	O
class	B
is	O
essentially	O
discrete	O
the	O
class	B
entropy	B
c	O
when	O
the	O
classes	B
are	O
equally	O
likely	O
so	O
that	O
c	O
g	O
has	O
maximal	O
value	O
d	O
as	O
an	O
effective	O
of	O
classes	B
a	O
useful	O
way	O
of	O
looking	O
at	O
the	O
entropy	B
c	O
is	O
at	O
most	O
log	O
where	O
is	O
to	O
regard	O
joint	B
entropy	B
of	O
class	B
and	O
attribute	O
c	O
the	O
joint	B
entropy	B
c	O
andy	O
g	O
of	O
two	O
variables	O
and	O
the	O
value	O
of	O
attributey	O
denotes	O
the	O
joint	O
g	O
if	O
combined	O
system	O
of	O
variables	O
i	O
e	O
the	O
pair	O
of	O
variablesc	O
probability	O
of	O
observing	O
class	B
c	O
log	O
andy	O
v	O
c	O
g	O
of	O
two	O
variables	O
the	O
mutual	O
informationv	O
c	O
the	O
joint	O
probability	O
of	O
observing	O
class	B
and	O
the	O
value	O
of	O
attributey	O
denotes	O
g	O
is	O
zero	O
if	O
there	O
is	O
no	O
shared	O
information	O
and	O
the	O
mutual	O
informationv	O
c	O
probability	O
of	O
class	B
is	O
and	O
if	O
the	O
marginal	O
probability	O
of	O
attributey	O
taking	O
on	O
its	O
then	O
the	O
mutual	B
information	I
is	O
defined	O
to	O
be	O
that	O
there	O
is	O
no	O
minus	O
sign	O
value	O
is	O
this	O
is	O
a	O
simple	O
extension	O
of	O
the	O
notion	O
of	O
entropy	B
to	O
the	O
combined	O
system	O
of	O
variables	O
mutual	B
information	I
of	O
class	B
and	O
attribute	O
is	O
a	O
measure	B
of	O
common	O
information	O
or	O
entropy	B
shared	O
between	O
the	O
two	O
variables	O
if	O
the	O
two	O
variables	O
are	O
independent	O
is	O
a	O
measure	B
of	O
total	O
entropy	B
of	O
the	O
the	O
joint	B
entropy	B
is	O
number	O
of	O
classes	B
defined	O
to	O
be	O
if	O
the	O
marginal	O
class	B
probability	O
distribution	O
variable	O
length	O
coding	O
scheme	O
and	O
there	O
are	O
direct	O
links	O
to	O
decision	B
trees	I
jones	O
is	O
the	O
number	O
h	O
f	O
c	O
g	O
g	O
g	O
g	O
y	O
g	O
y	O
y	O
y	O
g	O
y	O
g	O
y	O
y	O
methods	O
for	O
comparison	O
equivalent	O
definitions	O
are	O
since	O
there	O
are	O
many	O
attributes	B
we	O
have	O
tabulated	O
an	O
average	O
of	O
the	O
mutual	B
information	I
defined	O
formally	O
by	O
the	O
equation	O
in	O
which	O
it	O
appears	O
above	O
but	O
it	O
has	O
a	O
distinct	O
meaning	O
namely	O
the	O
entropy	B
randomness	O
or	O
noise	B
of	O
the	O
class	B
variable	O
that	O
is	O
not	O
removed	O
by	O
is	O
zero	O
and	O
this	O
occurs	O
when	O
class	B
and	O
attribute	O
are	O
independent	O
the	O
maximum	O
mutual	B
information	I
is	O
zero	O
suppose	O
for	O
example	B
that	O
is	O
zero	O
this	O
would	O
mean	O
that	O
the	O
value	O
of	O
class	B
is	O
fixed	O
once	O
the	O
in	O
contains	O
all	O
the	O
information	O
needed	O
to	O
specify	O
the	O
class	B
the	O
logc	O
v	O
c	O
o	O
e	O
cby	O
c	O
c	O
v	O
c	O
cby	O
v	O
c	O
the	O
conditional	O
entropy	B
c	O
v	O
c	O
g	O
for	O
example	B
which	O
we	O
have	O
not	O
yet	O
defined	O
may	O
be	O
knowing	O
the	O
value	O
of	O
the	O
attribute	O
x	O
minimum	O
mutual	O
informationv	O
c	O
g	O
occurs	O
when	O
one	O
of	O
c	O
g	O
or	O
c	O
v	O
c	O
value	O
ofy	O
is	O
then	O
completely	O
predictable	O
from	O
the	O
attributey	O
the	O
sense	O
that	O
attributey	O
is	O
known	O
class	B
g	O
are	O
g	O
c	O
cby	O
corresponding	O
limits	O
ofv	O
c	O
g	O
g	O
v	O
c	O
taken	O
over	O
all	O
attributesy	O
d	O
e	O
yh	O
v	O
c	O
v	O
c	O
v	O
c	O
the	O
information	O
required	O
to	O
specify	O
the	O
class	B
is	O
c	O
be	O
completely	O
successful	O
unless	O
it	O
provides	O
at	O
least	O
c	O
g	O
and	O
no	O
classification	B
scheme	O
can	O
g	O
bits	O
of	O
useful	O
information	O
g	O
of	O
all	O
attributes	B
together	O
vector	O
of	O
that	O
the	O
useful	O
information	O
v	O
c	O
g	O
is	O
greater	O
than	O
the	O
sum	O
of	O
the	O
individual	O
informations	O
g	O
however	O
in	O
the	O
simplest	O
most	O
unrealistic	O
case	O
that	O
all	O
e	O
v	O
c	O
v	O
c	O
y	O
v	O
c	O
e	O
v	O
c	O
v	O
c	O
g	O
and	O
the	O
average	O
mutual	B
information	I
the	O
ratio	O
between	O
the	O
class	B
entropy	B
c	O
g	O
c	O
en	O
attr	O
v	O
c	O
of	O
course	O
we	O
might	O
do	O
better	O
by	O
taking	O
the	O
attributes	B
with	O
highest	O
mutual	B
information	I
but	O
in	O
any	O
case	O
the	O
assumption	O
of	O
independent	O
useful	O
bits	O
of	O
information	O
is	O
very	O
dubious	O
in	O
any	O
case	O
so	O
this	O
simple	O
measure	B
is	O
probably	O
quite	O
sufficient	O
this	O
information	O
is	O
to	O
come	O
from	O
the	O
attributes	B
taken	O
together	O
and	O
it	O
is	O
quite	O
possible	O
stands	O
for	O
the	O
this	O
average	O
mutual	B
information	I
gives	O
a	O
measure	B
of	O
how	O
much	O
useful	O
information	O
about	O
classes	B
is	O
provided	O
by	O
the	O
average	O
attribute	O
mutual	B
information	I
may	O
be	O
used	O
as	O
a	O
splitting	B
criterion	I
in	O
decision	O
tree	O
algorithms	O
and	O
is	O
preferable	O
to	O
the	O
gain	O
ratio	O
criterion	O
of	O
haussler	O
equivalent	B
number	I
of	I
attributes	B
en	O
attr	O
attributes	B
are	O
independent	O
we	O
would	O
have	O
in	O
this	O
case	O
the	O
attributes	B
contribute	O
independent	O
bits	O
of	O
useful	O
information	O
for	O
classification	B
purposes	O
and	O
we	O
can	O
count	O
up	O
how	O
many	O
attributes	B
would	O
be	O
requiredon	O
average	O
by	O
taking	O
y	O
g	O
g	O
y	O
g	O
c	O
g	O
g	O
y	O
g	O
y	O
g	O
c	O
g	O
y	O
g	O
y	O
g	O
g	O
g	O
y	O
y	O
g	O
y	O
y	O
g	O
y	O
g	O
y	O
y	O
g	O
y	O
g	O
y	O
y	O
g	O
g	O
y	O
y	O
y	O
g	O
e	O
y	O
y	O
g	O
y	O
g	O
e	O
g	O
v	O
c	O
y	O
g	O
y	O
g	O
sec	O
characterisation	O
of	O
datasets	O
values	O
of	O
the	O
ratio	O
noisiness	O
of	O
attributes	B
ns	O
ratio	O
if	O
the	O
useful	O
information	O
is	O
only	O
a	O
small	O
fraction	O
of	O
the	O
total	O
information	O
we	O
may	O
say	O
that	O
there	O
is	O
a	O
large	O
amount	O
of	O
noise	B
thus	O
take	O
about	O
class	B
and	O
cby	O
g	O
as	O
a	O
measure	B
of	O
useful	O
information	O
v	O
c	O
g	O
as	O
a	O
measure	B
as	O
non-useful	O
information	O
then	O
large	O
ns	O
ratio	O
imply	O
a	O
dataset	O
that	O
contains	O
much	O
irrelevant	O
information	O
such	O
datasets	O
could	O
be	O
condensed	O
considerably	O
without	O
affecting	O
the	O
performance	O
of	O
the	O
classifier	B
for	O
example	B
by	O
removing	O
irrelevant	B
attributes	B
by	O
reducing	O
the	O
number	O
of	O
discrete	O
levels	O
used	O
to	O
specify	O
the	O
attributes	B
or	O
perhaps	O
by	O
merging	O
qualitative	O
factors	O
the	O
notation	O
ns	O
ratio	O
denotes	O
the	O
noise-signal-ratio	O
note	O
that	O
this	O
is	O
the	O
reciprocal	O
of	O
the	O
more	O
usual	O
signal-noise-ratio	O
irrelevant	B
attributes	B
v	O
c	O
v	O
c	O
cby	O
attribute	O
this	O
context	O
interpreting	O
the	O
mutual	B
information	I
as	O
a	O
deviance	O
statistic	O
would	O
be	O
useful	O
and	O
we	O
can	O
give	O
a	O
lower	O
bound	O
to	O
statistically	O
significant	O
values	O
for	O
mutual	B
information	I
and	O
class	B
are	O
in	O
fact	O
statistically	O
independent	O
and	O
suppose	O
is	O
large	O
then	O
it	O
is	O
approximately	O
equal	O
to	O
the	O
chisquare	O
statistic	O
for	O
testing	O
the	O
independence	O
of	O
attribute	O
and	O
class	B
example	B
agresti	O
g	O
between	O
class	B
and	O
attributeyh	O
can	O
be	O
used	O
to	O
judge	O
yh	O
if	O
attributeyh	O
could	O
of	O
itself	O
contribute	O
usefully	O
to	O
a	O
classification	B
scheme	O
attributes	B
the	O
mutual	O
informationv	O
c	O
g	O
would	O
not	O
by	O
themselves	O
be	O
useful	O
predictors	O
of	O
class	B
in	O
with	O
small	O
values	O
ofv	O
c	O
suppose	O
that	O
attributey	O
has	O
distinct	O
levels	O
assuming	O
further	O
that	O
the	O
sample	O
size	O
thaty	O
is	O
well	O
known	O
that	O
the	O
deviance	O
statistic	O
v	O
c	O
d	O
d	O
distribution	O
and	O
order	O
of	O
g	O
has	O
an	O
approximate	O
therefore	O
v	O
c	O
is	O
the	O
number	O
of	O
examples	O
and	O
feg	O
h	O
where	O
the	O
hypothesis	O
testing	O
sense	O
if	O
its	O
value	O
exceedsc	O
of	O
classes	B
in	O
our	O
measures	B
continuous	O
attributes	B
we	O
chose	O
hf	O
number	O
of	O
levels	O
for	O
so-called	O
continuous	O
attributes	B
was	O
less	O
than	O
d	O
distribution	O
as	O
twice	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
d	O
hf	O
if	O
we	O
adopt	O
with	O
our	O
chosen	O
value	O
of	O
this	O
is	O
of	O
orderfdhf	O
a	O
critical	O
level	O
for	O
the	O
fdg	O
fegih	O
as	O
c	O
magnitude	O
calculations	O
indicate	O
that	O
the	O
mutual	B
information	I
contributes	O
significantly	O
is	O
the	O
number	O
is	O
the	O
number	O
of	O
discrete	O
levels	O
for	O
the	O
is	O
the	O
number	O
of	O
levels	O
for	O
integer	O
or	O
binary	B
attributes	B
and	O
for	O
that	O
on	O
average	O
there	O
were	O
about	O
observations	O
per	O
cell	O
in	O
the	O
two-way	O
table	O
of	O
attribute	O
by	O
class	B
but	O
occasionally	O
the	O
we	O
have	O
not	O
quoted	O
any	O
measure	B
of	O
this	O
form	O
as	O
almost	O
all	O
attributes	B
are	O
relevant	O
in	O
this	O
sense	O
this	O
measure	B
would	O
have	O
little	O
information	O
content	O
in	O
any	O
case	O
an	O
equivalent	O
measure	B
would	O
be	O
the	O
difference	O
between	O
the	O
actual	O
number	O
of	O
attributes	B
and	O
the	O
value	O
of	O
en	O
attr	O
correlated	O
normal	O
attributes	B
when	O
attributes	B
are	O
correlated	O
the	O
calculation	O
of	O
information	B
measures	B
becomes	O
much	O
more	O
difficult	O
so	O
difficult	O
in	O
fact	O
that	O
we	O
have	O
avoided	O
it	O
altogether	O
the	O
above	O
univariate	O
for	O
the	O
sake	O
of	O
argument	O
we	O
obtain	O
an	O
approximate	O
critical	O
level	O
for	O
the	O
mutual	B
information	I
for	O
continuous	O
attributes	B
fdg	O
y	O
g	O
y	O
g	O
v	O
c	O
y	O
g	O
y	O
g	O
y	O
g	O
y	O
c	O
c	O
methods	O
for	O
comparison	O
measures	B
take	O
no	O
account	O
of	O
any	O
lack	O
of	O
independence	O
and	O
are	O
therefore	O
very	O
crude	O
approximations	O
to	O
reality	O
there	O
are	O
however	O
some	O
simple	O
results	O
concerning	O
the	O
multivariate	O
normal	B
distribution	I
for	O
which	O
the	O
entropy	B
is	O
logc	O
is	O
the	O
determinant	O
of	O
the	O
covariance	B
matrix	I
of	O
the	O
variables	O
similar	O
results	O
hold	O
for	O
mutual	B
information	I
and	O
there	O
are	O
then	O
links	O
with	O
the	O
statistical	B
measures	B
elaborated	O
in	O
section	O
unfortunately	O
even	O
if	O
such	O
measures	B
were	O
used	O
for	O
our	O
datasets	O
most	O
datasets	O
are	O
so	O
far	O
from	O
normality	O
that	O
the	O
interpretation	O
of	O
the	O
resulting	O
measures	B
would	O
be	O
very	O
questionable	O
where	O
pre-processing	O
usually	O
there	O
is	O
no	O
control	O
over	O
the	O
form	O
or	O
content	O
of	O
the	O
vast	O
majority	O
of	O
datasets	O
generally	O
they	O
are	O
already	O
converted	O
from	O
whatever	O
raw	O
data	O
was	O
available	O
into	O
some	O
suitable	O
format	O
and	O
there	O
is	O
no	O
way	O
of	O
knowing	O
if	O
the	O
manner	O
in	O
which	O
this	O
was	O
done	O
was	O
consistent	O
or	O
perhaps	O
chosen	O
to	O
fit	O
in	O
with	O
some	O
pre-conceived	O
type	O
of	O
analysis	O
in	O
some	O
datasets	O
it	O
is	O
very	O
clear	O
that	O
some	O
very	O
drastic	O
form	O
of	O
pre-processing	O
has	O
already	O
been	O
done	O
see	O
section	O
for	O
example	B
missing	B
values	I
some	O
algorithms	O
naive	B
bayes	I
cart	B
bayes	B
tree	I
newid	B
can	O
deal	O
with	O
missing	B
values	I
whereas	O
others	O
require	O
that	O
the	O
missing	B
values	I
be	O
replaced	O
the	O
procedure	O
discrim	B
was	O
not	O
able	O
to	O
handle	O
missing	B
values	I
although	O
this	O
can	O
be	O
done	O
in	O
principle	O
for	O
linear	B
discrimination	B
for	O
certain	O
types	O
of	O
missing	O
value	O
in	O
order	O
to	O
get	O
comparable	O
results	O
we	O
settled	O
on	O
a	O
general	O
policy	O
of	O
replacing	O
all	O
missing	B
values	I
where	O
an	O
attribute	O
value	O
was	O
missing	O
it	O
was	O
replaced	O
by	O
the	O
global	O
mean	O
or	O
median	O
for	O
that	O
attribute	O
if	O
the	O
class	B
value	O
was	O
missing	O
the	O
whole	O
observation	O
was	O
omitted	O
usually	O
the	O
proportion	O
of	O
cases	O
with	O
missing	O
information	O
was	O
very	O
low	O
as	O
a	O
separate	O
exercise	O
it	O
would	O
be	O
of	O
interest	O
to	O
learn	O
how	O
much	O
information	O
is	O
lost	O
gained	O
in	O
such	O
a	O
strategy	O
by	O
those	O
algorithms	O
that	O
can	O
handle	O
missing	B
values	I
unfortunately	O
there	O
are	O
various	O
ways	O
in	O
which	O
missing	B
values	I
might	O
arise	O
and	O
their	O
treatment	O
is	O
quite	O
different	O
for	O
example	B
a	O
clinician	O
may	O
normally	O
use	O
the	O
results	O
of	O
a	O
blood-test	O
in	O
making	O
a	O
diagnosis	O
if	O
the	O
blood-test	O
is	O
not	O
carried	O
out	O
perhaps	O
because	O
of	O
faulty	O
equipment	O
the	O
blood-test	O
measurements	O
are	O
missing	O
for	O
that	O
specimen	O
a	O
situation	O
that	O
may	O
appear	O
similar	O
results	O
from	O
doing	O
measurements	O
on	O
a	O
subset	O
of	O
the	O
population	O
for	O
example	B
only	O
doing	O
pregnancy	O
tests	O
on	O
women	O
where	O
the	O
test	O
is	O
not	O
relevant	O
for	O
men	O
so	O
is	O
missing	O
for	O
men	O
in	O
the	O
first	O
case	O
the	O
measurements	O
are	O
missing	O
at	O
random	O
and	O
in	O
the	O
second	O
the	O
measurements	O
are	O
structured	O
or	O
hierarchical	O
although	O
the	O
treatment	O
of	O
these	O
two	O
cases	O
should	O
be	O
radically	O
different	O
the	O
necessary	O
information	O
is	O
often	O
lacking	O
in	O
at	O
least	O
one	O
dataset	O
it	O
would	O
appear	O
that	O
this	O
problem	O
arises	O
in	O
a	O
very	O
extreme	O
manner	O
as	O
it	O
would	O
seem	O
that	O
missing	B
values	I
are	O
coded	O
as	O
zero	O
and	O
that	O
a	O
large	O
majority	O
of	O
observations	O
is	O
zero	O
feature	O
selection	O
and	O
extraction	O
some	O
datasets	O
are	O
so	O
large	O
that	O
many	O
algorithms	O
have	O
problems	O
just	O
entering	O
the	O
data	O
and	O
the	O
sheer	O
size	O
of	O
the	O
dataset	O
has	O
to	O
be	O
reduced	O
in	O
this	O
case	O
to	O
achieve	O
uniformity	O
a	O
data	O
g	O
sec	O
pre-processing	O
reduction	O
process	O
was	O
performed	O
in	O
advance	O
of	O
the	O
trials	O
again	O
it	O
is	O
of	O
interest	O
to	O
note	O
which	O
algorithms	O
can	O
cope	O
with	O
the	O
very	O
large	O
datasets	O
there	O
are	O
several	O
ways	O
in	O
which	O
data	O
reduction	O
can	O
take	O
place	O
for	O
example	B
the	O
karhunen-loeve	O
transformation	B
can	O
be	O
used	O
with	O
very	O
little	O
loss	O
of	O
information	O
see	O
section	O
for	O
an	O
example	B
another	O
way	O
of	O
reducing	O
the	O
number	O
of	O
variables	O
is	O
by	O
a	O
stepwise	O
procedure	O
in	O
a	O
linear	B
discriminant	I
procedure	O
for	O
example	B
this	O
was	O
tried	O
on	O
the	O
dataset	O
in	O
which	O
a	O
version	O
with	O
number	O
of	O
attributes	B
reduced	O
from	O
to	O
was	O
also	O
considered	O
results	O
for	O
both	O
these	O
versions	O
are	O
presented	O
and	O
make	O
for	O
an	O
interesting	O
paired	O
comparison	O
see	O
the	O
section	O
on	O
paired	O
comparisons	O
for	O
the	O
dataset	O
in	O
section	O
in	O
some	O
datasets	O
particularly	O
image	B
segmentation	I
extra	O
relevant	O
information	O
can	O
be	O
included	O
for	O
example	B
we	O
can	O
use	O
the	O
prior	O
knowledge	O
that	O
examples	O
which	O
are	O
neighbours	O
are	O
likely	O
to	O
have	O
the	O
same	O
class	B
a	O
dataset	O
of	O
this	O
type	O
is	O
considered	O
in	O
section	O
in	O
which	O
a	O
satellite	B
image	I
uses	O
the	O
fact	O
that	O
attributes	B
of	O
neighbouring	O
pixels	O
can	O
give	O
useful	O
information	O
in	O
classifying	O
the	O
given	O
pixel	O
especially	O
in	O
an	O
exploratory	O
study	O
practitioners	O
often	O
combine	O
attributes	B
in	O
an	O
attempt	O
to	O
increase	O
the	O
descriptive	O
power	O
of	O
the	O
resulting	O
decision	O
treerules	O
etc	O
for	O
example	B
it	O
that	O
is	O
important	O
rather	O
might	O
be	O
conjectured	O
that	O
it	O
is	O
the	O
sum	O
of	O
two	O
attributes	B
than	O
each	O
attribute	O
separately	O
alternatively	O
some	O
ratios	O
are	O
included	O
such	O
as	O
in	O
our	O
trials	O
we	O
did	O
not	O
introduce	O
any	O
such	O
combinations	O
on	O
the	O
other	O
hand	O
there	O
existed	O
already	O
some	O
linear	O
combinations	O
of	O
attributes	B
in	O
some	O
of	O
the	O
datasets	O
that	O
we	O
looked	O
at	O
we	O
took	O
the	O
view	O
that	O
these	O
combinations	O
were	O
included	O
because	O
the	O
dataset	O
provider	O
thought	O
that	O
these	O
particular	O
combinations	O
were	O
potentially	O
useful	O
although	O
capable	O
of	O
running	O
on	O
attributes	B
with	O
linear	O
dependencies	O
some	O
of	O
the	O
statistical	B
procedures	O
prefer	O
attributes	B
that	O
are	O
linearly	O
independent	O
so	O
when	O
it	O
came	O
to	O
running	O
lda	O
qda	O
and	O
logistic	B
discrimination	B
we	O
excluded	O
attributes	B
that	O
were	O
linear	O
combinations	O
of	O
others	O
this	O
was	O
the	O
case	O
for	O
the	O
belgian	O
power	O
data	O
which	O
is	O
described	O
in	O
section	O
although	O
in	O
principle	O
the	O
performance	O
of	O
linear	B
discriminant	I
procedures	O
is	O
not	O
affected	O
by	O
the	O
presence	O
of	O
linear	O
combinations	O
of	O
attributes	B
in	O
practice	O
the	O
resulting	O
singularities	O
are	O
best	O
avoided	O
for	O
numerical	O
reasons	O
g	O
c	O
e	O
as	O
the	O
performance	O
of	O
statistical	B
procedures	O
is	O
directly	O
related	O
to	O
the	O
statistical	B
properties	O
of	O
the	O
attributes	B
it	O
is	O
generally	O
advisable	O
to	O
transform	O
the	O
attributes	B
so	O
that	O
their	O
marginal	O
distributions	O
are	O
as	O
near	O
normal	O
as	O
possible	O
each	O
attribute	O
is	O
considered	O
in	O
turn	O
and	O
some	O
transformation	B
usually	O
from	O
the	O
power-law	O
family	O
is	O
made	O
on	O
the	O
attribute	O
most	O
frequently	O
this	O
is	O
done	O
by	O
taking	O
the	O
square-root	O
logarithm	O
or	O
reciprocal	O
transform	O
these	O
transforms	O
may	O
help	O
the	O
statistical	B
procedures	O
in	O
theory	O
at	O
least	O
they	O
have	O
no	O
effect	O
on	O
non-parametric	O
procedures	O
such	O
as	O
the	O
decision	B
trees	I
or	O
naive	B
bayes	I
large	O
number	O
of	O
categories	O
we	O
describe	O
now	O
the	O
problems	O
that	O
arise	O
for	O
decision	B
trees	I
and	O
statistical	B
algorithms	O
alike	O
when	O
an	O
attribute	O
has	O
a	O
large	O
number	O
of	O
categories	O
firstly	O
in	O
building	O
a	O
decision	O
tree	O
a	O
potential	O
split	O
of	O
a	O
categorical	O
attribute	O
is	O
based	O
on	O
some	O
partitioning	O
of	O
the	O
categories	O
one	O
partition	O
going	O
down	O
one	O
side	O
of	O
the	O
split	O
and	O
the	O
remainder	O
down	O
the	O
other	O
the	O
number	O
of	O
is	O
much	O
larger	O
than	O
ten	O
there	O
is	O
an	O
enormous	O
computational	O
load	O
and	O
the	O
tree	O
takes	O
a	O
very	O
long	O
time	B
to	O
train	O
however	O
there	O
is	O
a	O
computational	O
shortcut	O
that	O
applies	O
potential	O
splits	O
is	O
where	O
l	O
is	O
the	O
number	O
of	O
different	O
categories	O
of	O
the	O
attribute	O
clearly	O
if	O
e	O
h	O
methods	O
for	O
comparison	O
to	O
two-class	O
problems	O
clark	O
pregibon	O
for	O
example	B
the	O
shortcut	O
method	O
is	O
not	O
implemented	O
in	O
all	O
statlog	B
decision-tree	O
methods	O
with	O
the	O
statistical	B
algorithms	O
a	O
specification	O
of	O
the	O
attribute	O
categorical	O
attribute	O
with	O
categories	O
needs	O
l	O
f	O
binary	O
variables	O
for	O
a	O
complete	O
as	O
a	O
trivial	O
example	B
with	O
two	O
numerical	O
attributesy	O
andq	O
rq	O
probably	O
see	O
exactly	O
the	O
same	O
predictive	O
value	O
in	O
the	O
pair	O
of	O
attributes	B
q	O
the	O
original	O
pair	O
now	O
it	O
is	O
a	O
fact	O
that	O
decision	B
trees	I
behave	O
differently	O
for	O
categorical	O
and	O
numerical	O
data	O
two	O
datasets	O
may	O
be	O
logically	O
equivalent	O
yet	O
give	O
rise	O
to	O
different	O
decision	B
trees	I
statistical	B
algorithms	O
would	O
as	O
in	O
yet	O
the	O
decision	B
trees	I
would	O
be	O
different	O
as	O
the	O
decision	O
boundaries	O
would	O
now	O
be	O
at	O
an	O
angle	O
of	O
degrees	O
when	O
categorical	O
attributes	B
are	O
replaced	O
by	O
binary	O
variables	O
the	O
decision	B
trees	I
will	O
be	O
very	O
different	O
as	O
most	O
decision	O
tree	O
procedures	O
look	O
at	O
all	O
possible	O
subsets	O
of	O
attribute	O
values	O
when	O
considering	O
potential	O
splits	O
there	O
is	O
the	O
additional	O
although	O
perhaps	O
not	O
so	O
important	O
point	O
that	O
the	O
interpretation	O
of	O
the	O
tree	O
is	O
rendered	O
more	O
difficult	O
it	O
is	O
therefore	O
of	O
interest	O
to	O
note	O
where	O
decision	O
tree	O
procedures	O
get	O
almost	O
the	O
same	O
accuracies	O
on	O
an	O
original	O
categorical	O
dataset	O
and	O
the	O
processed	O
binary	O
data	O
newid	B
as	O
run	O
by	O
isoft	O
for	O
example	B
obtained	O
an	O
accuracy	B
of	O
on	O
the	O
processed	O
dna	B
data	O
and	O
on	O
the	O
original	O
dna	B
data	O
categorical	O
attributes	B
these	O
accuracies	O
are	O
probably	O
within	O
what	O
could	O
be	O
called	O
experimental	O
error	O
so	O
it	O
seems	O
that	O
newid	B
does	O
about	O
as	O
well	O
on	O
either	O
form	O
of	O
the	O
dna	B
dataset	I
in	O
such	O
circumstances	O
we	O
have	O
taken	O
the	O
view	O
that	O
for	O
comparative	O
purposes	O
it	O
is	O
better	O
that	O
all	O
algorithms	O
are	O
run	O
on	O
exactly	O
the	O
same	O
preprocessed	O
form	O
this	O
way	O
we	O
avoid	O
differences	O
in	O
preprocessing	B
when	O
comparing	O
performance	O
when	O
faced	O
with	O
a	O
new	O
application	O
it	O
will	O
pay	O
to	O
consider	O
very	O
carefully	O
what	O
form	O
of	O
preprocessing	B
should	O
be	O
done	O
this	O
is	O
just	O
as	O
true	O
for	O
statistical	B
algorithms	O
as	O
for	O
neural	O
nets	O
or	O
machine	O
learning	O
bias	B
in	O
class	B
proportions	O
first	O
some	O
general	O
remarks	O
on	O
potential	O
bias	B
in	O
credit	B
datasets	I
we	O
do	O
not	O
know	O
the	O
way	O
in	O
which	O
the	O
credit	B
datasets	I
were	O
collected	O
but	O
it	O
is	O
very	O
probable	O
that	O
they	O
were	O
biased	O
in	O
the	O
following	O
way	O
most	O
credit	O
companies	O
are	O
very	O
unwilling	O
to	O
give	O
credit	O
to	O
all	O
applicants	O
as	O
a	O
result	O
data	O
will	O
be	O
gathered	O
for	O
only	O
those	O
customers	O
who	O
were	O
given	O
credit	O
if	O
the	O
credit	O
approval	O
process	O
is	O
any	O
good	O
at	O
all	O
the	O
proportion	O
of	O
bad	O
risks	O
among	O
all	O
applicants	O
will	O
be	O
significantly	O
higher	O
than	O
in	O
the	O
given	O
dataset	O
it	O
is	O
very	O
likely	O
also	O
that	O
the	O
profiles	O
of	O
creditors	O
and	O
non-creditors	O
are	O
very	O
different	O
so	O
rules	O
deduced	O
from	O
the	O
creditors	O
will	O
have	O
much	O
less	O
relevance	O
to	O
the	O
target	O
population	O
all	O
applicants	O
when	O
the	O
numbers	O
of	O
good	O
and	O
bad	O
risk	O
examples	O
are	O
widely	O
different	O
and	O
one	O
would	O
expect	O
that	O
the	O
bad	O
risk	O
examples	O
would	O
be	O
relatively	O
infrequent	O
in	O
a	O
well	O
managed	O
lending	O
concern	O
it	O
becomes	O
rather	O
awkward	O
to	O
include	O
all	O
the	O
data	O
in	O
the	O
training	O
of	O
a	O
classification	B
procedure	O
on	O
the	O
one	O
hand	O
if	O
we	O
are	O
to	O
preserve	O
the	O
true	O
class	B
proportions	O
in	O
the	O
training	O
sample	O
the	O
total	O
number	O
of	O
examples	O
may	O
have	O
to	O
be	O
extremely	O
large	O
in	O
order	O
to	O
guarantee	O
sufficient	O
bad	O
risk	O
examples	O
for	O
a	O
reliable	O
rule	O
on	O
the	O
other	O
hand	O
if	O
we	O
follow	O
the	O
common	O
practice	O
in	O
such	O
cases	O
and	O
take	O
as	O
many	O
bad	O
risk	O
examples	O
as	O
possible	O
together	O
with	O
a	O
matching	O
number	O
of	O
good	O
risk	O
examples	O
we	O
are	O
constructing	O
a	O
classification	B
rule	I
with	O
its	O
boundaries	O
in	O
the	O
wrong	O
places	O
the	O
common	O
practice	O
is	O
to	O
make	O
an	O
adjustment	O
to	O
the	O
boundaries	O
to	O
take	O
account	O
of	O
the	O
true	O
class	B
proportions	O
in	O
the	O
case	O
of	O
two	O
classes	B
such	O
sec	O
pre-processing	O
an	O
adjustment	O
is	O
equivalent	O
to	O
allocating	O
different	O
misclassification	B
costs	B
sections	O
and	O
for	O
example	B
if	O
the	O
true	O
bad	O
risk	O
proportion	O
is	O
and	O
a	O
rule	O
is	O
trained	O
on	O
an	O
artificial	O
sample	O
with	O
equal	O
numbers	O
of	O
good	O
and	O
bad	O
risks	O
the	O
recommendation	O
would	O
be	O
to	O
classify	O
as	O
bad	O
risk	O
only	O
those	O
examples	O
whose	O
assessed	O
posterior	O
odds	B
of	O
being	O
bad	O
risk	O
were	O
to	O
to	O
this	O
is	O
equivalent	O
to	O
learning	O
on	O
the	O
artificial	O
sample	O
with	O
the	O
cost	O
of	O
misclassifying	O
bad	O
risks	O
as	O
times	O
that	O
of	O
misclassifying	O
good	O
risk	O
examples	O
for	O
such	O
a	O
procedure	O
to	O
work	O
it	O
is	O
necessary	O
that	O
a	O
classification	B
procedure	O
returns	O
class	B
probabilities	O
as	O
its	O
output	B
and	O
the	O
user	O
can	O
then	O
allocate	O
according	O
to	O
his	O
prior	B
probabilities	I
according	O
to	O
misclassification	B
costs	B
many	O
decision	B
trees	I
cart	B
and	O
bayes	B
tree	I
for	O
example	B
now	O
output	B
class	B
probabilities	O
rather	O
than	O
classes	B
but	O
the	O
majority	O
of	O
decision	B
trees	I
in	O
this	O
project	O
do	O
not	O
do	O
so	O
and	O
in	O
any	O
case	O
it	O
is	O
by	O
no	O
means	O
true	O
that	O
this	O
artificial	O
procedure	O
is	O
in	O
fact	O
a	O
proper	O
procedure	O
at	O
all	O
consider	O
again	O
the	O
case	O
where	O
bad	O
risks	O
form	O
of	O
the	O
population	O
and	O
suppose	O
that	O
we	O
are	O
given	O
a	O
single	O
normally	O
distributed	O
variable	O
bank	O
balance	O
on	O
which	O
to	O
classify	O
for	O
simplicity	O
suppose	O
also	O
that	O
good	O
and	O
bad	O
risk	O
customers	O
differ	O
only	O
in	O
their	O
mean	O
bank	O
balance	O
when	O
trained	O
on	O
an	O
artificial	O
sample	O
with	O
equal	O
good	O
and	O
bad	O
risks	O
a	O
decision	O
tree	O
method	O
would	O
correctly	O
divide	O
the	O
population	O
into	O
two	O
regions	O
above	O
and	O
below	O
the	O
midpoint	O
between	O
the	O
two	O
mean	O
bank	O
balances	O
in	O
the	O
artificial	O
sample	O
there	O
will	O
be	O
a	O
proportion	O
say	O
of	O
good	O
examples	O
above	O
this	O
boundary	O
and	O
approximately	O
bad	O
examples	O
below	O
the	O
boundary	O
so	O
for	O
example	B
probabilities	O
as	O
if	O
a	O
potential	O
customer	O
has	O
bank	O
balance	O
above	O
this	O
boundary	O
we	O
can	O
assess	O
the	O
class	B
for	O
bad	O
no	O
matter	O
what	O
adjustment	O
is	O
made	O
for	O
the	O
true	O
prior	O
odds	B
of	O
being	O
bad	O
risk	O
it	O
is	O
clear	O
that	O
the	O
allocation	O
rule	O
can	O
only	O
take	O
one	O
of	O
two	O
forms	O
either	O
allocate	O
everyone	O
to	O
being	O
good	O
bad	O
or	O
allocate	O
good	O
or	O
bad	O
according	O
as	O
bank	O
balance	O
is	O
above	O
or	O
below	O
the	O
established	O
boundary	O
in	O
the	O
situation	O
we	O
have	O
described	O
however	O
it	O
is	O
clear	O
that	O
it	O
is	O
the	O
boundary	O
that	O
should	O
move	O
rather	O
than	O
adjust	O
the	O
probabilities	O
the	O
way	O
to	O
modify	O
the	O
procedure	O
is	O
to	O
overgrow	O
the	O
tree	O
and	O
then	O
to	O
take	O
the	O
costs	B
andor	O
priors	O
into	O
account	O
when	O
pruning	B
see	O
michie	O
attar	O
for	O
further	O
details	O
for	O
being	O
good	O
andf	O
l	O
hierarchical	O
attributes	B
it	O
often	O
happens	O
that	O
information	O
is	O
relevant	O
only	O
to	O
some	O
of	O
the	O
examples	O
for	O
example	B
certain	O
questions	O
in	O
a	O
population	O
census	O
may	O
apply	O
only	O
to	O
the	O
householder	O
or	O
certain	O
medical	O
conditions	O
apply	O
to	O
females	O
there	O
is	O
then	O
a	O
hierarchy	B
of	O
attributes	B
primary	O
variables	O
refer	O
to	O
all	O
members	O
is	O
a	O
primary	B
attribute	I
secondary	O
attributes	B
are	O
only	O
relevant	O
when	O
the	O
appropriate	O
primary	B
attribute	I
is	O
applicable	O
is	O
secondary	O
to	O
sex	O
female	O
tertiary	O
variables	O
are	O
relevant	O
when	O
a	O
secondary	O
variable	O
applies	O
of	O
pregnancy	O
is	O
tertiary	O
to	O
pregnant	O
true	O
and	O
so	O
on	O
note	O
that	O
testing	O
all	O
members	O
of	O
a	O
population	O
for	O
characteristics	O
of	O
pregnancy	O
is	O
not	O
only	O
pointless	O
but	O
wasteful	O
decision	O
tree	O
methods	O
are	O
readily	O
adapted	O
to	O
deal	O
with	O
such	O
hierarchical	O
datasets	O
and	O
the	O
algorithm	O
has	O
been	O
so	O
designed	O
and	O
not	O
for	O
others	O
obviously	O
the	O
machine	O
fault	O
dataset	O
section	O
which	O
was	O
created	O
by	O
isoft	O
is	O
an	O
example	B
of	O
a	O
hierarchical	O
dataset	O
with	O
some	O
attributes	B
being	O
present	O
for	O
one	O
subclass	O
of	O
examples	O
the	O
viewpoint	O
of	O
the	O
other	O
algorithms	O
the	O
dataset	O
is	O
unreadable	O
as	O
it	O
has	O
a	O
variable	O
number	O
of	O
attributes	B
therefore	O
an	O
alternative	O
version	O
needs	O
to	O
be	O
prepared	O
of	O
course	O
the	O
flat	O
can	O
deal	O
with	O
this	O
dataset	O
in	O
its	O
original	O
form	O
but	O
from	O
methods	O
for	O
comparison	O
form	O
has	O
lost	O
some	O
of	O
the	O
information	O
that	O
was	O
available	O
in	O
the	O
hierarchical	B
structure	I
of	O
the	O
data	O
the	O
fact	O
that	O
does	O
best	O
on	O
this	O
dataset	O
when	O
it	O
uses	O
this	O
hierarchical	O
information	O
suggests	O
that	O
the	O
hierarchical	B
structure	I
is	O
related	O
to	O
the	O
decision	B
class	B
coding	O
of	O
hierarchical	O
attributes	B
hierarchical	O
attributes	B
can	O
be	O
coded	O
into	O
flat	O
format	O
without	O
difficulty	O
in	O
that	O
a	O
one-to-one	O
correspondence	O
can	O
be	O
set	O
up	O
between	O
the	O
hierarchically	O
structured	O
data	O
and	O
the	O
flat	O
format	O
we	O
illustrate	O
the	O
procedure	O
for	O
an	O
artificial	O
example	B
consider	O
the	O
primary	B
attribute	I
sex	O
when	O
sex	O
takes	O
the	O
value	O
male	O
the	O
value	O
of	O
attribute	O
baldness	O
is	O
recorded	O
as	O
one	O
of	O
no	O
but	O
when	O
sex	O
takes	O
the	O
value	O
female	O
the	O
attribute	O
baldness	O
is	O
simply	O
not	O
applicable	O
one	O
way	O
of	O
coding	O
this	O
information	O
in	O
flat	O
format	O
is	O
to	O
give	O
two	O
attributes	B
with	O
value	O
of	O
possible	O
values	O
for	O
and	O
in	O
this	O
formulation	O
the	O
primary	O
variable	O
is	O
explicitly	O
available	O
through	O
the	O
is	O
equal	O
to	O
it	O
is	O
not	O
clear	O
whether	O
this	O
means	O
not	O
bald	O
or	O
not	O
applicable	O
strictly	O
there	O
are	O
three	O
denoting	O
sex	O
and	O
but	O
there	O
is	O
the	O
difficulty	O
here	O
not	O
too	O
serious	O
that	O
when	O
baldness	O
the	O
three	O
possible	O
triples	O
of	O
values	O
are	O
bald	O
not	O
bald	O
and	O
not	O
applicable	O
the	O
first	O
two	O
possibilities	O
applying	O
only	O
to	O
males	O
this	O
gives	O
a	O
second	O
formulation	O
in	O
which	O
the	O
two	O
attributes	B
are	O
lumped	O
together	O
into	O
a	O
single	O
attribute	O
whose	O
possible	O
values	O
represent	O
the	O
possible	O
states	O
of	O
the	O
system	O
in	O
the	O
example	B
the	O
possible	O
states	O
are	O
bald	O
male	O
not	O
bald	O
male	O
and	O
female	O
of	O
course	O
none	O
of	O
the	O
above	O
codings	O
enables	O
ordinary	O
classifiers	O
to	O
make	O
use	O
of	O
the	O
hierarchical	B
structure	I
they	O
are	O
designed	O
merely	O
to	O
represent	O
the	O
information	O
in	O
flat	O
form	O
with	O
the	O
same	O
number	O
of	O
attributes	B
per	O
example	B
breiman	O
et	O
al	O
indicate	O
how	O
hierarchical	O
attributes	B
may	O
be	O
programmed	O
into	O
a	O
tree-building	O
procedure	O
a	O
logical	O
flag	O
indicates	O
if	O
a	O
test	O
on	O
an	O
attribute	O
is	O
permissible	O
and	O
for	O
a	O
secondary	B
attribute	I
this	O
flag	O
is	O
set	O
to	O
true	O
only	O
when	O
the	O
corresponding	O
primary	B
attribute	I
has	O
already	O
been	O
tested	O
collection	O
of	O
datasets	O
for	O
the	O
most	O
part	O
when	O
data	O
are	O
gathered	O
there	O
is	O
an	O
implicit	O
understanding	O
that	O
the	O
data	O
will	O
be	O
analysed	O
by	O
a	O
certain	O
procedure	O
and	O
the	O
data-gatherer	O
usually	O
sets	O
down	O
the	O
data	O
in	O
a	O
format	O
that	O
is	O
acceptable	O
to	O
that	O
procedure	O
for	O
example	B
if	O
linear	O
discriminants	O
are	O
to	O
be	O
used	O
it	O
is	O
inappropriate	O
to	O
include	O
linear	O
combinations	O
of	O
existing	O
attributes	B
yet	O
the	O
judicious	O
use	O
of	O
sums	O
or	O
differences	O
can	O
make	O
all	O
the	O
difference	O
to	O
a	O
decision	O
tree	O
procedure	O
in	O
other	O
cases	O
the	O
data	O
may	O
have	O
some	O
additional	O
structure	O
that	O
cannot	O
be	O
incorporated	O
in	O
the	O
given	O
procedure	O
and	O
this	O
structure	O
must	O
be	O
removed	O
or	O
ignored	O
in	O
some	O
way	O
preprocessing	B
strategy	O
in	O
statlog	B
the	O
general	O
strategy	O
with	O
datasets	O
was	O
to	O
circulate	O
the	O
datasets	O
exactly	O
as	O
received	O
and	O
datasets	O
were	O
sent	O
out	O
in	O
exactly	O
the	O
same	O
format	O
as	O
they	O
came	O
in	O
for	O
these	O
datasets	O
the	O
only	O
processing	O
was	O
to	O
permute	O
the	O
order	O
of	O
the	O
examples	O
in	O
four	O
datasets	O
substantial	O
preprocessing	B
was	O
necessary	O
and	O
in	O
three	O
of	O
these	O
datasets	O
it	O
is	O
possible	O
that	O
the	O
resulting	O
dataset	O
has	O
lost	O
some	O
vital	O
information	O
or	O
has	O
been	O
biased	O
in	O
some	O
way	O
for	O
example	B
the	O
credit	B
management	I
dataset	I
was	O
processed	O
to	O
make	O
the	O
class	B
proportions	O
representative	O
another	O
source	O
of	O
potential	O
bias	B
is	O
the	O
way	O
in	O
which	O
categorical	O
attributes	B
are	O
treated	O
a	O
problem	O
that	O
is	O
most	O
acute	O
in	O
the	O
dna	B
dataset	I
review	O
of	O
previous	O
empirical	O
comparisons	O
r	O
j	O
henery	O
university	O
of	O
strathclyde	O
introduction	O
it	O
is	O
very	O
difficult	O
to	O
make	O
sense	O
of	O
the	O
multitude	O
of	O
empirical	O
comparisons	O
that	O
have	O
been	O
made	O
so	O
often	O
the	O
results	O
are	O
apparently	O
in	O
direct	O
contradiction	O
with	O
one	O
author	O
claiming	O
that	O
decision	B
trees	I
are	O
superior	O
to	O
neural	O
nets	O
and	O
another	O
making	O
the	O
opposite	O
claim	O
even	O
allowing	O
for	O
differences	O
in	O
the	O
types	O
of	O
data	O
it	O
is	O
almost	O
impossible	O
to	O
reconcile	O
the	O
various	O
claims	O
that	O
are	O
made	O
for	O
this	O
or	O
that	O
algorithm	O
as	O
being	O
faster	O
or	O
more	O
accurate	O
or	O
easier	O
than	O
some	O
other	O
algorithm	O
there	O
are	O
no	O
agreed	O
objective	O
criteria	O
by	O
which	O
to	O
judge	O
algorithms	O
and	O
in	O
any	O
case	O
subjective	O
criteria	O
such	O
as	O
how	O
easy	O
an	O
algorithm	O
is	O
to	O
program	O
or	O
run	O
are	O
also	O
very	O
important	O
when	O
a	O
potential	O
user	O
makes	O
his	O
choice	O
from	O
the	O
many	O
methods	O
available	O
nor	O
is	O
it	O
much	O
help	O
to	O
say	O
to	O
the	O
potential	O
user	O
that	O
a	O
particular	O
neural	O
network	O
say	O
is	O
better	O
for	O
a	O
particular	O
dataset	O
nor	O
are	O
the	O
labels	O
neural	O
network	O
and	O
machine	O
learning	O
particularly	O
helpful	O
either	O
as	O
there	O
are	O
different	O
types	O
of	O
algorithms	O
within	O
these	O
categories	O
what	O
is	O
required	O
is	O
some	O
way	O
of	O
categorising	O
the	O
datasets	O
into	O
types	O
with	O
a	O
statement	O
that	O
for	O
such-and-such	O
a	O
type	O
of	O
dataset	O
such-and-such	O
a	O
type	O
of	O
algorithm	O
is	O
likely	O
to	O
do	O
well	O
the	O
situation	O
is	O
made	O
more	O
difficult	O
because	O
rapid	O
advances	O
are	O
being	O
made	O
in	O
all	O
three	O
areas	O
machine	O
learning	O
neural	B
networks	I
and	O
statistics	O
so	O
many	O
comparisons	O
are	O
made	O
between	O
say	O
a	O
state-of-the-art	O
neural	O
network	O
and	O
an	O
outmoded	O
machine	O
learning	O
procedure	O
like	O
basic	O
toolbox	O
of	O
algorithms	O
before	O
discussing	O
the	O
various	O
studies	O
let	O
us	O
make	O
tentative	O
proposals	O
for	O
candidates	O
in	O
future	O
comparative	B
trials	I
i	O
e	O
let	O
us	O
say	O
what	O
in	O
our	O
opinion	O
form	O
the	O
basis	O
of	O
a	O
toolbox	O
of	O
good	O
classification	B
procedures	O
in	O
doing	O
so	O
we	O
are	O
implicitly	O
making	O
a	O
criticism	O
of	O
any	O
comparative	O
studies	O
that	O
do	O
not	O
include	O
these	O
basic	O
algorithms	O
or	O
something	O
like	O
them	O
most	O
are	O
available	O
as	O
public	O
domain	O
software	O
any	O
that	O
are	O
not	O
can	O
be	O
made	O
available	O
m	O
address	O
for	O
correspondence	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
university	O
of	O
strathclyde	O
glasgow	O
u	O
k	O
review	O
of	O
empirical	O
comparisons	O
from	O
the	O
database	O
of	O
algorithms	O
administered	O
from	O
porto	O
appendix	O
b	O
so	O
there	O
is	O
no	O
excuse	O
for	O
not	O
including	O
them	O
in	O
future	O
studies	O
we	O
should	O
probably	O
always	O
include	O
the	O
linear	B
discriminant	I
rule	O
as	O
it	O
is	O
sometimes	O
best	O
but	O
for	O
the	O
other	O
good	O
reason	O
that	O
is	O
a	O
standard	O
algorithm	O
and	O
the	O
most	O
widely	O
available	O
of	O
all	O
procedures	O
winner	O
if	O
there	O
are	O
scaling	O
problems	O
it	O
was	O
sometimes	O
outright	O
loser	O
too	O
on	O
the	O
basis	O
of	O
our	O
results	O
thed	O
neighbour	O
method	O
was	O
often	O
the	O
outright	O
so	O
it	O
would	O
seem	O
sensible	O
to	O
included	O
neighbour	O
in	O
any	O
comparative	O
studies	O
although	O
the	O
generally	O
good	O
performance	O
ofd	O
neighbour	O
is	O
well	O
known	O
it	O
is	O
in	O
many	O
cases	O
whered	O
neighbour	O
did	O
badly	O
the	O
decision-tree	O
methods	O
did	O
surprising	O
how	O
few	O
past	O
studies	O
have	O
involved	O
this	O
procedure	O
especially	O
as	O
it	O
is	O
so	O
easy	O
to	O
program	O
relatively	O
well	O
for	O
example	B
in	O
the	O
credit	B
datasets	I
so	O
some	O
kind	O
of	O
decision	O
tree	O
should	O
be	O
included	O
yet	O
again	O
some	O
of	O
the	O
newer	O
statistical	B
procedures	O
got	O
very	O
good	O
results	O
when	O
all	O
other	O
methods	O
were	O
struggling	O
so	O
we	O
would	O
also	O
recommend	O
the	O
inclusion	O
of	O
say	O
smart	B
as	O
a	O
modern	O
statistical	B
procedure	O
representing	O
neural	B
networks	I
we	O
would	O
probably	O
choose	O
lvq	B
andor	O
radial	O
basis	O
functions	O
as	O
these	O
seem	O
to	O
have	O
a	O
distinct	O
edge	O
over	O
the	O
version	O
of	O
backpropagation	O
that	O
we	O
used	O
however	O
as	O
the	O
performance	O
of	O
lvq	B
seems	O
to	O
mirror	O
that	O
of	O
k-nn	B
rather	O
closely	O
we	O
would	O
recommend	O
inclusion	O
of	O
rbf	B
rather	O
than	O
lvq	B
if	O
k-nn	B
is	O
already	O
included	O
any	O
comparative	O
study	O
that	O
does	O
not	O
include	O
the	O
majority	O
of	O
these	O
algorithms	O
is	O
clearly	O
not	O
aiming	O
to	O
be	O
complete	O
also	O
any	O
comparative	O
study	O
that	O
looks	O
at	O
only	O
two	O
procedures	O
cannot	O
give	O
reliable	O
indicators	O
of	O
performance	O
as	O
our	O
results	O
show	O
difficulties	O
in	O
previous	O
studies	O
bearing	O
in	O
mind	O
our	O
choice	O
of	O
potential	O
candidates	O
for	O
comparative	O
studies	O
it	O
will	O
quickly	O
become	O
obvious	O
that	O
most	O
previous	O
studies	O
suffer	O
from	O
the	O
major	O
disadvantage	O
that	O
their	O
choice	O
of	O
algorithms	O
is	O
too	O
narrow	O
there	O
are	O
many	O
other	O
sources	O
of	O
difficulty	O
and	O
before	O
giving	O
detailed	O
consideration	O
of	O
past	O
empirical	O
studies	O
we	O
list	O
the	O
pitfalls	O
that	O
await	O
anyone	O
carrying	O
out	O
comparative	O
studies	O
of	O
course	O
our	O
own	O
study	O
was	O
not	O
entirely	O
free	O
from	O
them	O
either	O
the	O
choice	O
of	O
algorithms	O
is	O
too	O
narrow	O
in	O
many	O
cases	O
the	O
authors	O
have	O
developed	O
their	O
own	O
pet	O
algorithm	O
and	O
are	O
expert	O
in	O
their	O
own	O
field	O
but	O
they	O
are	O
not	O
so	O
expert	O
in	O
other	O
methods	O
resulting	O
in	O
a	O
natural	O
bias	B
against	O
other	O
methods	O
the	O
chosen	O
algorithms	O
may	O
not	O
represent	O
the	O
state	O
of	O
the	O
art	O
the	O
datasets	O
are	O
usually	O
small	O
or	O
simulated	O
and	O
so	O
not	O
representative	O
of	O
real-life	O
applications	O
there	O
is	O
a	O
substantial	O
bias	B
in	O
the	O
choice	O
of	O
dataset	O
in	O
simulations	O
especially	O
giving	O
a	O
substantial	O
bias	B
in	O
favour	O
of	O
certain	O
algorithms	O
often	O
the	O
choice	O
of	O
criteria	O
is	O
biased	O
in	O
favour	O
of	O
one	O
type	O
of	O
algorithm	O
sometimes	O
even	O
using	O
unrealistic	O
cost	O
criteria	O
sec	O
previous	O
empirical	O
comparisons	O
especially	O
across	O
comparative	O
studies	O
there	O
may	O
be	O
problems	O
due	O
to	O
differences	O
in	O
the	O
way	O
the	O
data	O
were	O
pre-processed	O
for	O
example	B
by	O
removing	O
or	O
replacing	O
missing	B
values	I
or	O
transforming	O
categorical	O
to	O
numerical	O
attributes	B
the	O
class	B
definitions	I
may	O
be	O
more	O
suited	O
to	O
some	O
algorithms	O
than	O
others	O
also	O
the	O
class	B
proportions	O
in	O
the	O
training	B
set	I
may	O
well	O
differ	O
substantially	O
from	O
the	O
population	O
values	O
often	O
deliberately	O
so	O
some	O
comparative	O
studies	O
used	O
variant	O
but	O
not	O
identical	O
datasets	O
and	O
algorithms	O
we	O
have	O
attempted	O
to	O
minimise	O
the	O
above	O
problems	O
in	O
our	O
own	O
study	O
for	O
example	B
by	O
adopting	O
a	O
uniform	B
policy	O
for	O
missing	B
values	I
and	O
a	O
uniform	B
manner	O
of	O
dealing	O
with	O
categorical	B
variables	I
in	O
some	O
but	O
not	O
all	O
of	O
the	O
datasets	O
previous	O
empirical	O
comparisons	O
while	O
it	O
is	O
easy	O
to	O
criticise	O
past	O
studies	O
on	O
the	O
above	O
grounds	O
nonetheless	O
many	O
useful	O
comparative	O
studies	O
have	O
been	O
carried	O
out	O
what	O
they	O
may	O
lack	O
in	O
generality	O
they	O
may	O
gain	O
in	O
specifics	O
the	O
conclusion	O
being	O
that	O
for	O
at	O
least	O
one	O
dataset	O
algorithm	O
a	O
is	O
superior	O
or	O
more	O
accurate	O
than	O
algorithm	O
b	O
other	O
studies	O
may	O
also	O
investigate	O
other	O
aspects	O
more	O
fully	O
than	O
we	O
did	O
here	O
for	O
example	B
by	O
studying	O
learning	B
curves	I
i	O
e	O
the	O
amount	O
of	O
data	O
that	O
must	O
be	O
presented	O
to	O
an	O
algorithm	O
before	O
it	O
learns	O
something	O
useful	O
in	O
studying	O
particular	O
characteristics	O
of	O
algorithms	O
the	O
role	O
of	O
simulations	O
is	O
crucial	O
as	O
it	O
enables	O
controlled	O
departures	O
from	O
assumptions	O
giving	O
a	O
measure	B
of	O
robustness	O
etc	O
we	O
have	O
used	O
some	O
simulated	O
data	O
in	O
our	O
study	O
namely	O
the	O
belgian	O
datasets	O
this	O
was	O
done	O
because	O
we	O
believed	O
that	O
the	O
simulations	O
were	O
very	O
close	O
to	O
the	O
real-world	O
problem	O
under	O
study	O
and	O
it	O
was	O
hoped	O
that	O
our	O
trials	O
would	O
help	O
in	O
understanding	O
this	O
particular	O
problem	O
here	O
we	O
will	O
not	O
discuss	O
the	O
very	O
many	O
studies	O
that	O
concentrate	O
on	O
just	O
one	O
procedure	O
or	O
set	O
of	O
cognate	O
procedures	O
rather	O
we	O
will	O
look	O
at	O
cross-disciplinary	O
studies	O
comparing	O
algorithms	O
with	O
widely	O
differing	O
capabilities	O
among	O
the	O
former	O
however	O
we	O
may	O
mention	O
comparisons	O
of	O
symbolic	O
procedures	O
in	O
clark	O
boswell	O
sammut	O
quinlan	O
et	O
al	O
and	O
aha	O
statistical	B
procedures	O
in	O
cherkaoui	O
cleroux	O
titterington	O
et	O
al	O
and	O
remme	O
et	O
al	O
and	O
neural	B
networks	I
in	O
huang	O
et	O
al	O
fahlman	O
xu	O
et	O
al	O
and	O
ersoy	O
hong	O
several	O
studies	O
use	O
simulated	O
data	O
to	O
explore	O
various	O
aspects	O
of	O
performance	O
under	O
controlled	O
conditions	O
for	O
example	B
cherkaoui	O
cleroux	O
and	O
remme	O
et	O
al	O
individual	O
results	O
particular	O
methods	O
may	O
do	O
well	O
in	O
some	O
specific	O
domains	O
and	O
for	O
some	O
performance	O
well	O
in	O
recognising	O
handwritten	O
characters	O
and	O
but	O
not	O
as	O
well	O
on	O
the	O
sonar-target	O
task	O
sejnowski	O
measures	B
but	O
not	O
in	O
all	O
applications	O
for	O
exampled	O
neighbour	O
performed	O
very	O
machine	O
learning	O
vs	O
neural	O
network	O
with	O
the	O
recent	O
surge	O
in	O
interest	O
in	O
both	O
machine	O
learning	O
and	O
neural	B
networks	I
there	O
are	O
many	O
recent	O
studies	O
comparing	O
algorithms	O
from	O
these	O
two	O
areas	O
commonly	O
such	O
studies	O
do	O
not	O
include	O
any	O
statistical	B
algorithms	O
for	O
example	B
fisher	O
mckusick	O
and	O
shavlik	O
et	O
al	O
and	O
shavlik	O
et	O
al	O
used	O
a	O
relatively	O
old	O
symbolic	O
algorithm	O
review	O
of	O
empirical	O
comparisons	O
which	O
has	O
been	O
repeatedly	O
shown	O
to	O
be	O
less	O
effective	O
than	O
its	O
successors	O
and	O
in	O
this	O
book	O
kirkwood	O
et	O
al	O
found	O
that	O
a	O
symbolic	O
algorithm	O
performed	O
better	O
than	O
discriminant	O
analysis	O
for	O
classifying	O
the	O
gait	O
cycle	O
of	O
artificial	O
limbs	O
tsaptsinos	O
et	O
al	O
also	O
found	O
that	O
was	O
more	O
preferable	O
on	O
an	O
engineering	O
control	O
problem	O
than	O
two	O
neural	O
network	O
algorithms	O
however	O
on	O
different	O
tasks	O
other	O
researchers	O
found	O
that	O
a	O
higher	O
order	O
neural	O
network	O
performed	O
better	O
than	O
reid	O
and	O
back-propagation	O
did	O
better	O
than	O
cart	B
et	O
al	O
gorman	O
sejnowski	O
reported	O
that	O
back-propagation	O
outperformed	O
nearest	B
neighbour	I
for	O
classifying	O
sonar	O
targets	O
whereas	O
some	O
bayes	O
algorithms	O
were	O
shown	O
to	O
be	O
better	O
on	O
other	O
tasks	O
d	O
argenio	O
more	O
extensive	O
comparisons	O
have	O
also	O
been	O
carried	O
out	O
between	O
neural	O
network	O
and	O
symbolic	O
methods	O
however	O
the	O
results	O
of	O
these	O
studies	O
were	O
inconclusive	O
for	O
example	B
whereas	O
weiss	O
kulikowski	O
and	O
weiss	O
kapouleas	O
reported	O
that	O
backpropagation	O
performed	O
worse	O
than	O
symbolic	O
methods	O
cart	B
and	O
pvm	O
fisher	O
mckusick	O
and	O
shavlik	O
et	O
al	O
indicated	O
that	O
back-propagation	O
did	O
as	O
well	O
or	O
better	O
than	O
since	O
these	O
are	O
the	O
most	O
extensive	O
comparisons	O
to	O
date	O
we	O
describe	O
their	O
findings	O
briefly	O
and	O
detail	O
their	O
limitations	O
in	O
the	O
following	O
two	O
paragraphs	O
first	O
fisher	O
mckusick	O
compared	O
the	O
accuracy	B
and	O
learning	O
speed	B
the	O
number	O
of	O
example	B
presentations	O
required	O
to	O
achieve	O
asymptotic	O
accuracy	B
of	O
and	O
backpropagation	O
this	O
study	O
is	O
restricted	O
in	O
the	O
selection	O
of	O
algorithmsevaluation	O
measures	B
and	O
data	O
sets	O
whereas	O
cannot	O
tolerate	O
noise	B
several	O
descendants	O
of	O
can	O
tolerate	O
noise	B
more	O
effectively	O
example	B
quinlan	O
which	O
would	O
improve	O
their	O
performance	O
on	O
many	O
noisy	B
data	I
sets	O
furthermore	O
their	O
measure	B
of	O
speed	B
which	O
simply	O
counted	O
the	O
number	O
of	O
example	B
presentations	O
until	O
asymptotic	O
accuracy	B
was	O
attained	O
unfairly	O
favours	O
whereas	O
the	O
training	O
examples	O
need	O
be	O
given	O
to	O
only	O
once	O
they	O
were	O
repeatedly	O
presented	O
to	O
back-propagation	O
to	O
attain	O
asymptotic	O
accuracies	O
however	O
their	O
measure	B
ignored	O
that	O
back-propagation	O
s	O
cost	O
per	O
example	B
presentation	O
is	O
much	O
lower	O
than	O
s	O
this	O
measure	B
of	O
speed	B
was	O
later	O
addressed	O
in	O
fisher	O
et	O
al	O
where	O
they	O
defined	O
speed	B
as	O
the	O
product	O
of	O
total	O
example	B
presentations	O
and	O
the	O
cost	O
per	O
presentation	O
finally	O
the	O
only	O
data	O
set	O
with	O
industrial	O
ramifications	O
used	O
in	O
fisher	O
mckusick	O
is	O
the	O
garvan	O
institute	O
s	O
thyroid	O
disease	O
data	O
set	O
we	O
advocate	O
using	O
more	O
such	O
data	O
sets	O
second	O
mooney	O
et	O
al	O
and	O
shavlik	O
et	O
al	O
compared	O
similar	O
algorithms	O
on	O
a	O
larger	O
collection	B
of	I
data	I
sets	O
there	O
were	O
only	O
three	O
algorithms	O
involved	O
perceptron	B
and	O
back-propagation	O
although	O
it	O
is	O
useful	O
to	O
compare	O
the	O
relative	O
performance	O
of	O
a	O
few	O
algorithms	O
the	O
symbolic	B
learning	I
and	O
neural	O
network	O
fields	O
are	O
rapidly	O
developing	O
there	O
are	O
many	O
newer	O
algorithms	O
that	O
can	O
also	O
solve	O
classification	B
tasks	O
example	B
boswell	O
and	O
radial	O
basis	O
networks	O
girosi	O
many	O
of	O
these	O
can	O
outperform	O
the	O
algorithms	O
selected	O
here	O
thus	O
they	O
should	O
also	O
be	O
included	O
in	O
a	O
broader	O
evaluation	O
in	O
both	O
fisher	O
mckusick	O
mooney	O
et	O
al	O
and	O
shavlik	O
et	O
al	O
data	O
sets	O
were	O
separated	O
into	O
a	O
collection	O
of	O
training	O
and	O
test	O
sets	O
after	O
each	O
system	O
processed	O
a	O
training	B
set	I
its	O
performance	O
in	O
terms	O
of	O
error	B
rate	I
and	O
training	O
time	B
was	O
measured	O
on	O
the	O
corresponding	O
test	B
set	I
the	O
final	O
error	B
rate	I
was	O
the	O
geometric	O
means	O
of	O
separate	O
tests	O
mooney	O
et	O
al	O
and	O
shavlik	O
et	O
al	O
measured	O
speed	B
differently	O
from	O
fisher	O
et	O
al	O
they	O
used	O
the	O
length	O
of	O
sec	O
studies	O
involving	O
ml	O
k-nn	B
and	O
statistics	O
training	O
in	O
both	O
measures	B
mooney	O
et	O
al	O
and	O
shavlik	O
et	O
al	O
and	O
fisher	O
et	O
al	O
found	O
that	O
back-propagation	O
was	O
significantly	O
slower	O
than	O
other	O
significant	O
characteristics	O
are	O
they	O
varied	O
the	O
number	O
of	O
training	O
examples	O
and	O
studied	O
the	O
effect	O
on	O
the	O
performance	O
that	O
this	O
will	O
have	O
and	O
they	O
degenerated	O
data	O
in	O
several	O
ways	O
and	O
investigated	O
the	O
sensitivity	O
of	O
the	O
algorithms	O
to	O
the	O
quality	O
of	O
data	O
studies	O
involving	O
ml	O
k-nn	B
and	O
statistics	O
thrun	O
mitchell	O
and	O
cheng	O
conducted	O
a	O
co-ordinated	O
comparison	O
study	O
of	O
many	O
algorithms	O
on	O
the	O
monk	O
s	O
problem	O
this	O
problem	O
features	B
simulated	O
robots	O
classified	O
into	O
two	O
classes	B
using	O
six	O
attributes	B
although	O
some	O
algorithms	O
outperformed	O
others	O
there	O
was	O
no	O
apparent	O
analysis	O
of	O
the	O
results	O
this	O
study	O
is	O
of	O
limited	O
practical	O
interest	O
as	O
it	O
involved	O
simulated	O
data	O
and	O
even	O
less	O
realistically	O
was	O
capable	O
of	O
error-free	O
classification	B
other	O
small-scale	O
comparisons	O
include	O
huang	O
lippmann	O
bonelli	O
parodi	O
and	O
sethi	O
otten	O
who	O
all	O
concluded	O
that	O
the	O
various	O
neural	B
networks	I
performed	O
similarly	O
to	O
or	O
slightly	O
better	O
than	O
symbolic	O
and	O
statistical	B
algorithms	O
weiss	O
kapouleas	O
involved	O
a	O
few	O
discriminants	O
and	O
ignored	O
much	O
of	O
the	O
new	O
development	O
in	O
modern	O
statistical	B
classification	B
methods	O
ripley	O
compared	O
a	O
diverse	O
set	O
of	O
statistical	B
methods	O
neural	B
networks	I
and	O
a	O
decision	O
tree	O
classifier	B
on	O
the	O
tsetse	O
fly	O
data	O
this	O
is	O
a	O
restricted	O
comparison	O
because	O
it	O
has	O
only	O
one	O
data	O
set	O
and	O
includes	O
only	O
one	O
symbolic	O
algorithm	O
however	O
some	O
findings	O
are	O
nevertheless	O
interesting	O
in	O
accuracy	B
the	O
results	O
favoured	O
nearest	B
neighbour	I
the	O
decision	O
tree	O
algorithm	O
back-propagation	O
and	O
projection	B
pursuit	I
the	O
decision	O
tree	O
algorithm	O
rapidly	O
produced	O
most	O
interpretable	O
results	O
more	O
importantly	O
ripley	O
also	O
described	O
the	O
degree	O
of	O
frustration	O
in	O
getting	O
some	O
algorithms	O
to	O
produce	O
the	O
eventual	O
results	O
others	O
for	O
example	B
fisher	O
mckusick	O
and	O
shavlik	O
et	O
al	O
did	O
not	O
the	O
neural	B
networks	I
were	O
bad	O
in	O
this	O
respect	O
they	O
were	O
very	O
sensitive	O
to	O
various	O
system	O
settings	O
example	B
hidden	B
units	O
and	O
the	O
stopping	O
criterion	O
and	O
they	O
generally	O
converged	O
to	O
the	O
final	O
accuracies	O
slowly	O
of	O
course	O
the	O
inclusion	O
of	O
statistical	B
algorithms	O
does	O
not	O
of	O
itself	O
make	O
the	O
comparisons	O
valid	O
for	O
example	B
statisticians	O
would	O
be	O
wary	O
of	O
applying	O
a	O
bayes	O
algorithm	O
to	O
the	O
four	O
problems	O
involved	O
in	O
weiss	O
kapouleas	O
because	O
of	O
the	O
lack	O
of	O
basic	O
information	O
regarding	O
the	O
prior	O
and	O
posterior	O
probabilities	O
in	O
the	O
data	O
this	O
same	O
criticism	O
could	O
be	O
applied	O
to	O
many	O
if	O
not	O
most	O
of	O
the	O
datasets	O
in	O
common	O
use	O
the	O
class	B
proportions	O
are	O
clearly	O
unrealistic	O
and	O
as	O
a	O
result	O
it	O
is	O
difficult	O
to	O
learn	O
the	O
appropriate	O
rule	O
machine	O
learning	O
algorithms	O
in	O
particular	O
are	O
generally	O
not	O
adaptable	O
to	O
changes	O
in	O
class	B
proportions	O
although	O
it	O
would	O
be	O
straightforward	O
to	O
implement	O
this	O
some	O
empirical	O
studies	O
relating	O
to	O
credit	O
risk	O
as	O
this	O
is	O
an	O
important	O
application	O
of	O
machine	O
learning	O
methods	O
we	O
take	O
some	O
time	B
to	O
mention	O
some	O
previous	O
empirical	O
studies	O
concerning	O
credit	B
datasets	I
traditional	O
and	O
statistical	B
approaches	O
an	O
empirical	O
study	O
of	O
a	O
point	O
awarding	O
approach	O
to	O
credit	B
scoring	I
is	O
made	O
by	O
h	O
aussler	O
fahrmeir	O
et	O
al	O
compare	O
the	O
results	O
of	O
a	O
point	O
awarding	O
approach	O
with	O
the	O
results	O
obtained	O
by	O
the	O
linear	B
discriminant	I
in	O
von	O
stein	O
ziegler	O
the	O
authors	O
use	O
thed	O
neighbour	O
approach	O
to	O
analyse	O
the	O
problem	O
of	O
prognosis	O
and	O
review	O
of	O
empirical	O
comparisons	O
surveillance	O
of	O
corporate	O
credit	O
risk	O
linear	B
discriminant	I
is	O
applied	O
by	O
bretzger	O
to	O
early	O
risk	O
recognition	O
in	O
disposition	O
credits	O
in	O
a	O
comprehensive	O
study	O
of	O
corporate	O
credit	O
granting	O
reported	O
in	O
srinivisan	O
kim	O
the	O
authors	O
evaluate	O
various	O
approaches	O
including	O
parametric	O
nonparametric	O
and	O
judgemental	O
classification	B
procedures	O
within	O
the	O
nonparametric	O
approaches	O
they	O
use	O
a	O
recursive	B
partitioning	I
method	O
based	O
on	O
the	O
decision	O
tree	O
concept	B
their	O
results	O
show	O
that	O
this	O
recursive	B
partitioning	I
approach	O
performs	O
better	O
than	O
the	O
others	O
machine	O
learning	O
and	O
neural	B
networks	I
several	O
empirical	O
studies	O
deal	O
with	O
credit-scoring	O
problem	O
using	O
machine	O
learning	O
and	O
neural	B
networks	I
the	O
cart	B
method	O
et	O
al	O
is	O
used	O
by	O
hofmann	O
to	O
analyse	O
consumer	O
credit	O
granting	O
hofmann	O
concludes	O
that	O
cart	B
has	O
major	O
advantages	O
over	O
discriminant	O
analysis	O
and	O
emphasises	O
the	O
ability	O
of	O
cart	B
to	O
deal	O
with	O
mixed	O
datasets	O
containing	O
both	O
qualitative	O
and	O
quantitative	O
attributes	B
carter	O
catlett	O
use	O
machine	O
learning	O
in	O
assessing	O
credit	O
card	O
applications	O
besides	O
decision	B
trees	I
they	O
also	O
apply	O
probability	O
trees	O
produce	O
probability	O
values	O
to	O
the	O
final	O
nodes	O
of	O
the	O
tree	O
this	O
means	O
that	O
the	O
algorithm	O
is	O
able	O
decide	O
for	O
a	O
good	O
or	O
bad	O
credit	O
risk	O
with	O
a	O
certain	O
probability	O
attached	O
as	O
well	O
as	O
incorporating	O
costs	B
one	O
example	B
of	O
the	O
application	O
of	O
neural	B
networks	I
to	O
solving	O
the	O
credit	B
scoring	I
problem	O
is	O
reported	O
in	O
schumann	O
et	O
al	O
michie	O
reports	O
a	O
case	O
where	O
the	O
aim	O
of	O
the	O
credit-granting	O
procedure	O
was	O
to	O
keep	O
the	O
bad	O
debt	O
rate	O
among	O
those	O
granted	O
credit	O
down	O
to	O
while	O
some	O
procedures	O
accepted	O
only	O
of	O
applications	O
the	O
ml	O
procedure	O
was	O
able	O
to	O
double	O
the	O
proportion	O
of	O
acceptances	O
while	O
keeping	O
the	O
bad-debt	O
rate	O
within	O
bounds	O
ml	O
procedures	O
almost	O
always	O
output	B
a	O
yes-no	O
decision	O
and	O
this	O
may	O
be	O
inconvenient	O
in	O
situations	O
where	O
costs	B
may	O
vary	O
from	O
applicant	O
to	O
applicant	O
in	O
some	O
situations	O
the	O
bad-debt	O
risk	O
could	O
be	O
allowed	O
to	O
rise	O
to	O
say	O
but	O
it	O
would	O
be	O
necessary	O
to	O
re-train	O
the	O
decision	O
tree	O
using	O
a	O
different	O
pruning	B
parameter	O
dataset	O
descriptions	O
and	O
results	O
various	O
statlog	B
partners	O
see	O
appendix	O
c	O
for	O
a	O
full	O
list	O
introduction	O
we	O
group	O
the	O
dataset	O
results	O
according	O
to	O
domain	O
type	O
although	O
this	O
distinction	O
is	O
perhaps	O
arbitrary	O
at	O
times	O
there	O
are	O
three	O
credit	B
datasets	I
of	O
which	O
two	O
follow	O
in	O
the	O
next	O
section	O
the	O
third	O
dataset	O
credit	O
involved	O
a	O
cost	B
matrix	I
and	O
so	O
is	O
included	O
in	O
section	O
with	O
other	O
cost	B
matrix	I
datasets	O
several	O
of	O
the	O
datasets	O
involve	O
image	O
data	O
of	O
one	O
form	O
or	O
another	O
in	O
some	O
cases	O
we	O
are	O
attempting	O
to	O
classify	O
each	O
pixel	O
and	O
thus	O
segment	O
the	O
image	O
and	O
in	O
other	O
cases	O
we	O
need	O
to	O
classify	O
the	O
whole	O
image	O
as	O
an	O
object	O
similarly	O
the	O
data	O
may	O
be	O
of	O
raw	O
pixel	O
form	O
or	O
else	O
processed	O
data	O
these	O
datasets	O
are	O
given	O
in	O
section	O
the	O
remainder	O
of	O
the	O
datasets	O
are	O
harder	O
to	O
group	O
and	O
are	O
contained	O
in	O
section	O
see	O
the	O
appendices	O
for	O
general	O
availability	O
of	O
datasets	O
algorithms	O
and	O
related	O
software	O
the	O
tables	O
contain	O
information	O
on	O
time	B
memory	B
and	O
error	O
rates	O
for	O
the	O
training	O
and	O
test	O
sets	O
the	O
time	B
has	O
been	O
standardised	O
for	O
a	O
sun	O
ipc	O
workstation	O
at	O
specs	O
and	O
for	O
the	O
cross-validation	O
studies	O
the	O
quoted	O
times	O
are	O
the	O
average	O
for	O
each	O
cycle	O
the	O
unit	O
of	O
memory	B
is	O
the	O
maximum	O
number	O
of	O
pages	O
used	O
during	O
run	O
time	B
this	O
quantity	O
is	O
obtained	O
from	O
the	O
set	O
time	B
unix	O
command	O
and	O
includes	O
the	O
program	O
requirements	O
as	O
well	O
as	O
data	O
and	O
rules	O
stored	O
during	O
execution	O
ideally	O
we	O
would	O
like	O
to	O
decompose	O
this	O
quantity	O
into	O
memory	B
required	O
by	O
the	O
program	O
itself	O
and	O
the	O
amount	O
during	O
the	O
training	O
and	O
testing	O
phase	O
but	O
this	O
was	O
not	O
possible	O
a	O
page	O
is	O
currently	O
bytes	O
but	O
the	O
quoted	O
figures	O
are	O
considered	O
to	O
be	O
very	O
crude	O
indeed	O
both	O
time	B
and	O
memory	B
measurements	O
should	O
be	O
treated	O
with	O
great	O
caution	O
and	O
only	O
taken	O
as	O
a	O
rough	O
indication	O
of	O
the	O
truth	O
in	O
all	O
tables	O
we	O
quote	O
the	O
error	B
rate	I
for	O
the	O
default	B
rule	I
in	O
which	O
each	O
observation	O
is	O
allocated	O
to	O
the	O
most	O
common	O
class	B
in	O
addition	O
there	O
is	O
a	O
rank	O
column	O
which	O
orders	O
the	O
algorithms	O
on	O
the	O
basis	O
of	O
the	O
error	B
rate	I
for	O
the	O
test	O
data	O
note	O
however	O
that	O
this	O
is	O
not	O
the	O
only	O
measure	B
on	O
which	O
they	O
could	O
be	O
ranked	O
and	O
many	O
practitioners	O
will	O
place	O
great	O
importance	O
on	O
time	B
memory	B
or	O
interpretability	O
of	O
the	O
algorithm	O
s	O
classifying	O
rule	O
we	O
use	O
the	O
notation	O
for	O
missing	O
not	O
applicable	O
information	O
and	O
fd	O
to	O
indicate	O
that	O
m	O
address	O
for	O
correspondence	O
charles	O
taylor	O
department	O
of	O
statistics	O
university	O
of	O
leeds	O
leeds	O
u	O
k	O
dataset	O
descriptions	O
and	O
results	O
an	O
algorithm	O
failed	O
on	O
that	O
dataset	O
we	O
tried	O
to	O
determine	O
reasons	O
for	O
failure	O
but	O
with	O
little	O
success	O
in	O
most	O
cases	O
it	O
was	O
a	O
segmentation	O
violation	O
probably	O
indicating	O
a	O
lack	O
of	O
memory	B
in	O
section	O
we	O
present	O
both	O
the	O
statistical	B
and	O
information-based	B
measures	B
for	O
all	O
of	O
the	O
datasets	O
and	O
give	O
an	O
interpreation	O
for	O
a	O
few	O
of	O
the	O
datasets	O
credit	B
datasets	I
credit	B
management	I
this	O
dataset	O
was	O
donated	O
to	O
the	O
project	O
by	O
a	O
major	O
british	O
engineering	O
company	O
and	O
comes	O
from	O
the	O
general	O
area	O
of	O
credit	B
management	I
that	O
is	O
to	O
say	O
assessing	O
methods	O
for	O
pursuing	O
debt	O
recovery	O
credit	B
scoring	I
is	O
one	O
way	O
of	O
giving	O
an	O
objective	O
score	O
indicative	O
of	O
credit	O
risk	O
it	O
aims	O
to	O
give	O
a	O
numerical	O
score	O
usually	O
containing	O
components	O
from	O
various	O
factors	O
indicative	O
of	O
risk	O
by	O
which	O
an	O
objective	O
measure	B
of	O
credit	O
risk	O
can	O
be	O
obtained	O
the	O
aim	O
of	O
a	O
credit	B
scoring	I
system	O
is	O
to	O
assess	O
the	O
risk	O
associated	O
with	O
each	O
application	O
for	O
credit	O
being	O
able	O
to	O
assess	O
the	O
risk	O
enables	O
the	O
bank	O
to	O
improve	O
their	O
pricing	O
marketing	O
and	O
debt	O
recovery	O
procedures	O
inability	O
to	O
assess	O
the	O
risk	O
can	O
result	O
in	O
lost	O
business	O
it	O
is	O
also	O
important	O
to	O
assess	O
the	O
determinants	O
of	O
the	O
risk	O
lawrence	O
smith	O
state	O
that	O
payment	O
history	O
is	O
the	O
overwhelming	O
factor	O
in	O
predicting	O
the	O
likelihood	O
of	O
default	B
in	O
mobile	O
home	O
credit	O
cases	O
risk	B
assessment	I
may	O
influence	O
the	O
severity	O
with	O
which	O
bad	O
debts	O
are	O
pursued	O
although	O
it	O
might	O
be	O
thought	O
that	O
the	O
proper	O
end	O
product	O
in	O
this	O
application	O
should	O
be	O
a	O
risk	O
factor	O
or	O
probability	O
assessment	O
rather	O
than	O
a	O
yes-no	O
decision	O
the	O
dataset	O
was	O
supplied	O
with	O
pre-allocated	O
classes	B
the	O
aim	O
in	O
this	O
dataset	O
was	O
therefore	O
to	O
classify	O
customers	O
simple	O
train-and-test	B
into	O
one	O
of	O
the	O
two	O
given	O
classes	B
the	O
classes	B
can	O
be	O
interpreted	O
as	O
the	O
method	O
by	O
which	O
debts	O
will	O
be	O
retrieved	O
but	O
for	O
the	O
sake	O
of	O
brevity	O
we	O
refer	O
to	O
classes	B
as	O
good	O
and	O
bad	O
risk	O
table	O
previously	O
obtained	O
results	O
for	O
the	O
original	O
credit	B
management	I
data	O
with	O
equal	O
class	B
proportions	O
supplied	O
by	O
the	O
turing	O
institute	O
supplied	O
by	O
the	O
dataset	O
providers	O
algorithm	O
newid	B
neural	O
net	O
error	B
rate	I
the	O
original	O
dataset	O
had	O
examples	O
of	O
each	O
class	B
to	O
make	O
this	O
more	O
representative	O
of	O
the	O
population	O
as	O
a	O
whole	O
approximately	O
of	O
credit	O
applicants	O
were	O
assessed	O
by	O
a	O
human	O
as	O
bad	O
risk	O
the	O
dataset	O
used	O
in	O
the	O
project	O
had	O
examples	O
with	O
of	O
these	O
being	O
class	B
credit	O
risk	O
and	O
class	B
credit	O
risk	O
as	O
is	O
common	O
when	O
the	O
proportion	O
of	O
bad	O
credits	O
is	O
very	O
small	O
the	O
default	B
rule	I
grant	O
credit	O
to	O
all	O
applicants	O
achieves	O
a	O
small	O
error	B
rate	I
is	O
clearly	O
in	O
this	O
case	O
in	O
such	O
circumstances	O
the	O
credit-granting	O
company	O
may	O
well	O
adopt	O
the	O
default	B
strategy	O
for	O
the	O
sake	O
of	O
good	O
customer	O
relations	O
see	O
lawrence	O
smith	O
however	O
most	O
decision	O
tree	O
algorithms	O
do	O
worse	O
than	O
the	O
default	B
if	O
they	O
are	O
allowed	O
to	O
train	O
on	O
the	O
given	O
data	O
which	O
is	O
strongly	O
biased	O
towards	O
bad	O
credits	O
decision	O
tree	O
algorithms	O
have	O
an	O
error	B
rate	I
of	O
around	O
error	B
rate	I
this	O
problem	O
disappears	O
if	O
the	O
training	B
set	I
has	O
the	O
proper	O
class	B
proportions	O
for	O
example	B
a	O
version	O
of	O
cartthe	O
splus	B
module	O
tree	O
obtained	O
an	O
error	B
rate	I
sec	O
credit	O
data	O
table	O
results	O
for	O
the	O
credit	B
management	I
dataset	I
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
fd	O
test	O
fd	O
error	B
rate	I
test	O
fd	O
train	O
fd	O
rank	O
of	O
on	O
the	O
supplied	O
data	O
but	O
only	O
on	O
the	O
dataset	O
with	O
proper	O
class	B
proportions	O
whereas	O
linear	O
discriminants	O
obtained	O
an	O
error	B
rate	I
of	O
on	O
the	O
supplied	O
data	O
and	O
on	O
the	O
modified	O
proportions	O
supplier	O
of	O
the	O
credit	B
management	I
dataset	I
quotes	O
error	O
rates	O
for	O
neural	O
nets	O
and	O
decision	B
trees	I
of	O
around	O
also	O
when	O
trained	O
on	O
the	O
dataset	O
note	O
that	O
the	O
effective	O
bias	B
is	O
in	O
favour	O
of	O
the	O
non-statistical	O
algorithms	O
here	O
as	O
statistical	B
algorithms	O
can	O
cope	O
to	O
a	O
greater	O
or	O
lesser	O
extent	O
with	O
prior	O
class	B
proportions	O
that	O
differ	O
from	O
the	O
training	O
proportions	O
in	O
this	O
dataset	O
the	O
classes	B
were	O
chosen	O
by	O
an	O
expert	O
on	O
the	O
basis	O
of	O
the	O
given	O
attributes	B
below	O
and	O
it	O
is	O
hoped	O
to	O
replace	O
the	O
expert	O
by	O
an	O
algorithm	O
rule	O
in	O
the	O
future	O
all	O
attribute	O
values	O
are	O
numeric	O
the	O
dataset	O
providers	O
supplied	O
the	O
performance	O
figures	O
for	O
algorithms	O
which	O
have	O
been	O
applied	O
to	O
the	O
data	O
drawn	O
from	O
the	O
same	O
source	O
note	O
that	O
the	O
figures	O
given	O
in	O
table	O
were	O
achieved	O
using	O
the	O
original	O
dataset	O
with	O
equal	O
numbers	O
of	O
examples	O
of	O
both	O
classes	B
the	O
best	O
results	O
terms	O
of	O
error	B
rate	I
were	O
achieved	O
by	O
smart	B
and	O
the	O
tree	O
algorithms	O
and	O
smart	B
is	O
very	O
time	B
consuming	O
to	O
run	O
however	O
with	O
credit	O
type	O
datasets	O
small	O
improvements	O
in	O
accuracy	B
can	O
save	O
vast	O
amounts	O
of	O
money	O
so	O
dataset	O
descriptions	O
and	O
results	O
this	O
has	O
to	O
be	O
considered	O
if	O
sacrificing	O
accuracy	B
for	O
time	B
k-nn	B
did	O
badly	O
due	O
to	O
irrelevant	B
attributes	B
with	O
a	O
variable	O
selection	O
procedure	O
it	O
obtained	O
an	O
error	B
rate	I
of	O
castle	B
kohonen	B
itrule	B
and	O
quadisc	B
perform	O
poorly	O
result	O
for	O
quadisc	B
equalling	O
the	O
default	B
rule	I
castle	B
uses	O
only	O
attribute	O
to	O
generate	O
the	O
rule	O
concluding	O
that	O
this	O
is	O
the	O
only	O
relevant	O
attribute	O
for	O
the	O
classification	B
kohonen	B
works	O
best	O
for	O
datasets	O
with	O
equal	O
class	B
distributions	O
which	O
is	O
not	O
the	O
case	O
for	O
the	O
dataset	O
as	O
preprocessed	O
here	O
at	O
the	O
cost	O
of	O
significantly	O
increasing	O
the	O
cpu	O
time	B
the	O
performance	O
might	O
be	O
improved	O
by	O
using	O
a	O
larger	O
kohonen	B
net	O
this	O
dataset	O
and	O
the	O
best	O
result	O
for	O
the	O
decision	O
tree	O
algorithms	O
was	O
obtained	O
by	O
which	O
used	O
the	O
smallest	O
tree	O
with	O
nodes	O
used	O
nodes	O
and	O
achieved	O
a	O
similar	O
error	B
rate	I
newid	B
used	O
and	O
nodes	O
respectively	O
which	O
suggests	O
that	O
they	O
over	O
trained	O
on	O
australian	B
credit	I
table	O
results	O
for	O
the	O
australian	B
credit	I
dataset	I
classes	B
attributes	B
observations	O
cross-validation	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
fd	O
test	O
fd	O
error	B
rate	I
test	O
fd	O
train	O
fd	O
rank	O
the	O
aim	O
is	O
to	O
devise	O
a	O
rule	O
for	O
assessing	O
applications	O
for	O
credit	O
cards	O
the	O
dataset	O
has	O
been	O
studied	O
before	O
interpretation	O
of	O
the	O
results	O
is	O
made	O
difficult	O
because	O
the	O
attributes	B
and	O
classes	B
have	O
been	O
coded	O
to	O
preserve	O
confidentiality	O
however	O
sec	O
image	O
data	O
object	O
recognition	O
examples	O
of	O
likely	O
attributes	B
are	O
given	O
for	O
another	O
credit	O
data	O
set	O
in	O
section	O
for	O
our	O
purposes	O
we	O
replaced	O
the	O
missing	B
values	I
by	O
the	O
overall	O
medians	O
or	O
means	O
of	O
the	O
examples	O
had	O
some	O
missing	O
information	O
due	O
to	O
the	O
confidentiality	O
of	O
the	O
classes	B
it	O
was	O
not	O
possible	O
to	O
assess	O
the	O
relative	O
costs	B
of	O
errors	O
nor	O
to	O
assess	O
the	O
prior	O
odds	B
of	O
good	O
to	O
bad	O
customers	O
we	O
decided	O
therefore	O
to	O
use	O
the	O
default	B
cost	B
matrix	I
the	O
use	O
of	O
the	O
default	B
cost	B
matrix	I
is	O
not	O
realistic	O
in	O
practice	O
it	O
is	O
generally	O
found	O
that	O
it	O
is	O
very	O
difficult	O
to	O
beat	O
the	O
simple	O
rule	O
give	O
credit	O
if	O
only	O
if	O
the	O
applicant	O
has	O
a	O
bank	O
account	O
we	O
do	O
not	O
know	O
with	O
this	O
dataset	O
what	O
success	O
this	O
default	B
rule	I
would	O
have	O
the	O
results	O
were	O
obtained	O
by	O
cross	B
validation	I
the	O
best	O
result	O
here	O
was	O
obtained	O
by	O
which	O
used	O
only	O
an	O
average	O
of	O
less	O
than	O
and	O
newid	B
used	O
around	O
nodes	O
and	O
achieved	O
higher	O
error	O
rates	O
which	O
suggests	O
that	O
pruning	B
is	O
necessary	O
nodes	O
in	O
its	O
decision	O
tree	O
by	O
contrast	O
image	B
datasets	I
handwritten	O
digits	O
this	O
dataset	O
consists	O
of	O
examples	O
of	O
the	O
digits	O
to	O
gathered	O
from	O
postcodes	O
on	O
letters	O
in	O
germany	O
the	O
handwritten	O
examples	O
were	O
digitised	O
onto	O
images	O
with	O
pixels	O
and	O
grey	O
levels	O
they	O
were	O
read	O
by	O
one	O
of	O
the	O
automatic	O
address	O
readers	O
built	O
by	O
a	O
german	O
company	O
these	O
were	O
initially	O
scaled	O
for	O
height	O
and	O
width	O
but	O
not	O
thinned	O
or	O
rotated	O
in	O
a	O
standard	O
manner	O
an	O
example	B
of	O
each	O
digit	O
is	O
given	O
in	O
figure	O
fig	O
hand-written	B
digits	I
from	O
german	O
postcodes	O
x	O
pixels	O
the	O
dataset	O
was	O
divided	O
into	O
a	O
training	B
set	I
with	O
examples	O
per	O
digit	O
and	O
a	O
test	B
set	I
with	O
examples	O
per	O
digit	O
due	O
to	O
lack	O
of	O
memory	B
very	O
few	O
algorithms	O
could	O
cope	O
with	O
the	O
full	O
dataset	O
in	O
order	O
to	O
get	O
comparable	O
results	O
we	O
used	O
a	O
version	O
with	O
attributes	B
prepared	O
by	O
averaging	O
over	O
neighbourhoods	O
in	O
the	O
original	O
images	O
for	O
the	O
k-nn	B
classifier	B
this	O
averaging	O
resulted	O
in	O
an	O
increase	O
of	O
the	O
error	B
rate	I
from	O
to	O
whereas	O
for	O
discrim	B
the	O
error	B
rate	I
increased	O
from	O
to	O
backprop	B
could	O
also	O
cope	O
with	O
all	O
attributes	B
but	O
when	O
presented	O
with	O
all	O
examples	O
in	O
the	O
training	B
set	I
took	O
an	O
excessively	O
long	O
time	B
to	O
train	O
two	O
cpu	O
days	O
the	O
fact	O
that	O
k-nn	B
and	O
lvq	B
do	O
quite	O
well	O
is	O
probably	O
explained	O
by	O
the	O
fact	O
that	O
they	O
make	O
the	O
fewest	O
restrictive	O
assumptions	O
about	O
the	O
data	O
discriminant	O
analysis	O
on	O
the	O
other	O
hand	O
assumes	O
that	O
the	O
data	O
follows	O
a	O
multi-variate	O
normal	B
distribution	I
with	O
the	O
attributes	B
obeying	O
a	O
common	O
covariance	B
matrix	I
and	O
can	O
model	O
only	O
linear	O
aspects	O
of	O
the	O
data	O
the	O
fact	O
that	O
quadisc	B
using	O
a	O
reduced	O
version	O
of	O
the	O
dataset	O
does	O
better	O
than	O
discrim	B
using	O
either	O
the	O
full	O
version	O
or	O
reduced	O
version	O
shows	O
the	O
advantage	O
of	O
being	O
able	O
to	O
model	O
non-linearity	O
castle	B
approximates	O
the	O
data	O
by	O
a	O
polytree	O
and	O
this	O
assumption	O
is	O
too	O
restrictive	O
in	O
this	O
case	O
naive	B
bayes	I
assumes	O
the	O
attributes	B
are	O
conditionally	O
independent	O
that	O
naive	B
bayes	I
does	O
so	O
badly	O
is	O
explained	O
by	O
the	O
fact	O
that	O
the	O
attributes	B
are	O
clearly	O
not	O
conditionally	O
independent	O
since	O
neighbouring	O
pixels	O
are	O
likely	O
to	O
have	O
similar	O
grey	O
levels	O
it	O
is	O
surprising	O
that	O
cascade	B
does	O
better	O
than	O
backprop	B
and	O
this	O
may	O
be	O
attributed	O
to	O
the	O
dataset	O
descriptions	O
and	O
results	O
observations	O
table	O
results	O
for	O
the	O
digit	O
dataset	O
classes	B
attributes	B
test	O
time	B
train	O
test	O
train	O
error	B
rate	I
test	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
cascade	B
default	B
baytree	O
naivebay	O
rank	O
backprop	B
procedure	O
being	O
trapped	O
in	O
a	O
local	O
minimum	O
or	O
to	O
having	O
insufficient	O
time	B
to	O
train	O
either	O
way	O
backprop	B
should	O
really	O
do	O
better	O
here	O
and	O
one	O
suggestion	O
would	O
be	O
to	O
start	O
the	O
backprop	B
procedure	O
with	O
the	O
parameters	O
found	O
from	O
cascade	B
in	O
this	O
project	O
we	O
ran	O
all	O
algorithms	O
independently	O
without	O
reference	O
to	O
others	O
and	O
we	O
did	O
not	O
try	O
to	O
hybridise	O
or	O
run	O
procedures	O
in	O
tandem	O
although	O
there	O
is	O
no	O
doubt	O
that	O
there	O
would	O
be	O
great	O
benefit	O
from	O
pooling	O
the	O
results	O
the	O
above	O
dataset	O
is	O
close	O
to	O
raw	O
pixel	O
data	O
a	O
minimum	O
of	O
processing	O
has	O
been	O
carried	O
out	O
and	O
the	O
results	O
could	O
almost	O
certainly	O
be	O
improved	O
upon	O
using	O
deformable	O
templates	O
or	O
some	O
other	O
statistical	B
pattern	B
recognition	I
technique	O
note	O
however	O
that	O
comparison	O
of	O
performance	O
across	O
handwritten	O
digit	O
datasets	O
should	O
not	O
be	O
made	O
since	O
they	O
vary	O
widely	O
in	O
quality	O
in	O
this	O
dataset	O
only	O
zeroes	O
and	O
sevens	O
with	O
strokes	O
are	O
used	O
and	O
there	O
are	O
a	O
few	O
intentional	O
mistakes	O
for	O
example	B
a	O
digitised	O
is	O
classified	O
as	O
a	O
and	O
the	O
capital	O
b	O
is	O
classed	O
as	O
an	O
the	O
original	O
attribute	O
dataset	O
has	O
been	O
analysed	O
by	O
kressel	O
using	O
a	O
multilayer	O
perceptron	B
with	O
one	O
hidden	B
layer	O
and	O
linear	O
discriminants	O
with	O
selected	O
sec	O
image	O
data	O
object	O
recognition	O
quadratic	O
terms	O
both	O
methods	O
achieved	O
about	O
error	O
rates	O
on	O
the	O
test	B
set	I
for	O
the	O
linearquadratic	O
classifier	B
and	O
errors	O
for	O
the	O
mlp	B
hidden	B
layer	O
karhunen-loeve	B
digits	I
table	O
results	O
for	O
the	O
kl	B
digits	B
dataset	I
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
cascade	B
default	B
time	B
train	O
fd	O
fd	O
test	O
fd	O
fd	O
train	O
fd	O
error	B
rate	I
test	O
fd	O
fd	O
fd	O
rank	O
using	O
the	O
first	O
principal	O
components	O
it	O
is	O
interesting	O
that	O
with	O
the	O
exception	O
of	O
cascade	B
correlation	B
the	O
order	O
of	O
performance	O
of	O
the	O
algorithms	O
is	O
virtually	O
unchanged	O
table	O
and	O
that	O
the	O
error	O
rates	O
are	O
now	O
very	O
similar	O
to	O
those	O
obtained	O
available	O
using	O
an	O
alternative	O
data	O
reduction	O
technique	O
the	O
averaging	O
above	O
was	O
carried	O
out	O
the	O
original	O
pixels	O
the	O
results	O
for	O
the	O
digits	B
dataset	I
and	O
the	O
kl	B
digits	B
dataset	I
are	O
very	O
similar	O
so	O
are	O
treated	O
together	O
most	O
algorithms	O
perform	O
a	O
few	O
percent	O
better	O
on	O
the	O
kl	B
digits	B
dataset	I
the	O
kl	B
digits	B
dataset	I
is	O
the	O
closest	O
to	O
being	O
normal	O
this	O
could	O
be	O
predicted	O
beforehand	O
as	O
it	O
is	O
a	O
linear	B
transformation	B
of	O
the	O
attributes	B
that	O
by	O
the	O
central	O
limit	O
theorem	O
would	O
be	O
closer	O
to	O
normal	O
than	O
the	O
original	O
because	O
there	O
are	O
very	O
many	O
attributes	B
in	O
each	O
linear	O
combination	O
the	O
kl	B
digits	B
dataset	I
is	O
very	O
close	O
to	O
normal	O
kurtosis	B
as	O
against	O
the	O
exact	O
normal	O
values	O
of	O
kurtosis	B
dataset	O
descriptions	O
and	O
results	O
in	O
both	O
digits	O
datasets	O
dataset	O
k-nn	B
comes	O
top	O
and	O
rbf	B
and	O
also	O
do	O
fairly	O
well	O
in	O
fact	O
failed	O
and	O
an	O
equivalent	O
kernel	O
method	O
with	O
smoothing	B
parameter	I
asymptotically	O
chosen	O
was	O
used	O
these	O
three	O
algorithms	O
are	O
all	O
closely	O
related	O
kohonen	B
also	O
does	O
well	O
in	O
the	O
digits	B
dataset	I
for	O
some	O
reason	O
failed	O
on	O
kl	O
digits	O
kohonen	B
has	O
some	O
similarities	O
with	O
k-nn	B
type	O
algorithms	O
the	O
success	O
of	O
such	O
algorithms	O
suggests	O
that	O
the	O
attributes	B
are	O
equally	O
scaled	O
and	O
equally	O
important	O
quadisc	B
also	O
does	O
well	O
coming	O
second	O
in	O
both	O
datasets	O
the	O
kl	O
version	O
of	O
digits	O
appears	O
to	O
be	O
well	O
suited	O
to	O
quadisc	B
there	O
is	O
a	O
substantial	O
difference	O
in	O
variances	O
ratio	O
while	O
at	O
the	O
same	O
time	B
the	O
distributions	O
are	O
not	O
too	O
far	O
from	O
multivariate	B
normality	I
with	O
kurtosis	B
of	O
order	O
backprop	B
and	O
lvq	B
do	O
quite	O
well	O
on	O
the	O
digits	B
dataset	I
bearing	O
out	O
the	O
oft	O
repeated	O
claim	O
in	O
the	O
neural	O
net	O
literature	O
that	O
neural	B
networks	I
are	O
very	O
well	O
suited	O
to	O
pattern	B
recognition	I
problems	O
hecht-nelson	O
the	O
decision	O
tree	O
algorithms	O
do	O
not	O
do	O
very	O
well	O
on	O
these	O
digits	O
datasets	O
the	O
tree	O
sizes	O
are	O
typically	O
in	O
the	O
region	O
of	O
nodes	O
vehicle	B
silhouettes	O
fig	O
vehicle	B
silhouettes	O
prior	O
to	O
high	O
level	O
feature	O
extraction	O
these	O
are	O
clockwise	O
from	O
top	O
left	O
double	O
decker	O
bus	O
opel	O
manta	O
saab	O
and	O
chevrolet	O
van	O
a	O
problem	O
in	O
object	O
recognition	O
is	O
to	O
find	O
a	O
method	O
of	O
distinguishing	O
objects	O
within	O
a	O
image	O
by	O
application	O
of	O
an	O
ensemble	O
of	O
shape	O
feature	O
extractors	O
to	O
the	O
silhouettes	O
of	O
the	O
objects	O
this	O
data	O
was	O
originally	O
gathered	O
at	O
the	O
turing	O
institute	O
in	O
by	O
j	O
p	O
siebert	O
four	O
corgi	O
model	O
vehicles	O
were	O
used	O
for	O
the	O
experiment	O
a	O
double	O
decker	O
bus	O
chevrolet	O
van	O
saab	O
and	O
an	O
opel	O
manta	O
this	O
particular	O
combination	O
of	O
vehicles	O
was	O
chosen	O
with	O
the	O
expectation	O
that	O
the	O
bus	O
van	O
and	O
either	O
one	O
of	O
the	O
cars	O
would	O
be	O
readily	O
distinguishable	O
but	O
it	O
would	O
be	O
more	O
difficult	O
to	O
distinguish	O
between	O
the	O
cars	O
the	O
vehicles	O
were	O
rotated	O
and	O
a	O
number	O
of	O
image	O
silhouettes	O
were	O
obtained	O
from	O
a	O
variety	O
of	O
orientations	O
and	O
angles	O
all	O
images	O
were	O
captured	O
with	O
a	O
spatial	O
resolution	O
of	O
sec	O
image	O
data	O
object	O
recognition	O
pixels	O
quantised	O
to	O
grey	O
levels	O
these	O
images	O
were	O
cleaned	O
up	O
binarised	O
and	O
subsequently	O
processed	O
to	O
produce	O
variables	O
intended	O
to	O
characterise	O
shape	O
for	O
example	B
circularity	O
radius	O
ratio	O
compactness	O
scaled	O
variance	O
along	O
major	O
and	O
minor	O
axes	O
etc	O
a	O
total	O
of	O
examples	O
were	O
obtained	O
but	O
were	O
retained	O
in	O
case	O
of	O
dispute	O
so	O
the	O
trials	O
reported	O
here	O
used	O
only	O
examples	O
and	O
the	O
algorithms	O
were	O
run	O
using	O
cross-validation	O
to	O
obtain	O
error	O
rates	O
given	O
in	O
table	O
table	O
results	O
for	O
the	O
vehicle	B
dataset	I
classes	B
attributes	B
observations	O
cross-validation	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
cascade	B
default	B
baytree	O
naivebay	O
time	B
train	O
test	O
train	O
error	B
rate	I
test	O
rank	O
one	O
would	O
expect	O
this	O
dataset	O
to	O
be	O
non-linear	O
since	O
the	O
attributes	B
depend	O
on	O
the	O
angle	O
at	O
which	O
the	O
vehicle	B
is	O
viewed	O
therefore	O
they	O
are	O
likely	O
to	O
have	O
a	O
sinusoidal	O
dependence	O
although	O
this	O
dependence	O
was	O
masked	O
by	O
issuing	O
the	O
dataset	O
in	O
permuted	O
order	O
quadisc	B
does	O
very	O
well	O
and	O
this	O
is	O
due	O
to	O
the	O
highly	O
non-linear	O
behaviour	O
of	O
this	O
data	O
one	O
would	O
have	O
expected	O
the	O
backprop	B
algorithm	O
to	O
perform	O
well	O
on	O
this	O
dataset	O
since	O
it	O
is	O
claimed	O
backprop	B
can	O
successfully	O
model	O
the	O
non-linear	O
aspects	O
of	O
a	O
dataset	O
however	O
backprop	B
is	O
not	O
straightforward	O
to	O
run	O
unlike	O
discriminant	O
analysis	O
which	O
requires	O
no	O
choice	O
of	O
free	O
parameters	O
backprop	B
requires	O
essentially	O
two	O
free	O
parameters	O
the	O
number	O
of	O
hidden	B
dataset	O
descriptions	O
and	O
results	O
nodes	O
and	O
the	O
training	O
time	B
neither	O
of	O
these	O
is	O
straightforward	O
to	O
decide	O
this	O
figure	O
for	O
backprop	B
was	O
obtained	O
using	O
hidden	B
nodes	I
and	O
a	O
training	O
time	B
of	O
four	O
hours	O
for	O
the	O
training	O
time	B
in	O
each	O
of	O
the	O
nine	O
cycles	O
of	O
cross-validation	O
however	O
one	O
can	O
say	O
that	O
the	O
sheer	O
effort	O
and	O
time	B
taken	O
to	O
optimise	O
the	O
performance	O
for	O
backprop	B
is	O
a	O
major	O
disadvantage	O
compared	O
to	O
quadisc	B
which	O
can	O
achieve	O
a	O
much	O
better	O
result	O
with	O
a	O
lot	O
less	O
effort	O
does	O
nearly	O
as	O
well	O
as	O
quadisc	B
as	O
compared	O
with	O
backprop	B
it	O
performs	O
better	O
and	O
is	O
quicker	O
to	O
run	O
it	O
determines	O
the	O
number	O
of	O
nodes	O
neurons	B
and	O
the	O
initial	O
weights	O
by	O
a	O
reasonable	O
procedure	O
at	O
the	O
beginning	O
and	O
doesn	O
t	O
use	O
an	O
additional	O
layer	O
of	O
hidden	B
units	O
but	O
instead	O
a	O
symbolic	O
level	O
the	O
poor	O
performance	O
of	O
castle	B
is	O
explained	O
by	O
the	O
fact	O
that	O
the	O
attributes	B
are	O
highly	O
correlated	O
in	O
consequence	O
the	O
relationship	O
between	O
class	B
and	O
attributes	B
is	O
not	O
built	O
strongly	O
into	O
the	O
polytree	O
the	O
same	O
explanation	O
accounts	O
for	O
the	O
poor	O
performance	O
of	O
naive	B
bayes	I
k-nn	B
which	O
performed	O
so	O
well	O
on	O
the	O
raw	O
digits	B
dataset	I
does	O
not	O
do	O
so	O
well	O
here	O
this	O
is	O
probably	O
because	O
in	O
the	O
case	O
of	O
the	O
digits	O
the	O
attributes	B
were	O
all	O
commensurate	O
and	O
carried	O
equal	O
weight	O
in	O
the	O
vehicle	B
dataset	I
the	O
attributes	B
all	O
have	O
different	O
meanings	O
and	O
it	O
is	O
not	O
clear	O
how	O
to	O
build	O
an	O
appropriate	O
distance	B
measure	B
the	O
attributes	B
for	O
the	O
vehicle	B
dataset	I
unlike	O
the	O
other	O
image	O
analysis	O
were	O
generated	O
using	O
image	O
analysis	O
tools	O
and	O
were	O
not	O
simply	O
based	O
on	O
brightness	O
levels	O
this	O
suggests	O
that	O
the	O
attributes	B
are	O
less	O
likely	O
to	O
be	O
equally	O
scaled	O
and	O
equally	O
important	O
this	O
is	O
confirmed	O
by	O
the	O
lower	O
performances	O
of	O
k-nn	B
lvq	B
and	O
radial	O
basis	O
functions	O
which	O
treat	O
all	O
attributes	B
equally	O
and	O
have	O
a	O
built	O
in	O
mechanism	O
for	O
normalising	O
which	O
is	O
often	O
not	O
optimal	O
did	O
not	O
perform	O
well	O
here	O
and	O
so	O
an	O
alternative	O
kernel	O
method	O
was	O
used	O
which	O
allowed	O
for	O
correlations	O
between	O
the	O
attributes	B
and	O
this	O
appeared	O
to	O
be	O
more	O
robust	O
than	O
the	O
other	O
three	O
algorithms	O
although	O
it	O
still	O
fails	O
to	O
learn	O
the	O
difference	O
between	O
the	O
cars	O
the	O
original	O
siebert	O
paper	O
showed	O
machine	O
learning	O
performing	O
better	O
and	O
were	O
and	O
nodes	O
respectively	O
than	O
k-nn	B
but	O
there	O
is	O
not	O
much	O
support	O
for	O
this	O
in	O
our	O
results	O
the	O
tree	O
sizes	O
for	O
the	O
high	O
value	O
of	O
table	O
might	O
indicate	O
that	O
linear	B
discrimination	B
could	O
be	O
based	O
on	O
just	O
two	O
discriminants	O
this	O
may	O
relate	O
to	O
the	O
fact	O
that	O
the	O
two	O
cars	O
are	O
not	O
easily	O
distinguishable	O
so	O
might	O
be	O
treated	O
as	O
one	O
dimensionality	O
of	O
the	O
mean	O
vectors	O
to	O
however	O
although	O
the	O
fraction	O
of	O
discriminating	O
power	O
for	O
the	O
third	O
discriminant	O
is	O
low	O
it	O
is	O
still	O
statistically	O
significant	O
so	O
cannot	O
be	O
discarded	O
without	O
a	O
small	O
loss	O
of	O
discrimination	B
letter	B
recognition	I
the	O
dataset	O
was	O
constructed	O
by	O
david	O
j	O
slate	O
odesta	O
corporation	O
evanston	O
il	O
the	O
objective	O
here	O
is	O
to	O
classify	O
each	O
of	O
a	O
large	O
number	O
of	O
black	O
and	O
white	O
rectangular	O
pixel	O
displays	O
as	O
one	O
of	O
the	O
capital	O
letters	O
of	O
the	O
english	O
alphabet	O
train	O
and	O
test	O
was	O
used	O
for	O
the	O
classification	B
the	O
character	O
images	O
produced	O
were	O
based	O
on	O
different	O
fonts	O
and	O
each	O
letter	O
within	O
these	O
fonts	O
was	O
randomly	O
distorted	O
to	O
produce	O
a	O
file	O
of	O
unique	O
images	O
for	O
each	O
image	O
numerical	O
attributes	B
were	O
calculated	O
using	O
edge	O
counts	O
and	O
measures	B
of	O
statistical	B
moments	O
which	O
were	O
scaled	O
and	O
discretised	O
into	O
a	O
range	O
of	O
integer	O
values	O
from	O
to	O
perfect	O
classification	B
performance	O
is	O
unlikely	O
to	O
be	O
possible	O
with	O
this	O
dataset	O
one	O
of	O
the	O
fonts	O
used	O
gothic	O
roman	O
appears	O
very	O
different	O
from	O
the	O
others	O
sec	O
image	O
data	O
object	O
recognition	O
table	O
results	O
for	O
the	O
letters	B
dataset	I
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
fd	O
test	O
fd	O
error	B
rate	I
test	O
fd	O
train	O
fd	O
rank	O
quadisc	B
is	O
the	O
best	O
of	O
the	O
classical	O
statistical	B
algorithms	O
on	O
this	O
dataset	O
this	O
is	O
perhaps	O
not	O
surprising	O
since	O
the	O
measures	B
data	O
gives	O
some	O
support	O
to	O
the	O
assumptions	O
underlying	O
the	O
method	O
discrim	B
does	O
not	O
perform	O
well	O
although	O
the	O
logistic	O
version	O
is	O
a	O
significant	O
improvement	O
smart	B
is	O
used	O
here	O
with	O
a	O
term	O
model	O
and	O
its	O
poor	O
performance	O
is	O
surprising	O
a	O
number	O
of	O
the	O
attributes	B
are	O
non	O
linear	O
combinations	O
of	O
some	O
others	O
and	O
smart	B
might	O
have	O
been	O
expected	O
to	O
model	O
this	O
well	O
achieves	O
the	O
best	O
performance	O
of	O
all	O
with	O
k-nn	B
close	O
behind	O
in	O
this	O
dataset	O
all	O
the	O
attributes	B
are	O
pre	O
scaled	O
and	O
all	O
appear	O
to	O
be	O
important	O
so	O
good	O
performance	O
from	O
k-nn	B
is	O
to	O
be	O
expected	O
castle	B
constructs	O
a	O
polytree	O
with	O
only	O
one	O
attribute	O
contributing	O
to	O
the	O
classification	B
which	O
is	O
too	O
restrictive	O
with	O
this	O
dataset	O
naive	B
bayes	I
assumes	O
conditional	O
independence	O
and	O
this	O
is	O
certainly	O
not	O
satisfied	O
for	O
a	O
number	O
of	O
the	O
attributes	B
newid	B
and	O
on	O
examples	O
drawn	O
from	O
the	O
full	O
training	B
set	I
and	O
that	O
in	O
part	O
explains	O
their	O
rather	O
uninspiring	O
performance	O
newid	B
builds	O
a	O
huge	O
tree	O
containing	O
over	O
nodes	O
while	O
the	O
tree	O
is	O
about	O
half	O
the	O
size	O
this	O
difference	O
probably	O
explains	O
some	O
of	O
the	O
difference	O
in	O
their	O
respective	O
results	O
and	O
also	O
build	O
complex	B
trees	O
while	O
generates	O
rules	O
in	O
order	O
to	O
classify	O
the	O
training	B
set	I
itrule	B
is	O
the	O
poorest	O
algorithm	O
on	O
this	O
dataset	O
were	O
only	O
trained	O
dataset	O
descriptions	O
and	O
results	O
generally	O
we	O
would	O
not	O
expect	O
itrule	B
to	O
perform	O
well	O
on	O
datasets	O
where	O
many	O
of	O
the	O
attributes	B
contributed	O
to	O
the	O
classification	B
as	O
it	O
is	O
severely	O
constrained	O
in	O
the	O
complexity	O
of	O
the	O
rules	O
it	O
can	O
construct	O
of	O
the	O
neural	O
network	O
algorithms	O
kohonen	B
and	O
lvq	B
would	O
be	O
expected	O
to	O
perform	O
well	O
for	O
the	O
same	O
reasons	O
as	O
k-nn	B
seen	O
in	O
that	O
light	O
the	O
kohonen	B
result	O
is	O
a	O
little	O
disappointing	O
in	O
a	O
previous	O
study	O
frey	O
slate	O
investigated	O
the	O
use	O
of	O
an	O
adaptive	O
classifier	B
system	O
and	O
achieved	O
a	O
best	O
error	B
rate	I
of	O
just	O
under	O
rank	O
chromosomes	B
table	O
results	O
for	O
the	O
chromosome	B
dataset	I
classes	B
attributes	B
test	O
observations	O
max	O
algorithm	O
storage	B
fd	O
fd	O
time	B
train	O
fd	O
fd	O
test	O
fd	O
fd	O
error	B
rate	I
test	O
fd	O
fd	O
train	O
fd	O
fd	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
this	O
data	O
was	O
obtained	O
via	O
the	O
mrc	O
human	O
genetics	O
unit	O
edinburgh	O
from	O
the	O
routine	O
amniotic	O
cell	O
data	O
set	O
c	O
lundsteen	O
righospitalet	O
copenhagen	O
in	O
our	O
trials	O
we	O
used	O
only	O
features	B
examples	O
which	O
are	O
a	O
subset	O
of	O
a	O
larger	O
database	O
which	O
has	O
features	B
and	O
nearly	O
examples	O
the	O
subset	O
was	O
selected	O
to	O
reduce	O
the	O
scale	O
of	O
the	O
problem	O
and	O
selecting	O
the	O
features	B
defined	O
as	O
level	O
directly	O
from	O
the	O
chromosome	O
image	O
and	O
level	O
requiring	O
the	O
axis	O
e	O
g	O
length	O
to	O
be	O
specified	O
we	O
omitted	O
observations	O
with	O
an	O
unknown	O
class	B
as	O
well	O
as	O
features	B
with	O
level	O
both	O
axis	O
and	O
profile	O
and	O
knowledge	O
of	O
the	O
chromosome	O
polarity	O
sec	O
image	O
data	O
segmentation	O
and	O
level	O
both	O
the	O
axis	O
and	O
both	O
the	O
polarity	O
and	O
the	O
centrometre	O
location	O
classification	B
was	O
done	O
using	O
one-shot	O
train-and-test	B
the	O
result	O
for	O
is	O
very	O
poor	O
and	O
the	O
reason	O
for	O
this	O
is	O
not	O
clear	O
an	O
alternative	O
kernel	O
classifier	B
a	O
cauchy	O
kernel	O
to	O
avoid	O
numerical	O
difficulties	O
gave	O
an	O
error	B
rate	I
of	O
which	O
is	O
much	O
better	O
although	O
quadratic	B
discriminants	I
do	O
best	O
here	O
there	O
is	O
reason	O
to	O
believe	O
that	O
its	O
error	B
rate	I
is	O
perhaps	O
not	O
optimal	O
as	O
there	O
is	O
clear	O
evidence	O
of	O
non-normality	O
in	O
the	O
distribution	O
of	O
the	O
attributes	B
the	O
best	O
of	O
decision	O
tree	O
results	O
is	O
obtained	O
by	O
which	O
has	O
rules	O
and	O
by	O
contrast	O
newid	B
has	O
terminal	O
nodes	O
but	O
does	O
about	O
as	O
well	O
as	O
have	O
and	O
terminal	O
nodes	O
respectively	O
and	O
yet	O
obtain	O
very	O
differnt	O
error	O
rates	O
further	O
details	O
of	O
this	O
dataset	O
can	O
be	O
found	O
in	O
piper	O
granum	O
who	O
have	O
done	O
extensive	O
experiments	O
on	O
selection	O
and	O
measurement	O
of	O
variables	O
for	O
the	O
dataset	O
which	O
resembled	O
the	O
one	O
above	O
most	O
closely	O
they	O
achieved	O
an	O
error	B
rate	I
of	O
landsat	O
satellite	B
image	I
the	O
original	O
landsat	O
data	O
for	O
this	O
database	O
was	O
generated	O
from	O
data	O
purchased	O
from	O
nasa	O
by	O
the	O
australian	O
centre	O
for	O
remote	O
sensing	O
and	O
used	O
for	O
research	O
at	O
the	O
university	O
of	O
new	O
south	O
wales	O
the	O
sample	O
database	O
was	O
generated	O
taking	O
a	O
small	O
section	O
rows	O
and	O
columns	O
from	O
the	O
original	O
data	O
the	O
classification	B
for	O
each	O
pixel	O
was	O
performed	O
on	O
the	O
basis	O
of	O
an	O
actual	O
site	O
visit	O
by	O
ms	O
karen	O
hall	O
when	O
working	O
for	O
professor	O
john	O
a	O
richards	O
at	O
the	O
centre	O
for	O
remote	O
sensing	O
the	O
database	O
is	O
a	O
sub-area	O
of	O
a	O
scene	O
consisting	O
of	O
pixels	O
each	O
pixel	O
covering	O
an	O
area	O
on	O
the	O
ground	O
of	O
approximately	O
metres	O
the	O
information	O
given	O
for	O
each	O
pixel	O
consists	O
of	O
the	O
class	B
value	O
and	O
the	O
intensities	O
in	O
four	O
spectral	O
bands	O
the	O
green	O
red	O
and	O
infra-red	O
regions	O
of	O
the	O
spectrum	O
the	O
original	O
data	O
are	O
presented	O
graphically	O
in	O
figure	O
the	O
first	O
four	O
plots	O
row	O
and	O
bottom	O
left	O
show	O
the	O
intensities	O
in	O
four	O
spectral	O
bands	O
spectral	O
bands	O
and	O
are	O
in	O
the	O
green	O
and	O
red	O
regions	O
of	O
the	O
visible	O
spectrum	O
while	O
spectral	O
bands	O
and	O
are	O
in	O
the	O
infra-red	O
shadings	O
represent	O
greatest	O
intensity	O
the	O
middle	O
bottom	O
diagram	O
shows	O
the	O
land	O
use	O
with	O
shadings	O
representing	O
the	O
seven	O
original	O
classes	B
in	O
the	O
order	O
red	O
soil	O
cotton	O
crop	O
vegetation	O
stubble	O
mixture	O
types	O
present	O
grey	O
soil	O
damp	O
grey	O
soil	O
and	O
very	O
damp	O
grey	O
soil	O
with	O
red	O
as	O
lightest	O
and	O
very	O
damp	O
grey	O
as	O
darkest	O
shading	O
also	O
shown	O
right	O
are	O
the	O
classes	B
as	O
predicted	O
by	O
linear	O
discriminants	O
note	O
that	O
the	O
most	O
accurate	O
predictions	O
are	O
for	O
cotton	O
crop	O
region	O
bottom	O
left	O
of	O
picture	O
and	O
that	O
the	O
predicted	O
boundary	O
damp-vary	O
damp	O
grey	O
soil	O
top	O
left	O
of	O
picture	O
is	O
not	O
well	O
positioned	O
so	O
that	O
information	O
from	O
the	O
neighbourhood	O
of	O
a	O
pixel	O
might	O
contribute	O
to	O
the	O
classification	B
of	O
that	O
pixel	O
the	O
spectra	O
of	O
the	O
eight	O
neighbours	O
of	O
a	O
pixel	O
were	O
included	O
as	O
attributes	B
together	O
with	O
the	O
four	O
spectra	O
of	O
that	O
pixel	O
each	O
line	O
of	O
data	O
corresponds	O
to	O
a	O
square	O
neighbourhood	O
of	O
pixels	O
completely	O
contained	O
within	O
the	O
sub-area	O
thus	O
each	O
line	O
contains	O
the	O
four	O
spectral	O
bands	O
of	O
each	O
of	O
the	O
pixels	O
in	O
the	O
neighbourhood	O
and	O
the	O
class	B
of	O
the	O
central	O
pixel	O
which	O
was	O
one	O
of	O
red	O
soil	O
cotton	O
crop	O
grey	O
soil	O
damp	O
grey	O
soil	O
soil	O
with	O
vegetation	O
stubble	O
very	O
damp	O
grey	O
soil	O
the	O
mixed-pixels	O
of	O
which	O
there	O
were	O
were	O
removed	O
for	O
our	O
purposes	O
so	O
that	O
there	O
are	O
only	O
six	O
classes	B
in	O
this	O
dataset	O
the	O
examples	O
were	O
randomised	O
and	O
certain	O
lines	O
were	O
deleted	O
so	O
that	O
simple	O
reconstruction	O
of	O
the	O
original	O
image	O
was	O
not	O
possible	O
the	O
data	O
were	O
divided	O
into	O
a	O
train	O
set	O
and	O
dataset	O
descriptions	O
and	O
results	O
spectral	O
band	O
spectral	O
band	O
spectral	O
band	O
spectral	O
band	O
land	O
use	O
land	O
use	O
fig	O
satellite	B
image	I
dataset	I
spectral	O
band	O
intensities	O
as	O
seen	O
from	O
a	O
satellite	O
for	O
a	O
small	O
km	O
region	O
of	O
australia	O
also	O
given	O
are	O
the	O
actual	O
land	O
use	O
as	O
determined	O
by	O
on-site	O
visit	O
and	O
the	O
estimated	O
classes	B
as	O
given	O
by	O
linear	O
discriminants	O
a	O
test	B
set	I
with	O
examples	O
in	O
the	O
train	O
set	O
and	O
in	O
the	O
test	B
set	I
and	O
the	O
error	O
rates	O
are	O
given	O
in	O
table	O
in	O
the	O
satellite	B
image	I
dataset	I
k-nn	B
performs	O
best	O
not	O
surprisingly	O
radial	O
basis	O
functions	O
lvq	B
and	O
also	O
do	O
fairly	O
well	O
as	O
these	O
three	O
algorithms	O
are	O
closely	O
related	O
fact	O
failed	O
on	O
this	O
dataset	O
so	O
an	O
equivalent	O
method	O
using	O
an	O
asymptotically	O
chosen	O
bandwidth	O
was	O
used	O
their	O
success	O
suggests	O
that	O
all	O
the	O
attributes	B
are	O
equally	O
scaled	O
and	O
equally	O
important	O
there	O
appears	O
to	O
be	O
little	O
to	O
choose	O
between	O
any	O
of	O
the	O
other	O
algorithms	O
except	O
that	O
naive	B
bayes	I
does	O
badly	O
its	O
close	O
relative	O
castle	B
also	O
does	O
relatively	O
badly	O
the	O
decision	O
tree	O
algorithms	O
perform	O
at	O
about	O
the	O
same	O
level	O
with	O
cart	B
giving	O
the	O
used	O
trees	O
with	O
and	O
nodes	O
respectively	O
which	O
suggests	O
more	O
pruning	B
is	O
desired	O
for	O
these	O
algorithms	O
best	O
result	O
using	O
nodes	O
and	O
this	O
dataset	O
has	O
the	O
highest	O
correlation	B
between	O
attributes	B
this	O
may	O
partly	O
explain	O
the	O
failure	O
of	O
naive	B
bayes	I
attributes	B
are	O
conditionally	O
independent	O
and	O
castle	B
if	O
several	O
attributes	B
contain	O
equal	O
amounts	O
of	O
information	O
note	O
that	O
only	O
three	O
canonical	B
discriminants	I
are	O
sufficient	O
to	O
separate	O
all	O
six	O
class	B
means	O
sec	O
image	O
data	O
segmentation	O
table	O
results	O
for	O
the	O
satellite	B
image	I
dataset	I
classes	B
attributes	B
test	O
observations	O
algorithm	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
cascade	B
default	B
maximum	O
storage	B
fd	O
time	B
train	O
fd	O
test	O
fd	O
train	O
error	B
rate	I
test	O
fd	O
fd	O
rank	O
this	O
may	O
be	O
interpreted	O
as	O
evidence	O
of	O
seriation	O
with	O
the	O
three	O
classes	B
grey	O
soil	O
damp	O
grey	O
soil	O
and	O
very	O
damp	O
grey	O
soil	O
forming	O
a	O
continuum	O
equally	O
this	O
result	O
can	O
be	O
interpreted	O
as	O
indicating	O
that	O
the	O
original	O
four	O
attributes	B
may	O
be	O
successfully	O
reduced	O
to	O
three	O
with	O
no	O
loss	O
of	O
information	O
here	O
information	O
should	O
be	O
interpreted	O
as	O
mean	O
square	O
distance	B
between	O
classes	B
or	O
equivalently	O
as	O
the	O
entropy	B
of	O
a	O
normal	B
distribution	I
the	O
examples	O
were	O
created	O
using	O
a	O
neighbourhood	O
so	O
it	O
is	O
no	O
surprise	O
that	O
there	O
is	O
a	O
very	O
large	O
correlation	B
amongst	O
the	O
variables	O
the	O
results	O
from	O
castle	B
suggest	O
that	O
only	O
three	O
of	O
the	O
variables	O
for	O
the	O
centre	O
pixel	O
are	O
necessary	O
to	O
classify	O
the	O
observation	O
however	O
other	O
algorithms	O
found	O
a	O
significant	O
improvement	O
when	O
information	O
from	O
the	O
neighbouring	O
pixels	O
was	O
used	O
image	B
segmentation	I
the	O
instances	O
were	O
drawn	O
randomly	O
from	O
a	O
database	O
of	O
outdoor	O
colour	O
images	O
these	O
were	O
hand	O
segmented	O
to	O
create	O
a	O
classification	B
for	O
every	O
pixel	O
as	O
one	O
of	O
brickface	O
sky	O
region	O
for	O
example	B
summary	O
measures	B
of	O
contrast	O
in	O
the	O
vertical	O
and	O
horizontal	O
directions	O
foliage	O
cement	O
window	O
path	O
grass	O
there	O
were	O
attributes	B
appropriate	O
for	O
each	O
dataset	O
descriptions	O
and	O
results	O
table	O
results	O
for	O
the	O
image	B
segmentation	B
dataset	I
classes	B
attributes	B
observations	O
cross-validation	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
test	O
error	B
rate	I
test	O
train	O
rank	O
average	O
error	O
rates	O
were	O
obtained	O
via	O
cross-validation	O
and	O
are	O
given	O
in	O
table	O
did	O
very	O
well	O
here	O
and	O
used	O
an	O
average	O
of	O
nodes	O
in	O
its	O
decision	B
trees	I
it	O
is	O
interesting	O
here	O
that	O
does	O
so	O
much	O
better	O
than	O
k-nn	B
the	O
reason	O
for	O
this	O
is	O
that	O
has	O
a	O
variable	O
selection	O
option	O
which	O
was	O
initially	O
run	O
on	O
the	O
data	O
and	O
only	O
of	O
the	O
original	O
attributes	B
were	O
finally	O
used	O
when	O
variables	O
were	O
used	O
the	O
error	B
rate	I
increased	O
to	O
indeed	O
a	O
similar	O
attribute	O
selection	O
procedure	O
increased	O
the	O
performance	O
of	O
k-nn	B
to	O
a	O
very	O
similar	O
error	B
rate	I
this	O
discrepancy	O
raises	O
the	O
whole	O
issue	O
of	O
preprocessing	B
the	O
data	O
before	O
algorithms	O
are	O
run	O
and	O
the	O
substantial	O
difference	O
this	O
can	O
make	O
it	O
is	O
clear	O
that	O
there	O
will	O
still	O
be	O
a	O
place	O
for	O
intelligent	O
analysis	O
alongside	O
any	O
black-box	O
techniques	O
for	O
quite	O
some	O
time	B
cut	B
this	O
dataset	O
was	O
supplied	O
by	O
a	O
statlog	B
partner	O
for	O
whom	O
it	O
is	O
commercially	O
confidential	O
the	O
dataset	O
was	O
constructed	O
during	O
an	O
investigation	O
into	O
the	O
problem	O
of	O
segmenting	O
individual	O
characters	O
from	O
joined	O
written	O
text	O
figure	O
shows	O
an	O
example	B
of	O
the	O
word	O
eins	O
for	O
one	O
each	O
example	B
consists	O
of	O
a	O
number	O
of	O
measurements	O
made	O
on	O
the	O
text	O
relative	O
to	O
a	O
potential	O
cut	B
point	O
along	O
with	O
a	O
decision	O
on	O
whether	O
to	O
cut	B
the	O
text	O
at	O
that	O
sec	O
image	O
data	O
segmentation	O
fig	O
the	O
german	O
word	O
eins	O
with	O
an	O
indication	O
of	O
where	O
it	O
should	O
be	O
cut	B
to	O
separate	O
the	O
individual	O
letters	O
point	O
or	O
not	O
as	O
supplied	O
the	O
dataset	O
contained	O
examples	O
with	O
real	O
valued	O
attributes	B
in	O
an	O
attempt	O
to	O
assess	O
the	O
performance	O
of	O
algorithms	O
relative	O
to	O
the	O
dimensionality	O
of	O
the	O
problem	O
a	O
second	O
dataset	O
was	O
constructed	O
from	O
the	O
original	O
using	O
the	O
best	O
attributes	B
selected	O
by	O
stepwise	O
regression	O
on	O
the	O
whole	O
dataset	O
this	O
was	O
the	O
only	O
processing	O
carried	O
out	O
on	O
this	O
dataset	O
the	O
original	O
and	O
reduced	O
datasets	O
were	O
tested	O
in	O
both	O
cases	O
training	O
sets	O
of	O
examples	O
and	O
test	O
sets	O
of	O
were	O
used	O
in	O
a	O
single	O
train-and-test	B
procedure	O
to	O
assess	O
accuracy	B
although	O
individual	O
results	O
differ	O
between	O
the	O
datasets	O
the	O
ranking	O
of	O
methods	O
is	O
broadly	O
the	O
same	O
and	O
so	O
we	O
shall	O
consider	O
all	O
the	O
results	O
together	O
the	O
default	B
rule	I
in	O
both	O
cases	O
would	O
give	O
an	O
error	B
rate	I
of	O
around	O
but	O
since	O
kohonen	B
the	O
only	O
unsupervised	O
method	O
in	O
the	O
project	O
achieves	O
an	O
error	B
rate	I
of	O
for	O
both	O
datasets	O
it	O
seems	O
reasonable	O
to	O
choose	O
this	O
value	O
as	O
our	O
performance	O
threshold	O
this	O
is	O
a	O
dataset	O
on	O
which	O
k	B
nearest	B
neighbour	I
might	O
be	O
expected	O
to	O
do	O
well	O
all	O
attributes	B
are	O
continuous	O
with	O
little	O
correlation	B
and	O
this	O
proves	O
to	O
be	O
the	O
case	O
indeed	O
with	O
a	O
variable	O
selection	O
option	O
k-nn	B
obtained	O
an	O
error	B
rate	I
of	O
only	O
conversely	O
the	O
fact	O
that	O
k-nn	B
does	O
well	O
indicates	O
that	O
many	O
variables	O
contribute	O
to	O
the	O
classification	B
approaches	O
k-nn	B
performance	O
by	O
undersmoothing	O
leading	O
to	O
overfitting	B
on	O
the	O
training	B
set	I
while	O
this	O
may	O
prove	O
to	O
be	O
an	O
effective	O
strategy	O
with	O
large	O
and	O
representative	O
training	O
sets	O
it	O
is	O
not	O
recommended	O
in	O
general	O
quadisc	B
castle	B
and	O
naive	B
bayes	I
perform	O
poorly	O
on	O
both	O
datasets	O
because	O
in	O
each	O
case	O
assumptions	O
underlying	O
the	O
method	O
do	O
not	O
match	O
the	O
data	O
quadisc	B
assumes	O
multi	O
variate	O
normality	O
and	O
unequal	O
covariance	B
matrices	O
and	O
neither	O
of	O
these	O
assumptions	O
is	O
supported	O
by	O
the	O
data	O
measures	B
castle	B
achieves	O
default	B
performance	O
using	O
only	O
one	O
variable	O
in	O
line	O
with	O
the	O
assumption	O
implicit	O
in	O
the	O
method	O
that	O
only	O
a	O
small	O
number	O
of	O
variables	O
will	O
determine	O
the	O
class	B
naive	B
bayes	I
assumes	O
conditional	O
independence	O
amongst	O
the	O
attributes	B
and	O
this	O
is	O
unlikely	O
to	O
hold	O
for	O
a	O
dataset	O
of	O
this	O
type	O
machine	O
learning	O
algorithms	O
generally	O
perform	O
well	O
although	O
with	O
wide	O
variation	O
in	O
tree	O
sizes	O
baytree	O
and	O
indcart	B
achieve	O
low	O
error	O
rates	O
at	O
the	O
expense	O
of	O
building	O
trees	O
containing	O
more	O
than	O
nodes	O
performs	O
almost	O
as	O
well	O
though	O
building	O
a	O
tree	O
containing	O
terminal	O
nodes	O
produces	O
a	O
very	O
parsimonious	O
tree	O
containing	O
only	O
nodes	O
for	O
the	O
dataset	O
which	O
is	O
very	O
easy	O
to	O
understand	O
and	O
newid	B
build	O
trees	O
dataset	O
descriptions	O
and	O
results	O
table	O
comparative	O
results	O
for	O
the	O
dataset	O
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
fd	O
test	O
fd	O
error	B
rate	I
test	O
fd	O
train	O
fd	O
rank	O
with	O
and	O
nodes	O
respectively	O
itrule	B
like	O
castle	B
cannot	O
deal	O
with	O
continuous	O
attributes	B
directly	O
and	O
also	O
discretises	O
such	O
variables	O
before	O
processing	O
the	O
major	O
reason	O
for	O
poor	O
performance	O
though	O
is	O
that	O
tests	O
were	O
restricted	O
to	O
conjunctions	O
of	O
up	O
to	O
two	O
attributes	B
which	O
tested	O
conjunctions	O
of	O
up	O
to	O
attributes	B
achieved	O
a	O
much	O
better	O
error	B
rate	I
subsample	O
could	O
not	O
handle	O
the	O
full	O
dataset	O
and	O
the	O
results	O
reported	O
are	O
for	O
a	O
it	O
is	O
interesting	O
that	O
almost	O
all	O
algorithms	O
achieve	O
a	O
better	O
result	O
on	O
than	O
this	O
suggests	O
that	O
the	O
attributes	B
excluded	O
from	O
the	O
reduced	O
dataset	O
contain	O
significant	O
discriminatory	O
power	O
achieves	O
its	O
better	O
performance	O
by	O
building	O
a	O
tree	O
five	O
times	O
larger	O
than	O
that	O
for	O
newid	B
and	O
and	O
nodes	O
and	O
classify	O
more	O
accurately	O
with	O
them	O
uses	O
a	O
tree	O
with	O
nodes	O
with	O
a	O
slight	O
improvement	O
in	O
accuracy	B
similarly	O
discovers	O
a	O
smaller	O
set	O
of	O
rules	O
for	O
which	O
deliver	O
improved	O
performance	O
this	O
general	O
improvement	O
in	O
performance	O
underlines	O
the	O
observation	O
that	O
what	O
is	O
best	O
or	O
optimal	O
in	O
linear	B
regression	I
terms	O
may	O
not	O
be	O
best	O
for	O
other	O
algorithms	O
both	O
build	O
significantly	O
smaller	O
trees	O
sec	O
cost	B
datasets	I
table	O
results	O
for	O
the	O
dataset	O
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
fd	O
test	O
fd	O
train	O
fd	O
error	B
rate	I
test	O
fd	O
rank	O
datasets	O
with	O
costs	B
the	O
following	O
three	O
datasets	O
were	O
all	O
tackled	O
using	O
cross-validation	O
the	O
error	O
rates	O
that	O
have	O
been	O
used	O
as	O
a	O
measure	B
of	O
performance	O
are	O
now	O
replaced	O
by	O
average	O
costs	B
per	O
observation	O
over	O
all	O
cycles	O
in	O
cross-validation	O
the	O
average	O
cost	O
is	O
obtained	O
for	O
all	O
algorithms	O
by	O
multiplying	O
the	O
confusion	O
matrix	O
by	O
the	O
cost	B
matrix	I
summing	O
the	O
entries	O
and	O
dividing	O
by	O
the	O
number	O
of	O
observations	O
in	O
the	O
case	O
of	O
a	O
cost	B
matrix	I
in	O
which	O
all	O
errors	O
have	O
unit	O
cost	O
normally	O
referred	O
to	O
as	O
no	O
cost	B
matrix	I
this	O
measure	B
of	O
average	O
cost	O
is	O
the	O
same	O
as	O
the	O
error	O
rates	O
quoted	O
previously	O
note	O
that	O
some	O
algorithms	O
did	O
not	O
implement	O
the	O
cost	B
matrix	I
although	O
in	O
principle	O
this	O
would	O
be	O
straightforward	O
however	O
we	O
still	O
include	O
all	O
of	O
the	O
algorithms	O
in	O
the	O
tables	O
partly	O
for	O
completeness	O
but	O
primarily	O
to	O
show	O
the	O
effect	O
of	O
ignoring	O
the	O
cost	B
matrix	I
in	O
general	O
those	O
algorithms	O
which	O
do	O
worse	O
than	O
the	O
default	B
rule	I
are	O
those	O
which	O
do	O
not	O
incorporate	O
costs	B
into	O
the	O
decision	O
making	O
process	O
head	B
injury	I
the	O
data	O
set	O
is	O
a	O
series	O
of	O
patients	O
with	O
severe	O
head	B
injury	I
collected	O
prospectively	O
by	O
neurosurgeons	O
between	O
and	O
this	O
head	B
injury	I
study	O
was	O
initiated	O
in	O
the	O
institute	O
dataset	O
descriptions	O
and	O
results	O
of	O
neurological	O
sciences	O
glasgow	O
after	O
years	O
netherlands	O
centres	O
and	O
groningen	O
joined	O
the	O
study	O
and	O
late	O
data	O
came	O
also	O
from	O
los	O
angeles	O
the	O
details	O
of	O
the	O
data	O
collection	O
are	O
given	O
in	O
jennet	O
et	O
al	O
the	O
original	O
purpose	O
of	O
the	O
head	B
injury	I
study	O
was	O
to	O
investigate	O
the	O
feasibility	O
of	O
predicting	O
the	O
degree	O
of	O
recovery	O
which	O
individual	O
patients	O
would	O
attain	O
using	O
data	O
collected	O
shortly	O
after	O
injury	O
severely	O
head	O
injured	O
patients	O
require	O
intensive	O
and	O
expensive	O
treatment	O
even	O
with	O
such	O
care	O
almost	O
half	O
of	O
them	O
die	O
and	O
some	O
survivors	O
remain	O
seriously	O
disabled	O
for	O
life	O
clinicians	O
are	O
concerned	O
to	O
recognise	O
which	O
patients	O
have	O
potential	O
for	O
recovery	O
so	O
as	O
to	O
concentrate	O
their	O
endeavours	O
on	O
them	O
outcome	O
was	O
categorised	O
according	O
to	O
the	O
glasgow	O
outcome	O
scale	O
but	O
the	O
five	O
categories	O
described	O
therein	O
were	O
reduced	O
to	O
three	O
for	O
the	O
purpose	O
of	O
prediction	O
these	O
were	O
dv	O
dead	O
or	O
vegetative	O
sev	O
severe	O
disability	O
mg	O
moderate	O
disability	O
or	O
good	O
recovery	O
table	O
gives	O
the	O
different	O
cost	O
of	O
various	O
possible	O
misclassifications	O
table	O
misclassification	B
costs	B
for	O
the	O
head	B
injury	I
dataset	I
the	O
column	O
represents	O
the	O
predicted	O
class	B
and	O
the	O
row	O
the	O
true	O
class	B
dv	O
sev	O
mg	O
dv	O
sev	O
mg	O
the	O
dataset	O
had	O
a	O
very	O
large	O
number	O
of	O
missing	B
values	I
for	O
patients	O
and	O
these	O
were	O
replaced	O
with	O
the	O
median	O
value	O
for	O
the	O
appropriate	O
class	B
this	O
makes	O
our	O
version	O
of	O
the	O
data	O
considerably	O
easier	O
for	O
classification	B
than	O
the	O
original	O
data	O
and	O
has	O
the	O
merit	O
that	O
all	O
procedures	O
can	O
be	O
applied	O
to	O
the	O
same	O
dataset	O
but	O
has	O
the	O
disadvantage	O
that	O
the	O
resulting	O
rules	O
are	O
unrealistic	O
in	O
that	O
this	O
replacement	O
strategy	O
is	O
not	O
possible	O
for	O
real	O
data	O
of	O
unknown	O
class	B
nine	O
fold	O
cross-validation	O
was	O
used	O
to	O
estimate	O
the	O
average	O
misclassification	O
cost	O
the	O
predictive	O
variables	O
are	O
age	O
and	O
various	O
indicators	O
of	O
the	O
brain	O
damage	O
as	O
reflected	O
in	O
brain	O
dysfunction	O
these	O
are	O
listed	O
below	O
indicators	O
of	O
brain	O
dysfunction	O
can	O
vary	O
considerably	O
during	O
the	O
few	O
days	O
after	O
injury	O
measurements	O
were	O
therefore	O
taken	O
frequently	O
and	O
for	O
each	O
indicant	O
the	O
best	O
and	O
worst	O
states	O
during	O
each	O
of	O
a	O
number	O
of	O
successive	O
time	B
periods	O
were	O
recorded	O
the	O
data	O
supplied	O
were	O
based	O
on	O
the	O
best	O
state	O
during	O
the	O
first	O
hours	O
after	O
the	O
onset	O
of	O
coma	O
the	O
emv	O
score	O
in	O
the	O
table	O
is	O
known	O
in	O
the	O
medical	O
literature	O
as	O
the	O
glasgow	O
coma	O
scale	O
d	O
d	O
i	O
the	O
sum	O
of	O
e	O
m	O
and	O
v	O
scores	O
i	O
e	O
emv	O
score	O
i	O
e	O
age	O
grouped	O
into	O
decades	O
eye	O
opening	O
in	O
response	O
to	O
stimulation	O
motor	O
response	O
of	O
best	O
limb	O
in	O
response	O
to	O
stimulation	O
verbal	O
response	O
to	O
stimulation	O
motor	O
response	O
pattern	O
an	O
overall	O
summary	O
of	O
the	O
motor	O
responses	O
in	O
all	O
four	O
limbs	O
change	O
in	O
neurological	O
function	O
over	O
the	O
first	O
hours	O
f	O
f	O
sec	O
cost	B
datasets	I
eye	O
indicant	O
a	O
summary	O
of	O
sem	O
ocs	O
and	O
ovs	O
i	O
e	O
spontaneous	O
eye	O
movements	O
oculocephalics	O
oculovestibulars	O
pupil	O
reaction	O
to	O
light	O
table	O
results	O
for	O
the	O
head	B
injury	I
dataset	I
classes	B
attributes	B
observations	O
cross-validation	O
algorithms	O
in	O
italics	O
have	O
not	O
incorporated	O
costs	B
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
cascade	B
default	B
baytree	O
naivebay	O
ac	O
time	B
train	O
test	O
average	O
costs	B
test	O
train	O
rank	O
smart	B
and	O
are	O
the	O
only	O
algorithms	O
that	O
as	O
standard	O
can	O
utilise	O
costs	B
directly	O
in	O
the	O
training	O
phase	O
used	O
in	O
our	O
results	O
a	O
modified	O
version	O
of	O
backprop	B
that	O
could	O
utilise	O
costs	B
but	O
this	O
is	O
very	O
experimental	O
however	O
although	O
these	O
two	O
algorithms	O
do	O
reasonably	O
well	O
they	O
are	O
not	O
the	O
best	O
logistic	O
regression	O
does	O
very	O
well	O
and	O
so	O
do	O
discrim	B
and	O
quadisc	B
cart	B
indcart	B
bayes	B
tree	I
and	O
are	O
the	O
only	O
decision	B
trees	I
that	O
used	O
a	O
cost	B
matrix	I
here	O
and	O
hence	O
the	O
others	O
have	O
performed	O
worse	O
than	O
the	O
default	B
rule	I
cart	B
and	O
nodes	O
however	O
using	O
error	B
rate	I
as	O
a	O
criterion	O
we	O
cannot	O
judge	O
whether	O
these	O
algorithms	O
both	O
had	O
trees	O
of	O
around	O
nodes	O
whereas	O
and	O
newid	B
both	O
had	O
around	O
dataset	O
descriptions	O
and	O
results	O
were	O
under-pruning	O
since	O
no	O
cost	B
matrix	I
was	O
used	O
in	O
the	O
classifier	B
but	O
for	O
interpretability	O
the	O
smaller	O
trees	O
are	O
preferred	O
titterington	O
et	O
al	O
compared	O
several	O
discrimination	B
procedures	O
on	O
this	O
data	O
our	O
dataset	O
differs	O
by	O
replacing	O
all	O
missing	B
values	I
with	O
the	O
class	B
median	O
and	O
so	O
the	O
results	O
are	O
not	O
directly	O
comparable	O
heart	B
disease	I
table	O
results	O
for	O
the	O
heart	B
disease	I
dataset	O
classes	B
attributes	B
observations	O
cross-validation	O
algorithms	O
in	O
italics	O
have	O
not	O
incorporated	O
costs	B
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
cascade	B
default	B
baytree	O
naivebay	O
ac	O
time	B
test	O
train	O
average	O
costs	B
test	O
train	O
rank	O
this	O
database	O
comes	O
from	O
the	O
cleveland	O
clinic	O
foundation	O
and	O
was	O
supplied	O
by	O
robert	O
detrano	O
m	O
d	O
ph	O
d	O
of	O
the	O
v	O
a	O
medical	O
center	O
long	O
beach	O
ca	O
it	O
is	O
part	O
of	O
the	O
collection	O
of	O
databases	O
at	O
the	O
university	O
of	O
california	O
irvine	O
collated	O
by	O
david	O
aha	O
the	O
purpose	O
of	O
the	O
dataset	O
is	O
to	O
predict	O
the	O
presence	O
or	O
absence	O
of	O
heart	B
disease	I
given	O
the	O
results	O
of	O
various	O
medical	O
tests	O
carried	O
out	O
on	O
a	O
patient	O
this	O
database	O
contains	O
attributes	B
which	O
have	O
been	O
extracted	O
from	O
a	O
larger	O
set	O
of	O
the	O
database	O
originally	O
contained	O
examples	O
but	O
of	O
these	O
contained	O
missing	O
class	B
values	O
and	O
so	O
were	O
discarded	O
leaving	O
of	O
these	O
were	O
retained	O
in	O
case	O
of	O
dispute	O
leaving	O
a	O
final	O
total	O
of	O
there	O
are	O
two	O
classes	B
presenceand	O
absenceof	O
heart-disease	O
this	O
is	O
a	O
reduction	O
of	O
the	O
number	O
of	O
classes	B
sec	O
cost	B
datasets	I
in	O
the	O
original	O
dataset	O
in	O
which	O
there	O
were	O
four	O
different	O
degrees	O
of	O
heart-disease	O
table	O
gives	O
the	O
different	O
costs	B
of	O
the	O
possible	O
misclassifications	O
nine	O
fold	O
cross-validation	O
was	O
used	O
to	O
estimate	O
the	O
average	O
misclassification	O
cost	O
naive	B
bayes	I
performed	O
best	O
on	O
the	O
heart	B
dataset	I
this	O
may	O
reflect	O
the	O
careful	O
selection	O
of	O
attributes	B
by	O
the	O
doctors	O
of	O
the	O
decision	B
trees	I
cart	B
and	O
performed	O
the	O
best	O
tuned	O
the	O
pruning	B
parameter	O
and	O
not	O
take	O
the	O
cost	B
matrix	I
into	O
account	O
so	O
the	O
prefered	O
pruning	B
is	O
still	O
an	O
open	O
question	O
used	O
an	O
average	O
of	O
nodes	O
in	O
the	O
trees	O
whereas	O
this	O
data	O
has	O
been	O
studied	O
in	O
the	O
literature	O
before	O
but	O
without	O
taking	O
any	O
cost	B
matrix	I
into	O
account	O
and	O
so	O
the	O
results	O
are	O
not	O
comparable	O
with	O
those	O
obtained	O
here	O
table	O
misclassification	B
costs	B
for	O
the	O
heart	B
disease	I
dataset	O
the	O
columns	O
represent	O
the	O
predicted	O
class	B
and	O
the	O
rows	O
the	O
true	O
class	B
used	O
nodes	O
however	O
did	O
absent	O
present	O
absent	O
present	O
german	B
credit	I
table	O
cost	B
matrix	I
for	O
the	O
german	B
credit	I
dataset	I
the	O
columns	O
are	O
the	O
predicted	O
class	B
and	O
the	O
rows	O
the	O
true	O
class	B
good	O
bad	O
good	O
bad	O
the	O
original	O
dataset	O
by	O
professor	O
dr	O
hans	O
hofmann	O
universit	O
at	O
hamburg	O
contained	O
some	O
categoricalsymbolic	O
attributes	B
for	O
algorithms	O
that	O
required	O
numerical	O
attributes	B
a	O
version	O
was	O
produced	O
with	O
several	O
indicator	B
variables	I
added	O
the	O
attributes	B
that	O
were	O
ordered	O
categorical	O
were	O
coded	O
as	O
integer	O
this	O
preprocessed	O
dataset	O
had	O
numerical	O
attributes	B
and	O
cross-validation	O
was	O
used	O
for	O
the	O
classification	B
and	O
for	O
uniformity	O
all	O
algorithms	O
used	O
this	O
preprocessed	O
version	O
it	O
is	O
of	O
interest	O
that	O
newid	B
did	O
the	O
trials	O
with	O
both	O
the	O
preprocessed	O
version	O
and	O
the	O
original	O
data	O
and	O
obtained	O
nearly	O
identical	O
error	O
rates	O
and	O
but	O
rather	O
different	O
tree	O
sizes	O
and	O
nodes	O
the	O
attributes	B
of	O
the	O
original	O
dataset	O
include	O
status	O
of	O
existing	O
current	O
account	O
duration	O
of	O
current	O
account	O
credit	O
history	O
reason	O
for	O
loan	O
request	O
new	O
car	O
furniture	O
credit	O
amount	O
savings	O
accountbonds	O
length	O
of	O
employment	O
installment	O
rate	O
in	O
percentage	O
of	O
disposable	O
income	O
marital	O
status	O
and	O
sex	O
length	O
of	O
time	B
at	O
presentresidence	O
age	O
and	O
job	O
dataset	O
descriptions	O
and	O
results	O
results	O
are	O
given	O
in	O
table	O
the	O
providers	O
of	O
this	O
dataset	O
suggest	O
the	O
cost	B
matrix	I
of	O
table	O
it	O
is	O
interesting	O
that	O
only	O
algorithms	O
do	O
better	O
than	O
the	O
default	B
the	O
results	O
clearly	O
demonstrate	O
that	O
some	O
decision	O
tree	O
algorithms	O
are	O
at	O
a	O
disadvantage	O
when	O
costs	B
are	O
taken	O
into	O
account	O
that	O
it	O
is	O
possible	O
to	O
include	O
costs	B
into	O
decision	B
trees	I
is	O
demonstrated	O
by	O
the	O
good	O
results	O
of	O
and	O
cart	B
et	O
al	O
achieved	O
a	O
good	O
result	O
with	O
an	O
average	O
of	O
only	O
nodes	O
which	O
would	O
lead	O
to	O
very	O
transparent	O
rules	O
of	O
those	O
algorithms	O
that	O
did	O
not	O
include	O
costs	B
used	O
a	O
tree	O
with	O
nodes	O
an	O
error	O
rates	O
of	O
and	O
respectively	O
table	O
results	O
for	O
the	O
german	B
credit	I
dataset	I
classes	B
attributes	B
observations	O
cross-validation	O
algorithms	O
in	O
italics	O
have	O
not	O
incorporated	O
costs	B
and	O
newid	B
used	O
an	O
average	O
of	O
over	O
nodes	O
error	B
rate	I
of	O
whereas	O
algorithm	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
ac	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
maximum	O
storage	B
time	B
train	O
test	O
average	O
costs	B
test	O
train	O
rank	O
other	O
datasets	O
this	O
section	O
contains	O
rather	O
a	O
mixed	O
bag	O
of	O
datasets	O
mostly	O
of	O
an	O
industrial	O
application	O
shuttle	B
control	I
the	O
dataset	O
was	O
provided	O
by	O
jason	O
catlett	O
who	O
was	O
then	O
at	O
the	O
basser	O
department	O
of	O
computer	O
science	O
university	O
of	O
sydney	O
n	O
s	O
w	O
australia	O
the	O
data	O
originated	O
from	O
nasa	O
and	O
concern	O
the	O
position	O
of	O
radiators	O
within	O
the	O
space	O
shuttle	B
the	O
problem	O
sec	O
miscellaneous	O
data	O
appears	O
to	O
be	O
noise-free	O
in	O
the	O
sense	O
that	O
arbitrarily	O
small	O
error	O
rates	O
are	O
possible	O
given	O
sufficient	O
data	O
rad	O
flow	O
rad	O
flow	O
high	O
x	O
rad	O
flow	O
high	O
fig	O
shuttle	B
data	O
attributes	B
and	O
for	O
the	O
two	O
classes	B
rad	O
flow	O
and	O
high	O
only	O
the	O
symbols	O
and	O
denote	O
the	O
state	O
rad	O
flow	O
and	O
high	O
respectively	O
the	O
examples	O
are	O
classified	O
correctly	O
by	O
the	O
decision	O
tree	O
in	O
the	O
right	O
diagram	O
the	O
data	O
was	O
divided	O
into	O
a	O
train	O
set	O
and	O
a	O
test	B
set	I
with	O
examples	O
in	O
the	O
train	O
set	O
and	O
in	O
the	O
test	B
set	I
a	O
single	O
train-and-test	B
was	O
used	O
to	O
calculate	O
the	O
accuracy	B
with	O
samples	O
of	O
this	O
size	O
it	O
should	O
be	O
possible	O
to	O
obtain	O
an	O
accuracy	B
of	O
approximately	O
of	O
the	O
data	O
belong	O
to	O
class	B
at	O
the	O
other	O
extreme	O
there	O
are	O
only	O
examples	O
of	O
class	B
in	O
the	O
learning	O
set	O
the	O
shuttle	B
dataset	I
also	O
departs	O
widely	O
from	O
typical	O
distribution	O
assumptions	O
the	O
attributes	B
are	O
numerical	O
and	O
appear	O
to	O
exhibit	O
multimodality	B
do	O
not	O
have	O
a	O
good	O
statistical	B
test	O
to	O
measure	B
this	O
some	O
feeling	O
for	O
this	O
dataset	O
can	O
be	O
gained	O
by	O
looking	O
at	O
figure	O
it	O
shows	O
that	O
a	O
rectangular	O
box	O
sides	O
parallel	O
to	O
the	O
axes	O
may	O
be	O
drawn	O
to	O
enclose	O
all	O
examples	O
in	O
the	O
class	B
high	O
although	O
the	O
lower	O
boundary	O
of	O
this	O
box	O
less	O
than	O
is	O
so	O
close	O
to	O
examples	O
of	O
class	B
rad	O
flow	O
that	O
this	O
particular	O
boundary	O
cannot	O
be	O
clearly	O
marked	O
to	O
the	O
scale	O
of	O
figure	O
in	O
the	O
whole	O
dataset	O
the	O
data	O
seem	O
to	O
consist	O
of	O
isolated	O
islands	O
or	O
clusters	O
of	O
points	O
each	O
of	O
which	O
is	O
pure	O
to	O
only	O
one	O
class	B
with	O
one	O
class	B
comprising	O
several	O
such	O
islands	O
however	O
neighbouring	O
islands	O
may	O
be	O
very	O
close	O
and	O
yet	O
come	O
from	O
different	O
populations	O
the	O
boundaries	O
of	O
the	O
islands	O
seem	O
to	O
be	O
parallel	O
with	O
the	O
coordinate	O
axes	O
if	O
this	O
picture	O
is	O
correct	O
and	O
the	O
present	O
data	O
do	O
not	O
contradict	O
it	O
as	O
it	O
is	O
possible	O
to	O
classify	O
the	O
combined	O
dataset	O
with	O
accuracy	B
using	O
a	O
decision	O
tree	O
then	O
it	O
is	O
of	O
interest	O
to	O
ask	O
which	O
of	O
our	O
algorithms	O
are	O
guaranteed	O
to	O
arrive	O
at	O
the	O
correct	O
classification	B
given	O
an	O
arbitrarily	O
large	O
learning	O
dataset	O
in	O
the	O
following	O
we	O
ignore	O
practical	O
matters	O
such	O
as	O
training	O
times	O
storage	B
requirements	O
etc	O
and	O
concentrate	O
on	O
the	O
limiting	O
behaviour	O
for	O
an	O
infinitely	O
large	O
training	B
set	I
dataset	O
descriptions	O
and	O
results	O
table	O
results	O
for	O
the	O
shuttle	B
dataset	I
with	O
error	O
rates	O
are	O
in	O
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
fd	O
test	O
fd	O
error	O
train	O
fd	O
test	O
rank	O
fd	O
procedures	O
which	O
might	O
therefore	O
be	O
expected	O
to	O
find	O
a	O
perfect	O
rule	O
for	O
this	O
dataset	O
would	O
seem	O
to	O
be	O
k-nn	B
backprop	B
and	O
failed	O
here	O
and	O
the	O
result	O
obtained	O
by	O
another	O
kernel	O
method	O
a	O
sphered	O
transformation	B
of	O
the	O
data	O
was	O
far	O
from	O
perfect	O
rbf	B
should	O
also	O
be	O
capable	O
of	O
perfect	O
accuracy	B
but	O
some	O
changes	O
would	O
be	O
required	O
in	O
the	O
particular	O
implementation	O
used	O
in	O
the	O
project	O
avoid	O
singularities	O
using	O
a	O
variable	O
selection	O
method	O
of	O
the	O
attributes	B
k-nn	B
achieved	O
an	O
error	B
rate	I
of	O
decision	B
trees	I
will	O
also	O
find	O
the	O
perfect	O
rule	O
provided	O
that	O
the	O
pruning	B
parameter	O
is	O
properly	O
set	O
but	O
may	O
not	O
do	O
so	O
under	O
all	O
circumstances	O
as	O
it	O
is	O
occasionally	O
necessary	O
to	O
override	O
the	O
splitting	B
criterion	I
et	O
al	O
although	O
a	O
machine	O
learning	O
procedure	O
may	O
find	O
a	O
decision	O
tree	O
which	O
classifies	O
perfectly	O
it	O
may	O
not	O
find	O
the	O
simplest	O
representation	O
the	O
tree	O
of	O
figure	O
which	O
was	O
produced	O
by	O
the	O
splus	B
procedure	O
tree	O
gets	O
accuracy	B
with	O
five	O
terminal	O
nodes	O
whereas	O
it	O
is	O
easy	O
to	O
construct	O
an	O
equivalent	O
tree	O
with	O
only	O
three	O
terminal	O
nodes	O
that	O
the	O
same	O
structure	O
occurs	O
in	O
both	O
halves	O
of	O
the	O
tree	O
in	O
figure	O
it	O
is	O
possible	O
to	O
classify	O
the	O
full	O
examples	O
with	O
only	O
errors	O
using	O
a	O
linear	B
decision	I
tree	I
with	O
nine	O
terminal	O
nodes	O
since	O
there	O
are	O
seven	O
classes	B
this	O
sec	O
miscellaneous	O
data	O
is	O
a	O
remarkably	O
simple	O
tree	O
this	O
suggests	O
that	O
the	O
data	O
have	O
been	O
generated	O
by	O
a	O
process	O
that	O
is	O
governed	O
by	O
a	O
linear	B
decision	I
tree	I
that	O
is	O
a	O
decision	O
tree	O
in	O
which	O
tests	O
are	O
applied	O
sequentially	O
the	O
result	O
of	O
each	O
test	O
being	O
to	O
allocate	O
one	O
section	O
of	O
the	O
data	O
to	O
one	O
class	B
and	O
to	O
apply	O
subsequent	O
tests	O
to	O
the	O
remaining	O
section	O
as	O
there	O
are	O
very	O
few	O
examples	O
of	O
class	B
in	O
the	O
whole	O
dataset	O
it	O
would	O
require	O
enormous	O
amounts	O
of	O
data	O
to	O
construct	O
reliable	O
classifiers	O
for	O
class	B
the	O
actual	O
trees	O
produced	O
by	O
the	O
algorithms	O
are	O
rather	O
small	O
as	O
expected	O
diabetes	B
has	O
nodes	O
and	O
both	O
and	O
cart	B
have	O
nodes	O
this	O
dataset	O
was	O
originally	O
donated	O
by	O
vincent	O
sigillito	O
applied	O
physics	O
laboratory	O
johns	O
hopkins	O
university	O
laurel	O
md	O
and	O
was	O
constructed	O
by	O
constrained	O
selection	O
from	O
a	O
larger	O
database	O
held	O
by	O
the	O
national	O
institute	O
of	O
diabetes	B
and	O
digestive	O
and	O
kidney	O
diseases	O
it	O
is	O
publicly	O
available	O
from	O
the	O
machine	O
learning	O
database	O
at	O
uci	O
appendix	O
a	O
all	O
patients	O
represented	O
in	O
this	O
dataset	O
are	O
females	O
at	O
least	O
years	O
old	O
of	O
pima	O
indian	O
heritage	O
living	O
near	O
phoenix	O
arizona	O
usa	O
the	O
problem	O
posed	O
here	O
is	O
to	O
predict	O
whether	O
a	O
patient	O
would	O
test	O
positive	O
for	O
diabetes	B
according	O
to	O
world	O
health	O
organization	O
criteria	O
if	O
the	O
patients	O
hour	O
post	O
load	O
plasma	O
glucose	O
is	O
at	O
least	O
mgdl	O
given	O
a	O
number	O
of	O
physiological	O
measurements	O
and	O
medical	O
test	O
results	O
the	O
attribute	O
details	O
are	O
given	O
below	O
number	O
of	O
times	O
pregnant	O
plasma	O
glucose	O
concentration	O
in	O
an	O
oral	O
glucose	O
tolerance	O
test	O
diastolic	O
blood	O
pressure	O
mmhg	O
triceps	O
skin	O
fold	O
thickness	O
mm	O
serum	O
insulin	O
mu	O
uml	O
body	O
mass	O
index	O
kgm	O
diabetes	B
pedigree	O
function	O
age	O
years	O
this	O
is	O
a	O
two	O
class	B
problem	O
with	O
class	B
value	O
being	O
interpreted	O
as	O
tested	O
positive	O
for	O
diabetes	B
there	O
are	O
examples	O
of	O
class	B
and	O
of	O
class	B
twelve	O
fold	O
cross	B
validation	I
was	O
used	O
to	O
estimate	O
prediction	O
accuracy	B
the	O
dataset	O
is	O
rather	O
difficult	O
to	O
classify	O
the	O
so-called	O
class	B
value	O
is	O
really	O
a	O
binarised	O
form	O
of	O
another	O
attribute	O
which	O
is	O
itself	O
highly	O
indicative	O
of	O
certain	O
types	O
of	O
diabetes	B
but	O
does	O
not	O
have	O
a	O
one	O
to	O
one	O
correspondence	O
with	O
the	O
medical	O
condition	O
of	O
being	O
diabetic	O
no	O
algorithm	O
performs	O
exceptionally	O
well	O
although	O
and	O
k-nn	B
seem	O
to	O
be	O
the	O
poorest	O
automatic	O
smoothing	B
parameter	I
selection	O
in	O
can	O
make	O
poor	O
choices	O
for	O
datasets	O
with	O
discrete	O
valued	O
attributes	B
and	O
k-nn	B
can	O
have	O
problems	O
scaling	O
such	O
datasets	O
overall	O
though	O
it	O
seems	O
reasonable	O
to	O
conclude	O
that	O
the	O
attributes	B
do	O
not	O
predict	O
the	O
class	B
well	O
uses	O
only	O
nodes	O
in	O
its	O
decision	O
tree	O
whereas	O
newid	B
which	O
performs	O
less	O
well	O
has	O
nodes	O
generates	O
rules	O
although	O
there	O
is	O
not	O
very	O
much	O
difference	O
in	O
the	O
error	O
rates	O
here	O
this	O
dataset	O
has	O
been	O
studied	O
by	O
smith	O
et	O
al	O
using	O
the	O
adap	O
algorithm	O
using	O
examples	O
as	O
a	O
training	B
set	I
adap	O
achieved	O
an	O
error	B
rate	I
of	O
on	O
the	O
remaining	O
instances	O
and	O
have	O
and	O
nodes	O
repectively	O
and	O
dataset	O
descriptions	O
and	O
results	O
table	O
results	O
for	O
the	O
diabetes	B
dataset	I
classes	B
attributes	B
observations	O
cross-validation	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
test	O
error	B
rate	I
test	O
train	O
rank	O
dna	B
this	O
classification	B
problem	O
is	O
drawn	O
from	O
the	O
field	O
of	O
molecular	O
biology	O
splice	O
junctions	O
are	O
points	O
on	O
a	O
dna	B
sequence	O
at	O
which	O
superfluous	O
dna	B
is	O
removed	O
during	O
protein	O
creation	O
the	O
problem	O
posed	O
here	O
is	O
to	O
recognise	O
given	O
a	O
sequence	O
of	O
dna	B
the	O
boundaries	O
between	O
exons	O
parts	O
of	O
the	O
dna	B
sequence	O
retained	O
after	O
splicing	O
and	O
introns	O
parts	O
of	O
the	O
dna	B
that	O
are	O
spliced	O
out	O
the	O
dataset	O
used	O
in	O
the	O
project	O
is	O
a	O
processed	O
version	O
of	O
the	O
irvine	O
primate	O
splice-junction	O
database	O
each	O
of	O
the	O
examples	O
in	O
the	O
database	O
consists	O
of	O
a	O
window	O
of	O
nucleotides	O
each	O
represented	O
by	O
one	O
of	O
four	O
symbolic	O
values	O
and	O
the	O
classification	B
of	O
the	O
middle	O
point	O
in	O
the	O
window	O
as	O
one	O
of	O
intron	O
extron	O
boundary	O
extron	O
intron	O
boundary	O
or	O
neither	O
of	O
these	O
processing	O
involved	O
the	O
removal	O
of	O
a	O
small	O
number	O
of	O
ambiguous	O
examples	O
conversion	O
of	O
the	O
original	O
symbolic	O
attributes	B
to	O
or	O
binary	B
attributes	B
and	O
the	O
conversion	O
of	O
symbolic	O
class	B
labels	O
to	O
numeric	O
labels	O
section	O
the	O
training	B
set	I
of	O
was	O
chosen	O
randomly	O
from	O
the	O
dataset	O
and	O
the	O
remaining	O
examples	O
were	O
used	O
as	O
the	O
test	B
set	I
this	O
is	O
basically	O
a	O
partitioning	O
problem	O
and	O
so	O
we	O
might	O
expect	O
in	O
advance	O
that	O
decision	O
tree	O
algorithms	O
should	O
do	O
well	O
the	O
classes	B
in	O
this	O
problem	O
have	O
a	O
heirarchical	O
sec	O
miscellaneous	O
data	O
table	O
results	O
for	O
the	O
dna	B
dataset	I
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
fd	O
test	O
fd	O
error	B
rate	I
test	O
fd	O
train	O
fd	O
rank	O
structure	O
the	O
primary	O
decision	O
is	O
whether	O
the	O
centre	O
point	O
in	O
the	O
window	O
is	O
a	O
splice	O
junction	O
or	O
not	O
if	O
it	O
is	O
a	O
splice	O
junction	O
then	O
the	O
secondary	O
classification	B
is	O
as	O
to	O
its	O
type	O
intron	O
extron	O
or	O
extron	O
intron	O
unfortunately	O
comparisons	O
between	O
algorithms	O
are	O
more	O
difficult	O
than	O
usual	O
with	O
this	O
dataset	O
as	O
a	O
number	O
of	O
methods	O
were	O
tested	O
with	O
a	O
restricted	O
number	O
of	O
attributes	B
some	O
were	O
tested	O
with	O
attribute	O
values	O
converted	O
to	O
binary	O
values	O
and	O
some	O
to	O
binary	O
values	O
castle	B
and	O
cart	B
only	O
used	O
the	O
middle	O
binary	O
variables	O
newid	B
and	O
used	O
the	O
original	O
categorical	B
variables	I
and	O
k-nn	B
kohonen	B
lvq	B
backprop	B
and	O
rbf	B
used	O
the	O
one	O
of	O
four	O
coding	O
the	O
classical	O
statistical	B
algorithms	O
perform	O
reasonable	O
well	O
achieving	O
roughly	O
error	B
rate	I
k-nn	B
is	O
probably	O
hampered	O
by	O
the	O
large	O
number	O
of	O
binary	B
attributes	B
but	O
naive	B
bayes	I
does	O
rather	O
well	O
helped	O
by	O
the	O
fact	O
that	O
the	O
attributes	B
are	O
independent	O
surprisingly	O
machine	O
learning	O
algorithms	O
do	O
not	O
outperform	O
classical	O
statistical	B
algorithms	O
on	O
this	O
problem	O
castle	B
and	O
cart	B
were	O
at	O
a	O
disadvantage	O
using	O
a	O
smaller	O
window	O
although	O
performing	O
reasonably	O
indcart	B
used	O
attributes	B
and	O
improved	O
on	O
the	O
cart	B
error	B
rate	I
by	O
around	O
itrule	B
and	O
are	O
the	O
poorest	O
performers	O
in	O
this	O
dataset	O
descriptions	O
and	O
results	O
itrule	B
using	O
only	O
uni	O
variate	O
and	O
bi	O
variate	O
tests	O
is	O
too	O
restricted	O
and	O
is	O
group	O
probably	O
confused	O
by	O
the	O
large	O
number	O
of	O
attributes	B
of	O
the	O
neural	O
network	O
algorithms	O
kohonen	B
performs	O
very	O
poorly	O
not	O
helped	O
by	O
unequal	O
class	B
proportions	O
in	O
the	O
dataset	O
constructs	O
an	O
effective	O
set	O
of	O
piecewise	O
linear	O
decision	O
boundaries	O
but	O
overall	O
rbf	B
is	O
the	O
most	O
accurate	O
algorithm	O
using	O
centres	O
it	O
is	O
rather	O
worrying	O
here	O
that	O
lvq	B
claimed	O
an	O
error	B
rate	I
of	O
and	O
this	O
result	O
was	O
unchanged	O
when	O
the	O
test	O
data	O
had	O
the	O
classes	B
permuted	O
no	O
reason	O
could	O
be	O
found	O
for	O
this	O
phenomenon	O
presumably	O
it	O
was	O
caused	O
by	O
the	O
excessive	O
number	O
of	O
attributes	B
but	O
that	O
the	O
algorithm	O
should	O
lie	O
with	O
no	O
explanation	O
or	O
warning	O
is	O
still	O
a	O
mystery	O
this	O
problem	O
did	O
not	O
occur	O
with	O
any	O
other	O
dataset	O
in	O
order	O
to	O
assess	O
the	O
importance	O
of	O
the	O
window	O
size	O
in	O
this	O
problem	O
we	O
can	O
examine	O
in	O
a	O
little	O
more	O
detail	O
the	O
performance	O
of	O
one	O
of	O
the	O
machine	O
learning	O
algorithms	O
classified	O
the	O
training	B
set	I
using	O
rules	O
involving	O
tests	O
on	O
from	O
to	O
attributes	B
and	O
misclassifying	O
examples	O
table	O
shows	O
how	O
frequently	O
attributes	B
in	O
different	O
ranges	O
appeared	O
in	O
those	O
rules	O
from	O
the	O
table	O
it	O
appears	O
that	O
a	O
window	O
of	O
size	O
contains	O
the	O
table	O
frequency	O
of	O
occurrence	O
of	O
attributes	B
in	O
rules	O
generated	O
by	O
for	O
the	O
dna	B
training	B
set	I
class	B
class	B
class	B
total	O
most	O
important	O
variables	O
attributes	B
just	O
after	O
the	O
middle	O
of	O
the	O
window	O
are	O
most	O
important	O
in	O
determining	O
class	B
and	O
those	O
just	O
before	O
the	O
middle	O
are	O
most	O
important	O
in	O
determining	O
class	B
for	O
class	B
variables	O
close	O
to	O
the	O
middle	O
on	O
either	O
side	O
are	O
equally	O
important	O
overall	O
though	O
variables	O
throughout	O
the	O
attribute	O
window	O
do	O
seem	O
to	O
contribute	O
the	O
question	O
of	O
how	O
many	O
attributes	B
to	O
use	O
in	O
the	O
window	O
is	O
vitally	O
important	O
for	O
procedures	O
that	O
include	O
many	O
parameters	O
quadisc	B
gets	O
much	O
better	O
results	O
rate	O
of	O
on	O
the	O
test	B
set	I
if	O
it	O
is	O
restricted	O
to	O
the	O
middle	O
categorical	O
attributes	B
it	O
is	O
therefore	O
of	O
interest	O
to	O
note	O
that	O
decision	O
tree	O
procedures	O
get	O
almost	O
the	O
same	O
accuracies	O
on	O
the	O
original	O
categorical	O
data	O
and	O
the	O
processed	O
binary	O
data	O
newid	B
obtained	O
an	O
error	B
rate	I
of	O
on	O
the	O
preprocessed	O
data	O
variables	O
and	O
on	O
the	O
original	O
data	O
categorical	O
attributes	B
these	O
accuracies	O
are	O
probably	O
within	O
what	O
could	O
be	O
called	O
experimental	O
error	O
so	O
it	O
seems	O
that	O
newid	B
does	O
about	O
as	O
well	O
on	O
either	O
form	O
of	O
the	O
dataset	O
there	O
is	O
a	O
little	O
more	O
to	O
the	O
story	O
however	O
as	O
the	O
university	O
of	O
wisconsin	O
ran	O
several	O
algorithms	O
on	O
this	O
dataset	O
in	O
table	O
we	O
quote	O
their	O
results	O
alongside	O
ours	O
for	O
nearest	B
neighbour	I
in	O
this	O
problem	O
and	O
newid	B
are	O
probably	O
equivalent	O
and	O
the	O
slight	O
discrepancies	O
in	O
error	O
rates	O
achieved	O
by	O
at	O
wisconsin	O
compared	O
to	O
newid	B
in	O
this	O
study	O
are	O
attributable	O
to	O
the	O
different	O
random	O
samples	O
used	O
this	O
cannot	O
be	O
the	O
explanation	O
for	O
the	O
differences	O
between	O
the	O
two	O
nearest	B
neighbour	I
results	O
there	O
appears	O
to	O
be	O
an	O
irreconcilable	O
difference	O
perhaps	O
due	O
to	O
preprocessing	B
perhaps	O
due	O
to	O
distance	B
being	O
measured	O
in	O
a	O
conditional	O
dependent	O
manner	O
certainly	O
the	O
kohonen	B
algorithm	O
used	O
here	O
encountered	O
a	O
problem	O
when	O
defining	O
dis	O
sec	O
miscellaneous	O
data	O
c	O
cb	O
tances	O
in	O
the	O
attribute	O
space	O
when	O
using	O
the	O
coding	O
of	O
attributes	B
the	O
euclidean	O
distances	O
between	O
pairs	O
were	O
not	O
the	O
same	O
squared	O
distances	O
were	O
for	O
pairs	O
g	O
but	O
only	O
for	O
the	O
pairs	O
involvingz	O
therefore	O
kohonen	B
needs	O
the	O
coding	O
of	O
attributes	B
this	O
coding	O
was	O
also	O
adopted	O
by	O
other	O
algorithms	O
using	O
distance	B
measures	B
lvq	B
table	O
dna	B
dataset	I
error	O
rates	O
for	O
each	O
of	O
the	O
three	O
classes	B
splice	O
junction	O
is	O
intron	O
extron	O
extron	O
intron	O
or	O
neither	O
all	O
trials	O
except	O
the	O
last	O
were	O
carried	O
out	O
by	O
the	O
university	O
of	O
wisconsin	O
sometimes	O
with	O
local	O
implementations	O
of	O
published	O
algorithms	O
using	O
ten-fold	O
cross-validation	O
on	O
examples	O
randomly	O
selected	O
from	O
the	O
complete	O
set	O
of	O
the	O
last	O
trial	O
was	O
conducted	O
with	O
a	O
training	B
set	I
of	O
examples	O
and	O
a	O
test	B
set	I
of	O
examples	O
g	O
c	O
algorithm	O
kbann	O
backprop	B
pebls	O
perceptron	B
cobweb	O
n	O
neighbour	O
n	O
neighbour	O
neither	O
ei	O
ie	O
overall	O
technical	B
table	O
the	O
four	O
most	O
common	O
classes	B
in	O
the	O
technical	B
data	O
classified	O
by	O
the	O
value	O
of	O
attribute	O
very	O
little	O
is	O
known	O
about	O
this	O
dataset	O
as	O
the	O
nature	O
of	O
the	O
problem	O
domain	O
is	O
secret	O
it	O
is	O
of	O
commercial	O
interest	O
to	O
daimler-benz	O
ag	O
germany	O
the	O
dataset	O
shows	O
indications	O
of	O
some	O
sort	O
of	O
preprocessing	B
probably	O
by	O
some	O
decision-tree	O
type	O
process	O
before	O
it	O
was	O
received	O
to	O
give	O
only	O
one	O
instance	O
consider	O
only	O
the	O
four	O
most	O
common	O
classes	B
and	O
consider	O
only	O
one	O
attribute	O
by	O
simply	O
tabulating	O
the	O
values	O
of	O
attribute	O
it	O
becomes	O
obvious	O
that	O
the	O
classifications	O
are	O
being	O
made	O
according	O
to	O
symmetrically	O
placed	O
boundaries	O
on	O
specifically	O
the	O
two	O
boundaries	O
at	O
and	O
and	O
also	O
the	O
boundaries	O
at	O
and	O
these	O
boundaries	O
divide	O
the	O
range	O
of	O
into	O
five	O
regions	O
and	O
if	O
we	O
look	O
at	O
the	O
classes	B
contained	O
in	O
these	O
regions	O
we	O
get	O
the	O
frequency	O
table	O
in	O
table	O
the	O
symmetric	O
nature	O
of	O
the	O
boundaries	O
suggests	O
strongly	O
that	O
the	O
classes	B
have	O
been	O
defined	O
by	O
their	O
attributes	B
and	O
that	O
the	O
class	B
definitions	I
are	O
only	O
concerned	O
with	O
inequalities	O
on	O
the	O
attributes	B
needless	O
to	O
say	O
such	O
a	O
system	O
is	O
perfectly	O
suited	O
to	O
decision	B
trees	I
and	O
we	O
may	O
remark	O
in	O
passing	O
that	O
the	O
above	O
table	O
was	O
discovered	O
range	O
of	O
g	O
g	O
c	O
z	O
g	O
c	O
z	O
g	O
c	O
z	O
dataset	O
descriptions	O
and	O
results	O
by	O
a	O
decision	O
tree	O
when	O
applied	O
to	O
the	O
reduced	O
technical	B
dataset	I
with	O
all	O
attributes	B
but	O
with	O
only	O
the	O
four	O
most	O
common	O
classes	B
other	O
words	O
the	O
decision	O
tree	O
could	O
classify	O
the	O
reduced	O
dataset	O
with	O
error	O
in	O
examples	O
using	O
only	O
one	O
attribute	O
table	O
results	O
for	O
the	O
technical	B
dataset	I
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
fd	O
fd	O
fd	O
fd	O
fd	O
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
fd	O
fd	O
fd	O
fd	O
fd	O
test	O
fd	O
fd	O
fd	O
fd	O
fd	O
error	B
rate	I
test	O
fd	O
fd	O
fd	O
fd	O
fd	O
train	O
fd	O
fd	O
fd	O
fd	O
fd	O
rank	O
the	O
dataset	O
consists	O
of	O
examples	O
with	O
attributes	B
and	O
classes	B
the	O
attributes	B
are	O
all	O
believed	O
to	O
be	O
real	O
however	O
the	O
majority	O
of	O
attribute	O
values	O
are	O
zero	O
this	O
may	O
be	O
the	O
numerical	O
value	O
or	O
more	O
likely	O
not	O
relevant	O
not	O
measured	O
or	O
not	O
applicable	O
one-shot	O
train	O
and	O
test	O
was	O
used	O
to	O
calculate	O
the	O
accuracy	B
the	O
results	O
for	O
this	O
dataset	O
seem	O
quite	O
poor	O
although	O
all	O
are	O
significantly	O
better	O
than	O
the	O
default	B
error	B
rate	I
of	O
several	O
algorithms	O
failed	O
to	O
run	O
on	O
the	O
dataset	O
as	O
they	O
could	O
not	O
cope	O
with	O
the	O
large	O
number	O
of	O
classes	B
the	O
decision	O
tree	O
algorithms	O
indcart	B
preprocessing	B
which	O
made	O
the	O
dataset	O
more	O
suited	O
to	O
decision	B
trees	I
algorithms	O
however	O
the	O
output	B
produced	O
by	O
the	O
tree	O
algorithms	O
is	O
surprisingly	O
difficult	O
to	O
interpret	O
newid	B
gave	O
the	O
best	O
results	O
in	O
terms	O
of	O
error	O
rates	O
this	O
reflects	O
the	O
nature	O
of	O
the	O
has	O
newid	B
and	O
has	O
a	O
tree	O
with	O
terminal	O
nodes	O
has	O
nodes	O
has	O
nodes	O
and	O
nodes	O
statistical	B
algorithms	O
gave	O
much	O
poorer	O
results	O
with	O
quadisc	B
giving	O
the	O
highest	O
error	B
rate	I
of	O
all	O
they	O
appear	O
to	O
over-train	O
slightly	O
as	O
a	O
result	O
of	O
too	O
many	O
parameters	O
sec	O
miscellaneous	O
data	O
belgian	O
power	O
table	O
results	O
for	O
the	O
belgian	B
power	I
i	I
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
cascade	B
default	B
time	B
train	O
test	O
error	B
rate	I
test	O
train	O
rank	O
the	O
object	O
of	O
this	O
dataset	O
is	O
to	O
find	O
a	O
fast	O
and	O
reliable	O
indicator	O
of	O
instability	O
in	O
large	O
scale	O
power	O
systems	O
the	O
dataset	O
is	O
confidential	O
to	O
statlog	B
and	O
belongs	O
to	O
t	O
van	O
cutsem	O
and	O
l	O
wehenkel	O
university	O
of	O
liege	O
institut	O
montefiore	O
sart-tilman	O
liege	O
belgium	O
the	O
emergency	O
control	O
of	O
voltage	O
stability	O
is	O
still	O
in	O
its	O
infancy	O
but	O
one	O
important	O
aspect	O
of	O
this	O
control	O
is	O
the	O
early	O
detection	O
of	O
critical	O
states	O
in	O
order	O
to	O
reliably	O
trigger	O
automatic	O
corrective	O
actions	O
this	O
dataset	O
has	O
been	O
constructed	O
by	O
simulating	O
up	O
to	O
five	O
minutes	O
of	O
the	O
system	O
behaviour	O
basically	O
a	O
case	O
is	O
labelled	O
stable	O
if	O
all	O
voltages	O
controlled	O
by	O
on-load	O
tap	O
changers	O
are	O
successfully	O
brought	O
back	O
to	O
their	O
set-point	O
values	O
otherwise	O
the	O
system	O
becomes	O
unstable	O
there	O
are	O
examples	O
of	O
stable	O
and	O
unstable	O
states	O
each	O
with	O
attributes	B
which	O
involve	O
measurements	O
of	O
voltage	O
magnitudes	O
active	O
and	O
reactive	O
power	O
flows	O
and	O
injections	O
statistical	B
algorithms	O
cannot	O
be	O
run	O
on	O
datasets	O
which	O
have	O
linearly	O
dependent	O
attributes	B
and	O
there	O
are	O
such	O
attributes	B
in	O
the	O
belgian	O
power	O
dataset	O
these	O
have	O
to	O
be	O
removed	O
when	O
running	O
the	O
classical	O
statistical	B
algorithms	O
no	O
other	O
form	O
of	O
pre-processing	O
was	O
done	O
to	O
this	O
dataset	O
train	O
and	O
test	O
sets	O
have	O
dataset	O
descriptions	O
and	O
results	O
kohonen	B
map	O
belgian	O
power	O
data	O
fig	O
kohonen	B
map	O
of	O
the	O
belgian	O
power	O
data	O
showing	O
potential	O
clustering	B
both	O
classes	B
and	O
appear	O
to	O
have	O
two	O
distinct	O
clusters	O
examples	O
each	O
and	O
single	O
train-and-test	B
is	O
used	O
for	O
the	O
classification	B
the	O
statistical	B
algorithms	O
smart	B
and	O
logdisc	B
produced	O
results	O
which	O
are	O
significantly	O
better	O
than	O
the	O
other	O
algorithms	O
tested	O
on	O
this	O
dataset	O
logdisc	B
is	O
approximately	O
times	O
quicker	O
at	O
training	O
than	O
smart	B
and	O
still	O
produced	O
an	O
error	B
rate	I
of	O
less	O
than	O
also	O
gives	O
a	O
fairly	O
low	O
error	B
rate	I
and	O
is	O
not	O
time	B
consuming	O
to	O
run	O
k-nn	B
was	O
confused	O
by	O
irrelevant	B
attributes	B
and	O
a	O
variable	O
selection	O
option	O
reduced	O
the	O
error	B
rate	I
to	O
the	O
kohonen	B
map	O
of	O
this	O
data	O
may	O
help	O
to	O
understand	O
this	O
dataset	O
the	O
clustering	B
apparent	O
in	O
fig	O
shows	O
for	O
example	B
that	O
there	O
may	O
be	O
two	O
distinct	O
types	O
of	O
stable	O
state	O
by	O
the	O
decision	B
trees	I
did	O
not	O
do	O
so	O
well	O
here	O
it	O
is	O
interesting	O
that	O
the	O
smallest	O
tree	O
was	O
produced	O
by	O
with	O
nodes	O
and	O
the	O
largest	O
tree	O
was	O
produced	O
by	O
newid	B
with	O
nodes	O
and	O
yet	O
the	O
error	O
rates	O
are	O
very	O
similar	O
at	O
and	O
respectively	O
information	O
about	O
class	B
clusters	O
can	O
be	O
incorporated	O
directly	O
into	O
the	O
model	O
and	O
helps	O
to	O
produce	O
more	O
accurate	O
results	O
there	O
is	O
a	O
more	O
technical	B
description	O
of	O
this	O
dataset	O
in	O
van	O
cutsem	O
et	O
al	O
belgian	B
power	I
ii	I
this	O
dataset	O
is	O
drawn	O
from	O
a	O
larger	O
simulation	O
than	O
the	O
one	O
which	O
produced	O
the	O
belgian	O
power	O
dataset	O
the	O
objective	O
remains	O
to	O
find	O
a	O
fast	O
and	O
reliable	O
indicator	O
of	O
instability	O
in	O
large	O
scale	O
power	O
systems	O
this	O
dataset	O
is	O
also	O
confidential	O
and	O
belongs	O
to	O
the	O
university	O
sec	O
miscellaneous	O
data	O
table	O
results	O
for	O
the	O
belgian	B
power	I
ii	I
dataset	I
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
test	O
error	B
rate	I
test	O
train	O
rank	O
of	O
liege	O
and	O
electricite	O
de	O
france	O
the	O
training	B
set	I
consists	O
of	O
examples	O
with	O
attributes	B
the	O
test	B
set	I
contains	O
examples	O
and	O
there	O
are	O
two	O
classes	B
no	O
pre-processing	O
was	O
done	O
and	O
one-shot	O
train-and-test	B
was	O
used	O
to	O
calculate	O
the	O
accuracy	B
as	O
for	O
the	O
previous	O
belgian	O
power	O
dataset	O
smart	B
comes	O
out	O
top	O
in	O
terms	O
of	O
test	O
error	B
rate	I
it	O
takes	O
far	O
longer	O
to	O
run	O
than	O
the	O
other	O
algorithms	O
considered	O
here	O
logdisc	B
hasn	O
t	O
done	O
so	O
well	O
on	O
this	O
larger	O
dataset	O
k-nn	B
was	O
again	O
confused	O
by	O
irrelevant	B
attributes	B
and	O
a	O
variable	O
selection	O
option	O
reduced	O
the	O
error	B
rate	I
to	O
the	O
machine	O
there	O
is	O
a	O
detailed	O
description	O
of	O
this	O
dataset	O
and	O
related	O
results	O
in	O
wehenkel	O
et	O
al	O
machine	B
faults	I
due	O
to	O
the	O
confidential	O
nature	O
of	O
the	O
problem	O
very	O
little	O
is	O
known	O
about	O
this	O
dataset	O
it	O
was	O
donated	O
to	O
the	O
project	O
by	O
the	O
software	O
company	O
isoft	O
chemin	O
de	O
moulon	O
learning	O
algorithms	O
indcart	B
newid	B
results	O
the	O
tree	O
sizes	O
here	O
were	O
more	O
similar	O
with	O
and	O
newid	B
using	O
nodes	O
naive	B
bayes	I
is	O
worst	O
and	O
along	O
with	O
kohonen	B
and	O
itrule	B
give	O
poorer	O
results	O
than	O
the	O
default	B
rule	I
for	O
the	O
test	B
set	I
error	B
rate	I
baytree	O
and	O
give	O
consistently	O
good	O
using	O
nodes	O
nodes	O
dataset	O
descriptions	O
and	O
results	O
table	O
results	O
for	O
the	O
machine	B
faults	I
dataset	I
classes	B
attributes	B
observations	O
cross-validation	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
test	O
error	B
rate	I
test	O
train	O
rank	O
gif	O
sur	O
yvette	O
france	O
the	O
only	O
information	O
known	O
about	O
the	O
dataset	O
is	O
that	O
it	O
involves	O
the	O
financial	O
aspect	O
of	O
mechanical	O
maintenance	O
and	O
repair	O
the	O
aim	O
is	O
to	O
evaluate	O
the	O
cost	O
of	O
repairing	O
damaged	O
entities	O
the	O
original	O
dataset	O
had	O
multiple	O
attribute	O
values	O
and	O
a	O
few	O
errors	O
this	O
was	O
processed	O
to	O
split	O
the	O
attributes	B
into	O
the	O
original	O
train	O
and	O
test	O
sets	O
supplied	O
by	O
isoft	O
were	O
concatenated	O
and	O
the	O
examples	O
permuted	O
randomly	O
to	O
form	O
a	O
dataset	O
with	O
examples	O
the	O
pre-processing	O
of	O
hierarchical	O
data	O
is	O
discussed	O
further	O
in	O
section	O
there	O
are	O
numerical	O
attributes	B
and	O
classes	B
and	O
classification	B
was	O
done	O
using	O
cross-validation	O
this	O
is	O
the	O
only	O
hierarchical	O
dataset	O
studied	O
here	O
compared	O
with	O
the	O
other	O
algorithms	O
trials	O
were	O
done	O
on	O
the	O
original	O
dataset	O
whereas	O
the	O
other	O
algorithms	O
on	O
the	O
project	O
used	O
a	O
transformed	O
dataset	O
because	O
they	O
cannot	O
handle	O
dataset	O
was	O
preprocessed	O
in	O
order	O
that	O
other	O
algorithms	O
could	O
handle	O
the	O
dataset	O
this	O
preprocessing	B
was	O
done	O
without	O
loss	O
of	O
information	O
on	O
the	O
attributes	B
but	O
the	O
hierarchy	B
between	O
attributes	B
was	O
destroyed	O
the	O
dataset	O
of	O
this	O
application	O
has	O
been	O
designed	O
to	O
run	O
gives	O
the	O
best	O
error	B
rate	I
the	O
in	O
other	O
words	O
this	O
datasets	O
expressed	O
in	O
the	O
knowledge	O
representation	O
language	O
of	O
thus	O
all	O
the	O
knowledge	O
entered	O
has	O
been	O
used	O
by	O
the	O
program	O
this	O
explains	O
with	O
and	O
underlines	O
the	O
importance	O
of	O
structuring	O
the	O
knowledge	O
part	O
the	O
performance	O
of	O
sec	O
miscellaneous	O
data	O
for	O
an	O
application	O
although	O
this	O
result	O
is	O
of	O
interest	O
it	O
was	O
not	O
strictly	O
a	O
fair	O
comparison	O
used	O
domain-specific	O
knowledge	O
which	O
the	O
other	O
algorithms	O
did	O
not	O
for	O
the	O
most	O
part	O
could	O
not	O
in	O
addition	O
it	O
should	O
be	O
pointed	O
out	O
that	O
the	O
cross-validation	O
involved	O
a	O
different	O
splitting	O
method	O
that	O
preserved	O
the	O
class	B
proportions	O
so	O
this	O
will	O
also	O
bias	B
the	O
result	O
somewhat	O
the	O
size	O
of	O
the	O
tree	O
produced	O
by	O
is	O
nodes	O
whereas	O
and	O
newid	B
used	O
trees	O
with	O
nodes	O
and	O
nodes	O
since	O
procedure	O
used	O
with	O
respectively	O
kohonen	B
gives	O
the	O
poorest	O
result	O
which	O
is	O
surprising	O
as	O
this	O
neural	O
net	O
algorithm	O
should	O
do	O
better	O
on	O
datasets	O
with	O
nearly	O
equal	O
class	B
numbers	O
it	O
is	O
interesting	O
to	O
compare	O
this	O
with	O
the	O
results	O
for	O
k-nn	B
the	O
algorithm	O
should	O
work	O
well	O
on	O
all	O
datasets	O
on	O
which	O
any	O
algorithm	O
similar	O
to	O
the	O
nearest	B
neighbour	I
algorithm	O
a	O
classical	O
cluster	O
analysis	O
works	O
well	O
the	O
fact	O
the	O
k-nn	B
performs	O
badly	O
on	O
this	O
dataset	O
suggests	O
that	O
kohonen	B
will	O
too	O
tsetse	O
fly	O
distribution	O
zimbabwe	O
tsetse	O
fly	O
distribution	O
e	O
d	O
u	O
t	O
t	O
a	O
l	O
i	O
longitude	O
fig	O
tsetse	O
map	O
the	O
symbols	O
and	O
denote	O
the	O
presence	O
and	O
absence	O
of	O
tsetse	O
flies	O
respectively	O
tsetse	O
flies	O
are	O
one	O
of	O
the	O
most	O
prevalent	O
insect	O
hosts	O
spreading	O
disease	O
tripanosomiasis	O
from	O
cattle	O
to	O
humans	O
in	O
africa	O
in	O
order	O
to	O
limit	O
the	O
spread	O
of	O
disease	O
it	O
is	O
of	O
interest	O
to	O
predict	O
the	O
distribution	O
of	O
flies	O
and	O
types	O
of	O
environment	O
to	O
which	O
they	O
are	O
best	O
suited	O
the	O
tsetse	B
dataset	I
contains	O
interpolated	O
data	O
contributed	O
by	O
csiro	O
division	O
of	O
forrestry	O
australia	O
et	O
al	O
and	O
was	O
donated	O
by	O
trevor	O
h	O
booth	O
po	O
box	O
dataset	O
descriptions	O
and	O
results	O
queen	O
victoria	O
terrace	O
canberra	O
act	O
australia	O
tsetse	O
files	O
were	O
eradicated	O
from	O
most	O
of	O
zimbabwe	O
but	O
a	O
map	O
of	O
presenceabsence	O
was	O
constructed	O
before	O
any	O
eradication	O
programme	O
and	O
this	O
provides	O
the	O
classified	O
examples	O
for	O
a	O
total	O
of	O
squares	O
of	O
side	O
data	O
has	O
been	O
collected	O
from	O
maps	O
climatic	O
databases	O
and	O
remotely	O
sensed	O
information	O
the	O
main	O
interest	O
is	O
in	O
the	O
environmental	O
conditions	O
under	O
which	O
the	O
tsetse	O
fly	O
thrives	O
and	O
the	O
dataset	O
used	O
here	O
consisted	O
of	O
attributes	B
related	O
to	O
this	O
below	O
the	O
classes	B
are	O
presence	O
or	O
absence	O
of	O
flies	O
and	O
the	O
classification	B
was	O
done	O
using	O
one-shot	O
train-and-test	B
the	O
training	B
set	I
had	O
examples	O
and	O
the	O
test	B
set	I
had	O
both	O
had	O
roughly	O
equal	O
numbers	O
in	O
both	O
classes	B
all	O
attribute	O
values	O
are	O
numeric	O
and	O
indicated	O
below	O
the	O
original	O
data	O
had	O
measurements	O
of	O
latitude	O
and	O
longitude	O
as	O
attributes	B
which	O
were	O
used	O
to	O
construct	O
the	O
map	O
these	O
attributes	B
were	O
dropped	O
as	O
the	O
purpose	O
is	O
to	O
identify	O
the	O
environmental	O
conditions	O
suitable	O
for	O
flies	O
elevation	O
annual	O
average	O
nvdi	O
vegetation	O
index	O
nvdi	O
vegetation	O
index	O
for	O
february	O
nvdi	O
vegetation	O
index	O
for	O
september	O
max	O
min	O
nvdi	O
index	O
annual	O
evaporation	O
annual	O
rainfall	O
max	O
of	O
monthly	O
mean	O
temperature	O
maxima	O
max	O
of	O
monthly	O
mean	O
temperature	O
mean	O
of	O
monthly	O
means	O
min	O
of	O
monthly	O
means	O
minima	O
min	O
of	O
monthly	O
means	O
max	O
of	O
monthly	O
mean	O
temperature	O
maxima	O
min	O
of	O
monthly	O
means	O
minima	O
the	O
machine	O
learning	O
algorithms	O
produce	O
the	O
best	O
and	O
worst	O
results	O
for	O
all	O
give	O
rise	O
to	O
number	O
of	O
months	O
with	O
temperature	O
degrees	O
this	O
dataset	O
the	O
decision	O
tree	O
algorithms	O
cart	B
newid	B
and	O
fairly	O
accurate	O
classification	B
rules	O
the	O
modern	O
statistical	B
algorithms	O
smart	B
and	O
k-nn	B
do	O
significantly	O
better	O
than	O
the	O
classical	O
statistical	B
algorithms	O
quadisc	B
and	O
logdisc	B
with	O
a	O
variable	O
selection	O
procedure	O
k-nn	B
obtains	O
an	O
error	B
rate	I
of	O
again	O
indicating	O
some	O
unhelpful	O
attributes	B
similar	O
work	O
has	O
been	O
done	O
on	O
this	O
dataset	O
by	O
booth	O
et	O
al	O
and	O
ripley	O
the	O
dataset	O
used	O
by	O
ripley	O
was	O
slightly	O
different	O
in	O
that	O
the	O
attributes	B
were	O
normalised	O
to	O
be	O
in	O
the	O
range	O
over	O
the	O
whole	O
dataset	O
also	O
the	O
train	O
and	O
test	O
sets	O
used	O
in	O
the	O
classification	B
were	O
both	O
samples	O
of	O
size	O
taken	O
from	O
the	O
full	O
dataset	O
which	O
explains	O
the	O
less	O
accurate	O
results	O
achieved	O
for	O
example	B
linear	O
discriminants	O
had	O
an	O
error	B
rate	I
of	O
an	O
algorithm	O
similar	O
to	O
smart	B
had	O
neighbour	O
had	O
and	O
backprop	B
had	O
the	O
best	O
results	O
for	O
lvq	B
was	O
and	O
for	O
tree	O
algorithms	O
an	O
error	B
rate	I
of	O
was	O
reduced	O
to	O
on	O
pruning	B
however	O
the	O
conclusions	O
of	O
both	O
studies	O
agree	O
the	O
nearest	B
neighbour	I
and	O
lvq	B
algorithms	O
work	O
well	O
they	O
provide	O
no	O
explanation	O
of	O
the	O
structure	O
in	O
the	O
dataset	O
sec	O
measures	B
table	O
results	O
for	O
the	O
tsetse	B
dataset	I
classes	B
attributes	B
test	O
observations	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
max	O
algorithm	O
storage	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
default	B
time	B
train	O
test	O
error	B
rate	I
test	O
train	O
rank	O
the	O
results	O
of	O
that	O
the	O
tree-based	O
methods	O
provide	O
a	O
very	O
good	O
and	O
interpretable	O
fit	O
can	O
be	O
seen	O
from	O
cart	B
and	O
newid	B
similar	O
error	O
rates	O
were	O
obtained	O
for	O
used	O
nodes	O
used	O
nodes	O
and	O
newid	B
used	O
nodes	O
however	O
used	O
only	O
nodes	O
and	O
achieved	O
a	O
slightly	O
higher	O
error	B
rate	I
which	O
possibly	O
suggests	O
over-pruning	O
castle	B
has	O
a	O
high	O
error	B
rate	I
compared	O
with	O
the	O
other	O
algorithms	O
it	O
appears	O
to	O
use	O
only	O
one	O
attribute	O
to	O
construct	O
the	O
classification	B
rule	I
the	O
mlp	B
result	O
is	O
directly	O
comparable	O
with	O
the	O
result	O
achieved	O
by	O
ripley	O
values	O
were	O
normalised	O
and	O
gave	O
a	O
slightly	O
better	O
result	O
rate	O
lower	O
however	O
the	O
overall	O
conclusion	O
is	O
the	O
same	O
in	O
that	O
mlps	O
did	O
about	O
the	O
same	O
as	O
lvq	B
and	O
nearest-neighbour	O
both	O
of	O
which	O
are	O
much	O
simpler	O
to	O
use	O
statistical	B
and	O
information	B
measures	B
we	O
give	O
in	O
tables	O
and	O
the	O
statistical	B
and	O
information	B
measures	B
as	O
described	O
in	O
section	O
and	O
for	O
all	O
of	O
the	O
datasets	O
as	O
the	O
calculation	O
of	O
the	O
measures	B
involved	O
substantial	O
computations	O
some	O
of	O
the	O
measures	B
were	O
calculated	O
for	O
reduced	O
datasets	O
for	O
example	B
the	O
measures	B
for	O
kl-digits	O
are	O
based	O
on	O
the	O
training	O
examples	O
only	O
the	O
following	O
notes	O
are	O
made	O
for	O
a	O
few	O
of	O
the	O
datasets	O
only	O
and	O
are	O
not	O
meant	O
to	O
be	O
dataset	O
descriptions	O
and	O
results	O
comprehensive	O
rather	O
some	O
instructive	O
points	O
are	O
chosen	O
for	O
illustrating	O
the	O
important	O
ideas	O
contained	O
in	O
the	O
measures	B
kl-digits	O
dataset	O
the	O
dataset	O
that	O
looks	O
closest	O
to	O
being	O
normal	O
is	O
the	O
karhunen-loeve	O
version	O
of	O
digits	O
this	O
could	O
be	O
predicted	O
beforehand	O
as	O
it	O
is	O
a	O
linear	B
transformation	B
of	O
the	O
attributes	B
that	O
by	O
the	O
central	O
limit	O
theorem	O
would	O
be	O
closer	O
to	O
normal	O
than	O
the	O
original	O
because	O
there	O
are	O
very	O
many	O
attributes	B
in	O
each	O
linear	O
combination	O
the	O
kl-digits	O
dataset	O
is	O
very	O
close	O
to	O
normal	O
with	O
skewness	B
and	O
kurtosis	B
as	O
against	O
the	O
exact	O
normal	O
values	O
of	O
skewness	B
and	O
kurtosis	B
rather	O
interestingly	O
the	O
multivariate	B
kurtosis	B
statisticd	O
for	O
kl	O
digits	O
show	O
a	O
very	O
marked	O
departure	O
from	O
multivariate	B
normality	I
despite	O
the	O
fact	O
that	O
the	O
univariate	O
statistics	O
are	O
close	O
to	O
normal	O
kurtosis	B
this	O
is	O
not	O
too	O
surprising	O
it	O
is	O
possible	O
to	O
take	O
a	O
linear	O
transform	O
from	O
karhunen-loeve	O
space	O
back	O
to	O
the	O
original	O
highly	O
non-normal	O
dataset	O
this	O
shows	O
the	O
practical	O
desirability	O
of	O
using	O
a	O
multivariate	O
version	O
of	O
kurtosis	B
d	O
the	O
kl	O
version	O
of	O
digits	O
appears	O
to	O
be	O
well	O
suited	O
to	O
quadratic	B
discriminants	I
there	O
is	O
a	O
substantial	O
difference	O
in	O
variances	O
ratio	O
while	O
at	O
the	O
same	O
time	B
the	O
distributions	O
are	O
not	O
too	O
far	O
from	O
multivariate	B
normality	I
with	O
kurtosis	B
of	O
order	O
also	O
and	O
more	O
importantly	O
there	O
are	O
sufficient	O
examples	O
that	O
the	O
many	O
parameters	O
of	O
the	O
quadratic	B
discriminants	I
can	O
be	O
estimated	O
fairly	O
accurately	O
vehicle	B
silhouettes	O
in	O
the	O
vehicle	B
dataset	I
the	O
high	O
value	O
of	O
might	O
indicate	O
that	O
discrimination	B
could	O
be	O
based	O
on	O
just	O
two	O
discriminants	O
this	O
may	O
relate	O
to	O
the	O
fact	O
that	O
the	O
two	O
cars	O
are	O
not	O
easily	O
distinguishable	O
so	O
might	O
be	O
treated	O
as	O
one	O
dimensionality	O
of	O
the	O
mean	O
vectors	O
to	O
however	O
although	O
the	O
fraction	O
of	O
discriminating	O
power	O
for	O
the	O
third	O
discriminant	O
is	O
low	O
it	O
is	O
still	O
statistically	O
significant	O
so	O
cannot	O
be	O
discarded	O
without	O
a	O
small	O
loss	O
of	O
discrimination	B
this	O
dataset	O
also	O
illustrates	O
that	O
using	O
mean	O
statistics	O
may	O
mask	O
significant	O
differences	O
in	O
behaviour	O
between	O
classes	B
for	O
example	B
in	O
the	O
vehicle	B
dataset	I
for	O
some	O
of	O
the	O
populations	O
types	O
and	O
mardia	O
s	O
kurtosis	B
statistic	O
is	O
not	O
significant	O
however	O
for	O
both	O
vehicle	B
types	O
and	O
the	O
univariate	O
statistics	O
are	O
very	O
significantly	O
low	O
indicating	O
marked	O
departure	O
from	O
normality	O
mardia	O
s	O
statistic	O
does	O
not	O
pick	O
this	O
up	O
partly	O
because	O
the	O
also	O
the	O
kl	O
version	O
appears	O
to	O
have	O
a	O
greater	O
difference	O
in	O
variances	O
ratio	O
than	O
the	O
raw	O
digit	O
data	O
ratio	O
this	O
is	O
an	O
artefact	O
the	O
digits	O
data	O
used	O
here	O
had	O
several	O
attributes	B
with	O
zero	O
variances	O
in	O
some	O
classes	B
giving	O
rise	O
to	O
an	O
infinite	O
value	O
for	O
sd	O
ratio	O
is	O
got	O
by	O
summing	O
over	O
a	O
set	O
of	O
pixels	O
the	O
original	O
digits	O
data	O
with	O
attributes	B
the	O
total	O
of	O
the	O
individual	O
mutual	O
informations	O
for	O
the	O
kl	O
dataset	O
is	O
namely	O
these	O
datasets	O
are	O
ultimately	O
derived	O
from	O
the	O
same	O
dataset	O
so	O
it	O
is	O
no	O
surprise	O
that	O
these	O
totals	O
are	O
rather	O
close	O
however	O
most	O
algorithms	O
found	O
the	O
kl	O
attributes	B
more	O
informative	O
about	O
class	B
so	O
obtained	O
reduced	O
error	O
rates	O
and	O
this	O
figure	O
can	O
be	O
compared	O
with	O
the	O
corresponding	O
total	O
for	O
the	O
digit	O
dataset	O
sec	O
measures	B
cred	O
man	O
cr	O
aust	O
table	O
table	O
of	O
measures	B
for	O
datasets	O
kl	O
segm	O
letter	O
chrom	O
satim	O
vehicle	B
cut	B
n	O
p	O
k	O
bin	O
att	O
cost	O
sd	O
corr	B
abs	I
skewness	B
kurtosis	B
c	O
v	O
c	O
n	O
p	O
k	O
bin	O
att	O
cost	O
sd	O
corr	B
abs	I
skewness	B
kurtosis	B
c	O
cby	O
v	O
c	O
g	O
g	O
y	O
g	O
g	O
g	O
y	O
g	O
dataset	O
descriptions	O
and	O
results	O
n	O
p	O
k	O
bin	O
att	O
cost	O
sd	O
corr	B
abs	I
skewness	B
kurtosis	B
c	O
v	O
c	O
table	O
table	O
of	O
measures	B
for	O
datasets	O
head	O
cr	O
ger	O
heart	O
shuttle	B
diab	O
dna	B
tech	O
belg	O
belgii	O
faults	O
tsetse	O
n	O
p	O
k	O
bin	O
att	O
cost	O
sd	O
corr	B
abs	I
skewness	B
kurtosis	B
c	O
v	O
c	O
g	O
g	O
y	O
g	O
g	O
g	O
y	O
g	O
sec	O
measures	B
number	O
of	O
attributes	B
is	O
fairly	O
large	O
in	O
relation	O
to	O
the	O
number	O
of	O
examples	O
per	O
class	B
and	O
partly	O
because	O
mardia	O
s	O
statistic	O
is	O
less	O
efficient	O
than	O
the	O
univariate	O
statistics	O
head	B
injury	I
among	O
the	O
datasets	O
with	O
more	O
than	O
two	O
classes	B
the	O
clearest	O
evidence	O
of	O
collinearity	O
is	O
in	O
the	O
head	B
injury	I
dataset	I
here	O
the	O
second	O
canonical	B
correlation	B
is	O
not	O
statistically	O
different	O
from	O
zero	O
with	O
a	O
critical	O
level	O
of	O
it	O
appears	O
that	O
a	O
single	O
linear	B
discriminant	I
is	O
sufficient	O
to	O
discriminate	O
between	O
the	O
classes	B
precisely	O
a	O
second	O
linear	B
discriminant	I
does	O
not	O
improve	O
discrimination	B
therefore	O
the	O
head	B
injury	I
dataset	I
is	O
very	O
close	O
to	O
linearity	O
this	O
may	O
also	O
be	O
observed	O
from	O
the	O
value	O
of	O
implying	O
that	O
the	O
three	O
class	B
means	O
lie	O
close	O
to	O
a	O
straight	O
line	O
in	O
turn	O
this	O
suggests	O
that	O
the	O
class	B
values	O
reflect	O
some	O
underlying	O
continuum	O
of	O
severity	O
so	O
this	O
is	O
not	O
a	O
true	O
discrimination	B
problem	O
note	O
the	O
similarity	O
with	O
fisher	O
s	O
original	O
use	O
of	O
discrimination	B
as	O
a	O
means	O
of	O
ordering	O
populations	O
perhaps	O
this	O
dataset	O
would	O
best	O
be	O
dealt	O
with	O
by	O
a	O
pure	O
regression	O
technique	O
either	O
linear	O
or	O
logistic	O
if	O
so	O
manova	B
gives	O
the	O
best	O
set	O
of	O
scores	O
for	O
the	O
three	O
categories	O
of	O
injury	O
as	O
indicating	O
that	O
the	O
middle	O
group	O
is	O
slightly	O
nearer	O
to	O
category	O
than	O
but	O
not	O
significantly	O
nearer	O
it	O
appears	O
that	O
there	O
is	O
not	O
much	O
difference	O
between	O
the	O
covariance	B
matrices	O
for	O
the	O
three	O
populations	O
in	O
the	O
head	B
dataset	I
ratio	O
so	O
the	O
procedure	O
quadratic	O
discrimination	B
is	O
not	O
expected	O
to	O
do	O
much	O
better	O
than	O
linear	B
discrimination	B
will	O
probably	O
do	O
worse	O
as	O
it	O
uses	O
many	O
more	O
parameters	O
heart	B
disease	I
the	O
leading	O
correlation	B
coefficient	O
in	O
the	O
heart	B
dataset	I
is	O
not	O
very	O
high	O
that	O
gives	O
a	O
measure	B
of	O
predictability	O
therefore	O
the	O
discriminating	O
power	O
of	O
the	O
linear	B
discriminant	I
is	O
only	O
moderate	O
this	O
ties	O
up	O
with	O
the	O
moderate	O
success	O
of	O
linear	O
discriminants	O
for	O
this	O
dataset	O
for	O
the	O
training	O
data	O
of	O
in	O
mind	O
that	O
it	O
is	O
correlation	B
satellite	B
image	I
dataset	I
the	O
satellite	B
image	I
data	O
is	O
the	O
only	O
dataset	O
for	O
which	O
there	O
appears	O
to	O
be	O
very	O
large	O
correlations	O
between	O
the	O
attributes	B
although	O
there	O
may	O
be	O
some	O
large	O
correlations	O
in	O
the	O
vehicle	B
dataset	I
not	O
too	O
many	O
presumably	O
since	O
here	O
corr	B
abs	I
note	O
that	O
only	O
three	O
linear	O
discriminants	O
are	O
sufficient	O
to	O
separate	O
all	O
six	O
class	B
means	O
this	O
may	O
be	O
interpreted	O
as	O
evidence	O
of	O
seriation	O
with	O
the	O
three	O
classes	B
grey	O
soil	O
damp	O
grey	O
soil	O
and	O
very	O
damp	O
grey	O
soil	O
forming	O
a	O
continuum	O
equally	O
this	O
result	O
can	O
be	O
interpreted	O
as	O
indicating	O
that	O
the	O
original	O
attributes	B
may	O
be	O
successfully	O
reduced	O
to	O
three	O
with	O
no	O
loss	O
of	O
information	O
here	O
information	O
should	O
be	O
interpreted	O
as	O
mean	O
square	O
distance	B
between	O
classes	B
or	O
equivalently	O
as	O
the	O
entropy	B
of	O
a	O
normal	B
distribution	I
is	O
and	O
this	O
figure	O
gives	O
an	O
effective	O
number	O
of	O
classes	B
of	O
which	O
is	O
approximately	O
this	O
can	O
be	O
interpreted	O
as	O
follows	O
although	O
shuttle	B
control	I
the	O
class	B
entropy	B
c	O
jd	O
g	O
dataset	O
descriptions	O
and	O
results	O
there	O
are	O
six	O
classes	B
in	O
the	O
shuttle	B
dataset	I
some	O
class	B
probabilities	O
are	O
very	O
low	O
indeed	O
so	O
low	O
in	O
fact	O
that	O
the	O
complexity	O
of	O
the	O
classification	B
problem	O
is	O
on	O
a	O
par	O
with	O
a	O
two-class	O
problem	O
technical	B
although	O
all	O
attributes	B
are	O
nominally	O
continuous	O
there	O
are	O
very	O
many	O
zeroes	O
so	O
many	O
that	O
we	O
can	O
regard	O
some	O
of	O
the	O
attributes	B
as	O
nearly	O
constant	O
equal	O
to	O
zero	O
this	O
is	O
shown	O
which	O
is	O
substantially	O
less	O
than	O
one	O
bit	O
by	O
the	O
average	O
attribute	B
entropy	B
cby	O
the	O
average	O
mutual	B
information	I
carried	O
by	O
each	O
attribute	O
so	O
that	O
although	O
the	O
attributes	B
contain	O
little	O
information	O
content	O
this	O
information	O
contains	O
relatively	O
little	O
noise	B
and	O
this	O
is	O
about	O
half	O
of	O
the	O
information	O
belgian	B
power	I
ii	I
the	O
belgian	B
power	I
ii	I
dataset	I
is	O
a	O
prime	O
candidate	O
for	O
data	O
compression	O
as	O
the	O
ratio	O
of	O
noise	B
to	O
useful	O
information	O
is	O
very	O
high	O
substantial	O
reduction	O
in	O
the	O
size	O
of	O
the	O
dataset	O
is	O
possible	O
without	O
affecting	O
the	O
accuracy	B
of	O
any	O
classification	B
procedure	O
this	O
does	O
not	O
mean	O
that	O
the	O
dataset	O
is	O
noisy	B
in	O
the	O
sense	O
of	O
not	O
allowing	O
good	O
prediction	O
the	O
better	O
algorithms	O
achieve	O
an	O
error	B
rate	I
of	O
less	O
than	O
on	O
the	O
existing	O
dataset	O
and	O
would	O
achieve	O
the	O
same	O
error	B
rate	I
on	O
the	O
condensed	O
dataset	O
this	O
is	O
particularly	O
true	O
for	O
the	O
decision	B
trees	I
typically	O
they	O
use	O
only	O
a	O
small	O
number	O
of	O
attributes	B
g	O
v	O
c	O
y	O
g	O
analysis	B
of	I
results	I
p	O
b	O
brazdil	O
and	O
r	O
j	O
henery	O
university	O
of	O
porto	O
and	O
university	O
of	O
strathclyde	O
introduction	O
we	O
analyse	O
the	O
results	O
of	O
the	O
trials	O
in	O
this	O
chapter	O
using	O
several	O
methods	O
the	O
section	O
on	O
results	O
by	O
subject	O
areas	O
shows	O
that	O
neural	O
network	O
and	O
statistical	B
methods	O
do	O
better	O
in	O
some	O
areas	O
and	O
machine	O
learning	O
procedures	O
in	O
others	O
the	O
idea	O
is	O
to	O
give	O
some	O
indication	O
of	O
the	O
subject	O
areas	O
where	O
certain	O
methods	O
do	O
best	O
the	O
various	O
methods	O
both	O
algorithms	O
and	O
datasets	O
using	O
the	O
performance	O
of	O
every	O
combination	O
multidimensional	B
scaling	I
is	O
a	O
method	O
that	O
can	O
be	O
used	O
to	O
point	O
out	O
similarities	O
in	O
algorithm	O
dataset	O
as	O
a	O
basis	O
the	O
aim	O
here	O
is	O
to	O
understand	O
the	O
relationship	O
between	O
we	O
also	O
describe	O
a	O
simple-minded	O
attempt	O
at	O
exploring	O
the	O
relationship	O
between	O
pruning	B
and	O
accuracy	B
of	O
decision	B
trees	I
a	O
principal	O
aim	O
of	O
statlog	B
was	O
to	O
relate	O
performance	O
of	O
algorithms	O
interpreted	O
as	O
accuracy	B
or	O
error-rate	O
to	O
characteristics	O
or	O
measures	B
of	O
datasets	O
here	O
the	O
aim	O
is	O
to	O
give	O
objective	O
measures	B
describing	O
a	O
dataset	O
and	O
to	O
predict	O
how	O
well	O
any	O
given	O
algorithm	O
will	O
perform	O
on	O
that	O
dataset	O
we	O
discuss	O
several	O
ways	O
in	O
which	O
this	O
might	O
be	O
done	O
this	O
includes	O
an	O
empirical	O
study	O
of	O
performance	O
related	O
to	O
statistical	B
and	O
information-theoretic	O
measures	B
of	O
the	O
datasets	O
in	O
particular	O
one	O
of	O
the	O
learning	O
algorithms	O
under	O
study	O
is	O
used	O
in	O
an	O
ingenious	O
attempt	O
to	O
predict	O
performance	O
of	O
all	O
algorithms	O
from	O
the	O
measures	B
on	O
a	O
given	O
dataset	O
the	O
performance	O
of	O
an	O
algorithm	O
may	O
be	O
predicted	O
by	O
the	O
performance	O
of	O
similar	O
algorithms	O
if	O
results	O
are	O
already	O
available	O
for	O
a	O
few	O
yardstick	B
methods	I
the	O
hope	O
is	O
that	O
the	O
performance	O
of	O
other	O
methods	O
can	O
be	O
predicted	O
from	O
the	O
yardstick	O
results	O
in	O
presenting	O
these	O
analyses	O
we	O
aim	O
to	O
give	O
many	O
different	O
views	O
of	O
the	O
results	O
so	O
that	O
a	O
reasonably	O
complete	O
perhaps	O
not	O
always	O
coherent	O
picture	O
can	O
be	O
presented	O
of	O
a	O
very	O
complex	B
problem	O
namely	O
the	O
problem	O
of	O
explaining	O
why	O
some	O
algorithms	O
do	O
better	O
m	O
address	O
for	O
correspondence	O
laboratory	O
of	O
ai	O
and	O
computer	O
science	O
university	O
of	O
porto	O
r	O
campo	O
alegre	O
porto	O
portugal	O
analysis	B
of	I
results	I
on	O
some	O
datasets	O
and	O
not	O
so	O
well	O
on	O
others	O
these	O
differing	O
analyses	O
may	O
give	O
conflicting	O
and	O
perhaps	O
irreconcilable	O
conclusions	O
however	O
we	O
are	O
not	O
yet	O
at	O
the	O
stage	O
where	O
we	O
can	O
say	O
that	O
this	O
or	O
that	O
analysis	O
is	O
the	O
final	O
and	O
only	O
word	O
on	O
the	O
subject	O
so	O
we	O
present	O
all	O
the	O
facts	O
in	O
the	O
hope	O
that	O
the	O
reader	O
will	O
be	O
able	O
to	O
judge	O
what	O
is	O
most	O
relevant	O
to	O
the	O
particular	O
application	O
at	O
hand	O
results	O
by	O
subject	O
areas	O
to	O
begin	O
with	O
the	O
results	O
of	O
the	O
trials	O
will	O
be	O
discussed	O
in	O
subject	O
areas	O
this	O
is	O
partly	O
because	O
this	O
makes	O
for	O
easier	O
description	O
and	O
interpretation	O
but	O
more	O
importantly	O
because	O
the	O
performance	O
of	O
the	O
various	O
algorithms	O
is	O
much	O
influenced	O
by	O
the	O
particular	O
application	O
several	O
datasets	O
are	O
closely	O
related	O
and	O
it	O
is	O
easier	O
to	O
spot	O
differences	O
when	O
comparisons	O
are	O
made	O
within	O
the	O
same	O
dataset	O
type	O
so	O
we	O
will	O
discuss	O
the	O
results	O
under	O
four	O
headings	O
datasets	O
involving	O
costs	B
credit	O
risk	O
datasets	O
image	O
related	O
datasets	O
others	O
of	O
course	O
these	O
headings	O
are	O
not	O
necessarily	O
disjoint	O
one	O
of	O
our	O
datasets	O
credit	O
was	O
a	O
credit	O
dataset	O
involving	O
costs	B
the	O
feature	O
dominating	O
performance	O
of	O
algorithms	O
is	O
costs	B
so	O
the	O
german	B
credit	I
dataset	I
is	O
listed	O
under	O
the	O
cost	B
datasets	I
we	O
do	O
not	O
attempt	O
to	O
give	O
any	O
absolute	O
assessment	O
of	O
accuracies	O
or	O
average	O
costs	B
but	O
we	O
have	O
listed	O
the	O
algorithms	O
in	O
each	O
heading	O
by	O
their	O
average	O
ranking	O
within	O
this	O
heading	O
algorithms	O
at	O
the	O
top	O
of	O
the	O
table	O
do	O
well	O
on	O
average	O
and	O
algorithms	O
at	O
the	O
bottom	O
do	O
badly	O
to	O
illustrate	O
how	O
the	O
ranking	O
was	O
calculated	O
consider	O
the	O
two	O
credit	B
datasets	I
because	O
for	O
example	B
is	O
ranked	O
in	O
the	O
australian	B
credit	I
and	O
in	O
the	O
credit	B
management	I
dataset	I
has	O
a	O
total	O
rank	O
of	O
which	O
is	O
the	O
smallest	O
total	O
of	O
all	O
and	O
is	O
therefore	O
top	O
of	O
the	O
listing	O
in	O
the	O
credit	B
datasets	I
similarly	O
has	O
a	O
total	O
rank	O
of	O
and	O
so	O
is	O
in	O
the	O
list	O
of	O
course	O
other	O
considerations	O
such	O
as	O
memory	B
storage	B
time	B
to	I
learn	I
etc	O
must	O
not	O
be	O
forgotten	O
in	O
this	O
chapter	O
we	O
take	O
only	O
error-rate	O
or	O
average	O
cost	O
into	O
account	O
credit	B
datasets	I
we	O
have	O
results	O
for	O
two	O
credit	B
datasets	I
in	O
two	O
of	O
these	O
the	O
problem	O
is	O
to	O
predict	O
the	O
creditworthiness	O
of	O
applicants	O
for	O
credit	O
but	O
they	O
are	O
all	O
either	O
coded	O
or	O
confidential	O
to	O
a	O
greater	O
or	O
lesser	O
extent	O
so	O
for	O
example	B
we	O
do	O
not	O
know	O
the	O
exact	O
definition	O
of	O
uncreditworthy	O
or	O
bad	O
risk	O
possible	O
definitions	O
are	O
more	O
than	O
one	O
month	O
late	O
with	O
the	O
first	O
payment	O
more	O
than	O
two	O
months	O
late	O
with	O
the	O
first	O
payment	O
or	O
even	O
the	O
credit	O
manager	O
has	O
already	O
refused	O
credit	O
to	O
this	O
person	O
credit	B
management	I
credit	B
management	I
data	O
from	O
the	O
uk	O
german	B
credit	I
risk	O
data	O
from	O
germany	O
australian	B
credit	I
risk	O
data	O
from	O
it	O
may	O
be	O
that	O
these	O
classifications	O
are	O
defined	O
by	O
a	O
human	O
if	O
so	O
then	O
the	O
aim	O
of	O
the	O
decision	O
rule	O
is	O
to	O
devise	O
a	O
procedure	O
that	O
mimics	O
the	O
human	O
decision	O
process	O
as	O
closely	O
as	O
possible	O
machine	O
learning	O
procedures	O
are	O
very	O
good	O
at	O
this	O
and	O
this	O
probably	O
reflects	O
a	O
natural	O
sec	O
results	O
by	O
subject	O
areas	O
tendency	O
for	O
human	O
decisions	O
to	O
be	O
made	O
in	O
a	O
sequential	O
manner	O
it	O
is	O
then	O
correspondingly	O
easy	O
for	O
a	O
human	O
to	O
understand	O
the	O
decision	O
tree	O
methods	O
as	O
this	O
best	O
reflects	O
the	O
human	O
decision	O
process	O
costs	B
of	O
misclassification	O
in	O
two	O
of	O
our	O
credit	B
datasets	I
we	O
were	O
unable	O
to	O
assess	O
either	O
the	O
prior	O
odds	B
of	O
good-bad	O
or	O
the	O
relative	O
costs	B
of	O
making	O
the	O
wrong	O
decisions	O
however	O
in	O
the	O
german	B
credit	I
data	O
we	O
were	O
given	O
an	O
independent	O
assessment	O
that	O
the	O
relative	O
cost	O
of	O
granting	O
credit	O
to	O
a	O
bad	O
risk	O
customer	O
was	O
times	O
that	O
of	O
turning	O
down	O
an	O
application	O
from	O
a	O
good	O
risk	O
customer	O
or	O
is	O
the	O
cost	O
of	O
misclassifying	O
a	O
bad	O
credit	O
risk	O
as	O
good	O
and	O
is	O
the	O
cost	O
of	O
misclassifying	O
a	O
good	O
credit	O
risk	O
as	O
bad	O
we	O
assume	O
that	O
the	O
proportions	O
of	O
good-bad	O
risks	O
in	O
the	O
training	O
sample	O
reflect	O
those	O
in	O
the	O
population	O
also	O
in	O
the	O
credit	B
management	I
dataset	I
it	O
was	O
explicitly	O
stated	O
by	O
the	O
dataset	O
provider	O
that	O
errors	O
of	O
either	O
type	O
were	O
equally	O
important	O
a	O
statement	O
that	O
we	O
interpreted	O
to	O
mean	O
that	O
the	O
cost-ratio	O
was	O
unity	O
mdcb	O
mdcb	O
mdc	O
gih	O
on	O
the	O
other	O
hand	O
the	O
definition	O
of	O
bad	O
risk	O
may	O
be	O
defined	O
by	O
the	O
lateness	O
of	O
payments	O
or	O
non-payment	O
the	O
task	O
here	O
is	O
to	O
assess	O
the	O
degree	O
of	O
risk	O
most	O
datasets	O
of	O
this	O
nature	O
lose	O
much	O
useful	O
information	O
by	O
binarising	O
some	O
measure	B
of	O
badness	O
for	O
example	B
a	O
customer	O
may	O
be	O
classed	O
as	O
a	O
bad	O
risk	O
if	O
the	O
first	O
repayment	O
is	O
more	O
than	O
one	O
month	O
late	O
whereas	O
a	O
more	O
natural	O
approach	O
would	O
be	O
to	O
predict	O
the	O
number	O
of	O
months	O
before	O
the	O
first	O
payment	O
is	O
made	O
the	O
statlog	B
versions	O
of	O
machine	O
learning	O
methods	O
were	O
not	O
generally	O
well	O
adapted	O
to	O
prediction	O
problems	O
however	O
apart	O
from	O
anything	O
else	O
prediction	O
problems	O
involve	O
some	O
cost	O
function	O
but	O
not	O
necessarily	O
quadratic	O
the	O
important	O
point	O
is	O
that	O
some	O
errors	O
are	O
more	O
serious	O
than	O
others	O
generally	O
in	O
credit	O
risk	B
assessment	I
the	O
cost	O
of	O
misclassification	O
is	O
a	O
vital	O
element	O
the	O
classification	B
of	O
a	O
bad	O
credit	O
risk	O
as	O
good	O
usually	O
costs	B
more	O
than	O
classification	B
of	O
a	O
good	O
credit	O
risk	O
as	O
bad	O
unfortunately	O
credit	O
institutes	O
cannot	O
give	O
precise	O
estimates	O
of	O
the	O
cost	O
of	O
misclassification	O
on	O
the	O
other	O
hand	O
many	O
of	O
the	O
algorithms	O
in	O
this	O
study	O
cannot	O
use	O
a	O
cost	B
matrix	I
in	O
performing	O
the	O
classification	B
task	O
although	O
there	O
have	O
recently	O
been	O
some	O
attempts	O
to	O
consider	O
misclassification	B
costs	B
in	O
learning	O
algorithms	O
such	O
newid	B
and	O
knoll	O
if	O
we	O
were	O
to	O
judge	O
learning	O
algorithms	O
solely	O
on	O
the	O
basis	O
of	O
average	O
misclassification	O
cost	O
this	O
would	O
penalise	O
the	O
ml	O
algorithms	O
in	O
some	O
of	O
the	O
datasets	O
therefore	O
we	O
used	O
the	O
average	O
error	B
rate	I
instead	O
this	O
is	O
equivalent	O
to	O
average	O
misclassification	O
cost	O
in	O
a	O
very	O
special	O
case	O
as	O
we	O
will	O
now	O
show	O
is	O
misclassification	O
as	O
the	O
error	O
rates	O
in	O
the	O
classification	B
of	O
bad	O
and	O
good	O
risks	O
respectively	O
denoting	O
the	O
prior	O
is	O
the	O
cost	O
of	O
misclassifying	O
a	O
bad	O
credit	O
risk	O
as	O
good	O
andmdcb	O
recall	O
thatmdc	O
g	O
and	O
g	O
are	O
the	O
cost	O
of	O
misclassifying	O
a	O
good	O
credit	O
risk	O
as	O
bad	O
suppose	O
also	O
that	O
probabilities	O
of	O
good	O
and	O
bad	O
risks	O
by	O
and	O
we	O
can	O
calculate	O
the	O
expected	O
cost	O
of	O
g	O
and	O
as	O
mentioned	O
above	O
in	O
practice	O
it	O
is	O
very	O
difficult	O
to	O
find	O
out	O
the	O
values	O
ofmdc	O
g	O
for	O
example	B
srinivisan	O
sim	O
because	O
of	O
this	O
it	O
is	O
often	O
assumed	O
that	O
mdcb	O
mdc	O
mdc	O
exmdcb	O
g	O
g	O
g	O
g	O
g	O
g	O
g	O
g	O
g	O
g	O
g	O
analysis	B
of	I
results	I
using	O
assumption	O
one	O
can	O
get	O
the	O
expected	O
misclassification	O
cost	O
k	O
from	O
equation	O
is	O
the	O
same	O
for	O
all	O
algorithms	O
so	O
one	O
can	O
use	O
the	O
c	O
e	O
total	O
error	B
rate	I
in	O
equation	O
the	O
ex	O
c	O
as	O
an	O
equivalent	O
evaluation	O
criterion	O
when	O
comparing	O
the	O
performance	O
of	O
algorithms	O
results	O
and	O
conclusions	O
table	O
error	O
rates	O
for	O
credit	B
datasets	I
ordered	O
by	O
their	O
average	O
rank	O
over	O
the	O
datasets	O
credit	O
logdisc	B
smart	B
indcart	B
bprop	O
discrim	B
rbf	B
baytree	O
itrule	B
k-nn	B
naivebay	O
castle	B
cart	B
newid	B
lvq	B
kohonen	B
quadisc	B
default	B
cr	O
aus	O
cr	O
man	O
the	O
table	O
of	O
error	O
rates	O
for	O
the	O
credit	B
datasets	I
is	O
given	O
in	O
table	O
in	O
reading	O
this	O
table	O
the	O
reader	O
should	O
beware	O
that	O
not	O
much	O
can	O
be	O
inferred	O
from	O
only	O
two	O
cases	O
re	O
the	O
suitability	O
of	O
this	O
or	O
that	O
algorithm	O
for	O
credit	B
datasets	I
generally	O
in	O
real	O
credit	O
applications	O
differential	O
misclassification	B
costs	B
tend	O
to	O
loom	O
large	O
if	O
not	O
explicitly	O
then	O
by	O
implication	O
it	O
is	O
noteworthy	O
that	O
three	O
of	O
the	O
top	O
six	O
algorithms	O
are	O
decision	B
trees	I
and	O
indcart	B
while	O
the	O
algorithm	O
in	O
second	O
place	O
is	O
akin	O
to	O
a	O
neural	O
network	O
we	O
may	O
conclude	O
that	O
decision	B
trees	I
do	O
reasonably	O
well	O
on	O
credit	B
datasets	I
this	O
conclusion	O
g	O
g	O
g	O
g	O
g	O
g	O
sec	O
results	O
by	O
subject	O
areas	O
would	O
probably	O
be	O
strengthened	O
if	O
we	O
had	O
persuaded	O
cart	B
to	O
run	O
on	O
the	O
credit	B
management	I
dataset	I
as	O
it	O
is	O
likely	O
that	O
the	O
error	B
rate	I
for	O
cart	B
would	O
be	O
fairly	O
similar	O
to	O
indcart	B
s	O
value	O
and	O
then	O
cart	B
would	O
come	O
above	O
indcart	B
in	O
this	O
table	O
however	O
where	O
values	O
were	O
missing	O
as	O
is	O
the	O
case	O
with	O
cart	B
the	O
result	O
was	O
assumed	O
to	O
be	O
the	O
default	B
value	O
an	O
admittedly	O
very	O
conservative	O
procedure	O
so	O
cart	B
appears	O
low	O
down	O
in	O
table	O
by	O
itself	O
the	O
conclusion	O
that	O
decision	B
trees	I
do	O
well	O
on	O
credit	B
datasets	I
while	O
giving	O
some	O
practical	O
guidance	O
on	O
a	O
specific	O
application	O
area	O
does	O
not	O
explain	O
why	O
decision	B
trees	I
should	O
be	O
successful	O
here	O
a	O
likely	O
explanation	O
is	O
that	O
both	O
datasets	O
are	O
partitioning	O
datasets	O
this	O
is	O
known	O
to	O
be	O
true	O
for	O
the	O
credit	B
management	I
dataset	I
where	O
a	O
human	O
classified	O
the	O
data	O
on	O
the	O
basis	O
of	O
the	O
attributes	B
we	O
suspect	O
that	O
it	O
holds	O
for	O
the	O
other	O
credit	O
dataset	O
also	O
in	O
view	O
of	O
the	O
following	O
facts	O
they	O
are	O
both	O
credit	B
datasets	I
they	O
are	O
near	O
each	O
other	O
in	O
the	O
multidimensional	B
scaling	I
representation	O
of	O
all	O
datasets	O
and	O
they	O
are	O
similar	O
in	O
terms	O
of	O
number	O
of	O
attributes	B
number	O
of	O
classes	B
presence	O
of	O
categorical	O
attributes	B
etc	O
part	O
of	O
the	O
reason	O
for	O
their	O
success	O
in	O
this	O
subject	O
area	O
is	O
undoubtedly	O
that	O
decision	O
tree	O
methods	O
can	O
cope	O
more	O
naturally	O
with	O
a	O
large	O
number	O
of	O
binary	O
or	O
categorical	O
attributes	B
the	O
number	O
of	O
categories	O
is	O
small	O
they	O
also	O
incorporate	O
interaction	O
terms	O
as	O
a	O
matter	O
of	O
course	O
and	O
perhaps	O
more	O
significantly	O
they	O
mirror	O
the	O
human	O
decision	O
process	O
image	B
datasets	I
image	O
classification	B
problems	O
occur	O
in	O
a	O
wide	O
variety	O
of	O
contexts	O
in	O
some	O
applications	O
the	O
entire	O
image	O
an	O
object	O
in	O
the	O
image	O
must	O
be	O
classified	O
whereas	O
in	O
other	O
cases	O
the	O
classification	B
proceeds	O
on	O
a	O
pixel-by-pixel	O
basis	O
with	O
extra	O
spatial	O
information	O
one	O
of	O
the	O
first	O
problems	O
to	O
be	O
tackled	O
was	O
of	O
landsat	O
data	O
where	O
switzer	O
considered	O
classification	B
of	O
each	O
pixel	O
in	O
a	O
spatial	O
context	O
a	O
similar	O
dataset	O
was	O
used	O
in	O
our	O
trials	O
whereby	O
the	O
attributes	B
not	O
the	O
class	B
of	O
neighbouring	O
pixels	O
was	O
used	O
to	O
aid	O
the	O
classification	B
a	O
further	O
image	B
segmentation	I
problem	O
of	O
classifying	O
each	O
pixel	O
is	O
considered	O
in	O
section	O
an	O
alternative	O
problem	O
is	O
to	O
classify	O
the	O
entire	O
image	O
into	O
one	O
of	O
several	O
classes	B
an	O
example	B
of	O
this	O
is	O
object	O
recognition	O
for	O
example	B
classifying	O
a	O
hand-written	O
character	O
or	O
a	O
remotely	O
sensed	O
vehicle	B
another	O
example	B
in	O
our	O
trials	O
is	O
the	O
classification	B
of	O
chromosomes	B
based	O
on	O
a	O
number	O
of	O
features	B
extracted	O
from	O
an	O
image	O
there	O
are	O
different	O
levels	O
of	O
image	O
data	O
at	O
the	O
simplest	O
level	O
we	O
can	O
consider	O
the	O
grey	O
values	O
at	O
each	O
pixel	O
as	O
the	O
set	O
of	O
variables	O
to	O
classify	O
each	O
pixel	O
or	O
the	O
whole	O
image	O
our	O
trials	O
suggest	O
that	O
the	O
latter	O
are	O
not	O
likely	O
to	O
work	O
unless	O
the	O
image	O
is	O
rather	O
small	O
for	O
example	B
classifying	O
a	O
hand-written	O
number	O
on	O
the	O
basis	O
off	O
most	O
of	O
our	O
algorithms	O
the	O
pixel	O
data	O
can	O
be	O
further	O
processed	O
to	O
yield	O
a	O
sharper	O
image	O
or	O
other	O
information	O
which	O
is	O
still	O
pixel-based	O
for	O
example	B
a	O
gradient	O
filter	O
can	O
be	O
used	O
to	O
extract	O
edges	O
a	O
more	O
promising	O
approach	O
to	O
classify	O
images	O
is	O
to	O
extract	O
and	O
select	O
appropriate	O
features	B
and	O
the	O
vehicle	B
silhouette	O
and	O
chromosome	O
datasets	O
are	O
of	O
this	O
type	O
the	O
issue	O
of	O
extracting	O
the	O
right	O
features	B
is	O
a	O
harder	O
problem	O
the	O
temptation	O
is	O
to	O
measure	B
everything	O
which	O
may	O
be	O
useful	O
but	O
additional	O
information	O
which	O
is	O
not	O
relevant	O
may	O
spoil	O
the	O
performance	O
of	O
a	O
classifier	B
for	O
example	B
the	O
nearest	B
neighbour	I
method	O
typically	O
treats	O
all	O
variables	O
with	O
equal	O
weight	O
and	O
if	O
some	O
are	O
of	O
no	O
value	O
then	O
very	O
poor	O
results	O
can	O
occur	O
other	O
algorithms	O
are	O
more	O
robust	O
to	O
this	O
pitfall	O
grey	O
levels	O
defeated	O
for	O
presentation	O
purposes	O
we	O
will	O
categorise	O
each	O
of	O
the	O
nine	O
image	B
datasets	I
as	O
being	O
f	O
analysis	B
of	I
results	I
one	O
of	O
segmentation	O
or	O
object	O
recognition	O
and	O
we	O
give	O
the	O
results	O
of	O
the	O
two	O
types	O
separately	O
results	O
and	O
conclusions	O
object	O
recognition	O
table	O
error	O
rates	O
for	O
object	B
recognition	I
datasets	I
algorithms	O
are	O
listed	O
in	O
order	O
of	O
their	O
average	O
ranking	O
over	O
the	O
five	O
datasets	O
algorithms	O
near	O
the	O
top	O
tend	O
to	O
do	O
well	O
at	O
object	O
recognition	O
object	O
quadisc	B
k-nn	B
lvq	B
logdiscr	O
discrim	B
smart	B
rbf	B
baytree	O
backprop	B
newid	B
indcart	B
cascade	B
kohonen	B
castle	B
cart	B
itrule	B
naivebay	O
default	B
kl	O
digits	O
vehic	O
chrom	O
letter	O
table	O
gives	O
the	O
error-rates	O
for	O
the	O
five	O
object	B
recognition	I
datasets	I
it	O
is	O
believed	O
that	O
this	O
group	O
contains	O
pure	O
discrimination	B
datasets	O
vehicle	B
and	O
letter	B
recognition	I
on	O
these	O
datasets	O
standard	O
statistical	B
procedures	O
and	O
neural	B
networks	I
do	O
well	O
overall	O
it	O
would	O
be	O
wrong	O
to	O
draw	O
general	O
conclusions	O
from	O
only	O
five	O
datasets	O
but	O
we	O
can	O
make	O
the	O
following	O
points	O
the	O
proponents	O
of	O
backpropagation	O
claim	O
that	O
it	O
has	O
a	O
special	O
ability	O
to	O
model	O
non-linear	O
behaviour	O
some	O
of	O
these	O
datasets	O
have	O
significant	O
non-linearity	O
and	O
it	O
is	O
true	O
that	O
backpropagation	O
does	O
well	O
however	O
in	O
the	O
case	O
of	O
the	O
digits	O
it	O
performs	O
only	O
marginally	O
better	O
than	O
quadratic	B
discriminants	I
which	O
can	O
also	O
model	O
non-linear	O
behaviour	O
and	O
in	O
the	O
case	O
of	O
the	O
vehicles	O
it	O
performs	O
significantly	O
worse	O
when	O
one	O
considers	O
the	O
large	O
amount	O
of	O
extra	O
effort	O
required	O
to	O
optimise	O
and	O
train	O
backpropagation	O
one	O
must	O
ask	O
whether	O
it	O
really	O
offers	O
an	O
advantage	O
over	O
more	O
traditional	O
algorithms	O
ripley	O
also	O
raises	O
some	O
important	O
points	O
on	O
the	O
use	O
and	O
claims	O
of	O
neural	O
net	O
methods	O
sec	O
results	O
by	O
subject	O
areas	O
castle	B
performs	O
poorly	O
but	O
this	O
is	O
probably	O
because	O
it	O
is	O
not	O
primarily	O
designed	O
for	O
discrimination	B
its	O
main	O
advantage	O
is	O
that	O
it	O
gives	O
an	O
easily	O
comprehensible	O
picture	O
of	O
the	O
structure	O
of	O
the	O
data	O
it	O
indicates	O
which	O
variables	O
influence	O
one	O
another	O
most	O
strongly	O
and	O
can	O
identify	O
which	O
subset	O
of	O
attributes	B
are	O
the	O
most	O
strongly	O
connected	O
to	O
the	O
decision	B
class	B
however	O
it	O
ignores	O
weak	O
connections	O
and	O
this	O
is	O
the	O
reason	O
for	O
its	O
poor	O
performance	O
in	O
that	O
weak	O
connections	O
may	O
still	O
have	O
an	O
influence	O
on	O
the	O
final	O
decision	B
class	B
smart	B
and	O
linear	O
discriminants	O
perform	O
similarly	O
on	O
these	O
datasets	O
both	O
of	O
these	O
work	O
with	O
linear	O
combinations	O
of	O
the	O
attributes	B
although	O
smart	B
is	O
more	O
general	O
in	O
that	O
it	O
takes	O
non-linear	O
functions	O
of	O
these	O
combinations	O
however	O
quadratic	B
discriminants	I
performs	O
rather	O
better	O
which	O
suggests	O
that	O
a	O
better	O
way	O
to	O
model	O
non-linearity	O
would	O
be	O
to	O
input	B
selected	O
quadratic	O
combinations	O
of	O
attributes	B
to	O
linear	O
discriminants	O
the	O
nearest	B
neighbour	I
algorithm	O
does	O
well	O
if	O
all	O
the	O
variables	O
are	O
useful	O
in	O
classification	B
and	O
if	O
there	O
are	O
no	O
problems	O
in	O
choosing	O
the	O
right	O
scaling	O
raw	O
pixel	O
data	O
such	O
as	O
the	O
satellite	O
data	O
and	O
the	O
hand-written	B
digits	I
satisfy	O
these	O
criteria	O
if	O
some	O
of	O
the	O
variables	O
are	O
misleading	O
or	O
unhelpful	O
then	O
a	O
variable	O
selection	O
procedure	O
should	O
precede	O
classification	B
the	O
algorithm	O
used	O
here	O
was	O
not	O
efficient	O
in	O
cpu	O
time	B
since	O
no	O
condensing	O
was	O
used	O
results	O
from	O
ripley	O
indicate	O
that	O
condensing	O
does	O
not	O
greatly	O
affect	O
the	O
classification	B
performance	O
paired	O
comparison	O
on	O
digits	O
data	O
kl	O
and	O
the	O
digits	O
data	O
represent	O
different	O
preprocessed	O
versions	O
of	O
one	O
and	O
the	O
same	O
original	O
dataset	O
not	O
unexpectedly	O
there	O
is	O
a	O
high	O
correlation	B
between	O
the	O
error-rates	O
with	O
two	O
missing	B
values	I
cart	B
and	O
kohonen	B
on	O
kl	O
of	O
much	O
more	O
interest	O
is	O
the	O
fact	O
that	O
the	O
statistical	B
and	O
neural	O
net	O
procedures	O
perform	O
much	O
better	O
on	O
the	O
kl	O
version	O
than	O
on	O
the	O
version	O
on	O
the	O
other	O
hand	O
machine	O
learning	O
methods	O
perform	O
rather	O
poorly	O
on	O
the	O
version	O
and	O
do	O
even	O
worse	O
on	O
the	O
kl	O
version	O
it	O
is	O
rather	O
difficult	O
to	O
account	O
for	O
this	O
phenomenon	O
ml	O
methods	O
by	O
their	O
nature	O
do	O
not	O
seem	O
to	O
cope	O
with	O
situations	O
where	O
the	O
information	O
is	O
spread	O
over	O
a	O
large	O
number	O
of	O
variables	O
by	O
construction	O
the	O
karhunen-loeve	O
dataset	O
deliberately	O
creates	O
variables	O
that	O
are	O
linear	O
combinations	O
of	O
the	O
original	O
pixel	O
gray	O
levels	O
with	O
the	O
first	O
variable	O
containing	O
most	O
information	O
the	O
second	O
variable	O
containing	O
the	O
maximum	O
information	O
orthogonal	O
to	O
the	O
first	O
etc	O
from	O
one	O
point	O
of	O
view	O
therefore	O
the	O
first	O
kl	O
attributes	B
contain	O
more	O
information	O
than	O
the	O
complete	O
set	O
of	O
attributes	B
in	O
the	O
digit	O
dataset	O
the	O
latter	O
is	O
a	O
particular	O
set	O
of	O
linear	O
combinations	O
of	O
the	O
original	O
data	O
and	O
the	O
improvement	O
in	O
error	O
rates	O
of	O
the	O
statistical	B
procedures	O
is	O
consistent	O
with	O
this	O
interpretation	O
results	O
and	O
conclusions	O
segmentation	O
table	O
gives	O
the	O
error	O
rates	O
for	O
the	O
four	O
segmentation	O
problems	O
machine	O
learning	O
procedures	O
do	O
fairly	O
well	O
in	O
segmentation	O
datasets	O
and	O
traditional	O
statistical	B
methods	O
do	O
very	O
badly	O
the	O
probable	O
explanation	O
is	O
that	O
these	O
datasets	O
originate	O
as	O
partitioning	O
problems	O
paired	O
comparison	O
of	O
and	O
the	O
dataset	O
consists	O
of	O
the	O
first	O
attributes	B
in	O
the	O
dataset	O
ordered	O
by	O
importance	O
in	O
a	O
stepwise	O
regression	O
procedure	O
one	O
would	O
therefore	O
expect	O
and	O
generally	O
one	O
observes	O
that	O
performance	O
deteriorates	O
when	O
the	O
number	O
of	O
attributes	B
is	O
decreased	O
that	O
the	O
information	O
content	O
is	O
decreased	O
one	O
exception	O
to	O
this	O
rule	O
is	O
quadratic	O
discrimination	B
which	O
does	O
badly	O
in	O
the	O
dataset	O
and	O
even	O
worse	O
in	O
the	O
data	O
this	O
is	O
the	O
converse	O
of	O
the	O
paired	O
comparison	O
in	O
the	O
digits	O
analysis	B
of	I
results	I
table	O
error	O
rates	O
for	O
segmentation	O
datasets	O
algorithms	O
are	O
listed	O
in	O
order	O
of	O
their	O
average	O
ranking	O
over	O
the	O
four	O
datasets	O
algorithms	O
near	O
the	O
top	O
tend	O
to	O
do	O
well	O
in	O
image	B
segmentation	I
problems	O
segment	O
baytree	O
k-nn	B
newid	B
indcart	B
lvq	B
rbf	B
backprop	B
smart	B
logdisc	B
cart	B
kohonen	B
discrim	B
castle	B
quadisc	B
default	B
naivebay	O
itrule	B
cascade	B
satim	O
segm	O
dataset	O
it	O
appears	O
that	O
algorithms	O
that	O
are	O
already	O
doing	O
badly	O
on	O
the	O
most	O
informative	O
set	O
of	O
attributes	B
do	O
even	O
worse	O
when	O
the	O
less	O
informative	O
attributes	B
are	O
added	O
similarly	O
machine	O
learning	O
methods	O
do	O
better	O
on	O
the	O
dataset	O
but	O
there	O
is	O
a	O
surprise	O
they	O
use	O
smaller	O
decision	B
trees	I
to	O
achieve	O
greater	O
accuracy	B
this	O
must	O
mean	O
that	O
some	O
of	O
the	O
less	O
significant	O
attributes	B
contribute	O
to	O
the	O
discrimination	B
by	O
means	O
of	O
interactions	O
non-linearities	O
here	O
the	O
phrase	O
less	O
significant	O
is	O
used	O
in	O
a	O
technical	B
sense	O
referring	O
to	O
the	O
least	O
informative	O
attributes	B
in	O
linear	O
discriminants	O
clearly	O
attributes	B
that	O
have	O
little	O
information	O
for	O
linear	O
discriminants	O
may	O
have	O
considerable	O
value	O
for	O
other	O
procedures	O
that	O
are	O
capable	O
of	O
incorporating	O
interactions	O
and	O
non-linearities	O
directly	O
k-nn	B
is	O
best	O
for	O
images	O
perhaps	O
the	O
most	O
striking	O
result	O
in	O
the	O
images	O
datasets	O
is	O
the	O
performance	O
of	O
k-nearest	B
neighbour	I
with	O
four	O
outright	O
top	O
places	O
and	O
two	O
runners-up	O
it	O
would	O
seem	O
that	O
in	O
terms	O
of	O
error-rate	O
best	O
results	O
in	O
image	O
data	O
are	O
obtained	O
by	O
k-nearest	B
neighbour	I
sec	O
datasets	O
with	O
costs	B
results	O
by	O
subject	O
areas	O
there	O
are	O
two	O
medical	B
datasets	I
and	O
one	O
credit	O
dataset	O
in	O
this	O
section	O
these	O
are	O
illustrative	O
of	O
the	O
application	O
areas	O
where	O
costs	B
are	O
important	O
there	O
are	O
two	O
ways	O
in	O
which	O
algorithms	O
can	O
incorporate	O
costs	B
into	O
a	O
decision	O
rule	O
at	O
the	O
learning	O
stage	O
or	O
during	O
the	O
test	O
stage	O
most	O
statistical	B
procedures	O
are	O
based	O
on	O
estimates	O
of	O
probabilities	O
and	O
incorporate	O
costs	B
only	O
at	O
the	O
final	O
test	O
stage	O
evaluating	O
the	O
expected	O
cost	O
of	O
misclassification	O
however	O
some	O
procedures	O
can	O
incorporate	O
costs	B
into	O
the	O
learning	O
stage	O
one	O
simple	O
way	O
to	O
do	O
this	O
might	O
be	O
to	O
give	O
extra	O
weight	O
to	O
observations	O
from	O
classes	B
with	O
high	O
costs	B
of	O
misclassification	O
results	O
and	O
conclusions	O
table	O
average	O
costs	B
for	O
datasets	O
with	O
cost	B
matrices	I
algorithms	O
are	O
listed	O
in	O
order	O
of	O
their	O
average	O
ranking	O
over	O
the	O
three	O
datasets	O
algorithms	O
near	O
the	O
bottom	O
cannot	O
cope	O
with	O
costs	B
costs	B
discrim	B
logdisc	B
castle	B
quadisc	B
cart	B
naivebay	O
smart	B
k-nn	B
cascade	B
backprop	B
baytree	O
indcart	B
default	B
itrule	B
lvq	B
newid	B
kohonen	B
rbf	B
head	O
heart	O
cr	O
ger	O
the	O
average	O
costs	B
of	O
the	O
various	O
algorithms	O
are	O
given	O
in	O
table	O
there	O
are	O
some	O
surprises	O
in	O
this	O
table	O
particularly	O
relating	O
to	O
the	O
default	B
procedure	O
and	O
the	O
performance	O
of	O
most	O
machine	O
learning	O
and	O
some	O
of	O
the	O
neural	O
network	O
procedures	O
overall	O
it	O
would	O
seem	O
that	O
the	O
ml	O
procedures	O
do	O
worse	O
than	O
the	O
default	B
granting	O
credit	O
to	O
everyone	O
or	O
declaring	O
everyone	O
to	O
be	O
seriously	O
ill	O
analysis	B
of	I
results	I
other	O
datasets	O
table	O
error	O
rates	O
for	O
remaining	O
datasets	O
the	O
shuttle	B
error	O
rates	O
are	O
in	O
algorithms	O
are	O
listed	O
in	O
order	O
of	O
their	O
average	O
ranking	O
over	O
the	O
eight	O
datasets	O
most	O
of	O
the	O
problems	O
in	O
the	O
table	O
are	O
partitioning	O
problems	O
so	O
it	O
is	O
fairly	O
safe	O
to	O
say	O
that	O
algorithms	O
near	O
the	O
top	O
of	O
the	O
table	O
are	O
most	O
suited	O
to	O
partitioning	O
problems	O
others	O
baytree	O
newid	B
indcart	B
smart	B
logdisc	B
cart	B
backprop	B
rbf	B
discrim	B
quadisc	B
naivebay	O
castle	B
k-nn	B
itrule	B
lvq	B
kohonen	B
default	B
belg	O
newbel	O
tset	O
diab	O
dna	B
faults	O
shutt	O
tech	O
of	O
the	O
remaining	O
datasets	O
at	O
least	O
two	O
and	O
technical	B
are	O
pure	O
partitioning	O
problems	O
with	O
boundaries	O
characteristically	O
parallel	O
to	O
the	O
attribute	O
axes	O
a	O
fact	O
that	O
can	O
be	O
judged	O
from	O
plots	O
of	O
the	O
attributes	B
two	O
are	O
simulated	O
datasets	O
and	O
belgian	B
power	I
ii	I
and	O
can	O
be	O
described	O
as	O
somewhere	O
between	O
prediction	O
and	O
partitioning	O
the	O
aim	O
of	O
the	O
tsetse	B
dataset	I
can	O
be	O
precisely	O
stated	O
as	O
partitioning	O
a	O
map	O
into	O
two	O
regions	O
so	O
as	O
to	O
reproduce	O
a	O
given	O
partitioning	O
as	O
closely	O
as	O
possible	O
the	O
tsetse	B
dataset	I
is	O
also	O
artificial	O
insofar	O
as	O
some	O
of	O
the	O
attributes	B
have	O
been	O
manufactured	O
an	O
interpolation	O
from	O
a	O
small	O
amount	O
of	O
information	O
the	O
diabetes	B
dataset	I
is	O
a	O
prediction	O
problem	O
the	O
nature	O
of	O
the	O
other	O
datasets	O
machine	B
faults	I
i	O
e	O
whether	O
we	O
are	O
dealing	O
with	O
partitioning	O
prediction	O
or	O
discrimination	B
is	O
not	O
known	O
precisely	O
results	O
and	O
conclusions	O
table	O
gives	O
the	O
error-rates	O
for	O
these	O
eight	O
datasets	O
it	O
is	O
perhaps	O
inappropriate	O
to	O
draw	O
general	O
conclusions	O
from	O
such	O
a	O
mixed	O
bag	O
of	O
datasets	O
however	O
it	O
would	O
appear	O
from	O
the	O
performance	O
of	O
the	O
algorithms	O
that	O
the	O
datasets	O
are	O
best	O
dealt	O
with	O
by	O
machine	O
learning	O
sec	O
top	O
five	O
algorithms	O
or	O
neural	O
network	O
procedures	O
how	O
much	O
relevance	O
this	O
has	O
to	O
practical	O
problems	O
is	O
debatable	O
however	O
as	O
two	O
are	O
simulated	O
and	O
two	O
are	O
pure	O
partitioning	O
datasets	O
top	O
five	O
algorithms	O
in	O
table	O
we	O
present	O
the	O
algorithms	O
that	O
came	O
out	O
top	O
for	O
each	O
of	O
the	O
datasets	O
only	O
the	O
top	O
five	O
algorithms	O
are	O
quoted	O
the	O
table	O
is	O
quoted	O
for	O
reference	O
only	O
so	O
that	O
readers	O
can	O
see	O
which	O
algorithms	O
do	O
well	O
on	O
a	O
particular	O
dataset	O
the	O
algorithms	O
that	O
make	O
the	O
top	O
five	O
most	O
frequently	O
are	O
times	O
discrim	B
logdiscr	O
and	O
quadisc	B
but	O
not	O
too	O
much	O
should	O
be	O
made	O
of	O
these	O
figures	O
as	O
they	O
depend	O
very	O
much	O
on	O
the	O
mix	O
of	O
problems	O
used	O
table	O
top	O
five	O
algorithms	O
for	O
all	O
datasets	O
first	O
fourth	O
second	O
third	O
quadisc	B
quadisc	B
lvq	B
cascade	B
discrim	B
logdiscr	O
dataset	O
kl	O
satim	O
vehic	O
head	O
heart	O
belg	O
segm	O
diab	O
cr	O
ger	O
chrom	O
cr	O
aus	O
shutt	O
dna	B
tech	O
newbel	O
isoft	O
tset	O
cr	O
man	O
letter	O
k-nn	B
k-nn	B
k-nn	B
quadisc	B
logdiscr	O
naivebay	O
smart	B
logdiscr	O
discrim	B
quadisc	B
newid	B
rbf	B
newid	B
smart	B
baytree	O
k-nn	B
smart	B
logdiscr	O
itrule	B
baytree	O
indcart	B
indcart	B
baytree	O
k-nn	B
k-nn	B
fifth	O
bprop	O
cart	B
lvq	B
cascade	B
rbf	B
lvq	B
logdiscr	O
quadisc	B
discrim	B
logdiscr	O
quadisc	B
discrim	B
bprop	O
newid	B
baytree	O
smart	B
discrim	B
rbf	B
castle	B
lvq	B
discrim	B
logdiscr	O
discrim	B
discrim	B
baytree	O
discrim	B
indcart	B
baytree	O
lvq	B
k-nn	B
cart	B
quadisc	B
newid	B
bprop	O
logdiscr	O
cart	B
newid	B
newid	B
bprop	O
quadisc	B
table	O
gives	O
the	O
same	O
information	O
as	O
table	O
but	O
here	O
it	O
is	O
the	O
type	O
of	O
algorithm	O
machine	O
learning	O
or	O
neural	O
net	O
that	O
is	O
quoted	O
in	O
the	O
head	B
injury	I
dataset	I
the	O
top	O
five	O
algorithms	O
are	O
all	O
statistical	B
whereas	O
the	O
top	O
five	O
are	O
all	O
machine	O
learning	O
for	O
the	O
shuttle	B
and	O
technical	B
datasets	O
between	O
these	O
two	O
extremes	O
there	O
is	O
a	O
variety	O
table	O
orders	O
the	O
datasets	O
by	O
the	O
number	O
of	O
machine	O
learning	O
statistical	B
or	O
neural	O
network	O
algorithms	O
that	O
are	O
in	O
the	O
top	O
five	O
from	O
inspection	O
of	O
the	O
frequencies	O
in	O
table	O
it	O
appears	O
that	O
neural	B
networks	I
and	O
statistical	B
procedures	O
do	O
well	O
on	O
the	O
same	O
kind	O
of	O
datasets	O
in	O
other	O
words	O
neural	O
nets	O
tend	O
to	O
do	O
well	O
when	O
statistical	B
procedures	O
do	O
well	O
and	O
vice	O
versa	O
as	O
an	O
objective	O
measure	B
of	O
this	O
tendency	O
a	O
correspondence	B
analysis	I
can	O
be	O
used	O
correspondence	B
analysis	I
attempts	O
analysis	B
of	I
results	I
table	O
top	O
five	O
algorithms	O
for	O
all	O
datasets	O
by	O
type	O
machine	O
learning	O
statistics	O
and	O
neural	O
net	O
dataset	O
kl	O
satim	O
vehic	O
head	O
heart	O
belg	O
segm	O
diab	O
cr	O
ger	O
chrom	O
cr	O
aus	O
shutt	O
dna	B
tech	O
newbel	O
isoft	O
tset	O
cr	O
man	O
letter	O
first	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
ml	O
ml	O
nn	O
ml	O
stat	O
ml	O
ml	O
ml	O
stat	O
stat	O
stat	O
second	O
third	O
stat	O
nn	O
nn	O
stat	O
stat	O
stat	O
nn	O
ml	O
stat	O
stat	O
stat	O
stat	O
ml	O
stat	O
ml	O
ml	O
stat	O
ml	O
ml	O
stat	O
ml	O
nn	O
stat	O
stat	O
nn	O
nn	O
nn	O
stat	O
stat	O
ml	O
nn	O
stat	O
nn	O
ml	O
ml	O
nn	O
ml	O
ml	O
nn	O
ml	O
stat	O
ml	O
nn	O
stat	O
fourth	O
nn	O
nn	O
nn	O
stat	O
stat	O
stat	O
nn	O
ml	O
stat	O
stat	O
nn	O
stat	O
ml	O
stat	O
ml	O
ml	O
stat	O
ml	O
stat	O
ml	O
ml	O
stat	O
fifth	O
nn	O
stat	O
stat	O
nn	O
ml	O
stat	O
stat	O
nn	O
nn	O
nn	O
stat	O
nn	O
ml	O
stat	O
ml	O
ml	O
nn	O
ml	O
ml	O
ml	O
nn	O
ml	O
to	O
give	O
scores	O
to	O
the	O
rows	O
datasets	O
and	O
columns	O
procedure	O
types	O
of	O
an	O
array	O
with	O
positive	O
entries	O
in	O
such	O
a	O
way	O
that	O
the	O
scores	O
are	O
mutually	O
consistent	O
and	O
maximally	O
correlated	O
for	O
a	O
description	O
of	O
correspondence	B
analysis	I
see	O
hill	O
and	O
mardia	O
et	O
al	O
it	O
turns	O
out	O
that	O
the	O
optimal	O
scores	O
for	O
columns	O
and	O
net	O
and	O
statistical	B
procedures	O
are	O
virtually	O
identical	O
but	O
these	O
are	O
quite	O
different	O
from	O
the	O
score	O
of	O
column	O
ml	O
procedures	O
it	O
would	O
appear	O
therefore	O
that	O
neural	O
nets	O
are	O
more	O
similar	O
to	O
statistical	B
procedures	O
than	O
to	O
ml	O
in	O
passing	O
we	O
may	O
note	O
that	O
the	O
optimal	O
scores	O
that	O
are	O
given	O
to	O
the	O
datasets	O
may	O
be	O
used	O
to	O
give	O
an	O
ordering	O
to	O
the	O
datasets	O
and	O
this	O
ordering	O
can	O
be	O
understood	O
as	O
a	O
measure	B
of	O
how	O
suited	O
the	O
dataset	O
is	O
to	O
ml	O
procedures	O
if	O
the	O
same	O
scores	O
are	O
allocated	O
to	O
neural	O
net	O
and	O
statistical	B
procedures	O
the	O
corresponding	O
ordering	O
of	O
the	O
datasets	O
is	O
exactly	O
that	O
given	O
in	O
the	O
table	O
with	O
datasets	O
at	O
the	O
bottom	O
being	O
more	O
of	O
type	O
ml	O
dominators	B
it	O
is	O
interesting	O
to	O
note	O
that	O
some	O
algorithms	O
always	O
do	O
better	O
than	O
the	O
default	B
the	O
datasets	O
we	O
have	O
looked	O
at	O
there	O
are	O
nine	O
such	O
discrim	B
logdisc	B
smart	B
k-nn	B
cart	B
and	O
cascade	B
these	O
algorithms	O
dominate	O
the	O
default	B
strategy	O
also	O
in	O
the	O
seven	O
datasets	O
on	O
which	O
cascade	B
was	O
run	O
itrule	B
is	O
dominated	O
by	O
cascade	B
the	O
only	O
other	O
case	O
of	O
an	O
algorithm	O
being	O
dominated	O
by	O
others	O
is	O
kohonen	B
it	O
sec	O
multidimensional	B
scaling	I
table	O
datasets	O
ordered	O
by	O
algorithm	O
type	O
datasets	O
at	O
the	O
top	O
are	O
most	O
suited	O
to	O
statistical	B
and	O
neural	O
net	O
procedures	O
datasets	O
at	O
the	O
bottom	O
most	O
suited	O
to	O
machine	O
learning	O
dataset	O
ml	O
nn	O
stat	O
heart	O
cr	O
ger	O
kl	O
vehic	O
belg	O
diab	O
chrom	O
dna	B
satim	O
head	O
letter	O
isoft	O
cr	O
aus	O
cr	O
man	O
segm	O
newbel	O
shutt	O
tech	O
tset	O
is	O
dominated	O
by	O
cascade	B
and	O
lvq	B
these	O
comparisons	O
do	O
not	O
include	O
datasets	O
where	O
results	O
is	O
missing	O
so	O
we	O
should	O
really	O
say	O
where	O
results	O
are	O
available	O
kohonen	B
is	O
always	O
worse	O
than	O
and	O
lvq	B
since	O
we	O
only	O
have	O
results	O
for	O
cascade	B
trials	O
the	O
comparison	O
cascade-kohonen	O
is	O
rather	O
meaningless	O
multidimensional	B
scaling	I
it	O
would	O
be	O
possible	O
to	O
combine	O
the	O
results	O
of	O
all	O
the	O
trials	O
to	O
rank	O
the	O
algorithms	O
by	O
overall	O
success	O
rate	O
or	O
average	O
success	O
rate	O
but	O
not	O
without	O
some	O
rather	O
arbitrary	O
assumptions	O
to	O
equate	O
error	O
rates	O
with	O
costs	B
we	O
do	O
not	O
attempt	O
to	O
give	O
such	O
an	O
ordering	O
as	O
we	O
believe	O
that	O
this	O
is	O
not	O
profitable	O
we	O
prefer	O
to	O
give	O
a	O
more	O
objective	O
approach	O
based	O
on	O
multidimensional	B
scaling	I
equivalent	O
procedure	O
would	O
be	O
correspondence	B
analysis	I
in	O
so	O
doing	O
the	O
aim	O
is	O
to	O
demonstrate	O
the	O
close	O
relationships	O
between	O
the	O
algorithms	O
and	O
at	O
the	O
same	O
time	B
the	O
close	O
similarities	O
between	O
many	O
of	O
the	O
datasets	O
multidimensional	B
scaling	I
has	O
no	O
background	O
theory	O
it	O
is	O
an	O
exploratory	O
tool	O
for	O
suggesting	O
relationships	O
in	O
data	O
rather	O
than	O
testing	O
pre-chosen	O
hypotheses	O
there	O
is	O
no	O
agreed	O
criterion	O
which	O
tells	O
us	O
if	O
the	O
scaling	O
is	O
successful	O
although	O
there	O
are	O
generally	O
accepted	O
guidelines	O
analysis	B
of	I
results	I
scaling	O
of	O
algorithms	O
to	O
apply	O
multidimensional	B
scaling	I
the	O
first	O
problem	O
paradoxically	O
is	O
to	O
scale	O
the	O
variables	O
the	O
idea	O
is	O
to	O
scale	O
the	O
error-rates	O
and	O
average	O
costs	B
in	O
such	O
a	O
way	O
that	O
each	O
dataset	O
carries	O
equal	O
weight	O
this	O
is	O
not	O
easy	O
in	O
each	O
dataset	O
we	O
rescaled	O
so	O
that	O
the	O
error-rate	O
average	O
cost	O
had	O
a	O
minimum	O
of	O
zero	O
and	O
a	O
maximum	O
of	O
unity	O
such	O
a	O
rescaling	O
is	O
arbitrary	O
and	O
can	O
only	O
be	O
justified	O
a	O
posteriori	O
insofar	O
as	O
the	O
results	O
confirm	O
known	O
relationships	O
once	O
the	O
initial	O
scaling	O
has	O
been	O
done	O
distances	O
between	O
all	O
pairs	O
of	O
algorithms	O
must	O
be	O
computed	O
distance	B
was	O
taken	O
to	O
be	O
the	O
euclidean	O
distance	B
between	O
the	O
rescaled	O
error-rates	O
on	O
the	O
datasets	O
this	O
results	O
in	O
a	O
distance	B
matrix	O
representing	O
distances	O
between	O
all	O
pairs	O
of	O
algorithms	O
in	O
space	O
the	O
distance	B
matrix	O
can	O
then	O
be	O
decomposed	O
by	O
an	O
orthogonal	O
decomposition	O
into	O
distances	O
in	O
a	O
reduced	O
space	O
most	O
conveniently	O
the	O
dimensions	O
of	O
the	O
reduced	O
space	O
are	O
chosen	O
to	O
be	O
two	O
so	O
that	O
the	O
algorithms	O
can	O
be	O
represented	O
as	O
points	O
in	O
a	O
plot	O
this	O
plot	O
is	O
given	O
in	O
figure	O
multidimensional	B
scaling	I
of	O
algorithms	O
datasets	O
cascade	B
logdiscr	O
discrim	B
quadisc	B
smart	B
bprop	O
knn	O
cart	B
baytree	O
lvq	B
rbf	B
indcart	B
kohonen	B
castle	B
e	O
t	O
i	O
a	O
n	O
d	O
r	O
o	O
o	O
c	O
g	O
n	O
i	O
l	O
a	O
c	O
s	O
d	O
n	O
o	O
c	O
e	O
s	O
newid	B
naivebay	O
itrule	B
first	O
scaling	O
coordinate	O
fig	O
multidimensional	B
scaling	I
representation	O
of	O
algorithms	O
in	O
the	O
space	O
dimension	O
is	O
an	O
error	B
rate	I
or	O
average	O
cost	O
measured	O
on	O
a	O
given	O
dataset	O
points	O
near	O
to	O
each	O
other	O
in	O
this	O
plot	O
are	O
not	O
necessarily	O
close	O
in	O
whether	O
the	O
plot	O
is	O
a	O
good	O
picture	O
of	O
space	O
can	O
be	O
judged	O
from	O
a	O
comparison	O
of	O
the	O
set	O
of	O
distances	O
in	O
compared	O
to	O
the	O
set	O
of	O
distances	O
in	O
one	O
simple	O
way	O
to	O
measure	B
the	O
goodness	O
of	O
the	O
representation	O
is	O
to	O
compare	O
the	O
total	O
squared	O
distances	O
let	O
be	O
the	O
total	O
of	O
the	O
squared	O
distances	O
taken	O
over	O
all	O
pairs	O
of	O
points	O
in	O
the	O
plot	O
and	O
let	O
be	O
the	O
total	O
squared	O
distances	O
over	O
all	O
pairs	O
of	O
points	O
in	O
the	O
stress	O
is	O
defined	O
to	O
be	O
for	O
figure	O
the	O
stress	O
figure	O
is	O
considering	O
the	O
number	O
of	O
initial	O
dimensions	O
is	O
very	O
high	O
this	O
is	O
a	O
reasonably	O
small	O
stress	O
although	O
we	O
should	O
say	O
that	O
conventionally	O
the	O
stress	O
is	O
sec	O
multidimensional	B
scaling	I
said	O
to	O
be	O
small	O
when	O
less	O
than	O
with	O
a	O
representation	O
the	O
stress	O
factor	O
would	O
be	O
indicating	O
that	O
it	O
would	O
be	O
more	O
sensible	O
to	O
think	O
of	O
algorithms	O
differing	O
in	O
at	O
least	O
a	O
three-dimensional	O
representation	O
would	O
raise	O
the	O
prospect	O
of	O
representing	O
all	O
results	O
in	O
terms	O
of	O
three	O
scaling	O
coordinates	O
which	O
might	O
be	O
interpretable	O
as	O
error-rates	O
of	O
three	O
notional	O
algorithms	O
because	O
the	O
stress	O
figure	O
is	O
low	O
relative	O
to	O
the	O
number	O
of	O
dimensions	O
points	O
near	O
each	O
other	O
in	O
figure	O
probably	O
represent	O
algorithms	O
that	O
are	O
similar	O
in	O
performance	O
for	O
example	B
the	O
machine	O
learning	O
methods	O
newid	B
and	O
indcart	B
are	O
very	O
close	O
to	O
each	O
other	O
and	O
in	O
general	O
all	O
the	O
machine	O
learning	O
procedures	O
are	O
close	O
in	O
figure	O
before	O
jumping	O
to	O
the	O
conclusion	O
that	O
they	O
are	O
indeed	O
similar	O
it	O
is	O
as	O
well	O
to	O
check	O
the	O
tables	O
of	O
results	O
the	O
stress	O
is	O
low	O
it	O
is	O
not	O
zero	O
so	O
the	O
distances	O
in	O
figure	O
are	O
approximate	O
only	O
looking	O
at	O
the	O
individual	O
tables	O
the	O
reader	O
should	O
see	O
that	O
for	O
example	B
newid	B
and	O
indcart	B
tend	O
to	O
come	O
at	O
about	O
the	O
same	O
place	O
in	O
every	O
table	O
apart	O
from	O
a	O
few	O
exceptions	O
so	O
strong	O
is	O
this	O
similarity	O
that	O
one	O
is	O
tempted	O
to	O
say	O
that	O
marked	O
deviations	O
from	O
this	O
general	O
pattern	O
should	O
be	O
regarded	O
with	O
suspicion	O
and	O
should	O
be	O
double	O
checked	O
hierarchical	B
clustering	B
of	O
algorithms	O
hierarchical	B
clustering	B
algorithms	O
datasets	O
i	O
c	O
s	O
d	O
a	O
u	O
q	O
l	O
e	O
u	O
r	O
t	O
i	O
e	O
l	O
t	O
s	O
a	O
c	O
y	O
a	O
b	O
e	O
v	O
a	O
n	O
i	O
q	O
v	O
l	O
n	O
e	O
n	O
o	O
h	O
o	O
k	O
n	O
n	O
k	O
c	O
o	O
l	O
l	O
a	O
f	O
b	O
r	O
c	O
c	O
a	O
m	O
i	O
r	O
c	O
s	O
d	O
i	O
i	O
r	O
c	O
s	O
d	O
g	O
o	O
l	O
t	O
r	O
a	O
m	O
s	O
e	O
d	O
a	O
c	O
s	O
a	O
c	O
t	O
r	O
a	O
c	O
l	O
a	O
c	O
p	O
o	O
r	O
p	O
b	O
l	O
o	O
p	O
d	O
i	O
e	O
e	O
r	O
t	O
y	O
a	O
b	O
t	O
r	O
a	O
c	O
d	O
n	O
i	O
n	O
c	O
i	O
d	O
w	O
e	O
n	O
fig	O
hierarchical	B
clustering	B
of	O
algorithms	O
using	O
standardised	O
error	O
rates	O
and	O
costs	B
there	O
is	O
another	O
way	O
to	O
look	O
at	O
relationships	O
between	O
the	O
algorithms	O
based	O
on	O
the	O
set	O
of	O
paired	O
distances	O
namely	O
by	O
a	O
hierarchical	B
clustering	B
of	O
the	O
algorithms	O
the	O
resulting	O
figure	O
does	O
indeed	O
capture	O
known	O
similarities	O
and	O
logistic	O
discriminants	O
are	O
very	O
close	O
and	O
is	O
very	O
suggestive	O
of	O
other	O
relationships	O
it	O
is	O
to	O
be	O
expected	O
that	O
some	O
of	O
the	O
similarities	O
picked	O
up	O
by	O
the	O
clustering	B
procedure	O
analysis	B
of	I
results	I
will	O
be	O
accidental	O
in	O
any	O
case	O
algorithms	O
should	O
not	O
be	O
declared	O
as	O
similar	O
on	O
the	O
basis	O
of	O
empirical	O
evidence	O
alone	O
and	O
true	O
understanding	O
of	O
the	O
relationships	O
will	O
follow	O
only	O
when	O
theoretical	O
grounds	O
are	O
found	O
for	O
similarities	O
in	O
behaviour	O
finally	O
we	O
should	O
say	O
something	O
about	O
some	O
dissimilarities	O
there	O
are	O
some	O
surprising	O
errors	O
in	O
the	O
clusterings	O
of	O
figure	O
for	O
example	B
cart	B
and	O
indcart	B
are	O
attached	O
to	O
slightly	O
different	O
clusterings	O
this	O
is	O
a	O
major	O
surprise	O
and	O
we	O
do	O
have	O
ideas	O
on	O
why	O
this	O
is	O
indeed	O
true	O
but	O
nonetheless	O
cart	B
and	O
indcart	B
were	O
grouped	O
together	O
in	O
tables	O
to	O
facilitate	O
comparisons	O
between	O
the	O
two	O
scaling	O
of	O
datasets	O
the	O
same	O
set	O
of	O
re-scaled	O
error	O
rates	O
may	O
be	O
used	O
to	O
give	O
a	O
plot	O
of	O
datasets	O
from	O
a	O
formal	O
point	O
of	O
view	O
the	O
multidimensional	B
scaling	I
procedure	O
is	O
applied	O
to	O
the	O
transpose	O
of	O
the	O
matrix	O
of	O
re-scaled	O
error	O
rates	O
the	O
default	B
algorithm	O
was	O
excluded	O
from	O
this	O
exercise	O
as	O
distances	O
from	O
this	O
to	O
the	O
other	O
algorithms	O
were	O
going	O
to	O
dominate	O
the	O
picture	O
multidimensional	B
scaling	I
of	O
datasets	O
algorithms	O
e	O
t	O
i	O
a	O
n	O
d	O
r	O
o	O
o	O
c	O
g	O
n	O
i	O
l	O
a	O
c	O
s	O
d	O
n	O
o	O
c	O
e	O
s	O
cr	O
aus	O
ml	O
belg	O
stat	O
isoft	O
ml	O
diab	O
stat	O
cr	O
ger	O
stat	O
head	O
stat	O
heart	O
stat	O
cr	O
man	O
nn	O
newbel	O
stat	O
dna	B
nn	O
vehic	O
stat	O
satim	O
stat	O
chrom	O
stat	O
tech	O
ml	O
shutt	O
ml	O
segm	O
stat	O
ml	O
stat	O
tset	O
ml	O
letter	O
stat	O
stat	O
kl	O
stat	O
first	O
scaling	O
coordinate	O
fig	O
multidimensional	B
scaling	I
representation	O
of	O
the	O
datasets	O
in	O
space	O
dimension	O
is	O
an	O
error	B
rate	I
and	O
cost	O
achieved	O
by	O
a	O
particular	O
algorithms	O
the	O
symbols	O
ml	O
nn	O
and	O
stat	O
below	O
each	O
dataset	O
indicate	O
which	O
type	O
of	O
algorithm	O
achieved	O
the	O
lowest	O
error-rate	O
or	O
cost	O
on	O
that	O
dataset	O
datasets	O
near	O
to	O
each	O
other	O
in	O
this	O
plot	O
are	O
not	O
necessarily	O
close	O
in	O
figure	O
is	O
a	O
multidimensional	B
scaling	I
representation	O
of	O
the	O
error	O
rates	O
and	O
costs	B
given	O
in	O
tables	O
each	O
dataset	O
in	O
tables	O
is	O
described	O
by	O
a	O
point	O
in	O
space	O
the	O
coordinates	O
of	O
which	O
are	O
the	O
error	O
rates	O
or	O
costs	B
of	O
the	O
various	O
algorithms	O
to	O
help	O
visualise	O
the	O
relationships	O
between	O
the	O
points	O
they	O
have	O
been	O
projected	O
down	O
to	O
in	O
such	O
a	O
way	O
as	O
to	O
preserve	O
their	O
mutual	O
sec	O
multidimensional	B
scaling	I
distances	O
as	O
much	O
as	O
possible	O
this	O
projection	O
is	O
fairly	O
successful	O
as	O
the	O
stress	O
factor	O
is	O
only	O
value	O
of	O
is	O
regarded	O
as	O
excellent	O
a	O
value	O
of	O
is	O
good	O
again	O
a	O
representation	O
might	O
be	O
more	O
acceptable	O
with	O
a	O
stress	O
factor	O
of	O
such	O
a	O
representation	O
could	O
be	O
interpreted	O
as	O
saying	O
that	O
datasets	O
differ	O
in	O
three	O
essentially	O
orthogonal	O
ways	O
and	O
is	O
suggestive	O
of	O
a	O
description	O
of	O
datasets	O
using	O
just	O
three	O
measures	B
this	O
idea	O
is	O
explored	O
further	O
in	O
the	O
next	O
subsection	O
several	O
interesting	O
similarities	O
are	O
obvious	O
from	O
figure	O
the	O
costs	B
datasets	O
are	O
close	O
to	O
each	O
other	O
as	O
are	O
the	O
two	O
types	O
of	O
image	B
datasets	I
in	O
addition	O
the	O
credit	B
datasets	I
are	O
all	O
at	O
the	O
top	O
of	O
the	O
diagram	O
for	O
the	O
german	B
credit	I
data	O
which	O
involves	O
costs	B
the	O
two	O
pathologically	O
partitioned	O
datasets	O
shuttle	B
and	O
technical	B
are	O
together	O
at	O
the	O
extreme	O
top	O
right	O
of	O
the	O
diagram	O
in	O
view	O
of	O
these	O
similarities	O
it	O
is	O
tempting	O
to	O
classify	O
datasets	O
of	O
unknown	O
origin	O
by	O
their	O
proximities	O
to	O
other	O
datasets	O
of	O
known	O
provenance	O
for	O
example	B
the	O
diabetes	B
dataset	I
is	O
somewhere	O
between	O
a	O
partitioning	O
type	O
dataset	O
credit	O
data	O
and	O
a	O
prediction	O
type	O
dataset	O
head	B
injury	I
interpretation	O
of	O
scaling	O
coordinates	O
the	O
plotting	O
coordinates	O
for	O
the	O
description	O
of	O
datasets	O
in	O
figure	O
are	O
derived	O
by	O
orthogonal	O
transformation	B
of	O
the	O
original	O
error	O
ratescosts	O
these	O
coordinates	O
clearly	O
represent	O
distinctive	O
features	B
of	O
the	O
datasets	O
as	O
similar	O
datasets	O
are	O
grouped	O
together	O
in	O
the	O
diagram	O
this	O
suggests	O
either	O
that	O
the	O
scaling	O
coordinates	O
might	O
be	O
used	O
as	O
characteristics	O
of	O
the	O
datasets	O
or	O
equivalently	O
might	O
be	O
related	O
to	O
characteristics	O
of	O
the	O
datasets	O
this	O
suggests	O
that	O
we	O
look	O
at	O
these	O
coordinates	O
and	O
try	O
to	O
relate	O
them	O
to	O
the	O
dataset	O
measures	B
that	O
we	O
defined	O
in	O
chapter	O
for	O
example	B
it	O
turns	O
out	O
that	O
the	O
first	O
scaling	O
coordinate	O
is	O
positively	O
correlated	O
with	O
the	O
number	O
of	O
examples	O
in	O
the	O
dataset	O
in	O
figure	O
this	O
means	O
that	O
there	O
is	O
a	O
tendency	O
for	O
the	O
larger	O
datasets	O
to	O
lie	O
to	O
the	O
right	O
of	O
the	O
diagram	O
the	O
second	O
is	O
the	O
number	O
of	O
classes	B
this	O
implies	O
that	O
a	O
dataset	O
with	O
small	O
kurtosis	B
and	O
large	O
number	O
of	O
classes	B
will	O
tend	O
to	O
lie	O
in	O
the	O
bottom	O
half	O
of	O
figure	O
however	O
the	O
correlations	O
are	O
quite	O
weak	O
and	O
in	O
any	O
case	O
only	O
relate	O
to	O
a	O
subspace	O
of	O
two	O
dimensions	O
with	O
a	O
stress	O
of	O
so	O
we	O
cannot	O
say	O
that	O
these	O
measures	B
capture	O
the	O
essential	O
differences	O
between	O
datasets	O
scaling	O
coordinate	O
is	O
correlated	O
with	O
the	O
curious	O
ratio	O
kurtosis	B
where	O
that	O
particular	O
dataset	O
for	O
example	B
the	O
algorithm	O
type	O
ml	O
comes	O
out	O
top	O
on	O
the	O
best	O
algorithms	O
for	O
datasets	O
in	O
figure	O
each	O
dataset	O
has	O
been	O
labelled	O
by	O
the	O
type	O
of	O
algorithm	O
that	O
does	O
best	O
on	O
faults	O
dataset	O
so	O
the	O
dataset	O
faults	O
has	O
the	O
label	O
ml	O
attached	O
inspecting	O
figure	O
a	O
very	O
clear	O
pattern	O
emerges	O
machine	O
learning	O
procedures	O
generally	O
do	O
best	O
on	O
datasets	O
at	O
the	O
top	O
or	O
at	O
the	O
extreme	O
right	O
of	O
the	O
diagram	O
statistical	B
and	O
neural	O
network	O
procedures	O
do	O
best	O
on	O
datasets	O
in	O
the	O
lower	O
half	O
and	O
to	O
the	O
left	O
of	O
the	O
diagram	O
of	O
course	O
this	O
pattern	O
is	O
very	O
closely	O
related	O
to	O
the	O
fact	O
that	O
datasets	O
from	O
particular	O
application	O
areas	O
are	O
clustered	O
together	O
in	O
the	O
spirit	O
of	O
correspondence	B
analysis	I
it	O
would	O
be	O
possible	O
to	O
use	O
the	O
scaling	O
coordinates	O
of	O
datasets	O
or	O
algorithms	O
to	O
come	O
up	O
with	O
a	O
mutually	O
consistent	O
set	O
of	O
coordinates	O
that	O
express	O
the	O
relationships	O
between	O
datasets	O
and	O
algorithms	O
this	O
can	O
be	O
done	O
but	O
there	O
are	O
too	O
many	O
missing	B
values	I
in	O
the	O
tables	O
for	O
the	O
usual	O
version	O
of	O
correspondence	B
analysis	I
missing	B
values	I
allowed	O
analysis	B
of	I
results	I
clustering	B
of	O
datasets	O
starting	O
from	O
the	O
distances	O
in	O
a	O
standard	O
clustering	B
algorithm	O
the	O
furthest	O
neighbour	O
option	O
gives	O
the	O
clustering	B
of	O
figure	O
hierarchical	B
clustering	B
datasets	O
on	O
algorithms	O
a	O
n	O
d	O
t	O
t	O
u	O
h	O
s	O
h	O
c	O
e	O
t	O
m	O
o	O
r	O
h	O
c	O
t	O
r	O
a	O
e	O
dh	O
a	O
e	O
h	O
r	O
e	O
g	O
r	O
c	O
m	O
i	O
t	O
a	O
s	O
r	O
e	O
t	O
t	O
e	O
l	O
t	O
e	O
s	O
t	O
m	O
g	O
e	O
s	O
t	O
u	O
c	O
t	O
u	O
c	O
i	O
c	O
h	O
e	O
v	O
b	O
a	O
d	O
i	O
s	O
u	O
a	O
r	O
c	O
l	O
g	O
e	O
b	O
t	O
f	O
o	O
s	O
i	O
l	O
e	O
b	O
w	O
e	O
n	O
n	O
a	O
m	O
r	O
c	O
l	O
k	O
g	O
d	O
i	O
fig	O
hierarchical	B
clustering	B
of	O
datasets	O
based	O
on	O
standardised	O
error	O
rates	O
and	O
costs	B
performance	O
related	O
to	O
measures	B
theoretical	O
there	O
are	O
very	O
few	O
theoretical	O
indicators	O
for	O
algorithm	O
accuracy	B
what	O
little	O
there	O
are	O
make	O
specific	O
distributional	O
assumptions	O
and	O
the	O
only	O
question	O
is	O
whether	O
these	O
specific	O
assumptions	O
are	O
valid	O
in	O
such	O
cases	O
it	O
is	O
possible	O
to	O
build	O
checks	O
into	O
the	O
algorithm	O
that	O
give	O
an	O
indication	O
if	O
the	O
assumptions	O
are	O
valid	O
normal	O
distributions	O
the	O
statistical	B
measures	B
were	O
defined	O
in	O
section	O
with	O
a	O
view	O
to	O
monitoring	O
the	O
success	O
of	O
the	O
two	O
discriminant	O
procedures	O
that	O
are	O
associated	O
with	O
the	O
normal	B
distribution	I
namely	O
linear	O
and	O
quadratic	B
discriminants	I
within	O
the	O
class	B
of	O
normal	O
distributions	O
the	O
measure	B
provides	O
a	O
guide	O
as	O
to	O
the	O
relative	O
suitability	O
of	O
linear	O
and	O
quadratic	O
if	O
sample	O
sizes	O
are	O
so	O
large	O
that	O
covariance	B
matrices	O
can	O
be	O
accurately	O
discrimination	B
measured	O
it	O
would	O
be	O
legitimate	O
to	O
use	O
the	O
quadratic	O
version	O
exclusively	O
as	O
it	O
reduces	O
to	O
the	O
linear	O
rule	O
in	O
the	O
special	O
case	O
of	O
equality	O
of	O
covariances	O
practically	O
speaking	O
the	O
advice	O
must	O
be	O
reversed	O
use	O
linear	O
discriminants	O
unless	O
the	O
sample	O
size	O
is	O
very	O
large	O
the	O
distribution	O
is	O
known	O
to	O
be	O
nearly	O
normal	O
and	O
the	O
covariances	O
are	O
very	O
different	O
so	O
we	O
consider	O
now	O
when	O
to	O
use	O
quadratic	B
discriminants	I
it	O
should	O
be	O
noted	O
that	O
this	O
advice	O
is	O
absolute	O
in	O
the	O
sense	O
that	O
it	O
is	O
based	O
only	O
on	O
measures	B
related	O
to	O
the	O
dataset	O
sec	O
performance	O
related	O
to	O
measures	B
theoretical	O
absolute	O
performance	O
quadratic	B
discriminants	I
in	O
theory	O
quadratic	O
discrimination	B
is	O
the	O
best	O
procedure	O
to	O
use	O
when	O
the	O
data	O
are	O
normally	O
distributed	O
especially	O
so	O
if	O
the	O
covariances	O
differ	O
because	O
it	O
makes	O
very	O
specific	O
distributional	O
assumptions	O
and	O
so	O
is	O
very	O
efficient	O
for	O
normal	O
distributions	O
it	O
is	O
inadvisable	O
to	O
use	O
quadratic	O
discrimination	B
for	O
non-normal	O
distributions	O
common	O
situation	O
with	O
parametric	O
procedures	O
they	O
are	O
not	O
robust	O
to	O
departures	O
from	O
the	O
assumptions	O
and	O
because	O
it	O
uses	O
many	O
more	O
parameters	O
it	O
is	O
also	O
not	O
advisable	O
to	O
use	O
quadratic	O
discrimination	B
when	O
the	O
sample	O
sizes	O
are	O
small	O
we	O
will	O
now	O
relate	O
these	O
facts	O
to	O
our	O
measures	B
for	O
the	O
datasets	O
tributed	O
dataset	O
with	O
widely	O
differing	O
covariance	B
matrices	O
the	O
ideal	O
dataset	O
for	O
quadratic	O
discrimination	B
would	O
be	O
a	O
very	O
large	O
normally	O
disin	O
terms	O
of	O
the	O
measures	B
kurtosis	B
and	O
sd	O
ratio	O
much	O
greater	O
than	O
unity	O
skewness	B
skewness	B
kurtosis	B
this	O
is	O
near	O
and	O
most	O
importantly	O
sd	O
ratio	O
this	O
is	O
much	O
greater	O
than	O
unity	O
this	O
dataset	O
is	O
nearest	O
ideal	O
so	O
it	O
is	O
predictable	O
that	O
quadratic	O
discrimination	B
will	O
achieve	O
the	O
lowest	O
error	B
rate	I
in	O
fact	O
quadratic	B
discriminants	I
achieve	O
an	O
error	B
rate	I
of	O
and	O
this	O
is	O
only	O
bettered	O
by	O
k-nn	B
with	O
an	O
error	B
rate	I
of	O
and	O
by	O
with	O
an	O
error	B
rate	I
of	O
ideally	O
we	O
want	O
the	O
most	O
normal	O
dataset	O
in	O
our	O
study	O
is	O
the	O
kl	B
digits	B
dataset	I
as	O
this	O
is	O
small	O
skewness	B
large	O
kurtosis	B
near	O
and	O
to	O
make	O
matters	O
worse	O
the	O
sd	O
ratio	O
this	O
is	O
not	O
much	O
greater	O
than	O
unity	O
therefore	O
we	O
can	O
predict	O
that	O
this	O
is	O
the	O
least	O
appropriate	O
dataset	O
for	O
quadratic	O
discrimination	B
and	O
it	O
is	O
no	O
surprise	O
that	O
quadratic	B
discriminants	I
achieve	O
an	O
error	B
rate	I
of	O
which	O
is	O
worst	O
of	O
all	O
our	O
results	O
for	O
the	O
shuttle	B
dataset	I
the	O
decision	O
tree	O
methods	O
get	O
error	O
rates	O
smaller	O
than	O
this	O
by	O
a	O
factor	O
of	O
at	O
the	O
other	O
extreme	O
the	O
least	O
normal	O
dataset	O
is	O
probably	O
the	O
shuttle	B
dataset	I
with	O
the	O
important	O
proviso	O
should	O
always	O
be	O
borne	O
in	O
mind	O
that	O
there	O
must	O
be	O
enough	O
data	O
to	O
estimate	O
all	O
parameters	O
accurately	O
relative	O
performance	O
logdisc	B
vs	O
another	O
fruitful	O
way	O
of	O
looking	O
at	O
the	O
behaviour	O
of	O
algorithms	O
is	O
by	O
making	O
paired	O
comparisons	O
between	O
closely	O
related	O
algorithms	O
this	O
extremely	O
useful	O
device	O
is	O
best	O
illustrated	O
by	O
comparing	O
logistic	B
discrimination	B
and	O
from	O
their	O
construction	O
we	O
can	O
see	O
that	O
and	O
logistic	B
discrimination	B
have	O
exactly	O
the	O
same	O
formal	O
decision	O
procedure	O
in	O
one	O
special	O
case	O
namely	O
the	O
case	O
of	O
two-class	O
problems	O
in	O
which	O
there	O
is	O
no	O
clustering	B
both	O
classes	B
are	O
pure	O
where	O
the	O
two	O
differ	O
then	O
will	O
be	O
in	O
multi-class	O
problems	O
as	O
the	O
digits	O
or	O
letters	O
datasets	O
or	O
in	O
two-class	O
problems	O
in	O
which	O
the	O
classes	B
are	O
impure	B
as	O
the	O
belgian	O
power	O
dataset	O
with	O
this	O
in	O
mind	O
it	O
is	O
of	O
interest	O
to	O
compare	O
the	O
performance	O
of	O
when	O
it	O
does	O
not	O
use	O
clustering	B
with	O
the	O
performance	O
of	O
logistic	B
discrimination	B
as	O
is	O
done	O
in	O
table	O
the	O
accuraciesaverage	O
costs	B
quoted	O
for	O
logistic	B
discrimination	B
are	O
those	O
in	O
the	O
main	O
tables	O
of	O
chapter	O
those	O
quoted	O
for	O
are	O
for	O
the	O
no-clustering	O
version	O
of	O
dipol	O
and	O
so	O
are	O
different	O
in	O
general	O
from	O
those	O
in	O
the	O
main	O
tables	O
either	O
in	O
table	O
or	O
in	O
the	O
main	O
tables	O
it	O
is	O
clear	O
that	O
sometimes	O
one	O
procedure	O
is	O
better	O
and	O
sometimes	O
the	O
other	O
from	O
what	O
is	O
known	O
about	O
the	O
algorithms	O
however	O
we	O
should	O
look	O
at	O
the	O
two-class	O
problems	O
separately	O
and	O
if	O
this	O
is	O
done	O
a	O
pattern	O
emerges	O
indeed	O
from	O
table	O
it	O
analysis	B
of	I
results	I
table	O
logistic	O
discriminants	O
vs	O
with	O
no	O
clustering	B
dataset	O
belgian	O
chromosome	O
credit	O
aus	O
credit	O
ger	O
credit	O
man	O
dna	B
diabetes	B
faults	O
kl	O
digit	O
letter	O
new	O
belg	O
sat	O
image	B
segmentation	I
shuttle	B
technical	B
tsetse	O
vehicle	B
logdisc	B
clustering	B
no	O
classes	B
seems	O
that	O
generally	O
logdisc	B
is	O
better	O
than	O
for	O
two-class	O
problems	O
knowing	O
this	O
we	O
can	O
look	O
back	O
at	O
the	O
main	O
tables	O
and	O
come	O
to	O
the	O
following	O
conclusions	O
about	O
the	O
relative	O
performance	O
of	O
logdisc	B
and	O
rules	O
comparing	O
logdisc	B
to	O
we	O
can	O
summarise	O
our	O
conclusions	O
viz-a-viz	O
logistic	O
and	O
dipol	O
by	O
the	O
following	O
rules	O
which	O
amount	O
to	O
saying	O
that	O
is	O
usually	O
better	O
than	O
logdisc	B
except	O
for	O
the	O
cases	O
stated	O
if	O
number	O
of	O
examples	O
is	O
small	O
or	O
if	O
cost	B
matrix	I
involved	O
or	O
if	O
number	O
of	O
classes	B
and	O
if	O
no	O
distinct	O
clusters	O
within	O
classes	B
then	O
logdisc	B
is	O
better	O
than	O
else	O
is	O
better	O
than	O
logdisc	B
pruning	B
of	O
decision	B
trees	I
this	O
section	O
looks	O
at	O
a	O
small	O
subset	O
of	O
the	O
trials	O
relating	O
to	O
decision	O
tree	O
methods	O
the	O
specific	O
aim	O
is	O
to	O
illustrate	O
how	O
error	B
rate	I
cost	O
is	O
related	O
to	O
the	O
complexity	O
of	O
nodes	O
of	O
the	O
decision	O
tree	O
there	O
is	O
no	O
obvious	O
way	O
of	O
telling	O
if	O
the	O
error-rate	O
of	O
a	O
decision	O
tree	O
is	O
near	O
optimal	O
indeed	O
the	O
whole	O
question	O
of	O
what	O
is	O
to	O
be	O
optimised	O
is	O
a	O
very	O
open	O
one	O
in	O
practice	O
a	O
sec	O
performance	O
related	O
to	O
measures	B
theoretical	O
hypothetical	O
error	O
rates	O
for	O
three	O
algorithms	O
dataset	O
i	O
dataset	O
ii	O
r	O
o	O
r	O
r	O
e	O
nodes	O
fig	O
hypothetical	O
dependence	O
of	O
error	O
rates	O
on	O
number	O
of	O
end	O
nodes	O
so	O
on	O
pruning	B
for	O
three	O
algorithms	O
on	O
two	O
datasets	O
balance	O
must	O
be	O
struck	O
between	O
conflicting	O
criteria	O
one	O
way	O
of	O
achieving	O
a	O
balance	O
is	O
the	O
use	O
of	O
cost-complexity	O
as	O
a	O
criterion	O
as	O
is	O
done	O
by	O
breiman	O
et	O
al	O
this	O
balances	O
complexity	O
of	O
the	O
tree	O
against	O
the	O
error	B
rate	I
and	O
is	O
used	O
in	O
their	O
cart	B
procedure	O
as	O
a	O
criterion	O
for	O
pruning	B
the	O
decision	O
tree	O
all	O
the	O
decision	B
trees	I
in	O
this	O
project	O
incorporate	O
some	O
kind	O
of	O
pruning	B
and	O
the	O
extent	O
of	O
pruning	B
is	O
controlled	O
by	O
a	O
parameter	O
generally	O
a	O
tree	O
that	O
is	O
overpruned	O
has	O
too	O
high	O
an	O
error	B
rate	I
because	O
the	O
decision	O
tree	O
does	O
not	O
represent	O
the	O
full	O
structure	O
of	O
the	O
dataset	O
and	O
the	O
tree	O
is	O
biased	O
on	O
the	O
other	O
hand	O
a	O
tree	O
that	O
is	O
not	O
pruned	O
has	O
too	O
much	O
random	O
variation	O
in	O
the	O
allocation	O
of	O
examples	O
in	O
between	O
these	O
two	O
extremes	O
there	O
is	O
usually	O
an	O
optimal	O
amount	O
of	O
pruning	B
if	O
an	O
investigator	O
is	O
prepared	O
to	O
spend	O
some	O
time	B
trying	O
different	O
values	O
of	O
this	O
pruning	B
parameter	O
and	O
the	O
error-rate	O
is	O
tested	O
against	O
an	O
independent	O
test	B
set	I
the	O
optimal	O
amount	O
of	O
pruning	B
can	O
be	O
found	O
by	O
plotting	O
the	O
error	B
rate	I
against	O
the	O
pruning	B
parameter	O
equivalently	O
the	O
error-rate	O
may	O
be	O
plotted	O
against	O
the	O
number	O
of	O
end	O
nodes	O
usually	O
the	O
error	B
rate	I
drops	O
quite	O
quickly	O
to	O
its	O
minimum	O
value	O
as	O
the	O
number	O
of	O
nodes	O
increases	O
increasing	O
slowly	O
as	O
the	O
nodes	O
increase	O
beyond	O
the	O
optimal	O
value	O
the	O
number	O
of	O
end	O
nodes	O
is	O
an	O
important	O
measure	B
of	O
the	O
complexity	O
of	O
a	O
decision	O
tree	O
if	O
the	O
decision	O
tree	O
achieves	O
something	O
near	O
the	O
optimal	O
error-rate	O
the	O
number	O
of	O
end	O
nodes	O
is	O
also	O
measure	B
of	O
the	O
complexity	O
of	O
the	O
dataset	O
although	O
it	O
is	O
not	O
to	O
be	O
expected	O
that	O
all	O
decision	B
trees	I
will	O
achieve	O
their	O
optimal	O
error-rates	O
with	O
the	O
same	O
number	O
of	O
end-nodes	O
it	O
seems	O
reasonable	O
that	O
most	O
decision	B
trees	I
will	O
achieve	O
their	O
optimal	O
error-rates	O
when	O
the	O
number	O
of	O
end-nodes	O
matches	O
the	O
complexity	O
of	O
the	O
dataset	O
considerations	O
like	O
these	O
lead	O
us	O
to	O
expect	O
that	O
the	O
error-rates	O
of	O
different	O
algorithms	O
analysis	B
of	I
results	I
on	O
the	O
same	O
dataset	O
will	O
behave	O
as	O
sketched	O
in	O
figure	O
to	O
achieve	O
some	O
kind	O
of	O
comparability	O
between	O
datasets	O
all	O
the	O
curves	O
for	O
one	O
dataset	O
can	O
be	O
moved	O
horizontally	O
and	O
vertically	O
on	O
the	O
logarithmic	O
scale	O
this	O
amounts	O
to	O
rescaling	O
all	O
the	O
results	O
on	O
that	O
dataset	O
so	O
that	O
the	O
global	O
minimum	O
error	B
rate	I
is	O
unity	O
and	O
the	O
number	O
of	O
nodes	O
at	O
the	O
global	O
minimum	O
is	O
unity	O
when	O
no	O
attempt	O
is	O
made	O
to	O
optimise	O
the	O
amount	O
of	O
pruning	B
we	O
resort	O
to	O
the	O
following	O
plausible	O
argument	O
to	O
compare	O
algorithms	O
consider	O
for	O
example	B
the	O
dataset	O
four	O
algorithms	O
were	O
tested	O
with	O
very	O
widely	O
differing	O
error	O
rates	O
and	O
nodes	O
as	O
shown	O
in	O
table	O
as	O
the	O
lowest	O
error	B
rate	I
is	O
achieved	O
by	O
make	O
everything	O
relative	O
to	O
so	O
that	O
the	O
relative	O
number	O
opt	O
of	O
nodes	O
and	O
relative	O
error	O
rates	O
opt	O
are	O
given	O
in	O
table	O
these	O
standardised	O
results	O
for	O
the	O
dataset	O
are	O
plotted	O
in	O
figure	O
along	O
table	O
error	O
rates	O
and	O
number	O
of	O
end	O
nodes	O
for	O
four	O
decision	B
trees	I
on	O
the	O
dataset	O
note	O
that	O
achieves	O
the	O
lowest	O
error	B
rate	I
so	O
we	O
speculate	O
that	O
the	O
optimal	O
number	O
of	O
end	O
nodes	O
for	O
decision	B
trees	I
is	O
about	O
algorithm	O
no	O
end	O
nodes	O
error	B
rate	I
newid	B
newid	B
that	O
are	O
not	O
near	O
this	O
optimal	O
point	O
note	O
that	O
appears	O
most	O
frequently	O
in	O
the	O
left	O
of	O
the	O
figure	O
it	O
has	O
less	O
table	O
error	O
rates	O
and	O
number	O
of	O
end	O
nodes	O
for	O
four	O
algorithms	O
relative	O
to	O
the	O
values	O
for	O
with	O
standardised	O
results	O
from	O
other	O
datasets	O
for	O
which	O
we	O
had	O
the	O
relevant	O
information	O
with	O
the	O
name	O
of	O
the	O
algorithm	O
as	O
label	O
of	O
course	O
each	O
dataset	O
will	O
give	O
rise	O
to	O
at	O
least	O
algorithm	O
and	O
but	O
we	O
are	O
here	O
concerned	O
with	O
the	O
results	O
appear	O
most	O
frequently	O
in	O
the	O
are	O
biased	O
often	O
use	O
very	O
one	O
point	O
with	O
nodes	O
than	O
the	O
best	O
algorithm	O
and	O
both	O
newid	B
and	O
deliberately	O
to	O
obtain	O
trees	O
with	O
simple	O
structure	O
whereas	O
newid	B
and	O
has	O
struck	O
the	O
right	O
balance	O
but	O
it	O
does	O
seem	O
clear	O
that	O
newid	B
and	O
right	O
of	O
the	O
diagram	O
they	O
have	O
too	O
many	O
nodes	O
it	O
would	O
also	O
appear	O
that	O
is	O
most	O
likely	O
to	O
use	O
the	O
best	O
number	O
of	O
nodes	O
and	O
this	O
is	O
very	O
indirect	O
evidence	O
that	O
the	O
amount	O
of	O
pruning	B
used	O
by	O
is	O
correct	O
on	O
average	O
although	O
this	O
conclusion	O
is	O
based	O
on	O
a	O
small	O
number	O
of	O
datasets	O
one	O
would	O
expect	O
that	O
a	O
well-trained	O
procedure	O
should	O
attain	O
the	O
optimal	O
number	O
of	O
nodes	O
on	O
average	O
but	O
it	O
is	O
clear	O
that	O
is	O
biased	O
towards	O
small	O
numbers	O
may	O
be	O
done	O
towards	O
more	O
complex	B
trees	O
in	O
the	O
absence	O
of	O
information	O
on	O
the	O
relative	O
weights	O
to	O
be	O
attached	O
to	O
complexity	O
of	O
nodes	O
or	O
cost	O
rate	O
we	O
cannot	O
say	O
whether	O
sec	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
excess	O
error	B
rate	I
vs	O
excess	O
no	O
nodes	O
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
newid	B
o	O
i	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
node	O
ratio	O
fig	O
error	B
rate	I
and	O
number	O
of	O
nodes	O
for	O
datasets	O
results	O
for	O
each	O
dataset	O
are	O
scaled	O
separately	O
so	O
that	O
the	O
algorithm	O
with	O
lowest	O
error	B
rate	I
on	O
that	O
dataset	O
has	O
unit	O
error	B
rate	I
and	O
unit	O
number	O
of	O
nodes	O
complex	B
structures	O
with	O
no	O
compensation	O
in	O
reduced	O
error	B
rate	I
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
many	O
different	O
statistical	B
and	O
machine	O
learning	O
algorithms	O
have	O
been	O
developed	O
in	O
the	O
past	O
if	O
we	O
are	O
interested	O
in	O
applying	O
these	O
algorithms	O
to	O
concrete	O
tasks	O
we	O
have	O
to	O
consider	O
which	O
learning	O
algorithm	O
is	O
best	O
suited	O
for	O
which	O
problem	O
a	O
satisfactory	O
answer	O
requires	O
a	O
certain	O
know-how	O
of	O
this	O
area	O
which	O
can	O
be	O
acquired	O
only	O
with	O
experience	O
we	O
consider	O
here	O
if	O
machine	O
learning	O
techniques	O
themselves	O
can	O
be	O
useful	O
in	O
organizing	O
this	O
knowledge	O
specifically	O
the	O
knowledge	O
embedded	O
in	O
the	O
empirical	O
results	O
of	O
the	O
statlog	B
trials	O
the	O
aim	O
is	O
to	O
relate	O
the	O
performance	O
of	O
algorithms	O
to	O
the	O
characteristics	O
of	O
the	O
datasets	O
using	O
only	O
the	O
empirical	O
data	O
the	O
process	O
of	O
generating	O
a	O
set	O
of	O
rules	O
capable	O
of	O
relating	O
these	O
two	O
concepts	O
is	O
referred	O
to	O
as	O
meta-level	O
learning	O
objectives	B
it	O
appears	O
that	O
datasets	O
can	O
be	O
characterised	O
using	O
certain	O
features	B
such	O
as	O
number	O
of	O
attributes	B
their	O
types	O
amount	O
of	O
unknown	O
values	O
or	O
other	O
statistical	B
parameters	O
it	O
is	O
reasonable	O
to	O
try	O
to	O
match	O
the	O
features	B
of	O
datasets	O
with	O
our	O
past	O
knowledge	O
concerning	O
the	O
algorithms	O
if	O
we	O
select	O
the	O
algorithm	O
that	O
most	O
closely	O
matches	O
the	O
features	B
of	O
the	O
dataset	O
then	O
we	O
increase	O
the	O
chances	O
of	O
obtaining	O
useful	O
results	O
the	O
advantage	O
is	O
that	O
not	O
all	O
algorithms	O
need	O
to	O
be	O
tried	O
out	O
those	O
algorithms	O
that	O
do	O
not	O
match	O
the	O
data	O
can	O
be	O
excluded	O
and	O
so	O
a	O
great	O
deal	O
of	O
effort	O
can	O
be	O
saved	O
analysis	B
of	I
results	I
in	O
order	O
to	O
achieve	O
this	O
aim	O
we	O
need	O
to	O
determine	O
which	O
dataset	O
features	B
are	O
relevant	O
after	O
that	O
various	O
instances	O
of	O
learning	O
tasks	O
can	O
be	O
examined	O
with	O
the	O
aim	O
of	O
formulating	O
a	O
theory	O
concerning	O
the	O
applicability	O
of	O
different	O
machine	O
learning	O
and	O
statistical	B
algorithms	O
the	O
knowledge	O
concerning	O
which	O
algorithm	O
is	O
applicable	O
can	O
be	O
summarised	O
in	O
the	O
form	O
of	O
rules	O
stating	O
that	O
if	O
the	O
given	O
dataset	O
has	O
certain	O
characteristics	O
then	O
learning	O
a	O
particular	O
algorithm	O
may	O
be	O
applicable	O
each	O
rule	O
can	O
in	O
addition	O
be	O
qualified	O
using	O
a	O
certain	O
measure	B
indicating	O
how	O
reliable	O
the	O
rule	O
is	O
rules	O
like	O
this	O
can	O
be	O
constructed	O
manually	O
or	O
with	O
the	O
help	O
of	O
machine	O
learning	O
methods	O
on	O
the	O
basis	O
of	O
past	O
cases	O
in	O
this	O
section	O
we	O
are	O
concerned	O
with	O
this	O
latter	O
method	O
the	O
process	O
of	O
constructing	O
the	O
rules	O
represents	O
a	O
kind	O
of	O
meta-level	O
learning	O
as	O
the	O
number	O
of	O
tests	O
was	O
generally	O
limited	O
few	O
people	O
have	O
attempted	O
to	O
automate	O
the	O
formulation	O
of	O
a	O
theory	O
concerning	O
the	O
applicability	O
of	O
different	O
algorithms	O
one	O
exception	O
was	O
the	O
work	O
of	O
aha	O
who	O
represented	O
this	O
knowledge	O
using	O
the	O
following	O
rule	O
schemas	O
lon	O
lon	O
gchfk	O
one	O
example	B
of	O
such	O
a	O
rule	O
schema	O
is	O
where	O
ghg	O
means	O
that	O
algorithm	O
is	O
predicted	O
to	O
have	O
significantly	O
higher	O
accuracies	O
than	O
algorithm	O
our	O
approach	O
differs	O
from	O
aha	O
s	O
in	O
several	O
respects	O
the	O
main	O
difference	O
is	O
that	O
we	O
are	O
not	O
concerned	O
with	O
just	O
a	O
comparison	O
between	O
two	O
algorithms	O
but	O
rather	O
a	O
group	O
of	O
them	O
our	O
aim	O
is	O
to	O
obtain	O
rules	O
which	O
would	O
indicate	O
when	O
a	O
particular	O
algorithm	O
works	O
better	O
than	O
the	O
rest	O
a	O
number	O
of	O
interesting	O
relationships	O
have	O
emerged	O
however	O
in	O
order	O
to	O
have	O
reliable	O
results	O
we	O
would	O
need	O
quite	O
an	O
extensive	O
set	O
of	O
test	O
results	O
certainly	O
much	O
more	O
than	O
the	O
datasets	O
considered	O
in	O
this	O
book	O
as	O
part	O
of	O
the	O
overall	O
aim	O
of	O
matching	O
features	B
of	O
datasets	O
with	O
our	O
past	O
knowledge	O
of	O
algorithms	O
we	O
need	O
to	O
determine	O
which	O
dataset	O
features	B
are	O
relevant	O
this	O
is	O
not	O
known	O
a	O
priori	O
so	O
for	O
exploratory	O
purposes	O
we	O
used	O
the	O
reduced	O
set	O
of	O
measures	B
given	O
in	O
table	O
this	O
includes	O
certain	O
simple	O
measures	B
such	O
as	O
number	O
of	O
examples	O
attributes	B
and	O
classes	B
and	O
more	O
complex	B
statistical	B
and	O
information-based	B
measures	B
some	O
measures	B
represent	O
derived	O
quantities	O
and	O
include	O
for	O
example	B
measures	B
that	O
are	O
ratios	O
of	O
other	O
measures	B
these	O
and	O
other	O
measures	B
are	O
given	O
in	O
sections	O
using	O
test	O
results	O
in	O
metalevel	B
learning	I
here	O
we	O
have	O
used	O
all	O
of	O
the	O
available	O
results	O
as	O
listed	O
in	O
chapter	O
the	O
results	O
for	O
each	O
dataset	O
were	O
analysed	O
with	O
the	O
objective	O
of	O
determining	O
which	O
algorithms	O
achieved	O
low	O
error	O
rates	O
costs	B
all	O
algorithms	O
with	O
low	O
error	O
rates	O
were	O
considered	O
applicable	O
to	O
e	O
sec	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
table	O
measures	B
used	O
in	O
metalevel	B
learning	I
definition	O
number	O
of	O
examples	O
number	O
of	O
attributes	B
number	O
of	O
classes	B
number	O
of	O
binary	B
attributes	B
cost	B
matrix	I
indicator	O
standard	O
deviation	O
ratio	O
mean	O
mean	O
absolute	O
correlation	B
of	O
attributes	B
entropy	B
of	O
class	B
mean	O
entropy	B
of	I
attributes	B
first	O
canonical	B
correlation	B
fraction	O
separability	O
due	O
to	O
skewness	B
mean	O
ofi	O
kurtosis	B
mean	O
ofi	O
jlk	O
jqk	O
equivalent	O
number	O
of	O
attributest	O
n	O
xykznyn	O
xykvn	O
noise-signal	O
ratioj	O
mean	O
mutual	B
information	I
of	O
class	B
and	O
attributes	B
jlkzn	O
measure	B
simple	O
n	O
p	O
q	O
bin	O
att	O
cost	O
statistical	B
sd	O
corr	B
abs	I
skewness	B
kurtosis	B
information	B
theory	I
n	O
jlkvn	O
xykzn	O
en	O
attr	O
ns	O
ratio	O
this	O
dataset	O
the	O
other	O
algorithms	O
were	O
considered	O
inapplicable	O
this	O
categorisation	O
of	O
the	O
test	O
results	O
can	O
be	O
seen	O
as	O
a	O
preparatory	O
step	O
for	O
the	O
metalevel	B
learning	I
task	O
of	O
course	O
the	O
categorisation	O
will	O
permit	O
us	O
also	O
to	O
make	O
prediction	O
regarding	O
which	O
algorithms	O
are	O
applicable	O
on	O
a	O
new	O
dataset	O
of	O
course	O
the	O
question	O
of	O
whether	O
the	O
error	B
rate	I
is	O
high	O
or	O
low	O
is	O
rather	O
relative	O
the	O
error	B
rate	I
of	O
may	O
be	O
excellent	O
in	O
some	O
domains	O
while	O
may	O
be	O
bad	O
in	O
others	O
this	O
problem	O
is	O
resolved	O
using	O
a	O
method	O
similar	O
to	O
subset	B
selection	I
in	O
statistics	O
first	O
the	O
best	O
algorithm	O
is	O
identified	O
according	O
to	O
the	O
error	O
rates	O
then	O
an	O
acceptable	O
margin	O
of	O
tolerance	O
is	O
calculated	O
all	O
algorithms	O
whose	O
error	O
rates	O
fall	O
within	O
this	O
margin	O
are	O
considered	O
applicable	O
while	O
the	O
others	O
are	O
labelled	O
as	O
inapplicable	O
the	O
level	O
of	O
tolerance	O
can	O
reasonably	O
be	O
defined	O
in	O
terms	O
of	O
the	O
standard	O
deviation	O
of	O
the	O
error	B
rate	I
but	O
since	O
each	O
algorithm	O
achieves	O
a	O
different	O
error	B
rate	I
the	O
appropriate	O
standard	O
deviation	O
will	O
vary	O
across	O
algorithms	O
then	O
the	O
standard	O
deviation	O
is	O
defined	O
by	O
to	O
keep	O
things	O
simple	O
we	O
will	O
quote	O
the	O
standard	O
deviations	O
for	O
the	O
error	B
rate	I
of	O
the	O
best	O
algorithm	O
i	O
e	O
that	O
which	O
achieves	O
the	O
lowest	O
error	B
rate	I
denote	O
the	O
lowest	O
error	B
rate	I
by	O
c	O
where	O
fall	O
within	O
the	O
intervalj	O
xo	O
n	O
are	O
considered	O
applicable	O
of	O
course	O
we	O
still	O
need	O
to	O
choose	O
a	O
value	O
for	O
which	O
determines	O
the	O
size	O
of	O
the	O
interval	O
this	O
affects	O
the	O
value	O
of	O
confidence	O
that	O
the	O
truly	O
best	O
algorithm	O
appears	O
in	O
the	O
group	O
considered	O
the	O
larger	O
the	O
is	O
the	O
number	O
of	O
examples	O
in	O
the	O
test	B
set	I
then	O
all	O
algorithms	O
whose	O
error	O
rates	O
the	O
higher	O
the	O
confidence	O
that	O
the	O
best	O
algorithm	O
will	O
be	O
in	O
this	O
interval	O
t	O
u	O
t	O
u	O
w	O
u	O
w	O
u	O
t	O
u	O
w	O
u	O
w	O
w	O
w	O
analysis	B
of	I
results	I
for	O
example	B
let	O
us	O
consider	O
the	O
tests	O
on	O
the	O
segmentation	B
dataset	I
consisting	O
of	O
which	O
is	O
in	O
this	O
example	B
we	O
can	O
say	O
with	O
high	O
confidence	O
that	O
the	O
best	O
algorithms	O
the	O
interval	O
is	O
relatively	O
examples	O
the	O
best	O
algorithm	O
appears	O
to	O
be	O
with	O
the	O
error	B
rate	I
of	O
n	O
then	O
are	O
in	O
the	O
group	O
with	O
error	O
rates	O
between	O
and	O
bayestree	O
apart	O
from	O
small	O
of	O
j	O
if	O
p	O
and	O
includes	O
only	O
two	O
other	O
algorithms	O
all	O
the	O
algorithms	O
that	O
lie	O
in	O
this	O
interval	O
can	O
be	O
considered	O
applicable	O
to	O
this	O
dataset	O
and	O
the	O
others	O
inapplicable	O
if	O
we	O
enlarge	O
the	O
margin	O
by	O
considering	O
larger	O
values	O
we	O
get	O
a	O
more	O
relaxed	O
notion	O
of	O
applicability	O
table	O
table	O
classified	O
test	O
results	O
on	O
image	B
segmentation	B
dataset	I
for	O
n	O
margin	O
margin	O
for	O
algorithm	O
error	O
class	B
appl	O
appl	O
bayestree	O
appl	O
newid	B
appl	O
margin	O
for	O
margin	O
for	O
appl	O
cart	B
appl	O
appl	O
appl	O
indcart	B
appl	O
lvq	B
appl	O
smart	B
appl	O
backprop	B
appl	O
kohonen	B
rbf	B
k-nn	B
appl	O
appl	O
appl	O
appl	O
margin	O
for	O
margin	O
for	O
margin	O
for	O
logdisc	B
non-appl	O
castle	B
non-appl	O
discrim	B
non-appl	O
non-appl	O
quadisc	B
bayes	O
non-appl	O
non-appl	O
itrule	B
default	B
non-appl	O
the	O
decision	O
as	O
to	O
where	O
to	O
draw	O
the	O
line	O
choosing	O
a	O
value	O
for	O
is	O
of	O
course	O
rather	O
subjective	O
in	O
this	O
work	O
we	O
had	O
to	O
consider	O
an	O
additional	O
constraint	O
related	O
to	O
the	O
purpose	O
we	O
had	O
in	O
mind	O
as	O
our	O
objective	O
is	O
to	O
generate	O
rules	O
concerning	O
applicability	O
of	O
w	O
sec	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
algorithms	O
we	O
have	O
opted	O
for	O
the	O
more	O
relaxed	O
scheme	O
of	O
appplicability	O
or	O
so	O
as	O
to	O
have	O
enough	O
examples	O
in	O
each	O
class	B
non-appl	O
some	O
of	O
the	O
tests	O
results	O
analysed	O
are	O
not	O
characterised	O
using	O
error	O
rates	O
but	O
rather	O
costs	B
consequently	O
the	O
notion	O
of	O
error	O
margin	O
discussed	O
earlier	O
has	O
to	O
be	O
adapted	O
to	O
costs	B
the	O
standard	O
error	O
of	O
the	O
mean	O
cost	O
can	O
be	O
calculated	O
from	O
the	O
confusion	O
matrices	O
by	O
testing	O
and	O
the	O
cost	B
matrix	I
the	O
values	O
obtained	O
for	O
the	O
leading	O
algorithm	O
in	O
the	O
three	O
relevant	O
datasets	O
were	O
dataset	O
german	B
credit	I
heart	B
disease	I
head	B
injury	I
algorithm	O
discrim	B
discrim	B
logdisc	B
mean	O
cost	O
standard	O
error	O
of	O
mean	O
in	O
the	O
experiments	O
reported	O
later	O
the	O
error	O
margin	O
was	O
simply	O
set	O
to	O
the	O
values	O
and	O
respectively	O
irrespective	O
of	O
the	O
algorithm	O
used	O
joining	O
data	O
relative	O
to	O
one	O
algorithm	O
the	O
problem	O
of	O
learning	O
was	O
divided	O
into	O
several	O
phases	O
in	O
each	O
phase	O
all	O
the	O
test	O
results	O
relative	O
to	O
just	O
one	O
particular	O
algorithm	O
example	B
cart	B
were	O
joined	O
while	O
all	O
the	O
other	O
results	O
to	O
other	O
algorithms	O
were	O
temporarily	O
ignored	O
the	O
purpose	O
of	O
this	O
strategy	O
was	O
to	O
simplify	O
the	O
class	B
structure	O
for	O
each	O
algorithm	O
we	O
would	O
have	O
just	O
two	O
classes	B
and	O
non-appl	O
this	O
strategy	O
worked	O
better	O
than	O
the	O
obvious	O
solution	O
that	O
included	O
all	O
available	O
data	O
for	O
training	O
for	O
example	B
when	O
considering	O
the	O
cart	B
algorithm	O
and	O
a	O
margin	O
of	O
v	O
we	O
get	O
the	O
scheme	O
illustrated	O
in	O
figure	O
the	O
classified	O
test	O
cart-non-appl	O
cart-non-appl	O
cart-non-appl	O
cart-non-appl	O
cart-non-appl	O
cart-non-appl	O
cart-non-appl	O
cart-non-appl	O
kl	O
chrom	O
shut	O
tech	O
cut	B
cr	O
man	O
letter	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
cart-appl	O
satim	O
vehic	O
head	O
heart	O
belg	O
segm	O
diab	O
cr	O
ger	O
cr	O
aust	O
dna	B
belgii	O
faults	O
tsetse	O
fig	O
classified	O
test	O
results	O
relative	O
to	O
one	O
particular	O
algorithm	O
results	O
are	O
then	O
modified	O
as	O
follows	O
the	O
dataset	O
name	O
is	O
simply	O
substituted	O
by	O
a	O
vector	O
containing	O
the	O
corresponding	O
dataset	O
characteristics	O
values	O
which	O
are	O
not	O
available	O
or	O
missing	O
are	O
simply	O
represented	O
by	O
this	O
extended	O
dataset	O
is	O
then	O
used	O
in	O
the	O
meta-level	O
learning	O
choice	O
of	O
algorithm	O
for	O
learning	O
a	O
question	O
arises	O
as	O
to	O
which	O
algorithm	O
we	O
should	O
use	O
in	O
the	O
process	O
of	O
meta-level	O
learning	O
we	O
have	O
decided	O
to	O
use	O
for	O
the	O
following	O
reasons	O
first	O
as	O
our	O
results	O
have	O
fig	O
decision	O
tree	O
generated	O
by	O
relative	O
to	O
cart	B
the	O
right	O
hand	O
side	O
of	O
each	O
leaf	O
are	O
either	O
of	O
the	O
form	O
or	O
where	O
n	O
represents	O
the	O
total	O
number	O
of	O
examples	O
satisfying	O
the	O
conditions	O
of	O
the	O
associated	O
branch	O
and	O
e	O
the	O
number	O
of	O
examples	O
of	O
other	O
classes	B
that	O
have	O
been	O
erroneously	O
covered	O
if	O
the	O
data	O
contains	O
unknown	O
values	O
the	O
numbers	O
n	O
and	O
e	O
may	O
be	O
fractional	O
it	O
has	O
been	O
argued	O
that	O
rules	O
are	O
more	O
legible	O
than	O
trees	O
the	O
decision	O
tree	O
shown	O
earlier	O
can	O
be	O
transformed	O
into	O
a	O
rule	O
form	O
using	O
a	O
very	O
simple	O
process	O
where	O
each	O
branch	O
of	O
a	O
tree	O
is	O
simply	O
transcribed	O
as	O
a	O
rule	O
the	O
applicability	O
of	O
cart	B
can	O
thus	O
be	O
characterised	O
using	O
the	O
rules	O
in	O
figure	O
cart-appl	O
cart-non-appl	O
cart-non-appl	O
n	O
skewg	O
ng	O
n	O
skew	O
analysis	B
of	I
results	I
demonstrated	O
this	O
algorithm	O
achieves	O
quite	O
good	O
results	O
overall	O
secondly	O
the	O
decision	O
tree	O
generated	O
by	O
can	O
be	O
inspected	O
and	O
analysed	O
this	O
is	O
not	O
the	O
case	O
with	O
some	O
statistical	B
and	O
neural	O
learning	O
algorithms	O
so	O
for	O
example	B
when	O
has	O
been	O
supplied	O
with	O
the	O
partial	O
test	O
results	O
relative	O
to	O
cart	B
algorithm	O
it	O
generated	O
the	O
decision	O
tree	O
in	O
figure	O
the	O
figures	O
that	O
appear	O
on	O
wfe	O
bd	O
dt	O
c	O
we	O
l	O
dt	O
dt	O
fig	O
rules	O
generated	O
by	O
relative	O
to	O
cart	B
quinlan	O
has	O
argued	O
that	O
rules	O
obtained	O
from	O
decision	B
trees	I
can	O
be	O
improved	O
upon	O
in	O
various	O
ways	O
for	O
example	B
it	O
is	O
possible	O
to	O
eliminate	O
conditions	O
that	O
are	O
irrelevant	O
or	O
even	O
drop	O
entire	O
rules	O
that	O
are	O
irrelevant	O
or	O
incorrect	O
in	O
addition	O
it	O
is	O
possible	O
to	O
reorder	O
the	O
rules	O
according	O
to	O
certain	O
criteria	O
and	O
introduce	O
a	O
default	B
rule	I
to	O
cover	B
the	O
cases	O
that	O
have	O
not	O
been	O
covered	O
the	O
program	O
includes	O
a	O
command	O
that	O
permits	O
the	O
user	O
to	O
transform	O
a	O
decision	O
tree	O
into	O
a	O
such	O
a	O
rule	O
set	O
the	O
rules	O
produced	O
by	O
the	O
system	O
are	O
characterised	O
using	O
error	B
rate	I
estimates	O
as	O
is	O
shown	O
in	O
the	O
next	O
section	O
error	B
rate	I
its	O
estimate	O
is	O
not	O
an	O
ideal	O
measure	B
however	O
this	O
is	O
particularly	O
evident	O
when	O
dealing	O
with	O
continuous	O
classes	B
this	O
problem	O
has	O
motivated	O
us	O
to	O
undertake	O
a	O
separate	O
evaluation	O
of	O
all	O
candidate	O
rules	O
and	O
characterise	O
them	O
using	O
a	O
new	O
measure	B
the	O
aim	O
is	O
to	O
identify	O
those	O
rules	O
that	O
appear	O
to	O
be	O
most	O
informative	O
characterizing	O
predictive	O
power	O
the	O
rules	O
concerning	O
applicability	O
of	O
a	O
particular	O
algorithm	O
were	O
generated	O
on	O
the	O
basis	O
of	O
only	O
about	O
examples	O
case	O
represents	O
the	O
results	O
of	O
particular	O
test	O
on	O
a	O
particular	O
dataset	O
of	O
these	O
only	O
a	O
part	O
represented	O
positive	O
examples	O
corresponding	O
to	O
the	O
datasets	O
on	O
which	O
the	O
particular	O
algorithm	O
performed	O
well	O
this	O
is	O
rather	O
a	O
modest	O
number	O
also	O
the	O
set	O
of	O
dataset	O
descriptors	O
used	O
may	O
not	O
be	O
optimal	O
we	O
could	O
thus	O
expect	O
that	O
the	O
rules	O
generated	O
capture	O
a	O
mixture	O
of	O
relevant	O
and	O
fortuitous	O
regularities	O
w	O
l	O
w	O
l	O
sec	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
in	O
order	O
to	O
strengthen	O
our	O
confidence	O
in	O
the	O
results	O
we	O
have	O
decided	O
to	O
evaluate	O
the	O
rules	O
generated	O
our	O
aim	O
was	O
to	O
determine	O
whether	O
the	O
rules	O
could	O
actually	O
be	O
used	O
to	O
make	O
useful	O
predictions	O
concerning	O
its	O
applicability	O
we	O
have	O
adopted	O
a	O
leave-one-out	B
procedure	O
and	O
applied	O
it	O
to	O
datasets	O
such	O
as	O
the	O
one	O
shown	O
in	O
table	O
following	O
this	O
procedure	O
we	O
used	O
all	O
but	O
one	O
items	O
in	O
training	O
while	O
the	O
remaining	O
item	O
was	O
used	O
for	O
testing	O
of	O
course	O
the	O
set	O
of	O
rules	O
generated	O
in	O
each	O
pass	O
could	O
be	O
slightly	O
different	O
but	O
the	O
form	O
of	O
the	O
rules	O
was	O
not	O
our	O
primary	O
interest	O
here	O
we	O
were	O
interested	O
to	O
verify	O
how	O
successful	O
the	O
rules	O
were	O
in	O
predicting	O
the	O
applicability	O
non-applicability	O
of	O
the	O
algorithm	O
let	O
us	O
analyse	O
an	O
example	B
consider	O
for	O
example	B
the	O
problem	O
of	O
predicting	O
the	O
applicability	O
of	O
cart	B
this	O
can	O
be	O
characterised	O
using	O
confusion	O
matrices	O
such	O
as	O
the	O
ones	O
shown	O
in	O
figure	O
showing	O
results	O
relative	O
to	O
the	O
error	O
margin	O
note	O
that	O
an	O
extra	O
dataset	O
has	O
been	O
used	O
in	O
the	O
following	O
calculations	O
and	O
tables	O
which	O
is	O
why	O
the	O
sum	O
is	O
now	O
appl	O
non-appl	O
appl	O
non-appl	O
fig	O
evaluation	O
of	O
the	O
meta-rules	O
concerning	O
applicability	O
of	O
cart	B
the	O
rows	O
represent	O
the	O
true	O
class	B
and	O
the	O
columns	O
the	O
predicted	O
class	B
the	O
confusion	O
matrix	O
shows	O
that	O
the	O
rules	O
generated	O
were	O
capable	O
of	O
correctly	O
predicting	O
the	O
applicability	O
of	O
cart	B
on	O
an	O
unseen	O
dataset	O
in	O
cases	O
incorrect	O
prediction	O
was	O
made	O
only	O
in	O
case	O
similarly	O
if	O
we	O
consider	O
non-applicability	O
we	O
see	O
that	O
correct	O
prediction	O
is	O
made	O
in	O
cases	O
and	O
incorrect	O
one	O
in	O
this	O
gives	O
a	O
rather	O
good	O
overall	O
success	O
rate	O
of	O
we	O
notice	O
that	O
success	O
rate	O
is	O
not	O
an	O
ideal	O
measure	B
however	O
as	O
the	O
margin	O
of	O
larger	O
more	O
cases	O
will	O
get	O
classified	O
as	O
applicable	O
if	O
we	O
consider	O
an	O
extreme	O
case	O
when	O
the	O
margin	O
covers	O
all	O
algorithms	O
we	O
will	O
get	O
an	O
apparent	O
success	O
rate	O
of	O
of	O
course	O
we	O
are	O
not	O
interested	O
in	O
such	O
a	O
useless	O
procedure	O
this	O
apparent	O
paradox	O
can	O
be	O
resolved	O
by	O
adopting	O
the	O
measure	B
called	O
information	B
score	I
bratko	O
in	O
the	O
evaluation	O
this	O
measure	B
takes	O
into	O
account	O
prior	B
probabilities	I
the	O
information	B
score	I
associated	O
with	O
a	O
definite	O
positive	O
classification	B
represents	O
the	O
prior	O
probability	O
of	O
class	B
c	O
the	O
information	O
scores	O
can	O
be	O
used	O
to	O
weigh	O
all	O
classifier	B
answers	O
in	O
our	O
case	O
we	O
have	O
two	O
classes	B
appl	O
and	O
non-appl	O
the	O
weights	O
can	O
be	O
represented	O
conveniently	O
in	O
the	O
form	O
of	O
an	O
information	B
score	I
matrix	O
as	O
shown	O
in	O
figure	O
applicability	O
is	O
extended	O
making	O
log	O
n	O
where	O
n	O
is	O
defined	O
as	O
log	O
j	O
appln	O
logj	O
c	O
a	O
j	O
applnn	O
fig	O
information	B
score	I
matrix	O
the	O
rows	O
represent	O
the	O
true	O
class	B
and	O
the	O
columns	O
the	O
predicted	O
class	B
non-	O
appl	O
logj	O
j	O
non-applnn	O
log	O
j	O
non-	O
appln	O
appl	O
appl	O
non-appl	O
the	O
information	O
scores	O
can	O
be	O
used	O
to	O
calculate	O
the	O
total	O
information	O
provided	O
by	O
a	O
rule	O
analysis	B
of	I
results	I
on	O
the	O
given	O
dataset	O
this	O
can	O
be	O
done	O
simply	O
by	O
multiplying	O
each	O
element	O
of	O
the	O
confusion	O
matrix	O
by	O
the	O
corresponding	O
element	O
of	O
the	O
information	B
score	I
matrix	O
quencies	O
if	O
we	O
consider	O
the	O
frequency	O
of	O
appl	O
and	O
non-appl	O
for	O
all	O
algorithms	O
of	O
the	O
algorithm	O
in	O
question	O
we	O
get	O
a	O
kind	O
of	O
absolute	O
reference	O
point	O
this	O
enables	O
us	O
to	O
make	O
comparisons	O
right	O
across	O
different	O
algorithms	O
the	O
quantities	O
j	O
appln	O
and	O
j	O
non-appln	O
can	O
be	O
estimated	O
from	O
the	O
appropriate	O
frefor	O
example	B
for	O
the	O
value	O
of	O
log	O
j	O
appln	O
we	O
consider	O
a	O
dataset	O
consisting	O
of	O
cases	O
algorithms	O
the	O
information	O
associated	O
with	O
j	O
appln	O
similarly	O
the	O
is	O
value	O
of	O
logj	O
is	O
log	O
j	O
non-appln	O
the	O
examples	O
of	O
applicable	O
cases	O
are	O
relatively	O
common	O
of	O
applicability	O
of	O
consequently	O
the	O
information	O
concerning	O
applicability	O
has	O
a	O
somewhat	O
smaller	O
weight	O
than	O
the	O
information	O
concerning	O
non-applicability	O
if	O
we	O
multiply	O
the	O
elements	O
of	O
the	O
confusion	O
matrix	O
for	O
cart	B
by	O
the	O
corresponding	O
we	O
notice	O
that	O
due	O
to	O
the	O
distribution	O
of	O
this	O
data	O
by	O
a	O
relatively	O
large	O
margin	O
datasets	O
as	O
it	O
happens	O
cases	O
fall	O
into	O
the	O
class	B
appl	O
elements	O
of	O
the	O
information	B
score	I
matrix	O
we	O
get	O
the	O
matrix	O
shown	O
in	O
figure	O
logj	O
appl	O
non-appl	O
appl	O
non-appl	O
fig	O
adjusted	O
confusion	O
matrix	O
for	O
cart	B
the	O
rows	O
represent	O
the	O
true	O
class	B
and	O
the	O
columns	O
the	O
predicted	O
class	B
this	O
matrix	O
is	O
in	O
a	O
way	O
similar	O
to	O
the	O
confusion	O
matrix	O
shown	O
earlier	O
with	O
the	O
exception	O
that	O
the	O
error	O
counts	O
have	O
been	O
weighted	O
by	O
the	O
appropriate	O
information	O
scores	O
to	O
obtain	O
an	O
estimate	O
of	O
the	O
average	O
information	O
relative	O
to	O
one	O
case	O
we	O
need	O
to	O
divide	O
all	O
elements	O
by	O
the	O
number	O
of	O
cases	O
considered	O
this	O
way	O
we	O
get	O
the	O
scaled	O
matrix	O
in	O
figure	O
appl	O
non-appl	O
appl	O
non-appl	O
bits	O
bits	O
fig	O
rescaled	O
adjusted	O
confusion	O
matrix	O
for	O
cart	B
this	O
information	O
provided	O
by	O
the	O
classification	B
of	O
appl	O
is	O
the	O
information	O
provided	O
by	O
classification	B
of	O
non-appl	O
is	O
similarly	O
this	O
information	O
obtained	O
in	O
the	O
manner	O
described	O
can	O
be	O
compared	O
to	O
the	O
information	O
provided	O
by	O
a	O
default	B
rule	I
this	O
can	O
be	O
calculated	O
simply	O
as	O
follows	O
first	O
we	O
need	O
to	O
decide	O
whether	O
the	O
algorithm	O
should	O
be	O
applicable	O
or	O
non-applicable	O
by	O
default	B
this	O
is	O
quite	O
simple	O
we	O
just	O
look	O
for	O
the	O
classification	B
which	O
provides	O
us	O
with	O
the	O
highest	O
information	O
is	O
because	O
the	O
information	O
associated	O
with	O
this	O
default	B
isj	O
which	O
is	O
greater	O
than	O
the	O
information	O
associated	O
with	O
the	O
converse	O
rule	O
that	O
if	O
we	O
consider	O
the	O
previous	O
example	B
the	O
class	B
appl	O
is	O
the	O
correct	O
default	B
for	O
cart	B
this	O
sec	O
cart	B
is	O
non-appl	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
how	O
can	O
we	O
decide	O
whether	O
the	O
rules	O
involved	O
in	O
classification	B
are	O
actually	O
useful	O
this	O
is	O
quite	O
straightforward	O
a	O
rule	O
can	O
be	O
considered	O
useful	O
if	O
it	O
provides	O
us	O
with	O
more	O
information	O
than	O
the	O
default	B
if	O
we	O
come	O
back	O
to	O
our	O
example	B
we	O
see	O
that	O
the	O
classification	B
for	O
appl	O
provides	O
us	O
with	O
bits	O
while	O
the	O
default	B
classification	B
provides	O
only	O
bits	O
this	O
indicates	O
that	O
the	O
rules	O
used	O
in	O
the	O
classification	B
are	O
more	O
informative	O
than	O
the	O
default	B
in	O
consequence	O
the	O
actual	O
rule	O
should	O
be	O
kept	O
and	O
the	O
default	B
rule	I
discarded	O
rules	O
generated	O
in	O
metalevel	B
learning	I
figure	O
contains	O
some	O
rules	O
generated	O
using	O
the	O
method	O
described	O
as	O
we	O
have	O
not	O
used	O
a	O
uniform	B
notion	O
of	O
applicability	O
throughout	O
each	O
rule	O
is	O
qualified	O
by	O
additional	O
represents	O
the	O
concept	B
of	O
applicability	O
derived	O
on	O
the	O
each	O
rule	O
also	O
shows	O
the	O
information	B
score	I
this	O
parameter	O
gives	O
an	O
estimate	O
of	O
the	O
usefulness	O
of	O
each	O
rule	O
the	O
rules	O
presented	O
could	O
be	O
supplemented	O
by	O
another	O
set	O
generated	O
on	O
the	O
basis	O
of	O
the	O
worst	O
error	B
rate	I
the	O
error	B
rate	I
associated	O
with	O
the	O
choice	O
of	O
most	O
information	O
the	O
symbol	O
appl	O
o	O
basis	O
of	O
the	O
best	O
error	B
rate	I
in	O
case	O
of	O
appl	O
the	O
interval	O
of	O
applicability	O
isj	O
best	O
error	B
rate	I
best	O
error	B
rate	I
std	O
sn	O
and	O
the	O
interval	O
of	O
non-applicability	O
isj	O
best	O
error	B
rate	I
std	O
s	O
the	O
interval	O
of	O
applicability	O
isj	O
best	O
error	O
common	O
class	B
or	O
worse	O
in	O
the	O
case	O
of	O
appl	O
y	O
rate	O
default	B
error	B
rate	I
std	O
sn	O
and	O
the	O
interval	O
of	O
non-applicability	O
isj	O
default	B
error	B
rate	I
std	O
s	O
recognised	O
do	O
not	O
have	O
any	O
conditions	O
on	O
the	O
right	O
hand	O
side	O
of	O
minimally	O
useful	O
information	O
scoreg	O
a	O
few	O
more	O
rules	O
which	O
are	O
a	O
bit	O
less	O
informative	O
inf	O
scores	O
down	O
to	O
cart	B
are	O
also	O
shown	O
as	O
these	O
were	O
discussed	O
earlier	O
in	O
the	O
implemented	O
system	O
we	O
use	O
each	O
rule	O
included	O
shows	O
also	O
the	O
normalised	O
information	B
score	I
this	O
parameter	O
gives	O
an	O
estimate	O
of	O
the	O
usefulness	O
of	O
each	O
rule	O
only	O
those	O
rules	O
that	O
could	O
be	O
considered	O
have	O
been	O
included	O
here	O
all	O
rules	O
for	O
the	O
set	O
of	O
rules	O
generated	O
includes	O
a	O
number	O
of	O
default	B
rules	O
which	O
can	O
be	O
easily	O
discussion	O
the	O
problem	O
of	O
learning	O
rules	O
for	O
all	O
algorithms	O
simultaneously	O
is	O
formidable	O
we	O
want	O
to	O
obtain	O
a	O
sufficient	O
number	O
rules	O
to	O
qualify	O
each	O
algorithm	O
to	O
limit	O
the	O
complexity	O
of	O
the	O
problem	O
we	O
have	O
considered	O
one	O
algorithm	O
at	O
a	O
time	B
this	O
facilitated	O
the	O
construction	O
of	O
rules	O
considering	O
that	O
the	O
problem	O
is	O
difficult	O
what	O
confidence	O
can	O
we	O
have	O
that	O
the	O
rules	O
generated	O
are	O
minimally	O
sensible	O
one	O
possibility	O
is	O
to	O
try	O
to	O
evaluate	O
the	O
rules	O
by	O
checking	O
whether	O
they	O
are	O
capable	O
of	O
giving	O
useful	O
predictions	O
this	O
is	O
what	O
we	O
have	O
done	O
in	O
one	O
of	O
the	O
earlier	O
sections	O
note	O
that	O
measuring	O
simply	O
the	O
success	O
rate	O
has	O
the	O
disadvantage	O
that	O
it	O
does	O
not	O
distinguish	O
between	O
predictions	O
that	O
are	O
easy	O
to	O
make	O
and	O
those	O
that	O
are	O
more	O
difficult	O
this	O
is	O
why	O
we	O
have	O
evaluated	O
the	O
rules	O
by	O
examining	O
how	O
informative	O
they	O
are	O
in	O
general	O
for	O
example	B
if	O
we	O
examine	O
the	O
rules	O
for	O
the	O
applicability	O
of	O
cart	B
we	O
observe	O
that	O
the	O
rules	O
provide	O
us	O
with	O
useful	O
information	O
if	O
invoked	O
these	O
measures	B
indicate	O
that	O
the	O
rules	O
generated	O
can	O
indeed	O
provide	O
us	O
with	O
useful	O
information	O
instead	O
of	O
evaluating	O
rules	O
in	O
the	O
way	O
shown	O
we	O
could	O
present	O
them	O
to	O
some	O
expert	O
to	O
see	O
if	O
he	O
would	O
find	O
them	O
minimally	O
sensible	O
on	O
a	O
quick	O
glance	O
the	O
condition	O
n	O
analysis	B
of	I
results	I
statistical	B
algorithms	O
decision	O
tree	O
and	O
rule	O
algorithms	O
n	O
kurtosisg	O
n	O
skewg	O
ng	O
k	O
ng	O
ng	O
n	O
ng	O
kg	O
n	O
n	O
kg	O
newid-appl	O
y	O
cart-appl	O
y	O
cart-appl	O
cart-non-appl	O
indcart-appl	O
itrule-non-appl	O
y	O
itrule-non-appl	O
discrim-appl	O
discrim-non-appl	O
discrim-non-appl	O
quadisc-appl	O
y	O
logdisc-appl	O
logdisc-non-appl	O
y	O
k-nn-appl	O
bayes-non-appl	O
y	O
bayes-non-appl	O
baytree-appl	O
k	O
baytree-non-appl	O
kg	O
castle-non-appl	O
y	O
ng	O
cost	O
castle-non-appl	O
bin	O
att	O
y	O
rbf-non-appl	O
lvq-appl	O
backprop-appl	O
kohonen-non-appl	O
y	O
cascade-non-appl	O
cascade-non-appl	O
neural	O
network	O
algorithms	O
n	O
fig	O
some	O
rules	O
generated	O
in	O
meta-level	O
learning	O
inf	O
score	O
sec	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
is	O
a	O
bit	O
puzzling	O
why	O
should	O
cart	B
perform	O
reasonably	O
well	O
if	O
the	O
number	O
of	O
examples	O
is	O
less	O
than	O
this	O
number	O
is	O
necessary	O
to	O
note	O
that	O
the	O
condition	O
n	O
obviously	O
as	O
the	O
rules	O
were	O
generated	O
on	O
the	O
basis	O
of	O
a	O
relatively	O
small	O
number	O
of	O
examples	O
the	O
rules	O
could	O
contain	O
some	O
fortuitous	O
features	B
of	O
course	O
unless	O
we	O
have	O
more	O
data	O
available	O
it	O
is	O
difficult	O
to	O
point	O
out	O
which	O
features	B
are	O
or	O
are	O
not	O
relevant	O
however	O
it	O
is	O
not	O
an	O
absolute	O
one	O
rules	O
should	O
not	O
be	O
simply	O
interpreted	O
as	O
the	O
algorithm	O
performs	O
well	O
if	O
such	O
and	O
such	O
condition	O
is	O
satisfied	O
the	O
correct	O
interpretation	O
is	O
something	O
like	O
the	O
algorithm	O
is	O
likely	O
to	O
compete	O
well	O
under	O
the	O
conditions	O
stated	O
provided	O
no	O
other	O
more	O
informative	O
rule	O
applies	O
this	O
view	O
helps	O
also	O
to	O
understand	O
better	O
the	O
rule	O
for	O
discrim	B
algorithm	O
generated	O
by	O
the	O
system	O
discrim-appl	O
the	O
condition	O
n	O
does	O
not	O
express	O
all	O
the	O
conditions	O
of	O
applicability	O
of	O
algorithm	O
discrim	B
and	O
could	O
appear	O
rather	O
strange	O
however	O
the	O
condition	O
does	O
make	O
sense	O
some	O
algorithms	O
have	O
a	O
faster	O
learning	O
rate	O
than	O
others	O
these	O
algorithms	O
compete	O
well	O
with	O
others	O
provided	O
the	O
number	O
of	O
examples	O
is	O
small	O
the	O
fast	O
learning	O
algorithms	O
may	O
however	O
be	O
overtaken	O
by	O
others	O
later	O
experiments	O
with	O
learning	B
curves	I
on	O
the	O
satellite	B
image	I
dataset	I
show	O
that	O
the	O
discrim	B
algorithm	O
is	O
among	O
the	O
first	O
six	O
algorithms	O
in	O
terms	O
of	O
error	B
rate	I
as	O
long	O
as	O
the	O
number	O
of	O
examples	O
is	O
relatively	O
small	O
etc	O
this	O
algorithm	O
seems	O
to	O
pick	O
up	O
quickly	O
what	O
is	O
relevant	O
and	O
so	O
we	O
could	O
say	O
it	O
competes	O
well	O
under	O
these	O
conditions	O
when	O
the	O
number	O
of	O
examples	O
is	O
larger	O
however	O
discrim	B
is	O
overtaken	O
by	O
other	O
algorithms	O
with	O
the	O
full	O
training	B
set	I
of	O
examples	O
discrim	B
is	O
in	O
place	O
in	O
the	O
ranking	O
this	O
is	O
consistent	O
with	O
the	O
rule	O
generated	O
by	O
our	O
system	O
the	O
condition	O
generated	O
by	O
the	O
system	O
is	O
not	O
so	O
puzzling	O
as	O
it	O
seems	O
at	O
first	O
glance	O
there	O
is	O
of	O
course	O
a	O
well	O
recognised	O
problem	O
that	O
should	O
be	O
tackled	O
many	O
conditions	O
contain	O
numeric	O
tests	O
which	O
are	O
either	O
true	O
or	O
false	O
it	O
does	O
not	O
make	O
sense	O
to	O
consider	O
the	O
discrim	B
algorithm	O
applicable	O
if	O
the	O
number	O
of	O
examples	O
is	O
less	O
than	O
and	O
inapplicable	O
if	O
this	O
number	O
is	O
just	O
a	O
bit	O
more	O
a	O
more	O
flexible	O
approach	O
is	O
needed	O
example	B
using	O
flexible	O
matching	O
application	O
assistant	B
rules	O
generated	O
in	O
the	O
way	O
described	O
permit	O
us	O
to	O
give	O
recommendations	O
as	O
to	O
which	O
classification	B
algorithm	O
could	O
be	O
used	O
with	O
a	O
given	O
dataset	O
this	O
is	O
done	O
with	O
the	O
help	O
of	O
a	O
kind	O
of	O
expert	O
system	O
called	O
an	O
application	O
assistant	B
this	O
system	O
contains	O
a	O
knowledge	O
base	O
which	O
is	O
interpreted	O
by	O
an	O
interpreter	O
the	O
knowledge	O
base	O
contains	O
all	O
the	O
rules	O
shown	O
in	O
the	O
previous	O
section	O
the	O
interpreter	O
is	O
quite	O
standard	O
but	O
uses	O
a	O
particular	O
method	O
for	O
resolution	O
of	O
conflicts	O
we	O
notice	O
that	O
the	O
knowledge	O
base	O
may	O
contain	O
potentially	O
conflicting	O
rules	O
in	O
general	O
several	O
rules	O
may	O
apply	O
some	O
of	O
which	O
may	O
recommend	O
the	O
use	O
of	O
a	O
particular	O
algorithm	O
while	O
others	O
may	O
be	O
against	O
it	O
some	O
people	O
believe	O
that	O
knowledge	O
bases	O
should	O
always	O
be	O
cleaned	O
up	O
so	O
that	O
such	O
situations	O
would	O
not	O
arise	O
this	O
would	O
amount	O
to	O
obliterating	O
certain	O
potentially	O
useful	O
information	O
and	O
so	O
we	O
prefer	O
to	O
deal	O
with	O
the	O
problem	O
in	O
the	O
following	O
way	O
for	O
every	O
algorithm	O
we	O
consider	O
all	O
the	O
rules	O
satisfying	O
the	O
conditions	O
and	O
sum	O
all	O
the	O
information	O
scores	O
the	O
information	O
scores	O
associated	O
with	O
the	O
recommendation	O
to	O
apply	O
analysis	B
of	I
results	I
an	O
algorithm	O
are	O
taken	O
with	O
a	O
positive	O
sign	O
the	O
others	O
with	O
a	O
negative	O
one	O
for	O
example	B
if	O
we	O
get	O
a	O
recommendation	O
to	O
apply	O
an	O
algorithm	O
with	O
an	O
indication	O
that	O
this	O
is	O
apparently	O
bits	O
worth	O
and	O
if	O
we	O
also	O
get	O
an	O
opposite	O
recommendation	O
not	O
to	O
apply	O
this	O
algorithm	O
with	O
an	O
indication	O
that	O
this	O
is	O
bits	O
worth	O
we	O
will	O
go	O
ahead	O
with	O
the	O
recommendation	O
but	O
decrease	O
the	O
information	B
score	I
accordingly	O
to	O
bits	O
the	O
output	B
of	O
this	O
phase	O
is	O
a	O
list	O
of	O
algorithms	O
accompanied	O
by	O
their	O
associated	O
overall	O
information	O
scores	O
a	O
positive	O
score	O
can	O
be	O
interpreted	O
as	O
an	O
argument	O
to	O
apply	O
the	O
algorithm	O
a	O
negative	O
score	O
can	O
be	O
interpreted	O
as	O
an	O
argument	O
against	O
the	O
application	O
of	O
the	O
algorithm	O
moreover	O
the	O
higher	O
the	O
score	O
the	O
more	O
informative	O
is	O
the	O
recommendation	O
in	O
general	O
the	O
information	B
score	I
can	O
be	O
then	O
considered	O
as	O
a	O
strength	O
of	O
the	O
recommendation	O
the	O
recommendations	O
given	O
are	O
of	O
course	O
not	O
perfect	O
they	O
do	O
not	O
guarantee	O
that	O
the	O
first	O
algorithm	O
in	O
the	O
recommendation	O
ordering	O
will	O
have	O
the	O
best	O
performance	O
in	O
reality	O
however	O
our	O
results	O
demonstrate	O
that	O
the	O
algorithms	O
accompanied	O
by	O
a	O
strong	O
recommendation	O
do	O
perform	O
quite	O
well	O
in	O
general	O
the	O
opposite	O
is	O
also	O
true	O
the	O
algorithms	O
that	O
have	O
not	O
been	O
recommended	O
have	O
a	O
poorer	O
performance	O
in	O
general	O
in	O
other	O
words	O
we	O
observe	O
that	O
there	O
is	O
a	O
reasonable	O
degree	O
of	O
correlation	B
between	O
the	O
recommendation	O
and	O
the	O
actual	O
test	O
results	O
this	O
is	O
illustrated	O
in	O
figure	O
which	O
shows	O
the	O
recommendations	O
generated	O
for	O
one	O
particular	O
dataset	O
t	O
e	O
a	O
r	O
s	O
s	O
e	O
c	O
c	O
u	O
s	O
k-nn	B
lvq	B
baytree	O
quadisc	B
newid	B
castle	B
kohonen	B
logdisc	B
rbf	B
discrim	B
smart	B
backprop	B
information	B
score	I
fig	O
recommendations	O
of	O
the	O
application	O
assistant	B
for	O
the	O
letters	B
dataset	I
the	O
recommendations	O
were	O
generated	O
on	O
the	O
basis	O
of	O
a	O
rules	O
set	O
similar	O
to	O
the	O
one	O
shown	O
in	O
figure	O
rule	O
set	O
included	O
just	O
a	O
few	O
more	O
rules	O
with	O
lower	O
information	O
scores	O
the	O
top	O
part	O
shows	O
the	O
algorithms	O
with	O
high	O
success	O
rates	O
the	O
algorithms	O
on	O
the	O
right	O
are	O
accompanied	O
by	O
a	O
strong	O
recommendation	O
concerning	O
applicability	O
we	O
notice	O
that	O
sec	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
several	O
algorithms	O
with	O
high	O
success	O
rates	O
apear	O
there	O
the	O
algorithm	O
that	O
is	O
accompanied	O
by	O
the	O
strongest	O
reccomendation	O
for	O
this	O
dataset	O
is	O
score	O
bits	O
this	O
algorithm	O
has	O
also	O
the	O
highest	O
success	O
rate	O
of	O
the	O
second	O
place	O
in	O
the	O
ordering	O
of	O
algorithms	O
recommended	O
is	O
k-nn	B
shared	O
by	O
k-nn	B
and	O
we	O
note	O
that	O
k-nn	B
is	O
a	O
very	O
good	O
choice	O
while	O
is	O
not	O
too	O
bad	O
either	O
the	O
correlation	B
between	O
the	O
information	B
score	I
and	O
success	O
rate	O
could	O
of	O
course	O
be	O
better	O
the	O
algorithm	O
castle	B
is	O
given	O
somewhat	O
too	O
much	O
weight	O
while	O
baytree	O
which	O
is	O
near	O
the	O
top	O
is	O
somewhat	O
undervalued	O
the	O
correlation	B
could	O
be	O
improved	O
in	O
the	O
first	O
place	O
by	O
obtaining	O
more	O
test	O
results	O
the	O
results	O
could	O
also	O
be	O
improved	O
by	O
incorporating	O
a	O
better	O
method	O
for	O
combining	O
rules	O
and	O
the	O
corresponding	O
information	O
scores	O
it	O
would	O
be	O
beneficial	O
to	O
consider	O
also	O
other	O
potentially	O
useful	O
sets	O
of	O
rules	O
including	O
the	O
ones	O
generated	O
on	O
the	O
basis	O
of	O
other	O
values	O
of	O
k	O
or	O
even	O
different	O
categorisation	O
schemes	O
for	O
example	B
all	O
algorithms	O
with	O
a	O
performance	O
near	O
the	O
default	B
rule	I
could	O
be	O
considered	O
non-applicable	O
while	O
all	O
others	O
could	O
be	O
classified	O
as	O
applicable	O
despite	O
the	O
fact	O
that	O
there	O
is	O
room	O
for	O
possible	O
improvements	O
the	O
application	O
assistant	B
seems	O
to	O
produce	O
promising	O
results	O
the	O
user	O
can	O
get	O
a	O
recommendation	O
as	O
to	O
which	O
algorithm	O
could	O
be	O
used	O
with	O
a	O
new	O
dataset	O
although	O
the	O
recommendation	O
is	O
not	O
guaranteed	O
always	O
to	O
give	O
the	O
best	O
possible	O
advice	O
it	O
narrows	O
down	O
the	O
user	O
s	O
choice	O
criticism	O
of	O
metalevel	B
learning	I
approach	O
before	O
accepting	O
any	O
rules	O
generated	O
by	O
or	O
otherwise	O
it	O
is	O
wise	O
to	O
check	O
them	O
against	O
known	O
theoretical	O
and	O
empirical	O
facts	O
the	O
rules	O
generated	O
in	O
metalevel	B
learning	I
could	O
contain	O
spurious	O
rules	O
with	O
no	O
foundation	O
in	O
theory	O
if	O
the	O
rule-based	O
approach	O
has	O
shortcomings	O
how	O
should	O
we	O
proceed	O
would	O
it	O
be	O
better	O
to	O
use	O
another	O
classification	B
scheme	O
in	O
place	O
of	O
the	O
metalevel	B
learning	I
approach	O
using	O
as	O
there	O
are	O
insufficient	O
data	O
to	O
construct	O
the	O
rules	O
the	O
answer	O
is	O
probably	O
to	O
use	O
an	O
interactive	O
method	O
capable	O
of	O
incorporating	O
prior	O
expert	O
knowledge	O
knowledge	O
as	O
one	O
simple	O
example	B
if	O
it	O
is	O
known	O
that	O
an	O
algorithm	O
can	O
handle	O
cost	B
matrices	I
this	O
could	O
simply	O
be	O
provided	O
to	O
is	O
the	O
system	O
as	O
another	O
example	B
the	O
knowledge	O
that	O
the	O
behaviour	O
of	O
newid	B
and	O
could	O
then	O
be	O
likely	O
to	O
be	O
similar	O
could	O
also	O
be	O
useful	O
to	O
the	O
system	O
the	O
rules	O
for	O
constructed	O
from	O
the	O
rule	O
for	O
newid	B
by	O
adding	O
suitable	O
conditions	O
concerning	O
for	O
example	B
the	O
hierarchical	B
structure	I
of	O
the	O
attributes	B
also	O
some	O
algorithms	O
have	O
inbuilt	O
checks	O
on	O
applicability	O
such	O
as	O
linear	O
or	O
quadratic	B
discriminants	I
and	O
these	O
should	O
be	O
incorporated	O
into	O
the	O
learnt	O
rules	O
criticism	O
of	O
measures	B
some	O
of	O
the	O
statistical	B
measures	B
are	O
in	O
fact	O
more	O
complex	B
in	O
structure	O
than	O
the	O
learning	O
the	O
rules	O
for	O
example	B
the	O
programming	O
effort	O
in	O
calculating	O
sd	O
ratio	O
is	O
greater	O
than	O
that	O
in	O
establishing	O
the	O
linear	B
discriminant	I
rule	O
indeed	O
to	O
find	O
sd	O
ratio	O
requires	O
virtually	O
all	O
the	O
quantities	O
needed	O
in	O
finding	O
the	O
quadratic	B
discriminant	I
this	O
poses	O
the	O
question	O
if	O
it	O
is	O
easier	O
to	O
run	O
say	O
linear	O
discriminants	O
and	O
newid	B
why	O
not	O
run	O
them	O
and	O
use	O
the	O
performance	O
of	O
these	O
procedures	O
as	O
yardsticks	O
by	O
which	O
to	O
judge	O
the	O
performance	O
of	O
other	O
algorithms	O
the	O
similarities	O
evident	O
in	O
the	O
empirical	O
results	O
strongly	O
suggest	O
that	O
the	O
best	O
predictor	O
for	O
logistic	O
regression	O
is	O
linear	O
discriminants	O
logistic	O
regression	O
doing	O
that	O
is	O
very	O
similar	O
to	O
newid	B
there	O
is	O
no	O
hierarchy	B
and	O
little	O
better	O
on	O
average	O
and	O
analysis	B
of	I
results	I
so	O
on	O
this	O
idea	O
can	O
be	O
formalised	O
as	O
we	O
indicate	O
in	O
the	O
next	O
section	O
prediction	O
of	O
performance	O
what	O
is	O
required	O
is	O
a	O
few	O
simple	O
yardstick	B
methods	I
readily	O
available	O
in	O
the	O
public	O
domain	O
that	O
can	O
be	O
run	O
quickly	O
on	O
the	O
given	O
dataset	O
we	O
also	O
need	O
a	O
set	O
of	O
rules	O
that	O
will	O
predict	O
the	O
performance	O
of	O
all	O
other	O
algorithms	O
from	O
the	O
yardstick	O
results	O
as	O
a	O
first	O
suggestion	O
consider	O
discrim	B
indcart	B
and	O
k-nn	B
they	O
contain	O
a	O
statistical	B
a	O
decisiontree	O
and	O
a	O
non-parametric	O
method	O
so	O
represent	O
the	O
main	O
strands	O
the	O
question	O
is	O
this	O
can	O
they	O
represent	O
the	O
full	O
range	O
of	O
algorithms	O
in	O
the	O
terminology	O
of	O
multidimensional	B
scaling	I
do	O
they	O
span	O
the	O
reduced	O
space	O
in	O
which	O
most	O
algorithm	O
results	O
reside	O
the	O
multidimensional	B
scaling	I
diagram	O
in	O
figure	O
suggests	O
that	O
a	O
three-	O
or	O
even	O
two-dimensional	O
subspace	O
is	O
sufficient	O
to	O
represent	O
all	O
results	O
to	O
give	O
a	O
few	O
examples	O
let	O
discrim	B
k-nn	B
and	O
indcart	B
represent	O
the	O
error	O
rates	O
achieved	O
by	O
the	O
respective	O
methods	O
to	O
predict	O
the	O
accuracy	B
of	O
logdisc	B
from	O
these	O
three	O
reference	O
figures	O
we	O
can	O
use	O
a	O
multiple	O
regression	O
of	O
logdisc	B
on	O
the	O
three	O
variables	O
discrim	B
k-nn	B
and	O
indcart	B
no	O
intercept	O
term	O
after	O
dropping	O
non-significant	O
terms	O
from	O
the	O
regression	O
this	O
produces	O
the	O
formula	O
with	O
a	O
squared	O
correlation	B
coefficient	O
of	O
see	O
table	O
for	O
a	O
summary	O
of	O
the	O
regression	O
formulae	O
for	O
all	O
the	O
algorithms	O
discrim	B
k-nn	B
and	O
indcart	B
naturally	O
d	O
table	O
predictors	O
for	O
error-rates	O
based	O
on	O
discrim	B
k-nn	B
and	O
indcart	B
algorithm	O
discrim	B
k-nn	B
indcart	B
r-square	O
quadisc	B
logdisc	B
smart	B
castle	B
cart	B
newid	B
n	O
trials	O
baytree	O
naivebay	O
itrule	B
kohonen	B
bprop	O
cascade	B
rbf	B
lvq	B
the	O
discrim	B
coefficient	O
of	O
in	O
the	O
logdisc	B
example	B
shows	O
that	O
logdisc	B
is	O
generally	O
about	O
more	O
accurate	O
than	O
discrim	B
and	O
that	O
the	O
performance	O
of	O
the	O
other	O
two	O
reference	O
methods	O
does	O
not	O
seem	O
to	O
help	O
in	O
the	O
prediction	O
with	O
an	O
r-squared	O
value	O
of	O
we	O
can	O
sec	O
prediction	O
of	O
performance	O
be	O
quite	O
confident	O
that	O
logdisc	B
does	O
better	O
than	O
discrim	B
this	O
result	O
should	O
be	O
qualified	O
with	O
information	O
on	O
the	O
number	O
of	O
attributes	B
normality	O
of	O
variables	O
etc	O
and	O
these	O
are	O
quantities	O
that	O
can	O
be	O
measured	O
in	O
the	O
context	O
of	O
deciding	O
if	O
further	O
trials	O
on	O
additional	O
algorithms	O
are	O
necessary	O
take	O
the	O
example	B
of	O
the	O
shuttle	B
dataset	I
and	O
consider	O
what	O
action	O
to	O
recommend	O
after	O
discovering	O
discrim	B
indcart	B
and	O
k-nn	B
it	O
does	O
not	O
look	O
as	O
if	O
the	O
error	O
rates	O
of	O
either	O
logdisc	B
or	O
smart	B
will	O
get	O
anywhere	O
near	O
indcart	B
s	O
value	O
and	O
the	O
best	O
prospect	O
of	O
improvement	O
lies	O
in	O
the	O
decision	O
tree	O
methods	O
consider	O
now	O
there	O
appears	O
to	O
be	O
no	O
really	O
good	O
predictor	O
as	O
the	O
r-squared	O
value	O
is	O
relatively	O
small	O
this	O
means	O
that	O
is	O
doing	O
something	O
outside	O
the	O
scope	O
of	O
the	O
three	O
reference	O
algorithms	O
the	O
best	O
single	O
predictor	O
is	O
discrim	B
apparently	O
indicating	O
that	O
is	O
usually	O
much	O
better	O
than	O
discrim	B
not	O
so	O
much	O
better	O
that	O
it	O
would	O
challenge	O
indcart	B
s	O
good	O
value	O
for	O
the	O
shuttle	B
dataset	I
this	O
formula	O
just	O
cannot	O
be	O
true	O
in	O
general	O
however	O
all	O
we	O
can	O
say	O
is	O
that	O
for	O
datasets	O
around	O
the	O
size	O
in	O
statlog	B
has	O
error	O
rates	O
about	O
one	O
third	O
that	O
of	O
discrim	B
but	O
considerable	O
fluctuation	O
round	O
this	O
value	O
is	O
possible	O
if	O
we	O
have	O
available	O
the	O
three	O
reference	O
results	O
the	O
formula	O
would	O
suggest	O
that	O
should	O
be	O
tried	O
unless	O
either	O
k-nn	B
or	O
cart	B
gets	O
an	O
accuracy	B
much	O
lower	O
than	O
a	O
third	O
of	O
discrim	B
knowing	O
the	O
structure	O
of	O
we	O
can	O
predict	O
a	O
good	O
deal	O
more	O
however	O
when	O
there	O
are	O
just	O
two	O
classes	B
of	O
our	O
datasets	O
were	O
problems	O
and	O
if	O
does	O
not	O
use	O
clustering	B
is	O
very	O
similar	O
indeed	O
to	O
logistic	O
regression	O
optimise	O
on	O
slightly	O
different	O
criteria	O
so	O
the	O
best	O
predictor	O
for	O
in	O
problems	O
with	O
no	O
obvious	O
clustering	B
will	O
be	O
logdisc	B
at	O
the	O
other	O
extreme	O
if	O
many	O
clusters	O
are	O
used	O
in	O
the	O
initial	O
stages	O
of	O
then	O
the	O
performance	O
is	O
bound	O
to	O
approach	O
that	O
of	O
say	O
radial	O
basis	O
functions	O
or	O
lvq	B
also	O
while	O
on	O
the	O
subject	O
of	O
giving	O
explanations	O
for	O
differences	O
in	O
behaviour	O
consider	O
the	O
performance	O
of	O
compared	O
to	O
k-nn	B
from	O
table	O
it	O
is	O
clear	O
that	O
usually	O
outperforms	O
k-nn	B
the	O
reason	O
is	O
probably	O
due	O
to	O
the	O
mechanism	O
within	O
whereby	O
irrelevant	B
attributes	B
are	O
dropped	O
or	O
perhaps	O
because	O
a	O
surrogate	O
was	O
substituted	O
for	O
when	O
it	O
performed	O
badly	O
if	O
such	O
strategies	O
were	O
instituted	O
for	O
k-nn	B
it	O
is	O
probable	O
that	O
their	O
performances	O
would	O
be	O
even	O
closer	O
finally	O
we	O
should	O
warn	O
that	O
such	O
rules	O
should	O
be	O
treated	O
with	O
great	O
caution	O
as	O
we	O
have	O
already	O
suggested	O
in	O
connection	O
with	O
the	O
rules	O
derived	O
by	O
it	O
is	O
especially	O
dangerous	O
to	O
draw	O
conclusions	O
from	O
incomplete	O
data	O
as	O
with	O
cart	B
for	O
example	B
for	O
the	O
reason	O
that	O
a	O
not	O
available	O
result	O
is	O
very	O
likely	O
associated	O
with	O
factors	O
leading	O
to	O
high	O
error	O
rates	O
such	O
as	O
inability	O
to	O
cope	O
with	O
large	O
numbers	O
of	O
categories	O
or	O
large	O
amounts	O
of	O
data	O
empirical	O
rules	O
such	O
as	O
those	O
we	O
have	O
put	O
forward	B
should	O
be	O
refined	O
by	O
the	O
inclusion	O
of	O
other	O
factors	O
in	O
the	O
regression	O
these	O
other	O
factors	O
being	O
directly	O
related	O
to	O
known	O
properties	O
of	O
the	O
algorithm	O
for	O
example	B
to	O
predict	O
quadisc	B
a	O
term	O
involving	O
the	O
measures	B
sd	O
ratio	O
would	O
be	O
required	O
that	O
is	O
not	O
too	O
circular	O
an	O
argument	O
ml	B
on	I
ml	I
vs	O
regression	O
two	O
methods	O
have	O
been	O
given	O
above	O
for	O
predicting	O
the	O
performance	O
of	O
algorithms	O
based	O
respectively	O
on	O
rule-based	O
advice	O
using	O
dataset	O
measures	B
on	O
ml	O
and	O
comparison	O
with	O
reference	O
algorithms	O
it	O
is	O
difficult	O
to	O
compare	O
directly	O
the	O
success	O
rates	O
of	O
the	O
respective	O
predictions	O
as	O
the	O
former	O
is	O
stated	O
in	O
terms	O
of	O
proportion	O
of	O
correct	O
predictions	O
analysis	B
of	I
results	I
formula	O
which	O
is	O
based	O
on	O
the	O
assumption	O
of	O
equal	O
numbers	O
of	O
non-appl	O
and	O
appl	O
and	O
the	O
latter	O
in	O
terms	O
of	O
squared	O
correlation	B
we	O
now	O
give	O
a	O
simple	O
method	O
of	O
comparing	O
from	O
the	O
predictability	O
of	O
performance	O
from	O
the	O
two	O
techniques	O
the	O
r-squared	O
value	O
regression	O
and	O
the	O
generated	O
rule	O
error	B
rate	I
can	O
be	O
compared	O
by	O
the	O
following	O
when	O
the	O
error	B
rate	I
is	O
as	O
pure	O
as	O
it	O
should	O
this	O
formula	O
gives	O
a	O
correlation	B
ofz	O
guesswork	O
would	O
get	O
half	O
the	O
cases	O
correct	O
to	O
give	O
an	O
example	B
in	O
using	O
this	O
formula	O
the	O
cart	B
rules	O
had	O
errors	O
in	O
with	O
an	O
error	B
rate	I
of	O
j	O
c	O
on	O
zj	O
c	O
a	O
and	O
an	O
approximate	O
r-square	O
value	O
of	O
this	O
is	O
somewhat	O
less	O
than	O
the	O
value	O
of	O
this	O
section	O
obtained	O
using	O
the	O
regression	O
techniques	O
n	O
conclusions	O
of	O
leeds	O
d	O
michie	O
d	O
j	O
spiegelhalter	O
and	O
c	O
c	O
taylor	O
university	O
of	O
strathclyde	O
mrc	O
biostatistics	O
unit	O
cambridge	O
and	O
university	O
introduction	O
in	O
this	O
chapter	O
we	O
try	O
to	O
draw	O
together	O
the	O
evidence	O
of	O
the	O
comparative	B
trials	I
and	O
subsequent	O
analyses	O
comment	O
on	O
the	O
experiences	O
of	O
the	O
users	O
of	O
the	O
algorithms	O
and	O
suggest	O
topics	O
and	O
areas	O
which	O
need	O
further	O
work	O
we	O
begin	O
with	O
some	O
comments	O
on	O
each	O
of	O
the	O
methods	O
it	O
should	O
be	O
noted	O
here	O
that	O
our	O
comments	O
are	O
often	O
directed	O
towards	O
a	O
specific	O
implementation	O
of	O
a	O
method	O
rather	O
than	O
the	O
method	O
per	O
se	O
in	O
some	O
instances	O
the	O
slowness	O
or	O
otherwise	O
poor	O
performance	O
of	O
an	O
algorithm	O
is	O
due	O
at	O
least	O
in	O
part	O
to	O
the	O
lack	O
of	O
sophistication	O
of	O
the	O
program	O
in	O
addition	O
to	O
the	O
potential	O
weakness	O
of	O
the	O
programmer	O
there	O
is	O
the	O
potential	O
reported	O
on	O
previous	O
chapters	O
were	O
based	O
on	O
a	O
version	O
programmed	O
in	O
lisp	O
a	O
version	O
is	O
now	O
available	O
in	O
the	O
c	O
language	O
which	O
cuts	O
the	O
cpu	O
time	B
by	O
a	O
factor	O
of	O
in	O
terms	O
of	O
error	O
rates	O
observed	O
differences	O
in	O
goodness	O
of	O
result	O
can	O
arise	O
from	O
inexperience	O
of	O
the	O
user	O
to	O
give	O
an	O
example	B
the	O
trials	O
of	O
different	O
suitabilities	O
of	O
the	O
basic	O
methods	O
for	O
given	O
datasets	O
different	O
sophistications	O
of	O
default	B
procedures	O
for	O
parameter	O
settings	O
different	O
sophistication	O
of	O
the	O
program	O
user	O
in	O
selection	O
of	O
options	O
and	O
tuning	B
of	I
parameters	I
occurrence	O
and	O
effectiveness	O
of	O
pre-processing	O
of	O
the	O
data	O
by	O
the	O
user	O
the	O
stronger	O
a	O
program	O
in	O
respect	O
of	O
then	O
the	O
better	O
buffered	O
against	O
shortcomings	O
in	O
alternatively	O
if	O
there	O
are	O
no	O
options	O
to	O
select	O
or	O
parameters	O
to	O
tune	O
then	O
item	O
is	O
not	O
important	O
we	O
give	O
a	O
general	O
view	O
of	O
the	O
ease-of-use	O
and	O
the	O
suitable	O
applications	O
of	O
the	O
algorithms	O
some	O
of	O
the	O
properties	O
are	O
subject	O
to	O
different	O
interpretations	O
for	O
example	B
in	O
general	O
a	O
decision	O
tree	O
is	O
considered	O
to	O
be	O
less	O
easy	O
to	O
understand	O
than	O
decision	O
rules	O
however	O
both	O
are	O
much	O
easier	O
to	O
understand	O
than	O
a	O
regression	O
formula	O
which	O
contains	O
only	O
coefficients	O
and	O
some	O
algorithms	O
do	O
not	O
give	O
any	O
easily	O
summarised	O
rule	O
at	O
all	O
example	B
k-nn	B
address	O
for	O
correspondence	O
department	O
of	O
statistics	O
university	O
of	O
leeds	O
leeds	O
u	O
k	O
conclusions	O
the	O
remaining	O
sections	O
discuss	O
more	O
general	O
issues	O
that	O
have	O
been	O
raised	O
in	O
the	O
trials	O
such	O
as	O
time	B
and	O
memory	B
requirements	O
the	O
use	O
of	O
cost	B
matrices	I
and	O
general	O
warnings	O
on	O
the	O
interpretation	O
of	O
our	O
results	O
user	O
s	O
guide	O
to	O
programs	O
here	O
we	O
tabulate	O
some	O
measures	B
to	O
summarise	O
each	O
algorithm	O
some	O
are	O
subjective	O
quantities	O
based	O
on	O
the	O
user	O
s	O
perception	O
of	O
the	O
programs	O
used	O
in	O
statlog	B
and	O
may	O
not	O
hold	O
for	O
other	O
implementations	O
of	O
the	O
method	O
for	O
example	B
many	O
of	O
the	O
classical	O
statistical	B
algorithms	O
can	O
handle	O
missing	B
values	I
whereas	O
those	O
used	O
in	O
this	O
project	O
could	O
not	O
this	O
would	O
necessitate	O
a	O
front-end	O
to	O
replace	O
missing	B
values	I
before	O
running	O
the	O
algorithm	O
similarly	O
all	O
of	O
these	O
programs	O
should	O
be	O
able	O
to	O
incorporate	O
costs	B
into	O
their	O
classification	B
procedure	O
yet	O
some	O
of	O
them	O
have	O
not	O
in	O
table	O
we	O
give	O
information	O
on	O
various	O
basic	O
capabilities	O
of	O
each	O
algorithm	O
statistical	B
algorithms	O
discriminants	O
it	O
can	O
fairly	O
be	O
said	O
that	O
the	O
performance	O
of	O
linear	O
and	O
quadratic	B
discriminants	I
was	O
exactly	O
as	O
might	O
be	O
predicted	O
on	O
the	O
basis	O
of	O
theory	O
when	O
there	O
was	O
sufficient	O
data	O
and	O
the	O
class	B
covariance	B
matrices	O
quite	O
dissimilar	O
then	O
quadratic	B
discriminant	I
did	O
better	O
although	O
at	O
the	O
expense	O
of	O
some	O
computational	O
costs	B
several	O
practical	O
problems	O
remain	O
however	O
the	O
problem	O
of	O
deleting	O
attributes	B
if	O
they	O
do	O
not	O
contribute	O
usefully	O
to	O
the	O
discrimination	B
between	O
classes	B
mclachlan	O
the	O
desirability	O
of	O
transforming	O
the	O
data	O
and	O
the	O
possibility	O
of	O
including	O
some	O
quadratic	O
terms	O
in	O
the	O
linear	B
discriminant	I
as	O
a	O
compromise	O
between	O
pure	O
linear	O
and	O
quadratic	O
discrimination	B
much	O
work	O
needs	O
to	O
be	O
done	O
in	O
this	O
area	O
we	O
found	O
that	O
there	O
was	O
little	O
practical	O
difference	O
in	O
the	O
performance	O
of	O
ordinary	O
and	O
logistic	B
discrimination	B
this	O
has	O
been	O
observed	O
before	O
fienberg	O
quotes	O
an	O
example	B
where	O
the	O
superiority	O
of	O
logistic	O
regression	O
over	O
discriminant	O
analysis	O
is	O
slight	O
and	O
is	O
related	O
to	O
the	O
well-known	O
fact	O
that	O
different	O
link	O
functions	O
in	O
generalised	O
linear	O
models	O
often	O
fit	O
empirical	O
data	O
equally	O
well	O
especially	O
in	O
the	O
region	O
near	O
classification	B
boundaries	O
where	O
the	O
curvature	O
of	O
the	O
probability	O
surface	O
may	O
be	O
negligible	O
mclachlan	O
quotes	O
several	O
empirical	O
studies	O
in	O
which	O
the	O
allocation	O
performance	O
of	O
logistic	O
regression	O
was	O
very	O
similar	O
to	O
that	O
of	O
linear	O
discriminants	O
in	O
view	O
of	O
the	O
much	O
greater	O
computational	O
burden	O
required	O
the	O
advice	O
must	O
be	O
to	O
use	O
linear	O
or	O
quadratic	B
discriminants	I
for	O
large	O
datasets	O
the	O
situation	O
may	O
well	O
be	O
different	O
for	O
small	O
datasets	O
this	O
algorithm	O
was	O
never	O
intended	O
for	O
the	O
size	O
of	O
datasets	O
considered	O
in	O
this	O
book	O
and	O
it	O
often	O
failed	O
on	O
the	O
larger	O
datasets	O
with	O
no	O
adequate	O
diagnostics	O
it	O
can	O
accept	O
attribute	O
data	O
with	O
both	O
numeric	O
and	O
logical	O
values	O
and	O
in	O
this	O
respect	O
appears	O
superior	O
to	O
the	O
other	O
statistical	B
algorithms	O
the	O
cross-validation	O
methods	O
for	O
parameter	O
selection	O
are	O
too	O
cumbersome	O
for	O
these	O
larger	O
datasets	O
although	O
in	O
principle	O
they	O
should	O
work	O
an	O
outstanding	O
problem	O
here	O
is	O
to	O
choose	O
good	O
smoothing	B
parameters	I
this	O
program	O
uses	O
a	O
sec	O
statistical	B
algorithms	O
table	O
users	O
guide	O
to	O
the	O
classification	B
algorithms	O
algorithm	O
mv	O
cost	O
interp	O
compreh	O
params	O
user-fr	O
data	O
discrim	B
quadisc	B
logdisc	B
smart	B
k-nn	B
castle	B
cart	B
indcart	B
newid	B
baytree	O
naivebay	O
itrule	B
kohonen	B
backprop	B
rbf	B
lvq	B
cascade	B
n	O
n	O
n	O
n	O
n	O
n	O
n	O
y	O
y	O
y	O
y	O
y	O
y	O
y	O
y	O
n	O
y	O
n	O
n	O
n	O
n	O
n	O
n	O
t	O
t	O
t	O
lt	O
lt	O
t	O
t	O
t	O
t	O
n	O
n	O
t	O
t	O
n	O
n	O
n	O
lt	O
n	O
lt	O
t	O
n	O
n	O
t	O
y	O
y	O
y	O
n	O
n	O
n	O
y	O
y	O
y	O
y	O
y	O
n	O
y	O
y	O
y	O
n	O
y	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
nc	O
nc	O
n	O
nc	O
nc	O
nc	O
nc	O
nch	O
nc	O
n	O
nc	O
nc	O
nc	O
nc	O
n	O
nc	O
n	O
n	O
n	O
n	O
key	O
mv	O
cost	O
interp	O
compreh	O
params	O
user-fr	O
data	O
whether	O
the	O
program	O
accepts	O
missing	B
values	I
whether	O
the	O
program	O
has	O
a	O
built-in	O
facility	O
to	O
deal	O
with	O
a	O
cost	B
matrix	I
at	O
learning	O
testing	O
or	O
not	O
at	O
all	O
whether	O
the	O
program	O
gives	O
an	O
easily	O
understood	O
classifier	B
very	O
easy	O
to	O
interpret	O
whether	O
the	O
principle	O
of	O
the	O
method	O
is	O
easily	O
understood	O
very	O
easy	O
to	O
grasp	O
whether	O
the	O
program	O
has	O
good	O
user-guidelines	O
or	O
automatic	O
selection	O
of	O
important	O
parameters	O
whether	O
the	O
program	O
is	O
user-friendly	O
type	O
of	O
data	O
allowed	O
in	O
the	O
attributes	B
numerical	O
c	O
categorical	O
h	O
hierarchical	O
however	O
note	O
that	O
categorical	O
data	O
can	O
always	O
be	O
transormed	O
to	O
numerical	O
conclusions	O
multiplicative	O
kernel	O
which	O
may	O
be	O
rather	O
inflexible	O
if	O
some	O
of	O
the	O
attributes	B
are	O
highly	O
correlated	O
fukunaga	O
suggests	O
a	O
pre-whitening	O
of	O
the	O
data	O
which	O
is	O
equivalent	O
to	O
using	O
a	O
multivariate	O
normal	O
kernel	O
with	O
parameters	O
estimated	O
from	O
the	O
sample	O
covariance	B
matrix	I
this	O
method	O
has	O
shown	O
promise	O
in	O
some	O
of	O
the	O
datasets	O
here	O
although	O
it	O
is	O
not	O
very	O
robust	O
and	O
of	O
course	O
still	O
needs	O
smoothing	B
parameter	I
choices	O
has	O
a	O
slightly	O
lower	O
error-rate	O
than	O
k-nn	B
and	O
uses	O
marginally	O
less	O
storage	B
but	O
takes	O
about	O
twice	O
as	O
long	O
in	O
training	O
and	O
testing	O
k-nn	B
is	O
already	O
a	O
very	O
slow	O
this	O
may	O
not	O
generally	O
be	O
true	O
is	O
a	O
special	O
case	O
of	O
the	O
kernel	O
method	O
so	O
it	O
should	O
be	O
expected	O
to	O
do	O
better	O
algorithm	O
however	O
since	O
k-nn	B
was	O
always	O
set	O
to	O
indeed	O
nearest	B
neighbour	I
although	O
this	O
method	O
did	O
very	O
well	O
on	O
the	O
whole	O
as	O
expected	O
it	O
was	O
slowest	O
of	O
all	O
for	O
the	O
very	O
large	O
datasets	O
however	O
it	O
is	O
known	O
that	O
substantial	O
time	B
saving	O
can	O
be	O
effected	O
at	O
the	O
expense	O
of	O
some	O
slight	O
loss	O
of	O
accuracy	B
by	O
using	O
a	O
condensed	O
version	O
of	O
the	O
training	O
data	O
an	O
area	O
that	O
requires	O
further	O
study	O
is	O
in	O
fast	O
data-based	O
methods	O
for	O
choosing	O
appropriate	O
distance	B
measures	B
variable	O
selection	O
and	O
the	O
appropriate	O
number	O
of	O
neighbours	O
the	O
program	O
in	O
these	O
trials	O
normally	O
used	O
just	O
the	O
nearest	B
neighbour	I
which	O
is	O
certainly	O
not	O
optimal	O
a	O
simulation	O
study	O
on	O
this	O
problem	O
was	O
carried	O
out	O
by	O
enas	O
choi	O
it	O
is	O
clear	O
from	O
many	O
of	O
the	O
results	O
that	O
substantial	O
improved	O
accuracy	B
can	O
be	O
obtained	O
with	O
careful	O
choice	B
of	I
variables	I
but	O
the	O
current	O
implementation	O
is	O
much	O
too	O
slow	O
indeed	O
lvq	B
has	O
about	O
the	O
same	O
error-rate	O
as	O
k-nn	B
but	O
is	O
about	O
times	O
faster	O
and	O
uses	O
about	O
less	O
storage	B
where	O
scaling	O
of	O
attributes	B
is	O
not	O
important	O
such	O
as	O
in	O
object	B
recognition	I
datasets	I
k-nearest	B
neighbour	I
is	O
first	O
in	O
the	O
trials	O
yet	O
the	O
explanatory	O
power	O
of	O
k-nearest	B
neighbour	I
might	O
be	O
said	O
to	O
be	O
very	O
small	O
smart	B
smart	B
is	O
both	O
a	O
classification	B
and	O
regression	O
type	O
algorithm	O
which	O
is	O
most	O
easily	O
used	O
in	O
batch	O
mode	O
it	O
is	O
a	O
very	O
slow	O
algorithm	O
to	O
train	O
but	O
quite	O
quick	O
in	O
the	O
classification	B
stage	O
the	O
output	B
is	O
virtually	O
incomprehensible	O
to	O
a	O
non-statistician	O
but	O
a	O
graphical	O
frontend	O
could	O
be	O
written	O
to	O
improve	O
the	O
interpretability	O
see	O
the	O
example	B
in	O
section	O
in	O
addition	O
there	O
are	O
some	O
difficulties	O
in	O
choosing	O
the	O
number	O
of	O
terms	O
to	O
include	O
in	O
the	O
model	O
this	O
is	O
a	O
similar	O
problem	O
to	O
choosing	O
the	O
smoothing	B
parameter	I
in	O
kernel	O
methods	O
or	O
the	O
number	O
of	O
neighbours	O
in	O
a	O
nearest	B
neighbour	I
classifier	B
a	O
major	O
advantage	O
smart	B
has	O
over	O
most	O
of	O
the	O
other	O
algorithms	O
is	O
that	O
it	O
accepts	O
a	O
cost	B
matrix	I
in	O
its	O
training	O
as	O
well	O
as	O
in	O
its	O
testing	O
phase	O
which	O
in	O
principle	O
ought	O
to	O
make	O
it	O
much	O
more	O
suited	O
for	O
tackling	O
problems	O
where	O
costs	B
are	O
important	O
naive	B
bayes	I
naive	B
bayes	I
can	O
easily	O
handle	O
unknown	O
or	O
missing	B
values	I
the	O
main	O
drawback	O
of	O
the	O
algorithm	O
is	O
perhaps	O
its	O
na	O
vety	O
i	O
e	O
it	O
uses	O
directly	O
bayes	B
theorem	I
to	O
classify	O
examples	O
in	O
addition	O
for	O
those	O
not	O
fluent	O
with	O
the	O
statistical	B
background	O
there	O
is	O
generally	O
little	O
indication	O
of	O
why	O
it	O
has	O
classified	O
some	O
examples	O
in	O
one	O
class	B
or	O
the	O
other	O
theory	O
indicates	O
and	O
our	O
experience	O
confirms	O
that	O
naive	B
bayes	I
does	O
best	O
if	O
the	O
attributes	B
are	O
conditionally	O
independent	O
given	O
the	O
class	B
this	O
seems	O
to	O
hold	O
true	O
for	O
many	O
sec	O
decision	B
trees	I
medical	B
datasets	I
one	O
reason	O
for	O
this	O
might	O
be	O
that	O
doctors	O
gather	O
as	O
many	O
different	O
independent	O
bits	O
of	O
relevant	O
information	O
as	O
possible	O
but	O
they	O
do	O
not	O
include	O
two	O
attributes	B
where	O
one	O
would	O
do	O
for	O
example	B
it	O
could	O
be	O
that	O
only	O
one	O
measure	B
of	O
high	O
blood	O
pressure	O
diastolic	O
would	O
be	O
quoted	O
although	O
two	O
and	O
systolic	O
would	O
be	O
available	O
castle	B
in	O
essence	O
castle	B
is	O
a	O
full	O
bayesian	O
modelling	O
algorithm	O
i	O
e	O
it	O
builds	O
a	O
comprehensive	O
probabilistic	O
model	O
of	O
the	O
events	O
this	O
case	O
attributes	B
of	O
the	O
empirical	O
data	O
it	O
can	O
be	O
used	O
to	O
infer	O
the	O
probability	O
of	O
attributes	B
as	O
well	O
as	O
classes	B
given	O
the	O
values	O
of	O
other	O
attributes	B
the	O
main	O
reason	O
for	O
using	O
castle	B
is	O
that	O
the	O
polytree	O
models	O
the	O
whole	O
structure	O
of	O
the	O
data	O
and	O
no	O
special	O
role	O
is	O
given	O
to	O
the	O
variable	O
being	O
predicted	O
viz	O
the	O
class	B
of	O
the	O
object	O
however	O
instructive	O
this	O
may	O
be	O
it	O
is	O
not	O
the	O
principal	O
task	O
in	O
the	O
above	O
trials	O
is	O
to	O
produce	O
a	O
classification	B
procedure	O
so	O
maybe	O
there	O
should	O
be	O
an	O
option	O
in	O
castle	B
to	O
produce	O
a	O
polytree	O
which	O
classifies	O
rather	O
than	O
fits	O
all	O
the	O
variables	O
to	O
emphasise	O
the	O
point	O
it	O
is	O
easy	O
to	O
deflect	O
the	O
polytree	O
algorithm	O
by	O
making	O
it	O
fit	O
irrelevant	O
bits	O
of	O
the	O
tree	O
are	O
strongly	O
related	O
to	O
each	O
other	O
but	O
are	O
irrelevant	O
to	O
classification	B
castle	B
can	O
normally	O
be	O
used	O
in	O
both	O
interactive	O
and	O
batch	O
modes	O
it	O
accepts	O
any	O
data	O
described	O
in	O
probabilities	O
and	O
events	O
including	O
descriptions	O
of	O
attributes-and-class	O
pairs	O
of	O
data	O
such	O
as	O
that	O
used	O
here	O
however	O
all	O
attributes	B
and	O
classes	B
must	O
be	O
discretised	O
to	O
categorical	O
or	O
logical	O
data	O
the	O
results	O
of	O
castle	B
are	O
in	O
the	O
form	O
of	O
a	O
polytree	O
that	O
provides	O
a	O
graphical	O
explanation	O
of	O
the	O
probabilistic	O
relationships	O
between	O
attributes	B
and	O
classes	B
thus	O
it	O
is	O
better	O
in	O
term	O
of	O
comprehensibility	B
compared	O
to	O
some	O
of	O
the	O
other	O
statistical	B
algorithms	O
in	O
its	O
explanation	O
of	O
the	O
probabilistic	O
relationships	O
between	O
attributes	B
and	O
classes	B
the	O
performance	O
of	O
castle	B
should	O
be	O
related	O
to	O
how	O
tree-like	O
the	O
dataset	O
is	O
a	O
major	O
criticism	O
of	O
castle	B
is	O
that	O
there	O
is	O
no	O
internal	O
measure	B
that	O
tells	O
us	O
how	O
closely	O
the	O
empirical	O
data	O
are	O
fitted	O
by	O
the	O
chosen	O
polytree	O
we	O
recommend	O
that	O
any	O
future	O
implementation	O
of	O
castle	B
incorporates	O
such	O
a	O
polytree	O
measure	B
it	O
should	O
be	O
straightforward	O
to	O
build	O
a	O
goodness-of-fit	O
measure	B
into	O
castle	B
based	O
on	O
a	O
standard	O
test	O
as	O
a	O
classifier	B
castle	B
did	O
best	O
in	O
the	O
credit	B
datasets	I
where	O
generally	O
only	O
a	O
few	O
attributes	B
are	O
important	O
but	O
its	O
most	O
useful	O
feature	O
is	O
the	O
ability	O
to	O
produce	O
simple	O
models	O
of	O
the	O
data	O
unfortunately	O
simple	O
models	O
fitted	O
only	O
a	O
few	O
of	O
our	O
datasets	O
decision	B
trees	I
there	O
is	O
a	O
confusing	O
diversity	O
of	O
decision	O
tree	O
algorithms	O
but	O
they	O
all	O
seem	O
to	O
perform	O
at	O
about	O
the	O
same	O
level	O
five	O
of	O
the	O
decision	B
trees	I
considered	O
in	O
this	O
book	O
are	O
similar	O
in	O
structure	O
to	O
the	O
original	O
algorithm	O
with	O
partitions	O
being	O
made	O
by	O
splitting	O
on	O
an	O
attribute	O
and	O
with	O
an	O
entropy	B
measure	B
for	O
the	O
splits	O
there	O
are	O
no	O
indications	O
that	O
this	O
or	O
that	O
splitting	B
criterion	I
is	O
best	O
but	O
the	O
case	O
for	O
using	O
some	O
kind	O
of	O
pruning	B
is	O
overwhelming	O
although	O
again	O
our	O
results	O
are	O
too	O
limited	O
to	O
say	O
exactly	O
how	O
much	O
pruning	B
to	O
use	O
it	O
was	O
hoped	O
to	O
relate	O
the	O
performance	O
of	O
a	O
decision	O
tree	O
to	O
some	O
measures	B
of	O
complexity	O
and	O
pruning	B
specifically	O
the	O
average	O
depth	O
of	O
the	O
tree	O
and	O
the	O
number	O
of	O
terminal	O
nodes	O
in	O
a	O
sense	O
cart	B
s	O
cost-complexity	O
pruning	B
automates	O
this	O
has	O
generally	O
much	O
fewer	O
nodes	O
so	O
gives	O
a	O
simpler	O
tree	O
many	O
more	O
nodes	O
and	O
occasionally	O
scores	O
a	O
success	O
because	O
of	O
that	O
the	O
fact	O
that	O
all	O
newid	B
indcart	B
generally	O
has	O
conclusions	O
the	O
decision	B
trees	I
perform	O
at	O
the	O
same	O
accuracy	B
with	O
such	O
different	O
pruning	B
procedures	O
suggests	O
that	O
much	O
work	O
needs	O
to	O
be	O
done	O
on	O
the	O
question	O
of	O
how	O
many	O
nodes	O
to	O
use	O
on	O
the	O
basis	O
of	O
our	O
trials	O
on	O
the	O
tsetse	O
fly	O
data	O
and	O
the	O
segmentation	O
data	O
we	O
speculate	O
that	O
decision	O
tree	O
methods	O
will	O
work	O
well	O
compared	O
to	O
classical	O
statistical	B
methods	O
when	O
the	O
data	O
are	O
multimodal	O
their	O
success	O
in	O
the	O
shuttle	B
and	O
technical	B
datasets	O
is	O
due	O
to	O
the	O
special	O
structure	O
of	O
these	O
datasets	O
in	O
the	O
case	O
of	O
the	O
technical	B
dataset	I
observations	O
were	O
partly	O
pre-classified	O
by	O
the	O
use	O
of	O
a	O
decision	O
tree	O
and	O
in	O
the	O
shuttle	B
dataset	I
we	O
believe	O
that	O
this	O
may	O
also	O
be	O
so	O
although	O
we	O
have	O
been	O
unable	O
to	O
obtain	O
confirmation	O
from	O
the	O
data	O
provider	O
among	O
the	O
decision	B
trees	I
indcart	B
cart	B
and	O
method	O
emerge	O
as	O
superior	O
to	O
others	O
because	O
they	O
incorporate	O
costs	B
into	O
decisions	O
both	O
cart	B
and	O
indcart	B
can	O
deal	O
with	O
categorical	B
variables	I
and	O
cart	B
has	O
an	O
important	O
additional	O
feature	O
in	O
that	O
it	O
has	O
a	O
systematic	O
method	O
for	O
dealing	O
with	O
missing	B
values	I
however	O
for	O
the	O
larger	O
datasets	O
the	O
commercial	O
package	O
cart	B
often	O
failed	O
where	O
the	O
indcart	B
implementation	O
did	O
not	O
in	O
common	O
with	O
all	O
other	O
decision	B
trees	I
cart	B
indcart	B
and	O
have	O
the	O
advantage	O
of	O
being	O
distribution	O
free	O
incorporating	O
prior	O
knowledge	O
about	O
the	O
dataset	O
in	O
particular	O
certain	O
forms	O
of	O
hierarchical	B
structure	I
see	O
chapter	O
we	O
looked	O
at	O
one	O
dataset	O
that	O
was	O
hierarchical	O
in	O
nature	O
in	O
newid	B
and	O
classifiers	O
is	O
very	O
close	O
the	O
main	O
reason	O
for	O
choosing	O
of	O
the	O
which	O
and	O
newid	B
are	O
direct	O
descendants	O
of	O
and	O
empirically	O
their	O
performance	O
as	O
would	O
be	O
to	O
use	O
other	O
aspects	O
package	O
for	O
example	B
the	O
interactive	O
graphical	O
package	O
and	O
the	O
possibility	O
of	O
showed	O
considerable	O
advantage	O
over	O
other	O
methods	O
see	O
section	O
newid	B
is	O
based	O
on	O
ross	O
quinlan	O
s	O
original	O
program	O
which	O
generates	O
decision	B
trees	I
from	O
examples	O
it	O
is	O
similar	O
to	O
in	O
its	O
interface	O
and	O
command	O
system	O
similar	O
to	O
newid	B
can	O
be	O
used	O
in	O
both	O
interactive	O
and	O
batch	O
mode	O
the	O
interactive	O
mode	O
is	O
its	O
native	O
mode	O
and	O
to	O
run	O
in	O
batch	O
mode	O
users	O
need	O
to	O
write	O
a	O
unix	O
shell	O
script	O
as	O
for	O
newid	B
accepts	O
attribute-value	O
data	O
sets	O
with	O
both	O
logical	O
and	O
numeric	O
data	O
newid	B
has	O
a	O
post-pruning	O
facility	O
that	O
is	O
used	O
to	O
deal	O
with	O
noise	B
it	O
can	O
also	O
deal	O
with	O
unknown	O
values	O
newid	B
outputs	O
a	O
confusion	O
matrix	O
but	O
this	O
confusion	O
matrix	O
must	O
be	O
used	O
with	O
care	O
because	O
the	O
matrix	O
has	O
an	O
extra	O
row	O
and	O
column	O
for	O
unclassified	O
examples	O
some	O
examples	O
are	O
not	O
classified	O
by	O
the	O
decision	O
tree	O
it	O
does	O
not	O
accept	O
or	O
incorporate	O
a	O
cost	B
matrix	I
different	O
from	O
the	O
usual	O
format	O
mainly	O
due	O
to	O
the	O
need	O
to	O
express	O
hierarchical	O
attributes	B
when	O
there	O
are	O
such	O
but	O
for	O
non-hierarchical	O
data	O
there	O
is	O
very	O
limited	O
requirement	O
for	O
data	O
is	O
an	O
extension	O
to	O
style	O
of	O
decision	O
tree	O
classifiers	O
to	O
learn	O
structures	O
from	O
a	O
predefined	O
hierarchy	B
of	O
attributes	B
similarly	O
to	O
it	O
uses	O
an	O
attribute-value	O
based	O
format	O
for	O
examples	O
with	O
both	O
logical	O
and	O
numeric	O
data	O
because	O
of	O
its	O
hierarchical	O
representation	O
it	O
can	O
also	O
encode	O
some	O
relations	O
between	O
attribute	O
values	O
it	O
can	O
be	O
run	O
in	O
interactive	O
mode	O
and	O
data	O
can	O
be	O
edited	O
visually	O
under	O
its	O
user	O
interface	O
conversion	O
user	O
interacts	O
with	O
uses	O
an	O
internal	O
format	O
that	O
is	O
can	O
deal	O
with	O
unknown	O
values	O
in	O
examples	O
and	O
multi-valued	O
attributes	B
via	O
a	O
graphical	O
interface	O
this	O
interface	O
consists	O
of	O
graphical	O
it	O
is	O
also	O
able	O
to	O
deal	O
with	O
knowledge	O
concerning	O
the	O
studied	O
domain	O
but	O
with	O
the	O
exception	O
of	O
the	O
machine	B
faults	I
dataset	I
this	O
aspect	O
was	O
deliberately	O
not	O
studied	O
in	O
this	O
book	O
the	O
sec	O
decision	B
trees	I
editors	O
which	O
enable	O
the	O
user	O
to	O
define	O
the	O
knowledge	O
of	O
the	O
domain	O
to	O
interactively	O
build	O
the	O
example	B
base	O
and	O
to	O
go	O
through	O
the	O
hierarchy	B
of	O
classes	B
and	O
the	O
decision	O
tree	O
produces	O
decision	B
trees	I
which	O
can	O
be	O
very	O
large	O
compared	O
to	O
the	O
other	O
decision	O
tree	O
algorithms	O
the	O
trials	O
reported	O
here	O
suggest	O
that	O
is	O
relatively	O
slow	O
this	O
older	O
version	O
used	O
common	O
lisp	O
and	O
has	O
now	O
been	O
superseded	O
by	O
a	O
c	O
version	O
resulting	O
in	O
a	O
much	O
faster	O
program	O
is	O
the	O
direct	O
descendent	O
of	O
it	O
is	O
run	O
in	O
batch	O
mode	O
for	O
training	O
with	O
attribute-value	O
data	O
input	B
for	O
testing	O
both	O
interactive	O
and	O
batch	O
modes	O
are	O
available	O
both	O
logical	O
and	O
numeric	O
values	O
can	O
be	O
used	O
in	O
the	O
attributes	B
it	O
needs	O
a	O
declaration	O
for	O
the	O
types	O
and	O
range	O
of	O
attributes	B
and	O
such	O
information	O
needs	O
to	O
be	O
placed	O
in	O
a	O
separate	O
file	O
is	O
very	O
easy	O
to	O
set	O
up	O
and	O
run	O
in	O
fact	O
it	O
is	O
only	O
a	O
set	O
of	O
unix	O
commands	O
which	O
should	O
be	O
familiar	O
to	O
all	O
unix	O
users	O
there	O
are	O
very	O
few	O
parameters	O
apart	O
from	O
the	O
pruning	B
criterion	O
no	O
major	O
parameter	O
adjustment	O
is	O
needed	O
for	O
most	O
applications	O
in	O
the	O
trials	O
reported	O
here	O
the	O
windowing	O
facility	O
was	O
disabled	O
produces	O
a	O
confusion	O
matrix	O
from	O
classification	B
results	O
however	O
it	O
does	O
not	O
incorporate	O
a	O
cost	B
matrix	I
allows	O
the	O
users	O
to	O
adjust	O
the	O
degree	O
of	O
the	O
tracing	O
information	O
displayed	O
while	O
the	O
algorithm	O
is	O
running	O
this	O
facility	O
can	O
satisfy	O
both	O
the	O
users	O
who	O
do	O
not	O
need	O
to	O
know	O
the	O
internal	O
operations	O
of	O
the	O
algorithm	O
and	O
the	O
users	O
who	O
need	O
to	O
monitor	O
the	O
intermidate	O
steps	O
of	O
tree	O
construction	O
note	O
that	O
has	O
a	O
rule-generating	O
module	O
which	O
often	O
improves	O
the	O
error	B
rate	I
and	O
almost	O
invariably	O
the	O
user-transparancy	O
but	O
this	O
was	O
not	O
used	O
in	O
the	O
comparative	B
trials	I
reported	O
in	O
chapter	O
cart	B
and	O
indcart	B
cart	B
and	O
indcart	B
are	O
decision	O
tree	O
algorithms	O
based	O
on	O
the	O
work	O
of	O
breiman	O
et	O
al	O
the	O
statlog	B
version	O
of	O
cart	B
is	O
the	O
commercial	O
derivative	O
of	O
the	O
original	O
algorithm	O
developed	O
at	O
caltech	O
both	O
are	O
classification	B
and	O
regression	O
algorithms	O
but	O
they	O
treat	O
regression	O
and	O
unknown	O
values	O
in	O
the	O
data	O
somewhat	O
differently	O
in	O
both	O
systems	O
there	O
are	O
very	O
few	O
parameters	O
to	O
adjust	O
for	O
new	O
tasks	O
the	O
noise	B
handling	O
mechanism	O
of	O
the	O
two	O
algorithms	O
are	O
very	O
similar	O
both	O
can	O
also	O
deal	O
with	O
unknown	O
values	O
though	O
in	O
different	O
ways	O
the	O
algorithms	O
both	O
output	B
a	O
decision	O
tree	O
and	O
a	O
confusion	O
matrix	O
as	O
output	B
but	O
only	O
cart	B
incorporates	O
costs	B
it	O
does	O
so	O
in	O
both	O
training	O
and	O
test	O
phases	O
note	O
that	O
cart	B
failed	O
to	O
run	O
in	O
many	O
of	O
trials	O
involving	O
very	O
large	O
datasets	O
is	O
a	O
numeric	O
value	O
decision	O
tree	O
classifier	B
using	O
statistical	B
methods	O
thus	O
discrete	O
values	O
have	O
to	O
be	O
changed	O
into	O
numeric	O
ones	O
is	O
very	O
easy	O
to	O
set	O
up	O
and	O
run	O
it	O
has	O
a	O
number	O
of	O
menus	O
to	O
guide	O
the	O
user	O
to	O
complete	O
operations	O
however	O
there	O
are	O
a	O
number	O
of	O
parameters	O
and	O
for	O
novice	O
users	O
the	O
meanings	O
of	O
these	O
parameters	O
are	O
not	O
very	O
easy	O
to	O
understand	O
the	O
results	O
from	O
different	O
parameter	O
settings	O
can	O
be	O
very	O
different	O
but	O
tuning	B
of	I
parameters	I
is	O
implemented	O
in	O
a	O
semi-automatic	O
manner	O
the	O
decision	B
trees	I
produced	O
by	O
are	O
usually	O
quite	O
small	O
and	O
are	O
reasonably	O
easy	O
to	O
understand	O
compared	O
to	O
algorithms	O
such	O
as	O
when	O
used	O
with	O
default	B
settings	O
of	O
conclusions	O
pruning	B
parameters	O
occasionally	O
from	O
the	O
point	O
of	O
view	O
of	O
minimising	O
error	O
rates	O
the	O
tree	O
is	O
over-pruned	O
though	O
of	O
course	O
the	O
rules	O
are	O
then	O
more	O
transparent	O
produces	O
a	O
confusion	O
matrix	O
and	O
incorporates	O
a	O
cost	B
matrix	I
bayes	B
tree	I
our	O
trials	O
confirm	O
the	O
results	O
reported	O
in	O
buntine	O
bayes	O
trees	O
are	O
generally	O
slower	O
in	O
learning	O
and	O
testing	O
but	O
perform	O
at	O
around	O
the	O
same	O
accuracy	B
as	O
say	O
or	O
newid	B
however	O
it	O
is	O
not	O
so	O
similar	O
to	O
these	O
two	O
algorithms	O
as	O
one	O
might	O
expect	O
sometimes	O
being	O
substantially	O
better	O
the	O
cost	B
datasets	I
sometimes	O
marginally	O
better	O
the	O
segmented	O
image	B
datasets	I
and	O
sometimes	O
noticeably	O
worse	O
bayes	B
tree	I
also	O
did	O
surprisingly	O
badly	O
for	O
a	O
decision	O
tree	O
on	O
the	O
technical	B
dataset	I
this	O
is	O
probably	O
due	O
to	O
the	O
relatively	O
small	O
sample	O
sizes	O
for	O
a	O
large	O
number	O
of	O
the	O
classes	B
samples	O
with	O
very	O
small	O
a	O
priori	O
probabilities	O
are	O
allocated	O
to	O
the	O
most	O
frequent	O
classes	B
as	O
the	O
dataset	O
is	O
not	O
large	O
enough	O
for	O
the	O
a	O
priori	O
probabilities	O
to	O
be	O
adapted	O
by	O
the	O
empirical	O
probabilities	O
apart	O
from	O
the	O
technical	B
dataset	I
bayes	O
trees	O
probably	O
do	O
well	O
as	O
a	O
result	O
of	O
the	O
explicit	O
mechanism	O
for	O
pruning	B
via	O
smoothing	O
class	B
probabilities	O
and	O
their	O
success	O
gives	O
empirical	O
justification	O
for	O
the	O
at-first-sight-artificial	O
model	O
of	O
tree	O
probabilities	O
rule-based	B
methods	I
the	O
rule-based	O
algorithm	O
also	O
belongs	O
to	O
the	O
general	O
class	B
of	O
recursive	B
partitioning	I
algorithms	O
of	O
the	O
two	O
possible	O
variants	O
ordered	O
and	O
unordered	O
rules	O
it	O
appears	O
that	O
unordered	O
rules	O
give	O
best	O
results	O
and	O
then	O
the	O
performance	O
is	O
practically	O
indistinguishable	O
from	O
the	O
decision	B
trees	I
while	O
at	O
the	O
same	O
time	B
offering	O
gains	O
in	O
mental	B
fit	I
over	O
decision	B
trees	I
however	O
performed	O
badly	O
on	O
the	O
datasets	O
involving	O
costs	B
although	O
this	O
should	O
not	O
be	O
difficult	O
to	O
fix	O
as	O
a	O
decision	O
tree	O
may	O
be	O
expressed	O
in	O
the	O
form	O
of	O
rules	O
viceversa	O
there	O
appears	O
to	O
be	O
no	O
practical	O
reason	O
for	O
choosing	O
rule-based	B
methods	I
except	O
when	O
the	O
complexity	O
of	O
the	O
data-domain	O
demands	O
some	O
simplifying	O
change	O
of	O
representation	O
this	O
is	O
not	O
an	O
aspect	O
with	O
which	O
this	O
book	O
has	O
been	O
concerned	O
can	O
be	O
used	O
in	O
both	O
interactive	O
and	O
batch	O
mode	O
the	O
interactive	O
mode	O
is	O
its	O
native	O
mode	O
and	O
to	O
run	O
in	O
batch	O
mode	O
users	O
need	O
to	O
write	O
a	O
unix	O
shell	O
script	O
that	O
gives	O
the	O
algorithm	O
a	O
sequence	O
of	O
instructions	O
to	O
run	O
the	O
slight	O
deviation	O
from	O
the	O
other	O
algorithms	O
is	O
that	O
it	O
needs	O
a	O
set	O
of	O
declarations	O
that	O
defines	O
the	O
types	O
and	O
range	O
of	O
attribute-values	O
for	O
each	O
attribute	O
in	O
general	O
there	O
is	O
very	O
little	O
effort	O
needed	O
for	O
data	O
conversion	O
is	O
very	O
easy	O
to	O
set	O
up	O
and	O
run	O
in	O
interactive	O
mode	O
the	O
operations	O
are	O
completely	O
menu	O
driven	O
after	O
some	O
familiarity	O
it	O
would	O
be	O
very	O
easy	O
to	O
write	O
a	O
unix	O
shell	O
script	O
to	O
run	O
the	O
algorithm	O
in	O
batch	O
mode	O
there	O
are	O
a	O
few	O
parameters	O
that	O
the	O
users	O
will	O
have	O
to	O
choose	O
however	O
there	O
is	O
only	O
one	O
parameter	O
rule	O
types	O
which	O
may	O
have	O
significant	O
effect	O
on	O
the	O
training	O
results	O
for	O
most	O
applications	O
itrule	B
strictly	O
speaking	O
itrule	B
is	O
not	O
a	O
classification	B
type	O
algorithm	O
and	O
was	O
not	O
designed	O
for	O
large	O
datasets	O
or	O
for	O
problems	O
with	O
many	O
classes	B
it	O
is	O
an	O
exploratory	O
tool	O
and	O
is	O
best	O
regarded	O
as	O
a	O
way	O
of	O
extracting	O
isolated	O
interesting	O
facts	O
rules	O
from	O
a	O
dataset	O
the	O
facts	O
are	O
not	O
meant	O
to	O
cover	B
all	O
examples	O
we	O
may	O
say	O
that	O
itrule	B
does	O
not	O
look	O
for	O
sec	O
neural	B
networks	I
the	O
best	O
set	O
of	O
rules	O
for	O
classification	B
for	O
any	O
other	O
purpose	O
rather	O
it	O
looks	O
for	O
a	O
set	O
of	O
best	O
rules	O
each	O
rule	O
being	O
very	O
simple	O
in	O
form	O
restricted	O
to	O
conjunctions	O
of	O
two	O
conditions	O
with	O
the	O
rules	O
being	O
selected	O
as	O
having	O
high	O
information	O
content	O
the	O
sense	O
of	O
having	O
high	O
within	O
these	O
limitations	O
and	O
also	O
with	O
the	O
limitation	O
of	O
discretised	O
variates	O
the	O
search	O
for	O
the	O
rules	O
is	O
exhaustive	O
and	O
therefore	O
time-consuming	O
therefore	O
the	O
number	O
of	O
rules	O
found	O
is	O
usually	O
limited	O
to	O
some	O
small	O
number	O
which	O
can	O
be	O
as	O
high	O
as	O
or	O
more	O
however	O
for	O
use	O
in	O
classification	B
problems	O
if	O
the	O
preset	O
rules	O
have	O
been	O
exhausted	O
a	O
default	B
rule	I
must	O
be	O
applied	O
and	O
it	O
is	O
probable	O
that	O
most	O
errors	O
are	O
committed	O
at	O
this	O
stage	O
in	O
some	O
datasets	O
itrule	B
may	O
generate	O
contradictory	O
rules	O
rules	O
with	O
identical	O
condition	O
parts	O
but	O
different	O
conclusions	O
and	O
this	O
may	O
also	O
contribute	O
to	O
a	O
high	O
error-rate	O
this	O
last	O
fact	O
is	O
connected	O
with	O
the	O
asymmetric	O
nature	O
of	O
the	O
compared	O
to	O
the	O
usual	O
entropy	B
measure	B
the	O
algorithm	O
does	O
not	O
incorporate	O
a	O
cost	B
matrix	I
facility	O
but	O
it	O
would	O
appear	O
a	O
relatively	O
simple	O
task	O
to	O
incorporate	O
costs	B
as	O
all	O
rules	O
are	O
associated	O
with	O
a	O
probability	O
measure	B
multi-class	O
problems	O
approximate	O
costs	B
would	O
need	O
to	O
be	O
used	O
because	O
each	O
probability	O
measure	B
refers	O
to	O
the	O
odds	B
of	O
observing	O
a	O
class	B
or	O
not	O
neural	B
networks	I
with	O
care	O
neural	B
networks	I
perform	O
very	O
well	O
as	O
measured	O
by	O
error	B
rate	I
they	O
seem	O
to	O
provide	O
either	O
the	O
best	O
or	O
near	O
to	O
best	O
predictive	O
performance	O
in	O
nearly	O
all	O
cases	O
the	O
notable	O
exceptions	O
are	O
the	O
datasets	O
with	O
cost	B
matrices	I
in	O
terms	O
of	O
computational	O
burden	O
and	O
the	O
level	O
of	O
expertise	O
required	O
they	O
are	O
much	O
more	O
complex	B
than	O
say	O
the	O
machine	O
learning	O
procedures	O
and	O
there	O
are	O
still	O
several	O
unsolved	O
problems	O
most	O
notably	O
the	O
problems	O
of	O
how	O
to	O
incorporate	O
costs	B
into	O
the	O
learning	O
phase	O
and	O
the	O
optimal	O
choice	O
of	O
architecture	O
one	O
major	O
weakness	O
of	O
neural	O
nets	O
is	O
the	O
lack	O
of	O
diagnostic	O
help	O
if	O
something	O
goes	O
wrong	O
it	O
is	O
difficult	O
to	O
pinpoint	O
the	O
difficulty	O
from	O
the	O
mass	O
of	O
inter-related	O
weights	O
and	O
connectivities	O
in	O
the	O
net	O
because	O
the	O
result	O
of	O
learning	O
is	O
a	O
completed	O
network	O
with	O
layers	O
and	O
nodes	O
linked	O
together	O
with	O
nonlinear	O
functions	O
whose	O
relationship	O
cannot	O
easily	O
be	O
described	O
in	O
qualitative	O
terms	O
neural	B
networks	I
are	O
generally	O
difficult	O
to	O
understand	O
these	O
algorithms	O
are	O
usually	O
very	O
demanding	O
on	O
the	O
part	O
of	O
the	O
user	O
he	O
will	O
have	O
to	O
be	O
responsible	O
for	O
setting	O
up	O
the	O
initial	O
weights	O
of	O
the	O
network	O
selecting	O
the	O
correct	O
number	O
of	O
hidden	B
layers	O
and	O
the	O
number	O
of	O
nodes	O
at	O
each	O
layer	O
adjusting	O
these	O
parameters	O
of	O
learning	O
is	O
often	O
a	O
laborious	O
task	O
in	O
addition	O
some	O
of	O
these	O
algorithms	O
are	O
computationally	O
inefficient	O
a	O
notable	O
exception	O
here	O
is	O
lvq	B
which	O
is	O
relatively	O
easy	O
to	O
set	O
up	O
and	O
fast	O
to	O
run	O
backprop	B
this	O
software	O
package	O
contains	O
programs	O
which	O
implement	O
mult-layer	O
perceptrons	O
and	O
radial	O
basis	O
functions	O
as	O
well	O
as	O
several	O
neural	O
network	O
models	O
which	O
are	O
not	O
discussed	O
here	O
including	O
recurrent	B
networks	I
it	O
is	O
reasonably	O
versatile	O
and	O
flexible	O
in	O
that	O
it	O
can	O
be	O
used	O
to	O
train	O
a	O
variety	O
of	O
networks	O
with	O
a	O
variety	O
of	O
methods	O
using	O
a	O
variety	O
of	O
training	O
data	O
formats	O
however	O
its	O
functionality	B
is	O
not	O
embellished	O
with	O
a	O
friendly	O
user	O
interface	O
so	O
its	O
users	O
need	O
at	O
least	O
a	O
cursory	O
familiarity	O
with	O
unix	O
and	O
neural	B
networks	I
and	O
a	O
significant	O
block	O
of	O
time	B
to	O
peruse	O
the	O
documentation	O
and	O
work	O
through	O
the	O
demonstrations	O
the	O
package	O
is	O
also	O
modular	O
and	O
extensible	O
by	O
anyone	O
willing	O
to	O
write	O
source	O
code	O
for	O
conclusions	O
new	O
modules	O
based	O
on	O
existing	O
templates	O
and	O
hooks	O
one	O
of	O
the	O
fundamental	O
modules	O
provides	O
routines	O
for	O
manipulating	O
matrices	O
submatrices	O
and	O
linked	O
lists	O
of	O
submatrices	O
it	O
includes	O
a	O
set	O
of	O
macros	O
written	O
for	O
the	O
unix	O
utility	O
which	O
allows	O
complicated	O
array-handling	O
routines	O
to	O
be	O
written	O
using	O
relatively	O
simple	O
source	O
code	O
which	O
in	O
turn	O
is	O
translated	O
into	O
c	O
source	O
by	O
all	O
memory	B
management	O
is	O
handled	O
dynamically	O
there	O
are	O
several	O
neural	O
network	O
modules	O
written	O
as	O
applications	O
to	O
the	O
minimisation	O
module	O
these	O
include	O
a	O
special	O
purpose	O
mlp	B
a	O
fully-connected	O
recurrent	O
mlp	B
a	O
fully-connected	O
recurrent	O
mlp	B
with	O
an	O
unusual	O
training	O
algorithm	O
almeida	O
and	O
general	O
mlp	B
with	O
architecture	O
specified	O
at	O
runtime	O
there	O
is	O
also	O
an	O
rbf	B
network	O
which	O
shares	O
the	O
io	O
routines	O
but	O
does	O
not	O
use	O
the	O
minimiser	O
there	O
is	O
a	O
general	O
feeling	O
especially	O
among	O
statisticians	O
that	O
the	O
multilayer	O
perceptron	B
is	O
just	O
a	O
highly-parameterised	O
form	O
of	O
non-linear	O
regression	O
this	O
is	O
not	O
our	O
experience	O
in	O
practice	O
the	O
backprop	B
procedure	O
lies	O
somewhere	O
between	O
a	O
regression	O
technique	O
and	O
a	O
decision	O
tree	O
sometimes	O
being	O
closer	O
to	O
one	O
and	O
sometimes	O
closer	O
to	O
the	O
other	O
as	O
a	O
result	O
we	O
cannot	O
make	O
general	O
statements	O
about	O
the	O
nature	O
of	O
the	O
decision	O
surfaces	O
but	O
it	O
would	O
seem	O
that	O
they	O
are	O
not	O
in	O
any	O
sense	O
local	O
there	O
would	O
be	O
a	O
greater	O
similarity	O
with	O
k-nn	B
generally	O
the	O
absence	O
of	O
diagnostic	O
information	O
and	O
the	O
inability	O
to	O
interpret	O
the	O
output	B
is	O
a	O
great	O
disadvantage	O
kohonen	B
and	O
lvq	B
kohonen	B
s	O
net	O
is	O
an	O
implementation	O
of	O
the	O
self-organising	O
feature	O
mapping	O
algorithm	O
based	O
on	O
the	O
work	O
of	O
kohonen	B
kohonen	B
nets	O
have	O
an	O
inherent	O
parallel	O
feature	O
in	O
the	O
evaluation	O
of	O
links	O
between	O
neurons	B
so	O
this	O
program	O
is	O
implemented	O
by	O
luebeck	O
university	O
of	O
germany	O
on	O
a	O
transputer	O
with	O
an	O
ibm	O
pc	O
as	O
the	O
front-end	O
for	O
user	O
interaction	O
this	O
special	O
hardware	O
requirement	O
thus	O
differs	O
from	O
the	O
norm	O
and	O
makes	O
comparison	O
of	O
memory	B
and	O
cpu	O
time	B
rather	O
difficult	O
kohonen	B
nets	O
are	O
more	O
general	O
than	O
a	O
number	O
of	O
other	O
neural	O
net	O
algorithms	O
such	O
as	O
backpropagation	O
in	O
a	O
sense	O
it	O
is	O
a	O
modelling	O
tool	O
that	O
can	O
be	O
used	O
to	O
model	O
the	O
behaviour	O
of	O
a	O
system	O
with	O
its	O
input	B
and	O
output	B
variables	O
all	O
modelled	O
as	O
linked	O
neuronal	O
in	O
this	O
respect	O
it	O
is	O
very	O
similar	O
to	O
the	O
statistical	B
algorithm	O
castle	B
both	O
can	O
be	O
used	O
in	O
wider	O
areas	O
of	O
applications	O
including	O
classification	B
and	O
regression	O
in	O
this	O
book	O
however	O
we	O
are	O
primarily	O
concerned	O
with	O
classification	B
the	O
network	O
can	O
accept	O
attribute-value	O
data	O
with	O
numeric	O
values	O
only	O
this	O
makes	O
it	O
necessary	O
to	O
convert	O
logical	O
or	O
categorical	O
attributes	B
into	O
numeric	O
data	O
in	O
use	O
there	O
are	O
very	O
few	O
indications	O
as	O
to	O
how	O
many	O
nodes	O
the	O
system	O
should	O
have	O
and	O
how	O
many	O
times	O
the	O
examples	O
should	O
be	O
repeatedly	O
fed	O
to	O
the	O
system	O
for	O
training	O
all	O
such	O
parameters	O
can	O
only	O
be	O
decided	O
on	O
a	O
trial-and-error	O
basis	O
kohonen	B
does	O
not	O
accept	O
unknown	O
values	O
so	O
data	O
sets	O
must	O
have	O
their	O
missing	O
attribute-values	O
replaced	O
by	O
estimated	O
values	O
through	O
some	O
statistical	B
methods	O
similar	O
to	O
all	O
neural	B
networks	I
the	O
output	B
of	O
the	O
kohonen	B
net	O
normally	O
gives	O
very	O
little	O
insight	O
to	O
users	O
as	O
to	O
why	O
the	O
conclusions	O
have	O
been	O
derived	O
from	O
the	O
given	O
input	B
data	O
the	O
weights	O
on	O
the	O
links	O
of	O
the	O
nodes	O
in	O
the	O
net	O
are	O
not	O
generally	O
easy	O
to	O
explain	O
from	O
a	O
viewpoint	O
of	O
human	O
understanding	O
lvq	B
is	O
also	O
based	O
on	O
a	O
kohonen	B
net	O
and	O
the	O
essential	O
difference	O
between	O
these	O
two	O
programs	O
is	O
that	O
lvq	B
uses	O
supervised	O
training	O
so	O
it	O
should	O
be	O
no	O
surprise	O
that	O
in	O
all	O
the	O
trials	O
the	O
exception	O
of	O
the	O
dna	B
dataset	I
the	O
results	O
of	O
lvq	B
are	O
better	O
than	O
those	O
of	O
sec	O
memory	B
and	O
time	B
kohonen	B
so	O
the	O
use	O
of	O
kohonen	B
should	O
be	O
limited	O
to	O
clustering	B
or	O
unsupervised	B
learning	I
and	O
lvq	B
should	O
always	O
be	O
preferred	O
for	O
standard	O
classification	B
tasks	O
unfortunately	O
lvq	B
has	O
at	O
least	O
one	O
bug	O
that	O
may	O
give	O
seriously	O
misleading	O
results	O
so	O
the	O
output	B
should	O
be	O
checked	O
carefully	O
reported	O
error	O
rates	O
of	O
zero	O
radial	B
basis	I
function	I
neural	O
network	O
the	O
radial	B
basis	I
function	I
neural	O
network	O
for	O
short	O
is	O
similar	O
to	O
other	O
neural	O
net	O
algorithms	O
but	O
it	O
uses	O
a	O
different	O
error	O
estimation	O
and	O
gradient	B
descent	I
function	O
i	O
e	O
the	O
radial	B
basis	I
function	I
similar	O
to	O
other	O
neural	O
net	O
algorithms	O
the	O
results	O
produced	O
by	O
rbf	B
are	O
very	O
difficult	O
to	O
understand	O
rbf	B
uses	O
a	O
cross-validation	O
technique	O
to	O
handle	O
the	O
noise	B
as	O
the	O
algorithm	O
trains	O
it	O
continually	O
tests	O
on	O
a	O
small	O
set	O
called	O
the	O
cross-validation	O
set	O
when	O
the	O
error	O
on	O
this	O
set	O
starts	O
to	O
increase	O
it	O
stops	O
training	O
thus	O
it	O
can	O
automatically	O
decide	O
when	O
to	O
stop	O
training	O
which	O
is	O
a	O
major	O
advantage	O
of	O
this	O
algorithm	O
compared	O
to	O
other	O
neural	O
net	O
algorithms	O
however	O
it	O
cannot	O
cope	O
with	O
unknown	O
values	O
the	O
algorithm	O
is	O
fairly	O
well	O
implemented	O
so	O
it	O
is	O
relatively	O
easy	O
to	O
use	O
compared	O
to	O
many	O
neural	O
network	O
algorithms	O
because	O
it	O
only	O
has	O
one	O
parameter	O
to	O
adjust	O
for	O
each	O
new	O
application	O
the	O
number	O
of	O
centres	O
of	O
the	O
radial	B
basis	I
function	I
it	O
is	O
fairly	O
easy	O
to	O
use	O
this	O
algorithm	O
has	O
been	O
included	O
as	O
a	O
neural	O
network	O
and	O
is	O
perhaps	O
closest	O
to	O
madaline	B
but	O
in	O
fact	O
it	O
is	O
rather	O
a	O
hybrid	O
and	O
could	O
also	O
have	O
been	O
classified	O
as	O
a	O
nonparametric	O
statistical	B
algorithm	O
it	O
uses	O
methods	O
related	O
to	O
logistic	O
regression	O
in	O
the	O
first	O
stage	O
except	O
that	O
it	O
sets	O
up	O
a	O
discriminating	O
hyperplane	O
between	O
all	O
pairs	O
of	O
classes	B
and	O
then	O
minimises	O
an	O
error	O
function	O
by	O
gradient	B
descent	I
in	O
addition	O
an	O
optional	O
clustering	B
procedure	O
allows	O
a	O
class	B
to	O
be	O
treated	O
as	O
several	O
subclasses	O
this	O
is	O
a	O
new	O
algorithm	O
and	O
the	O
results	O
are	O
very	O
encouraging	O
although	O
it	O
never	O
quite	O
comes	O
first	O
in	O
any	O
one	O
trial	O
it	O
is	O
very	O
often	O
second	O
best	O
and	O
its	O
overall	O
performance	O
is	O
excellent	O
it	O
would	O
be	O
useful	O
to	O
quantify	O
how	O
much	O
the	O
success	O
of	O
is	O
due	O
to	O
the	O
multi-way	O
hyperplane	O
treatment	O
and	O
how	O
much	O
is	O
due	O
to	O
the	O
initial	O
clustering	B
and	O
it	O
would	O
also	O
be	O
useful	O
to	O
automate	O
the	O
selection	O
of	O
clusters	O
present	O
the	O
number	O
of	O
subclasses	O
is	O
a	O
user-defined	O
parameter	O
it	O
is	O
easy	O
to	O
use	O
and	O
is	O
intermediate	O
between	O
linear	O
discriminants	O
and	O
multilayer	O
perceptron	B
in	O
ease	O
of	O
interpretation	O
it	O
strengthens	O
the	O
case	O
for	O
other	O
hybrid	O
algorithms	O
to	O
be	O
explored	O
memory	B
and	O
time	B
so	O
far	O
we	O
have	O
said	O
very	O
little	O
about	O
either	O
memory	B
requirements	O
or	O
cpu	O
time	B
to	O
train	O
and	O
test	O
on	O
each	O
dataset	O
on	O
reason	O
for	O
this	O
is	O
that	O
these	O
can	O
vary	O
considerably	O
from	O
one	O
implementation	O
to	O
another	O
we	O
can	O
however	O
make	O
a	O
few	O
comments	O
memory	B
in	O
most	O
of	O
these	O
large	O
datasets	O
memory	B
was	O
not	O
a	O
problem	O
the	O
exception	O
to	O
this	O
was	O
the	O
full	O
version	O
of	O
the	O
hand-written	O
digit	O
dataset	O
see	O
section	O
this	O
dataset	O
had	O
variables	O
and	O
examples	O
and	O
most	O
algorithms	O
on	O
an	O
mb	O
machine	O
could	O
conclusions	O
not	O
handle	O
it	O
however	O
such	O
problems	O
are	O
likely	O
to	O
be	O
rare	O
in	O
most	O
applications	O
a	O
problem	O
with	O
the	O
interpretation	O
of	O
these	O
figures	O
is	O
that	O
they	O
were	O
obtained	O
from	O
the	O
unix	O
command	O
set	O
time	B
and	O
for	O
a	O
simple	O
fortran	O
program	O
for	O
example	B
the	O
output	B
is	O
directly	O
related	O
to	O
the	O
dimension	O
declarations	O
so	O
an	O
edited	O
version	O
could	O
be	O
cut	B
to	O
fit	O
the	O
given	O
dataset	O
and	O
produce	O
a	O
smaller	O
memory	B
requirement	O
a	O
more	O
sensible	O
way	O
to	O
quantify	O
memory	B
would	O
be	O
in	O
terms	O
of	O
the	O
size	O
of	O
the	O
data	O
for	O
example	B
in	O
the	O
sas	O
manual	O
it	O
states	O
that	O
the	O
memory	B
required	O
for	O
nearest	B
neighbour	I
is	O
for	O
most	O
situations	O
of	O
order	O
if	O
similar	O
results	O
could	O
be	O
stated	O
for	O
all	O
our	O
algorithms	O
this	O
would	O
make	O
comparisons	O
much	O
more	O
transparent	O
and	O
also	O
enable	O
predictions	O
for	O
new	O
datasets	O
as	O
far	O
as	O
our	O
results	O
are	O
concerned	O
it	O
is	O
clear	O
that	O
the	O
main	O
difference	O
in	O
memory	B
requirements	O
will	O
depend	O
on	O
whether	O
the	O
algorithm	O
has	O
to	O
store	O
all	O
the	O
data	O
or	O
can	O
process	O
it	O
in	O
pieces	O
the	O
theory	O
should	O
determine	O
this	O
as	O
well	O
as	O
the	O
numbers	O
but	O
it	O
is	O
clear	O
that	O
linear	O
and	O
quadratic	B
discriminant	I
classifiers	O
are	O
the	O
most	O
efficient	O
here	O
p	O
c	O
i	O
e	O
time	B
again	O
the	O
results	O
here	O
are	O
rather	O
confusing	O
the	O
times	O
do	O
not	O
always	O
measure	B
the	O
same	O
thing	O
for	O
example	B
if	O
there	O
are	O
parameters	O
to	O
select	O
there	O
are	O
two	O
options	O
user	O
may	O
decide	O
may	O
decide	O
to	O
choose	O
the	O
parameters	O
by	O
cross-validation	O
and	O
reduce	O
the	O
error	B
rate	I
at	O
the	O
to	O
just	O
plug	O
in	O
the	O
parameters	O
and	O
suffer	O
a	O
slight	O
loss	O
in	O
accuracy	B
of	O
the	O
classifier	B
user	O
expense	O
of	O
a	O
vastly	O
inflated	O
training	O
time	B
it	O
is	O
clear	O
then	O
that	O
more	O
explanation	O
is	O
required	O
and	O
a	O
more	O
thorough	O
investigation	O
to	O
determine	O
selection	O
of	O
parameters	O
and	O
the	O
trade-off	O
between	O
time	B
and	O
error	B
rate	I
in	O
individual	O
circumstances	O
there	O
are	O
other	O
anomalies	O
for	O
example	B
smart	B
often	O
quotes	O
the	O
smallest	O
time	B
to	I
test	I
and	O
the	O
amount	O
of	O
computation	O
required	O
is	O
a	O
superset	O
of	O
that	O
required	O
for	O
discrim	B
which	O
usually	O
takes	O
longer	O
so	O
it	O
appears	O
that	O
the	O
interpretation	O
of	O
results	O
will	O
again	O
be	O
influenced	O
by	O
the	O
implementation	O
it	O
is	O
of	O
interest	O
that	O
smart	B
has	O
the	O
largest	O
ratio	O
of	O
training	O
to	O
testing	O
time	B
in	O
nearly	O
all	O
of	O
the	O
datasets	O
as	O
with	O
memory	B
requirements	O
a	O
statement	O
that	O
time	B
is	O
proportional	O
to	O
some	O
function	O
of	O
the	O
data	O
size	O
would	O
be	O
preferred	O
for	O
example	B
the	O
sas	O
manual	O
quotes	O
the	O
time	B
is	O
the	O
number	O
of	O
observations	O
in	O
the	O
training	O
data	O
the	O
above	O
warnings	O
should	O
make	O
us	O
cautious	O
in	O
drawing	O
conclusions	O
in	O
that	O
some	O
algorithms	O
may	O
not	O
require	O
parameter	O
selection	O
however	O
if	O
we	O
sum	O
the	O
training	O
and	O
testing	O
times	O
we	O
can	O
say	O
generally	O
that	O
for	O
the	O
nearest	B
neighbour	I
classifier	B
to	O
test	O
as	O
proportional	O
to	O
where	O
indcart	B
takes	O
longer	O
than	O
cart	B
among	O
the	O
statistical	B
algorithms	O
the	O
nonparametric	O
ones	O
take	O
longer	O
especially	O
k-nn	B
smart	B
and	O
among	O
the	O
decision	O
tree	O
algorithms	O
and	O
itrule	B
take	O
longer	O
among	O
the	O
neural	O
net	O
algorithms	O
is	O
probably	O
the	O
quickest	O
general	O
issues	O
cost	B
matrices	I
if	O
a	O
cost	B
matrix	I
is	O
involved	O
be	O
warned	O
that	O
only	O
cart	B
the	O
statistical	B
procedures	O
and	O
some	O
of	O
the	O
neural	O
nets	O
take	O
costs	B
into	O
account	O
at	O
all	O
even	O
then	O
with	O
the	O
exception	O
of	O
j	O
sec	O
general	O
issues	O
and	O
smart	B
they	O
do	O
not	O
use	O
costs	B
as	O
part	O
of	O
the	O
learning	O
process	O
of	O
those	O
algorithms	O
which	O
do	O
not	O
incorporate	O
costs	B
many	O
output	B
a	O
measure	B
which	O
can	O
be	O
interpreted	O
as	O
a	O
probability	O
and	O
costs	B
could	O
therefore	O
be	O
incorporated	O
this	O
book	O
has	O
only	O
considered	O
three	O
datasets	O
which	O
include	O
costs	B
partly	O
for	O
the	O
very	O
reason	O
that	O
some	O
of	O
the	O
decision	O
tree	O
programs	O
cannot	O
cope	O
with	O
them	O
there	O
is	O
a	O
clear	O
need	O
to	O
have	O
the	O
option	O
of	O
incorporating	O
a	O
cost	B
matrix	I
into	O
all	O
classification	B
algorithms	O
and	O
in	O
principle	O
this	O
should	O
be	O
a	O
simple	O
matter	O
interpretation	O
of	O
error	O
rates	O
the	O
previous	O
chapter	O
has	O
already	O
analysed	O
the	O
results	O
from	O
the	O
trials	O
and	O
some	O
sort	O
of	O
a	O
pattern	O
is	O
emerging	O
it	O
is	O
hoped	O
that	O
one	O
day	O
we	O
can	O
find	O
a	O
set	O
of	O
measures	B
which	O
can	O
be	O
obtained	O
from	O
the	O
data	O
and	O
then	O
using	O
these	O
measures	B
alone	O
we	O
can	O
predict	O
with	O
some	O
degree	O
of	O
confidence	O
which	O
algorithms	O
or	O
methods	O
will	O
perform	O
the	O
best	O
there	O
is	O
some	O
theory	O
here	O
for	O
example	B
the	O
similarity	O
of	O
within-class	O
covariance	B
matrices	O
will	O
determine	O
the	O
relative	O
performance	O
of	O
linear	O
and	O
quadratic	B
discriminant	I
functions	O
and	O
also	O
the	O
performance	O
of	O
these	O
relative	O
to	O
decision	O
tree	O
methods	O
conditional	O
dependencies	O
will	O
favour	O
trees	O
however	O
from	O
an	O
empirical	O
perspective	O
there	O
is	O
still	O
some	O
way	O
to	O
go	O
both	O
from	O
the	O
point	O
of	O
view	O
of	O
determining	O
which	O
measures	B
are	O
important	O
and	O
how	O
best	O
to	O
make	O
the	O
prediction	O
the	O
attempts	O
of	O
the	O
previous	O
chapter	O
show	O
how	O
this	O
may	O
be	O
done	O
although	O
more	O
datasets	O
are	O
required	O
before	O
confidence	O
can	O
be	O
attached	O
to	O
the	O
conclusions	O
the	O
request	O
for	O
more	O
datasets	O
raises	O
another	O
issue	O
what	O
kind	O
of	O
datasets	O
it	O
is	O
clear	O
that	O
we	O
could	O
obtain	O
very	O
biased	O
results	O
if	O
we	O
limit	O
our	O
view	O
to	O
certain	O
types	O
and	O
the	O
question	O
of	O
what	O
is	O
representative	O
is	O
certainly	O
unanswered	O
section	O
outlines	O
a	O
number	O
of	O
different	O
dataset	O
types	O
and	O
is	O
likely	O
that	O
this	O
consideration	O
will	O
play	O
the	O
most	O
important	O
r	O
ole	O
in	O
determining	O
the	O
choice	O
of	O
algorithm	O
the	O
comparison	O
of	O
algorithms	O
here	O
is	O
almost	O
entirely	O
of	O
a	O
black-box	O
nature	O
so	O
the	O
recommendations	O
as	O
they	O
stand	O
are	O
really	O
only	O
applicable	O
to	O
the	O
na	O
ve	O
user	O
in	O
the	O
hands	O
of	O
an	O
expert	O
the	O
performance	O
of	O
an	O
algorithm	O
can	O
be	O
radically	O
different	O
and	O
of	O
course	O
there	O
is	O
always	O
the	O
possibility	O
of	O
transforming	O
or	O
otherwise	O
preprocessing	B
the	O
data	O
these	O
considerations	O
will	O
often	O
outweigh	O
any	O
choice	O
of	O
algorithm	O
structuring	O
the	O
results	O
much	O
of	O
the	O
analysis	O
in	O
the	O
previous	O
chapter	O
depends	O
on	O
the	O
scaling	O
of	O
the	O
results	O
it	O
is	O
clear	O
that	O
to	O
combine	O
results	O
across	O
many	O
datasets	O
care	O
will	O
need	O
to	O
be	O
taken	O
that	O
they	O
are	O
treated	O
equally	O
in	O
sections	O
and	O
the	O
scaling	O
was	O
taken	O
so	O
that	O
the	O
error	O
rates	O
costs	B
for	O
each	O
dataset	O
were	O
mapped	O
to	O
the	O
interval	O
x	O
whereas	O
in	O
section	O
the	O
scaling	O
was	O
done	O
using	O
an	O
estimated	O
standard	O
error	O
for	O
the	O
error	O
rates	O
costs	B
the	O
different	O
approaches	O
makes	O
the	O
interpretation	O
of	O
the	O
comparison	O
in	O
section	O
rather	O
difficult	O
the	O
pattern	O
which	O
emerges	O
from	O
the	O
multidimensional	B
scaling	I
and	O
associated	O
hierarchical	B
clustering	B
of	O
the	O
algorithms	O
is	O
very	O
encouraging	O
it	O
is	O
clear	O
that	O
there	O
is	O
a	O
strong	O
similarity	O
between	O
the	O
construction	O
and	O
the	O
performance	O
of	O
the	O
algorithms	O
the	O
hierarchical	B
clustering	B
of	O
the	O
datasets	O
is	O
not	O
so	O
convincing	O
however	O
the	O
overall	O
picture	O
in	O
figure	O
confirms	O
the	O
breakdown	O
of	O
analysis	O
by	O
subject	O
area	O
section	O
in	O
that	O
convex	O
hulls	O
which	O
do	O
not	O
overlap	O
can	O
be	O
drawn	O
around	O
the	O
datasets	O
of	O
the	O
specific	O
subject	O
areas	O
conclusions	O
an	O
outlier	O
here	O
is	O
the	O
tsetse	O
fly	O
data	O
which	O
could	O
also	O
easily	O
been	O
placed	O
in	O
the	O
category	O
of	O
image	O
datasetssegmentation	O
since	O
the	O
data	O
are	O
of	O
a	O
spatial	O
nature	O
although	O
it	O
is	O
not	O
a	O
standard	O
image	O
the	O
analysis	O
of	O
section	O
is	O
a	O
promising	O
one	O
though	O
there	O
is	O
not	O
enough	O
data	O
to	O
make	O
strong	O
conclusions	O
or	O
to	O
take	O
the	O
rules	O
too	O
seriously	O
however	O
it	O
might	O
be	O
better	O
to	O
predict	O
performance	O
on	O
a	O
continuous	O
scale	O
rather	O
than	O
the	O
current	O
approach	O
which	O
discretises	O
the	O
algorithms	O
into	O
applicable	O
and	O
non-applicable	O
indeed	O
the	O
choice	O
of	O
section	O
is	O
very	O
much	O
larger	O
than	O
the	O
more	O
commonly	O
used	O
or	O
standard	O
errors	O
in	O
hypothesis	O
testing	O
that	O
the	O
error	B
rate	I
for	O
could	O
be	O
predicted	O
by	O
taking	O
the	O
attempts	O
to	O
predict	O
performance	O
using	O
the	O
performance	O
of	O
benchmark	O
algorithms	O
section	O
is	O
highly	O
dependent	O
on	O
the	O
choice	O
of	O
datasets	O
used	O
also	O
it	O
needs	O
to	O
be	O
remembered	O
that	O
the	O
coefficients	O
reported	O
in	O
table	O
are	O
not	O
absolute	O
they	O
are	O
again	O
based	O
on	O
a	O
transformation	B
of	O
all	O
the	O
results	O
to	O
the	O
unit	O
interval	O
so	O
for	O
example	B
the	O
result	O
the	O
error	B
rate	I
for	O
k-nn	B
takes	O
into	O
account	O
the	O
error	O
rates	O
for	O
all	O
of	O
the	O
other	O
algorithms	O
if	O
we	O
only	O
consider	O
this	O
pair	O
and	O
then	O
we	O
get	O
a	O
coefficient	O
of	O
but	O
this	O
is	O
still	O
influenced	O
by	O
one	O
or	O
two	O
observations	O
an	O
alternative	O
is	O
to	O
consider	O
the	O
average	O
percentage	O
improvement	O
of	O
which	O
is	O
but	O
none	O
of	O
these	O
possibilities	O
takes	O
account	O
of	O
the	O
different	O
sample	O
sizes	O
or	O
removal	O
of	O
irrelevant	B
attributes	B
there	O
are	O
many	O
examples	O
where	O
the	O
performance	O
of	O
algorithms	O
may	O
be	O
improved	O
by	O
removing	O
irrelevant	B
attributes	B
a	O
specific	O
example	B
is	O
the	O
dna	B
dataset	I
where	O
the	O
middle	O
of	O
the	O
nominal	O
attributes	B
are	O
by	O
far	O
the	O
most	O
relevant	O
if	O
a	O
decision	O
tree	O
for	O
example	B
is	O
presented	O
with	O
this	O
middle	O
section	O
of	O
the	O
data	O
it	O
performs	O
much	O
better	O
the	O
same	O
is	O
true	O
of	O
quadratic	B
discriminants	I
and	O
this	O
is	O
a	O
very	O
general	O
problem	O
with	O
black-box	O
procedures	O
there	O
are	O
ways	O
of	O
removing	O
variables	O
in	O
linear	O
discriminants	O
for	O
example	B
but	O
these	O
did	O
not	O
have	O
much	O
effect	O
on	O
accuracy	B
and	O
this	O
variable	O
selection	O
method	O
does	O
not	O
extend	O
to	O
other	O
algorithms	O
diagnostics	O
and	O
plotting	O
very	O
few	O
procedures	O
contain	O
internal	O
consistency	O
checks	O
even	O
where	O
they	O
are	O
available	O
in	O
principle	O
they	O
have	O
not	O
been	O
programmed	O
into	O
our	O
implementations	O
for	O
example	B
quadratic	O
discrimination	B
relies	O
on	O
multivariatenormality	O
and	O
there	O
are	O
tests	O
for	O
this	O
but	O
they	O
are	O
programmed	O
separately	O
similarly	O
castle	B
should	O
be	O
able	O
to	O
check	O
if	O
the	O
assumption	O
of	O
polytree	O
structure	O
is	O
a	O
reasonable	O
one	O
but	O
this	O
is	O
not	O
programmed	O
in	O
the	O
user	O
must	O
then	O
rely	O
on	O
other	O
ways	O
of	O
doing	O
such	O
checks	O
an	O
important	O
but	O
very	O
much	O
underused	O
method	O
is	O
simply	O
to	O
plot	O
selected	O
portions	O
of	O
the	O
data	O
for	O
example	B
pairs	O
of	O
coordinates	O
with	O
the	O
classes	B
as	O
labels	O
this	O
often	O
gives	O
very	O
important	O
insights	O
into	O
the	O
data	O
the	O
manova	B
procedure	O
multidimensional	B
scaling	I
principal	O
components	O
and	O
projection	B
pursuit	I
all	O
give	O
useful	O
ways	O
in	O
which	O
multidimensional	O
data	O
can	O
be	O
plotted	O
in	O
two	O
dimensions	O
exploratory	O
data	O
if	O
the	O
object	O
of	O
the	O
exercise	O
is	O
to	O
explore	O
the	O
process	O
underlying	O
the	O
classifications	O
themselves	O
for	O
example	B
by	O
finding	O
out	O
which	O
variables	O
are	O
important	O
or	O
by	O
gaining	O
an	O
insight	O
sec	O
general	O
issues	O
into	O
the	O
structure	O
of	O
the	O
classification	B
process	O
then	O
neural	O
nets	O
k-nearest	B
neighbour	I
and	O
are	O
unlikely	O
to	O
be	O
much	O
use	O
no	O
matter	O
what	O
procedure	O
is	O
actually	O
used	O
it	O
is	O
often	O
best	O
to	O
prune	O
radically	O
by	O
keeping	O
only	O
two	O
or	O
three	O
significant	O
terms	O
in	O
a	O
regression	O
or	O
by	O
using	O
trees	O
of	O
depth	O
two	O
or	O
using	O
only	O
a	O
small	O
number	O
of	O
rules	O
in	O
the	O
hope	O
that	O
the	O
important	O
structure	O
is	O
retained	O
less	O
important	O
structures	O
can	O
be	O
added	O
on	O
later	O
as	O
greater	O
accuracy	B
is	O
required	O
it	O
should	O
also	O
be	O
borne	O
in	O
mind	O
that	O
in	O
exploratory	O
work	O
it	O
is	O
common	O
to	O
include	O
anything	O
at	O
all	O
that	O
might	O
conceivably	O
be	O
relevant	O
and	O
that	O
often	O
the	O
first	O
task	O
is	O
to	O
weed	O
out	O
the	O
irrelevant	O
information	O
before	O
the	O
task	O
of	O
exploring	O
structure	O
can	O
begin	O
special	O
features	B
if	O
a	O
particular	O
application	O
has	O
some	O
special	O
features	B
such	O
as	O
missing	B
values	I
hierarchical	B
structure	I
in	O
the	O
attributes	B
ordered	O
classes	B
presence	O
of	O
known	O
subgroups	O
within	O
classes	B
of	O
classes	B
etc	O
etc	O
this	O
extra	O
structure	O
can	O
be	O
used	O
in	O
the	O
classification	B
process	O
to	O
improve	O
performance	O
and	O
to	O
improve	O
understanding	O
also	O
it	O
is	O
crucial	O
to	O
understand	O
if	O
the	O
class	B
values	O
are	O
in	O
any	O
sense	O
random	O
variables	O
or	O
outcomes	O
of	O
a	O
chance	O
experiment	O
as	O
this	O
alters	O
radically	O
the	O
approach	O
that	O
should	O
be	O
adopted	O
the	O
procrustean	O
approach	O
of	O
forcing	O
all	O
datasets	O
into	O
a	O
common	O
format	O
as	O
we	O
have	O
done	O
in	O
the	O
trials	O
of	O
this	O
book	O
for	O
comparative	O
purposes	O
is	O
not	O
recommended	O
in	O
general	O
the	O
general	O
rule	O
is	O
to	O
use	O
all	O
the	O
available	O
external	O
information	O
and	O
not	O
to	O
throw	O
it	O
away	O
from	O
classification	B
to	O
knowledge	O
organisation	O
and	O
synthesis	O
in	O
chapter	O
it	O
was	O
stressed	O
that	O
machine	O
learning	O
classifiers	O
should	O
possess	O
a	O
mental	B
fit	I
to	O
the	O
data	O
so	O
that	O
the	O
learned	O
concepts	O
are	O
meaningful	O
to	O
and	O
evaluable	O
by	O
humans	O
on	O
this	O
criterion	O
the	O
neural	O
net	O
algorithms	O
are	O
relatively	O
opaque	O
whereas	O
most	O
of	O
the	O
statistical	B
methods	O
which	O
do	O
not	O
have	O
mental	B
fit	I
can	O
at	O
least	O
determine	O
which	O
of	O
the	O
attributes	B
are	O
important	O
however	O
the	O
specific	O
black-box	O
use	O
of	O
methods	O
would	O
never	O
take	O
place	O
and	O
it	O
is	O
worth	O
looking	O
forwards	O
more	O
speculatively	O
to	O
ai	O
uses	O
of	O
classification	B
methods	O
for	O
example	B
kardio	B
s	O
comprehensive	O
treatise	O
on	O
ecg	B
interpretation	O
et	O
al	O
does	O
not	O
contain	O
a	O
single	O
rule	O
of	O
human	O
authorship	O
seen	O
in	O
this	O
light	O
it	O
becomes	O
clear	O
that	O
classification	B
and	O
discrimination	B
are	O
not	O
narrow	O
fields	O
within	O
statistics	O
or	O
machine	O
learning	O
but	O
that	O
the	O
art	O
of	O
classification	B
can	O
generate	O
substantial	O
contributions	O
to	O
organise	O
improve	O
human	O
knowledge	O
even	O
as	O
in	O
kardio	B
to	O
manufacture	O
new	O
knowledge	O
another	O
context	O
in	O
which	O
knowledge	O
derived	O
from	O
humans	O
and	O
data	O
is	O
synthesised	O
is	O
in	O
the	O
area	O
of	O
bayesian	O
expert	B
systems	I
et	O
al	O
in	O
which	O
subjective	O
judgments	O
of	O
model	O
structure	O
and	O
conditional	O
probabilities	O
are	O
formally	O
combined	O
with	O
likelihoods	O
derived	O
from	O
data	O
by	O
bayes	B
theorem	I
this	O
provides	O
a	O
way	O
for	O
a	O
system	O
to	O
smoothly	O
adapt	O
a	O
model	O
from	O
being	O
initially	O
expert-based	O
towards	O
one	O
derived	O
from	O
data	O
however	O
this	O
representation	O
of	O
knowledge	O
by	O
causal	O
nets	O
is	O
necessarily	O
rather	O
restricted	O
because	O
it	O
does	O
demand	O
an	O
exhaustive	O
specification	O
of	O
the	O
full	O
joint	O
distribution	O
however	O
such	O
systems	O
form	O
a	O
complete	O
model	O
of	O
a	O
process	O
and	O
are	O
intended	O
for	O
more	O
than	O
simply	O
classification	B
indeed	O
they	O
provide	O
a	O
unified	O
structure	O
for	O
many	O
complex	B
stochastic	O
problems	O
with	O
connections	O
to	O
image	O
processing	O
dynamic	O
modelling	O
and	O
so	O
on	O
knowledge	O
representation	O
claude	O
sammut	O
university	O
of	O
new	O
south	O
wales	O
introduction	O
in	O
bruner	O
goodnow	O
and	O
austin	O
published	O
their	O
book	O
a	O
study	O
of	O
thinking	O
which	O
became	O
a	O
landmark	O
in	O
psychology	O
and	O
would	O
later	O
have	O
a	O
major	O
impact	O
on	O
machine	O
learning	O
the	O
experiments	O
reported	O
by	O
bruner	O
goodnow	O
and	O
austin	O
were	O
directed	O
towards	O
understanding	O
a	O
human	O
s	O
ability	O
to	O
categorise	O
and	O
how	O
categories	O
are	O
learned	O
we	O
begin	O
with	O
what	O
seems	O
a	O
paradox	O
the	O
world	O
of	O
experience	O
of	O
any	O
normal	O
man	O
is	O
composed	O
of	O
a	O
tremendous	O
array	O
of	O
discriminably	O
different	O
objects	O
events	O
people	O
impressions	O
but	O
were	O
we	O
to	O
utilise	O
fully	O
our	O
capacity	O
for	O
registering	O
the	O
differences	O
in	O
things	O
and	O
to	O
respond	O
to	O
each	O
event	O
encountered	O
as	O
unique	O
we	O
would	O
soon	O
be	O
overwhelmed	O
by	O
the	O
complexity	O
of	O
our	O
environment	O
the	O
resolution	O
of	O
this	O
seeming	O
paradox	O
is	O
achieved	O
by	O
man	O
s	O
capacity	O
to	O
categorise	O
to	O
categorise	O
is	O
to	O
render	O
discriminably	O
different	O
things	O
equivalent	O
to	O
group	O
objects	O
and	O
events	O
and	O
people	O
around	O
us	O
into	O
classes	B
the	O
process	O
of	O
categorizing	O
involves	O
an	O
act	O
of	O
invention	O
if	O
we	O
have	O
learned	O
the	O
class	B
house	O
as	O
a	O
concept	B
new	O
exemplars	O
can	O
be	O
readily	O
recognised	O
the	O
category	O
becomes	O
a	O
tool	O
for	O
further	O
use	O
the	O
learning	O
and	O
utilisation	O
of	O
categories	O
represents	O
one	O
of	O
the	O
most	O
elementary	O
and	O
general	O
forms	O
of	O
cognition	O
by	O
which	O
man	O
adjusts	O
to	O
his	O
environment	O
the	O
first	O
question	O
that	O
they	O
had	O
to	O
deal	O
with	O
was	O
that	O
of	O
representation	O
what	O
is	O
a	O
concept	B
they	O
assumed	O
that	O
objects	O
and	O
events	O
could	O
be	O
described	O
by	O
a	O
set	O
of	O
attributes	B
and	O
were	O
concerned	O
with	O
how	O
inferences	O
could	O
be	O
drawn	O
from	O
attributes	B
to	O
class	B
membership	O
categories	O
were	O
considered	O
to	O
be	O
of	O
three	O
types	O
conjunctive	O
disjunctive	O
and	O
relational	O
one	O
learns	O
to	O
categorise	O
a	O
subset	O
of	O
events	O
in	O
a	O
certain	O
way	O
one	O
is	O
doing	O
more	O
than	O
simply	O
learning	O
to	O
recognise	O
instances	O
encountered	O
one	O
is	O
also	O
learning	O
a	O
rule	O
that	O
may	O
be	O
applied	O
to	O
new	O
instances	O
the	O
concept	B
or	O
category	O
is	O
basically	O
this	O
rule	O
of	O
grouping	O
and	O
it	O
is	O
such	O
rules	O
that	O
one	O
constructs	O
in	O
forming	O
and	O
attaining	O
concepts	O
address	O
for	O
correspondence	O
school	O
of	O
computer	O
science	O
and	O
engineeringartificialintelligence	O
laboratory	O
university	O
of	O
new	O
south	O
wales	O
po	O
box	O
kensigton	O
nsw	O
australia	O
sec	O
learning	O
measurement	O
and	O
representation	O
the	O
notion	O
of	O
a	O
rule	O
as	O
an	O
abstract	O
representation	O
of	O
a	O
concept	B
in	O
the	O
human	O
mind	O
came	O
to	O
be	O
questioned	O
by	O
psychologists	O
and	O
there	O
is	O
still	O
no	O
good	O
theory	O
to	O
explain	O
how	O
we	O
store	O
concepts	O
however	O
the	O
same	O
questions	O
about	O
the	O
nature	O
of	O
representation	O
arise	O
in	O
machine	O
learning	O
for	O
the	O
choice	O
of	O
representation	O
heavily	O
determines	O
the	O
nature	O
of	O
a	O
learning	O
algorithm	O
thus	O
one	O
critical	O
point	O
of	O
comparison	O
among	O
machine	O
learning	O
algorithms	O
is	O
the	O
method	O
of	O
knowledge	O
representation	O
employed	O
in	O
this	O
chapter	O
we	O
will	O
discuss	O
various	O
methods	O
of	O
representation	O
and	O
compare	O
them	O
according	O
to	O
their	O
power	O
to	O
express	O
complex	B
concepts	O
and	O
the	O
effects	O
of	O
representation	O
on	O
the	O
time	B
and	O
space	O
costs	B
of	O
learning	O
and	O
some	O
input	B
learning	O
measurement	O
and	O
representation	O
a	O
learning	O
program	O
is	O
one	O
that	O
is	O
capable	O
of	O
improving	O
its	O
performance	O
through	O
experience	O
a	O
normal	O
program	O
would	O
yield	O
the	O
same	O
result	O
given	O
a	O
program	O
j	O
after	O
every	O
application	O
however	O
a	O
learning	O
program	O
can	O
alter	O
its	O
initial	O
state	O
p	O
so	O
that	O
its	O
performance	O
is	O
modified	O
with	O
each	O
application	O
thus	O
we	O
can	O
say	O
j	O
given	O
the	O
initial	O
state	O
the	O
that	O
is	O
goal	O
of	O
learning	O
is	O
to	O
construct	O
a	O
new	O
initial	O
so	O
that	O
the	O
program	O
alters	O
its	O
behaviour	O
to	O
give	O
a	O
more	O
accurate	O
or	O
quicker	O
result	O
thus	O
one	O
way	O
of	O
thinking	O
about	O
what	O
a	O
learning	O
program	O
does	O
is	O
that	O
it	O
builds	O
an	O
increasingly	O
accurate	O
approximation	O
to	O
a	O
mapping	O
from	O
input	B
to	O
output	B
is	O
the	O
result	O
of	O
applying	O
program	O
to	O
input	B
the	O
most	O
common	O
learning	O
task	O
is	O
that	O
of	O
acquiring	O
a	O
function	O
which	O
maps	O
objects	O
that	O
share	O
common	O
properties	O
to	O
the	O
same	O
class	B
value	O
this	O
is	O
the	O
categorisation	O
problem	O
to	O
which	O
bruner	O
goodnow	O
and	O
austin	O
referred	O
and	O
much	O
of	O
our	O
discussion	O
will	O
be	O
concerned	O
with	O
categorisation	O
learning	O
experience	O
may	O
be	O
in	O
the	O
form	O
of	O
examples	O
from	O
a	O
trainer	O
or	O
the	O
results	O
of	O
trial	O
and	O
error	O
in	O
either	O
case	O
the	O
program	O
must	O
be	O
able	O
to	O
represent	O
its	O
observations	O
of	O
the	O
world	O
and	O
it	O
must	O
also	O
be	O
able	O
to	O
represent	O
hypotheses	O
about	O
the	O
patterns	O
it	O
may	O
find	O
in	O
those	O
observations	O
thus	O
we	O
will	O
often	O
refer	O
to	O
the	O
observation	B
language	I
and	O
the	O
hypothesis	B
language	I
the	O
observation	B
language	I
describes	O
the	O
inputs	O
and	O
outputs	O
of	O
the	O
program	O
and	O
the	O
hypothesis	B
language	I
describes	O
the	O
internal	O
state	O
of	O
the	O
learning	O
program	O
which	O
corresponds	O
to	O
its	O
theory	O
of	O
the	O
concepts	O
or	O
patterns	O
that	O
exist	O
in	O
the	O
data	O
the	O
input	B
to	O
a	O
learning	O
program	O
consists	O
of	O
descriptions	O
of	O
objects	O
from	O
the	O
universe	O
and	O
in	O
the	O
case	O
of	O
supervised	B
learning	I
an	O
output	B
value	O
associated	O
with	O
the	O
example	B
the	O
universe	O
can	O
be	O
an	O
abstract	O
one	O
such	O
as	O
the	O
set	O
of	O
all	O
natural	O
numbers	O
or	O
the	O
universe	O
may	O
be	O
a	O
subset	O
of	O
the	O
real-world	O
no	O
matter	O
which	O
method	O
of	O
representation	O
we	O
choose	O
descriptions	O
of	O
objects	O
in	O
the	O
real	O
world	O
must	O
ultimately	O
rely	O
on	O
measurements	O
of	O
some	O
properties	O
of	O
those	O
objects	O
these	O
may	O
be	O
physical	O
properties	O
such	O
as	O
size	O
weight	O
colour	O
etc	O
or	O
they	O
may	O
be	O
defined	O
for	O
objects	O
for	O
example	B
the	O
length	O
of	O
time	B
a	O
person	O
has	O
been	O
employed	O
for	O
the	O
purpose	O
of	O
approving	O
a	O
loan	O
the	O
accuracy	B
and	O
reliability	O
of	O
a	O
learned	O
concept	B
depends	O
heavily	O
on	O
the	O
accuracy	B
and	O
reliability	O
of	O
the	O
measurements	O
a	O
program	O
is	O
limited	O
in	O
the	O
concepts	O
that	O
it	O
can	O
learn	O
by	O
the	O
representational	O
capabilities	O
of	O
both	O
observation	O
and	O
hypothesis	O
languages	O
for	O
example	B
if	O
an	O
attributevalue	O
list	O
is	O
used	O
to	O
represent	O
examples	O
for	O
an	O
induction	O
program	O
the	O
measurement	O
of	O
certain	O
attributes	B
and	O
not	O
others	O
clearly	O
places	O
bounds	O
on	O
the	O
kinds	O
of	O
patterns	O
that	O
the	O
learner	O
can	O
find	O
the	O
learner	O
is	O
said	O
to	O
be	O
biased	O
by	O
its	O
observation	B
language	I
the	O
hypothesis	B
language	I
also	O
places	O
knowledge	O
representation	O
constraints	O
on	O
what	O
may	O
and	O
may	O
not	O
be	O
learned	O
for	O
example	B
in	O
the	O
language	O
of	O
attributes	B
and	O
values	O
relationships	O
between	O
objects	O
are	O
difficult	O
to	O
represent	O
whereas	O
a	O
more	O
expressive	O
language	O
such	O
as	O
first	B
order	I
logic	I
can	O
easily	O
be	O
used	O
to	O
describe	O
relationships	O
unfortunately	O
representational	O
power	O
comes	O
at	O
a	O
price	O
learning	O
can	O
be	O
viewed	O
as	O
a	O
search	O
through	O
the	O
space	O
of	O
all	O
sentences	O
in	O
a	O
language	O
for	O
a	O
sentence	O
that	O
best	O
describes	O
the	O
data	O
the	O
richer	O
the	O
language	O
the	O
larger	O
the	O
search	O
space	O
when	O
the	O
search	O
space	O
is	O
small	O
it	O
is	O
possible	O
to	O
use	O
brute	O
force	O
search	O
methods	O
if	O
the	O
search	O
space	O
is	O
very	O
large	O
additional	O
knowledge	O
is	O
required	O
to	O
reduce	O
the	O
search	O
we	O
will	O
divide	O
our	O
attention	O
among	O
three	O
different	O
classes	B
of	O
machine	O
learning	O
algo	O
rithms	O
that	O
use	O
distinctly	O
different	O
approaches	O
to	O
the	O
problem	O
of	O
representation	O
instance-based	B
learning	O
algorithms	O
learn	O
concepts	O
by	O
storing	O
prototypic	O
instances	O
of	O
the	O
concept	B
and	O
do	O
not	O
construct	O
abstract	O
representations	O
at	O
all	O
function	B
approximation	I
algorithms	O
include	O
connectionist	O
and	O
statistics	O
methods	O
these	O
algorithms	O
are	O
most	O
closely	O
related	O
to	O
traditional	O
mathematical	O
notions	O
of	O
approximation	O
and	O
interpolation	O
and	O
represent	O
concepts	O
as	O
mathematical	O
formulae	O
symbolic	B
learning	I
algorithms	O
learn	O
concepts	O
by	O
constructing	O
a	O
symbolic	O
which	O
describes	O
a	O
class	B
of	O
objects	O
we	O
will	O
consider	O
algorithms	O
that	O
work	O
with	O
representations	O
equivalent	O
to	O
propositional	O
logic	O
and	O
first-order	O
logic	O
prototypes	B
the	O
simplest	O
form	O
of	O
learning	O
is	O
memorisation	O
when	O
an	O
object	O
is	O
observed	O
or	O
the	O
solution	O
to	O
a	O
problem	O
is	O
found	O
it	O
is	O
stored	O
in	O
memory	B
for	O
future	O
use	O
memory	B
can	O
be	O
thought	O
of	O
as	O
a	O
look	O
up	O
table	O
when	O
a	O
new	O
problem	O
is	O
encountered	O
memory	B
is	O
searched	O
to	O
find	O
if	O
the	O
same	O
problem	O
has	O
been	O
solved	O
before	O
if	O
an	O
exact	O
match	O
for	O
the	O
search	O
is	O
required	O
learning	O
is	O
slow	O
and	O
consumes	O
very	O
large	O
amounts	O
of	O
memory	B
however	O
approximate	O
matching	O
allows	O
a	O
degree	O
of	O
generalisation	O
that	O
both	O
speeds	O
learning	O
and	O
saves	O
memory	B
for	O
example	B
if	O
we	O
are	O
shown	O
an	O
object	O
and	O
we	O
want	O
to	O
know	O
if	O
it	O
is	O
a	O
chair	O
then	O
we	O
compare	O
the	O
description	O
of	O
this	O
new	O
object	O
with	O
descriptions	O
of	O
typical	O
chairs	O
that	O
we	O
have	O
encountered	O
before	O
if	O
the	O
description	O
of	O
the	O
new	O
object	O
is	O
close	O
to	O
the	O
description	O
of	O
one	O
of	O
the	O
stored	O
instances	O
then	O
we	O
may	O
call	O
it	O
a	O
chair	O
obviously	O
we	O
must	O
defined	O
what	O
we	O
mean	O
by	O
typical	O
and	O
close	O
to	O
better	O
understand	O
the	O
issues	O
involved	O
in	O
learning	O
prototypes	B
we	O
will	O
briefly	O
describe	O
three	O
experiments	O
in	O
instance-based	B
learning	O
by	O
aha	O
kibler	O
albert	O
ibl	O
learns	O
to	O
classify	O
objects	O
by	O
being	O
shown	O
examples	O
of	O
objects	O
described	O
by	O
an	O
attributevalue	O
list	O
along	O
with	O
the	O
class	B
to	O
which	O
each	O
example	B
belongs	O
experiment	O
in	O
the	O
first	O
experiment	O
to	O
learn	O
a	O
concept	B
simply	O
required	O
the	O
program	O
to	O
store	O
every	O
example	B
when	O
an	O
unclassified	O
object	O
was	O
presented	O
for	O
classification	B
by	O
the	O
program	O
it	O
used	O
a	O
simple	O
euclidean	O
distance	B
measure	B
to	O
determine	O
the	O
nearest	B
neighbour	I
of	O
the	O
object	O
and	O
the	O
class	B
given	O
to	O
it	O
was	O
the	O
class	B
of	O
the	O
neighbour	O
this	O
simple	O
scheme	O
works	O
well	O
and	O
is	O
tolerant	O
to	O
some	O
noise	B
in	O
the	O
data	O
its	O
major	O
disadvantage	O
is	O
that	O
it	O
requires	O
a	O
large	O
amount	O
of	O
storage	B
capacity	O
sec	O
experiment	O
prototypes	B
the	O
second	O
experiment	O
attempted	O
to	O
improve	O
the	O
space	O
performance	O
of	O
in	O
this	O
case	O
when	O
new	O
instances	O
of	O
classes	B
were	O
presented	O
to	O
the	O
program	O
the	O
program	O
attempted	O
to	O
classify	O
them	O
instances	O
that	O
were	O
correctly	O
classified	O
were	O
ignored	O
and	O
only	O
incorrectly	O
classified	O
instances	O
were	O
stored	O
to	O
become	O
part	O
of	O
the	O
concept	B
while	O
this	O
scheme	O
reduced	O
storage	B
dramatically	O
it	O
was	O
less	O
noise-tolerant	O
than	O
the	O
first	O
experiment	O
the	O
third	O
experiment	O
used	O
a	O
more	O
sophisticated	O
method	O
for	O
evaluating	O
instances	O
to	O
decide	O
if	O
they	O
should	O
be	O
kept	O
or	O
not	O
is	O
similar	O
to	O
with	O
the	O
following	O
additions	O
maintains	O
a	O
record	O
of	O
the	O
number	O
of	O
correct	O
and	O
incorrect	O
classification	B
attempts	O
for	O
each	O
saved	O
instance	O
this	O
record	O
summarised	O
an	O
instance	O
s	O
classification	B
performance	O
uses	O
a	O
significance	O
test	O
to	O
determine	O
which	O
instances	O
are	O
good	O
classifiers	O
and	O
which	O
ones	O
are	O
believed	O
to	O
be	O
noisy	B
the	O
latter	O
are	O
discarded	O
from	O
the	O
concept	B
description	O
this	O
method	O
strengthens	O
noise	B
tolerance	O
while	O
keeping	O
storage	B
requirements	O
down	O
discussion	O
fig	O
the	O
extension	O
of	O
an	O
ibl	O
concept	B
is	O
shown	O
in	O
solid	O
lines	O
the	O
dashed	O
lines	O
represent	O
the	O
target	O
concept	B
a	O
sample	O
of	O
positive	O
and	O
negative	O
examples	O
is	O
shown	O
adapted	O
from	O
aha	O
kibler	O
and	O
albert	O
knowledge	O
representation	O
is	O
strongly	O
related	O
to	O
the	O
neighbour	O
methods	O
described	O
in	O
section	O
here	O
is	O
the	O
main	O
contribution	O
of	O
aha	O
kibler	O
and	O
albert	O
is	O
the	O
attempt	O
to	O
achieve	O
satisfactory	O
accuracy	B
while	O
using	O
less	O
storage	B
the	O
algorithms	O
presented	O
in	O
chapter	O
assumed	O
that	O
all	O
training	O
data	O
are	O
available	O
whereas	O
and	O
examine	O
methods	O
for	O
forgetting	O
instances	O
that	O
do	O
not	O
improve	O
classification	B
accuracy	B
figure	O
shows	O
the	O
boundaries	O
of	O
an	O
imaginary	O
concept	B
in	O
a	O
two	O
dimensions	O
space	O
the	O
dashed	O
lines	O
represent	O
the	O
boundaries	O
of	O
the	O
target	O
concept	B
the	O
learning	O
procedure	O
attempts	O
to	O
approximate	O
these	O
boundaries	O
by	O
nearest	B
neighbour	I
matches	O
note	O
that	O
the	O
boundaries	O
defined	O
by	O
the	O
matching	O
procedure	O
are	O
quite	O
irregular	O
this	O
can	O
have	O
its	O
advantages	O
when	O
the	O
target	O
concept	B
does	O
not	O
have	O
a	O
regular	O
shape	O
learning	O
by	O
remembering	O
typical	O
examples	O
of	O
a	O
concept	B
has	O
several	O
other	O
advantages	O
if	O
an	O
efficient	O
indexing	O
mechanism	O
can	O
be	O
devised	O
to	O
find	O
near	O
matches	O
this	O
representation	O
can	O
be	O
very	O
fast	O
as	O
a	O
classifier	B
since	O
it	O
reduces	O
to	O
a	O
table	O
look	O
up	O
it	O
does	O
not	O
require	O
any	O
sophisticated	O
reasoning	O
system	O
and	O
is	O
very	O
flexible	O
as	O
we	O
shall	O
see	O
later	O
representations	O
that	O
rely	O
on	O
abstractions	O
of	O
concepts	O
can	O
run	O
into	O
trouble	O
with	O
what	O
appear	O
to	O
be	O
simple	O
concepts	O
for	O
example	B
an	O
abstract	O
representation	O
of	O
a	O
chair	O
may	O
consist	O
of	O
a	O
description	O
of	O
the	O
number	O
legs	O
the	O
height	O
etc	O
however	O
exceptions	O
abound	O
since	O
anything	O
that	O
can	O
be	O
sat	O
on	O
can	O
be	O
thought	O
of	O
as	O
a	O
chair	O
thus	O
abstractions	O
must	O
often	O
be	O
augmented	O
by	O
lists	O
of	O
exceptions	O
instance-based	B
representation	O
does	O
not	O
suffer	O
from	O
this	O
problem	O
since	O
it	O
only	O
consists	O
exceptions	O
and	O
is	O
designed	O
to	O
handle	O
them	O
efficiently	O
one	O
of	O
the	O
major	O
disadvantages	O
of	O
this	O
style	O
of	O
representation	O
is	O
that	O
it	O
is	O
necessary	O
to	O
define	O
a	O
similarity	O
metric	O
for	O
objects	O
in	O
the	O
universe	O
this	O
can	O
often	O
be	O
difficult	O
to	O
do	O
when	O
the	O
objects	O
are	O
quite	O
complex	B
another	O
disadvantage	O
is	O
that	O
the	O
representation	O
is	O
not	O
human	O
readable	O
in	O
the	O
previous	O
section	O
we	O
made	O
the	O
distinction	O
between	O
an	O
language	O
of	O
observation	O
and	O
a	O
hypothesis	B
language	I
when	O
learning	O
using	O
prototypes	B
the	O
language	O
of	O
observation	O
may	O
be	O
an	O
attributevalue	O
representation	O
the	O
hypothesis	B
language	I
is	O
simply	O
a	O
set	O
of	O
attributevalue	O
or	O
feature	O
vectors	O
representing	O
the	O
prototypes	B
while	O
examples	O
are	O
often	O
a	O
useful	O
means	O
of	O
communicating	O
ideas	O
a	O
very	O
large	O
set	O
of	O
examples	O
can	O
easily	O
swamp	O
the	O
reader	O
with	O
unnecessary	O
detail	O
and	O
fails	O
to	O
emphasis	O
important	O
features	B
of	O
a	O
class	B
thus	O
a	O
collection	O
of	O
typical	O
instances	O
may	O
not	O
convey	O
much	O
insight	O
into	O
the	O
concept	B
that	O
has	O
been	O
learned	O
function	B
approximation	I
as	O
we	O
saw	O
in	O
chapters	O
and	O
statistical	B
and	O
connectionist	O
approaches	O
to	O
machine	O
learning	O
are	O
related	O
to	O
function	B
approximation	I
methods	O
in	O
mathematics	O
for	O
the	O
purposes	O
of	O
illustration	O
let	O
us	O
assume	O
that	O
the	O
learning	O
task	O
is	O
one	O
of	O
classification	B
that	O
is	O
we	O
wish	O
to	O
find	O
ways	O
of	O
grouping	O
objects	O
in	O
a	O
universe	O
in	O
figure	O
we	O
have	O
a	O
universe	O
of	O
objects	O
that	O
belong	O
to	O
either	O
of	O
two	O
classes	B
or	O
by	O
function	O
approximationwe	O
describe	O
a	O
surface	O
that	O
separates	O
the	O
objects	O
into	O
different	O
regions	O
the	O
simplest	O
function	O
is	O
that	O
of	O
a	O
line	O
and	O
linear	B
regression	I
methods	O
and	O
perceptrons	O
are	O
used	O
to	O
find	O
linear	B
discriminant	I
functions	O
section	O
described	O
the	O
perceptron	B
pattern	O
classifier	B
given	O
a	O
binary	O
input	B
vector	O
x	O
a	O
weight	O
vector	O
w	O
and	O
a	O
threshold	O
value	O
if	O
g	O
sec	O
function	B
approximation	I
fig	O
a	O
linear	B
discrimination	B
between	O
two	O
classes	B
then	O
the	O
output	B
is	O
indicating	O
membership	O
of	O
a	O
class	B
otherwise	O
it	O
is	O
indicating	O
exclusion	O
from	O
the	O
class	B
clearly	O
w	O
x	O
describes	O
a	O
hyperplane	O
and	O
the	O
goal	O
of	O
perceptron	B
learning	O
is	O
to	O
find	O
a	O
weight	O
vector	O
w	O
that	O
results	O
in	O
correct	O
classification	B
for	O
all	O
training	O
examples	O
the	O
perceptron	B
is	O
an	O
example	B
of	O
a	O
linear	O
threshold	O
unit	O
a	O
single	O
ltu	O
can	O
only	O
recognise	O
one	O
kind	O
of	O
pattern	O
provided	O
that	O
the	O
input	B
space	O
is	O
linearly	O
separable	O
if	O
we	O
wish	O
to	O
recognise	O
more	O
than	O
one	O
pattern	O
several	O
ltu	O
s	O
can	O
be	O
combined	O
in	O
this	O
case	O
instead	O
of	O
having	O
a	O
vector	O
of	O
weights	O
we	O
have	O
an	O
array	O
the	O
output	B
will	O
now	O
be	O
a	O
vector	O
m	O
jq	O
on	O
where	O
each	O
element	O
of	O
u	O
indicates	O
membership	O
of	O
a	O
class	B
and	O
each	O
row	O
in	O
w	O
is	O
the	O
set	O
of	O
weights	O
for	O
one	O
ltu	O
this	O
architecture	O
is	O
called	O
a	O
pattern	O
associator	O
ltu	O
s	O
can	O
only	O
produce	O
linear	B
discriminant	I
functions	O
and	O
consequently	O
they	O
are	O
limited	O
in	O
the	O
kinds	O
of	O
classes	B
that	O
can	O
be	O
learned	O
however	O
it	O
was	O
found	O
that	O
by	O
cascading	O
pattern	O
associators	O
it	O
is	O
possible	O
to	O
approximate	O
decision	O
surfaces	O
that	O
are	O
of	O
a	O
higher	O
order	O
than	O
simple	O
hyperplanes	O
in	O
cascaded	O
system	O
the	O
outputs	O
of	O
one	O
pattern	O
associator	O
are	O
fed	O
into	O
the	O
inputs	O
of	O
another	O
thus	O
to	O
facilitate	O
learning	O
a	O
further	O
modification	O
must	O
be	O
made	O
rather	O
than	O
using	O
a	O
simple	O
threshold	O
as	O
in	O
the	O
perceptron	B
multi-layer	O
networks	O
usually	O
use	O
a	O
non-linear	O
threshold	O
such	O
as	O
a	O
sigmoid	O
function	O
like	O
perceptron	B
learning	O
back-propagation	O
attempts	O
to	O
reduce	O
the	O
errors	O
between	O
the	O
output	B
of	O
the	O
network	O
and	O
the	O
desired	O
result	O
despite	O
the	O
non-linear	O
threshold	O
multi-layer	O
networks	O
can	O
still	O
be	O
thought	O
of	O
as	O
describing	O
a	O
complex	B
collection	O
of	O
hyperplanes	O
that	O
approximate	O
the	O
required	O
decision	O
surface	O
knowledge	O
representation	O
x	O
fig	O
a	O
pole	O
balancer	O
discussion	O
function	B
approximation	I
methods	O
can	O
often	O
produce	O
quite	O
accurate	O
classifiers	O
because	O
they	O
are	O
capable	O
of	O
constructing	O
complex	B
decision	O
surfaces	O
the	O
observation	B
language	I
for	O
algorithms	O
of	O
this	O
class	B
is	O
usually	O
a	O
vector	O
of	O
numbers	O
often	O
preprocessing	B
will	O
convert	O
raw	O
data	O
into	O
a	O
suitable	O
form	O
for	O
example	B
pomerleau	O
accepts	O
raw	O
data	O
from	O
a	O
camera	O
mounted	O
on	O
a	O
moving	O
vehicle	B
and	O
selects	O
portions	O
of	O
the	O
image	O
to	O
process	O
for	O
input	B
to	O
a	O
neural	O
net	O
that	O
learns	O
how	O
to	O
steer	O
the	O
vehicle	B
the	O
knowledge	O
acquired	O
by	O
such	O
a	O
system	O
is	O
stored	O
as	O
weights	O
in	O
a	O
matrix	O
therefore	O
the	O
hypothesis	B
language	I
is	O
usually	O
an	O
array	O
of	O
real	O
numbers	O
thus	O
the	O
results	O
of	O
learning	O
are	O
not	O
easily	O
available	O
for	O
inspection	O
by	O
a	O
human	O
reader	O
moreover	O
the	O
design	O
of	O
a	O
network	O
usually	O
requires	O
informed	O
guesswork	O
on	O
the	O
part	O
of	O
the	O
user	O
in	O
order	O
to	O
obtain	O
satisfactory	O
results	O
although	O
some	O
effort	O
has	O
been	O
devoted	O
to	O
extracting	O
meaning	O
from	O
networks	O
the	O
still	O
communicate	O
little	O
about	O
the	O
data	O
connectionist	O
learning	O
algorithms	O
are	O
still	O
computationally	O
expensive	O
a	O
critical	O
factor	O
in	O
their	O
speed	B
is	O
the	O
encoding	O
of	O
the	O
inputs	O
to	O
the	O
network	O
this	O
is	O
also	O
critical	O
to	O
genetic	B
algorithms	I
and	O
we	O
will	O
illustrate	O
that	O
problem	O
in	O
the	O
next	O
section	O
genetic	B
algorithms	I
genetic	B
algorithms	I
perform	O
a	O
search	O
for	O
the	O
solution	O
to	O
a	O
problem	O
by	O
generating	O
candidate	O
solutions	O
from	O
the	O
space	O
of	O
all	O
solutions	O
and	O
testing	O
the	O
performance	O
of	O
the	O
candidates	O
the	O
search	O
method	O
is	O
based	O
on	O
ideas	O
from	O
genetics	O
and	O
the	O
size	O
of	O
the	O
search	O
space	O
is	O
determined	O
by	O
the	O
representation	O
of	O
the	O
domain	O
an	O
understanding	O
of	O
genetic	B
algorithms	I
will	O
be	O
aided	O
by	O
an	O
example	B
a	O
very	O
common	O
problem	O
in	O
adaptive	O
control	O
is	O
learning	O
to	O
balance	O
a	O
pole	O
that	O
is	O
hinged	O
on	O
a	O
cart	B
that	O
can	O
move	O
in	O
one	O
dimension	O
along	O
a	O
track	O
of	O
fixed	O
length	O
as	O
show	O
in	O
figure	O
the	O
control	O
must	O
use	O
bang-bang	O
control	O
that	O
is	O
a	O
force	O
of	O
fixed	O
magnitude	O
can	O
be	O
applied	O
to	O
push	O
the	O
cart	B
to	O
the	O
left	O
or	O
right	O
before	O
we	O
can	O
begin	O
to	O
learn	O
how	O
to	O
control	O
this	O
system	O
it	O
is	O
necessary	O
to	O
represent	O
it	O
somehow	O
we	O
will	O
use	O
the	O
boxes	B
method	O
that	O
was	O
devised	O
by	O
michie	O
chambers	O
q	O
sec	O
genetic	B
algorithms	I
x	O
x	O
q	O
q	O
fig	O
discretisation	O
of	O
pole	O
balancer	O
state	O
space	O
and	O
its	O
velocity	O
rather	O
than	O
treat	O
the	O
four	O
variables	O
as	O
continuous	O
values	O
michie	O
and	O
chambers	O
chose	O
to	O
discretise	O
each	O
dimension	O
of	O
the	O
state	O
space	O
one	O
possible	O
discretisation	O
is	O
shown	O
in	O
figure	O
the	O
measurements	O
taken	O
of	O
the	O
physical	O
system	O
are	O
the	O
angle	O
of	O
the	O
pole	O
and	O
its	O
angular	O
velocity	O
and	O
the	O
position	O
of	O
the	O
cart	B
this	O
discretisation	O
results	O
in	O
boxes	B
that	O
partition	O
the	O
state	O
space	O
h	O
boxes	B
there	O
are	O
each	O
box	O
has	O
associated	O
with	O
it	O
an	O
action	O
setting	O
which	O
tells	O
the	O
controller	O
that	O
when	O
the	O
system	O
is	O
in	O
that	O
part	O
of	O
the	O
state	O
space	O
the	O
controller	O
should	O
apply	O
that	O
action	O
which	O
is	O
a	O
push	O
to	O
the	O
left	O
or	O
a	O
push	O
to	O
the	O
right	O
since	O
there	O
is	O
a	O
simple	O
binary	O
choice	O
and	O
there	O
are	O
possible	O
control	O
strategies	O
for	O
the	O
pole	O
balancer	O
the	O
simplest	O
kind	O
of	O
learning	O
in	O
this	O
case	O
is	O
to	O
exhaustively	O
search	O
for	O
the	O
right	O
combination	O
however	O
this	O
is	O
clearly	O
impractical	O
given	O
the	O
size	O
of	O
the	O
search	O
space	O
instead	O
we	O
can	O
invoke	O
a	O
genetic	B
search	O
strategy	O
that	O
will	O
reduce	O
the	O
amount	O
of	O
search	O
considerably	O
in	O
genetic	B
learning	O
we	O
assume	O
that	O
there	O
is	O
a	O
population	O
of	O
individuals	O
each	O
one	O
of	O
which	O
represents	O
a	O
candidate	O
problem	O
solver	O
for	O
a	O
given	O
task	O
like	O
evolution	O
genetic	B
algorithms	I
test	O
each	O
individual	O
from	O
the	O
population	O
and	O
only	O
the	O
fittest	O
survive	O
to	O
reproduce	O
for	O
the	O
next	O
generation	O
the	O
algorithm	O
creates	O
new	O
generations	O
until	O
at	O
least	O
one	O
individual	O
is	O
found	O
that	O
can	O
solve	O
the	O
problem	O
adequately	O
each	O
problem	O
solver	O
is	O
a	O
chromosome	O
a	O
position	O
or	O
set	O
of	O
positions	O
in	O
a	O
chromosome	O
is	O
called	O
a	O
gene	O
the	O
possible	O
values	O
a	O
fixed	O
set	O
of	O
symbols	O
of	O
a	O
gene	O
are	O
known	O
as	O
alleles	O
in	O
most	O
genetic	B
algorithm	O
implementations	O
the	O
set	O
of	O
symbols	O
is	O
chromosome	O
lengths	O
are	O
fixed	O
most	O
implementations	O
also	O
use	O
fixed	O
population	O
sizes	O
the	O
most	O
critical	O
problem	O
in	O
applying	O
a	O
genetic	B
algorithm	O
is	O
in	O
finding	O
a	O
suitable	O
encoding	O
of	O
the	O
examples	O
in	O
the	O
problem	O
domain	O
to	O
a	O
chromosome	O
a	O
good	O
choice	O
of	O
representation	O
will	O
make	O
the	O
search	O
easy	O
by	O
limiting	O
the	O
search	O
space	O
a	O
poor	O
choice	O
will	O
result	O
in	O
a	O
large	O
search	O
space	O
for	O
our	O
pole	B
balancing	I
example	B
we	O
will	O
use	O
a	O
very	O
simple	O
encoding	O
a	O
chromosome	O
is	O
a	O
string	O
of	O
boxes	B
each	O
box	O
or	O
gene	O
can	O
take	O
values	O
push	O
left	O
or	O
push	O
right	O
choosing	O
the	O
size	O
of	O
the	O
population	O
can	O
be	O
tricky	O
since	O
a	O
small	O
population	O
size	O
provides	O
an	O
insufficient	O
sample	O
size	O
over	O
the	O
space	O
of	O
solutions	O
for	O
a	O
problem	O
and	O
large	O
population	O
requires	O
a	O
lot	O
of	O
evaluation	O
and	O
will	O
be	O
slow	O
in	O
this	O
example	B
is	O
a	O
suitable	O
population	O
size	O
and	O
knowledge	O
representation	O
each	O
iteration	O
in	O
a	O
genetic	B
algorithm	O
is	O
called	O
a	O
generation	O
each	O
chromosome	O
in	O
a	O
population	O
is	O
used	O
to	O
solve	O
a	O
problem	O
its	O
performance	O
is	O
evaluated	O
and	O
the	O
chromosome	O
is	O
given	O
some	O
rating	O
of	O
fitness	O
the	O
population	O
is	O
also	O
given	O
an	O
overall	O
fitness	O
rating	O
based	O
on	O
the	O
performance	O
of	O
its	O
members	O
the	O
fitness	O
value	O
indicates	O
how	O
close	O
a	O
chromosome	O
or	O
population	O
is	O
to	O
the	O
required	O
solution	O
for	O
pole	B
balancing	I
the	O
fitness	O
value	O
of	O
a	O
chromosome	O
may	O
be	O
the	O
number	O
of	O
time	B
steps	O
that	O
the	O
chromosome	O
is	O
able	O
to	O
keep	O
the	O
pole	O
balanced	O
for	O
new	O
sets	O
of	O
chromosomes	B
are	O
produced	O
from	O
one	O
generation	O
to	O
the	O
next	O
reproduction	O
takes	O
place	O
when	O
selected	O
chromosomes	B
from	O
one	O
generation	O
are	O
recombined	O
with	O
others	O
to	O
form	O
chromosomes	B
for	O
the	O
next	O
generation	O
the	O
new	O
ones	O
are	O
called	O
offspring	O
selection	O
of	O
chromosomes	B
for	O
reproduction	O
is	O
based	O
on	O
their	O
fitness	O
values	O
the	O
average	O
fitness	O
of	O
population	O
may	O
also	O
be	O
calculated	O
at	O
end	O
of	O
each	O
generation	O
for	O
pole	B
balancing	I
individuals	O
whose	O
fitness	O
is	O
below	O
average	O
are	O
replaced	O
by	O
reproduction	O
of	O
above	O
average	O
chromosomes	B
the	O
strategy	O
must	O
be	O
modified	O
if	O
two	O
few	O
or	O
two	O
many	O
chromosomes	B
survive	O
for	O
example	B
at	O
least	O
and	O
at	O
most	O
must	O
survive	O
operators	O
that	O
recombine	O
the	O
selected	O
chromosomes	B
are	O
called	O
genetic	B
operators	O
two	O
common	O
operators	O
are	O
crossover	O
and	O
mutation	O
crossover	O
exchanges	O
portions	O
of	O
a	O
pair	O
of	O
chromosomesat	O
a	O
randomly	O
chosen	O
point	O
called	O
the	O
crossover	O
point	O
some	O
implementations	O
and	O
have	O
more	O
than	O
one	O
crossover	O
point	O
for	O
example	B
if	O
there	O
are	O
two	O
chromosomesk	O
and	O
the	O
crossover	O
point	O
is	O
the	O
resulting	O
offspring	O
are	O
offspring	O
produced	O
by	O
crossover	O
cannot	O
contain	O
information	O
that	O
is	O
not	O
already	O
in	O
the	O
population	O
so	O
an	O
additional	O
operatormutation	O
is	O
required	O
mutation	O
generates	O
an	O
offspring	O
by	O
randomly	O
changing	O
the	O
values	O
of	O
genes	O
at	O
one	O
or	O
more	O
gene	O
positions	O
of	O
a	O
selected	O
chromosome	O
for	O
example	B
if	O
the	O
following	O
chromosome	O
h	O
k	O
is	O
mutated	O
at	O
positions	O
and	O
then	O
the	O
resulting	O
offspring	O
is	O
the	O
number	O
of	O
offspring	O
produced	O
for	O
each	O
new	O
generation	O
depends	O
on	O
how	O
members	O
are	O
introduced	O
so	O
as	O
to	O
maintain	O
a	O
fixed	O
population	O
size	O
in	O
a	O
pure	O
replacement	O
strategy	O
the	O
whole	O
population	O
is	O
replaced	O
by	O
a	O
new	O
one	O
in	O
an	O
elitist	O
strategy	O
a	O
proportion	O
of	O
the	O
population	O
survives	O
to	O
the	O
next	O
generation	O
in	O
pole	B
balancing	I
all	O
offspring	O
are	O
created	O
by	O
crossover	O
when	O
more	O
the	O
will	O
survive	O
for	O
more	O
than	O
three	O
generations	O
when	O
the	O
rate	O
is	O
reduced	O
to	O
only	O
being	O
produced	O
by	O
crossover	O
mutation	O
is	O
a	O
background	O
operator	O
which	O
helps	O
to	O
sustain	O
exploration	O
each	O
offspring	O
produced	O
by	O
crossover	O
has	O
a	O
probability	O
of	O
of	O
being	O
mutated	O
before	O
it	O
enters	O
the	O
population	O
if	O
more	O
then	O
will	O
survive	O
the	O
mutation	O
rate	O
is	O
increased	O
to	O
the	O
number	O
of	O
offspring	O
an	O
individual	O
can	O
produce	O
by	O
crossover	O
is	O
proportional	O
to	O
its	O
fitness	O
o	O
d	O
j	O
o	O
sec	O
propositional	B
learning	I
systems	I
v	O
small	O
small	O
medium	O
large	O
v	O
large	O
red	O
orange	O
yellow	O
green	O
blue	O
violet	O
fig	O
discrimination	B
on	O
attributes	B
and	O
values	O
where	O
the	O
number	O
of	O
children	O
is	O
the	O
total	O
number	O
of	O
individuals	O
to	O
be	O
replaced	O
mates	O
are	O
chosen	O
at	O
random	O
among	O
the	O
survivors	O
the	O
pole	B
balancing	I
experiments	O
described	O
above	O
were	O
conducted	O
by	O
odetayo	O
this	O
may	O
not	O
be	O
the	O
only	O
way	O
of	O
encoding	O
the	O
problem	O
for	O
a	O
genetic	B
algorithm	O
and	O
so	O
other	O
solutions	O
may	O
be	O
possible	O
however	O
this	O
requires	O
effort	O
on	O
the	O
part	O
of	O
the	O
user	O
to	O
devise	O
a	O
clever	O
encoding	O
propositional	B
learning	I
systems	I
rather	O
than	O
searching	O
for	O
discriminant	O
functions	O
symbolic	B
learning	I
systems	O
find	O
expressions	O
equivalent	O
to	O
sentences	O
in	O
some	O
form	O
of	O
logic	O
for	O
example	B
we	O
may	O
distinguish	O
objects	O
according	O
to	O
two	O
attributes	B
size	O
and	O
colour	O
we	O
may	O
say	O
that	O
an	O
object	O
belongs	O
to	O
class	B
if	O
its	O
colour	O
is	O
red	O
and	O
its	O
size	O
is	O
very	O
small	O
to	O
medium	O
following	O
the	O
notation	O
of	O
michalski	O
the	O
classes	B
in	O
figure	O
may	O
be	O
written	O
as	O
c	O
k	O
v	O
d	O
z	O
d	O
v	O
j	O
v	O
note	O
that	O
this	O
kind	O
of	O
description	O
partitions	O
the	O
universe	O
into	O
blocks	O
unlike	O
the	O
function	B
approximation	I
methods	O
that	O
find	O
smooth	O
surfaces	O
to	O
discriminate	O
classes	B
interestingly	O
one	O
of	O
the	O
popular	O
early	O
machine	O
learning	O
algorithms	O
aq	B
had	O
its	O
origins	O
in	O
switching	O
theory	O
one	O
of	O
the	O
concerns	O
of	O
switching	O
theory	O
is	O
to	O
find	O
ways	O
of	O
minimising	O
logic	O
circuits	O
that	O
is	O
simplifying	O
the	O
truth	O
table	O
description	O
of	O
the	O
function	O
of	O
a	O
circuit	O
to	O
a	O
simple	O
expression	O
in	O
boolean	O
logic	O
many	O
of	O
the	O
algorithms	O
in	O
switching	O
theory	O
take	O
tables	O
like	O
figure	O
and	O
search	O
for	O
the	O
best	O
way	O
of	O
covering	O
all	O
of	O
the	O
entries	O
in	O
the	O
table	O
aq	B
uses	O
a	O
covering	B
algorithm	I
to	O
build	O
its	O
concept	B
description	O
ed	O
e	O
knowledge	O
representation	O
f	O
v	O
d	O
fig	O
decision	O
tree	O
learning	O
v	O
n	O
the	O
best	O
expression	O
is	O
usually	O
some	O
compromise	O
between	O
the	O
desire	O
to	O
cover	B
as	O
many	O
positive	O
examples	O
as	O
possible	O
and	O
the	O
desire	O
to	O
have	O
as	O
compact	O
and	O
readable	O
a	O
representation	O
as	O
possible	O
in	O
designing	O
aq	B
michalski	O
was	O
particularly	O
concerned	O
with	O
the	O
expressiveness	O
of	O
the	O
concept	B
description	O
language	O
a	O
drawback	O
of	O
the	O
aq	B
learning	O
algorithm	O
is	O
that	O
it	O
does	O
not	O
use	O
statistical	B
information	O
present	O
in	O
the	O
training	O
sample	O
to	O
guide	O
induction	O
however	O
decision	O
tree	O
learning	O
algorithms	O
do	O
the	O
basic	O
method	O
of	O
building	O
a	O
decision	O
tree	O
is	O
summarised	O
in	O
figure	O
an	O
simple	O
attributevalue	O
representation	O
is	O
used	O
and	O
so	O
like	O
aq	B
decision	B
trees	I
are	O
incapable	O
of	O
representing	O
relational	O
information	O
they	O
are	O
however	O
very	O
quick	O
and	O
easy	O
to	O
build	O
f	O
and	O
create	O
a	O
decision	O
node	O
the	O
algorithm	O
operates	O
over	O
a	O
set	O
of	O
training	O
instances	O
if	O
all	O
instances	O
in	O
create	O
a	O
node	O
are	O
in	O
class	B
partition	O
the	O
traning	O
instances	O
in	O
into	O
subsets	O
according	O
to	O
the	O
values	O
apply	O
the	O
algorithm	O
recursively	O
to	O
each	O
if	O
the	O
subsets	O
of	O
of	O
f	O
and	O
stop	O
otherwise	O
select	O
a	O
feature	O
e	O
sec	O
propositional	B
learning	I
systems	I
fig	O
the	O
dashed	O
line	O
shows	O
the	O
real	O
division	O
of	O
objects	O
in	O
the	O
universe	O
the	O
solid	O
lines	O
show	O
a	O
decision	O
tree	O
approximation	O
decision	O
tree	O
learning	O
algorithms	O
can	O
be	O
seen	O
as	O
methods	O
for	O
partitioning	O
the	O
universe	O
into	O
successively	O
smaller	O
rectangles	O
with	O
the	O
goal	O
that	O
each	O
rectangle	O
only	O
contains	O
objects	O
of	O
one	O
class	B
this	O
is	O
illustrated	O
in	O
figure	O
discussion	O
michalski	O
has	O
always	O
argued	O
in	O
favour	O
of	O
rule-based	O
representations	O
over	O
tree	O
structured	O
representations	O
on	O
the	O
grounds	O
of	O
readability	O
when	O
the	O
domain	O
is	O
complex	B
decision	B
trees	I
can	O
become	O
very	O
bushy	O
and	O
difficult	O
to	O
understand	O
whereas	O
rules	O
tend	O
to	O
be	O
modular	O
and	O
can	O
be	O
read	O
in	O
isolation	O
of	O
the	O
rest	O
of	O
the	O
knowledge-base	O
constructed	O
by	O
induction	O
on	O
the	O
other	O
hand	O
decision	B
trees	I
induction	O
programs	O
are	O
usually	O
very	O
fast	O
a	O
compromise	O
is	O
to	O
use	O
decision	O
tree	O
induction	O
to	O
build	O
an	O
initial	O
tree	O
and	O
then	O
derive	O
rules	O
from	O
the	O
tree	O
thus	O
transforming	O
an	O
efficient	O
but	O
opaque	O
representation	O
into	O
a	O
transparent	O
one	O
it	O
is	O
instructive	O
to	O
compare	O
the	O
shapes	O
that	O
are	O
produced	O
by	O
various	O
learning	O
systems	O
when	O
they	O
partition	O
the	O
universe	O
figure	O
demonstrates	O
one	O
weakness	O
of	O
decision	O
tree	O
and	O
other	O
symbolic	O
classification	B
since	O
they	O
approximate	O
partitions	O
with	O
rectangles	O
the	O
universe	O
is	O
there	O
is	O
an	O
inherent	O
inaccuracy	O
when	O
dealing	O
with	O
domains	O
with	O
continuous	O
attributes	B
function	B
approximation	I
methods	O
and	O
ibl	O
may	O
be	O
able	O
to	O
attain	O
higher	O
accuracy	B
but	O
at	O
the	O
expense	O
of	O
transparency	O
of	O
the	O
resulting	O
theory	O
it	O
is	O
more	O
difficult	O
to	O
make	O
general	O
comments	O
about	O
genetic	B
algorithms	I
since	O
the	O
encoding	O
method	O
knowledge	O
representation	O
fig	O
generalisation	O
as	O
set	O
covering	O
will	O
affect	O
both	O
accuracy	B
and	O
readability	O
as	O
we	O
have	O
seen	O
useful	O
insights	O
into	O
induction	O
can	O
be	O
gained	O
by	O
visualising	O
it	O
as	O
searching	O
for	O
a	O
cover	B
of	O
objects	O
in	O
the	O
universe	O
unfortunately	O
there	O
are	O
limits	O
to	O
this	O
geometric	O
interpretation	O
of	O
learning	O
if	O
we	O
wish	O
to	O
learn	O
concepts	O
that	O
describe	O
complex	B
objects	O
and	O
relationships	O
between	O
the	O
objects	O
it	O
becomes	O
very	O
difficult	O
to	O
visualise	O
the	O
universe	O
for	O
this	O
reason	O
it	O
is	O
often	O
useful	O
to	O
rely	O
on	O
reasoning	O
about	O
the	O
concept	B
description	O
language	O
as	O
we	O
saw	O
the	O
cover	B
in	O
figure	O
can	O
be	O
expressed	O
as	O
clauses	O
in	O
propositional	O
logic	O
we	O
can	O
establish	O
a	O
correspondence	O
between	O
sentences	O
in	O
the	O
concept	B
description	O
language	O
hypothesis	B
language	I
and	O
a	O
diagrammatic	O
representation	O
of	O
the	O
concept	B
more	O
importantly	O
we	O
can	O
create	O
a	O
correspondence	O
between	O
generalisation	O
and	O
specialisation	O
operations	O
on	O
the	O
sets	O
of	O
objects	O
and	O
generalisation	O
and	O
specialisation	O
operations	O
on	O
the	O
sentences	O
of	O
the	O
language	O
for	O
example	B
figure	O
shows	O
two	O
sets	O
labelled	O
class	B
and	O
class	B
it	O
is	O
clear	O
that	O
class	B
is	O
a	O
generalisation	O
of	O
class	B
since	O
it	O
includes	O
a	O
larger	O
number	O
of	O
objects	O
in	O
the	O
universe	O
we	O
also	O
call	O
class	B
a	O
specialisation	O
of	O
class	B
by	O
convention	O
we	O
say	O
the	O
description	O
of	O
class	B
is	O
a	O
generalisation	O
of	O
the	O
description	O
of	O
class	B
thus	O
is	O
a	O
generalisation	O
of	O
c	O
c	O
k	O
once	O
we	O
have	O
established	O
the	O
correspondence	O
between	O
sets	O
of	O
objects	O
and	O
their	O
descriptions	O
it	O
is	O
often	O
convenient	O
to	O
forget	O
about	O
the	O
objects	O
and	O
only	O
consider	O
that	O
we	O
are	O
working	O
with	O
expressions	O
in	O
a	O
language	O
the	O
reason	O
is	O
simple	O
beyond	O
a	O
certain	O
point	O
of	O
complexity	O
it	O
is	O
not	O
possible	O
to	O
visualise	O
sets	O
but	O
it	O
is	O
relatively	O
easy	O
to	O
apply	O
simple	O
transformations	O
on	O
sentences	O
in	O
a	O
formal	O
language	O
for	O
example	B
clause	O
can	O
be	O
generalised	O
very	O
easily	O
to	O
clause	O
by	O
dropping	O
one	O
of	O
the	O
conditions	O
sec	O
relations	O
and	O
background	B
knowledge	I
in	O
the	O
next	O
section	O
we	O
will	O
look	O
at	O
learning	O
algorithms	O
that	O
deal	O
with	O
relational	O
information	O
in	O
this	O
case	O
the	O
emphasis	O
on	O
language	O
is	O
essential	O
since	O
geometric	O
interpretations	O
no	O
longer	O
provide	O
us	O
with	O
any	O
real	O
insight	O
into	O
the	O
operation	O
of	O
these	O
algorithms	O
relations	O
and	O
background	B
knowledge	I
inductions	O
systems	O
as	O
we	O
have	O
seen	O
so	O
far	O
might	O
be	O
described	O
as	O
what	O
you	O
see	O
is	O
what	O
you	O
get	O
that	O
is	O
the	O
output	B
class	B
descriptions	O
use	O
the	O
same	O
vocabulary	O
as	O
the	O
input	B
examples	O
however	O
we	O
will	O
see	O
in	O
this	O
section	O
that	O
it	O
is	O
often	O
useful	O
to	O
incorporate	O
background	B
knowledge	I
into	O
learning	O
we	O
use	O
a	O
simple	O
example	B
from	O
banerji	O
to	O
the	O
use	O
of	O
background	B
knowledge	I
there	O
is	O
a	O
language	O
for	O
describing	O
instances	O
of	O
a	O
concept	B
and	O
another	O
for	O
describing	O
concepts	O
suppose	O
we	O
wish	O
to	O
represent	O
the	O
binary	O
number	O
by	O
a	O
left-recursive	O
binary	O
tree	O
of	O
digits	O
and	O
head	O
and	O
tail	O
are	O
the	O
names	O
of	O
attributes	B
their	O
values	O
follow	O
the	O
colon	O
the	O
concepts	O
of	O
binary	O
digit	O
and	O
binary	O
number	O
are	O
defined	O
as	O
v	O
d	O
vd	O
d	O
j	O
j	O
z	O
c	O
d	O
j	O
j	O
z	O
d	O
n	O
thus	O
an	O
object	O
belongs	O
to	O
a	O
particular	O
class	B
or	O
concept	B
if	O
it	O
satisfies	O
the	O
logical	O
expression	O
in	O
the	O
body	O
of	O
the	O
description	O
predicates	O
in	O
the	O
expression	O
may	O
test	O
the	O
membership	O
of	O
an	O
object	O
in	O
a	O
previously	O
learned	O
concept	B
banerji	O
always	O
emphasised	O
the	O
importance	O
of	O
a	O
description	O
language	O
that	O
could	O
grow	O
that	O
is	O
its	O
descriptive	O
power	O
should	O
increase	O
as	O
new	O
concepts	O
are	O
learned	O
this	O
can	O
clearly	O
be	O
seen	O
in	O
the	O
example	B
above	O
having	O
learned	O
to	O
describe	O
binary	O
digits	O
the	O
concept	B
of	O
digit	O
becomes	O
available	O
for	O
use	O
in	O
the	O
description	O
of	O
more	O
complex	B
concepts	O
such	O
as	O
binary	O
number	O
and	O
an	O
instance	O
extensibility	O
is	O
a	O
natural	O
and	O
easily	O
implemented	O
feature	O
of	O
horn-clause	O
logic	O
in	O
addition	O
a	O
description	O
in	O
horn-clause	O
logic	O
is	O
a	O
logic	O
program	O
and	O
can	O
be	O
executed	O
for	O
example	B
to	O
recognise	O
an	O
object	O
a	O
horn	O
clause	O
can	O
be	O
interpreted	O
in	O
a	O
forward	B
chaining	O
manner	O
suppose	O
we	O
have	O
a	O
set	O
of	O
clauses	O
clause	O
recognises	O
the	O
first	O
two	O
terms	O
in	O
expression	O
reducing	O
it	O
to	O
v	O
clause	O
reduces	O
this	O
to	O
that	O
is	O
clauses	O
and	O
recognise	O
expression	O
as	O
the	O
description	O
of	O
an	O
instance	O
of	O
concept	B
when	O
clauses	O
are	O
executed	O
in	O
a	O
backward	B
chaining	O
manner	O
they	O
can	O
either	O
verify	O
that	O
the	O
input	B
object	O
belongs	O
to	O
a	O
concept	B
or	O
produce	O
instances	O
of	O
concepts	O
in	O
other	O
words	O
knowledge	O
representation	O
largerhammer	O
feather	O
denserhammer	O
feather	O
heaviera	O
b	O
densera	O
b	O
largera	O
b	O
heavierhammer	O
feather	O
heaviera	O
b	O
densera	O
b	O
largera	O
b	O
heavierhammer	O
feather	O
denserhammer	O
feather	O
largerhammer	O
feather	O
denser	O
hammer	O
feather	O
larger	O
hammer	O
feather	O
largerhammer	O
feather	O
fig	O
a	O
resolution	O
proof	O
tree	O
from	O
muggleton	O
feng	O
we	O
attempt	O
to	O
prove	O
an	O
assertion	O
is	O
true	O
with	O
respect	O
to	O
a	O
background	O
theory	O
resolution	O
provides	O
an	O
efficient	O
means	O
of	O
deriving	O
a	O
solution	O
to	O
a	O
problem	O
giving	O
a	O
set	O
of	O
axioms	O
which	O
define	O
the	O
task	O
environment	O
the	O
algorithm	O
takes	O
two	O
terms	O
and	O
resolves	O
them	O
into	O
a	O
most	O
general	O
unifier	O
as	O
illustrated	O
in	O
figure	O
by	O
the	O
execution	O
of	O
a	O
simple	O
prolog	O
program	O
the	O
box	O
in	O
the	O
figure	O
contains	O
clauses	O
that	O
make	O
up	O
the	O
theory	O
or	O
knowledge	O
base	O
and	O
the	O
question	O
to	O
be	O
answered	O
namely	O
is	O
it	O
true	O
that	O
a	O
hammer	O
is	O
heavier	O
than	O
a	O
feather	O
a	O
resolution	O
proof	O
is	O
a	O
proof	O
by	O
refutation	O
that	O
is	O
answer	O
the	O
question	O
we	O
assume	O
that	O
it	O
is	O
false	O
and	O
then	O
see	O
if	O
the	O
addition	O
to	O
the	O
theory	O
of	O
this	O
negative	O
statement	O
results	O
in	O
a	O
contradiction	O
the	O
literals	O
on	O
the	O
left	O
hand	O
side	O
of	O
a	O
prolog	O
clause	O
are	O
positive	O
those	O
on	O
the	O
left	O
hand	O
side	O
are	O
negative	O
the	O
proof	O
procedure	O
looks	O
for	O
complimentary	O
literals	O
in	O
two	O
clauses	O
n	O
and	O
i	O
e	O
literals	O
of	O
opposite	O
sign	O
that	O
unify	O
in	O
the	O
example	B
in	O
figure	O
x	O
k	O
unify	O
to	O
create	O
the	O
first	O
resolvent	O
o	O
k	O
xo	O
k	O
a	O
side	O
effect	O
of	O
unification	O
is	O
to	O
create	O
variable	O
substitutions	O
by	O
continued	O
application	O
of	O
resolution	O
we	O
can	O
eventually	O
derive	O
the	O
empty	O
clause	O
which	O
indicates	O
a	O
contradiction	O
plotkin	O
s	O
work	O
originated	O
with	O
a	O
suggestion	O
of	O
r	O
j	O
popplestone	O
that	O
since	O
unification	O
is	O
useful	O
in	O
automatic	O
deduction	O
by	O
the	O
resolution	O
method	O
its	O
dual	O
might	O
prove	O
helpful	O
for	O
induction	O
the	O
dual	O
of	O
the	O
most	O
general	O
unifier	O
of	O
two	O
literals	O
is	O
called	O
the	O
least	O
general	O
generalisation	O
at	O
about	O
the	O
same	O
time	B
that	O
plotkin	O
took	O
up	O
this	O
idea	O
j	O
c	O
reynolds	O
was	O
also	O
developing	O
the	O
use	O
of	O
least	O
general	O
generalisations	O
reynolds	O
also	O
recognised	O
the	O
connection	O
between	O
deductive	O
theorem	O
proving	O
and	O
inductive	B
learning	I
sec	O
relations	O
and	O
background	B
knowledge	I
robinson	O
s	O
unification	O
algorithm	O
allows	O
the	O
computation	O
of	O
the	O
greatest	O
common	O
instance	O
of	O
any	O
finite	O
set	O
of	O
unifiable	O
atomic	O
formulas	O
this	O
suggests	O
the	O
existence	O
of	O
a	O
dual	O
operation	O
of	O
least	O
common	O
generalisation	O
it	O
turns	O
out	O
that	O
such	O
an	O
operation	O
exists	O
and	O
can	O
be	O
computed	O
by	O
a	O
simple	O
algorithm	O
such	O
that	O
the	O
least	O
general	O
generalisation	O
of	O
background	O
information	O
which	O
may	O
assist	O
generalisation	O
suppose	O
we	O
are	O
given	O
two	O
instances	O
of	O
a	O
concept	B
cuddly	O
pet	O
buntine	O
pointed	O
out	O
that	O
simple	O
subsumption	O
is	O
unable	O
to	O
take	O
advantage	O
of	O
if	O
there	O
is	O
a	O
substitution	O
the	O
method	O
of	O
least	O
general	O
generalisations	O
is	O
based	O
on	O
subsumption	O
a	O
clause	O
subsumes	O
or	O
is	O
more	O
general	O
than	O
another	O
clause	O
jq	O
x	O
o	O
jl	O
n	O
xo	O
n	O
jl	O
xkvn	O
under	O
the	O
substitution	O
is	O
equivalent	O
to	O
and	O
under	O
the	O
substitution	O
is	O
equivalent	O
to	O
therefore	O
the	O
least	O
general	O
generalisation	O
of	O
jq	O
x	O
and	O
is	O
jl	O
xkvn	O
and	O
results	O
in	O
the	O
inverse	O
substitution	O
jl	O
n	O
xo	O
n	O
o	O
d	O
jqkvn	O
o	O
d	O
jqkvn	O
jqkvn	O
jqkvn	O
jqkvn	O
o	O
d	O
jqkvn	O
o	O
d	O
jqkvn	O
jlkvn	O
since	O
unmatched	O
literals	O
are	O
dropped	O
from	O
the	O
clause	O
however	O
given	O
the	O
background	B
knowledge	I
we	O
can	O
see	O
that	O
this	O
is	O
an	O
over-generalisation	O
a	O
better	O
one	O
is	O
the	O
moral	O
being	O
that	O
a	O
generalisation	O
should	O
only	O
be	O
done	O
when	O
the	O
relevant	O
background	B
knowledge	I
suggests	O
it	O
so	O
observing	O
use	O
clause	O
as	O
a	O
rewrite	O
rule	O
to	O
produce	O
a	O
generalisation	O
which	O
is	O
clause	O
which	O
also	O
subsumes	O
clause	O
buntine	O
drew	O
on	O
earlier	O
work	O
by	O
sammut	O
banerji	O
in	O
constructing	O
his	O
generalised	O
subsumption	O
muggleton	O
buntine	O
took	O
this	O
approach	O
a	O
step	O
further	O
and	O
realised	O
that	O
through	O
the	O
application	O
of	O
a	O
few	O
simple	O
rules	O
they	O
could	O
invert	O
resolution	O
as	O
plotkin	O
and	O
reynolds	O
had	O
wished	O
here	O
are	O
two	O
of	O
the	O
rewrite	O
rules	O
in	O
propositional	O
form	O
given	O
a	O
set	O
of	O
clauses	O
the	O
body	O
of	O
one	O
of	O
which	O
is	O
completely	O
contained	O
in	O
the	O
bodies	O
according	O
to	O
subsumption	O
the	O
least	O
general	O
generalisation	O
of	O
and	O
is	O
suppose	O
we	O
also	O
know	O
the	O
following	O
o	O
jlkvn	O
of	O
the	O
others	O
such	O
as	O
z	O
k	O
knowledge	O
representation	O
the	O
absorption	O
operation	O
results	O
in	O
intra-construction	O
takes	O
a	O
group	O
of	O
rules	O
all	O
having	O
the	O
same	O
head	O
such	O
as	O
and	O
replaces	O
them	O
with	O
z	O
z	O
z	O
c	O
these	O
two	O
operations	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
the	O
proof	O
tree	O
shown	O
in	O
figure	O
resolution	O
accepts	O
two	O
clauses	O
and	O
applies	O
unification	O
to	O
find	O
the	O
maximal	O
common	O
unifier	O
in	O
the	O
diagram	O
two	O
clauses	O
at	O
the	O
top	O
of	O
a	O
v	O
are	O
resolved	O
to	O
produce	O
the	O
resolvent	O
at	O
the	O
apex	O
of	O
the	O
v	O
absorption	O
accepts	O
the	O
resolvent	O
and	O
one	O
of	O
the	O
other	O
two	O
clauses	O
to	O
produce	O
the	O
third	O
thus	O
it	O
inverts	O
the	O
resolution	O
step	O
intra-construction	O
automatically	O
creates	O
a	O
new	O
term	O
in	O
its	O
attempt	O
to	O
simplify	O
descriptions	O
this	O
is	O
an	O
essential	O
feature	O
of	O
inverse	O
resolution	O
since	O
there	O
may	O
be	O
terms	O
in	O
a	O
theory	O
that	O
are	O
not	O
explicitly	O
shown	O
in	O
an	O
example	B
and	O
may	O
have	O
to	O
be	O
invented	O
by	O
the	O
learning	O
program	O
discussion	O
these	O
methods	O
and	O
others	O
feng	O
quinlan	O
have	O
made	O
relational	B
learning	I
quite	O
efficient	O
because	O
the	O
language	O
of	O
horn-clause	O
logic	O
is	O
more	O
expressive	O
than	O
the	O
other	O
concept	B
description	O
languages	O
we	O
have	O
seen	O
it	O
is	O
now	O
possible	O
to	O
learn	O
far	O
more	O
complex	B
concepts	O
than	O
was	O
previously	O
possible	O
a	O
particularly	O
important	O
application	O
of	O
this	O
style	O
of	O
learning	O
is	O
knowledge	O
discovery	O
there	O
are	O
now	O
vast	O
databases	O
accumulating	O
information	O
on	O
the	O
genetic	B
structure	O
of	O
human	O
beings	O
aircraft	O
accidents	O
company	O
inventories	O
pharmaceuticals	O
and	O
countless	O
more	O
powerful	O
induction	O
programs	O
that	O
use	O
expressive	O
languages	O
may	O
be	O
a	O
vital	O
aid	O
in	O
discovering	O
useful	O
patterns	O
in	O
all	O
these	O
data	O
for	O
example	B
the	O
realities	O
of	O
drug	O
design	O
require	O
descriptive	O
powers	O
that	O
encompass	O
stereo-spatial	O
and	O
other	O
long-range	O
relations	O
between	O
different	O
parts	O
of	O
a	O
molecule	O
and	O
can	O
generate	O
in	O
effect	O
new	O
theories	O
the	O
pharmaceutical	O
industry	O
spends	O
over	O
million	O
for	O
each	O
new	O
drug	O
released	O
onto	O
the	O
market	O
the	O
greater	O
part	O
of	O
this	O
expenditure	O
reflects	O
today	O
s	O
unavoidably	O
scatter-gun	O
synthesis	O
of	O
compounds	O
which	O
might	O
possess	O
biological	O
activity	O
even	O
a	O
limited	O
capability	O
to	O
construct	O
predictive	O
theories	O
from	O
data	O
promises	O
high	O
returns	O
the	O
relational	O
program	O
golem	B
was	O
applied	O
to	O
the	O
drug	O
design	O
problem	O
of	O
modelling	O
structure-activity	O
relations	O
et	O
al	O
training	O
data	O
for	O
the	O
program	O
was	O
trimethoprim	O
analogues	O
and	O
their	O
observed	O
inhibition	O
of	O
e	O
coli	O
dihydrofolate	O
reductase	O
a	O
further	O
compounds	O
were	O
used	O
as	O
unseen	O
test	O
data	O
golem	B
obtained	O
rules	O
that	O
were	O
statistically	O
more	O
accurate	O
on	O
the	O
training	O
data	O
and	O
also	O
better	O
on	O
the	O
test	O
data	O
than	O
a	O
hansch	O
linear	B
regression	I
model	O
importantly	O
relational	B
learning	I
yields	O
understandable	O
rules	O
k	O
k	O
k	O
k	O
sec	O
conclusion	O
that	O
characterise	O
the	O
stereochemistry	O
of	O
the	O
interaction	O
of	O
trimethoprim	O
with	O
dihydrofolate	O
reductase	O
observed	O
crystallographically	O
in	O
this	O
domain	O
relational	B
learning	I
thus	O
offers	O
a	O
new	O
approach	O
which	O
complements	O
other	O
methods	O
directing	O
the	O
time-consuming	O
process	O
of	O
the	O
design	O
of	O
potent	O
pharmacological	O
agents	O
from	O
a	O
lead	O
compound	O
variants	O
of	O
which	O
need	O
to	O
be	O
characterised	O
for	O
likely	O
biological	O
activity	O
before	O
committing	O
resources	O
to	O
their	O
synthesis	O
conclusions	O
we	O
have	O
now	O
completed	O
a	O
rapid	O
tour	O
of	O
a	O
variety	O
of	O
learning	O
algorithms	O
and	O
seen	O
how	O
the	O
method	O
of	O
representing	O
knowledge	O
is	O
crucial	O
in	O
the	O
following	O
ways	O
knowledge	O
representation	O
determines	O
the	O
concepts	O
that	O
an	O
algorithm	O
can	O
and	O
cannot	O
learn	O
knowledge	O
representation	O
affects	O
the	O
speed	B
of	O
learning	O
some	O
representations	O
lend	O
themselves	O
to	O
more	O
efficient	O
implementation	O
than	O
others	O
also	O
the	O
more	O
expressive	O
the	O
language	O
the	O
larger	O
is	O
the	O
search	O
space	O
knowledge	O
representation	O
determines	O
the	O
readability	O
of	O
the	O
concept	B
description	O
a	O
representation	O
that	O
is	O
opaque	O
to	O
the	O
user	O
may	O
allow	O
the	O
program	O
to	O
learn	O
but	O
a	O
representation	O
that	O
is	O
transparent	O
also	O
allows	O
the	O
user	O
to	O
learn	O
thus	O
when	O
approaching	O
a	O
machine	O
learning	O
problem	O
the	O
choice	O
of	O
knowledge	O
representation	O
formalism	O
is	O
just	O
as	O
important	O
as	O
the	O
choice	O
of	O
learning	O
algorithm	O
learning	O
to	O
control	O
dynamic	O
systems	O
tanja	O
urban	O
ci	O
c	O
and	O
ivan	O
bratko	O
jo	O
zef	O
stefan	O
institute	O
and	O
and	O
university	O
of	O
ljubljana	O
introduction	O
the	O
emphasis	O
in	O
controller	B
design	I
has	O
shifted	O
from	O
the	O
precision	O
requirements	O
towards	O
the	O
following	O
objectivesleitch	O
francis	O
enterline	O
verbruggen	O
and	O
astr	O
om	O
astr	O
om	O
sammut	O
michie	O
control	O
without	O
complete	O
prior	O
knowledge	O
extend	O
the	O
range	O
of	O
automatic	O
control	O
applications	O
reliability	O
robustness	O
and	O
adaptivity	O
provide	O
successful	O
performance	O
in	O
the	O
realworld	O
environment	O
transparency	O
of	O
solutions	O
enable	O
understanding	O
and	O
verification	O
generality	O
facilitate	O
the	O
transfer	O
of	O
solutions	O
to	O
similar	O
problems	O
realisation	O
of	O
specified	O
characteristics	O
of	O
system	O
response	O
please	O
customers	O
these	O
problems	O
are	O
tackled	O
in	O
different	O
ways	O
for	O
example	B
by	O
using	O
expert	B
systems	I
neural	B
networks	I
et	O
al	O
hunt	O
et	O
al	O
fuzzy	O
control	O
and	O
genetic	B
algorithms	I
nordvik	O
however	O
in	O
the	O
absence	O
of	O
a	O
complete	O
review	O
and	O
comparative	O
evaluations	O
the	O
decision	O
about	O
how	O
to	O
solve	O
a	O
problem	O
at	O
hand	O
remains	O
a	O
difficult	O
task	O
and	O
is	O
often	O
taken	O
ad	O
hoc	O
leitch	O
has	O
introduced	O
a	O
step	O
towards	O
a	O
systematisation	O
that	O
could	O
provide	O
some	O
guidelines	O
however	O
most	O
of	O
the	O
approaches	O
provide	O
only	O
partial	O
fulfilment	O
of	O
the	O
objectives	B
stated	O
above	O
taking	O
into	O
account	O
also	O
increasing	O
complexity	O
of	O
modern	O
systems	O
together	O
with	O
real-time	O
requirements	O
one	O
must	O
agree	O
with	O
schoppers	O
that	O
designing	O
control	O
means	O
looking	O
for	O
a	O
suitable	O
compromise	O
it	O
should	O
be	O
tailored	O
to	O
the	O
particular	O
problem	O
specifications	O
since	O
some	O
objectives	B
are	O
normally	O
achieved	O
at	O
the	O
cost	O
of	O
some	O
others	O
another	O
important	O
research	O
theme	O
is	O
concerned	O
with	O
the	O
replication	O
of	O
human	O
operators	O
subconscious	O
skill	O
experienced	O
operators	O
manage	O
to	O
control	O
systems	O
that	O
are	O
extremely	O
difficult	O
to	O
be	O
modelled	O
and	O
controlled	O
by	O
classical	O
methods	O
therefore	O
a	O
natural	O
choice	O
would	O
be	O
to	O
mimic	O
such	O
skilful	O
operators	O
one	O
way	O
of	O
doing	O
this	O
is	O
by	O
modelling	O
the	O
address	O
for	O
correspondence	O
jo	O
zef	O
stefan	O
institute	O
univerza	O
v	O
lubljani	O
ljubljana	O
slovenia	O
sec	O
introduction	O
operator	O
s	O
strategy	O
in	O
the	O
form	O
of	O
rules	O
the	O
main	O
problem	O
is	O
how	O
to	O
establish	O
the	O
appropriate	O
set	O
of	O
rules	O
while	O
gaining	O
skill	O
people	O
often	O
lose	O
their	O
awareness	O
of	O
what	O
they	O
are	O
actually	O
doing	O
their	O
knowledge	O
is	O
implicit	O
meaning	O
that	O
it	O
can	O
be	O
demonstrated	O
and	O
observed	O
but	O
hardly	O
ever	O
described	O
explicitly	O
in	O
a	O
way	O
needed	O
for	O
the	O
direct	O
transfer	O
into	O
an	O
automatic	O
controller	O
although	O
the	O
problem	O
is	O
general	O
it	O
is	O
particularly	O
tough	O
in	O
the	O
case	O
of	O
control	O
of	O
fast	O
dynamic	O
systems	O
where	O
subconscious	O
actions	O
are	O
more	O
or	O
less	O
the	O
prevailing	O
form	O
of	O
performance	O
dynamic	O
system	O
learning	O
system	O
control	O
rule	O
partial	O
knowledge	O
dynamic	O
system	O
learning	O
system	O
control	O
rule	O
operator	O
dynamic	O
system	O
learning	O
system	O
control	O
rule	O
fig	O
three	O
modes	O
of	O
learning	O
to	O
control	O
a	O
dynamic	O
system	O
exploiting	O
partial	O
knowledge	O
extracting	O
human	O
operator	O
s	O
skill	O
learning	O
from	O
scratch	O
the	O
aim	O
of	O
this	O
chapter	O
is	O
to	O
show	O
how	O
the	O
methods	O
of	O
machine	O
learning	O
can	O
help	O
in	O
the	O
construction	O
of	O
controllers	O
and	O
in	O
bridging	O
the	O
gap	O
between	O
the	O
subcognitive	O
skill	O
and	O
its	O
machine	O
implementation	O
first	O
successful	O
attempts	O
in	O
learning	O
control	O
treated	O
the	O
controlled	O
system	O
as	O
a	O
black	O
box	O
example	B
michie	O
chambers	O
and	O
a	O
program	O
learnt	O
to	O
control	O
it	O
by	O
trials	O
due	O
to	O
the	O
black	O
box	O
assumption	O
initial	O
control	O
decisions	O
are	O
practically	O
random	O
resulting	O
in	O
very	O
poor	O
performance	O
in	O
the	O
first	O
experiments	O
on	O
the	O
basis	O
of	O
experimental	O
evidence	O
control	O
decisions	O
are	O
evaluated	O
and	O
possibly	O
changed	O
learning	O
takes	O
place	O
until	O
a	O
certain	O
success	O
criterion	O
is	O
met	O
later	O
on	O
this	O
basic	O
idea	O
was	O
implemented	O
in	O
different	O
ways	O
ranging	O
from	O
neural	B
networks	I
example	B
barto	O
learning	O
to	O
control	O
dynamic	O
systems	O
et	O
al	O
anderson	O
to	O
genetic	B
algorithms	I
example	B
odetayo	O
mcgregor	O
recently	O
the	O
research	O
concentrated	O
on	O
removing	O
the	O
deficiencies	O
inherent	O
to	O
these	O
methods	O
like	O
the	O
obscurity	O
and	O
unreliability	O
of	O
the	O
learned	O
control	O
rules	O
sammut	O
michie	O
sammut	O
cribb	O
and	O
time-consuming	O
experimentation	O
while	O
still	O
presuming	O
no	O
prior	O
knowledge	O
until	O
recently	O
this	O
kind	O
of	O
learning	O
control	O
has	O
remained	O
predominant	O
however	O
some	O
of	O
the	O
mentioned	O
deficiences	O
are	O
closely	O
related	O
to	O
the	O
black	O
box	O
assumption	O
which	O
is	O
hardly	O
ever	O
necessary	O
in	O
such	O
a	O
strict	O
form	O
therefore	O
the	O
latest	O
attempts	O
take	O
advantage	O
of	O
the	O
existing	O
knowledge	O
being	O
explicit	O
and	O
formulated	O
at	O
the	O
symbolic	O
level	O
example	B
urban	O
ci	O
c	O
bratko	O
bratko	O
var	O
sek	O
et	O
al	O
or	O
implicit	O
and	O
observable	O
just	O
as	O
operator	O
s	O
skill	O
et	O
al	O
sammut	O
et	O
al	O
camacho	O
michie	O
michie	O
camacho	O
the	O
structure	O
of	O
the	O
chapter	O
follows	O
this	O
introductory	O
discussion	O
we	O
consider	O
three	O
modes	O
of	O
learning	O
to	O
control	O
a	O
system	O
the	O
three	O
modes	O
illustrated	O
in	O
figure	O
are	O
the	O
learning	O
system	O
learns	O
to	O
control	O
a	O
dynamic	O
system	O
by	O
trial	O
and	O
error	O
without	O
any	O
prior	O
knowledge	O
about	O
the	O
system	O
to	O
be	O
controlled	O
from	O
scratch	O
as	O
in	O
but	O
the	O
learning	O
system	O
exploits	O
some	O
partial	O
explicit	O
knowledge	O
about	O
the	O
dynamic	O
system	O
the	O
learning	O
system	O
observes	O
a	O
human	O
operator	O
and	O
learns	O
to	O
replicate	O
the	O
operator	O
s	O
skill	O
experiments	O
in	O
learning	O
to	O
control	O
are	O
popularly	O
carried	O
out	O
using	O
the	O
task	O
of	O
controlling	O
the	O
pole-and-cart	O
system	O
in	O
section	O
we	O
therefore	O
describe	O
this	O
experimental	O
domain	O
sections	O
and	O
describe	O
two	O
approaches	O
to	O
learning	O
from	O
scratch	O
boxes	B
and	O
genetic	B
learning	O
in	O
section	O
the	O
learning	O
system	O
exploits	O
partial	O
explicit	O
knowledge	O
in	O
section	O
the	O
learning	O
system	O
exploits	O
the	O
operator	O
s	O
skill	O
experimental	O
domain	O
the	O
main	O
ideas	O
presented	O
in	O
this	O
chapter	O
will	O
be	O
illustrated	O
by	O
using	O
the	O
pole	B
balancing	I
problem	O
miller	O
as	O
a	O
case	O
study	O
so	O
let	O
us	O
start	O
with	O
a	O
description	O
of	O
this	O
control	O
task	O
which	O
has	O
often	O
been	O
chosen	O
to	O
demonstrate	O
both	O
classical	O
and	O
nonconventional	O
control	O
techniques	O
besides	O
being	O
an	O
attractive	O
benchmark	O
it	O
also	O
bears	O
similarities	O
with	O
tasks	O
of	O
significant	O
practical	O
importance	O
such	O
as	O
two-legged	O
walking	O
and	O
satellite	O
attitude	O
control	O
michie	O
the	O
system	O
consists	O
of	O
a	O
rigid	O
pole	O
and	O
a	O
cart	B
the	O
cart	B
can	O
move	O
left	O
and	O
right	O
on	O
a	O
bounded	O
track	O
the	O
pole	O
is	O
hinged	O
to	O
the	O
top	O
of	O
the	O
cart	B
so	O
that	O
it	O
can	O
swing	O
in	O
the	O
vertical	O
plane	O
in	O
the	O
ai	O
literature	O
the	O
task	O
is	O
usually	O
just	O
to	O
prevent	O
the	O
pole	O
from	O
falling	O
and	O
to	O
keep	O
the	O
cart	B
position	O
within	O
the	O
specified	O
limits	O
while	O
the	O
control	O
regime	O
is	O
that	O
of	O
bang-bang	O
the	O
control	O
force	O
has	O
a	O
fixed	O
magnitude	O
and	O
all	O
the	O
controller	O
can	O
do	O
is	O
to	O
change	O
the	O
force	O
direction	O
in	O
regular	O
time	B
intervals	O
classical	O
methods	O
example	B
kwakernaak	O
sivan	O
can	O
be	O
applied	O
to	O
controlling	O
the	O
system	O
under	O
several	O
assumptions	O
including	O
complete	O
knowledge	O
about	O
the	O
system	O
that	O
is	O
a	O
differential	O
equations	O
model	O
up	O
to	O
numerical	O
values	O
of	O
its	O
parameters	O
alternative	O
approaches	O
tend	O
to	O
weaken	O
these	O
assumptions	O
by	O
constructing	O
control	O
rules	O
in	O
two	O
essentially	O
different	O
ways	O
by	O
learning	O
from	O
experience	O
and	O
by	O
qualitative	O
reasoning	O
the	O
first	O
one	O
will	O
be	O
presented	O
in	O
more	O
detail	O
later	O
in	O
this	O
chapter	O
the	O
second	O
one	O
will	O
be	O
described	O
here	O
only	O
up	O
to	O
the	O
level	O
needed	O
for	O
comparison	O
and	O
understanding	O
giving	O
a	O
general	O
idea	O
about	O
two	O
solutions	O
of	O
this	O
kind	O
sec	O
experimental	O
domain	O
critical	O
critical	O
critical	O
critical	O
left	O
left	O
left	O
left	O
critical	O
critical	O
critical	O
critical	O
right	O
right	O
right	O
right	O
fig	O
makarovi	O
c	O
s	O
rule	O
for	O
pole	B
balancing	I
a	O
solution	O
distinguished	O
by	O
its	O
simplicity	O
was	O
derived	O
by	O
makarovi	O
c	O
figure	O
rules	O
of	O
the	O
same	O
tree	O
structure	O
but	O
with	O
the	O
state	O
variables	O
ordered	O
in	O
different	O
ways	O
were	O
experimentally	O
studied	O
by	O
d	O
zeroski	O
he	O
showed	O
that	O
no	O
less	O
than	O
seven	O
permutations	O
of	O
state	O
variables	O
yielded	O
successful	O
control	O
rules	O
we	O
is	O
a	O
permutation	O
of	O
the	O
variables	O
determining	O
their	O
top-down	O
order	O
another	O
solution	O
was	O
inferred	O
by	O
bratko	O
from	O
a	O
very	O
simple	O
qualitative	O
model	O
of	O
the	O
inverted	O
pendulum	O
system	O
the	O
derived	O
control	O
rule	O
is	O
described	O
by	O
the	O
following	O
denote	O
such	O
rules	O
as	O
where	O
relations	O
ref	O
goal	O
ref	O
goal	O
goal	O
ref	O
and	O
where	O
values	O
required	O
for	O
successful	O
control	O
and	O
goal	O
ref	O
denote	O
reference	O
values	O
to	O
be	O
reached	O
ref	O
goal	O
and	O
goal	O
denote	O
goal	O
denotes	O
a	O
monotonically	O
increasing	O
learning	O
to	O
control	O
dynamic	O
systems	O
be	O
simplified	O
and	O
normalised	O
resulting	O
in	O
function	O
passing	O
through	O
the	O
point-	O
when	O
the	O
system	O
is	O
to	O
be	O
controlled	O
under	O
the	O
bang-bang	O
regime	O
control	O
action	O
is	O
determined	O
by	O
the	O
sign	O
of	O
force	O
assuming	O
without	O
loss	O
of	O
generality	O
equations	O
can	O
where	O
if	O
and	O
ref	O
sign	O
are	O
numerical	O
parameters	O
both	O
makarovi	O
c	O
s	O
and	O
bratko	O
s	O
rule	O
successfully	O
control	O
the	O
inverted	O
pendulum	O
provided	O
the	O
appropriate	O
values	O
of	O
the	O
numerical	O
parameters	O
are	O
chosen	O
moreover	O
there	O
exists	O
a	O
set	O
of	O
parameter	O
values	O
that	O
makes	O
bratko	O
s	O
rule	O
equivalent	O
to	O
the	O
bang-bang	O
variant	O
of	O
a	O
classical	O
control	O
rule	O
using	O
the	O
sign	O
of	O
pole-placement	O
controller	O
output	B
zeroski	O
then	O
else	O
learning	O
to	O
control	O
from	O
scratch	O
boxes	B
in	O
learning	O
approaches	O
trials	O
are	O
performed	O
in	O
order	O
to	O
gain	O
experimental	O
evidence	O
about	O
different	O
control	O
decisions	O
a	O
trial	O
starts	O
with	O
the	O
system	O
positioned	O
in	O
an	O
initial	O
state	O
chosen	O
from	O
a	O
specified	O
region	O
and	O
lasts	O
until	O
failure	O
occurs	O
or	O
successful	O
control	O
is	O
performed	O
for	O
a	O
prescribed	O
maximal	O
period	O
of	O
time	B
failure	O
occurs	O
when	O
the	O
cart	B
position	O
or	O
pole	O
inclination	O
exceeds	O
the	O
given	O
boundaries	O
the	O
duration	O
of	O
a	O
trial	O
is	O
called	O
survival	O
time	B
learning	O
is	O
carried	O
out	O
by	O
performing	O
trials	O
repeatedly	O
until	O
a	O
certain	O
success	O
criterion	O
is	O
met	O
typically	O
this	O
criterion	O
requires	O
successful	O
control	O
within	O
a	O
trial	O
to	O
exceed	O
a	O
prescribed	O
period	O
of	O
time	B
initial	O
control	O
decisions	O
are	O
usually	O
random	O
on	O
the	O
basis	O
of	O
experimental	O
evidence	O
they	O
are	O
evaluated	O
and	O
possibly	O
changed	O
thus	O
improving	O
control	O
quality	O
this	O
basic	O
idea	O
has	O
been	O
implemented	O
in	O
many	O
different	O
ways	O
for	O
example	B
in	O
boxes	B
chambers	O
adaptive	O
critic	O
reinforcement	O
method	O
et	O
al	O
cart	B
utgoff	O
multilayer	O
connectionist	O
approach	O
and	O
many	O
others	O
geva	O
and	O
sitte	O
provide	O
an	O
exhaustive	O
review	O
here	O
two	O
methods	O
will	O
be	O
described	O
in	O
more	O
detail	O
boxes	B
chambers	O
and	O
genetic	B
learning	O
of	O
control	O
sek	O
et	O
al	O
the	O
choice	O
of	O
methods	O
presented	O
here	O
is	O
subjective	O
it	O
was	O
guided	O
by	O
our	O
aim	O
to	O
describe	O
recent	O
efforts	O
in	O
changing	O
or	O
upgrading	O
the	O
original	O
ideas	O
we	O
chose	O
boxes	B
because	O
it	O
introduced	O
a	O
learning	O
scheme	O
that	O
was	O
inspirational	O
to	O
much	O
further	O
work	O
boxes	B
the	O
boxes	B
program	O
chambers	O
learns	O
a	O
state-action	O
table	O
i	O
e	O
a	O
set	O
of	O
rules	O
that	O
specify	O
action	O
to	O
be	O
applied	O
to	O
the	O
system	O
in	O
a	O
given	O
state	O
of	O
course	O
this	O
would	O
not	O
be	O
possible	O
for	O
the	O
original	O
infinite	O
state	O
space	O
therefore	O
the	O
state	O
space	O
is	O
divided	O
into	O
boxes	B
a	O
box	O
is	O
defined	O
as	O
the	O
cartesian	O
product	O
of	O
the	O
values	O
of	O
the	O
system	O
variables	O
where	O
all	O
the	O
values	O
belong	O
to	O
an	O
interval	O
from	O
a	O
predefined	O
partition	O
a	O
typical	O
giving	O
boxes	B
all	O
the	O
points	O
within	O
a	O
box	O
are	O
mapped	O
to	O
the	O
same	O
control	O
decision	O
during	O
one	O
trial	O
the	O
state-action	O
table	O
is	O
fixed	O
when	O
a	O
failure	O
is	O
detected	O
a	O
trial	O
ends	O
decisions	O
are	O
evaluated	O
with	O
respect	O
to	O
the	O
accumulated	O
numeric	O
information	O
partition	O
of	O
the	O
four	O
dimensional	O
state	O
space	O
into	O
boxes	B
distinguish	O
values	O
of	O
of	O
and	O
of	O
of	O
sec	O
learning	O
to	O
control	O
from	O
scratch	O
boxes	B
how	O
many	O
times	O
the	O
system	O
entered	O
a	O
particular	O
state	O
how	O
successful	O
it	O
was	O
after	O
particular	O
decisions	O
etc	O
the	O
following	O
information	O
is	O
accumulated	O
for	O
each	O
box	O
left	O
life	O
weighted	O
sum	O
of	O
survival	O
times	O
after	O
left	O
decision	O
was	O
taken	O
in	O
this	O
state	O
right	O
life	O
the	O
same	O
for	O
the	O
right	O
decision	O
left	O
usage	O
weighted	O
sum	O
of	O
the	O
number	O
of	O
left	O
decisions	O
taken	O
in	O
this	O
state	O
during	O
previous	O
trials	O
during	O
previous	O
trials	O
ded	O
fgd	O
dih	O
fh	O
trial	O
the	O
program	O
chooses	O
the	O
action	O
with	O
the	O
higher	O
estimate	O
to	O
be	O
applied	O
in	O
the	O
box	O
during	O
the	O
performance	O
of	O
boxes	B
is	O
generally	O
described	O
by	O
the	O
number	O
of	O
trials	O
needed	O
for	O
first	O
achieving	O
step	O
survival	O
figures	O
vary	O
considerably	O
from	O
paper	O
to	O
paper	O
and	O
are	O
between	O
sitte	O
and	O
although	O
interesting	O
these	O
figures	O
are	O
not	O
sufficient	O
to	O
validate	O
the	O
learning	O
results	O
reliability	O
robustness	O
and	O
characteristics	O
of	O
the	O
controller	O
performance	O
are	O
important	O
as	O
well	O
and	O
are	O
discussed	O
in	O
many	O
papers	O
devoted	O
to	O
boxes	B
times	O
steps	O
at	O
which	O
the	O
system	O
enters	O
this	O
state	O
during	O
the	O
current	O
after	O
a	O
trial	O
the	O
program	O
updates	O
these	O
figures	O
for	O
the	O
states	O
in	O
which	O
decision	O
left	O
was	O
taken	O
the	O
new	O
values	O
are	O
right	O
usage	O
the	O
same	O
for	O
right	O
decisions	O
where	O
the	O
meaning	O
of	O
the	O
parameters	O
is	O
as	O
follows	O
number	O
of	O
entries	O
into	O
the	O
state	O
during	O
the	O
run	O
dlder-sut	O
dzhr-sut	O
fgder	O
sut	O
fhr	O
sut	O
dldnmpoq	O
dzhmpoq	O
fgdnmo	O
q	O
mpoq	O
fh	O
sut	O
constant	O
that	O
weighs	O
recent	O
experience	O
relative	O
to	O
earlier	O
experience	O
for	O
the	O
whole	O
systemb	O
global	O
life	O
andb	O
mpo	O
q	O
der	O
sut	O
each	O
trialb	O
mo	O
q	O
ofbg	O
ced	O
dih	O
ci	O
fh	O
analogous	O
updates	O
are	O
made	O
for	O
the	O
states	O
with	O
decision	O
right	O
is	O
constant	O
that	O
weighs	O
global	O
experience	O
relative	O
to	O
local	O
experience	O
deh	O
dlh	O
finishing	O
time	B
of	O
the	O
trial	O
dld	O
fgd	O
wheret	O
the	O
next	O
trial	O
these	O
values	O
are	O
used	O
for	O
a	O
numeric	O
evaluation	O
of	O
the	O
success	O
for	O
both	O
actions	O
the	O
estimates	O
are	O
computed	O
after	O
a	O
trial	O
for	O
each	O
qualitative	O
state	O
suta	O
global	O
usage	O
are	O
computed	O
after	O
j	O
j	O
k	O
v	O
j	O
j	O
y	O
d	O
h	O
d	O
b	O
j	O
y	O
b	O
h	O
b	O
t	O
b	O
h	O
t	O
g	O
t	O
b	O
h	O
t	O
learning	O
to	O
control	O
dynamic	O
systems	O
variablesded	O
action	O
lifetimedih	O
refinements	O
of	O
boxes	B
sammut	O
describes	O
some	O
recent	O
refinements	O
of	O
the	O
basic	O
michie-chambers	O
learning	O
scheme	O
the	O
central	O
mechanism	O
of	O
learning	O
in	O
boxes	B
is	O
the	O
decision	O
rule	O
based	O
on	O
the	O
experience	O
of	O
each	O
box	O
the	O
experience	O
for	O
each	O
individual	O
box	O
is	O
accumulated	O
in	O
the	O
for	O
the	O
right	O
action	O
the	O
michie-chambers	O
rule	O
determines	O
the	O
decision	O
between	O
left	O
and	O
right	O
action	O
depending	O
on	O
these	O
variables	O
the	O
rule	O
is	O
designed	O
so	O
that	O
it	O
combines	O
two	O
possibly	O
conflicting	O
interests	O
exploitation	O
and	O
exploration	O
the	O
first	O
is	O
to	O
perform	O
the	O
action	O
that	O
in	O
the	O
past	O
produced	O
the	O
best	O
results	O
is	O
maximum	O
lifetime	O
and	O
the	O
second	O
is	O
to	O
explore	O
the	O
alternatives	O
the	O
alternatives	O
may	O
in	O
the	O
future	O
turn	O
out	O
in	O
fact	O
to	O
be	O
superior	O
to	O
what	O
appears	O
to	O
be	O
best	O
at	O
present	O
action	O
usagefgd	O
andfh	O
the	O
original	O
michie-chambers	O
formulas	O
find	O
a	O
particular	O
compromise	O
between	O
these	O
two	O
interests	O
the	O
compromise	O
can	O
be	O
adjusted	O
by	O
varying	O
the	O
parameters	O
in	O
the	O
formulas	O
sammut	O
describes	O
a	O
series	O
of	O
modifications	O
of	O
the	O
original	O
michie-chambers	O
rule	O
the	O
following	O
elegant	O
rule	O
after	O
law	O
sammut	O
experimentally	O
performed	O
the	O
best	O
in	O
terms	O
of	O
learning	O
rate	O
and	O
stability	O
of	O
learning	O
fdlh-fhn	O
dedeh	O
dihn	O
if	O
an	O
action	O
has	O
not	O
been	O
tested	O
then	O
choose	O
that	O
action	O
is	O
a	O
user	O
defined	O
parameter	O
that	O
adjusts	O
the	O
relative	O
importance	O
of	O
exploitation	O
and	O
is	O
this	O
corresponds	O
to	O
pure	O
exploitation	O
the	O
system	O
s	O
mentality	O
changes	O
towards	O
experimentalist	O
then	O
the	O
system	O
is	O
willing	O
to	O
experiment	O
with	O
actions	O
that	O
from	O
past	O
experience	O
look	O
inferior	O
else	O
ifdedeh-dzhn	O
else	O
iffdlh-fhn	O
exploration	O
the	O
lowest	O
reasonable	O
value	O
fort	O
without	O
any	O
desire	O
to	O
explore	O
the	O
untested	O
by	O
increasingt	O
a	O
suitable	O
compromise	O
fort	O
pole-and-cart	O
problem	O
it	O
was	O
experimentally	O
found	O
thatt	O
rate	O
is	O
relatively	O
stable	O
for	O
values	O
oft	O
whereas	O
the	O
law	O
sammut	O
rule	O
needed	O
trials	O
in	O
trying	O
to	O
test	O
the	O
stability	O
of	O
the	O
law	O
sammut	O
rule	O
it	O
was	O
found	O
thatt	O
was	O
slightly	O
but	O
not	O
significantly	O
is	O
needed	O
for	O
overall	O
good	O
performance	O
for	O
the	O
classical	O
is	O
optimal	O
the	O
learning	O
between	O
and	O
and	O
it	O
degrades	O
rapidly	O
when	O
decreases	O
below	O
or	O
increases	O
above	O
the	O
following	O
improvement	O
of	O
the	O
law	O
sammut	O
rule	O
with	O
respect	O
to	O
the	O
michie	O
chambers	O
rule	O
was	O
reported	O
on	O
the	O
average	O
over	O
experiments	O
the	O
original	O
boxes	B
needed	O
trials	O
to	O
learn	O
to	O
control	O
the	O
system	O
sensitive	O
to	O
small	O
changes	O
in	O
the	O
learning	O
problem	O
such	O
as	O
changing	O
the	O
number	O
of	O
boxes	B
from	O
to	O
or	O
introducing	O
asymmetry	O
in	O
the	O
force	O
push	O
twice	O
the	O
right	O
push	O
else	O
choose	O
an	O
action	O
at	O
random	O
then	O
choose	O
left	O
then	O
choose	O
right	O
geva	O
and	O
sitte	O
carried	O
out	O
exhaustive	O
experiments	O
concerning	O
the	O
same	O
topic	O
with	O
the	O
appropriate	O
parameter	O
setting	O
the	O
boxes	B
method	O
performed	O
as	O
well	O
as	O
the	O
adaptive	O
critic	O
reinforcement	O
learning	O
et	O
al	O
they	O
got	O
an	O
average	O
of	O
trials	O
out	O
of	O
learning	O
experiments	O
deviation	O
was	O
learning	O
to	O
control	O
from	O
scratch	O
genetic	B
learning	O
genetic	B
algorithms	I
are	O
loosely	O
based	O
on	O
darwinian	O
principles	O
of	O
evolution	O
reproduction	O
genetic	B
recombination	O
and	O
the	O
survival	O
of	O
the	O
fittest	O
goldberg	O
they	O
maintain	O
a	O
set	O
of	O
candidate	O
solutions	O
called	O
a	O
population	O
candidate	O
solutions	O
t	O
o	O
t	O
sec	O
learning	O
to	O
control	O
from	O
scratch	O
genetic	B
learning	O
are	O
usually	O
represented	O
as	O
binary	O
coded	O
strings	O
of	O
fixed	O
length	O
the	O
initial	O
population	O
is	O
generated	O
at	O
random	O
what	O
happens	O
during	O
cycles	O
called	O
generations	O
is	O
as	O
follows	O
each	O
member	O
of	O
the	O
population	O
is	O
evaluated	O
using	O
a	O
fitness	O
function	O
after	O
that	O
the	O
population	O
undergoes	O
reproduction	O
parents	O
are	O
chosen	O
stochastically	O
but	O
strings	O
with	O
a	O
higher	O
value	O
of	O
fitness	O
function	O
have	O
higher	O
probability	O
of	O
contributing	O
an	O
offspring	O
genetic	B
operators	O
such	O
as	O
crossover	O
and	O
mutation	O
are	O
applied	O
to	O
parents	O
to	O
produce	O
offspring	O
a	O
subset	O
of	O
the	O
population	O
is	O
replaced	O
by	O
the	O
offspring	O
and	O
the	O
process	O
continues	O
on	O
this	O
new	O
generation	O
through	O
recombination	O
and	O
selection	O
the	O
evolution	O
converges	O
to	O
highly	O
fit	O
population	O
members	O
representing	O
near-optimal	O
solutions	O
to	O
the	O
considered	O
problem	O
when	O
controllers	O
are	O
to	O
be	O
built	O
without	O
having	O
an	O
accurate	O
mathematical	O
model	O
of	O
the	O
system	O
to	O
be	O
controlled	O
two	O
problems	O
arise	O
first	O
how	O
to	O
establish	O
the	O
structure	O
of	O
the	O
controller	O
and	O
second	O
how	O
to	O
choose	O
numerical	O
values	O
for	O
the	O
controller	O
parameters	O
in	O
the	O
following	O
we	O
present	O
a	O
three-stage	O
framework	O
proposed	O
by	O
var	O
sek	O
et	O
al	O
cq	O
ieee	O
first	O
control	O
rules	O
represented	O
as	O
tables	O
are	O
obtained	O
without	O
prior	O
knowledge	O
about	O
the	O
system	O
to	O
be	O
controlled	O
next	O
if-then	O
rules	O
are	O
synthesized	O
by	O
structuring	O
information	O
encoded	O
in	O
the	O
tables	O
yielding	O
comprehensible	O
control	O
knowledge	O
this	O
control	O
knowledge	O
has	O
adequate	O
structure	O
but	O
it	O
may	O
be	O
non-operational	O
because	O
of	O
inadequate	O
settings	O
of	O
its	O
numerical	O
parameters	O
control	O
knowledge	O
is	O
finally	O
made	O
operational	O
by	O
fine-tuning	O
numerical	O
parameters	O
that	O
are	O
part	O
of	O
this	O
knowledge	O
the	O
same	O
fine-tuning	O
mechanism	O
can	O
also	O
be	O
applied	O
when	O
available	O
partial	O
domain	B
knowledge	I
suffices	O
to	O
determine	O
the	O
structure	O
of	O
a	O
control	O
rule	O
in	O
advance	O
in	O
this	O
approach	O
the	O
control	O
learning	O
process	O
is	O
considered	O
to	O
be	O
an	O
instance	O
of	O
a	O
combinatorial	O
optimisation	B
problem	O
in	O
contrast	O
to	O
the	O
previously	O
described	O
learning	O
approach	O
in	O
boxes	B
where	O
the	O
goal	O
is	O
to	O
maximise	O
survival	O
time	B
here	O
the	O
goal	O
is	O
to	O
maximise	O
survival	O
time	B
and	O
simultaneously	O
to	O
minimise	O
the	O
discrepancy	O
between	O
the	O
desired	O
and	O
actual	O
system	O
behaviour	O
this	O
criterion	O
is	O
embodied	O
in	O
a	O
cost	O
function	O
called	O
the	O
raw	O
fitness	O
function	O
used	O
to	O
evaluate	O
candidate	O
control	O
rules	O
during	O
the	O
learning	O
process	O
y	O
max	O
mw	O
is	O
calculated	O
as	O
follows	O
raw	O
fitnessrtsvup	O
max	O
is	O
the	O
normalised	O
survival	O
timez	O
is	O
the	O
normalised	O
error	O
wherex	O
trials	O
performed	O
to	O
evaluate	O
a	O
candidate	O
solutiony	O
is	O
the	O
survival	O
time	B
in	O
the	O
trial	O
y	O
max	O
is	O
the	O
maximal	O
duration	O
of	O
a	O
trial	O
and	O
is	O
the	O
cumulative	O
error	O
of	O
the	O
trial	O
after	O
completing	O
the	O
learning	O
process	O
solutions	O
were	O
thoroughly	O
evaluated	O
by	O
performing	O
trials	O
with	O
maximal	O
duration	O
of	O
a	O
trial	O
set	O
to	O
steps	O
corresponding	O
to	O
over	O
hours	O
of	O
simulated	O
time	B
note	O
that	O
the	O
maximal	O
duration	O
of	O
a	O
trial	O
most	O
frequently	O
found	O
in	O
the	O
ai	O
literature	O
is	O
seconds	O
is	O
the	O
number	O
of	O
r	O
x	O
y	O
x	O
y	O
k	O
v	O
w	O
y	O
z	O
k	O
v	O
w	O
y	O
v	O
y	O
learning	O
to	O
control	O
dynamic	O
systems	O
phase	O
obtaining	O
control	O
without	O
prior	O
knowledge	O
and	O
during	O
this	O
phase	O
boxes-like	O
decision	O
rules	O
were	O
learned	O
for	O
each	O
of	O
the	O
pole-cart	O
variables	O
each	O
decision	O
rule	O
is	O
then	O
represented	O
as	O
a	O
four-dimensional	O
array	O
where	O
each	O
the	O
domain	O
is	O
partitioned	O
into	O
three	O
labelled	O
entry	O
represents	O
a	O
control	O
action	O
in	O
addition	O
two	O
partitioning	O
thresholds	O
are	O
required	O
for	O
each	O
system	O
variable	O
candidate	O
solutions	O
comprising	O
a	O
decision	O
rule	O
along	O
with	O
the	O
corresponding	O
thresholds	O
are	O
represented	O
as	O
binary	O
strings	O
to	O
calculate	O
a	O
fitness	O
value	O
for	O
each	O
individual	O
trials	O
were	O
carried	O
out	O
with	O
the	O
maximal	O
duration	O
of	O
a	O
trial	O
set	O
to	O
steps	O
populations	O
of	O
size	O
were	O
observed	O
for	O
generations	O
the	O
experiment	O
was	O
repeated	O
ten	O
times	O
on	O
average	O
after	O
about	O
generations	O
individualsrepresenting	O
rules	O
better	O
than	O
makarovi	O
c	O
s	O
phase	O
inducing	O
rule	O
structure	O
discovered	O
rule	O
were	O
action	O
positive	O
or	O
negative	O
control	O
force	O
the	O
obtained	O
rules	O
are	O
very	O
close	O
in	O
form	O
to	O
makarovi	O
c	O
s	O
rule	O
from	O
the	O
rules	O
shown	O
to	O
automatically	O
synthesize	O
comprehensible	O
rules	O
obtained	O
during	O
phase	O
an	O
inductive	B
learning	I
technique	O
was	O
employed	O
a	O
derivative	O
of	O
the	O
algorithm	O
niblett	O
named	O
ginesys	O
pc	O
c	O
gams	O
was	O
used	O
to	O
compress	O
the	O
ga-induced	O
boxes-like	O
rules	O
into	O
the	O
if-then	O
form	O
the	O
learning	O
domain	O
for	O
the	O
compression	O
phase	O
was	O
described	O
in	O
terms	O
of	O
four	O
attributes	B
and	O
the	O
class	B
the	O
attribute	O
values	O
were	O
interval	O
and	O
the	O
class	B
represented	O
the	O
corresponding	O
labels	O
for	O
the	O
pole-cart	O
variables	O
by	O
d	O
zeroski	O
to	O
successfully	O
control	O
the	O
pole-cart	O
system	O
rules	O
and	O
were	O
discovered	O
automatically	O
the	O
performance	O
of	O
the	O
compressed	O
rules	O
decreased	O
with	O
respect	O
to	O
the	O
original	O
ga-induced	O
boxes-like	O
rules	O
due	O
to	O
inaccurate	O
interpretation	O
of	O
the	O
interval	O
labels	O
as	O
in	O
the	O
case	O
of	O
table	O
the	O
failure	O
rate	O
of	O
the	O
compressed	O
rule	O
indicates	O
that	O
this	O
rule	O
was	O
never	O
able	O
to	O
balance	O
the	O
system	O
for	O
steps	O
since	O
the	O
defining	O
thresholds	O
were	O
learned	O
during	O
phase	O
to	O
perform	O
well	O
with	O
the	O
original	O
ga-induced	O
rules	O
these	O
thresholds	O
should	O
be	O
adapted	O
to	O
suit	O
the	O
new	O
compressed	O
rules	O
and	O
phase	O
fine-tuning	O
by	O
optimizing	O
control	O
performance	O
in	O
phase	O
the	O
interpretation	O
of	O
symbolic	O
values	O
i	O
e	O
and	O
left	O
unchanged	O
throughout	O
the	O
optimisation	B
process	O
interval	O
labels	O
appearing	O
in	O
the	O
found	O
in	O
phase	O
was	O
adjusted	O
to	O
maximise	O
the	O
control	O
quality	O
for	O
this	O
purpose	O
a	O
ga	O
was	O
employed	O
again	O
this	O
time	B
each	O
chromosome	O
qualitative	O
rule	O
represented	O
four	O
binary	O
coded	O
thresholds	O
while	O
the	O
rule	O
structure	O
was	O
set	O
to	O
to	O
calculate	O
a	O
fitness	O
value	O
for	O
each	O
individual	O
only	O
trials	O
were	O
carried	O
out	O
with	O
maximal	O
duration	O
of	O
a	O
trial	O
set	O
to	O
steps	O
populations	O
of	O
size	O
were	O
evolved	O
for	O
generations	O
after	O
generations	O
individuals	O
representing	O
rules	O
better	O
than	O
those	O
obtained	O
during	O
phase	O
were	O
generated	O
through	O
the	O
extensive	O
evaluation	O
the	O
fine-tuned	O
rules	O
were	O
shown	O
reliable	O
results	O
in	O
table	O
additional	O
experiments	O
were	O
carried	O
out	O
the	O
robustness	O
of	O
learning	O
from	O
scratch	O
was	O
robustness	O
and	O
adaptation	O
tested	O
by	O
performing	O
the	O
experiment	O
twice	O
first	O
with	O
force	O
s	O
n	O
n	O
and	O
sec	O
exploiting	O
partial	O
explicit	O
knowledge	O
table	O
cq	O
ieee	O
control	O
performance	O
of	O
ga-induced	O
boxes-like	O
rule	O
compressed	O
rule	O
and	O
the	O
original	O
makarovi	O
c	O
s	O
rule	O
fine-tuned	O
rule	O
failures	O
avg	O
survival	O
rule	O
time	B
ga-based	O
compressed	O
fine-tuned	O
makarovi	O
c	O
s	O
fitness	O
performing	O
two	O
further	O
fine-tuning	O
experiments	O
second	O
with	O
asymmetrical	O
force	O
n	O
the	O
possibility	O
of	O
adaptation	O
of	O
s	O
n	O
obtained	O
for	O
symmetrical	O
force	O
s	O
the	O
qualitative	O
rule	O
n	O
n	O
and	O
the	O
new	O
conditions	O
n	O
was	O
examined	O
by	O
n	O
n	O
s	O
table	O
cq	O
ieee	O
control	O
performance	O
of	O
ga-induced	O
boxes-like	O
rules	O
for	O
n	O
and	O
rule	O
n	O
s	O
fine-tuned	O
for	O
n	O
s	O
n	O
table	O
shows	O
the	O
performance	O
of	O
four	O
rules	O
obtained	O
in	O
these	O
experiments	O
it	O
can	O
be	O
seen	O
that	O
gas	O
can	O
successfully	O
learn	O
to	O
control	O
the	O
pole-cart	O
system	O
also	O
under	O
modified	O
conditions	O
s	O
n	O
and	O
n	O
and	O
failures	O
avg	O
survival	O
n	O
n	O
s	O
s	O
n	O
to	O
rule	O
time	B
fitness	O
to	O
summarise	O
successful	O
and	O
comprehensible	O
control	O
rules	O
were	O
synthesized	O
automatically	O
in	O
three	O
phases	O
here	O
a	O
remark	O
should	O
be	O
made	O
about	O
the	O
number	O
of	O
performed	O
trials	O
in	O
this	O
research	O
it	O
was	O
very	O
high	O
due	O
to	O
the	O
following	O
reasons	O
first	O
the	O
emphasis	O
was	O
put	O
on	O
the	O
reliability	O
of	O
learned	O
rules	O
and	O
this	O
of	O
course	O
demands	O
much	O
more	O
experimentation	O
in	O
order	O
to	O
ensure	O
good	O
performance	O
on	O
a	O
wide	O
range	O
of	O
initial	O
states	O
in	O
our	O
recent	O
experiments	O
with	O
a	O
more	O
narrow	O
range	O
of	O
initial	O
states	O
the	O
number	O
of	O
trials	O
was	O
considerably	O
reduced	O
without	O
affecting	O
the	O
reliability	O
second	O
the	O
performance	O
of	O
the	O
rules	O
after	O
the	O
first	O
phase	O
was	O
practically	O
the	O
same	O
as	O
that	O
of	O
the	O
rules	O
after	O
the	O
third	O
phase	O
maybe	O
the	O
same	O
controller	O
structure	O
could	O
be	O
obtained	O
in	O
the	O
second	O
phase	O
from	O
less	O
perfect	O
rules	O
however	O
it	O
is	O
difficult	O
to	O
know	O
when	O
the	O
learned	O
evidence	O
suffices	O
to	O
conclude	O
the	O
exhaustiveness	O
of	O
these	O
experiments	O
was	O
conciously	O
accepted	O
by	O
the	O
authors	O
in	O
order	O
to	O
show	O
that	O
reliable	O
rules	O
can	O
be	O
learned	O
from	O
scratch	O
exploiting	O
partial	O
explicit	O
knowledge	O
boxes	B
with	O
partial	O
knowledge	O
to	O
see	O
how	O
adding	O
domain	B
knowledge	I
affects	O
speed	B
and	O
results	O
of	O
learning	O
three	O
series	O
of	O
experiments	O
were	O
done	O
by	O
urban	O
ci	O
c	O
bratko	O
the	O
following	O
variants	O
of	O
learning	O
control	O
rules	O
with	O
program	O
boxes	B
were	O
explored	O
learning	O
to	O
control	O
dynamic	O
systems	O
a	O
without	O
domain	B
knowledge	I
b	O
with	O
partial	O
domain	B
knowledge	I
considered	O
as	O
definitely	O
correct	O
and	O
c	O
with	O
partial	O
initial	O
domain	B
knowledge	I
allowed	O
to	O
be	O
changed	O
during	O
learning	O
the	O
following	O
rule	O
served	O
as	O
partial	O
domain	B
knowledge	I
if	O
else	O
if	O
d	O
then	O
action	O
right	O
d	O
then	O
action	O
left	O
although	O
the	O
rule	O
alone	O
is	O
not	O
effective	O
at	O
all	O
survival	O
was	O
steps	O
it	O
considerably	O
decreased	O
the	O
number	O
of	O
trials	O
needed	O
for	O
achieving	O
survival	O
time	B
steps	O
at	O
the	O
same	O
time	B
the	O
reliability	O
the	O
percentage	O
of	O
trials	O
with	O
the	O
learned	O
state-action	O
table	O
surviving	O
more	O
than	O
simulation	O
steps	O
increased	O
from	O
to	O
more	O
detailed	O
description	O
of	O
the	O
experiments	O
is	O
available	O
in	O
urban	O
ci	O
c	O
bratko	O
table	O
experimental	O
results	O
showing	O
the	O
influence	O
of	O
partial	O
knowledge	O
version	O
length	O
of	O
learning	O
av	O
reliability	O
av	O
survival	O
num	O
of	O
trials	O
a	O
b	O
c	O
exploiting	O
domain	B
knowledge	I
in	O
genetic	B
learning	O
of	O
control	O
domain	B
knowledge	I
can	O
be	O
exploited	O
to	O
bypass	O
the	O
costly	O
process	O
of	O
learning	O
a	O
control	O
rule	O
from	O
scratch	O
instead	O
of	O
searching	O
for	O
both	O
the	O
structure	O
of	O
a	O
rule	O
and	O
the	O
values	O
of	O
numerical	O
parameters	O
required	O
by	O
the	O
rule	O
we	O
can	O
start	O
with	O
a	O
known	O
rule	O
structure	O
derived	O
by	O
bratko	O
from	O
a	O
qualitative	O
model	O
of	O
pole	O
and	O
cart	B
then	O
we	O
employ	O
a	O
ga	O
to	O
tune	O
the	O
parameters	O
and	O
appearing	O
in	O
the	O
rule	O
to	O
calculate	O
a	O
fitness	O
value	O
of	O
an	O
individual	O
trials	O
were	O
carried	O
out	O
with	O
maximal	O
duration	O
of	O
a	O
trial	O
set	O
to	O
steps	O
corresponding	O
to	O
seconds	O
of	O
simulated	O
time	B
populations	O
of	O
size	O
were	O
evolved	O
for	O
generations	O
the	O
ga	O
was	O
run	O
times	O
in	O
all	O
the	O
runs	O
the	O
parameter	O
settings	O
that	O
ensured	O
maximal	O
survival	O
of	O
the	O
system	O
for	O
all	O
initial	O
states	O
were	O
found	O
table	O
gives	O
the	O
best	O
three	O
obtained	O
parameter	O
settings	O
along	O
with	O
their	O
fitness	O
values	O
n	O
the	O
parameter	O
tuning	O
and	O
evaluation	O
procedures	O
were	O
repeated	O
identically	O
for	O
two	O
modified	O
versions	O
of	O
the	O
pole-cart	O
system	O
one	O
being	O
controlled	O
with	O
symmetrical	O
force	O
sv	O
problems	O
were	O
found	O
no	O
harder	O
for	O
the	O
ga	O
than	O
the	O
n	O
and	O
the	O
other	O
with	O
asymmetrical	O
force	O
s	O
n	O
it	O
can	O
be	O
noted	O
that	O
in	O
this	O
case	O
the	O
genetic	B
algorithm	O
is	O
applied	O
just	O
to	O
tune	O
a	O
controller	O
with	O
known	O
structure	O
in	O
a	O
similar	O
way	O
other	O
types	O
of	O
controllers	O
can	O
be	O
tuned	O
for	O
example	B
the	O
classical	O
pid	O
controller	O
ci	O
c	O
et	O
al	O
s	O
n	O
n	O
case	O
n	O
the	O
exploiting	O
operator	O
s	O
skill	O
learning	O
to	O
pilot	O
a	O
plane	O
sammut	O
et	O
al	O
and	O
michie	O
sammut	O
describe	O
experiments	O
in	O
extracting	O
by	O
machine	O
learning	O
the	O
pilot	O
s	O
subcognitive	O
component	O
of	O
the	O
skill	O
of	O
flying	O
a	O
plane	O
in	O
i	O
g	O
i	O
g	O
sec	O
table	O
cq	O
ieee	O
control	O
performance	O
of	O
bratko	O
s	O
control	O
rule	O
with	O
parameter	O
values	O
found	O
by	O
a	O
ga	O
and	O
with	O
parameter	O
values	O
that	O
make	O
the	O
rule	O
equivalent	O
to	O
the	O
bang-bang	O
variant	O
of	O
the	O
classical	O
control	O
rule	O
exploiting	O
operator	O
s	O
skill	O
parameters	O
failures	O
avg	O
survival	O
parameters	O
time	B
fitness	O
failures	O
avg	O
survival	O
time	B
fitness	O
these	O
experiments	O
a	O
simulator	O
of	O
the	O
cessna	O
airplane	O
was	O
used	O
human	O
pilots	O
were	O
asked	O
to	O
fly	O
the	O
simulated	O
plane	O
according	O
to	O
a	O
well	O
defined	O
flight	O
plan	O
this	O
plan	O
consisted	O
of	O
seven	O
stages	O
including	O
manouevres	O
like	O
take	O
off	O
flying	O
to	O
a	O
specified	O
point	O
turning	O
lining	O
up	O
with	O
the	O
runway	O
descending	O
to	O
the	O
runway	O
and	O
landing	O
the	O
pilots	O
control	O
actions	O
during	O
flight	O
were	O
recorded	O
as	O
events	O
each	O
event	O
record	O
consisted	O
of	O
the	O
plane	O
s	O
state	O
variables	O
and	O
the	O
control	O
action	O
the	O
values	O
of	O
state	O
variables	O
belonging	O
to	O
an	O
event	O
were	O
actually	O
taken	O
a	O
little	O
earlier	O
than	O
the	O
pilot	O
s	O
action	O
the	O
reason	O
for	O
this	O
was	O
that	O
the	O
action	O
was	O
assumed	O
to	O
be	O
the	O
pilot	O
s	O
response	O
with	O
some	O
delay	O
to	O
the	O
current	O
state	O
of	O
the	O
plane	O
variables	O
sammut	O
et	O
al	O
stated	O
that	O
it	O
remains	O
debatable	O
what	O
a	O
really	O
appropriate	O
delay	O
is	O
between	O
the	O
state	O
of	O
the	O
plane	O
variables	O
and	O
control	O
action	O
invoked	O
by	O
that	O
state	O
the	O
action	O
was	O
performed	O
some	O
time	B
later	O
in	O
response	O
to	O
the	O
stimulus	O
but	O
how	O
do	O
we	O
know	O
what	O
the	O
stimulus	O
was	O
unfortunately	O
there	O
is	O
no	O
way	O
of	O
knowing	O
the	O
plane	O
s	O
state	O
variables	O
included	O
elevation	O
elevation	O
speed	B
azimuth	O
azimuth	O
speed	B
airspeed	O
etc	O
the	O
possible	O
control	O
actions	O
affected	O
four	O
control	O
variables	O
rollers	O
elevator	O
thrust	O
and	O
flaps	O
the	O
problem	O
was	O
decomposed	O
into	O
four	O
induction	O
problems	O
one	O
for	O
each	O
of	O
the	O
four	O
control	O
variables	O
these	O
four	O
learning	O
problems	O
were	O
assumed	O
independent	O
the	O
control	O
rules	O
were	O
induced	O
by	O
the	O
induction	O
program	O
the	O
total	O
data	O
set	O
consisted	O
of	O
events	O
collected	O
from	O
three	O
pilots	O
and	O
flights	O
by	O
each	O
pilot	O
the	O
data	O
was	O
segmented	O
into	O
seven	O
stages	O
of	O
the	O
complete	O
flight	O
plan	O
and	O
separate	O
rules	O
were	O
induced	O
for	O
each	O
stage	O
separate	O
control	O
rules	O
were	O
induced	O
for	O
each	O
of	O
the	O
three	O
pilots	O
it	O
was	O
decided	O
that	O
it	O
was	O
best	O
not	O
to	O
mix	O
the	O
data	O
corresponding	O
to	O
different	O
individuals	O
because	O
different	O
pilots	O
carry	O
out	O
their	O
manouevres	O
in	O
different	O
styles	O
there	O
was	O
a	O
technical	B
difficulty	O
in	O
using	O
in	O
that	O
it	O
requires	O
discrete	O
class	B
values	O
whereas	O
in	O
the	O
flight	O
problem	O
the	O
control	O
variables	O
are	O
mostly	O
continuous	O
the	O
continuous	O
ranges	O
therefore	O
had	O
to	O
be	O
converted	O
to	O
discrete	O
classes	B
by	O
segmentation	O
into	O
intervals	O
this	O
segmentation	O
was	O
done	O
manually	O
a	O
more	O
natural	O
learning	O
tool	O
for	O
this	O
induction	O
task	O
would	O
therefore	O
be	O
one	O
that	O
allows	O
continuous	O
class	B
such	O
as	O
the	O
techniques	O
of	O
learning	O
regression	O
trees	O
implemented	O
in	O
the	O
programs	O
cart	B
et	O
al	O
and	O
retis	B
c	O
sammut	O
et	O
al	O
state	O
that	O
control	O
rules	O
for	O
a	O
complete	O
flight	O
were	O
successfully	O
synthesized	O
resulting	O
in	O
an	O
inductively	O
constructed	O
autopilot	O
this	O
autopilot	O
flies	O
the	O
cessna	O
learning	O
to	O
control	O
dynamic	O
systems	O
in	O
a	O
manner	O
very	O
similar	O
to	O
that	O
of	O
the	O
human	O
pilot	O
whose	O
data	O
was	O
used	O
to	O
construct	O
the	O
rules	O
in	O
some	O
cases	O
the	O
autopilot	O
flies	O
more	O
smoothly	O
than	O
the	O
pilot	O
we	O
have	O
observed	O
a	O
clean-up	O
effect	O
noted	O
in	O
michie	O
bain	O
and	O
hayes-michie	O
the	O
flight	O
log	O
of	O
any	O
trainer	O
will	O
contain	O
many	O
spurious	O
actions	O
due	O
to	O
human	O
inconsistency	O
and	O
corrections	O
required	O
as	O
a	O
result	O
of	O
inattention	O
it	O
appears	O
that	O
effects	O
of	O
these	O
examples	O
are	O
pruned	O
away	O
by	O
leaving	O
a	O
control	O
rule	O
which	O
flies	O
very	O
smoothly	O
it	O
is	O
interesting	O
to	O
note	O
the	O
comments	O
of	O
sammut	O
et	O
al	O
regarding	O
the	O
contents	O
of	O
the	O
induced	O
rules	O
one	O
of	O
the	O
limitations	O
we	O
have	O
encountered	O
with	O
existing	O
learning	O
algorithms	O
is	O
that	O
they	O
can	O
only	O
use	O
the	O
primitive	O
attributes	B
supplied	O
in	O
the	O
data	O
this	O
results	O
in	O
control	O
rules	O
that	O
cannot	O
be	O
understood	O
by	O
a	O
human	O
expert	O
the	O
rules	O
constructed	O
by	O
are	O
purely	O
reactive	O
they	O
make	O
decisions	O
on	O
the	O
basis	O
of	O
the	O
values	O
in	O
a	O
single	O
step	O
of	O
simulation	O
the	O
induction	O
program	O
has	O
no	O
concept	B
of	O
time	B
and	O
causality	O
in	O
connection	O
with	O
this	O
some	O
strange	O
rules	O
can	O
turn	O
up	O
learning	O
to	O
control	O
container	B
cranes	I
the	O
world	O
market	O
requires	O
container	B
cranes	I
with	O
as	O
high	O
capacity	O
as	O
possible	O
one	O
way	O
to	O
meet	O
this	O
requirement	O
is	O
to	O
build	O
bigger	O
and	O
faster	O
cranes	O
however	O
this	O
approach	O
is	O
limited	O
by	O
construction	O
problems	O
as	O
well	O
as	O
by	O
unpleasant	O
feelings	O
drivers	O
have	O
when	O
moving	O
with	O
high	O
speeds	O
and	O
accelerations	O
the	O
other	O
solution	O
is	O
to	O
make	O
the	O
best	O
of	O
the	O
cranes	O
of	O
reasonable	O
size	O
meaning	O
in	O
the	O
first	O
place	O
the	O
optimisation	B
of	O
the	O
working	O
cycle	O
and	O
efficient	O
swing	O
damping	O
it	O
is	O
known	O
that	O
experienced	O
crane	O
drivers	O
can	O
perform	O
very	O
quickly	O
as	O
long	O
as	O
everything	O
goes	O
as	O
expected	O
while	O
each	O
subsequent	O
correction	O
considerably	O
affects	O
the	O
time	B
needed	O
for	O
accomplishing	O
the	O
task	O
also	O
it	O
is	O
very	O
difficult	O
to	O
drive	O
for	O
hours	O
and	O
hours	O
with	O
the	O
same	O
attention	O
not	O
to	O
mention	O
the	O
years	O
of	O
training	O
needed	O
to	O
gain	O
required	O
skill	O
consequently	O
interest	O
for	O
cooperation	O
has	O
been	O
reported	O
by	O
chief	O
designer	O
of	O
metalna	O
machine	O
builders	O
steel	O
fabricators	O
and	O
erectors	O
maribor	O
which	O
is	O
known	O
world-wide	O
for	O
its	O
large-scale	O
container	B
cranes	I
they	O
are	O
aware	O
of	O
insufficiency	O
of	O
classical	O
automatic	O
controllers	O
example	B
sakawa	O
shinido	O
which	O
can	O
be	O
easily	O
disturbed	O
in	O
the	O
presence	O
of	O
wind	O
or	O
other	O
unpredictable	O
factors	O
this	O
explains	O
their	O
interest	O
in	O
what	O
can	O
be	O
offered	O
by	O
alternative	O
methods	O
impressive	O
results	O
have	O
been	O
obtained	O
by	O
predictive	O
fuzzy	O
control	O
yasunobu	O
hasegawa	O
their	O
method	O
involves	O
steps	O
such	O
as	O
describing	O
human	O
operator	O
strategies	O
defining	O
the	O
meaning	O
of	O
linguistic	O
performance	O
indices	O
defining	O
the	O
models	O
for	O
predicting	O
operation	O
results	O
and	O
converting	O
the	O
linguistic	O
human	O
operator	O
strategies	O
into	O
predictive	O
fuzzy	O
control	O
rules	O
in	O
general	O
these	O
tasks	O
can	O
be	O
very	O
time	B
consuming	O
so	O
our	O
focus	O
of	O
attention	O
was	O
on	O
the	O
automated	O
synthesis	O
of	O
control	O
rules	O
directly	O
from	O
the	O
recorded	O
performance	O
of	O
welltrained	O
operators	O
in	O
this	O
idea	O
we	O
are	O
following	O
the	O
work	O
of	O
michie	O
et	O
al	O
sammut	O
et	O
al	O
and	O
michie	O
camacho	O
who	O
confirmed	O
the	O
findings	O
of	O
sammut	O
et	O
using	O
the	O
acm	O
public-domain	O
simulation	O
of	O
an	O
combat	O
plane	O
when	O
al	O
trying	O
to	O
solve	O
the	O
crane	O
control	O
problem	O
in	O
a	O
manner	O
similar	O
to	O
their	O
autopilot	O
construction	O
sec	O
exploiting	O
operator	O
s	O
skill	O
we	O
encountered	O
some	O
difficulties	O
which	O
are	O
to	O
be	O
investigated	O
more	O
systematically	O
if	O
the	O
method	O
is	O
to	O
become	O
general	O
to	O
transport	O
a	O
container	O
from	O
shore	O
to	O
a	O
target	O
position	O
on	O
a	O
ship	O
two	O
operations	O
are	O
to	O
be	O
performed	O
positioning	O
of	O
the	O
trolley	O
bringing	O
it	O
above	O
the	O
target	O
load	O
position	O
and	O
rope	O
operation	O
bringing	O
the	O
load	O
to	O
the	O
desired	O
height	O
the	O
performance	O
requirements	O
are	O
as	O
follows	O
basic	O
safety	O
obstacles	O
must	O
be	O
avoided	O
swinging	O
must	O
be	O
kept	O
within	O
prescribed	O
limits	O
stop-gap	O
accuracy	B
the	O
gap	O
between	O
the	O
final	O
load	O
position	O
and	O
the	O
target	O
position	O
must	O
be	O
within	O
prescribed	O
limits	O
high	O
capacity	O
time	B
needed	O
for	O
transportation	O
is	O
to	O
be	O
minimised	O
the	O
last	O
requirement	O
forces	O
the	O
two	O
operations	O
to	O
be	O
performed	O
simultaneously	O
the	O
task	O
parameters	O
specifying	O
stop-gap	O
accuracy	B
swinging	O
limits	O
and	O
capacity	O
are	O
given	O
by	O
the	O
customer	O
and	O
vary	O
from	O
case	O
to	O
case	O
is	O
specified	O
by	O
six	O
variables	O
instead	O
of	O
a	O
real	O
crane	O
a	O
simulator	O
was	O
used	O
in	O
our	O
experiments	O
the	O
state	O
of	O
the	O
system	O
trolley	O
position	O
and	O
its	O
velocity	O
and	O
rope	O
inclination	O
angle	O
and	O
its	O
angular	O
velocity	O
and	O
rope	O
length	O
and	O
the	O
length	O
velocity	O
and	O
forces	O
are	O
applied	O
direction	O
of	O
the	O
rope	O
dk	O
is	O
applied	O
to	O
the	O
trolley	O
in	O
the	O
horizontal	O
direction	O
and	O
time	B
is	O
measured	O
in	O
steps	O
at	O
each	O
step	O
the	O
state	O
of	O
the	O
system	O
is	O
measured	O
and	O
two	O
control	O
in	O
the	O
is	O
the	O
force	O
in	O
the	O
vertical	O
direction	O
the	O
next	O
state	O
is	O
computed	O
using	O
runge-kutta	O
numerical	O
simulation	O
of	O
fourth	O
order	O
taking	O
into	O
account	O
the	O
dynamic	O
equations	O
of	O
the	O
system	O
parameters	O
of	O
the	O
system	O
heights	O
masses	O
etc	O
are	O
the	O
same	O
as	O
those	O
of	O
the	O
real	O
container	B
cranes	I
in	O
port	O
of	O
koper	O
simulation	O
runs	O
on	O
ibm	O
pc	O
compatible	O
computers	O
and	O
is	O
real-time	O
for	O
mhz	O
or	O
faster	O
with	O
a	O
mathematical	O
co-processor	O
when	O
experimenting	O
with	O
the	O
simulator	O
one	O
can	O
choose	O
input	B
mode	O
record	O
play	O
or	O
auto	O
output	B
mode	O
picture	O
or	O
instruments	O
where	O
one	O
strike	O
at	O
the	O
step	O
similarly	O
arrows	O
and	O
in	O
the	O
record	O
mode	O
the	O
values	O
of	O
the	O
current	O
control	O
forces	O
are	O
read	O
from	O
the	O
keyboard	O
for	O
a	O
certain	O
predefined	O
or	O
means	O
a	O
decrease	O
or	O
increase	O
ofn	O
indicate	O
the	O
change	O
of	O
d	O
a	O
file	O
containing	O
all	O
control	O
actions	O
together	O
with	O
the	O
corresponding	O
times	O
and	O
system	O
states	O
is	O
recorded	O
in	O
the	O
play	O
mode	O
recorded	O
experiments	O
can	O
be	O
viewed	O
again	O
using	O
the	O
recorded	O
files	O
as	O
input	B
when	O
auto	O
mode	O
is	O
chosen	O
the	O
current	O
values	O
of	O
control	O
forces	O
are	O
determined	O
by	O
a	O
procedure	O
representing	O
an	O
automatic	O
controller	O
the	O
choice	O
of	O
the	O
output	B
mode	O
enables	O
the	O
graphical	O
representation	O
of	O
the	O
scene	O
picture	O
or	O
the	O
variant	O
where	O
the	O
six	O
state	O
variables	O
and	O
the	O
force	O
values	O
are	O
presented	O
as	O
columns	O
with	O
dynamically	O
changing	O
height	O
imitating	O
measuring	O
instruments	O
six	O
students	O
volunteered	O
in	O
an	O
experiment	O
where	O
they	O
were	O
asked	O
to	O
learn	O
to	O
control	O
the	O
crane	O
simulator	O
simply	O
by	O
playing	O
with	O
the	O
simulator	O
and	O
trying	O
various	O
control	O
strategies	O
d	O
learning	O
to	O
control	O
dynamic	O
systems	O
they	O
were	O
given	O
just	O
the	O
instrument	O
version	O
in	O
fact	O
they	O
didn	O
t	O
know	O
which	O
dynamic	O
system	O
underlied	O
the	O
simulator	O
in	O
spite	O
of	O
that	O
they	O
succeeded	O
to	O
learn	O
the	O
task	O
although	O
the	O
differences	O
in	O
time	B
needed	O
for	O
this	O
as	O
well	O
as	O
the	O
quality	O
of	O
control	O
were	O
remarkable	O
to	O
learn	O
to	O
control	O
the	O
crane	O
reasonably	O
well	O
it	O
took	O
a	O
subject	O
between	O
about	O
and	O
trials	O
this	O
amounts	O
to	O
about	O
to	O
hours	O
of	O
real	O
time	B
spent	O
with	O
the	O
simulator	O
our	O
aim	O
was	O
to	O
build	O
automatic	O
controllers	O
from	O
human	O
operators	O
traces	O
we	O
applied	O
retis	B
a	O
program	O
for	O
regression	B
tree	I
construction	O
c	O
to	O
the	O
recorded	O
data	O
the	O
first	O
problem	O
to	O
solve	O
was	O
how	O
to	O
choose	O
an	O
appropriate	O
set	O
of	O
learning	O
examples	O
out	O
of	O
this	O
enormous	O
set	O
of	O
recorded	O
data	O
after	O
some	O
initial	O
experiments	O
we	O
found	O
as	O
in	O
sammut	O
et	O
al	O
that	O
it	O
was	O
beneficial	O
to	O
use	O
different	O
trials	O
performed	O
by	O
the	O
same	O
student	O
since	O
it	O
was	O
practically	O
impossible	O
to	O
find	O
trials	O
perfect	O
in	O
all	O
aspects	O
even	O
among	O
the	O
successful	O
cases	O
in	O
the	O
preparation	O
of	O
learning	O
data	O
performance	O
was	O
sampled	O
each	O
second	O
the	O
actions	O
were	O
related	O
to	O
the	O
states	O
with	O
delay	O
which	O
was	O
also	O
second	O
the	O
performance	O
of	O
the	O
best	O
autodriver	O
induced	O
in	O
these	O
initial	O
experiments	O
can	O
be	O
seen	O
in	O
figure	O
it	O
resulted	O
from	O
learning	O
examples	O
for	O
and	O
examples	O
for	O
d	O
the	O
control	O
strategy	O
it	O
uses	O
is	O
rather	O
conservative	O
minimising	O
the	O
swinging	O
but	O
at	O
the	O
cost	O
of	O
time	B
in	O
further	O
experiments	O
we	O
will	O
try	O
to	O
build	O
an	O
autodriver	O
which	O
will	O
successfully	O
cope	O
with	O
load	O
swinging	O
resulting	O
in	O
faster	O
and	O
more	O
robust	O
performance	O
diffx	O
times	O
g	O
e	O
d	O
m	O
e	O
c	O
n	O
a	O
t	O
s	O
d	O
i	O
n	O
k	O
e	O
c	O
r	O
o	O
f	O
l	O
o	O
r	O
t	O
n	O
o	O
c	O
times	O
fx	O
fig	O
the	O
crane	O
simulator	O
response	O
to	O
the	O
control	O
actions	O
of	O
the	O
autodriver	O
these	O
experiments	O
indicate	O
that	O
further	O
work	O
is	O
needed	O
regarding	O
the	O
following	O
questions	O
what	O
is	O
the	O
actual	O
delay	O
between	O
the	O
system	O
s	O
state	O
and	O
the	O
operator	O
s	O
action	O
robustness	O
of	O
induced	O
rules	O
with	O
respect	O
to	O
initial	O
states	O
comprehensibility	B
of	O
induced	O
control	O
rules	O
inducing	O
higher	O
level	O
conceptual	O
description	O
of	O
control	O
strategies	O
conclusions	O
conclusions	O
in	O
this	O
chapter	O
we	O
have	O
treated	O
the	O
problem	O
of	O
controlling	O
a	O
dynamic	O
system	O
mainly	O
as	O
a	O
classification	B
problem	O
we	O
introduced	O
three	O
modes	O
of	O
learning	O
to	O
control	O
depending	O
on	O
the	O
information	O
available	O
to	O
the	O
learner	O
this	O
information	O
included	O
in	O
addition	O
to	O
the	O
usual	O
examples	O
of	O
the	O
behaviour	O
of	O
the	O
controlled	O
system	O
also	O
explicit	O
symbolic	O
knowledge	O
about	O
the	O
controlled	O
system	O
and	O
example	B
actions	O
performed	O
by	O
a	O
skilled	O
human	O
operator	O
one	O
point	O
that	O
the	O
described	O
experiments	O
emphasise	O
is	O
the	O
importance	O
of	O
incomplete	O
partial	O
knowledge	O
about	O
the	O
controlled	O
system	O
methods	O
described	O
in	O
this	O
chapter	O
enable	O
natural	O
use	O
of	O
partial	O
symbolic	O
knowledge	O
although	O
incomplete	O
this	O
knowledge	O
may	O
drastically	O
constrain	O
the	O
search	O
for	O
control	O
rules	O
thereby	O
eliminating	O
in	O
advance	O
large	O
numbers	O
of	O
totally	O
unreasonable	O
rules	O
our	O
choice	O
of	O
the	O
approaches	O
to	O
learning	O
to	O
control	O
in	O
this	O
chapter	O
was	O
subjective	O
among	O
a	O
large	O
number	O
of	O
known	O
approaches	O
we	O
chose	O
for	O
more	O
detailed	O
presentation	O
those	O
that	O
first	O
we	O
had	O
personal	O
experimental	O
experience	O
with	O
and	O
second	O
that	O
enable	O
the	O
use	O
of	O
partial	O
symbolic	O
prior	O
knowledge	O
in	O
all	O
the	O
approaches	O
described	O
there	O
was	O
an	O
aspiration	O
to	O
generate	O
comprehensible	O
control	O
rules	O
sometimes	O
at	O
the	O
cost	O
of	O
an	O
additional	O
learning	O
stage	O
an	O
interesting	O
theme	O
also	O
described	O
is	O
behavioural	B
cloning	I
where	O
a	O
human	O
s	O
behavioural	O
skill	O
is	O
cloned	O
by	O
a	O
learned	O
rule	O
behavioural	B
cloning	I
is	O
interesting	O
both	O
from	O
the	O
practical	O
and	O
the	O
research	O
points	O
of	O
view	O
much	O
further	O
work	O
is	O
needed	O
before	O
behavioural	B
cloning	I
may	O
become	O
routinely	O
applicable	O
in	O
practice	O
behavioural	B
cloning	I
is	O
essentially	O
the	O
regression	O
of	O
the	O
operator	O
s	O
decision	O
function	O
from	O
examples	O
of	O
hisher	O
decisions	O
it	O
is	O
relevant	O
in	O
this	O
respect	O
to	O
notice	O
a	O
similarity	O
between	O
this	O
and	O
traditional	O
top-down	O
derivation	O
of	O
control	O
from	O
a	O
detailed	O
model	O
of	O
the	O
system	O
to	O
be	O
controlled	O
this	O
similarity	O
is	O
illustrated	O
by	O
the	O
fact	O
that	O
such	O
a	O
top-down	O
approach	O
for	O
the	O
pole-and-cart	O
system	O
gives	O
the	O
known	O
linear	O
control	O
rule	O
which	O
looks	O
just	O
like	O
regression	O
equation	O
as	O
stated	O
in	O
the	O
introduction	O
there	O
are	O
several	O
criteria	O
for	O
and	O
goals	O
of	O
learning	O
to	O
control	O
and	O
several	O
assumptions	O
regarding	O
the	O
problem	O
as	O
shown	O
by	O
the	O
experience	O
with	O
various	O
learning	O
approaches	O
it	O
is	O
important	O
to	O
clarify	O
very	O
precisely	O
what	O
these	O
goals	O
and	O
assumptions	O
really	O
are	O
in	O
the	O
present	O
problem	O
correct	O
statement	O
of	O
these	O
may	O
considerably	O
affect	O
the	O
efficiency	O
of	O
the	O
learning	O
process	O
for	O
example	B
it	O
is	O
important	O
to	O
consider	O
whether	O
some	O
symbolic	O
knowledge	O
exists	O
about	O
the	O
domain	O
and	O
not	O
to	O
assume	O
automatically	O
that	O
it	O
is	O
necessary	O
or	O
best	O
to	O
learn	O
everything	O
from	O
scratch	O
in	O
some	O
approaches	O
reviewed	O
such	O
incomplete	O
prior	O
knowledge	O
could	O
also	O
result	O
from	O
a	O
previous	O
stage	O
of	O
learning	O
when	O
another	O
learning	O
technique	O
was	O
employed	O
acknowledgements	O
this	O
work	O
was	O
supported	O
by	O
the	O
ministry	O
of	O
science	O
and	O
technology	O
of	O
slovenia	O
the	O
authors	O
would	O
like	O
to	O
thank	O
donald	O
michie	O
for	O
comments	O
and	O
suggestions	O
appendices	O
a	O
dataset	O
availability	O
the	O
public	O
domain	O
datasets	O
are	O
listed	O
below	O
with	O
an	O
anonymous	O
ftp	O
address	O
if	O
you	O
do	O
not	O
have	O
access	O
to	O
these	O
then	O
you	O
can	O
obtain	O
the	O
datasets	O
on	O
diskette	O
from	O
dr	O
p	O
b	O
brazdil	O
university	O
of	O
porto	O
laboratory	O
of	O
ai	O
and	O
computer	O
science	O
r	O
campo	O
alegre	O
porto	O
potugal	O
the	O
main	O
source	O
of	O
datasets	O
is	O
ics	O
uci	O
edu	O
the	O
uci	O
repository	O
of	O
machine	O
learning	O
databases	O
and	O
domain	O
theories	O
which	O
is	O
managed	O
by	O
d	O
w	O
aha	O
the	O
following	O
datasets	O
many	O
others	O
are	O
in	O
pubmachine-learningdatabases	O
australian	B
credit	I
statlogaustralian	O
diabetes	B
dna	B
heart	B
disease	I
statlogheart	O
letter	B
recognition	I
image	B
segmentation	I
shuttle	B
control	I
landsat	O
satellite	B
image	I
vehicle	B
recognition	I
the	O
datasets	O
were	O
often	O
processed	O
and	O
the	O
processed	O
form	O
can	O
be	O
found	O
in	O
the	O
statlog	B
subdirectory	O
where	O
mentioned	O
above	O
in	O
addition	O
the	O
processed	O
datasets	O
used	O
in	O
this	O
book	O
can	O
also	O
be	O
obtained	O
from	O
ftp	O
strath	O
ac	O
uk	O
in	O
directory	O
stamsstatlog	O
these	O
datasets	O
are	O
australiandiabetesdnagermanheartlettersatimage	O
segmentshuttleshuttle	O
and	O
there	O
are	O
associated	O
files	O
as	O
well	O
as	O
a	O
split	O
into	O
train	O
and	O
test	B
set	I
used	O
in	O
the	O
statlog	B
project	O
for	O
the	O
larger	O
datasets	O
b	O
software	O
sources	O
and	O
details	O
many	O
of	O
the	O
classical	O
statistical	B
algorithms	O
are	O
available	O
in	O
standard	O
statistical	B
packages	O
here	O
we	O
list	O
some	O
public	O
domain	O
versions	O
and	O
sources	O
and	O
some	O
commercial	O
packages	O
if	O
a	O
simple	O
rule	O
has	O
been	O
adopted	O
for	O
parameter	O
selection	O
then	O
we	O
have	O
also	O
described	O
this	O
b	O
software	O
sources	O
and	O
details	O
is	O
a	O
fortran	O
program	O
available	O
from	O
j	O
hermans	O
dept	O
of	O
medical	O
statistics	O
niels	O
bohrweg	O
ca	O
leiden	O
university	O
of	O
leiden	O
the	O
netherlands	O
smart	B
is	O
a	O
collection	O
of	O
fortran	O
subroutines	O
developed	O
by	O
j	O
h	O
friedman	O
dept	O
of	O
statistics	O
sequoia	O
hall	O
stanford	O
university	O
stanford	O
ca	O
usa	O
castle	B
can	O
be	O
obtained	O
from	O
r	O
molina	O
dept	O
of	O
computer	O
science	O
and	O
a	O
i	O
faculty	O
of	O
science	O
university	O
of	O
granada	O
spain	O
indcart	B
bayes	B
tree	I
and	O
naive	B
bayes	I
are	O
part	O
of	O
the	O
ind	B
package	I
which	O
is	O
available	O
from	O
w	O
buntine	O
nasa	O
ames	O
research	O
center	O
ms	O
moffett	O
field	O
ca	O
usa	O
p	O
e	O
x	O
l	O
and	O
is	O
available	O
from	O
f	O
wysotzki	O
fraunhofer-institute	O
kurstrasse	O
berlin	O
germany	O
for	O
the	O
number	O
of	O
clusters	O
has	O
to	O
be	O
fixed	O
by	O
the	O
user	O
with	O
some	O
systematic	O
experimentation	O
all	O
other	O
parameters	O
are	O
determined	O
by	O
the	O
algorithm	O
for	O
the	O
confidence	O
level	O
for	O
estimation	O
and	O
the	O
threshold	O
for	O
tree	O
pruning	B
were	O
optimised	O
either	O
by	O
hand	O
or	O
a	O
special	O
c	O
shell	O
an	O
entropy	B
measure	B
to	O
choose	O
the	O
best	O
discrimination	B
attribute	O
at	O
each	O
current	O
node	O
was	O
used	O
logistic	O
discriminants	O
quadratic	O
discrminants	O
and	O
logistic	O
discriminants	O
are	O
fortran	O
programs	O
available	O
from	O
r	O
j	O
henery	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
university	O
of	O
strathclyde	O
glasgow	O
uk	O
there	O
are	O
also	O
available	O
by	O
anonymous	O
ftp	O
from	O
ftp	O
strath	O
ac	O
uk	O
in	O
directory	O
stamsstatlogprograms	O
is	O
available	O
from	O
h	O
perdrix	O
isoft	O
chemin	O
de	O
moulon	O
gif	O
sur	O
yvette	O
france	O
the	O
user	O
must	O
choose	O
between	O
evaluation	O
functions	O
e	O
b	O
e	O
b	O
i	O
e	O
e	O
e	O
in	O
the	O
reported	O
results	O
the	O
fourth	O
option	O
was	O
chosen	O
backprop	B
cascade	B
correlation	B
and	O
radial	B
basis	I
function	I
are	O
fortran	O
programs	O
available	O
from	O
r	O
rohwer	O
department	O
of	O
computer	O
science	O
and	O
applied	O
mathematics	O
aston	O
university	O
birmingham	O
uk	O
the	O
inputs	O
for	O
all	O
datasets	O
were	O
normalised	O
to	O
zero	O
mean	O
and	O
unit	O
variance	O
the	O
outputs	O
problem	O
was	O
represented	O
as	O
an	O
n-dimensional	O
vector	O
with	O
all	O
components	O
equal	O
to	O
except	O
the	O
multilayer	O
perceptron	B
simulations	O
were	O
done	O
with	O
autonet	O
on	O
sun	O
unix	O
workstations	O
autonet	O
is	O
commercial	O
software	O
available	O
from	O
paul	O
gregory	O
recognition	O
research	O
church	O
lane	O
marple	O
stockport	O
uk	O
were	O
converted	O
to	O
a	O
representation	O
ie	O
the	O
th	O
class	B
of	O
an	O
n-class	O
classification	B
the	O
th	O
which	O
is	O
e	O
e	O
e	O
runs	O
were	O
made	O
with	O
of	O
the	O
runs	O
was	O
best	O
random	O
number	O
seed	O
for	O
each	O
run	O
was	O
run	O
number	O
having	O
picked	O
the	O
best	O
net	O
by	O
cross	B
validation	I
within	O
the	O
training	O
e	O
t	O
the	O
settings	O
were	O
i	O
e	O
e	O
learning	O
to	O
control	O
dynamic	O
systems	O
set	O
these	O
nets	O
were	O
then	O
used	O
for	O
supplying	O
the	O
performance	O
figures	O
on	O
the	O
whole	O
training	B
set	I
and	O
on	O
the	O
test	B
set	I
the	O
figures	O
averaged	O
for	O
cross	B
validation	I
performance	B
measures	B
were	O
also	O
for	O
the	O
best	O
nets	O
found	O
during	O
local	O
cross-validation	O
within	O
the	O
individual	O
training	O
sets	O
training	O
proceeds	O
in	O
four	O
stages	O
with	O
different	O
stages	O
using	O
different	O
subsets	O
of	O
the	O
training	O
data	O
larger	O
each	O
time	B
training	O
proceeds	O
until	O
no	O
improvement	O
in	O
error	O
is	O
achieved	O
for	O
a	O
run	O
of	O
updates	O
the	O
rrnn	O
simulator	O
provided	O
the	O
radial	B
basis	I
function	I
code	O
this	O
is	O
freely	O
available	O
at	O
the	O
time	B
of	O
writing	O
by	O
anonymous	O
ftp	O
from	O
uk	O
ac	O
aston	O
cs	O
this	O
package	O
also	O
contains	O
mlp	B
code	O
using	O
the	O
conjugate	B
gradient	I
algorithm	O
as	O
does	O
autonet	O
and	O
several	O
other	O
algorithms	O
reports	O
on	O
benchmark	O
excercises	O
are	O
available	O
for	O
some	O
of	O
these	O
mlp	B
programs	O
in	O
rohwer	O
the	O
centres	O
for	O
the	O
radial	O
basis	O
functions	O
were	O
selected	O
randomly	O
from	O
the	O
training	O
data	O
except	O
that	O
centres	O
were	O
allocated	O
to	O
each	O
class	B
in	O
proportion	O
to	O
the	O
number	O
of	O
representatives	O
of	O
that	O
class	B
in	O
the	O
dataset	O
with	O
at	O
least	O
one	O
centre	O
provided	O
to	O
each	O
class	B
in	O
any	O
case	O
each	O
gaussian	O
radius	O
was	O
set	O
to	O
the	O
distance	B
to	O
the	O
nearest	O
neighboring	O
centre	O
the	O
linear	O
system	O
was	O
solved	O
by	O
singular	O
value	O
decomposition	O
for	O
the	O
small	O
datasets	O
the	O
number	O
of	O
centres	O
and	O
thier	O
locations	O
were	O
selected	O
by	O
training	O
with	O
various	O
numbers	O
of	O
centres	O
using	O
different	O
random	O
number	O
seeds	O
for	O
each	O
number	O
and	O
evaluating	O
with	O
a	O
cross	B
validation	I
set	O
withheld	O
from	O
the	O
training	O
data	O
precisely	O
as	O
was	O
done	O
for	O
the	O
mlps	O
for	O
the	O
large	O
datasets	O
time	B
constraints	O
were	O
met	O
by	O
compromising	O
rigour	O
in	O
that	O
the	O
test	B
set	I
was	O
used	O
for	O
the	O
cross-validation	O
set	O
results	O
for	O
these	O
sets	O
should	O
therefore	O
be	O
viewed	O
with	O
some	O
caution	O
this	O
was	O
the	O
case	O
for	O
all	O
data	O
sets	O
until	O
those	O
for	O
which	O
cross-validation	O
was	O
explicitly	O
required	O
diabetes	B
german	O
isoft	O
segment	O
were	O
repeated	O
with	O
cross-validation	O
to	O
select	O
the	O
number	O
of	O
centres	O
carried	O
out	O
within	O
the	O
training	B
set	I
only	O
the	O
rough	O
guideline	O
followed	O
for	O
deciding	O
on	O
numbers	O
of	O
centres	O
to	O
try	O
is	O
that	O
the	O
number	O
should	O
be	O
about	O
times	O
the	O
dimension	O
of	O
the	O
input	B
space	O
unless	O
that	O
would	O
be	O
more	O
than	O
of	O
the	O
size	O
of	O
the	O
dataset	O
lvq	B
is	O
available	O
from	O
the	O
laboratory	O
of	O
computer	O
science	O
and	O
information	O
science	O
helsinki	O
university	O
of	O
technology	O
rakentajanaukio	O
c	O
sf	O
espoo	O
finland	O
it	O
can	O
also	O
be	O
obtained	O
by	O
anonymous	O
ftp	O
from	O
cochlea	O
hut	O
fi	O
cart	B
is	O
a	O
licensed	O
product	O
of	O
california	O
statistical	B
software	O
inc	O
yorkshire	O
court	O
lafayette	O
ca	O
usa	O
is	O
availbale	O
from	O
j	O
r	O
quinlan	O
dept	O
of	O
computer	O
science	O
madsen	O
building	O
university	O
of	O
sydney	O
new	O
south	O
wales	O
new	O
south	O
wales	O
the	O
parameters	O
used	O
were	O
the	O
defaults	O
the	O
heuristic	O
was	O
information	O
gain	O
e	O
p	O
i	O
e	O
a	O
e	O
e	O
b	O
p	O
l	O
e	O
however	O
since	O
the	O
statlog	B
project	O
was	O
completed	O
there	O
is	O
a	O
more	O
recent	O
version	O
of	O
so	O
the	O
results	O
contained	O
in	O
this	O
book	O
may	O
not	O
be	O
exactly	O
reproducable	O
c	O
contributors	O
for	O
e	O
newid	B
and	O
are	O
available	O
from	O
robin	O
boswell	O
and	O
tim	O
niblett	O
respectively	O
at	O
the	O
turing	O
institute	O
george	O
house	O
north	O
hanover	O
street	O
glasgow	O
uk	O
for	O
newid	B
v	O
e	O
i	O
p	O
p	O
i	O
e	O
b	O
i	O
p	O
i	O
e	O
p	O
e	O
p	O
i	O
e	O
e	O
e	O
u	O
b	O
k-nn	B
is	O
still	O
under	O
development	O
for	O
all	O
datasets	O
except	O
the	O
satellite	B
image	I
dataset	I
although	O
the	O
two	O
belgian	O
power	O
datasets	O
were	O
run	O
with	O
the	O
above	O
parameters	O
set	O
to	O
and	O
kohonen	B
was	O
written	O
by	O
j	O
paul	O
dhamstr	O
schmallenberg	O
germany	O
for	O
a	O
pc	O
with	O
an	O
attached	O
transputer	O
board	O
itrule	B
is	O
available	O
from	O
prof	O
r	O
goodman	O
california	O
institute	O
of	O
technology	O
electrical	O
engineering	O
mail	O
code	O
pasadena	O
ca	O
usa	O
for	O
most	O
of	O
the	O
datasets	O
the	O
parameters	O
used	O
were	O
distance	B
was	O
scaled	O
in	O
a	O
class	B
dependent	O
manner	O
using	O
the	O
standard	O
deviation	O
further	O
details	O
can	O
be	O
obtained	O
from	O
c	O
c	O
taylor	O
department	O
of	O
statistics	O
university	O
of	O
leeds	O
leeds	O
uk	O
e	O
p	O
c	O
contributors	O
this	O
volume	O
is	O
based	O
on	O
the	O
statlog	B
project	O
which	O
involved	O
many	O
workers	O
at	O
over	O
institutions	O
in	O
this	O
list	O
we	O
aim	O
to	O
include	O
those	O
who	O
contributed	O
to	O
the	O
project	O
and	O
the	O
institutions	O
at	O
which	O
they	O
were	O
primarily	O
based	O
at	O
that	O
time	B
g	O
nakhaeizadeh	O
j	O
graf	O
a	O
merkel	O
h	O
keller	O
laue	O
h	O
ulrich	O
g	O
kressel	O
daimler	O
benz	O
ag	O
r	O
j	O
henery	O
d	O
michie	O
j	O
m	O
o	O
mitchell	O
a	O
i	O
sutherland	O
r	O
king	O
s	O
haque	O
c	O
kay	O
d	O
young	O
w	O
buntine	O
b	O
d	O
ripley	O
university	O
of	O
strathclyde	O
s	O
h	O
muggleton	O
c	O
feng	O
t	O
niblett	O
r	O
boswell	O
turing	O
institute	O
h	O
perdrix	O
t	O
brunet	O
t	O
marre	O
j-j	O
cannat	O
isoft	O
j	O
stender	O
p	O
ristau	O
d	O
picu	O
i	O
chorbadjiev	O
c	O
kennedy	O
g	O
ruedke	O
f	O
boehme	O
s	O
schulze-kremer	O
brainware	O
gmbh	O
p	O
b	O
brazdil	O
j	O
gama	O
l	O
torgo	O
university	O
of	O
porto	O
r	O
molina	O
n	O
p	O
erez	O
de	O
la	O
blanca	O
s	O
acid	O
l	O
m	O
de	O
campos	O
gonzalez	O
university	O
of	O
granada	O
f	O
wysotzki	O
w	O
mueller	O
der	O
buhlau	O
schmelowski	O
funke	O
villman	O
h	O
herzheim	O
b	O
schulmeister	O
fraunhofer	O
institute	O
learning	O
to	O
control	O
dynamic	O
systems	O
t	O
bueckle	O
c	O
ziegler	O
m	O
surauer	O
messerschmitt	O
boelkow	O
blohm	O
j	O
paul	O
p	O
von	O
goldammer	O
univeristy	O
of	O
l	O
ubeck	O
c	O
c	O
taylor	O
x	O
feng	O
university	O
of	O
leeds	O
r	O
rohwer	O
m	O
wynne-jones	O
aston	O
university	O
d	O
spiegelhalter	O
mrc	O
cambridge	O
references	O
acid	O
s	O
campos	O
l	O
m	O
d	O
gonz	O
alez	O
a	O
molina	O
r	O
and	O
p	O
erez	O
de	O
la	O
blanca	O
n	O
bayesian	O
learning	O
algorithms	O
in	O
castle	B
report	O
no	O
university	O
of	O
granada	O
acid	O
s	O
campos	O
l	O
m	O
d	O
gonz	O
alez	O
a	O
molina	O
r	O
and	O
p	O
erez	O
de	O
la	O
blanca	O
n	O
castle	B
causal	O
structures	O
from	O
inductive	B
learning	I
release	O
report	O
no	O
university	O
of	O
granada	O
agresti	O
a	O
categorical	O
data	O
analysis	O
wiley	O
new	O
york	O
aha	O
d	O
generalising	O
case	O
studies	O
a	O
case	O
study	O
in	O
int	O
conf	O
on	O
machine	O
learning	O
pages	O
san	O
mateo	O
cal	O
morgan	O
kaufmann	O
aha	O
d	O
w	O
kibler	O
d	O
and	O
albert	O
m	O
k	O
instance-based	B
learning	O
algorithms	O
machine	O
learning	O
preprints	O
of	O
the	O
ifacifipimacs	O
international	O
symposium	O
on	O
artificial	O
intelligence	O
in	O
real-time	O
control	O
delft	O
the	O
netherlands	O
pages	O
aitchison	O
j	O
and	O
aitken	O
c	O
g	O
g	O
multivariate	O
binary	O
discrimination	B
by	O
the	O
kernel	O
method	O
biometrika	O
al-attar	O
a	O
structured	O
decision	O
tasks	O
methodology	O
attar	O
software	O
ltd	O
new	O
lands	O
house	O
newlands	O
road	O
leigh	O
lancs	O
aleksander	O
i	O
thomas	O
w	O
v	O
and	O
bowden	O
p	O
a	O
wisard	O
a	O
radical	O
step	O
forward	B
in	O
image	O
recognition	O
sensor	O
review	O
anderson	O
c	O
w	O
strategy	O
learning	O
with	O
multilayer	O
connectionist	O
representations	O
in	O
lengley	O
p	O
editor	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
machine	O
learning	O
pages	O
morgan	O
kaufmann	O
anderson	O
c	O
w	O
and	O
miller	O
w	O
t	O
challenging	O
control	O
problems	O
in	O
miller	O
w	O
t	O
sutton	O
r	O
s	O
and	O
werbos	O
p	O
j	O
editors	O
neural	B
networks	I
for	O
control	O
pages	O
the	O
mit	O
press	O
anderson	O
j	O
a	O
regression	O
and	O
ordered	O
categorical	B
variables	I
j	O
r	O
statist	O
soc	O
b	O
anderson	O
t	O
w	O
an	O
introduction	O
to	O
multivariate	O
statistical	B
analysis	O
john	O
wiley	O
new	O
york	O
references	O
andrews	O
d	O
f	O
and	O
herzberg	O
a	O
m	O
data	O
a	O
collection	O
of	O
problems	O
from	O
many	O
fields	O
for	O
the	O
student	O
and	O
research	O
worker	O
springer-verlag	O
new	O
york	O
arbab	O
b	O
and	O
michie	O
d	O
generating	O
expert	O
rules	O
from	O
examples	O
in	O
prolog	O
in	O
hayes	O
j	O
e	O
michie	O
d	O
and	O
richards	O
j	O
editors	O
machine	O
intelligence	O
pages	O
oxford	O
university	O
press	O
oxford	O
ash	O
t	O
dynamic	O
node	O
creation	O
in	O
back-propagation	O
networks	O
ics	O
report	O
institute	O
of	O
cognitive	O
science	O
university	O
of	O
california	O
san	O
diego	O
la	O
jolla	O
california	O
usa	O
astr	O
om	O
k	O
j	O
intelligent	O
control	O
in	O
proceedings	O
of	O
the	O
ecc	O
european	O
control	O
conference	O
pages	O
grenoble	O
atlas	O
l	O
connor	O
j	O
and	O
park	O
d	O
et	O
al	O
a	O
performance	O
comparison	O
of	O
trained	O
multi-layer	O
perceptrons	O
and	O
trained	O
classification	B
trees	O
in	O
systems	O
man	O
and	O
cybernetics	O
proceedings	O
of	O
the	O
ieee	O
international	O
conference	O
pages	O
cambridge	O
ma	O
hyatt	O
regency	O
bain	O
m	O
machine-learned	O
rule-based	O
control	O
in	O
grimble	O
m	O
mcghee	O
j	O
and	O
mowforth	O
p	O
editors	O
knowledge-based	O
systems	O
in	O
industrial	O
control	O
pages	O
stevenage	O
peter	O
peregrinus	O
bain	O
m	O
communication	O
learning	O
logical	O
exceptions	O
in	O
chess	O
phd	O
thesis	O
university	O
of	O
strathclyde	O
glasgow	O
banerji	O
r	O
b	O
artificial	O
intelligence	O
a	O
theoretical	O
approach	O
north	O
holland	O
new	O
york	O
barto	O
a	O
g	O
sutton	O
r	O
s	O
and	O
anderson	O
c	O
w	O
neuronlike	O
adaptive	O
elements	O
that	O
can	O
solve	O
difficult	O
learning	O
control	O
problems	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
begg	O
c	O
b	O
and	O
gray	O
r	O
calculation	O
of	O
polychotomous	O
logistic	O
regression	O
param	O
eters	O
using	O
individualized	O
regressions	O
biometrika	O
bilbro	O
g	O
and	O
den	O
bout	O
d	O
v	O
maximum	O
entropy	B
and	O
learning	O
theory	O
neural	O
computation	O
bledsoe	O
w	O
w	O
further	O
results	O
on	O
the	O
n-tuple	O
pattern	B
recognition	I
method	O
ire	O
trans	O
comp	O
bledsoe	O
w	O
w	O
and	O
browning	O
i	O
pattern	B
recognition	I
and	O
reading	O
by	O
machine	O
in	O
proceedings	O
of	O
the	O
eastern	O
joint	O
computer	O
conference	O
pages	O
boston	O
blyth	O
c	O
r	O
note	O
on	O
estimating	O
information	O
annals	O
of	O
math	O
stats	O
bonelli	O
p	O
and	O
parodi	O
a	O
an	O
efficient	O
classifier	B
system	O
and	O
its	O
experimental	O
comparisons	O
with	O
two	O
representative	O
learning	O
methods	O
on	O
three	O
medical	O
domains	O
in	O
genetic	B
algorithms	I
proceedings	O
of	O
the	O
fourth	O
international	O
conference	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
booth	O
t	O
h	O
stein	O
j	O
a	O
hutchinson	O
m	O
f	O
and	O
nix	O
h	O
a	O
identifying	O
areas	O
within	O
a	O
country	O
climatically	O
suitable	O
for	O
particular	O
tree	O
species	O
an	O
example	B
using	O
zimbabwe	O
international	O
tree	O
crops	O
journal	O
bourlard	O
h	O
and	O
wellekens	O
c	O
j	O
links	O
between	O
markov	O
models	O
and	O
multilayer	O
perceptrons	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
bratko	O
i	O
generating	O
human-understandable	O
decision	O
rules	O
technical	B
report	O
ljubljana	O
university	O
slovenia	O
references	O
bratko	O
i	O
qualitative	O
modelling	O
learning	O
and	O
control	O
in	O
proceedings	O
of	O
the	O
czechoslovak	O
conference	O
on	O
artificial	O
intelligence	O
prague	O
bratko	O
i	O
qualitative	O
reasoning	O
about	O
control	O
in	O
proceedings	O
of	O
the	O
etfa	O
conference	O
cairns	O
australia	O
bratko	O
i	O
mozetic	O
i	O
and	O
lavrac	O
l	O
kardio	B
a	O
study	O
in	O
deep	O
and	O
qualitative	O
knowledge	O
for	O
expert	B
systems	I
mit	O
press	O
cambridge	O
ma	O
and	O
london	O
breiman	O
l	O
and	O
friedman	O
j	O
h	O
estimating	O
optimal	O
transformations	O
for	O
multiple	O
regression	O
and	O
correlation	B
discussion	O
journal	O
of	O
the	O
american	O
statistical	B
association	O
no	O
breiman	O
l	O
friedman	O
j	O
h	O
olshen	O
r	O
a	O
and	O
stone	O
c	O
j	O
classification	B
and	O
regression	O
trees	O
wadsworth	O
and	O
brooks	O
monterey	O
ca	O
breiman	O
l	O
meisel	O
w	O
and	O
purcell	O
e	O
variable	O
kernel	O
estimates	O
of	O
multivariate	O
densities	O
technometrics	O
bretzger	O
t	O
m	O
die	O
anwendung	O
statistischer	O
verfahren	O
zur	O
risikofruherkennung	O
bei	O
dispositionskrediten	O
phd	O
thesis	O
universitat	O
hohenheim	O
broomhead	O
d	O
s	O
and	O
lowe	O
d	O
multi-variable	O
functional	O
interpolation	O
and	O
adaptive	O
networks	O
complex	B
systems	O
bruner	O
j	O
s	O
goodnow	O
j	O
j	O
and	O
austin	O
g	O
a	O
a	O
study	O
of	O
thinking	O
wiley	O
new	O
york	O
buntine	O
w	O
ind	B
package	I
of	O
machine	O
learning	O
algorithms	O
ind	O
technical	B
report	O
ms	O
research	O
institute	O
for	O
advanced	O
computer	O
science	O
nasa	O
ames	O
research	O
center	O
moffett	O
field	O
ca	O
buntine	O
w	O
generalized	O
subsumption	O
and	O
its	O
applications	O
to	O
induction	O
and	O
redun	O
dancy	O
artificial	O
intelligence	O
buntine	O
w	O
learning	O
classification	B
trees	O
statistics	O
and	O
computing	O
camacho	O
r	O
and	O
michie	O
d	O
an	O
engineering	O
model	O
of	O
subcognition	O
in	O
proceedings	O
of	O
the	O
issek	O
workshop	O
bled	O
slovenia	O
carpenter	O
b	O
e	O
and	O
doran	O
r	O
w	O
editors	O
a	O
m	O
turing	O
s	O
ace	B
report	O
and	O
other	O
papers	O
mit	O
press	O
cambridge	O
ma	O
carter	O
c	O
and	O
catlett	O
j	O
assessing	O
credit	O
card	O
applications	O
using	O
machine	O
learning	O
ieee	O
expert	O
intelligent	O
systems	O
and	O
their	O
applications	O
cestnik	O
b	O
and	O
bratko	O
i	O
on	O
estimating	O
probabilities	O
in	O
tree	O
pruning	B
in	O
ewsl	O
porto	O
portugal	O
berlin	O
springer-verlag	O
cestnik	O
b	O
kononenko	O
i	O
and	O
bratko	O
i	O
assistant	B
a	O
knowledge-elicitation	O
tool	O
for	O
sophisticated	O
users	O
in	O
progress	O
in	O
machine	O
learning	O
proceedings	O
of	O
pages	O
bled	O
yugoslavia	O
sigma	O
press	O
cherkaoui	O
o	O
and	O
cleroux	O
r	O
comparative	O
study	O
of	O
six	O
discriminant	O
analysis	O
procedures	O
for	O
mixtures	O
of	O
variables	O
in	O
proceedings	O
of	O
interface	O
conference	O
morgan	O
kaufmann	O
clark	O
l	O
a	O
and	O
pregibon	O
d	O
tree-based	O
models	O
in	O
chambers	O
j	O
and	O
hastie	O
t	O
editors	O
statistical	B
models	O
in	O
s	O
wadsworth	O
brooks	O
pacific	O
grove	O
california	O
clark	O
p	O
and	O
boswell	O
r	O
rule	O
induction	O
with	O
some	O
recent	O
improvements	O
in	O
ewsl	O
porto	O
portugal	O
pages	O
berlin	O
springer-verlag	O
references	O
clark	O
p	O
and	O
niblett	O
t	O
the	O
induction	O
algorithm	O
machine	O
learning	O
clarke	O
w	O
r	O
lachenbruch	O
p	O
a	O
and	O
broffitt	O
b	O
how	O
nonnormality	O
affects	O
the	O
quadratic	B
discriminant	I
function	O
comm	O
statist	O
theory	O
meth	O
connell	O
m	O
e	O
and	O
utgoff	O
p	O
e	O
learning	O
to	O
control	O
a	O
dynamic	O
physical	O
system	O
in	O
proceedings	O
of	O
the	O
national	O
conference	O
on	O
artificial	O
intelligence	O
pages	O
morgan	O
kaufmann	O
cooper	O
g	O
f	O
nestro	O
a	O
computer-based	O
medical	O
diagnostic	O
that	O
integrates	O
causal	O
and	O
probabilistic	O
knowledge	O
report	O
no	O
stanford	O
university	O
stanford	O
california	O
cooper	O
g	O
f	O
and	O
herkovsits	O
e	O
a	O
bayesian	O
method	O
for	O
the	O
induction	O
of	O
proba	O
bilistic	O
networks	O
from	O
data	O
technical	B
report	O
stanford	O
university	O
cover	B
t	O
m	O
geometrical	O
and	O
statistical	B
properties	O
of	O
systems	O
of	O
linear	O
inequalities	O
with	O
applications	O
in	O
pattern	B
recognition	I
ieee	O
transactions	O
on	O
electronic	O
computers	O
cox	O
d	O
r	O
some	O
procedures	O
associated	O
with	O
the	O
logistic	O
qualitative	O
response	O
curve	O
in	O
david	O
f	O
n	O
editor	O
research	O
papers	O
on	O
statistics	O
festschrift	O
for	O
j	O
neyman	O
pages	O
john	O
wiley	O
new	O
york	O
crawford	O
s	O
l	O
extensions	O
to	O
the	O
cart	B
algorithm	O
int	O
j	O
man-machine	O
studies	O
cutsem	O
van	O
t	O
wehenkel	O
l	O
pavella	O
m	O
heilbronn	O
b	O
and	O
goubin	O
m	O
decision	B
trees	I
for	O
detecting	O
emergency	O
voltage	O
conditions	O
in	O
second	O
international	O
workshop	O
on	O
bulk	O
power	O
system	O
voltage	O
phenomena	O
voltage	O
stability	O
and	O
security	O
pages	O
usa	O
mchenry	O
davies	O
e	O
r	O
training	O
sets	O
and	O
a	O
priori	O
probabilities	O
with	O
the	O
nearest	B
neighbour	I
method	O
of	O
pattern	B
recognition	I
pattern	B
recognition	I
letters	O
day	O
n	O
e	O
and	O
kerridge	O
d	O
f	O
a	O
general	O
maximum	B
likelihood	I
discriminant	O
biometrics	O
devijver	O
p	O
a	O
and	O
kittler	O
j	O
v	O
pattern	B
recognition	I
a	O
statistical	B
approach	O
prentice	O
hall	O
englewood	O
cliffs	O
djeroski	O
s	O
cestnik	O
b	O
and	O
petrovski	O
i	O
using	O
the	O
m-estimate	O
in	O
rule	O
induction	O
j	O
computing	O
and	O
inf	O
technology	O
dubes	O
r	O
and	O
jain	O
a	O
k	O
clustering	B
techniques	O
the	O
user	O
s	O
dilemma	O
pattern	B
recognition	I
duin	O
r	O
p	O
w	O
on	O
the	O
choice	O
of	O
smoothing	B
parameters	I
for	O
parzen	O
estimators	O
of	O
probability	O
density	O
functions	O
ieee	O
transactions	O
on	O
computers	O
dvorak	O
d	O
l	O
expert	B
systems	I
for	O
monitoring	O
and	O
control	O
technical	B
report	O
technical	B
report	O
artificial	O
intelligence	O
laboratory	O
the	O
university	O
of	O
texas	O
at	O
austin	O
d	O
zeroski	O
s	O
control	O
of	O
inverted	O
pendulum	O
b	O
sc	O
thesis	O
faculty	O
of	O
electrical	O
engineering	O
and	O
computer	O
science	O
university	O
of	O
ljubljana	O
slovenian	O
efron	O
b	O
estimating	O
the	O
error	B
rate	I
of	O
a	O
prediction	O
rule	O
improvements	O
on	O
cross	B
validation	I
j	O
amer	O
stat	O
ass	O
references	O
enas	O
g	O
g	O
and	O
choi	O
s	O
c	O
choice	O
of	O
the	O
smoothing	B
parameter	I
and	O
efficiency	O
of	O
nearest	B
neighbour	I
classification	B
comput	O
math	O
applic	O
enterline	O
l	O
l	O
strategic	O
requirements	O
for	O
total	O
facility	O
automation	O
control	O
engi	O
the	O
neering	O
ersoy	O
o	O
k	O
and	O
hong	O
d	O
parallel	O
self-organizing	O
hierarchical	O
neural	B
networks	I
for	O
vision	O
and	O
systems	O
control	O
in	O
kaynak	O
o	O
editor	O
intelligent	O
motion	O
control	O
proceedings	O
of	O
the	O
ieee	O
international	O
workshop	O
new	O
york	O
ieee	O
fahlman	O
s	O
e	O
an	O
empirical	O
study	O
of	O
learning	O
speed	B
in	O
back-propagation	O
tech	O
nical	O
report	O
carnegie	O
mellon	O
university	O
usa	O
fahlman	O
s	O
e	O
faster	O
learning	O
variation	O
on	O
back-propagation	O
an	O
empirical	O
study	O
in	O
proccedings	O
of	O
the	O
connectionist	O
models	O
summer	O
school	O
morgan	O
kaufmann	O
fahlman	O
s	O
e	O
the	O
cascade-correlation	O
learning	O
algorithm	O
on	O
the	O
monk	O
s	O
problems	O
in	O
thrun	O
s	O
bala	O
j	O
bloedorn	O
e	O
and	O
bratko	O
i	O
editors	O
the	O
monk	O
s	O
problems	O
a	O
performance	O
comparison	O
of	O
different	O
learning	O
algorithms	O
pages	O
carnegie	O
mellon	O
university	O
computer	O
science	O
department	O
fahlman	O
s	O
e	O
the	O
recurrent	O
cascade-correlation	O
architecture	O
technical	B
report	O
carnegie	O
mellon	O
university	O
fahlman	O
s	O
e	O
and	O
lebiere	O
c	O
the	O
cascade	B
correlation	B
learning	O
architecture	O
in	O
tourzetsky	O
d	O
s	O
editor	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
morgan	O
kaufmann	O
fahrmeir	O
l	O
haussler	O
w	O
and	O
tutz	O
g	O
diskriminanzanalyse	O
in	O
fahrmeir	O
l	O
and	O
hamerle	O
a	O
editors	O
multivariate	O
statistische	O
verfahren	O
verlag	O
de	O
gruyter	O
berlin	O
fienberg	O
s	O
the	O
analysis	O
of	O
cross-classified	O
categorical	O
data	O
mit	O
press	O
cam	O
bridge	O
mass	O
fisher	O
d	O
h	O
and	O
mckusick	O
k	O
b	O
an	O
empirical	O
comparison	O
of	O
and	O
backpropagation	O
in	O
ijcai	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
fisher	O
d	O
h	O
and	O
mckusick	O
k	O
b	O
et	O
al	O
processing	O
issues	O
in	O
comparisons	O
of	O
symbolic	O
and	O
connectionist	O
learning	O
systems	O
in	O
spatz	O
b	O
editor	O
proceedings	O
of	O
the	O
sixth	O
international	O
workshop	O
on	O
machine	O
learning	O
cornell	O
university	O
ithaca	O
new	O
york	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
fisher	O
r	O
a	O
the	O
use	O
of	O
multiple	O
measurements	O
in	O
taxonomic	B
problems	O
annals	O
of	O
eugenics	O
fisher	O
r	O
a	O
the	O
statistical	B
utilisation	O
of	O
multiple	O
measurements	O
ann	O
eugen	O
fix	O
e	O
and	O
hodges	O
j	O
l	O
discriminatory	O
analysis	O
nonparametric	O
estimation	O
consistency	O
properties	O
report	O
no	O
project	O
no	O
uasf	O
school	O
of	O
aviation	O
medicine	O
randolph	O
field	O
texas	O
frean	O
m	O
short	O
paths	O
and	O
small	O
nets	O
optimizing	O
neural	O
computation	O
phd	O
thesis	O
university	O
of	O
edinburgh	O
uk	O
frean	O
m	O
the	O
upstart	B
algorithm	O
a	O
method	O
for	O
constructing	O
and	O
training	O
feed	O
forward	B
neural	B
networks	I
neural	O
computation	O
frey	O
p	O
w	O
and	O
slate	O
d	O
j	O
letter	B
recognition	I
using	O
holland-style	O
adaptive	O
classifiers	O
machine	O
learning	O
references	O
friedman	O
j	O
regularized	O
discriminant	O
analysis	O
j	O
amer	O
statist	O
assoc	O
friedman	O
j	O
h	O
smart	B
user	O
s	O
guide	O
tech	O
report	O
laboratory	O
of	O
computational	O
statistics	O
department	O
of	O
statistics	O
stanford	O
university	O
friedman	O
j	O
h	O
multivariate	O
adaptive	O
regression	O
splines	O
discussion	O
annals	O
of	O
statistics	O
friedman	O
j	O
h	O
and	O
stuetzle	O
w	O
projection	B
pursuit	I
regression	O
j	O
amer	O
statist	O
assoc	O
fukunaga	O
k	O
introduction	O
to	O
statistical	B
pattern	B
recognition	I
academic	O
press	O
edition	O
fukunaga	O
k	O
and	O
narendra	O
p	O
m	O
a	O
branch	O
and	O
bound	O
algorithm	O
for	O
computing	O
nearest	O
neighbours	O
ieee	O
trans	O
comput	O
networks	O
neural	B
networks	I
funahashi	O
k	O
on	O
the	O
approximate	O
realization	O
of	O
continuous	O
mappings	O
by	O
neural	O
fung	O
r	O
m	O
and	O
crawford	O
s	O
l	O
constructor	O
a	O
system	O
for	O
the	O
induction	O
of	O
probabilistic	O
models	O
in	O
proceedings	O
eighth	O
of	O
the	O
conference	O
on	O
artificial	O
intelligence	O
pages	O
boston	O
massachussetts	O
gallant	O
s	O
i	O
the	O
pocket	O
algorithm	O
for	O
perceptron	B
learning	O
technical	B
report	O
northeastern	O
university	O
college	O
of	O
computer	O
science	O
usa	O
gates	O
g	O
w	O
the	O
reduced	B
nearest	B
neighbour	I
rule	O
ieee	O
transactions	O
on	O
information	B
theory	I
geman	O
s	O
neural	B
networks	I
and	O
the	O
biasvariance	O
dilemma	O
neural	O
computation	O
geva	O
s	O
and	O
sitte	O
j	O
boxes	B
revisited	O
in	O
proceedings	O
of	O
the	O
icann	O
amster	O
dam	O
geva	O
s	O
and	O
sitte	O
j	O
the	O
cart-pole	O
experiment	O
as	O
a	O
benchmark	O
for	O
trainable	O
controllers	O
submitted	O
to	O
ieee	O
control	O
systems	O
magazine	O
gilbert	O
e	O
s	O
the	O
effect	O
of	O
unequal	O
variance	O
covariance	B
matrices	O
on	O
fisher	O
s	O
linear	B
discriminant	I
function	O
biometrics	O
glymour	O
c	O
scheines	O
r	O
spirtes	O
p	O
and	O
kelley	O
k	O
discovering	O
causal	O
structures	O
statistics	O
and	O
search	O
academic	O
press	O
new	O
york	O
goldberg	O
d	O
e	O
genetic	B
algorithms	I
in	O
search	O
optimization	O
and	O
machine	O
learning	O
addison-wesley	O
good	O
i	O
j	O
probability	O
and	O
the	O
weighing	O
of	O
evidence	O
griffin	O
london	O
goodman	O
r	O
m	O
and	O
smyth	O
p	O
the	O
induction	O
of	O
probabilistic	O
rule	O
sets	O
the	O
itrule	B
algorithm	O
in	O
spatz	O
b	O
editor	O
proceedings	O
of	O
the	O
sixth	O
international	O
workshop	O
on	O
machine	O
learning	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
gorman	O
r	O
p	O
and	O
sejnowski	O
t	O
j	O
analysis	O
of	O
hidden	B
units	O
in	O
a	O
layered	O
network	O
trained	O
to	O
classify	O
sonar	O
targets	O
neural	B
networks	I
habbema	O
j	O
d	O
f	O
hermans	O
j	O
and	O
van	O
der	O
burght	O
a	O
t	O
cases	O
of	O
doubt	O
in	O
allocation	O
problems	O
biometrika	O
hampshire	O
j	O
and	O
pearlmuter	O
b	O
equivalence	O
proofs	O
for	O
the	O
multilayer	O
perceptron	B
classifier	B
and	O
the	O
bayes	O
discriminant	O
function	O
in	O
proceedings	O
of	O
the	O
connectionist	O
models	O
summer	O
school	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
references	O
hand	O
d	O
j	O
discrimination	B
and	O
classification	B
john	O
wiley	O
chichester	O
hand	O
d	O
j	O
and	O
batchelor	O
b	O
g	O
an	O
edited	B
nearest	B
neighbour	I
rule	O
information	O
sciences	O
hanson	O
s	O
j	O
meiosis	O
networks	O
in	O
tourzetsky	O
d	O
s	O
editor	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
morgan	O
kaufmann	O
hart	O
p	O
e	O
the	O
condensed	B
nearest	B
neighbour	I
rule	O
ieee	O
transactions	O
on	O
information	B
theory	I
h	O
aussler	O
w	O
m	O
empirische	O
ergebnisse	O
zur	O
diskriminationsverfahren	O
bei	O
kred	O
itscoringsystemen	O
zeitschrift	O
fur	O
operations	O
research	O
serie	O
b	O
h	O
aussler	O
w	O
m	O
methoden	O
der	O
punktebewertung	O
fur	O
kreditscoringsysteme	O
zeitschrift	O
fur	O
operations	O
research	O
h	O
aussler	O
w	O
m	O
punktebewertung	O
bei	O
kreditscoringsystemen	O
knapp	O
frankfurt	O
hebb	O
d	O
o	O
the	O
organisation	O
of	O
behaviour	O
john	O
wiley	O
and	O
sons	O
hecht-nielsen	O
r	O
neurocomputing	O
addison-wesley	O
reading	O
mass	O
herkovsits	O
e	O
and	O
cooper	O
g	O
f	O
kutat	O
o	O
an	O
entropy-driven	O
system	O
for	O
the	O
construction	O
of	O
probabilistic	O
expert	B
systems	I
from	O
databases	O
report	O
stanford	O
university	O
hermans	O
j	O
habbema	O
j	O
d	O
f	O
kasanmoentalib	O
t	O
k	O
d	O
and	O
raatgeven	O
j	O
w	O
manual	O
for	O
the	O
discriminant	O
analysis	O
program	O
leiden	O
the	O
netherlands	O
hertz	O
j	O
krogh	O
a	O
and	O
palmer	O
r	O
introduction	O
to	O
the	O
theory	O
of	O
neural	O
compu	O
tation	O
addison-wesley	O
higonnet	O
r	O
a	O
and	O
grea	O
r	O
a	O
logical	O
design	O
of	O
electrical	O
circuits	O
mcgraw-hill	O
book	O
co	O
ltd	O
hill	O
m	O
correspondence	B
analysis	I
in	O
encyclopaedia	O
of	O
statistical	B
sciences	O
vol	O
ume	O
pages	O
wiley	O
new	O
york	O
hinton	O
g	O
e	O
rumelhart	O
d	O
e	O
and	O
williams	O
r	O
j	O
learning	O
internal	O
representations	O
by	O
back-propagating	O
errors	O
in	O
rumelhart	O
d	O
e	O
mcclelland	O
j	O
l	O
and	O
the	O
pdp	O
research	O
group	O
editors	O
parallel	O
distributed	O
processing	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
volume	O
chapter	O
mit	O
press	O
cambridge	O
ma	O
hinton	O
g	O
e	O
rumelhart	O
d	O
e	O
and	O
williams	O
r	O
j	O
learning	O
representatinos	O
by	O
back-propagating	O
errors	O
nature	O
hoehfeld	O
m	O
and	O
fahlman	O
s	O
e	O
learning	O
with	O
limited	O
precision	O
using	O
the	O
cascadecorrelation	O
algorithm	O
technical	B
report	O
carnegie	O
mellon	O
university	O
hofmann	O
h	O
j	O
die	O
anwendung	O
des	O
cart-verfahrens	O
zur	O
statistischen	O
bonitatsanalyse	O
von	O
konsumentenkrediten	O
zeitschrift	O
fur	O
betriebswirtschaft	O
h	O
jsgaard	O
s	O
skj	O
th	O
f	O
and	O
thiesson	O
b	O
user	O
s	O
guide	O
to	O
bifrost	B
aalborg	O
university	O
aalborg	O
denmark	O
holland	O
j	O
h	O
adaptation	O
in	O
natural	O
and	O
artificial	O
systems	O
university	O
of	O
michigan	O
press	O
ann	O
arbor	O
michigan	O
holland	O
j	O
h	O
adaptation	O
in	O
natural	O
and	O
artificial	O
systems	O
university	O
of	O
michi	O
gan	O
press	O
ann	O
arbor	O
mi	O
references	O
huang	O
h	O
h	O
zhang	O
c	O
lee	O
s	O
and	O
wang	O
h	O
p	O
implementation	O
and	O
comparison	O
of	O
neural	O
network	O
learning	O
paradigms	O
back	O
propagation	O
simulated	O
annealing	O
and	O
tabu	O
search	O
in	O
dagli	O
c	O
kumara	O
s	O
and	O
shin	O
y	O
c	O
editors	O
intelligent	O
engineering	O
systems	O
through	O
artificial	O
neural	B
networks	I
proceedings	O
of	O
the	O
artificial	O
neural	B
networks	I
in	O
engineering	O
conference	O
new	O
york	O
american	O
society	O
of	O
mechanical	O
engineers	O
huang	O
w	O
y	O
and	O
lippmann	O
r	O
p	O
comparisons	O
between	O
neural	O
net	O
and	O
conventional	O
classifiers	O
in	O
proceedings	O
of	O
the	O
ieee	O
first	O
international	O
conference	O
on	O
neural	B
networks	I
pages	O
piscataway	O
nj	O
ieee	O
hunt	O
e	O
b	O
concept	B
learning	I
an	O
information	O
processing	O
problem	O
wiley	O
hunt	O
e	O
b	O
martin	O
j	O
and	O
stone	O
p	O
i	O
experiemnts	O
in	O
induction	O
academic	O
press	O
new	O
york	O
hunt	O
k	O
j	O
sbarbaro	O
d	O
zbikovski	O
r	O
and	O
gawthrop	O
p	O
j	O
neural	B
networks	I
for	O
control	O
systems	O
a	O
survey	O
automatica	O
jacobs	O
r	O
increased	O
rates	O
of	O
convergence	O
through	O
learning	O
rate	O
adaptation	O
neural	B
networks	I
jennet	O
b	O
teasdale	O
g	O
braakman	O
r	O
minderhoud	O
j	O
heiden	O
j	O
and	O
kurzi	O
t	O
prognosis	O
of	O
patients	O
with	O
severe	O
head	B
injury	I
neurosurgery	O
jones	O
d	O
s	O
elementary	O
information	B
theory	I
clarendon	O
press	O
oxford	O
karali	O
c	O
a	O
employing	O
linear	B
regression	I
in	O
regression	B
tree	I
leaves	O
in	O
proceedings	O
of	O
the	O
european	O
conference	O
on	O
artificial	O
intelligence	O
pages	O
wiley	O
sons	O
wien	O
austria	O
karali	O
c	O
a	O
and	O
gams	O
m	O
implementation	O
of	O
the	O
gynesis	O
pc	O
inductive	B
learning	I
system	O
in	O
proceedings	O
of	O
the	O
etan	O
conference	O
pages	O
novi	O
sad	O
slovenian	O
kass	O
g	O
v	O
an	O
exploratory	O
technique	O
for	O
investigating	O
large	O
quantities	O
of	O
categorical	O
data	O
appl	O
statist	O
kendall	O
m	O
g	O
stuart	O
a	O
and	O
ord	O
j	O
k	O
the	O
advanced	O
theory	O
of	O
statistics	O
vol	O
design	O
and	O
analysis	O
and	O
time	B
series	O
chapter	O
griffin	O
london	O
fourth	O
edition	O
king	O
r	O
d	O
lewis	O
r	O
a	O
muggleton	O
s	O
h	O
and	O
sternberg	O
m	O
j	O
e	O
drug	O
design	O
by	O
machine	O
learning	O
the	O
use	O
of	O
inductive	B
logic	I
programming	I
to	O
model	O
the	O
structure-activity	O
relationship	O
of	O
trimethoprim	O
analogues	O
binding	O
to	O
dihydrofolate	O
reductase	O
proceedings	O
of	O
the	O
national	O
academy	O
science	O
kirkwood	O
c	O
andrews	O
b	O
and	O
mowforth	O
p	O
automatic	O
detection	O
of	O
gait	O
events	O
a	O
case	O
study	O
using	O
inductive	B
learning	I
techniques	O
journal	O
of	O
biomedical	O
engineering	O
knoll	O
u	O
kostenoptimiertes	O
prunen	O
in	O
entscheidungsbaumen	O
daimler-benz	O
forschung	O
und	O
technik	O
ulm	O
kohonen	B
t	O
self-organization	O
and	O
associative	O
memory	B
springer	O
verlag	O
berlin	O
kohonen	B
t	O
self-organization	O
and	O
associative	O
memory	B
springer-verlag	O
berlin	O
edition	O
kohonen	B
t	O
barna	O
g	O
and	O
chrisley	O
r	O
statistical	B
pattern	B
recognition	I
with	O
neural	B
networks	I
benchmarking	O
studies	O
in	O
ieee	O
international	O
conference	O
on	O
neural	B
networks	I
volume	O
pages	O
new	O
york	O
diego	O
ieee	O
references	O
kononenko	O
i	O
and	O
bratko	O
i	O
information-based	B
evaluation	O
criterion	O
for	O
classifier	B
s	O
performance	O
machine	O
learning	O
kressel	O
u	O
the	O
impact	O
of	O
the	O
learning	O
set	O
size	O
in	O
handwritten	O
digits	O
recognition	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
artifical	O
neural	B
networks	I
helsinki	O
finland	O
krishnaiah	O
p	O
and	O
kanal	O
l	O
editors	O
classification	B
pattern	B
recognition	I
and	O
reduction	O
of	O
dimensionality	O
volume	O
of	O
handbook	O
of	O
statistics	O
north	O
holland	O
amsterdam	O
kwakernaak	O
h	O
and	O
sivan	O
r	O
linear	O
optimal	O
control	O
systems	O
john	O
wiley	O
lachenbruch	O
p	O
and	O
mickey	O
r	O
estimation	O
of	O
error	O
rates	O
in	O
discriminant	O
analysis	O
technometrics	O
lachenbruch	O
p	O
a	O
and	O
mickey	O
m	O
r	O
discriminant	O
analysis	O
hafner	O
press	O
new	O
york	O
lang	O
k	O
j	O
waibel	O
a	O
h	O
and	O
hinton	O
g	O
e	O
a	O
time-delay	O
neural	O
network	O
architecture	O
for	O
isolated	O
word	O
recognition	O
neural	B
networks	I
lauritzen	O
s	O
l	O
and	O
spiegelhalter	O
d	O
j	O
local	O
computations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
and	O
their	O
application	O
to	O
expert	B
systems	I
discussion	O
j	O
royal	O
statist	O
soc	O
series	O
b	O
lawrence	O
e	O
c	O
and	O
smith	O
l	O
d	O
an	O
analysis	O
of	O
default	B
risk	O
in	O
mobile	O
home	O
credit	O
j	O
banking	O
and	O
finance	O
le	O
cun	O
y	O
boser	O
b	O
denker	O
j	O
s	O
henderson	O
d	O
howard	O
r	O
e	O
hubbard	O
w	O
and	O
d	O
j	O
l	O
backpropagation	O
applied	O
to	O
handwritten	O
zip	O
code	O
recognition	O
neural	O
computation	O
lee	O
c	O
fuzzy	O
logic	O
in	O
control	O
systems	O
fuzzy	O
logic	O
controller	O
part	O
part	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
leech	O
w	O
j	O
a	O
rule	O
based	O
process	O
control	O
method	O
with	O
feedback	O
advances	O
in	O
instrumentation	O
leitch	O
r	O
knowledge	O
based	O
control	O
selecting	O
the	O
right	O
tool	O
for	O
the	O
job	O
in	O
preprints	O
of	O
the	O
ifacifipimacs	O
international	O
symposium	O
on	O
artificial	O
intelligence	O
in	O
real-time	O
control	O
pages	O
delft	O
the	O
netherlands	O
leitch	O
r	O
r	O
and	O
francis	O
j	O
c	O
towards	O
intelligent	O
control	O
systems	O
in	O
mamdani	O
a	O
and	O
efstathiou	O
j	O
editors	O
expert	B
systems	I
and	O
optimisation	B
in	O
process	O
control	O
pages	O
aldershot	O
england	O
gower	O
technical	B
press	O
luttrell	O
s	O
p	O
derivation	O
of	O
a	O
class	B
of	O
training	O
algorithms	O
ieee	O
transactions	O
on	O
neural	B
networks	I
luttrell	O
s	O
p	O
a	O
bayesian	O
analysis	O
of	O
vector	O
quantization	O
algorithms	O
submitted	O
to	O
neural	O
computation	O
machado	O
s	O
g	O
two	O
statistics	O
for	O
testing	O
for	O
multivariate	B
normality	I
biometrika	O
mackay	O
d	O
the	O
evidence	O
framework	O
applied	O
to	O
classification	B
networks	O
neural	O
computation	O
mackay	O
d	O
a	O
practical	O
bayesian	O
framework	O
for	O
backpropagation	O
networks	O
neural	O
computation	O
references	O
makarovi	O
c	O
a	O
a	O
qualitative	O
way	O
of	O
solving	O
the	O
pole	B
balancing	I
problem	O
technical	B
report	O
memorandum	O
university	O
of	O
twente	O
also	O
in	O
machine	O
intelligence	O
j	O
hayes	O
d	O
michie	O
e	O
tyugu	O
oxford	O
university	O
press	O
pp	O
mardia	O
k	O
v	O
applications	O
of	O
some	O
measures	B
of	O
multivariate	B
skewness	B
and	O
kurtosis	B
in	O
testing	O
normality	O
and	O
robustness	O
studies	O
sankhya	O
b	O
mardia	O
k	O
v	O
kent	O
j	O
t	O
and	O
bibby	O
j	O
m	O
multivariate	O
analysis	O
academic	O
press	O
london	O
marks	O
s	O
and	O
dunn	O
o	O
j	O
discriminant	O
functions	O
when	O
covariance	B
matrices	O
are	O
unequal	O
j	O
amer	O
statist	O
assoc	O
mccarthy	O
j	O
and	O
hayes	O
p	O
j	O
some	O
philosophical	O
problems	O
from	O
the	O
standpoint	O
of	O
artificial	O
intelligence	O
in	O
meltzer	O
b	O
and	O
michie	O
d	O
editors	O
machine	O
intelligence	O
pages	O
eup	O
edinburgh	O
mccullagh	O
p	O
and	O
nelder	O
j	O
a	O
generalized	O
linear	O
models	O
chapman	O
and	O
hall	O
london	O
edition	O
mcculloch	O
w	O
s	O
and	O
pitts	O
w	O
a	O
logical	O
calculus	O
of	O
the	O
ideas	O
immanent	O
in	O
nervous	O
activity	O
forms	O
bulletin	O
of	O
methematical	O
biophysics	O
mclachlan	O
g	O
j	O
discriminant	O
analysis	O
and	O
statistical	B
pattern	B
recognition	I
john	O
wiley	O
new	O
york	O
meyer-br	O
otz	O
g	O
and	O
sch	O
urmann	O
j	O
methoden	O
der	O
automatischen	O
zeichenerken	O
nung	O
akademie-verlag	O
berlin	O
m	O
ezard	O
m	O
and	O
nadal	O
j	O
p	O
learning	O
in	O
feed-forward	O
layered	O
networks	O
the	O
tiling	B
algorithm	I
journal	O
of	O
physics	O
a	O
mathematics	O
general	O
m	O
g	O
kendall	O
a	O
s	O
and	O
ord	O
j	O
the	O
advanced	O
theory	O
of	O
statistics	O
vol	O
distri	O
bution	O
theory	O
griffin	O
london	O
fourth	O
edition	O
michalski	O
r	O
s	O
on	O
the	O
quasi-minimal	O
solution	O
of	O
the	O
general	O
covering	O
problem	O
in	O
proc	O
of	O
the	O
fifth	O
internat	O
symp	O
on	O
inform	O
processing	O
pages	O
bled	O
slovenia	O
michalski	O
r	O
s	O
discovering	O
classification	B
rules	O
using	O
variable	O
valued	O
logic	O
system	O
in	O
third	O
international	O
joint	O
conference	O
on	O
artificial	O
intelligence	O
pages	O
michalski	O
r	O
s	O
a	O
theory	O
and	O
methodology	O
of	O
inductive	B
learning	I
in	O
r	O
s	O
michalski	O
j	O
g	O
c	O
and	O
mitchell	O
t	O
m	O
editors	O
machine	O
learning	O
an	O
artificial	O
intelligence	O
approach	O
tioga	O
palo	O
alto	O
michalski	O
r	O
s	O
and	O
chilauski	O
r	O
l	O
knowledge	O
acquisition	O
by	O
encoding	O
expert	O
rules	O
versus	O
computer	O
induction	O
from	O
examples	O
a	O
case	O
study	O
involving	O
soybean	B
pathology	O
int	O
j	O
man-machine	O
studies	O
michalski	O
r	O
s	O
and	O
larson	O
j	O
b	O
selection	O
of	O
the	O
most	O
representative	O
training	O
examples	O
and	O
incremental	O
generation	O
of	O
hypothesis	O
the	O
underlying	O
methodology	O
and	O
the	O
description	O
of	O
programs	O
esel	O
and	O
technical	B
report	O
dept	O
of	O
computer	O
sciencence	O
u	O
of	O
illinois	O
urbana	O
michie	O
d	O
problems	O
of	O
computer-aided	O
concept	B
formation	O
in	O
quinlan	O
j	O
r	O
editor	O
applications	O
of	O
expert	B
systems	I
volume	O
pages	O
addison-wesley	O
london	O
michie	O
d	O
personal	O
models	O
of	O
rationality	O
j	O
statist	O
planning	O
and	O
inference	O
references	O
michie	O
d	O
methodologies	O
from	O
machine	O
learning	O
in	O
data	O
analysis	O
and	O
software	O
computer	O
journal	O
michie	O
d	O
and	O
al	O
attar	O
a	O
use	O
of	O
sequential	O
bayes	O
with	O
class	B
probability	I
trees	I
in	O
hayes	O
j	O
michie	O
d	O
and	O
tyugu	O
e	O
editors	O
machine	O
intelligence	O
pages	O
oxford	O
university	O
press	O
michie	O
d	O
and	O
bain	O
m	O
machine	O
acquisition	O
of	O
concepts	O
from	O
sample	O
data	O
in	O
kopec	O
d	O
and	O
thompson	O
r	O
b	O
editors	O
artificial	O
intelligence	O
and	O
intelligent	O
tutoring	O
systems	O
pages	O
ellis	O
horwood	O
ltd	O
chichester	O
michie	O
d	O
bain	O
m	O
and	O
hayes-michie	O
j	O
cognitive	O
models	O
from	O
subcognitive	O
skills	O
in	O
grimble	O
m	O
mcghee	O
j	O
and	O
mowforth	O
p	O
editors	O
knowledge-based	O
systems	O
in	O
industrial	O
control	O
pages	O
stevenage	O
peter	O
peregrinus	O
michie	O
d	O
and	O
camacho	O
r	O
building	O
symbolic	O
representations	O
of	O
intuitive	O
realtime	O
skills	O
from	O
performance	O
data	O
to	O
appear	O
in	O
machine	O
intelligence	O
and	O
inductive	B
learning	I
vol	O
furukawa	O
k	O
and	O
muggleton	O
s	O
h	O
new	O
series	O
of	O
machine	O
intelligence	O
ed	O
in	O
chief	O
d	O
michie	O
oxford	O
oxford	O
university	O
press	O
michie	O
d	O
and	O
chambers	O
r	O
a	O
boxes	B
an	O
experiment	O
in	O
adaptive	O
control	O
in	O
dale	O
e	O
and	O
michie	O
d	O
editors	O
machine	O
intelligence	O
oliver	O
and	O
boyd	O
edinburgh	O
michie	O
d	O
and	O
chambers	O
r	O
a	O
boxes	B
an	O
experiment	O
in	O
adaptive	O
control	O
in	O
dale	O
e	O
and	O
michie	O
d	O
editors	O
machine	O
intelligence	O
pages	O
edinburgh	O
university	O
press	O
michie	O
d	O
and	O
sammut	O
c	O
machine	O
learning	O
from	O
real-time	O
input-output	O
behaviour	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
design	O
to	O
manufacture	O
in	O
modern	O
industry	O
pages	O
miller	O
w	O
t	O
sutton	O
r	O
s	O
and	O
werbos	O
p	O
j	O
editors	O
neural	B
networks	I
for	O
control	O
the	O
mit	O
press	O
minsky	O
m	O
c	O
and	O
papert	O
s	O
perceptrons	O
mit	O
press	O
cambridge	O
ma	O
usa	O
m	O
ller	O
m	O
a	O
scaled	O
conjugate	B
gradient	I
algorithm	O
for	O
fast	O
supervised	B
learning	I
neural	B
networks	I
mooney	O
r	O
shavlik	O
j	O
towell	O
g	O
and	O
gove	O
a	O
an	O
experimental	O
comparison	O
of	O
symbolic	O
and	O
connectionist	O
learning	O
algorithms	O
in	O
ijcai	O
proceedings	O
of	O
the	O
eleventh	O
international	O
joint	O
conference	O
on	O
artificial	O
intelligence	O
detroit	O
mi	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
for	O
international	O
joint	O
conferences	O
on	O
artificial	O
intelligence	O
muggleton	O
s	O
h	O
logic	O
and	O
learning	O
turing	O
s	O
legacy	O
in	O
muggleton	O
s	O
h	O
and	O
michie	O
d	O
furukaw	O
k	O
editors	O
machine	O
intelligence	O
oxford	O
university	O
press	O
oxford	O
muggleton	O
s	O
h	O
bain	O
m	O
hayes-michie	O
j	O
e	O
and	O
michie	O
d	O
an	O
experimental	O
comparison	O
of	O
learning	O
formalisms	O
in	O
sixth	O
internat	O
workshop	O
on	O
mach	O
learning	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
muggleton	O
s	O
h	O
and	O
buntine	O
w	O
machine	O
invention	O
of	O
first-order	O
predicates	O
by	O
inverting	O
resolution	O
in	O
r	O
s	O
michalski	O
t	O
m	O
m	O
and	O
carbonell	O
j	O
g	O
editors	O
proceedings	O
of	O
the	O
fifth	O
international	O
machine	O
learning	O
conference	O
pages	O
morgan	O
kaufmann	O
ann	O
arbor	O
michigan	O
references	O
muggleton	O
s	O
h	O
and	O
feng	O
c	O
efficient	O
induction	O
of	O
logic	O
programs	O
in	O
first	O
international	O
conference	O
on	O
algorithmic	O
learning	O
theory	O
pages	O
tokyo	O
japan	O
japanese	O
society	O
for	O
artificial	O
intellligence	O
neapolitan	O
e	O
probabilistic	O
reasoning	O
in	O
expert	B
systems	I
john	O
wiley	O
nowlan	O
s	O
and	O
hinton	O
g	O
simplifying	O
neural	B
networks	I
by	O
soft	O
weight-sharing	O
neural	O
computation	O
odetayo	O
m	O
o	O
balancing	O
a	O
pole-cart	O
system	O
using	O
genetic	B
algorithms	I
master	O
s	O
thesis	O
department	O
of	O
computer	O
science	O
university	O
of	O
strathclyde	O
odetayo	O
m	O
o	O
and	O
mcgregor	O
d	O
r	O
genetic	B
algorithm	O
for	O
inducing	O
control	O
rules	O
for	O
a	O
dynamic	O
system	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
genetic	B
algorithms	I
pages	O
morgan	O
kaufmann	O
ozturk	O
a	O
and	O
romeu	O
j	O
l	O
a	O
new	O
method	O
for	O
assessing	O
multivariate	B
normality	I
with	O
graphical	O
applications	O
communications	O
in	O
statistics	O
simulation	O
pearce	O
d	O
the	O
induction	O
of	O
fault	O
diagnosis	O
systems	O
from	O
qualitative	O
models	O
in	O
proc	O
seventh	O
nat	O
conf	O
on	O
art	O
intell	O
pages	O
st	O
paul	O
minnesota	O
pearl	O
j	O
probabilistic	O
reasoning	O
in	O
intelligent	O
systems	O
networks	O
of	O
plausible	O
inference	O
morgan	O
kaufmann	O
san	O
mateo	O
piper	O
j	O
and	O
granum	O
e	O
on	O
fully	O
automatic	O
feature	O
measurement	O
for	O
banded	O
chromosome	O
classification	B
cytometry	O
plotkin	O
g	O
d	O
a	O
note	O
on	O
inductive	O
generalization	O
in	O
meltzer	O
b	O
and	O
michie	O
d	O
editors	O
machine	O
intelligence	O
pages	O
edinburgh	O
university	O
press	O
poggio	O
t	O
and	O
girosi	O
f	O
networks	O
for	O
approximation	O
and	O
learning	O
proceedings	O
of	O
the	O
ieee	O
pomerleau	O
d	O
a	O
alvinn	O
an	O
autonomous	O
land	O
vehicle	B
in	O
a	O
neural	O
network	O
in	O
touretzky	O
d	O
s	O
editor	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
morgan	O
kaufmann	O
publishers	O
san	O
mateo	O
ca	O
prager	O
r	O
w	O
and	O
fallside	O
f	O
the	O
modified	O
kanerva	O
model	O
for	O
automatic	O
speech	O
recognition	O
computer	O
speech	O
and	O
language	O
press	O
w	O
h	O
flannery	O
b	O
p	O
teukolsky	O
s	O
a	O
and	O
vettering	O
w	O
t	O
numerical	O
recipes	O
in	O
c	O
the	O
art	O
of	O
scientific	O
computing	O
cambridge	O
university	O
press	O
cambridge	O
quinlan	O
j	O
r	O
induction	O
of	O
decision	B
trees	I
machine	O
learning	O
quinlan	O
j	O
r	O
generating	O
production	O
rules	O
from	O
decision	B
trees	I
in	O
international	O
joint	O
conference	O
on	O
artificial	O
intelligence	O
pages	O
milan	O
quinlan	O
j	O
r	O
generating	O
production	O
rules	O
from	O
decision	B
trees	I
in	O
proceedings	O
of	O
the	O
tenth	O
international	O
joint	O
conference	O
on	O
artificial	O
intelligence	O
pages	O
morgan	O
kaufmann	O
san	O
mateo	O
ca	O
quinlan	O
j	O
r	O
simplifying	O
decision	B
trees	I
int	O
j	O
man-machine	O
studies	O
quinlan	O
j	O
r	O
learning	O
logical	O
definitions	O
from	O
relations	O
machine	O
learning	O
quinlan	O
j	O
r	O
programs	O
for	O
machine	O
learning	O
morgan	O
kaufmann	O
san	O
mateo	O
ca	O
quinlan	O
j	O
r	O
compton	O
p	O
j	O
horn	O
k	O
a	O
and	O
lazarus	O
l	O
inductive	O
knowledge	O
acquisition	O
a	O
case	O
study	O
in	O
proceedings	O
of	O
the	O
second	O
australian	O
conference	O
on	O
references	O
applications	O
of	O
expert	B
systems	I
pages	O
sydney	O
new	O
south	O
wales	O
institute	O
of	O
technology	O
reaven	O
g	O
m	O
and	O
miller	O
r	O
g	O
an	O
attempt	O
to	O
define	O
the	O
nature	O
of	O
chemical	O
diabetes	B
using	O
a	O
multidimensional	O
analysis	O
diabetologia	O
refenes	O
a	O
n	O
and	O
vithlani	O
s	O
constructive	O
learning	O
by	O
specialisation	O
in	O
proceed	O
ings	O
of	O
the	O
international	O
conference	O
on	O
artificial	O
neural	B
networks	I
helsinki	O
finland	O
remme	O
j	O
habbema	O
j	O
d	O
f	O
and	O
hermans	O
j	O
a	O
simulative	O
comparison	O
of	O
linear	O
quadratic	O
and	O
kernel	O
discrimination	B
j	O
statist	O
comput	O
simul	O
renals	O
s	O
and	O
rohwer	O
r	O
phoneme	O
classification	B
experiments	O
using	O
radial	O
basis	O
functions	O
in	O
proceedings	O
of	O
the	O
international	O
joint	O
conference	O
on	O
neural	B
networks	I
volume	O
i	O
pages	O
washington	O
dc	O
renders	O
j	O
m	O
and	O
nordvik	O
j	O
p	O
genetic	B
algorithms	I
for	O
process	O
control	O
a	O
survey	O
in	O
preprints	O
of	O
the	O
ifacifipimacs	O
international	O
symposium	O
on	O
artificial	O
intelligence	O
in	O
real-time	O
control	O
pages	O
delft	O
the	O
netherlands	O
reynolds	O
j	O
c	O
transformational	O
systems	O
and	O
the	O
algebraic	O
structure	O
of	O
atomic	O
formulas	O
in	O
meltzer	O
b	O
and	O
michie	O
d	O
editors	O
machine	O
intelligence	O
pages	O
edinburgh	O
university	O
press	O
ripley	O
b	O
statistical	B
aspects	O
of	O
neural	B
networks	I
in	O
barndorff-nielsen	O
o	O
cox	O
d	O
jensen	O
j	O
and	O
kendall	O
w	O
editors	O
chaos	O
and	O
networks	O
statistical	B
and	O
probabilistic	O
aspects	O
chapman	O
and	O
hall	O
robinson	O
j	O
a	O
a	O
machine	O
oriented	O
logic	O
based	O
on	O
the	O
resolution	O
principle	O
journal	O
of	O
the	O
acm	O
rohwer	O
r	O
description	O
and	O
training	O
of	O
neural	O
network	O
dynamics	O
in	O
pasemann	O
f	O
and	O
doebner	O
h	O
editors	O
neurodynamics	O
proceedings	O
of	O
the	O
summer	O
workshop	O
clausthal	O
germany	O
world	O
scientific	O
rohwer	O
r	O
neural	B
networks	I
for	O
time-varying	O
data	O
in	O
murtagh	O
f	O
editor	O
neural	B
networks	I
for	O
statistical	B
and	O
economic	O
data	O
pages	O
statistical	B
office	O
of	O
the	O
european	O
communities	O
luxembourg	O
rohwer	O
r	O
time	B
trials	O
on	O
second-order	B
and	O
variable-learning-rate	O
algorithms	O
in	O
lippmann	O
r	O
moody	O
j	O
and	O
touretzky	O
d	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
volume	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
rohwer	O
r	O
a	O
representation	O
of	O
representation	O
applied	O
to	O
a	O
discussion	O
of	O
variable	O
binding	O
technical	B
report	O
dept	O
of	O
computer	O
science	O
and	O
applied	O
maths	O
aston	O
university	O
rohwer	O
r	O
and	O
cressy	O
d	O
phoneme	O
classification	B
by	O
boolean	O
networks	O
in	O
proceedings	O
of	O
the	O
european	O
conference	O
on	O
speech	O
communication	O
and	O
technology	O
pages	O
paris	O
rohwer	O
r	O
grant	O
b	O
and	O
limb	O
p	O
r	O
towards	O
a	O
connectionist	O
reasoning	O
system	O
british	O
telecom	O
technology	O
journal	O
rohwer	O
r	O
and	O
renals	O
s	O
training	O
recurrent	B
networks	I
in	O
personnaz	O
l	O
and	O
dreyfus	O
g	O
editors	O
neural	B
networks	I
from	O
models	O
to	O
applications	O
pages	O
i	O
d	O
s	O
e	O
t	O
paris	O
rosenblatt	O
f	O
psychological	O
review	O
rosenblatt	O
f	O
principles	O
of	O
neurodynamics	O
spartan	O
books	O
new	O
york	O
references	O
rumelhart	O
d	O
e	O
hinton	O
g	O
e	O
and	O
j	O
w	O
r	O
learning	O
internal	O
representations	O
by	O
error	O
propagation	O
in	O
rumelhart	O
d	O
e	O
and	O
mcclelland	O
j	O
l	O
editors	O
parallel	O
distributed	O
processing	O
volume	O
pages	O
mit	O
press	O
cambridge	O
ma	O
sakawa	O
y	O
and	O
shinido	O
y	O
optimal	O
control	O
of	O
container	O
crane	O
automatica	O
sammut	O
c	O
experimental	O
results	O
from	O
an	O
evaluation	O
of	O
algorithms	O
that	O
learn	O
to	O
control	O
dynamic	O
systems	O
in	O
laird	O
j	O
editor	O
proceedings	O
of	O
the	O
fifth	O
international	O
conference	O
on	O
machine	O
learning	O
ann	O
arbor	O
michigan	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
sammut	O
c	O
recent	O
progress	O
with	O
boxes	B
to	O
appear	O
in	O
machine	O
intelligence	O
and	O
inductive	B
learning	I
vol	O
furukawa	O
k	O
and	O
muggleton	O
s	O
h	O
new	O
series	O
of	O
machine	O
intelligence	O
ed	O
in	O
chief	O
d	O
michie	O
oxford	O
oxford	O
university	O
press	O
sammut	O
c	O
and	O
cribb	O
j	O
is	O
learning	O
rate	O
a	O
good	O
performance	O
criterion	O
of	O
learning	O
in	O
proceedings	O
of	O
the	O
seventh	O
international	O
machine	O
learning	O
conference	O
pages	O
austin	O
texas	O
morgan	O
kaufmann	O
sammut	O
c	O
hurst	O
s	O
kedzier	O
d	O
and	O
michie	O
d	O
learning	O
to	O
fly	O
in	O
sleeman	O
d	O
and	O
edwards	O
p	O
editors	O
proceedings	O
of	O
the	O
ninth	O
international	O
workshop	O
on	O
machine	O
learning	O
pages	O
morgan	O
kaufmann	O
sammut	O
c	O
and	O
michie	O
d	O
controlling	O
a	O
black	O
box	O
simulation	O
of	O
a	O
space	O
craft	O
ai	O
magazine	O
sammut	O
c	O
a	O
and	O
banerji	O
r	O
b	O
learning	O
concepts	O
by	O
asking	O
questions	O
in	O
r	O
s	O
michalski	O
j	O
c	O
and	O
mitchell	O
t	O
editors	O
machine	O
learning	O
an	O
artificial	O
intelligence	O
approach	O
vol	O
pages	O
morgan	O
kaufmann	O
los	O
altos	O
california	O
sas	O
statistical	B
analysis	O
system	O
sas	O
institute	O
inc	O
cary	O
nc	O
version	O
edition	O
scalero	O
r	O
and	O
tepedelenlioglu	O
n	O
a	O
fast	O
new	O
algorithm	O
for	O
training	O
feedforward	O
neural	B
networks	I
ieee	O
transactions	O
on	O
signal	O
processing	O
schalkoff	O
r	O
j	O
pattern	O
recognotion	O
statistical	B
structural	O
and	O
neural	O
ap	O
proaches	O
wiley	O
singapore	O
schoppers	O
m	O
real-time	O
knowledge-based	O
control	O
systems	O
communications	O
of	O
the	O
acm	O
schumann	O
m	O
lehrbach	O
t	O
and	O
bahrs	O
p	O
versuche	O
zur	O
kreditwurdigkeitsprognose	O
mit	O
kunstlichen	O
neuronalen	O
netzen	O
universitat	O
gottingen	O
scott	O
d	O
w	O
multivariate	O
density	B
estimation	I
theory	O
practice	O
and	O
visualization	O
john	O
wiley	O
new	O
york	O
sethi	O
i	O
k	O
and	O
otten	O
m	O
comparison	O
between	O
entropy	B
net	O
and	O
decision	O
tree	O
classifiers	O
in	O
proceedings	O
of	O
the	O
international	O
joint	O
conference	O
on	O
neural	B
networks	I
pages	O
ann	O
arbor	O
mi	O
ieee	O
neural	B
networks	I
council	O
shadmehr	O
r	O
and	O
d	O
argenio	O
z	O
a	O
comparison	O
of	O
a	O
neural	O
network	O
based	O
estimator	O
and	O
two	O
statistical	B
estimators	O
in	O
a	O
sparse	O
and	O
noisy	B
environment	O
in	O
proceedings	O
of	O
the	O
international	O
joint	O
conference	O
on	O
neural	B
networks	I
pages	O
ann	O
arbor	O
mi	O
ieee	O
neural	B
networks	I
council	O
shapiro	O
a	O
d	O
structured	B
induction	I
in	O
expert	B
systems	I
addison	O
wesley	O
london	O
references	O
shapiro	O
a	O
d	O
and	O
michie	O
d	O
a	O
self-commenting	O
facility	O
for	O
inductively	O
synthesized	O
end-game	O
expertise	O
in	O
beal	O
d	O
f	O
editor	O
advances	O
in	O
computer	O
chess	O
pergamon	O
oxford	O
shapiro	O
a	O
d	O
and	O
niblett	O
t	O
automatic	O
induction	O
of	O
classification	B
rules	O
for	O
a	O
chess	O
endgame	O
in	O
clarke	O
m	O
r	O
b	O
editor	O
advances	O
in	O
computer	O
chess	O
pergamon	O
oxford	O
shastri	O
l	O
and	O
ajjangadde	O
v	O
from	O
simple	O
associations	O
to	O
systematic	O
reasoning	O
a	O
connectionist	O
representation	O
of	O
rules	O
variables	O
and	O
dynamic	O
bindings	O
using	O
temporal	O
synchrony	O
behavioral	O
and	O
brain	O
sciences	O
to	O
appear	O
shavlik	O
j	O
mooney	O
r	O
and	O
towell	O
g	O
symbolic	O
and	O
neural	O
learning	O
algorithms	O
an	O
experimental	O
comparison	O
machine	O
learning	O
siebert	O
j	O
p	O
vehicle	B
recognition	I
using	O
rule	O
based	O
methods	O
turing	O
institute	O
silva	O
f	O
m	O
and	O
almeida	O
l	O
b	O
acceleration	O
techniques	O
for	O
the	O
backpropagation	O
algorithm	O
in	O
almeida	O
l	O
b	O
and	O
wellekens	O
c	O
j	O
editors	O
lecture	O
notes	O
in	O
computer	O
science	O
neural	B
networks	I
pages	O
springer-verlag	O
berlin	O
silverman	O
b	O
w	O
density	B
estimation	I
for	O
statistics	O
and	O
data	O
analysis	O
chapman	O
and	O
hall	O
london	O
smith	O
j	O
w	O
everhart	O
j	O
e	O
dickson	O
w	O
c	O
knowler	O
w	O
c	O
and	O
johannes	O
r	O
s	O
using	O
the	O
adap	O
learning	O
algorithm	O
to	O
forecast	O
the	O
onset	O
of	O
diabetes	B
mellitus	O
in	O
proceedings	O
of	O
the	O
symposium	O
on	O
computer	O
applications	O
and	O
medical	O
care	O
pages	O
ieee	O
computer	O
society	O
press	O
smith	O
p	O
l	O
curve	O
fitting	O
and	O
modeling	O
with	O
splines	O
using	O
statistical	B
variable	O
selection	O
techniques	O
technical	B
report	O
nasa	O
langley	O
research	O
center	O
hampton	O
va	O
snedecor	O
w	O
and	O
cochran	O
w	O
g	O
statistical	B
methods	O
edition	O
iowa	O
state	O
university	O
press	O
iowa	O
u	O
s	O
a	O
spiegelhalter	O
d	O
j	O
dawid	O
a	O
p	O
lauritzen	O
s	O
l	O
and	O
cowell	O
r	O
g	O
bayesian	O
analysis	O
in	O
expert	B
systems	I
statistical	B
science	O
spikovska	O
l	O
and	O
reid	O
m	O
b	O
an	O
empirical	O
comparison	O
of	O
and	O
honns	O
for	O
distortion	O
invariant	O
object	O
recognition	O
in	O
tools	O
for	O
artificial	O
intelligence	O
proceedings	O
of	O
the	O
international	O
ieee	O
conference	O
los	O
alamitos	O
ca	O
ieee	O
computer	O
society	O
press	O
spirtes	O
p	O
scheines	O
r	O
glymour	O
c	O
and	O
meek	O
c	O
tetradii	O
tools	O
for	O
discovery	O
srinivisan	O
v	O
and	O
kim	O
y	O
h	O
credit	O
granting	O
a	O
comparative	O
analysis	O
of	O
classifi	O
cation	O
procedures	O
the	O
journal	O
of	O
finance	O
statsci	O
s-plus	O
user	O
s	O
manual	O
technical	B
report	O
statsci	O
europe	O
oxford	O
u	O
k	O
stein	O
von	O
j	O
h	O
and	O
ziegler	O
w	O
the	O
prognosis	O
and	O
surveillance	O
of	O
risks	O
from	O
commercial	O
credit	O
borrowers	O
journal	O
of	O
banking	O
and	O
finance	O
stone	O
m	O
cross-validatory	O
choice	O
and	O
assessment	O
of	O
statistical	B
predictions	O
j	O
roy	O
statist	O
soc	O
discussion	O
switzer	O
p	O
extensions	O
of	O
linear	B
discriminant	I
analysis	O
for	O
statistical	B
classification	B
of	O
remotely	O
sensed	O
satellite	O
imagery	O
j	O
int	O
assoc	O
for	O
mathematical	O
geology	O
references	O
switzer	O
p	O
some	O
spatial	O
statistics	O
for	O
the	O
interpretation	O
of	O
satellite	O
data	O
bull	O
int	O
stat	O
inst	O
thrun	O
s	O
b	O
mitchell	O
t	O
and	O
cheng	O
j	O
the	O
monk	O
s	O
comparison	O
of	O
learning	O
algorithms	O
introduction	O
and	O
survey	O
in	O
thrun	O
s	O
bala	O
j	O
bloedorn	O
e	O
and	O
bratko	O
i	O
editors	O
the	O
monk	O
s	O
problems	O
a	O
performance	O
comparison	O
of	O
different	O
learning	O
algorithms	O
pages	O
carnegie	O
mellon	O
university	O
computer	O
science	O
department	O
titterington	O
d	O
m	O
murray	O
g	O
d	O
murray	O
l	O
s	O
spiegelhalter	O
d	O
j	O
skene	O
a	O
m	O
habbema	O
j	O
d	O
f	O
and	O
gelpke	O
g	O
j	O
comparison	O
of	O
discrimination	B
techniques	O
applied	O
to	O
a	O
complex	B
data	O
set	O
of	O
head	O
injured	O
patients	O
discussion	O
j	O
royal	O
statist	O
soc	O
a	O
nearest	B
neighbour	I
method	O
the	O
influence	O
of	O
data	O
transformations	O
and	O
metrics	O
chemometrics	O
intell	O
labor	O
syst	O
todeschini	O
r	O
toolenaere	O
t	O
supersab	O
fast	O
adaptive	O
back	O
propagation	O
with	O
good	O
scaling	O
prop	O
erties	O
neural	B
networks	I
tsaptsinos	O
d	O
mirzai	O
a	O
and	O
jervis	O
b	O
comparison	O
of	O
machine	O
learning	O
paradigms	O
in	O
a	O
classification	B
task	O
in	O
rzevski	O
g	O
editor	O
applications	O
of	O
artificial	O
intelligence	O
in	O
engineering	O
v	O
proceedings	O
of	O
the	O
fifth	O
international	O
conference	O
berlin	O
springer-verlag	O
turing	O
a	O
m	O
lecture	O
to	O
the	O
london	O
mathematical	O
society	O
on	O
february	O
in	O
carpenter	O
b	O
e	O
and	O
doran	O
r	O
w	O
editors	O
a	O
m	O
turing	O
s	O
ace	B
report	O
and	O
other	O
papers	O
mit	O
press	O
cambridge	O
ma	O
unger	O
s	O
and	O
wysotzki	O
f	O
lernf	O
ahige	O
klassifizierungssysteme	O
akademie-verlag	O
berlin	O
urban	O
ci	O
c	O
t	O
and	O
bratko	O
i	O
knowledge	O
acquisition	O
for	O
dynamic	O
system	O
control	O
in	O
sou	O
cek	O
b	O
editor	O
dynamic	O
genetic	B
and	O
chaotic	O
programming	O
pages	O
wiley	O
sons	O
urban	O
ci	O
c	O
t	O
juri	O
ci	O
c	O
d	O
filipi	O
c	O
b	O
and	O
bratko	O
i	O
automated	O
synthesis	O
of	O
control	O
for	O
non-linear	O
dynamic	O
systems	O
in	O
preprints	O
of	O
the	O
ifacifipimacs	O
international	O
symposium	O
on	O
artificial	O
intelligence	O
in	O
real-time	O
control	O
pages	O
delft	O
the	O
netherlands	O
var	O
sek	O
a	O
urban	O
ci	O
c	O
t	O
and	O
filipi	O
c	O
b	O
genetic	B
algorithms	I
in	O
controller	B
design	I
and	O
tuning	O
ieee	O
transactions	O
on	O
systems	O
man	O
and	O
cybernetics	O
verbruggen	O
h	O
b	O
and	O
astr	O
om	O
k	O
j	O
artificial	O
intelligence	O
and	O
feedback	O
control	O
in	O
proceedings	O
of	O
the	O
second	O
ifac	O
workshop	O
on	O
artificial	O
intelligence	O
in	O
real-time	O
control	O
pages	O
shenyang	O
prc	O
wald	O
a	O
sequential	O
analysis	O
chapman	O
hall	O
london	O
wasserman	O
p	O
d	O
neural	O
computing	O
theory	O
and	O
practice	O
van	O
nostrand	O
reinhold	O
watkins	O
c	O
j	O
c	O
h	O
combining	O
cross-validation	O
and	O
search	O
in	O
bratko	O
i	O
and	O
lavrac	O
n	O
editors	O
progress	O
in	O
machine	O
learning	O
pages	O
wimslow	O
sigma	O
books	O
wehenkel	O
l	O
pavella	O
m	O
euxibie	O
e	O
and	O
heilbronn	O
b	O
decision	O
tree	O
based	O
transient	O
stability	O
assessment	O
a	O
case	O
study	O
volume	O
proceedings	O
of	O
ieeepes	O
winter	O
meeting	O
columbus	O
oh	O
janfeb	O
pages	O
paper	O
wm	O
pwrs	O
references	O
weiss	O
s	O
m	O
and	O
kapouleas	O
i	O
an	O
empirical	O
comparison	O
of	O
pattern	B
recognition	I
neural	O
nets	O
and	O
machine	O
learning	O
classification	B
methods	O
in	O
ijcai	O
proceedings	O
of	O
the	O
eleventh	O
international	O
joint	O
conference	O
on	O
artificial	O
intelligence	O
detroit	O
mi	O
pages	O
san	O
mateo	O
ca	O
morgan	O
kaufmann	O
weiss	O
s	O
m	O
and	O
kulikowski	O
c	O
a	O
computer	O
systems	O
that	O
learn	O
classification	B
and	O
prediction	O
methods	O
from	O
statistics	O
neural	B
networks	I
machine	O
learning	O
and	O
expert	B
systems	I
morgan	O
kaufmann	O
san	O
mateo	O
ca	O
werbos	O
p	O
beyond	O
regression	O
new	O
tools	O
for	O
prediction	O
and	O
analysis	O
in	O
the	O
behavioural	O
sciences	O
phd	O
thesis	O
harvard	O
university	O
also	O
printed	O
as	O
a	O
report	O
of	O
the	O
harvard	O
mit	O
cambridge	O
project	O
whittaker	O
j	O
graphical	O
models	O
in	O
applied	O
multivariate	O
analysis	O
john	O
wiley	O
chichester	O
widrow	O
b	O
generalization	O
and	O
information	O
in	O
networks	O
of	O
adaline	O
neurons	B
in	O
yovits	O
j	O
and	O
goldstein	O
editors	O
self-organizing	O
systems	O
washington	O
spartan	O
books	O
wolpert	O
d	O
h	O
a	O
rigorous	O
investigation	O
of	O
evidence	O
and	O
occam	O
factors	O
in	O
bayesian	O
reasoning	O
technical	B
report	O
the	O
sante	O
fe	O
institute	O
old	O
pecos	O
trail	O
suite	O
a	O
sante	O
fe	O
nm	O
usa	O
wu	O
j	O
x	O
and	O
chan	O
c	O
a	O
three	O
layer	O
adaptive	O
network	O
for	O
pattern	O
density	B
estimation	I
and	O
classification	B
international	O
journal	O
of	O
neural	O
systems	O
wynne-jones	O
m	O
constructive	B
algorithms	I
and	O
pruning	B
improving	O
the	O
multi	B
layer	I
perceptron	B
in	O
proceedings	O
of	O
imacs	O
the	O
world	O
congress	O
on	O
computation	O
and	O
applied	O
mathematics	O
dublin	O
volume	O
pages	O
wynne-jones	O
m	O
node	O
splitting	O
a	O
constructive	O
algorithm	O
for	O
feed-forard	O
neural	B
networks	I
in	O
moody	O
j	O
e	O
hanson	O
s	O
j	O
and	O
lippmann	O
r	O
p	O
editors	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pages	O
morgan	O
kaufmann	O
wynne-jones	O
m	O
node	O
splitting	O
a	O
constructive	O
algorithm	O
for	O
feed-forward	O
neural	B
networks	I
neural	O
computing	O
and	O
applications	O
xu	O
l	O
krzyzak	O
a	O
and	O
oja	O
e	O
neural	O
nets	O
for	O
dual	O
subspace	O
pattern	B
recognition	I
method	O
international	O
journal	O
of	O
neural	O
systems	O
yang	O
j	O
and	O
honavar	O
v	O
experiments	O
with	O
the	O
cascade-correlation	O
algorithm	O
technical	B
report	O
department	O
of	O
computer	O
science	O
iowa	O
state	O
university	O
ames	O
ia	O
usa	O
yasunobu	O
s	O
and	O
hasegawa	O
t	O
evaluation	O
of	O
an	O
automatic	O
container	O
crane	O
operation	O
system	O
based	O
on	O
predictive	O
fuzzy	O
control	O
control-theory	O
and	O
advanced	O
technology	O
index	O
accuracy	B
ace	B
algorithms	O
function	B
approximation	I
algorithms	O
instance-based	B
algorithms	O
symbolic	B
learning	I
alternating	B
conditional	I
expectation	I
analysis	B
of	I
results	I
aocdl	B
aq	B
aq	B
architectures	B
assistant	B
attribute	B
coding	I
attribute	B
entropy	B
attribute	B
noise	B
attribute	B
reduction	I
attribute	B
types	I
attribute	B
vector	I
attributes	B
australian	B
credit	I
dataset	I
background	B
knowledge	I
backprop	B
bayes	B
minimum	B
cost	I
rule	I
bayes	B
rule	I
bayes	B
rule	I
bayes	B
theorem	I
bayes	B
tree	I
bayes	B
tree	I
bayes-tree	B
bayesian	B
evidence	I
bayesian	B
methods	I
bayesian	B
networks	I
bayesian	O
regularisation	B
cascade	B
correlation	B
behavioural	B
cloning	I
belgian	B
power	I
i	I
dataset	I
belgian	B
power	I
ii	I
dataset	I
bias	B
bifrost	B
binary	B
attributes	B
binomial	B
bootstrap	B
boxes	B
canonical	B
correlation	B
canonical	B
discriminants	I
canonical	B
variates	I
cart	B
cascade	B
cascade	B
correlation	B
castle	B
categorical	B
variables	I
causal	B
network	I
causal	B
networks	I
chaid	B
chernobyl	B
chi-square	B
test	I
of	I
independence	I
choice	B
of	I
variables	I
chromosome	B
dataset	I
class	B
class	B
definitions	I
class	B
entropy	B
class	B
probability	I
tree	I
class	B
probability	I
trees	I
classes	B
classical	B
discrimination	B
techniques	I
classification	B
classification	B
rule	I
classification	B
definition	O
cls	B
clustering	B
code	B
vector	I
coding	B
of	I
categories	I
combination	B
of	I
attributes	B
combinations	B
of	I
variables	I
comparative	B
trials	I
complex	B
comprehensibility	B
concept	B
concept	B
learning	I
concept	B
learning	I
system	I
concept-recognisers	B
condensed	B
nearest	B
neighbour	I
conditional	B
dependency	I
conjugate	B
gradient	I
constructive	B
algorithms	I
constructive	B
algorithms	I
pruning	B
container	B
cranes	I
index	O
controller	B
design	I
corr	B
abs	I
correlation	B
correspondence	B
analysis	I
cost	B
datasets	I
cost	B
matrices	I
cost	B
matrix	I
costs	B
covariance	B
covariance	B
matrix	I
cover	B
covering	B
algorithm	I
credit	B
datasets	I
credit	B
management	I
dataset	I
credit	B
scoring	I
cross	B
validation	I
cross-entropy	B
dataset	O
dataset	O
dag	O
acyclic	O
graph	O
data	O
soybean	B
dataset	O
australian	B
credit	I
dataset	I
belgian	B
power	I
i	I
dataset	I
belgian	B
power	I
ii	I
dataset	I
chromosomes	B
dataset	O
credit	B
management	I
dataset	I
cut	B
dataset	O
diabetes	B
dataset	I
dna	B
dataset	I
german	B
credit	I
dataset	I
dataset	O
hand-written	B
digits	B
dataset	I
head	B
injury	I
index	O
dataset	O
heart	B
disease	I
dataset	O
image	B
segmentation	B
dataset	I
karhunen-loeve	B
digits	B
dataset	I
letter	B
recognition	I
dataset	O
machine	B
faults	I
dataset	I
satellite	B
image	I
dataset	I
shuttle	B
control	I
dataset	O
technical	B
dataset	I
technical	B
dataset	I
tsetse	O
fly	O
distribution	O
dataset	O
vehicle	B
recognition	I
dataset	O
credit	B
management	I
dataset	I
cut	B
dataset	O
karhunen-loeve	B
digits	B
dataset	I
shuttle	B
control	I
dataset	O
shuttle	B
dataset	B
characterisation	I
dataset	B
collection	I
decision	B
class	B
decision	B
problems	I
decision	B
trees	I
default	B
default	B
rule	I
density	B
estimates	I
density	B
estimation	I
diabetes	B
dataset	I
digits	B
dataset	I
directed	O
acyclic	O
graph	O
discrim	B
discrimination	B
distance	B
distribution-free	B
methods	I
dna	B
dataset	I
domain	B
knowledge	I
dominators	B
ea	B
ecg	B
edited	B
nearest	B
neighbour	I
en	O
attr	O
entropy	B
entropy	B
estimation	I
entropy	B
of	I
attributes	B
entropy	B
of	I
classes	B
epistemologically	B
adequate	I
equivalent	B
number	I
of	I
attributes	B
error	B
rate	I
error	B
rate	I
estimation	I
evaluation	B
assistant	B
examples	B
of	I
classifiers	I
expert	B
systems	I
extensions	B
to	I
linear	B
discrimination	B
features	B
feed-forward	B
networks	I
feedforward	B
network	I
first	B
order	I
logic	I
fisher	O
s	O
linear	B
discriminant	I
fract	B
k	I
fractk	B
gaussian	B
distribution	I
general-to-specific	B
generalised	B
delta	I
rule	I
generalised	O
linear	O
models	O
genetic	B
genetic	B
algorithms	I
genetic	B
algorithms	I
german	B
credit	I
gini	B
function	I
gini	B
index	I
glim	B
golem	B
golem	B
gradient	B
descent	I
gradient	B
descent	I
mlp	B
gradient	B
descent	I
second-order	B
gradient	B
methods	I
head	B
dataset	I
head	B
injury	I
dataset	I
heart	B
dataset	I
heuristically	B
adequate	I
hidden	B
nodes	I
hierarchical	B
clustering	B
hierarchical	B
structure	I
hierarchy	B
human	B
brain	I
hypothesis	B
language	I
ilp	B
image	B
datasets	I
image	B
segmentation	I
impure	B
impure	B
node	I
impurity	B
ind	B
package	I
ind	B
package	I
indcart	B
indicator	B
variables	I
inductive	B
learning	I
inductive	B
logic	I
programming	I
inductive	B
logic	I
programming	I
inductive	B
logic	I
programming	I
information	B
measures	B
information	B
score	I
information	B
theory	I
instatnce-based	O
learning	O
iris	B
data	I
irrelevant	B
attributes	B
isoft	B
dataset	I
itrule	B
j-measure	B
index	O
jackknife	B
joint	B
entropy	B
k	B
nearest	B
neighbour	I
k-means	B
clustering	B
k-means	B
clustering	B
k-nearest	B
neighbour	I
k-nearest	B
neighbour	I
k-nearest	B
neighbour	I
k-nn	B
k-nn	B
cross	B
validation	I
k-r-k	B
problem	I
kalman	B
filter	I
kardio	B
kernel	O
classifier	B
kernel	O
window	B
width	I
kernel	O
density	O
kernel	B
density	B
estimation	I
kernel	B
function	I
kernels	B
kl	B
digits	B
dataset	I
kohonen	B
kohonen	B
networks	I
kohonen	B
self-organising	I
net	I
kullback-leibler	B
information	I
kurtosis	B
layer	O
hidden	B
layer	O
input	B
layer	O
output	B
learning	B
curves	I
learning	B
graphical	I
representations	I
learning	O
vector	O
quantization	O
learning	B
vector	I
quantizer	I
learning	B
vector	B
quantizers	I
leave-one-out	B
letters	B
dataset	I
likelihood	B
ratio	I
linear	B
decision	I
tree	I
index	O
linear	B
discriminant	I
linear	B
discrimination	B
linear	B
independent	I
linear	B
regression	I
linear	O
threshold	O
unit	O
linear	B
transformation	B
linear	B
trees	I
linesearches	B
link	B
function	I
log	B
likelihood	I
logdisc	B
logistic	B
discriminant	I
logistic	B
discrimination	B
logistic	B
discrimination	B
programming	O
lvq	B
m	B
statistic	I
machine	B
faults	I
dataset	I
machine	B
learning	I
approaches	I
machine	B
learning	I
approaches	I
to	O
classifica	O
tion	B
madaline	B
manova	B
many	B
categories	I
marginalisation	B
mars	B
maximum	B
conditional	I
likelihood	I
maximum	B
likelihood	I
mcculloch-pitts	B
neuron	I
mdl	B
measure	B
of	I
collinearity	I
measures	B
measures	B
information-based	B
measures	B
statistical	B
measures	B
of	I
normality	I
medical	B
datasets	I
memory	B
mental	B
fit	I
mental	B
fit	I
metalevel	B
learning	I
minimisation	B
methods	I
minimum	B
cost	I
rule	I
minimum	O
description	O
length	O
prin	O
ciple	B
minimum	B
risk	I
rule	I
misclassification	B
costs	B
missing	B
values	I
ml	B
on	I
ml	I
mlp	B
mntal	B
fit	I
multi	B
layer	I
perceptron	B
multi	B
layer	I
perceptron	B
functionality	B
multi-class	B
trees	I
multidimensional	B
scaling	I
multimodality	B
multivariate	O
analysis	O
of	O
variance	O
multivariate	B
kurtosis	B
multivariate	B
normality	I
multivariate	B
skewness	B
mutual	B
information	I
naive	B
bayes	I
nearest	B
neighbour	I
nearest	B
neighbour	I
example	B
neural	B
network	I
approaches	I
neural	B
networks	I
neurons	B
newid	B
no	B
data	I
rule	I
node	O
hidden	B
node	O
impure	B
node	I
input	B
node	O
output	B
node	O
purity	B
node	O
winning	B
noise	B
noise	B
signal	I
ratio	I
noisy	B
noisy	B
data	I
nonlinear	B
regression	I
nonparametric	B
density	I
estimator	I
nonparametric	B
methods	I
nonparametric	B
statistics	I
normal	B
distribution	I
ns	O
ratio	O
object	B
recognition	I
datasets	I
observation	B
language	I
odds	B
optimisation	B
ordered	B
categories	I
over-fitting	B
overfitting	B
parametric	B
methods	I
partitioning	B
as	I
classification	B
parzen	B
window	I
pattern	B
recognition	I
perceptron	B
performance	B
measures	B
performance	B
prediction	I
plug-in	B
estimates	I
polak-ribiere	B
pole	B
balancing	I
polytrees	B
polytrees	B
polytrees	B
as	B
classifiers	I
pooled	B
covariance	B
matrix	I
prediction	B
as	I
classification	B
preprocessing	B
primary	B
attribute	I
prior	O
uniform	B
prior	B
probabilities	I
probabilistic	B
inference	I
products	B
of	I
attributes	B
projection	B
pursuit	I
projection	B
pursuit	I
projection	B
pursuit	I
classification	B
propositional	B
learning	I
systems	I
index	O
prototypes	B
pruning	B
pruning	B
backward	B
pruning	B
cost	B
complexity	I
pruning	B
forward	B
purity	B
purity	B
measure	B
purity	B
measure	B
quadisc	B
quadiscr	B
quadratic	B
discriminant	I
quadratic	B
discriminants	I
quadratic	B
functions	I
of	I
attributes	B
radial	B
basis	I
function	I
radial	B
basis	I
function	I
radial	B
basis	I
function	I
network	I
ramnets	B
rbf	B
recurrent	B
networks	I
recursive	B
partitioning	I
reduced	B
nearest	B
neighbour	I
reference	B
class	B
regression	B
tree	I
regularisation	B
relational	B
learning	I
retis	B
rg	B
risk	B
assessment	I
rule-based	B
methods	I
rule-learning	B
satellite	B
image	I
dataset	I
scaling	B
parameter	I
scatterplot	B
smoother	I
sdratio	B
secific-to-general	B
secondary	B
attribute	I
segmentation	B
dataset	I
selector	B
time	B
time	B
to	I
learn	I
time	B
to	I
test	I
train-and-test	B
training	O
optimisation	B
training	B
set	I
transformation	B
transformation	B
of	I
attributes	B
transformations	B
of	I
variables	I
tree-learning	B
trees-into-rules	B
tsetse	B
dataset	I
tuning	B
of	I
parameters	I
uk	B
credit	I
dataset	I
uniform	B
distribution	I
univariate	B
kurtosis	B
univariate	B
skewness	B
universal	B
approximators	I
universal	B
computers	I
unsupervised	B
learning	I
upstart	B
user	O
s	O
guide	O
to	O
algorithms	O
vector	B
quantizers	I
vehicle	B
vehicle	B
dataset	I
vertebrate	B
vertebrate	B
species	I
voronoi	B
tessellation	I
xpertrule	B
yardstick	B
methods	I
zero	B
variance	I
index	O
shuttle	B
shuttle	B
dataset	I
simulated	B
digits	I
data	I
skew	B
abs	I
skewness	B
smart	B
smoothing	B
parameter	I
smoothing	B
parameters	I
snr	B
specific-to-general	B
speed	B
splitiing	B
criteria	I
splitting	B
criteria	I
splitting	B
criterion	I
splus	B
statistical	B
approaches	I
to	I
classification	B
statistical	B
measures	B
statlog	B
statlog	B
collection	B
of	I
data	I
statlog	B
objectives	B
statlog	B
preprocessing	B
stepwise	B
selection	I
stochastic	B
gradient	I
storage	B
structured	B
induction	I
subset	B
selection	I
sum	B
of	I
squares	I
supervised	B
learning	I
supervised	B
networks	I
supervised	B
vector	I
supervisor	B
symbolic	B
learning	I
symbolic	B
ml	I
taxonomic	B
taxonomy	B
technical	B
dataset	I
tertiary	B
attribute	I
test	B
environment	I
test	B
set	I
three-mile	B
island	I
tiling	B
algorithm	I
